DiÔ¨Äerential Equations
Linear, Nonlinear, Ordinary, Partial
A.C. King, J. Billingham and S.R. Otto

Ôù£Ôù°Ôù≠Ôù¢Ôù≤Ôù©Ôù§ÔùßÔù• ÔùµÔùÆÔù©Ôù∂Ôù•Ôù≤Ôù≥Ôù©Ôù¥Ôùπ Ôù∞Ôù≤Ôù•Ôù≥Ôù≥
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, S√£o Paulo
Cambridge University Press
The Edinburgh Building, Cambridge Ôù£Ôù¢Ôú≤ Ôú≤Ôù≤Ôùµ, United Kingdom
First published in print format 
Ôù©Ôù≥Ôù¢ÔùÆ-Ôú±Ôú≥    ÔúπÔú∑Ôú∏-Ôú∞-ÔúµÔú≤Ôú±-Ôú∏Ôú±Ôú∂ÔúµÔú∏-Ôú∏
Ôù©Ôù≥Ôù¢ÔùÆ-Ôú±Ôú≥    ÔúπÔú∑Ôú∏-Ôú∞-ÔúµÔú≤Ôú±-Ôú∞Ôú±Ôú∂Ôú∏Ôú∑-Ôú≤
Ôù©Ôù≥Ôù¢ÔùÆ-Ôú±Ôú≥    ÔúπÔú∑Ôú∏-Ôú∞-ÔúµÔú±Ôú±-Ôú∞Ôú∑Ôú∏Ôú≥Ôú±-Ôú∂
¬© Cambridge University Press 2003
2003
Information on this title: www.cambridge.org/9780521816588
This book is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.
Ôù©Ôù≥Ôù¢ÔùÆ-Ôú±Ôú∞    Ôú∞-ÔúµÔú±Ôú±-Ôú∞Ôú∑Ôú∏Ôú≥Ôú±-Ôúµ
Ôù©Ôù≥Ôù¢ÔùÆ-Ôú±Ôú∞    Ôú∞-ÔúµÔú≤Ôú±-Ôú∏Ôú±Ôú∂ÔúµÔú∏-Ôú∞
Ôù©Ôù≥Ôù¢ÔùÆ-Ôú±Ôú∞    Ôú∞-ÔúµÔú≤Ôú±-Ôú∞Ôú±Ôú∂Ôú∏Ôú∑-Ôú∏
Cambridge University Press has no responsibility for the persistence or accuracy of
ÔùµÔù≤Ôù¨s for external or third-party internet websites referred to in this book, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
hardback
paperback
paperback
eBook (NetLibrary)
eBook (NetLibrary)
hardback

Contents
Preface
page ix
Part One: Linear Equations
1
1
Variable CoeÔ¨Écient, Second Order, Linear, Ordinary
DiÔ¨Äerential Equations
3
1.1
The Method of Reduction of Order
5
1.2
The Method of Variation of Parameters
7
1.3
Solution by Power Series: The Method of Frobenius
11
2
Legendre Functions
31
2.1
DeÔ¨Ånition of the Legendre Polynomials, Pn(x)
31
2.2
The Generating Function for Pn(x)
35
2.3
DiÔ¨Äerential and Recurrence Relations Between Legendre
Polynomials
38
2.4
Rodrigues‚Äô Formula
39
2.5
Orthogonality of the Legendre Polynomials
41
2.6
Physical Applications of the Legendre Polynomials
44
2.7
The Associated Legendre Equation
52
3
Bessel Functions
58
3.1
The Gamma Function and the Pockhammer Symbol
58
3.2
Series Solutions of Bessel‚Äôs Equation
60
3.3
The Generating Function for Jn(x), n an integer
64
3.4
DiÔ¨Äerential and Recurrence Relations Between Bessel Functions
69
3.5
ModiÔ¨Åed Bessel Functions
71
3.6
Orthogonality of the Bessel Functions
71
3.7
Inhomogeneous Terms in Bessel‚Äôs Equation
77
3.8
Solutions Expressible as Bessel Functions
79
3.9
Physical Applications of the Bessel Functions
80
4
Boundary Value Problems, Green‚Äôs Functions and
Sturm‚ÄìLiouville Theory
93
4.1
Inhomogeneous Linear Boundary Value Problems
96
4.2
The Solution of Boundary Value Problems by Eigenfunction
Expansions
100
4.3
Sturm‚ÄìLiouville Systems
107

vi
CONTENTS
5
Fourier Series and the Fourier Transform
123
5.1
General Fourier Series
127
5.2
The Fourier Transform
133
5.3
Green‚Äôs Functions Revisited
141
5.4
Solution of Laplace‚Äôs Equation Using Fourier Transforms
143
5.5
Generalization to Higher Dimensions
145
6
Laplace Transforms
152
6.1
DeÔ¨Ånition and Examples
152
6.2
Properties of the Laplace Transform
154
6.3
The Solution of Ordinary DiÔ¨Äerential Equations using Laplace
Transforms
157
6.4
The Inversion Formula for Laplace Transforms
162
7
ClassiÔ¨Åcation, Properties and Complex Variable Methods for
Second Order Partial DiÔ¨Äerential Equations
175
7.1
ClassiÔ¨Åcation and Properties of Linear, Second Order Partial
DiÔ¨Äerential Equations in Two Independent Variables
175
7.2
Complex Variable Methods for Solving Laplace‚Äôs Equation
186
Part Two: Nonlinear Equations and Advanced Techniques
201
8
Existence, Uniqueness, Continuity and Comparison of
Solutions of Ordinary DiÔ¨Äerential Equations
203
8.1
Local Existence of Solutions
204
8.2
Uniqueness of Solutions
210
8.3
Dependence of the Solution on the Initial Conditions
211
8.4
Comparison Theorems
212
9
Nonlinear Ordinary DiÔ¨Äerential Equations: Phase Plane
Methods
217
9.1
Introduction: The Simple Pendulum
217
9.2
First Order Autonomous Nonlinear Ordinary DiÔ¨Äerential
Equations
222
9.3
Second Order Autonomous Nonlinear Ordinary DiÔ¨Äerential
Equations
224
9.4
Third Order Autonomous Nonlinear Ordinary DiÔ¨Äerential
Equations
249
10
Group Theoretical Methods
256
10.1
Lie Groups
257
10.2
Invariants Under Group Action
261
10.3
The Extended Group
262
10.4
Integration of a First Order Equation with a Known Group
Invariant
263

CONTENTS
vii
10.5
Towards the Systematic Determination of Groups Under Which
a First Order Equation is Invariant
265
10.6
Invariants for Second Order DiÔ¨Äerential Equations
266
10.7
Partial DiÔ¨Äerential Equations
270
11
Asymptotic Methods: Basic Ideas
274
11.1
Asymptotic Expansions
275
11.2
The Asymptotic Evaluation of Integrals
280
12
Asymptotic Methods: DiÔ¨Äerential Equations
303
12.1
An Instructive Analogy: Algebraic Equations
303
12.2
Ordinary DiÔ¨Äerential Equations
306
12.3
Partial DiÔ¨Äerential Equations
351
13
Stability, Instability and Bifurcations
372
13.1
Zero Eigenvalues and the Centre Manifold Theorem
372
13.2
Lyapunov‚Äôs Theorems
381
13.3
Bifurcation Theory
388
14
Time-Optimal Control in the Phase Plane
417
14.1
DeÔ¨Ånitions
418
14.2
First Order Equations
418
14.3
Second Order Equations
422
14.4
Examples of Second Order Control Problems
426
14.5
Properties of the Controllable Set
429
14.6
The Controllability Matrix
433
14.7
The Time-Optimal Maximum Principle (TOMP)
436
15
An Introduction to Chaotic Systems
447
15.1
Three Simple Chaotic Systems
447
15.2
Mappings
452
15.3
The Poincar¬¥e Return Map
467
15.4
Homoclinic Tangles
472
15.5
Quantifying Chaos: Lyapunov Exponents and the Lyapunov
Spectrum
484
Appendix 1 Linear Algebra
495
Appendix 2 Continuity and DiÔ¨Äerentiability
502
Appendix 3 Power Series
505
Appendix 4 Sequences of Functions
509
Appendix 5 Ordinary DiÔ¨Äerential Equations
511
Appendix 6 Complex Variables
517
Appendix 7 A Short Introduction to MATLAB
526
Bibliography
534
Index
536

Preface
When mathematical modelling is used to describe physical, biological or chemical
phenomena, one of the most common results is either a diÔ¨Äerential equation or
a system of diÔ¨Äerential equations, together with appropriate boundary and initial
conditions. These diÔ¨Äerential equations may be ordinary or partial, and Ô¨Ånding
and interpreting their solution is at the heart of applied mathematics. A thorough
introduction to diÔ¨Äerential equations is therefore a necessary part of the education
of any applied mathematician, and this book is aimed at building up skills in this
area. For similar reasons, the book should also be of use to mathematically-inclined
physicists and engineers.
Although the importance of studying diÔ¨Äerential equations is not generally in
question, exactly how the theory of diÔ¨Äerential equations should be taught, and
what aspects should be emphasized, is more controversial. In our experience, text-
books on diÔ¨Äerential equations usually fall into one of two categories. Firstly, there
is the type of textbook that emphasizes the importance of abstract mathematical
results, proving each of its theorems with full mathematical rigour. Such textbooks
are usually aimed at graduate students, and are inappropriate for the average un-
dergraduate. Secondly, there is the type of textbook that shows the student how
to construct solutions of diÔ¨Äerential equations, with particular emphasis on algo-
rithmic methods. These textbooks often tackle only linear equations, and have no
pretension to mathematical rigour. However, they are usually well-stocked with
interesting examples, and often include sections on numerical solution methods.
In this textbook, we steer a course between these two extremes, starting at the
level of preparedness of a typical, but well-motivated, second year undergraduate
at a British university. As such, the book begins in an unsophisticated style with
the clear objective of obtaining quantitative results for a particular linear ordi-
nary diÔ¨Äerential equation. The text is, however, written in a progressive manner,
with the aim of developing a deeper understanding of ordinary and partial diÔ¨Äer-
ential equations, including conditions for the existence and uniqueness of solutions,
solutions by group theoretical and asymptotic methods, the basic ideas of con-
trol theory, and nonlinear systems, including bifurcation theory and chaos. The
emphasis of the book is on analytical and asymptotic solution methods. However,
where appropriate, we have supplemented the text by including numerical solutions
and graphs produced using MATLAB‚Ä†, version 6. We assume some knowledge of
‚Ä† MATLAB is a registered trademark of The MathWorks, Inc.

x
PREFACE
MATLAB (summarized in Appendix 7), but explain any nontrivial aspects as they
arise. Where mathematical rigour is required, we have presented the appropriate
analysis, on the basis that the student has taken Ô¨Årst courses in analysis and linear
algebra. We have, however, avoided any functional analysis. Most of the material
in the book has been taught by us in courses for undergraduates at the University
of Birmingham. This has given us some insight into what students Ô¨Ånd diÔ¨Écult,
and, as a consequence, what needs to be emphasized and re-iterated.
The book is divided into two parts. In the Ô¨Årst of these, we tackle linear diÔ¨Äer-
ential equations. The Ô¨Årst three chapters are concerned with variable coeÔ¨Écient,
linear, second order ordinary diÔ¨Äerential equations, emphasizing the methods of
reduction of order and variation of parameters, and series solution by the method
of Frobenius. In particular, we discuss Legendre functions (Chapter 2) and Bessel
functions (Chapter 3) in detail, and motivate this by giving examples of how they
arise in real modelling problems. These examples lead to partial diÔ¨Äerential equa-
tions, and we use separation of variables to obtain Legendre‚Äôs and Bessel‚Äôs equa-
tions. In Chapter 4, the emphasis is on boundary value problems, and we show
how these diÔ¨Äer from initial value problems. We introduce Sturm‚ÄìLiouville theory
in this chapter, and prove various results on eigenvalue problems. The next two
chapters of the Ô¨Årst part of the book are concerned with Fourier series, and Fourier
and Laplace transforms. We discuss in detail the convergence of Fourier series, since
the analysis involved is far more straightforward than that associated with other
basis functions. Our approach to Fourier transforms involves a short introduction
to the theory of generalized functions. The advantage of this approach is that a
discussion of what types of function possess a Fourier transform is straightforward,
since all generalized functions possess a Fourier transform. We show how Fourier
transforms can be used to construct the free space Green‚Äôs function for both ordi-
nary and partial diÔ¨Äerential equations. We also use Fourier transforms to derive
the solutions of the Dirichlet and Neumann problems for Laplace‚Äôs equation. Our
discussion of the Laplace transform includes an outline proof of the inversion the-
orem, and several examples of physical problems, for example involving diÔ¨Äusion,
that can be solved by this method. In Chapter 7 we discuss the classiÔ¨Åcation of
linear, second order partial diÔ¨Äerential equations, emphasizing the reasons why the
canonical examples of elliptic, parabolic and hyperbolic equations, namely Laplace‚Äôs
equation, the diÔ¨Äusion equation and the wave equation, have the properties that
they do. We also consider complex variable methods for solving Laplace‚Äôs equation,
emphasizing their application to problems in Ô¨Çuid mechanics.
The second part of the book is concerned with nonlinear problems and more
advanced techniques. Although we have used a lot of the material in Chapters 9
and 14 (phase plane techniques and control theory) in a course for second year
undergraduates, the bulk of the material here is aimed at third year students. We
begin in Chapter 8 with a brief introduction to the rigorous analysis of ordinary
diÔ¨Äerential equations.
Here the emphasis is on existence, uniqueness and com-
parison theorems. In Chapter 9 we introduce the phase plane and its associated
techniques. This is the Ô¨Årst of three chapters (the others being Chapters 13 and 15)
that form an introduction to the theory of nonlinear ordinary diÔ¨Äerential equations,

PREFACE
xi
often known as dynamical systems. In Chapter 10, we show how the ideas of group
theory can be used to Ô¨Ånd exact solutions of ordinary and partial diÔ¨Äerential equa-
tions. In Chapters 11 and 12 we discuss the theory and practice of asymptotic
analysis. After discussing the basic ideas at the beginning of Chapter 11, we move
on to study the three most important techniques for the asymptotic evaluation of
integrals: Laplace‚Äôs method, the method of stationary phase and the method of
steepest descents. Chapter 12 is devoted to the asymptotic solution of diÔ¨Äerential
equations, and we introduce the method of matched asymptotic expansions, and
the associated idea of asymptotic matching, the method of multiple scales, includ-
ing Kuzmak‚Äôs method for analysing the slow damping of nonlinear oscillators, and
the WKB expansion. We illustrate each of these methods with a wide variety of
examples, for both nonlinear ordinary diÔ¨Äerential equations and partial diÔ¨Äerential
equations. In Chapter 13 we cover the centre manifold theorem, Lyapunov func-
tions and an introduction to bifurcation theory. Chapter 14 is about time-optimal
control theory in the phase plane, and includes a discussion of the controllability
matrix and the time-optimal maximum principle for second order linear systems of
ordinary diÔ¨Äerential equations. Chapter 15 is on chaotic systems, and, after some
illustrative examples, emphasizes the theory of homoclinic tangles and Mel‚Äônikov
theory.
There is a set of exercises at the end of each chapter.
Harder exercises are
marked with a star, and many chapters include a project, which is rather longer
than the average exercise, and whose solution involves searches in the library or on
the Internet, and deeper study. Bona Ô¨Åde teachers and instructors can obtain full
worked solutions to many of the exercises by emailing solutions@cambridge.org.
In order to follow many of the ideas and calculations that we describe in this
book, and to fully appreciate the more advanced material, the reader may need
to acquire (or refresh) some basic skills.
These are covered in the appendices,
and fall into six basic areas: linear algebra, continuity and diÔ¨Äerentiability, power
series, sequences and series of functions, ordinary diÔ¨Äerential equations and complex
variables.
We would like to thank our friends and colleagues, Adam Burbidge (Nestl¬¥e Re-
search Centre, Lausanne), Norrie Everitt (Birmingham), Chris Good (Birming-
ham), Ray Jones (Birmingham), John King (Nottingham), Dave Needham (Read-
ing), Nigel Scott (East Anglia) and Warren Smith (Birmingham), who read and
commented on one or more chapters of the book before it was published.
Any
nonsense remaining is, of course, our fault and not theirs.
ACK, JB and SRO, Birmingham 2002

Part One
Linear Equations
1

CHAPTER ONE
Variable CoeÔ¨Écient, Second Order, Linear,
Ordinary DiÔ¨Äerential Equations
Many physical, chemical and biological systems can be described using mathemat-
ical models. Once the model is formulated, we usually need to solve a diÔ¨Äerential
equation in order to predict and quantify the features of the system being mod-
elled. As a precursor to this, we consider linear, second order ordinary diÔ¨Äerential
equations of the form
P(x)d 2y
dx2 + Q(x)dy
dx + R(x)y = F(x),
with P(x), Q(x) and R(x) Ô¨Ånite polynomials that contain no common factor. This
equation is inhomogeneous and has variable coeÔ¨Écients. The form of these poly-
nomials varies according to the underlying physical problem that we are studying.
However, we will postpone any discussion of the physical origin of such equations
until we have considered some classical mathematical models in Chapters 2 and 3.
After dividing through by P(x), we obtain the more convenient, equivalent form,
d 2y
dx2 + a1(x)dy
dx + a0(x)y = f(x).
(1.1)
This process is mathematically legitimate, provided that P(x) Ã∏= 0. If P(x0) = 0
at some point x = x0, it is not legitimate, and we call x0 a singular point of the
equation. If P(x0) Ã∏= 0, x0 is a regular or ordinary point of the equation. If
P(x) Ã∏= 0 for all points x in the interval where we want to solve the equation, we
say that the equation is nonsingular, or regular, on the interval.
We usually need to solve (1.1) subject to either initial conditions of the form
y(a) = Œ±, y‚Ä≤(a) = Œ≤ or boundary conditions, of which y(a) = Œ± and y(b) = Œ≤
are typical examples. It is worth reminding ourselves that, given the ordinary dif-
ferential equation and initial conditions (an initial value problem), the objective
is to determine the solution for other values of x, typically, x > a, as illustrated in
Figure 1.1. As an example, consider a projectile. The initial conditions are the po-
sition of the projectile and the speed and angle to the horizontal at which it is Ô¨Åred.
We then want to know the path of the projectile, given these initial conditions.
For initial value problems of this form, it is possible to show that:
(i) If a1(x), a0(x) and f(x) are continuous on some open interval I that contains
the initial point a, a unique solution of the initial value problem exists on
the interval I, as we shall demonstrate in Chapter 8.

4
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
calculate the
solution for x > a
given y(a) = Œ±
and y'(a) = Œ≤
y
Œ±
a
x
Fig. 1.1. An initial value problem.
(ii) The structure of the solution of the initial value problem is of the form
y =
A u1(x) + B u2(x)



Complementary function
+
G(x)
  
Particular integral
,
where A, B are constants that are Ô¨Åxed by the initial conditions and u1(x)
and u2(x) are linearly independent solutions of the corresponding homoge-
neous problem y‚Ä≤‚Ä≤ + a1(x)y‚Ä≤ + a0(x)y = 0.
These results can be proved rigorously, but nonconstructively, by studying the
operator
Ly ‚â°d 2y
dx2 + a1(x)dy
dx + a0(x)y,
and regarding L : C2(I) ‚ÜíC0(I) as a linear transformation from the space of
twice-diÔ¨Äerentiable functions deÔ¨Åned on the interval I to the space of continuous
functions deÔ¨Åned on I. The solutions of the homogeneous equation are elements
of the null space of L.
This subspace is completely determined once its basis
is known.
The solution of the inhomogeneous problem, Ly = f, is then given
formally as y = L‚àí1f. Unfortunately, if we actually want to construct the solution
of a particular equation, there is a lot more work to do.
Before we try to construct the general solution of the inhomogeneous initial value
problem, we will outline a series of subproblems that are more tractable.

1.1 THE METHOD OF REDUCTION OF ORDER
5
1.1
The Method of Reduction of Order
As a Ô¨Årst simpliÔ¨Åcation we discuss the solution of the homogeneous diÔ¨Äerential
equation
d 2y
dx2 + a1(x)dy
dx + a0(x)y = 0,
(1.2)
on the assumption that we know one solution, say y(x) = u1(x), and only need to
Ô¨Ånd the second solution. We will look for a solution of the form y(x) = U(x)u1(x).
DiÔ¨Äerentiating y(x) using the product rule gives
dy
dx = dU
dx u1 + U du1
dx ,
d 2y
dx2 = d 2U
dx2 u1 + 2dU
dx
du1
dx + U d 2u1
dx2 .
If we substitute these expressions into (1.2) we obtain
d 2U
dx2 u1 + 2dU
dx
du1
dx + U d 2u1
dx2 + a1(x)
dU
dx u1 + U du1
dx

+ a0(x)Uu1 = 0.
We can now collect terms to get
U
d 2u1
dx2 + a1(x)du1
dx + a0(x)u1

+ u1
d 2U
dx2 + dU
dx

2du1
dx + a1u1

= 0.
Now, since u1(x) is a solution of (1.2), the term multiplying U is zero. We have
therefore obtained a diÔ¨Äerential equation for dU/dx, and, by deÔ¨Åning Z = dU/dx,
have
u1
dZ
dx + Z

2du1
dx + a1u1

= 0.
Dividing through by Zu1 we have
1
Z
dZ
dx + 2
u1
du1
dx + a1 = 0,
which can be integrated directly to yield
log |Z| + 2 log |u1| +
 x
a1(s) ds = C,
where s is a dummy variable, for some constant C. Thus
Z = c
u2
1
exp

‚àí
 x
a1(s) ds
	
= dU
dx
where c = eC. This can then be integrated to give
U(x) =
 x
c
u2
1(t) exp

‚àí
 t
a1(s) ds
	
dt + Àúc,
for some constant Àúc. The solution is therefore

6
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
y(x) = u1(x)
 x
c
u2
1(t) exp

‚àí
 t
a1(s) ds
	
dt + Àúcu1(x).
We can recognize Àúcu1(x) as the part of the complementary function that we knew
to start with, and
u2(x) = u1(x)
 x
1
u2
1(t) exp

‚àí
 t
a1(s)ds
	
dt
(1.3)
as the second part of the complementary function. This result is called the reduc-
tion of order formula.
Example
Let‚Äôs try to determine the full solution of the diÔ¨Äerential equation
(1 ‚àíx2)d 2y
dx2 ‚àí2xdy
dx + 2y = 0,
given that y = u1(x) = x is a solution. We Ô¨Årstly write the equation in standard
form as
d 2y
dx2 ‚àí
2x
1 ‚àíx2
dy
dx +
2
1 ‚àíx2 y = 0.
Comparing this with (1.2), we have a1(x) = ‚àí2x/(1 ‚àíx2). After noting that
 t
a1(s) ds =
 t
‚àí
2s
1 ‚àís2 ds = log(1 ‚àít2),
the reduction of order formula gives
u2(x) = x
 x 1
t2 exp

‚àílog(1 ‚àít2)

dt = x
 x
dt
t2(1 ‚àít2).
We can express the integrand in terms of its partial fractions as
1
t2(1 ‚àít2) = 1
t2 +
1
1 ‚àít2 = 1
t2 +
1
2(1 + t) +
1
2(1 ‚àít).
This gives the second solution of (1.2) as
u2(x) = x
 x  1
t2 +
1
2(1 + t) +
1
2(1 ‚àít)
	
dt
= x

‚àí1
t + 1
2 log
1 + t
1 ‚àít
x
= x
2 log
1 + x
1 ‚àíx

‚àí1,
and hence the general solution is
y = Ax + B
x
2 log
1 + x
1 ‚àíx

‚àí1
	
.

1.2 THE METHOD OF VARIATION OF PARAMETERS
7
1.2
The Method of Variation of Parameters
Let‚Äôs now consider how to Ô¨Ånd the particular integral given the complementary
function, comprising u1(x) and u2(x). As the name of this technique suggests, we
take the constants in the complementary function to be variable, and assume that
y = c1(x)u1(x) + c2(x)u2(x).
DiÔ¨Äerentiating, we Ô¨Ånd that
dy
dx = c1
du1
dx + u1
dc1
dx + c2
du2
dx + u2
dc2
dx .
We will choose to impose the condition
u1
dc1
dx + u2
dc2
dx = 0,
(1.4)
and thus have
dy
dx = c1
du1
dx + c2
du2
dx ,
which, when diÔ¨Äerentiated again, yields
d 2y
dx2 = c1
d 2u1
dx2 + du1
dx
dc1
dx + c2
d 2u2
dx2 + du2
dx
dc2
dx .
This form can then be substituted into the original diÔ¨Äerential equation to give
c1
d 2u1
dx2 + du1
dx
dc1
dx + c2
d 2u2
dx2 + du2
dx
dc2
dx + a1

c1
du1
dx + c2
du2
dx

+ a0 (c1u1 + c2u2) = f.
This can be rearranged to show that
c1
d 2u1
dx2 + a1
du1
dx + a0u1

+ c2
d 2u2
dx2 + a1
du2
dx + a0u2

+ du1
dx
dc1
dx + du2
dx
dc2
dx = f.
Since u1 and u2 are solutions of the homogeneous equation, the Ô¨Årst two terms are
zero, which gives us
du1
dx
dc1
dx + du2
dx
dc2
dx = f.
(1.5)
We now have two simultaneous equations, (1.4) and (1.5), for c‚Ä≤
1 = dc1/dx and
c‚Ä≤
2 = dc2/dx, which can be written in matrix form as
 u1
u2
u‚Ä≤
1
u‚Ä≤
2
  c‚Ä≤
1
c‚Ä≤
2

=
 0
f

.
These can easily be solved to give
c‚Ä≤
1 = ‚àífu2
W ,
c‚Ä≤
2 = fu1
W ,
where
W = u1u‚Ä≤
2 ‚àíu2u‚Ä≤
1 =

u1
u2
u‚Ä≤
1
u‚Ä≤
2


8
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
is called the Wronskian. These expressions can be integrated to give
c1 =
 x
‚àíf(s)u2(s)
W(s)
ds + A,
c2 =
 x f(s)u1(s)
W(s)
ds + B.
We can now write down the solution of the entire problem as
y(x) = u1(x)
 x
‚àíf(s)u2(s)
W(s)
ds + u2(x)
 x f(s)u1(s)
W(s)
ds + Au1(x) + Bu2(x).
The particular integral is therefore
y(x) =
 x
f(s)
u1(s)u2(x) ‚àíu1(x)u2(s)
W(s)
	
ds.
(1.6)
This is called the variation of parameters formula.
Example
Consider the equation
d 2y
dx2 + y = x sin x.
The homogeneous form of this equation has constant coeÔ¨Écients, with solutions
u1(x) = cos x,
u2(x) = sin x.
The variation of parameters formula then gives the particular integral as
y =
 x
s sin s
cos s sin x ‚àícos x sin s
1
	
ds,
since
W =

cos x
sin x
‚àísin x
cos x
 = cos2 x + sin2 x = 1.
We can split the particular integral into two integrals as
y(x) = sin x
 x
s sin s cos s ds ‚àícos x
 x
s sin2 s ds
= 1
2 sin x
 x
s sin 2s ds ‚àí1
2 cos x
 x
s (1 ‚àícos 2s) ds.
Using integration by parts, we can evaluate this, and Ô¨Ånd that
y(x) = ‚àí1
4x2 cos x + 1
4x sin x + 1
8 cos x
is the required particular integral. The general solution is therefore
y = c1 cos x + c2 sin x ‚àí1
4x2 cos x + 1
4x sin x.
Although we have given a rational derivation of the reduction of order and vari-
ation of parameters formulae, we have made no comment so far about why the
procedures we used in the derivation should work at all! It turns out that this has
a close connection with the theory of continuous groups, which we will investigate
in Chapter 10.

1.2 THE METHOD OF VARIATION OF PARAMETERS
9
1.2.1
The Wronskian
Before we carry on, let‚Äôs pause to discuss some further properties of the Wronskian.
Recall that if V is a vector space over R, then two elements v1, v2 ‚ààV are linearly
dependent if ‚àÉŒ±1, Œ±2 ‚ààR, with Œ±1 and Œ±2 not both zero, such that Œ±1v1+Œ±2v2 = 0.
Now let V = C1(a, b) be the set of once-diÔ¨Äerentiable functions over the interval
a < x < b. If u1, u2 ‚ààC1(a, b) are linearly dependent, ‚àÉŒ±1, Œ±2 ‚ààR such that
Œ±1u1(x) + Œ±2u2(x) = 0 ‚àÄx ‚àà(a, b). Notice that, by direct diÔ¨Äerentiation, this also
gives Œ±1u‚Ä≤
1(x) + Œ±2u‚Ä≤
2(x) = 0 or, in matrix form,
 u1(x)
u2(x)
u‚Ä≤
1(x)
u‚Ä≤
2(x)
  Œ±1
Œ±2

=
 0
0

.
These are homogeneous equations of the form
Ax = 0,
which only have nontrivial solutions if det(A) = 0, that is
W =

u1(x)
u2(x)
u‚Ä≤
1(x)
u‚Ä≤
2(x)
 = u1u‚Ä≤
2 ‚àíu‚Ä≤
1u2 = 0.
In other words, the Wronskian of two linearly dependent functions is identically
zero on (a, b). The contrapositive of this result is that if W Ã∏‚â°0 on (a, b), then u1
and u2 are linearly independent on (a, b).
Example
The functions u1(x) = x2 and u2(x) = x3 are linearly independent on the interval
(‚àí1, 1). To see this, note that, since u1(x) = x2, u2(x) = x3, u‚Ä≤
1(x) = 2x, and
u‚Ä≤
2(x) = 3x2, the Wronskian of these two functions is
W =

x2
x3
2x
3x2
 = 3x4 ‚àí2x4 = x4.
This quantity is not identically zero, and hence x2 and x3 are linearly independent
on (‚àí1, 1).
Example
The functions u1(x) = f(x) and u2(x) = kf(x), with k a constant, are linearly
dependent on any interval, since their Wronskian is
W =

f
kf
f ‚Ä≤
kf ‚Ä≤
 = 0.
If the functions u1 and u2 are solutions of (1.2), we can show by diÔ¨Äerentiating
W = u1u‚Ä≤
2 ‚àíu‚Ä≤
1u2 directly that
dW
dx + a1(x)W = 0.

10
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
This Ô¨Årst order diÔ¨Äerential equation has solution
W(x) = W(x0) exp

‚àí
 x
x0
a1(t)dt
	
,
(1.7)
which is known as Abel‚Äôs formula.
This gives us an easy way of Ô¨Ånding the
Wronskian of the solutions of any second order diÔ¨Äerential equation without having
to construct the solutions themselves.
Example
Consider the equation
y‚Ä≤‚Ä≤ + 1
xy‚Ä≤ +

1 ‚àí1
x2

y = 0.
Using Abel‚Äôs formula, this has Wronskian
W(x) = W(x0) exp

‚àí
 x
x0
dt
t
	
= x0W(x0)
x
= A
x
for some constant A. To Ô¨Ånd this constant, it is usually necessary to know more
about the solutions u1(x) and u2(x). We will describe a technique for doing this in
Section 1.3.
We end this section with a couple of useful theorems.
Theorem 1.1 If u1 and u2 are linearly independent solutions of the homoge-
neous, nonsingular ordinary diÔ¨Äerential equation (1.2), then the Wronskian is either
strictly positive or strictly negative.
Proof
From Abel‚Äôs formula, and since the exponential function does not change
sign, the Wronskian is identically positive, identically negative or identically zero.
We just need to exclude the possibility that W is ever zero. Suppose that W(x1) =
0.
The vectors
 u1(x1)
u‚Ä≤
1(x1)

and
 u2(x1)
u‚Ä≤
2(x1)

are then linearly dependent, and
hence u1(x1) = ku2(x1) and u‚Ä≤
1(x) = ku‚Ä≤
2(x) for some constant k. The function
u(x) = u1(x)‚àíku2(x) is also a solution of (1.2) by linearity, and satisÔ¨Åes the initial
conditions u(x1) = 0, u‚Ä≤(x1) = 0. Since (1.2) has a unique solution, the obvious
solution, u ‚â°0, is the only solution. This means that u1 ‚â°ku2. Hence u1 and u2
are linearly dependent ‚Äì a contradiction.
The nonsingularity of the diÔ¨Äerential equation is crucial here. If we consider the
equation x2y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + 2y = 0, which has u1(x) = x2 and u2(x) = x as its linearly
independent solutions, the Wronksian is ‚àíx2, which vanishes at x = 0. This is
because the coeÔ¨Écient of y‚Ä≤‚Ä≤ also vanishes at x = 0.
Theorem 1.2 (The Sturm separation theorem) If u1(x) and u2(x) are the
linearly independent solutions of a nonsingular, homogeneous equation, (1.2), then

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
11
the zeros of u1(x) and u2(x) occur alternately. In other words, successive zeros of
u1(x) are separated by successive zeros of u2(x) and vice versa.
Proof
Suppose that x1 and x2 are successive zeros of u2(x), so that W(xi) =
u1(xi)u‚Ä≤
2(xi) for i = 1 or 2. We also know that W(x) is of one sign on [x1, x2],
since u1(x) and u2(x) are linearly independent. This means that u1(xi) and u‚Ä≤
2(xi)
are nonzero. Now if u‚Ä≤
2(x1) is positive then u‚Ä≤
2(x2) is negative (or vice versa), since
u2(x2) is zero. Since the Wronskian cannot change sign between x1 and x2, u1(x)
must change sign, and hence u1 has a zero in [x1, x2], as we claimed.
As an example of this, consider the equation y‚Ä≤‚Ä≤+œâ2y = 0, which has solution y =
A sin œâx + B cos œâx. If we consider any two of the zeros of sin œâx, it is immediately
clear that cos œâx has a zero between them.
1.3
Solution by Power Series: The Method of Frobenius
Up to this point, we have considered ordinary diÔ¨Äerential equations for which we
know at least one solution of the homogeneous problem. From this we have seen that
we can easily construct the second independent solution and, in the inhomogeneous
case, the particular integral.
We now turn our attention to the more diÔ¨Écult
case, in which we cannot determine a solution of the homogeneous problem by
inspection. We must devise a method that is capable of solving variable coeÔ¨Écient
ordinary diÔ¨Äerential equations in general. As we noted at the start of the chapter,
we will restrict our attention to the case where the variable coeÔ¨Écients are simple
polynomials. This suggests that we can look for a solution of the form
y = xc
‚àû

n=0
anxn =
‚àû

n=0
anxn+c,
(1.8)
and hence
dy
dx =
‚àû

n=0
an(n + c)xn+c‚àí1,
(1.9)
d 2y
dx2 =
‚àû

n=0
an(n + c)(n + c ‚àí1)xn+c‚àí2,
(1.10)
where the constants c, a0, a1, . . . , are as yet undetermined. This is known as the
method of Frobenius. Later on, we will give some idea of why and when this
method can be used. For the moment, we will just try to make it work. We proceed
by example, with the simplest case Ô¨Årst.
1.3.1
The Roots of the Indicial Equation DiÔ¨Äer by an Integer
Consider the equation
x2 d 2y
dx2 + xdy
dx +

x2 ‚àí1
4

y = 0.
(1.11)

12
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
We substitute (1.8) to (1.10) into (1.11), which gives
x2
‚àû

n=0
an(n + c)(n + c ‚àí1)xn+c‚àí2 + x
‚àû

n=0
an(n + c)xn+c‚àí1
+

x2 ‚àí1
4
 ‚àû

n=0
anxn+c = 0.
We can rearrange this slightly to obtain
‚àû

n=0
an

(n + c)(n + c ‚àí1) + (n + c) ‚àí1
4
	
xn+c +
‚àû

n=0
anxn+c+2 = 0,
and hence, after simplifying the terms in the Ô¨Årst summation,
‚àû

n=0
an

(n + c)2 ‚àí1
4
	
xn+c +
‚àû

n=0
anxn+c+2 = 0.
We now extract the Ô¨Årst two terms from the Ô¨Årst summation to give
a0

c2 ‚àí1
4

xc + a1

(c + 1)2 ‚àí1
4
	
xc+1
+
‚àû

n=2
an

(n + c)2 ‚àí1
4
	
xn+c +
‚àû

n=0
anxn+c+2 = 0.
(1.12)
Notice that the Ô¨Årst term is the only one containing xc and similarly for the second
term in xc+1.
The two summations in (1.12) begin at the same power of x, namely x2+c. If we
let m = n + 2 in the last summation (notice that if n = 0 then m = 2, and n = ‚àû
implies that m = ‚àû), (1.12) becomes
a0

c2 ‚àí1
4

xc + a1

(c + 1)2 ‚àí1
4
	
xc+1
+
‚àû

n=2
an

(n + c)2 ‚àí1
4
	
xn+c +
‚àû

m=2
am‚àí2xm+c = 0.
Since the variables in the summations are merely dummy variables,
‚àû

m=2
am‚àí2xm+c =
‚àû

n=2
an‚àí2xn+c,
and hence
a0

c2 ‚àí1
4

xc + a1

(c + 1)2 ‚àí1
4
	
xc+1
+
‚àû

n=2
an

(n + c)2 ‚àí1
4
	
xn+c +
‚àû

n=2
an‚àí2xn+c = 0.

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
13
Since the last two summations involve identical powers of x, we can combine them
to obtain
a0

c2 ‚àí1
4

xc + a1

(c + 1)2 ‚àí1
4
	
xc+1
+
‚àû

n=2

an

(n + c)2 ‚àí1
4
	
+ an‚àí2

xn+c = 0.
(1.13)
Although the operations above are straightforward, we need to take some care to
avoid simple slips.
Since (1.13) must hold for all values of x, the coeÔ¨Écient of each power of x must
be zero. The coeÔ¨Écient of xc is therefore
a0

c2 ‚àí1
4

= 0.
Up to this point, most Frobenius analysis is very similar. It is here that the diÔ¨Äerent
structures come into play. If we were to use the solution a0 = 0, the series (1.8)
would have a1xc+1 as its Ô¨Årst term. This is just equivalent to increasing c by 1. We
therefore assume that a0 Ã∏= 0, which means that c must satisfy c2 ‚àí1
4 = 0. This is
called the indicial equation, and implies that c = ¬± 1
2. Now, progressing to the
next term, proportional to xc+1, we Ô¨Ånd that
a1

(c + 1)2 ‚àí1
4
	
= 0.
Choosing c = 1
2 gives a1 = 0, and, if we were to do this, we would Ô¨Ånd that we had
constructed a solution with one arbitrary constant. However, if we choose c = ‚àí1
2
the indicial equation is satisÔ¨Åed for arbitrary values of a1, and a1 will act as the
second arbitrary constant for the solution. In order to generate this more general
solution, we therefore let c = ‚àí1
2.
We now progress to the individual terms in the summation. The general term
yields
an

n ‚àí1
2
2
‚àí1
4

+ an‚àí2 = 0
for n = 2, 3, . . . .
This is called a recurrence relation. We solve it by observation as follows. We
start by rearranging to give
an = ‚àí
an‚àí2
n(n ‚àí1).
(1.14)
By putting n = 2 in (1.14) we obtain
a2 = ‚àía0
2 ¬∑ 1.
For n = 3,
a3 = ‚àía1
3 ¬∑ 2.

14
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
For n = 4,
a4 = ‚àía2
4 ¬∑ 3,
and substituting for a2 in terms of a0 gives
a4 = ‚àí1
4 ¬∑ 3

‚àía0
2 ¬∑ 1

=
a0
4 ¬∑ 3 ¬∑ 2 ¬∑ 1 = a0
4! .
Similarly for n = 5, using the expression for a3 in terms of a1,
a5 = ‚àía3
5 ¬∑ 4 = ‚àí1
5 ¬∑ 4

‚àía1
3 ¬∑ 2

= a1
5! .
A pattern is emerging here and we propose that
a2n = (‚àí1)n a0
(2n)!,
a2n+1 = (‚àí1)n
a1
(2n + 1)!.
(1.15)
This can be proved in a straightforward manner by induction, although we will not
dwell upon the details here.‚Ä†
We can now deduce the full solution. Starting from (1.8), we substitute c = ‚àí1
2,
and write out the Ô¨Årst few terms in the summation
y = x‚àí1/2(a0 + a1x + a2x2 + ¬∑ ¬∑ ¬∑ ).
Now, using the forms of the even and odd coeÔ¨Écients given in (1.15),
y = x‚àí1/2

a0 + a1x ‚àía0x2
2!
‚àía1x3
3!
+ a0x4
4!
+ a1x5
5!
+ ¬∑ ¬∑ ¬∑

.
This series splits naturally into two proportional to a0 and a1, namely
y = x‚àí1/2a0

1 ‚àíx2
2! + x4
4! ‚àí¬∑ ¬∑ ¬∑

+ x‚àí1/2a1

x ‚àíx3
3! + x5
5! ‚àí¬∑ ¬∑ ¬∑

.
The solution is therefore
y(x) = a0
cos x
x1/2 + a1
sin x
x1/2 ,
since we can recognize the Taylor series expansions for sine and cosine.
This particular diÔ¨Äerential equation is an example of the use of the method of
Frobenius, formalized by
Frobenius General Rule I
If the indicial equation has two distinct roots,
c = Œ±, Œ≤ (Œ± < Œ≤), whose diÔ¨Äerence is an in-
teger, and one of the coeÔ¨Écients of xk becomes
indeterminate on putting c = Œ±, both solutions
can be generated by putting c = Œ± in the recur-
rence relation.
‚Ä† In the usual way, we must show that (1.15) is true for n = 0 and that, when the value of a2n+1
is substituted into the recurrence relation, we obtain a2(n+1)+1, as given by substituting n + 1
for n in (1.15).

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
15
In the above example the indicial equation was c2 ‚àí1
4 = 0, which has solutions c =
¬± 1
2, whose diÔ¨Äerence is an integer. The coeÔ¨Écient of xc+1 was a1

(c + 1)2 ‚àí1
4

=
0. When we choose the lower of the two values (c = ‚àí1
2) this expression does not
give us any information about the constant a1, in other words a1 is indeterminate.
1.3.2
The Roots of the Indicial Equation DiÔ¨Äer by a Noninteger
Quantity
We now consider the diÔ¨Äerential equation
2x(1 ‚àíx)d 2y
dx2 + (1 ‚àíx)dy
dx + 3y = 0.
(1.16)
As before, let‚Äôs assume that the solution can be written as the power series (1.8).
As in the previous example, this can be diÔ¨Äerentiated and substituted into the
equation to yield
2x(1 ‚àíx)
‚àû

n=0
an(n + c)(n + c ‚àí1)xn+c‚àí2 + (1 ‚àíx)
‚àû

n=0
an(n + c)xn+c‚àí1
+3
‚àû

n=0
anxn+c = 0.
The various terms can be multiplied out, which gives us
‚àû

n=0
an(n + c)(n + c ‚àí1)2xn+c‚àí1 ‚àí
‚àû

n=0
an(n + c)(n + c ‚àí1)2xn+c
+
‚àû

n=0
an(n + c)xn+c‚àí1 ‚àí
‚àû

n=0
an(n + c)xn+c + 3
‚àû

n=0
anxn+c = 0.
Collecting similar terms gives
‚àû

n=0
an{2(n + c)(n + c ‚àí1) + (n + c)}xn+c‚àí1
+
‚àû

n=0
an{3 ‚àí2(n + c)(n + c ‚àí1) ‚àí(n + c)}xn+c = 0,
and hence
‚àû

n=0
an(n + c)(2n + 2c ‚àí1)xn+c‚àí1 +
‚àû

n=0
an{3 ‚àí(n + c)(2n + 2c ‚àí1)}xn+c = 0.
We now extract the Ô¨Årst term from the left hand summation so that both summa-
tions start with a term proportional to xc. This gives
a0c(2c ‚àí1)xc‚àí1 +
‚àû

n=1
an(n + c)(2n + 2c ‚àí1)xn+c‚àí1

16
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
+
‚àû

n=0
an{3 ‚àí(n + c)(2n + 2c ‚àí1)}xn+c = 0.
We now let m = n + 1 in the second summation, which then becomes
‚àû

m=1
am‚àí1{3 ‚àí(m ‚àí1 + c)(2(m ‚àí1) + 2c ‚àí1)}xm+c‚àí1.
We again note that m is merely a dummy variable which for ease we rewrite as n,
which gives
a0c(2c ‚àí1)xc‚àí1 +
‚àû

n=1
an(n + c)(2n + 2c ‚àí1)xn+c‚àí1
+
‚àû

n=1
an‚àí1 {3 ‚àí(n ‚àí1 + c)(2n + 2c ‚àí3)} xn+c‚àí1 = 0.
Finally, we can combine the two summations to give
a0c(2c ‚àí1)xc‚àí1
+
‚àû

n=1
{an(n + c)(2n + 2c ‚àí1) + an‚àí1{3 ‚àí(n ‚àí1 + c)(2n + 2c ‚àí3)}}xn+c‚àí1 = 0.
As in the previous example we can now consider the coeÔ¨Écients of successive
powers of x. We start with the coeÔ¨Écient of xc‚àí1, which gives the indicial equation,
a0c(2c ‚àí1) = 0. Since a0 Ã∏= 0, this implies that c = 0 or c = 1
2. Notice that these
roots do not diÔ¨Äer by an integer. The general term in the summation shows that
an = an‚àí1
(n + c ‚àí1)(2n + 2c ‚àí3) ‚àí3
(n + c)(2n + 2c ‚àí1)
	
for n = 1, 2, . . . .
(1.17)
We now need to solve this recurrence relation, considering each root of the indicial
equation separately.
Case I: c = 0
In this case, we can rewrite the recurrence relation (1.17) as
an = an‚àí1
(n ‚àí1)(2n ‚àí3) ‚àí3
n(2n ‚àí1)
	
= an‚àí1
 2n2 ‚àí5n
n(2n ‚àí1)
	
= an‚àí1
2n ‚àí5
2n ‚àí1

.
We recall that this holds for n ‚©æ1, so we start with n = 1, which yields
a1 = a0

‚àí3
1

= ‚àí3a0.
For n = 2
a2 = a1

‚àí1
3

= ‚àí3a0

‚àí1
3

= a0,

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
17
where we have used the expression for a1 in terms of a0. Now progressing to n = 3,
we have
a3 = a2
1
5

= a0
3
5 ¬∑ 3,
and for n = 4,
a4 = a3
3
7

= a0
3
7 ¬∑ 5.
Finally, for n = 5 we have
a5 = a4
5
9

= a0
3
9 ¬∑ 7.
In general,
an =
3a0
(2n ‚àí1)(2n ‚àí3),
which again can be proved by induction. We conclude that one solution of the
diÔ¨Äerential equation is
y = xc
‚àû

n=0
anxn = x0
‚àû

n=0
3a0
(2n ‚àí1)(2n ‚àí3)xn.
This can be tidied up by putting 3a0 = A, so that the solution is
y = A
‚àû

n=0
xn
(2n ‚àí1)(2n ‚àí3).
(1.18)
Note that there is no obvious way of writing this solution in terms of elementary
functions. In addition, a simple application of the ratio test shows that this power
series is only convergent for |x| ‚©Ω1, for reasons that we discuss below.
A simple MATLAB‚Ä† function that evaluates (1.18) is




function frob = frob(x)
n = 100:-1:0; a = 1./(2*n-1)./(2*n-3);
frob = polyval(a,x);
which sums the Ô¨Årst 100 terms of the series. The function polyval evaluates the
polynomial formed by the Ô¨Årst 100 terms in the sum (1.18) in an eÔ¨Écient manner.
Figure 1.2 can then be produced using the command ezplot(@frob,[-1,1]).
Although we could now use the method of reduction of order, since we have
constructed a solution, this would be very complicated. It is easier to consider the
second root of the indicial equation.
‚Ä† See Appendix 7 for a short introduction to MATLAB.

18
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
Fig. 1.2. The solution of (1.16) given by (1.18).
Case II: c = 1
2
In this case, we simplify the recurrence relation (1.17) to give
an = an‚àí1

n ‚àí1
2

(2n ‚àí2) ‚àí3

n + 1
2

2n

= an‚àí1
2n2 ‚àí3n ‚àí2
2n2 + n

= an‚àí1
(2n + 1)(n ‚àí2)
n(2n + 1)
	
= an‚àí1
n ‚àí2
n

.
We again recall that this relation holds for n ‚©æ1 and start with n = 1, which gives
a1 = a0(‚àí1). Substituting n = 2 gives a2 = 0 and, since all successive ai will be
written in terms of a2, ai = 0 for i = 2, 3, . . . . The second solution of the equation
is therefore y = Bx1/2(1‚àíx). We can now use this simple solution in the reduction
of order formula, (1.3), to determine an analytical formula for the Ô¨Årst solution,
(1.18). For example, for 0 ‚©Ωx ‚©Ω1, we Ô¨Ånd that (1.18) can be written as
y = ‚àí1
6A

3x ‚àí2 + 3x1/2 (1 ‚àíx) log

1 + x1/2
(1 ‚àíx)1/2

.
This expression has a logarithmic singularity in its derivative at x = 1, which
explains why the radius of convergence of the power series solution (1.18) is |x| ‚©Ω1.
This diÔ¨Äerential equation is an example of the second major case of the method
of Frobenius, formalized by

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
19
Frobenius General Rule II
If the indicial equation has two distinct roots,
c = Œ±, Œ≤ (Œ± < Œ≤), whose diÔ¨Äerence is not an
integer, the general solution of the equation is
found by successively substituting c = Œ± then c =
Œ≤ into the general recurrence relation.
1.3.3
The Roots of the Indicial Equation are Equal
Let‚Äôs try to determine the two solutions of the diÔ¨Äerential equation
xd 2y
dx2 + (1 + x)dy
dx + 2y = 0.
We substitute in the standard power series, (1.8), which gives
x
‚àû

n=0
an(n + c)(n + c ‚àí1)xn+c‚àí2 + (1 + x)
‚àû

n=0
an(n + c)xn+c‚àí1
+2
‚àû

n=0
anxn+c = 0.
This can be simpliÔ¨Åed to give
‚àû

n=0
an(n + c)2xn+c‚àí1 +
‚àû

n=0
an(n + c + 2)xn+c = 0.
We can extract the Ô¨Årst term from the left hand summation to give
a0c2xc‚àí1 +
‚àû

n=1
an(n + c)2xn+c‚àí1 +
‚àû

n=0
an(n + c + 2)xn+c = 0.
Now shifting the series using m = n + 1 (and subsequently changing dummy vari-
ables from m to n) we have
a0c2xc‚àí1 +
‚àû

n=1
{an(n + c)2 + an‚àí1(n + c + 1)}xn+c = 0,
(1.19)
where we have combined the two summations. The indicial equation is c2 = 0
which has a double root at c = 0. We know that there must be two solutions, but
it appears that there is only one available to us. For the moment let‚Äôs see how far
we can get by setting c = 0. The recurrence relation is then
an = ‚àían‚àí1
n + 1
n2
for n = 1, 2, . . . .
When n = 1 we Ô¨Ånd that
a1 = ‚àía0
2
12 ,

20
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
and with n = 2,
a2 = ‚àía1
3
22 = a0
3 ¬∑ 2
12 ¬∑ 22 .
Using n = 3 gives
a3 = ‚àía2
4
32 = ‚àía0
4 ¬∑ 3 ¬∑ 2
12 ¬∑ 22 ¬∑ 32 ,
and we conclude that
an = (‚àí1)n (n + 1)!
(n!)2
a0 = (‚àí1)n n + 1
n!
a0.
One solution is therefore
y = a0
‚àû

n=0
(‚àí1)n (n + 1)
n!
xn,
which can also be written as
y = a0

x
‚àû

n=1
(‚àí1)nxn‚àí1
(n ‚àí1)!
+
‚àû

n=0
(‚àí1)nxn
n!

= a0

‚àíx
‚àû

m=0
(‚àí1)mxm
m!
+ e‚àíx

= a0(1 ‚àíx)e‚àíx.
This solution is one that we could not have readily determined simply by inspection.
We could now use the method of reduction of order to Ô¨Ånd the second solution, but
we will proceed with the method of Frobenius so that we can see how it works in
this case.
Consider (1.19), which we write out more fully as
xd 2y
dx2 + (1 + x)dy
dx + 2y =
a0c2xc‚àí1 +
‚àû

n=1
{an(n + c)2 + an‚àí1(n + c + 1)}xn+c = 0.
The best we can do at this stage is to set an(n+c)2 +an‚àí1(n+c+1) = 0 for n ‚©æ1,
as this gets rid of most of the terms. This gives us an as a function of c for n ‚©æ1,
and leaves us with
xd 2y
dx2 + (1 + x)dy
dx + 2y = a0c2xc‚àí1.
(1.20)
Let‚Äôs now take a partial derivative with respect to c, where we regard y as a function
of both x and c, making use of
d
dx = ‚àÇ
‚àÇx,
‚àÇ
‚àÇc
‚àÇy
‚àÇx

= ‚àÇ
‚àÇx
‚àÇy
‚àÇc

.
This gives
x ‚àÇ2
‚àÇx2
‚àÇy
‚àÇc

+ (1 + x) ‚àÇ
‚àÇx
‚àÇy
‚àÇc

+ 2
‚àÇy
‚àÇc

= a0
‚àÇ
‚àÇc(c2xc‚àí1).

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
21
Notice that we have used the fact that a0 is independent of c. We need to be careful
when evaluating the right hand side of this expression. DiÔ¨Äerentiating using the
product rule we have
‚àÇ
‚àÇc(c2xc‚àí1) = c2 ‚àÇ
‚àÇc(xc‚àí1) + xc‚àí1 ‚àÇ
‚àÇc(c2).
We rewrite xc‚àí1 as xc x‚àí1 = ec log xx‚àí1, so that we have
‚àÇ
‚àÇc(c2xc‚àí1) = c2 ‚àÇ
‚àÇc(ec log xx‚àí1) + xc‚àí1 ‚àÇ
‚àÇc(c2).
DiÔ¨Äerentiating the exponential gives
‚àÇ
‚àÇc(c2xc‚àí1) = c2(log x ec log x)x‚àí1 + xc‚àí12c,
which we can tidy up to give
‚àÇ
‚àÇc(c2xc‚àí1) = c2xc‚àí1 log x + xc‚àí12c.
Substituting this form back into the diÔ¨Äerential equation gives
x ‚àÇ2
‚àÇx2
‚àÇy
‚àÇc

+ (1 + x) ‚àÇ
‚àÇx
‚àÇy
‚àÇc

+ 2‚àÇy
‚àÇc = a0{c2xc‚àí1 log x + xc‚àí12c}.
Now letting c ‚Üí0 gives
x ‚àÇ2
‚àÇx2
‚àÇy
‚àÇc

c=0
+ (1 + x) ‚àÇ
‚àÇx
‚àÇy
‚àÇc

c=0
+ 2 ‚àÇy
‚àÇc

c=0
= 0.
Notice that this procedure only works because (1.20) has a repeated root at c = 0.
We conclude that ‚àÇy
‚àÇc

c=0
is a second solution of our ordinary diÔ¨Äerential equation.
To construct this solution, we diÔ¨Äerentiate the power series (1.8) (carefully!) to
give
‚àÇy
‚àÇc = xc
‚àû

n=0
dan
dc xn +
‚àû

n=0
anxn xc log x,
using a similar technique as before to deal with the diÔ¨Äerentiation of xc with respect
to c. Note that, although a0 is not a function of c, the other coeÔ¨Écients are. Putting
c = 0 gives
‚àÇy
‚àÇc

c=0
=
‚àû

n=0
dan
dc

c=0
xn + log x
‚àû

n=0
an|c=0 xn.
We therefore need to determine dan
dc

c=0
. We begin with the recurrence relation,
which is
an = ‚àían‚àí1(n + c + 1)
(n + c)2
.

22
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
Starting with n = 1 we Ô¨Ånd that
a1 = ‚àía0(c + 2)
(c + 1)2
,
whilst for n = 2,
a2 = ‚àía1(c + 3)
(c + 2)2
,
and substituting for a1 in terms of a0 gives us
a2 = a0(c + 2)(c + 3)
(c + 1)2(c + 2)2 .
This process can be continued to give
an = (‚àí1)na0
(c + 2)(c + 3) . . . (c + n + 1)
(c + 1)2(c + 2)2 . . . (c + n)2 ,
which we can write as
an = (‚àí1)na0
n
j=1(c + j + 1)
n
j=1(c + j)
2 .
We now take the logarithm of this expression, recalling that the logarithm of a
product is the sum of the terms, which leads to
log(an)
=
log((‚àí1)na0) + log
Ô£´
Ô£≠
n

j=1
(c + j + 1)
Ô£∂
Ô£∏‚àí2 log
Ô£´
Ô£≠
n

j=1
(c + j)
Ô£∂
Ô£∏
=
log((‚àí1)na0) +
n

j=1
log(c + j + 1) ‚àí2
n

j=1
log(c + j).
Now diÔ¨Äerentiating with respect to c gives
1
an
dan
dc =
n

j=1
1
c + j + 1 ‚àí2
n

j=1
1
c + j ,
and setting c = 0 we have
 1
an
dan
dc

c=0
=
n

j=1
1
j + 1 ‚àí2
n

j=1
1
j .
Since we know an when c = 0, we can write
dan
dc

c=0
=
(‚àí1)na0
n
j=1(j + 1)
n
j=1 j
2
Ô£´
Ô£≠
n

j=1
1
j + 1 ‚àí2
n

j=1
1
j
Ô£∂
Ô£∏,
=
(‚àí1)na0
(n + 1)!
(n!)2
Ô£´
Ô£≠
n+1

j=1
1
j ‚àí1 ‚àí2
n

j=1
1
j
Ô£∂
Ô£∏.
In this expression, we have manipulated the products and written them as facto-
rials, changed the Ô¨Årst summation and removed the extra term that this incurs.

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
23
Simplifying, we obtain
dan
dc

c=0
= (‚àí1)na0
(n + 1)
n!
(œÜ(n + 1) ‚àí2œÜ(n) ‚àí1) ,
where we have introduced the notation
œÜ(n) ‚â°
n

m=1
1
m.
(1.21)
The second solution is therefore
y = a0
 ‚àû

n=0
(‚àí1)n (n + 1)
n!
{œÜ(n + 1) ‚àí2œÜ(n) ‚àí1}xn +
‚àû

n=0
(‚àí1)n (n + 1)
n!
xn log x

.
This methodology is formalized in
Frobenius General Rule III
If the indicial equation has a double root, c = Œ±,
one solution is obtained by putting c = Œ± into the
recurrence relation.
The second independent solution is (‚àÇy/‚àÇc)c=Œ±
where an = an(c) for the calculation.
There are several other cases that can occur in the method of Frobenius, which,
due to their complexity, we will not go into here. One method of dealing with these
is to notice that the method outlined in this chapter always produces a solution of
the form y = u1(x) =  ‚àû
n=0 anxn+c. This can be used in the reduction of order
formula, (1.3), to Ô¨Ånd the second linearly independent solution. Of course, it is
rather diÔ¨Écult to get all of u2(x) this way, but the Ô¨Årst few terms are usually easy
enough to compute by expanding for small x. Having got these, we can assume a
general series form for u2(x), and Ô¨Ånd the coeÔ¨Écients in the usual way.
Example
Let‚Äôs try to solve
x2y‚Ä≤‚Ä≤ + xy‚Ä≤ + (x2 ‚àí1)y = 0,
(1.22)
using the method of Frobenius. If we look for a solution in the usual form, y =
 ‚àû
n=0 anxn+c, we Ô¨Ånd that
a0(c2 ‚àí1)xc + a1

(1 + c)2 ‚àí1

xc+1 +
‚àû

k=2
!
ak

(k + c)2 ‚àí1

+ ak‚àí2
"
xk+c = 0.
The indicial equation has roots c = ¬±1, and, by choosing either of these, we Ô¨Ånd
that a1 = 0. If we now look for the general solution of
ak = ‚àí
ak‚àí2
(k + c)2 ‚àí1,
we Ô¨Ånd that
a2 = ‚àí
a0
(2 + c)2 ‚àí1 = ‚àí
a0
(1 + c)(3 + c),

24
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
a4 = ‚àí
a2
(4 + c)2 ‚àí1 =
a0
(1 + c)(3 + c)2(5 + c),
and so on. This gives us a solution of the form
y(x, c) = a0xc

1 ‚àí
x2
(1 + c)(3 + c) +
x4
(1 + c)(3 + c)2(5 + c) ‚àí¬∑ ¬∑ ¬∑
	
.
Now, by choosing c = 1, we obtain one of the linearly independent solutions of
(1.22),
u1(x) = y(x, 1) = x

1 ‚àíx2
2 ¬∑ 4 +
x4
2 ¬∑ 42 ¬∑ 6 ‚àí¬∑ ¬∑ ¬∑

.
However, if c = ‚àí1, the coeÔ¨Écients a2n for n = 1, 2, . . . are singular.
In order to Ô¨Ånd the structure of the second linearly independent solution, we use
the reduction of order formula, (1.3). Substituting for u1(x) gives
u2(x)
=
x

1 ‚àíx2
8 + ¬∑ ¬∑ ¬∑
  x
1
t2

1 ‚àít2
8 + ¬∑ ¬∑ ¬∑
2 exp

‚àí
 t 1
s ds

dt
=
x

1 ‚àíx2
8 + ¬∑ ¬∑ ¬∑
  x 1
t2

1 + t2
4 + ¬∑ ¬∑ ¬∑
 1
t dt
=
x

1 ‚àíx2
8 + ¬∑ ¬∑ ¬∑
 
‚àí1
2x2 + 1
4 log x + ¬∑ ¬∑ ¬∑

.
The second linearly independent solution of (1.22) therefore has the structure
u2(x) = 1
4u1(x) log x ‚àí1
2xv(x),
where v(x) = 1 + b2x2 + b4x4 + ¬∑ ¬∑ ¬∑ . If we assume a solution structure of this form
and substitute it into (1.22), it is straightforward to pick oÔ¨Äthe coeÔ¨Écients b2n.
Finally, note that we showed in Section 1.2.1 that the Wronskian of (1.22) is
W = A/x for some constant A.
Now, since we know that u1 = x + ¬∑ ¬∑ ¬∑ and
u2 = ‚àí1/2x+¬∑ ¬∑ ¬∑ , we must have W = x(1/2x2)+1/2x+¬∑ ¬∑ ¬∑ = 1/x+¬∑ ¬∑ ¬∑ , and hence
A = 1.
1.3.4
Singular Points of DiÔ¨Äerential Equations
In this section, we give some deÔ¨Ånitions and a statement of a theorem that tells
us when the method of Frobenius can be used, and for what values of x the inÔ¨Ånite
series will converge. We consider a second order, variable coeÔ¨Écient diÔ¨Äerential
equation of the form
P(x)d 2y
dx2 + Q(x)dy
dx + R(x)y = 0.
(1.23)
Before we proceed, we need to further reÔ¨Åne our ideas about singular points. If x0
is a singular point and (x‚àíx0)Q(x)/P(x) and (x‚àíx0)2R(x)/P(x) have convergent

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
25
Taylor series expansions about x0, then x = x0 is called a regular singular point;
otherwise, x0 is called an irregular singular point.
Example
Consider the equation
x(x ‚àí1)d 2y
dx2 + (1 + x)dy
dx + y = 0.
(1.24)
There are singular points where x2 ‚àíx = 0, at x = 0 and x = 1. Let‚Äôs start by
looking at the singular point x = 0. Consider the expression
xQ
P
= x(1 + x)
x2 ‚àíx
= (1 + x)
(x ‚àí1) = ‚àí(1 + x)(1 ‚àíx)‚àí1.
Upon expanding (1 ‚àíx)‚àí1 using the binomial expansion we have
xQ
P
= ‚àí(1 + x)(1 + x + x2 + ¬∑ ¬∑ ¬∑ + xn + ¬∑ ¬∑ ¬∑ ),
which can be multiplied out to give
xQ
P
= ‚àí1 ‚àí2x + ¬∑ ¬∑ ¬∑ .
This power series is convergent provided |x| < 1 (by considering the binomial
expansion used above). Now
x2R
P
=
x2
x2 ‚àíx =
x
x ‚àí1 = ‚àíx(1 ‚àíx)‚àí1.
Again, using the binomial expansion, which is convergent provided |x| < 1,
x2R
P
= ‚àíx(1 + x + x2 + ¬∑ ¬∑ ¬∑ ) = ‚àíx ‚àíx2 ‚àíx3 ‚àí¬∑ ¬∑ ¬∑ .
Since xQ/P and x2R/P have convergent Taylor series about x = 0, this is a regular
singular point.
Now consider the other singular point, x = 1. We note that
(x ‚àí1)Q
P = (x ‚àí1)(1 + x)
(x2 ‚àíx)
= (x ‚àí1)(1 + x)
x(x ‚àí1)
= 1 + x
x
.
At this point we need to recall that we want information near x = 1, so we rewrite
x as 1 ‚àí(1 ‚àíx), and hence
(x ‚àí1)Q
P =
2 ‚àí(1 ‚àíx)
{1 ‚àí(1 ‚àíx)}.
Expanding in powers of (1 ‚àíx) using the binomial expansion gives
(x ‚àí1)Q
P = 2

1 ‚àí1 ‚àíx
2

{1 + (1 ‚àíx) + (1 ‚àíx)2 + ¬∑ ¬∑ ¬∑ },

26
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
which is a power series in (x ‚àí1) that is convergent provided |x ‚àí1| < 1. We also
need to consider
(x ‚àí1)2 R
P = (x ‚àí1)2
x2 ‚àíx = (x ‚àí1)2
x(x ‚àí1) = x ‚àí1
x
=
x ‚àí1
{1 ‚àí(1 ‚àíx)}
= (x ‚àí1){1 ‚àí(1 ‚àíx)}‚àí1 = (x ‚àí1){1 + (x ‚àí1) + (x ‚àí1)2 + ¬∑ ¬∑ ¬∑ },
which is again convergent provided |x‚àí1| < 1. Therefore x = 1 is a regular singular
point.
Theorem 1.3 If x0 is an ordinary point of the ordinary diÔ¨Äerential equation (1.23),
then there exists a unique series solution in the neighbourhood of x0 which converges
for |x ‚àíx0| < œÅ, where œÅ is the smaller of the radii of convergence of the series for
Q(x)/P(x) and R(x)/P(x).
If x0 is a regular singular point, then there exists a unique series solution in the
neighbourhood of x0, which converges for |x ‚àíx0| < œÅ, where œÅ is the smaller of the
radii of convergence of the series (x ‚àíx0)Q(x)/P(x) and (x ‚àíx0)2R(x)/P(x).
Proof This can be found in Kreider, Kuller, Ostberg and Perkins (1966) and is due
to Fuchs. We give an outline of why the result should hold in Section 1.3.5. We
are more concerned here with using the result to tell us when a series solution will
converge.
Example
Consider the diÔ¨Äerential equation (1.24). We have already seen that x = 0 is a
regular singular point. The radii of convergence of xQ(x)/P(x) and x2R(x)/P(x)
are both unity, and hence the series solution  ‚àû
n=0 anxn+c exists, is unique, and
will converge for |x| < 1.
Example
Consider the diÔ¨Äerential equation
x4 d 2y
dx2 ‚àídy
dx + y = 0.
For this equation, x = 0 is a singular point but it is not regular. The series solution
 ‚àû
n=0 anxn+c is not guaranteed to exist, since xQ/P = ‚àí1/x3, which cannot be
expanded about x = 0.
1.3.5
An outline proof of Theorem 1.3
We will now give a sketch of why Theorem 1.3 holds. Consider the equation
P(x)y‚Ä≤‚Ä≤ + Q(x)y‚Ä≤ + R(x)y = 0.
When x = 0 is an ordinary point, assuming that
P(x) = P0 + xP1 + ¬∑ ¬∑ ¬∑ ,
Q(x) = Q0 + xQ1 + ¬∑ ¬∑ ¬∑ ,
R(x) = R0 + xR1 + ¬∑ ¬∑ ¬∑ ,

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
27
we can look for a solution using the method of Frobenius. When the terms are
ordered, we Ô¨Ånd that the Ô¨Årst two terms in the expansion are
P0a0c(c ‚àí1)xc‚àí2 + {P0a1c(c + 1) + P1a1c(c ‚àí1) + Q0a1c} xc‚àí1 + ¬∑ ¬∑ ¬∑ = 0.
The indicial equation, c(c ‚àí1) = 0, has two distinct roots that diÔ¨Äer by an integer
and, following Frobenius General Rule I, we can choose c = 0 and Ô¨Ånd a solution
of the form y = a0y0(x) + a1y1(x).
When x = 0 is a singular point, the simplest case to consider is when
P(x) = P1x + P2x2 + ¬∑ ¬∑ ¬∑ .
We can then ask what form Q(x) and R(x) must take to ensure that a series solution
exists. When x = 0 is an ordinary point, the indicial equation was formed from the
y‚Ä≤‚Ä≤ term in the equation alone. Let‚Äôs now try to include the y‚Ä≤ and y terms as well,
by making the assumption that
Q(x) = Q0 + Q1x + ¬∑ ¬∑ ¬∑ ,
R(x) = R‚àí1
x
+ R0 + ¬∑ ¬∑ ¬∑ .
Then, after substitution of the Frobenius series into the equation, the coeÔ¨Écient of
xc‚àí1 gives the indicial equation as
P1c(c ‚àí1) + Q0c + R‚àí1 = 0.
This is a quadratic equation in c, with the usual possibilities for its types of roots.
As x ‚Üí0
xQ(x)
P(x) ‚ÜíQ0
P1
,
x2R(x)
P(x)
‚ÜíR‚àí1
P1
,
so that both of these quantities have a Taylor series expansion about x = 0. This
makes it clear that, when P(x) = P1x + ¬∑ ¬∑ ¬∑ , these choices of expressions for the
behaviour of Q(x) and R(x) close to x = 0 are what is required to make it a regular
singular point. That the series converges, as claimed by the theorem, is most easily
shown using the theory of complex variables; this is done in Section A6.5.
1.3.6
The point at inÔ¨Ånity
Our discussion of singular points can be extended to include the point at inÔ¨Ånity
by deÔ¨Åning s = 1/x and considering the properties of the point s = 0. In particular,
after using
dy
dx = ‚àís2 dy
ds,
d 2y
dx2 = s2 d
ds

s2 dy
ds

in (1.23), we Ô¨Ånd that
ÀÜP(s)d 2y
ds2 + ÀÜQ(s)dy
ds + ÀÜR(s)y = 0,
where
ÀÜP(s) = s4P
1
s

,
ÀÜQ(s) = 2s3P
1
s

‚àís2Q
1
s

,
ÀÜR(s) = R
1
s

.

28
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
For example, Bessel‚Äôs equation, which we will study in Chapter 3, has P(x) = x2,
Q(x) = x and R(x) = x2 ‚àíŒΩ2, where ŒΩ is a constant, and hence has a regular
singular point at x = 0. In this case, ÀÜP(s) = s2, ÀÜQ(s) = s and ÀÜR(s) = 1/s2 ‚àíŒΩ2.
Since ÀÜP(0) = 0, Bessel‚Äôs equation also has a singular point at inÔ¨Ånity. In addition,
s2 ÀÜR(s)/ ÀÜP(s) =

1 ‚àís2ŒΩ

/s, which is singular at s = 0, and we conclude that
the point at inÔ¨Ånity is an irregular singular point. We will study the behaviour
of solutions of Bessel‚Äôs equation in the neighbourhood of the point at inÔ¨Ånity in
Section 12.2.7.
Exercises
1.1
Show that the functions u1 are solutions of the diÔ¨Äerential equations given
below. Use the reduction of order method to Ô¨Ånd the second independent
solution, u2.
(a) u1 = ex,
(x ‚àí1)d 2y
dx2 ‚àíxdy
dx + y = 0,
(b) u1 = x‚àí1 sin x,
xd 2y
dx2 + 2dy
dx + xy = 0.
1.2
Find the Wronskian of
(a) x, x2,
(b) ex, e‚àíx,
(c) x cos(log |x|), x sin(log |x|).
Which of these pairs of functions are linearly independent on the interval
[‚àí1, 1]?
1.3
Find the general solution of
(a) d 2y
dx2 ‚àí2dy
dx + y = x3/2ex,
(b) d 2y
dx2 + 4y = 2 sec 2x,
(c) d 2y
dx2 + 1
x
dy
dx +

1 ‚àí
1
4x2

y = x,
(d) d 2y
dx2 + y = f(x), subject to y(0) = y‚Ä≤(0) = 0.
1.4
If u1 and u2 are linearly independent solutions of
y‚Ä≤‚Ä≤ + p(x)y‚Ä≤ + q(x)y = 0
and y is any other solution, show that the Wronskian of {y, u1, u2},
W(x) =

y
u1
u2
y‚Ä≤
u‚Ä≤
1
u‚Ä≤
2
y‚Ä≤‚Ä≤
u‚Ä≤‚Ä≤
1
u‚Ä≤‚Ä≤
2

,
is zero everywhere. Hence Ô¨Ånd a second order diÔ¨Äerential equation which
has solutions y = x and y = log x.

EXERCISES
29
1.5
Find the Wronskian of the solutions of the diÔ¨Äerential equation
(1‚àíx2)y‚Ä≤‚Ä≤‚àí2xy‚Ä≤+2y = 0 to within a constant. Use the method of Frobenius
to determine this constant.
1.6
Find the two linearly independent solutions of each of the diÔ¨Äerential equa-
tions
(a) x2 d 2y
dx2 + x

x ‚àí1
2
 dy
dx + 1
2y = 0,
(b) x2 d 2y
dx2 + x (x + 1) dy
dx ‚àíy = 0,
using the method of Frobenius.
1.7
Show that the indicial equation for
x(1 ‚àíx)d 2y
dx2 + (1 ‚àí5x)dy
dx ‚àí4y = 0
has a double root. Obtain one series solution of the equation in the form
y = A
‚àû

n=1
n2xn‚àí1 = Au1(x).
What is the radius of convergence of this series? Obtain the second solution
of the equation in the form
u2(x) = u1(x) log x + u1(x) (‚àí4x + ¬∑ ¬∑ ¬∑ ) .
1.8
(a) The points x = ¬±1 are singular points of the diÔ¨Äerential equation
(x2 ‚àí1)2 d 2y
dx2 + (x + 1)dy
dx ‚àíy = 0.
Show that one of them is a regular singular point and that the other
is an irregular singular point.
(b) Find two linearly independent Frobenius solutions of
xd 2y
dx2 + 4dy
dx ‚àíxy = 0,
which are valid for x > 0.
1.9
Find the general solution of the diÔ¨Äerential equation
2xd 2y
dx2 + (1 + x)dy
dx ‚àíky = 0
(where k is a real constant) in power series form. For which values of k is
there a polynomial solution?
1.10
Let Œ±, Œ≤, Œ≥ denote real numbers and consider the hypergeometric equation
x(1 ‚àíx)d 2y
dx2 + {Œ≥ ‚àí(Œ± + Œ≤ + 1)x}dy
dx ‚àíŒ±Œ≤y = 0.
Show that x = 0 and x = 1 are regular singular points and obtain the roots
of the indicial equation at the point x = 0. Show that if Œ≥ is not an integer,
there are two linearly independent solutions of Frobenius form. Express a1

30
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
and a2 in terms of a0 for each of the solutions.
1.11
Show that each of the equations
(a) x3 d 2y
dx2 + x2 dy
dx + y = 0,
(b) x2 d 2y
dx2 + dy
dx ‚àí2y = 0,
has an irregular singular point at x = 0. Show that equation (a) has no
solution of Frobenius type but that equation (b) does. Obtain this solution
and hence Ô¨Ånd the general solution of equation (b).
1.12
Show that x = 0 is a regular singular point of the diÔ¨Äerential equation
2x2 d 2y
dx2 + x(1 ‚àíx)dy
dx ‚àíy = 0.
Find two linearly independent Frobenius solutions and show that one of
these solutions can be expressed in terms of elementary functions. Verify
directly that this function satisÔ¨Åes the diÔ¨Äerential equation.
1.13
Find the two linearly independent solutions of the ordinary diÔ¨Äerential
equation
x(x ‚àí1)y‚Ä≤‚Ä≤ + 3xy‚Ä≤ + y = 0
in the form of a power series. Hint: It is straightforward to Ô¨Ånd one solution,
but you will need to use the reduction of order formula to determine the
structure of the second solution.
1.14
‚àóShow that, if f(x) and g(x) are nontrivial solutions of the diÔ¨Äerential
equations u‚Ä≤‚Ä≤ + p(x)u = 0 and v‚Ä≤‚Ä≤ + q(x)v = 0, and p(x) ‚©æq(x), f(x)
vanishes at least once between any two zeros of g(x) unless p ‚â°q and
f = ¬µg, ¬µ ‚ààR (this is known as the Sturm comparison theorem).
1.15
‚àóShow that, if q(x) ‚©Ω0, no nontrivial solution of u‚Ä≤‚Ä≤ + q(x)u = 0 can have
more than one zero.

CHAPTER TWO
Legendre Functions
Legendre‚Äôs equation occurs in many areas of applied mathematics, physics and
chemistry in physical situations with a spherical geometry. We are now in a position
to examine Legendre‚Äôs equation in some detail using the ideas that we developed
in the previous chapter. The Legendre functions are the solutions of Legendre‚Äôs
equation, a second order linear diÔ¨Äerential equation with variable coeÔ¨Écients. This
equation was introduced by Legendre in the late 18th century, and takes the form
(1 ‚àíx2)d 2y
dx2 ‚àí2xdy
dx + n(n + 1)y = 0,
(2.1)
where n is a parameter, called the order of the equation. The equation is usually
deÔ¨Åned for ‚àí1 < x < 1 for reasons that will become clear in Section 2.6. We can see
immediately that x = 0 is an ordinary point of the equation, and, by Theorem 1.3,
a series solution will be convergent for |x| < 1.
2.1
DeÔ¨Ånition of the Legendre Polynomials, Pn(x)
We will use the method of Frobenius, and seek a power series solution of (2.1) in
the form
y =
‚àû

i=0
aixi+c.
Substitution of this series into (2.1) leads to
(1 ‚àíx2)
‚àû

i=0
ai(i + c)(i + c ‚àí1)xi+c‚àí2 ‚àí2x
‚àû

i=0
ai(i + c)xi+c‚àí1
+n(n + 1)
‚àû

i=0
aixi+c = 0.
Tidying this up gives
‚àû

i=0
ai(i + c)(i + c ‚àí1)xi+c‚àí2 +
‚àû

i=0
ai{n(n + 1) ‚àí(i + c)(i + c + 1)}xi+c = 0,
and collecting like powers of x we get
a0c(c ‚àí1)xc‚àí2 + a1c(c + 1)xc‚àí1 +
‚àû

i=2
ai(i + c)(i + c ‚àí1)xi+c‚àí2

32
LEGENDRE FUNCTIONS
+
‚àû

i=0
ai{n(n + 1) ‚àí(i + c)(i + c + 1)}xi+c = 0.
Rearranging the series gives us
a0c(c ‚àí1)xc‚àí2 + a1c(c + 1)xc‚àí1
+
‚àû

i=2
{ai(i + c)(i + c ‚àí1) + ai‚àí2(n(n + 1) ‚àí(i + c ‚àí2)(i + c ‚àí1))}xi+c‚àí2 = 0.
The indicial equation is therefore
c(c ‚àí1) = 0,
which gives c = 0 or 1. Following Frobenius General Rule I, we choose c = 0, so
that a1 is arbitrary and acts as a second constant. In general we have,
ai = (i ‚àí1)(i ‚àí2) ‚àín(n + 1)
i(i ‚àí1)
ai‚àí2 for i = 2, 3, . . . .
For i = 2,
a2 = ‚àín(n + 1)
2
a0.
For i = 3,
a3 = {2 ¬∑ 1 ‚àín(n + 1)}
3 ¬∑ 2
a1.
For i = 4,
a4 = {3 ¬∑ 2 ‚àín(n + 1)}
4 ¬∑ 3
a2 = ‚àí{3 ¬∑ 2 ‚àín(n + 1)}n(n + 1)
4!
a0.
For i = 5,
a5 = {4 ¬∑ 3 ‚àín(n + 1)}
5 ¬∑ 4
a3 = {4 ¬∑ 3 ‚àín(n + 1)}{2 ¬∑ 1 ‚àín(n + 1)}
5!
a1.
The solution of Legendre‚Äôs equation is therefore
y = a0

1 ‚àín(n + 1)
2!
x2 ‚àí{3 ¬∑ 2 ‚àín(n + 1)}n(n + 1)
4!
x4 + ¬∑ ¬∑ ¬∑

+a1

x + {2 ¬∑ 1 ‚àín(n + 1)}
3!
x3 + {4 ¬∑ 3 ‚àín(n + 1)}(2 ¬∑ 1 ‚àín(n + 1))
5!
x5 + ¬∑ ¬∑ ¬∑

,
(2.2)
where a0 and a1 are arbitrary constants. If n is not a positive integer, we have two
inÔ¨Ånite series solutions, convergent for |x| < 1. If n is a positive integer, one of the
inÔ¨Ånite series terminates to give a simple polynomial solution.
If we write the solution (2.2) as y = a0un(x) + a1vn(x), when n is an integer,

2.1 DEFINITION OF THE LEGENDRE POLYNOMIALS, Pn(x)
33
then u0(x), v1(x), u2(x) and v3(x) are the Ô¨Årst four polynomial solutions. It is
convenient to write the solution in the form
y
=
APn(x)
+
BQn(x),
‚Üë
‚Üë
Polynomial
InÔ¨Ånite series,
of degree n
converges for |x| < 1
where we have deÔ¨Åned Pn(x) = un(x)/un(1) for n even and Pn(x) = vn(x)/vn(1)
for n odd. The polynomials Pn(x) are called the Legendre polynomials and can
be written as
Pn(x) =
m

r=0
(‚àí1)r
(2n ‚àí2r)!xn‚àí2r
2nr!(n ‚àír)!(n ‚àí2r)!,
where m is the integer part of n/2. Note that by deÔ¨Ånition Pn(1) = 1. The Ô¨Årst
Ô¨Åve of these are
P0(x) = 1,
P1(x) = x,
P2(x) = 1
2(3x2 ‚àí1),
P3(x) = 1
2(5x3 ‚àí3x),
P4(x) = 35
8 x4 ‚àí15
4 x2 + 3
8.
Graphs of these Legendre polynomials are shown in Figure 2.1, which was generated
using the MATLAB script
'
&
$
%
x = linspace(-1,1,500); pout = [];
for k = 1:4
p = legendre(k,x); p=p(1,:);
pout = [pout; p];
end
plot(x,pout(1,:),x,pout(2,:),‚Äô--‚Äô,...
x,pout(3,:),‚Äô-.‚Äô,x,pout(4,:),‚Äô:‚Äô)
legend(‚ÄôP_1(x)‚Äô,‚ÄôP_2(x)‚Äô,‚ÄôP_3(x)‚Äô,‚ÄôP_4(x)‚Äô,4),xlabel(‚Äôx‚Äô)
Note that the MATLAB function legendre(n,x) generates both the Legendre
polynomial of order n and the associated Legendre functions of orders 1 to n,
which we will meet later, so we have to pick oÔ¨Äthe Legendre polynomial as the
Ô¨Årst row of the matrix p.
Simple expressions for the Qn(x) are available for n = 0, 1, 2, 3 using the reduc-
tion of order formula, (1.3). In particular
Q0(x) = 1
2 log
1 + x
1 ‚àíx

,
Q1(x) = 1
2x log
1 + x
1 ‚àíx

‚àí1,
Q2(x) = 1
4(3x2 ‚àí1) log
1 + x
1 ‚àíx

‚àí3
2x,
Q3(x) = 1
4(5x3 ‚àí3x) log
1 + x
1 ‚àíx

‚àí5
2x2 + 2
3.
These functions are singular at x = ¬±1. Notice that part of the inÔ¨Ånite series has

34
LEGENDRE FUNCTIONS
Fig. 2.1. The Legendre polynomials P1(x), P2(x), P3(x) and P4(x).
been summed to give us the logarithmic terms. Graphs of these functions are shown
in Figure 2.2.
Example
Let‚Äôs try to Ô¨Ånd the general solution of
(1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + 2y =
1
1 ‚àíx2 ,
for ‚àí1 < x < 1. This is just an inhomogeneous version of Legendre‚Äôs equation of
order one. The complementary function is
yh = AP1(x) + BQ1(x).
The variation of parameters formula, (1.6), then shows that the particular integral
is
yp =
 x
P1(s)Q1(x) ‚àíP1(x)Q1(s)
(1 ‚àís2){P1(s)Q‚Ä≤
1(s) ‚àíP ‚Ä≤
1(s)Q1(s)}ds.
We can considerably simplify this rather complicated looking result. Firstly, Abel‚Äôs
formula, (1.7), shows that the Wronskian is
W = P1(s)Q‚Ä≤
1(s) ‚àíP ‚Ä≤
1(s)Q1(s) =
W0
1 ‚àís2 .

2.2 THE GENERATING FUNCTION FOR Pn(x)
35
Fig. 2.2. The Ô¨Årst four Legendre functions, Q0(x), Q1(x), Q2(x) and Q3(x).
We can determine the constant W0 by considering the behaviour of W as s ‚Üí0.
Since
P1(s) = s,
Q1(s) = 1
2s log
1 + s
1 ‚àís

‚àí1,
W = 1 + s2 + ¬∑ ¬∑ ¬∑ for s ‚â™1. From the binomial theorem, 1/(1 ‚àís2) = 1 + s2 + ¬∑ ¬∑ ¬∑
for s ‚â™1. We conclude that W0 = 1. This means that
yp = Q1(x)
 x
P1(s) ds ‚àíP1(x)
 x
Q1(s) ds
= 1
2x2Q1(x) ‚àíx
1
4(x2 ‚àí1) log
1 + x
1 ‚àíx

‚àí1
2x
	
.
The general solution is this particular integral plus the complementary function
(yp(x) + yh(x)).
2.2
The Generating Function for Pn(x)
In order to make a more systematic study of the Legendre polynomials, it is helpful
to introduce a generating function, G(x, t). This function is deÔ¨Åned in such a
way that the coeÔ¨Écients of the Taylor series of G(x, t) around t = 0 are Pn(x).
We start with the assertion that
G(x, t) = (1 ‚àí2xt + t2)‚àí1/2 =
‚àû

n=0
Pn(x)tn.
(2.3)

36
LEGENDRE FUNCTIONS
Just to motivate this formula, let‚Äôs consider the Ô¨Årst terms in the Taylor series
expansion about t = 0. Using the binomial expansion formula, which is convergent
for
t2 ‚àí2xt
 < 1 (for |x| < 1 we can ensure that this holds by making |t| small
enough),
{1 + (‚àí2xt + t2)}‚àí1/2 =
1 +

‚àí1
2

(‚àí2xt + t2) +

‚àí1
2
 
‚àí3
2

2!
(‚àí2xt + t2)2 + ¬∑ ¬∑ ¬∑
= 1 + xt + 1
2(3x2 ‚àí1)t2 + ¬∑ ¬∑ ¬∑ = P0(x) + P1(x)t + P2(x)t2 + ¬∑ ¬∑ ¬∑ ,
as expected. With a little extra work we can derive (2.3).
We start by working with
(1 ‚àí2xt + t2)‚àí1/2 =
‚àû

n=0
Zn(x)tn,
(2.4)
where, using the binomial expansion, we know that Zn(x) is a polynomial of degree
n. We Ô¨Årst diÔ¨Äerentiate with respect to x, which gives us
t(1 ‚àí2xt + t2)‚àí3/2 =
‚àû

n=0
Z‚Ä≤
n(x)tn,
and again gives
3t2(1 ‚àí2xt + t2)‚àí5/2 =
‚àû

n=0
Z‚Ä≤‚Ä≤
n(x)tn.
Now we diÔ¨Äerentiate (2.4) with respect to t, which leads to
(x ‚àít)(1 ‚àí2xt + t2)‚àí3/2 =
‚àû

n=0
Zn(x)ntn‚àí1.
Multiplying this last result by t2 and diÔ¨Äerentiating with respect to t gives
‚àû

n=0
Zn(x)n(n + 1)tn = ‚àÇ
‚àÇt{t2(x ‚àít)(1 ‚àí2xt + t2)‚àí3/2},
= t2{(x ‚àít)(1 ‚àí2xt + t2)‚àí5/23(x ‚àít) + (1 ‚àí2xt + t2)‚àí3/2 ‚àí1}
+(x ‚àít)(1 ‚àí2xt + t2)‚àí3/22t,
which we can simplify to get
(1 ‚àí2xt + t2)‚àí3/2{3t2(x ‚àít)2(1 ‚àí2xt + t2)‚àí1 ‚àí1 + 2t(x ‚àít)}
=
‚àû

n=0
Zn(x)n(n + 1)tn.

2.2 THE GENERATING FUNCTION FOR Pn(x)
37
Combining all of these results gives
(1 ‚àíx2)
‚àû

n=0
Z‚Ä≤‚Ä≤
n(x)tn ‚àí2x
‚àû

n=0
Z‚Ä≤
n(x)tn +
‚àû

n=0
n(n + 1)Zn(x)tn
= (1 ‚àíx2)3t2(1 ‚àí2xt + t2)‚àí5/2 ‚àí2xt(1 ‚àí2xt + t2)‚àí3/2
+(1 ‚àí2xt + t2)‚àí3/2{3t2(x ‚àít)2(1 ‚àí2xt + t2)‚àí1 ‚àí1 + 2t(x ‚àít)} = 0,
for any t. Therefore
(1 ‚àíx2)Z‚Ä≤‚Ä≤
n(x) ‚àí2xZ‚Ä≤
n(x) + n(n + 1)Zn(x) = 0,
which is just Legendre‚Äôs equation. This means that Zn(x) = Œ±Pn(x) + Œ≤Qn(x)
where Œ±, Œ≤ are constants. As Zn(x) is a polynomial of degree n, Œ≤ must be zero.
Finally, we need to show that Zn(1) = 1. This is done by putting x = 1 in the
generating function relationship, (2.3), to obtain
(1 ‚àí2t + t2)‚àí1/2 =
‚àû

n=0
Zn(1)tn.
Since
(1 ‚àí2t + t2)‚àí1/2 =

(1 ‚àít)2‚àí1/2 = (1 ‚àít)‚àí1 =
‚àû

n=0
tn,
at least for |t| < 1, we have Zn(1) = 1. Since we know that Pn(1) = 1, we conclude
that Zn(x) ‚â°Pn(x), as required.
The generating function, G(x, t) = (1 ‚àí2xt + t2)‚àí1/2, can be used to prove a
number of interesting properties of Legendre polynomials, as well as some recurrence
formulae. We will give a few examples of its application.
Special Values
The generating function is useful for determining the values of the Legendre poly-
nomials for certain special values of x. For example, substituting x = ‚àí1 in (2.3)
gives
(1 + 2t + t2)‚àí1/2 =
‚àû

n=0
Pn(‚àí1)tn.
By the binomial expansion we have that
(1 + 2t + t2)‚àí1/2 = {(1 + t)2}‚àí1/2 = (1 + t)‚àí1
= 1 ‚àít + t2 ‚àí¬∑ ¬∑ ¬∑ + (‚àít)n + ¬∑ ¬∑ ¬∑ =
‚àû

n=0
(‚àí1)ntn.
We conclude that
‚àû

n=0
(‚àí1)ntn =
‚àû

n=0
Pn(‚àí1)tn,
and therefore Pn(‚àí1) = (‚àí1)n for n = 1, 2, . . . .

38
LEGENDRE FUNCTIONS
2.3
DiÔ¨Äerential and Recurrence Relations Between Legendre
Polynomials
The generating function can also be used to derive recurrence relations between the
various Pn(x). Starting with (2.3), we diÔ¨Äerentiate with respect to t, and Ô¨Ånd that
(x ‚àít)(1 ‚àí2xt + t2)‚àí3/2 =
‚àû

n=0
Pn(x)ntn‚àí1.
We now multiply through by (1 ‚àí2xt + t2) to obtain
(x ‚àít)(1 ‚àí2xt + t2)‚àí1/2 = (1 ‚àí2xt + t2)
‚àû

n=0
Pn(x)ntn‚àí1,
which leads to
x
‚àû

n=0
Pn(x)tn ‚àí
‚àû

n=0
Pn(x)tn+1
= n
‚àû

n=0
Pn(x)tn‚àí1 ‚àí2xn
‚àû

n=0
Pn(x)tn + n
‚àû

n=0
Pn(x)tn+1.
Equating coeÔ¨Écients of tn on both sides shows that
xPn(x) ‚àíPn‚àí1(x) = (n + 1)Pn+1(x) ‚àí2xnPn(x) + (n ‚àí1)Pn‚àí1(x),
and hence
(n + 1)Pn+1(x) ‚àí(2n + 1)xPn(x) + nPn‚àí1(x) = 0.
(2.5)
This is a recurrence relation between Pn+1(x), Pn(x) and Pn‚àí1(x), which can be
used to compute the polynomials Pn(x). Starting with P0(x) = 1 and P1(x) = x,
we substitute n = 1 into (2.5), which gives
2P2(x) ‚àí3x2 + 1 = 0,
and hence
P2(x) = 1
2(3x2 ‚àí1).
By iterating this procedure, we can generate the Legendre polynomials Pn(x) for
any n.
In a rather similar manner we can generate a recurrence relation that involves
the derivatives of the Legendre polynomials. Firstly, we diÔ¨Äerentiate the generating
function, (2.3), with respect to x to get
t(1 ‚àí2xt + t2)‚àí3/2 =
‚àû

n=0
tnP ‚Ä≤
n(x).
DiÔ¨Äerentiation of (2.3) with respect to t gives
(x ‚àít)(1 ‚àí2xt + t2)‚àí3/2 =
‚àû

n=0
ntn‚àí1Pn(x).

2.4 RODRIGUES‚Äô FORMULA
39
Combining these gives
‚àû

n=0
ntnPn(x) = (x ‚àít)
‚àû

n=0
tnP ‚Ä≤
n(x),
and by equating coeÔ¨Écients of tn we obtain the recurrence relation
nPn(x) = xP ‚Ä≤
n(x) ‚àíP ‚Ä≤
n‚àí1(x).
An Example From Electrostatics
In electrostatics, the potential due to a unit point charge at r = r0 is
V =
1
|r ‚àír0|.
If this unit charge lies on the z-axis, at x = y = 0, z = a, this becomes
V =
1
#
x2 + y2 + (z ‚àía)2 .
In terms of spherical polar coordinates, (r, Œ∏, œÜ),
x = r sin Œ∏ cos œÜ,
y = r sin Œ∏ sin œÜ,
z = r cos Œ∏.
This means that
x2 + y2 + (z ‚àía)2 = x2 + y2 + z2 ‚àí2az + a2 = r2 + a2 ‚àí2az,
and hence
V =
1
‚àö
r2 + a2 ‚àí2ar cos Œ∏
= 1
a

1 ‚àí2 cos Œ∏ r
a + r2
a2
‚àí1/2
.
As we would expect from the symmetry of the problem, there is no dependence
upon the azimuthal angle, œÜ. We can now use the generating function to write this
as a power series,
V = 1
a
‚àû

n=0
Pn (cos Œ∏)
r
a
n
.
2.4
Rodrigues‚Äô Formula
There are other methods of generating the Legendre polynomials, and the most
useful of these is Rodrigues‚Äô formula,
Pn(x) =
1
2nn!
d n
dxn {(x2 ‚àí1)n}.
For example, for n = 1,
P1(x) =
1
211!
d 1
dx1 {(x2 ‚àí1)} = x,

40
LEGENDRE FUNCTIONS
whilst for n = 2,
P2(x) =
1
222!
d 2
dx2 {(x2 ‚àí1)2} =
1
4 ¬∑ 2
d
dx{4x(x2 ‚àí1)} = 1
2(3x2 ‚àí1).
The general proof of this result is by induction on n, which we leave as an exercise.
Rodrigues‚Äô formula can also be used to develop an integral representation of the
Legendre polynomials. In order to show how this is done, it is convenient to switch
from the real variable x to the complex variable z = x + iy. We deÔ¨Åne the Ô¨Ånite
complex Legendre polynomials in the same way as for a real variable. In particular
Rodrigues‚Äô formula,
Pn(z) =
1
2nn!
d n
dzn

z2 ‚àí1
n ,
will be useful to us here. Recall from complex variable theory (see Appendix 6)
that, if f(z) is analytic and single-valued inside and on a simple closed curve C,
f (n)(z) = n!
2œÄi

C
f(Œæ)
(Œæ ‚àíz)n+1 dŒæ,
for n ‚©æ0, when z is an interior point of C. Now, using Rodrigues‚Äô formula, we have
Pn(z) =
1
2n+1œÄi

C
(Œæ2 ‚àí1)n
(Œæ ‚àíz)n+1 dŒæ,
(2.6)
which is known as Schl¬®aÔ¨Çi‚Äôs representation. The contour C must, of course,
enclose the point Œæ = z and be traversed in an anticlockwise sense. To simplify
matters, we now choose C to be a circle, centred on Œæ = z with radius |
‚àö
z2 ‚àí1|,
with z Ã∏= 1. Putting Œæ = z +
‚àö
z2 ‚àí1eiŒ∏ gives, after some simple manipulation,
Pn(z) = 1
2œÄ
 2œÄ
Œ∏=0

z +
#
z2 ‚àí1 cos Œ∏
n
dŒ∏.
This is known as Laplace‚Äôs representation. In fact it is also valid when z = 1,
since
Pn(1) = 1
2œÄ
 2œÄ
0
1dŒ∏ = 1.
Laplace‚Äôs representation is useful, amongst other things, for providing a bound on
the size of the Legendre polynomials of real argument. For z ‚àà[‚àí1, 1], we can write
z = cos œÜ and use Laplace‚Äôs representation to show that
|Pn(cos œÜ)| ‚©Ω1
2œÄ
 2œÄ
0
| cos œÜ + i sin œÜ cos Œ∏|n dŒ∏.
Now, since
| cos œÜ + i sin œÜ cos Œ∏| =
$
cos2 œÜ + sin2 œÜ cos2 Œ∏ ‚©Ω
$
cos2 œÜ + sin2 œÜ = 1,
we have |Pn(cos œÜ)| ‚©Ω1.

2.5 ORTHOGONALITY OF THE LEGENDRE POLYNOMIALS
41
2.5
Orthogonality of the Legendre Polynomials
Legendre polynomials have the very important property of orthogonality on
[‚àí1, 1], that is
 1
‚àí1
Pn(x)Pm(x)dx =
2
2n + 1Œ¥mn,
(2.7)
where the Kronecker delta is deÔ¨Åned by
Œ¥mn =
 1
for m = n,
0
for m Ã∏= n.
To show this, note that if f ‚ààC[‚àí1, 1], Rodrigues‚Äô formula shows that
 1
‚àí1
f(x)Pn(x)dx =
1
2nn!
 1
‚àí1
f(x) d n
dxn {(x2 ‚àí1)n}dx
=
1
2nn!

f(x) d n‚àí1
dxn‚àí1 {(x2 ‚àí1)n}
1
‚àí1
‚àí
 1
‚àí1
f ‚Ä≤(x) d n‚àí1
dxn‚àí1 {(x2 ‚àí1)n}dx

= ‚àí
1
2nn!
 1
‚àí1
f ‚Ä≤(x) d n‚àí1
dxn‚àí1 {(x2 ‚àí1)n}dx.
Repeating this integration by parts (n ‚àí1) more times gives
 1
‚àí1
f(x)Pn(x)dx = (‚àí1)n
2nn!
 1
‚àí1
(x2 ‚àí1)nf (n)(x)dx,
(2.8)
where f (n)(x) is the nth derivative of f(x). This result is interesting in its own
right, but for the time being consider the case f(x) = Pm(x) with m < n, so that
 1
‚àí1
Pm(x)Pn(x)dx = (‚àí1)n
2nn!
 1
‚àí1
(x2 ‚àí1)nP (n)
m (x)dx.
Since the nth derivative of an mth order polynomial is zero for m < n, we have
 1
‚àí1
Pm(x)Pn(x)dx = 0 for m < n.
By symmetry, this must also hold for m > n.
Let‚Äôs now consider the case n = m, for which
 1
‚àí1
Pn(x)Pn(x)dx = (‚àí1)n
2nn!
 1
‚àí1
(x2 ‚àí1)nP (n)
n
(x)dx
= (‚àí1)n
2nn!
 1
‚àí1
(x2 ‚àí1)n
1
2nn!
d 2n
dx2n {(x2 ‚àí1)n}dx.
Noting the fact that
(x2 ‚àí1)n = x2n + ¬∑ ¬∑ ¬∑ + (‚àí1)n,

42
LEGENDRE FUNCTIONS
and hence that
d 2n
dx2n (x2 ‚àí1)n = 2n(2n ‚àí1) . . . 3 ¬∑ 2 ¬∑ 1 = (2n)!,
we can see that
 1
‚àí1
Pn(x)Pn(x)dx = (‚àí1)n
2nn!
 1
‚àí1
(x2 ‚àí1)n
1
2nn!(2n)!dx
= (‚àí1)n(2n)!
22n(n!)2
 1
‚àí1
(x2 ‚àí1)ndx.
To evaluate the remaining integral, we use a reduction formula to show that
 1
‚àí1
(x2 ‚àí1)n dx = 22n+2n!(n + 1)!(‚àí1)n
(2n + 2)!
,
and hence
 1
‚àí1
P 2
n(x)dx =
2
2n + 1.
This completes the derivation of (2.7). There is an easier proof of the Ô¨Årst part
of this, using the idea of a self-adjoint linear operator, which we will discuss in
Chapter 4.
We have now shown that the Legendre polynomials are orthogonal on [‚àí1, 1].
It is also possible to show that these polynomials are complete in the function
space C[‚àí1, 1]. This means that a continuous function can be expanded as a linear
combination of the Legendre polynomials. The proof of the completeness property
is rather diÔ¨Écult, and will be omitted here.‚Ä†
What we can present here is the
procedure for obtaining the coeÔ¨Écients of such an expansion for a given function
f(x) belonging to C[‚àí1, 1]. To do this we write
f(x) = a0P0(x) + a1P1(x) + ¬∑ ¬∑ ¬∑ + anPn(x) + ¬∑ ¬∑ ¬∑ =
‚àû

n=0
anPn(x).
Multiplying by Pm(x) and integrating over [‚àí1, 1] (more precisely, forming the inner
product with Pm(x)) gives
 1
‚àí1
f(x)Pm(x)dx =
 1
‚àí1
Pm(x)
‚àû

n=0
anPn(x)dx.
‚Ä† Just to give a Ô¨Çavour of the completeness proof for the case of Legendre polynomials, we note
that, because of their polynomial form, we can deduce that any polynomial can be written
as a linear combination of Legendre polynomials. However, according to a fundamental result
due to Weierstrass (the Weierstrass polynomial approximation theorem) any function which is
continuous on some interval can be approximated as closely as we wish by a polynomial. The
completeness then follows from the application of this theorem. The treatment of completeness
of other solutions of Sturm‚ÄìLiouville problems may be more complicated, for example for Bessel
functions. A complete proof of this can be found in Kreider, Kuller, Ostberg and Perkins (1966).
We will return to this topic in Chapter 4.

2.5 ORTHOGONALITY OF THE LEGENDRE POLYNOMIALS
43
Interchanging the order of summation and integration leads to
 1
‚àí1
f(x)Pm(x)dx =
‚àû

n=0
an
 1
‚àí1
Pn(x)Pm(x)dx
	
=
‚àû

n=0
an
2
2m + 1Œ¥mn =
2am
2m + 1,
using the orthogonality property, (2.7). This means that
am = 2m + 1
2
 1
‚àí1
f(x)Pm(x)dx,
(2.9)
and we write the series as
f(x) =
‚àû

n=0
2n + 1
2
 1
‚àí1
f(x)Pn(x)dx
	
Pn(x).
This is called a Fourier‚ÄìLegendre series.
Let‚Äôs consider a couple of examples. Firstly, when f(x) = x2,
am = 2m + 1
2
 1
‚àí1
x2Pm(x)dx,
so that
a0 = 1
2
 1
‚àí1
x2 ¬∑ 1dx = 1
2
x3
3
1
‚àí1
= 1
2
2
3 = 1
3,
a1 = (2 ¬∑ 1) + 1
2
 1
‚àí1
x2 ¬∑ xdx = 3
2
x4
4
1
‚àí1
= 0,
a2 = (2 ¬∑ 2) + 1
2
 1
‚àí1
x2 1
2(3x2 ‚àí1)dx = 5
2
 3x5
2 ¬∑ 5 ‚àíx3
3 ¬∑ 2
1
‚àí1
= 2
3.
Also, (2.8) shows that
am = 0 for m = 3, 4, . . . ,
and therefore,
x2 = 1
3P0(x) + 2
3P2(x).
A Ô¨Ånite polynomial clearly has a Ô¨Ånite Fourier‚ÄìLegendre series.
Secondly, consider f(x) = ex. In this case
am = 2m + 1
2
 1
‚àí1
exPm(x) dx,
and hence
a0 = 1
2
 1
‚àí1
ex dx = 1
2

e ‚àíe‚àí1
,

44
LEGENDRE FUNCTIONS
a1 = 3
2
 1
‚àí1
x ex dx = 3 e‚àí1.
To proceed with this calculation it is necessary to Ô¨Ånd a recurrence relation between
the an. This is best done by using Rodrigues‚Äô formula, which gives
an = (2n + 1)
 an‚àí2
2n ‚àí3 ‚àían‚àí1

for n = 2, 3, . . . ,
(2.10)
from which the values of a4, a5, . . . are easily computed.
We will not examine the convergence of Fourier‚ÄìLegendre series here as the de-
tails are rather technical. Instead we content ourselves with a statement that the
Fourier‚ÄìLegendre series converges uniformly on any closed subinterval of (‚àí1, 1)
in which f is continuous and diÔ¨Äerentiable.
An extension of this result to the
space of piecewise continuous functions is that the series converges to the value
1
2

f(x+
0 ) + f(x‚àí
0 )

at each point x0 ‚àà(‚àí1, 1) where f has a right and left deriva-
tive. We will prove a related theorem in Chapter 5.
2.6
Physical Applications of the Legendre Polynomials
In this section we present some examples of Legendre polynomials as they arise
in mathematical models of heat conduction and Ô¨Çuid Ô¨Çow in spherical geometries.
In general, we will encounter the Legendre equation in situations where we have
to solve partial diÔ¨Äerential equations containing the Laplacian in spherical polar
coordinates.
2.6.1
Heat Conduction
Let‚Äôs derive the equation that governs the evolution of an initial distribution of
heat in a solid body with temperature T, density œÅ, speciÔ¨Åc heat capacity c and
thermal conductivity k. Recall that the speciÔ¨Åc heat capacity, c, is the amount
of heat required to raise the temperature of a unit mass of a substance by one
degree. The thermal conductivity, k, of a body appears in Fourier‚Äôs law, which
states that the heat Ô¨Çux per unit area, per unit time, Q = (Qx, Qy, Qz), is related
to the temperature gradient, ‚àáT, by the simple linear relationship Q = ‚àík‚àáT. If
we now consider a small element of our solid body at (x, y, z) with sides of length
Œ¥x, Œ¥y and Œ¥z, the temperature change in this element over a time interval Œ¥t is
determined by the diÔ¨Äerence between the amount of heat that Ô¨Çows into the element
and the amount of heat that Ô¨Çows out, which gives
œÅc {T (x, y, z, t + Œ¥t) ‚àíT (x, y, z, t)} Œ¥xŒ¥yŒ¥z
= {Qx (x, y, z, t) ‚àíQx (x + Œ¥x, y, z, t)} Œ¥tŒ¥yŒ¥z
+ {Qy (x, y, z, t) ‚àíQy (x, y + Œ¥y, z, t)} Œ¥tŒ¥xŒ¥z
(2.11)
+ {Qz (x, y, z, t) ‚àíQz (x, y, z + Œ¥z, t)} Œ¥tŒ¥xŒ¥y.

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
45
œÅ
k
c
K
kg m‚àí3
J m‚àí1 s‚àí1 K‚àí1
J kg‚àí1 K‚àí1
m2 s‚àí1
copper
8920
385
386
1.1 √ó 10‚àí4
water
1000
254
4186
6.1 √ó 10‚àí5
glass
2800
0.8
840
3.4 √ó 10‚àí7
Table 2.1. Some typical physical properties of copper, water (at room temperature
and pressure) and glass.
Note that a typical term on the right hand side of this, for example,
{Qx (x, y, z, t) ‚àíQx (x + Œ¥x, y, z, t)} Œ¥tŒ¥yŒ¥z,
is the amount of heat crossing the x-orientated faces of the element, each with area
Œ¥yŒ¥z, during the time interval (t, t + Œ¥t). Taking the limit Œ¥t, Œ¥x, Œ¥y, Œ¥z ‚Üí0, we
obtain
œÅc‚àÇT
‚àÇt = ‚àí
‚àÇQx
‚àÇx + ‚àÇQy
‚àÇy + ‚àÇQz
‚àÇz
	
= ‚àí‚àá¬∑ Q.
Substituting in Fourier‚Äôs law, Q = ‚àík‚àáT, gives the diÔ¨Äusion equation,
‚àÇT
‚àÇt = K‚àá2T,
(2.12)
where K = k/œÅc is called the thermal diÔ¨Äusivity. Table 2.1 contains the values
of relevant properties for three everyday materials.
When the temperature reaches a steady state (‚àÇT/‚àÇt = 0), this equation takes
the simple form
‚àá2T = 0,
(2.13)
which is known as Laplace‚Äôs equation. It must be solved in conjunction with
appropriate boundary conditions, which drive the temperature gradients in the
body.
Example
Let‚Äôs try to Ô¨Ånd the steady state temperature distribution in a solid, uniform sphere
of unit radius, when the surface temperature is held at f(Œ∏) = T0 sin4 Œ∏ in spherical
polar coordinates, (r, Œ∏, œÜ).
This temperature distribution will satisfy Laplace‚Äôs
equation, (2.13). Since the equation and boundary conditions do not depend upon
the azimuthal angle, œÜ, neither does the solution, and hence Laplace‚Äôs equation
takes the form
1
r2
‚àÇ
‚àÇr

r2 ‚àÇT
‚àÇr

+
1
r2 sin Œ∏
‚àÇ
‚àÇŒ∏

sin Œ∏‚àÇT
‚àÇŒ∏

= 0.
Let‚Äôs look for a separable solution, T(r, Œ∏) = R(r)Œò(Œ∏). This gives
Œò d
dr

r2 dR
dr

+
R
sin Œ∏
d
dŒ∏

sin Œ∏dŒò
dŒ∏

= 0,

46
LEGENDRE FUNCTIONS
and hence
1
R
d
dr

r2 dR
dr

= ‚àí
1
Œò sin Œ∏
d
dŒ∏

sin Œ∏dŒò
dŒ∏

.
Since the left hand side is a function of r only and the right hand side is a function
of Œ∏ only, this equality can only be valid if both sides are equal to some constant,
with
1
R
d
dr

r2 dR
dr

= ‚àí
1
Œò sin Œ∏
d
dŒ∏

sin Œ∏dŒò
dŒ∏

= constant = n(n + 1).
This choice of constant may seem rather ad hoc at this point, but all will become
clear later.
We now have an ordinary diÔ¨Äerential equation for Œò, namely
1
sin Œ∏
d
dŒ∏

sin Œ∏dŒò
dŒ∏

+ n(n + 1)Œò = 0.
Changing variables to
¬µ = cos Œ∏,
y(¬µ) = Œò(Œ∏),
and using
sin2 Œ∏ = 1 ‚àí¬µ2,
d
d¬µ = dŒ∏
d¬µ
d
dŒ∏ = ‚àí1
sin Œ∏
d
dŒ∏,
leads to
d
d¬µ

(1 ‚àí¬µ2) dy
d¬µ

+ n(n + 1)y = 0,
or, equivalently
(1 ‚àí¬µ2)y‚Ä≤‚Ä≤ ‚àí2¬µy‚Ä≤ + n(n + 1)y = 0,
which is Legendre‚Äôs equation. Since a physically meaningful solution must in this
case be Ô¨Ånite at ¬µ = ¬±1 (the north and south poles of the sphere), the solution,
to within an arbitrary multiplicative constant, is the Legendre polynomial, y(¬µ) =
Pn(¬µ), and hence Œòn(Œ∏) = Pn(cos Œ∏). We have introduced the subscript n for Œò so
that we can specify that this is the solution corresponding to a particular choice of
n. Note that we have just solved our Ô¨Årst boundary value problem. SpeciÔ¨Åcally, we
have found the solution of Legendre‚Äôs equation that is bounded at ¬µ = ¬±1.
We must now consider the solution of the equation for R(r). This is
d
dr

r2 dR
dr

= n(n + 1)R.
By inspection, by the method of Frobenius or by noting that the equation is in-
variant under the transformation group (r, R) ‚Üí(Œªr, ŒªR) (see Chapter 10), we can
Ô¨Ånd the solution of this equation as
Rn(r) = Anrn + Bnr‚àí1‚àín,
where again the subscript n denotes the dependence of the solution on n and An
and Bn are arbitrary constants. At the centre of the sphere, the temperature must

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
47
be Ô¨Ånite, so we set Bn = 0. Our solution of Laplace‚Äôs equation therefore takes the
form
T = AnrnPn(cos Œ∏).
As this is a solution for arbitrary n, the general solution for the temperature will
be a linear combination of these solutions,
T =
‚àû

n=0
AnrnPn(cos Œ∏).
The remaining task is to evaluate the coeÔ¨Écients An. This can be done using the
speciÔ¨Åed temperature on the surface of the sphere, T = T0 sin4 Œ∏ at r = 1. We
substitute r = 1 into the general expression for the temperature to get
f(Œ∏) = T0 sin4 Œ∏ =
‚àû

n=0
AnPn(cos Œ∏).
The An will therefore be the coeÔ¨Écients in the Fourier‚ÄìLegendre expansion of the
function f(Œ∏) = T0 sin4 Œ∏.
It is best to work in terms of the variable ¬µ = cos Œ∏. We then have
T0(1 ‚àí¬µ2)2 =
‚àû

n=0
AnPn(¬µ).
From (2.9),
An = 2n + 1
2
 1
‚àí1
T0(1 ‚àí¬µ2)2Pn(¬µ)d¬µ.
Since the function that we want to expand is a Ô¨Ånite polynomial in ¬µ, we expect
to obtain a Ô¨Ånite Fourier‚ÄìLegendre series.
A straightforward calculation of the
integral gives us
A0 = 1
2 ¬∑ 16
15T0,
A1 = 0,
A2 = ‚àí5
2 ¬∑ 32
105T0,
A3 = 0,
A4 = 9
2 ¬∑ 16
315T0,
Am = 0 for m = 5, 6, . . . .
The solution is therefore
T = T0
 8
15P0(cos Œ∏) ‚àí16
21r2P2(cos Œ∏) + 8
35r4P4(cos Œ∏)
	
.
(2.14)
This solution when T0 = 1 is shown in Figure 2.3, which is a polar plot in a plane
of constant œÜ. Note that the temperature at the centre of the sphere is 8T0/15. We
produced Figure 2.3 using the MATLAB script




ezmesh(‚Äôr*cos(t)‚Äô,‚Äôr*sin(t)‚Äô,‚Äô8/15-8*r^2*(3*cos(t)^2-1)/21+...
8*r^4*(35*cos(t)^4/8-15*cos(t)^2/4+3/8)/35‚Äô, [0 1 0 pi])

48
LEGENDRE FUNCTIONS
The function ezmesh gives an easy way of plotting parametric surfaces like this.
The Ô¨Årst three arguments give the x, y and z coordinates as parametric functions
of r and t = Œ∏, whilst the fourth speciÔ¨Åes the ranges of r and t.
Fig. 2.3. The steady state temperature in a uniform sphere with surface temperature
sin4 Œ∏, given by (2.14).
2.6.2
Fluid Flow
Consider a Ô¨Åxed volume V bounded by a surface S within an incompressible
Ô¨Çuid. Although Ô¨Çuid Ô¨Çows into and out of V , the mass of Ô¨Çuid within V remains
constant, since the Ô¨Çuid is incompressible, so any Ô¨Çux out of V at one place is
balanced by a Ô¨Çux into V at another place. Mathematically, we can express this as

S
u ¬∑ n dS = 0,
where u is the velocity of the Ô¨Çuid, n is the outward unit normal to S, and hence
u¬∑n is the normal component of the Ô¨Çuid velocity out through S. If the Ô¨Çuid velocity
Ô¨Åeld, u, is smooth, we can use the divergence theorem to rewrite this statement of
conservation of mass as

V
‚àá¬∑ u dV = 0.
As this applies to an arbitrary volume V within the Ô¨Çuid, we must have ‚àá¬∑ u = 0
throughout the Ô¨Çow.
If we also suppose that the Ô¨Çuid is inviscid (there is no
friction as one Ô¨Çuid element Ô¨Çows past another), it is possible to make the further
assumption that the Ô¨Çow is irrotational (‚àá√ó u = 0). Physically, this means that

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
49
there is no spin in any Ô¨Çuid element as it Ô¨Çows around.
Inviscid, irrotational,
incompressible Ô¨Çows are therefore governed by the two equations ‚àá¬∑ u = 0 and
‚àá√óu = 0.‚Ä† For simply connected Ô¨Çow domains, ‚àá√óu = 0 if and only if u = ‚àáœÜ for
some scalar potential function œÜ, known as the velocity potential. Substituting
into ‚àá¬∑ u = 0 gives ‚àá2œÜ = 0.
In other words, the velocity potential satisÔ¨Åes
Laplace‚Äôs equation. As for boundary conditions, there can be no Ô¨Çux of Ô¨Çuid into
a solid body so that u ¬∑ n = n ¬∑ ‚àáœÜ = ‚àÇœÜ/‚àÇn = 0 where the Ô¨Çuid is in contact with
a solid body.
As an example of such a Ô¨Çow, let‚Äôs consider what happens when a sphere of radius
r = a is placed in a uniform stream, assuming that the Ô¨Çow is steady, inviscid and
irrotational. The Ô¨Çow at inÔ¨Ånity must be uniform, with u = Ui where i is the
unit vector in the x-direction. First of all, it is clear that the problem will be best
treated in spherical polar coordinates. We know from the previous example that
the bounded, axisymmetric solution of Laplace‚Äôs equation is
œÜ =
‚àû

n=0

Anrn + Bnr‚àí1‚àín
Pn(cos Œ∏).
(2.15)
The Ô¨Çow at inÔ¨Ånity has potential œÜ = Ux = U r cos Œ∏. Since P1(cos Œ∏) = cos Œ∏, we
see that we must take A1 = 1 and An = 0 for n > 1. To Ô¨Åx the constants Bn,
notice that there can be no Ô¨Çow through the surface of the sphere. This gives us
the boundary condition on the radial velocity as
ur = ‚àÇœÜ
‚àÇr = 0
at r = a.
On substituting (2.15) into this boundary condition, we Ô¨Ånd that B1 = 1
2a3 and
Bn = 0 for n > 1. The solution is therefore
œÜ = U

r + a3
2 r2

cos Œ∏.
(2.16)
The streamlines (continuous curves that are tangent to the velocity vector) are
shown in Figure 2.4 when a = U = 1. In order to obtain this Ô¨Ågure, we note that
(see Section 7.2.2) the streamlines are given by
œà = U

r ‚àía3
r2

sin Œ∏ = Uy
%
1 ‚àí
a3
(x2 + y2)3/2
&
= constant.
We can then use the MATLAB script




x = linspace(-2,2,400); y = linspace(0,2,200);
[X Y] = meshgrid(x,y);
Z = Y.*(1-1./(X.^2+Y.^2).^(3/2));
v = linspace(0,2,15); contour(X,Y,Z,v)
‚Ä† We will consider an example of viscous Ô¨Çow in Section 6.4.

50
LEGENDRE FUNCTIONS
The command meshgrid creates a grid suitable for use with the plotting command
contour out of the two vectors, x and y. The vector v speciÔ¨Åes the values of œà for
which a contour is to be plotted.
Fig. 2.4. The streamlines for inviscid, irrotational Ô¨Çow about a unit sphere.
In order to complete this example, we must consider the pressure in our ideal
Ô¨Çuid. The force exerted by the Ô¨Çuid on a surface S by the Ô¨Çuid outside S is purely
a pressure, p, which acts normally to S. In other words, the force on a surface
element with area dS and outward unit normal n is ‚àípn dS. We would now like to
apply Newton‚Äôs second law to the motion of the Ô¨Çuid within V , the volume enclosed
by S. In order to do this, we need an expression for the acceleration of the Ô¨Çuid.
Let‚Äôs consider the change in the velocity of a Ô¨Çuid particle between times t and
t + Œ¥t, which, for small Œ¥t, we can Taylor expand to give
u(x(t + Œ¥t), t + Œ¥t) ‚àíu(x(t), t) = Œ¥t
‚àÇu
‚àÇt (x, t) + dx
dt ¬∑ ‚àáu(x, t)
	
+ ¬∑ ¬∑ ¬∑
= Œ¥t
‚àÇu
‚àÇt (x, t) + (u(x, t) ¬∑ ‚àá) u(x, t)
	
+ ¬∑ ¬∑ ¬∑ ,
since u = dx/dt. This means that the Ô¨Çuid acceleration is
Du
Dt = ‚àÇu
‚àÇt + (u ¬∑ ‚àá) u,
where D/Dt is the usual notation for the material derivative, or time derivative
following the Ô¨Çuid particle.
We can now use Newton‚Äôs second law on the Ô¨Çuid within S to obtain

S
‚àípn dS =

V
œÅDu
Dt dV.
After noting that

S
pn dS = i

S
(pi) ¬∑ n dS + j

S
(pj) ¬∑ n dS + k

S
(pk) ¬∑ n dS,

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
51
where i, j, k are unit vectors in the coordinate directions, the divergence theorem,
applied to each of these integrals, shows that

S
pn dS = i

V
‚àÇp
‚àÇx dV + j

V
‚àÇp
‚àÇy dV + k

V
‚àÇp
‚àÇz dV =

V
‚àáp dV,
and hence that

V

œÅDu
Dt + ‚àáp
	
dV = 0.
Since V is arbitrary,
Du
Dt = ‚àí1
œÅ‚àáp,
and for steady Ô¨Çows
(u.‚àá) u = ‚àí1
œÅ‚àáp.
(2.17)
We earlier used the irrotational nature of the Ô¨Çow to write the velocity Ô¨Åeld as the
gradient of a scalar potential, u = ‚àáœÜ. Since (u ¬∑ ‚àá) u ‚â°‚àá
 1
2u ¬∑ u

‚àíu √ó (‚àá√ó u),
for irrotational Ô¨Çow (u ¬∑ ‚àá) u = ‚àá
 1
2u ¬∑ u

and we can write (2.17) as
‚àá
1
2 |‚àáœÜ|2

= ‚àí1
œÅ‚àáp,
and hence
‚àá
p
œÅ + 1
2 |‚àáœÜ|2

= 0,
which we can integrate to give
p
œÅ + 1
2 |‚àáœÜ|2 = C,
(2.18)
which is known as Bernoulli‚Äôs equation. Its implication for inviscid, irrotational
Ô¨Çow is that the pressure can be calculated once we know the velocity potential, œÜ.
In our example, œÜ is given by (2.16), so that
|‚àáœÜ|2 = œÜ2
r + 1
r2 œÜ2
Œ∏ = U 2

1 ‚àía3
r3
2
cos2 Œ∏ + 1
r2

r + a3
2r2
2
sin2 Œ∏

,
and Bernoulli‚Äôs equation gives
p = p‚àû+ 1
2œÅU 2

1 ‚àí

1 ‚àía3
r3
2
cos2 Œ∏ + 1
r2

r + a3
2r2
2
sin2 Œ∏

,
where we have written p‚àûfor the pressure at inÔ¨Ånity. This expression simpliÔ¨Åes on
the surface of the sphere to give
p|r=a = p‚àû+ 1
2œÅU 2

1 ‚àí9
4 sin2 Œ∏

.
This shows that the pressure is highest at Œ∏ = 0 and œÄ, the stream-facing poles of
the sphere, with p = p‚àû+ 1
2œÅU 2, and drops below p‚àûover a portion of the rest of

52
LEGENDRE FUNCTIONS
the boundary, with the lowest pressure, p = p‚àû‚àí5
8œÅU 2, on the equator, Œ∏ = œÄ/2.
The implications of this and the modelling assumptions made are discussed more
fully by Acheson (1990).
2.7
The Associated Legendre Equation
We can also look for a separable solution of Laplace‚Äôs equation in spherical polar
coordinates (r, Œ∏, œÜ), that is not necessarily axisymmetric. If we seek a solution of
the form y(Œ∏)Œ¶(œÜ)R(r), we Ô¨Ånd that Œ¶ = eimœÜ with m an integer. The equation
for y is, after using the change of variable x = cos Œ∏,
(1 ‚àíx2)d 2y
dx2 ‚àí2xdy
dx +

n(n + 1) ‚àí
m2
1 ‚àíx2
	
y = 0.
(2.19)
This equation is known as the associated Legendre equation. It trivially reduces
to Legendre‚Äôs equation when m = 0, corresponding to separable axisymmetric
solutions of Laplace‚Äôs equation. However, the connection is more profound than
this.
If we deÔ¨Åne
Y = (1 ‚àíx2)‚àím/2y,
Y is the solution of
(1 ‚àíx2)Y ‚Ä≤‚Ä≤ ‚àí2(m + 1)xY ‚Ä≤ + (n ‚àím)(n + m + 1)Y = 0.
(2.20)
If we write Legendre‚Äôs equation in the form
(1 ‚àíx2)Z‚Ä≤‚Ä≤ ‚àí2xZ‚Ä≤ + n(n + 1)Z = 0,
which has solution Z = APn(x) + BQn(x), and diÔ¨Äerentiate m times, we get
(1 ‚àíx2)[Z(m)]‚Ä≤‚Ä≤ ‚àí2(m + 1)x[Z(m)]‚Ä≤ + (n ‚àím)(n + m + 1)Z(m) = 0,
(2.21)
where we have written Z(m) = d mZ/dxm. A comparison of (2.20) and (2.21) shows
that
Y = d m
dxm [APn(x) + BQn(x)] ,
and therefore that
y = (1 ‚àíx2)m/2

A d m
dxm Pn(x) + B d m
dxm Qn(x)
	
.
We have now shown that the solutions of the diÔ¨Äerential equation (2.19) are
y = AP m
n (x) + BQm
n (x),
where
P m
n (x) = (1 ‚àíx2)m/2 d m
dxm Pn(x),
Qm
n (x) = (1 ‚àíx2)m/2 d m
dxm Qn(x).
The functions P m
n (x) and Qm
n (x) are called the associated Legendre functions.
Clearly, from what we know about the Legendre polynomials, P m
n (x) = 0 if m > n

2.7 THE ASSOCIATED LEGENDRE EQUATION
53
and Qm
n (x) is singular at x = ¬±1. It is straightforward to show from their deÔ¨Ånitions
that
P 1
1 (x) = (1 ‚àíx2)1/2 = sin Œ∏,
P 1
2 (x) = 3x(1 ‚àíx2)1/2 = 3 sin Œ∏ cos Œ∏ = 3
2 sin 2Œ∏,
P 2
2 (x) = 3(1 ‚àíx2) = 3 sin2 Œ∏ = 3
2(1 ‚àícos 2Œ∏),
P 1
3 (x) = 3
2(5x2 ‚àí1)(1 ‚àíx2)1/2 = 3
8(sin Œ∏ + 5 sin 3Œ∏),
where x = cos Œ∏.
There are various recurrence formulae and orthogonality relations between the
associated Legendre functions, which can be derived in much the same way as those
for the ordinary Legendre functions.
Example: Spherical harmonics
Let‚Äôs try to Ô¨Ånd a representation of the solutions of Laplace‚Äôs equation in three
dimensions,
‚àÇ2u
‚àÇx2 + ‚àÇ2u
‚àÇy2 + ‚àÇ2u
‚àÇz2 = 0,
that are homogeneous in x, y and z. By homogeneous we mean of the form xiyjzk.
It is simplest to work in spherical polar coordinates, in terms of which Laplace‚Äôs
equation takes the form
‚àÇ
‚àÇr

r2 ‚àÇu
‚àÇr

+
1
sin Œ∏
‚àÇ
‚àÇŒ∏

sin Œ∏‚àÇu
‚àÇŒ∏

+
1
sin2 Œ∏
‚àÇ2u
‚àÇœÜ2 = 0.
A homogeneous solution of order n = i + j + k in (x, y, z) will look, in the new
coordinates, like u = rnSn(Œ∏, œÜ). Substituting this expression for u in Laplace‚Äôs
equation, we Ô¨Ånd that
1
sin Œ∏
‚àÇ
‚àÇŒ∏

sin Œ∏‚àÇSn
‚àÇŒ∏

+
1
sin2 Œ∏
‚àÇ2Sn
‚àÇœÜ2 + n(n + 1)Sn = 0.
Separable solutions take the form
Sn = (Am cos mœÜ + Bm sin mœÜ)F(Œ∏),
with m an integer. The function F(Œ∏) satisÔ¨Åes
1
sin Œ∏
d
dŒ∏

sin Œ∏dF
dŒ∏

+

n(n + 1) ‚àí
m2
sin2 Œ∏
	
F = 0.
If we now make the usual transformation, ¬µ = cos Œ∏, F(Œ∏) = y(¬µ), we get
(1 ‚àí¬µ2)y‚Ä≤‚Ä≤ ‚àí2¬µy‚Ä≤ +

n(n + 1) ‚àí
m2
1 ‚àí¬µ2
	
y = 0,

54
LEGENDRE FUNCTIONS
which is the associated Legendre equation. Our solutions can therefore be written
in the form
u = rn(Am cos mœÜ + Bm sin mœÜ){Cn,mP m
n (cos Œ∏) + Dn,mQm
n (cos Œ∏)}.
The quantities rn cos mœÜ P m
n (cos Œ∏), which appear as typical terms in the solution,
are called spherical harmonics and appear widely in the solution of linear bound-
ary value problems in spherical geometry. We will see how they arise in a quantum
mechanical description of the hydrogen atom in Section 4.3.6.
Exercises
2.1
Solve Legendre‚Äôs equation of order two, (1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + 6y = 0, by the
method of Frobenius. What is the simplest solution of this equation? By
using this simple solution and the reduction of order method Ô¨Ånd a closed
form expression for Q1(x).
2.2
Use the generating function to evaluate (a) P ‚Ä≤
n(1), (b) Pn(0).
2.3
Prove that
(a)
P ‚Ä≤
n+1(x) ‚àíP ‚Ä≤
n‚àí1(x) = (2n + 1)Pn(x),
(b)
(1 ‚àíx2)P ‚Ä≤
n(x) = nPn‚àí1(x) ‚àínxPn(x).
2.4
Find the Ô¨Årst four nonzero terms in the Fourier‚ÄìLegendre expansion of the
function
f(x) =
 0
for ‚àí1 < x < 0,
1
for 0 < x < 1.
What value will this series have at x = 0?
2.5
Establish the results
(a)
 1
‚àí1
xPn(x)Pn‚àí1(x)dx =
2n
4n2 ‚àí1 for n = 1, 2, . . . ,
(b)
 1
‚àí1
Pn(x)P ‚Ä≤
n+1(x)dx = 2 for n = 0, 1, . . . ,
(c)
 1
‚àí1
xP ‚Ä≤
n(x)Pn(x)dx =
2n
2n + 1 for n = 0, 1, . . . .
2.6
Determine the Wronskian of Pn and Qn for n = 0, 1, 2, . . . .
2.7
Solve the axisymmetric boundary value problem for Laplace‚Äôs equation,
‚àá2T
=
0 for 0 < r < a, 0 < Œ∏ < œÄ,
T(a, Œ∏)
=
2 cos5 Œ∏.
2.8
Show that
(a) P 2
3 (x)
=
15x(1 ‚àíx2),
(b) P 1
4 (x)
=
5
2(7x3 ‚àí3x)(1 ‚àíx2)1/2.

EXERCISES
55
2.9
‚àóProve that |P ‚Ä≤
n(x)| < n2 and |P ‚Ä≤‚Ä≤
n (x)| < n4 for ‚àí1 < x < 1.
2.10
Derive Equation (2.10).
2.11
‚àóFind the solution of the Dirichlet problem, ‚àá2Œ¶ = 0 in r > 2 subject to
Œ¶ ‚Üí0 as r ‚Üí‚àûand Œ¶(2, Œ∏, œÜ) = sin2 Œ∏ cos 2œÜ.
2.12
‚àóThe self-adjoint form of the associated Legendre equation is
d
dx

(1 ‚àíx2)P m
n (x)

+

n(n + 1) ‚àí
m2
1 ‚àíx2
	
P m
n (x) = 0.
Using this directly, prove the orthogonality property
 1
‚àí1
P m
l (x)P m
n (x)dx = 0 for l Ã∏= n.
Evaluate
 1
‚àí1
[P m
m (x)]2 dx.
2.13
(a) Suppose Pn(x0) = 0 for some x0 ‚àà(‚àí1, 1). Show that x0 is a simple
zero.
(b) Show that Pn with n ‚©æ1 has n distinct zeros in (‚àí1, 1).
2.14
Project A simpliÔ¨Åed model for the left ventricle of a human heart is pro-
vided by a sphere of time-dependent radius R = R(t) with a circular aortic
opening of constant area A, as shown in Figure 2.5. During contraction
we suppose that the opening remains Ô¨Åxed whilst the centre of the sphere
moves directly toward the centre of the opening and the radius R(t) de-
creases accordingly.
As a result, some of the blood Ô¨Ålling the ventricle
cavity is ejected though the opening with mean speed U = U(t) into the
attached cylindrical aorta. This occurs suÔ¨Éciently rapidly that we can as-
sume that the Ô¨Çow is inviscid, irrotational, incompressible, and symmetric
with respect to the aortal axis.
(a) State a partial diÔ¨Äerential equation appropriate to the Ô¨Çuid Ô¨Çow for
this situation.
(b) During contraction, show that a point on the surface of the ventricle
has velocity
ÀôRn ‚àí( ÀôRa)i,
where n is the outward unit normal at time t, i is the unit vector
in the aortic Ô¨Çow direction and a = cos Œ±(t).
Show that having
(R sin Œ±)2 = R2(1 ‚àía2) constant gives
‚àÇœÜ
‚àÇn = f(s) =

Us
for a < s < 1,
ÀôR(1 ‚àís/a)
for ‚àí1 < s < a,
where s = cos Œ∏ with Œ∏ the usual spherical polar coordinate.

56
LEGENDRE FUNCTIONS
Aorta
U (t)
Œ± (t)
R (t)
V
e
n
t
r
i
c
l
e
i
Fig. 2.5. A simple model for the left ventricle of the human heart.
(c) Show that, for a solution to exist,
' 1
‚àí1 f(s)ds = 0.
Deduce that
ÀôR = [a(a ‚àí1)/(a + 1)]U, which relates the geometry to the mean
aortal speed.
(d) Let V = V (t) denote both the interior of the sphere at time t and
its volume. The total momentum in the direction of i is the blood-
density times the integral
I =

V
‚àáœÜ ¬∑ idV =

S
(sœÜ)|r=RdS,
where S is the surface of V . Hence show that
I = 2œÄR2
‚àû

n=0
cn
 1
‚àí1
sPn(s)ds = 4
3œÄR2c1 = 3
2V
 1
‚àí1
sf(s) ds.

EXERCISES
57
(e) Use the answer to part (c) to show that 4I = (1‚àía)(a2+4+3a)V U.
(f) Explain how this model could be made more realistic in terms of the
Ô¨Çuid mechanics and the physiology. You may like to refer to Pedley
(1980) for some ideas on this.

CHAPTER THREE
Bessel Functions
In this chapter, we will discuss a class of functions known as Bessel functions.
These are named after the German mathematician and astronomer Friedrich Bessel,
who Ô¨Årst used them to analyze planetary orbits, as we shall discuss later. Bessel
functions occur in many other physical problems, usually in a cylindrical geometry,
and we will discuss some examples of these at the end of this chapter.
Bessel‚Äôs equation can be written in the form
x2 d 2y
dx2 + xdy
dx +

x2 ‚àíŒΩ2
y = 0,
(3.1)
with ŒΩ real and positive. Note that (3.1) has a regular singular point at x = 0.
Using the notation of Chapter 1,
xQ
P
= x2
x2 = 1,
x2R
P
= x2 
x2 ‚àíŒΩ2
x2
= x2 ‚àíŒΩ2,
both of which are polynomials and have Taylor expansions with inÔ¨Ånite radii of
convergence. Any series solution will therefore also have an inÔ¨Ånite radius of con-
vergence.
3.1
The Gamma Function and the Pockhammer Symbol
Before we use the method of Frobenius to construct the solutions of Bessel‚Äôs equa-
tion, it will be useful for us to make a couple of deÔ¨Ånitions. The gamma function
is deÔ¨Åned by
Œì(x) =
 ‚àû
0
e‚àíqqx‚àí1 dq,
for x > 0.
(3.2)
Note that the integration is over the dummy variable q and x is treated as constant
during the integration. We will start by considering the function evaluated at x = 1.
By deÔ¨Ånition,
Œì(1) =
 ‚àû
0
e‚àíq dq = 1.
We also note that
Œì(x + 1) =
 ‚àû
0
e‚àíqqx dq,

3.1 THE GAMMA FUNCTION AND THE POCKHAMMER SYMBOL
59
which can be integrated by parts to give
Œì(x + 1) =
(
‚àíqxe‚àíq)‚àû
0 +
 ‚àû
0
e‚àíqxqx‚àí1 dq = x
 ‚àû
0
e‚àíqqx‚àí1 dq = xŒì(x).
We therefore have the recursion formula
Œì(x + 1) = xŒì(x).
(3.3)
Suppose that x = n is a positive integer. Then
Œì(n + 1) = nŒì(n) = n(n ‚àí1)Œì(n ‚àí1) = ¬∑ ¬∑ ¬∑ = n(n ‚àí1) . . . 2 ¬∑ 1 = n!.
(3.4)
We therefore have the useful result, Œì(n + 1) = n! for n a positive integer.
We will often need to know Œì (1/2). Firstly, consider the deÔ¨Ånition,
Œì
1
2

=
 ‚àû
0
e‚àíqq‚àí1/2dq.
If we introduce the new variable Q = ‚àöq, so that dQ = 1
2q‚àí1/2dq, this integral
becomes
Œì
1
2

= 2
 ‚àû
0
e‚àíQ2dQ.
We can also write this integral in terms of another new variable, Q = ÀúQ, to obtain

Œì
1
2
	2
=

2
 ‚àû
0
e‚àíQ2dQ
 
2
 ‚àû
0
e‚àíÀú
Q2d ÀúQ

.
Since the limits are independent, we can combine the integrals as

Œì
1
2
	2
= 4
 ‚àû
0
 ‚àû
0
e‚àí(Q2+ Àú
Q2) dQ d ÀúQ.
If we now change to standard polar coordinates we have dQ d ÀúQ = r dr dŒ∏, where
Q = r cos Œ∏ and ÀúQ = r sin Œ∏, and hence

Œì
1
2
	2
= 4
 œÄ/2
Œ∏=0
 ‚àû
r=0
e‚àír2r dr dŒ∏.
The limits of integration give us the positive quadrant of the (Q, ÀúQ)-plane, as re-
quired. Performing the integration over Œ∏ we have

Œì
1
2
	2
= 2œÄ
 ‚àû
0
re‚àír2 dr,
and integrating with respect to r gives

Œì
1
2
	2
= 2œÄ

‚àí1
2e‚àír2‚àû
0
= œÄ.
Finally, we have
Œì
1
2

= 2
 ‚àû
0
e‚àíQ2 dQ = ‚àöœÄ.
(3.5)

60
BESSEL FUNCTIONS
We can use Œì(x) = Œì(x + 1)/x to deÔ¨Åne Œì(x) for negative values of x.
For
example,
Œì

‚àí1
2

= Œì

‚àí1
2 + 1

‚àí1
2
= ‚àí2Œì
1
2

= ‚àí2‚àöœÄ.
We also Ô¨Ånd that Œì(x) is singular at x = 0. From the deÔ¨Ånition, (3.2), the integrand
diverges like 1/q as q ‚Üí0, which is not integrable. Alternatively, Œì(x) = Œì(x+1)/x
shows that Œì(x) ‚àº1/x as x ‚Üí0. Note that the gamma function is available in
MATLAB as the function gamma.
The Pockhammer symbol is a simple way of writing down long products. It
is deÔ¨Åned as
(Œ±)r = Œ±(Œ± + 1)(Œ± + 2) . . . (Œ± + r ‚àí1),
so that, for example, (Œ±)1 = Œ± and (Œ±)2 = Œ±(Œ± + 1), and, in general, (Œ±)r is a
product of r terms. We also choose to deÔ¨Åne (Œ±)0 = 1. Note that (1)n = n!. A
relationship between the gamma function and the Pockhammer symbol that we will
need later is
Œì(x) (x)n = Œì(x + n)
(3.6)
for x real and n a positive integer. To derive this, we start with the deÔ¨Ånition of
the Pockhammer symbol,
(x)n = x(x + 1)(x + 2) . . . (x + n ‚àí1).
Now
Œì(x) (x)n = Œì(x) {x(x + 1)(x + 2) . . . (x + n ‚àí1)}
= {Œì(x)x} {(x + 1)(x + 2) . . . (x + n ‚àí1)} .
Using the recursion relation (3.3),
Œì(x) (x)n = Œì(x + 1) {(x + 1)(x + 2) . . . (x + n ‚àí1)} .
We can repeat this to give
Œì(x) (x)n = Œì(x + n ‚àí1) (x + n ‚àí1) = Œì(x + n).
3.2
Series Solutions of Bessel‚Äôs Equation
We can now proceed to consider a Frobenius solution,
y(x) =
‚àû

n=0
anxn+c.
When substituted into (3.1), this yields
x2
‚àû

n=0
an(n + c)(n + c ‚àí1)xn+c‚àí2 + x
‚àû

n=0
an(n + c)xn+c‚àí1

3.2 SERIES SOLUTIONS OF BESSEL‚ÄôS EQUATION
61
+(x2 ‚àíŒΩ2)
‚àû

n=0
anxn+c = 0.
We can now rearrange this equation and combine corresponding terms to obtain
‚àû

n=0
an{(n + c)2 ‚àíŒΩ2}xn+c +
‚àû

n=0
anxn+c+2 = 0.
We can extract two terms from the Ô¨Årst summation and then shift the second one
to obtain
a0(c2 ‚àíŒΩ2)xc + a1{(1 + c)2 ‚àíŒΩ2}xc+1
+
‚àû

n=2
[an{(n + c)2 ‚àíŒΩ2} + an‚àí2]xn+c = 0.
(3.7)
The indicial equation is therefore c2 ‚àíŒΩ2 = 0, so that c = ¬±ŒΩ.
We can now distinguish various cases. We start with the case for which the
diÔ¨Äerence between the two roots of the indicial equation, 2ŒΩ, is not an integer. Using
Frobenius General Rule II, we can consider both roots of the indicial equation at
once. From the second term of (3.7), which is proportional to xc+1, we have
a1{(1 ¬± ŒΩ)2 ‚àíŒΩ2} = a1(1 ¬± 2ŒΩ) = 0,
which implies that a1 = 0 since 2ŒΩ is not an integer. The recurrence relation that
we obtain from the general term of (3.7) is
an

(n ¬± ŒΩ)2 ‚àíŒΩ2
+ an‚àí2 = 0 for n = 2, 3, . . . ,
and hence an = 0 for n odd. Note that it is not possible for the series solution
to terminate and give a polynomial solution, which is what happens to give the
Legendre polynomials, which we studied in the previous chapter. This makes the
Bessel functions rather more diÔ¨Écult to study.
We will now determine an expression for the value of an for general values of the
parameter ŒΩ. The recurrence relation gives us
an = ‚àí
an‚àí2
n(n ¬± 2ŒΩ).
Let‚Äôs start with n = 2, which yields
a2 = ‚àí
a0
2(2 ¬± 2ŒΩ),
and now with n = 4,
a4 = ‚àí
a2
4(4 ¬± 2ŒΩ).
Substituting for a2 in terms of a0 gives
a4 =
a0
(4 ¬± 2ŒΩ)(2 ¬± 2ŒΩ) ¬∑ 4 ¬∑ 2 =
a0
22(2 ¬± ŒΩ)(1 ¬± ŒΩ)22(2 ¬∑ 1).

62
BESSEL FUNCTIONS
We can continue this process, and we Ô¨Ånd that
a2n = (‚àí1)n
a0
22n (1 ¬± ŒΩ)n n!,
(3.8)
where we have used the Pockhammer symbol to simplify the expression. From this
expression for a2n,
y(x) = a0x¬±ŒΩ
‚àû

n=0
(‚àí1)n
x2n
22n (1 ¬± ŒΩ)n n!.
With a suitable choice of a0 we can write this as
y(x) = A
x¬±ŒΩ
2¬±ŒΩŒì(1 ¬± ŒΩ)
‚àû

n=0
(‚àí1)n

x2/4
n
(1 ¬± ŒΩ)n n! = AJ¬±ŒΩ(x).
These are the Bessel functions of order ¬±ŒΩ. The general solution of Bessel‚Äôs equa-
tion, (3.1), is therefore
y(x) = AJŒΩ(x) + BJ‚àíŒΩ(x),
for arbitrary constants A and B, with
J¬±ŒΩ(x) =
x¬±ŒΩ
2¬±ŒΩŒì(1 ¬± ŒΩ)
‚àû

n=0
(‚àí1)n

x2/4
n
(1 ¬± ŒΩ)n n!.
(3.9)
Remember that 2ŒΩ is not an integer.
Let‚Äôs now consider what happens when ŒΩ = 0, in which case the indicial equation
has a repeated root and we need to apply Frobenius General Rule III. By setting
ŒΩ = 0 in the expression (3.8) for an and exploiting the fact that (1)n = n!, one
solution is
J0(x) =
‚àû

n=0
(‚àí1)n
1
(n!)2
x2
4
n
.
Using Frobenius General Rule III, we can show that the other solution is
Y0(x) = J0(x) log x ‚àí
‚àû

n=0
(‚àí1)n œÜ(n)
(n!)2
x2
4
n
,
which is called Weber‚Äôs Bessel function of order zero. This expression can be
derived by evaluating ‚àÇy/‚àÇc at c = 0. Note that we have made use of the function
œÜ(n) deÔ¨Åned in (1.21).
We now consider the case for which 2ŒΩ is a nonzero integer, beginning with 2ŒΩ
an odd integer, 2n + 1. In this case, the solution takes the form
y(x) = AJn+1/2(x) + BJ‚àín‚àí1/2(x).
As an example, let‚Äôs consider the case ŒΩ = 1
2 so that Bessel‚Äôs equation is
d 2y
dx2 + 1
x
dy
dx +

1 ‚àí
1
4x2

y = 0.

3.2 SERIES SOLUTIONS OF BESSEL‚ÄôS EQUATION
63
We considered this example in detail in Section 1.3.1, and found that
y(x) = a0
cos x
x1/2 + a1
sin x
x1/2 .
This means that (see Exercise 3.2)
J1/2(x) =
*
2
œÄx sin x,
J‚àí1/2(x) =
*
2
œÄx cos x.
The recurrence relations (3.21) and (3.22), which we will derive later, then show
that Jn+1/2(x) and J‚àín‚àí1/2(x) are products of Ô¨Ånite polynomials with sin x and
cos x.
Finally we consider what happens when 2ŒΩ is an even integer, and hence ŒΩ is an
integer. A rather lengthy calculation allows us to write the solution in the form
y = AJŒΩ(x) + BYŒΩ(x),
where YŒΩ is Weber‚Äôs Bessel function of order ŒΩ deÔ¨Åned as
YŒΩ(x) = JŒΩ(x) cos ŒΩœÄ ‚àíJ‚àíŒΩ(x)
sin ŒΩœÄ
.
(3.10)
Notice that the denominator of this expression is obviously zero when ŒΩ is an integer,
so this case requires careful treatment. We note that the second solution of Bessel‚Äôs
equation can also be determined using the method of reduction of order as
y(x) = AJŒΩ(x) + BJŒΩ(x)
 x
1
q JŒΩ(q)2 dq.
In Figure 3.1 we show Ji(x) for i = 0 to 3. Note that J0(0) = 1, but that Ji(0) = 0
for i > 0, and that J(j)
i
(0) = 0 for j < i, i > 1. We generated Figure 3.1 using the
MATLAB script
#
"
 
!
x=0:0.02:20;
subplot(2,2,1), plot(x,besselj(0,x)), title(‚ÄôJ_0(x)‚Äô)
subplot(2,2,2), plot(x,besselj(1,x)), title(‚ÄôJ_1(x)‚Äô)
subplot(2,2,3), plot(x,besselj(2,x)), title(‚ÄôJ_2(x)‚Äô)
subplot(2,2,4), plot(x,besselj(3,x)), title(‚ÄôJ_3(x)‚Äô)
We produced Figures 3.2, 3.5 and 3.6 in a similar way, using the MATLAB functions
bessely, besseli and besselk.
In Figure 3.2 we show the Ô¨Årst two Weber‚Äôs Bessel functions of integer order.
Notice that as x ‚Üí0, Yn(x) ‚Üí‚àí‚àû. As you can see, all of these Bessel functions
are oscillatory. The Ô¨Årst three zeros of J0(x) are 2.4048, 5.5201 and 8.6537, whilst
the Ô¨Årst three nontrivial zeros of J1(x) are 3.8317, 7.0156 and 10.1735, all to four
decimal places.

64
BESSEL FUNCTIONS
Fig. 3.1. The functions J0(x), J1(x), J2(x) and J3(x).
Fig. 3.2. The functions Y0(x) and Y1(x).
3.3
The Generating Function for Jn(x), n an integer
Rather like the Legendre polynomials, there is a simple function that will generate
all of the Bessel functions of integer order. In order to establish this, it is useful to
manipulate the deÔ¨Ånition of the JŒΩ(x). From (3.9), we have, for ŒΩ = n,
Jn(x) =
xn
2nŒì(1 + n)
‚àû

i=0

‚àíx2/4
i
i!(1 + n)i
=
‚àû

i=0
(‚àí1)ix2i+n
22i+ni!(1 + n)iŒì(1 + n).

3.3 THE GENERATING FUNCTION FOR Jn(x), n AN INTEGER
65
Using (3.6) we note that Œì(1 + n)(1 + n)i = Œì(1 + n + i), and using (3.4) we Ô¨Ånd
that this is equal to (n + i)!. For n an integer, we can therefore write
Jn(x) =
‚àû

i=0
(‚àí1)ix2i+n
22i+ni!(n + i)!.
(3.11)
Let‚Äôs now consider the generating function
g(x, t) = exp
1
2x

t ‚àí1
t
	
.
(3.12)
The series expansions of each of the constituents of this are
exp (xt/2) =
‚àû

j=0
(xt/2)j
j!
,
exp

‚àíx
2t

=
‚àû

i=0
(‚àíx/2t)i
i!
,
both from the Taylor series for ex. These summations can be combined to produce
g(x, t) =
‚àû

j=0
‚àû

i=0
(‚àí1)ixi+jtj‚àíi
2j+ii!j!
.
Now, putting j = i + n so that ‚àí‚àû‚©Ωn ‚©Ω‚àû, this becomes
g(x, t) =
‚àû

n=‚àí‚àû
 ‚àû

i=0
(‚àí1)ix2i+n
22i+ni!(n + i)!

tn.
Comparing the coeÔ¨Écients in this series with the expression (3.11) we Ô¨Ånd that
g(x, t) = exp
1
2x

t ‚àí1
t
	
=
‚àû

n=‚àí‚àû
Jn(x)tn.
(3.13)
We can now exploit this relation to study Bessel functions.
Using the fact that the generating function is invariant under t ‚Üí‚àí1/t, we have
‚àû

n=‚àí‚àû
Jn(x)tn =
‚àû

n=‚àí‚àû
Jn(x)

‚àí1
t
n
=
‚àû

n=‚àí‚àû
Jn(x)(‚àí1)nt‚àín.
Now putting m = ‚àín, this is equal to
‚àí‚àû

m=‚àû
J‚àím(x)(‚àí1)mtm.
Now let n = m in the series on the right hand side, which gives
‚àû

n=‚àí‚àû
Jn(x)tn =
‚àû

n=‚àí‚àû
J‚àín(x)(‚àí1)ntn.
Comparing like terms in the series, we Ô¨Ånd that Jn(x) = (‚àí1)nJ‚àín(x), and hence
that Jn(x) and J‚àín(x) are linearly dependent over R (see Section 1.2.1).
This
explains why the solution of Bessel‚Äôs equation proceeds rather diÔ¨Äerently when ŒΩ is
an integer, since JŒΩ(x) and J‚àíŒΩ(x) cannot then be independent solutions.

66
BESSEL FUNCTIONS
The generating function can also be used to derive an integral representation of
Jn(x). The Ô¨Årst step is to put t = e¬±iŒ∏ in (3.13), which yields
e¬±ix sin Œ∏ = J0(x) +
‚àû

n=1
(¬±1)n 
eniŒ∏ + (‚àí1)ne‚àíniŒ∏
Jn(x),
or, in terms of sine and cosine,
e¬±ix sin Œ∏ = J0(x) + 2
‚àû

n=1
J2n(x) cos 2nŒ∏ ¬± i
‚àû

n=0
J2n+1(x) sin(2n + 1)Œ∏.
Appropriate combinations of these two cases show that
cos(x sin Œ∏) = J0(x) + 2
‚àû

n=1
J2n(x) cos 2nŒ∏,
(3.14)
sin(x sin Œ∏) = 2
‚àû

n=0
J2n+1(x) sin (2n + 1) Œ∏,
(3.15)
and, substituting Œ∑ = œÄ
2 ‚àíŒ∏, we obtain
cos(x cos Œ∑) = J0(x) + 2
‚àû

n=1
(‚àí1)nJ2n(x) cos 2nŒ∑,
(3.16)
sin(x cos Œ∑) = 2
‚àû

n=0
(‚àí1)nJ2n+1(x) cos (2n + 1) Œ∑.
(3.17)
Multiplying (3.16) by cos mŒ∑ and integrating from zero to œÄ, we Ô¨Ånd that
 œÄ
Œ∑=0
cos mŒ∑ cos(x cos Œ∑) dŒ∑ =
 œÄJm(x)
for m even,
0
for m odd.
Similarly we Ô¨Ånd that
 œÄ
Œ∑=0
sin mŒ∑ sin(x cos Œ∑) dŒ∑ =

0
for m even,
œÄJm(x)
for m odd,
from (3.17). Adding these expressions gives us the integral representation
Jn(x) = 1
œÄ
 œÄ
0
cos (nŒ∏ ‚àíx sin Œ∏) dŒ∏.
(3.18)
We shall now describe a problem concerning planetary motion that makes use
of (3.18). This is the context in which Bessel originally studied these functions.
Example: Planetary motion
We consider the motion of a planet under the action of the gravitational force ex-
erted by the Sun. In doing this, we neglect the eÔ¨Äect of the gravitational attraction
of the other bodies in the solar system, which are all much less massive than the
Sun. Under this approximation, it is straightforward to prove Kepler‚Äôs Ô¨Årst and
second laws, that the planet moves on an ellipse, with the Sun at one of its foci,

3.3 THE GENERATING FUNCTION FOR Jn(x), n AN INTEGER
67
and that the line from the planet to the Sun (PS in Figure 3.3) sweeps out equal
areas of the ellipse in equal times (see, for example, Lunn, 1990). Our aim now
is to use Kepler‚Äôs Ô¨Årst and second laws to obtain a measure of how the passage of
the planet around its orbit depends upon time. We will denote the length of the
a
ea
Fig. 3.3. The elliptical orbit of a planet, P, around the Sun at a focus, S. Here, C is the
centre of the ellipse and A and A‚Ä≤ the extrema of the orbit on the major axis of the ellipse.
semi-major axis, A‚Ä≤C, by a and the eccentricity by e. Note that distance of the Sun
from the centre of the ellipse, SC, is ea. We also deÔ¨Åne the mean anomaly to be
¬µ = 2œÄ Area of the elliptic sector ASP
Area of the ellipse
,
which Kepler‚Äôs second law tells us is proportional to the time of passage from A to
P.
Let‚Äôs now consider the auxiliary circle, which has centre C and passes through
A and A‚Ä≤, as shown in Figure 3.4. We label the projection of the point P onto the
auxiliary circle as Q. We will also need to introduce the eccentric anomaly of P,
which is deÔ¨Åned to be the angle ACQ, and can be written as
œÜ = 2œÄ Area of the sector ACQ
Area of the auxiliary circle.
We now note that, by orthogonal projection, the ratio of the area of ASP to that
of the ellipse is the same as the ratio of the area of ASQ to that of the auxiliary
circle. The area of ASQ is given by the area of the sector ACQ ( 1
2œÜa2) minus the
area of the triangle CSQ ( 1
2ea2 sin œÜ), so that
¬µ
œÄ =
1
2a2œÜ ‚àí1
2ea2 sin œÜ
1
2œÄa2
,
and hence
¬µ = œÜ ‚àíe sin œÜ.
(3.19)
Now, in order to determine œÜ as a function of ¬µ, we note that œÜ ‚àí¬µ is a periodic

68
BESSEL FUNCTIONS
Fig. 3.4. The auxiliary circle and the projection of P onto Q.
function of ¬µ, which vanishes when P and Q are coincident with A or A‚Ä≤, that is
when ¬µ is an integer multiple of œÄ. Hence we must be able to write
œÜ ‚àí¬µ =
‚àû

n=1
An sin n¬µ.
(3.20)
As we shall see in Chapter 5, this is a Fourier series. In order to determine the
constant coeÔ¨Écients An, we diÔ¨Äerentiate (3.20) with respect to ¬µ to yield
dœÜ
d¬µ ‚àí1 =
‚àû

n=1
nAn cos n¬µ.
We can now exploit the orthogonality of the functions cos n¬µ to determine An. We
multiply through by cos m¬µ and integrate from zero to œÄ to obtain
 œÄ
¬µ=0
dœÜ
d¬µ ‚àí1

cos m¬µ d¬µ =
 œÄ
¬µ=0
cos m¬µdœÜ
d¬µ d¬µ = œÄm
2 Am.
Since œÜ = 0 when ¬µ = 0 and œÜ = œÄ when ¬µ = œÄ, we can change the independent
variable to give
œÄm
2 Am =
 œÄ
œÜ=0
cos m¬µ dœÜ.
Substituting for ¬µ from (3.19), we have that
Am =
2
mœÄ
 œÄ
œÜ=0
cos m (œÜ ‚àíe sin œÜ) dœÜ.
Finally, by direct comparison with (3.18), Am = 2
mJm(me), so that
œÜ = ¬µ + 2

J1(e) sin ¬µ + 1
2J2(2e) sin 2¬µ + 1
3J3(3e) sin 3¬µ + ¬∑ ¬∑ ¬∑
	
.

3.4 DIFFERENTIAL AND RECURRENCE RELATIONS
69
Since ¬µ is proportional to the time the planet takes to travel from A to P, this
expression gives us the variation of the angle œÜ with time.
3.4
DiÔ¨Äerential and Recurrence Relations Between Bessel Functions
It is often useful to Ô¨Ånd relationships between Bessel functions with diÔ¨Äerent indices.
We will derive two such relationships. We start with (3.9), multiply by xŒΩ and
diÔ¨Äerentiate to obtain
d
dx[xŒΩJŒΩ(x)] = d
dx
 ‚àû

n=0
(‚àí1)n x2n+2ŒΩ
22n+ŒΩ n! Œì(1 + ŒΩ + n)

=
‚àû

n=0
(‚àí1)n (2n + 2ŒΩ) x2n+2ŒΩ‚àí1
22n+ŒΩ n! Œì(1 + ŒΩ + n)
.
Since Œì(1 + ŒΩ + n) = (n + ŒΩ)Œì(n + ŒΩ), this gives a factor that cancels with the term
2(n + ŒΩ) in the numerator to give
d
dx[xŒΩJŒΩ(x)] =
‚àû

n=0
(‚àí1)nx2n xŒΩ xŒΩ‚àí1
22n+ŒΩ‚àí1 n! Œì(ŒΩ + n).
We can rewrite this so that we have the series expansion for JŒΩ‚àí1(x), as
d
dx[xŒΩJŒΩ(x)] = xŒΩxŒΩ‚àí1
2ŒΩ‚àí1
‚àû

n=0
(‚àí1)n x2n
22n n! Œì(ŒΩ)(ŒΩ)n
,
so that
d
dx {xŒΩJŒΩ(x)} = xŒΩJŒΩ‚àí1(x).
(3.21)
Later, we will use this expression to develop relations between general Bessel func-
tions. Note that by putting ŒΩ equal to zero
d
dx {J0(x)} = J‚àí1(x).
However, we recall that Jn(x) = (‚àí1)nJ‚àín(x) for n an integer, so that J1(x) =
‚àíJ‚àí1(x) and hence
J‚Ä≤
0(x) = ‚àíJ1(x),
where we have used a prime to denote the derivative with respect to x.
In the same vein as the derivation of (3.21),
d
dx(x‚àíŒΩJŒΩ(x)) = d
dx
%
x‚àíŒΩxŒΩ
2ŒΩŒì(1 + ŒΩ)
‚àû

n=0

‚àíx2/4
n
n! (1 + ŒΩ)n
&
= d
dx
% ‚àû

n=0
(‚àí1)n x2n
22n+ŒΩ n! Œì(1 + ŒΩ + n)
&
=
‚àû

n=0
(‚àí1)n x2n‚àí1 n
22n+ŒΩ‚àí1 n! Œì(1 + ŒΩ + n).
Notice that the Ô¨Årst term in this series is zero (due to the factor of n in the numer-
ator), so we can start the series at n = 1 and cancel the factors of n. The series

70
BESSEL FUNCTIONS
can then be expressed in terms of the dummy variable m = n ‚àí1 as
d
dx(x‚àíŒΩJŒΩ(x)) =
1
2ŒΩ‚àí1
‚àû

m=0
(‚àí1)m+1 x2m+1
22m+2 m! Œì(ŒΩ + m + 2)
= ‚àí
1
2ŒΩ+1
‚àû

m=0
(‚àí1)m x2m+1
22m m! Œì(ŒΩ + m + 2).
Using the fact that Œì(ŒΩ + m + 2) = Œì(ŒΩ + 2) (ŒΩ + 2)m and the series expansion of
JŒΩ+1(x), (3.11), we have
d
dx

x‚àíŒΩJŒΩ(x)

= ‚àí
x
2ŒΩ+1Œì(ŒΩ + 1)
‚àû

m=0

‚àíx2/4
m
m! (2 + ŒΩ)m
,
and consequently
d
dx

x‚àíŒΩJŒΩ(x)

= ‚àíx‚àíŒΩJŒΩ+1(x).
(3.22)
Notice that (3.21) and (3.22) both hold for YŒΩ(x) as well.
We can use these relationships to derive recurrence relations between the
Bessel functions. We expand the diÔ¨Äerentials in each expression to give the equa-
tions
J‚Ä≤
ŒΩ(x) + ŒΩ
xJŒΩ(x) = JŒΩ‚àí1(x),
where we have divided through by xŒΩ, and
J‚Ä≤
ŒΩ(x) ‚àíŒΩ
xJŒΩ(x) = ‚àíJŒΩ+1(x),
where this time we have multiplied by xŒΩ. By adding these expressions we Ô¨Ånd that
J‚Ä≤
ŒΩ(x) = 1
2 {JŒΩ‚àí1(x) ‚àíJŒΩ+1(x)} ,
and by subtracting them
2ŒΩ
x JŒΩ(x) = JŒΩ‚àí1(x) + JŒΩ+1(x),
which is a pure recurrence relationship.
These results can also be used when integrating Bessel functions. For example,
consider the integral
I =

xJ0(x) dx.
This can be integrated using (3.21) with ŒΩ = 1, since
I =

xJ0(x) dx =

(xJ1(x))‚Ä≤ dx = xJ1(x).

3.5 MODIFIED BESSEL FUNCTIONS
71
3.5
ModiÔ¨Åed Bessel Functions
We will now consider the solutions of the modiÔ¨Åed Bessel equation,
x2 d 2y
dx2 + xdy
dx ‚àí(x2 + ŒΩ2)y = 0,
(3.23)
which can be derived from Bessel‚Äôs equation using x ‚Üíix, so that the solutions
could be written down as conventional Bessel functions with purely imaginary ar-
guments. However, it is more transparent to introduce the modiÔ¨Åed Bessel func-
tion of Ô¨Årst kind of order ŒΩ, IŒΩ(x), so that the complete solution of (3.23) is
y(x) = AIŒΩ(x) + BI‚àíŒΩ(x),
provided ŒΩ is not an integer. As was the case when we introduced the function YŒΩ(x),
there is a corresponding function here KŒΩ(x), the modiÔ¨Åed Bessel function of
second kind of order ŒΩ. This is deÔ¨Åned as
KŒΩ(x) =
œÄ
2 sin ŒΩœÄ {I‚àíŒΩ(x) ‚àíIŒΩ(x)} .
Note that the slight diÔ¨Äerence between the deÔ¨Ånition of this Bessel function and
that of Weber‚Äôs Bessel function of order ŒΩ, (3.10), occurs because these functions
must agree with the deÔ¨Ånition of the ordinary Bessel functions when ŒΩ is an integer.
Most of the results we have derived in this chapter can be modiÔ¨Åed by changing the
argument from x to ix in deriving, for example, the recurrence relations. The Ô¨Årst
few modiÔ¨Åed Bessel functions are shown in Figures 3.5 and 3.6. Note the contrast
in behaviour between these and the Bessel functions JŒΩ(x) and YŒΩ(x).
Equations (3.21) and (3.22) also hold for the modiÔ¨Åed Bessel functions and are
given by
d
dx {xŒΩIŒΩ(x)} = xŒΩIŒΩ‚àí1(x),
d
dx

x‚àíŒΩIŒΩ(x)

= x‚àíŒΩIŒΩ+1(x)
and
d
dx {xŒΩKŒΩ(x)} = ‚àíxŒΩKŒΩ‚àí1(x),
d
dx

x‚àíŒΩKŒΩ(x)

= ‚àíx‚àíŒΩKŒΩ+1(x).
3.6
Orthogonality of the Bessel Functions
In this section we will show that the Bessel functions are orthogonal, and hence can
be used as a basis over a certain interval. This will then allow us to develop the
Fourier‚ÄìBessel series, which can be used to represent functions in much the same
way as the Fourier‚ÄìLegendre series.
We will consider the interval [0, a] where, at this stage, a remains arbitrary. We
start from the fact that the function JŒΩ(x) satisÔ¨Åes the Bessel equation, (3.1), and
make a simple transformation, replacing x by Œªx, where Œª is a real constant, to
give
x2 d 2
dx2 JŒΩ(Œªx) + x d
dxJŒΩ(Œªx) + (Œª2x2 ‚àíŒΩ2)JŒΩ(Œªx) = 0.
(3.24)
We choose Œª so that JŒΩ(Œªa) is equal to zero. There is a countably inÔ¨Ånite number

72
BESSEL FUNCTIONS
Fig. 3.5. The modiÔ¨Åed Bessel functions of Ô¨Årst kind of order zero to three. Note that
they are bounded at x = 0 and monotone increasing for x > 0, with In(x) ‚àºex/
‚àö
2œÄx as
x ‚Üí‚àû.
Fig. 3.6. The modiÔ¨Åed Bessel functions of the second kind of order zero and order one.
Note that they are singular, with K0(x) ‚àº‚àílog x and Kn(x) ‚àº2n‚àí1(n ‚àí1)!/xn for n a
positive integer, as x ‚Üí0. These functions are monotone decreasing for x > 0, tending to
zero exponentially fast as x ‚Üí‚àû.
of values of Œª for which this is true, as we can deduce from Figure 3.1. Now choose
¬µ Ã∏= Œª so that JŒΩ(¬µa) = 0. Of course JŒΩ(¬µx) also satisÔ¨Åes (3.24) with Œª replaced by
¬µ. We now multiply (3.24) through by JŒΩ(¬µx)/x and integrate between zero and
a, which yields
 a
0
JŒΩ(¬µx)

x d 2
dx2 JŒΩ(Œªx) + d
dxJŒΩ(Œªx) + 1
x(Œª2x2 ‚àíŒΩ2)JŒΩ(Œªx)
	
dx = 0.
(3.25)

3.6 ORTHOGONALITY OF THE BESSEL FUNCTIONS
73
Notice that we could have also multiplied the diÔ¨Äerential equation for JŒΩ(¬µx) by
JŒΩ(Œªx)/x and integrated to give
 a
0
JŒΩ(Œªx)

x d 2
dx2 JŒΩ(¬µx) + d
dxJŒΩ(¬µx) + 1
x(¬µ2x2 ‚àíŒΩ2)JŒΩ (¬µx)
	
dx = 0.
(3.26)
We now subtract (3.25) from (3.26) to give
 a
0

JŒΩ(¬µx) d
dx

x d
dxJŒΩ(Œªx)

‚àíJŒΩ(Œªx) d
dx

x d
dxJŒΩ(¬µx)

+ x(Œª2 ‚àí¬µ2)JŒΩ(Œªx)JŒΩ(¬µx)
	
dx = 0.
(3.27)
We have simpliÔ¨Åed these expressions slightly by observing that xJ‚Ä≤‚Ä≤
ŒΩ + J‚Ä≤
ŒΩ = (xJ‚Ä≤
ŒΩ)‚Ä≤.
Now consider the Ô¨Årst term of the integrand and note that it can be integrated by
parts to give
 a
0
JŒΩ(¬µx) d
dx

x d
dxJŒΩ(Œªx)

dx =

JŒΩ(¬µx)x d
dxJŒΩ(Œªx)
a
0
‚àí
 a
0
x d
dxJŒΩ(Œªx) d
dxJŒΩ(¬µx)dx.
Similarly for the second term, which is eÔ¨Äectively the same with ¬µ and Œª inter-
changed,
 a
0
JŒΩ(Œªx) d
dx

x d
dxJŒΩ(¬µx)

dx =

JŒΩ(Œªx)x d
dxJŒΩ(¬µx)
a
0
‚àí
 a
0
x d
dxJŒΩ(¬µx) d
dxJŒΩ(Œªx)dx.
Using these expressions in (3.27), we Ô¨Ånd that
(Œª2 ‚àí¬µ2)
 a
0
xJŒΩ(Œªx)JŒΩ(¬µx)dx = JŒΩ(Œªa)a¬µJ‚Ä≤
ŒΩ(¬µa) ‚àíJŒΩ(¬µa)aŒªJ‚Ä≤
ŒΩ(Œªa).
(3.28)
Finally, since we chose Œª and ¬µ so that JŒΩ(Œªa) = JŒΩ(¬µa) = 0 and ¬µ Ã∏= Œª,
 a
0
xJŒΩ(¬µx)JŒΩ(Œªx)dx = 0.
This is an orthogonality relation for the Bessel functions, with weighting function
w(x) = x. We now need to calculate the value of
' a
0 xJŒΩ(¬µx)2 dx. To this end, we
substitute Œª = ¬µ + œµ into (3.28). For œµ ‚â™1, neglecting terms in œµ2 and smaller, we
Ô¨Ånd that
‚àí2¬µœµ
 a
0
xJŒΩ(¬µx)JŒΩ(¬µx + œµx) dx
= a [¬µJŒΩ(¬µa + œµa)J‚Ä≤
ŒΩ(¬µa) ‚àí(¬µ + œµ)JŒΩ(¬µa)J‚Ä≤
ŒΩ(¬µa + œµa)] .
(3.29)
In order to deal with the terms evaluated at x = a(¬µ + œµ) we consider Taylor series

74
BESSEL FUNCTIONS
expansions, JŒΩ(a(¬µ + œµ)) = JŒΩ(a¬µ) + œµaJ‚Ä≤
ŒΩ(a¬µ) + ¬∑ ¬∑ ¬∑ , J‚Ä≤
ŒΩ(a(¬µ + œµ)) = J‚Ä≤
ŒΩ(a¬µ) +
œµaJ‚Ä≤‚Ä≤
ŒΩ (a¬µ) + ¬∑ ¬∑ ¬∑ . These expansions can then be substituted into (3.29). On dividing
through by œµ and considering the limit œµ ‚Üí0, we Ô¨Ånd that
 a
0
x[JŒΩ(¬µx)]2 dx = a2
2 [J‚Ä≤
ŒΩ(¬µa)]2 ‚àí1
2a2JŒΩ(¬µa)J‚Ä≤‚Ä≤
ŒΩ (¬µa) ‚àía
2¬µJŒΩ(¬µa)J‚Ä≤
ŒΩ(¬µa).
We now suppose that JŒΩ(¬µa) = 0, which gives
 a
0
x[JŒΩ(¬µx)]2 dx = a2
2 [J‚Ä≤
ŒΩ(¬µa)]2.
In general,
 a
0
xJŒΩ(¬µx)JŒΩ(Œªx) dx = a2
2 [J‚Ä≤
ŒΩ(¬µa)]2Œ¥Œª¬µ,
(3.30)
where JŒΩ(¬µa) = JŒΩ(Œªa) = 0 and Œ¥Œª¬µ is the Kronecker delta function.
We can now construct a series expansion of the form
f(x) =
‚àû

i=1
CiJŒΩ(Œªix).
(3.31)
This is known as a Fourier‚ÄìBessel series, and the Œªi are chosen such that
JŒΩ(Œªia) = 0 for i = 1, 2, . . . , Œª1 < Œª2 < ¬∑ ¬∑ ¬∑ . As we shall see later, both f(x)
and f ‚Ä≤(x) must be piecewise continuous for this series to converge. After multiply-
ing both sides of (3.31) by xJŒΩ(Œªjx) and integrating over the interval [0, a] we Ô¨Ånd
that
 a
0
xJŒΩ(Œªjx)f(x) dx =
 a
0
xJŒΩ(Œªjx)
‚àû

i=1
CiJŒΩ(Œªix) dx.
Assuming that the series converges, we can interchange the integral and the sum-
mation to obtain
 a
0
xJŒΩ(Œªjx)f(x) dx =
‚àû

i=1
Ci
 a
0
xJŒΩ(Œªjx)JŒΩ(Œªix) dx.
We can now use (3.30) to give
 a
0
xJŒΩ(Œªjx)f(x) dx =
‚àû

i=1
Ci
a2
2 [J‚Ä≤
ŒΩ(Œªja)]2 Œ¥ŒªjŒªi = Cj
Œªja2
2
[J‚Ä≤
ŒΩ(Œªja)]2 ,
and hence
Cj =
2
a2 [J‚Ä≤ŒΩ(Œªja)]2
 a
0
xJŒΩ(Œªjx)f(x) dx.
(3.32)
Example
Let‚Äôs try to expand the function f(x) = 1 on the interval 0 ‚©Ωx ‚©Ω1, as a Fourier‚Äì
Bessel series. Since J0(0) = 1 but Ji(0) = 0 for i > 0, we will choose ŒΩ to be zero for
our expansion. We rely on the existence of a set of values Œªj such that J0(Œªj) = 0

3.6 ORTHOGONALITY OF THE BESSEL FUNCTIONS
75
for j = 1, 2, . . . (see Figure 3.1). We will need to determine these values either from
tables or numerically.
Using (3.32), we have
Cj =
2
[J‚Ä≤
0(Œªj)]2
 1
0
xJ0(Œªjx) dx.
If we introduce the variable y = Œªjx (so that dy = Œªjdx), the integral becomes
Cj =
2
[J‚Ä≤
0(Œªj)]2
1
Œª2
j
 Œªj
y=0
yJ0(y) dy.
Using the expression (3.21) with ŒΩ = 1, we have
Cj =
2
[J‚Ä≤
0(Œªj)]2
1
Œª2
j
 Œªj
y=0
d
dy (yJ1(y)) dy
=
2
Œª2
j [J‚Ä≤
0(Œªj)]2 [yJ1(y)]Œªj
0 =
2J1(Œªj)
Œªj [J‚Ä≤
0(Œªj)]2 =
2
ŒªjJ1(Œªj),
and hence
‚àû

i=1
2
ŒªiJ1(Œªi)J0(Œªix) = 1 for 0 ‚©Ωx < 1,
where J0(Œªi) = 0 for i = 1, 2, . . . . In Figure 3.7 we show the sum of the Ô¨Årst Ô¨Åfteen
terms of the Fourier‚ÄìBessel series. Notice the oscillatory nature of the solution,
which is more pronounced in the neighbourhood of the discontinuity at x = 1. This
phenomenon always occurs in series expansions relative to sequences of orthogonal
functions, and is called Gibbs‚Äô phenomenon.
Before we can give a MATLAB script that produces Figure 3.7, we need to be
able to calculate the zeros of Bessel functions. Here we merely state a couple of
simple results and explain how these are helpful. The interested reader is referred
to Watson (1922, Chapter 15) for a full discussion of this problem. The Bessel‚Äì
Lommel theorem on the location of the zeros of the Bessel functions JŒΩ(x) states
that when ‚àí1
2 < ŒΩ ‚©Ω1
2 and mœÄ < x < (m + 1
2)œÄ, JŒΩ(x) is positive for even m and
negative for odd m. This implies that JŒΩ(x) has an odd number of zeros in the
intervals (2n ‚àí1)œÄ/2 < x < 2nœÄ for n an integer. In fact, it can be shown that the
positive zeros of J0(x) lie in the intervals nœÄ + 3œÄ/4 < x < nœÄ + 7œÄ/8. This allows
us to use the ends of these intervals as an initial bracketing interval for the roots
of J0(x). A simple MATLAB script that uses this result is
'
&
$
%
global nu ze
nu=0;
for ir=1:20
start_int = (ir-1)*pi+3*pi/4;
end_int
= (ir-1)*pi+7*pi/8;
ze(ir) = fzero(‚Äôbessel‚Äô,[start_int end_int]);
end

76
BESSEL FUNCTIONS
Fig. 3.7. Fourier‚ÄìBessel series representation of the function f(x) = 1 on the interval [0, 1)
(truncated after Ô¨Åfteen terms). The expansion is only valid in the interval x ‚àà[0, 1). The
series is shown for x > 1 only to demonstrate that convergence is only guaranteed for the
associated interval.
together with the function




function bessel = bessel(x);
global nu
bessel = besselj(nu,x);
The MATLAB function fzero Ô¨Ånds a zero of functions of a single variable in a given
interval. By deÔ¨Åning the variables nu and ze as global, we make them available
to other functions. In particular, this allows us to use the computed positions of
the zeros, ze, in the script below.
The zeros of J1(x) interlace those of J0(x).
We can see this by noting that
J1(x) = ‚àíJ‚Ä≤
0(x) and that both functions are continuous. Consequently the zeros of
J0(x) can be used as the bracketing intervals for the determination of the zeros of
J1(x). A MATLAB script for this is

3.7 INHOMOGENEOUS TERMS IN BESSEL‚ÄôS EQUATION
77
'
&
$
%
global nu ze ze1
nu=1;
for ir=1:19
start_int = ze(ir);
end_int
= ze(ir+1);
ze1(ir)
= fzero(‚Äôbessel‚Äô,[start_int end_int]);
end
The zeros of the other Bessel functions can be found by exploiting similar analytical
results.
We can now deÔ¨Åne a MATLAB function
'
&
$
%
function fourierbessel = fourierbessel(x)
global ze
n=15; a = 2./ze(1:n)./besselj(1,ze(1:n));
for k = 1:n
X(:,k) = besselj(0,ze(k)*x(:));
end
fourierbessel = X*a‚Äô;
which can be plotted with ezplot(@fourierbessel, [0 2]) to produce Figure 3.7.
3.7
Inhomogeneous Terms in Bessel‚Äôs Equation
So far we have only concerned ourselves with homogeneous forms of Bessel‚Äôs equa-
tion. The inhomogeneous version of Bessel‚Äôs equation,
x2 d 2y
dx2 + xdy
dx + (x2 ‚àíŒΩ2)y = f(x),
can be dealt with by using the technique of variation of parameters (see Section 1.2).
The solution can be written as
y(x) =
 x s sin ŒΩœÄ
2ŒΩ
f(s) (JŒΩ(s)YŒΩ(x) ‚àíJŒΩ(x)YŒΩ(s)) ds + AJŒΩ(x) + BYŒΩ(x). (3.33)
Here we have made use of the fact that the Wronskian associated with JŒΩ(x) and
YŒΩ(x) is 2ŒΩ/x sin ŒΩœÄ, which can be derived using Abel‚Äôs formula, (1.7). The constant
can then be found by considering the behaviour of the functions close to x = 0 (see
Exercise 3.5).
Example
Let‚Äôs Ô¨Ånd the general solution of
x2 d 2y
dx2 + xdy
dx + x2y = x2.

78
BESSEL FUNCTIONS
This can be determined by using (3.33) with f(x) = x2 and ŒΩ = 0, so that the
general solution is
y(x) =
 x œÄs
2 (J0(s)Y0(x) ‚àíJ0(x)Y0(s)) ds + AJ0(x) + BY0(x).
In order to integrate sJ0(s) we note that this can be written as (sJ1(s))‚Ä≤ and
similarly for sY0(s), which gives
y(x) = œÄx
2 (J1(x)Y0(x) ‚àíJ0(x)Y1(x)) + AJ0(x) + BY0(x).
But we note that J1(x) = ‚àíJ‚Ä≤
0(x) and Y1(x) = ‚àíY ‚Ä≤
0(x), so that the expression in
the brackets is merely the Wronskian, and hence
y(x) = 1 + AJ0(x) + BY0(x).
Although it is clear, with hindsight, that y(x) = 1 is the particular integral solution,
simple solutions are not always easy to spot a priori.
Example
Let‚Äôs Ô¨Ånd the particular integral of
x2 d 2y
dx2 + xdy
dx + (x2 ‚àíŒΩ2)y = x.
(3.34)
We will look for a series solution as used in the method of Frobenius, namely
y(x) =
‚àû

n=0
anxn+c.
Substituting this into (3.34), we obtain an expression similar to (3.7),
a0(c2 ‚àíŒΩ2)xc + a1{(1 + c)2 ‚àíŒΩ2}xc+1
+
‚àû

n=2
[an{(n + c)2 ‚àíŒΩ2} + an‚àí2]xn+c = x.
Note that xc needs to match with the x on the right hand side so that c = 1 and
a0 = 1/(1 ‚àíŒΩ2). We will defer discussion of the case ŒΩ = 1. At next order we Ô¨Ånd
that a1(22 ‚àíŒΩ2) = 0, and consequently, unless ŒΩ = 2, we have a1 = 0. For the
general terms in the summation we have
an = ‚àí
an‚àí2
{(n + 1)2 ‚àíŒΩ2}.
Note that since a1 = 0, an = 0 for n odd. It now remains to determine the general
term in the sequence. For n = 2 we Ô¨Ånd that
a2 = ‚àí
a0
32 ‚àíŒΩ2 = ‚àí
1
(12 ‚àíŒΩ2)(32 ‚àíŒΩ2)
and then with n = 4 and using the form of a2 we have
a4 =
1
(12 ‚àíŒΩ2)(32 ‚àíŒΩ2)(52 ‚àíŒΩ2).

3.8 SOLUTIONS EXPRESSIBLE AS BESSEL FUNCTIONS
79
The general expression is
a2n = (‚àí1)n
n+1

i=1
1
(2i ‚àí1)2 ‚àíŒΩ2 .
This can be manipulated by factorizing the denominator and extracting a factor of
22 from each term in the product (of which there are n + 1). This gives
a2n = (‚àí1)n
22n+2
n+1

i=1
1
i ‚àí1
2 + 1
2ŒΩ
n+1

i=1
1
i ‚àí1
2 ‚àí1
2ŒΩ = (‚àí1)n
22n+2
1
 1
2 + 1
2ŒΩ

n+1
 1
2 ‚àí1
2ŒΩ

n+1
.
Hence the particular integral of the diÔ¨Äerential equation is
y(x) = x
‚àû

n=0
(‚àí1)n
22n+2
x2n
 1
2 + 1
2ŒΩ

n+1
 1
2 ‚àí1
2ŒΩ

n+1
.
(3.35)
In fact, solutions of the equation
x2 d 2y
dx2 + xdy
dx + (x2 ‚àíŒΩ2)y = x¬µ+1
(3.36)
are commonly referred to as s¬µ,ŒΩ and are called Lommel‚Äôs functions. They are
undeÔ¨Åned when ¬µ ¬± ŒΩ is an odd negative integer, a case that is discussed in depth
by Watson (1922). The series expansion of the solution of (3.36) is
y(x) = x¬µ‚àí1
‚àû

m=0
(‚àí1)m  1
2x
2m+2 Œì
 1
2¬µ ‚àí1
2ŒΩ + 1
2

Œì
 1
2¬µ + 1
2ŒΩ + 1
2

Œì
 1
2¬µ ‚àí1
2ŒΩ + m + 3
2

Œì
 1
2¬µ + 1
2ŒΩ + m + 3
2

.
We can use this to check that (3.35) is correct. Note that we need to use (3.6).
3.8
Solutions Expressible as Bessel Functions
There are many other diÔ¨Äerential equations whose solutions can be written in terms
of Bessel functions. In order to determine some of these, we consider the transfor-
mation
y(x) = xŒ±Àúy(xŒ≤).
Since Œ± and Œ≤ could be fractional, we will restrict our attention to x ‚©æ0. We
substitute this expression into the diÔ¨Äerential equation and seek values of Œ± and Œ≤
which give Bessel‚Äôs equation for Àúy.
Example
Let‚Äôs try to express the solutions of the diÔ¨Äerential equation
d 2y
dx2 ‚àíxy = 0
in terms of Bessel functions. This is called Airy‚Äôs equation and has solutions
Ai(x) and Bi(x), the Airy functions. We start by introducing the function Àúy.

80
BESSEL FUNCTIONS
DiÔ¨Äerentiating with respect to x we have
dy
dx = Œ±xŒ±‚àí1Àúy + Œ≤xŒ±+Œ≤‚àí1Àúy‚Ä≤.
DiÔ¨Äerentiating again we obtain
d 2y
dx2 = Œ±(Œ± ‚àí1)xŒ±‚àí2Àúy + (2Œ±Œ≤ + Œ≤2 ‚àíŒ≤)xŒ±+Œ≤‚àí2Àúy‚Ä≤ + Œ≤2xŒ±+2Œ≤‚àí2Àúy‚Ä≤‚Ä≤.
These expressions can now be substituted into Airy‚Äôs equation to give
Œ±(Œ± ‚àí1)xŒ±‚àí2Àúy + (2Œ±Œ≤ + Œ≤2 ‚àíŒ≤)xŒ±+Œ≤‚àí2Àúy‚Ä≤ + Œ≤2xŒ±+2Œ≤‚àí2Àúy‚Ä≤‚Ä≤ ‚àíxŒ±+1Àúy = 0.
It is now convenient to multiply the entire equation by x‚àíŒ±+2/Œ≤2 (this means that
the coeÔ¨Écient of Àúy‚Ä≤‚Ä≤ is x2Œ≤), which gives
x2Œ≤ Àúy‚Ä≤‚Ä≤ + (2Œ± + Œ≤ ‚àí1)
Œ≤
xŒ≤ Àúy‚Ä≤ +

‚àí1
Œ≤2 x3 + Œ±(Œ± ‚àí1)
Œ≤2
	
Àúy = 0.
Considering the coeÔ¨Écient of Àúy we note that we require x3 ‚àùx2Œ≤ which gives Œ≤ = 3
2.
The coeÔ¨Écient of Àúy‚Ä≤ gives us that Œ± = 1
2. The equation is now
x2Œ≤ Àúy‚Ä≤‚Ä≤ + xŒ≤ Àúy‚Ä≤ +

‚àí
2xŒ≤
3
2
‚àí1
9

Àúy = 0,
which has solutions K1/3(2x3/2/3) and I1/3(2x3/2/3). The general solution of Airy‚Äôs
equation in terms of Bessel‚Äôs functions is therefore
y(x) = x1/2

AK1/3
2
3x3/2

+ BI1/3
2
3x3/2
	
.
In fact, Ai(x) =
#
x/3K1/3(2x3/2/3)/œÄ. A graph of this Airy function is shown in
Figure 11.12. The Airy functions Ai and Bi are available in MATLAB through the
function airy.
3.9
Physical Applications of the Bessel Functions
3.9.1
Vibrations of an Elastic Membrane
We will now derive the equation that governs small displacements, z = z(x, y, t),
of an elastic membrane.
We start by considering a small membrane with sides
of length Œ¥Sx in the x-direction and Œ¥Sy in the y-direction, which makes angles
œàx, œàx+Œ¥œàx and œày, œày+Œ¥œày with the horizontal, as shown in Figure 3.8. Newton‚Äôs
second law of motion in the vertical, z-direction gives
‚àÇ2z
‚àÇt2 œÅŒ¥SxŒ¥Sy = Œ¥Sy{T sin(œàx + Œ¥œàx) ‚àíT sin(œàx)} + Œ¥Sx{T sin(œày + Œ¥œày) ‚àíT sin(œày)},
where œÅ is the constant density (mass per unit area) of the membrane and T is the
tension, assumed constant for small vibrations of the membrane. We will eventually
consider the angles œàx and œày to be small, but at the outset we will consider the

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
81
T
T
T
y
x
T
Œ¥Sy
Œ¥Sx
œàx + Œ¥œàx
œày + Œ¥œày
œày
œàx
Fig. 3.8. A small section of an elastic membrane.
changes in these angles, Œ¥œàx and Œ¥œày, to be smaller. Accordingly we expand the
trigonometric functions and Ô¨Ånd that
œÅ
T
‚àÇ2z
‚àÇt2 = cos œàx
Œ¥œàx
Œ¥Sx
+ cos œày
Œ¥œày
Œ¥Sy
+ ¬∑ ¬∑ ¬∑ ,
where we have divided through by the area of the element, Œ¥SxŒ¥Sy. We now consider
the limit as the size of the element shrinks to zero, and therefore let Œ¥Sx and Œ¥Sy
tend to zero. Consequently we Ô¨Ånd that
œÅ
T
‚àÇ2z
‚àÇt2 = cos œàx
‚àÇœàx
‚àÇSx
+ cos œày
‚àÇœày
‚àÇSy
.
(3.37)
We can now use the deÔ¨Ånition of the partial derivatives,
tan œàx = ‚àÇz
‚àÇx,
tan œày = ‚àÇz
‚àÇy .
By diÔ¨Äerentiating these expressions with respect to x and y respectively we Ô¨Ånd
that
sec2œàx
‚àÇœàx
‚àÇx = ‚àÇ2z
‚àÇx2 ,
sec2œày
‚àÇœày
‚àÇy = ‚àÇ2z
‚àÇy2 .
For small slopes, cos œàx and sec2œàx are both approximately unity (and similarly for
variation in the y direction). Also, using the formula for arc length we have
dSx =
+
1 +
 ‚àÇz
‚àÇx
2
dx ‚âàdx,

82
BESSEL FUNCTIONS
when |‚àÇz/‚àÇx| ‚â™1. Similarly dSy ‚âàdy when |‚àÇz/‚àÇy| ‚â™1. Consequently
‚àÇœàx
‚àÇSx
‚âà‚àÇœàx
‚àÇx ,
‚àÇœày
‚àÇSy
‚âà‚àÇœày
‚àÇy .
Combining this information yields the governing equation for small deÔ¨Çections z =
z(x, y, t) of an elastic membrane,
œÅ
T
‚àÇ2z
‚àÇt2 = ‚àÇ2z
‚àÇx2 + ‚àÇ2z
‚àÇy2 = ‚àá2z,
(3.38)
the two-dimensional wave equation. We will deÔ¨Åne appropriate boundary con-
ditions in due course. At this stage we have not speciÔ¨Åed the domain of solution.
One-Dimensional Solutions of the Wave Equation
We will start by considering the solution of this equation in one dimension.
If
we look for solutions of the form z ‚â°z(x, t), independent of y, as illustrated in
Figure 3.9, we need to solve the one-dimensional wave equation,
‚àÇ2z
‚àÇt2 = c2 ‚àÇ2z
‚àÇx2 ,
(3.39)
where c =
#
T/œÅ. This equation also governs the propagation of small-amplitude
waves on a stretched string (for further details, see Billingham and King, 2001).
The easiest way to solve (3.39) is to deÔ¨Åne new variables, known as characteristic
Fig. 3.9. A one-dimensional solution of the two-dimensional wave equation.
variables, Œæ = x ‚àíct and Œ∑ = x + ct (see Section 7.1 for an explanation of where

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
83
these come from). In terms of these variables, (3.39) becomes
‚àÇ2z
‚àÇŒæ‚àÇŒ∑ = 0.
Integrating this with respect to Œ∑ gives
‚àÇz
‚àÇŒæ = F(Œæ),
and with respect to Œæ
z =
 Œæ
F(s) ds + g(Œ∑) = f(Œæ) + f(Œ∑),
and hence we have that
z(x, t) = f(x ‚àíct) + g(x + ct).
(3.40)
This represents the sum of two waves, one, represented by f(x ‚àíct), propagating
from left to right without change of form at speed c, and one, represented by
g(x+ct), from right to left at speed c. To see this, consider, for example, the solution
y = f(x ‚àíct), and simply note that on the paths x = ct + constant, f(x ‚àíct) is
constant. Similarly, g(x + ct) is constant on the paths x = ‚àíct + constant.
The functions f and g can be determined from the initial displacement and
velocity. If
z(x, 0) = z0(x),
‚àÇz
‚àÇt (x, 0) = u0(x),
then (3.40) with t = 0 gives us
f(x) + g(x) = z0(x).
(3.41)
If we now diÔ¨Äerentiate (3.40) with respect to time, we have
‚àÇz
‚àÇt = ‚àícf ‚Ä≤(x ‚àíct) + cg‚Ä≤(x + ct),
and hence when t = 0,
‚àícf ‚Ä≤(x) + cg‚Ä≤(x) = u0(x).
This can be integrated to yield
‚àícf(x) + cg(x) =
 x
a
u0(s) ds,
(3.42)
where a is an arbitrary constant. Solving the simultaneous equations (3.41) and
(3.42), we Ô¨Ånd that
f(x) = 1
2z0(x) ‚àí1
2c
 x
a
u0(s) ds,
g(x) = 1
2z0(x) + 1
2c
 x
a
u0(s) ds.
On substituting these into (3.40), we obtain d‚ÄôAlembert‚Äôs solution of the one-
dimensional wave equation,
z(x, t) = 1
2 {z0(x ‚àíct) + z0(x + ct)} + 1
2c
 x+ct
x‚àíct
u0(s) ds.
(3.43)

84
BESSEL FUNCTIONS
In particular, if u0 = 0, a string released from rest, the solution consists of a
left-travelling and a right-travelling wave, each with the same shape but half the
amplitude of the initial displacement. The solution when z0 = 0 for |x| > a and
z0 = 1 for |x| < a, a top hat initial displacement, is shown in Figure 3.10.
Fig. 3.10. D‚ÄôAlembert‚Äôs solution for an initially stationary top hat displacement.
Two-Dimensional Solutions of the Wave Equation
Let‚Äôs now consider the solution of the two-dimensional wave equation, (3.38), for
an elastic, disc-shaped membrane Ô¨Åxed to a circular support. In cylindrical polar
coordinates, the two-dimensional wave equation becomes
1
c2
‚àÇ2z
‚àÇt2 = ‚àÇ2z
‚àÇr2 + 1
r
‚àÇz
‚àÇr + 1
r2
‚àÇ2z
‚àÇŒ∏2 .
(3.44)
We will look for solutions in the circular domain 0 ‚©Ωr ‚©Ωa and 0 ‚©ΩŒ∏ < 2œÄ. Such
solutions must be periodic in Œ∏ with period 2œÄ. The boundary condition is z = 0 at
r = a. We seek a separable solution, z = R(r)œÑ(t)Œò(Œ∏). On substituting this into
(3.44), we Ô¨Ånd that
1
c2
œÑ ‚Ä≤‚Ä≤
œÑ = rR‚Ä≤‚Ä≤ + R‚Ä≤
rR
+ 1
r2
Œò‚Ä≤‚Ä≤
Œò = ‚àíœâ2
c2 ,

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
85
where ‚àíœâ2/c2 is the separation constant. An appropriate solution is œÑ = eiœât, which
represents a time-periodic solution. This is what we would expect for the vibrations
of an elastic membrane, which is, after all, a drum. The angular frequency, œâ,
is yet to be determined. We can now write
r2R‚Ä≤‚Ä≤ + rR‚Ä≤
R
+ œâ2r2
c2
= ‚àíŒò‚Ä≤‚Ä≤
Œò = n2,
where n2 is the next separation constant. This gives us Œò = A cos nŒ∏ + B sin nŒ∏,
with n a positive integer for 2œÄ-periodicity. Finally, R(r) satisÔ¨Åes
r2 d 2R
dr2 + rdR
dr +
œâ2
c2 r2 ‚àín2

R = 0.
We can simplify this by introducing a new coordinate s = Œªr where Œª = œâ/c, which
gives
s2 d 2R
ds2 + sdR
ds +

s2 ‚àín2
R = 0,
which is Bessel‚Äôs equation with ŒΩ = n. Consequently the solutions can be written
as
R(s) = AJn(s) + BYn(s).
We need a solution that is bounded at the origin so, since Yn(s) is unbounded at
s = 0, we require B = 0. The other boundary condition is that the membrane is
constrained not to move at the edge of the domain, so that R(s) = 0 at s = Œªa.
This gives the condition
Jn(Œªa) = 0,
(3.45)
which has an inÔ¨Ånite number of solutions Œª = Œªni. Specifying the value of Œª = œâ/c
prescribes the frequency at which the membrane will oscillate. Consequently the
functional form of the natural modes of oscillation of a circular membrane is
z = Jn(Œªnir) (A cos nŒ∏ + B sin nŒ∏) eiœâit,
(3.46)
where œâi = cŒªni and the values of Œªni are solutions of (3.45). Figure 3.11 shows
a few of these natural modes when a = 1, which we created using the MATLAB
function ezmesh (see Section 2.6.1).
Here we have considered the natural modes of oscillation. We could however
have tackled an initial value problem. Let‚Äôs consider an example where the initial
displacement of the membrane is speciÔ¨Åed to be z(r, Œ∏, 0) = G(r, Œ∏) and the mem-
brane is released from rest, so that ‚àÇz/‚àÇt = 0, when t = 0. The fact that the
membrane is released from rest implies that the temporal variation will be even, so
we need only consider a solution proportional to cos œâit. Consequently, using the
linearity of the wave equation to add all the possible solutions of the form (3.46),
the general form of the displacement of the membrane is
z(r, Œ∏, t) =
‚àû

i=0
‚àû

n=0
Jn(Œªnir) (Ani cos nŒ∏ + Bni sin nŒ∏) cos œâit.

86
BESSEL FUNCTIONS
Fig. 3.11. Six diÔ¨Äerent modes of oscillation of an elastic membrane.
Using the condition that z(r, Œ∏, 0) = G(r, Œ∏) we have the equations
G(r, Œ∏) =
‚àû

i=0
‚àû

n=0
Jn(Œªnir) (Ani cos nŒ∏ + Bni sin nŒ∏) .
(3.47)
In order to Ô¨Ånd the coeÔ¨Écients Ani and Bni we need exploit the orthogonality of
the Bessel functions, given by (3.30), and of the trigonometric functions, using
 2œÄ
0
cos mŒ∏ cos nŒ∏ dŒ∏ =
 2œÄ
0
sin mŒ∏ sin nŒ∏ dŒ∏ = œÄŒ¥mn,
for m and n integers. Multiplying (3.47) through by cos mŒ∏ and integrating from 0
to 2œÄ we have
1
œÄ
 2œÄ
0
cos mŒ∏ G(r, Œ∏) dŒ∏ =
‚àû

i=0
AmiJn(Œªnir),
and with sin mŒ∏ we have
1
œÄ
 2œÄ
0
sin mŒ∏ G(r, Œ∏) dŒ∏ =
‚àû

i=0
BmiJn(Œªnir).

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
87
Now, using (3.32), we have
Amj =
2
a2œÄ [J‚Ä≤n(Œªja)]2
 a
r=0
rJn(Œªjr)
 2œÄ
0
cos mŒ∏ G(r, Œ∏) dŒ∏ dr
and
Bmj =
2
a2œÄ [J‚Ä≤n(Œªja)]2
 a
r=0
rJn(Œªjr)
 2œÄ
0
sin mŒ∏ G(r, Œ∏) dŒ∏ dr.
Note that if the function G(r, Œ∏) is even, Bmj = 0, and similarly if it is odd, Amj = 0.
The expansion in Œ∏ is an example of a Fourier series expansion, about which we
will have more to say in Chapter 5.
3.9.2
Frequency Modulation (FM)
We will now discuss an application in which Bessel functions occur within the
description of a modulated wave (for further information, see Dunlop and Smith,
1977). The expression for a frequency carrier comprises a carrier frequency fc and
the modulating signal with frequency fm. The phase shift of the carrier is related
to the time integral of the modulating wave Fm(t) = cos 2œÄfmt, so that the actual
signal is
Fc(t) = cos

2œÄfct + 2œÄK2
 t
0
a cos(2œÄfmt)dt
	
,
which we can integrate to obtain
Fc(t) = cos {2œÄfct + Œ≤ sin(2œÄfmt)} ,
(3.48)
where Œ≤ = K2a/fm, a constant called the modulation index. We can expand
(3.48) to give
Fc(t) = cos(2œÄfct) cos {Œ≤ sin(2œÄfmt)} ‚àísin(2œÄfct) sin {Œ≤ sin(2œÄfmt)} .
(3.49)
An example of this signal is shown in Figure 3.12.
We would now like to split the signal (3.49) into its various frequency compo-
nents. We can do this by exploiting (3.16) and (3.17) to give
Fc(t) = cos(2œÄfct)

J0(Œ≤) + 2
‚àû

n=1
(‚àí1)nJ2n(Œ≤) cos 2n(2œÄfmt)

+ sin(2œÄfct)

2
‚àû

n=0
(‚àí1)nJ2n+1(Œ≤) cos (2n + 1) (2œÄfmt)

.
(3.50)
Using simple trigonometry,
2 cos(2œÄfct) cos {(2n)2œÄfmt} =
cos {2œÄfct + (2n)2œÄfmt} + cos {2œÄfct ‚àí(2n)2œÄfmt} ,

88
BESSEL FUNCTIONS
Fig. 3.12. An example of frequency modulation, with Œ≤ = 1.2, fm = 1.2 and fc = 1.
and
2 sin(2œÄfct) sin {(2n + 1)2œÄfmt} =
‚àícos {2œÄfct + (2n + 1)2œÄfmt} + cos {2œÄfct ‚àí(2n + 1)2œÄfmt} .
Substituting these expressions back into (3.50), we Ô¨Ånd that Fc(t) can be written
as
Fc(t) = J0(Œ≤) cos(2œÄfct)
+
‚àû

n=1
J2n(Œ≤) [cos {2œÄt (fc + 2nfm)} + cos {2œÄt (fc ‚àí2nfm)}]
‚àí
‚àû

n=0
J2n+1(Œ≤) [cos {2œÄt (fc ‚àí(2n + 1)fm)} ‚àícos {2œÄt (fc + (2n + 1)fm)}] ,
the Ô¨Årst few terms of which are given by
Fc(t) = J0(Œ≤) cos(2œÄfct) ‚àíJ1(Œ≤) [cos {2œÄ(fc ‚àífm)t} ‚àícos {2œÄ(fc + fm)t}]
+ J2(Œ≤) [cos {2œÄ(fc ‚àí2fm)t} + cos {2œÄ(fc + 2fm)t}]
‚àíJ3(Œ≤) [cos {2œÄ(fc ‚àí3fm)t} ‚àícos {2œÄ(fc + 3fm)t}] + ¬∑ ¬∑ ¬∑ .
This means that the main carrier signal has frequency fc and amplitude J0(Œ≤), and
that the other components, known as sidebands, have frequencies fc ¬± nfm for

EXERCISES
89
Fig. 3.13. The frequency spectrum of a signal with Œ≤ = 0.2, fm = 1.2 and fc = 1.
n an integer, and amplitudes Jn(Œ≤). These amplitudes, known as the frequency
spectrum of the signal, are shown in Figure 3.13.
Exercises
3.1
The functions J0(x) and Y0(x) are solutions of the equation
xd 2y
dx2 + dy
dx + xy = 0.
Show that if Y0(x) is the singular solution then
Y0(x) = J0(x) log x ‚àí
‚àû

i=1
œÜ(i)
(i!)2

‚àíx2
4
i
,
where
œÜ(i) =
i

k=1
1
k .
3.2
Using the deÔ¨Ånition
JŒΩ(x) =
xŒΩ
2ŒΩŒì(1 + ŒΩ)
‚àû

i=0
(‚àíx2/4)i
i!(1 + ŒΩ)i
,
show that J1/2(x) =
$
2
œÄx sin x.

90
BESSEL FUNCTIONS
3.3
‚àóShow that
(a) 2J‚Ä≤
ŒΩ(x) = JŒΩ‚àí1(x) ‚àíJŒΩ+1(x),
(b) 2ŒΩJŒΩ(x) = xJŒΩ+1(x) + xJŒΩ‚àí1(x).
3.4
Determine the series expansions for the modiÔ¨Åed Bessel functions IŒΩ(x) and
I‚àíŒΩ(x).
3.5
Find the Wronskian of the functions (a) JŒΩ(x) and J‚àíŒΩ(x), (b) JŒΩ(x) and
YŒΩ(x) for ŒΩ not an integer.
3.6
Determine the series expansion of

x¬µJŒΩ(x) dx.
When ¬µ = 1 and ŒΩ = 0 show that this is equivalent to xJ1(x).
3.7
Give the solutions, where possible in terms of Bessel functions, of the dif-
ferential equations
(a) d 2y
dx2 ‚àíx2y = 0,
(b) xd 2y
dx2 + dy
dx + y = 0,
(c) xd 2y
dx2 + (x + 1)2y = 0,
(d) d 2y
dx2 + Œ±2y = 0,
(e) d 2y
dx2 ‚àíŒ±2y = 0,
(f) d 2y
dx2 + Œ≤ dy
dx + Œ≥y = 0,
(g) (1 ‚àíx2)d 2y
dx2 ‚àí2xdy
dx + n(n + 1)y = 0.
3.8
Using the expression
JŒΩ(z) = 1
2œÄ
 2œÄ
0
cos (ŒΩŒ∏ ‚àíz sin Œ∏) dŒ∏,
show that JŒΩ(0) = 0 for ŒΩ a nonzero integer and J0(0) = 1.
3.9
Determine the coeÔ¨Écients of the Fourier‚ÄìBessel series for the function
f(x) =

1
for 0 ‚©Ωx < 1,
‚àí1
for 1 ‚©Ωx ‚©Ω2,
in terms of the Bessel function J0(x).
3.10
Determine the coeÔ¨Écients of the Fourier‚ÄìBessel series for the function
f(x) = x on the interval 0 ‚©Ωx ‚©Ω1 in terms of the Bessel function J1(x)
(and repeat the exercise for J2(x)). Modify the MATLAB code used to
generate Figure 3.7 to check your answers.
3.11
Calculate the Fourier‚ÄìBessel expansion for the functions

EXERCISES
91
(a)
f(x) =

x
for 0 ‚©Ωx < 1,
2 ‚àíx
for 1 ‚©Ωx ‚©Ω2,
(b)
f(x) =
 x2 + 2
for 0 ‚©Ωx < 1,
3
for 1 ‚©Ωx ‚©Ω3,
in terms of the Bessel function J1(x).
3.12
Construct the general solution of the diÔ¨Äerential equation
x2 d 2y
dx2 + xdy
dx + (x2 ‚àíŒΩ2)y = sin x.
3.13
Project This project arises from attempts to model the baking of food-
stuÔ¨Äs, and thereby improve the quality of mass-produced baked foods. A
key element of such modelling is the temperature distribution in the food,
which, as we showed in Chapter 2, is governed by the diÔ¨Äusion equation,
‚àÇT
‚àÇt = ‚àá¬∑ (D‚àáT) .
The diÔ¨Äusivity, D, is a function of the properties of the food.
We will consider some problems associated with the baking of inÔ¨Ånitely-
long, cylindrical, axisymmetric foodstuÔ¨Äs under axisymmetric conditions
(a Ô¨Årst approximation to, for example, the baking of a loaf of bread). In
this case, the diÔ¨Äusion equation becomes
‚àÇT
‚àÇt = 1
r
‚àÇ
‚àÇr

rD(r)‚àÇT
‚àÇr

.
(E3.1)
(a) Look for a separable solution, T(r, t) = f(r)eœât, of (E3.1) with
D(r) = D0, a constant, subject to the boundary conditions ‚àÇT/‚àÇr =
0 at r = 0 and ‚àÇT/‚àÇr = h(Ta ‚àíT) at r = r0. Here, h > 0 is a heat
transfer coeÔ¨Écient and Ta is the ambient temperature. Determine
the possible values of œâ.
(b) If the initial temperature proÔ¨Åle within the foodstuÔ¨Äis T(r, 0) =
T0(r), determine the temperature for t > 0.
(c) In many baking problems, there are two distinct layers of food.
When
D(r) =
 D0
for 0 ‚©Ωr ‚©Ωr1,
D1
for r1 < r ‚©Ωr0,
and the heat Ô¨Çux and the temperature are continuous at r = r1,
determine the eÔ¨Äect of changing r1/r0 and D0 and D1 on the possible
values of œâ.
(d) Solve the initial‚Äìboundary value problem when the initial tempera-
ture is uniform in each layer, with
T(r, 0) =
 T0
for 0 ‚©Ωr ‚©Ωr1,
T1
for r1 < r ‚©Ωr0.

92
BESSEL FUNCTIONS
(e) Find some realistic values of D0 and h for bread dough, either from
your library or the Internet. How does the temperature at the centre
of baking bread vary with time, if this simple model is to be believed?

CHAPTER FOUR
Boundary Value Problems, Green‚Äôs Functions and
Sturm‚ÄìLiouville Theory
We now turn our attention to boundary value problems for ordinary diÔ¨Äerential
equations, for which the boundary conditions are speciÔ¨Åed at two diÔ¨Äerent points,
x = a and x = b. The solutions of boundary value problems have some rather
diÔ¨Äerent properties to those of solutions of initial value problems.
In order to
ground our discussion in terms of a real physical problem, we will consider the
dynamics of a string Ô¨Åxed at x = y = 0 and x = l, y = 0, and rotating steadily
about the x-axis at constant angular velocity, as shown in Figure 4.1.
This is
rather like a skipping rope, but one whose ends are motionless. In order to be able
Œ∏(x)
x = l
œâ
x
y
Fig. 4.1. A string rotating about the x-axis, Ô¨Åxed at x = 0 and x = l.
to formulate a diÔ¨Äerential equation that captures the dynamics of this string, we
will make several assumptions.

94
BOUNDARY VALUE PROBLEMS
(i) The tension in the string is large enough that any additional forces intro-
duced by the bending of the string are negligible in comparison with the
tension force.
(ii) The tension force acts along the local tangent to the string, and is of constant
magnitude, T.
(iii) The slope of the string, and hence its displacement from the x-axis, is small.
(iv) The eÔ¨Äect of gravity is negligible.
(v) There is no friction, either due to air resistance on the string or to the Ô¨Åxing
of the string at each of its ends.
(vi) The thickness of the string is negligible, but it has a constant line density,
œÅ, a mass per unit length.
We denote the constant angular velocity of the string about the x-axis by œâ,
and the angle that the tangent to the string makes with the x-axis by the function
Œ∏(x). Working in a frame of reference that rotates with the string, in which the
string is stationary, Newton‚Äôs Ô¨Årst law shows that the forces that act on the string,
including the centrifugal force, must be in balance, and hence that
T sin Œ∏(x + Œ¥x) ‚àíT sin Œ∏(x) = ‚àíœÅœâ2y
#
Œ¥x2 + Œ¥y2,
as shown in Figure 4.2. If we divide through by Œ¥x and take the limit Œ¥x ‚Üí0 we
obtain
T d
dx {sin Œ∏ (x)} + œÅœâ2y
+
1 +
dy
dx
2
= 0.
By deÔ¨Ånition, tan Œ∏ = dy/dx, so, by elementary trigonometry,
sin Œ∏ = dy
dx

1 +
dy
dx
2‚àí1/2
,
and hence
T d 2y
dx2 + œÅœâ2y

1 +
dy
dx
22
= 0.
(4.1)
This equation of motion for the string must be solved subject to y(0) = y(l) =
0, which constitutes a rather nasty nonlinear boundary value problem, with no
elementary solution apart from the equilibrium solution, y = 0.‚Ä†
If, however,
we invoke assumption (iii), that |dy/dx| ‚â™1, the leading order boundary value
problem is
d 2y
dx2 + Œªy = 0,
subject to y(0) = y(l) = 0,
(4.2)
where Œª = œÅœâ2/T. This is now a linear boundary value problem, which we can
solve by elementary methods.
‚Ä† This nonlinear boundary value problem can, however, easily be studied in the phase plane (see
Chapter 9).

BOUNDARY VALUE PROBLEMS
95
T
x
y
T
Œ∏(x)
Œ∏(x+ Œ¥x)
Œ¥x
Œ¥y
x+Œ¥x
x
Fig. 4.2. A small element of the rotating string.
If we look for a solution of the form y = Aemx, we obtain m2 +Œª = 0, and hence
m = ¬±iŒª1/2, so that the solution is y = AeiŒª1/2x + Be‚àíiŒª1/2x. Since y(0) = 0,
B = ‚àíA, and y(l) = 0 gives
eiŒª1/2l ‚àíe‚àíiŒª1/2l = 0.
(4.3)
At this stage, we do not know whether Œª is real, although, since it represents a
ratio of real quantities, it should be. If we write Œª1/2 = Œ± + iŒ≤ and equate real and
imaginary parts in (4.3), we Ô¨Ånd that

e‚àíŒ≤l ‚àíeŒ≤l
cos Œ±l = 0,

e‚àíŒ≤l + eŒ≤l
sin Œ±l = 0.
(4.4)
From the Ô¨Årst of these, we have either e‚àíŒ≤l ‚àíeŒ≤l = 0 or cos Œ±l = 0, and hence
either Œ≤ = 0 or Œ±l =

n + 1
2

œÄ for n an integer. The latter leaves the second of
equations (4.4) with no solution, so we must have Œ≤ = 0, and hence sin Œ±l = 0. This
gives Œ±l = nœÄ with n an integer, which means that Œª1/2 is real, as expected, with
Œª1/2 = nœÄ/l and y = An

einœÄx/l ‚àíe‚àíinœÄx/l
= 2iAn sin (nœÄx/l). To ensure that y,
the displacement of the string, is real, we write An = an/2i for an real, which gives
y = an sin
nœÄx
l

for n = 1, 2, . . . .
(4.5)
Note that the solution that corresponds to n = 0 is the trivial solution, y = 0.
The values of Œª for which there is a nontrivial solution of this problem, namely
Œª = n2œÄ2/l2, are called the eigenvalues of the boundary value problem, whilst the
corresponding solutions, y = sin Œª1/2x, are the eigenfunctions. Note that there is
an inÔ¨Ånite sequence of eigenvalues, which are real and have magnitudes that tend

96
BOUNDARY VALUE PROBLEMS
to inÔ¨Ånity as n tends to inÔ¨Ånity. In terms of the physical problem, for a string of
given line density œÅ, length l and tension T, a nonequilibrium steady motion is only
possible at a discrete set of angular velocities, the eigenfrequencies,
œâ = nœÄ
l
+
T
œÅ .
4.1
Inhomogeneous Linear Boundary Value Problems
Continuing with our example of a rotating string, let‚Äôs consider what happens
when there is a steady, imposed external force TF(x) acting towards the x-axis.
The linearized boundary value problem is then
d 2y
dx2 + Œªy = F(x),
subject to y(0) = y(l) = 0.
(4.6)
We can solve this using the variation of parameters formula, (1.6), which gives
y(x) = A cos Œª1/2x + B sin Œª1/2x +
1
Œª1/2
 x
0
F(s) sin Œª1/2(x ‚àís) ds.
To satisfy the boundary condition y(0) = 0 we need A = 0, whilst y(l) = 0 gives
B sin Œª1/2l +
1
Œª1/2
 l
0
F(s) sin Œª1/2(x ‚àís) ds = 0.
(4.7)
When Œª is not an eigenvalue, sin Œª1/2l Ã∏= 0, and (4.7) has a unique solution
B = ‚àí
1
Œª1/2 sin Œª1/2l
 l
0
F(s) sin Œª1/2(x ‚àís) ds.
However, when Œª is an eigenvalue, we have sin Œª1/2l = 0, so that there is a solution
for arbitrary values of B, namely
y(x) = B sin Œª1/2x +
1
Œª1/2
 x
0
F(s) sin Œª1/2(x ‚àís) ds,
(4.8)
provided that
 l
0
F(s) sin Œª1/2(l ‚àís) ds = 0.
(4.9)
If F(s) does not satisfy this integral constraint there is no solution.
These cases, where there may be either no solution or many solutions depending
on the form of F(x), are in complete contrast to the solutions of an initial value
problem, for which, as we shall see in Chapter 8, we can prove theorems that guar-
antee the existence and uniqueness of solutions, subject to a few mild conditions
on the form of the ordinary diÔ¨Äerential equation. This situation also arises for any
system that can be written in the form Ax = b, where A is a linear operator. A fa-
mous result (see, for example, Courant and Hilbert, 1937) known as the Fredholm
alternative, shows that either there is a unique solution of Ax = b, or Ax = 0
has nontrivial solutions.

4.1 INHOMOGENEOUS LINEAR BOUNDARY VALUE PROBLEMS
97
In terms of the physics of the rotating string problem, if œâ is not an eigenfre-
quency, for arbitrary forcing of the string there is a unique steady solution. If œâ
is an eigenfrequency and the forcing satisÔ¨Åes (4.9), there is a solution, (4.8), that
is a linear combination of the eigensolution and the response to the forcing. The
size of B depends upon how the steady state was reached, just as for the unforced
problem. If œâ is an eigenfrequency and the forcing does not satisfy (4.9) there is no
steady solution. This reÔ¨Çects the fact that the forcing has a component that drives
a response at the eigenfrequency. We say that there is a resonance. In practice, if
the string were forced in this way, the amplitude of the motion would grow linearly,
whilst varying spatially like sin Œª1/2x, until the nonlinear terms in (4.1) were no
longer negligible.
4.1.1
Solubility
As we have now seen, an inhomogeneous, linear boundary value problem may
have no solutions. Let‚Äôs examine this further for the general boundary value prob-
lem
(p(x)y‚Ä≤(x))‚Ä≤ + q(x)y(x) = f(x) subject to y(a) = y(b) = 0.
(4.10)
If u(x) is a solution of the homogeneous problem, so that (p(x)u‚Ä≤(x))‚Ä≤+q(x)u(x) = 0
and u(a) = u(b) = 0, then multiplying (4.10) by u(x) and integrating over the
interval [a, b] gives
 b
a
u(x)

(p(x)y‚Ä≤(x))‚Ä≤ + q(x)y(x)

dx =
 b
a
u(x)f(x) dx.
(4.11)
Now, using integration by parts,
 b
a
u(x) (p(x)y‚Ä≤(x))‚Ä≤ dx = [u(x)p(x)y‚Ä≤(x)]b
a ‚àí
 b
a
p(x)y‚Ä≤(x)u‚Ä≤(x) dx
= ‚àí
 b
a
p(x)y‚Ä≤(x)u‚Ä≤(x) dx,
using u(a) = u(b) = 0. Integrating by parts again gives
‚àí
 b
a
p(x)y‚Ä≤(x)u‚Ä≤(x) dx
= ‚àí[p(x)u‚Ä≤(x)y(x)]b
a +
 b
a
(p(x)u‚Ä≤(x))‚Ä≤ y(x) dx =
 b
a
(p(x)u‚Ä≤(x))‚Ä≤ y(x) dx,
using y(a) = y(b) = 0. Substituting this into (4.11) gives
 b
a
y(x)

(p(x)u‚Ä≤(x))‚Ä≤ + q(x)u(x)

dx =
 b
a
u(x)f(x) dx,
and, since u(x) is a solution of the homogeneous problem,
 b
a
u(x)f(x) dx = 0.
(4.12)

98
BOUNDARY VALUE PROBLEMS
A necessary condition for there to be a solution of the inhomogeneous boundary
value problem (4.10) is therefore (4.12), which we call a solvability or solubility
condition. We say that the forcing term, f(x), must be orthogonal to the solution
of the homogeneous problem, a terminology that we will explore in more detail in
Section 4.2.3. Note that variations in the form of the boundary conditions will give
some variation in the form of the solvability condition.
4.1.2
The Green‚Äôs Function
Let‚Äôs now solve (4.10) using the variation of parameters formula, (1.6). This
gives
y(x) = Au1(x) + Bu2(x) +
 x
s=a
f(s)
W(s) {u1(s)u2(x) ‚àíu1(x)u2(s)} ds,
(4.13)
where u1(x) and u2(x) are solutions of the homogeneous problem and W(x) =
u1(x)u‚Ä≤
2(x)‚àíu‚Ä≤
1(x)u2(x) is the Wronskian of the homogeneous equation. The bound-
ary conditions show that
Au1(a) + Bu2(a) = 0,
Au1(b) + Bu2(b) =
 b
a
f(s)
W(s) {u1(b)u2(s) ‚àíu1(s)u2(b)} ds.
Provided that u1(a)u2(b) Ã∏= u1(b)u2(a), we can solve these simultaneous equations
and substitute back into (4.13) to obtain
y(x) =
 b
a
f(s)
W(s)
{u1(b)u2(s) ‚àíu1(s)u2(b)} {u1(a)u2(x) ‚àíu1(x)u2(a)}
u1(a)u2(b) ‚àíu1(b)u2(a)
ds
+
 x
a
f(s)
W(s) {u1(s)u2(x) ‚àíu1(x)u2(s)} ds.
(4.14)
This form of solution, although correct, is not the most convenient one to use.
To improve it, we note that the functions v1(x) = u1(a)u2(x) ‚àíu1(x)u2(a) and
v2(x) = u1(b)u2(x) ‚àíu1(x)u2(b), which appear in (4.14) as a product, are linear
combinations of solutions of the homogeneous problem, and are therefore them-
selves solutions of the homogeneous problem. They also satisfy v1(a) = v2(b) = 0.
Because of the way that they appear in (4.14), it makes sense to look for a solution
of the inhomogeneous boundary value problem in the form
y(x) =
 b
a
f(s)G(x, s) ds,
(4.15)
where
G(x, s) =
 v1(s)v2(x) = G<(x, s)
for a ‚©Ωs < x,
v1(x)v2(s) = G>(x, s)
for x < s ‚©Ωb.
The function G(x, s) is known as the Green‚Äôs function for the boundary value
problem. From the deÔ¨Ånition, it is clear that G is continuous at s = x, and that

4.1 INHOMOGENEOUS LINEAR BOUNDARY VALUE PROBLEMS
99
y(a) = y(b) = 0, since, for example, G>(a, s) = 0 and y(a) =
' b
a G>(a, s)f(s) ds =
0. If we calculate the partial derivative with respect to x, we Ô¨Ånd that
Gx(x, s = x‚àí) ‚àíGx(x, s = x+) = v1(x)v‚Ä≤
2(x) ‚àív‚Ä≤
1(x)v2(x) = W =
C
p(x),
where, using Abel‚Äôs formula, C is a constant.
For this to be useful, we must now show that y(x), as deÔ¨Åned by (4.15), actually
satisÔ¨Åes the inhomogeneous diÔ¨Äerential equation, and also determine the value of
C. To do this, we split the range of integration and write
y(x) =
 x
a
G<(x, s)f(s) ds +
 b
x
G>(x, s)f(s) ds.
If we now diÔ¨Äerentiate under the integral sign, we obtain
y‚Ä≤(x) =
 x
a
G<,x(x, s)f(s) ds + G<(x, x)f(x) +
 b
x
G>,x(x, s)f(s) ds ‚àíG>(x, x)f(x),
where G<,x = ‚àÇG</‚àÇx. This simpliÔ¨Åes, by virtue of the continuity of the Green‚Äôs
function at x = s, to give
y‚Ä≤(x) =
 x
a
G<x(x, s)f(s) ds +
 b
x
G>x(x, s)f(s) ds
=
 x
a
v1(s)v2x(x)f(s) ds +
 b
x
v1x(x)v2(s)f(s) ds,
and hence
(py‚Ä≤)‚Ä≤ =
 x
a
v1(s)(pv2x)xf(s) ds + p(x)v1(x)v2x(x)f(x)
+
 b
x
(pv1x)xv2(s)f(s) ds ‚àíp(x)v1x(x)v2(x)f(x)
=
 x
a
v1(s)(pv2x)xf(s) ds +
 b
x
(pv1x)xv2(s)f(s) ds + Cf(x),
using the deÔ¨Ånition of C. If we substitute this into the diÔ¨Äerential equation (4.10),
(py‚Ä≤)‚Ä≤ + qy = f, we obtain
 x
a
v1(s)(pv2x)xf(s) ds +
 b
x
(pv1x)xv2(s)f(s) ds + Cf(x)
+q(x)
 x
a
v1(s)v2(x)f(s) ds +
 b
x
v1(x)v2(s)f(s) ds

= f(x).
Since v1 and v2 are solutions of the homogeneous problem, the integral terms vanish
and, if we choose C = 1, our representation provides us with a solution of the
diÔ¨Äerential equation.

100
BOUNDARY VALUE PROBLEMS
As an example, consider the boundary value problem
y‚Ä≤‚Ä≤(x) ‚àíy(x) = f(x) subject to y(0) = y(1) = 0.
The solutions of the homogeneous problem are e‚àíx and ex. Appropriate combi-
nations of these that satisfy v1(0) = v2(1) = 0 are v1(x) = A sinh x and v2(x) =
B sinh(1 ‚àíx), which gives the Green‚Äôs function
G(x, s) =
 AB sinh(1 ‚àíx) sinh s
for 0 ‚©Ωs < x,
AB sinh(1 ‚àís) sinh x
for x < s ‚©Ω1,
which is continuous at s = x. In addition,
Gx(x, s = x‚àí) ‚àíGx(x, s = x+)
= ‚àíAB cosh(1 ‚àíx) sinh x ‚àíAB sinh(1 ‚àíx) cosh x = ‚àíAB sinh 1.
Since p(x) = 1, we require AB = ‚àí1/ sinh 1, and the Ô¨Ånal Green‚Äôs function is
G(x, s) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚àísinh(1 ‚àíx) sinh s
sinh 1
for 0 ‚©Ωs < x,
‚àísinh(1 ‚àís) sinh x
sinh 1
for x < s ‚©Ω1.
The solution of the inhomogeneous boundary value problem can therefore be written
as
y(x) = ‚àí
 x
0
f(s)sinh(1 ‚àíx) sinh s
sinh 1
ds ‚àí
 1
x
f(s)sinh(1 ‚àís) sinh x
sinh 1
ds.
We will return to the subject of Green‚Äôs functions in the next chapter.
4.2
The Solution of Boundary Value Problems by Eigenfunction
Expansions
In the previous section, we developed the idea of a Green‚Äôs function, with which we
can solve inhomogeneous boundary value problems for linear, second order ordinary
diÔ¨Äerential equations. We will now develop an alternative approach that draws
heavily upon the ideas of linear algebra (see Appendix 1 for a reminder). Before
we start, it is useful to be able to work with the simplest possible type of linear
diÔ¨Äerential operator.
4.2.1
Self-Adjoint Operators
We deÔ¨Åne a linear diÔ¨Äerential operator, L : C2[a, b] ‚ÜíC[a, b], as being in self-
adjoint form if
L = d
dx

p(x) d
dx

+ q(x),
(4.16)

4.2 EIGENFUNCTION EXPANSIONS
101
where p(x) ‚ààC1[a, b] and is strictly nonzero for all x ‚àà(a, b), and q(x) ‚ààC[a, b].
The reasons for referring to such an operator as self-adjoint will become clear later
in this chapter.
This deÔ¨Ånition encompasses a wide class of second order diÔ¨Äerential operators.
For example, if
L1 ‚â°a2(x) d 2
dx2 + a1(x) d
dx + a0(x)
(4.17)
is nonsingular on [a, b], we can write it in self-adjoint form by deÔ¨Åning (see Exer-
cise 4.5)
p(x) = exp
 x a1(t)
a2(t) dt

,
q(x) = a0(x)
a2(x) exp
 x a1(t)
a2(t) dt

.
(4.18)
Note that p(x) Ã∏= 0 for x ‚àà[a, b]. By studying inhomogeneous boundary value
problems of the form Ly = f, or
d
dx

p(x)dy
dx

+ q(x)y = f(x),
(4.19)
we are therefore considering all second order, nonsingular, linear diÔ¨Äerential oper-
ators. For example, consider Hermite‚Äôs equation,
d 2y
dx2 ‚àí2xdy
dx + Œªy = 0,
(4.20)
for ‚àí‚àû< x < ‚àû. This is not in self-adjoint form, but, if we follow the above
procedure, the self-adjoint form of the equation is
d
dx

e‚àíx2 dy
dx

+ Œªe‚àíx2y = 0.
This can be simpliÔ¨Åed, and kept in self-adjoint form, by writing u = e‚àíx2/2y, to
obtain
d 2u
dx2 ‚àí(x2 ‚àí1)u = ‚àíŒªu.
(4.21)
4.2.2
Boundary Conditions
To complete the deÔ¨Ånition of a boundary value problem associated with (4.19),
we need to know the boundary conditions. In general these will be of the form
Œ±1y(a) + Œ±2y(b) + Œ±3y‚Ä≤(a) + Œ±4y‚Ä≤(b) = 0,
Œ≤1y(a) + Œ≤2y(b) + Œ≤3y‚Ä≤(a) + Œ≤4y‚Ä≤(b) = 0.
(4.22)
Since each of these is dependent on the values of y and y‚Ä≤ at each end of [a, b],
we refer to these as mixed or coupled boundary conditions. It is unnecessarily
complicated to work with the boundary conditions in this form, and we can start
to simplify matters by deriving Lagrange‚Äôs identity.

102
BOUNDARY VALUE PROBLEMS
Lemma 4.1 (Lagrange‚Äôs identity) If L is the linear diÔ¨Äerential operator given
by (4.16) on [a, b], and if y1, y2 ‚ààC2[a, b], then
y1 (Ly2) ‚àíy2 (Ly1) = [ p (y1y‚Ä≤
2 ‚àíy‚Ä≤
1y2)]‚Ä≤ .
(4.23)
Proof From the deÔ¨Ånition of L,
y1 (Ly2) ‚àíy2 (Ly1) = y1
(
(py‚Ä≤
2)‚Ä≤ + qy2
)
‚àíy2
(
(py‚Ä≤
1)‚Ä≤ + qy1
)
= y1 (py‚Ä≤
2)‚Ä≤ ‚àíy2 (py‚Ä≤
1)‚Ä≤ = y1 (py‚Ä≤‚Ä≤
2 + p‚Ä≤y‚Ä≤
2) ‚àíy2 (py‚Ä≤‚Ä≤
1 + p‚Ä≤y‚Ä≤
1)
= p‚Ä≤ (y1y‚Ä≤
2 ‚àíy‚Ä≤
1y2) + p (y1y‚Ä≤‚Ä≤
2 ‚àíy‚Ä≤‚Ä≤
1y2) = [ p (y1y‚Ä≤
2 ‚àíy‚Ä≤
1y2)]‚Ä≤ .
Now recall that the space C[a, b] is a real inner product space with a standard
inner product deÔ¨Åned by
‚ü®f, g‚ü©=
 b
a
f(x)g(x) dx.
If we now integrate (4.23) over [a, b] then
‚ü®y1, Ly2‚ü©‚àí‚ü®Ly1, y2‚ü©= [ p (y1y‚Ä≤
2 ‚àíy‚Ä≤
1y2)]b
a .
(4.24)
This result can be used to motivate the following deÔ¨Ånitions. The adjoint operator
to T, written ¬ØT, satisÔ¨Åes ‚ü®y1, Ty2‚ü©= ‚ü®¬ØTy1, y2‚ü©for all y1 and y2. For example, let‚Äôs
see if we can construct the adjoint to the operator
D ‚â°d 2
dx2 + Œ≥ d
dx + Œ¥,
with Œ≥, Œ¥ ‚ààR, on the interval [0, 1], when the functions on which D operates are
zero at x = 0 and x = 1. After integrating by parts and applying these boundary
conditions, we Ô¨Ånd that
‚ü®œÜ1, DœÜ2‚ü©=
 1
0
œÜ1 (œÜ‚Ä≤‚Ä≤
2 + Œ≥œÜ‚Ä≤
2 + Œ¥œÜ2) dx =
(
œÜ1œÜ‚Ä≤
2
)1
0 ‚àí
 1
0
œÜ‚Ä≤
1œÜ‚Ä≤
2 dx
+
(
Œ≥œÜ1œÜ2
)1
0 ‚àí
 1
0
Œ≥œÜ‚Ä≤
1œÜ2 dx +
 1
0
Œ¥œÜ1œÜ2 dx
= ‚àí
(
œÜ‚Ä≤
1œÜ2
)1
0 +
 1
0
œÜ‚Ä≤‚Ä≤
1œÜ2 dx ‚àí
 1
0
Œ≥œÜ‚Ä≤
1œÜ2 dx +
 1
0
Œ¥œÜ1œÜ2 dx = ‚ü®¬ØDœÜ1, œÜ2‚ü©,
where
¬ØD ‚â°d 2
dx2 ‚àíŒ≥ d
dx + Œ¥.

4.2 EIGENFUNCTION EXPANSIONS
103
A linear operator is said to be Hermitian, or self-adjoint, if ‚ü®y1, Ty2‚ü©=
‚ü®Ty1, y2‚ü©for all y1 and y2. It is clear from (4.24) that L is a Hermitian, or self-
adjoint, operator if and only if
(
p (y1y‚Ä≤
2 ‚àíy‚Ä≤
1y2)
)b
a = 0,
and hence
p(b) {y1(b)y‚Ä≤
2(b) ‚àíy‚Ä≤
1(b)y2(b)} ‚àíp(a) {y1(a)y‚Ä≤
2(a) ‚àíy‚Ä≤
1(a)y2(a)} = 0.
(4.25)
In other words, whether or not L is Hermitian depends only upon the boundary
values of the functions in the space upon which it operates.
There are three diÔ¨Äerent ways in which (4.25) can occur.
(i) p(a) = p(b) = 0. Note that this doesn‚Äôt violate our deÔ¨Ånition of p as strictly
nonzero on the open interval (a, b). This is the case of singular boundary
conditions.
(ii) p(a) = p(b) Ã∏= 0, yi(a) = yi(b) and y‚Ä≤
i(a) = y‚Ä≤
i(b). This is the case of periodic
boundary conditions.
(iii) Œ±1yi(a) + Œ±2y‚Ä≤
i(a) = 0 and Œ≤1yi(b) + Œ≤2y‚Ä≤
i(b) = 0, with at least one of the Œ±i
and one of the Œ≤i nonzero. These conditions then have nontrivial solutions
if and only if
y1(a)y‚Ä≤
2(a) ‚àíy‚Ä≤
1(a)y2(a) = 0,
y1(b)y‚Ä≤
2(b) ‚àíy‚Ä≤
1(b)y2(b) = 0,
and hence (4.25) is satisÔ¨Åed.
Conditions (iii), each of which involves y and y‚Ä≤ at a single endpoint, are called
unmixed or separated.
We have therefore shown that our linear diÔ¨Äerential
operator is Hermitian with respect to a pair of unmixed boundary conditions. The
signiÔ¨Åcance of this result becomes apparent when we examine the eigenvalues and
eigenfunctions of Hermitian linear operators.
As an example of how such boundary conditions arise when we model physical
systems, consider a string that is rotating (as in the example at the start of this
chapter) or vibrating with its ends Ô¨Åxed. This leads to boundary conditions y(0) =
y(a) = 0 ‚Äì separated boundary conditions. In the study of the motion of electrons
in a crystal lattice, the periodic conditions p(0) = p(l), y(0) = y(l) are frequently
used to represent the repeating structure of the lattice.
4.2.3
Eigenvalues and Eigenfunctions of Hermitian Linear
Operators
The eigenvalues and eigenfunctions of a Hermitian, linear operator L are the
nontrivial solutions of Ly = Œªy subject to appropriate boundary conditions.
Theorem 4.1 Eigenfunctions belonging to distinct eigenvalues of a Hermitian lin-
ear operator are orthogonal.

104
BOUNDARY VALUE PROBLEMS
Proof Let y1 and y2 be eigenfunctions that correspond to the distinct eigenvalues
Œª1 and Œª2. Then
‚ü®Ly1, y2‚ü©= ‚ü®Œª1y1, y2‚ü©= Œª1‚ü®y1, y2‚ü©,
and
‚ü®y1, Ly2‚ü©= ‚ü®y1, Œª2y2‚ü©= Œª2‚ü®y1, y2‚ü©,
so that the Hermitian property ‚ü®Ly1, y2‚ü©= ‚ü®y1, Ly2‚ü©gives
(Œª1 ‚àíŒª2) ‚ü®y1, y2‚ü©= 0.
Since Œª1 Ã∏= Œª2, ‚ü®y1, y2‚ü©= 0, and y1 and y2 are orthogonal.
As we shall see in the next section, all of the eigenvalues of a Hermitian linear
operator are real, a result that we will prove once we have deÔ¨Åned the notion of a
complex inner product.
If the space of functions C2[a, b] were of Ô¨Ånite dimension, we would now argue
that the orthogonal eigenfunctions generated by a Hermitian operator are linearly
independent and can be used as a basis (or in the case of repeated eigenvalues,
extended into a basis). Unfortunately, C2[a, b] is not Ô¨Ånite dimensional, and we
cannot use this argument. We will have to content ourselves with presenting a
credible method for solving inhomogeneous boundary value problems based upon
the ideas we have developed, and simply state a theorem that guarantees that the
method will work in certain circumstances.
4.2.4
Eigenfunction Expansions
In order to solve the inhomogeneous boundary value problem given by (4.19) with
f ‚ààC[a, b] and unmixed boundary conditions, we begin by Ô¨Ånding the eigenvalues
and eigenfunctions of L. We denote these eigenvalues by Œª1, Œª2, . . . , Œªn, . . . , and
the eigenfunctions by œÜ1(x), œÜ2(x), . . . , œÜn(x), . . . . Next, we expand f(x) in terms
of these eigenfunctions, as
f(x) =
‚àû

n=1
cnœÜn(x).
(4.26)
By making use of the orthogonality of the eigenfunctions, after taking the inner
product of (4.26) with œÜn, we Ô¨Ånd that the expansion coeÔ¨Écients are
cn = ‚ü®f, œÜn‚ü©
‚ü®œÜn, œÜn‚ü©.
(4.27)
Next, we expand the solution of the boundary value problem in terms of the eigen-
functions, as
y(x) =
‚àû

n=1
dnœÜn(x),
(4.28)

4.2 EIGENFUNCTION EXPANSIONS
105
and substitute (4.27) and (4.28) into (4.19) to obtain
L
 ‚àû

n=1
dnœÜn(x)

=
‚àû

n=1
cnœÜn(x).
From the linearity of L and the deÔ¨Ånition of œÜn this becomes
‚àû

n=1
dnŒªnœÜn(x) =
‚àû

n=1
cnœÜn(x).
We have therefore constructed a solution of the boundary value problem with dn =
cn/Œªn, if the series (4.28) converges and deÔ¨Ånes a function in C2[a, b]. This process
will work correctly and give a unique solution provided that none of the eigenvalues
Œªn is zero. When Œªm = 0, there is no solution if cm Ã∏= 0 and an inÔ¨Ånite number of
solutions if cm = 0, as we saw in Section 4.1.
Example
Consider the boundary value problem
‚àíy‚Ä≤‚Ä≤ = f(x) subject to y(0) = y(œÄ) = 0.
(4.29)
In this case, the eigenfunctions are solutions of
y‚Ä≤‚Ä≤ + Œªy = 0 subject to y(0) = y(œÄ) = 0,
which we already know to be Œªn = n2, œÜn(x) = sin nx. We therefore write
f(x) =
‚àû

n=1
cn sin nx,
and the solution of the inhomogeneous problem (4.29) is
y(x) =
‚àû

n=1
cn
n2 sin nx.
In the case f(x) = x,
cn =
' œÄ
0 x sin nx dx
' œÄ
0 sin2 nx dx = 2(‚àí1)n+1
n
,
so that
y(x) = 2
‚àû

n=1
(‚àí1)n+1
n3
sin nx.
We will discuss the convergence of this type of series, known as a Fourier series, in
detail in Chapter 5.
This example is, of course, rather artiÔ¨Åcial, and we could have integrated (4.29)
directly. There are, however, many boundary value problems for which this eigen-
function expansion method is the only way to proceed analytically, such as the
example given in Section 3.9.1 on Bessel functions.

106
BOUNDARY VALUE PROBLEMS
Example
Consider the inhomogeneous equation
(1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy + 2y = f(x) on ‚àí1 < x < 1,
(4.30)
with f ‚ààC[‚àí1, 1], subject to the condition that y should be bounded on [‚àí1, 1].
We begin by noting that there is a solubility condition associated with this problem.
If u(x) is a solution of the homogeneous problem, then, after multiplying through
by u and integrating over [‚àí1, 1], we Ô¨Ånd that
!
u(1 ‚àíx2)y‚Ä≤"1
‚àí1 ‚àí
!
u‚Ä≤(1 ‚àíx2)y
"1
‚àí1 =
 1
‚àí1
u(x)f(x) dx.
If u and y are bounded on [‚àí1, 1], the left hand side of this equation vanishes, so
that
' 1
‚àí1 u(x)f(x) dx = 0. Since the Legendre polynomial, u = P1(x) = x, is the
bounded solution of the homogeneous problem, we have
 1
‚àí1
P1(x)f(x) dx = 0.
Now, to solve the boundary value problem, we Ô¨Årst construct the eigenfunction
solutions by solving Ly = Œªy, which is
(1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + (2 ‚àíŒª)y = 0.
The choice 2‚àíŒª = n(n+1), with n a positive integer, gives us Legendre‚Äôs equation
of integer order, which has bounded solutions yn(x) = Pn(x).
These Legendre
polynomials are orthogonal over [‚àí1, 1] (as we shall show in Theorem 4.4), and
form a basis for C[‚àí1, 1]. If we now write
f(x) =
‚àû

m=0
AmPm(x),
where A1 = 0 by the solubility condition, and then expand y(x) =  ‚àû
m=0 BmPm(x),
we Ô¨Ånd that
{2 ‚àím(m + 1)} Bm = Am for m ‚©æ0.
The required solution is therefore
y(x) = 1
2A0 + B1P1(x) +
‚àû

m=2
Am
2 ‚àím(m + 1)Pm(x),
with B1 an arbitrary constant.
Having seen that this method works, we can now state a theorem that gives the
method a rigorous foundation.
Theorem 4.2 If L is a nonsingular, linear diÔ¨Äerential operator deÔ¨Åned on a closed
interval [a, b] and subject to unmixed boundary conditions at both endpoints, then

4.3 STURM‚ÄìLIOUVILLE SYSTEMS
107
(i) L has an inÔ¨Ånite sequence of real eigenvalues Œª0, Œª1, . . . , which can be or-
dered so that
|Œª0| < |Œª1| < ¬∑ ¬∑ ¬∑ < |Œªn| < ¬∑ ¬∑ ¬∑
and
lim
n‚Üí‚àû|Œªn| = ‚àû.
(ii) The eigenfunctions that correspond to these eigenvalues form a basis for
C[a, b], and the series expansion relative to this basis of a piecewise con-
tinuous function y with piecewise continuous derivative on [a, b] converges
uniformly to y on any subinterval of [a, b] in which y is continuous.
We will not prove this result here.‚Ä† Instead, we return to the equation, Ly = Œªy,
which deÔ¨Ånes the eigenfunctions and eigenvalues. For a self-adjoint, second order,
linear diÔ¨Äerential operator, this is
d
dx

p(x)dy
dx

+ q(x)y = Œªy,
(4.31)
which, in its simplest form, is subject to the unmixed boundary conditions
Œ±1y(a) + Œ±2y‚Ä≤(a) = 0,
Œ≤1y(b) + Œ≤2y‚Ä≤(b) = 0,
(4.32)
with Œ±2
1 + Œ±2
2 > 0 and Œ≤2
1 + Œ≤2
2 > 0 to avoid a trivial condition. This is an example
of a Sturm‚ÄìLiouville system, and we will devote the rest of this chapter to a
study of the properties of the solutions of such systems.
4.3
Sturm‚ÄìLiouville Systems
In the Ô¨Årst three chapters, we have studied linear second order diÔ¨Äerential equations.
After examining some solution techniques that are applicable to such equations in
general, we studied the particular cases of Legendre‚Äôs equation and Bessel‚Äôs equa-
tion, since they frequently arise in models of physical systems in spherical and
cylindrical geometries. We saw that, in each case, we can construct a set of orthog-
onal solutions that can be used as the basis for a series expansion of the solution of
the physical problem in question, namely the Fourier‚ÄìLegendre and Fourier‚ÄìBessel
series. In this chapter we will see that Legendre‚Äôs and Bessel‚Äôs equations are exam-
ples of Sturm‚ÄìLiouville equations, and that we can deduce many properties of
such equations independent of the functional form of the coeÔ¨Écients.
4.3.1
The Sturm‚ÄìLiouville Equation
Sturm‚ÄìLiouville equations are of the form
(p(x)y‚Ä≤(x))‚Ä≤ + q(x)y(x) = ‚àíŒªr(x)y(x),
(4.33)
‚Ä† For a proof see Ince (1956).

108
BOUNDARY VALUE PROBLEMS
which can be written more concisely as
Sy(x, Œª) = ‚àíŒªr(x)y(x, Œª),
(4.34)
where the diÔ¨Äerential operator S is deÔ¨Åned as
SœÜ ‚â°d
dx

p(x)dœÜ
dx

+ q(x)œÜ.
(4.35)
This is a slightly more general equation than (4.31). In (4.33), the number Œª is
the eigenvalue, whose possible values, which may be complex, are critically depen-
dent upon the given boundary conditions. It is often more important to know the
properties of Œª than it is to construct the actual solutions of (4.33).
We seek to solve the Sturm‚ÄìLiouville equation, (4.33), on an open interval, (a, b),
of the real line. We will also make some assumptions about the behaviour of the
coeÔ¨Écients of (4.33) for x ‚àà(a, b), namely that
(i)
p(x), q(x) and r(x) are real-valued and continuous,
(ii)
p(x) is diÔ¨Äerentiable,
(iii)
p(x) > 0 and r(x) > 0.
(4.36)
Some Examples of Sturm‚ÄìLiouville Equations
Perhaps the simplest example of a Sturm‚ÄìLiouville equation is Fourier‚Äôs equa-
tion,
y‚Ä≤‚Ä≤(x, Œª) = ‚àíŒªy(x, Œª),
(4.37)
which has solutions cos(x
‚àö
Œª) and sin(x
‚àö
Œª). We discussed a physical problem that
leads naturally to Fourier‚Äôs equation at the start of this chapter, and we will meet
another at the beginning of Chapter 5.
We can write Legendre‚Äôs equation and Bessel‚Äôs equation as Sturm‚ÄìLiouville prob-
lems. Recall that Legendre‚Äôs equation is
d 2y
dx2 ‚àí
2x
1 ‚àíx2
dy
dx +
Œª
1 ‚àíx2 y = 0,
and we are usually interested in solving this for ‚àí1 < x < 1. This can be written
as
((1 ‚àíx2)y‚Ä≤)‚Ä≤ = ‚àíŒªy.
If Œª = n(n + 1), we showed in Chapter 2 that this has solutions Pn(x) and Qn(x).
Similarly, Bessel‚Äôs equation, which is usually solved for 0 < x < a, is
x2y‚Ä≤‚Ä≤ + xy‚Ä≤ + (Œªx2 ‚àíŒΩ2)œÜ = 0.
This can be rearranged into the form
(xy‚Ä≤)‚Ä≤ ‚àíŒΩ2
x y = ‚àíŒªxy.
Again, from the results of Chapter 3, we know that this has solutions of the form
JŒΩ(x
‚àö
Œª) and YŒΩ(x
‚àö
Œª).

4.3 STURM‚ÄìLIOUVILLE SYSTEMS
109
Although the Sturm‚ÄìLiouville forms of these equations may look more cumber-
some than the original forms, we will see that they are very convenient for the
analysis that follows. This is because of the self-adjoint nature of the diÔ¨Äerential
operator.
4.3.2
Boundary Conditions
We begin with a couple of deÔ¨Ånitions. The endpoint, x = a, of the interval (a, b)
is a regular endpoint if a is Ô¨Ånite and the conditions (4.36) hold on the closed
interval [a, c] for each c ‚àà(a, b). The endpoint x = a is a singular endpoint if
a = ‚àí‚àûor if a is Ô¨Ånite but the conditions (4.36) do not hold on the closed interval
[a, c] for some c ‚àà(a, b). Similar deÔ¨Ånitions hold for the other endpoint, x = b. For
example, Fourier‚Äôs equation has regular endpoints if a and b are Ô¨Ånite. Legendre‚Äôs
equation has regular endpoints if ‚àí1 < a < b < 1, but singular endpoints if a = ‚àí1
or b = 1, since p(x) = 1 ‚àíx2 = 0 when x = ¬±1. Bessel‚Äôs equation has regular
endpoints for 0 < a < b < ‚àû, but singular endpoints if a = 0 or b = ‚àû, since
q(x) = ‚àíŒΩ2/x is unbounded at x = 0.
We can now deÔ¨Åne the types of boundary condition that can be applied to a
Sturm‚ÄìLiouville equation.
(i) On a Ô¨Ånite interval, [a, b], with regular endpoints, we prescribe unmixed, or
separated, boundary conditions, of the form
Œ±0y(a, Œª) + Œ±1y‚Ä≤(a, Œª) = 0,
Œ≤0y(b, Œª) + Œ≤1y‚Ä≤(b, Œª) = 0.
(4.38)
These boundary conditions are said to be real if the constants Œ±0, Œ±1, Œ≤0
and Œ≤1 are real, with Œ±2
0 + Œ±2
1 > 0 and Œ≤2
0 + Œ≤2
1 > 0.
(ii) On an interval with one or two singular endpoints, the boundary conditions
that arise in models of physical problems are usually boundedness condi-
tions. In many problems, these are equivalent to Friedrichs boundary
conditions, that for some c ‚àà(a, b) there exists A ‚ààR+ such that
|y(x, Œª)| ‚©ΩA for all x ‚àà(a, c],
and similarly if the other endpoint, x = b, is singular, there exists B ‚ààR+
such that
|y(x, Œª)| ‚©ΩB for all x ‚àà[c, b).
We can now deÔ¨Åne the Sturm‚ÄìLiouville boundary value problem to be the
Sturm‚ÄìLiouville equation,
(p(x)y‚Ä≤(x))‚Ä≤ + q(x)y(x) = ‚àíŒªr(x)y(x) for x ‚àà(a, b),
where the coeÔ¨Écient functions satisfy the conditions (4.36), to be solved subject to a
separated boundary condition at each regular endpoint and a Friedrichs boundary
condition at each singular endpoint.
Note that this boundary value problem is
homogeneous and therefore always has the trivial solution, y = 0. A nontrivial
solution, y(x, Œª) Ã∏‚â°0, is an eigenfunction, and Œª is the corresponding eigenvalue.

110
BOUNDARY VALUE PROBLEMS
Some Examples of Sturm‚ÄìLiouville Boundary Value Problems
Consider Fourier‚Äôs equation,
y‚Ä≤‚Ä≤(x, Œª) = ‚àíŒªy(x, Œª) for x ‚àà(0, 1),
subject to the boundary conditions y(0, Œª) = y(1, Œª) = 0, which are appropriate
since both endpoints are regular. The eigenfunctions of this system are sin ‚àöŒªnx
for n = 1, 2, . . . , with corresponding eigenvalues Œª = Œªn = n2œÄ2.
Legendre‚Äôs equation is

(1 ‚àíx2)y‚Ä≤(x, Œª)
‚Ä≤ = ‚àíŒªy(x, Œª) for x ‚àà(‚àí1, 1).
Note that this is singular at both endpoints, since p(¬±1) = 0. We therefore apply
Friedrichs boundary conditions, for example with c = 0, in the form
|y(x, Œª)| ‚©ΩA for x ‚àà(‚àí1, 0], |y(x, Œª)| ‚©ΩB for x ‚àà[0, 1),
for some A, B ‚ààR+. In Chapter 2 we used the method of Frobenius to construct the
solutions of Legendre‚Äôs equation, and we know that the only eigenfunctions bounded
at both the endpoints are the Legendre polynomials, Pn(x) for n = 0, 1, 2, . . . , with
corresponding eigenvalues Œª = Œªn = n(n + 1).
Let‚Äôs now consider Bessel‚Äôs equation with ŒΩ = 1, over the interval (0, 1),
(xy‚Ä≤)‚Ä≤ ‚àíy
x = ‚àíŒªxy.
Because of the form of q(x), x = 0 is a singular endpoint, whilst x = 1 is a regular
endpoint. Suitable boundary conditions are therefore
|y(x, Œª)| ‚©ΩA for x ‚àà

0, 1
2
"
, y(1, Œª) = 0,
for some A ‚ààR+. In Chapter 3 we constructed the solutions of this equation using
the method of Frobenius. The solution that is bounded at x = 0 is J1(x
‚àö
Œª). The
eigenvalues are solutions of
J1(
#
Œªn) = 0,
which we write as Œª = Œª2
1, Œª2
2, . . . , where J1(Œªn) = 0.
Finally, let‚Äôs examine Bessel‚Äôs equation with ŒΩ = 1, but now for x ‚àà(0, ‚àû).
Since both endpoints are now singular, appropriate boundary conditions are
|y(x, Œª)| ‚©ΩA for x ‚àà

0, 1
2
"
, |y(x, Œª)| ‚©ΩB for x ‚àà
! 1
2, ‚àû

,
for some A, B ‚ààR+. The eigenfunctions are again J1(x
‚àö
Œª), but now the eigenval-
ues lie on the half-line [0, ‚àû). In other words, the eigenfunctions exist for all real,
positive Œª. The set of eigenvalues for a Sturm‚ÄìLiouville system is often called the
spectrum. In the Ô¨Årst of the Bessel function examples above, we have a discrete
spectrum, whereas for the second there is a continuous spectrum.
We will
focus our attention on problems that have a discrete spectrum only.

4.3 STURM‚ÄìLIOUVILLE SYSTEMS
111
4.3.3
Properties of the Eigenvalues and Eigenfunctions
In order to further study the properties of the eigenfunctions and eigenvalues,
we begin by deÔ¨Åning the inner product of two complex-valued functions over an
interval I to be
‚ü®œÜ1(x), œÜ2(x)‚ü©=

I
œÜ‚àó
1(x)œÜ2(x) dx,
where a superscript asterisk denotes the complex conjugate. This means that the
inner product has the properties
(i) ‚ü®œÜ1, œÜ2‚ü©= ‚ü®œÜ2, œÜ1‚ü©‚àó,
(ii) ‚ü®a1œÜ1, a2œÜ2‚ü©= a‚àó
1a2‚ü®œÜ1, œÜ2‚ü©,
(iii) ‚ü®œÜ1, œÜ2 + œÜ3‚ü©= ‚ü®œÜ1, œÜ2‚ü©+ ‚ü®œÜ1, œÜ3‚ü©, ‚ü®œÜ1 + œÜ2, œÜ3‚ü©= ‚ü®œÜ1, œÜ3‚ü©+ ‚ü®œÜ2, œÜ3‚ü©,
(iv) ‚ü®œÜ, œÜ‚ü©=
'
I |œÜ|2 dx ‚©æ0, with equality if and only if œÜ(x) ‚â°0 in I.
Note that this reduces to the deÔ¨Ånition of a real inner product if œÜ1 and œÜ2 are real.
If ‚ü®œÜ1, œÜ2‚ü©= 0 with œÜ1 Ã∏‚â°0 and œÜ2 Ã∏‚â°0, we say that œÜ1 and œÜ2 are orthogonal.
Let y1(x), y2(x) ‚ààC2[a, b] be twice-diÔ¨Äerentiable complex-valued functions. By
integrating by parts, it is straightforward to show that (see Lemma 4.1)
‚ü®y2, Sy1‚ü©‚àí‚ü®Sy2, y1‚ü©=
(
p(x)

y1(x)(y‚àó
2(x))‚Ä≤ ‚àíy‚Ä≤
1(x)y‚àó
2(x)
)Œ≤
Œ±,
(4.39)
which is known as Green‚Äôs formula. The inner products are deÔ¨Åned over a sub-
interval [Œ±, Œ≤] ‚äÇ(a, b), so that we can take the limits Œ± ‚Üía+ and Œ≤ ‚Üíb‚àíwhen
the endpoints are singular, and the Sturm‚ÄìLiouville operator, S, is given by (4.35).
Now if x = a is a regular endpoint and the functions y1 and y2 satisfy a separated
boundary condition at a, then
p(a)

y1(a)(y‚àó
2(a))‚Ä≤ ‚àíy‚Ä≤
1(a)y‚àó
2(a)

= 0.
(4.40)
If a is a Ô¨Ånite singular endpoint and the functions y1 and y2 satisfy the Friedrichs
boundary condition at a,
lim
x‚Üía+
(
p(x)

y1(x)(y‚àó
2(x))‚Ä≤ ‚àíy‚Ä≤
1(x)y‚àó
2(x)
)
= 0.
(4.41)
Similar results hold at x = b.
We can now derive several results concerning the eigenvalues and eigenfunctions
of a Sturm‚ÄìLiouville boundary value problem.
Theorem 4.3 The eigenvalues of a Sturm‚ÄìLiouville boundary value problem are
real.
Proof If we substitute y1(x) = y(x, Œª) and y2(x) = y‚àó(x, Œª) into Green‚Äôs formula
over the entire interval, [a, b], we have
‚ü®y‚àó(x, Œª), Sy(x, Œª)‚ü©‚àí‚ü®Sy‚àó(x, Œª), y(x, Œª)‚ü©
=
(
p(x)

y(x, Œª)(y‚àó(x, Œª))‚Ä≤ ‚àíy‚Ä≤(x, Œª)y‚àó(x, Œª)
)b
a = 0,

112
BOUNDARY VALUE PROBLEMS
making use of (4.40) and (4.41). Now, using the fact that the functions y(x, Œª) and
y‚àó(x, Œª) are solutions of (4.33) and its complex conjugate, we Ô¨Ånd that
 b
a
r(x)y(x, Œª)y‚àó(x, Œª)(Œª ‚àíŒª‚àó) dx = (Œª ‚àíŒª‚àó)
 b
a
r(x)|y(x, Œª)|2 dx = 0.
Since r(x) > 0 and y(x, Œª) is nontrivial, we must have Œª = Œª‚àó, and hence Œª ‚ààR.
Theorem 4.4 If y(x, Œª) and y(x, ÀúŒª) are eigenfunctions of the Sturm‚ÄìLiouville
boundary value problem, with Œª Ã∏= ÀúŒª, then these eigenfunctions are orthogonal over
(a, b) with respect to the weighting function r(x), so that
 b
a
r(x)y(x, Œª)y(x, ÀúŒª) dx = 0.
(4.42)
Proof Firstly, notice that the separated boundary condition, (4.38), at x = a takes
the form
Œ±0y1(a) + Œ±1y‚Ä≤
1(a) = 0,
Œ±0y2(a) + Œ±1y‚Ä≤
2(a) = 0.
(4.43)
Taking the complex conjugate of the second of these gives
Œ±0y‚àó
2(a) + Œ±1 (y‚Ä≤
2(a))‚àó= 0,
(4.44)
since Œ±0 and Œ±1 are real. For the pair of equations (4.43)2 and (4.44) to have a
nontrivial solution, we need
y1(a) (y‚Ä≤
2(a))‚àó‚àíy‚Ä≤
1(a)y‚àó
2(a) = 0.
A similar result holds at the other endpoint, x = b. This clearly shows that
p(x)

y(x, Œª)

y‚Ä≤(x, ÀúŒª)
‚àó
‚àíy‚Ä≤(x, Œª)

y(x, ÀúŒª)
‚àó
‚Üí0
as x ‚Üía and x ‚Üíb, so that, from Green‚Äôs formula, (4.39),
‚ü®y(x, ÀúŒª), Sy(x, Œª)‚ü©= ‚ü®Sy(x, ÀúŒª), y(x, Œª)‚ü©.
If we evaluate this formula, we Ô¨Ånd that
 b
a
r(x)y(x, Œª)y(x, ÀúŒª) dx = 0,
so that the eigenfunctions associated with the distinct eigenvalues Œª and ÀúŒª are
orthogonal with respect to the weighting function r(x).
Example
Consider Hermite‚Äôs equation, (4.20). By using the method of Frobenius, we can
show that there are polynomial solutions, Hn(x), when Œª = 2n for n = 0, 1, 2, . . . .
For example, H0(x) = 1, H1(x) = 2x and H2(x) = 4x2 ‚àí2. The solutions of (4.21),
the self-adjoint form of the equation, that are bounded at inÔ¨Ånity for Œª = 2n then

4.3 STURM‚ÄìLIOUVILLE SYSTEMS
113
take the form un = e‚àíx2/2Hn(x), and, from Theorem 4.4, satisfy the orthogonality
condition
 ‚àû
‚àí‚àû
e‚àíx2Hn(x)Hm(x) dx = 0 for n Ã∏= m.
4.3.4
Bessel‚Äôs Inequality, Approximation in the Mean and
Completeness
We can now deÔ¨Åne a sequence of orthonormal eigenfunctions
œÜn(x) =
#
r(x)y(x, Œªn)
‚ü®
#
r(x)y(x, Œªn),
#
r(x)y(x, Œªn)‚ü©
,
which satisfy
‚ü®œÜn(x), œÜm(x)‚ü©= Œ¥nm,
(4.45)
where Œ¥nm is the Kronecker delta. We will try to establish when we can write a
piecewise continuous function f(x) in the form
f(x) =
‚àû

i=0
aiœÜi(x).
(4.46)
Taking the inner product of both sides of this series with œÜj(x) shows that
aj = ‚ü®f(x), œÜj(x)‚ü©,
(4.47)
using the orthonormality condition, (4.45). The quantities ai are known as the ex-
pansion coeÔ¨Écients, or generalized Fourier coeÔ¨Écients. In order to motivate
the inÔ¨Ånite series expansion (4.46), we start by approximating f(x) by a Ô¨Ånite sum,
fN(x) =
N

i=0
AiœÜ(x, Œªi),
for some Ô¨Ånite N, where the Ai are to be determined so that this provides the most
accurate approximation to f(x). The error in this approximation is
RN(x) = f(x) ‚àí
N

i=0
AiœÜ(x, Œªi).
We now try to minimize this error by minimizing its norm
||RN||2 = ‚ü®RN(x), RN(x)‚ü©=
 b
a

f(x) ‚àí
N

i=0
AiœÜi(x)
2
dx,
which is the mean square error in the approximation. Now
||RN||2 =
0
f(x) ‚àí
N

i=0
AiœÜi(x), f(x) ‚àí
N

i=0
AiœÜi(x)
1

114
BOUNDARY VALUE PROBLEMS
= ||f(x)||2 ‚àí
0
f(x),
N

i=0
AiœÜi(x)
1
‚àí
0 N

i=0
AiœÜi(x), f(x)
1
+
0 N

i=0
AiœÜi(x),
N

i=0
AiœÜi(x)
1
.
We can now use the orthonormality of the eigenfunctions, (4.45), and the expression
(4.47), which determines the coeÔ¨Écients ai, to obtain
||RN(x)||2 = ||f(x)||2 ‚àí
N

i=0
Ai ‚ü®f(x), œÜi(x)‚ü©
‚àí
N

i=0
A‚àó
i ‚ü®œÜi(x), f(x)‚ü©+
N

i=0
A‚àó
i Ai ‚ü®œÜi(x), œÜi(x)‚ü©
= ||f(x)||2 +
N

i=0
{‚àíAiai ‚àíA‚àó
i a‚àó
i + A‚àó
i Ai}
= ||f(x)||2 +
N

i=0

|Ai ‚àíai|2 ‚àí|ai|2
.
The error is therefore smallest when Ai = ai for i = 0, 1, . . . , N, so the most
accurate approximation is formed by simply truncating the series (4.46) after N
terms. In addition, since the norm of RN(x) is positive,
N

i=0
|ai|2 ‚©Ω
 b
a
|f(x)|2 dx.
As the right hand side of this is independent of N, it follows that
‚àû

i=0
|ai|2 ‚©Ω
 b
a
|f(x)|2 dx,
(4.48)
which is Bessel‚Äôs inequality.
This shows that the sum of the squares of the
expansion coeÔ¨Écients converges. Approximations by the method of least squares
are often referred to as approximations in the mean, because of the way the error
is minimized.
If, for a given orthonormal system, œÜ1(x), œÜ2(x), . . . , any piecewise continuous
function can be approximated in the mean to any desired degree of accuracy by
choosing N large enough, then the orthonormal system is said to be complete. For
complete orthonormal systems, RN(x) ‚Üí0 as N ‚Üí‚àû, so that Bessel‚Äôs inequality
becomes an equality,
‚àû

i=0
|ai|2 =
 b
a
|f(x)|2 dx,
(4.49)
for every function f(x).

4.3 STURM‚ÄìLIOUVILLE SYSTEMS
115
The completeness of orthonormal systems, as expressed by
lim
N‚Üí‚àû
 b
a

f(x) ‚àí
N

i=0
aiœÜi(x)
2
dx = 0,
does not necessarily imply that f(x) =  ‚àû
i=0 aiœÜi(x), in other words that f(x)
has an expansion in terms of the œÜi(x).
If, however, the series  ‚àû
i=0 aiœÜi(x) is
uniformly convergent, then the limit and the integral can be interchanged, the
expansion is valid, and we say that  ‚àû
i=0 aiœÜi(x) converges in the mean to f(x).
The completeness of the system œÜ1(x), œÜ2(x), . . . , should be seen as a necessary
condition for the validity of the expansion, but, for an arbitrary function f(x), the
question of convergence requires a more detailed investigation.
The Legendre polynomials P0(x), P1(x), . . . on the interval [‚àí1, 1] and the Bessel
functions JŒΩ(Œª1x), JŒΩ(Œª2x), . . . on the interval [0, a] are both examples of complete
orthogonal systems (they can easily be made orthonormal), and the expansions
of Chapters 2 and 3 are special cases of the more general results of this chapter.
For example, the Bessel functions JŒΩ(
‚àö
Œªx) satisfy the Sturm‚ÄìLiouville equation,
(4.33), with p(x) = x, q(x) = ‚àíŒΩ2/x and r(x) = x. They satisfy the orthogonality
relation
 a
0
xJŒΩ(‚àö¬µx)JŒΩ(
‚àö
Œªx) dx = 0,
if Œª and ¬µ are distinct eigenvalues. Using the regular endpoint condition JŒΩ(
‚àö
Œªa) =
0 and the singular endpoint condition at x = 0, the eigenvalues, that is the zeros
of JŒΩ(x), can be written as
‚àö
Œªa = Œª1a, Œª2a, . . . , so that
‚àö
Œª = Œªi for i = 1, 2, . . . ,
and we can write
f(x) =
‚àû

i=1
aiJŒΩ(Œªix),
with
ai =
2
a2 {J‚Ä≤ŒΩ(Œªia)}2
 a
0
xJŒΩ(Œªix)f(x) dx,
consistent with (3.32).
4.3.5
Further Properties of Sturm‚ÄìLiouville Systems
We conclude this section by investigating some of the qualitative properties of
solutions of the Sturm‚ÄìLiouville system (4.33).
In particular, we will establish
that the nth eigenfunction has n zeros in the open interval (a, b). We will take
a geometrical point of view in order to establish this result, although we could
have used an analytical framework.
To achieve this, we introduce the Pr¬®ufer
substitution,
p(x)y‚Ä≤(x) = R(x) cos Œ∏(x),
y(x) = R(x) sin Œ∏(x).
(4.50)

116
BOUNDARY VALUE PROBLEMS
The new dependent variables, R and Œ∏, are then deÔ¨Åned by
R2 = y2 + p2(y‚Ä≤)2,
Œ∏ = tan‚àí1
 y
py‚Ä≤

(4.51)
and, by analogy with polar coordinates, we call R the amplitude and Œ∏ the phase
angle.
Nontrivial eigenfunction solutions of the Sturm‚ÄìLiouville equation have
R > 0, since R = 0 at any point x = x0 would mean that y(x0) = y‚Ä≤(x0) = 0, and
hence give the trivial solution for all x.
If we now write cot Œ∏(x) = py‚Ä≤/y and diÔ¨Äerentiate, we obtain
‚àícosec2Œ∏ dŒ∏
dx = (py‚Ä≤)‚Ä≤
y
‚àíp (y‚Ä≤)2
y2
= ‚àíŒªr ‚àíq ‚àí1
p cot2 Œ∏,
and hence
dŒ∏
dx = (q(x) + Œªr(x)) sin2 Œ∏ +
1
p(x) cos2 Œ∏.
(4.52)
If we now diÔ¨Äerentiate (4.51)1, some simple manipulation gives
dR
dx = 1
2
 1
p(x) ‚àíq(x) ‚àíŒªr(x)
	
R sin 2Œ∏.
(4.53)
The Pr¬®ufer substitution has therefore changed our second order linear diÔ¨Äerential
equation into a system of two Ô¨Årst order nonlinear diÔ¨Äerential equations in R and
Œ∏ over the interval a < x < b, (4.52) and (4.53). The equation for Œ∏, (4.52), is
however independent of R and is, as we shall see, relatively easy to analyze.
If we now consider the separated boundary conditions
Œ±1y(a) + Œ±2y‚Ä≤(a) = 0,
Œ≤1y(b) + Œ≤2y‚Ä≤(b) = 0,
we can deÔ¨Åne two phase angles, Œ≥ and Œ¥, such that
tan Œ≥ =
y(a)
p(a)y‚Ä≤(a) = ‚àí
Œ±2
p(a)Œ±1
for 0 ‚©ΩŒ≥ ‚©ΩœÄ,
tan Œ¥ =
y(b)
p(b)y‚Ä≤(b) = ‚àí
Œ≤2
p(b)Œ≤1
for 0 ‚©ΩŒ¥ ‚©ΩœÄ,
and the eigenvalue problem that we have to solve is (4.52) subject to
Œ∏(a) = Œ≥,
Œ∏(b) = Œ¥ + nœÄ for n = 0, 1, 2, . . . .
(4.54)
We need to add this multiple of nœÄ because of the periodicity of the tangent func-
tion.
We can infer the qualitative form of the solution of (4.52) by drawing its di-
rection Ô¨Åeld. This is the set of small line segments of slope dŒ∏/dx in the (Œ∏, x)
plane, as sketched in Figure 4.3. Note that dŒ∏/dx = 1/p(x) at Œ∏ = nœÄ, which is
independent of Œª. In addition, dŒ∏/dx = q(x) + Œªp(x) at Œ∏ =

n + 1
2

œÄ, which, for
Ô¨Åxed x, increases with increasing Œª. From Figure 4.4, which shows some typical
solution curves for various values of Œª, we can see that, for any initial condition
Œ∏(a) = Œ≥, Œ∏(b) is an increasing function of Œª. As Œª increases from ‚àí‚àû, there is a

4.3 STURM‚ÄìLIOUVILLE SYSTEMS
117
Ô¨Årst value, Œª = Œª0, for which Œ∏(b) = Œ¥. As Œª increases further, there is a sequence
of values for which Œ∏ = Œ¥ + nœÄ for n = 1, 2, . . . . Each of these is associated with an
eigenfunction, yn(x) = Rn(x) sin Œ∏(x, Œªn). This has a zero when sin Œ∏ = 0 and, in
the interval Œ≥ ‚©ΩŒ∏ ‚©ΩŒ¥ + nœÄ, there are precisely n zeros at Œ∏ = œÄ, 2œÄ, . . . , nœÄ.
Fig. 4.3. The direction Ô¨Åeld (lines of slope dŒ∏/dx) for (4.52) when p(x) = x, q(x) = ‚àí1/x
and r(x) = x, which corresponds to Bessel‚Äôs equation of order one.
Returning now to the example y‚Ä≤‚Ä≤ + Œªy = 0 subject to y(0) = y(œÄ) = 0, we have
eigenvalues Œªn = n2 and eigenfunctions sin nx for n = 1, 2, . . . . Because of the way
we have labelled these, Œª0 = 1 and y0(x) = sin x is the zeroth eigenfunction, which
has no zeros in 0 < x < œÄ. We can also see that Œª1 = 22 and the Ô¨Årst eigenfunction,
y1(x) = sin 2x, has one zero in 0 < x < œÄ, at x = œÄ
2 , and so on. We can formalize
this analysis as a theorem.
Theorem 4.5 A regular Sturm‚ÄìLiouville system has an inÔ¨Ånite sequence of real
eigenvalues, Œª0 < Œª1 < ¬∑ ¬∑ ¬∑ < Œªn < ¬∑ ¬∑ ¬∑ , with Œªn ‚Üí‚àûas n ‚Üí‚àû. The correspond-
ing eigenfunctions, yn(x), have n zeros in the interval a < x < b.
We can prove another useful result if we add an additional constraint. If q(x) < 0,
then all the eigenvalues are positive. To see why this should be so, consider the
boundary value problem
d
dx

p(x)dy
dx

+ (Œªr(x) + q(x)) y = 0 subject to y(a) = y(b) = 0.

118
BOUNDARY VALUE PROBLEMS
Fig. 4.4. Typical solutions of (4.52) for various values of Œª when p(x) = x, q(x) = ‚àí1/x
and r(x) = x, which corresponds to Bessel‚Äôs equation of order one.
If we multiply through by y and integrate over [a, b], we obtain
Œª
 b
a
r(x)y2 dx =
 b
a
‚àíq(x)y2 dx ‚àí
 b
a
y d
dx

p(x)dy
dx

dx
=
 b
a

‚àíq(x)y2 + p(x) (y‚Ä≤)2
dx,
using integration by parts. This shows that
Œª =
 b
a

‚àíq(x)y2 + p(x) (y‚Ä≤)2
dx
2 b
a
r(x)y2 dx ,
which is positive when p and r are positive and q is negative.
4.3.6
Two Examples from Quantum Mechanics
One of the areas of mathematical physics where Sturm‚ÄìLiouville equations arise
most often is quantum mechanics, the theory that governs the behaviour of mat-
ter on very small length scales. Three important postulates of quantum mechanics
are (see SchiÔ¨Ä, 1968):
(i) A system is completely speciÔ¨Åed by a state function, or wave function,
œà(r, t), with ‚ü®œà, œà‚ü©= 1. For example, if the system consists of a particle

4.3 STURM‚ÄìLIOUVILLE SYSTEMS
119
moving in an external potential, then |œà(r, t)|2 d 3r is the probability of Ô¨Ånd-
ing the particle in a small volume d 3r that surrounds the point r, and hence
we need
  
R3 |œà(r)|2 d 3r = ‚ü®œà, œà‚ü©= 1.
(ii) For every system there exists a certain Hermitian operator, H, called the
Hamiltonian operator, such that
i‚Ñè‚àÇœà
‚àÇt = Hœà,
where 2œÄ‚Ñè‚âà6.62 √ó 10‚àí34 J s‚àí1 is Planck‚Äôs constant.
(iii) To each observable property of the system there corresponds a linear, Her-
mitian operator, A, and any measurement of the property gives one of the
eigenvalues of A. For example, the operators that correspond to momentum
and energy are ‚àíi‚Ñè‚àáand i‚Ñè‚àÇ/‚àÇt.
For a single particle of mass m moving in a potential Ô¨Åeld V (r, t), the classical (as
opposed to quantum mechanical) total energy is
E = p2/2m + V (r, t),
(4.55)
where p is the momentum of the particle.
This is just the sum of the kinetic
and potential energies. To obtain the quantum mechanical analogue of this, we
quantize the classical result (4.55) by substituting the appropriate operators for
momentum and energy, and arrive at
i‚Ñè‚àÇœà
‚àÇt = ‚àí‚Ñè2
2m‚àá2œà + V (r, t)œà.
(4.56)
This is Schr¬®odinger‚Äôs equation, which governs the evolution of the wave function.
Let‚Äôs look for a separable solution of (4.56) when V is independent of time. We
write œà = u(r)T(t), and Ô¨Ånd that
i‚ÑèT ‚Ä≤
T = ‚àí‚Ñè2
2mu‚àá2u + V (r) = E,
where E is the separation constant. Since i‚ÑèT ‚Ä≤ = ET,
i‚Ñè‚àÇœà
‚àÇt = Eœà,
and hence E is the energy of the particle. The equation for u is then the time-
independent Schr¬®odinger equation,
‚àí‚Ñè2
2m‚àá2u + (V (r) ‚àíE) u = 0.
(4.57)
We seek solutions of this equation subject to the conditions that u should be Ô¨Ånite,
and u and ‚àáu continuous throughout the domain of solution. After we impose
appropriate boundary conditions, we will Ô¨Ånd that the energy of the system is not
arbitrary, as classical physics would suggest. Instead, the energy must be one of
the eigenvalues of the boundary value problem associated with (4.57), and hence
the eigenvalues are of great interest. The energy is said to be quantized.

120
BOUNDARY VALUE PROBLEMS
Example: A conÔ¨Åned particle
Let‚Äôs consider the one-dimensional problem of a particle of mass m conÔ¨Åned in a
region of zero potential by an inÔ¨Ånite potential at x = 0 and x = a. What energies
can the particle have?
Since the probability of Ô¨Ånding the particle outside the region 0 < x < a is zero,
we must have œà = 0 there. By continuity, we therefore have œà = 0 at x = 0 and
x = a. We must therefore solve the eigenvalue problem
‚àí‚Ñè2
2m
‚àÇ2u
‚àÇx2 = Eu,
(4.58)
subject to u = 0 at x = 0 and x = a. However, this is precisely the problem, given
by (4.2), with which we began this chapter, with Œª = 2mE/‚Ñè2. We conclude that
the allowed energies of the particle are E = En = ‚Ñè2Œªn/2m = ‚Ñè2n2œÄ2/2ma2.
Example: The hydrogen atom
The hydrogen atom consists of an electron and a proton. Since the mass of the
proton is much larger than that of the electron, let‚Äôs assume that the proton is
at rest. The steady state wave function, œà(r), then satisÔ¨Åes (4.57) which, after
rescaling r and E to eliminate the constants, we write as
‚àá2œà ‚àí2V œà = ‚àí2Eœà for |r| > 0.
(4.59)
The potential V (r) = V (r) = ‚àíq2/r is the Coulomb potential due to the electri-
cal attraction between the proton and the electron, where ‚àíq is the charge on the
electron and q that on the proton.
We can now look for separable solutions in spherical polar coordinates (r, Œ∏, œÜ)
in the form œà = R(r)Y (s)Œò(œÜ), where s = cos Œ∏. Substituting this into (4.59) gives
us
r2 R‚Ä≤‚Ä≤
R + 2rR‚Ä≤
R + 2r2 (E ‚àíV (r)) = ‚àí

(1 ‚àís2)Y ‚Ä≤‚Ä≤
Y
‚àí
1
1 ‚àís2
Œò‚Ä≤‚Ä≤
Œò = Œª,
for some separation constant Œª. If we take Œò‚Ä≤‚Ä≤/Œò = ‚àím2 with m = 1, 2, . . . for
periodicity, we obtain Œò = Am cos mœÜ + Bm sin mœÜ. If Œª = n(n + 1), with n ‚©æm,
then the equation for Y is the associated Legendre equation, which we studied in
Chapter 2. The bounded solution of this is Y = CnP m
n (s), and we have a solution
in the form
œà(r, Œ∏, œÜ) = R(r)P m
n (s) (Dn,m cos mœÜ + En,m sin mœÜ) ,
where Dn,m and En,m are arbitrary constants and R satisÔ¨Åes the diÔ¨Äerential equa-
tion
r2R‚Ä≤‚Ä≤ + 2rR ‚àín(n + 1)R + 2r2

E + q2
r

R = 0.
We can simplify matters by deÔ¨Åning S = rR, so that
S‚Ä≤‚Ä≤ ‚àín(n + 1)
r
S + 2

E + q2
r

S = 0 for r > 0,
(4.60)

EXERCISES
121
to be solved subject to the condition that S/r ‚Üí0 as r ‚Üí‚àû. This is an eigenvalue
problem for E, the allowable energy levels of the electron in the hydrogen atom.
Some thought and experimentation leads one to try a solution in the form S =
rn+1e‚àí¬µr, which will satisfy (4.60) provided that ¬µ = q2/(n + 1) and E = ‚àí1
2¬µ2.
A reduction of order argument then shows that there are no other solutions with
E < 0 that satisfy the condition at inÔ¨Ånity. Each energy level, EN = ‚àíq4/2N 2 for
N = 1, 2, . . . , corresponds to a possible steady state for our model of the hydrogen
atom, and is in agreement with the experimentally observed values. However, this
is not the end of the matter, as it is possible to show that every positive value of
E corresponds to a bounded, nontrivial solution of (4.60). In other words, there is
both a continuous and a discrete part to the spectrum. These states with positive
energy can be shown to be unstable, as the electron has too much energy.
Exercises
4.1
Use the eigenfunction expansion method to Ô¨Ånd a solution of the boundary
value problem y‚Ä≤‚Ä≤(x) = ‚àíh(x) for 0 < x < 2œÄ, subject to the boundary
conditions y(0) = y(2œÄ), y‚Ä≤(0) = y‚Ä≤(2œÄ), with h ‚ààC[0, 2œÄ].
4.2
Find the Green‚Äôs function for the boundary value problems
(a) y‚Ä≤‚Ä≤(x) = f(x) subject to y(‚àí1) = y(1) = 0,
(b) y‚Ä≤‚Ä≤(x) + œâ2y(x) = f(x) subject to y(0) = y(œÄ/2) = 0.
4.3
Comment on the diÔ¨Éculties that you face when trying to construct the
Green‚Äôs function for the boundary value problem
y‚Ä≤‚Ä≤(x) + y(x) = f(x) subject to y(a) = y‚Ä≤(b) = 0.
4.4
Show that when y‚Ä≤‚Ä≤ + Œªy = 0 for 0 < x < œÄ, the eigenvalues when (a)
y(0) = y‚Ä≤(œÄ) = 0, (b) y‚Ä≤(0) = y(œÄ) = 0, (c) y‚Ä≤(0) = y‚Ä≤(œÄ) = 0, are

n + 1
2
2,

n + 1
2
2 and n2 respectively, where n = 0, 1, 2, . . . . What are
the corresponding eigenfunctions?
4.5
Show that the equation
d 2y
dx2 + A(x)dy
dx + {ŒªB(x) ‚àíC(x)} y = 0
can be written in Sturm‚ÄìLiouville form by deÔ¨Åning p(x) = exp
'
A(x) dx

.
What are q(x) and r(x) in terms of A, B and C?
4.6
Write the generalized Legendre equation,
(1 ‚àíx2)d 2y
dx2 ‚àí2xdy
dx +

n(n + 1) ‚àí
m2
1 ‚àí¬µ2
	
y = 0,
as a Sturm‚ÄìLiouville equation.
4.7
Determine the eigenvalues, Œª, of the fourth order equation y(4) + Œªy = 0
subject to y(0) = y‚Ä≤(0) = y(œÄ) = y‚Ä≤(œÄ) = 0 for 0 ‚©Ωx ‚©ΩœÄ.

122
BOUNDARY VALUE PROBLEMS
4.8
Consider the singular Sturm‚ÄìLiouville system

xe‚àíxy‚Ä≤‚Ä≤ + Œªe‚àíxy = 0 for x > 0,
with y bounded as x ‚Üí0 and e‚àíxy ‚Üí0 as x ‚Üí‚àû. Show that when
Œª = 0, 1, 2, . . . there are polynomial eigenfunctions. These are known as
the Laguerre polynomials.
4.9
Show that the boundary value problem
y‚Ä≤‚Ä≤(x) + A(x)y‚Ä≤(x) + B(x)y(x) = C(x) for all x ‚àà(a, b),
subject to
Œ±1y(a) + Œ≤1y(b) + ÀúŒ±1y‚Ä≤(a) + ÀúŒ≤1y‚Ä≤(b) = Œ≥1
Œ±2y(a) + Œ≤2y(b) + ÀúŒ±2y‚Ä≤(a) + ÀúŒ≤2y‚Ä≤(b) = Œ≥2,
is self-adjoint provided that
Œ≤1 ÀúŒ≤2 ‚àíÀúŒ≤1Œ≤2 = (Œ±1ÀúŒ±2 ‚àíÀúŒ±1Œ±2) exp
 b
a
A(x) dx

.
4.10
Show that
‚àí(xy‚Ä≤(x))‚Ä≤ = Œªxy(x)
is self-adjoint on the interval (0, 1), with x = 0 a singular endpoint and
x = 1 a regular endpoint with the condition y(1) = 0.
4.11
Find the eigenvalues of the system consisting of Fourier‚Äôs equation and the
conditions y(0) ‚àíy‚Ä≤(0) = 0 and y(œÄ) = 0. Show that these eigenfunctions
are orthogonal on the interval (0, œÄ).
4.12
Prove that sin mx has at least one zero between each pair of consecutive
zeros of sin nx, when m > n.
4.13
‚àóUsing the Sturm comparison theorem (see Exercise 1.14), show that every
solution of Airy‚Äôs equation, y‚Ä≤‚Ä≤(x) ‚àíxy(x) = 0, vanishes inÔ¨Ånitely often on
the positive x-axis, and at most once on the negative x-axis.

CHAPTER FIVE
Fourier Series and the Fourier Transform
In order to motivate our discussion of Fourier series, we shall consider the solution of
a diÔ¨Äusion problem. Let‚Äôs suppose that we have a long, thin, cylindrical metal bar
of length L whose curved sides and one end are insulated from its surroundings.
Suppose also that, initially, the temperature of the bar is T = T0, but that the
uninsulated end is suddenly cooled to a temperature T1 < T0.
How does the
temperature in the metal bar vary for t > 0? Physical intuition suggests that heat
will Ô¨Çow out of the end of the bar, and that, as time progresses, the temperature
will approach T1 throughout the bar.
In order to quantify this, we note that the temperature in the bar satisÔ¨Åes the
one-dimensional diÔ¨Äusion equation
‚àÇT
‚àÇt = K ‚àÇ2T
‚àÇx2
for 0 < x < L,
(5.1)
where x measures distance along the bar and K is its thermal diÔ¨Äusivity (see Sec-
tion 2.6.1). The initial and boundary conditions are
T(x, 0) = T0,
T(0, t) = T1,
‚àÇT
‚àÇx (L, t) = 0.
(5.2)
Before we solve this initial‚Äìboundary value problem, it is convenient to deÔ¨Åne di-
mensionless variables,
ÀÜT = T ‚àíT1
T0 ‚àíT1
,
ÀÜx = x
L,
ÀÜt = Kt
L2 .
In terms of these variables, (5.1) and (5.2) become
‚àÇÀÜT
‚àÇÀÜt = ‚àÇ2 ÀÜT
‚àÇÀÜx2
for 0 < ÀÜx < 1,
(5.3)
ÀÜT(ÀÜx, 0) = 1,
ÀÜT(0, ÀÜt ) = 0,
‚àÇÀÜT
‚àÇÀÜx (1, ÀÜt ) = 0.
(5.4)
As you can see, by choosing appropriate dimensionless variables, we have managed
to eliminate all of the physical constants from the problem.
The use of dimensionless variables has additional advantages, which makes it
essential to use them in studying most mathematical models of real physical prob-
lems. Consider the length of the metal bar in the diÔ¨Äusion problem that we are
studying. It is usual to measure lengths in terms of metres, and, at Ô¨Årst sight, it

124
FOURIER SERIES AND THE FOURIER TRANSFORM
seems reasonable to say that a metal bar with length 100 m is long, whilst a bar of
length 10‚àí2 m = 1 cm is short. In eÔ¨Äect, we choose a bar of length 1 m as a refer-
ence relative to which we measure the length of our actual bar. Is this a reasonable
thing to do? In fact, the only length that is deÔ¨Åned in the problem is the length
of the bar itself. This is the most sensible length with which to make the problem
dimensionless, and leads to a bar that lies between ÀÜx = 0 and ÀÜx = 1. For any given
problem, it is essential to choose a length scale that is relevant to the physics of
the problem itself, not some basically arbitrary length, such as 1 m. For example,
problems in celestial mechanics are often best made dimensionless using the mean
distance from the Earth to the Sun, whilst problems in molecular dynamics may be
made dimensionless using the mean atomic separation in a hydrogen atom. These
length scales are enormously diÔ¨Äerent from 1 m. The same argument applies to
all of the dimensional, physical quantities that are used in a mathematical model.
By dividing each variable by a suitable constant with the same dimensions, we can
state that a variable is small or large in a meaningful way. For example, using the
dimensionless time, ÀÜt = Kt/L2, which we deÔ¨Åned above, the solution when ÀÜt ‚â™1,
the small time solution, really does represent the behaviour of the temperature in
the metal bar when diÔ¨Äusion has had little eÔ¨Äect on the initial state, over the length
scale, L, that characterizes the full length of the bar. The other advantage of using
dimensionless variables is that any physical constants that remain explicitly in the
dimensionless problem appear in dimensionless groups, which can themselves be
said to be large or small in a meaningful way. No dimensionless groups appear in
the simple diÔ¨Äusion problem given by (5.3) and (5.4), and we will defer any further
discussion of them until Chapter 11.
We can now continue with our study of the diÔ¨Äusion of heat in a metal bar by
seeking a separable solution, ÀÜT(ÀÜx, ÀÜt ) = X(ÀÜx)F(ÀÜt ). On substituting this into (5.3)
we obtain
X‚Ä≤‚Ä≤
X =
ÀôF
F = ‚àík2,
the separation constant.
The equation ÀôF = ‚àík2F has solution F = ae‚àík2ÀÜt, whilst X‚Ä≤‚Ä≤ + k2X = 0, Fourier‚Äôs
equation, (4.37), which we studied in Chapter 4, has solution
X = A sin kÀÜx + B cos kÀÜx.
The condition ÀÜT(0, ÀÜt ) = 0 means that X(0) = 0, and hence that B = 0. Similarly,
the condition that there should be no Ô¨Çux of heat through the bar at ÀÜx = 1 leads
to kA cos k = 0, and hence shows that
k = œÄ
2 + nœÄ for n = 0, 1, 2, . . . .
There is therefore a countably-inÔ¨Ånite sequence of solutions,
ÀÜTn = An exp

‚àíœÄ2

n + 1
2
2
ÀÜt

sin

œÄ

n + 1
2

ÀÜx
	
.

FOURIER SERIES AND THE FOURIER TRANSFORM
125
Since this is a linear problem, the general solution is
ÀÜT(ÀÜx, ÀÜt ) =
‚àû

n=0
An exp

‚àíœÄ2

n + 1
2
2
ÀÜt

sin

œÄ

n + 1
2

ÀÜx
	
.
(5.5)
We are now left with the task of determining the constants An.
The only information we have not used is the initial condition, ÀÜT(ÀÜx, 0) = 1, which
shows that
‚àû

n=0
An sin

œÄ

n + 1
2

ÀÜx
	
= 1.
(5.6)
We now multiply this expression through by sin

œÄ

m + 1
2

ÀÜx

and integrate over
the length of the bar. After noting that
 1
0
sin

œÄ

n + 1
2

ÀÜx
	
sin

œÄ

m + 1
2

ÀÜx
	
dÀÜx
=
 1
0
1
2 [cos {œÄ (m ‚àín) ÀÜx} ‚àícos {œÄ (m + n + 1) ÀÜx}] dÀÜx = 0 for m Ã∏= n,
(5.7)
and
 1
0
sin2

œÄ

n + 1
2

ÀÜx
	
dÀÜx =
 1
0
1
2 [1 ‚àícos {œÄ (2m + 1) ÀÜx}] dÀÜx = 1
2 for m = n,
(5.8)
we conclude that
An = 2
 1
0
sin

œÄ

n + 1
2

ÀÜx
	
dÀÜx =
4
(2n + 1)œÄ ,
and hence that
ÀÜT(ÀÜx, ÀÜt ) =
‚àû

n=0
4
(2n + 1)œÄ exp

‚àíœÄ2

n + 1
2
2
ÀÜt

sin

œÄ

n + 1
2

ÀÜx
	
.
(5.9)
This is a Fourier series solution, and is shown in Figure 5.1 at various times. We
produced Figure 5.1 by plotting the MATLAB function
'
&
$
%
function heat = heat(x,t)
acc = 10^-8; n=ceil(sqrt(-log(acc)/t)-0.5);
N = 0:n; a = 4*exp(-pi^2*(N+0.5).^2*t)/pi./(2*N+1);
for k = 0:n
X(:,k+1) = sin(pi*(k+0.5)*x(:));
end
heat = X*a‚Äô;
This adds enough terms of the series that exp{‚àíœÄ2(n + 1
2)2t} is less than some
small number (acc = 10‚àí8) in the Ô¨Ånal term. Note that the function ceil rounds
its argument upwards to the nearest integer (the ceiling function).
It is clear that ÀÜT ‚Üí0, and hence T ‚ÜíT1, as ÀÜt ‚Üí‚àû, as expected, and that the

126
FOURIER SERIES AND THE FOURIER TRANSFORM
Fig. 5.1. The solution of the initial‚Äìboundary value problem, (5.3) and (5.4), given by
(5.9), at various times.
heat Ô¨Çows out of the cool end of the bar, ÀÜx = 0. It is precisely problems of this sort,
which were Ô¨Årst solved by Fourier himself in the 1820s, that led to the development
of the theory of Fourier series. Note also that (5.9) with ÀÜt = 0 and ÀÜx = 1/2 and 1
leads to the interesting results
1 ‚àí1
3 + 1
5 ‚àí1
7 + 1
9 ‚àí1
11 + ¬∑ ¬∑ ¬∑ = œÄ
4 ,
1 + 1
3 ‚àí1
5 ‚àí1
7 + 1
9 + 1
11 ‚àí¬∑ ¬∑ ¬∑ =
œÄ
2
‚àö
2.
That the basis functions are orthogonal, as given by (5.7) and (5.8), should
come as no surprise, since Fourier‚Äôs equation is a Sturm‚ÄìLiouville equation, and
the Fourier series is just another example of a series expansion in terms of an or-
thogonal basis, similar to the Fourier‚ÄìLegendre and Fourier‚ÄìBessel series. We have
developed the Fourier series solution of this problem without too much attention to
mathematical rigour. However, a number of questions arise. Under what conditions
does a series of the form
‚àû

n=0
An sin

œÄ

n + 1
2

ÀÜx
	

5.1 GENERAL FOURIER SERIES
127
converge, and is this convergence uniform? Does every function have a convergent
Fourier series representation? These questions can also be asked of expansions in
terms of other sequences of orthogonal functions, such as the Fourier‚ÄìLegendre and
Fourier‚ÄìBessel series. The technical details are, however, rather more straightfor-
ward for the Fourier series.
5.1
General Fourier Series
The most general form for a Fourier series representation of a function, f(t), with
period T is
f(t) = 1
2A0 +
‚àû

n=1

An cos
2œÄnt
T

+ Bn sin
2œÄnt
T
	
,
(5.10)
and, using the method described above, the Fourier coeÔ¨Écients, An and Bn, are
given by
An = 2
T

1
2 T
‚àí1
2 T
cos
2œÄnt
T

f(t) dt,
Bn = 2
T

1
2 T
‚àí1
2 T
sin
2œÄnt
T

f(t) dt.
(5.11)
Note that if f is an odd function of t, An = 0, since cos(2œÄnt/T) is an even function
of t. This means that the resulting Fourier series is a sum of just the odd functions
sin(2œÄnt/T), a Fourier sine series. Similarly, if f is an even function of t, the
resulting expansion is a Fourier cosine series. Equations (5.10) and (5.11) can
also be written in a more compact, complex form as
f(t) =
‚àû

n=‚àí‚àû
Cne2œÄint/T ,
(5.12)
with complex Fourier coeÔ¨Écients
Cn = 1
2 (An ‚àíiBn) = 1
T

1
2 T
‚àí1
2 T
e‚àí2œÄint/T f(t) dt.
(5.13)
As we have seen, Fourier series can arise as solutions of diÔ¨Äerential equations,
but they are also useful for representing periodic functions in general. For example,
consider the function of period 2œÄ, deÔ¨Åned by
f(t) = t for ‚àíœÄ < t < œÄ, f(t) = f(t ¬± 2nœÄ) for n = 1, 2, . . . ,
which is piecewise continuous. Using (5.10) and (5.11), we conclude that
f(t) =
‚àû

n=1
2
n(‚àí1)n+1 sin nt = 2

sin t ‚àí1
2 sin 2t + 1
3 sin 3t ‚àí¬∑ ¬∑ ¬∑

.
(5.14)
The partial sums of this series,
fN(t) =
N

n=1
2
n(‚àí1)n+1 sin nt,

128
FOURIER SERIES AND THE FOURIER TRANSFORM
are shown in Figure 5.2 for various N. An inspection of this Ô¨Ågure suggests that
the series does indeed converge, but that the convergence is not uniform. At the
points of discontinuity, t = ¬±œÄ, (remember that f(t) has period 2œÄ, and therefore
f ‚ÜíœÄ as t ‚ÜíœÄ‚àí, but f ‚Üí‚àíœÄ as t ‚ÜíœÄ+) the series (5.14) gives the value
zero, the mean of the two limits as t approaches œÄ from above and below. In the
neighbourhood of t = ¬±œÄ, the diÔ¨Äerence between the partial sums, fN, and f(t)
appears not to become smaller as N increases, but the size of the region where
this occurs decreases ‚Äì a sure sign of nonuniform convergence. This nonuniform,
oscillatory behaviour close to discontinuities is known as Gibbs‚Äô phenomenon,
which we also met brieÔ¨Çy in Chapter 3.
Fig. 5.2. The partial sums of the Fourier series for t, (5.14), for N = 5, 15, 25 and 35.
The function being approximated is shown as a dotted line.
Before we proceed, we need to construct the Dirichlet kernel and prove the
Riemann‚ÄìLebesgue lemma, both of which are crucial in what follows.
Lemma 5.1 (The Dirichlet kernel) Let f be a bounded function of period T

5.1 GENERAL FOURIER SERIES
129
with f ‚ààPC[a, b]‚Ä†, and let
Sn(f, t) =
n

m=‚àín
Cme2œÄimt/T
(5.15)
be the nth partial sum of the Fourier series of f(t), with Cm given by (5.13). If
we deÔ¨Åne the Dirichlet kernel, Dn(t), to be a function of period T given for
‚àí1
2T < t < 1
2T by
Dn(t) =
Ô£±
Ô£≤
Ô£≥
sin{(2n+1) œÄt
T }
sin( œÄt
T )
when t Ã∏= 0,
2n + 1
when t = 0,
(5.16)
which is illustrated in Figure 5.3 for T = 2œÄ, then
Sn(f, t) = 1
T

1
2 T
‚àí1
2 T
f(œÑ)Dn(t ‚àíœÑ) dœÑ.
(5.17)
Fig. 5.3. The Dirichlet kernel, Dn for n = 10, 20, 30 and 40 and T = 2œÄ.
‚Ä† See Appendix 2.

130
FOURIER SERIES AND THE FOURIER TRANSFORM
Proof From (5.13) and (5.15),
Sn(f, t) =
n

m=‚àín
%
1
T

1
2 T
‚àí1
2 T
f(œÑ)e‚àí2œÄinœÑ/T dœÑ
&
e2œÄint/T
= 1
T

1
2 T
‚àí1
2 T
f(œÑ)
%
n

m=‚àín
e2œÄin(t‚àíœÑ)/T
&
dœÑ = 1
T

1
2 T
‚àí1
2 T
f(œÑ)Dn(t ‚àíœÑ) dœÑ,
where
Dn(t) =
n

m=‚àín
e2œÄint/T .
Clearly Dn(0) = 2n+1. When t Ã∏= 0, putting m = r ‚àín gives the simple geometric
progression
Dn(t) = e‚àí2œÄint/T
2n

r=0
e2œÄirt/T = e‚àí2œÄint/T e4œÄi(n+1/2)t/T ‚àí1
e2œÄit/T ‚àí1
= sin

(2n + 1) œÄt
T

sin
 œÄt
T

and the result is proved.
Lemma 5.1 tells us that the nth partial sum of a Fourier series can be written as
(5.17), a simple integral that involves the underlying function and the Dirichlet
kernel‚Ä†. As we can see in Figure 5.3, as n increases, the Dirichlet kernel becomes
more and more concentrated around the origin. What we need to show is that,
as n ‚Üí‚àû, the Dirichlet kernel just picks out the value f(œÑ) at œÑ = t in (5.17),
and hence that Sn(f, t) ‚Üíf(t) as n ‚Üí‚àû. As we will see in the next section, the
sequence {Dn(t)} is closely related to the delta function.
Lemma 5.2 (The Riemann‚ÄìLebesgue lemma) If f : [a, b] ‚ÜíR, f ‚ààPC[a, b]
and f is bounded, then
lim
Œª‚Üí‚àû
 b
a
f(t) sin Œªt dt = lim
Œª‚Üí‚àû
 b
a
f(t) cos Œªt dt = 0.
Proof
This seems intuitively obvious, since, as Œª increases, the period of oscilla-
tion of the trigonometric function becomes smaller, and the contributions from the
positive and negative parts of the integrand cancel out.
Let [c, d] be a subinterval of [a, b] on which f is continuous. DeÔ¨Åne
I(Œª) =
 d
c
f(t) sin Œªt dt.
(5.18)
The argument for f(t) cos Œªt is identical. By making the substitution t = œÑ + œÄ/Œª,
we obtain
I(Œª) = ‚àí
 d‚àíœÄ/Œª
c‚àíœÄ/Œª
f

œÑ + œÄ
Œª

sin ŒªœÑ dœÑ.
(5.19)
‚Ä† As we shall see, this is actually a convolution integral.

5.1 GENERAL FOURIER SERIES
131
Adding (5.18) and (5.19) gives
2I(Œª) = ‚àí
 c
c‚àíœÄ/Œª
f

t + œÄ
Œª

sin Œªt dt +
 d
d‚àíœÄ/Œª
f(t) sin Œªt dt
+
 d‚àíœÄ/Œª
c

f(t) ‚àíf

t + œÄ
Œª

sin Œªt dt.
Let K be the maximum value of |f| on [c, d], which we know exists by Theorem A2.1.
If we also assume that Œª is large enough that œÄ/Œª ‚©Ωd ‚àíc, then, remembering that
| sin Œªt| ‚©Ω1,
|I(Œª)| ‚©ΩKœÄ
Œª
+ 1
2
 d‚àíœÄ/Œª
c
f(t) ‚àíf

t + œÄ
Œª
 dt.
(5.20)
Now, since f is continuous on the closed interval [c, d], it is also uniformly continuous
there and, given any constant œµ, we can Ô¨Ånd a constant Œª0 such that
f(t) ‚àíf

t + œÄ
Œª
 <
œµ
d ‚àíc ‚àíœÄ/Œª ‚àÄŒª > Œª0.
Since we can also choose Œª0 so that KœÄ/Œª < œµ/2 ‚àÄŒª > Œª0, (5.20) shows that
|I(Œª)| < œµ, and hence that I(Œª) ‚Üí0 as Œª ‚Üí‚àû. Applying this result to all of the
subintervals of [a, b] on which f is continuous completes the proof.
Theorem 5.1 (The Fourier theorem) If f and f ‚Ä≤ are bounded functions of
period T with f, f ‚Ä≤ ‚ààPC[a, b], then the right hand side of (5.12), with Cn given by
(5.13), converges pointwise to
1
2

lim
œÑ‚Üít‚àíf(œÑ) + lim
œÑ‚Üít+ f(œÑ)
	
for ‚àí1
2T < t < 1
2T,
1
2

lim
œÑ‚Üí‚àí1
2 T + f(œÑ) +
lim
œÑ‚Üí1
2 T ‚àíf(œÑ)

for t = ‚àí1
2T or 1
2T.
Note that, at points where f(t) is continuous, (5.10) converges pointwise to f(t).
Proof At any point t = t0 ‚àà(‚àí1
2T, 1
2T),
f ‚Üíf‚àías t ‚Üít‚àí
0 , f ‚Üíf+ as t ‚Üít+
0 ,
since f is piecewise continuous, with f‚àí= f+ if f is continuous at t = t0. Similarly,
since f ‚Ä≤ is piecewise continuous, f has well-deÔ¨Åned left and right derivatives
f ‚Ä≤
‚àí(t0) = lim
h‚Üí0
f‚àí‚àíf(t0 ‚àíh)
h
,
f ‚Ä≤
+(t0) = lim
h‚Üí0
f(t0 + h) ‚àíf+
h
,
with f ‚Ä≤
‚àí(t0) = f ‚Ä≤
+(t0) if f ‚Ä≤ is continuous at t = t0. By the mean value theorem
(Theorem A2.2), for h small enough that f is continuous when t0 ‚àíh ‚©Ωt < t0,
f‚àí‚àíf(t0 ‚àíh) = f ‚Ä≤(c)h for some t0 ‚àíh < c < t0.

132
FOURIER SERIES AND THE FOURIER TRANSFORM
Since f ‚Ä≤ is bounded, there exists some M such that
|f‚àí‚àíf(t0 ‚àíh)| ‚©Ω1
2Mh.
After using a similar argument for t > t0, we arrive at
|f‚àí‚àíf(t0 ‚àíh)| + |f(t0 + h) ‚àíf+| ‚©ΩMh
(5.21)
for all h > 0 such that f is continuous when t0 ‚àíh < t0 < t0 + h.
Now, in (5.17) we make the change of variable œÑ = t + œÑ ‚Ä≤ and, using the facts
that Dn(t) = Dn(‚àít), and f and Dn have period T, we Ô¨Ånd that
Sn(f, t) = 1
T

1
2 T
‚àí1
2 T
f(t + œÑ ‚Ä≤)D(œÑ ‚Ä≤) dœÑ ‚Ä≤
= 1
T

1
2 T
‚àí1
2 T
1
2 {f(t + œÑ ‚Ä≤) + f(t ‚àíœÑ ‚Ä≤)} D(œÑ ‚Ä≤) dœÑ ‚Ä≤,
and hence that
Sn(f, t) ‚àí1
2 (f‚àí+ f+) = 1
T

1
2 T
‚àí1
2 T
g(t, œÑ ‚Ä≤) sin

(2n + 1)œÄœÑ ‚Ä≤
T
	
dœÑ ‚Ä≤,
(5.22)
where
g(t, œÑ ‚Ä≤) = f(t + œÑ ‚Ä≤) ‚àíf+ + f(t ‚àíœÑ ‚Ä≤) ‚àíf‚àí
2 sin(œÄœÑ ‚Ä≤/T)
.
Considered as a function of œÑ ‚Ä≤, g(t, œÑ ‚Ä≤) is bounded and piecewise continuous, except
possibly at œÑ ‚Ä≤ = 0. However, for suÔ¨Éciently small œÑ ‚Ä≤, (5.21) shows that
|g(t, œÑ ‚Ä≤)| ‚©Ω
M|œÑ ‚Ä≤|
2| sin(œÄœÑ ‚Ä≤/T)|,
which is bounded as œÑ ‚Ä≤ ‚Üí0. The function g therefore satisÔ¨Åes the conditions of the
Riemann‚ÄìLebesgue lemma, and we conclude from (5.22) that Sn ‚Üí1
2(f‚àí+ f+) as
n ‚Üí‚àûand the result is proved. By exploiting the periodicity of f, the same proof
works for t = ‚àí1
2T or 1
2T with minor modiÔ¨Åcations.
Theorem 5.2 (Uniform convergence of Fourier series) For a function f(t)
that satisÔ¨Åes the conditions of Theorem 5.1, in any closed subinterval of [‚àí1
2T, 1
2T],
the right hand side of (5.12), with Cn given by (5.13), converges uniformly to f(t)
if and only if f(t) is continuous there.
We will not give a proof, but note that this is consistent with our earlier discussion
of Gibbs‚Äô phenomenon.
Finally, returning to the question of whether all bounded, piecewise continuous,
periodic functions have a convergent Fourier series expansion, we note that this

5.2 THE FOURIER TRANSFORM
133
diÔ¨Écult question was not Ô¨Ånally resolved until 1964, when Carleson showed that
such functions exist whose Fourier series expansions diverge at a Ô¨Ånite or countably-
inÔ¨Ånite set of points (for example, the rational numbers). Of course, as we have
seen in Theorem 5.1, these functions cannot possess a bounded piecewise continuous
derivative, even though they are continuous, and are clearly rather peculiar (see, for
example, Figure A2.1). However, such functions do arise in the theory of Brownian
motion and stochastic processes, which are widely applicable to real world problems,
for example in models of Ô¨Ånancial derivatives. For further details, see K¬®orner (1988),
and references therein.
5.2
The Fourier Transform
In order to motivate the deÔ¨Ånition of the Fourier transform, consider (5.12) and
(5.13), which deÔ¨Åne the complex form of the Fourier series expansion of a periodic
function, f(t). What happens as the period, T, tends to inÔ¨Ånity? Combining (5.12)
and (5.13) gives
f(t) = 1
T
‚àû

n=‚àí‚àû
e2œÄint/T

1
2 T
‚àí1
2 T
e‚àí2œÄint‚Ä≤/T f(t‚Ä≤) dt‚Ä≤.
If we now let kn = 2œÄn/T and ‚àÜkn = kn ‚àíkn‚àí1 = 2œÄ/T, we have
f(t) = 1
2œÄ
‚àû

n=‚àí‚àû
‚àÜkneiknt

1
2 T
‚àí1
2 T
e‚àíiknt‚Ä≤f(t‚Ä≤) dt‚Ä≤.
If we now momentarily abandon mathematical rigour, as T ‚Üí‚àû, kn becomes a
continuous variable and the summation becomes an integral, so that we obtain
f(t) = 1
2œÄ
 ‚àû
‚àí‚àû
eikt
 ‚àû
‚àí‚àû
e‚àíikt‚Ä≤f(t‚Ä≤) dt‚Ä≤ dk,
(5.23)
which is known as the Fourier integral. We will prove this result rigorously later.
If we now deÔ¨Åne the Fourier transform of f(t) as
F[f(t)] = Àúf(k) =
 ‚àû
‚àí‚àû
eiktf(t) dt,
(5.24)
we immediately have an inversion formula,
f(t) = 1
2œÄ
 ‚àû
‚àí‚àû
e‚àíikt Àúf(k) dk.
(5.25)
Note that some texts alter the deÔ¨Ånition of the Fourier transform (and its inverse) to
take account of the factor of 1/2œÄ in a diÔ¨Äerent way, for example with the transform
and its inverse each multiplied by a factor of 1/
‚àö
2œÄ. This is a minor, but irritating,
detail, which does not aÔ¨Äect the basic ideas.
The Fourier transform maps a function of t to a function of k. In the same
way as the Fourier series expansion of a periodic function decomposes the function
into its constituent harmonic parts, the Fourier transform produces a function of
a continuous variable whose value indicates the frequency content of the original

134
FOURIER SERIES AND THE FOURIER TRANSFORM
function. This has led to the widespread use of the Fourier transform to analyze the
form of time-varying signals, for example in electrical engineering and seismology.
We will be more concerned here with the use of the Fourier transform to solve
partial diÔ¨Äerential equations.
Before we can proceed, we need to give (5.24) and (5.25) a Ô¨Årmer mathematical
basis. What restrictions must we place on the function f(t) for its Fourier trans-
form to exist? This is a diÔ¨Écult question, and our approach is to treat f(t) as a
generalized function. The advantage of this is that every generalized function
has a Fourier transform and an inverse Fourier transform, and that the ordinary
functions in whose Fourier transforms we are interested form a subset of the gen-
eralized functions. We will not go into great detail, for which the reader is referred
to Lighthill (1958), the classic, and very accessible, introduction to the subject.
5.2.1
Generalized Functions
We begin with some deÔ¨Ånitions. A good function, g(x), is a function in C‚àû(R)
that decays rapidly enough that g(x) and all of its derivatives tend to zero faster
than |x|‚àíN as x ‚Üí¬±‚àûfor all N > 0. For example, e‚àíx2 and sech2x are good
functions.
A sequence of good functions, {fn(x)}, is said to be regular if, for any good
function F(x),
lim
n‚Üí‚àû
 ‚àû
‚àí‚àû
fn(x)F(x) dx
(5.26)
exists. For example fn(x) = G(x)/n is a regular sequence for any good function
G(x), with
lim
n‚Üí‚àû
 ‚àû
‚àí‚àû
fn(x)F(x) dx = lim
n‚Üí‚àû
1
n
 ‚àû
‚àí‚àû
G(x)F(x) dx = 0.
Two regular sequences of good functions are equivalent if, for any good function
F(x), the limit (5.26) exists and is the same for each sequence. For example, the
regular sequences {G(x)/n} and {G(x)/n2} are equivalent.
A generalized function, f(x), is a regular sequence of good functions, and two
generalized functions are equal if their deÔ¨Åning sequences are equivalent. Gener-
alized functions are therefore only deÔ¨Åned in terms of their action on integrals of
good functions, with
 ‚àû
‚àí‚àû
f(x)F(x) dx = lim
n‚Üí‚àû
 ‚àû
‚àí‚àû
fn(x)F(x) dx,
for any good function F(x).
If f(x) is an ordinary function such that (1 + x2)‚àíNf(x) is integrable from ‚àí‚àû
to ‚àûfor some N, then the generalized function f(x) equivalent to the ordinary
function is deÔ¨Åned as any sequence of good functions {fn(x)} such that, for any
good function F(x),
lim
n‚Üí‚àû
 ‚àû
‚àí‚àû
fn(x)F(x) dx =
 ‚àû
‚àí‚àû
f(x)F(x) dx.

5.2 THE FOURIER TRANSFORM
135
For example, the generalized function equivalent to zero can be represented by
either of the sequences {G(x)/n} and {G(x)/n2}.
The zero generalized function is very simple. Let‚Äôs consider some more useful
generalized functions.
‚Äî The unit function, I(x), is deÔ¨Åned so that for any good function F(x),
 ‚àû
‚àí‚àû
I(x)F(x) dx =
 ‚àû
‚àí‚àû
F(x) dx.
A useful sequence of good functions that deÔ¨Ånes the unit function is {e‚àíx2/4n}.
The unit function is the generalized function equivalent to the ordinary function
f(x) = 1.
‚Äî The Heaviside function, H(x), is deÔ¨Åned so that for any good function F(x),
 ‚àû
‚àí‚àû
H(x)F(x) dx =
 ‚àû
0
F(x) dx.
The generalized function H(x) is equivalent to the ordinary step function‚Ä†
H(x) =
 0
for x < 0,
1
for x > 0.
‚Äî The sign function, sgn(x), is deÔ¨Åned so that for any good function F(x),
 ‚àû
‚àí‚àû
sgn(x)F(x) dx =
 ‚àû
0
F(x) dx ‚àí
 0
‚àí‚àû
F(x) dx.
Then sgn(x) can be identiÔ¨Åed with the ordinary function
sgn(x) =
 ‚àí1
for x < 0,
1
for x > 0.
In fact, sgn(x) = 2H(x) ‚àíI(x), since we can note that
 ‚àû
‚àí‚àû
{2H(x) ‚àíI(x)} g(x) dx = 2
 ‚àû
‚àí‚àû
H(x)g(x) dx ‚àí
 ‚àû
‚àí‚àû
I(x)g(x) dx
= 2
 ‚àû
0
g(x) dx ‚àí
 ‚àû
‚àí‚àû
g(x) dx =
 ‚àû
0
g(x) dx ‚àí
 0
‚àí‚àû
g(x) dx,
using the deÔ¨Ånition of the Heaviside and unit functions. This is just the deÔ¨Ånition
of the function sgn(x).
‚Äî The Dirac delta function, Œ¥(x), is deÔ¨Åned so that for any good function F(x),
 ‚àû
‚àí‚àû
Œ¥(x)F(x) dx = F(0).
No ordinary function can be equivalent to the delta function. We can see from
‚Ä† Since generalized functions are only deÔ¨Åned through their action on integrals of good functions,
the value of H at x = 0 does not have any signiÔ¨Åcance in this context.

136
FOURIER SERIES AND THE FOURIER TRANSFORM
(5.17) and Theorem 5.1 that a sequence based on the Dirichlet kernel, fn(x) =
Dn(x)/2œÄ = sin

(2n + 1) œÄt
T

/2œÄ sin
 œÄt
T

, satisÔ¨Åes
F(0) = lim
n‚Üí‚àû

1
2 T
‚àí1
2 T
fn(x)F(x) dx.
As we can see in Figure 5.3, the function becomes more and more concentrated
about the origin as n increases, eÔ¨Äectively plucking the value of F(0) out of the
integral. However, this only works on a Ô¨Ånite domain. On an inÔ¨Ånite domain,
the sequence
fn(x) =
n
œÄ
1/2
e‚àínx2,
(5.27)
which is illustrated in Figure 5.4 for various n, is a useful way of deÔ¨Åning Œ¥(x).
Fig. 5.4. The sequence (5.27), which is equivalent to Œ¥(x).
5.2.2
Derivatives of Generalized Functions
Derivatives of generalized functions are deÔ¨Åned by the derivative of any of the
equivalent sequences of good functions. Since we can integrate by parts using any

5.2 THE FOURIER TRANSFORM
137
member of the sequence, we can also, formally, do so for the equivalent generalized
function, treating it as if it were zero at inÔ¨Ånity. For example
 ‚àû
‚àí‚àû
Œ¥‚Ä≤(x)F(x) dx = ‚àí
 ‚àû
‚àí‚àû
Œ¥(x)F ‚Ä≤(x) dx = ‚àíF ‚Ä≤(0).
This does not allow us to represent Œ¥‚Ä≤(x) in terms of other generalized functions,
but does show us how it acts in an integral, which is all we need to know.
We can also show that H‚Ä≤(x) = Œ¥(x), since
 ‚àû
‚àí‚àû
H‚Ä≤(x)F(x) dx = ‚àí
 ‚àû
‚àí‚àû
H(x)F ‚Ä≤(x) dx = ‚àí
 ‚àû
0
F ‚Ä≤(x) dx = ‚àí[F(x)]‚àû
0 = F(0).
Another useful result is
f(x)Œ¥(x) = f(0)Œ¥(x).
The proof is straightforward, since
 ‚àû
‚àí‚àû
f(x)Œ¥(x)F(x) dx = f(0)F(0),
and
 ‚àû
‚àí‚àû
f(0)Œ¥(x)F(x) dx =
 ‚àû
‚àí‚àû
Œ¥(x)[f(0)F(x)]dx = f(0)F(0).
We can deÔ¨Åne the modulus function in terms of the function sgn(x) through
|x| = x sgn(x). Now consider the derivative of the modulus function. Using the
product rule, which is valid because it works for the equivalent sequence of good
functions,
d
dx|x| = d
dx {x sgn(x)} = x d
dx {sgn(x)} + sgn(x) d
dx(x).
We can now use the fact that sgn(x) = 2H(x) ‚àíI(x) to show that
d
dx|x| = x d
dx {2H(x) ‚àíI(x)} + sgn(x) = 2xŒ¥(x) + sgn(x) = sgn(x),
since xŒ¥(x) = 0.
5.2.3
Fourier Transforms of Generalized Functions
As we have seen, the Fourier transform of a function f(x) is deÔ¨Åned as
F[f(x)] =
 ‚àû
‚àí‚àû
eikxf(x) dx.
For example, consider the Fourier transform of the function e‚àí|x|, which, using the
deÔ¨Ånition, is
F
(
e‚àí|x|)
=
 0
‚àí‚àû
exeikx dx +
 ‚àû
0
e‚àíxeikx dx
=
1
1 + ik ‚àí
1
‚àí1 + ik =
2
1 + k2 .
(5.28)

138
FOURIER SERIES AND THE FOURIER TRANSFORM
We would also like to know the Fourier transform of a constant, c. However, it
is not clear whether
F[c] = c
 ‚àû
‚àí‚àû
eikx dx
is a well-deÔ¨Åned integral. Instead we note that, treated as a generalized function,
c = cI(x), and we can deal with the Fourier transform of an equivalent sequence
instead, for example
F[ce‚àíx2/4n] = c
 ‚àû
‚àí‚àû
eikx‚àíx2/4n dx = ce‚àínk2  ‚àû
‚àí‚àû
e‚àí(x/2‚àön‚àíik‚àön)
2
dx.
By writing z = x ‚àí2ikn and deforming the contour of integration in the complex
plane (see Exercise 5.10 for an alternative method), we Ô¨Ånd that
F[ce‚àíx2/4n] = ce‚àínk2  ‚àû
‚àí‚àû
e‚àíz2/4n dz = 2œÄc
*n
œÄ e‚àínk2,
using (3.5).
Since {e‚àíx2/4n} is a sequence equivalent to the unit function, and
{# n
œÄe‚àínk2} is a sequence equivalent to the delta function, we conclude that
F[c] = 2œÄcŒ¥(k).
Another useful result is that, if F[f(x)] = Àúf(k), F[f(ax)] = Àúf(k/a)/a for a > 0.
To prove this, note that from the deÔ¨Ånition
F[f(ax)] =
 ‚àû
‚àí‚àû
eikxf(ax) dx.
Making the change of variable y = ax gives
F[f(ax)] = 1
a
 ‚àû
‚àí‚àû
eiky/af(y) dy,
which is equal to Àúf(k/a)/a as required. As an example of how this result can be
used, we know that F[e‚àí|x|] = 2/(1 + k2), so
F[e‚àía|x|] = 1
a
2
1 +
 k
a
2 =
2a
a2 + k2 .
Finally, the Fourier transformation is clearly a linear transformation, so that
F[Œ±f + Œ≤g] = Œ±F[f] + Œ≤F[g].
(5.29)
This means that linear combinations of functions can be transformed separately.
5.2.4
The Inverse Fourier Transform
If we can show that (5.25) holds for all good functions, it follows that it holds
for all generalized functions. We begin with a useful lemma.
Lemma 5.3 The Fourier transform of a good function is a good function.

5.2 THE FOURIER TRANSFORM
139
Proof If f(x) is a good function, its Fourier transform clearly exists and is given
by
F[f] = Àúf(k) =
 ‚àû
‚àí‚àû
eikxf(x) dx.
If we diÔ¨Äerentiate p times and integrate by parts N times we Ô¨Ånd that
 Àúf (p)(k)
 ‚©Ω

(‚àí1)N
(ik)N
 ‚àû
‚àí‚àû
eikx d N
dxN {(ix)pf(x)} dx

‚©Ω
1
|k|N
 ‚àû
‚àí‚àû

d N
dxN {xpf(x)}
 dx.
All derivatives of Àúf therefore decay at least as fast as |k|‚àíN as |k| ‚Üí‚àûfor any
N > 0, and hence Àúf is a good function.
Theorem 5.3 (The Fourier inversion theorem) If f(x) is a good function with
Fourier transform
Àúf(k) =
 ‚àû
‚àí‚àû
eikxf(x) dx,
then the inverse Fourier transform is given by
f(x) = 1
2œÄ
 ‚àû
‚àí‚àû
e‚àíikx Àúf(k) dk.
Proof Firstly, we note that for œµ > 0,
F
(
e‚àíœµx2 Àúf(‚àíx)
)
=
 ‚àû
‚àí‚àû
eikx‚àíœµx2  ‚àû
‚àí‚àû
e‚àíixtf(t) dt
	
dx.
Since f is a good function, we can exchange the order of integration and arrive at
F
(
e‚àíœµx2 Àúf(‚àíx)
)
=
 ‚àû
‚àí‚àû
f(t)
 ‚àû
‚àí‚àû
ei(k‚àít)x‚àíœµx2 dx dt
=
 ‚àû
‚àí‚àû
f(t)e‚àí(k‚àít)2/4œµ
 ‚àû
‚àí‚àû
exp

‚àí

œµ1/2x ‚àíi(k ‚àít)
2œµ
2
dx dt.
Now, by making the change of variable x = ¬Øx + i(k ‚àít)/2œµ3/2, checking that this
change of contour is possible in the complex x-plane, we Ô¨Ånd that
 ‚àû
‚àí‚àû
exp

‚àí

œµ1/2x ‚àíi(k ‚àít)
2œµ
2
dx =
 ‚àû
‚àí‚àû
e‚àíœµ¬Øx2 d¬Øx =
*œÄ
œµ .
This means that
F
(
e‚àíœµx2 Àúf(‚àíx)
)
=
*œÄ
œµ
 ‚àû
‚àí‚àû
e‚àí(k‚àít)2/4œµf(t) dt.

140
FOURIER SERIES AND THE FOURIER TRANSFORM
In addition,
*œÄ
œµ
 ‚àû
‚àí‚àû
e‚àí(k‚àít)2/4œµ dt =
*œÄ
œµ
 ‚àû
‚àí‚àû
e‚àít2/4œµ dt = 2œÄ,
so we can write
1
2œÄ F
(
e‚àíœµx2 Àúf(‚àíx)
)
‚àíf(k) = 1
2œÄ
*œÄ
œµ
 ‚àû
‚àí‚àû
e‚àí(k‚àít)2/4œµ {f(t) ‚àíf(k)} dt.
Since f is a good function,

f(t) ‚àíf(k)
t ‚àík
 ‚©Ωmax
x‚ààR |f ‚Ä≤(x)| ,
and hence

1
2œÄ F
(
e‚àíœµx2 Àúf(‚àíx)
)
‚àíf(k)
 ‚©Ω1
2œÄ
*œÄ
œµ max
x‚ààR |f ‚Ä≤(x)|
 ‚àû
‚àí‚àû
e‚àí(k‚àít)2/4œµ|t ‚àík| dt
= 1
2œÄ
*œÄ
œµ max
x‚ààR |f ‚Ä≤(x)| 4œµ
 ‚àû
‚àí‚àû
e‚àíX2|X| dX ‚Üí0 as œµ ‚Üí0.
We conclude that
f(k) = 1
2œÄ F
(
Àúf(‚àíx)
)
= 1
2œÄ
 ‚àû
‚àí‚àû
eikx
 ‚àû
‚àí‚àû
e‚àíixtf(t) dt.
This is precisely the Fourier integral, (5.23), and hence the result is proved.
5.2.5
Transforms of Derivatives and Convolutions
Fourier transforms are an appropriate tool for solving diÔ¨Äerential equations on
the unbounded domain ‚àí‚àû< x < ‚àû. In order to proceed, we need to be able
to Ô¨Ånd the Fourier transform of a derivative. For good functions, this can be done
using integration by parts. We Ô¨Ånd that
F[f ‚Ä≤(x)] =
 ‚àû
‚àí‚àû
f ‚Ä≤(x)eikx dx = ‚àíik
 ‚àû
‚àí‚àû
f(x)eikx dx = ‚àíikF[f],
since the good function f must tend to zero as x ‚Üí¬±‚àû. Since generalized func-
tions are deÔ¨Åned in terms of sequences of good functions, this result also holds for
generalized functions. Similarly, the second derivative is
F[f ‚Ä≤‚Ä≤(x)] = ‚àík2F[f].
We can also deÔ¨Åne the convolution of two functions f and g as
f ‚àóg =
 ‚àû
‚àí‚àû
f(y)g(x ‚àíy) dy,
which is a function of x only. Note that f ‚àóg = g ‚àóf. As we shall see below, the
solutions of diÔ¨Äerential equations can often be written as a convolution because of
the key property
F[f ‚àóg] = F[f]F[g].

5.3 GREEN‚ÄôS FUNCTIONS REVISITED
141
To derive this, we write down the deÔ¨Ånition of the Fourier transform of f ‚àóg,
F[f ‚àóg] =
 ‚àû
‚àí‚àû
eikx
 ‚àû
‚àí‚àû
f(y)g(x ‚àíy) dy
	
dx.
Note that the factor eikx is independent of y and so can be taken inside the inner
integral to give
F[f ‚àóg] =
 ‚àû
x=‚àí‚àû
 ‚àû
y=‚àí‚àû
eikxf(y)g(x ‚àíy) dy dx.
Since the limits of integration are independent of one another, we can exchange the
order of integration so that
F[f ‚àóg] =
 ‚àû
y=‚àí‚àû
 ‚àû
x=‚àí‚àû
eikxf(y)g(x ‚àíy) dx dy.
Now f(y) is independent of x, and can be extracted from the inner integral to give
F[f ‚àóg] =
 ‚àû
y=‚àí‚àû
f(y)
 ‚àû
x=‚àí‚àû
eikxg(x ‚àíy) dx
	
dy.
By making the transformation z = x ‚àíy (so that dz = dx) in the inner integral,
we have
F[f ‚àóg] =
 ‚àû
y=‚àí‚àû
f(y)
 ‚àû
z=‚àí‚àû
eik(z+y)g(z) dz
	
dy,
and now extracting the factor eiky, since this is independent of z, allows us to write
F[f ‚àóg] =
 ‚àû
y=‚àí‚àû
f(y)eiky dy
 ‚àû
z=‚àí‚àû
eikzg(z) dz = F[f]F[g].
5.3
Green‚Äôs Functions Revisited
Let‚Äôs, for the moment, forget our previous deÔ¨Ånition of a Green‚Äôs function (Sec-
tion 4.1.2), and deÔ¨Åne it instead, for a linear operator L with domain ‚àí‚àû< x < ‚àû,
as the solution of the diÔ¨Äerential equation LG = Œ¥(x) subject to G ‚Üí0 as |x| ‚Üí‚àû.
If we assume that G is a good function, we can use the Fourier transform to Ô¨Ånd
G. For example, consider the operator
L ‚â°d 2
dx2 ‚àí1,
so that LG = Œ¥ is
d 2G
dx2 ‚àíG = Œ¥(x).
We will solve this equation subject to G ‚Üí0 as |x| ‚Üí‚àû. Taking the Fourier
transform of both sides of this equation and exploiting the linearity of the transform,
(5.29), gives
F
d 2G
dx2

‚àíF[G] = F[Œ¥(x)].

142
FOURIER SERIES AND THE FOURIER TRANSFORM
Firstly we need to determine the Fourier transform of the delta function. From the
deÔ¨Ånition
F[Œ¥(x)] =
 ‚àû
‚àí‚àû
eikxŒ¥(x) dx =

eikx
x=0 = 1.
Therefore
‚àík2F[G] ‚àíF[G] = 1,
using the fact that F[G‚Ä≤‚Ä≤] = ‚àík2F[G]. After rearrangement this becomes
F[G] = ‚àí
1
k2 + 1,
which we know from (5.28) means that
G(x) = ‚àí1
2e‚àí|x|.
Why should we want to construct such a Green‚Äôs function? As before, the answer
is, to be able to solve the inhomogeneous diÔ¨Äerential equation. Suppose that we
need to solve LœÜ = P. The solution is œÜ = G ‚àóP. To see this, note that
L(G ‚àóP) = L
 ‚àû
‚àí‚àû
G(x ‚àíy)P(y) dy =
 ‚àû
‚àí‚àû
LG(x ‚àíy)P(y) dy
=
 ‚àû
‚àí‚àû
Œ¥(x ‚àíy)P(y) dy = P(x).
Once we know the Green‚Äôs function we can therefore write down the solution of
LœÜ = P as œÜ = G ‚àóP.
There are both diÔ¨Äerences and similarities between this deÔ¨Ånition of the Green‚Äôs
function, which is sometimes referred to as the free space Green‚Äôs function, and
the deÔ¨Ånition that we gave in Section 4.1.2. Firstly, the free space Green‚Äôs function
depends only on x, whilst the other deÔ¨Ånition depends upon both x and s. We can
see some similarities by considering the self-adjoint problem
d
dx

p(x)dG
dx

+ q(x)G = Œ¥(x).
On any interval that does not contain the point x = 0, G is clearly the solution
of the homogeneous problem, as before, and is continuous there. If we integrate
between x = ‚àíœµ and x = œµ, we get

p(x)dG
dx
œµ
‚àíœµ
+
 œµ
‚àíœµ
q(x)G(x) dx =
 œµ
‚àíœµ
Œ¥(x) dx = 1.
If p(x) is continuous at x = 0, when we take the limit œµ ‚Üí0 this reduces to
dG
dx
x=0+
x=0‚àí=
1
p(0),
which, apart from a sign diÔ¨Äerence, is the result that we obtained in Section 4.1.2.

5.4 SOLUTION OF LAPLACE‚ÄôS EQUATION USING FOURIER TRANSFORMS
143
The end result from constructing the Green‚Äôs function is the same as in Sec-
tion 4.1.2. We can express the solution of the inhomogeneous boundary value prob-
lem as an integral that involves the Green‚Äôs function and the inhomogeneous term.
We will return to consider the Green‚Äôs function for partial diÔ¨Äerential equations in
Section 5.5.
5.4
Solution of Laplace‚Äôs Equation Using Fourier Transforms
Let‚Äôs consider the problem of solving Laplace‚Äôs equation, ‚àá2œÜ = 0, for y > 0 subject
to œÜ = f(x) on y = 0 and œÜ bounded as y ‚Üí‚àû. Boundary value problems for
partial diÔ¨Äerential equations where the value of the dependent variable is prescribed
on the boundary are often referred to as Dirichlet problems. We can solve this
Dirichlet problem using a Fourier transform with respect to x. We deÔ¨Åne
ÀúœÜ(k, y) =
 ‚àû
‚àí‚àû
eikxœÜ(x, y) dx = F[œÜ].
We begin by taking the Fourier transform of Laplace‚Äôs equation, noting that
F
‚àÇ2œÜ
‚àÇy2

= ‚àÇ2
‚àÇy2 F [œÜ] = ‚àÇ2 ÀúœÜ
‚àÇy2 ,
which is easily veriÔ¨Åed from the deÔ¨Ånition of the transform. This gives us
F
‚àÇ2œÜ
‚àÇy2 + ‚àÇ2œÜ
‚àÇx2

= ‚àÇ2 ÀúœÜ
‚àÇy2 ‚àík2 ÀúœÜ = 0.
This has solution ÀúœÜ = A(k)e|k|y+B(k)e‚àí|k|y. However, we require that œÜ is bounded
as y ‚Üí‚àû, which gives A(k) = 0, and hence
ÀúœÜ(k, y) = B(k)e‚àí|k|y.
It now remains to satisfy the condition at y = 0, namely œÜ(x, 0) = f(x). We take
the Fourier transform of this condition to give ÀúœÜ(k, 0) = Àúf(k), where Àúf(k) = F[f].
By putting y = 0 we therefore Ô¨Ånd that B(k) = Àúf(k), so that
ÀúœÜ(k, y) = Àúf(k)e‚àí|k|y.
We can invert this Fourier transform of the solution using the convolution theo-
rem. Since
F‚àí1[ Àúf Àúg] = F‚àí1[F[f] F[g]] = f ‚àóg,
the solution is just the convolution of the boundary condition, f(x), with the inverse
transform of Àúg(k) = e‚àí|k|y. To Ô¨Ånd g(x) = F‚àí1[Àúg(k)] = F‚àí1[e‚àí|k|y], we note from
(5.28) that F[e‚àí|x|] = 2/(1 + k2), and exploit the fact that F[f(ax)] = Àúf(k/a)/a,
so that F[e‚àía|x|] = 2a/(a2 + k2), and hence e‚àía|x| = F‚àí1[2a/(a2 + k2)]. Using the
formula for the inverse transform gives
e‚àía|x| = 1
2œÄ
 ‚àû
‚àí‚àû
e‚àíikx
2a
a2 + k2 dk.

144
FOURIER SERIES AND THE FOURIER TRANSFORM
We can exploit the similarity between the Fourier transform and its inverse by
making the transformation k ‚Üí‚àíx, x ‚Üík, which leads to
e‚àía|k| = 1
2œÄ
 ‚àû
‚àí‚àû
eikx
2a
a2 + x2 dx = 1
2œÄ F

2a
a2 + x2

,
and hence
F‚àí1 (
e‚àí|k|y)
=
y
œÄ(y2 + x2).
In the deÔ¨Ånition of the convolution we use the variable Œæ as the dummy variable
rather than y to avoid confusion with the spatial coordinate in the problem, so that
f ‚àóg =
 ‚àû
‚àí‚àû
f(Œæ)g(x ‚àíŒæ) dŒæ.
The solution œÜ can now be written as the convolution integral
œÜ(x, y) =
 ‚àû
‚àí‚àû
f(Œæ)
y
œÄ{y2 + (x ‚àíŒæ)2} dŒæ = y
œÄ
 ‚àû
‚àí‚àû
f(Œæ)
y2 + (x ‚àíŒæ)2 dŒæ.
(5.30)
As an example, consider the two-dimensional, inviscid, irrotational Ô¨Çow in the upper
half plane (see Section 2.6.2), driven by the Dirichlet condition œÜ(x, 0) = f(x),
where
f(x) =
 1
for ‚àí1 < x < 1,
0
elsewhere.
Using the formula we have derived,
œÜ = y
œÄ
 1
‚àí1
dŒæ
y2 + (x ‚àíŒæ)2 = y
œÄ
1
y tan‚àí1
Œæ ‚àíx
y
1
‚àí1
= 1
œÄ

tan‚àí1
1 ‚àíx
y

+ tan‚àí1
1 + x
y
	
.
(5.31)
Figure 5.5 shows some contours of equal œÜ.‚Ä†
Finally, let‚Äôs consider the Neumann problem for Laplace‚Äôs equation in the
upper half plane, y > 0. This is the same as the Dirichlet problem except that the
boundary condition is in terms of a derivative, with ‚àÇœÜ/‚àÇy = f(x) at y = 0. As
before we Ô¨Ånd that ÀúœÜ(k, y) = B(k)e‚àí|k|y, and the condition at y = 0 tells us that
B(k) = ‚àí
Àúf(k)
|k| ,
and hence
ÀúœÜ(k, y) = ‚àíÀúf(k)e‚àí|k|y
|k|
.
In order to invert this Fourier transform we recall that
 ‚àû
‚àí‚àû
eikx
2y
y2 + x2 dx = 2œÄe‚àí|k|y,
‚Ä† See Section 2.6.2 for details of how to create this type of contour plot in MATLAB.

5.5 GENERALIZATION TO HIGHER DIMENSIONS
145
2
4
6
8
10
‚àí4
‚àí2
‚àí6
‚àí8
‚àí10
Fig. 5.5. Contours of constant potential function, œÜ, given by (5.31).
and integrate both sides with respect to y to obtain
 ‚àû
x=‚àí‚àû
eikx log(y2 + x2) dx = ‚àí2œÄ e‚àí|k|y
|k|
.
In other words
‚àíe‚àí|k|y
|k|
= 1
2œÄ F
!
log

y2 + x2"
,
and hence the solution is
œÜ(x, y) = 1
2œÄ
 ‚àû
‚àí‚àû
f(Œæ) log{y2 + (x ‚àíŒæ)2} dŒæ.
5.5
Generalization to Higher Dimensions
The theory of Fourier transforms and generalized functions can be extended to
higher dimensions. This allows us to use the techniques that we have developed
above to solve other partial diÔ¨Äerential equations.
5.5.1
The Delta Function in Higher Dimensions
If we let x = (x1, x2, . . . , xn) be a vector in Rn, we can deÔ¨Åne the delta function
in Rn, Œ¥(x), through the integral

Rn Œ¥(x)F(x) d nx = F(0).

146
FOURIER SERIES AND THE FOURIER TRANSFORM
This is a multiple integral over the whole of Rn, and F is a good function in each
of the coordinates xi.
Cartesian Coordinates
In Cartesian coordinates we have Œ¥(x) = Œ¥(x1)Œ¥(x2) . . . Œ¥(xn), since

Rn Œ¥(x)F(x) d nx
=

Rn‚àí1 Œ¥(x1) . . . Œ¥(xn‚àí1)

R
Œ¥(xn)F(x1, . . . , xn‚àí1, xn) dxn
	
d n‚àí1x
=

Rn‚àí1 Œ¥(x1) . . . Œ¥(xn‚àí1)F(x1, . . . , xn‚àí1, 0)d n‚àí1x = ¬∑ ¬∑ ¬∑ = F(0, 0, . . . , 0).
Plane Polar Coordinates
In terms of standard plane polar coordinates, (r, Œ∏), in R2, Œ¥(x) must be isotropic,
in other words independent of Œ∏, and a multiple of Œ¥(r). We therefore let Œ¥(x) =
a(r)Œ¥(r) and, noting that d 2x = r dr dŒ∏,
 2œÄ
Œ∏=0
 ‚àû
r=0
Œ¥(x) d 2x = 2œÄ
 ‚àû
0
a(r)Œ¥(r)r dr.
By symmetry,
 ‚àû
0
Œ¥(r) dr = 1
2,
so we can take a(r) = 1/œÄr, and hence
Œ¥(x) = 1
œÄrŒ¥(r).
Spherical Polar Coordinates
In R3, the isotropic volume element can be written as d 3x = 4œÄr2 dr, the volume
of a thin, spherical shell, where r is now |x|, the distance from the origin. Again,
the delta function must be a multiple of Œ¥(r), and the same argument gives
Œ¥(x) =
1
2œÄr2 Œ¥(r).
5.5.2
Fourier Transforms in Higher Dimensions
If f(x) = f(x1, x2, . . . , xn) and k = (k1, k2, . . . , kn), we can deÔ¨Åne the Fourier
transform of f as
Àúf(k) =

Rn f(x)eik¬∑x d nx.
(5.32)
For example, in R3
Àúf(k1, k2, k3) =
 ‚àû
x3=‚àí‚àû
 ‚àû
x2=‚àí‚àû
 ‚àû
x1=‚àí‚àû
f(x1, x2, x3)ei(k1x1+k2x2+k3x3) dx1 dx2 dx3.

5.5 GENERALIZATION TO HIGHER DIMENSIONS
147
We can proceed as we did for one-dimensional functions and show, although we will
not provide the details, that:
(i) If f(x) is a generalized function, then its Fourier transform exists as a gen-
eralized function.
(ii) The inversion formula is
f(x) =
1
(2œÄ)n

Rn
Àúf(k)e‚àíik¬∑x d nk.
(5.33)
The inversion of Fourier transforms in higher dimensions is considerably more dif-
Ô¨Åcult than in one dimension, as we shall see.
Example: Laplace‚Äôs equation
Let‚Äôs try to construct the free space Green‚Äôs function for Laplace‚Äôs equation in R3.
We introduced the idea of a Green‚Äôs function for a linear operator in Section 5.3.
In this case, we seek a solution of
‚àá2G = Œ¥(x) subject to G ‚Üí0 as |x| ‚Üí‚àû.
(5.34)
The Fourier transform of G is
ÀúG(k1, k2, k3) =
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
G(x1, x2, x3)ei(k1x1+k2x2+k3x3) dx1 dx2 dx3,
so that
F
 ‚àÇG
‚àÇx1

=
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
‚àÇG
‚àÇx1
(x1, x2, x3)ei(k1x1+k2x2+k3x3) dx1 dx2 dx3
=
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
(
ei(k1x1+k2x2+k3x3)G
)‚àû
‚àí‚àû
‚àí
 ‚àû
‚àí‚àû
Gik1ei(k1x1+k2x2+k3x3) dx1
	
dx2 dx3,
after integrating by parts. Since G vanishes at inÔ¨Ånity, we conclude that
 ‚àÇG
‚àÇx1

= ‚àíik1
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
Gei(k1x1+k2x2+k3x3) dx1 dx2 dx3 = ‚àíik1 ÀúG.
This result is, of course, analogous to the one-dimensional result that we derived
in Section 5.2.5.
If we now take the Fourier transform of (5.34), we Ô¨Ånd that

(‚àíik1)2 + (‚àíik2)2 + (‚àíik3)2 ÀúG =

R3 Œ¥(x)eik¬∑x d 3x = 1,
and hence ÀúG = ‚àí1/|k|2. The inversion formula, (5.33), then shows that
G(x) = ‚àí1
8œÄ3

R3
e‚àíik¬∑x
|k|2
d 3k.
(5.35)
In order to evaluate this integral, we need to introduce spherical polar coordinates

148
FOURIER SERIES AND THE FOURIER TRANSFORM
(k, Œ∏, œÜ), with the line Œ∏ = 0 in the x-direction. Then k ¬∑ x = |k| |x| cos Œ∏ = kr cos Œ∏
and (5.35) becomes
G(x) = ‚àí1
8œÄ3
 ‚àû
k=0
 œÄ
Œ∏=0
 2œÄ
œÜ=0
e‚àíikr cos Œ∏
k2
k2 sin Œ∏ dk dŒ∏ dœÜ
= ‚àí1
4œÄ2
 ‚àû
k=0
e‚àíikr cos Œ∏
ikr
œÄ
Œ∏=0
dk = ‚àí1
2œÄ2
 ‚àû
k=0
sin kr
kr
dk
= ‚àí
1
2œÄ2r
 ‚àû
z=0
sin z
z
dz = ‚àí1
4œÄr,
using the standard result,
' ‚àû
0
(sin z/z) dz = œÄ/2 (see, for example, Ablowitz and
Fokas, 1997).
Now that we know the Green‚Äôs function, we are able to solve the inhomogeneous
problem
‚àá2œÜ = Q(x) subject to œÜ ‚Üí0 as |x| ‚Üí‚àû,
(5.36)
in terms of a convolution integral, by a direct generalization of the results presented
in Section 5.2.5. We Ô¨Ånd that
œÜ = ‚àí1
4œÄ

R3
Q(y)
|x ‚àíy|d 3y.
This result is fundamental in the theory of electrostatics, where Q is the distribution
of charge density, and œÜ the corresponding electrical potential.
Example: The wave equation
Let‚Äôs try to solve the three-dimensional wave equation‚Ä†,
‚àá2u = 1
c2
‚àÇ2u
‚àÇt2
for t > 0 and x ‚ààR3,
(5.37)
subject to the initial conditions
u(x, y, z, 0) = 0,
‚àÇu
‚àÇt (x, y, z, 0) = f(x, y, z).
(5.38)
The Fourier transform of (5.37) is
‚àÇ2Àúu
‚àÇt2 = ‚àíc2k2Àúu,
(5.39)
where Àúu is the Fourier transform of u and k2 = k2
1 +k2
2 +k2
3. The initial conditions,
(5.38), then become
Àúu(k, 0) = 0,
‚àÇÀúu
‚àÇt (k, 0) = Àúf(k).
The general solution of (5.39) is
Àúu = A(k)eickt + B(k)e‚àíickt,
‚Ä† See Section 3.9.1 for a derivation of the wave equation in two dimensions, and Billingham and
King (2001) for a derivation of (5.37) for sound waves.

EXERCISES
149
and when we enforce the initial conditions, this becomes
Àúu =
Àúf(k)
2ick

eickt ‚àíe‚àíickt
.
The inversion formula, (5.33), then shows that
u(x, t) =
1
8œÄ3

R3
Àúf(k)
2ick {exp (ickt ‚àíik ¬∑ x) ‚àíexp (‚àíickt ‚àíik ¬∑ x)} d 3k.
(5.40)
For any given value of k, the terms exp (¬±ickt ‚àíik ¬∑ x) represent plane travelling
wave solutions of the three-dimensional wave equation, since they remain constant
on the planes k.x = ¬±ckt, which move perpendicular to themselves, in the direction
¬±k, at speed c. Since the integral (5.40) is a weighted sum of these plane waves at
diÔ¨Äerent wavenumbers k, we can see that the Fourier transform has revealed that
the solution of (5.37) subject to (5.38) can be written as a continuous spectrum of
plane wave solutions travelling in all directions. It is somewhat easier to interpret
solutions like (5.40) in the large time limit, t ‚â´1, with |x|/t Ô¨Åxed. We will do this
in Section 11.2.2 using the method of stationary phase.
Exercises
5.1
Determine the Fourier series expansion of f(x) for ‚àíœÄ < x < œÄ, where (a)
f(x) = ex, (b) f(x) = | sin x| , (c) f(x) = x2.
5.2
Determine the Fourier series expansion of the function
f(x) =
 0
for ‚àíœÄ < x ‚©Ω0,
x
for 0 < x < œÄ.
Using this series, show that
1 + 1
32 + 1
52 + ¬∑ ¬∑ ¬∑ = œÄ2
8 .
5.3
Show that
(a) Œ¥(Œ±x) = 1
|Œ±|Œ¥(x),
(b) Œ¥(x2 ‚àíŒ±2) = 1
2Œ± {Œ¥(x + Œ±) + Œ¥(x ‚àíŒ±)}.
5.4
Express Œ¥(ax + b) in the form ¬µŒ¥(x + œÉ) for appropriately chosen constants
¬µ and œÉ.
5.5
What is the general solution of the equation (x ‚àía)f(x) = b, if f is a
generalized function?
5.6
If g(x) = 0 at the points xi for i = 1, 2, . . . , n, Ô¨Ånd an expression for Œ¥(g(x))
in terms of the sequence of delta functions {Œ¥(x ‚àíxi)}.
5.7
Show that d
dxsgn(x) = 2Œ¥(x). What is d
dxe‚àí|x|?
5.8
Calculate the Fourier transforms of

150
FOURIER SERIES AND THE FOURIER TRANSFORM
(a)
f(x) =
Ô£±
Ô£≤
Ô£≥
0
for x < ‚àí1 and x > 1,
‚àí1
for ‚àí1 < x < 0,
1
for 0 < x < 1,
(b)
f(x) =
 ex
for x < 0,
0
for x > 0,
(c)
f(x) =
 xe‚àíx
for x > 0,
0
for x < 0.
5.9
Show that the Fourier transform has the ‚Äòshifting property‚Äô,
F[f(x ‚àía)] = eikaF[f(x)].
5.10
If
J(k) =
 ‚àû
‚àí‚àû
e‚àí(x‚àíik/2)2 dx,
show by diÔ¨Äerentiation under the integral sign that dJ/dk = 0. Verify that
J(0) = ‚àöœÄ and hence evaluate F
(
e‚àíx2)
.
5.11
Use Fourier transforms to show that the solution of the initial boundary
value problem,
‚àÇu
‚àÇt = k ‚àÇ2u
‚àÇx2 ‚àíŒªu,
for ‚àí‚àû< x < ‚àû, t > 0,
with Œª and k real constants, subject to
u(x, 0) = f(x),
u ‚Üí0
as |x| ‚Üí‚àû,
can be written in convolution form as
u(x, t) =
e‚àíŒªt
‚àö
4œÄkt
 ‚àû
‚àí‚àû
exp

‚àí(x ‚àíy)2
4kt
	
f(y) dy.
5.12
For n ‚ààN and k ‚ààZ, evaluate the integral
 œÄ
‚àíœÄ
ekxeinx dx,
and hence show that
 œÄ
‚àíœÄ
ekx cos nx dx = (‚àí1)n+12n sinh kœÄ
k2 + n2
.
Obtain the Fourier series of the function (of period 2œÄ) deÔ¨Åned by
f(x) = cosh kx
for
‚àíœÄ < x < œÄ.

EXERCISES
151
Find the value of
‚àû

n=1
1
k2 + n2
as a function of k and use (4.49) to evaluate
‚àû

n=1
1
(k2 + n2)2 .
5.13
The forced wave equation is
‚àÇ2œÜ
‚àÇx2 ‚àí1
c2
0
‚àÇ2œÜ
‚àÇt2 = Q(x, t),
where Q is the forcing function, which satisÔ¨Åes Q ‚Üí0 as |x| ‚Üí‚àû.
(a) Show that if Q = q(x)e‚àíiœât then a separable solution exists with
œÜ = e‚àíiœâtf(x) and f ‚Ä≤‚Ä≤ + k2
0f = q(x), where k0 = œâ/c0.
(b) Find the Green‚Äôs function solution of G‚Ä≤‚Ä≤ +k2
0G = Œ¥(x) that behaves
like e¬±ik0x as x ‚Üí¬±‚àû. What is the physical signiÔ¨Åcance of these
boundary conditions?
(c) Show that
œÜ = e‚àíiœâtG ‚àóq
= e‚àíiœât
2ik0
 x
‚àí‚àû
q(y)eik0(x‚àíy) dy +
 ‚àû
x
q(y)e‚àíik0(x‚àíy) dy
	
.
(d) Show that, as x ‚Üí‚àû,
œÜ ‚àºe‚àíi(œât‚àík0x)
2ik0
 ‚àû
‚àí‚àû
q(y)e‚àíik0y dy.
What is the physical interpretation of this result?
5.14
The free space Green‚Äôs function for the modiÔ¨Åed Helmholtz equation
in three dimensions satisÔ¨Åes

‚àá2 ‚àím2
G = Œ¥(x1, x2, x3),
with G ‚Üí0 as |x| ‚Üí‚àû. Use Fourier transforms to show that
G(x) = ‚àí1
8œÄ3

R3
e‚àíik¬∑x
k2 + m2 d 3k.
Use contour integration to show that this can be simpliÔ¨Åed to
G(x) = ‚àí1
4œÄre‚àímr,
where r = |x|. Verify by direct substitution that this function satisÔ¨Åes the
modiÔ¨Åed Helmholtz equation.

CHAPTER SIX
Laplace Transforms
Integral transforms of the form
T [f(x)] =

I
K(x, k)f(x) dx,
where I is some interval on the real line, K(x, k) is the kernel of the transform and k
is the transform variable, are useful tools for solving linear diÔ¨Äerential equations.
Other types of equation, such as linear integral equations, can also be solved using
integral transforms. In Chapter 5 we met the Fourier transform, for which the kernel
is eikx and I = R. The Fourier transform allows us to solve linear boundary value
problems whose domain of solution is the whole real line. In this chapter we will
study the Laplace transform, for which the usual notation for the original variable
is t and for the transform variable is s, the kernel is e‚àíst and I = R+ = [0, ‚àû).
This transform allows us to solve linear initial value problems, with t representing
time. As we shall see, it is closely related to the Fourier transform.
6.1
DeÔ¨Ånition and Examples
The Laplace transform of f(t) is
L[f(t)] = F(s) =
 ‚àû
0
e‚àístf(t) dt.
(6.1)
We will consider for what values of s the integral is convergent later in the chapter,
and begin with some examples.
Example 1
Consider f(t) = ekt, where k is a constant. Substituting this into (6.1), we have
L[ekt] =
 ‚àû
0
e‚àístekt dt =
 ‚àû
0
e‚àí(s‚àík)t dt =
 e‚àí(s‚àík)t
‚àí(s ‚àík)
‚àû
0
.
Now, in order that e‚àí(s‚àík)t ‚Üí0 as t ‚Üí‚àû, and hence that the integral converges,
we need s > k. If s is a complex-valued variable, which is how we will need to treat
s when we use the inversion formula, (6.5), we need Re(s) > Re(k). This shows
that
L[ekt] =
1
s ‚àík
for Re(s) > Re(k).

6.1 DEFINITION AND EXAMPLES
153
Example 2
Consider f(t) = cos œât, where œâ is a constant. Since cos œât = 1
2(eiœât + e‚àíiœât),
L[cos œât] = 1
2
 ‚àû
0

e(iœâ‚àís)t + e‚àí(iœâ+s)t
ds = 1
2

1
s ‚àíiœâ +
1
s + iœâ

,
provided Re(s) > 0. We can combine the two fractions to show that
L[cos œât] =
s
s2 + œâ2
for
Re(s) > 0.
It is easy to show that L[sin œât] = œâ/(s2 + œâ2) using the same technique. In the
same vein,
L[cosh(at)] =
s
s2 ‚àía2 ,
L[sinh(at)] =
a
s2 ‚àía2 .
Example 3
Consider f(t) = tn for n ‚ààR. By deÔ¨Ånition,
L[tn] =
 ‚àû
0
e‚àísttn dt.
If we let x = st so that dx = s dt, we obtain
L[tn] =
 ‚àû
0
e‚àíx x
s
n 1
s dx =
1
sn+1
 ‚àû
0
e‚àíxxn dx = Œì(n + 1)
sn+1
.
If n is an integer, this gives L[tn] = n!/sn+1, a result that we could have obtained
directly using integration by parts.
In order to use the Laplace transform, we will need to know how to invert it, so
that we can determine f(t) from a given function F(s). For the Fourier transform,
this inversion process involves the integral formula (5.25). The inverse of a Laplace
transform is rather similar, and involves an integral in the complex s-plane. We will
return to give an outline derivation of the inversion formula, (6.5), in Section 6.4.
For the moment, we will deal with the problem of inverting Laplace transforms by
trying to recognize the function f(t) from the form of F(s). As we shall see, there
are a lot of techniques available that allow us to invert the Laplace transform in an
elementary manner.
6.1.1
The Existence of Laplace Transforms
So far, we have rather glossed over the question of when the Laplace transform
of a particular function actually exists. We have, however, discussed in passing that
the real part of s needs to be greater than some constant value in some cases. In
fact the deÔ¨Ånition of the Laplace transform is an improper integral and should be
written as
L[f(t)] = lim
N‚Üí‚àû
 N
0
e‚àístf(t) dt.

154
LAPLACE TRANSFORMS
We will now Ô¨Åx matters by deÔ¨Åning a class of functions for which the Laplace
transform does exist. We say that a function f(t) is of exponential order on
0 ‚©Ωt < ‚àûif there exist constants A and b such that |f(t)| < Aebt for t ‚àà[0, ‚àû).
Theorem 6.1 (Lerch‚Äôs theorem) A piecewise continuous function of exponential
order on [0, ‚àû) has a Laplace transform.
The proof of this is rather technical, and the interested reader is referred to Kreider,
Kuller, Ostberg and Perkins (1966).
Theorem 6.2 If f is of exponential order on [0, ‚àû), then L[f] ‚Üí0 as |s| ‚Üí‚àû.
Proof Consider the deÔ¨Ånition of L[f(t)] and its modulus
|L[f(t)]| =

 ‚àû
0
e‚àístf(t) dt
 ‚©Ω
 ‚àû
0
e‚àístf(t)
 dt =
 ‚àû
0
e‚àíst |f(t)| dt,
using the triangle inequality. If f is of exponential order,
|L[f(t)]| <
 ‚àû
0
e‚àístAebt dt =
 ‚àû
0
Ae(b‚àís)t dt
=

A
b ‚àíse(b‚àís)t
‚àû
0
= lim
Y ‚Üí‚àû

A
b ‚àíse(b‚àís)Y ‚àí
A
b ‚àís

.
Provided s > b, we therefore have
|L[f(t)]| <
A
s ‚àíb,
and hence |L[f(t)]| ‚Üí0 as |s| ‚Üí‚àû.
Conversely, if lims‚Üí‚àûF(s) Ã∏= 0, then F(s) is not the Laplace transform of a function
of exponential order. For example, s2/(s2 + 1) ‚Üí1 as s ‚Üí‚àû, and is not therefore
the Laplace transform of a function of exponential order.
In contrast, if f is not of exponential order and grows too quickly as t ‚Üí‚àû,
the integral will not converge. For example, consider the Laplace transform of the
function et2,
L
(
et2)
= lim
N‚Üí‚àû
 N
0
et2e‚àíst dt.
It should be clear that et2 grows faster than est for all s, so that the integrand
diverges, and hence that the Laplace transform of et2 does not exist.
6.2
Properties of the Laplace Transform
Theorem 6.3 (Linearity) The Laplace transform and inverse Laplace transform
are linear operators.

6.2 PROPERTIES OF THE LAPLACE TRANSFORM
155
Proof By deÔ¨Ånition,
L[Œ±f(t) + Œ≤g(t)] =
 ‚àû
0
e‚àíst (Œ±f(t) + Œ≤g(t)) dt
= Œ±
 ‚àû
0
e‚àístf(t) dt + Œ≤
 ‚àû
0
e‚àístg(t) dt = Œ±L[f(t)] + Œ≤L[g(t)],
so the Laplace transform is a linear operator. Taking the inverse Laplace transform
of both sides gives
Œ±f(t) + Œ≤g(t) = L‚àí1 [Œ±L[f(t)] + Œ≤L[g(t)]] .
Since we can write f(t) as L‚àí1[L[f(t)]] and similarly for g(t), this gives
Œ±L‚àí1[L[f(t)]] + Œ≤L‚àí1[L[g(t)]] = L‚àí1 [Œ±L[f(t)] + Œ≤L[g(t)]] .
If we now deÔ¨Åne F(s) = L[f] and G(s) = L[g], we obtain
Œ±L‚àí1[F] + Œ≤L‚àí1[G] = L‚àí1 [Œ±F + Œ≤G] ,
so the inverse Laplace transform is also a linear operator.
This is extremely useful, since we can calculate, for example L[2t2 ‚àít + 1] and
L[e3t + cos 2t] easily in terms of the Laplace transforms of their constituent parts.
As we mentioned earlier, the Laplace inversion formula involves complex inte-
gration, which we would prefer to avoid when possible. Often we can recognize
the constituents of an expression whose inverse Laplace transform we seek. For
example, consider L‚àí1[1/(s2 ‚àí5s + 6)]. We can proceed by splitting this rational
function into its partial fractions representation and exploit the linearity of the
inverse Laplace transform. We have
L‚àí1

1
s2 ‚àí5s + 6

= L‚àí1

1
(s ‚àí2)(s ‚àí3)

= L‚àí1

‚àí
1
s ‚àí2 +
1
s ‚àí3

= ‚àíL‚àí1

1
s ‚àí2

+ L‚àí1

1
s + 3

= ‚àíe2t + e3t.
We pause here to note that the inversion of Laplace transforms using standard
forms is only possible because the operation is a bijection, that is, it is one-to-one
and onto. For every function f(t) the Laplace transform L[f(t)] is uniquely deÔ¨Åned
and vice versa. This is a direct consequence of Lerch‚Äôs theorem, 6.1.
Theorem 6.4 (First shifting theorem) If L[f(t)] = F(s) for Re(s) > b, then
L[eatf(t)] = F(s ‚àía) for Re(s) > a + b.
Proof By deÔ¨Ånition,
L[eatf(t)] =
 ‚àû
0
e‚àísteatf(t) dt =
 ‚àû
0
e‚àí(s‚àía)tf(t) dt = F(s ‚àía),
provided that Re(s ‚àía) > b.

156
LAPLACE TRANSFORMS
Example 1
Consider the function f(t) = e3t cos 4t. We recall from the previous section that
L [cos 4t] =
s
s2 + 42
for Re(s) > 0.
Using Theorem 6.4,
L
!
e3t cos 4t
"
=
s ‚àí3
(s ‚àí3)2 + 42
for Re(s) > 3.
Example 2
Consider the function F(s) = s/(s2 +s+1). What is its inverse Laplace transform?
We begin by completing the square of the denominator, which gives
F(s) =
s

s + 1
2
2 + 3
4
=
s + 1
2

s + 1
2
2 + 3
4
‚àí
1
2

s + 1
2
2 + 3
4
,
and hence
L‚àí1

s
s2 + s + 1

= L‚àí1

s + 1
2

s + 1
2
2 + 3
4

‚àíL‚àí1

1
‚àö
3
‚àö
3
2

s + 1
2
2 + 3
4

.
Using the Ô¨Årst shifting theorem then gives us
L‚àí1

s
s2 + s + 1

= e‚àít/2 cos
‚àö
3t
2
‚àí1
‚àö
3e‚àít/2 sin
‚àö
3t
2 .
Theorem 6.5 (Second shifting theorem) If the Laplace transform of f(t) is
F(s), then the Laplace transform of the function g(t) = H(t‚àía)f(t‚àía) is e‚àísaF(s),
where H is the Heaviside step function.
Proof By deÔ¨Ånition,
L[g(t)] =
 ‚àû
0
g(t)e‚àíst dt =
 ‚àû
a
f(t ‚àía)e‚àíst dt.
By writing œÑ = t ‚àía, this becomes
L[g(t)] =
 ‚àû
0
f(œÑ)e‚àísœÑe‚àísa dœÑ = e‚àísaF(s),
since the deÔ¨Ånition of the Laplace transform, (6.1), can be written in terms of any
dummy variable of integration.
For example, to determine the inverse transform of the function e‚àí3s/s3, we Ô¨Årstly
note that L[t2] = 2/s3, using Example 3 of Section 6.1. The second shifting theorem
then shows immediately that
L‚àí1
e‚àí3s
s3

= 1
2H(t ‚àí3)(t ‚àí3)2.

6.3 THE SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
157
6.3
The Solution of Ordinary DiÔ¨Äerential Equations Using Laplace
Transforms
In order to be able to take the Laplace transform of a diÔ¨Äerential equation, we will
need to be able to calculate the Laplace transform of the derivative of a function.
By deÔ¨Ånition,
L[f ‚Ä≤] =
 ‚àû
0
e‚àístf ‚Ä≤(t) dt.
After integrating by parts, we Ô¨Ånd that
L[f ‚Ä≤] =
!
e‚àístf(t)
"‚àû
0 ‚àí
 ‚àû
0
‚àíse‚àístf(t) dt.
At this stage, we will assume that the values of s are restricted so that e‚àístf(t) ‚Üí0
as t ‚Üí‚àû. This means that
L[f ‚Ä≤] = sL[f] ‚àíf(0).
(6.2)
A useful corollary of (6.2) is that, if
g(t) =
 t
0
f(œÑ)dœÑ,
so that, except where f(t) is discontinuous, g‚Ä≤(t) = f(t), we have L[f] = sL[g]‚àíg(0).
Since g(0) = 0 by deÔ¨Ånition,
L[g] = L
 t
0
f(œÑ)dœÑ

= 1
sL[f],
and hence
L‚àí1
1
sF(s)

=
 t
0
f(œÑ) dœÑ.
This can be useful for inverting Laplace transforms, for example,
F(s) =
1
s(s2 + œâ2).
We know that L[sin œât]/œâ = 1/(s2 + œâ2) so that
f(t) = L‚àí1
1
s
1
s2 + œâ2

= L‚àí1
1
sL
 1
œâ sin œât

=
 t
0
1
œâ sin œâœÑ dœÑ = 1
œâ2 (1 ‚àícos œât) .
Let‚Äôs now try to solve the simple diÔ¨Äerential equation
dy
dt ‚àí2y = 0,
subject to the initial condition y(0) = 1.
Of course, it is trivial to solve this

158
LAPLACE TRANSFORMS
separable equation, but it is a useful illustrative example. We begin by taking the
Laplace transform of the diÔ¨Äerential equation, which gives
L
dy
dt

‚àí2L[y] = sY (s) ‚àíy(0) ‚àí2Y (s) = 0,
where Y (s) = L[y(t)]. Using the initial condition and manipulating the equation
gives
Y (s) =
1
s ‚àí2,
which is easily inverted to give y(t) = e2t.
Many of the diÔ¨Äerential equations that we will try to solve are of second order,
so we need to determine L[f ‚Ä≤‚Ä≤]. If we introduce the function g(t) = f ‚Ä≤(t), (6.2)
shows that
L[g‚Ä≤] = sG(s) ‚àíg(0),
where G(s) = L[g] = L[f ‚Ä≤] = sF(s) ‚àíf(0). We conclude that
L[f ‚Ä≤‚Ä≤] = L[g‚Ä≤] = s2F(s) ‚àísf(0) ‚àíf ‚Ä≤(0).
(6.3)
We can obtain the same result by integrating the deÔ¨Ånition of the Laplace transform
of f ‚Ä≤‚Ä≤ twice by parts. It is also straightforward to show by induction that
L[f (n)] = snF(s) ‚àísn‚àí1f(0) ‚àísn‚àí2f ‚Ä≤(0) ‚àí¬∑ ¬∑ ¬∑ ‚àíf (n‚àí1)(0).
For example, we can now solve the diÔ¨Äerential equation
d 2y
dt2 ‚àí5dy
dt + 6y = 0,
subject to the initial conditions y(0) = 0 and y‚Ä≤(0) = 1 using Laplace transforms.
We Ô¨Ånd that
s2Y (s) ‚àísy(0) ‚àíy‚Ä≤(0) ‚àí5(sY (s) ‚àíy(0)) + 6Y = 0.
Using the initial conditions then shows that
Y (s) =
1
s2 ‚àí5s + 6.
In order to invert the Laplace transform we split the fraction into its constituent
partial fractions,
Y (s) =
1
s ‚àí3 ‚àí
1
s ‚àí2,
which immediately shows that
y(t) = e3t ‚àíe2t.
Let‚Äôs now consider the solution of the coupled system of equations
dy1
dt + y1 = y2,
dy2
dt ‚àíy2 = y1,
subject to the initial conditions that y1(0) = y2(0) = 1.
Although we could

6.3 THE SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
159
combine these two Ô¨Årst order equations to obtain a second order equation, we will
solve them directly using Laplace transforms. The transform of the equations is
sY1 ‚àí1 + Y1 = Y2,
sY2 ‚àí1 ‚àíY2 = Y1,
where L[yj(t)] = Yj(s). The solution of these algebraic equations is
Y1(s) =
s
s2 ‚àí2,
Y2(s) = s + 2
s2 ‚àí2,
which we can easily invert by inspection to give
y1(t) = cosh
‚àö
2t,
y2(t) = cosh
‚àö
2t +
‚àö
2 sinh
‚àö
2t.
Note that the reduction of a system of diÔ¨Äerential equations to a system of algebraic
equations is the key beneÔ¨Åt of these transform methods.
Of course, for simple scalar equations, it is easier to use the standard solution
techniques described in Appendix 5.
The real power of the Laplace transform
method lies in the solution of problems for which the inhomogeneity is not of a
simple form, for example
d2y
dt2 + dy
dt = Œ¥(t ‚àí1).
In order to take the Laplace transform of this equation, we need to know the Laplace
transform of the delta function. We can calculate this directly, as
L[Œ¥(t ‚àía)] =
 ‚àû
0
Œ¥(t ‚àía)e‚àíst dt = e‚àísa,
so that the diÔ¨Äerential equation becomes
s2Y (s) ‚àísy(0) ‚àíy‚Ä≤(0) + sY (s) ‚àíy(0) = e‚àís,
and hence
Y (s) = e‚àís + (s + 1)y(0) + y‚Ä≤(0)
s(s + 1)
.
Judicious use of the two shifting theorems now allows us to invert this Laplace
transform. Note that
L‚àí1
1
s

= 1,
and, using the Ô¨Årst shifting theorem,
L‚àí1

1
s + 1

= e‚àít.
This means that
L‚àí1

1
s(s + 1)

= L‚àí1
1
s ‚àí
1
s + 1

= 1 ‚àíe‚àít,
and, using the second shifting theorem,
L‚àí1

e‚àís
s(s + 1)

= H(t ‚àí1)(1 ‚àíe‚àí(t‚àí1)).

160
LAPLACE TRANSFORMS
Combining all of these results, and using the linearity of the Laplace transform,
shows that
y(t) = y(0) + y‚Ä≤(0)(1 ‚àíe‚àít) + H(t ‚àí1)(1 ‚àíe‚àí(t‚àí1)).
6.3.1
The Convolution Theorem
The Laplace transform of the convolution of two functions plays an important
role in the solution of inhomogeneous diÔ¨Äerential equations, just as it did for Fourier
transforms. However, when using Laplace transforms, we deÔ¨Åne the convolution of
two functions as
f ‚àóg =
 t
0
f(œÑ)g(t ‚àíœÑ) dœÑ.
With this deÔ¨Ånition, if f(t) and g(t) have Laplace transforms, then
L[f ‚àóg] = L[f(t)]L[g(t)].
We can show this by taking the Laplace transform of the convolution integral, which
is itself a function of t, to obtain
L[f ‚àóg] =
 ‚àû
0
e‚àíst
 t
0
f(œÑ)g(t ‚àíœÑ) dœÑ dt.
Since e‚àíst is independent of œÑ it can be moved inside the inner integral so that we
have
L[f ‚àóg] =
 ‚àû
0
 t
0
e‚àístf(œÑ)g(t ‚àíœÑ) dœÑ dt.
We now note that the domain of integration is a triangular region delimited by the
lines œÑ = 0 and œÑ = t, as shown in Figure 6.1. If we switch the order of integration,
this becomes
L[f ‚àóg] =
 œÑ=‚àû
œÑ=0
 ‚àû
t=œÑ
e‚àístf(œÑ)g(t ‚àíœÑ) dt dœÑ.
If we now introduce the variable z = t ‚àíœÑ, so that dz = dt, we can transform the
inner integral into
L[f ‚àóg] =
 ‚àû
0
f(œÑ)
 ‚àû
0
e‚àís(z+œÑ)g(z) dz dœÑ,
and hence
L[f ‚àóg] =
 ‚àû
œÑ=0
f(œÑ)e‚àísœÑ dœÑ
 ‚àû
z=0
e‚àíszg(z) dz = L[f] L[g].
This result is most useful in the form
L‚àí1[F(s)G(s)] = L‚àí1[F(s)] ‚àóL‚àí1[G(s)].

6.3 THE SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
161
t
t = œÑ
œÑ
Fig. 6.1. The domain of integration of the Laplace transform of a convolution integral.
Example 1
Consider the Laplace transform F(s) = 1/(s ‚àí2)(s ‚àí3). This rational function
has two easily recognizable constituents, namely 1/(s ‚àí2) = L[e2t] and 1/(s ‚àí3) =
L[e3t], and hence
L‚àí1

1
(s ‚àí2)(s ‚àí3)

= L‚àí1

1
s ‚àí2

‚àóL‚àí1

1
s ‚àí3

= e2t ‚àóe3t.
We now need to calculate the convolution integral, which is
e2t ‚àóe3t =
 t
œÑ=0
e2œÑe3(t‚àíœÑ) dœÑ = e3t
 t
œÑ=0
e‚àíœÑ dœÑ
= e3t !
‚àíe‚àíœÑ"t
0 = e3t 
‚àíe‚àít + 1

= ‚àíe2t + e3t.
This is, of course, the same result as we obtained using partial fractions.
Example 2
Consider the Laplace transform
F(s) =
s
(s2 + 1)(s ‚àí2).

162
LAPLACE TRANSFORMS
This is the product of the Laplace transforms of cos t and e2t, and hence is the
Laplace transform of
cos t ‚àóe2t =
 t
0
e2œÑ cos(t ‚àíœÑ) dœÑ = 1
2
 t
0
e2œÑ 
ei(t‚àíœÑ) + e‚àíi(t‚àíœÑ)
dœÑ
=
ei(t‚àíœÑ)+2œÑ
2(2 + i)
+ e‚àíi(t‚àíœÑ)+2œÑ
2(2 ‚àíi)
t
0
= 2
5e2t ‚àí1
5 (2 cos t ‚àísin t) .
This is somewhat easier than the partial fractions method.
Example 3: Volterra integral equations
A Volterra integral equation can be written in the standard form
y(t) = f(t) +
 t
0
y(œÑ)K(t ‚àíœÑ) dœÑ for t > 0.
(6.4)
The function K is called the kernel of the equation. The integral is in the form
of a convolution and, if we treat t as time, is an integral over the history of the
solution. The integral equation (6.4) can therefore be written as
y = f + y ‚àóK.
If we now take a Laplace transform of this equation, we obtain
Y = F + L[y ‚àóK] = F + Y L[K],
and hence
Y =
F
1 ‚àíL[K],
where F(s) = L[f] and Y = L[y]. For example, to solve
y(t) = 1 +
 t
0
(t ‚àíœÑ)y(œÑ) dœÑ,
for which f(t) = 1 and K(t) = t, we note that F = 1/s and L[K] = 1/s2, and
hence
Y (s) =
s
s2 ‚àí1.
This gives us the solution, y(t) = cosh t.
6.4
The Inversion Formula for Laplace Transforms
We have now seen that many Laplace transforms can be inverted from a knowledge
of the transforms of a few common functions along with the linearity of the Laplace
transform and the Ô¨Årst and second shifting theorems. However, these techniques
are often inadequate to invert Laplace transforms that arise as solutions of more
complicated problems, in particular of partial diÔ¨Äerential equations. We will need
an inversion formula.
We can derive this in an informal way using the Fourier
integral, (5.23).

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
163
Let g(t) be a function of exponential order, in particular with Œ≥ the smallest
real number such that e‚àíŒ≥tg(t) is bounded as t ‚Üí‚àû.
As we have seen, g(t)
therefore has a Laplace transform G(s), which exists for Re(s) > Œ≥. We now deÔ¨Åne
h(t) = e‚àíŒ≥tg(t)H(t). Since h(t) is bounded as t ‚Üí‚àû, the Fourier integral (5.23)
exists, and shows that
h(t) = 1
2œÄ
 ‚àû
‚àí‚àû
e‚àíikt
 ‚àû
‚àí‚àû
eikT h(T) dT dk,
and hence that
e‚àíŒ≥tg(t) = 1
2œÄ
 ‚àû
‚àí‚àû
e‚àíikt
 ‚àû
0
e‚àí(Œ≥‚àíik)T g(T) dT dk.
If we now make the change of variable s = Œ≥ ‚àíik, so that ds = ‚àíi dk, we Ô¨Ånd that‚Ä†
g(t) =
1
2œÄi
 Œ≥+i‚àû
Œ≥‚àíi‚àû
est
 ‚àû
0
e‚àísT g(T) dT ds.
Finally, from the deÔ¨Ånition of the Laplace transform, we arrive at the inversion
formula, sometimes called the Bromwich inversion integral,
g(t) =
1
2œÄi
 Œ≥+i‚àû
Œ≥‚àíi‚àû
estG(s) ds.
(6.5)
Note that the contour of integration is a vertical line in the complex s-plane. Since
G(s) is only guaranteed to exist for Re(s) > Œ≥, this contour lies to the right of any
singularities of G(s). It is often possible to simplify (6.5) by closing the contour
of integration using a large semicircle in the left half plane. As we shall see in the
following examples, if the contour can be closed in this way, the result will depend
crucially on the residues at any poles of estG(s).
Example 1
We start with the simple case G(s) = Œ≤/(s ‚àíŒ±). Consider the integral
I(t) =
1
2œÄi

C
est
Œ≤
s ‚àíŒ± ds,
(6.6)
where the closed contour C is shown in Figure 6.2. By the residue theorem, I(t) is
equal to the sum of the residues at any poles of estG(s) enclosed by C. Let‚Äôs assume
that the contour encloses the simple pole of G(s) at s = Œ±, and hence that the
straight boundary of C lies to the right of the pole at s = Œ±. The residue theorem
then shows that I(t) = Œ≤eŒ±t. As b ‚Üí‚àû, the semicircular part of the contour
becomes large and, since |G(s)| is algebraically small when |s| ‚â´1, we conclude
from Jordan‚Äôs lemma (see Section A6.4) that the integral along the semicircle tends
to zero. On the straight part of the contour C, as b ‚Üí‚àûwe recover the inversion
integral (6.5), so that I(t) = g(t), and hence the inverse Laplace transform is
g(t) = Œ≤eŒ±t, as we would expect.
‚Ä† This is the point at which the derivation is informal, since we have not shown that this change
in the contour of integration is possible.

164
LAPLACE TRANSFORMS
C
s = a ‚àí ib
s = a + ib
Re(s)
Im(s)
Fig. 6.2. The contour C used to evaluate the inverse Laplace transform of Œ≤/(s ‚àíŒ±).
Example 2
Let‚Äôs try to determine the inverse Laplace transform of G(s) = 1/(s2 + 1). In this
case G(s) has simple poles at s = ¬±i, since
G(s) = 1
2i

1
s ‚àíi ‚àí
1
s + i

.
Choosing the contour C as we did in Example 1, we again Ô¨Ånd that g(t) is the sum
of the residues at the two simple poles of estG(s), and hence that, as expected,
g(t) = 1
2i

eit ‚àíe‚àíit
= sin t.
Example 3
We now consider a Laplace transform that has a nonsimple pole, G(s) = 1/(s‚àíŒ±)3.
The simplest way to calculate the residue of estG(s) at s = Œ± is to note that
est
(s ‚àíŒ±)3 =
eŒ±t
(s ‚àíŒ±)3 e(s‚àíŒ±)t =
eŒ±t
(s ‚àíŒ±)3

1 + (s ‚àíŒ±)t + 1
2(s ‚àíŒ±)2t2 + ¬∑ ¬∑ ¬∑

,
and hence that g(t) = 1
2t2eŒ±t. We can check this result by noting that L[t2] =
Œì(3)/s3 = 2/s3 and using the Ô¨Årst shifting theorem.
Example 4
Consider the inverse Laplace transform of G(s) = e‚àíŒ±s1/2/s. Since G(s) contains
a fractional power, s1/2, the point s = 0 is a branch point, and the deÔ¨Ånition of

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
165
G(s) is incomplete.
We need to introduce a branch cut in order to make G(s)
single-valued. It is convenient to place the branch cut along the negative real axis,
so that, if s = |s|eiŒ∏ with ‚àíœÄ < Œ∏ < œÄ, s1/2 = +
#
|s|eiŒ∏/2 and the real part of s1/2
is positive. We cannot now integrate estG(s) along any contour that crosses the
negative real axis, such as C. Because of this, we use CB, which avoids the branch
cut, as shown in Figure 6.3. This contour also includes a small circle around the
origin of radius œµ, since G(s) has a simple pole there, and is often referred to as a
keyhole contour.
F
B
E
A
Re(s)
Œ¥
Im(s)
C
D
s = a ‚àí ib
s = a + ib
Fig. 6.3. The keyhole inversion contour, CB, used for inverting Laplace transforms with
a branch cut along the negative real axis.
Since estG(s) is analytic within CB, the integral around CB is zero, by Cauchy‚Äôs
theorem (see Appendix 6). On the circular arcs AB and EF, G(s) ‚Üí0 exponen-
tially fast as b ‚Üí‚àû, since s1/2 has positive real part, and hence the contributions to
the integral from these arcs tend to zero. As before, the integral along the straight
contour AF tends to g(t) as b ‚Üí‚àû, by the inversion formula (6.5). We conclude
that
g(t) = ‚àílim
b‚Üí‚àû
œµ‚Üí0
1
2œÄi

BC
+

CD
+

DE
	 est‚àíŒ±s1/2
s
ds.
(6.7)
Let‚Äôs consider the contributions from the lines BC and DE. We can parameterize
these lines as s = xe¬±iœÄ respectively. In this way, we ensure that we use the correct
value of s1/2 on either side of the branch cut. Along BC, s1/2 = x1/2eiœÄ/2 = ix1/2,

166
LAPLACE TRANSFORMS
and hence, in the limit as œµ ‚Üí0 and b ‚Üí‚àû

BC
est‚àíŒ±s1/2
s
ds =
 0
‚àû
e‚àíxt‚àíŒ±ix1/2
x
dx.
Similarly

DE
est‚àíŒ±s1/2
s
ds =
 ‚àû
0
e‚àíxt+Œ±ix1/2
x
dx,
and hence

BC
+

DE
	 est‚àíŒ±s1/2
s
ds = 2i
 ‚àû
0
e‚àíxt sin Œ±x1/2
x
dx.
(6.8)
In order to calculate this integral, we note that
I =
 ‚àû
0
e‚àíxt sin Œ±x1/2
x
dx =
 ‚àû
0
e‚àíxt
x
‚àû

n=1
(Œ±x1/2)2n‚àí1
(2n ‚àí1)!
dx,
using the Taylor series expansion of sin Œ±x1/2. Since this series is uniformly con-
vergent for all x, we can interchange the order of summation and integration, so
that
I =
‚àû

n=1
Œ±2n‚àí1
(2n ‚àí1)!
 ‚àû
0
e‚àíxtxn‚àí3/2 dx =
‚àû

n=1
Œ±2n‚àí1
(2n ‚àí1)!
1
tn‚àí1/2
 ‚àû
0
e‚àíXXn‚àí3/2 dX
=
‚àû

n=1
 Œ±
t1/2
2n‚àí1 Œì

n ‚àí1
2

(2n ‚àí1)! ,
using the change of variable X = xt. Now, since
Œì

n ‚àí1
2

=

n ‚àí3
2

Œì

n ‚àí3
2

=

n ‚àí3
2
 
n ‚àí5
2

. . . 1
2Œì
1
2

=
1
2n‚àí1 (2n ‚àí3)(2n ‚àí5) . . . 3 ¬∑ 1 ¬∑ ‚àöœÄ,
we Ô¨Ånd that
Œì

n ‚àí1
2

(2n ‚àí1)! =
‚àöœÄ
2n‚àí1
(2n ‚àí3)(2n ‚àí5) . . . 3 ¬∑ 1
(2n ‚àí1)(2n ‚àí2) . . . 3 ¬∑ 2 ¬∑ 1
=
‚àöœÄ
2n‚àí1(2n ‚àí1)
1
(2n ‚àí2)(2n ‚àí4) . . . 4 ¬∑ 2 =
‚àöœÄ
22(n‚àí1)(2n ‚àí1)(n ‚àí1)!,
and hence
I = 2
‚àû

n=1

Œ±
2t1/2
2n‚àí1
‚àöœÄ
(2n ‚àí1)(n ‚àí1)! = 2‚àöœÄ
 Œ±/2t1/2
0
‚àû

n=1
(s2)n‚àí1
(n ‚àí1)! ds
= 2‚àöœÄ
 Œ±/2t1/2
0
e‚àís2 ds = œÄ erf

Œ±
2t1/2

,

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
167
where erf(x) is the error function, deÔ¨Åned by
erf(x) =
2
‚àöœÄ
 x
0
e‚àíq2 dq.
We can parameterize the small circle CD using s = œµeiŒ∏, where Œ∏ runs from œÄ to
‚àíœÄ. On this curve we have s1/2 = œµ1/2eiŒ∏/2, so that

CD
est‚àíŒ±s1/2
s
ds =
 ‚àíœÄ
œÄ
eœµteiŒ∏‚àíŒ±œµ1/2eiŒ∏/2
œµeiŒ∏
iœµeiŒ∏ dŒ∏ =
 ‚àíœÄ
œÄ
eœµteiŒ∏‚àíŒ±œµ1/2eiŒ∏/2i dŒ∏.
As œµ ‚Üí0 we therefore have

CD
est‚àíŒ±s1/2
s
ds =
 ‚àíœÄ
œÄ
i dŒ∏ = ‚àí2œÄi.
(6.9)
If we now use (6.8) and (6.9) in (6.7), we conclude that
L‚àí1

e‚àíŒ±s1/2
s

= 1 ‚àíerf

Œ±
2t1/2

= erfc

Œ±
2t1/2

,
where erfc(x) is the complementary error function, deÔ¨Åned by
erfc(x) =
2
‚àöœÄ
 ‚àû
x
e‚àíq2 dq.
Laplace transforms of the error function and complementary error function arise
very frequently when solving diÔ¨Äusion problems, as we shall see in the following
example.
Example 5: Flow due to an impulsively-started Ô¨Çat plate
Let‚Äôs consider the two-dimensional Ô¨Çow of a semi-inÔ¨Ånite expanse of viscous Ô¨Çuid
caused by the sudden motion of a Ô¨Çat plate in its own plane. We will use Cartesian
coordinates with the x-axis lying in the plane of the plate and the y-axis pointing
into the semi-inÔ¨Ånite body of Ô¨Çuid.
This is a uni-directional Ô¨Çow with velocity
u(x, y, t) in the x-direction only and associated scalar pressure Ô¨Åeld p(x, y, t). The
continuity equation, ux + vy = 0, which we derived in Chapter 2, shows that
the streamwise velocity, u, is solely a function of y and t. We now consider the
streamwise momentum within a small element of Ô¨Çuid, as shown in Figure 6.4.
Balancing forces in the x-direction, and taking the limit Œ¥x, Œ¥y, Œ¥t ‚Üí0, we Ô¨Ånd
that
œÅDu
Dt = œÅ
‚àÇu
‚àÇt + u‚àÇu
‚àÇx

= ‚àí‚àÇp
‚àÇx + ‚àÇœÑ
‚àÇy ,
where œÅ is the density and œÑ is the shear stress. Here we have used the convective
derivative, which we derived in Chapter 2. For a one-dimensional Ô¨Çow, it is found
experimentally that œÑ = ¬µ‚àÇu/‚àÇy, where ¬µ is the dynamic viscosity (see Acheson,
1990). To be consistent with u = u(y, t), we now insist that ‚àÇ/‚àÇx ‚â°0. This reduces
the x-momentum equation to ut = ŒΩuyy, where we have introduced the quantity
ŒΩ = ¬µ/œÅ, the kinematic viscosity. Flows with high values of ŒΩ are extremely

168
LAPLACE TRANSFORMS
œÑ (x, y, t)
œÑ (x, y + Œ¥y, t)
p (x, y, t)
p (x + Œ¥x, y, t)
Fig. 6.4. The x-momentum balance on a small element of Ô¨Çuid.
viscous, for example tar, lava and mucus, whilst those with low viscosity include
air, water and inert gases. For example, the kinematic viscosity of lava is around
10 m2 s‚àí1, whilst the values for air and water at room temperature and pressure,
10‚àí6 m2 s‚àí1 and 1.5 √ó 10‚àí5 m2 s‚àí1, respectively, are very similar.
Our Ô¨Ånal initial‚Äìboundary value problem is
‚àÇu
‚àÇt = ŒΩ ‚àÇ2u
‚àÇy2 ,
(6.10)
to be solved subject to
u = 0 when t = 0 for y > 0,
(6.11)
u = U,
at y = 0 for t > 0,
(6.12)
u ‚Üí0,
as y ‚Üí‚àûfor t > 0.
(6.13)
This is known as Rayleigh‚Äôs problem. Equation (6.10) states that the only pro-
cess involved in this Ô¨Çow is the diÔ¨Äusion of x-momentum into the bulk of the Ô¨Çuid.
Of course, this initial‚Äìboundary value problem can also be thought of as modelling
other diÔ¨Äusive systems, for example, a semi-inÔ¨Ånite bar of metal, insulated along
its sides, suddenly heated up at one end.
In order to solve this initial‚Äìboundary value problem, we will take a Laplace
transform with respect to time, so that
L[u(y, t)] = U(y, s) =
 ‚àû
0
e‚àístu(y, t) dt.

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
169
Taking two derivatives of this deÔ¨Ånition with respect to y shows that
L
‚àÇ2u
‚àÇy2

= ‚àÇ2U
‚àÇy2 .
In general, y-derivatives are not aÔ¨Äected by a Laplace transform with respect to t.
After using (6.2) to determine the Laplace transform of ‚àÇu/‚àÇt, (6.10) becomes
sU = ŒΩ ‚àÇ2U
‚àÇy2 .
(6.14)
The transform variable only appears as a parameter in this equation, which there-
fore has the solution
U(y, s) = A(s)es1/2y/ŒΩ1/2 + B(s)e‚àís1/2y/ŒΩ1/2.
Since u ‚Üí0 as y ‚Üí‚àû, we must also have U ‚Üí0 as y ‚Üí‚àû, so that A(s) = 0. We
now need to transform the boundary condition (6.12), which gives
U(0, s) =
 ‚àû
0
u(0, t)e‚àíst dt =
 ‚àû
0
Ue‚àíst dt = U
s ,
and hence B(s) = U/s. We conclude that
U(y, s) = U
s e‚àís1/2y/ŒΩ1/2.
From the result of the previous example, we Ô¨Ånd that
u(y, t) = U erfc

y
2
‚àö
ŒΩt

.
Some typical velocity proÔ¨Åles are shown in Figure 6.5.
These are easy to plot,
since the error and complementary error functions are available as erf and erfc in
MATLAB.
We can also consider what happens when the velocity of the plate is a function
of time. All we need to change is the boundary condition (6.12), which becomes
u = Uf(t),
at y = 0 for t > 0.
(6.15)
The Laplace transform of this condition is U(0, s) = UF(s), where F(s) is the
Laplace transform of f(t). Now B(s) = F(s), and hence
U(y, s) = UF(s)e‚àís1/2y/ŒΩ1/2 = UsF(s)e‚àís1/2y/ŒΩ1/2
s
.
(6.16)
Using (6.2), we can see that sF(s) = L[f ‚Ä≤(t)] + f(0), and hence
U(y, s) = U {L[f ‚Ä≤(t)] + f(0)} L

erfc

y
2
‚àö
ŒΩt

.
(6.17)
We can now invert this Laplace transform using the convolution theorem, to give
u(y, t) = U
 t
œÑ=0
{f ‚Ä≤(t ‚àíœÑ) + f(0)} erfc

y
2‚àöŒΩœÑ

dœÑ

.
(6.18)

170
LAPLACE TRANSFORMS
Exact solutions of the equations that govern the Ô¨Çow of a viscous Ô¨Çuid, the Navier‚Äì
Stokes equations, are very useful for verifying that numerical solution methods
are working correctly. Their stability can also be studied more easily than for Ô¨Çows
where no analytical solution is available.
By evaluating (6.18) numerically, we can calculate the Ô¨Çow proÔ¨Åles for any f(t).
A MATLAB function that evaluates u(y, t), for any speciÔ¨Åed f(t), is
'
&
$
%
function rayleigh(yout,tout)
for t = tout
u = [];
for y = yout
u = [u quadl(@integrand,0,t,10^-4,0,t,y)];
end
plot(u,yout), xlabel(‚Äôu‚Äô), ylabel(‚Äôy‚Äô)
title(strcat(‚Äôt = ‚Äô,num2str(t))), Xlim([0 1])
pause(0.5)
end
function integrand = integrand(tau,t,y)
nu = 1;
df = f(t-tau);
integrand = df.*erfc(y/2./(eps+sqrt(nu*tau)));
function f = f(t)
df = 2*cos(2*t); f0 = 0; f = df + f0;
This uses the MATLAB function quadl to evaluate the integral correct to four
decimal places, and then plots the solution at the points given in yout at the
times given in tout. In the example shown, f(t) = sin 2t, which corresponds to
an oscillating plate, started from rest. Note that we add the inbuilt small quantity
eps to the denominator of the argument of the error function to avoid MATLAB
producing lots of irritating division by zero warnings. In fact, MATLAB is able to
evaluate erfc(y/0) = erfc(Inf) = 0 for y positive.
As a Ô¨Ånal example, let‚Äôs consider what we would do if we did not have access
to a computer, but wanted to know what happens when the plate oscillates, with
f(t) = sin œât. Since F(s) = œâ/(s2 + œâ2), (6.16), along with the inversion formula
(6.5), gives
u(y, t) =
1
2œÄi
 Œ≥+i‚àû
Œ≥‚àíi‚àû
œâest‚àíy‚àö
s/ŒΩ
s2 + œâ2
ds,
(6.19)
where Œ≥ > 0, since the integrand has poles at s = ¬±iœâ. Although (6.18) is the
most convenient form to use for numerical integration, (6.19) gives us the most
helpful way of approaching this speciÔ¨Åc problem. We proceed as we did in Example
4, evaluating the integral on the contour CB shown in Figure 6.3. The analysis

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
171
Fig. 6.5. The Ô¨Çow due to an impulsively-started Ô¨Çat plate when U = 1 and ŒΩ = 1.
proceeds exactly as it did in Example 4, except that now the integral around CB
is equal to the sum of the residues of the integrand at the poles at s = ¬±iœâ. In
addition, the integral around the small circle CD tends to zero as œµ ‚Üí0. After
taking all of the contributions into account (see Exercise 6.8), we Ô¨Ånd that
u(y, t) = œâ
œÄ
 ‚àû
0
e‚àíœÉt sin
# œÉ
ŒΩ y

œÉ2 + œâ2
dœÉ + e‚àí‚àöœâ
2ŒΩ y sin

œât ‚àí
* œâ
2ŒΩ y
	
.
(6.20)
The Ô¨Årst term, which comes from the branch cut integrals, tends to zero as t ‚Üí‚àû,
and represents the initial transient that arises because the plate starts at rest. The
second term, which comes from the residues at the poles, represents the oscillatory
motion of the Ô¨Çuid, whose phase changes with y and whose amplitude decays ex-
ponentially fast with y. This second term therefore gives the large time behaviour
of the Ô¨Çuid. You can see what this solution looks like by running the MATLAB
function that we gave above.

172
LAPLACE TRANSFORMS
Exercises
6.1
Find the Laplace transforms of the functions, (a) t(t+1)(t+2), (b) sinh(œât),
(c) cosh(œât), (d) e‚àít sin t. Specify the values of s for which each transform
exists.
6.2
Find the inverse Laplace transforms of the functions, (a) 1/(s2 ‚àí3s + 2),
(b) 1/s3(s+1), (c) 1/(2s2‚àís+1), (d) 1/s2(s+1)2, (e) (2s+3)/(s2‚àí4s+20),
(f) 1/(s4 + 9s2), (g) (2s ‚àí4)/(s ‚àí1)4, (h) (s2 + 1)/(s3 ‚àís2 + 2s ‚àí2),
(i) s3/(s + 3)2(s + 2)2.
6.3
Using Laplace transforms, solve the initial value problems
(a) d 2y
dt2 + 9y = 18t, subject to y(0) = 0, y
œÄ
2

= 0,
(b) d 2y
dt2 ‚àí4dy
dt + 3y = f(t), subject to y(0) = 1 and y‚Ä≤(0) = 0,
(c) dx
dt + x + 2y = t, d 2x
dt2 + 5x + 3dy
dt = 0,
subject to x‚Ä≤(0) = x(0) = y(0) = 0,
(d)
 d 2
dt2 ‚àí4 d
dt + 4

y = f(t), subject to y(0) = ‚àí2, y‚Ä≤(0) = 1, with
f(t) =
 t
if
0 ‚©Ωt ‚©Ω3,
t + 2
if
t ‚©æ3.
(e) dx
dt + x + dy
dt = 0, dx
dt ‚àíx + 2dy
dt = e‚àít, subject to x(0) = y(0) = 1.
(f) d 2x
dt2 = ‚àí2x + y,
d 2y
dt2 = x ‚àí2y, subject to the initial conditions
x(0) = y(0) = 1 and x‚Ä≤(0) = y‚Ä≤(0) = 0.
6.4
Using Laplace transforms, solve the integral and integro-diÔ¨Äerential equa-
tions
(a) y(t) = t + 1
6
 t
0
y(œÑ)(t ‚àíœÑ)3 dœÑ,
(b)
 t
0
y(œÑ) cos(t ‚àíœÑ) dœÑ = dy
dt , subject to y(0) = 1.
6.5
Show that L[tf(t)] = ‚àídF/ds, where F(s) = L[f(t)].
Hence solve the
initial value problem
d 2x
dt2 + 2tdx
dt ‚àí4x = 1,
subject to x(0) = x‚Ä≤(0) = 0.
6.6
Determine the inverse Laplace transform of
F(s) =
s
(s2 + 1)(s ‚àí2)
using (a) the inversion formula (6.5), and (b) the residue theorem.

EXERCISES
173
6.7
Find the inverse Laplace transform of
F(s) =
1
(s ‚àí1)(s2 + 1),
by (a) expressing F(s) as partial fractions and inverting the constituent
parts and (b) using the convolution theorem.
6.8
Evaluate the integral (6.19), and hence verify that (6.20) holds.
6.9
Consider the solution of the diÔ¨Äusion equation
‚àÇœÜ
‚àÇt = D‚àÇ2œÜ
‚àÇy2
for t > 0 and y ‚àà[0, L], subject to the initial conditions œÜ(y, 0) = 0 and
the boundary conditions œÜ(0, t) = œÜ(L, t) = 1.
Show that the Laplace
transform of the solution is
Œ¶(y, s) =
1
s (1 + e‚àíŒ∫L)

e‚àíŒ∫y + e‚àíŒ∫(L‚àíy)
,
where Œ∫ =
#
s/D. By expanding the denominator, show that the solution
can be written as
œÜ(y, t) =
‚àû

j=0
(‚àí1)j

erfc
y + jL
2
‚àö
Dt

‚àíerfc
(j + 1)L ‚àíy
2
‚àö
Dt

.
6.10
(a) Show that a small displacement, y(x, t), of a uniform string with
constant tension T and line density œÅ subject to a uniform gravita-
tional acceleration, g, downwards satisÔ¨Åes
‚àÇ2y
‚àÇt2 = c2 ‚àÇ2y
‚àÇx2 ‚àíg,
where c =
#
T/œÅ (see Section 3.9.1).
(b) Such a string is semi-inÔ¨Ånite in extent, and has y = yt = 0 for x ‚©æ0
when t = 0. Use Laplace transforms to determine the solution when
the string is Ô¨Åxed at x = 0, satisÔ¨Åes yx ‚Üí0 as x ‚Üí‚àûand is allowed
to fall under gravity. Sketch the solution, and explain how and why
the qualitative form of the solution depends upon the sign of x ‚àíct.
6.11
Project The voltage, v(t), in an RLC circuit with implied current i(t) is
given by the solution of the diÔ¨Äerential equation
C d 2v
dt2 + 1
R
dv
dt + v
L = di
dt,
where C is the capacitance, R the resistance and L the inductance.
(a) Solve this equation using Laplace transforms when i(t) = H(t‚àí1)‚àí
H(t).
(b) Write a MATLAB script that inverts the Laplace transform of this
equation directly, so that it works when i(t) is supplied by an input
routine input.m.

174
LAPLACE TRANSFORMS
(c) Compare your results for various values of the parameters R, L and
C.
(d) Extend this further to consider what happens when the implied
current is periodic.

CHAPTER SEVEN
ClassiÔ¨Åcation, Properties and Complex Variable
Methods for Second Order Partial DiÔ¨Äerential
Equations
In this chapter, we will consider a class of partial diÔ¨Äerential equations, of which
Laplace‚Äôs equation, the diÔ¨Äusion equation and the wave equation are the canonical
examples. We will discuss what sort of boundary conditions are appropriate, and
why solutions of these equations have their distinctive properties.
We will also
demonstrate that complex variable methods are powerful tools for solving bound-
ary value problems for Laplace‚Äôs equation, with particular application to certain
problems in Ô¨Çuid mechanics.
7.1
ClassiÔ¨Åcation and Properties of Linear, Second Order Partial
DiÔ¨Äerential Equations in Two Independent Variables
Consider a second order linear partial diÔ¨Äerential equation in two independent
variables, which we can write as
a(x, y)‚àÇ2œÜ
‚àÇx2 + 2b(x, y) ‚àÇ2œÜ
‚àÇx‚àÇy + c(x, y)‚àÇ2œÜ
‚àÇy2
+d1(x, y)‚àÇœÜ
‚àÇx + d2(x, y)‚àÇœÜ
‚àÇy + d3(x, y)œÜ = f(x, y).
(7.1)
Equations of this type arise frequently in mathematical modelling, as we have al-
ready seen. We will show that the Ô¨Årst three terms of (7.1) allow us to classify the
equation into one of three distinct types: elliptic, for example Laplace‚Äôs equation,
(2.13), parabolic, for example the diÔ¨Äusion equation, (2.12), or hyperbolic, for
example the wave equation, (3.39). Each of these types of equation has distinctive
properties. These mathematical properties are related to the physical properties of
the system that the equation models.
7.1.1
ClassiÔ¨Åcation
We would like to know about properties of (7.1) that are unchanged by an
invertible change of coordinates, since these must be of fundamental signiÔ¨Åcance,
and not just a result of our choice of coordinate system. We can write this change

176
CLASSIFICATION AND COMPLEX VARIABLE METHODS
of coordinates as‚Ä†
(x, y) ‚Üí(Œæ (x, y) , Œ∑ (x, y)) ,
with ‚àÇ(Œæ, Œ∑)
‚àÇ(x, y) Ã∏= 0.
(7.2)
In particular, if (7.1) is a model for a physical system, a change of coordinates
should not aÔ¨Äect its qualitative behaviour. Writing œÜ(x, y) ‚â°œà(Œæ, Œ∑) and using
subscripts to denote partial derivatives, we Ô¨Ånd that
œÜx = ŒæxœàŒæ + Œ∑xœàŒ∑,
œÜxx = Œæ2
xœàŒæŒæ + 2ŒæxŒ∑xœàŒæŒ∑ + Œ∑2
xœàŒ∑Œ∑ + ŒæxxœàŒæ + Œ∑xxœàŒ∑,
and similarly for the other derivatives. Substituting these into (7.1) gives us
AœàŒæŒæ + 2BœàŒæŒ∑ + CœàŒ∑Œ∑ + b1(Œæ, Œ∑)œàŒæ + b2(Œæ, Œ∑)œàŒ∑ + b3(Œæ, Œ∑)œà = g (Œæ, Œ∑) ,
(7.3)
where
A = aŒæ2
x + 2bŒæxŒæy + cŒæ2
y,
B = aŒæxŒ∑x + b (Œ∑xŒæy + ŒæxŒ∑y) + cŒæyŒ∑y,
(7.4)
C = aŒ∑2
x + 2bŒ∑xŒ∑y + cŒ∑2
y.
We do not need to consider the other coeÔ¨Écient functions here. We can express
(7.4) in a concise matrix form as
 A
B
B
C

=
 Œæx
Œ∑x
Œæy
Œ∑y
  a
b
b
c
  Œæx
Œæy
Œ∑x
Œ∑y

,
(7.5)
which shows that
det
 A
B
B
C

= det
 a
b
b
c
  ‚àÇ(Œæ, Œ∑)
‚àÇ(x, y)
2
.
(7.6)
This shows that the sign of ac‚àíb2 is independent of the choice of coordinate system,
which allows us to classify the equation.
‚Äî An elliptic equation has ac > b2, for example, Laplace‚Äôs equation
‚àÇ2œÜ
‚àÇx2 + ‚àÇ2œÜ
‚àÇy2 = 0.
‚Äî A parabolic equation has ac = b2, for example, the diÔ¨Äusion equation
K ‚àÇ2œÜ
‚àÇx2 ‚àí‚àÇœÜ
‚àÇy = 0.
‚Ä† Note that
‚àÇ(Œæ, Œ∑)
‚àÇ(x, y) = det

Œæx
Œæy
Œ∑x
Œ∑y

is the Jacobian of the transformation. The Jacobian is the factor by which the transformation
changes inÔ¨Ånitesimal volume elements.

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
177
‚Äî A hyperbolic equation has ac < b2, for example, the wave equation
1
c2
‚àÇ2œÜ
‚àÇx2 ‚àí‚àÇ2œÜ
‚àÇy2 = 0.
Note that, although these three examples are of the given type throughout the
(x, y)-plane, equations of mixed type are possible. For example, Tricomi‚Äôs equa-
tion, œÜxx = xœÜyy, is elliptic for x < 0 and hyperbolic for x > 0.
7.1.2
Canonical Forms
Any equation of the form given by (7.1) can be written in canonical form
by choosing the canonical coordinate system, in terms of which the second
derivatives appear in the simplest possible way.
Hyperbolic Equations: ac < b2
In this case, we can factorize A and C to give
A = aŒæ2
x + 2bŒæxŒæy + cŒæ2
y = (p1Œæx + q1Œæy) (p2Œæx + q2Œæy) ,
C = aŒ∑2
x + 2bŒ∑xŒ∑y + cŒ∑2
y = (p1Œ∑x + q1Œ∑y) (p2Œ∑x + q2Œ∑y) ,
with the two factors not multiples of each other. We can then choose Œæ and Œ∑ so
that
p1Œæx + q1Œæy = p2Œ∑x + q2Œ∑y = 0,
and hence A = C = 0. This means that
Œæ is constant on curves with dy
dx = q1
p1
,
Œ∑ is constant on curves with dy
dx = q2
p2
.
We can therefore write p1dy ‚àíq1dx = p2dy ‚àíq2dx = 0, and hence
(p1dy ‚àíq1dx) (p2dy ‚àíq2dx) = 0,
which gives
a dy2 ‚àí2b dx dy + c dx2 = 0.
(7.7)
As we shall see, this is the easiest equation to use to determine (Œæ, Œ∑). We call
(Œæ, Œ∑) the characteristic coordinate system, in terms of which (7.1) takes its
canonical form
œàŒæŒ∑ + b1(Œæ, Œ∑)œàŒæ + b2(Œæ, Œ∑)œàŒ∑ + b3(Œæ, Œ∑)œà = g (Œæ, Œ∑) .
(7.8)
The curves where Œæ is constant and the curves where Œ∑ is constant are called the
characteristic curves, or simply characteristics.
As we shall see, it is the
existence, or nonexistence, of characteristic curves for the three types of equation
that determines the distinctive properties of their solutions.
We discussed the
reduction of the wave equation to this canonical form in Section 3.9.1.
As a less trivial example, consider the hyperbolic equation
œÜxx ‚àísech4x œÜyy = 0.
(7.9)

178
CLASSIFICATION AND COMPLEX VARIABLE METHODS
Equation (7.7) shows that the characteristics are given by
dy2 ‚àísech4x dx2 =

dy + sech2x dx
 
dy ‚àísech2x dx

= 0,
and hence
dy
dx = ¬±sech2x.
The characteristics are therefore y ¬± tanh x = constant, and the characteristic
coordinates are Œæ = y + tanh x, Œ∑ = y ‚àítanh x. On writing (7.9) in terms of these
variables, with œÜ(x, y) = œà(Œæ, Œ∑), we Ô¨Ånd that its canonical form is
œàŒæŒ∑ = (Œ∑ ‚àíŒæ) (œàŒæ ‚àíœàŒ∑)
4 ‚àí(Œæ ‚àíŒ∑)2
,
(7.10)
in the domain (Œ∑ ‚àíŒæ)2 < 4.
Parabolic Equations: ac = b2
In this case,
A = aŒæ2
x + 2bŒæxŒæy + cŒæ2
y = (pŒæx + qŒæy)2 ,
C = aŒ∑2
x + 2bŒ∑xŒ∑y + cŒ∑2
y = (pŒ∑x + qŒ∑y)2 ,
so we can only construct one set of characteristic curves. We therefore take Œæ to be
constant on the curves p dy ‚àíq dx = 0. This gives us A = 0 and, since AC = B2,
B = 0. For any set of curves where Œ∑ is constant that is never parallel to the
characteristics, C does not vanish, and the canonical form is
œàŒ∑Œ∑ + b1(Œæ, Œ∑)œàŒæ + b2(Œæ, Œ∑)œàŒ∑ + b3(Œæ, Œ∑)œà = g (Œæ, Œ∑) .
(7.11)
We can now see that the diÔ¨Äusion equation is in canonical form.
As a further example, consider the parabolic equation
œàxx + 2cosec y œÜxy + cosec2y œÜyy = 0.
(7.12)
The characteristic curves satisfy
dy2 ‚àí2cosec y dx dy + cosec2y dx2 = (dy ‚àícosec y dx)2 = 0,
and hence
dy
dx = cosec y.
The characteristic curves are therefore given by x + cos y = constant, and we can
take Œæ = x + cos y as the characteristic coordinate.
A suitable choice for the
other coordinate is Œ∑ = y.
On writing (7.12) in terms of these variables, with
œÜ(x, y) = œà(Œæ, Œ∑), we Ô¨Ånd that its canonical form is
œàŒ∑Œ∑ = sin2 Œ∑ cos Œ∑ œàŒæ,
(7.13)
in the whole (Œæ, Œ∑)-plane.

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
179
Elliptic Equations: ac > b2
In this case, we can make neither A nor C zero, since no real characteristic curves
exist. Instead, we can simplify by making A = C and B = 0, so that the second
derivatives form the Laplacian, ‚àá2œà, and the canonical form is
œàŒæŒæ + œàŒ∑Œ∑ + b1(Œæ, Œ∑)œàŒæ + b2(Œæ, Œ∑)œàŒ∑ + b3(Œæ, Œ∑)œà = g (Œæ, Œ∑) .
(7.14)
Clearly, Laplace‚Äôs equation is in canonical form.
In order to proceed, we must solve
A ‚àíC = a

Œæ2
x ‚àíŒ∑2
x

+ 2b (ŒæxŒæy ‚àíŒ∑xŒ∑y) + c

Œæ2
y ‚àíŒ∑2
y

= 0,
B = aŒæxŒ∑x + b (Œ∑xŒæy + ŒæxŒ∑y) + cŒæyŒ∑y = 0.
We can do this by deÔ¨Åning œá = Œæ + iŒ∑, and noting that these two equations form
the real and imaginary parts of
aœá2
x + 2bœáxœáy + cœá2
y = 0,
and hence that
œáx
œáy
= ‚àíb ¬± i
‚àö
ac ‚àíb2
a
.
(7.15)
Now œá is constant on curves given by œáy dy + œáx dx = 0, and hence, from (7.15),
on
dy
dx = b ‚àìi
‚àö
ac ‚àíb2
a
.
(7.16)
By solving (7.16) we can deduce Œæ and Œ∑. For example, consider the elliptic equation
œÜxx + sech4x œÜyy = 0.
(7.17)
In this case, œá = Œæ + iŒ∑ is constant on the curves given by
dy
dx = ¬±i sech2x,
and hence y ‚àìi tanh x = constant. We can therefore take œá = y + i tanh x, and
hence Œæ = y, Œ∑ = tanh x.
On writing (7.17) in terms of these variables, with
œÜ(x, y) = œà(Œæ, Œ∑), we Ô¨Ånd that its canonical form is
œàŒæŒæ + œàŒ∑Œ∑ =
2Œ∑
1 ‚àíŒ∑2 œàŒ∑,
(7.18)
in the domain |Œ∑| < 1.
We can now describe some of the properties of the three diÔ¨Äerent types of equa-
tion. For more detailed information, the reader is referred to Kevorkian (1990) and
Carrier and Pearson (1988).

180
CLASSIFICATION AND COMPLEX VARIABLE METHODS
7.1.3
Properties of Hyperbolic Equations
Hyperbolic equations are distinguished by the existence of two sets of charac-
teristics. This allows us to establish two key properties. Firstly, characteristics
are carriers of information. For the wave equation, we saw in Section 3.9.1 that
solutions propagate at speeds ¬±c, which corresponds to propagation on the char-
acteristic curves, x ¬± ct = constant. Indeed, the use of the independent variable
t, time, instead of y suggests that the equation has an evolutionary nature. More
speciÔ¨Åcally, consider the Cauchy problem for a hyperbolic equation of the form
(7.1). In a Cauchy problem, we specify a curve C in the (x, y)-plane upon which we
know the Cauchy data; œÜ and the derivative of œÜ normal to C, ‚àÇœÜ/‚àÇn. The initial
value problem for the wave equation that we studied in Section 3.9.1 is a Cauchy
problem. Does this problem have a solution in general? Let‚Äôs assume that œÜ and
‚àÇœÜ/‚àÇn on C can be expanded as power series, and that the functions that appear
as coeÔ¨Écients in (7.1) can also be expanded as power series in the neighbourhood
of C. We can then write
œÜ(Œæ, Œ∑) = œÜ(Œæ, 0) + Œ∑ ‚àÇœÜ
‚àÇŒ∑ (Œæ, 0) + 1
2!Œ∑2 ‚àÇ2œÜ
‚àÇŒ∑2 (Œæ, 0) + ¬∑ ¬∑ ¬∑ ,
where the orthogonal coordinate system (Œæ, Œ∑) is set up so that Œ∑ = 0 is the curve
C, which is then parameterized by Œæ. As we have seen, we can write (7.1) in terms
of this new coordinate system as (7.3). We know œÜ(Œæ, 0) and the normal derivative,
‚àÇœÜ/‚àÇŒ∑(Œæ, 0), and, provided that the coeÔ¨Écient of ‚àÇ2œÜ/‚àÇŒ∑2 does not vanish on C,
we can deduce ‚àÇ2œÜ/‚àÇŒ∑2(Œæ, 0) from (7.3).
When does the coeÔ¨Écient of ‚àÇ2œÜ/‚àÇŒ∑2
vanish on C? Precisely when C is a characteristic curve. Provided that C is not
a characteristic curve, we can also deduce higher derivatives from derivatives of
(7.3), and hence construct a power series solution, valid in the neighbourhood of
C. The formal statement of this informally presented procedure is the Cauchy‚Äì
Kowalewski theorem, a local existence theorem (see Garabedian, 1964, for a
formal statement and proof). The eÔ¨Äects of the initial data propagate into the
(x, y)-plane on the characteristics, so it is inconsistent to specify initial data upon a
characteristic curve. In addition, the solution at any point (x, y) is only dependent
on the initial conditions that lie between the two characteristics through (x, y),
as shown in Figure 7.1. For the wave equation, this is immediately obvious from
d‚ÄôAlembert‚Äôs solution, (3.43).
Secondly, discontinuities in the second derivative of œÜ can propagate on charac-
teristic curves. To see this, consider a curve C in the (x, y)-plane, not necessarily
a characteristic, given by Œæ(x, y) = Œæ0. Suppose that œÜŒæŒæ is not continuous on C,
but that œÜ satisÔ¨Åes the hyperbolic equation on either side of C. Can we choose Œæ
in such a way that the equation is satisÔ¨Åed, even though œÜŒæŒæ

Œæ+
0 , Œ∑

Ã∏= œÜŒæŒæ

Œæ‚àí
0 , Œ∑

?
If we evaluate the equation on either side of the curve and subtract, we Ô¨Ånd that
A(Œæ0, Œ∑)

œÜŒæŒæ

Œæ+
0 , Œ∑

‚àíœÜŒæŒæ

Œæ‚àí
0 , Œ∑

= 0.
We can therefore satisfy the equation if A(Œæ0, Œ∑) = 0, and hence C is a characteristic
curve. We conclude that discontinuities in the second derivative can propagate on
characteristic curves. In general, œÜ and its Ô¨Årst derivatives are continuous, but for

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
181
y
x
Characteristics
(x, y)
Initial line
C
Fig. 7.1. The domain of dependence of the solution of a hyperbolic equation. The solution
at (x, y) depends only upon the initial data on the marked part of the initial line, C.
the wave equation, in which only second derivatives appear, discontinuities in œÜ
and its derivatives can also propagate on characteristics, as shown in Figure 3.10.
7.1.4
Properties of Elliptic Equations
For elliptic equations, A and C are never zero, and there are no characteristics.
Solutions are therefore inÔ¨Ånitely-diÔ¨Äerentiable. Moreover, there is no timelike vari-
able; for example, Laplace‚Äôs equation is written in terms of the spatial variables, x
and y. For physical problems that can be modelled using elliptic equations, bound-
ary value problems, rather than initial value problems, usually arise naturally; for
example the steady state diÔ¨Äusion problem discussed in Section 2.6.1. In fact, the
solution of the Cauchy problem for elliptic equations does not depend continuously
on the initial conditions, and is therefore not a sensible representation of any real,
physical problem.‚Ä† We can easily demonstrate this for Laplace‚Äôs equation.
Consider the Cauchy, or initial value, problem
œÜxx + œÜtt = 0 for t ‚©æ0, ‚àí‚àû< x < ‚àû,
(7.19)
subject to
œÜ(x, 0) = œÜ0(x),
œÜt(x, 0) = v0(x).
(7.20)
‚Ä† We say that the problem is ill-posed.

182
CLASSIFICATION AND COMPLEX VARIABLE METHODS
If œÜ0 = 0 and v0 = 1
Œª sin Œªx, a solution is œÜ = ÀÜœÜ(x, t), where
ÀÜœÜ(x, t) = 1
Œª2 sin Œªx sinh Œªt.
As Œª ‚Üí‚àû, ÀÜœÜt(x, 0) = 1
Œª sin Œªx ‚Üí0. However, for t > 0, max |ÀÜœÜ| =
1
Œª2 sinh Œªt ‚Üí‚àû
as Œª ‚Üí‚àû. The smaller the initial condition, the larger the solution. Now, let
Œ¶(x, t) be the general solution of (7.19) subject to (7.20). Consider the function
ÀÜŒ¶ = Œ¶ + ÀÜœÜ. The larger the value of Œª, the closer ÀÜŒ¶t(x, 0) is to v0(x). However, |ÀÜŒ¶|
increases without bound for t > 0 as Œª increases. An arbitrarily small change in
the boundary data produces an arbitrarily large change in the solution.
Laplace‚Äôs equation is the simplest possible elliptic equation, and, as we have
seen, arises in many diÔ¨Äerent physical contexts. Let‚Äôs consider some more of its
properties.
Theorem 7.1 (The maximum principle for Laplace‚Äôs equation) Let D be
a connected, bounded, open set in two or three dimensions, and œÜ a solution of
Laplace‚Äôs equation in D. Then œÜ attains its maximum and minimum values on
‚àÇD, the boundary of D, and nowhere in the interior of D, unless œÜ is a constant.
Proof The idea of the proof is straightforward. For example in two dimensions, at
a local maximum in the interior of D, œÜx = œÜy = 0, œÜxx ‚©Ω0 and œÜyy ‚©Ω0. At a
maximum with œÜxx < 0 or œÜyy < 0, we have ‚àá2œÜ = œÜxx + œÜyy < 0, and œÜ cannot
be a solution of Laplace‚Äôs equation. However, it is possible to have œÜxx = œÜyy = 0
at an interior local maximum, for example, if the Taylor series expansion close to
(x, y) = (x0, y0) is œÜ = (x ‚àíx0)4 + (y ‚àíy0)4 + ¬∑ ¬∑ ¬∑ . Although this is clearly not a
solution of Laplace‚Äôs equation, we do need to do a little work in order to exclude
this possibility in general.
Let œà = œÜ + œµ|x|2, with œµ > 0 and |x|2 = x2 + y2 in two dimensions and
|x|2 = x2 + y2 + z2 in three dimensions. We then have
‚àá2œà = ‚àá2œÜ + œµ‚àá2|x|2 = kœµ > 0,
where k = 4 in two dimensions and k = 6 in three dimensions. Since ‚àá2œà ‚©Ω0 at an
interior local maximum, we conclude that œà has no local maximum in the interior
of D, and hence that œà must attain its maximum value on the boundary of D, say
at x = x0. But, by deÔ¨Ånition
œÜ(x) ‚©Ωœà(x) ‚©Ωœà(x0) = œÜ(x0) + œµ|x0|2 ‚©Ωmax
‚àÇD œÜ + œµl2,
where l is the greatest distance from ‚àÇD to the origin. Since this is true for all
œµ > 0, we can make œµ arbitrarily small, and conclude that
œÜ(x) ‚©Ωmax
‚àÇD œÜ for all x ‚ààD.
Similarly, ‚àíœÜ, which also satisÔ¨Åes Laplace‚Äôs equation, attains its maximum value
on ‚àÇD, and hence œÜ attains its minimum value on ‚àÇD.

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
183
We can use the maximum principle to show that the Dirichlet problem for
Laplace‚Äôs equation has a unique solution.
Theorem 7.2 The Dirichlet problem
‚àá2œÜ = 0 for x ‚ààD,
(7.21)
with
œÜ(x) = f(x) on ‚àÇD,
(7.22)
has a unique solution.
Proof Let œÜ1 and œÜ2 be solutions of (7.21) subject to (7.22). Then œà = œÜ1 ‚àíœÜ2
satisÔ¨Åes
‚àá2œà = 0 for x ‚ààD,
(7.23)
with
œà(x) = 0 on ‚àÇD.
(7.24)
By Theorem 7.1, œà attains its maximum and minimum values of ‚àÇD, so that
0 ‚©Ωœà(x) ‚©Ω0 for x ‚ààD, and hence œà ‚â°0. This means that œÜ1 = œÜ2, and hence
that there is unique solution.
The maximum principle can also be used to study other elliptic equations. For
example, consider the boundary value problem
‚àá2œÜ = ‚àí1 for x ‚ààX =

(x, y)

|x|
a + |y|
b < 1
	
,
(7.25)
with a > b > 0, subject to
œÜ = 0 on ‚àÇX.
(7.26)
How big is œÜ(0, 0)? We can obtain some bounds on this quantity by transforming
(7.25) into Laplace‚Äôs equation. We deÔ¨Åne œà = (x2 + y2)/4, so that ‚àá2œà = 1, and
let ÀÜœÜ = œÜ + œà, so that ‚àá2 ÀÜœÜ = 0. We can also see that ÀÜœÜ = œà on ‚àÇX, and that
ÀÜœÜ(0, 0) = œÜ(0, 0). Theorem 7.1 then shows that
min
‚àÇX œà ‚©ΩœÜ(0, 0) ‚©Ωmax
‚àÇX œà.
We can therefore bound œÜ(0, 0) using the maximum and minimum values of œà =
(x2 + y2)/4 on the boundary of X. The maximum value is clearly a2/4. Since X
is symmetric about both coordinate axes, we can Ô¨Ånd the minimum value of œà by
determining the value of the radius r0 for which the circle x2 +y2 = r2
0 just touches
the straight line x/a + y/b = 1, which forms the boundary of X in the quadrant
x > 0, y > 0. A little algebra shows that r2
0 = a2b2/(a2 + b2), and hence that
a2b2
4(a2 + b2) ‚©ΩœÜ(0, 0) ‚©Ωa2
4 .
Finally, the other type of boundary value problem that often arises for Laplace‚Äôs

184
CLASSIFICATION AND COMPLEX VARIABLE METHODS
equation is the Neumann problem (see Section 5.4), where the normal derivative of
œÜ is speciÔ¨Åed on the boundary of the solution domain. Such problems can only be
solved if the boundary data satisÔ¨Åes a solubility condition.
Theorem 7.3 A necessary condition for the existence of a solution of the Neumann
problem
‚àá2œÜ = 0 for x ‚ààD ‚äÇR3,
(7.27)
with
‚àÇœÜ
‚àÇn(x) = g(x) on ‚àÇD,
(7.28)
is

‚àÇD
g(x) d 2x = 0.
(7.29)
Proof If we integrate (7.27) over D and use the divergence theorem and (7.28), we
Ô¨Ånd that

D
‚àá2œÜ d 3x =

‚àÇD
n.‚àáœÜ d 2x =

‚àÇD
g(x) d 2x = 0.
This solubility condition has a simple interpretation in terms of inviscid, incom-
pressible, irrotational Ô¨Çuid Ô¨Çow, which we introduced in Section 2.6.2. In this case,
the Neumann problem speciÔ¨Åes a steady Ô¨Çow in D with the normal velocity of the
Ô¨Çuid on ‚àÇD given by g(x). Since the Ô¨Çuid is incompressible, such a Ô¨Çow can only
exist if the total Ô¨Çux into D through ‚àÇD is zero, as expressed by (7.29).
7.1.5
Properties of Parabolic Equations
Parabolic equations have just one set of characteristics. For example, for the
diÔ¨Äusion equation, KœÜxx = œÜt with K > 0, t is constant on the characteristic
curves. As we have seen, any localized disturbance is therefore felt everywhere in
‚àí‚àû< x < ‚àûinstantaneously, as we can see from Example 5 of Section 6.4. In
addition, solutions are inÔ¨Ånitely-diÔ¨Äerentiable with respect to x. We can also prove
a maximum principle for the diÔ¨Äusion equation.
Theorem 7.4 (The maximum principle for the diÔ¨Äusion equation) Let œÜ
be a solution of the diÔ¨Äusion equation. Consider the domain 0 ‚©Ωx ‚©ΩL, 0 ‚©Ωt ‚©ΩT.
Then œÜ attains its maximum value on x = 0, 0 ‚©Ωt ‚©ΩT, or x = L, 0 ‚©Ωt ‚©ΩT, or
t = 0, 0 ‚©Ωx ‚©ΩL.
Proof
This is very similar to the proof of the maximum principle for Laplace‚Äôs
equation, and we leave it as Exercise 7.8(a).

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
185
We can use this maximum principle to show that the solution of the initial‚Äì
boundary value problem given by
‚àÇœÜ
‚àÇt = K ‚àÇ2œÜ
‚àÇx2
for t > 0, 0 < x < L,
(7.30)
subject to
œÜ(x, 0) = œÜ0(x) for 0 ‚©Ωx ‚©ΩL,
(7.31)
and
œÜ(0, t) = œÜ1(t),
œÜ(L, t) = œÜ2(t) for t > 0,
(7.32)
with œÜ0, œÜ1 and œÜ2 prescribed functions, is unique. We leave the details as Exer-
cise 7.8(b).
We have now seen that parabolic equations have a single set of characteristics,
and that for a canonical example, the diÔ¨Äusion equation, there is a maximum prin-
ciple, so that parabolic equations share some of the features of both hyperbolic and
elliptic equations. We end this section by stating a useful theorem that holds for
reaction‚ÄìdiÔ¨Äusion equations, which are parabolic. We will meet reaction‚ÄìdiÔ¨Äusion
equations frequently in Part 2.
Theorem 7.5 (A comparison theorem for reaction‚ÄìdiÔ¨Äusion equations)
Consider the reaction‚ÄìdiÔ¨Äusion equation
œÜt = K‚àá2œÜ + f (œÜ, x, t)
for x ‚ààD, t > 0,
(7.33)
with K > 0 and f a smooth function. If ¬ØœÜ(x, t) is a bounded function that satisÔ¨Åes
¬ØœÜt ‚©æK‚àá2 ¬ØœÜ + f
¬ØœÜ, x, t

for x ‚ààD, t > 0,
we say that ¬ØœÜ is a supersolution of (7.33). Similarly, if œÜ(x, t) is a bounded function
that satisÔ¨Åes
œÜt ‚©ΩK‚àá2œÜ + f

œÜ, x, t

for x ‚ààD, t > 0,
we say that œÜ is a subsolution of (7.33). If there also exist constants Œ± and Œ≤ with
Œ±2 + Œ≤2 Ã∏= 0, such that
Œ±¬ØœÜ ‚àíŒ≤ ‚àÇ¬ØœÜ
‚àÇn ‚©æŒ±œÜ ‚àíŒ≤ ‚àÇœÜ
‚àÇn
for x ‚àà‚àÇD, t > 0,
and
¬ØœÜ(x, 0) ‚©æœÜ(x, 0) for x ‚ààD,
then
¬ØœÜ(x, t) ‚©æœÜ(x, t) for x ‚ààD, t > 0.
We will not prove this result here (see Grindrod, 1991, for further details).
To see how this theorem can be used, consider the case f = œÜ(1 ‚àíœÜ), with
‚àÇœÜ/‚àÇn = 0 on ‚àÇD. This problem arises in a model for the propagation of chemical
waves (see Billingham and King, 2001). If 0 ‚©ΩœÜ(x, 0) ‚©Ω1, then by taking Ô¨Årstly
¬ØœÜ = 1, œÜ = œÜ, and secondly ¬ØœÜ = œÜ, œÜ = 0, we Ô¨Ånd that 0 ‚©ΩœÜ(x, t) ‚©Ω1 for t ‚©æ0.

186
CLASSIFICATION AND COMPLEX VARIABLE METHODS
In other words, the comparison theorem allows us to determine upper and lower
bounds on the solution (see Exercise 7.9 for another example).
7.2
Complex Variable Methods for Solving Laplace‚Äôs Equation
In Section 2.6.2 we described how steady, inviscid, incompressible, irrotational Ô¨Çuid
Ô¨Çow, commonly referred to as ideal Ô¨Çuid Ô¨Çow, past a rigid body can be modelled
as a boundary value problem for Laplace‚Äôs equation, ‚àá2œÜ = 0, in the Ô¨Çuid, with
‚àÇœÜ/‚àÇn = 0 on the boundary of the body, where œÜ is the velocity potential and
u = ‚àáœÜ the velocity Ô¨Åeld.
Appropriate conditions at inÔ¨Ånity also need to be
prescribed, for example, a uniform stream. Once œÜ is known, we can determine the
pressure using Bernoulli‚Äôs equation, (2.18). In general, solutions of this boundary
value problem are hard to Ô¨Ånd, except for the simplest body shapes. However, two-
dimensional Ô¨Çow is an exception, since powerful complex variable methods can,
without too much eÔ¨Äort, give simple descriptions of the Ô¨Çow past many simply-
connected body shapes, such as aircraft wing sections, channels with junctions and
Ô¨Çows with free surfaces (see, for example, Milne-Thompson, 1960).
7.2.1
The Complex Potential
Recall from Section 2.6.2 that the velocity in a two-dimensional ideal Ô¨Çuid Ô¨Çow,
u = (u, v) = (œÜx, œÜy), satisÔ¨Åes the continuity equation, ux + vy = 0.
We can
therefore introduce a stream function, œà(x, y), deÔ¨Åned by u = œày, v = ‚àíœàx,
which, for suÔ¨Éciently smooth functions, satisÔ¨Åes the continuity equation identically.
Elementary calculus shows that the stream function is constant on any streamline
in the Ô¨Çow, and the change in œà between any two streamlines is equal to the Ô¨Çux of
Ô¨Çuid between them. If we now look at the deÔ¨Ånitions we have for the components
of velocity, for compatibility we need œÜx = œày and œÜy = ‚àíœàx.
These are the
Cauchy‚ÄìRiemann equations for the complex function w = œÜ + iœà and, with our
smoothness assumption, imply that w(z) is an analytic function of the complex
variable z = x + iy in the domain occupied by the Ô¨Çuid.‚Ä† The quantity w(z) is
called the complex potential for the Ô¨Çow and, for simple Ô¨Çows, can be found
easily. We can also see that
dw
dz = œÜx + iœàx = u ‚àíiv = qe‚àíiŒ∏,
where q is the magnitude of the velocity and Œ∏ the angle that it makes with the
x-axis, so that, once we know w, we can easily compute the components of the
velocity, and vice versa.
Let‚Äôs consider some examples.
(i) A uniform stream Ô¨Çowing at an angle Œ± to the horizontal has u = U cos Œ±,
v = U sin Œ±, so that
dw
dz = u ‚àíiv = Ue‚àíiŒ±,
‚Ä† See Appendix 6 for a reminder of some basic ideas in the theory of complex variables.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE‚ÄôS EQUATION
187
and hence w = Ue‚àíiŒ±z.
(ii) A point vortex is a system of concentric circular streamlines centred on
z = z0, with complex potential
w = iŒ∫
2œÄ log (z ‚àíz0) .
If we now deÔ¨Åne the circulation to be
'
C u¬∑dr, where C is any closed contour
that encloses z = z0, the point vortex has circulation Œ∫. By taking real and
imaginary parts, we can see that œÜ = ‚àíŒ∫Œ∏/2œÄ, œà = Œ∫ log r/2œÄ, where r is
the polar coordinate centred on z = z0. The streamlines are therefore given
by œà = constant, and hence r = constant, a family of concentric circles, as
expected.
(iii) A point source of Ô¨Çuid at z = z0 ejects m units of Ô¨Çuid per unit area per
unit time. Its complex potential is
w = m
2œÄ log(z ‚àíz0),
for a source at z = z0. The streamlines are straight lines passing through
z = z0 and, since
dw
dz = qe‚àíiŒ∏ = m
2œÄ
1
z ‚àíz0
,
we have that q = m/2œÄr, where r is the polar coordinate centred at z = z0,
and that the Ô¨Çow is purely radial. It is simple to show that the Ô¨Çow through
any closed curve, C, that encloses z0 is

C
u ¬∑ n dl = m,
as expected.
A word of warning is required here. The Ô¨Årst of these complex potentials is analytic
over the whole z-plane. The second and third fail to be analytic at z = z0. In reality,
we need to invoke some extra physics close to this point. For example, any real
source of Ô¨Çuid will be of Ô¨Ånite size, and, close to z = z0, we need to take this into
account. For a point vortex, close to the singularity the eÔ¨Äect of viscosity becomes
important, and we need to include it. In each case, this can be done by deÔ¨Åning
an inner asymptotic region, using the methods that we will discuss in Chapter 12.
Alternatively, if there are no sources or vortices in the Ô¨Çow domain that we are
considering, but we wish to use sources and vortices to model the Ô¨Çow, the points
where the complex potential is not analytic must be excluded from the Ô¨Çow domain.
We will see an example of this in the next section.
7.2.2
Simple Flows Around Blunt Bodies
Since Laplace‚Äôs equation is linear, complex potentials can be added together to
produce more complicated Ô¨Çuid Ô¨Çow Ô¨Åelds. For example, consider a uniform stream
of magnitude U Ô¨Çowing parallel to the x-axis and a source of strength 2œÄm situated

188
CLASSIFICATION AND COMPLEX VARIABLE METHODS
on the x-axis at z = a. The complex potential is w = Uz +m log(z ‚àía), from which
we can deduce that
œà = Uy + m tan‚àí1

y
x ‚àía

.
(7.34)
This is an odd function of y, so that the velocity Ô¨Åeld associated with it is an even
function of y, as we would expect from the symmetry of the Ô¨Çow. The x-axis (y = 0)
is a streamline with œà = 0, whilst for x > 0 there is another streamline with œà = 0,
given by
y = (a ‚àíx) tan
Uy
m

,
(7.35)
which is a blunt-nosed curve. The streamlines are shown in Figure 7.2. As there is
no Ô¨Çow across a streamline, we can replace it with the surface of a rigid body without
aÔ¨Äecting the Ô¨Çow pattern. The complex potential (7.34) therefore represents the
Ô¨Çow past a blunt body of the form given by (7.35). At the point z = a ‚àím/U,
dw/dz = 0, and hence both components of the velocity vanish there. This is called
a stagnation point of the Ô¨Çow. Note that the singularity due to the point source
does not lie within the Ô¨Çow domain, and can be disregarded.
‚àí2
‚àí1
0
1
1.5
2
‚àí2
‚àí1
‚àí1.5
‚àí0.5
‚àí1.5
‚àí0.5
0.5
0
0.5
1
1.5
2
x
y
BLUNT
BODY 
Fig. 7.2. The Ô¨Çow past a blunt body given by the stream function (7.34) with U = m =
a = 1. There is a stagnation point at the origin.
Adding potentials is one way to construct ideal Ô¨Çuid Ô¨Çows around rigid bodies.
However, since it is an inverse method (for a given Ô¨Çow Ô¨Åeld, we can introduce a
rigid body bounded by any streamline), it has its drawbacks. We would prefer a

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE‚ÄôS EQUATION
189
direct method, whereby we can calculate a stream function for a given rigid body.
Before we describe such a method, we need the circle theorem.
Theorem 7.6 (The circle theorem) If w = f(z) is a complex potential for a
Ô¨Çow with no singularities in the domain |z| ‚©Ωa, then w = f(z) + f ‚àó(a2/z) is a
complex potential for the same Ô¨Çow obstructed by a circle of radius a centred at the
origin.
Proof Firstly, since f(z) is analytic in |z| < a, f ‚àó(a2/z) is analytic in |z| > a, as
a2/z is just an inversion of the point z. Secondly, on the boundary of the circle,
z = aeiŒ∏ with 0 ‚©ΩŒ∏ ‚©Ω2œÄ, so that f(z) + f ‚àó(a2/z) = f(aeiŒ∏) + f ‚àó(ae‚àíiŒ∏), which, as
it is the sum of complex conjugate functions, is real. Therefore, œà = 0 on z = aeiŒ∏,
and the boundary of the circle is a streamline. Finally, since w ‚àºf(z) + f ‚àó(0) as
|z| ‚Üí‚àû, the Ô¨Çow in the far Ô¨Åeld is given by f(z).
We can illustrate this theorem by Ô¨Ånding the complex potential for a uniform
stream of strength U Ô¨Çowing past a circle of radius a. The complex potential for the
stream is w = Ue‚àíiŒ±z, which has no singularities in |z| ‚©Ωa. By the circle theorem,
the complex potential for the Ô¨Çow when the circular boundary is introduced is
w = Ue‚àíiŒ±z + UeiŒ± a2
z .
When Œ± = 0, the Ô¨Çow is symmetric about the x-axis, with
dw
dz = U

1 ‚àía2
z2

,
and dw/dz ‚àºU as |z| ‚Üí‚àû, as expected. By taking real and imaginary parts, we
can recover the velocity Ô¨Åeld, potential and stream function, which we discussed in
Section 2.6.2. Before leaving this problem, we note that the complex potential is
not unique, since we can add a point vortex, centred upon z = 0, and still have the
circle as a streamline, without introducing a singularity in the Ô¨Çow domain. When
Œ± = 0, this gives the potential
w = U

z + a2
z

+ iŒ∫
2œÄ log z.
(7.36)
The introduction of this nonzero circulation about the circle has important conse-
quences, as it breaks the symmetry (see Figure 7.3), and, as we shall now show,
causes a nonzero lift force on the body.
In order to calculate the force exerted by an ideal Ô¨Çuid Ô¨Çow upon a rigid body,
consider an inÔ¨Ånitesimal element of arc on the surface of the body, ds. Let the
tangent to this element of arc make an angle Œ∏ with the x-axis. The force on this
inÔ¨Ånitesimal arc is due to the pressure, and given by
‚àíp sin Œ∏ ds + ip cos Œ∏ ds = ipeiŒ∏ds.

190
CLASSIFICATION AND COMPLEX VARIABLE METHODS
‚àí5
0
5
‚àí5
0
5
x
y
Œ∫ = 0
‚àí5
0
5
‚àí5
0
5
x
y
Œ∫ = 20
Fig. 7.3. Flow past a circle, with no circulation and with Œ∫ = 20.
Since dz = dx + i dy = cos Œ∏ ds + i sin Œ∏ ds = eiŒ∏ds, the total force on the body is
Fx + iFy =

body
ipeiŒ∏ ds =

body
ip dz.
Now, from Bernoulli‚Äôs equation, (2.18), p = p0 ‚àí1
2œÅq2 and qe‚àíiŒ∏ = dw/dz, where Œ∏
is the angle of the Ô¨Çow, since this is equal to the tangential angle at the surface of
the rigid body, so that
Fx + iFy =

body
i

p0 ‚àí1
2œÅ

dw
dz

2
dz.
As p0 is a constant,
Fx + iFy = ‚àí1
2œÅi

body

dw
dz

2
dz.
To get a more usable result, it is convenient to manipulate this. Firstly, taking the
complex conjugate,
Fx ‚àíiFy = 1
2œÅi

body

dw
dz

2
dz‚àó.
Now note that

dw
dz

2
= dw
dz
dw
dz
‚àó
= dw
dz
dw‚àó
dz‚àó,
so that

dw
dz

2
dz‚àó= dw
dz dw‚àó.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE‚ÄôS EQUATION
191
Since the boundary of the body is a streamline, dw‚àó= dw, so that dw‚àó= (dw/dz)dz,
and hence
Fx ‚àíiFy = 1
2œÅi

body
dw
dz
2
dz.
(7.37)
In this form, the force can usually be evaluated using the residue theorem. For the
example of Ô¨Çow around a circle with circulation Œ∫, we have
Fx ‚àíiFy = 1
2œÅi

body

U

1 ‚àía2
z2

+ iŒ∫
2œÄz
	2
dz.
After evaluating the residue, which is just due to the simple pole, we Ô¨Ånd that
Fx ‚àíiFy = ‚àíiœÅUŒ∫, so that there is a vertical force of magnitude œÅUŒ∫, which tries
to lift the body in the direction of increasing y.
This lift force arises from the
pressure distribution around the circle, with pressures at the lower surface of the
body higher than those on the upper surface. As we shall see in Example 4 in the
next section, it is also the circulation around an aerofoil that provides the lift.
7.2.3
Conformal Transformations
Suppose we have a Ô¨Çow in the z-plane past a body whose shape cannot imme-
diately be seen to be the streamline of a simple Ô¨Çow. If we can transform this
problem into a new plane, the Œ∂-plane, where the shape of the body is simpler,
such as a half-plane or a circle, we may be able to solve for the Ô¨Çow in the Œ∂-plane,
and then invert the transformation to get the Ô¨Çow in the original, z-plane. Specif-
ically, if we seek w = w(z), the complex potential in the z-plane, and we have a
transformation, Œ∂ = f(z), which maps the surface of the rigid body, and the Ô¨Çow
outside it, onto a Ô¨Çow outside a half-plane or circle in the Œ∂-plane, then, by deÔ¨Åning
W(Œ∂) = w(z), we have a correspondence between Ô¨Çow and geometry in both planes.
Any streamline in the z-plane transforms to a streamline in the Œ∂-plane because of
this correspondence in complex potentials. The complex velocity is
dw
dz = dw
dŒ∂
dŒ∂
dz = dw
dŒ∂ f ‚Ä≤(z).
If |f ‚Ä≤(z)| is bounded and nonzero, except perhaps for isolated points on the bound-
ary of the domain to be transformed, we say that the transformation between the
z- and Œ∂-planes is conformal, and a unique inverse transformation can be deÔ¨Åned.
Conformal mappings are so called because they also preserve the angle between
line segments except at any isolated points where f ‚Ä≤(z) is zero or inÔ¨Ånity (see Ex-
ercise 7.10).
A consequence of this is that if we want to map a domain whose
boundary has corners to a domain with a smooth boundary, we must have f ‚Ä≤(z)
equal to zero or inÔ¨Ånity at these corner points. Such points can cause diÔ¨Éculties.
For example, if |f ‚Ä≤(z)| = ‚àû, we could induce an inÔ¨Ånite velocity in the z-plane,
which is unphysical.
Although beautifully simple in principle, there is a practical diÔ¨Éculty with this

192
CLASSIFICATION AND COMPLEX VARIABLE METHODS
method: how can we construct the transformation, f(z)? Fortunately, dictionar-
ies of common conformal transformations are available (see, for example, Milne-
Thompson, 1952). Let‚Äôs try to make things clearer with some examples.
Example 1: Mapping a three-quarter-plane onto a half-plane
Consider the three-quarter-plane that lies in the z-plane shown in Figure 7.4(a).
How can we map this onto the half-plane in the Œ∂-plane shown in Figure 7.4(b)?
Let‚Äôs try the transformation Œ∂ = Azn, so that arg Œ∂ = arg A + n arg z. On BC,
arg z = ‚àíœÄ/2, and we require B‚Ä≤C‚Ä≤, the image of BC, to have arg Œ∂ = 0, which
shows that arg A = nœÄ/2. On AB, arg z = œÄ, and we require A‚Ä≤B‚Ä≤, the image
of AB, to have arg Œ∂ = œÄ. This means that œÄ = arg A + nœÄ and hence n = 2/3,
arg A = œÄ/3. This means that the family of transformations Œ∂ = |A|eiœÄ/3z2/3 will
map the three-quarter-plane to the half-plane. If we further require that the image
of z = ‚àí1 should be Œ∂ = ‚àí1, then |A| = 1, and Œ∂ = eiœÄ/3z2/3. Note that this
transformation is not conformal at z = 0, since the angle of the boundary changes
there.
A
(a)
(b)
C
y
x
B
A
B
C
Œæ
Œ∑
Œ∂-plane
z-plane
Fig. 7.4. Mapping a three-quarter-plane to a half-plane.
Example 2: Mapping a strip onto a half-plane
Consider the strip of width 2 that lies in the z-plane shown in Figure 7.5(a). How
can we map this onto the half-plane in the Œ∂-plane shown in Figure 7.5(b)? Let‚Äôs
try the transformation z = K log Œ∂ + L. On A‚Ä≤B‚Ä≤, arg Œ∂ = 0, so if Œ∂ = Œæ + iŒ∑,
z = K log |Œæ| + L. If K is real, the choice L = ‚àíi makes z range from ‚àíi ‚àí‚àûto
‚àíi+‚àû, as required. On C‚Ä≤D‚Ä≤, arg Œ∂ = œÄ, so z = K log |Œæ|+iœÄK‚àíi. If we now choose
iœÄK = 2i, then z ranges from i ‚àí‚àûto i + ‚àû, as required. The transformation is
therefore
z = 2
œÄ log Œ∂ ‚àíi,
Œ∂ = ieœÄz/2,
which is conformal in the Ô¨Ånite z-plane.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE‚ÄôS EQUATION
193
D
A
B
C
C'
D'
A'
B'
Œæ
Œ∑
Œ∂-plane
z-plane
(a)
(b)
y
x
i
‚àíi
Fig. 7.5. Mapping a strip to a half-plane.
We now pause to note that, in both of these examples, we can use the conformal
mapping to solve the Dirichlet problem for Laplace‚Äôs equation, in the three-quarter-
plane and strip respectively, by mapping to the half-plane. We can solve Laplace‚Äôs
equation in the half-plane using the formula (5.30), which we derived using Fourier
transforms.
We can then easily write down the result in the z-plane using the
inverse transformation. To justify this, note that if œÜ(x, y) = Œ¶(x(Œæ, Œ∑), y(Œæ, Œ∑)), we
can diÔ¨Äerentiate to show that
‚àÇ2œÜ
‚àÇx2 + ‚àÇ2œÜ
‚àÇy2 =

dŒ∂
dz

2 ‚àÇ2Œ¶
‚àÇŒæ2 + ‚àÇ2Œ¶
‚àÇŒ∑2

,
(7.38)
so that, provided Œ∂ = f(z) is conformal, a function that is harmonic in the (Œæ, Œ∑)-
plane is harmonic in the (x, y)-plane, and vice versa (see Exercise 7.11).
We can illustrate this by solving a Dirichlet problem for Laplace‚Äôs equation in a
strip, namely
‚àá2œÜ = 0 for ‚àí‚àû< x < ‚àûand ‚àí1 < y < 1,
(7.39)
subject to
œÜ(x, ‚àí1) = 0,
œÜ(x, 1) = e‚àíx2 for ‚àí‚àû< x < ‚àû.
(7.40)
We know that the transformation that maps this strip in the z = x + iy-plane to
the upper-half-plane in the Œ∂ = Œæ + iŒ∑-plane is Œ∂ = ieœÄz/2, and hence
Œæ = ‚àíeœÄx/2 sin
1
2œÄy

,
Œ∑ = eœÄx/2 cos
1
2œÄy

.
In the Œ∂-plane, (7.39) and (7.40) become
‚àÇ2Œ¶
‚àÇŒæ2 + ‚àÇ2Œ¶
‚àÇŒ∑2 = 0 for ‚àí‚àû< Œæ < ‚àûand 0 < Œ∑ < ‚àû,
subject to
œÜ(x, 0) =

0
for 0 < Œæ < ‚àû,
exp

‚àí4
œÄ2 log2 (‚àíŒæ)

for ‚àí‚àû< Œæ < 0,

194
CLASSIFICATION AND COMPLEX VARIABLE METHODS
and Œ¶ ‚Üí0 as Œæ2 + Œ∑2 ‚Üí‚àû. The solution, given by (5.30), is
Œ¶(Œæ, Œ∑) = 1
œÄ
 0
s=‚àí‚àû
Œ∑ exp

‚àí4
œÄ2 log2 (‚àís)

(Œæ ‚àís)2 + Œ∑2
ds,
and hence
œÜ(x, y) = 1
œÄ
 0
s=‚àí‚àû
eœÄx/2 cos
 1
2œÄy

exp

‚àí4
œÄ2 log2 (‚àís)


eœÄx/2 sin
 1
2œÄy

+ s
2 + eœÄx cos2  1
2œÄy
 ds.
Example 3: Flow past a Ô¨Çat plate
Consider a plate of length 2a positioned perpendicular to a uniform Ô¨Çow with speed
U, as shown in Figure 7.6(a) in the z-plane. We want to map the exterior of the
plate to the exterior of a circle in the Œ∂-plane, as shown in Figure 7.6(b). This can
be done using
z = 1
2i

Œ∂ + a2
Œ∂

.
(7.41)
To show this, we write the surface of the circle as Œ∂ = aeiŒ∏, so that z = ia cos Œ∏.
The points labelled in Figure 7.6(a) in the z-plane then map to the corresponding
points labelled in Figure 7.6(b). Following the sequence A‚Ä≤B‚Ä≤C‚Ä≤D‚Ä≤E‚Ä≤, the Ô¨Çuid is on
the right hand side in the Œ∂-plane, and consequently is on the right hand side in
the z-plane, so that the outside of the circle maps to the outside of the plate. As
|Œ∂| ‚Üí‚àû, z ‚àº1
2iŒ∂, so that
dw
dz ‚àº‚àí2idw
dŒ∂ .
Since we require that dw/dz ‚àºU, we must have dw/dŒ∂ ‚àºUi/2, so that the stream
at inÔ¨Ånity in the Œ∂-plane is half the strength of the one in the z-plane, and has been
rotated through œÄ/2 radians.
C'
A'
B'
a
Œ∏
D'
E'
D B
E A
(a)
(b)
ia
C
‚àíia
x
z-plane
y
Œ∑
Œ∂-plane
‚Ä¢
‚Ä¢
Œæ
Fig. 7.6. Mapping a Ô¨Çat plate to a unit circle.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE‚ÄôS EQUATION
195
We can now solve the Ô¨Çow problem in the Œ∂-plane using the circle theorem, to
give
w(Œ∂) = 1
2UeiœÄ/2Œ∂ + 1
2Ue‚àíiœÄ/2 a2
Œ∂ .
(7.42)
Next, we need to write this potential in terms of z. From (7.41),
Œ∂2 ‚àí2z
i Œ∂ + a2 = 0,
and hence
Œ∂ = ‚àíi
#
z2 + a2 + z

,
choosing this root so that |Œ∂| ‚Üí‚àûas |z| ‚Üí‚àû. From the potential (7.42), we Ô¨Ånd
that
w = 1
2U
#
z2 + a2 + z +
a2
‚àö
z2 + a2 + z

.
(7.43)
Although this result is correct, it does give rise to inÔ¨Ånite velocities at z = ¬±ia, the
ends of the plate, which are not physical, and arise because of the sharpness of the
boundary at these points. In reality, viscosity will become important close to these
points, and lead to Ô¨Ånite velocities.
Example 4: Flow past an aerofoil
As we have seen, the transformation
z = 1
2

Œ∂ + 1
Œ∂

(7.44)
maps the exterior of a unit circle in the Œ∂-plane to the exterior of a Ô¨Çat plate in
the z-plane. In contrast, the exterior of circles of radius a > 1 whose centres are
not at the origin, but which still pass through the point Œ∂ = 1 and enclose the
point Œ∂ = ‚àí1, are mapped to shapes, known as Joukowski proÔ¨Åles, that look
rather like aerofoils. These shapes have two distinguishing characteristics: a blunt
nose, or leading edge, and a sharp tail, or trailing edge, at z = Œ∂ = 1, as shown in
Figure 7.7.
Let‚Äôs now consider the Ô¨Çow of a stream of strength U, at an angle Œ± to the hori-
zontal, around a Joukowski proÔ¨Åle. Since dz/dŒ∂ = 0 at Œ∂ = 1, the transformation is
not conformal at the trailing edge, and the velocity will be inÔ¨Ånite there, unless we
can make the Ô¨Çow in the transform plane have a stagnation point there. This can
be achieved by including an appropriate circulation around the aerofoil. We must
choose the strength of the circulation to make the velocity Ô¨Ånite at the trailing
edge. This is called the Kutta condition on the Ô¨Çow.
If the centre of the circle in the Œ∂-plane is at Œ∂ = Œ∂c, we must begin by making
another conformal transformation, Œ∂ = (Œ∂ ‚àíŒ∂c) /a, so that we can use the circle
theorem. The Ô¨Çow in the ¬ØŒ∂-plane is a uniform stream and a point vortex, so that
¬Øw(¬ØŒ∂) = V e‚àíiŒ≤ ¬ØŒ∂ + V eiŒ≤ 1
¬ØŒ∂ + iŒ∫
2œÄ log ¬ØŒ∂.

196
CLASSIFICATION AND COMPLEX VARIABLE METHODS
‚àí1
0
0.5
1
0.2
0
0.2
x
y
2
‚àí1
0
1
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
Œæ
Œ∑
Œ∂c = 0.25
‚àí1
‚àí0.5
0
0.5
0
0.2
0.4
x
y
‚àí2
‚àí1
0
1
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
Œæ
Œ∑
Œ∂c = 0.25( ‚àí1+i)
‚àí
‚àí
‚àí0.5
Fig. 7.7. Two examples of Joukowski proÔ¨Åles in the z-plane, and the corresponding circles
in the Œ∂-plane, where Œ∂ = Œæ + iŒ∑ and z = x + iy.
To determine V , we note that
z = 1
2

a¬ØŒ∂ + Œ∂c +
1
a¬ØŒ∂ + Œ∂c

,
and hence that z ‚àºa¬ØŒ∂/2 as |¬ØŒ∂| ‚Üí‚àû, so that we need Ue‚àíiŒ± = 2V e‚àíiŒ≤/a, and
hence V = aU/2 and Œ≤ = Œ±. The complex potential is therefore
¬Øw(¬ØŒ∂) = 1
2aU

e‚àíiŒ±¬ØŒ∂ + eiŒ± 1
¬ØŒ∂

+ iŒ∫
2œÄ log ¬ØŒ∂,
(7.45)
and hence
d ¬Øw
d¬ØŒ∂ = 1
2aU

e‚àíiŒ± ‚àíeiŒ± 1
¬ØŒ∂2

+ iŒ∫
2œÄ¬ØŒ∂ .
(7.46)
We can therefore make the trailing edge, ¬ØŒ∂ = (1 ‚àíŒ∂c) /a ‚â°¬ØŒ∂s, a stagnation point

EXERCISES
197
by choosing
Œ∫ = iœÄaU

e‚àíiŒ±¬ØŒ∂s ‚àíeiŒ± 1
¬ØŒ∂s

.
(7.47)
This expression is real, because |¬ØŒ∂s| = 1 and hence the quantities e‚àíiŒ±¬ØŒ∂s and eiŒ±/¬ØŒ∂s
are complex conjugate.
In order to calculate the force on the Joukowski proÔ¨Åle, we need to evaluate
Fx ‚àíiFy = 1
2œÅi

body
dw
dz
2
dz = 1
2œÅi

|¬ØŒ∂|=1
d ¬Øw
d¬ØŒ∂
2 d¬ØŒ∂
dz d¬ØŒ∂
= 1
2œÅia

|¬ØŒ∂|=1

 1
2U

e‚àíiŒ± ‚àíeiŒ±/¬ØŒ∂2
+ iŒ∫/2œÄa¬ØŒ∂
2
1
2

1 ‚àí1/(a¬ØŒ∂ + Œ∂c)2
d¬ØŒ∂.
This integral can be evaluated using residue calculus.
Since we know that the
integrand is analytic for |¬ØŒ∂| > 1, we can evaluate the residue at the point at inÔ¨Ånity
by making the transformation ¬ØŒ∂ = 1/s and evaluating the residue at s = 0. We
Ô¨Ånd, after some algebra, that
Fx ‚àíiFy = ‚àíiœÅŒ∫Ue‚àíiŒ±,
or equivalently,
Fx + iFy = œÅŒ∫Uei(œÄ/2+Œ±).
From this, we can see that the lift force on the aerofoil is directed perpendicular to
the incoming stream. The streamlines for the Ô¨Çow around the aerofoils illustrated
in Figure 7.7 are shown in Figure 7.8 with Œ± = œÄ/4.
Exercises
7.1
Find and sketch the regions in the (x, y)-plane where the equation
(1 + x)œÜxx + 2xyœÜxy + y2œÜyy = 0
is elliptic, parabolic and hyperbolic.
7.2
Determine the type of each of the equations
(a) œÜxx ‚àí5œÜxy + 5œÜyy = 0,
(b) œÜxx ‚àí2œÜxy + 3œÜyy + 24œÜy + 5œÜ = 0,
(c) œÜxx + 6yœÜxy + 9y2œÜyy + 4œÜ = 0,
and reduce them to canonical form.
7.3
Determine the characteristics of Tricomi‚Äôs equation, œÜxx = xœÜyy, in the
hyperbolic region, x > 0. Transform the equation to canonical form in (a)
the hyperbolic region and (b) the elliptic region.
7.4
Show that any solution, œÜ(x, t), of the one-dimensional wave equation,
(3.39), satisÔ¨Åes the diÔ¨Äerence equation y1‚àíy2+y3‚àíy4 = 0, where y1, y2, y3
and y4 are the values of the solution at successive corners of any quadrilat-
eral whose edges are characteristics.

198
CLASSIFICATION AND COMPLEX VARIABLE METHODS
‚àí2
‚àí1
0
1
2
‚àí1
‚àí0.5
0
0.5
1
x
y
‚àí2
‚àí1
0
1
2
‚àí1
‚àí0.5
0
0.5
1
x
y
Fig. 7.8. The streamlines for Ô¨Çow past the two examples of Joukowski proÔ¨Åles shown in
Figure 7.7, with Œ± = œÄ/4.
7.5
Consider the initial‚Äìboundary value problem
c‚àí2œÜtt + 2kœÜt ‚àíœÜxx = 0 for t > 0, x > 0,
subject to
œÜ(x, 0) = œÜt(x, 0) = 0 for x ‚©æ0,

EXERCISES
199
œÜ(0, t) = 1
2t2 for t ‚©æ0.
By diÔ¨Äerentiating the canonical form of the equation, obtain an equation
for the jump in a second derivative of œÜ across the characteristic curve
x = ct. By solving this equation, show that the jump in œÜtt across x = ct
is e‚àíkcx.
7.6
Consider the boundary value problem given by (7.25) and (7.26). By using
suitable functions œà = Ax2 + By2 that satisfy ‚àá2œà = 1 with A > 0 and
B > 0, show that
a2b2
2(a + b)2 ‚©ΩœÜ(0, 0) ‚©Ω
a2b2
2(a2 + b2).
7.7
The function œÜ(r, Œ∏) satisÔ¨Åes Laplace‚Äôs equation in region a < r < b, where
r and Œ∏ are polar coordinates and a and b are positive constants. Using
the maximum principle and the fact that ‚àá2 (log r) = 0, show that
m(R) log
 b
a

‚©Ωm(b) log
R
a

‚àím(a) log
R
b

,
where m(R) is the maximum value of œÜ on the circle r = R, with a < R < b.
7.8
(a) Prove Theorem 7.4. (b) Hence show that the solution of the initial‚Äì
boundary value problem given by (7.30) to (7.32) is unique.
7.9
Consider the initial‚Äìboundary value problem
œÜt = K‚àá2œÜ ‚àíœÜ3 for x ‚ààD, t > 0,
with k > 0, subject to
œÜ(x, 0) = œÜ0(x) for x ‚ààD,
and
‚àÇœÜ
‚àÇn = 0 for x ‚àà‚àÇD.
Use Theorem 7.5 to show that œÜ ‚Üí0 as t ‚Üí‚àû, uniformly in D.
7.10
Show that the angle between two line segments that intersect at z = z0
in the complex z-plane is unchanged by a conformal mapping Œ∂ = f(z),
provided that f ‚Ä≤(z0) is bounded and nonzero.
7.11
Show that, if œÜ(x, y) = Œ¶(x(Œæ, Œ∑), y(Œæ, Œ∑)), Œ∂ = Œæ + iŒ∑ and z = x + iy, then
‚àÇ2œÜ
‚àÇx2 + ‚àÇ2œÜ
‚àÇy2 =

dŒ∂
dz

2 ‚àÇ2Œ¶
‚àÇŒæ2 + ‚àÇ2Œ¶
‚àÇŒ∑2

.
7.12
Show that the mapping Œ∂ = sin (œÄz/2k) maps a semi-inÔ¨Ånite strip in the
z-plane to the upper half Œ∂-plane.
7.13
Show that the image of a circle of radius r in the z-plane under the trans-
formation Œ∂ = (z + 1/z) /2 is an ellipse.

200
CLASSIFICATION AND COMPLEX VARIABLE METHODS
7.14
Find the function T(x, y) that is harmonic in the lens-shaped region deÔ¨Åned
by the intersection of the circles |z ‚àíi| = 1 and |z ‚àí1| = 1 and takes the
value 0 on the upper circular arc and 1 on the lower circular arc.
Hint: Consider the eÔ¨Äect of the transformation Œ∂ = z/ (z ‚àí1 ‚àíi) on
this region.
7.15
Find the lift on an aerofoil section that consists of a unit circle centred on
the origin, joined at z = 1 to a horizontal Ô¨Çat plate of unit length, when a
unit stream is incident at an angle of 45o. Hint: Firstly apply a Joukowski
transformation to map the section to a straight line, and then use an inverse
Joukowski transformation to map this straight line to a circle.

Part Two
Nonlinear Equations and Advanced Techniques
201


CHAPTER EIGHT
Existence, Uniqueness, Continuity and
Comparison of Solutions of Ordinary DiÔ¨Äerential
Equations
Up to this point in the book we have implicitly assumed that each of the diÔ¨Äerential
equations that we have been studying has a solution of the appropriate form. This
is not always the case. For example, the equation
dy
dt =
#
y2 ‚àí1
has no real solution for |y| < 1. When we ask whether an equation actually has a
solution, we are considering the question of existence of solutions. A related issue
is that of uniqueness of solutions. If we have a diÔ¨Äerential equation for which
solutions exist, does the prescription of initial conditions specify a unique solution?
Not necessarily. Consider the equation
dy
dt = 3y2/3 subject to y(0) = 0 for y > 0.
If we integrate this separable equation we obtain y = (t ‚àíc)3. The initial condition
gives c = 0 and the solution y = t3. However, it is fairly obvious that y = 0 is
another solution that satisÔ¨Åes the initial condition. In fact, this equation has an
inÔ¨Ånite number of solutions that satisfy the initial condition, namely
y =

0
for 0 ‚©Ωt ‚©Ωc,
(t ‚àíc)3
for t > c,
for any c ‚©æ0.
Another question that we should consider arises from the process of mathematical
modelling. If a diÔ¨Äerential equation provides a model of a real physical system,
how sensitive are the solutions of the diÔ¨Äerential equation to changes in the initial
conditions? If we performed two experiments with nearly identical initial conditions
(exact repetition of an experiment being impossible in practice), should we expect
nearly identical outcomes?
In this chapter we will prove some rigorous results related to the issues we have
discussed above.

204
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
8.1
Local Existence of Solutions
We will start with the simplest type of scalar diÔ¨Äerential equation by proving a
local existence theorem for
dy
dt = f(y, t) subject to y(t0) = y0 in the domain |t ‚àít0| < Œ±.
(8.1)
Here, Œ± > 0 deÔ¨Ånes the size of the region where we will be able to show that a
solution exists. We begin by deÔ¨Åning a closed rectangle,
R = {(y, t) | |y ‚àíy0| ‚©Ωb, |t ‚àít0| ‚©Ωa} ,
centred upon the initial point, (y0, t0), within which we will make certain assump-
tions about the behaviour of f. If we integrate (8.1) with respect to t and apply
the initial condition, we obtain
y(t) = y0 +
 t
t0
f(y(s), s) ds.
(8.2)
This is an integral equation for the unknown solutions, y = y(t), which is equivalent
to the original diÔ¨Äerential equation. Our strategy now is to use this to produce a
set of successive approximations to the solution that we seek. We will do this
using the initial condition as the starting point.
We deÔ¨Åne
y0(t) = y0,
y1(t) = y0 +
 t
t0
f(y0, s) ds,
y2(t) = y0 +
 t
t0
f(y1(s), s) ds,
...
yk+1(t) = y0 +
 t
t0
f(yk(s), s) ds.
(8.3)
As an example of how this works, consider the simple diÔ¨Äerential equation y‚Ä≤ = y
subject to y(0) = 2. In this case,
y0(t) = 2,
y1(t) = 2 +
 t
0
2 ds = 2(1 + t),
y2(t) = 2 +
 t
0
2(1 + s) ds = 2

1 + t + 1
2t2

,
...
yk+1(t) = 2
k+1

n=0
1
n!tn.
As k ‚Üí‚àû, {yk(t)} ‚Üí2et, the correct solution. In general, if the sequence of
functions (8.3) converges uniformly to a limit, {yk(t)} ‚Üíy‚àû(t), then y‚àû(t) is a
continuous solution of the integral equation. For continuous f(y, t), we can then

8.1 LOCAL EXISTENCE OF SOLUTIONS
205
diÔ¨Äerentiate under the integral sign, to show that y is a solution of the original
diÔ¨Äerential equation, (8.1). We will shortly prove that, in order to ensure that the
sequence (8.3) converges to a continuous function, it is suÔ¨Écient that f(y, t) and
‚àÇf
‚àÇy (y, t) are continuous in R. In other words, we need f, ‚àÇf/‚àÇy ‚ààC0(R).‚Ä†
As a preliminary to the main proof, recall Theorem A2.1, which says that a
function continuous on a bounded region is itself bounded, so that there exist
strictly positive, real constants M and K such that |f(y, t)| < M and
 ‚àÇf
‚àÇy (y, t)
 < K
for all (y, t) ‚ààR. If (y1, t) and (y2, t) are two points in R, Theorem A2.2, the mean
value theorem, states that
f(y2, t) ‚àíf(y1, t) = ‚àÇf
‚àÇy (c, t)(y2 ‚àíy1) for some c with y1 < c < y2.
Since (c, t) ‚ààR, we have
 ‚àÇf
‚àÇy (c, t)
 < K, and hence
|f(y2, t) ‚àíf(y1, t)| < K |y2 ‚àíy1|
‚àÄ(y2, t), (y1, t) ‚ààR.
(8.4)
Functions that satisfy the inequality (8.4) are said to satisfy a Lipschitz condition
in R.
It is possible for a function to satisfy a Lipschitz condition in a region
R without having a continuous partial derivative everywhere in R. For example,
f(y, t) = t|y| satisÔ¨Åes |f(y2, t) ‚àíf(y1, t)| < |y2 ‚àíy1| in the unit square centred on
the origin, so that it is Lipschitz with K = 1. However, ‚àÇf/‚àÇy is not continuous on
the line y = 0. Our assumption about the continuity of the Ô¨Årst partial derivative
automatically leads to functions that satisfy a Lipschitz condition, which is the key
to proving the main result of this section.
As we stated earlier, we are going to use the successive approximations (8.3) to
establish the existence of a solution. Prior to using this, we must show that the
elements of this sequence are well-deÔ¨Åned. SpeciÔ¨Åcally, if yk+1(t) is to be deÔ¨Åned on
some interval I, we must establish that the point (yk(s), s) remains in the rectangle
R = {(y, t) | |y ‚àíy0| ‚©Ωb, |t ‚àít0| ‚©Ωa} for all s ‚ààI.
Lemma 8.1 If Œ± = min (a, b/M), then the successive approximations,
y0(t) = y0,
yk+1(t) = y0 +
 t
t0
f(yk(s), s) ds
are well-deÔ¨Åned in the interval I = {t | |t ‚àít0| < Œ±}, and on this interval |yk(t) ‚àí
y0| < M|t ‚àít0| ‚©Ωb, where |f| < M.
Proof
We proceed by induction.
It is clear that y0(t) is deÔ¨Åned on I, as it is
constant. We assume that
yn(t) = y0 +
 t
t0
f(yn‚àí1(s), s) ds
‚Ä† If these conditions hold in a larger domain, D, we can use the local result repeatedly until the
solution moves out of D.

206
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
is well-deÔ¨Åned on I, so that the point (yn(t), t) remains in R for t ‚ààI. By deÔ¨Ånition,
yn+1(t) = y0 +
 t
t0
f(yn(s), s) ds,
so we have yn+1(t) deÔ¨Åned in I. Now
|yn+1(t) ‚àíy0| =

 t
t0
f(yn(s), s) ds
 ‚©Ω
 t
t0
|f(yn(s), s)| ds ‚©ΩM|t ‚àít0| ‚©ΩMŒ± ‚©Ωb,
which is what we claimed.
To see rather less formally why we chose Œ± = min(a, b/M), note that the condi-
tion |f(y, t)| < M implies that the solution of the diÔ¨Äerential equation, y = y(t),
cannot cross lines of slope M or ‚àíM through the initial point (y0, t0), as shown
in Figure 8.1. The relationship |yk(t) ‚àíy0| < M|t ‚àít0|, which we established in
Lemma 8.1, means that the successive approximations cannot cross these lines ei-
ther. These lines intersect the boundary of the rectangle R, and the length of the
interval I depends upon whether they meet the horizontal sides (Œ± = b/M) or the
vertical sides (Œ± = a), as shown in Figure 8.1.
We can now proceed to establish the main result of this section.
Theorem 8.1 (Local existence) If f and ‚àÇf/‚àÇy are in C0(R), then the successive
approximations yk(t), deÔ¨Åned by (8.3), converge on I to a solution of the diÔ¨Äerential
equation y‚Ä≤ = f(y, t) that satisÔ¨Åes the initial condition y(t0) = y0.
Proof We begin with the identity
yj(t) = y0(t) + {y1(t) ‚àíy0(t)} + {y2(t) ‚àíy1(t)} + ¬∑ ¬∑ ¬∑ + {yj(t) ‚àíyj‚àí1(t)}
= y0(t) +
j‚àí1

n=0
{yn+1(t) ‚àíyn(t)} .
(8.5)
In order to use this, we need to estimate the value of yn+1(t) ‚àíyn(t). Using the
deÔ¨Ånition (8.3), we have, for n ‚©æ1 and |t ‚àít0| < Œ±,
|yn+1(t) ‚àíyn(t)| =

 t
t0
{f(yn(s), s) ‚àíf(yn‚àí1(s), s)} ds

‚©Ω
 t
t0
|f(yn(s), s) ‚àíf(yn‚àí1(s), s)| ds ‚©ΩK
 t
t0
|yn(s) ‚àíyn‚àí1(s)| ds.
For n = 0 we have
|y1(t) ‚àíy0(t)| =

 t
t0
f(y0(s), s) ds
 ‚©ΩM|t ‚àít0|,
by using the continuity bound on f. If we now repeatedly use these inequalities, it
is straightforward to show that
|yn+1(t) ‚àíyn(t)| ‚©ΩMKn|t ‚àít0|n+1
(n + 1)!
for n ‚©æ1, |t ‚àít0| ‚©ΩŒ±,

8.1 LOCAL EXISTENCE OF SOLUTIONS
207
Fig. 8.1. The two cases that determine the length of the interval, Œ±.
and hence that
|yn+1(t) ‚àíyn(t)| ‚©ΩM(KŒ±)n+1
K(n + 1)! .
(8.6)
If we denote by y(t) the limit of the sequence {yk(t)}, we now have, from (8.5),
y(t) = y0(t) +
‚àû

n=0
{yn+1(t) ‚àíyn(t)} .
(8.7)
Each term in this inÔ¨Ånite series is dominated by M(KŒ±)n+1/K(n + 1)!, a positive

208
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
constant; and
M
K
‚àû

n=0
(KŒ±)n+1
(n + 1)! = M
K

eKŒ± ‚àí1

,
(8.8)
which is a Ô¨Ånite constant. The comparison theorem applied to the series in (8.7)
shows that it converges absolutely on the interval |t ‚àít0| < Œ±, and we have now
proved that the sequence of successive approximations, (8.3), converges. We can
now return to Lemma 8.1 with the knowledge that y(t) = limk‚Üí‚àûyk(t) is well-
deÔ¨Åned, and, since the right hand side of (8.8) is independent of k, immediately
claim that (y(s), s) ‚ààR.
We now show that the limiting function, y(t), actually satisÔ¨Åes the integral
equation, (8.2). Now
|y(t) ‚àíyk(t)| =

‚àû

n=k
{yn+1(t) ‚àíyn(t)}
 ‚©ΩM
K
‚àû

m=k
(KŒ±)m+1
(m + 1)!
‚©ΩM
K
(KŒ±)k+1
(k + 1)!
‚àû

m=0
(KŒ±)m
m!
‚©ΩM
K
(KŒ±)k+1
(k + 1)! eKŒ± = œµk for |t ‚àít0| < Œ±.
(8.9)
Using the Lipschitz condition in the interval |t ‚àít0| < Œ± and Lemma 8.1,

 t
t0
{f(y(s), s) ‚àíf(yk(s), s)} ds
 ‚©ΩK
 t
t0
|y(s) ‚àíyk(s)| ds ‚©ΩŒ±M(KŒ±)k+1
(k + 1)!
eKŒ±.
For Ô¨Åxed Œ±, M and K,
Œ±M(KŒ±)k+1
(k + 1)!
eKŒ± ‚Üí0 as k ‚Üí‚àû,
so that
lim
k‚Üí‚àû
 t
t0
f(yk(s), s) ds =
 t
t0
f(y(s), s) ds,
and consequently y(t) satisÔ¨Åes (8.2).
To prove that y(t) is continuous on |t ‚àít0| < Œ±, we can use some of the auxiliary
results that we derived above. Consider
y(t + h) ‚àíy(t) = y(t + h) ‚àíyk(t + h) + yk(t + h) ‚àíyk(t) + yk(t) ‚àíy(t),
so that, using the triangle inequality,
|y(t + h) ‚àíy(t)| ‚©Ω|y(t + h) ‚àíyk(t + h)| + |yk(t + h) ‚àíyk(t)| + |yk(t) ‚àíy(t)|.
By choosing k suÔ¨Éciently large, using the estimate (8.9), we have
|yk(t) ‚àíy(t)| ‚©Ωœµk,
so that
|y(t + h) ‚àíy(t)| ‚©Ω2œµk + |yk(t + h) ‚àíyk(t)| ‚©Ωœµ,

8.1 LOCAL EXISTENCE OF SOLUTIONS
209
for suÔ¨Éciently small h, using the continuity of yk(t). Hence, y(t) is continuous, as
we claimed at the beginning of this section, and the equivalence of the integral and
diÔ¨Äerential equations can be established by diÔ¨Äerentiation under the integral sign.
Remarks
(i) For a particular diÔ¨Äerential equation, Theorem 8.1 is easy to use. For ex-
ample, if y‚Ä≤ = y + t, both f(y, t) = y + t and ‚àÇf/‚àÇy = 1 are continuous for
‚àí‚àû< t < ‚àû, ‚àí‚àû< y < ‚àû, so the successive approximations are guar-
anteed to converge in this domain. If y‚Ä≤ = 3y2/3, f(y, t) = 3y2/3, which is
continuous in ‚àí‚àû< y < ‚àû, ‚àí‚àû< t < ‚àû. However, its partial derivative,
‚àÇf/‚àÇy = 2y‚àí1/3, is discontinuous at y = 0. Consequently, the successive
approximations are not guaranteed to converge. This should be no surprise,
since, as we noted in the introduction to this chapter, the solution of this
equation is not unique for some initial data.
(ii) Consider the equation y‚Ä≤ = y2 subject to y(0) = 1.
Since this equation
satisÔ¨Åes the conditions of Theorem 8.1, the successive approximations will
converge, and therefore a solution exists in some rectangle containing the
initial point, namely
R = {(y, t) | |t| ‚©Ωa, |y ‚àí1| ‚©Ωb} .
Let‚Äôs now try to determine the values of the constants a and b. Since M =
maxR y2 = (1 + b)2, Œ± = min(a, b/(1 + b)2). From simple calculus,
max
b>0
b
(1 + b)2 = 1
4,
so that, independent of the particular choice of a, Œ± ‚©Ω
1
4.
Theorem 8.1
therefore shows that a solution exists for |t| ‚©Ω
1
4.
In fact, the solution,
y = 1/(1‚àít), exists for ‚àí‚àû< t < 1. It is rather more diÔ¨Écult to determine
regions of global existence like this, than it is to establish local existence,
the point of Theorem 8.1, and we will not attempt it here.
(iii) We can also consider the local existence of solutions of systems of Ô¨Årst order
equations in the form
y‚Ä≤ = f(y, t),
(8.10)
where
y =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
y1
y2
...
yn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏,
f =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
f1(y1, y2, . . . , yn, t)
f2(y1, y2, . . . , yn, t)
...
fn(y1, y2, . . . , yn, t)
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏
are vectors with n components and t is a real scalar. The vector form of
the Lipschitz condition can be established by requiring f and ‚àÇf/‚àÇyi to be

210
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
continuous with respect to all of their components and t, so that


‚àÇf
‚àÇyi

 ‚©ΩK
for all (y, t) ‚ààD ‚äÇRn+1, where ||.|| is a suitable vector norm. It then follows
that for any points (y, t) and (z, t) in D, ||f(y, t)‚àíf(z, t)|| ‚©ΩK||y ‚àíz||. The
proof then follows that of Theorem 8.1 step by step and line by line, with
modulus signs replaced by vector norms.
(iv) For a selection of results similar to, but in some ways better than, Theo-
rem 8.1, see Coddington and Levinson (1955).
8.2
Uniqueness of Solutions
We will now show that, under the same conditions as Theorem 8.1, the solution of
the initial value problem (8.1) is unique. In order to prove this, we need a result
called Gronwall‚Äôs inequality.
Lemma 8.2 (Gronwall‚Äôs inequality) If f(t) and g(t) are non-negative functions
on the interval Œ± ‚©Ωt ‚©ΩŒ≤, L is a non-negative constant and
f(t) ‚©ΩL +
 t
Œ±
f(s)g(s) ds for t ‚àà[Œ±, Œ≤],
then
f(t) ‚©ΩL exp
 t
Œ±
g(s) ds
	
for t ‚àà[Œ±, Œ≤].
Proof DeÔ¨Åne
h(t) = L +
 t
Œ±
f(s)g(s) ds,
so that h(Œ±) = L. By hypothesis, f(t) ‚©Ωh(t), and by the fundamental theorem of
calculus, since g(t) ‚©æ0, we have
h‚Ä≤(t) = f(t)g(t) ‚©Ωh(t)g(t) for t ‚àà[Œ±, Œ≤].
Using the idea of an integrating factor, this can be rewritten as
d
dt

h(t) exp

‚àí
 t
Œ±
g(s) ds
	
‚©Ω0.
Integrating from Œ± to t gives
h(t) exp

‚àí
 t
Œ±
g(s) ds
	
‚àíL ‚©Ω0,
and, since f(t) ‚©Ωh(t),
f(t) ‚©ΩL exp
 t
Œ±
g(s) ds
	
.

8.3 DEPENDENCE OF THE SOLUTION ON THE INITIAL CONDITIONS
211
Theorem 8.2 (Uniqueness) If f,
‚àÇf
‚àÇy ‚ààC0(R), then the solution of the initial
value problem y‚Ä≤ = f(y, t) subject to y(t0) = y0 is unique on |t ‚àít0| < Œ±.
Proof We proceed by contradiction. Suppose that there exist two solutions, y =
y1(t) and y = y2(t), with y1(t0) = y2(t0) = y0. These functions satisfy the integral
equations
y1(t) = y0 +
 t
t0
f(y1(s), s) ds,
y2(t) = y0 +
 t
t0
f(y2(s), s) ds,
(8.11)
and hence the points (yi(s), s) lie in the region R.
Taking the modulus of the
diÔ¨Äerence of these leads to
|y1(t) ‚àíy2(t)| =

 t
t0
{f(y1(s), s) ‚àíf(y2(s), s)} ds

‚©Ω
 t
t0
|{f(y1(s), s) ‚àíf(y2(s), s)}| ds ‚©Ω
 t
t0
K|y1(s) ‚àíy2(s)| ds for |t ‚àít0| ‚©ΩŒ±,
where K > 0 is the Lipschitz constant for the region R.
We can now apply
Gronwall‚Äôs inequality to the non-negative function |y1(t) ‚àíy2(t)| with L = 0 and
g(s) = K > 0 to conclude that |y1(t) ‚àíy2(t)| ‚©Ω0. However, since the modulus of
a function cannot be negative, |y1(t) ‚àíy2(t)| = 0, and hence y1(t) ‚â°y2(t).
Just as we saw earlier when discussing the existence of solutions, this uniqueness
result can be extended to systems of ordinary diÔ¨Äerential equations. Under the
conditions given in Remark (iii) in the previous section, the solution of (8.10) exists
and is unique.
8.3
Dependence of the Solution on the Initial Conditions
A solution of y‚Ä≤ = f(y, t) that passes through the initial point (y0, t0) depends
continuously on the three variables t0, y0 and t. For example, y‚Ä≤ = 3y2/3 subject
to y(t0) = y0 has solution y = (t ‚àít0 + y1/3
0
)3, which depends continuously on t0,
y0 and t. Earlier, we hypothesized that solutions of identical diÔ¨Äerential equations
with initial conditions that are close to each other should remain close, at least for
values of t close to the initial point, t = t0. We can now prove a theorem about
this.
Theorem 8.3 Let y = y0(t) be the solution of y‚Ä≤ = f(y, t) that passes through the
initial point (t0, y0) and y = y1(t) the solution that passes through (t1, y1). If f,
‚àÇf/‚àÇy ‚ààC0(R) and both y0(t) and y1(t) exist on some interval Œ± < t < Œ≤ with t0,
t1 ‚àà(Œ±, Œ≤), then ‚àÄœµ > 0 ‚àÉŒ¥ > 0 such that if |t0 ‚àít1| < Œ¥ and |y0 ‚àíy1| < Œ¥ then
|y0(t) ‚àíy1(t)| < œµ ‚àÄt ‚àà(Œ±, Œ≤).

212
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
Proof Since y1(t) and y2(t) satisfy the integral equations (8.11), we can take their
diÔ¨Äerence and, by splitting up the range of integration, write‚Ä†
y0(t) ‚àíy1(t) = y0 ‚àíy1 +
 t1
t0
f(y0(s), s) ds +
 t
t1
{f(y0(s), s) ‚àíf(y1(s), s)} ds.
Taking the modulus gives
|y0(t) ‚àíy1(t)| ‚©Ω|y0 ‚àíy1| +

 t1
t0
f(y0(s), s) ds
 +

 t
t1
{f(y0(s), s) ‚àíf(y1(s), s)} ds

‚©Ω|y0 ‚àíy1| + M(t1 ‚àít0) +
 t
t1
K|y0(s) ‚àíy1(s)| ds,
by using the upper bound on f and the Lipschitz condition on R. If |t1 ‚àít0| < Œ¥
and |y1 ‚àíy0| < Œ¥, this reduces to
|y0(t) ‚àíy1(t)| ‚©Ω(M + 1)Œ¥ +
 t
t1
K|y0(s) ‚àíy1(s)| ds.
We can now apply Gronwall‚Äôs inequality to obtain
|y0(t) ‚àíy1(t)| ‚©Ω(M + 1)Œ¥ exp
 t
t1
K ds
	
= (M + 1)Œ¥ exp {K(t ‚àít1)} ,
and, since |t ‚àít1| < Œ≤ ‚àíŒ±, we have
|y0(t) ‚àíy1(t)| ‚©Ω(M + 1)Œ¥ exp {K(Œ≤ ‚àíŒ±)} .
If we now choose Œ¥ < œµ exp {‚àíK(Œ≤ ‚àíŒ±)} /(M + 1), we have |y0(t) ‚àíy1(t)| < œµ, and
the proof is complete.
8.4
Comparison Theorems
Since a large class of diÔ¨Äerential equations cannot be solved explicitly, it is useful to
have a method of placing bounds on the solution of a given equation by comparing
it with solutions of a related equation that is simpler to solve. For example, y‚Ä≤ =
e‚àíty subject to y(0) = 1 is a rather nasty nonlinear diÔ¨Äerential equation, and the
properties of its solution are not immediately obvious. However, since 0 ‚©Ωe‚àíty ‚©Ω1
for 0 ‚©Ωt < ‚àûand 0 ‚©Ωy < ‚àû, it would seem plausible that 0 ‚©Ωy‚Ä≤ ‚©Ω1. Integration
of this traps the solution of the initial value problem in the range 1 ‚©Ωy ‚©Ω1 + t. In
order to prove this result rigorously, we need some preliminary results and ideas.
Note that, to simplify things, we will prove all of the results in this section for one
side of a point only. Firstly, we need some deÔ¨Ånitions.
A function F(y, t) satisÔ¨Åes a one-sided Lipschitz condition in a domain D if,
for some constant K,
y2 > y1 ‚áíF(y2, t) ‚àíF(y1, t) ‚©ΩK(y2 ‚àíy1) for (y2, t), (y1, t) ‚ààD.
‚Ä† We have assumed that t1 > t0, but the argument needs only a slight modiÔ¨Åcation if this is not
the case.

8.4 COMPARISON THEOREMS
213
If œÉ(t) is diÔ¨Äerentiable for t > a, then œÉ‚Ä≤(t) ‚©ΩK is called a diÔ¨Äerential in-
equality. Note that if œÉ‚Ä≤(t) ‚©Ω0 for t ‚©æa, then œÉ(t) ‚©ΩœÉ(a).
Lemma 8.3 If œÉ(t) is a diÔ¨Äerentiable function that satisÔ¨Åes the diÔ¨Äerential inequal-
ity œÉ‚Ä≤(t) ‚©ΩKœÉ(t) for t ‚àà[a, b] and some constant K, then œÉ(t) ‚©ΩœÉ(a)eK(t‚àía) for
t ‚àà[a, b].
Proof
We write the inequality as œÉ‚Ä≤(t) ‚àíKœÉ(t) ‚©Ω0 and multiply through by
e‚àíKt > 0 to obtain
d
dt

e‚àíKtœÉ(t)

‚©Ω0.
This leads to e‚àíKtœÉ(t) ‚©Ωe‚àíKaœÉ(a), and hence the result. Note that this is really
just a special case of Gronwall‚Äôs inequality.
Lemma 8.4 If g(t) is a solution of y‚Ä≤ = F(y, t) for t ‚©æa and f(t) is a function
that satisÔ¨Åes the diÔ¨Äerential inequality f ‚Ä≤(t) ‚©ΩF(f(t), t) for t ‚©æa with f(a) = g(a),
then, provided that F satisÔ¨Åes a one-sided Lipschitz condition for t ‚©æa, f(t) ‚©Ωg(t)
for all t ‚©æa.
Proof
We will proceed by contradiction.
Suppose that f(t1) > g(t1) for some
t1 > a. DeÔ¨Åne t0 to be the largest t in the interval a ‚©Ωt ‚©Ωt1 such that f(t) ‚©Ωg(t),
and hence f(t0) = g(t0). DeÔ¨Åne œÉ(t) = f(t) ‚àíg(t), so that œÉ(t) ‚©æ0 for t0 ‚©Ωt ‚©Ωt1
and œÉ(t0) = 0. The one-sided Lipschitz condition shows that
œÉ‚Ä≤(t) = f ‚Ä≤(t) ‚àíg‚Ä≤(t) ‚©ΩF(f(t), t) ‚àíF(g(t), t) ‚©ΩK {f(t) ‚àíg(t)} = KœÉ(t),
and hence œÉ‚Ä≤(t) ‚©ΩKœÉ(t) for t > t0. By Lemma 8.3, œÉ(t) ‚©ΩœÉ(t0)eK(t‚àít0) ‚©Ω0.
However, since œÉ(t) is non-negative for t ‚©æt0, œÉ(t) ‚â°0.
This contradicts the
hypothesis that œÉ(t1) = f(t1) ‚àíg(t1) > 0, and hence the result is proved.
Theorem 8.4 Let f(t) and g(t) be solutions of the diÔ¨Äerential equations y‚Ä≤ =
F(y, t) and z‚Ä≤ = G(z, t) respectively, on the strip a ‚©Ωt ‚©Ωb, with f(a) = g(a). If
F(y, t) ‚©ΩG(y, t) and F or G is one-sided Lipschitz on this strip, then f(t) ‚©Ωg(t)
for a ‚©Ωt ‚©Ωb
Proof Since y‚Ä≤ = F(y, t) ‚©ΩG(y, t), g satisÔ¨Åes a diÔ¨Äerential inequality of the form
described in Lemma 8.4, and f satisÔ¨Åes the diÔ¨Äerential equation. This gives us the
result immediately.
Note that Theorem 8.4 and the preceding two lemmas can be made two-sided in
the neighbourhood of the initial point with minor modiÔ¨Åcations.
Comparison theorems, such as Theorem 8.4, are of considerable practical use.
Consider as an example the initial value problem
y‚Ä≤ = t2 + y2 subject to y(0) = 1.
(8.12)
It is known, either from analysis of the solution or by numerical integration, that

214
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
the solution of this equation ‚Äòblows up‚Äô to inÔ¨Ånity at some Ô¨Ånite value of t = t‚àû. We
can estimate the position of this blowup point by comparing the solution of (8.12)
with the solutions of similar, but simpler, diÔ¨Äerential equations. Since t2 + y2 ‚©æy2
for 0 ‚©Ωt < ‚àû, the solution of (8.12) is bounded below by the solution of y‚Ä≤ = y2
subject to y(0) = 1, which is y = 1/(1‚àít). Since this blows up when t = 1, we must
have t‚àû‚©Ω1. Also, since t2 +y2 ‚©Ω1+y2 for 0 ‚©Ωt ‚©Ω1, an upper bound is provided
by the solution of y‚Ä≤ = 1 + y2 subject to y(0) = 1, namely y = tan

t + œÄ
4

. This
blows up when t = œÄ/4. By sandwiching the solution of (8.12) between two simpler
solutions, we are able to conclude that the blowup time satisÔ¨Åes œÄ
4 ‚©Ωt‚àû‚©Ω1.
Comparison theorems are also available for second order diÔ¨Äerential equations.
These take a particularly simple form for the equation
y‚Ä≤‚Ä≤(x) + g(x)y(x) = 0,
(8.13)
from which the Ô¨Årst derivative of y is absent. In fact, we can transform any lin-
ear, second order, ordinary diÔ¨Äerential equation into this form, as we shall see in
Section 12.2.7. We will compare the solution of (8.13) with that of
z‚Ä≤‚Ä≤(x) + h(x)z(x) = 0,
(8.14)
and prove the following theorem.
Theorem 8.5 If g(x) < h(x) for x ‚©æx0, y(x) is the solution of (8.13) with y(x0) =
y0 Ã∏= 0, y‚Ä≤(x0) = y1, these conditions being such that y(x) > 0 for x0 ‚©Ωx ‚©Ωx1, and
z(x) is the solution of (8.14) with z(x0) = y0 and z‚Ä≤(x0) = y1, then y(x) > z(x) for
x0 < x ‚©Ωx1, provided that z(x) > 0 on this interval.
Proof From (8.13) and (8.14),
y‚Ä≤‚Ä≤z ‚àíyz‚Ä≤‚Ä≤ = (h ‚àíg) yz.
Integrating this equation from x0 to x < x1, we obtain
y‚Ä≤(x)z(x) ‚àíy(x)z‚Ä≤(x) =
 x
x0
(h(t) ‚àíg(t)) y(t)z(t) dt.
By hypothesis, the right hand side of this is positive. Also, by direct diÔ¨Äerentiation,
d
dx
y(x)
z(x)

= y‚Ä≤(x)z(x) ‚àíy(x)z‚Ä≤(x)
z2(x)
> 0,
so that y/z is an increasing function of x. Since y(x0)/z(x0) = 1, we have y(x) >
z(x) for x0 < x ‚©Ωx1.
As an example of the use of Theorem 8.5, consider Airy‚Äôs equation,
¬®y(t) ‚àíty(t) = 0
(see Sections 3.8 and 11.2), in ‚àí1 < t < 0 with y(0) = 1 and Àôy(0) = 0.
By
making the transformation t ‚Üí‚àíx, we arrive at y‚Ä≤‚Ä≤(x) + xy(x) = 0 in 0 < x < 1,
with y(0) = 1 and y‚Ä≤(0) = 0. The solution of this is positive for 0 ‚©Ωx < x1.
If we consider z‚Ä≤‚Ä≤(x) + z(x) = 0 with z(0) = 1 and z‚Ä≤(0) = 0, then clearly z =

EXERCISES
215
cos x, which is positive for 0 ‚©Ωx < œÄ/2, and hence for 0 ‚©Ωx ‚©Ω1. Since the
equations and boundary conditions that govern y(x) and z(x) satisfy the conditions
of Theorem 8.5, we conclude that y(x) > cos x for 0 < x < 1. In fact, we can see
from the exact solution,
y(x) = Bi‚Ä≤(0)Ai(‚àíx) ‚àíAi‚Ä≤(0)Bi(‚àíx)
Bi‚Ä≤(0)Ai(0) ‚àíAi‚Ä≤(0)Bi(0)
,
shown in Figure 8.2, that x1 ‚âà1.986.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
‚àí0.5
0
0.5
1
x
y
cos x
Fig. 8.2. The exact solution of y‚Ä≤‚Ä≤ + xy = 0 subject to y(0) = 1 and y‚Ä≤(0) = 0.
Exercises
8.1
Determine the integral equations equivalent to the initial value problems
(a) y‚Ä≤ = t2 + y4 subject to y(0) = 1,
(b) y‚Ä≤ = y + t subject to y(0) = 0,
and determine the Ô¨Årst two successive approximations to the solution.
8.2
Show that the functions
(a) f(y, t) = te‚àíy2 for |t| ‚©Ω1, |y| < ‚àû,
(b) f(y, t) = t2 + y2 for |t| ‚©Ω2, |y| ‚©Ω3,
are Lipschitz in the regions indicated, and Ô¨Ånd the Lipschitz constant, K,
in each case.

216
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
8.3
How could successive approximations to the solution of y‚Ä≤ = 3y2/3 fail to
converge to a solution?
8.4
Let f(y, t) and g(y, t) be continuous and satisfy a Lipschitz condition with
respect to y in a region D. Suppose |f(y, t) ‚àíg(y, t)| < œµ in D for some
œµ > 0.
If y1(t) is a solution of y‚Ä≤ = f(y, t) and y2(t) is a solution of
y‚Ä≤ = g(y, t), such that |y2(t0) ‚àíy1(t0)| < Œ¥ for some t0 and Œ¥ > 0, show
that, for all t for which y1(t) and y2(t) both exist,
|y2(t) ‚àíy1(t)| ‚©ΩŒ¥ exp (K|t ‚àít0|) + œµ
K {exp (K|t ‚àít0|) ‚àí1} ,
where K is the Lipschitz constant. Hint: Use the Gronwall inequality.
8.5
Find upper and lower bounds to the solutions of the diÔ¨Äerential equations
(a) y‚Ä≤ = sin(xy) subject to y(0) = 1/2 for x ‚©æ0,
(b) y‚Ä≤ = y3 ‚àíy subject to y(0) = 1/4.
8.6
If œÉ(t) ‚ààC1[a, a + œµ] and positive satisÔ¨Åes the diÔ¨Äerential inequality œÉ‚Ä≤ ‚©Ω
KœÉ log œÉ, show that
œÉ(t) ‚©ΩœÉ(a)eK(t‚àía) for t ‚àà[a, a + œµ].
8.7
For each Ô¨Åxed x, let F(x, y) be a nonincreasing function of y. Show that,
if f(x) and g(x) are two solutions of y‚Ä≤ = F(x, y) and b > a, then |f(b) ‚àí
g(b)| ‚©Ω|f(a) ‚àíg(a)|. Hence deduce a result concerning the uniqueness of
solutions. This is known as the Peano uniqueness theorem.

CHAPTER NINE
Nonlinear Ordinary DiÔ¨Äerential Equations: Phase
Plane Methods
9.1
Introduction: The Simple Pendulum
Ordinary diÔ¨Äerential equations can be used to model many diÔ¨Äerent types of phys-
ical system. We now know a lot about second order linear ordinary diÔ¨Äerential
equations. For example, simple harmonic motion,
d 2Œ∏
dt2 + œâ2Œ∏ = 0,
(9.1)
describes many physical systems that oscillate with small amplitude Œ∏. The general
solution is Œ∏ = A sin œât + B cos œât, where A and B are constants that can be Ô¨Åxed
from the initial values of Œ∏ and dŒ∏/dt. The solution is an oscillatory function of t.
Note that we can also write this as Œ∏ = Ceiœât+De‚àíiœât, where C and D are complex
constants. In the real world, the physics of a problem is rarely as simple as this.
Let‚Äôs consider the frictionless simple pendulum, shown in Figure 9.1. A mass, m,
is attached to a light, rigid rod of length l, which can rotate without friction about
the point O.
Fig. 9.1. A simple pendulum.

218
PHASE PLANE METHODS
Using Newton‚Äôs second law on the force perpendicular to the rod gives
‚àímg sin Œ∏ = mld 2Œ∏
dt2 ,
and hence
d 2Œ∏
dt2 + œâ2 sin Œ∏ = 0,
(9.2)
where œâ2 = g/l and t is time. For oscillations of small amplitude, Œ∏ ‚â™1, so that
sin Œ∏ ‚àºŒ∏, and we obtain simple harmonic motion, (9.1). If Œ∏ is not small, we must
study the full equation of motion, (9.2), which is nonlinear. Do we expect the
solutions to be qualitatively diÔ¨Äerent to those of simple harmonic motion? If we
push a pendulum hard enough, we should be able to make it swing round and round
its point of support, with Œ∏ increasing continuously with t (remember there is no
friction), so we would hope that (9.2) has solutions of this type.
In general, nonlinear ordinary diÔ¨Äerential equations cannot be solved analyti-
cally, but for equations like (9.2), where the Ô¨Årst derivative, dŒ∏/dt does not appear
explicitly, an analytical solution is available. Using the notation ÀôŒ∏ = dŒ∏/dt, the
trick is to treat ÀôŒ∏ as a function of Œ∏ instead of t. Note that
d 2Œ∏
dt2 = d
dt
dŒ∏
dt

= d ÀôŒ∏
dt = dŒ∏
dt
d ÀôŒ∏
dŒ∏ = ÀôŒ∏d ÀôŒ∏
dŒ∏ = d
dŒ∏
1
2
ÀôŒ∏2

.
This allows us to write (9.2) as
d
dŒ∏
1
2
ÀôŒ∏2

= ‚àíœâ2 sin Œ∏,
which we can integrate once to give
1
2
ÀôŒ∏2 = œâ2 cos Œ∏ + constant.
Using œâ2 = g/l, we can write this as
1
2ml2 ÀôŒ∏2 ‚àímgl cos Œ∏ = E.
(9.3)
This is just a statement of conservation of energy, E, with the Ô¨Årst term representing
kinetic energy, and the second, gravitational potential energy. Systems like (9.2),
which can be integrated once to determine a conserved quantity, here energy,
E, are called conservative systems. Note that if we try to account for a small
amount of friction at the point of suspension of the pendulum, we need to add a
term proportional to dŒ∏/dt to the left hand side of (9.2). The system is then no
longer conservative, with dramatic consequences for the motion of the pendulum
(see Exercise 9.6).
From (9.3) we can see that
dŒ∏
dt = ¬±
*
2E
ml2 + 2g
l cos Œ∏.

9.1 INTRODUCTION: THE SIMPLE PENDULUM
219
We can integrate this to arrive at the implicit solution
t = ¬±
 Œ∏
Œ∏0
dŒ∏‚Ä≤
$
2E
ml2 + 2g
l cos Œ∏‚Ä≤
,
(9.4)
where Œ∏0 is the angle of the pendulum when t = 0. Note that the two constants of
integration are E and Œ∏0, the initial energy and angle of the pendulum. Equation
(9.4) is a simple representation of the solution, which, if necessary, we can write in
terms of Jacobian elliptic functions (see Section 9.4), so now everything is clear . . . ,
except of course that it isn‚Äôt! Presumably equation (9.4) gives oscillatory solutions
for small initial angles and kinetic energies, and solutions with Œ∏ increasing with t
for large enough initial energies, but this doesn‚Äôt really leap oÔ¨Äthe page at you.
From (9.4) we have a quantitative expression for the solution, but we are really
more interested in the qualitative nature of the solution.
Let‚Äôs go back to (9.3) and write
ÀôŒ∏2 = 2E
ml2 + 2g
l cos Œ∏.
Graphs of ÀôŒ∏2 as a function of Œ∏ are shown in Figure 9.2(a) for diÔ¨Äerent values of E.
‚Äî For E > mgl the curves lie completely above the Œ∏-axis.
‚Äî For ‚àímgl < E < mgl the curves intersect the Œ∏-axis.
‚Äî For E < ‚àímgl the curves lie completely below the Œ∏-axis (remember, ‚àí1 ‚©Ω
cos Œ∏ ‚©Ω1).
We can now determine ÀôŒ∏ as a function of Œ∏ by taking the square root of the curves
in Figure 9.2(a) to obtain the curves in Figure 9.2(b), remembering to take both
the positive and negative square root.
‚Äî For E > mgl, the curves lie either fully above or fully below the Œ∏-axis.
‚Äî For ‚àímgl < E < mgl, only Ô¨Ånite portions of the graph of ÀôŒ∏2 lie above the Œ∏-axis,
so the square root gives Ô¨Ånite, closed curves.
‚Äî For E < ‚àímgl, there is no real solution. This corresponds to the fact that the
pendulum always has a gravitational potential energy of at least ‚àímgl, so we
must have E ‚©æ‚àímgl.
As we shall see later, the solution with E = mgl is an important one. The graph
of ÀôŒ∏2 just touches the Œ∏-axis at Œ∏ = ¬±(2n ‚àí1)œÄ, for n = 1, 2, . . . , and taking the
square root gives the curves that pass through these points shown in Figure 9.2(b).
How do Œ∏ and ÀôŒ∏ vary along these solution curves as t increases? If ÀôŒ∏ is positive, Œ∏
increases with t, and vice versa (remember, ÀôŒ∏ = dŒ∏/dt is, by deÔ¨Ånition, the rate at
which Œ∏ changes with t). This allows us to add arrows to Figure 9.2(b), indicating
in which direction the solution changes with time. We have now constructed our
Ô¨Årst phase portrait for a nonlinear ordinary diÔ¨Äerential equation.
The (Œ∏, ÀôŒ∏)-
plane is called the phase plane. Each of the solution curves represents a possible
solution of (9.2), and is known as an integral path or trajectory. If we know the
initial conditions, Œ∏ = Œ∏0, ÀôŒ∏ = ÀôŒ∏0 =
$
2E
ml2 + 2g
l cos Œ∏0, the integral path that passes

220
PHASE PLANE METHODS
through the point (Œ∏0, ÀôŒ∏0) when t = 0 represents the solution. Finally, note that,
since Œ∏ = œÄ is equivalent to Œ∏ = ‚àíœÄ, we only need to consider the phase portrait for
‚àíœÄ ‚©ΩŒ∏ ‚©ΩœÄ. Alternatively, we can cut the phase plane along the lines Œ∏ = ‚àíœÄ and
Œ∏ = œÄ and join them up, so that the integral paths lie on the surface of a cylinder.
Fig. 9.2. (a) ÀôŒ∏2 as a function of Œ∏ for diÔ¨Äerent values of E. (b) ÀôŒ∏ as a function of Œ∏ ‚Äì
the phase portrait for the simple pendulum. The cross at (0, 0) indicates an equilibrium
solution. The arrows indicate the path followed by the solution as t increases. Note that
the phase portrait for |Œ∏| ‚©æœÄ is the periodic extension of the phase portrait for |Œ∏| ‚©ΩœÄ.
Having gone to the trouble of constructing a phase portrait for (9.2), does it
tell us what the pendulum does in a form more digestible than that given by
(9.4)? Let‚Äôs consider the three qualitatively diÔ¨Äerent types of integral path shown
in Figure 9.2(b).
(i) Equilibrium solutions The points ÀôŒ∏ = 0, Œ∏ = 0 or ¬±œÄ, represent the two
equilibrium solutions of (9.2). The point (0, 0) is the equilibrium with the
pendulum vertically downward, (œÄ, 0) the equilibrium with the pendulum
vertically upward. Points close to (0, 0) lie on small closed trajectories close
to (0, 0). This indicates that (0, 0) is a stable equilibrium point, since
a small change in the state of the system away from equilibrium leads to
solutions that remain close to equilibrium.
If you cough on a pendulum
hanging downwards, you will only excite a small oscillation. In contrast,
points close to (œÄ, 0) lie on trajectories that take the solution far away from
(œÄ, 0), and we say that this is an unstable equilibrium point.
If you
cough on a pendulum balanced precariously above its point of support, it
will fall. Of course, in practice it is impossible to balance a pendulum in

9.1 INTRODUCTION: THE SIMPLE PENDULUM
221
this way, precisely because the equilibrium is unstable. We will reÔ¨Åne our
deÔ¨Ånitions of stability and instability in Chapter 13.
(ii) Periodic solutions Integral paths with ‚àímgl < E < mgl are closed, and
represent periodic solutions. They are often referred to as limit cycles or
periodic orbits. The pendulum swings back and forth without reaching the
upward vertical. These orbits are stable, since nearby orbits remain nearby.
Note that the frequency of the oscillation depends upon its amplitude, a
situation that is typical for nonlinear oscillators. For the simple pendulum,
the amplitude of the motion is Œ∏max = cos‚àí1 (E/mgl), and (9.4) shows that
the period of the motion, T, is given by
T = 2
 Œ∏max
‚àíŒ∏max
dŒ∏
$
2E
ml2 + 2g
l cos Œ∏
.
Small closed trajectories in the neighbourhood of the equilibrium point at
(0, 0) are described by simple harmonic motion, (9.1). Note that, in contrast
to the full nonlinear system, the frequency of simple harmonic motion is
independent of its amplitude. The idea of linearizing a nonlinear system
of ordinary diÔ¨Äerential equations close to an equilibrium point in order to
determine what the phase portrait looks like there, is one that we will return
to later.
In terms of the phase portrait on the cylindrical surface, trajectories with
E > mgl are also stable periodic solutions, looping round and round the
cylinder.
The pendulum has enough kinetic energy to swing round and
round its point of support.
(iii) Heteroclinic solutions The two integral paths that connect (‚àíœÄ, 0) and
(œÄ, 0) have E = mgl. The path with ÀôŒ∏ ‚©æ0 has Œ∏ ‚Üí¬±œÄ as t ‚Üí¬±‚àû(we
will prove this later). This solution represents a motion where the pendu-
lum swings around towards the upward vertical, and is just caught between
falling back and swinging over. This is known as a heteroclinic path, since
it connects diÔ¨Äerent equilibrium points.‚Ä† Heteroclinic paths are important,
because they represent the boundaries between qualitatively diÔ¨Äerent types
of behaviour. They are also unstable, since nearby orbits behave qualita-
tively diÔ¨Äerently. Here, the heteroclinic orbits separate motions where the
pendulum swings back and forth from motions where it swings round and
round (diÔ¨Äerent types of periodic solution). In terms of the phase portrait on
a cylindrical surface, we can consider these paths to be homoclinic paths,
since they connect an equilibrium point to itself.‚Ä°
We have now seen that, if we can determine the qualitative nature of the phase
portrait of a second order nonlinear ordinary diÔ¨Äerential equation and sketch it, we
can extract a lot of information about the qualitative behaviour of its solutions.
This information provides rather more insight than the analytical solution, (9.4),
‚Ä† Greek hetero = diÔ¨Äerent
‚Ä° Greek homo = same

222
PHASE PLANE METHODS
into the behaviour of the physical system that the equation models. For nonconser-
vative equations, we cannot integrate the equation directly to get at the equation
of the integral paths, and we have to be rather more cunning in order to sketch the
phase portrait. We will develop methods to tackle second order, nonconservative,
ordinary diÔ¨Äerential equations in Section 9.3.
Before that, we will consider the
simpler case of Ô¨Årst order nonlinear ordinary diÔ¨Äerential equations.
9.2
First Order Autonomous Nonlinear Ordinary DiÔ¨Äerential
Equations
An autonomous ordinary diÔ¨Äerential equation is one in which the independent
variable does not appear explicitly, for example (9.2). The equations ¬®Œ∏+t2Œ∏ = 0 and
Àôx = t‚àíx2 are nonautonomous. Note, however, that an nth order nonautonomous
ordinary diÔ¨Äerential equation can always be written as a (n + 1)th order system of
autonomous ordinary diÔ¨Äerential equations. For example Àôx = t ‚àíx2 is equivalent
to Àôx = y ‚àíx2, Àôy = 1 with y = 0 when t = 0.
In this section we focus on the qualitative behaviour of solutions of Ô¨Årst order,
autonomous, ordinary diÔ¨Äerential equations, which can always be written as
dx
dt = Àôx = X(x),
(9.5)
with X(x) a given function of x. Of course, such an equation is separable, with the
solution subject to x(0) = x0 given by
t =
 x
x0
dx‚Ä≤
X(x‚Ä≤).
As we found in the previous section, solving the equation analytically is not nec-
essarily the easiest way to determine the qualitative behaviour of the system (see
Exercise 9.2).
9.2.1
The Phase Line
Consider the graph of the function X(x). An example is shown in Figure 9.3(a).
If X(x1) = 0, then Àôx = 0, and hence x = x1 is an equilibrium solution of (9.5).
For the example shown in Figure 9.3 there are three equilibrium points, at x = x1,
x2 and x3.
We can also see that Àôx = X(x) < 0, and hence x is a decreasing
function of t for x < x1 and x2 < x < x3. Similarly, Àôx = X(x) > 0, and hence
x increases as t increases, for x1 < x < x2 and x > x3. By analogy with the
phase plane, where we constructed the phase portrait for a second order system
in the previous section, we can draw a phase line for this Ô¨Årst order equation, as
shown in Figure 9.3(b). The arrows indicate whether x increases or decreases with
t. Clearly, the diÔ¨Äerent types of behaviour that are possible are rather limited by
the constraint that the trajectories lie in a single dimension. Both for this example
and in general, trajectories either enter an equilibrium point or head oÔ¨Äto inÔ¨Ånity.

9.2 FIRST ORDER EQUATIONS
223
In particular, periodic solutions are not possible.‚Ä† Solutions that begin close to
x = x1 or x = x3 move away from the equilibrium point, so that x = x1 and x = x3
are unstable. In contrast, x = x2 is a stable equilibrium point, since solutions that
start close to it approach it.
x2
x1
x1
x2
x3
x
X(x)
(a)
(b)
‚Ä¢
‚Ä¢
x3
‚Ä¢
Fig. 9.3. (a) An example of a graph of Àôx = X(x). (b) The equivalent phase line.
From Figure 9.3(b) we can see that
x ‚Üí‚àí‚àûas t ‚Üí‚àû
when x0 < x1,
x ‚Üíx2 as t ‚Üí‚àû
when x1 < x0 < x3,
x ‚Üí‚àûas t ‚Üí‚àû
when x0 > x3.
The set D‚àí‚àû= {x | x < x1} is called the domain of attraction or basin of
attraction of minus inÔ¨Ånity. Similarly, D2 = {x | x1 < x < x3} is the domain of
attraction of x2 and D‚àû= {x | x > x3} that of plus inÔ¨Ånity. All of this information
has come from a qualitative analysis of the graph of Àôx = X(x).
9.2.2
Local Analysis at an Equilibrium Point
If x = x1 is an equilibrium solution of (9.5), what happens quantitatively close to
x1? Firstly, let‚Äôs move the equilibrium point to the origin by deÔ¨Åning ¬Øx = x ‚àíx1,
‚Ä† If there is some geometrical periodicity in the problem so that, for example, by analogy with
the simple pendulum, ‚àíœÄ ‚©Ωx ‚©ΩœÄ and x = ‚àíœÄ is equivalent to x = œÄ, we can construct a
phase loop, around which the solution can orbit indeÔ¨Ånitely.

224
PHASE PLANE METHODS
so that
d¬Øx
dt = X(x1 + ¬Øx).
We can expand X(x1 + ¬Øx) as a Taylor series,
X(x1 + ¬Øx) = X(x1) + ¬ØxX‚Ä≤(x1) + 1
2 ¬Øx2X‚Ä≤‚Ä≤(x1) + ¬∑ ¬∑ ¬∑ ,
(9.6)
where X‚Ä≤ = dX/dx. In the neighbourhood of the equilibrium point, ¬Øx ‚â™1, and
hence
d¬Øx
dt ‚âàX(x1) + ¬ØxX‚Ä≤(x1).
Since x = x1 is an equilibrium solution, X(x1) = 0, and, assuming that X‚Ä≤(x1) Ã∏= 0,
d¬Øx
dt ‚âàX‚Ä≤(x1)¬Øx
for ¬Øx ‚â™1.
This has solution ¬Øx = k exp {X‚Ä≤(x1)t} for some constant k. If X‚Ä≤(x1) < 0, ¬Øx ‚Üí0
(x ‚Üíx1) as t ‚Üí‚àû, and the equilibrium point is therefore stable. If X‚Ä≤(x1) > 0,
¬Øx ‚Üí0 (x ‚Üíx1) as t ‚Üí‚àí‚àû, and the equilibrium point is therefore unstable. This
is consistent with our qualitative analysis (note the slope of X(x) at the equilibrium
points in Figure 9.3(a)). Moreover, we now know that solutions approach stable
equilibrium points exponentially fast as t ‚Üí‚àû, a piece of quantitative information
well worth knowing.
If X‚Ä≤(x1) Ã∏= 0, we say that x = x1 is a hyperbolic equilibrium point, and
this analysis determines how solutions behave in its neighbourhood. In particular,
solutions that do not start at a hyperbolic equilibrium point cannot reach it in a
Ô¨Ånite time. If X‚Ä≤(x1) = 0, x = x1 is a nonhyperbolic equilibrium point, and
we need to retain more terms in the Taylor expansion of X, (9.6), in order to sort
out what happens close to the equilibrium point. We will consider this further in
Chapter 13.
9.3
Second Order Autonomous Nonlinear Ordinary DiÔ¨Äerential
Equations
Any second order, autonomous, ordinary diÔ¨Äerential equation can be written as a
system of two Ô¨Årst order equations, in the form
Àôx = X(x, y),
Àôy = Y (x, y).
(9.7)
For example, consider (9.2), which governs the motion of a simple pendulum. Let‚Äôs
deÔ¨Åne x = Œ∏ and y = Àôx. Now, Àôy = ¬®x = ¬®Œ∏ = ‚àí(g/l) sin Œ∏, and hence
Àôx = y,
Àôy = ‚àíg
l sin x.
From now on, we will assume that the right hand sides in (9.7) are continu-
ously diÔ¨Äerentiable‚Ä†, and hence that a solution exists and is unique in the sense of
Theorem 8.2, for all of the systems that we study.
‚Ä† Continuously diÔ¨Äerentiable functions are continuous and have derivatives with respect to the
independent variables that are continuous.

9.3 SECOND ORDER EQUATIONS
225
9.3.1
The Phase Plane
In Section 9.1, we saw how the solutions of the equations of motion of a simple
pendulum can be represented in the phase plane. Let‚Äôs now consider the phase
plane for a general second order system.
An integral path is a solution (x(t), y(t)) of (9.7), plotted in the (x, y)-plane, or
phase plane. The slope of an integral path is
dy
dx = dy/dt
dx/dt = Àôy
Àôx = Y (x, y)
X(x, y).
This slope is uniquely deÔ¨Åned at all points x = x0 and y = y0 at which X(x0, y0)
and Y (x0, y0) are not both zero. These are known as ordinary points. Since
the solution through such a point exists and is unique, we deduce that integral
paths cannot cross at ordinary points. This is possibly the most useful piece of
information that you need to bear in mind when sketching phase portraits. Integral
paths cannot cross at ordinary points!
Points (x0, y0) that are not ordinary have X(x0, y0) = Y (x0, y0) = 0, and are
therefore equilibrium points. In other words, Àôx = Àôy = 0 when x = x0, y = y0, and
this point represents an equilibrium solution of (9.7). At an equilibrium point, the
slope, dy/dx, of the integral paths is not well-deÔ¨Åned, and we deduce that integral
paths can meet only at equilibrium points, and only as t ‚Üí¬±‚àû. This will become
clearer in the next section.
9.3.2
Equilibrium Points
In order to determine what the phase portrait looks like close to an equilibrium
point, we proceed as we did for Ô¨Årst order systems. We begin by shifting the origin
to an equilibrium point at (x0, y0), using
x = x0 + ¬Øx,
y = y0 + ¬Øy.
Now we Taylor expand the functions X and Y , remembering that X(x0, y0) =
Y (x0, y0) = 0, to obtain
X(x0 + ¬Øx, y0 + ¬Øy) = ¬Øx‚àÇX
‚àÇx (x0, y0) + ¬Øy ‚àÇX
‚àÇy (x0, y0) + ¬∑ ¬∑ ¬∑ ,
Y (x0 + ¬Øx, y0 + ¬Øy) = ¬Øx‚àÇY
‚àÇx (x0, y0) + ¬Øy ‚àÇY
‚àÇy (x0, y0) + ¬∑ ¬∑ ¬∑ .
In the neighbourhood of the equilibrium point, ¬Øx ‚â™1 and ¬Øy ‚â™1, and hence
d¬Øx
dt ‚âà¬ØxXx(x0, y0) + ¬ØyXy(x0, y0),
d¬Øy
dt ‚âà¬ØxYx(x0, y0) + ¬ØyYy(x0, y0),
(9.8)
where we have used the notation Xx = ‚àÇX/‚àÇx. The most convenient way of writing
this is in matrix form, as
du
dt = J(x0, y0)u,
(9.9)

226
PHASE PLANE METHODS
where
u =
 ¬Øx
¬Øy

,
J(x0, y0) =
 Xx(x0, y0)
Xy(x0, y0)
Yx(x0, y0)
Yy(x0, y0)

.
We call J(x0, y0) the Jacobian matrix at the equilibrium point (x0, y0). Equation
(9.9) represents a pair of linear, Ô¨Årst order ordinary diÔ¨Äerential equations with
constant coeÔ¨Écients, which suggests that we can look for solutions of the form
u = u0eŒªt, where u0 and Œª are constants to be determined. Substituting this form
of solution into (9.9) gives
Œªu0 = Ju0.
(9.10)
This is an eigenvalue problem. There are two eigenvalues, Œª = Œª1 and Œª = Œª2,
possibly equal, possibly complex, and corresponding unit eigenvectors u0 = u1 and
u0 = u2. These provide us with two possible solutions of (9.9), u = u1eŒª1t and
u = u2eŒª2t, so that the general solution is the linear combination
u = A1u1eŒª1t + A2u2eŒª2t,
(9.11)
for any constants A1 and A2.
When the eigenvalues Œª1 and Œª2 are real, distinct and nonzero, so are the eigen-
vectors u1 and u2, and (9.11) suggests that the form of the solution is simpler if
we make a linear transformation so that u1 and u2 lie along the coordinate axes.
Such a transformation is given by u = Pv, where v = (ÀÜx, ÀÜy) and P = (u1, u2) is a
matrix with columns given by u1 and u2. Substituting this into (9.9) gives
P Àôv = JPv,
Àôv = P ‚àí1JPv = Œõv,
where
Œõ =
 Œª1
0
0
Œª2

.
Therefore ÀôÀÜx = Œª1ÀÜx, ÀôÀÜy = Œª2ÀÜy, so that
ÀÜx = k1eŒª1t,
ÀÜy = k2eŒª2t,
(9.12)
with k1 and k2 constants, and hence
ÀÜy = k3ÀÜxŒª2/Œª1,
(9.13)
where k3 = kŒª2/Œª1
1
/k2. This is the equation of the integral paths in the transformed
coordinates. Note that the transformed coordinate axes are integral paths. There
are now three main cases to consider.
(i) Distinct, real, positive eigenvalues (Œª1, Œª2 > 0) The solution (9.12)
shows that ÀÜx, ÀÜy ‚Üí0 as t ‚Üí‚àí‚àû, exponentially fast, so the equilibrium point
is unstable. The equation of the integral paths, (9.13), then shows that they
all meet at the equilibrium point at the origin. The local phase portrait is
sketched in Figure 9.4, in both the transformed and untransformed coordi-
nate systems. This type of equilibrium point is called an unstable node.

9.3 SECOND ORDER EQUATIONS
227
Remember, this is the phase portrait close to the equilibrium point. As the
integral paths head away from the equilibrium point, the linearization that
we have performed in order to obtain (9.9) becomes inappropriate and we
must consider how this local phase portrait Ô¨Åts into the full, global picture.
Figure 9.4(a) illustrates the situation when Œª2 > Œª1. This means that ÀÜy
grows more quickly than ÀÜx as t increases, and thereby causes the integral
paths to bend as they do.
Figure 9.4(b) shows that when Œª2 > Œª1, the
solution grows more rapidly in the u2-direction than the u1-direction.
x
x
y
y
(a)
(b)
u1
u2
‚àí
‚àí
Fig. 9.4. An unstable node with Œª2 > Œª1 sketched in (a) the transformed and (b) the
untransformed coordinate systems.
(ii) Distinct, real, negative eigenvalues (Œª1, Œª2 < 0) In this case, all we
have to do is consider the situation with the sense of t reversed, and we
recover the previous case. The situation is as shown in Figure 9.4, but with
the arrows reversed. This type of equilibrium point is called a stable node.
(iii) Real eigenvalues of opposite sign (Œª1Œª2 < 0) In this case, the coordinate
axes are the only integral paths in the (ÀÜx, ÀÜy)-plane that enter the equilibrium
point. On the other integral paths, given by (9.13), ÀÜx ‚Üí¬±‚àûas ÀÜy ‚Üí0, and
vice versa, as shown in Figure 9.5(a).
When Œª2 > 0 > Œª1, ÀÜx ‚Üí0 and
ÀÜy ‚Üí¬±‚àûas t ‚Üí‚àû. The integral paths in the directions of the eigenvectors
u1 and u2, shown in Figure 9.5(b), are therefore the only ones that enter the
equilibrium point, and are called the stable and unstable separatrices‚Ä†.
This type of equilibrium point is called a saddle point. The separatrices
of saddle points usually represent the boundaries between diÔ¨Äerent types
of behaviour in a phase portrait. See if you can spot the saddle points in
Figure 9.2(b). Remember, although the separatrices are straight trajectories
‚Ä† singular form, separatrix.

228
PHASE PLANE METHODS
in the neighbourhood of the saddle point, they will start to bend as they
head away from it and become governed by the full nonlinear equations.
x
y
x
y
(b)
(a)
u2
u1
‚àí
‚àí
Fig. 9.5. A saddle point with Œª2 > 0 > Œª1 sketched in (a) the transformed and (b) the
untransformed coordinate systems.
The degenerate case where Œª1 = Œª2 is slightly diÔ¨Äerent from that of a stable
or unstable node, and we will not consider it here, but refer you to Exercise 9.4.
However, it should be clear that when Œª1 = Œª2 > 0 the equilibrium point is unstable,
whilst for Œª1 = Œª2 < 0 it is stable.
Let‚Äôs now consider what the phase portrait looks like when the eigenvalues Œª1
and Œª2 are complex. Since the eigenvalues are the solutions of a quadratic equation
with real coeÔ¨Écients, they must be complex conjugate, so we can write Œª = Œ± ¬± iŒ≤,
with Œ± and Œ≤ real. The general solution, (9.11), then becomes
u = eŒ±t 
A1u1eiŒ≤t + A2u2e‚àíiŒ≤t
.
(9.14)
There are two cases to consider.
(i) Eigenvalues with strictly positive real part (Œ± > 0) The term eŒ±t
in (9.14) means that the solution grows exponentially with t, so that the
equilibrium point is unstable. The remaining term in (9.14) is oscillatory,
and we conclude that the integral paths spiral away from the equilibrium
point, as shown in Figure 9.6. This type of equilibrium point is called an
unstable spiral or unstable focus. To determine whether the sense of
rotation is clockwise or anticlockwise, it is easiest just to consider the sign
of d¬Øy/dt on the positive ¬Øx-axis. When ¬Øy = 0, (9.8) shows that d¬Øy/dt =
Xx(x0, y0)¬Øx, and hence the spiral is anticlockwise if Xx(x0, y0) > 0, clockwise
if Xx(x0, y0) < 0.‚Ä†
‚Ä† If Xx(x0, y0) = 0, try looking on the positive ¬Øy-axis.

9.3 SECOND ORDER EQUATIONS
229
Fig. 9.6. An unstable, anticlockwise spiral.
(ii) Eigenvalues with strictly negative real part (Œ± < 0) This is just the
same as the previous case, but with the sense of t reversed. This is a stable
spiral or stable focus,
and has the phase portrait shown in Figure 9.6,
but with the arrows reversed.
Note that, for complex conjugate eigenvalues, we can transform the system into a
more convenient form by deÔ¨Åning the transformation matrix, P, slightly diÔ¨Äerently.
If we choose either of the eigenvectors, for example u1, and write Œª1 = ¬µ + iœâ, we
can deÔ¨Åne
P = (Im(u1), Re(u1)) ,
and u = Pv. As before, Àôv = P ‚àí1JPv, but now
JP = J (Im(u1), Re(u1)) = (Im(Ju1), Re(Ju1)) = (Im(Œª1u1), Re(Œª1u1))
= (¬µIm(u1) + œâRe(u1), ¬µRe(u1) ‚àíœâIm(u1)) = P

¬µ
‚àíœâ
œâ
¬µ

,
and hence
P ‚àí1JP =

¬µ
‚àíœâ
œâ
¬µ

.

230
PHASE PLANE METHODS
If v = (v1, v2)T, then
Àôv1 = ¬µv1 ‚àíœâv2,
Àôv2 = œâv1 + ¬µv2,
and, eliminating v2,
¬®v1 ‚àí2¬µÀôv1 +

œâ2 + ¬µ2
v1 = 0.
This is the usual equation for damped simple harmonic motion, with natural fre-
quency œâ and damping given by ‚àí¬µ.
Note that all of the above analysis is rather informal, since it is based on a simple
linearization about the equilibrium point. Can we guarantee that the behaviour
that we have deduced persists when we study the full, nonlinear system, (9.7)?
Yes, we can, provided that the equilibrium point is hyperbolic. We can formalize
this in the following theorem, which holds for autonomous nonlinear systems of
arbitrary order.
Theorem 9.1 (Hartman‚ÄìGrobman) If ¬Øx is a hyperbolic equilibrium point of the
nth order system Àôx = f(x), then there is homeomorphism (a mapping that is one-to-
one, onto and continuous and has a continuous inverse) from Rn to Rn deÔ¨Åned in
a neighbourhood of ¬Øx that maps trajectories of the nonlinear system to trajectories
of the local linearized system.
We will not give the proof, but refer the interested reader to the original papers by
Hartman (1960) and Grobman (1959).
Each of the Ô¨Åve cases we have discussed above is indeed an example of a hyper-
bolic equilibrium point. If at least one eigenvalue has zero real part, the equilibrium
point is said to be nonhyperbolic. As we shall see in Chapter 13, the behaviour in
the neighbourhood of a nonhyperbolic equilibrium point is determined by higher
order, nonlinear, terms in the Taylor expansions of X and Y .
An important example where it may not be necessary to consider higher order
terms in the Taylor expansions is when the eigenvectors are purely imaginary, with
Œª = ¬±iŒ≤. In this case, (9.14) with Œ± = 0 shows that the solution is purely oscillatory.
The integral paths are closed and consist of a set of nested limit cycles around the
equilibrium point. This type of equilibrium point is called a centre. However, the
eÔ¨Äect of higher order, nonlinear, terms in the Taylor expansions may be to make
the local solutions spiral into or out of the equilibrium point. We say that the
equilibrium point is a linear centre, but a nonlinear spiral. On the other hand,
there are many physical systems where a linear centre persists, even in the presence
of the higher order, nonlinear terms, and is called a nonlinear centre.
As an example, consider the simple pendulum, with
Àôx = y,
Àôy = ‚àíg
l sin x.
This has
J =

0
1
‚àíg
l cos x
0

and
J(0, 0) =

0
1
‚àíg
l
0

.

9.3 SECOND ORDER EQUATIONS
231
The eigenvalues of J(0, 0) are Œª = ¬±i
#
g/l. As we can see in Figure 9.2, the phase
portrait close to the origin is indeed a nonlinear centre. What is it about this system
that allows the centre to persist in the full nonlinear analysis? Well, the obvious
answer is that there is a conserved quantity, the energy, which parameterizes the set
of limit cycles. However, another way of looking at this is to ask whether the system
has any symmetries. Here we know that if we map x ‚Üí‚àíx and t ‚Üí‚àít, reÔ¨Çecting
the phase portrait in the y-axis and reversing time, the equations are unchanged,
and hence the phase portrait should not change. Under this transformation, a stable
spiral would become an unstable spiral, and vice versa, because of the reversal of
time. Therefore, since the phase portrait should not change, the origin cannot be
a spiral and must be a nonlinear centre.
9.3.3
An Example from Mechanics
Consider a rigid block of mass M attached to a horizontal spring. The block
lies Ô¨Çat on a horizontal conveyor belt that moves at speed U and tries to carry the
block away with it, as shown in Figure 9.7. From Newton‚Äôs second law of motion
and Hooke‚Äôs law,
M ¬®x = F( Àôx) ‚àík(x ‚àíxe),
where x is the length of the spring, xe is the equilibrium length of the spring, k is
the spring constant, F( Àôx) is the frictional force exerted on the block by the conveyor
belt, and a dot denotes d/dt. We model the frictional force as
F( Àôx) =

F0
for Àôx < U,
‚àíF0
for Àôx > U,
with F0 a constant force. When Àôx = U, the block moves at the same speed as
the conveyor belt, and this occurs when k|x ‚àíxe| < F0. In other words, the force
exerted by the spring must exceed the frictional force for the block to move. This
immediately gives us a solution, Àôx = U for xe ‚àíF0/k ‚©Ωx ‚©Ωxe + F0/k.
spring
Block,
mass M
Friction
here
U
x
Fig. 9.7. A spring-mounted, rigid block on a conveyor belt.

232
PHASE PLANE METHODS
Our model involves Ô¨Åve physical parameters, M, k, xe, F0 and U. If we now
deÔ¨Åne dimensionless variables ¬Øx = x/xe and ¬Øt = t/
#
M/k, we obtain
¬®¬Øx = ¬ØF ‚àí¬Øx + 1,
(9.15)
where
¬ØF(Àô¬Øx) =

¬ØF0
for Àô¬Øx < ¬ØU,
‚àí¬ØF0
for Àô¬Øx > ¬ØU.
(9.16)
We also have the possible solution Àô¬Øx = ¬ØU for 1 ‚àí¬ØF0 ‚©Ω¬Øx ‚©Ω1 + ¬ØF0. There are now
just two dimensionless parameters,
¬ØF0 = F0
kxe
,
¬ØU = U
xe
*
M
k .
We can now write (9.15) as the system
Àôx = y,
Àôy = F(y) ‚àíx + 1.
(9.17)
We have left out the overbars for notational convenience. This system has a single
equilibrium point at x = 1 + F0, y = 0. Since y = 0 < ¬ØU, the system is linear in
the neighbourhood of this point, with
 Àôx
Àôy

=

0
1
‚àí1
0
  x
y

+

0
F0 + 1

.
The Jacobian matrix has eigenvalues ¬±i, so the equilibrium point is a linear centre.
In fact, since Àôx (x ‚àí1 ‚àíF) + y Àôy = 0, we can integrate to obtain
{x ‚àí(1 + F0)}2 + y2 = constant
for y < U,
{x ‚àí(1 ‚àíF0)}2 + y2 = constant
for y > U.
(9.18)
The solutions for y Ã∏= U are therefore concentric circles, and we conclude that the
equilibrium point remains a centre when we take into account the eÔ¨Äect of the
nonlinear terms. The phase portrait is sketched in Figure 9.8.
We now need to take some care with integral paths that meet the line y = U.
Since the right hand side of (9.17) is discontinuous at y = U, the slope of the
integral paths is discontinuous there. For x < 1 ‚àíF0 and x > 1 + F0, trajectories
simply cross the line y = U. However, we have already seen that the line y = U
for 1 ‚àíF0 ‚©Ωx ‚©Ω1 + F0 is itself a solution. We conclude that an integral path that
meets y = U with x in this range follows this trajectory until x = 1 + F0, when
it moves oÔ¨Äon the limit cycle through the point D in Figure 9.8. For example,
consider the trajectory that starts at the point A. This corresponds to an initially
stationary block, with the spring stretched so far that it can immediately overcome
the frictional force.
The solution follows the circular trajectory until it reaches
B. At this point, the direction of the frictional force changes, and the solution
follows a diÔ¨Äerent circular trajectory, until it reaches C. At this point, the block is
stationary relative to the conveyor belt, which carries it along with it until the spring
is stretched far enough that the force it exerts exceeds the frictional force. This
occurs at the point D. Thereafter, the solution remains on the periodic solution

9.3 SECOND ORDER EQUATIONS
233
through D. On this periodic solution, the speed of the block is always less than
that of the conveyor belt, so the frictional force remains constant, and the block
undergoes simple harmonic motion.
x
y
D
C
B
A
‚Ä¢
Fig. 9.8. The phase portrait for a spring-mounted, rigid block on a conveyor belt.
9.3.4
Example: Population Dynamics
Consider two species of animals that live on an island with populations X(t)
and Y (t). If P is a typical size of a population, we can deÔ¨Åne x(t) = X(t)/P and
y(t) = Y (t)/P as dimensionless measures of the population of each species, and
regard x and y as continuous functions of time, t. A model for the way in which
the two species interact with each other and their environment is
Àôx = x (A + a1x + b1y) ,
Àôy = y (B + b2x + a2y) ,
(9.19)
where A, B, a1, b1, a2 and b2 are constants. We can interpret each of these equations
as
Rate of change of population = Present population √ó (Birth rate ‚àíDeath rate) .
Let‚Äôs now consider what each of the terms that model the diÔ¨Äerence between the
birth and death rates represents. If A > 0, the population of species x grows when
x ‚â™1 and y ‚â™1, so we can say that x does not rely on eating species y to survive.
In contrast, if A < 0, the population of x dies out, and therefore x must need to
eat species y to survive. The term a1x represents the eÔ¨Äect of overcrowding and
competition for resources within species x, so we require that a1 < 0. The term
b1y represents the interaction between the two species. If species x eats species y,
b1 > 0, so that the more of species y that is available, the faster the population of
x grows. If species y competes with x for the available resources, b1 < 0.

234
PHASE PLANE METHODS
We will consider two species that do not eat each other, but compete for re-
sources, for example sheep and goats (see Exercise 9.10 for an example of a predator‚Äì
prey system). SpeciÔ¨Åcally, we will study the typical system
Àôx = x(3 ‚àí2x ‚àí2y),
Àôy = y(2 ‚àí2x ‚àíy).
(9.20)
We will use this system to illustrate the full range of techniques that are available
to obtain information about the phase portrait of second order systems. The Ô¨Årst
thing to do is to determine where any equilibrium points are, and what their types
are. This will tell us almost everything we need to know in order to sketch the
phase portrait, as integral paths can only meet at equilibrium points. Equilibrium
points are the main structural features of the system, and the integral paths are
organized around them.
At equilibrium points, Àôx = Àôy = 0, and hence
(x = 0 or 2x + 2y = 3) and (y = 0 or 2x + y = 2) .
The four diÔ¨Äerent possibilities show that there are four equilibrium points, P1 =
(0, 0), P2 = (0, 2), P3 = (3/2, 0) and P4 = (1/2, 1). The Jacobian matrix is
J =
 Xx
Xy
Yx
Yy

=
 3 ‚àí4x ‚àí2y
‚àí2x
‚àí2y
2 ‚àí2x ‚àí2y

,
and hence
J(0, 0) =
 3
0
0
2

,
J(0, 2) =
 ‚àí1
0
‚àí4
‚àí2

,
J(3/2, 0) =
 ‚àí3
‚àí3
0
‚àí1

,
J(1/2, 1) =
 ‚àí1
‚àí1
‚àí2
‚àí1

.
Three of these matrices have at least one oÔ¨Ä-diagonal element equal to zero, so
their eigenvalues can be read directly from the diagonal elements. P1 has eigenval-
ues 3 and 2 and is therefore an unstable node. P2 has eigenvalues ‚àí1 and ‚àí2 and
is therefore a stable node. P3 has eigenvalues ‚àí3 and ‚àí1 and is therefore also a
stable node. We could work out the direction of the eigenvectors for these three
equilibrium points, but they are not really important for sketching the phase por-
trait. The Ô¨Ånal equilibrium point, P4, has eigenvalues Œª = Œª¬± = ‚àí1 ¬±
‚àö
2. Since
‚àö
2 > 1, P4 has one positive and one negative eigenvalue, and is therefore a saddle
point. For saddle points it is usually important to determine the directions of the
eigenvectors, since these determine the directions in which the separatrices leave
the equilibrium point. These separatrices are the only points that meet P4, and
we will see that determining their global behaviour is the key to sketching the full
phase portrait. The unit eigenvectors of P4 are u¬± = (
#
1/3, ‚àì
#
2/3)T.
Next, we can consider the nullclines. These are the lines on which either Àôx = 0
or Àôy = 0. The vertical nullclines, where Àôx = 0 (only y is varying so the integral
paths are vertical on the nullcline), are given by
x = 0 or 2x + 2y = 3.

9.3 SECOND ORDER EQUATIONS
235
The horizontal nullclines, where Àôy = 0, are given by
y = 0 or 2x + y = 2.
In general, the nullclines are not integral paths. However, in this case, since Àôx = 0
when x = 0 and Àôy = 0 when y = 0, the coordinate axes are themselves integral
paths. This is clear from the biology of the problem, since, if species x is absent
initially, there is no way for it to appear spontaneously, and similarly for species y.
All of the information we have amassed so far is shown in Figure 9.9(a). We
will conÔ¨Åne our attention to the positive quadrant, because we are not interested
in negative populations. Moreover, the fact that the coordinate axes are integral
paths prevents any trajectory from leaving the positive quadrant. Note that the
equilibrium points lie at the points where the nullclines intersect. The directions
of the arrows are determined by considering the signs of Àôx and Àôy at any point.
For example, on the x-axis, Àôx = x(3 ‚àí2x), so that Àôx > 0, which means that x is
increasing with t, for 0 < x < 3/2, and Àôx < 0, which means that x is decreasing
with t, for x > 3/2. From all of this local information, and the fact that integral
paths can only cross at equilibrium points, we can sketch a plausible and consistent
phase portrait, as shown in Figure 9.9(b). Apart from the stable separatrices of
the saddle point P4, labelled S1 and S2, all trajectories asymptote to either P2 or
P3 as t ‚Üí‚àû. These separatrices therefore represent the boundary between two
very diÔ¨Äerent types of behaviour. Depending upon the initial conditions, either
species x or species y dies out completely. Although neither species eats the other,
by competing for the same resources there can only be one eventual winner in the
competition to stay alive.
Although this is obviously a very simple model, the
displacement of the native red squirrel by the grey squirrel in Britain, and the
extinction of Neanderthal man after the arrival of modern man in Europe, are
examples of situations where two species competed for the same resources. It is
not necessary that one of the species kills the other directly. Simply a suÔ¨Écient
advantage in numbers or enough extra eÔ¨Éciency in exploiting natural resources
could have been enough for modern man to consign Neanderthal man to oblivion.
Returning to our simple model, there are some questions that we really ought
to answer before we can be conÔ¨Ådent that Figure 9.9(b) is an accurate sketch of
the phase portrait of (9.20). Firstly, can we be sure that there are no limit cycle
solutions? An associated question is, if we think that a system does possess a limit
cycle, can we prove it? Secondly, since the position of the stable separatrices of P4 is
so important, can we prove that we have sketched them correctly? More speciÔ¨Åcally,
how do we know that S1 originates at P1 and that S2 originates at inÔ¨Ånity? Finally,
what does the phase portrait look like far from the origin? We will consider some
mathematical tools for answering these questions in the next four sections. We can,
however, also test whether our phase portrait is correct by solving equations (9.19)
numerically in MATLAB. We must Ô¨Årst deÔ¨Åne the MATLAB function

236
PHASE PLANE METHODS
P4
P3
P1
P2
P3
P2
S2
S1
P4
P1
y
x
y
(a)
(b)
x
x + y = 3/2
2x + y = 2
Fig. 9.9. (a) Local information at the equilibrium points and the nullclines, and (b) the
full phase portrait for the population dynamical system (9.20).




function dy = population(t,y)
dy(1) = y(1)*(3-2*y(1)-2*y(2));
dy(2) = y(2)*(2-2*y(1)-
y(2));
dy=dy‚Äô;

9.3 SECOND ORDER EQUATIONS
237
which returns a column vector with elements given by the right hand sides of
(9.19). Then [t y] = ode45(@population, [0 10], [1 2]) integrates the equa-
tions numerically, in this case for 0 ‚©Ωt ‚©Ω10, with initial conditions x = y(1) = 1,
y = y(2) = 2. The results can then be displayed with plot(t,y). Figure 9.10
shows the eÔ¨Äect of holding the initial value of x Ô¨Åxed, and varying the initial value
of y. For small enough initial values of y, y ‚Üí0 as t ‚Üí‚àû. However, when the
initial value of y is suÔ¨Éciently large, the type of behaviour changes, and x ‚Üí0 as
t ‚Üí‚àû, consistent with the phase portrait sketched in Figure 9.9(b).
Fig. 9.10. The solution of (9.19) for various initial conditions. The dashed line is y, the
solid line is x.
9.3.5
The Poincar¬¥e Index
The Poincar¬¥e index of an equilibrium point provides a very simple way of deter-
mining whether a given equilibrium point or collection of equilibrium points can be
surrounded by one or more limit cycles.
Consider the second order system of nonlinear, autonomous ordinary diÔ¨Äerential
equations given by (9.7). Let Œì be any smooth, closed, nonself-intersecting curve in
the (x, y)-phase plane, not necessarily an integral path, that does not pass through
any equilibrium points, as shown in Figure 9.11. At any point P lying on Œì, there

238
PHASE PLANE METHODS
is a unique integral path through P, with a well-deÔ¨Åned direction that we can
characterize using the angle, œà(P), that it makes with the horizontal (tan œà(P) =
Y/X). The angle œà varies continuously as P moves around Œì. If œà changes by an
amount 2nŒìœÄ as P makes one complete, anticlockwise circuit around Œì, we call nŒì
the Poincar¬¥e index, which must be an integer.
P
Œì
œà
Fig. 9.11. The deÔ¨Ånition of the Poincar¬¥e index.
We can now deduce some properties of the Poincar¬¥e index.
(i) If Œì is continuously distorted without passing through any equilibrium points,
nŒì does not change.
Proof
Since nŒì must vary continuously under the action of a continuous
distortion, but must also be an integer, the Poincar¬¥e index cannot change
from its initial value.
(ii) If Œì does not enclose any equilibrium points, nŒì = 0.
Proof Using the previous result, we can continuously shrink Œì down to an
arbitrarily small circle without changing nŒì. On this circle, œà(P) is almost
constant, and therefore nŒì = 0.
(iii) If Œì is the sum of two closed curves, then Œì1 and Œì2, nŒì = nŒì1 + nŒì2.
Proof
Consider the curves shown in Figure 9.12. On the curve where Œì1
and Œì2 intersect, the amount by which œà varies on traversing Œì1 is equal
and opposite to the amount by which œà varies on traversing Œì2, so these
contributions cancel and nŒì = nŒì1 + nŒì2.
(iv) If Œì is a limit cycle then nŒì = 1.
Proof This result is obvious.
(v) If integral paths either all enter the region enclosed by Œì or all leave this
region, nŒì = 1.
Proof This result is also obvious.

9.3 SECOND ORDER EQUATIONS
239
Œì1
Œì2
Fig. 9.12. The sum of two curves.
(vi) If Œì encloses a single node, focus or nonlinear centre, nŒì = 1.
Proof
For a node or focus, Œì can be continuously deformed into a small
circle surrounding the equilibrium point. The linearized solution then shows
that integral paths either all enter or all leave the region enclosed by Œì, and
the previous result shows that nŒì = 1. For a nonlinear centre, Œì can be con-
tinuously deformed into one of the limit cycles that encloses the equilibrium
point, and result (iv) shows that nŒì = 1.
(vii) If Œì encloses a single saddle point, nŒì = ‚àí1.
Proof Œì can be continuously deformed into a small circle surrounding the
saddle point, where the linearized solution gives the phase portrait shown
in Figure 9.5. The direction of the integral paths makes a single clockwise
rotation as P traverses Œì, and hence nŒì = ‚àí1.
These results show that we can deÔ¨Åne the Poincar¬¥e index of an equilibrium
point to be the Poincar¬¥e index of any curve that encloses the equilibrium point and
no others. In particular, a node, focus or nonlinear centre has Poincar¬¥e index n = 1,
whilst a saddle has Poincar¬¥e index n = ‚àí1. Nonhyperbolic equilibrium points can
have other Poincar¬¥e indices, but we will not consider these here. Now result (iii)
shows that nŒì is simply the sum of the Poincar¬¥e indices of all the equilibrium points
enclosed by Œì. Finally, and this is what all of this has been leading up to, result (iv)
shows that the sum of the Poincar¬¥e indices of the equilibrium points enclosed by a
limit cycle must be 1. A corollary of this is that a limit cycle must enclose at least
one node, focus or centre.
Now let‚Äôs return to our example population dynamical system, (9.20). If a limit
cycle exists, it cannot cross the coordinate axes, since these are integral paths.
However, there is no node, focus or nonlinear centre in the positive quadrant of the
phase plane, and we conclude that no limit cycle exists.

240
PHASE PLANE METHODS
When a system does possess a node, focus or nonlinear centre, the Poincar¬¥e
index can be used to determine which sets of equilibrium points a limit cycle could
enclose if it existed, but it would be useful to have a way of ruling out the existence
of limit cycles altogether, if indeed no limit cycles exist. We now consider a way of
doing this that works for many types of system.
9.3.6
Bendixson‚Äôs Negative Criterion and Dulac‚Äôs Extension
Theorem 9.2 (Bendixson‚Äôs negative criterion) For the second order system of
nonlinear, autonomous ordinary diÔ¨Äerential equations given by (9.7), there are no
limit cycles in any simply-connected region of the (x, y)-phase plane where Xx + Yy
does not change sign.
Proof Let Œì be a limit cycle. For the vector Ô¨Åeld (X(x, y), Y (x, y)), Stokes‚Äô theorem
states that

D
‚àÇX
‚àÇx + ‚àÇY
‚àÇy

dx dy =

Œì
(Xdy ‚àíY dx) ,
where D is the region enclosed by Œì. However, we can write the right hand side of
this as

Œì

X dy
dt ‚àíY dx
dt

dt =

Œì
(XY ‚àíY X) dt = 0,
and hence

D
‚àÇX
‚àÇx + ‚àÇY
‚àÇy

dx dy = 0.
If Xx + Yy does not change sign in D, then the integral is either strictly positive or
strictly negative‚Ä†, which is a contradiction, and hence the result is proved.
Note that the restriction of this result to simply-connected regions (regions without
holes) is crucial.
If we now apply this theorem to the system (9.20), we Ô¨Ånd that Xx + Yx =
5‚àí6x‚àí4y. Although this tells us that any limit cycle solution must be intersected
by the line 6x + 4y = 5, we cannot rule out the possibility of the existence of
limit cycles in this way. We need a more powerful version of Bendixson‚Äôs negative
criterion, which is provided by the following theorem.
Theorem 9.3 (Dulac‚Äôs extension to Bendixson‚Äôs negative criterion) If
œÅ(x, y) is continuously diÔ¨Äerentiable in some simply-connected region D in the
(x, y)-phase plane, there are no limit cycle solutions if (œÅX)x + (œÅY )y does not
change sign.
‚Ä† ignoring the degenerate case where Xx + Yy ‚â°0 in D.

9.3 SECOND ORDER EQUATIONS
241
Proof Since
 
D
‚àÇ(œÅX)
‚àÇx
+ ‚àÇ(œÅY )
‚àÇy

dx dy =

Œì
œÅ (Xdy ‚àíY dx) ,
the proof is as for Bendixson‚Äôs negative criterion.
Returning again to (9.20), we can choose œÅ = 1/xy and consider the positive
quadrant where œÅ(x, y) is continuously diÔ¨Äerentiable. We Ô¨Ånd that
‚àÇ(œÅX)
‚àÇx
+ ‚àÇ(œÅY )
‚àÇy
= ‚àí1
x ‚àí2
y < 0,
and hence that there can be no limit cycle solutions in the positive quadrant, as
expected.
Now that we know how to try to rule out the existence of limit cycles, how can
we go about showing that limit cycles do exist for appropriate systems? We will
consider this in the next section using the Poincar¬¥e‚ÄìBendixson theorem. This will
also allow us to prove that the stable separatrices of P4, shown in Figure 9.9, do
indeed behave as sketched.
9.3.7
The Poincar¬¥e‚ÄìBendixson Theorem
We begin with a deÔ¨Ånition. Consider the second order system (9.7). If I+ is
a closed subset of the phase plane and any integral path that lies in I+ when
t = 0 remains in I+ for all t ‚©æ0, we say that I+ is a positively invariant set.
Similarly, a negatively invariant set is a closed subset, I‚àí, of the phase plane
and all integral paths in I‚àíwhen t = 0 remain there when t < 0. For example, an
equilibrium point is both a positively and a negatively invariant set, as is a limit
cycle. As a less prosaic example, any subset, S, of the phase plane with outward
unit normal n that has (X, Y ) ¬∑ n ‚©Ω0 on its boundary, so that all integral paths
enter S, is a positively invariant set.
Theorem 9.4 (Poincar¬¥e‚ÄìBendixson) If there exists a bounded, invariant region,
I, of the phase plane, and I contains no equilibrium points, then I contains at least
one limit cycle.
Note that it is crucial that the region I be bounded, but, in contrast to Bendix-
son‚Äôs negative criterion, I does not need to be simply-connected. As we shall see,
if I contains a limit cycle, it cannot be simply-connected. The Poincar¬¥e‚ÄìBendixson
theorem says that the integral paths in I cannot wander around for ever without
asymptoting to a limit cycle. Although this seems obvious, we shall see in Chap-
ter 15 that the Poincar¬¥e‚ÄìBendixson theorem does not hold for third and higher
order systems, in which integral paths that represent chaotic solutions can indeed
wander around indeÔ¨Ånitely without being attracted to a periodic solution.
The details of the proof are rather technical, and can be omitted on Ô¨Årst reading.

242
PHASE PLANE METHODS
Proof of the Poincar¬¥e‚ÄìBendixson Theorem
We will assume that I is a positively invariant region. The proof when I is negatively
invariant is identical, but with time reversed. We now need two further deÔ¨Ånitions.
Let x(t; x0) be the solution of (9.7) with x = x0 when t = 0. We say that x1
is an œâ-limit point of x0 if there exists a sequence, {ti}, such that ti ‚Üí‚àûand
x(ti; x0) ‚Üíx1 as i ‚Üí‚àû. For example, all the points on an integral path that enters
a stable node or focus at x = xe have xe as their only œâ-limit point. Similarly, all
points on an integral path that asymptotes to a stable limit cycle as t ‚Üí‚àûhave
all the points that lie on the limit cycle as œâ-limit points. We call the set of all
œâ-limit points of x0 the œâ-limit set of x0, which is denoted by œâ(x0).
Let L be a Ô¨Ånite curve such that all integral paths that meet L cross it in the
same direction. We say that L is a transversal. If x0 is not an equilibrium point,
it is always possible to construct a transversal through x0, since the slope of the
integral paths is well-deÔ¨Åned in the neighbourhood of x0.
Lemma 9.1 Let I be a positively invariant region in the phase plane, and L ‚äÇI a
transversal through x0 ‚ààI. The integral path, x(t; x0), that passes through x0 when
t = 0 intersects L in a monotone sequence. In other words, if xi is the point where
x(t; x0) meets L for the ith time, then xi lies in the segment of L between xi‚àí1 and
xi+1.
Proof Consider the invariant region D bounded by the integral path from xi‚àí1 to
xi and the segment of L between xi‚àí1 and xi. There are two possibilities. Firstly,
the integral path through xi enters D and remains there (see Figure 9.13(a)). In
this case, xi+1, if it exists, lies in D, and xi therefore lies in the segment of L
between xi‚àí1 and xi+1. Secondly, the integral path through xi‚àí1 originates in D
(see Figure 9.13(b)). In this case, xi‚àí2, if it exists, lies in D, and xi‚àí1 therefore
lies in the segment of L between xi‚àí2 and xi.
L
D
xi ‚àí 1
xi +1
xi
L
(a)
(b)
xi
xi ‚àí 2
xi ‚àí 1
Fig. 9.13. The intersection of an integral path in an invariant region with a transversal.

9.3 SECOND ORDER EQUATIONS
243
Note that Lemma 9.1 does not hold in systems of higher dimension, since it relies
crucially on the fact that a closed curve separates an inside from an outside (the
Jordan curve theorem).
Lemma 9.2 Consider the region I, transverse curve L and point x0 ‚ààI deÔ¨Åned in
Lemma 9.1. The œâ-limit set of x0, œâ(x0), intersects L at most once.
Proof Suppose that œâ(x0) intersects L twice, at ÀÜx and ¬Øx. We can therefore Ô¨Ånd
sequences of points, {ÀÜxi} and {¬Øxi}, lying in L such that {ÀÜxi} ‚ÜíÀÜx and {¬Øxi} ‚Üí¬Øx as
i ‚Üí‚àû. However, this cannot occur, since Lemma 9.1 states that these intersections
must be monotonic. Hence the result is proved by contradiction.
Lemma 9.3 If x1 ‚ààœâ(x0) is not an equilibrium point and lies on x(t; x0), the
integral path through x0, then x(t; x1), the integral path through x1, is a closed
curve, also passing through x0.
Proof
Since x1 lies in x(t; x0), œâ(x1) = œâ(x0), and hence x1 ‚ààœâ(x1). Let L be
a curve through x1 transverse to the integral paths. By Lemma 9.2, x(t; x1) can
only meet L once, and hence is a closed curve.
We can now Ô¨Ånally prove Theorem 9.4, the Poincar¬¥e‚ÄìBendixson theorem.
Proof
Let x0 be a point in I, and hence x(t; x0) ‚äÇI and œâ(x0) ‚äÇI. Choose
x1 ‚ààœâ(x0).
If x1 ‚ààx(t; x0), then, by Lemma 9.3, the integral path through x1 is a closed
curve and, since there are no equilibrium points in I, must be a limit cycle.
If x1 Ã∏‚ààx(t; x0), let x2 ‚ààœâ(x1).
Let L be a transversal through x2.
Since
x2 ‚ààœâ(x1), the integral path though x1 must intersect L at a monotonic sequence
of points x1i, such that x1i ‚Üíx2 as i ‚Üí‚àû. But x1i ‚ààœâ(x0), so the integral path
through x0 must pass arbitrarily close to each of the points x1i as t ‚Üí‚àû. However,
the intersections of the integral path through x0 with L should be monotonic, by
Lemma 9.1, and we conclude that x1i = x2, and hence that œâ(x1) is the closed
limit cycle through x1.
Example
Consider the system
Àôx = x ‚àíy ‚àí2x(x2 + y2),
Àôy = x + y ‚àíy(x2 + y2).
(9.21)
In order to analyze these equations, it is convenient to write them in terms of
polar coordinates, (r, Œ∏). From the deÔ¨Ånitions, r = (x2 + y2)1/2 and x = r cos Œ∏,
y = r sin Œ∏, the chain rule gives
Àôr = (x Àôx + y Àôy)(x2 + y2)‚àí1/2 = cos Œ∏ Àôx + sin Œ∏ Àôy.
(9.22)
For (9.21),
Àôr = cos Œ∏

r cos Œ∏ ‚àír sin Œ∏ ‚àí2r3 cos Œ∏

+ sin Œ∏

r cos Œ∏ + r sin Œ∏ ‚àír3 sin Œ∏


244
PHASE PLANE METHODS
= r ‚àír3 
1 + cos2 Œ∏

.
Since 0 ‚©Ωcos2 Œ∏ ‚©Ω1, we conclude that
r(1 ‚àí2r2) ‚©ΩÀôr ‚©Ωr(1 ‚àír2).
Therefore Àôr > 0 for 0 < r < 1/
‚àö
2 and Àôr < 0 for r > 1. Remembering that if Àôr > 0, r
is an increasing function of t, and therefore integral paths are directed away from the
origin, we can deÔ¨Åne a closed, bounded, annular region D = {(r, Œ∏) | r0 ‚©Ωr ‚©Ωr1}
with 0 < r0 < 1/
‚àö
2 and r1 > 1, such that all integral paths enter the region I, as
shown in Figure 9.14. If we can show that there are no equilibrium points in I, the
Poincar¬¥e‚ÄìBendixson theorem shows that at least one limit cycle exists in I.
Œì = Œì1
Œì = Œì0
Œô
Fig. 9.14. The region I that contains a limit cycle for the system (9.21).
From the deÔ¨Ånition of Œ∏ = tan‚àí1(y/x),
ÀôŒ∏ =
1
1 + (y/x)2
x Àôy ‚àíy Àôx
x2
= 1
r (cos Œ∏ Àôy ‚àísin Œ∏ Àôx) .
(9.23)
For (9.21),
ÀôŒ∏ = 1
r

cos Œ∏

r cos Œ∏ + r sin Œ∏ ‚àír3 sin Œ∏

‚àísin Œ∏

r cos Œ∏ ‚àír sin Œ∏ ‚àí2r3 sin Œ∏

= 1 + 1
2r2 sin 2Œ∏.

9.3 SECOND ORDER EQUATIONS
245
Since ‚àí1 ‚©Ωsin 2Œ∏ ‚©Ω1, we conclude that ÀôŒ∏ > 0 provided r <
‚àö
2, and hence
that there are no equilibrium points for 0 < r <
‚àö
2. Therefore, provided that
1 < r1 <
‚àö
2, there are no equilibrium points in I, and hence, by the Poincar¬¥e‚Äì
Bendixson theorem, there is at least one limit cycle in I. Note that we know that
a limit cycle must enclose at least one node, focus or centre. In this example, there
is an unstable focus at r = 0, which is enclosed by any limit cycles, but which does
not lie within I. In general, if we can construct a region in the phase plane that
traps a limit cycle in this way, it cannot be simply-connected.
A corollary of the Poincar¬¥e‚ÄìBendixson theorem is that, if I is a closed, bounded,
invariant region of the phase plane, and I contains no limit cycles, we can de-
duce that all of the integral paths that meet the boundary of I terminate at an
equilibrium point contained within I. We can use similar ideas to determine the
positions of the stable separatrices of P4 for our example population dynamical
system, (9.20).
Consider the region
R1 = {(x, y) | x ‚©æ0, y ‚©æ0, 2x + y ‚©Ω2, 2x + 2y ‚©Ω3} ,
shown in Figure 9.15. The separatrix S1 lies in R1 in the neighbourhood of the
saddle point, P4. No integral path enters the region R1 through its boundaries, and
we conclude that S1 must originate at the equilibrium point at the origin, P1.
Now consider the region
R2 =

(x, y) | x ‚©æ0, y ‚©æ0, 2x + y ‚©æ2, 2x + 2y ‚©æ3, x2 + y2 ‚©Ωr2
0

,
with r0 > 3, as shown in Figure 9.15. The separatrix S2 lies in R2 in the neigh-
bourhood of the saddle point, P4. No integral path enters the region R2 through
any of its straight boundaries, and we conclude that S1 must enter R2 through its
curved boundary, x2 +y2 = r2
0. Since we can make r0 arbitrarily large, we conclude
that S2 originates at inÔ¨Ånity.
9.3.8
The Phase Portrait at InÔ¨Ånity
In order to study the behaviour of a second order system far from the origin,
it is often useful to map the phase plane to a new coordinate system where the
point at inÔ¨Ånity is mapped to a Ô¨Ånite point. There are several ways of doing this,
but the most useful is the Poincar¬¥e projection. Consider the plane that passes
through the line x = 1, perpendicular to the (x, y)-plane, as shown in Figure 9.16.
We can set up a Cartesian coordinate system, (u, v), in this plane, with origin a
unit distance above x = 1, y = 0, u-axis in the same direction as the y-axis and
v-axis vertically upwards. We label the origin of the (x, y)-plane as O, and denote
the point a unit distance above the origin as O‚Ä≤. We now consider a point B in
the (x, y)-plane. The Poincar¬¥e projection maps the point B to the point C where
the line O‚Ä≤B intersects the (u, v)-plane. In particular, as x ‚Üí‚àû, C approaches
the u-axis, so that this projection is useful for studying the behaviour of a second
order system for x ‚â´1.
In order to relate u and v to x and y, we Ô¨Årstly consider the triangles OAB and

246
PHASE PLANE METHODS
R2
S2
S1
P4
2x + y = 2
2x + 2y = 3
x
y
R1
Fig. 9.15. The regions R1 and R2 for the population dynamical system (9.20).
OA‚Ä≤B‚Ä≤, shown in Figure 9.16, which are similar. This shows that u = y/x and
#
x2 + y2 ‚àím =
#
x2 + y2/x. Secondly, we consider the similar triangles BB‚Ä≤C
and BOO‚Ä≤, which show that (1 + v)/m = 1/
#
x2 + y2. By eliminating m, we Ô¨Ånd
that v = ‚àí1/x. The simple change of variables u = y/x, v = ‚àí1/x, and hence
Àôu = uv Àôx ‚àív Àôy,
Àôv = v2 Àôx,
(9.24)
is therefore a Poincar¬¥e projection.
If we apply this to the population dynamics example (9.20), we Ô¨Ånd that
Àôu = ‚àíu

1 + u
v

,
Àôv = ‚àí(2u + 3v + 2).
(9.25)
This has Ô¨Ånite equilibrium points at u = 0, v = ‚àí2/3, which corresponds to P3, and
u = 2, v = ‚àí2, which corresponds to P4. The nature of these equilibrium points
remains the same after the projection, so that P3 is a stable node and P4 is a saddle
point. In order to determine the behaviour of (9.20) for x ‚â´1, we need to consider
(9.25) close to the u-axis for v < 0 and u > 0. When ‚àív ‚â™1, Àôu ‚â´1, whilst Àôv < 0.
Integral paths close to the u-axis therefore start parallel to it, but then move away.
Figure 9.17 is a sketch of the phase portrait in the (u, v)-plane. We conclude that
the integral paths of (9.20) for x ‚â´1 lead into the Ô¨Ånite (x, y)-plane as sketched in
Figure 9.9(b). The analogous transformation, u = x/y, v = ‚àí1/y is also a Poincar¬¥e
projection, and can be used to examine the behaviour for y ‚â´1.

9.3 SECOND ORDER EQUATIONS
247
O‚Ä≤
O
A = (1, 0)
A = (x, 0)
x
y
y
1
m
1+v
C = (u, v)
B‚Ä≤
B = (x, y)
u
v
 '
Fig. 9.16. A Poincar¬¥e projection.
We have now answered all of the questions concerning the phase portrait of (9.20)
that we asked at the end of Section 9.3.4.
9.3.9
A Final Example: Hamiltonian Systems
Let ‚Ñ¶be a region of the (x, y)-plane and H : ‚Ñ¶‚ÜíR be a real-valued, continu-
ously diÔ¨Äerentiable function deÔ¨Åned on ‚Ñ¶. The two-dimensional system of ordinary
diÔ¨Äerential equations,
Àôx = Hy(x, y),
Àôy = ‚àíHx(x, y),
(9.26)
is called a Hamiltonian system and H(x, y) is called the Hamiltonian. Such
systems occur frequently in mechanics. One example is the simple pendulum, which
we studied earlier. As we have seen, this has Àôx = y = Hy, Àôy = ‚àíœâ2 sin x = ‚àíHx,
so that H = 1
2y2 ‚àíœâ2 cos x, the total energy, is a Hamiltonian for the system.
Hamiltonian systems have several general properties, which we will now investi-
gate.
Theorem 9.5 The integral paths of a Hamiltonian system are given by H(x, y) =
constant.
Proof On an integral path (x(t), y(t)), H = H(x(t), y(t)) and
dH
dt = Hx Àôx + Hy Àôy = HxHy ‚àíHyHx = 0,
so that H is constant.

248
PHASE PLANE METHODS
P3
P4
u
v
Fig. 9.17. A sketch of the phase portrait of the system of equations (9.25).
For the simple pendulum, the integral paths are therefore given by the curves
y2 ‚àí2œâ2 cos x = constant.
Theorem 9.6 If x = xe is an equilibrium point of a Hamiltonian system with
nonzero eigenvalues, then it is either a saddle point or a centre.
Proof The Jacobian of (9.26) at x = xe is
J =

Hyx
Hyy
‚àíHxx
‚àíHxy

x=xe
.
Since Hxy = Hyx, the eigenvalues of J satisfy
Œª2 = H2
xy(xe, ye) ‚àíHxx(xe, ye)Hyy(xe, ye).
Note that H2
xy Ã∏= HxxHyy at x = xe since the eigenvalues are nonzero. If H2
xy >
HxxHyy at x = xe, there is one positive and one negative eigenvalue, so xe is
a saddle point. If H2
xy < HxxHyy at x = xe, there are two complex conjugate
imaginary eigenvalues. Since the integral paths are given by H = constant and the
conditions Hx = Hy = 0 and HxxHyy > H2
xy are those for a local maximum or
minimum at x = xe, the level curves of H are closed and surround x = xe. We
conclude that the equilibrium point is a nonlinear centre.

9.4 THIRD ORDER EQUATIONS
249
Theorem 9.7 (Liouville‚Äôs theorem) Hamiltonian systems are area-preserving.
Proof
Consider a small triangle with one vertex at x0, the other two vertices at
x0 + Œ¥x1 and x0 + Œ¥x2, and |Œ¥x1|, |Œ¥x2| ‚â™1. The area of this triangle is |A|, where
A = 1
2 (Œ¥x1 √ó Œ¥x2) .
If all of the vertices move along integral paths of the Hamiltonian system (9.26), a
Taylor expansion shows that
d
dtŒ¥xi ‚àº(Œ¥xiHyx(x0) + Œ¥yiHyy(x0), ‚àíŒ¥xiHxx(x0) ‚àíŒ¥yiHxy(x0)) ,
for i = 1, 2, where Œ¥xi = (Œ¥xi, Œ¥yi). Up to O(|Œ¥xi|), we therefore have
2dA
dt = d
dtŒ¥x1 √ó Œ¥x2 + Œ¥x1 √ó d
dtŒ¥x2
= Œ¥y2 {Œ¥x1Hyx(x0) + Œ¥y1Hyy(x0)} + Œ¥x2 {Œ¥x1Hxx(x0) + Œ¥y1Hxy(x0)}
‚àíŒ¥y1 {Œ¥x2Hyx(x0) + Œ¥y2Hyy(x0)} ‚àíŒ¥x1 {Œ¥x2Hxx(x0) + Œ¥y2Hxy(x0)} = 0,
so the area of the triangle is unchanged under the action of a Hamiltonian system.
Since any area in the phase plane can be broken up into inÔ¨Ånitesimal triangles, the
Hamiltonian system is area-preserving.
9.4
Third Order Autonomous Nonlinear Ordinary DiÔ¨Äerential
Equations
The solutions of the third order system
Àôx = X(x, y, z),
Àôy = Y (x, y, z),
Àôz = Z(x, y, z),
(9.27)
can be analyzed in terms of integral paths in a three-dimensional, (x, y, z)-phase
space. However, most of the useful results that hold for the phase plane do not
hold in three or more dimensions. The crucial diÔ¨Äerence is that, in three or more
dimensions, closed curves no longer divide the phase space into two distinct regions,
inside and outside the curve. For example, integral paths inside a limit cycle in the
phase plane are trapped there, but this is not the case in a three-dimensional phase
space. There is no analogue of the Poincar¬¥e index or Bendixson‚Äôs negative criterion,
nor, as we noted earlier, is there an analogue of the Poincar¬¥e‚ÄìBendixson theorem.
In third or higher order systems, integral paths can be attracted to strange or
chaotic attractors, which have fractal or noninteger dimensions, and rep-
resent chaotic solutions. A simple way to get a grasp of this is to remember
that cars, which drive about on a plane, often hit each other, but aircraft, which
have an extra dimension to use, do so more rarely.
We will examine some ele-
mentary techniques for studying chaotic solutions in Chapter 15. There are also
some interesting, and useful, conservative third order systems for which a more
straightforward analysis is possible (see also Exercise 13.4).

250
PHASE PLANE METHODS
Example
Consider the third order system
Àôx = yz,
Àôy = ‚àíxz,
Àôz = ‚àík2xy.
(9.28)
In this rather degenerate system, any point on the x-, y- or z-axis is an equilibrium
point. We can also see that
d
dt(x2 + y2) = 2x Àôx + 2y Àôy = 2xyz ‚àí2yxz = 0.
Therefore, x2 + y2 is constant on any integral path, and hence all integral paths lie
on the surface of a cylinder with its axis pointing in the z-direction.
Consider the integral paths on the surface of the cylinder x2 + y2 = 1. This
surface is two-dimensional, so we should be able to analyze the behaviour of integral
paths on it using phase plane techniques. There are equilibrium points at (0, ¬±1, 0)
and (¬±1, 0, 0) and the Jacobian matrix is
J =
Ô£´
Ô£≠
Xx
Xy
Xz
Yx
Yy
Yz
Zx
Zy
Zz
Ô£∂
Ô£∏=
Ô£´
Ô£≠
0
z
y
‚àíz
0
‚àíx
‚àík2y
‚àík2x
0
Ô£∂
Ô£∏.
This gives
J(¬±1, 0, 0) =
Ô£´
Ô£≠
0
0
0
0
0
‚àì1
0
‚àìk2
0
Ô£∂
Ô£∏,
J(0, ¬±1, 0) =
Ô£´
Ô£≠
0
0
¬±1
0
0
0
‚àìk2
0
0
Ô£∂
Ô£∏.
The points (¬±1, 0, 0) each have eigenvalues Œª = 0, ¬±k. The zero eigenvalue, with
eigenvector (0, 1, 0)T, corresponds to the fact that the y-axis is completely made up
of equilibrium points. The remaining two eigenvalues are real and of opposite sign,
and control the dynamics on the cylinder x2 +y2 = 1, where the equilibrium points
are saddles. Similarly, the points (0, ¬±1, 0) each have eigenvalues Œª = 0, ¬±ik, and
are therefore linear centres on x2 + y2 = 1. These remain centres when nonlinear
terms are taken into account, using the argument that we described earlier for the
simple pendulum, since the system is unchanged by the transformation z ‚Üí‚àíz,
t ‚Üí‚àít. The phase portrait is sketched in Figure 9.18.
We can conÔ¨Årm that this phase portrait is correct by noting that this system
actually has two other conserved quantities. From (9.28),
d
dt(k2y2 ‚àíz2) = d
dt(k2x2 + z2) = 0,
and hence k2y2 ‚àíz2 and k2x2 +z2 are constant on any integral path. Integral paths
therefore lie on the intersection of the circular cylinder x2 + y2 = constant, the
hyperboloidal cylinder k2y2 ‚àíz2 = constant, and the elliptical cylinder k2x2 +z2 =
constant. This is precisely what the phase portrait in Figure 9.18 shows.
Finally, consider the integral path with x = 0, y = z = 1 when t = 0. On this
integral path, k2x2 + z2 = 1 and x2 + y2 = 1, so that
Àôx =
#
1 ‚àíx2#
1 ‚àík2x2,

EXERCISES
251
Fig. 9.18. The phase portrait of the third order system (9.28) on the surface of the cylinder
x2 + y2 = 1 when k = 2.
which gives‚Ä†
t =
 x
0
ds
‚àö
1 ‚àís2‚àö
1 ‚àík2s2 .
This is the deÔ¨Ånition of the Jacobian elliptic function sn(t ; k). On this integral
path y and z are also Jacobian elliptic functions, y = cn(t ; k) and z = dn(t ; k). The
phase portrait that we have just determined now allows us to see qualitatively that
these elliptic functions are periodic with t, provided that k Ã∏= 1. In Sections 12.2.3
and 12.2.4 we will develop asymptotic expansions for sn(t ; k), Ô¨Årstly when k is close
to unity, and secondly when k ‚â™1. The Jacobian elliptic functions will also prove
to be useful in Section 12.2.5.
Exercises
9.1
Consider the second order, autonomous ordinary diÔ¨Äerential equation
¬®x = 3x2 ‚àí1,
where a dot represents d/dt. By integrating this equation once, obtain a
relation between Àôx and x. Sketch the phase portrait in the (x, Àôx)-phase
plane. Determine the coordinates of the two equilibrium points and show
that there is a homoclinic orbit associated with one of them. What types
of behaviour occur inside and outside the homoclinic orbit?
‚Ä† Note that the properties of the particular integral path with x = 0, y = z = 1 when t = 0
ensure that the arguments of the square roots remain positive.

252
PHASE PLANE METHODS
9.2
By sketching the curve Àôx = X(x), determine the equilibrium points and
corresponding domains of attraction when
(a) X(x) = x2 ‚àíx ‚àí2,
(b) X(x) = e‚àíx ‚àí1,
(c) X(x) = sin x.
Now check that this qualitative analysis is correct by actually solving each
equation with initial conditions x = x0 when t = 0. Which method do you
think is easier to use, qualitative or quantitative?
9.3
Find the eigenvalues and eigenvectors of the matrix A, and then sketch the
phase portrait of the linear system Àôu = Au, where u = (x, y)T, for A =
(a)

1
‚àí5
1
‚àí1

,
(b)

1
1
1
‚àí2

,
(c)
 ‚àí4
2
3
‚àí2

,
(d)

1
2
2
2

,
(e)

4
‚àí2
3
‚àí1

,
(f)

2
1
‚àí1
1

.
9.4
Consider a second order linear system Àôu = Au, when the constant matrix
A has two equal eigenvalues, Œª. By using the Cayley‚ÄìHamilton theorem,
show that there must exist a linear transformation that takes A to either
Œõ1 =
 Œª
0
0
Œª

or Œõ2 =
 Œª
1
0
Œª

.
Solve the linear system of equations Àôv = Œõjv for j = 1 and j = 2, and
hence sketch the phase portrait in each case. Note that in the case j = 1,
the equilibrium point at the origin is known as a star, whilst when j = 2
it is an improper node.
9.5
A certain second order autonomous system has exactly two equilibrium
points, both of which are saddles. Sketch a phase portrait in which (a) a
separatrix connects the saddle points, (b) no separatrix connects the saddle
points.
9.6
The weight at the end of a simple pendulum experiences a frictional force
proportional to its velocity. Determine the equation of motion and write
it as a pair of Ô¨Årst order equations. Show that the equilibrium points are
either stable points or saddle points.
Sketch the phase portrait.
What
happens after a long time?
9.7
Find all of the equilibrium points of each of the following systems, and
determine their type. Sketch the phase portrait in each case.
(a) Àôx = x ‚àíy,
Àôy = x + y ‚àí2xy2,
(b) Àôx = ‚àí3y + xy ‚àí10,
Àôy = y2 ‚àíx2,
(c) Àôx = y2 ‚àí1,
Àôy = sin x.
9.8
Consider the system of ordinary diÔ¨Äerential equations
dx
dt = x

x2y2 + 1

,
dy
dt = y (y ‚àí2 ‚àíx) .

EXERCISES
253
Determine the position and type of each equilibrium point in the (x, y)-
plane. Show that the coordinate axes are integral paths. Sketch the phase
portrait, taking care to ensure that your sketch is consistent with the po-
sition of the horizontal nullcline. If x = ‚àí1 and y = ‚àí1 when t = 0, how
does the solution behave as t ‚Üí‚àû?
9.9
The second order system
dr
dt = r(3 ‚àír ‚àís),
ds
dt = s(2 ‚àír ‚àís),
can be used to model the population of sheep (s) and rabbits (r) in a closed
ecosystem. Determine the position and type of all the equilibrium points.
Find the directions of the separatrices close to any saddle points. Assuming
that there are no limit cycles, sketch the phase portrait for r > 0 and s > 0.
Which animal becomes extinct?
9.10
Explain how the system
Àôx = x(‚àíA + b1y),
Àôy = y(B ‚àíb2x),
with the constants A, B, b1 and b2 positive, models the populations of a
carnivorous, predator species and its herbivorous prey in a closed ecosys-
tem. Which variable is the predator population, x or y? Determine the
type of each of the equilibrium points. Determine dy/dx as a function of
x and y, and integrate once to obtain an equation of the form E(x, y) =
constant. Show that E(x, y) has a local minimum at one of the equilibrium
points, and hence deduce that it is a nonlinear centre. Sketch the phase
portrait. What happens to the populations?
9.11
Use the concept of the Poincar¬¥e index to determine which of the following
can be surrounded by a limit cycle in the phase portrait of a second order
system.
(a) an unstable node,
(b) a saddle point,
(c) two saddle points, a stable node and an unstable focus,
(d) a saddle point, an unstable focus and a stable node.
Sketch a possible phase portrait in each of the cases where a limit cycle
can surround the equilibrium points.
9.12
Consider the system
dx
dt = x(x2 + y2 ‚àí1),
dy
dt = y(x2 + y2 ‚àí2).
Show that the x- and y-axes are integral paths. Show that there are no
limit cycle solutions using
(a) Dulac‚Äôs extension to Bendixson‚Äôs negative criterion with auxiliary
function œÅ(x, y) = 1/xy,
(b) the Poincar¬¥e index.
Sketch the phase portrait.

254
PHASE PLANE METHODS
9.13
Use the Poincar¬¥e index, Bendixson‚Äôs negative criterion or Dulac‚Äôs exten-
sion as appropriate to show that the following systems have no limit cycle
solutions.
(a) Àôx = y,
Àôy = 1 + x2 ‚àí(1 ‚àíx)y,
(b) Àôx = ‚àí(1 ‚àíx)3 + xy2,
Àôy = y + y3,
(c) Àôx = 2xy + x3,
Àôy = ‚àíx2 + y ‚àíy2 + y3,
(d) Àôx = x,
Àôy = 1 + x + y2,
(e) Àôx = 1 ‚àíx3 + y2,
Àôy = 2xy,
(f) Àôx = y + x2,
Àôy = ‚àíx ‚àíy + x2 + y2.
(Hint: For (f), use Dulac‚Äôs extension to Bendixson‚Äôs negative criterion with
auxiliary function œÅ(x, y) = eax+by.)
9.14
Write each of the following systems in terms of polar coordinates (r, Œ∏), and
use the Poincar¬¥e‚ÄìBendixson theorem to show that at least one limit cycle
solution exists.
(a) Àôx = 2x + 2y ‚àíx(2x2 + y2),
Àôy = ‚àí2x + 2y ‚àíy(2x2 + y2),
(b) Àôx = x ‚àíy ‚àíx(x2 + 3
2y2),
Àôy = x + y ‚àíy(x2 + 1
2y2).
9.15
(a) Write the system
dx
dt = x ‚àíy ‚àí(2x2 + y2)x,
dy
dt = x + y ‚àí(x2 + 2y2)y
in terms of polar coordinates, and then use the Poincar¬¥e‚ÄìBendixson
theorem to show that there is at least one limit cycle solution.
(b) Use Dulac‚Äôs extension to Bendixson‚Äôs negative criterion (with an
auxiliary function of the form eax+by for some suitable constants a
and b) to show that there is no limit cycle solution of the system
with
dx
dt = y,
dy
dt = ‚àíx ‚àíy + x2 + y2.
9.16
(a) Write the system of ordinary diÔ¨Äerential equations
dx
dt = x ‚àíy ‚àíy3 ‚àí2x5 ‚àí2x3y2 ‚àíxy4,
dy
dt = x + y + xy2 ‚àí2yx4 ‚àí2y3x2 ‚àíy5
in terms of polar coordinates (r, Œ∏), and use the Poincar¬¥e‚ÄìBendixson
theorem to show that at least one limit cycle solution exists.
(b) Write the system of ordinary diÔ¨Äerential equations
dx
dt = xy ‚àíx2y + y3,
dy
dt = y2 + x3 ‚àíxy2
in terms of polar coordinates, (r, Œ∏).
Show that there is a single
equilibrium point at the origin and that it is nonhyperbolic. Show
that the lines Œ∏ = ¬±œÄ/4 and Œ∏ = ¬±3œÄ/4 are integral paths. Show
that dr/dt = 0 when Œ∏ = 0 or œÄ. Sketch the phase portrait.

EXERCISES
255
9.17
Consider the second order system
dp
ds = ‚àípq,
dq
ds = p + q ‚àí2,
which arises in a thermal ignition problem (see Section 12.2.3). Show that
there are just two Ô¨Ånite equilibrium points, one of which is a saddle. Con-
sider S, the stable separatrix of the saddle that lies in p > 0. Show that this
separatrix asymptotes to the other equilibrium point as s ‚Üí‚àí‚àû. (Hint:
First show that S must meet the p-axis with p > 2. Next show that S
must go on to meet the p-axis with p < 2. Finally, use the coordinate axes
and S up to its second intersection with the p-axis to construct a trapping
region for S.)
9.18
Project A particle of mass m moves under the action of an attractive
central force of magnitude Œ≥m/rŒ±, where (r, Œ∏) are polar coordinates and
Œ≥ and Œ± are positive constants. By using Newton‚Äôs second law of motion
in polar form, show that u = 1/r satisÔ¨Åes the equation
d 2u
dŒ∏2 + u = Œ≥
h2 uŒ±‚àí2,
(E9.1)
where h is the angular momentum of the particle.
(a) Find the equilibrium points in the (u, du/dŒ∏)-phase plane, and clas-
sify them. What feature of the linear approximation will carry over
to the solutions of the full, nonlinear system?
(b) If the particle moves at relativistic speeds, it can be shown that
(E9.1) is modiÔ¨Åed, in the case of the inverse square law, Œ± = 2,
appropriate to a gravitational attraction, to
d 2u
dŒ∏2 + u = Œ≥
h2 + œµu2,
(E9.2)
where œµ is a small positive constant, and the term œµu2 is called Ein-
stein‚Äôs correction. Find the equilibrium point that corresponds to
a small perturbation of the Newtonian case (œµ = 0), and show that
it is a centre.
(c) Use MATLAB to solve (E9.2) numerically, and hence draw the phase
portrait for the values of œµ, Œ≥ and h appropriate to each of the
three planets nearest to the Sun (you‚Äôll need to Ô¨Ånd an appropriate
astronomy book in your library), and relate what you obtain to part
(b) above.

CHAPTER TEN
Group Theoretical Methods
In this chapter we will develop an approach to the solution of diÔ¨Äerential equations
based on Ô¨Ånding a group invariant. In order to introduce the general idea, let‚Äôs
begin by considering some simple, Ô¨Årst order ordinary diÔ¨Äerential equations.
The solution of the separable equation,
dy
dx = f(x)g(y),
is

dy
g(y) =

f(x)dx + constant.
Another simple class of equations, often referred to as exact equations, takes the
form
‚àÇœÜ
‚àÇx + ‚àÇœÜ
‚àÇy
dy
dx = 0.
In order to stress the equal role played by the independent variables in this equation,
we will usually write
‚àÇœÜ
‚àÇxdx + ‚àÇœÜ
‚àÇy dy = 0.
This has the solution œÜ(x, y) = constant.
Let‚Äôs now consider the equation
dy
dx = f
y
x

.
In general, this is neither separable nor exact, and we are stuck unless we can use
some other property of the equation. An inspection reveals that the substitution
x = ŒªÀÜx, y = ŒªÀÜy, where Œª is any real constant, leaves the form of the equation
unchanged, since
dÀÜy
dÀÜx = f
 ÀÜy
ÀÜx

.
We say that the equation is invariant under the transformation x ‚ÜíŒªÀÜx, y ‚ÜíŒªÀÜy.
The quantity y/x ‚ÜíÀÜy/ÀÜx is also invariant under the transformation. If we use this
invariant quantity, v = y/x, as a new dependent variable, the equation becomes
v + xdv
dx = f(v),

10.1 LIE GROUPS
257
which is separable, with solution

dv
f(v) ‚àív =
 dx
x + constant.
If we regard the parameter Œª as a continuous variable, the set of transformations
x ‚ÜíŒªÀÜx, y ‚ÜíŒªÀÜy forms a group under composition of transformations.
More
speciÔ¨Åcally, this is an example of a Lie group, named after the mathematician
Sophus Lie. We will discuss exactly what we mean by a group and a Lie group
below.
It is the invariance of the diÔ¨Äerential equation under the action of this
group that allows us to Ô¨Ånd a solution in closed form.
In the following sections, we begin by developing as much of the theory of Lie
groups as we will need, and then show how this can be used as a practical tool for
the solution of diÔ¨Äerential equations.
10.1
Lie Groups
Let D be a subset of R2 on which x ‚Üíx1 = f(x, y; œµ), y ‚Üíy1 = g(x, y; œµ) is
a well-deÔ¨Åned transformation from D into R2.
We also assume that x1 and y1
vary continuously with the parameter œµ.
This set of transformations forms a
one-parameter group, or Lie group, if
(i) the transformation with œµ = 0 is the identity transformation, so that
f(x, y; 0) = x,
g(x, y; 0) = y,
(ii) the transformation with ‚àíœµ gives the inverse transformation, so that, if x1 =
f(x, y; œµ) and y1 = g(x, y; œµ), then x = f(x1, y1; ‚àíœµ) and y = g(x1, y1; ‚àíœµ),
(iii) the composition of two transformations is also a member of the set of trans-
formations, so that if x1 = f(x, y; œµ), y1 = g(x, y; œµ), x2 = f(x1, y1; Œ¥) and
y2 = g(x1, y1; Œ¥), then x2 = f(x, y; œµ + Œ¥) and y2 = g(x, y; œµ + Œ¥).
Some simple one-parameter groups are:
(a) Horizontal translation, H(œµ):
x1 = x + œµ,
y1 = y,
(b) Vertical translation, V(œµ):
x1 = x,
y1 = y + œµ,
(c) MagniÔ¨Åcation, M(œµ):
x1 = eœµx,
y1 = eœµy,
(d) Rotation, R(œµ):
x1 = x cos œµ ‚àíy sin œµ,
y1 = x sin œµ + y cos œµ.
For example, to show that the set of transformations M(œµ) forms a group, Ô¨Årstly
note that when œµ = 0, x1 = x and y1 = y.
Secondly, a simple rearrangement
gives x = e‚àíœµx1, y = e‚àíœµy1, so that the inverse transformation is given by M(‚àíœµ).
Finally, if x2 = eŒ¥x1 and y2 = eŒ¥y1 then x2 = eŒ¥.eœµx = eŒ¥+œµx, and similarly with
y2.

258
GROUP THEORETICAL METHODS
10.1.1
The InÔ¨Ånitesimal Transformation
By deÔ¨Åning a Lie group via the transformation x1 = f(x, y; œµ), y1 = g(x, y; œµ),
we are giving the Ô¨Ånite form of the group. Consider what happens when œµ ‚â™1.
Since œµ = 0 gives the identity transformation, we can Taylor expand to obtain
x1 = x + œµ
dx1
dœµ

œµ=0
+ ¬∑ ¬∑ ¬∑ ,
y1 = y + œµ
dy1
dœµ

œµ=0
+ ¬∑ ¬∑ ¬∑ .
If we now introduce the functions
Œæ(x, y) =
dx1
dœµ

œµ=0
,
Œ∑(x, y) =
dy1
dœµ

œµ=0
,
(10.1)
and just retain the Ô¨Årst two terms in the Taylor series expansions, we obtain x1 =
x + œµŒæ(x, y), y1 = y + œµŒ∑(x, y). This is called the inÔ¨Ånitesimal form of the group.
We will show later that every one-parameter group is associated with a unique
inÔ¨Ånitesimal group.
For example, the transformation x1 = x cos œµ ‚àíy sin œµ, y1 = x sin œµ + y cos œµ
forms the rotation group R(œµ). When œµ = 0 this gives the identity transformation.
Using the approximations cos œµ = 1 + ¬∑ ¬∑ ¬∑ , sin œµ = œµ + ¬∑ ¬∑ ¬∑ for œµ ‚â™1, we obtain the
inÔ¨Ånitesimal rotation group as x1 ‚àºx‚àíœµy, y1 ‚àºy+œµx, and hence Œæ(x, y) = ‚àíy and
Œ∑(x, y) = x. The transformation x1 = eœµx, y1 = eœµy forms the magniÔ¨Åcation group
M(œµ). Using eœµ = 1 + œµ + ¬∑ ¬∑ ¬∑ for œµ ‚â™1, we obtain the inÔ¨Ånitesimal magniÔ¨Åcation
group as x1 ‚àº(1 + œµ)x, y1 ‚àº(1 + œµ)y, so that Œæ(x, y) = x and Œ∑(x, y) = y.
We will now show that every inÔ¨Ånitesimal transformation group is similar, or
isomorphic, to a translation group. This means that, by using a change of vari-
ables, we can make any inÔ¨Ånitesimal transformation group look like H(œµ) or V(œµ),
which we deÔ¨Åned earlier. Consider the equations that deÔ¨Åne Œæ and Œ∑ and write
them in the form
dx1
dœµ = Œæ(x1, y1),
dy1
dœµ = Œ∑(x1, y1),
a result that is correct at leading order by virtue of the inÔ¨Ånitesimal nature of the
transformation, and which we shall soon see is exact. We can also write this in the
form
dx1
Œæ
= dy1
Œ∑
= dœµ.
Integration of this gives solutions that are, in principle, expressible in the form
F1(x1, y1) = C1 and F2(x1, y1) = C2 + œµ for some constants C1 and C2. Since
œµ = 0 corresponds to the identity transformation, we can deduce that F1(x1, y1) =
F1(x, y) and F2(x1, y1) = F2(x, y) + œµ. This means that if we deÔ¨Åne u = F1(x, y)
and v = F2(x, y) as new variables, then the group can be represented by u1 = u
and v1 = v + œµ, so that the original group is isomorphic to the translation group
V(œµ).

10.1 LIE GROUPS
259
10.1.2
InÔ¨Ånitesimal Generators and the Lie Series
Consider the change, Œ¥œÜ, that occurs in a given smooth function œÜ(x, y) under
an inÔ¨Ånitesimal transformation. We Ô¨Ånd that
Œ¥œÜ = œÜ(x1, y1) ‚àíœÜ(x, y) = œÜ(x + œµŒæ, y + œµŒ∑) ‚àíœÜ(x, y) = œµ

Œæ ‚àÇœÜ
‚àÇx + Œ∑ ‚àÇœÜ
‚àÇy

+ ¬∑ ¬∑ ¬∑ .
If we retain just this single term, which is consistent with the way we derived the
inÔ¨Ånitesimal transformation, we can see that Œ¥œÜ can be written in terms of the
quantity
UœÜ = Œæ ‚àÇœÜ
‚àÇx + Œ∑ ‚àÇœÜ
‚àÇy ,
(10.2)
or, in operator notation,
U ‚â°Œæ ‚àÇ
‚àÇx + Œ∑ ‚àÇ
‚àÇy .
This is called the inÔ¨Ånitesimal generator of the group. Any inÔ¨Ånitesimal trans-
formation is completely speciÔ¨Åed by UœÜ. For example, if
UœÜ = ‚àíy ‚àÇœÜ
‚àÇx + x‚àÇœÜ
‚àÇy ,
Œæ(x, y) = ‚àíy and Œ∑(x, y) = x, so that the transformation is given by x1 = x ‚àíœµy,
y1 = y + œµx. From the deÔ¨Ånition (10.2), Ux = Œæ and Uy = Œ∑, so that
UœÜ = Ux ‚àÇœÜ
‚àÇx + Uy ‚àÇœÜ
‚àÇy ,
and if a group acts on (x, y) to produce new values (x1, y1) then
UœÜ(x1, y1) = Ux1
‚àÇœÜ
‚àÇx1
+ Uy1
‚àÇœÜ
‚àÇy1
.
Let‚Äôs now consider a group deÔ¨Åned in Ô¨Ånite form by x1 = f(x, y; œµ), y1 = g(x, y; œµ)
and a function œÜ = œÜ(x, y). If we regard œÜ(x1, y1; œµ) as a function of œµ, with a prime
denoting d/dœµ, we Ô¨Ånd that
œÜ(x1, y1; œµ) = œÜ(x1, y1; 0) + œµœÜ‚Ä≤(x1, y1; 0) + 1
2œµ2œÜ‚Ä≤‚Ä≤(x1, y1; 0) + ¬∑ ¬∑ ¬∑ .
Since
œÜ(x1, y1; 0) = œÜ(x, y),
œÜ‚Ä≤(x1, y1; 0) = dœÜ
dœµ

œµ=0
=
 ‚àÇœÜ
‚àÇx1
dx1
dœµ + ‚àÇœÜ
‚àÇy1
dy1
dœµ

œµ=0
=

Œæ ‚àÇœÜ
‚àÇx1

œµ=0
+ Œ∑ ‚àÇœÜ
‚àÇy1

œµ=0

= Œæ ‚àÇœÜ
‚àÇx + Œ∑ ‚àÇœÜ
‚àÇy = UœÜ,
and
œÜ‚Ä≤‚Ä≤(x1, y1; 0) = d
dœµ
 dœÜ
dœµ

œµ=0
= U 2œÜ,

260
GROUP THEORETICAL METHODS
we have
œÜ(x1, y1; œµ) = œÜ(x, y, 0) + œµUœÜ + 1
2œµ2U 2œÜ + ¬∑ ¬∑ ¬∑ .
This is known as a Lie series, and can be written more compactly in operator
form as
œÜ(x1, y1; œµ) = eœµUœÜ(x, y).
In particular, if we take œÜ(x, y, 0) = x,
x1 = x + œµUx + 1
2œµ2U 2x + ¬∑ ¬∑ ¬∑ = x + œµŒæ + 1
2œµ2UŒæ + ¬∑ ¬∑ ¬∑
= x + œµŒæ + 1
2œµ2

Œæ ‚àÇŒæ
‚àÇx + Œ∑ ‚àÇŒæ
‚àÇy

+ ¬∑ ¬∑ ¬∑ .
Similarly,
y1 = y + œµŒ∑ + 1
2œµ2

Œæ ‚àÇŒ∑
‚àÇx + Œ∑ ‚àÇŒ∑
‚àÇy

+ ¬∑ ¬∑ ¬∑ .
These two relations are a representation of the group in Ô¨Ånite form. It should now be
clear that we can calculate the Ô¨Ånite form of the group from the inÔ¨Ånitesimal group
(via the Lie series) and the inÔ¨Ånitesimal group from the Ô¨Ånite form of the group
(via expansions for small œµ). For example, if an inÔ¨Ånitesimal group is represented
by
UœÜ = x‚àÇœÜ
‚àÇx + y ‚àÇœÜ
‚àÇy ,
then
x1 = x + œµx + 1
2!œµ2x + 1
3!œµ3x + ¬∑ ¬∑ ¬∑ = xeœµ,
y1 = y + œµy + 1
2!œµ2y + 1
3!œµ3y + ¬∑ ¬∑ ¬∑ = yeœµ.
The Ô¨Ånite form is therefore M(œµ), the magniÔ¨Åcation group.
As a further example, if an inÔ¨Ånitesimal group is represented by
UœÜ = ‚àíy ‚àÇœÜ
‚àÇx + x‚àÇœÜ
‚àÇy ,
then
Ux = ‚àíy,
Uy = x,
U 2x = ‚àíx,
U 2y = ‚àíy,
U 3x = y,
U 3y = ‚àíx,
U 4x = x,
U 4y = y.
U is therefore a cyclic operation with period 4 and the equations of the Ô¨Ånite form
of the group are
x1 = x ‚àíœµy ‚àí1
2!œµ2x + 1
3!œµ3y + 1
4!œµ4x + ¬∑ ¬∑ ¬∑

10.2 INVARIANTS UNDER GROUP ACTION
261
= x

1 ‚àí1
2!œµ2 + 1
4!œµ4 ‚àí¬∑ ¬∑ ¬∑

‚àíy

œµ ‚àí1
3!œµ3 + ¬∑ ¬∑ ¬∑

= x cos œµ ‚àíy sin œµ.
Similarly,
y1 = y + œµx ‚àí1
2!œµ3y ‚àí1
3!œµ3x + 1
4!œµ4y + ¬∑ ¬∑ ¬∑
= y

1 ‚àí1
2!œµ2 + 1
4!œµ4 ‚àí¬∑ ¬∑ ¬∑

+ x

œµ ‚àí1
3!œµ3 + ¬∑ ¬∑ ¬∑

= y cos œµ + x sin œµ,
and we have the rotation group R(œµ).
There is a rather more concise way of doing this, using the fact that
dx1
dœµ = Œæ(x1, y1),
dy1
dœµ = Œ∑(x1, y1),
subject to x1 = x and y1 = y at œµ = 0
is an exact relationship according to (10.1). For the Ô¨Årst example above, this gives
dx1/dœµ = x1 and dy1/dœµ = y1, with x = x1 and y = y1 at œµ = 0. This Ô¨Årst order
system can readily be integrated to give x1 = xeœµ and y1 = yeœµ.
10.2
Invariants Under Group Action
Let x1 = f(x, y; œµ), y1 = g(x, y; œµ) be the Ô¨Ånite form of a group and let the inÔ¨Ånites-
imal transformation associated with the group have inÔ¨Ånitesimal generator
UœÜ = Œæ ‚àÇœÜ
‚àÇx + Œ∑ ‚àÇœÜ
‚àÇy .
A function ‚Ñ¶(x, y) is said to be invariant under the action of this group if, when x1
and y1 are derived from x and y by the operations of the group, ‚Ñ¶(x1, y1) = ‚Ñ¶(x, y).
Using the Lie series, we can write
‚Ñ¶(x1, y1) = ‚Ñ¶(x, y) + œµU‚Ñ¶+ 1
2!œµ2U 2‚Ñ¶+ ¬∑ ¬∑ ¬∑ = ‚Ñ¶(x, y) + œµU‚Ñ¶+ 1
2!œµ2U(U‚Ñ¶) + ¬∑ ¬∑ ¬∑ ,
so that a necessary and suÔ¨Écient condition for invariance is U‚Ñ¶= 0, and hence
that
Œæ ‚àÇ‚Ñ¶
‚àÇx + Œ∑ ‚àÇ‚Ñ¶
‚àÇy = 0.
This is a partial diÔ¨Äerential equation for ‚Ñ¶, whose solution is ‚Ñ¶(x, y) = C, a con-
stant, on the curve
dx
Œæ = dy
Œ∑ .
(10.3)
Since this equation has only one solution, it follows that a one-parameter group has
only one invariant.
Now let‚Äôs take a point (x0, y0) and apply the inÔ¨Ånitesimal transformation to it,
so that it is mapped to (x0 + œµŒæ, y0 + œµŒ∑). If we repeat this procedure inÔ¨Ånitely
often, we can obtain a curve that is an integral of the diÔ¨Äerential system given by
(10.3). By varying the initial point (x0, y0) we then obtain a family of curves, all
of which are solutions of (10.3), which we denote by ‚Ñ¶f(x, y) = C. This family of

262
GROUP THEORETICAL METHODS
curves is invariant under the action of the group in the sense that each curve in
the family is transformed into another curve of the same family under the action
of the group. Suppose (x, y) becomes (x1, y1) under the action of the group. This
means that ‚Ñ¶f(x1, y1) = constant must represent the same family of curves. Using
the Lie series we can write
‚Ñ¶f(x1, y1) = ‚Ñ¶f(x, y) + œµU‚Ñ¶f + 1
2!œµ2U 2‚Ñ¶f + ¬∑ ¬∑ ¬∑ .
The most general condition that forces the Ô¨Årst two terms to be constant is that
U‚Ñ¶f = constant should represent one family of curves, because
U n‚Ñ¶f = U n‚àí1 (U‚Ñ¶f) = U n‚àí1 (constant) = 0.
This is conveniently written as U‚Ñ¶f = F(‚Ñ¶f) for some arbitrary nonzero function
F.
For example, the rotation group is represented in inÔ¨Ånitesimal form by
UœÜ = ‚àíy ‚àÇœÜ
‚àÇx + x‚àÇœÜ
‚àÇy .
The equation for the invariants of this group is
‚àídx
y = dy
x ,
which can be easily integrated to give ‚Ñ¶= x2 + y2 = C. This gives the intuitively
obvious result that circles are invariant under rotation!
10.3
The Extended Group
If x1 = f(x, y; œµ) and y1 = g(x, y; œµ) form a group of transformations in the usual
way, we can extend the group by regarding the diÔ¨Äerential coeÔ¨Écient p = dy/dx
as a third independent variable. Under the transformation this becomes
p1 = dy1
dx1
= gx + pgy
fx + pfy
= h(x, y, p ; œµ).
It can easily be veriÔ¨Åed that the triple given by (x, y, p) forms a group under the
transformations above. This is known as the extended group of the given group.
This extended group also has an inÔ¨Ånitesimal form associated with it. If we write
x1 = x + œµŒæ(x, y), y1 = y + œµŒ∑(x, y), then
p1 = œµŒ∑x + p(1 + œµŒ∑y)
1 + œµŒæx + pœµŒæy
.
Expanding this using the binomial theorem for small œµ, we Ô¨Ånd that
p1 = p + œµ

Œ∑x + (Œ∑y ‚àíŒæx) p ‚àíŒæyp2
= p + œµŒ∂.
(10.4)
The inÔ¨Ånitesimal generator associated with this three-element group is
U ‚Ä≤œÜ = ŒæœÜx + Œ∑œÜy + Œ∂œÜp.

10.4 INTEGRATION OF A FIRST ORDER EQUATION
263
It is of course possible, though algebraically more messy, to form further extensions
of a given group by including second and higher derivatives.
We are now at the stage where we can use the theory that we have developed
in this chapter to solve Ô¨Årst order diÔ¨Äerential equations. Prior to this, it is helpful
to outline two generic situations that may occur when you are confronted with a
diÔ¨Äerential equation that needs to be solved.
(i) It is straightforward to spot the group of transformations under which the
equation is invariant. In this case we can give a recipe for using this invari-
ance to solve the equation.
(ii) No obvious group of transformations can be spotted and we need a more sys-
tematic approach to construct the group. This is called Lie‚Äôs fundamental
problem and is considerably more diÔ¨Écult than (i). Indeed, no general so-
lution is known to Lie‚Äôs fundamental problem for Ô¨Årst order equations.
10.4
Integration of a First Order Equation with a Known Group
Invariant
To show more explicitly that this group invariance property will lead to a more
tractable diÔ¨Äerential equation than the original, let‚Äôs consider a general Ô¨Årst order
ordinary diÔ¨Äerential equation, F(x, y, p) = 0, that is invariant under the extended
group
U ‚Ä≤œÜ = Œæ ‚àÇœÜ
‚àÇx + Œ∑ ‚àÇœÜ
‚àÇy + Œ∂ ‚àÇœÜ
‚àÇp
derived from
UœÜ = Œæ ‚àÇœÜ
‚àÇx + Œ∑ ‚àÇœÜ
‚àÇy .
We have seen that a suÔ¨Écient condition for the invariance property is that U ‚Ä≤œÜ = 0,
so we are faced with solving
Œæ ‚àÇœÜ
‚àÇx + Œ∑ ‚àÇœÜ
‚àÇy + Œ∂ ‚àÇœÜ
‚àÇp = 0.
The solution curves of this partial diÔ¨Äerential equation, where œÜ is constant, are
the two independent solutions of the simultaneous system
dx
Œæ = dy
Œ∑ = dp
Œ∂ .
Let u(x, y) = Œ± be a solution of
dx
Œæ = dy
Œ∑ ,
and v(p, x, y) = Œ≤ be the other independent solution.
We now show that if we know U, Ô¨Ånding v is simply a matter of integration. To
do this, recall the earlier result that any group with one parameter is similar to the

264
GROUP THEORETICAL METHODS
translation group. Let the change of variables from (x, y) to (x1, y1) reduce UœÜ to
the group of translations parallel to the y1 axis, and call the inÔ¨Ånitesimal generator
of this group U1f. Then
U1f = Ux1
‚àÇf
‚àÇx1
+ Uy1
‚àÇf
‚àÇy1
= ‚àÇf
‚àÇy1
,
from which we see that Ux1 = 0 and Uy1 = 1, or more explicitly,
Œæ ‚àÇx1
‚àÇx + Œ∑ ‚àÇx1
‚àÇy = 0,
Œæ ‚àÇy1
‚àÇx + Œ∑ ‚àÇy1
‚àÇy = 1.
The Ô¨Årst of these equations has the solution x1 = u(x, y) and the second is equiva-
lent to the simultaneous system
dx
Œæ = dy
Œ∑ = dy1
1 .
Again, one solution of this system is u(x, y) = Œ±. This can be used to eliminate x
from the second independent solution, given by
dy1
dy =
1
Œ∑(x, y),
so that by a simple integration we can obtain y1 as a function of x and y. As the
extended group of translations, U ‚Ä≤
1f, is identical to U1f, the most general diÔ¨Äerential
equation invariant under U ‚Ä≤
1 in the new x1, y1 variables will therefore be a solution
of the simultaneous system
dx1
0
= dy1
1
= dp1
0 .
This particularly simple system has solutions x1 = constant and p1 = constant, so
that the diÔ¨Äerential equation can be put in the form p1 = F(x1) for some calculable
function F. In principle, it is straightforward to solve equations of this form, as
they are separable. The solution of the original equation can then be obtained by
returning to the (x, y) variables.
Example
The diÔ¨Äerential equation
dy
dx =
1
#
x + y2
is invariant under the transformation x = e‚àí2œµx1, y = e‚àíœµy1. The inÔ¨Ånitesimal
transformation associated with this is x1 = x+2œµx, y1 = y+œµy, so that Œæ(x, y) = 2x
and Œ∑(x, y) = y. If we solve the system
dx
Œæ = dy
Œ∑ ,
we Ô¨Ånd that y/x1/2 = eC, so that x1 = y/x1/2. Solving
dy1
dy = 1
Œ∑ = 1
y

10.5 DETERMINING GROUPS UNDER WHICH AN EQUATION IS INVARIANT
265
gives y1 = log y. Some simple calculus then shows that the original diÔ¨Äerential
equation transforms to
1
2x3
1
dy1
dx1
x1
dy1
dx1
‚àí1
=
x1
#
x2
1 + 1
,
which, on rearranging, gives
dy1
dx1
=
1
x1

1 ‚àí1
2x1
#
x2
1 + 1
.
This is the separable equation that we are promised by the theory we have de-
veloped. The Ô¨Ånal integration of the equation can be achieved by the successive
substitutions z = log x1 and t2 = 1 + e‚àí2z.
10.5
Towards the Systematic Determination of Groups Under Which
a First Order Equation is Invariant
If we consider a diÔ¨Äerential equation in the form
dy
dx = F(x, y)
and an inÔ¨Ånitesimal transformation of the form x1 ‚àºx+œµŒæ(x, y), y1 ‚àºy +œµŒ∑(x, y),
(10.4) shows that
dy1
dx1
= dy
dx + œµ

Œ∑x + (Œ∑y ‚àíŒæx) dy
dx ‚àíŒæy
dy
dx
2
+ ¬∑ ¬∑ ¬∑ .
Using the diÔ¨Äerential equation to eliminate dy/dx, we Ô¨Ånd that the equation will
be invariant under the action of the group provided that
Œæ ‚àÇF
‚àÇx + Œ∑ ‚àÇF
‚àÇy = Œ∑x + (Œ∑y ‚àíŒæx) F ‚àíŒæyF 2.
(10.5)
So, given the function F, the fundamental problem is to determine two functions
Œæ and Œ∑ that satisfy this Ô¨Årst order partial diÔ¨Äerential equation. Of course, this is
an underdetermined problem and has no unique solution. However, by choosing a
special form for either Œæ or Œ∑ there are occasions when the process works, as the
following example shows.
Example
Consider the equation
dy
dx =
y
x + x2 + y2 .
(10.6)
In this case,
‚àÇF
‚àÇx =
‚àí(1 + 2x) y
(x + x2 + y2)2 ,
‚àÇF
‚àÇy =
x + x2 ‚àíy2
(x + x2 + y2)2 ,

266
GROUP THEORETICAL METHODS
and (10.5) takes the form
(x + x2 ‚àíy2)Œ∑ ‚àí(1 + 2x)yŒæ = (x + x2 + y2)2Œ∑x + (Œ∑x ‚àíŒæx)y(x + x2 + y2) ‚àíy2Œæy.
If now we choose Œ∑ = 1, this reduces to
(x + x2 ‚àíy2) + (1 + 2x)yŒæ = y(x + x2 + y2)Œæx + y2Œæy.
It is not easy to solve even this equation in general, but after some trial and error,
we can Ô¨Ånd the solution Œæ = x/y. The inÔ¨Ånitesimal transformation in this case is
x1 = x + œµx/y, y1 = y + œµ and, following the procedure outlined in the last section,
we now solve
dx
x/y = dy
1
to obtain y/x = constant. We therefore take x1 = y/x and hence y1 = y as our
new variables. In terms of these variables, (10.6) becomes
dy1
dx1
= ‚àí
1
(1 + x2
1),
with solution y1 = ‚àítan‚àí1 x1+C. The solution of our original diÔ¨Äerential equation
can therefore be written in the form y + tan‚àí1 (y/x) = C.
10.6
Invariants for Second Order DiÔ¨Äerential Equations
First order diÔ¨Äerential equations can be invariant under an inÔ¨Ånite number of one-
parameter groups. Second order diÔ¨Äerential equations can only be invariant under
at most eight groups. To see where this Ô¨Ågure comes from, let‚Äôs consider the simplest
form of a variable coeÔ¨Écient, linear, second order, diÔ¨Äerential equation,
d 2y
dx2 + q(x)y = 0.
(10.7)
Writing x1 = x + œµŒæ and y1 = y + œµŒ∑ we have already shown that
dy1
dx1
= dy
dx + œµ

Œ∑x + (Œ∑y ‚àíŒæx) dy
dx ‚àíŒæy
dy
dx
2
= dy
dx + œµŒ†

x, y, dy
dx

= p + œµŒ† (x, y, p) .
Now
d 2y1
dx2
1
=
d
dx1
 dy1
dx1

= d
dx
 dy1
dx1
 dx
dx1
and
d
dx
 dy1
dx1

= dp
dx + œµ (Œ†x + Œ†yp + Œ†ppx) + ¬∑ ¬∑ ¬∑
= 1
5dx1
dx =
1
1 + œµ (Œæx + Œæyp) + ¬∑ ¬∑ ¬∑ ,

10.6 INVARIANTS FOR SECOND ORDER DIFFERENTIAL EQUATIONS
267
so that
d 2y1
dx2
1
= d 2y
dx2 + œµ

Œ†x + Œ†y
dy
dx + Œ†p
d 2y
dx2 ‚àíd 2y
dx2

Œæx + Œæy
dy
dx
	
+ ¬∑ ¬∑ ¬∑ .
The condition for invariance is therefore
Œ†x + Œ†y
dy
dx + d 2y
dx2 Œ†p ‚àíd 2y
dx2

Œæx + Œæy
dy
dx

+ Œæq‚Ä≤(x1)y1 + Œ∑q(x1) = 0.
Some simple calculation leads to
{Œ∑xx + (2Œæx ‚àíŒ∑y) q(x)y + Œæq‚Ä≤(x)y + q(x)Œ∑} + {2Œ∑xy ‚àíŒæxx + 3q(x)yŒæy} dy
dx
+ {Œ∑yy ‚àí2Œæxy}
dy
dx
2
‚àíŒæyy
dy
dx
3
= 0.
If we now set the coeÔ¨Écients of powers of dy/dx to zero, we obtain
Œæyy = 0,
Œ∑yy ‚àí2Œæxy = 0,
2Œ∑xy ‚àíŒæxx + 3q(x)yŒæy = 0,
(10.8)
Œ∑xx + (2Œæx ‚àíŒ∑y)q(x)y + Œæq‚Ä≤(x)y + q(x)Œ∑ = 0.
(10.9)
Equation (10.8)1 can be integrated to give
Œæ = œÅ(x)y + Œæ(x).
Substitution of this into (10.8)2 gives Œ∑yy = 2œÅ‚Ä≤(x), which can be integrated to give
Œ∑ = œÅ‚Ä≤(x)y2 + ÀúŒ∑(x)y + Œ∂(x).
From (10.8)3,
3œÅ(x)q(x)y + 3œÅ‚Ä≤‚Ä≤(x)y + 2ÀúŒ∑‚Ä≤(x) ‚àíŒæ‚Ä≤‚Ä≤(x) = 0.
(10.10)
Finally, (10.9) gives

œÅ‚Ä≤‚Ä≤‚Ä≤(x)y2 + ÀúŒ∑‚Ä≤‚Ä≤(x)y + Œ∂‚Ä≤‚Ä≤(x)

‚àíq(x)y {ÀúŒ∑(x) ‚àí2Œæ‚Ä≤(x)} + {œÅ(x)y + Œæ(x)} q‚Ä≤(x)y
+

œÅ‚Ä≤(x)y2 + ÀúŒ∑(x)y + Œ∂(x)

q(x) = 0.
(10.11)
We can Ô¨Ånd a solution of (10.10) and (10.11) by noting that the coeÔ¨Écient of each
power of y must be zero, which gives us four independent equations,
Œ∂‚Ä≤‚Ä≤(x) + q(x)Œ∂(x) = 0,
œÅ‚Ä≤‚Ä≤(x) + q(x)œÅ(x) = 0,
ÀúŒ∑‚Ä≤‚Ä≤(x) ‚àíq(x) (ÀúŒ∑(x) ‚àí2Œæ‚Ä≤(x)) + Œæ(x)q‚Ä≤(x) + ÀúŒ∑(x)q(x) = 0,
Œæ‚Ä≤‚Ä≤(x) = 2ÀúŒ∑‚Ä≤(x).
At this stage notice that œÅ(x) and Œ∂(x) satisfy the original second order equation,
(10.7), the solution of which will involve four constants.
There are also second
order equations for Œæ(x) and ÀúŒ∑(x), the solution of which gives rise to a further four
constants. Each of these constants will generate a one-parameter group that leaves
the original equation invariant, so the original equation is invariant under at most

268
GROUP THEORETICAL METHODS
eight one-parameter groups. Some of these groups are obvious. For example, since
(10.7) is invariant under magniÔ¨Åcation in y, it must be invariant under x1 = x,
y1 = eœµy.
Let‚Äôs consider the group generated by œÅ(x) in more detail. In the usual way, we
have to integrate
dx
Œæ(x, y) =
dy
Œ∑(x, y),
which leads to the equation
dy
dx = y

œÅ‚Ä≤(x)y + 1
2Œæ‚Ä≤(x) + Œ∂(x)

œÅ(x)y + Œæ(x)
.
This has a solution of the form y = CœÅ(x) provided that Œæ(x) = CœÅ2(x) and Œ∂(x) = 0
for some constant C. Note that ÀúŒ∑(x) = 1
2Œæ‚Ä≤(x) by direct integration. This means
that, following the ideas of the previous section, we should deÔ¨Åne x1 = y/œÅ(x).
Now,
dy1
dy =
1
Œ∑(x, y) =
1
œÅ‚Ä≤(x)y2 + CœÅœÅ‚Ä≤y =
1
œÅ‚Ä≤(x) (y2 + CœÅy),
and, since y = CœÅ(x),
dy1
dx =
1
2CœÅ2 ,
which gives
y1 = 1
2C
 x
x0
dx
œÅ2 = œÅ(x)
2y
 x
x0
dt
œÅ2(t).
We can write the diÔ¨Äerential equation in terms of these new variables by noting
that
x1y1 =
 x
x0
dt
2œÅ2(t),
so that
d
dx(x1y1) =
1
2œÅ2(x).
Now
dx1
d(x1y1) = dx1
dx
dx
d(x1y1) = 2œÅ2
1
œÅ
dy
dx ‚àíœÅ‚Ä≤y
œÅ2
	
= 2

œÅdy
dx ‚àíœÅ‚Ä≤y

,
which gives
d
dx

dx1
d(x1y1)

= 2

œÅd 2y
dx2 + œÅ‚Ä≤ dy
dx ‚àíœÅ‚Ä≤‚Ä≤y ‚àíœÅ‚Ä≤ dy
dx
	
= 2œÅ
d 2y
dx2 + q(x)y
	
= 0.
Integrating this expression gives
dx1
d(x1y1) = C1,

10.6 INVARIANTS FOR SECOND ORDER DIFFERENTIAL EQUATIONS
269
so that x1 = C1x1y1 + C2 or, in terms of the original variables,
y = C1œÅ(x)
 x
x0
dt
œÅ2(t) + C2œÅ(x).
This is just the reduction of order formula, which we derived in Chapter 1. Although
the invariance property produces this via a route that is rather diÔ¨Äerent from that
taken in Chapter 1, it is not particularly useful to us in Ô¨Ånding solutions.
Let‚Äôs now consider the invariance x1 = x and y1 = eœµy, which has Œæ = 0 and
Œ∑ = y. This means that
dx
0 = dy
y ,
and we can see that this suggests using the new variables x1 = x and y1 = log y.
Since y‚Ä≤‚Ä≤ = ey1(y‚Ä≤‚Ä≤
1 +y‚Ä≤2
1 ) under this change of variables, we obtain y‚Ä≤‚Ä≤
1 +y‚Ä≤2
1 +q(x) = 0.
Putting Y1 = y‚Ä≤
1 leads to Y ‚Ä≤
1 + Y 2
1 + q(x) = 0. This is a Ô¨Årst order equation, so
the group invariance property has allowed us to reduce the order of the original
equation. As an example of this, consider the equation
y‚Ä≤‚Ä≤ +
1
4x2 y = 0.
In this case, we obtain
Y ‚Ä≤
1 + Y 2
1 +
1
4x2 = 0.
This is a form of Ricatti‚Äôs equation, which in general is diÔ¨Écult to solve.
The
exception to this is if we can spot a solution, when the equation will linearize. For
this example, we can see that Y1 = 1/2x is a solution. Writing Y1 = 1/2x + V ‚àí1
linearizes the equation and it is straightforward to show that the general solution
is y = C1x1/2 + C2x1/2 log x.
At this stage, you could of course argue that you could have guessed the solu-
tion u1(x) = x1/2 and then reduced the order of the equation to obtain the second
solution u2(x) = x1/2 log x. The counter argument to this is that the group theo-
retical method gives both the technique of reduction of order and an algorithm for
reducing the original equation to a Ô¨Årst order diÔ¨Äerential equation. The point can
perhaps be reinforced by considering the nonautonomous equation
y‚Ä≤‚Ä≤ + 1
xy‚Ä≤ + ey = 0.
Using the methods derived in this chapter, we Ô¨Årst introduce a new dependent
variable, Y , deÔ¨Åned by y = ‚àí2 log x + Y . This gives us
Y ‚Ä≤‚Ä≤ + 1
xY ‚Ä≤ + eY
x2 = 0.
The invariance of this under x-magniÔ¨Åcation suggests introducing z = log x and
leads to
d 2Y
dz2 + eY = 0,

270
GROUP THEORETICAL METHODS
which we can immediately integrate to
1
2
dY
dz
2
= C ‚àíeY .
The analysis of this equation is of course much simpler than the original one.
10.7
Partial DiÔ¨Äerential Equations
The ideas given in this chapter can be considerably extended, particularly to the
area of partial diÔ¨Äerential equations. As a simple example, consider the equation
for the diÔ¨Äusion of heat that we derived in Chapter 2, namely
‚àÇT
‚àÇt = D‚àÇ2T
‚àÇx2 .
(10.12)
This equation is invariant under the group of transformations
x = eœµx1,
t = e2œµt1,
so that x/t1/2 is an invariant of the transformation (can you spot which other groups
it is invariant under?). If we write Œ∑ = x/t1/2 (here Œ∑ is known as a similarity
variable), this reduces the partial diÔ¨Äerential equation to the ordinary diÔ¨Äerential
equation
Dd 2T
dŒ∑2 + 1
2Œ∑ dT
dŒ∑ = 0.
For the initial condition T(x, 0) = H(‚àíx), which is also invariant under the trans-
formation, it is readily established from the ordinary diÔ¨Äerential equation that the
solution is
T(x, t) =
1
‚àöœÄ
 ‚àû
x/
‚àö
4Dt
e‚àís2 ds = 2erfc

x
‚àö
4Dt

,
(10.13)
which is shown in Figure 10.1.
Finally, if the initial condition is T(x, 0) = Œ¥(x), we can use the fact that (10.12)
is also invariant under the two-parameter group
x = eœµx1,
t = e2œµt1,
T = e¬µT1.
The initial condition transforms to e¬µT1(x1, 0) = e‚àíœµŒ¥(x1), since Œ¥(ax) = Œ¥(x)/a,
so that the choice ¬µ = ‚àíœµ makes both diÔ¨Äerential equation and initial condition
invariant. As before, x/t1/2 is an invariant, and now t1/2T is invariant as well. If
we therefore look for a solution of the form T(x, t) = t‚àí1/2F(x/t1/2), we Ô¨Ånd that
DFŒ∑Œ∑ + 1
2Œ∑FŒ∑ + 1
2F = 0.
A suitable solution of this is Ae‚àíŒ∑2/4D (for example, using the method of Frobenius),
and hence
T(x, t) = At‚àí1/2e‚àíx2/4Dt.
(10.14)

10.7 PARTIAL DIFFERENTIAL EQUATIONS
271
Fig. 10.1. The solution, (10.13), of the diÔ¨Äusion equation.
To Ô¨Ånd the constant A, we can integrate (10.12) to obtain the integral conserva-
tion law,
d
dt
 ‚àû
‚àí‚àû
T dx = 0.
Since
 ‚àû
‚àí‚àû
T(x, 0) dx =
 ‚àû
‚àí‚àû
Œ¥(x) dx = 1,
we must have
' ‚àû
‚àí‚àûT(x, t) dx = 1. Then, from (10.14), A = 1/
‚àö
4œÄD, and hence
T(x, t) =
1
‚àö
4œÄDt
e‚àíx2/4Dt.
(10.15)
This is known as the point source solution of the diÔ¨Äusion equation. Notice the
similarity between this solution and the sequence (5.27), which we can use to deÔ¨Åne
the Dirac delta function, as shown in Figure 5.4.
Further details of extensions to the basic method can be found in Bluman and
Cole (1974) and Hydon (2000). We end this chapter with a recommendation. Given
an ordinary or partial diÔ¨Äerential equation, try to Ô¨Ånd a simple invariant, such as
translation, magniÔ¨Åcation or rotation. If you Ô¨Ånd one, this will lead to a reduction
of order for an ordinary diÔ¨Äerential equation, or the transformation of a partial
diÔ¨Äerential equation into an ordinary diÔ¨Äerential equation. If you cannot Ô¨Ånd an
invariant by inspection, then you need to make a systematic search for invariants.

272
GROUP THEORETICAL METHODS
This can involve a lot of calculation, and is best done by a computer algebra package.
Hydon (2000) gives a list and critique of some widely available packages.
Exercises
10.1
Show that each of the diÔ¨Äerential equations
(a) xdy
dx + y = x4
dy
dx
2
,
(b) xdy
dx ‚àíyxm = 0,
(c)
dy
dx
2
= y + x2,
is invariant under a group of transformations with inÔ¨Ånitesimal generator
UœÜ = ax‚àÇœÜ
‚àÇx + by ‚àÇœÜ
‚àÇy ,
and hence integrate each equation.
10.2
Find the general diÔ¨Äerential equation of Ô¨Årst order, invariant under the
groups
(a) UœÜ = ‚àÇœÜ
‚àÇx + x‚àÇœÜ
‚àÇy ,
(b) UœÜ = x‚àÇœÜ
‚àÇx + ay ‚àÇœÜ
‚àÇy ,
(c) UœÜ = y ‚àÇœÜ
‚àÇx + ‚àÇœÜ
‚àÇy .
10.3
If the diÔ¨Äerential equation
dx
P(x, y) =
dy
Q(x, y)
is invariant under a group with inÔ¨Ånitesimal generators Œæ and Œ∑, show that
it has a solution in integrating factor form,
 P dy ‚àíQ dx
PŒ∑ ‚àíQŒæ
= constant.
Hence Ô¨Ånd the solution of
dy
dx = y4 ‚àí2x3y
2xy3 ‚àíx4 .
10.4
Show that the diÔ¨Äerential equation
d 2y
dx2 + p(x)y2 = 0
is invariant under at most six one-parameter groups. For the case p(x) =
xm, reduce the order of the equation.

EXERCISES
273
10.5
Find the most general Ô¨Årst order diÔ¨Äerential equation invariant under the
group with inÔ¨Ånitesimal generator
UœÜ = exp

p(x)dx
	 ‚àÇœÜ
‚àÇy .
10.6
Derive the integrating factor for y‚Ä≤ + P(x)y = Q(x) by group theoretical
methods.
10.7
Find a similarity reduction of the porous medium equation, ut =
(uux)x. Can you Ô¨Ånd any solutions of the resulting diÔ¨Äerential equation?
10.8
The equation ut = uxx ‚àíup, with p ‚ààR+, arises in mathematical models
of tumour growth, and other areas. Find some invariants of this equation,
and the corresponding ordinary diÔ¨Äerential equations. Can the point source
problem be resolved for this sink-like equation?

CHAPTER ELEVEN
Asymptotic Methods: Basic Ideas
The vast majority of diÔ¨Äerential equations that arise as models for real physical
systems cannot be solved directly by analytical methods.
Often, the only way
to proceed is to use a computer to calculate an approximate, numerical solution.
However, if one or more small, dimensionless parameters appear in the diÔ¨Äeren-
tial equation, it may be possible to use an asymptotic method to obtain an
approximate solution. Moreover, the presence of a small parameter often leads to
a singular perturbation problem, which can be diÔ¨Écult, if not impossible, to
solve numerically.
Small, dimensionless parameters usually arise when one physical process occurs
much more slowly than another, or when one geometrical length in the problem
is much shorter than another. Examples occur in many diÔ¨Äerent areas of applied
mathematics, and we will meet several in Chapter 12. As we shall see, dimensionless
parameters arise naturally when we use dimensionless variables, which we discussed
at the beginning of Chapter 5. Some other examples are:
‚Äî Waves on the surface of a body of Ô¨Çuid or an elastic solid, with amplitude a and
wavelength Œª, are said to be of small amplitude if œµ = a/Œª ‚â™1. A simpliÔ¨Åcation
of the governing equations based on the fact that œµ ‚â™1 leads to a system of
linear partial diÔ¨Äerential equations (see, for example, Billingham and King, 2001).
This is an example of a regular perturbation problem, where the problem is
simpliÔ¨Åed throughout the domain of solution.
‚Äî In the high speed Ô¨Çow of a viscous Ô¨Çuid past a Ô¨Çat plate of length L, pressure
changes due to accelerations are much greater than those due to viscous stresses,
as expressed by Re = œÅUL/¬µ ‚â´1. Here, Re is the Reynolds number, a dimen-
sionless parameter that measures the ratio of acceleration to viscous forces, œÅ is
the Ô¨Çuid density, U the Ô¨Çuid velocity away from the plate and ¬µ the Ô¨Çuid vis-
cosity. The solution for Re‚àí1 ‚â™1 has the Ô¨Çuid velocity equal to U everywhere
except for a small neighbourhood of the plate, known as a boundary layer,
where viscosity becomes important (see, for example, Acheson, 1990). This is
an example of a singular perturbation problem, where an apparently negligible
physical eÔ¨Äect, here viscosity, becomes important in a small region.
‚Äî In aerodynamics, it is crucial to be able to calculate the lift force on the cross-
section of a wing due to the Ô¨Çow of an inviscid Ô¨Çuid around it. These cross-
sections are usually long and thin with aspect ratio œµ = l/L ‚â™1, where l is a
typical vertical width, and L a typical horizontal length. Thin aerofoil theory

11.1 ASYMPTOTIC EXPANSIONS
275
exploits the size of œµ to greatly simplify the calculation of the lift force (see, for
example, Milne-Thompson, 1960, and Van Dyke, 1964).
11.1
Asymptotic Expansions
In this chapter, we will study how to approximate various integrals as series ex-
pansions, whilst in Chapter 12, we will look at series solutions of diÔ¨Äerential equa-
tions. These series are asymptotic expansions. The crucial diÔ¨Äerence between
asymptotic expansions and the power series that we met earlier in the book is that
asymptotic expansions need not be convergent in the usual sense. We can illustrate
this using an example, but Ô¨Årstly, there is some useful notation for comparing the
sizes of functions that we will introduce here and use extensively later.
11.1.1
Gauge Functions
(i) If
lim
œµ‚Üí0
f(œµ)
g(œµ) = A,
for some nonzero constant A, we write f(œµ) = O (g (œµ)) for œµ ‚â™1. We say
that f is of order g for small œµ. Here g(œµ) is a gauge function, since it is
used to gauge the size of f(œµ). For example, when œµ ‚â™1,
sin œµ = O(œµ),
cos œµ = O(1),
e‚àíœµ = O(1),
cos œµ ‚àí1 = O(œµ2),
all of which can be found from the Taylor series expansions of these functions.
This notation tells us nothing about the constant A. For example, 1010 =
O(1). The order notation only tells us how functions behave as œµ ‚Üí0. It is
not meant to be used for comparing constants, which are all, by deÔ¨Ånition,
of O(1).
(ii) We also have a notation available that displays more information about the
behaviour of the functions. If
lim
œµ‚Üí0
f(œµ)
g(œµ) = 1,
we write f(œµ) ‚àºg(œµ), and say that f(œµ) is asymptotic to g(œµ) as œµ ‚Üí0. For
example,
sin œµ ‚àºœµ,
1 ‚àícos œµ ‚àº1
2œµ2,
eœµ ‚àí1 ‚àºœµ,
as œµ ‚Üí0.
(iii) If
lim
œµ‚Üí0
f(œµ)
g(œµ) = 0,
we write f(œµ) = o (g (œµ)), and say that f is much less than g. For example
sin œµ = o(1),
cos œµ = o(œµ‚àí1),
e‚àíœµ = o(log œµ)
for œµ ‚â™1.

276
ASYMPTOTIC METHODS: BASIC IDEAS
11.1.2
Example: Series Expansions of the Exponential Integral,
Ei(x)
Let‚Äôs consider the exponential integral
Ei(x) =
 ‚àû
x
e‚àít
t
dt,
(11.1)
for x > 0. We can integrate by parts to obtain
Ei(x) =

‚àíe‚àít
t
‚àû
x
‚àí
 ‚àû
x
e‚àít
t2 dt = e‚àíx
x
‚àí
 ‚àû
x
e‚àít
t2 dt.
Doing this repeatedly leads to
Ei(x) = e‚àíx
N

m=1
(‚àí1)m‚àí1 (m ‚àí1)!
xm
+ RN,
(11.2)
where
RN = (‚àí1)NN!
 ‚àû
x
e‚àít
tN+1 dt.
(11.3)
This result is exact. Now, let‚Äôs consider how big RN is for x ‚â´1. Using the fact
that x‚àí(N+1) > t‚àí(N+1) for t > x,
|RN| = N!
 ‚àû
x
e‚àít
tN+1 dt ‚©Ω
N!
xN+1
 ‚àû
x
e‚àít dt = N!e‚àíx
xN+1 ,
and hence
|RN| = O

e‚àíxx‚àí(N+1)
for x ‚â´1.
Therefore, if we truncate the series expansion (11.2) at a Ô¨Åxed value of N by
neglecting RN, it converges to Ei(x) as x ‚Üí‚àû. This is our Ô¨Årst example of an
asymptotic expansion.
In common with most useful asymptotic expansions,
(11.2) does not converge in the usual sense. The ratio of the (N + 1)th and N th
terms is
(‚àí1)NN!
xN+1
xN
(‚àí1)N‚àí1(N ‚àí1)! = ‚àíN
x ,
which is unbounded as N ‚Üí‚àû, so the series diverges, by the ratio test. How-
ever, for x even moderately large, (11.2) provides an extremely eÔ¨Écient method of
calculating Ei(x), as we shall see.
In order to develop the power series representation of Ei(x) about x = 0, we
would like to use the series expansion of e‚àít. However, term by term integration is
not possible as things stand, since the integrals will not be convergent. Moreover,
the integral for Ei(x) does not converge as x ‚Üí0. We therefore have to be a little
more cunning in order to obtain our power series. Firstly, note that
Ei(x) =
 ‚àû
x
1
t

e‚àít ‚àí
1
1 + t

dt +
 ‚àû
x
1
t(1 + t) dt

11.1 ASYMPTOTIC EXPANSIONS
277
=
 ‚àû
x
1
t

e‚àít ‚àí
1
1 + t

dt ‚àílog

x
1 + x

.
With this rearrangement, we can take the limit x ‚Üí0 in the Ô¨Årst integral, with
the second integral giving Ei(x) ‚àº‚àílog x as x ‚Üí0. A further rearrangement then
gives
Ei(x) =
 ‚àû
0
1
t

e‚àít ‚àí
1
1 + t

dt ‚àí
 x
0
1
t

e‚àít ‚àí1

dt
‚àí
 x
0
1
1 + t dt ‚àílog

x
1 + x

=
 ‚àû
0
1
t

e‚àít ‚àí
1
1 + t

dt ‚àílog x ‚àí
 x
0
1
t

e‚àít ‚àí1

dt.
The Ô¨Årst term is just a constant, which we could evaluate numerically, but can be
shown to be equal to minus Euler‚Äôs constant, Œ≥ = 0.5772 . . . , although we will not
prove this here. We can now use the power series representation of e‚àít to give
Ei(x) = ‚àíŒ≥ ‚àílog x ‚àí
 x
0
1
t
‚àû

n=1
(‚àít)n
n!
dt.
Since this power series converges uniformly for all t, we can interchange the order
of summation and integration, and Ô¨Ånally arrive at
Ei(x) = ‚àíŒ≥ ‚àílog x ‚àí
‚àû

n=1
(‚àíx)n
n ¬∑ n! .
(11.4)
This representation of Ei(x) is convergent for all x > 0.
The philosophy that we have used earlier in the book is that we can now use (11.4)
to calculate the function for any given x. There are, however, practical problems
associated with this. Firstly, the series converges very slowly. For example, if we
take x = 5 we need 20 terms of the series in order to get three-Ô¨Ågure accuracy.
Secondly, even if we take enough terms in the series to get an accurate answer,
the result is the diÔ¨Äerence of many large terms.
For example, the largest term
in the series for Ei(20) is approximately 2.3 √ó 106, whilst for Ei(40) this rises to
approximately 3.8 √ó 1014. Unless the computer used to sum the series stores many
signiÔ¨Åcant Ô¨Ågures, there will be a large roundoÔ¨Äerror involved in calculating the
Ei(x) as the small diÔ¨Äerence of many large terms. These diÔ¨Éculties do not arise for
the asymptotic expansion, (11.2). Consider the two-term expansion
Ei(x) ‚àºe‚àíx
x

1 ‚àí1
x

.
Figure 11.1 shows the relative error using this expansion compared with that using
the Ô¨Årst 25 terms of the convergent series representation, (11.4). The advantages
of the asymptotic series over the convergent series are immediately apparent. Note
that the convergent series (11.4) also provides an asymptotic expansion valid for
x ‚â™1. We conclude that asymptotic expansions can also be convergent series.

278
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.1. The relative error in calculating Ei(x) using the Ô¨Årst 25 terms of the convergent
series expansion (11.4), solid line, and the two-term asymptotic expansion (11.2), dashed
line.
To summarize, let
SN(x) =
N

n=0
fn(x).
‚Äî If SN is a convergent series representation of S(x), SN ‚ÜíS(x) as N ‚Üí‚àûfor
Ô¨Åxed x within the radius of convergence.
‚Äî If SN is an asymptotic series representation of S(x), SN(x) ‚àºS(x) as x ‚Üí‚àû
(or whatever limit is appropriate) for any Ô¨Åxed N ‚©æ0.
11.1.3
Asymptotic Sequences of Gauge Functions
Let Œ¥n(œµ), n = 0, 1, 2, . . . be a sequence of functions such that Œ¥n = o(Œ¥n‚àí1) for
œµ ‚â™1. Such a sequence is called an asymptotic sequence of gauge functions.
For example, Œ¥n = œµn, Œ¥n = œµn/2, Œ¥n = (cot œµ)‚àín, Œ¥n = (‚àílog œµ)‚àín. If we can write
f(œµ) ‚àº
N

n=0
anŒ¥n(œµ)
as œµ ‚Üí0,
for some sequence of constants an, we say that f(œµ) has an asymptotic expan-
sion relative to the asymptotic sequence Œ¥n(œµ) for œµ ‚â™1. For example, Ei(x) has

11.1 ASYMPTOTIC EXPANSIONS
279
an asymptotic expansion, (11.2), relative to the asymptotic sequence x‚àíne‚àíx for
x‚àí1 ‚â™1. The asymptotic expansion of a function relative to a given sequence of
gauge functions is unique, since each an can be calculated in turn from
a0 = lim
œµ‚Üí0
f(œµ)
Œ¥0(œµ),
an = lim
œµ‚Üí0

f(œµ) ‚àí
n‚àí1

m=0
anŒ¥m(œµ)

/Œ¥n(œµ),
for n = 1, 2, . . . . However, a given function can have diÔ¨Äerent asymptotic expansions
with respect to diÔ¨Äerent sequences of gauge functions. For example,
if Œ¥n(œµ) = œµn,
sin œµ =
‚àû

n=1
(‚àí1)n+1
œµ2n‚àí1
(2n ‚àí1)!,
if Œ¥n(œµ) = (1 ‚àíeœµ)n,
sin œµ = Œ¥0 + 1
2Œ¥1 + 1
3Œ¥2 + 5
4Œ¥3 + ¬∑ ¬∑ ¬∑ .
Some sequences of gauge functions are clearly easier to use than others. We also
note that diÔ¨Äerent functions may have the same asymptotic expansion when we
consider only a Ô¨Ånite number of terms. For example,
eœµ ‚àº
N

n=0
œµn
n!
as œµ ‚Üí0+, for any N > 0,
eœµ + e‚àí1/œµ ‚àº
N

n=0
œµn
n!
as œµ ‚Üí0+, for any N > 0.
Since e‚àí1/œµ is exponentially small, it will only appear in the asymptotic expansion
after all of the algebraic terms.
Now let‚Äôs consider functions of a single variable x and a parameter œµ, f(x; œµ).
We can think of this as a typical solution of an ordinary diÔ¨Äerential equation with
independent variable x and a small parameter, œµ. If f has an asymptotic expansion,
f(x; œµ) ‚àº
N

n=0
fn(x)Œ¥n(œµ)
as œµ ‚Üí0,
that is valid for all x in some domain R, we say that the expansion is uniformly
valid in R. If this is not the case, we say that the expansion becomes nonuniform
in some subdomain. For example,
sin(x + œµ) = sin x + œµ cos x ‚àí1
2!œµ2 sin x ‚àí1
3!œµ3 cos x + O(œµ4)
as œµ ‚Üí0,
for all x, so the expansion is uniformly valid. Now consider
‚àö
x + œµ = ‚àöx

1 + œµ
x
1/2
‚àº‚àöx

1 + œµ
2x ‚àíœµ2
8x2 + O(œµ3)
	
as œµ ‚Üí0,
provided x ‚â´œµ. We say that the expansion becomes nonuniform as x ‚Üí0, when
x = O(œµ). Note that each successive term is smaller by a factor of œµ/x, and therefore
the expansion fails to be asymptotic when x = O(œµ). To determine an asymptotic

280
ASYMPTOTIC METHODS: BASIC IDEAS
expansion valid when x = O(œµ), we deÔ¨Åne a new, scaled variable, a procedure
that we will use again and again later, given by x = œµX, with X = O(1) as œµ ‚Üí0.
In terms of X,
‚àö
x + œµ = œµ1/2‚àö
X + 1.
This is the trivial asymptotic expansion valid for X = O(1)‚Ä†.
11.2
The Asymptotic Evaluation of Integrals
We have already seen in Chapters 5 and 6 that the solution of a diÔ¨Äerential equation
can often be found in closed form in terms of an integral. Instead of trying to
develop the asymptotic solution of a diÔ¨Äerential equation as some parameter or
variable becomes small, it is often more convenient to Ô¨Ånd an integral expression for
the solution and then seek an asymptotic expansion of the integral. Let‚Äôs consider
integrals of the type
I(Œª) =

C
eŒªf(t)g(t) dt,
(11.5)
with f and g analytic functions of t, Œª real and positive and C a contour that
joins two distinct points in the complex t-plane, one or both of which may be at
inÔ¨Ånity.
Such integrals arise very commonly in this context.
For example, the
Fourier transform, which we studied in Chapter 5, takes this form with f(t) = it,
whilst the Laplace inversion integral, which we discussed in Section 6.4, has f(t) = t.
Another common example is the solution of Airy‚Äôs equation, y‚Ä≤‚Ä≤ ‚àíxy = 0, which
we wrote in terms of Bessel functions in Section 3.8. The solution y = Ai(x) can
also be written in integral form as
Ai(x) =
1
2œÄi

C
ext‚àí1
3 t3 dt.
(11.6)
To see that this is a solution of Airy‚Äôs equation, note that
Ai‚Ä≤‚Ä≤ ‚àíxAi =
1
2œÄi

C
(t2 ‚àíx)ext‚àí1
3 t3 dt = ‚àí1
2œÄi

C
d
dt

ext‚àí1
3 t3
dt = 0,
provided that ext‚àí1
3 t3 ‚Üí0 as |t| ‚Üí‚àûon the contour C. For |t| ‚â´1, xt ‚àí1
3t3 ‚àº
‚àí1
3t3, so we need Re(t3) > 0 on the contour C, and therefore ‚àíœÄ
6 < arg(t) <
œÄ
6 ,
œÄ
2 < arg(t) <
5œÄ
6
or ‚àíœÄ
2 > arg(t) > ‚àí5œÄ
6 .
As we shall see later, Ai(x) is
distinguished from Bi(x) in that Ai(x) ‚Üí0 as x ‚Üí‚àû, in particular, with Ai(x) ‚àº
x‚àí1/4 exp

‚àí2
3x3/2
/2‚àöœÄ. This means that an appropriate contour C originates
with arg(t) = ‚àí2œÄ
3 and terminates with arg(t) = 2œÄ
3 (for example, the contour C in
Figure 11.10).
Rather than diving straight in and trying to evaluate I(Œª) in (11.5), we can
proceed by considering two simpler cases Ô¨Årst.
‚Ä† and of course valid for all X in this simple example.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
281
11.2.1
Laplace‚Äôs Method
Consider the complex integral along the real line
I1(Œª) =
 b
a
eŒªf(t)g(t) dt with f(t) real and Œª ‚â´1.
(11.7)
In this case, the integrand is largest at the point where f(t) is largest. In fact,
we can approximate the integral by simply considering the contribution from the
neighbourhood of the point where f(t) takes its maximum value. This is known as
Laplace‚Äôs Method
Integrals of the form (11.7) can be estimated by
considering the contribution from the neighbour-
hood of the maximum value of f(t) alone.
We will see how we can justify this rigorously later. For the moment, let‚Äôs consider
some examples.
Example 1
Consider the integral representation
KŒΩ(x) =
 ‚àû
0
e‚àíx cosh t cosh ŒΩt dt,
of the modiÔ¨Åed Bessel function, KŒΩ(x). When x ‚â´1, this is in the form (11.7)
with f(t) = ‚àícosh t and g(t) = cosh ŒΩt. The maximum value of f(t) = ‚àícosh t is
at t = 0, where f(t) = g(t) = 1. For t ‚â™1, cosh t ‚àº1 + 1
2t2, and we can therefore
use Laplace‚Äôs method to approximate KŒΩ(x) as
KŒΩ(x) ‚àº
 ‚àû
0
e‚àíx(1+ 1
2 t2) dt = e‚àíx
 ‚àû
0
e‚àí1
2 xt2 dt.
After making the substitution t = ÀÜt
#
2/x, this becomes
KŒΩ(x) ‚àº
*
2
xe‚àíx
 ‚àû
0
e‚àíÀÜt2 dÀÜt =
* œÄ
2xe‚àíx,
for x ‚â´1,
using (3.5).
Example 2
Consider the deÔ¨Ånition,
Œì(1 + Œª) =
 ‚àû
0
tŒªe‚àít dt =
 ‚àû
0
eŒª log t‚àít dt,
of the gamma function, which we met in Section 3.1. Can we Ô¨Ånd the asymptotic
form of the gamma function when Œª ‚â´1? Since the deÔ¨Ånition is not quite in the
form that we require for Laplace‚Äôs method, we need to make a transformation. If
we let t = ŒªœÑ, we Ô¨Ånd that
Œì(1 + Œª) = Œª1+Œª
 ‚àû
0
eŒª(log œÑ‚àíœÑ) dœÑ.

282
ASYMPTOTIC METHODS: BASIC IDEAS
This is in the form of (11.7) with f(œÑ) = log œÑ ‚àíœÑ. Since f ‚Ä≤(œÑ) = 1/œÑ ‚àí1 and
f ‚Ä≤‚Ä≤(œÑ) = ‚àí1/œÑ 2 < 0, f has a local maximum at œÑ = 1. Laplace‚Äôs method states that
Œì(1 + Œª) is dominated by the contribution to the integral from the neighbourhood
of œÑ = 1. We use a Taylor series expansion,
log œÑ ‚àíœÑ = log {1 + (œÑ ‚àí1)} ‚àí1 ‚àí(œÑ ‚àí1) = ‚àí1 ‚àí1
2(œÑ ‚àí1)2 + ¬∑ ¬∑ ¬∑
for |œÑ ‚àí1| ‚â™1,
and extend the range of integration, to obtain
Œì(1 + Œª) ‚àºŒª1+Œªe‚àíŒª
 ‚àû
‚àí‚àû
e‚àí1
2 ŒªT 2 dT
as Œª ‚Üí‚àû.
If we now let T = ÀÜT/Œª1/2, we Ô¨Ånd that
Œì(1 + Œª) ‚àºŒªŒª+ 1
2 e‚àíŒª
 ‚àû
‚àí‚àû
e‚àí1
2 ÀÜT 2 d ÀÜT,
and hence
Œª! = Œì(1 + Œª) ‚àº
‚àö
2œÄŒªŒª+ 1
2 e‚àíŒª as Œª ‚Üí‚àû.
(11.8)
This is known as Stirling‚Äôs formula, and provides an excellent approximation to
the gamma function, for Œª > 2, as shown in Figure 11.2.
Example 3
Consider the integral
I(Œª) =
 10
0
e‚àíŒªt
1 + t dt.
In this case, f = ‚àít and g = 1/(1 + t). Since f ‚Ä≤(t) = ‚àí1 Ã∏= 0 and the maximum
value of f occurs at t = 0 for t ‚àà[0, 10],
I(Œª) ‚àº
 10
0
e‚àíŒªt dt = 1
Œª

1 ‚àíe‚àí10Œª
‚àº1
Œª as Œª ‚Üí‚àû.
In fact we can use the binomial expansion
(1 + t)‚àí1 = 1 ‚àít + t2 ‚àít3 + ¬∑ ¬∑ ¬∑ + (‚àí1)ntn + ¬∑ ¬∑ ¬∑ ,
even though this is only convergent for |t| < 1, since the integrand is exponentially
small away from t = 0. We can also extend the range of integration to inÔ¨Ånity
without aÔ¨Äecting the result, to give
I(Œª) ‚àº
N

n=0
 ‚àû
0
(‚àí1)ntne‚àíŒªt dt =
N

n=0
(‚àí1)nŒì(n + 1)
Œªn+1
=
N

n=0
(‚àí1)nn!
Œªn+1
as Œª ‚Üí‚àû,
since, using the substitution œÑ = Œªt,
 ‚àû
0
tne‚àíŒªt dt =
1
Œªn+1
 ‚àû
0
œÑ ne‚àíœÑ dœÑ =
1
Œªn+1 Œì(n + 1) =
n!
Œªn+1 .
Note that, as we found for Ei(x), this is an asymptotic, rather than convergent,
series representation of I(x).
We can justify this procedure using the following
lemma.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
283
Fig. 11.2. The gamma function, Œì(1 + Œª) = Œª!, and its asymptotic approximation using
Stirling‚Äôs formula, (11.8).
Lemma 11.1 (Watson‚Äôs lemma) If
I(Œª) =
 A
0
e‚àíŒªtg(t) dt,
for A > 0, with g either bounded or having only integrable singularities,
g(t) ‚àº
N

n=0
antŒ±n
as t ‚Üí0,
and ‚àí1 < Œ±0 < Œ±1 < ¬∑ ¬∑ ¬∑ < Œ±N (which ensures that e‚àíŒªtg(t) has at worst an
integrable singularity at t = 0), then
I(Œª) ‚àº
 ‚àû
0
e‚àíŒªt
N

n=0
antŒ±n dt =
N

n=0
anŒª‚àíŒ±n‚àí1Œì(Œ±n + 1) as Œª ‚Üí‚àû.
Note that this lemma simply says that the integral is dominated by the contribu-
tion in the neighbourhood of t = 0, so that we can replace g with its asymptotic
expansion and extend the range of integration to inÔ¨Ånity.

284
ASYMPTOTIC METHODS: BASIC IDEAS
Proof We begin by separating oÔ¨Äthe contribution to the integral from the neigh-
bourhood of the origin by noting that
I(Œª) ‚àí
N

n=0
anŒª‚àíŒ±n‚àí1Œì(Œ±n + 1)
 =
I(Œª) ‚àí
N

n=0
an
 ‚àû
0
tŒ±ne‚àíŒªt dt

‚©Ω

 Œ¥
0
e‚àíŒªtg(t) dt ‚àí
 ‚àû
0
e‚àíŒªt
N

n=0
antŒ±n dt
 +

 A
Œ¥
e‚àíŒªtg(t) dt

for any real Œ¥ with 0 < Œ¥ < A. Next, we make use of the asymptotic behaviour of
g. Since
g(t) ‚àí
N

n=0
antŒ±n
 < KtŒ±n+1
for some K > 0 when 0 < t < Œ¥ and Œ¥ is suÔ¨Éciently small, we have
I(Œª) ‚àí
N

n=0
anŒª‚àíŒ±n‚àí1Œì(Œ±n + 1)

<

 Œ¥
0
e‚àíŒªt
N

n=0
antŒ±n dt + K
 Œ¥
0
e‚àíŒªttŒ±n+1 dt ‚àí
 ‚àû
0
e‚àíŒªt
N

n=0
antŒ±n dt

+

 A
Œ¥
e‚àíŒªtg(t) dt

<
‚àí
N

n=0
an
 ‚àû
Œ¥
tŒ±ne‚àíŒªt dt + K
 ‚àû
0
e‚àíŒªttŒ±n+1 dt ‚àíK
 ‚àû
Œ¥
e‚àíŒªttŒ±n+1 dt

+

 A
Œ¥
e‚àíŒªtg(t) dt

<
N

n=0
|an|
 ‚àû
Œ¥
tŒ±ne‚àíŒªt dt + KŒª‚àíŒ±n+1‚àí1Œì(Œ±n+1 + 1)
+K
 ‚àû
Œ¥
e‚àíŒªttŒ±n+1 dt +
 A
Œ¥
e‚àíŒªt|g(t)| dt.
Now, since e‚àíŒªt < e‚àí(Œª‚àí1)Œ¥e‚àít for t > Œ¥,
 ‚àû
Œ¥
tŒ±ne‚àíŒªt dt < e‚àí(Œª‚àí1)Œ¥
 ‚àû
Œ¥
tŒ±ne‚àít dt < e‚àí(Œª‚àí1)Œ¥Œì(Œ±n + 1),
and also
 A
Œ¥
e‚àíŒªt|g(t)| dt < Ge‚àíŒªŒ¥,

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
285
for t ‚àà[Œ¥, A], where G =
' A
Œ¥ |g(t)| dt. This Ô¨Ånally shows that
I(Œª) ‚àí
N

n=0
anŒª‚àíŒ±n‚àí1Œì(Œ±n + 1)
 < KŒª‚àíŒ±n+1‚àí1Œì(Œ±n+1 + 1) + Ge‚àíŒªŒ¥
+e‚àí(Œª‚àí1)Œ¥
 N

n=0
|an|
 ‚àû
Œ¥
tŒ±ne‚àít dt + K
 ‚àû
Œ¥
tŒ±n+1e‚àít dt

= O(Œª‚àíŒ±n+1‚àí1),
and hence the result is proved.
A simple modiÔ¨Åcation of this proof can be used to justify the use of Laplace‚Äôs
method in general.
11.2.2
The Method of Stationary Phase
Let‚Äôs now consider an integral, again along the real line, of the form
I2(Œª) =
 b
a
eiŒªF (t)g(t) dt,
with F(t) real and Œª ‚â´1.
(11.9)
Integrals of this type arise when Fourier transforms are used to solve diÔ¨Äerential
equations (see Chapter 5). Points where F ‚Ä≤(t) = 0 are called points of stationary
phase, and the integral can be evaluated by considering the sum of the contribu-
tions from each of these points. We will consider this more carefully in the next
section. For the moment, we can illustrate why this should be so by considering,
as an example, the case F(t) = (1 ‚àít)2, g(t) = t, which has F ‚Ä≤(t) = 0 when t = 1,
and
g(t)eiŒªF (t) = t cos Œª(1 ‚àít)2 + it sin Œª(1 ‚àít)2.
The rapid oscillations of this integrand lead to almost complete cancellation, except
in the neighbourhood of the point of stationary phase, as can be seen in Figure 11.3.
Let‚Äôs consider the situation when there is a single point of stationary phase at
t = t0. Then, since F ‚Ä≤(t0) = 0,
F(t) ‚àºF(t0) + 1
2(t ‚àít0)2F ‚Ä≤‚Ä≤(t0) + O

(t ‚àít0)3
for |t ‚àít0| ‚â™1,
provided that F ‚Ä≤‚Ä≤(t0) Ã∏= 0 (see Exercise 11.11). If we assume that the integral is
dominated by the contribution from the neighbourhood of the point of stationary
phase,
I2(Œª) ‚àº
 t0+Œ¥
t0‚àíŒ¥
g(t0) exp

iŒª

F(t0) + 1
2F ‚Ä≤‚Ä≤(t0)(t ‚àít0)2
	
dt
= eiŒªF (t0)g(t0)
 t0+Œ¥
t0‚àíŒ¥
exp
1
2iŒªF ‚Ä≤‚Ä≤(t0)(t ‚àít0)2
	
dt

286
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.3. The function t cos Œª(1 ‚àít)2 for Œª = 5, 10 and 20.
for some Œ¥ ‚â™1. If we now let
T = (t ‚àít0)
*
1
2Œª|F ‚Ä≤‚Ä≤(t0)|,
this becomes
I2(Œª) ‚àºeiŒªF (t0)g(t0)
+
2
Œª|F ‚Ä≤‚Ä≤(t0)|
 Œ¥‚àö
1
2 Œª|F ‚Ä≤‚Ä≤(t0)|
‚àíŒ¥‚àö
1
2 Œª|F ‚Ä≤‚Ä≤(t0)|
ei sgn{F ‚Ä≤‚Ä≤(t0)}T 2 dT,
and, at leading order as Œª ‚Üí‚àû,
I2(Œª) ‚àºeiŒªF (t0)g(t0)
+
2
Œª|F ‚Ä≤‚Ä≤(t0)|
 ‚àû
‚àí‚àû
e¬±iT 2 dT.
We now just need to calculate
J =
 ‚àû
0
eiT 2 dT = 1
2
 ‚àû
‚àí‚àû
eiT 2 dT.
To do this, consider the contours C1, C2 and C3 in the complex T-plane, illustrated

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
287
in Figure 11.4. Considering the contour C2 Ô¨Årst,
C1
C2
C3
Re(T)
Im(T)
Fig. 11.4. The contours C1, C2 and C3 in the complex T-plane.


C2
eiT 2 dT
 =

 œÄ/4
0
eiR2(cos 2Œ∏+i sin 2Œ∏)iReiŒ∏ dŒ∏

‚©ΩR
 œÄ/4
0
e‚àíR2 sin 2Œ∏ Œ∏ ‚Üí0
as R ‚Üí‚àû,
by Jordan‚Äôs lemma. On the remaining contours,

C3
eiT 2 dT =
 0
R
e‚àíÀÜT 2eiœÄ/4 d ÀÜT ‚Üí‚àí
‚àöœÄ
2 eiœÄ/4 as R ‚Üí‚àû,
and

C1
eiT 2 dT =
 R
0
eiT 2 dT ‚Üí
 ‚àû
0
eiT 2 dT
as R ‚Üí‚àû.
By Cauchy‚Äôs theorem

C1+C2+C3
eiT 2 dT = 0,
and hence
 ‚àû
0
eiT 2 dT =
‚àöœÄ
2 eiœÄ/4.

288
ASYMPTOTIC METHODS: BASIC IDEAS
Similarly,
 ‚àû
0
e‚àíiT 2 dT =
‚àöœÄ
2 e‚àíiœÄ/4.
We conclude that
I2(Œª) ‚àºeiŒªF (t0)esgn(F ‚Ä≤‚Ä≤(t0))iœÄ/4g(t0)
+
2œÄ
Œª|F ‚Ä≤‚Ä≤(t0)| = O(Œª‚àí1/2) as Œª ‚Üí‚àû.
In our example, F(t) = (1 ‚àít)2, so that t0 = 1, F(t0) = 0 and F ‚Ä≤‚Ä≤(t0) = 2, and
g(t) = t, so that g(t0) = 1. We conclude that
 b
a
teiŒª(1‚àít)2 dt ‚àºe‚àíiœÄ/4
*œÄ
Œª as Œª ‚Üí‚àû, when a < 1 < b.
(11.10)
If a = 1 or b = 1, we get half of the contribution from the stationary phase point.
If a > 1 or b < 1, there are no points of stationary phase. In this case, we can, in
general, integrate by parts to obtain
I2(Œª) ‚àº‚àíi
Œª
 g(b)
F ‚Ä≤(b)eiŒªF (b) ‚àíg(a)
F ‚Ä≤(a)eiŒªF (a)
	
+ O
 1
Œª2

.
(11.11)
Note that the contribution from the endpoints is of O(Œª‚àí1), whilst contributions
from points of stationary phase are larger, of O(Œª‚àí1/2).
Example
Consider (5.40), the solution of an initial value problem for the wave equation
that we considered in Section 5.5.2. Let‚Äôs analyze this solution when t ‚â´1 at a
point x = vt that moves with constant velocity, v. Since (5.40) is written in a
form independent of the orientation of the coordinate axes, we can assume that
v = v(0, 0, 1). Consider
I+ =
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
g(k)eitF+(k) dkx dky dkz,
where
g(k) =
Àúf(k)
16œÄ3ick ,
F+(k) = c
$
k2x + k2y + k2z ‚àívkz
and k = (kx, ky, kz). Starting with the kx integral, we can see that
‚àÇF+
‚àÇkx
=
ckx
$
k2x + k2y + k2z
,
which is zero when kx = 0. This is, therefore, a unique point of stationary phase
and, noting that
‚àÇ2F+
‚àÇk2x

kx=0
=
c
$
k2y + k2z
,

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
289
the method of stationary phase shows that
I+ ‚àºeiœÄ/4
*
2œÄ
ct
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû

k2
y + k2
z
1/4 ÀÜg(ky, kz)eit ÀÜ
F+(ky,kz) dky dkz,
where
ÀÜg(ky, kz) =
Àúf(0, ky, kz)
16œÄ3ick
,
ÀÜF+(ky, kz) = c
$
k2y + k2z ‚àívkz.
Similarly, we can use the method of stationary phase on the ky integral, and arrive
at
I+ ‚àº
1
8œÄ2c2t
 ‚àû
‚àí‚àû
Àúf(0, 0, kz)ei(c‚àív)tkz dkz.
When v Ã∏= c, a simple change of variable, ÀÜkz = tkz, shows that I+ = O(1/t2).
However, when v = c, I+ is much larger, of O(1/t). This is consistent with the fact
that disturbances propagate at speed c. We therefore assume that v = c, so that
we are considering a point moving in the z-direction with constant speed c, which
gives us
I+ ‚àº
1
8œÄ2c2t
 ‚àû
‚àí‚àû
Àúf(0, 0, kz) dkz.
If we now deÔ¨Åne
I‚àí=
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
g(k)eitF‚àí(k) dkx dky dkz,
with
F‚àí(k) = ‚àíc
$
k2x + k2y + k2z ‚àívkz,
and follow the analysis through in the same way, we Ô¨Ånd that
I‚àí‚àº
1
8œÄ2c2t
 ‚àû
‚àí‚àû
Àúf(0, 0, kz)e‚àíi(c+v)tkz dkz.
Since we have already decided to consider the case v = c, I‚àí= O(1/t2) for t ‚â´1,
and therefore
u(vt) ‚àºI+ ‚àº
1
8œÄ2c2t
 ‚àû
‚àí‚àû
Àúf(0, 0, kz) dkz,
where v = c(0, 0, 1). Since the z-direction can be chosen arbitrarily in this problem,
we conclude that the large time solution is small, of O(1/t2), except on the surface
of a sphere of radius ct, where the solution is given by
u(cte) ‚àº
1
8œÄ2c2t
 ‚àû
‚àí‚àû
Àúf(se) ds = O
1
t

,
(11.12)
with e an arbitrary unit vector. The amplitude of the solution therefore decays like
1/t, and also depends upon the total frequency content of the initial conditions,
(5.38), in the direction of e, as given by the integral of their Fourier transform,
' ‚àû
‚àí‚àûÀúf(se) ds.

290
ASYMPTOTIC METHODS: BASIC IDEAS
In summary,
The Method of Stationary Phase
Integrals of the form (11.9) can be estimated by
considering the contribution from the neighbour-
hood of each point of stationary phase, where
F ‚Ä≤(t) = 0. In the absence of such points, the in-
tegral is dominated by the contributions from the
endpoints of the range of integration.
11.2.3
The Method of Steepest Descents
Let‚Äôs return to consider the more general case,
I(Œª) =

C
eŒªf(t)g(t) dt
(11.13)
for Œª ‚â´1, f and g analytic functions of t = x + iy and C a contour in the complex
t-plane. Laplace‚Äôs method and the method of stationary phase are special cases
of this. Since f(t) and g(t) are analytic, we can deform the contour C without
changing I(Œª). If we deform C onto a contour C1, on which the imaginary part of
f(t) is a constant, f(t) = œÜ(t) + iœà0, then (11.13) becomes
I(Œª) = eiŒªœà0

C1
eŒªœÜ(t)g(t) dt,
and we can simply use Laplace‚Äôs method. This is what we really want to do, because
we have shown rigorously, in Lemma 11.1, that this sort of integral is dominated by
the neighbourhood of the point on C1 where the real-valued function œÜ(t) is largest.
In order to take this further, we need to know what the curves œÜ = constant
and œà = constant look like for an analytic function f(t) = œÜ(x, y) + iœà(x, y). The
Cauchy‚ÄìRiemann equations for the analytic function f(t) are (see Appendix 6)
‚àÇœÜ
‚àÇx = ‚àÇœà
‚àÇy ,
‚àÇœÜ
‚àÇy = ‚àí‚àÇœà
‚àÇx ,
which show that
‚àáœÜ ¬∑ ‚àáœà = ‚àÇœÜ
‚àÇx
‚àÇœà
‚àÇx + ‚àÇœÜ
‚àÇy
‚àÇœà
‚àÇy = ‚àí‚àÇœÜ
‚àÇx
‚àÇœÜ
‚àÇy + ‚àÇœÜ
‚àÇy
‚àÇœÜ
‚àÇx = 0,
and hence that ‚àáœÜ is perpendicular to ‚àáœà. Recall that ‚àáœÜ is normal to lines of
constant œÜ, from which we conclude that the lines of constant œÜ are perpendicular
to the lines of constant œà. Moreover, œÜ changes most rapidly in the direction of ‚àáœÜ,
in other words on the lines of constant œà. We say that the lines where œà is constant
are contours of steepest descent and ascent for œÜ. Our strategy for evaluating
the integral (11.13) is therefore to deform the contour of integration into one of
steepest descent. Figure 11.5 illustrates these ideas for the function f(t) = t2.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
291
Fig. 11.5. The real part, œÜ(x, y) = x2 ‚àíy2, of f(t) = t2, and the lines of steepest ascent
and descent for the analytic function f(t) = t2.
Example: f(t) = t2
When f(t) = t2,
I(Œª) =

C
g(t)eŒªt2 dt,
with C a contour that joins two points, P and Q, in the complex t-plane. In this
case, f(t) = t2 = (x + iy)2 = x2 ‚àíy2 + 2ixy, and hence œÜ = x2 ‚àíy2 and œà = 2xy.
Therefore, œÜ is constant on the hyperbolas x2 ‚àíy2 = œÜ and œà is constant on the
hyperbolas xy = 1
2œà, as shown in Figure 11.5.
Case 1: œà(P) > 0, œà(Q) > 0 and œÜ(P) Ã∏= œÜ(Q)
In this case, the ends of the contour C lie in the upper half plane, and without
loss of generality, we take œÜ(P) > œÜ(Q). We can deform C into the contour C1+C2,
with C1 the steepest descent contour on which œà = œà(P) and C2 the contour on
which œÜ = œÜ(Q), as shown in Figure 11.6. On C1 we can therefore make the change
of variable
f(t) = f(P) ‚àíœÑ,
with 0 ‚©ΩœÑ ‚©ΩœÜ(P) ‚àíœÜ(Q).
The real part of f varies from œÜ(P) to œÜ(Q) as œÑ varies from zero to œÜ(P) ‚àíœÜ(Q),
whilst œà = œà(P) is constant. Since dœÑ/dt = ‚àíf ‚Ä≤(t),

C1
g(t)eŒªf(t) dt = ‚àíeŒªf(P )
 œÜ(P )‚àíœÜ(Q)
0
g(t)
f ‚Ä≤(t)e‚àíŒªœÑdœÑ
‚àº‚àíeŒªf(P ) g(P)
f ‚Ä≤(P)
1
Œª as Œª ‚Üí‚àû,

292
ASYMPTOTIC METHODS: BASIC IDEAS
P
Q
C1
C2
Re(t)
Im(t)
œà = œà(P)
œà = œà(Q)
œÜ = œÜ(Q)
œÜ = œÜ(P)
‚Ä¢
‚Ä¢
Fig. 11.6. The contours C1 and C2 in case 1.
using Watson‚Äôs lemma, provided that g(P) is bounded and nonzero. On C2, œÜ =
œÜ(Q) and the imaginary part, œà, varies. We can write


C2
g(t)eŒªf(t) dt
 =
eŒªœÜ(Q)

C2
g(t)eiŒªœà(t) dt
 ‚©ΩGLeŒªœÜ(Q) ‚â™

C1
g(t)eŒªf(t) dt,
since œÜ(Q) < œÜ(P), where G = maxC2 |g(t)| and L = length of C2. We conclude
that
I(Œª) ‚àº‚àíeŒªf(P ) g(P)
f ‚Ä≤(P)
1
Œª as Œª ‚Üí‚àû.
Case 2: œà(P) > 0, œà(Q) > 0 and œÜ(P) = œÜ(Q)
In this case, we must deform C into C1 +C2 +C3, with œà constant on C1 and C3
and œÜ < œÜ(P) constant on C3, as shown in Figure 11.7. The exact choice of contour
C2 is not important, since, as in case 1, its contribution is exponentially smaller
than the contributions from C1 and C3. This is very similar to case 1, except that
we now also have, using Laplace‚Äôs method, a contribution at leading order from C3.
We Ô¨Ånd that
I(Œª) ‚àº

‚àíeŒªf(P ) g(P)
f ‚Ä≤(P) + eŒªf(Q) g(Q)
f ‚Ä≤(Q)
	 1
Œª as Œª ‚Üí‚àû.
Let‚Äôs consider a more interesting example.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
293
P
Q
C1
C2
C3
Re(t)
Im(t)
œà = œà(Q)
œà = œà(P)
œÜ = œÜ(P) = œÜ(Q)
œÜ < œÜ(P)
‚Ä¢
‚Ä¢
Fig. 11.7. The contours C1, C2 and C3 in case 2.
Example: The Bessel function of order zero with large argument
We saw in Chapter 3 that the Bessel function Jn(Œª) has an integral representation,
(3.18). Setting n = 0 and making the change of variable t = sin Œ∏ leads to
J0(Œª) = 1
œÄ
 1
‚àí1
eiŒªt
‚àö
1 ‚àít2 dt.
(11.14)
This is of the form (11.13) with f(t) = it, and hence œÜ = ‚àíy, œà = x. In this
case, P = ‚àí1 and Q = 1, so that œÜ(P) = œÜ(Q) = 0. The contours of steepest
descent through P and Q are just the straight lines œà = x = ‚àí1 and œà = x = 1
respectively. We therefore deform the contour C, which is the portion of the real
axis ‚àí1 ‚©Ωx ‚©Ω1, into C1 + C2 + C3, as shown in Figure 11.8. The contribution
from C2, y = Y > 0, is exponentially small whatever the choice of Y (Y = 1
in Figure 11.8), whilst the contributions from C1 and C3 are dominated by the
neighbourhoods of the endpoints on the real axis. On C1 we make the change of
variable t = ‚àí1 + iy, so that
1
œÄ

C1
eiŒªt
‚àö
1 ‚àít2 dt = 1
œÄ
 Y
0
e‚àíiŒªe‚àíŒªy
#
2iy + y2 i dy
‚àºe‚àíiŒªi
œÄ
‚àö
2i
 ‚àû
0
y‚àí1/2e‚àíŒªy as Œª ‚Üí‚àû,

294
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.8. The contours C1, C2 and C3 for evaluating the asymptotic behaviour of J0(Œª).
using Laplace‚Äôs method. By making the change of variable ÀÜy = Œªy, we can write
this integral in terms of a gamma function, Œì(1/2) = ‚àöœÄ, and arrive at
1
œÄ

C1
eiŒªt
‚àö
1 ‚àít2 dt ‚àºe‚àíiŒª+iœÄ/4
‚àö
2œÄŒª
as Œª ‚Üí‚àû.
Similarly,
1
œÄ

C3
eiŒªt
‚àö
1 ‚àít2 dt ‚àºeiŒª‚àíiœÄ/4
‚àö
2œÄŒª
as Œª ‚Üí‚àû,
and hence
J0(Œª) ‚àº
*
2
ŒªœÄ cos

Œª ‚àíœÄ
4

as Œª ‚Üí‚àû.
So, is this the whole story? Let‚Äôs return to our original example, f(t) = t2. Since
f ‚Ä≤(t) = 2t, the real and imaginary parts of f are stationary at t = 0. However,
t = 0 is neither a local maximum nor a local minimum. It is a saddle point. In

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
295
fact, no analytic function can have a local maximum or minimum, since its real and
imaginary parts are harmonic (see Section 7.1.4 and Appendix 6). The real part of
f(t) = t2 has a ridge along the line y = 0 and a valley along the line x = 0, as can
be seen in Figure 11.5. In cases 1 and 2, which we studied above, we were able to
deform the contour C within one of the valleys.
Case 3: œà(P) > 0 and œà(Q) < 0
In this case, P and Q lie in diÔ¨Äerent valleys of the function f(t) = t2, and
we must deform the contour so that it runs through the line of steepest descent
at the saddle point, since this is the only line with œà constant that connects the
two valleys, as shown in Figure 11.9. As usual, the integrals on C2 and C4 are
P
Q
C1
C2
C3
C4
C5
Im(t)
Re(t)
œà = œà(P) > 0
œà = œà(Q) < 0
Fig. 11.9. The contours C1, C2, C3, C4 and C5 in case 3.
exponentially small, and, using Laplace‚Äôs method as in cases 1 and 2,

C1
g(t)eŒªt2 dt ‚àº‚àíeŒªf(P ) g(P)
f ‚Ä≤(P)
1
Œª,

C5
g(t)eŒªt2 dt ‚àºeŒªf(Q) g(Q)
f ‚Ä≤(Q)
1
Œª.
On the steepest descent contour C3, we can make the simple change of variable
t = iy, to arrive at

C3
g(t)eŒªt2 dt = i
 œà(Q)
œà(P )
g(iy)e‚àíŒªy2 dy.

296
ASYMPTOTIC METHODS: BASIC IDEAS
This integral is dominated by the behaviour close to the saddle point, y = 0, and,
if g(0) is bounded and nonzero, Laplace‚Äôs method shows that

C3
g(t)eŒªt2 dt ‚àº‚àíig(0)
 ‚àû
‚àí‚àû
e‚àíŒªy2 dy = ‚àíig(0)
*œÄ
Œª.
(11.15)
If œÜ(P) ‚©Ω0 and œÜ(Q) ‚©Ω0, this is the dominant contribution.
Finally, what if the contour of integration, C, extends to inÔ¨Ånity? For such an
integral to converge, it must extend into the valleys of f(t) as |t| ‚Üí‚àû. In our
example, if we let P and Q head oÔ¨Äto inÔ¨Ånity along the two diÔ¨Äerent valleys, the
integral is dominated by the contribution from the steepest descent contour through
the saddle point, given by (11.15).
Example: The Airy function, Ai(x), for |x| ‚â´1
As we have seen, (11.6),
Ai(x) =
1
2œÄi

C
ext‚àí1
3 t3 dt,
(11.16)
with arg(t) = ¬± 2œÄ
3 as |t| ‚Üí‚àû. For x > 0 we can make the change of variable
t = x1/2œÑ and, since arg(t) = arg(œÑ), deform back to the original contour to obtain
Ai(x) = x1/2
2œÄi

C
ex3/2(œÑ‚àí1
3 œÑ 3) dœÑ.
(11.17)
This is now in the standard form, with f(œÑ) = œÑ ‚àí1
3œÑ 3. Since f ‚Ä≤(œÑ) = 1 ‚àíœÑ 2,
there are saddle points at œÑ = ¬±1. The contours of constant imaginary part, œà,
are shown in Figure 11.10. We can now deform C into the steepest descent contour
through œÑ = ‚àí1, marked as C in Figure 11.10. As we have seen, the integral is
dominated by the contribution from the neighbourhood of this saddle point. Since
the contour C is vertical at this point, we write œÑ = ‚àí1 + iy and use Laplace‚Äôs
method to show that
Ai(x) ‚àºx1/2
2œÄi
 ‚àû
‚àí‚àû
ex3/2(‚àí2/3‚àíy2)i dy =
1
2‚àöœÄ x‚àí1/4 exp

‚àí2
3x3/2

as x ‚Üí‚àû.
(11.18)
Similarly, when x < 0, we can make the transformation t = (‚àíx)1/2T, so that
Ai(x) = (‚àíx)1/2
2œÄi

C
e(‚àíx)3/2(‚àíT ‚àí1
3 T 3) dT.
(11.19)
In this case, f(T) = ‚àíT ‚àí1
3T 3 and f ‚Ä≤(T) = ‚àí1 ‚àíT 2, so there are saddle points at
T = ¬±i. The contours of constant imaginary part, œà, are shown in Figure 11.11.
By deforming C into the contour C1 + C2 + C3, we can see that the integral is
dominated by the contributions from the two saddle points. The integral along C2
is exponentially smaller. In order to calculate the leading order contribution from
C1, note that this contour passes through T = ‚àíi in the direction of 1 + i. We

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
297
Fig. 11.10. Contours where the imaginary part of f(œÑ) = œÑ ‚àí1
3œÑ 3 is constant.
therefore make the substitution T = ‚àíi + (1 + i)s and use Laplace‚Äôs method to
arrive at
(‚àíx)1/2
2œÄi

C1
e(‚àíx)3/2(‚àíT ‚àí1
3 T 3) dt ‚àº(‚àíx)1/2
2œÄi
e
2
3 i(‚àíx)3/2(1 + i)
 ‚àû
‚àí‚àû
e‚àí2(‚àíx)3/2s2 ds
= (‚àíx)‚àí1/4(1 + i)
2œÄi
*œÄ
2 e
2
3 i(‚àíx)3/2.
Similarly,
(‚àíx)1/2
2œÄi

C3
e(‚àíx)3/2(‚àíT ‚àí1
3 T 3) dt ‚àº(‚àíx)‚àí1/4(‚àí1 + i)
2œÄi
*œÄ
2 e‚àí2
3 i(‚àíx)3/2,
and hence
Ai(x) ‚àº(‚àíx)‚àí1/4
‚àöœÄ
sin
2
3(‚àíx)3/2 + œÄ
4
	
as x ‚Üí‚àí‚àû.
(11.20)
The transition from exponential decay as x ‚Üí‚àûto decaying oscillations as x ‚Üí
‚àí‚àûis shown in Figure 11.12. Also shown is the behaviour predicted by our analysis

298
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.11. Contours where the imaginary part of f(T) = ‚àíT ‚àí1
3T 3 is constant, along
with a single contour, C2, where the real part of f is constant.
for large |x|, which can be seen to be in excellent agreement with Ai(x), even for
moderate values of |x|.
To end this section, we give a justiÔ¨Åcation for the success of the method of
stationary phase. Consider the example that we looked at earlier,
I2(Œª) =
 b
a
teiŒª(1‚àít)2 dt,
with a < 1 < b. Using Cauchy‚Äôs theorem on the analytic function teiŒª(1‚àít)2, we can
deform the contour C, the real axis with a ‚©Ωx ‚©Ωb, into C1 + C2 + C3 + C4 + C5,
as shown in Figure 11.13. The same arguments as those that we used above show
that the largest contribution comes from the neighbourhood of the saddle point on
the steepest descent contour, C3. This is, however, just the neighbourhood of the
single point of stationary phase, and, even though the contour is diÔ¨Äerent, because
the integrand is analytic, we obtain the same result, (11.10).

EXERCISES
299
Fig. 11.12. The Airy function Ai(t).
Exercises
11.1
Calculate the order of magnitude of the following functions in terms of the
simplest function of œµ possible, as œµ ‚Üí0+. (a) sinh œµ, (b) tan œµ,
(c) sinh(1/œµ), (d) e‚àíœµ, (e) cot œµ, (f) log(1 + œµ), (g) (1 ‚àícos œµ)/(1 + cos œµ),
(h) exp {‚àícosh (1/œµ)}.
11.2
Show that e‚àí1/œµ = o(œµn) as œµ ‚Üí0 for all real n, provided that the complex
variable œµ is restricted to a domain that you should determine.
11.3
Consider the integral
I(x) = e‚àíx
 x
1
et
t dt as x ‚Üí‚àû.
By integrating by parts repeatedly, develop a series expansion of the form
I(x) = 1
x + 1
x2 + 2
x3 + 3!
x4 ‚àí(1 + 1 + 2 + 3!) e1‚àíx + ¬∑ ¬∑ ¬∑ .
By considering the error in terminating the expansion after the term in
x‚àín, show that the series is asymptotic as x ‚Üí‚àû.
11.4
Show that the following are asymptotic sequences.

300
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.13. The contours C1, C2, C3, C4 and C5 for the method of stationary phase.
(a)

sin 1
œµ
n
, n = 0, 1, . . . , as œµ ‚Üí‚àû,
(b) log(1 + œµn), n = 0, 1, . . . , as œµ ‚Üí0.
11.5
Use Watson‚Äôs lemma to show that
 ‚àû
0
(t + t2)‚àí1/2e‚àíŒªtdt ‚àº
*œÄ
Œª

1 ‚àí1
4Œª +
9
32Œª2

as Œª ‚Üí‚àû.
11.6
After making the substitution t = uŒª, use Laplace‚Äôs method to show that
 ‚àû
0
e4Œª3/2t1/2‚àít2
1 + (t/Œª)2 dt ‚àº
*œÄ
6 e3Œª2 as Œª ‚Üí‚àû.
11.7
Use Laplace‚Äôs method to show that the modiÔ¨Åed Bessel function KŒΩ(z),
which has an integral representation
KŒΩ(z) = 1
2
 ‚àû
‚àí‚àû
eŒΩt‚àíz cosh tdt,

EXERCISES
301
can be approximated as ŒΩ ‚Üí‚àû, with z = O(1) and positive, using
KŒΩ(z) ‚àº
* œÄ
2ŒΩ e‚àíŒΩ
2ŒΩ
z
ŒΩ
.
You may assume that the integral is dominated by the contribution from
the neighbourhood of the maximum value of ŒΩt ‚àíz cosh t.
11.8
Use the method of stationary phase to show that
 ‚àû
0
eiŒª( 1
3 t3‚àít)
1 + t
dt ‚àº1
2
*œÄ
Œªe‚àí2iŒª/3+iœÄ/4 as Œª ‚Üí‚àû.
11.9
Use the method of stationary phase to show that
 œÄ/2
0
eiŒª(2t‚àísin 2t)dt ‚àº1
2Œì
4
3
  6
Œª
1/3
eiœÄ/6 as Œª ‚Üí‚àû.
11.10
Show that as Œª ‚Üí‚àû
(a)
 1+5i
4
eiŒªt2dt ‚àºie16iŒª
8
 1
Œª ‚àí
i
32Œª2

,
(b)
 1+5i
‚àí5‚àíi
eiŒªt2dt ‚àº
*œÄ
ŒªeiœÄ/4.
11.11
Consider the integral (11.9) when F(t) has a single point of stationary
phase at t = t0 with a < t0 < b, F ‚Ä≤‚Ä≤(t0) = 0 and F ‚Ä≤‚Ä≤‚Ä≤(t0) Ã∏= 0. Use the
method of stationary phase to show that
I2 ‚àº2
3g(t0)eiŒªF (t0)+iœÄsgn(F ‚Ä≤‚Ä≤‚Ä≤(t0))/6Œì
1
3
 
6
Œª|F ‚Ä≤‚Ä≤‚Ä≤(t0)|
1/3
for Œª ‚â´1.
11.12
Consider the integral
I(Œª) =
 Q
P
eŒª( 1
2 t2+it)dt,
where P and Q are points in the complex t plane. Sketch the contours of
constant real and imaginary parts of 1
2t2 + it. Show that if P = ‚àí1
2 and
Q = 2,
I(Œª) ‚àºe(2+2i)Œª
(2 + i)Œª as Œª ‚Üí‚àû.
Show that if P = ‚àí1
2 + i and Q = 1 ‚àí3i,
I(Œª) ‚àº‚àíi
*
2œÄeŒª
Œª
as Œª ‚Üí‚àû.
11.13
Show that Stirling‚Äôs formula for Œì(z + 1) when z ‚â´1 holds for complex z
provided |z| ‚â´1 and ‚àíœÄ < arg(z) < œÄ. (Hint: Let z = ReiŒ∏.) Determine
the next term in the asymptotic expansion.

302
ASYMPTOTIC METHODS: BASIC IDEAS
11.14
From the integral representation,
IŒΩ(x) = 1
œÄ
 œÄ
0
ex cos Œ∏ cos ŒΩŒ∏ dŒ∏ ‚àísin ŒΩœÄ
œÄ
 ‚àû
0
e‚àíx cosh t‚àíŒΩt dt,
of the modiÔ¨Åed Bessel function of order ŒΩ, show that
IŒΩ(x) ‚àº
ex
‚àö
2œÄx,
as x ‚Üí‚àûfor real x and Ô¨Åxed ŒΩ.
11.15
The parabolic cylinder functions are deÔ¨Åned by
D‚àí2m(x) =
1
(m ‚àí1)!xe‚àíx2/4
 ‚àû
0
e‚àíssm‚àí1 
x2 + 2s
‚àím‚àí1/2 ds,
D‚àí2m+1(x) =
1
(m ‚àí1)!xe‚àíx2/4
 ‚àû
0
e‚àíssm‚àí1 
x2 + 2s
‚àím+1/2 ds,
for m a positive integer. Show that for real x and Ô¨Åxed m,
D‚àí2m(x) ‚àºx‚àí2me‚àíx2/4,
D‚àí2m+1 ‚àºx‚àí2m+1e‚àíx2/4,
as x ‚Üí‚àû.

CHAPTER TWELVE
Asymptotic Methods: DiÔ¨Äerential Equations
In this chapter we will apply the ideas that we developed in the previous chapter
to the solution of ordinary and partial diÔ¨Äerential equations.
12.1
An Instructive Analogy: Algebraic Equations
Many of the essential ideas that we will need in order to solve diÔ¨Äerential equa-
tions using asymptotic methods can be illustrated using algebraic equations. These
are much more straightforward to deal with than diÔ¨Äerential equations, and the
ideas that we will use are far more transparent. We will consider two illustrative
examples.
12.1.1
Example: A Regular Perturbation
Consider the cubic equation
x3 ‚àíx + œµ = 0.
(12.1)
Although there is an explicit formula for the solution of cubic equations, it is rather
cumbersome to use. Let‚Äôs suppose instead that we only need to Ô¨Ånd the solutions
for œµ ‚â™1. If we simply set œµ = 0, we get x3 ‚àíx = 0, and hence x = ‚àí1, 0 or 1.
These are called the leading order solutions of the equation. These solutions
are obviously not exact when œµ is small but nonzero, so let‚Äôs try to improve the
accuracy of our approximation by seeking an asymptotic expansion of the solution
(or more succinctly, an asymptotic solution) of the form
x = x0 + œµx1 + œµ2x2 + O(œµ3).
(12.2)
We can now substitute this into (12.1) and equate powers of œµ. This gives us

x0 + œµx1 + œµ2x2
3 ‚àí

x0 + œµx1 + œµ2x2

+ œµ + O

œµ3
= 0,
which we can rearrange into a hierarchy of powers of œµ in the form

x3
0 ‚àíx0

+ œµ {(3x0 ‚àí1) x1 + 1} + œµ2 
3x0x2
1 +

3x2
0 ‚àí1

x2

+ O

œµ3
= 0.
At leading order we obviously get x3
0 ‚àíx0 = 0, and hence x0 = ‚àí1, 0 or 1. We will
concentrate on the solution with x0 = 1. At O(œµ), (3x0 ‚àí1) x1 + 1 = 2x1 + 1 = 0,
and hence x1 = ‚àí1
2. At O(œµ2), 3x0x2
1 +

3x2
0 ‚àí1

x2 = 3
4 + 2x2 = 0, and hence

304
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
x2 = ‚àí3
8. We could, of course, continue to higher order if necessary. This shows
that
x = 1 ‚àí1
2œµ ‚àí3
8œµ2 + O(œµ3)
for œµ ‚â™1.
Similar expansions can be found for the other two solutions of (12.1). This is a
regular perturbation problem, since we have found asymptotic expansions for all
three roots of the cubic equation using the simple expansion (12.2). Figure 12.1
shows that the function x3 ‚àíx + œµ is qualitatively similar for œµ = 0 and 0 < œµ ‚â™1.
Fig. 12.1. The function x3 ‚àíx + œµ for œµ = 0, solid line, and œµ = 0.1, broken line.
12.1.2
Example: A Singular Perturbation
Consider the cubic equation
œµx3 + x2 ‚àí1 = 0.
(12.3)
At leading order for œµ ‚â™1, x2 ‚àí1 = 0, and hence x = ¬±1. However, we know
that a cubic equation is meant to have three solutions. What‚Äôs happened to the
other solution? This is an example of a singular perturbation problem, where the
solution for œµ = 0 is qualitatively diÔ¨Äerent to the solution when 0 < œµ ‚â™1. The
equation changes from quadratic to cubic, and the number of solutions goes from
two to three. The key point is that we have implicitly assumed that x = O(1).
However, the term œµx3, which we neglect at leading order when x = O(1), becomes
comparable to the term x2 for suÔ¨Éciently large x, speciÔ¨Åcally when x = O(œµ‚àí1).
Figure 12.2 shows how the function œµx3 +x2 ‚àí1 changes qualitatively for œµ = 0 and
0 < œµ ‚â™1.

12.1 AN INSTRUCTIVE ANALOGY: ALGEBRAIC EQUATIONS
305
Fig. 12.2. The function œµx3 + x2 ‚àí1 for œµ = 0, solid line, and œµ = 0.1, broken line.
So, how can we proceed in a systematic way? If we expand x = x0 +œµx1 +O(œµ2),
we can construct the two O(1) solutions, x = ¬±1 + O(œµ), in the same manner as we
did for the previous example. Since we know that there must be three solutions, we
conclude that the other solution cannot have x = O(1), and assume that x = O(œµŒ±),
with Œ± to be determined. If we deÔ¨Åne a scaled variable, x = œµŒ±X, with X = O(1)
for œµ ‚â™1, (12.3) becomes
œµ3Œ±+1X3 + œµ2Œ±X2 ‚àí1 = 0.
(12.4)
We must choose Œ± in order to obtain an asymptotic balance between two of
the terms in (12.4). If Œ± > 0, the Ô¨Årst two terms are small and cannot balance
the third term, which is of O(1). If Œ± < 0, the Ô¨Årst two terms are large, and we
can choose Œ± so that they are of the same asymptotic order. This requires that
œµ3Œ±+1X3 = O(œµ2Œ±X2), and hence œµ3Œ±+1 = O(œµ2Œ±). This gives 3Œ± + 1 = 2Œ±, and
hence Œ± = ‚àí1. This means that x = œµ‚àí1X = O(œµ‚àí1), as expected. Equation (12.4)
now becomes
X3 + X2 ‚àíœµ2 = 0.
(12.5)
Since only œµ2 and not œµ appears in this rescaled equation, we expand X = X0 +
œµ2X1 + O(œµ4). At leading order, X3
0 + X2
0 = 0, and hence X0 = ‚àí1 or 0. Of course,
X0 = 0 will just give us the two solutions with x = O(1) that we have already
considered, so we take X0 = ‚àí1. At O(œµ2),
(‚àí1 + œµ2X1)3 + (‚àí1 + œµ2X1)2 ‚àíœµ2 + O(œµ4) = 0

306
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
gives
‚àí1 + 3œµ2X1 + 1 ‚àí2œµ2X1 ‚àíœµ2 + O(œµ4) = 0,
and hence X1 = 1. Therefore X = ‚àí1+œµ2 +O(œµ4), and hence x = ‚àí1/œµ+œµ+O(œµ3).
12.2
Ordinary DiÔ¨Äerential Equations
The solution of ordinary diÔ¨Äerential equations by asymptotic methods often pro-
ceeds in a similar way to the solution of algebraic equations, which we discussed
in the previous section. We assume that an asymptotic expansion of the solution
exists, substitute into the equation and boundary conditions, and equate powers of
the small parameter. This determines a sequence of simpler equations and bound-
ary conditions that we can solve. In order to introduce the main ideas, we will
begin by considering some simple, constant coeÔ¨Écient linear ordinary diÔ¨Äerential
equations before moving on to study both nonlinear ordinary diÔ¨Äerential equations
and some simple partial diÔ¨Äerential equations.
12.2.1
Regular Perturbations
Consider the ordinary diÔ¨Äerential equation
y‚Ä≤‚Ä≤ + 2œµy‚Ä≤ ‚àíy = 0,
(12.6)
to be solved for 0 ‚©Ωx ‚©Ω1, subject to the boundary conditions
y(0) = 0,
y(1) = 1.
(12.7)
Of course, we could solve this constant coeÔ¨Écient ordinary diÔ¨Äerential equation
analytically using the method described in Appendix 5, but it is instructive to try
to construct the asymptotic solution when œµ ‚â™1. We seek a solution of the form
y(x) = y0(x) + œµy1(x) + O(œµ2).
At leading order, y‚Ä≤‚Ä≤
0 ‚àíy0 = 0, subject to y0(0) = 0 and y0(1) = 1, which has
solution
y0(x) = sinh x
sinh 1 .
If we now substitute the expansion for y into (12.6) and retain terms up to O(œµ),
we obtain
(y0 + œµy1)‚Ä≤‚Ä≤ + 2œµ(y0 + œµy1)‚Ä≤ ‚àí(y0 + œµy1) + O(œµ2)
= y‚Ä≤‚Ä≤
0 + œµy‚Ä≤‚Ä≤
1 + 2œµy‚Ä≤
0 ‚àíy0 ‚àíœµy1 + O(œµ2) = 0,
and hence
y‚Ä≤‚Ä≤
1 ‚àíy1 = ‚àí2y‚Ä≤
0 = ‚àí2cosh x
sinh 1 .
(12.8)
Similarly, the boundary conditions (12.7) show that
y0(0) + œµy1(0) + O(œµ2) = 0,
y0(1) + œµy1(1) + O(œµ2) = 1,

12.2 ORDINARY DIFFERENTIAL EQUATIONS
307
and hence
y1(0) = 0,
y1(0) = 0.
(12.9)
By seeking a particular integral solution of (12.8) in the form y1p = kx sinh x, and
using the constants in the homogeneous solution, y1h = A sinh x + B cosh x, to
satisfy the boundary conditions (12.9), we arrive at
y1 = (1 ‚àíx)sinh x
cosh 1,
and hence
y(x) = sinh x
sinh 1 + œµ(1 ‚àíx)sinh x
cosh 1 + O(œµ2),
(12.10)
for œµ ‚â™1. The ratio of the second to the Ô¨Årst term in this expansion is œµ(1 ‚àí
x) tanh 1, which is uniformly small for 0 ‚©Ωx ‚©Ω1. This leads us to believe that
the asymptotic solution (12.10) is uniformly valid in the region of solution. The
situation is analogous to the example that we studied in Section 12.1.1.
One subtle point is that, although we believe that the next term in the asymptotic
expansion of the solution, which we write as O(œµ2) in (12.10), is uniformly smaller
than the two retained terms for œµ suÔ¨Éciently small, we have not proved this. We
do not have a rigorous estimate for the size of the neglected term in the way that
we did when we considered the exponential integral, where we were able to Ô¨Ånd an
upper bound for RN, given by (11.3). Although, for this simple, constant coeÔ¨Écient
ordinary diÔ¨Äerential equation, we could write down the exact solution and prove
that the remainder is of O(œµ2), in general, and in particular for nonlinear problems,
this is not possible, and an act of faith is involved in trusting that our asymptotic
solution provides a valid representation of the exact solution. This faith can be
bolstered in a number of ways, for example, by comparing asymptotic solutions with
numerical solutions, and by checking that the asymptotic solution makes sense in
terms of the underlying physics of the system that we are modelling. The sensible
applied mathematician always has a small, nagging doubt at the back of their mind
about the validity of an asymptotic solution. For (12.6), our faith is justiÔ¨Åed, as
can be seen in Figure 12.3.
12.2.2
The Method of Matched Asymptotic Expansions
Consider the ordinary diÔ¨Äerential equation
œµy‚Ä≤‚Ä≤ + 2y‚Ä≤ ‚àíy = 0,
(12.11)
to be solved for 0 ‚©Ωx ‚©Ω1, subject to the boundary conditions
y(0) = 0,
y(1) = 1.
(12.12)
The observant reader will notice that this is the same as the previous example, but
with the small parameter œµ moved to the highest derivative term. We again seek
an asymptotic solution of the form
y(x) = y0(x) + œµy1(x) + O(œµ2).

308
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.3. Exact and asymptotic solutions of (12.6) when œµ = 0.25.
At leading order, 2y‚Ä≤
0 ‚àíy0 = 0, which has the solution y0 = Aex/2 for some constant
A. However, the boundary conditions require that y0(0) = 0 and y0(1) = 1. How
can we satisfy two boundary conditions using only one constant? Well, of course
we can‚Äôt. The problem is that for œµ = 0, the equation is of Ô¨Årst order, and there-
fore qualitatively diÔ¨Äerent from the full, second order equation. This is a singular
perturbation, and is analogous to the example we studied in Section 12.1.2.
Let‚Äôs proceed by satisfying the boundary condition y0(1) = 1, which gives
y0(x) = e(x‚àí1)/2.
At O(œµ) we have
2y‚Ä≤
1 ‚àíy1 = ‚àíy‚Ä≤‚Ä≤
0 = ‚àí1
4e(x‚àí1)/2,
to be solved subject to y1(0) = 0 and y1(1) = 0. This equation can be solved using
an integrating factor (see Section A5.2), which gives
y1(x) = ‚àí1
2xe(x‚àí1)/2 + ke(x‚àí1)/2,
for some constant k. Again, we cannot satisfy both boundary conditions, and we
just use y1(1) = 0, which gives
y1(x) = 1
2(1 ‚àíx)e(x‚àí1)/2.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
309
Finally, this gives
y = e(x‚àí1)/2

1 + 1
2(1 ‚àíx)œµ
	
+ O(œµ2),
(12.13)
for œµ ‚â™1. This suggests that y ‚Üíe‚àí1/2(1 + 1
2œµ) as x ‚Üí0, which clearly does not
satisfy the boundary condition y(0) = 0. We must therefore introduce a boundary
layer at x = 0, across which y adjusts to satisfy the boundary condition. The idea
is that, in some small neighbourhood of x = 0, the term œµy‚Ä≤‚Ä≤, which we neglected
at leading order, becomes important because y varies rapidly.
We rescale by deÔ¨Åning x = œµŒ±X, with Œ± > 0 (so that x ‚â™1) and X = O(1) as
œµ ‚Üí0, and write y(x) = Y (X) for X = O(1). Substituting this into (12.11) gives
œµ1‚àí2Œ± d 2Y
dX2 + 2œµ‚àíŒ± dY
dX ‚àíY = 0.
Since Œ± > 0, the second term in this equation is large, and to obtain an asymptotic
balance at leading order we must have œµ1‚àí2Œ± = O(œµ‚àíŒ±), which means that 1‚àí2Œ± =
‚àíŒ±, and hence Œ± = 1. So x = œµX,
d 2Y
dX2 + 2dY
dX ‚àíœµY = 0,
(12.14)
and Y (0) = 0. It is usual to refer to the region where œµ ‚â™x ‚©Ω1 as the outer
region, with outer solution y(x), and the boundary layer region where x = O(œµ)
as the inner region with inner solution Y (X). The other boundary condition
is to be applied at x = 1. However, x = 1 does not lie in the inner region, where
x = O(œµ). In order to Ô¨Åx a second boundary condition for (12.14), we will have to
make sure that the solution in the inner region is consistent, in a sense that we will
make clear below, with the solution in the outer region, which does satisfy y(1) = 1.
We now expand
Y (X) = Y0(X) + œµY1(X) + O(œµ2).
At leading order, Y ‚Ä≤‚Ä≤
0 + 2Y ‚Ä≤
0 = 0, to be solved subject to Y0(0) = 0. The solution is
Y0 = A(1 ‚àíe‚àí2X),
for some constant A. At leading order, we now know that
y ‚àºe(x‚àí1)/2
for œµ ‚â™x ‚©Ω1
(the outer expansion),
Y ‚àºA(1 ‚àíe‚àí2X)
for X = O(1), x = O(œµ)
(the inner expansion).
For these two expansions to be consistent with each other, we must have
lim
X‚Üí‚àûY (X) = lim
x‚Üí0 y(x),
(12.15)
which gives A = e‚àí1/2. We will see below, where we make this vague notion of
‚Äúconsistency‚Äù more precise, that this is correct.
At O(œµ) we obtain the equation for Y1(X) as
Y ‚Ä≤‚Ä≤
1 + 2Y ‚Ä≤
1 = Y0 = A(1 ‚àíe‚àí2X).

310
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Integrating this once gives
Y ‚Ä≤
1 + 2Y1 = A

X + 1
2e‚àí2X

+ c1,
for some constant c1. This can now be solved using an integrating factor, and the
solution that satisÔ¨Åes Y1(0) = 0 is
Y1 = 1
2AX(1 + e‚àí2X) ‚àíc2(1 ‚àíe‚àí2X),
for some constant c2, which we need to determine. To summarize, the two-term
asymptotic expansions are
y ‚àºe(x‚àí1)/2 + 1
2œµ(1 ‚àíx)e(x‚àí1)/2 for œµ ‚â™x ‚©Ω1,
Y ‚àºA(1 ‚àíe‚àí2X) + œµ
1
2AX(1 + e‚àí2X) ‚àíc2(1 ‚àíe‚àí2X)
	
for X = O(1), x = O(œµ).
We can determine the constants A and c2 by forcing the two expansions to be
consistent in the sense that they should be equal in an intermediate region or
overlap region, where œµ ‚â™x ‚â™1. The point is that in such a region we expect
both expansions to be valid.
We deÔ¨Åne x = œµŒ≤ ÀÜX with 0 < Œ≤ < 1, and write y = ÀÜY ( ÀÜX). In terms of the
intermediate variable, ÀÜX, the outer expansion becomes
ÀÜY ‚àºe‚àí1/2 exp
1
2œµŒ≤ ÀÜX

+ 1
2œµ(1 ‚àíœµŒ≤ ÀÜX)e‚àí1/2 exp
1
2œµŒ≤ ÀÜX

.
When ÀÜX = O(1), we can expand the exponentials as Taylor series, and Ô¨Ånd that
ÀÜY = e‚àí1/2

1 + 1
2œµŒ≤ ÀÜX

+ 1
2e‚àí1/2œµ + o(œµ).
(12.16)
Since x = œµX = œµŒ≤ ÀÜX gives X = œµŒ≤‚àí1 ÀÜX, the inner expansion is
ÀÜY ‚àºA

1 ‚àíexp

‚àí2œµŒ≤‚àí1 ÀÜX

+œµ
1
2AœµŒ≤‚àí1 ÀÜX

1 + exp

‚àí2œµŒ≤‚àí1 ÀÜX

‚àíc2

1 ‚àíexp

‚àí2œµŒ≤‚àí1 ÀÜX

.
Now, since exp(‚àí2œµŒ≤‚àí1 ÀÜX) = o(œµn) for all n > 0 (it is exponentially small for Œ≤ < 1),
we have
ÀÜY = A + 1
2AœµŒ≤ ÀÜX ‚àíc2œµ + o(œµ).
(12.17)
Since (12.16) and (12.17) must be identical, we need A = e‚àí1/2, consistent with
the crude assumption, (12.15), that we made above, and also c2 = ‚àí1
2e‚àí1/2. This
process, whereby we make the inner and outer expansions consistent, is known
as asymptotic matching, and the inner and outer expansions are known as
matched asymptotic expansions. A comparison between the one-term inner
and outer solutions and the exact solution is given in Figure 12.4. It should be

12.2 ORDINARY DIFFERENTIAL EQUATIONS
311
clear that the inner expansion is a poor approximation in the outer region and
vice versa. A little later, we will show how to construct, using the inner and outer
expansions, a composite expansion that is uniformly valid for 0 ‚©Ωx ‚©Ω1.
Fig. 12.4. Exact and asymptotic solutions of (12.11) when œµ = 0.25.
Van Dyke‚Äôs Matching Principle
The use of an intermediate variable in an overlap region can get very tedious in
more complicated problems. A method that works most, but not all, of the time,
and is much easier to use, is Van Dyke‚Äôs matching principle. This principle is
much easier to use than to explain, but let‚Äôs start with the explanation.
Let
y(x) ‚àº
N

n=0
œÜn(œµ)yn(x)
be the outer expansion and
Y (X) ‚àº
N

n=0
œàn(œµ)Yn(X)
be the inner expansion with respect to the asymptotic sequences œÜn(œµ) and œàn(œµ),
with x = f(œµ)X. In order to analyze how the outer expansion behaves in the inner
region, we can write y(x) in terms of X = x/f(œµ), and retain M terms in the
resulting asymptotic expansion. We denote this by y(N,M), the M th order inner
approximation of the outer expansion. Similarly, we can write Y (X) in terms of x,

312
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
and retain M terms in the resulting expansion. We denote this by Y (N,M), the M th
order outer approximation of the inner expansion. Van Dyke‚Äôs matching principle
states that y(N,M) = Y (M,N). Let‚Äôs see how this works for our example.
In terms of the outer variable, the inner expansion is
Y (X) ‚àºA(1 ‚àíe‚àí2x/œµ) + œµ
1
2Ax
œµ

1 + e‚àí2x/œµ
‚àíc2

1 ‚àíe‚àí2x/œµ	
‚àºY (2,2) = A + 1
2Ax ‚àíc2œµ,
for x = O(1). In terms of the inner variable, the outer expansion is
y(x) ‚àºexp

‚àí1
2 + 1
2œµX

+ 1
2œµ (1 ‚àíœµX) exp

‚àí1
2 + 1
2œµX

‚àºy(2,2) = e‚àí1/2

1 + 1
2œµX + 1
2œµ

,
for X = O(1). In terms of the outer variable,
y(2,2) = e‚àí1/2

1 + 1
2x + 1
2œµ

.
Van Dyke‚Äôs matching principle states that Y (2,2) = y(2,2), and therefore gives A =
e‚àí1/2 and c2 = ‚àí1
2e‚àí1/2 rather more painlessly than before.
Composite Expansions
Although we now know how to construct inner and outer solutions, valid in the
inner and outer regions, it would be more useful to have an asymptotic solution
valid uniformly across the whole domain of solution, 0 ‚©Ωx ‚©Ω1 in the example. We
can construct such a uniformly valid composite expansion by using the inner
and outer expansions. We simply need to add the two expansions and subtract
the expression that gives their overlap.
The overlap is just the expression that
appears to the appropriate order in the intermediate region, (12.17) or (12.16),
or equivalently the matched part, y(2,2) or Y (2,2). For our example problem, the
one-term composite expansion is
y ‚àºy0 + Y0 ‚àíy(1,1) = e(x‚àí1)/2 + e‚àí1/2(1 ‚àíe‚àí2X) ‚àíe‚àí1/2
= e(x‚àí1)/2 ‚àíe‚àí1/2‚àí2x/œµ for 0 ‚©Ωx ‚©Ω1 as œµ ‚Üí0.
This composite expansion is shown in Figure 12.4, and shows good agreement with
the exact solution across the whole domain, as expected. Note that, in terms of
Van Dyke‚Äôs matching principle, we can write the composite solution of any order
as
y ‚àºy(M,N)
c
=
M

n=0
yn(x) +
N

n=0
Yn(X) ‚àíy(M,N).

12.2 ORDINARY DIFFERENTIAL EQUATIONS
313
The Location of the Boundary Layer
In our example, when we constructed the outer solution, we chose to satisfy the
boundary condition at x = 1 and assume that there was a boundary layer at x = 0.
Why is this correct? Let‚Äôs see what happens if we assume that there is a boundary
layer at x = x0. Strictly speaking, if x0 Ã∏= 0 and x0 Ã∏= 1 this is an interior layer.
We deÔ¨Åne scaled variables y(x) = Y (X) and x = x0 + œµŒ±X, with Œ± > 0 and Y ,
X = O(1) for œµ ‚â™1. As before, we Ô¨Ånd that we can only obtain an asymptotic
balance at leading order by taking Œ± = 1, so that x = x0 + œµX and
Y ‚Ä≤‚Ä≤ + 2Y ‚Ä≤ ‚àíœµ = 0.
At leading order, as before, Y0 = A0 + B0e‚àí2X. As X ‚Üí‚àí‚àû, Y0 becomes expo-
nentially large, and cannot be matched to the outer solution. This forces us to take
x0 = 0, since then this solution is only needed for X ‚©æ0, and, as we have seen, we
can construct an asymptotic solution.
Interior Layers
Singular perturbations of ordinary diÔ¨Äerential equations need not always result in
a boundary layer. As an example, consider
œµy‚Ä≤‚Ä≤ + 2xy‚Ä≤ + 2x = 0,
(12.18)
to be solved for ‚àí1 < x < 1, subject to the boundary conditions
y(‚àí1) = 2,
y(1) = 3.
(12.19)
We will try to construct the leading order solution for œµ ‚â™1. The leading order
outer solution satisÔ¨Åes 2x(y‚Ä≤ + 1) = 0, and hence y = k ‚àíx for some constant k. If
y(‚àí1) = 2 we need y = 1‚àíx, whilst if y(1) = 3 we need y = 4‚àíx. We clearly cannot
satisfy both boundary conditions with the same outer solution, so let‚Äôs look for a
boundary or interior layer at x = x0 by deÔ¨Åning y(x) = Y (X) and x = x0 + œµŒ±X,
with Y , X = O(1). In terms of these scaled variables, (12.18) becomes
œµ1‚àí2Œ±YXX + 2(x0 + œµŒ±X)(œµ‚àíŒ±YX + 1) = 0.
If x0 Ã∏= 0, for a leading order balance we need œµ1‚àí2Œ± = O(œµ‚àíŒ±), and hence Œ± = 1.
In this case, at leading order,
YXX + 2x0YX = 0,
and hence Y = A + Be‚àí2x0X. For x0 > 0 this grows exponentially as X ‚Üí‚àí‚àû,
whilst for x0 < 0 this grows exponentially as X ‚Üí‚àû. In either case, we cannot
match these exponentially growing terms with the outer solution. This suggests
that we need x0 = 0, when
œµ1‚àí2Œ±YXX + 2XYX + 2œµŒ±X = 0.
For a leading order asymptotic balance we need Œ± = 1/2, and hence a boundary
layer with width of O(œµ1/2). At leading order,
YXX + 2XYX = 0,

314
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
which, after multiplying by the integrating factor, eX2, gives
d
dX

eX2YX

= 0,
and hence
Y = B + A
 X
‚àí‚àû
e‚àís2ds.
Now, since the interior layer is at x = 0, the outer solution must be
y =
 1 ‚àíx
for ‚àí1 ‚©Ωx < O(œµ1/2),
4 ‚àíx
for O(œµ1/2) < x ‚©Ω1.
Since y ‚Üí4 as x ‚Üí0+ and y ‚Üí1 as x ‚Üí0‚àí, we must have Y ‚Üí4 as X ‚Üí‚àûand
Y ‚Üí1 as X ‚Üí‚àí‚àû. This allows us to Ô¨Åx the constants A and B and Ô¨Ånd that
Y (X) = 1 +
3
‚àöœÄ
 X
‚àí‚àû
e‚àís2ds = 1
2 (5 + 3 erf (x)) ,
which leads to the one-term composite solution
y ‚àºyc = ‚àíx + 1
2

5 + 3 erf
 x
‚àöœµ
	
for ‚àí1 ‚©Ωx ‚©Ω1 and œµ ‚â™1.
(12.20)
This is illustrated in Figure 12.5 for various values of œµ. Note that the boundary
conditions at x = ¬±1 are only satisÔ¨Åed by the composite expansion at leading order.
12.2.3
Nonlinear Problems
Example 1: Elliptic functions of large period
As we have already seen in Section 9.4, the Jacobian elliptic function x = sn(t; k)
satisÔ¨Åes the equation
dx
dt =
#
1 ‚àíx2#
1 ‚àík2x2,
(12.21)
subject to x = 0 when t = 0, and has periodic solutions for k Ã∏= 1. When k = 1,
the solution that corresponds to sn(t; k) is a heteroclinic path that connects the
equilibrium points (¬±1, 0, 0) in the phase portrait shown in Figure 9.18, and hence
the period tends to inÔ¨Ånity as k ‚Üí1. When k is close to unity, it seems reasonable
to assume that the period of the solution is large but Ô¨Ånite. Can we quantify this?
Let‚Äôs assume that k2 = 1 ‚àíŒ¥2, with Œ¥ ‚â™1, and seek an asymptotic solution for
the Ô¨Årst quarter period of x(t), with 0 ‚©Ωx ‚©Ω1. Figure 12.6 shows sn(t; k) for
various values of Œ¥, and we can see that the period does increase as Œ¥ decreases and
k approaches unity. The function sn(t; k) is available in MATLAB as ellipj. The
quarter period is simply the value of t when sn(t; k) reaches its Ô¨Årst maximum.
We seek an asymptotic solution of the form
x = x0 + Œ¥2x1 + Œ¥4x2 + O

Œ¥6
.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
315
Fig. 12.5. The composite asymptotic solution, (12.20), of (12.18).
Using a binomial expansion, (12.21) is
dx
dt =

1 ‚àíx2 
1 + Œ¥2
x2
1 ‚àíx2
1/2
= 1 ‚àíx2 + 1
2Œ¥2x2 + 1
8Œ¥4
x4
1 ‚àíx2 + O(Œ¥6).
This binomial expansion is only valid when x is not too close unity, so we should
expect any asymptotic expansion that we develop to become nonuniform as x ‚Üí1,
and we treat this as the outer expansion.
At leading order,
dx0
dt = 1 ‚àíx2
0,
subject to x0(0) = 0,
which has solution x0 = tanh t. At O(Œ¥2),
dx1
dt = ‚àí2x0x1 + 1
2x2
0 = ‚àí2 tanh tx1 + 1
2 tanh2 t,
subject to x1(0) = 0.
Using the integrating factor cosh2 t, we can Ô¨Ånd the solution
x1 = 1
4

tanh t ‚àít sech2t

.
We can now see that
x ‚àº1 ‚àí2e‚àí2t + 1
4Œ¥2 + O(Œ¥4) as t ‚Üí‚àû.

316
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.6. The Jacobian elliptic function sn(t; k) for various values of Œ¥.
Although x approaches unity as t ‚Üí‚àû, there is no nonuniformity in this expansion,
so we need to go to O(Œ¥4). At this order,
dx2
dt = ‚àíx2
1 ‚àí2x0x2 + x0x1 ‚àí1
8

x4
0
1 ‚àíx2
0

subject to x2(0) = 0.
Solving this problem would be extremely tedious. Fortunately, we don‚Äôt really want
to know the exact expression for x2, just its behaviour as t ‚Üí‚àû. Using the known
behaviour of the various hyperbolic functions, we Ô¨Ånd that
dx2
dt + 2x2 ‚àº‚àí1
32e2t as t ‚Üí‚àû,
and hence from solving this simple linear equation,
x2 ‚àº‚àí1
128e2t as t ‚Üí‚àû.
This shows that
x ‚àº1 ‚àí2e‚àí2t + 1
4Œ¥2 ‚àí
1
128Œ¥4e2t + O(Œ¥6) as t ‚Üí‚àû.
(12.22)
We can now see that the fourth term in this expansion becomes comparable to the
third when Œ¥4e2t = O(Œ¥2), and hence as t ‚Üílog(1/Œ¥), when x = 1 + O(Œ¥2).

12.2 ORDINARY DIFFERENTIAL EQUATIONS
317
We therefore deÔ¨Åne new variables for an inner region close to x = 1 as
x = 1 ‚àíŒ¥2X,
t = log
1
Œ¥

+ T.
On substituting these inner variables into (12.21), we Ô¨Ånd that, at leading order,
dX
dT = ‚àí
#
2X (2X + 1).
Using the substitution X = ¬ØX ‚àí1
4 brings this separable equation into a standard
form, and the solution is
X = 1
4 {cosh (K ‚àí2T) ‚àí1} .
(12.23)
We now need to determine the constant K by matching the inner solution,
(12.23), with the outer solution, whose behaviour as x ‚Üí1 is given by (12.22).
Writing the inner expansion in terms of the outer variables and retaining terms up
to O(Œ¥2) leads to
x = 1 ‚àí1
8eKe‚àí2t + 1
4Œ¥2 + O(Œ¥4),
for t = O(1). Comparing this with (12.22) shows that we need 1
8eKe‚àí2t = 2e‚àí2t,
and hence K = 4 log 2, which gives
x = 1 ‚àí1
4Œ¥2 {cosh (4 log 2 ‚àí2T) ‚àí1} + O(Œ¥4)
(12.24)
when T = O(1). From this leading order approximation, x = 1 when T = T0 =
2 log 2 + O(Œ¥2). This is the quarter period of the solution, so the period œÑ satisÔ¨Åes
1
4œÑ = log
1
Œ¥

+ T0,
and hence
œÑ = 4 log
4
Œ¥

+ O(Œ¥2),
for Œ¥ ‚â™1. We conclude that the period of the solution does indeed tend to inÔ¨Ånity
as Œ¥ ‚Üí0, k ‚Üí1‚àí, but only logarithmically fast. Figure 12.7 shows a comparison
between the exact and analytical solutions. The agreement is very good for all
Œ¥ ‚©Ω1. We produced this Ô¨Ågure using the MATLAB script
'
&
$
%
Texact = []; d = 10.^(-7:0.25:0); Tasymp = 4*log(4./d);
options = optimset(‚ÄôDisplay‚Äô,‚Äôoff‚Äô,‚ÄôTolX‚Äô, 10^-10);
for del = d
k = sqrt(1-del^2); T2 = 2*log(4/del);
Texact = [Texact 2*fzero(@ellipj,T2,options,k)];
end
plot(log10(d),Texact,log10(d),Tasymp, ‚Äô--‚Äô)
xlabel(‚Äôlog_{10}\delta‚Äô), ylabel(‚ÄôT‚Äô)
legend(‚Äôexact‚Äô,‚Äôasymptotic‚Äô)

318
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
This uses the MATLAB function fzero to Ô¨Ånd where the elliptic function is zero,
using the asymptotic expression as an initial estimate.
Note that the function
optimset allows us to create a variable options that we can pass to fzero as a
parameter, which controls the details of its execution. In this case, we turn oÔ¨Äthe
output of intermediate results, and set the convergence tolerance to 10‚àí10.
Fig. 12.7. A comparison of the exact period of the elliptic function sn(t; k) for k =
‚àö
1 ‚àíŒ¥2.
Finally, by adding the solutions in the inner and outer regions and subtracting
the matched part, 1+ 1
4Œ¥2 ‚àí2e‚àí2t, we can obtain a composite expansion, uniformly
valid for 0 ‚©Ωt ‚©Ω1
4œÑ = log(4/Œ¥) + O(Œ¥2), as
x = tanh t + 2e‚àí2t + 1
4Œ¥2

tanh t ‚àít sech2t ‚àícosh

log
16
Œ¥2

‚àí2t
	
+ O(Œ¥4).
Example 2: A thermal ignition problem
Many materials decompose to produce heat. This decomposition is usually more
rapid the higher the temperature. This leads to the possibility of thermal ignition.
As a material gets hotter, it releases heat more rapidly, which heats it more rapidly,
and so on. This positive feedback mechanism can lead to the unwanted, and poten-
tially disastrous, combustion of many common materials, ranging from packets of
powdered milk to haystacks. The most common physical mechanism that can break
this feedback loop is the diÔ¨Äusion of heat through the material and out through
its surface. The rate of heat production due to decomposition is proportional to
the volume of the material and the rate of heat loss from its surface proportional
to surface area. For a suÔ¨Éciently large volume of material, heat production dom-
inates heat loss, and the material ignites. Determining the critical temperature

12.2 ORDINARY DIFFERENTIAL EQUATIONS
319
below which it is safe to store a potentially combustible material is an important
and diÔ¨Écult problem.‚Ä†
We now want to develop a mathematical model of this problem. In Section 2.6.1,
we showed how to derive the diÔ¨Äusion equation, (2.12), which governs the Ô¨Çow of
heat in a body.
We now need to include the eÔ¨Äect of a chemical reaction that
produces R(x, y, z, t) units of heat, per unit volume, per unit time, by adding a
term R Œ¥t Œ¥x Œ¥y Œ¥z to the right hand side of (2.11). On taking the limit Œ¥t, Œ¥x, Œ¥y,
Œ¥z ‚Üí0, we arrive at
œÅc‚àÇT
‚àÇt = ‚àí‚àá¬∑ Q + R,
and hence, for a steady state solution (‚àÇ/‚àÇt = 0), and since Fourier‚Äôs law of heat
conduction is Q = ‚àík‚àáT,
k‚àá2T + R = 0.
(12.25)
The rate of combustion of the material can be modelled using the Arrhenius law,
R = Ae‚àíTa/T , where A is a constant and Ta is the activation temperature, also
a constant. It is important to note that T is the absolute temperature here.
The Arrhenius law can be derived from Ô¨Årst principles using statistical mechanics,
although we will not attempt this here (see, for example, Flowers and Mendoza,
1970). Figure 12.8 shows that the reaction rate is zero at absolute zero (T = 0),
and remains small until T approaches the activation temperature, Ta, when it
increases, with R ‚ÜíA as T ‚Üí‚àû. After deÔ¨Åning u = T/Ta and rescaling distances
Fig. 12.8. The Arrhenius reaction rate law.
‚Ä† For more background on combustion theory, see Buckmaster and Ludford (1982).

320
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
with
#
k/A, we arrive at the nonlinear partial diÔ¨Äerential equation
‚àá2u + e‚àí1/u = 0.
For a uniform sphere of combustible material with a spherically-symmetric temper-
ature distribution, this becomes
d 2u
dr2 + 2
r
du
dr + e‚àí1/u = 0,
(12.26)
subject to
du
dr = 0 at r = 0, u = ua at r = ra.
(12.27)
Here, ua is the dimensionless absolute temperature of the surroundings and ra the
dimensionless radius of the sphere. Note that, as we discussed earlier, we would
expect that the larger ra, the smaller ua must be to prevent the ignition of the
sphere.‚Ä† A positive solution of this boundary value problem represents a possible
steady state solution in which these two physical processes are in balance. If no
such steady state solution exists, we can conclude that the material will ignite. A
small trick that makes the study of this problem easier is to replace (12.27) with
du
dr = 0,
u = œµ at r = 0.
(12.28)
We can then solve the initial value problem given by (12.26) and (12.28) for a given
value of œµ, the dimensionless temperature at the centre of the sphere, and then
determine the corresponding value of ua = u(ra). Our task is therefore to construct
an asymptotic solution of the initial value problem given by (12.26) and (12.28)
when œµ ‚â™1. Note that by using the integrating factor r2, we can write (12.26) as
du
dr = ‚àí1
r2
 r
0
s2e‚àí1/u(s) ds < 0,
and hence conclude that u is a monotone decreasing function of r. The temperature
of the sphere decreases from centre to surface.
Asymptotic analysis: Region I
Since u = œµ at r = 0, we deÔ¨Åne a new variable ÀÜu = u/œµ, with ÀÜu = O(1) for œµ ‚â™1.
In terms of this variable, (12.26) and (12.28) become
d 2ÀÜu
dr2 + 2
r
dÀÜu
dr + 1
œµ exp

‚àí1
œµÀÜu

= 0,
subject to
dÀÜu
dr = 0,
ÀÜu = 1 at r = 0.
‚Ä† For all the technical details of this problem, which was Ô¨Årst studied by Gel‚Äôfand (1963), see
Billingham (2000).

12.2 ORDINARY DIFFERENTIAL EQUATIONS
321
At leading order,
d 2ÀÜu
dr2 + 2
r
dÀÜu
dr = 0,
(12.29)
which has the trivial solution ÀÜu = 1. We‚Äôre going to need more than this to be able
to proceed, so let‚Äôs look for an asymptotic expansion of the form
ÀÜu = 1 + œÜ1(œµ)ÀÜu1 + œÜ2(œµ)ÀÜu2 + ¬∑ ¬∑ ¬∑ ,
where œÜ2 ‚â™œÜ1 ‚â™1 are to be determined. As we shall see, we need a three-term
asymptotic expansion to be able to determine the scalings for the next asymptotic
region.
Firstly, note that
1
œµ e‚àí1/œµÀÜu ‚àº1
œµ exp

‚àí1
œµ (1 + œÜ1ÀÜu1)‚àí1
	
‚àº1
œµ exp

‚àí1
œµ (1 ‚àíœÜ1ÀÜu1)
	
‚àº1
œµ e‚àí1/œµ exp
œÜ1
œµ ÀÜu1

‚àº1
œµ e‚àí1/œµ

1 + œÜ1
œµ ÀÜu1

,
provided that œÜ1 ‚â™œµ, which we can check later. Equation (12.26) then shows that
œÜ1
d 2ÀÜu1
dr2 + 2
r
dÀÜu1
dr

+ œÜ2
d 2ÀÜu2
dr2 + 2
r
dÀÜu2
dr

‚àº‚àí1
œµ e‚àí1/œµ

1 + œÜ1
œµ ÀÜu1

.
(12.30)
In order to obtain a balance of terms, we therefore take
œÜ1 = 1
œµ e‚àí1/œµ ‚â™œµ,
œÜ2 = 1
œµ2 e‚àí1/œµœÜ1 = 1
œµ3 e‚àí2/œµ,
and hence expand
ÀÜu = 1 + 1
œµ e‚àí1/œµÀÜu1 + 1
œµ3 e‚àí2/œµÀÜu2 + ¬∑ ¬∑ ¬∑ .
Now, using (12.30),
d 2ÀÜu1
dr2 + 2
r
dÀÜu1
dr = ‚àí1,
(12.31)
subject to
dÀÜu1
dr = ÀÜu1 = 0 at r = 0.
Using the integrating factor r2, we Ô¨Ånd that the solution is ÀÜu1 = ‚àí1
6r2. Similarly,
d 2ÀÜu2
dr2 + 2
r
dÀÜu2
dr = 1
6r2,
subject to
dÀÜu2
dr = ÀÜu2 = 0 at r = 0,
and hence ÀÜu2 =
1
120r4. This means that
ÀÜu ‚àº1 ‚àí1
6œµe‚àí1/œµr2 +
1
120œµ3 e‚àí2/œµr4
(12.32)

322
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
for œµ ‚â™1. This expansion is not uniformly valid, since for suÔ¨Éciently large r the
second term is comparable to the Ô¨Årst, when r = O(œµ1/2e1/2œµ), and the third is
comparable to the second, when r = O(œµe1/2œµ) ‚â™œµ1/2e1/2œµ. As r increases, the Ô¨Årst
nonuniformity to occur is therefore when r = O(œµe1/2œµ) and u = œµ + O(œµ2). Note
that this is why we needed a three-term expansion to work out the scalings for the
next asymptotic region.
What would have happened if we had taken only a two-term expansion and
looked for a new asymptotic region with r = O(œµ1/2e1/2œµ) and u = O(œµ)? If you try
this, you will Ô¨Ånd that it is impossible to match the solutions in the new region to the
solutions in region I. After some thought, you notice that the equations at O(1) and
O( 1
œµ e‚àí1/œµ) in region I, (12.29) and (12.31), do not depend at all on the functional
form of the term e‚àí1/œµÀÜu, which could be replaced by e‚àí1/œµ without aÔ¨Äecting (12.29)
or (12.31). This is a sign that we need another term in the expansion in order to
capture the eÔ¨Äect of the only nonlinearity in the problem.
Asymptotic analysis: Region II
In this region we deÔ¨Åne scaled variables u = œµ+œµ2U, r = œµe1/2œµR, with U = O(1)
and R = O(1) for œµ ‚â™1. At leading order, (12.26) becomes
d 2U
dR2 + 2
R
dU
dR + eU = 0,
(12.33)
to be solved subject to the matching condition
U ‚àº‚àí1
6R2 as R ‚Üí0.
(12.34)
Equation (12.33) is nonlinear and nonautonomous, which usually means that we
must resort to Ô¨Ånding a numerical solution. However, we have seen in Chapter 10
that we can often make progress using group theoretical methods. Equation (12.33)
is invariant under the transformation U ‚ÜíU + k, R ‚Üíe‚àík/2R. We can therefore
make the nonlinear transformation
p(s) = e‚àí2seU,
q(s) = 2 + e‚àís dU
dR,
R = e‚àís,
after which (12.33) and (12.34) become
dp
ds = ‚àípq,
dq
ds = p + q ‚àí2,
(12.35)
subject to
p ‚àºe‚àí2s,
q ‚àº2 ‚àí1
3e‚àí2s as s ‚Üí‚àû.
(12.36)
The problem is still nonlinear, but is now autonomous, so we can use the phase
plane methods that we studied in Chapter 9.
There are two Ô¨Ånite equilibrium points, at P1 = (0, 2) and P2 = (2, 0) in the
(p, q) phase plane.
After determining the Jacobian at each of these points and
calculating the eigenvalues in the usual way, we Ô¨Ånd that P1 is a saddle point and
P2 is an unstable, anticlockwise spiral. Since (12.36) shows that we are interested

12.2 ORDINARY DIFFERENTIAL EQUATIONS
323
in a solution that asymptotes to P1 as s ‚Üí‚àû, this solution must be represented by
one of the stable separatrices of P1. Furthermore, since p = e‚àí2seU > 0, the unique
integral path that represents the solution is the stable separatrix of P1 that lies in
the half plane p > 0. What happens to this separatrix as s ‚Üí‚àí‚àû? A sensible, and
correct, guess would be that it asymptotes to the unstable spiral at P2, as shown in
Figure 12.9, for which we calculated the solution numerically using MATLAB (see
Section 9.3.4). The proof that this is the case is Exercise 9.17, which comes with
some hints.
Fig. 12.9. The behaviour of the solution of (12.35) subject to (12.36) in the (p, q)-phase
plane.
Since the solution asymptotes to P2, we can determine its behaviour as s ‚Üí‚àí‚àû
by considering the local solution there. The eigenvalues of P2 are 1
2(1 ¬± i
‚àö
7), and
therefore
p ‚àº2 + Aes/2 sin
%‚àö
7
2 s + B
&
as s ‚Üí‚àí‚àû,
for some constants A and B. Since U = 2s + log p and s = ‚àílog R, this shows that
U ‚àº‚àí2 log R + log 2 ‚àí
A
2
‚àö
R
sin
%‚àö
7
2 log R ‚àíB
&
as R ‚Üí‚àû.
(12.37)
We conclude that
u ‚àºœµ + œµ2(log 2 ‚àí2 log R) as R ‚Üí‚àû,
for œµ ‚â™1. When log R = O(œµ‚àí1), the second term is comparable to the Ô¨Årst term,
and we have a further nonuniformity.

324
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Asymptotic analysis: Region III
We deÔ¨Åne scaled variables u = œµ ¬ØU and S = œµ log R = ‚àí1
2 ‚àíœµ log œµ + œµ log r,
r = œµe1/2œµeS/œµ, with U = O(1) and S = O(1) for œµ ‚â™1. In terms of these variables,
(12.26) becomes
œµd 2 ¬ØU
dS2 + d ¬ØU
dS + exp
1
œµ

‚àí1
¬ØU + 1 + 2S
	
= 0.
(12.38)
We expand ¬ØU as
¬ØU = ¬ØU0 + œµ ¬ØU1 + ¬∑ ¬∑ ¬∑ .
To Ô¨Ånd the matching conditions, we write expansion (12.37) in region I in terms of
the new variables and retain terms up to O(œµ), which gives
¬ØU0 ‚àº1 ‚àí2S,
¬ØU1 ‚àºlog 2 as S ‚Üí0.
(12.39)
At leading order, the solution is given by the exponential term in (12.38) as
¬ØU0 =
1
1 + 2S ,
which satisÔ¨Åes (12.39). At O(œµ), we Ô¨Ånd that
d ¬ØU0
dS + exp

(1 + 2S)2 ¬ØU1

= 0,
and hence that
¬ØU1 =
1
(1 + 2S)2 log

2
(1 + 2S)2
	
,
which also satisÔ¨Åes (12.39). We conclude that
u =
œµ
1 + 2S +
œµ2
(1 + 2S)2 log

2
(1 + 2S)2
	
+ O(œµ3),
for S = O(1) and œµ ‚â™1. This expansion remains uniform, with u ‚Üí0 as S ‚Üí‚àû,
and hence R ‚Üí‚àû, and the solution is complete.
We can now determine ua = u(ra). If ra ‚â™œµe1/2œµ, r = ra lies in region I, so that
ua ‚àºœµ ‚àí1
6e‚àí1/œµr2
a ‚àºœµ. In other words, for ua suÔ¨Éciently small that ra ‚â™uae1/2ua,
a steady state solution is always available, and we predict that the sphere will not
ignite.
If ra = O(œµe1/2œµ), we need to consider the solution in region II. The oscillations
in p lead to oscillations in ua as a function of œµ, as shown in Figure 12.10.
In
particular, the fact that p < pmax ‚âà3.322, as can be seen in Figure 12.9, leads to
a maximum value of ua for which a steady state solution is available, namely
uamax ‚àºœµ + œµ2 log
pmaxœµ2e1/œµ
r2a

.
(12.40)
Finally, if ra = O(œµe3/2œµ), the solution in region III shows that
ua ‚àº
œµ
1
2 + 2œµ log(ra/œµ) < uamax.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
325
We conclude that uamax (the critical ignition temperature or critical storage
temperature) gives an asymptotic approximation to the hottest ambient temper-
ature at which a steady state solution exists, and hence at which the material will
not ignite.
Fig. 12.10. The ambient temperature, ua, as a function of œµ in region II when ra = 109.
12.2.4
The Method of Multiple Scales
Let‚Äôs now return to solving a simple, linear, constant coeÔ¨Écient ordinary dif-
ferential equation that, at Ô¨Årst sight, seems like a regular perturbation problem.
Consider
¬®y + 2œµ Àôy + y = 0,
(12.41)
to be solved subject to
y(0) = 1,
Àôy(0) = 0,
(12.42)
for t ‚©æ0, where a dot denotes d/dt. Since this is an initial value problem, we can
think of y developing as a function of time, t. As usual, we expand
y = y0(t) + œµy1(t) + O(œµ2)
for œµ ‚â™1. At leading order, ¬®y0 + y0 = 0, subject to y0(0) = 1, Àôy0(0) = 0, which has
solution y0 = cos t. At O(œµ),
¬®y1 + y1 = ‚àí2 Àôy0 = 2 sin t,

326
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
subject to y1(0) = 0, Àôy1(0) = 0. After noting that the particular integral solution
of this equation takes the form y1p = kt cos t for some constant k, we Ô¨Ånd that
y1 = ‚àít cos t + sin t. This means that
y(t) ‚àºcos t + œµ (‚àít cos t + sin t)
(12.43)
as œµ ‚Üí0. As t ‚Üí‚àû, the ratio of the second term to the Ô¨Årst term in this expansion
is asymptotic to œµt, and is therefore no longer small when t = O(œµ‚àí1). We conclude
that the asymptotic solution given by (12.43) is only valid for t ‚â™œµ‚àí1.
To see where the problem lies, let‚Äôs examine the exact solution of (12.41) subject
to the boundary conditions (12.42), which is
y = e‚àíœµt

cos
#
1 ‚àíœµ2t +
œµ
‚àö
1 ‚àíœµ2 sin
#
1 ‚àíœµ2t

.
(12.44)
As we can see from Figure 12.11, the solution is a decaying oscillation, with the de-
cay occurring over a timescale of O(œµ‚àí1). At leading order, (12.41) is an undamped,
linear oscillator. The term 2œµ Àôy represents the eÔ¨Äect of weak damping, which slowly
reduces the amplitude of the oscillation. The problem with the asymptotic expan-
sion (12.43) is that, although it correctly captures the fact that e‚àíœµt ‚àº1 ‚àíœµt for
œµ ‚â™1 and t ‚â™œµ‚àí1, we need to keep the exponential rather than its Taylor series
expansion if we are to construct a solution that is valid when t = O(œµ‚àí1). Fig-
ure 12.11 shows that the two-term asymptotic expansion, (12.43), rapidly becomes
inaccurate once t = O(œµ‚àí1).
Fig. 12.11. The exact solution, (12.44), two-term asymptotic solution, (12.43), and one-
term multiple scales solution (12.50) of (12.41) when œµ = 0.1.
The method of multiple scales, in its most basic form, consists of deÔ¨Åning a
new slow time variable, T = œµt, so that when t = O(1/œµ), T = O(1), and the

12.2 ORDINARY DIFFERENTIAL EQUATIONS
327
slow decay can therefore be accounted for. We then look for an asymptotic solution
y = y0(t, T) + œµy1(t, T) + O(œµ2),
with each term a function of both t, to capture the oscillation, and T, to capture
the slow decay. After noting that
d
dt = ‚àÇ
‚àÇt + œµ ‚àÇ
‚àÇT ,
(12.41) becomes
‚àÇ2y
‚àÇt2 + 2œµ ‚àÇ2y
‚àÇt‚àÇT + œµ2 ‚àÇ2y
‚àÇT 2 + 2œµ‚àÇy
‚àÇt + 2œµ2 ‚àÇy
‚àÇT + y = 0,
(12.45)
to be solved subject to
y(0, 0) = 1,
‚àÇy
‚àÇt (0, 0) + œµ ‚àÇy
‚àÇT (0, 0) = 0.
(12.46)
At leading order,
‚àÇ2y0
‚àÇt2 + y0 = 0,
subject to
y0(0, 0) = 1,
‚àÇy0
‚àÇt (0, 0) = 0.
Although this is a partial diÔ¨Äerential equation, the only derivatives are with respect
to t, so we can solve as if it were an ordinary diÔ¨Äerential equation in t. However,
we must remember that the ‚Äòconstants‚Äô of integration can actually be functions of
the other variable, T. This means that
y0 = A0(T) cos t + B0(T) sin t,
and the boundary conditions show that
A0(0) = 1,
B0(0) = 0.
(12.47)
The functions A0(T) and B0(T) are, as yet, undetermined.
At O(œµ),
‚àÇ2y1
‚àÇt2 + y1= ‚àí2 ‚àÇ2y0
‚àÇt‚àÇT ‚àí2‚àÇy0
‚àÇt = 2
dA0
dT + A0

sin t ‚àí2
dB0
dT + B0

cos t.
(12.48)
Because of the appearance on the right hand side of the equation of the terms
cos t and sin t, which are themselves solutions of the homogeneous version of the
equation, the particular integral solution will involve the terms t sin t and t cos t. As
we have seen, it is precisely terms of this type that lead to a nonuniformity in the
asymptotic expansion when t = O(œµ‚àí1). The terms proportional to sin t and cos t
in (12.48) are known as secular terms and, to keep the asymptotic expansion

328
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
uniform, we must eliminate them by choosing A0 and B0 appropriately. In this
example, we need
dA0
dT + A0 = 0,
dB0
dT + B0 = 0,
(12.49)
and hence
y1 = A1(T) cos t + B1(T) sin t.
The initial conditions for (12.49) are given by (12.47), so the solutions are A0 = e‚àíT
and B0 = 0. We conclude that
y ‚àºe‚àíT cos t = e‚àíœµt cos t
(12.50)
for œµ ‚â™1. This asymptotic solution is consistent with the exact solution, (12.44),
and remains valid when t = O(œµ‚àí1), as can be seen in Figure 12.11. In fact, we will
see below that this solution is only valid for t ‚â™œµ‚àí2.
In order to proceed to Ô¨Ånd more terms in the asymptotic expansion using the
method of multiple scales, we can take the exact solution, (12.44), as a guide. We
know that
y ‚àºe‚àíœµt

cos

1 ‚àí1
2œµ2

t + œµ sin

1 ‚àí1
2œµ2

t
	
,
(12.51)
for œµ ‚â™1. This shows that the phase of the oscillation changes by an O(1) amount
when t = O(œµ‚àí2). In order to capture this, we seek a solution that is a function of
the two timescales
T = œµt,
œÑ =

1 + aœµ2 + bœµ3 + ¬∑ ¬∑ ¬∑

t,
with the constants a, b, . . . to be determined. Although this looks like a bit of a
cheat, since we are only doing this because we know the exact solution, this approach
works for a wide range of problems, including nonlinear diÔ¨Äerential equations.
In order to develop a one-term multiple scale expansion, we needed to consider
the solution up to O(œµ). This suggests that we will need to expand up to O(œµ2) to
construct a two-term multiple scales expansion, with
y = y0(œÑ, T) + œµy1(œÑ, T) + œµ2y2(œÑ, T) + O(œµ3).
After noting that
d
dt = œµ ‚àÇ
‚àÇT +

1 + aœµ2 + bœµ3 + ¬∑ ¬∑ ¬∑
 ‚àÇ
‚àÇœÑ ,
equation (12.41) becomes
‚àÇ2y
‚àÇœÑ 2 + 2aœµ2 ‚àÇ2y
‚àÇœÑ 2 + 2œµ ‚àÇ2y
‚àÇœÑ‚àÇT + œµ2 ‚àÇ2y
‚àÇT 2 + 2œµ‚àÇy
‚àÇœÑ + 2œµ2 ‚àÇy
‚àÇT + y + O(œµ3) = 0,
(12.52)
to be solved subject to
y(0, 0) = 1,

1 + aœµ2 + bœµ3 + ¬∑ ¬∑ ¬∑
 ‚àÇy
‚àÇœÑ (0, 0) + œµ ‚àÇy
‚àÇT (0, 0) = 0.
(12.53)
We already know that
y0 = e‚àíT cos œÑ,
y1 = A1(T) cos œÑ + B1(T) sin œÑ.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
329
At O(œµ2),
‚àÇ2y2
‚àÇœÑ 2 + y2 = ‚àí2a‚àÇ2y0
‚àÇœÑ 2 ‚àí2 ‚àÇ2y1
‚àÇœÑ‚àÇT + ‚àÇ2y0
‚àÇT 2 ‚àí2‚àÇy1
‚àÇœÑ ‚àí2‚àÇy0
‚àÇT
= 2
dA1
dt + A1

sin œÑ ‚àí2
dB1
dt + B1 ‚àí

a + 1
2

e‚àíT
	
cos œÑ.
In order to remove the secular terms we need
dA1
dt + A1 = 0,
dB1
dt + B1 =

a + 1
2

e‚àíT .
At O(œµ) the boundary conditions are
y1(0, 0) = A1(0) = 0,
‚àÇy1
‚àÇœÑ (0, 0) + ‚àÇy0
‚àÇT (0, 0) = B1(0) ‚àí1 = 0,
and hence
A1 = 0,
B1 =

a + 1
2

Te‚àíT + e‚àíT .
This gives us
y ‚àºe‚àíT cos œÑ + œµ

a + 1
2

Te‚àíT + e‚àíT
	
sin œÑ.
However, the part of the O(œµ) term proportional to T will lead to a nonuniformity
in the expansion when T = O(œµ‚àí1), and we must therefore remove it by choosing
a = ‚àí1/2. We could have deduced this directly from the diÔ¨Äerential equation for
B1, since the term proportional to e‚àíT is secular. We conclude that
œÑ =

1 ‚àí1
2œµ2 + ¬∑ ¬∑ ¬∑

t,
and hence obtain (12.51), as expected.
Example 1: The van der Pol Oscillator
The governing equation for the van der Pol oscillator is
d 2y
dt2 + œµ(y2 ‚àí1)dy
dt + y = 0,
(12.54)
for t ‚©æ0. For œµ ‚â™1 this is a linear oscillator with a weak nonlinear term, œµ(y2‚àí1) Àôy.
For |y| < 1 this term tends to drive the oscillations to a greater amplitude, whilst
for |y| > 1, this term damps the oscillations. It is straightforward (at least if you‚Äôre
an electrical engineer!) to build an electronic circuit from resistors and capacitors
whose output is governed by (12.54). It was in this context that this system was Ô¨Årst
studied extensively as a prototypical nonlinear oscillator. It is also straightforward
to construct a forced van der Pol oscillator, which leads to a nonzero right hand
side in (12.54), and study the chaotic response of the circuit.
Since the damping is zero when y = 1, a reasonable guess at the behaviour of
the solution for œµ ‚â™1 would be that there is an oscillatory solution with unit

330
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
amplitude. Let‚Äôs investigate this plausible, but incorrect, guess by considering the
solution with initial conditions
y(0) = 1,
dy
dt (0) = 0.
(12.55)
We will use the method of multiple scales, and deÔ¨Åne a slow time scale, T = œµt. We
seek an asymptotic solution of the form
y = y0(t, T) + œµy1(t, T) + O(œµ2).
As for our linear example, we Ô¨Ånd that
y0 = A0(T) cos {t + œÜ0(T)} .
For this problem, it is more convenient to write the solution in terms of an ampli-
tude, A0(T), and phase, œÜ0(T). The boundary conditions show that
A0(0) = 1,
œÜ0(0) = 0.
(12.56)
At O(œµ),
‚àÇ2y1
‚àÇt2 + y1 = ‚àí2 ‚àÇ2y0
‚àÇt‚àÇT ‚àí(y2
0 ‚àí1)‚àÇy0
‚àÇt
= 2dA0
dT sin(t + œÜ0) + 2A0
dœÜ0
dT cos(t + œÜ0) + A0 sin(t + œÜ0)

A2
0 cos2(t + œÜ0) ‚àí1

.
In order to pick out the secular terms on the right hand side, we note that‚Ä†
sin Œ∏ cos2 Œ∏ = sin Œ∏ ‚àísin3 Œ∏ = sin Œ∏ ‚àí3
4 sin Œ∏ + 1
4 sin 3Œ∏ = 1
4 sin Œ∏ + 1
4 sin 3Œ∏.
This means that
‚àÇ2y1
‚àÇt2 + y1 =

2dA0
dT + 1
4A3
0 ‚àíA0
	
sin(t + œÜ0) + 2A0
dœÜ0
dT cos(t + œÜ0) + 1
4A3
0 sin 3(t + œÜ0).
To suppress the secular terms we therefore need
dœÜ0
dT = 0,
dA0
dT = 1
8A0(4 ‚àíA2
0).
Subject to the boundary conditions (12.56), the solutions are
œÜ0 = 0,
A0 = 2(1 + 3e‚àíT )‚àí1/2.
Therefore
y ‚àº2(1 + 3e‚àíT )‚àí1/2 cos t
for œµ ‚â™1, and we conclude that the amplitude of the oscillation actually tends to
2 as t ‚Üí‚àû, as shown in Figure 12.12.
‚Ä† To get cosn Œ∏ in terms of cos mŒ∏ for m = 1, 2, . . . , n, use einŒ∏ = cos nŒ∏ + i sin nŒ∏ = (eiŒ∏)n =
(cos Œ∏ + i sin Œ∏)n and equate real and imaginary parts.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
331
Fig. 12.12. The leading order solution of the van der Pol equation, (12.54), subject to
y(0) = 1, Àôy(0) = 0, when œµ = 0.1.
Example 2: Jacobian elliptic functions with almost simple harmonic behaviour
Let‚Äôs again turn our attention to the Jacobian elliptic function x = sn(t; k), which
satisÔ¨Åes (12.21). When k ‚â™1, this function is oscillatory and, at leading order,
performs simple harmonic motion. We can see this more clearly by diÔ¨Äerentiating
(12.21) and eliminating dx/dt to obtain
d 2x
dt2 + (1 + k2)x = 2k2x3.
(12.57)
The initial conditions, of which there must be two for this second order equation,
are x = 0 and, from (12.21), dx/dt = 1 when t = 0. Let‚Äôs now use the method
of multiple scales to see how this small perturbation aÔ¨Äects the oscillation after a
long time. As usual, we deÔ¨Åne T = k2t and x = x(t, T), in terms of which (12.57)
becomes
‚àÇ2x
‚àÇt2 + 2k2 ‚àÇ2x
‚àÇt‚àÇT + k4 ‚àÇ2x
‚àÇ2T + (1 + k2)x = 2k2x3.
(12.58)
We seek an asymptotic solution
x = x0(t, T) + k2x1(t, T) + O(k4).
At leading order
‚àÇ2x0
‚àÇt2 + x0 = 0,

332
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
subject to x0(0, 0) = 0 and ‚àÇx0
‚àÇt (0, 0) = 1. This has solution
x0 = A(T) sin {t + œÜ(t)} ,
and
A(0) = 1,
œÜ(0) = 0.
(12.59)
At O(k2),
‚àÇ2x1
‚àÇt2 + x1 = 2x3
0 ‚àíx0 ‚àí2 ‚àÇ2x0
‚àÇt‚àÇT
= 2A3 sin3(t + œÜ) ‚àíA sin(t + œÜ) ‚àí2dA
dT cos(t + œÜ) + 2A dœÜ
dT sin(t + œÜ)
=
3
2A3 ‚àíA + 2A dœÜ
dT

sin(t + œÜ) ‚àí2dA
dT cos(t + œÜ) ‚àí1
2A3 sin 3(t + œÜ).
In order to remove the secular terms, we must set the coeÔ¨Écients of sin(t + œÜ) and
cos(t + œÜ) to zero. This gives us two simple ordinary diÔ¨Äerential equations to be
solved subject to (12.59), which gives
A = 1,
œÜ = ‚àí1
4T,
and hence
x = sin

1 ‚àí1
4k2

t
	
+ O(k2),
for k ‚â™1. We can see that, as we would expect from the analysis given in Sec-
tion 9.4, the leading order amplitude of the oscillation does not change with t, in
contrast to the solution of the van der Pol equation that we studied earlier. How-
ever, the period of the oscillation changes by O(k2) even at this lowest order. If
we take the analysis to O(k4), we Ô¨Ånd that the amplitude of the oscillation is also
dependent on k (see King, 1988).
12.2.5
Slowly Damped Nonlinear Oscillations: Kuzmak‚Äôs Method
The method of multiple scales, as we have described it above, is appropriate for
weakly perturbed linear oscillators. Can we make any progress if the leading order
problem is nonlinear? We will concentrate on the simple example,
d 2y
dt2 + 2œµdy
dt + y ‚àíy3 = 0,
(12.60)
subject to
y(0) = 0,
dy
dt (0) = v0 > 0,
(12.61)
with œµ small and positive. Let‚Äôs begin by considering the leading order problem,
with œµ = 0. As we saw in Chapter 9, since dy/dt does not appear in (12.60) when

12.2 ORDINARY DIFFERENTIAL EQUATIONS
333
œµ = 0, we can integrate once to obtain
dy
dt = ¬±
*
E ‚àíy2 + 1
2y4,
(12.62)
with E = v2
0 from the initial conditions, (12.61). If we now assume that v2
0 < 1/2‚Ä†,
and scale y and t using
y =

1 ‚àí
‚àö
1 ‚àí2E
1/2
ÀÜy,
t =
1 ‚àí
‚àö
1 ‚àí2E
E
1/2
ÀÜt,
(12.62) becomes
dÀÜy
dÀÜt = ¬±
#
1 ‚àíÀÜy2#
1 ‚àík2ÀÜy2,
(12.63)
where
k =
1 ‚àí
‚àö
1 ‚àí2E
1 +
‚àö
1 ‚àí2E
1/2
.
(12.64)
If we compare (12.63) with the system that we studied in Section 9.4, we Ô¨Ånd that
y =

1 ‚àí
‚àö
1 ‚àí2E
1/2
sn

E
1 ‚àí
‚àö
1 ‚àí2E
1/2
t ; k

.
(12.65)
In the absence of any damping (œµ = 0), y varies periodically in a manner described
by the Jacobian elliptic function sn. In addition, Example 2 in the previous section
shows that y ‚àºv0 sin t when v0 ‚â™1. This is to be expected, since the nonlinear
term in (12.60) is negligible when v0, and hence y, is small.
For œµ small and positive, but nonzero, by analogy with what we found using the
method of multiple scales in the previous section, we expect that weak damping
leads to a slow decrease in the amplitude of the oscillation, and possibly some change
in its phase. In order to construct an asymptotic solution valid for t = O(œµ‚àí1), when
the amplitude and phase of the oscillation have changed signiÔ¨Åcantly, we begin in
the usual way by deÔ¨Åning a slow time scale, T = œµt. However, for a nonlinear
oscillator, the frequency of the leading order solution depends upon the amplitude
of the oscillation, so it is now convenient to deÔ¨Åne
œà = Œ∏(T)
œµ
+ œÜ(T),
Œ∏(0) = 0,
and seek a solution y ‚â°y(œà, T). This was Ô¨Årst done by Kuzmak (1959), although
not in full generality.
Since
dy
dt = (Œ∏‚Ä≤ + œµœÜ‚Ä≤) ‚àÇy
‚àÇœà + œµ ‚àÇy
‚àÇT ,
where a prime denotes d/dT, we can see that Œ∏‚Ä≤(T) ‚â°œâ(T) is the frequency of the
oscillation at leading order and œÜ(T) the change in phase, both of which we must
‚Ä† The usual graphical argument shows that this is a suÔ¨Écient condition for the solution to be
periodic in t.

334
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
determine as part of the solution. In terms of these new variables, (12.60) and
(12.61) become
(œâ + œµœÜ‚Ä≤)2 ‚àÇ2y
‚àÇœà2 + 2œµ (œâ + œµœÜ‚Ä≤) ‚àÇ2y
‚àÇT‚àÇœà + œµ2 ‚àÇ2y
‚àÇT 2 + œµ (œâ‚Ä≤ + œµœÜ‚Ä≤‚Ä≤) ‚àÇy
‚àÇœà
+2œµ (œâ + œµœÜ‚Ä≤) ‚àÇy
‚àÇœà + 2œµ2 ‚àÇy
‚àÇT + y ‚àíy3 = 0,
(12.66)
subject to
y(0, 0) = 0,
(œâ(0) + œµœÜ‚Ä≤(0)) ‚àÇy
‚àÇœà (0, 0) + œµ ‚àÇy
‚àÇT (0, 0) = v0.
(12.67)
We now expand
y = y0(œà, T) + œµy1(œà, T) + O(œµ2),
and substitute into (12.66) and (12.67). At leading order, we obtain
œâ2(T)‚àÇ2y0
‚àÇœà2 + y0 ‚àíy3
0 = 0,
(12.68)
subject to
y0(0, 0) = 0,
œâ(0)‚àÇy0
‚àÇœà (0, 0) = v0.
(12.69)
As we have seen, this has solution
y0 =

1 ‚àí
#
1 ‚àí2E(T)
1/2
sn
Ô£±
Ô£≤
Ô£≥
%
E(T)
1 ‚àí
#
1 ‚àí2E(T)
&1/2
œà
œâ(T) ; k(T)
Ô£º
Ô£Ω
Ô£æ,
(12.70)
where k is given by (12.64). The initial conditions, (12.69), show that
E(0) = v2
0,
œÜ(0) = 0.
(12.71)
The period of the oscillation, P(T), can be determined by noting that the quarter
period is
1
4P(T) = œâ(T)
 
1‚àí‚àö
1‚àí2E(T )
1/2
0
ds
$
E(T) ‚àís2 + 1
2s4
,
which, after a simple rescaling, shows that
P(T) = 4œâ(T)
%
1 ‚àí
#
1 ‚àí2E(T)
E(T)
&1/2
K(k(E)),
(12.72)
where
K(k) =
 1
0
ds
‚àö
1 ‚àís2‚àö
1 ‚àík2s2
(12.73)
is the complete elliptic integral of the Ô¨Årst kind.
As in the method of multiple scales, we need to go to O(œµ) to determine the

12.2 ORDINARY DIFFERENTIAL EQUATIONS
335
behaviour of the solution on the slow time scale, T. Firstly, note that we have
three unknowns to determine, E(T), œÜ(T) and œâ(T), whilst for the method of
multiple scales, we had just two, equivalent to E(T) and œÜ(T). Since we introduced
Œ∏(T) simply to account for the fact that the period of the oscillation changes with
the amplitude, we have the freedom to choose this new time scale so that the period
of the oscillation is constant. For convenience, we take P = 1, so that
dŒ∏
dT = œâ ‚â°œâ(E) =
1
4K(k(E))

E
1 ‚àí
‚àö
1 ‚àí2E
1/2
.
(12.74)
We also need to note for future reference the parity with respect to œà of y0 and
its derivatives. Both y0 and ‚àÇ2y0/‚àÇœà2 are odd functions, whilst ‚àÇy0/‚àÇœà is an even
function of œà. In addition, we can now treat y0 as a function of œà and E, with
y0(œà, E) =

1 ‚àí
‚àö
1 ‚àí2E
1/2
sn {4K(k(E))œà ; k(E)} .
(12.75)
At O(œµ) we obtain
œâ2 ‚àÇ2y1
‚àÇœà2 +

1 ‚àí3y2
0

y1 = ‚àí2œâœÜ‚Ä≤ ‚àÇ2y0
‚àÇœà2 ‚àí2œâ ‚àÇ2y0
‚àÇœà‚àÇE
dE
dT ‚àíœâ‚Ä≤ ‚àÇy0
‚àÇœà ‚àí2œâ ‚àÇy0
‚àÇœà ,
which we write as
L(y1) = R1odd + R1even,
(12.76)
where
L = œâ2 ‚àÇ2
‚àÇœà2 + 1 ‚àí3y2
0,
(12.77)
and
R1odd = ‚àí2œâœÜ‚Ä≤ ‚àÇ2y0
‚àÇœà2 ,
R1even = ‚àí2œâ ‚àÇ2y0
‚àÇœà‚àÇE
dE
dT ‚àíœâ‚Ä≤ ‚àÇy0
‚àÇœà ‚àí2œâ ‚àÇy0
‚àÇœà .
(12.78)
Now, by diÔ¨Äerentiating (12.68) with respect to œà, we Ô¨Ånd that
L
‚àÇy0
‚àÇœà

= 0,
so that ‚àÇy0/‚àÇœà is a solution of the homogeneous version of (12.76). For the solution
of (12.76) to be periodic, the right hand side must be orthogonal to the solution of
the homogeneous equation, and therefore orthogonal to ‚àÇy0/‚àÇœà.‚Ä† This is equivalent
to the elimination of secular terms in the method of multiple scales. Since ‚àÇy0/‚àÇœà
is even in œà, this means that
 1
0
‚àÇy0
‚àÇœà R1evendœà = 0,
which is the equivalent of the secularity condition that determines the amplitude
‚Ä† Strictly speaking, this is a result that arises from Floquet theory, the theory of linear ordinary
diÔ¨Äerential equations with periodic coeÔ¨Écients.

336
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
of the oscillation in the method of multiple scales. Using (12.78), we can write this
as‚Ä†
d
dE

œâ
 1
0
‚àÇy0
‚àÇœà
2
dœà

dE
dT + 2œâ
 1
0
‚àÇy0
‚àÇœà
2
dœà = 0.
(12.79)
To proceed, we Ô¨Årstly note that
 1
0
‚àÇy0
‚àÇœà
2
dœà = 4
 (1‚àí‚àö1‚àí2E)
1/2
0
‚àÇy0
‚àÇœà dy0
= 4
œâ
 (1‚àí‚àö1‚àí2E)
1/2
0
*
E ‚àíy2
0 + 1
2y4
0 dy0,
and hence that
‚àÇ
‚àÇE

œâ
 1
0
‚àÇy0
‚àÇœà
2
dœà

= 2
 (1‚àí‚àö1‚àí2E)
1/2
0
dy0
$
E ‚àíy2
0 + 1
2y4
0
= 1
2œâ .
(12.80)
Secondly, using the results of Section 9.4, we Ô¨Ånd that
 1
0
‚àÇy0
‚àÇœà
2
dœà
= 16K2(E)

1 ‚àí
‚àö
1 ‚àí2E
  1
0

1 ‚àísn2 (4Kœà ; k)
 
1 ‚àík2sn2 (4Kœà ; k)

dœà
= 16K(E)

1 ‚àí
‚àö
1 ‚àí2E
  K
0

1 ‚àísn2 (œà ; k)
 
1 ‚àík2sn2 (œà ; k)

dœà.
We now need a standard result on elliptic functions‚Ä°, namely that
 K
0

1 ‚àísn2 (œà ; k)
 
1 ‚àík2sn2 (œà ; k)

dœà
=
1
3k2


1 + k2
L(k) ‚àí

1 ‚àík2
K(k)

,
where
L(k) =
 1
0
*
1 ‚àík2s2
1 ‚àís2 ds,
is the complete elliptic integral of the second kind¬ß. Equation (12.79) and
the deÔ¨Ånition of œâ, (12.74), then show that
dE
dT = ‚àí4E
3k2K


1 + k2
L (k) ‚àí

1 ‚àík2
K(k)

.
(12.81)
‚Ä† Note that the quantity in the curly brackets in (12.79) is often referred to as the action.
‚Ä° See, for example, Byrd (1971).
¬ß Although the usual notation for the complete elliptic integral of the second kind is E(k), the
symbol E is already spoken for in this analysis.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
337
This equation, along with the initial condition, (12.71), determines E(T). For v0 ‚â™
1, this reduces to the multiple scales result, dE/dT = ‚àí2E (see Exercise 12.10). It
is straightforward to integrate (12.81) using MATLAB, since the complete elliptic
integrals of the Ô¨Årst and second kinds can be calculated using the built-in function
ellipke. Note that we can simultaneously calculate Œ∏(T) numerically by solving
(12.74) subject to Œ∏(0) = 0.
Figure 12.13 shows the function E(T) calculated
for E(0) = 0.45, and also the corresponding result using multiple scales on the
linearized problem, E = E(0)e‚àí2T .
0
0.5
1
1.5
2
2.5
3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
T
E(T)
Kuzmak
Linearized multiple scales
Fig. 12.13. The solution of (12.81) when E(0) = 0.45, and the corresponding multiple
scales solution of the linearized problem, E(T) = E(0)e‚àí2T .
We now need to Ô¨Ånd an equation that determines the phase, œÜ(T). Unfortunately,
unlike the method of multiple scales, we need to determine the solution at O(œµ) in
order to make progress. By diÔ¨Äerentiating (12.68) with respect to E, we Ô¨Ånd that
L
‚àÇy0
‚àÇE

= ‚àí2œâ dœâ
dE
‚àÇ2y0
‚àÇœà2 .
We also note that
L

œà ‚àÇy0
‚àÇœà

= 2œâ2 ‚àÇ2y0
‚àÇœà2 ,
(12.82)

338
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
and hence that
L (y0odd) = 0,
where
y0odd = œâ ‚àÇy0
‚àÇE + dœâ
dE œà ‚àÇy0
‚àÇœà .
(12.83)
The general solution of the homogeneous version of (12.76) is therefore a linear
combination of the even function ‚àÇy0/‚àÇœà and the odd function y0odd. To Ô¨Ånd the
particular integral solution of (12.76), we note from (12.82) that
L

‚àíœÜ‚Ä≤
œâ œà ‚àÇy0
‚àÇœà

= ‚àí2œâœÜ‚Ä≤ ‚àÇ2y0
‚àÇœà2 = R1odd.
We therefore have
y1 = A(T)‚àÇy0
‚àÇœà + B(T)

œâ ‚àÇy0
‚àÇE + dœâ
dE œà ‚àÇy0
‚àÇœà

‚àíœÜ‚Ä≤
œâ œà ‚àÇy0
‚àÇœà + y1even,
(12.84)
where y1even is the part of the particular integral generated by R1even, and is itself
even in œà. For y1 to be bounded as œà ‚Üí‚àû, the coeÔ¨Écient of œà must be zero.
Noting from (12.75) that
‚àÇy0
‚àÇE ‚àº4dK
dE œà ‚àÇy0
‚àÇœà
as œà ‚Üí‚àû,
this means that
B(T) =
œÜ‚Ä≤/œâ
4œâdK/dE + dœâ/dE .
(12.85)
The easiest way to proceed is to multiply (12.66) by ‚àÇy/‚àÇœà, and integrate over
one period of the oscillation. After taking into account the parity of the components
of each integrand, we Ô¨Ånd that
d
dT

e2T (œâ + œµœÜ‚Ä≤)
 1
0
 ‚àÇy
‚àÇœà
2
dœà

= 0.
At leading order, this reproduces (12.79), whilst at O(œµ) we Ô¨Ånd that
d
dT

e2T
%
2œâ
 1
0
‚àÇy0
‚àÇœà
‚àÇy1
‚àÇœà dœà + œÜ‚Ä≤
 1
0
‚àÇy0
‚àÇœà
2
dœà
&
= 0.
(12.86)
Now, using what we know about the parity of the various components of y1, we can
show that
 1
0
‚àÇy0
‚àÇœà
‚àÇy1
‚àÇœà dœà =
œÜ‚Ä≤
dœâ/dE
 1
0
‚àÇy0
‚àÇœà
‚àÇ2y0
‚àÇœà‚àÇE dœà =
œÜ‚Ä≤
2dœâ/dE
‚àÇ
‚àÇE
 1
0
‚àÇy0
‚àÇœà
2
dœà,
and hence from (12.86) that
d
dT

e2T
œÜ‚Ä≤
dœâ/dE
‚àÇ
‚àÇE

œâ
 1
0
‚àÇy0
‚àÇœà
2
dœà

= 0.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
339
Using (12.80), this becomes
d
dT
 e2T œÜ‚Ä≤
œâdœâ/dE

= 0.
(12.87)
This is a second order equation for œÜ(T). Although we know that œÜ(0) = 0, to be
able to solve (12.87) we also need to know œÜ‚Ä≤(0).
At O(œµ), the initial conditions, (12.67), are
y1(0, 0) = 0,
œâ(E(0))‚àÇy1
‚àÇœà (0, 0) = ‚àíE‚Ä≤(0)‚àÇy0
‚àÇE (0, E(0)) ‚àíœÜ‚Ä≤(0)‚àÇy0
‚àÇœà (0, E(0)).
(12.88)
By substituting the solution (12.84) for y1 into the second of these, we Ô¨Ånd that
œâ(E(0))

A(0)‚àÇ2y0
‚àÇœà2 (0, E(0)) + B(0)œâ(E(0)) ‚àÇ2y0
‚àÇœà‚àÇE (0, E(0))
+

B(0) dœâ
dE (E(0)) ‚àí
œÜ‚Ä≤(0)
œâ(E(0))
 ‚àÇy0
‚àÇœà (0, E(0)) + ‚àÇy1even
‚àÇœà
(0, 0)
	
= ‚àíE‚Ä≤(0)‚àÇy0
‚àÇE (0, E(0)) ‚àíœÜ‚Ä≤(0)‚àÇy0
‚àÇœà (0, E(0)).
Using (12.85), all of the terms that do not involve œÜ‚Ä≤(0) are odd in œà, and therefore
vanish when œà = 0. We conclude that œÜ‚Ä≤(0) = 0, and hence from (12.87) that
œÜ(T) = 0.
Figure 12.14 shows a comparison between the leading order solution computed
using Kuzmak‚Äôs method, the leading order multiple scales solution of the linearized
problem, y = ‚àöv0e‚àíT sin t, and the numerical solution of the full problem when
œµ = 0.01. The numerical and Kuzmak solutions are indistinguishable. Although
the multiple scales solution gives a good estimate of E(T), as shown in Ô¨Ågure 12.13,
it does not give an accurate solution of the full problem.
To see how the method works in general for damped nonlinear oscillators, the
interested reader is referred to Bourland and Haberman (1988).
12.2.6
The EÔ¨Äect of Fine Scale Structure on Reaction‚ÄìDiÔ¨Äusion
Processes
Consider the two-point boundary value problem
d
dx

D

x, x
œµ
 dŒ∏
dx
	
+ R

Œ∏, x, x
œµ

= 0 for 0 < x < 1,
(12.89)
subject to the boundary conditions
dŒ∏
dx = 0 at x = 0 and x = 1.
(12.90)
We can think of Œ∏(x) as a dimensionless temperature, so that this boundary value
problem models the steady distribution of heat in a conducting material. When
œµ ‚â™1, this material has a Ô¨Åne scale structure varying on a length scale of O(œµ), and

340
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
T
y
Numerical
Kuzmak
Linearized multiple scales
Fig. 12.14. The numerical solution of (12.60) subject to (12.61) when v2
0 = 0.45 and
œµ = 0.01 compared to asymptotic solutions calculated using the leading order Kuzmak
solution and the leading order multiple scales solution of the linearized problem.
a coarse, background structure varying on a length scale of O(1). The steady state
temperature distribution represents a balance between the diÔ¨Äusion of heat, (DŒ∏x)x
(where subscripts denote derivatives), and its production by some chemical reaction,
R (see Section 12.2.3, Example 2 for a derivation of this type of balance law). If
we integrate (12.89) over 0 ‚©Ωx ‚©Ω1 and apply the boundary conditions (12.90), we
Ô¨Ånd that the chemical reaction term must satisfy the solvability condition
 1
0
R

Œ∏(x), x, x
œµ

dx = 0
(12.91)
for a solution to exist. Physically, since (12.90) states that no heat can escape from
the ends of the material, (12.91) simply says that the overall rate at which heat
is produced must be zero for a steady state to be possible, with sources of heat in
some regions of the material balanced by heat sinks in other regions.
In order to use asymptotic methods to determine the solution at leading order
when œµ ‚â™1, we begin by introducing the fast variable, ÀÜx = x/œµ. In the usual way

12.2 ORDINARY DIFFERENTIAL EQUATIONS
341
(see Section 12.2.4), (12.89) and (12.90) become
(DŒ∏ÀÜx)ÀÜx + œµ {(DŒ∏ÀÜx)x + (DŒ∏x)ÀÜx} + œµ2 {(DŒ∏x)x + R} = 0,
(12.92)
subject to
Œ∏ÀÜx + œµŒ∏x = 0 at x = ÀÜx = 0 and x = 1, ÀÜx = 1/œµ,
(12.93)
with R ‚â°R (Œ∏(x, ÀÜx), x, ÀÜx), D ‚â°D(x, ÀÜx). It is quite awkward to apply a boundary
condition at ÀÜx = 1/œµ ‚â´1, but we shall see that we can determine the equation
satisÔ¨Åed by Œ∏ at leading order without using this, and we will not consider it below.
We now expand Œ∏ as
Œ∏ = Œ∏0(x, ÀÜx) + œµŒ∏1(x, ÀÜx) + œµ2Œ∏2(x, ÀÜx) + O(œµ3).
At leading order,
(DŒ∏0ÀÜx)ÀÜx = 0,
subject to
Œ∏0ÀÜx = 0 at x = ÀÜx = 0.
We can integrate this once to obtain DŒ∏0ÀÜx = Œ±(x), with Œ±(0) = 0, and then once
more, which gives
Œ∏0 = Œ±(x)
 ÀÜx
0
ds
D(x, s) + f0(x).
At O(œµ), we Ô¨Ånd that
(DŒ∏1ÀÜx)ÀÜx = ‚àí(DŒ∏0ÀÜx)x ‚àí(DŒ∏0x)ÀÜx ,
which we can write as
(DŒ∏1ÀÜx + DŒ∏0x)ÀÜx = ‚àíŒ±‚Ä≤(x).
(12.94)
We can integrate (12.94) to obtain
DŒ∏1ÀÜx + DŒ∏0x = ‚àíŒ±‚Ä≤(x)ÀÜx + Œ≤(x).
Substituting for Œ∏0 and integrating again shows that
Œ∏1 = f1(x) ‚àíf ‚Ä≤
0(x)ÀÜx ‚àíŒ±‚Ä≤(x)ÀÜx
 ÀÜx
0
ds
D(x, s)
‚àíŒ±(x) ‚àÇ
‚àÇx
 ÀÜx
0
ÀÜx ‚àís
D(x, s)ds + Œ≤(x)
 ÀÜx
0
ds
D(x, s).
(12.95)
When ÀÜx is large, there are terms in this expression that are of O(ÀÜx2). These are
secular, and become comparable with the leading order term in the expansion for
Œ∏ when ÀÜx = 1/œµ. We must eliminate this secularity by taking
lim
œµ‚Üí0

Œ±‚Ä≤(x)œµ
 1/œµ
0
ds
D(x, s) ‚àíŒ±(x)œµ2 ‚àÇ
‚àÇx
 1/œµ
0
1/œµ ‚àís
D(x, s)ds

= 0.
(12.96)
This is a Ô¨Årst order ordinary diÔ¨Äerential equation for Œ±(x). Since Œ±(0) = 0, we

342
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
conclude that Œ± ‚â°0. Note that each of the terms in (12.96) is of O(1) for œµ ‚â™1.
For example, œµ
' 1/œµ
0
ds
D(x,s) is the mean value of 1/D over the Ô¨Åne spatial scale. We
now have simply Œ∏0 = f0(x). This means that, at leading order, Œ∏ varies only on
the coarse, O(1) length scale. This does not mean that the Ô¨Åne scale structure has
no eÔ¨Äect, as we shall see.
Since we now know that Œ∏0 = O(1) when ÀÜx = 1/œµ, we must also eliminate the
secular terms that are of O(ÀÜx) when ÀÜx is large in (12.95). We therefore require that
f ‚Ä≤
0 = dŒ∏0
dx = lim
œµ‚Üí0

œµŒ≤(x)
 1/œµ
0
ds
D(x, s)

.
(12.97)
In order to determine Œ≤, and hence an equation satisÔ¨Åed by Œ∏0, we must consider
one further order in the expansion.
At O(œµ2), (12.92) gives
(DŒ∏2ÀÜx + DŒ∏1x)ÀÜx = ‚àí(DŒ∏1ÀÜx)x ‚àí(DŒ∏0x)x ‚àíR(Œ∏0(x), x, ÀÜx)
= ‚àíŒ≤‚Ä≤(x) ‚àíR(Œ∏0(x), x, ÀÜx),
(12.98)
which we can integrate twice to obtain
Œ∏2 = f2(x) ‚àíf ‚Ä≤
1(x)ÀÜx + 1
2
d 2Œ∏0
dx2 ÀÜx2 ‚àí‚àÇ
‚àÇx

Œ≤(x)
 ÀÜx
0
ÀÜx ‚àís
D(x, s)ds

+ Œ≥(x)
 ÀÜx
0
ds
D(x, s)
‚àíŒ≤‚Ä≤(x)
 ÀÜx
0
s
D(x, s)ds ‚àí
 ÀÜx
0
1
D(x, s)
 s
0
R(Œ∏0(x), x, u) du ds.
(12.99)
In order to eliminate the secular terms of O(ÀÜx2), we therefore require that
lim
œµ‚Üí0

1
2
d 2Œ∏0
dx2 ‚àíœµ2 ‚àÇ
‚àÇx

Œ≤(x)
 1/œµ
0
1/œµ ‚àís
D(x, s)ds

‚àíŒ≤‚Ä≤(x)œµ2
 1/œµ
0
s
D(x, s)ds ‚àíœµ2
 1/œµ
0
1
D(x, s)
 s
0
R(Œ∏0(x), x, u) du ds

= 0.
If we now use (12.97) to eliminate Œ≤(x), we arrive at
lim
œµ‚Üí0
Ô£±
Ô£≤
Ô£≥
d 2Œ∏0
dx2 ‚àí
2 d
dx

œµ2 ' 1/œµ
0
s
D(x,s)ds

œµ
' 1/œµ
0
ds
D(x,s)
dŒ∏0
dx
+2œµ2
 1/œµ
0
1
D(x, s)
 s
0
R(Œ∏0(x), x, u) du ds
Ô£º
Ô£Ω
Ô£æ= 0.
After multiplying through by a suitable integrating factor, we can see that Œ∏0 sat-
isÔ¨Åes the reaction‚ÄìdiÔ¨Äusion equation

 ¬ØD(x)Œ∏0x

x + ¬ØR(Œ∏0(x), x) = 0,
(12.100)

12.2 ORDINARY DIFFERENTIAL EQUATIONS
343
on the coarse scale, where
¬ØD(x) = lim
œµ‚Üí0
Ô£Æ
Ô£∞exp
Ô£±
Ô£≤
Ô£≥‚àí2
 x
0
d
dX

œµ2 ' 1/œµ
0
s
D(X,s)ds

œµ
' 1/œµ
0
ds
D(X,s)
dX
Ô£º
Ô£Ω
Ô£æ
Ô£π
Ô£ª,
(12.101)
¬ØR(Œ∏0(x), x) = lim
œµ‚Üí0

2 ¬ØD(x)œµ2
 1/œµ
0
1
D(x, s)
 s
0
R(Œ∏0(x), x, u) du ds

(12.102)
are the Ô¨Åne scale averages.
This asymptotic analysis, which is called homogenization, shows that the lead-
ing order temperature does not vary on the Ô¨Åne scale, and satisÔ¨Åes a standard
reaction‚ÄìdiÔ¨Äusion equation on the coarse scale. However, the Ô¨Åne scale structure
modiÔ¨Åes the reaction term and diÔ¨Äusion coeÔ¨Écient through (12.101) and (12.102),
with ¬ØD the homogenized diÔ¨Äusion coeÔ¨Écient and ¬ØR the homogenized reac-
tion term. If we were to seek higher order corrections, we would Ô¨Ånd that there
are variations in the temperature on the Ô¨Åne scale, but that these are at most of
O(œµ).
One case where ¬ØD and ¬ØR take a particularly simple form is when D(x, ÀÜx) =
D0(x) ÀÜD(ÀÜx) and R(Œ∏(x), x, ÀÜx) = R0(Œ∏(x), x) ÀÜR(ÀÜx). On substituting these into (12.101)
and (12.102), we Ô¨Ånd, after cancelling a constant common factor between ¬ØD and
¬ØR, that we can use ¬ØD(x) = D0(x) and ¬ØR(Œ∏, x) = KR0(Œ∏, x), where
K = lim
œµ‚Üí0

2œµ
 1/œµ
0
1
ÀÜD(s)
 s
0
ÀÜR(u) du ds

.
The homogenized diÔ¨Äusion coeÔ¨Écient and reaction term are simply given by the
terms D0(x) and R0(Œ∏, x), modiÔ¨Åed by a measure of the mean value of the ratio
of the Ô¨Åne scale variation of each, given by the constant K. In particular, when
ÀÜD(ÀÜx) = 1/ (1 + A1 sin k1ÀÜx) and ÀÜR(ÀÜx) = 1 + A2 sin k2ÀÜx for some positive constants
k1, k2, A1 and A2, with A1 < 1 and A2 < 1, we Ô¨Ånd that K = 1. We can illustrate
this for a simple case where it is straightforward to Ô¨Ånd the exact solution of both
(12.89) and (12.100) analytically. Figure 12.15 shows a comparison between the
exact and asymptotic solutions for various values of œµ when k1 = k2 = 1, A1 =
A2 = 1/2, D0(x) = 1/(1 + x) and R0 = 2x ‚àí1. The analytical solution of (12.89)
that vanishes at x = 0 (the solution would otherwise contain an arbitrary constant
in this case) is
Œ∏ = 1
2x2 ‚àí1
4x4 + œµ2
1
2(1 ‚àí2x)
1
8 cos
2x
œµ

‚àísin
x
œµ
	
‚àí1
4x ‚àí1
16

+œµ3

2 cos
x
œµ

+ 3
16 sin
2x
œµ

‚àí2

,
whilst the corresponding solution of (12.100) correctly reproduces the leading order
part of this.
Homogenization has been used successfully in many more challenging appli-
cations than this linear, steady state reaction diÔ¨Äusion equation, for example,

344
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
x
Œ∏
asymptotic
analytical, Œµ = 0.2
analytical, Œµ = 0.1
analytical, Œµ = 0.05
Fig. 12.15. Analytical and asymptotic solutions of (12.89) when D = 1/(1+x) 
1 + 1
2 sin x
and R = (2x ‚àí1) 
1 + 1
2 sin x
.
in assessing the strength of elastic media with small carbon Ô¨Åbre reinforcements
(Bakhvalov and Panasenko, 1989).
12.2.7
The WKB Approximation
In all of the examples that we have seen so far, we have used an expansion in
algebraic powers of a small parameter to develop perturbation series solutions of dif-
ferential or algebraic equations. This procedure has been at its most complex when
we have needed to match a slowly varying outer solution with an exponentially-
rapidly varying, or dissipative, boundary layer.
This procedure doesn‚Äôt always
work! For example, if we consider the two-point boundary value problem
œµ2y‚Ä≤‚Ä≤(x) + œÜ(x)y(x) = 0 subject to y(0) = 0 and y(1) = 1,
(12.103)
the procedure fails as there are no terms to balance with the leading order term,
œÜ(x)y(x). If there were a Ô¨Årst derivative term in this problem, the procedure would
work, although we would have a singular perturbation problem. However, a Ô¨Årst
derivative term can always be removed. Suppose that we have a diÔ¨Äerential equation

12.2 ORDINARY DIFFERENTIAL EQUATIONS
345
of the form
w‚Ä≤‚Ä≤ + p(x)w‚Ä≤ + q(x)w = 0.
By writing w = Wu, we can easily show that
W ‚Ä≤‚Ä≤ +
2u‚Ä≤
u + p

W ‚Ä≤ +
u‚Ä≤‚Ä≤
u + pu‚Ä≤
u + q

W = 0.
By choosing 2u‚Ä≤
u +p = 0 and hence u = exp

‚àí1
2
' x p(t) dt

, we can remove the Ô¨Årst
derivative term. Because of this, there is a sense in which œµ2y‚Ä≤‚Ä≤ +œÜy = 0 is a generic
second order ordinary diÔ¨Äerential equation, and we need to develop a perturbation
method that can deal with it.
The Basic Expansion
A suitable method was proposed by Wentzel, Kramers and Brillioun (and perhaps
others as well) in the 1920s. The appropriate asymptotic development is of the
form
y = exp
œà0(x)
œµ
+ œà1(x) + O(œµ)
	
.
DiÔ¨Äerentiating this gives
y‚Ä≤ = exp
œà0
œµ + œà1
	 œà‚Ä≤
0
œµ + œà‚Ä≤
1 + O(œµ)
	
,
and
y‚Ä≤‚Ä≤ = exp
œà0
œµ + œà1 + O(œµ)
	 œà‚Ä≤‚Ä≤
0
œµ + œà‚Ä≤‚Ä≤
1(x) + O(œµ)
	
+ exp
œà0
œµ + œà1 + O(œµ)
	 œà‚Ä≤
0
œµ + œà‚Ä≤
1 + O(œµ)
	2
= exp
œà0
œµ + œà1
	 (œà‚Ä≤
0)2
œµ2
+ 1
œµ (2œà‚Ä≤
0œà‚Ä≤
1 + œà‚Ä≤‚Ä≤
0) + O(1)
	
.
If we substitute these into (12.103), we obtain
(œà‚Ä≤
0)2 + œµ (2œà‚Ä≤
0œà‚Ä≤
1 + œà‚Ä≤‚Ä≤
0) + œÜ(x) + O(œµ2) = 0,
and hence
(œà‚Ä≤
0)2 = ‚àíœÜ(x),
œà‚Ä≤
1 = ‚àíœà‚Ä≤‚Ä≤
0
2œà‚Ä≤
0
= ‚àí1
2
d
dx (log œà‚Ä≤
0) .
(12.104)
If œÜ(x) > 0, say for x > 0, we can simply integrate these equations to obtain
œà0 = ¬±i
 x
œÜ1/2(t) dt + constant,
œà1 = ‚àí1
4 log (œÜ(x)) + constant.

346
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
All of these constants of integration can be absorbed into a single constant, which
may depend upon œµ, in front of the exponential.
The leading order solution is
rapidly oscillating, or dispersive, in this case, and can be written as
y =
A(œµ)
œÜ1/4(x) exp

i
' x œÜ1/2(t) dt
œµ

+
B(œµ)
œÜ1/4(x) exp

‚àíi
' x œÜ1/2(t) dt
œµ

+ ¬∑ ¬∑ ¬∑ .
(12.105)
Note that this asymptotic development has assumed that x = O(1), and, in order
that it remains uniform, we must have œà1 ‚â™œà0/œµ.
If œÜ(x) < 0, say for x < 0, then some minor modiÔ¨Åcations give an exponential,
or dissipative, solution of the form,
y =
C(œµ)
|œÜ(x)|1/4 exp
' x |œÜ(t)|1/2 dt
œµ

+
D(œµ)
|œÜ(x)|1/4 exp

‚àí
' x |œÜ(t)|1/2 dt
œµ

+ ¬∑ ¬∑ ¬∑ .
(12.106)
If œÜ(x) is of one sign in the domain of solution, one or other of the expansions
(12.105) and (12.106) will be appropriate. If, however, œÜ(x) changes sign, we will
have an oscillatory solution on one side of the zero and an exponentially growing or
decaying solution on the other. We will consider how to deal with this combination
of dispersive and dissipative behaviour after studying a couple of examples.
Example 1: Bessel functions for x ‚â´1
We saw in Chapter 3 that Bessel‚Äôs equation is
y‚Ä≤‚Ä≤ + 1
xy‚Ä≤ +

1 ‚àíŒΩ2
x2

y = 0.
If we make the transformation y = x‚àí1/2Y , we obtain the generic form of the
equation,
Y ‚Ä≤‚Ä≤ +

1 +
1
2 ‚àíŒΩ2
x2

Y = 0.
(12.107)
Although this equation currently contains no small parameter, we can introduce
one in a useful way by deÔ¨Åning ¬Øx = Œ¥x. If x is large and positive, we can have
¬Øx = O(1) by choosing Œ¥ suÔ¨Éciently small. We have introduced this artiÔ¨Åcial small
parameter as a device to help us determine how the Bessel function behaves for
x ‚â´1, and it cannot appear in the Ô¨Ånal result when we change variables back from
¬Øx to x.
In terms of ¬Øx and ¬ØY (¬Øx) = Y (x), (12.107) becomes
Œ¥2 ¬ØY ‚Ä≤‚Ä≤ +

1 + Œ¥2
1
2 ‚àíŒΩ2
¬Øx2

¬ØY = 0.
(12.108)
By direct comparison with the derivation of the WKB expansion above, in which
we neglected terms of O(Œ¥2),
œà0 = ¬±i
 ¬Øx
dt = ¬±i¬Øx,
œà1 = ‚àí1
4 log 1 = 0,

12.2 ORDINARY DIFFERENTIAL EQUATIONS
347
so that
¬ØY ‚àºA(Œ¥) exp
i¬Øx
Œ¥

+ B(Œ¥) exp

‚àíi¬Øx
Œ¥

+ ¬∑ ¬∑ ¬∑ ,
and hence
y ‚àº
1
x1/2

Aeix + Be‚àíix
is the required expansion. Note that, although we can clearly see that the Bessel
functions are slowly decaying, oscillatory functions of x as x ‚Üí‚àû, we cannot
determine the constants A and B using this technique. As we have already seen, it
is more appropriate to use the integral representation (11.14).
We can show that the WKB method is not restricted to ordinary diÔ¨Äerential
equations with terms in just y‚Ä≤‚Ä≤ and y by considering a further example.
Example 2: A boundary layer
Let‚Äôs try to Ô¨Ånd a uniformly valid approximation to the solution of the two-point
boundary value problem
œµy‚Ä≤‚Ä≤ + p(x)y‚Ä≤ + q(x)y = 0 subject to y(0) = Œ±, y(1) = Œ≤
(12.109)
when œµ ‚â™1 and p(x) > 0. If we assume a WKB expansion,
y = exp
œà0(x)
œµ
+ œà1(x) + O(œµ)
	
,
and substitute into (12.109), we obtain at O(1/œµ)
œà‚Ä≤
0 {œà‚Ä≤
0 + p(x)} = 0,
(12.110)
and at O(1),
2œà‚Ä≤
0œà‚Ä≤
1 + œà‚Ä≤‚Ä≤
0 + p(x)œà‚Ä≤
1 + q(x) = 0.
(12.111)
Using the solution œà‚Ä≤
0 = 0 of (12.110) and substituting into (12.111) gives œà‚Ä≤
1 =
‚àíq(x)/p(x), which generates a solution of the form
y1 = exp
c1
œµ ‚àí
 x
0
q(t)
p(t) + ¬∑ ¬∑ ¬∑
	
‚àºC1(œµ) exp

‚àí
 x
0
q(t)
p(t)
	
.
(12.112)
The second solution of (12.110) has œà‚Ä≤
0 = ‚àíp(x) and hence
œà0 = ‚àí
 x
p(t) dt + c2.
Equation (12.111) then gives
œà1 = ‚àílog p(x) +
 x
0
q(t)
p(t) dt.

348
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
The second independent solution therefore takes the form
y2 = exp

c2 ‚àí
' x
0 p(t) dt
œµ
‚àílog p(x) +
 x
0
q(t)
p(t) dt + ¬∑ ¬∑ ¬∑

‚àºC2
p(x) exp

‚àí1
œµ
 x
0
p(t) dt +
 x
0
q(t)
p(t) dt
	
.
(12.113)
Combining (12.112) and (12.113) then gives the general asymptotic solution as
y = C1 exp

‚àí
 x
0
q(t)
p(t)
	
+ C2
p(x) exp

‚àí1
œµ
 x
0
p(t) dt +
 x
0
q(t)
p(t) dt
	
.
(12.114)
We can now apply the boundary conditions in (12.109) to obtain
Œ± = C1 + C2
p(0),
Œ≤ = C1 exp

‚àí
 1
0
q(t)
p(t)
	
+ C2
p(1) exp

‚àí1
œµ
 1
0
p(t) dt +
 1
0
q(t)
p(t) dt
	
.
The term exp

‚àí1
œµ
' 1
0 p(t) dt

is uniformly small and can be neglected, so that the
asymptotic solution can be written as
y ‚àºŒ≤ exp
 1
x
q(t)
p(t) dt
	
+ p(0)
p(x)

Œ± ‚àíŒ≤ exp
 1
0
q(t)
p(t) dt
	
exp

‚àí1
œµ
 x
0
p(t) dt +
 x
0
q(t)
p(t) dt
	
.
Finally, the last exponential in this solution is negligibly small unless x = O(œµ) (the
boundary layer), so we can write
y ‚àºŒ≤ exp
 1
x
q(t)
p(t) dt
	
+ p(0)
p(x)

Œ± ‚àíŒ≤ exp
 1
0
q(t)
p(t) dt
	
exp

‚àíp(0)x
œµ
	
.
This is precisely the composite expansion that we would have obtained if we had
used the method of matched asymptotic expansions instead.
Connection Problems
Let‚Äôs now consider the boundary value problem
œµ2y‚Ä≤‚Ä≤(x) + œÜ(x)y(x) = 0 subject to y(0) = 1, y ‚Üí0 as x ‚Üí‚àí‚àû,
(12.115)
with
œÜ(x) > 0
for x > 0,
œÜ(x) ‚àºœÜ1x
for |x| ‚â™1, œÜ1 > 0,
œÜ(x) < 0
for x < 0.
(12.116)
To prevent nonuniformities as |x| ‚Üí‚àû, we will also insist that |œÜ(x)| ‚â´x‚àí2 for
|x| ‚â´1. Using the expansions (12.105) and (12.106), we can immediately write
y =
A(œµ)
œÜ1/4(x) exp

i
' x
0 œÜ1/2(t) dt
œµ

+

12.2 ORDINARY DIFFERENTIAL EQUATIONS
349
B(œµ)
œÜ1/4(x) exp

‚àíi
' x
0 œÜ1/2(t) dt
œµ

+ ¬∑ ¬∑ ¬∑
for x > 0,
(12.117)
y = C(œµ) exp

‚àí1
œµ
 0
x
|œÜ(t)|1/2 dt ‚àí1
4 log |œÜ(x)| + ¬∑ ¬∑ ¬∑
	
for x < 0.
(12.118)
The problem of determining how A and B depend upon C is known as a connection
problem, and can be solved by considering an inner solution in the neighbourhood
of the origin.
For |x| ‚â™1, œÜ ‚àºœÜ1x, so we can estimate the sizes of the terms in the WKB
expansion. For x < 0
y ‚àºC(œµ) exp

‚àí1
œµ
 0
x
(‚àíœÜ1t)1/2 dt ‚àí1
4 log(‚àíœÜ1x)
	
‚àºC(œµ) exp

‚àí2
3œµœÜ1/2
1
(‚àíx)3/2 ‚àí1
4 log œÜ1 ‚àí1
4 log(‚àíx)
	
.
We can now see that the second term becomes comparable to the Ô¨Årst when ‚àíx =
O(œµ2/3). A similar estimate of the solution for x > 0 also gives a nonuniformity
when x = O(œµ2/3). The WKB solutions will therefore be valid in two outer regions
with |x| ‚â´œµ2/3. We will need a small inner region, centred on the origin, and the
inner solution must match with the outer solutions. Equation (12.115) shows that
the only rescaling possible near to the origin is in a region where x = O(œµ2/3), and
¬Øx = x/œµ2/3 = O(1) for œµ ‚â™1. Writing (12.117) and (12.118) in terms of ¬Øx leads to
the matching conditions
¬Øy ‚àº
C(œµ)
œÜ1/4
1
(‚àí¬Øx)1/4œµ1/6 exp

‚àí2
3(‚àí¬Øx)3/2œÜ1/2
1
	
as ¬Øx ‚Üí‚àí‚àû,
(12.119)
¬Øy ‚àº
A(œµ)
œÜ1/4
1
¬Øx1/4œµ1/6 exp

i2
3 ¬Øx3/2œÜ1/2
1
	
+
B(œµ)
œÜ1/4
1
¬Øx1/4œµ1/6 exp

‚àíi2
3 ¬Øx3/2œÜ1/2
1
	
as ¬Øx ‚Üí‚àû.
(12.120)
If we now rewrite (12.115) in the inner region, making use of œÜ ‚àºœµ2/3œÜ1¬Øx at leading
order, we arrive at
d 2¬Øy
d¬Øx2 + œÜ1¬Øx¬Øy = 0,
subject to ¬Øy(0) = 1, and the matching conditions (12.119) and (12.120).
(12.121)
We can write (12.121)‚Ä† in terms of a standard equation by deÔ¨Åning t = ‚àíœÜ1/3
1
¬Øx, in
terms of which (12.121) becomes
d 2¬Øy
dt2 = t¬Øy.
‚Ä† Note that equation (12.121) is valid for |x| ‚â™1, and we would expect its solution to be valid
in the same domain. Hence there is an overlap of the domains for which the inner and outer
solutions are valid, namely œµ2/3 ‚â™|x| ‚â™1, and we can expect the asymptotic matching
process to be successful. In fact the overlap domain can be reÔ¨Åned to œµ2/3 ‚â™|x| ‚â™œµ2/5 (see
Exercise 12.17).

350
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
This is Airy‚Äôs equation, which we met in Section 3.8, so the solution can be written
as
¬Øy = aAi(t) + bBi(t) = aAi

‚àíœÜ1/3
1
¬Øx

+ bBi

‚àíœÜ1/3
1
¬Øx

.
(12.122)
In Section 11.2.3 we determined the asymptotic behaviour of Ai(t) for |t| ‚â´1 using
the method of steepest descents. The same technique can be used for Bi(t), and we
Ô¨Ånd that
Ai(t) ‚àº1
2œÄ‚àí1/2t‚àí1/4 exp

‚àí2
3t3/2

,
Bi(t) ‚àºœÄ‚àí1/2t‚àí1/4 exp
2
3t3/2

as t ‚Üí‚àû,
(12.123)
Ai(t) ‚àº
1
‚àöœÄ (‚àít)‚àí1/4 sin
2
3(‚àít)3/2 + œÄ
4
	
,
Bi(t) ‚àº
1
‚àöœÄ (‚àít)‚àí1/4 cos
2
3(‚àít)3/2 + œÄ
4
	
as t ‚Üí‚àí‚àû.
(12.124)
Using this known behaviour to determine the behaviour of the inner solution,
(12.122), shows that
¬Øy ‚àºa1
2œÄ‚àí1/2 
œÜ1/3
1
¬Øx
‚àí1/4
exp

‚àí2
3

œÜ1/3
1
¬Øx
3/2	
+bœÄ‚àí1/2 
œÜ1/3
1
¬Øx
‚àí1/4
exp
2
3

œÜ1/3
1
¬Øx
3/2	
as ¬Øx ‚Üí‚àí‚àû.
In order to satisfy the matching condition (12.119), we must have b = 0, so that
only the Airy function Ai appears in the solution, and
C(œµ) = 1
2œÄ‚àí1/2œµ1/6œÜ1/6
1
a.
(12.125)
The boundary condition ¬Øy(0) = 1 then gives a = 1/Ai(0) = Œì
 2
3

32/3, and hence
determines C(œµ) through (12.125).
As ¬Øx ‚Üí‚àû,
¬Øy ‚àºa 1
‚àöœÄ

œÜ1/3
1
¬Øx
‚àí1/4
sin
2
3

œÜ1/3
1
¬Øx
3/2
+ œÄ
4
	
‚àº
a
‚àöœÄœÜ1/12
1
¬Øx1/4
1
2i

exp
2
3iœÜ1/2
1
¬Øx3/2 + œÄ
4

‚àíexp

‚àí2
3iœÜ1/2
1
¬Øx3/2 ‚àíœÄ
4
	
.
We can therefore satisfy the matching condition (12.120) by taking
A(œµ) = B‚àó(œµ) = aœÜ1/6
1
œµ1/6
2i‚àöœÄ
.
Since A and B are complex conjugate, the solution is real for x > 0, as of course
it should be. This determines all of the unknown constants, and completes the

12.3 PARTIAL DIFFERENTIAL EQUATIONS
351
solution. The Airy function Ai(t) is shown in Figure 11.12, which clearly shows
the transition from dispersive oscillatory behaviour as t ‚Üí‚àí‚àûto dissipative,
exponential decay as t ‚Üí‚àû.
12.3
Partial DiÔ¨Äerential Equations
Many of the asymptotic methods that we have met can also be applied to partial
diÔ¨Äerential equations. As you might expect, the task is usually rather more diÔ¨Écult
than we have found it to be for ordinary diÔ¨Äerential equations. We will proceed by
considering four generic examples.
Example 1: Asymptotic solutions of the Helmholtz equation
As an example of an elliptic partial diÔ¨Äerential equation, let‚Äôs consider the solution
of the Helmholtz equation,
‚àá2œÜ + œµ2œÜ = 0 for r ‚©Ω1,
(12.126)
subject to œÜ(1, Œ∏) = sin Œ∏ for œµ ‚â™1. This arises naturally as the equation that
governs time-harmonic solutions of the wave equation,
1
c2
‚àÇ2z
‚àÇt2 = ‚àá2z,
which we met in Chapter 3. If we write z = eiœâtœÜ(x), we obtain (12.126), with
œµ = œâ/c. Since œÜ = O(1) on the boundary, we expand œÜ = œÜ0 + œµ2œÜ2 + O(œµ4), and
obtain, at leading order,
‚àá2œÜ0 = 0,
subject to œÜ0(1, Œ∏) = sin Œ∏.
If we seek a separable solution of the form œÜ0 = f(r) sin Œ∏, we obtain
f ‚Ä≤‚Ä≤ + 1
r f ‚Ä≤ ‚àí1
r2 f = 0,
subject to f(1) = 1.
This has solutions of the form f = Ar+Br‚àí1, so the bounded solution that satisÔ¨Åes
the boundary condition is f(r) = r, and hence œÜ0 = r sin Œ∏. At O(œµ2), (12.126) gives
‚àá2œÜ2 = ‚àír sin Œ∏,
subject to œÜ2(1, Œ∏) = 0.
If we again seek a separable solution, œÜ2 = F(r) sin Œ∏, we arrive at
F ‚Ä≤‚Ä≤ + 1
r F ‚Ä≤ ‚àí1
r2 F = ‚àír
a,
subject to F(1) = 0.
Using the variation of parameters formula, this has the bounded solution
œÜ2 = 1
8

r ‚àír3
sin Œ∏.
The two-term asymptotic expansion of the solution can therefore be written as
œÜ = r sin Œ∏ + 1
8œµ2 
r ‚àír3
sin Œ∏ + O(œµ4),
which is bounded and uniformly valid throughout the circle, r ‚©Ω1.

352
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Let‚Äôs also consider a boundary value problem for the modiÔ¨Åed Helmholtz
equation,
œµ2‚àáœÜ ‚àíœÜ = 0 subject to œÜ(1, Œ∏) = 1 and œÜ ‚Üí0 as r ‚Üí‚àû.
(12.127)
Note that œÜ = 0 satisÔ¨Åes both the partial diÔ¨Äerential equation and the far Ô¨Åeld
boundary condition, but not the boundary condition at r = 1. This suggests that
we need a boundary layer near r = 1. If we deÔ¨Åne r = 1 + œµ¬Ør with ¬Ør = O(1) in the
boundary layer for œµ ‚â™1, we obtain
œÜ¬Ør¬Ør ‚àíœÜ = 0,
at leading order. This has solution
œÜ = ¬ØA(Œ∏)e¬Ør + ¬ØB(Œ∏)e‚àí¬Ør,
which will match with the far Ô¨Åeld solution if ¬ØA = 0, and satisfy the boundary
condition at ¬Ør = 0 if ¬ØB = 1. The inner solution, and also a composite solution valid
at leading order for all r ‚©æa, is therefore
œÜ = exp
r ‚àí1
œµ

.
Example 2: The small and large time solutions of a diÔ¨Äusion problem
Consider the initial value problem for the diÔ¨Äusion equation,
‚àÇc
‚àÇt = D ‚àÇ2c
‚àÇx2
for ‚àí‚àû< x < ‚àûand t > 0,
(12.128)
to be solved subject to the initial condition
c(x, 0) =
 f0(x)
for x ‚©Ω0,
0
for x > 0,
(12.129)
with f0 ‚ààC2(R), f0 ‚Üí0 as x ‚Üí‚àí‚àû, f0(0) Ã∏= 0 and
 0
‚àí‚àû
f0(x) dx = ftot.
(12.130)
We could solve this using either Laplace or Fourier transforms . The result, however,
would be in the form of a convolution integral, which does not shed much light on
the structure of the solution. We can gain a lot of insight by asking how the solution
behaves just after the initial state begins to diÔ¨Äuse (the small time solution, t ‚â™1),
and after a long time (t ‚â´1).
The small time solution, t ‚â™1
The general eÔ¨Äect of diÔ¨Äusion is to smooth out gradients in the function c(x, t).
It can be helpful to think of c as a distribution of heat or a chemical concentration.
This smoothing is particularly pronounced at points where c is initially discontinu-
ous, in this case at x = 0 only. DiÔ¨Äusion will also spread the initial data into x > 0,
where c = 0 initially. For this reason, we anticipate that there will be three distinct
asymptotic regions.

12.3 PARTIAL DIFFERENTIAL EQUATIONS
353
‚Äî Region I: x < 0. In this region we expect a gradual smoothing out of the initial
data.
‚Äî Region II: |x| ‚â™1. The major feature in this region will be an instantaneous
smoothing out of the initial discontinuity.
‚Äî Region III: x > 0. There will be a Ô¨Çux from x < 0 into this region, so we expect
an immediate change from c = 0 to c nonzero.
Thinking about the physics of the problem before doing any detailed calculation
is usually vital to unlocking the structure of the asymptotic solution of a partial
diÔ¨Äerential equation.
Let‚Äôs begin our analysis in region I by posing an asymptotic expansion valid for
t ‚â™1,
c(x, t) = f0(x) + tf1(x) + t2f2(x) + O(t3).
Substituting this into (12.128) gives
f1 = Df ‚Ä≤‚Ä≤
0 ,
2f2 = Df ‚Ä≤‚Ä≤
1 ,
and hence
c(x, t) = f0(x) + tDf ‚Ä≤‚Ä≤
0 (x) + 1
2t2D 2f ‚Ä≤‚Ä≤‚Ä≤‚Ä≤
0 (x) + O(t3).
(12.131)
Note that c increases in regions where f ‚Ä≤‚Ä≤
0 > 0, and vice versa, as physical intuition
would lead us to expect. Note also that as x ‚Üí0‚àí, c(x, t) ‚àºf0(0). However, in
x > 0 we would expect c to be small, and the solutions in regions I and III will not
match together without a boundary layer centred on the origin, namely region II.
Before setting up this boundary layer, it is convenient to Ô¨Ånd the solution in
region III, where we have noted that c is small. If we try a WKB expansion of the
form‚Ä†
c(x, t) = exp

‚àíA(x)
t
+ B(x) log t + C(x) + o(1)
	
,
and substitute into (12.128), we obtain
A = DA2
x,
AxBx = 0,
B = ‚àíD (Axx + 2AxCx) .
The solutions of these equations are
A = x2
4D + Œ≤x + DŒ≤2,
B = b,
C = ‚àí

b + 1
2

log (2Œ≤D + x) + d,
where Œ≤, b and d are constants of integration. The WKB solution in region III is
therefore
c = exp

‚àí1
t
 x2
4D + Œ≤x + DŒ≤2

+ b log t ‚àí

b + 1
2

log (2Œ≤D + x) + d + o(1)
	
.
(12.132)
‚Ä† Note that we are using the small time, t, in the WKB expansion that we developed in Sec-
tion 12.2.7.

354
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
In the boundary layer, region II, we deÔ¨Åne a new variable, Œ∑ = x/tŒ±. In region
II, |Œ∑| = O(1), and hence |x| = O(tŒ±) ‚â™1, with Œ± > 0 to be determined. In terms
of Œ∑, (12.128) becomes
‚àÇc
‚àÇt ‚àíŒ±
t Œ∑ ‚àÇc
‚àÇŒ∑ = D
t2Œ±
‚àÇ2c
‚àÇŒ∑2 .
Since c = O(1) in the boundary layer (remember, c must match with the solution
in region I as Œ∑ ‚Üí‚àí‚àû, where c = O(1)), we can balance terms in this equation
to Ô¨Ånd a distinguished limit when Œ± = 1/2.
The boundary layer therefore has
thickness of O(t1/2), which is a typical diÔ¨Äusive length scale. If we now expand as
c = c0(Œ∑) + o(1), we have, at leading order,
‚àí1
2Œ∑c0Œ∑ = Dc0Œ∑Œ∑.
(12.133)
If we now write the solutions in regions I and III, given by (12.131) and (12.132),
in terms of Œ∑, we arrive at the matching conditions
c0 ‚àºf0(0) as Œ∑ ‚Üí‚àí‚àû,
(12.134)
c0 ‚àºexp

‚àíDŒ≤2
t
‚àíŒ≤Œ∑
t1/2 ‚àíŒ∑2
4D + b log t‚àí

b + 1
2

log

2Œ≤D + t1/2Œ∑

+ d + o(1)
	
as Œ∑ ‚Üí‚àû.
(12.135)
The solution of (12.133) is
c0(Œ∑) = F + G
 Œ∑
‚àí‚àû
e‚àís2/4D ds.
As Œ∑ ‚Üí‚àí‚àû, c ‚àºF = f0(0). As Œ∑ ‚Üí‚àû, we can use integration by parts to show
that
c0 = f0(0) + G
 ‚àû
‚àí‚àû
e‚àís2/4D ds ‚àí2D
Œ∑ e‚àíŒ∑2/4D + O
 1
Œ∑2 e‚àíŒ∑2/4D
	
.
In order that this is consistent with the matching condition (12.135), we need
f0(0) + G
 ‚àû
‚àí‚àû
e‚àís2/4D ds = 0,
and hence G = ‚àíf0(0)/
‚àö
4œÄD. This leaves us with
c0 ‚àºexp

‚àíŒ∑2
4D + log (‚àí2DG) ‚àílog Œ∑
	
.
For this to be consistent with (12.135) we need Œ≤ = 0, b = 1/2 and d = log (‚àí2DG).
The structure of the solution that we have constructed allows us to be rather
more precise about how diÔ¨Äusion aÔ¨Äects the initial data. For |x| ‚â´t1/2, x < 0 there
is a slow smoothing of the initial data that involves algebraic powers of t, given by
(12.131). For |x| ‚â´t1/2, x > 0, c is exponentially small, driven by a diÔ¨Äusive Ô¨Çux
across the boundary layer. For |x| = O(t1/2) there is a boundary layer, with the

12.3 PARTIAL DIFFERENTIAL EQUATIONS
355
solution changing by O(1) over a small length of O(t1/2). This small time solution
continues to evolve, and, when t = O(1), is not calculable by asymptotic methods.
When t is suÔ¨Éciently large, a new asymptotic structure emerges, which we shall
consider next.
The large time solution, t ‚â´1
After a long time, diÔ¨Äusion will have spread out the initial data in a more or
less uniform manner, and the structure of the solution is rather diÔ¨Äerent from that
which we discussed above for t ‚â™1. We will start our asymptotic development
where x = O(1), and seek a solution of the form
c(x, t) = c0(t) + c1(x, t) + ¬∑ ¬∑ ¬∑ ,
with |c1| ‚â™|c0| for t ‚â´1 to ensure that the expansion is asymptotic.
If we
substitute this into (12.128), we obtain
Àôc0(t) = Dc1xx,
at leading order, which can be integrated to give
c1(x, t) = Àôc0(t)
2D x2 + Œ±¬±
1 (t)x + Œ≤¬±
1 (t).
(12.136)
The distinction between Œ±+
1 and Œ±‚àí
1 , and similarly for Œ≤¬±
1 , is to account for dif-
ferences in the solution for x > 0 and x < 0, introduced by the linear terms. As
|x| ‚Üí‚àû, c1 grows quadratically, which causes a nonuniformity in the expansion,
speciÔ¨Åcally when x = O
#
c0(t)/|Àôc0(t)|

. In order to deal with this, we introduce a
scaled variable, Œ∑ = x/
#
c0(t)/|Àôc0(t)|, with Œ∑ = O(1) for t ‚â´1 in this outer region.
In order to match with the solution in the inner region, where x = O(1), we need
c(Œ∑, t) ‚Üíc0(t) as Œ∑ ‚Üí0.
(12.137)
In terms of Œ∑, (12.128) becomes
‚àÇc
‚àÇt ‚àí1
2Œ∑ |Àôc0(t)|
c0(t)
d
dt
 c0(t)
|Àôc0(t)|
 ‚àÇc
‚àÇŒ∑ = |Àôc0(t)|
c0(t) D ‚àÇ2c
‚àÇŒ∑2 .
(12.138)
Motivated by the matching condition (12.137), we will try to solve this using the
expansion c = c0(t)F¬±(Œ∑) + o(c0(t)), subject to F¬± ‚Üí1 as Œ∑ ‚Üí0¬± and F¬± ‚Üí0
as Œ∑ ‚Üí¬±‚àû. The superscript ¬± indicates whether the solution is for Œ∑ > 0 or
Œ∑ < 0. It is straightforward to substitute this into (12.138), but this leads to some
options. The Ô¨Årst and third terms are of O(Àôc0(t)), whilst the second term is of
O(Àôc0(t) d
dt

c0(t)
| Àôc0(t)|

. Should we include the second term in the leading order balance
or not? Let‚Äôs see what happens if we do decide to balance these terms to get the
richest limit. We must then have c0(t)/Àôc0(t) = O(t), and hence c0 = Ct‚àíŒ± for some
constants C and Œ±. This looks like a sensible gauge function.
If we proceed, (12.138) becomes, at leading order,
F¬± ‚àí1
2Œ∑F ‚Ä≤
¬± = DF ‚Ä≤‚Ä≤
¬±.
(12.139)

356
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
This is slightly more diÔ¨Écult to solve than (12.133). However, if we look for a
quadratic solution, we quickly Ô¨Ånd that F¬± = Œ∑2 + 2D is a solution. Using the
method of reduction of order, the general solution of (12.139) is
F¬± = (Œ∑2 + 2D)

A¬± + B¬±
 Œ∑
0
e‚àís2/2D
(s2 + 2D)2 ds

.
(12.140)
As Œ∑ ‚Üí0, by Taylor expanding the integrand, we can show that
F¬± = (Œ∑2 + 2D)

A¬± + B¬± Œ∑
2D

+ O(Œ∑3).
To match with the inner solution, we therefore require that A¬± = 1/2D.
As
Œ∑ ‚Üí¬±‚àû, using integration by parts, we Ô¨Ånd that
F¬± = (Œ∑2 + 2D)

A¬± ¬± B¬±
 ‚àû
0
e‚àís2/2D
(s2 + 2D)2 ds + O
 1
Œ∑5 e‚àíŒ∑2/4D

,
so that we require
A¬± ¬± B¬±
 ‚àû
0
e‚àís2/2D
(s2 + 2D)2 ds = 0.
The outer solution can therefore be written as
F¬± =

1 + Œ∑2
2D
 
1 ‚àì
 Œ∑
0
e‚àís2/2D
(s2 + 2D)2 ds
5  ‚àû
0
e‚àís2/2D
(s2 + 2D)2 ds

.
(12.141)
In order to determine Œ±, and hence the size of c0(t), some further work is needed.
Firstly, we can integrate (12.128) and apply the initial condition, to obtain
 ‚àû
‚àí‚àû
c(x, t) dx =
 0
‚àí‚àû
f0(x) dx = ftot.
(12.142)
This just says that mass is conserved during the diÔ¨Äusion process. Secondly, we
can write down the composite expansion
c = cinner + couter ‚àí(cinner)outer = c0(t)F¬±(Œ∑),
and use this in (12.142) to obtain
 ‚àû
‚àí‚àû
c0(t)F¬±(Œ∑) dx = c0(t)
+
c0(t)
|Àôc0(t)|
 ‚àû
‚àí‚àû
F¬±(Œ∑) dŒ∑ = ftot.
This is now a diÔ¨Äerential equation for c0 in the form
c3/2
0
(t)
|Àôc0(t)|1/2 =
ftot
' ‚àû
‚àí‚àûF¬±(Œ∑) dŒ∑ .
In Exercise 12.18 we Ô¨Ånd that
' ‚àû
‚àí‚àûF¬±(Œ∑) dŒ∑ =
‚àö
2œÄD and hence that c0(t) =

12.3 PARTIAL DIFFERENTIAL EQUATIONS
357
ftot/
‚àö
4œÄDt. We can now calculate that
#
c0(t)/|Àôc0(t)| = O(t1/2), which is the
usual diÔ¨Äusive length scale. Our asymptotic solution therefore takes the form
c(x, t) ‚àº
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
ftot
‚àö
4œÄDt
for |x| = O(t1/2),
ftot
‚àö
4œÄDt
F¬±(Œ∑)
for |x| ‚â´t1/2.
(12.143)
The success of this approach justiÔ¨Åes our decision to choose c0(t) in order to obtain
the richest distinguished limit. Notice that the large time solution has ‚Äúforgotten‚Äù
the precise details of the initial conditions. It only ‚Äúremembers‚Äù the area under the
initial data, at leading order.
If we consider the particular case f0(t) = ex, we Ô¨Ånd (see Exercise 12.18) that
an exact solution is available, namely c(x, t) = 1
2ex+Dterfc

x
‚àö
4Dt +
‚àö
Dt

. This
solution is plotted in Figure 12.16 at various times, and we can clearly see the
structures that our asymptotic solutions predict emerging for both small and large
times. In Figure 12.17 we plot c(0, t) as a function of Dt. Our asymptotic solution
predicts that c(0, t) =
1
2 + o(1) for t ‚â™1, consistent with Figure 12.17(a).
In
Figure 12.17(b) we can see that the asymptotic solution, c(0, t) ‚àº1/
‚àö
4œÄDt as
t ‚Üí‚àû, is in excellent agreement with the exact solution.
Example 3: The wave equation with weak damping
(i) Linear damping
Consider the equation
‚àÇ2y
‚àÇt2 = c2 ‚àÇ2y
‚àÇx2 ‚àíœµ‚àÇy
‚àÇt ,
for t > 0 and ‚àí‚àû< x < ‚àû,
(12.144)
subject to the initial conditions
y(x, 0) = Y0(x),
‚àÇy
‚àÇt (x, 0) = 0,
(12.145)
with œµ ‚â™1. The one-dimensional wave equation, (12.144) with œµ = 0, governs the
small amplitude motion of an elastic string, which we met in Section 3.9.1. The
additional term, œµyt, represents a weak, linear damping, proportional to the velocity
of the string, for example due to drag on the string as it moves through the air.
The form of the initial conditions suggests that we should consider an asymptotic
expansion y = y0 + œµy1 + O(œµ2). On substituting this into (12.144) and (12.145) we
obtain
‚àÇ2y0
‚àÇt2 ‚àíc2 ‚àÇ2y0
‚àÇx2 = 0,
subject to y0(x, 0) = Y0(x), y0t(x, 0) = 0,
(12.146)
‚àÇ2y1
‚àÇt2 ‚àíc2 ‚àÇ2y1
‚àÇx2 = ‚àí‚àÇy0
‚àÇt ,
subject to y1(x, 0) = y1t(x, 0) = 0.
(12.147)
The initial value problem given by (12.146) is one that we studied in Section 3.9.1,
and has d‚ÄôAlembert‚Äôs solution, (3.43),
y0(x, t) = 1
2 {Y0(x ‚àíct) + Y0(x + ct)} .
(12.148)

358
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.16. The solution of the diÔ¨Äusion equation with f0(x) = ex at various times.
This solution represents the sum of two waves, one travelling to the left and one
travelling to the right, each with speed c, without change of form, and with half
the initial amplitude. This is illustrated in Figure 12.18 for the initial condition
Y0(x) = 1/(1 + x2). The splitting of the initial proÔ¨Åle into left- and right-travelling
waves is clearly visible.
In terms of the characteristic variables, Œæ = x ‚àíct, Œ∑ = x + ct, (12.147) becomes
‚àí4c2 ‚àÇ2y1
‚àÇŒæ‚àÇŒ∑ = c
‚àÇy0
‚àÇŒæ ‚àí‚àÇy0
‚àÇŒ∑

= 1
2c {Y ‚Ä≤
0(Œæ) ‚àíY ‚Ä≤
0(Œ∑)} .
Integrating this expression twice gives the solution
y1 = 1
8c {ŒæY0(Œ∑) ‚àíŒ∑Y0(Œæ)} + F1(Œæ) + G1(Œ∑).
(12.149)
The initial conditions show that
F1(x) + G1(x) = 0,
and
F ‚Ä≤
1(x) ‚àíG‚Ä≤
1(x) = 1
4c {xY ‚Ä≤
0(x) ‚àíY0(x)} ,

12.3 PARTIAL DIFFERENTIAL EQUATIONS
359
Fig. 12.17. The solution of the diÔ¨Äusion equation with f0(x) = ex at x = 0.
which can be integrated once to give
F1(x) ‚àíG1(x) = 1
4c

xY0(x) ‚àí2
 x
0
Y0(s) ds
	
+ b.
Finally,
F1(x) = ‚àíG1(x) = 1
8c

xY0(x) ‚àí2
 x
0
Y0(s) ds
	
+ 1
2b,
which, in conjunction with (12.149), shows that
y1 = ‚àí1
4t {Y0(x + ct) + Y0(x ‚àíct)} + 1
4c
 x+ct
x‚àíct
Y0(s) ds.
(12.150)
We can now see that y1 = O(t) for t ‚â´1, and therefore that our asymptotic
expansion becomes nonuniform when t = O(œµ‚àí1).
We will proceed using the method of multiple scales, deÔ¨Åning a slow time scale
T = œµt, and looking for a solution y = y(x, t, T). In terms of these new independent
variables, (12.144) becomes
ytt + 2œµytT + œµ2yT T = c2yxx ‚àíœµyt ‚àíœµ2yT .
(12.151)

360
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.18. The solution of (12.144) when c = 1 and Y0(x) = 1/(1 + x2) at equal time
intervals, t = 0, 4, 8, 12, 16, 20, when œµ = 0 and 0.1.
If we now seek an asymptotic solution of the form y = y0(x, t, T) + œµy1(x, t, T) +
O(œµ2), at leading order we obtain (12.146), as before, but now the solution is
y0(x, t, T) = F0(Œæ, T) + G0(Œ∑, T),
(12.152)
with
F0(Œæ, 0) = 1
2Y0(Œæ),
G0(Œ∑, 0) = 1
2Y0(Œ∑).
(12.153)
As usual in the method of multiple scales, we need to go to O(œµ) to determine F0
and G0. We Ô¨Ånd that
‚àí4cy1ŒæŒ∑ = F0Œæ ‚àíG0Œ∑ + 2 (F0ŒæT ‚àíG0Œ∑T ) .
(12.154)
On solving this equation, the presence of the terms of the right hand side causes y1
to grow linearly with t. In order to eliminate them, we must have
F0ŒæT = ‚àí1
2F0Œæ,
G0Œ∑T = ‚àí1
2G0Œ∑.
(12.155)

12.3 PARTIAL DIFFERENTIAL EQUATIONS
361
If we solve these equations subject to the initial conditions (12.153), we obtain
F0 = 1
2e‚àíT Y0(Œæ),
G0 = 1
2e‚àíT Y0(Œ∑),
and hence
y0 = 1
2e‚àíœµt/2 {Y0(x ‚àíct) + Y0(x + ct)} .
(12.156)
This shows that the small term, œµyt, in (12.144) leads to an exponential decay of
the amplitude of the solution over the slow time scale, t = O(œµ‚àí1), consistent with
our interpretation of this as a damping term. Figure 12.18 shows how this slow
exponential decay aÔ¨Äects the solution.
(ii) Nonlinear damping
What happens if we replace the linear damping term œµyt with a nonlinear damping
term, œµ(yt)3? We must then solve
‚àÇ2y
‚àÇt2 = c2 ‚àÇ2y
‚àÇx2 ‚àíœµ
‚àÇy
‚àÇt
3
,
for t > 0 and ‚àí‚àû< x < ‚àû,
(12.157)
subject to the initial conditions
y(x, 0) = Y0(x),
‚àÇy
‚àÇt (x, 0) = 0.
(12.158)
We would again expect a nonuniformity when t = O(œµ‚àí1), so let‚Äôs go straight to a
multiple scales expansion, y = y0(x, t, T) + œµy1(x, t, T). At leading order, as before,
we have (12.152) and (12.153). At O(œµ),
‚àí4cy1ŒæŒ∑ = c2(F0Œæ ‚àíG0Œ∑)3 + 2 (F0ŒæT ‚àíG0Œ∑T ) .
(12.159)
In order to see clearly which terms are secular, we integrate the expression (F0Œæ ‚àí
G0Œ∑)3 twice to obtain
Œ∑
 Œæ
0
F 3
0Œæ(s) ds ‚àí3G0(Œ∑)
 Œæ
0
F 2
0Œæ(s) ds + 3F0(Œæ)
 Œ∑
0
G2
0Œæ(s) ds ‚àíŒæ
 Œ∑
0
G3
0Œ∑(s) ds.
Assuming that F0(s) and G0(s) are integrable as s ‚Üí¬±‚àû, we can see that the
terms that become unbounded as Œæ and Œ∑ become large are those associated with
F 3
0Œæ and G3
0Œ∑. We conclude that, to eliminate secular terms in (12.159), we need
F0ŒæT = ‚àí1
2c2F 3
0Œæ,
G0Œ∑T = ‚àí1
2c2G3
0Œ∑,
to be solved subject to (12.153). The solutions are
F0Œæ =
Y ‚Ä≤
0(Œæ)
$
4 + c2T {Y ‚Ä≤
0(Œæ)}2 ,
G0Œ∑ =
Y ‚Ä≤
0(Œ∑)
$
4 + c2T {Y ‚Ä≤
0(Œ∑)}2 ,
and hence
y0 = ‚àí
Ô£Æ
Ô£∞
 ‚àû
Œæ
Y ‚Ä≤
0(s)
$
4 + c2T {Y ‚Ä≤
0(s)}2 ds +
 ‚àû
Œ∑
Y ‚Ä≤
0(s)
$
4 + c2T {Y ‚Ä≤
0(s)}2 ds
Ô£π
Ô£ª.
(12.160)

362
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
In general, these integrals cannot be determined analytically, but we can see that
the amplitudes of the waves do decay as T increases, and are of O(T ‚àí1/2) for T ‚â´1.
Example 4: The measurement of oil fractions using local electrical probes
As we remarked at the beginning of the book, many problems that arise in engi-
neering are susceptible to mathematical modelling. We can break the modelling
process down into separate steps.
(i) Identify the important physical processes that are involved.
(ii) Write down the governing equations and boundary conditions.
(iii) DeÔ¨Åne dimensionless variables and identify dimensionless constants.
(iv) Solve the governing equations using either a numerical method or an asymp-
totic method.
Note that, although it is possible that we can Ô¨Ånd an analytical solution, this is
highly unlikely when studying real world problems. As we discussed at the start
of Chapter 11, when one or more of the dimensionless parameters is small, we can
use an asymptotic solution technique. Let‚Äôs now discuss an example of this type of
situation.
For obvious reasons, oil companies are interested in how much oil is coming out
of their oilwells, and often want to make this measurement at the point where oil
is entering the well as droplets, rather than at the surface. One tool that can be
lowered into a producing oilwell to assist with this task is a local probe. This is
a device with a tip that senses whether it is in oil or water. The output from the
probe can be time-averaged to give the local oil fraction at the tip, and an array
of probes deployed to give a measurement of how the oil fraction varies across the
well. We will consider a simple device that distinguishes between oil and water by
measuring electrical conductivity, which is several orders of magnitude higher in
saline water than in oil.
The geometry of the electrical probe, which is made from sharpening the tip
of a coaxial cable like a pencil, is shown in Figure 12.19. A voltage is applied to
the core of the probe, whilst the outer layer, or cladding, is earthed. A measure-
ment of the current between the core and the cladding is then made to determine
the conductivity of the surrounding medium. Although this measurement gives a
straightforward way of distinguishing between oil and water when only one liquid
is present, for example when dipping the probe into a beaker containing a single
liquid, the diÔ¨Éculty lies in interpreting the change in conductivity as a droplet of
oil approaches, meets, deforms around and is penetrated by the probe. If we want
to understand and model this process, there is clearly a diÔ¨Écult Ô¨Çuid mechanical
problem to be solved before we can begin to relate the conÔ¨Åguration of the oil
droplet to the current through the probe (see Billingham and King, 1995). We will
pre-empt all of this Ô¨Çuid mechanical complexity by considering what happens if, in
the course of the interaction of an oil droplet with a probe, a thin layer of oil forms
on the surface of the probe. How thin must this oil layer become before the current
through the probe is eÔ¨Äectively equal to that of a probe in pure water?

12.3 PARTIAL DIFFERENTIAL EQUATIONS
363
Fig. 12.19. A cross-section through an axisymmetric electrical probe.
In order to answer this question, we must solve a problem in electrostatics,
since the speed at which oil‚Äìwater interfaces move is much less than the speed at
which electromagnetic disturbances travel (the speed of light)‚Ä†. The electrostatic
potential, œÜ, is an axisymmetric solution of Laplace‚Äôs equation,
‚àá2œÜ = 0.
(12.161)
We will assume that the conducting parts of the probe are perfect conductors,
so that
œÜ =
 1
at the surface of the core,
0
at the earthed surface of the cladding.
(12.162)
At interfaces between diÔ¨Äerent media, for example oil and water or oil and insulator,
we have the jump conditions
[œÜ] = 0,

œÉ ‚àÇœÜ
‚àÇn

= 0.
(12.163)
Square brackets indicate the change in the enclosed quantity across an interface, œÉ is
the conductivity, which is diÔ¨Äerent in each medium (oil, water and insulator), and
‚àÇ/‚àÇn is the derivative in the direction normal to the interface. Equation (12.163)
represents continuity of potential and continuity of current at an interface.
To
complete the problem, we have the far Ô¨Åeld conditions that
œÜ ‚Üí0 as r2 + z2 ‚Üí‚àûoutside the probe,
(12.164)
and
œÜ ‚àºœÜ‚àû(r) as z ‚Üí‚àûfor r0 < r < r1,
(12.165)
using cylindrical polar coordinates coaxial with the probe, and r = 0 at the tip. Here
r0 and r1 are the inner and outer radii of the insulator, as shown in Figure 12.19.
‚Ä† This, in itself, is an asymptotic approximation that can be made rigorous by deÔ¨Åning a small
parameter, the ratio of a typical Ô¨Çuid speed to the speed of light. Some approximations are,
however, so obvious that justifying them rigorously is a little too pedantic.
For a simple
introduction to electromagnetism, see Billingham and King (2001).

364
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
The far Ô¨Åeld potential must satisfy
‚àá2œÜ‚àû(r) = d 2œÜ‚àû
dr2
+ 1
r
dœÜ‚àû
dr
= 0,
for r0 < r < r1,
subject to
œÜ = 1 at r = r0, œÜ = 0 at r = r1.
This has solution
œÜ‚àû= log(r1/r)
log(r1/r0).
(12.166)
Finally, we will assume that the probe is surrounded by water, except for a uniform
layer of oil on its surface of thickness h ‚â™r0. Our task is to solve the boundary
value problem given by (12.161) to (12.165).
This is an example of a problem where the governing equation and boundary
conditions are fairly straightforward, but the geometry is complicated. Problems
like this are usually best solved numerically. However, in this case we have one
region where the aspect ratio is small ‚Äì the thin oil Ô¨Ålm. The potential is likely
to change rapidly across this Ô¨Ålm compared with its variation elsewhere, and a
numerical method will have diÔ¨Éculty handling this. We can, however, make some
progress by looking for an asymptotic solution in the thin Ô¨Ålm. The Ô¨Årst thing
to do is to set up a local coordinate system in the oil Ô¨Ålm. The quantities h and
r0 are the natural length scales with which to measure displacements across and
along the Ô¨Ålm, so we let Œ∑ measure displacement across the Ô¨Ålm, with Œ∑ = 0 at
the surface of the probe and Œ∑ = 1 at the surface of the water, and let Œæ measure
displacement along the Ô¨Ålm, with Œæ = 0 at the probe tip and Œæ = 1 a distance
r0 from the tip. Away from the tip and the edge of the probe, which we will not
consider for the moment, this provides us with an orthogonal coordinate system,
and (12.161) becomes
‚àÇ2œÜ
‚àÇŒ∑2 + Œ¥2 ‚àÇ2œÜ
‚àÇŒæ2 = 0,
where
Œ¥ = h
r0
‚â™1.
At leading order, ‚àÇ2œÜ/‚àÇŒ∑2 = 0, and hence œÜ varies linearly across the Ô¨Ålm, with
œÜ = A(Œæ)Œ∑ + B(Œæ).
(12.167)
Turning our attention now to (12.163)2, since we expect variations of œÜ in the water
and the insulator to take place over the geometrical length scale r0, we have
Œ¥j
‚àÇœÜ
‚àÇŒ∑ = Œ¥ ‚àÇœÜ
‚àÇn at interfaces,
(12.168)
where
Œ¥j = œÉo
œÉj
,

12.3 PARTIAL DIFFERENTIAL EQUATIONS
365
with the subscripts o, w and i indicating oil, water and insulator respectively.
We expect that Œ¥w ‚â™1 and Œ¥i = O(1), since oil and the insulator have similar
conductivities, both much less than that of saline water. We conclude that, at the
interface between oil and insulator, at leading order ‚àÇœÜ/‚àÇŒ∑ = 0, and hence A(Œæ) = 0
there. Also, from the conditions at the surface of the cladding and core, (12.162),
B(Œæ) = 0 at the cladding and B(Œæ) = 1 at the core.
Returning now to (12.168) with j = w, note that we have two small parameters,
Œ¥ and Œ¥w. Double limiting processes like this (Œ¥ ‚Üí0, Œ¥w ‚Üí0) have to be treated
with care, as the Ô¨Ånal result usually depends on how fast one parameter tends
to zero compared with the other. In this case, we obtain the richest asymptotic
balance by assuming that
Œ¥
Œ¥w
= K = hœÉw
r0œÉo
= O(1) as Œ¥ ‚Üí0,
and
‚àÇœÜ
‚àÇn = K ‚àÇœÜ
‚àÇŒ∑ = KA(Œæ).
We can now combine all of the information that we have, to show that at the
surface of the probe, the potential in the water satisÔ¨Åes
‚àÇœÜ
‚àÇn =
Ô£±
Ô£≤
Ô£≥
K (œÜ ‚àí1)
at the surface of the core,
0
at the surface of the insulator,
KœÜ
at the surface of the cladding.
(12.169)
The fact that the oil Ô¨Ålm is thin allows us to apply these conditions at the surface
of the probe at leading order. The key point is that this asymptotic analysis allows
us to eliminate the thin Ô¨Ålm from the geometry of the problem at leading order,
and instead include its eÔ¨Äect in the boundary conditions (12.169). The solution of
(12.161) subject to (12.164), (12.165) and (12.169) in the region outside the probe
is geometrically simple, and easily achieved using a computer. We will not show
how to do this here, as it is outside the scope of this book. We can, however, extract
one vital piece of information from our analysis. We have proceeded on the basis
that K = O(1). What happens if K ‚â´1 or K ‚â™1? If K ‚â´1, at leading order
(12.169) becomes
œÜ =
 1
at the surface of the core,
0
at the surface of the cladding,
‚àÇœÜ
‚àÇn = 0 at the surface of the insulator.
(12.170)
These are precisely the boundary conditions that would apply at leading order in
the absence of an oil layer. We conclude that if K ‚â´1, and hence h ‚â™r0œÉo/œÉw,
the Ô¨Ålm of oil is too thin to prevent a current from passing from core to cladding
through the water, and the oil cannot be detected by the probe. If K ‚â™1, at
leading order (12.169) becomes ‚àÇœÜ/‚àÇn = 0 at the surface of the probe, and hence
œÜ = 0 in the water. This then shows that œÜ = 1 ‚àíŒ∑ in the oil Ô¨Ålm over the core
and œÜ = 0 in the rest of the Ô¨Ålm, from which it is straightforward to calculate

366
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
the current Ô¨Çowing from core to cladding. In this case the oil Ô¨Ålm dominates the
response of the probe, eÔ¨Äectively insulating it from the water outside.
We conclude that there is a critical oil Ô¨Ålm thickness, hc = r0œÉo/œÉw. For h ‚â™hc
the oil Ô¨Ålm is eÔ¨Äectively invisible, for h ‚â´hc the external Ô¨Çuid is eÔ¨Äectively invisible,
and the current through the probe is determined by the thickness of the Ô¨Ålm, whilst
for h = O(hc) both the oil and water aÔ¨Äect the current through the boundary
conditions (12.169). For a typical oil and saline water, hc ‚âà10‚àí9 m. This is such a
small length that, in practice, any thin oil Ô¨Ålm coating a probe insulates it from the
external Ô¨Çuid, and can lead to practical diÔ¨Éculties with this technique. In reality,
local probes are used with alternating rather than direct current driving the core.
One helpful eÔ¨Äect of this is to increase the value of hc, due to the way that the
impedances‚Ä† of oil and water change with the frequency of the driving potential.
Exercises
12.1
Determine the Ô¨Årst two terms in the asymptotic expansion for 0 < œµ ‚â™1
of all the roots of each of the equations
(a) x3 + œµx2 ‚àíx + œµ = 0,
(b) œµx3 + x2 ‚àí1 = 0,
(c) œµx4 + (1 ‚àí3œµ)x3 ‚àí(1 + 3œµ)x2 ‚àí(1 + œµ)x + 1 = 0,
(d) œµx4 + (1 ‚àí3œµ)x3 ‚àí(1 ‚àí3œµ)x2 ‚àí(1 + œµ)x + 1 = 0.
In each case, sketch the left hand side of the equation for œµ = 0 and œµ ‚â™1.
12.2
The function y(x) satisÔ¨Åes the ordinary diÔ¨Äerential equation
œµy‚Ä≤‚Ä≤ + (4 + x2)(y‚Ä≤ + 2y) = 0,
for 0 ‚©Ωx ‚©Ω1,
subject to y(0) = 0 and y(1) = 1, with œµ ‚â™1. Show that a boundary layer is
possible only at x = 0. Use the method of matched asymptotic expansions
to determine two-term inner and outer expansions, which you should match
using either Van Dyke‚Äôs matching principle, or an intermediate variable.
Hence show that
y‚Ä≤(0) ‚àº4e2
œµ
‚àí4e2 + 8e2 tan‚àí1
1
2

as œµ ‚Üí0.
Construct a composite expansion, valid up to O(œµ).
12.3
Determine the leading order outer and inner approximations to the solution
of
œµy‚Ä≤‚Ä≤ + x1/2y‚Ä≤ + y = 0 for 0 ‚©Ωx ‚©Ω1,
subject to y(0) = 0 and y(1) = 1, when œµ ‚â™1. Hence show that
y‚Ä≤(0) ‚àºœµ‚àí2/3
e2
Œì
 2
3

3
2
1/3
.
‚Ä† the a.c. equivalents of the conductivities.

EXERCISES
367
12.4
The function y(x) satisÔ¨Åes the ordinary diÔ¨Äerential equation
œµy‚Ä≤‚Ä≤ + (1 + x)y‚Ä≤ ‚àíy + 1 = 0,
for 0 ‚©Ωx ‚©Ω1, subject to the boundary conditions y(0) = y(1) = 0, with
œµ ‚â™1, where a prime denotes d/dx. Determine a two-term inner expansion
and a one-term outer expansion. Match the expansions using either Van
Dyke‚Äôs matching principle or an intermediate region. Hence show that
y‚Ä≤(0) ‚àº1
2œµ + 1 as œµ ‚Üí0.
12.5
Consider the boundary value problem
œµ(2y + y‚Ä≤‚Ä≤) + 2xy‚Ä≤ ‚àí4x2 = 0 for ‚àí1 ‚©Ωx ‚©Ω2,
subject to
y(‚àí1) = 2,
y(2) = 7,
with œµ ‚â™1. Show that it is not possible to have a boundary layer at either
x = ‚àí1 or x = 2. Determine the rescaling needed for an interior layer at
x = 0. Find the leading order outer solution away from this interior layer,
and the leading order inner solution. Match these two solutions, and hence
show that y(0) ‚àº2 as œµ ‚Üí0. Sketch the leading order solution.
Now determine the outer solutions up to O(œµ). Show that a term of
O(œµ log œµ) is required in the inner expansion. Match the two-term inner
and outer expansions, and hence show that y(0) = 2 ‚àí3
2œµ log œµ + O(œµ) for
œµ ‚â™1.
12.6
Consider the ordinary diÔ¨Äerential equation
œµy‚Ä≤‚Ä≤ + yy‚Ä≤ ‚àíy = 0 for 0 ‚©Ωx ‚©Ω1,
subject to y(0) = Œ±, y(1) = Œ≤, with Œ± and Œ≤ constants, and œµ ‚â™1.
(a) Assuming that there is a boundary layer at x = 0, determine the
leading order inner and outer solutions when Œ± = 0 and Œ≤ = 3.
(b) Assuming that there is an interior layer at x = x0, determine the
leading order inner and outer solutions, and hence show that x0 =
1/2 when Œ± = ‚àí1 and Œ≤ = 1.
12.7
Use the method of multiple scales to determine the leading order solution,
uniformly valid for t ‚â™œµ‚àí2, of
d 2y
dt2 + y = œµy3
dy
dt
2
,
subject to y = 1, dy/dt = 0 when t = 0, for œµ ‚â™1.
12.8
Consider the ordinary diÔ¨Äerential equation
¬®y + œµ Àôy + y + œµ2y cos2 t = 0,
for t ‚©æ0, subject to y(0) = 1, Àôy(0) = 0, where a dot denotes d/dt. Use the
method of multiple scales to determine a two-term asymptotic expansion,
uniformly valid for all t ‚â™œµ‚àí3 when œµ ‚â™1.

368
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
12.9
Consider the ordinary diÔ¨Äerential equation
¬®y + œµ Àôy + y + œµ2y3 = 0,
for t ‚©æ0, subject to y(0) = 1, Àôy(0) = 0, where a dot denotes d/dt. Use the
method of multiple scales, with
T = œµt,
œÑ = t + œµ2at,
y ‚â°y(œÑ, T),
to show that
y ‚àºe‚àíT/2 cos œÑ for œµ ‚â™1.
Show further that a = ‚àí1/8 and determine the next term in the asymptotic
expansion. (You will need to consider the Ô¨Årst three terms in the asymptotic
expansion for y.)
12.10
Show that when v0 ‚â™1, (12.81) becomes dE/dT = ‚àí2E at leading order.
12.11
Consider the initial value problem
d 2y
dt2 ‚àí2œµdy
dt ‚àíy + y3 = 0,
(E12.1)
subject to
y(0) = yi > 1,
dy
dt (0) = 0.
(E12.2)
Use a graphical argument to show that when œµ = 0, y is positive, provided
that yi <
‚àö
2. Use Kuzmak‚Äôs method to determine the leading order ap-
proximation to the solution when 0 < œµ ‚â™1 and 1 < yi <
‚àö
2. You should
check that your solution is consistent with the linearized solution when
yi ‚àí1 ‚â™1. Hence show that y Ô¨Årst becomes negative when t = t0 ‚àºT0/œµ,
where
T0 =
 0
1
2 y2
i(y2
i ‚àí2)
3K
 1
k

4
‚àö
1 + 2E
‚àö
1 + 2E + 1

√ó
1

(2k2 ‚àí1) L
 1
k

‚àí2 (k2 ‚àí1) K
 1
k
dE,
k ‚â°k(E) =
‚àö
1 + 2E + 1
2
‚àö
1 + 2E
1/2
.
Hints:
(a) The leading order solution can be written in terms of the Jacobian
elliptic function cn (see Section 9.4).
(b)
 1
$
1‚àí1
k2
#
1 ‚àíx2#
1 ‚àík2 + k2x2 dx
= 1
3k

2k2 ‚àí1

L
1
k

‚àí2

k2 ‚àí1

K
1
k
	
.

EXERCISES
369
12.12
Show that when D(x, ÀÜx) = D0(x) ÀÜD(ÀÜx) and R(Œ∏(x), x, ÀÜx) = R0(Œ∏(x), x) ÀÜR(ÀÜx),
the homogenized diÔ¨Äusion coeÔ¨Écient and reaction term given by (12.101)
and (12.102) can be written in the simple form described in Section 12.2.6.
12.13
Give a physical example that would give rise to the initial‚Äìboundary value
problem
‚àÇŒ∏
‚àÇt = ‚àÇ
‚àÇx

D

x, x
œµ
 ‚àÇŒ∏
‚àÇx

for 0 < x < 1,
subject to
Œ∏(x, 0) = Œ∏i(x) for 0 ‚©Ωx ‚©Ω1,
‚àÇŒ∏
‚àÇx = 0 at x = 0 and x = 1 for t > 0.
When 0 < œµ ‚â™1, use homogenization theory to show that, at leading
order, Œ∏ is a function of x and t only, and satisÔ¨Åes an equation of the form
F(x)‚àÇŒ∏
‚àÇt = ‚àÇ
‚àÇx

¬ØD(x) ‚àÇŒ∏
‚àÇx

,
where ¬ØD(x) is given by (12.101), and F(x) is a function that you should
determine. If D(x, x/œµ) = D0(x) ÀÜD(x/œµ), show that, at leading order, Œ∏
satisÔ¨Åes the diÔ¨Äusion equation
‚àÇŒ∏
‚àÇÀÜt = ‚àÇ
‚àÇx

D0(x) ‚àÇŒ∏
‚àÇx

,
where
ÀÜt =
t
limœµ‚Üí0

2œµ2 ' 1/œµ
0
s
ÀÜ
D(s)ds
.
12.14
Use the WKB method to Ô¨Ånd the eigensolutions of the diÔ¨Äerential equation
y‚Ä≤‚Ä≤(x) + (Œª ‚àíx2)y(x) = 0,
subject to y ‚Üí0 as |x| ‚Üí‚àû, when Œª ‚â´1.
12.15
Find the Ô¨Årst two terms in the WKB approximation to the solution of the
fourth order equation
œµy‚Ä≤‚Ä≤‚Ä≤‚Ä≤(x) = {1 ‚àíœµV (x)} y(x)
that satisÔ¨Åes V (¬±‚àû) = 0 and y(¬±‚àû) = 0, when œµ ‚â™1.
12.16
Solve the connection problem
œµ2y‚Ä≤‚Ä≤(x) + cx2y(x) = 0,
subject to y(0) = 1 and y ‚Üí0 as x ‚Üí‚àí‚àû, when œµ ‚â™1.
12.17
By determining the next term in the WKB expansion, verify that the over-
lap domain for the inner and outer solutions of the boundary value problem
(12.115) is œµ2/3 ‚â™|x| ‚â™œµ2/5 (see the footnote just after (12.121)).

370
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
12.18
Use Fourier transforms to solve (12.128) subject to (12.129) with f0(x) =
ex, and hence show that c(x, t) =
1
2ex+Dterfc

x
‚àö
4Dt +
‚àö
Dt

.
Use this
expression to show that c(0, t) ‚àº1/
‚àö
4œÄDt as t ‚Üí‚àû.
By comparing
this result with the large time asymptotic solution that we derived in Sec-
tion 12.3, show that
' ‚àû
‚àí‚àûF¬±(Œ∑) dŒ∑ =
‚àö
2œÄD.
12.19
Find the small time solution of the reaction‚ÄìdiÔ¨Äusion equation
ut = Duxx + Œ±u,
subject to the initial condition
u(x, 0) =
 f0(x)
for |x| < Œ±,
0
for |x| ‚©æŒ±.
What feature of the solution arises as a result of the reaction term, Œ±u?
12.20
Find the leading order solution of the partial diÔ¨Äerential equation
ct = (D + œµx)2cxx for x > 0, t > 0,
when œµ ‚â™1, subject to the initial condition c(x, 0) = f(x) and the bound-
ary condition c(0, t) = 0. Your solution should remain valid for large x and
t.
12.21
Find a uniformly valid solution of the hyperbolic equation
œµ(ut + ux) + (t ‚àí1)2u = 1 for ‚àí‚àû< x < ‚àû, t > 0,
when œµ ‚â™1, subject to the initial condition u(x, 0) = 0.
12.22
Find the leading order asymptotic solution of
œµ(uxx + uyy) + ux + Œ≤uy = 0 for x > 0, 0 < y < L,
when œµ ‚â™1, subject to the boundary conditions u(x, 0) = f(x), u(0, y) =
g(y) and u(x, L) = 0.
Your solution should be uniformly valid in the
domain of solution, so you will need to resolve any boundary layers that
are required.
12.23
Find a uniformly valid leading order asymptotic solution of
œµ(utt ‚àíc2uxx) + ut + Œ±ux = 0 for ‚àí‚àû< x < ‚àû, t > 0,
when œµ ‚â™1, subject to the initial conditions u(x, 0) = f(x), ut(x, 0) = 0.
How does the solution for c > |Œ±| diÔ¨Äer from the solution when c < |Œ±|?
12.24
Project: The triple deck
Consider the innocuous looking two-point boundary value problem
œµy‚Ä≤‚Ä≤ + x3y‚Ä≤ + (x3 ‚àíœµ)y = 0,
subject to y(0) = 1
2, y(1) = 1,
(E12.3)
with œµ ‚â™1.
(a) Show that the outer solution is y = e1‚àíx.

EXERCISES
371
(b) Since the outer solution does not satisfy the boundary condition at
x = 0, there must be a boundary layer. By writing ¬Øx = x/œµ¬µ, show
that there are two possibilities, ¬µ = 1 and ¬µ = 1
2.
As there is now a danger of confusion in our notation, we will deÔ¨Åne
¬Øx = x/œµ, and refer to the region where ¬Øx = O(1) as the lower deck,
and x‚àó= x/œµ1/2, and refer to the region where x‚àó= O(1) as the
middle deck. We will refer to the region where x = O(1) as the
upper deck. This nomenclature comes from the study of boundary
layers in high Reynolds number Ô¨Çuid Ô¨Çow.
(c) Show that the leading order solution in the lower deck is ¬Øy = 1
2e‚àí¬Øx,
and in the middle deck y‚àó= a exp

‚àí1/2 (x‚àó)2
.
(d) Apply the simplest form of matching condition to show that a = e,
and hence that the solution in the middle deck is
y‚àó= exp

1 ‚àí
1
2 (x‚àó)2

.
(e) The matching between the lower and middle decks requires that
lim
¬Øx‚Üí‚àû¬Øy = lim
x‚àó‚Üí0 y‚àó,
which is satisÔ¨Åed automatically without Ô¨Åxing any constants. Show
that the composite solution takes the form
y = e1‚àíx + exp

1 ‚àí
1
2 (x‚àó)2

+ 1
2e‚àí¬Øx ‚àíe.
(f) Integrate (E12.3) numerically using MATLAB, and compare the nu-
merical solution with the asymptotic solution for œµ = 0.1, 0.01 and
0.001. Can you see the structure of the triple deck?
(g) The equations for steady, high Reynolds number Ô¨Çow of a viscous
Newtonian Ô¨Çuid with velocity u and pressure p are
‚àá¬∑ u = 0,
(u ¬∑ ‚àáu) = ‚àí‚àáp + 1
Re‚àá2u,
where Re is the Reynolds number.
If these are to be solved in
‚àí‚àû< x < ‚àû, y > f(x) subject to u = 0 on y = f(x) and u ‚àºUi
as x2 + y2 ‚Üí‚àû, where would you expect a triple or double deck
structure to appear?
You can Ô¨Ånd some more references and background material in
Sobey (2000). From the data in this book, estimate the maximum
thickness of the boundary layer on a plate 1 m long, with water
Ô¨Çowing past it at 10 m s‚àí1.

CHAPTER THIRTEEN
Stability, Instability and Bifurcations
In this chapter, we will build upon the ideas that we introduced in Chapter 9,
which was concerned with phase plane methods for nonlinear, autonomous, ordinary
diÔ¨Äerential equations. We will study what happens when an equilibrium point has
one or more eigenvalues with zero real part. These are nonhyperbolic equilibrium
points, which we were unable to study by simple linearization in Chapter 9. Next,
we will introduce the idea of Lyapunov functions, and show how they can be used
to study the stability of nonhyperbolic equilibrium points. We will also consider
diÔ¨Äerential equations that contain a parameter, and examine what can happen to
the qualitative form of the solutions as the parameter varies. At parameter values
where one or more equilibrium points is nonhyperbolic, a local bifurcation point,
the qualitative nature of the solutions changes. Finally, we will look at an example
of a global bifurcation.
13.1
Zero Eigenvalues and the Centre Manifold Theorem
Let‚Äôs consider the structure of an equilibrium point which has one zero eigenvalue
and one negative eigenvalue. After shifting the equilibrium point to the origin,
writing the system in terms of coordinates with axes in the directions of the eigen-
vectors, and rescaling time so that the negative eigenvalue has unit magnitude, the
system will have the form
Àôx = P(x, y),
Àôy = ‚àíy + Q(x, y),
(13.1)
where P and Q are nonlinear functions with P(0, 0) = Q(0, 0) = 0. The linear
approximation to (13.1) is Àôx = 0, Àôy = ‚àíy, which has the solution x = 0, y = y0e‚àít
that passes through the origin. This shows that points on the y-axis close to the
origin approach the origin as t ‚Üí‚àû. We say that the local stable manifold‚Ä† in
some neighbourhood, U, of the origin is
œâs(0, U) = {(x, y) | x = 0,
(x, y) ‚ààU} .
This is the set of points that are, locally, attracted to the origin in the direction
of the eigenvector that corresponds to the negative eigenvalue. We are going to
have to work harder to determine the behaviour of the other integral paths near
the origin.
‚Ä† We will carefully deÔ¨Åne what we mean by a manifold in Section 13.1.2.

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
373
In order to work out what is going on, we deÔ¨Åne a local centre manifold for
(13.1) to be a Ck curve œâc(0, U) in a neighbourhood U of the origin such that
(i) œâc(0, U) is an invariant set within U, so that if x(0) ‚ààœâc(0, U), then
x(t) ‚ààœâc(0, U) when x(t) ‚ààU. In other words, a solution that initially
lies on the centre manifold remains there when it lies in U.
(ii) œâc(0, U) is the graph of a Ck function that is tangent to the x-axis at the
origin, so that
œâc(0, U) =

(x, y) | y = h(x), h(0) = dh
dx(0) = 0,
(x, y) ‚ààU
	
.
This gives us the picture of the local stable and centre manifolds shown in Fig-
ure 13.1. The main idea now is that the qualitative behaviour of the solution in
x
y
œâc
œâs
Fig. 13.1. The local stable and centre manifolds of the system (13.1).
the neighbourhood of the origin, excluding the stable manifold, is determined by
the behaviour of the solution on the centre manifold. This means that the local
dynamics are governed by a Ô¨Årst order diÔ¨Äerential equation.
Theorem 13.1 (The centre manifold theorem) The equilibrium point at the
origin of the system (13.1) is stable/unstable if and only if the equilibrium point at
x = 0 of the Ô¨Årst order diÔ¨Äerential equation
Àôx = P(x, h(x)),
(13.2)
where y = h(x) is the local centre manifold, is stable/unstable. Integral paths in the

374
STABILITY, INSTABILITY AND BIFURCATIONS
neighbourhood of the local centre manifold are attracted onto the centre manifold
exponentially fast.
Proof This can be found in Wiggins (1990).
13.1.1
Construction of the Centre Manifold
Now that we know that a local centre manifold exists, how can we determine its
equation, y = h(x)? Since Àôy = Àôxdh/dx, (13.2) shows that
dh
dxP(x, h(x)) = ‚àíh(x) + Q(x, h(x)),
(13.3)
subject to
h(0) = dh
dx(0) = 0.
(13.4)
We will not usually be able to solve (13.3) analytically, but we can proceed to
determine the local form of the centre manifold by assuming that h(x) can be
represented as a power series,
h(x) = a2x2 + a3x3 + ¬∑ ¬∑ ¬∑ ,
(13.5)
which automatically satisÔ¨Åes (13.4).
As an example, let‚Äôs consider the system
Àôx = ax3 + xy,
Àôy = ‚àíy + y2 + x2y + x3
(13.6)
with a > 0. This is of the form (13.1), so we need to substitute (13.5) into (13.3).
This shows that
(2a2x + 3a3x2 + ¬∑ ¬∑ ¬∑ )(ax3 + a2x3 + a3x4 + ¬∑ ¬∑ ¬∑ )
= ‚àía2x2 ‚àía3x3 + (a2x2 + a3x3 + ¬∑ ¬∑ ¬∑ )2 + x2(a2x2 + a3x3 + ¬∑ ¬∑ ¬∑ ) + x3.
Equating powers of x gives us a2 = 0 at O(x2) and a3 = +1 at O(x3), so that
the centre manifold is given by y = h(x) = +x3 + ¬∑ ¬∑ ¬∑ . On the centre manifold,
we therefore have Àôx = ax3 ‚àíx4 + ¬∑ ¬∑ ¬∑ . For |x| ‚â™1 we can ignore the quartic term
and just consider Àôx ‚âàax3, so that Àôx > 0 for x > 0 and Àôx < 0 for x < 0. Integral
paths that begin on the local centre manifold therefore asymptote to the origin as
t ‚Üí‚àí‚àû, and we conclude that the local phase portrait is of a nonlinear saddle,
as shown in Figure 13.2. More speciÔ¨Åcally, if x = x0 on the centre manifold when
t = 0, x ‚âàx0/
#
1 ‚àí2ax2
0t, so that x ‚àºx0/
#
‚àí2ax2
0t as t ‚Üí‚àí‚àû. This algebraic
behaviour on the centre manifold is in contrast to the exponential behaviour that
occurs on the unstable separatrix of a linear saddle point.
Example: Travelling waves in cubic autocatalysis
If two chemicals, which we label A and B, react through a mechanism known as
cubic autocatalysis, we write
A + 2B ‚Üí3B,
rate kab2,
(13.7)

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
375
x
y
œâc
œâs
Fig. 13.2. The local phase portrait for the system (13.6).
where k is the reaction rate constant and a and b are the concentrations of the
two chemicals, which are measured in moles m‚àí3‚Ä†. The chemical B is known as the
autocatalyst, since it catalyses its own production. The greater the concentration
of B, the faster it is produced by the reaction (13.7). If these two chemicals then
react in a long thin tube, so that their concentrations only vary in the x-direction,
the main physical processes that act, in the absence of any underlying Ô¨Çuid Ô¨Çow,
are chemical reaction and one-dimensional diÔ¨Äusion. We can derive the partial dif-
ferential equations that govern this situation using the arguments that we described
in Section 12.2.3, example 2. SpeciÔ¨Åcally, since the rate of change of the amount
of each chemical in a control volume is equal to the total diÔ¨Äusive Ô¨Çux through
the bounding surface plus the total rate of production of that chemical within the
volume, we Ô¨Ånd that
‚àÇa
‚àÇt = D ‚àÇ2a
‚àÇx2 ‚àíkab2,
‚àÇb
‚àÇt = D ‚àÇ2b
‚àÇx2 + kab2,
(13.8)
where t is time and D the constant diÔ¨Äusivity of the chemicals. If a small amount
of the autocatalyst is introduced locally into a spatially uniform expanse of A with
a = a0, waves of chemical reaction will propagate away from the initiation site. We
can study one of these by looking for a wave that propagates from left to right at
constant speed v > 0, represented by a solution of the form a = a(y), b = b(y),
where y = x ‚àívt. Such a solution is called a travelling wave solution.‚Ä°
With a and b functions of y alone, (13.8) become nonlinear ordinary diÔ¨Äerential
‚Ä† A mole is a Ô¨Åxed number (Avogadro‚Äôs number ‚âà6.02 √ó 1023) of molecules of the substance.
‚Ä° For more background and details on reaction‚ÄìdiÔ¨Äusion equations and travelling wave solutions,
see Billingham and King (2001).

376
STABILITY, INSTABILITY AND BIFURCATIONS
equations,
Dd 2a
dy2 + v da
dy ‚àíkab2 = 0,
Dd 2b
dy2 + v db
dy + kab2 = 0,
(13.9)
to be solved subject to the boundary condition
a ‚Üía0,
b ‚Üí0 as y ‚Üí‚àû.
(13.10)
This represents the unreacted state ahead of the wave. If we now add equations
(13.9), we obtain a linear diÔ¨Äerential equation for a + b, which we can solve to
obtain
a + b = k0 + k1e‚àívy/D.
The boundary condition (13.10) then shows that k0 = a0. We also require that a+b
is bounded as y ‚Üí‚àí‚àû, so that k1 = 0, and hence a + b = a0. We can therefore
eliminate a from (13.9) and arrive at a second order system
db
dy = c,
D dc
dy = ‚àívc ‚àíkb2 (a0 ‚àíb) ,
(13.11)
subject to
b ‚Üí0,
c ‚Üí0 as y ‚Üí‚àû.
(13.12)
Note that, since we require that both of the chemical concentrations should be
positive for a physically meaningful solution, we need 0 ‚©Ωb ‚©Ωa0.
The only equilibrium points of (13.11) are b = c = 0 and b = a0, c = 0. The
second of these represents the fully reacted state behind the travelling wave, where
all of the chemical A has been converted into the autocatalyst, B, so we also require
that
b ‚Üía0,
c ‚Üí0 as y ‚Üí‚àí‚àû.
(13.13)
We can write the boundary value problem given by (13.11) to (13.13) in a more
convenient form by deÔ¨Åning the dimensionless variables
Œ≤ = b
a0
,
Œ≥ =
*
ka2
0
D
c
a0
z =
*
ka2
0
D y,
V =
v
#
ka2
0D
,
so that
dŒ≤
dz = Œ≥,
dŒ≥
dz = ‚àíV Œ≥ ‚àíŒ≤2 (1 ‚àíŒ≤) ,
(13.14)
subject to
Œ≤ ‚Üí0,
Œ≥ ‚Üí0 as z ‚Üí‚àû,
(13.15)
Œ≤ ‚Üí1,
Œ≥ ‚Üí0 as z ‚Üí‚àí‚àû,
(13.16)
and 0 ‚©ΩŒ≤ ‚©Ω1 for a physically meaningful solution. The dimensionless wave speed,
V , is now the only parameter.
The next step is to study the solutions of (13.14) subject to (13.15) and (13.16) in
the (Œ≤, Œ≥) phase plane and determine for what values of the wave speed, V , solutions

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
377
exist. As a preliminary, let‚Äôs see if we can guess a solution of this system. A plausible
functional form is Œ≥ = kŒ≤(1 ‚àíŒ≤), which satisÔ¨Åes the boundary conditions. Since
dŒ≥
dŒ≤ = ‚àíV ‚àíŒ≤2(1 ‚àíŒ≤)
Œ≥
,
(13.17)
we Ô¨Ånd that we can satisfy this equation with k = ‚àí1/
‚àö
2 and V = 1/
‚àö
2. This
gives
dŒ≤
dz = ‚àí1
‚àö
2Œ≤(1 ‚àíŒ≤),
and hence
Œ≤ = Œ≤e =
1
1 + e(z‚àíz0)/
‚àö
2 ,
Œ≥ = Œ≥e = ‚àí1
‚àö
2
e(z‚àíz0)/
‚àö
2
(1 + e(z‚àíz0)/
‚àö
2)2 .
(13.18)
This is an exact solution for V = 1/
‚àö
2 and any constant z0, as shown in Fig-
ure 13.3. The presence of z0 in (13.18) simply shows that the solution can be given
an arbitrary displacement in the z-direction and remain a solution, as we would
expect for a wave that propagates at constant speed without change of form.
Fig. 13.3. The analytical solution of the cubic travelling wave problem, Œ≤ = Œ≤e(z), with
z0 = 0.
So now we know that a solution exists for V = 1/
‚àö
2. What about other values of
V ? Let‚Äôs go along our usual route, and determine the nature of the two equilibrium
points, P1 = (1, 0) and P2 = (0, 0). The Jacobian is
J =

0
1
‚àí2Œ≤ + 3Œ≤2
‚àíV

.

378
STABILITY, INSTABILITY AND BIFURCATIONS
At P1, the eigenvalues are real and of opposite sign, so that P1 is a saddle point.
Boundary condition (13.16) shows that we need the integral path that represents
the solution to asymptote to P1 as z ‚Üí‚àí‚àû. Since the unstable separatrices of P1
are the only integral paths that do this, the solution must be represented by the
unstable separatrix of P1 that lies in the physically meaningful region, 0 ‚©ΩŒ≤ ‚©Ω1,
shown in Figure 13.4 as S1. The other boundary condition, (13.15), shows that
we need S1 to asymptote to the other equilibrium point, P2, as z ‚Üí‚àû, if it is to
represent a solution.
S1
P1
P2
S2
Œ≤
Œ≥
Fig. 13.4. The local behaviour in the neighbourhood of the two equilibrium points.
At P2 = (0, 0), the eigenvalues are ‚àíV and zero, with associated eigenvectors
e‚àí= (1, ‚àíV ) and e0 = (1, 0), respectively, so that this is a nonhyperbolic equi-
librium point. Since V > 0, there is a local stable manifold in the direction of e‚àí
and also a local centre manifold tangent to the Œ≤-axis (the direction of e0). We
can construct a local approximation to the centre manifold, Œ≥ = h(Œ≤), by assuming
that Œ≥ ‚àºAŒ≤2 as Œ≤ ‚Üí0 for some constant A. The governing equations, (13.14),
then show that
dŒ≥
dz ‚àº2AŒ≤ dŒ≤
dz ‚àº2AŒ≤Œ≥ ‚àº2A2Œ≤3,
and hence that
2A2Œ≤3 ‚àº‚àíV AŒ≤2 ‚àíŒ≤2(1 ‚àíŒ≤).
By equating coeÔ¨Écients of Œ≤2, we Ô¨Ånd that A = ‚àí1/V , and hence that the local
centre manifold has Œ≥ ‚àº‚àíŒ≤2/V as Œ≤ ‚Üí0. This means that Œ≥ = dŒ≤/dz < 0 on
the local centre manifold. Points on the centre manifold in Œ≤ > 0 are therefore
attracted to P2 as z ‚Üí‚àûwith, from dŒ≤/dz ‚àº‚àíŒ≤2/V , Œ≤ ‚àºV/z as z ‚Üí‚àû. In
contrast, points on the centre manifold in Œ≤ < 0 are swept away as z increases. This
type of behaviour is characteristic of a new type of equilibrium point, known as a
saddle‚Äìnode. To the right of the stable manifold, the point behaves like a stable
node, attracting integral paths onto the centre manifold and into the origin, whilst
to the left of the stable manifold the point is unstable, as shown in Figure 13.4.

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
379
Apart from the local stable manifold, on which Œ≤ = O(e‚àíV z) as z ‚Üí‚àû(recall that
the negative eigenvalue of P2 is ‚àíV ), any other integral paths that asymptote to
the origin as z ‚Üí‚àûdo so on the centre manifold, with Œ≤ ‚àºV/z.
Finally, for any given value of V , we need to determine whether the integral path
S1 lies to the right of the stable manifold of P2, which we label S2, and therefore
enters the origin and represents a solution, or whether S1 lies to the left of S2, is
swept away from the origin into Œ≤ < 0, and therefore does not represent a physically
meaningful solution. We will return to this problem in Section 13.3.3.
13.1.2
The Stable, Unstable and Centre Manifolds
We end this section by deÔ¨Åning more carefully what we mean by a manifold,
and generalizing the deÔ¨Ånitions of the stable, unstable and centre manifolds to nth
order systems.
Let‚Äôs begin with some deÔ¨Ånitions. A homeomorphism is a mapping f : L ‚ÜíN
that is one-to-one, onto and continuous and has a continuous inverse. Here, L and
N are subsets of Rn. A Ck diÔ¨Äeomorphism is a mapping f : L ‚ÜíN that is
one-to-one, onto and k times diÔ¨Äerentiable with a k-times diÔ¨Äerentiable inverse. A
smooth diÔ¨Äeomorphism is a C‚àûdiÔ¨Äeomorphism and a homeomorphism is a C0
diÔ¨Äeomorphism. An m-dimensional manifold is a set M ‚äÇRn for which each
x ‚ààM has a neighbourhood U in which there exists a homeomorphism œÜ : U ‚ÜíRm,
where m ‚©Ωn. A manifold is said to be diÔ¨Äerentiable if there is a diÔ¨Äeomorphism
rather than a homeomorphism œÜ : U ‚ÜíRm.
For example, a smooth curve in
R3 is a one-dimensional diÔ¨Äerentiable manifold, and the surface of a sphere is a
two-dimensional diÔ¨Äerentiable manifold.
Now that we have deÔ¨Åned these ideas, let‚Äôs consider the behaviour of the solutions
of
dx
dt = f(x),
(13.19)
where x, f(x) ‚ààRn. In the neighbourhood of an equilibrium point, ¬Øx, of (13.19),
there exist three invariant manifolds.
(i) The local stable manifold, œâs
loc, of dimension s, is spanned by the eigen-
vectors of A whose eigenvalues have real parts less than zero.
(ii) The local unstable manifold, œâu
loc, of dimension u, is spanned by the
eigenvectors of A whose eigenvalues have real parts greater than zero.
(iii) The local centre manifold, œâc
loc, of dimension c, is spanned by the eigen-
vectors of A whose eigenvalues have zero real parts.
Note that s + c + u = n. Solutions lying in œâs
loc are characterized by exponential
decay and those in œâu
loc by exponential growth. The behaviour on the centre mani-
fold is determined by the nonlinear terms in (13.19), as we described earlier in this
section. For a linear system these manifolds exist globally, whilst for a nonlinear
system they exist in some neighbourhood of the equilibrium point.

380
STABILITY, INSTABILITY AND BIFURCATIONS
Example
Let‚Äôs consider the simple system
Àôx = x(2 ‚àíx),
Àôy = ‚àíy + x,
(13.20)
and try to determine the equations of the local manifolds that pass through the
equilibrium point at the origin. The Jacobian at the origin is
J =
 2
0
1
‚àí1

,
which has eigenvalues 2 and ‚àí1 and associated eigenvectors of (3, 1)T and (0, 1)T
respectively, so that œâu
loc is the line y = x/3 and œâs
loc is the y-axis. We can solve
the nonlinear system directly, since the equation for x is independent of y, and the
equation for y is linear. The solution is
x =
2A
A + 2e‚àít ,
y = e‚àít

B + 2et ‚àí4
A log(Aet + 2)
	
,
where A Ã∏= 0 and B are constants.
There is also the obvious solution x = 0,
y = Be‚àít, the y-axis, which gives the local stable manifold, œâs
loc, and also the
global stable manifold, œâs(¬Øx). Points that lie in the local unstable manifold,
œâu
loc, have y ‚Üí0 as t ‚Üí‚àí‚àû. Since y ‚àºe‚àít (B ‚àí4 log 2/A) as t ‚Üí‚àí‚àû, we must
have B = A + 4/A log 2, so that the global unstable manifold, œâu(¬Øx), is given
in parametric form by

2A
A + 2e‚àít , e‚àít

A + 4
A log 2 + 2et ‚àí4
A log(Aet + 2)
	
.
The phase portrait is sketched in Figure 13.5.
x
y
œâu
œâs
œâu
œâs
Fig. 13.5. The phase portrait of the system (13.20).

13.2 LYAPUNOV‚ÄôS THEOREMS
381
13.2
Lyapunov‚Äôs Theorems
Although we are now familiar with the idea of the stability of an equilibrium point
in an informal way, in order to develop the idea of a Lyapunov function, we need
to consider some more formal deÔ¨Ånitions of stability. Note that, in this section,
we will consider systems of autonomous ordinary diÔ¨Äerential equations, written in
vector form as Àôx = f(x), that have an isolated equilibrium point at the origin.
An equilibrium point, x = xe, is Lyapunov stable if for all œµ > 0 there exists
a Œ¥ > 0 such that for all x(0) with |x(0) ‚àíxe| < Œ¥, |x(t) ‚àíxe| < œµ for all t > 0. In
other words, integral paths that start close to a Lyapunov stable equilibrium point
remain close for all time, as shown in Figure 13.6.
x0
|x ‚àí xe| < Œµ
‚àÄ t > 0
xe
Œ¥
Œµ
Fig. 13.6. A Lyapunov stable equilibrium point of a second order system.
An equilibrium point, x = xe, is asymptotically stable if there exists a Œ¥ > 0
such that for all x(0) with |x(0) ‚àíxe| < Œ¥, |x(t) ‚àíxe| ‚Üí0 as t ‚Üí‚àû. This is
a stronger deÔ¨Ånition of stability than Lyapunov stability, and states that integral
paths that start suÔ¨Éciently close to an asymptotically stable equilibrium point are
attracted into it, as illustrated in Figure 13.7. Stable nodes and spirals are both
Lyapunov and asymptotically stable. It should be clear that asymptotically stable
equilibrium points are also Lyapunov stable. However, Lyapunov stable equilibrium
points, for example centres, are not necessarily asymptotically stable.
Now that we have formalized our notions of stability, we need one more new
concept. Let V : ‚Ñ¶‚ÜíR, where ‚Ñ¶‚äÇRn and 0 ‚àà‚Ñ¶. We say that the function V is
positive deÔ¨Ånite on ‚Ñ¶if and only if V (0) = 0 and V (x) > 0 for x ‚àà‚Ñ¶, x Ã∏= 0. For
example, for n = 3, V (x) = V (x1, x2, x3) = x2
1 + x2
2 + x2
3 is positive deÔ¨Ånite in R3,
whilst V (x1, x2, x3) = x2
2 is not, since it is zero on the plane x2 = 0, and not just
at the origin. Note that if ‚àíV (x) is a positive deÔ¨Ånite function, we say that V is a
negative deÔ¨Ånite function. In the following, we will assume that V is continuous

382
STABILITY, INSTABILITY AND BIFURCATIONS
xe
|x ‚àí xe| ‚Üí 0
as t  ‚Üí‚àû
x0
Œ¥
Fig. 13.7. An asymptotically stable equilibrium point of a second order system.
and has well-deÔ¨Åned partial derivatives with respect to each of its arguments, so
that V ‚ààC1(‚Ñ¶).
We can now introduce the idea of the derivative of a function V (x) with respect
to the system Àôx = f(x) = (f1, f2, . . . , fn), which is deÔ¨Åned to be the scalar product
V ‚àó(x) = ‚àáV ¬∑ f(x) = ‚àÇV
‚àÇx1
f1(x) + ¬∑ ¬∑ ¬∑ + ‚àÇV
‚àÇxn
fn(x).
This derivative can be calculated for given V (x) and f(x), without knowing the
solution of the diÔ¨Äerential equation. In particular,
dV
dt = ‚àÇV
‚àÇx1
Àôx1 + ¬∑ ¬∑ ¬∑ + ‚àÇV
‚àÇxn
Àôxn = ‚àÇV
‚àÇx1
f1(x) + ¬∑ ¬∑ ¬∑ + ‚àÇV
‚àÇxn
fn(x) = V ‚àó(x),
so that the total derivative of V with respect to the solution of the equations
coincides with our deÔ¨Ånition of the derivative with respect to the system. This
allows us to prove three important theorems.
Theorem 13.2 If, in some region ‚Ñ¶‚äÇRn that contains the origin, there exists
a scalar function V (x) that is positive deÔ¨Ånite and for which V ‚àó(x) ‚©Ω0, then the
origin is Lyapunov stable. The function V (x) is known as a Lyapunov function.
Proof
Since V is positive deÔ¨Ånite in ‚Ñ¶, there exists a sphere of radius r > 0‚Ä†
contained within ‚Ñ¶such that
V (x) > 0 for x Ã∏= 0 and |x| < r,
V ‚àó(x) ‚©Ω0 for |x| ‚©Ωr.
‚Ä† the set of points with |x| ‚©Ωr in Rn.

13.2 LYAPUNOV‚ÄôS THEOREMS
383
Let x = x(t) be the solution of the diÔ¨Äerential equation Àôx = f(x) with x(0) = x0.
By the local existence theorem, Theorem 8.1, extended to higher order systems,
this solution exists for 0 ‚©Ωt < t‚àówith t‚àó> 0. This solution can then be continued
for t ‚©æt‚àó, and we denote by t1 the largest value of t for which the solution exists.
There are two possibilities, either t1 = ‚àûor t1 < ‚àû. We now show that, for |x0|
suÔ¨Éciently small, t1 = ‚àû.
From the deÔ¨Ånition of the derivative with respect to a system,
dV
dt (x(t)) = V ‚àó(x(t)) for 0 ‚©Ωt < t1.
We can integrate this equation to give
V (x(t)) ‚àíV (x0) =
 t
0
V ‚àó(x(s)) ds ‚©Ω0,
since V ‚àóis negative deÔ¨Ånite. This means that 0 < V (x(t)) ‚©ΩV (x0) for 0 ‚©Ωt < t1.
Now let œµ satisfy 0 < œµ ‚©Ωr, and let S be the closed, spherical shell with inner and
outer radii œµ and r‚Ä†. By continuity of V , and since S is closed, ¬µ = minx‚ààS V (x)
exists and is strictly positive. Since V (x) ‚Üí0 as |x| ‚Üí0, we can choose Œ¥ with
0 < Œ¥ < ¬µ such that for |x0| ‚©ΩŒ¥, V (x0) < ¬µ, so that 0 < V (x(t)) ‚©ΩV (x0) < ¬µ
for 0 ‚©Ωt < t1. Since ¬µ is the minimum value of V in S, this gives |x(t)| < œµ for
0 ‚©Ωt < t1. If there exists t2 such that |x(t2)| = œµ, then, when t = t2, we also
have, from the deÔ¨Ånition of ¬µ, ¬µ ‚©ΩV (x(t2)) ‚©ΩV (x0) < ¬µ, which cannot hold. We
conclude that t1 = ‚àû, and that, for a given œµ > 0, there exists a Œ¥ > 0 such that
when |x0| < Œ¥, |x(t)| < œµ for t ‚©æ0, and hence that the origin is Lyapunov stable.
The proofs of the following two theorems are rather similar, and we will not give
them here.
Theorem 13.3 If, in some region ‚Ñ¶‚äÇRn that contains the origin, there exists
a scalar function V (x) that is positive deÔ¨Ånite and for which V ‚àó(x) is negative
deÔ¨Ånite, then the origin is asymptotically stable.
Theorem 13.4 If, in some region ‚Ñ¶‚äÇRn that contains the origin, there exists a
scalar function V (x) such that V (0) = 0 and V ‚àó(x) is either positive deÔ¨Ånite or
negative deÔ¨Ånite, and if, in every neighbourhood N of the origin with N ‚äÇ‚Ñ¶, there
exists at least one point a such that V (a) has the same sign as V ‚àó(a), then the
origin is unstable.
Theorems 13.2 to 13.4 are known as Lyapunov‚Äôs theorems, and have a geometri-
cal interpretation that is particularly attractive for two-dimensional systems. The
equation V (x) = c then represents a surface in the (x, y, V )-space. By varying c
(through positive values only, since V is positive deÔ¨Ånite), we can obtain a series of
contour lines, with V = 0 at the origin a local minimum on the surface, as shown
in Figure 13.8. Since Àôx = f(x), the vector Ô¨Åeld f represents the direction taken by
‚Ä† the set of points with œµ ‚©Ω|x| ‚©Ωr in Rn.

384
STABILITY, INSTABILITY AND BIFURCATIONS
an integral path at any point. The vector normal to the surface V (x) = c is ‚àáV ,
so that, if V ‚àó= ‚àáV ¬∑ f ‚©Ω0, integral paths cannot point into the exterior of the
region V (x) < c. We conclude that an integral path that starts suÔ¨Éciently close
to the origin, for example with V (x0) < c1, cannot leave the region bounded by
V (x) = c1, and hence that the origin is Lyapunov stable. Similarly, if V ‚àó< 0, the
integral paths must actually cross from the exterior to the interior of the region.
Hence V will decrease monotonically to zero from its initial value when the integral
path enters the region ‚Ñ¶, and we conclude that the origin is asymptotically stable.
V
V(x) = c2
V(x) = c2
y
x
y
x
V(x) = c1 < c2
V(x) = c1 < c2
f(x)
(b)
(a)
‚àáV
Fig. 13.8. (a) The local behaviour and (b) a contour plot of a Lyapunov function near the
origin.
Although we can now see why a Lyapunov function is useful, it can take consid-
erable ingenuity to actually construct one for a given system.
Example 1
Consider the system
Àôx = ‚àíx ‚àí2y2,
Àôy = xy ‚àíy3.
The origin is the only equilibrium point, and the linearized system is Àôx = ‚àíx, Àôy = 0.
The eigenvalues are therefore 0 and ‚àí1, so this is a nonhyperbolic equilibrium point.
Let‚Äôs try to construct a Lyapunov function. We start by trying V = x2 +Œ±y2. This
is clearly positive deÔ¨Ånite for Œ± > 0, and V (0, 0) = 0. In addition,
V ‚àó= dV
dt = 2x(‚àíx ‚àí2y2) + 2Œ±y(xy ‚àíy3) = ‚àí2x2 + 2(Œ± ‚àí2)xy2 ‚àí2Œ±y4.
If we choose Œ± = 2, then dV/dt = ‚àí2x2 ‚àí4y4 < 0 for all x and y excluding
the origin.
From Theorems 13.2 and 13.3 we conclude that the origin is both
Lyapunov and asymptotically stable. As a general guideline, it is worth looking for
a homogeneous, algebraic Lyapunov function when f has a simple algebraic form.

13.2 LYAPUNOV‚ÄôS THEOREMS
385
Example 2
Consider the second order diÔ¨Äerential equation ¬®Œ∏ + f(Œ∏) = 0 for ‚àíœÄ ‚©ΩŒ∏ ‚©ΩœÄ, with
Œ∏f(Œ∏) positive, f diÔ¨Äerentiable and f(0) = 0. We can write this as a second order
system,
Àôx1 = x2,
Àôx2 = ‚àíf(x1),
where x1 = Œ∏. The origin is an equilibrium point, but is it stable? In order to
construct a Lyapunov function, it is helpful to think in terms of an equivalent
physical system.
By analogy with the model for a simple pendulum, which we
discussed in Section 9.1, we can think of f(Œ∏) as the restoring force and ÀôŒ∏ as the
angular velocity.
The total energy of the system is the sum of the kinetic and
potential energies, which we can write as E =
1
2 ÀôŒ∏2 +
' Œ∏
0 f(s) ds.
If this energy
were to decrease/not grow, we would expect the motionless, vertical state of the
pendulum to be asymptotically/Lyapunov stable. Guided by this physical insight,
we deÔ¨Åne
V = 1
2x2
2 +
 x1
0
f(s) ds.
Clearly V (0, 0) = 0, and, since
' x1
0
f(s) ds is positive by the assumption that
Œ∏f(Œ∏) ‚©æ0, V is positive deÔ¨Ånite for ‚àíœÄ ‚©Ωx1 ‚©ΩœÄ. Finally,
V ‚àó= dV
dt = f(x1)x2 + x2 ¬∑ ‚àíf(x1) = 0.
By Theorem 13.2, V is a Lyapunov function, and the origin is Lyapunov stable.
Example 3
Consider the diÔ¨Äerential equation ¬®x+ Àôx+x+x2 = 0. We can write this as a second
order system,
Àôx1 = x2,
Àôx2 = ‚àíx1 ‚àíx2
1 ‚àíx2,
(13.21)
where x1 = x. This has two equilibrium points, at (‚àí1, 0) and (0, 0). It is straight-
forward to determine the eigenvalues of these equilibrium points and show that both
are hyperbolic, with (‚àí1, 0) a saddle point and (0, 0) a stable, clockwise spiral. We
can now construct a Lyapunov function that will give us some idea of the domain
of attraction of the stable equilibrium point at the origin. Consider the function
V = 1
2(x2
1 + x2
2) + 1
3x3
1.
This function vanishes at the origin and is positive in the region
‚Ñ¶=

(x1, x2) | x2
2 > ‚àíx2
1 ‚àí2
3x3
1
	
,
which is sketched, along with the phase portrait, in Figure 13.9.
Let‚Äôs consider the curve V = 1
6, which passes through the saddle point and the
point ( 1
2, 0), as shown in Figure 13.10. If V = V0 < 1
6, we have a curve that encloses
the origin, but not the saddle point. By taking V = V0 < 1
6 arbitrarily close to
1
6, we can make the curve V = V0 arbitrarily close to the saddle point. As we are

386
STABILITY, INSTABILITY AND BIFURCATIONS
‚àí2
‚àí1.5
‚àí1.5
‚àí1
‚àí0.5
1.5
1
0.5
0
‚àí1
‚àí0.5
0
x1
x2
0.5
1
Fig. 13.9. The region ‚Ñ¶and the phase portrait of the system (13.21). The region ‚Ñ¶lies
to the right of the curved, broken line, which is V = 0.
interested in the domain of attraction of the equilibrium point at the origin, we will
focus on ‚Ñ¶0, the subset of ‚Ñ¶given by V < V0 < 1
6, with V0 close to 1
6.
Since
V ‚àó= dV
dt = x2

x1 + x2
1

+

‚àíx1 ‚àíx2
1 ‚àíx2

x2 = ‚àíx2
2 ‚©Ω0,
we immediately have from Theorem 13.3 that the origin is Lyapunov stable. To
prove asymptotic stability requires more work, as V ‚àó= 0 on x2 = 0, which could
allow trajectories to escape from the region ‚Ñ¶0 through the two points where V = V0
meets the x1-axis, which are labelled as A and B in Figure 13.10. There are various
ways of dealing with this.
The obvious one is to choose a diÔ¨Äerent Lyapunov
function, which is possible, but technically diÔ¨Écult.
We will use a phase plane
analysis. Consider SA, the integral path through the point A. All other integral
paths through the boundary of ‚Ñ¶0 in the neighbourhood of A enter ‚Ñ¶0. The integral
path SA cannot, therefore, lie along the boundary of ‚Ñ¶0. If SA does not enter ‚Ñ¶0,
it must intersect the integral path that enters ‚Ñ¶0 at x2 = 0+, which is not possible.
We conclude that the integral path through A enters ‚Ñ¶0, as shown in Figure 13.11.
A similar argument holds at B.
Since the Lyapunov function, V , is monotone decreasing away from the x1-axis,
there cannot be any limit cycle solutions in ‚Ñ¶0. Finally, since ‚Ñ¶0 has all integral
paths entering it, and contains a single, stable equilibrium point and no limit cycles,

13.2 LYAPUNOV‚ÄôS THEOREMS
387
Fig. 13.10. The regions ‚Ñ¶and ‚Ñ¶‚àóand the curves V = 0, V = 1
6 and V = V0 < 1
6.
we conclude from the Poincar¬¥e‚ÄìBendixson theorem, Theorem 9.4, that all integral
paths in ‚Ñ¶0 enter the origin, which is therefore asymptotically stable, and that
‚Ñ¶0 lies within the domain of attraction of the origin. In fact, we can see from
Figure 13.9 that this domain of attraction is considerably larger than ‚Ñ¶0, and is
bounded by the stable separatrices of the saddle point at (‚àí1, 0).
Example 4
Consider the system
Àôx = x2 ‚àíy2,
Àôy = ‚àí2xy.
This is a genuinely nonlinear system with an equilibrium point at the origin. The
linear approximation at the origin is Àôx = Àôy = 0, so both eigenvalues are zero. Let‚Äôs
try a Lyapunov function of the form V = Œ±xy2 ‚àíx3, which has V (0, 0) = 0. Since
V ‚àó= dV
dt = (Œ±y2 ‚àí3x2)(x2 ‚àíy2) ‚àí4Œ±x2y2 = 3(1 ‚àíŒ±)x2y2 ‚àíŒ±y4 ‚àí3x4,
we can choose Œ± = 1, so that V ‚àó= ‚àíy4 ‚àí3x4, which is negative deÔ¨Ånite. We can
see that V = x(y2 ‚àíx2) = 0 when x = 0 or y = ¬±x, so that V changes sign six
times on any circle that surrounds the origin. In particular, in every neighbourhood
of the origin there is at least one point where V has the same sign as V ‚àó, so that
all of the conditions of Theorem 13.4 are satisÔ¨Åed by V , and hence the origin is
unstable.

388
STABILITY, INSTABILITY AND BIFURCATIONS
‚àí1
‚àí0.96
‚àí0.98
‚àí0.15
‚àí0.1
‚àí0.05
0.05
‚àí0.2
0.2
0.15
0.1
0
‚àí0.94 ‚àí0.92 ‚àí0.9
‚àí0.88 ‚àí0.86 ‚àí0.84 ‚àí0.82 ‚àí0.8
x1
A
V = V0
x2
Fig. 13.11. The phase portrait in the neighbourhood of the point A and the saddle point
at (‚àí1, 0).
Further reÔ¨Ånements exist of the Lyapunov Theorems 13.2 to 13.4 that we have
studied in this section, and the interested reader is referred to Coddington and
Levinson (1955) for further information.
13.3
Bifurcation Theory
13.3.1
First Order Ordinary DiÔ¨Äerential Equations
Let‚Äôs consider the Ô¨Årst order ordinary diÔ¨Äerential equation, (9.5), whose hyper-
bolic equilibrium points we studied in Section 9.2. For a hyperbolic equilibrium
point at x = x1, we saw that a simple linearization about x = x1 determines the
local behaviour and stability. If X‚Ä≤(x1) = 0, x = x1 is a nonhyperbolic equilibrium
point, and we need to retain more terms in the Taylor expansion of X, (9.6), in
order to sort out what happens close to the equilibrium point. For example, if
X(x1) = X‚Ä≤(x1) = 0 and X‚Ä≤‚Ä≤(x1) Ã∏= 0,
d¬Øx
dt ‚âà1
2X‚Ä≤‚Ä≤(x1)¬Øx2
for ¬Øx ‚â™1,

13.3 BIFURCATION THEORY
389
and hence
¬Øx ‚âà‚àí
2
X‚Ä≤‚Ä≤(x1)(t ‚àít0),
for some constant t0. The graph of X(x1+¬Øx) close to ¬Øx = 0 is shown in Figure 13.12
for X‚Ä≤‚Ä≤(x1) < 0. Focusing on this case, we can see that ¬Øx ‚Üí0 (x ‚Üíx1) as t ‚Üí‚àû
for t0 < 0, whilst ¬Øx ‚Üí‚àí‚àûas t ‚Üít0 for t0 > 0. This nonhyperbolic equilibrium
point therefore attracts solutions from x ‚©æx1 and repels them from x < x1. Note
that the rate at which solutions asymptote to x = x1 from x > x1 is algebraic,
in contrast to the faster, exponential approach associated with stable hyperbolic
equilibrium points.
dx
 dt
x
Fig. 13.12. The graph of d¬Øx/dt = X(x1 + ¬Øx) ‚âà1
2X‚Ä≤‚Ä≤(x1)¬Øx2 for X‚Ä≤‚Ä≤(x1) < 0.
A system that contains one or more nonhyperbolic equilibrium points is said
to be structurally unstable. This means that a small perturbation, not to the
solution but to the model itself, for example the addition of a small extra term to
X(x), can lead to a qualitative diÔ¨Äerence in the structure of the set of solutions, for
example, a change in the number of equilibrium points or in their stability. Consider
the function shown in Figure 13.12. The addition of a small positive constant to
X(x) would shift the graph upwards by a small amount, and give two equilibrium
solutions, whilst the addition of a small negative constant would shift the graph
downwards and lead to the absence of any equilibrium solutions. Let‚Äôs investigate
this further.
Consider the equation
Àôx = ¬µ ‚àíx2,
(13.22)

390
STABILITY, INSTABILITY AND BIFURCATIONS
where ¬µ is a parameter. For ¬µ > 0 there are two hyperbolic equilibrium points
at x = ¬±‚àö¬µ, whilst for ¬µ < 0 there are no equilibrium points, as shown in Fig-
ure 13.13. When ¬µ = 0, X(x) = ‚àíx2 and x = 0 is a nonhyperbolic equilibrium
point of the type that we analyzed above. We can now draw a bifurcation di-
agram, which shows the position of the equilibrium solutions as a function of ¬µ,
with stable equilibria as solid lines and unstable equilibria as dashed lines, as shown
in Figure 13.14. The point ¬µ = 0, x = 0 is called a bifurcation point, because
the qualitative nature of the phase line changes there. The bifurcation associated
with Àôx = ¬µ ‚àíx2 is called a saddle‚Äìnode bifurcation, for reasons that will become
clear in Section 13.3.2. Any Ô¨Årst order system that undergoes a saddle‚Äìnode bifur-
cation, in other words one that contains a bifurcation point where two equilibrium
solutions meet and then disappear, can be written in the form Àôx = ¬µ ‚àíx2 in the
neighbourhood the bifurcation point. The equation Àôx = ¬µ‚àíx2 is called the normal
form for the saddle‚Äìnode bifurcation.
x
x.
x
x.
(b)
(a)
Fig. 13.13. Graphs of Àôx = ¬µ ‚àíx2 for (a) ¬µ > 0, (b) ¬µ < 0.
Example
Consider the ordinary diÔ¨Äerential equation
Àôy = Œª ‚àí2Œªy ‚àíy2.
(13.23)
This has equilibrium points at y = ‚àíŒª ¬±
‚àö
Œª2 + Œª, so there are no real equilibrium
points for ‚àí1 < Œª < 0 and two equilibrium points otherwise. This suggests that
there are saddle‚Äìnode bifurcations at Œª = 0, y = 0 and Œª = ‚àí1, y = 1. Now note
that, at the equilibrium points,
d Àôy
dy = ‚àí2Œª ‚àí2y = ‚àì
#
Œª2 + Œª.
Using the analysis of the previous section, we can see that the equilibrium point
with the larger value of y is stable, whilst the other is unstable. The bifurcation
diagram is shown in Figure 13.15, and certainly looks as if it contains two saddle‚Äì
node bifurcations.
For y ‚â™1 and Œª ‚â™1, Àôy = Œª ‚àí2Œªy ‚àíy2 ‚âàŒª ‚àíy2, which is precisely the normal
form for the saddle‚Äìnode bifurcation. All of the terms on the right hand side of

13.3 BIFURCATION THEORY
391
stable
x
unstable
¬µ
Fig. 13.14. The saddle‚Äìnode bifurcation diagram.
Fig. 13.15. The bifurcation diagram for Àôy = Œª ‚àí2Œªy ‚àíy2.
(13.23) are small, but the only one that is sure to be smaller than at least one of
the others is the second, since y ‚â™1 means that Œªy ‚â™Œª. We make no assumption
about how big Œª is compared with y2. To examine the neighbourhood of the other
bifurcation point, we shift the origin there using Œª = ‚àí1 + A¬µ, y = 1 + Bx, where

392
STABILITY, INSTABILITY AND BIFURCATIONS
A and B are constant scaling factors that we will Ô¨Åx later. In terms of ¬µ and x,
(13.23) becomes
Àôx = ‚àíA
B ¬µ ‚àí2A¬µx ‚àíBx2 ‚âà‚àíA
B ¬µ ‚àíBx2
for ¬µ ‚â™1 and x ‚â™1.
By choosing A = ‚àí1 and B = 1 this becomes the normal form for the saddle‚Äìnode
bifurcation. Note that the required change of coordinate, Œª = ‚àí1 ‚àí¬µ, indicates
that the sense of the bifurcation is reversed with respect to Œª, as can be seen in
Figure 13.15.
We can formalize the notion of a saddle‚Äìnode bifurcation using the following
theorem.
Theorem 13.5 (Saddle‚Äìnode bifurcation) Consider the Ô¨Årst order diÔ¨Äerential
equation
Àôx = f(x, ¬µ),
with f(0, 0) = fx(0, 0) = 0. Provided that f¬µ(0, 0) Ã∏= 0 and fxx(0, 0) Ã∏= 0, there
exists a continuous curve of equilibrium points in the neighbourhood of (0, 0), which
is tangent to the line ¬µ = 0 there. In addition,
(i) if f¬µ(0, 0)fxx(0, 0) < 0, then there are no equilibrium points in the neigh-
bourhood of (0, 0) for ¬µ < 0, whilst for ¬µ > 0, in a suÔ¨Éciently small neigh-
bourhood of (0, 0) there are two hyperbolic equilibrium points.
(ii) if f¬µ(0, 0)fxx(0, 0) > 0, then there are no equilibrium points in the neigh-
bourhood of (0, 0) for ¬µ > 0, whilst for ¬µ < 0, in a suÔ¨Éciently small neigh-
bourhood of (0, 0) there are two hyperbolic equilibrium points.
If fxx(0, 0) < 0, the equilibrium point with the larger value of x is stable, whilst the
other is unstable, and vice versa for fxx(0, 0) > 0.
Proof
We will give an informal proof. Since f(0, 0) = fx(0, 0) = 0, the Taylor
expansion of f(x, ¬µ) about (0, 0) shows that
Àôx ‚àºA0(¬µ) + A1(¬µ)x + A2(¬µ)x2 for |x| ‚â™1 and |¬µ| ‚â™1,
where
A0(¬µ) = f¬µ(0, 0)¬µ + 1
2f¬µ¬µ(0, 0)¬µ2,
A1(¬µ) = fx¬µ(0, 0)¬µ,
A2(¬µ) = 1
2fxx(0, 0).
There are therefore equilibrium points at
x = ‚àíA1 ¬±
#
A2
1 ‚àí4A0A2
2A2
‚àº¬±
+
‚àí2f¬µ(0, 0)¬µ
fxx(0, 0)
for |¬µ| ‚â™1.
This shows that the location of the equilibrium points is as described in the theorem.
Finally, at the equilibrium points,
d Àôx
dx ‚àºA1(¬µ) + 2A2(¬µ)x ‚àºfxx(0, 0)x,

13.3 BIFURCATION THEORY
393
since x = O(|¬µ|1/2). The stability of the equilibrium points is therefore determined
by the sign of fxx(0, 0).
Example: The CSTR
Many industrially important chemicals are produced in bulk using a continuous
Ô¨Çow, stirred tank reactor (CSTR). This is simply a large container, to which fresh
chemicals are continuously supplied and from which the resulting reactants are
continuously withdrawn, as sketched in Figure 13.16. A stirrer ensures that the
chemicals in the CSTR are well mixed. Let‚Äôs consider what happens when chemicals
A and B that react through the cubic autocatalytic reaction step (13.7) are fed into
a CSTR, and assume that the idea is to convert as much of the reactant A into the
autocatalyst B as possible.
Reactants
in
Products and
unused reactants
out
CSTR
MIXER
Fig. 13.16. A continuous Ô¨Çow, stirred tank reactor (CSTR).
Since the CSTR is well stirred, we assume that the concentrations of the chem-
icals are spatially uniform, and given by a(t) and b(t). The rate of change of the
total amount of species A in the CSTR is equal to the rate at which it is produced
by chemical reaction plus the rate at which it Ô¨Çows in minus the rate at which it
Ô¨Çows out. If the CSTR has constant volume V , constant inlet and (by conservation
of mass) outlet Ô¨Çowrate q and inlet concentration of A given by a0, we have
d
dt (V a) = ‚àíV kab2 + q (a0 ‚àía) ,
and hence
da
dt = ‚àíkab2 + a0 ‚àía
tres
.
(13.24)
The residence time, tres = V/q, is the time it takes for a volume V of fresh
reactants to Ô¨Çow into the CSTR, and characterizes the period for which a Ô¨Çuid
element typically remains within the CSTR. Similarly,
db
dt = kab2 + b0 ‚àíb
tres
,
(13.25)
where b0 is the inlet concentration of the autocatalyst, B.
We now deÔ¨Åne dimensionless variables
Œ± = a
a0
,
Œ≤ = b
a0
,
œÑ = ka2
0t,

394
STABILITY, INSTABILITY AND BIFURCATIONS
in terms of which (13.24) and (13.25) become
dŒ±
dœÑ = ‚àíŒ±Œ≤2 + 1 ‚àíŒ±
œÑres
,
(13.26)
dŒ≤
dœÑ = Œ±Œ≤2 + Œ≤0 ‚àíŒ≤
œÑres
,
(13.27)
where
œÑres = ka2
0tres,
Œ≤0 = b0
a0
.
We can now add (13.26) and (13.27) to obtain
d
dœÑ (Œ± + Œ≤) = 1 + Œ≤0 ‚àí(Œ± + Œ≤)
œÑres
,
a linear equation that we can solve for Œ± + Œ≤ using an integrating factor.
The
solution is
Œ± + Œ≤ = 1 + Œ≤0 + ke‚àíœÑ/œÑres,
where k is a constant. Clearly, Œ± + Œ≤ ‚Üí1 + Œ≤0 as œÑ ‚Üí‚àû. In particular, for
œÑ ‚â´œÑres, and hence for t ‚â´tres, Œ± + Œ≤ ‚àº1 + Œ≤0. The term ke‚àíœÑ/œÑres represents
an initial transient, which decays to zero exponentially fast over a time scale given
by the residence time. We therefore assume that Œ± + Œ≤ = 1 + Œ≤0, and can thereby
eliminate Œ≤ from (13.26). In the analysis that follows, it is more convenient to work
in terms of z = 1 ‚àíŒ±, the extent to which the reactant A has been converted to B.
This gives us a nonlinear, Ô¨Årst order ordinary diÔ¨Äerential equation for z,
dz
dœÑ = R(z) ‚àíF(z),
(13.28)
where
R(z) = (1 ‚àíz) (z + Œ≤0)2 ,
F(z) =
z
œÑres
.
We can now see that a steady state, where dz/dœÑ = 0, occurs when the rate of
reaction, R(z), is balanced by the rate at which A Ô¨Çows into the CSTR, F(z), and
hence when F(z) = R(z). The function R(z) is a cubic polynomial, whilst F(z)
is a straight line through the origin. The steady states are therefore given by the
points of intersection of these two curves.
Case 1: No autocatalyst in the inÔ¨Çow, Œ≤0 = 0
When Œ≤0 = 0, R(z) has a repeated root at z = 0, as shown in Figure 13.17.
The straight line F(z) always passes through z = 0, which is therefore always a
steady state. The state z = 0 represents a CSTR that contains no autocatalyst,
just the reactant A supplied by the inlet Ô¨Çow. A simple calculation of the steady
state solutions shows that for œÑres < 4, z = 0 is the only steady state, whilst for

13.3 BIFURCATION THEORY
395
œÑres > 4, there are two further points of intersection between F(z) and R(z), and
hence two other steady states, at z = z1 and z = z2, where
z1 = 1
2

1 ‚àí
*
1 ‚àí
4
œÑres

,
z2 = 1
2

1 +
*
1 ‚àí
4
œÑres

.
Note that 0 < z1 < 1
2 and 1
2 < z2 < 1, and z1 ‚Üí0, z2 ‚Üí1 as œÑres ‚Üí‚àû. The
steady states are sketched in Figure 13.18. The stability of the steady states is
easily calculated, and we Ô¨Ånd that z = 0 and z = z2 are stable states, whilst z = z1
is unstable. There is a saddle‚Äìnode bifurcation at œÑres = 4. This situation, with
Œ≤0 = 0, is a realistic one, since it is probably desirable to run the system with just
a single species entering the CSTR. We clearly want to run the CSTR in the state
z = z2, where more than half of the reactant A is converted to B. However, we also
want to make the residence time as small as possible, to increase the rate at which
B is produced. However, we can now see that, if œÑres is slowly decreased, z2 will also
slowly decrease, and that when œÑres decreases past 4, the saddle‚Äìnode bifurcation
point, the situation changes dramatically. The only available steady state is z = 0,
so that no autocatalyst remains in the CSTR, and the reaction stops.
This is
known as washout. Attempts to recover the desirable state z = z2 by increasing
the residence time, œÑres, are doomed to failure when there is no autocatalyst entering
the CSTR.
Fig. 13.17. The curves R(z) and F(z) when Œ≤0 = 0.

396
STABILITY, INSTABILITY AND BIFURCATIONS
Fig. 13.18. The bifurcation diagram when Œ≤0 = 0.
Case 2: Autocatalyst in the inÔ¨Çow, Œ≤0 > 0
Let‚Äôs now see how the situation is aÔ¨Äected by including some autocatalyst in
the inÔ¨Çow. When Œ≤0 > 0, the cubic polynomial R(z) has a single positive root at
z = 1, and is strictly positive for 0 ‚©Ωz < 1, as shown in Figure 13.19. In order to
determine when F(z) is tangent to R(z), we must simultaneously solve R(z) = F(z)
and R‚Ä≤(z) = F ‚Ä≤(z), which gives
(z + Œ≤0)2 (1 ‚àíz) =
z
œÑres
,
(13.29)
(z + Œ≤0) (‚àí3z + 2 ‚àíŒ≤0) =
1
œÑres
.
(13.30)
Eliminating œÑres between these two equations gives 2z2 ‚àíz + Œ≤0 = 0, and hence the
points of tangency are at
z = z¬± = 1
4

1 ¬±
#
1 ‚àí8Œ≤0

.
We conclude that there are no such points of tangency for Œ≤0 >
1
8, and hence
that there is a unique stable solution, as shown in Figures 13.19 and 13.20. For
0 < Œ≤0 < 1
8, there is a unique solution for 0 ‚©ΩœÑres ‚©ΩœÑ+ and œÑres > œÑ‚àí, and three
solutions for œÑ+ < œÑres < œÑ‚àí, where, from (13.30),
œÑ¬± =
1
(z¬± + Œ≤0) (2 ‚àíŒ≤0 ‚àí3z¬±).
There are now two saddle‚Äìnode bifurcations, at œÑres = œÑ¬±.

13.3 BIFURCATION THEORY
397
Fig. 13.19. The curves R(z) and F(z) when Œ≤0 > 0.
We conclude that for Œ≤0 > 1
8, there is no possibility of washout, just a unique
steady state solution for any given residence time, œÑres. For 0 < Œ≤ < 1
8, we again
have the possibility, if not of a washout of the autocatalyst, at least of a dramatic
decrease in the concentration of B when œÑres falls below œÑ+. However, if œÑres is
now increased again, there will be a dramatic increase in the concentration of B as
œÑres increases past œÑ‚àí. This change in the steady state from when a parameter is
increased to when it is decreased is known as hysteresis.
Finally, note that when Œ≤ = 1
8 the two saddle‚Äìnode points merge and disappear
at œÑres = 64
27. This is itself a bifurcation, and is known as a codimension two
bifurcation. Such a bifurcation can only occur in a system that has at least two
parameters (here Œ≤0 and œÑres).
We will study two other types of bifurcation. Consider the normal form
Àôx = ¬µx ‚àíx2.
(13.31)
This system has equilibrium points at x = 0 and x = ¬µ. When ¬µ = 0 we again have
the nonhyperbolic equilibrium point given by Àôx = ‚àíx2, whilst when ¬µ Ã∏= 0 there
are always two equilibrium points. In this case,
d Àôx
dx = ¬µ ‚àí2x =
 ‚àí¬µ
at x = ¬µ,
¬µ
at x = 0,

398
STABILITY, INSTABILITY AND BIFURCATIONS
Fig. 13.20. The bifurcation diagram when Œ≤0 > 0.
and hence
x =
¬µ is

stable for ¬µ > 0,
unstable for ¬µ < 0,
x = ‚àí¬µ is
 unstable for ¬µ > 0,
stable for ¬µ < 0.
The bifurcation diagram is shown in Figure 13.21. This is called a transcritical
bifurcation. At the bifurcation point, the two equilibrium solutions pass through
each other and exchange stabilities, so this sort of bifurcation is often referred to
as an exchange of stabilities.
Theorem 13.6 (Transcritical bifurcation) Consider the Ô¨Årst order diÔ¨Äerential
equation
Àôx = f(x, ¬µ),
with f(0, 0) = fx(0, 0) = f¬µ(0, 0) = 0. Provided that fxx(0, 0) Ã∏= 0 and f 2
¬µx(0, 0) ‚àí
fxx(0, 0)f¬µ¬µ(0, 0) > 0, there exist two continuous curves of equilibrium points in
the neighbourhood of (0, 0). These curves intersect transversely at (0, 0). For each
¬µ Ã∏= 0 there are two hyperbolic equilibrium points in the neighbourhood of x = 0. If
fxx(0, 0) < 0, the equilibrium point with the larger value of x is stable, whilst the
other is unstable, and vice versa for fxx(0, 0) > 0.
We will leave the informal proof as Exercise 13.9.

13.3 BIFURCATION THEORY
399
stable
x
unstable
¬µ
Fig. 13.21. The transcritical bifurcation.
Finally, consider the normal form
Àôx = ¬µx ‚àíx3.
(13.32)
This has a single equilibrium point at x = 0 for ¬µ < 0, and three equilibrium
points at x = 0 and x = ¬±‚àö¬µ for ¬µ > 0.
The bifurcation diagram is shown
in Figure 13.22(a). The bifurcation at x = 0, ¬µ = 0 is called a supercritical
pitchfork bifurcation. A similar bifurcation, in which the two new equilibrium
points created at the bifurcation point are unstable, is the subcritical pitchfork
bifurcation, with normal form Àôx = ¬µx + x3, whose bifurcation diagram is shown
in Figure 13.22(b).
Theorem 13.7 (Pitchfork bifurcation) Consider the Ô¨Årst order diÔ¨Äerential
equation
Àôx = f(x, ¬µ),
with f(0, 0) = fx(0, 0) = f¬µ(0, 0) = fxx(0, 0) = 0. Provided that f¬µx(0, 0) Ã∏= 0
and fxxx(0, 0) Ã∏= 0, there exist two continuous curves of equilibrium points in the
neighbourhood of (0, 0). One curve passes through (0, 0) transverse to the line ¬µ = 0,
whilst the other is tangent to ¬µ = 0 at x = 0. In addition,
(i) if f¬µx(0, 0)fxxx(0, 0) < 0, then, close to (0, 0), there is a single equilibrium
point for ¬µ < 0 and three equilibrium points for ¬µ > 0.
(ii) if f¬µx(0, 0)fxxx(0, 0) > 0, then, close to (0, 0), there is a single equilibrium
point for ¬µ > 0 and three equilibrium points for ¬µ < 0.
If fxxx(0, 0) < 0, the single equilibrium point and the outer two of the three equilib-

400
STABILITY, INSTABILITY AND BIFURCATIONS
rium points are stable, whilst the middle of the three equilibrium points is unstable,
and vice versa for fxxx(0, 0) > 0.
We leave the informal proof as Exercise 13.9.
(a) supercritical
x
¬µ
(b) subcritical
x
¬µ
stable
unstable
Fig. 13.22. The (a) supercritical and (b) subcritical pitchfork bifurcation.
Although these bifurcations seem rather abstract, they often describe the quali-
tative behaviour of complicated physical systems, as we saw for the CSTR. Another
simple example is the buckling beam. Consider a straight beam of elastic material
under compression from two equal and opposite forces along its axis, one applied
at each end, as shown in Figure 13.23(a). Now consider the displacement, x, of the
midpoint of the beam. For suÔ¨Éciently small applied forces, the beam undergoes a
simple axial compression and x = 0. As the applied forces are slowly increased, at a
critical value the beam buckles, as shown in Figure 13.23(b), because the unbuckled
state becomes unstable. If the beam has perfect left‚Äìright symmetry, it is equally
likely to buckle left or right. When the displacement of the midpoint of the beam
is plotted as a function of magnitude of the applied forces, we obtain the bifurca-

13.3 BIFURCATION THEORY
401
tion diagram corresponding to a supercritical pitchfork, as in Figure 13.22(a). Of
course, in practice, a beam, for example a ruler held between your foreÔ¨Ångers, will
not have perfect symmetry, and will buckle in a preferred direction. This suggests
that the pitchfork bifurcation is itself structurally unstable. You can investigate
this by trying Exercise 13.7.
F0
(a)
F0
F1 > F0
(b)
F1 > F0
Fig. 13.23. An elastic beam under compression (a) before and (b) after buckling.
13.3.2
Second Order Ordinary DiÔ¨Äerential Equations
As for Ô¨Årst order equations, second order systems that possess an equilibrium
point with an eigenvalue with zero real part are structurally unstable. A small
change in the governing equations can change the qualitative nature of the phase
portrait. For example, the simple, frictionless pendulum, with phase portrait shown
in Figure 9.2(b), contains a nonlinear centre. Since a centre has two eigenvalues
with zero real part, we conclude that this system is structurally unstable. In terms
of the physics, consider what happens if we allow for a tiny amount of friction.
This will gradually reduce the energy and the amplitude of the motion, with the
pendulum being brought to rest as t ‚Üí‚àû. This is reÔ¨Çected in what happens to the
phase portrait when a term ¬µdŒ∏/dt, with ¬µ > 0, is added to the left hand side of
(9.2) to model the eÔ¨Äect of friction. No matter how small ¬µ is, the nonlinear centre
is transformed into a stable focus, and all integral paths asymptote to Œ∏ = ÀôŒ∏ = 0 as
t ‚Üí‚àû(see Exercise 9.6).
Now consider the second order system
Àôx = ¬µ ‚àíx2,
Àôy = ‚àíy.
(13.33)

402
STABILITY, INSTABILITY AND BIFURCATIONS
For ¬µ < 0 there are no equilibrium points, whilst for ¬µ > 0 there are two equilibrium
points at P+ = (+‚àö¬µ, 0) and P‚àí= (‚àí‚àö¬µ, 0). The point P+ is a stable node, whilst
P‚àíis a saddle point. When ¬µ = 0 there is a single, nonhyperbolic equilibrium point
at (0, 0), a saddle‚Äìnode. The various phase portraits are shown in Figure 13.24. The
point ¬µ = 0, x = y = 0 is called a saddle‚Äìnode bifurcation point, and (13.33)
is its normal form. At such a point, a saddle and a node collide and disappear.
Note that, since Àôy = ‚àíy, all integral paths asymptote to the x-axis as t ‚Üí‚àû,
where the dynamics are controlled by Àôx = ¬µ ‚àíx2. This is just the normal form
of the saddle‚Äìnode bifurcation in a Ô¨Årst order equation. In eÔ¨Äect, we can ignore
the dynamics in the y-direction, and concentrate on the dynamics on the x-axis,
which is the centre manifold for the bifurcation. This shows why it was important
to study bifurcations in Ô¨Årst order systems, since the important dynamics of higher
order systems occur on a lower order centre manifold. For more on bifurcation
theory, see Arrowsmith and Place (1990).
Two other simple types of bifurcation are (see Exercise 13.8) the transcritical
bifurcation, with normal form
Àôx = ¬µx ‚àíx2,
Àôy = ‚àíy,
(13.34)
and the supercritical and subcritical pitchfork bifurcations, with normal
forms
Àôx = ¬µx ¬± x3,
Àôy = ‚àíy.
(13.35)
The behaviour on the x-axis is, in each case, analogous to the corresponding bifur-
cation in a one-dimensional system, which we studied in Section 13.3.1. In each of
these examples, one eigenvalue passes through zero at the bifurcation point. We
will not consider the more degenerate case where both eigenvalues pass through
zero (see Guckenheimer and Holmes, 1983). However, we will consider one other
type of bifurcation, which occurs when a complex conjugate pair of eigenvalues
passes through the imaginary axis away from the origin. This is called the Hopf
bifurcation, and has no counterpart in Ô¨Årst order systems.
The normal form of the Hopf bifurcation is most easily written in terms of polar
coordinates as
Àôr = ¬µr + ar3,
ÀôŒ∏ = œâ.
(13.36)
Let‚Äôs assume that a < 0. For ¬µ ‚©Ω0 there is a stable focus at the origin, and no other
equilibrium points, and all trajectories spiral into the origin from inÔ¨Ånity. For ¬µ > 0
there is an unstable focus at the origin, and no other equilibrium points, but there
is also a stable limit cycle, given by r =
#
‚àí¬µ/a, ÀôŒ∏ = œâ. Trajectories spiral onto the
limit cycle both from the origin and from inÔ¨Ånity, as shown in Figure 13.25. When
¬µ = 0 a linear analysis shows that the origin is a centre, and hence has eigenvalues
with zero real part. However, this is an example where the nonlinear terms, here
Àôr = ar3, cause the integral paths to spiral into the equilibrium point. Note that
this is a supercritical Hopf bifurcation. The subcritical Hopf bifurcation,
for which the limit cycle is unstable and exists for ¬µ < 0, occurs when a > 0. In
general, for equations that model real physical systems, any limit cycle solutions

13.3 BIFURCATION THEORY
403
(a) ¬µ > 0
y
x
P+
P‚àí
(b) ¬µ = 0
y
x
y
x
(c) ¬µ < 0
Fig. 13.24. A saddle‚Äìnode bifurcation in a second order system, for (a) ¬µ > 0, (b) ¬µ = 0,
(c) ¬µ < 0.
that exist are formed at Hopf bifurcations, and it is therefore crucial to determine
the position of these bifurcations. In order to demonstrate that a Hopf bifurcation
occurs when a pair of eigenvalues crosses the imaginary axis, we can appeal to the
Hopf bifurcation theorem.
Theorem 13.8 (Hopf) Consider the second order system
Àôx = X(x, y; ¬µ),
Àôy = Y (x, y; ¬µ).
Suppose that

404
STABILITY, INSTABILITY AND BIFURCATIONS
x
x
y
(b) ¬µ > 0
y
<
Fig. 13.25. A Hopf bifurcation for (a) a < 0, ¬µ ‚©Ω0 and (b) a < 0, ¬µ > 0.
(i) X(0, 0; ¬µ) = Y (0, 0; ¬µ) = 0 for each ¬µ ‚àà[‚àí¬µ0, ¬µ0] for some ¬µ0 > 0,
(ii) the Jacobian matrix evaluated at the origin with ¬µ = 0 is
 0
‚àíœâ
œâ
0

for some œâ Ã∏= 0,
(iii) the eigenvalues of the equilibrium point at the origin are complex conjugate
for ¬µ ‚àà[‚àí¬µ0, ¬µ0], and given by Œ±(¬µ) ¬± iŒ≤(¬µ), with Œ± and Œ≤ real,
(iv) a Ã∏= 0, where
a = 1
16 (Xxxx + Yxxy + Xxyy + Yyyy)
+ 1
16œâ {Xxy (Xxx + Xyy) ‚àíYxy (Yxx + Yyy) ‚àíXxxYxx + XyyYyy} , (13.37)
with all of these partial derivatives evaluated at the origin with ¬µ = 0.
Then
(i) If aŒ±‚Ä≤(0) < 0, a unique limit cycle solution bifurcates from the origin in
¬µ > 0 as ¬µ passes through zero. For ¬µ < 0 there exists a neighbourhood of
the origin that does not contain a limit cycle solution. The stability of the
limit cycle is the same as that of the origin in ¬µ < 0.
(ii) If aŒ±‚Ä≤(0) > 0, a unique limit cycle solution bifurcates from the origin in
¬µ < 0 as ¬µ passes through zero. For ¬µ > 0 there exists a neighbourhood of
the origin that does not contain a limit cycle solution. The stability of the
limit cycle is the same as that of the origin in ¬µ > 0.
The amplitude of the limit cycle grows like |¬µ|1/2 and its period tends to 2œÄ/|œâ| as
¬µ ‚Üí0.
We will not prove this theorem here. The idea of the proof is to go through a series
of algebraic transformations of the diÔ¨Äerential equations that reduce them to the

13.3 BIFURCATION THEORY
405
normal form, (13.36), and is algebraically very unpleasant. For as straightforward
an explanation of the details as you could hope for, see Glendinning (1994). Instead,
we will concentrate on a concrete example to show how the proof of the theorem
works.
Example
Consider the second order system
Àôx = X(z, y; ¬µ) = ¬µx ‚àíœây + x3,
Àôy = Y (x, y; ¬µ) = œâx + ¬µy + y2,
(13.38)
where ¬µ and œâ are constants. The linear part of the system is in the normal form for
an equilibrium point at the origin with complex conjugate eigenvalues Œª = ¬µ + iœâ,
which we discussed in Section 9.3.2. This means that the origin is a stable focus
for ¬µ < 0, a linear centre for ¬µ = 0 and an unstable focus for ¬µ > 0. The system
therefore satisÔ¨Åes conditions (i), (ii) and (iii) of Theorem 13.8. If we now substitute
this particular choice of X and Y into (13.37), we Ô¨Ånd that a = 3/8, and hence
that aŒ±‚Ä≤(0) = 3/8 > 0. The Hopf bifurcation at ¬µ = 0 is therefore subcritical, with
an unstable limit cycle emerging from the origin in ¬µ < 0.
Let‚Äôs now try to write (13.38) in the normal form (13.36) for suÔ¨Éciently small
x, y and ¬µ, using the same transformations as those used in the proof of the Hopf
bifurcation theorem. We begin by deÔ¨Åning z = x + iy, in terms of which (13.38)
can be written concisely as
Àôz = Œªz ‚àí1
4i (z ‚àíz‚àó)2 + 1
8 (z + z‚àó)3 .
(13.39)
The next step is to make a quadratic near identity transformation,
z = w + a1w2 + a2ww‚àó+ a3w‚àó2.
(13.40)
The idea is that such a transformation leaves the linear part of the equation un-
changed when written in terms of w, but can be used to simplify the nonlinear
part by an appropriate choice of the constants a1, a2 and a3. SpeciÔ¨Åcally, we can
eliminate quadratic terms from (13.39). For |z| ‚â™1,
w = z ‚àía1w2 ‚àía2ww‚àó‚àía3w‚àó2 = z ‚àía1

z ‚àía1w2 ‚àía2ww‚àó‚àía3w‚àó22
‚àía2

z ‚àía1w2 ‚àía2ww‚àó‚àía3w‚àó2 
z‚àó‚àía‚àó
1w‚àó2 ‚àía‚àó
2ww‚àó‚àía‚àó
3w2
‚àía3

z‚àó‚àía‚àó
1w‚àó2 ‚àía‚àó
2ww‚àó‚àía‚àó
3w22 = z ‚àía1z2 ‚àía2zz‚àó‚àía3z‚àó2 +

2a2
1 + a2a‚àó
3

z3
+

3a1a2 + a2a‚àó
2 + 2a2
3

z2z‚àó
+

2a1a3 + a‚àó
1a2 + a2
2 + 2a‚àó
2a3

zz‚àó2 + (a2a3 + 2a‚àó
1a3) z‚àó3 + O(|z|4).
If we now take this expression, diÔ¨Äerentiate, and replace Àôz and Àôz‚àóusing (13.39), we
arrive, after considerable eÔ¨Äort, at
Àôw = Œªz ‚àí1
4i (z ‚àíz‚àó)2 + 1
8 (z ‚àíz‚àó)3

406
STABILITY, INSTABILITY AND BIFURCATIONS
‚àí(2a1z + a2z‚àó)

Œªz ‚àí1
4i (z ‚àíz‚àó)2
	
‚àí(a2z + 2a3z‚àó)

Œª‚àóz‚àó+ 1
4i (z ‚àíz‚àó)2
	
+3

2a2
1 + a2a‚àó
3

Œªz3 +

3a1a2 + a2a‚àó
2 + 2a2
3

(2Œª + Œª‚àó) z2z‚àó
+

2a1a3 + a‚àó
1a2 + a2
2 + 2a‚àó
2a3

(Œª + 2Œª‚àó) zz‚àó2 + 3 (a2a3 + 2a‚àó
1a3) Œª‚àóz‚àó3 + O(|z|4).
We can now use the deÔ¨Ånition, (13.40), of w to eliminate z from the right hand
side, retaining only cubic terms and larger, and arrive at
Àôw = Œª

w + a1w2 + a2ww‚àó+ a3w‚àó2
‚àí
1
4i + 2a1Œª
 
w2 + 2a1w3 + 2a2w2w‚àó+ 2a3ww‚àó2
+
1
2i ‚àía2 (Œª + Œª‚àó)
	 
ww‚àó+ a‚àó
3w3 + (a1 + a‚àó
2) w2w‚àó+ (a‚àó
1 + a2) ww‚àó2 + a3w‚àó3
‚àí
1
4i + 2a3Œª‚àó
 
w‚àó2 + 2a‚àó
1w‚àó3 + 2a‚àó
2w‚àó2w + 2a‚àó
3w‚àów2
+1
4i (2a1w + a2w‚àó) (w ‚àíw‚àó)2 ‚àí1
4i (a2w + 2a3w‚àó) (w ‚àíw‚àó)2 + 1
8 (w + w‚àó)3
+3

2a2
1 + a2a‚àó
3

Œªw3 +

3a1a2 + a2a‚àó
2 + 2a2
3

(2Œª + Œª‚àó) w2w‚àó
+

2a1a3 + a‚àó
1a2 + a2
2 + 2a‚àó
2a3

(Œª + 2Œª‚àó) ww‚àó2 + 3 (a2a3 + 2a‚àó
1a3) Œª‚àów‚àó3 + O(|w|4).
We can now see that we can eliminate all of the quadratic terms by choosing
a1 = ‚àíi
4Œª,
a2 = i
2Œª,
a3 =
i
4 (Œª ‚àí2Œª‚àó),
which leaves us with
Àôw = Œªw + A1w3 + A2w2w‚àó+ A3ww‚àó2 + A4w‚àó3,
where
A1 = ‚àí2a1
1
4i + 2a1Œª

+ a‚àó
3
1
2i ‚àía2 (Œª + Œª‚àó)
	
+ 1
8
+1
4i (2a1 ‚àía2) + 3 (2a1 + a2a‚àó
3) Œª,
A2 = ‚àí2a2
1
4i + 2a1Œª

+ (a1 + a‚àó
2)
1
2i ‚àía2 (Œª + Œª‚àó)
	
+ 3
8
‚àí2a‚àó
3
1
4i + 2a3Œª‚àó

+ 1
4i (3a2 ‚àí2a3 ‚àí4a1) +

3a1a2 + a2a‚àó
2 + 2a2
3

(2Œª + Œª‚àó) ,
A3 = ‚àí2a3
1
4i + 2a1Œª

+ (a‚àó
1 + a2)
1
2i ‚àía2 (Œª + Œª‚àó)
	
+ 3
8

13.3 BIFURCATION THEORY
407
‚àí2a‚àó
2
1
4i + 2a3Œª‚àó

+ 1
4i (4a3 ‚àí3a2 + 2a1) +

2a1a3 + a‚àó
1a2 + a2
2 + 2a‚àó
2a3

(Œª + 2Œª‚àó) ,
A4 = ‚àí2a‚àó
1
1
4i + 2a3Œª‚àó

+ a3
1
2i ‚àía2 (Œª + Œª‚àó)
	
+ 1
8
+1
4i (a2 ‚àí2a3) + 3 (a2a3 + a‚àó
1a3) Œª‚àó.
The next step is to make another near identity transformation,
w = v + b1v3 + b2v2v‚àó+ b3vv‚àó2 + b4v‚àó3,
(13.41)
and try to eliminate the cubic terms as well. Proceeding exactly as we did for
the quadratic terms, although the algebra is now easier since we only retain cubic
terms, we Ô¨Ånd that
Àôv = Œªv + (A1 ‚àí2Œªb1) v3 + {A2 ‚àí(Œª + Œª‚àó) b2} v2v‚àó+ (A3 ‚àí2Œª‚àób3) vv‚àó2
+ {A4 + (Œª ‚àí3Œª‚àó) b4} v‚àó3 + O(|v|4).
At Ô¨Årst sight, it would appear that we can eliminate all of the cubic terms in
the same way as we did the quadratic terms.
However, the coeÔ¨Écient of b2 is
Œª + Œª‚àó= 2¬µ, which is small when ¬µ ‚â™1. Since we need b2 = O(1) for |v| ‚â™1 and
¬µ ‚â™1, we conclude that we cannot eliminate the v2v‚àóterm, and hence that the
simplest normal form is
Àôv ‚àºŒªv + av|v|2,
using v2v‚àó= v|v|2, with a = A2(0). A little algebra then shows that a = A2(0) =
3/8, consistent with our earlier calculation of a from (13.37). Indeed, the hard part
of the proof the Hopf bifurcation is to show that a, as deÔ¨Åned in (13.37), appears
in the normal form in this way. If we now write v = reiŒ∏ and separate real and
imaginary parts, we recover the normal form (13.36), and we are done.
We should reiterate that this transformation and the ensuing algebra is not what
needs to be done whenever the system you are studying contains a Hopf bifurcation
point. The Hopf bifurcation theorem is far easier to use. We went through the
details of this example purely to illustrate the steps involved in the proof.
Figure 13.26 shows the unstable limit cycle solution for various ¬µ < 0.
We
calculated these solutions numerically using MATLAB (see Section 9.3.4).
For
|¬µ| suÔ¨Éciently small, we can see that the limit cycle is circular. However, as |¬µ|
increases, the limit cycle moves away from the neighbourhood of the origin and
becomes increasingly distorted as it begins to interact with a saddle point that lies
close to (‚àí1, ‚àí1). In fact, when ¬µ = ¬µ0 ‚âà‚àí0.12176, the limit cycle collides with
this saddle point, and is destroyed. This is an example of a homoclinic bifurcation,
which we discuss below.

408
STABILITY, INSTABILITY AND BIFURCATIONS
Fig. 13.26. The unstable limit cycle solution of (13.36) for ¬µ = ‚àí0.002, ‚àí0.005, ‚àí0.01,
‚àí0.05, ‚àí0.1, ‚àí0.12 and ‚àí0.12176.
13.3.3
Global Bifurcations
With the exception of the previous example, all of the bifurcations that we have
studied so far have been local bifurcations. That is, they have arisen because of a
change in the nature of an equilibrium point as a parameter passes through a critical
value. A global bifurcation is one that occurs because the qualitative nature of
the solutions changes due to the interaction of two or more distinct features of
the phase portrait. For second order systems, the crucial features are usually limit
cycles and the separatrices of any saddle points. Figure 13.27 illustrates an example
of a homoclinic bifurcation, which can occur when a limit cycle interacts with
a saddle point. A limit cycle is formed in a Hopf bifurcation when the bifurcation
parameter, ¬µ, is equal to ¬µ1.
As ¬µ increases, the amplitude of the limit cycle
grows until, when ¬µ = ¬µ2 > ¬µ1, the limit cycle collides with the saddle point,

13.3 BIFURCATION THEORY
409
forming a connection between the stable and unstable separatrices. This is the
homoclinic bifurcation point. With the separatrices joined up like this, the system
is structurally unstable, since an arbitrarily small change to the equations can
destroy the connection. For ¬µ > ¬µ2, there is no limit cycle.
Of course, it is one thing to describe a global bifurcation in qualitative terms,
but quite another to be able to quantify when the bifurcation occurs, since a local
analysis is of no help. We must usually resort to numerical methods, as we did for
the example shown in Figure 13.26. We will therefore conÔ¨Åne ourselves to a single
example of a global bifurcation where analytical progress is possible.
(a)
(b)
(c)
¬µ1 < ¬µ < ¬µ2
¬µ > ¬µ2
¬µ = ¬µ2
Fig. 13.27. A homoclinic bifurcation.
Example: Travelling waves in cubic autocatalysis (continued)
In Section 13.1.1 we performed a local analysis of the two equilibrium points of
(13.14). In order to determine for what values of V a physically meaningful solution
exists, we need to determine for what values of V the unstable separatrix of P1,
labelled S1 in Figure 13.4, enters the origin as z ‚Üí‚àû. This is a global problem
involving the relative positions of S1 and the stable manifold of P2, labelled S2 in
Figure 13.4.
We begin by deÔ¨Åning the region
R =

(Œ≤, Œ≥) | 0 < Œ≤ < 2
3, ‚àí4
27V < Œ≥ < 0
	
= 
(Œ≤, Œ≥) | 2
3 ‚©ΩŒ≤ < 1, Œ≥H(Œ≤) < Œ≥ < 0
	
,
where Œ≥H(Œ≤) = ‚àíŒ≤2(1 ‚àíŒ≤)/V is the horizontal isocline. The region R is shown
in Figure 13.28. Note that the point Œ≤ = 2
3, Œ≥ = ‚àí4
27V is the local minimum of
Œ≥H(Œ≤). This region is constructed so that there are three qualitatively diÔ¨Äerent
possibilities.
‚Äî Case (a): S2 enters R through the Œ≤-axis,

410
STABILITY, INSTABILITY AND BIFURCATIONS
‚Äî Case (b): S2 = S1, and hence asymptotes to P1 as z ‚Üí‚àí‚àû,
‚Äî Case (c): S2 enters R through its lower boundary.
Note that S2 cannot enter R through the Œ≥-axis, since dŒ≤/dz = Œ≥ < 0 there.
Cases (a), (b) and (c) are illustrated in Figure 13.28. In case (a), S1 lies below
S2 and therefore does not enter the origin and is swept into the region Œ≤ < 0. It
cannot represent a physically meaningful solution in this case. In case (b), S2 = S1
represents a physically meaningful solution, and this solution enters the origin on
the stable manifold. In case (c), S1 lies above S2 and is therefore attracted into the
origin on the centre manifold. These arguments can be made more rigorous, but
we will not do this here (see Billingham and Needham, 1991, for more details).
We now need to determine which of these three cases arises for each value of V .
We can do this by deÔ¨Åning a function f(V ), as illustrated in Figure 13.29.
‚Äî Case (a): f(V ) is equal to the value of Œ≤ where S2 crosses the Œ≤-axis leaving the
region R,
‚Äî Case (b): f(V ) = 1,
‚Äî Case (c): f(V ) = 1 ‚àíŒ≥0, where Œ≥0 is the value of Œ≥ where S2 crosses the line
Œ≤ = 1 (which it does, since dŒ≤/dz = Œ≥ < 0).
DeÔ¨Åned is this way, f(V ) is continuous, and there is no physically meaningful
solution of (13.14) when f(V ) < 1 (case(a)), and a unique physically meaningful
solution when f(V ) ‚©æ1 (cases (b) and (c)).
Lemma 13.1 f(V ) is strictly monotone increasing for V > 0.
Proof When V = V0 > 0, we deÔ¨Åne the region
D(V0) =

(Œ≤, Œ≥) | 0 ‚©ΩŒ≥ ‚©ΩŒ≥S(Œ≤)|V =V0 , 0 ‚©ΩŒ≤ ‚©Ω1

,
where Œ≥ = Œ≥S(Œ≤) is the equation of S2 within the region R, as illustrated in Fig-
ure 13.29. From (13.17),
‚àÇ
‚àÇV
dŒ≥
dŒ≤

= ‚àí1 < 0.
This means that the slope of the integral path through any Ô¨Åxed point is strictly
monotone decreasing as V increases. In particular, when V = V1 > V0, all integral
paths that meet the curved boundary of D(V0) (a boundary given by S2 when
V = V0) enter D(V0). In addition, from (13.14), all integral paths that meet the
straight parts of the boundary of D(V0) also enter D(V0).
Finally, since S2 is
directed along the vector e‚àí= (1, ‚àíV ) as it enters the origin, it lies outside D(V0)
in a suÔ¨Éciently small neighbourhood of the origin when V = V1. We conclude that
when V = V1 > V0, S2 cannot pass through the boundary of D(V0), and therefore
that f(V1) > f(V0).
We will leave the next stage of our argument as Exercise 13.11, in which you
are asked, helped by some hints, to show that f(V ) = O(V 2) for V ‚â™1, and that
f(V ) ‚àºV for V ‚â´1. Now, since f(V ) < 1 for V suÔ¨Éciently small, f(V ) > 1 for V

13.3 BIFURCATION THEORY
411
S2
S1
S2
S1
S2 = S1
P1
P2
P2
P1
P2
P1
Œ≤
(a)
(b)
(c)
Œ≥
Œ≥
Œ≥
Œ≤
Œ≤
Fig. 13.28. The region R and the three possible types of global behaviour of S1 and S2.
suÔ¨Éciently large, f(V ) is strictly monotone increasing by Lemma 13.1, and f(V )
is continuous, we conclude from the intermediate value theorem that there exists a
unique value V = V ‚àó, such that f(V ) ‚©æ1 for V ‚©æV ‚àóand f(V ) < 1 for V < V ‚àó.
When V = V ‚àóthere is therefore a global bifurcation, since we have now shown that

412
STABILITY, INSTABILITY AND BIFURCATIONS
D
(c)
(b)
(a)
Œ≤
Œ≤
Œ≤
D
D
S2
S2 = S1
S1
Œ≤ = 1
Œ≤ = 1
Œ≤ = 1
f(V) < 1
f(V) = 1
f(V) > 1
Œ≥
Œ≥
Œ≥
Fig. 13.29. The deÔ¨Ånition of f(V ) and the region D(V ) in each of the three cases.
case (a) occurs when V < V ‚àó, case (b) occurs when V = V ‚àóand case (c) occurs
when V > V ‚àó. This global bifurcation comes about purely because of the relative
positions of the manifolds S1 and S2.
In order to determine the numerical value of V ‚àó, we would usually have to solve

EXERCISES
413
the governing equations (13.14) numerically. However, we do know that when V =
V ‚àóthe solution asymptotes to the origin on the stable manifold with Œ≤ = O(e‚àíV ‚àóz)
as z ‚Üí‚àû, whilst for V > V ‚àó, the solution asymptotes to the origin on the centre
manifold, with Œ≤ ‚àºV/z as z ‚Üí‚àû. We were also able to show that there is an
analytical solution, (13.18), when V = 1/
‚àö
2, which has Œ≤ = O(e‚àíz/
‚àö
2) as z ‚Üí‚àû.
This must therefore be the unique solution that corresponds to V = V ‚àó, and we
conclude that the global bifurcation point is V = V ‚àó= 1/
‚àö
2.
In conclusion, we have shown that a unique travelling wave solution exists for
each V ‚©æ1/
‚àö
2, and that the solution with V = 1/
‚àö
2 asymptotes to zero expo-
nentially fast as z ‚Üí‚àû, whilst this decay is only algebraic for V > 1/
‚àö
2. This has
implications for the selection of the speed of the waves generated in an initial value
problem for equations (13.8), in particular that localized initial inputs of autocat-
alyst generate waves with the minimum speed, V = 1/
‚àö
2. The reader is referred
to Billingham and Needham (1991) for further details, and to King and Needham
(1994) for a similar analysis when the diÔ¨Äusion coeÔ¨Écient is not constant.
Exercises
13.1
Sketch the phase portraits of the systems
(a) Àôx = ‚àíx ‚àí2y2,
Àôy = xy ‚àíy3,
(b) Àôx = x2,
Àôy = ‚àíy ‚àíx2,
(c) Àôx = y + x2,
Àôy = ‚àíy ‚àíx2,
in the neighbourhood of the origin, including in your sketch the unstable
and centre manifolds.
13.2
Use arguments based on Lyapunov‚Äôs theorems to determine the stability of
the equilibrium points of
(a) Àôx = x2 ‚àí2y2,
Àôy = ‚àí4xy,
(b) Àôx = xy2 ‚àíx,
Àôy = x2y ‚àíy.
13.3
Consider the second order diÔ¨Äerential equation ¬®Œ∏ + k ÀôŒ∏ + f(Œ∏) = 0, where
f is continuously diÔ¨Äerentiable on the interval |Œ∏| < Œ±, with Œ± > 0 a
given constant, Œ∏f(Œ∏) > 0 for Œ∏ Ã∏= 0 and f(0) = 0.
By considering a
suitable Lyapunov function, show that the origin is an asymptotically stable
equilibrium point.
13.4
Euler‚Äôs equations for a rigid body spinning freely about a Ô¨Åxed point in
the absence of external forces are
A Àôœâ1 ‚àí(B ‚àíC)œâ2œâ3 = 0,
B Àôœâ2 ‚àí(C ‚àíA)œâ3œâ1 = 0,
C Àôœâ3 ‚àí(A ‚àíB)œâ1œâ2 = 0,
where A, B and C are the principal moments of inertia, and œâ = (œâ1, œâ2, œâ3)
is the angular velocity of the body relative to its principal axes.
Find all of the steady states of Euler‚Äôs equations. Using the ideas that we
developed in Section 9.4, show that œâ2
1+œâ2
2+œâ2
3 is a constant of the motion.

414
STABILITY, INSTABILITY AND BIFURCATIONS
Sketch the phase portrait on the surface of the sphere œâ2
1 + œâ2
2 + œâ2
3 = œâ2
0.
Deduce that the steady state with œâ1 = œâ0, œâ2 = œâ3 = 0 is unstable if
C < A < B or B < A < C, but stable otherwise.
Show that
V =
!
B(A ‚àíB)œâ2
2 + C(A ‚àíC)œâ2
3

+ Bœâ2
2 + Cœâ2
3 + A

œâ2
1 + 2œâ0œâ1
"2 ,
is a Lyapunov function for the case when A is the largest moment of inertia,
so that the state œâ1 = œâ0, œâ2 = œâ3 = 0 is stable. Suggest a Lyapunov
function that will establish the stability of this state when A is the smallest
moment of inertia.
Are these states asymptotically stable?
Perform a
simple experiment to verify your conclusions.
13.5
A particle of mass m lies at r = (x, y, z) and moves in a potential Ô¨Åeld
W(x, y, z), so that its equation of motion is
m¬®r = ‚àí‚àáW.
By writing Àôx = u, Àôy = v and Àôz = w, express this equation of motion in
terms of Ô¨Årst derivatives only. Suppose that W has a local minimum at
r = 0. By using the Lyapunov function
V = W + 1
2m

u2 + v2 + w2
,
show that the origin is a stable point of equilibrium for the particle. What
do the level curves of V represent physically? Is the origin asymptotically
stable?
If an additional, nonconservative force, f(u, v, w), also acts on the par-
ticle, so that
m¬®r = ‚àí‚àáW + f,
describe qualitatively how the stability of the point of equilibrium at the
origin is aÔ¨Äected.
13.6
Consider the three systems
(a) Àôy = y(y ‚àí1)(y ‚àí2Œª),
(b) Àôy = y2 + 4Œª2 ‚àí1,
(c) Àôy = ‚àíy(4y2 + Œª2 ‚àí1).
Sketch the bifurcation diagram for each system, and show that each system
has two bifurcation points of the same type, which you should determine.
Close to each bifurcation point, write each system in the normal form
appropriate to the bifurcation.
13.7
(a) Consider the system
Àôx = ‚àíœµ + ¬µx ‚àíx2.
When œµ = 0 this is the normal form for a transcritical bifurcation.
Sketch the bifurcation diagram when œµ Ã∏= 0, dealing separately with
the cases œµ > 0 and œµ < 0. In one case there are two saddle‚Äìnode
bifurcation points, in the other there are no bifurcation points.

EXERCISES
415
(b) Consider the system
Àôx = ¬µx + 2œµx2 ‚àíx3,
with œµ ‚©æ0. When œµ = 0 this is the normal form for a supercriti-
cal pitchfork bifurcation. Sketch the bifurcation diagram when œµ is
small, but nonzero.
What can you deduce from the answers to the two parts of this question?
13.8
Sketch the phase portraits when ¬µ > 0, ¬µ = 0 and ¬µ < 0 for the normal
forms of (a) the transcritical and (b) the supercritical pitchfork bifurcation,
given by (13.34) and (13.35).
13.9
Give an informal proof of the transcritical bifurcation theorem and the
pitchfork bifurcation theorem.
13.10
Consider the system Àôx = ¬µx ‚àíœây + x3, Àôy = œâx + ¬µy + y3. Use the Hopf
bifurcation theorem to determine the nature of the Hopf bifurcation at
¬µ = 0. Use a near identity transformation to write this system in normal
form, and conÔ¨Årm that this is consistent with the Hopf bifurcation theorem.
Use MATLAB to solve this system of equations numerically. Plot how the
limit cycle solution changes with ¬µ, and determine for what range of values
of ¬µ it exists.
13.11
(a) Seek an asymptotic solution of (13.17) subject to the boundary con-
dition (13.15), valid when V ‚â´1, by rescaling Œ≥ = V ¬ØŒ≥ and using an
asymptotic expansion
¬ØŒ≥(Œ≤) = ¬ØŒ≥0(Œ≤) + V ‚àí2¬ØŒ≥1(Œ≤) + O(V ‚àí4).
Hence show that f(V ), as deÔ¨Åned in Section 13.3.3, satisÔ¨Åes
f(V ) = V + 1 ‚àí1
6V ‚àí1 + O(V ‚àí3) for V ‚â´1.
(b) Repeat part (a) when V ‚â™1. In this case, you will need to seek
a rescaling of the form Œ≤ = œÜ(V )ÀÜŒ≤, Œ≥ = œà(V )ÀÜŒ≥, and determine œÜ
and œà by seeking an asymptotic balance in (13.17) and (13.15). You
should Ô¨Ånd that, at leading order,
dÀÜŒ≥
dÀÜŒ≤
= ‚àí1 ‚àí
ÀÜŒ≤2
ÀÜŒ≥ ,
subject to ÀÜŒ≥ ‚àº‚àíÀÜŒ≤ as ÀÜŒ≤ ‚Üí0.
Integrate this equation numerically using MATLAB, and hence show
that
f(V ) ‚àºŒ≤‚àóV 2 as V ‚Üí0,
where Œ≤‚àóis a constant that you should determine numerically.
13.12
Project Consider the CSTR system that we studied in Section 13.3.1, but
where, in addition, the autocatalyst is itself unstable, and breaks down to
form the Ô¨Ånal product C through the chemical reaction
B ‚ÜíC,
rate k2b.

416
STABILITY, INSTABILITY AND BIFURCATIONS
(a) Show that the concentrations of A and B now satisfy the equations
dŒ±
dœÑ = ‚àíŒ±Œ≤2 + 1 ‚àíŒ±
œÑres
,
(E13.1)
dŒ≤
dœÑ = Œ±Œ≤2 + Œ≤0 ‚àíŒ≤
œÑres
‚àíŒ≤
œÑ2
,
(E13.2)
where œÑ2 is a dimensionless constant that you should determine.
(b) Using the ideas that we developed in Section 13.3.1, show that the
steady state solutions are again given by the points of intersection
of a cubic polynomial and a straight line through the origin. With-
out making any quantitative calculations, sketch the position of the
steady states in the (œÑres, z)-plane, Ô¨Årstly when there is no autocat-
alyst in the inÔ¨Çow, and secondly when there is.
(c) Now restrict your attention to the case where there is no autocatalyst
in the inÔ¨Çow. Determine the range of values for which there are
three steady state solutions. Show that the smallest of these steady
states is stable and that the middle steady state is a saddle point.
Show that the largest steady state loses stability through a Hopf
bifurcation, whose location you should determine.
Use the Hopf
bifurcation theorem to determine when this is supercritical and when
it is subcritical.
(d) Use MATLAB to integrate (E13.1) and (E13.2) numerically, and
hence investigate what happens to the limit cycle that forms at the
Hopf bifurcation point.
Draw the complete bifurcation diagram,
indicating the location of any limit cycles. What advice would you
give to an engineer trying to maximize the output of C from the
CSTR? Hint: For some parameter ranges, there is more than one
limit cycle.

CHAPTER FOURTEEN
Time-Optimal Control in the Phase Plane
Many physical systems that are amenable to mathematical modelling do not exist
in isolation from human intervention.
A good example is the British economy,
for which the Treasury has a complicated mathematical model. The state of the
system (the economy) is given by values of the dependent variables (for example,
unemployment, foreign exchange rates, growth, consumer spending and inÔ¨Çation),
and the government attempts to control the state of the system to a target state
(low inÔ¨Çation, high employment, high growth) by varying several control parameters
(most notably taxes and government spending). There is also a cost associated with
any particular action, which the government tries to minimize (some function of,
for example, government borrowing and, one would hope, the environmental cost
of any government action or inaction). The optimal control leads to the economy
reaching the target state with the smallest possible cost.
Another system, for which we have studied a simple mathematical model, con-
sists of two populations of diÔ¨Äerent species coexisting on an isolated island. For the
case of two herbivorous species, which we studied in Chapter 9, we saw that one
species or the other will eventually die out. If the island is under human manage-
ment, this may well be undesirable, and we would like to know how to intervene to
maintain the island close to a state of equilibrium, which we know, if left uncon-
trolled, is unstable. We could choose between either continually culling the more
successful species, continually introducing animals of the less successful species or
some combination of these two methods of control. Each of these actions has a cost
associated with it.
These are examples of optimal control problems. Optimal control is a huge topic,
and in this short, introductory chapter we will study just about the simplest type of
problem, which involves linear, constant coeÔ¨Écient ordinary diÔ¨Äerential equations.
These are, however, extremely important, as it is often necessary to control small
deviations from a steady state, for example when steering a ship, for which linear
equations are a good approximation. We will also restrict our attention to time-
optimal control, for which the cost function is the time taken for the system to reach
the target state. We wish to drive the system to the target state in the shortest
possible time.
The crucial results that we will work towards are the properties of the controlla-
bility matrix and the application of the time-optimal maximum principle. Although
we will give proofs of the various results that we need, the main thing is to know
how to apply them.

418
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.1
DeÔ¨Ånitions
Let‚Äôs begin by formalizing some of the ideas that we introduced above. Consider
the system of ordinary diÔ¨Äerential equations
Àôx = f(x, u, t),
subject to x(0) = x0.
Note that we use the superscript 0 to indicate the initial state and the superscript 1
to indicate the Ô¨Ånal state. We say that x(t) is the state vector, whose components
x = (x1(t), x2(t), . . . , xn(t)) are the state variables, and u is the control vector,
whose components u = (u1(t), u2(t), . . . , um(t)) are the control variables. We
assume that f is continuously diÔ¨Äerentiable with respect to x, u and t, but that u is
merely integrable, so that we can allow for discontinuous changes in its components,
the control variables. As we have seen, these conditions guarantee the existence
and uniqueness of the solution for a given control u.
A control problem takes the form of a question: if x = x0 when t = 0, can we
choose the control vector u(t) so that x = x1, the target state, when t = t1? In
other words, can the system be controlled from x0 to x1, reaching x1 when t = t1?
If there is a cost function,
J =
 t1
0
g0(x(t), u(t), t) dt,
associated with the control problem, such that we seek controls u for which J is a
minimum, we have an optimal control problem. If g0 = 1, and hence J = t1,
so that we seek to minimize the time taken to reach the state x1, we have a time-
optimal control problem. This is the type of problem that we will be studying.
In particular, for Ô¨Årst order, linear, constant coeÔ¨Écient equations,
dx
dt = Ax + Bu(t),
and for second order, linear, constant coeÔ¨Écient equations,
dx1
dt = A11x1 + A12x2 + B11u1(t) + B12u2(t),
dx2
dt = A21x1 + A22x2 + B21u1(t) + B22u2(t).
We can write this more concisely using matrix notation as
dx
dt = Ax + Bu,
where A and B are constant, 2 √ó 2 matrices.
Note that there are at most n
independent control variables for an nth order linear system of this form.
14.2
First Order Equations
A good way of introducing many of the important, basic concepts of control theory
is to study one-dimensional systems, in other words, Ô¨Årst order, linear ordinary
diÔ¨Äerential equations with constant coeÔ¨Écients.

14.2 FIRST ORDER EQUATIONS
419
Example: The tightrope walker
A tightrope walker is inherently unstable. Walking a tightrope is rather like trying
to balance a pencil on its tip. Although the walker has an equilibrium position,
it is unstable, and she must push herself back towards the vertical to stay up-
right. Moreover, if her deviations from the vertical become too large, they cannot
be controlled, and she falls oÔ¨Äthe tightrope.
This is typical of many types of
problem where the idea is to control small deviations from equilibrium as quickly
as possible ‚Äì linear, time-optimal control.
A very simple model for the tightrope walker is
dx
dt = x + u,
(14.1)
where x represents her angular deviation from the vertical and u her attempts to
control her balance. Let‚Äôs assume that x = x0 > 0 when t = 0. Can this deviation
from the vertical be controlled at all? If so, how quickly can it be controlled, and
what should the control be? If there is no attempt to control the deviation, u = 0,
x = x0et and the tightrope walker falls oÔ¨Ä. The upright position, x = 0, is an
unstable equilibrium state. For a general control u(t), we can solve (14.1) using the
integrating factor e‚àít, so that
d
dt(e‚àítx) = e‚àítu(t),
and hence
x = x0et + et
 t
0
e‚àíœÑu(œÑ) dœÑ
(14.2)
Since we want to control the system to x = 0 when t = t1, we must have
x0 +
 t1
0
e‚àíœÑu(œÑ) dœÑ = 0.
(14.3)
Clearly, since x0 > 0, u must be negative for at least some of the period 0 ‚©Ω
t ‚©Ωt1.
Now, if u(t) is not bounded below (the tightrope walker can apply an
arbitrarily large restoring force), it is easy to choose u to satisfy (14.3), for example
with u(t) = ‚àíetx0/t1. This is unrealistic, since we can control the system in an
arbitrarily short time t1 by making |u| suÔ¨Éciently large. The time-optimal control
problem is meaningless if the control is unbounded. From now on we restrict our
attention to bounded controls with ‚àí1 ‚©Ωu(t) ‚©Ω1. We will see later how to scale
a slightly more realistic problem so that u lies in this convenient range. In general,
the components of a bounded control vector satisfy ‚àí1 ‚©Ωui(t) ‚©Ω1.
Intuitively, we can see that to push x back towards equilibrium as quickly as
possible, we need to push in the appropriate direction as hard as we can by taking
u(t) = ‚àí1, so that (14.3) gives
x0 ‚àí
 t1
0
e‚àíœÑ dœÑ = 0,
and hence
t1 = ‚àílog(1 ‚àíx0),
(14.4)

420
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
with solution
x(t) = 1 + (x0 ‚àí1)et.
(14.5)
For t > t1 we take u = 0, and the system remains in its equilibrium state, x = 0.
Equation (14.4) shows that t1 ‚Üí‚àûas x0 ‚Üí1‚àí. If x0 ‚©æ1, the system cannot
be controlled back to equilibrium. The tightrope walker cannot push hard enough,
and she falls oÔ¨Ä. Similarly, if x0 < 0, the time-optimal control is u(t) = 1, pushing
in the other direction. Some time-optimal solutions are shown in Figure 14.1.
Fig. 14.1. Some time-optimal solutions, (14.5), of the tightrope walker problem.
We can now make a couple of deÔ¨Ånitions. The controllable set at time t1
is the set of initial states that can be controlled to the origin in time t1. For the
tightrope walker problem, this is the set
C(t1) =

x | |x| ‚©Ω1 ‚àíe‚àít1
.
The controllable set is the set of initial states that can be controlled to the origin
in some Ô¨Ånite time. This is just the union of all the controllable sets at time t1 ‚©æ0.
For the tightrope walker problem, the controllable set is
C =
=
t1‚©æ0
C(t1) = {x | |x| < 1} .
If C is the whole real line, C = R, we say that the system is completely control-
lable. The tightrope walker system is not completely controllable, since C Ã∏= R
(see Exercise 14.1). This is a good point at which to deÔ¨Åne the reachable set. If

14.2 FIRST ORDER EQUATIONS
421
x = x0 when t = 0, the reachable set in time t1, R(t1, x0), is the set of points x1
for which there exists a control u(t) such that x(t1) = x1. From (14.2),
x1 = x0et1 + et1
 t1
0
e‚àíœÑu(œÑ) dœÑ,
and hence, since |u(t)| ‚©Ω1,
x1e‚àít1 ‚àíx0 =

 t1
0
e‚àíœÑu(œÑ) dœÑ
 ‚©Ω
 t1
0
e‚àíœÑ|u(œÑ)| dœÑ ‚©Ω
 t1
0
e‚àíœÑ dœÑ = 1 ‚àíe‚àít1,
so that
(x0 ‚àí1)et1 + 1 ‚©Ωx1 ‚©Ω(x0 + 1)et1 ‚àí1,
(14.6)
deÔ¨Ånes the points in the reachable set, R(t1, x0).
For |x0| ‚©æ1, the reachable
set does not contain the origin, whilst for |x0| < 1 the origin is reachable for
t1 ‚©æ‚àílog(1 ‚àí|x0|), consistent with what we know about the controllable set.
The reachable set is shown for two diÔ¨Äerent cases in Figure 14.2. Note that the
boundaries of the reachable set are given by the solutions with the controls u(t) =
¬±1, a fact that will prove to be important later.
t1
x1
x1
Fig. 14.2. The reachable set for the tightrope walker problem lies between the curved
lines.

422
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.3
Second Order Equations
For second order, linear, constant coeÔ¨Écient systems, we can examine the behaviour
of the state variables, x1 and x2, in the (x1, x2)-phase plane (see Chapter 9). We
will show that our intuitive notion that only the extreme values of the bounded
control are used in the time-optimal control is correct. This is known as bang-
bang control. Before we can do this, there are a few mathematical ideas that we
must consider. All of the following can be generalized to systems of higher order,
and some of it to nonlinear, nonautonomous systems.
14.3.1
Properties of sets of points in the plane
‚Äî A convex set, S, is one for which the line segment between any two points in S
lies entirely within S (see Figure 14.3 for some examples). Note that necessary
but not suÔ¨Écient conditions are that S must be both connected (any two points
in S can be joined by a curve lying within S) and simply-connected (S has no
holes‚Ä†). Formally, if S is a convex set and x, y ‚ààS, cx + (1 ‚àíc)y ‚ààS for all c
such that 0 ‚©Ωc ‚©Ω1.
Fig. 14.3. Some examples of convex and nonconvex sets of points in the plane.
‚Ä† More formally, S is connected and any closed loop lying in S can be shrunk continuously to a
point without leaving S.

14.3 SECOND ORDER EQUATIONS
423
‚Äî An interior point of S is a point x ‚ààS for which there exists a disc of points
centred on x, all of which lie in S (see Figure 14.4).
‚Äî The interior, Int(S), of a set S is the set of all the interior points of S.
‚Äî An exterior point of S is a point in the interior of SC, the complement of S.
‚Äî The exterior, Ext(S), of a set S is the set of all the exterior points of S.
‚Äî A boundary point of S is a point, not necessarily in S, that lies in neither the
interior nor the exterior of S (see Figure 14.4). Note that all discs centred on a
boundary point of S contain a point that is not in S.
‚Äî The boundary of a set S is the set of all boundary points of S, and can therefore
be written as
‚àÇS = {x | (x Ã∏‚ààInt(S))} ‚à©

x | (x Ã∏‚ààInt(SC))

.
Fig. 14.4. Examples of interior and boundary points of S, and discs centred on them.
‚Äî If S does not contain any of its boundary points, it is said to be open. For
example, the set of points with |x| < 1 (the open unit disc) is open. Note that
the boundary of the open unit disc is the unit circle, |x| = 1. An open set has
S = Int(S).
‚Äî A closed set contains all of its boundary points. For example, the set of points
with |x| ‚©Ω1 (the closed unit disc) is closed.
‚Äî A set S is strictly convex if, for each pair of points in S, the line segment joining
them is entirely made up of interior points (see Figure 14.5). For a strictly convex
set, the tangent to any boundary point does not meet S at any other boundary
point. If S is convex, but not strictly convex, some of the tangents to the set will
meet the boundary at more than one point along a straight part of the boundary
(see Figure 14.6).

424
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.5. Examples of strictly convex and convex, but not strictly convex sets.
Fig. 14.6. Examples of tangents to convex and strictly convex sets.
14.3.2
Matrix solution of systems of constant coeÔ¨Écient ordinary
diÔ¨Äerential equations
Consider the system of n diÔ¨Äerential equations
dx
dt = Ax + b(t),
subject to x(0) = x0,
(14.7)
with A an n √ó n matrix of constants. In order to be able to write the solution of
this equation in a compact form, we deÔ¨Åne the matrix exponential of At to be
exp(At) =
‚àû

k=0
Aktk
k! ,
(14.8)
with A0 = I, the unit matrix. Note that this power series is convergent for all A
and t. We can see immediately that
d
dt exp(At) =
‚àû

k=1
Aktk‚àí1
(k ‚àí1)! = A exp(At),
a property that the matrix exponential shares with its scalar counterpart, eat.

14.3 SECOND ORDER EQUATIONS
425
Now consider the product
P(t) = exp(At) exp(‚àíAt).
Firstly, we note that P(0) = I. Secondly, using the product rule,
dP
dt = A exp(At) exp(‚àíAt) ‚àíexp(At)A exp(‚àíAt) = 0,
using the fact that, from the deÔ¨Ånition, (14.8),
A exp(At) = exp(At)A.
Similarly, d nP/dtn = 0, and hence from its Taylor series expansion, P(t) = I. In
other words,
{exp(At)}‚àí1 = exp(‚àíAt),
again, in line with the result eate‚àíat = 1 for the scalar exponential function. Note
that, in general, exp(A) exp(B) Ã∏= exp(B) exp(A) unless AB = BA (see Exer-
cise 14.4).
These results mean that when b(t) = 0, so that (14.7) is homogeneous, the
solution can be written as
x = exp(At)x0.
When the equation is inhomogeneous, we can use a matrix integrating factor to
write
exp(‚àíAt)dx
dt ‚àíexp(‚àíAt)Ax = d
dt {exp(‚àíAt)x} = exp(‚àíAt)b,
and hence,
x = exp(At)

x0 +
 t
0
exp(‚àíAœÑ)b(œÑ) dœÑ
	
.
(14.9)
This is the generalization to an nth order system of the Ô¨Årst order solution, an
example of which is given by (14.2).
Finally, note that we know from the Cayley‚ÄìHamilton theorem (Theorem A1.1)
which states that every matrix satisÔ¨Åes its own characteristic equation, that, for
an n √ó n matrix A and k ‚©æn, Ak can be written as a linear combination of
I, A, A2, . . . , An‚àí1, and hence so can exp(At). In particular, for a 2 √ó 2 matrix,
exp(At) is a linear combination of I and A.
Example: Simple harmonic motion
Simple harmonic motion with angular frequency œâ is governed by
d 2x
dt2 + œâ2x = 0.
We can write this as a system of two Ô¨Årst order equations in the usual way as
Àôx1 = x2,
Àôx2 = ‚àíœâ2x1,

426
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
with x = x1. In terms of the generic equation (14.7),
A =

0
1
‚àíœâ2
0

.
We now note that
A2 =
 ‚àíœâ2
0
0
‚àíœâ2

= ‚àíœâ2I.
This means that
A2n = œâ2n(‚àí1)nI,
A2n+1 = œâ2n(‚àí1)nA,
and hence
exp(At) =
‚àû

k=0
Aktk
k!
=
‚àû

n=0
A2nt2n
(2n)! +
‚àû

n=0
A2n+1t2n+1
(2n + 1)!
= I
‚àû

n=0
(‚àí1)nœâ2nt2n
(2n)!
+ A
‚àû

n=0
(‚àí1)nœâ2nt2n+1
(2n + 1)!
= cos œâtI + 1
œâ sin œâtA =
%
cos œât
1
œâ sin œât
‚àíœâ sin œât
cos œât
&
.
With the initial condition x1(0) = x0
1, x2(0) = x0
2, this means that the solution is
x = x0 exp(At) =
Ô£´
Ô£≠
x0
1 cos œât + x0
2
œâ sin œât
‚àíœâx0
1 sin œât + x0
2 cos œât
Ô£∂
Ô£∏.
14.4
Examples of Second Order Control Problems
Example 1: The positioning problem
Consider the one-dimensional problem of positioning an object of mass m in a
frictionless groove using a bounded applied force F(t) such that ‚àíFmax ‚©ΩF(t) ‚©Ω
Fmax. Newton‚Äôs second law gives
F(t) = md 2x
dt2 ,
subject to x(0) = X, Àôx(0) = 0, x(t1) = 0, Àôx(t1) = 0,
minimizing t1. If we let
x1 = Fmax
m x,
x2 = Fmax
m
Àôx,
u = F(t)
Fmax
,
x0
1 = Fmax
m X,
we have a problem in the standard form, with |u| ‚©Ω1,
dx1
dt = x2,
dx2
dt = u,
x1(0) = x0
1,
x2(0) = 0.
(14.10)

14.4 EXAMPLES OF SECOND ORDER CONTROL PROBLEMS
427
In terms of our matrix notation‚Ä†
A =
 0
1
0
0

,
B =
 0
1

.
Let‚Äôs now think what the optimal control might be. We want to push the particle
to the origin as quickly as possible, but it must be at rest when we get it there.
Presumably we should push as hard as we can for some period of time, 0 ‚©Ωt ‚©ΩT,
then decelerate as strongly as we can by pushing as hard as possible in the opposite
direction for T ‚©Ωt ‚©Ωt1, so that the particle is at rest at the origin when t = t1.
We will prove later that this is indeed the optimal method of control, but for now
let‚Äôs just construct the solution.
Assuming that x0
1 > 0, we take u = ‚àí1 for 0 ‚©Ωt ‚©ΩT. We can easily integrate
(14.10) and Ô¨Ånd that
x1 = x0
1 ‚àí1
2t2,
x2 = ‚àít for 0 ‚©Ωt ‚©ΩT.
(14.11)
Then we take u = 1 for T ‚©Ωt ‚©Ωt1, and use x1 = x0
1 ‚àí1
2T 2, x2 = ‚àíT when t = T
to Ô¨Åx the constants of integration, which gives us
x1 = 1
2t2 ‚àí2Tt + T 2 + x0
1,
x2 = t ‚àí2T
for T ‚©Ωt ‚©Ωt1.
(14.12)
Now we just need to Ô¨Ånd T, the time at which we have to switch from accelerating
as hard as possible to decelerating as hard as possible, and t1, the total time taken
to control the particle to the origin. We can obtain this by using x1 = x2 = 0 when
t = t1. This gives T = 1
2t1, so that the periods of acceleration and deceleration are
equal, and t1 = 2
#
x0
1. The full solution is
x1 =
Ô£±
Ô£≤
Ô£≥
x0
1 ‚àí1
2t2
for 0 ‚©Ωt ‚©Ω
#
x0
1,
1
2t2 ‚àí2
#
x0
1t + 2x0
1
for
#
x0
1 ‚©Ωt ‚©Ω2
#
x0
1,
(14.13)
x2 =
Ô£±
Ô£≤
Ô£≥
‚àít
for 0 ‚©Ωt ‚©Ω
#
x0
1,
t ‚àí2
#
x0
1
for
#
x0
1 ‚©Ωt ‚©Ω2
#
x0
1.
(14.14)
A typical solution is plotted as a function of t, and various solutions plotted in
the (x1, x2)-phase plane in Figure 14.7. We will return later to the problem of
determining the time-optimal control if the particle is not initially stationary, so
that x2(0) Ã∏= 0.
Example 2: The steering problem / The positioning problem with friction
The forced motion of a ship is unstable, and tends to drift oÔ¨Äcourse if it is not
controlled. If x1 represents the deviation of the ship from a straight path, we can
‚Ä† Note that, strictly speaking, the matrix B should have another column of zeros, since there is
no dependence on a second control function in this problem. We have omitted this for clarity.

428
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.7. Some time-optimal solutions of the positioning problem when the particle is
initially stationary.
write a simple model as
dx1
dt = x2,
dx2
dt = ‚àíqx2 + u.
(14.15)
The drag force caused by the resistance of the water to the lateral motion of the
ship is represented by the term ‚àíqx2, with q a positive constant. This is exactly
the same as the positioning problem, but in a groove with friction. In terms of our
matrix notation,
A =
 0
1
0
‚àíq

,
B =
 0
1

.
Example 3: Controlling a linear oscillator
The equation for a linear oscillator (simple harmonic motion) of unit angular fre-
quency, subject to an external force, u(t), can be written as
dx1
dt = x2,
dx2
dt = ‚àíx1 + u.
(14.16)
In terms of our matrix notation,
A =

0
1
‚àí1
0

,
B =
 0
1

.

14.5 PROPERTIES OF THE CONTROLLABLE SET
429
If the oscillator is not initially at rest and we wish to bring it to a halt as quickly
as possible, we must solve (14.16) subject to
x1(0) = x0
1,
x2(0) = x0
2,
x1(t1) = x2(t1) = 0,
(14.17)
minimizing t1. We can think of this as the problem of stopping a child on a swing
as quickly as possible.
Example 4: The positioning problem with two controls
Suppose that in the positioning problem we are also able to change the velocity of
the particle through an extra control, so that
dx1
dt = x2 + u1,
dx2
dt = u2.
(14.18)
In terms of our matrix notation,
A =
 0
1
0
0

,
B =
 1
0
0
1

.
With the exception of the steering problem (see Exercise 14.9), we will solve all
of these problems below after we have studied some more of the theory relevant to
this type of control problem.
14.5
Properties of the Controllable Set
Apart from determining the time-optimal control, we often want to construct C(t1),
the controllable set for any time t1. There are a number of statements that we can
make about the geometry of these sets, and of C, the controllable set.
‚Äî If t1 < t2, C(t1) ‚äÇC(t2). In other words, the controllable set never gets smaller
as t increases. Any state controllable to zero in time t1 is also controllable to
zero in time t2 > t1.
‚Äî C(t) and C are connected sets.
‚Äî C is open if and only if 0 ‚ààInt(C).
These results hold for nonlinear, nonautonomous systems of ordinary diÔ¨Äerential
equations. For linear, constant coeÔ¨Écient equations, we also have that C(t) and C
are symmetric about the origin and convex. Note that this also implies that these
sets are simply-connected, since all convex sets are simply-connected.
Theorem 14.1 If t1 < t2, C(t1) ‚äÇC(t2).
Proof Let x0 be a point in C(t1), with control u = v(t). If we apply the control
u =
 v(t)
for 0 ‚©Ωt ‚©Ωt1,
0
for t1 < t ‚©Ωt2,
the trajectory reaches x = 0 when t = t1, then remains there for t1 ‚©Ωt ‚©Ωt2, since
x = 0, u = 0 is an equilibrium state. Therefore x0 ‚ààC(t2), which means that
C(t1) ‚äÇC(t2) (see Figure 14.8).

430
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.8. If t1 < t2, C(t1) ‚äÇC(t2).
Theorem 14.2 If x0 ‚ààC(t1) and y is a point on the trajectory from x0 to 0,
y ‚ààC(t1). In other words, all points on controllable trajectories are controllable.
Proof Let x = X(t) be the trajectory containing both x0 and y, with control u(t).
When t = œÑ1, X(œÑ1) = y, and when t = œÑ2, X(œÑ2) = 0, with œÑ1 ‚©ΩœÑ2 ‚©Ωt1 (see
Figure 14.9). If we now consider the solution with control v(t) = u(t + œÑ1) and
Fig. 14.9. All points on controllable trajectories are controllable.
initial condition x(0) = y, the system follows the same trajectory, x = X(t + œÑ1),
and reaches x = 0 when t = œÑ2‚àíœÑ1. Therefore y ‚ààC(œÑ2‚àíœÑ1) and, by Theorem 14.1,
y ‚ààC(t1).

14.5 PROPERTIES OF THE CONTROLLABLE SET
431
Theorem 14.3 C(t1) and C are connected sets.
Proof
If x0 ‚ààC(t1) and y0 ‚ààC(t1), there are, by deÔ¨Ånition, trajectories that
connect each point to the origin (see Figure 14.10). By Theorem 14.2, all points on
Fig. 14.10. C(t1) and C are connected sets.
each of these trajectories lie in C(t1). Therefore the union of these two trajectories
is a curve made up of points in C(t1) that connects x0 and y0, and hence, by
deÔ¨Ånition, C(t1) is connected. Since C = >
t1‚©æ0 C(t1), C is also connected.
Theorem 14.4 C is open if and only if 0 ‚ààInt(C).
Proof If C is open, all of its points are interior points, so clearly 0 ‚ààInt(C). It is
less straightforward to prove that 0 ‚ààInt(C) implies that C is open.
If 0 ‚ààInt(C), by deÔ¨Ånition there is a disc of radius r centred on 0, which
we write as D(0, r), that lies entirely within C. Now suppose that u = v(t) is
a control that steers some point x0 to 0 in time t1. Let D(x0, r0) be a disc of
radius r0 centred on x0, and let y0 be another point within this disc, as shown in
Figure 14.11. By continuity of the solutions of the underlying diÔ¨Äerential equations,
if r0 is suÔ¨Éciently small, the control v(t) steers y0 into the disc D(0, r) on a path
y(t) with y(t1) ‚ààD(0, r) at some time t1. Since D(0, r) ‚ààC, we can also Ô¨Ånd a
control ÀÜv(t) that steers y(t1) to 0 in some time t2. Therefore y0 can be controlled
to the origin in time t1 + t2 using the control
u(t) =
 v(t)
for 0 ‚©Ωt ‚©Ωt1,
ÀÜv(t)
for t1 < t ‚©Ωt2.
Therefore y0 ‚ààC(t1 + t2) ‚äÇC, and hence, for r0 suÔ¨Éciently small, D(x0, r0) ‚ààC
for all x0 ‚ààC. By deÔ¨Ånition, C is therefore open.

432
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.11. C is open if and only if 0 ‚ààInt(C).
Theorems 14.1, 14.2, 14.3 and 14.4 all hold for nonlinear, nonautonomous sys-
tems of ordinary diÔ¨Äerential equations. We now focus on linear, constant coeÔ¨Écient
equations. Recall that if
dx
dt = Ax + Bu,
with A and B constant matrices, the solution is
x = exp(At)

x0 +
 t
0
exp(‚àíAœÑ)Bu(œÑ) dœÑ
	
.
This means that x0 ‚ààC(t1) if and only if there is a control u(t) such that
x0 = ‚àí
 t1
0
exp(‚àíAœÑ)Bu(œÑ) dœÑ.
(14.19)
Theorem 14.5 C(t1) is symmetric about the origin and convex.
Proof If x0 ‚ààC(t1) with control u(t), (14.19) shows that ‚àíx0 ‚ààC(t1) with control
‚àíu(t).‚Ä† Therefore C(t1) is symmetric about the origin.
Now note that the set of bounded controls
U = {u(t) | ‚àí1 ‚©Ωui(t) ‚©Ω1}
is convex, since if u0 ‚ààU and u1 ‚ààU, cu0 +(1‚àíc)u1 ‚ààU for 0 ‚©Ωc ‚©Ω1. Therefore,
if u0 and u1 are controls that steer x0 and x1 to the origin in time t1,
cx0 + (1 ‚àíc)x1 = ‚àí
 t1
0
exp(‚àíAœÑ)B

cu0(œÑ) + (1 ‚àíc)u1(œÑ)

dœÑ,
‚Ä† Note that the fact that the control variables are scaled so that ‚àí1 ‚©Ωui(t) ‚©Ω1 is crucial here.

14.6 THE CONTROLLABILITY MATRIX
433
and hence the control cu0(œÑ) + (1 ‚àíc)u1(œÑ) ‚ààU steers cx0 + (1 ‚àíc)x1 to the origin
in time t1. The line segment that joins x0 to x1 therefore lies entirely within C(t1),
and hence, by deÔ¨Ånition, C(t1) is convex.
Theorem 14.6 C is symmetric about the origin and convex.
Proof A union of symmetric sets is symmetric, so C = >
t1‚©æ0 C(t1) is symmetric.
Although a union of convex sets is not necessarily convex (see Figure 14.12), since
Theorem 14.3 tells us that C(t1) ‚äÇC(t2) for t1 < t2, C is a union of nested convex
sets and therefore is itself convex.
Fig. 14.12. A union of convex sets is not necessarily convex.
14.6
The Controllability Matrix
For a second order system, we deÔ¨Åne the controllability matrix to be
M = [B AB].
(14.20)
We will show that the system is completely controllable (C = R2) if and only if
(i) rank(M) = 2,
(ii) all the eigenvalues of A have zero or negative real part.
Although we will not do so here, it is straightforward to generalize this result to
nth order systems.
Example: The positioning problem
For the positioning problem
A =
 0
1
0
0

,
B =
 0
1

,
and hence
M =
 0
1
1
0

.
Clearly, rank(M) = 2, since its columns are linearly independent. Also, since
det(A ‚àíŒªI) = Œª2,

434
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
the matrix A has the repeated eigenvalue zero. We conclude that the system is
completely controllable. All of the other examples that we described in Section 14.4
are also completely controllable (see Exercise 14.6).
Let‚Äôs now prove that these properties of the controllability matrix do determine
whether or not a second order, linear, constant coeÔ¨Écient system is controllable.
Theorem 14.7 0 ‚ààInt(C) if and only if rank(M) = 2.
Proof Suppose that rank(M) < 2. If rank(M) = 1, there is a single direction, y,
orthogonal to every column of M. This means that yTB = yTAB = 0, and hence
by the Cayley‚ÄìHamilton theorem,
yTAkB = 0 for k = 0, 1, 2, . . . .
This means that yT exp(‚àíAœÑ)B = 0. Now, if x0 ‚ààC(t1), (14.19) shows that
yTx0 = y ¬∑ x0 = ‚àí
 t1
0
yT exp(‚àíAœÑ)Bu(œÑ)dœÑ = 0.
Therefore if rank(M) = 1, x0 lies on the straight line through the origin perpen-
dicular to y, which is a closed set, and hence 0 Ã∏‚ààInt(C). If rank(M) = 0, M has
only zero entries, and hence so does B. There are therefore no controls, C = {0},
and hence 0 Ã∏‚ààInt(C).
Now suppose that 0 Ã∏‚ààInt(C). Since C(t1) ‚äÇC, 0 Ã∏‚ààInt(C(t1)) at any time t1.
Since 0 ‚ààC(t1), the origin must be a boundary point of C(t1). Since C(t1) is convex
(Theorem 14.5), there is a tangent to C(t1) through 0 with outward normal z, and
for all x0 ‚ààC(t1), zTx0 = z ¬∑ x0 ‚©Ω0 (see Figure 14.13). Equation 14.19 then shows
Fig. 14.13. The tangent and outward normal to the set C(t1) at the origin.

14.6 THE CONTROLLABILITY MATRIX
435
that
 t1
0
zT exp(‚àíAœÑ)Bu(œÑ) dœÑ ‚©æ0
for all controls u. However, since ‚àíu is also an admissible control,
‚àí
 t1
0
zT exp(‚àíAœÑ)Bu(œÑ) dœÑ ‚©æ0,
and hence
 t1
0
zT exp(‚àíAœÑ)Bu(œÑ) dœÑ = 0 for 0 ‚©Ωt ‚©Ωt1.
Since this must hold for all controls u,
zT exp(‚àíAt)B = 0 for 0 ‚©Ωt ‚©Ωt1.
If we now set t = 0 we get zTB = 0, and if we diÔ¨Äerentiate and put t = 0,
zTAB = 0. This means that z is orthogonal to all of the columns of M, and hence
rank(M) < 2.
Since the system can only be completely controllable if 0 ‚ààInt(C), this theorem
shows that a necessary condition for the system to be completely controllable is
that rank(M) = 2.
Theorem 14.8 If rank(M) = 2 and Re(Œªi) ‚©Ω0 for each eigenvalue Œªi of A, the
system is completely controllable.
Proof We proceed by contradiction. Suppose that rank(M) = 2 and A has eigen-
values with zero or negative real parts, but that C Ã∏= R2. Consider a point y Ã∏‚ààC.
There is then a tangent to C, with equation n ¬∑ x = p, that separates y from each
x0 ‚ààC, with n ¬∑ x0 ‚©Ωp and n ¬∑ y ‚©æp (see Figure 14.14). Let z = n {exp(‚àíAt)B}.
Because rank(M) = 2, z Ã∏= 0 for 0 ‚©Ωt ‚©Ωt1. Now choose a control with components
ui(t) = ‚àísgn(zi(t)), so that
n ¬∑ x0 = ‚àí
 t1
0
nT exp(‚àíAœÑ)Bu(œÑ) dœÑ
= ‚àí
 t1
0
z(œÑ)u(œÑ) dœÑ =
 t1
0
(|z1| + |z2|) dœÑ.
By choosing an appropriate coordinate system, we can make each component of z
a sum of terms proportional to e‚àíŒªiœÑ. If any eigenvalue has zero part, the corre-
sponding component of z will be either a polynomial or a periodic function of t.
In each case,
' t1
0 (|z1| + |z2|) dœÑ ‚Üí‚àûas t1 ‚Üí‚àû. In particular, for t1 suÔ¨Éciently
large, n ¬∑ x0 > p. This is a contradiction, and we conclude that C = R2.
Theorem 14.9 If rank(M) = 2 and A has at least one eigenvalue with positive
real part, the system is not completely controllable.

436
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.14. A tangent to the set C that separates each x0 ‚ààC from y Ã∏‚ààC.
Proof
Let Œª be an eigenvalue of A with positive real part, and e the associ-
ated eigenvector, so that eTA = ŒªeT, and eTAk = ŒªkeT.
This means that
eT exp(‚àíAœÑ) = e‚àíŒªœÑeT, and hence
eTx0 = e ¬∑ x0 = ‚àí
 t1
0
e‚àíŒªœÑeTBu(œÑ) dœÑ.
This integral converges as t1 ‚Üí‚àû, and is bounded above by some constant c, so
that e ¬∑ x0 ‚©Ωc. The controllable set therefore lies on one side of a line in the plane,
and hence C Ã∏= R2.
This concludes our proof that the controllability matrix has the properties that
we outlined at the start of this section.
14.7
The Time-Optimal Maximum Principle (TOMP)
Consider the set reachable from x0 in time t, R(t, x0). As time increases, this set
traces out a volume in (x1, x2, t)-space, which we label RT(t, x0). If the system can
be controlled to the origin, the shortest time in which this can be achieved is t‚àó,
where t‚àóis the Ô¨Årst time when t‚àó‚ààR(t‚àó, x0) (see Figure 14.15).
Theorem 14.10 The time-optimal trajectory lies in the boundary, ‚àÇRT(t, x0).
Proof Let u = u‚àó(t) be the optimal control for x(0) = x0, and let y(t) be a solution
with u = u‚àó. Now suppose that y(t0) ‚ààInt(R(t0, x0)). There must therefore be
a disc of suÔ¨Éciently small radius r, D(y(t0), r), that lies entirely within R(t0, x0).
If we now apply the optimal control, u‚àó, to all of the points within D(y(t0), r),
they will lie in the neighbourhood of y(t1) when t = t1 > t0, and must also lie in
R(t1, x0). Therefore y(t1) ‚ààInt(R(t1, x0)), so that any trajectory that starts in

14.7 THE TIME-OPTIMAL MAXIMUM PRINCIPLE (TOMP)
437
Fig. 14.15. The reachable set, RT(t, x0).
Int(R(t, x0)) remains there. Since the origin lies in the boundary, ‚àÇRT(t, x0), the
time-optimal trajectory must lie entirely within this boundary.
Theorem 14.11 (The time-optimal maximum principle (TOMP)) The con-
trol u(t) is time-optimal if and only if there exists a nonzero vector, h, such that
for 0 ‚©Ωt ‚©Ωt1,
hT {exp(‚àíAt)Bu(t)} = sup
v(t)
hT {exp(‚àíAt)Bv(t)} .
The components of the time-optimal control are
ui(t) = sgn
!
hT {exp(‚àíAt)B}
"
i
for i = 1, 2, . . . , m
‚Äì bang-bang control.
Proof Let u(t) be a control that steers x from x0 when t = 0 to x1 ‚àà‚àÇR(t1, x0)
when t = t1. The reachable set is convex (this can be proved in the same way that
we proved that the controllable set is convex in Theorem 14.5), so there is a tangent
at x1 with normal n such that
n ¬∑ x =
sup
y1‚ààR(t1,x0)
n ¬∑ y1.
If v(t) is an arbitrary control and y(t) the corresponding solution,
y1 = exp(At1)

x0 +
 t1
0
exp(‚àíAœÑ)Bv(œÑ) dœÑ
	
,

438
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
and y1 = x1 when v = u. This means that
n ¬∑ exp(At1)x0 + n ¬∑

exp(At1)
 t1
0
exp(‚àíAœÑ)Bu(œÑ) dœÑ
	
= sup
v

n ¬∑ exp(At1)x0 + n ¬∑

exp(At1)
 t1
0
exp(‚àíAœÑ)Bv(œÑ) dœÑ
	
,
and hence

nT exp(At1)
  t1
0
exp(‚àíAœÑ)Bu(œÑ) dœÑ
= sup
v

nT exp(At1)
  t1
0
exp(‚àíAœÑ)Bv(œÑ) dœÑ.
If we let hT = nT exp(At1), we Ô¨Ånd that
 t1
0
hT exp(‚àíAœÑ)Bu(œÑ) dœÑ = sup
v
 t1
0
hT exp(‚àíAœÑ)Bv(œÑ) dœÑ
	
.
(14.21)
Note that h is nonzero, because exp(At) is a nonsingular matrix since it always has
an inverse, exp(‚àíAt). Since (14.21) holds for all t1 > 0, we must have
hT exp(‚àíAœÑ)Bu(œÑ) = sup
v

hT exp(‚àíAœÑ)Bv(œÑ)

.
(14.22)
All of the steps of this argument can be reversed, and we have therefore proved the
Ô¨Årst part of the theorem.
To obtain the maximum value of the right hand side of (14.22), we must take
vi(t) = sgn
!
hT exp(‚àíAt)B
"
i
for 0 ‚©Ωt ‚©Ωt1.
In other words, each component of the time-optimal control must take one of its
extreme values, 1 or ‚àí1, and change when
!
hT exp(‚àíAt)B
"
i changes sign ‚Äì bang-
bang control.
We shall see that this is the main result that we need to solve time-optimal control
problems in the phase plane.
Example: The positioning problem
For this problem
A =
 0
1
0
0

,
B =
 0
1

.
Since A2 = 0,
exp(‚àíAt) = I ‚àíAt =
 1
‚àít
0
1

,
and
exp(‚àíAt)B =
 ‚àít
1

.

14.7 THE TIME-OPTIMAL MAXIMUM PRINCIPLE (TOMP)
439
Therefore, if we write hT = (Œ±, Œ≤), we Ô¨Ånd that
hT exp(‚àíAt)B = Œ≤ ‚àíŒ±t.
This means that the time-optimal, bang-bang control changes sign at most once.
The TOMP does not tell us when, or even whether, this change in sign occurs.
However, with x1(0) = x0
1 and x2(0) = 0, an initially stationary particle, we have
seen that the control must change sign at least once if the solution is to reach
the origin. We also constructed the unique solution with bang-bang control that
changes sign just once. The TOMP now shows that this is indeed the time-optimal
solution.
Let‚Äôs now consider what happens if the particle is initially moving, with x1(0) =
x0
1 and x2(0) = x0
2. Rather than just solving the equations, let‚Äôs think about what
the time-optimal solution will look like in the phase plane.
We know that the
time-optimal control is bang-bang with u = ¬±1, so the integral paths are given by
dx1
dx2
= ¬±x2,
and hence are parabolas, with
x1 = k ¬± 1
2x2
2
for some constant of integration k. The only two time-optimal paths that reach the
origin are therefore the appropriate branches of x1 = ¬± 1
2x2
2, which are labelled as
S¬± in Figure 14.16. On S+ we need u = 1, whilst on S‚àíwe need u = ‚àí1. Any initial
conditions that lie on these curves can be controlled to the origin without changing
the sign of u.
For any other initial conditions, the system must be controlled
onto one of the curves S¬±, when the control must change sign, as illustrated in
Figure 14.16.
Example: Controlling a linear oscillator
For a linear oscillator with unit angular frequency,
exp(‚àíAt) =
 cos t
‚àísin t
sin t
cos t

,
and
exp(‚àíAt)B =
 ‚àísin t
cos t

.
With hT = (Œ±, Œ≤),
hT exp(‚àíAt)B = Œ≤ cos t ‚àíŒ± sin t.
The TOMP therefore shows that the time-optimal control is
u(t) = sgn(Œ≤ cos t ‚àíŒ± sin t) = sgn {a cos(t + b)} ,
for some constants a and b. Since cos(t + b) changes sign at intervals of œÄ, so must
u(t). The Ô¨Årst change of sign occurs when t = T0 with 0 < T0 ‚©ΩœÄ, and must

440
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.16. The time-optimal trajectories for the positioning problem.
subsequently change when t = Tn = T0 + nœÄ, for n = 0, 1, 2, . . . . Let‚Äôs consider the
solution when u = ¬±1 in the phase plane. The governing equations, (14.16), show
that
(x1 ‚àì1)dx1
dt + x2
dx2
dt = 0,
and hence that
(x1 ‚àì1)2 + x2
2 = k2,
for some constant of integration, k. These trajectories are circles, centred on (¬±1, 0),
as shown in Figure 14.17. Proceeding as we did for the positioning problem, we
can see that only the circles marked S¬± in Figure 14.17, given by (x‚àì1)2 +x2
2 = 1,
enter the origin. We can now construct the time-optimal solution by considering
the solutions that meet S¬±, with the control changing sign there. A typical example
is shown in Figure 14.18. The sign of the control must change with period œÄ, so the
time-optimal solution consists of part of S+ or S‚àí, and a succession of semicircles
of increasing radius with centres alternating between (1, 0) and (‚àí1, 0). Intuitively,
we would perhaps have expected the control to change sign when x2 = 0. Thinking
in terms of stopping a child on a swing, we might have expected to push in the
opposite direction to the motion. In fact, the time-optimal solution is out of phase
with this intuitive solution in order to allow the velocity to be zero precisely when
the swing is vertical.

14.7 THE TIME-OPTIMAL MAXIMUM PRINCIPLE (TOMP)
441
Fig. 14.17. The bang-bang trajectories for the problem of controlling a linear oscillator.
Example: The positioning problem with two controls
For this problem
exp(‚àíAt)B =
 1
‚àít
0
1

.
With hT = (Œ±, Œ≤),
hT exp(‚àíAt)B = (Œ±, Œ≤ ‚àíŒ±t).
The TOMP therefore says that the time-optimal control u2 = sgn(Œ≤ ‚àíŒ±t) changes
sign at most once, as was the case for the positioning problem with just one control
variable. In contrast, u1 = sgn(Œ±) does not change sign in the time-optimal control,
provided that Œ± Ã∏= 0. If, however, Œ± = 0, u2 does not change sign and u1 is unde-
termined. Let‚Äôs begin by considering this case, with u2 = ‚àí1. This immediately
gives us
x2 = x0
2 ‚àít,
which is therefore only appropriate when x0
2 > 0.
The optimal control time is
therefore t1 = x0
2. If we now integrate the equation for x1, we obtain
x1 = x0
1 + x0
2t ‚àí1
2t2 +
 t1
0
u1(œÑ) dœÑ.

442
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
x1
x2
S¬Ø
x
x
Fig. 14.18. A typical time-optimal trajectory for the problem of controlling a linear oscil-
lator.
Since x1(t1) = 0, we have a time-optimal trajectory for any control u1 such that
 x0
2
0
u1(œÑ) dœÑ = ‚àíx0
1 ‚àí1
2(x0
2)2.
However, since |u1| ‚©Ω1, this can only be achieved when
‚àíx0
1 ‚àí1
2(x0
2)2
 ‚©Ωx0
2,
and hence when
‚àíx0
2 ‚àí1
2(x0
2)2 ‚©Ωx0
1 ‚©Ωx0
2 ‚àí1
2(x0
2)2.
This is marked as region B in Figure 14.19. Similarly, when x0
2 < 0 and u2 = 1 we
have nonunique time-optimal paths in region A, where
x0
2 + 1
2(x0
2)2 ‚©Ωx0
1 ‚©Ω‚àíx0
2 + 1
2(x0
2)2.
Outside these two regions, the time-optimal control is unique, with u1 = u2 = ‚àí1 in
region C and u1 = u2 = 1 in region D, as marked in Figure 14.19. In region C, each
time-optimal path meets the boundary of region A, where the sign of u1 changes,
and then follows the boundary of region A to the origin. Similarly in region D.
Note that, since t1 = |x0
2| in regions A and B where the time-optimal solution

EXERCISES
443
Fig. 14.19. The regions A, B, C and D, and some time-optimal solutions for the positioning
problem with two controls.
is not unique, the boundaries of C(t1), the reachable set in time t1, are straight
lines in these two regions. This means that C(t1) is not strictly convex. Although
we will not discuss this further, controllable sets that are not strictly convex are
always associated with nonunique time-optimal solutions.
In practice, the diÔ¨Éculty with applying bang-bang control lies in determining
when the control needs to change by making measurements of the state of the
system. This is known as measurement‚Äìaction lag (see, for example, Marlin,
1995).
Exercises
14.1
We have seen that the tightrope walker system, Àôx = x+u, is not completely
controllable.
Show, by solving the governing equations, that the other
two qualitatively diÔ¨Äerent, Ô¨Årst order, linear, constant coeÔ¨Écient systems,
Àôx = ‚àíx + u and Àôx = u, are completely controllable.
14.2
For the tightrope walker system, Àôx = x + u(t) with |u| ‚©Ω1, Ô¨Ånd the
reachable set from x = 2 in time t1, R(t1, 2) and the set controllable to
x = 2 in time t1, C(t1, 2), and sketch them. Show that R(t1, 2) Ã∏‚äÇR(t2, 2)
and C(t1, 2) Ã∏‚äÇC(t2, 2) when t1 < t2.

444
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.3
Consider the system of ordinary diÔ¨Äerential equations
dx
dt = Ax + b(t),
subject to x(0) = x0 = (x0
1, x0
2)T, with b(t) = (b1(t), b2(t)). Determine the
exponential matrices, exp(At) and exp(‚àíAt), and hence write down both
the general solution and the solution when b is independent of t when A =
(i)
 0
0
1
0

(ii)
 1
0
0
2

(iii)

1
0
‚àíq
0

.
14.4
Show that
exp(At) exp(Bt) = exp((A + B)t),
if and only if the matrices A and B commute.
14.5
Classify each of the following sets of points in the plane Ô¨Årstly as either
open, closed, or neither open nor closed, and secondly as either strictly
convex, convex but not strictly convex, or not convex.
(a)

(x1, x2) | (x1 ‚àí1)2 + x2
2 < 1

‚à™

(x1, x2) | x2
1 + x2
2 < 1

,
(b)

(x1, x2) | (x1 ‚àí1)2 + x2
2 ‚©Ω1

‚à©

(x1, x2) | x2
1 + x2
2 ‚©Ω1

,
(c)

(x1, x2) | x2
1 + x2
2 ‚©Ω1

‚à©{(x1, x2) | x1 ‚©æ0} ,
(d)

(x1, x2) | x2
1 + x2
2 ‚©Ω1

‚à©{(x1, x2) | x1 > 0} .
14.6
Use the controllability matrix to show that Examples 2, 3 and 4 given in
Section 14.4 are completely controllable systems.
14.7
If Àôx = Ax+Bu with u the vector of bounded controls, construct the control-
lability matrix, M, and hence determine whether the system is completely
controllable when
(a)
A =
 0
1
0
2

,
B =
 1
1

,
(b)
A =
 ‚àí1
1
0
0

,
B =
 1
1

,
(c)
A =
 ‚àí1
1
0
‚àí1

,
B =
 1
0
0
1

.
14.8
Consider the positioning problem. Show that for initial conditions lying
above the curves S¬± the switching time is
T = x0
2 +
*
x0
1 + 1
2(x0
2)2,
and that the optimal control time is
t1 = x0
2 + 2
*
x0
1 + 1
2(x0
2)2.
Determine C(t1), the controllable set in time t1.

EXERCISES
445
14.9
Consider the steering problem. Show that
exp(‚àíAt) =
Ô£´
Ô£≠1
1
q (1 ‚àíeqt)
0
eqt
Ô£∂
Ô£∏.
Use the TOMP to show that the time-optimal control has at most one
change of sign. If x1(0) = x0
1 and x2(0) = 0, show that the optimal control
time is
t1 = 2
q log

exp
1
2q2x0
1

+
$
exp (q2x0
1) ‚àí1
	
.
Show that the integral paths for u = ¬±1 are
x1 = k ‚àí1
q2 {qx2 ¬± log (1 ‚àìqx2)} ,
with k a constant. Sketch the time-optimal trajectories and the controllable
set in time t1 in the (x1, x2)-phase plane.
14.10
The population of a pest is increasing exponentially. To control the pest,
a genetically engineered, sterile, predatory beetle is introduced into the
environment.
Since the beetles are harmful to crops, it is desirable to
remove them as soon as possible.
Let the populations of the pest and the beetle be denoted by x1(t) and
x2(t) creatures per square metre respectively. Initially, x1 = X > 0 and
x2 = 0, and the target is to reduce both populations to zero simultaneously
and in the shortest possible time. Model equations for the two populations
are
Àôx1 = x1 ‚àíx2,
Àôx2 = ‚àíx2 + u(t),
with |u| ‚©Ω1 representing the rate at which beetles are released into the
environment. Use the time-optimal maximum principle to show that the
optimal control changes sign just once.
Show that the optimal control time is
t1 = log

X + 1 +
‚àö
X2 + 4X
2X ‚àí1

.
What is the upper limit on X below which the system is controllable?
14.11
Consider the system
Àôx1 = x1 + x2,
Àôx2 = ‚àíx2 + u(t),
subject to x1(0) = x0
1, x2(0) = x0
2, with |u(t)| ‚©Ω1. Use the time-optimal
maximum principle to show that the sign of the optimal control changes
at most once. Show that when x2 is initially positive, the time-optimal
solution on which the control does not change sign has
x0
1 = ‚àí
(x0
2)2
2(1 + x0
2).

446
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.12
Determine the matrices exp(‚àíAt) and exp(At) when
A =
1
‚àö
2
 1
1
1
‚àí1

.
Consider the second order system of ordinary diÔ¨Äerential equations
dx
dt = Ax + Bu,
where x is the vector of state variables, u the vector of bounded control
variables with |ui(t)| ‚©Ω1, and
B =
 1
0

.
Use the time-optimal maximum principle to show that the sign of u(t)
can change no more than once in the time-optimal control. Show that the
time-optimal solutions on which the control does not change sign have
x0
1 = ¬±

sinh t1 ‚àí1
‚àö
2 cosh t1 + 1
‚àö
2

,
x0
2 = ¬±

‚àí1
‚àö
2 cosh t1 + 1
‚àö
2

,
where t1 is the time taken to control the system to the origin.

448
AN INTRODUCTION TO CHAOTIC SYSTEMS
If we now deÔ¨Åne the dimensionless variables
x =
+
l
2a2(l ‚àía)Y,
ÀÜt =
*
¬µ(l ‚àía)
ma
t,
we obtain
¬®x + œµŒ¥ Àôx ‚àíx + x3 = F(ÀÜt ),
(15.2)
where
œµŒ¥ = k
*
ma
¬µ(l ‚àía),
F(ÀÜt ) = ‚àí

¬®œÜ + k ÀôœÜ
 +
2m2a4
¬µ2l(l ‚àía).
The reason for our rather odd choice of dimensionless parameters will become clear
later. Equation (15.2) with F = 0 is known as DuÔ¨Éng‚Äôs equation, and arises
in many other contexts in mechanics. With F Ã∏= 0, (15.2) is the forced DuÔ¨Éng
equation.‚Ä† We will assume that the forcing takes the form F = œµŒ≥ cos œât (dropping
the hat on the time variable for convenience), so that (15.2) becomes
¬®x + œµŒ¥ Àôx ‚àíx + x3 = œµŒ≥ cos œât.
(15.3)
a
y(t)
mky
m
œÜ(t)
.
Fig. 15.1. A mechanical system whose small amplitude oscillations are governed by the
forced DuÔ¨Éng equation, (15.3).
Figure 15.2 shows the solution, calculated numerically using MATLAB (see Sec-
tion 9.3.4), when œµŒ¥ = 1
2, œµŒ≥ = 3
5 and y = dy/dt = 0 when t = 0. This solution
‚Ä† For more on the dynamics of the forced DuÔ¨Éng equation, see Arrowsmith and Place (1990)
and references therein.

CHAPTER FIFTEEN
An Introduction to Chaotic Systems
In order to introduce the idea of a chaotic solution, we will begin by studying
three simple chaotic systems that arise in diÔ¨Äerent physical contexts. We then look
at some examples of mappings, which are important because ordinary diÔ¨Äerential
equations can be related to mappings through the Poincar¬¥e return map.
After
investigating homoclinic tangles in Poincar¬¥e return maps, which contain chaotic
solutions, we investigate how their existence can be established by examining the
zeros of the Mel‚Äônikov function. Finally, we discuss the computation of the Lya-
punov spectrum of a diÔ¨Äerential equation, from which a quantitative measure of
chaos can be obtained.
15.1
Three Simple Chaotic Systems
15.1.1
A Mechanical Oscillator
Consider the mechanical system that consists of two rings of mass m threaded
onto two horizontal wires a distance a apart, as shown in Figure 15.1. The rings
are joined by a spring of natural length l > a that obeys Hooke‚Äôs law with elastic
constant ¬µ. If we move the upper ring, what happens to the lower ring? We denote
the displacement of the upper ring from a Ô¨Åxed vertical line by œÜ(t), and that of
the lower ring by y(t). On the assumption that a frictional force of magnitude
mk Àôy opposes the motion of the lower ring, Newton‚Äôs second law in the horizontal
direction shows that
‚àí¬µ
#
a2 + (y ‚àíœÜ)2 ‚àíl

(y ‚àíœÜ)
#
a2 + (y ‚àíœÜ)2
‚àímk Àôy = m¬®y,
(15.1)
where a dot denotes d/dt. Let‚Äôs assume that the relative displacement of the two
rings, Y = y ‚àíœÜ, is much less than the distance, a, between the wires, so that
#
a2 + (y ‚àíœÜ)2 = a
*
1 + Y 2
a2 ‚àºa + Y 2
2a .
In terms of Y , (15.1) becomes
¬®Y + k ÀôY ‚àí¬µ(l ‚àía)
ma
Y +
¬µl
2ma3 Y 3 ‚àº‚àí¬®œÜ ‚àík ÀôœÜ.

15.1 THREE SIMPLE CHAOTIC SYSTEMS
449
behaves erratically. Indeed, it is tempting to think of it as ‚Äòrandom‚Äô in some sense.
However, we know from Chapter 8 that the solution of (15.3) exists and is unique.
Figure 15.3 superimposes another solution, this time with y = 10‚àí4, dy/dt = 0
when t = 0. The two solutions begin close to each other, but, as time increases,
drift further apart, and soon diverge completely. We say that the system exhibits
sensitive dependence upon initial conditions. In practice, we can only know
the initial state of a physical system with a Ô¨Ånite degree of accuracy. After a suf-
Ô¨Åcient time has elapsed, solutions with diÔ¨Äerent initial conditions, but which are
close enough together that, in practice, they are indistinguishable, will diverge in a
chaotic system.
Fig. 15.2. The solution of the forced DuÔ¨Éng equation, (15.3), when œµŒ¥ = 1
2, œµŒ≥ = 3
5 and
y = dy/dt = 0 when t = 0.
We can now give an informal deÔ¨Ånition of a chaotic solution as a bounded,
aperiodic, recurrent solution, that has a random aspect due to its sensitive depen-
dence on initial conditions. Adjacent chaotic solutions diverge exponentially fast,
a property that we will later measure using the Lyapunov spectrum, and remain
in a bounded region, where they undergo repeated folding, and are, in practice,
unpredictable in the long term.

450
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.3. The solution of the forced DuÔ¨Éng equation, (15.3), when œµŒ¥ = 1
2, œµŒ≥ = 3
5 and
y = dy/dt = 0 when t = 0 (solid line) and y = 10‚àí4, dy/dt = 0 when t = 0 (broken line).
15.1.2
A Chemical Oscillator
The next example that we will consider is a well-stirred system of reacting chem-
icals, known as the cubic crosscatalator, which has at its heart the cubic auto-
catalytic step that we studied in Chapter 13. The reaction scheme is
P ‚ÜíA rate k0p, precursor decay,
(15.4)
P + C ‚ÜíA + C rate k1pc, catalysis of precursor decay,
(15.5)
A ‚ÜíB rate kua, uncatalyzed conversion,
(15.6)
A + 2B ‚Üí3B rate k1ab2, cubic autocatalysis,
(15.7)
B ‚ÜíC rate k2b, autocatalyst decay,
(15.8)
C ‚ÜíD rate k3c, catalyst decay.
(15.9)
In addition to the reactant, A, and autocatalyst, B, there is a precursor, P, which
decays to produce A, and a catalyst, C, which accelerates the decay of the pre-
cursor, and is produced by the decay of the autocatalyst. The action of C both at

15.1 THREE SIMPLE CHAOTIC SYSTEMS
451
the start of the reaction cascade, catalyzing the decay of the precursor, and as a
product at the end of the sequence P ‚ÜíA ‚ÜíB ‚ÜíC is the essential ingredient that
leads to complex behaviour. The reactant A also decays spontaneously to produce
B, and C is itself unstable, decaying to the inert product D.
Under the assumption that the precursor, P, is in large excess and decays slowly,
we can derive the dimensionless governing equations (see Exercise 15.1)
ÀôŒ±
=
Œ∫ (1 + Œ∑Œ≥) ‚àíŒ±Œ≤2 ‚àíœµŒ±,
ÀôŒ≤
=
Œ±Œ≤2 ‚àíŒ≤ + œµŒ±,
(15.10)
ÀôŒ≥
=
Œ≤ ‚àíœáŒ≥,
where Œ±, Œ≤ and Œ≥ are the dimensionless concentrations of A, B and C, and Œ∫,
Œ∑, œµ and œá are dimensionless constants.
Although this system possesses a very
complicated set of diÔ¨Äerent types of behavior (see Petrov, Scott and Showalter
1992), we will focus on a single chaotic solution. Figure 15.4 shows the solution
when Œ∫ = 0.71, Œ∑ = 0.054, œµ = 0.005 and œá = 0.25. The single equilibrium point
has a one-dimensional stable manifold and a two-dimensional unstable manifold
associated with complex eigenvalue. The solution is continually attracted towards
the equilibrium point close to the stable manifold, and then spirals away close to
the unstable manifold before the process begins again.
15.1.3
The Lorenz Equations
Our third example is the system of three autonomous diÔ¨Äerential equations,
Àôx
=
‚àí8
3x + yz,
Àôy
=
‚àí10(y ‚àíz),
(15.11)
Àôz
=
‚àíxy + 28y ‚àíz,
known as the Lorenz equations. Equations (15.11) were derived by Lorenz (1963)
as the leading order approximation to the behaviour of an idealized model of the
Earth‚Äôs atmosphere. To claim that these simple equations model the weather is
perhaps going a little too far, but they certainly have very interesting dynamics.
There are three equilibrium points, one at the origin and two at x = 27,
y = z = ¬±6
‚àö
2, each of which is unstable.
The two equilibrium points away
from the origin each have a two-dimensional unstable manifold, associated with
complex eigenvalues, and hence oscillatory behaviour, and a one-dimensional sta-
ble manifold. The system is rather like two copies of the cubic crosscatalator system
interacting with each other. Typical solutions bounce back and forth between the
two equilibrium points away from the origin, continually being attracted towards
an equilibrium point along a trajectory close to the stable manifold, and then spi-
ralling away close to the unstable manifold, as shown in Figure 15.5 for the solution
with x = y = z = 1 when t = 0.
The best way to get a feel for the dynamics of the Lorenz equations, and an
indication of their iconic status as the Ô¨Årst chaotic system to be discovered, is to
type lorenz in MATLAB, which runs an animated simulation.

452
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.4. The solution of the cubic crosscatalator equations, (15.10), with Œ∫ = 0.71,
Œ∑ = 0.054, œµ = 0.005, œá = 0.25 and Œ±(0) = Œ≤(0) = Œ≥(0) = 0.
15.2
Mappings
Although this book is about diÔ¨Äerential equations, we will see later that it is often
helpful to relate the solutions of diÔ¨Äerential equations to those of mappings, some-
times known as diÔ¨Äerence equations. Before we give some basic deÔ¨Ånitions, let‚Äôs
consider three examples of nonlinear mappings, which illustrate how complicated
the solutions of these deceptively simple systems can be.
Example: A shift map
The shift map is deÔ¨Åned by
x ‚Üíax|1,
(15.12)
which maps [0, 1) to itself, where b|c means the remainder when b is divided by
c. Let‚Äôs focus on the case a = 10. The shift map is then equivalent to shifting
the decimal point one place to the right and throwing away the integer part. For
example,
1
8 = 0.125 maps to 0.25, which maps to 0.5, which maps to zero, an
equilibrium, or Ô¨Åxed point of the map. Another equilibrium point of the map
is
1
9 = 0.11111 . . . . The map also has many periodic solutions, for example,
1
11 = 0.09090909 . . . ‚Üí10
11 = 0.909090 . . . ‚Üí
1
11 ‚Üí. . . , which has period 2.

15.2 MAPPINGS
453
Fig. 15.5. The solution of the Lorenz equations, (15.11), with x(0) = y(0) = z(0) = 1.
Every rational number has either a Ô¨Ånite decimal expansion, and therefore is
eventually mapped to zero, for example 1
8, or has a repeated decimal expansion,
in which case it is part of a periodic solution, for example
1
11. Irrational numbers,
for example
‚àö
2, œÄ and e, have decimal expansions that do not repeat themselves
and have no pattern, for example
‚àö
2 ‚àí1 ‚âà0.4142135 . . . ‚Üí0.1421356 . . . ‚Üí
0.4213562 ‚Üí0.2135623 ‚Üí. . . . In addition, two irrational numbers may be arbi-
trarily close to each other, but the corresponding solutions eventually diverge. For
example, consider x = x1 =
‚àö
2 ‚àí1 and x = x2 =
‚àö
2 ‚àí1 + 10‚àí5œÄ ‚âà0.4142449 . . . .
These diÔ¨Äer by just 10‚àí5œÄ, but after four iterations of (15.12), x1 maps to 0.135 . . .
whilst x2 maps to 0.449 . . . . This is a simple example of sensitive dependence on
initial conditions. Each solution initially at an irrational number therefore satisÔ¨Åes
our conditions to be called a chaotic solution, since they behave in an apparently
random manner, and initially close solutions diverge. The chaotic solutions, which
correspond to the irrational numbers, are dense in [0, 1), as are the periodic solu-
tions, which correspond to rational numbers with no Ô¨Ånite decimal representation.

454
AN INTRODUCTION TO CHAOTIC SYSTEMS
Example: The logistic map
Consider the logistic map
xn+1 = rxn(1 ‚àíxn),
(15.13)
where r is a constant, with 0 < r ‚©Ω4 and n an integer. The logistic map is a simple
model for the growth of the population of a single species. Starting from an initial
population, x0, (15.13) gives a measure of the population in subsequent generations.
The state xn = 0 represents the complete absence of the species, and for xn ‚â™1,
xn+1 ‚àºrxn, so that the next generation grows by a factor of r. When xn is not
small, the factor (1‚àíxn), which models the eÔ¨Äect of overcrowding and competition
for resources, is no longer close to unity, and the full, nonlinear equation, (15.13),
determines the size of the next generation. Note that if 0 ‚©Ωx0 ‚©Ω1, then 0 ‚©Ωxn ‚©Ω1
for n ‚©æ0. The interval [0, 1] is also the physically meaningful range for this map.
Let‚Äôs begin by trying to Ô¨Ånd the Ô¨Åxed points of (15.13). These satisfy xn+1 = xn,
and hence xn = rxn(1‚àíxn). The Ô¨Åxed points are therefore x = 0 and x = (r‚àí1)/r.
The nontrivial Ô¨Åxed point lies in the meaningful range only if r > 1. Let‚Äôs now
determine whether there are any solutions of period 2, or 2-cycles. These satisfy
xn+1 = rxn(1 ‚àíxn),
xn+2 = xn = rxn+1(1 ‚àíxn+1).
By eliminating xn+1, we obtain the equation for xn,
xn

xn ‚àír ‚àí1
r
 
r2x2
n ‚àír(1 + r)xn + (1 + r)

= 0.
This equation is easy to factorize, since we know that it must also be satisÔ¨Åed by
the two Ô¨Åxed points. The discriminant of the quadratic factor is r2(r2 ‚àí2r ‚àí3),
which is positive provided r > 3. This means there are points of period 2 for r > 3.
In fact, it can be shown that the nontrivial Ô¨Åxed point is stable for r < 3, but loses
stability in a bifurcation‚Ä† at r = 3, where the points of period 2 emerge and are
stable. Similarly, as r increases, the points of period 2 eventually lose stability, and
a stable 4-cycle emerges. This process is known as period doubling, and, as r
increases, eventually leads to chaotic solutions. We will not go into the details of
this process, since we want to concentrate on maps relevant to diÔ¨Äerential equations.
The interested reader is referred to Arrowsmith and Place (1990). Figure 15.6 shows
the period doubling process as a bifurcation diagram. This was produced using the
MATLAB script
‚Ä† a Ô¨Çip bifurcation

15.2 MAPPINGS
455
'
&
$
%
for r = 0:0.005:4
x = rand(1);
for j = 1:100
x = r*x*(1-x);
end
xout = [];
for j = 1:400
x = r*x*(1-x); xout = [xout x];
end
plot(r*ones(size(xout)),xout,‚Äô.‚Äô,‚ÄôMarkerSize‚Äô,3)
axis([0 4 0 1]), hold on, pause(0.01)
end
This iterates the map 100 times, starting from randomly generated initial conditions
(x = rand(1)), and then saves the next 400 iterates of the map before plotting
them. Figure 15.7 shows 100 iterates of the logistic map with r = 4 for two initial
conditions separated by just 10‚àí16. The apparently random nature of the solution
can be seen, as can the fact that these initially very close solutions have completely
diverged after about 50 iterations of the map.
Fig. 15.6. The bifurcation diagram for the logistic map.

456
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.7. A sequence of 100 iterates of the logistic map with r = 4 and x0 = 0.1 and
x0 = 0.1 + 10‚àí16.
We will also discuss another phenomenon that can occur in the logistic map,
known as intermittency.
A system that behaves in an intermittent manner
exhibits bursts of chaotic behaviour interspersed with simpler, more regular be-
haviour.
Many other systems, both maps and Ô¨Çows, can exhibit intermittency,
which often begins to occur as a parameter is changed, and is a prelude to full
chaos. For example, the Ô¨Çow of water down a pipe is smooth and steady, or lami-
nar, at suÔ¨Éciently low Ô¨Çow rates.‚Ä† At suÔ¨Éciently high Ô¨Çow rates, the Ô¨Çow becomes
unsteady and chaotic, or turbulent.
However, at intermediate Ô¨Çow rates, tur-
bulence can appear in intermittent, spatially localized bursts (see, for example,
Mathieu and Scott, 2000).
Consider the Ô¨Åfth iterate of the logistic map, f (5)(x), which is shown in Fig-
ure 15.8 for r = 3.7.
Note that there are four points where the curve is close
to the straight line that represents the identity mapping, f(x) = x, but does not
touch it.
We will focus on the point close to x = 0.65, the image of which is
shown in Figure 15.8, and look for the nearby values of x and r at which f (5)(x)
actually touches the straight line. This is then a Ô¨Åxed point of f (5)(x), and hence
part of a periodic solution of the logistic map of period 5. It is straightforward
‚Ä† Strictly speaking, at suÔ¨Éciently low Reynolds numbers.

15.2 MAPPINGS
457
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
f5(x)
Fig. 15.8. The Ô¨Åfth iterate of the logistic map for r = 3.7. Also shown is the image of the
point x = 0.65.
to show, using MATLAB, that g(x) = f (5)(x) ‚àíx and its derivative are zero when
r = rc ‚âà3.73817237526634 and x = xc ‚âà0.66045050397608. Figure 15.9 shows
f (5)(x) in the neighbourhood of this point when r = rc and for two values of r
slightly above and below rc. We can see that for r > rc there are two Ô¨Åxed points,
one stable, one unstable, whilst for r < rc, locally there are no Ô¨Åxed points. Clearly,
r = rc is a bifurcation point of the map f (5), analogous to the saddle‚Äìnode bifurca-
tion that we discussed in Section 13.3.1, and is known as a tangent bifurcation.
For values of r slightly greater that rc, points initially close to xc are attracted
to the Ô¨Åxed point, and therefore the solution of the logistic map is attracted to
a stable periodic solution of period 5. For r slightly less than rc, we can see in
Figure 15.9 that f (5)(x) is very close to the straight line, and that iterates of the
map can be trapped close to x = xc before moving away, and also close to the
other points where the curve is close to the straight line. The solution therefore
exhibits almost steady behaviour before moving away and behaving irregularly, as
shown in Figure 15.10. This is intermittency. The equivalent solution of the logistic
map displays almost periodic behaviour interrupted by chaotic bursts, which is also
shown in Figure 15.10. For further examples of intermittency, see Guckenheimer
and Holmes (1983).

458
AN INTRODUCTION TO CHAOTIC SYSTEMS
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
x
f5(x)
(a) r = rc ‚âà 3.73817237526634.
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
x
f5(x)
(b) r = rc ‚àí 1.5 √ó 10‚àí3
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
x
f5(x)
(c) r = rc + 1.5 √ó 10‚àí3
Fig. 15.9. The function f 5(x) at rc and rc ¬± 1.5 √ó 10‚àí3. The iterates of a point in this
neighbourhood are also shown.

15.2 MAPPINGS
459
1000
2000
3000
4000
5000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
xn+1 = f(xn) = rx(1‚àíx)
n
xn
0
200
400
600
800
1000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
xn+1 = f(5)(xn)
n
xn
Fig. 15.10. The action of the logistic map and its Ô¨Åfth iterate with x1 = xc and r =
rc ‚àí1 √ó 10‚àí6.
Example: The H¬¥enon map
The H¬¥enon map is a nonlinear, two-dimensional map, which is deÔ¨Åned by
xn+1 = xn cos Œ∏ ‚àíyn sin Œ∏ + x2
n sin Œ∏,
yn+1 = xn sin Œ∏ + yn cos Œ∏ ‚àíx2
n cos Œ∏.
This is the most simple area-preserving, quadratically nonlinear map with linear
part a rotation through the angle Œ∏ about the origin. We can see that it preserves
areas, since the determinant of its Jacobian, the deÔ¨Ånition of which we will discuss
in more detail below, is

‚àÇxn+1
‚àÇxn
‚àÇxn+1
‚àÇyn
‚àÇyn+1
‚àÇxn
‚àÇyn+1
‚àÇyn

=

cos Œ∏ + 2xn sin Œ∏
‚àísin Œ∏
sin Œ∏ ‚àí2xn cos Œ∏
cos Œ∏
 = 1.
In Figure 15.11 we show the results of an iteration using the MATLAB script

460
AN INTRODUCTION TO CHAOTIC SYSTEMS
'
&
$
%
cosa=0.24; sina=sqrt(1-cosa^2);
ii=0;
for st=-0.5:0.05:0.5
x=st; y=st;
for its=1:1000
ii=ii+1;
xn=cosa*x-sina*y+x^2*sina;
yn=sina*x+cosa*y-x^2*cosa;
x=xn; y=yn; po(ii)=x+i*y;
if ((abs(x)>10)|(abs(y)>10))
break
end
end
end
plot(po,‚Äô.‚Äô,‚ÄôMarkerSize‚Äô,2)
axis equal, axis([-1 1 -1 1])
Note that we have used MATLAB‚Äôs ability to plot complex numbers, and that |
is the logical or operator. In addition, it is necessary to conÔ¨Åne the iterates by
stopping the calculation once the points have left the domain of interest.
Fig. 15.11. Iterates of the H¬¥enon map for cos Œ∏ = 0.24 and cos Œ∏ = 0.3.
It is clear that this apparently simple map gives rise to extremely complex be-
haviour. The regions where there are concentric sets of points, or islands, owe their
appearance to the existence of periodic solutions. In particular, for cos Œ∏ = 0.24
we see evidence of a period 5 structure. In fact, there is a 5-cycle consisting of the
Ô¨Åve points at the centre of the chain of Ô¨Åve islands, separated by Ô¨Åve further un-
stable equilibrium points. Chaotic solutions exist near these points. If we consider

15.2 MAPPINGS
461
other values of cos Œ∏, other periodic structures become evident (for instance, when
cos Œ∏ = 0.34 we Ô¨Ånd a structure of period 11). We also note that if we consider
certain regions in more detail, we can see structures with even longer periods (see,
for example, Figure 15.12, which we obtained from Figure 15.11 using MATLAB‚Äôs
ability to zoom in on a small region of an existing Ô¨Ågure).
Fig. 15.12. Iterates of the H¬¥enon map for cos Œ∏ = 0.24 in the neighbourhood of one of the
hyperbolic Ô¨Åxed points.
15.2.1
Fixed and Periodic Points of Maps
Consider the map
xn+1 = g(xn),
(15.14)
where g : Rn ‚ÜíRn is a diÔ¨Äeomorphism; that is g is a bijection and is dif-
ferentiable with a diÔ¨Äerentiable inverse‚Ä†. A Ô¨Åxed point, ¬Øx, of the map satisÔ¨Åes
¬Øx = g(¬Øx). A point of period k, x‚àó, satisÔ¨Åes x‚àó= gk(x‚àó), and x‚àóÃ∏= gm(x‚àó) for
all positive integer m < k.
‚Ä† Note that, of the maps that we consider in this section, only the H¬¥enon map is a diÔ¨Äeomorphism,
since the logistic, tent and horseshoe maps are not bijective and have discontinuous derivatives.

462
AN INTRODUCTION TO CHAOTIC SYSTEMS
Let‚Äôs now consider the linearization of (15.14) about a Ô¨Åxed point, ¬Øx. If we let
xn = ¬Øx + ÀÜxn we Ô¨Ånd that, at leading order,
ÀÜxn+1 = Dg(¬Øx)ÀÜxn,
(15.15)
where Dg(¬Øx) is the Jacobian matrix associated with g at x = ¬Øx. For one-dimensional
maps, the solution of xn+1 = g‚Ä≤(0)xn is xn = (g‚Ä≤(0))n x0, and hence the stability
of x = 0 depends upon whether |g‚Ä≤(0)| is greater than or less than unity. Similarly,
(15.15) has solution
ÀÜxn = (Dg(¬Øx))n x0,
(15.16)
and the stability of the Ô¨Åxed point depends upon the size of the moduli of the n
eigenvalues. We say that the equilibrium point is hyperbolic if none of the eigen-
values of its Jacobian have modulus unity, and that it is nonhyperbolic otherwise.
In the same way as we found for diÔ¨Äerential systems, in the neighbourhood of an
equilibrium point of a map, there exist three invariant manifolds.
(i) The local stable manifold, œâs
loc, of dimension s, is spanned by the eigen-
vectors of Dg(¬Øx) whose eigenvalues have modulus less than unity.
(ii) The local unstable manifold, œâu
loc, of dimension u, is spanned by the
eigenvectors of Dg(¬Øx) whose eigenvalues have modulus greater than unity.
(iii) The local centre manifold, œâc
loc, of dimension c, is spanned by the eigen-
vectors of Dg(¬Øx) whose eigenvalues have modulus equal to unity.
These local manifolds exist as global manifolds when we consider the full, nonlinear
system, just as they do for systems of diÔ¨Äerential equations.
Theorem 15.1 Consider the diÔ¨Äeomorphism (15.14).
(i) The stable manifolds of diÔ¨Äerent equilibrium points cannot intersect.
(ii) The unstable manifolds of diÔ¨Äerent equilibrium points cannot intersect.
(iii) The stable manifold of an equilibrium point cannot intersect itself.
(iv) The unstable manifold of an equilibrium point cannot intersect itself.
Proof Recall that every point x ‚ààRn has a unique image and preimage, since g is
a diÔ¨Äeomorphism.
(i) Since a point in the stable manifold of an equilibrium point, ¬Øx, must asymp-
tote to ¬Øx as n ‚Üí‚àû, it cannot also asymptote to a diÔ¨Äerent equilibrium
point as n ‚Üí‚àû, which proves (i).
(ii) As for case (i) but with n ‚Üí‚àí‚àû.
(iii) Consider a point, x‚àó, that is mapped to a point of intersection of the stable
manifold with itself, g(x‚àó). Points on the stable manifold in the neighbour-
hood of x‚àómust map to points on the stable manifold in the neighbourhood
of g(x‚àó). Since g is continuous, this is not possible.
(iv) As for case (iii), but on the unstable manifold.

15.2 MAPPINGS
463
Although this theorem eliminates several possibilities, it is possible for a stable
manifold to intersect an unstable manifold, either of the same equilibrium point or
a diÔ¨Äerent one. This will prove to be crucial later.
15.2.2
Tents and Horseshoes
Before we return to consider systems of diÔ¨Äerential equations, we will examine
two more examples of maps, which will prove to be of direct relevance later.
Example: A tent map
A tent map is a map x ‚Üíf(x) where f : R ‚ÜíR and
f(x) =

sx
for x ‚©Ω1
2,
s(1 ‚àíx)
for x ‚©æ1
2.
We will concentrate on the case s = 3, when the function f is as shown in Fig-
ure 15.13. Note that f(x) = 1 when x = 1
3 or x = 2
3 and that the only equilibrium
Fig. 15.13. The function f(x) when s = 3.
point of the map is zero. Let‚Äôs now consider where this tent map sends various sets
of points.
(i) If x < 0, x ‚Üí3x < x. Clearly, f n(x) ‚Üí‚àí‚àûas n ‚Üí‚àûfor x ‚àà(‚àí‚àû, 0).
(ii) If x > 1, x ‚Üí3(1 ‚àíx) < 0. After one iteration, all points with x > 1 are
therefore mapped to a point with x < 0, and case (i) applies for subsequent
iterations, with f n(x) ‚Üí‚àí‚àûas n ‚Üí‚àûfor x ‚àà(1, ‚àû).

464
AN INTRODUCTION TO CHAOTIC SYSTEMS
(iii) If x ‚àà
 1
3, 2
3

, x ‚Üíf(x) > 1. After one iteration, all points with 1
3 < x <
2
3 are therefore mapped to a point with x > 1, and case (ii) applies for
subsequent iterations, with f n(x) ‚Üí‚àí‚àûas n ‚Üí‚àû.
(iv) Now consider points with x ‚àà[0, 1
3] ‚à™[ 2
3, 1], which are mapped to [0, 1].
Only points in the set [0, 1
9] ‚à™[ 2
9, 3
9] ‚à™[ 6
9, 7
9] ‚à™[ 8
9, 1] have images in the set
[0, 1
3] ‚à™[ 2
3, 1], so the remaining points have f n(x) ‚Üí‚àí‚àûas n ‚Üí‚àû.
If we continue this process, we can see that the set of points, K, that are not expelled
from [0, 1] as n ‚Üí‚àûcan be constructed in an iterative way by deleting the middle
third from K0 = [0, 1] to leave K1 = [0, 1
3] ‚à™[ 2
3, 1], then deleting the middle third
from each of the remaining intervals to leave K2 = [0, 1
9] ‚à™[ 2
9, 3
9] ‚à™[ 6
9, 7
9] ‚à™[ 8
9, 1],
and so on, with Kn the union of the 2n closed subintervals [r/3n, (r + 1)/3n], each
of which has length 3‚àín. The index r takes the values in the set Rn, which can be
generated iteratively using
R0 = {0} ,
Rn = {Rn‚àí1, 3n ‚àí1 ‚àíRn‚àí1}
for n ‚©æ1.
We then have
K =
‚àû
?
n=0
Kn,
which is known as Cantor‚Äôs middle-third set and was Ô¨Årst constructed by Cantor
in 1883 as an example of an inÔ¨Ånite, completely disconnected set. All points
initially in K remain in K as n ‚Üí‚àû, so that K is a positively invariant set for the
tent map with s = 3. Note that the length of Kn is 2n/3n, which tends to zero as
n ‚Üí‚àû, and hence the length of K is zero.
A more convenient deÔ¨Ånition of K is provided by looking at the numbers in
[0, 1] expressed in ternary, or base three form. We can write all points in K as
x = 0.d1d2d3 . . . = d1/3+d2/32 +d3/33 +¬∑ ¬∑ ¬∑ , with di = 0 or 2 for n = 1, 2, . . . . To
see this, note that each point in K1 has ternary form 0.0x2x3 . . . if it lies in [0, 1
3] =
[0, 0.0222 . . . ] and 0.2x2x3 . . . if it lies in [ 2
3, 1] = [0.2, 0.222 . . . ]. Similarly, each
x in K2 has ternary form 0.00x3x4 . . . , 0.02x3x4 . . . , 0.20x3x4 . . . , or 0.22x3x4 . . .
according to which of the four subintervals of K2 it lies in, and so on. We can
see that excluding the middle third of each successive subinterval is equivalent to
excluding numbers that have di = 1 for i = 1, 2, . . . .
Lemma 15.1 Cantor‚Äôs middle-third set, K, has uncountably many points.
Proof Suppose that K is countable, so that all the members of K can be ordered as
x1 = 0.x11x12x13 . . . < x2 = 0.x21x22x23 . . . , in ternary form. We can now deÔ¨Åne
y = 0.y1y2y3 . . . , with ym = 0 if xmm = 2 and ym = 2 if xmm = 0. Then 0 < y < 1,
y ‚ààK and y Ã∏= xn for all n ‚Äì a contradiction. The elements of K cannot therefore
be ordered, and must be uncountable.‚Ä†
‚Ä† This is just a variation of Cantor‚Äôs famous diagonalization proof that the real numbers are
uncountable.

15.2 MAPPINGS
465
Nonetheless, by construction, K contains no subinterval, however small, of [0, 1],
and [0, 1] contains inÔ¨Ånitely many subintervals that do not intersect K.
Other Cantor sets can be constructed by successively removing diÔ¨Äerent parts
of [0, 1]. An invariant set that is also a Cantor set is, as we shall see, typical of a
chaotic system.
Example: The Smale horseshoe map
Consider a two-dimensional map from the unit square,
D =

(x, y) ‚ààR2 | 0 ‚©Ωx ‚©Ω1, 0 ‚©Ωy ‚©Ω1

,
to itself, f : D ‚ÜíD. The function f contracts the square in the horizontal direction
and expands it in the vertical direction, and then folds the resulting strip back on
itself, as shown in Figure 15.14. The map is only deÔ¨Åned on the unit square, D,
and points that are mapped out of D are discarded. This is the Smale horseshoe
map. The inverse map, which can be visualized in terms of stretching and folding
the unit square in the opposite way, is shown in Figure 15.15.
Fig. 15.14. The Smale horseshoe map.
The invariant set, Œõ, which is both positively and negatively invariant, is the
intersection of the image of D under any number of forward or backward iterations,
Œõ =
¬∑ ¬∑ ¬∑ ‚à©f ‚àí2(D) ‚à©f ‚àí1(D) ‚à©D ‚à©f(D) ‚à©f 2(D) ‚à©¬∑ ¬∑ ¬∑ .
As we can see, each application of the map removes the middle third of each re-
maining strip in each direction, so we conclude that Œõ is a two-dimensional Cantor
set given by the set of points (x, y) such that x ‚ààKx and y ‚ààKy, where Kx and
Ky are the Cantor middle-third sets in each direction.
It is now helpful to show that each point in the invariant set can be uniquely
labelled by a sequence of 0‚Äôs and 1‚Äôs. After each forward iteration of the map, we
append ak to the right of the symbol sequence, where
ak =
0
for points in the left half of D,
1
for points in the right half of D,

466
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.15. The inverse Smale horseshoe map.
Fig. 15.16. The construction of the left half of the bi-inÔ¨Ånite sequence.

15.3 THE POINCAR¬¥E RETURN MAP
467
as illustrated in Figure 15.16. After each backward iteration of the map we append
ak to the left of the symbol sequence, where
ak =
0
for points in the bottom half of D,
1
for points in the top half of D,
as shown in Figure 15.17. We can then represent any point in Œõ as the bi-inÔ¨Ånite
Fig. 15.17. The construction of the right half of the bi-inÔ¨Ånite sequence.
sequence of symbols,
a =
. . . a2a1a0.a‚àí1a‚àí2 . . . ,
as shown in Figure 15.18. The digits to the right of the decimal point reÔ¨Çect the
vertical location and to the left the horizontal location. For any point in Œõ, the
action of the horseshoe map is simply to shift the decimal point in the bi-inÔ¨Ånite
sequence one place to the right. The map is therefore equivalent to the shift map
x ‚Üí2x|1, which is known as the Bernoulli shift map. For the same reasons that
the shift map that we studied as our Ô¨Årst example had chaotic solutions, so does
the Bernoulli shift map, and hence the horseshoe map.
15.3
The Poincar¬¥e Return Map
Now that we have seen several examples of maps, we will demonstrate how the
solutions of a system of ordinary diÔ¨Äerential equations can be related to the solutions
of a map, the Poincar¬¥e map, that is easier to work with than the original system,
and then develop a technique for deciding whether this map is chaotic.
Consider the autonomous system of ordinary diÔ¨Äerential equations
dx
dt = f(x),
(15.17)
where f : Rn ‚ÜíRn and x ‚ààRn. We introduce the Ô¨Çow evolution operator or
Ô¨Çow operator, œÜt1,t0, which maps the point x0 ‚ààRn and an interval (t0, t1) ‚ààR

468
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.18. The points in Œõ, the invariant set.
to the point x1 ‚ààRn, where x1 = x(t1) and x is the solution of (15.17) subject
to x = x0 when t = t0.
That is x(t0) = x0 evolves to x(t1) = x1 in the n-
dimensional phase space during the time interval (t0, t1). Note that œÜt0,t0(x0) = x0
and œÜt2,t1 ¬∑ œÜt1,t0 ‚â°œÜt2,t0 for all t1 ‚àà(t0, t2).
In addition, since (15.17) is an
autonomous system, we can express the Ô¨Çow evolution operator in terms of the
length of the time interval alone, and we write
œÜt1,t0(x0) = œÜt1‚àít0(x0).
Consequently we have that œÜ0 is the identity transformation, œÜt ¬∑ œÜs ‚â°œÜt+s and
(œÜt)‚àí1 = œÜ‚àít, so that œÜ is a group operator (see Chapter 10).
Example: The logistic equation
Let‚Äôs construct the Ô¨Çow operator for the logistic equation,
dx
dt = x(1 ‚àíx).
This is the continuous version of the logistic map, which we studied earlier. The
logistic equation is separable, and we can therefore integrate it subject to the initial

15.3 THE POINCAR¬¥E RETURN MAP
469
condition that x = x0 when t = 0 to obtain the solution, and hence the evolution
operator,
x(t) = œÜt(x0) =
x0et
x0et ‚àíx0 + 1.
Note that solutions of the logistic equation all have x ‚Üí1 as t ‚Üí‚àû, in complete
contrast to the solutions of the logistic map. This shows that choosing to use a
continuous rather than a discrete system can have dramatic consequences for the
behaviour of the solutions.
We can now verify that œÜ0(x0) = x0 and
œÜt ¬∑ œÜs(x0) = œÜt(œÜs(x0)) =
œÜs(x0)et
œÜs(x0)et ‚àíœÜs(x0) + 1,
and, since œÜs(x0) = x0es/(x0es ‚àíx0 + 1),
œÜt ¬∑ œÜs(x0) =
x0eset
x0eset ‚àíx0es + (x0es ‚àíx0 + 1) =
x0es+t
x0es+t ‚àíx0 + 1,
which is equal to œÜs+t(x0), as expected.
Equilibrium points of (15.17) satisfy œÜt(x‚àó) = x‚àófor all t ‚ààR (or equivalently
f(x‚àó) = 0). Periodic solutions of (15.17) with period T satisfy x‚àó(t) = x‚àó(t + T)
for all t ‚ààR, and can be written in terms of the Ô¨Çow operator as œÜt+T (x‚àó) = œÜt(x‚àó)
for all t ‚ààR.
One way of obtaining a map from the system (15.17) is to sample the solution
with period œÑ, so that œÜn(x0) = x(t0 + nœÑ) for n ‚ààZ. This allows us to track
the trajectory of particles in a stroboscopic way. A more useful map, which we
can use to analyze the behaviour of integral paths close to a periodic solution, is
the Poincar¬¥e return map. Let Œ≥ be a trajectory of (15.17), and consider the
intersections of Œ≥ with Œ£ ‚äÇRn such that
(i) Œ£ has dimension n ‚àí1,
(ii) Œ£ is transverse (nowhere parallel) to the integral paths,
(iii) all solutions in the neighbourhood of Œ≥ pass through Œ£.
If Œ≥ intersects Œ£ at x = p, and its next intersection with Œ£ is at x = q, then
the Poincar¬¥e return map, P : Œ£ ‚ÜíŒ£, maps the point p to the point q. This is
illustrated in Figure 15.19 for n = 3 where Œ£ is a plane. Note that an equilibrium
point of the Poincar¬¥e return map corresponds to a limit cycle that intersects Œ£
once, and a periodic solution of period k corresponds to a limit cycle that intersects
Œ£ k times.
Example
Let‚Äôs try to construct the Poincar¬¥e return map for (15.17) with n = 2 when
f(x) =
 4x + 4y ‚àíx(x2 + y2)
4y ‚àí4x ‚àíy(x2 + y2)

,

470
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.19. The deÔ¨Ånition of the Poincar¬¥e return map.
and Œ£ is the positive x axis. It is easier to write this system in terms of plane polar
coordinates, and, using (9.22) and (9.23), we Ô¨Ånd that Àôr = r(4 ‚àír2) and ÀôŒ∏ = ‚àí4.
These expressions can be integrated with initial conditions in Œ£, the positive x-axis,
so that r(0) = x0 and Œ∏(0) = 0, to obtain
r =
+
4x2
0e8t
4 ‚àíx2
0 + x2
0e8t ,
Œ∏ = ‚àí4t.
The orbit next returns to this axis when Œ∏ is an integer multiple of 2œÄ, that is when
t = œÄ/2, so that
x1 = P(x0) =
+
4x2
0e4œÄ
4 ‚àíx2
0 + x2
0e4œÄ .
The solution for x0 = 10 is shown in Figure 15.20, along with the Poincar¬¥e return
map, which rapidly asymptotes to x = 2.
Example: The Lorenz equations
Solutions of the Lorenz equations repeatedly cross the plane y = ‚àíz, which sepa-
rates the two equilibrium points at x = 27, y = z = ¬±6
‚àö
2. Indeed the equations
are symmetric about this plane, since they are unchanged by the transformation
y ‚Üí‚àíy, z ‚Üí‚àíz. We can therefore use the plane y = ‚àíz as Œ£, and hence de-
Ô¨Åne a Poincar¬¥e return map. In order to investigate this map, we must proceed
numerically. In MATLAB, we need to deÔ¨Åne an event function

15.3 THE POINCAR¬¥E RETURN MAP
471
Fig. 15.20. The solution when x0 = 10 and the corresponding Poincar¬¥e return map.




function [value, isterminal, direction] = lorenzevent(t,y)
value = y(2)+y(3); isterminal = 0; direction = 1;
This function returns the value zero when the solution intersects Œ£ (when y + z =
y(2)+y(3) = 0), but only as the solution approaches Œ£ with y + z increasing
(direction = 1), and allows the integration to continue (isterminal = 0). We
can then use the commands




options = odeset(‚ÄôEvents‚Äô,@lorenzevent);
solution = ode45(@lor,[0 500], [1 1 1], options);
This integrates the Lorenz equations, which are in the function lor
#
"
 
!
function dy = lor(t,y)
dy(1) = -8*y(1)/3+y(2)*y(3);
dy(2) = 10*(y(3)-y(2));
dy(3) = -y(2)*y(1)+28*y(2)-y(3);
dy=dy‚Äô;
for 0 ‚©Ωt ‚©Ω500, starting from x = y = z = 1 when t = 0. Note that the function
odeset allows us to create a variable options that we can pass to ode45 as an
argument, which controls its execution. In this case, we tell ode45 to detect the

472
AN INTRODUCTION TO CHAOTIC SYSTEMS
event coded for in lorenzevent. The variable‚Ä† solution then contains the points
where the solution crosses Œ£, in solution.ye, at times solution.te. The Poincar¬¥e
return map is shown in Figure 15.21. As you can see, the map is eÔ¨Äectively one-
dimensional, since the points where the solution meets Œ£ lie on a simple curve.
The dynamics are, however, apparently chaotic (see Sparrow, 1982, for a detailed
discussion of the Lorenz equations).
Fig. 15.21. A Poincar¬¥e return map for the Lorenz equations.
15.4
Homoclinic Tangles
Let‚Äôs now consider a two-dimensional Poincar¬¥e return map, with f : Œ£ ‚ÜíŒ£ and Œ£ ‚äÇ
R2, associated with a three-dimensional system, (15.17). Each limit cycle solution
of (15.17) is associated with an equilibrium point, ¬Øx, of the map. If this equilibrium
point is stable, the limit cycle is stable. Let‚Äôs assume that a particular equilibrium
point, ¬Øx of the Poincar¬¥e return map, which corresponds to a limit cycle solution
‚Ä† The variable solution, produced by ode45, is a structure, and eÔ¨Äectively contains more than
one type of data in its deÔ¨Ånition.

15.4 HOMOCLINIC TANGLES
473
of (15.17), has a one-dimensional stable manifold, œâs(¬Øx), and a one-dimensional
unstable manifold, œâu(¬Øx). As we saw earlier, it is possible for these manifolds to
intersect. As we shall see, if this intersection is transverse, the manifolds become
tangled, intersecting inÔ¨Ånitely often, as shown in Figure 15.22. The manifolds then
contain embedded horseshoe maps, and thus have chaotic solutions. We will now
discuss why this should be so.
Although we will proceed through lemmas and
theorems, our approach is fairly informal, and should not be read as a rigorous
proof.
Fig. 15.22. A homoclinic tangle.
A homoclinic point is a point x Ã∏= ¬Øx that lies in the set œâs(¬Øx) ‚à©œâu(¬Øx), the
intersection of the stable and unstable manifolds of ¬Øx. Such a point asymptotes to ¬Øx
as n ‚Üí¬±‚àû. Under successive applications of the map and its inverse, a homoclinic
point is mapped to a homoclinic orbit, a discrete set of points that is a subset of
the stable and unstable manifolds.
We will now assume that the map f is area-preserving, so that, for any set
D ‚äÇŒ£, the area of f(D) is equal to that of D. This assumption greatly simpliÔ¨Åes
the following discussion, but is not actually necessary (see Wiggins, 1988).
Lemma 15.2 If x0 is a transverse homoclinic point, then all positive and negative
iterates of x0 have the same orientation, either US or SU, as shown in Figure 15.23.
Proof
The Poincar¬¥e return map is associated with a Ô¨Çow.
Figure 15.24 shows
the trajectory through the points x0 and f(x0).
Since the stable and unstable
manifolds associated with this trajectory vary continuously, their orientations can
at most rotate and cannot Ô¨Çip during their passage from one side of Œ£ to the other.

474
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.23. US and SU orientation at a homoclinic point. Remember that the manifolds
are stable or unstable with regard to the equilibrium point ¬Øx, not x0.
x0
f (x0)
s
s
u
s
u
u
‚àë
Fig. 15.24. The stable and unstable manifolds of the limit cycle.
Lemma 15.3 If x0 is a transverse homoclinic point, then there must be another
transverse homoclinic point between x0 and f(x0). In other words, the next trans-
verse homoclinic point along say œâs after x0 cannot be f(x0).

15.4 HOMOCLINIC TANGLES
475
Proof If the orientation associated with x0 is US, then the next homoclinic point
along œâs, z, has SU orientation, and by Lemma 15.2, z Ã∏= f(x0), as shown in
Figure 15.25.
Note that this means that there are at least two transverse homoclinic orbits, one
associated with x0 and one associated with z.
Fig. 15.25. The orientation of successive homoclinic points.
Lemma 15.4 The existence of a single transverse homoclinic point ensures the
existence of an inÔ¨Ånite number of transverse homoclinic points.
Proof We consider the image of the points within the lobe L0, which is bounded
by the stable and unstable manifolds through x0 and z, as shown in Figure 15.26.
The image of L0 is the lobe L1, which is bounded by the portions of œâu and œâs
between f(x0) and f(z). Similarly if x ‚ààL0 then f j(x) ‚ààLj which is bounded
by the portions of œâu and œâs between f j(x0) and f j(z).
Since we know that
f n(x0) ‚Üí¬Øx and f n(z) ‚Üí¬Øx along œâs as n ‚Üí‚àû, the distance between these
images must decrease. However, the distance between the points along œâu must
increase. This leads to long thin lobes, bounded by short sections of œâs and long
sections of œâu. We therefore have a Ô¨Ånite area covered by an inÔ¨Ånite set of lobes
of Ô¨Ånite area equal to the area of A0 (recall that we are assuming that the map
preserves areas). Consequently, these lobes must overlap. We can assume, without
loss of generality, that they overlap between z and f(x0). There are, therefore,
two further transverse intersection points, which we label a and b. We can repeat
this argument indeÔ¨Ånitely, and conclude that there must be an inÔ¨Ånite number of
homoclinic points.
Theorem 15.2 (Smale‚ÄìBirkhoÔ¨Ä) Let f : R2 ‚ÜíR2 be a diÔ¨Äeomorphism with a
hyperbolic equilibrium point ¬Øx. If œâs(¬Øx) and œâu(¬Øx) intersect transversally at a point
other than ¬Øx, in other words, if there is a homoclinic tangle, then the map has a
horseshoe map embedded within it.
Note that, since horseshoe maps have chaotic orbits, this theorem shows that the

476
AN INTRODUCTION TO CHAOTIC SYSTEMS
f(x0)
L1
L0
b
a
z
x0
f j(z)
f j(x0)
œâu
f(z)
Fig. 15.26. The homoclinic points a and b.
existence of a homoclinic tangle implies the existence of chaotic orbits.
As we
shall see later, there is an algebraic test, Mel‚Äônikov‚Äôs method, that can be used to
determine whether a system has a transverse homoclinic point.
Proof
We will simply give an outline of the proof here. A more detailed proof,
which uses the idea of Markov partitioning, is given in Guckenheimer and Holmes
(1983). Our aim here is simply to convince you that the forward and backward
maps of the region D, which we deÔ¨Åne below, intersect.
Consider a region D that contains a transverse homoclinic point, x0, associated
with an equilibrium point, ¬Øx of a map f : R2 ‚ÜíR2, as shown in Figure 15.27. Then
(i) f k(D) contains the iterates of f k(x0) for k ‚ààZ.
(ii) For k < 0, f k(D) is stretched along the stable manifold and contracted along
the unstable manifold.
(iii) For k > 0, f k(D) is stretched along the unstable manifold and contracted
along the stable manifold.
(iv) For some backward iterate ‚àíq, f ‚àíq(D) = D ‚àíwill have a horseshoe shape
and intersect with f p(D) = D + for some forward iterate p.
(v) The map f ‚àí(p+q)(D +) = D ‚àí, which maps the region D + into a horseshoe
shaped region, D ‚àí, with overlap between D ‚àíand D +, is a horseshoe map.

15.4 HOMOCLINIC TANGLES
477
x0
œâu
D‚àí
D+
œâs
x
D
Fig. 15.27. A horseshoe map arising from the dynamics in a homoclinic tangle.
15.4.1
Mel‚Äônikov Theory
We will now describe an algebraic test with which we can determine whether a
system has a homoclinic tangle, and hence, by Theorem 15.2, chaotic solutions. We
will focus on two-dimensional Hamiltonian systems perturbed by a periodic function
of time only, although the method can also be used for perturbed non-Hamiltonian
systems (see Wiggins, 1988).
We consider a Hamiltonian system (see Section 9.3.9) with Hamiltonian H =
H(x, y) and associated diÔ¨Äerential equations
Àôx = f(x)
where f = (‚àí‚àÇH/‚àÇy, ‚àÇH/‚àÇx)T or, written in component form,
dx
dt = ‚àí‚àÇH
‚àÇy ,
dy
dt = ‚àÇH
‚àÇx .
(15.18)
We will consider this system under the inÔ¨Çuence of a small perturbation that is a
function of space and periodic in time, in the form
Àôx = f(x) + œµg(x, t),
(15.19)
with g(x, t) = g(x, t+T) for some period T > 0, and œµ ‚â™1. We start by considering
the unperturbed system.

478
AN INTRODUCTION TO CHAOTIC SYSTEMS
15.4.2
Unperturbed System (œµ = 0)
As we saw in Section 9.3.9, this system is area-preserving, and its equilibrium
points are either nonlinear centres or saddles. We suppose that ¬Øx0 is a saddle point
and that there is a homoclinic orbit x0(t) such that x0(t) ‚Üí¬Øx0 as t ‚Üí¬±‚àû. The
interior of the homoclinic orbit must contain concentric limit cycles surrounding a
centre at x‚àó, as shown in Figure 15.28.
x = x0(t)
x‚àó
x0
Fig. 15.28. A homoclinic orbit of the unperturbed Hamiltonian system.
15.4.3
Perturbed System (0 < œµ ‚â™1)
We can think of the perturbed system as autonomous in the three-dimensional
(x, y, t)-phase space. We can deÔ¨Åne an associated map by stroboscopically sampling
the Ô¨Çow at t = nT for n ‚ààZ. Since g(x, t) = g(x, t + nT), this is equivalent to a
Poincar¬¥e return map with Œ£ the plane t = 0. Note that the equilibrium point, ¬Øx0,
of the unperturbed system of diÔ¨Äerential equations is also an equilibrium point of
the associated map, and that the stable and unstable manifolds of ¬Øx0 are the same
for the unperturbed system of diÔ¨Äerential equations and the associated map.
We assume that the inÔ¨Çuence of the perturbation is to modify the equilibrium
point of the map to the point xœµ, with, since œµ ‚â™1, |xœµ‚àíx0| ‚â™1. For the perturbed
map, the stable and unstable manifolds of the saddle point do not necessarily
connect smoothly, as shown in Figure 15.29.
We would now like to Ô¨Ånd some
way of distinguishing between the two cases ‚Äì either disjoint or tangled manifolds.
Let t0 denote the time at which we wish to consider the fate of the solutions. Since
the unperturbed system is autonomous, its orbits are invariant under arbitrary
transformations in time, so that the orbits x0(t) and x0(t ‚àít0) are the same. This
is not true for the solutions of the perturbed system. In Figure 15.30, we show the
orbit x0(t) as a dashed line and the stable and unstable manifolds of the saddle
point of the perturbed system as solid lines. We will now introduce the idea of the
distance between points on the stable and unstable manifolds of the saddle point.
The idea is to Ô¨Ånd an expression for this distance and determine whether it can be

15.4 HOMOCLINIC TANGLES
479
Fig. 15.29. The perturbed stable and unstable manifolds of the associated map.
zero. We can do this by deÔ¨Åning the Mel‚Äônikov function, which then provides an
algebraic suÔ¨Écient condition for the existence of transverse homoclinic intersections
and hence chaotic dynamics.
Fig. 15.30. The perturbed (solid lines) and unperturbed (broken lines) stable and unstable
manifolds of the saddle point.
The Mel‚Äônikov function, D(t, t0), is proportional to the component of the
distance between corresponding points on the stable and unstable manifolds in the
direction normal to the unperturbed homoclinic orbit, and is given by
D(t, t0) = N(t, t0) ¬∑ d(t, t0).
Here N(t, t0) is a normal to the unperturbed orbit and d(t, t0) connects correspond-
ing points on the stable and unstable manifolds. If D(t, t0) has simple zeros, then
there must be transverse homoclinic intersections, and we can conclude that the
system has chaotic solutions.
Firstly, we construct the normal to the unperturbed homoclinic orbit, x0(t). The
tangent to the orbit is f, and hence N(t, t0) ¬∑ f(x0(t ‚àít0)) = 0. In component form,

480
AN INTRODUCTION TO CHAOTIC SYSTEMS
N1f1 + N2f2 = 0, where f = (f1, f2), so we can take
N = (‚àíf2(x0(t ‚àít0)), f1(x0(t ‚àít0))).
Note that N is not a unit normal. The vector displacement between correspond-
ing points on orbits xs(t) and xu(t) in the stable and unstable manifolds of xœµ is
d(t, t0) = xs(t, t0) ‚àíxu(t, t0), so the Mel‚Äônikov function is
D(t, t0) = N(t, t0) ¬∑ d(t, t0) = ‚àíf2d1 + f1d2,
where d(t, t0) = (d1(t, t0), d2(t, t0))T. For notational convenience, we now introduce
the binary operation ‚àßsuch that u ‚àßv = u1v2 ‚àív1u2, so that D ‚â°f ‚àßd. We can
now use perturbation theory to obtain an expression for D(t, t0).
We assume that points on the orbits associated with xœµ remain close to points
on the homoclinic orbit x0(t ‚àít0), so that we can write their locations as a posi-
tion on the homoclinic orbit plus a small perturbation proportional to œµ. We also
introduce the superscript s,u so that we can discuss the stable and unstable cases
simultaneously, writing
xs,u(t, t0) ‚àºx0(t ‚àít0) + œµxs,u
1 (t, t0),
(15.20)
and hence
d(t, t0) ‚àºœµ (xs
1(t, t0) ‚àíxu
1(t, t0))
and
D ‚àºœµ (f ‚àßxs
1 ‚àíf ‚àßxu
1) .
It is now convenient to introduce two subsidiary Mel‚Äônikov functions, D s(t, t0) and
D u(t, t0), such that
D ‚àºœµD s(t, t0) ‚àíœµD u(t, t0),
where D s,u(t, t0) = f ‚àßxs,u
1 . We now substitute (15.20) into the perturbed system,
(15.19), and obtain
Àôx0 + œµÀôxs,u
1
= f(x0 + œµxs,u
1 ) + œµg(x0 + œµxs,u
1 , t)
= f(x0) + œµDf(x0)xs,u
1
+ œµg(x0, t) + O(œµ2),
where Df(x0) is the Jacobian of the unperturbed system. This equation is au-
tomatically satisÔ¨Åed at leading order, since x0 is a solution of the unperturbed
equation, whilst at O(œµ),
Àôxs,u
1 (t, t0) = Df(x0(t ‚àít0))xs,u
1 (t, t0) + g(x0(t ‚àít0), t).
(15.21)
Now, diÔ¨Äerentiating the functions D s,u(t, t0), we obtain
ÀôD s,u(t, t0) = Àôf ‚àßxs,u
1
+ f ‚àßÀôxs,u
1 ,
since there is a product rule associated with the ‚àßoperator. However, we note that
Àôf = d
dtf(x0(t ‚àít0)) = Df(x0(t ‚àít0))Àôx0(t ‚àít0),

15.4 HOMOCLINIC TANGLES
481
which implies that Àôf = Dff, and hence
ÀôD s,u(t, t0) = (Dff) ‚àßxs,u
1
+ f ‚àßÀôxs,u
1 .
Substituting from (15.21) into this equation gives
ÀôD s,u(t, t0) = (Dff) ‚àßxs,u
1
+ f ‚àßDfxs,u
1
+ f ‚àßg.
If we now use the identity
Dff ‚àßx + f ‚àßDfx ‚â°(‚àá¬∑ f)f ‚àßx,
and note that ‚àá¬∑ f = 0 since the unperturbed system is Hamiltonian, we Ô¨Ånd that
ÀôD s,u(t, t0) = f ‚àßg.
Considering the unstable manifold Ô¨Årst, we integrate from ‚àí‚àûto t0 to obtain
 t0
‚àí‚àû
ÀôD u(t, t0) dt =
 t0
‚àí‚àû
f ‚àßg dt,
which gives
D u(t0, t0) ‚àíD u(‚àí‚àû, t0) =
 t0
‚àí‚àû
f ‚àßg dt.
(15.22)
Now consider the stable case integrated from t0 to +‚àû, which gives
 ‚àû
t0
ÀôD s(t, t0) dt =
 ‚àû
t0
f ‚àßg dt,
and hence
‚àíD s(t0, t0) + D s(‚àû, t0) =
 ‚àû
t0
f ‚àßg dt.
(15.23)
We note that as t ‚Üí‚àí‚àûalong the unstable orbit and as t ‚Üí‚àûalong the stable
orbit, both solutions tend to xœµ, so that D u(‚àí‚àû, t0) = D s(‚àû, t0). Adding (15.22)
and (15.23) and changing notation slightly so that we replace (t0, t0) with (t0), we
have
D(t0) ‚àºœµ(D s(t0) ‚àíD u(t0)) = ‚àíœµ
 ‚àû
‚àí‚àû
f ‚àßg dt
= ‚àíœµ
 ‚àû
‚àí‚àû
!
f(x0(t ‚àít0)) ‚àßg(x0(t ‚àít0), t)
"
dt.
Note that
(i) If D(t0) has simple zeros for suÔ¨Éciently small œµ, then œâu and œâs intersect
transversally, and hence by Theorem 15.2, there are chaotic solutions.
(ii) If D(t0) is bounded away from zero, there are no transverse homoclinic
intersections.
(iii) If we can Ô¨Ånd one zero of a Mel‚Äônikov function, the dynamics of the system
ensure that there will be inÔ¨Ånitely many zeros.

482
AN INTRODUCTION TO CHAOTIC SYSTEMS
Example
Consider the perturbed Hamiltonian system given by the forced DuÔ¨Éng equation,
(15.3),
¬®x = x ‚àíx3 ‚àíœµ(Œ¥ Àôx ‚àíŒ≥ cos œât),
when œµ ‚â™1. For what values of Œ¥, Œ≥ and œâ are there chaotic solutions?
We begin by rewriting the system as
d
dt
 x
y

=

y
x ‚àíx3 ‚àíœµ(Œ¥y ‚àíŒ≥ cos œât)

.
(15.24)
The unperturbed system is
d
dt
 x
y

=

y
x ‚àíx3

.
This is a Hamiltonian system with
d
dt
 x
y

=

‚àÇH/‚àÇy
‚àí‚àÇH/‚àÇx

.
and Hamiltonian
H = 1
2y2 + 1
4x4 ‚àí1
2x2.
The unperturbed system has equilibrium points at (x, y) = (0, 0) and (x, y) =
(¬±1, 0), and it is straightforward to show that the origin is a saddle point and that
the other two equilibria are centres. The unperturbed phase portrait is shown in
Figure 15.31. We now need to determine the equation of the homoclinic orbit. This
orbit must pass through the origin, where H = 0. However, H is a constant on the
orbit, so the homoclinic orbit is given by
y2 = x2 ‚àí1
2x4.
Setting y equal to zero, we Ô¨Ånd that the homoclinic orbit also meets the x-axis
where x = ¬±
‚àö
2. We will take y(0) = 0 and x(0) =
‚àö
2. We can then solve the
diÔ¨Äerential equation for the homoclinic orbit and Ô¨Ånd that x0(t) =
‚àö
2secht. This
gives us
x0(t ‚àít0) =
‚àö
2sech(t ‚àít0),
y0(t ‚àít0) = ‚àí
‚àö
2sech(t ‚àít0) tanh(t ‚àít0).
We can now construct the Mel‚Äônikov function. The vectors f and g are
f(x0(t ‚àít0)) =

y0(t ‚àít0)
x0(t ‚àít0) ‚àí(x0(t ‚àít0))3

and
g(x0(t ‚àít0)) =

0
Œ≥ cos œât ‚àíŒ¥y0(t ‚àít0)

,
so that their wedge product is
f ‚àßg = y0(t ‚àít0)(Œ≥ cos œât ‚àíŒ¥y0(t ‚àít0)).

15.4 HOMOCLINIC TANGLES
483
Fig. 15.31. The phase portrait of (15.24) with œµ = 0.
Now, using the deÔ¨Ånition of the Mel‚Äônikov function,
D(t0) = ‚àíœµ
 ‚àû
‚àí‚àû
‚àí
‚àö
2sech(t ‚àít0) tanh(t ‚àít0)
√ó
(
cos œât + Œ¥
‚àö
2sech(t ‚àít0) tanh(t ‚àít0)
)
dt.
This can be integrated to give
D(t0) = ‚àíœµ

‚àí
‚àö
2œÄŒ≥œâsech
œÄœâ
2

sin œât0 + 4Œ¥
3

.
This has simple zeros when
Œ¥ = 3
‚àö
2œÄ
4
Œ≥œâsech
œÄœâ
2

sin œât0,
provided that
Œ¥ < 3
‚àö
2œÄ
4
Œ≥œâsech
œÄœâ
2

.
This means that there are transverse homoclinic points when this condition is sat-
isÔ¨Åed, and we can infer that the system is chaotic. Figure 15.32 shows the Poincar¬¥e
return map when œµ = 0.1, Œ¥ = 0, Œ≥ = 1 and œâ = 1
3.

484
AN INTRODUCTION TO CHAOTIC SYSTEMS
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí10
‚àí5
0
5
10
Points in the Poincar√© return map
x
dx/dt
0
100
200
300
400
500
600
700
800
900
1000
‚àí4
‚àí2
0
2
4
t
x
Successive iterations of the Poincar√© return map
Fig. 15.32. The Poincar¬¥e return map for the forced DuÔ¨Éng equation when œµ = 0.1, Œ¥ = 0,
Œ≥ = 1 and œâ = 1
3.
15.5
Quantifying Chaos: Lyapunov Exponents and the Lyapunov
Spectrum
We have now seen how to determine analytically when chaotic solutions exist for
weakly, periodically perturbed Hamiltonian systems. What can we say about other
nonlinear systems of diÔ¨Äerential equations? If we can solve such a system numeri-
cally, and it appears to have chaotic solutions, can we characterize and quantify the
chaos? In this Ô¨Ånal section, we will introduce the ideas of the maximum Lyapunov
exponent and the Lyapunov spectrum, which we can use for this purpose.
15.5.1
Lyapunov Exponents of Systems of Ordinary DiÔ¨Äerential
Equations
In a chaotic system, we have seen that neighbouring trajectories diverge.
In
fact this divergence is usually exponentially fast. If we can quantify this rate of

15.5 LYAPUNOV EXPONENTS AND THE LYAPUNOV SPECTRUM
485
divergence, we can quantify the chaotic solutions. Consider the system
dx
dt = f(x),
(15.25)
where f : Rn ‚ÜíRn. Let ¬Øx(t) be a reference trajectory, and consider a neighbour-
ing trajectory, y(t), that has y(0) = ¬Øx(0) + ‚àÜx(0). As t ‚Üí‚àû, and the trajectories
diverge, we expect that ‚àÜx(t) = y(t) ‚àí¬Øx(t) ‚àº‚àÜx(0)eŒªt, as shown in Figure 15.33.
If Œª < 0, the trajectories actually converge, whilst if Œª > 0 the trajectories diverge,
and Œª gives a measure of the rate of divergence.
Fig. 15.33. Neighbouring trajectories diverge exponentially fast.
Formally, we deÔ¨Åne the maximum Lyapunov exponent with respect to a
reference trajectory of a Ô¨Çow as
Œªmax =
lim
||‚àÜx(0)||‚Üí0
t‚Üí‚àû
1
t log ||‚àÜx(t)||
||‚àÜx(0)||,
(15.26)
where ||x|| =
# 
i x2
i is the Euclidean norm (see Section A1.2). This gives us
the basic numerical recipe for computing Œªmax from two neighbouring solutions.
Recall that the idea which is central to this deÔ¨Ånition is linearization about the
trajectory ¬Øx(t), and consequently we need to ensure that we can indeed linearize by
considering neighbouring trajectories (‚àÜx(0) ‚Üí0). In this limit, ‚àÜx(t) is governed
by the linearized equation
d‚àÜx
dt
= Df(¬Øx(t))‚àÜx.
(15.27)
Formally ‚àÜx(t) = œÜt
L(‚àÜx(0)) where œÜt
L(.) is a linear evolution operator, so that
œÜt
L(Œ±‚àÜx) = Œ±œÜt
L(‚àÜx). If Œªmax is positive, numerical integration of (15.27) will lead
to exponentially growing solutions. This can be avoided in a nonlinear system by
renormalizing. The idea is to integrate forward in time until the two trajectories
become a given distance apart, and then scale ‚àÜx, so that the calculation can
continue with a trajectory that is within a small distance of the reference orbit, as
shown in Figure 15.34. One way of doing this is to renormalize after regular time
intervals œÑ. The maximum Lyapunov exponent is then given by the average of the

486
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.34. Renormalization of diverging trajectories.
exponent calculated between each renormalization,
Œªmax = lim
n‚Üí‚àû
1
(n + 1)œÑ
n

j=0
log ||‚àÜxj(œÑ)||
||‚àÜxj(0)||,
(15.28)
provided that we deÔ¨Åne our renormalization to be
‚àÜxn(0) = Œ¥ ‚àÜxn‚àí1(œÑ)
||‚àÜxn‚àí1(œÑ)|| for n = 1, 2, . . . ,
where Œ¥ ‚â™1 and we note that ||xn(0)|| = Œ¥.
Example: The forced DuÔ¨Éng equation
Consider the forced DuÔ¨Éng equation, (15.3), with œµŒ¥ = 1
2 and œµŒ≥ = 3
5, a solution
of which is shown in Figure 15.2. We can now construct the maximum Lyapunov
exponent using the MATLAB function

15.5 LYAPUNOV EXPONENTS AND THE LYAPUNOV SPECTRUM
487
'
&
$
%
function avls = lyapunov(n,tau,del0,y,eqn)
t=0; ls=zeros(1,n); avls=ls;
del=del0*[1 1]/sqrt(2);
for i=1:n
tspan = [t t+tau];
[tout1 yout1]=ode45(eqn,tspan, y);
[tout2 yout2]=ode45(eqn,tspan, y+del);
delxe= [yout1(end,:)-yout2(end,:)];
nd=norm(delxe);
ls(i) = log(nd/del0); avls(i) = sum(ls)/i/tau;
del = del0*delxe/nd;
y = yout1(end,:); t = t+tau;
end
The arguments of the function lyapunov are n, the number of separate evaluations
of the maximum Lyapunov exponent, tau = œÑ, del0 = Œ¥, y, the vector of initial
data, and eqn, a handle containing the name of the equation to be integrated. The
built-in function norm(delxe) calculates the Euclidean norm of the vector delxe.
The command
plot(lyapunov(1000,1,0.01,[0 0],@duffing))
produces Figure 15.35, which shows how the average maximum Lyapunov exponent
converges to a value of about 0.1. Since this is positive, neighbouring trajectories
diverge exponentially fast ‚Äì an indication that the solution has sensitive dependence
upon initial conditions, and hence is chaotic.
One diÔ¨Éculty associated with calculating the maximum Lyapunov exponent in
this way is that the choice of the direction of the initial displacement, ‚àÜx0, can
have an eÔ¨Äect. We will see how to overcome this problem in the next section.
15.5.2
The Lyapunov Spectrum
Although we now have a way of characterizing the rate of divergence of neigh-
bouring trajectories, this is rather a blunt tool, and can depend upon the direction of
the initial displacement from the reference trajectory. For an n-dimensional system
of diÔ¨Äerential equations, we can overcome these problems by deÔ¨Åning n quantities
that characterize the growth of line‚Ä†, area, volume and hypervolume elements.
Consider the system (15.25).
As we saw above, the time evolution of small
variations ‚àÜx(t) about the reference trajectory, ¬Øx(t), is governed by
d‚àÜx
dt
= Df(¬Øx(t))‚àÜx.
We can write the solution of this system as
‚àÜx(t) = M(t)‚àÜx(0),
‚Ä† This is just the maximum Lyapunov exponent.

488
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.35. The maximum Lyapunov exponent of the forced DuÔ¨Éng equation, estimated
over 1000 iterations.
where the evolution operator M(t) is the fundamental matrix. We can construct
M numerically. The Ô¨Årst column of M is ‚àÜx(1)(t), the solution subject to the initial
condition ‚àÜx(1)(0) = (1, 0, . . . , 0)T. The jth column is ‚àÜx(j) subject to the initial
condition ‚àÜx(j)(0) = (0, . . . , 1, 0, . . . , 0)T where the one is in the jth position. By
using this construction we Ô¨Ånd that
‚àÜx(j)(t) = M(t)‚àÜx(j)(0),
with
M(t) = (‚àÜx(1)(t), ‚àÜx(2)(t), . . . , ‚àÜx(n)(t)).
The fundamental matrix M(t) has n eigenvalues {mi(t)} and the Lyapunov
exponents are deÔ¨Åned as
Œªi = lim
t‚Üí‚àû
1
t log |mi(t)| for i = 1, 2, . . . , n.
(15.29)
We say that the set of n Lyapunov exponents is the Lyapunov spectrum. The
exponents can be ordered as Œª1 ‚©æŒª2 ‚©æ¬∑ ¬∑ ¬∑ ‚©æŒªn, and Œª1 = Œªmax, the maximum
Lyapunov exponent. As before, if one of the exponents is positive, this is a sign of

15.5 LYAPUNOV EXPONENTS AND THE LYAPUNOV SPECTRUM
489
sensitive dependence upon initial conditions, and we need to use a renormalization
scheme to calculate the Lyapunov spectrum.
Example: The cubic crosscatalator
We can calculate the Lyapunov spectrum of the cubic crosscatalator equations,
(15.10) using the MATLAB function
'
&
$
%
function avlambda = lyapunovspectrum(n,tau,del0,y,eqn)
t=0; avlambda = zeros(3,n); lambda = avlambda;
ex = [1 0 0]; ey = [0 1 0]; ez = [0 0 1];
for i=1:n
tspan = [t t+tau];
[tout yout]=ode45(eqn,tspan, y);
[toutx youtx]=ode45(eqn,tspan, y+del0*ex);
[touty youty]=ode45(eqn,tspan, y+del0*ey);
[toutz youtz]=ode45(eqn,tspan, y+del0*ez);
delx= [youtx(end,:)-yout(end,:)]/del0;
dely= [youty(end,:)-yout(end,:)]/del0;
delz= [youtz(end,:)-yout(end,:)]/del0;
m = eig([delx; dely; delz]);
lambda(:,i) = log(abs(m));
avlambda(:,i) = sum(lambda,2)/i/tau;
y = yout(end,:); t = t+tau;
end
The built-in function eig(A) calculates the eigenvalues of the matrix A. Figure 15.36
shows the estimates of the three Lyapunov exponents converging over 1000 itera-
tions. One of these is positive, which indicates that the solution depends sensitively
upon initial conditions. Since the other two elements of the Lyapunov spectrum
are negative, this indicates that neighbouring solutions diverge in one direction and
approach each other in the two perpendicular directions.
We should note that there are some diÔ¨Éculties associated with calculating the
Lyapunov exponents numerically. There may be diÔ¨Äerent sets of exponents in dif-
ferent regions of phase space. This means that the Lyapunov spectrum may depend
upon the starting condition. It is also notoriously hard to obtain convergence for
Lyapunov exponents, especially where there are regions characterized by rotation,
such as in the neighbourhood of centres.
There are several further points that we can make about the Lyapunov spectrum.

490
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.36. The Lyapunov spectrum of the cubic crosscatalator equations.
‚Äî We can also deÔ¨Åne the N-dimensional Lyapunov exponent,
ŒõN =
N

j=1
Œªj
This allows us to consider the growth of various elements, so that line elements
grow like eŒõ1t, area elements grow like eŒõ2t, and so on. For the cubic crosscata-
lator, we can see that Œõ1 and Œõ2 are positive, so that line and area elements
expand, but that Œõ3 is negative, so that volume elements contract.
‚Äî The Lyapunov exponents of an equilibrium point are just the real parts of its
eigenvalues. To see this, note that, in the neighbourhood of an equilibrium point,
x = x‚àó,
d‚àÜx
dt
= Df(x‚àó)‚àÜx,
with Df(x‚àó) a constant matrix.
This means that the fundamental matrix is
M = exp (Df(x‚àó)t) (see Section 14.3.2). If ¬µi are the eigenvalues of Df(x‚àó),
then the eigenvalues of M are e¬µit. On substituting these into the deÔ¨Ånition
(15.29), we obtain Œªi = Re(¬µi).

EXERCISES
491
‚Äî Every point in the basin of attraction of an attractor has the same Lyapunov
spectrum.
‚Äî Lyapunov exponents characterize average rates of expansion (Œªi > 0) and con-
traction (Œªi < 0) in phase space. For conservative systems,  n
i=1 Œªi = 0, since
the determinant of the Jacobian is unity, which means that the product of its
eigenvalues is unity, and hence the sum of the Lyapunov exponents is zero. A
dissipative system, for which volume elements contract, has  n
i=1 Œªi < 0.
Finally, we note that if a dynamical system is chaotic then at least one Lyapunov
exponent is positive.
Exercises
15.1
Write down the equations that govern the concentrations of the chemicals
involved in the cubic crosscatalator scheme, (15.4) to (15.9). After deÔ¨Åning
suitable dimensionless variables, derive (15.10), noting carefully the con-
ditions on the initial concentration of P and the rate at which P decays
under which the equations are a good approximation.
15.2
Determine the period 2 points of the map
f : x ‚Üí4x|1.
15.3
Consider the map
H(x) =

3x,
for x ‚àà
!
0, 1
2
"
,
‚àí2 + 3x,
for x ‚àà
 1
2, 1
"
.
Prove that the only points that remain in [0, 1] have the form
x =
‚àû

n=1
an
3n ,
with an ‚àà{0, 2}.
If a(x) = a1a2a3 . . . , with an ‚àà{0, 2}, show that
a(H(x)) = œÉ(a(x)),
where œÉa = b if bn = an+1. For what value of x is a(x) = 002002002002 . . .?
Show that this point is periodic of period 3 under H.
15.4
In order to determine the nth roots of a, we can try to determine the zeros
of f(x, a) = xn ‚àía using the Newton‚ÄìRaphson iteration. This is given by
xi+1 = xi ‚àíf(xi)
f ‚Ä≤(xi).
(a) Write down the map for the iterates in the determination of the
roots, and show for n = 2 that
xi+1 = 1
2

xi + a
xi

.

492
AN INTRODUCTION TO CHAOTIC SYSTEMS
(b) Show that, for general n, the Ô¨Åxed points of the map are a1/n, and
determine the stability of these points by considering xi+œµ for œµ ‚â™1.
Do you think that this is a good way of determining the nth roots
of a?
15.5
Determine the Ô¨Åxed point of the map
yn+1 =
1
1 + yn
for yn > 0,
and show that with yn = xn/xn+1, xn are the Fibonacci numbers.
15.6
Consider two maps f and g such that f(x) : x ‚ÜíAx and g(x) : x ‚ÜíBx,
where x = (x, y)T, and A, B are real 2 √ó 2 matrices.
(a) Determine a condition for f(x) to have a Ô¨Åxed point other than
the trivial one. Comment on the Ô¨Åxed points of the composition
of f(g(x)) and g(f(x)), stating a condition for which these are the
same set of points.
(b) If det(A) = det(B) = 1, and hence the corresponding maps are area-
preserving, comment on the properties of the composition f(g(x)).
Discuss the diÔ¨Äerent options for f and g given that the maps are
area-preserving.
(c) Consider
A =
 1
0
0
‚àí1

,
B =
 1
1
0
1

,
and determine the unstable manifolds, where applicable, of the maps
f, g and f(g).
15.7
Express the system
Àôx = x3 + xy2 ‚àíx ‚àí2y,
Àôy = yx2 + y3 ‚àíy + 2x
in terms of the polar coordinates (r, Œ∏) and hence calculate the Poincar¬¥e
return map P : R ‚ÜíR as the map of successive intersections of the orbit
x(t) with the positive y-axis. Show that orbits starting on the y-axis outside
the circle r =
#
e2œÄ/ (e2œÄ ‚àí1) never return to the y-axis.
15.8
After deÔ¨Åning a suitable plane Œ£, use MATLAB to calculate a Poincar¬¥e
return map for (i) the forced DuÔ¨Éng equation and (ii) the cubic crosscata-
lator equations.
15.9
The equation of motion of a forced simple pendulum is
d 2Œ∏
dt2 + sin Œ∏ = œµ(Œ± + Œ≥ cos œât),
where Œ±, Œ≥ and œµ are positive constants.
Show that if œµ = 0 then there is a pair of heteroclinic orbits connecting
saddle points of (¬±œÄ, 0) in the (Œ∏, œÜ)-plane, where œÜ = dŒ∏/dt. Deduce that
one of the orbits is given by
Œ∏0(t) = 2 tan‚àí1(sinh t),
œÜ0(t) = 2sech(t).

EXERCISES
493
Show that the Mel‚Äônikov function for the perturbation problem of inter-
section near (0, 2) of the unstable manifold from (‚àíœÄ, 0) and the stable
manifold to (œÄ, 0) can be expressed as
M(t0) =
‚àû

‚àí‚àû
œÜ0(t ‚àít0)(Œ± + Œ≥ cos œât) dt,
and deduce that
M(t0) = 2œÄ

Œ± + Œ≥sech
œÄœâ
2

cos(œât0)

.
Hence show that there is chaos for small œµ if Œ≥ > Œ± cosh( 1
2œÄœâ).
15.10
Arnol‚Äôd‚Äôs cat map maps the torus T2 = R2/Z2 to itself, and is given by
xn+1 = f(xn), where xn = (xn, yn) and
f(x, y) = (x + y modulo 1, x + 2y modulo 1).
Show that this map is area-preserving. Find its Lyapunov exponents.
15.11
Calculate the Lyapunov spectrum of the Lorenz equations.
15.12
Project The two-dimensional motion of a particle in a Ô¨Çow with stream
function œà = œà(x, y, t) is governed by the equations
Àôx = ‚àÇœà
‚àÇy ,
Àôy = ‚àí‚àÇœà
‚àÇx .
(E15.1)
Consider the motion of a particle under the inÔ¨Çuence of an impulsive Stokes
Ô¨Çow, for which momentum is negligibly small, and œà = œà‚àó(x, y)Œ¥(t ‚àít0).
We can integrate (E15.1) to show that
x1 ‚àíx0 = ‚àÇœà‚àó
‚àÇy

(x0,y0)
,
y1 ‚àíy0 = ‚àí‚àÇœà‚àó
‚àÇx

(x0,y0)
,
(E15.2)
where (x0, y0) is the position of the particle before the impulse and (x1, y1)
its position after the impulse.
(a) Show that the map deÔ¨Åned in (E15.2) is not area-preserving, but
that the map
x1 ‚àíx0 = ‚àÇœà‚àó
‚àÇy

(x0,y1)
y1 ‚àíy0 = ‚àí‚àÇœà‚àó
‚àÇx

(x0,y1)
,
(E15.3)
is area-preserving.
(b) Write a MATLAB script that iterates points forward under the in-
Ô¨Çuence of the pulsed Ô¨Çow
œà(x, y, t) =
‚àû

n=0
œà‚àó(x, y)Œ¥(t ‚àín),
where the stream function is that associated with a point force at

494
AN INTRODUCTION TO CHAOTIC SYSTEMS
(x, y) = (d, h) above a solid plane at y = 0. This is known as a
Stokeslet, and its stream function is given by
œà‚àó
d,h(x, y) =
Œ±(x ‚àíd)
1
2 log
(x ‚àíd)2 + (y + h)2
(x ‚àíd)2 + (y ‚àíh)2
	
‚àí
2hy
(x ‚àíd)2 + (y + h)2

.
Here Œ± is the strength parameter (see Otto, Yannacopolous and
Blake, 2001, for more details). Use the area-preserving map (E15.3).
This will give a stroboscopic plot of the trajectory of the point.
(c) Consider the eÔ¨Äect of alternating Stokeslets at (0, 1
2) and (0, 3
2) with
the same strengths. Show that this leads to chaotic dynamics. This
is similar to Aref‚Äôs blinking vortex which leads to chaotic advection
(Aref, 1984).
(d) Now consider the Ô¨Çow associated with Stokeslets that are not on a
vertical line, for instance (‚àí1
2, 1
2) and ( 1
2, 1
2) or (‚àí1
2, 1
2) and ( 1
2, 3
2).
(e) By constructing the Jacobian associated with the Ô¨Çow, determine
the nature of any Ô¨Åxed or periodic points, and determine the Lya-
punov exponents associated with the Ô¨Çow.
(f) This model can be extended to other Ô¨Çows (see Ottino, 1989). In-
vestigate the combination of other fundamental solutions of Stokes
Ô¨Çow.

APPENDIX 1
Linear Algebra
A1.1
Vector Spaces Over the Real Numbers
Let V be a set of objects called vectors, of the form V = {. . . , x, y, z, . . . }, and
let R denote the real numbers, or scalars. The set V forms a vector space over
R if, for all x, y, z ‚ààV and Œ±, Œ≤ ‚ààR,
(i) x + y = y + x,
(ii) (x + y) + z = x + (y + z),
(iii) x + 0 = x,
(iv) x + (‚àíx) = 0,
(v) Œ±(x + y) = Œ±x + Œ±y,
(vi) (Œ± + Œ≤)x = Œ±x + Œ≤x,
(vii) (Œ±Œ≤)x = Œ±(Œ≤x),
(viii) 1x = x.
These conditions are the familiar laws of commutativity, associativity and distribu-
tivity for the vectors and scalars, together with the existence of inverses and iden-
tities for the scalars and vectors.
Examples
(i) V = Rn, so that
x =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
x1
x2
...
xn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏,
y =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
y1
y2
...
yn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏,
are n-dimensional vectors that can be written in terms of their coordinates.
If we then deÔ¨Åne vector addition and scalar multiplication by
x + y =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
x1 + y1
x2 + y2
...
xn + yn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏,
Œ±x =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±x1
Œ±x2
...
Œ±xn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏,
it is straightforward to verify that Rn is a vector space over R.

496
LINEAR ALGEBRA
(ii) V = Pn =

anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0 | an ‚ààR, x ‚àà[Œ±, Œ≤]

, so
that V is the set of polynomials of degree n with domain x ‚àà[Œ±, Œ≤]. A
typical member of V would be an object of the form 6x3 ‚àí2x + 1. If we
deÔ¨Åne vector addition and scalar multiplication by
(f + g)(x) = f(x) + g(x),
(Œ±f)(x) = Œ±f(x),
for x ‚àà[Œ±, Œ≤], and the zero function by 0(x) = 0, it is again easy to verify
that V is a vector space over R.
A subset B = {b1, b2, . . . , bn} of a vector space V is said to be linearly inde-
pendent if Œ±1b1 + Œ±2b2 + ¬∑ ¬∑ ¬∑ + Œ±nbn = 0 implies that Œ±1 = Œ±2 = ¬∑ ¬∑ ¬∑ = Œ±n = 0.
If Œ±1x1 + Œ±2x2 + ¬∑ ¬∑ ¬∑ + Œ±nxn = 0, and the Œ±i are not all zero, we say that the set
of vectors x1, x2, . . . , xn is linearly dependent.
The subset B forms a basis for V if, for every x ‚ààV , we can write x as a linear
combination of the elements of B, so that x = Œ±1b1 + Œ±2b2 + ¬∑ ¬∑ ¬∑ + Œ±nbn, for
some Œ±i ‚ààR. The set of all linear combinations of b1, . . . , bn is called the span of
these vectors. If span (b1, . . . , bn) = V , then b1, . . . , bn form a basis for V .
A vector space V is Ô¨Ånite dimensional if it has a basis with a Ô¨Ånite number of
elements. If it is not Ô¨Ånite dimensional, and it has a basis with an inÔ¨Ånite number
of elements, it is said to be inÔ¨Ånite dimensional.
Examples
(i) Consider V = Rn. The subset
B =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
1
0
...
0
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏,
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
0
1
...
0
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏, . . . ,
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
0
0
...
1
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£æ
= {b1, b2, . . . , bn} ,
forms a basis, since
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
x1
x2
...
xn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏= x1b1 + x2b2 + ¬∑ ¬∑ ¬∑ + xnbn.
There are other bases for Rn, but this is the simplest. All other bases also
have n elements.
(ii) Consider the vector space
V =
 ‚àû

n=0
an cos nx
 an ‚ààR, x ‚àà[‚àíœÄ, œÄ]

,
which consists of convergent Fourier series deÔ¨Åned on ‚àíœÄ ‚©Ωx ‚©ΩœÄ. A basis
for V is B = {1, cos x, cos 2x, . . . }, which contains an inÔ¨Ånite number of
elements. This shows that V is inÔ¨Ånite dimensional.

A1.2 INNER PRODUCT SPACES
497
A1.2
Inner Product Spaces
The inner or dot product of two elements, x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn),
of Rn is deÔ¨Åned to be
‚ü®x, y‚ü©= x1y1 + x2y2 + ¬∑ ¬∑ ¬∑ + xnyn.
For a general vector space, V over R, the inner product is a mapping, ‚ü®. , . ‚ü©:
V √ó V ‚ÜíR, with the three properties
(i) ‚ü®x, x‚ü©‚©æ0,
(ii) ‚ü®x, x‚ü©= 0 if and only if x = 0,
(iii) ‚ü®Œ±x + Œ≤y, z‚ü©= Œ±‚ü®x, z‚ü©+ Œ≤‚ü®y, z‚ü©,
for x, y, z ‚ààV , Œ±, Œ≤ ‚ààR.
A vector space with an inner product deÔ¨Åned on it is called an inner product
space. An important example is the space of all real-valued functions, C(I), deÔ¨Åned
on an interval I = [a, b]. It is straightforward to conÔ¨Årm, using the properties of
the Riemann integral, that
‚ü®f, g‚ü©=
 b
a
f(x) g(x) dx
is an inner product. For example, if I = [‚àí1, 1], f(x) = x and g(x) = x3, then
‚ü®f, g‚ü©=
' 1
‚àí1 x4 dx = 2
5.
Two nonzero vectors, x and y, in an inner product space are said to be orthog-
onal if ‚ü®x, y‚ü©= 0. A set of nonzero vectors in an inner product space, {xi} for
i ‚©æ1, whose members are mutually orthogonal is necessarily linearly independent.
To see this, suppose that Œ±1x1 + Œ±2x2 + ¬∑ ¬∑ ¬∑ + Œ±nxn = 0 for Œ±1, Œ±2, . . . , Œ±n ‚ààR.
This means that Œ±1‚ü®x1, xj‚ü©+ Œ±2‚ü®x2, xj‚ü©+ ¬∑ ¬∑ ¬∑ + Œ±n‚ü®xn, xj‚ü©= Œ±j‚ü®xj, xj‚ü©= 0, and
hence, by property (ii) above, Œ±j = 0 for j ‚©æ0.
A norm, ||x||, of a vector x must have the four properties
(i) ||x|| is a non-negative real number,
(ii) ||x|| = 0 if and only if x = 0,
(iii) ||kx|| = |k| ||x|| for all real k,
(iv) ||x + y|| ‚©Ω||x|| + ||y||, the triangle inequality.
The Euclidean norm of a vector is deÔ¨Åned to be ||x|| =
#
‚ü®x, x‚ü©, and gives the
size or length of x. This is familiar in R3, with ‚ü®x, y‚ü©= x1y1 + x2y2 + x3y3, so that
||x|| =
#
x2
1 + x2
2 + x2
3. In C(I), there are diÔ¨Äerent types of norm. Using the inner
product discussed above, we can deÔ¨Åne a norm
||f|| =
+ b
a
f 2(x) dx.
A useful relationship between the inner product and this norm is the Cauchy‚Äì
Schwartz inequality,
|‚ü®x, y‚ü©| ‚©Ω||x|| ||y||.

498
LINEAR ALGEBRA
We can also deÔ¨Åne the sup norm through
||f|| = sup {f(x) | x ‚ààI} .
The function space C(I) is complete under the sup norm.‚Ä† This can be useful
when proving rigorous results about diÔ¨Äerential equations.
A1.3
Linear Transformations and Matrices
A transformation, T, from a vector space, V , into itself, denoted by T : V ‚ÜíV , is
linear if
(i) T(x + y) = T(x) + T(y) ‚àÄx, y ‚ààV ,
(ii) T(Œªx) = ŒªT(x) ‚àÄŒª ‚ààR, x ‚ààV .
It follows immediately from this deÔ¨Ånition that T(Œªx + ¬µy) = ŒªT(x) + ¬µT(y) and
T(0) = 0.
Examples
(i) If V = R3 and T : R3 ‚ÜíR3 is deÔ¨Åned by
T
Ô£´
Ô£≠
x1
x2
x3
Ô£∂
Ô£∏=
Ô£´
Ô£≠
x2
x3
x1
Ô£∂
Ô£∏,
then
T(x + y) = T
Ô£´
Ô£≠
Ô£´
Ô£≠
x1 + y1
x2 + y2
x3 + y3
Ô£∂
Ô£∏
Ô£∂
Ô£∏=
Ô£´
Ô£≠
x2 + y2
x3 + y3
x1 + y1
Ô£∂
Ô£∏= T(x) + T(y),
T(Œªx) = T
Ô£´
Ô£≠
Ô£´
Ô£≠
Œªx1
Œªx2
Œªx3
Ô£∂
Ô£∏
Ô£∂
Ô£∏=
Ô£´
Ô£≠
Œªx2
Œªx3
Œªx1
Ô£∂
Ô£∏= ŒªT(x),
so that T is a linear transformation.
(ii) If V = Pn, the vector space of polynomials of degree n, and
T(anxn + ¬∑ ¬∑ ¬∑ + a1x + a0) = nanxn‚àí1 + ¬∑ ¬∑ ¬∑ + a1,
T is a linear transformation, and can be identiÔ¨Åed with the operation of
diÔ¨Äerentiation.
Linear transformations can be represented by considering their eÔ¨Äect on the
basis vectors. If T(bj) = Œ±1jb1 +Œ±2jb2 +¬∑ ¬∑ ¬∑+Œ±njbn, and we take a general vector
x = Œª1b1 + Œª2b2 + ¬∑ ¬∑ ¬∑ Œªnbn, then
T(x) = Œª1T(b1) + Œª2T(b2) + ¬∑ ¬∑ ¬∑ + ŒªnT(bn),
= Œª1(Œ±11b1 + Œ±21b2 + ¬∑ ¬∑ ¬∑ + Œ±n1bn) + ¬∑ ¬∑ ¬∑ + Œªn(Œ±1nb1 + Œ±2nb2 + ¬∑ ¬∑ ¬∑ + Œ±nnbn),
‚Ä† See Kreider, Kuller, Ostberg and Perkins (1966) for a discussion of completeness.

A1.4 THE EIGENVALUES AND EIGENVECTORS OF A MATRIX
499
which, using the standard deÔ¨Ånition of matrix multiplication, we can write as
T
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œª1
Œª2
...
Œªn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏=
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±11
Œ±12
. . .
Œ±1n
Œ±21
Œ±22
. . .
Œ±2n
...
...
...
Œ±n1
Œ±n2
. . .
Œ±nn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œª1
Œª2
...
Œªn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏.
We say that the matrix
A =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±11
Œ±12
. . .
Œ±1n
Œ±21
Œ±22
. . .
Œ±2n
...
...
...
Œ±n1
Œ±n2
. . .
Œ±nn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏
is a representation of the transformation. In example (i) above,
T
Ô£´
Ô£≠
x1
x2
x3
Ô£∂
Ô£∏=
Ô£´
Ô£≠
x2
x3
x1
Ô£∂
Ô£∏,
so that
A =
Ô£´
Ô£≠
0
1
0
0
0
1
1
0
0
Ô£∂
Ô£∏.
A1.4
The Eigenvalues and Eigenvectors of a Matrix
If A is an n √ó n matrix and x is an n √ó 1 column vector, the eigenvalues of A
are deÔ¨Åned as those values of Œª for which the equation Ax = Œªx has a nontrivial
solution. For each of these values of Œª, the eigenvectors of A are the corresponding
values of x. This deÔ¨Åning equation can be rearranged into the form (A ‚àíŒªI)x = 0,
where I is the n√ón identity matrix, for which the condition for nontrivial solutions
is det(A‚àíŒªI) = 0. This is known as the characteristic equation associated with
the matrix A.
Example
Find the eigenvalues and eigenvectors of the matrix
A =
 4
‚àí1
2
1

.
The eigenvalues satisfy
det
 4 ‚àíŒª
‚àí1
2
1 ‚àíŒª

= 0.
This gives Œª2 ‚àí5Œª + 6 = 0 so that Œª = 2 or 3. The eigenvectors satisfy
 4
‚àí1
2
1
  x
y

= Œª
 x
y

.

500
LINEAR ALGEBRA
For Œª = 2, 2x ‚àíy = 0, so that the eigenvector is Œ± (1, 2)T for any nonzero real Œ±.
Here, the superscript T denotes the transpose of the vector. For Œª = 3, x ‚àíy = 0
so that the eigenvector is Œ≤ (1, 1)T for any nonzero real Œ≤. The eigenvectors are
linearly independent and span R2.
Example
Find the eigenvalues and eigenvectors of the matrix
A =
Ô£´
Ô£≠
0
0
1
0
‚àí1
0
2
2
1
Ô£∂
Ô£∏.
The eigenvalues satisfy
det
Ô£´
Ô£≠
‚àíŒª
0
1
0
‚àí1 ‚àíŒª
0
2
2
1 ‚àíŒª
Ô£∂
Ô£∏= 0,
so that (Œª ‚àí2)(Œª + 1)2 = 0, and hence Œª = 2 or ‚àí1. The associated eigenvectors
are Œ±(1, 0, 2)T corresponding to Œª = 2, and Œ≤(1, 0, ‚àí1)T corresponding to Œª = ‚àí1.
These eigenvectors span a two-dimensional subspace of R3.
A particularly useful result concerning eigenvalues is that, if an n √ó n matrix
A has n distinct eigenvalues Œª1, Œª2, . . . , Œªn, there exists a nonsingular matrix B,
whose columns are the eigenvectors of A, such that
B‚àí1AB = diag(Œª1, Œª2, . . . , Œªn) =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œª1
0
. . .
0
0
Œª2
0
. . .
...
0
0
. . .
Œªn
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏.
The only nonzero entries of this matrix are on its diagonal, and are the eigenvalues
of A.
Consider the system of diÔ¨Äerential equations Àôx = Ax, where x = (x1, x2, . . . , xn)T
and a dot denotes diÔ¨Äerentiation.
If we write x = By, the system changes to
Àôy = B‚àí1ABy, which is considerably easier to analyze if B‚àí1AB is diagonal, since
the diÔ¨Äerential equations are all decoupled.
Example
Let‚Äôs try to Ô¨Ånd a simpliÔ¨Åcation of the system of diÔ¨Äerential equations
Àôx1
=
x3,
Àôx2
=
x1,
Àôx3
=
2x1 + x2.
We can write this in matrix form as Àôx = Ax with
A =
Ô£´
Ô£≠
0
0
1
1
0
0
2
1
0
Ô£∂
Ô£∏.

A1.4 THE EIGENVALUES AND EIGENVECTORS OF A MATRIX
501
We need to simplify the structure of the matrix A. To do this we Ô¨Årstly Ô¨Ånd the
eigenvalues, Œª = ‚àí1, 1
2(1+
‚àö
5) and 1
2(1‚àí
‚àö
5), and the corresponding eigenvectors,
Ô£´
Ô£≠
1
‚àí1
‚àí1
Ô£∂
Ô£∏,
Ô£´
Ô£≠
1
‚àí1
2(1 ‚àí
‚àö
5)
1
2(1 +
‚àö
5)
Ô£∂
Ô£∏,
Ô£´
Ô£≠
1
‚àí1
2(1 +
‚àö
5)
1
2(1 ‚àí
‚àö
5)
Ô£∂
Ô£∏.
These are orthogonal, and hence linearly independent, and form a basis for R3.
After choosing our matrix B to be
B =
Ô£´
Ô£≠
1
1
1
‚àí1
‚àí1
2(1 ‚àí
‚àö
5)
‚àí1
2(1 +
‚àö
5)
‚àí1
1
2(1 +
‚àö
5)
1
2(1 ‚àí
‚àö
5)
Ô£∂
Ô£∏,
with inverse
B‚àí1 =
Ô£´
Ô£¨
Ô£≠
1
1
‚àí1
1
‚àö
5
1
2
‚àö
5(3 ‚àí
‚àö
5)
‚àí
1
2
‚àö
5(1 ‚àí
‚àö
5)
1
‚àö
5
‚àí
1
2
‚àö
5(3 +
‚àö
5)
1
2
‚àö
5(1 +
‚àö
5)
Ô£∂
Ô£∑
Ô£∏,
we Ô¨Ånd that
B‚àí1AB =
Ô£´
Ô£≠
‚àí1
0
0
0
1
2(1 +
‚àö
5)
0
0
0
1
2(1 ‚àí
‚àö
5)
Ô£∂
Ô£∏= diag(‚àí1, 1
2(1 +
‚àö
5), 1
2(1 ‚àí
‚àö
5))
is the representation of A with respect to the basis of eigenvectors. The transformed
system of diÔ¨Äerential equations therefore takes the form
Àôy1 = ‚àíy1,
Àôy2 = 1
2(1 +
‚àö
5)y2,
Àôy3 = 1
2(1 ‚àí
‚àö
5)y3.
These have the simple solutions y1 = c1 exp(‚àít), y2 = c2 exp{ 1
2(1 +
‚àö
5)t}, y3 =
c3 exp{ 1
2(1 ‚àí
‚àö
5)t}. Finally, the solution is x1 = y1 + y2 + y3, x2 = ‚àíy1 ‚àí1
2(1 ‚àí
‚àö
5)y2 ‚àí1
2(1 +
‚àö
5)y3 and x3 = ‚àíy1 + 1
2(1 +
‚àö
5)y2 + 1
2(1 ‚àí
‚àö
5)y3, in terms of the
original variables.
Finally, we will often make use of the Cayley‚ÄìHamilton theorem.
Theorem A1.1 (Cayley‚ÄìHamilton) Every square matrix A satisÔ¨Åes its own
characteristic equation, det(A ‚àíŒªI) = 0.
The proof of this theorem is rather involved, and we will not consider it here (see
Morris, 1982). The Cayley‚ÄìHamilton theorem shows that Ak, with k ‚©æn, can be
written as a linear combination of I, A, A2, . . . , An‚àí1.

APPENDIX 2
Continuity and DiÔ¨Äerentiability
Let f be a real-valued function deÔ¨Åned on some open interval that contains the
point x = c. We say that
f is continuous at x = c if and only if lim
x‚Üíc f(x) = f(c).
The function f is continuous on the interval (a, b) if and only if it is continuous for
all x ‚àà(a, b). The set of all continuous functions f : (a, b) ‚ÜíR (R denotes the set
of real numbers) forms a vector space denoted by C(a, b) or C0(a, b).
A function f is continuous on [a, b] if it is continuous on (a, b) and
lim
x‚Üía+ f(x) = f(a),
lim
x‚Üíb‚àíf(x) = f(b).
In other words, there is no singular behaviour at the ends of the interval. We will
use C[a, b] to denote the vector space of continuous functions f : [a, b] ‚ÜíR.
An alternative, but equivalent, deÔ¨Ånition of continuity is that f is continuous
at a point x = x0 if, for every œµ > 0, there exists a function Œ¥(x0, œµ) such that
|f(x) ‚àíf(x0)| < œµ when |x ‚àíx0| < Œ¥. If Œ¥ depends upon œµ but not upon x0 in some
interval I, we say that f is uniformly continuous on I. It can be shown that, if
a function is continuous on a closed interval, then it is also uniformly continuous
there.
If limx‚Üíc+ f(x) Ã∏= limx‚Üíc‚àíf(x), f(x) is not continuous at x = c. There is an
important class of functions of this form, for which there are a Ô¨Ånite number of
discontinuities, at each of which f(x) jumps by a Ô¨Ånite amount. These are known
as piecewise continuous functions, and, like the continuous functions, form a
vector space, which we denote by PC(a, b).
If the limit of {f(x) ‚àíf(c)} /(x ‚àíc) exists as x ‚Üíc, we say that f is diÔ¨Äeren-
tiable at x = c, and denote its derivative by
f ‚Ä≤(c) = lim
x‚Üíc
f(x) ‚àíf(c)
x ‚àíc
	
.
The set of all once-diÔ¨Äerentiable functions on (a, b) forms a vector space which we
denote by C1(a, b). We can make similar deÔ¨Ånitions of C2(a, b), C3(a, b), . . . , Cn(a, b),
. . . , C‚àû(a, b), in which the higher derivatives are deÔ¨Åned in terms of limits of deriva-
tives of one order lower.
Examples of functions that live in these spaces are:
(i) f : (0, 2œÄ) ‚ÜíR : f(x) = sin x. Since f ‚Ä≤(x) = cos x and f ‚Ä≤‚Ä≤(x) = ‚àísin x =

CONTINUITY AND DIFFERENTIABILITY
503
‚àíf(x), this function can be diÔ¨Äerentiated as many times as we like and the
result will be a continuous function. We conclude that f ‚ààC‚àû(0, 2œÄ).
(ii) f : (‚àí1, 1) ‚ÜíR : f(x) = x2 for x ‚©æ0, f(x) = 0 for x < 0. This function
can be diÔ¨Äerentiated twice, but, since f ‚Ä≤‚Ä≤(x) = 0 for x < 0 and f ‚Ä≤‚Ä≤(x) = 2 for
x ‚©æ0, no more than twice. We conclude that f ‚ààC2(‚àí1, 1).
(iii) f : R ‚ÜíR : f(x) =  ‚àû
n=1
1
n! sin (n!)2 x. This function, which is illustrated
in Figure A2.1, is continuous on R but nowhere diÔ¨Äerentiable, so that f ‚àà
C0(R), but f /‚ààCn(R) for any n > 0.
Fig. A2.1. The continuous, nowhere diÔ¨Äerentiable function f(x) =  ‚àû
n=1
1
n! sin (n!)2 x.
(iv) f : [0, 3] ‚ÜíR :
f(x) =
Ô£±
Ô£≤
Ô£≥
x2
for 0 ‚©Ωx < 1,
cos x
for 1 ‚©Ωx ‚©Ω2,
e‚àíx
for 2 < x ‚©Ω3,
(A2.1)
which is plotted in Figure A2.2. This function is discontinuous at x = 1 and
x = 2, and therefore f ‚ààPC[0, 3].

504
CONTINUITY AND DIFFERENTIABILITY
Fig. A2.2. The piecewise continuous function given as example (iv). A cross indicates the
value of the function at a point of discontinuity.
If f is diÔ¨Äerentiable at c ‚àà(a, b), it is continuous at c (but the converse of this is false,
see example (iii) above) so that C0(a, b) ‚äÉC1(a, b) ‚äÉC2(a, b) ¬∑ ¬∑ ¬∑ ‚äÉCn(a, b) ¬∑ ¬∑ ¬∑ .
We conclude this section with two useful theorems that you should recall from
your Ô¨Årst course in real analysis.
Theorem A2.1 If f : [a, b] ‚ÜíR and f ‚ààC[a, b], then ‚àÉK such that |f(x)| <
K ‚àÄx ‚àà[a, b]. In words, a continuous function on a closed, bounded interval is
bounded.
Theorem A2.2 (The mean value theorem) If f : (a, b) ‚ÜíR and f ‚ààC1(a, b),
then ‚àÉc ‚àà(a, b) such that f(b) ‚àíf(a) = (b ‚àía)f ‚Ä≤(c).

APPENDIX 3
Power Series
Many commonly encountered functions can be expressed as power series, for exam-
ple,
sin x = x ‚àíx3
3! + x5
5! ‚àí¬∑ ¬∑ ¬∑ =
‚àû

n=0
(‚àí1)n
x2n+1
(2n + 1)!,
cos x = 1 ‚àíx2
2! + x4
4! ‚àí¬∑ ¬∑ ¬∑ =
‚àû

n=0
(‚àí1)n x2n
(2n)!.
(A3.1)
In general, we can develop a power series representation for a suÔ¨Éciently diÔ¨Äeren-
tiable function, provided that it involves no singularities, fractional powers of x or
logarithms.
A3.1
Maclaurin Series
The Maclaurin series is a power series about x = 0. If f ‚ààC‚àû(a, b) for some
a < 0 < b, the series takes the form
f(x) = f(0) + xf ‚Ä≤(0) + x2
2! f ‚Ä≤‚Ä≤(0) + ¬∑ ¬∑ ¬∑ + xn
n! f (n)(0) + ¬∑ ¬∑ ¬∑.
Here, a prime denotes a derivative with respect to x and f (n)(x) is the nth derivative.
Example
Let‚Äôs determine the Maclaurin series expansion for the function f(x) = 1/(1 + x).
Since
f(x) =
1
1 + x,
f(0) = 1,
f ‚Ä≤(x) = ‚àí
1
(1 + x)2 ,
f ‚Ä≤(0) = ‚àí1,
f ‚Ä≤‚Ä≤(x) =
2
(1 + x)3 ,
f ‚Ä≤‚Ä≤(0) = 2,
f (3)(x) =
‚àí3 ¬∑ 2
(1 + x)4 ,
f (3)(0) = ‚àí3!,
...
f (n)(x) =
(‚àí1)nn!
(1 + x)n+1 ,
f (n)(0) = (‚àí1)nn!,

506
POWER SERIES
the Maclaurin series is
f(x) = 1 + (‚àí1)x + 2x2
2! + (‚àí1)3!x3
3! + ¬∑ ¬∑ ¬∑ + (‚àí1)nn!xn
n! + ¬∑ ¬∑ ¬∑
= 1 ‚àíx + x2 ‚àíx3 + ¬∑ ¬∑ ¬∑ + (‚àí1)nxn + ¬∑ ¬∑ ¬∑ =
‚àû

n=0
(‚àí1)nxn.
A3.2
Taylor Series
The Maclaurin series is a special case of the Taylor series, which is a power series
about x = x0. Its existence also requires reasonable behaviour of the function at
x = x0, and for a C‚àûfunction takes the form,
f(x) = f(x0) + (x ‚àíx0)f ‚Ä≤(x0) + (x ‚àíx0)2
2!
f ‚Ä≤‚Ä≤(x0) + ¬∑ ¬∑ ¬∑ + (x ‚àíx0)
n!
n
f (n)(x0) + ¬∑ ¬∑ ¬∑ .
Example
Let‚Äôs determine the Taylor series of the function f(x) = 1/(1 + x) at the point
x0 = 1. Since
f(x) =
1
1 + x,
f(x0) = 1
2,
f ‚Ä≤(x) = ‚àí
1
(1 + x)2 ,
f ‚Ä≤(x0) = ‚àí1
22 ,
f ‚Ä≤‚Ä≤(x) =
2
(1 + x)3 ,
f ‚Ä≤‚Ä≤(x0) = 2
23 ,
f (3)(x) =
‚àí3 ¬∑ 2
(1 + x)4 ,
f (3)(x0) = ‚àí3!
24 ,
...
f (n)(x) =
(‚àí1)nn!
(1 + x)n+1 ,
f (n)(x0) = (‚àí1)n n!
2n+1 ,
the Taylor series is
f(x) =
‚àû

n=0
(x ‚àí1)n
n!
(‚àí1)nn!
2n+1
= 1
2
‚àû

n=0
(x ‚àí1)n(‚àí1)n
2n
.
(A3.2)
A3.3
Convergence of Power Series
In order to determine for what values of x a power series is convergent, we can
usually use the ratio test, which says that
the series
‚àû
 
n=0
an
Ô£±
Ô£≤
Ô£≥
converges if limn‚Üí‚àû|an+1/an| < 1,
diverges if limn‚Üí‚àû|an+1/an| > 1.

A3.4 TAYLOR SERIES FOR FUNCTIONS OF TWO VARIABLES
507
In the previous example, (A3.2),
an = (x ‚àí1)n(‚àí1)n
2n+1
,
an+1 = (x ‚àí1)n+1(‚àí1)n+1
2n+2
,
so that, for convergence, we need
lim
n‚Üí‚àû

an+1
an
 =

x ‚àí1
2
 < 1.
This means that the series converges when ‚àí1 < x < 3. This deÔ¨Ånes the domain
of convergence of the power series (A3.2). This is often written as |x ‚àí1| < 2,
which deÔ¨Ånes the radius of convergence of the series as two units from the point
x = 1. Notice that the function f(x) is singular at x = ‚àí1, and will not have a
Taylor series about this point. It is worth pointing out that the Taylor series of a
polynomial is a terminating series and hence has an inÔ¨Ånite radius of convergence.
A useful composite result from the theory of Taylor series is that if f(x) =
 ‚àû
n=0 an(x ‚àíx0)n is a Taylor series with radius of convergence R (it converges for
|x ‚àíx0| < R), f is diÔ¨Äerentiable for all x such that |x ‚àíx0| < R, and f ‚Ä≤(x) =
 ‚àû
n=1 nan(x ‚àíx0)n‚àí1 has the same radius for convergence as the series for f(x).
This ‚Äòterm by term‚Äô diÔ¨Äerentiation result allows us to develop a method for solving
diÔ¨Äerential equations using power series ‚Äì the method of Frobenius, which we discuss
in Chapter 1.
Note that another useful test for the convergence of a series is the comparison
test, which says that
if  ‚àû
n=0 an and  ‚àû
n=0 bn are series of positive terms then
(i) if  ‚àû
n=0 an converges and bk ‚©Ωak for k > k0,  ‚àû
n=0 bn also converges,
(ii) if  ‚àû
n=0 an diverges and bk ‚©æak for k > k0,  ‚àû
n=0 bn also diverges.
A3.4
Taylor Series for Functions of Two Variables
If f = f(x, y) is a scalar Ô¨Åeld that has suÔ¨Écient partial derivatives at the point
(x0, y0), the Taylor series about this point is
f(x, y) = f(x0, y0) + (x ‚àíx0) ‚àÇf
‚àÇx

(x0,y0)
+ (y ‚àíy0)‚àÇf
‚àÇy

(x0,y0)
+ (x ‚àíx0)2
2!
‚àÇ2f
‚àÇx2

(x0,y0)
+ (x ‚àíx0)(y ‚àíy0) ‚àÇ2f
‚àÇx‚àÇy

(x0,y0)
+ (y ‚àíy0)2
2!
‚àÇ2f
‚àÇy2

(x0,y0)
+ ¬∑ ¬∑ ¬∑ .
Example
Let‚Äôs Ô¨Ånd the Ô¨Årst two terms in the Taylor series of e‚àí(x2+y2) about the point (0, 0).
‚àÇf
‚àÇx

(0,0)
= ‚àÇf
‚àÇy

(0,0)
= 0,

508
POWER SERIES
‚àÇ2f
‚àÇx2

(0,0)
= ‚àÇ2f
‚àÇy2

(0,0)
= ‚àí2,
‚àÇ2f
‚àÇxdy

(0,0)
= 0,
so that e‚àí(x2+y2) = 1 ‚àíx2 ‚àíy2 + ¬∑ ¬∑ ¬∑ .
We can also generalize the Taylor series from scalar- to vector-valued functions
of two variables. Let‚Äôs deÔ¨Åne a vector Ô¨Åeld,
f(x) =
f(x)
g(x)

,
x =
x
y

.
Using the Taylor series for each component of this vector we can write
f(x) = f(x0) + J(f)(x0)(x ‚àíx0) + ¬∑ ¬∑ ¬∑ ,
where
J(f)(x0) =
Ô£´
Ô£¨
Ô£¨
Ô£≠
‚àÇf
‚àÇx(x0)
‚àÇf
‚àÇy (x0)
‚àÇg
‚àÇx(x0)
‚àÇg
‚àÇy (x0)
Ô£∂
Ô£∑
Ô£∑
Ô£∏
is called the Jacobian matrix. This result proves to be very useful in Chapter 9,
where we study nonlinear diÔ¨Äerential equations. Note that the deÔ¨Ånition of the
Jacobian can easily be generalized to higher dimensional vector Ô¨Åelds.
Example
Let‚Äôs Ô¨Ånd the linear approximation to the vector Ô¨Åeld
f =
 ex ‚àí1
(1 ‚àíy)ex

about the point x0 =
 0
1

. Since f(x0) = 0, the Taylor series takes the form
f(x) = J(f)
0
1
 
x ‚àí
0
1
	
+ ¬∑ ¬∑ ¬∑ .
After calculating the Jacobian, this gives
f(x) =
 1
0
0
‚àí1
 
x
y ‚àí1

+ ¬∑ ¬∑ ¬∑ =

x
1 ‚àíy

+ ¬∑ ¬∑ ¬∑ .

APPENDIX 4
Sequences of Functions
The concepts that we brieÔ¨Çy describe in this section are used in Chapter 5, where
we discuss the convergence of Fourier series. Consider a sequence of functions,
{fk(x)} for k = 1, 2, . . . , deÔ¨Åned on some interval of the real line, I. We say that
{fk(x)} converges pointwise to f(x) on I if limk‚Üí‚àûfk(x0) exists for all x0 ‚ààI,
and fk(x0) ‚Üíf(x0). For example, consider the sequence {fk(x)} = {x + 1/k}
on R. This sequence converges pointwise to f(x) = x as k ‚Üí‚àû. The functions
fk(x) and their limit, f(x) = x, are continuous. In contrast, consider the sequence
{fk(x)} = {xk} on [0, 1]. Although each of these functions is continuous, as k ‚Üí‚àû,
fk(x) converges pointwise to
f(x) =
 0
for 0 ‚©Ωx < 1,
1
for x = 1,
which is not continuous. As a Ô¨Ånal example, consider the sequence fk(x), deÔ¨Åned
on x ‚©æ0, with
fk(x) =
 k2x(1 ‚àíkx)
for 0 ‚©Ωx ‚©Ω1/k,
0
for x ‚©æ1/k,
(A4.1)
which is illustrated in Figure A4.1. Since fk(x) = 0 for x ‚©æ1/k, fk(x0) ‚Üí0 as
k ‚Üí‚àûfor all x0 > 0. Moreover, fk(0) = 0 for all k, so we conclude that {fk(x)}
converges pointwise to f(x) = 0, a continuous function. However, the individual
members of the sequence don‚Äôt really provide a good approximation to the pointwise
limit, Ô¨Årstly because the maximum value of fk(x) is k/4, which is unbounded as
k ‚Üí‚àû, and secondly because
 ‚àû
0
fk(x) dx = 1/6,
 ‚àû
0
f(x) dx = 0.
In order to eliminate this sort of behaviour, we need to introduce the concept of
uniform convergence. A sequence of functions {fk(x)} is uniformly convergent
to a function f(x) on an interval of the real line, I, as k ‚Üí‚àûif for every œµ > 0 there
exists a positive integer, K(œµ), which is not a function of x, such that |fk(x)‚àíf(x)| <
œµ for all k ‚©æK(œµ) and x ‚ààI. This means that, for suÔ¨Éciently large k, we can make
fk(x) arbitrarily close to f(x) over the entire interval I. For example, to see that
the sequence deÔ¨Åned by (A4.1) does not converge uniformly to zero, note that, for a
given value of œµ, we can only guarantee that |fk(x)‚àíf(x)| = fk(x) < œµ for k ‚©æ1/x,
which is not independent of x.

510
SEQUENCES OF FUNCTIONS
Fig. A4.1. The sequence of functions given by (A4.1).
Theorem A4.1 If {fk(x)} is a sequence of continuous functions deÔ¨Åned on an
interval of the real line, I = [a, b], that converges uniformly to f(x), then
(i) f(x) is continuous
(ii) for every x ‚ààI
lim
k‚Üí‚àû
 x
a
fk(s) ds =
 x
a
f(s) ds.
A proof of this theorem can be found in any textbook on analysis.

APPENDIX 5
Ordinary DiÔ¨Äerential Equations
Chapters 1 to 4 are concerned with the solution of linear second order diÔ¨Äerential
equations with variable coeÔ¨Écients. In this appendix, we will review simple methods
for solving Ô¨Årst order diÔ¨Äerential equations, and second order diÔ¨Äerential equations
with constant coeÔ¨Écients.
A5.1
Variables Separable
We will start by looking at a class of equations that can be rewritten in the form
g(y)dy
dt = f(t).
(A5.1)
These equations can be integrated with respect to t to give

g(y)dy
dt dt =

f(t) dt,
and, using the chain rule,

g(y) dy =

f(t) dt.
Example
Determine the solution of the diÔ¨Äerential equation
dy
dt + ey cos t = 0,
subject to the condition that y(0) = 1. Firstly, we convert the equation to the form
(A5.1), so that
e‚àíy dy
dt = ‚àícos t.
Integrating with respect to t we have
‚àíe‚àíy = ‚àísin t + C.
Using the boundary condition we Ô¨Ånd that C = ‚àíe‚àí1, and rearranging gives the
solution
y(t) = log

1
e‚àí1 + sin t

.

512
ORDINARY DIFFERENTIAL EQUATIONS
A5.2
Integrating Factors
We now consider the linear, Ô¨Årst order diÔ¨Äerential equation
dy
dx + P(x)y = Q(x).
You should recognize this as a type of equation that can be solved by Ô¨Ånding an
integrating factor. If we multiply through by exp

' x P(t)dt

, where t is a dummy
variable for the integration, we have
exp
 x
P(t)dt
	 dy
dx + exp
 x
P(t)dt
	
P(x)y = Q(x) exp
 x
P(t)dt
	
.
We can immediately see that the left hand side of this is d
dx

exp
 x
P(t)dt
	
y
	
,
so we have
d
dx

exp
 x
P(t)dt
	
y
	
= Q(x) exp
 x
P(t)dt
	
.
This can be directly integrated to give
y = A exp

‚àí
 x
P(t)dt
	
+ exp

‚àí
 x
P(t)dt
	  x
Q(s) exp
 s
P(t)dt
	
ds,
where A is a constant of integration. Here it is important to notice the structure
of the solution as y = yh + yp where
yh = A exp

‚àí
 x
P(t)dt
	
is the solution of the homogeneous diÔ¨Äerential equation,
dyh
dx + P(x)yh = 0,
and
yp = exp

‚àí
 x
P(t)dt
	  x
Q(s) exp
 s
P(t)dt
	
ds
is the particular integral solution of the inhomogeneous diÔ¨Äerential equa-
tion
dyp
dx + P(x)yp = Q(x).
The key idea of an integrating factor used here may seem rather like pulling a rabbit
out of a hat. In fact, the derivation can be performed systematically using the idea
of a Lie group (see Chapter 10).
Example
Let‚Äôs Ô¨Ånd the solution of the diÔ¨Äerential equation
dy
dx ‚àíy = ex,

A5.3 SECOND ORDER EQUATIONS WITH CONSTANT COEFFICIENTS
513
subject to the condition that y(0) = 0. The integrating factor is
exp
 x
‚àí1 dx

= e‚àíx,
so that we have
e‚àíx dy
dx ‚àíe‚àíxy = d
dx

e‚àíxy

= 1.
We can integrate this to obtain e‚àíxy = x+c, where c is a constant. Since y(0) = 0,
we must have c = 0 and hence y = xex.
A5.3
Second Order Equations with Constant CoeÔ¨Écients
We will now remind you how to solve second order ordinary diÔ¨Äerential equations
with constant coeÔ¨Écients through a series of examples.
Example
Solve the second order ordinary diÔ¨Äerential equation
d 2y
dx2 + 3dy
dx + 2y = 0,
subject to the boundary conditions that y(0) = 0 and y‚Ä≤(0) = 1.
We can solve constant coeÔ¨Écient equations by seeking a solution of the form
y = emx, which we substitute into the equation in order to determine m. This gives
d 2y
dx2 + 3dy
dx + 2y =

m2 + 3m + 2

emx = 0.
Since emx is never zero, we require m2 + 3m + 2 = (m + 2)(m + 1) = 0. This gives
m = ‚àí1 or m = ‚àí2. The general solution of the equation is therefore
y(x) = Ae‚àíx + Be‚àí2x,
with A and B constants. The Ô¨Årst boundary condition, y(0) = 0, yields
A + B = 0,
(A5.2)
whilst the second, y‚Ä≤(0) = 1, gives
‚àíA ‚àí2B = 1.
(A5.3)
We now need to solve (A5.2) and (A5.3), which gives us A = 1 and B = ‚àí1, so
that the solution is
y(x) = e‚àíx ‚àíe‚àí2x.
It is possible to solve this equation by direct integration, but the method we have
presented above is far simpler. Notice that it is not necessary that both boundary
conditions are imposed at the same place. Boundary conditions at diÔ¨Äerent values
of x will just lead to slightly more complicated simultaneous equations.

514
ORDINARY DIFFERENTIAL EQUATIONS
Example: Equal roots
Solve the diÔ¨Äerential equation
d 2y
dx2 + 2dy
dx + y = 0,
subject to the boundary conditions y(0) = 1 and y‚Ä≤(0) = 0.
Again we can look for a solution of the form y = emx. We Ô¨Ånd that m satisÔ¨Åes
the quadratic equation m2 + 2m + 1 = (m + 1)2 = 0, so that m = ‚àí1 is a repeated
root, and we have only determined one solution. In fact, the full solution has the
form
y(x) = Ae‚àíx + Bxe‚àíx.
(A5.4)
The boundary condition y(0) = 1 gives A = 1. Since
y‚Ä≤(x) = ‚àíAe‚àíx + B(‚àíx + 1)e‚àíx,
y‚Ä≤(0) = 0 gives ‚àíA + B = 0, and hence B = 1. The solution is therefore
y(x) = e‚àíx + xe‚àíx = (x + 1)e‚àíx.
Example: Imaginary roots
Solve the diÔ¨Äerential equation
d 2y
dx2 + 4y = 0,
subject to the boundary conditions y(0) = 1 and y‚Ä≤(0) = 3.
As usual, we seek a solution of the form y = emx, and Ô¨Ånd that m satisÔ¨Åes
m2 + 4 = 0. This means that m = ¬±2i. The general solution is therefore
y(x) = Ae2ix + Be‚àí2ix.
(A5.5)
We could proceed with the solution in this form. However, it may be better to use
eiŒ± = cos Œ± + i sin Œ±. Substituting this into (A5.5) gives
y(x) = A(cos 2x + i sin 2x) + B(cos 2x ‚àíi sin 2x)
= (A + B) cos 2x + i(A ‚àíB) sin 2x = ÀúA cos 2x + ÀúB sin 2x.
We have introduced new constants ÀúA and ÀúB, which we determine from the boundary
conditions to be ÀúA = 1 and ÀúB = 3/2. The solution is therefore
y(x) = cos 2x + 3
2 sin 2x.
Example: An inhomogeneous equation
Solve the diÔ¨Äerential equation
d 2y
dx2 + 3dy
dx + 2y = ex,
subject to the boundary conditions y(0) = y‚Ä≤(0) = 1.

A5.3 SECOND ORDER EQUATIONS WITH CONSTANT COEFFICIENTS
515
Initially we consider just the homogeneous part of the equation and solve
d 2yh
dx2 + 3dyh
dx + 2yh = 0.
As we saw earlier, the general solution of this is
yh(x) = Ae‚àíx + Be‚àí2x.
We now have to Ô¨Ånd a function f(x), the particular integral, which, when substi-
tuted into the inhomogeneous diÔ¨Äerential equation, yields the right hand side. We
notice that when we diÔ¨Äerentiate ex we get it back again, so we postulate that f(x)
takes the form Œ±ex. Substituting y = f(x) = Œ±ex into the diÔ¨Äerential equation
gives
d 2f
dx2 + 3 df
dx + 2f = Œ±ex + 3Œ±ex + 2Œ±ex = ex.
From this expression we Ô¨Ånd that we need Œ± = 1/6. The solution of the inhomoge-
neous equation is therefore
y(x) = Ae‚àíx + Be‚àí2x + 1
6ex.
It is at this point that we impose the boundary conditions, which show that A = 5/2
and B = ‚àí5/3, and hence the solution is
y(x) = 5
2e‚àíx ‚àí5
3e‚àí2x + 1
6ex.
Example: Right hand side a solution of the homogeneous equation
Solve the diÔ¨Äerential equation equation
d 2y
dx2 + 3dy
dx + 2y = e‚àíx,
subject to the boundary conditions y(0) = y‚Ä≤(0) = 1.
Following the previous
example, we need to Ô¨Ånd a function which, when substituted into the left hand side
of the equation, yields the right hand side. We could try y = Œ±e‚àíx, but, since e‚àíx
is a solution of the equation, substituting it into the left hand side just gives us
zero. Taking a lead from the case of repeated roots, we try a solution of the form
y = f(x) = Œ±xe‚àíx. Since
f(x) = Œ±xe‚àíx,
f ‚Ä≤(x) = Œ±(e‚àíx ‚àíxe‚àíx) = Œ±(1 ‚àíx)e‚àíx,
f ‚Ä≤‚Ä≤(x) = Œ±(‚àí1 ‚àí(1 ‚àíx))e‚àíx = Œ±(x ‚àí2)e‚àíx,
on substituting into the diÔ¨Äerential equation we obtain
Œ±(x ‚àí2)e‚àíx + 3Œ±(1 ‚àíx)e‚àíx + 2Œ±xe‚àíx = 3Œ±e‚àíx = e‚àíx,
which gives Œ± = 1/3. The solution is therefore
y(x) = Ae‚àíx + Be‚àí2x + 1
3xe‚àíx.

516
ORDINARY DIFFERENTIAL EQUATIONS
We now need to satisfy the boundary conditions, which give A = 8/3 and B = ‚àí5/3,
and hence
y(x) = 8
3e‚àíx ‚àí5
3e‚àí2x + 1
3xe‚àíx.
This technique is usually referred to as the trial solution method. Clearly it is not
completely satisfactory, as it requires an element of guesswork. We present a more
systematic method of solution in Chapter 1.

APPENDIX 6
Complex Variables
The aim of this brief appendix is to provide a reminder of the results that are needed
in order to be able to invert Laplace transforms using the Bromwich inversion
integral (6.5), and to understand the material on the asymptotic evaluation of
complex integrals in Section 11.2. We have had to be selective in what we have
presented, and note that this appendix is no substitute for a proper course on
complex variables! A good textbook is Ablowitz and Fokas (1997).
A6.1
Analyticity and the Cauchy‚ÄìRiemann Equations
Consider a complex-valued function of a complex variable, f(s), with s = x + iy.
The natural way to deÔ¨Åne the derivative of f is
df
ds = lim
‚àÜs‚Üí0
f(s + ‚àÜs) ‚àíf(s)
‚àÜs
,
provided that this limit is independent of the way that ‚àÜs = ‚àÜx + i‚àÜy tends to
zero. A function f(s) is said to be analytic in some region, R, of the complex
s-plane if df/ds exists and is unique in R.
Theorem A6.1 If the complex-valued function f(s) = u(x, y) + iv(x, y), where
s = x + iy and u, v, x and y are real, is analytic in a region, R, of the complex
s-plane, then
‚àÇu
‚àÇx = ‚àÇv
‚àÇy ,
‚àÇv
‚àÇx = ‚àí‚àÇu
‚àÇy .
(A6.1)
These are known as the Cauchy‚ÄìRiemann equations.
Proof By deÔ¨Ånition,
df
ds = lim
‚àÜs‚Üí0
f(s + ‚àÜs) ‚àíf(s)
‚àÜs
= lim
‚àÜx‚Üí0
‚àÜy‚Üí0
u(x + ‚àÜx, y + ‚àÜy) + iv(x + ‚àÜx, y + ‚àÜy) ‚àíu(x, y) ‚àíiv(x, y)
‚àÜx + i‚àÜy
.
If ‚àÜy = 0,
df
ds = lim
‚àÜx‚Üí0
u(x + ‚àÜx, y) ‚àíu(x, y) + i {v(x + ‚àÜx, y) ‚àív(x, y)}
‚àÜx
= ‚àÇu
‚àÇx + i‚àÇv
‚àÇx,

518
COMPLEX VARIABLES
whilst if ‚àÜx = 0,
df
ds = lim
‚àÜy‚Üí0
u(x, y + ‚àÜy) ‚àíu(x, y) + i {v(x, y + ‚àÜy) ‚àív(x, y)}
i‚àÜy
= ‚àÇv
‚àÇy ‚àíi‚àÇu
‚àÇy .
These are clearly not equal unless the Cauchy‚ÄìRiemann equations hold.
Theorem A6.2 If the Cauchy‚ÄìRiemann equations, (A6.1), hold in a region, R,
of the complex s-plane for some complex-valued function f(s) = u(x, y) + iv(x, y),
where s = x + iy and u, v, x and y are real, then, provided that all of the partial
derivatives in (A6.1) are continuous in R, f(s) is analytic in R.
We will not give a proof of this here.
A6.2
Cauchy‚Äôs Theorem, Cauchy‚Äôs Integral Formula and Taylor‚Äôs
Theorem
The contour integral of a complex-valued function f(s) along some contour C in
the complex s-plane is deÔ¨Åned to be

C
f(s) ds =
 b
a
f(s(t))ds
dt (t) dt,
where s(t) for a ‚©Ωt ‚©Ωb is a parametric representation of the contour C. By con-
vention, the integral around a closed contour is taken in the anticlockwise direction.
Theorem A6.3 (Cauchy‚Äôs theorem) If f(s) is a single-valued, analytic function
in a simply-connected domain D in the complex s-plane, then, along any simple
closed contour, C, in D,

C
f(s) ds = 0.
Proof If f(s) = u + iv and ds = dx + i dy,

C
f(s) ds =

C
(u dx ‚àív dy) + i

C
(v dx + u dy).
If df/ds is continuous in D, then u and v have continuous partial derivatives there.
We can therefore use Green‚Äôs theorem in the plane to write

C
f(s) ds = ‚àí
 
D
‚àÇv
‚àÇx + ‚àÇu
‚àÇy

dx dy + i
 
D
‚àÇu
‚àÇx ‚àí‚àÇv
‚àÇy

dx dy.
Since f is analytic in D, the Cauchy‚ÄìRiemann equations hold, and the result is
proved. If df/ds is not continuous in D, the proof is rather more technical, and we
will not give it here.

A6.2 CAUCHY‚ÄôS THEOREM AND TAYLOR‚ÄôS THEOREM
519
Theorem A6.4 (Cauchy‚Äôs integral formula) If f(s) is a single-valued, analytic
function in a simply-connected domain D in the complex s-plane, then
f(z) =
1
2œÄi

C
f(s)
(s ‚àíz) ds,
where C is any simple closed contour in D that encloses the point s = z.
Proof
Let CŒ¥ be a small circle of radius Œ¥, centred on the point s = z. Using
Cauchy‚Äôs theorem,

C
f(s)
(s ‚àíz) ds =

CŒ¥
f(s)
(s ‚àíz) ds,
which we can rewrite as

C
f(s)
(s ‚àíz) ds = f(z)

CŒ¥
ds
(s ‚àíz) +

CŒ¥
f(s) ‚àíf(z)
(s ‚àíz)
ds.
By writing the Ô¨Årst of these two integrals in terms of polar coordinates, s = z+Œ¥eiŒ∏,
we Ô¨Ånd that

CŒ¥
ds
(s ‚àíz) =
 2œÄ
0
iŒ¥eiŒ∏
Œ¥eiŒ∏ dŒ∏ = 2œÄi.
We can deal with the second integral by noting that, since f(s) is continuous,
|f(s) ‚àíf(z)| < œµ for suÔ¨Éciently small |s ‚àíz| = Œ¥. This means that


CŒ¥
f(s) ‚àíf(z)
(s ‚àíz)
ds
 ‚©Ω

CŒ¥
|f(s) ‚àíf(z)|
|s ‚àíz|
|ds| ‚©Ωœµ
Œ¥

CŒ¥
|ds| = 2œÄœµ.
Since œµ ‚Üí0 as Œ¥ ‚Üí0, the result is proved.
An extension of Cauchy‚Äôs integral formula is
Theorem A6.5 If f(s) is a single-valued, analytic function in a simply-connected
domain D in the complex s-plane, then all the derivatives of f exist in D, and are
given by
f (n)(z) = n!
2œÄi

C
f(s)
(s ‚àíz)n+1 ds,
(A6.2)
where C is any simple closed contour in D that encloses the point s = z.
The proof of this is similar to that of Cauchy‚Äôs integral formula.
Theorem A6.6 (Taylor‚Äôs theorem) If f(z) is analytic and single-valued inside
and on a simple closed contour, C, and z0 is a point inside C, then
f(z) = f(z0) + (z ‚àíz0)f ‚Ä≤(z0) + 1
2! (z ‚àíz0)2f ‚Ä≤‚Ä≤(z0) + ¬∑ ¬∑ ¬∑ =
‚àû

n=0
1
n!(z ‚àíz0)nf (n)(z0).
(A6.3)

520
COMPLEX VARIABLES
Proof Let Œ¥ be the minimum distance from z0 to the curve C, and let Œ≥ be any circle
centred on z0 with radius œÅ < Œ¥, so that, for any point z inside Œ≥, |z ‚àíz0| < œÅ < Œ¥.
By Cauchy‚Äôs integral formula,
f(z) =
1
2œÄi

C
f(w)
w ‚àíz dw =
1
2œÄi

Œ≥
f(w)
w ‚àíz dw.
We also have
1
w ‚àíz =
1
w ‚àíz0 ‚àíz + z0
=
1
w ‚àíz0
1
1 ‚àíz‚àíz0
w‚àíz0
=
1
w ‚àíz0

1 + z ‚àíz0
w ‚àíz0
+ (z ‚àíz0)2
(w ‚àíz0)2 + ¬∑ ¬∑ ¬∑

=
1
w ‚àíz0
+
z ‚àíz0
(w ‚àíz0)2 + ¬∑ ¬∑ ¬∑ +
(z ‚àíz0)n
(w ‚àíz0)n+1 + ¬∑ ¬∑ ¬∑ ,
and this series is uniformly convergent inside Œ≥, since |z ‚àíz0| / |w ‚àíz0| < 1. This
means that
f(z) =
1
2œÄi

Œ≥
f(w)
w ‚àíz dw =
1
2œÄi

Œ≥
f(w)
w ‚àíz0
dw + (z ‚àíz0) 1
2œÄi

Œ≥
f(w)
(w ‚àíz0)2 dw + ¬∑ ¬∑ ¬∑
+ (z ‚àíz0)n
1
2œÄi

Œ≥
f(w)
(w ‚àíz0)n+1 dw + ¬∑ ¬∑ ¬∑ .
Equation (A6.2) then gives us (A6.3). This series converges in any circle centred
on z0 with radius less than Œ¥. If z0 = 0 we get the simpler form
f(z) = f(0) + zf ‚Ä≤(0) + 1
2!z2f ‚Ä≤‚Ä≤(z) + ¬∑ ¬∑ ¬∑ .
Theorem A6.7 (Cauchy‚Äôs inequality) If f(z) is analytic for |z| < R with
f(z) =  ‚àû
n=0 anzn, then, if Mr is the supremum of |f(z)| on the circle |z| = r < R,
we have |an| < Mr/rn.
Proof By Taylor‚Äôs theorem,
an =
1
2œÄi

|z|=r
f(z)
zn+1 dz,
so that
|an| ‚©Ω1
2œÄ

|z|=r
|f(z)|
rn+1 |dz| ‚©Ω1
2œÄ
Mr
rn+1 2œÄr = Mr
rn .

A6.3 THE LAURENT SERIES AND RESIDUE CALCULUS
521
A6.3
The Laurent Series and Residue Calculus
Theorem A6.8 Any function f(s) that is single-valued and analytic in an annulus
r1 ‚©Ω|s ‚àís0| ‚©Ωr2 for r1 < r2 real and positive has a series expansion
f(s) =
‚àû

n=‚àí‚àû
cn(s ‚àís0)n,
which is convergent for r1 < |s ‚àís0| < r2. This is known as the Laurent series,
and its coeÔ¨Écients are given by
cn =
1
2œÄi

C
f(s)
(s ‚àís0)n+1 ds
for any simple closed contour that encloses s = s0 and lies in the annulus r1 ‚©Ω
|s ‚àís0| ‚©Ωr2.
Although we quote this important theorem, which underlies the techniques of
residue calculus, we will not give a proof here. We do note however that

C
f(s) ds = 2œÄic‚àí1.
For an analytic function, c‚àí1 = 0, and we have Cauchy‚Äôs theorem. If f is not ana-
lytic within C, this result shows that we can calculate the integral by determining
the coeÔ¨Écient of 1/(s ‚àís0) in the Laurent series. The coeÔ¨Écient c‚àí1 is called the
residue of f(s) at s = s0.
Theorem A6.9 (The residue theorem) Let C be a simple, closed contour, and
let f(s) be a complex-valued function that is single-valued and analytic on and within
C, except at n isolated singular points, s = s1, s2, . . . , sn. Then

C
f(s) ds = 2œÄi
n

j=1
aj,
where aj is the residue of f(s) at s = sj.
Proof Consider the closed contour ÀÜC, shown in Figure A6.1, which is the contour
C deformed into small circles around each singular point and joined to the original
position of C by straight contours. Since f is analytic within ÀÜC,

ÀÜ
C
f(s) ds = 0,
by Cauchy‚Äôs theorem. The integrals along the straight parts of ÀÜC cancel out in the
limit as they approach each other, so we conclude that

ÀÜ
C
f(s) ds =

C
f(s) ds ‚àí
n

j=1

Cj
f(s) ds = 0,

522
COMPLEX VARIABLES
and hence

C
f(s) ds = 2œÄi
n

j=1
aj.
x
x
x
C2
C1
Cn
C
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Fig. A6.1. The contours C and Cj.
The residue theorem simply states that the integral around a simple, closed
contour is equal to 2œÄi times the sum of the residues enclosed by C. This can
be used in a variety of ways, in particular to evaluate the integral in the Laplace
inversion formula, (6.5).
Note that, for a function with a pole of order m at s = s0, and Laurent series
expansion
f(s) =
a‚àím
(s ‚àís0)m +
a‚àí(m‚àí1)
(s ‚àís0)m‚àí1 + ¬∑ ¬∑ ¬∑ ,
we can write
f(s) =
1
(s ‚àís0)m g(s),
where
g(s) = a‚àím + a‚àí(m‚àí1)(s ‚àís0) + ¬∑ ¬∑ ¬∑ + a‚àí1(s ‚àís0)m‚àí1 + ¬∑ ¬∑ ¬∑ ,
and hence
d m‚àí1g
dsm‚àí1 = (m ‚àí1)!a‚àí1 + m! a0(s ‚àís0) + ¬∑ ¬∑ ¬∑ ,

A6.4 JORDAN‚ÄôS LEMMA
523
which shows that the residue of f(s) at s = s0 is
a‚àí1 =
1
(m ‚àí1)!
d m‚àí1
dsm‚àí1 {(s ‚àís0)m f(s)}

s=s0
.
In particular, at a simple pole, for which m = 1, a‚àí1 = (s ‚àís0)f(s0), and at a
double pole, for which m = 2,
a‚àí1 = d
ds

(s ‚àís0)2 f(s)

s=s0
.
A6.4
Jordan‚Äôs Lemma
Jordan‚Äôs lemma allows us to neglect an integral that often arises when evaluating
inverse Laplace transforms of functions F(s), provided that |F(s)| ‚Üí0 uniformly
as |s| ‚Üí‚àûin the left half plane.
Lemma A6.1 (Jordan) Let CJ be a semi-circular arc of radius R centred at s = s0
in the left half of the complex s-plane. If F(s) is a complex-valued function of s
and |F(s)| ‚Üí0 uniformly on CJ as R ‚Üí‚àû, then
lim
R‚Üí‚àû

CJ
estF(s) ds = 0,
for t a positive constant.
Fig. A6.2. sin Œ∏ ‚©æ2Œ∏/œÄ.

524
COMPLEX VARIABLES
Proof On CJ, s = s0 + ReiŒ∏, so
I =

CJ
estF(s) ds =
 3œÄ/2
œÄ/2
ets0+tReiŒ∏F(s0 + ReiŒ∏)iReiŒ∏ dŒ∏,
and hence
|I| ‚©ΩR
 3œÄ/2
œÄ/2
ets0+tReiŒ∏F(s0 + ReiŒ∏)
 dŒ∏
= R
 3œÄ/2
œÄ/2
ets0 etR cos Œ∏+itR sin Œ∏F(s0 + ReiŒ∏)
 dŒ∏
= R
ets0
 3œÄ/2
œÄ/2
etR cos Œ∏ F(s0 + ReiŒ∏)
 dŒ∏
= R
ets0
 œÄ
0
e‚àítR sin Œ∏‚Ä≤ F

s0 + Rei(Œ∏‚Ä≤+ œÄ
2 ) dŒ∏‚Ä≤,
after making the change of variable Œ∏ = Œ∏‚Ä≤ + œÄ/2. Since |F(s0 + ReiŒ∏)| ‚Üí0 as
R ‚Üí‚àû, |F(s0 + ReiŒ∏)| < K(R) for some real-valued, positive function K(R),
such that K(R) ‚Üí0 as R ‚Üí‚àû. In addition, since sin Œ∏ ‚©æ2Œ∏/œÄ, as shown in
Figure A6.2,
|I| ‚©ΩR
ets0 K(R)
 œÄ
0
e‚àí2tRŒ∏‚Ä≤/œÄ dŒ∏‚Ä≤ = œÄ |ets0| K(R)
2t

1 ‚àíe‚àí2tR
‚Üí0 as R ‚Üí‚àû.
A6.5
Linear Ordinary DiÔ¨Äerential Equations in the Complex Plane
Linear ordinary diÔ¨Äerential equations in the complex variable z = x + iy can be
studied in much the same way as those in a real variable, although their solutions
tend to have a richer structure due to the appearence of branch cuts, singularities
and other features associated with the complex plane. In particular, the solution
of linear second order equations by power series works in the same way as the real
variable case, with the proviso that there is a circle of convergence rather than an
interval of convergence. In this section, we will prove that power series solutions of
w‚Ä≤‚Ä≤ + q(z)w‚Ä≤ + r(z)w = 0,
(A6.4)
with zq(z) and z2r(z) analytic at z = 0, are convergent. Note that these are pre-
cisely the conditions for Theorem 1.3, the convergence theorem for a real diÔ¨Äerential
equation at a regular singular point.
If we assume a series solution,
w =
‚àû

n=0
anzn+c,

A6.5 ORDINARY DIFFERENTIAL EQUATIONS IN THE COMPLEX PLANE
525
and expansions
zq(z) = q0 + q1z + ¬∑ ¬∑ ¬∑ ,
z2r(z) = r0 + r1z + ¬∑ ¬∑ ¬∑ ,
then the indicial equation is
a0 {c(c ‚àí1) + cq0 + r0} = 0,
and, in general,
anf(c + n) +
n‚àí1

s=0
as {(c + s)qn‚àís + rn‚àís} = 0,
(A6.5)
where f(c) = c(c‚àí1)+cq0 +r0. The indicial equation always has one root that will
produce a well-deÔ¨Åned series solution. We will proceed assuming that we are not
dealing with a diÔ¨Écult case in which f(c + n) vanishes, when special care would be
needed.
We can rewrite (A6.5) in the form
ann (n + c1 ‚àíc2) = ‚àí
n‚àí1

s=0
as {(c1 + s) qn‚àís + rn‚àís} ,
(A6.6)
where c1 and c2 are the solutions of the indicial equation, and we have used the
fact that c1 + c2 = 1 ‚àíq0. Since zq(z) and z2r(z) are analytic at z = 0, they each
have Taylor expansions that converge in some disc centred on z = 0. Let R be the
smaller of these two radii. Then, by Cauchy‚Äôs inequality (Theorem A6.7), there
exist r and K = K(r) such that
|qn| ‚©ΩK
rn ,
|rn| ‚©ΩK
rn
for r < R and n = 0, 1, 2, . . . .
If we take the modulus of (A6.6), we can now see that
n |an| |n + c1 ‚àíc2| ‚©ΩK
n‚àí1

s=0
|as||c1| + s + 1
rn‚àís
.
Writing |c1 ‚àíc2| = Œª and |c1| = ¬µ, we can deÔ¨Åne a sequence of coeÔ¨Écients An with
|an| ‚©ΩAn using
An = |an|
for 0 ‚©Ωn < Œª,
n(n ‚àíŒª)An = K  n‚àí1
s=0 An (¬µ + s + 1) /rn‚àís
for n ‚©æŒª.
The deÔ¨Ånitions of An‚àí1 and An for n ‚©æŒª show that
n(n ‚àíŒª)An ‚àí(n ‚àí1)(n ‚àí1 ‚àíŒª)An‚àí1
r
= K(¬µ + n)An‚àí1
r
.
If we now divide through by n(n‚àíŒª)An‚àí1 and let n ‚Üí‚àû, we Ô¨Ånd that An/An‚àí1 ‚Üí
1/r. This shows that the radius of convergence of  ‚àû
n=0 Anzn is r. But, from our
deÔ¨Ånition, |an| ‚©ΩAn, so that the radius of convergence of  ‚àû
n=0 anzn is at least r
for all r < R. Hence  ‚àû
n=0 anzn converges for |z| < R as we claimed.

APPENDIX 7
A Short Introduction to MATLAB
MATLAB‚Ä† is a programming language and environment that is both powerful and
easy to use. It contains built-in functions that perform most of the tasks that we
need in order to illustrate the material in this book, for example, the numerical inte-
gration of ordinary diÔ¨Äerential equations‚Ä°, the graphical display of data, and much
more. In particular, MATLAB was originally developed to provide an easy way to
access programs that act on matrices, and contains extremely eÔ¨Écient routines, for
example, to solve systems of linear equations and to extract the eigenvalues of a
matrix¬ß.
In this appendix, our aim is to show complete newcomers to MATLAB enough
for them to be able understand the material in the book that uses MATLAB. We
introduce several other MATLAB functions in the main part of the text. For a
more extensive guide to the power of MATLAB, see Higham and Higham (2000).
A7.1
Getting Started
MATLAB can be used in two ways.
As we shall see, we can save functions,
which have arguments and return values, and scripts, which are just sequences of
MATLAB commands, as Ô¨Åles, and call them from MATLAB. Alternatively, we can
simply type commands into MATLAB at the command prompt (>>), and obtain
results immediately. For example, MATLAB has all the functionality of a scientiÔ¨Åc
calculator.
>> (4+5-6)*14/5, exp(0.4), gamma(0.5)/sqrt(pi)
ans = 8.4000
ans = 1.4918
ans = 1.0000
Note that pi is a built-in approximation to œÄ and gamma is the gamma function,
Œì(x). The three expressions separated by commas are evaluated successively, and
the calculated answers displayed. The default is to display Ô¨Åve signiÔ¨Åcant digits of
the answer. This can be changed using the format command. For example,
‚Ä† MATrix LABoratory
‚Ä° We will often use the built-in subroutine ode45 to solve ordinary diÔ¨Äerential equations. An
explanation of this is given in Section 9.3.4.
¬ß See Trefethen and Bau (1997) for an introduction to numerical linear algebra that uses MAT-
LAB extensively to illustrate the material.

A7.2 VARIABLES, VECTORS AND MATRICES
527
>> format long
>> (4+5-6)*14/5, exp(0.4), gamma(0.5)/sqrt(pi)
ans = 8.40000000000000
ans = 1.49182469764127
ans = 1.00000000000000
MATLAB has comprehensive online documentation, which can be accessed with
the command help. Typing help format at the command prompt lists the many
other formats that are available.
It should be noted that MATLAB, like any programming language, can only
perform calculations with a Ô¨Ånite precision, since numbers can only be represented
digitally with a Ô¨Ånite precision. In particular, each calculation is subject to an error
of the order of eps, the machine precision.
>> eps,sin(pi)
ans = 2.2204e-016
ans = 1.2246e-016
This tells us that the machine precision is about 10‚àí16‚Ä†, which corresponds to
double precision arithmetic. A calculation of sin œÄ = 0 leads to an error comparable
in magnitude to eps.
A7.2
Variables, Vectors and Matrices
As well as being able to perform calculations directly, MATLAB allows the use of
variables. Most programming languages require the user to declare the type of
each variable, for example a double precision scalar or a single precision complex
array. This is not necessary in MATLAB, as storage is allocated as it is needed.
For example,
>> A = 1, B = 1:3, C = 0:0.1:0.45, D = linspace(1,2,5)‚Äô...
E = [1 2 3; 4 5 6], F = [1+i; 2+2*i]
A = 1
B = 1
2
3
C = 0
0.1000
0.2000
0.3000
0.4000
D = 1.0000
1.2500
1.5000
1.7500
2.0000
E = 1
2
3
4
5
6
F = 1.0000 + 1.0000i
2.0000 + 2.0000i
‚Ä† The precise value of eps varies depending upon the system on which MATLAB is installed.

528
A SHORT INTRODUCTION TO MATLAB
Note that ... allows the current command to overÔ¨Çow onto a new line. We also
need to remember that MATLAB variables are case-sensitive. For example, X
and x denote distinct variables.
‚Äî A is a 1 √ó 1 matrix with the single, real entry, 1. Of course, we can just treat this
as a scalar.
‚Äî B is a 1√ó3 matrix (a row vector). The colon notation, a:b, just denotes a vector
with entries running from a to b at unit intervals.
‚Äî C is another row vector. In the notation a:c:b, c denotes the spacing as the
vector runs from a to b.
‚Äî D is a 5 √ó 1 matrix (a column vector). The function linspace(a,b,n) generates
a row vector running from a to b with n evenly spaced points. Note that A‚Äô is
the transpose of A when A is real, and its adjoint if A is complex.
‚Äî E is a 2 √ó 3 matrix. The semicolon is used to denote a new row.
‚Äî F is a complex column vector. Note that i is the square root of ‚àí1.
MATLAB has a rich variety of functions that operate upon matrices (type help
elmat), which we do not really need for the purposes of this book. However, as an
example, matrix multiplication takes the obvious form,
>> E‚Äô*F
ans = 9.0000 +
9.0000i
12.0000 + 12.0000i
15.0000 + 15.0000i
>> E*F
??? Error using ==> *
Inner matrix dimensions must agree.
Remember, an m1 √ó n1 matrix can only be multiplied by an m2 √ó n2 matrix if
n1 = m2.
It is also possible to operate on matrices element by element, for example
>> B.*C
ans = 0
0.2000
0.6000
1.2000
2.0000
This takes each element of B and multiplies it by the corresponding element of C.
The matrices B and C must be of the same size. The use of a full stop in, as another
example, B./C, always denotes element by element calculation. Matrix addition
and subtraction take the obvious form. For example,
>> C + D‚Äô
ans = 1.0000
1.3500
1.7000
2.0500
2.4000
>> C - D
??? Error using ==> -
Matrix dimensions must agree.
The one exception to the rule that matrices must have the same dimensions comes
when we want to add a scalar to each element of a matrix. For example,

A7.3 USER-DEFINED FUNCTIONS
529
>> C + 1
ans = 1.0000
1.1000
1.2000
1.3000
1.4000
This adds 1 to each element of C, even though 1 is a scalar.
It is often useful to be able to extract rows or columns of a matrix. For example,
>> E(:,3), E(2,:)
ans = 3
6
ans = 4
5
6
extracts the third column and second row of E.
Most of MATLAB‚Äôs built-in functions can also take matrix arguments.
For
example
>> sin(C)
ans = 0
0.0998
0.1987
0.2955
0.3894
>> E.^2
ans = 1
4
9
16
25
36
A7.3
User-DeÔ¨Åned Functions
We can add to MATLAB‚Äôs set of built-in functions by deÔ¨Åning our own functions.
For example, if we save the commands




function a = x2(x)
a = x.^2;
in a Ô¨Åle named x2.m, we will be able to access this function, which simply squares
each element of x, from the command prompt. Note that the semicolon suppresses
the output of a. For example,
>> x2(0:-0.1:-0.5)
ans = 0
0.0100
0.0400
0.0900
0.1600
0.2500
It is always prudent to write functions in such a way that they can take a matrix
argument.
For example, if we had written x2 using a = x^2, MATLAB would
return an error unless x were a square matrix, when the result would be the matrix
x*x.
A7.4
Graphics
One of the most useful features of MATLAB is its wide range of diÔ¨Äerent methods
of displaying data graphically (type help graphics). The most useful of these is
the plot command. The basic idea is that plot(x,y) plots the data contained in
the vector y against that contained in the vector x, which must, of course have the
same dimensions. For example,

530
A SHORT INTRODUCTION TO MATLAB
>> x = linspace(1,10,500); y = log(gamma(x)); plot(x,y)
produces a plot of the logarithm of the gamma function with 1 ‚©Ωx ‚©Ω10. We can
add axis labels and titles with
>> xlabel(‚Äôx‚Äô), ylabel(‚Äôlog \Gamma(x)‚Äô)
>> title(‚ÄôThe logarithm of the gamma function‚Äô)
This produces the plot shown in Figure A7.1. Note that MATLAB automatically
picks an appropriate range for the y axis. We can override this using, for exam-
ple, YLim([a b]), to reset the limits of the y axis. There are many other ways of
controlling the axes (type help axis).
Fig. A7.1. A plot of log Œì(x), produced by MATLAB.
The command plot can produce graphs of more than one function. For example,
plot(x1,y1,x2,y2,x3,y3) produces three lines in the obvious way. By default,
the lines are produced in diÔ¨Äerent colours to distinguish between them, and the
command legend(‚Äôlabel 1‚Äô, ‚Äôlabel 2‚Äô, ‚Äôlabel 3‚Äô) adds a legend that names
each line. The style of each line can be controlled, and individual points plotted if
necessary. For example, plot(x1,y1,‚Äô--‚Äô,x2,y2,‚Äôx‚Äô,x3,y3,‚Äôo-‚Äô) plots the Ô¨Årst
data set as a dashed line, the second as discrete crosses, and the third as a solid
line with circles at the discrete data points. For a comprehensive list of options,
type help plot.
As we shall see in the main part of the book, there is also an easy way of
plotting functions to get an idea of what they look like. For example, ezplot(@sin)

A7.5 PROGRAMMING IN MATLAB
531
produces a plot of sin x. We will also use the command ezmesh, which produces
a mesh plot of a function of two variables (see Section 2.6.1 for an explanation).
The quantity @sin is called a function handle. These allow functions to take
other functions as arguments, by passing the function handle as a parameter. This
includes functions supplied by the user, so that, for example, ezplot(@x2) plots
the function x2 that we deÔ¨Åned earlier.
A7.5
Programming in MATLAB
All of the control structures that you would expect a programming language to
have are available in MATLAB. The most commonly used of these are FOR loops
and the IF, ELSEIF structure.
FOR Loops
There are many examples of FOR loops in the main text. The basic syntax is
for c = x
statements
end
The statements are executed with c taking as its value the successive columns of
x. For example, if we save the script
#
"
 
!
x = linspace(0,2*pi,500);
for k = 1:4
plot(x,sin(k*x))
pause
end
to a Ô¨Åle sinkx.m, and then type sinkx at the command prompt, the commands
in sinkx.m are executed. This is a MATLAB script. Note that the indentation of
the lines is not necessary, but makes the script more readable. The editor supplied
with MATLAB formats scripts in this way automatically. The command pause
waits for the user to hit any key before continuing, so this script successively plots
sin x, sin 2x, sin 3x and sin 4x for 0 ‚©Ωx ‚©Ω2œÄ.
As a general rule, if a script needs to execute quickly, FOR loops should be
avoided where possible, in favour of matrix and vector operations. For example,
>> tic, A = (1:50000).^2; toc
elapsed_time = 0.0100
>> tic, for k = 1:50000, A(k) = k^2; end, toc
elapsed_time = 0.4010
The command tic sets an internal clock to zero, and the variable toc contains
the time elapsed, in seconds, since the last tic. We can see that allocating k2 to
A(k), the kth entry of A, using element by element squaring of 1:50000, known as
vectorizing the function, executes about forty times faster than performing the

532
A SHORT INTRODUCTION TO MATLAB
same task using a FOR loop. For an in-depth discussion of vectorization, see Van
Loan (1997).
Another way of speeding up a MATLAB script is to preallocate the array. In
the above sequence, the array A already exists, and is of the correct size when the
FOR loop executes. Consider
>>A = [];
>>tic, for k = 1:50000, A(k) = k^2; end,toc
elapsed_time = 94.1160
If we set A to be the empty array, and then allocate k2 to A(k), expecting MATLAB
to successively increase the size of A at each iteration of the loop, we can see that
the execution time becomes huge compared with that when A is already known to
be a vector of length 50000.
The IF, ELSEIF Structure
The basic syntax for an IF, ELSEIF statement is
if expression
statements
elseif expression
statements
elseif expression
statements
.
.
else
statements
end
For example, consider the function
'
&
$
%
function f = f1(x)
f = zeros(size(x));
for k = 1:length(x)
if (x(k)<0)|(x(k)>3)
disp(‚Äôx out of range‚Äô)
elseif x(k)<1
f(k) = x(k)^2;
elseif x(k)<2
f(k) = cos(x(k));
else
f(k) = exp(-x(k));
end
end

A7.5 PROGRAMMING IN MATLAB
533
This evaluates the function (A2.1), which is plotted in Figure A2.2, at the points
in the vector x, and displays a warning if any of the elements of x lie outside the
range [0, 3], where the function is deÔ¨Åned. The logical function or is denoted by
| in MATLAB. The function length(x) calculates the length of a vector x. Note
that we initialize f as a vector of zeros with the same dimensions as x using f =
zeros(size(x)). As we have already discussed, this preallocation of the matrix
signiÔ¨Åcantly speeds up the execution of the function. In fact, although this is a
transparent way of programming the function, it is still not as eÔ¨Écient as it could
be. Consider the function
'
&
$
%
function f = f2(x)
if ((x<0)|(x>3)) == zeros(size(x))
f = ((x>=0)&(x<1)).*x.^2 + ((x>=1)&(x<2)).*cos(x)...
+ ((x>=2)&(x<=3)).*exp(-x);
else
disp(‚Äôx out of range‚Äô)
end
Note that ==, not =, is used to test the equality of two matrices. The vector (x>=0)
is the same size as x, and contains ones where the corresponding element of x is
positive or zero, and zeros elsewhere. The operator & acts in the obvious way as
the logical function and. The function f2 evaluates f(x) in a vectorized manner,
avoiding the use of a FOR loop. Now consider
>> tic, y = f1(x); toc
elapsed_time = 1.4320
>> tic, y = f2(x); toc
elapsed_time = 0.1300
It is clear that the vectorized function f2 evaluates (A2.1) far more eÔ¨Éciently than
f1, which uses a FOR loop and an IF, ELSEIF statement.

Bibliography
Ablowitz, M.J. and Fokas, A.S., 1997, Complex Variables, Cambridge University Press.
Acheson, D.J., 1990, Elementary Fluid Dynamics, Oxford University Press.
Aref, H., 1984, ‚ÄòStirring by chaotic advection‚Äô, J. Fluid Mech., 143, 1‚Äì21.
Arrowsmith, D.K. and Place, C.M., 1990, An Introduction to Dynamical Systems,
Cambridge University Press.
Bakhvalov, N. and Panasenko, G., 1989, Homogenization: Averaging Processes in
Periodic Media, Kluwer.
Billingham, J., 2000, ‚ÄòSteady state solutions for strongly exothermic thermal ignition in
symmetric geometries‚Äô, IMA J. of Appl. Math., 65, 283‚Äì313.
Billingham, J. and King, A.C., 1995, ‚ÄòThe interaction of a Ô¨Çuid/Ô¨Çuid interface with a Ô¨Çat
plate‚Äô, J. Fluid Mech., 296, 325‚Äì351.
Billingham, J. and King, A.C., 2001, Wave Motion, Cambridge University Press.
Billingham, J. and Needham, D.J., 1991, ‚ÄòA note on the properties of a family of
travelling wave solutions arising in cubic autocatalysis‚Äô, Dynamics and Stability of
Systems, 6, 1, 33‚Äì49.
Bluman, G.W. and Cole, J.D., 1974, Similarity Methods for DiÔ¨Äerential Equations,
Springer-Verlag.
Bourland, F.J. and Haberman, R., 1988, ‚ÄòThe modulated phase shift for strongly
nonlinear, slowly varying, and weakly damped oscillators‚Äô, SIAM J. Appl. Math.,
48, 737‚Äì748.
Brauer, F. and Noble, J.A., 1969, Qualitative Theory of Ordinary DiÔ¨Äerential Equations,
Benjamin.
Buckmaster, J.D. and Ludford, G.S.S., 1982, Theory of Laminar Flames, Cambridge
University Press.
Byrd, P.F., 1971, Handbook of Elliptic Integrals for Engineers and Scientists,
Springer-Verlag.
Carrier, G.F. and Pearson, C.E., 1988, Partial DiÔ¨Äerential Equations: Theory and
Technique, Academic Press.
Coddington, E.A. and Levinson, N., 1955, Theory of Ordinary DiÔ¨Äerential Equations,
McGraw-Hill.
Courant, R. and Hilbert, D., 1937, Methods of Mathematical Physics, Interscience.
Dunlop, J. and Smith, D.G., 1977, Telecomunications Engineering, Chapman and Hall.
Flowers, B.H. and Mendoza, E., 1970, Properties of Matter, John Wiley.
Garabedian, P.R., 1964, Partial DiÔ¨Äerential Equations, John Wiley.
Gel‚Äôfand, I.M., 1963, ‚ÄòSome problems in the theory of quasilinear equations‚Äô, Amer.
Math. Soc. Translations Series 2, 29, 295‚Äì381.
Glendinning, P., 1994, Stability, Instability and Chaos: An Introduction to the Theory of
Nonlinear DiÔ¨Äerential Equations, Cambridge University Press.
Grindrod, P., 1991, Patterns and Waves: The Theory and Applications of
Reaction-DiÔ¨Äusion Equations, Oxford University Press.
Grobman, D.M., 1959, ‚ÄòHomeomorphisms of systems of diÔ¨Äerential equations‚Äô, Dokl.
Akad. Nauk SSSR, 128, 880.
Guckenheimer, J. and Holmes, P.J., 1983, Nonlinear Oscillations, Dynamical Systems

BIBLIOGRAPHY
535
and Bifurcations of Vector Fields, Springer-Verlag.
Hartman, P., 1960, ‚ÄòA lemma in the theory of structural stability of diÔ¨Äerential
equations‚Äô, Proc. Amer. Math. Soc., 11, 610‚Äì620.
Higham, D.J. and Higham, N.J., 2000, MATLAB Guide, SIAM.
Hydon, P.E., 2000, Symmetry Methods for DiÔ¨Äerential Equations, Cambridge University
Press.
Ince, E.L., 1956, Ordinary DiÔ¨Äerential Equations, Dover.
Kevorkian, J., 1990, Partial DiÔ¨Äerential Equations: Analytical Solution Techniques,
Wadsworth and Brooks.
King, A.C., 1988, ‚ÄòPeriodic approximations to an elliptic function‚Äô, Applicable Analysis,
27, 271‚Äì278.
King, A.C. and Needham, D.J., 1994, ‚ÄòThe eÔ¨Äects of variable diÔ¨Äusivity on the
development of travelling waves in a class of reaction-diÔ¨Äusion equations‚Äô, Phil.
Trans. R. Soc. Lond. A, 348, 229‚Äì260.
K¬®orner, T.W., 1988, Fourier Analysis, Cambridge University Press.
Kreider, D.L., Kuller, R.G., Ostberg, D.R. and Perkins, F.W., 1966, An Introduction to
Linear Analysis, Addison-Wesley.
Kuzmak, G.E., 1959, ‚ÄòAsymptotic solutions of nonlinear second order diÔ¨Äerential
equations with variable coeÔ¨Écients‚Äô, J. Appl. Math. Mech., 23, 730‚Äì744.
Landau, L.D. and Lifschitz, E.M., 1959, Theory of Elasticity, Pergamon.
Lighthill, M.J., 1958, Introduction to Fourier Analysis and Generalised Functions,
Cambridge University Press.
Lorenz, E.N., 1963 ‚ÄòDeterministic non-periodic Ô¨Çows‚Äô, J. Atmos. Sci., 20, 130‚Äì141.
Lunn, M., 1990, A First Course in Mechanics, Oxford University Press.
Marlin, T.E., 1995, Process Control: Designing Processes and Control Systems for
Dynamic Performance, McGraw-Hill.
Mathieu, J. and Scott, J., 2000, An Introduction to Turbulent Flow, Cambridge
University Press.
Milne-Thompson, L.M., 1952, Theoretical Aerodynamics, Macmillan.
Milne-Thompson, L.M., 1960, Theoretical Hydrodynamics, Macmillan.
Morris, A.O., 1982, Linear Algebra: An Introduction, Van Nostrand Reinhold.
Ottino, J.M., 1989, The Kinematics of Mixing: Stretching, Chaos and Transport,
Cambridge University Press.
Otto, S.R., Yannacopoulos, A. and Blake, J.R., 2001, ‚ÄòTransport and mixing in Stokes
Ô¨Çow: the eÔ¨Äect of chaotic dynamics on the blinking stokeslet‚Äô, J. Fluid Mech., 430,
1‚Äì26.
Pedley, T.J., 1980, The Fluid Mechanics of Large Blood Vessels, Cambridge University
Press.
Petrov, V., Scott, S.K. and Showalter, K., 1992, ‚ÄòMixed-mode oscillations in chemical
systems‚Äô, J. Chem. Phys., 97, 6191‚Äì6198.
SchiÔ¨Ä, L.I., 1968, Quantum Mechanics, McGraw-Hill.
Sobey, I., 2000, An Introduction to Interactive Boundary Layer Theory, Oxford
University Press.
Sparrow, C.T., 1982, The Lorentz Equations: Bifurcations, Chaos and Strange
Attractors, Springer-Verlag.
Trefethen, L.N. and Bau, D., III, 1997, Numerical Linear Algebra, SIAM.
Van Dyke, M., 1964, Perturbation Methods in Fluid Mechanics, Academic Press.
Van Loan, C.F., 1997, Introduction to ScientiÔ¨Åc Computing, Prentice Hall.
Watson, G.N., 1922, A Treatise on the Theory of Bessel Functions, Cambridge
University Press.
Wiggins, S., 1988, Global Bifurcations and Chaos, Springer-Verlag.
Wiggins, S., 1990, Introduction to Applied Nonlinear Dynamical Systems and Chaos,
Springer-Verlag.

Index
Abel‚Äôs formula, 10, 34, 77, 99
aerofoils, 195, 274
Airy‚Äôs equation, 79, 214, 350
analyticity, 517
Arrhenius law, 319
asymptotic balance, 305
autocatalysis, 374, 409
bang-bang control, 422, 437
basin of attraction, 223
Bendixson‚Äôs negative criterion, 240
Dulac‚Äôs extension, 240, 253E
Bernoulli shift map, 467
Bernoulli‚Äôs equation, 190
Bessel functions, 42, 58, 293
Fourier‚ÄìBessel series, 71, 74
generating function, 64
modiÔ¨Åed Bessel functions of Ô¨Årst and
second kind, 71
orthogonality, 71
recurrence relations, 69
Weber‚Äôs, 62
Bessel‚Äôs equation, 28, 58, 108
ŒΩ = 1, 10
ŒΩ = 1
2 , 12, 28E
inhomogeneity, 77
Lommel‚Äôs functions, 79
Bessel‚Äôs inequality, 114
Bessel‚ÄìLommel theorem, 75
bifurcation
codimension two, 397
diagram, 390
Ô¨Çip, 454
global, 408
homoclinic, 408
Hopf, 402
pitchfork, 399, 402
saddle‚Äìnode, 390, 392, 402, 457
subcritical Hopf, 402
subcritical pitchfork, 399
supercritical Hopf, 402
supercritical pitchfork, 399
tangent, 457
transcritical, 398, 402
bifurcation theory, 388
boundary value problem, 46, 54E, 93, 121E,
183, 370, 376
inhomogeneous, 96
bounded controls, 419
Cantor
diagonalization proof, 464
cantor
middle-third set, 464
Cauchy problem, 180
Cauchy‚Äôs integral formula, 519
Cauchy‚Äôs theorem, 287, 298, 518
Cauchy‚ÄìKowalewski theorem, 180
Cauchy‚ÄìRiemann equations, 186
Cauchy‚ÄìSchwartz inequality, 497
Cayley‚ÄìHamilton theorem, 425, 434, 501
centre manifold
local, 373, 379, 462
centre manifold theorem, 372
chain rule, 511
chaotic solutions, 449
islands, 460
characteristic equation, 499
characteristic variables, 358
chemical oscillator, 450
comparison theorems, 212
complete elliptic integral
Ô¨Årst kind, 334
second kind, 336
completely controllable, 420, 444E
completeness, 42, 114, 498
complex analysis
branch cut, 165
branch point, 164
Cauchy‚ÄìRiemann equations, 186
keyhole contour, 165
poles and residues, 163, 521
composite expansions, 312
conformal mapping, 191
Joukowski transformation, 195
connection problems, 348
conservation law, 271
conservative systems, 218
continuity, 502
piecewise, 502
uniform, 502
contour integral, 518
control
bang-bang, 422
variables, 418
vector, 418
536

INDEX
537
control problem
optimal, 418
time-optimal, 418
controllability matrix, 433
controllable set, 420
convolution, 130, 160, 352
CSTR (continuous Ô¨Çow, stirred tank reactor),
393, 400, 415
cubic autocatalysis, 374
cubic crosscatalator, 450, 489, 492E
D‚ÄôAlembert‚Äôs solution, 180, 357
diÔ¨Äeomorphism, 379, 461, 475
smooth, 379
diÔ¨Äerentiable, 502
diÔ¨Äusion equation
heat, 45, 123
point source solution, 271
diÔ¨Äusion problem, 352
dimensionless
groups, 274
variables, 123
Dirac delta function, 135, 271
Dirichlet kernel, 128
Dirichlet problem, 55, 143, 183, 193
domain of attraction, 223
eigenfunctions, 95
expansions, 104
eigensolutions, 369E
eigenvalues, 95, 103, 372, 499
eigenvectors, 103, 499
elastic membrane, 80
electrostatics, 39, 148
equation
Airy, 79, 122E, 214, 280, 350
Bernoulli, 190
Cauchy‚ÄìRiemann, 186, 290, 517
characteristic (matrices), 425
diÔ¨Äusion, 123, 175, 184, 270
DuÔ¨Éng‚Äôs, 448
Euler, 413E
forced DuÔ¨Éng, 448, 482, 486, 492E
forced wave, 151
Fourier, 124
Helmholtz, 351
Laplace, 143, 147, 175, 179, 182, 186, 193
logistic, 468
Lorenz, 451, 470, 493E
modiÔ¨Åed Helmholtz, 151, 352
Navier‚ÄìStokes, 170
porous medium, 273
reaction‚ÄìdiÔ¨Äusion, 185, 370, 375
Ricatti‚Äôs, 269
Schr¬®odinger‚Äôs, 119
Tricomi‚Äôs, 177
Volterra integral, 162
wave, 148, 175, 288, 357
equilibrium point
centre, 230
hyperbolic, 224, 475
nonhyperbolic, 224, 384
nonlinear centre, 401
saddle, 227, 294, 378
stable, 220
stable node, 227
stable spiral, focus, 229
unstable, 220
unstable node, 226
unstable spiral, focus, 228
equilibrium solutions, 220
Euclidean norm, 485, 487, 497
exchange of stabilities, 398
existence, 418
local, 204
exponential integral, 276
Floquet theory, 335
Ô¨Çow evolution operator, 467
Ô¨Çuid dynamics, 48
Bernoulli‚Äôs equation, 51, 186
boundary conditions, 49
boundary layer, 274
circle theorem, 189
complex potential, 186
Ô¨Çow past a Ô¨Çat plate, 194
Ô¨Çow past an aerofoil, 195
inviscid, irrotational, 186
Kutta condition, 195
Navier-Stokes equations, 170
Reynolds number, 274, 371, 456
stream function, 186
triple deck, 370
turbulence, 456
velocity potential, 49
viscosity, 167
Fourier integral, 133
Fourier series, 68, 125, 496, 509
Bessel functions, 71, 74
cosine, 127
generalised coeÔ¨Écients, 113
Gibbs‚Äô phenomenon, 75, 128
Legendre polynomials, 43
sine, 127
uniform convergence, 132
Fourier theorem, 131
Fourier transform, 133, 280, 289, 352
convolution integral, 140
higher dimensions, 145
inverse, 138
inversion formula, 133
linearity, 138
shifting, 150
Fourier‚Äôs equation, 108, 124
Fourier‚Äôs law, 44
Fourier‚ÄìBessel series, 71, 74, 90E, 126
Fourier‚ÄìLegendre series, 126
Fredholm alternative, 96
frequency modulation, 87
Friedrichs boundary conditions, 109
functions
œÜ(n), 23
Airy, 79, 296
Bessel, 280

538
INDEX
functions (cont.)
Bessel (large argument), 346
Bessel of order zero, 293
complementary error, 167, 270, 357
cost (control), 418
Dirac delta, 135
error, 167
gamma, 58, 153, 281
gauge, 275
good, 134
Heaviside, 135
Jacobian elliptic, 251, 314, 331, 333, 368E
Kronecker delta, 74
Laguerre polynomials, 122
Mel‚Äônikov, 479
modiÔ¨Åed Bessel, 71, 281, 300E, 302E
negative deÔ¨Ånite, 381
of exponential order, 154
parabolic cylinder, 302
positive deÔ¨Ånite, 381
sequences of, 509
sign function, 135
unit function, 135
gamma function, 153, 294
gauge functions, 275
generalized function, 134
Dirac delta, 135
Heaviside, 135
multi-dimensional delta function, 145
sign function, 135
unit function, 135
generating functions
Bessel functions, 64
Legendre polynomials, 35
Gibbs‚Äô phenomenon, 75, 128
good functions, 134
Green‚Äôs formula, 111
Green‚Äôs function, 98, 121E, 141
free space, 142
Gronwall‚Äôs inequality, 210, 212, 216E
group
extended, 262
inÔ¨Ånitesimal generators, 259
isomorphic, 258
magniÔ¨Åcation, 260
one-parameter, 257, 266
PDEs, 270
rotation, 261
two-parameter, 270
group invariants, 256, 261
group theoretic methods, 322, 512
Hamiltonian system, 247, 477
Hartman-Grobman theorem, 230
Heaviside function, 135, 156
Hermite‚Äôs equation, 101, 112
Hermitian, 103
heteroclinic
path, 221
solutions, 221
homeomorphism, 379
homoclinic
path, 221
homoclinic point
transverse, 474
homoclinic tangles, 447, 472
Hopf bifurcation theorem, 403, 415E
hypergeometric equation, 29
hysteresis, 397
initial value problem, 3, 181, 288, 357
inner product, 102, 497
complex functions, 111
integral equations, 172E
Volterra, 162
integral path, 219, 225, 373
integrating factors, 512
intermittency, 456
Inverse
Fourier transform, 138
Jacobian, 234, 248, 250, 322, 377, 380, 480,
508
Jordan curve theorem, 243
Jordan‚Äôs lemma, 163, 287, 523
Kronecker delta, 113
Kuzmak‚Äôs method, 332
Lagrange‚Äôs identity, 101
Laguerre polynomials, 122
Laplace transform, 152, 352
Bromwich inversion integral, 163
convolution, 160
Ô¨Årst shifting theorem, 155, 164
inverse, 155, 280
inversion, 162
linearity, 154
ODEs, 158
of a derivative, 157
second shifting theorem, 156
Laplace‚Äôs equation, 45, 54E, 143, 193
Laplace‚Äôs method, 281, 300E
Laurent series, 521
leading order solutions, 303
Legendre equation
n = 1, 6
Legendre polynomials, 31, 61, 106
P0(x), P1(x), P2(x), P3(x) and P4(x), 33
associated Legendre polynomials, 52
Fourier-Legendre series, 43, 54E
generating function, 35, 54E
Laplace‚Äôs representation, 40
orthogonality, 41
recurrence relations, 38
Rodrigues‚Äô formula, 39
Schl¬®aÔ¨Çi‚Äôs representation, 40
special values, 37
Legendre‚Äôs equation, 31, 108
associated, 52, 120
general solution, 32

INDEX
539
generalized, 121
order, 31
Lerch‚Äôs theorem, 154
Lie group, 257
Lie series, 259
limit cycles, 221
limit point
œâ, 242
linear dependence, 9, 496
linear independence, 9, 496
linear operator
Hermitian, 103
self-adjacency, 103
self-adjoint, 42, 100
linear oscillator
controlling, 428, 439
linear transformations, 498
linearity
Fourier transform, 138
Laplace transform, 154
linearization of nonlinear systems, 221
Liouville‚Äôs theorem, 249
Lipschitz condition, 205, 212
lobe, 475
Lommel‚Äôs functions, 79
Lyapunov exponents, 484
maximum, 485
spectrum, 487
Lyapunov function, 372, 381
Lyapunov stable, 381
Maclaurin series, 505
manifold, 379
maps, 452
Bernoulli shift, 467
H¬¥enon, 459
horseshoe, 475
logistic, 454
Poincar¬¥e return, 467
shift, 452
Smale horseshoe, 465
tent, 463
matched asymptotic expansions, 310
MATLAB functions
airy, 80
besselj, 63
ceil, 125
contour, 50
eig, 489
ellipj, 314, 318
ellipke, 337
erf, 169
erfc, 169
ezmesh, 48
fzero, 76, 318
gamma, 60
legend, 33
legendre, 33
linspace, 33
lorenz, 451
meshgrid, 50
ode45, 237, 489
plot, 33, 529
polyval, 17
subplot, 63
matrix exponential, 424, 444
mean value theorem, 504
Mel‚Äônikov function, 493E
Mel‚Äônikov theory, 477
method of Frobenius, 11‚Äì24, 31, 54E, 60, 507
General Rule I, 15, 27, 32
General Rule II, 19, 61
General Rule III, 23, 62
indicial equation, 13
method of matched asymptotic expansions,
307, 366E
method of multiple scales, 325, 332, 333, 359,
367E
method of reduction of order, 5, 33, 54E, 269
formula, 6
method of stationary phase, 285‚Äì290, 301E
method of steepest descents, 290, 350
Method of variation of parameters, 7, 34, 77,
96, 351
formula, 8, 98
modiÔ¨Åed Bessel functions, 71
negatively invariant set, 241
Neumann problem, 144, 184
Newton‚Äôs
Ô¨Årst law, 94
second law, 50, 80, 218, 231, 255, 426, 447
nonautonomous, 269, 322, 429, 432
normal form, 390
pitchfork bifurcation, 402, 415E
transcritical bifurcation, 402, 414E
nullclines, 234
ODEs, 511
autonomous, 222
complementary function, 4
coupled systems, 158
existence, 203, 204
integral paths, 225
integrating factors, 512
irregular singular point, 25
nonautonomous, 222
nonsingular, 3, 10
ordinary points, 225
particular integral, 4, 8
regular point, 3
second order equations (constant
coeÔ¨Écients), 513
separable variables, 511
singular point, 3, 24
successive approximations, 204
uniqueness, 203, 210
one-parameter group
horizontal translation, 257
magniÔ¨Åcation, 257
rotation, 257
vertical translation, 257

540
INDEX
orthogonality, 98, 103, 111, 497
Bessel functions, 71
Legendre polynomials, 41
orthonormality, 113
particular integral, 512
PDEs
asymptotic methods, 351
canonical form, 175, 177
Cauchy problem, 180
characteristic variables, 83, 177, 358
classiÔ¨Åcation, 175
d‚ÄôAlembert‚Äôs solution, 180
elliptic, 175
Fourier transforms, 143
group theoretic methods, 270
hyperbolic, 175
parabolic, 175
separable solution, 124
similarity variable, 270
Peano uniqueness theorem, 216E
period doubling, 454
periodic solutions, 221
perturbation problem
regular, 274, 303
singular, 274, 304
phase plane, 219
phase portrait, 245
plane polar coordinates, 146
planetary motion, 66
Pockhammer symbol, 60
Poincar¬¥e return map, 467, 492E
Poincar¬¥e index, 237, 253E
Poincar¬¥e projection, 245
Poincar¬¥e‚ÄìBendixson theorem, 241,
387
population dynamics, 233
positioning problem, 426, 433, 438,
444E
with friction, 427
with two controls, 429, 441
positively invariant set, 241
power series
comparison test, 507
convergence, 506
radius of convergence, 507
ratio test, 506
Pr¬®ufer substitution, 115
predator‚Äìprey system, 234, 253E,
445E
product rule, 5, 425
quantum mechanics, 118
hydrogen atom, 120
Rayleigh‚Äôs problem, 168
reachable set, 420, 421
residue theorem, 521
resonance, 97
Riemann integral, 497
Riemann‚ÄìLebesgue lemma, 130
Rodrigues‚Äô formula, 39
saddle point, 227
Schr¬®odinger‚Äôs equation, 119
secular terms, 327
separatrices, 227, 252E, 323, 378, 408
set
Cantor‚Äôs middle-third, 464
closed, 423
completely disconnected, 464
connected, 431
controllable, 429
convex, 422
open, 423
reachable, 421
strictly convex, 423
simple harmonic motion, 217, 233, 331,
425
Smale-BirkhoÔ¨Ätheorem, 475
solubility, 97
spherical harmonics, 54
spherical polar coordinates, 39, 53, 146
stability
asymptotic, 381
exchange of, 398
Lyapunov, 381
structurally unstable, 389
stable manifold
global, 380
local, 372, 378, 379, 462
stable node, 227
stable spiral, focus, 229
state variables, 418
state vector, 418
steering problem, 427
Stirling‚Äôs formula, 282, 301E
structurally unstable, 389
Sturm comparison theorem, 30
Sturm separation theorem, 11
Sturm‚ÄìLiouville problems, 42, 107
regular endpoint, 109
singular endpoint, 109
target state, 418
Taylor series, 506
functions of two variables, 507
thermal ignition, 318
tightrope walker, 419, 443E
time-optimal control problem, 418
Time-optimal maximum principle,
436
transformations
identity, 257
inÔ¨Ånitesimal, 258
inverse, 257
magniÔ¨Åcation, 260
rotation, 261
transversal, 242
travelling wave solution, 375, 409
triangle inequality, 208, 497

INDEX
541
uniqueness, 210, 418
unstable manifold
global, 380
local, 379, 462
unstable node, 227
unstable spiral, focus, 228
van der Pol Oscillator, 329
Van Dyke‚Äôs matching principle, 311, 366E
Vector space, 9, 495
basis, 496
Watson‚Äôs lemma, 283, 292, 300E
wave equation, 82
d‚ÄôAlembert‚Äôs solution, 83
WKB approximation, 344, 369E
Wronskian, 8, 9, 28E, 54E, 77, 90E,
98


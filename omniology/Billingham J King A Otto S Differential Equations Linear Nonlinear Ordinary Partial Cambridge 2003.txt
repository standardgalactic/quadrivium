Diﬀerential Equations
Linear, Nonlinear, Ordinary, Partial
A.C. King, J. Billingham and S.R. Otto

  
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge  , United Kingdom
First published in print format 
-    ----
-    ----
-    ----
© Cambridge University Press 2003
2003
Information on this title: www.cambridge.org/9780521816588
This book is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.
-    ---
-    ---
-    ---
Cambridge University Press has no responsibility for the persistence or accuracy of
s for external or third-party internet websites referred to in this book, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
hardback
paperback
paperback
eBook (NetLibrary)
eBook (NetLibrary)
hardback

Contents
Preface
page ix
Part One: Linear Equations
1
1
Variable Coeﬃcient, Second Order, Linear, Ordinary
Diﬀerential Equations
3
1.1
The Method of Reduction of Order
5
1.2
The Method of Variation of Parameters
7
1.3
Solution by Power Series: The Method of Frobenius
11
2
Legendre Functions
31
2.1
Deﬁnition of the Legendre Polynomials, Pn(x)
31
2.2
The Generating Function for Pn(x)
35
2.3
Diﬀerential and Recurrence Relations Between Legendre
Polynomials
38
2.4
Rodrigues’ Formula
39
2.5
Orthogonality of the Legendre Polynomials
41
2.6
Physical Applications of the Legendre Polynomials
44
2.7
The Associated Legendre Equation
52
3
Bessel Functions
58
3.1
The Gamma Function and the Pockhammer Symbol
58
3.2
Series Solutions of Bessel’s Equation
60
3.3
The Generating Function for Jn(x), n an integer
64
3.4
Diﬀerential and Recurrence Relations Between Bessel Functions
69
3.5
Modiﬁed Bessel Functions
71
3.6
Orthogonality of the Bessel Functions
71
3.7
Inhomogeneous Terms in Bessel’s Equation
77
3.8
Solutions Expressible as Bessel Functions
79
3.9
Physical Applications of the Bessel Functions
80
4
Boundary Value Problems, Green’s Functions and
Sturm–Liouville Theory
93
4.1
Inhomogeneous Linear Boundary Value Problems
96
4.2
The Solution of Boundary Value Problems by Eigenfunction
Expansions
100
4.3
Sturm–Liouville Systems
107

vi
CONTENTS
5
Fourier Series and the Fourier Transform
123
5.1
General Fourier Series
127
5.2
The Fourier Transform
133
5.3
Green’s Functions Revisited
141
5.4
Solution of Laplace’s Equation Using Fourier Transforms
143
5.5
Generalization to Higher Dimensions
145
6
Laplace Transforms
152
6.1
Deﬁnition and Examples
152
6.2
Properties of the Laplace Transform
154
6.3
The Solution of Ordinary Diﬀerential Equations using Laplace
Transforms
157
6.4
The Inversion Formula for Laplace Transforms
162
7
Classiﬁcation, Properties and Complex Variable Methods for
Second Order Partial Diﬀerential Equations
175
7.1
Classiﬁcation and Properties of Linear, Second Order Partial
Diﬀerential Equations in Two Independent Variables
175
7.2
Complex Variable Methods for Solving Laplace’s Equation
186
Part Two: Nonlinear Equations and Advanced Techniques
201
8
Existence, Uniqueness, Continuity and Comparison of
Solutions of Ordinary Diﬀerential Equations
203
8.1
Local Existence of Solutions
204
8.2
Uniqueness of Solutions
210
8.3
Dependence of the Solution on the Initial Conditions
211
8.4
Comparison Theorems
212
9
Nonlinear Ordinary Diﬀerential Equations: Phase Plane
Methods
217
9.1
Introduction: The Simple Pendulum
217
9.2
First Order Autonomous Nonlinear Ordinary Diﬀerential
Equations
222
9.3
Second Order Autonomous Nonlinear Ordinary Diﬀerential
Equations
224
9.4
Third Order Autonomous Nonlinear Ordinary Diﬀerential
Equations
249
10
Group Theoretical Methods
256
10.1
Lie Groups
257
10.2
Invariants Under Group Action
261
10.3
The Extended Group
262
10.4
Integration of a First Order Equation with a Known Group
Invariant
263

CONTENTS
vii
10.5
Towards the Systematic Determination of Groups Under Which
a First Order Equation is Invariant
265
10.6
Invariants for Second Order Diﬀerential Equations
266
10.7
Partial Diﬀerential Equations
270
11
Asymptotic Methods: Basic Ideas
274
11.1
Asymptotic Expansions
275
11.2
The Asymptotic Evaluation of Integrals
280
12
Asymptotic Methods: Diﬀerential Equations
303
12.1
An Instructive Analogy: Algebraic Equations
303
12.2
Ordinary Diﬀerential Equations
306
12.3
Partial Diﬀerential Equations
351
13
Stability, Instability and Bifurcations
372
13.1
Zero Eigenvalues and the Centre Manifold Theorem
372
13.2
Lyapunov’s Theorems
381
13.3
Bifurcation Theory
388
14
Time-Optimal Control in the Phase Plane
417
14.1
Deﬁnitions
418
14.2
First Order Equations
418
14.3
Second Order Equations
422
14.4
Examples of Second Order Control Problems
426
14.5
Properties of the Controllable Set
429
14.6
The Controllability Matrix
433
14.7
The Time-Optimal Maximum Principle (TOMP)
436
15
An Introduction to Chaotic Systems
447
15.1
Three Simple Chaotic Systems
447
15.2
Mappings
452
15.3
The Poincar´e Return Map
467
15.4
Homoclinic Tangles
472
15.5
Quantifying Chaos: Lyapunov Exponents and the Lyapunov
Spectrum
484
Appendix 1 Linear Algebra
495
Appendix 2 Continuity and Diﬀerentiability
502
Appendix 3 Power Series
505
Appendix 4 Sequences of Functions
509
Appendix 5 Ordinary Diﬀerential Equations
511
Appendix 6 Complex Variables
517
Appendix 7 A Short Introduction to MATLAB
526
Bibliography
534
Index
536

Preface
When mathematical modelling is used to describe physical, biological or chemical
phenomena, one of the most common results is either a diﬀerential equation or
a system of diﬀerential equations, together with appropriate boundary and initial
conditions. These diﬀerential equations may be ordinary or partial, and ﬁnding
and interpreting their solution is at the heart of applied mathematics. A thorough
introduction to diﬀerential equations is therefore a necessary part of the education
of any applied mathematician, and this book is aimed at building up skills in this
area. For similar reasons, the book should also be of use to mathematically-inclined
physicists and engineers.
Although the importance of studying diﬀerential equations is not generally in
question, exactly how the theory of diﬀerential equations should be taught, and
what aspects should be emphasized, is more controversial. In our experience, text-
books on diﬀerential equations usually fall into one of two categories. Firstly, there
is the type of textbook that emphasizes the importance of abstract mathematical
results, proving each of its theorems with full mathematical rigour. Such textbooks
are usually aimed at graduate students, and are inappropriate for the average un-
dergraduate. Secondly, there is the type of textbook that shows the student how
to construct solutions of diﬀerential equations, with particular emphasis on algo-
rithmic methods. These textbooks often tackle only linear equations, and have no
pretension to mathematical rigour. However, they are usually well-stocked with
interesting examples, and often include sections on numerical solution methods.
In this textbook, we steer a course between these two extremes, starting at the
level of preparedness of a typical, but well-motivated, second year undergraduate
at a British university. As such, the book begins in an unsophisticated style with
the clear objective of obtaining quantitative results for a particular linear ordi-
nary diﬀerential equation. The text is, however, written in a progressive manner,
with the aim of developing a deeper understanding of ordinary and partial diﬀer-
ential equations, including conditions for the existence and uniqueness of solutions,
solutions by group theoretical and asymptotic methods, the basic ideas of con-
trol theory, and nonlinear systems, including bifurcation theory and chaos. The
emphasis of the book is on analytical and asymptotic solution methods. However,
where appropriate, we have supplemented the text by including numerical solutions
and graphs produced using MATLAB†, version 6. We assume some knowledge of
† MATLAB is a registered trademark of The MathWorks, Inc.

x
PREFACE
MATLAB (summarized in Appendix 7), but explain any nontrivial aspects as they
arise. Where mathematical rigour is required, we have presented the appropriate
analysis, on the basis that the student has taken ﬁrst courses in analysis and linear
algebra. We have, however, avoided any functional analysis. Most of the material
in the book has been taught by us in courses for undergraduates at the University
of Birmingham. This has given us some insight into what students ﬁnd diﬃcult,
and, as a consequence, what needs to be emphasized and re-iterated.
The book is divided into two parts. In the ﬁrst of these, we tackle linear diﬀer-
ential equations. The ﬁrst three chapters are concerned with variable coeﬃcient,
linear, second order ordinary diﬀerential equations, emphasizing the methods of
reduction of order and variation of parameters, and series solution by the method
of Frobenius. In particular, we discuss Legendre functions (Chapter 2) and Bessel
functions (Chapter 3) in detail, and motivate this by giving examples of how they
arise in real modelling problems. These examples lead to partial diﬀerential equa-
tions, and we use separation of variables to obtain Legendre’s and Bessel’s equa-
tions. In Chapter 4, the emphasis is on boundary value problems, and we show
how these diﬀer from initial value problems. We introduce Sturm–Liouville theory
in this chapter, and prove various results on eigenvalue problems. The next two
chapters of the ﬁrst part of the book are concerned with Fourier series, and Fourier
and Laplace transforms. We discuss in detail the convergence of Fourier series, since
the analysis involved is far more straightforward than that associated with other
basis functions. Our approach to Fourier transforms involves a short introduction
to the theory of generalized functions. The advantage of this approach is that a
discussion of what types of function possess a Fourier transform is straightforward,
since all generalized functions possess a Fourier transform. We show how Fourier
transforms can be used to construct the free space Green’s function for both ordi-
nary and partial diﬀerential equations. We also use Fourier transforms to derive
the solutions of the Dirichlet and Neumann problems for Laplace’s equation. Our
discussion of the Laplace transform includes an outline proof of the inversion the-
orem, and several examples of physical problems, for example involving diﬀusion,
that can be solved by this method. In Chapter 7 we discuss the classiﬁcation of
linear, second order partial diﬀerential equations, emphasizing the reasons why the
canonical examples of elliptic, parabolic and hyperbolic equations, namely Laplace’s
equation, the diﬀusion equation and the wave equation, have the properties that
they do. We also consider complex variable methods for solving Laplace’s equation,
emphasizing their application to problems in ﬂuid mechanics.
The second part of the book is concerned with nonlinear problems and more
advanced techniques. Although we have used a lot of the material in Chapters 9
and 14 (phase plane techniques and control theory) in a course for second year
undergraduates, the bulk of the material here is aimed at third year students. We
begin in Chapter 8 with a brief introduction to the rigorous analysis of ordinary
diﬀerential equations.
Here the emphasis is on existence, uniqueness and com-
parison theorems. In Chapter 9 we introduce the phase plane and its associated
techniques. This is the ﬁrst of three chapters (the others being Chapters 13 and 15)
that form an introduction to the theory of nonlinear ordinary diﬀerential equations,

PREFACE
xi
often known as dynamical systems. In Chapter 10, we show how the ideas of group
theory can be used to ﬁnd exact solutions of ordinary and partial diﬀerential equa-
tions. In Chapters 11 and 12 we discuss the theory and practice of asymptotic
analysis. After discussing the basic ideas at the beginning of Chapter 11, we move
on to study the three most important techniques for the asymptotic evaluation of
integrals: Laplace’s method, the method of stationary phase and the method of
steepest descents. Chapter 12 is devoted to the asymptotic solution of diﬀerential
equations, and we introduce the method of matched asymptotic expansions, and
the associated idea of asymptotic matching, the method of multiple scales, includ-
ing Kuzmak’s method for analysing the slow damping of nonlinear oscillators, and
the WKB expansion. We illustrate each of these methods with a wide variety of
examples, for both nonlinear ordinary diﬀerential equations and partial diﬀerential
equations. In Chapter 13 we cover the centre manifold theorem, Lyapunov func-
tions and an introduction to bifurcation theory. Chapter 14 is about time-optimal
control theory in the phase plane, and includes a discussion of the controllability
matrix and the time-optimal maximum principle for second order linear systems of
ordinary diﬀerential equations. Chapter 15 is on chaotic systems, and, after some
illustrative examples, emphasizes the theory of homoclinic tangles and Mel’nikov
theory.
There is a set of exercises at the end of each chapter.
Harder exercises are
marked with a star, and many chapters include a project, which is rather longer
than the average exercise, and whose solution involves searches in the library or on
the Internet, and deeper study. Bona ﬁde teachers and instructors can obtain full
worked solutions to many of the exercises by emailing solutions@cambridge.org.
In order to follow many of the ideas and calculations that we describe in this
book, and to fully appreciate the more advanced material, the reader may need
to acquire (or refresh) some basic skills.
These are covered in the appendices,
and fall into six basic areas: linear algebra, continuity and diﬀerentiability, power
series, sequences and series of functions, ordinary diﬀerential equations and complex
variables.
We would like to thank our friends and colleagues, Adam Burbidge (Nestl´e Re-
search Centre, Lausanne), Norrie Everitt (Birmingham), Chris Good (Birming-
ham), Ray Jones (Birmingham), John King (Nottingham), Dave Needham (Read-
ing), Nigel Scott (East Anglia) and Warren Smith (Birmingham), who read and
commented on one or more chapters of the book before it was published.
Any
nonsense remaining is, of course, our fault and not theirs.
ACK, JB and SRO, Birmingham 2002

Part One
Linear Equations
1

CHAPTER ONE
Variable Coeﬃcient, Second Order, Linear,
Ordinary Diﬀerential Equations
Many physical, chemical and biological systems can be described using mathemat-
ical models. Once the model is formulated, we usually need to solve a diﬀerential
equation in order to predict and quantify the features of the system being mod-
elled. As a precursor to this, we consider linear, second order ordinary diﬀerential
equations of the form
P(x)d 2y
dx2 + Q(x)dy
dx + R(x)y = F(x),
with P(x), Q(x) and R(x) ﬁnite polynomials that contain no common factor. This
equation is inhomogeneous and has variable coeﬃcients. The form of these poly-
nomials varies according to the underlying physical problem that we are studying.
However, we will postpone any discussion of the physical origin of such equations
until we have considered some classical mathematical models in Chapters 2 and 3.
After dividing through by P(x), we obtain the more convenient, equivalent form,
d 2y
dx2 + a1(x)dy
dx + a0(x)y = f(x).
(1.1)
This process is mathematically legitimate, provided that P(x) ̸= 0. If P(x0) = 0
at some point x = x0, it is not legitimate, and we call x0 a singular point of the
equation. If P(x0) ̸= 0, x0 is a regular or ordinary point of the equation. If
P(x) ̸= 0 for all points x in the interval where we want to solve the equation, we
say that the equation is nonsingular, or regular, on the interval.
We usually need to solve (1.1) subject to either initial conditions of the form
y(a) = α, y′(a) = β or boundary conditions, of which y(a) = α and y(b) = β
are typical examples. It is worth reminding ourselves that, given the ordinary dif-
ferential equation and initial conditions (an initial value problem), the objective
is to determine the solution for other values of x, typically, x > a, as illustrated in
Figure 1.1. As an example, consider a projectile. The initial conditions are the po-
sition of the projectile and the speed and angle to the horizontal at which it is ﬁred.
We then want to know the path of the projectile, given these initial conditions.
For initial value problems of this form, it is possible to show that:
(i) If a1(x), a0(x) and f(x) are continuous on some open interval I that contains
the initial point a, a unique solution of the initial value problem exists on
the interval I, as we shall demonstrate in Chapter 8.

4
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
calculate the
solution for x > a
given y(a) = α
and y'(a) = β
y
α
a
x
Fig. 1.1. An initial value problem.
(ii) The structure of the solution of the initial value problem is of the form
y =
A u1(x) + B u2(x)



Complementary function
+
G(x)
  
Particular integral
,
where A, B are constants that are ﬁxed by the initial conditions and u1(x)
and u2(x) are linearly independent solutions of the corresponding homoge-
neous problem y′′ + a1(x)y′ + a0(x)y = 0.
These results can be proved rigorously, but nonconstructively, by studying the
operator
Ly ≡d 2y
dx2 + a1(x)dy
dx + a0(x)y,
and regarding L : C2(I) →C0(I) as a linear transformation from the space of
twice-diﬀerentiable functions deﬁned on the interval I to the space of continuous
functions deﬁned on I. The solutions of the homogeneous equation are elements
of the null space of L.
This subspace is completely determined once its basis
is known.
The solution of the inhomogeneous problem, Ly = f, is then given
formally as y = L−1f. Unfortunately, if we actually want to construct the solution
of a particular equation, there is a lot more work to do.
Before we try to construct the general solution of the inhomogeneous initial value
problem, we will outline a series of subproblems that are more tractable.

1.1 THE METHOD OF REDUCTION OF ORDER
5
1.1
The Method of Reduction of Order
As a ﬁrst simpliﬁcation we discuss the solution of the homogeneous diﬀerential
equation
d 2y
dx2 + a1(x)dy
dx + a0(x)y = 0,
(1.2)
on the assumption that we know one solution, say y(x) = u1(x), and only need to
ﬁnd the second solution. We will look for a solution of the form y(x) = U(x)u1(x).
Diﬀerentiating y(x) using the product rule gives
dy
dx = dU
dx u1 + U du1
dx ,
d 2y
dx2 = d 2U
dx2 u1 + 2dU
dx
du1
dx + U d 2u1
dx2 .
If we substitute these expressions into (1.2) we obtain
d 2U
dx2 u1 + 2dU
dx
du1
dx + U d 2u1
dx2 + a1(x)
dU
dx u1 + U du1
dx

+ a0(x)Uu1 = 0.
We can now collect terms to get
U
d 2u1
dx2 + a1(x)du1
dx + a0(x)u1

+ u1
d 2U
dx2 + dU
dx

2du1
dx + a1u1

= 0.
Now, since u1(x) is a solution of (1.2), the term multiplying U is zero. We have
therefore obtained a diﬀerential equation for dU/dx, and, by deﬁning Z = dU/dx,
have
u1
dZ
dx + Z

2du1
dx + a1u1

= 0.
Dividing through by Zu1 we have
1
Z
dZ
dx + 2
u1
du1
dx + a1 = 0,
which can be integrated directly to yield
log |Z| + 2 log |u1| +
 x
a1(s) ds = C,
where s is a dummy variable, for some constant C. Thus
Z = c
u2
1
exp

−
 x
a1(s) ds
	
= dU
dx
where c = eC. This can then be integrated to give
U(x) =
 x
c
u2
1(t) exp

−
 t
a1(s) ds
	
dt + ˜c,
for some constant ˜c. The solution is therefore

6
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
y(x) = u1(x)
 x
c
u2
1(t) exp

−
 t
a1(s) ds
	
dt + ˜cu1(x).
We can recognize ˜cu1(x) as the part of the complementary function that we knew
to start with, and
u2(x) = u1(x)
 x
1
u2
1(t) exp

−
 t
a1(s)ds
	
dt
(1.3)
as the second part of the complementary function. This result is called the reduc-
tion of order formula.
Example
Let’s try to determine the full solution of the diﬀerential equation
(1 −x2)d 2y
dx2 −2xdy
dx + 2y = 0,
given that y = u1(x) = x is a solution. We ﬁrstly write the equation in standard
form as
d 2y
dx2 −
2x
1 −x2
dy
dx +
2
1 −x2 y = 0.
Comparing this with (1.2), we have a1(x) = −2x/(1 −x2). After noting that
 t
a1(s) ds =
 t
−
2s
1 −s2 ds = log(1 −t2),
the reduction of order formula gives
u2(x) = x
 x 1
t2 exp

−log(1 −t2)

dt = x
 x
dt
t2(1 −t2).
We can express the integrand in terms of its partial fractions as
1
t2(1 −t2) = 1
t2 +
1
1 −t2 = 1
t2 +
1
2(1 + t) +
1
2(1 −t).
This gives the second solution of (1.2) as
u2(x) = x
 x  1
t2 +
1
2(1 + t) +
1
2(1 −t)
	
dt
= x

−1
t + 1
2 log
1 + t
1 −t
x
= x
2 log
1 + x
1 −x

−1,
and hence the general solution is
y = Ax + B
x
2 log
1 + x
1 −x

−1
	
.

1.2 THE METHOD OF VARIATION OF PARAMETERS
7
1.2
The Method of Variation of Parameters
Let’s now consider how to ﬁnd the particular integral given the complementary
function, comprising u1(x) and u2(x). As the name of this technique suggests, we
take the constants in the complementary function to be variable, and assume that
y = c1(x)u1(x) + c2(x)u2(x).
Diﬀerentiating, we ﬁnd that
dy
dx = c1
du1
dx + u1
dc1
dx + c2
du2
dx + u2
dc2
dx .
We will choose to impose the condition
u1
dc1
dx + u2
dc2
dx = 0,
(1.4)
and thus have
dy
dx = c1
du1
dx + c2
du2
dx ,
which, when diﬀerentiated again, yields
d 2y
dx2 = c1
d 2u1
dx2 + du1
dx
dc1
dx + c2
d 2u2
dx2 + du2
dx
dc2
dx .
This form can then be substituted into the original diﬀerential equation to give
c1
d 2u1
dx2 + du1
dx
dc1
dx + c2
d 2u2
dx2 + du2
dx
dc2
dx + a1

c1
du1
dx + c2
du2
dx

+ a0 (c1u1 + c2u2) = f.
This can be rearranged to show that
c1
d 2u1
dx2 + a1
du1
dx + a0u1

+ c2
d 2u2
dx2 + a1
du2
dx + a0u2

+ du1
dx
dc1
dx + du2
dx
dc2
dx = f.
Since u1 and u2 are solutions of the homogeneous equation, the ﬁrst two terms are
zero, which gives us
du1
dx
dc1
dx + du2
dx
dc2
dx = f.
(1.5)
We now have two simultaneous equations, (1.4) and (1.5), for c′
1 = dc1/dx and
c′
2 = dc2/dx, which can be written in matrix form as
 u1
u2
u′
1
u′
2
  c′
1
c′
2

=
 0
f

.
These can easily be solved to give
c′
1 = −fu2
W ,
c′
2 = fu1
W ,
where
W = u1u′
2 −u2u′
1 =

u1
u2
u′
1
u′
2


8
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
is called the Wronskian. These expressions can be integrated to give
c1 =
 x
−f(s)u2(s)
W(s)
ds + A,
c2 =
 x f(s)u1(s)
W(s)
ds + B.
We can now write down the solution of the entire problem as
y(x) = u1(x)
 x
−f(s)u2(s)
W(s)
ds + u2(x)
 x f(s)u1(s)
W(s)
ds + Au1(x) + Bu2(x).
The particular integral is therefore
y(x) =
 x
f(s)
u1(s)u2(x) −u1(x)u2(s)
W(s)
	
ds.
(1.6)
This is called the variation of parameters formula.
Example
Consider the equation
d 2y
dx2 + y = x sin x.
The homogeneous form of this equation has constant coeﬃcients, with solutions
u1(x) = cos x,
u2(x) = sin x.
The variation of parameters formula then gives the particular integral as
y =
 x
s sin s
cos s sin x −cos x sin s
1
	
ds,
since
W =

cos x
sin x
−sin x
cos x
 = cos2 x + sin2 x = 1.
We can split the particular integral into two integrals as
y(x) = sin x
 x
s sin s cos s ds −cos x
 x
s sin2 s ds
= 1
2 sin x
 x
s sin 2s ds −1
2 cos x
 x
s (1 −cos 2s) ds.
Using integration by parts, we can evaluate this, and ﬁnd that
y(x) = −1
4x2 cos x + 1
4x sin x + 1
8 cos x
is the required particular integral. The general solution is therefore
y = c1 cos x + c2 sin x −1
4x2 cos x + 1
4x sin x.
Although we have given a rational derivation of the reduction of order and vari-
ation of parameters formulae, we have made no comment so far about why the
procedures we used in the derivation should work at all! It turns out that this has
a close connection with the theory of continuous groups, which we will investigate
in Chapter 10.

1.2 THE METHOD OF VARIATION OF PARAMETERS
9
1.2.1
The Wronskian
Before we carry on, let’s pause to discuss some further properties of the Wronskian.
Recall that if V is a vector space over R, then two elements v1, v2 ∈V are linearly
dependent if ∃α1, α2 ∈R, with α1 and α2 not both zero, such that α1v1+α2v2 = 0.
Now let V = C1(a, b) be the set of once-diﬀerentiable functions over the interval
a < x < b. If u1, u2 ∈C1(a, b) are linearly dependent, ∃α1, α2 ∈R such that
α1u1(x) + α2u2(x) = 0 ∀x ∈(a, b). Notice that, by direct diﬀerentiation, this also
gives α1u′
1(x) + α2u′
2(x) = 0 or, in matrix form,
 u1(x)
u2(x)
u′
1(x)
u′
2(x)
  α1
α2

=
 0
0

.
These are homogeneous equations of the form
Ax = 0,
which only have nontrivial solutions if det(A) = 0, that is
W =

u1(x)
u2(x)
u′
1(x)
u′
2(x)
 = u1u′
2 −u′
1u2 = 0.
In other words, the Wronskian of two linearly dependent functions is identically
zero on (a, b). The contrapositive of this result is that if W ̸≡0 on (a, b), then u1
and u2 are linearly independent on (a, b).
Example
The functions u1(x) = x2 and u2(x) = x3 are linearly independent on the interval
(−1, 1). To see this, note that, since u1(x) = x2, u2(x) = x3, u′
1(x) = 2x, and
u′
2(x) = 3x2, the Wronskian of these two functions is
W =

x2
x3
2x
3x2
 = 3x4 −2x4 = x4.
This quantity is not identically zero, and hence x2 and x3 are linearly independent
on (−1, 1).
Example
The functions u1(x) = f(x) and u2(x) = kf(x), with k a constant, are linearly
dependent on any interval, since their Wronskian is
W =

f
kf
f ′
kf ′
 = 0.
If the functions u1 and u2 are solutions of (1.2), we can show by diﬀerentiating
W = u1u′
2 −u′
1u2 directly that
dW
dx + a1(x)W = 0.

10
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
This ﬁrst order diﬀerential equation has solution
W(x) = W(x0) exp

−
 x
x0
a1(t)dt
	
,
(1.7)
which is known as Abel’s formula.
This gives us an easy way of ﬁnding the
Wronskian of the solutions of any second order diﬀerential equation without having
to construct the solutions themselves.
Example
Consider the equation
y′′ + 1
xy′ +

1 −1
x2

y = 0.
Using Abel’s formula, this has Wronskian
W(x) = W(x0) exp

−
 x
x0
dt
t
	
= x0W(x0)
x
= A
x
for some constant A. To ﬁnd this constant, it is usually necessary to know more
about the solutions u1(x) and u2(x). We will describe a technique for doing this in
Section 1.3.
We end this section with a couple of useful theorems.
Theorem 1.1 If u1 and u2 are linearly independent solutions of the homoge-
neous, nonsingular ordinary diﬀerential equation (1.2), then the Wronskian is either
strictly positive or strictly negative.
Proof
From Abel’s formula, and since the exponential function does not change
sign, the Wronskian is identically positive, identically negative or identically zero.
We just need to exclude the possibility that W is ever zero. Suppose that W(x1) =
0.
The vectors
 u1(x1)
u′
1(x1)

and
 u2(x1)
u′
2(x1)

are then linearly dependent, and
hence u1(x1) = ku2(x1) and u′
1(x) = ku′
2(x) for some constant k. The function
u(x) = u1(x)−ku2(x) is also a solution of (1.2) by linearity, and satisﬁes the initial
conditions u(x1) = 0, u′(x1) = 0. Since (1.2) has a unique solution, the obvious
solution, u ≡0, is the only solution. This means that u1 ≡ku2. Hence u1 and u2
are linearly dependent – a contradiction.
The nonsingularity of the diﬀerential equation is crucial here. If we consider the
equation x2y′′ −2xy′ + 2y = 0, which has u1(x) = x2 and u2(x) = x as its linearly
independent solutions, the Wronksian is −x2, which vanishes at x = 0. This is
because the coeﬃcient of y′′ also vanishes at x = 0.
Theorem 1.2 (The Sturm separation theorem) If u1(x) and u2(x) are the
linearly independent solutions of a nonsingular, homogeneous equation, (1.2), then

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
11
the zeros of u1(x) and u2(x) occur alternately. In other words, successive zeros of
u1(x) are separated by successive zeros of u2(x) and vice versa.
Proof
Suppose that x1 and x2 are successive zeros of u2(x), so that W(xi) =
u1(xi)u′
2(xi) for i = 1 or 2. We also know that W(x) is of one sign on [x1, x2],
since u1(x) and u2(x) are linearly independent. This means that u1(xi) and u′
2(xi)
are nonzero. Now if u′
2(x1) is positive then u′
2(x2) is negative (or vice versa), since
u2(x2) is zero. Since the Wronskian cannot change sign between x1 and x2, u1(x)
must change sign, and hence u1 has a zero in [x1, x2], as we claimed.
As an example of this, consider the equation y′′+ω2y = 0, which has solution y =
A sin ωx + B cos ωx. If we consider any two of the zeros of sin ωx, it is immediately
clear that cos ωx has a zero between them.
1.3
Solution by Power Series: The Method of Frobenius
Up to this point, we have considered ordinary diﬀerential equations for which we
know at least one solution of the homogeneous problem. From this we have seen that
we can easily construct the second independent solution and, in the inhomogeneous
case, the particular integral.
We now turn our attention to the more diﬃcult
case, in which we cannot determine a solution of the homogeneous problem by
inspection. We must devise a method that is capable of solving variable coeﬃcient
ordinary diﬀerential equations in general. As we noted at the start of the chapter,
we will restrict our attention to the case where the variable coeﬃcients are simple
polynomials. This suggests that we can look for a solution of the form
y = xc
∞

n=0
anxn =
∞

n=0
anxn+c,
(1.8)
and hence
dy
dx =
∞

n=0
an(n + c)xn+c−1,
(1.9)
d 2y
dx2 =
∞

n=0
an(n + c)(n + c −1)xn+c−2,
(1.10)
where the constants c, a0, a1, . . . , are as yet undetermined. This is known as the
method of Frobenius. Later on, we will give some idea of why and when this
method can be used. For the moment, we will just try to make it work. We proceed
by example, with the simplest case ﬁrst.
1.3.1
The Roots of the Indicial Equation Diﬀer by an Integer
Consider the equation
x2 d 2y
dx2 + xdy
dx +

x2 −1
4

y = 0.
(1.11)

12
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
We substitute (1.8) to (1.10) into (1.11), which gives
x2
∞

n=0
an(n + c)(n + c −1)xn+c−2 + x
∞

n=0
an(n + c)xn+c−1
+

x2 −1
4
 ∞

n=0
anxn+c = 0.
We can rearrange this slightly to obtain
∞

n=0
an

(n + c)(n + c −1) + (n + c) −1
4
	
xn+c +
∞

n=0
anxn+c+2 = 0,
and hence, after simplifying the terms in the ﬁrst summation,
∞

n=0
an

(n + c)2 −1
4
	
xn+c +
∞

n=0
anxn+c+2 = 0.
We now extract the ﬁrst two terms from the ﬁrst summation to give
a0

c2 −1
4

xc + a1

(c + 1)2 −1
4
	
xc+1
+
∞

n=2
an

(n + c)2 −1
4
	
xn+c +
∞

n=0
anxn+c+2 = 0.
(1.12)
Notice that the ﬁrst term is the only one containing xc and similarly for the second
term in xc+1.
The two summations in (1.12) begin at the same power of x, namely x2+c. If we
let m = n + 2 in the last summation (notice that if n = 0 then m = 2, and n = ∞
implies that m = ∞), (1.12) becomes
a0

c2 −1
4

xc + a1

(c + 1)2 −1
4
	
xc+1
+
∞

n=2
an

(n + c)2 −1
4
	
xn+c +
∞

m=2
am−2xm+c = 0.
Since the variables in the summations are merely dummy variables,
∞

m=2
am−2xm+c =
∞

n=2
an−2xn+c,
and hence
a0

c2 −1
4

xc + a1

(c + 1)2 −1
4
	
xc+1
+
∞

n=2
an

(n + c)2 −1
4
	
xn+c +
∞

n=2
an−2xn+c = 0.

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
13
Since the last two summations involve identical powers of x, we can combine them
to obtain
a0

c2 −1
4

xc + a1

(c + 1)2 −1
4
	
xc+1
+
∞

n=2

an

(n + c)2 −1
4
	
+ an−2

xn+c = 0.
(1.13)
Although the operations above are straightforward, we need to take some care to
avoid simple slips.
Since (1.13) must hold for all values of x, the coeﬃcient of each power of x must
be zero. The coeﬃcient of xc is therefore
a0

c2 −1
4

= 0.
Up to this point, most Frobenius analysis is very similar. It is here that the diﬀerent
structures come into play. If we were to use the solution a0 = 0, the series (1.8)
would have a1xc+1 as its ﬁrst term. This is just equivalent to increasing c by 1. We
therefore assume that a0 ̸= 0, which means that c must satisfy c2 −1
4 = 0. This is
called the indicial equation, and implies that c = ± 1
2. Now, progressing to the
next term, proportional to xc+1, we ﬁnd that
a1

(c + 1)2 −1
4
	
= 0.
Choosing c = 1
2 gives a1 = 0, and, if we were to do this, we would ﬁnd that we had
constructed a solution with one arbitrary constant. However, if we choose c = −1
2
the indicial equation is satisﬁed for arbitrary values of a1, and a1 will act as the
second arbitrary constant for the solution. In order to generate this more general
solution, we therefore let c = −1
2.
We now progress to the individual terms in the summation. The general term
yields
an

n −1
2
2
−1
4

+ an−2 = 0
for n = 2, 3, . . . .
This is called a recurrence relation. We solve it by observation as follows. We
start by rearranging to give
an = −
an−2
n(n −1).
(1.14)
By putting n = 2 in (1.14) we obtain
a2 = −a0
2 · 1.
For n = 3,
a3 = −a1
3 · 2.

14
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
For n = 4,
a4 = −a2
4 · 3,
and substituting for a2 in terms of a0 gives
a4 = −1
4 · 3

−a0
2 · 1

=
a0
4 · 3 · 2 · 1 = a0
4! .
Similarly for n = 5, using the expression for a3 in terms of a1,
a5 = −a3
5 · 4 = −1
5 · 4

−a1
3 · 2

= a1
5! .
A pattern is emerging here and we propose that
a2n = (−1)n a0
(2n)!,
a2n+1 = (−1)n
a1
(2n + 1)!.
(1.15)
This can be proved in a straightforward manner by induction, although we will not
dwell upon the details here.†
We can now deduce the full solution. Starting from (1.8), we substitute c = −1
2,
and write out the ﬁrst few terms in the summation
y = x−1/2(a0 + a1x + a2x2 + · · · ).
Now, using the forms of the even and odd coeﬃcients given in (1.15),
y = x−1/2

a0 + a1x −a0x2
2!
−a1x3
3!
+ a0x4
4!
+ a1x5
5!
+ · · ·

.
This series splits naturally into two proportional to a0 and a1, namely
y = x−1/2a0

1 −x2
2! + x4
4! −· · ·

+ x−1/2a1

x −x3
3! + x5
5! −· · ·

.
The solution is therefore
y(x) = a0
cos x
x1/2 + a1
sin x
x1/2 ,
since we can recognize the Taylor series expansions for sine and cosine.
This particular diﬀerential equation is an example of the use of the method of
Frobenius, formalized by
Frobenius General Rule I
If the indicial equation has two distinct roots,
c = α, β (α < β), whose diﬀerence is an in-
teger, and one of the coeﬃcients of xk becomes
indeterminate on putting c = α, both solutions
can be generated by putting c = α in the recur-
rence relation.
† In the usual way, we must show that (1.15) is true for n = 0 and that, when the value of a2n+1
is substituted into the recurrence relation, we obtain a2(n+1)+1, as given by substituting n + 1
for n in (1.15).

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
15
In the above example the indicial equation was c2 −1
4 = 0, which has solutions c =
± 1
2, whose diﬀerence is an integer. The coeﬃcient of xc+1 was a1

(c + 1)2 −1
4

=
0. When we choose the lower of the two values (c = −1
2) this expression does not
give us any information about the constant a1, in other words a1 is indeterminate.
1.3.2
The Roots of the Indicial Equation Diﬀer by a Noninteger
Quantity
We now consider the diﬀerential equation
2x(1 −x)d 2y
dx2 + (1 −x)dy
dx + 3y = 0.
(1.16)
As before, let’s assume that the solution can be written as the power series (1.8).
As in the previous example, this can be diﬀerentiated and substituted into the
equation to yield
2x(1 −x)
∞

n=0
an(n + c)(n + c −1)xn+c−2 + (1 −x)
∞

n=0
an(n + c)xn+c−1
+3
∞

n=0
anxn+c = 0.
The various terms can be multiplied out, which gives us
∞

n=0
an(n + c)(n + c −1)2xn+c−1 −
∞

n=0
an(n + c)(n + c −1)2xn+c
+
∞

n=0
an(n + c)xn+c−1 −
∞

n=0
an(n + c)xn+c + 3
∞

n=0
anxn+c = 0.
Collecting similar terms gives
∞

n=0
an{2(n + c)(n + c −1) + (n + c)}xn+c−1
+
∞

n=0
an{3 −2(n + c)(n + c −1) −(n + c)}xn+c = 0,
and hence
∞

n=0
an(n + c)(2n + 2c −1)xn+c−1 +
∞

n=0
an{3 −(n + c)(2n + 2c −1)}xn+c = 0.
We now extract the ﬁrst term from the left hand summation so that both summa-
tions start with a term proportional to xc. This gives
a0c(2c −1)xc−1 +
∞

n=1
an(n + c)(2n + 2c −1)xn+c−1

16
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
+
∞

n=0
an{3 −(n + c)(2n + 2c −1)}xn+c = 0.
We now let m = n + 1 in the second summation, which then becomes
∞

m=1
am−1{3 −(m −1 + c)(2(m −1) + 2c −1)}xm+c−1.
We again note that m is merely a dummy variable which for ease we rewrite as n,
which gives
a0c(2c −1)xc−1 +
∞

n=1
an(n + c)(2n + 2c −1)xn+c−1
+
∞

n=1
an−1 {3 −(n −1 + c)(2n + 2c −3)} xn+c−1 = 0.
Finally, we can combine the two summations to give
a0c(2c −1)xc−1
+
∞

n=1
{an(n + c)(2n + 2c −1) + an−1{3 −(n −1 + c)(2n + 2c −3)}}xn+c−1 = 0.
As in the previous example we can now consider the coeﬃcients of successive
powers of x. We start with the coeﬃcient of xc−1, which gives the indicial equation,
a0c(2c −1) = 0. Since a0 ̸= 0, this implies that c = 0 or c = 1
2. Notice that these
roots do not diﬀer by an integer. The general term in the summation shows that
an = an−1
(n + c −1)(2n + 2c −3) −3
(n + c)(2n + 2c −1)
	
for n = 1, 2, . . . .
(1.17)
We now need to solve this recurrence relation, considering each root of the indicial
equation separately.
Case I: c = 0
In this case, we can rewrite the recurrence relation (1.17) as
an = an−1
(n −1)(2n −3) −3
n(2n −1)
	
= an−1
 2n2 −5n
n(2n −1)
	
= an−1
2n −5
2n −1

.
We recall that this holds for n ⩾1, so we start with n = 1, which yields
a1 = a0

−3
1

= −3a0.
For n = 2
a2 = a1

−1
3

= −3a0

−1
3

= a0,

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
17
where we have used the expression for a1 in terms of a0. Now progressing to n = 3,
we have
a3 = a2
1
5

= a0
3
5 · 3,
and for n = 4,
a4 = a3
3
7

= a0
3
7 · 5.
Finally, for n = 5 we have
a5 = a4
5
9

= a0
3
9 · 7.
In general,
an =
3a0
(2n −1)(2n −3),
which again can be proved by induction. We conclude that one solution of the
diﬀerential equation is
y = xc
∞

n=0
anxn = x0
∞

n=0
3a0
(2n −1)(2n −3)xn.
This can be tidied up by putting 3a0 = A, so that the solution is
y = A
∞

n=0
xn
(2n −1)(2n −3).
(1.18)
Note that there is no obvious way of writing this solution in terms of elementary
functions. In addition, a simple application of the ratio test shows that this power
series is only convergent for |x| ⩽1, for reasons that we discuss below.
A simple MATLAB† function that evaluates (1.18) is




function frob = frob(x)
n = 100:-1:0; a = 1./(2*n-1)./(2*n-3);
frob = polyval(a,x);
which sums the ﬁrst 100 terms of the series. The function polyval evaluates the
polynomial formed by the ﬁrst 100 terms in the sum (1.18) in an eﬃcient manner.
Figure 1.2 can then be produced using the command ezplot(@frob,[-1,1]).
Although we could now use the method of reduction of order, since we have
constructed a solution, this would be very complicated. It is easier to consider the
second root of the indicial equation.
† See Appendix 7 for a short introduction to MATLAB.

18
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
Fig. 1.2. The solution of (1.16) given by (1.18).
Case II: c = 1
2
In this case, we simplify the recurrence relation (1.17) to give
an = an−1

n −1
2

(2n −2) −3

n + 1
2

2n

= an−1
2n2 −3n −2
2n2 + n

= an−1
(2n + 1)(n −2)
n(2n + 1)
	
= an−1
n −2
n

.
We again recall that this relation holds for n ⩾1 and start with n = 1, which gives
a1 = a0(−1). Substituting n = 2 gives a2 = 0 and, since all successive ai will be
written in terms of a2, ai = 0 for i = 2, 3, . . . . The second solution of the equation
is therefore y = Bx1/2(1−x). We can now use this simple solution in the reduction
of order formula, (1.3), to determine an analytical formula for the ﬁrst solution,
(1.18). For example, for 0 ⩽x ⩽1, we ﬁnd that (1.18) can be written as
y = −1
6A

3x −2 + 3x1/2 (1 −x) log

1 + x1/2
(1 −x)1/2

.
This expression has a logarithmic singularity in its derivative at x = 1, which
explains why the radius of convergence of the power series solution (1.18) is |x| ⩽1.
This diﬀerential equation is an example of the second major case of the method
of Frobenius, formalized by

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
19
Frobenius General Rule II
If the indicial equation has two distinct roots,
c = α, β (α < β), whose diﬀerence is not an
integer, the general solution of the equation is
found by successively substituting c = α then c =
β into the general recurrence relation.
1.3.3
The Roots of the Indicial Equation are Equal
Let’s try to determine the two solutions of the diﬀerential equation
xd 2y
dx2 + (1 + x)dy
dx + 2y = 0.
We substitute in the standard power series, (1.8), which gives
x
∞

n=0
an(n + c)(n + c −1)xn+c−2 + (1 + x)
∞

n=0
an(n + c)xn+c−1
+2
∞

n=0
anxn+c = 0.
This can be simpliﬁed to give
∞

n=0
an(n + c)2xn+c−1 +
∞

n=0
an(n + c + 2)xn+c = 0.
We can extract the ﬁrst term from the left hand summation to give
a0c2xc−1 +
∞

n=1
an(n + c)2xn+c−1 +
∞

n=0
an(n + c + 2)xn+c = 0.
Now shifting the series using m = n + 1 (and subsequently changing dummy vari-
ables from m to n) we have
a0c2xc−1 +
∞

n=1
{an(n + c)2 + an−1(n + c + 1)}xn+c = 0,
(1.19)
where we have combined the two summations. The indicial equation is c2 = 0
which has a double root at c = 0. We know that there must be two solutions, but
it appears that there is only one available to us. For the moment let’s see how far
we can get by setting c = 0. The recurrence relation is then
an = −an−1
n + 1
n2
for n = 1, 2, . . . .
When n = 1 we ﬁnd that
a1 = −a0
2
12 ,

20
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
and with n = 2,
a2 = −a1
3
22 = a0
3 · 2
12 · 22 .
Using n = 3 gives
a3 = −a2
4
32 = −a0
4 · 3 · 2
12 · 22 · 32 ,
and we conclude that
an = (−1)n (n + 1)!
(n!)2
a0 = (−1)n n + 1
n!
a0.
One solution is therefore
y = a0
∞

n=0
(−1)n (n + 1)
n!
xn,
which can also be written as
y = a0

x
∞

n=1
(−1)nxn−1
(n −1)!
+
∞

n=0
(−1)nxn
n!

= a0

−x
∞

m=0
(−1)mxm
m!
+ e−x

= a0(1 −x)e−x.
This solution is one that we could not have readily determined simply by inspection.
We could now use the method of reduction of order to ﬁnd the second solution, but
we will proceed with the method of Frobenius so that we can see how it works in
this case.
Consider (1.19), which we write out more fully as
xd 2y
dx2 + (1 + x)dy
dx + 2y =
a0c2xc−1 +
∞

n=1
{an(n + c)2 + an−1(n + c + 1)}xn+c = 0.
The best we can do at this stage is to set an(n+c)2 +an−1(n+c+1) = 0 for n ⩾1,
as this gets rid of most of the terms. This gives us an as a function of c for n ⩾1,
and leaves us with
xd 2y
dx2 + (1 + x)dy
dx + 2y = a0c2xc−1.
(1.20)
Let’s now take a partial derivative with respect to c, where we regard y as a function
of both x and c, making use of
d
dx = ∂
∂x,
∂
∂c
∂y
∂x

= ∂
∂x
∂y
∂c

.
This gives
x ∂2
∂x2
∂y
∂c

+ (1 + x) ∂
∂x
∂y
∂c

+ 2
∂y
∂c

= a0
∂
∂c(c2xc−1).

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
21
Notice that we have used the fact that a0 is independent of c. We need to be careful
when evaluating the right hand side of this expression. Diﬀerentiating using the
product rule we have
∂
∂c(c2xc−1) = c2 ∂
∂c(xc−1) + xc−1 ∂
∂c(c2).
We rewrite xc−1 as xc x−1 = ec log xx−1, so that we have
∂
∂c(c2xc−1) = c2 ∂
∂c(ec log xx−1) + xc−1 ∂
∂c(c2).
Diﬀerentiating the exponential gives
∂
∂c(c2xc−1) = c2(log x ec log x)x−1 + xc−12c,
which we can tidy up to give
∂
∂c(c2xc−1) = c2xc−1 log x + xc−12c.
Substituting this form back into the diﬀerential equation gives
x ∂2
∂x2
∂y
∂c

+ (1 + x) ∂
∂x
∂y
∂c

+ 2∂y
∂c = a0{c2xc−1 log x + xc−12c}.
Now letting c →0 gives
x ∂2
∂x2
∂y
∂c

c=0
+ (1 + x) ∂
∂x
∂y
∂c

c=0
+ 2 ∂y
∂c

c=0
= 0.
Notice that this procedure only works because (1.20) has a repeated root at c = 0.
We conclude that ∂y
∂c

c=0
is a second solution of our ordinary diﬀerential equation.
To construct this solution, we diﬀerentiate the power series (1.8) (carefully!) to
give
∂y
∂c = xc
∞

n=0
dan
dc xn +
∞

n=0
anxn xc log x,
using a similar technique as before to deal with the diﬀerentiation of xc with respect
to c. Note that, although a0 is not a function of c, the other coeﬃcients are. Putting
c = 0 gives
∂y
∂c

c=0
=
∞

n=0
dan
dc

c=0
xn + log x
∞

n=0
an|c=0 xn.
We therefore need to determine dan
dc

c=0
. We begin with the recurrence relation,
which is
an = −an−1(n + c + 1)
(n + c)2
.

22
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
Starting with n = 1 we ﬁnd that
a1 = −a0(c + 2)
(c + 1)2
,
whilst for n = 2,
a2 = −a1(c + 3)
(c + 2)2
,
and substituting for a1 in terms of a0 gives us
a2 = a0(c + 2)(c + 3)
(c + 1)2(c + 2)2 .
This process can be continued to give
an = (−1)na0
(c + 2)(c + 3) . . . (c + n + 1)
(c + 1)2(c + 2)2 . . . (c + n)2 ,
which we can write as
an = (−1)na0
n
j=1(c + j + 1)
n
j=1(c + j)
2 .
We now take the logarithm of this expression, recalling that the logarithm of a
product is the sum of the terms, which leads to
log(an)
=
log((−1)na0) + log


n

j=1
(c + j + 1)

−2 log


n

j=1
(c + j)


=
log((−1)na0) +
n

j=1
log(c + j + 1) −2
n

j=1
log(c + j).
Now diﬀerentiating with respect to c gives
1
an
dan
dc =
n

j=1
1
c + j + 1 −2
n

j=1
1
c + j ,
and setting c = 0 we have
 1
an
dan
dc

c=0
=
n

j=1
1
j + 1 −2
n

j=1
1
j .
Since we know an when c = 0, we can write
dan
dc

c=0
=
(−1)na0
n
j=1(j + 1)
n
j=1 j
2


n

j=1
1
j + 1 −2
n

j=1
1
j

,
=
(−1)na0
(n + 1)!
(n!)2


n+1

j=1
1
j −1 −2
n

j=1
1
j

.
In this expression, we have manipulated the products and written them as facto-
rials, changed the ﬁrst summation and removed the extra term that this incurs.

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
23
Simplifying, we obtain
dan
dc

c=0
= (−1)na0
(n + 1)
n!
(φ(n + 1) −2φ(n) −1) ,
where we have introduced the notation
φ(n) ≡
n

m=1
1
m.
(1.21)
The second solution is therefore
y = a0
 ∞

n=0
(−1)n (n + 1)
n!
{φ(n + 1) −2φ(n) −1}xn +
∞

n=0
(−1)n (n + 1)
n!
xn log x

.
This methodology is formalized in
Frobenius General Rule III
If the indicial equation has a double root, c = α,
one solution is obtained by putting c = α into the
recurrence relation.
The second independent solution is (∂y/∂c)c=α
where an = an(c) for the calculation.
There are several other cases that can occur in the method of Frobenius, which,
due to their complexity, we will not go into here. One method of dealing with these
is to notice that the method outlined in this chapter always produces a solution of
the form y = u1(x) =  ∞
n=0 anxn+c. This can be used in the reduction of order
formula, (1.3), to ﬁnd the second linearly independent solution. Of course, it is
rather diﬃcult to get all of u2(x) this way, but the ﬁrst few terms are usually easy
enough to compute by expanding for small x. Having got these, we can assume a
general series form for u2(x), and ﬁnd the coeﬃcients in the usual way.
Example
Let’s try to solve
x2y′′ + xy′ + (x2 −1)y = 0,
(1.22)
using the method of Frobenius. If we look for a solution in the usual form, y =
 ∞
n=0 anxn+c, we ﬁnd that
a0(c2 −1)xc + a1

(1 + c)2 −1

xc+1 +
∞

k=2
!
ak

(k + c)2 −1

+ ak−2
"
xk+c = 0.
The indicial equation has roots c = ±1, and, by choosing either of these, we ﬁnd
that a1 = 0. If we now look for the general solution of
ak = −
ak−2
(k + c)2 −1,
we ﬁnd that
a2 = −
a0
(2 + c)2 −1 = −
a0
(1 + c)(3 + c),

24
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
a4 = −
a2
(4 + c)2 −1 =
a0
(1 + c)(3 + c)2(5 + c),
and so on. This gives us a solution of the form
y(x, c) = a0xc

1 −
x2
(1 + c)(3 + c) +
x4
(1 + c)(3 + c)2(5 + c) −· · ·
	
.
Now, by choosing c = 1, we obtain one of the linearly independent solutions of
(1.22),
u1(x) = y(x, 1) = x

1 −x2
2 · 4 +
x4
2 · 42 · 6 −· · ·

.
However, if c = −1, the coeﬃcients a2n for n = 1, 2, . . . are singular.
In order to ﬁnd the structure of the second linearly independent solution, we use
the reduction of order formula, (1.3). Substituting for u1(x) gives
u2(x)
=
x

1 −x2
8 + · · ·
  x
1
t2

1 −t2
8 + · · ·
2 exp

−
 t 1
s ds

dt
=
x

1 −x2
8 + · · ·
  x 1
t2

1 + t2
4 + · · ·
 1
t dt
=
x

1 −x2
8 + · · ·
 
−1
2x2 + 1
4 log x + · · ·

.
The second linearly independent solution of (1.22) therefore has the structure
u2(x) = 1
4u1(x) log x −1
2xv(x),
where v(x) = 1 + b2x2 + b4x4 + · · · . If we assume a solution structure of this form
and substitute it into (1.22), it is straightforward to pick oﬀthe coeﬃcients b2n.
Finally, note that we showed in Section 1.2.1 that the Wronskian of (1.22) is
W = A/x for some constant A.
Now, since we know that u1 = x + · · · and
u2 = −1/2x+· · · , we must have W = x(1/2x2)+1/2x+· · · = 1/x+· · · , and hence
A = 1.
1.3.4
Singular Points of Diﬀerential Equations
In this section, we give some deﬁnitions and a statement of a theorem that tells
us when the method of Frobenius can be used, and for what values of x the inﬁnite
series will converge. We consider a second order, variable coeﬃcient diﬀerential
equation of the form
P(x)d 2y
dx2 + Q(x)dy
dx + R(x)y = 0.
(1.23)
Before we proceed, we need to further reﬁne our ideas about singular points. If x0
is a singular point and (x−x0)Q(x)/P(x) and (x−x0)2R(x)/P(x) have convergent

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
25
Taylor series expansions about x0, then x = x0 is called a regular singular point;
otherwise, x0 is called an irregular singular point.
Example
Consider the equation
x(x −1)d 2y
dx2 + (1 + x)dy
dx + y = 0.
(1.24)
There are singular points where x2 −x = 0, at x = 0 and x = 1. Let’s start by
looking at the singular point x = 0. Consider the expression
xQ
P
= x(1 + x)
x2 −x
= (1 + x)
(x −1) = −(1 + x)(1 −x)−1.
Upon expanding (1 −x)−1 using the binomial expansion we have
xQ
P
= −(1 + x)(1 + x + x2 + · · · + xn + · · · ),
which can be multiplied out to give
xQ
P
= −1 −2x + · · · .
This power series is convergent provided |x| < 1 (by considering the binomial
expansion used above). Now
x2R
P
=
x2
x2 −x =
x
x −1 = −x(1 −x)−1.
Again, using the binomial expansion, which is convergent provided |x| < 1,
x2R
P
= −x(1 + x + x2 + · · · ) = −x −x2 −x3 −· · · .
Since xQ/P and x2R/P have convergent Taylor series about x = 0, this is a regular
singular point.
Now consider the other singular point, x = 1. We note that
(x −1)Q
P = (x −1)(1 + x)
(x2 −x)
= (x −1)(1 + x)
x(x −1)
= 1 + x
x
.
At this point we need to recall that we want information near x = 1, so we rewrite
x as 1 −(1 −x), and hence
(x −1)Q
P =
2 −(1 −x)
{1 −(1 −x)}.
Expanding in powers of (1 −x) using the binomial expansion gives
(x −1)Q
P = 2

1 −1 −x
2

{1 + (1 −x) + (1 −x)2 + · · · },

26
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
which is a power series in (x −1) that is convergent provided |x −1| < 1. We also
need to consider
(x −1)2 R
P = (x −1)2
x2 −x = (x −1)2
x(x −1) = x −1
x
=
x −1
{1 −(1 −x)}
= (x −1){1 −(1 −x)}−1 = (x −1){1 + (x −1) + (x −1)2 + · · · },
which is again convergent provided |x−1| < 1. Therefore x = 1 is a regular singular
point.
Theorem 1.3 If x0 is an ordinary point of the ordinary diﬀerential equation (1.23),
then there exists a unique series solution in the neighbourhood of x0 which converges
for |x −x0| < ρ, where ρ is the smaller of the radii of convergence of the series for
Q(x)/P(x) and R(x)/P(x).
If x0 is a regular singular point, then there exists a unique series solution in the
neighbourhood of x0, which converges for |x −x0| < ρ, where ρ is the smaller of the
radii of convergence of the series (x −x0)Q(x)/P(x) and (x −x0)2R(x)/P(x).
Proof This can be found in Kreider, Kuller, Ostberg and Perkins (1966) and is due
to Fuchs. We give an outline of why the result should hold in Section 1.3.5. We
are more concerned here with using the result to tell us when a series solution will
converge.
Example
Consider the diﬀerential equation (1.24). We have already seen that x = 0 is a
regular singular point. The radii of convergence of xQ(x)/P(x) and x2R(x)/P(x)
are both unity, and hence the series solution  ∞
n=0 anxn+c exists, is unique, and
will converge for |x| < 1.
Example
Consider the diﬀerential equation
x4 d 2y
dx2 −dy
dx + y = 0.
For this equation, x = 0 is a singular point but it is not regular. The series solution
 ∞
n=0 anxn+c is not guaranteed to exist, since xQ/P = −1/x3, which cannot be
expanded about x = 0.
1.3.5
An outline proof of Theorem 1.3
We will now give a sketch of why Theorem 1.3 holds. Consider the equation
P(x)y′′ + Q(x)y′ + R(x)y = 0.
When x = 0 is an ordinary point, assuming that
P(x) = P0 + xP1 + · · · ,
Q(x) = Q0 + xQ1 + · · · ,
R(x) = R0 + xR1 + · · · ,

1.3 SOLUTION BY POWER SERIES: THE METHOD OF FROBENIUS
27
we can look for a solution using the method of Frobenius. When the terms are
ordered, we ﬁnd that the ﬁrst two terms in the expansion are
P0a0c(c −1)xc−2 + {P0a1c(c + 1) + P1a1c(c −1) + Q0a1c} xc−1 + · · · = 0.
The indicial equation, c(c −1) = 0, has two distinct roots that diﬀer by an integer
and, following Frobenius General Rule I, we can choose c = 0 and ﬁnd a solution
of the form y = a0y0(x) + a1y1(x).
When x = 0 is a singular point, the simplest case to consider is when
P(x) = P1x + P2x2 + · · · .
We can then ask what form Q(x) and R(x) must take to ensure that a series solution
exists. When x = 0 is an ordinary point, the indicial equation was formed from the
y′′ term in the equation alone. Let’s now try to include the y′ and y terms as well,
by making the assumption that
Q(x) = Q0 + Q1x + · · · ,
R(x) = R−1
x
+ R0 + · · · .
Then, after substitution of the Frobenius series into the equation, the coeﬃcient of
xc−1 gives the indicial equation as
P1c(c −1) + Q0c + R−1 = 0.
This is a quadratic equation in c, with the usual possibilities for its types of roots.
As x →0
xQ(x)
P(x) →Q0
P1
,
x2R(x)
P(x)
→R−1
P1
,
so that both of these quantities have a Taylor series expansion about x = 0. This
makes it clear that, when P(x) = P1x + · · · , these choices of expressions for the
behaviour of Q(x) and R(x) close to x = 0 are what is required to make it a regular
singular point. That the series converges, as claimed by the theorem, is most easily
shown using the theory of complex variables; this is done in Section A6.5.
1.3.6
The point at inﬁnity
Our discussion of singular points can be extended to include the point at inﬁnity
by deﬁning s = 1/x and considering the properties of the point s = 0. In particular,
after using
dy
dx = −s2 dy
ds,
d 2y
dx2 = s2 d
ds

s2 dy
ds

in (1.23), we ﬁnd that
ˆP(s)d 2y
ds2 + ˆQ(s)dy
ds + ˆR(s)y = 0,
where
ˆP(s) = s4P
1
s

,
ˆQ(s) = 2s3P
1
s

−s2Q
1
s

,
ˆR(s) = R
1
s

.

28
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
For example, Bessel’s equation, which we will study in Chapter 3, has P(x) = x2,
Q(x) = x and R(x) = x2 −ν2, where ν is a constant, and hence has a regular
singular point at x = 0. In this case, ˆP(s) = s2, ˆQ(s) = s and ˆR(s) = 1/s2 −ν2.
Since ˆP(0) = 0, Bessel’s equation also has a singular point at inﬁnity. In addition,
s2 ˆR(s)/ ˆP(s) =

1 −s2ν

/s, which is singular at s = 0, and we conclude that
the point at inﬁnity is an irregular singular point. We will study the behaviour
of solutions of Bessel’s equation in the neighbourhood of the point at inﬁnity in
Section 12.2.7.
Exercises
1.1
Show that the functions u1 are solutions of the diﬀerential equations given
below. Use the reduction of order method to ﬁnd the second independent
solution, u2.
(a) u1 = ex,
(x −1)d 2y
dx2 −xdy
dx + y = 0,
(b) u1 = x−1 sin x,
xd 2y
dx2 + 2dy
dx + xy = 0.
1.2
Find the Wronskian of
(a) x, x2,
(b) ex, e−x,
(c) x cos(log |x|), x sin(log |x|).
Which of these pairs of functions are linearly independent on the interval
[−1, 1]?
1.3
Find the general solution of
(a) d 2y
dx2 −2dy
dx + y = x3/2ex,
(b) d 2y
dx2 + 4y = 2 sec 2x,
(c) d 2y
dx2 + 1
x
dy
dx +

1 −
1
4x2

y = x,
(d) d 2y
dx2 + y = f(x), subject to y(0) = y′(0) = 0.
1.4
If u1 and u2 are linearly independent solutions of
y′′ + p(x)y′ + q(x)y = 0
and y is any other solution, show that the Wronskian of {y, u1, u2},
W(x) =

y
u1
u2
y′
u′
1
u′
2
y′′
u′′
1
u′′
2

,
is zero everywhere. Hence ﬁnd a second order diﬀerential equation which
has solutions y = x and y = log x.

EXERCISES
29
1.5
Find the Wronskian of the solutions of the diﬀerential equation
(1−x2)y′′−2xy′+2y = 0 to within a constant. Use the method of Frobenius
to determine this constant.
1.6
Find the two linearly independent solutions of each of the diﬀerential equa-
tions
(a) x2 d 2y
dx2 + x

x −1
2
 dy
dx + 1
2y = 0,
(b) x2 d 2y
dx2 + x (x + 1) dy
dx −y = 0,
using the method of Frobenius.
1.7
Show that the indicial equation for
x(1 −x)d 2y
dx2 + (1 −5x)dy
dx −4y = 0
has a double root. Obtain one series solution of the equation in the form
y = A
∞

n=1
n2xn−1 = Au1(x).
What is the radius of convergence of this series? Obtain the second solution
of the equation in the form
u2(x) = u1(x) log x + u1(x) (−4x + · · · ) .
1.8
(a) The points x = ±1 are singular points of the diﬀerential equation
(x2 −1)2 d 2y
dx2 + (x + 1)dy
dx −y = 0.
Show that one of them is a regular singular point and that the other
is an irregular singular point.
(b) Find two linearly independent Frobenius solutions of
xd 2y
dx2 + 4dy
dx −xy = 0,
which are valid for x > 0.
1.9
Find the general solution of the diﬀerential equation
2xd 2y
dx2 + (1 + x)dy
dx −ky = 0
(where k is a real constant) in power series form. For which values of k is
there a polynomial solution?
1.10
Let α, β, γ denote real numbers and consider the hypergeometric equation
x(1 −x)d 2y
dx2 + {γ −(α + β + 1)x}dy
dx −αβy = 0.
Show that x = 0 and x = 1 are regular singular points and obtain the roots
of the indicial equation at the point x = 0. Show that if γ is not an integer,
there are two linearly independent solutions of Frobenius form. Express a1

30
VARIABLE COEFFICIENT, SECOND ORDER DIFFERENTIAL EQUATIONS
and a2 in terms of a0 for each of the solutions.
1.11
Show that each of the equations
(a) x3 d 2y
dx2 + x2 dy
dx + y = 0,
(b) x2 d 2y
dx2 + dy
dx −2y = 0,
has an irregular singular point at x = 0. Show that equation (a) has no
solution of Frobenius type but that equation (b) does. Obtain this solution
and hence ﬁnd the general solution of equation (b).
1.12
Show that x = 0 is a regular singular point of the diﬀerential equation
2x2 d 2y
dx2 + x(1 −x)dy
dx −y = 0.
Find two linearly independent Frobenius solutions and show that one of
these solutions can be expressed in terms of elementary functions. Verify
directly that this function satisﬁes the diﬀerential equation.
1.13
Find the two linearly independent solutions of the ordinary diﬀerential
equation
x(x −1)y′′ + 3xy′ + y = 0
in the form of a power series. Hint: It is straightforward to ﬁnd one solution,
but you will need to use the reduction of order formula to determine the
structure of the second solution.
1.14
∗Show that, if f(x) and g(x) are nontrivial solutions of the diﬀerential
equations u′′ + p(x)u = 0 and v′′ + q(x)v = 0, and p(x) ⩾q(x), f(x)
vanishes at least once between any two zeros of g(x) unless p ≡q and
f = µg, µ ∈R (this is known as the Sturm comparison theorem).
1.15
∗Show that, if q(x) ⩽0, no nontrivial solution of u′′ + q(x)u = 0 can have
more than one zero.

CHAPTER TWO
Legendre Functions
Legendre’s equation occurs in many areas of applied mathematics, physics and
chemistry in physical situations with a spherical geometry. We are now in a position
to examine Legendre’s equation in some detail using the ideas that we developed
in the previous chapter. The Legendre functions are the solutions of Legendre’s
equation, a second order linear diﬀerential equation with variable coeﬃcients. This
equation was introduced by Legendre in the late 18th century, and takes the form
(1 −x2)d 2y
dx2 −2xdy
dx + n(n + 1)y = 0,
(2.1)
where n is a parameter, called the order of the equation. The equation is usually
deﬁned for −1 < x < 1 for reasons that will become clear in Section 2.6. We can see
immediately that x = 0 is an ordinary point of the equation, and, by Theorem 1.3,
a series solution will be convergent for |x| < 1.
2.1
Deﬁnition of the Legendre Polynomials, Pn(x)
We will use the method of Frobenius, and seek a power series solution of (2.1) in
the form
y =
∞

i=0
aixi+c.
Substitution of this series into (2.1) leads to
(1 −x2)
∞

i=0
ai(i + c)(i + c −1)xi+c−2 −2x
∞

i=0
ai(i + c)xi+c−1
+n(n + 1)
∞

i=0
aixi+c = 0.
Tidying this up gives
∞

i=0
ai(i + c)(i + c −1)xi+c−2 +
∞

i=0
ai{n(n + 1) −(i + c)(i + c + 1)}xi+c = 0,
and collecting like powers of x we get
a0c(c −1)xc−2 + a1c(c + 1)xc−1 +
∞

i=2
ai(i + c)(i + c −1)xi+c−2

32
LEGENDRE FUNCTIONS
+
∞

i=0
ai{n(n + 1) −(i + c)(i + c + 1)}xi+c = 0.
Rearranging the series gives us
a0c(c −1)xc−2 + a1c(c + 1)xc−1
+
∞

i=2
{ai(i + c)(i + c −1) + ai−2(n(n + 1) −(i + c −2)(i + c −1))}xi+c−2 = 0.
The indicial equation is therefore
c(c −1) = 0,
which gives c = 0 or 1. Following Frobenius General Rule I, we choose c = 0, so
that a1 is arbitrary and acts as a second constant. In general we have,
ai = (i −1)(i −2) −n(n + 1)
i(i −1)
ai−2 for i = 2, 3, . . . .
For i = 2,
a2 = −n(n + 1)
2
a0.
For i = 3,
a3 = {2 · 1 −n(n + 1)}
3 · 2
a1.
For i = 4,
a4 = {3 · 2 −n(n + 1)}
4 · 3
a2 = −{3 · 2 −n(n + 1)}n(n + 1)
4!
a0.
For i = 5,
a5 = {4 · 3 −n(n + 1)}
5 · 4
a3 = {4 · 3 −n(n + 1)}{2 · 1 −n(n + 1)}
5!
a1.
The solution of Legendre’s equation is therefore
y = a0

1 −n(n + 1)
2!
x2 −{3 · 2 −n(n + 1)}n(n + 1)
4!
x4 + · · ·

+a1

x + {2 · 1 −n(n + 1)}
3!
x3 + {4 · 3 −n(n + 1)}(2 · 1 −n(n + 1))
5!
x5 + · · ·

,
(2.2)
where a0 and a1 are arbitrary constants. If n is not a positive integer, we have two
inﬁnite series solutions, convergent for |x| < 1. If n is a positive integer, one of the
inﬁnite series terminates to give a simple polynomial solution.
If we write the solution (2.2) as y = a0un(x) + a1vn(x), when n is an integer,

2.1 DEFINITION OF THE LEGENDRE POLYNOMIALS, Pn(x)
33
then u0(x), v1(x), u2(x) and v3(x) are the ﬁrst four polynomial solutions. It is
convenient to write the solution in the form
y
=
APn(x)
+
BQn(x),
↑
↑
Polynomial
Inﬁnite series,
of degree n
converges for |x| < 1
where we have deﬁned Pn(x) = un(x)/un(1) for n even and Pn(x) = vn(x)/vn(1)
for n odd. The polynomials Pn(x) are called the Legendre polynomials and can
be written as
Pn(x) =
m

r=0
(−1)r
(2n −2r)!xn−2r
2nr!(n −r)!(n −2r)!,
where m is the integer part of n/2. Note that by deﬁnition Pn(1) = 1. The ﬁrst
ﬁve of these are
P0(x) = 1,
P1(x) = x,
P2(x) = 1
2(3x2 −1),
P3(x) = 1
2(5x3 −3x),
P4(x) = 35
8 x4 −15
4 x2 + 3
8.
Graphs of these Legendre polynomials are shown in Figure 2.1, which was generated
using the MATLAB script
'
&
$
%
x = linspace(-1,1,500); pout = [];
for k = 1:4
p = legendre(k,x); p=p(1,:);
pout = [pout; p];
end
plot(x,pout(1,:),x,pout(2,:),’--’,...
x,pout(3,:),’-.’,x,pout(4,:),’:’)
legend(’P_1(x)’,’P_2(x)’,’P_3(x)’,’P_4(x)’,4),xlabel(’x’)
Note that the MATLAB function legendre(n,x) generates both the Legendre
polynomial of order n and the associated Legendre functions of orders 1 to n,
which we will meet later, so we have to pick oﬀthe Legendre polynomial as the
ﬁrst row of the matrix p.
Simple expressions for the Qn(x) are available for n = 0, 1, 2, 3 using the reduc-
tion of order formula, (1.3). In particular
Q0(x) = 1
2 log
1 + x
1 −x

,
Q1(x) = 1
2x log
1 + x
1 −x

−1,
Q2(x) = 1
4(3x2 −1) log
1 + x
1 −x

−3
2x,
Q3(x) = 1
4(5x3 −3x) log
1 + x
1 −x

−5
2x2 + 2
3.
These functions are singular at x = ±1. Notice that part of the inﬁnite series has

34
LEGENDRE FUNCTIONS
Fig. 2.1. The Legendre polynomials P1(x), P2(x), P3(x) and P4(x).
been summed to give us the logarithmic terms. Graphs of these functions are shown
in Figure 2.2.
Example
Let’s try to ﬁnd the general solution of
(1 −x2)y′′ −2xy′ + 2y =
1
1 −x2 ,
for −1 < x < 1. This is just an inhomogeneous version of Legendre’s equation of
order one. The complementary function is
yh = AP1(x) + BQ1(x).
The variation of parameters formula, (1.6), then shows that the particular integral
is
yp =
 x
P1(s)Q1(x) −P1(x)Q1(s)
(1 −s2){P1(s)Q′
1(s) −P ′
1(s)Q1(s)}ds.
We can considerably simplify this rather complicated looking result. Firstly, Abel’s
formula, (1.7), shows that the Wronskian is
W = P1(s)Q′
1(s) −P ′
1(s)Q1(s) =
W0
1 −s2 .

2.2 THE GENERATING FUNCTION FOR Pn(x)
35
Fig. 2.2. The ﬁrst four Legendre functions, Q0(x), Q1(x), Q2(x) and Q3(x).
We can determine the constant W0 by considering the behaviour of W as s →0.
Since
P1(s) = s,
Q1(s) = 1
2s log
1 + s
1 −s

−1,
W = 1 + s2 + · · · for s ≪1. From the binomial theorem, 1/(1 −s2) = 1 + s2 + · · ·
for s ≪1. We conclude that W0 = 1. This means that
yp = Q1(x)
 x
P1(s) ds −P1(x)
 x
Q1(s) ds
= 1
2x2Q1(x) −x
1
4(x2 −1) log
1 + x
1 −x

−1
2x
	
.
The general solution is this particular integral plus the complementary function
(yp(x) + yh(x)).
2.2
The Generating Function for Pn(x)
In order to make a more systematic study of the Legendre polynomials, it is helpful
to introduce a generating function, G(x, t). This function is deﬁned in such a
way that the coeﬃcients of the Taylor series of G(x, t) around t = 0 are Pn(x).
We start with the assertion that
G(x, t) = (1 −2xt + t2)−1/2 =
∞

n=0
Pn(x)tn.
(2.3)

36
LEGENDRE FUNCTIONS
Just to motivate this formula, let’s consider the ﬁrst terms in the Taylor series
expansion about t = 0. Using the binomial expansion formula, which is convergent
for
t2 −2xt
 < 1 (for |x| < 1 we can ensure that this holds by making |t| small
enough),
{1 + (−2xt + t2)}−1/2 =
1 +

−1
2

(−2xt + t2) +

−1
2
 
−3
2

2!
(−2xt + t2)2 + · · ·
= 1 + xt + 1
2(3x2 −1)t2 + · · · = P0(x) + P1(x)t + P2(x)t2 + · · · ,
as expected. With a little extra work we can derive (2.3).
We start by working with
(1 −2xt + t2)−1/2 =
∞

n=0
Zn(x)tn,
(2.4)
where, using the binomial expansion, we know that Zn(x) is a polynomial of degree
n. We ﬁrst diﬀerentiate with respect to x, which gives us
t(1 −2xt + t2)−3/2 =
∞

n=0
Z′
n(x)tn,
and again gives
3t2(1 −2xt + t2)−5/2 =
∞

n=0
Z′′
n(x)tn.
Now we diﬀerentiate (2.4) with respect to t, which leads to
(x −t)(1 −2xt + t2)−3/2 =
∞

n=0
Zn(x)ntn−1.
Multiplying this last result by t2 and diﬀerentiating with respect to t gives
∞

n=0
Zn(x)n(n + 1)tn = ∂
∂t{t2(x −t)(1 −2xt + t2)−3/2},
= t2{(x −t)(1 −2xt + t2)−5/23(x −t) + (1 −2xt + t2)−3/2 −1}
+(x −t)(1 −2xt + t2)−3/22t,
which we can simplify to get
(1 −2xt + t2)−3/2{3t2(x −t)2(1 −2xt + t2)−1 −1 + 2t(x −t)}
=
∞

n=0
Zn(x)n(n + 1)tn.

2.2 THE GENERATING FUNCTION FOR Pn(x)
37
Combining all of these results gives
(1 −x2)
∞

n=0
Z′′
n(x)tn −2x
∞

n=0
Z′
n(x)tn +
∞

n=0
n(n + 1)Zn(x)tn
= (1 −x2)3t2(1 −2xt + t2)−5/2 −2xt(1 −2xt + t2)−3/2
+(1 −2xt + t2)−3/2{3t2(x −t)2(1 −2xt + t2)−1 −1 + 2t(x −t)} = 0,
for any t. Therefore
(1 −x2)Z′′
n(x) −2xZ′
n(x) + n(n + 1)Zn(x) = 0,
which is just Legendre’s equation. This means that Zn(x) = αPn(x) + βQn(x)
where α, β are constants. As Zn(x) is a polynomial of degree n, β must be zero.
Finally, we need to show that Zn(1) = 1. This is done by putting x = 1 in the
generating function relationship, (2.3), to obtain
(1 −2t + t2)−1/2 =
∞

n=0
Zn(1)tn.
Since
(1 −2t + t2)−1/2 =

(1 −t)2−1/2 = (1 −t)−1 =
∞

n=0
tn,
at least for |t| < 1, we have Zn(1) = 1. Since we know that Pn(1) = 1, we conclude
that Zn(x) ≡Pn(x), as required.
The generating function, G(x, t) = (1 −2xt + t2)−1/2, can be used to prove a
number of interesting properties of Legendre polynomials, as well as some recurrence
formulae. We will give a few examples of its application.
Special Values
The generating function is useful for determining the values of the Legendre poly-
nomials for certain special values of x. For example, substituting x = −1 in (2.3)
gives
(1 + 2t + t2)−1/2 =
∞

n=0
Pn(−1)tn.
By the binomial expansion we have that
(1 + 2t + t2)−1/2 = {(1 + t)2}−1/2 = (1 + t)−1
= 1 −t + t2 −· · · + (−t)n + · · · =
∞

n=0
(−1)ntn.
We conclude that
∞

n=0
(−1)ntn =
∞

n=0
Pn(−1)tn,
and therefore Pn(−1) = (−1)n for n = 1, 2, . . . .

38
LEGENDRE FUNCTIONS
2.3
Diﬀerential and Recurrence Relations Between Legendre
Polynomials
The generating function can also be used to derive recurrence relations between the
various Pn(x). Starting with (2.3), we diﬀerentiate with respect to t, and ﬁnd that
(x −t)(1 −2xt + t2)−3/2 =
∞

n=0
Pn(x)ntn−1.
We now multiply through by (1 −2xt + t2) to obtain
(x −t)(1 −2xt + t2)−1/2 = (1 −2xt + t2)
∞

n=0
Pn(x)ntn−1,
which leads to
x
∞

n=0
Pn(x)tn −
∞

n=0
Pn(x)tn+1
= n
∞

n=0
Pn(x)tn−1 −2xn
∞

n=0
Pn(x)tn + n
∞

n=0
Pn(x)tn+1.
Equating coeﬃcients of tn on both sides shows that
xPn(x) −Pn−1(x) = (n + 1)Pn+1(x) −2xnPn(x) + (n −1)Pn−1(x),
and hence
(n + 1)Pn+1(x) −(2n + 1)xPn(x) + nPn−1(x) = 0.
(2.5)
This is a recurrence relation between Pn+1(x), Pn(x) and Pn−1(x), which can be
used to compute the polynomials Pn(x). Starting with P0(x) = 1 and P1(x) = x,
we substitute n = 1 into (2.5), which gives
2P2(x) −3x2 + 1 = 0,
and hence
P2(x) = 1
2(3x2 −1).
By iterating this procedure, we can generate the Legendre polynomials Pn(x) for
any n.
In a rather similar manner we can generate a recurrence relation that involves
the derivatives of the Legendre polynomials. Firstly, we diﬀerentiate the generating
function, (2.3), with respect to x to get
t(1 −2xt + t2)−3/2 =
∞

n=0
tnP ′
n(x).
Diﬀerentiation of (2.3) with respect to t gives
(x −t)(1 −2xt + t2)−3/2 =
∞

n=0
ntn−1Pn(x).

2.4 RODRIGUES’ FORMULA
39
Combining these gives
∞

n=0
ntnPn(x) = (x −t)
∞

n=0
tnP ′
n(x),
and by equating coeﬃcients of tn we obtain the recurrence relation
nPn(x) = xP ′
n(x) −P ′
n−1(x).
An Example From Electrostatics
In electrostatics, the potential due to a unit point charge at r = r0 is
V =
1
|r −r0|.
If this unit charge lies on the z-axis, at x = y = 0, z = a, this becomes
V =
1
#
x2 + y2 + (z −a)2 .
In terms of spherical polar coordinates, (r, θ, φ),
x = r sin θ cos φ,
y = r sin θ sin φ,
z = r cos θ.
This means that
x2 + y2 + (z −a)2 = x2 + y2 + z2 −2az + a2 = r2 + a2 −2az,
and hence
V =
1
√
r2 + a2 −2ar cos θ
= 1
a

1 −2 cos θ r
a + r2
a2
−1/2
.
As we would expect from the symmetry of the problem, there is no dependence
upon the azimuthal angle, φ. We can now use the generating function to write this
as a power series,
V = 1
a
∞

n=0
Pn (cos θ)
r
a
n
.
2.4
Rodrigues’ Formula
There are other methods of generating the Legendre polynomials, and the most
useful of these is Rodrigues’ formula,
Pn(x) =
1
2nn!
d n
dxn {(x2 −1)n}.
For example, for n = 1,
P1(x) =
1
211!
d 1
dx1 {(x2 −1)} = x,

40
LEGENDRE FUNCTIONS
whilst for n = 2,
P2(x) =
1
222!
d 2
dx2 {(x2 −1)2} =
1
4 · 2
d
dx{4x(x2 −1)} = 1
2(3x2 −1).
The general proof of this result is by induction on n, which we leave as an exercise.
Rodrigues’ formula can also be used to develop an integral representation of the
Legendre polynomials. In order to show how this is done, it is convenient to switch
from the real variable x to the complex variable z = x + iy. We deﬁne the ﬁnite
complex Legendre polynomials in the same way as for a real variable. In particular
Rodrigues’ formula,
Pn(z) =
1
2nn!
d n
dzn

z2 −1
n ,
will be useful to us here. Recall from complex variable theory (see Appendix 6)
that, if f(z) is analytic and single-valued inside and on a simple closed curve C,
f (n)(z) = n!
2πi

C
f(ξ)
(ξ −z)n+1 dξ,
for n ⩾0, when z is an interior point of C. Now, using Rodrigues’ formula, we have
Pn(z) =
1
2n+1πi

C
(ξ2 −1)n
(ξ −z)n+1 dξ,
(2.6)
which is known as Schl¨aﬂi’s representation. The contour C must, of course,
enclose the point ξ = z and be traversed in an anticlockwise sense. To simplify
matters, we now choose C to be a circle, centred on ξ = z with radius |
√
z2 −1|,
with z ̸= 1. Putting ξ = z +
√
z2 −1eiθ gives, after some simple manipulation,
Pn(z) = 1
2π
 2π
θ=0

z +
#
z2 −1 cos θ
n
dθ.
This is known as Laplace’s representation. In fact it is also valid when z = 1,
since
Pn(1) = 1
2π
 2π
0
1dθ = 1.
Laplace’s representation is useful, amongst other things, for providing a bound on
the size of the Legendre polynomials of real argument. For z ∈[−1, 1], we can write
z = cos φ and use Laplace’s representation to show that
|Pn(cos φ)| ⩽1
2π
 2π
0
| cos φ + i sin φ cos θ|n dθ.
Now, since
| cos φ + i sin φ cos θ| =
$
cos2 φ + sin2 φ cos2 θ ⩽
$
cos2 φ + sin2 φ = 1,
we have |Pn(cos φ)| ⩽1.

2.5 ORTHOGONALITY OF THE LEGENDRE POLYNOMIALS
41
2.5
Orthogonality of the Legendre Polynomials
Legendre polynomials have the very important property of orthogonality on
[−1, 1], that is
 1
−1
Pn(x)Pm(x)dx =
2
2n + 1δmn,
(2.7)
where the Kronecker delta is deﬁned by
δmn =
 1
for m = n,
0
for m ̸= n.
To show this, note that if f ∈C[−1, 1], Rodrigues’ formula shows that
 1
−1
f(x)Pn(x)dx =
1
2nn!
 1
−1
f(x) d n
dxn {(x2 −1)n}dx
=
1
2nn!

f(x) d n−1
dxn−1 {(x2 −1)n}
1
−1
−
 1
−1
f ′(x) d n−1
dxn−1 {(x2 −1)n}dx

= −
1
2nn!
 1
−1
f ′(x) d n−1
dxn−1 {(x2 −1)n}dx.
Repeating this integration by parts (n −1) more times gives
 1
−1
f(x)Pn(x)dx = (−1)n
2nn!
 1
−1
(x2 −1)nf (n)(x)dx,
(2.8)
where f (n)(x) is the nth derivative of f(x). This result is interesting in its own
right, but for the time being consider the case f(x) = Pm(x) with m < n, so that
 1
−1
Pm(x)Pn(x)dx = (−1)n
2nn!
 1
−1
(x2 −1)nP (n)
m (x)dx.
Since the nth derivative of an mth order polynomial is zero for m < n, we have
 1
−1
Pm(x)Pn(x)dx = 0 for m < n.
By symmetry, this must also hold for m > n.
Let’s now consider the case n = m, for which
 1
−1
Pn(x)Pn(x)dx = (−1)n
2nn!
 1
−1
(x2 −1)nP (n)
n
(x)dx
= (−1)n
2nn!
 1
−1
(x2 −1)n
1
2nn!
d 2n
dx2n {(x2 −1)n}dx.
Noting the fact that
(x2 −1)n = x2n + · · · + (−1)n,

42
LEGENDRE FUNCTIONS
and hence that
d 2n
dx2n (x2 −1)n = 2n(2n −1) . . . 3 · 2 · 1 = (2n)!,
we can see that
 1
−1
Pn(x)Pn(x)dx = (−1)n
2nn!
 1
−1
(x2 −1)n
1
2nn!(2n)!dx
= (−1)n(2n)!
22n(n!)2
 1
−1
(x2 −1)ndx.
To evaluate the remaining integral, we use a reduction formula to show that
 1
−1
(x2 −1)n dx = 22n+2n!(n + 1)!(−1)n
(2n + 2)!
,
and hence
 1
−1
P 2
n(x)dx =
2
2n + 1.
This completes the derivation of (2.7). There is an easier proof of the ﬁrst part
of this, using the idea of a self-adjoint linear operator, which we will discuss in
Chapter 4.
We have now shown that the Legendre polynomials are orthogonal on [−1, 1].
It is also possible to show that these polynomials are complete in the function
space C[−1, 1]. This means that a continuous function can be expanded as a linear
combination of the Legendre polynomials. The proof of the completeness property
is rather diﬃcult, and will be omitted here.†
What we can present here is the
procedure for obtaining the coeﬃcients of such an expansion for a given function
f(x) belonging to C[−1, 1]. To do this we write
f(x) = a0P0(x) + a1P1(x) + · · · + anPn(x) + · · · =
∞

n=0
anPn(x).
Multiplying by Pm(x) and integrating over [−1, 1] (more precisely, forming the inner
product with Pm(x)) gives
 1
−1
f(x)Pm(x)dx =
 1
−1
Pm(x)
∞

n=0
anPn(x)dx.
† Just to give a ﬂavour of the completeness proof for the case of Legendre polynomials, we note
that, because of their polynomial form, we can deduce that any polynomial can be written
as a linear combination of Legendre polynomials. However, according to a fundamental result
due to Weierstrass (the Weierstrass polynomial approximation theorem) any function which is
continuous on some interval can be approximated as closely as we wish by a polynomial. The
completeness then follows from the application of this theorem. The treatment of completeness
of other solutions of Sturm–Liouville problems may be more complicated, for example for Bessel
functions. A complete proof of this can be found in Kreider, Kuller, Ostberg and Perkins (1966).
We will return to this topic in Chapter 4.

2.5 ORTHOGONALITY OF THE LEGENDRE POLYNOMIALS
43
Interchanging the order of summation and integration leads to
 1
−1
f(x)Pm(x)dx =
∞

n=0
an
 1
−1
Pn(x)Pm(x)dx
	
=
∞

n=0
an
2
2m + 1δmn =
2am
2m + 1,
using the orthogonality property, (2.7). This means that
am = 2m + 1
2
 1
−1
f(x)Pm(x)dx,
(2.9)
and we write the series as
f(x) =
∞

n=0
2n + 1
2
 1
−1
f(x)Pn(x)dx
	
Pn(x).
This is called a Fourier–Legendre series.
Let’s consider a couple of examples. Firstly, when f(x) = x2,
am = 2m + 1
2
 1
−1
x2Pm(x)dx,
so that
a0 = 1
2
 1
−1
x2 · 1dx = 1
2
x3
3
1
−1
= 1
2
2
3 = 1
3,
a1 = (2 · 1) + 1
2
 1
−1
x2 · xdx = 3
2
x4
4
1
−1
= 0,
a2 = (2 · 2) + 1
2
 1
−1
x2 1
2(3x2 −1)dx = 5
2
 3x5
2 · 5 −x3
3 · 2
1
−1
= 2
3.
Also, (2.8) shows that
am = 0 for m = 3, 4, . . . ,
and therefore,
x2 = 1
3P0(x) + 2
3P2(x).
A ﬁnite polynomial clearly has a ﬁnite Fourier–Legendre series.
Secondly, consider f(x) = ex. In this case
am = 2m + 1
2
 1
−1
exPm(x) dx,
and hence
a0 = 1
2
 1
−1
ex dx = 1
2

e −e−1
,

44
LEGENDRE FUNCTIONS
a1 = 3
2
 1
−1
x ex dx = 3 e−1.
To proceed with this calculation it is necessary to ﬁnd a recurrence relation between
the an. This is best done by using Rodrigues’ formula, which gives
an = (2n + 1)
 an−2
2n −3 −an−1

for n = 2, 3, . . . ,
(2.10)
from which the values of a4, a5, . . . are easily computed.
We will not examine the convergence of Fourier–Legendre series here as the de-
tails are rather technical. Instead we content ourselves with a statement that the
Fourier–Legendre series converges uniformly on any closed subinterval of (−1, 1)
in which f is continuous and diﬀerentiable.
An extension of this result to the
space of piecewise continuous functions is that the series converges to the value
1
2

f(x+
0 ) + f(x−
0 )

at each point x0 ∈(−1, 1) where f has a right and left deriva-
tive. We will prove a related theorem in Chapter 5.
2.6
Physical Applications of the Legendre Polynomials
In this section we present some examples of Legendre polynomials as they arise
in mathematical models of heat conduction and ﬂuid ﬂow in spherical geometries.
In general, we will encounter the Legendre equation in situations where we have
to solve partial diﬀerential equations containing the Laplacian in spherical polar
coordinates.
2.6.1
Heat Conduction
Let’s derive the equation that governs the evolution of an initial distribution of
heat in a solid body with temperature T, density ρ, speciﬁc heat capacity c and
thermal conductivity k. Recall that the speciﬁc heat capacity, c, is the amount
of heat required to raise the temperature of a unit mass of a substance by one
degree. The thermal conductivity, k, of a body appears in Fourier’s law, which
states that the heat ﬂux per unit area, per unit time, Q = (Qx, Qy, Qz), is related
to the temperature gradient, ∇T, by the simple linear relationship Q = −k∇T. If
we now consider a small element of our solid body at (x, y, z) with sides of length
δx, δy and δz, the temperature change in this element over a time interval δt is
determined by the diﬀerence between the amount of heat that ﬂows into the element
and the amount of heat that ﬂows out, which gives
ρc {T (x, y, z, t + δt) −T (x, y, z, t)} δxδyδz
= {Qx (x, y, z, t) −Qx (x + δx, y, z, t)} δtδyδz
+ {Qy (x, y, z, t) −Qy (x, y + δy, z, t)} δtδxδz
(2.11)
+ {Qz (x, y, z, t) −Qz (x, y, z + δz, t)} δtδxδy.

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
45
ρ
k
c
K
kg m−3
J m−1 s−1 K−1
J kg−1 K−1
m2 s−1
copper
8920
385
386
1.1 × 10−4
water
1000
254
4186
6.1 × 10−5
glass
2800
0.8
840
3.4 × 10−7
Table 2.1. Some typical physical properties of copper, water (at room temperature
and pressure) and glass.
Note that a typical term on the right hand side of this, for example,
{Qx (x, y, z, t) −Qx (x + δx, y, z, t)} δtδyδz,
is the amount of heat crossing the x-orientated faces of the element, each with area
δyδz, during the time interval (t, t + δt). Taking the limit δt, δx, δy, δz →0, we
obtain
ρc∂T
∂t = −
∂Qx
∂x + ∂Qy
∂y + ∂Qz
∂z
	
= −∇· Q.
Substituting in Fourier’s law, Q = −k∇T, gives the diﬀusion equation,
∂T
∂t = K∇2T,
(2.12)
where K = k/ρc is called the thermal diﬀusivity. Table 2.1 contains the values
of relevant properties for three everyday materials.
When the temperature reaches a steady state (∂T/∂t = 0), this equation takes
the simple form
∇2T = 0,
(2.13)
which is known as Laplace’s equation. It must be solved in conjunction with
appropriate boundary conditions, which drive the temperature gradients in the
body.
Example
Let’s try to ﬁnd the steady state temperature distribution in a solid, uniform sphere
of unit radius, when the surface temperature is held at f(θ) = T0 sin4 θ in spherical
polar coordinates, (r, θ, φ).
This temperature distribution will satisfy Laplace’s
equation, (2.13). Since the equation and boundary conditions do not depend upon
the azimuthal angle, φ, neither does the solution, and hence Laplace’s equation
takes the form
1
r2
∂
∂r

r2 ∂T
∂r

+
1
r2 sin θ
∂
∂θ

sin θ∂T
∂θ

= 0.
Let’s look for a separable solution, T(r, θ) = R(r)Θ(θ). This gives
Θ d
dr

r2 dR
dr

+
R
sin θ
d
dθ

sin θdΘ
dθ

= 0,

46
LEGENDRE FUNCTIONS
and hence
1
R
d
dr

r2 dR
dr

= −
1
Θ sin θ
d
dθ

sin θdΘ
dθ

.
Since the left hand side is a function of r only and the right hand side is a function
of θ only, this equality can only be valid if both sides are equal to some constant,
with
1
R
d
dr

r2 dR
dr

= −
1
Θ sin θ
d
dθ

sin θdΘ
dθ

= constant = n(n + 1).
This choice of constant may seem rather ad hoc at this point, but all will become
clear later.
We now have an ordinary diﬀerential equation for Θ, namely
1
sin θ
d
dθ

sin θdΘ
dθ

+ n(n + 1)Θ = 0.
Changing variables to
µ = cos θ,
y(µ) = Θ(θ),
and using
sin2 θ = 1 −µ2,
d
dµ = dθ
dµ
d
dθ = −1
sin θ
d
dθ,
leads to
d
dµ

(1 −µ2) dy
dµ

+ n(n + 1)y = 0,
or, equivalently
(1 −µ2)y′′ −2µy′ + n(n + 1)y = 0,
which is Legendre’s equation. Since a physically meaningful solution must in this
case be ﬁnite at µ = ±1 (the north and south poles of the sphere), the solution,
to within an arbitrary multiplicative constant, is the Legendre polynomial, y(µ) =
Pn(µ), and hence Θn(θ) = Pn(cos θ). We have introduced the subscript n for Θ so
that we can specify that this is the solution corresponding to a particular choice of
n. Note that we have just solved our ﬁrst boundary value problem. Speciﬁcally, we
have found the solution of Legendre’s equation that is bounded at µ = ±1.
We must now consider the solution of the equation for R(r). This is
d
dr

r2 dR
dr

= n(n + 1)R.
By inspection, by the method of Frobenius or by noting that the equation is in-
variant under the transformation group (r, R) →(λr, λR) (see Chapter 10), we can
ﬁnd the solution of this equation as
Rn(r) = Anrn + Bnr−1−n,
where again the subscript n denotes the dependence of the solution on n and An
and Bn are arbitrary constants. At the centre of the sphere, the temperature must

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
47
be ﬁnite, so we set Bn = 0. Our solution of Laplace’s equation therefore takes the
form
T = AnrnPn(cos θ).
As this is a solution for arbitrary n, the general solution for the temperature will
be a linear combination of these solutions,
T =
∞

n=0
AnrnPn(cos θ).
The remaining task is to evaluate the coeﬃcients An. This can be done using the
speciﬁed temperature on the surface of the sphere, T = T0 sin4 θ at r = 1. We
substitute r = 1 into the general expression for the temperature to get
f(θ) = T0 sin4 θ =
∞

n=0
AnPn(cos θ).
The An will therefore be the coeﬃcients in the Fourier–Legendre expansion of the
function f(θ) = T0 sin4 θ.
It is best to work in terms of the variable µ = cos θ. We then have
T0(1 −µ2)2 =
∞

n=0
AnPn(µ).
From (2.9),
An = 2n + 1
2
 1
−1
T0(1 −µ2)2Pn(µ)dµ.
Since the function that we want to expand is a ﬁnite polynomial in µ, we expect
to obtain a ﬁnite Fourier–Legendre series.
A straightforward calculation of the
integral gives us
A0 = 1
2 · 16
15T0,
A1 = 0,
A2 = −5
2 · 32
105T0,
A3 = 0,
A4 = 9
2 · 16
315T0,
Am = 0 for m = 5, 6, . . . .
The solution is therefore
T = T0
 8
15P0(cos θ) −16
21r2P2(cos θ) + 8
35r4P4(cos θ)
	
.
(2.14)
This solution when T0 = 1 is shown in Figure 2.3, which is a polar plot in a plane
of constant φ. Note that the temperature at the centre of the sphere is 8T0/15. We
produced Figure 2.3 using the MATLAB script




ezmesh(’r*cos(t)’,’r*sin(t)’,’8/15-8*r^2*(3*cos(t)^2-1)/21+...
8*r^4*(35*cos(t)^4/8-15*cos(t)^2/4+3/8)/35’, [0 1 0 pi])

48
LEGENDRE FUNCTIONS
The function ezmesh gives an easy way of plotting parametric surfaces like this.
The ﬁrst three arguments give the x, y and z coordinates as parametric functions
of r and t = θ, whilst the fourth speciﬁes the ranges of r and t.
Fig. 2.3. The steady state temperature in a uniform sphere with surface temperature
sin4 θ, given by (2.14).
2.6.2
Fluid Flow
Consider a ﬁxed volume V bounded by a surface S within an incompressible
ﬂuid. Although ﬂuid ﬂows into and out of V , the mass of ﬂuid within V remains
constant, since the ﬂuid is incompressible, so any ﬂux out of V at one place is
balanced by a ﬂux into V at another place. Mathematically, we can express this as

S
u · n dS = 0,
where u is the velocity of the ﬂuid, n is the outward unit normal to S, and hence
u·n is the normal component of the ﬂuid velocity out through S. If the ﬂuid velocity
ﬁeld, u, is smooth, we can use the divergence theorem to rewrite this statement of
conservation of mass as

V
∇· u dV = 0.
As this applies to an arbitrary volume V within the ﬂuid, we must have ∇· u = 0
throughout the ﬂow.
If we also suppose that the ﬂuid is inviscid (there is no
friction as one ﬂuid element ﬂows past another), it is possible to make the further
assumption that the ﬂow is irrotational (∇× u = 0). Physically, this means that

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
49
there is no spin in any ﬂuid element as it ﬂows around.
Inviscid, irrotational,
incompressible ﬂows are therefore governed by the two equations ∇· u = 0 and
∇×u = 0.† For simply connected ﬂow domains, ∇×u = 0 if and only if u = ∇φ for
some scalar potential function φ, known as the velocity potential. Substituting
into ∇· u = 0 gives ∇2φ = 0.
In other words, the velocity potential satisﬁes
Laplace’s equation. As for boundary conditions, there can be no ﬂux of ﬂuid into
a solid body so that u · n = n · ∇φ = ∂φ/∂n = 0 where the ﬂuid is in contact with
a solid body.
As an example of such a ﬂow, let’s consider what happens when a sphere of radius
r = a is placed in a uniform stream, assuming that the ﬂow is steady, inviscid and
irrotational. The ﬂow at inﬁnity must be uniform, with u = Ui where i is the
unit vector in the x-direction. First of all, it is clear that the problem will be best
treated in spherical polar coordinates. We know from the previous example that
the bounded, axisymmetric solution of Laplace’s equation is
φ =
∞

n=0

Anrn + Bnr−1−n
Pn(cos θ).
(2.15)
The ﬂow at inﬁnity has potential φ = Ux = U r cos θ. Since P1(cos θ) = cos θ, we
see that we must take A1 = 1 and An = 0 for n > 1. To ﬁx the constants Bn,
notice that there can be no ﬂow through the surface of the sphere. This gives us
the boundary condition on the radial velocity as
ur = ∂φ
∂r = 0
at r = a.
On substituting (2.15) into this boundary condition, we ﬁnd that B1 = 1
2a3 and
Bn = 0 for n > 1. The solution is therefore
φ = U

r + a3
2 r2

cos θ.
(2.16)
The streamlines (continuous curves that are tangent to the velocity vector) are
shown in Figure 2.4 when a = U = 1. In order to obtain this ﬁgure, we note that
(see Section 7.2.2) the streamlines are given by
ψ = U

r −a3
r2

sin θ = Uy
%
1 −
a3
(x2 + y2)3/2
&
= constant.
We can then use the MATLAB script




x = linspace(-2,2,400); y = linspace(0,2,200);
[X Y] = meshgrid(x,y);
Z = Y.*(1-1./(X.^2+Y.^2).^(3/2));
v = linspace(0,2,15); contour(X,Y,Z,v)
† We will consider an example of viscous ﬂow in Section 6.4.

50
LEGENDRE FUNCTIONS
The command meshgrid creates a grid suitable for use with the plotting command
contour out of the two vectors, x and y. The vector v speciﬁes the values of ψ for
which a contour is to be plotted.
Fig. 2.4. The streamlines for inviscid, irrotational ﬂow about a unit sphere.
In order to complete this example, we must consider the pressure in our ideal
ﬂuid. The force exerted by the ﬂuid on a surface S by the ﬂuid outside S is purely
a pressure, p, which acts normally to S. In other words, the force on a surface
element with area dS and outward unit normal n is −pn dS. We would now like to
apply Newton’s second law to the motion of the ﬂuid within V , the volume enclosed
by S. In order to do this, we need an expression for the acceleration of the ﬂuid.
Let’s consider the change in the velocity of a ﬂuid particle between times t and
t + δt, which, for small δt, we can Taylor expand to give
u(x(t + δt), t + δt) −u(x(t), t) = δt
∂u
∂t (x, t) + dx
dt · ∇u(x, t)
	
+ · · ·
= δt
∂u
∂t (x, t) + (u(x, t) · ∇) u(x, t)
	
+ · · · ,
since u = dx/dt. This means that the ﬂuid acceleration is
Du
Dt = ∂u
∂t + (u · ∇) u,
where D/Dt is the usual notation for the material derivative, or time derivative
following the ﬂuid particle.
We can now use Newton’s second law on the ﬂuid within S to obtain

S
−pn dS =

V
ρDu
Dt dV.
After noting that

S
pn dS = i

S
(pi) · n dS + j

S
(pj) · n dS + k

S
(pk) · n dS,

2.6 PHYSICAL APPLICATIONS OF THE LEGENDRE POLYNOMIALS
51
where i, j, k are unit vectors in the coordinate directions, the divergence theorem,
applied to each of these integrals, shows that

S
pn dS = i

V
∂p
∂x dV + j

V
∂p
∂y dV + k

V
∂p
∂z dV =

V
∇p dV,
and hence that

V

ρDu
Dt + ∇p
	
dV = 0.
Since V is arbitrary,
Du
Dt = −1
ρ∇p,
and for steady ﬂows
(u.∇) u = −1
ρ∇p.
(2.17)
We earlier used the irrotational nature of the ﬂow to write the velocity ﬁeld as the
gradient of a scalar potential, u = ∇φ. Since (u · ∇) u ≡∇
 1
2u · u

−u × (∇× u),
for irrotational ﬂow (u · ∇) u = ∇
 1
2u · u

and we can write (2.17) as
∇
1
2 |∇φ|2

= −1
ρ∇p,
and hence
∇
p
ρ + 1
2 |∇φ|2

= 0,
which we can integrate to give
p
ρ + 1
2 |∇φ|2 = C,
(2.18)
which is known as Bernoulli’s equation. Its implication for inviscid, irrotational
ﬂow is that the pressure can be calculated once we know the velocity potential, φ.
In our example, φ is given by (2.16), so that
|∇φ|2 = φ2
r + 1
r2 φ2
θ = U 2

1 −a3
r3
2
cos2 θ + 1
r2

r + a3
2r2
2
sin2 θ

,
and Bernoulli’s equation gives
p = p∞+ 1
2ρU 2

1 −

1 −a3
r3
2
cos2 θ + 1
r2

r + a3
2r2
2
sin2 θ

,
where we have written p∞for the pressure at inﬁnity. This expression simpliﬁes on
the surface of the sphere to give
p|r=a = p∞+ 1
2ρU 2

1 −9
4 sin2 θ

.
This shows that the pressure is highest at θ = 0 and π, the stream-facing poles of
the sphere, with p = p∞+ 1
2ρU 2, and drops below p∞over a portion of the rest of

52
LEGENDRE FUNCTIONS
the boundary, with the lowest pressure, p = p∞−5
8ρU 2, on the equator, θ = π/2.
The implications of this and the modelling assumptions made are discussed more
fully by Acheson (1990).
2.7
The Associated Legendre Equation
We can also look for a separable solution of Laplace’s equation in spherical polar
coordinates (r, θ, φ), that is not necessarily axisymmetric. If we seek a solution of
the form y(θ)Φ(φ)R(r), we ﬁnd that Φ = eimφ with m an integer. The equation
for y is, after using the change of variable x = cos θ,
(1 −x2)d 2y
dx2 −2xdy
dx +

n(n + 1) −
m2
1 −x2
	
y = 0.
(2.19)
This equation is known as the associated Legendre equation. It trivially reduces
to Legendre’s equation when m = 0, corresponding to separable axisymmetric
solutions of Laplace’s equation. However, the connection is more profound than
this.
If we deﬁne
Y = (1 −x2)−m/2y,
Y is the solution of
(1 −x2)Y ′′ −2(m + 1)xY ′ + (n −m)(n + m + 1)Y = 0.
(2.20)
If we write Legendre’s equation in the form
(1 −x2)Z′′ −2xZ′ + n(n + 1)Z = 0,
which has solution Z = APn(x) + BQn(x), and diﬀerentiate m times, we get
(1 −x2)[Z(m)]′′ −2(m + 1)x[Z(m)]′ + (n −m)(n + m + 1)Z(m) = 0,
(2.21)
where we have written Z(m) = d mZ/dxm. A comparison of (2.20) and (2.21) shows
that
Y = d m
dxm [APn(x) + BQn(x)] ,
and therefore that
y = (1 −x2)m/2

A d m
dxm Pn(x) + B d m
dxm Qn(x)
	
.
We have now shown that the solutions of the diﬀerential equation (2.19) are
y = AP m
n (x) + BQm
n (x),
where
P m
n (x) = (1 −x2)m/2 d m
dxm Pn(x),
Qm
n (x) = (1 −x2)m/2 d m
dxm Qn(x).
The functions P m
n (x) and Qm
n (x) are called the associated Legendre functions.
Clearly, from what we know about the Legendre polynomials, P m
n (x) = 0 if m > n

2.7 THE ASSOCIATED LEGENDRE EQUATION
53
and Qm
n (x) is singular at x = ±1. It is straightforward to show from their deﬁnitions
that
P 1
1 (x) = (1 −x2)1/2 = sin θ,
P 1
2 (x) = 3x(1 −x2)1/2 = 3 sin θ cos θ = 3
2 sin 2θ,
P 2
2 (x) = 3(1 −x2) = 3 sin2 θ = 3
2(1 −cos 2θ),
P 1
3 (x) = 3
2(5x2 −1)(1 −x2)1/2 = 3
8(sin θ + 5 sin 3θ),
where x = cos θ.
There are various recurrence formulae and orthogonality relations between the
associated Legendre functions, which can be derived in much the same way as those
for the ordinary Legendre functions.
Example: Spherical harmonics
Let’s try to ﬁnd a representation of the solutions of Laplace’s equation in three
dimensions,
∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 = 0,
that are homogeneous in x, y and z. By homogeneous we mean of the form xiyjzk.
It is simplest to work in spherical polar coordinates, in terms of which Laplace’s
equation takes the form
∂
∂r

r2 ∂u
∂r

+
1
sin θ
∂
∂θ

sin θ∂u
∂θ

+
1
sin2 θ
∂2u
∂φ2 = 0.
A homogeneous solution of order n = i + j + k in (x, y, z) will look, in the new
coordinates, like u = rnSn(θ, φ). Substituting this expression for u in Laplace’s
equation, we ﬁnd that
1
sin θ
∂
∂θ

sin θ∂Sn
∂θ

+
1
sin2 θ
∂2Sn
∂φ2 + n(n + 1)Sn = 0.
Separable solutions take the form
Sn = (Am cos mφ + Bm sin mφ)F(θ),
with m an integer. The function F(θ) satisﬁes
1
sin θ
d
dθ

sin θdF
dθ

+

n(n + 1) −
m2
sin2 θ
	
F = 0.
If we now make the usual transformation, µ = cos θ, F(θ) = y(µ), we get
(1 −µ2)y′′ −2µy′ +

n(n + 1) −
m2
1 −µ2
	
y = 0,

54
LEGENDRE FUNCTIONS
which is the associated Legendre equation. Our solutions can therefore be written
in the form
u = rn(Am cos mφ + Bm sin mφ){Cn,mP m
n (cos θ) + Dn,mQm
n (cos θ)}.
The quantities rn cos mφ P m
n (cos θ), which appear as typical terms in the solution,
are called spherical harmonics and appear widely in the solution of linear bound-
ary value problems in spherical geometry. We will see how they arise in a quantum
mechanical description of the hydrogen atom in Section 4.3.6.
Exercises
2.1
Solve Legendre’s equation of order two, (1 −x2)y′′ −2xy′ + 6y = 0, by the
method of Frobenius. What is the simplest solution of this equation? By
using this simple solution and the reduction of order method ﬁnd a closed
form expression for Q1(x).
2.2
Use the generating function to evaluate (a) P ′
n(1), (b) Pn(0).
2.3
Prove that
(a)
P ′
n+1(x) −P ′
n−1(x) = (2n + 1)Pn(x),
(b)
(1 −x2)P ′
n(x) = nPn−1(x) −nxPn(x).
2.4
Find the ﬁrst four nonzero terms in the Fourier–Legendre expansion of the
function
f(x) =
 0
for −1 < x < 0,
1
for 0 < x < 1.
What value will this series have at x = 0?
2.5
Establish the results
(a)
 1
−1
xPn(x)Pn−1(x)dx =
2n
4n2 −1 for n = 1, 2, . . . ,
(b)
 1
−1
Pn(x)P ′
n+1(x)dx = 2 for n = 0, 1, . . . ,
(c)
 1
−1
xP ′
n(x)Pn(x)dx =
2n
2n + 1 for n = 0, 1, . . . .
2.6
Determine the Wronskian of Pn and Qn for n = 0, 1, 2, . . . .
2.7
Solve the axisymmetric boundary value problem for Laplace’s equation,
∇2T
=
0 for 0 < r < a, 0 < θ < π,
T(a, θ)
=
2 cos5 θ.
2.8
Show that
(a) P 2
3 (x)
=
15x(1 −x2),
(b) P 1
4 (x)
=
5
2(7x3 −3x)(1 −x2)1/2.

EXERCISES
55
2.9
∗Prove that |P ′
n(x)| < n2 and |P ′′
n (x)| < n4 for −1 < x < 1.
2.10
Derive Equation (2.10).
2.11
∗Find the solution of the Dirichlet problem, ∇2Φ = 0 in r > 2 subject to
Φ →0 as r →∞and Φ(2, θ, φ) = sin2 θ cos 2φ.
2.12
∗The self-adjoint form of the associated Legendre equation is
d
dx

(1 −x2)P m
n (x)

+

n(n + 1) −
m2
1 −x2
	
P m
n (x) = 0.
Using this directly, prove the orthogonality property
 1
−1
P m
l (x)P m
n (x)dx = 0 for l ̸= n.
Evaluate
 1
−1
[P m
m (x)]2 dx.
2.13
(a) Suppose Pn(x0) = 0 for some x0 ∈(−1, 1). Show that x0 is a simple
zero.
(b) Show that Pn with n ⩾1 has n distinct zeros in (−1, 1).
2.14
Project A simpliﬁed model for the left ventricle of a human heart is pro-
vided by a sphere of time-dependent radius R = R(t) with a circular aortic
opening of constant area A, as shown in Figure 2.5. During contraction
we suppose that the opening remains ﬁxed whilst the centre of the sphere
moves directly toward the centre of the opening and the radius R(t) de-
creases accordingly.
As a result, some of the blood ﬁlling the ventricle
cavity is ejected though the opening with mean speed U = U(t) into the
attached cylindrical aorta. This occurs suﬃciently rapidly that we can as-
sume that the ﬂow is inviscid, irrotational, incompressible, and symmetric
with respect to the aortal axis.
(a) State a partial diﬀerential equation appropriate to the ﬂuid ﬂow for
this situation.
(b) During contraction, show that a point on the surface of the ventricle
has velocity
˙Rn −( ˙Ra)i,
where n is the outward unit normal at time t, i is the unit vector
in the aortic ﬂow direction and a = cos α(t).
Show that having
(R sin α)2 = R2(1 −a2) constant gives
∂φ
∂n = f(s) =

Us
for a < s < 1,
˙R(1 −s/a)
for −1 < s < a,
where s = cos θ with θ the usual spherical polar coordinate.

56
LEGENDRE FUNCTIONS
Aorta
U (t)
α (t)
R (t)
V
e
n
t
r
i
c
l
e
i
Fig. 2.5. A simple model for the left ventricle of the human heart.
(c) Show that, for a solution to exist,
' 1
−1 f(s)ds = 0.
Deduce that
˙R = [a(a −1)/(a + 1)]U, which relates the geometry to the mean
aortal speed.
(d) Let V = V (t) denote both the interior of the sphere at time t and
its volume. The total momentum in the direction of i is the blood-
density times the integral
I =

V
∇φ · idV =

S
(sφ)|r=RdS,
where S is the surface of V . Hence show that
I = 2πR2
∞

n=0
cn
 1
−1
sPn(s)ds = 4
3πR2c1 = 3
2V
 1
−1
sf(s) ds.

EXERCISES
57
(e) Use the answer to part (c) to show that 4I = (1−a)(a2+4+3a)V U.
(f) Explain how this model could be made more realistic in terms of the
ﬂuid mechanics and the physiology. You may like to refer to Pedley
(1980) for some ideas on this.

CHAPTER THREE
Bessel Functions
In this chapter, we will discuss a class of functions known as Bessel functions.
These are named after the German mathematician and astronomer Friedrich Bessel,
who ﬁrst used them to analyze planetary orbits, as we shall discuss later. Bessel
functions occur in many other physical problems, usually in a cylindrical geometry,
and we will discuss some examples of these at the end of this chapter.
Bessel’s equation can be written in the form
x2 d 2y
dx2 + xdy
dx +

x2 −ν2
y = 0,
(3.1)
with ν real and positive. Note that (3.1) has a regular singular point at x = 0.
Using the notation of Chapter 1,
xQ
P
= x2
x2 = 1,
x2R
P
= x2 
x2 −ν2
x2
= x2 −ν2,
both of which are polynomials and have Taylor expansions with inﬁnite radii of
convergence. Any series solution will therefore also have an inﬁnite radius of con-
vergence.
3.1
The Gamma Function and the Pockhammer Symbol
Before we use the method of Frobenius to construct the solutions of Bessel’s equa-
tion, it will be useful for us to make a couple of deﬁnitions. The gamma function
is deﬁned by
Γ(x) =
 ∞
0
e−qqx−1 dq,
for x > 0.
(3.2)
Note that the integration is over the dummy variable q and x is treated as constant
during the integration. We will start by considering the function evaluated at x = 1.
By deﬁnition,
Γ(1) =
 ∞
0
e−q dq = 1.
We also note that
Γ(x + 1) =
 ∞
0
e−qqx dq,

3.1 THE GAMMA FUNCTION AND THE POCKHAMMER SYMBOL
59
which can be integrated by parts to give
Γ(x + 1) =
(
−qxe−q)∞
0 +
 ∞
0
e−qxqx−1 dq = x
 ∞
0
e−qqx−1 dq = xΓ(x).
We therefore have the recursion formula
Γ(x + 1) = xΓ(x).
(3.3)
Suppose that x = n is a positive integer. Then
Γ(n + 1) = nΓ(n) = n(n −1)Γ(n −1) = · · · = n(n −1) . . . 2 · 1 = n!.
(3.4)
We therefore have the useful result, Γ(n + 1) = n! for n a positive integer.
We will often need to know Γ (1/2). Firstly, consider the deﬁnition,
Γ
1
2

=
 ∞
0
e−qq−1/2dq.
If we introduce the new variable Q = √q, so that dQ = 1
2q−1/2dq, this integral
becomes
Γ
1
2

= 2
 ∞
0
e−Q2dQ.
We can also write this integral in terms of another new variable, Q = ˜Q, to obtain

Γ
1
2
	2
=

2
 ∞
0
e−Q2dQ
 
2
 ∞
0
e−˜
Q2d ˜Q

.
Since the limits are independent, we can combine the integrals as

Γ
1
2
	2
= 4
 ∞
0
 ∞
0
e−(Q2+ ˜
Q2) dQ d ˜Q.
If we now change to standard polar coordinates we have dQ d ˜Q = r dr dθ, where
Q = r cos θ and ˜Q = r sin θ, and hence

Γ
1
2
	2
= 4
 π/2
θ=0
 ∞
r=0
e−r2r dr dθ.
The limits of integration give us the positive quadrant of the (Q, ˜Q)-plane, as re-
quired. Performing the integration over θ we have

Γ
1
2
	2
= 2π
 ∞
0
re−r2 dr,
and integrating with respect to r gives

Γ
1
2
	2
= 2π

−1
2e−r2∞
0
= π.
Finally, we have
Γ
1
2

= 2
 ∞
0
e−Q2 dQ = √π.
(3.5)

60
BESSEL FUNCTIONS
We can use Γ(x) = Γ(x + 1)/x to deﬁne Γ(x) for negative values of x.
For
example,
Γ

−1
2

= Γ

−1
2 + 1

−1
2
= −2Γ
1
2

= −2√π.
We also ﬁnd that Γ(x) is singular at x = 0. From the deﬁnition, (3.2), the integrand
diverges like 1/q as q →0, which is not integrable. Alternatively, Γ(x) = Γ(x+1)/x
shows that Γ(x) ∼1/x as x →0. Note that the gamma function is available in
MATLAB as the function gamma.
The Pockhammer symbol is a simple way of writing down long products. It
is deﬁned as
(α)r = α(α + 1)(α + 2) . . . (α + r −1),
so that, for example, (α)1 = α and (α)2 = α(α + 1), and, in general, (α)r is a
product of r terms. We also choose to deﬁne (α)0 = 1. Note that (1)n = n!. A
relationship between the gamma function and the Pockhammer symbol that we will
need later is
Γ(x) (x)n = Γ(x + n)
(3.6)
for x real and n a positive integer. To derive this, we start with the deﬁnition of
the Pockhammer symbol,
(x)n = x(x + 1)(x + 2) . . . (x + n −1).
Now
Γ(x) (x)n = Γ(x) {x(x + 1)(x + 2) . . . (x + n −1)}
= {Γ(x)x} {(x + 1)(x + 2) . . . (x + n −1)} .
Using the recursion relation (3.3),
Γ(x) (x)n = Γ(x + 1) {(x + 1)(x + 2) . . . (x + n −1)} .
We can repeat this to give
Γ(x) (x)n = Γ(x + n −1) (x + n −1) = Γ(x + n).
3.2
Series Solutions of Bessel’s Equation
We can now proceed to consider a Frobenius solution,
y(x) =
∞

n=0
anxn+c.
When substituted into (3.1), this yields
x2
∞

n=0
an(n + c)(n + c −1)xn+c−2 + x
∞

n=0
an(n + c)xn+c−1

3.2 SERIES SOLUTIONS OF BESSEL’S EQUATION
61
+(x2 −ν2)
∞

n=0
anxn+c = 0.
We can now rearrange this equation and combine corresponding terms to obtain
∞

n=0
an{(n + c)2 −ν2}xn+c +
∞

n=0
anxn+c+2 = 0.
We can extract two terms from the ﬁrst summation and then shift the second one
to obtain
a0(c2 −ν2)xc + a1{(1 + c)2 −ν2}xc+1
+
∞

n=2
[an{(n + c)2 −ν2} + an−2]xn+c = 0.
(3.7)
The indicial equation is therefore c2 −ν2 = 0, so that c = ±ν.
We can now distinguish various cases. We start with the case for which the
diﬀerence between the two roots of the indicial equation, 2ν, is not an integer. Using
Frobenius General Rule II, we can consider both roots of the indicial equation at
once. From the second term of (3.7), which is proportional to xc+1, we have
a1{(1 ± ν)2 −ν2} = a1(1 ± 2ν) = 0,
which implies that a1 = 0 since 2ν is not an integer. The recurrence relation that
we obtain from the general term of (3.7) is
an

(n ± ν)2 −ν2
+ an−2 = 0 for n = 2, 3, . . . ,
and hence an = 0 for n odd. Note that it is not possible for the series solution
to terminate and give a polynomial solution, which is what happens to give the
Legendre polynomials, which we studied in the previous chapter. This makes the
Bessel functions rather more diﬃcult to study.
We will now determine an expression for the value of an for general values of the
parameter ν. The recurrence relation gives us
an = −
an−2
n(n ± 2ν).
Let’s start with n = 2, which yields
a2 = −
a0
2(2 ± 2ν),
and now with n = 4,
a4 = −
a2
4(4 ± 2ν).
Substituting for a2 in terms of a0 gives
a4 =
a0
(4 ± 2ν)(2 ± 2ν) · 4 · 2 =
a0
22(2 ± ν)(1 ± ν)22(2 · 1).

62
BESSEL FUNCTIONS
We can continue this process, and we ﬁnd that
a2n = (−1)n
a0
22n (1 ± ν)n n!,
(3.8)
where we have used the Pockhammer symbol to simplify the expression. From this
expression for a2n,
y(x) = a0x±ν
∞

n=0
(−1)n
x2n
22n (1 ± ν)n n!.
With a suitable choice of a0 we can write this as
y(x) = A
x±ν
2±νΓ(1 ± ν)
∞

n=0
(−1)n

x2/4
n
(1 ± ν)n n! = AJ±ν(x).
These are the Bessel functions of order ±ν. The general solution of Bessel’s equa-
tion, (3.1), is therefore
y(x) = AJν(x) + BJ−ν(x),
for arbitrary constants A and B, with
J±ν(x) =
x±ν
2±νΓ(1 ± ν)
∞

n=0
(−1)n

x2/4
n
(1 ± ν)n n!.
(3.9)
Remember that 2ν is not an integer.
Let’s now consider what happens when ν = 0, in which case the indicial equation
has a repeated root and we need to apply Frobenius General Rule III. By setting
ν = 0 in the expression (3.8) for an and exploiting the fact that (1)n = n!, one
solution is
J0(x) =
∞

n=0
(−1)n
1
(n!)2
x2
4
n
.
Using Frobenius General Rule III, we can show that the other solution is
Y0(x) = J0(x) log x −
∞

n=0
(−1)n φ(n)
(n!)2
x2
4
n
,
which is called Weber’s Bessel function of order zero. This expression can be
derived by evaluating ∂y/∂c at c = 0. Note that we have made use of the function
φ(n) deﬁned in (1.21).
We now consider the case for which 2ν is a nonzero integer, beginning with 2ν
an odd integer, 2n + 1. In this case, the solution takes the form
y(x) = AJn+1/2(x) + BJ−n−1/2(x).
As an example, let’s consider the case ν = 1
2 so that Bessel’s equation is
d 2y
dx2 + 1
x
dy
dx +

1 −
1
4x2

y = 0.

3.2 SERIES SOLUTIONS OF BESSEL’S EQUATION
63
We considered this example in detail in Section 1.3.1, and found that
y(x) = a0
cos x
x1/2 + a1
sin x
x1/2 .
This means that (see Exercise 3.2)
J1/2(x) =
*
2
πx sin x,
J−1/2(x) =
*
2
πx cos x.
The recurrence relations (3.21) and (3.22), which we will derive later, then show
that Jn+1/2(x) and J−n−1/2(x) are products of ﬁnite polynomials with sin x and
cos x.
Finally we consider what happens when 2ν is an even integer, and hence ν is an
integer. A rather lengthy calculation allows us to write the solution in the form
y = AJν(x) + BYν(x),
where Yν is Weber’s Bessel function of order ν deﬁned as
Yν(x) = Jν(x) cos νπ −J−ν(x)
sin νπ
.
(3.10)
Notice that the denominator of this expression is obviously zero when ν is an integer,
so this case requires careful treatment. We note that the second solution of Bessel’s
equation can also be determined using the method of reduction of order as
y(x) = AJν(x) + BJν(x)
 x
1
q Jν(q)2 dq.
In Figure 3.1 we show Ji(x) for i = 0 to 3. Note that J0(0) = 1, but that Ji(0) = 0
for i > 0, and that J(j)
i
(0) = 0 for j < i, i > 1. We generated Figure 3.1 using the
MATLAB script
#
"
 
!
x=0:0.02:20;
subplot(2,2,1), plot(x,besselj(0,x)), title(’J_0(x)’)
subplot(2,2,2), plot(x,besselj(1,x)), title(’J_1(x)’)
subplot(2,2,3), plot(x,besselj(2,x)), title(’J_2(x)’)
subplot(2,2,4), plot(x,besselj(3,x)), title(’J_3(x)’)
We produced Figures 3.2, 3.5 and 3.6 in a similar way, using the MATLAB functions
bessely, besseli and besselk.
In Figure 3.2 we show the ﬁrst two Weber’s Bessel functions of integer order.
Notice that as x →0, Yn(x) →−∞. As you can see, all of these Bessel functions
are oscillatory. The ﬁrst three zeros of J0(x) are 2.4048, 5.5201 and 8.6537, whilst
the ﬁrst three nontrivial zeros of J1(x) are 3.8317, 7.0156 and 10.1735, all to four
decimal places.

64
BESSEL FUNCTIONS
Fig. 3.1. The functions J0(x), J1(x), J2(x) and J3(x).
Fig. 3.2. The functions Y0(x) and Y1(x).
3.3
The Generating Function for Jn(x), n an integer
Rather like the Legendre polynomials, there is a simple function that will generate
all of the Bessel functions of integer order. In order to establish this, it is useful to
manipulate the deﬁnition of the Jν(x). From (3.9), we have, for ν = n,
Jn(x) =
xn
2nΓ(1 + n)
∞

i=0

−x2/4
i
i!(1 + n)i
=
∞

i=0
(−1)ix2i+n
22i+ni!(1 + n)iΓ(1 + n).

3.3 THE GENERATING FUNCTION FOR Jn(x), n AN INTEGER
65
Using (3.6) we note that Γ(1 + n)(1 + n)i = Γ(1 + n + i), and using (3.4) we ﬁnd
that this is equal to (n + i)!. For n an integer, we can therefore write
Jn(x) =
∞

i=0
(−1)ix2i+n
22i+ni!(n + i)!.
(3.11)
Let’s now consider the generating function
g(x, t) = exp
1
2x

t −1
t
	
.
(3.12)
The series expansions of each of the constituents of this are
exp (xt/2) =
∞

j=0
(xt/2)j
j!
,
exp

−x
2t

=
∞

i=0
(−x/2t)i
i!
,
both from the Taylor series for ex. These summations can be combined to produce
g(x, t) =
∞

j=0
∞

i=0
(−1)ixi+jtj−i
2j+ii!j!
.
Now, putting j = i + n so that −∞⩽n ⩽∞, this becomes
g(x, t) =
∞

n=−∞
 ∞

i=0
(−1)ix2i+n
22i+ni!(n + i)!

tn.
Comparing the coeﬃcients in this series with the expression (3.11) we ﬁnd that
g(x, t) = exp
1
2x

t −1
t
	
=
∞

n=−∞
Jn(x)tn.
(3.13)
We can now exploit this relation to study Bessel functions.
Using the fact that the generating function is invariant under t →−1/t, we have
∞

n=−∞
Jn(x)tn =
∞

n=−∞
Jn(x)

−1
t
n
=
∞

n=−∞
Jn(x)(−1)nt−n.
Now putting m = −n, this is equal to
−∞

m=∞
J−m(x)(−1)mtm.
Now let n = m in the series on the right hand side, which gives
∞

n=−∞
Jn(x)tn =
∞

n=−∞
J−n(x)(−1)ntn.
Comparing like terms in the series, we ﬁnd that Jn(x) = (−1)nJ−n(x), and hence
that Jn(x) and J−n(x) are linearly dependent over R (see Section 1.2.1).
This
explains why the solution of Bessel’s equation proceeds rather diﬀerently when ν is
an integer, since Jν(x) and J−ν(x) cannot then be independent solutions.

66
BESSEL FUNCTIONS
The generating function can also be used to derive an integral representation of
Jn(x). The ﬁrst step is to put t = e±iθ in (3.13), which yields
e±ix sin θ = J0(x) +
∞

n=1
(±1)n 
eniθ + (−1)ne−niθ
Jn(x),
or, in terms of sine and cosine,
e±ix sin θ = J0(x) + 2
∞

n=1
J2n(x) cos 2nθ ± i
∞

n=0
J2n+1(x) sin(2n + 1)θ.
Appropriate combinations of these two cases show that
cos(x sin θ) = J0(x) + 2
∞

n=1
J2n(x) cos 2nθ,
(3.14)
sin(x sin θ) = 2
∞

n=0
J2n+1(x) sin (2n + 1) θ,
(3.15)
and, substituting η = π
2 −θ, we obtain
cos(x cos η) = J0(x) + 2
∞

n=1
(−1)nJ2n(x) cos 2nη,
(3.16)
sin(x cos η) = 2
∞

n=0
(−1)nJ2n+1(x) cos (2n + 1) η.
(3.17)
Multiplying (3.16) by cos mη and integrating from zero to π, we ﬁnd that
 π
η=0
cos mη cos(x cos η) dη =
 πJm(x)
for m even,
0
for m odd.
Similarly we ﬁnd that
 π
η=0
sin mη sin(x cos η) dη =

0
for m even,
πJm(x)
for m odd,
from (3.17). Adding these expressions gives us the integral representation
Jn(x) = 1
π
 π
0
cos (nθ −x sin θ) dθ.
(3.18)
We shall now describe a problem concerning planetary motion that makes use
of (3.18). This is the context in which Bessel originally studied these functions.
Example: Planetary motion
We consider the motion of a planet under the action of the gravitational force ex-
erted by the Sun. In doing this, we neglect the eﬀect of the gravitational attraction
of the other bodies in the solar system, which are all much less massive than the
Sun. Under this approximation, it is straightforward to prove Kepler’s ﬁrst and
second laws, that the planet moves on an ellipse, with the Sun at one of its foci,

3.3 THE GENERATING FUNCTION FOR Jn(x), n AN INTEGER
67
and that the line from the planet to the Sun (PS in Figure 3.3) sweeps out equal
areas of the ellipse in equal times (see, for example, Lunn, 1990). Our aim now
is to use Kepler’s ﬁrst and second laws to obtain a measure of how the passage of
the planet around its orbit depends upon time. We will denote the length of the
a
ea
Fig. 3.3. The elliptical orbit of a planet, P, around the Sun at a focus, S. Here, C is the
centre of the ellipse and A and A′ the extrema of the orbit on the major axis of the ellipse.
semi-major axis, A′C, by a and the eccentricity by e. Note that distance of the Sun
from the centre of the ellipse, SC, is ea. We also deﬁne the mean anomaly to be
µ = 2π Area of the elliptic sector ASP
Area of the ellipse
,
which Kepler’s second law tells us is proportional to the time of passage from A to
P.
Let’s now consider the auxiliary circle, which has centre C and passes through
A and A′, as shown in Figure 3.4. We label the projection of the point P onto the
auxiliary circle as Q. We will also need to introduce the eccentric anomaly of P,
which is deﬁned to be the angle ACQ, and can be written as
φ = 2π Area of the sector ACQ
Area of the auxiliary circle.
We now note that, by orthogonal projection, the ratio of the area of ASP to that
of the ellipse is the same as the ratio of the area of ASQ to that of the auxiliary
circle. The area of ASQ is given by the area of the sector ACQ ( 1
2φa2) minus the
area of the triangle CSQ ( 1
2ea2 sin φ), so that
µ
π =
1
2a2φ −1
2ea2 sin φ
1
2πa2
,
and hence
µ = φ −e sin φ.
(3.19)
Now, in order to determine φ as a function of µ, we note that φ −µ is a periodic

68
BESSEL FUNCTIONS
Fig. 3.4. The auxiliary circle and the projection of P onto Q.
function of µ, which vanishes when P and Q are coincident with A or A′, that is
when µ is an integer multiple of π. Hence we must be able to write
φ −µ =
∞

n=1
An sin nµ.
(3.20)
As we shall see in Chapter 5, this is a Fourier series. In order to determine the
constant coeﬃcients An, we diﬀerentiate (3.20) with respect to µ to yield
dφ
dµ −1 =
∞

n=1
nAn cos nµ.
We can now exploit the orthogonality of the functions cos nµ to determine An. We
multiply through by cos mµ and integrate from zero to π to obtain
 π
µ=0
dφ
dµ −1

cos mµ dµ =
 π
µ=0
cos mµdφ
dµ dµ = πm
2 Am.
Since φ = 0 when µ = 0 and φ = π when µ = π, we can change the independent
variable to give
πm
2 Am =
 π
φ=0
cos mµ dφ.
Substituting for µ from (3.19), we have that
Am =
2
mπ
 π
φ=0
cos m (φ −e sin φ) dφ.
Finally, by direct comparison with (3.18), Am = 2
mJm(me), so that
φ = µ + 2

J1(e) sin µ + 1
2J2(2e) sin 2µ + 1
3J3(3e) sin 3µ + · · ·
	
.

3.4 DIFFERENTIAL AND RECURRENCE RELATIONS
69
Since µ is proportional to the time the planet takes to travel from A to P, this
expression gives us the variation of the angle φ with time.
3.4
Diﬀerential and Recurrence Relations Between Bessel Functions
It is often useful to ﬁnd relationships between Bessel functions with diﬀerent indices.
We will derive two such relationships. We start with (3.9), multiply by xν and
diﬀerentiate to obtain
d
dx[xνJν(x)] = d
dx
 ∞

n=0
(−1)n x2n+2ν
22n+ν n! Γ(1 + ν + n)

=
∞

n=0
(−1)n (2n + 2ν) x2n+2ν−1
22n+ν n! Γ(1 + ν + n)
.
Since Γ(1 + ν + n) = (n + ν)Γ(n + ν), this gives a factor that cancels with the term
2(n + ν) in the numerator to give
d
dx[xνJν(x)] =
∞

n=0
(−1)nx2n xν xν−1
22n+ν−1 n! Γ(ν + n).
We can rewrite this so that we have the series expansion for Jν−1(x), as
d
dx[xνJν(x)] = xνxν−1
2ν−1
∞

n=0
(−1)n x2n
22n n! Γ(ν)(ν)n
,
so that
d
dx {xνJν(x)} = xνJν−1(x).
(3.21)
Later, we will use this expression to develop relations between general Bessel func-
tions. Note that by putting ν equal to zero
d
dx {J0(x)} = J−1(x).
However, we recall that Jn(x) = (−1)nJ−n(x) for n an integer, so that J1(x) =
−J−1(x) and hence
J′
0(x) = −J1(x),
where we have used a prime to denote the derivative with respect to x.
In the same vein as the derivation of (3.21),
d
dx(x−νJν(x)) = d
dx
%
x−νxν
2νΓ(1 + ν)
∞

n=0

−x2/4
n
n! (1 + ν)n
&
= d
dx
% ∞

n=0
(−1)n x2n
22n+ν n! Γ(1 + ν + n)
&
=
∞

n=0
(−1)n x2n−1 n
22n+ν−1 n! Γ(1 + ν + n).
Notice that the ﬁrst term in this series is zero (due to the factor of n in the numer-
ator), so we can start the series at n = 1 and cancel the factors of n. The series

70
BESSEL FUNCTIONS
can then be expressed in terms of the dummy variable m = n −1 as
d
dx(x−νJν(x)) =
1
2ν−1
∞

m=0
(−1)m+1 x2m+1
22m+2 m! Γ(ν + m + 2)
= −
1
2ν+1
∞

m=0
(−1)m x2m+1
22m m! Γ(ν + m + 2).
Using the fact that Γ(ν + m + 2) = Γ(ν + 2) (ν + 2)m and the series expansion of
Jν+1(x), (3.11), we have
d
dx

x−νJν(x)

= −
x
2ν+1Γ(ν + 1)
∞

m=0

−x2/4
m
m! (2 + ν)m
,
and consequently
d
dx

x−νJν(x)

= −x−νJν+1(x).
(3.22)
Notice that (3.21) and (3.22) both hold for Yν(x) as well.
We can use these relationships to derive recurrence relations between the
Bessel functions. We expand the diﬀerentials in each expression to give the equa-
tions
J′
ν(x) + ν
xJν(x) = Jν−1(x),
where we have divided through by xν, and
J′
ν(x) −ν
xJν(x) = −Jν+1(x),
where this time we have multiplied by xν. By adding these expressions we ﬁnd that
J′
ν(x) = 1
2 {Jν−1(x) −Jν+1(x)} ,
and by subtracting them
2ν
x Jν(x) = Jν−1(x) + Jν+1(x),
which is a pure recurrence relationship.
These results can also be used when integrating Bessel functions. For example,
consider the integral
I =

xJ0(x) dx.
This can be integrated using (3.21) with ν = 1, since
I =

xJ0(x) dx =

(xJ1(x))′ dx = xJ1(x).

3.5 MODIFIED BESSEL FUNCTIONS
71
3.5
Modiﬁed Bessel Functions
We will now consider the solutions of the modiﬁed Bessel equation,
x2 d 2y
dx2 + xdy
dx −(x2 + ν2)y = 0,
(3.23)
which can be derived from Bessel’s equation using x →ix, so that the solutions
could be written down as conventional Bessel functions with purely imaginary ar-
guments. However, it is more transparent to introduce the modiﬁed Bessel func-
tion of ﬁrst kind of order ν, Iν(x), so that the complete solution of (3.23) is
y(x) = AIν(x) + BI−ν(x),
provided ν is not an integer. As was the case when we introduced the function Yν(x),
there is a corresponding function here Kν(x), the modiﬁed Bessel function of
second kind of order ν. This is deﬁned as
Kν(x) =
π
2 sin νπ {I−ν(x) −Iν(x)} .
Note that the slight diﬀerence between the deﬁnition of this Bessel function and
that of Weber’s Bessel function of order ν, (3.10), occurs because these functions
must agree with the deﬁnition of the ordinary Bessel functions when ν is an integer.
Most of the results we have derived in this chapter can be modiﬁed by changing the
argument from x to ix in deriving, for example, the recurrence relations. The ﬁrst
few modiﬁed Bessel functions are shown in Figures 3.5 and 3.6. Note the contrast
in behaviour between these and the Bessel functions Jν(x) and Yν(x).
Equations (3.21) and (3.22) also hold for the modiﬁed Bessel functions and are
given by
d
dx {xνIν(x)} = xνIν−1(x),
d
dx

x−νIν(x)

= x−νIν+1(x)
and
d
dx {xνKν(x)} = −xνKν−1(x),
d
dx

x−νKν(x)

= −x−νKν+1(x).
3.6
Orthogonality of the Bessel Functions
In this section we will show that the Bessel functions are orthogonal, and hence can
be used as a basis over a certain interval. This will then allow us to develop the
Fourier–Bessel series, which can be used to represent functions in much the same
way as the Fourier–Legendre series.
We will consider the interval [0, a] where, at this stage, a remains arbitrary. We
start from the fact that the function Jν(x) satisﬁes the Bessel equation, (3.1), and
make a simple transformation, replacing x by λx, where λ is a real constant, to
give
x2 d 2
dx2 Jν(λx) + x d
dxJν(λx) + (λ2x2 −ν2)Jν(λx) = 0.
(3.24)
We choose λ so that Jν(λa) is equal to zero. There is a countably inﬁnite number

72
BESSEL FUNCTIONS
Fig. 3.5. The modiﬁed Bessel functions of ﬁrst kind of order zero to three. Note that
they are bounded at x = 0 and monotone increasing for x > 0, with In(x) ∼ex/
√
2πx as
x →∞.
Fig. 3.6. The modiﬁed Bessel functions of the second kind of order zero and order one.
Note that they are singular, with K0(x) ∼−log x and Kn(x) ∼2n−1(n −1)!/xn for n a
positive integer, as x →0. These functions are monotone decreasing for x > 0, tending to
zero exponentially fast as x →∞.
of values of λ for which this is true, as we can deduce from Figure 3.1. Now choose
µ ̸= λ so that Jν(µa) = 0. Of course Jν(µx) also satisﬁes (3.24) with λ replaced by
µ. We now multiply (3.24) through by Jν(µx)/x and integrate between zero and
a, which yields
 a
0
Jν(µx)

x d 2
dx2 Jν(λx) + d
dxJν(λx) + 1
x(λ2x2 −ν2)Jν(λx)
	
dx = 0.
(3.25)

3.6 ORTHOGONALITY OF THE BESSEL FUNCTIONS
73
Notice that we could have also multiplied the diﬀerential equation for Jν(µx) by
Jν(λx)/x and integrated to give
 a
0
Jν(λx)

x d 2
dx2 Jν(µx) + d
dxJν(µx) + 1
x(µ2x2 −ν2)Jν (µx)
	
dx = 0.
(3.26)
We now subtract (3.25) from (3.26) to give
 a
0

Jν(µx) d
dx

x d
dxJν(λx)

−Jν(λx) d
dx

x d
dxJν(µx)

+ x(λ2 −µ2)Jν(λx)Jν(µx)
	
dx = 0.
(3.27)
We have simpliﬁed these expressions slightly by observing that xJ′′
ν + J′
ν = (xJ′
ν)′.
Now consider the ﬁrst term of the integrand and note that it can be integrated by
parts to give
 a
0
Jν(µx) d
dx

x d
dxJν(λx)

dx =

Jν(µx)x d
dxJν(λx)
a
0
−
 a
0
x d
dxJν(λx) d
dxJν(µx)dx.
Similarly for the second term, which is eﬀectively the same with µ and λ inter-
changed,
 a
0
Jν(λx) d
dx

x d
dxJν(µx)

dx =

Jν(λx)x d
dxJν(µx)
a
0
−
 a
0
x d
dxJν(µx) d
dxJν(λx)dx.
Using these expressions in (3.27), we ﬁnd that
(λ2 −µ2)
 a
0
xJν(λx)Jν(µx)dx = Jν(λa)aµJ′
ν(µa) −Jν(µa)aλJ′
ν(λa).
(3.28)
Finally, since we chose λ and µ so that Jν(λa) = Jν(µa) = 0 and µ ̸= λ,
 a
0
xJν(µx)Jν(λx)dx = 0.
This is an orthogonality relation for the Bessel functions, with weighting function
w(x) = x. We now need to calculate the value of
' a
0 xJν(µx)2 dx. To this end, we
substitute λ = µ + ϵ into (3.28). For ϵ ≪1, neglecting terms in ϵ2 and smaller, we
ﬁnd that
−2µϵ
 a
0
xJν(µx)Jν(µx + ϵx) dx
= a [µJν(µa + ϵa)J′
ν(µa) −(µ + ϵ)Jν(µa)J′
ν(µa + ϵa)] .
(3.29)
In order to deal with the terms evaluated at x = a(µ + ϵ) we consider Taylor series

74
BESSEL FUNCTIONS
expansions, Jν(a(µ + ϵ)) = Jν(aµ) + ϵaJ′
ν(aµ) + · · · , J′
ν(a(µ + ϵ)) = J′
ν(aµ) +
ϵaJ′′
ν (aµ) + · · · . These expansions can then be substituted into (3.29). On dividing
through by ϵ and considering the limit ϵ →0, we ﬁnd that
 a
0
x[Jν(µx)]2 dx = a2
2 [J′
ν(µa)]2 −1
2a2Jν(µa)J′′
ν (µa) −a
2µJν(µa)J′
ν(µa).
We now suppose that Jν(µa) = 0, which gives
 a
0
x[Jν(µx)]2 dx = a2
2 [J′
ν(µa)]2.
In general,
 a
0
xJν(µx)Jν(λx) dx = a2
2 [J′
ν(µa)]2δλµ,
(3.30)
where Jν(µa) = Jν(λa) = 0 and δλµ is the Kronecker delta function.
We can now construct a series expansion of the form
f(x) =
∞

i=1
CiJν(λix).
(3.31)
This is known as a Fourier–Bessel series, and the λi are chosen such that
Jν(λia) = 0 for i = 1, 2, . . . , λ1 < λ2 < · · · . As we shall see later, both f(x)
and f ′(x) must be piecewise continuous for this series to converge. After multiply-
ing both sides of (3.31) by xJν(λjx) and integrating over the interval [0, a] we ﬁnd
that
 a
0
xJν(λjx)f(x) dx =
 a
0
xJν(λjx)
∞

i=1
CiJν(λix) dx.
Assuming that the series converges, we can interchange the integral and the sum-
mation to obtain
 a
0
xJν(λjx)f(x) dx =
∞

i=1
Ci
 a
0
xJν(λjx)Jν(λix) dx.
We can now use (3.30) to give
 a
0
xJν(λjx)f(x) dx =
∞

i=1
Ci
a2
2 [J′
ν(λja)]2 δλjλi = Cj
λja2
2
[J′
ν(λja)]2 ,
and hence
Cj =
2
a2 [J′ν(λja)]2
 a
0
xJν(λjx)f(x) dx.
(3.32)
Example
Let’s try to expand the function f(x) = 1 on the interval 0 ⩽x ⩽1, as a Fourier–
Bessel series. Since J0(0) = 1 but Ji(0) = 0 for i > 0, we will choose ν to be zero for
our expansion. We rely on the existence of a set of values λj such that J0(λj) = 0

3.6 ORTHOGONALITY OF THE BESSEL FUNCTIONS
75
for j = 1, 2, . . . (see Figure 3.1). We will need to determine these values either from
tables or numerically.
Using (3.32), we have
Cj =
2
[J′
0(λj)]2
 1
0
xJ0(λjx) dx.
If we introduce the variable y = λjx (so that dy = λjdx), the integral becomes
Cj =
2
[J′
0(λj)]2
1
λ2
j
 λj
y=0
yJ0(y) dy.
Using the expression (3.21) with ν = 1, we have
Cj =
2
[J′
0(λj)]2
1
λ2
j
 λj
y=0
d
dy (yJ1(y)) dy
=
2
λ2
j [J′
0(λj)]2 [yJ1(y)]λj
0 =
2J1(λj)
λj [J′
0(λj)]2 =
2
λjJ1(λj),
and hence
∞

i=1
2
λiJ1(λi)J0(λix) = 1 for 0 ⩽x < 1,
where J0(λi) = 0 for i = 1, 2, . . . . In Figure 3.7 we show the sum of the ﬁrst ﬁfteen
terms of the Fourier–Bessel series. Notice the oscillatory nature of the solution,
which is more pronounced in the neighbourhood of the discontinuity at x = 1. This
phenomenon always occurs in series expansions relative to sequences of orthogonal
functions, and is called Gibbs’ phenomenon.
Before we can give a MATLAB script that produces Figure 3.7, we need to be
able to calculate the zeros of Bessel functions. Here we merely state a couple of
simple results and explain how these are helpful. The interested reader is referred
to Watson (1922, Chapter 15) for a full discussion of this problem. The Bessel–
Lommel theorem on the location of the zeros of the Bessel functions Jν(x) states
that when −1
2 < ν ⩽1
2 and mπ < x < (m + 1
2)π, Jν(x) is positive for even m and
negative for odd m. This implies that Jν(x) has an odd number of zeros in the
intervals (2n −1)π/2 < x < 2nπ for n an integer. In fact, it can be shown that the
positive zeros of J0(x) lie in the intervals nπ + 3π/4 < x < nπ + 7π/8. This allows
us to use the ends of these intervals as an initial bracketing interval for the roots
of J0(x). A simple MATLAB script that uses this result is
'
&
$
%
global nu ze
nu=0;
for ir=1:20
start_int = (ir-1)*pi+3*pi/4;
end_int
= (ir-1)*pi+7*pi/8;
ze(ir) = fzero(’bessel’,[start_int end_int]);
end

76
BESSEL FUNCTIONS
Fig. 3.7. Fourier–Bessel series representation of the function f(x) = 1 on the interval [0, 1)
(truncated after ﬁfteen terms). The expansion is only valid in the interval x ∈[0, 1). The
series is shown for x > 1 only to demonstrate that convergence is only guaranteed for the
associated interval.
together with the function




function bessel = bessel(x);
global nu
bessel = besselj(nu,x);
The MATLAB function fzero ﬁnds a zero of functions of a single variable in a given
interval. By deﬁning the variables nu and ze as global, we make them available
to other functions. In particular, this allows us to use the computed positions of
the zeros, ze, in the script below.
The zeros of J1(x) interlace those of J0(x).
We can see this by noting that
J1(x) = −J′
0(x) and that both functions are continuous. Consequently the zeros of
J0(x) can be used as the bracketing intervals for the determination of the zeros of
J1(x). A MATLAB script for this is

3.7 INHOMOGENEOUS TERMS IN BESSEL’S EQUATION
77
'
&
$
%
global nu ze ze1
nu=1;
for ir=1:19
start_int = ze(ir);
end_int
= ze(ir+1);
ze1(ir)
= fzero(’bessel’,[start_int end_int]);
end
The zeros of the other Bessel functions can be found by exploiting similar analytical
results.
We can now deﬁne a MATLAB function
'
&
$
%
function fourierbessel = fourierbessel(x)
global ze
n=15; a = 2./ze(1:n)./besselj(1,ze(1:n));
for k = 1:n
X(:,k) = besselj(0,ze(k)*x(:));
end
fourierbessel = X*a’;
which can be plotted with ezplot(@fourierbessel, [0 2]) to produce Figure 3.7.
3.7
Inhomogeneous Terms in Bessel’s Equation
So far we have only concerned ourselves with homogeneous forms of Bessel’s equa-
tion. The inhomogeneous version of Bessel’s equation,
x2 d 2y
dx2 + xdy
dx + (x2 −ν2)y = f(x),
can be dealt with by using the technique of variation of parameters (see Section 1.2).
The solution can be written as
y(x) =
 x s sin νπ
2ν
f(s) (Jν(s)Yν(x) −Jν(x)Yν(s)) ds + AJν(x) + BYν(x). (3.33)
Here we have made use of the fact that the Wronskian associated with Jν(x) and
Yν(x) is 2ν/x sin νπ, which can be derived using Abel’s formula, (1.7). The constant
can then be found by considering the behaviour of the functions close to x = 0 (see
Exercise 3.5).
Example
Let’s ﬁnd the general solution of
x2 d 2y
dx2 + xdy
dx + x2y = x2.

78
BESSEL FUNCTIONS
This can be determined by using (3.33) with f(x) = x2 and ν = 0, so that the
general solution is
y(x) =
 x πs
2 (J0(s)Y0(x) −J0(x)Y0(s)) ds + AJ0(x) + BY0(x).
In order to integrate sJ0(s) we note that this can be written as (sJ1(s))′ and
similarly for sY0(s), which gives
y(x) = πx
2 (J1(x)Y0(x) −J0(x)Y1(x)) + AJ0(x) + BY0(x).
But we note that J1(x) = −J′
0(x) and Y1(x) = −Y ′
0(x), so that the expression in
the brackets is merely the Wronskian, and hence
y(x) = 1 + AJ0(x) + BY0(x).
Although it is clear, with hindsight, that y(x) = 1 is the particular integral solution,
simple solutions are not always easy to spot a priori.
Example
Let’s ﬁnd the particular integral of
x2 d 2y
dx2 + xdy
dx + (x2 −ν2)y = x.
(3.34)
We will look for a series solution as used in the method of Frobenius, namely
y(x) =
∞

n=0
anxn+c.
Substituting this into (3.34), we obtain an expression similar to (3.7),
a0(c2 −ν2)xc + a1{(1 + c)2 −ν2}xc+1
+
∞

n=2
[an{(n + c)2 −ν2} + an−2]xn+c = x.
Note that xc needs to match with the x on the right hand side so that c = 1 and
a0 = 1/(1 −ν2). We will defer discussion of the case ν = 1. At next order we ﬁnd
that a1(22 −ν2) = 0, and consequently, unless ν = 2, we have a1 = 0. For the
general terms in the summation we have
an = −
an−2
{(n + 1)2 −ν2}.
Note that since a1 = 0, an = 0 for n odd. It now remains to determine the general
term in the sequence. For n = 2 we ﬁnd that
a2 = −
a0
32 −ν2 = −
1
(12 −ν2)(32 −ν2)
and then with n = 4 and using the form of a2 we have
a4 =
1
(12 −ν2)(32 −ν2)(52 −ν2).

3.8 SOLUTIONS EXPRESSIBLE AS BESSEL FUNCTIONS
79
The general expression is
a2n = (−1)n
n+1

i=1
1
(2i −1)2 −ν2 .
This can be manipulated by factorizing the denominator and extracting a factor of
22 from each term in the product (of which there are n + 1). This gives
a2n = (−1)n
22n+2
n+1

i=1
1
i −1
2 + 1
2ν
n+1

i=1
1
i −1
2 −1
2ν = (−1)n
22n+2
1
 1
2 + 1
2ν

n+1
 1
2 −1
2ν

n+1
.
Hence the particular integral of the diﬀerential equation is
y(x) = x
∞

n=0
(−1)n
22n+2
x2n
 1
2 + 1
2ν

n+1
 1
2 −1
2ν

n+1
.
(3.35)
In fact, solutions of the equation
x2 d 2y
dx2 + xdy
dx + (x2 −ν2)y = xµ+1
(3.36)
are commonly referred to as sµ,ν and are called Lommel’s functions. They are
undeﬁned when µ ± ν is an odd negative integer, a case that is discussed in depth
by Watson (1922). The series expansion of the solution of (3.36) is
y(x) = xµ−1
∞

m=0
(−1)m  1
2x
2m+2 Γ
 1
2µ −1
2ν + 1
2

Γ
 1
2µ + 1
2ν + 1
2

Γ
 1
2µ −1
2ν + m + 3
2

Γ
 1
2µ + 1
2ν + m + 3
2

.
We can use this to check that (3.35) is correct. Note that we need to use (3.6).
3.8
Solutions Expressible as Bessel Functions
There are many other diﬀerential equations whose solutions can be written in terms
of Bessel functions. In order to determine some of these, we consider the transfor-
mation
y(x) = xα˜y(xβ).
Since α and β could be fractional, we will restrict our attention to x ⩾0. We
substitute this expression into the diﬀerential equation and seek values of α and β
which give Bessel’s equation for ˜y.
Example
Let’s try to express the solutions of the diﬀerential equation
d 2y
dx2 −xy = 0
in terms of Bessel functions. This is called Airy’s equation and has solutions
Ai(x) and Bi(x), the Airy functions. We start by introducing the function ˜y.

80
BESSEL FUNCTIONS
Diﬀerentiating with respect to x we have
dy
dx = αxα−1˜y + βxα+β−1˜y′.
Diﬀerentiating again we obtain
d 2y
dx2 = α(α −1)xα−2˜y + (2αβ + β2 −β)xα+β−2˜y′ + β2xα+2β−2˜y′′.
These expressions can now be substituted into Airy’s equation to give
α(α −1)xα−2˜y + (2αβ + β2 −β)xα+β−2˜y′ + β2xα+2β−2˜y′′ −xα+1˜y = 0.
It is now convenient to multiply the entire equation by x−α+2/β2 (this means that
the coeﬃcient of ˜y′′ is x2β), which gives
x2β ˜y′′ + (2α + β −1)
β
xβ ˜y′ +

−1
β2 x3 + α(α −1)
β2
	
˜y = 0.
Considering the coeﬃcient of ˜y we note that we require x3 ∝x2β which gives β = 3
2.
The coeﬃcient of ˜y′ gives us that α = 1
2. The equation is now
x2β ˜y′′ + xβ ˜y′ +

−
2xβ
3
2
−1
9

˜y = 0,
which has solutions K1/3(2x3/2/3) and I1/3(2x3/2/3). The general solution of Airy’s
equation in terms of Bessel’s functions is therefore
y(x) = x1/2

AK1/3
2
3x3/2

+ BI1/3
2
3x3/2
	
.
In fact, Ai(x) =
#
x/3K1/3(2x3/2/3)/π. A graph of this Airy function is shown in
Figure 11.12. The Airy functions Ai and Bi are available in MATLAB through the
function airy.
3.9
Physical Applications of the Bessel Functions
3.9.1
Vibrations of an Elastic Membrane
We will now derive the equation that governs small displacements, z = z(x, y, t),
of an elastic membrane.
We start by considering a small membrane with sides
of length δSx in the x-direction and δSy in the y-direction, which makes angles
ψx, ψx+δψx and ψy, ψy+δψy with the horizontal, as shown in Figure 3.8. Newton’s
second law of motion in the vertical, z-direction gives
∂2z
∂t2 ρδSxδSy = δSy{T sin(ψx + δψx) −T sin(ψx)} + δSx{T sin(ψy + δψy) −T sin(ψy)},
where ρ is the constant density (mass per unit area) of the membrane and T is the
tension, assumed constant for small vibrations of the membrane. We will eventually
consider the angles ψx and ψy to be small, but at the outset we will consider the

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
81
T
T
T
y
x
T
δSy
δSx
ψx + δψx
ψy + δψy
ψy
ψx
Fig. 3.8. A small section of an elastic membrane.
changes in these angles, δψx and δψy, to be smaller. Accordingly we expand the
trigonometric functions and ﬁnd that
ρ
T
∂2z
∂t2 = cos ψx
δψx
δSx
+ cos ψy
δψy
δSy
+ · · · ,
where we have divided through by the area of the element, δSxδSy. We now consider
the limit as the size of the element shrinks to zero, and therefore let δSx and δSy
tend to zero. Consequently we ﬁnd that
ρ
T
∂2z
∂t2 = cos ψx
∂ψx
∂Sx
+ cos ψy
∂ψy
∂Sy
.
(3.37)
We can now use the deﬁnition of the partial derivatives,
tan ψx = ∂z
∂x,
tan ψy = ∂z
∂y .
By diﬀerentiating these expressions with respect to x and y respectively we ﬁnd
that
sec2ψx
∂ψx
∂x = ∂2z
∂x2 ,
sec2ψy
∂ψy
∂y = ∂2z
∂y2 .
For small slopes, cos ψx and sec2ψx are both approximately unity (and similarly for
variation in the y direction). Also, using the formula for arc length we have
dSx =
+
1 +
 ∂z
∂x
2
dx ≈dx,

82
BESSEL FUNCTIONS
when |∂z/∂x| ≪1. Similarly dSy ≈dy when |∂z/∂y| ≪1. Consequently
∂ψx
∂Sx
≈∂ψx
∂x ,
∂ψy
∂Sy
≈∂ψy
∂y .
Combining this information yields the governing equation for small deﬂections z =
z(x, y, t) of an elastic membrane,
ρ
T
∂2z
∂t2 = ∂2z
∂x2 + ∂2z
∂y2 = ∇2z,
(3.38)
the two-dimensional wave equation. We will deﬁne appropriate boundary con-
ditions in due course. At this stage we have not speciﬁed the domain of solution.
One-Dimensional Solutions of the Wave Equation
We will start by considering the solution of this equation in one dimension.
If
we look for solutions of the form z ≡z(x, t), independent of y, as illustrated in
Figure 3.9, we need to solve the one-dimensional wave equation,
∂2z
∂t2 = c2 ∂2z
∂x2 ,
(3.39)
where c =
#
T/ρ. This equation also governs the propagation of small-amplitude
waves on a stretched string (for further details, see Billingham and King, 2001).
The easiest way to solve (3.39) is to deﬁne new variables, known as characteristic
Fig. 3.9. A one-dimensional solution of the two-dimensional wave equation.
variables, ξ = x −ct and η = x + ct (see Section 7.1 for an explanation of where

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
83
these come from). In terms of these variables, (3.39) becomes
∂2z
∂ξ∂η = 0.
Integrating this with respect to η gives
∂z
∂ξ = F(ξ),
and with respect to ξ
z =
 ξ
F(s) ds + g(η) = f(ξ) + f(η),
and hence we have that
z(x, t) = f(x −ct) + g(x + ct).
(3.40)
This represents the sum of two waves, one, represented by f(x −ct), propagating
from left to right without change of form at speed c, and one, represented by
g(x+ct), from right to left at speed c. To see this, consider, for example, the solution
y = f(x −ct), and simply note that on the paths x = ct + constant, f(x −ct) is
constant. Similarly, g(x + ct) is constant on the paths x = −ct + constant.
The functions f and g can be determined from the initial displacement and
velocity. If
z(x, 0) = z0(x),
∂z
∂t (x, 0) = u0(x),
then (3.40) with t = 0 gives us
f(x) + g(x) = z0(x).
(3.41)
If we now diﬀerentiate (3.40) with respect to time, we have
∂z
∂t = −cf ′(x −ct) + cg′(x + ct),
and hence when t = 0,
−cf ′(x) + cg′(x) = u0(x).
This can be integrated to yield
−cf(x) + cg(x) =
 x
a
u0(s) ds,
(3.42)
where a is an arbitrary constant. Solving the simultaneous equations (3.41) and
(3.42), we ﬁnd that
f(x) = 1
2z0(x) −1
2c
 x
a
u0(s) ds,
g(x) = 1
2z0(x) + 1
2c
 x
a
u0(s) ds.
On substituting these into (3.40), we obtain d’Alembert’s solution of the one-
dimensional wave equation,
z(x, t) = 1
2 {z0(x −ct) + z0(x + ct)} + 1
2c
 x+ct
x−ct
u0(s) ds.
(3.43)

84
BESSEL FUNCTIONS
In particular, if u0 = 0, a string released from rest, the solution consists of a
left-travelling and a right-travelling wave, each with the same shape but half the
amplitude of the initial displacement. The solution when z0 = 0 for |x| > a and
z0 = 1 for |x| < a, a top hat initial displacement, is shown in Figure 3.10.
Fig. 3.10. D’Alembert’s solution for an initially stationary top hat displacement.
Two-Dimensional Solutions of the Wave Equation
Let’s now consider the solution of the two-dimensional wave equation, (3.38), for
an elastic, disc-shaped membrane ﬁxed to a circular support. In cylindrical polar
coordinates, the two-dimensional wave equation becomes
1
c2
∂2z
∂t2 = ∂2z
∂r2 + 1
r
∂z
∂r + 1
r2
∂2z
∂θ2 .
(3.44)
We will look for solutions in the circular domain 0 ⩽r ⩽a and 0 ⩽θ < 2π. Such
solutions must be periodic in θ with period 2π. The boundary condition is z = 0 at
r = a. We seek a separable solution, z = R(r)τ(t)Θ(θ). On substituting this into
(3.44), we ﬁnd that
1
c2
τ ′′
τ = rR′′ + R′
rR
+ 1
r2
Θ′′
Θ = −ω2
c2 ,

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
85
where −ω2/c2 is the separation constant. An appropriate solution is τ = eiωt, which
represents a time-periodic solution. This is what we would expect for the vibrations
of an elastic membrane, which is, after all, a drum. The angular frequency, ω,
is yet to be determined. We can now write
r2R′′ + rR′
R
+ ω2r2
c2
= −Θ′′
Θ = n2,
where n2 is the next separation constant. This gives us Θ = A cos nθ + B sin nθ,
with n a positive integer for 2π-periodicity. Finally, R(r) satisﬁes
r2 d 2R
dr2 + rdR
dr +
ω2
c2 r2 −n2

R = 0.
We can simplify this by introducing a new coordinate s = λr where λ = ω/c, which
gives
s2 d 2R
ds2 + sdR
ds +

s2 −n2
R = 0,
which is Bessel’s equation with ν = n. Consequently the solutions can be written
as
R(s) = AJn(s) + BYn(s).
We need a solution that is bounded at the origin so, since Yn(s) is unbounded at
s = 0, we require B = 0. The other boundary condition is that the membrane is
constrained not to move at the edge of the domain, so that R(s) = 0 at s = λa.
This gives the condition
Jn(λa) = 0,
(3.45)
which has an inﬁnite number of solutions λ = λni. Specifying the value of λ = ω/c
prescribes the frequency at which the membrane will oscillate. Consequently the
functional form of the natural modes of oscillation of a circular membrane is
z = Jn(λnir) (A cos nθ + B sin nθ) eiωit,
(3.46)
where ωi = cλni and the values of λni are solutions of (3.45). Figure 3.11 shows
a few of these natural modes when a = 1, which we created using the MATLAB
function ezmesh (see Section 2.6.1).
Here we have considered the natural modes of oscillation. We could however
have tackled an initial value problem. Let’s consider an example where the initial
displacement of the membrane is speciﬁed to be z(r, θ, 0) = G(r, θ) and the mem-
brane is released from rest, so that ∂z/∂t = 0, when t = 0. The fact that the
membrane is released from rest implies that the temporal variation will be even, so
we need only consider a solution proportional to cos ωit. Consequently, using the
linearity of the wave equation to add all the possible solutions of the form (3.46),
the general form of the displacement of the membrane is
z(r, θ, t) =
∞

i=0
∞

n=0
Jn(λnir) (Ani cos nθ + Bni sin nθ) cos ωit.

86
BESSEL FUNCTIONS
Fig. 3.11. Six diﬀerent modes of oscillation of an elastic membrane.
Using the condition that z(r, θ, 0) = G(r, θ) we have the equations
G(r, θ) =
∞

i=0
∞

n=0
Jn(λnir) (Ani cos nθ + Bni sin nθ) .
(3.47)
In order to ﬁnd the coeﬃcients Ani and Bni we need exploit the orthogonality of
the Bessel functions, given by (3.30), and of the trigonometric functions, using
 2π
0
cos mθ cos nθ dθ =
 2π
0
sin mθ sin nθ dθ = πδmn,
for m and n integers. Multiplying (3.47) through by cos mθ and integrating from 0
to 2π we have
1
π
 2π
0
cos mθ G(r, θ) dθ =
∞

i=0
AmiJn(λnir),
and with sin mθ we have
1
π
 2π
0
sin mθ G(r, θ) dθ =
∞

i=0
BmiJn(λnir).

3.9 PHYSICAL APPLICATIONS OF THE BESSEL FUNCTIONS
87
Now, using (3.32), we have
Amj =
2
a2π [J′n(λja)]2
 a
r=0
rJn(λjr)
 2π
0
cos mθ G(r, θ) dθ dr
and
Bmj =
2
a2π [J′n(λja)]2
 a
r=0
rJn(λjr)
 2π
0
sin mθ G(r, θ) dθ dr.
Note that if the function G(r, θ) is even, Bmj = 0, and similarly if it is odd, Amj = 0.
The expansion in θ is an example of a Fourier series expansion, about which we
will have more to say in Chapter 5.
3.9.2
Frequency Modulation (FM)
We will now discuss an application in which Bessel functions occur within the
description of a modulated wave (for further information, see Dunlop and Smith,
1977). The expression for a frequency carrier comprises a carrier frequency fc and
the modulating signal with frequency fm. The phase shift of the carrier is related
to the time integral of the modulating wave Fm(t) = cos 2πfmt, so that the actual
signal is
Fc(t) = cos

2πfct + 2πK2
 t
0
a cos(2πfmt)dt
	
,
which we can integrate to obtain
Fc(t) = cos {2πfct + β sin(2πfmt)} ,
(3.48)
where β = K2a/fm, a constant called the modulation index. We can expand
(3.48) to give
Fc(t) = cos(2πfct) cos {β sin(2πfmt)} −sin(2πfct) sin {β sin(2πfmt)} .
(3.49)
An example of this signal is shown in Figure 3.12.
We would now like to split the signal (3.49) into its various frequency compo-
nents. We can do this by exploiting (3.16) and (3.17) to give
Fc(t) = cos(2πfct)

J0(β) + 2
∞

n=1
(−1)nJ2n(β) cos 2n(2πfmt)

+ sin(2πfct)

2
∞

n=0
(−1)nJ2n+1(β) cos (2n + 1) (2πfmt)

.
(3.50)
Using simple trigonometry,
2 cos(2πfct) cos {(2n)2πfmt} =
cos {2πfct + (2n)2πfmt} + cos {2πfct −(2n)2πfmt} ,

88
BESSEL FUNCTIONS
Fig. 3.12. An example of frequency modulation, with β = 1.2, fm = 1.2 and fc = 1.
and
2 sin(2πfct) sin {(2n + 1)2πfmt} =
−cos {2πfct + (2n + 1)2πfmt} + cos {2πfct −(2n + 1)2πfmt} .
Substituting these expressions back into (3.50), we ﬁnd that Fc(t) can be written
as
Fc(t) = J0(β) cos(2πfct)
+
∞

n=1
J2n(β) [cos {2πt (fc + 2nfm)} + cos {2πt (fc −2nfm)}]
−
∞

n=0
J2n+1(β) [cos {2πt (fc −(2n + 1)fm)} −cos {2πt (fc + (2n + 1)fm)}] ,
the ﬁrst few terms of which are given by
Fc(t) = J0(β) cos(2πfct) −J1(β) [cos {2π(fc −fm)t} −cos {2π(fc + fm)t}]
+ J2(β) [cos {2π(fc −2fm)t} + cos {2π(fc + 2fm)t}]
−J3(β) [cos {2π(fc −3fm)t} −cos {2π(fc + 3fm)t}] + · · · .
This means that the main carrier signal has frequency fc and amplitude J0(β), and
that the other components, known as sidebands, have frequencies fc ± nfm for

EXERCISES
89
Fig. 3.13. The frequency spectrum of a signal with β = 0.2, fm = 1.2 and fc = 1.
n an integer, and amplitudes Jn(β). These amplitudes, known as the frequency
spectrum of the signal, are shown in Figure 3.13.
Exercises
3.1
The functions J0(x) and Y0(x) are solutions of the equation
xd 2y
dx2 + dy
dx + xy = 0.
Show that if Y0(x) is the singular solution then
Y0(x) = J0(x) log x −
∞

i=1
φ(i)
(i!)2

−x2
4
i
,
where
φ(i) =
i

k=1
1
k .
3.2
Using the deﬁnition
Jν(x) =
xν
2νΓ(1 + ν)
∞

i=0
(−x2/4)i
i!(1 + ν)i
,
show that J1/2(x) =
$
2
πx sin x.

90
BESSEL FUNCTIONS
3.3
∗Show that
(a) 2J′
ν(x) = Jν−1(x) −Jν+1(x),
(b) 2νJν(x) = xJν+1(x) + xJν−1(x).
3.4
Determine the series expansions for the modiﬁed Bessel functions Iν(x) and
I−ν(x).
3.5
Find the Wronskian of the functions (a) Jν(x) and J−ν(x), (b) Jν(x) and
Yν(x) for ν not an integer.
3.6
Determine the series expansion of

xµJν(x) dx.
When µ = 1 and ν = 0 show that this is equivalent to xJ1(x).
3.7
Give the solutions, where possible in terms of Bessel functions, of the dif-
ferential equations
(a) d 2y
dx2 −x2y = 0,
(b) xd 2y
dx2 + dy
dx + y = 0,
(c) xd 2y
dx2 + (x + 1)2y = 0,
(d) d 2y
dx2 + α2y = 0,
(e) d 2y
dx2 −α2y = 0,
(f) d 2y
dx2 + β dy
dx + γy = 0,
(g) (1 −x2)d 2y
dx2 −2xdy
dx + n(n + 1)y = 0.
3.8
Using the expression
Jν(z) = 1
2π
 2π
0
cos (νθ −z sin θ) dθ,
show that Jν(0) = 0 for ν a nonzero integer and J0(0) = 1.
3.9
Determine the coeﬃcients of the Fourier–Bessel series for the function
f(x) =

1
for 0 ⩽x < 1,
−1
for 1 ⩽x ⩽2,
in terms of the Bessel function J0(x).
3.10
Determine the coeﬃcients of the Fourier–Bessel series for the function
f(x) = x on the interval 0 ⩽x ⩽1 in terms of the Bessel function J1(x)
(and repeat the exercise for J2(x)). Modify the MATLAB code used to
generate Figure 3.7 to check your answers.
3.11
Calculate the Fourier–Bessel expansion for the functions

EXERCISES
91
(a)
f(x) =

x
for 0 ⩽x < 1,
2 −x
for 1 ⩽x ⩽2,
(b)
f(x) =
 x2 + 2
for 0 ⩽x < 1,
3
for 1 ⩽x ⩽3,
in terms of the Bessel function J1(x).
3.12
Construct the general solution of the diﬀerential equation
x2 d 2y
dx2 + xdy
dx + (x2 −ν2)y = sin x.
3.13
Project This project arises from attempts to model the baking of food-
stuﬀs, and thereby improve the quality of mass-produced baked foods. A
key element of such modelling is the temperature distribution in the food,
which, as we showed in Chapter 2, is governed by the diﬀusion equation,
∂T
∂t = ∇· (D∇T) .
The diﬀusivity, D, is a function of the properties of the food.
We will consider some problems associated with the baking of inﬁnitely-
long, cylindrical, axisymmetric foodstuﬀs under axisymmetric conditions
(a ﬁrst approximation to, for example, the baking of a loaf of bread). In
this case, the diﬀusion equation becomes
∂T
∂t = 1
r
∂
∂r

rD(r)∂T
∂r

.
(E3.1)
(a) Look for a separable solution, T(r, t) = f(r)eωt, of (E3.1) with
D(r) = D0, a constant, subject to the boundary conditions ∂T/∂r =
0 at r = 0 and ∂T/∂r = h(Ta −T) at r = r0. Here, h > 0 is a heat
transfer coeﬃcient and Ta is the ambient temperature. Determine
the possible values of ω.
(b) If the initial temperature proﬁle within the foodstuﬀis T(r, 0) =
T0(r), determine the temperature for t > 0.
(c) In many baking problems, there are two distinct layers of food.
When
D(r) =
 D0
for 0 ⩽r ⩽r1,
D1
for r1 < r ⩽r0,
and the heat ﬂux and the temperature are continuous at r = r1,
determine the eﬀect of changing r1/r0 and D0 and D1 on the possible
values of ω.
(d) Solve the initial–boundary value problem when the initial tempera-
ture is uniform in each layer, with
T(r, 0) =
 T0
for 0 ⩽r ⩽r1,
T1
for r1 < r ⩽r0.

92
BESSEL FUNCTIONS
(e) Find some realistic values of D0 and h for bread dough, either from
your library or the Internet. How does the temperature at the centre
of baking bread vary with time, if this simple model is to be believed?

CHAPTER FOUR
Boundary Value Problems, Green’s Functions and
Sturm–Liouville Theory
We now turn our attention to boundary value problems for ordinary diﬀerential
equations, for which the boundary conditions are speciﬁed at two diﬀerent points,
x = a and x = b. The solutions of boundary value problems have some rather
diﬀerent properties to those of solutions of initial value problems.
In order to
ground our discussion in terms of a real physical problem, we will consider the
dynamics of a string ﬁxed at x = y = 0 and x = l, y = 0, and rotating steadily
about the x-axis at constant angular velocity, as shown in Figure 4.1.
This is
rather like a skipping rope, but one whose ends are motionless. In order to be able
θ(x)
x = l
ω
x
y
Fig. 4.1. A string rotating about the x-axis, ﬁxed at x = 0 and x = l.
to formulate a diﬀerential equation that captures the dynamics of this string, we
will make several assumptions.

94
BOUNDARY VALUE PROBLEMS
(i) The tension in the string is large enough that any additional forces intro-
duced by the bending of the string are negligible in comparison with the
tension force.
(ii) The tension force acts along the local tangent to the string, and is of constant
magnitude, T.
(iii) The slope of the string, and hence its displacement from the x-axis, is small.
(iv) The eﬀect of gravity is negligible.
(v) There is no friction, either due to air resistance on the string or to the ﬁxing
of the string at each of its ends.
(vi) The thickness of the string is negligible, but it has a constant line density,
ρ, a mass per unit length.
We denote the constant angular velocity of the string about the x-axis by ω,
and the angle that the tangent to the string makes with the x-axis by the function
θ(x). Working in a frame of reference that rotates with the string, in which the
string is stationary, Newton’s ﬁrst law shows that the forces that act on the string,
including the centrifugal force, must be in balance, and hence that
T sin θ(x + δx) −T sin θ(x) = −ρω2y
#
δx2 + δy2,
as shown in Figure 4.2. If we divide through by δx and take the limit δx →0 we
obtain
T d
dx {sin θ (x)} + ρω2y
+
1 +
dy
dx
2
= 0.
By deﬁnition, tan θ = dy/dx, so, by elementary trigonometry,
sin θ = dy
dx

1 +
dy
dx
2−1/2
,
and hence
T d 2y
dx2 + ρω2y

1 +
dy
dx
22
= 0.
(4.1)
This equation of motion for the string must be solved subject to y(0) = y(l) =
0, which constitutes a rather nasty nonlinear boundary value problem, with no
elementary solution apart from the equilibrium solution, y = 0.†
If, however,
we invoke assumption (iii), that |dy/dx| ≪1, the leading order boundary value
problem is
d 2y
dx2 + λy = 0,
subject to y(0) = y(l) = 0,
(4.2)
where λ = ρω2/T. This is now a linear boundary value problem, which we can
solve by elementary methods.
† This nonlinear boundary value problem can, however, easily be studied in the phase plane (see
Chapter 9).

BOUNDARY VALUE PROBLEMS
95
T
x
y
T
θ(x)
θ(x+ δx)
δx
δy
x+δx
x
Fig. 4.2. A small element of the rotating string.
If we look for a solution of the form y = Aemx, we obtain m2 +λ = 0, and hence
m = ±iλ1/2, so that the solution is y = Aeiλ1/2x + Be−iλ1/2x. Since y(0) = 0,
B = −A, and y(l) = 0 gives
eiλ1/2l −e−iλ1/2l = 0.
(4.3)
At this stage, we do not know whether λ is real, although, since it represents a
ratio of real quantities, it should be. If we write λ1/2 = α + iβ and equate real and
imaginary parts in (4.3), we ﬁnd that

e−βl −eβl
cos αl = 0,

e−βl + eβl
sin αl = 0.
(4.4)
From the ﬁrst of these, we have either e−βl −eβl = 0 or cos αl = 0, and hence
either β = 0 or αl =

n + 1
2

π for n an integer. The latter leaves the second of
equations (4.4) with no solution, so we must have β = 0, and hence sin αl = 0. This
gives αl = nπ with n an integer, which means that λ1/2 is real, as expected, with
λ1/2 = nπ/l and y = An

einπx/l −e−inπx/l
= 2iAn sin (nπx/l). To ensure that y,
the displacement of the string, is real, we write An = an/2i for an real, which gives
y = an sin
nπx
l

for n = 1, 2, . . . .
(4.5)
Note that the solution that corresponds to n = 0 is the trivial solution, y = 0.
The values of λ for which there is a nontrivial solution of this problem, namely
λ = n2π2/l2, are called the eigenvalues of the boundary value problem, whilst the
corresponding solutions, y = sin λ1/2x, are the eigenfunctions. Note that there is
an inﬁnite sequence of eigenvalues, which are real and have magnitudes that tend

96
BOUNDARY VALUE PROBLEMS
to inﬁnity as n tends to inﬁnity. In terms of the physical problem, for a string of
given line density ρ, length l and tension T, a nonequilibrium steady motion is only
possible at a discrete set of angular velocities, the eigenfrequencies,
ω = nπ
l
+
T
ρ .
4.1
Inhomogeneous Linear Boundary Value Problems
Continuing with our example of a rotating string, let’s consider what happens
when there is a steady, imposed external force TF(x) acting towards the x-axis.
The linearized boundary value problem is then
d 2y
dx2 + λy = F(x),
subject to y(0) = y(l) = 0.
(4.6)
We can solve this using the variation of parameters formula, (1.6), which gives
y(x) = A cos λ1/2x + B sin λ1/2x +
1
λ1/2
 x
0
F(s) sin λ1/2(x −s) ds.
To satisfy the boundary condition y(0) = 0 we need A = 0, whilst y(l) = 0 gives
B sin λ1/2l +
1
λ1/2
 l
0
F(s) sin λ1/2(x −s) ds = 0.
(4.7)
When λ is not an eigenvalue, sin λ1/2l ̸= 0, and (4.7) has a unique solution
B = −
1
λ1/2 sin λ1/2l
 l
0
F(s) sin λ1/2(x −s) ds.
However, when λ is an eigenvalue, we have sin λ1/2l = 0, so that there is a solution
for arbitrary values of B, namely
y(x) = B sin λ1/2x +
1
λ1/2
 x
0
F(s) sin λ1/2(x −s) ds,
(4.8)
provided that
 l
0
F(s) sin λ1/2(l −s) ds = 0.
(4.9)
If F(s) does not satisfy this integral constraint there is no solution.
These cases, where there may be either no solution or many solutions depending
on the form of F(x), are in complete contrast to the solutions of an initial value
problem, for which, as we shall see in Chapter 8, we can prove theorems that guar-
antee the existence and uniqueness of solutions, subject to a few mild conditions
on the form of the ordinary diﬀerential equation. This situation also arises for any
system that can be written in the form Ax = b, where A is a linear operator. A fa-
mous result (see, for example, Courant and Hilbert, 1937) known as the Fredholm
alternative, shows that either there is a unique solution of Ax = b, or Ax = 0
has nontrivial solutions.

4.1 INHOMOGENEOUS LINEAR BOUNDARY VALUE PROBLEMS
97
In terms of the physics of the rotating string problem, if ω is not an eigenfre-
quency, for arbitrary forcing of the string there is a unique steady solution. If ω
is an eigenfrequency and the forcing satisﬁes (4.9), there is a solution, (4.8), that
is a linear combination of the eigensolution and the response to the forcing. The
size of B depends upon how the steady state was reached, just as for the unforced
problem. If ω is an eigenfrequency and the forcing does not satisfy (4.9) there is no
steady solution. This reﬂects the fact that the forcing has a component that drives
a response at the eigenfrequency. We say that there is a resonance. In practice, if
the string were forced in this way, the amplitude of the motion would grow linearly,
whilst varying spatially like sin λ1/2x, until the nonlinear terms in (4.1) were no
longer negligible.
4.1.1
Solubility
As we have now seen, an inhomogeneous, linear boundary value problem may
have no solutions. Let’s examine this further for the general boundary value prob-
lem
(p(x)y′(x))′ + q(x)y(x) = f(x) subject to y(a) = y(b) = 0.
(4.10)
If u(x) is a solution of the homogeneous problem, so that (p(x)u′(x))′+q(x)u(x) = 0
and u(a) = u(b) = 0, then multiplying (4.10) by u(x) and integrating over the
interval [a, b] gives
 b
a
u(x)

(p(x)y′(x))′ + q(x)y(x)

dx =
 b
a
u(x)f(x) dx.
(4.11)
Now, using integration by parts,
 b
a
u(x) (p(x)y′(x))′ dx = [u(x)p(x)y′(x)]b
a −
 b
a
p(x)y′(x)u′(x) dx
= −
 b
a
p(x)y′(x)u′(x) dx,
using u(a) = u(b) = 0. Integrating by parts again gives
−
 b
a
p(x)y′(x)u′(x) dx
= −[p(x)u′(x)y(x)]b
a +
 b
a
(p(x)u′(x))′ y(x) dx =
 b
a
(p(x)u′(x))′ y(x) dx,
using y(a) = y(b) = 0. Substituting this into (4.11) gives
 b
a
y(x)

(p(x)u′(x))′ + q(x)u(x)

dx =
 b
a
u(x)f(x) dx,
and, since u(x) is a solution of the homogeneous problem,
 b
a
u(x)f(x) dx = 0.
(4.12)

98
BOUNDARY VALUE PROBLEMS
A necessary condition for there to be a solution of the inhomogeneous boundary
value problem (4.10) is therefore (4.12), which we call a solvability or solubility
condition. We say that the forcing term, f(x), must be orthogonal to the solution
of the homogeneous problem, a terminology that we will explore in more detail in
Section 4.2.3. Note that variations in the form of the boundary conditions will give
some variation in the form of the solvability condition.
4.1.2
The Green’s Function
Let’s now solve (4.10) using the variation of parameters formula, (1.6). This
gives
y(x) = Au1(x) + Bu2(x) +
 x
s=a
f(s)
W(s) {u1(s)u2(x) −u1(x)u2(s)} ds,
(4.13)
where u1(x) and u2(x) are solutions of the homogeneous problem and W(x) =
u1(x)u′
2(x)−u′
1(x)u2(x) is the Wronskian of the homogeneous equation. The bound-
ary conditions show that
Au1(a) + Bu2(a) = 0,
Au1(b) + Bu2(b) =
 b
a
f(s)
W(s) {u1(b)u2(s) −u1(s)u2(b)} ds.
Provided that u1(a)u2(b) ̸= u1(b)u2(a), we can solve these simultaneous equations
and substitute back into (4.13) to obtain
y(x) =
 b
a
f(s)
W(s)
{u1(b)u2(s) −u1(s)u2(b)} {u1(a)u2(x) −u1(x)u2(a)}
u1(a)u2(b) −u1(b)u2(a)
ds
+
 x
a
f(s)
W(s) {u1(s)u2(x) −u1(x)u2(s)} ds.
(4.14)
This form of solution, although correct, is not the most convenient one to use.
To improve it, we note that the functions v1(x) = u1(a)u2(x) −u1(x)u2(a) and
v2(x) = u1(b)u2(x) −u1(x)u2(b), which appear in (4.14) as a product, are linear
combinations of solutions of the homogeneous problem, and are therefore them-
selves solutions of the homogeneous problem. They also satisfy v1(a) = v2(b) = 0.
Because of the way that they appear in (4.14), it makes sense to look for a solution
of the inhomogeneous boundary value problem in the form
y(x) =
 b
a
f(s)G(x, s) ds,
(4.15)
where
G(x, s) =
 v1(s)v2(x) = G<(x, s)
for a ⩽s < x,
v1(x)v2(s) = G>(x, s)
for x < s ⩽b.
The function G(x, s) is known as the Green’s function for the boundary value
problem. From the deﬁnition, it is clear that G is continuous at s = x, and that

4.1 INHOMOGENEOUS LINEAR BOUNDARY VALUE PROBLEMS
99
y(a) = y(b) = 0, since, for example, G>(a, s) = 0 and y(a) =
' b
a G>(a, s)f(s) ds =
0. If we calculate the partial derivative with respect to x, we ﬁnd that
Gx(x, s = x−) −Gx(x, s = x+) = v1(x)v′
2(x) −v′
1(x)v2(x) = W =
C
p(x),
where, using Abel’s formula, C is a constant.
For this to be useful, we must now show that y(x), as deﬁned by (4.15), actually
satisﬁes the inhomogeneous diﬀerential equation, and also determine the value of
C. To do this, we split the range of integration and write
y(x) =
 x
a
G<(x, s)f(s) ds +
 b
x
G>(x, s)f(s) ds.
If we now diﬀerentiate under the integral sign, we obtain
y′(x) =
 x
a
G<,x(x, s)f(s) ds + G<(x, x)f(x) +
 b
x
G>,x(x, s)f(s) ds −G>(x, x)f(x),
where G<,x = ∂G</∂x. This simpliﬁes, by virtue of the continuity of the Green’s
function at x = s, to give
y′(x) =
 x
a
G<x(x, s)f(s) ds +
 b
x
G>x(x, s)f(s) ds
=
 x
a
v1(s)v2x(x)f(s) ds +
 b
x
v1x(x)v2(s)f(s) ds,
and hence
(py′)′ =
 x
a
v1(s)(pv2x)xf(s) ds + p(x)v1(x)v2x(x)f(x)
+
 b
x
(pv1x)xv2(s)f(s) ds −p(x)v1x(x)v2(x)f(x)
=
 x
a
v1(s)(pv2x)xf(s) ds +
 b
x
(pv1x)xv2(s)f(s) ds + Cf(x),
using the deﬁnition of C. If we substitute this into the diﬀerential equation (4.10),
(py′)′ + qy = f, we obtain
 x
a
v1(s)(pv2x)xf(s) ds +
 b
x
(pv1x)xv2(s)f(s) ds + Cf(x)
+q(x)
 x
a
v1(s)v2(x)f(s) ds +
 b
x
v1(x)v2(s)f(s) ds

= f(x).
Since v1 and v2 are solutions of the homogeneous problem, the integral terms vanish
and, if we choose C = 1, our representation provides us with a solution of the
diﬀerential equation.

100
BOUNDARY VALUE PROBLEMS
As an example, consider the boundary value problem
y′′(x) −y(x) = f(x) subject to y(0) = y(1) = 0.
The solutions of the homogeneous problem are e−x and ex. Appropriate combi-
nations of these that satisfy v1(0) = v2(1) = 0 are v1(x) = A sinh x and v2(x) =
B sinh(1 −x), which gives the Green’s function
G(x, s) =
 AB sinh(1 −x) sinh s
for 0 ⩽s < x,
AB sinh(1 −s) sinh x
for x < s ⩽1,
which is continuous at s = x. In addition,
Gx(x, s = x−) −Gx(x, s = x+)
= −AB cosh(1 −x) sinh x −AB sinh(1 −x) cosh x = −AB sinh 1.
Since p(x) = 1, we require AB = −1/ sinh 1, and the ﬁnal Green’s function is
G(x, s) =









−sinh(1 −x) sinh s
sinh 1
for 0 ⩽s < x,
−sinh(1 −s) sinh x
sinh 1
for x < s ⩽1.
The solution of the inhomogeneous boundary value problem can therefore be written
as
y(x) = −
 x
0
f(s)sinh(1 −x) sinh s
sinh 1
ds −
 1
x
f(s)sinh(1 −s) sinh x
sinh 1
ds.
We will return to the subject of Green’s functions in the next chapter.
4.2
The Solution of Boundary Value Problems by Eigenfunction
Expansions
In the previous section, we developed the idea of a Green’s function, with which we
can solve inhomogeneous boundary value problems for linear, second order ordinary
diﬀerential equations. We will now develop an alternative approach that draws
heavily upon the ideas of linear algebra (see Appendix 1 for a reminder). Before
we start, it is useful to be able to work with the simplest possible type of linear
diﬀerential operator.
4.2.1
Self-Adjoint Operators
We deﬁne a linear diﬀerential operator, L : C2[a, b] →C[a, b], as being in self-
adjoint form if
L = d
dx

p(x) d
dx

+ q(x),
(4.16)

4.2 EIGENFUNCTION EXPANSIONS
101
where p(x) ∈C1[a, b] and is strictly nonzero for all x ∈(a, b), and q(x) ∈C[a, b].
The reasons for referring to such an operator as self-adjoint will become clear later
in this chapter.
This deﬁnition encompasses a wide class of second order diﬀerential operators.
For example, if
L1 ≡a2(x) d 2
dx2 + a1(x) d
dx + a0(x)
(4.17)
is nonsingular on [a, b], we can write it in self-adjoint form by deﬁning (see Exer-
cise 4.5)
p(x) = exp
 x a1(t)
a2(t) dt

,
q(x) = a0(x)
a2(x) exp
 x a1(t)
a2(t) dt

.
(4.18)
Note that p(x) ̸= 0 for x ∈[a, b]. By studying inhomogeneous boundary value
problems of the form Ly = f, or
d
dx

p(x)dy
dx

+ q(x)y = f(x),
(4.19)
we are therefore considering all second order, nonsingular, linear diﬀerential oper-
ators. For example, consider Hermite’s equation,
d 2y
dx2 −2xdy
dx + λy = 0,
(4.20)
for −∞< x < ∞. This is not in self-adjoint form, but, if we follow the above
procedure, the self-adjoint form of the equation is
d
dx

e−x2 dy
dx

+ λe−x2y = 0.
This can be simpliﬁed, and kept in self-adjoint form, by writing u = e−x2/2y, to
obtain
d 2u
dx2 −(x2 −1)u = −λu.
(4.21)
4.2.2
Boundary Conditions
To complete the deﬁnition of a boundary value problem associated with (4.19),
we need to know the boundary conditions. In general these will be of the form
α1y(a) + α2y(b) + α3y′(a) + α4y′(b) = 0,
β1y(a) + β2y(b) + β3y′(a) + β4y′(b) = 0.
(4.22)
Since each of these is dependent on the values of y and y′ at each end of [a, b],
we refer to these as mixed or coupled boundary conditions. It is unnecessarily
complicated to work with the boundary conditions in this form, and we can start
to simplify matters by deriving Lagrange’s identity.

102
BOUNDARY VALUE PROBLEMS
Lemma 4.1 (Lagrange’s identity) If L is the linear diﬀerential operator given
by (4.16) on [a, b], and if y1, y2 ∈C2[a, b], then
y1 (Ly2) −y2 (Ly1) = [ p (y1y′
2 −y′
1y2)]′ .
(4.23)
Proof From the deﬁnition of L,
y1 (Ly2) −y2 (Ly1) = y1
(
(py′
2)′ + qy2
)
−y2
(
(py′
1)′ + qy1
)
= y1 (py′
2)′ −y2 (py′
1)′ = y1 (py′′
2 + p′y′
2) −y2 (py′′
1 + p′y′
1)
= p′ (y1y′
2 −y′
1y2) + p (y1y′′
2 −y′′
1y2) = [ p (y1y′
2 −y′
1y2)]′ .
Now recall that the space C[a, b] is a real inner product space with a standard
inner product deﬁned by
⟨f, g⟩=
 b
a
f(x)g(x) dx.
If we now integrate (4.23) over [a, b] then
⟨y1, Ly2⟩−⟨Ly1, y2⟩= [ p (y1y′
2 −y′
1y2)]b
a .
(4.24)
This result can be used to motivate the following deﬁnitions. The adjoint operator
to T, written ¯T, satisﬁes ⟨y1, Ty2⟩= ⟨¯Ty1, y2⟩for all y1 and y2. For example, let’s
see if we can construct the adjoint to the operator
D ≡d 2
dx2 + γ d
dx + δ,
with γ, δ ∈R, on the interval [0, 1], when the functions on which D operates are
zero at x = 0 and x = 1. After integrating by parts and applying these boundary
conditions, we ﬁnd that
⟨φ1, Dφ2⟩=
 1
0
φ1 (φ′′
2 + γφ′
2 + δφ2) dx =
(
φ1φ′
2
)1
0 −
 1
0
φ′
1φ′
2 dx
+
(
γφ1φ2
)1
0 −
 1
0
γφ′
1φ2 dx +
 1
0
δφ1φ2 dx
= −
(
φ′
1φ2
)1
0 +
 1
0
φ′′
1φ2 dx −
 1
0
γφ′
1φ2 dx +
 1
0
δφ1φ2 dx = ⟨¯Dφ1, φ2⟩,
where
¯D ≡d 2
dx2 −γ d
dx + δ.

4.2 EIGENFUNCTION EXPANSIONS
103
A linear operator is said to be Hermitian, or self-adjoint, if ⟨y1, Ty2⟩=
⟨Ty1, y2⟩for all y1 and y2. It is clear from (4.24) that L is a Hermitian, or self-
adjoint, operator if and only if
(
p (y1y′
2 −y′
1y2)
)b
a = 0,
and hence
p(b) {y1(b)y′
2(b) −y′
1(b)y2(b)} −p(a) {y1(a)y′
2(a) −y′
1(a)y2(a)} = 0.
(4.25)
In other words, whether or not L is Hermitian depends only upon the boundary
values of the functions in the space upon which it operates.
There are three diﬀerent ways in which (4.25) can occur.
(i) p(a) = p(b) = 0. Note that this doesn’t violate our deﬁnition of p as strictly
nonzero on the open interval (a, b). This is the case of singular boundary
conditions.
(ii) p(a) = p(b) ̸= 0, yi(a) = yi(b) and y′
i(a) = y′
i(b). This is the case of periodic
boundary conditions.
(iii) α1yi(a) + α2y′
i(a) = 0 and β1yi(b) + β2y′
i(b) = 0, with at least one of the αi
and one of the βi nonzero. These conditions then have nontrivial solutions
if and only if
y1(a)y′
2(a) −y′
1(a)y2(a) = 0,
y1(b)y′
2(b) −y′
1(b)y2(b) = 0,
and hence (4.25) is satisﬁed.
Conditions (iii), each of which involves y and y′ at a single endpoint, are called
unmixed or separated.
We have therefore shown that our linear diﬀerential
operator is Hermitian with respect to a pair of unmixed boundary conditions. The
signiﬁcance of this result becomes apparent when we examine the eigenvalues and
eigenfunctions of Hermitian linear operators.
As an example of how such boundary conditions arise when we model physical
systems, consider a string that is rotating (as in the example at the start of this
chapter) or vibrating with its ends ﬁxed. This leads to boundary conditions y(0) =
y(a) = 0 – separated boundary conditions. In the study of the motion of electrons
in a crystal lattice, the periodic conditions p(0) = p(l), y(0) = y(l) are frequently
used to represent the repeating structure of the lattice.
4.2.3
Eigenvalues and Eigenfunctions of Hermitian Linear
Operators
The eigenvalues and eigenfunctions of a Hermitian, linear operator L are the
nontrivial solutions of Ly = λy subject to appropriate boundary conditions.
Theorem 4.1 Eigenfunctions belonging to distinct eigenvalues of a Hermitian lin-
ear operator are orthogonal.

104
BOUNDARY VALUE PROBLEMS
Proof Let y1 and y2 be eigenfunctions that correspond to the distinct eigenvalues
λ1 and λ2. Then
⟨Ly1, y2⟩= ⟨λ1y1, y2⟩= λ1⟨y1, y2⟩,
and
⟨y1, Ly2⟩= ⟨y1, λ2y2⟩= λ2⟨y1, y2⟩,
so that the Hermitian property ⟨Ly1, y2⟩= ⟨y1, Ly2⟩gives
(λ1 −λ2) ⟨y1, y2⟩= 0.
Since λ1 ̸= λ2, ⟨y1, y2⟩= 0, and y1 and y2 are orthogonal.
As we shall see in the next section, all of the eigenvalues of a Hermitian linear
operator are real, a result that we will prove once we have deﬁned the notion of a
complex inner product.
If the space of functions C2[a, b] were of ﬁnite dimension, we would now argue
that the orthogonal eigenfunctions generated by a Hermitian operator are linearly
independent and can be used as a basis (or in the case of repeated eigenvalues,
extended into a basis). Unfortunately, C2[a, b] is not ﬁnite dimensional, and we
cannot use this argument. We will have to content ourselves with presenting a
credible method for solving inhomogeneous boundary value problems based upon
the ideas we have developed, and simply state a theorem that guarantees that the
method will work in certain circumstances.
4.2.4
Eigenfunction Expansions
In order to solve the inhomogeneous boundary value problem given by (4.19) with
f ∈C[a, b] and unmixed boundary conditions, we begin by ﬁnding the eigenvalues
and eigenfunctions of L. We denote these eigenvalues by λ1, λ2, . . . , λn, . . . , and
the eigenfunctions by φ1(x), φ2(x), . . . , φn(x), . . . . Next, we expand f(x) in terms
of these eigenfunctions, as
f(x) =
∞

n=1
cnφn(x).
(4.26)
By making use of the orthogonality of the eigenfunctions, after taking the inner
product of (4.26) with φn, we ﬁnd that the expansion coeﬃcients are
cn = ⟨f, φn⟩
⟨φn, φn⟩.
(4.27)
Next, we expand the solution of the boundary value problem in terms of the eigen-
functions, as
y(x) =
∞

n=1
dnφn(x),
(4.28)

4.2 EIGENFUNCTION EXPANSIONS
105
and substitute (4.27) and (4.28) into (4.19) to obtain
L
 ∞

n=1
dnφn(x)

=
∞

n=1
cnφn(x).
From the linearity of L and the deﬁnition of φn this becomes
∞

n=1
dnλnφn(x) =
∞

n=1
cnφn(x).
We have therefore constructed a solution of the boundary value problem with dn =
cn/λn, if the series (4.28) converges and deﬁnes a function in C2[a, b]. This process
will work correctly and give a unique solution provided that none of the eigenvalues
λn is zero. When λm = 0, there is no solution if cm ̸= 0 and an inﬁnite number of
solutions if cm = 0, as we saw in Section 4.1.
Example
Consider the boundary value problem
−y′′ = f(x) subject to y(0) = y(π) = 0.
(4.29)
In this case, the eigenfunctions are solutions of
y′′ + λy = 0 subject to y(0) = y(π) = 0,
which we already know to be λn = n2, φn(x) = sin nx. We therefore write
f(x) =
∞

n=1
cn sin nx,
and the solution of the inhomogeneous problem (4.29) is
y(x) =
∞

n=1
cn
n2 sin nx.
In the case f(x) = x,
cn =
' π
0 x sin nx dx
' π
0 sin2 nx dx = 2(−1)n+1
n
,
so that
y(x) = 2
∞

n=1
(−1)n+1
n3
sin nx.
We will discuss the convergence of this type of series, known as a Fourier series, in
detail in Chapter 5.
This example is, of course, rather artiﬁcial, and we could have integrated (4.29)
directly. There are, however, many boundary value problems for which this eigen-
function expansion method is the only way to proceed analytically, such as the
example given in Section 3.9.1 on Bessel functions.

106
BOUNDARY VALUE PROBLEMS
Example
Consider the inhomogeneous equation
(1 −x2)y′′ −2xy + 2y = f(x) on −1 < x < 1,
(4.30)
with f ∈C[−1, 1], subject to the condition that y should be bounded on [−1, 1].
We begin by noting that there is a solubility condition associated with this problem.
If u(x) is a solution of the homogeneous problem, then, after multiplying through
by u and integrating over [−1, 1], we ﬁnd that
!
u(1 −x2)y′"1
−1 −
!
u′(1 −x2)y
"1
−1 =
 1
−1
u(x)f(x) dx.
If u and y are bounded on [−1, 1], the left hand side of this equation vanishes, so
that
' 1
−1 u(x)f(x) dx = 0. Since the Legendre polynomial, u = P1(x) = x, is the
bounded solution of the homogeneous problem, we have
 1
−1
P1(x)f(x) dx = 0.
Now, to solve the boundary value problem, we ﬁrst construct the eigenfunction
solutions by solving Ly = λy, which is
(1 −x2)y′′ −2xy′ + (2 −λ)y = 0.
The choice 2−λ = n(n+1), with n a positive integer, gives us Legendre’s equation
of integer order, which has bounded solutions yn(x) = Pn(x).
These Legendre
polynomials are orthogonal over [−1, 1] (as we shall show in Theorem 4.4), and
form a basis for C[−1, 1]. If we now write
f(x) =
∞

m=0
AmPm(x),
where A1 = 0 by the solubility condition, and then expand y(x) =  ∞
m=0 BmPm(x),
we ﬁnd that
{2 −m(m + 1)} Bm = Am for m ⩾0.
The required solution is therefore
y(x) = 1
2A0 + B1P1(x) +
∞

m=2
Am
2 −m(m + 1)Pm(x),
with B1 an arbitrary constant.
Having seen that this method works, we can now state a theorem that gives the
method a rigorous foundation.
Theorem 4.2 If L is a nonsingular, linear diﬀerential operator deﬁned on a closed
interval [a, b] and subject to unmixed boundary conditions at both endpoints, then

4.3 STURM–LIOUVILLE SYSTEMS
107
(i) L has an inﬁnite sequence of real eigenvalues λ0, λ1, . . . , which can be or-
dered so that
|λ0| < |λ1| < · · · < |λn| < · · ·
and
lim
n→∞|λn| = ∞.
(ii) The eigenfunctions that correspond to these eigenvalues form a basis for
C[a, b], and the series expansion relative to this basis of a piecewise con-
tinuous function y with piecewise continuous derivative on [a, b] converges
uniformly to y on any subinterval of [a, b] in which y is continuous.
We will not prove this result here.† Instead, we return to the equation, Ly = λy,
which deﬁnes the eigenfunctions and eigenvalues. For a self-adjoint, second order,
linear diﬀerential operator, this is
d
dx

p(x)dy
dx

+ q(x)y = λy,
(4.31)
which, in its simplest form, is subject to the unmixed boundary conditions
α1y(a) + α2y′(a) = 0,
β1y(b) + β2y′(b) = 0,
(4.32)
with α2
1 + α2
2 > 0 and β2
1 + β2
2 > 0 to avoid a trivial condition. This is an example
of a Sturm–Liouville system, and we will devote the rest of this chapter to a
study of the properties of the solutions of such systems.
4.3
Sturm–Liouville Systems
In the ﬁrst three chapters, we have studied linear second order diﬀerential equations.
After examining some solution techniques that are applicable to such equations in
general, we studied the particular cases of Legendre’s equation and Bessel’s equa-
tion, since they frequently arise in models of physical systems in spherical and
cylindrical geometries. We saw that, in each case, we can construct a set of orthog-
onal solutions that can be used as the basis for a series expansion of the solution of
the physical problem in question, namely the Fourier–Legendre and Fourier–Bessel
series. In this chapter we will see that Legendre’s and Bessel’s equations are exam-
ples of Sturm–Liouville equations, and that we can deduce many properties of
such equations independent of the functional form of the coeﬃcients.
4.3.1
The Sturm–Liouville Equation
Sturm–Liouville equations are of the form
(p(x)y′(x))′ + q(x)y(x) = −λr(x)y(x),
(4.33)
† For a proof see Ince (1956).

108
BOUNDARY VALUE PROBLEMS
which can be written more concisely as
Sy(x, λ) = −λr(x)y(x, λ),
(4.34)
where the diﬀerential operator S is deﬁned as
Sφ ≡d
dx

p(x)dφ
dx

+ q(x)φ.
(4.35)
This is a slightly more general equation than (4.31). In (4.33), the number λ is
the eigenvalue, whose possible values, which may be complex, are critically depen-
dent upon the given boundary conditions. It is often more important to know the
properties of λ than it is to construct the actual solutions of (4.33).
We seek to solve the Sturm–Liouville equation, (4.33), on an open interval, (a, b),
of the real line. We will also make some assumptions about the behaviour of the
coeﬃcients of (4.33) for x ∈(a, b), namely that
(i)
p(x), q(x) and r(x) are real-valued and continuous,
(ii)
p(x) is diﬀerentiable,
(iii)
p(x) > 0 and r(x) > 0.
(4.36)
Some Examples of Sturm–Liouville Equations
Perhaps the simplest example of a Sturm–Liouville equation is Fourier’s equa-
tion,
y′′(x, λ) = −λy(x, λ),
(4.37)
which has solutions cos(x
√
λ) and sin(x
√
λ). We discussed a physical problem that
leads naturally to Fourier’s equation at the start of this chapter, and we will meet
another at the beginning of Chapter 5.
We can write Legendre’s equation and Bessel’s equation as Sturm–Liouville prob-
lems. Recall that Legendre’s equation is
d 2y
dx2 −
2x
1 −x2
dy
dx +
λ
1 −x2 y = 0,
and we are usually interested in solving this for −1 < x < 1. This can be written
as
((1 −x2)y′)′ = −λy.
If λ = n(n + 1), we showed in Chapter 2 that this has solutions Pn(x) and Qn(x).
Similarly, Bessel’s equation, which is usually solved for 0 < x < a, is
x2y′′ + xy′ + (λx2 −ν2)φ = 0.
This can be rearranged into the form
(xy′)′ −ν2
x y = −λxy.
Again, from the results of Chapter 3, we know that this has solutions of the form
Jν(x
√
λ) and Yν(x
√
λ).

4.3 STURM–LIOUVILLE SYSTEMS
109
Although the Sturm–Liouville forms of these equations may look more cumber-
some than the original forms, we will see that they are very convenient for the
analysis that follows. This is because of the self-adjoint nature of the diﬀerential
operator.
4.3.2
Boundary Conditions
We begin with a couple of deﬁnitions. The endpoint, x = a, of the interval (a, b)
is a regular endpoint if a is ﬁnite and the conditions (4.36) hold on the closed
interval [a, c] for each c ∈(a, b). The endpoint x = a is a singular endpoint if
a = −∞or if a is ﬁnite but the conditions (4.36) do not hold on the closed interval
[a, c] for some c ∈(a, b). Similar deﬁnitions hold for the other endpoint, x = b. For
example, Fourier’s equation has regular endpoints if a and b are ﬁnite. Legendre’s
equation has regular endpoints if −1 < a < b < 1, but singular endpoints if a = −1
or b = 1, since p(x) = 1 −x2 = 0 when x = ±1. Bessel’s equation has regular
endpoints for 0 < a < b < ∞, but singular endpoints if a = 0 or b = ∞, since
q(x) = −ν2/x is unbounded at x = 0.
We can now deﬁne the types of boundary condition that can be applied to a
Sturm–Liouville equation.
(i) On a ﬁnite interval, [a, b], with regular endpoints, we prescribe unmixed, or
separated, boundary conditions, of the form
α0y(a, λ) + α1y′(a, λ) = 0,
β0y(b, λ) + β1y′(b, λ) = 0.
(4.38)
These boundary conditions are said to be real if the constants α0, α1, β0
and β1 are real, with α2
0 + α2
1 > 0 and β2
0 + β2
1 > 0.
(ii) On an interval with one or two singular endpoints, the boundary conditions
that arise in models of physical problems are usually boundedness condi-
tions. In many problems, these are equivalent to Friedrichs boundary
conditions, that for some c ∈(a, b) there exists A ∈R+ such that
|y(x, λ)| ⩽A for all x ∈(a, c],
and similarly if the other endpoint, x = b, is singular, there exists B ∈R+
such that
|y(x, λ)| ⩽B for all x ∈[c, b).
We can now deﬁne the Sturm–Liouville boundary value problem to be the
Sturm–Liouville equation,
(p(x)y′(x))′ + q(x)y(x) = −λr(x)y(x) for x ∈(a, b),
where the coeﬃcient functions satisfy the conditions (4.36), to be solved subject to a
separated boundary condition at each regular endpoint and a Friedrichs boundary
condition at each singular endpoint.
Note that this boundary value problem is
homogeneous and therefore always has the trivial solution, y = 0. A nontrivial
solution, y(x, λ) ̸≡0, is an eigenfunction, and λ is the corresponding eigenvalue.

110
BOUNDARY VALUE PROBLEMS
Some Examples of Sturm–Liouville Boundary Value Problems
Consider Fourier’s equation,
y′′(x, λ) = −λy(x, λ) for x ∈(0, 1),
subject to the boundary conditions y(0, λ) = y(1, λ) = 0, which are appropriate
since both endpoints are regular. The eigenfunctions of this system are sin √λnx
for n = 1, 2, . . . , with corresponding eigenvalues λ = λn = n2π2.
Legendre’s equation is

(1 −x2)y′(x, λ)
′ = −λy(x, λ) for x ∈(−1, 1).
Note that this is singular at both endpoints, since p(±1) = 0. We therefore apply
Friedrichs boundary conditions, for example with c = 0, in the form
|y(x, λ)| ⩽A for x ∈(−1, 0], |y(x, λ)| ⩽B for x ∈[0, 1),
for some A, B ∈R+. In Chapter 2 we used the method of Frobenius to construct the
solutions of Legendre’s equation, and we know that the only eigenfunctions bounded
at both the endpoints are the Legendre polynomials, Pn(x) for n = 0, 1, 2, . . . , with
corresponding eigenvalues λ = λn = n(n + 1).
Let’s now consider Bessel’s equation with ν = 1, over the interval (0, 1),
(xy′)′ −y
x = −λxy.
Because of the form of q(x), x = 0 is a singular endpoint, whilst x = 1 is a regular
endpoint. Suitable boundary conditions are therefore
|y(x, λ)| ⩽A for x ∈

0, 1
2
"
, y(1, λ) = 0,
for some A ∈R+. In Chapter 3 we constructed the solutions of this equation using
the method of Frobenius. The solution that is bounded at x = 0 is J1(x
√
λ). The
eigenvalues are solutions of
J1(
#
λn) = 0,
which we write as λ = λ2
1, λ2
2, . . . , where J1(λn) = 0.
Finally, let’s examine Bessel’s equation with ν = 1, but now for x ∈(0, ∞).
Since both endpoints are now singular, appropriate boundary conditions are
|y(x, λ)| ⩽A for x ∈

0, 1
2
"
, |y(x, λ)| ⩽B for x ∈
! 1
2, ∞

,
for some A, B ∈R+. The eigenfunctions are again J1(x
√
λ), but now the eigenval-
ues lie on the half-line [0, ∞). In other words, the eigenfunctions exist for all real,
positive λ. The set of eigenvalues for a Sturm–Liouville system is often called the
spectrum. In the ﬁrst of the Bessel function examples above, we have a discrete
spectrum, whereas for the second there is a continuous spectrum.
We will
focus our attention on problems that have a discrete spectrum only.

4.3 STURM–LIOUVILLE SYSTEMS
111
4.3.3
Properties of the Eigenvalues and Eigenfunctions
In order to further study the properties of the eigenfunctions and eigenvalues,
we begin by deﬁning the inner product of two complex-valued functions over an
interval I to be
⟨φ1(x), φ2(x)⟩=

I
φ∗
1(x)φ2(x) dx,
where a superscript asterisk denotes the complex conjugate. This means that the
inner product has the properties
(i) ⟨φ1, φ2⟩= ⟨φ2, φ1⟩∗,
(ii) ⟨a1φ1, a2φ2⟩= a∗
1a2⟨φ1, φ2⟩,
(iii) ⟨φ1, φ2 + φ3⟩= ⟨φ1, φ2⟩+ ⟨φ1, φ3⟩, ⟨φ1 + φ2, φ3⟩= ⟨φ1, φ3⟩+ ⟨φ2, φ3⟩,
(iv) ⟨φ, φ⟩=
'
I |φ|2 dx ⩾0, with equality if and only if φ(x) ≡0 in I.
Note that this reduces to the deﬁnition of a real inner product if φ1 and φ2 are real.
If ⟨φ1, φ2⟩= 0 with φ1 ̸≡0 and φ2 ̸≡0, we say that φ1 and φ2 are orthogonal.
Let y1(x), y2(x) ∈C2[a, b] be twice-diﬀerentiable complex-valued functions. By
integrating by parts, it is straightforward to show that (see Lemma 4.1)
⟨y2, Sy1⟩−⟨Sy2, y1⟩=
(
p(x)

y1(x)(y∗
2(x))′ −y′
1(x)y∗
2(x)
)β
α,
(4.39)
which is known as Green’s formula. The inner products are deﬁned over a sub-
interval [α, β] ⊂(a, b), so that we can take the limits α →a+ and β →b−when
the endpoints are singular, and the Sturm–Liouville operator, S, is given by (4.35).
Now if x = a is a regular endpoint and the functions y1 and y2 satisfy a separated
boundary condition at a, then
p(a)

y1(a)(y∗
2(a))′ −y′
1(a)y∗
2(a)

= 0.
(4.40)
If a is a ﬁnite singular endpoint and the functions y1 and y2 satisfy the Friedrichs
boundary condition at a,
lim
x→a+
(
p(x)

y1(x)(y∗
2(x))′ −y′
1(x)y∗
2(x)
)
= 0.
(4.41)
Similar results hold at x = b.
We can now derive several results concerning the eigenvalues and eigenfunctions
of a Sturm–Liouville boundary value problem.
Theorem 4.3 The eigenvalues of a Sturm–Liouville boundary value problem are
real.
Proof If we substitute y1(x) = y(x, λ) and y2(x) = y∗(x, λ) into Green’s formula
over the entire interval, [a, b], we have
⟨y∗(x, λ), Sy(x, λ)⟩−⟨Sy∗(x, λ), y(x, λ)⟩
=
(
p(x)

y(x, λ)(y∗(x, λ))′ −y′(x, λ)y∗(x, λ)
)b
a = 0,

112
BOUNDARY VALUE PROBLEMS
making use of (4.40) and (4.41). Now, using the fact that the functions y(x, λ) and
y∗(x, λ) are solutions of (4.33) and its complex conjugate, we ﬁnd that
 b
a
r(x)y(x, λ)y∗(x, λ)(λ −λ∗) dx = (λ −λ∗)
 b
a
r(x)|y(x, λ)|2 dx = 0.
Since r(x) > 0 and y(x, λ) is nontrivial, we must have λ = λ∗, and hence λ ∈R.
Theorem 4.4 If y(x, λ) and y(x, ˜λ) are eigenfunctions of the Sturm–Liouville
boundary value problem, with λ ̸= ˜λ, then these eigenfunctions are orthogonal over
(a, b) with respect to the weighting function r(x), so that
 b
a
r(x)y(x, λ)y(x, ˜λ) dx = 0.
(4.42)
Proof Firstly, notice that the separated boundary condition, (4.38), at x = a takes
the form
α0y1(a) + α1y′
1(a) = 0,
α0y2(a) + α1y′
2(a) = 0.
(4.43)
Taking the complex conjugate of the second of these gives
α0y∗
2(a) + α1 (y′
2(a))∗= 0,
(4.44)
since α0 and α1 are real. For the pair of equations (4.43)2 and (4.44) to have a
nontrivial solution, we need
y1(a) (y′
2(a))∗−y′
1(a)y∗
2(a) = 0.
A similar result holds at the other endpoint, x = b. This clearly shows that
p(x)

y(x, λ)

y′(x, ˜λ)
∗
−y′(x, λ)

y(x, ˜λ)
∗
→0
as x →a and x →b, so that, from Green’s formula, (4.39),
⟨y(x, ˜λ), Sy(x, λ)⟩= ⟨Sy(x, ˜λ), y(x, λ)⟩.
If we evaluate this formula, we ﬁnd that
 b
a
r(x)y(x, λ)y(x, ˜λ) dx = 0,
so that the eigenfunctions associated with the distinct eigenvalues λ and ˜λ are
orthogonal with respect to the weighting function r(x).
Example
Consider Hermite’s equation, (4.20). By using the method of Frobenius, we can
show that there are polynomial solutions, Hn(x), when λ = 2n for n = 0, 1, 2, . . . .
For example, H0(x) = 1, H1(x) = 2x and H2(x) = 4x2 −2. The solutions of (4.21),
the self-adjoint form of the equation, that are bounded at inﬁnity for λ = 2n then

4.3 STURM–LIOUVILLE SYSTEMS
113
take the form un = e−x2/2Hn(x), and, from Theorem 4.4, satisfy the orthogonality
condition
 ∞
−∞
e−x2Hn(x)Hm(x) dx = 0 for n ̸= m.
4.3.4
Bessel’s Inequality, Approximation in the Mean and
Completeness
We can now deﬁne a sequence of orthonormal eigenfunctions
φn(x) =
#
r(x)y(x, λn)
⟨
#
r(x)y(x, λn),
#
r(x)y(x, λn)⟩
,
which satisfy
⟨φn(x), φm(x)⟩= δnm,
(4.45)
where δnm is the Kronecker delta. We will try to establish when we can write a
piecewise continuous function f(x) in the form
f(x) =
∞

i=0
aiφi(x).
(4.46)
Taking the inner product of both sides of this series with φj(x) shows that
aj = ⟨f(x), φj(x)⟩,
(4.47)
using the orthonormality condition, (4.45). The quantities ai are known as the ex-
pansion coeﬃcients, or generalized Fourier coeﬃcients. In order to motivate
the inﬁnite series expansion (4.46), we start by approximating f(x) by a ﬁnite sum,
fN(x) =
N

i=0
Aiφ(x, λi),
for some ﬁnite N, where the Ai are to be determined so that this provides the most
accurate approximation to f(x). The error in this approximation is
RN(x) = f(x) −
N

i=0
Aiφ(x, λi).
We now try to minimize this error by minimizing its norm
||RN||2 = ⟨RN(x), RN(x)⟩=
 b
a

f(x) −
N

i=0
Aiφi(x)
2
dx,
which is the mean square error in the approximation. Now
||RN||2 =
0
f(x) −
N

i=0
Aiφi(x), f(x) −
N

i=0
Aiφi(x)
1

114
BOUNDARY VALUE PROBLEMS
= ||f(x)||2 −
0
f(x),
N

i=0
Aiφi(x)
1
−
0 N

i=0
Aiφi(x), f(x)
1
+
0 N

i=0
Aiφi(x),
N

i=0
Aiφi(x)
1
.
We can now use the orthonormality of the eigenfunctions, (4.45), and the expression
(4.47), which determines the coeﬃcients ai, to obtain
||RN(x)||2 = ||f(x)||2 −
N

i=0
Ai ⟨f(x), φi(x)⟩
−
N

i=0
A∗
i ⟨φi(x), f(x)⟩+
N

i=0
A∗
i Ai ⟨φi(x), φi(x)⟩
= ||f(x)||2 +
N

i=0
{−Aiai −A∗
i a∗
i + A∗
i Ai}
= ||f(x)||2 +
N

i=0

|Ai −ai|2 −|ai|2
.
The error is therefore smallest when Ai = ai for i = 0, 1, . . . , N, so the most
accurate approximation is formed by simply truncating the series (4.46) after N
terms. In addition, since the norm of RN(x) is positive,
N

i=0
|ai|2 ⩽
 b
a
|f(x)|2 dx.
As the right hand side of this is independent of N, it follows that
∞

i=0
|ai|2 ⩽
 b
a
|f(x)|2 dx,
(4.48)
which is Bessel’s inequality.
This shows that the sum of the squares of the
expansion coeﬃcients converges. Approximations by the method of least squares
are often referred to as approximations in the mean, because of the way the error
is minimized.
If, for a given orthonormal system, φ1(x), φ2(x), . . . , any piecewise continuous
function can be approximated in the mean to any desired degree of accuracy by
choosing N large enough, then the orthonormal system is said to be complete. For
complete orthonormal systems, RN(x) →0 as N →∞, so that Bessel’s inequality
becomes an equality,
∞

i=0
|ai|2 =
 b
a
|f(x)|2 dx,
(4.49)
for every function f(x).

4.3 STURM–LIOUVILLE SYSTEMS
115
The completeness of orthonormal systems, as expressed by
lim
N→∞
 b
a

f(x) −
N

i=0
aiφi(x)
2
dx = 0,
does not necessarily imply that f(x) =  ∞
i=0 aiφi(x), in other words that f(x)
has an expansion in terms of the φi(x).
If, however, the series  ∞
i=0 aiφi(x) is
uniformly convergent, then the limit and the integral can be interchanged, the
expansion is valid, and we say that  ∞
i=0 aiφi(x) converges in the mean to f(x).
The completeness of the system φ1(x), φ2(x), . . . , should be seen as a necessary
condition for the validity of the expansion, but, for an arbitrary function f(x), the
question of convergence requires a more detailed investigation.
The Legendre polynomials P0(x), P1(x), . . . on the interval [−1, 1] and the Bessel
functions Jν(λ1x), Jν(λ2x), . . . on the interval [0, a] are both examples of complete
orthogonal systems (they can easily be made orthonormal), and the expansions
of Chapters 2 and 3 are special cases of the more general results of this chapter.
For example, the Bessel functions Jν(
√
λx) satisfy the Sturm–Liouville equation,
(4.33), with p(x) = x, q(x) = −ν2/x and r(x) = x. They satisfy the orthogonality
relation
 a
0
xJν(√µx)Jν(
√
λx) dx = 0,
if λ and µ are distinct eigenvalues. Using the regular endpoint condition Jν(
√
λa) =
0 and the singular endpoint condition at x = 0, the eigenvalues, that is the zeros
of Jν(x), can be written as
√
λa = λ1a, λ2a, . . . , so that
√
λ = λi for i = 1, 2, . . . ,
and we can write
f(x) =
∞

i=1
aiJν(λix),
with
ai =
2
a2 {J′ν(λia)}2
 a
0
xJν(λix)f(x) dx,
consistent with (3.32).
4.3.5
Further Properties of Sturm–Liouville Systems
We conclude this section by investigating some of the qualitative properties of
solutions of the Sturm–Liouville system (4.33).
In particular, we will establish
that the nth eigenfunction has n zeros in the open interval (a, b). We will take
a geometrical point of view in order to establish this result, although we could
have used an analytical framework.
To achieve this, we introduce the Pr¨ufer
substitution,
p(x)y′(x) = R(x) cos θ(x),
y(x) = R(x) sin θ(x).
(4.50)

116
BOUNDARY VALUE PROBLEMS
The new dependent variables, R and θ, are then deﬁned by
R2 = y2 + p2(y′)2,
θ = tan−1
 y
py′

(4.51)
and, by analogy with polar coordinates, we call R the amplitude and θ the phase
angle.
Nontrivial eigenfunction solutions of the Sturm–Liouville equation have
R > 0, since R = 0 at any point x = x0 would mean that y(x0) = y′(x0) = 0, and
hence give the trivial solution for all x.
If we now write cot θ(x) = py′/y and diﬀerentiate, we obtain
−cosec2θ dθ
dx = (py′)′
y
−p (y′)2
y2
= −λr −q −1
p cot2 θ,
and hence
dθ
dx = (q(x) + λr(x)) sin2 θ +
1
p(x) cos2 θ.
(4.52)
If we now diﬀerentiate (4.51)1, some simple manipulation gives
dR
dx = 1
2
 1
p(x) −q(x) −λr(x)
	
R sin 2θ.
(4.53)
The Pr¨ufer substitution has therefore changed our second order linear diﬀerential
equation into a system of two ﬁrst order nonlinear diﬀerential equations in R and
θ over the interval a < x < b, (4.52) and (4.53). The equation for θ, (4.52), is
however independent of R and is, as we shall see, relatively easy to analyze.
If we now consider the separated boundary conditions
α1y(a) + α2y′(a) = 0,
β1y(b) + β2y′(b) = 0,
we can deﬁne two phase angles, γ and δ, such that
tan γ =
y(a)
p(a)y′(a) = −
α2
p(a)α1
for 0 ⩽γ ⩽π,
tan δ =
y(b)
p(b)y′(b) = −
β2
p(b)β1
for 0 ⩽δ ⩽π,
and the eigenvalue problem that we have to solve is (4.52) subject to
θ(a) = γ,
θ(b) = δ + nπ for n = 0, 1, 2, . . . .
(4.54)
We need to add this multiple of nπ because of the periodicity of the tangent func-
tion.
We can infer the qualitative form of the solution of (4.52) by drawing its di-
rection ﬁeld. This is the set of small line segments of slope dθ/dx in the (θ, x)
plane, as sketched in Figure 4.3. Note that dθ/dx = 1/p(x) at θ = nπ, which is
independent of λ. In addition, dθ/dx = q(x) + λp(x) at θ =

n + 1
2

π, which, for
ﬁxed x, increases with increasing λ. From Figure 4.4, which shows some typical
solution curves for various values of λ, we can see that, for any initial condition
θ(a) = γ, θ(b) is an increasing function of λ. As λ increases from −∞, there is a

4.3 STURM–LIOUVILLE SYSTEMS
117
ﬁrst value, λ = λ0, for which θ(b) = δ. As λ increases further, there is a sequence
of values for which θ = δ + nπ for n = 1, 2, . . . . Each of these is associated with an
eigenfunction, yn(x) = Rn(x) sin θ(x, λn). This has a zero when sin θ = 0 and, in
the interval γ ⩽θ ⩽δ + nπ, there are precisely n zeros at θ = π, 2π, . . . , nπ.
Fig. 4.3. The direction ﬁeld (lines of slope dθ/dx) for (4.52) when p(x) = x, q(x) = −1/x
and r(x) = x, which corresponds to Bessel’s equation of order one.
Returning now to the example y′′ + λy = 0 subject to y(0) = y(π) = 0, we have
eigenvalues λn = n2 and eigenfunctions sin nx for n = 1, 2, . . . . Because of the way
we have labelled these, λ0 = 1 and y0(x) = sin x is the zeroth eigenfunction, which
has no zeros in 0 < x < π. We can also see that λ1 = 22 and the ﬁrst eigenfunction,
y1(x) = sin 2x, has one zero in 0 < x < π, at x = π
2 , and so on. We can formalize
this analysis as a theorem.
Theorem 4.5 A regular Sturm–Liouville system has an inﬁnite sequence of real
eigenvalues, λ0 < λ1 < · · · < λn < · · · , with λn →∞as n →∞. The correspond-
ing eigenfunctions, yn(x), have n zeros in the interval a < x < b.
We can prove another useful result if we add an additional constraint. If q(x) < 0,
then all the eigenvalues are positive. To see why this should be so, consider the
boundary value problem
d
dx

p(x)dy
dx

+ (λr(x) + q(x)) y = 0 subject to y(a) = y(b) = 0.

118
BOUNDARY VALUE PROBLEMS
Fig. 4.4. Typical solutions of (4.52) for various values of λ when p(x) = x, q(x) = −1/x
and r(x) = x, which corresponds to Bessel’s equation of order one.
If we multiply through by y and integrate over [a, b], we obtain
λ
 b
a
r(x)y2 dx =
 b
a
−q(x)y2 dx −
 b
a
y d
dx

p(x)dy
dx

dx
=
 b
a

−q(x)y2 + p(x) (y′)2
dx,
using integration by parts. This shows that
λ =
 b
a

−q(x)y2 + p(x) (y′)2
dx
2 b
a
r(x)y2 dx ,
which is positive when p and r are positive and q is negative.
4.3.6
Two Examples from Quantum Mechanics
One of the areas of mathematical physics where Sturm–Liouville equations arise
most often is quantum mechanics, the theory that governs the behaviour of mat-
ter on very small length scales. Three important postulates of quantum mechanics
are (see Schiﬀ, 1968):
(i) A system is completely speciﬁed by a state function, or wave function,
ψ(r, t), with ⟨ψ, ψ⟩= 1. For example, if the system consists of a particle

4.3 STURM–LIOUVILLE SYSTEMS
119
moving in an external potential, then |ψ(r, t)|2 d 3r is the probability of ﬁnd-
ing the particle in a small volume d 3r that surrounds the point r, and hence
we need
  
R3 |ψ(r)|2 d 3r = ⟨ψ, ψ⟩= 1.
(ii) For every system there exists a certain Hermitian operator, H, called the
Hamiltonian operator, such that
iℏ∂ψ
∂t = Hψ,
where 2πℏ≈6.62 × 10−34 J s−1 is Planck’s constant.
(iii) To each observable property of the system there corresponds a linear, Her-
mitian operator, A, and any measurement of the property gives one of the
eigenvalues of A. For example, the operators that correspond to momentum
and energy are −iℏ∇and iℏ∂/∂t.
For a single particle of mass m moving in a potential ﬁeld V (r, t), the classical (as
opposed to quantum mechanical) total energy is
E = p2/2m + V (r, t),
(4.55)
where p is the momentum of the particle.
This is just the sum of the kinetic
and potential energies. To obtain the quantum mechanical analogue of this, we
quantize the classical result (4.55) by substituting the appropriate operators for
momentum and energy, and arrive at
iℏ∂ψ
∂t = −ℏ2
2m∇2ψ + V (r, t)ψ.
(4.56)
This is Schr¨odinger’s equation, which governs the evolution of the wave function.
Let’s look for a separable solution of (4.56) when V is independent of time. We
write ψ = u(r)T(t), and ﬁnd that
iℏT ′
T = −ℏ2
2mu∇2u + V (r) = E,
where E is the separation constant. Since iℏT ′ = ET,
iℏ∂ψ
∂t = Eψ,
and hence E is the energy of the particle. The equation for u is then the time-
independent Schr¨odinger equation,
−ℏ2
2m∇2u + (V (r) −E) u = 0.
(4.57)
We seek solutions of this equation subject to the conditions that u should be ﬁnite,
and u and ∇u continuous throughout the domain of solution. After we impose
appropriate boundary conditions, we will ﬁnd that the energy of the system is not
arbitrary, as classical physics would suggest. Instead, the energy must be one of
the eigenvalues of the boundary value problem associated with (4.57), and hence
the eigenvalues are of great interest. The energy is said to be quantized.

120
BOUNDARY VALUE PROBLEMS
Example: A conﬁned particle
Let’s consider the one-dimensional problem of a particle of mass m conﬁned in a
region of zero potential by an inﬁnite potential at x = 0 and x = a. What energies
can the particle have?
Since the probability of ﬁnding the particle outside the region 0 < x < a is zero,
we must have ψ = 0 there. By continuity, we therefore have ψ = 0 at x = 0 and
x = a. We must therefore solve the eigenvalue problem
−ℏ2
2m
∂2u
∂x2 = Eu,
(4.58)
subject to u = 0 at x = 0 and x = a. However, this is precisely the problem, given
by (4.2), with which we began this chapter, with λ = 2mE/ℏ2. We conclude that
the allowed energies of the particle are E = En = ℏ2λn/2m = ℏ2n2π2/2ma2.
Example: The hydrogen atom
The hydrogen atom consists of an electron and a proton. Since the mass of the
proton is much larger than that of the electron, let’s assume that the proton is
at rest. The steady state wave function, ψ(r), then satisﬁes (4.57) which, after
rescaling r and E to eliminate the constants, we write as
∇2ψ −2V ψ = −2Eψ for |r| > 0.
(4.59)
The potential V (r) = V (r) = −q2/r is the Coulomb potential due to the electri-
cal attraction between the proton and the electron, where −q is the charge on the
electron and q that on the proton.
We can now look for separable solutions in spherical polar coordinates (r, θ, φ)
in the form ψ = R(r)Y (s)Θ(φ), where s = cos θ. Substituting this into (4.59) gives
us
r2 R′′
R + 2rR′
R + 2r2 (E −V (r)) = −

(1 −s2)Y ′′
Y
−
1
1 −s2
Θ′′
Θ = λ,
for some separation constant λ. If we take Θ′′/Θ = −m2 with m = 1, 2, . . . for
periodicity, we obtain Θ = Am cos mφ + Bm sin mφ. If λ = n(n + 1), with n ⩾m,
then the equation for Y is the associated Legendre equation, which we studied in
Chapter 2. The bounded solution of this is Y = CnP m
n (s), and we have a solution
in the form
ψ(r, θ, φ) = R(r)P m
n (s) (Dn,m cos mφ + En,m sin mφ) ,
where Dn,m and En,m are arbitrary constants and R satisﬁes the diﬀerential equa-
tion
r2R′′ + 2rR −n(n + 1)R + 2r2

E + q2
r

R = 0.
We can simplify matters by deﬁning S = rR, so that
S′′ −n(n + 1)
r
S + 2

E + q2
r

S = 0 for r > 0,
(4.60)

EXERCISES
121
to be solved subject to the condition that S/r →0 as r →∞. This is an eigenvalue
problem for E, the allowable energy levels of the electron in the hydrogen atom.
Some thought and experimentation leads one to try a solution in the form S =
rn+1e−µr, which will satisfy (4.60) provided that µ = q2/(n + 1) and E = −1
2µ2.
A reduction of order argument then shows that there are no other solutions with
E < 0 that satisfy the condition at inﬁnity. Each energy level, EN = −q4/2N 2 for
N = 1, 2, . . . , corresponds to a possible steady state for our model of the hydrogen
atom, and is in agreement with the experimentally observed values. However, this
is not the end of the matter, as it is possible to show that every positive value of
E corresponds to a bounded, nontrivial solution of (4.60). In other words, there is
both a continuous and a discrete part to the spectrum. These states with positive
energy can be shown to be unstable, as the electron has too much energy.
Exercises
4.1
Use the eigenfunction expansion method to ﬁnd a solution of the boundary
value problem y′′(x) = −h(x) for 0 < x < 2π, subject to the boundary
conditions y(0) = y(2π), y′(0) = y′(2π), with h ∈C[0, 2π].
4.2
Find the Green’s function for the boundary value problems
(a) y′′(x) = f(x) subject to y(−1) = y(1) = 0,
(b) y′′(x) + ω2y(x) = f(x) subject to y(0) = y(π/2) = 0.
4.3
Comment on the diﬃculties that you face when trying to construct the
Green’s function for the boundary value problem
y′′(x) + y(x) = f(x) subject to y(a) = y′(b) = 0.
4.4
Show that when y′′ + λy = 0 for 0 < x < π, the eigenvalues when (a)
y(0) = y′(π) = 0, (b) y′(0) = y(π) = 0, (c) y′(0) = y′(π) = 0, are

n + 1
2
2,

n + 1
2
2 and n2 respectively, where n = 0, 1, 2, . . . . What are
the corresponding eigenfunctions?
4.5
Show that the equation
d 2y
dx2 + A(x)dy
dx + {λB(x) −C(x)} y = 0
can be written in Sturm–Liouville form by deﬁning p(x) = exp
'
A(x) dx

.
What are q(x) and r(x) in terms of A, B and C?
4.6
Write the generalized Legendre equation,
(1 −x2)d 2y
dx2 −2xdy
dx +

n(n + 1) −
m2
1 −µ2
	
y = 0,
as a Sturm–Liouville equation.
4.7
Determine the eigenvalues, λ, of the fourth order equation y(4) + λy = 0
subject to y(0) = y′(0) = y(π) = y′(π) = 0 for 0 ⩽x ⩽π.

122
BOUNDARY VALUE PROBLEMS
4.8
Consider the singular Sturm–Liouville system

xe−xy′′ + λe−xy = 0 for x > 0,
with y bounded as x →0 and e−xy →0 as x →∞. Show that when
λ = 0, 1, 2, . . . there are polynomial eigenfunctions. These are known as
the Laguerre polynomials.
4.9
Show that the boundary value problem
y′′(x) + A(x)y′(x) + B(x)y(x) = C(x) for all x ∈(a, b),
subject to
α1y(a) + β1y(b) + ˜α1y′(a) + ˜β1y′(b) = γ1
α2y(a) + β2y(b) + ˜α2y′(a) + ˜β2y′(b) = γ2,
is self-adjoint provided that
β1 ˜β2 −˜β1β2 = (α1˜α2 −˜α1α2) exp
 b
a
A(x) dx

.
4.10
Show that
−(xy′(x))′ = λxy(x)
is self-adjoint on the interval (0, 1), with x = 0 a singular endpoint and
x = 1 a regular endpoint with the condition y(1) = 0.
4.11
Find the eigenvalues of the system consisting of Fourier’s equation and the
conditions y(0) −y′(0) = 0 and y(π) = 0. Show that these eigenfunctions
are orthogonal on the interval (0, π).
4.12
Prove that sin mx has at least one zero between each pair of consecutive
zeros of sin nx, when m > n.
4.13
∗Using the Sturm comparison theorem (see Exercise 1.14), show that every
solution of Airy’s equation, y′′(x) −xy(x) = 0, vanishes inﬁnitely often on
the positive x-axis, and at most once on the negative x-axis.

CHAPTER FIVE
Fourier Series and the Fourier Transform
In order to motivate our discussion of Fourier series, we shall consider the solution of
a diﬀusion problem. Let’s suppose that we have a long, thin, cylindrical metal bar
of length L whose curved sides and one end are insulated from its surroundings.
Suppose also that, initially, the temperature of the bar is T = T0, but that the
uninsulated end is suddenly cooled to a temperature T1 < T0.
How does the
temperature in the metal bar vary for t > 0? Physical intuition suggests that heat
will ﬂow out of the end of the bar, and that, as time progresses, the temperature
will approach T1 throughout the bar.
In order to quantify this, we note that the temperature in the bar satisﬁes the
one-dimensional diﬀusion equation
∂T
∂t = K ∂2T
∂x2
for 0 < x < L,
(5.1)
where x measures distance along the bar and K is its thermal diﬀusivity (see Sec-
tion 2.6.1). The initial and boundary conditions are
T(x, 0) = T0,
T(0, t) = T1,
∂T
∂x (L, t) = 0.
(5.2)
Before we solve this initial–boundary value problem, it is convenient to deﬁne di-
mensionless variables,
ˆT = T −T1
T0 −T1
,
ˆx = x
L,
ˆt = Kt
L2 .
In terms of these variables, (5.1) and (5.2) become
∂ˆT
∂ˆt = ∂2 ˆT
∂ˆx2
for 0 < ˆx < 1,
(5.3)
ˆT(ˆx, 0) = 1,
ˆT(0, ˆt ) = 0,
∂ˆT
∂ˆx (1, ˆt ) = 0.
(5.4)
As you can see, by choosing appropriate dimensionless variables, we have managed
to eliminate all of the physical constants from the problem.
The use of dimensionless variables has additional advantages, which makes it
essential to use them in studying most mathematical models of real physical prob-
lems. Consider the length of the metal bar in the diﬀusion problem that we are
studying. It is usual to measure lengths in terms of metres, and, at ﬁrst sight, it

124
FOURIER SERIES AND THE FOURIER TRANSFORM
seems reasonable to say that a metal bar with length 100 m is long, whilst a bar of
length 10−2 m = 1 cm is short. In eﬀect, we choose a bar of length 1 m as a refer-
ence relative to which we measure the length of our actual bar. Is this a reasonable
thing to do? In fact, the only length that is deﬁned in the problem is the length
of the bar itself. This is the most sensible length with which to make the problem
dimensionless, and leads to a bar that lies between ˆx = 0 and ˆx = 1. For any given
problem, it is essential to choose a length scale that is relevant to the physics of
the problem itself, not some basically arbitrary length, such as 1 m. For example,
problems in celestial mechanics are often best made dimensionless using the mean
distance from the Earth to the Sun, whilst problems in molecular dynamics may be
made dimensionless using the mean atomic separation in a hydrogen atom. These
length scales are enormously diﬀerent from 1 m. The same argument applies to
all of the dimensional, physical quantities that are used in a mathematical model.
By dividing each variable by a suitable constant with the same dimensions, we can
state that a variable is small or large in a meaningful way. For example, using the
dimensionless time, ˆt = Kt/L2, which we deﬁned above, the solution when ˆt ≪1,
the small time solution, really does represent the behaviour of the temperature in
the metal bar when diﬀusion has had little eﬀect on the initial state, over the length
scale, L, that characterizes the full length of the bar. The other advantage of using
dimensionless variables is that any physical constants that remain explicitly in the
dimensionless problem appear in dimensionless groups, which can themselves be
said to be large or small in a meaningful way. No dimensionless groups appear in
the simple diﬀusion problem given by (5.3) and (5.4), and we will defer any further
discussion of them until Chapter 11.
We can now continue with our study of the diﬀusion of heat in a metal bar by
seeking a separable solution, ˆT(ˆx, ˆt ) = X(ˆx)F(ˆt ). On substituting this into (5.3)
we obtain
X′′
X =
˙F
F = −k2,
the separation constant.
The equation ˙F = −k2F has solution F = ae−k2ˆt, whilst X′′ + k2X = 0, Fourier’s
equation, (4.37), which we studied in Chapter 4, has solution
X = A sin kˆx + B cos kˆx.
The condition ˆT(0, ˆt ) = 0 means that X(0) = 0, and hence that B = 0. Similarly,
the condition that there should be no ﬂux of heat through the bar at ˆx = 1 leads
to kA cos k = 0, and hence shows that
k = π
2 + nπ for n = 0, 1, 2, . . . .
There is therefore a countably-inﬁnite sequence of solutions,
ˆTn = An exp

−π2

n + 1
2
2
ˆt

sin

π

n + 1
2

ˆx
	
.

FOURIER SERIES AND THE FOURIER TRANSFORM
125
Since this is a linear problem, the general solution is
ˆT(ˆx, ˆt ) =
∞

n=0
An exp

−π2

n + 1
2
2
ˆt

sin

π

n + 1
2

ˆx
	
.
(5.5)
We are now left with the task of determining the constants An.
The only information we have not used is the initial condition, ˆT(ˆx, 0) = 1, which
shows that
∞

n=0
An sin

π

n + 1
2

ˆx
	
= 1.
(5.6)
We now multiply this expression through by sin

π

m + 1
2

ˆx

and integrate over
the length of the bar. After noting that
 1
0
sin

π

n + 1
2

ˆx
	
sin

π

m + 1
2

ˆx
	
dˆx
=
 1
0
1
2 [cos {π (m −n) ˆx} −cos {π (m + n + 1) ˆx}] dˆx = 0 for m ̸= n,
(5.7)
and
 1
0
sin2

π

n + 1
2

ˆx
	
dˆx =
 1
0
1
2 [1 −cos {π (2m + 1) ˆx}] dˆx = 1
2 for m = n,
(5.8)
we conclude that
An = 2
 1
0
sin

π

n + 1
2

ˆx
	
dˆx =
4
(2n + 1)π ,
and hence that
ˆT(ˆx, ˆt ) =
∞

n=0
4
(2n + 1)π exp

−π2

n + 1
2
2
ˆt

sin

π

n + 1
2

ˆx
	
.
(5.9)
This is a Fourier series solution, and is shown in Figure 5.1 at various times. We
produced Figure 5.1 by plotting the MATLAB function
'
&
$
%
function heat = heat(x,t)
acc = 10^-8; n=ceil(sqrt(-log(acc)/t)-0.5);
N = 0:n; a = 4*exp(-pi^2*(N+0.5).^2*t)/pi./(2*N+1);
for k = 0:n
X(:,k+1) = sin(pi*(k+0.5)*x(:));
end
heat = X*a’;
This adds enough terms of the series that exp{−π2(n + 1
2)2t} is less than some
small number (acc = 10−8) in the ﬁnal term. Note that the function ceil rounds
its argument upwards to the nearest integer (the ceiling function).
It is clear that ˆT →0, and hence T →T1, as ˆt →∞, as expected, and that the

126
FOURIER SERIES AND THE FOURIER TRANSFORM
Fig. 5.1. The solution of the initial–boundary value problem, (5.3) and (5.4), given by
(5.9), at various times.
heat ﬂows out of the cool end of the bar, ˆx = 0. It is precisely problems of this sort,
which were ﬁrst solved by Fourier himself in the 1820s, that led to the development
of the theory of Fourier series. Note also that (5.9) with ˆt = 0 and ˆx = 1/2 and 1
leads to the interesting results
1 −1
3 + 1
5 −1
7 + 1
9 −1
11 + · · · = π
4 ,
1 + 1
3 −1
5 −1
7 + 1
9 + 1
11 −· · · =
π
2
√
2.
That the basis functions are orthogonal, as given by (5.7) and (5.8), should
come as no surprise, since Fourier’s equation is a Sturm–Liouville equation, and
the Fourier series is just another example of a series expansion in terms of an or-
thogonal basis, similar to the Fourier–Legendre and Fourier–Bessel series. We have
developed the Fourier series solution of this problem without too much attention to
mathematical rigour. However, a number of questions arise. Under what conditions
does a series of the form
∞

n=0
An sin

π

n + 1
2

ˆx
	

5.1 GENERAL FOURIER SERIES
127
converge, and is this convergence uniform? Does every function have a convergent
Fourier series representation? These questions can also be asked of expansions in
terms of other sequences of orthogonal functions, such as the Fourier–Legendre and
Fourier–Bessel series. The technical details are, however, rather more straightfor-
ward for the Fourier series.
5.1
General Fourier Series
The most general form for a Fourier series representation of a function, f(t), with
period T is
f(t) = 1
2A0 +
∞

n=1

An cos
2πnt
T

+ Bn sin
2πnt
T
	
,
(5.10)
and, using the method described above, the Fourier coeﬃcients, An and Bn, are
given by
An = 2
T

1
2 T
−1
2 T
cos
2πnt
T

f(t) dt,
Bn = 2
T

1
2 T
−1
2 T
sin
2πnt
T

f(t) dt.
(5.11)
Note that if f is an odd function of t, An = 0, since cos(2πnt/T) is an even function
of t. This means that the resulting Fourier series is a sum of just the odd functions
sin(2πnt/T), a Fourier sine series. Similarly, if f is an even function of t, the
resulting expansion is a Fourier cosine series. Equations (5.10) and (5.11) can
also be written in a more compact, complex form as
f(t) =
∞

n=−∞
Cne2πint/T ,
(5.12)
with complex Fourier coeﬃcients
Cn = 1
2 (An −iBn) = 1
T

1
2 T
−1
2 T
e−2πint/T f(t) dt.
(5.13)
As we have seen, Fourier series can arise as solutions of diﬀerential equations,
but they are also useful for representing periodic functions in general. For example,
consider the function of period 2π, deﬁned by
f(t) = t for −π < t < π, f(t) = f(t ± 2nπ) for n = 1, 2, . . . ,
which is piecewise continuous. Using (5.10) and (5.11), we conclude that
f(t) =
∞

n=1
2
n(−1)n+1 sin nt = 2

sin t −1
2 sin 2t + 1
3 sin 3t −· · ·

.
(5.14)
The partial sums of this series,
fN(t) =
N

n=1
2
n(−1)n+1 sin nt,

128
FOURIER SERIES AND THE FOURIER TRANSFORM
are shown in Figure 5.2 for various N. An inspection of this ﬁgure suggests that
the series does indeed converge, but that the convergence is not uniform. At the
points of discontinuity, t = ±π, (remember that f(t) has period 2π, and therefore
f →π as t →π−, but f →−π as t →π+) the series (5.14) gives the value
zero, the mean of the two limits as t approaches π from above and below. In the
neighbourhood of t = ±π, the diﬀerence between the partial sums, fN, and f(t)
appears not to become smaller as N increases, but the size of the region where
this occurs decreases – a sure sign of nonuniform convergence. This nonuniform,
oscillatory behaviour close to discontinuities is known as Gibbs’ phenomenon,
which we also met brieﬂy in Chapter 3.
Fig. 5.2. The partial sums of the Fourier series for t, (5.14), for N = 5, 15, 25 and 35.
The function being approximated is shown as a dotted line.
Before we proceed, we need to construct the Dirichlet kernel and prove the
Riemann–Lebesgue lemma, both of which are crucial in what follows.
Lemma 5.1 (The Dirichlet kernel) Let f be a bounded function of period T

5.1 GENERAL FOURIER SERIES
129
with f ∈PC[a, b]†, and let
Sn(f, t) =
n

m=−n
Cme2πimt/T
(5.15)
be the nth partial sum of the Fourier series of f(t), with Cm given by (5.13). If
we deﬁne the Dirichlet kernel, Dn(t), to be a function of period T given for
−1
2T < t < 1
2T by
Dn(t) =



sin{(2n+1) πt
T }
sin( πt
T )
when t ̸= 0,
2n + 1
when t = 0,
(5.16)
which is illustrated in Figure 5.3 for T = 2π, then
Sn(f, t) = 1
T

1
2 T
−1
2 T
f(τ)Dn(t −τ) dτ.
(5.17)
Fig. 5.3. The Dirichlet kernel, Dn for n = 10, 20, 30 and 40 and T = 2π.
† See Appendix 2.

130
FOURIER SERIES AND THE FOURIER TRANSFORM
Proof From (5.13) and (5.15),
Sn(f, t) =
n

m=−n
%
1
T

1
2 T
−1
2 T
f(τ)e−2πinτ/T dτ
&
e2πint/T
= 1
T

1
2 T
−1
2 T
f(τ)
%
n

m=−n
e2πin(t−τ)/T
&
dτ = 1
T

1
2 T
−1
2 T
f(τ)Dn(t −τ) dτ,
where
Dn(t) =
n

m=−n
e2πint/T .
Clearly Dn(0) = 2n+1. When t ̸= 0, putting m = r −n gives the simple geometric
progression
Dn(t) = e−2πint/T
2n

r=0
e2πirt/T = e−2πint/T e4πi(n+1/2)t/T −1
e2πit/T −1
= sin

(2n + 1) πt
T

sin
 πt
T

and the result is proved.
Lemma 5.1 tells us that the nth partial sum of a Fourier series can be written as
(5.17), a simple integral that involves the underlying function and the Dirichlet
kernel†. As we can see in Figure 5.3, as n increases, the Dirichlet kernel becomes
more and more concentrated around the origin. What we need to show is that,
as n →∞, the Dirichlet kernel just picks out the value f(τ) at τ = t in (5.17),
and hence that Sn(f, t) →f(t) as n →∞. As we will see in the next section, the
sequence {Dn(t)} is closely related to the delta function.
Lemma 5.2 (The Riemann–Lebesgue lemma) If f : [a, b] →R, f ∈PC[a, b]
and f is bounded, then
lim
λ→∞
 b
a
f(t) sin λt dt = lim
λ→∞
 b
a
f(t) cos λt dt = 0.
Proof
This seems intuitively obvious, since, as λ increases, the period of oscilla-
tion of the trigonometric function becomes smaller, and the contributions from the
positive and negative parts of the integrand cancel out.
Let [c, d] be a subinterval of [a, b] on which f is continuous. Deﬁne
I(λ) =
 d
c
f(t) sin λt dt.
(5.18)
The argument for f(t) cos λt is identical. By making the substitution t = τ + π/λ,
we obtain
I(λ) = −
 d−π/λ
c−π/λ
f

τ + π
λ

sin λτ dτ.
(5.19)
† As we shall see, this is actually a convolution integral.

5.1 GENERAL FOURIER SERIES
131
Adding (5.18) and (5.19) gives
2I(λ) = −
 c
c−π/λ
f

t + π
λ

sin λt dt +
 d
d−π/λ
f(t) sin λt dt
+
 d−π/λ
c

f(t) −f

t + π
λ

sin λt dt.
Let K be the maximum value of |f| on [c, d], which we know exists by Theorem A2.1.
If we also assume that λ is large enough that π/λ ⩽d −c, then, remembering that
| sin λt| ⩽1,
|I(λ)| ⩽Kπ
λ
+ 1
2
 d−π/λ
c
f(t) −f

t + π
λ
 dt.
(5.20)
Now, since f is continuous on the closed interval [c, d], it is also uniformly continuous
there and, given any constant ϵ, we can ﬁnd a constant λ0 such that
f(t) −f

t + π
λ
 <
ϵ
d −c −π/λ ∀λ > λ0.
Since we can also choose λ0 so that Kπ/λ < ϵ/2 ∀λ > λ0, (5.20) shows that
|I(λ)| < ϵ, and hence that I(λ) →0 as λ →∞. Applying this result to all of the
subintervals of [a, b] on which f is continuous completes the proof.
Theorem 5.1 (The Fourier theorem) If f and f ′ are bounded functions of
period T with f, f ′ ∈PC[a, b], then the right hand side of (5.12), with Cn given by
(5.13), converges pointwise to
1
2

lim
τ→t−f(τ) + lim
τ→t+ f(τ)
	
for −1
2T < t < 1
2T,
1
2

lim
τ→−1
2 T + f(τ) +
lim
τ→1
2 T −f(τ)

for t = −1
2T or 1
2T.
Note that, at points where f(t) is continuous, (5.10) converges pointwise to f(t).
Proof At any point t = t0 ∈(−1
2T, 1
2T),
f →f−as t →t−
0 , f →f+ as t →t+
0 ,
since f is piecewise continuous, with f−= f+ if f is continuous at t = t0. Similarly,
since f ′ is piecewise continuous, f has well-deﬁned left and right derivatives
f ′
−(t0) = lim
h→0
f−−f(t0 −h)
h
,
f ′
+(t0) = lim
h→0
f(t0 + h) −f+
h
,
with f ′
−(t0) = f ′
+(t0) if f ′ is continuous at t = t0. By the mean value theorem
(Theorem A2.2), for h small enough that f is continuous when t0 −h ⩽t < t0,
f−−f(t0 −h) = f ′(c)h for some t0 −h < c < t0.

132
FOURIER SERIES AND THE FOURIER TRANSFORM
Since f ′ is bounded, there exists some M such that
|f−−f(t0 −h)| ⩽1
2Mh.
After using a similar argument for t > t0, we arrive at
|f−−f(t0 −h)| + |f(t0 + h) −f+| ⩽Mh
(5.21)
for all h > 0 such that f is continuous when t0 −h < t0 < t0 + h.
Now, in (5.17) we make the change of variable τ = t + τ ′ and, using the facts
that Dn(t) = Dn(−t), and f and Dn have period T, we ﬁnd that
Sn(f, t) = 1
T

1
2 T
−1
2 T
f(t + τ ′)D(τ ′) dτ ′
= 1
T

1
2 T
−1
2 T
1
2 {f(t + τ ′) + f(t −τ ′)} D(τ ′) dτ ′,
and hence that
Sn(f, t) −1
2 (f−+ f+) = 1
T

1
2 T
−1
2 T
g(t, τ ′) sin

(2n + 1)πτ ′
T
	
dτ ′,
(5.22)
where
g(t, τ ′) = f(t + τ ′) −f+ + f(t −τ ′) −f−
2 sin(πτ ′/T)
.
Considered as a function of τ ′, g(t, τ ′) is bounded and piecewise continuous, except
possibly at τ ′ = 0. However, for suﬃciently small τ ′, (5.21) shows that
|g(t, τ ′)| ⩽
M|τ ′|
2| sin(πτ ′/T)|,
which is bounded as τ ′ →0. The function g therefore satisﬁes the conditions of the
Riemann–Lebesgue lemma, and we conclude from (5.22) that Sn →1
2(f−+ f+) as
n →∞and the result is proved. By exploiting the periodicity of f, the same proof
works for t = −1
2T or 1
2T with minor modiﬁcations.
Theorem 5.2 (Uniform convergence of Fourier series) For a function f(t)
that satisﬁes the conditions of Theorem 5.1, in any closed subinterval of [−1
2T, 1
2T],
the right hand side of (5.12), with Cn given by (5.13), converges uniformly to f(t)
if and only if f(t) is continuous there.
We will not give a proof, but note that this is consistent with our earlier discussion
of Gibbs’ phenomenon.
Finally, returning to the question of whether all bounded, piecewise continuous,
periodic functions have a convergent Fourier series expansion, we note that this

5.2 THE FOURIER TRANSFORM
133
diﬃcult question was not ﬁnally resolved until 1964, when Carleson showed that
such functions exist whose Fourier series expansions diverge at a ﬁnite or countably-
inﬁnite set of points (for example, the rational numbers). Of course, as we have
seen in Theorem 5.1, these functions cannot possess a bounded piecewise continuous
derivative, even though they are continuous, and are clearly rather peculiar (see, for
example, Figure A2.1). However, such functions do arise in the theory of Brownian
motion and stochastic processes, which are widely applicable to real world problems,
for example in models of ﬁnancial derivatives. For further details, see K¨orner (1988),
and references therein.
5.2
The Fourier Transform
In order to motivate the deﬁnition of the Fourier transform, consider (5.12) and
(5.13), which deﬁne the complex form of the Fourier series expansion of a periodic
function, f(t). What happens as the period, T, tends to inﬁnity? Combining (5.12)
and (5.13) gives
f(t) = 1
T
∞

n=−∞
e2πint/T

1
2 T
−1
2 T
e−2πint′/T f(t′) dt′.
If we now let kn = 2πn/T and ∆kn = kn −kn−1 = 2π/T, we have
f(t) = 1
2π
∞

n=−∞
∆kneiknt

1
2 T
−1
2 T
e−iknt′f(t′) dt′.
If we now momentarily abandon mathematical rigour, as T →∞, kn becomes a
continuous variable and the summation becomes an integral, so that we obtain
f(t) = 1
2π
 ∞
−∞
eikt
 ∞
−∞
e−ikt′f(t′) dt′ dk,
(5.23)
which is known as the Fourier integral. We will prove this result rigorously later.
If we now deﬁne the Fourier transform of f(t) as
F[f(t)] = ˜f(k) =
 ∞
−∞
eiktf(t) dt,
(5.24)
we immediately have an inversion formula,
f(t) = 1
2π
 ∞
−∞
e−ikt ˜f(k) dk.
(5.25)
Note that some texts alter the deﬁnition of the Fourier transform (and its inverse) to
take account of the factor of 1/2π in a diﬀerent way, for example with the transform
and its inverse each multiplied by a factor of 1/
√
2π. This is a minor, but irritating,
detail, which does not aﬀect the basic ideas.
The Fourier transform maps a function of t to a function of k. In the same
way as the Fourier series expansion of a periodic function decomposes the function
into its constituent harmonic parts, the Fourier transform produces a function of
a continuous variable whose value indicates the frequency content of the original

134
FOURIER SERIES AND THE FOURIER TRANSFORM
function. This has led to the widespread use of the Fourier transform to analyze the
form of time-varying signals, for example in electrical engineering and seismology.
We will be more concerned here with the use of the Fourier transform to solve
partial diﬀerential equations.
Before we can proceed, we need to give (5.24) and (5.25) a ﬁrmer mathematical
basis. What restrictions must we place on the function f(t) for its Fourier trans-
form to exist? This is a diﬃcult question, and our approach is to treat f(t) as a
generalized function. The advantage of this is that every generalized function
has a Fourier transform and an inverse Fourier transform, and that the ordinary
functions in whose Fourier transforms we are interested form a subset of the gen-
eralized functions. We will not go into great detail, for which the reader is referred
to Lighthill (1958), the classic, and very accessible, introduction to the subject.
5.2.1
Generalized Functions
We begin with some deﬁnitions. A good function, g(x), is a function in C∞(R)
that decays rapidly enough that g(x) and all of its derivatives tend to zero faster
than |x|−N as x →±∞for all N > 0. For example, e−x2 and sech2x are good
functions.
A sequence of good functions, {fn(x)}, is said to be regular if, for any good
function F(x),
lim
n→∞
 ∞
−∞
fn(x)F(x) dx
(5.26)
exists. For example fn(x) = G(x)/n is a regular sequence for any good function
G(x), with
lim
n→∞
 ∞
−∞
fn(x)F(x) dx = lim
n→∞
1
n
 ∞
−∞
G(x)F(x) dx = 0.
Two regular sequences of good functions are equivalent if, for any good function
F(x), the limit (5.26) exists and is the same for each sequence. For example, the
regular sequences {G(x)/n} and {G(x)/n2} are equivalent.
A generalized function, f(x), is a regular sequence of good functions, and two
generalized functions are equal if their deﬁning sequences are equivalent. Gener-
alized functions are therefore only deﬁned in terms of their action on integrals of
good functions, with
 ∞
−∞
f(x)F(x) dx = lim
n→∞
 ∞
−∞
fn(x)F(x) dx,
for any good function F(x).
If f(x) is an ordinary function such that (1 + x2)−Nf(x) is integrable from −∞
to ∞for some N, then the generalized function f(x) equivalent to the ordinary
function is deﬁned as any sequence of good functions {fn(x)} such that, for any
good function F(x),
lim
n→∞
 ∞
−∞
fn(x)F(x) dx =
 ∞
−∞
f(x)F(x) dx.

5.2 THE FOURIER TRANSFORM
135
For example, the generalized function equivalent to zero can be represented by
either of the sequences {G(x)/n} and {G(x)/n2}.
The zero generalized function is very simple. Let’s consider some more useful
generalized functions.
— The unit function, I(x), is deﬁned so that for any good function F(x),
 ∞
−∞
I(x)F(x) dx =
 ∞
−∞
F(x) dx.
A useful sequence of good functions that deﬁnes the unit function is {e−x2/4n}.
The unit function is the generalized function equivalent to the ordinary function
f(x) = 1.
— The Heaviside function, H(x), is deﬁned so that for any good function F(x),
 ∞
−∞
H(x)F(x) dx =
 ∞
0
F(x) dx.
The generalized function H(x) is equivalent to the ordinary step function†
H(x) =
 0
for x < 0,
1
for x > 0.
— The sign function, sgn(x), is deﬁned so that for any good function F(x),
 ∞
−∞
sgn(x)F(x) dx =
 ∞
0
F(x) dx −
 0
−∞
F(x) dx.
Then sgn(x) can be identiﬁed with the ordinary function
sgn(x) =
 −1
for x < 0,
1
for x > 0.
In fact, sgn(x) = 2H(x) −I(x), since we can note that
 ∞
−∞
{2H(x) −I(x)} g(x) dx = 2
 ∞
−∞
H(x)g(x) dx −
 ∞
−∞
I(x)g(x) dx
= 2
 ∞
0
g(x) dx −
 ∞
−∞
g(x) dx =
 ∞
0
g(x) dx −
 0
−∞
g(x) dx,
using the deﬁnition of the Heaviside and unit functions. This is just the deﬁnition
of the function sgn(x).
— The Dirac delta function, δ(x), is deﬁned so that for any good function F(x),
 ∞
−∞
δ(x)F(x) dx = F(0).
No ordinary function can be equivalent to the delta function. We can see from
† Since generalized functions are only deﬁned through their action on integrals of good functions,
the value of H at x = 0 does not have any signiﬁcance in this context.

136
FOURIER SERIES AND THE FOURIER TRANSFORM
(5.17) and Theorem 5.1 that a sequence based on the Dirichlet kernel, fn(x) =
Dn(x)/2π = sin

(2n + 1) πt
T

/2π sin
 πt
T

, satisﬁes
F(0) = lim
n→∞

1
2 T
−1
2 T
fn(x)F(x) dx.
As we can see in Figure 5.3, the function becomes more and more concentrated
about the origin as n increases, eﬀectively plucking the value of F(0) out of the
integral. However, this only works on a ﬁnite domain. On an inﬁnite domain,
the sequence
fn(x) =
n
π
1/2
e−nx2,
(5.27)
which is illustrated in Figure 5.4 for various n, is a useful way of deﬁning δ(x).
Fig. 5.4. The sequence (5.27), which is equivalent to δ(x).
5.2.2
Derivatives of Generalized Functions
Derivatives of generalized functions are deﬁned by the derivative of any of the
equivalent sequences of good functions. Since we can integrate by parts using any

5.2 THE FOURIER TRANSFORM
137
member of the sequence, we can also, formally, do so for the equivalent generalized
function, treating it as if it were zero at inﬁnity. For example
 ∞
−∞
δ′(x)F(x) dx = −
 ∞
−∞
δ(x)F ′(x) dx = −F ′(0).
This does not allow us to represent δ′(x) in terms of other generalized functions,
but does show us how it acts in an integral, which is all we need to know.
We can also show that H′(x) = δ(x), since
 ∞
−∞
H′(x)F(x) dx = −
 ∞
−∞
H(x)F ′(x) dx = −
 ∞
0
F ′(x) dx = −[F(x)]∞
0 = F(0).
Another useful result is
f(x)δ(x) = f(0)δ(x).
The proof is straightforward, since
 ∞
−∞
f(x)δ(x)F(x) dx = f(0)F(0),
and
 ∞
−∞
f(0)δ(x)F(x) dx =
 ∞
−∞
δ(x)[f(0)F(x)]dx = f(0)F(0).
We can deﬁne the modulus function in terms of the function sgn(x) through
|x| = x sgn(x). Now consider the derivative of the modulus function. Using the
product rule, which is valid because it works for the equivalent sequence of good
functions,
d
dx|x| = d
dx {x sgn(x)} = x d
dx {sgn(x)} + sgn(x) d
dx(x).
We can now use the fact that sgn(x) = 2H(x) −I(x) to show that
d
dx|x| = x d
dx {2H(x) −I(x)} + sgn(x) = 2xδ(x) + sgn(x) = sgn(x),
since xδ(x) = 0.
5.2.3
Fourier Transforms of Generalized Functions
As we have seen, the Fourier transform of a function f(x) is deﬁned as
F[f(x)] =
 ∞
−∞
eikxf(x) dx.
For example, consider the Fourier transform of the function e−|x|, which, using the
deﬁnition, is
F
(
e−|x|)
=
 0
−∞
exeikx dx +
 ∞
0
e−xeikx dx
=
1
1 + ik −
1
−1 + ik =
2
1 + k2 .
(5.28)

138
FOURIER SERIES AND THE FOURIER TRANSFORM
We would also like to know the Fourier transform of a constant, c. However, it
is not clear whether
F[c] = c
 ∞
−∞
eikx dx
is a well-deﬁned integral. Instead we note that, treated as a generalized function,
c = cI(x), and we can deal with the Fourier transform of an equivalent sequence
instead, for example
F[ce−x2/4n] = c
 ∞
−∞
eikx−x2/4n dx = ce−nk2  ∞
−∞
e−(x/2√n−ik√n)
2
dx.
By writing z = x −2ikn and deforming the contour of integration in the complex
plane (see Exercise 5.10 for an alternative method), we ﬁnd that
F[ce−x2/4n] = ce−nk2  ∞
−∞
e−z2/4n dz = 2πc
*n
π e−nk2,
using (3.5).
Since {e−x2/4n} is a sequence equivalent to the unit function, and
{# n
πe−nk2} is a sequence equivalent to the delta function, we conclude that
F[c] = 2πcδ(k).
Another useful result is that, if F[f(x)] = ˜f(k), F[f(ax)] = ˜f(k/a)/a for a > 0.
To prove this, note that from the deﬁnition
F[f(ax)] =
 ∞
−∞
eikxf(ax) dx.
Making the change of variable y = ax gives
F[f(ax)] = 1
a
 ∞
−∞
eiky/af(y) dy,
which is equal to ˜f(k/a)/a as required. As an example of how this result can be
used, we know that F[e−|x|] = 2/(1 + k2), so
F[e−a|x|] = 1
a
2
1 +
 k
a
2 =
2a
a2 + k2 .
Finally, the Fourier transformation is clearly a linear transformation, so that
F[αf + βg] = αF[f] + βF[g].
(5.29)
This means that linear combinations of functions can be transformed separately.
5.2.4
The Inverse Fourier Transform
If we can show that (5.25) holds for all good functions, it follows that it holds
for all generalized functions. We begin with a useful lemma.
Lemma 5.3 The Fourier transform of a good function is a good function.

5.2 THE FOURIER TRANSFORM
139
Proof If f(x) is a good function, its Fourier transform clearly exists and is given
by
F[f] = ˜f(k) =
 ∞
−∞
eikxf(x) dx.
If we diﬀerentiate p times and integrate by parts N times we ﬁnd that
 ˜f (p)(k)
 ⩽

(−1)N
(ik)N
 ∞
−∞
eikx d N
dxN {(ix)pf(x)} dx

⩽
1
|k|N
 ∞
−∞

d N
dxN {xpf(x)}
 dx.
All derivatives of ˜f therefore decay at least as fast as |k|−N as |k| →∞for any
N > 0, and hence ˜f is a good function.
Theorem 5.3 (The Fourier inversion theorem) If f(x) is a good function with
Fourier transform
˜f(k) =
 ∞
−∞
eikxf(x) dx,
then the inverse Fourier transform is given by
f(x) = 1
2π
 ∞
−∞
e−ikx ˜f(k) dk.
Proof Firstly, we note that for ϵ > 0,
F
(
e−ϵx2 ˜f(−x)
)
=
 ∞
−∞
eikx−ϵx2  ∞
−∞
e−ixtf(t) dt
	
dx.
Since f is a good function, we can exchange the order of integration and arrive at
F
(
e−ϵx2 ˜f(−x)
)
=
 ∞
−∞
f(t)
 ∞
−∞
ei(k−t)x−ϵx2 dx dt
=
 ∞
−∞
f(t)e−(k−t)2/4ϵ
 ∞
−∞
exp

−

ϵ1/2x −i(k −t)
2ϵ
2
dx dt.
Now, by making the change of variable x = ¯x + i(k −t)/2ϵ3/2, checking that this
change of contour is possible in the complex x-plane, we ﬁnd that
 ∞
−∞
exp

−

ϵ1/2x −i(k −t)
2ϵ
2
dx =
 ∞
−∞
e−ϵ¯x2 d¯x =
*π
ϵ .
This means that
F
(
e−ϵx2 ˜f(−x)
)
=
*π
ϵ
 ∞
−∞
e−(k−t)2/4ϵf(t) dt.

140
FOURIER SERIES AND THE FOURIER TRANSFORM
In addition,
*π
ϵ
 ∞
−∞
e−(k−t)2/4ϵ dt =
*π
ϵ
 ∞
−∞
e−t2/4ϵ dt = 2π,
so we can write
1
2π F
(
e−ϵx2 ˜f(−x)
)
−f(k) = 1
2π
*π
ϵ
 ∞
−∞
e−(k−t)2/4ϵ {f(t) −f(k)} dt.
Since f is a good function,

f(t) −f(k)
t −k
 ⩽max
x∈R |f ′(x)| ,
and hence

1
2π F
(
e−ϵx2 ˜f(−x)
)
−f(k)
 ⩽1
2π
*π
ϵ max
x∈R |f ′(x)|
 ∞
−∞
e−(k−t)2/4ϵ|t −k| dt
= 1
2π
*π
ϵ max
x∈R |f ′(x)| 4ϵ
 ∞
−∞
e−X2|X| dX →0 as ϵ →0.
We conclude that
f(k) = 1
2π F
(
˜f(−x)
)
= 1
2π
 ∞
−∞
eikx
 ∞
−∞
e−ixtf(t) dt.
This is precisely the Fourier integral, (5.23), and hence the result is proved.
5.2.5
Transforms of Derivatives and Convolutions
Fourier transforms are an appropriate tool for solving diﬀerential equations on
the unbounded domain −∞< x < ∞. In order to proceed, we need to be able
to ﬁnd the Fourier transform of a derivative. For good functions, this can be done
using integration by parts. We ﬁnd that
F[f ′(x)] =
 ∞
−∞
f ′(x)eikx dx = −ik
 ∞
−∞
f(x)eikx dx = −ikF[f],
since the good function f must tend to zero as x →±∞. Since generalized func-
tions are deﬁned in terms of sequences of good functions, this result also holds for
generalized functions. Similarly, the second derivative is
F[f ′′(x)] = −k2F[f].
We can also deﬁne the convolution of two functions f and g as
f ∗g =
 ∞
−∞
f(y)g(x −y) dy,
which is a function of x only. Note that f ∗g = g ∗f. As we shall see below, the
solutions of diﬀerential equations can often be written as a convolution because of
the key property
F[f ∗g] = F[f]F[g].

5.3 GREEN’S FUNCTIONS REVISITED
141
To derive this, we write down the deﬁnition of the Fourier transform of f ∗g,
F[f ∗g] =
 ∞
−∞
eikx
 ∞
−∞
f(y)g(x −y) dy
	
dx.
Note that the factor eikx is independent of y and so can be taken inside the inner
integral to give
F[f ∗g] =
 ∞
x=−∞
 ∞
y=−∞
eikxf(y)g(x −y) dy dx.
Since the limits of integration are independent of one another, we can exchange the
order of integration so that
F[f ∗g] =
 ∞
y=−∞
 ∞
x=−∞
eikxf(y)g(x −y) dx dy.
Now f(y) is independent of x, and can be extracted from the inner integral to give
F[f ∗g] =
 ∞
y=−∞
f(y)
 ∞
x=−∞
eikxg(x −y) dx
	
dy.
By making the transformation z = x −y (so that dz = dx) in the inner integral,
we have
F[f ∗g] =
 ∞
y=−∞
f(y)
 ∞
z=−∞
eik(z+y)g(z) dz
	
dy,
and now extracting the factor eiky, since this is independent of z, allows us to write
F[f ∗g] =
 ∞
y=−∞
f(y)eiky dy
 ∞
z=−∞
eikzg(z) dz = F[f]F[g].
5.3
Green’s Functions Revisited
Let’s, for the moment, forget our previous deﬁnition of a Green’s function (Sec-
tion 4.1.2), and deﬁne it instead, for a linear operator L with domain −∞< x < ∞,
as the solution of the diﬀerential equation LG = δ(x) subject to G →0 as |x| →∞.
If we assume that G is a good function, we can use the Fourier transform to ﬁnd
G. For example, consider the operator
L ≡d 2
dx2 −1,
so that LG = δ is
d 2G
dx2 −G = δ(x).
We will solve this equation subject to G →0 as |x| →∞. Taking the Fourier
transform of both sides of this equation and exploiting the linearity of the transform,
(5.29), gives
F
d 2G
dx2

−F[G] = F[δ(x)].

142
FOURIER SERIES AND THE FOURIER TRANSFORM
Firstly we need to determine the Fourier transform of the delta function. From the
deﬁnition
F[δ(x)] =
 ∞
−∞
eikxδ(x) dx =

eikx
x=0 = 1.
Therefore
−k2F[G] −F[G] = 1,
using the fact that F[G′′] = −k2F[G]. After rearrangement this becomes
F[G] = −
1
k2 + 1,
which we know from (5.28) means that
G(x) = −1
2e−|x|.
Why should we want to construct such a Green’s function? As before, the answer
is, to be able to solve the inhomogeneous diﬀerential equation. Suppose that we
need to solve Lφ = P. The solution is φ = G ∗P. To see this, note that
L(G ∗P) = L
 ∞
−∞
G(x −y)P(y) dy =
 ∞
−∞
LG(x −y)P(y) dy
=
 ∞
−∞
δ(x −y)P(y) dy = P(x).
Once we know the Green’s function we can therefore write down the solution of
Lφ = P as φ = G ∗P.
There are both diﬀerences and similarities between this deﬁnition of the Green’s
function, which is sometimes referred to as the free space Green’s function, and
the deﬁnition that we gave in Section 4.1.2. Firstly, the free space Green’s function
depends only on x, whilst the other deﬁnition depends upon both x and s. We can
see some similarities by considering the self-adjoint problem
d
dx

p(x)dG
dx

+ q(x)G = δ(x).
On any interval that does not contain the point x = 0, G is clearly the solution
of the homogeneous problem, as before, and is continuous there. If we integrate
between x = −ϵ and x = ϵ, we get

p(x)dG
dx
ϵ
−ϵ
+
 ϵ
−ϵ
q(x)G(x) dx =
 ϵ
−ϵ
δ(x) dx = 1.
If p(x) is continuous at x = 0, when we take the limit ϵ →0 this reduces to
dG
dx
x=0+
x=0−=
1
p(0),
which, apart from a sign diﬀerence, is the result that we obtained in Section 4.1.2.

5.4 SOLUTION OF LAPLACE’S EQUATION USING FOURIER TRANSFORMS
143
The end result from constructing the Green’s function is the same as in Sec-
tion 4.1.2. We can express the solution of the inhomogeneous boundary value prob-
lem as an integral that involves the Green’s function and the inhomogeneous term.
We will return to consider the Green’s function for partial diﬀerential equations in
Section 5.5.
5.4
Solution of Laplace’s Equation Using Fourier Transforms
Let’s consider the problem of solving Laplace’s equation, ∇2φ = 0, for y > 0 subject
to φ = f(x) on y = 0 and φ bounded as y →∞. Boundary value problems for
partial diﬀerential equations where the value of the dependent variable is prescribed
on the boundary are often referred to as Dirichlet problems. We can solve this
Dirichlet problem using a Fourier transform with respect to x. We deﬁne
˜φ(k, y) =
 ∞
−∞
eikxφ(x, y) dx = F[φ].
We begin by taking the Fourier transform of Laplace’s equation, noting that
F
∂2φ
∂y2

= ∂2
∂y2 F [φ] = ∂2 ˜φ
∂y2 ,
which is easily veriﬁed from the deﬁnition of the transform. This gives us
F
∂2φ
∂y2 + ∂2φ
∂x2

= ∂2 ˜φ
∂y2 −k2 ˜φ = 0.
This has solution ˜φ = A(k)e|k|y+B(k)e−|k|y. However, we require that φ is bounded
as y →∞, which gives A(k) = 0, and hence
˜φ(k, y) = B(k)e−|k|y.
It now remains to satisfy the condition at y = 0, namely φ(x, 0) = f(x). We take
the Fourier transform of this condition to give ˜φ(k, 0) = ˜f(k), where ˜f(k) = F[f].
By putting y = 0 we therefore ﬁnd that B(k) = ˜f(k), so that
˜φ(k, y) = ˜f(k)e−|k|y.
We can invert this Fourier transform of the solution using the convolution theo-
rem. Since
F−1[ ˜f ˜g] = F−1[F[f] F[g]] = f ∗g,
the solution is just the convolution of the boundary condition, f(x), with the inverse
transform of ˜g(k) = e−|k|y. To ﬁnd g(x) = F−1[˜g(k)] = F−1[e−|k|y], we note from
(5.28) that F[e−|x|] = 2/(1 + k2), and exploit the fact that F[f(ax)] = ˜f(k/a)/a,
so that F[e−a|x|] = 2a/(a2 + k2), and hence e−a|x| = F−1[2a/(a2 + k2)]. Using the
formula for the inverse transform gives
e−a|x| = 1
2π
 ∞
−∞
e−ikx
2a
a2 + k2 dk.

144
FOURIER SERIES AND THE FOURIER TRANSFORM
We can exploit the similarity between the Fourier transform and its inverse by
making the transformation k →−x, x →k, which leads to
e−a|k| = 1
2π
 ∞
−∞
eikx
2a
a2 + x2 dx = 1
2π F

2a
a2 + x2

,
and hence
F−1 (
e−|k|y)
=
y
π(y2 + x2).
In the deﬁnition of the convolution we use the variable ξ as the dummy variable
rather than y to avoid confusion with the spatial coordinate in the problem, so that
f ∗g =
 ∞
−∞
f(ξ)g(x −ξ) dξ.
The solution φ can now be written as the convolution integral
φ(x, y) =
 ∞
−∞
f(ξ)
y
π{y2 + (x −ξ)2} dξ = y
π
 ∞
−∞
f(ξ)
y2 + (x −ξ)2 dξ.
(5.30)
As an example, consider the two-dimensional, inviscid, irrotational ﬂow in the upper
half plane (see Section 2.6.2), driven by the Dirichlet condition φ(x, 0) = f(x),
where
f(x) =
 1
for −1 < x < 1,
0
elsewhere.
Using the formula we have derived,
φ = y
π
 1
−1
dξ
y2 + (x −ξ)2 = y
π
1
y tan−1
ξ −x
y
1
−1
= 1
π

tan−1
1 −x
y

+ tan−1
1 + x
y
	
.
(5.31)
Figure 5.5 shows some contours of equal φ.†
Finally, let’s consider the Neumann problem for Laplace’s equation in the
upper half plane, y > 0. This is the same as the Dirichlet problem except that the
boundary condition is in terms of a derivative, with ∂φ/∂y = f(x) at y = 0. As
before we ﬁnd that ˜φ(k, y) = B(k)e−|k|y, and the condition at y = 0 tells us that
B(k) = −
˜f(k)
|k| ,
and hence
˜φ(k, y) = −˜f(k)e−|k|y
|k|
.
In order to invert this Fourier transform we recall that
 ∞
−∞
eikx
2y
y2 + x2 dx = 2πe−|k|y,
† See Section 2.6.2 for details of how to create this type of contour plot in MATLAB.

5.5 GENERALIZATION TO HIGHER DIMENSIONS
145
2
4
6
8
10
−4
−2
−6
−8
−10
Fig. 5.5. Contours of constant potential function, φ, given by (5.31).
and integrate both sides with respect to y to obtain
 ∞
x=−∞
eikx log(y2 + x2) dx = −2π e−|k|y
|k|
.
In other words
−e−|k|y
|k|
= 1
2π F
!
log

y2 + x2"
,
and hence the solution is
φ(x, y) = 1
2π
 ∞
−∞
f(ξ) log{y2 + (x −ξ)2} dξ.
5.5
Generalization to Higher Dimensions
The theory of Fourier transforms and generalized functions can be extended to
higher dimensions. This allows us to use the techniques that we have developed
above to solve other partial diﬀerential equations.
5.5.1
The Delta Function in Higher Dimensions
If we let x = (x1, x2, . . . , xn) be a vector in Rn, we can deﬁne the delta function
in Rn, δ(x), through the integral

Rn δ(x)F(x) d nx = F(0).

146
FOURIER SERIES AND THE FOURIER TRANSFORM
This is a multiple integral over the whole of Rn, and F is a good function in each
of the coordinates xi.
Cartesian Coordinates
In Cartesian coordinates we have δ(x) = δ(x1)δ(x2) . . . δ(xn), since

Rn δ(x)F(x) d nx
=

Rn−1 δ(x1) . . . δ(xn−1)

R
δ(xn)F(x1, . . . , xn−1, xn) dxn
	
d n−1x
=

Rn−1 δ(x1) . . . δ(xn−1)F(x1, . . . , xn−1, 0)d n−1x = · · · = F(0, 0, . . . , 0).
Plane Polar Coordinates
In terms of standard plane polar coordinates, (r, θ), in R2, δ(x) must be isotropic,
in other words independent of θ, and a multiple of δ(r). We therefore let δ(x) =
a(r)δ(r) and, noting that d 2x = r dr dθ,
 2π
θ=0
 ∞
r=0
δ(x) d 2x = 2π
 ∞
0
a(r)δ(r)r dr.
By symmetry,
 ∞
0
δ(r) dr = 1
2,
so we can take a(r) = 1/πr, and hence
δ(x) = 1
πrδ(r).
Spherical Polar Coordinates
In R3, the isotropic volume element can be written as d 3x = 4πr2 dr, the volume
of a thin, spherical shell, where r is now |x|, the distance from the origin. Again,
the delta function must be a multiple of δ(r), and the same argument gives
δ(x) =
1
2πr2 δ(r).
5.5.2
Fourier Transforms in Higher Dimensions
If f(x) = f(x1, x2, . . . , xn) and k = (k1, k2, . . . , kn), we can deﬁne the Fourier
transform of f as
˜f(k) =

Rn f(x)eik·x d nx.
(5.32)
For example, in R3
˜f(k1, k2, k3) =
 ∞
x3=−∞
 ∞
x2=−∞
 ∞
x1=−∞
f(x1, x2, x3)ei(k1x1+k2x2+k3x3) dx1 dx2 dx3.

5.5 GENERALIZATION TO HIGHER DIMENSIONS
147
We can proceed as we did for one-dimensional functions and show, although we will
not provide the details, that:
(i) If f(x) is a generalized function, then its Fourier transform exists as a gen-
eralized function.
(ii) The inversion formula is
f(x) =
1
(2π)n

Rn
˜f(k)e−ik·x d nk.
(5.33)
The inversion of Fourier transforms in higher dimensions is considerably more dif-
ﬁcult than in one dimension, as we shall see.
Example: Laplace’s equation
Let’s try to construct the free space Green’s function for Laplace’s equation in R3.
We introduced the idea of a Green’s function for a linear operator in Section 5.3.
In this case, we seek a solution of
∇2G = δ(x) subject to G →0 as |x| →∞.
(5.34)
The Fourier transform of G is
˜G(k1, k2, k3) =
 ∞
−∞
 ∞
−∞
 ∞
−∞
G(x1, x2, x3)ei(k1x1+k2x2+k3x3) dx1 dx2 dx3,
so that
F
 ∂G
∂x1

=
 ∞
−∞
 ∞
−∞
 ∞
−∞
∂G
∂x1
(x1, x2, x3)ei(k1x1+k2x2+k3x3) dx1 dx2 dx3
=
 ∞
−∞
 ∞
−∞
(
ei(k1x1+k2x2+k3x3)G
)∞
−∞
−
 ∞
−∞
Gik1ei(k1x1+k2x2+k3x3) dx1
	
dx2 dx3,
after integrating by parts. Since G vanishes at inﬁnity, we conclude that
 ∂G
∂x1

= −ik1
 ∞
−∞
 ∞
−∞
 ∞
−∞
Gei(k1x1+k2x2+k3x3) dx1 dx2 dx3 = −ik1 ˜G.
This result is, of course, analogous to the one-dimensional result that we derived
in Section 5.2.5.
If we now take the Fourier transform of (5.34), we ﬁnd that

(−ik1)2 + (−ik2)2 + (−ik3)2 ˜G =

R3 δ(x)eik·x d 3x = 1,
and hence ˜G = −1/|k|2. The inversion formula, (5.33), then shows that
G(x) = −1
8π3

R3
e−ik·x
|k|2
d 3k.
(5.35)
In order to evaluate this integral, we need to introduce spherical polar coordinates

148
FOURIER SERIES AND THE FOURIER TRANSFORM
(k, θ, φ), with the line θ = 0 in the x-direction. Then k · x = |k| |x| cos θ = kr cos θ
and (5.35) becomes
G(x) = −1
8π3
 ∞
k=0
 π
θ=0
 2π
φ=0
e−ikr cos θ
k2
k2 sin θ dk dθ dφ
= −1
4π2
 ∞
k=0
e−ikr cos θ
ikr
π
θ=0
dk = −1
2π2
 ∞
k=0
sin kr
kr
dk
= −
1
2π2r
 ∞
z=0
sin z
z
dz = −1
4πr,
using the standard result,
' ∞
0
(sin z/z) dz = π/2 (see, for example, Ablowitz and
Fokas, 1997).
Now that we know the Green’s function, we are able to solve the inhomogeneous
problem
∇2φ = Q(x) subject to φ →0 as |x| →∞,
(5.36)
in terms of a convolution integral, by a direct generalization of the results presented
in Section 5.2.5. We ﬁnd that
φ = −1
4π

R3
Q(y)
|x −y|d 3y.
This result is fundamental in the theory of electrostatics, where Q is the distribution
of charge density, and φ the corresponding electrical potential.
Example: The wave equation
Let’s try to solve the three-dimensional wave equation†,
∇2u = 1
c2
∂2u
∂t2
for t > 0 and x ∈R3,
(5.37)
subject to the initial conditions
u(x, y, z, 0) = 0,
∂u
∂t (x, y, z, 0) = f(x, y, z).
(5.38)
The Fourier transform of (5.37) is
∂2˜u
∂t2 = −c2k2˜u,
(5.39)
where ˜u is the Fourier transform of u and k2 = k2
1 +k2
2 +k2
3. The initial conditions,
(5.38), then become
˜u(k, 0) = 0,
∂˜u
∂t (k, 0) = ˜f(k).
The general solution of (5.39) is
˜u = A(k)eickt + B(k)e−ickt,
† See Section 3.9.1 for a derivation of the wave equation in two dimensions, and Billingham and
King (2001) for a derivation of (5.37) for sound waves.

EXERCISES
149
and when we enforce the initial conditions, this becomes
˜u =
˜f(k)
2ick

eickt −e−ickt
.
The inversion formula, (5.33), then shows that
u(x, t) =
1
8π3

R3
˜f(k)
2ick {exp (ickt −ik · x) −exp (−ickt −ik · x)} d 3k.
(5.40)
For any given value of k, the terms exp (±ickt −ik · x) represent plane travelling
wave solutions of the three-dimensional wave equation, since they remain constant
on the planes k.x = ±ckt, which move perpendicular to themselves, in the direction
±k, at speed c. Since the integral (5.40) is a weighted sum of these plane waves at
diﬀerent wavenumbers k, we can see that the Fourier transform has revealed that
the solution of (5.37) subject to (5.38) can be written as a continuous spectrum of
plane wave solutions travelling in all directions. It is somewhat easier to interpret
solutions like (5.40) in the large time limit, t ≫1, with |x|/t ﬁxed. We will do this
in Section 11.2.2 using the method of stationary phase.
Exercises
5.1
Determine the Fourier series expansion of f(x) for −π < x < π, where (a)
f(x) = ex, (b) f(x) = | sin x| , (c) f(x) = x2.
5.2
Determine the Fourier series expansion of the function
f(x) =
 0
for −π < x ⩽0,
x
for 0 < x < π.
Using this series, show that
1 + 1
32 + 1
52 + · · · = π2
8 .
5.3
Show that
(a) δ(αx) = 1
|α|δ(x),
(b) δ(x2 −α2) = 1
2α {δ(x + α) + δ(x −α)}.
5.4
Express δ(ax + b) in the form µδ(x + σ) for appropriately chosen constants
µ and σ.
5.5
What is the general solution of the equation (x −a)f(x) = b, if f is a
generalized function?
5.6
If g(x) = 0 at the points xi for i = 1, 2, . . . , n, ﬁnd an expression for δ(g(x))
in terms of the sequence of delta functions {δ(x −xi)}.
5.7
Show that d
dxsgn(x) = 2δ(x). What is d
dxe−|x|?
5.8
Calculate the Fourier transforms of

150
FOURIER SERIES AND THE FOURIER TRANSFORM
(a)
f(x) =



0
for x < −1 and x > 1,
−1
for −1 < x < 0,
1
for 0 < x < 1,
(b)
f(x) =
 ex
for x < 0,
0
for x > 0,
(c)
f(x) =
 xe−x
for x > 0,
0
for x < 0.
5.9
Show that the Fourier transform has the ‘shifting property’,
F[f(x −a)] = eikaF[f(x)].
5.10
If
J(k) =
 ∞
−∞
e−(x−ik/2)2 dx,
show by diﬀerentiation under the integral sign that dJ/dk = 0. Verify that
J(0) = √π and hence evaluate F
(
e−x2)
.
5.11
Use Fourier transforms to show that the solution of the initial boundary
value problem,
∂u
∂t = k ∂2u
∂x2 −λu,
for −∞< x < ∞, t > 0,
with λ and k real constants, subject to
u(x, 0) = f(x),
u →0
as |x| →∞,
can be written in convolution form as
u(x, t) =
e−λt
√
4πkt
 ∞
−∞
exp

−(x −y)2
4kt
	
f(y) dy.
5.12
For n ∈N and k ∈Z, evaluate the integral
 π
−π
ekxeinx dx,
and hence show that
 π
−π
ekx cos nx dx = (−1)n+12n sinh kπ
k2 + n2
.
Obtain the Fourier series of the function (of period 2π) deﬁned by
f(x) = cosh kx
for
−π < x < π.

EXERCISES
151
Find the value of
∞

n=1
1
k2 + n2
as a function of k and use (4.49) to evaluate
∞

n=1
1
(k2 + n2)2 .
5.13
The forced wave equation is
∂2φ
∂x2 −1
c2
0
∂2φ
∂t2 = Q(x, t),
where Q is the forcing function, which satisﬁes Q →0 as |x| →∞.
(a) Show that if Q = q(x)e−iωt then a separable solution exists with
φ = e−iωtf(x) and f ′′ + k2
0f = q(x), where k0 = ω/c0.
(b) Find the Green’s function solution of G′′ +k2
0G = δ(x) that behaves
like e±ik0x as x →±∞. What is the physical signiﬁcance of these
boundary conditions?
(c) Show that
φ = e−iωtG ∗q
= e−iωt
2ik0
 x
−∞
q(y)eik0(x−y) dy +
 ∞
x
q(y)e−ik0(x−y) dy
	
.
(d) Show that, as x →∞,
φ ∼e−i(ωt−k0x)
2ik0
 ∞
−∞
q(y)e−ik0y dy.
What is the physical interpretation of this result?
5.14
The free space Green’s function for the modiﬁed Helmholtz equation
in three dimensions satisﬁes

∇2 −m2
G = δ(x1, x2, x3),
with G →0 as |x| →∞. Use Fourier transforms to show that
G(x) = −1
8π3

R3
e−ik·x
k2 + m2 d 3k.
Use contour integration to show that this can be simpliﬁed to
G(x) = −1
4πre−mr,
where r = |x|. Verify by direct substitution that this function satisﬁes the
modiﬁed Helmholtz equation.

CHAPTER SIX
Laplace Transforms
Integral transforms of the form
T [f(x)] =

I
K(x, k)f(x) dx,
where I is some interval on the real line, K(x, k) is the kernel of the transform and k
is the transform variable, are useful tools for solving linear diﬀerential equations.
Other types of equation, such as linear integral equations, can also be solved using
integral transforms. In Chapter 5 we met the Fourier transform, for which the kernel
is eikx and I = R. The Fourier transform allows us to solve linear boundary value
problems whose domain of solution is the whole real line. In this chapter we will
study the Laplace transform, for which the usual notation for the original variable
is t and for the transform variable is s, the kernel is e−st and I = R+ = [0, ∞).
This transform allows us to solve linear initial value problems, with t representing
time. As we shall see, it is closely related to the Fourier transform.
6.1
Deﬁnition and Examples
The Laplace transform of f(t) is
L[f(t)] = F(s) =
 ∞
0
e−stf(t) dt.
(6.1)
We will consider for what values of s the integral is convergent later in the chapter,
and begin with some examples.
Example 1
Consider f(t) = ekt, where k is a constant. Substituting this into (6.1), we have
L[ekt] =
 ∞
0
e−stekt dt =
 ∞
0
e−(s−k)t dt =
 e−(s−k)t
−(s −k)
∞
0
.
Now, in order that e−(s−k)t →0 as t →∞, and hence that the integral converges,
we need s > k. If s is a complex-valued variable, which is how we will need to treat
s when we use the inversion formula, (6.5), we need Re(s) > Re(k). This shows
that
L[ekt] =
1
s −k
for Re(s) > Re(k).

6.1 DEFINITION AND EXAMPLES
153
Example 2
Consider f(t) = cos ωt, where ω is a constant. Since cos ωt = 1
2(eiωt + e−iωt),
L[cos ωt] = 1
2
 ∞
0

e(iω−s)t + e−(iω+s)t
ds = 1
2

1
s −iω +
1
s + iω

,
provided Re(s) > 0. We can combine the two fractions to show that
L[cos ωt] =
s
s2 + ω2
for
Re(s) > 0.
It is easy to show that L[sin ωt] = ω/(s2 + ω2) using the same technique. In the
same vein,
L[cosh(at)] =
s
s2 −a2 ,
L[sinh(at)] =
a
s2 −a2 .
Example 3
Consider f(t) = tn for n ∈R. By deﬁnition,
L[tn] =
 ∞
0
e−sttn dt.
If we let x = st so that dx = s dt, we obtain
L[tn] =
 ∞
0
e−x x
s
n 1
s dx =
1
sn+1
 ∞
0
e−xxn dx = Γ(n + 1)
sn+1
.
If n is an integer, this gives L[tn] = n!/sn+1, a result that we could have obtained
directly using integration by parts.
In order to use the Laplace transform, we will need to know how to invert it, so
that we can determine f(t) from a given function F(s). For the Fourier transform,
this inversion process involves the integral formula (5.25). The inverse of a Laplace
transform is rather similar, and involves an integral in the complex s-plane. We will
return to give an outline derivation of the inversion formula, (6.5), in Section 6.4.
For the moment, we will deal with the problem of inverting Laplace transforms by
trying to recognize the function f(t) from the form of F(s). As we shall see, there
are a lot of techniques available that allow us to invert the Laplace transform in an
elementary manner.
6.1.1
The Existence of Laplace Transforms
So far, we have rather glossed over the question of when the Laplace transform
of a particular function actually exists. We have, however, discussed in passing that
the real part of s needs to be greater than some constant value in some cases. In
fact the deﬁnition of the Laplace transform is an improper integral and should be
written as
L[f(t)] = lim
N→∞
 N
0
e−stf(t) dt.

154
LAPLACE TRANSFORMS
We will now ﬁx matters by deﬁning a class of functions for which the Laplace
transform does exist. We say that a function f(t) is of exponential order on
0 ⩽t < ∞if there exist constants A and b such that |f(t)| < Aebt for t ∈[0, ∞).
Theorem 6.1 (Lerch’s theorem) A piecewise continuous function of exponential
order on [0, ∞) has a Laplace transform.
The proof of this is rather technical, and the interested reader is referred to Kreider,
Kuller, Ostberg and Perkins (1966).
Theorem 6.2 If f is of exponential order on [0, ∞), then L[f] →0 as |s| →∞.
Proof Consider the deﬁnition of L[f(t)] and its modulus
|L[f(t)]| =

 ∞
0
e−stf(t) dt
 ⩽
 ∞
0
e−stf(t)
 dt =
 ∞
0
e−st |f(t)| dt,
using the triangle inequality. If f is of exponential order,
|L[f(t)]| <
 ∞
0
e−stAebt dt =
 ∞
0
Ae(b−s)t dt
=

A
b −se(b−s)t
∞
0
= lim
Y →∞

A
b −se(b−s)Y −
A
b −s

.
Provided s > b, we therefore have
|L[f(t)]| <
A
s −b,
and hence |L[f(t)]| →0 as |s| →∞.
Conversely, if lims→∞F(s) ̸= 0, then F(s) is not the Laplace transform of a function
of exponential order. For example, s2/(s2 + 1) →1 as s →∞, and is not therefore
the Laplace transform of a function of exponential order.
In contrast, if f is not of exponential order and grows too quickly as t →∞,
the integral will not converge. For example, consider the Laplace transform of the
function et2,
L
(
et2)
= lim
N→∞
 N
0
et2e−st dt.
It should be clear that et2 grows faster than est for all s, so that the integrand
diverges, and hence that the Laplace transform of et2 does not exist.
6.2
Properties of the Laplace Transform
Theorem 6.3 (Linearity) The Laplace transform and inverse Laplace transform
are linear operators.

6.2 PROPERTIES OF THE LAPLACE TRANSFORM
155
Proof By deﬁnition,
L[αf(t) + βg(t)] =
 ∞
0
e−st (αf(t) + βg(t)) dt
= α
 ∞
0
e−stf(t) dt + β
 ∞
0
e−stg(t) dt = αL[f(t)] + βL[g(t)],
so the Laplace transform is a linear operator. Taking the inverse Laplace transform
of both sides gives
αf(t) + βg(t) = L−1 [αL[f(t)] + βL[g(t)]] .
Since we can write f(t) as L−1[L[f(t)]] and similarly for g(t), this gives
αL−1[L[f(t)]] + βL−1[L[g(t)]] = L−1 [αL[f(t)] + βL[g(t)]] .
If we now deﬁne F(s) = L[f] and G(s) = L[g], we obtain
αL−1[F] + βL−1[G] = L−1 [αF + βG] ,
so the inverse Laplace transform is also a linear operator.
This is extremely useful, since we can calculate, for example L[2t2 −t + 1] and
L[e3t + cos 2t] easily in terms of the Laplace transforms of their constituent parts.
As we mentioned earlier, the Laplace inversion formula involves complex inte-
gration, which we would prefer to avoid when possible. Often we can recognize
the constituents of an expression whose inverse Laplace transform we seek. For
example, consider L−1[1/(s2 −5s + 6)]. We can proceed by splitting this rational
function into its partial fractions representation and exploit the linearity of the
inverse Laplace transform. We have
L−1

1
s2 −5s + 6

= L−1

1
(s −2)(s −3)

= L−1

−
1
s −2 +
1
s −3

= −L−1

1
s −2

+ L−1

1
s + 3

= −e2t + e3t.
We pause here to note that the inversion of Laplace transforms using standard
forms is only possible because the operation is a bijection, that is, it is one-to-one
and onto. For every function f(t) the Laplace transform L[f(t)] is uniquely deﬁned
and vice versa. This is a direct consequence of Lerch’s theorem, 6.1.
Theorem 6.4 (First shifting theorem) If L[f(t)] = F(s) for Re(s) > b, then
L[eatf(t)] = F(s −a) for Re(s) > a + b.
Proof By deﬁnition,
L[eatf(t)] =
 ∞
0
e−steatf(t) dt =
 ∞
0
e−(s−a)tf(t) dt = F(s −a),
provided that Re(s −a) > b.

156
LAPLACE TRANSFORMS
Example 1
Consider the function f(t) = e3t cos 4t. We recall from the previous section that
L [cos 4t] =
s
s2 + 42
for Re(s) > 0.
Using Theorem 6.4,
L
!
e3t cos 4t
"
=
s −3
(s −3)2 + 42
for Re(s) > 3.
Example 2
Consider the function F(s) = s/(s2 +s+1). What is its inverse Laplace transform?
We begin by completing the square of the denominator, which gives
F(s) =
s

s + 1
2
2 + 3
4
=
s + 1
2

s + 1
2
2 + 3
4
−
1
2

s + 1
2
2 + 3
4
,
and hence
L−1

s
s2 + s + 1

= L−1

s + 1
2

s + 1
2
2 + 3
4

−L−1

1
√
3
√
3
2

s + 1
2
2 + 3
4

.
Using the ﬁrst shifting theorem then gives us
L−1

s
s2 + s + 1

= e−t/2 cos
√
3t
2
−1
√
3e−t/2 sin
√
3t
2 .
Theorem 6.5 (Second shifting theorem) If the Laplace transform of f(t) is
F(s), then the Laplace transform of the function g(t) = H(t−a)f(t−a) is e−saF(s),
where H is the Heaviside step function.
Proof By deﬁnition,
L[g(t)] =
 ∞
0
g(t)e−st dt =
 ∞
a
f(t −a)e−st dt.
By writing τ = t −a, this becomes
L[g(t)] =
 ∞
0
f(τ)e−sτe−sa dτ = e−saF(s),
since the deﬁnition of the Laplace transform, (6.1), can be written in terms of any
dummy variable of integration.
For example, to determine the inverse transform of the function e−3s/s3, we ﬁrstly
note that L[t2] = 2/s3, using Example 3 of Section 6.1. The second shifting theorem
then shows immediately that
L−1
e−3s
s3

= 1
2H(t −3)(t −3)2.

6.3 THE SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
157
6.3
The Solution of Ordinary Diﬀerential Equations Using Laplace
Transforms
In order to be able to take the Laplace transform of a diﬀerential equation, we will
need to be able to calculate the Laplace transform of the derivative of a function.
By deﬁnition,
L[f ′] =
 ∞
0
e−stf ′(t) dt.
After integrating by parts, we ﬁnd that
L[f ′] =
!
e−stf(t)
"∞
0 −
 ∞
0
−se−stf(t) dt.
At this stage, we will assume that the values of s are restricted so that e−stf(t) →0
as t →∞. This means that
L[f ′] = sL[f] −f(0).
(6.2)
A useful corollary of (6.2) is that, if
g(t) =
 t
0
f(τ)dτ,
so that, except where f(t) is discontinuous, g′(t) = f(t), we have L[f] = sL[g]−g(0).
Since g(0) = 0 by deﬁnition,
L[g] = L
 t
0
f(τ)dτ

= 1
sL[f],
and hence
L−1
1
sF(s)

=
 t
0
f(τ) dτ.
This can be useful for inverting Laplace transforms, for example,
F(s) =
1
s(s2 + ω2).
We know that L[sin ωt]/ω = 1/(s2 + ω2) so that
f(t) = L−1
1
s
1
s2 + ω2

= L−1
1
sL
 1
ω sin ωt

=
 t
0
1
ω sin ωτ dτ = 1
ω2 (1 −cos ωt) .
Let’s now try to solve the simple diﬀerential equation
dy
dt −2y = 0,
subject to the initial condition y(0) = 1.
Of course, it is trivial to solve this

158
LAPLACE TRANSFORMS
separable equation, but it is a useful illustrative example. We begin by taking the
Laplace transform of the diﬀerential equation, which gives
L
dy
dt

−2L[y] = sY (s) −y(0) −2Y (s) = 0,
where Y (s) = L[y(t)]. Using the initial condition and manipulating the equation
gives
Y (s) =
1
s −2,
which is easily inverted to give y(t) = e2t.
Many of the diﬀerential equations that we will try to solve are of second order,
so we need to determine L[f ′′]. If we introduce the function g(t) = f ′(t), (6.2)
shows that
L[g′] = sG(s) −g(0),
where G(s) = L[g] = L[f ′] = sF(s) −f(0). We conclude that
L[f ′′] = L[g′] = s2F(s) −sf(0) −f ′(0).
(6.3)
We can obtain the same result by integrating the deﬁnition of the Laplace transform
of f ′′ twice by parts. It is also straightforward to show by induction that
L[f (n)] = snF(s) −sn−1f(0) −sn−2f ′(0) −· · · −f (n−1)(0).
For example, we can now solve the diﬀerential equation
d 2y
dt2 −5dy
dt + 6y = 0,
subject to the initial conditions y(0) = 0 and y′(0) = 1 using Laplace transforms.
We ﬁnd that
s2Y (s) −sy(0) −y′(0) −5(sY (s) −y(0)) + 6Y = 0.
Using the initial conditions then shows that
Y (s) =
1
s2 −5s + 6.
In order to invert the Laplace transform we split the fraction into its constituent
partial fractions,
Y (s) =
1
s −3 −
1
s −2,
which immediately shows that
y(t) = e3t −e2t.
Let’s now consider the solution of the coupled system of equations
dy1
dt + y1 = y2,
dy2
dt −y2 = y1,
subject to the initial conditions that y1(0) = y2(0) = 1.
Although we could

6.3 THE SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
159
combine these two ﬁrst order equations to obtain a second order equation, we will
solve them directly using Laplace transforms. The transform of the equations is
sY1 −1 + Y1 = Y2,
sY2 −1 −Y2 = Y1,
where L[yj(t)] = Yj(s). The solution of these algebraic equations is
Y1(s) =
s
s2 −2,
Y2(s) = s + 2
s2 −2,
which we can easily invert by inspection to give
y1(t) = cosh
√
2t,
y2(t) = cosh
√
2t +
√
2 sinh
√
2t.
Note that the reduction of a system of diﬀerential equations to a system of algebraic
equations is the key beneﬁt of these transform methods.
Of course, for simple scalar equations, it is easier to use the standard solution
techniques described in Appendix 5.
The real power of the Laplace transform
method lies in the solution of problems for which the inhomogeneity is not of a
simple form, for example
d2y
dt2 + dy
dt = δ(t −1).
In order to take the Laplace transform of this equation, we need to know the Laplace
transform of the delta function. We can calculate this directly, as
L[δ(t −a)] =
 ∞
0
δ(t −a)e−st dt = e−sa,
so that the diﬀerential equation becomes
s2Y (s) −sy(0) −y′(0) + sY (s) −y(0) = e−s,
and hence
Y (s) = e−s + (s + 1)y(0) + y′(0)
s(s + 1)
.
Judicious use of the two shifting theorems now allows us to invert this Laplace
transform. Note that
L−1
1
s

= 1,
and, using the ﬁrst shifting theorem,
L−1

1
s + 1

= e−t.
This means that
L−1

1
s(s + 1)

= L−1
1
s −
1
s + 1

= 1 −e−t,
and, using the second shifting theorem,
L−1

e−s
s(s + 1)

= H(t −1)(1 −e−(t−1)).

160
LAPLACE TRANSFORMS
Combining all of these results, and using the linearity of the Laplace transform,
shows that
y(t) = y(0) + y′(0)(1 −e−t) + H(t −1)(1 −e−(t−1)).
6.3.1
The Convolution Theorem
The Laplace transform of the convolution of two functions plays an important
role in the solution of inhomogeneous diﬀerential equations, just as it did for Fourier
transforms. However, when using Laplace transforms, we deﬁne the convolution of
two functions as
f ∗g =
 t
0
f(τ)g(t −τ) dτ.
With this deﬁnition, if f(t) and g(t) have Laplace transforms, then
L[f ∗g] = L[f(t)]L[g(t)].
We can show this by taking the Laplace transform of the convolution integral, which
is itself a function of t, to obtain
L[f ∗g] =
 ∞
0
e−st
 t
0
f(τ)g(t −τ) dτ dt.
Since e−st is independent of τ it can be moved inside the inner integral so that we
have
L[f ∗g] =
 ∞
0
 t
0
e−stf(τ)g(t −τ) dτ dt.
We now note that the domain of integration is a triangular region delimited by the
lines τ = 0 and τ = t, as shown in Figure 6.1. If we switch the order of integration,
this becomes
L[f ∗g] =
 τ=∞
τ=0
 ∞
t=τ
e−stf(τ)g(t −τ) dt dτ.
If we now introduce the variable z = t −τ, so that dz = dt, we can transform the
inner integral into
L[f ∗g] =
 ∞
0
f(τ)
 ∞
0
e−s(z+τ)g(z) dz dτ,
and hence
L[f ∗g] =
 ∞
τ=0
f(τ)e−sτ dτ
 ∞
z=0
e−szg(z) dz = L[f] L[g].
This result is most useful in the form
L−1[F(s)G(s)] = L−1[F(s)] ∗L−1[G(s)].

6.3 THE SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
161
t
t = τ
τ
Fig. 6.1. The domain of integration of the Laplace transform of a convolution integral.
Example 1
Consider the Laplace transform F(s) = 1/(s −2)(s −3). This rational function
has two easily recognizable constituents, namely 1/(s −2) = L[e2t] and 1/(s −3) =
L[e3t], and hence
L−1

1
(s −2)(s −3)

= L−1

1
s −2

∗L−1

1
s −3

= e2t ∗e3t.
We now need to calculate the convolution integral, which is
e2t ∗e3t =
 t
τ=0
e2τe3(t−τ) dτ = e3t
 t
τ=0
e−τ dτ
= e3t !
−e−τ"t
0 = e3t 
−e−t + 1

= −e2t + e3t.
This is, of course, the same result as we obtained using partial fractions.
Example 2
Consider the Laplace transform
F(s) =
s
(s2 + 1)(s −2).

162
LAPLACE TRANSFORMS
This is the product of the Laplace transforms of cos t and e2t, and hence is the
Laplace transform of
cos t ∗e2t =
 t
0
e2τ cos(t −τ) dτ = 1
2
 t
0
e2τ 
ei(t−τ) + e−i(t−τ)
dτ
=
ei(t−τ)+2τ
2(2 + i)
+ e−i(t−τ)+2τ
2(2 −i)
t
0
= 2
5e2t −1
5 (2 cos t −sin t) .
This is somewhat easier than the partial fractions method.
Example 3: Volterra integral equations
A Volterra integral equation can be written in the standard form
y(t) = f(t) +
 t
0
y(τ)K(t −τ) dτ for t > 0.
(6.4)
The function K is called the kernel of the equation. The integral is in the form
of a convolution and, if we treat t as time, is an integral over the history of the
solution. The integral equation (6.4) can therefore be written as
y = f + y ∗K.
If we now take a Laplace transform of this equation, we obtain
Y = F + L[y ∗K] = F + Y L[K],
and hence
Y =
F
1 −L[K],
where F(s) = L[f] and Y = L[y]. For example, to solve
y(t) = 1 +
 t
0
(t −τ)y(τ) dτ,
for which f(t) = 1 and K(t) = t, we note that F = 1/s and L[K] = 1/s2, and
hence
Y (s) =
s
s2 −1.
This gives us the solution, y(t) = cosh t.
6.4
The Inversion Formula for Laplace Transforms
We have now seen that many Laplace transforms can be inverted from a knowledge
of the transforms of a few common functions along with the linearity of the Laplace
transform and the ﬁrst and second shifting theorems. However, these techniques
are often inadequate to invert Laplace transforms that arise as solutions of more
complicated problems, in particular of partial diﬀerential equations. We will need
an inversion formula.
We can derive this in an informal way using the Fourier
integral, (5.23).

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
163
Let g(t) be a function of exponential order, in particular with γ the smallest
real number such that e−γtg(t) is bounded as t →∞.
As we have seen, g(t)
therefore has a Laplace transform G(s), which exists for Re(s) > γ. We now deﬁne
h(t) = e−γtg(t)H(t). Since h(t) is bounded as t →∞, the Fourier integral (5.23)
exists, and shows that
h(t) = 1
2π
 ∞
−∞
e−ikt
 ∞
−∞
eikT h(T) dT dk,
and hence that
e−γtg(t) = 1
2π
 ∞
−∞
e−ikt
 ∞
0
e−(γ−ik)T g(T) dT dk.
If we now make the change of variable s = γ −ik, so that ds = −i dk, we ﬁnd that†
g(t) =
1
2πi
 γ+i∞
γ−i∞
est
 ∞
0
e−sT g(T) dT ds.
Finally, from the deﬁnition of the Laplace transform, we arrive at the inversion
formula, sometimes called the Bromwich inversion integral,
g(t) =
1
2πi
 γ+i∞
γ−i∞
estG(s) ds.
(6.5)
Note that the contour of integration is a vertical line in the complex s-plane. Since
G(s) is only guaranteed to exist for Re(s) > γ, this contour lies to the right of any
singularities of G(s). It is often possible to simplify (6.5) by closing the contour
of integration using a large semicircle in the left half plane. As we shall see in the
following examples, if the contour can be closed in this way, the result will depend
crucially on the residues at any poles of estG(s).
Example 1
We start with the simple case G(s) = β/(s −α). Consider the integral
I(t) =
1
2πi

C
est
β
s −α ds,
(6.6)
where the closed contour C is shown in Figure 6.2. By the residue theorem, I(t) is
equal to the sum of the residues at any poles of estG(s) enclosed by C. Let’s assume
that the contour encloses the simple pole of G(s) at s = α, and hence that the
straight boundary of C lies to the right of the pole at s = α. The residue theorem
then shows that I(t) = βeαt. As b →∞, the semicircular part of the contour
becomes large and, since |G(s)| is algebraically small when |s| ≫1, we conclude
from Jordan’s lemma (see Section A6.4) that the integral along the semicircle tends
to zero. On the straight part of the contour C, as b →∞we recover the inversion
integral (6.5), so that I(t) = g(t), and hence the inverse Laplace transform is
g(t) = βeαt, as we would expect.
† This is the point at which the derivation is informal, since we have not shown that this change
in the contour of integration is possible.

164
LAPLACE TRANSFORMS
C
s = a − ib
s = a + ib
Re(s)
Im(s)
Fig. 6.2. The contour C used to evaluate the inverse Laplace transform of β/(s −α).
Example 2
Let’s try to determine the inverse Laplace transform of G(s) = 1/(s2 + 1). In this
case G(s) has simple poles at s = ±i, since
G(s) = 1
2i

1
s −i −
1
s + i

.
Choosing the contour C as we did in Example 1, we again ﬁnd that g(t) is the sum
of the residues at the two simple poles of estG(s), and hence that, as expected,
g(t) = 1
2i

eit −e−it
= sin t.
Example 3
We now consider a Laplace transform that has a nonsimple pole, G(s) = 1/(s−α)3.
The simplest way to calculate the residue of estG(s) at s = α is to note that
est
(s −α)3 =
eαt
(s −α)3 e(s−α)t =
eαt
(s −α)3

1 + (s −α)t + 1
2(s −α)2t2 + · · ·

,
and hence that g(t) = 1
2t2eαt. We can check this result by noting that L[t2] =
Γ(3)/s3 = 2/s3 and using the ﬁrst shifting theorem.
Example 4
Consider the inverse Laplace transform of G(s) = e−αs1/2/s. Since G(s) contains
a fractional power, s1/2, the point s = 0 is a branch point, and the deﬁnition of

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
165
G(s) is incomplete.
We need to introduce a branch cut in order to make G(s)
single-valued. It is convenient to place the branch cut along the negative real axis,
so that, if s = |s|eiθ with −π < θ < π, s1/2 = +
#
|s|eiθ/2 and the real part of s1/2
is positive. We cannot now integrate estG(s) along any contour that crosses the
negative real axis, such as C. Because of this, we use CB, which avoids the branch
cut, as shown in Figure 6.3. This contour also includes a small circle around the
origin of radius ϵ, since G(s) has a simple pole there, and is often referred to as a
keyhole contour.
F
B
E
A
Re(s)
δ
Im(s)
C
D
s = a − ib
s = a + ib
Fig. 6.3. The keyhole inversion contour, CB, used for inverting Laplace transforms with
a branch cut along the negative real axis.
Since estG(s) is analytic within CB, the integral around CB is zero, by Cauchy’s
theorem (see Appendix 6). On the circular arcs AB and EF, G(s) →0 exponen-
tially fast as b →∞, since s1/2 has positive real part, and hence the contributions to
the integral from these arcs tend to zero. As before, the integral along the straight
contour AF tends to g(t) as b →∞, by the inversion formula (6.5). We conclude
that
g(t) = −lim
b→∞
ϵ→0
1
2πi

BC
+

CD
+

DE
	 est−αs1/2
s
ds.
(6.7)
Let’s consider the contributions from the lines BC and DE. We can parameterize
these lines as s = xe±iπ respectively. In this way, we ensure that we use the correct
value of s1/2 on either side of the branch cut. Along BC, s1/2 = x1/2eiπ/2 = ix1/2,

166
LAPLACE TRANSFORMS
and hence, in the limit as ϵ →0 and b →∞

BC
est−αs1/2
s
ds =
 0
∞
e−xt−αix1/2
x
dx.
Similarly

DE
est−αs1/2
s
ds =
 ∞
0
e−xt+αix1/2
x
dx,
and hence

BC
+

DE
	 est−αs1/2
s
ds = 2i
 ∞
0
e−xt sin αx1/2
x
dx.
(6.8)
In order to calculate this integral, we note that
I =
 ∞
0
e−xt sin αx1/2
x
dx =
 ∞
0
e−xt
x
∞

n=1
(αx1/2)2n−1
(2n −1)!
dx,
using the Taylor series expansion of sin αx1/2. Since this series is uniformly con-
vergent for all x, we can interchange the order of summation and integration, so
that
I =
∞

n=1
α2n−1
(2n −1)!
 ∞
0
e−xtxn−3/2 dx =
∞

n=1
α2n−1
(2n −1)!
1
tn−1/2
 ∞
0
e−XXn−3/2 dX
=
∞

n=1
 α
t1/2
2n−1 Γ

n −1
2

(2n −1)! ,
using the change of variable X = xt. Now, since
Γ

n −1
2

=

n −3
2

Γ

n −3
2

=

n −3
2
 
n −5
2

. . . 1
2Γ
1
2

=
1
2n−1 (2n −3)(2n −5) . . . 3 · 1 · √π,
we ﬁnd that
Γ

n −1
2

(2n −1)! =
√π
2n−1
(2n −3)(2n −5) . . . 3 · 1
(2n −1)(2n −2) . . . 3 · 2 · 1
=
√π
2n−1(2n −1)
1
(2n −2)(2n −4) . . . 4 · 2 =
√π
22(n−1)(2n −1)(n −1)!,
and hence
I = 2
∞

n=1

α
2t1/2
2n−1
√π
(2n −1)(n −1)! = 2√π
 α/2t1/2
0
∞

n=1
(s2)n−1
(n −1)! ds
= 2√π
 α/2t1/2
0
e−s2 ds = π erf

α
2t1/2

,

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
167
where erf(x) is the error function, deﬁned by
erf(x) =
2
√π
 x
0
e−q2 dq.
We can parameterize the small circle CD using s = ϵeiθ, where θ runs from π to
−π. On this curve we have s1/2 = ϵ1/2eiθ/2, so that

CD
est−αs1/2
s
ds =
 −π
π
eϵteiθ−αϵ1/2eiθ/2
ϵeiθ
iϵeiθ dθ =
 −π
π
eϵteiθ−αϵ1/2eiθ/2i dθ.
As ϵ →0 we therefore have

CD
est−αs1/2
s
ds =
 −π
π
i dθ = −2πi.
(6.9)
If we now use (6.8) and (6.9) in (6.7), we conclude that
L−1

e−αs1/2
s

= 1 −erf

α
2t1/2

= erfc

α
2t1/2

,
where erfc(x) is the complementary error function, deﬁned by
erfc(x) =
2
√π
 ∞
x
e−q2 dq.
Laplace transforms of the error function and complementary error function arise
very frequently when solving diﬀusion problems, as we shall see in the following
example.
Example 5: Flow due to an impulsively-started ﬂat plate
Let’s consider the two-dimensional ﬂow of a semi-inﬁnite expanse of viscous ﬂuid
caused by the sudden motion of a ﬂat plate in its own plane. We will use Cartesian
coordinates with the x-axis lying in the plane of the plate and the y-axis pointing
into the semi-inﬁnite body of ﬂuid.
This is a uni-directional ﬂow with velocity
u(x, y, t) in the x-direction only and associated scalar pressure ﬁeld p(x, y, t). The
continuity equation, ux + vy = 0, which we derived in Chapter 2, shows that
the streamwise velocity, u, is solely a function of y and t. We now consider the
streamwise momentum within a small element of ﬂuid, as shown in Figure 6.4.
Balancing forces in the x-direction, and taking the limit δx, δy, δt →0, we ﬁnd
that
ρDu
Dt = ρ
∂u
∂t + u∂u
∂x

= −∂p
∂x + ∂τ
∂y ,
where ρ is the density and τ is the shear stress. Here we have used the convective
derivative, which we derived in Chapter 2. For a one-dimensional ﬂow, it is found
experimentally that τ = µ∂u/∂y, where µ is the dynamic viscosity (see Acheson,
1990). To be consistent with u = u(y, t), we now insist that ∂/∂x ≡0. This reduces
the x-momentum equation to ut = νuyy, where we have introduced the quantity
ν = µ/ρ, the kinematic viscosity. Flows with high values of ν are extremely

168
LAPLACE TRANSFORMS
τ (x, y, t)
τ (x, y + δy, t)
p (x, y, t)
p (x + δx, y, t)
Fig. 6.4. The x-momentum balance on a small element of ﬂuid.
viscous, for example tar, lava and mucus, whilst those with low viscosity include
air, water and inert gases. For example, the kinematic viscosity of lava is around
10 m2 s−1, whilst the values for air and water at room temperature and pressure,
10−6 m2 s−1 and 1.5 × 10−5 m2 s−1, respectively, are very similar.
Our ﬁnal initial–boundary value problem is
∂u
∂t = ν ∂2u
∂y2 ,
(6.10)
to be solved subject to
u = 0 when t = 0 for y > 0,
(6.11)
u = U,
at y = 0 for t > 0,
(6.12)
u →0,
as y →∞for t > 0.
(6.13)
This is known as Rayleigh’s problem. Equation (6.10) states that the only pro-
cess involved in this ﬂow is the diﬀusion of x-momentum into the bulk of the ﬂuid.
Of course, this initial–boundary value problem can also be thought of as modelling
other diﬀusive systems, for example, a semi-inﬁnite bar of metal, insulated along
its sides, suddenly heated up at one end.
In order to solve this initial–boundary value problem, we will take a Laplace
transform with respect to time, so that
L[u(y, t)] = U(y, s) =
 ∞
0
e−stu(y, t) dt.

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
169
Taking two derivatives of this deﬁnition with respect to y shows that
L
∂2u
∂y2

= ∂2U
∂y2 .
In general, y-derivatives are not aﬀected by a Laplace transform with respect to t.
After using (6.2) to determine the Laplace transform of ∂u/∂t, (6.10) becomes
sU = ν ∂2U
∂y2 .
(6.14)
The transform variable only appears as a parameter in this equation, which there-
fore has the solution
U(y, s) = A(s)es1/2y/ν1/2 + B(s)e−s1/2y/ν1/2.
Since u →0 as y →∞, we must also have U →0 as y →∞, so that A(s) = 0. We
now need to transform the boundary condition (6.12), which gives
U(0, s) =
 ∞
0
u(0, t)e−st dt =
 ∞
0
Ue−st dt = U
s ,
and hence B(s) = U/s. We conclude that
U(y, s) = U
s e−s1/2y/ν1/2.
From the result of the previous example, we ﬁnd that
u(y, t) = U erfc

y
2
√
νt

.
Some typical velocity proﬁles are shown in Figure 6.5.
These are easy to plot,
since the error and complementary error functions are available as erf and erfc in
MATLAB.
We can also consider what happens when the velocity of the plate is a function
of time. All we need to change is the boundary condition (6.12), which becomes
u = Uf(t),
at y = 0 for t > 0.
(6.15)
The Laplace transform of this condition is U(0, s) = UF(s), where F(s) is the
Laplace transform of f(t). Now B(s) = F(s), and hence
U(y, s) = UF(s)e−s1/2y/ν1/2 = UsF(s)e−s1/2y/ν1/2
s
.
(6.16)
Using (6.2), we can see that sF(s) = L[f ′(t)] + f(0), and hence
U(y, s) = U {L[f ′(t)] + f(0)} L

erfc

y
2
√
νt

.
(6.17)
We can now invert this Laplace transform using the convolution theorem, to give
u(y, t) = U
 t
τ=0
{f ′(t −τ) + f(0)} erfc

y
2√ντ

dτ

.
(6.18)

170
LAPLACE TRANSFORMS
Exact solutions of the equations that govern the ﬂow of a viscous ﬂuid, the Navier–
Stokes equations, are very useful for verifying that numerical solution methods
are working correctly. Their stability can also be studied more easily than for ﬂows
where no analytical solution is available.
By evaluating (6.18) numerically, we can calculate the ﬂow proﬁles for any f(t).
A MATLAB function that evaluates u(y, t), for any speciﬁed f(t), is
'
&
$
%
function rayleigh(yout,tout)
for t = tout
u = [];
for y = yout
u = [u quadl(@integrand,0,t,10^-4,0,t,y)];
end
plot(u,yout), xlabel(’u’), ylabel(’y’)
title(strcat(’t = ’,num2str(t))), Xlim([0 1])
pause(0.5)
end
function integrand = integrand(tau,t,y)
nu = 1;
df = f(t-tau);
integrand = df.*erfc(y/2./(eps+sqrt(nu*tau)));
function f = f(t)
df = 2*cos(2*t); f0 = 0; f = df + f0;
This uses the MATLAB function quadl to evaluate the integral correct to four
decimal places, and then plots the solution at the points given in yout at the
times given in tout. In the example shown, f(t) = sin 2t, which corresponds to
an oscillating plate, started from rest. Note that we add the inbuilt small quantity
eps to the denominator of the argument of the error function to avoid MATLAB
producing lots of irritating division by zero warnings. In fact, MATLAB is able to
evaluate erfc(y/0) = erfc(Inf) = 0 for y positive.
As a ﬁnal example, let’s consider what we would do if we did not have access
to a computer, but wanted to know what happens when the plate oscillates, with
f(t) = sin ωt. Since F(s) = ω/(s2 + ω2), (6.16), along with the inversion formula
(6.5), gives
u(y, t) =
1
2πi
 γ+i∞
γ−i∞
ωest−y√
s/ν
s2 + ω2
ds,
(6.19)
where γ > 0, since the integrand has poles at s = ±iω. Although (6.18) is the
most convenient form to use for numerical integration, (6.19) gives us the most
helpful way of approaching this speciﬁc problem. We proceed as we did in Example
4, evaluating the integral on the contour CB shown in Figure 6.3. The analysis

6.4 THE INVERSION FORMULA FOR LAPLACE TRANSFORMS
171
Fig. 6.5. The ﬂow due to an impulsively-started ﬂat plate when U = 1 and ν = 1.
proceeds exactly as it did in Example 4, except that now the integral around CB
is equal to the sum of the residues of the integrand at the poles at s = ±iω. In
addition, the integral around the small circle CD tends to zero as ϵ →0. After
taking all of the contributions into account (see Exercise 6.8), we ﬁnd that
u(y, t) = ω
π
 ∞
0
e−σt sin
# σ
ν y

σ2 + ω2
dσ + e−√ω
2ν y sin

ωt −
* ω
2ν y
	
.
(6.20)
The ﬁrst term, which comes from the branch cut integrals, tends to zero as t →∞,
and represents the initial transient that arises because the plate starts at rest. The
second term, which comes from the residues at the poles, represents the oscillatory
motion of the ﬂuid, whose phase changes with y and whose amplitude decays ex-
ponentially fast with y. This second term therefore gives the large time behaviour
of the ﬂuid. You can see what this solution looks like by running the MATLAB
function that we gave above.

172
LAPLACE TRANSFORMS
Exercises
6.1
Find the Laplace transforms of the functions, (a) t(t+1)(t+2), (b) sinh(ωt),
(c) cosh(ωt), (d) e−t sin t. Specify the values of s for which each transform
exists.
6.2
Find the inverse Laplace transforms of the functions, (a) 1/(s2 −3s + 2),
(b) 1/s3(s+1), (c) 1/(2s2−s+1), (d) 1/s2(s+1)2, (e) (2s+3)/(s2−4s+20),
(f) 1/(s4 + 9s2), (g) (2s −4)/(s −1)4, (h) (s2 + 1)/(s3 −s2 + 2s −2),
(i) s3/(s + 3)2(s + 2)2.
6.3
Using Laplace transforms, solve the initial value problems
(a) d 2y
dt2 + 9y = 18t, subject to y(0) = 0, y
π
2

= 0,
(b) d 2y
dt2 −4dy
dt + 3y = f(t), subject to y(0) = 1 and y′(0) = 0,
(c) dx
dt + x + 2y = t, d 2x
dt2 + 5x + 3dy
dt = 0,
subject to x′(0) = x(0) = y(0) = 0,
(d)
 d 2
dt2 −4 d
dt + 4

y = f(t), subject to y(0) = −2, y′(0) = 1, with
f(t) =
 t
if
0 ⩽t ⩽3,
t + 2
if
t ⩾3.
(e) dx
dt + x + dy
dt = 0, dx
dt −x + 2dy
dt = e−t, subject to x(0) = y(0) = 1.
(f) d 2x
dt2 = −2x + y,
d 2y
dt2 = x −2y, subject to the initial conditions
x(0) = y(0) = 1 and x′(0) = y′(0) = 0.
6.4
Using Laplace transforms, solve the integral and integro-diﬀerential equa-
tions
(a) y(t) = t + 1
6
 t
0
y(τ)(t −τ)3 dτ,
(b)
 t
0
y(τ) cos(t −τ) dτ = dy
dt , subject to y(0) = 1.
6.5
Show that L[tf(t)] = −dF/ds, where F(s) = L[f(t)].
Hence solve the
initial value problem
d 2x
dt2 + 2tdx
dt −4x = 1,
subject to x(0) = x′(0) = 0.
6.6
Determine the inverse Laplace transform of
F(s) =
s
(s2 + 1)(s −2)
using (a) the inversion formula (6.5), and (b) the residue theorem.

EXERCISES
173
6.7
Find the inverse Laplace transform of
F(s) =
1
(s −1)(s2 + 1),
by (a) expressing F(s) as partial fractions and inverting the constituent
parts and (b) using the convolution theorem.
6.8
Evaluate the integral (6.19), and hence verify that (6.20) holds.
6.9
Consider the solution of the diﬀusion equation
∂φ
∂t = D∂2φ
∂y2
for t > 0 and y ∈[0, L], subject to the initial conditions φ(y, 0) = 0 and
the boundary conditions φ(0, t) = φ(L, t) = 1.
Show that the Laplace
transform of the solution is
Φ(y, s) =
1
s (1 + e−κL)

e−κy + e−κ(L−y)
,
where κ =
#
s/D. By expanding the denominator, show that the solution
can be written as
φ(y, t) =
∞

j=0
(−1)j

erfc
y + jL
2
√
Dt

−erfc
(j + 1)L −y
2
√
Dt

.
6.10
(a) Show that a small displacement, y(x, t), of a uniform string with
constant tension T and line density ρ subject to a uniform gravita-
tional acceleration, g, downwards satisﬁes
∂2y
∂t2 = c2 ∂2y
∂x2 −g,
where c =
#
T/ρ (see Section 3.9.1).
(b) Such a string is semi-inﬁnite in extent, and has y = yt = 0 for x ⩾0
when t = 0. Use Laplace transforms to determine the solution when
the string is ﬁxed at x = 0, satisﬁes yx →0 as x →∞and is allowed
to fall under gravity. Sketch the solution, and explain how and why
the qualitative form of the solution depends upon the sign of x −ct.
6.11
Project The voltage, v(t), in an RLC circuit with implied current i(t) is
given by the solution of the diﬀerential equation
C d 2v
dt2 + 1
R
dv
dt + v
L = di
dt,
where C is the capacitance, R the resistance and L the inductance.
(a) Solve this equation using Laplace transforms when i(t) = H(t−1)−
H(t).
(b) Write a MATLAB script that inverts the Laplace transform of this
equation directly, so that it works when i(t) is supplied by an input
routine input.m.

174
LAPLACE TRANSFORMS
(c) Compare your results for various values of the parameters R, L and
C.
(d) Extend this further to consider what happens when the implied
current is periodic.

CHAPTER SEVEN
Classiﬁcation, Properties and Complex Variable
Methods for Second Order Partial Diﬀerential
Equations
In this chapter, we will consider a class of partial diﬀerential equations, of which
Laplace’s equation, the diﬀusion equation and the wave equation are the canonical
examples. We will discuss what sort of boundary conditions are appropriate, and
why solutions of these equations have their distinctive properties.
We will also
demonstrate that complex variable methods are powerful tools for solving bound-
ary value problems for Laplace’s equation, with particular application to certain
problems in ﬂuid mechanics.
7.1
Classiﬁcation and Properties of Linear, Second Order Partial
Diﬀerential Equations in Two Independent Variables
Consider a second order linear partial diﬀerential equation in two independent
variables, which we can write as
a(x, y)∂2φ
∂x2 + 2b(x, y) ∂2φ
∂x∂y + c(x, y)∂2φ
∂y2
+d1(x, y)∂φ
∂x + d2(x, y)∂φ
∂y + d3(x, y)φ = f(x, y).
(7.1)
Equations of this type arise frequently in mathematical modelling, as we have al-
ready seen. We will show that the ﬁrst three terms of (7.1) allow us to classify the
equation into one of three distinct types: elliptic, for example Laplace’s equation,
(2.13), parabolic, for example the diﬀusion equation, (2.12), or hyperbolic, for
example the wave equation, (3.39). Each of these types of equation has distinctive
properties. These mathematical properties are related to the physical properties of
the system that the equation models.
7.1.1
Classiﬁcation
We would like to know about properties of (7.1) that are unchanged by an
invertible change of coordinates, since these must be of fundamental signiﬁcance,
and not just a result of our choice of coordinate system. We can write this change

176
CLASSIFICATION AND COMPLEX VARIABLE METHODS
of coordinates as†
(x, y) →(ξ (x, y) , η (x, y)) ,
with ∂(ξ, η)
∂(x, y) ̸= 0.
(7.2)
In particular, if (7.1) is a model for a physical system, a change of coordinates
should not aﬀect its qualitative behaviour. Writing φ(x, y) ≡ψ(ξ, η) and using
subscripts to denote partial derivatives, we ﬁnd that
φx = ξxψξ + ηxψη,
φxx = ξ2
xψξξ + 2ξxηxψξη + η2
xψηη + ξxxψξ + ηxxψη,
and similarly for the other derivatives. Substituting these into (7.1) gives us
Aψξξ + 2Bψξη + Cψηη + b1(ξ, η)ψξ + b2(ξ, η)ψη + b3(ξ, η)ψ = g (ξ, η) ,
(7.3)
where
A = aξ2
x + 2bξxξy + cξ2
y,
B = aξxηx + b (ηxξy + ξxηy) + cξyηy,
(7.4)
C = aη2
x + 2bηxηy + cη2
y.
We do not need to consider the other coeﬃcient functions here. We can express
(7.4) in a concise matrix form as
 A
B
B
C

=
 ξx
ηx
ξy
ηy
  a
b
b
c
  ξx
ξy
ηx
ηy

,
(7.5)
which shows that
det
 A
B
B
C

= det
 a
b
b
c
  ∂(ξ, η)
∂(x, y)
2
.
(7.6)
This shows that the sign of ac−b2 is independent of the choice of coordinate system,
which allows us to classify the equation.
— An elliptic equation has ac > b2, for example, Laplace’s equation
∂2φ
∂x2 + ∂2φ
∂y2 = 0.
— A parabolic equation has ac = b2, for example, the diﬀusion equation
K ∂2φ
∂x2 −∂φ
∂y = 0.
† Note that
∂(ξ, η)
∂(x, y) = det

ξx
ξy
ηx
ηy

is the Jacobian of the transformation. The Jacobian is the factor by which the transformation
changes inﬁnitesimal volume elements.

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
177
— A hyperbolic equation has ac < b2, for example, the wave equation
1
c2
∂2φ
∂x2 −∂2φ
∂y2 = 0.
Note that, although these three examples are of the given type throughout the
(x, y)-plane, equations of mixed type are possible. For example, Tricomi’s equa-
tion, φxx = xφyy, is elliptic for x < 0 and hyperbolic for x > 0.
7.1.2
Canonical Forms
Any equation of the form given by (7.1) can be written in canonical form
by choosing the canonical coordinate system, in terms of which the second
derivatives appear in the simplest possible way.
Hyperbolic Equations: ac < b2
In this case, we can factorize A and C to give
A = aξ2
x + 2bξxξy + cξ2
y = (p1ξx + q1ξy) (p2ξx + q2ξy) ,
C = aη2
x + 2bηxηy + cη2
y = (p1ηx + q1ηy) (p2ηx + q2ηy) ,
with the two factors not multiples of each other. We can then choose ξ and η so
that
p1ξx + q1ξy = p2ηx + q2ηy = 0,
and hence A = C = 0. This means that
ξ is constant on curves with dy
dx = q1
p1
,
η is constant on curves with dy
dx = q2
p2
.
We can therefore write p1dy −q1dx = p2dy −q2dx = 0, and hence
(p1dy −q1dx) (p2dy −q2dx) = 0,
which gives
a dy2 −2b dx dy + c dx2 = 0.
(7.7)
As we shall see, this is the easiest equation to use to determine (ξ, η). We call
(ξ, η) the characteristic coordinate system, in terms of which (7.1) takes its
canonical form
ψξη + b1(ξ, η)ψξ + b2(ξ, η)ψη + b3(ξ, η)ψ = g (ξ, η) .
(7.8)
The curves where ξ is constant and the curves where η is constant are called the
characteristic curves, or simply characteristics.
As we shall see, it is the
existence, or nonexistence, of characteristic curves for the three types of equation
that determines the distinctive properties of their solutions.
We discussed the
reduction of the wave equation to this canonical form in Section 3.9.1.
As a less trivial example, consider the hyperbolic equation
φxx −sech4x φyy = 0.
(7.9)

178
CLASSIFICATION AND COMPLEX VARIABLE METHODS
Equation (7.7) shows that the characteristics are given by
dy2 −sech4x dx2 =

dy + sech2x dx
 
dy −sech2x dx

= 0,
and hence
dy
dx = ±sech2x.
The characteristics are therefore y ± tanh x = constant, and the characteristic
coordinates are ξ = y + tanh x, η = y −tanh x. On writing (7.9) in terms of these
variables, with φ(x, y) = ψ(ξ, η), we ﬁnd that its canonical form is
ψξη = (η −ξ) (ψξ −ψη)
4 −(ξ −η)2
,
(7.10)
in the domain (η −ξ)2 < 4.
Parabolic Equations: ac = b2
In this case,
A = aξ2
x + 2bξxξy + cξ2
y = (pξx + qξy)2 ,
C = aη2
x + 2bηxηy + cη2
y = (pηx + qηy)2 ,
so we can only construct one set of characteristic curves. We therefore take ξ to be
constant on the curves p dy −q dx = 0. This gives us A = 0 and, since AC = B2,
B = 0. For any set of curves where η is constant that is never parallel to the
characteristics, C does not vanish, and the canonical form is
ψηη + b1(ξ, η)ψξ + b2(ξ, η)ψη + b3(ξ, η)ψ = g (ξ, η) .
(7.11)
We can now see that the diﬀusion equation is in canonical form.
As a further example, consider the parabolic equation
ψxx + 2cosec y φxy + cosec2y φyy = 0.
(7.12)
The characteristic curves satisfy
dy2 −2cosec y dx dy + cosec2y dx2 = (dy −cosec y dx)2 = 0,
and hence
dy
dx = cosec y.
The characteristic curves are therefore given by x + cos y = constant, and we can
take ξ = x + cos y as the characteristic coordinate.
A suitable choice for the
other coordinate is η = y.
On writing (7.12) in terms of these variables, with
φ(x, y) = ψ(ξ, η), we ﬁnd that its canonical form is
ψηη = sin2 η cos η ψξ,
(7.13)
in the whole (ξ, η)-plane.

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
179
Elliptic Equations: ac > b2
In this case, we can make neither A nor C zero, since no real characteristic curves
exist. Instead, we can simplify by making A = C and B = 0, so that the second
derivatives form the Laplacian, ∇2ψ, and the canonical form is
ψξξ + ψηη + b1(ξ, η)ψξ + b2(ξ, η)ψη + b3(ξ, η)ψ = g (ξ, η) .
(7.14)
Clearly, Laplace’s equation is in canonical form.
In order to proceed, we must solve
A −C = a

ξ2
x −η2
x

+ 2b (ξxξy −ηxηy) + c

ξ2
y −η2
y

= 0,
B = aξxηx + b (ηxξy + ξxηy) + cξyηy = 0.
We can do this by deﬁning χ = ξ + iη, and noting that these two equations form
the real and imaginary parts of
aχ2
x + 2bχxχy + cχ2
y = 0,
and hence that
χx
χy
= −b ± i
√
ac −b2
a
.
(7.15)
Now χ is constant on curves given by χy dy + χx dx = 0, and hence, from (7.15),
on
dy
dx = b ∓i
√
ac −b2
a
.
(7.16)
By solving (7.16) we can deduce ξ and η. For example, consider the elliptic equation
φxx + sech4x φyy = 0.
(7.17)
In this case, χ = ξ + iη is constant on the curves given by
dy
dx = ±i sech2x,
and hence y ∓i tanh x = constant. We can therefore take χ = y + i tanh x, and
hence ξ = y, η = tanh x.
On writing (7.17) in terms of these variables, with
φ(x, y) = ψ(ξ, η), we ﬁnd that its canonical form is
ψξξ + ψηη =
2η
1 −η2 ψη,
(7.18)
in the domain |η| < 1.
We can now describe some of the properties of the three diﬀerent types of equa-
tion. For more detailed information, the reader is referred to Kevorkian (1990) and
Carrier and Pearson (1988).

180
CLASSIFICATION AND COMPLEX VARIABLE METHODS
7.1.3
Properties of Hyperbolic Equations
Hyperbolic equations are distinguished by the existence of two sets of charac-
teristics. This allows us to establish two key properties. Firstly, characteristics
are carriers of information. For the wave equation, we saw in Section 3.9.1 that
solutions propagate at speeds ±c, which corresponds to propagation on the char-
acteristic curves, x ± ct = constant. Indeed, the use of the independent variable
t, time, instead of y suggests that the equation has an evolutionary nature. More
speciﬁcally, consider the Cauchy problem for a hyperbolic equation of the form
(7.1). In a Cauchy problem, we specify a curve C in the (x, y)-plane upon which we
know the Cauchy data; φ and the derivative of φ normal to C, ∂φ/∂n. The initial
value problem for the wave equation that we studied in Section 3.9.1 is a Cauchy
problem. Does this problem have a solution in general? Let’s assume that φ and
∂φ/∂n on C can be expanded as power series, and that the functions that appear
as coeﬃcients in (7.1) can also be expanded as power series in the neighbourhood
of C. We can then write
φ(ξ, η) = φ(ξ, 0) + η ∂φ
∂η (ξ, 0) + 1
2!η2 ∂2φ
∂η2 (ξ, 0) + · · · ,
where the orthogonal coordinate system (ξ, η) is set up so that η = 0 is the curve
C, which is then parameterized by ξ. As we have seen, we can write (7.1) in terms
of this new coordinate system as (7.3). We know φ(ξ, 0) and the normal derivative,
∂φ/∂η(ξ, 0), and, provided that the coeﬃcient of ∂2φ/∂η2 does not vanish on C,
we can deduce ∂2φ/∂η2(ξ, 0) from (7.3).
When does the coeﬃcient of ∂2φ/∂η2
vanish on C? Precisely when C is a characteristic curve. Provided that C is not
a characteristic curve, we can also deduce higher derivatives from derivatives of
(7.3), and hence construct a power series solution, valid in the neighbourhood of
C. The formal statement of this informally presented procedure is the Cauchy–
Kowalewski theorem, a local existence theorem (see Garabedian, 1964, for a
formal statement and proof). The eﬀects of the initial data propagate into the
(x, y)-plane on the characteristics, so it is inconsistent to specify initial data upon a
characteristic curve. In addition, the solution at any point (x, y) is only dependent
on the initial conditions that lie between the two characteristics through (x, y),
as shown in Figure 7.1. For the wave equation, this is immediately obvious from
d’Alembert’s solution, (3.43).
Secondly, discontinuities in the second derivative of φ can propagate on charac-
teristic curves. To see this, consider a curve C in the (x, y)-plane, not necessarily
a characteristic, given by ξ(x, y) = ξ0. Suppose that φξξ is not continuous on C,
but that φ satisﬁes the hyperbolic equation on either side of C. Can we choose ξ
in such a way that the equation is satisﬁed, even though φξξ

ξ+
0 , η

̸= φξξ

ξ−
0 , η

?
If we evaluate the equation on either side of the curve and subtract, we ﬁnd that
A(ξ0, η)

φξξ

ξ+
0 , η

−φξξ

ξ−
0 , η

= 0.
We can therefore satisfy the equation if A(ξ0, η) = 0, and hence C is a characteristic
curve. We conclude that discontinuities in the second derivative can propagate on
characteristic curves. In general, φ and its ﬁrst derivatives are continuous, but for

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
181
y
x
Characteristics
(x, y)
Initial line
C
Fig. 7.1. The domain of dependence of the solution of a hyperbolic equation. The solution
at (x, y) depends only upon the initial data on the marked part of the initial line, C.
the wave equation, in which only second derivatives appear, discontinuities in φ
and its derivatives can also propagate on characteristics, as shown in Figure 3.10.
7.1.4
Properties of Elliptic Equations
For elliptic equations, A and C are never zero, and there are no characteristics.
Solutions are therefore inﬁnitely-diﬀerentiable. Moreover, there is no timelike vari-
able; for example, Laplace’s equation is written in terms of the spatial variables, x
and y. For physical problems that can be modelled using elliptic equations, bound-
ary value problems, rather than initial value problems, usually arise naturally; for
example the steady state diﬀusion problem discussed in Section 2.6.1. In fact, the
solution of the Cauchy problem for elliptic equations does not depend continuously
on the initial conditions, and is therefore not a sensible representation of any real,
physical problem.† We can easily demonstrate this for Laplace’s equation.
Consider the Cauchy, or initial value, problem
φxx + φtt = 0 for t ⩾0, −∞< x < ∞,
(7.19)
subject to
φ(x, 0) = φ0(x),
φt(x, 0) = v0(x).
(7.20)
† We say that the problem is ill-posed.

182
CLASSIFICATION AND COMPLEX VARIABLE METHODS
If φ0 = 0 and v0 = 1
λ sin λx, a solution is φ = ˆφ(x, t), where
ˆφ(x, t) = 1
λ2 sin λx sinh λt.
As λ →∞, ˆφt(x, 0) = 1
λ sin λx →0. However, for t > 0, max |ˆφ| =
1
λ2 sinh λt →∞
as λ →∞. The smaller the initial condition, the larger the solution. Now, let
Φ(x, t) be the general solution of (7.19) subject to (7.20). Consider the function
ˆΦ = Φ + ˆφ. The larger the value of λ, the closer ˆΦt(x, 0) is to v0(x). However, |ˆΦ|
increases without bound for t > 0 as λ increases. An arbitrarily small change in
the boundary data produces an arbitrarily large change in the solution.
Laplace’s equation is the simplest possible elliptic equation, and, as we have
seen, arises in many diﬀerent physical contexts. Let’s consider some more of its
properties.
Theorem 7.1 (The maximum principle for Laplace’s equation) Let D be
a connected, bounded, open set in two or three dimensions, and φ a solution of
Laplace’s equation in D. Then φ attains its maximum and minimum values on
∂D, the boundary of D, and nowhere in the interior of D, unless φ is a constant.
Proof The idea of the proof is straightforward. For example in two dimensions, at
a local maximum in the interior of D, φx = φy = 0, φxx ⩽0 and φyy ⩽0. At a
maximum with φxx < 0 or φyy < 0, we have ∇2φ = φxx + φyy < 0, and φ cannot
be a solution of Laplace’s equation. However, it is possible to have φxx = φyy = 0
at an interior local maximum, for example, if the Taylor series expansion close to
(x, y) = (x0, y0) is φ = (x −x0)4 + (y −y0)4 + · · · . Although this is clearly not a
solution of Laplace’s equation, we do need to do a little work in order to exclude
this possibility in general.
Let ψ = φ + ϵ|x|2, with ϵ > 0 and |x|2 = x2 + y2 in two dimensions and
|x|2 = x2 + y2 + z2 in three dimensions. We then have
∇2ψ = ∇2φ + ϵ∇2|x|2 = kϵ > 0,
where k = 4 in two dimensions and k = 6 in three dimensions. Since ∇2ψ ⩽0 at an
interior local maximum, we conclude that ψ has no local maximum in the interior
of D, and hence that ψ must attain its maximum value on the boundary of D, say
at x = x0. But, by deﬁnition
φ(x) ⩽ψ(x) ⩽ψ(x0) = φ(x0) + ϵ|x0|2 ⩽max
∂D φ + ϵl2,
where l is the greatest distance from ∂D to the origin. Since this is true for all
ϵ > 0, we can make ϵ arbitrarily small, and conclude that
φ(x) ⩽max
∂D φ for all x ∈D.
Similarly, −φ, which also satisﬁes Laplace’s equation, attains its maximum value
on ∂D, and hence φ attains its minimum value on ∂D.

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
183
We can use the maximum principle to show that the Dirichlet problem for
Laplace’s equation has a unique solution.
Theorem 7.2 The Dirichlet problem
∇2φ = 0 for x ∈D,
(7.21)
with
φ(x) = f(x) on ∂D,
(7.22)
has a unique solution.
Proof Let φ1 and φ2 be solutions of (7.21) subject to (7.22). Then ψ = φ1 −φ2
satisﬁes
∇2ψ = 0 for x ∈D,
(7.23)
with
ψ(x) = 0 on ∂D.
(7.24)
By Theorem 7.1, ψ attains its maximum and minimum values of ∂D, so that
0 ⩽ψ(x) ⩽0 for x ∈D, and hence ψ ≡0. This means that φ1 = φ2, and hence
that there is unique solution.
The maximum principle can also be used to study other elliptic equations. For
example, consider the boundary value problem
∇2φ = −1 for x ∈X =

(x, y)

|x|
a + |y|
b < 1
	
,
(7.25)
with a > b > 0, subject to
φ = 0 on ∂X.
(7.26)
How big is φ(0, 0)? We can obtain some bounds on this quantity by transforming
(7.25) into Laplace’s equation. We deﬁne ψ = (x2 + y2)/4, so that ∇2ψ = 1, and
let ˆφ = φ + ψ, so that ∇2 ˆφ = 0. We can also see that ˆφ = ψ on ∂X, and that
ˆφ(0, 0) = φ(0, 0). Theorem 7.1 then shows that
min
∂X ψ ⩽φ(0, 0) ⩽max
∂X ψ.
We can therefore bound φ(0, 0) using the maximum and minimum values of ψ =
(x2 + y2)/4 on the boundary of X. The maximum value is clearly a2/4. Since X
is symmetric about both coordinate axes, we can ﬁnd the minimum value of ψ by
determining the value of the radius r0 for which the circle x2 +y2 = r2
0 just touches
the straight line x/a + y/b = 1, which forms the boundary of X in the quadrant
x > 0, y > 0. A little algebra shows that r2
0 = a2b2/(a2 + b2), and hence that
a2b2
4(a2 + b2) ⩽φ(0, 0) ⩽a2
4 .
Finally, the other type of boundary value problem that often arises for Laplace’s

184
CLASSIFICATION AND COMPLEX VARIABLE METHODS
equation is the Neumann problem (see Section 5.4), where the normal derivative of
φ is speciﬁed on the boundary of the solution domain. Such problems can only be
solved if the boundary data satisﬁes a solubility condition.
Theorem 7.3 A necessary condition for the existence of a solution of the Neumann
problem
∇2φ = 0 for x ∈D ⊂R3,
(7.27)
with
∂φ
∂n(x) = g(x) on ∂D,
(7.28)
is

∂D
g(x) d 2x = 0.
(7.29)
Proof If we integrate (7.27) over D and use the divergence theorem and (7.28), we
ﬁnd that

D
∇2φ d 3x =

∂D
n.∇φ d 2x =

∂D
g(x) d 2x = 0.
This solubility condition has a simple interpretation in terms of inviscid, incom-
pressible, irrotational ﬂuid ﬂow, which we introduced in Section 2.6.2. In this case,
the Neumann problem speciﬁes a steady ﬂow in D with the normal velocity of the
ﬂuid on ∂D given by g(x). Since the ﬂuid is incompressible, such a ﬂow can only
exist if the total ﬂux into D through ∂D is zero, as expressed by (7.29).
7.1.5
Properties of Parabolic Equations
Parabolic equations have just one set of characteristics. For example, for the
diﬀusion equation, Kφxx = φt with K > 0, t is constant on the characteristic
curves. As we have seen, any localized disturbance is therefore felt everywhere in
−∞< x < ∞instantaneously, as we can see from Example 5 of Section 6.4. In
addition, solutions are inﬁnitely-diﬀerentiable with respect to x. We can also prove
a maximum principle for the diﬀusion equation.
Theorem 7.4 (The maximum principle for the diﬀusion equation) Let φ
be a solution of the diﬀusion equation. Consider the domain 0 ⩽x ⩽L, 0 ⩽t ⩽T.
Then φ attains its maximum value on x = 0, 0 ⩽t ⩽T, or x = L, 0 ⩽t ⩽T, or
t = 0, 0 ⩽x ⩽L.
Proof
This is very similar to the proof of the maximum principle for Laplace’s
equation, and we leave it as Exercise 7.8(a).

7.1 CLASSIFICATION OF PARTIAL DIFFERENTIAL EQUATIONS
185
We can use this maximum principle to show that the solution of the initial–
boundary value problem given by
∂φ
∂t = K ∂2φ
∂x2
for t > 0, 0 < x < L,
(7.30)
subject to
φ(x, 0) = φ0(x) for 0 ⩽x ⩽L,
(7.31)
and
φ(0, t) = φ1(t),
φ(L, t) = φ2(t) for t > 0,
(7.32)
with φ0, φ1 and φ2 prescribed functions, is unique. We leave the details as Exer-
cise 7.8(b).
We have now seen that parabolic equations have a single set of characteristics,
and that for a canonical example, the diﬀusion equation, there is a maximum prin-
ciple, so that parabolic equations share some of the features of both hyperbolic and
elliptic equations. We end this section by stating a useful theorem that holds for
reaction–diﬀusion equations, which are parabolic. We will meet reaction–diﬀusion
equations frequently in Part 2.
Theorem 7.5 (A comparison theorem for reaction–diﬀusion equations)
Consider the reaction–diﬀusion equation
φt = K∇2φ + f (φ, x, t)
for x ∈D, t > 0,
(7.33)
with K > 0 and f a smooth function. If ¯φ(x, t) is a bounded function that satisﬁes
¯φt ⩾K∇2 ¯φ + f
¯φ, x, t

for x ∈D, t > 0,
we say that ¯φ is a supersolution of (7.33). Similarly, if φ(x, t) is a bounded function
that satisﬁes
φt ⩽K∇2φ + f

φ, x, t

for x ∈D, t > 0,
we say that φ is a subsolution of (7.33). If there also exist constants α and β with
α2 + β2 ̸= 0, such that
α¯φ −β ∂¯φ
∂n ⩾αφ −β ∂φ
∂n
for x ∈∂D, t > 0,
and
¯φ(x, 0) ⩾φ(x, 0) for x ∈D,
then
¯φ(x, t) ⩾φ(x, t) for x ∈D, t > 0.
We will not prove this result here (see Grindrod, 1991, for further details).
To see how this theorem can be used, consider the case f = φ(1 −φ), with
∂φ/∂n = 0 on ∂D. This problem arises in a model for the propagation of chemical
waves (see Billingham and King, 2001). If 0 ⩽φ(x, 0) ⩽1, then by taking ﬁrstly
¯φ = 1, φ = φ, and secondly ¯φ = φ, φ = 0, we ﬁnd that 0 ⩽φ(x, t) ⩽1 for t ⩾0.

186
CLASSIFICATION AND COMPLEX VARIABLE METHODS
In other words, the comparison theorem allows us to determine upper and lower
bounds on the solution (see Exercise 7.9 for another example).
7.2
Complex Variable Methods for Solving Laplace’s Equation
In Section 2.6.2 we described how steady, inviscid, incompressible, irrotational ﬂuid
ﬂow, commonly referred to as ideal ﬂuid ﬂow, past a rigid body can be modelled
as a boundary value problem for Laplace’s equation, ∇2φ = 0, in the ﬂuid, with
∂φ/∂n = 0 on the boundary of the body, where φ is the velocity potential and
u = ∇φ the velocity ﬁeld.
Appropriate conditions at inﬁnity also need to be
prescribed, for example, a uniform stream. Once φ is known, we can determine the
pressure using Bernoulli’s equation, (2.18). In general, solutions of this boundary
value problem are hard to ﬁnd, except for the simplest body shapes. However, two-
dimensional ﬂow is an exception, since powerful complex variable methods can,
without too much eﬀort, give simple descriptions of the ﬂow past many simply-
connected body shapes, such as aircraft wing sections, channels with junctions and
ﬂows with free surfaces (see, for example, Milne-Thompson, 1960).
7.2.1
The Complex Potential
Recall from Section 2.6.2 that the velocity in a two-dimensional ideal ﬂuid ﬂow,
u = (u, v) = (φx, φy), satisﬁes the continuity equation, ux + vy = 0.
We can
therefore introduce a stream function, ψ(x, y), deﬁned by u = ψy, v = −ψx,
which, for suﬃciently smooth functions, satisﬁes the continuity equation identically.
Elementary calculus shows that the stream function is constant on any streamline
in the ﬂow, and the change in ψ between any two streamlines is equal to the ﬂux of
ﬂuid between them. If we now look at the deﬁnitions we have for the components
of velocity, for compatibility we need φx = ψy and φy = −ψx.
These are the
Cauchy–Riemann equations for the complex function w = φ + iψ and, with our
smoothness assumption, imply that w(z) is an analytic function of the complex
variable z = x + iy in the domain occupied by the ﬂuid.† The quantity w(z) is
called the complex potential for the ﬂow and, for simple ﬂows, can be found
easily. We can also see that
dw
dz = φx + iψx = u −iv = qe−iθ,
where q is the magnitude of the velocity and θ the angle that it makes with the
x-axis, so that, once we know w, we can easily compute the components of the
velocity, and vice versa.
Let’s consider some examples.
(i) A uniform stream ﬂowing at an angle α to the horizontal has u = U cos α,
v = U sin α, so that
dw
dz = u −iv = Ue−iα,
† See Appendix 6 for a reminder of some basic ideas in the theory of complex variables.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE’S EQUATION
187
and hence w = Ue−iαz.
(ii) A point vortex is a system of concentric circular streamlines centred on
z = z0, with complex potential
w = iκ
2π log (z −z0) .
If we now deﬁne the circulation to be
'
C u·dr, where C is any closed contour
that encloses z = z0, the point vortex has circulation κ. By taking real and
imaginary parts, we can see that φ = −κθ/2π, ψ = κ log r/2π, where r is
the polar coordinate centred on z = z0. The streamlines are therefore given
by ψ = constant, and hence r = constant, a family of concentric circles, as
expected.
(iii) A point source of ﬂuid at z = z0 ejects m units of ﬂuid per unit area per
unit time. Its complex potential is
w = m
2π log(z −z0),
for a source at z = z0. The streamlines are straight lines passing through
z = z0 and, since
dw
dz = qe−iθ = m
2π
1
z −z0
,
we have that q = m/2πr, where r is the polar coordinate centred at z = z0,
and that the ﬂow is purely radial. It is simple to show that the ﬂow through
any closed curve, C, that encloses z0 is

C
u · n dl = m,
as expected.
A word of warning is required here. The ﬁrst of these complex potentials is analytic
over the whole z-plane. The second and third fail to be analytic at z = z0. In reality,
we need to invoke some extra physics close to this point. For example, any real
source of ﬂuid will be of ﬁnite size, and, close to z = z0, we need to take this into
account. For a point vortex, close to the singularity the eﬀect of viscosity becomes
important, and we need to include it. In each case, this can be done by deﬁning
an inner asymptotic region, using the methods that we will discuss in Chapter 12.
Alternatively, if there are no sources or vortices in the ﬂow domain that we are
considering, but we wish to use sources and vortices to model the ﬂow, the points
where the complex potential is not analytic must be excluded from the ﬂow domain.
We will see an example of this in the next section.
7.2.2
Simple Flows Around Blunt Bodies
Since Laplace’s equation is linear, complex potentials can be added together to
produce more complicated ﬂuid ﬂow ﬁelds. For example, consider a uniform stream
of magnitude U ﬂowing parallel to the x-axis and a source of strength 2πm situated

188
CLASSIFICATION AND COMPLEX VARIABLE METHODS
on the x-axis at z = a. The complex potential is w = Uz +m log(z −a), from which
we can deduce that
ψ = Uy + m tan−1

y
x −a

.
(7.34)
This is an odd function of y, so that the velocity ﬁeld associated with it is an even
function of y, as we would expect from the symmetry of the ﬂow. The x-axis (y = 0)
is a streamline with ψ = 0, whilst for x > 0 there is another streamline with ψ = 0,
given by
y = (a −x) tan
Uy
m

,
(7.35)
which is a blunt-nosed curve. The streamlines are shown in Figure 7.2. As there is
no ﬂow across a streamline, we can replace it with the surface of a rigid body without
aﬀecting the ﬂow pattern. The complex potential (7.34) therefore represents the
ﬂow past a blunt body of the form given by (7.35). At the point z = a −m/U,
dw/dz = 0, and hence both components of the velocity vanish there. This is called
a stagnation point of the ﬂow. Note that the singularity due to the point source
does not lie within the ﬂow domain, and can be disregarded.
−2
−1
0
1
1.5
2
−2
−1
−1.5
−0.5
−1.5
−0.5
0.5
0
0.5
1
1.5
2
x
y
BLUNT
BODY 
Fig. 7.2. The ﬂow past a blunt body given by the stream function (7.34) with U = m =
a = 1. There is a stagnation point at the origin.
Adding potentials is one way to construct ideal ﬂuid ﬂows around rigid bodies.
However, since it is an inverse method (for a given ﬂow ﬁeld, we can introduce a
rigid body bounded by any streamline), it has its drawbacks. We would prefer a

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE’S EQUATION
189
direct method, whereby we can calculate a stream function for a given rigid body.
Before we describe such a method, we need the circle theorem.
Theorem 7.6 (The circle theorem) If w = f(z) is a complex potential for a
ﬂow with no singularities in the domain |z| ⩽a, then w = f(z) + f ∗(a2/z) is a
complex potential for the same ﬂow obstructed by a circle of radius a centred at the
origin.
Proof Firstly, since f(z) is analytic in |z| < a, f ∗(a2/z) is analytic in |z| > a, as
a2/z is just an inversion of the point z. Secondly, on the boundary of the circle,
z = aeiθ with 0 ⩽θ ⩽2π, so that f(z) + f ∗(a2/z) = f(aeiθ) + f ∗(ae−iθ), which, as
it is the sum of complex conjugate functions, is real. Therefore, ψ = 0 on z = aeiθ,
and the boundary of the circle is a streamline. Finally, since w ∼f(z) + f ∗(0) as
|z| →∞, the ﬂow in the far ﬁeld is given by f(z).
We can illustrate this theorem by ﬁnding the complex potential for a uniform
stream of strength U ﬂowing past a circle of radius a. The complex potential for the
stream is w = Ue−iαz, which has no singularities in |z| ⩽a. By the circle theorem,
the complex potential for the ﬂow when the circular boundary is introduced is
w = Ue−iαz + Ueiα a2
z .
When α = 0, the ﬂow is symmetric about the x-axis, with
dw
dz = U

1 −a2
z2

,
and dw/dz ∼U as |z| →∞, as expected. By taking real and imaginary parts, we
can recover the velocity ﬁeld, potential and stream function, which we discussed in
Section 2.6.2. Before leaving this problem, we note that the complex potential is
not unique, since we can add a point vortex, centred upon z = 0, and still have the
circle as a streamline, without introducing a singularity in the ﬂow domain. When
α = 0, this gives the potential
w = U

z + a2
z

+ iκ
2π log z.
(7.36)
The introduction of this nonzero circulation about the circle has important conse-
quences, as it breaks the symmetry (see Figure 7.3), and, as we shall now show,
causes a nonzero lift force on the body.
In order to calculate the force exerted by an ideal ﬂuid ﬂow upon a rigid body,
consider an inﬁnitesimal element of arc on the surface of the body, ds. Let the
tangent to this element of arc make an angle θ with the x-axis. The force on this
inﬁnitesimal arc is due to the pressure, and given by
−p sin θ ds + ip cos θ ds = ipeiθds.

190
CLASSIFICATION AND COMPLEX VARIABLE METHODS
−5
0
5
−5
0
5
x
y
κ = 0
−5
0
5
−5
0
5
x
y
κ = 20
Fig. 7.3. Flow past a circle, with no circulation and with κ = 20.
Since dz = dx + i dy = cos θ ds + i sin θ ds = eiθds, the total force on the body is
Fx + iFy =

body
ipeiθ ds =

body
ip dz.
Now, from Bernoulli’s equation, (2.18), p = p0 −1
2ρq2 and qe−iθ = dw/dz, where θ
is the angle of the ﬂow, since this is equal to the tangential angle at the surface of
the rigid body, so that
Fx + iFy =

body
i

p0 −1
2ρ

dw
dz

2
dz.
As p0 is a constant,
Fx + iFy = −1
2ρi

body

dw
dz

2
dz.
To get a more usable result, it is convenient to manipulate this. Firstly, taking the
complex conjugate,
Fx −iFy = 1
2ρi

body

dw
dz

2
dz∗.
Now note that

dw
dz

2
= dw
dz
dw
dz
∗
= dw
dz
dw∗
dz∗,
so that

dw
dz

2
dz∗= dw
dz dw∗.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE’S EQUATION
191
Since the boundary of the body is a streamline, dw∗= dw, so that dw∗= (dw/dz)dz,
and hence
Fx −iFy = 1
2ρi

body
dw
dz
2
dz.
(7.37)
In this form, the force can usually be evaluated using the residue theorem. For the
example of ﬂow around a circle with circulation κ, we have
Fx −iFy = 1
2ρi

body

U

1 −a2
z2

+ iκ
2πz
	2
dz.
After evaluating the residue, which is just due to the simple pole, we ﬁnd that
Fx −iFy = −iρUκ, so that there is a vertical force of magnitude ρUκ, which tries
to lift the body in the direction of increasing y.
This lift force arises from the
pressure distribution around the circle, with pressures at the lower surface of the
body higher than those on the upper surface. As we shall see in Example 4 in the
next section, it is also the circulation around an aerofoil that provides the lift.
7.2.3
Conformal Transformations
Suppose we have a ﬂow in the z-plane past a body whose shape cannot imme-
diately be seen to be the streamline of a simple ﬂow. If we can transform this
problem into a new plane, the ζ-plane, where the shape of the body is simpler,
such as a half-plane or a circle, we may be able to solve for the ﬂow in the ζ-plane,
and then invert the transformation to get the ﬂow in the original, z-plane. Specif-
ically, if we seek w = w(z), the complex potential in the z-plane, and we have a
transformation, ζ = f(z), which maps the surface of the rigid body, and the ﬂow
outside it, onto a ﬂow outside a half-plane or circle in the ζ-plane, then, by deﬁning
W(ζ) = w(z), we have a correspondence between ﬂow and geometry in both planes.
Any streamline in the z-plane transforms to a streamline in the ζ-plane because of
this correspondence in complex potentials. The complex velocity is
dw
dz = dw
dζ
dζ
dz = dw
dζ f ′(z).
If |f ′(z)| is bounded and nonzero, except perhaps for isolated points on the bound-
ary of the domain to be transformed, we say that the transformation between the
z- and ζ-planes is conformal, and a unique inverse transformation can be deﬁned.
Conformal mappings are so called because they also preserve the angle between
line segments except at any isolated points where f ′(z) is zero or inﬁnity (see Ex-
ercise 7.10).
A consequence of this is that if we want to map a domain whose
boundary has corners to a domain with a smooth boundary, we must have f ′(z)
equal to zero or inﬁnity at these corner points. Such points can cause diﬃculties.
For example, if |f ′(z)| = ∞, we could induce an inﬁnite velocity in the z-plane,
which is unphysical.
Although beautifully simple in principle, there is a practical diﬃculty with this

192
CLASSIFICATION AND COMPLEX VARIABLE METHODS
method: how can we construct the transformation, f(z)? Fortunately, dictionar-
ies of common conformal transformations are available (see, for example, Milne-
Thompson, 1952). Let’s try to make things clearer with some examples.
Example 1: Mapping a three-quarter-plane onto a half-plane
Consider the three-quarter-plane that lies in the z-plane shown in Figure 7.4(a).
How can we map this onto the half-plane in the ζ-plane shown in Figure 7.4(b)?
Let’s try the transformation ζ = Azn, so that arg ζ = arg A + n arg z. On BC,
arg z = −π/2, and we require B′C′, the image of BC, to have arg ζ = 0, which
shows that arg A = nπ/2. On AB, arg z = π, and we require A′B′, the image
of AB, to have arg ζ = π. This means that π = arg A + nπ and hence n = 2/3,
arg A = π/3. This means that the family of transformations ζ = |A|eiπ/3z2/3 will
map the three-quarter-plane to the half-plane. If we further require that the image
of z = −1 should be ζ = −1, then |A| = 1, and ζ = eiπ/3z2/3. Note that this
transformation is not conformal at z = 0, since the angle of the boundary changes
there.
A
(a)
(b)
C
y
x
B
A
B
C
ξ
η
ζ-plane
z-plane
Fig. 7.4. Mapping a three-quarter-plane to a half-plane.
Example 2: Mapping a strip onto a half-plane
Consider the strip of width 2 that lies in the z-plane shown in Figure 7.5(a). How
can we map this onto the half-plane in the ζ-plane shown in Figure 7.5(b)? Let’s
try the transformation z = K log ζ + L. On A′B′, arg ζ = 0, so if ζ = ξ + iη,
z = K log |ξ| + L. If K is real, the choice L = −i makes z range from −i −∞to
−i+∞, as required. On C′D′, arg ζ = π, so z = K log |ξ|+iπK−i. If we now choose
iπK = 2i, then z ranges from i −∞to i + ∞, as required. The transformation is
therefore
z = 2
π log ζ −i,
ζ = ieπz/2,
which is conformal in the ﬁnite z-plane.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE’S EQUATION
193
D
A
B
C
C'
D'
A'
B'
ξ
η
ζ-plane
z-plane
(a)
(b)
y
x
i
−i
Fig. 7.5. Mapping a strip to a half-plane.
We now pause to note that, in both of these examples, we can use the conformal
mapping to solve the Dirichlet problem for Laplace’s equation, in the three-quarter-
plane and strip respectively, by mapping to the half-plane. We can solve Laplace’s
equation in the half-plane using the formula (5.30), which we derived using Fourier
transforms.
We can then easily write down the result in the z-plane using the
inverse transformation. To justify this, note that if φ(x, y) = Φ(x(ξ, η), y(ξ, η)), we
can diﬀerentiate to show that
∂2φ
∂x2 + ∂2φ
∂y2 =

dζ
dz

2 ∂2Φ
∂ξ2 + ∂2Φ
∂η2

,
(7.38)
so that, provided ζ = f(z) is conformal, a function that is harmonic in the (ξ, η)-
plane is harmonic in the (x, y)-plane, and vice versa (see Exercise 7.11).
We can illustrate this by solving a Dirichlet problem for Laplace’s equation in a
strip, namely
∇2φ = 0 for −∞< x < ∞and −1 < y < 1,
(7.39)
subject to
φ(x, −1) = 0,
φ(x, 1) = e−x2 for −∞< x < ∞.
(7.40)
We know that the transformation that maps this strip in the z = x + iy-plane to
the upper-half-plane in the ζ = ξ + iη-plane is ζ = ieπz/2, and hence
ξ = −eπx/2 sin
1
2πy

,
η = eπx/2 cos
1
2πy

.
In the ζ-plane, (7.39) and (7.40) become
∂2Φ
∂ξ2 + ∂2Φ
∂η2 = 0 for −∞< ξ < ∞and 0 < η < ∞,
subject to
φ(x, 0) =

0
for 0 < ξ < ∞,
exp

−4
π2 log2 (−ξ)

for −∞< ξ < 0,

194
CLASSIFICATION AND COMPLEX VARIABLE METHODS
and Φ →0 as ξ2 + η2 →∞. The solution, given by (5.30), is
Φ(ξ, η) = 1
π
 0
s=−∞
η exp

−4
π2 log2 (−s)

(ξ −s)2 + η2
ds,
and hence
φ(x, y) = 1
π
 0
s=−∞
eπx/2 cos
 1
2πy

exp

−4
π2 log2 (−s)


eπx/2 sin
 1
2πy

+ s
2 + eπx cos2  1
2πy
 ds.
Example 3: Flow past a ﬂat plate
Consider a plate of length 2a positioned perpendicular to a uniform ﬂow with speed
U, as shown in Figure 7.6(a) in the z-plane. We want to map the exterior of the
plate to the exterior of a circle in the ζ-plane, as shown in Figure 7.6(b). This can
be done using
z = 1
2i

ζ + a2
ζ

.
(7.41)
To show this, we write the surface of the circle as ζ = aeiθ, so that z = ia cos θ.
The points labelled in Figure 7.6(a) in the z-plane then map to the corresponding
points labelled in Figure 7.6(b). Following the sequence A′B′C′D′E′, the ﬂuid is on
the right hand side in the ζ-plane, and consequently is on the right hand side in
the z-plane, so that the outside of the circle maps to the outside of the plate. As
|ζ| →∞, z ∼1
2iζ, so that
dw
dz ∼−2idw
dζ .
Since we require that dw/dz ∼U, we must have dw/dζ ∼Ui/2, so that the stream
at inﬁnity in the ζ-plane is half the strength of the one in the z-plane, and has been
rotated through π/2 radians.
C'
A'
B'
a
θ
D'
E'
D B
E A
(a)
(b)
ia
C
−ia
x
z-plane
y
η
ζ-plane
•
•
ξ
Fig. 7.6. Mapping a ﬂat plate to a unit circle.

7.2 COMPLEX VARIABLE METHODS FOR SOLVING LAPLACE’S EQUATION
195
We can now solve the ﬂow problem in the ζ-plane using the circle theorem, to
give
w(ζ) = 1
2Ueiπ/2ζ + 1
2Ue−iπ/2 a2
ζ .
(7.42)
Next, we need to write this potential in terms of z. From (7.41),
ζ2 −2z
i ζ + a2 = 0,
and hence
ζ = −i
#
z2 + a2 + z

,
choosing this root so that |ζ| →∞as |z| →∞. From the potential (7.42), we ﬁnd
that
w = 1
2U
#
z2 + a2 + z +
a2
√
z2 + a2 + z

.
(7.43)
Although this result is correct, it does give rise to inﬁnite velocities at z = ±ia, the
ends of the plate, which are not physical, and arise because of the sharpness of the
boundary at these points. In reality, viscosity will become important close to these
points, and lead to ﬁnite velocities.
Example 4: Flow past an aerofoil
As we have seen, the transformation
z = 1
2

ζ + 1
ζ

(7.44)
maps the exterior of a unit circle in the ζ-plane to the exterior of a ﬂat plate in
the z-plane. In contrast, the exterior of circles of radius a > 1 whose centres are
not at the origin, but which still pass through the point ζ = 1 and enclose the
point ζ = −1, are mapped to shapes, known as Joukowski proﬁles, that look
rather like aerofoils. These shapes have two distinguishing characteristics: a blunt
nose, or leading edge, and a sharp tail, or trailing edge, at z = ζ = 1, as shown in
Figure 7.7.
Let’s now consider the ﬂow of a stream of strength U, at an angle α to the hori-
zontal, around a Joukowski proﬁle. Since dz/dζ = 0 at ζ = 1, the transformation is
not conformal at the trailing edge, and the velocity will be inﬁnite there, unless we
can make the ﬂow in the transform plane have a stagnation point there. This can
be achieved by including an appropriate circulation around the aerofoil. We must
choose the strength of the circulation to make the velocity ﬁnite at the trailing
edge. This is called the Kutta condition on the ﬂow.
If the centre of the circle in the ζ-plane is at ζ = ζc, we must begin by making
another conformal transformation, ζ = (ζ −ζc) /a, so that we can use the circle
theorem. The ﬂow in the ¯ζ-plane is a uniform stream and a point vortex, so that
¯w(¯ζ) = V e−iβ ¯ζ + V eiβ 1
¯ζ + iκ
2π log ¯ζ.

196
CLASSIFICATION AND COMPLEX VARIABLE METHODS
−1
0
0.5
1
0.2
0
0.2
x
y
2
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
ξ
η
ζc = 0.25
−1
−0.5
0
0.5
0
0.2
0.4
x
y
−2
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
ξ
η
ζc = 0.25( −1+i)
−
−
−0.5
Fig. 7.7. Two examples of Joukowski proﬁles in the z-plane, and the corresponding circles
in the ζ-plane, where ζ = ξ + iη and z = x + iy.
To determine V , we note that
z = 1
2

a¯ζ + ζc +
1
a¯ζ + ζc

,
and hence that z ∼a¯ζ/2 as |¯ζ| →∞, so that we need Ue−iα = 2V e−iβ/a, and
hence V = aU/2 and β = α. The complex potential is therefore
¯w(¯ζ) = 1
2aU

e−iα¯ζ + eiα 1
¯ζ

+ iκ
2π log ¯ζ,
(7.45)
and hence
d ¯w
d¯ζ = 1
2aU

e−iα −eiα 1
¯ζ2

+ iκ
2π¯ζ .
(7.46)
We can therefore make the trailing edge, ¯ζ = (1 −ζc) /a ≡¯ζs, a stagnation point

EXERCISES
197
by choosing
κ = iπaU

e−iα¯ζs −eiα 1
¯ζs

.
(7.47)
This expression is real, because |¯ζs| = 1 and hence the quantities e−iα¯ζs and eiα/¯ζs
are complex conjugate.
In order to calculate the force on the Joukowski proﬁle, we need to evaluate
Fx −iFy = 1
2ρi

body
dw
dz
2
dz = 1
2ρi

|¯ζ|=1
d ¯w
d¯ζ
2 d¯ζ
dz d¯ζ
= 1
2ρia

|¯ζ|=1

 1
2U

e−iα −eiα/¯ζ2
+ iκ/2πa¯ζ
2
1
2

1 −1/(a¯ζ + ζc)2
d¯ζ.
This integral can be evaluated using residue calculus.
Since we know that the
integrand is analytic for |¯ζ| > 1, we can evaluate the residue at the point at inﬁnity
by making the transformation ¯ζ = 1/s and evaluating the residue at s = 0. We
ﬁnd, after some algebra, that
Fx −iFy = −iρκUe−iα,
or equivalently,
Fx + iFy = ρκUei(π/2+α).
From this, we can see that the lift force on the aerofoil is directed perpendicular to
the incoming stream. The streamlines for the ﬂow around the aerofoils illustrated
in Figure 7.7 are shown in Figure 7.8 with α = π/4.
Exercises
7.1
Find and sketch the regions in the (x, y)-plane where the equation
(1 + x)φxx + 2xyφxy + y2φyy = 0
is elliptic, parabolic and hyperbolic.
7.2
Determine the type of each of the equations
(a) φxx −5φxy + 5φyy = 0,
(b) φxx −2φxy + 3φyy + 24φy + 5φ = 0,
(c) φxx + 6yφxy + 9y2φyy + 4φ = 0,
and reduce them to canonical form.
7.3
Determine the characteristics of Tricomi’s equation, φxx = xφyy, in the
hyperbolic region, x > 0. Transform the equation to canonical form in (a)
the hyperbolic region and (b) the elliptic region.
7.4
Show that any solution, φ(x, t), of the one-dimensional wave equation,
(3.39), satisﬁes the diﬀerence equation y1−y2+y3−y4 = 0, where y1, y2, y3
and y4 are the values of the solution at successive corners of any quadrilat-
eral whose edges are characteristics.

198
CLASSIFICATION AND COMPLEX VARIABLE METHODS
−2
−1
0
1
2
−1
−0.5
0
0.5
1
x
y
−2
−1
0
1
2
−1
−0.5
0
0.5
1
x
y
Fig. 7.8. The streamlines for ﬂow past the two examples of Joukowski proﬁles shown in
Figure 7.7, with α = π/4.
7.5
Consider the initial–boundary value problem
c−2φtt + 2kφt −φxx = 0 for t > 0, x > 0,
subject to
φ(x, 0) = φt(x, 0) = 0 for x ⩾0,

EXERCISES
199
φ(0, t) = 1
2t2 for t ⩾0.
By diﬀerentiating the canonical form of the equation, obtain an equation
for the jump in a second derivative of φ across the characteristic curve
x = ct. By solving this equation, show that the jump in φtt across x = ct
is e−kcx.
7.6
Consider the boundary value problem given by (7.25) and (7.26). By using
suitable functions ψ = Ax2 + By2 that satisfy ∇2ψ = 1 with A > 0 and
B > 0, show that
a2b2
2(a + b)2 ⩽φ(0, 0) ⩽
a2b2
2(a2 + b2).
7.7
The function φ(r, θ) satisﬁes Laplace’s equation in region a < r < b, where
r and θ are polar coordinates and a and b are positive constants. Using
the maximum principle and the fact that ∇2 (log r) = 0, show that
m(R) log
 b
a

⩽m(b) log
R
a

−m(a) log
R
b

,
where m(R) is the maximum value of φ on the circle r = R, with a < R < b.
7.8
(a) Prove Theorem 7.4. (b) Hence show that the solution of the initial–
boundary value problem given by (7.30) to (7.32) is unique.
7.9
Consider the initial–boundary value problem
φt = K∇2φ −φ3 for x ∈D, t > 0,
with k > 0, subject to
φ(x, 0) = φ0(x) for x ∈D,
and
∂φ
∂n = 0 for x ∈∂D.
Use Theorem 7.5 to show that φ →0 as t →∞, uniformly in D.
7.10
Show that the angle between two line segments that intersect at z = z0
in the complex z-plane is unchanged by a conformal mapping ζ = f(z),
provided that f ′(z0) is bounded and nonzero.
7.11
Show that, if φ(x, y) = Φ(x(ξ, η), y(ξ, η)), ζ = ξ + iη and z = x + iy, then
∂2φ
∂x2 + ∂2φ
∂y2 =

dζ
dz

2 ∂2Φ
∂ξ2 + ∂2Φ
∂η2

.
7.12
Show that the mapping ζ = sin (πz/2k) maps a semi-inﬁnite strip in the
z-plane to the upper half ζ-plane.
7.13
Show that the image of a circle of radius r in the z-plane under the trans-
formation ζ = (z + 1/z) /2 is an ellipse.

200
CLASSIFICATION AND COMPLEX VARIABLE METHODS
7.14
Find the function T(x, y) that is harmonic in the lens-shaped region deﬁned
by the intersection of the circles |z −i| = 1 and |z −1| = 1 and takes the
value 0 on the upper circular arc and 1 on the lower circular arc.
Hint: Consider the eﬀect of the transformation ζ = z/ (z −1 −i) on
this region.
7.15
Find the lift on an aerofoil section that consists of a unit circle centred on
the origin, joined at z = 1 to a horizontal ﬂat plate of unit length, when a
unit stream is incident at an angle of 45o. Hint: Firstly apply a Joukowski
transformation to map the section to a straight line, and then use an inverse
Joukowski transformation to map this straight line to a circle.

Part Two
Nonlinear Equations and Advanced Techniques
201


CHAPTER EIGHT
Existence, Uniqueness, Continuity and
Comparison of Solutions of Ordinary Diﬀerential
Equations
Up to this point in the book we have implicitly assumed that each of the diﬀerential
equations that we have been studying has a solution of the appropriate form. This
is not always the case. For example, the equation
dy
dt =
#
y2 −1
has no real solution for |y| < 1. When we ask whether an equation actually has a
solution, we are considering the question of existence of solutions. A related issue
is that of uniqueness of solutions. If we have a diﬀerential equation for which
solutions exist, does the prescription of initial conditions specify a unique solution?
Not necessarily. Consider the equation
dy
dt = 3y2/3 subject to y(0) = 0 for y > 0.
If we integrate this separable equation we obtain y = (t −c)3. The initial condition
gives c = 0 and the solution y = t3. However, it is fairly obvious that y = 0 is
another solution that satisﬁes the initial condition. In fact, this equation has an
inﬁnite number of solutions that satisfy the initial condition, namely
y =

0
for 0 ⩽t ⩽c,
(t −c)3
for t > c,
for any c ⩾0.
Another question that we should consider arises from the process of mathematical
modelling. If a diﬀerential equation provides a model of a real physical system,
how sensitive are the solutions of the diﬀerential equation to changes in the initial
conditions? If we performed two experiments with nearly identical initial conditions
(exact repetition of an experiment being impossible in practice), should we expect
nearly identical outcomes?
In this chapter we will prove some rigorous results related to the issues we have
discussed above.

204
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
8.1
Local Existence of Solutions
We will start with the simplest type of scalar diﬀerential equation by proving a
local existence theorem for
dy
dt = f(y, t) subject to y(t0) = y0 in the domain |t −t0| < α.
(8.1)
Here, α > 0 deﬁnes the size of the region where we will be able to show that a
solution exists. We begin by deﬁning a closed rectangle,
R = {(y, t) | |y −y0| ⩽b, |t −t0| ⩽a} ,
centred upon the initial point, (y0, t0), within which we will make certain assump-
tions about the behaviour of f. If we integrate (8.1) with respect to t and apply
the initial condition, we obtain
y(t) = y0 +
 t
t0
f(y(s), s) ds.
(8.2)
This is an integral equation for the unknown solutions, y = y(t), which is equivalent
to the original diﬀerential equation. Our strategy now is to use this to produce a
set of successive approximations to the solution that we seek. We will do this
using the initial condition as the starting point.
We deﬁne
y0(t) = y0,
y1(t) = y0 +
 t
t0
f(y0, s) ds,
y2(t) = y0 +
 t
t0
f(y1(s), s) ds,
...
yk+1(t) = y0 +
 t
t0
f(yk(s), s) ds.
(8.3)
As an example of how this works, consider the simple diﬀerential equation y′ = y
subject to y(0) = 2. In this case,
y0(t) = 2,
y1(t) = 2 +
 t
0
2 ds = 2(1 + t),
y2(t) = 2 +
 t
0
2(1 + s) ds = 2

1 + t + 1
2t2

,
...
yk+1(t) = 2
k+1

n=0
1
n!tn.
As k →∞, {yk(t)} →2et, the correct solution. In general, if the sequence of
functions (8.3) converges uniformly to a limit, {yk(t)} →y∞(t), then y∞(t) is a
continuous solution of the integral equation. For continuous f(y, t), we can then

8.1 LOCAL EXISTENCE OF SOLUTIONS
205
diﬀerentiate under the integral sign, to show that y is a solution of the original
diﬀerential equation, (8.1). We will shortly prove that, in order to ensure that the
sequence (8.3) converges to a continuous function, it is suﬃcient that f(y, t) and
∂f
∂y (y, t) are continuous in R. In other words, we need f, ∂f/∂y ∈C0(R).†
As a preliminary to the main proof, recall Theorem A2.1, which says that a
function continuous on a bounded region is itself bounded, so that there exist
strictly positive, real constants M and K such that |f(y, t)| < M and
 ∂f
∂y (y, t)
 < K
for all (y, t) ∈R. If (y1, t) and (y2, t) are two points in R, Theorem A2.2, the mean
value theorem, states that
f(y2, t) −f(y1, t) = ∂f
∂y (c, t)(y2 −y1) for some c with y1 < c < y2.
Since (c, t) ∈R, we have
 ∂f
∂y (c, t)
 < K, and hence
|f(y2, t) −f(y1, t)| < K |y2 −y1|
∀(y2, t), (y1, t) ∈R.
(8.4)
Functions that satisfy the inequality (8.4) are said to satisfy a Lipschitz condition
in R.
It is possible for a function to satisfy a Lipschitz condition in a region
R without having a continuous partial derivative everywhere in R. For example,
f(y, t) = t|y| satisﬁes |f(y2, t) −f(y1, t)| < |y2 −y1| in the unit square centred on
the origin, so that it is Lipschitz with K = 1. However, ∂f/∂y is not continuous on
the line y = 0. Our assumption about the continuity of the ﬁrst partial derivative
automatically leads to functions that satisfy a Lipschitz condition, which is the key
to proving the main result of this section.
As we stated earlier, we are going to use the successive approximations (8.3) to
establish the existence of a solution. Prior to using this, we must show that the
elements of this sequence are well-deﬁned. Speciﬁcally, if yk+1(t) is to be deﬁned on
some interval I, we must establish that the point (yk(s), s) remains in the rectangle
R = {(y, t) | |y −y0| ⩽b, |t −t0| ⩽a} for all s ∈I.
Lemma 8.1 If α = min (a, b/M), then the successive approximations,
y0(t) = y0,
yk+1(t) = y0 +
 t
t0
f(yk(s), s) ds
are well-deﬁned in the interval I = {t | |t −t0| < α}, and on this interval |yk(t) −
y0| < M|t −t0| ⩽b, where |f| < M.
Proof
We proceed by induction.
It is clear that y0(t) is deﬁned on I, as it is
constant. We assume that
yn(t) = y0 +
 t
t0
f(yn−1(s), s) ds
† If these conditions hold in a larger domain, D, we can use the local result repeatedly until the
solution moves out of D.

206
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
is well-deﬁned on I, so that the point (yn(t), t) remains in R for t ∈I. By deﬁnition,
yn+1(t) = y0 +
 t
t0
f(yn(s), s) ds,
so we have yn+1(t) deﬁned in I. Now
|yn+1(t) −y0| =

 t
t0
f(yn(s), s) ds
 ⩽
 t
t0
|f(yn(s), s)| ds ⩽M|t −t0| ⩽Mα ⩽b,
which is what we claimed.
To see rather less formally why we chose α = min(a, b/M), note that the condi-
tion |f(y, t)| < M implies that the solution of the diﬀerential equation, y = y(t),
cannot cross lines of slope M or −M through the initial point (y0, t0), as shown
in Figure 8.1. The relationship |yk(t) −y0| < M|t −t0|, which we established in
Lemma 8.1, means that the successive approximations cannot cross these lines ei-
ther. These lines intersect the boundary of the rectangle R, and the length of the
interval I depends upon whether they meet the horizontal sides (α = b/M) or the
vertical sides (α = a), as shown in Figure 8.1.
We can now proceed to establish the main result of this section.
Theorem 8.1 (Local existence) If f and ∂f/∂y are in C0(R), then the successive
approximations yk(t), deﬁned by (8.3), converge on I to a solution of the diﬀerential
equation y′ = f(y, t) that satisﬁes the initial condition y(t0) = y0.
Proof We begin with the identity
yj(t) = y0(t) + {y1(t) −y0(t)} + {y2(t) −y1(t)} + · · · + {yj(t) −yj−1(t)}
= y0(t) +
j−1

n=0
{yn+1(t) −yn(t)} .
(8.5)
In order to use this, we need to estimate the value of yn+1(t) −yn(t). Using the
deﬁnition (8.3), we have, for n ⩾1 and |t −t0| < α,
|yn+1(t) −yn(t)| =

 t
t0
{f(yn(s), s) −f(yn−1(s), s)} ds

⩽
 t
t0
|f(yn(s), s) −f(yn−1(s), s)| ds ⩽K
 t
t0
|yn(s) −yn−1(s)| ds.
For n = 0 we have
|y1(t) −y0(t)| =

 t
t0
f(y0(s), s) ds
 ⩽M|t −t0|,
by using the continuity bound on f. If we now repeatedly use these inequalities, it
is straightforward to show that
|yn+1(t) −yn(t)| ⩽MKn|t −t0|n+1
(n + 1)!
for n ⩾1, |t −t0| ⩽α,

8.1 LOCAL EXISTENCE OF SOLUTIONS
207
Fig. 8.1. The two cases that determine the length of the interval, α.
and hence that
|yn+1(t) −yn(t)| ⩽M(Kα)n+1
K(n + 1)! .
(8.6)
If we denote by y(t) the limit of the sequence {yk(t)}, we now have, from (8.5),
y(t) = y0(t) +
∞

n=0
{yn+1(t) −yn(t)} .
(8.7)
Each term in this inﬁnite series is dominated by M(Kα)n+1/K(n + 1)!, a positive

208
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
constant; and
M
K
∞

n=0
(Kα)n+1
(n + 1)! = M
K

eKα −1

,
(8.8)
which is a ﬁnite constant. The comparison theorem applied to the series in (8.7)
shows that it converges absolutely on the interval |t −t0| < α, and we have now
proved that the sequence of successive approximations, (8.3), converges. We can
now return to Lemma 8.1 with the knowledge that y(t) = limk→∞yk(t) is well-
deﬁned, and, since the right hand side of (8.8) is independent of k, immediately
claim that (y(s), s) ∈R.
We now show that the limiting function, y(t), actually satisﬁes the integral
equation, (8.2). Now
|y(t) −yk(t)| =

∞

n=k
{yn+1(t) −yn(t)}
 ⩽M
K
∞

m=k
(Kα)m+1
(m + 1)!
⩽M
K
(Kα)k+1
(k + 1)!
∞

m=0
(Kα)m
m!
⩽M
K
(Kα)k+1
(k + 1)! eKα = ϵk for |t −t0| < α.
(8.9)
Using the Lipschitz condition in the interval |t −t0| < α and Lemma 8.1,

 t
t0
{f(y(s), s) −f(yk(s), s)} ds
 ⩽K
 t
t0
|y(s) −yk(s)| ds ⩽αM(Kα)k+1
(k + 1)!
eKα.
For ﬁxed α, M and K,
αM(Kα)k+1
(k + 1)!
eKα →0 as k →∞,
so that
lim
k→∞
 t
t0
f(yk(s), s) ds =
 t
t0
f(y(s), s) ds,
and consequently y(t) satisﬁes (8.2).
To prove that y(t) is continuous on |t −t0| < α, we can use some of the auxiliary
results that we derived above. Consider
y(t + h) −y(t) = y(t + h) −yk(t + h) + yk(t + h) −yk(t) + yk(t) −y(t),
so that, using the triangle inequality,
|y(t + h) −y(t)| ⩽|y(t + h) −yk(t + h)| + |yk(t + h) −yk(t)| + |yk(t) −y(t)|.
By choosing k suﬃciently large, using the estimate (8.9), we have
|yk(t) −y(t)| ⩽ϵk,
so that
|y(t + h) −y(t)| ⩽2ϵk + |yk(t + h) −yk(t)| ⩽ϵ,

8.1 LOCAL EXISTENCE OF SOLUTIONS
209
for suﬃciently small h, using the continuity of yk(t). Hence, y(t) is continuous, as
we claimed at the beginning of this section, and the equivalence of the integral and
diﬀerential equations can be established by diﬀerentiation under the integral sign.
Remarks
(i) For a particular diﬀerential equation, Theorem 8.1 is easy to use. For ex-
ample, if y′ = y + t, both f(y, t) = y + t and ∂f/∂y = 1 are continuous for
−∞< t < ∞, −∞< y < ∞, so the successive approximations are guar-
anteed to converge in this domain. If y′ = 3y2/3, f(y, t) = 3y2/3, which is
continuous in −∞< y < ∞, −∞< t < ∞. However, its partial derivative,
∂f/∂y = 2y−1/3, is discontinuous at y = 0. Consequently, the successive
approximations are not guaranteed to converge. This should be no surprise,
since, as we noted in the introduction to this chapter, the solution of this
equation is not unique for some initial data.
(ii) Consider the equation y′ = y2 subject to y(0) = 1.
Since this equation
satisﬁes the conditions of Theorem 8.1, the successive approximations will
converge, and therefore a solution exists in some rectangle containing the
initial point, namely
R = {(y, t) | |t| ⩽a, |y −1| ⩽b} .
Let’s now try to determine the values of the constants a and b. Since M =
maxR y2 = (1 + b)2, α = min(a, b/(1 + b)2). From simple calculus,
max
b>0
b
(1 + b)2 = 1
4,
so that, independent of the particular choice of a, α ⩽
1
4.
Theorem 8.1
therefore shows that a solution exists for |t| ⩽
1
4.
In fact, the solution,
y = 1/(1−t), exists for −∞< t < 1. It is rather more diﬃcult to determine
regions of global existence like this, than it is to establish local existence,
the point of Theorem 8.1, and we will not attempt it here.
(iii) We can also consider the local existence of solutions of systems of ﬁrst order
equations in the form
y′ = f(y, t),
(8.10)
where
y =





y1
y2
...
yn




,
f =





f1(y1, y2, . . . , yn, t)
f2(y1, y2, . . . , yn, t)
...
fn(y1, y2, . . . , yn, t)





are vectors with n components and t is a real scalar. The vector form of
the Lipschitz condition can be established by requiring f and ∂f/∂yi to be

210
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
continuous with respect to all of their components and t, so that


∂f
∂yi

 ⩽K
for all (y, t) ∈D ⊂Rn+1, where ||.|| is a suitable vector norm. It then follows
that for any points (y, t) and (z, t) in D, ||f(y, t)−f(z, t)|| ⩽K||y −z||. The
proof then follows that of Theorem 8.1 step by step and line by line, with
modulus signs replaced by vector norms.
(iv) For a selection of results similar to, but in some ways better than, Theo-
rem 8.1, see Coddington and Levinson (1955).
8.2
Uniqueness of Solutions
We will now show that, under the same conditions as Theorem 8.1, the solution of
the initial value problem (8.1) is unique. In order to prove this, we need a result
called Gronwall’s inequality.
Lemma 8.2 (Gronwall’s inequality) If f(t) and g(t) are non-negative functions
on the interval α ⩽t ⩽β, L is a non-negative constant and
f(t) ⩽L +
 t
α
f(s)g(s) ds for t ∈[α, β],
then
f(t) ⩽L exp
 t
α
g(s) ds
	
for t ∈[α, β].
Proof Deﬁne
h(t) = L +
 t
α
f(s)g(s) ds,
so that h(α) = L. By hypothesis, f(t) ⩽h(t), and by the fundamental theorem of
calculus, since g(t) ⩾0, we have
h′(t) = f(t)g(t) ⩽h(t)g(t) for t ∈[α, β].
Using the idea of an integrating factor, this can be rewritten as
d
dt

h(t) exp

−
 t
α
g(s) ds
	
⩽0.
Integrating from α to t gives
h(t) exp

−
 t
α
g(s) ds
	
−L ⩽0,
and, since f(t) ⩽h(t),
f(t) ⩽L exp
 t
α
g(s) ds
	
.

8.3 DEPENDENCE OF THE SOLUTION ON THE INITIAL CONDITIONS
211
Theorem 8.2 (Uniqueness) If f,
∂f
∂y ∈C0(R), then the solution of the initial
value problem y′ = f(y, t) subject to y(t0) = y0 is unique on |t −t0| < α.
Proof We proceed by contradiction. Suppose that there exist two solutions, y =
y1(t) and y = y2(t), with y1(t0) = y2(t0) = y0. These functions satisfy the integral
equations
y1(t) = y0 +
 t
t0
f(y1(s), s) ds,
y2(t) = y0 +
 t
t0
f(y2(s), s) ds,
(8.11)
and hence the points (yi(s), s) lie in the region R.
Taking the modulus of the
diﬀerence of these leads to
|y1(t) −y2(t)| =

 t
t0
{f(y1(s), s) −f(y2(s), s)} ds

⩽
 t
t0
|{f(y1(s), s) −f(y2(s), s)}| ds ⩽
 t
t0
K|y1(s) −y2(s)| ds for |t −t0| ⩽α,
where K > 0 is the Lipschitz constant for the region R.
We can now apply
Gronwall’s inequality to the non-negative function |y1(t) −y2(t)| with L = 0 and
g(s) = K > 0 to conclude that |y1(t) −y2(t)| ⩽0. However, since the modulus of
a function cannot be negative, |y1(t) −y2(t)| = 0, and hence y1(t) ≡y2(t).
Just as we saw earlier when discussing the existence of solutions, this uniqueness
result can be extended to systems of ordinary diﬀerential equations. Under the
conditions given in Remark (iii) in the previous section, the solution of (8.10) exists
and is unique.
8.3
Dependence of the Solution on the Initial Conditions
A solution of y′ = f(y, t) that passes through the initial point (y0, t0) depends
continuously on the three variables t0, y0 and t. For example, y′ = 3y2/3 subject
to y(t0) = y0 has solution y = (t −t0 + y1/3
0
)3, which depends continuously on t0,
y0 and t. Earlier, we hypothesized that solutions of identical diﬀerential equations
with initial conditions that are close to each other should remain close, at least for
values of t close to the initial point, t = t0. We can now prove a theorem about
this.
Theorem 8.3 Let y = y0(t) be the solution of y′ = f(y, t) that passes through the
initial point (t0, y0) and y = y1(t) the solution that passes through (t1, y1). If f,
∂f/∂y ∈C0(R) and both y0(t) and y1(t) exist on some interval α < t < β with t0,
t1 ∈(α, β), then ∀ϵ > 0 ∃δ > 0 such that if |t0 −t1| < δ and |y0 −y1| < δ then
|y0(t) −y1(t)| < ϵ ∀t ∈(α, β).

212
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
Proof Since y1(t) and y2(t) satisfy the integral equations (8.11), we can take their
diﬀerence and, by splitting up the range of integration, write†
y0(t) −y1(t) = y0 −y1 +
 t1
t0
f(y0(s), s) ds +
 t
t1
{f(y0(s), s) −f(y1(s), s)} ds.
Taking the modulus gives
|y0(t) −y1(t)| ⩽|y0 −y1| +

 t1
t0
f(y0(s), s) ds
 +

 t
t1
{f(y0(s), s) −f(y1(s), s)} ds

⩽|y0 −y1| + M(t1 −t0) +
 t
t1
K|y0(s) −y1(s)| ds,
by using the upper bound on f and the Lipschitz condition on R. If |t1 −t0| < δ
and |y1 −y0| < δ, this reduces to
|y0(t) −y1(t)| ⩽(M + 1)δ +
 t
t1
K|y0(s) −y1(s)| ds.
We can now apply Gronwall’s inequality to obtain
|y0(t) −y1(t)| ⩽(M + 1)δ exp
 t
t1
K ds
	
= (M + 1)δ exp {K(t −t1)} ,
and, since |t −t1| < β −α, we have
|y0(t) −y1(t)| ⩽(M + 1)δ exp {K(β −α)} .
If we now choose δ < ϵ exp {−K(β −α)} /(M + 1), we have |y0(t) −y1(t)| < ϵ, and
the proof is complete.
8.4
Comparison Theorems
Since a large class of diﬀerential equations cannot be solved explicitly, it is useful to
have a method of placing bounds on the solution of a given equation by comparing
it with solutions of a related equation that is simpler to solve. For example, y′ =
e−ty subject to y(0) = 1 is a rather nasty nonlinear diﬀerential equation, and the
properties of its solution are not immediately obvious. However, since 0 ⩽e−ty ⩽1
for 0 ⩽t < ∞and 0 ⩽y < ∞, it would seem plausible that 0 ⩽y′ ⩽1. Integration
of this traps the solution of the initial value problem in the range 1 ⩽y ⩽1 + t. In
order to prove this result rigorously, we need some preliminary results and ideas.
Note that, to simplify things, we will prove all of the results in this section for one
side of a point only. Firstly, we need some deﬁnitions.
A function F(y, t) satisﬁes a one-sided Lipschitz condition in a domain D if,
for some constant K,
y2 > y1 ⇒F(y2, t) −F(y1, t) ⩽K(y2 −y1) for (y2, t), (y1, t) ∈D.
† We have assumed that t1 > t0, but the argument needs only a slight modiﬁcation if this is not
the case.

8.4 COMPARISON THEOREMS
213
If σ(t) is diﬀerentiable for t > a, then σ′(t) ⩽K is called a diﬀerential in-
equality. Note that if σ′(t) ⩽0 for t ⩾a, then σ(t) ⩽σ(a).
Lemma 8.3 If σ(t) is a diﬀerentiable function that satisﬁes the diﬀerential inequal-
ity σ′(t) ⩽Kσ(t) for t ∈[a, b] and some constant K, then σ(t) ⩽σ(a)eK(t−a) for
t ∈[a, b].
Proof
We write the inequality as σ′(t) −Kσ(t) ⩽0 and multiply through by
e−Kt > 0 to obtain
d
dt

e−Ktσ(t)

⩽0.
This leads to e−Ktσ(t) ⩽e−Kaσ(a), and hence the result. Note that this is really
just a special case of Gronwall’s inequality.
Lemma 8.4 If g(t) is a solution of y′ = F(y, t) for t ⩾a and f(t) is a function
that satisﬁes the diﬀerential inequality f ′(t) ⩽F(f(t), t) for t ⩾a with f(a) = g(a),
then, provided that F satisﬁes a one-sided Lipschitz condition for t ⩾a, f(t) ⩽g(t)
for all t ⩾a.
Proof
We will proceed by contradiction.
Suppose that f(t1) > g(t1) for some
t1 > a. Deﬁne t0 to be the largest t in the interval a ⩽t ⩽t1 such that f(t) ⩽g(t),
and hence f(t0) = g(t0). Deﬁne σ(t) = f(t) −g(t), so that σ(t) ⩾0 for t0 ⩽t ⩽t1
and σ(t0) = 0. The one-sided Lipschitz condition shows that
σ′(t) = f ′(t) −g′(t) ⩽F(f(t), t) −F(g(t), t) ⩽K {f(t) −g(t)} = Kσ(t),
and hence σ′(t) ⩽Kσ(t) for t > t0. By Lemma 8.3, σ(t) ⩽σ(t0)eK(t−t0) ⩽0.
However, since σ(t) is non-negative for t ⩾t0, σ(t) ≡0.
This contradicts the
hypothesis that σ(t1) = f(t1) −g(t1) > 0, and hence the result is proved.
Theorem 8.4 Let f(t) and g(t) be solutions of the diﬀerential equations y′ =
F(y, t) and z′ = G(z, t) respectively, on the strip a ⩽t ⩽b, with f(a) = g(a). If
F(y, t) ⩽G(y, t) and F or G is one-sided Lipschitz on this strip, then f(t) ⩽g(t)
for a ⩽t ⩽b
Proof Since y′ = F(y, t) ⩽G(y, t), g satisﬁes a diﬀerential inequality of the form
described in Lemma 8.4, and f satisﬁes the diﬀerential equation. This gives us the
result immediately.
Note that Theorem 8.4 and the preceding two lemmas can be made two-sided in
the neighbourhood of the initial point with minor modiﬁcations.
Comparison theorems, such as Theorem 8.4, are of considerable practical use.
Consider as an example the initial value problem
y′ = t2 + y2 subject to y(0) = 1.
(8.12)
It is known, either from analysis of the solution or by numerical integration, that

214
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
the solution of this equation ‘blows up’ to inﬁnity at some ﬁnite value of t = t∞. We
can estimate the position of this blowup point by comparing the solution of (8.12)
with the solutions of similar, but simpler, diﬀerential equations. Since t2 + y2 ⩾y2
for 0 ⩽t < ∞, the solution of (8.12) is bounded below by the solution of y′ = y2
subject to y(0) = 1, which is y = 1/(1−t). Since this blows up when t = 1, we must
have t∞⩽1. Also, since t2 +y2 ⩽1+y2 for 0 ⩽t ⩽1, an upper bound is provided
by the solution of y′ = 1 + y2 subject to y(0) = 1, namely y = tan

t + π
4

. This
blows up when t = π/4. By sandwiching the solution of (8.12) between two simpler
solutions, we are able to conclude that the blowup time satisﬁes π
4 ⩽t∞⩽1.
Comparison theorems are also available for second order diﬀerential equations.
These take a particularly simple form for the equation
y′′(x) + g(x)y(x) = 0,
(8.13)
from which the ﬁrst derivative of y is absent. In fact, we can transform any lin-
ear, second order, ordinary diﬀerential equation into this form, as we shall see in
Section 12.2.7. We will compare the solution of (8.13) with that of
z′′(x) + h(x)z(x) = 0,
(8.14)
and prove the following theorem.
Theorem 8.5 If g(x) < h(x) for x ⩾x0, y(x) is the solution of (8.13) with y(x0) =
y0 ̸= 0, y′(x0) = y1, these conditions being such that y(x) > 0 for x0 ⩽x ⩽x1, and
z(x) is the solution of (8.14) with z(x0) = y0 and z′(x0) = y1, then y(x) > z(x) for
x0 < x ⩽x1, provided that z(x) > 0 on this interval.
Proof From (8.13) and (8.14),
y′′z −yz′′ = (h −g) yz.
Integrating this equation from x0 to x < x1, we obtain
y′(x)z(x) −y(x)z′(x) =
 x
x0
(h(t) −g(t)) y(t)z(t) dt.
By hypothesis, the right hand side of this is positive. Also, by direct diﬀerentiation,
d
dx
y(x)
z(x)

= y′(x)z(x) −y(x)z′(x)
z2(x)
> 0,
so that y/z is an increasing function of x. Since y(x0)/z(x0) = 1, we have y(x) >
z(x) for x0 < x ⩽x1.
As an example of the use of Theorem 8.5, consider Airy’s equation,
¨y(t) −ty(t) = 0
(see Sections 3.8 and 11.2), in −1 < t < 0 with y(0) = 1 and ˙y(0) = 0.
By
making the transformation t →−x, we arrive at y′′(x) + xy(x) = 0 in 0 < x < 1,
with y(0) = 1 and y′(0) = 0. The solution of this is positive for 0 ⩽x < x1.
If we consider z′′(x) + z(x) = 0 with z(0) = 1 and z′(0) = 0, then clearly z =

EXERCISES
215
cos x, which is positive for 0 ⩽x < π/2, and hence for 0 ⩽x ⩽1. Since the
equations and boundary conditions that govern y(x) and z(x) satisfy the conditions
of Theorem 8.5, we conclude that y(x) > cos x for 0 < x < 1. In fact, we can see
from the exact solution,
y(x) = Bi′(0)Ai(−x) −Ai′(0)Bi(−x)
Bi′(0)Ai(0) −Ai′(0)Bi(0)
,
shown in Figure 8.2, that x1 ≈1.986.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−0.5
0
0.5
1
x
y
cos x
Fig. 8.2. The exact solution of y′′ + xy = 0 subject to y(0) = 1 and y′(0) = 0.
Exercises
8.1
Determine the integral equations equivalent to the initial value problems
(a) y′ = t2 + y4 subject to y(0) = 1,
(b) y′ = y + t subject to y(0) = 0,
and determine the ﬁrst two successive approximations to the solution.
8.2
Show that the functions
(a) f(y, t) = te−y2 for |t| ⩽1, |y| < ∞,
(b) f(y, t) = t2 + y2 for |t| ⩽2, |y| ⩽3,
are Lipschitz in the regions indicated, and ﬁnd the Lipschitz constant, K,
in each case.

216
EXISTENCE, UNIQUENESS, CONTINUITY AND COMPARISON
8.3
How could successive approximations to the solution of y′ = 3y2/3 fail to
converge to a solution?
8.4
Let f(y, t) and g(y, t) be continuous and satisfy a Lipschitz condition with
respect to y in a region D. Suppose |f(y, t) −g(y, t)| < ϵ in D for some
ϵ > 0.
If y1(t) is a solution of y′ = f(y, t) and y2(t) is a solution of
y′ = g(y, t), such that |y2(t0) −y1(t0)| < δ for some t0 and δ > 0, show
that, for all t for which y1(t) and y2(t) both exist,
|y2(t) −y1(t)| ⩽δ exp (K|t −t0|) + ϵ
K {exp (K|t −t0|) −1} ,
where K is the Lipschitz constant. Hint: Use the Gronwall inequality.
8.5
Find upper and lower bounds to the solutions of the diﬀerential equations
(a) y′ = sin(xy) subject to y(0) = 1/2 for x ⩾0,
(b) y′ = y3 −y subject to y(0) = 1/4.
8.6
If σ(t) ∈C1[a, a + ϵ] and positive satisﬁes the diﬀerential inequality σ′ ⩽
Kσ log σ, show that
σ(t) ⩽σ(a)eK(t−a) for t ∈[a, a + ϵ].
8.7
For each ﬁxed x, let F(x, y) be a nonincreasing function of y. Show that,
if f(x) and g(x) are two solutions of y′ = F(x, y) and b > a, then |f(b) −
g(b)| ⩽|f(a) −g(a)|. Hence deduce a result concerning the uniqueness of
solutions. This is known as the Peano uniqueness theorem.

CHAPTER NINE
Nonlinear Ordinary Diﬀerential Equations: Phase
Plane Methods
9.1
Introduction: The Simple Pendulum
Ordinary diﬀerential equations can be used to model many diﬀerent types of phys-
ical system. We now know a lot about second order linear ordinary diﬀerential
equations. For example, simple harmonic motion,
d 2θ
dt2 + ω2θ = 0,
(9.1)
describes many physical systems that oscillate with small amplitude θ. The general
solution is θ = A sin ωt + B cos ωt, where A and B are constants that can be ﬁxed
from the initial values of θ and dθ/dt. The solution is an oscillatory function of t.
Note that we can also write this as θ = Ceiωt+De−iωt, where C and D are complex
constants. In the real world, the physics of a problem is rarely as simple as this.
Let’s consider the frictionless simple pendulum, shown in Figure 9.1. A mass, m,
is attached to a light, rigid rod of length l, which can rotate without friction about
the point O.
Fig. 9.1. A simple pendulum.

218
PHASE PLANE METHODS
Using Newton’s second law on the force perpendicular to the rod gives
−mg sin θ = mld 2θ
dt2 ,
and hence
d 2θ
dt2 + ω2 sin θ = 0,
(9.2)
where ω2 = g/l and t is time. For oscillations of small amplitude, θ ≪1, so that
sin θ ∼θ, and we obtain simple harmonic motion, (9.1). If θ is not small, we must
study the full equation of motion, (9.2), which is nonlinear. Do we expect the
solutions to be qualitatively diﬀerent to those of simple harmonic motion? If we
push a pendulum hard enough, we should be able to make it swing round and round
its point of support, with θ increasing continuously with t (remember there is no
friction), so we would hope that (9.2) has solutions of this type.
In general, nonlinear ordinary diﬀerential equations cannot be solved analyti-
cally, but for equations like (9.2), where the ﬁrst derivative, dθ/dt does not appear
explicitly, an analytical solution is available. Using the notation ˙θ = dθ/dt, the
trick is to treat ˙θ as a function of θ instead of t. Note that
d 2θ
dt2 = d
dt
dθ
dt

= d ˙θ
dt = dθ
dt
d ˙θ
dθ = ˙θd ˙θ
dθ = d
dθ
1
2
˙θ2

.
This allows us to write (9.2) as
d
dθ
1
2
˙θ2

= −ω2 sin θ,
which we can integrate once to give
1
2
˙θ2 = ω2 cos θ + constant.
Using ω2 = g/l, we can write this as
1
2ml2 ˙θ2 −mgl cos θ = E.
(9.3)
This is just a statement of conservation of energy, E, with the ﬁrst term representing
kinetic energy, and the second, gravitational potential energy. Systems like (9.2),
which can be integrated once to determine a conserved quantity, here energy,
E, are called conservative systems. Note that if we try to account for a small
amount of friction at the point of suspension of the pendulum, we need to add a
term proportional to dθ/dt to the left hand side of (9.2). The system is then no
longer conservative, with dramatic consequences for the motion of the pendulum
(see Exercise 9.6).
From (9.3) we can see that
dθ
dt = ±
*
2E
ml2 + 2g
l cos θ.

9.1 INTRODUCTION: THE SIMPLE PENDULUM
219
We can integrate this to arrive at the implicit solution
t = ±
 θ
θ0
dθ′
$
2E
ml2 + 2g
l cos θ′
,
(9.4)
where θ0 is the angle of the pendulum when t = 0. Note that the two constants of
integration are E and θ0, the initial energy and angle of the pendulum. Equation
(9.4) is a simple representation of the solution, which, if necessary, we can write in
terms of Jacobian elliptic functions (see Section 9.4), so now everything is clear . . . ,
except of course that it isn’t! Presumably equation (9.4) gives oscillatory solutions
for small initial angles and kinetic energies, and solutions with θ increasing with t
for large enough initial energies, but this doesn’t really leap oﬀthe page at you.
From (9.4) we have a quantitative expression for the solution, but we are really
more interested in the qualitative nature of the solution.
Let’s go back to (9.3) and write
˙θ2 = 2E
ml2 + 2g
l cos θ.
Graphs of ˙θ2 as a function of θ are shown in Figure 9.2(a) for diﬀerent values of E.
— For E > mgl the curves lie completely above the θ-axis.
— For −mgl < E < mgl the curves intersect the θ-axis.
— For E < −mgl the curves lie completely below the θ-axis (remember, −1 ⩽
cos θ ⩽1).
We can now determine ˙θ as a function of θ by taking the square root of the curves
in Figure 9.2(a) to obtain the curves in Figure 9.2(b), remembering to take both
the positive and negative square root.
— For E > mgl, the curves lie either fully above or fully below the θ-axis.
— For −mgl < E < mgl, only ﬁnite portions of the graph of ˙θ2 lie above the θ-axis,
so the square root gives ﬁnite, closed curves.
— For E < −mgl, there is no real solution. This corresponds to the fact that the
pendulum always has a gravitational potential energy of at least −mgl, so we
must have E ⩾−mgl.
As we shall see later, the solution with E = mgl is an important one. The graph
of ˙θ2 just touches the θ-axis at θ = ±(2n −1)π, for n = 1, 2, . . . , and taking the
square root gives the curves that pass through these points shown in Figure 9.2(b).
How do θ and ˙θ vary along these solution curves as t increases? If ˙θ is positive, θ
increases with t, and vice versa (remember, ˙θ = dθ/dt is, by deﬁnition, the rate at
which θ changes with t). This allows us to add arrows to Figure 9.2(b), indicating
in which direction the solution changes with time. We have now constructed our
ﬁrst phase portrait for a nonlinear ordinary diﬀerential equation.
The (θ, ˙θ)-
plane is called the phase plane. Each of the solution curves represents a possible
solution of (9.2), and is known as an integral path or trajectory. If we know the
initial conditions, θ = θ0, ˙θ = ˙θ0 =
$
2E
ml2 + 2g
l cos θ0, the integral path that passes

220
PHASE PLANE METHODS
through the point (θ0, ˙θ0) when t = 0 represents the solution. Finally, note that,
since θ = π is equivalent to θ = −π, we only need to consider the phase portrait for
−π ⩽θ ⩽π. Alternatively, we can cut the phase plane along the lines θ = −π and
θ = π and join them up, so that the integral paths lie on the surface of a cylinder.
Fig. 9.2. (a) ˙θ2 as a function of θ for diﬀerent values of E. (b) ˙θ as a function of θ –
the phase portrait for the simple pendulum. The cross at (0, 0) indicates an equilibrium
solution. The arrows indicate the path followed by the solution as t increases. Note that
the phase portrait for |θ| ⩾π is the periodic extension of the phase portrait for |θ| ⩽π.
Having gone to the trouble of constructing a phase portrait for (9.2), does it
tell us what the pendulum does in a form more digestible than that given by
(9.4)? Let’s consider the three qualitatively diﬀerent types of integral path shown
in Figure 9.2(b).
(i) Equilibrium solutions The points ˙θ = 0, θ = 0 or ±π, represent the two
equilibrium solutions of (9.2). The point (0, 0) is the equilibrium with the
pendulum vertically downward, (π, 0) the equilibrium with the pendulum
vertically upward. Points close to (0, 0) lie on small closed trajectories close
to (0, 0). This indicates that (0, 0) is a stable equilibrium point, since
a small change in the state of the system away from equilibrium leads to
solutions that remain close to equilibrium.
If you cough on a pendulum
hanging downwards, you will only excite a small oscillation. In contrast,
points close to (π, 0) lie on trajectories that take the solution far away from
(π, 0), and we say that this is an unstable equilibrium point.
If you
cough on a pendulum balanced precariously above its point of support, it
will fall. Of course, in practice it is impossible to balance a pendulum in

9.1 INTRODUCTION: THE SIMPLE PENDULUM
221
this way, precisely because the equilibrium is unstable. We will reﬁne our
deﬁnitions of stability and instability in Chapter 13.
(ii) Periodic solutions Integral paths with −mgl < E < mgl are closed, and
represent periodic solutions. They are often referred to as limit cycles or
periodic orbits. The pendulum swings back and forth without reaching the
upward vertical. These orbits are stable, since nearby orbits remain nearby.
Note that the frequency of the oscillation depends upon its amplitude, a
situation that is typical for nonlinear oscillators. For the simple pendulum,
the amplitude of the motion is θmax = cos−1 (E/mgl), and (9.4) shows that
the period of the motion, T, is given by
T = 2
 θmax
−θmax
dθ
$
2E
ml2 + 2g
l cos θ
.
Small closed trajectories in the neighbourhood of the equilibrium point at
(0, 0) are described by simple harmonic motion, (9.1). Note that, in contrast
to the full nonlinear system, the frequency of simple harmonic motion is
independent of its amplitude. The idea of linearizing a nonlinear system
of ordinary diﬀerential equations close to an equilibrium point in order to
determine what the phase portrait looks like there, is one that we will return
to later.
In terms of the phase portrait on the cylindrical surface, trajectories with
E > mgl are also stable periodic solutions, looping round and round the
cylinder.
The pendulum has enough kinetic energy to swing round and
round its point of support.
(iii) Heteroclinic solutions The two integral paths that connect (−π, 0) and
(π, 0) have E = mgl. The path with ˙θ ⩾0 has θ →±π as t →±∞(we
will prove this later). This solution represents a motion where the pendu-
lum swings around towards the upward vertical, and is just caught between
falling back and swinging over. This is known as a heteroclinic path, since
it connects diﬀerent equilibrium points.† Heteroclinic paths are important,
because they represent the boundaries between qualitatively diﬀerent types
of behaviour. They are also unstable, since nearby orbits behave qualita-
tively diﬀerently. Here, the heteroclinic orbits separate motions where the
pendulum swings back and forth from motions where it swings round and
round (diﬀerent types of periodic solution). In terms of the phase portrait on
a cylindrical surface, we can consider these paths to be homoclinic paths,
since they connect an equilibrium point to itself.‡
We have now seen that, if we can determine the qualitative nature of the phase
portrait of a second order nonlinear ordinary diﬀerential equation and sketch it, we
can extract a lot of information about the qualitative behaviour of its solutions.
This information provides rather more insight than the analytical solution, (9.4),
† Greek hetero = diﬀerent
‡ Greek homo = same

222
PHASE PLANE METHODS
into the behaviour of the physical system that the equation models. For nonconser-
vative equations, we cannot integrate the equation directly to get at the equation
of the integral paths, and we have to be rather more cunning in order to sketch the
phase portrait. We will develop methods to tackle second order, nonconservative,
ordinary diﬀerential equations in Section 9.3.
Before that, we will consider the
simpler case of ﬁrst order nonlinear ordinary diﬀerential equations.
9.2
First Order Autonomous Nonlinear Ordinary Diﬀerential
Equations
An autonomous ordinary diﬀerential equation is one in which the independent
variable does not appear explicitly, for example (9.2). The equations ¨θ+t2θ = 0 and
˙x = t−x2 are nonautonomous. Note, however, that an nth order nonautonomous
ordinary diﬀerential equation can always be written as a (n + 1)th order system of
autonomous ordinary diﬀerential equations. For example ˙x = t −x2 is equivalent
to ˙x = y −x2, ˙y = 1 with y = 0 when t = 0.
In this section we focus on the qualitative behaviour of solutions of ﬁrst order,
autonomous, ordinary diﬀerential equations, which can always be written as
dx
dt = ˙x = X(x),
(9.5)
with X(x) a given function of x. Of course, such an equation is separable, with the
solution subject to x(0) = x0 given by
t =
 x
x0
dx′
X(x′).
As we found in the previous section, solving the equation analytically is not nec-
essarily the easiest way to determine the qualitative behaviour of the system (see
Exercise 9.2).
9.2.1
The Phase Line
Consider the graph of the function X(x). An example is shown in Figure 9.3(a).
If X(x1) = 0, then ˙x = 0, and hence x = x1 is an equilibrium solution of (9.5).
For the example shown in Figure 9.3 there are three equilibrium points, at x = x1,
x2 and x3.
We can also see that ˙x = X(x) < 0, and hence x is a decreasing
function of t for x < x1 and x2 < x < x3. Similarly, ˙x = X(x) > 0, and hence
x increases as t increases, for x1 < x < x2 and x > x3. By analogy with the
phase plane, where we constructed the phase portrait for a second order system
in the previous section, we can draw a phase line for this ﬁrst order equation, as
shown in Figure 9.3(b). The arrows indicate whether x increases or decreases with
t. Clearly, the diﬀerent types of behaviour that are possible are rather limited by
the constraint that the trajectories lie in a single dimension. Both for this example
and in general, trajectories either enter an equilibrium point or head oﬀto inﬁnity.

9.2 FIRST ORDER EQUATIONS
223
In particular, periodic solutions are not possible.† Solutions that begin close to
x = x1 or x = x3 move away from the equilibrium point, so that x = x1 and x = x3
are unstable. In contrast, x = x2 is a stable equilibrium point, since solutions that
start close to it approach it.
x2
x1
x1
x2
x3
x
X(x)
(a)
(b)
•
•
x3
•
Fig. 9.3. (a) An example of a graph of ˙x = X(x). (b) The equivalent phase line.
From Figure 9.3(b) we can see that
x →−∞as t →∞
when x0 < x1,
x →x2 as t →∞
when x1 < x0 < x3,
x →∞as t →∞
when x0 > x3.
The set D−∞= {x | x < x1} is called the domain of attraction or basin of
attraction of minus inﬁnity. Similarly, D2 = {x | x1 < x < x3} is the domain of
attraction of x2 and D∞= {x | x > x3} that of plus inﬁnity. All of this information
has come from a qualitative analysis of the graph of ˙x = X(x).
9.2.2
Local Analysis at an Equilibrium Point
If x = x1 is an equilibrium solution of (9.5), what happens quantitatively close to
x1? Firstly, let’s move the equilibrium point to the origin by deﬁning ¯x = x −x1,
† If there is some geometrical periodicity in the problem so that, for example, by analogy with
the simple pendulum, −π ⩽x ⩽π and x = −π is equivalent to x = π, we can construct a
phase loop, around which the solution can orbit indeﬁnitely.

224
PHASE PLANE METHODS
so that
d¯x
dt = X(x1 + ¯x).
We can expand X(x1 + ¯x) as a Taylor series,
X(x1 + ¯x) = X(x1) + ¯xX′(x1) + 1
2 ¯x2X′′(x1) + · · · ,
(9.6)
where X′ = dX/dx. In the neighbourhood of the equilibrium point, ¯x ≪1, and
hence
d¯x
dt ≈X(x1) + ¯xX′(x1).
Since x = x1 is an equilibrium solution, X(x1) = 0, and, assuming that X′(x1) ̸= 0,
d¯x
dt ≈X′(x1)¯x
for ¯x ≪1.
This has solution ¯x = k exp {X′(x1)t} for some constant k. If X′(x1) < 0, ¯x →0
(x →x1) as t →∞, and the equilibrium point is therefore stable. If X′(x1) > 0,
¯x →0 (x →x1) as t →−∞, and the equilibrium point is therefore unstable. This
is consistent with our qualitative analysis (note the slope of X(x) at the equilibrium
points in Figure 9.3(a)). Moreover, we now know that solutions approach stable
equilibrium points exponentially fast as t →∞, a piece of quantitative information
well worth knowing.
If X′(x1) ̸= 0, we say that x = x1 is a hyperbolic equilibrium point, and
this analysis determines how solutions behave in its neighbourhood. In particular,
solutions that do not start at a hyperbolic equilibrium point cannot reach it in a
ﬁnite time. If X′(x1) = 0, x = x1 is a nonhyperbolic equilibrium point, and
we need to retain more terms in the Taylor expansion of X, (9.6), in order to sort
out what happens close to the equilibrium point. We will consider this further in
Chapter 13.
9.3
Second Order Autonomous Nonlinear Ordinary Diﬀerential
Equations
Any second order, autonomous, ordinary diﬀerential equation can be written as a
system of two ﬁrst order equations, in the form
˙x = X(x, y),
˙y = Y (x, y).
(9.7)
For example, consider (9.2), which governs the motion of a simple pendulum. Let’s
deﬁne x = θ and y = ˙x. Now, ˙y = ¨x = ¨θ = −(g/l) sin θ, and hence
˙x = y,
˙y = −g
l sin x.
From now on, we will assume that the right hand sides in (9.7) are continu-
ously diﬀerentiable†, and hence that a solution exists and is unique in the sense of
Theorem 8.2, for all of the systems that we study.
† Continuously diﬀerentiable functions are continuous and have derivatives with respect to the
independent variables that are continuous.

9.3 SECOND ORDER EQUATIONS
225
9.3.1
The Phase Plane
In Section 9.1, we saw how the solutions of the equations of motion of a simple
pendulum can be represented in the phase plane. Let’s now consider the phase
plane for a general second order system.
An integral path is a solution (x(t), y(t)) of (9.7), plotted in the (x, y)-plane, or
phase plane. The slope of an integral path is
dy
dx = dy/dt
dx/dt = ˙y
˙x = Y (x, y)
X(x, y).
This slope is uniquely deﬁned at all points x = x0 and y = y0 at which X(x0, y0)
and Y (x0, y0) are not both zero. These are known as ordinary points. Since
the solution through such a point exists and is unique, we deduce that integral
paths cannot cross at ordinary points. This is possibly the most useful piece of
information that you need to bear in mind when sketching phase portraits. Integral
paths cannot cross at ordinary points!
Points (x0, y0) that are not ordinary have X(x0, y0) = Y (x0, y0) = 0, and are
therefore equilibrium points. In other words, ˙x = ˙y = 0 when x = x0, y = y0, and
this point represents an equilibrium solution of (9.7). At an equilibrium point, the
slope, dy/dx, of the integral paths is not well-deﬁned, and we deduce that integral
paths can meet only at equilibrium points, and only as t →±∞. This will become
clearer in the next section.
9.3.2
Equilibrium Points
In order to determine what the phase portrait looks like close to an equilibrium
point, we proceed as we did for ﬁrst order systems. We begin by shifting the origin
to an equilibrium point at (x0, y0), using
x = x0 + ¯x,
y = y0 + ¯y.
Now we Taylor expand the functions X and Y , remembering that X(x0, y0) =
Y (x0, y0) = 0, to obtain
X(x0 + ¯x, y0 + ¯y) = ¯x∂X
∂x (x0, y0) + ¯y ∂X
∂y (x0, y0) + · · · ,
Y (x0 + ¯x, y0 + ¯y) = ¯x∂Y
∂x (x0, y0) + ¯y ∂Y
∂y (x0, y0) + · · · .
In the neighbourhood of the equilibrium point, ¯x ≪1 and ¯y ≪1, and hence
d¯x
dt ≈¯xXx(x0, y0) + ¯yXy(x0, y0),
d¯y
dt ≈¯xYx(x0, y0) + ¯yYy(x0, y0),
(9.8)
where we have used the notation Xx = ∂X/∂x. The most convenient way of writing
this is in matrix form, as
du
dt = J(x0, y0)u,
(9.9)

226
PHASE PLANE METHODS
where
u =
 ¯x
¯y

,
J(x0, y0) =
 Xx(x0, y0)
Xy(x0, y0)
Yx(x0, y0)
Yy(x0, y0)

.
We call J(x0, y0) the Jacobian matrix at the equilibrium point (x0, y0). Equation
(9.9) represents a pair of linear, ﬁrst order ordinary diﬀerential equations with
constant coeﬃcients, which suggests that we can look for solutions of the form
u = u0eλt, where u0 and λ are constants to be determined. Substituting this form
of solution into (9.9) gives
λu0 = Ju0.
(9.10)
This is an eigenvalue problem. There are two eigenvalues, λ = λ1 and λ = λ2,
possibly equal, possibly complex, and corresponding unit eigenvectors u0 = u1 and
u0 = u2. These provide us with two possible solutions of (9.9), u = u1eλ1t and
u = u2eλ2t, so that the general solution is the linear combination
u = A1u1eλ1t + A2u2eλ2t,
(9.11)
for any constants A1 and A2.
When the eigenvalues λ1 and λ2 are real, distinct and nonzero, so are the eigen-
vectors u1 and u2, and (9.11) suggests that the form of the solution is simpler if
we make a linear transformation so that u1 and u2 lie along the coordinate axes.
Such a transformation is given by u = Pv, where v = (ˆx, ˆy) and P = (u1, u2) is a
matrix with columns given by u1 and u2. Substituting this into (9.9) gives
P ˙v = JPv,
˙v = P −1JPv = Λv,
where
Λ =
 λ1
0
0
λ2

.
Therefore ˙ˆx = λ1ˆx, ˙ˆy = λ2ˆy, so that
ˆx = k1eλ1t,
ˆy = k2eλ2t,
(9.12)
with k1 and k2 constants, and hence
ˆy = k3ˆxλ2/λ1,
(9.13)
where k3 = kλ2/λ1
1
/k2. This is the equation of the integral paths in the transformed
coordinates. Note that the transformed coordinate axes are integral paths. There
are now three main cases to consider.
(i) Distinct, real, positive eigenvalues (λ1, λ2 > 0) The solution (9.12)
shows that ˆx, ˆy →0 as t →−∞, exponentially fast, so the equilibrium point
is unstable. The equation of the integral paths, (9.13), then shows that they
all meet at the equilibrium point at the origin. The local phase portrait is
sketched in Figure 9.4, in both the transformed and untransformed coordi-
nate systems. This type of equilibrium point is called an unstable node.

9.3 SECOND ORDER EQUATIONS
227
Remember, this is the phase portrait close to the equilibrium point. As the
integral paths head away from the equilibrium point, the linearization that
we have performed in order to obtain (9.9) becomes inappropriate and we
must consider how this local phase portrait ﬁts into the full, global picture.
Figure 9.4(a) illustrates the situation when λ2 > λ1. This means that ˆy
grows more quickly than ˆx as t increases, and thereby causes the integral
paths to bend as they do.
Figure 9.4(b) shows that when λ2 > λ1, the
solution grows more rapidly in the u2-direction than the u1-direction.
x
x
y
y
(a)
(b)
u1
u2
−
−
Fig. 9.4. An unstable node with λ2 > λ1 sketched in (a) the transformed and (b) the
untransformed coordinate systems.
(ii) Distinct, real, negative eigenvalues (λ1, λ2 < 0) In this case, all we
have to do is consider the situation with the sense of t reversed, and we
recover the previous case. The situation is as shown in Figure 9.4, but with
the arrows reversed. This type of equilibrium point is called a stable node.
(iii) Real eigenvalues of opposite sign (λ1λ2 < 0) In this case, the coordinate
axes are the only integral paths in the (ˆx, ˆy)-plane that enter the equilibrium
point. On the other integral paths, given by (9.13), ˆx →±∞as ˆy →0, and
vice versa, as shown in Figure 9.5(a).
When λ2 > 0 > λ1, ˆx →0 and
ˆy →±∞as t →∞. The integral paths in the directions of the eigenvectors
u1 and u2, shown in Figure 9.5(b), are therefore the only ones that enter the
equilibrium point, and are called the stable and unstable separatrices†.
This type of equilibrium point is called a saddle point. The separatrices
of saddle points usually represent the boundaries between diﬀerent types
of behaviour in a phase portrait. See if you can spot the saddle points in
Figure 9.2(b). Remember, although the separatrices are straight trajectories
† singular form, separatrix.

228
PHASE PLANE METHODS
in the neighbourhood of the saddle point, they will start to bend as they
head away from it and become governed by the full nonlinear equations.
x
y
x
y
(b)
(a)
u2
u1
−
−
Fig. 9.5. A saddle point with λ2 > 0 > λ1 sketched in (a) the transformed and (b) the
untransformed coordinate systems.
The degenerate case where λ1 = λ2 is slightly diﬀerent from that of a stable
or unstable node, and we will not consider it here, but refer you to Exercise 9.4.
However, it should be clear that when λ1 = λ2 > 0 the equilibrium point is unstable,
whilst for λ1 = λ2 < 0 it is stable.
Let’s now consider what the phase portrait looks like when the eigenvalues λ1
and λ2 are complex. Since the eigenvalues are the solutions of a quadratic equation
with real coeﬃcients, they must be complex conjugate, so we can write λ = α ± iβ,
with α and β real. The general solution, (9.11), then becomes
u = eαt 
A1u1eiβt + A2u2e−iβt
.
(9.14)
There are two cases to consider.
(i) Eigenvalues with strictly positive real part (α > 0) The term eαt
in (9.14) means that the solution grows exponentially with t, so that the
equilibrium point is unstable. The remaining term in (9.14) is oscillatory,
and we conclude that the integral paths spiral away from the equilibrium
point, as shown in Figure 9.6. This type of equilibrium point is called an
unstable spiral or unstable focus. To determine whether the sense of
rotation is clockwise or anticlockwise, it is easiest just to consider the sign
of d¯y/dt on the positive ¯x-axis. When ¯y = 0, (9.8) shows that d¯y/dt =
Xx(x0, y0)¯x, and hence the spiral is anticlockwise if Xx(x0, y0) > 0, clockwise
if Xx(x0, y0) < 0.†
† If Xx(x0, y0) = 0, try looking on the positive ¯y-axis.

9.3 SECOND ORDER EQUATIONS
229
Fig. 9.6. An unstable, anticlockwise spiral.
(ii) Eigenvalues with strictly negative real part (α < 0) This is just the
same as the previous case, but with the sense of t reversed. This is a stable
spiral or stable focus,
and has the phase portrait shown in Figure 9.6,
but with the arrows reversed.
Note that, for complex conjugate eigenvalues, we can transform the system into a
more convenient form by deﬁning the transformation matrix, P, slightly diﬀerently.
If we choose either of the eigenvectors, for example u1, and write λ1 = µ + iω, we
can deﬁne
P = (Im(u1), Re(u1)) ,
and u = Pv. As before, ˙v = P −1JPv, but now
JP = J (Im(u1), Re(u1)) = (Im(Ju1), Re(Ju1)) = (Im(λ1u1), Re(λ1u1))
= (µIm(u1) + ωRe(u1), µRe(u1) −ωIm(u1)) = P

µ
−ω
ω
µ

,
and hence
P −1JP =

µ
−ω
ω
µ

.

230
PHASE PLANE METHODS
If v = (v1, v2)T, then
˙v1 = µv1 −ωv2,
˙v2 = ωv1 + µv2,
and, eliminating v2,
¨v1 −2µ˙v1 +

ω2 + µ2
v1 = 0.
This is the usual equation for damped simple harmonic motion, with natural fre-
quency ω and damping given by −µ.
Note that all of the above analysis is rather informal, since it is based on a simple
linearization about the equilibrium point. Can we guarantee that the behaviour
that we have deduced persists when we study the full, nonlinear system, (9.7)?
Yes, we can, provided that the equilibrium point is hyperbolic. We can formalize
this in the following theorem, which holds for autonomous nonlinear systems of
arbitrary order.
Theorem 9.1 (Hartman–Grobman) If ¯x is a hyperbolic equilibrium point of the
nth order system ˙x = f(x), then there is homeomorphism (a mapping that is one-to-
one, onto and continuous and has a continuous inverse) from Rn to Rn deﬁned in
a neighbourhood of ¯x that maps trajectories of the nonlinear system to trajectories
of the local linearized system.
We will not give the proof, but refer the interested reader to the original papers by
Hartman (1960) and Grobman (1959).
Each of the ﬁve cases we have discussed above is indeed an example of a hyper-
bolic equilibrium point. If at least one eigenvalue has zero real part, the equilibrium
point is said to be nonhyperbolic. As we shall see in Chapter 13, the behaviour in
the neighbourhood of a nonhyperbolic equilibrium point is determined by higher
order, nonlinear, terms in the Taylor expansions of X and Y .
An important example where it may not be necessary to consider higher order
terms in the Taylor expansions is when the eigenvectors are purely imaginary, with
λ = ±iβ. In this case, (9.14) with α = 0 shows that the solution is purely oscillatory.
The integral paths are closed and consist of a set of nested limit cycles around the
equilibrium point. This type of equilibrium point is called a centre. However, the
eﬀect of higher order, nonlinear, terms in the Taylor expansions may be to make
the local solutions spiral into or out of the equilibrium point. We say that the
equilibrium point is a linear centre, but a nonlinear spiral. On the other hand,
there are many physical systems where a linear centre persists, even in the presence
of the higher order, nonlinear terms, and is called a nonlinear centre.
As an example, consider the simple pendulum, with
˙x = y,
˙y = −g
l sin x.
This has
J =

0
1
−g
l cos x
0

and
J(0, 0) =

0
1
−g
l
0

.

9.3 SECOND ORDER EQUATIONS
231
The eigenvalues of J(0, 0) are λ = ±i
#
g/l. As we can see in Figure 9.2, the phase
portrait close to the origin is indeed a nonlinear centre. What is it about this system
that allows the centre to persist in the full nonlinear analysis? Well, the obvious
answer is that there is a conserved quantity, the energy, which parameterizes the set
of limit cycles. However, another way of looking at this is to ask whether the system
has any symmetries. Here we know that if we map x →−x and t →−t, reﬂecting
the phase portrait in the y-axis and reversing time, the equations are unchanged,
and hence the phase portrait should not change. Under this transformation, a stable
spiral would become an unstable spiral, and vice versa, because of the reversal of
time. Therefore, since the phase portrait should not change, the origin cannot be
a spiral and must be a nonlinear centre.
9.3.3
An Example from Mechanics
Consider a rigid block of mass M attached to a horizontal spring. The block
lies ﬂat on a horizontal conveyor belt that moves at speed U and tries to carry the
block away with it, as shown in Figure 9.7. From Newton’s second law of motion
and Hooke’s law,
M ¨x = F( ˙x) −k(x −xe),
where x is the length of the spring, xe is the equilibrium length of the spring, k is
the spring constant, F( ˙x) is the frictional force exerted on the block by the conveyor
belt, and a dot denotes d/dt. We model the frictional force as
F( ˙x) =

F0
for ˙x < U,
−F0
for ˙x > U,
with F0 a constant force. When ˙x = U, the block moves at the same speed as
the conveyor belt, and this occurs when k|x −xe| < F0. In other words, the force
exerted by the spring must exceed the frictional force for the block to move. This
immediately gives us a solution, ˙x = U for xe −F0/k ⩽x ⩽xe + F0/k.
spring
Block,
mass M
Friction
here
U
x
Fig. 9.7. A spring-mounted, rigid block on a conveyor belt.

232
PHASE PLANE METHODS
Our model involves ﬁve physical parameters, M, k, xe, F0 and U. If we now
deﬁne dimensionless variables ¯x = x/xe and ¯t = t/
#
M/k, we obtain
¨¯x = ¯F −¯x + 1,
(9.15)
where
¯F(˙¯x) =

¯F0
for ˙¯x < ¯U,
−¯F0
for ˙¯x > ¯U.
(9.16)
We also have the possible solution ˙¯x = ¯U for 1 −¯F0 ⩽¯x ⩽1 + ¯F0. There are now
just two dimensionless parameters,
¯F0 = F0
kxe
,
¯U = U
xe
*
M
k .
We can now write (9.15) as the system
˙x = y,
˙y = F(y) −x + 1.
(9.17)
We have left out the overbars for notational convenience. This system has a single
equilibrium point at x = 1 + F0, y = 0. Since y = 0 < ¯U, the system is linear in
the neighbourhood of this point, with
 ˙x
˙y

=

0
1
−1
0
  x
y

+

0
F0 + 1

.
The Jacobian matrix has eigenvalues ±i, so the equilibrium point is a linear centre.
In fact, since ˙x (x −1 −F) + y ˙y = 0, we can integrate to obtain
{x −(1 + F0)}2 + y2 = constant
for y < U,
{x −(1 −F0)}2 + y2 = constant
for y > U.
(9.18)
The solutions for y ̸= U are therefore concentric circles, and we conclude that the
equilibrium point remains a centre when we take into account the eﬀect of the
nonlinear terms. The phase portrait is sketched in Figure 9.8.
We now need to take some care with integral paths that meet the line y = U.
Since the right hand side of (9.17) is discontinuous at y = U, the slope of the
integral paths is discontinuous there. For x < 1 −F0 and x > 1 + F0, trajectories
simply cross the line y = U. However, we have already seen that the line y = U
for 1 −F0 ⩽x ⩽1 + F0 is itself a solution. We conclude that an integral path that
meets y = U with x in this range follows this trajectory until x = 1 + F0, when
it moves oﬀon the limit cycle through the point D in Figure 9.8. For example,
consider the trajectory that starts at the point A. This corresponds to an initially
stationary block, with the spring stretched so far that it can immediately overcome
the frictional force.
The solution follows the circular trajectory until it reaches
B. At this point, the direction of the frictional force changes, and the solution
follows a diﬀerent circular trajectory, until it reaches C. At this point, the block is
stationary relative to the conveyor belt, which carries it along with it until the spring
is stretched far enough that the force it exerts exceeds the frictional force. This
occurs at the point D. Thereafter, the solution remains on the periodic solution

9.3 SECOND ORDER EQUATIONS
233
through D. On this periodic solution, the speed of the block is always less than
that of the conveyor belt, so the frictional force remains constant, and the block
undergoes simple harmonic motion.
x
y
D
C
B
A
•
Fig. 9.8. The phase portrait for a spring-mounted, rigid block on a conveyor belt.
9.3.4
Example: Population Dynamics
Consider two species of animals that live on an island with populations X(t)
and Y (t). If P is a typical size of a population, we can deﬁne x(t) = X(t)/P and
y(t) = Y (t)/P as dimensionless measures of the population of each species, and
regard x and y as continuous functions of time, t. A model for the way in which
the two species interact with each other and their environment is
˙x = x (A + a1x + b1y) ,
˙y = y (B + b2x + a2y) ,
(9.19)
where A, B, a1, b1, a2 and b2 are constants. We can interpret each of these equations
as
Rate of change of population = Present population × (Birth rate −Death rate) .
Let’s now consider what each of the terms that model the diﬀerence between the
birth and death rates represents. If A > 0, the population of species x grows when
x ≪1 and y ≪1, so we can say that x does not rely on eating species y to survive.
In contrast, if A < 0, the population of x dies out, and therefore x must need to
eat species y to survive. The term a1x represents the eﬀect of overcrowding and
competition for resources within species x, so we require that a1 < 0. The term
b1y represents the interaction between the two species. If species x eats species y,
b1 > 0, so that the more of species y that is available, the faster the population of
x grows. If species y competes with x for the available resources, b1 < 0.

234
PHASE PLANE METHODS
We will consider two species that do not eat each other, but compete for re-
sources, for example sheep and goats (see Exercise 9.10 for an example of a predator–
prey system). Speciﬁcally, we will study the typical system
˙x = x(3 −2x −2y),
˙y = y(2 −2x −y).
(9.20)
We will use this system to illustrate the full range of techniques that are available
to obtain information about the phase portrait of second order systems. The ﬁrst
thing to do is to determine where any equilibrium points are, and what their types
are. This will tell us almost everything we need to know in order to sketch the
phase portrait, as integral paths can only meet at equilibrium points. Equilibrium
points are the main structural features of the system, and the integral paths are
organized around them.
At equilibrium points, ˙x = ˙y = 0, and hence
(x = 0 or 2x + 2y = 3) and (y = 0 or 2x + y = 2) .
The four diﬀerent possibilities show that there are four equilibrium points, P1 =
(0, 0), P2 = (0, 2), P3 = (3/2, 0) and P4 = (1/2, 1). The Jacobian matrix is
J =
 Xx
Xy
Yx
Yy

=
 3 −4x −2y
−2x
−2y
2 −2x −2y

,
and hence
J(0, 0) =
 3
0
0
2

,
J(0, 2) =
 −1
0
−4
−2

,
J(3/2, 0) =
 −3
−3
0
−1

,
J(1/2, 1) =
 −1
−1
−2
−1

.
Three of these matrices have at least one oﬀ-diagonal element equal to zero, so
their eigenvalues can be read directly from the diagonal elements. P1 has eigenval-
ues 3 and 2 and is therefore an unstable node. P2 has eigenvalues −1 and −2 and
is therefore a stable node. P3 has eigenvalues −3 and −1 and is therefore also a
stable node. We could work out the direction of the eigenvectors for these three
equilibrium points, but they are not really important for sketching the phase por-
trait. The ﬁnal equilibrium point, P4, has eigenvalues λ = λ± = −1 ±
√
2. Since
√
2 > 1, P4 has one positive and one negative eigenvalue, and is therefore a saddle
point. For saddle points it is usually important to determine the directions of the
eigenvectors, since these determine the directions in which the separatrices leave
the equilibrium point. These separatrices are the only points that meet P4, and
we will see that determining their global behaviour is the key to sketching the full
phase portrait. The unit eigenvectors of P4 are u± = (
#
1/3, ∓
#
2/3)T.
Next, we can consider the nullclines. These are the lines on which either ˙x = 0
or ˙y = 0. The vertical nullclines, where ˙x = 0 (only y is varying so the integral
paths are vertical on the nullcline), are given by
x = 0 or 2x + 2y = 3.

9.3 SECOND ORDER EQUATIONS
235
The horizontal nullclines, where ˙y = 0, are given by
y = 0 or 2x + y = 2.
In general, the nullclines are not integral paths. However, in this case, since ˙x = 0
when x = 0 and ˙y = 0 when y = 0, the coordinate axes are themselves integral
paths. This is clear from the biology of the problem, since, if species x is absent
initially, there is no way for it to appear spontaneously, and similarly for species y.
All of the information we have amassed so far is shown in Figure 9.9(a). We
will conﬁne our attention to the positive quadrant, because we are not interested
in negative populations. Moreover, the fact that the coordinate axes are integral
paths prevents any trajectory from leaving the positive quadrant. Note that the
equilibrium points lie at the points where the nullclines intersect. The directions
of the arrows are determined by considering the signs of ˙x and ˙y at any point.
For example, on the x-axis, ˙x = x(3 −2x), so that ˙x > 0, which means that x is
increasing with t, for 0 < x < 3/2, and ˙x < 0, which means that x is decreasing
with t, for x > 3/2. From all of this local information, and the fact that integral
paths can only cross at equilibrium points, we can sketch a plausible and consistent
phase portrait, as shown in Figure 9.9(b). Apart from the stable separatrices of
the saddle point P4, labelled S1 and S2, all trajectories asymptote to either P2 or
P3 as t →∞. These separatrices therefore represent the boundary between two
very diﬀerent types of behaviour. Depending upon the initial conditions, either
species x or species y dies out completely. Although neither species eats the other,
by competing for the same resources there can only be one eventual winner in the
competition to stay alive.
Although this is obviously a very simple model, the
displacement of the native red squirrel by the grey squirrel in Britain, and the
extinction of Neanderthal man after the arrival of modern man in Europe, are
examples of situations where two species competed for the same resources. It is
not necessary that one of the species kills the other directly. Simply a suﬃcient
advantage in numbers or enough extra eﬃciency in exploiting natural resources
could have been enough for modern man to consign Neanderthal man to oblivion.
Returning to our simple model, there are some questions that we really ought
to answer before we can be conﬁdent that Figure 9.9(b) is an accurate sketch of
the phase portrait of (9.20). Firstly, can we be sure that there are no limit cycle
solutions? An associated question is, if we think that a system does possess a limit
cycle, can we prove it? Secondly, since the position of the stable separatrices of P4 is
so important, can we prove that we have sketched them correctly? More speciﬁcally,
how do we know that S1 originates at P1 and that S2 originates at inﬁnity? Finally,
what does the phase portrait look like far from the origin? We will consider some
mathematical tools for answering these questions in the next four sections. We can,
however, also test whether our phase portrait is correct by solving equations (9.19)
numerically in MATLAB. We must ﬁrst deﬁne the MATLAB function

236
PHASE PLANE METHODS
P4
P3
P1
P2
P3
P2
S2
S1
P4
P1
y
x
y
(a)
(b)
x
x + y = 3/2
2x + y = 2
Fig. 9.9. (a) Local information at the equilibrium points and the nullclines, and (b) the
full phase portrait for the population dynamical system (9.20).




function dy = population(t,y)
dy(1) = y(1)*(3-2*y(1)-2*y(2));
dy(2) = y(2)*(2-2*y(1)-
y(2));
dy=dy’;

9.3 SECOND ORDER EQUATIONS
237
which returns a column vector with elements given by the right hand sides of
(9.19). Then [t y] = ode45(@population, [0 10], [1 2]) integrates the equa-
tions numerically, in this case for 0 ⩽t ⩽10, with initial conditions x = y(1) = 1,
y = y(2) = 2. The results can then be displayed with plot(t,y). Figure 9.10
shows the eﬀect of holding the initial value of x ﬁxed, and varying the initial value
of y. For small enough initial values of y, y →0 as t →∞. However, when the
initial value of y is suﬃciently large, the type of behaviour changes, and x →0 as
t →∞, consistent with the phase portrait sketched in Figure 9.9(b).
Fig. 9.10. The solution of (9.19) for various initial conditions. The dashed line is y, the
solid line is x.
9.3.5
The Poincar´e Index
The Poincar´e index of an equilibrium point provides a very simple way of deter-
mining whether a given equilibrium point or collection of equilibrium points can be
surrounded by one or more limit cycles.
Consider the second order system of nonlinear, autonomous ordinary diﬀerential
equations given by (9.7). Let Γ be any smooth, closed, nonself-intersecting curve in
the (x, y)-phase plane, not necessarily an integral path, that does not pass through
any equilibrium points, as shown in Figure 9.11. At any point P lying on Γ, there

238
PHASE PLANE METHODS
is a unique integral path through P, with a well-deﬁned direction that we can
characterize using the angle, ψ(P), that it makes with the horizontal (tan ψ(P) =
Y/X). The angle ψ varies continuously as P moves around Γ. If ψ changes by an
amount 2nΓπ as P makes one complete, anticlockwise circuit around Γ, we call nΓ
the Poincar´e index, which must be an integer.
P
Γ
ψ
Fig. 9.11. The deﬁnition of the Poincar´e index.
We can now deduce some properties of the Poincar´e index.
(i) If Γ is continuously distorted without passing through any equilibrium points,
nΓ does not change.
Proof
Since nΓ must vary continuously under the action of a continuous
distortion, but must also be an integer, the Poincar´e index cannot change
from its initial value.
(ii) If Γ does not enclose any equilibrium points, nΓ = 0.
Proof Using the previous result, we can continuously shrink Γ down to an
arbitrarily small circle without changing nΓ. On this circle, ψ(P) is almost
constant, and therefore nΓ = 0.
(iii) If Γ is the sum of two closed curves, then Γ1 and Γ2, nΓ = nΓ1 + nΓ2.
Proof
Consider the curves shown in Figure 9.12. On the curve where Γ1
and Γ2 intersect, the amount by which ψ varies on traversing Γ1 is equal
and opposite to the amount by which ψ varies on traversing Γ2, so these
contributions cancel and nΓ = nΓ1 + nΓ2.
(iv) If Γ is a limit cycle then nΓ = 1.
Proof This result is obvious.
(v) If integral paths either all enter the region enclosed by Γ or all leave this
region, nΓ = 1.
Proof This result is also obvious.

9.3 SECOND ORDER EQUATIONS
239
Γ1
Γ2
Fig. 9.12. The sum of two curves.
(vi) If Γ encloses a single node, focus or nonlinear centre, nΓ = 1.
Proof
For a node or focus, Γ can be continuously deformed into a small
circle surrounding the equilibrium point. The linearized solution then shows
that integral paths either all enter or all leave the region enclosed by Γ, and
the previous result shows that nΓ = 1. For a nonlinear centre, Γ can be con-
tinuously deformed into one of the limit cycles that encloses the equilibrium
point, and result (iv) shows that nΓ = 1.
(vii) If Γ encloses a single saddle point, nΓ = −1.
Proof Γ can be continuously deformed into a small circle surrounding the
saddle point, where the linearized solution gives the phase portrait shown
in Figure 9.5. The direction of the integral paths makes a single clockwise
rotation as P traverses Γ, and hence nΓ = −1.
These results show that we can deﬁne the Poincar´e index of an equilibrium
point to be the Poincar´e index of any curve that encloses the equilibrium point and
no others. In particular, a node, focus or nonlinear centre has Poincar´e index n = 1,
whilst a saddle has Poincar´e index n = −1. Nonhyperbolic equilibrium points can
have other Poincar´e indices, but we will not consider these here. Now result (iii)
shows that nΓ is simply the sum of the Poincar´e indices of all the equilibrium points
enclosed by Γ. Finally, and this is what all of this has been leading up to, result (iv)
shows that the sum of the Poincar´e indices of the equilibrium points enclosed by a
limit cycle must be 1. A corollary of this is that a limit cycle must enclose at least
one node, focus or centre.
Now let’s return to our example population dynamical system, (9.20). If a limit
cycle exists, it cannot cross the coordinate axes, since these are integral paths.
However, there is no node, focus or nonlinear centre in the positive quadrant of the
phase plane, and we conclude that no limit cycle exists.

240
PHASE PLANE METHODS
When a system does possess a node, focus or nonlinear centre, the Poincar´e
index can be used to determine which sets of equilibrium points a limit cycle could
enclose if it existed, but it would be useful to have a way of ruling out the existence
of limit cycles altogether, if indeed no limit cycles exist. We now consider a way of
doing this that works for many types of system.
9.3.6
Bendixson’s Negative Criterion and Dulac’s Extension
Theorem 9.2 (Bendixson’s negative criterion) For the second order system of
nonlinear, autonomous ordinary diﬀerential equations given by (9.7), there are no
limit cycles in any simply-connected region of the (x, y)-phase plane where Xx + Yy
does not change sign.
Proof Let Γ be a limit cycle. For the vector ﬁeld (X(x, y), Y (x, y)), Stokes’ theorem
states that

D
∂X
∂x + ∂Y
∂y

dx dy =

Γ
(Xdy −Y dx) ,
where D is the region enclosed by Γ. However, we can write the right hand side of
this as

Γ

X dy
dt −Y dx
dt

dt =

Γ
(XY −Y X) dt = 0,
and hence

D
∂X
∂x + ∂Y
∂y

dx dy = 0.
If Xx + Yy does not change sign in D, then the integral is either strictly positive or
strictly negative†, which is a contradiction, and hence the result is proved.
Note that the restriction of this result to simply-connected regions (regions without
holes) is crucial.
If we now apply this theorem to the system (9.20), we ﬁnd that Xx + Yx =
5−6x−4y. Although this tells us that any limit cycle solution must be intersected
by the line 6x + 4y = 5, we cannot rule out the possibility of the existence of
limit cycles in this way. We need a more powerful version of Bendixson’s negative
criterion, which is provided by the following theorem.
Theorem 9.3 (Dulac’s extension to Bendixson’s negative criterion) If
ρ(x, y) is continuously diﬀerentiable in some simply-connected region D in the
(x, y)-phase plane, there are no limit cycle solutions if (ρX)x + (ρY )y does not
change sign.
† ignoring the degenerate case where Xx + Yy ≡0 in D.

9.3 SECOND ORDER EQUATIONS
241
Proof Since
 
D
∂(ρX)
∂x
+ ∂(ρY )
∂y

dx dy =

Γ
ρ (Xdy −Y dx) ,
the proof is as for Bendixson’s negative criterion.
Returning again to (9.20), we can choose ρ = 1/xy and consider the positive
quadrant where ρ(x, y) is continuously diﬀerentiable. We ﬁnd that
∂(ρX)
∂x
+ ∂(ρY )
∂y
= −1
x −2
y < 0,
and hence that there can be no limit cycle solutions in the positive quadrant, as
expected.
Now that we know how to try to rule out the existence of limit cycles, how can
we go about showing that limit cycles do exist for appropriate systems? We will
consider this in the next section using the Poincar´e–Bendixson theorem. This will
also allow us to prove that the stable separatrices of P4, shown in Figure 9.9, do
indeed behave as sketched.
9.3.7
The Poincar´e–Bendixson Theorem
We begin with a deﬁnition. Consider the second order system (9.7). If I+ is
a closed subset of the phase plane and any integral path that lies in I+ when
t = 0 remains in I+ for all t ⩾0, we say that I+ is a positively invariant set.
Similarly, a negatively invariant set is a closed subset, I−, of the phase plane
and all integral paths in I−when t = 0 remain there when t < 0. For example, an
equilibrium point is both a positively and a negatively invariant set, as is a limit
cycle. As a less prosaic example, any subset, S, of the phase plane with outward
unit normal n that has (X, Y ) · n ⩽0 on its boundary, so that all integral paths
enter S, is a positively invariant set.
Theorem 9.4 (Poincar´e–Bendixson) If there exists a bounded, invariant region,
I, of the phase plane, and I contains no equilibrium points, then I contains at least
one limit cycle.
Note that it is crucial that the region I be bounded, but, in contrast to Bendix-
son’s negative criterion, I does not need to be simply-connected. As we shall see,
if I contains a limit cycle, it cannot be simply-connected. The Poincar´e–Bendixson
theorem says that the integral paths in I cannot wander around for ever without
asymptoting to a limit cycle. Although this seems obvious, we shall see in Chap-
ter 15 that the Poincar´e–Bendixson theorem does not hold for third and higher
order systems, in which integral paths that represent chaotic solutions can indeed
wander around indeﬁnitely without being attracted to a periodic solution.
The details of the proof are rather technical, and can be omitted on ﬁrst reading.

242
PHASE PLANE METHODS
Proof of the Poincar´e–Bendixson Theorem
We will assume that I is a positively invariant region. The proof when I is negatively
invariant is identical, but with time reversed. We now need two further deﬁnitions.
Let x(t; x0) be the solution of (9.7) with x = x0 when t = 0. We say that x1
is an ω-limit point of x0 if there exists a sequence, {ti}, such that ti →∞and
x(ti; x0) →x1 as i →∞. For example, all the points on an integral path that enters
a stable node or focus at x = xe have xe as their only ω-limit point. Similarly, all
points on an integral path that asymptotes to a stable limit cycle as t →∞have
all the points that lie on the limit cycle as ω-limit points. We call the set of all
ω-limit points of x0 the ω-limit set of x0, which is denoted by ω(x0).
Let L be a ﬁnite curve such that all integral paths that meet L cross it in the
same direction. We say that L is a transversal. If x0 is not an equilibrium point,
it is always possible to construct a transversal through x0, since the slope of the
integral paths is well-deﬁned in the neighbourhood of x0.
Lemma 9.1 Let I be a positively invariant region in the phase plane, and L ⊂I a
transversal through x0 ∈I. The integral path, x(t; x0), that passes through x0 when
t = 0 intersects L in a monotone sequence. In other words, if xi is the point where
x(t; x0) meets L for the ith time, then xi lies in the segment of L between xi−1 and
xi+1.
Proof Consider the invariant region D bounded by the integral path from xi−1 to
xi and the segment of L between xi−1 and xi. There are two possibilities. Firstly,
the integral path through xi enters D and remains there (see Figure 9.13(a)). In
this case, xi+1, if it exists, lies in D, and xi therefore lies in the segment of L
between xi−1 and xi+1. Secondly, the integral path through xi−1 originates in D
(see Figure 9.13(b)). In this case, xi−2, if it exists, lies in D, and xi−1 therefore
lies in the segment of L between xi−2 and xi.
L
D
xi − 1
xi +1
xi
L
(a)
(b)
xi
xi − 2
xi − 1
Fig. 9.13. The intersection of an integral path in an invariant region with a transversal.

9.3 SECOND ORDER EQUATIONS
243
Note that Lemma 9.1 does not hold in systems of higher dimension, since it relies
crucially on the fact that a closed curve separates an inside from an outside (the
Jordan curve theorem).
Lemma 9.2 Consider the region I, transverse curve L and point x0 ∈I deﬁned in
Lemma 9.1. The ω-limit set of x0, ω(x0), intersects L at most once.
Proof Suppose that ω(x0) intersects L twice, at ˆx and ¯x. We can therefore ﬁnd
sequences of points, {ˆxi} and {¯xi}, lying in L such that {ˆxi} →ˆx and {¯xi} →¯x as
i →∞. However, this cannot occur, since Lemma 9.1 states that these intersections
must be monotonic. Hence the result is proved by contradiction.
Lemma 9.3 If x1 ∈ω(x0) is not an equilibrium point and lies on x(t; x0), the
integral path through x0, then x(t; x1), the integral path through x1, is a closed
curve, also passing through x0.
Proof
Since x1 lies in x(t; x0), ω(x1) = ω(x0), and hence x1 ∈ω(x1). Let L be
a curve through x1 transverse to the integral paths. By Lemma 9.2, x(t; x1) can
only meet L once, and hence is a closed curve.
We can now ﬁnally prove Theorem 9.4, the Poincar´e–Bendixson theorem.
Proof
Let x0 be a point in I, and hence x(t; x0) ⊂I and ω(x0) ⊂I. Choose
x1 ∈ω(x0).
If x1 ∈x(t; x0), then, by Lemma 9.3, the integral path through x1 is a closed
curve and, since there are no equilibrium points in I, must be a limit cycle.
If x1 ̸∈x(t; x0), let x2 ∈ω(x1).
Let L be a transversal through x2.
Since
x2 ∈ω(x1), the integral path though x1 must intersect L at a monotonic sequence
of points x1i, such that x1i →x2 as i →∞. But x1i ∈ω(x0), so the integral path
through x0 must pass arbitrarily close to each of the points x1i as t →∞. However,
the intersections of the integral path through x0 with L should be monotonic, by
Lemma 9.1, and we conclude that x1i = x2, and hence that ω(x1) is the closed
limit cycle through x1.
Example
Consider the system
˙x = x −y −2x(x2 + y2),
˙y = x + y −y(x2 + y2).
(9.21)
In order to analyze these equations, it is convenient to write them in terms of
polar coordinates, (r, θ). From the deﬁnitions, r = (x2 + y2)1/2 and x = r cos θ,
y = r sin θ, the chain rule gives
˙r = (x ˙x + y ˙y)(x2 + y2)−1/2 = cos θ ˙x + sin θ ˙y.
(9.22)
For (9.21),
˙r = cos θ

r cos θ −r sin θ −2r3 cos θ

+ sin θ

r cos θ + r sin θ −r3 sin θ


244
PHASE PLANE METHODS
= r −r3 
1 + cos2 θ

.
Since 0 ⩽cos2 θ ⩽1, we conclude that
r(1 −2r2) ⩽˙r ⩽r(1 −r2).
Therefore ˙r > 0 for 0 < r < 1/
√
2 and ˙r < 0 for r > 1. Remembering that if ˙r > 0, r
is an increasing function of t, and therefore integral paths are directed away from the
origin, we can deﬁne a closed, bounded, annular region D = {(r, θ) | r0 ⩽r ⩽r1}
with 0 < r0 < 1/
√
2 and r1 > 1, such that all integral paths enter the region I, as
shown in Figure 9.14. If we can show that there are no equilibrium points in I, the
Poincar´e–Bendixson theorem shows that at least one limit cycle exists in I.
Γ = Γ1
Γ = Γ0
Ι
Fig. 9.14. The region I that contains a limit cycle for the system (9.21).
From the deﬁnition of θ = tan−1(y/x),
˙θ =
1
1 + (y/x)2
x ˙y −y ˙x
x2
= 1
r (cos θ ˙y −sin θ ˙x) .
(9.23)
For (9.21),
˙θ = 1
r

cos θ

r cos θ + r sin θ −r3 sin θ

−sin θ

r cos θ −r sin θ −2r3 sin θ

= 1 + 1
2r2 sin 2θ.

9.3 SECOND ORDER EQUATIONS
245
Since −1 ⩽sin 2θ ⩽1, we conclude that ˙θ > 0 provided r <
√
2, and hence
that there are no equilibrium points for 0 < r <
√
2. Therefore, provided that
1 < r1 <
√
2, there are no equilibrium points in I, and hence, by the Poincar´e–
Bendixson theorem, there is at least one limit cycle in I. Note that we know that
a limit cycle must enclose at least one node, focus or centre. In this example, there
is an unstable focus at r = 0, which is enclosed by any limit cycles, but which does
not lie within I. In general, if we can construct a region in the phase plane that
traps a limit cycle in this way, it cannot be simply-connected.
A corollary of the Poincar´e–Bendixson theorem is that, if I is a closed, bounded,
invariant region of the phase plane, and I contains no limit cycles, we can de-
duce that all of the integral paths that meet the boundary of I terminate at an
equilibrium point contained within I. We can use similar ideas to determine the
positions of the stable separatrices of P4 for our example population dynamical
system, (9.20).
Consider the region
R1 = {(x, y) | x ⩾0, y ⩾0, 2x + y ⩽2, 2x + 2y ⩽3} ,
shown in Figure 9.15. The separatrix S1 lies in R1 in the neighbourhood of the
saddle point, P4. No integral path enters the region R1 through its boundaries, and
we conclude that S1 must originate at the equilibrium point at the origin, P1.
Now consider the region
R2 =

(x, y) | x ⩾0, y ⩾0, 2x + y ⩾2, 2x + 2y ⩾3, x2 + y2 ⩽r2
0

,
with r0 > 3, as shown in Figure 9.15. The separatrix S2 lies in R2 in the neigh-
bourhood of the saddle point, P4. No integral path enters the region R2 through
any of its straight boundaries, and we conclude that S1 must enter R2 through its
curved boundary, x2 +y2 = r2
0. Since we can make r0 arbitrarily large, we conclude
that S2 originates at inﬁnity.
9.3.8
The Phase Portrait at Inﬁnity
In order to study the behaviour of a second order system far from the origin,
it is often useful to map the phase plane to a new coordinate system where the
point at inﬁnity is mapped to a ﬁnite point. There are several ways of doing this,
but the most useful is the Poincar´e projection. Consider the plane that passes
through the line x = 1, perpendicular to the (x, y)-plane, as shown in Figure 9.16.
We can set up a Cartesian coordinate system, (u, v), in this plane, with origin a
unit distance above x = 1, y = 0, u-axis in the same direction as the y-axis and
v-axis vertically upwards. We label the origin of the (x, y)-plane as O, and denote
the point a unit distance above the origin as O′. We now consider a point B in
the (x, y)-plane. The Poincar´e projection maps the point B to the point C where
the line O′B intersects the (u, v)-plane. In particular, as x →∞, C approaches
the u-axis, so that this projection is useful for studying the behaviour of a second
order system for x ≫1.
In order to relate u and v to x and y, we ﬁrstly consider the triangles OAB and

246
PHASE PLANE METHODS
R2
S2
S1
P4
2x + y = 2
2x + 2y = 3
x
y
R1
Fig. 9.15. The regions R1 and R2 for the population dynamical system (9.20).
OA′B′, shown in Figure 9.16, which are similar. This shows that u = y/x and
#
x2 + y2 −m =
#
x2 + y2/x. Secondly, we consider the similar triangles BB′C
and BOO′, which show that (1 + v)/m = 1/
#
x2 + y2. By eliminating m, we ﬁnd
that v = −1/x. The simple change of variables u = y/x, v = −1/x, and hence
˙u = uv ˙x −v ˙y,
˙v = v2 ˙x,
(9.24)
is therefore a Poincar´e projection.
If we apply this to the population dynamics example (9.20), we ﬁnd that
˙u = −u

1 + u
v

,
˙v = −(2u + 3v + 2).
(9.25)
This has ﬁnite equilibrium points at u = 0, v = −2/3, which corresponds to P3, and
u = 2, v = −2, which corresponds to P4. The nature of these equilibrium points
remains the same after the projection, so that P3 is a stable node and P4 is a saddle
point. In order to determine the behaviour of (9.20) for x ≫1, we need to consider
(9.25) close to the u-axis for v < 0 and u > 0. When −v ≪1, ˙u ≫1, whilst ˙v < 0.
Integral paths close to the u-axis therefore start parallel to it, but then move away.
Figure 9.17 is a sketch of the phase portrait in the (u, v)-plane. We conclude that
the integral paths of (9.20) for x ≫1 lead into the ﬁnite (x, y)-plane as sketched in
Figure 9.9(b). The analogous transformation, u = x/y, v = −1/y is also a Poincar´e
projection, and can be used to examine the behaviour for y ≫1.

9.3 SECOND ORDER EQUATIONS
247
O′
O
A = (1, 0)
A = (x, 0)
x
y
y
1
m
1+v
C = (u, v)
B′
B = (x, y)
u
v
 '
Fig. 9.16. A Poincar´e projection.
We have now answered all of the questions concerning the phase portrait of (9.20)
that we asked at the end of Section 9.3.4.
9.3.9
A Final Example: Hamiltonian Systems
Let Ωbe a region of the (x, y)-plane and H : Ω→R be a real-valued, continu-
ously diﬀerentiable function deﬁned on Ω. The two-dimensional system of ordinary
diﬀerential equations,
˙x = Hy(x, y),
˙y = −Hx(x, y),
(9.26)
is called a Hamiltonian system and H(x, y) is called the Hamiltonian. Such
systems occur frequently in mechanics. One example is the simple pendulum, which
we studied earlier. As we have seen, this has ˙x = y = Hy, ˙y = −ω2 sin x = −Hx,
so that H = 1
2y2 −ω2 cos x, the total energy, is a Hamiltonian for the system.
Hamiltonian systems have several general properties, which we will now investi-
gate.
Theorem 9.5 The integral paths of a Hamiltonian system are given by H(x, y) =
constant.
Proof On an integral path (x(t), y(t)), H = H(x(t), y(t)) and
dH
dt = Hx ˙x + Hy ˙y = HxHy −HyHx = 0,
so that H is constant.

248
PHASE PLANE METHODS
P3
P4
u
v
Fig. 9.17. A sketch of the phase portrait of the system of equations (9.25).
For the simple pendulum, the integral paths are therefore given by the curves
y2 −2ω2 cos x = constant.
Theorem 9.6 If x = xe is an equilibrium point of a Hamiltonian system with
nonzero eigenvalues, then it is either a saddle point or a centre.
Proof The Jacobian of (9.26) at x = xe is
J =

Hyx
Hyy
−Hxx
−Hxy

x=xe
.
Since Hxy = Hyx, the eigenvalues of J satisfy
λ2 = H2
xy(xe, ye) −Hxx(xe, ye)Hyy(xe, ye).
Note that H2
xy ̸= HxxHyy at x = xe since the eigenvalues are nonzero. If H2
xy >
HxxHyy at x = xe, there is one positive and one negative eigenvalue, so xe is
a saddle point. If H2
xy < HxxHyy at x = xe, there are two complex conjugate
imaginary eigenvalues. Since the integral paths are given by H = constant and the
conditions Hx = Hy = 0 and HxxHyy > H2
xy are those for a local maximum or
minimum at x = xe, the level curves of H are closed and surround x = xe. We
conclude that the equilibrium point is a nonlinear centre.

9.4 THIRD ORDER EQUATIONS
249
Theorem 9.7 (Liouville’s theorem) Hamiltonian systems are area-preserving.
Proof
Consider a small triangle with one vertex at x0, the other two vertices at
x0 + δx1 and x0 + δx2, and |δx1|, |δx2| ≪1. The area of this triangle is |A|, where
A = 1
2 (δx1 × δx2) .
If all of the vertices move along integral paths of the Hamiltonian system (9.26), a
Taylor expansion shows that
d
dtδxi ∼(δxiHyx(x0) + δyiHyy(x0), −δxiHxx(x0) −δyiHxy(x0)) ,
for i = 1, 2, where δxi = (δxi, δyi). Up to O(|δxi|), we therefore have
2dA
dt = d
dtδx1 × δx2 + δx1 × d
dtδx2
= δy2 {δx1Hyx(x0) + δy1Hyy(x0)} + δx2 {δx1Hxx(x0) + δy1Hxy(x0)}
−δy1 {δx2Hyx(x0) + δy2Hyy(x0)} −δx1 {δx2Hxx(x0) + δy2Hxy(x0)} = 0,
so the area of the triangle is unchanged under the action of a Hamiltonian system.
Since any area in the phase plane can be broken up into inﬁnitesimal triangles, the
Hamiltonian system is area-preserving.
9.4
Third Order Autonomous Nonlinear Ordinary Diﬀerential
Equations
The solutions of the third order system
˙x = X(x, y, z),
˙y = Y (x, y, z),
˙z = Z(x, y, z),
(9.27)
can be analyzed in terms of integral paths in a three-dimensional, (x, y, z)-phase
space. However, most of the useful results that hold for the phase plane do not
hold in three or more dimensions. The crucial diﬀerence is that, in three or more
dimensions, closed curves no longer divide the phase space into two distinct regions,
inside and outside the curve. For example, integral paths inside a limit cycle in the
phase plane are trapped there, but this is not the case in a three-dimensional phase
space. There is no analogue of the Poincar´e index or Bendixson’s negative criterion,
nor, as we noted earlier, is there an analogue of the Poincar´e–Bendixson theorem.
In third or higher order systems, integral paths can be attracted to strange or
chaotic attractors, which have fractal or noninteger dimensions, and rep-
resent chaotic solutions. A simple way to get a grasp of this is to remember
that cars, which drive about on a plane, often hit each other, but aircraft, which
have an extra dimension to use, do so more rarely.
We will examine some ele-
mentary techniques for studying chaotic solutions in Chapter 15. There are also
some interesting, and useful, conservative third order systems for which a more
straightforward analysis is possible (see also Exercise 13.4).

250
PHASE PLANE METHODS
Example
Consider the third order system
˙x = yz,
˙y = −xz,
˙z = −k2xy.
(9.28)
In this rather degenerate system, any point on the x-, y- or z-axis is an equilibrium
point. We can also see that
d
dt(x2 + y2) = 2x ˙x + 2y ˙y = 2xyz −2yxz = 0.
Therefore, x2 + y2 is constant on any integral path, and hence all integral paths lie
on the surface of a cylinder with its axis pointing in the z-direction.
Consider the integral paths on the surface of the cylinder x2 + y2 = 1. This
surface is two-dimensional, so we should be able to analyze the behaviour of integral
paths on it using phase plane techniques. There are equilibrium points at (0, ±1, 0)
and (±1, 0, 0) and the Jacobian matrix is
J =


Xx
Xy
Xz
Yx
Yy
Yz
Zx
Zy
Zz

=


0
z
y
−z
0
−x
−k2y
−k2x
0

.
This gives
J(±1, 0, 0) =


0
0
0
0
0
∓1
0
∓k2
0

,
J(0, ±1, 0) =


0
0
±1
0
0
0
∓k2
0
0

.
The points (±1, 0, 0) each have eigenvalues λ = 0, ±k. The zero eigenvalue, with
eigenvector (0, 1, 0)T, corresponds to the fact that the y-axis is completely made up
of equilibrium points. The remaining two eigenvalues are real and of opposite sign,
and control the dynamics on the cylinder x2 +y2 = 1, where the equilibrium points
are saddles. Similarly, the points (0, ±1, 0) each have eigenvalues λ = 0, ±ik, and
are therefore linear centres on x2 + y2 = 1. These remain centres when nonlinear
terms are taken into account, using the argument that we described earlier for the
simple pendulum, since the system is unchanged by the transformation z →−z,
t →−t. The phase portrait is sketched in Figure 9.18.
We can conﬁrm that this phase portrait is correct by noting that this system
actually has two other conserved quantities. From (9.28),
d
dt(k2y2 −z2) = d
dt(k2x2 + z2) = 0,
and hence k2y2 −z2 and k2x2 +z2 are constant on any integral path. Integral paths
therefore lie on the intersection of the circular cylinder x2 + y2 = constant, the
hyperboloidal cylinder k2y2 −z2 = constant, and the elliptical cylinder k2x2 +z2 =
constant. This is precisely what the phase portrait in Figure 9.18 shows.
Finally, consider the integral path with x = 0, y = z = 1 when t = 0. On this
integral path, k2x2 + z2 = 1 and x2 + y2 = 1, so that
˙x =
#
1 −x2#
1 −k2x2,

EXERCISES
251
Fig. 9.18. The phase portrait of the third order system (9.28) on the surface of the cylinder
x2 + y2 = 1 when k = 2.
which gives†
t =
 x
0
ds
√
1 −s2√
1 −k2s2 .
This is the deﬁnition of the Jacobian elliptic function sn(t ; k). On this integral
path y and z are also Jacobian elliptic functions, y = cn(t ; k) and z = dn(t ; k). The
phase portrait that we have just determined now allows us to see qualitatively that
these elliptic functions are periodic with t, provided that k ̸= 1. In Sections 12.2.3
and 12.2.4 we will develop asymptotic expansions for sn(t ; k), ﬁrstly when k is close
to unity, and secondly when k ≪1. The Jacobian elliptic functions will also prove
to be useful in Section 12.2.5.
Exercises
9.1
Consider the second order, autonomous ordinary diﬀerential equation
¨x = 3x2 −1,
where a dot represents d/dt. By integrating this equation once, obtain a
relation between ˙x and x. Sketch the phase portrait in the (x, ˙x)-phase
plane. Determine the coordinates of the two equilibrium points and show
that there is a homoclinic orbit associated with one of them. What types
of behaviour occur inside and outside the homoclinic orbit?
† Note that the properties of the particular integral path with x = 0, y = z = 1 when t = 0
ensure that the arguments of the square roots remain positive.

252
PHASE PLANE METHODS
9.2
By sketching the curve ˙x = X(x), determine the equilibrium points and
corresponding domains of attraction when
(a) X(x) = x2 −x −2,
(b) X(x) = e−x −1,
(c) X(x) = sin x.
Now check that this qualitative analysis is correct by actually solving each
equation with initial conditions x = x0 when t = 0. Which method do you
think is easier to use, qualitative or quantitative?
9.3
Find the eigenvalues and eigenvectors of the matrix A, and then sketch the
phase portrait of the linear system ˙u = Au, where u = (x, y)T, for A =
(a)

1
−5
1
−1

,
(b)

1
1
1
−2

,
(c)
 −4
2
3
−2

,
(d)

1
2
2
2

,
(e)

4
−2
3
−1

,
(f)

2
1
−1
1

.
9.4
Consider a second order linear system ˙u = Au, when the constant matrix
A has two equal eigenvalues, λ. By using the Cayley–Hamilton theorem,
show that there must exist a linear transformation that takes A to either
Λ1 =
 λ
0
0
λ

or Λ2 =
 λ
1
0
λ

.
Solve the linear system of equations ˙v = Λjv for j = 1 and j = 2, and
hence sketch the phase portrait in each case. Note that in the case j = 1,
the equilibrium point at the origin is known as a star, whilst when j = 2
it is an improper node.
9.5
A certain second order autonomous system has exactly two equilibrium
points, both of which are saddles. Sketch a phase portrait in which (a) a
separatrix connects the saddle points, (b) no separatrix connects the saddle
points.
9.6
The weight at the end of a simple pendulum experiences a frictional force
proportional to its velocity. Determine the equation of motion and write
it as a pair of ﬁrst order equations. Show that the equilibrium points are
either stable points or saddle points.
Sketch the phase portrait.
What
happens after a long time?
9.7
Find all of the equilibrium points of each of the following systems, and
determine their type. Sketch the phase portrait in each case.
(a) ˙x = x −y,
˙y = x + y −2xy2,
(b) ˙x = −3y + xy −10,
˙y = y2 −x2,
(c) ˙x = y2 −1,
˙y = sin x.
9.8
Consider the system of ordinary diﬀerential equations
dx
dt = x

x2y2 + 1

,
dy
dt = y (y −2 −x) .

EXERCISES
253
Determine the position and type of each equilibrium point in the (x, y)-
plane. Show that the coordinate axes are integral paths. Sketch the phase
portrait, taking care to ensure that your sketch is consistent with the po-
sition of the horizontal nullcline. If x = −1 and y = −1 when t = 0, how
does the solution behave as t →∞?
9.9
The second order system
dr
dt = r(3 −r −s),
ds
dt = s(2 −r −s),
can be used to model the population of sheep (s) and rabbits (r) in a closed
ecosystem. Determine the position and type of all the equilibrium points.
Find the directions of the separatrices close to any saddle points. Assuming
that there are no limit cycles, sketch the phase portrait for r > 0 and s > 0.
Which animal becomes extinct?
9.10
Explain how the system
˙x = x(−A + b1y),
˙y = y(B −b2x),
with the constants A, B, b1 and b2 positive, models the populations of a
carnivorous, predator species and its herbivorous prey in a closed ecosys-
tem. Which variable is the predator population, x or y? Determine the
type of each of the equilibrium points. Determine dy/dx as a function of
x and y, and integrate once to obtain an equation of the form E(x, y) =
constant. Show that E(x, y) has a local minimum at one of the equilibrium
points, and hence deduce that it is a nonlinear centre. Sketch the phase
portrait. What happens to the populations?
9.11
Use the concept of the Poincar´e index to determine which of the following
can be surrounded by a limit cycle in the phase portrait of a second order
system.
(a) an unstable node,
(b) a saddle point,
(c) two saddle points, a stable node and an unstable focus,
(d) a saddle point, an unstable focus and a stable node.
Sketch a possible phase portrait in each of the cases where a limit cycle
can surround the equilibrium points.
9.12
Consider the system
dx
dt = x(x2 + y2 −1),
dy
dt = y(x2 + y2 −2).
Show that the x- and y-axes are integral paths. Show that there are no
limit cycle solutions using
(a) Dulac’s extension to Bendixson’s negative criterion with auxiliary
function ρ(x, y) = 1/xy,
(b) the Poincar´e index.
Sketch the phase portrait.

254
PHASE PLANE METHODS
9.13
Use the Poincar´e index, Bendixson’s negative criterion or Dulac’s exten-
sion as appropriate to show that the following systems have no limit cycle
solutions.
(a) ˙x = y,
˙y = 1 + x2 −(1 −x)y,
(b) ˙x = −(1 −x)3 + xy2,
˙y = y + y3,
(c) ˙x = 2xy + x3,
˙y = −x2 + y −y2 + y3,
(d) ˙x = x,
˙y = 1 + x + y2,
(e) ˙x = 1 −x3 + y2,
˙y = 2xy,
(f) ˙x = y + x2,
˙y = −x −y + x2 + y2.
(Hint: For (f), use Dulac’s extension to Bendixson’s negative criterion with
auxiliary function ρ(x, y) = eax+by.)
9.14
Write each of the following systems in terms of polar coordinates (r, θ), and
use the Poincar´e–Bendixson theorem to show that at least one limit cycle
solution exists.
(a) ˙x = 2x + 2y −x(2x2 + y2),
˙y = −2x + 2y −y(2x2 + y2),
(b) ˙x = x −y −x(x2 + 3
2y2),
˙y = x + y −y(x2 + 1
2y2).
9.15
(a) Write the system
dx
dt = x −y −(2x2 + y2)x,
dy
dt = x + y −(x2 + 2y2)y
in terms of polar coordinates, and then use the Poincar´e–Bendixson
theorem to show that there is at least one limit cycle solution.
(b) Use Dulac’s extension to Bendixson’s negative criterion (with an
auxiliary function of the form eax+by for some suitable constants a
and b) to show that there is no limit cycle solution of the system
with
dx
dt = y,
dy
dt = −x −y + x2 + y2.
9.16
(a) Write the system of ordinary diﬀerential equations
dx
dt = x −y −y3 −2x5 −2x3y2 −xy4,
dy
dt = x + y + xy2 −2yx4 −2y3x2 −y5
in terms of polar coordinates (r, θ), and use the Poincar´e–Bendixson
theorem to show that at least one limit cycle solution exists.
(b) Write the system of ordinary diﬀerential equations
dx
dt = xy −x2y + y3,
dy
dt = y2 + x3 −xy2
in terms of polar coordinates, (r, θ).
Show that there is a single
equilibrium point at the origin and that it is nonhyperbolic. Show
that the lines θ = ±π/4 and θ = ±3π/4 are integral paths. Show
that dr/dt = 0 when θ = 0 or π. Sketch the phase portrait.

EXERCISES
255
9.17
Consider the second order system
dp
ds = −pq,
dq
ds = p + q −2,
which arises in a thermal ignition problem (see Section 12.2.3). Show that
there are just two ﬁnite equilibrium points, one of which is a saddle. Con-
sider S, the stable separatrix of the saddle that lies in p > 0. Show that this
separatrix asymptotes to the other equilibrium point as s →−∞. (Hint:
First show that S must meet the p-axis with p > 2. Next show that S
must go on to meet the p-axis with p < 2. Finally, use the coordinate axes
and S up to its second intersection with the p-axis to construct a trapping
region for S.)
9.18
Project A particle of mass m moves under the action of an attractive
central force of magnitude γm/rα, where (r, θ) are polar coordinates and
γ and α are positive constants. By using Newton’s second law of motion
in polar form, show that u = 1/r satisﬁes the equation
d 2u
dθ2 + u = γ
h2 uα−2,
(E9.1)
where h is the angular momentum of the particle.
(a) Find the equilibrium points in the (u, du/dθ)-phase plane, and clas-
sify them. What feature of the linear approximation will carry over
to the solutions of the full, nonlinear system?
(b) If the particle moves at relativistic speeds, it can be shown that
(E9.1) is modiﬁed, in the case of the inverse square law, α = 2,
appropriate to a gravitational attraction, to
d 2u
dθ2 + u = γ
h2 + ϵu2,
(E9.2)
where ϵ is a small positive constant, and the term ϵu2 is called Ein-
stein’s correction. Find the equilibrium point that corresponds to
a small perturbation of the Newtonian case (ϵ = 0), and show that
it is a centre.
(c) Use MATLAB to solve (E9.2) numerically, and hence draw the phase
portrait for the values of ϵ, γ and h appropriate to each of the
three planets nearest to the Sun (you’ll need to ﬁnd an appropriate
astronomy book in your library), and relate what you obtain to part
(b) above.

CHAPTER TEN
Group Theoretical Methods
In this chapter we will develop an approach to the solution of diﬀerential equations
based on ﬁnding a group invariant. In order to introduce the general idea, let’s
begin by considering some simple, ﬁrst order ordinary diﬀerential equations.
The solution of the separable equation,
dy
dx = f(x)g(y),
is

dy
g(y) =

f(x)dx + constant.
Another simple class of equations, often referred to as exact equations, takes the
form
∂φ
∂x + ∂φ
∂y
dy
dx = 0.
In order to stress the equal role played by the independent variables in this equation,
we will usually write
∂φ
∂xdx + ∂φ
∂y dy = 0.
This has the solution φ(x, y) = constant.
Let’s now consider the equation
dy
dx = f
y
x

.
In general, this is neither separable nor exact, and we are stuck unless we can use
some other property of the equation. An inspection reveals that the substitution
x = λˆx, y = λˆy, where λ is any real constant, leaves the form of the equation
unchanged, since
dˆy
dˆx = f
 ˆy
ˆx

.
We say that the equation is invariant under the transformation x →λˆx, y →λˆy.
The quantity y/x →ˆy/ˆx is also invariant under the transformation. If we use this
invariant quantity, v = y/x, as a new dependent variable, the equation becomes
v + xdv
dx = f(v),

10.1 LIE GROUPS
257
which is separable, with solution

dv
f(v) −v =
 dx
x + constant.
If we regard the parameter λ as a continuous variable, the set of transformations
x →λˆx, y →λˆy forms a group under composition of transformations.
More
speciﬁcally, this is an example of a Lie group, named after the mathematician
Sophus Lie. We will discuss exactly what we mean by a group and a Lie group
below.
It is the invariance of the diﬀerential equation under the action of this
group that allows us to ﬁnd a solution in closed form.
In the following sections, we begin by developing as much of the theory of Lie
groups as we will need, and then show how this can be used as a practical tool for
the solution of diﬀerential equations.
10.1
Lie Groups
Let D be a subset of R2 on which x →x1 = f(x, y; ϵ), y →y1 = g(x, y; ϵ) is
a well-deﬁned transformation from D into R2.
We also assume that x1 and y1
vary continuously with the parameter ϵ.
This set of transformations forms a
one-parameter group, or Lie group, if
(i) the transformation with ϵ = 0 is the identity transformation, so that
f(x, y; 0) = x,
g(x, y; 0) = y,
(ii) the transformation with −ϵ gives the inverse transformation, so that, if x1 =
f(x, y; ϵ) and y1 = g(x, y; ϵ), then x = f(x1, y1; −ϵ) and y = g(x1, y1; −ϵ),
(iii) the composition of two transformations is also a member of the set of trans-
formations, so that if x1 = f(x, y; ϵ), y1 = g(x, y; ϵ), x2 = f(x1, y1; δ) and
y2 = g(x1, y1; δ), then x2 = f(x, y; ϵ + δ) and y2 = g(x, y; ϵ + δ).
Some simple one-parameter groups are:
(a) Horizontal translation, H(ϵ):
x1 = x + ϵ,
y1 = y,
(b) Vertical translation, V(ϵ):
x1 = x,
y1 = y + ϵ,
(c) Magniﬁcation, M(ϵ):
x1 = eϵx,
y1 = eϵy,
(d) Rotation, R(ϵ):
x1 = x cos ϵ −y sin ϵ,
y1 = x sin ϵ + y cos ϵ.
For example, to show that the set of transformations M(ϵ) forms a group, ﬁrstly
note that when ϵ = 0, x1 = x and y1 = y.
Secondly, a simple rearrangement
gives x = e−ϵx1, y = e−ϵy1, so that the inverse transformation is given by M(−ϵ).
Finally, if x2 = eδx1 and y2 = eδy1 then x2 = eδ.eϵx = eδ+ϵx, and similarly with
y2.

258
GROUP THEORETICAL METHODS
10.1.1
The Inﬁnitesimal Transformation
By deﬁning a Lie group via the transformation x1 = f(x, y; ϵ), y1 = g(x, y; ϵ),
we are giving the ﬁnite form of the group. Consider what happens when ϵ ≪1.
Since ϵ = 0 gives the identity transformation, we can Taylor expand to obtain
x1 = x + ϵ
dx1
dϵ

ϵ=0
+ · · · ,
y1 = y + ϵ
dy1
dϵ

ϵ=0
+ · · · .
If we now introduce the functions
ξ(x, y) =
dx1
dϵ

ϵ=0
,
η(x, y) =
dy1
dϵ

ϵ=0
,
(10.1)
and just retain the ﬁrst two terms in the Taylor series expansions, we obtain x1 =
x + ϵξ(x, y), y1 = y + ϵη(x, y). This is called the inﬁnitesimal form of the group.
We will show later that every one-parameter group is associated with a unique
inﬁnitesimal group.
For example, the transformation x1 = x cos ϵ −y sin ϵ, y1 = x sin ϵ + y cos ϵ
forms the rotation group R(ϵ). When ϵ = 0 this gives the identity transformation.
Using the approximations cos ϵ = 1 + · · · , sin ϵ = ϵ + · · · for ϵ ≪1, we obtain the
inﬁnitesimal rotation group as x1 ∼x−ϵy, y1 ∼y+ϵx, and hence ξ(x, y) = −y and
η(x, y) = x. The transformation x1 = eϵx, y1 = eϵy forms the magniﬁcation group
M(ϵ). Using eϵ = 1 + ϵ + · · · for ϵ ≪1, we obtain the inﬁnitesimal magniﬁcation
group as x1 ∼(1 + ϵ)x, y1 ∼(1 + ϵ)y, so that ξ(x, y) = x and η(x, y) = y.
We will now show that every inﬁnitesimal transformation group is similar, or
isomorphic, to a translation group. This means that, by using a change of vari-
ables, we can make any inﬁnitesimal transformation group look like H(ϵ) or V(ϵ),
which we deﬁned earlier. Consider the equations that deﬁne ξ and η and write
them in the form
dx1
dϵ = ξ(x1, y1),
dy1
dϵ = η(x1, y1),
a result that is correct at leading order by virtue of the inﬁnitesimal nature of the
transformation, and which we shall soon see is exact. We can also write this in the
form
dx1
ξ
= dy1
η
= dϵ.
Integration of this gives solutions that are, in principle, expressible in the form
F1(x1, y1) = C1 and F2(x1, y1) = C2 + ϵ for some constants C1 and C2. Since
ϵ = 0 corresponds to the identity transformation, we can deduce that F1(x1, y1) =
F1(x, y) and F2(x1, y1) = F2(x, y) + ϵ. This means that if we deﬁne u = F1(x, y)
and v = F2(x, y) as new variables, then the group can be represented by u1 = u
and v1 = v + ϵ, so that the original group is isomorphic to the translation group
V(ϵ).

10.1 LIE GROUPS
259
10.1.2
Inﬁnitesimal Generators and the Lie Series
Consider the change, δφ, that occurs in a given smooth function φ(x, y) under
an inﬁnitesimal transformation. We ﬁnd that
δφ = φ(x1, y1) −φ(x, y) = φ(x + ϵξ, y + ϵη) −φ(x, y) = ϵ

ξ ∂φ
∂x + η ∂φ
∂y

+ · · · .
If we retain just this single term, which is consistent with the way we derived the
inﬁnitesimal transformation, we can see that δφ can be written in terms of the
quantity
Uφ = ξ ∂φ
∂x + η ∂φ
∂y ,
(10.2)
or, in operator notation,
U ≡ξ ∂
∂x + η ∂
∂y .
This is called the inﬁnitesimal generator of the group. Any inﬁnitesimal trans-
formation is completely speciﬁed by Uφ. For example, if
Uφ = −y ∂φ
∂x + x∂φ
∂y ,
ξ(x, y) = −y and η(x, y) = x, so that the transformation is given by x1 = x −ϵy,
y1 = y + ϵx. From the deﬁnition (10.2), Ux = ξ and Uy = η, so that
Uφ = Ux ∂φ
∂x + Uy ∂φ
∂y ,
and if a group acts on (x, y) to produce new values (x1, y1) then
Uφ(x1, y1) = Ux1
∂φ
∂x1
+ Uy1
∂φ
∂y1
.
Let’s now consider a group deﬁned in ﬁnite form by x1 = f(x, y; ϵ), y1 = g(x, y; ϵ)
and a function φ = φ(x, y). If we regard φ(x1, y1; ϵ) as a function of ϵ, with a prime
denoting d/dϵ, we ﬁnd that
φ(x1, y1; ϵ) = φ(x1, y1; 0) + ϵφ′(x1, y1; 0) + 1
2ϵ2φ′′(x1, y1; 0) + · · · .
Since
φ(x1, y1; 0) = φ(x, y),
φ′(x1, y1; 0) = dφ
dϵ

ϵ=0
=
 ∂φ
∂x1
dx1
dϵ + ∂φ
∂y1
dy1
dϵ

ϵ=0
=

ξ ∂φ
∂x1

ϵ=0
+ η ∂φ
∂y1

ϵ=0

= ξ ∂φ
∂x + η ∂φ
∂y = Uφ,
and
φ′′(x1, y1; 0) = d
dϵ
 dφ
dϵ

ϵ=0
= U 2φ,

260
GROUP THEORETICAL METHODS
we have
φ(x1, y1; ϵ) = φ(x, y, 0) + ϵUφ + 1
2ϵ2U 2φ + · · · .
This is known as a Lie series, and can be written more compactly in operator
form as
φ(x1, y1; ϵ) = eϵUφ(x, y).
In particular, if we take φ(x, y, 0) = x,
x1 = x + ϵUx + 1
2ϵ2U 2x + · · · = x + ϵξ + 1
2ϵ2Uξ + · · ·
= x + ϵξ + 1
2ϵ2

ξ ∂ξ
∂x + η ∂ξ
∂y

+ · · · .
Similarly,
y1 = y + ϵη + 1
2ϵ2

ξ ∂η
∂x + η ∂η
∂y

+ · · · .
These two relations are a representation of the group in ﬁnite form. It should now be
clear that we can calculate the ﬁnite form of the group from the inﬁnitesimal group
(via the Lie series) and the inﬁnitesimal group from the ﬁnite form of the group
(via expansions for small ϵ). For example, if an inﬁnitesimal group is represented
by
Uφ = x∂φ
∂x + y ∂φ
∂y ,
then
x1 = x + ϵx + 1
2!ϵ2x + 1
3!ϵ3x + · · · = xeϵ,
y1 = y + ϵy + 1
2!ϵ2y + 1
3!ϵ3y + · · · = yeϵ.
The ﬁnite form is therefore M(ϵ), the magniﬁcation group.
As a further example, if an inﬁnitesimal group is represented by
Uφ = −y ∂φ
∂x + x∂φ
∂y ,
then
Ux = −y,
Uy = x,
U 2x = −x,
U 2y = −y,
U 3x = y,
U 3y = −x,
U 4x = x,
U 4y = y.
U is therefore a cyclic operation with period 4 and the equations of the ﬁnite form
of the group are
x1 = x −ϵy −1
2!ϵ2x + 1
3!ϵ3y + 1
4!ϵ4x + · · ·

10.2 INVARIANTS UNDER GROUP ACTION
261
= x

1 −1
2!ϵ2 + 1
4!ϵ4 −· · ·

−y

ϵ −1
3!ϵ3 + · · ·

= x cos ϵ −y sin ϵ.
Similarly,
y1 = y + ϵx −1
2!ϵ3y −1
3!ϵ3x + 1
4!ϵ4y + · · ·
= y

1 −1
2!ϵ2 + 1
4!ϵ4 −· · ·

+ x

ϵ −1
3!ϵ3 + · · ·

= y cos ϵ + x sin ϵ,
and we have the rotation group R(ϵ).
There is a rather more concise way of doing this, using the fact that
dx1
dϵ = ξ(x1, y1),
dy1
dϵ = η(x1, y1),
subject to x1 = x and y1 = y at ϵ = 0
is an exact relationship according to (10.1). For the ﬁrst example above, this gives
dx1/dϵ = x1 and dy1/dϵ = y1, with x = x1 and y = y1 at ϵ = 0. This ﬁrst order
system can readily be integrated to give x1 = xeϵ and y1 = yeϵ.
10.2
Invariants Under Group Action
Let x1 = f(x, y; ϵ), y1 = g(x, y; ϵ) be the ﬁnite form of a group and let the inﬁnites-
imal transformation associated with the group have inﬁnitesimal generator
Uφ = ξ ∂φ
∂x + η ∂φ
∂y .
A function Ω(x, y) is said to be invariant under the action of this group if, when x1
and y1 are derived from x and y by the operations of the group, Ω(x1, y1) = Ω(x, y).
Using the Lie series, we can write
Ω(x1, y1) = Ω(x, y) + ϵUΩ+ 1
2!ϵ2U 2Ω+ · · · = Ω(x, y) + ϵUΩ+ 1
2!ϵ2U(UΩ) + · · · ,
so that a necessary and suﬃcient condition for invariance is UΩ= 0, and hence
that
ξ ∂Ω
∂x + η ∂Ω
∂y = 0.
This is a partial diﬀerential equation for Ω, whose solution is Ω(x, y) = C, a con-
stant, on the curve
dx
ξ = dy
η .
(10.3)
Since this equation has only one solution, it follows that a one-parameter group has
only one invariant.
Now let’s take a point (x0, y0) and apply the inﬁnitesimal transformation to it,
so that it is mapped to (x0 + ϵξ, y0 + ϵη). If we repeat this procedure inﬁnitely
often, we can obtain a curve that is an integral of the diﬀerential system given by
(10.3). By varying the initial point (x0, y0) we then obtain a family of curves, all
of which are solutions of (10.3), which we denote by Ωf(x, y) = C. This family of

262
GROUP THEORETICAL METHODS
curves is invariant under the action of the group in the sense that each curve in
the family is transformed into another curve of the same family under the action
of the group. Suppose (x, y) becomes (x1, y1) under the action of the group. This
means that Ωf(x1, y1) = constant must represent the same family of curves. Using
the Lie series we can write
Ωf(x1, y1) = Ωf(x, y) + ϵUΩf + 1
2!ϵ2U 2Ωf + · · · .
The most general condition that forces the ﬁrst two terms to be constant is that
UΩf = constant should represent one family of curves, because
U nΩf = U n−1 (UΩf) = U n−1 (constant) = 0.
This is conveniently written as UΩf = F(Ωf) for some arbitrary nonzero function
F.
For example, the rotation group is represented in inﬁnitesimal form by
Uφ = −y ∂φ
∂x + x∂φ
∂y .
The equation for the invariants of this group is
−dx
y = dy
x ,
which can be easily integrated to give Ω= x2 + y2 = C. This gives the intuitively
obvious result that circles are invariant under rotation!
10.3
The Extended Group
If x1 = f(x, y; ϵ) and y1 = g(x, y; ϵ) form a group of transformations in the usual
way, we can extend the group by regarding the diﬀerential coeﬃcient p = dy/dx
as a third independent variable. Under the transformation this becomes
p1 = dy1
dx1
= gx + pgy
fx + pfy
= h(x, y, p ; ϵ).
It can easily be veriﬁed that the triple given by (x, y, p) forms a group under the
transformations above. This is known as the extended group of the given group.
This extended group also has an inﬁnitesimal form associated with it. If we write
x1 = x + ϵξ(x, y), y1 = y + ϵη(x, y), then
p1 = ϵηx + p(1 + ϵηy)
1 + ϵξx + pϵξy
.
Expanding this using the binomial theorem for small ϵ, we ﬁnd that
p1 = p + ϵ

ηx + (ηy −ξx) p −ξyp2
= p + ϵζ.
(10.4)
The inﬁnitesimal generator associated with this three-element group is
U ′φ = ξφx + ηφy + ζφp.

10.4 INTEGRATION OF A FIRST ORDER EQUATION
263
It is of course possible, though algebraically more messy, to form further extensions
of a given group by including second and higher derivatives.
We are now at the stage where we can use the theory that we have developed
in this chapter to solve ﬁrst order diﬀerential equations. Prior to this, it is helpful
to outline two generic situations that may occur when you are confronted with a
diﬀerential equation that needs to be solved.
(i) It is straightforward to spot the group of transformations under which the
equation is invariant. In this case we can give a recipe for using this invari-
ance to solve the equation.
(ii) No obvious group of transformations can be spotted and we need a more sys-
tematic approach to construct the group. This is called Lie’s fundamental
problem and is considerably more diﬃcult than (i). Indeed, no general so-
lution is known to Lie’s fundamental problem for ﬁrst order equations.
10.4
Integration of a First Order Equation with a Known Group
Invariant
To show more explicitly that this group invariance property will lead to a more
tractable diﬀerential equation than the original, let’s consider a general ﬁrst order
ordinary diﬀerential equation, F(x, y, p) = 0, that is invariant under the extended
group
U ′φ = ξ ∂φ
∂x + η ∂φ
∂y + ζ ∂φ
∂p
derived from
Uφ = ξ ∂φ
∂x + η ∂φ
∂y .
We have seen that a suﬃcient condition for the invariance property is that U ′φ = 0,
so we are faced with solving
ξ ∂φ
∂x + η ∂φ
∂y + ζ ∂φ
∂p = 0.
The solution curves of this partial diﬀerential equation, where φ is constant, are
the two independent solutions of the simultaneous system
dx
ξ = dy
η = dp
ζ .
Let u(x, y) = α be a solution of
dx
ξ = dy
η ,
and v(p, x, y) = β be the other independent solution.
We now show that if we know U, ﬁnding v is simply a matter of integration. To
do this, recall the earlier result that any group with one parameter is similar to the

264
GROUP THEORETICAL METHODS
translation group. Let the change of variables from (x, y) to (x1, y1) reduce Uφ to
the group of translations parallel to the y1 axis, and call the inﬁnitesimal generator
of this group U1f. Then
U1f = Ux1
∂f
∂x1
+ Uy1
∂f
∂y1
= ∂f
∂y1
,
from which we see that Ux1 = 0 and Uy1 = 1, or more explicitly,
ξ ∂x1
∂x + η ∂x1
∂y = 0,
ξ ∂y1
∂x + η ∂y1
∂y = 1.
The ﬁrst of these equations has the solution x1 = u(x, y) and the second is equiva-
lent to the simultaneous system
dx
ξ = dy
η = dy1
1 .
Again, one solution of this system is u(x, y) = α. This can be used to eliminate x
from the second independent solution, given by
dy1
dy =
1
η(x, y),
so that by a simple integration we can obtain y1 as a function of x and y. As the
extended group of translations, U ′
1f, is identical to U1f, the most general diﬀerential
equation invariant under U ′
1 in the new x1, y1 variables will therefore be a solution
of the simultaneous system
dx1
0
= dy1
1
= dp1
0 .
This particularly simple system has solutions x1 = constant and p1 = constant, so
that the diﬀerential equation can be put in the form p1 = F(x1) for some calculable
function F. In principle, it is straightforward to solve equations of this form, as
they are separable. The solution of the original equation can then be obtained by
returning to the (x, y) variables.
Example
The diﬀerential equation
dy
dx =
1
#
x + y2
is invariant under the transformation x = e−2ϵx1, y = e−ϵy1. The inﬁnitesimal
transformation associated with this is x1 = x+2ϵx, y1 = y+ϵy, so that ξ(x, y) = 2x
and η(x, y) = y. If we solve the system
dx
ξ = dy
η ,
we ﬁnd that y/x1/2 = eC, so that x1 = y/x1/2. Solving
dy1
dy = 1
η = 1
y

10.5 DETERMINING GROUPS UNDER WHICH AN EQUATION IS INVARIANT
265
gives y1 = log y. Some simple calculus then shows that the original diﬀerential
equation transforms to
1
2x3
1
dy1
dx1
x1
dy1
dx1
−1
=
x1
#
x2
1 + 1
,
which, on rearranging, gives
dy1
dx1
=
1
x1

1 −1
2x1
#
x2
1 + 1
.
This is the separable equation that we are promised by the theory we have de-
veloped. The ﬁnal integration of the equation can be achieved by the successive
substitutions z = log x1 and t2 = 1 + e−2z.
10.5
Towards the Systematic Determination of Groups Under Which
a First Order Equation is Invariant
If we consider a diﬀerential equation in the form
dy
dx = F(x, y)
and an inﬁnitesimal transformation of the form x1 ∼x+ϵξ(x, y), y1 ∼y +ϵη(x, y),
(10.4) shows that
dy1
dx1
= dy
dx + ϵ

ηx + (ηy −ξx) dy
dx −ξy
dy
dx
2
+ · · · .
Using the diﬀerential equation to eliminate dy/dx, we ﬁnd that the equation will
be invariant under the action of the group provided that
ξ ∂F
∂x + η ∂F
∂y = ηx + (ηy −ξx) F −ξyF 2.
(10.5)
So, given the function F, the fundamental problem is to determine two functions
ξ and η that satisfy this ﬁrst order partial diﬀerential equation. Of course, this is
an underdetermined problem and has no unique solution. However, by choosing a
special form for either ξ or η there are occasions when the process works, as the
following example shows.
Example
Consider the equation
dy
dx =
y
x + x2 + y2 .
(10.6)
In this case,
∂F
∂x =
−(1 + 2x) y
(x + x2 + y2)2 ,
∂F
∂y =
x + x2 −y2
(x + x2 + y2)2 ,

266
GROUP THEORETICAL METHODS
and (10.5) takes the form
(x + x2 −y2)η −(1 + 2x)yξ = (x + x2 + y2)2ηx + (ηx −ξx)y(x + x2 + y2) −y2ξy.
If now we choose η = 1, this reduces to
(x + x2 −y2) + (1 + 2x)yξ = y(x + x2 + y2)ξx + y2ξy.
It is not easy to solve even this equation in general, but after some trial and error,
we can ﬁnd the solution ξ = x/y. The inﬁnitesimal transformation in this case is
x1 = x + ϵx/y, y1 = y + ϵ and, following the procedure outlined in the last section,
we now solve
dx
x/y = dy
1
to obtain y/x = constant. We therefore take x1 = y/x and hence y1 = y as our
new variables. In terms of these variables, (10.6) becomes
dy1
dx1
= −
1
(1 + x2
1),
with solution y1 = −tan−1 x1+C. The solution of our original diﬀerential equation
can therefore be written in the form y + tan−1 (y/x) = C.
10.6
Invariants for Second Order Diﬀerential Equations
First order diﬀerential equations can be invariant under an inﬁnite number of one-
parameter groups. Second order diﬀerential equations can only be invariant under
at most eight groups. To see where this ﬁgure comes from, let’s consider the simplest
form of a variable coeﬃcient, linear, second order, diﬀerential equation,
d 2y
dx2 + q(x)y = 0.
(10.7)
Writing x1 = x + ϵξ and y1 = y + ϵη we have already shown that
dy1
dx1
= dy
dx + ϵ

ηx + (ηy −ξx) dy
dx −ξy
dy
dx
2
= dy
dx + ϵΠ

x, y, dy
dx

= p + ϵΠ (x, y, p) .
Now
d 2y1
dx2
1
=
d
dx1
 dy1
dx1

= d
dx
 dy1
dx1
 dx
dx1
and
d
dx
 dy1
dx1

= dp
dx + ϵ (Πx + Πyp + Πppx) + · · ·
= 1
5dx1
dx =
1
1 + ϵ (ξx + ξyp) + · · · ,

10.6 INVARIANTS FOR SECOND ORDER DIFFERENTIAL EQUATIONS
267
so that
d 2y1
dx2
1
= d 2y
dx2 + ϵ

Πx + Πy
dy
dx + Πp
d 2y
dx2 −d 2y
dx2

ξx + ξy
dy
dx
	
+ · · · .
The condition for invariance is therefore
Πx + Πy
dy
dx + d 2y
dx2 Πp −d 2y
dx2

ξx + ξy
dy
dx

+ ξq′(x1)y1 + ηq(x1) = 0.
Some simple calculation leads to
{ηxx + (2ξx −ηy) q(x)y + ξq′(x)y + q(x)η} + {2ηxy −ξxx + 3q(x)yξy} dy
dx
+ {ηyy −2ξxy}
dy
dx
2
−ξyy
dy
dx
3
= 0.
If we now set the coeﬃcients of powers of dy/dx to zero, we obtain
ξyy = 0,
ηyy −2ξxy = 0,
2ηxy −ξxx + 3q(x)yξy = 0,
(10.8)
ηxx + (2ξx −ηy)q(x)y + ξq′(x)y + q(x)η = 0.
(10.9)
Equation (10.8)1 can be integrated to give
ξ = ρ(x)y + ξ(x).
Substitution of this into (10.8)2 gives ηyy = 2ρ′(x), which can be integrated to give
η = ρ′(x)y2 + ˜η(x)y + ζ(x).
From (10.8)3,
3ρ(x)q(x)y + 3ρ′′(x)y + 2˜η′(x) −ξ′′(x) = 0.
(10.10)
Finally, (10.9) gives

ρ′′′(x)y2 + ˜η′′(x)y + ζ′′(x)

−q(x)y {˜η(x) −2ξ′(x)} + {ρ(x)y + ξ(x)} q′(x)y
+

ρ′(x)y2 + ˜η(x)y + ζ(x)

q(x) = 0.
(10.11)
We can ﬁnd a solution of (10.10) and (10.11) by noting that the coeﬃcient of each
power of y must be zero, which gives us four independent equations,
ζ′′(x) + q(x)ζ(x) = 0,
ρ′′(x) + q(x)ρ(x) = 0,
˜η′′(x) −q(x) (˜η(x) −2ξ′(x)) + ξ(x)q′(x) + ˜η(x)q(x) = 0,
ξ′′(x) = 2˜η′(x).
At this stage notice that ρ(x) and ζ(x) satisfy the original second order equation,
(10.7), the solution of which will involve four constants.
There are also second
order equations for ξ(x) and ˜η(x), the solution of which gives rise to a further four
constants. Each of these constants will generate a one-parameter group that leaves
the original equation invariant, so the original equation is invariant under at most

268
GROUP THEORETICAL METHODS
eight one-parameter groups. Some of these groups are obvious. For example, since
(10.7) is invariant under magniﬁcation in y, it must be invariant under x1 = x,
y1 = eϵy.
Let’s consider the group generated by ρ(x) in more detail. In the usual way, we
have to integrate
dx
ξ(x, y) =
dy
η(x, y),
which leads to the equation
dy
dx = y

ρ′(x)y + 1
2ξ′(x) + ζ(x)

ρ(x)y + ξ(x)
.
This has a solution of the form y = Cρ(x) provided that ξ(x) = Cρ2(x) and ζ(x) = 0
for some constant C. Note that ˜η(x) = 1
2ξ′(x) by direct integration. This means
that, following the ideas of the previous section, we should deﬁne x1 = y/ρ(x).
Now,
dy1
dy =
1
η(x, y) =
1
ρ′(x)y2 + Cρρ′y =
1
ρ′(x) (y2 + Cρy),
and, since y = Cρ(x),
dy1
dx =
1
2Cρ2 ,
which gives
y1 = 1
2C
 x
x0
dx
ρ2 = ρ(x)
2y
 x
x0
dt
ρ2(t).
We can write the diﬀerential equation in terms of these new variables by noting
that
x1y1 =
 x
x0
dt
2ρ2(t),
so that
d
dx(x1y1) =
1
2ρ2(x).
Now
dx1
d(x1y1) = dx1
dx
dx
d(x1y1) = 2ρ2
1
ρ
dy
dx −ρ′y
ρ2
	
= 2

ρdy
dx −ρ′y

,
which gives
d
dx

dx1
d(x1y1)

= 2

ρd 2y
dx2 + ρ′ dy
dx −ρ′′y −ρ′ dy
dx
	
= 2ρ
d 2y
dx2 + q(x)y
	
= 0.
Integrating this expression gives
dx1
d(x1y1) = C1,

10.6 INVARIANTS FOR SECOND ORDER DIFFERENTIAL EQUATIONS
269
so that x1 = C1x1y1 + C2 or, in terms of the original variables,
y = C1ρ(x)
 x
x0
dt
ρ2(t) + C2ρ(x).
This is just the reduction of order formula, which we derived in Chapter 1. Although
the invariance property produces this via a route that is rather diﬀerent from that
taken in Chapter 1, it is not particularly useful to us in ﬁnding solutions.
Let’s now consider the invariance x1 = x and y1 = eϵy, which has ξ = 0 and
η = y. This means that
dx
0 = dy
y ,
and we can see that this suggests using the new variables x1 = x and y1 = log y.
Since y′′ = ey1(y′′
1 +y′2
1 ) under this change of variables, we obtain y′′
1 +y′2
1 +q(x) = 0.
Putting Y1 = y′
1 leads to Y ′
1 + Y 2
1 + q(x) = 0. This is a ﬁrst order equation, so
the group invariance property has allowed us to reduce the order of the original
equation. As an example of this, consider the equation
y′′ +
1
4x2 y = 0.
In this case, we obtain
Y ′
1 + Y 2
1 +
1
4x2 = 0.
This is a form of Ricatti’s equation, which in general is diﬃcult to solve.
The
exception to this is if we can spot a solution, when the equation will linearize. For
this example, we can see that Y1 = 1/2x is a solution. Writing Y1 = 1/2x + V −1
linearizes the equation and it is straightforward to show that the general solution
is y = C1x1/2 + C2x1/2 log x.
At this stage, you could of course argue that you could have guessed the solu-
tion u1(x) = x1/2 and then reduced the order of the equation to obtain the second
solution u2(x) = x1/2 log x. The counter argument to this is that the group theo-
retical method gives both the technique of reduction of order and an algorithm for
reducing the original equation to a ﬁrst order diﬀerential equation. The point can
perhaps be reinforced by considering the nonautonomous equation
y′′ + 1
xy′ + ey = 0.
Using the methods derived in this chapter, we ﬁrst introduce a new dependent
variable, Y , deﬁned by y = −2 log x + Y . This gives us
Y ′′ + 1
xY ′ + eY
x2 = 0.
The invariance of this under x-magniﬁcation suggests introducing z = log x and
leads to
d 2Y
dz2 + eY = 0,

270
GROUP THEORETICAL METHODS
which we can immediately integrate to
1
2
dY
dz
2
= C −eY .
The analysis of this equation is of course much simpler than the original one.
10.7
Partial Diﬀerential Equations
The ideas given in this chapter can be considerably extended, particularly to the
area of partial diﬀerential equations. As a simple example, consider the equation
for the diﬀusion of heat that we derived in Chapter 2, namely
∂T
∂t = D∂2T
∂x2 .
(10.12)
This equation is invariant under the group of transformations
x = eϵx1,
t = e2ϵt1,
so that x/t1/2 is an invariant of the transformation (can you spot which other groups
it is invariant under?). If we write η = x/t1/2 (here η is known as a similarity
variable), this reduces the partial diﬀerential equation to the ordinary diﬀerential
equation
Dd 2T
dη2 + 1
2η dT
dη = 0.
For the initial condition T(x, 0) = H(−x), which is also invariant under the trans-
formation, it is readily established from the ordinary diﬀerential equation that the
solution is
T(x, t) =
1
√π
 ∞
x/
√
4Dt
e−s2 ds = 2erfc

x
√
4Dt

,
(10.13)
which is shown in Figure 10.1.
Finally, if the initial condition is T(x, 0) = δ(x), we can use the fact that (10.12)
is also invariant under the two-parameter group
x = eϵx1,
t = e2ϵt1,
T = eµT1.
The initial condition transforms to eµT1(x1, 0) = e−ϵδ(x1), since δ(ax) = δ(x)/a,
so that the choice µ = −ϵ makes both diﬀerential equation and initial condition
invariant. As before, x/t1/2 is an invariant, and now t1/2T is invariant as well. If
we therefore look for a solution of the form T(x, t) = t−1/2F(x/t1/2), we ﬁnd that
DFηη + 1
2ηFη + 1
2F = 0.
A suitable solution of this is Ae−η2/4D (for example, using the method of Frobenius),
and hence
T(x, t) = At−1/2e−x2/4Dt.
(10.14)

10.7 PARTIAL DIFFERENTIAL EQUATIONS
271
Fig. 10.1. The solution, (10.13), of the diﬀusion equation.
To ﬁnd the constant A, we can integrate (10.12) to obtain the integral conserva-
tion law,
d
dt
 ∞
−∞
T dx = 0.
Since
 ∞
−∞
T(x, 0) dx =
 ∞
−∞
δ(x) dx = 1,
we must have
' ∞
−∞T(x, t) dx = 1. Then, from (10.14), A = 1/
√
4πD, and hence
T(x, t) =
1
√
4πDt
e−x2/4Dt.
(10.15)
This is known as the point source solution of the diﬀusion equation. Notice the
similarity between this solution and the sequence (5.27), which we can use to deﬁne
the Dirac delta function, as shown in Figure 5.4.
Further details of extensions to the basic method can be found in Bluman and
Cole (1974) and Hydon (2000). We end this chapter with a recommendation. Given
an ordinary or partial diﬀerential equation, try to ﬁnd a simple invariant, such as
translation, magniﬁcation or rotation. If you ﬁnd one, this will lead to a reduction
of order for an ordinary diﬀerential equation, or the transformation of a partial
diﬀerential equation into an ordinary diﬀerential equation. If you cannot ﬁnd an
invariant by inspection, then you need to make a systematic search for invariants.

272
GROUP THEORETICAL METHODS
This can involve a lot of calculation, and is best done by a computer algebra package.
Hydon (2000) gives a list and critique of some widely available packages.
Exercises
10.1
Show that each of the diﬀerential equations
(a) xdy
dx + y = x4
dy
dx
2
,
(b) xdy
dx −yxm = 0,
(c)
dy
dx
2
= y + x2,
is invariant under a group of transformations with inﬁnitesimal generator
Uφ = ax∂φ
∂x + by ∂φ
∂y ,
and hence integrate each equation.
10.2
Find the general diﬀerential equation of ﬁrst order, invariant under the
groups
(a) Uφ = ∂φ
∂x + x∂φ
∂y ,
(b) Uφ = x∂φ
∂x + ay ∂φ
∂y ,
(c) Uφ = y ∂φ
∂x + ∂φ
∂y .
10.3
If the diﬀerential equation
dx
P(x, y) =
dy
Q(x, y)
is invariant under a group with inﬁnitesimal generators ξ and η, show that
it has a solution in integrating factor form,
 P dy −Q dx
Pη −Qξ
= constant.
Hence ﬁnd the solution of
dy
dx = y4 −2x3y
2xy3 −x4 .
10.4
Show that the diﬀerential equation
d 2y
dx2 + p(x)y2 = 0
is invariant under at most six one-parameter groups. For the case p(x) =
xm, reduce the order of the equation.

EXERCISES
273
10.5
Find the most general ﬁrst order diﬀerential equation invariant under the
group with inﬁnitesimal generator
Uφ = exp

p(x)dx
	 ∂φ
∂y .
10.6
Derive the integrating factor for y′ + P(x)y = Q(x) by group theoretical
methods.
10.7
Find a similarity reduction of the porous medium equation, ut =
(uux)x. Can you ﬁnd any solutions of the resulting diﬀerential equation?
10.8
The equation ut = uxx −up, with p ∈R+, arises in mathematical models
of tumour growth, and other areas. Find some invariants of this equation,
and the corresponding ordinary diﬀerential equations. Can the point source
problem be resolved for this sink-like equation?

CHAPTER ELEVEN
Asymptotic Methods: Basic Ideas
The vast majority of diﬀerential equations that arise as models for real physical
systems cannot be solved directly by analytical methods.
Often, the only way
to proceed is to use a computer to calculate an approximate, numerical solution.
However, if one or more small, dimensionless parameters appear in the diﬀeren-
tial equation, it may be possible to use an asymptotic method to obtain an
approximate solution. Moreover, the presence of a small parameter often leads to
a singular perturbation problem, which can be diﬃcult, if not impossible, to
solve numerically.
Small, dimensionless parameters usually arise when one physical process occurs
much more slowly than another, or when one geometrical length in the problem
is much shorter than another. Examples occur in many diﬀerent areas of applied
mathematics, and we will meet several in Chapter 12. As we shall see, dimensionless
parameters arise naturally when we use dimensionless variables, which we discussed
at the beginning of Chapter 5. Some other examples are:
— Waves on the surface of a body of ﬂuid or an elastic solid, with amplitude a and
wavelength λ, are said to be of small amplitude if ϵ = a/λ ≪1. A simpliﬁcation
of the governing equations based on the fact that ϵ ≪1 leads to a system of
linear partial diﬀerential equations (see, for example, Billingham and King, 2001).
This is an example of a regular perturbation problem, where the problem is
simpliﬁed throughout the domain of solution.
— In the high speed ﬂow of a viscous ﬂuid past a ﬂat plate of length L, pressure
changes due to accelerations are much greater than those due to viscous stresses,
as expressed by Re = ρUL/µ ≫1. Here, Re is the Reynolds number, a dimen-
sionless parameter that measures the ratio of acceleration to viscous forces, ρ is
the ﬂuid density, U the ﬂuid velocity away from the plate and µ the ﬂuid vis-
cosity. The solution for Re−1 ≪1 has the ﬂuid velocity equal to U everywhere
except for a small neighbourhood of the plate, known as a boundary layer,
where viscosity becomes important (see, for example, Acheson, 1990). This is
an example of a singular perturbation problem, where an apparently negligible
physical eﬀect, here viscosity, becomes important in a small region.
— In aerodynamics, it is crucial to be able to calculate the lift force on the cross-
section of a wing due to the ﬂow of an inviscid ﬂuid around it. These cross-
sections are usually long and thin with aspect ratio ϵ = l/L ≪1, where l is a
typical vertical width, and L a typical horizontal length. Thin aerofoil theory

11.1 ASYMPTOTIC EXPANSIONS
275
exploits the size of ϵ to greatly simplify the calculation of the lift force (see, for
example, Milne-Thompson, 1960, and Van Dyke, 1964).
11.1
Asymptotic Expansions
In this chapter, we will study how to approximate various integrals as series ex-
pansions, whilst in Chapter 12, we will look at series solutions of diﬀerential equa-
tions. These series are asymptotic expansions. The crucial diﬀerence between
asymptotic expansions and the power series that we met earlier in the book is that
asymptotic expansions need not be convergent in the usual sense. We can illustrate
this using an example, but ﬁrstly, there is some useful notation for comparing the
sizes of functions that we will introduce here and use extensively later.
11.1.1
Gauge Functions
(i) If
lim
ϵ→0
f(ϵ)
g(ϵ) = A,
for some nonzero constant A, we write f(ϵ) = O (g (ϵ)) for ϵ ≪1. We say
that f is of order g for small ϵ. Here g(ϵ) is a gauge function, since it is
used to gauge the size of f(ϵ). For example, when ϵ ≪1,
sin ϵ = O(ϵ),
cos ϵ = O(1),
e−ϵ = O(1),
cos ϵ −1 = O(ϵ2),
all of which can be found from the Taylor series expansions of these functions.
This notation tells us nothing about the constant A. For example, 1010 =
O(1). The order notation only tells us how functions behave as ϵ →0. It is
not meant to be used for comparing constants, which are all, by deﬁnition,
of O(1).
(ii) We also have a notation available that displays more information about the
behaviour of the functions. If
lim
ϵ→0
f(ϵ)
g(ϵ) = 1,
we write f(ϵ) ∼g(ϵ), and say that f(ϵ) is asymptotic to g(ϵ) as ϵ →0. For
example,
sin ϵ ∼ϵ,
1 −cos ϵ ∼1
2ϵ2,
eϵ −1 ∼ϵ,
as ϵ →0.
(iii) If
lim
ϵ→0
f(ϵ)
g(ϵ) = 0,
we write f(ϵ) = o (g (ϵ)), and say that f is much less than g. For example
sin ϵ = o(1),
cos ϵ = o(ϵ−1),
e−ϵ = o(log ϵ)
for ϵ ≪1.

276
ASYMPTOTIC METHODS: BASIC IDEAS
11.1.2
Example: Series Expansions of the Exponential Integral,
Ei(x)
Let’s consider the exponential integral
Ei(x) =
 ∞
x
e−t
t
dt,
(11.1)
for x > 0. We can integrate by parts to obtain
Ei(x) =

−e−t
t
∞
x
−
 ∞
x
e−t
t2 dt = e−x
x
−
 ∞
x
e−t
t2 dt.
Doing this repeatedly leads to
Ei(x) = e−x
N

m=1
(−1)m−1 (m −1)!
xm
+ RN,
(11.2)
where
RN = (−1)NN!
 ∞
x
e−t
tN+1 dt.
(11.3)
This result is exact. Now, let’s consider how big RN is for x ≫1. Using the fact
that x−(N+1) > t−(N+1) for t > x,
|RN| = N!
 ∞
x
e−t
tN+1 dt ⩽
N!
xN+1
 ∞
x
e−t dt = N!e−x
xN+1 ,
and hence
|RN| = O

e−xx−(N+1)
for x ≫1.
Therefore, if we truncate the series expansion (11.2) at a ﬁxed value of N by
neglecting RN, it converges to Ei(x) as x →∞. This is our ﬁrst example of an
asymptotic expansion.
In common with most useful asymptotic expansions,
(11.2) does not converge in the usual sense. The ratio of the (N + 1)th and N th
terms is
(−1)NN!
xN+1
xN
(−1)N−1(N −1)! = −N
x ,
which is unbounded as N →∞, so the series diverges, by the ratio test. How-
ever, for x even moderately large, (11.2) provides an extremely eﬃcient method of
calculating Ei(x), as we shall see.
In order to develop the power series representation of Ei(x) about x = 0, we
would like to use the series expansion of e−t. However, term by term integration is
not possible as things stand, since the integrals will not be convergent. Moreover,
the integral for Ei(x) does not converge as x →0. We therefore have to be a little
more cunning in order to obtain our power series. Firstly, note that
Ei(x) =
 ∞
x
1
t

e−t −
1
1 + t

dt +
 ∞
x
1
t(1 + t) dt

11.1 ASYMPTOTIC EXPANSIONS
277
=
 ∞
x
1
t

e−t −
1
1 + t

dt −log

x
1 + x

.
With this rearrangement, we can take the limit x →0 in the ﬁrst integral, with
the second integral giving Ei(x) ∼−log x as x →0. A further rearrangement then
gives
Ei(x) =
 ∞
0
1
t

e−t −
1
1 + t

dt −
 x
0
1
t

e−t −1

dt
−
 x
0
1
1 + t dt −log

x
1 + x

=
 ∞
0
1
t

e−t −
1
1 + t

dt −log x −
 x
0
1
t

e−t −1

dt.
The ﬁrst term is just a constant, which we could evaluate numerically, but can be
shown to be equal to minus Euler’s constant, γ = 0.5772 . . . , although we will not
prove this here. We can now use the power series representation of e−t to give
Ei(x) = −γ −log x −
 x
0
1
t
∞

n=1
(−t)n
n!
dt.
Since this power series converges uniformly for all t, we can interchange the order
of summation and integration, and ﬁnally arrive at
Ei(x) = −γ −log x −
∞

n=1
(−x)n
n · n! .
(11.4)
This representation of Ei(x) is convergent for all x > 0.
The philosophy that we have used earlier in the book is that we can now use (11.4)
to calculate the function for any given x. There are, however, practical problems
associated with this. Firstly, the series converges very slowly. For example, if we
take x = 5 we need 20 terms of the series in order to get three-ﬁgure accuracy.
Secondly, even if we take enough terms in the series to get an accurate answer,
the result is the diﬀerence of many large terms.
For example, the largest term
in the series for Ei(20) is approximately 2.3 × 106, whilst for Ei(40) this rises to
approximately 3.8 × 1014. Unless the computer used to sum the series stores many
signiﬁcant ﬁgures, there will be a large roundoﬀerror involved in calculating the
Ei(x) as the small diﬀerence of many large terms. These diﬃculties do not arise for
the asymptotic expansion, (11.2). Consider the two-term expansion
Ei(x) ∼e−x
x

1 −1
x

.
Figure 11.1 shows the relative error using this expansion compared with that using
the ﬁrst 25 terms of the convergent series representation, (11.4). The advantages
of the asymptotic series over the convergent series are immediately apparent. Note
that the convergent series (11.4) also provides an asymptotic expansion valid for
x ≪1. We conclude that asymptotic expansions can also be convergent series.

278
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.1. The relative error in calculating Ei(x) using the ﬁrst 25 terms of the convergent
series expansion (11.4), solid line, and the two-term asymptotic expansion (11.2), dashed
line.
To summarize, let
SN(x) =
N

n=0
fn(x).
— If SN is a convergent series representation of S(x), SN →S(x) as N →∞for
ﬁxed x within the radius of convergence.
— If SN is an asymptotic series representation of S(x), SN(x) ∼S(x) as x →∞
(or whatever limit is appropriate) for any ﬁxed N ⩾0.
11.1.3
Asymptotic Sequences of Gauge Functions
Let δn(ϵ), n = 0, 1, 2, . . . be a sequence of functions such that δn = o(δn−1) for
ϵ ≪1. Such a sequence is called an asymptotic sequence of gauge functions.
For example, δn = ϵn, δn = ϵn/2, δn = (cot ϵ)−n, δn = (−log ϵ)−n. If we can write
f(ϵ) ∼
N

n=0
anδn(ϵ)
as ϵ →0,
for some sequence of constants an, we say that f(ϵ) has an asymptotic expan-
sion relative to the asymptotic sequence δn(ϵ) for ϵ ≪1. For example, Ei(x) has

11.1 ASYMPTOTIC EXPANSIONS
279
an asymptotic expansion, (11.2), relative to the asymptotic sequence x−ne−x for
x−1 ≪1. The asymptotic expansion of a function relative to a given sequence of
gauge functions is unique, since each an can be calculated in turn from
a0 = lim
ϵ→0
f(ϵ)
δ0(ϵ),
an = lim
ϵ→0

f(ϵ) −
n−1

m=0
anδm(ϵ)

/δn(ϵ),
for n = 1, 2, . . . . However, a given function can have diﬀerent asymptotic expansions
with respect to diﬀerent sequences of gauge functions. For example,
if δn(ϵ) = ϵn,
sin ϵ =
∞

n=1
(−1)n+1
ϵ2n−1
(2n −1)!,
if δn(ϵ) = (1 −eϵ)n,
sin ϵ = δ0 + 1
2δ1 + 1
3δ2 + 5
4δ3 + · · · .
Some sequences of gauge functions are clearly easier to use than others. We also
note that diﬀerent functions may have the same asymptotic expansion when we
consider only a ﬁnite number of terms. For example,
eϵ ∼
N

n=0
ϵn
n!
as ϵ →0+, for any N > 0,
eϵ + e−1/ϵ ∼
N

n=0
ϵn
n!
as ϵ →0+, for any N > 0.
Since e−1/ϵ is exponentially small, it will only appear in the asymptotic expansion
after all of the algebraic terms.
Now let’s consider functions of a single variable x and a parameter ϵ, f(x; ϵ).
We can think of this as a typical solution of an ordinary diﬀerential equation with
independent variable x and a small parameter, ϵ. If f has an asymptotic expansion,
f(x; ϵ) ∼
N

n=0
fn(x)δn(ϵ)
as ϵ →0,
that is valid for all x in some domain R, we say that the expansion is uniformly
valid in R. If this is not the case, we say that the expansion becomes nonuniform
in some subdomain. For example,
sin(x + ϵ) = sin x + ϵ cos x −1
2!ϵ2 sin x −1
3!ϵ3 cos x + O(ϵ4)
as ϵ →0,
for all x, so the expansion is uniformly valid. Now consider
√
x + ϵ = √x

1 + ϵ
x
1/2
∼√x

1 + ϵ
2x −ϵ2
8x2 + O(ϵ3)
	
as ϵ →0,
provided x ≫ϵ. We say that the expansion becomes nonuniform as x →0, when
x = O(ϵ). Note that each successive term is smaller by a factor of ϵ/x, and therefore
the expansion fails to be asymptotic when x = O(ϵ). To determine an asymptotic

280
ASYMPTOTIC METHODS: BASIC IDEAS
expansion valid when x = O(ϵ), we deﬁne a new, scaled variable, a procedure
that we will use again and again later, given by x = ϵX, with X = O(1) as ϵ →0.
In terms of X,
√
x + ϵ = ϵ1/2√
X + 1.
This is the trivial asymptotic expansion valid for X = O(1)†.
11.2
The Asymptotic Evaluation of Integrals
We have already seen in Chapters 5 and 6 that the solution of a diﬀerential equation
can often be found in closed form in terms of an integral. Instead of trying to
develop the asymptotic solution of a diﬀerential equation as some parameter or
variable becomes small, it is often more convenient to ﬁnd an integral expression for
the solution and then seek an asymptotic expansion of the integral. Let’s consider
integrals of the type
I(λ) =

C
eλf(t)g(t) dt,
(11.5)
with f and g analytic functions of t, λ real and positive and C a contour that
joins two distinct points in the complex t-plane, one or both of which may be at
inﬁnity.
Such integrals arise very commonly in this context.
For example, the
Fourier transform, which we studied in Chapter 5, takes this form with f(t) = it,
whilst the Laplace inversion integral, which we discussed in Section 6.4, has f(t) = t.
Another common example is the solution of Airy’s equation, y′′ −xy = 0, which
we wrote in terms of Bessel functions in Section 3.8. The solution y = Ai(x) can
also be written in integral form as
Ai(x) =
1
2πi

C
ext−1
3 t3 dt.
(11.6)
To see that this is a solution of Airy’s equation, note that
Ai′′ −xAi =
1
2πi

C
(t2 −x)ext−1
3 t3 dt = −1
2πi

C
d
dt

ext−1
3 t3
dt = 0,
provided that ext−1
3 t3 →0 as |t| →∞on the contour C. For |t| ≫1, xt −1
3t3 ∼
−1
3t3, so we need Re(t3) > 0 on the contour C, and therefore −π
6 < arg(t) <
π
6 ,
π
2 < arg(t) <
5π
6
or −π
2 > arg(t) > −5π
6 .
As we shall see later, Ai(x) is
distinguished from Bi(x) in that Ai(x) →0 as x →∞, in particular, with Ai(x) ∼
x−1/4 exp

−2
3x3/2
/2√π. This means that an appropriate contour C originates
with arg(t) = −2π
3 and terminates with arg(t) = 2π
3 (for example, the contour C in
Figure 11.10).
Rather than diving straight in and trying to evaluate I(λ) in (11.5), we can
proceed by considering two simpler cases ﬁrst.
† and of course valid for all X in this simple example.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
281
11.2.1
Laplace’s Method
Consider the complex integral along the real line
I1(λ) =
 b
a
eλf(t)g(t) dt with f(t) real and λ ≫1.
(11.7)
In this case, the integrand is largest at the point where f(t) is largest. In fact,
we can approximate the integral by simply considering the contribution from the
neighbourhood of the point where f(t) takes its maximum value. This is known as
Laplace’s Method
Integrals of the form (11.7) can be estimated by
considering the contribution from the neighbour-
hood of the maximum value of f(t) alone.
We will see how we can justify this rigorously later. For the moment, let’s consider
some examples.
Example 1
Consider the integral representation
Kν(x) =
 ∞
0
e−x cosh t cosh νt dt,
of the modiﬁed Bessel function, Kν(x). When x ≫1, this is in the form (11.7)
with f(t) = −cosh t and g(t) = cosh νt. The maximum value of f(t) = −cosh t is
at t = 0, where f(t) = g(t) = 1. For t ≪1, cosh t ∼1 + 1
2t2, and we can therefore
use Laplace’s method to approximate Kν(x) as
Kν(x) ∼
 ∞
0
e−x(1+ 1
2 t2) dt = e−x
 ∞
0
e−1
2 xt2 dt.
After making the substitution t = ˆt
#
2/x, this becomes
Kν(x) ∼
*
2
xe−x
 ∞
0
e−ˆt2 dˆt =
* π
2xe−x,
for x ≫1,
using (3.5).
Example 2
Consider the deﬁnition,
Γ(1 + λ) =
 ∞
0
tλe−t dt =
 ∞
0
eλ log t−t dt,
of the gamma function, which we met in Section 3.1. Can we ﬁnd the asymptotic
form of the gamma function when λ ≫1? Since the deﬁnition is not quite in the
form that we require for Laplace’s method, we need to make a transformation. If
we let t = λτ, we ﬁnd that
Γ(1 + λ) = λ1+λ
 ∞
0
eλ(log τ−τ) dτ.

282
ASYMPTOTIC METHODS: BASIC IDEAS
This is in the form of (11.7) with f(τ) = log τ −τ. Since f ′(τ) = 1/τ −1 and
f ′′(τ) = −1/τ 2 < 0, f has a local maximum at τ = 1. Laplace’s method states that
Γ(1 + λ) is dominated by the contribution to the integral from the neighbourhood
of τ = 1. We use a Taylor series expansion,
log τ −τ = log {1 + (τ −1)} −1 −(τ −1) = −1 −1
2(τ −1)2 + · · ·
for |τ −1| ≪1,
and extend the range of integration, to obtain
Γ(1 + λ) ∼λ1+λe−λ
 ∞
−∞
e−1
2 λT 2 dT
as λ →∞.
If we now let T = ˆT/λ1/2, we ﬁnd that
Γ(1 + λ) ∼λλ+ 1
2 e−λ
 ∞
−∞
e−1
2 ˆT 2 d ˆT,
and hence
λ! = Γ(1 + λ) ∼
√
2πλλ+ 1
2 e−λ as λ →∞.
(11.8)
This is known as Stirling’s formula, and provides an excellent approximation to
the gamma function, for λ > 2, as shown in Figure 11.2.
Example 3
Consider the integral
I(λ) =
 10
0
e−λt
1 + t dt.
In this case, f = −t and g = 1/(1 + t). Since f ′(t) = −1 ̸= 0 and the maximum
value of f occurs at t = 0 for t ∈[0, 10],
I(λ) ∼
 10
0
e−λt dt = 1
λ

1 −e−10λ
∼1
λ as λ →∞.
In fact we can use the binomial expansion
(1 + t)−1 = 1 −t + t2 −t3 + · · · + (−1)ntn + · · · ,
even though this is only convergent for |t| < 1, since the integrand is exponentially
small away from t = 0. We can also extend the range of integration to inﬁnity
without aﬀecting the result, to give
I(λ) ∼
N

n=0
 ∞
0
(−1)ntne−λt dt =
N

n=0
(−1)nΓ(n + 1)
λn+1
=
N

n=0
(−1)nn!
λn+1
as λ →∞,
since, using the substitution τ = λt,
 ∞
0
tne−λt dt =
1
λn+1
 ∞
0
τ ne−τ dτ =
1
λn+1 Γ(n + 1) =
n!
λn+1 .
Note that, as we found for Ei(x), this is an asymptotic, rather than convergent,
series representation of I(x).
We can justify this procedure using the following
lemma.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
283
Fig. 11.2. The gamma function, Γ(1 + λ) = λ!, and its asymptotic approximation using
Stirling’s formula, (11.8).
Lemma 11.1 (Watson’s lemma) If
I(λ) =
 A
0
e−λtg(t) dt,
for A > 0, with g either bounded or having only integrable singularities,
g(t) ∼
N

n=0
antαn
as t →0,
and −1 < α0 < α1 < · · · < αN (which ensures that e−λtg(t) has at worst an
integrable singularity at t = 0), then
I(λ) ∼
 ∞
0
e−λt
N

n=0
antαn dt =
N

n=0
anλ−αn−1Γ(αn + 1) as λ →∞.
Note that this lemma simply says that the integral is dominated by the contribu-
tion in the neighbourhood of t = 0, so that we can replace g with its asymptotic
expansion and extend the range of integration to inﬁnity.

284
ASYMPTOTIC METHODS: BASIC IDEAS
Proof We begin by separating oﬀthe contribution to the integral from the neigh-
bourhood of the origin by noting that
I(λ) −
N

n=0
anλ−αn−1Γ(αn + 1)
 =
I(λ) −
N

n=0
an
 ∞
0
tαne−λt dt

⩽

 δ
0
e−λtg(t) dt −
 ∞
0
e−λt
N

n=0
antαn dt
 +

 A
δ
e−λtg(t) dt

for any real δ with 0 < δ < A. Next, we make use of the asymptotic behaviour of
g. Since
g(t) −
N

n=0
antαn
 < Ktαn+1
for some K > 0 when 0 < t < δ and δ is suﬃciently small, we have
I(λ) −
N

n=0
anλ−αn−1Γ(αn + 1)

<

 δ
0
e−λt
N

n=0
antαn dt + K
 δ
0
e−λttαn+1 dt −
 ∞
0
e−λt
N

n=0
antαn dt

+

 A
δ
e−λtg(t) dt

<
−
N

n=0
an
 ∞
δ
tαne−λt dt + K
 ∞
0
e−λttαn+1 dt −K
 ∞
δ
e−λttαn+1 dt

+

 A
δ
e−λtg(t) dt

<
N

n=0
|an|
 ∞
δ
tαne−λt dt + Kλ−αn+1−1Γ(αn+1 + 1)
+K
 ∞
δ
e−λttαn+1 dt +
 A
δ
e−λt|g(t)| dt.
Now, since e−λt < e−(λ−1)δe−t for t > δ,
 ∞
δ
tαne−λt dt < e−(λ−1)δ
 ∞
δ
tαne−t dt < e−(λ−1)δΓ(αn + 1),
and also
 A
δ
e−λt|g(t)| dt < Ge−λδ,

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
285
for t ∈[δ, A], where G =
' A
δ |g(t)| dt. This ﬁnally shows that
I(λ) −
N

n=0
anλ−αn−1Γ(αn + 1)
 < Kλ−αn+1−1Γ(αn+1 + 1) + Ge−λδ
+e−(λ−1)δ
 N

n=0
|an|
 ∞
δ
tαne−t dt + K
 ∞
δ
tαn+1e−t dt

= O(λ−αn+1−1),
and hence the result is proved.
A simple modiﬁcation of this proof can be used to justify the use of Laplace’s
method in general.
11.2.2
The Method of Stationary Phase
Let’s now consider an integral, again along the real line, of the form
I2(λ) =
 b
a
eiλF (t)g(t) dt,
with F(t) real and λ ≫1.
(11.9)
Integrals of this type arise when Fourier transforms are used to solve diﬀerential
equations (see Chapter 5). Points where F ′(t) = 0 are called points of stationary
phase, and the integral can be evaluated by considering the sum of the contribu-
tions from each of these points. We will consider this more carefully in the next
section. For the moment, we can illustrate why this should be so by considering,
as an example, the case F(t) = (1 −t)2, g(t) = t, which has F ′(t) = 0 when t = 1,
and
g(t)eiλF (t) = t cos λ(1 −t)2 + it sin λ(1 −t)2.
The rapid oscillations of this integrand lead to almost complete cancellation, except
in the neighbourhood of the point of stationary phase, as can be seen in Figure 11.3.
Let’s consider the situation when there is a single point of stationary phase at
t = t0. Then, since F ′(t0) = 0,
F(t) ∼F(t0) + 1
2(t −t0)2F ′′(t0) + O

(t −t0)3
for |t −t0| ≪1,
provided that F ′′(t0) ̸= 0 (see Exercise 11.11). If we assume that the integral is
dominated by the contribution from the neighbourhood of the point of stationary
phase,
I2(λ) ∼
 t0+δ
t0−δ
g(t0) exp

iλ

F(t0) + 1
2F ′′(t0)(t −t0)2
	
dt
= eiλF (t0)g(t0)
 t0+δ
t0−δ
exp
1
2iλF ′′(t0)(t −t0)2
	
dt

286
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.3. The function t cos λ(1 −t)2 for λ = 5, 10 and 20.
for some δ ≪1. If we now let
T = (t −t0)
*
1
2λ|F ′′(t0)|,
this becomes
I2(λ) ∼eiλF (t0)g(t0)
+
2
λ|F ′′(t0)|
 δ√
1
2 λ|F ′′(t0)|
−δ√
1
2 λ|F ′′(t0)|
ei sgn{F ′′(t0)}T 2 dT,
and, at leading order as λ →∞,
I2(λ) ∼eiλF (t0)g(t0)
+
2
λ|F ′′(t0)|
 ∞
−∞
e±iT 2 dT.
We now just need to calculate
J =
 ∞
0
eiT 2 dT = 1
2
 ∞
−∞
eiT 2 dT.
To do this, consider the contours C1, C2 and C3 in the complex T-plane, illustrated

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
287
in Figure 11.4. Considering the contour C2 ﬁrst,
C1
C2
C3
Re(T)
Im(T)
Fig. 11.4. The contours C1, C2 and C3 in the complex T-plane.


C2
eiT 2 dT
 =

 π/4
0
eiR2(cos 2θ+i sin 2θ)iReiθ dθ

⩽R
 π/4
0
e−R2 sin 2θ θ →0
as R →∞,
by Jordan’s lemma. On the remaining contours,

C3
eiT 2 dT =
 0
R
e−ˆT 2eiπ/4 d ˆT →−
√π
2 eiπ/4 as R →∞,
and

C1
eiT 2 dT =
 R
0
eiT 2 dT →
 ∞
0
eiT 2 dT
as R →∞.
By Cauchy’s theorem

C1+C2+C3
eiT 2 dT = 0,
and hence
 ∞
0
eiT 2 dT =
√π
2 eiπ/4.

288
ASYMPTOTIC METHODS: BASIC IDEAS
Similarly,
 ∞
0
e−iT 2 dT =
√π
2 e−iπ/4.
We conclude that
I2(λ) ∼eiλF (t0)esgn(F ′′(t0))iπ/4g(t0)
+
2π
λ|F ′′(t0)| = O(λ−1/2) as λ →∞.
In our example, F(t) = (1 −t)2, so that t0 = 1, F(t0) = 0 and F ′′(t0) = 2, and
g(t) = t, so that g(t0) = 1. We conclude that
 b
a
teiλ(1−t)2 dt ∼e−iπ/4
*π
λ as λ →∞, when a < 1 < b.
(11.10)
If a = 1 or b = 1, we get half of the contribution from the stationary phase point.
If a > 1 or b < 1, there are no points of stationary phase. In this case, we can, in
general, integrate by parts to obtain
I2(λ) ∼−i
λ
 g(b)
F ′(b)eiλF (b) −g(a)
F ′(a)eiλF (a)
	
+ O
 1
λ2

.
(11.11)
Note that the contribution from the endpoints is of O(λ−1), whilst contributions
from points of stationary phase are larger, of O(λ−1/2).
Example
Consider (5.40), the solution of an initial value problem for the wave equation
that we considered in Section 5.5.2. Let’s analyze this solution when t ≫1 at a
point x = vt that moves with constant velocity, v. Since (5.40) is written in a
form independent of the orientation of the coordinate axes, we can assume that
v = v(0, 0, 1). Consider
I+ =
 ∞
−∞
 ∞
−∞
 ∞
−∞
g(k)eitF+(k) dkx dky dkz,
where
g(k) =
˜f(k)
16π3ick ,
F+(k) = c
$
k2x + k2y + k2z −vkz
and k = (kx, ky, kz). Starting with the kx integral, we can see that
∂F+
∂kx
=
ckx
$
k2x + k2y + k2z
,
which is zero when kx = 0. This is, therefore, a unique point of stationary phase
and, noting that
∂2F+
∂k2x

kx=0
=
c
$
k2y + k2z
,

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
289
the method of stationary phase shows that
I+ ∼eiπ/4
*
2π
ct
 ∞
−∞
 ∞
−∞

k2
y + k2
z
1/4 ˆg(ky, kz)eit ˆ
F+(ky,kz) dky dkz,
where
ˆg(ky, kz) =
˜f(0, ky, kz)
16π3ick
,
ˆF+(ky, kz) = c
$
k2y + k2z −vkz.
Similarly, we can use the method of stationary phase on the ky integral, and arrive
at
I+ ∼
1
8π2c2t
 ∞
−∞
˜f(0, 0, kz)ei(c−v)tkz dkz.
When v ̸= c, a simple change of variable, ˆkz = tkz, shows that I+ = O(1/t2).
However, when v = c, I+ is much larger, of O(1/t). This is consistent with the fact
that disturbances propagate at speed c. We therefore assume that v = c, so that
we are considering a point moving in the z-direction with constant speed c, which
gives us
I+ ∼
1
8π2c2t
 ∞
−∞
˜f(0, 0, kz) dkz.
If we now deﬁne
I−=
 ∞
−∞
 ∞
−∞
 ∞
−∞
g(k)eitF−(k) dkx dky dkz,
with
F−(k) = −c
$
k2x + k2y + k2z −vkz,
and follow the analysis through in the same way, we ﬁnd that
I−∼
1
8π2c2t
 ∞
−∞
˜f(0, 0, kz)e−i(c+v)tkz dkz.
Since we have already decided to consider the case v = c, I−= O(1/t2) for t ≫1,
and therefore
u(vt) ∼I+ ∼
1
8π2c2t
 ∞
−∞
˜f(0, 0, kz) dkz,
where v = c(0, 0, 1). Since the z-direction can be chosen arbitrarily in this problem,
we conclude that the large time solution is small, of O(1/t2), except on the surface
of a sphere of radius ct, where the solution is given by
u(cte) ∼
1
8π2c2t
 ∞
−∞
˜f(se) ds = O
1
t

,
(11.12)
with e an arbitrary unit vector. The amplitude of the solution therefore decays like
1/t, and also depends upon the total frequency content of the initial conditions,
(5.38), in the direction of e, as given by the integral of their Fourier transform,
' ∞
−∞˜f(se) ds.

290
ASYMPTOTIC METHODS: BASIC IDEAS
In summary,
The Method of Stationary Phase
Integrals of the form (11.9) can be estimated by
considering the contribution from the neighbour-
hood of each point of stationary phase, where
F ′(t) = 0. In the absence of such points, the in-
tegral is dominated by the contributions from the
endpoints of the range of integration.
11.2.3
The Method of Steepest Descents
Let’s return to consider the more general case,
I(λ) =

C
eλf(t)g(t) dt
(11.13)
for λ ≫1, f and g analytic functions of t = x + iy and C a contour in the complex
t-plane. Laplace’s method and the method of stationary phase are special cases
of this. Since f(t) and g(t) are analytic, we can deform the contour C without
changing I(λ). If we deform C onto a contour C1, on which the imaginary part of
f(t) is a constant, f(t) = φ(t) + iψ0, then (11.13) becomes
I(λ) = eiλψ0

C1
eλφ(t)g(t) dt,
and we can simply use Laplace’s method. This is what we really want to do, because
we have shown rigorously, in Lemma 11.1, that this sort of integral is dominated by
the neighbourhood of the point on C1 where the real-valued function φ(t) is largest.
In order to take this further, we need to know what the curves φ = constant
and ψ = constant look like for an analytic function f(t) = φ(x, y) + iψ(x, y). The
Cauchy–Riemann equations for the analytic function f(t) are (see Appendix 6)
∂φ
∂x = ∂ψ
∂y ,
∂φ
∂y = −∂ψ
∂x ,
which show that
∇φ · ∇ψ = ∂φ
∂x
∂ψ
∂x + ∂φ
∂y
∂ψ
∂y = −∂φ
∂x
∂φ
∂y + ∂φ
∂y
∂φ
∂x = 0,
and hence that ∇φ is perpendicular to ∇ψ. Recall that ∇φ is normal to lines of
constant φ, from which we conclude that the lines of constant φ are perpendicular
to the lines of constant ψ. Moreover, φ changes most rapidly in the direction of ∇φ,
in other words on the lines of constant ψ. We say that the lines where ψ is constant
are contours of steepest descent and ascent for φ. Our strategy for evaluating
the integral (11.13) is therefore to deform the contour of integration into one of
steepest descent. Figure 11.5 illustrates these ideas for the function f(t) = t2.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
291
Fig. 11.5. The real part, φ(x, y) = x2 −y2, of f(t) = t2, and the lines of steepest ascent
and descent for the analytic function f(t) = t2.
Example: f(t) = t2
When f(t) = t2,
I(λ) =

C
g(t)eλt2 dt,
with C a contour that joins two points, P and Q, in the complex t-plane. In this
case, f(t) = t2 = (x + iy)2 = x2 −y2 + 2ixy, and hence φ = x2 −y2 and ψ = 2xy.
Therefore, φ is constant on the hyperbolas x2 −y2 = φ and ψ is constant on the
hyperbolas xy = 1
2ψ, as shown in Figure 11.5.
Case 1: ψ(P) > 0, ψ(Q) > 0 and φ(P) ̸= φ(Q)
In this case, the ends of the contour C lie in the upper half plane, and without
loss of generality, we take φ(P) > φ(Q). We can deform C into the contour C1+C2,
with C1 the steepest descent contour on which ψ = ψ(P) and C2 the contour on
which φ = φ(Q), as shown in Figure 11.6. On C1 we can therefore make the change
of variable
f(t) = f(P) −τ,
with 0 ⩽τ ⩽φ(P) −φ(Q).
The real part of f varies from φ(P) to φ(Q) as τ varies from zero to φ(P) −φ(Q),
whilst ψ = ψ(P) is constant. Since dτ/dt = −f ′(t),

C1
g(t)eλf(t) dt = −eλf(P )
 φ(P )−φ(Q)
0
g(t)
f ′(t)e−λτdτ
∼−eλf(P ) g(P)
f ′(P)
1
λ as λ →∞,

292
ASYMPTOTIC METHODS: BASIC IDEAS
P
Q
C1
C2
Re(t)
Im(t)
ψ = ψ(P)
ψ = ψ(Q)
φ = φ(Q)
φ = φ(P)
•
•
Fig. 11.6. The contours C1 and C2 in case 1.
using Watson’s lemma, provided that g(P) is bounded and nonzero. On C2, φ =
φ(Q) and the imaginary part, ψ, varies. We can write


C2
g(t)eλf(t) dt
 =
eλφ(Q)

C2
g(t)eiλψ(t) dt
 ⩽GLeλφ(Q) ≪

C1
g(t)eλf(t) dt,
since φ(Q) < φ(P), where G = maxC2 |g(t)| and L = length of C2. We conclude
that
I(λ) ∼−eλf(P ) g(P)
f ′(P)
1
λ as λ →∞.
Case 2: ψ(P) > 0, ψ(Q) > 0 and φ(P) = φ(Q)
In this case, we must deform C into C1 +C2 +C3, with ψ constant on C1 and C3
and φ < φ(P) constant on C3, as shown in Figure 11.7. The exact choice of contour
C2 is not important, since, as in case 1, its contribution is exponentially smaller
than the contributions from C1 and C3. This is very similar to case 1, except that
we now also have, using Laplace’s method, a contribution at leading order from C3.
We ﬁnd that
I(λ) ∼

−eλf(P ) g(P)
f ′(P) + eλf(Q) g(Q)
f ′(Q)
	 1
λ as λ →∞.
Let’s consider a more interesting example.

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
293
P
Q
C1
C2
C3
Re(t)
Im(t)
ψ = ψ(Q)
ψ = ψ(P)
φ = φ(P) = φ(Q)
φ < φ(P)
•
•
Fig. 11.7. The contours C1, C2 and C3 in case 2.
Example: The Bessel function of order zero with large argument
We saw in Chapter 3 that the Bessel function Jn(λ) has an integral representation,
(3.18). Setting n = 0 and making the change of variable t = sin θ leads to
J0(λ) = 1
π
 1
−1
eiλt
√
1 −t2 dt.
(11.14)
This is of the form (11.13) with f(t) = it, and hence φ = −y, ψ = x. In this
case, P = −1 and Q = 1, so that φ(P) = φ(Q) = 0. The contours of steepest
descent through P and Q are just the straight lines ψ = x = −1 and ψ = x = 1
respectively. We therefore deform the contour C, which is the portion of the real
axis −1 ⩽x ⩽1, into C1 + C2 + C3, as shown in Figure 11.8. The contribution
from C2, y = Y > 0, is exponentially small whatever the choice of Y (Y = 1
in Figure 11.8), whilst the contributions from C1 and C3 are dominated by the
neighbourhoods of the endpoints on the real axis. On C1 we make the change of
variable t = −1 + iy, so that
1
π

C1
eiλt
√
1 −t2 dt = 1
π
 Y
0
e−iλe−λy
#
2iy + y2 i dy
∼e−iλi
π
√
2i
 ∞
0
y−1/2e−λy as λ →∞,

294
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.8. The contours C1, C2 and C3 for evaluating the asymptotic behaviour of J0(λ).
using Laplace’s method. By making the change of variable ˆy = λy, we can write
this integral in terms of a gamma function, Γ(1/2) = √π, and arrive at
1
π

C1
eiλt
√
1 −t2 dt ∼e−iλ+iπ/4
√
2πλ
as λ →∞.
Similarly,
1
π

C3
eiλt
√
1 −t2 dt ∼eiλ−iπ/4
√
2πλ
as λ →∞,
and hence
J0(λ) ∼
*
2
λπ cos

λ −π
4

as λ →∞.
So, is this the whole story? Let’s return to our original example, f(t) = t2. Since
f ′(t) = 2t, the real and imaginary parts of f are stationary at t = 0. However,
t = 0 is neither a local maximum nor a local minimum. It is a saddle point. In

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
295
fact, no analytic function can have a local maximum or minimum, since its real and
imaginary parts are harmonic (see Section 7.1.4 and Appendix 6). The real part of
f(t) = t2 has a ridge along the line y = 0 and a valley along the line x = 0, as can
be seen in Figure 11.5. In cases 1 and 2, which we studied above, we were able to
deform the contour C within one of the valleys.
Case 3: ψ(P) > 0 and ψ(Q) < 0
In this case, P and Q lie in diﬀerent valleys of the function f(t) = t2, and
we must deform the contour so that it runs through the line of steepest descent
at the saddle point, since this is the only line with ψ constant that connects the
two valleys, as shown in Figure 11.9. As usual, the integrals on C2 and C4 are
P
Q
C1
C2
C3
C4
C5
Im(t)
Re(t)
ψ = ψ(P) > 0
ψ = ψ(Q) < 0
Fig. 11.9. The contours C1, C2, C3, C4 and C5 in case 3.
exponentially small, and, using Laplace’s method as in cases 1 and 2,

C1
g(t)eλt2 dt ∼−eλf(P ) g(P)
f ′(P)
1
λ,

C5
g(t)eλt2 dt ∼eλf(Q) g(Q)
f ′(Q)
1
λ.
On the steepest descent contour C3, we can make the simple change of variable
t = iy, to arrive at

C3
g(t)eλt2 dt = i
 ψ(Q)
ψ(P )
g(iy)e−λy2 dy.

296
ASYMPTOTIC METHODS: BASIC IDEAS
This integral is dominated by the behaviour close to the saddle point, y = 0, and,
if g(0) is bounded and nonzero, Laplace’s method shows that

C3
g(t)eλt2 dt ∼−ig(0)
 ∞
−∞
e−λy2 dy = −ig(0)
*π
λ.
(11.15)
If φ(P) ⩽0 and φ(Q) ⩽0, this is the dominant contribution.
Finally, what if the contour of integration, C, extends to inﬁnity? For such an
integral to converge, it must extend into the valleys of f(t) as |t| →∞. In our
example, if we let P and Q head oﬀto inﬁnity along the two diﬀerent valleys, the
integral is dominated by the contribution from the steepest descent contour through
the saddle point, given by (11.15).
Example: The Airy function, Ai(x), for |x| ≫1
As we have seen, (11.6),
Ai(x) =
1
2πi

C
ext−1
3 t3 dt,
(11.16)
with arg(t) = ± 2π
3 as |t| →∞. For x > 0 we can make the change of variable
t = x1/2τ and, since arg(t) = arg(τ), deform back to the original contour to obtain
Ai(x) = x1/2
2πi

C
ex3/2(τ−1
3 τ 3) dτ.
(11.17)
This is now in the standard form, with f(τ) = τ −1
3τ 3. Since f ′(τ) = 1 −τ 2,
there are saddle points at τ = ±1. The contours of constant imaginary part, ψ,
are shown in Figure 11.10. We can now deform C into the steepest descent contour
through τ = −1, marked as C in Figure 11.10. As we have seen, the integral is
dominated by the contribution from the neighbourhood of this saddle point. Since
the contour C is vertical at this point, we write τ = −1 + iy and use Laplace’s
method to show that
Ai(x) ∼x1/2
2πi
 ∞
−∞
ex3/2(−2/3−y2)i dy =
1
2√π x−1/4 exp

−2
3x3/2

as x →∞.
(11.18)
Similarly, when x < 0, we can make the transformation t = (−x)1/2T, so that
Ai(x) = (−x)1/2
2πi

C
e(−x)3/2(−T −1
3 T 3) dT.
(11.19)
In this case, f(T) = −T −1
3T 3 and f ′(T) = −1 −T 2, so there are saddle points at
T = ±i. The contours of constant imaginary part, ψ, are shown in Figure 11.11.
By deforming C into the contour C1 + C2 + C3, we can see that the integral is
dominated by the contributions from the two saddle points. The integral along C2
is exponentially smaller. In order to calculate the leading order contribution from
C1, note that this contour passes through T = −i in the direction of 1 + i. We

11.2 THE ASYMPTOTIC EVALUATION OF INTEGRALS
297
Fig. 11.10. Contours where the imaginary part of f(τ) = τ −1
3τ 3 is constant.
therefore make the substitution T = −i + (1 + i)s and use Laplace’s method to
arrive at
(−x)1/2
2πi

C1
e(−x)3/2(−T −1
3 T 3) dt ∼(−x)1/2
2πi
e
2
3 i(−x)3/2(1 + i)
 ∞
−∞
e−2(−x)3/2s2 ds
= (−x)−1/4(1 + i)
2πi
*π
2 e
2
3 i(−x)3/2.
Similarly,
(−x)1/2
2πi

C3
e(−x)3/2(−T −1
3 T 3) dt ∼(−x)−1/4(−1 + i)
2πi
*π
2 e−2
3 i(−x)3/2,
and hence
Ai(x) ∼(−x)−1/4
√π
sin
2
3(−x)3/2 + π
4
	
as x →−∞.
(11.20)
The transition from exponential decay as x →∞to decaying oscillations as x →
−∞is shown in Figure 11.12. Also shown is the behaviour predicted by our analysis

298
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.11. Contours where the imaginary part of f(T) = −T −1
3T 3 is constant, along
with a single contour, C2, where the real part of f is constant.
for large |x|, which can be seen to be in excellent agreement with Ai(x), even for
moderate values of |x|.
To end this section, we give a justiﬁcation for the success of the method of
stationary phase. Consider the example that we looked at earlier,
I2(λ) =
 b
a
teiλ(1−t)2 dt,
with a < 1 < b. Using Cauchy’s theorem on the analytic function teiλ(1−t)2, we can
deform the contour C, the real axis with a ⩽x ⩽b, into C1 + C2 + C3 + C4 + C5,
as shown in Figure 11.13. The same arguments as those that we used above show
that the largest contribution comes from the neighbourhood of the saddle point on
the steepest descent contour, C3. This is, however, just the neighbourhood of the
single point of stationary phase, and, even though the contour is diﬀerent, because
the integrand is analytic, we obtain the same result, (11.10).

EXERCISES
299
Fig. 11.12. The Airy function Ai(t).
Exercises
11.1
Calculate the order of magnitude of the following functions in terms of the
simplest function of ϵ possible, as ϵ →0+. (a) sinh ϵ, (b) tan ϵ,
(c) sinh(1/ϵ), (d) e−ϵ, (e) cot ϵ, (f) log(1 + ϵ), (g) (1 −cos ϵ)/(1 + cos ϵ),
(h) exp {−cosh (1/ϵ)}.
11.2
Show that e−1/ϵ = o(ϵn) as ϵ →0 for all real n, provided that the complex
variable ϵ is restricted to a domain that you should determine.
11.3
Consider the integral
I(x) = e−x
 x
1
et
t dt as x →∞.
By integrating by parts repeatedly, develop a series expansion of the form
I(x) = 1
x + 1
x2 + 2
x3 + 3!
x4 −(1 + 1 + 2 + 3!) e1−x + · · · .
By considering the error in terminating the expansion after the term in
x−n, show that the series is asymptotic as x →∞.
11.4
Show that the following are asymptotic sequences.

300
ASYMPTOTIC METHODS: BASIC IDEAS
Fig. 11.13. The contours C1, C2, C3, C4 and C5 for the method of stationary phase.
(a)

sin 1
ϵ
n
, n = 0, 1, . . . , as ϵ →∞,
(b) log(1 + ϵn), n = 0, 1, . . . , as ϵ →0.
11.5
Use Watson’s lemma to show that
 ∞
0
(t + t2)−1/2e−λtdt ∼
*π
λ

1 −1
4λ +
9
32λ2

as λ →∞.
11.6
After making the substitution t = uλ, use Laplace’s method to show that
 ∞
0
e4λ3/2t1/2−t2
1 + (t/λ)2 dt ∼
*π
6 e3λ2 as λ →∞.
11.7
Use Laplace’s method to show that the modiﬁed Bessel function Kν(z),
which has an integral representation
Kν(z) = 1
2
 ∞
−∞
eνt−z cosh tdt,

EXERCISES
301
can be approximated as ν →∞, with z = O(1) and positive, using
Kν(z) ∼
* π
2ν e−ν
2ν
z
ν
.
You may assume that the integral is dominated by the contribution from
the neighbourhood of the maximum value of νt −z cosh t.
11.8
Use the method of stationary phase to show that
 ∞
0
eiλ( 1
3 t3−t)
1 + t
dt ∼1
2
*π
λe−2iλ/3+iπ/4 as λ →∞.
11.9
Use the method of stationary phase to show that
 π/2
0
eiλ(2t−sin 2t)dt ∼1
2Γ
4
3
  6
λ
1/3
eiπ/6 as λ →∞.
11.10
Show that as λ →∞
(a)
 1+5i
4
eiλt2dt ∼ie16iλ
8
 1
λ −
i
32λ2

,
(b)
 1+5i
−5−i
eiλt2dt ∼
*π
λeiπ/4.
11.11
Consider the integral (11.9) when F(t) has a single point of stationary
phase at t = t0 with a < t0 < b, F ′′(t0) = 0 and F ′′′(t0) ̸= 0. Use the
method of stationary phase to show that
I2 ∼2
3g(t0)eiλF (t0)+iπsgn(F ′′′(t0))/6Γ
1
3
 
6
λ|F ′′′(t0)|
1/3
for λ ≫1.
11.12
Consider the integral
I(λ) =
 Q
P
eλ( 1
2 t2+it)dt,
where P and Q are points in the complex t plane. Sketch the contours of
constant real and imaginary parts of 1
2t2 + it. Show that if P = −1
2 and
Q = 2,
I(λ) ∼e(2+2i)λ
(2 + i)λ as λ →∞.
Show that if P = −1
2 + i and Q = 1 −3i,
I(λ) ∼−i
*
2πeλ
λ
as λ →∞.
11.13
Show that Stirling’s formula for Γ(z + 1) when z ≫1 holds for complex z
provided |z| ≫1 and −π < arg(z) < π. (Hint: Let z = Reiθ.) Determine
the next term in the asymptotic expansion.

302
ASYMPTOTIC METHODS: BASIC IDEAS
11.14
From the integral representation,
Iν(x) = 1
π
 π
0
ex cos θ cos νθ dθ −sin νπ
π
 ∞
0
e−x cosh t−νt dt,
of the modiﬁed Bessel function of order ν, show that
Iν(x) ∼
ex
√
2πx,
as x →∞for real x and ﬁxed ν.
11.15
The parabolic cylinder functions are deﬁned by
D−2m(x) =
1
(m −1)!xe−x2/4
 ∞
0
e−ssm−1 
x2 + 2s
−m−1/2 ds,
D−2m+1(x) =
1
(m −1)!xe−x2/4
 ∞
0
e−ssm−1 
x2 + 2s
−m+1/2 ds,
for m a positive integer. Show that for real x and ﬁxed m,
D−2m(x) ∼x−2me−x2/4,
D−2m+1 ∼x−2m+1e−x2/4,
as x →∞.

CHAPTER TWELVE
Asymptotic Methods: Diﬀerential Equations
In this chapter we will apply the ideas that we developed in the previous chapter
to the solution of ordinary and partial diﬀerential equations.
12.1
An Instructive Analogy: Algebraic Equations
Many of the essential ideas that we will need in order to solve diﬀerential equa-
tions using asymptotic methods can be illustrated using algebraic equations. These
are much more straightforward to deal with than diﬀerential equations, and the
ideas that we will use are far more transparent. We will consider two illustrative
examples.
12.1.1
Example: A Regular Perturbation
Consider the cubic equation
x3 −x + ϵ = 0.
(12.1)
Although there is an explicit formula for the solution of cubic equations, it is rather
cumbersome to use. Let’s suppose instead that we only need to ﬁnd the solutions
for ϵ ≪1. If we simply set ϵ = 0, we get x3 −x = 0, and hence x = −1, 0 or 1.
These are called the leading order solutions of the equation. These solutions
are obviously not exact when ϵ is small but nonzero, so let’s try to improve the
accuracy of our approximation by seeking an asymptotic expansion of the solution
(or more succinctly, an asymptotic solution) of the form
x = x0 + ϵx1 + ϵ2x2 + O(ϵ3).
(12.2)
We can now substitute this into (12.1) and equate powers of ϵ. This gives us

x0 + ϵx1 + ϵ2x2
3 −

x0 + ϵx1 + ϵ2x2

+ ϵ + O

ϵ3
= 0,
which we can rearrange into a hierarchy of powers of ϵ in the form

x3
0 −x0

+ ϵ {(3x0 −1) x1 + 1} + ϵ2 
3x0x2
1 +

3x2
0 −1

x2

+ O

ϵ3
= 0.
At leading order we obviously get x3
0 −x0 = 0, and hence x0 = −1, 0 or 1. We will
concentrate on the solution with x0 = 1. At O(ϵ), (3x0 −1) x1 + 1 = 2x1 + 1 = 0,
and hence x1 = −1
2. At O(ϵ2), 3x0x2
1 +

3x2
0 −1

x2 = 3
4 + 2x2 = 0, and hence

304
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
x2 = −3
8. We could, of course, continue to higher order if necessary. This shows
that
x = 1 −1
2ϵ −3
8ϵ2 + O(ϵ3)
for ϵ ≪1.
Similar expansions can be found for the other two solutions of (12.1). This is a
regular perturbation problem, since we have found asymptotic expansions for all
three roots of the cubic equation using the simple expansion (12.2). Figure 12.1
shows that the function x3 −x + ϵ is qualitatively similar for ϵ = 0 and 0 < ϵ ≪1.
Fig. 12.1. The function x3 −x + ϵ for ϵ = 0, solid line, and ϵ = 0.1, broken line.
12.1.2
Example: A Singular Perturbation
Consider the cubic equation
ϵx3 + x2 −1 = 0.
(12.3)
At leading order for ϵ ≪1, x2 −1 = 0, and hence x = ±1. However, we know
that a cubic equation is meant to have three solutions. What’s happened to the
other solution? This is an example of a singular perturbation problem, where the
solution for ϵ = 0 is qualitatively diﬀerent to the solution when 0 < ϵ ≪1. The
equation changes from quadratic to cubic, and the number of solutions goes from
two to three. The key point is that we have implicitly assumed that x = O(1).
However, the term ϵx3, which we neglect at leading order when x = O(1), becomes
comparable to the term x2 for suﬃciently large x, speciﬁcally when x = O(ϵ−1).
Figure 12.2 shows how the function ϵx3 +x2 −1 changes qualitatively for ϵ = 0 and
0 < ϵ ≪1.

12.1 AN INSTRUCTIVE ANALOGY: ALGEBRAIC EQUATIONS
305
Fig. 12.2. The function ϵx3 + x2 −1 for ϵ = 0, solid line, and ϵ = 0.1, broken line.
So, how can we proceed in a systematic way? If we expand x = x0 +ϵx1 +O(ϵ2),
we can construct the two O(1) solutions, x = ±1 + O(ϵ), in the same manner as we
did for the previous example. Since we know that there must be three solutions, we
conclude that the other solution cannot have x = O(1), and assume that x = O(ϵα),
with α to be determined. If we deﬁne a scaled variable, x = ϵαX, with X = O(1)
for ϵ ≪1, (12.3) becomes
ϵ3α+1X3 + ϵ2αX2 −1 = 0.
(12.4)
We must choose α in order to obtain an asymptotic balance between two of
the terms in (12.4). If α > 0, the ﬁrst two terms are small and cannot balance
the third term, which is of O(1). If α < 0, the ﬁrst two terms are large, and we
can choose α so that they are of the same asymptotic order. This requires that
ϵ3α+1X3 = O(ϵ2αX2), and hence ϵ3α+1 = O(ϵ2α). This gives 3α + 1 = 2α, and
hence α = −1. This means that x = ϵ−1X = O(ϵ−1), as expected. Equation (12.4)
now becomes
X3 + X2 −ϵ2 = 0.
(12.5)
Since only ϵ2 and not ϵ appears in this rescaled equation, we expand X = X0 +
ϵ2X1 + O(ϵ4). At leading order, X3
0 + X2
0 = 0, and hence X0 = −1 or 0. Of course,
X0 = 0 will just give us the two solutions with x = O(1) that we have already
considered, so we take X0 = −1. At O(ϵ2),
(−1 + ϵ2X1)3 + (−1 + ϵ2X1)2 −ϵ2 + O(ϵ4) = 0

306
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
gives
−1 + 3ϵ2X1 + 1 −2ϵ2X1 −ϵ2 + O(ϵ4) = 0,
and hence X1 = 1. Therefore X = −1+ϵ2 +O(ϵ4), and hence x = −1/ϵ+ϵ+O(ϵ3).
12.2
Ordinary Diﬀerential Equations
The solution of ordinary diﬀerential equations by asymptotic methods often pro-
ceeds in a similar way to the solution of algebraic equations, which we discussed
in the previous section. We assume that an asymptotic expansion of the solution
exists, substitute into the equation and boundary conditions, and equate powers of
the small parameter. This determines a sequence of simpler equations and bound-
ary conditions that we can solve. In order to introduce the main ideas, we will
begin by considering some simple, constant coeﬃcient linear ordinary diﬀerential
equations before moving on to study both nonlinear ordinary diﬀerential equations
and some simple partial diﬀerential equations.
12.2.1
Regular Perturbations
Consider the ordinary diﬀerential equation
y′′ + 2ϵy′ −y = 0,
(12.6)
to be solved for 0 ⩽x ⩽1, subject to the boundary conditions
y(0) = 0,
y(1) = 1.
(12.7)
Of course, we could solve this constant coeﬃcient ordinary diﬀerential equation
analytically using the method described in Appendix 5, but it is instructive to try
to construct the asymptotic solution when ϵ ≪1. We seek a solution of the form
y(x) = y0(x) + ϵy1(x) + O(ϵ2).
At leading order, y′′
0 −y0 = 0, subject to y0(0) = 0 and y0(1) = 1, which has
solution
y0(x) = sinh x
sinh 1 .
If we now substitute the expansion for y into (12.6) and retain terms up to O(ϵ),
we obtain
(y0 + ϵy1)′′ + 2ϵ(y0 + ϵy1)′ −(y0 + ϵy1) + O(ϵ2)
= y′′
0 + ϵy′′
1 + 2ϵy′
0 −y0 −ϵy1 + O(ϵ2) = 0,
and hence
y′′
1 −y1 = −2y′
0 = −2cosh x
sinh 1 .
(12.8)
Similarly, the boundary conditions (12.7) show that
y0(0) + ϵy1(0) + O(ϵ2) = 0,
y0(1) + ϵy1(1) + O(ϵ2) = 1,

12.2 ORDINARY DIFFERENTIAL EQUATIONS
307
and hence
y1(0) = 0,
y1(0) = 0.
(12.9)
By seeking a particular integral solution of (12.8) in the form y1p = kx sinh x, and
using the constants in the homogeneous solution, y1h = A sinh x + B cosh x, to
satisfy the boundary conditions (12.9), we arrive at
y1 = (1 −x)sinh x
cosh 1,
and hence
y(x) = sinh x
sinh 1 + ϵ(1 −x)sinh x
cosh 1 + O(ϵ2),
(12.10)
for ϵ ≪1. The ratio of the second to the ﬁrst term in this expansion is ϵ(1 −
x) tanh 1, which is uniformly small for 0 ⩽x ⩽1. This leads us to believe that
the asymptotic solution (12.10) is uniformly valid in the region of solution. The
situation is analogous to the example that we studied in Section 12.1.1.
One subtle point is that, although we believe that the next term in the asymptotic
expansion of the solution, which we write as O(ϵ2) in (12.10), is uniformly smaller
than the two retained terms for ϵ suﬃciently small, we have not proved this. We
do not have a rigorous estimate for the size of the neglected term in the way that
we did when we considered the exponential integral, where we were able to ﬁnd an
upper bound for RN, given by (11.3). Although, for this simple, constant coeﬃcient
ordinary diﬀerential equation, we could write down the exact solution and prove
that the remainder is of O(ϵ2), in general, and in particular for nonlinear problems,
this is not possible, and an act of faith is involved in trusting that our asymptotic
solution provides a valid representation of the exact solution. This faith can be
bolstered in a number of ways, for example, by comparing asymptotic solutions with
numerical solutions, and by checking that the asymptotic solution makes sense in
terms of the underlying physics of the system that we are modelling. The sensible
applied mathematician always has a small, nagging doubt at the back of their mind
about the validity of an asymptotic solution. For (12.6), our faith is justiﬁed, as
can be seen in Figure 12.3.
12.2.2
The Method of Matched Asymptotic Expansions
Consider the ordinary diﬀerential equation
ϵy′′ + 2y′ −y = 0,
(12.11)
to be solved for 0 ⩽x ⩽1, subject to the boundary conditions
y(0) = 0,
y(1) = 1.
(12.12)
The observant reader will notice that this is the same as the previous example, but
with the small parameter ϵ moved to the highest derivative term. We again seek
an asymptotic solution of the form
y(x) = y0(x) + ϵy1(x) + O(ϵ2).

308
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.3. Exact and asymptotic solutions of (12.6) when ϵ = 0.25.
At leading order, 2y′
0 −y0 = 0, which has the solution y0 = Aex/2 for some constant
A. However, the boundary conditions require that y0(0) = 0 and y0(1) = 1. How
can we satisfy two boundary conditions using only one constant? Well, of course
we can’t. The problem is that for ϵ = 0, the equation is of ﬁrst order, and there-
fore qualitatively diﬀerent from the full, second order equation. This is a singular
perturbation, and is analogous to the example we studied in Section 12.1.2.
Let’s proceed by satisfying the boundary condition y0(1) = 1, which gives
y0(x) = e(x−1)/2.
At O(ϵ) we have
2y′
1 −y1 = −y′′
0 = −1
4e(x−1)/2,
to be solved subject to y1(0) = 0 and y1(1) = 0. This equation can be solved using
an integrating factor (see Section A5.2), which gives
y1(x) = −1
2xe(x−1)/2 + ke(x−1)/2,
for some constant k. Again, we cannot satisfy both boundary conditions, and we
just use y1(1) = 0, which gives
y1(x) = 1
2(1 −x)e(x−1)/2.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
309
Finally, this gives
y = e(x−1)/2

1 + 1
2(1 −x)ϵ
	
+ O(ϵ2),
(12.13)
for ϵ ≪1. This suggests that y →e−1/2(1 + 1
2ϵ) as x →0, which clearly does not
satisfy the boundary condition y(0) = 0. We must therefore introduce a boundary
layer at x = 0, across which y adjusts to satisfy the boundary condition. The idea
is that, in some small neighbourhood of x = 0, the term ϵy′′, which we neglected
at leading order, becomes important because y varies rapidly.
We rescale by deﬁning x = ϵαX, with α > 0 (so that x ≪1) and X = O(1) as
ϵ →0, and write y(x) = Y (X) for X = O(1). Substituting this into (12.11) gives
ϵ1−2α d 2Y
dX2 + 2ϵ−α dY
dX −Y = 0.
Since α > 0, the second term in this equation is large, and to obtain an asymptotic
balance at leading order we must have ϵ1−2α = O(ϵ−α), which means that 1−2α =
−α, and hence α = 1. So x = ϵX,
d 2Y
dX2 + 2dY
dX −ϵY = 0,
(12.14)
and Y (0) = 0. It is usual to refer to the region where ϵ ≪x ⩽1 as the outer
region, with outer solution y(x), and the boundary layer region where x = O(ϵ)
as the inner region with inner solution Y (X). The other boundary condition
is to be applied at x = 1. However, x = 1 does not lie in the inner region, where
x = O(ϵ). In order to ﬁx a second boundary condition for (12.14), we will have to
make sure that the solution in the inner region is consistent, in a sense that we will
make clear below, with the solution in the outer region, which does satisfy y(1) = 1.
We now expand
Y (X) = Y0(X) + ϵY1(X) + O(ϵ2).
At leading order, Y ′′
0 + 2Y ′
0 = 0, to be solved subject to Y0(0) = 0. The solution is
Y0 = A(1 −e−2X),
for some constant A. At leading order, we now know that
y ∼e(x−1)/2
for ϵ ≪x ⩽1
(the outer expansion),
Y ∼A(1 −e−2X)
for X = O(1), x = O(ϵ)
(the inner expansion).
For these two expansions to be consistent with each other, we must have
lim
X→∞Y (X) = lim
x→0 y(x),
(12.15)
which gives A = e−1/2. We will see below, where we make this vague notion of
“consistency” more precise, that this is correct.
At O(ϵ) we obtain the equation for Y1(X) as
Y ′′
1 + 2Y ′
1 = Y0 = A(1 −e−2X).

310
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Integrating this once gives
Y ′
1 + 2Y1 = A

X + 1
2e−2X

+ c1,
for some constant c1. This can now be solved using an integrating factor, and the
solution that satisﬁes Y1(0) = 0 is
Y1 = 1
2AX(1 + e−2X) −c2(1 −e−2X),
for some constant c2, which we need to determine. To summarize, the two-term
asymptotic expansions are
y ∼e(x−1)/2 + 1
2ϵ(1 −x)e(x−1)/2 for ϵ ≪x ⩽1,
Y ∼A(1 −e−2X) + ϵ
1
2AX(1 + e−2X) −c2(1 −e−2X)
	
for X = O(1), x = O(ϵ).
We can determine the constants A and c2 by forcing the two expansions to be
consistent in the sense that they should be equal in an intermediate region or
overlap region, where ϵ ≪x ≪1. The point is that in such a region we expect
both expansions to be valid.
We deﬁne x = ϵβ ˆX with 0 < β < 1, and write y = ˆY ( ˆX). In terms of the
intermediate variable, ˆX, the outer expansion becomes
ˆY ∼e−1/2 exp
1
2ϵβ ˆX

+ 1
2ϵ(1 −ϵβ ˆX)e−1/2 exp
1
2ϵβ ˆX

.
When ˆX = O(1), we can expand the exponentials as Taylor series, and ﬁnd that
ˆY = e−1/2

1 + 1
2ϵβ ˆX

+ 1
2e−1/2ϵ + o(ϵ).
(12.16)
Since x = ϵX = ϵβ ˆX gives X = ϵβ−1 ˆX, the inner expansion is
ˆY ∼A

1 −exp

−2ϵβ−1 ˆX

+ϵ
1
2Aϵβ−1 ˆX

1 + exp

−2ϵβ−1 ˆX

−c2

1 −exp

−2ϵβ−1 ˆX

.
Now, since exp(−2ϵβ−1 ˆX) = o(ϵn) for all n > 0 (it is exponentially small for β < 1),
we have
ˆY = A + 1
2Aϵβ ˆX −c2ϵ + o(ϵ).
(12.17)
Since (12.16) and (12.17) must be identical, we need A = e−1/2, consistent with
the crude assumption, (12.15), that we made above, and also c2 = −1
2e−1/2. This
process, whereby we make the inner and outer expansions consistent, is known
as asymptotic matching, and the inner and outer expansions are known as
matched asymptotic expansions. A comparison between the one-term inner
and outer solutions and the exact solution is given in Figure 12.4. It should be

12.2 ORDINARY DIFFERENTIAL EQUATIONS
311
clear that the inner expansion is a poor approximation in the outer region and
vice versa. A little later, we will show how to construct, using the inner and outer
expansions, a composite expansion that is uniformly valid for 0 ⩽x ⩽1.
Fig. 12.4. Exact and asymptotic solutions of (12.11) when ϵ = 0.25.
Van Dyke’s Matching Principle
The use of an intermediate variable in an overlap region can get very tedious in
more complicated problems. A method that works most, but not all, of the time,
and is much easier to use, is Van Dyke’s matching principle. This principle is
much easier to use than to explain, but let’s start with the explanation.
Let
y(x) ∼
N

n=0
φn(ϵ)yn(x)
be the outer expansion and
Y (X) ∼
N

n=0
ψn(ϵ)Yn(X)
be the inner expansion with respect to the asymptotic sequences φn(ϵ) and ψn(ϵ),
with x = f(ϵ)X. In order to analyze how the outer expansion behaves in the inner
region, we can write y(x) in terms of X = x/f(ϵ), and retain M terms in the
resulting asymptotic expansion. We denote this by y(N,M), the M th order inner
approximation of the outer expansion. Similarly, we can write Y (X) in terms of x,

312
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
and retain M terms in the resulting expansion. We denote this by Y (N,M), the M th
order outer approximation of the inner expansion. Van Dyke’s matching principle
states that y(N,M) = Y (M,N). Let’s see how this works for our example.
In terms of the outer variable, the inner expansion is
Y (X) ∼A(1 −e−2x/ϵ) + ϵ
1
2Ax
ϵ

1 + e−2x/ϵ
−c2

1 −e−2x/ϵ	
∼Y (2,2) = A + 1
2Ax −c2ϵ,
for x = O(1). In terms of the inner variable, the outer expansion is
y(x) ∼exp

−1
2 + 1
2ϵX

+ 1
2ϵ (1 −ϵX) exp

−1
2 + 1
2ϵX

∼y(2,2) = e−1/2

1 + 1
2ϵX + 1
2ϵ

,
for X = O(1). In terms of the outer variable,
y(2,2) = e−1/2

1 + 1
2x + 1
2ϵ

.
Van Dyke’s matching principle states that Y (2,2) = y(2,2), and therefore gives A =
e−1/2 and c2 = −1
2e−1/2 rather more painlessly than before.
Composite Expansions
Although we now know how to construct inner and outer solutions, valid in the
inner and outer regions, it would be more useful to have an asymptotic solution
valid uniformly across the whole domain of solution, 0 ⩽x ⩽1 in the example. We
can construct such a uniformly valid composite expansion by using the inner
and outer expansions. We simply need to add the two expansions and subtract
the expression that gives their overlap.
The overlap is just the expression that
appears to the appropriate order in the intermediate region, (12.17) or (12.16),
or equivalently the matched part, y(2,2) or Y (2,2). For our example problem, the
one-term composite expansion is
y ∼y0 + Y0 −y(1,1) = e(x−1)/2 + e−1/2(1 −e−2X) −e−1/2
= e(x−1)/2 −e−1/2−2x/ϵ for 0 ⩽x ⩽1 as ϵ →0.
This composite expansion is shown in Figure 12.4, and shows good agreement with
the exact solution across the whole domain, as expected. Note that, in terms of
Van Dyke’s matching principle, we can write the composite solution of any order
as
y ∼y(M,N)
c
=
M

n=0
yn(x) +
N

n=0
Yn(X) −y(M,N).

12.2 ORDINARY DIFFERENTIAL EQUATIONS
313
The Location of the Boundary Layer
In our example, when we constructed the outer solution, we chose to satisfy the
boundary condition at x = 1 and assume that there was a boundary layer at x = 0.
Why is this correct? Let’s see what happens if we assume that there is a boundary
layer at x = x0. Strictly speaking, if x0 ̸= 0 and x0 ̸= 1 this is an interior layer.
We deﬁne scaled variables y(x) = Y (X) and x = x0 + ϵαX, with α > 0 and Y ,
X = O(1) for ϵ ≪1. As before, we ﬁnd that we can only obtain an asymptotic
balance at leading order by taking α = 1, so that x = x0 + ϵX and
Y ′′ + 2Y ′ −ϵ = 0.
At leading order, as before, Y0 = A0 + B0e−2X. As X →−∞, Y0 becomes expo-
nentially large, and cannot be matched to the outer solution. This forces us to take
x0 = 0, since then this solution is only needed for X ⩾0, and, as we have seen, we
can construct an asymptotic solution.
Interior Layers
Singular perturbations of ordinary diﬀerential equations need not always result in
a boundary layer. As an example, consider
ϵy′′ + 2xy′ + 2x = 0,
(12.18)
to be solved for −1 < x < 1, subject to the boundary conditions
y(−1) = 2,
y(1) = 3.
(12.19)
We will try to construct the leading order solution for ϵ ≪1. The leading order
outer solution satisﬁes 2x(y′ + 1) = 0, and hence y = k −x for some constant k. If
y(−1) = 2 we need y = 1−x, whilst if y(1) = 3 we need y = 4−x. We clearly cannot
satisfy both boundary conditions with the same outer solution, so let’s look for a
boundary or interior layer at x = x0 by deﬁning y(x) = Y (X) and x = x0 + ϵαX,
with Y , X = O(1). In terms of these scaled variables, (12.18) becomes
ϵ1−2αYXX + 2(x0 + ϵαX)(ϵ−αYX + 1) = 0.
If x0 ̸= 0, for a leading order balance we need ϵ1−2α = O(ϵ−α), and hence α = 1.
In this case, at leading order,
YXX + 2x0YX = 0,
and hence Y = A + Be−2x0X. For x0 > 0 this grows exponentially as X →−∞,
whilst for x0 < 0 this grows exponentially as X →∞. In either case, we cannot
match these exponentially growing terms with the outer solution. This suggests
that we need x0 = 0, when
ϵ1−2αYXX + 2XYX + 2ϵαX = 0.
For a leading order asymptotic balance we need α = 1/2, and hence a boundary
layer with width of O(ϵ1/2). At leading order,
YXX + 2XYX = 0,

314
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
which, after multiplying by the integrating factor, eX2, gives
d
dX

eX2YX

= 0,
and hence
Y = B + A
 X
−∞
e−s2ds.
Now, since the interior layer is at x = 0, the outer solution must be
y =
 1 −x
for −1 ⩽x < O(ϵ1/2),
4 −x
for O(ϵ1/2) < x ⩽1.
Since y →4 as x →0+ and y →1 as x →0−, we must have Y →4 as X →∞and
Y →1 as X →−∞. This allows us to ﬁx the constants A and B and ﬁnd that
Y (X) = 1 +
3
√π
 X
−∞
e−s2ds = 1
2 (5 + 3 erf (x)) ,
which leads to the one-term composite solution
y ∼yc = −x + 1
2

5 + 3 erf
 x
√ϵ
	
for −1 ⩽x ⩽1 and ϵ ≪1.
(12.20)
This is illustrated in Figure 12.5 for various values of ϵ. Note that the boundary
conditions at x = ±1 are only satisﬁed by the composite expansion at leading order.
12.2.3
Nonlinear Problems
Example 1: Elliptic functions of large period
As we have already seen in Section 9.4, the Jacobian elliptic function x = sn(t; k)
satisﬁes the equation
dx
dt =
#
1 −x2#
1 −k2x2,
(12.21)
subject to x = 0 when t = 0, and has periodic solutions for k ̸= 1. When k = 1,
the solution that corresponds to sn(t; k) is a heteroclinic path that connects the
equilibrium points (±1, 0, 0) in the phase portrait shown in Figure 9.18, and hence
the period tends to inﬁnity as k →1. When k is close to unity, it seems reasonable
to assume that the period of the solution is large but ﬁnite. Can we quantify this?
Let’s assume that k2 = 1 −δ2, with δ ≪1, and seek an asymptotic solution for
the ﬁrst quarter period of x(t), with 0 ⩽x ⩽1. Figure 12.6 shows sn(t; k) for
various values of δ, and we can see that the period does increase as δ decreases and
k approaches unity. The function sn(t; k) is available in MATLAB as ellipj. The
quarter period is simply the value of t when sn(t; k) reaches its ﬁrst maximum.
We seek an asymptotic solution of the form
x = x0 + δ2x1 + δ4x2 + O

δ6
.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
315
Fig. 12.5. The composite asymptotic solution, (12.20), of (12.18).
Using a binomial expansion, (12.21) is
dx
dt =

1 −x2 
1 + δ2
x2
1 −x2
1/2
= 1 −x2 + 1
2δ2x2 + 1
8δ4
x4
1 −x2 + O(δ6).
This binomial expansion is only valid when x is not too close unity, so we should
expect any asymptotic expansion that we develop to become nonuniform as x →1,
and we treat this as the outer expansion.
At leading order,
dx0
dt = 1 −x2
0,
subject to x0(0) = 0,
which has solution x0 = tanh t. At O(δ2),
dx1
dt = −2x0x1 + 1
2x2
0 = −2 tanh tx1 + 1
2 tanh2 t,
subject to x1(0) = 0.
Using the integrating factor cosh2 t, we can ﬁnd the solution
x1 = 1
4

tanh t −t sech2t

.
We can now see that
x ∼1 −2e−2t + 1
4δ2 + O(δ4) as t →∞.

316
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.6. The Jacobian elliptic function sn(t; k) for various values of δ.
Although x approaches unity as t →∞, there is no nonuniformity in this expansion,
so we need to go to O(δ4). At this order,
dx2
dt = −x2
1 −2x0x2 + x0x1 −1
8

x4
0
1 −x2
0

subject to x2(0) = 0.
Solving this problem would be extremely tedious. Fortunately, we don’t really want
to know the exact expression for x2, just its behaviour as t →∞. Using the known
behaviour of the various hyperbolic functions, we ﬁnd that
dx2
dt + 2x2 ∼−1
32e2t as t →∞,
and hence from solving this simple linear equation,
x2 ∼−1
128e2t as t →∞.
This shows that
x ∼1 −2e−2t + 1
4δ2 −
1
128δ4e2t + O(δ6) as t →∞.
(12.22)
We can now see that the fourth term in this expansion becomes comparable to the
third when δ4e2t = O(δ2), and hence as t →log(1/δ), when x = 1 + O(δ2).

12.2 ORDINARY DIFFERENTIAL EQUATIONS
317
We therefore deﬁne new variables for an inner region close to x = 1 as
x = 1 −δ2X,
t = log
1
δ

+ T.
On substituting these inner variables into (12.21), we ﬁnd that, at leading order,
dX
dT = −
#
2X (2X + 1).
Using the substitution X = ¯X −1
4 brings this separable equation into a standard
form, and the solution is
X = 1
4 {cosh (K −2T) −1} .
(12.23)
We now need to determine the constant K by matching the inner solution,
(12.23), with the outer solution, whose behaviour as x →1 is given by (12.22).
Writing the inner expansion in terms of the outer variables and retaining terms up
to O(δ2) leads to
x = 1 −1
8eKe−2t + 1
4δ2 + O(δ4),
for t = O(1). Comparing this with (12.22) shows that we need 1
8eKe−2t = 2e−2t,
and hence K = 4 log 2, which gives
x = 1 −1
4δ2 {cosh (4 log 2 −2T) −1} + O(δ4)
(12.24)
when T = O(1). From this leading order approximation, x = 1 when T = T0 =
2 log 2 + O(δ2). This is the quarter period of the solution, so the period τ satisﬁes
1
4τ = log
1
δ

+ T0,
and hence
τ = 4 log
4
δ

+ O(δ2),
for δ ≪1. We conclude that the period of the solution does indeed tend to inﬁnity
as δ →0, k →1−, but only logarithmically fast. Figure 12.7 shows a comparison
between the exact and analytical solutions. The agreement is very good for all
δ ⩽1. We produced this ﬁgure using the MATLAB script
'
&
$
%
Texact = []; d = 10.^(-7:0.25:0); Tasymp = 4*log(4./d);
options = optimset(’Display’,’off’,’TolX’, 10^-10);
for del = d
k = sqrt(1-del^2); T2 = 2*log(4/del);
Texact = [Texact 2*fzero(@ellipj,T2,options,k)];
end
plot(log10(d),Texact,log10(d),Tasymp, ’--’)
xlabel(’log_{10}\delta’), ylabel(’T’)
legend(’exact’,’asymptotic’)

318
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
This uses the MATLAB function fzero to ﬁnd where the elliptic function is zero,
using the asymptotic expression as an initial estimate.
Note that the function
optimset allows us to create a variable options that we can pass to fzero as a
parameter, which controls the details of its execution. In this case, we turn oﬀthe
output of intermediate results, and set the convergence tolerance to 10−10.
Fig. 12.7. A comparison of the exact period of the elliptic function sn(t; k) for k =
√
1 −δ2.
Finally, by adding the solutions in the inner and outer regions and subtracting
the matched part, 1+ 1
4δ2 −2e−2t, we can obtain a composite expansion, uniformly
valid for 0 ⩽t ⩽1
4τ = log(4/δ) + O(δ2), as
x = tanh t + 2e−2t + 1
4δ2

tanh t −t sech2t −cosh

log
16
δ2

−2t
	
+ O(δ4).
Example 2: A thermal ignition problem
Many materials decompose to produce heat. This decomposition is usually more
rapid the higher the temperature. This leads to the possibility of thermal ignition.
As a material gets hotter, it releases heat more rapidly, which heats it more rapidly,
and so on. This positive feedback mechanism can lead to the unwanted, and poten-
tially disastrous, combustion of many common materials, ranging from packets of
powdered milk to haystacks. The most common physical mechanism that can break
this feedback loop is the diﬀusion of heat through the material and out through
its surface. The rate of heat production due to decomposition is proportional to
the volume of the material and the rate of heat loss from its surface proportional
to surface area. For a suﬃciently large volume of material, heat production dom-
inates heat loss, and the material ignites. Determining the critical temperature

12.2 ORDINARY DIFFERENTIAL EQUATIONS
319
below which it is safe to store a potentially combustible material is an important
and diﬃcult problem.†
We now want to develop a mathematical model of this problem. In Section 2.6.1,
we showed how to derive the diﬀusion equation, (2.12), which governs the ﬂow of
heat in a body.
We now need to include the eﬀect of a chemical reaction that
produces R(x, y, z, t) units of heat, per unit volume, per unit time, by adding a
term R δt δx δy δz to the right hand side of (2.11). On taking the limit δt, δx, δy,
δz →0, we arrive at
ρc∂T
∂t = −∇· Q + R,
and hence, for a steady state solution (∂/∂t = 0), and since Fourier’s law of heat
conduction is Q = −k∇T,
k∇2T + R = 0.
(12.25)
The rate of combustion of the material can be modelled using the Arrhenius law,
R = Ae−Ta/T , where A is a constant and Ta is the activation temperature, also
a constant. It is important to note that T is the absolute temperature here.
The Arrhenius law can be derived from ﬁrst principles using statistical mechanics,
although we will not attempt this here (see, for example, Flowers and Mendoza,
1970). Figure 12.8 shows that the reaction rate is zero at absolute zero (T = 0),
and remains small until T approaches the activation temperature, Ta, when it
increases, with R →A as T →∞. After deﬁning u = T/Ta and rescaling distances
Fig. 12.8. The Arrhenius reaction rate law.
† For more background on combustion theory, see Buckmaster and Ludford (1982).

320
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
with
#
k/A, we arrive at the nonlinear partial diﬀerential equation
∇2u + e−1/u = 0.
For a uniform sphere of combustible material with a spherically-symmetric temper-
ature distribution, this becomes
d 2u
dr2 + 2
r
du
dr + e−1/u = 0,
(12.26)
subject to
du
dr = 0 at r = 0, u = ua at r = ra.
(12.27)
Here, ua is the dimensionless absolute temperature of the surroundings and ra the
dimensionless radius of the sphere. Note that, as we discussed earlier, we would
expect that the larger ra, the smaller ua must be to prevent the ignition of the
sphere.† A positive solution of this boundary value problem represents a possible
steady state solution in which these two physical processes are in balance. If no
such steady state solution exists, we can conclude that the material will ignite. A
small trick that makes the study of this problem easier is to replace (12.27) with
du
dr = 0,
u = ϵ at r = 0.
(12.28)
We can then solve the initial value problem given by (12.26) and (12.28) for a given
value of ϵ, the dimensionless temperature at the centre of the sphere, and then
determine the corresponding value of ua = u(ra). Our task is therefore to construct
an asymptotic solution of the initial value problem given by (12.26) and (12.28)
when ϵ ≪1. Note that by using the integrating factor r2, we can write (12.26) as
du
dr = −1
r2
 r
0
s2e−1/u(s) ds < 0,
and hence conclude that u is a monotone decreasing function of r. The temperature
of the sphere decreases from centre to surface.
Asymptotic analysis: Region I
Since u = ϵ at r = 0, we deﬁne a new variable ˆu = u/ϵ, with ˆu = O(1) for ϵ ≪1.
In terms of this variable, (12.26) and (12.28) become
d 2ˆu
dr2 + 2
r
dˆu
dr + 1
ϵ exp

−1
ϵˆu

= 0,
subject to
dˆu
dr = 0,
ˆu = 1 at r = 0.
† For all the technical details of this problem, which was ﬁrst studied by Gel’fand (1963), see
Billingham (2000).

12.2 ORDINARY DIFFERENTIAL EQUATIONS
321
At leading order,
d 2ˆu
dr2 + 2
r
dˆu
dr = 0,
(12.29)
which has the trivial solution ˆu = 1. We’re going to need more than this to be able
to proceed, so let’s look for an asymptotic expansion of the form
ˆu = 1 + φ1(ϵ)ˆu1 + φ2(ϵ)ˆu2 + · · · ,
where φ2 ≪φ1 ≪1 are to be determined. As we shall see, we need a three-term
asymptotic expansion to be able to determine the scalings for the next asymptotic
region.
Firstly, note that
1
ϵ e−1/ϵˆu ∼1
ϵ exp

−1
ϵ (1 + φ1ˆu1)−1
	
∼1
ϵ exp

−1
ϵ (1 −φ1ˆu1)
	
∼1
ϵ e−1/ϵ exp
φ1
ϵ ˆu1

∼1
ϵ e−1/ϵ

1 + φ1
ϵ ˆu1

,
provided that φ1 ≪ϵ, which we can check later. Equation (12.26) then shows that
φ1
d 2ˆu1
dr2 + 2
r
dˆu1
dr

+ φ2
d 2ˆu2
dr2 + 2
r
dˆu2
dr

∼−1
ϵ e−1/ϵ

1 + φ1
ϵ ˆu1

.
(12.30)
In order to obtain a balance of terms, we therefore take
φ1 = 1
ϵ e−1/ϵ ≪ϵ,
φ2 = 1
ϵ2 e−1/ϵφ1 = 1
ϵ3 e−2/ϵ,
and hence expand
ˆu = 1 + 1
ϵ e−1/ϵˆu1 + 1
ϵ3 e−2/ϵˆu2 + · · · .
Now, using (12.30),
d 2ˆu1
dr2 + 2
r
dˆu1
dr = −1,
(12.31)
subject to
dˆu1
dr = ˆu1 = 0 at r = 0.
Using the integrating factor r2, we ﬁnd that the solution is ˆu1 = −1
6r2. Similarly,
d 2ˆu2
dr2 + 2
r
dˆu2
dr = 1
6r2,
subject to
dˆu2
dr = ˆu2 = 0 at r = 0,
and hence ˆu2 =
1
120r4. This means that
ˆu ∼1 −1
6ϵe−1/ϵr2 +
1
120ϵ3 e−2/ϵr4
(12.32)

322
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
for ϵ ≪1. This expansion is not uniformly valid, since for suﬃciently large r the
second term is comparable to the ﬁrst, when r = O(ϵ1/2e1/2ϵ), and the third is
comparable to the second, when r = O(ϵe1/2ϵ) ≪ϵ1/2e1/2ϵ. As r increases, the ﬁrst
nonuniformity to occur is therefore when r = O(ϵe1/2ϵ) and u = ϵ + O(ϵ2). Note
that this is why we needed a three-term expansion to work out the scalings for the
next asymptotic region.
What would have happened if we had taken only a two-term expansion and
looked for a new asymptotic region with r = O(ϵ1/2e1/2ϵ) and u = O(ϵ)? If you try
this, you will ﬁnd that it is impossible to match the solutions in the new region to the
solutions in region I. After some thought, you notice that the equations at O(1) and
O( 1
ϵ e−1/ϵ) in region I, (12.29) and (12.31), do not depend at all on the functional
form of the term e−1/ϵˆu, which could be replaced by e−1/ϵ without aﬀecting (12.29)
or (12.31). This is a sign that we need another term in the expansion in order to
capture the eﬀect of the only nonlinearity in the problem.
Asymptotic analysis: Region II
In this region we deﬁne scaled variables u = ϵ+ϵ2U, r = ϵe1/2ϵR, with U = O(1)
and R = O(1) for ϵ ≪1. At leading order, (12.26) becomes
d 2U
dR2 + 2
R
dU
dR + eU = 0,
(12.33)
to be solved subject to the matching condition
U ∼−1
6R2 as R →0.
(12.34)
Equation (12.33) is nonlinear and nonautonomous, which usually means that we
must resort to ﬁnding a numerical solution. However, we have seen in Chapter 10
that we can often make progress using group theoretical methods. Equation (12.33)
is invariant under the transformation U →U + k, R →e−k/2R. We can therefore
make the nonlinear transformation
p(s) = e−2seU,
q(s) = 2 + e−s dU
dR,
R = e−s,
after which (12.33) and (12.34) become
dp
ds = −pq,
dq
ds = p + q −2,
(12.35)
subject to
p ∼e−2s,
q ∼2 −1
3e−2s as s →∞.
(12.36)
The problem is still nonlinear, but is now autonomous, so we can use the phase
plane methods that we studied in Chapter 9.
There are two ﬁnite equilibrium points, at P1 = (0, 2) and P2 = (2, 0) in the
(p, q) phase plane.
After determining the Jacobian at each of these points and
calculating the eigenvalues in the usual way, we ﬁnd that P1 is a saddle point and
P2 is an unstable, anticlockwise spiral. Since (12.36) shows that we are interested

12.2 ORDINARY DIFFERENTIAL EQUATIONS
323
in a solution that asymptotes to P1 as s →∞, this solution must be represented by
one of the stable separatrices of P1. Furthermore, since p = e−2seU > 0, the unique
integral path that represents the solution is the stable separatrix of P1 that lies in
the half plane p > 0. What happens to this separatrix as s →−∞? A sensible, and
correct, guess would be that it asymptotes to the unstable spiral at P2, as shown in
Figure 12.9, for which we calculated the solution numerically using MATLAB (see
Section 9.3.4). The proof that this is the case is Exercise 9.17, which comes with
some hints.
Fig. 12.9. The behaviour of the solution of (12.35) subject to (12.36) in the (p, q)-phase
plane.
Since the solution asymptotes to P2, we can determine its behaviour as s →−∞
by considering the local solution there. The eigenvalues of P2 are 1
2(1 ± i
√
7), and
therefore
p ∼2 + Aes/2 sin
%√
7
2 s + B
&
as s →−∞,
for some constants A and B. Since U = 2s + log p and s = −log R, this shows that
U ∼−2 log R + log 2 −
A
2
√
R
sin
%√
7
2 log R −B
&
as R →∞.
(12.37)
We conclude that
u ∼ϵ + ϵ2(log 2 −2 log R) as R →∞,
for ϵ ≪1. When log R = O(ϵ−1), the second term is comparable to the ﬁrst term,
and we have a further nonuniformity.

324
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Asymptotic analysis: Region III
We deﬁne scaled variables u = ϵ ¯U and S = ϵ log R = −1
2 −ϵ log ϵ + ϵ log r,
r = ϵe1/2ϵeS/ϵ, with U = O(1) and S = O(1) for ϵ ≪1. In terms of these variables,
(12.26) becomes
ϵd 2 ¯U
dS2 + d ¯U
dS + exp
1
ϵ

−1
¯U + 1 + 2S
	
= 0.
(12.38)
We expand ¯U as
¯U = ¯U0 + ϵ ¯U1 + · · · .
To ﬁnd the matching conditions, we write expansion (12.37) in region I in terms of
the new variables and retain terms up to O(ϵ), which gives
¯U0 ∼1 −2S,
¯U1 ∼log 2 as S →0.
(12.39)
At leading order, the solution is given by the exponential term in (12.38) as
¯U0 =
1
1 + 2S ,
which satisﬁes (12.39). At O(ϵ), we ﬁnd that
d ¯U0
dS + exp

(1 + 2S)2 ¯U1

= 0,
and hence that
¯U1 =
1
(1 + 2S)2 log

2
(1 + 2S)2
	
,
which also satisﬁes (12.39). We conclude that
u =
ϵ
1 + 2S +
ϵ2
(1 + 2S)2 log

2
(1 + 2S)2
	
+ O(ϵ3),
for S = O(1) and ϵ ≪1. This expansion remains uniform, with u →0 as S →∞,
and hence R →∞, and the solution is complete.
We can now determine ua = u(ra). If ra ≪ϵe1/2ϵ, r = ra lies in region I, so that
ua ∼ϵ −1
6e−1/ϵr2
a ∼ϵ. In other words, for ua suﬃciently small that ra ≪uae1/2ua,
a steady state solution is always available, and we predict that the sphere will not
ignite.
If ra = O(ϵe1/2ϵ), we need to consider the solution in region II. The oscillations
in p lead to oscillations in ua as a function of ϵ, as shown in Figure 12.10.
In
particular, the fact that p < pmax ≈3.322, as can be seen in Figure 12.9, leads to
a maximum value of ua for which a steady state solution is available, namely
uamax ∼ϵ + ϵ2 log
pmaxϵ2e1/ϵ
r2a

.
(12.40)
Finally, if ra = O(ϵe3/2ϵ), the solution in region III shows that
ua ∼
ϵ
1
2 + 2ϵ log(ra/ϵ) < uamax.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
325
We conclude that uamax (the critical ignition temperature or critical storage
temperature) gives an asymptotic approximation to the hottest ambient temper-
ature at which a steady state solution exists, and hence at which the material will
not ignite.
Fig. 12.10. The ambient temperature, ua, as a function of ϵ in region II when ra = 109.
12.2.4
The Method of Multiple Scales
Let’s now return to solving a simple, linear, constant coeﬃcient ordinary dif-
ferential equation that, at ﬁrst sight, seems like a regular perturbation problem.
Consider
¨y + 2ϵ ˙y + y = 0,
(12.41)
to be solved subject to
y(0) = 1,
˙y(0) = 0,
(12.42)
for t ⩾0, where a dot denotes d/dt. Since this is an initial value problem, we can
think of y developing as a function of time, t. As usual, we expand
y = y0(t) + ϵy1(t) + O(ϵ2)
for ϵ ≪1. At leading order, ¨y0 + y0 = 0, subject to y0(0) = 1, ˙y0(0) = 0, which has
solution y0 = cos t. At O(ϵ),
¨y1 + y1 = −2 ˙y0 = 2 sin t,

326
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
subject to y1(0) = 0, ˙y1(0) = 0. After noting that the particular integral solution
of this equation takes the form y1p = kt cos t for some constant k, we ﬁnd that
y1 = −t cos t + sin t. This means that
y(t) ∼cos t + ϵ (−t cos t + sin t)
(12.43)
as ϵ →0. As t →∞, the ratio of the second term to the ﬁrst term in this expansion
is asymptotic to ϵt, and is therefore no longer small when t = O(ϵ−1). We conclude
that the asymptotic solution given by (12.43) is only valid for t ≪ϵ−1.
To see where the problem lies, let’s examine the exact solution of (12.41) subject
to the boundary conditions (12.42), which is
y = e−ϵt

cos
#
1 −ϵ2t +
ϵ
√
1 −ϵ2 sin
#
1 −ϵ2t

.
(12.44)
As we can see from Figure 12.11, the solution is a decaying oscillation, with the de-
cay occurring over a timescale of O(ϵ−1). At leading order, (12.41) is an undamped,
linear oscillator. The term 2ϵ ˙y represents the eﬀect of weak damping, which slowly
reduces the amplitude of the oscillation. The problem with the asymptotic expan-
sion (12.43) is that, although it correctly captures the fact that e−ϵt ∼1 −ϵt for
ϵ ≪1 and t ≪ϵ−1, we need to keep the exponential rather than its Taylor series
expansion if we are to construct a solution that is valid when t = O(ϵ−1). Fig-
ure 12.11 shows that the two-term asymptotic expansion, (12.43), rapidly becomes
inaccurate once t = O(ϵ−1).
Fig. 12.11. The exact solution, (12.44), two-term asymptotic solution, (12.43), and one-
term multiple scales solution (12.50) of (12.41) when ϵ = 0.1.
The method of multiple scales, in its most basic form, consists of deﬁning a
new slow time variable, T = ϵt, so that when t = O(1/ϵ), T = O(1), and the

12.2 ORDINARY DIFFERENTIAL EQUATIONS
327
slow decay can therefore be accounted for. We then look for an asymptotic solution
y = y0(t, T) + ϵy1(t, T) + O(ϵ2),
with each term a function of both t, to capture the oscillation, and T, to capture
the slow decay. After noting that
d
dt = ∂
∂t + ϵ ∂
∂T ,
(12.41) becomes
∂2y
∂t2 + 2ϵ ∂2y
∂t∂T + ϵ2 ∂2y
∂T 2 + 2ϵ∂y
∂t + 2ϵ2 ∂y
∂T + y = 0,
(12.45)
to be solved subject to
y(0, 0) = 1,
∂y
∂t (0, 0) + ϵ ∂y
∂T (0, 0) = 0.
(12.46)
At leading order,
∂2y0
∂t2 + y0 = 0,
subject to
y0(0, 0) = 1,
∂y0
∂t (0, 0) = 0.
Although this is a partial diﬀerential equation, the only derivatives are with respect
to t, so we can solve as if it were an ordinary diﬀerential equation in t. However,
we must remember that the ‘constants’ of integration can actually be functions of
the other variable, T. This means that
y0 = A0(T) cos t + B0(T) sin t,
and the boundary conditions show that
A0(0) = 1,
B0(0) = 0.
(12.47)
The functions A0(T) and B0(T) are, as yet, undetermined.
At O(ϵ),
∂2y1
∂t2 + y1= −2 ∂2y0
∂t∂T −2∂y0
∂t = 2
dA0
dT + A0

sin t −2
dB0
dT + B0

cos t.
(12.48)
Because of the appearance on the right hand side of the equation of the terms
cos t and sin t, which are themselves solutions of the homogeneous version of the
equation, the particular integral solution will involve the terms t sin t and t cos t. As
we have seen, it is precisely terms of this type that lead to a nonuniformity in the
asymptotic expansion when t = O(ϵ−1). The terms proportional to sin t and cos t
in (12.48) are known as secular terms and, to keep the asymptotic expansion

328
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
uniform, we must eliminate them by choosing A0 and B0 appropriately. In this
example, we need
dA0
dT + A0 = 0,
dB0
dT + B0 = 0,
(12.49)
and hence
y1 = A1(T) cos t + B1(T) sin t.
The initial conditions for (12.49) are given by (12.47), so the solutions are A0 = e−T
and B0 = 0. We conclude that
y ∼e−T cos t = e−ϵt cos t
(12.50)
for ϵ ≪1. This asymptotic solution is consistent with the exact solution, (12.44),
and remains valid when t = O(ϵ−1), as can be seen in Figure 12.11. In fact, we will
see below that this solution is only valid for t ≪ϵ−2.
In order to proceed to ﬁnd more terms in the asymptotic expansion using the
method of multiple scales, we can take the exact solution, (12.44), as a guide. We
know that
y ∼e−ϵt

cos

1 −1
2ϵ2

t + ϵ sin

1 −1
2ϵ2

t
	
,
(12.51)
for ϵ ≪1. This shows that the phase of the oscillation changes by an O(1) amount
when t = O(ϵ−2). In order to capture this, we seek a solution that is a function of
the two timescales
T = ϵt,
τ =

1 + aϵ2 + bϵ3 + · · ·

t,
with the constants a, b, . . . to be determined. Although this looks like a bit of a
cheat, since we are only doing this because we know the exact solution, this approach
works for a wide range of problems, including nonlinear diﬀerential equations.
In order to develop a one-term multiple scale expansion, we needed to consider
the solution up to O(ϵ). This suggests that we will need to expand up to O(ϵ2) to
construct a two-term multiple scales expansion, with
y = y0(τ, T) + ϵy1(τ, T) + ϵ2y2(τ, T) + O(ϵ3).
After noting that
d
dt = ϵ ∂
∂T +

1 + aϵ2 + bϵ3 + · · ·
 ∂
∂τ ,
equation (12.41) becomes
∂2y
∂τ 2 + 2aϵ2 ∂2y
∂τ 2 + 2ϵ ∂2y
∂τ∂T + ϵ2 ∂2y
∂T 2 + 2ϵ∂y
∂τ + 2ϵ2 ∂y
∂T + y + O(ϵ3) = 0,
(12.52)
to be solved subject to
y(0, 0) = 1,

1 + aϵ2 + bϵ3 + · · ·
 ∂y
∂τ (0, 0) + ϵ ∂y
∂T (0, 0) = 0.
(12.53)
We already know that
y0 = e−T cos τ,
y1 = A1(T) cos τ + B1(T) sin τ.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
329
At O(ϵ2),
∂2y2
∂τ 2 + y2 = −2a∂2y0
∂τ 2 −2 ∂2y1
∂τ∂T + ∂2y0
∂T 2 −2∂y1
∂τ −2∂y0
∂T
= 2
dA1
dt + A1

sin τ −2
dB1
dt + B1 −

a + 1
2

e−T
	
cos τ.
In order to remove the secular terms we need
dA1
dt + A1 = 0,
dB1
dt + B1 =

a + 1
2

e−T .
At O(ϵ) the boundary conditions are
y1(0, 0) = A1(0) = 0,
∂y1
∂τ (0, 0) + ∂y0
∂T (0, 0) = B1(0) −1 = 0,
and hence
A1 = 0,
B1 =

a + 1
2

Te−T + e−T .
This gives us
y ∼e−T cos τ + ϵ

a + 1
2

Te−T + e−T
	
sin τ.
However, the part of the O(ϵ) term proportional to T will lead to a nonuniformity
in the expansion when T = O(ϵ−1), and we must therefore remove it by choosing
a = −1/2. We could have deduced this directly from the diﬀerential equation for
B1, since the term proportional to e−T is secular. We conclude that
τ =

1 −1
2ϵ2 + · · ·

t,
and hence obtain (12.51), as expected.
Example 1: The van der Pol Oscillator
The governing equation for the van der Pol oscillator is
d 2y
dt2 + ϵ(y2 −1)dy
dt + y = 0,
(12.54)
for t ⩾0. For ϵ ≪1 this is a linear oscillator with a weak nonlinear term, ϵ(y2−1) ˙y.
For |y| < 1 this term tends to drive the oscillations to a greater amplitude, whilst
for |y| > 1, this term damps the oscillations. It is straightforward (at least if you’re
an electrical engineer!) to build an electronic circuit from resistors and capacitors
whose output is governed by (12.54). It was in this context that this system was ﬁrst
studied extensively as a prototypical nonlinear oscillator. It is also straightforward
to construct a forced van der Pol oscillator, which leads to a nonzero right hand
side in (12.54), and study the chaotic response of the circuit.
Since the damping is zero when y = 1, a reasonable guess at the behaviour of
the solution for ϵ ≪1 would be that there is an oscillatory solution with unit

330
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
amplitude. Let’s investigate this plausible, but incorrect, guess by considering the
solution with initial conditions
y(0) = 1,
dy
dt (0) = 0.
(12.55)
We will use the method of multiple scales, and deﬁne a slow time scale, T = ϵt. We
seek an asymptotic solution of the form
y = y0(t, T) + ϵy1(t, T) + O(ϵ2).
As for our linear example, we ﬁnd that
y0 = A0(T) cos {t + φ0(T)} .
For this problem, it is more convenient to write the solution in terms of an ampli-
tude, A0(T), and phase, φ0(T). The boundary conditions show that
A0(0) = 1,
φ0(0) = 0.
(12.56)
At O(ϵ),
∂2y1
∂t2 + y1 = −2 ∂2y0
∂t∂T −(y2
0 −1)∂y0
∂t
= 2dA0
dT sin(t + φ0) + 2A0
dφ0
dT cos(t + φ0) + A0 sin(t + φ0)

A2
0 cos2(t + φ0) −1

.
In order to pick out the secular terms on the right hand side, we note that†
sin θ cos2 θ = sin θ −sin3 θ = sin θ −3
4 sin θ + 1
4 sin 3θ = 1
4 sin θ + 1
4 sin 3θ.
This means that
∂2y1
∂t2 + y1 =

2dA0
dT + 1
4A3
0 −A0
	
sin(t + φ0) + 2A0
dφ0
dT cos(t + φ0) + 1
4A3
0 sin 3(t + φ0).
To suppress the secular terms we therefore need
dφ0
dT = 0,
dA0
dT = 1
8A0(4 −A2
0).
Subject to the boundary conditions (12.56), the solutions are
φ0 = 0,
A0 = 2(1 + 3e−T )−1/2.
Therefore
y ∼2(1 + 3e−T )−1/2 cos t
for ϵ ≪1, and we conclude that the amplitude of the oscillation actually tends to
2 as t →∞, as shown in Figure 12.12.
† To get cosn θ in terms of cos mθ for m = 1, 2, . . . , n, use einθ = cos nθ + i sin nθ = (eiθ)n =
(cos θ + i sin θ)n and equate real and imaginary parts.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
331
Fig. 12.12. The leading order solution of the van der Pol equation, (12.54), subject to
y(0) = 1, ˙y(0) = 0, when ϵ = 0.1.
Example 2: Jacobian elliptic functions with almost simple harmonic behaviour
Let’s again turn our attention to the Jacobian elliptic function x = sn(t; k), which
satisﬁes (12.21). When k ≪1, this function is oscillatory and, at leading order,
performs simple harmonic motion. We can see this more clearly by diﬀerentiating
(12.21) and eliminating dx/dt to obtain
d 2x
dt2 + (1 + k2)x = 2k2x3.
(12.57)
The initial conditions, of which there must be two for this second order equation,
are x = 0 and, from (12.21), dx/dt = 1 when t = 0. Let’s now use the method
of multiple scales to see how this small perturbation aﬀects the oscillation after a
long time. As usual, we deﬁne T = k2t and x = x(t, T), in terms of which (12.57)
becomes
∂2x
∂t2 + 2k2 ∂2x
∂t∂T + k4 ∂2x
∂2T + (1 + k2)x = 2k2x3.
(12.58)
We seek an asymptotic solution
x = x0(t, T) + k2x1(t, T) + O(k4).
At leading order
∂2x0
∂t2 + x0 = 0,

332
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
subject to x0(0, 0) = 0 and ∂x0
∂t (0, 0) = 1. This has solution
x0 = A(T) sin {t + φ(t)} ,
and
A(0) = 1,
φ(0) = 0.
(12.59)
At O(k2),
∂2x1
∂t2 + x1 = 2x3
0 −x0 −2 ∂2x0
∂t∂T
= 2A3 sin3(t + φ) −A sin(t + φ) −2dA
dT cos(t + φ) + 2A dφ
dT sin(t + φ)
=
3
2A3 −A + 2A dφ
dT

sin(t + φ) −2dA
dT cos(t + φ) −1
2A3 sin 3(t + φ).
In order to remove the secular terms, we must set the coeﬃcients of sin(t + φ) and
cos(t + φ) to zero. This gives us two simple ordinary diﬀerential equations to be
solved subject to (12.59), which gives
A = 1,
φ = −1
4T,
and hence
x = sin

1 −1
4k2

t
	
+ O(k2),
for k ≪1. We can see that, as we would expect from the analysis given in Sec-
tion 9.4, the leading order amplitude of the oscillation does not change with t, in
contrast to the solution of the van der Pol equation that we studied earlier. How-
ever, the period of the oscillation changes by O(k2) even at this lowest order. If
we take the analysis to O(k4), we ﬁnd that the amplitude of the oscillation is also
dependent on k (see King, 1988).
12.2.5
Slowly Damped Nonlinear Oscillations: Kuzmak’s Method
The method of multiple scales, as we have described it above, is appropriate for
weakly perturbed linear oscillators. Can we make any progress if the leading order
problem is nonlinear? We will concentrate on the simple example,
d 2y
dt2 + 2ϵdy
dt + y −y3 = 0,
(12.60)
subject to
y(0) = 0,
dy
dt (0) = v0 > 0,
(12.61)
with ϵ small and positive. Let’s begin by considering the leading order problem,
with ϵ = 0. As we saw in Chapter 9, since dy/dt does not appear in (12.60) when

12.2 ORDINARY DIFFERENTIAL EQUATIONS
333
ϵ = 0, we can integrate once to obtain
dy
dt = ±
*
E −y2 + 1
2y4,
(12.62)
with E = v2
0 from the initial conditions, (12.61). If we now assume that v2
0 < 1/2†,
and scale y and t using
y =

1 −
√
1 −2E
1/2
ˆy,
t =
1 −
√
1 −2E
E
1/2
ˆt,
(12.62) becomes
dˆy
dˆt = ±
#
1 −ˆy2#
1 −k2ˆy2,
(12.63)
where
k =
1 −
√
1 −2E
1 +
√
1 −2E
1/2
.
(12.64)
If we compare (12.63) with the system that we studied in Section 9.4, we ﬁnd that
y =

1 −
√
1 −2E
1/2
sn

E
1 −
√
1 −2E
1/2
t ; k

.
(12.65)
In the absence of any damping (ϵ = 0), y varies periodically in a manner described
by the Jacobian elliptic function sn. In addition, Example 2 in the previous section
shows that y ∼v0 sin t when v0 ≪1. This is to be expected, since the nonlinear
term in (12.60) is negligible when v0, and hence y, is small.
For ϵ small and positive, but nonzero, by analogy with what we found using the
method of multiple scales in the previous section, we expect that weak damping
leads to a slow decrease in the amplitude of the oscillation, and possibly some change
in its phase. In order to construct an asymptotic solution valid for t = O(ϵ−1), when
the amplitude and phase of the oscillation have changed signiﬁcantly, we begin in
the usual way by deﬁning a slow time scale, T = ϵt. However, for a nonlinear
oscillator, the frequency of the leading order solution depends upon the amplitude
of the oscillation, so it is now convenient to deﬁne
ψ = θ(T)
ϵ
+ φ(T),
θ(0) = 0,
and seek a solution y ≡y(ψ, T). This was ﬁrst done by Kuzmak (1959), although
not in full generality.
Since
dy
dt = (θ′ + ϵφ′) ∂y
∂ψ + ϵ ∂y
∂T ,
where a prime denotes d/dT, we can see that θ′(T) ≡ω(T) is the frequency of the
oscillation at leading order and φ(T) the change in phase, both of which we must
† The usual graphical argument shows that this is a suﬃcient condition for the solution to be
periodic in t.

334
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
determine as part of the solution. In terms of these new variables, (12.60) and
(12.61) become
(ω + ϵφ′)2 ∂2y
∂ψ2 + 2ϵ (ω + ϵφ′) ∂2y
∂T∂ψ + ϵ2 ∂2y
∂T 2 + ϵ (ω′ + ϵφ′′) ∂y
∂ψ
+2ϵ (ω + ϵφ′) ∂y
∂ψ + 2ϵ2 ∂y
∂T + y −y3 = 0,
(12.66)
subject to
y(0, 0) = 0,
(ω(0) + ϵφ′(0)) ∂y
∂ψ (0, 0) + ϵ ∂y
∂T (0, 0) = v0.
(12.67)
We now expand
y = y0(ψ, T) + ϵy1(ψ, T) + O(ϵ2),
and substitute into (12.66) and (12.67). At leading order, we obtain
ω2(T)∂2y0
∂ψ2 + y0 −y3
0 = 0,
(12.68)
subject to
y0(0, 0) = 0,
ω(0)∂y0
∂ψ (0, 0) = v0.
(12.69)
As we have seen, this has solution
y0 =

1 −
#
1 −2E(T)
1/2
sn



%
E(T)
1 −
#
1 −2E(T)
&1/2
ψ
ω(T) ; k(T)


,
(12.70)
where k is given by (12.64). The initial conditions, (12.69), show that
E(0) = v2
0,
φ(0) = 0.
(12.71)
The period of the oscillation, P(T), can be determined by noting that the quarter
period is
1
4P(T) = ω(T)
 
1−√
1−2E(T )
1/2
0
ds
$
E(T) −s2 + 1
2s4
,
which, after a simple rescaling, shows that
P(T) = 4ω(T)
%
1 −
#
1 −2E(T)
E(T)
&1/2
K(k(E)),
(12.72)
where
K(k) =
 1
0
ds
√
1 −s2√
1 −k2s2
(12.73)
is the complete elliptic integral of the ﬁrst kind.
As in the method of multiple scales, we need to go to O(ϵ) to determine the

12.2 ORDINARY DIFFERENTIAL EQUATIONS
335
behaviour of the solution on the slow time scale, T. Firstly, note that we have
three unknowns to determine, E(T), φ(T) and ω(T), whilst for the method of
multiple scales, we had just two, equivalent to E(T) and φ(T). Since we introduced
θ(T) simply to account for the fact that the period of the oscillation changes with
the amplitude, we have the freedom to choose this new time scale so that the period
of the oscillation is constant. For convenience, we take P = 1, so that
dθ
dT = ω ≡ω(E) =
1
4K(k(E))

E
1 −
√
1 −2E
1/2
.
(12.74)
We also need to note for future reference the parity with respect to ψ of y0 and
its derivatives. Both y0 and ∂2y0/∂ψ2 are odd functions, whilst ∂y0/∂ψ is an even
function of ψ. In addition, we can now treat y0 as a function of ψ and E, with
y0(ψ, E) =

1 −
√
1 −2E
1/2
sn {4K(k(E))ψ ; k(E)} .
(12.75)
At O(ϵ) we obtain
ω2 ∂2y1
∂ψ2 +

1 −3y2
0

y1 = −2ωφ′ ∂2y0
∂ψ2 −2ω ∂2y0
∂ψ∂E
dE
dT −ω′ ∂y0
∂ψ −2ω ∂y0
∂ψ ,
which we write as
L(y1) = R1odd + R1even,
(12.76)
where
L = ω2 ∂2
∂ψ2 + 1 −3y2
0,
(12.77)
and
R1odd = −2ωφ′ ∂2y0
∂ψ2 ,
R1even = −2ω ∂2y0
∂ψ∂E
dE
dT −ω′ ∂y0
∂ψ −2ω ∂y0
∂ψ .
(12.78)
Now, by diﬀerentiating (12.68) with respect to ψ, we ﬁnd that
L
∂y0
∂ψ

= 0,
so that ∂y0/∂ψ is a solution of the homogeneous version of (12.76). For the solution
of (12.76) to be periodic, the right hand side must be orthogonal to the solution of
the homogeneous equation, and therefore orthogonal to ∂y0/∂ψ.† This is equivalent
to the elimination of secular terms in the method of multiple scales. Since ∂y0/∂ψ
is even in ψ, this means that
 1
0
∂y0
∂ψ R1evendψ = 0,
which is the equivalent of the secularity condition that determines the amplitude
† Strictly speaking, this is a result that arises from Floquet theory, the theory of linear ordinary
diﬀerential equations with periodic coeﬃcients.

336
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
of the oscillation in the method of multiple scales. Using (12.78), we can write this
as†
d
dE

ω
 1
0
∂y0
∂ψ
2
dψ

dE
dT + 2ω
 1
0
∂y0
∂ψ
2
dψ = 0.
(12.79)
To proceed, we ﬁrstly note that
 1
0
∂y0
∂ψ
2
dψ = 4
 (1−√1−2E)
1/2
0
∂y0
∂ψ dy0
= 4
ω
 (1−√1−2E)
1/2
0
*
E −y2
0 + 1
2y4
0 dy0,
and hence that
∂
∂E

ω
 1
0
∂y0
∂ψ
2
dψ

= 2
 (1−√1−2E)
1/2
0
dy0
$
E −y2
0 + 1
2y4
0
= 1
2ω .
(12.80)
Secondly, using the results of Section 9.4, we ﬁnd that
 1
0
∂y0
∂ψ
2
dψ
= 16K2(E)

1 −
√
1 −2E
  1
0

1 −sn2 (4Kψ ; k)
 
1 −k2sn2 (4Kψ ; k)

dψ
= 16K(E)

1 −
√
1 −2E
  K
0

1 −sn2 (ψ ; k)
 
1 −k2sn2 (ψ ; k)

dψ.
We now need a standard result on elliptic functions‡, namely that
 K
0

1 −sn2 (ψ ; k)
 
1 −k2sn2 (ψ ; k)

dψ
=
1
3k2


1 + k2
L(k) −

1 −k2
K(k)

,
where
L(k) =
 1
0
*
1 −k2s2
1 −s2 ds,
is the complete elliptic integral of the second kind§. Equation (12.79) and
the deﬁnition of ω, (12.74), then show that
dE
dT = −4E
3k2K


1 + k2
L (k) −

1 −k2
K(k)

.
(12.81)
† Note that the quantity in the curly brackets in (12.79) is often referred to as the action.
‡ See, for example, Byrd (1971).
§ Although the usual notation for the complete elliptic integral of the second kind is E(k), the
symbol E is already spoken for in this analysis.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
337
This equation, along with the initial condition, (12.71), determines E(T). For v0 ≪
1, this reduces to the multiple scales result, dE/dT = −2E (see Exercise 12.10). It
is straightforward to integrate (12.81) using MATLAB, since the complete elliptic
integrals of the ﬁrst and second kinds can be calculated using the built-in function
ellipke. Note that we can simultaneously calculate θ(T) numerically by solving
(12.74) subject to θ(0) = 0.
Figure 12.13 shows the function E(T) calculated
for E(0) = 0.45, and also the corresponding result using multiple scales on the
linearized problem, E = E(0)e−2T .
0
0.5
1
1.5
2
2.5
3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
T
E(T)
Kuzmak
Linearized multiple scales
Fig. 12.13. The solution of (12.81) when E(0) = 0.45, and the corresponding multiple
scales solution of the linearized problem, E(T) = E(0)e−2T .
We now need to ﬁnd an equation that determines the phase, φ(T). Unfortunately,
unlike the method of multiple scales, we need to determine the solution at O(ϵ) in
order to make progress. By diﬀerentiating (12.68) with respect to E, we ﬁnd that
L
∂y0
∂E

= −2ω dω
dE
∂2y0
∂ψ2 .
We also note that
L

ψ ∂y0
∂ψ

= 2ω2 ∂2y0
∂ψ2 ,
(12.82)

338
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
and hence that
L (y0odd) = 0,
where
y0odd = ω ∂y0
∂E + dω
dE ψ ∂y0
∂ψ .
(12.83)
The general solution of the homogeneous version of (12.76) is therefore a linear
combination of the even function ∂y0/∂ψ and the odd function y0odd. To ﬁnd the
particular integral solution of (12.76), we note from (12.82) that
L

−φ′
ω ψ ∂y0
∂ψ

= −2ωφ′ ∂2y0
∂ψ2 = R1odd.
We therefore have
y1 = A(T)∂y0
∂ψ + B(T)

ω ∂y0
∂E + dω
dE ψ ∂y0
∂ψ

−φ′
ω ψ ∂y0
∂ψ + y1even,
(12.84)
where y1even is the part of the particular integral generated by R1even, and is itself
even in ψ. For y1 to be bounded as ψ →∞, the coeﬃcient of ψ must be zero.
Noting from (12.75) that
∂y0
∂E ∼4dK
dE ψ ∂y0
∂ψ
as ψ →∞,
this means that
B(T) =
φ′/ω
4ωdK/dE + dω/dE .
(12.85)
The easiest way to proceed is to multiply (12.66) by ∂y/∂ψ, and integrate over
one period of the oscillation. After taking into account the parity of the components
of each integrand, we ﬁnd that
d
dT

e2T (ω + ϵφ′)
 1
0
 ∂y
∂ψ
2
dψ

= 0.
At leading order, this reproduces (12.79), whilst at O(ϵ) we ﬁnd that
d
dT

e2T
%
2ω
 1
0
∂y0
∂ψ
∂y1
∂ψ dψ + φ′
 1
0
∂y0
∂ψ
2
dψ
&
= 0.
(12.86)
Now, using what we know about the parity of the various components of y1, we can
show that
 1
0
∂y0
∂ψ
∂y1
∂ψ dψ =
φ′
dω/dE
 1
0
∂y0
∂ψ
∂2y0
∂ψ∂E dψ =
φ′
2dω/dE
∂
∂E
 1
0
∂y0
∂ψ
2
dψ,
and hence from (12.86) that
d
dT

e2T
φ′
dω/dE
∂
∂E

ω
 1
0
∂y0
∂ψ
2
dψ

= 0.

12.2 ORDINARY DIFFERENTIAL EQUATIONS
339
Using (12.80), this becomes
d
dT
 e2T φ′
ωdω/dE

= 0.
(12.87)
This is a second order equation for φ(T). Although we know that φ(0) = 0, to be
able to solve (12.87) we also need to know φ′(0).
At O(ϵ), the initial conditions, (12.67), are
y1(0, 0) = 0,
ω(E(0))∂y1
∂ψ (0, 0) = −E′(0)∂y0
∂E (0, E(0)) −φ′(0)∂y0
∂ψ (0, E(0)).
(12.88)
By substituting the solution (12.84) for y1 into the second of these, we ﬁnd that
ω(E(0))

A(0)∂2y0
∂ψ2 (0, E(0)) + B(0)ω(E(0)) ∂2y0
∂ψ∂E (0, E(0))
+

B(0) dω
dE (E(0)) −
φ′(0)
ω(E(0))
 ∂y0
∂ψ (0, E(0)) + ∂y1even
∂ψ
(0, 0)
	
= −E′(0)∂y0
∂E (0, E(0)) −φ′(0)∂y0
∂ψ (0, E(0)).
Using (12.85), all of the terms that do not involve φ′(0) are odd in ψ, and therefore
vanish when ψ = 0. We conclude that φ′(0) = 0, and hence from (12.87) that
φ(T) = 0.
Figure 12.14 shows a comparison between the leading order solution computed
using Kuzmak’s method, the leading order multiple scales solution of the linearized
problem, y = √v0e−T sin t, and the numerical solution of the full problem when
ϵ = 0.01. The numerical and Kuzmak solutions are indistinguishable. Although
the multiple scales solution gives a good estimate of E(T), as shown in ﬁgure 12.13,
it does not give an accurate solution of the full problem.
To see how the method works in general for damped nonlinear oscillators, the
interested reader is referred to Bourland and Haberman (1988).
12.2.6
The Eﬀect of Fine Scale Structure on Reaction–Diﬀusion
Processes
Consider the two-point boundary value problem
d
dx

D

x, x
ϵ
 dθ
dx
	
+ R

θ, x, x
ϵ

= 0 for 0 < x < 1,
(12.89)
subject to the boundary conditions
dθ
dx = 0 at x = 0 and x = 1.
(12.90)
We can think of θ(x) as a dimensionless temperature, so that this boundary value
problem models the steady distribution of heat in a conducting material. When
ϵ ≪1, this material has a ﬁne scale structure varying on a length scale of O(ϵ), and

340
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
T
y
Numerical
Kuzmak
Linearized multiple scales
Fig. 12.14. The numerical solution of (12.60) subject to (12.61) when v2
0 = 0.45 and
ϵ = 0.01 compared to asymptotic solutions calculated using the leading order Kuzmak
solution and the leading order multiple scales solution of the linearized problem.
a coarse, background structure varying on a length scale of O(1). The steady state
temperature distribution represents a balance between the diﬀusion of heat, (Dθx)x
(where subscripts denote derivatives), and its production by some chemical reaction,
R (see Section 12.2.3, Example 2 for a derivation of this type of balance law). If
we integrate (12.89) over 0 ⩽x ⩽1 and apply the boundary conditions (12.90), we
ﬁnd that the chemical reaction term must satisfy the solvability condition
 1
0
R

θ(x), x, x
ϵ

dx = 0
(12.91)
for a solution to exist. Physically, since (12.90) states that no heat can escape from
the ends of the material, (12.91) simply says that the overall rate at which heat
is produced must be zero for a steady state to be possible, with sources of heat in
some regions of the material balanced by heat sinks in other regions.
In order to use asymptotic methods to determine the solution at leading order
when ϵ ≪1, we begin by introducing the fast variable, ˆx = x/ϵ. In the usual way

12.2 ORDINARY DIFFERENTIAL EQUATIONS
341
(see Section 12.2.4), (12.89) and (12.90) become
(Dθˆx)ˆx + ϵ {(Dθˆx)x + (Dθx)ˆx} + ϵ2 {(Dθx)x + R} = 0,
(12.92)
subject to
θˆx + ϵθx = 0 at x = ˆx = 0 and x = 1, ˆx = 1/ϵ,
(12.93)
with R ≡R (θ(x, ˆx), x, ˆx), D ≡D(x, ˆx). It is quite awkward to apply a boundary
condition at ˆx = 1/ϵ ≫1, but we shall see that we can determine the equation
satisﬁed by θ at leading order without using this, and we will not consider it below.
We now expand θ as
θ = θ0(x, ˆx) + ϵθ1(x, ˆx) + ϵ2θ2(x, ˆx) + O(ϵ3).
At leading order,
(Dθ0ˆx)ˆx = 0,
subject to
θ0ˆx = 0 at x = ˆx = 0.
We can integrate this once to obtain Dθ0ˆx = α(x), with α(0) = 0, and then once
more, which gives
θ0 = α(x)
 ˆx
0
ds
D(x, s) + f0(x).
At O(ϵ), we ﬁnd that
(Dθ1ˆx)ˆx = −(Dθ0ˆx)x −(Dθ0x)ˆx ,
which we can write as
(Dθ1ˆx + Dθ0x)ˆx = −α′(x).
(12.94)
We can integrate (12.94) to obtain
Dθ1ˆx + Dθ0x = −α′(x)ˆx + β(x).
Substituting for θ0 and integrating again shows that
θ1 = f1(x) −f ′
0(x)ˆx −α′(x)ˆx
 ˆx
0
ds
D(x, s)
−α(x) ∂
∂x
 ˆx
0
ˆx −s
D(x, s)ds + β(x)
 ˆx
0
ds
D(x, s).
(12.95)
When ˆx is large, there are terms in this expression that are of O(ˆx2). These are
secular, and become comparable with the leading order term in the expansion for
θ when ˆx = 1/ϵ. We must eliminate this secularity by taking
lim
ϵ→0

α′(x)ϵ
 1/ϵ
0
ds
D(x, s) −α(x)ϵ2 ∂
∂x
 1/ϵ
0
1/ϵ −s
D(x, s)ds

= 0.
(12.96)
This is a ﬁrst order ordinary diﬀerential equation for α(x). Since α(0) = 0, we

342
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
conclude that α ≡0. Note that each of the terms in (12.96) is of O(1) for ϵ ≪1.
For example, ϵ
' 1/ϵ
0
ds
D(x,s) is the mean value of 1/D over the ﬁne spatial scale. We
now have simply θ0 = f0(x). This means that, at leading order, θ varies only on
the coarse, O(1) length scale. This does not mean that the ﬁne scale structure has
no eﬀect, as we shall see.
Since we now know that θ0 = O(1) when ˆx = 1/ϵ, we must also eliminate the
secular terms that are of O(ˆx) when ˆx is large in (12.95). We therefore require that
f ′
0 = dθ0
dx = lim
ϵ→0

ϵβ(x)
 1/ϵ
0
ds
D(x, s)

.
(12.97)
In order to determine β, and hence an equation satisﬁed by θ0, we must consider
one further order in the expansion.
At O(ϵ2), (12.92) gives
(Dθ2ˆx + Dθ1x)ˆx = −(Dθ1ˆx)x −(Dθ0x)x −R(θ0(x), x, ˆx)
= −β′(x) −R(θ0(x), x, ˆx),
(12.98)
which we can integrate twice to obtain
θ2 = f2(x) −f ′
1(x)ˆx + 1
2
d 2θ0
dx2 ˆx2 −∂
∂x

β(x)
 ˆx
0
ˆx −s
D(x, s)ds

+ γ(x)
 ˆx
0
ds
D(x, s)
−β′(x)
 ˆx
0
s
D(x, s)ds −
 ˆx
0
1
D(x, s)
 s
0
R(θ0(x), x, u) du ds.
(12.99)
In order to eliminate the secular terms of O(ˆx2), we therefore require that
lim
ϵ→0

1
2
d 2θ0
dx2 −ϵ2 ∂
∂x

β(x)
 1/ϵ
0
1/ϵ −s
D(x, s)ds

−β′(x)ϵ2
 1/ϵ
0
s
D(x, s)ds −ϵ2
 1/ϵ
0
1
D(x, s)
 s
0
R(θ0(x), x, u) du ds

= 0.
If we now use (12.97) to eliminate β(x), we arrive at
lim
ϵ→0



d 2θ0
dx2 −
2 d
dx

ϵ2 ' 1/ϵ
0
s
D(x,s)ds

ϵ
' 1/ϵ
0
ds
D(x,s)
dθ0
dx
+2ϵ2
 1/ϵ
0
1
D(x, s)
 s
0
R(θ0(x), x, u) du ds


= 0.
After multiplying through by a suitable integrating factor, we can see that θ0 sat-
isﬁes the reaction–diﬀusion equation

 ¯D(x)θ0x

x + ¯R(θ0(x), x) = 0,
(12.100)

12.2 ORDINARY DIFFERENTIAL EQUATIONS
343
on the coarse scale, where
¯D(x) = lim
ϵ→0

exp


−2
 x
0
d
dX

ϵ2 ' 1/ϵ
0
s
D(X,s)ds

ϵ
' 1/ϵ
0
ds
D(X,s)
dX




,
(12.101)
¯R(θ0(x), x) = lim
ϵ→0

2 ¯D(x)ϵ2
 1/ϵ
0
1
D(x, s)
 s
0
R(θ0(x), x, u) du ds

(12.102)
are the ﬁne scale averages.
This asymptotic analysis, which is called homogenization, shows that the lead-
ing order temperature does not vary on the ﬁne scale, and satisﬁes a standard
reaction–diﬀusion equation on the coarse scale. However, the ﬁne scale structure
modiﬁes the reaction term and diﬀusion coeﬃcient through (12.101) and (12.102),
with ¯D the homogenized diﬀusion coeﬃcient and ¯R the homogenized reac-
tion term. If we were to seek higher order corrections, we would ﬁnd that there
are variations in the temperature on the ﬁne scale, but that these are at most of
O(ϵ).
One case where ¯D and ¯R take a particularly simple form is when D(x, ˆx) =
D0(x) ˆD(ˆx) and R(θ(x), x, ˆx) = R0(θ(x), x) ˆR(ˆx). On substituting these into (12.101)
and (12.102), we ﬁnd, after cancelling a constant common factor between ¯D and
¯R, that we can use ¯D(x) = D0(x) and ¯R(θ, x) = KR0(θ, x), where
K = lim
ϵ→0

2ϵ
 1/ϵ
0
1
ˆD(s)
 s
0
ˆR(u) du ds

.
The homogenized diﬀusion coeﬃcient and reaction term are simply given by the
terms D0(x) and R0(θ, x), modiﬁed by a measure of the mean value of the ratio
of the ﬁne scale variation of each, given by the constant K. In particular, when
ˆD(ˆx) = 1/ (1 + A1 sin k1ˆx) and ˆR(ˆx) = 1 + A2 sin k2ˆx for some positive constants
k1, k2, A1 and A2, with A1 < 1 and A2 < 1, we ﬁnd that K = 1. We can illustrate
this for a simple case where it is straightforward to ﬁnd the exact solution of both
(12.89) and (12.100) analytically. Figure 12.15 shows a comparison between the
exact and asymptotic solutions for various values of ϵ when k1 = k2 = 1, A1 =
A2 = 1/2, D0(x) = 1/(1 + x) and R0 = 2x −1. The analytical solution of (12.89)
that vanishes at x = 0 (the solution would otherwise contain an arbitrary constant
in this case) is
θ = 1
2x2 −1
4x4 + ϵ2
1
2(1 −2x)
1
8 cos
2x
ϵ

−sin
x
ϵ
	
−1
4x −1
16

+ϵ3

2 cos
x
ϵ

+ 3
16 sin
2x
ϵ

−2

,
whilst the corresponding solution of (12.100) correctly reproduces the leading order
part of this.
Homogenization has been used successfully in many more challenging appli-
cations than this linear, steady state reaction diﬀusion equation, for example,

344
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
x
θ
asymptotic
analytical, ε = 0.2
analytical, ε = 0.1
analytical, ε = 0.05
Fig. 12.15. Analytical and asymptotic solutions of (12.89) when D = 1/(1+x) 
1 + 1
2 sin x
and R = (2x −1) 
1 + 1
2 sin x
.
in assessing the strength of elastic media with small carbon ﬁbre reinforcements
(Bakhvalov and Panasenko, 1989).
12.2.7
The WKB Approximation
In all of the examples that we have seen so far, we have used an expansion in
algebraic powers of a small parameter to develop perturbation series solutions of dif-
ferential or algebraic equations. This procedure has been at its most complex when
we have needed to match a slowly varying outer solution with an exponentially-
rapidly varying, or dissipative, boundary layer.
This procedure doesn’t always
work! For example, if we consider the two-point boundary value problem
ϵ2y′′(x) + φ(x)y(x) = 0 subject to y(0) = 0 and y(1) = 1,
(12.103)
the procedure fails as there are no terms to balance with the leading order term,
φ(x)y(x). If there were a ﬁrst derivative term in this problem, the procedure would
work, although we would have a singular perturbation problem. However, a ﬁrst
derivative term can always be removed. Suppose that we have a diﬀerential equation

12.2 ORDINARY DIFFERENTIAL EQUATIONS
345
of the form
w′′ + p(x)w′ + q(x)w = 0.
By writing w = Wu, we can easily show that
W ′′ +
2u′
u + p

W ′ +
u′′
u + pu′
u + q

W = 0.
By choosing 2u′
u +p = 0 and hence u = exp

−1
2
' x p(t) dt

, we can remove the ﬁrst
derivative term. Because of this, there is a sense in which ϵ2y′′ +φy = 0 is a generic
second order ordinary diﬀerential equation, and we need to develop a perturbation
method that can deal with it.
The Basic Expansion
A suitable method was proposed by Wentzel, Kramers and Brillioun (and perhaps
others as well) in the 1920s. The appropriate asymptotic development is of the
form
y = exp
ψ0(x)
ϵ
+ ψ1(x) + O(ϵ)
	
.
Diﬀerentiating this gives
y′ = exp
ψ0
ϵ + ψ1
	 ψ′
0
ϵ + ψ′
1 + O(ϵ)
	
,
and
y′′ = exp
ψ0
ϵ + ψ1 + O(ϵ)
	 ψ′′
0
ϵ + ψ′′
1(x) + O(ϵ)
	
+ exp
ψ0
ϵ + ψ1 + O(ϵ)
	 ψ′
0
ϵ + ψ′
1 + O(ϵ)
	2
= exp
ψ0
ϵ + ψ1
	 (ψ′
0)2
ϵ2
+ 1
ϵ (2ψ′
0ψ′
1 + ψ′′
0) + O(1)
	
.
If we substitute these into (12.103), we obtain
(ψ′
0)2 + ϵ (2ψ′
0ψ′
1 + ψ′′
0) + φ(x) + O(ϵ2) = 0,
and hence
(ψ′
0)2 = −φ(x),
ψ′
1 = −ψ′′
0
2ψ′
0
= −1
2
d
dx (log ψ′
0) .
(12.104)
If φ(x) > 0, say for x > 0, we can simply integrate these equations to obtain
ψ0 = ±i
 x
φ1/2(t) dt + constant,
ψ1 = −1
4 log (φ(x)) + constant.

346
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
All of these constants of integration can be absorbed into a single constant, which
may depend upon ϵ, in front of the exponential.
The leading order solution is
rapidly oscillating, or dispersive, in this case, and can be written as
y =
A(ϵ)
φ1/4(x) exp

i
' x φ1/2(t) dt
ϵ

+
B(ϵ)
φ1/4(x) exp

−i
' x φ1/2(t) dt
ϵ

+ · · · .
(12.105)
Note that this asymptotic development has assumed that x = O(1), and, in order
that it remains uniform, we must have ψ1 ≪ψ0/ϵ.
If φ(x) < 0, say for x < 0, then some minor modiﬁcations give an exponential,
or dissipative, solution of the form,
y =
C(ϵ)
|φ(x)|1/4 exp
' x |φ(t)|1/2 dt
ϵ

+
D(ϵ)
|φ(x)|1/4 exp

−
' x |φ(t)|1/2 dt
ϵ

+ · · · .
(12.106)
If φ(x) is of one sign in the domain of solution, one or other of the expansions
(12.105) and (12.106) will be appropriate. If, however, φ(x) changes sign, we will
have an oscillatory solution on one side of the zero and an exponentially growing or
decaying solution on the other. We will consider how to deal with this combination
of dispersive and dissipative behaviour after studying a couple of examples.
Example 1: Bessel functions for x ≫1
We saw in Chapter 3 that Bessel’s equation is
y′′ + 1
xy′ +

1 −ν2
x2

y = 0.
If we make the transformation y = x−1/2Y , we obtain the generic form of the
equation,
Y ′′ +

1 +
1
2 −ν2
x2

Y = 0.
(12.107)
Although this equation currently contains no small parameter, we can introduce
one in a useful way by deﬁning ¯x = δx. If x is large and positive, we can have
¯x = O(1) by choosing δ suﬃciently small. We have introduced this artiﬁcial small
parameter as a device to help us determine how the Bessel function behaves for
x ≫1, and it cannot appear in the ﬁnal result when we change variables back from
¯x to x.
In terms of ¯x and ¯Y (¯x) = Y (x), (12.107) becomes
δ2 ¯Y ′′ +

1 + δ2
1
2 −ν2
¯x2

¯Y = 0.
(12.108)
By direct comparison with the derivation of the WKB expansion above, in which
we neglected terms of O(δ2),
ψ0 = ±i
 ¯x
dt = ±i¯x,
ψ1 = −1
4 log 1 = 0,

12.2 ORDINARY DIFFERENTIAL EQUATIONS
347
so that
¯Y ∼A(δ) exp
i¯x
δ

+ B(δ) exp

−i¯x
δ

+ · · · ,
and hence
y ∼
1
x1/2

Aeix + Be−ix
is the required expansion. Note that, although we can clearly see that the Bessel
functions are slowly decaying, oscillatory functions of x as x →∞, we cannot
determine the constants A and B using this technique. As we have already seen, it
is more appropriate to use the integral representation (11.14).
We can show that the WKB method is not restricted to ordinary diﬀerential
equations with terms in just y′′ and y by considering a further example.
Example 2: A boundary layer
Let’s try to ﬁnd a uniformly valid approximation to the solution of the two-point
boundary value problem
ϵy′′ + p(x)y′ + q(x)y = 0 subject to y(0) = α, y(1) = β
(12.109)
when ϵ ≪1 and p(x) > 0. If we assume a WKB expansion,
y = exp
ψ0(x)
ϵ
+ ψ1(x) + O(ϵ)
	
,
and substitute into (12.109), we obtain at O(1/ϵ)
ψ′
0 {ψ′
0 + p(x)} = 0,
(12.110)
and at O(1),
2ψ′
0ψ′
1 + ψ′′
0 + p(x)ψ′
1 + q(x) = 0.
(12.111)
Using the solution ψ′
0 = 0 of (12.110) and substituting into (12.111) gives ψ′
1 =
−q(x)/p(x), which generates a solution of the form
y1 = exp
c1
ϵ −
 x
0
q(t)
p(t) + · · ·
	
∼C1(ϵ) exp

−
 x
0
q(t)
p(t)
	
.
(12.112)
The second solution of (12.110) has ψ′
0 = −p(x) and hence
ψ0 = −
 x
p(t) dt + c2.
Equation (12.111) then gives
ψ1 = −log p(x) +
 x
0
q(t)
p(t) dt.

348
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
The second independent solution therefore takes the form
y2 = exp

c2 −
' x
0 p(t) dt
ϵ
−log p(x) +
 x
0
q(t)
p(t) dt + · · ·

∼C2
p(x) exp

−1
ϵ
 x
0
p(t) dt +
 x
0
q(t)
p(t) dt
	
.
(12.113)
Combining (12.112) and (12.113) then gives the general asymptotic solution as
y = C1 exp

−
 x
0
q(t)
p(t)
	
+ C2
p(x) exp

−1
ϵ
 x
0
p(t) dt +
 x
0
q(t)
p(t) dt
	
.
(12.114)
We can now apply the boundary conditions in (12.109) to obtain
α = C1 + C2
p(0),
β = C1 exp

−
 1
0
q(t)
p(t)
	
+ C2
p(1) exp

−1
ϵ
 1
0
p(t) dt +
 1
0
q(t)
p(t) dt
	
.
The term exp

−1
ϵ
' 1
0 p(t) dt

is uniformly small and can be neglected, so that the
asymptotic solution can be written as
y ∼β exp
 1
x
q(t)
p(t) dt
	
+ p(0)
p(x)

α −β exp
 1
0
q(t)
p(t) dt
	
exp

−1
ϵ
 x
0
p(t) dt +
 x
0
q(t)
p(t) dt
	
.
Finally, the last exponential in this solution is negligibly small unless x = O(ϵ) (the
boundary layer), so we can write
y ∼β exp
 1
x
q(t)
p(t) dt
	
+ p(0)
p(x)

α −β exp
 1
0
q(t)
p(t) dt
	
exp

−p(0)x
ϵ
	
.
This is precisely the composite expansion that we would have obtained if we had
used the method of matched asymptotic expansions instead.
Connection Problems
Let’s now consider the boundary value problem
ϵ2y′′(x) + φ(x)y(x) = 0 subject to y(0) = 1, y →0 as x →−∞,
(12.115)
with
φ(x) > 0
for x > 0,
φ(x) ∼φ1x
for |x| ≪1, φ1 > 0,
φ(x) < 0
for x < 0.
(12.116)
To prevent nonuniformities as |x| →∞, we will also insist that |φ(x)| ≫x−2 for
|x| ≫1. Using the expansions (12.105) and (12.106), we can immediately write
y =
A(ϵ)
φ1/4(x) exp

i
' x
0 φ1/2(t) dt
ϵ

+

12.2 ORDINARY DIFFERENTIAL EQUATIONS
349
B(ϵ)
φ1/4(x) exp

−i
' x
0 φ1/2(t) dt
ϵ

+ · · ·
for x > 0,
(12.117)
y = C(ϵ) exp

−1
ϵ
 0
x
|φ(t)|1/2 dt −1
4 log |φ(x)| + · · ·
	
for x < 0.
(12.118)
The problem of determining how A and B depend upon C is known as a connection
problem, and can be solved by considering an inner solution in the neighbourhood
of the origin.
For |x| ≪1, φ ∼φ1x, so we can estimate the sizes of the terms in the WKB
expansion. For x < 0
y ∼C(ϵ) exp

−1
ϵ
 0
x
(−φ1t)1/2 dt −1
4 log(−φ1x)
	
∼C(ϵ) exp

−2
3ϵφ1/2
1
(−x)3/2 −1
4 log φ1 −1
4 log(−x)
	
.
We can now see that the second term becomes comparable to the ﬁrst when −x =
O(ϵ2/3). A similar estimate of the solution for x > 0 also gives a nonuniformity
when x = O(ϵ2/3). The WKB solutions will therefore be valid in two outer regions
with |x| ≫ϵ2/3. We will need a small inner region, centred on the origin, and the
inner solution must match with the outer solutions. Equation (12.115) shows that
the only rescaling possible near to the origin is in a region where x = O(ϵ2/3), and
¯x = x/ϵ2/3 = O(1) for ϵ ≪1. Writing (12.117) and (12.118) in terms of ¯x leads to
the matching conditions
¯y ∼
C(ϵ)
φ1/4
1
(−¯x)1/4ϵ1/6 exp

−2
3(−¯x)3/2φ1/2
1
	
as ¯x →−∞,
(12.119)
¯y ∼
A(ϵ)
φ1/4
1
¯x1/4ϵ1/6 exp

i2
3 ¯x3/2φ1/2
1
	
+
B(ϵ)
φ1/4
1
¯x1/4ϵ1/6 exp

−i2
3 ¯x3/2φ1/2
1
	
as ¯x →∞.
(12.120)
If we now rewrite (12.115) in the inner region, making use of φ ∼ϵ2/3φ1¯x at leading
order, we arrive at
d 2¯y
d¯x2 + φ1¯x¯y = 0,
subject to ¯y(0) = 1, and the matching conditions (12.119) and (12.120).
(12.121)
We can write (12.121)† in terms of a standard equation by deﬁning t = −φ1/3
1
¯x, in
terms of which (12.121) becomes
d 2¯y
dt2 = t¯y.
† Note that equation (12.121) is valid for |x| ≪1, and we would expect its solution to be valid
in the same domain. Hence there is an overlap of the domains for which the inner and outer
solutions are valid, namely ϵ2/3 ≪|x| ≪1, and we can expect the asymptotic matching
process to be successful. In fact the overlap domain can be reﬁned to ϵ2/3 ≪|x| ≪ϵ2/5 (see
Exercise 12.17).

350
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
This is Airy’s equation, which we met in Section 3.8, so the solution can be written
as
¯y = aAi(t) + bBi(t) = aAi

−φ1/3
1
¯x

+ bBi

−φ1/3
1
¯x

.
(12.122)
In Section 11.2.3 we determined the asymptotic behaviour of Ai(t) for |t| ≫1 using
the method of steepest descents. The same technique can be used for Bi(t), and we
ﬁnd that
Ai(t) ∼1
2π−1/2t−1/4 exp

−2
3t3/2

,
Bi(t) ∼π−1/2t−1/4 exp
2
3t3/2

as t →∞,
(12.123)
Ai(t) ∼
1
√π (−t)−1/4 sin
2
3(−t)3/2 + π
4
	
,
Bi(t) ∼
1
√π (−t)−1/4 cos
2
3(−t)3/2 + π
4
	
as t →−∞.
(12.124)
Using this known behaviour to determine the behaviour of the inner solution,
(12.122), shows that
¯y ∼a1
2π−1/2 
φ1/3
1
¯x
−1/4
exp

−2
3

φ1/3
1
¯x
3/2	
+bπ−1/2 
φ1/3
1
¯x
−1/4
exp
2
3

φ1/3
1
¯x
3/2	
as ¯x →−∞.
In order to satisfy the matching condition (12.119), we must have b = 0, so that
only the Airy function Ai appears in the solution, and
C(ϵ) = 1
2π−1/2ϵ1/6φ1/6
1
a.
(12.125)
The boundary condition ¯y(0) = 1 then gives a = 1/Ai(0) = Γ
 2
3

32/3, and hence
determines C(ϵ) through (12.125).
As ¯x →∞,
¯y ∼a 1
√π

φ1/3
1
¯x
−1/4
sin
2
3

φ1/3
1
¯x
3/2
+ π
4
	
∼
a
√πφ1/12
1
¯x1/4
1
2i

exp
2
3iφ1/2
1
¯x3/2 + π
4

−exp

−2
3iφ1/2
1
¯x3/2 −π
4
	
.
We can therefore satisfy the matching condition (12.120) by taking
A(ϵ) = B∗(ϵ) = aφ1/6
1
ϵ1/6
2i√π
.
Since A and B are complex conjugate, the solution is real for x > 0, as of course
it should be. This determines all of the unknown constants, and completes the

12.3 PARTIAL DIFFERENTIAL EQUATIONS
351
solution. The Airy function Ai(t) is shown in Figure 11.12, which clearly shows
the transition from dispersive oscillatory behaviour as t →−∞to dissipative,
exponential decay as t →∞.
12.3
Partial Diﬀerential Equations
Many of the asymptotic methods that we have met can also be applied to partial
diﬀerential equations. As you might expect, the task is usually rather more diﬃcult
than we have found it to be for ordinary diﬀerential equations. We will proceed by
considering four generic examples.
Example 1: Asymptotic solutions of the Helmholtz equation
As an example of an elliptic partial diﬀerential equation, let’s consider the solution
of the Helmholtz equation,
∇2φ + ϵ2φ = 0 for r ⩽1,
(12.126)
subject to φ(1, θ) = sin θ for ϵ ≪1. This arises naturally as the equation that
governs time-harmonic solutions of the wave equation,
1
c2
∂2z
∂t2 = ∇2z,
which we met in Chapter 3. If we write z = eiωtφ(x), we obtain (12.126), with
ϵ = ω/c. Since φ = O(1) on the boundary, we expand φ = φ0 + ϵ2φ2 + O(ϵ4), and
obtain, at leading order,
∇2φ0 = 0,
subject to φ0(1, θ) = sin θ.
If we seek a separable solution of the form φ0 = f(r) sin θ, we obtain
f ′′ + 1
r f ′ −1
r2 f = 0,
subject to f(1) = 1.
This has solutions of the form f = Ar+Br−1, so the bounded solution that satisﬁes
the boundary condition is f(r) = r, and hence φ0 = r sin θ. At O(ϵ2), (12.126) gives
∇2φ2 = −r sin θ,
subject to φ2(1, θ) = 0.
If we again seek a separable solution, φ2 = F(r) sin θ, we arrive at
F ′′ + 1
r F ′ −1
r2 F = −r
a,
subject to F(1) = 0.
Using the variation of parameters formula, this has the bounded solution
φ2 = 1
8

r −r3
sin θ.
The two-term asymptotic expansion of the solution can therefore be written as
φ = r sin θ + 1
8ϵ2 
r −r3
sin θ + O(ϵ4),
which is bounded and uniformly valid throughout the circle, r ⩽1.

352
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Let’s also consider a boundary value problem for the modiﬁed Helmholtz
equation,
ϵ2∇φ −φ = 0 subject to φ(1, θ) = 1 and φ →0 as r →∞.
(12.127)
Note that φ = 0 satisﬁes both the partial diﬀerential equation and the far ﬁeld
boundary condition, but not the boundary condition at r = 1. This suggests that
we need a boundary layer near r = 1. If we deﬁne r = 1 + ϵ¯r with ¯r = O(1) in the
boundary layer for ϵ ≪1, we obtain
φ¯r¯r −φ = 0,
at leading order. This has solution
φ = ¯A(θ)e¯r + ¯B(θ)e−¯r,
which will match with the far ﬁeld solution if ¯A = 0, and satisfy the boundary
condition at ¯r = 0 if ¯B = 1. The inner solution, and also a composite solution valid
at leading order for all r ⩾a, is therefore
φ = exp
r −1
ϵ

.
Example 2: The small and large time solutions of a diﬀusion problem
Consider the initial value problem for the diﬀusion equation,
∂c
∂t = D ∂2c
∂x2
for −∞< x < ∞and t > 0,
(12.128)
to be solved subject to the initial condition
c(x, 0) =
 f0(x)
for x ⩽0,
0
for x > 0,
(12.129)
with f0 ∈C2(R), f0 →0 as x →−∞, f0(0) ̸= 0 and
 0
−∞
f0(x) dx = ftot.
(12.130)
We could solve this using either Laplace or Fourier transforms . The result, however,
would be in the form of a convolution integral, which does not shed much light on
the structure of the solution. We can gain a lot of insight by asking how the solution
behaves just after the initial state begins to diﬀuse (the small time solution, t ≪1),
and after a long time (t ≫1).
The small time solution, t ≪1
The general eﬀect of diﬀusion is to smooth out gradients in the function c(x, t).
It can be helpful to think of c as a distribution of heat or a chemical concentration.
This smoothing is particularly pronounced at points where c is initially discontinu-
ous, in this case at x = 0 only. Diﬀusion will also spread the initial data into x > 0,
where c = 0 initially. For this reason, we anticipate that there will be three distinct
asymptotic regions.

12.3 PARTIAL DIFFERENTIAL EQUATIONS
353
— Region I: x < 0. In this region we expect a gradual smoothing out of the initial
data.
— Region II: |x| ≪1. The major feature in this region will be an instantaneous
smoothing out of the initial discontinuity.
— Region III: x > 0. There will be a ﬂux from x < 0 into this region, so we expect
an immediate change from c = 0 to c nonzero.
Thinking about the physics of the problem before doing any detailed calculation
is usually vital to unlocking the structure of the asymptotic solution of a partial
diﬀerential equation.
Let’s begin our analysis in region I by posing an asymptotic expansion valid for
t ≪1,
c(x, t) = f0(x) + tf1(x) + t2f2(x) + O(t3).
Substituting this into (12.128) gives
f1 = Df ′′
0 ,
2f2 = Df ′′
1 ,
and hence
c(x, t) = f0(x) + tDf ′′
0 (x) + 1
2t2D 2f ′′′′
0 (x) + O(t3).
(12.131)
Note that c increases in regions where f ′′
0 > 0, and vice versa, as physical intuition
would lead us to expect. Note also that as x →0−, c(x, t) ∼f0(0). However, in
x > 0 we would expect c to be small, and the solutions in regions I and III will not
match together without a boundary layer centred on the origin, namely region II.
Before setting up this boundary layer, it is convenient to ﬁnd the solution in
region III, where we have noted that c is small. If we try a WKB expansion of the
form†
c(x, t) = exp

−A(x)
t
+ B(x) log t + C(x) + o(1)
	
,
and substitute into (12.128), we obtain
A = DA2
x,
AxBx = 0,
B = −D (Axx + 2AxCx) .
The solutions of these equations are
A = x2
4D + βx + Dβ2,
B = b,
C = −

b + 1
2

log (2βD + x) + d,
where β, b and d are constants of integration. The WKB solution in region III is
therefore
c = exp

−1
t
 x2
4D + βx + Dβ2

+ b log t −

b + 1
2

log (2βD + x) + d + o(1)
	
.
(12.132)
† Note that we are using the small time, t, in the WKB expansion that we developed in Sec-
tion 12.2.7.

354
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
In the boundary layer, region II, we deﬁne a new variable, η = x/tα. In region
II, |η| = O(1), and hence |x| = O(tα) ≪1, with α > 0 to be determined. In terms
of η, (12.128) becomes
∂c
∂t −α
t η ∂c
∂η = D
t2α
∂2c
∂η2 .
Since c = O(1) in the boundary layer (remember, c must match with the solution
in region I as η →−∞, where c = O(1)), we can balance terms in this equation
to ﬁnd a distinguished limit when α = 1/2.
The boundary layer therefore has
thickness of O(t1/2), which is a typical diﬀusive length scale. If we now expand as
c = c0(η) + o(1), we have, at leading order,
−1
2ηc0η = Dc0ηη.
(12.133)
If we now write the solutions in regions I and III, given by (12.131) and (12.132),
in terms of η, we arrive at the matching conditions
c0 ∼f0(0) as η →−∞,
(12.134)
c0 ∼exp

−Dβ2
t
−βη
t1/2 −η2
4D + b log t−

b + 1
2

log

2βD + t1/2η

+ d + o(1)
	
as η →∞.
(12.135)
The solution of (12.133) is
c0(η) = F + G
 η
−∞
e−s2/4D ds.
As η →−∞, c ∼F = f0(0). As η →∞, we can use integration by parts to show
that
c0 = f0(0) + G
 ∞
−∞
e−s2/4D ds −2D
η e−η2/4D + O
 1
η2 e−η2/4D
	
.
In order that this is consistent with the matching condition (12.135), we need
f0(0) + G
 ∞
−∞
e−s2/4D ds = 0,
and hence G = −f0(0)/
√
4πD. This leaves us with
c0 ∼exp

−η2
4D + log (−2DG) −log η
	
.
For this to be consistent with (12.135) we need β = 0, b = 1/2 and d = log (−2DG).
The structure of the solution that we have constructed allows us to be rather
more precise about how diﬀusion aﬀects the initial data. For |x| ≫t1/2, x < 0 there
is a slow smoothing of the initial data that involves algebraic powers of t, given by
(12.131). For |x| ≫t1/2, x > 0, c is exponentially small, driven by a diﬀusive ﬂux
across the boundary layer. For |x| = O(t1/2) there is a boundary layer, with the

12.3 PARTIAL DIFFERENTIAL EQUATIONS
355
solution changing by O(1) over a small length of O(t1/2). This small time solution
continues to evolve, and, when t = O(1), is not calculable by asymptotic methods.
When t is suﬃciently large, a new asymptotic structure emerges, which we shall
consider next.
The large time solution, t ≫1
After a long time, diﬀusion will have spread out the initial data in a more or
less uniform manner, and the structure of the solution is rather diﬀerent from that
which we discussed above for t ≪1. We will start our asymptotic development
where x = O(1), and seek a solution of the form
c(x, t) = c0(t) + c1(x, t) + · · · ,
with |c1| ≪|c0| for t ≫1 to ensure that the expansion is asymptotic.
If we
substitute this into (12.128), we obtain
˙c0(t) = Dc1xx,
at leading order, which can be integrated to give
c1(x, t) = ˙c0(t)
2D x2 + α±
1 (t)x + β±
1 (t).
(12.136)
The distinction between α+
1 and α−
1 , and similarly for β±
1 , is to account for dif-
ferences in the solution for x > 0 and x < 0, introduced by the linear terms. As
|x| →∞, c1 grows quadratically, which causes a nonuniformity in the expansion,
speciﬁcally when x = O
#
c0(t)/|˙c0(t)|

. In order to deal with this, we introduce a
scaled variable, η = x/
#
c0(t)/|˙c0(t)|, with η = O(1) for t ≫1 in this outer region.
In order to match with the solution in the inner region, where x = O(1), we need
c(η, t) →c0(t) as η →0.
(12.137)
In terms of η, (12.128) becomes
∂c
∂t −1
2η |˙c0(t)|
c0(t)
d
dt
 c0(t)
|˙c0(t)|
 ∂c
∂η = |˙c0(t)|
c0(t) D ∂2c
∂η2 .
(12.138)
Motivated by the matching condition (12.137), we will try to solve this using the
expansion c = c0(t)F±(η) + o(c0(t)), subject to F± →1 as η →0± and F± →0
as η →±∞. The superscript ± indicates whether the solution is for η > 0 or
η < 0. It is straightforward to substitute this into (12.138), but this leads to some
options. The ﬁrst and third terms are of O(˙c0(t)), whilst the second term is of
O(˙c0(t) d
dt

c0(t)
| ˙c0(t)|

. Should we include the second term in the leading order balance
or not? Let’s see what happens if we do decide to balance these terms to get the
richest limit. We must then have c0(t)/˙c0(t) = O(t), and hence c0 = Ct−α for some
constants C and α. This looks like a sensible gauge function.
If we proceed, (12.138) becomes, at leading order,
F± −1
2ηF ′
± = DF ′′
±.
(12.139)

356
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
This is slightly more diﬃcult to solve than (12.133). However, if we look for a
quadratic solution, we quickly ﬁnd that F± = η2 + 2D is a solution. Using the
method of reduction of order, the general solution of (12.139) is
F± = (η2 + 2D)

A± + B±
 η
0
e−s2/2D
(s2 + 2D)2 ds

.
(12.140)
As η →0, by Taylor expanding the integrand, we can show that
F± = (η2 + 2D)

A± + B± η
2D

+ O(η3).
To match with the inner solution, we therefore require that A± = 1/2D.
As
η →±∞, using integration by parts, we ﬁnd that
F± = (η2 + 2D)

A± ± B±
 ∞
0
e−s2/2D
(s2 + 2D)2 ds + O
 1
η5 e−η2/4D

,
so that we require
A± ± B±
 ∞
0
e−s2/2D
(s2 + 2D)2 ds = 0.
The outer solution can therefore be written as
F± =

1 + η2
2D
 
1 ∓
 η
0
e−s2/2D
(s2 + 2D)2 ds
5  ∞
0
e−s2/2D
(s2 + 2D)2 ds

.
(12.141)
In order to determine α, and hence the size of c0(t), some further work is needed.
Firstly, we can integrate (12.128) and apply the initial condition, to obtain
 ∞
−∞
c(x, t) dx =
 0
−∞
f0(x) dx = ftot.
(12.142)
This just says that mass is conserved during the diﬀusion process. Secondly, we
can write down the composite expansion
c = cinner + couter −(cinner)outer = c0(t)F±(η),
and use this in (12.142) to obtain
 ∞
−∞
c0(t)F±(η) dx = c0(t)
+
c0(t)
|˙c0(t)|
 ∞
−∞
F±(η) dη = ftot.
This is now a diﬀerential equation for c0 in the form
c3/2
0
(t)
|˙c0(t)|1/2 =
ftot
' ∞
−∞F±(η) dη .
In Exercise 12.18 we ﬁnd that
' ∞
−∞F±(η) dη =
√
2πD and hence that c0(t) =

12.3 PARTIAL DIFFERENTIAL EQUATIONS
357
ftot/
√
4πDt. We can now calculate that
#
c0(t)/|˙c0(t)| = O(t1/2), which is the
usual diﬀusive length scale. Our asymptotic solution therefore takes the form
c(x, t) ∼





ftot
√
4πDt
for |x| = O(t1/2),
ftot
√
4πDt
F±(η)
for |x| ≫t1/2.
(12.143)
The success of this approach justiﬁes our decision to choose c0(t) in order to obtain
the richest distinguished limit. Notice that the large time solution has “forgotten”
the precise details of the initial conditions. It only “remembers” the area under the
initial data, at leading order.
If we consider the particular case f0(t) = ex, we ﬁnd (see Exercise 12.18) that
an exact solution is available, namely c(x, t) = 1
2ex+Dterfc

x
√
4Dt +
√
Dt

. This
solution is plotted in Figure 12.16 at various times, and we can clearly see the
structures that our asymptotic solutions predict emerging for both small and large
times. In Figure 12.17 we plot c(0, t) as a function of Dt. Our asymptotic solution
predicts that c(0, t) =
1
2 + o(1) for t ≪1, consistent with Figure 12.17(a).
In
Figure 12.17(b) we can see that the asymptotic solution, c(0, t) ∼1/
√
4πDt as
t →∞, is in excellent agreement with the exact solution.
Example 3: The wave equation with weak damping
(i) Linear damping
Consider the equation
∂2y
∂t2 = c2 ∂2y
∂x2 −ϵ∂y
∂t ,
for t > 0 and −∞< x < ∞,
(12.144)
subject to the initial conditions
y(x, 0) = Y0(x),
∂y
∂t (x, 0) = 0,
(12.145)
with ϵ ≪1. The one-dimensional wave equation, (12.144) with ϵ = 0, governs the
small amplitude motion of an elastic string, which we met in Section 3.9.1. The
additional term, ϵyt, represents a weak, linear damping, proportional to the velocity
of the string, for example due to drag on the string as it moves through the air.
The form of the initial conditions suggests that we should consider an asymptotic
expansion y = y0 + ϵy1 + O(ϵ2). On substituting this into (12.144) and (12.145) we
obtain
∂2y0
∂t2 −c2 ∂2y0
∂x2 = 0,
subject to y0(x, 0) = Y0(x), y0t(x, 0) = 0,
(12.146)
∂2y1
∂t2 −c2 ∂2y1
∂x2 = −∂y0
∂t ,
subject to y1(x, 0) = y1t(x, 0) = 0.
(12.147)
The initial value problem given by (12.146) is one that we studied in Section 3.9.1,
and has d’Alembert’s solution, (3.43),
y0(x, t) = 1
2 {Y0(x −ct) + Y0(x + ct)} .
(12.148)

358
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.16. The solution of the diﬀusion equation with f0(x) = ex at various times.
This solution represents the sum of two waves, one travelling to the left and one
travelling to the right, each with speed c, without change of form, and with half
the initial amplitude. This is illustrated in Figure 12.18 for the initial condition
Y0(x) = 1/(1 + x2). The splitting of the initial proﬁle into left- and right-travelling
waves is clearly visible.
In terms of the characteristic variables, ξ = x −ct, η = x + ct, (12.147) becomes
−4c2 ∂2y1
∂ξ∂η = c
∂y0
∂ξ −∂y0
∂η

= 1
2c {Y ′
0(ξ) −Y ′
0(η)} .
Integrating this expression twice gives the solution
y1 = 1
8c {ξY0(η) −ηY0(ξ)} + F1(ξ) + G1(η).
(12.149)
The initial conditions show that
F1(x) + G1(x) = 0,
and
F ′
1(x) −G′
1(x) = 1
4c {xY ′
0(x) −Y0(x)} ,

12.3 PARTIAL DIFFERENTIAL EQUATIONS
359
Fig. 12.17. The solution of the diﬀusion equation with f0(x) = ex at x = 0.
which can be integrated once to give
F1(x) −G1(x) = 1
4c

xY0(x) −2
 x
0
Y0(s) ds
	
+ b.
Finally,
F1(x) = −G1(x) = 1
8c

xY0(x) −2
 x
0
Y0(s) ds
	
+ 1
2b,
which, in conjunction with (12.149), shows that
y1 = −1
4t {Y0(x + ct) + Y0(x −ct)} + 1
4c
 x+ct
x−ct
Y0(s) ds.
(12.150)
We can now see that y1 = O(t) for t ≫1, and therefore that our asymptotic
expansion becomes nonuniform when t = O(ϵ−1).
We will proceed using the method of multiple scales, deﬁning a slow time scale
T = ϵt, and looking for a solution y = y(x, t, T). In terms of these new independent
variables, (12.144) becomes
ytt + 2ϵytT + ϵ2yT T = c2yxx −ϵyt −ϵ2yT .
(12.151)

360
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
Fig. 12.18. The solution of (12.144) when c = 1 and Y0(x) = 1/(1 + x2) at equal time
intervals, t = 0, 4, 8, 12, 16, 20, when ϵ = 0 and 0.1.
If we now seek an asymptotic solution of the form y = y0(x, t, T) + ϵy1(x, t, T) +
O(ϵ2), at leading order we obtain (12.146), as before, but now the solution is
y0(x, t, T) = F0(ξ, T) + G0(η, T),
(12.152)
with
F0(ξ, 0) = 1
2Y0(ξ),
G0(η, 0) = 1
2Y0(η).
(12.153)
As usual in the method of multiple scales, we need to go to O(ϵ) to determine F0
and G0. We ﬁnd that
−4cy1ξη = F0ξ −G0η + 2 (F0ξT −G0ηT ) .
(12.154)
On solving this equation, the presence of the terms of the right hand side causes y1
to grow linearly with t. In order to eliminate them, we must have
F0ξT = −1
2F0ξ,
G0ηT = −1
2G0η.
(12.155)

12.3 PARTIAL DIFFERENTIAL EQUATIONS
361
If we solve these equations subject to the initial conditions (12.153), we obtain
F0 = 1
2e−T Y0(ξ),
G0 = 1
2e−T Y0(η),
and hence
y0 = 1
2e−ϵt/2 {Y0(x −ct) + Y0(x + ct)} .
(12.156)
This shows that the small term, ϵyt, in (12.144) leads to an exponential decay of
the amplitude of the solution over the slow time scale, t = O(ϵ−1), consistent with
our interpretation of this as a damping term. Figure 12.18 shows how this slow
exponential decay aﬀects the solution.
(ii) Nonlinear damping
What happens if we replace the linear damping term ϵyt with a nonlinear damping
term, ϵ(yt)3? We must then solve
∂2y
∂t2 = c2 ∂2y
∂x2 −ϵ
∂y
∂t
3
,
for t > 0 and −∞< x < ∞,
(12.157)
subject to the initial conditions
y(x, 0) = Y0(x),
∂y
∂t (x, 0) = 0.
(12.158)
We would again expect a nonuniformity when t = O(ϵ−1), so let’s go straight to a
multiple scales expansion, y = y0(x, t, T) + ϵy1(x, t, T). At leading order, as before,
we have (12.152) and (12.153). At O(ϵ),
−4cy1ξη = c2(F0ξ −G0η)3 + 2 (F0ξT −G0ηT ) .
(12.159)
In order to see clearly which terms are secular, we integrate the expression (F0ξ −
G0η)3 twice to obtain
η
 ξ
0
F 3
0ξ(s) ds −3G0(η)
 ξ
0
F 2
0ξ(s) ds + 3F0(ξ)
 η
0
G2
0ξ(s) ds −ξ
 η
0
G3
0η(s) ds.
Assuming that F0(s) and G0(s) are integrable as s →±∞, we can see that the
terms that become unbounded as ξ and η become large are those associated with
F 3
0ξ and G3
0η. We conclude that, to eliminate secular terms in (12.159), we need
F0ξT = −1
2c2F 3
0ξ,
G0ηT = −1
2c2G3
0η,
to be solved subject to (12.153). The solutions are
F0ξ =
Y ′
0(ξ)
$
4 + c2T {Y ′
0(ξ)}2 ,
G0η =
Y ′
0(η)
$
4 + c2T {Y ′
0(η)}2 ,
and hence
y0 = −


 ∞
ξ
Y ′
0(s)
$
4 + c2T {Y ′
0(s)}2 ds +
 ∞
η
Y ′
0(s)
$
4 + c2T {Y ′
0(s)}2 ds

.
(12.160)

362
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
In general, these integrals cannot be determined analytically, but we can see that
the amplitudes of the waves do decay as T increases, and are of O(T −1/2) for T ≫1.
Example 4: The measurement of oil fractions using local electrical probes
As we remarked at the beginning of the book, many problems that arise in engi-
neering are susceptible to mathematical modelling. We can break the modelling
process down into separate steps.
(i) Identify the important physical processes that are involved.
(ii) Write down the governing equations and boundary conditions.
(iii) Deﬁne dimensionless variables and identify dimensionless constants.
(iv) Solve the governing equations using either a numerical method or an asymp-
totic method.
Note that, although it is possible that we can ﬁnd an analytical solution, this is
highly unlikely when studying real world problems. As we discussed at the start
of Chapter 11, when one or more of the dimensionless parameters is small, we can
use an asymptotic solution technique. Let’s now discuss an example of this type of
situation.
For obvious reasons, oil companies are interested in how much oil is coming out
of their oilwells, and often want to make this measurement at the point where oil
is entering the well as droplets, rather than at the surface. One tool that can be
lowered into a producing oilwell to assist with this task is a local probe. This is
a device with a tip that senses whether it is in oil or water. The output from the
probe can be time-averaged to give the local oil fraction at the tip, and an array
of probes deployed to give a measurement of how the oil fraction varies across the
well. We will consider a simple device that distinguishes between oil and water by
measuring electrical conductivity, which is several orders of magnitude higher in
saline water than in oil.
The geometry of the electrical probe, which is made from sharpening the tip
of a coaxial cable like a pencil, is shown in Figure 12.19. A voltage is applied to
the core of the probe, whilst the outer layer, or cladding, is earthed. A measure-
ment of the current between the core and the cladding is then made to determine
the conductivity of the surrounding medium. Although this measurement gives a
straightforward way of distinguishing between oil and water when only one liquid
is present, for example when dipping the probe into a beaker containing a single
liquid, the diﬃculty lies in interpreting the change in conductivity as a droplet of
oil approaches, meets, deforms around and is penetrated by the probe. If we want
to understand and model this process, there is clearly a diﬃcult ﬂuid mechanical
problem to be solved before we can begin to relate the conﬁguration of the oil
droplet to the current through the probe (see Billingham and King, 1995). We will
pre-empt all of this ﬂuid mechanical complexity by considering what happens if, in
the course of the interaction of an oil droplet with a probe, a thin layer of oil forms
on the surface of the probe. How thin must this oil layer become before the current
through the probe is eﬀectively equal to that of a probe in pure water?

12.3 PARTIAL DIFFERENTIAL EQUATIONS
363
Fig. 12.19. A cross-section through an axisymmetric electrical probe.
In order to answer this question, we must solve a problem in electrostatics,
since the speed at which oil–water interfaces move is much less than the speed at
which electromagnetic disturbances travel (the speed of light)†. The electrostatic
potential, φ, is an axisymmetric solution of Laplace’s equation,
∇2φ = 0.
(12.161)
We will assume that the conducting parts of the probe are perfect conductors,
so that
φ =
 1
at the surface of the core,
0
at the earthed surface of the cladding.
(12.162)
At interfaces between diﬀerent media, for example oil and water or oil and insulator,
we have the jump conditions
[φ] = 0,

σ ∂φ
∂n

= 0.
(12.163)
Square brackets indicate the change in the enclosed quantity across an interface, σ is
the conductivity, which is diﬀerent in each medium (oil, water and insulator), and
∂/∂n is the derivative in the direction normal to the interface. Equation (12.163)
represents continuity of potential and continuity of current at an interface.
To
complete the problem, we have the far ﬁeld conditions that
φ →0 as r2 + z2 →∞outside the probe,
(12.164)
and
φ ∼φ∞(r) as z →∞for r0 < r < r1,
(12.165)
using cylindrical polar coordinates coaxial with the probe, and r = 0 at the tip. Here
r0 and r1 are the inner and outer radii of the insulator, as shown in Figure 12.19.
† This, in itself, is an asymptotic approximation that can be made rigorous by deﬁning a small
parameter, the ratio of a typical ﬂuid speed to the speed of light. Some approximations are,
however, so obvious that justifying them rigorously is a little too pedantic.
For a simple
introduction to electromagnetism, see Billingham and King (2001).

364
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
The far ﬁeld potential must satisfy
∇2φ∞(r) = d 2φ∞
dr2
+ 1
r
dφ∞
dr
= 0,
for r0 < r < r1,
subject to
φ = 1 at r = r0, φ = 0 at r = r1.
This has solution
φ∞= log(r1/r)
log(r1/r0).
(12.166)
Finally, we will assume that the probe is surrounded by water, except for a uniform
layer of oil on its surface of thickness h ≪r0. Our task is to solve the boundary
value problem given by (12.161) to (12.165).
This is an example of a problem where the governing equation and boundary
conditions are fairly straightforward, but the geometry is complicated. Problems
like this are usually best solved numerically. However, in this case we have one
region where the aspect ratio is small – the thin oil ﬁlm. The potential is likely
to change rapidly across this ﬁlm compared with its variation elsewhere, and a
numerical method will have diﬃculty handling this. We can, however, make some
progress by looking for an asymptotic solution in the thin ﬁlm. The ﬁrst thing
to do is to set up a local coordinate system in the oil ﬁlm. The quantities h and
r0 are the natural length scales with which to measure displacements across and
along the ﬁlm, so we let η measure displacement across the ﬁlm, with η = 0 at
the surface of the probe and η = 1 at the surface of the water, and let ξ measure
displacement along the ﬁlm, with ξ = 0 at the probe tip and ξ = 1 a distance
r0 from the tip. Away from the tip and the edge of the probe, which we will not
consider for the moment, this provides us with an orthogonal coordinate system,
and (12.161) becomes
∂2φ
∂η2 + δ2 ∂2φ
∂ξ2 = 0,
where
δ = h
r0
≪1.
At leading order, ∂2φ/∂η2 = 0, and hence φ varies linearly across the ﬁlm, with
φ = A(ξ)η + B(ξ).
(12.167)
Turning our attention now to (12.163)2, since we expect variations of φ in the water
and the insulator to take place over the geometrical length scale r0, we have
δj
∂φ
∂η = δ ∂φ
∂n at interfaces,
(12.168)
where
δj = σo
σj
,

12.3 PARTIAL DIFFERENTIAL EQUATIONS
365
with the subscripts o, w and i indicating oil, water and insulator respectively.
We expect that δw ≪1 and δi = O(1), since oil and the insulator have similar
conductivities, both much less than that of saline water. We conclude that, at the
interface between oil and insulator, at leading order ∂φ/∂η = 0, and hence A(ξ) = 0
there. Also, from the conditions at the surface of the cladding and core, (12.162),
B(ξ) = 0 at the cladding and B(ξ) = 1 at the core.
Returning now to (12.168) with j = w, note that we have two small parameters,
δ and δw. Double limiting processes like this (δ →0, δw →0) have to be treated
with care, as the ﬁnal result usually depends on how fast one parameter tends
to zero compared with the other. In this case, we obtain the richest asymptotic
balance by assuming that
δ
δw
= K = hσw
r0σo
= O(1) as δ →0,
and
∂φ
∂n = K ∂φ
∂η = KA(ξ).
We can now combine all of the information that we have, to show that at the
surface of the probe, the potential in the water satisﬁes
∂φ
∂n =



K (φ −1)
at the surface of the core,
0
at the surface of the insulator,
Kφ
at the surface of the cladding.
(12.169)
The fact that the oil ﬁlm is thin allows us to apply these conditions at the surface
of the probe at leading order. The key point is that this asymptotic analysis allows
us to eliminate the thin ﬁlm from the geometry of the problem at leading order,
and instead include its eﬀect in the boundary conditions (12.169). The solution of
(12.161) subject to (12.164), (12.165) and (12.169) in the region outside the probe
is geometrically simple, and easily achieved using a computer. We will not show
how to do this here, as it is outside the scope of this book. We can, however, extract
one vital piece of information from our analysis. We have proceeded on the basis
that K = O(1). What happens if K ≫1 or K ≪1? If K ≫1, at leading order
(12.169) becomes
φ =
 1
at the surface of the core,
0
at the surface of the cladding,
∂φ
∂n = 0 at the surface of the insulator.
(12.170)
These are precisely the boundary conditions that would apply at leading order in
the absence of an oil layer. We conclude that if K ≫1, and hence h ≪r0σo/σw,
the ﬁlm of oil is too thin to prevent a current from passing from core to cladding
through the water, and the oil cannot be detected by the probe. If K ≪1, at
leading order (12.169) becomes ∂φ/∂n = 0 at the surface of the probe, and hence
φ = 0 in the water. This then shows that φ = 1 −η in the oil ﬁlm over the core
and φ = 0 in the rest of the ﬁlm, from which it is straightforward to calculate

366
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
the current ﬂowing from core to cladding. In this case the oil ﬁlm dominates the
response of the probe, eﬀectively insulating it from the water outside.
We conclude that there is a critical oil ﬁlm thickness, hc = r0σo/σw. For h ≪hc
the oil ﬁlm is eﬀectively invisible, for h ≫hc the external ﬂuid is eﬀectively invisible,
and the current through the probe is determined by the thickness of the ﬁlm, whilst
for h = O(hc) both the oil and water aﬀect the current through the boundary
conditions (12.169). For a typical oil and saline water, hc ≈10−9 m. This is such a
small length that, in practice, any thin oil ﬁlm coating a probe insulates it from the
external ﬂuid, and can lead to practical diﬃculties with this technique. In reality,
local probes are used with alternating rather than direct current driving the core.
One helpful eﬀect of this is to increase the value of hc, due to the way that the
impedances† of oil and water change with the frequency of the driving potential.
Exercises
12.1
Determine the ﬁrst two terms in the asymptotic expansion for 0 < ϵ ≪1
of all the roots of each of the equations
(a) x3 + ϵx2 −x + ϵ = 0,
(b) ϵx3 + x2 −1 = 0,
(c) ϵx4 + (1 −3ϵ)x3 −(1 + 3ϵ)x2 −(1 + ϵ)x + 1 = 0,
(d) ϵx4 + (1 −3ϵ)x3 −(1 −3ϵ)x2 −(1 + ϵ)x + 1 = 0.
In each case, sketch the left hand side of the equation for ϵ = 0 and ϵ ≪1.
12.2
The function y(x) satisﬁes the ordinary diﬀerential equation
ϵy′′ + (4 + x2)(y′ + 2y) = 0,
for 0 ⩽x ⩽1,
subject to y(0) = 0 and y(1) = 1, with ϵ ≪1. Show that a boundary layer is
possible only at x = 0. Use the method of matched asymptotic expansions
to determine two-term inner and outer expansions, which you should match
using either Van Dyke’s matching principle, or an intermediate variable.
Hence show that
y′(0) ∼4e2
ϵ
−4e2 + 8e2 tan−1
1
2

as ϵ →0.
Construct a composite expansion, valid up to O(ϵ).
12.3
Determine the leading order outer and inner approximations to the solution
of
ϵy′′ + x1/2y′ + y = 0 for 0 ⩽x ⩽1,
subject to y(0) = 0 and y(1) = 1, when ϵ ≪1. Hence show that
y′(0) ∼ϵ−2/3
e2
Γ
 2
3

3
2
1/3
.
† the a.c. equivalents of the conductivities.

EXERCISES
367
12.4
The function y(x) satisﬁes the ordinary diﬀerential equation
ϵy′′ + (1 + x)y′ −y + 1 = 0,
for 0 ⩽x ⩽1, subject to the boundary conditions y(0) = y(1) = 0, with
ϵ ≪1, where a prime denotes d/dx. Determine a two-term inner expansion
and a one-term outer expansion. Match the expansions using either Van
Dyke’s matching principle or an intermediate region. Hence show that
y′(0) ∼1
2ϵ + 1 as ϵ →0.
12.5
Consider the boundary value problem
ϵ(2y + y′′) + 2xy′ −4x2 = 0 for −1 ⩽x ⩽2,
subject to
y(−1) = 2,
y(2) = 7,
with ϵ ≪1. Show that it is not possible to have a boundary layer at either
x = −1 or x = 2. Determine the rescaling needed for an interior layer at
x = 0. Find the leading order outer solution away from this interior layer,
and the leading order inner solution. Match these two solutions, and hence
show that y(0) ∼2 as ϵ →0. Sketch the leading order solution.
Now determine the outer solutions up to O(ϵ). Show that a term of
O(ϵ log ϵ) is required in the inner expansion. Match the two-term inner
and outer expansions, and hence show that y(0) = 2 −3
2ϵ log ϵ + O(ϵ) for
ϵ ≪1.
12.6
Consider the ordinary diﬀerential equation
ϵy′′ + yy′ −y = 0 for 0 ⩽x ⩽1,
subject to y(0) = α, y(1) = β, with α and β constants, and ϵ ≪1.
(a) Assuming that there is a boundary layer at x = 0, determine the
leading order inner and outer solutions when α = 0 and β = 3.
(b) Assuming that there is an interior layer at x = x0, determine the
leading order inner and outer solutions, and hence show that x0 =
1/2 when α = −1 and β = 1.
12.7
Use the method of multiple scales to determine the leading order solution,
uniformly valid for t ≪ϵ−2, of
d 2y
dt2 + y = ϵy3
dy
dt
2
,
subject to y = 1, dy/dt = 0 when t = 0, for ϵ ≪1.
12.8
Consider the ordinary diﬀerential equation
¨y + ϵ ˙y + y + ϵ2y cos2 t = 0,
for t ⩾0, subject to y(0) = 1, ˙y(0) = 0, where a dot denotes d/dt. Use the
method of multiple scales to determine a two-term asymptotic expansion,
uniformly valid for all t ≪ϵ−3 when ϵ ≪1.

368
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
12.9
Consider the ordinary diﬀerential equation
¨y + ϵ ˙y + y + ϵ2y3 = 0,
for t ⩾0, subject to y(0) = 1, ˙y(0) = 0, where a dot denotes d/dt. Use the
method of multiple scales, with
T = ϵt,
τ = t + ϵ2at,
y ≡y(τ, T),
to show that
y ∼e−T/2 cos τ for ϵ ≪1.
Show further that a = −1/8 and determine the next term in the asymptotic
expansion. (You will need to consider the ﬁrst three terms in the asymptotic
expansion for y.)
12.10
Show that when v0 ≪1, (12.81) becomes dE/dT = −2E at leading order.
12.11
Consider the initial value problem
d 2y
dt2 −2ϵdy
dt −y + y3 = 0,
(E12.1)
subject to
y(0) = yi > 1,
dy
dt (0) = 0.
(E12.2)
Use a graphical argument to show that when ϵ = 0, y is positive, provided
that yi <
√
2. Use Kuzmak’s method to determine the leading order ap-
proximation to the solution when 0 < ϵ ≪1 and 1 < yi <
√
2. You should
check that your solution is consistent with the linearized solution when
yi −1 ≪1. Hence show that y ﬁrst becomes negative when t = t0 ∼T0/ϵ,
where
T0 =
 0
1
2 y2
i(y2
i −2)
3K
 1
k

4
√
1 + 2E
√
1 + 2E + 1

×
1

(2k2 −1) L
 1
k

−2 (k2 −1) K
 1
k
dE,
k ≡k(E) =
√
1 + 2E + 1
2
√
1 + 2E
1/2
.
Hints:
(a) The leading order solution can be written in terms of the Jacobian
elliptic function cn (see Section 9.4).
(b)
 1
$
1−1
k2
#
1 −x2#
1 −k2 + k2x2 dx
= 1
3k

2k2 −1

L
1
k

−2

k2 −1

K
1
k
	
.

EXERCISES
369
12.12
Show that when D(x, ˆx) = D0(x) ˆD(ˆx) and R(θ(x), x, ˆx) = R0(θ(x), x) ˆR(ˆx),
the homogenized diﬀusion coeﬃcient and reaction term given by (12.101)
and (12.102) can be written in the simple form described in Section 12.2.6.
12.13
Give a physical example that would give rise to the initial–boundary value
problem
∂θ
∂t = ∂
∂x

D

x, x
ϵ
 ∂θ
∂x

for 0 < x < 1,
subject to
θ(x, 0) = θi(x) for 0 ⩽x ⩽1,
∂θ
∂x = 0 at x = 0 and x = 1 for t > 0.
When 0 < ϵ ≪1, use homogenization theory to show that, at leading
order, θ is a function of x and t only, and satisﬁes an equation of the form
F(x)∂θ
∂t = ∂
∂x

¯D(x) ∂θ
∂x

,
where ¯D(x) is given by (12.101), and F(x) is a function that you should
determine. If D(x, x/ϵ) = D0(x) ˆD(x/ϵ), show that, at leading order, θ
satisﬁes the diﬀusion equation
∂θ
∂ˆt = ∂
∂x

D0(x) ∂θ
∂x

,
where
ˆt =
t
limϵ→0

2ϵ2 ' 1/ϵ
0
s
ˆ
D(s)ds
.
12.14
Use the WKB method to ﬁnd the eigensolutions of the diﬀerential equation
y′′(x) + (λ −x2)y(x) = 0,
subject to y →0 as |x| →∞, when λ ≫1.
12.15
Find the ﬁrst two terms in the WKB approximation to the solution of the
fourth order equation
ϵy′′′′(x) = {1 −ϵV (x)} y(x)
that satisﬁes V (±∞) = 0 and y(±∞) = 0, when ϵ ≪1.
12.16
Solve the connection problem
ϵ2y′′(x) + cx2y(x) = 0,
subject to y(0) = 1 and y →0 as x →−∞, when ϵ ≪1.
12.17
By determining the next term in the WKB expansion, verify that the over-
lap domain for the inner and outer solutions of the boundary value problem
(12.115) is ϵ2/3 ≪|x| ≪ϵ2/5 (see the footnote just after (12.121)).

370
ASYMPTOTIC METHODS: DIFFERENTIAL EQUATIONS
12.18
Use Fourier transforms to solve (12.128) subject to (12.129) with f0(x) =
ex, and hence show that c(x, t) =
1
2ex+Dterfc

x
√
4Dt +
√
Dt

.
Use this
expression to show that c(0, t) ∼1/
√
4πDt as t →∞.
By comparing
this result with the large time asymptotic solution that we derived in Sec-
tion 12.3, show that
' ∞
−∞F±(η) dη =
√
2πD.
12.19
Find the small time solution of the reaction–diﬀusion equation
ut = Duxx + αu,
subject to the initial condition
u(x, 0) =
 f0(x)
for |x| < α,
0
for |x| ⩾α.
What feature of the solution arises as a result of the reaction term, αu?
12.20
Find the leading order solution of the partial diﬀerential equation
ct = (D + ϵx)2cxx for x > 0, t > 0,
when ϵ ≪1, subject to the initial condition c(x, 0) = f(x) and the bound-
ary condition c(0, t) = 0. Your solution should remain valid for large x and
t.
12.21
Find a uniformly valid solution of the hyperbolic equation
ϵ(ut + ux) + (t −1)2u = 1 for −∞< x < ∞, t > 0,
when ϵ ≪1, subject to the initial condition u(x, 0) = 0.
12.22
Find the leading order asymptotic solution of
ϵ(uxx + uyy) + ux + βuy = 0 for x > 0, 0 < y < L,
when ϵ ≪1, subject to the boundary conditions u(x, 0) = f(x), u(0, y) =
g(y) and u(x, L) = 0.
Your solution should be uniformly valid in the
domain of solution, so you will need to resolve any boundary layers that
are required.
12.23
Find a uniformly valid leading order asymptotic solution of
ϵ(utt −c2uxx) + ut + αux = 0 for −∞< x < ∞, t > 0,
when ϵ ≪1, subject to the initial conditions u(x, 0) = f(x), ut(x, 0) = 0.
How does the solution for c > |α| diﬀer from the solution when c < |α|?
12.24
Project: The triple deck
Consider the innocuous looking two-point boundary value problem
ϵy′′ + x3y′ + (x3 −ϵ)y = 0,
subject to y(0) = 1
2, y(1) = 1,
(E12.3)
with ϵ ≪1.
(a) Show that the outer solution is y = e1−x.

EXERCISES
371
(b) Since the outer solution does not satisfy the boundary condition at
x = 0, there must be a boundary layer. By writing ¯x = x/ϵµ, show
that there are two possibilities, µ = 1 and µ = 1
2.
As there is now a danger of confusion in our notation, we will deﬁne
¯x = x/ϵ, and refer to the region where ¯x = O(1) as the lower deck,
and x∗= x/ϵ1/2, and refer to the region where x∗= O(1) as the
middle deck. We will refer to the region where x = O(1) as the
upper deck. This nomenclature comes from the study of boundary
layers in high Reynolds number ﬂuid ﬂow.
(c) Show that the leading order solution in the lower deck is ¯y = 1
2e−¯x,
and in the middle deck y∗= a exp

−1/2 (x∗)2
.
(d) Apply the simplest form of matching condition to show that a = e,
and hence that the solution in the middle deck is
y∗= exp

1 −
1
2 (x∗)2

.
(e) The matching between the lower and middle decks requires that
lim
¯x→∞¯y = lim
x∗→0 y∗,
which is satisﬁed automatically without ﬁxing any constants. Show
that the composite solution takes the form
y = e1−x + exp

1 −
1
2 (x∗)2

+ 1
2e−¯x −e.
(f) Integrate (E12.3) numerically using MATLAB, and compare the nu-
merical solution with the asymptotic solution for ϵ = 0.1, 0.01 and
0.001. Can you see the structure of the triple deck?
(g) The equations for steady, high Reynolds number ﬂow of a viscous
Newtonian ﬂuid with velocity u and pressure p are
∇· u = 0,
(u · ∇u) = −∇p + 1
Re∇2u,
where Re is the Reynolds number.
If these are to be solved in
−∞< x < ∞, y > f(x) subject to u = 0 on y = f(x) and u ∼Ui
as x2 + y2 →∞, where would you expect a triple or double deck
structure to appear?
You can ﬁnd some more references and background material in
Sobey (2000). From the data in this book, estimate the maximum
thickness of the boundary layer on a plate 1 m long, with water
ﬂowing past it at 10 m s−1.

CHAPTER THIRTEEN
Stability, Instability and Bifurcations
In this chapter, we will build upon the ideas that we introduced in Chapter 9,
which was concerned with phase plane methods for nonlinear, autonomous, ordinary
diﬀerential equations. We will study what happens when an equilibrium point has
one or more eigenvalues with zero real part. These are nonhyperbolic equilibrium
points, which we were unable to study by simple linearization in Chapter 9. Next,
we will introduce the idea of Lyapunov functions, and show how they can be used
to study the stability of nonhyperbolic equilibrium points. We will also consider
diﬀerential equations that contain a parameter, and examine what can happen to
the qualitative form of the solutions as the parameter varies. At parameter values
where one or more equilibrium points is nonhyperbolic, a local bifurcation point,
the qualitative nature of the solutions changes. Finally, we will look at an example
of a global bifurcation.
13.1
Zero Eigenvalues and the Centre Manifold Theorem
Let’s consider the structure of an equilibrium point which has one zero eigenvalue
and one negative eigenvalue. After shifting the equilibrium point to the origin,
writing the system in terms of coordinates with axes in the directions of the eigen-
vectors, and rescaling time so that the negative eigenvalue has unit magnitude, the
system will have the form
˙x = P(x, y),
˙y = −y + Q(x, y),
(13.1)
where P and Q are nonlinear functions with P(0, 0) = Q(0, 0) = 0. The linear
approximation to (13.1) is ˙x = 0, ˙y = −y, which has the solution x = 0, y = y0e−t
that passes through the origin. This shows that points on the y-axis close to the
origin approach the origin as t →∞. We say that the local stable manifold† in
some neighbourhood, U, of the origin is
ωs(0, U) = {(x, y) | x = 0,
(x, y) ∈U} .
This is the set of points that are, locally, attracted to the origin in the direction
of the eigenvector that corresponds to the negative eigenvalue. We are going to
have to work harder to determine the behaviour of the other integral paths near
the origin.
† We will carefully deﬁne what we mean by a manifold in Section 13.1.2.

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
373
In order to work out what is going on, we deﬁne a local centre manifold for
(13.1) to be a Ck curve ωc(0, U) in a neighbourhood U of the origin such that
(i) ωc(0, U) is an invariant set within U, so that if x(0) ∈ωc(0, U), then
x(t) ∈ωc(0, U) when x(t) ∈U. In other words, a solution that initially
lies on the centre manifold remains there when it lies in U.
(ii) ωc(0, U) is the graph of a Ck function that is tangent to the x-axis at the
origin, so that
ωc(0, U) =

(x, y) | y = h(x), h(0) = dh
dx(0) = 0,
(x, y) ∈U
	
.
This gives us the picture of the local stable and centre manifolds shown in Fig-
ure 13.1. The main idea now is that the qualitative behaviour of the solution in
x
y
ωc
ωs
Fig. 13.1. The local stable and centre manifolds of the system (13.1).
the neighbourhood of the origin, excluding the stable manifold, is determined by
the behaviour of the solution on the centre manifold. This means that the local
dynamics are governed by a ﬁrst order diﬀerential equation.
Theorem 13.1 (The centre manifold theorem) The equilibrium point at the
origin of the system (13.1) is stable/unstable if and only if the equilibrium point at
x = 0 of the ﬁrst order diﬀerential equation
˙x = P(x, h(x)),
(13.2)
where y = h(x) is the local centre manifold, is stable/unstable. Integral paths in the

374
STABILITY, INSTABILITY AND BIFURCATIONS
neighbourhood of the local centre manifold are attracted onto the centre manifold
exponentially fast.
Proof This can be found in Wiggins (1990).
13.1.1
Construction of the Centre Manifold
Now that we know that a local centre manifold exists, how can we determine its
equation, y = h(x)? Since ˙y = ˙xdh/dx, (13.2) shows that
dh
dxP(x, h(x)) = −h(x) + Q(x, h(x)),
(13.3)
subject to
h(0) = dh
dx(0) = 0.
(13.4)
We will not usually be able to solve (13.3) analytically, but we can proceed to
determine the local form of the centre manifold by assuming that h(x) can be
represented as a power series,
h(x) = a2x2 + a3x3 + · · · ,
(13.5)
which automatically satisﬁes (13.4).
As an example, let’s consider the system
˙x = ax3 + xy,
˙y = −y + y2 + x2y + x3
(13.6)
with a > 0. This is of the form (13.1), so we need to substitute (13.5) into (13.3).
This shows that
(2a2x + 3a3x2 + · · · )(ax3 + a2x3 + a3x4 + · · · )
= −a2x2 −a3x3 + (a2x2 + a3x3 + · · · )2 + x2(a2x2 + a3x3 + · · · ) + x3.
Equating powers of x gives us a2 = 0 at O(x2) and a3 = +1 at O(x3), so that
the centre manifold is given by y = h(x) = +x3 + · · · . On the centre manifold,
we therefore have ˙x = ax3 −x4 + · · · . For |x| ≪1 we can ignore the quartic term
and just consider ˙x ≈ax3, so that ˙x > 0 for x > 0 and ˙x < 0 for x < 0. Integral
paths that begin on the local centre manifold therefore asymptote to the origin as
t →−∞, and we conclude that the local phase portrait is of a nonlinear saddle,
as shown in Figure 13.2. More speciﬁcally, if x = x0 on the centre manifold when
t = 0, x ≈x0/
#
1 −2ax2
0t, so that x ∼x0/
#
−2ax2
0t as t →−∞. This algebraic
behaviour on the centre manifold is in contrast to the exponential behaviour that
occurs on the unstable separatrix of a linear saddle point.
Example: Travelling waves in cubic autocatalysis
If two chemicals, which we label A and B, react through a mechanism known as
cubic autocatalysis, we write
A + 2B →3B,
rate kab2,
(13.7)

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
375
x
y
ωc
ωs
Fig. 13.2. The local phase portrait for the system (13.6).
where k is the reaction rate constant and a and b are the concentrations of the
two chemicals, which are measured in moles m−3†. The chemical B is known as the
autocatalyst, since it catalyses its own production. The greater the concentration
of B, the faster it is produced by the reaction (13.7). If these two chemicals then
react in a long thin tube, so that their concentrations only vary in the x-direction,
the main physical processes that act, in the absence of any underlying ﬂuid ﬂow,
are chemical reaction and one-dimensional diﬀusion. We can derive the partial dif-
ferential equations that govern this situation using the arguments that we described
in Section 12.2.3, example 2. Speciﬁcally, since the rate of change of the amount
of each chemical in a control volume is equal to the total diﬀusive ﬂux through
the bounding surface plus the total rate of production of that chemical within the
volume, we ﬁnd that
∂a
∂t = D ∂2a
∂x2 −kab2,
∂b
∂t = D ∂2b
∂x2 + kab2,
(13.8)
where t is time and D the constant diﬀusivity of the chemicals. If a small amount
of the autocatalyst is introduced locally into a spatially uniform expanse of A with
a = a0, waves of chemical reaction will propagate away from the initiation site. We
can study one of these by looking for a wave that propagates from left to right at
constant speed v > 0, represented by a solution of the form a = a(y), b = b(y),
where y = x −vt. Such a solution is called a travelling wave solution.‡
With a and b functions of y alone, (13.8) become nonlinear ordinary diﬀerential
† A mole is a ﬁxed number (Avogadro’s number ≈6.02 × 1023) of molecules of the substance.
‡ For more background and details on reaction–diﬀusion equations and travelling wave solutions,
see Billingham and King (2001).

376
STABILITY, INSTABILITY AND BIFURCATIONS
equations,
Dd 2a
dy2 + v da
dy −kab2 = 0,
Dd 2b
dy2 + v db
dy + kab2 = 0,
(13.9)
to be solved subject to the boundary condition
a →a0,
b →0 as y →∞.
(13.10)
This represents the unreacted state ahead of the wave. If we now add equations
(13.9), we obtain a linear diﬀerential equation for a + b, which we can solve to
obtain
a + b = k0 + k1e−vy/D.
The boundary condition (13.10) then shows that k0 = a0. We also require that a+b
is bounded as y →−∞, so that k1 = 0, and hence a + b = a0. We can therefore
eliminate a from (13.9) and arrive at a second order system
db
dy = c,
D dc
dy = −vc −kb2 (a0 −b) ,
(13.11)
subject to
b →0,
c →0 as y →∞.
(13.12)
Note that, since we require that both of the chemical concentrations should be
positive for a physically meaningful solution, we need 0 ⩽b ⩽a0.
The only equilibrium points of (13.11) are b = c = 0 and b = a0, c = 0. The
second of these represents the fully reacted state behind the travelling wave, where
all of the chemical A has been converted into the autocatalyst, B, so we also require
that
b →a0,
c →0 as y →−∞.
(13.13)
We can write the boundary value problem given by (13.11) to (13.13) in a more
convenient form by deﬁning the dimensionless variables
β = b
a0
,
γ =
*
ka2
0
D
c
a0
z =
*
ka2
0
D y,
V =
v
#
ka2
0D
,
so that
dβ
dz = γ,
dγ
dz = −V γ −β2 (1 −β) ,
(13.14)
subject to
β →0,
γ →0 as z →∞,
(13.15)
β →1,
γ →0 as z →−∞,
(13.16)
and 0 ⩽β ⩽1 for a physically meaningful solution. The dimensionless wave speed,
V , is now the only parameter.
The next step is to study the solutions of (13.14) subject to (13.15) and (13.16) in
the (β, γ) phase plane and determine for what values of the wave speed, V , solutions

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
377
exist. As a preliminary, let’s see if we can guess a solution of this system. A plausible
functional form is γ = kβ(1 −β), which satisﬁes the boundary conditions. Since
dγ
dβ = −V −β2(1 −β)
γ
,
(13.17)
we ﬁnd that we can satisfy this equation with k = −1/
√
2 and V = 1/
√
2. This
gives
dβ
dz = −1
√
2β(1 −β),
and hence
β = βe =
1
1 + e(z−z0)/
√
2 ,
γ = γe = −1
√
2
e(z−z0)/
√
2
(1 + e(z−z0)/
√
2)2 .
(13.18)
This is an exact solution for V = 1/
√
2 and any constant z0, as shown in Fig-
ure 13.3. The presence of z0 in (13.18) simply shows that the solution can be given
an arbitrary displacement in the z-direction and remain a solution, as we would
expect for a wave that propagates at constant speed without change of form.
Fig. 13.3. The analytical solution of the cubic travelling wave problem, β = βe(z), with
z0 = 0.
So now we know that a solution exists for V = 1/
√
2. What about other values of
V ? Let’s go along our usual route, and determine the nature of the two equilibrium
points, P1 = (1, 0) and P2 = (0, 0). The Jacobian is
J =

0
1
−2β + 3β2
−V

.

378
STABILITY, INSTABILITY AND BIFURCATIONS
At P1, the eigenvalues are real and of opposite sign, so that P1 is a saddle point.
Boundary condition (13.16) shows that we need the integral path that represents
the solution to asymptote to P1 as z →−∞. Since the unstable separatrices of P1
are the only integral paths that do this, the solution must be represented by the
unstable separatrix of P1 that lies in the physically meaningful region, 0 ⩽β ⩽1,
shown in Figure 13.4 as S1. The other boundary condition, (13.15), shows that
we need S1 to asymptote to the other equilibrium point, P2, as z →∞, if it is to
represent a solution.
S1
P1
P2
S2
β
γ
Fig. 13.4. The local behaviour in the neighbourhood of the two equilibrium points.
At P2 = (0, 0), the eigenvalues are −V and zero, with associated eigenvectors
e−= (1, −V ) and e0 = (1, 0), respectively, so that this is a nonhyperbolic equi-
librium point. Since V > 0, there is a local stable manifold in the direction of e−
and also a local centre manifold tangent to the β-axis (the direction of e0). We
can construct a local approximation to the centre manifold, γ = h(β), by assuming
that γ ∼Aβ2 as β →0 for some constant A. The governing equations, (13.14),
then show that
dγ
dz ∼2Aβ dβ
dz ∼2Aβγ ∼2A2β3,
and hence that
2A2β3 ∼−V Aβ2 −β2(1 −β).
By equating coeﬃcients of β2, we ﬁnd that A = −1/V , and hence that the local
centre manifold has γ ∼−β2/V as β →0. This means that γ = dβ/dz < 0 on
the local centre manifold. Points on the centre manifold in β > 0 are therefore
attracted to P2 as z →∞with, from dβ/dz ∼−β2/V , β ∼V/z as z →∞. In
contrast, points on the centre manifold in β < 0 are swept away as z increases. This
type of behaviour is characteristic of a new type of equilibrium point, known as a
saddle–node. To the right of the stable manifold, the point behaves like a stable
node, attracting integral paths onto the centre manifold and into the origin, whilst
to the left of the stable manifold the point is unstable, as shown in Figure 13.4.

13.1 ZERO EIGENVALUES AND THE CENTRE MANIFOLD THEOREM
379
Apart from the local stable manifold, on which β = O(e−V z) as z →∞(recall that
the negative eigenvalue of P2 is −V ), any other integral paths that asymptote to
the origin as z →∞do so on the centre manifold, with β ∼V/z.
Finally, for any given value of V , we need to determine whether the integral path
S1 lies to the right of the stable manifold of P2, which we label S2, and therefore
enters the origin and represents a solution, or whether S1 lies to the left of S2, is
swept away from the origin into β < 0, and therefore does not represent a physically
meaningful solution. We will return to this problem in Section 13.3.3.
13.1.2
The Stable, Unstable and Centre Manifolds
We end this section by deﬁning more carefully what we mean by a manifold,
and generalizing the deﬁnitions of the stable, unstable and centre manifolds to nth
order systems.
Let’s begin with some deﬁnitions. A homeomorphism is a mapping f : L →N
that is one-to-one, onto and continuous and has a continuous inverse. Here, L and
N are subsets of Rn. A Ck diﬀeomorphism is a mapping f : L →N that is
one-to-one, onto and k times diﬀerentiable with a k-times diﬀerentiable inverse. A
smooth diﬀeomorphism is a C∞diﬀeomorphism and a homeomorphism is a C0
diﬀeomorphism. An m-dimensional manifold is a set M ⊂Rn for which each
x ∈M has a neighbourhood U in which there exists a homeomorphism φ : U →Rm,
where m ⩽n. A manifold is said to be diﬀerentiable if there is a diﬀeomorphism
rather than a homeomorphism φ : U →Rm.
For example, a smooth curve in
R3 is a one-dimensional diﬀerentiable manifold, and the surface of a sphere is a
two-dimensional diﬀerentiable manifold.
Now that we have deﬁned these ideas, let’s consider the behaviour of the solutions
of
dx
dt = f(x),
(13.19)
where x, f(x) ∈Rn. In the neighbourhood of an equilibrium point, ¯x, of (13.19),
there exist three invariant manifolds.
(i) The local stable manifold, ωs
loc, of dimension s, is spanned by the eigen-
vectors of A whose eigenvalues have real parts less than zero.
(ii) The local unstable manifold, ωu
loc, of dimension u, is spanned by the
eigenvectors of A whose eigenvalues have real parts greater than zero.
(iii) The local centre manifold, ωc
loc, of dimension c, is spanned by the eigen-
vectors of A whose eigenvalues have zero real parts.
Note that s + c + u = n. Solutions lying in ωs
loc are characterized by exponential
decay and those in ωu
loc by exponential growth. The behaviour on the centre mani-
fold is determined by the nonlinear terms in (13.19), as we described earlier in this
section. For a linear system these manifolds exist globally, whilst for a nonlinear
system they exist in some neighbourhood of the equilibrium point.

380
STABILITY, INSTABILITY AND BIFURCATIONS
Example
Let’s consider the simple system
˙x = x(2 −x),
˙y = −y + x,
(13.20)
and try to determine the equations of the local manifolds that pass through the
equilibrium point at the origin. The Jacobian at the origin is
J =
 2
0
1
−1

,
which has eigenvalues 2 and −1 and associated eigenvectors of (3, 1)T and (0, 1)T
respectively, so that ωu
loc is the line y = x/3 and ωs
loc is the y-axis. We can solve
the nonlinear system directly, since the equation for x is independent of y, and the
equation for y is linear. The solution is
x =
2A
A + 2e−t ,
y = e−t

B + 2et −4
A log(Aet + 2)
	
,
where A ̸= 0 and B are constants.
There is also the obvious solution x = 0,
y = Be−t, the y-axis, which gives the local stable manifold, ωs
loc, and also the
global stable manifold, ωs(¯x). Points that lie in the local unstable manifold,
ωu
loc, have y →0 as t →−∞. Since y ∼e−t (B −4 log 2/A) as t →−∞, we must
have B = A + 4/A log 2, so that the global unstable manifold, ωu(¯x), is given
in parametric form by

2A
A + 2e−t , e−t

A + 4
A log 2 + 2et −4
A log(Aet + 2)
	
.
The phase portrait is sketched in Figure 13.5.
x
y
ωu
ωs
ωu
ωs
Fig. 13.5. The phase portrait of the system (13.20).

13.2 LYAPUNOV’S THEOREMS
381
13.2
Lyapunov’s Theorems
Although we are now familiar with the idea of the stability of an equilibrium point
in an informal way, in order to develop the idea of a Lyapunov function, we need
to consider some more formal deﬁnitions of stability. Note that, in this section,
we will consider systems of autonomous ordinary diﬀerential equations, written in
vector form as ˙x = f(x), that have an isolated equilibrium point at the origin.
An equilibrium point, x = xe, is Lyapunov stable if for all ϵ > 0 there exists
a δ > 0 such that for all x(0) with |x(0) −xe| < δ, |x(t) −xe| < ϵ for all t > 0. In
other words, integral paths that start close to a Lyapunov stable equilibrium point
remain close for all time, as shown in Figure 13.6.
x0
|x − xe| < ε
∀ t > 0
xe
δ
ε
Fig. 13.6. A Lyapunov stable equilibrium point of a second order system.
An equilibrium point, x = xe, is asymptotically stable if there exists a δ > 0
such that for all x(0) with |x(0) −xe| < δ, |x(t) −xe| →0 as t →∞. This is
a stronger deﬁnition of stability than Lyapunov stability, and states that integral
paths that start suﬃciently close to an asymptotically stable equilibrium point are
attracted into it, as illustrated in Figure 13.7. Stable nodes and spirals are both
Lyapunov and asymptotically stable. It should be clear that asymptotically stable
equilibrium points are also Lyapunov stable. However, Lyapunov stable equilibrium
points, for example centres, are not necessarily asymptotically stable.
Now that we have formalized our notions of stability, we need one more new
concept. Let V : Ω→R, where Ω⊂Rn and 0 ∈Ω. We say that the function V is
positive deﬁnite on Ωif and only if V (0) = 0 and V (x) > 0 for x ∈Ω, x ̸= 0. For
example, for n = 3, V (x) = V (x1, x2, x3) = x2
1 + x2
2 + x2
3 is positive deﬁnite in R3,
whilst V (x1, x2, x3) = x2
2 is not, since it is zero on the plane x2 = 0, and not just
at the origin. Note that if −V (x) is a positive deﬁnite function, we say that V is a
negative deﬁnite function. In the following, we will assume that V is continuous

382
STABILITY, INSTABILITY AND BIFURCATIONS
xe
|x − xe| → 0
as t  →∞
x0
δ
Fig. 13.7. An asymptotically stable equilibrium point of a second order system.
and has well-deﬁned partial derivatives with respect to each of its arguments, so
that V ∈C1(Ω).
We can now introduce the idea of the derivative of a function V (x) with respect
to the system ˙x = f(x) = (f1, f2, . . . , fn), which is deﬁned to be the scalar product
V ∗(x) = ∇V · f(x) = ∂V
∂x1
f1(x) + · · · + ∂V
∂xn
fn(x).
This derivative can be calculated for given V (x) and f(x), without knowing the
solution of the diﬀerential equation. In particular,
dV
dt = ∂V
∂x1
˙x1 + · · · + ∂V
∂xn
˙xn = ∂V
∂x1
f1(x) + · · · + ∂V
∂xn
fn(x) = V ∗(x),
so that the total derivative of V with respect to the solution of the equations
coincides with our deﬁnition of the derivative with respect to the system. This
allows us to prove three important theorems.
Theorem 13.2 If, in some region Ω⊂Rn that contains the origin, there exists
a scalar function V (x) that is positive deﬁnite and for which V ∗(x) ⩽0, then the
origin is Lyapunov stable. The function V (x) is known as a Lyapunov function.
Proof
Since V is positive deﬁnite in Ω, there exists a sphere of radius r > 0†
contained within Ωsuch that
V (x) > 0 for x ̸= 0 and |x| < r,
V ∗(x) ⩽0 for |x| ⩽r.
† the set of points with |x| ⩽r in Rn.

13.2 LYAPUNOV’S THEOREMS
383
Let x = x(t) be the solution of the diﬀerential equation ˙x = f(x) with x(0) = x0.
By the local existence theorem, Theorem 8.1, extended to higher order systems,
this solution exists for 0 ⩽t < t∗with t∗> 0. This solution can then be continued
for t ⩾t∗, and we denote by t1 the largest value of t for which the solution exists.
There are two possibilities, either t1 = ∞or t1 < ∞. We now show that, for |x0|
suﬃciently small, t1 = ∞.
From the deﬁnition of the derivative with respect to a system,
dV
dt (x(t)) = V ∗(x(t)) for 0 ⩽t < t1.
We can integrate this equation to give
V (x(t)) −V (x0) =
 t
0
V ∗(x(s)) ds ⩽0,
since V ∗is negative deﬁnite. This means that 0 < V (x(t)) ⩽V (x0) for 0 ⩽t < t1.
Now let ϵ satisfy 0 < ϵ ⩽r, and let S be the closed, spherical shell with inner and
outer radii ϵ and r†. By continuity of V , and since S is closed, µ = minx∈S V (x)
exists and is strictly positive. Since V (x) →0 as |x| →0, we can choose δ with
0 < δ < µ such that for |x0| ⩽δ, V (x0) < µ, so that 0 < V (x(t)) ⩽V (x0) < µ
for 0 ⩽t < t1. Since µ is the minimum value of V in S, this gives |x(t)| < ϵ for
0 ⩽t < t1. If there exists t2 such that |x(t2)| = ϵ, then, when t = t2, we also
have, from the deﬁnition of µ, µ ⩽V (x(t2)) ⩽V (x0) < µ, which cannot hold. We
conclude that t1 = ∞, and that, for a given ϵ > 0, there exists a δ > 0 such that
when |x0| < δ, |x(t)| < ϵ for t ⩾0, and hence that the origin is Lyapunov stable.
The proofs of the following two theorems are rather similar, and we will not give
them here.
Theorem 13.3 If, in some region Ω⊂Rn that contains the origin, there exists
a scalar function V (x) that is positive deﬁnite and for which V ∗(x) is negative
deﬁnite, then the origin is asymptotically stable.
Theorem 13.4 If, in some region Ω⊂Rn that contains the origin, there exists a
scalar function V (x) such that V (0) = 0 and V ∗(x) is either positive deﬁnite or
negative deﬁnite, and if, in every neighbourhood N of the origin with N ⊂Ω, there
exists at least one point a such that V (a) has the same sign as V ∗(a), then the
origin is unstable.
Theorems 13.2 to 13.4 are known as Lyapunov’s theorems, and have a geometri-
cal interpretation that is particularly attractive for two-dimensional systems. The
equation V (x) = c then represents a surface in the (x, y, V )-space. By varying c
(through positive values only, since V is positive deﬁnite), we can obtain a series of
contour lines, with V = 0 at the origin a local minimum on the surface, as shown
in Figure 13.8. Since ˙x = f(x), the vector ﬁeld f represents the direction taken by
† the set of points with ϵ ⩽|x| ⩽r in Rn.

384
STABILITY, INSTABILITY AND BIFURCATIONS
an integral path at any point. The vector normal to the surface V (x) = c is ∇V ,
so that, if V ∗= ∇V · f ⩽0, integral paths cannot point into the exterior of the
region V (x) < c. We conclude that an integral path that starts suﬃciently close
to the origin, for example with V (x0) < c1, cannot leave the region bounded by
V (x) = c1, and hence that the origin is Lyapunov stable. Similarly, if V ∗< 0, the
integral paths must actually cross from the exterior to the interior of the region.
Hence V will decrease monotonically to zero from its initial value when the integral
path enters the region Ω, and we conclude that the origin is asymptotically stable.
V
V(x) = c2
V(x) = c2
y
x
y
x
V(x) = c1 < c2
V(x) = c1 < c2
f(x)
(b)
(a)
∇V
Fig. 13.8. (a) The local behaviour and (b) a contour plot of a Lyapunov function near the
origin.
Although we can now see why a Lyapunov function is useful, it can take consid-
erable ingenuity to actually construct one for a given system.
Example 1
Consider the system
˙x = −x −2y2,
˙y = xy −y3.
The origin is the only equilibrium point, and the linearized system is ˙x = −x, ˙y = 0.
The eigenvalues are therefore 0 and −1, so this is a nonhyperbolic equilibrium point.
Let’s try to construct a Lyapunov function. We start by trying V = x2 +αy2. This
is clearly positive deﬁnite for α > 0, and V (0, 0) = 0. In addition,
V ∗= dV
dt = 2x(−x −2y2) + 2αy(xy −y3) = −2x2 + 2(α −2)xy2 −2αy4.
If we choose α = 2, then dV/dt = −2x2 −4y4 < 0 for all x and y excluding
the origin.
From Theorems 13.2 and 13.3 we conclude that the origin is both
Lyapunov and asymptotically stable. As a general guideline, it is worth looking for
a homogeneous, algebraic Lyapunov function when f has a simple algebraic form.

13.2 LYAPUNOV’S THEOREMS
385
Example 2
Consider the second order diﬀerential equation ¨θ + f(θ) = 0 for −π ⩽θ ⩽π, with
θf(θ) positive, f diﬀerentiable and f(0) = 0. We can write this as a second order
system,
˙x1 = x2,
˙x2 = −f(x1),
where x1 = θ. The origin is an equilibrium point, but is it stable? In order to
construct a Lyapunov function, it is helpful to think in terms of an equivalent
physical system.
By analogy with the model for a simple pendulum, which we
discussed in Section 9.1, we can think of f(θ) as the restoring force and ˙θ as the
angular velocity.
The total energy of the system is the sum of the kinetic and
potential energies, which we can write as E =
1
2 ˙θ2 +
' θ
0 f(s) ds.
If this energy
were to decrease/not grow, we would expect the motionless, vertical state of the
pendulum to be asymptotically/Lyapunov stable. Guided by this physical insight,
we deﬁne
V = 1
2x2
2 +
 x1
0
f(s) ds.
Clearly V (0, 0) = 0, and, since
' x1
0
f(s) ds is positive by the assumption that
θf(θ) ⩾0, V is positive deﬁnite for −π ⩽x1 ⩽π. Finally,
V ∗= dV
dt = f(x1)x2 + x2 · −f(x1) = 0.
By Theorem 13.2, V is a Lyapunov function, and the origin is Lyapunov stable.
Example 3
Consider the diﬀerential equation ¨x+ ˙x+x+x2 = 0. We can write this as a second
order system,
˙x1 = x2,
˙x2 = −x1 −x2
1 −x2,
(13.21)
where x1 = x. This has two equilibrium points, at (−1, 0) and (0, 0). It is straight-
forward to determine the eigenvalues of these equilibrium points and show that both
are hyperbolic, with (−1, 0) a saddle point and (0, 0) a stable, clockwise spiral. We
can now construct a Lyapunov function that will give us some idea of the domain
of attraction of the stable equilibrium point at the origin. Consider the function
V = 1
2(x2
1 + x2
2) + 1
3x3
1.
This function vanishes at the origin and is positive in the region
Ω=

(x1, x2) | x2
2 > −x2
1 −2
3x3
1
	
,
which is sketched, along with the phase portrait, in Figure 13.9.
Let’s consider the curve V = 1
6, which passes through the saddle point and the
point ( 1
2, 0), as shown in Figure 13.10. If V = V0 < 1
6, we have a curve that encloses
the origin, but not the saddle point. By taking V = V0 < 1
6 arbitrarily close to
1
6, we can make the curve V = V0 arbitrarily close to the saddle point. As we are

386
STABILITY, INSTABILITY AND BIFURCATIONS
−2
−1.5
−1.5
−1
−0.5
1.5
1
0.5
0
−1
−0.5
0
x1
x2
0.5
1
Fig. 13.9. The region Ωand the phase portrait of the system (13.21). The region Ωlies
to the right of the curved, broken line, which is V = 0.
interested in the domain of attraction of the equilibrium point at the origin, we will
focus on Ω0, the subset of Ωgiven by V < V0 < 1
6, with V0 close to 1
6.
Since
V ∗= dV
dt = x2

x1 + x2
1

+

−x1 −x2
1 −x2

x2 = −x2
2 ⩽0,
we immediately have from Theorem 13.3 that the origin is Lyapunov stable. To
prove asymptotic stability requires more work, as V ∗= 0 on x2 = 0, which could
allow trajectories to escape from the region Ω0 through the two points where V = V0
meets the x1-axis, which are labelled as A and B in Figure 13.10. There are various
ways of dealing with this.
The obvious one is to choose a diﬀerent Lyapunov
function, which is possible, but technically diﬃcult.
We will use a phase plane
analysis. Consider SA, the integral path through the point A. All other integral
paths through the boundary of Ω0 in the neighbourhood of A enter Ω0. The integral
path SA cannot, therefore, lie along the boundary of Ω0. If SA does not enter Ω0,
it must intersect the integral path that enters Ω0 at x2 = 0+, which is not possible.
We conclude that the integral path through A enters Ω0, as shown in Figure 13.11.
A similar argument holds at B.
Since the Lyapunov function, V , is monotone decreasing away from the x1-axis,
there cannot be any limit cycle solutions in Ω0. Finally, since Ω0 has all integral
paths entering it, and contains a single, stable equilibrium point and no limit cycles,

13.2 LYAPUNOV’S THEOREMS
387
Fig. 13.10. The regions Ωand Ω∗and the curves V = 0, V = 1
6 and V = V0 < 1
6.
we conclude from the Poincar´e–Bendixson theorem, Theorem 9.4, that all integral
paths in Ω0 enter the origin, which is therefore asymptotically stable, and that
Ω0 lies within the domain of attraction of the origin. In fact, we can see from
Figure 13.9 that this domain of attraction is considerably larger than Ω0, and is
bounded by the stable separatrices of the saddle point at (−1, 0).
Example 4
Consider the system
˙x = x2 −y2,
˙y = −2xy.
This is a genuinely nonlinear system with an equilibrium point at the origin. The
linear approximation at the origin is ˙x = ˙y = 0, so both eigenvalues are zero. Let’s
try a Lyapunov function of the form V = αxy2 −x3, which has V (0, 0) = 0. Since
V ∗= dV
dt = (αy2 −3x2)(x2 −y2) −4αx2y2 = 3(1 −α)x2y2 −αy4 −3x4,
we can choose α = 1, so that V ∗= −y4 −3x4, which is negative deﬁnite. We can
see that V = x(y2 −x2) = 0 when x = 0 or y = ±x, so that V changes sign six
times on any circle that surrounds the origin. In particular, in every neighbourhood
of the origin there is at least one point where V has the same sign as V ∗, so that
all of the conditions of Theorem 13.4 are satisﬁed by V , and hence the origin is
unstable.

388
STABILITY, INSTABILITY AND BIFURCATIONS
−1
−0.96
−0.98
−0.15
−0.1
−0.05
0.05
−0.2
0.2
0.15
0.1
0
−0.94 −0.92 −0.9
−0.88 −0.86 −0.84 −0.82 −0.8
x1
A
V = V0
x2
Fig. 13.11. The phase portrait in the neighbourhood of the point A and the saddle point
at (−1, 0).
Further reﬁnements exist of the Lyapunov Theorems 13.2 to 13.4 that we have
studied in this section, and the interested reader is referred to Coddington and
Levinson (1955) for further information.
13.3
Bifurcation Theory
13.3.1
First Order Ordinary Diﬀerential Equations
Let’s consider the ﬁrst order ordinary diﬀerential equation, (9.5), whose hyper-
bolic equilibrium points we studied in Section 9.2. For a hyperbolic equilibrium
point at x = x1, we saw that a simple linearization about x = x1 determines the
local behaviour and stability. If X′(x1) = 0, x = x1 is a nonhyperbolic equilibrium
point, and we need to retain more terms in the Taylor expansion of X, (9.6), in
order to sort out what happens close to the equilibrium point. For example, if
X(x1) = X′(x1) = 0 and X′′(x1) ̸= 0,
d¯x
dt ≈1
2X′′(x1)¯x2
for ¯x ≪1,

13.3 BIFURCATION THEORY
389
and hence
¯x ≈−
2
X′′(x1)(t −t0),
for some constant t0. The graph of X(x1+¯x) close to ¯x = 0 is shown in Figure 13.12
for X′′(x1) < 0. Focusing on this case, we can see that ¯x →0 (x →x1) as t →∞
for t0 < 0, whilst ¯x →−∞as t →t0 for t0 > 0. This nonhyperbolic equilibrium
point therefore attracts solutions from x ⩾x1 and repels them from x < x1. Note
that the rate at which solutions asymptote to x = x1 from x > x1 is algebraic,
in contrast to the faster, exponential approach associated with stable hyperbolic
equilibrium points.
dx
 dt
x
Fig. 13.12. The graph of d¯x/dt = X(x1 + ¯x) ≈1
2X′′(x1)¯x2 for X′′(x1) < 0.
A system that contains one or more nonhyperbolic equilibrium points is said
to be structurally unstable. This means that a small perturbation, not to the
solution but to the model itself, for example the addition of a small extra term to
X(x), can lead to a qualitative diﬀerence in the structure of the set of solutions, for
example, a change in the number of equilibrium points or in their stability. Consider
the function shown in Figure 13.12. The addition of a small positive constant to
X(x) would shift the graph upwards by a small amount, and give two equilibrium
solutions, whilst the addition of a small negative constant would shift the graph
downwards and lead to the absence of any equilibrium solutions. Let’s investigate
this further.
Consider the equation
˙x = µ −x2,
(13.22)

390
STABILITY, INSTABILITY AND BIFURCATIONS
where µ is a parameter. For µ > 0 there are two hyperbolic equilibrium points
at x = ±√µ, whilst for µ < 0 there are no equilibrium points, as shown in Fig-
ure 13.13. When µ = 0, X(x) = −x2 and x = 0 is a nonhyperbolic equilibrium
point of the type that we analyzed above. We can now draw a bifurcation di-
agram, which shows the position of the equilibrium solutions as a function of µ,
with stable equilibria as solid lines and unstable equilibria as dashed lines, as shown
in Figure 13.14. The point µ = 0, x = 0 is called a bifurcation point, because
the qualitative nature of the phase line changes there. The bifurcation associated
with ˙x = µ −x2 is called a saddle–node bifurcation, for reasons that will become
clear in Section 13.3.2. Any ﬁrst order system that undergoes a saddle–node bifur-
cation, in other words one that contains a bifurcation point where two equilibrium
solutions meet and then disappear, can be written in the form ˙x = µ −x2 in the
neighbourhood the bifurcation point. The equation ˙x = µ−x2 is called the normal
form for the saddle–node bifurcation.
x
x.
x
x.
(b)
(a)
Fig. 13.13. Graphs of ˙x = µ −x2 for (a) µ > 0, (b) µ < 0.
Example
Consider the ordinary diﬀerential equation
˙y = λ −2λy −y2.
(13.23)
This has equilibrium points at y = −λ ±
√
λ2 + λ, so there are no real equilibrium
points for −1 < λ < 0 and two equilibrium points otherwise. This suggests that
there are saddle–node bifurcations at λ = 0, y = 0 and λ = −1, y = 1. Now note
that, at the equilibrium points,
d ˙y
dy = −2λ −2y = ∓
#
λ2 + λ.
Using the analysis of the previous section, we can see that the equilibrium point
with the larger value of y is stable, whilst the other is unstable. The bifurcation
diagram is shown in Figure 13.15, and certainly looks as if it contains two saddle–
node bifurcations.
For y ≪1 and λ ≪1, ˙y = λ −2λy −y2 ≈λ −y2, which is precisely the normal
form for the saddle–node bifurcation. All of the terms on the right hand side of

13.3 BIFURCATION THEORY
391
stable
x
unstable
µ
Fig. 13.14. The saddle–node bifurcation diagram.
Fig. 13.15. The bifurcation diagram for ˙y = λ −2λy −y2.
(13.23) are small, but the only one that is sure to be smaller than at least one of
the others is the second, since y ≪1 means that λy ≪λ. We make no assumption
about how big λ is compared with y2. To examine the neighbourhood of the other
bifurcation point, we shift the origin there using λ = −1 + Aµ, y = 1 + Bx, where

392
STABILITY, INSTABILITY AND BIFURCATIONS
A and B are constant scaling factors that we will ﬁx later. In terms of µ and x,
(13.23) becomes
˙x = −A
B µ −2Aµx −Bx2 ≈−A
B µ −Bx2
for µ ≪1 and x ≪1.
By choosing A = −1 and B = 1 this becomes the normal form for the saddle–node
bifurcation. Note that the required change of coordinate, λ = −1 −µ, indicates
that the sense of the bifurcation is reversed with respect to λ, as can be seen in
Figure 13.15.
We can formalize the notion of a saddle–node bifurcation using the following
theorem.
Theorem 13.5 (Saddle–node bifurcation) Consider the ﬁrst order diﬀerential
equation
˙x = f(x, µ),
with f(0, 0) = fx(0, 0) = 0. Provided that fµ(0, 0) ̸= 0 and fxx(0, 0) ̸= 0, there
exists a continuous curve of equilibrium points in the neighbourhood of (0, 0), which
is tangent to the line µ = 0 there. In addition,
(i) if fµ(0, 0)fxx(0, 0) < 0, then there are no equilibrium points in the neigh-
bourhood of (0, 0) for µ < 0, whilst for µ > 0, in a suﬃciently small neigh-
bourhood of (0, 0) there are two hyperbolic equilibrium points.
(ii) if fµ(0, 0)fxx(0, 0) > 0, then there are no equilibrium points in the neigh-
bourhood of (0, 0) for µ > 0, whilst for µ < 0, in a suﬃciently small neigh-
bourhood of (0, 0) there are two hyperbolic equilibrium points.
If fxx(0, 0) < 0, the equilibrium point with the larger value of x is stable, whilst the
other is unstable, and vice versa for fxx(0, 0) > 0.
Proof
We will give an informal proof. Since f(0, 0) = fx(0, 0) = 0, the Taylor
expansion of f(x, µ) about (0, 0) shows that
˙x ∼A0(µ) + A1(µ)x + A2(µ)x2 for |x| ≪1 and |µ| ≪1,
where
A0(µ) = fµ(0, 0)µ + 1
2fµµ(0, 0)µ2,
A1(µ) = fxµ(0, 0)µ,
A2(µ) = 1
2fxx(0, 0).
There are therefore equilibrium points at
x = −A1 ±
#
A2
1 −4A0A2
2A2
∼±
+
−2fµ(0, 0)µ
fxx(0, 0)
for |µ| ≪1.
This shows that the location of the equilibrium points is as described in the theorem.
Finally, at the equilibrium points,
d ˙x
dx ∼A1(µ) + 2A2(µ)x ∼fxx(0, 0)x,

13.3 BIFURCATION THEORY
393
since x = O(|µ|1/2). The stability of the equilibrium points is therefore determined
by the sign of fxx(0, 0).
Example: The CSTR
Many industrially important chemicals are produced in bulk using a continuous
ﬂow, stirred tank reactor (CSTR). This is simply a large container, to which fresh
chemicals are continuously supplied and from which the resulting reactants are
continuously withdrawn, as sketched in Figure 13.16. A stirrer ensures that the
chemicals in the CSTR are well mixed. Let’s consider what happens when chemicals
A and B that react through the cubic autocatalytic reaction step (13.7) are fed into
a CSTR, and assume that the idea is to convert as much of the reactant A into the
autocatalyst B as possible.
Reactants
in
Products and
unused reactants
out
CSTR
MIXER
Fig. 13.16. A continuous ﬂow, stirred tank reactor (CSTR).
Since the CSTR is well stirred, we assume that the concentrations of the chem-
icals are spatially uniform, and given by a(t) and b(t). The rate of change of the
total amount of species A in the CSTR is equal to the rate at which it is produced
by chemical reaction plus the rate at which it ﬂows in minus the rate at which it
ﬂows out. If the CSTR has constant volume V , constant inlet and (by conservation
of mass) outlet ﬂowrate q and inlet concentration of A given by a0, we have
d
dt (V a) = −V kab2 + q (a0 −a) ,
and hence
da
dt = −kab2 + a0 −a
tres
.
(13.24)
The residence time, tres = V/q, is the time it takes for a volume V of fresh
reactants to ﬂow into the CSTR, and characterizes the period for which a ﬂuid
element typically remains within the CSTR. Similarly,
db
dt = kab2 + b0 −b
tres
,
(13.25)
where b0 is the inlet concentration of the autocatalyst, B.
We now deﬁne dimensionless variables
α = a
a0
,
β = b
a0
,
τ = ka2
0t,

394
STABILITY, INSTABILITY AND BIFURCATIONS
in terms of which (13.24) and (13.25) become
dα
dτ = −αβ2 + 1 −α
τres
,
(13.26)
dβ
dτ = αβ2 + β0 −β
τres
,
(13.27)
where
τres = ka2
0tres,
β0 = b0
a0
.
We can now add (13.26) and (13.27) to obtain
d
dτ (α + β) = 1 + β0 −(α + β)
τres
,
a linear equation that we can solve for α + β using an integrating factor.
The
solution is
α + β = 1 + β0 + ke−τ/τres,
where k is a constant. Clearly, α + β →1 + β0 as τ →∞. In particular, for
τ ≫τres, and hence for t ≫tres, α + β ∼1 + β0. The term ke−τ/τres represents
an initial transient, which decays to zero exponentially fast over a time scale given
by the residence time. We therefore assume that α + β = 1 + β0, and can thereby
eliminate β from (13.26). In the analysis that follows, it is more convenient to work
in terms of z = 1 −α, the extent to which the reactant A has been converted to B.
This gives us a nonlinear, ﬁrst order ordinary diﬀerential equation for z,
dz
dτ = R(z) −F(z),
(13.28)
where
R(z) = (1 −z) (z + β0)2 ,
F(z) =
z
τres
.
We can now see that a steady state, where dz/dτ = 0, occurs when the rate of
reaction, R(z), is balanced by the rate at which A ﬂows into the CSTR, F(z), and
hence when F(z) = R(z). The function R(z) is a cubic polynomial, whilst F(z)
is a straight line through the origin. The steady states are therefore given by the
points of intersection of these two curves.
Case 1: No autocatalyst in the inﬂow, β0 = 0
When β0 = 0, R(z) has a repeated root at z = 0, as shown in Figure 13.17.
The straight line F(z) always passes through z = 0, which is therefore always a
steady state. The state z = 0 represents a CSTR that contains no autocatalyst,
just the reactant A supplied by the inlet ﬂow. A simple calculation of the steady
state solutions shows that for τres < 4, z = 0 is the only steady state, whilst for

13.3 BIFURCATION THEORY
395
τres > 4, there are two further points of intersection between F(z) and R(z), and
hence two other steady states, at z = z1 and z = z2, where
z1 = 1
2

1 −
*
1 −
4
τres

,
z2 = 1
2

1 +
*
1 −
4
τres

.
Note that 0 < z1 < 1
2 and 1
2 < z2 < 1, and z1 →0, z2 →1 as τres →∞. The
steady states are sketched in Figure 13.18. The stability of the steady states is
easily calculated, and we ﬁnd that z = 0 and z = z2 are stable states, whilst z = z1
is unstable. There is a saddle–node bifurcation at τres = 4. This situation, with
β0 = 0, is a realistic one, since it is probably desirable to run the system with just
a single species entering the CSTR. We clearly want to run the CSTR in the state
z = z2, where more than half of the reactant A is converted to B. However, we also
want to make the residence time as small as possible, to increase the rate at which
B is produced. However, we can now see that, if τres is slowly decreased, z2 will also
slowly decrease, and that when τres decreases past 4, the saddle–node bifurcation
point, the situation changes dramatically. The only available steady state is z = 0,
so that no autocatalyst remains in the CSTR, and the reaction stops.
This is
known as washout. Attempts to recover the desirable state z = z2 by increasing
the residence time, τres, are doomed to failure when there is no autocatalyst entering
the CSTR.
Fig. 13.17. The curves R(z) and F(z) when β0 = 0.

396
STABILITY, INSTABILITY AND BIFURCATIONS
Fig. 13.18. The bifurcation diagram when β0 = 0.
Case 2: Autocatalyst in the inﬂow, β0 > 0
Let’s now see how the situation is aﬀected by including some autocatalyst in
the inﬂow. When β0 > 0, the cubic polynomial R(z) has a single positive root at
z = 1, and is strictly positive for 0 ⩽z < 1, as shown in Figure 13.19. In order to
determine when F(z) is tangent to R(z), we must simultaneously solve R(z) = F(z)
and R′(z) = F ′(z), which gives
(z + β0)2 (1 −z) =
z
τres
,
(13.29)
(z + β0) (−3z + 2 −β0) =
1
τres
.
(13.30)
Eliminating τres between these two equations gives 2z2 −z + β0 = 0, and hence the
points of tangency are at
z = z± = 1
4

1 ±
#
1 −8β0

.
We conclude that there are no such points of tangency for β0 >
1
8, and hence
that there is a unique stable solution, as shown in Figures 13.19 and 13.20. For
0 < β0 < 1
8, there is a unique solution for 0 ⩽τres ⩽τ+ and τres > τ−, and three
solutions for τ+ < τres < τ−, where, from (13.30),
τ± =
1
(z± + β0) (2 −β0 −3z±).
There are now two saddle–node bifurcations, at τres = τ±.

13.3 BIFURCATION THEORY
397
Fig. 13.19. The curves R(z) and F(z) when β0 > 0.
We conclude that for β0 > 1
8, there is no possibility of washout, just a unique
steady state solution for any given residence time, τres. For 0 < β < 1
8, we again
have the possibility, if not of a washout of the autocatalyst, at least of a dramatic
decrease in the concentration of B when τres falls below τ+. However, if τres is
now increased again, there will be a dramatic increase in the concentration of B as
τres increases past τ−. This change in the steady state from when a parameter is
increased to when it is decreased is known as hysteresis.
Finally, note that when β = 1
8 the two saddle–node points merge and disappear
at τres = 64
27. This is itself a bifurcation, and is known as a codimension two
bifurcation. Such a bifurcation can only occur in a system that has at least two
parameters (here β0 and τres).
We will study two other types of bifurcation. Consider the normal form
˙x = µx −x2.
(13.31)
This system has equilibrium points at x = 0 and x = µ. When µ = 0 we again have
the nonhyperbolic equilibrium point given by ˙x = −x2, whilst when µ ̸= 0 there
are always two equilibrium points. In this case,
d ˙x
dx = µ −2x =
 −µ
at x = µ,
µ
at x = 0,

398
STABILITY, INSTABILITY AND BIFURCATIONS
Fig. 13.20. The bifurcation diagram when β0 > 0.
and hence
x =
µ is

stable for µ > 0,
unstable for µ < 0,
x = −µ is
 unstable for µ > 0,
stable for µ < 0.
The bifurcation diagram is shown in Figure 13.21. This is called a transcritical
bifurcation. At the bifurcation point, the two equilibrium solutions pass through
each other and exchange stabilities, so this sort of bifurcation is often referred to
as an exchange of stabilities.
Theorem 13.6 (Transcritical bifurcation) Consider the ﬁrst order diﬀerential
equation
˙x = f(x, µ),
with f(0, 0) = fx(0, 0) = fµ(0, 0) = 0. Provided that fxx(0, 0) ̸= 0 and f 2
µx(0, 0) −
fxx(0, 0)fµµ(0, 0) > 0, there exist two continuous curves of equilibrium points in
the neighbourhood of (0, 0). These curves intersect transversely at (0, 0). For each
µ ̸= 0 there are two hyperbolic equilibrium points in the neighbourhood of x = 0. If
fxx(0, 0) < 0, the equilibrium point with the larger value of x is stable, whilst the
other is unstable, and vice versa for fxx(0, 0) > 0.
We will leave the informal proof as Exercise 13.9.

13.3 BIFURCATION THEORY
399
stable
x
unstable
µ
Fig. 13.21. The transcritical bifurcation.
Finally, consider the normal form
˙x = µx −x3.
(13.32)
This has a single equilibrium point at x = 0 for µ < 0, and three equilibrium
points at x = 0 and x = ±√µ for µ > 0.
The bifurcation diagram is shown
in Figure 13.22(a). The bifurcation at x = 0, µ = 0 is called a supercritical
pitchfork bifurcation. A similar bifurcation, in which the two new equilibrium
points created at the bifurcation point are unstable, is the subcritical pitchfork
bifurcation, with normal form ˙x = µx + x3, whose bifurcation diagram is shown
in Figure 13.22(b).
Theorem 13.7 (Pitchfork bifurcation) Consider the ﬁrst order diﬀerential
equation
˙x = f(x, µ),
with f(0, 0) = fx(0, 0) = fµ(0, 0) = fxx(0, 0) = 0. Provided that fµx(0, 0) ̸= 0
and fxxx(0, 0) ̸= 0, there exist two continuous curves of equilibrium points in the
neighbourhood of (0, 0). One curve passes through (0, 0) transverse to the line µ = 0,
whilst the other is tangent to µ = 0 at x = 0. In addition,
(i) if fµx(0, 0)fxxx(0, 0) < 0, then, close to (0, 0), there is a single equilibrium
point for µ < 0 and three equilibrium points for µ > 0.
(ii) if fµx(0, 0)fxxx(0, 0) > 0, then, close to (0, 0), there is a single equilibrium
point for µ > 0 and three equilibrium points for µ < 0.
If fxxx(0, 0) < 0, the single equilibrium point and the outer two of the three equilib-

400
STABILITY, INSTABILITY AND BIFURCATIONS
rium points are stable, whilst the middle of the three equilibrium points is unstable,
and vice versa for fxxx(0, 0) > 0.
We leave the informal proof as Exercise 13.9.
(a) supercritical
x
µ
(b) subcritical
x
µ
stable
unstable
Fig. 13.22. The (a) supercritical and (b) subcritical pitchfork bifurcation.
Although these bifurcations seem rather abstract, they often describe the quali-
tative behaviour of complicated physical systems, as we saw for the CSTR. Another
simple example is the buckling beam. Consider a straight beam of elastic material
under compression from two equal and opposite forces along its axis, one applied
at each end, as shown in Figure 13.23(a). Now consider the displacement, x, of the
midpoint of the beam. For suﬃciently small applied forces, the beam undergoes a
simple axial compression and x = 0. As the applied forces are slowly increased, at a
critical value the beam buckles, as shown in Figure 13.23(b), because the unbuckled
state becomes unstable. If the beam has perfect left–right symmetry, it is equally
likely to buckle left or right. When the displacement of the midpoint of the beam
is plotted as a function of magnitude of the applied forces, we obtain the bifurca-

13.3 BIFURCATION THEORY
401
tion diagram corresponding to a supercritical pitchfork, as in Figure 13.22(a). Of
course, in practice, a beam, for example a ruler held between your foreﬁngers, will
not have perfect symmetry, and will buckle in a preferred direction. This suggests
that the pitchfork bifurcation is itself structurally unstable. You can investigate
this by trying Exercise 13.7.
F0
(a)
F0
F1 > F0
(b)
F1 > F0
Fig. 13.23. An elastic beam under compression (a) before and (b) after buckling.
13.3.2
Second Order Ordinary Diﬀerential Equations
As for ﬁrst order equations, second order systems that possess an equilibrium
point with an eigenvalue with zero real part are structurally unstable. A small
change in the governing equations can change the qualitative nature of the phase
portrait. For example, the simple, frictionless pendulum, with phase portrait shown
in Figure 9.2(b), contains a nonlinear centre. Since a centre has two eigenvalues
with zero real part, we conclude that this system is structurally unstable. In terms
of the physics, consider what happens if we allow for a tiny amount of friction.
This will gradually reduce the energy and the amplitude of the motion, with the
pendulum being brought to rest as t →∞. This is reﬂected in what happens to the
phase portrait when a term µdθ/dt, with µ > 0, is added to the left hand side of
(9.2) to model the eﬀect of friction. No matter how small µ is, the nonlinear centre
is transformed into a stable focus, and all integral paths asymptote to θ = ˙θ = 0 as
t →∞(see Exercise 9.6).
Now consider the second order system
˙x = µ −x2,
˙y = −y.
(13.33)

402
STABILITY, INSTABILITY AND BIFURCATIONS
For µ < 0 there are no equilibrium points, whilst for µ > 0 there are two equilibrium
points at P+ = (+√µ, 0) and P−= (−√µ, 0). The point P+ is a stable node, whilst
P−is a saddle point. When µ = 0 there is a single, nonhyperbolic equilibrium point
at (0, 0), a saddle–node. The various phase portraits are shown in Figure 13.24. The
point µ = 0, x = y = 0 is called a saddle–node bifurcation point, and (13.33)
is its normal form. At such a point, a saddle and a node collide and disappear.
Note that, since ˙y = −y, all integral paths asymptote to the x-axis as t →∞,
where the dynamics are controlled by ˙x = µ −x2. This is just the normal form
of the saddle–node bifurcation in a ﬁrst order equation. In eﬀect, we can ignore
the dynamics in the y-direction, and concentrate on the dynamics on the x-axis,
which is the centre manifold for the bifurcation. This shows why it was important
to study bifurcations in ﬁrst order systems, since the important dynamics of higher
order systems occur on a lower order centre manifold. For more on bifurcation
theory, see Arrowsmith and Place (1990).
Two other simple types of bifurcation are (see Exercise 13.8) the transcritical
bifurcation, with normal form
˙x = µx −x2,
˙y = −y,
(13.34)
and the supercritical and subcritical pitchfork bifurcations, with normal
forms
˙x = µx ± x3,
˙y = −y.
(13.35)
The behaviour on the x-axis is, in each case, analogous to the corresponding bifur-
cation in a one-dimensional system, which we studied in Section 13.3.1. In each of
these examples, one eigenvalue passes through zero at the bifurcation point. We
will not consider the more degenerate case where both eigenvalues pass through
zero (see Guckenheimer and Holmes, 1983). However, we will consider one other
type of bifurcation, which occurs when a complex conjugate pair of eigenvalues
passes through the imaginary axis away from the origin. This is called the Hopf
bifurcation, and has no counterpart in ﬁrst order systems.
The normal form of the Hopf bifurcation is most easily written in terms of polar
coordinates as
˙r = µr + ar3,
˙θ = ω.
(13.36)
Let’s assume that a < 0. For µ ⩽0 there is a stable focus at the origin, and no other
equilibrium points, and all trajectories spiral into the origin from inﬁnity. For µ > 0
there is an unstable focus at the origin, and no other equilibrium points, but there
is also a stable limit cycle, given by r =
#
−µ/a, ˙θ = ω. Trajectories spiral onto the
limit cycle both from the origin and from inﬁnity, as shown in Figure 13.25. When
µ = 0 a linear analysis shows that the origin is a centre, and hence has eigenvalues
with zero real part. However, this is an example where the nonlinear terms, here
˙r = ar3, cause the integral paths to spiral into the equilibrium point. Note that
this is a supercritical Hopf bifurcation. The subcritical Hopf bifurcation,
for which the limit cycle is unstable and exists for µ < 0, occurs when a > 0. In
general, for equations that model real physical systems, any limit cycle solutions

13.3 BIFURCATION THEORY
403
(a) µ > 0
y
x
P+
P−
(b) µ = 0
y
x
y
x
(c) µ < 0
Fig. 13.24. A saddle–node bifurcation in a second order system, for (a) µ > 0, (b) µ = 0,
(c) µ < 0.
that exist are formed at Hopf bifurcations, and it is therefore crucial to determine
the position of these bifurcations. In order to demonstrate that a Hopf bifurcation
occurs when a pair of eigenvalues crosses the imaginary axis, we can appeal to the
Hopf bifurcation theorem.
Theorem 13.8 (Hopf) Consider the second order system
˙x = X(x, y; µ),
˙y = Y (x, y; µ).
Suppose that

404
STABILITY, INSTABILITY AND BIFURCATIONS
x
x
y
(b) µ > 0
y
<
Fig. 13.25. A Hopf bifurcation for (a) a < 0, µ ⩽0 and (b) a < 0, µ > 0.
(i) X(0, 0; µ) = Y (0, 0; µ) = 0 for each µ ∈[−µ0, µ0] for some µ0 > 0,
(ii) the Jacobian matrix evaluated at the origin with µ = 0 is
 0
−ω
ω
0

for some ω ̸= 0,
(iii) the eigenvalues of the equilibrium point at the origin are complex conjugate
for µ ∈[−µ0, µ0], and given by α(µ) ± iβ(µ), with α and β real,
(iv) a ̸= 0, where
a = 1
16 (Xxxx + Yxxy + Xxyy + Yyyy)
+ 1
16ω {Xxy (Xxx + Xyy) −Yxy (Yxx + Yyy) −XxxYxx + XyyYyy} , (13.37)
with all of these partial derivatives evaluated at the origin with µ = 0.
Then
(i) If aα′(0) < 0, a unique limit cycle solution bifurcates from the origin in
µ > 0 as µ passes through zero. For µ < 0 there exists a neighbourhood of
the origin that does not contain a limit cycle solution. The stability of the
limit cycle is the same as that of the origin in µ < 0.
(ii) If aα′(0) > 0, a unique limit cycle solution bifurcates from the origin in
µ < 0 as µ passes through zero. For µ > 0 there exists a neighbourhood of
the origin that does not contain a limit cycle solution. The stability of the
limit cycle is the same as that of the origin in µ > 0.
The amplitude of the limit cycle grows like |µ|1/2 and its period tends to 2π/|ω| as
µ →0.
We will not prove this theorem here. The idea of the proof is to go through a series
of algebraic transformations of the diﬀerential equations that reduce them to the

13.3 BIFURCATION THEORY
405
normal form, (13.36), and is algebraically very unpleasant. For as straightforward
an explanation of the details as you could hope for, see Glendinning (1994). Instead,
we will concentrate on a concrete example to show how the proof of the theorem
works.
Example
Consider the second order system
˙x = X(z, y; µ) = µx −ωy + x3,
˙y = Y (x, y; µ) = ωx + µy + y2,
(13.38)
where µ and ω are constants. The linear part of the system is in the normal form for
an equilibrium point at the origin with complex conjugate eigenvalues λ = µ + iω,
which we discussed in Section 9.3.2. This means that the origin is a stable focus
for µ < 0, a linear centre for µ = 0 and an unstable focus for µ > 0. The system
therefore satisﬁes conditions (i), (ii) and (iii) of Theorem 13.8. If we now substitute
this particular choice of X and Y into (13.37), we ﬁnd that a = 3/8, and hence
that aα′(0) = 3/8 > 0. The Hopf bifurcation at µ = 0 is therefore subcritical, with
an unstable limit cycle emerging from the origin in µ < 0.
Let’s now try to write (13.38) in the normal form (13.36) for suﬃciently small
x, y and µ, using the same transformations as those used in the proof of the Hopf
bifurcation theorem. We begin by deﬁning z = x + iy, in terms of which (13.38)
can be written concisely as
˙z = λz −1
4i (z −z∗)2 + 1
8 (z + z∗)3 .
(13.39)
The next step is to make a quadratic near identity transformation,
z = w + a1w2 + a2ww∗+ a3w∗2.
(13.40)
The idea is that such a transformation leaves the linear part of the equation un-
changed when written in terms of w, but can be used to simplify the nonlinear
part by an appropriate choice of the constants a1, a2 and a3. Speciﬁcally, we can
eliminate quadratic terms from (13.39). For |z| ≪1,
w = z −a1w2 −a2ww∗−a3w∗2 = z −a1

z −a1w2 −a2ww∗−a3w∗22
−a2

z −a1w2 −a2ww∗−a3w∗2 
z∗−a∗
1w∗2 −a∗
2ww∗−a∗
3w2
−a3

z∗−a∗
1w∗2 −a∗
2ww∗−a∗
3w22 = z −a1z2 −a2zz∗−a3z∗2 +

2a2
1 + a2a∗
3

z3
+

3a1a2 + a2a∗
2 + 2a2
3

z2z∗
+

2a1a3 + a∗
1a2 + a2
2 + 2a∗
2a3

zz∗2 + (a2a3 + 2a∗
1a3) z∗3 + O(|z|4).
If we now take this expression, diﬀerentiate, and replace ˙z and ˙z∗using (13.39), we
arrive, after considerable eﬀort, at
˙w = λz −1
4i (z −z∗)2 + 1
8 (z −z∗)3

406
STABILITY, INSTABILITY AND BIFURCATIONS
−(2a1z + a2z∗)

λz −1
4i (z −z∗)2
	
−(a2z + 2a3z∗)

λ∗z∗+ 1
4i (z −z∗)2
	
+3

2a2
1 + a2a∗
3

λz3 +

3a1a2 + a2a∗
2 + 2a2
3

(2λ + λ∗) z2z∗
+

2a1a3 + a∗
1a2 + a2
2 + 2a∗
2a3

(λ + 2λ∗) zz∗2 + 3 (a2a3 + 2a∗
1a3) λ∗z∗3 + O(|z|4).
We can now use the deﬁnition, (13.40), of w to eliminate z from the right hand
side, retaining only cubic terms and larger, and arrive at
˙w = λ

w + a1w2 + a2ww∗+ a3w∗2
−
1
4i + 2a1λ
 
w2 + 2a1w3 + 2a2w2w∗+ 2a3ww∗2
+
1
2i −a2 (λ + λ∗)
	 
ww∗+ a∗
3w3 + (a1 + a∗
2) w2w∗+ (a∗
1 + a2) ww∗2 + a3w∗3
−
1
4i + 2a3λ∗
 
w∗2 + 2a∗
1w∗3 + 2a∗
2w∗2w + 2a∗
3w∗w2
+1
4i (2a1w + a2w∗) (w −w∗)2 −1
4i (a2w + 2a3w∗) (w −w∗)2 + 1
8 (w + w∗)3
+3

2a2
1 + a2a∗
3

λw3 +

3a1a2 + a2a∗
2 + 2a2
3

(2λ + λ∗) w2w∗
+

2a1a3 + a∗
1a2 + a2
2 + 2a∗
2a3

(λ + 2λ∗) ww∗2 + 3 (a2a3 + 2a∗
1a3) λ∗w∗3 + O(|w|4).
We can now see that we can eliminate all of the quadratic terms by choosing
a1 = −i
4λ,
a2 = i
2λ,
a3 =
i
4 (λ −2λ∗),
which leaves us with
˙w = λw + A1w3 + A2w2w∗+ A3ww∗2 + A4w∗3,
where
A1 = −2a1
1
4i + 2a1λ

+ a∗
3
1
2i −a2 (λ + λ∗)
	
+ 1
8
+1
4i (2a1 −a2) + 3 (2a1 + a2a∗
3) λ,
A2 = −2a2
1
4i + 2a1λ

+ (a1 + a∗
2)
1
2i −a2 (λ + λ∗)
	
+ 3
8
−2a∗
3
1
4i + 2a3λ∗

+ 1
4i (3a2 −2a3 −4a1) +

3a1a2 + a2a∗
2 + 2a2
3

(2λ + λ∗) ,
A3 = −2a3
1
4i + 2a1λ

+ (a∗
1 + a2)
1
2i −a2 (λ + λ∗)
	
+ 3
8

13.3 BIFURCATION THEORY
407
−2a∗
2
1
4i + 2a3λ∗

+ 1
4i (4a3 −3a2 + 2a1) +

2a1a3 + a∗
1a2 + a2
2 + 2a∗
2a3

(λ + 2λ∗) ,
A4 = −2a∗
1
1
4i + 2a3λ∗

+ a3
1
2i −a2 (λ + λ∗)
	
+ 1
8
+1
4i (a2 −2a3) + 3 (a2a3 + a∗
1a3) λ∗.
The next step is to make another near identity transformation,
w = v + b1v3 + b2v2v∗+ b3vv∗2 + b4v∗3,
(13.41)
and try to eliminate the cubic terms as well. Proceeding exactly as we did for
the quadratic terms, although the algebra is now easier since we only retain cubic
terms, we ﬁnd that
˙v = λv + (A1 −2λb1) v3 + {A2 −(λ + λ∗) b2} v2v∗+ (A3 −2λ∗b3) vv∗2
+ {A4 + (λ −3λ∗) b4} v∗3 + O(|v|4).
At ﬁrst sight, it would appear that we can eliminate all of the cubic terms in
the same way as we did the quadratic terms.
However, the coeﬃcient of b2 is
λ + λ∗= 2µ, which is small when µ ≪1. Since we need b2 = O(1) for |v| ≪1 and
µ ≪1, we conclude that we cannot eliminate the v2v∗term, and hence that the
simplest normal form is
˙v ∼λv + av|v|2,
using v2v∗= v|v|2, with a = A2(0). A little algebra then shows that a = A2(0) =
3/8, consistent with our earlier calculation of a from (13.37). Indeed, the hard part
of the proof the Hopf bifurcation is to show that a, as deﬁned in (13.37), appears
in the normal form in this way. If we now write v = reiθ and separate real and
imaginary parts, we recover the normal form (13.36), and we are done.
We should reiterate that this transformation and the ensuing algebra is not what
needs to be done whenever the system you are studying contains a Hopf bifurcation
point. The Hopf bifurcation theorem is far easier to use. We went through the
details of this example purely to illustrate the steps involved in the proof.
Figure 13.26 shows the unstable limit cycle solution for various µ < 0.
We
calculated these solutions numerically using MATLAB (see Section 9.3.4).
For
|µ| suﬃciently small, we can see that the limit cycle is circular. However, as |µ|
increases, the limit cycle moves away from the neighbourhood of the origin and
becomes increasingly distorted as it begins to interact with a saddle point that lies
close to (−1, −1). In fact, when µ = µ0 ≈−0.12176, the limit cycle collides with
this saddle point, and is destroyed. This is an example of a homoclinic bifurcation,
which we discuss below.

408
STABILITY, INSTABILITY AND BIFURCATIONS
Fig. 13.26. The unstable limit cycle solution of (13.36) for µ = −0.002, −0.005, −0.01,
−0.05, −0.1, −0.12 and −0.12176.
13.3.3
Global Bifurcations
With the exception of the previous example, all of the bifurcations that we have
studied so far have been local bifurcations. That is, they have arisen because of a
change in the nature of an equilibrium point as a parameter passes through a critical
value. A global bifurcation is one that occurs because the qualitative nature of
the solutions changes due to the interaction of two or more distinct features of
the phase portrait. For second order systems, the crucial features are usually limit
cycles and the separatrices of any saddle points. Figure 13.27 illustrates an example
of a homoclinic bifurcation, which can occur when a limit cycle interacts with
a saddle point. A limit cycle is formed in a Hopf bifurcation when the bifurcation
parameter, µ, is equal to µ1.
As µ increases, the amplitude of the limit cycle
grows until, when µ = µ2 > µ1, the limit cycle collides with the saddle point,

13.3 BIFURCATION THEORY
409
forming a connection between the stable and unstable separatrices. This is the
homoclinic bifurcation point. With the separatrices joined up like this, the system
is structurally unstable, since an arbitrarily small change to the equations can
destroy the connection. For µ > µ2, there is no limit cycle.
Of course, it is one thing to describe a global bifurcation in qualitative terms,
but quite another to be able to quantify when the bifurcation occurs, since a local
analysis is of no help. We must usually resort to numerical methods, as we did for
the example shown in Figure 13.26. We will therefore conﬁne ourselves to a single
example of a global bifurcation where analytical progress is possible.
(a)
(b)
(c)
µ1 < µ < µ2
µ > µ2
µ = µ2
Fig. 13.27. A homoclinic bifurcation.
Example: Travelling waves in cubic autocatalysis (continued)
In Section 13.1.1 we performed a local analysis of the two equilibrium points of
(13.14). In order to determine for what values of V a physically meaningful solution
exists, we need to determine for what values of V the unstable separatrix of P1,
labelled S1 in Figure 13.4, enters the origin as z →∞. This is a global problem
involving the relative positions of S1 and the stable manifold of P2, labelled S2 in
Figure 13.4.
We begin by deﬁning the region
R =

(β, γ) | 0 < β < 2
3, −4
27V < γ < 0
	
= 
(β, γ) | 2
3 ⩽β < 1, γH(β) < γ < 0
	
,
where γH(β) = −β2(1 −β)/V is the horizontal isocline. The region R is shown
in Figure 13.28. Note that the point β = 2
3, γ = −4
27V is the local minimum of
γH(β). This region is constructed so that there are three qualitatively diﬀerent
possibilities.
— Case (a): S2 enters R through the β-axis,

410
STABILITY, INSTABILITY AND BIFURCATIONS
— Case (b): S2 = S1, and hence asymptotes to P1 as z →−∞,
— Case (c): S2 enters R through its lower boundary.
Note that S2 cannot enter R through the γ-axis, since dβ/dz = γ < 0 there.
Cases (a), (b) and (c) are illustrated in Figure 13.28. In case (a), S1 lies below
S2 and therefore does not enter the origin and is swept into the region β < 0. It
cannot represent a physically meaningful solution in this case. In case (b), S2 = S1
represents a physically meaningful solution, and this solution enters the origin on
the stable manifold. In case (c), S1 lies above S2 and is therefore attracted into the
origin on the centre manifold. These arguments can be made more rigorous, but
we will not do this here (see Billingham and Needham, 1991, for more details).
We now need to determine which of these three cases arises for each value of V .
We can do this by deﬁning a function f(V ), as illustrated in Figure 13.29.
— Case (a): f(V ) is equal to the value of β where S2 crosses the β-axis leaving the
region R,
— Case (b): f(V ) = 1,
— Case (c): f(V ) = 1 −γ0, where γ0 is the value of γ where S2 crosses the line
β = 1 (which it does, since dβ/dz = γ < 0).
Deﬁned is this way, f(V ) is continuous, and there is no physically meaningful
solution of (13.14) when f(V ) < 1 (case(a)), and a unique physically meaningful
solution when f(V ) ⩾1 (cases (b) and (c)).
Lemma 13.1 f(V ) is strictly monotone increasing for V > 0.
Proof When V = V0 > 0, we deﬁne the region
D(V0) =

(β, γ) | 0 ⩽γ ⩽γS(β)|V =V0 , 0 ⩽β ⩽1

,
where γ = γS(β) is the equation of S2 within the region R, as illustrated in Fig-
ure 13.29. From (13.17),
∂
∂V
dγ
dβ

= −1 < 0.
This means that the slope of the integral path through any ﬁxed point is strictly
monotone decreasing as V increases. In particular, when V = V1 > V0, all integral
paths that meet the curved boundary of D(V0) (a boundary given by S2 when
V = V0) enter D(V0). In addition, from (13.14), all integral paths that meet the
straight parts of the boundary of D(V0) also enter D(V0).
Finally, since S2 is
directed along the vector e−= (1, −V ) as it enters the origin, it lies outside D(V0)
in a suﬃciently small neighbourhood of the origin when V = V1. We conclude that
when V = V1 > V0, S2 cannot pass through the boundary of D(V0), and therefore
that f(V1) > f(V0).
We will leave the next stage of our argument as Exercise 13.11, in which you
are asked, helped by some hints, to show that f(V ) = O(V 2) for V ≪1, and that
f(V ) ∼V for V ≫1. Now, since f(V ) < 1 for V suﬃciently small, f(V ) > 1 for V

13.3 BIFURCATION THEORY
411
S2
S1
S2
S1
S2 = S1
P1
P2
P2
P1
P2
P1
β
(a)
(b)
(c)
γ
γ
γ
β
β
Fig. 13.28. The region R and the three possible types of global behaviour of S1 and S2.
suﬃciently large, f(V ) is strictly monotone increasing by Lemma 13.1, and f(V )
is continuous, we conclude from the intermediate value theorem that there exists a
unique value V = V ∗, such that f(V ) ⩾1 for V ⩾V ∗and f(V ) < 1 for V < V ∗.
When V = V ∗there is therefore a global bifurcation, since we have now shown that

412
STABILITY, INSTABILITY AND BIFURCATIONS
D
(c)
(b)
(a)
β
β
β
D
D
S2
S2 = S1
S1
β = 1
β = 1
β = 1
f(V) < 1
f(V) = 1
f(V) > 1
γ
γ
γ
Fig. 13.29. The deﬁnition of f(V ) and the region D(V ) in each of the three cases.
case (a) occurs when V < V ∗, case (b) occurs when V = V ∗and case (c) occurs
when V > V ∗. This global bifurcation comes about purely because of the relative
positions of the manifolds S1 and S2.
In order to determine the numerical value of V ∗, we would usually have to solve

EXERCISES
413
the governing equations (13.14) numerically. However, we do know that when V =
V ∗the solution asymptotes to the origin on the stable manifold with β = O(e−V ∗z)
as z →∞, whilst for V > V ∗, the solution asymptotes to the origin on the centre
manifold, with β ∼V/z as z →∞. We were also able to show that there is an
analytical solution, (13.18), when V = 1/
√
2, which has β = O(e−z/
√
2) as z →∞.
This must therefore be the unique solution that corresponds to V = V ∗, and we
conclude that the global bifurcation point is V = V ∗= 1/
√
2.
In conclusion, we have shown that a unique travelling wave solution exists for
each V ⩾1/
√
2, and that the solution with V = 1/
√
2 asymptotes to zero expo-
nentially fast as z →∞, whilst this decay is only algebraic for V > 1/
√
2. This has
implications for the selection of the speed of the waves generated in an initial value
problem for equations (13.8), in particular that localized initial inputs of autocat-
alyst generate waves with the minimum speed, V = 1/
√
2. The reader is referred
to Billingham and Needham (1991) for further details, and to King and Needham
(1994) for a similar analysis when the diﬀusion coeﬃcient is not constant.
Exercises
13.1
Sketch the phase portraits of the systems
(a) ˙x = −x −2y2,
˙y = xy −y3,
(b) ˙x = x2,
˙y = −y −x2,
(c) ˙x = y + x2,
˙y = −y −x2,
in the neighbourhood of the origin, including in your sketch the unstable
and centre manifolds.
13.2
Use arguments based on Lyapunov’s theorems to determine the stability of
the equilibrium points of
(a) ˙x = x2 −2y2,
˙y = −4xy,
(b) ˙x = xy2 −x,
˙y = x2y −y.
13.3
Consider the second order diﬀerential equation ¨θ + k ˙θ + f(θ) = 0, where
f is continuously diﬀerentiable on the interval |θ| < α, with α > 0 a
given constant, θf(θ) > 0 for θ ̸= 0 and f(0) = 0.
By considering a
suitable Lyapunov function, show that the origin is an asymptotically stable
equilibrium point.
13.4
Euler’s equations for a rigid body spinning freely about a ﬁxed point in
the absence of external forces are
A ˙ω1 −(B −C)ω2ω3 = 0,
B ˙ω2 −(C −A)ω3ω1 = 0,
C ˙ω3 −(A −B)ω1ω2 = 0,
where A, B and C are the principal moments of inertia, and ω = (ω1, ω2, ω3)
is the angular velocity of the body relative to its principal axes.
Find all of the steady states of Euler’s equations. Using the ideas that we
developed in Section 9.4, show that ω2
1+ω2
2+ω2
3 is a constant of the motion.

414
STABILITY, INSTABILITY AND BIFURCATIONS
Sketch the phase portrait on the surface of the sphere ω2
1 + ω2
2 + ω2
3 = ω2
0.
Deduce that the steady state with ω1 = ω0, ω2 = ω3 = 0 is unstable if
C < A < B or B < A < C, but stable otherwise.
Show that
V =
!
B(A −B)ω2
2 + C(A −C)ω2
3

+ Bω2
2 + Cω2
3 + A

ω2
1 + 2ω0ω1
"2 ,
is a Lyapunov function for the case when A is the largest moment of inertia,
so that the state ω1 = ω0, ω2 = ω3 = 0 is stable. Suggest a Lyapunov
function that will establish the stability of this state when A is the smallest
moment of inertia.
Are these states asymptotically stable?
Perform a
simple experiment to verify your conclusions.
13.5
A particle of mass m lies at r = (x, y, z) and moves in a potential ﬁeld
W(x, y, z), so that its equation of motion is
m¨r = −∇W.
By writing ˙x = u, ˙y = v and ˙z = w, express this equation of motion in
terms of ﬁrst derivatives only. Suppose that W has a local minimum at
r = 0. By using the Lyapunov function
V = W + 1
2m

u2 + v2 + w2
,
show that the origin is a stable point of equilibrium for the particle. What
do the level curves of V represent physically? Is the origin asymptotically
stable?
If an additional, nonconservative force, f(u, v, w), also acts on the par-
ticle, so that
m¨r = −∇W + f,
describe qualitatively how the stability of the point of equilibrium at the
origin is aﬀected.
13.6
Consider the three systems
(a) ˙y = y(y −1)(y −2λ),
(b) ˙y = y2 + 4λ2 −1,
(c) ˙y = −y(4y2 + λ2 −1).
Sketch the bifurcation diagram for each system, and show that each system
has two bifurcation points of the same type, which you should determine.
Close to each bifurcation point, write each system in the normal form
appropriate to the bifurcation.
13.7
(a) Consider the system
˙x = −ϵ + µx −x2.
When ϵ = 0 this is the normal form for a transcritical bifurcation.
Sketch the bifurcation diagram when ϵ ̸= 0, dealing separately with
the cases ϵ > 0 and ϵ < 0. In one case there are two saddle–node
bifurcation points, in the other there are no bifurcation points.

EXERCISES
415
(b) Consider the system
˙x = µx + 2ϵx2 −x3,
with ϵ ⩾0. When ϵ = 0 this is the normal form for a supercriti-
cal pitchfork bifurcation. Sketch the bifurcation diagram when ϵ is
small, but nonzero.
What can you deduce from the answers to the two parts of this question?
13.8
Sketch the phase portraits when µ > 0, µ = 0 and µ < 0 for the normal
forms of (a) the transcritical and (b) the supercritical pitchfork bifurcation,
given by (13.34) and (13.35).
13.9
Give an informal proof of the transcritical bifurcation theorem and the
pitchfork bifurcation theorem.
13.10
Consider the system ˙x = µx −ωy + x3, ˙y = ωx + µy + y3. Use the Hopf
bifurcation theorem to determine the nature of the Hopf bifurcation at
µ = 0. Use a near identity transformation to write this system in normal
form, and conﬁrm that this is consistent with the Hopf bifurcation theorem.
Use MATLAB to solve this system of equations numerically. Plot how the
limit cycle solution changes with µ, and determine for what range of values
of µ it exists.
13.11
(a) Seek an asymptotic solution of (13.17) subject to the boundary con-
dition (13.15), valid when V ≫1, by rescaling γ = V ¯γ and using an
asymptotic expansion
¯γ(β) = ¯γ0(β) + V −2¯γ1(β) + O(V −4).
Hence show that f(V ), as deﬁned in Section 13.3.3, satisﬁes
f(V ) = V + 1 −1
6V −1 + O(V −3) for V ≫1.
(b) Repeat part (a) when V ≪1. In this case, you will need to seek
a rescaling of the form β = φ(V )ˆβ, γ = ψ(V )ˆγ, and determine φ
and ψ by seeking an asymptotic balance in (13.17) and (13.15). You
should ﬁnd that, at leading order,
dˆγ
dˆβ
= −1 −
ˆβ2
ˆγ ,
subject to ˆγ ∼−ˆβ as ˆβ →0.
Integrate this equation numerically using MATLAB, and hence show
that
f(V ) ∼β∗V 2 as V →0,
where β∗is a constant that you should determine numerically.
13.12
Project Consider the CSTR system that we studied in Section 13.3.1, but
where, in addition, the autocatalyst is itself unstable, and breaks down to
form the ﬁnal product C through the chemical reaction
B →C,
rate k2b.

416
STABILITY, INSTABILITY AND BIFURCATIONS
(a) Show that the concentrations of A and B now satisfy the equations
dα
dτ = −αβ2 + 1 −α
τres
,
(E13.1)
dβ
dτ = αβ2 + β0 −β
τres
−β
τ2
,
(E13.2)
where τ2 is a dimensionless constant that you should determine.
(b) Using the ideas that we developed in Section 13.3.1, show that the
steady state solutions are again given by the points of intersection
of a cubic polynomial and a straight line through the origin. With-
out making any quantitative calculations, sketch the position of the
steady states in the (τres, z)-plane, ﬁrstly when there is no autocat-
alyst in the inﬂow, and secondly when there is.
(c) Now restrict your attention to the case where there is no autocatalyst
in the inﬂow. Determine the range of values for which there are
three steady state solutions. Show that the smallest of these steady
states is stable and that the middle steady state is a saddle point.
Show that the largest steady state loses stability through a Hopf
bifurcation, whose location you should determine.
Use the Hopf
bifurcation theorem to determine when this is supercritical and when
it is subcritical.
(d) Use MATLAB to integrate (E13.1) and (E13.2) numerically, and
hence investigate what happens to the limit cycle that forms at the
Hopf bifurcation point.
Draw the complete bifurcation diagram,
indicating the location of any limit cycles. What advice would you
give to an engineer trying to maximize the output of C from the
CSTR? Hint: For some parameter ranges, there is more than one
limit cycle.

CHAPTER FOURTEEN
Time-Optimal Control in the Phase Plane
Many physical systems that are amenable to mathematical modelling do not exist
in isolation from human intervention.
A good example is the British economy,
for which the Treasury has a complicated mathematical model. The state of the
system (the economy) is given by values of the dependent variables (for example,
unemployment, foreign exchange rates, growth, consumer spending and inﬂation),
and the government attempts to control the state of the system to a target state
(low inﬂation, high employment, high growth) by varying several control parameters
(most notably taxes and government spending). There is also a cost associated with
any particular action, which the government tries to minimize (some function of,
for example, government borrowing and, one would hope, the environmental cost
of any government action or inaction). The optimal control leads to the economy
reaching the target state with the smallest possible cost.
Another system, for which we have studied a simple mathematical model, con-
sists of two populations of diﬀerent species coexisting on an isolated island. For the
case of two herbivorous species, which we studied in Chapter 9, we saw that one
species or the other will eventually die out. If the island is under human manage-
ment, this may well be undesirable, and we would like to know how to intervene to
maintain the island close to a state of equilibrium, which we know, if left uncon-
trolled, is unstable. We could choose between either continually culling the more
successful species, continually introducing animals of the less successful species or
some combination of these two methods of control. Each of these actions has a cost
associated with it.
These are examples of optimal control problems. Optimal control is a huge topic,
and in this short, introductory chapter we will study just about the simplest type of
problem, which involves linear, constant coeﬃcient ordinary diﬀerential equations.
These are, however, extremely important, as it is often necessary to control small
deviations from a steady state, for example when steering a ship, for which linear
equations are a good approximation. We will also restrict our attention to time-
optimal control, for which the cost function is the time taken for the system to reach
the target state. We wish to drive the system to the target state in the shortest
possible time.
The crucial results that we will work towards are the properties of the controlla-
bility matrix and the application of the time-optimal maximum principle. Although
we will give proofs of the various results that we need, the main thing is to know
how to apply them.

418
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.1
Deﬁnitions
Let’s begin by formalizing some of the ideas that we introduced above. Consider
the system of ordinary diﬀerential equations
˙x = f(x, u, t),
subject to x(0) = x0.
Note that we use the superscript 0 to indicate the initial state and the superscript 1
to indicate the ﬁnal state. We say that x(t) is the state vector, whose components
x = (x1(t), x2(t), . . . , xn(t)) are the state variables, and u is the control vector,
whose components u = (u1(t), u2(t), . . . , um(t)) are the control variables. We
assume that f is continuously diﬀerentiable with respect to x, u and t, but that u is
merely integrable, so that we can allow for discontinuous changes in its components,
the control variables. As we have seen, these conditions guarantee the existence
and uniqueness of the solution for a given control u.
A control problem takes the form of a question: if x = x0 when t = 0, can we
choose the control vector u(t) so that x = x1, the target state, when t = t1? In
other words, can the system be controlled from x0 to x1, reaching x1 when t = t1?
If there is a cost function,
J =
 t1
0
g0(x(t), u(t), t) dt,
associated with the control problem, such that we seek controls u for which J is a
minimum, we have an optimal control problem. If g0 = 1, and hence J = t1,
so that we seek to minimize the time taken to reach the state x1, we have a time-
optimal control problem. This is the type of problem that we will be studying.
In particular, for ﬁrst order, linear, constant coeﬃcient equations,
dx
dt = Ax + Bu(t),
and for second order, linear, constant coeﬃcient equations,
dx1
dt = A11x1 + A12x2 + B11u1(t) + B12u2(t),
dx2
dt = A21x1 + A22x2 + B21u1(t) + B22u2(t).
We can write this more concisely using matrix notation as
dx
dt = Ax + Bu,
where A and B are constant, 2 × 2 matrices.
Note that there are at most n
independent control variables for an nth order linear system of this form.
14.2
First Order Equations
A good way of introducing many of the important, basic concepts of control theory
is to study one-dimensional systems, in other words, ﬁrst order, linear ordinary
diﬀerential equations with constant coeﬃcients.

14.2 FIRST ORDER EQUATIONS
419
Example: The tightrope walker
A tightrope walker is inherently unstable. Walking a tightrope is rather like trying
to balance a pencil on its tip. Although the walker has an equilibrium position,
it is unstable, and she must push herself back towards the vertical to stay up-
right. Moreover, if her deviations from the vertical become too large, they cannot
be controlled, and she falls oﬀthe tightrope.
This is typical of many types of
problem where the idea is to control small deviations from equilibrium as quickly
as possible – linear, time-optimal control.
A very simple model for the tightrope walker is
dx
dt = x + u,
(14.1)
where x represents her angular deviation from the vertical and u her attempts to
control her balance. Let’s assume that x = x0 > 0 when t = 0. Can this deviation
from the vertical be controlled at all? If so, how quickly can it be controlled, and
what should the control be? If there is no attempt to control the deviation, u = 0,
x = x0et and the tightrope walker falls oﬀ. The upright position, x = 0, is an
unstable equilibrium state. For a general control u(t), we can solve (14.1) using the
integrating factor e−t, so that
d
dt(e−tx) = e−tu(t),
and hence
x = x0et + et
 t
0
e−τu(τ) dτ
(14.2)
Since we want to control the system to x = 0 when t = t1, we must have
x0 +
 t1
0
e−τu(τ) dτ = 0.
(14.3)
Clearly, since x0 > 0, u must be negative for at least some of the period 0 ⩽
t ⩽t1.
Now, if u(t) is not bounded below (the tightrope walker can apply an
arbitrarily large restoring force), it is easy to choose u to satisfy (14.3), for example
with u(t) = −etx0/t1. This is unrealistic, since we can control the system in an
arbitrarily short time t1 by making |u| suﬃciently large. The time-optimal control
problem is meaningless if the control is unbounded. From now on we restrict our
attention to bounded controls with −1 ⩽u(t) ⩽1. We will see later how to scale
a slightly more realistic problem so that u lies in this convenient range. In general,
the components of a bounded control vector satisfy −1 ⩽ui(t) ⩽1.
Intuitively, we can see that to push x back towards equilibrium as quickly as
possible, we need to push in the appropriate direction as hard as we can by taking
u(t) = −1, so that (14.3) gives
x0 −
 t1
0
e−τ dτ = 0,
and hence
t1 = −log(1 −x0),
(14.4)

420
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
with solution
x(t) = 1 + (x0 −1)et.
(14.5)
For t > t1 we take u = 0, and the system remains in its equilibrium state, x = 0.
Equation (14.4) shows that t1 →∞as x0 →1−. If x0 ⩾1, the system cannot
be controlled back to equilibrium. The tightrope walker cannot push hard enough,
and she falls oﬀ. Similarly, if x0 < 0, the time-optimal control is u(t) = 1, pushing
in the other direction. Some time-optimal solutions are shown in Figure 14.1.
Fig. 14.1. Some time-optimal solutions, (14.5), of the tightrope walker problem.
We can now make a couple of deﬁnitions. The controllable set at time t1
is the set of initial states that can be controlled to the origin in time t1. For the
tightrope walker problem, this is the set
C(t1) =

x | |x| ⩽1 −e−t1
.
The controllable set is the set of initial states that can be controlled to the origin
in some ﬁnite time. This is just the union of all the controllable sets at time t1 ⩾0.
For the tightrope walker problem, the controllable set is
C =
=
t1⩾0
C(t1) = {x | |x| < 1} .
If C is the whole real line, C = R, we say that the system is completely control-
lable. The tightrope walker system is not completely controllable, since C ̸= R
(see Exercise 14.1). This is a good point at which to deﬁne the reachable set. If

14.2 FIRST ORDER EQUATIONS
421
x = x0 when t = 0, the reachable set in time t1, R(t1, x0), is the set of points x1
for which there exists a control u(t) such that x(t1) = x1. From (14.2),
x1 = x0et1 + et1
 t1
0
e−τu(τ) dτ,
and hence, since |u(t)| ⩽1,
x1e−t1 −x0 =

 t1
0
e−τu(τ) dτ
 ⩽
 t1
0
e−τ|u(τ)| dτ ⩽
 t1
0
e−τ dτ = 1 −e−t1,
so that
(x0 −1)et1 + 1 ⩽x1 ⩽(x0 + 1)et1 −1,
(14.6)
deﬁnes the points in the reachable set, R(t1, x0).
For |x0| ⩾1, the reachable
set does not contain the origin, whilst for |x0| < 1 the origin is reachable for
t1 ⩾−log(1 −|x0|), consistent with what we know about the controllable set.
The reachable set is shown for two diﬀerent cases in Figure 14.2. Note that the
boundaries of the reachable set are given by the solutions with the controls u(t) =
±1, a fact that will prove to be important later.
t1
x1
x1
Fig. 14.2. The reachable set for the tightrope walker problem lies between the curved
lines.

422
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.3
Second Order Equations
For second order, linear, constant coeﬃcient systems, we can examine the behaviour
of the state variables, x1 and x2, in the (x1, x2)-phase plane (see Chapter 9). We
will show that our intuitive notion that only the extreme values of the bounded
control are used in the time-optimal control is correct. This is known as bang-
bang control. Before we can do this, there are a few mathematical ideas that we
must consider. All of the following can be generalized to systems of higher order,
and some of it to nonlinear, nonautonomous systems.
14.3.1
Properties of sets of points in the plane
— A convex set, S, is one for which the line segment between any two points in S
lies entirely within S (see Figure 14.3 for some examples). Note that necessary
but not suﬃcient conditions are that S must be both connected (any two points
in S can be joined by a curve lying within S) and simply-connected (S has no
holes†). Formally, if S is a convex set and x, y ∈S, cx + (1 −c)y ∈S for all c
such that 0 ⩽c ⩽1.
Fig. 14.3. Some examples of convex and nonconvex sets of points in the plane.
† More formally, S is connected and any closed loop lying in S can be shrunk continuously to a
point without leaving S.

14.3 SECOND ORDER EQUATIONS
423
— An interior point of S is a point x ∈S for which there exists a disc of points
centred on x, all of which lie in S (see Figure 14.4).
— The interior, Int(S), of a set S is the set of all the interior points of S.
— An exterior point of S is a point in the interior of SC, the complement of S.
— The exterior, Ext(S), of a set S is the set of all the exterior points of S.
— A boundary point of S is a point, not necessarily in S, that lies in neither the
interior nor the exterior of S (see Figure 14.4). Note that all discs centred on a
boundary point of S contain a point that is not in S.
— The boundary of a set S is the set of all boundary points of S, and can therefore
be written as
∂S = {x | (x ̸∈Int(S))} ∩

x | (x ̸∈Int(SC))

.
Fig. 14.4. Examples of interior and boundary points of S, and discs centred on them.
— If S does not contain any of its boundary points, it is said to be open. For
example, the set of points with |x| < 1 (the open unit disc) is open. Note that
the boundary of the open unit disc is the unit circle, |x| = 1. An open set has
S = Int(S).
— A closed set contains all of its boundary points. For example, the set of points
with |x| ⩽1 (the closed unit disc) is closed.
— A set S is strictly convex if, for each pair of points in S, the line segment joining
them is entirely made up of interior points (see Figure 14.5). For a strictly convex
set, the tangent to any boundary point does not meet S at any other boundary
point. If S is convex, but not strictly convex, some of the tangents to the set will
meet the boundary at more than one point along a straight part of the boundary
(see Figure 14.6).

424
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.5. Examples of strictly convex and convex, but not strictly convex sets.
Fig. 14.6. Examples of tangents to convex and strictly convex sets.
14.3.2
Matrix solution of systems of constant coeﬃcient ordinary
diﬀerential equations
Consider the system of n diﬀerential equations
dx
dt = Ax + b(t),
subject to x(0) = x0,
(14.7)
with A an n × n matrix of constants. In order to be able to write the solution of
this equation in a compact form, we deﬁne the matrix exponential of At to be
exp(At) =
∞

k=0
Aktk
k! ,
(14.8)
with A0 = I, the unit matrix. Note that this power series is convergent for all A
and t. We can see immediately that
d
dt exp(At) =
∞

k=1
Aktk−1
(k −1)! = A exp(At),
a property that the matrix exponential shares with its scalar counterpart, eat.

14.3 SECOND ORDER EQUATIONS
425
Now consider the product
P(t) = exp(At) exp(−At).
Firstly, we note that P(0) = I. Secondly, using the product rule,
dP
dt = A exp(At) exp(−At) −exp(At)A exp(−At) = 0,
using the fact that, from the deﬁnition, (14.8),
A exp(At) = exp(At)A.
Similarly, d nP/dtn = 0, and hence from its Taylor series expansion, P(t) = I. In
other words,
{exp(At)}−1 = exp(−At),
again, in line with the result eate−at = 1 for the scalar exponential function. Note
that, in general, exp(A) exp(B) ̸= exp(B) exp(A) unless AB = BA (see Exer-
cise 14.4).
These results mean that when b(t) = 0, so that (14.7) is homogeneous, the
solution can be written as
x = exp(At)x0.
When the equation is inhomogeneous, we can use a matrix integrating factor to
write
exp(−At)dx
dt −exp(−At)Ax = d
dt {exp(−At)x} = exp(−At)b,
and hence,
x = exp(At)

x0 +
 t
0
exp(−Aτ)b(τ) dτ
	
.
(14.9)
This is the generalization to an nth order system of the ﬁrst order solution, an
example of which is given by (14.2).
Finally, note that we know from the Cayley–Hamilton theorem (Theorem A1.1)
which states that every matrix satisﬁes its own characteristic equation, that, for
an n × n matrix A and k ⩾n, Ak can be written as a linear combination of
I, A, A2, . . . , An−1, and hence so can exp(At). In particular, for a 2 × 2 matrix,
exp(At) is a linear combination of I and A.
Example: Simple harmonic motion
Simple harmonic motion with angular frequency ω is governed by
d 2x
dt2 + ω2x = 0.
We can write this as a system of two ﬁrst order equations in the usual way as
˙x1 = x2,
˙x2 = −ω2x1,

426
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
with x = x1. In terms of the generic equation (14.7),
A =

0
1
−ω2
0

.
We now note that
A2 =
 −ω2
0
0
−ω2

= −ω2I.
This means that
A2n = ω2n(−1)nI,
A2n+1 = ω2n(−1)nA,
and hence
exp(At) =
∞

k=0
Aktk
k!
=
∞

n=0
A2nt2n
(2n)! +
∞

n=0
A2n+1t2n+1
(2n + 1)!
= I
∞

n=0
(−1)nω2nt2n
(2n)!
+ A
∞

n=0
(−1)nω2nt2n+1
(2n + 1)!
= cos ωtI + 1
ω sin ωtA =
%
cos ωt
1
ω sin ωt
−ω sin ωt
cos ωt
&
.
With the initial condition x1(0) = x0
1, x2(0) = x0
2, this means that the solution is
x = x0 exp(At) =


x0
1 cos ωt + x0
2
ω sin ωt
−ωx0
1 sin ωt + x0
2 cos ωt

.
14.4
Examples of Second Order Control Problems
Example 1: The positioning problem
Consider the one-dimensional problem of positioning an object of mass m in a
frictionless groove using a bounded applied force F(t) such that −Fmax ⩽F(t) ⩽
Fmax. Newton’s second law gives
F(t) = md 2x
dt2 ,
subject to x(0) = X, ˙x(0) = 0, x(t1) = 0, ˙x(t1) = 0,
minimizing t1. If we let
x1 = Fmax
m x,
x2 = Fmax
m
˙x,
u = F(t)
Fmax
,
x0
1 = Fmax
m X,
we have a problem in the standard form, with |u| ⩽1,
dx1
dt = x2,
dx2
dt = u,
x1(0) = x0
1,
x2(0) = 0.
(14.10)

14.4 EXAMPLES OF SECOND ORDER CONTROL PROBLEMS
427
In terms of our matrix notation†
A =
 0
1
0
0

,
B =
 0
1

.
Let’s now think what the optimal control might be. We want to push the particle
to the origin as quickly as possible, but it must be at rest when we get it there.
Presumably we should push as hard as we can for some period of time, 0 ⩽t ⩽T,
then decelerate as strongly as we can by pushing as hard as possible in the opposite
direction for T ⩽t ⩽t1, so that the particle is at rest at the origin when t = t1.
We will prove later that this is indeed the optimal method of control, but for now
let’s just construct the solution.
Assuming that x0
1 > 0, we take u = −1 for 0 ⩽t ⩽T. We can easily integrate
(14.10) and ﬁnd that
x1 = x0
1 −1
2t2,
x2 = −t for 0 ⩽t ⩽T.
(14.11)
Then we take u = 1 for T ⩽t ⩽t1, and use x1 = x0
1 −1
2T 2, x2 = −T when t = T
to ﬁx the constants of integration, which gives us
x1 = 1
2t2 −2Tt + T 2 + x0
1,
x2 = t −2T
for T ⩽t ⩽t1.
(14.12)
Now we just need to ﬁnd T, the time at which we have to switch from accelerating
as hard as possible to decelerating as hard as possible, and t1, the total time taken
to control the particle to the origin. We can obtain this by using x1 = x2 = 0 when
t = t1. This gives T = 1
2t1, so that the periods of acceleration and deceleration are
equal, and t1 = 2
#
x0
1. The full solution is
x1 =



x0
1 −1
2t2
for 0 ⩽t ⩽
#
x0
1,
1
2t2 −2
#
x0
1t + 2x0
1
for
#
x0
1 ⩽t ⩽2
#
x0
1,
(14.13)
x2 =



−t
for 0 ⩽t ⩽
#
x0
1,
t −2
#
x0
1
for
#
x0
1 ⩽t ⩽2
#
x0
1.
(14.14)
A typical solution is plotted as a function of t, and various solutions plotted in
the (x1, x2)-phase plane in Figure 14.7. We will return later to the problem of
determining the time-optimal control if the particle is not initially stationary, so
that x2(0) ̸= 0.
Example 2: The steering problem / The positioning problem with friction
The forced motion of a ship is unstable, and tends to drift oﬀcourse if it is not
controlled. If x1 represents the deviation of the ship from a straight path, we can
† Note that, strictly speaking, the matrix B should have another column of zeros, since there is
no dependence on a second control function in this problem. We have omitted this for clarity.

428
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.7. Some time-optimal solutions of the positioning problem when the particle is
initially stationary.
write a simple model as
dx1
dt = x2,
dx2
dt = −qx2 + u.
(14.15)
The drag force caused by the resistance of the water to the lateral motion of the
ship is represented by the term −qx2, with q a positive constant. This is exactly
the same as the positioning problem, but in a groove with friction. In terms of our
matrix notation,
A =
 0
1
0
−q

,
B =
 0
1

.
Example 3: Controlling a linear oscillator
The equation for a linear oscillator (simple harmonic motion) of unit angular fre-
quency, subject to an external force, u(t), can be written as
dx1
dt = x2,
dx2
dt = −x1 + u.
(14.16)
In terms of our matrix notation,
A =

0
1
−1
0

,
B =
 0
1

.

14.5 PROPERTIES OF THE CONTROLLABLE SET
429
If the oscillator is not initially at rest and we wish to bring it to a halt as quickly
as possible, we must solve (14.16) subject to
x1(0) = x0
1,
x2(0) = x0
2,
x1(t1) = x2(t1) = 0,
(14.17)
minimizing t1. We can think of this as the problem of stopping a child on a swing
as quickly as possible.
Example 4: The positioning problem with two controls
Suppose that in the positioning problem we are also able to change the velocity of
the particle through an extra control, so that
dx1
dt = x2 + u1,
dx2
dt = u2.
(14.18)
In terms of our matrix notation,
A =
 0
1
0
0

,
B =
 1
0
0
1

.
With the exception of the steering problem (see Exercise 14.9), we will solve all
of these problems below after we have studied some more of the theory relevant to
this type of control problem.
14.5
Properties of the Controllable Set
Apart from determining the time-optimal control, we often want to construct C(t1),
the controllable set for any time t1. There are a number of statements that we can
make about the geometry of these sets, and of C, the controllable set.
— If t1 < t2, C(t1) ⊂C(t2). In other words, the controllable set never gets smaller
as t increases. Any state controllable to zero in time t1 is also controllable to
zero in time t2 > t1.
— C(t) and C are connected sets.
— C is open if and only if 0 ∈Int(C).
These results hold for nonlinear, nonautonomous systems of ordinary diﬀerential
equations. For linear, constant coeﬃcient equations, we also have that C(t) and C
are symmetric about the origin and convex. Note that this also implies that these
sets are simply-connected, since all convex sets are simply-connected.
Theorem 14.1 If t1 < t2, C(t1) ⊂C(t2).
Proof Let x0 be a point in C(t1), with control u = v(t). If we apply the control
u =
 v(t)
for 0 ⩽t ⩽t1,
0
for t1 < t ⩽t2,
the trajectory reaches x = 0 when t = t1, then remains there for t1 ⩽t ⩽t2, since
x = 0, u = 0 is an equilibrium state. Therefore x0 ∈C(t2), which means that
C(t1) ⊂C(t2) (see Figure 14.8).

430
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.8. If t1 < t2, C(t1) ⊂C(t2).
Theorem 14.2 If x0 ∈C(t1) and y is a point on the trajectory from x0 to 0,
y ∈C(t1). In other words, all points on controllable trajectories are controllable.
Proof Let x = X(t) be the trajectory containing both x0 and y, with control u(t).
When t = τ1, X(τ1) = y, and when t = τ2, X(τ2) = 0, with τ1 ⩽τ2 ⩽t1 (see
Figure 14.9). If we now consider the solution with control v(t) = u(t + τ1) and
Fig. 14.9. All points on controllable trajectories are controllable.
initial condition x(0) = y, the system follows the same trajectory, x = X(t + τ1),
and reaches x = 0 when t = τ2−τ1. Therefore y ∈C(τ2−τ1) and, by Theorem 14.1,
y ∈C(t1).

14.5 PROPERTIES OF THE CONTROLLABLE SET
431
Theorem 14.3 C(t1) and C are connected sets.
Proof
If x0 ∈C(t1) and y0 ∈C(t1), there are, by deﬁnition, trajectories that
connect each point to the origin (see Figure 14.10). By Theorem 14.2, all points on
Fig. 14.10. C(t1) and C are connected sets.
each of these trajectories lie in C(t1). Therefore the union of these two trajectories
is a curve made up of points in C(t1) that connects x0 and y0, and hence, by
deﬁnition, C(t1) is connected. Since C = >
t1⩾0 C(t1), C is also connected.
Theorem 14.4 C is open if and only if 0 ∈Int(C).
Proof If C is open, all of its points are interior points, so clearly 0 ∈Int(C). It is
less straightforward to prove that 0 ∈Int(C) implies that C is open.
If 0 ∈Int(C), by deﬁnition there is a disc of radius r centred on 0, which
we write as D(0, r), that lies entirely within C. Now suppose that u = v(t) is
a control that steers some point x0 to 0 in time t1. Let D(x0, r0) be a disc of
radius r0 centred on x0, and let y0 be another point within this disc, as shown in
Figure 14.11. By continuity of the solutions of the underlying diﬀerential equations,
if r0 is suﬃciently small, the control v(t) steers y0 into the disc D(0, r) on a path
y(t) with y(t1) ∈D(0, r) at some time t1. Since D(0, r) ∈C, we can also ﬁnd a
control ˆv(t) that steers y(t1) to 0 in some time t2. Therefore y0 can be controlled
to the origin in time t1 + t2 using the control
u(t) =
 v(t)
for 0 ⩽t ⩽t1,
ˆv(t)
for t1 < t ⩽t2.
Therefore y0 ∈C(t1 + t2) ⊂C, and hence, for r0 suﬃciently small, D(x0, r0) ∈C
for all x0 ∈C. By deﬁnition, C is therefore open.

432
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.11. C is open if and only if 0 ∈Int(C).
Theorems 14.1, 14.2, 14.3 and 14.4 all hold for nonlinear, nonautonomous sys-
tems of ordinary diﬀerential equations. We now focus on linear, constant coeﬃcient
equations. Recall that if
dx
dt = Ax + Bu,
with A and B constant matrices, the solution is
x = exp(At)

x0 +
 t
0
exp(−Aτ)Bu(τ) dτ
	
.
This means that x0 ∈C(t1) if and only if there is a control u(t) such that
x0 = −
 t1
0
exp(−Aτ)Bu(τ) dτ.
(14.19)
Theorem 14.5 C(t1) is symmetric about the origin and convex.
Proof If x0 ∈C(t1) with control u(t), (14.19) shows that −x0 ∈C(t1) with control
−u(t).† Therefore C(t1) is symmetric about the origin.
Now note that the set of bounded controls
U = {u(t) | −1 ⩽ui(t) ⩽1}
is convex, since if u0 ∈U and u1 ∈U, cu0 +(1−c)u1 ∈U for 0 ⩽c ⩽1. Therefore,
if u0 and u1 are controls that steer x0 and x1 to the origin in time t1,
cx0 + (1 −c)x1 = −
 t1
0
exp(−Aτ)B

cu0(τ) + (1 −c)u1(τ)

dτ,
† Note that the fact that the control variables are scaled so that −1 ⩽ui(t) ⩽1 is crucial here.

14.6 THE CONTROLLABILITY MATRIX
433
and hence the control cu0(τ) + (1 −c)u1(τ) ∈U steers cx0 + (1 −c)x1 to the origin
in time t1. The line segment that joins x0 to x1 therefore lies entirely within C(t1),
and hence, by deﬁnition, C(t1) is convex.
Theorem 14.6 C is symmetric about the origin and convex.
Proof A union of symmetric sets is symmetric, so C = >
t1⩾0 C(t1) is symmetric.
Although a union of convex sets is not necessarily convex (see Figure 14.12), since
Theorem 14.3 tells us that C(t1) ⊂C(t2) for t1 < t2, C is a union of nested convex
sets and therefore is itself convex.
Fig. 14.12. A union of convex sets is not necessarily convex.
14.6
The Controllability Matrix
For a second order system, we deﬁne the controllability matrix to be
M = [B AB].
(14.20)
We will show that the system is completely controllable (C = R2) if and only if
(i) rank(M) = 2,
(ii) all the eigenvalues of A have zero or negative real part.
Although we will not do so here, it is straightforward to generalize this result to
nth order systems.
Example: The positioning problem
For the positioning problem
A =
 0
1
0
0

,
B =
 0
1

,
and hence
M =
 0
1
1
0

.
Clearly, rank(M) = 2, since its columns are linearly independent. Also, since
det(A −λI) = λ2,

434
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
the matrix A has the repeated eigenvalue zero. We conclude that the system is
completely controllable. All of the other examples that we described in Section 14.4
are also completely controllable (see Exercise 14.6).
Let’s now prove that these properties of the controllability matrix do determine
whether or not a second order, linear, constant coeﬃcient system is controllable.
Theorem 14.7 0 ∈Int(C) if and only if rank(M) = 2.
Proof Suppose that rank(M) < 2. If rank(M) = 1, there is a single direction, y,
orthogonal to every column of M. This means that yTB = yTAB = 0, and hence
by the Cayley–Hamilton theorem,
yTAkB = 0 for k = 0, 1, 2, . . . .
This means that yT exp(−Aτ)B = 0. Now, if x0 ∈C(t1), (14.19) shows that
yTx0 = y · x0 = −
 t1
0
yT exp(−Aτ)Bu(τ)dτ = 0.
Therefore if rank(M) = 1, x0 lies on the straight line through the origin perpen-
dicular to y, which is a closed set, and hence 0 ̸∈Int(C). If rank(M) = 0, M has
only zero entries, and hence so does B. There are therefore no controls, C = {0},
and hence 0 ̸∈Int(C).
Now suppose that 0 ̸∈Int(C). Since C(t1) ⊂C, 0 ̸∈Int(C(t1)) at any time t1.
Since 0 ∈C(t1), the origin must be a boundary point of C(t1). Since C(t1) is convex
(Theorem 14.5), there is a tangent to C(t1) through 0 with outward normal z, and
for all x0 ∈C(t1), zTx0 = z · x0 ⩽0 (see Figure 14.13). Equation 14.19 then shows
Fig. 14.13. The tangent and outward normal to the set C(t1) at the origin.

14.6 THE CONTROLLABILITY MATRIX
435
that
 t1
0
zT exp(−Aτ)Bu(τ) dτ ⩾0
for all controls u. However, since −u is also an admissible control,
−
 t1
0
zT exp(−Aτ)Bu(τ) dτ ⩾0,
and hence
 t1
0
zT exp(−Aτ)Bu(τ) dτ = 0 for 0 ⩽t ⩽t1.
Since this must hold for all controls u,
zT exp(−At)B = 0 for 0 ⩽t ⩽t1.
If we now set t = 0 we get zTB = 0, and if we diﬀerentiate and put t = 0,
zTAB = 0. This means that z is orthogonal to all of the columns of M, and hence
rank(M) < 2.
Since the system can only be completely controllable if 0 ∈Int(C), this theorem
shows that a necessary condition for the system to be completely controllable is
that rank(M) = 2.
Theorem 14.8 If rank(M) = 2 and Re(λi) ⩽0 for each eigenvalue λi of A, the
system is completely controllable.
Proof We proceed by contradiction. Suppose that rank(M) = 2 and A has eigen-
values with zero or negative real parts, but that C ̸= R2. Consider a point y ̸∈C.
There is then a tangent to C, with equation n · x = p, that separates y from each
x0 ∈C, with n · x0 ⩽p and n · y ⩾p (see Figure 14.14). Let z = n {exp(−At)B}.
Because rank(M) = 2, z ̸= 0 for 0 ⩽t ⩽t1. Now choose a control with components
ui(t) = −sgn(zi(t)), so that
n · x0 = −
 t1
0
nT exp(−Aτ)Bu(τ) dτ
= −
 t1
0
z(τ)u(τ) dτ =
 t1
0
(|z1| + |z2|) dτ.
By choosing an appropriate coordinate system, we can make each component of z
a sum of terms proportional to e−λiτ. If any eigenvalue has zero part, the corre-
sponding component of z will be either a polynomial or a periodic function of t.
In each case,
' t1
0 (|z1| + |z2|) dτ →∞as t1 →∞. In particular, for t1 suﬃciently
large, n · x0 > p. This is a contradiction, and we conclude that C = R2.
Theorem 14.9 If rank(M) = 2 and A has at least one eigenvalue with positive
real part, the system is not completely controllable.

436
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.14. A tangent to the set C that separates each x0 ∈C from y ̸∈C.
Proof
Let λ be an eigenvalue of A with positive real part, and e the associ-
ated eigenvector, so that eTA = λeT, and eTAk = λkeT.
This means that
eT exp(−Aτ) = e−λτeT, and hence
eTx0 = e · x0 = −
 t1
0
e−λτeTBu(τ) dτ.
This integral converges as t1 →∞, and is bounded above by some constant c, so
that e · x0 ⩽c. The controllable set therefore lies on one side of a line in the plane,
and hence C ̸= R2.
This concludes our proof that the controllability matrix has the properties that
we outlined at the start of this section.
14.7
The Time-Optimal Maximum Principle (TOMP)
Consider the set reachable from x0 in time t, R(t, x0). As time increases, this set
traces out a volume in (x1, x2, t)-space, which we label RT(t, x0). If the system can
be controlled to the origin, the shortest time in which this can be achieved is t∗,
where t∗is the ﬁrst time when t∗∈R(t∗, x0) (see Figure 14.15).
Theorem 14.10 The time-optimal trajectory lies in the boundary, ∂RT(t, x0).
Proof Let u = u∗(t) be the optimal control for x(0) = x0, and let y(t) be a solution
with u = u∗. Now suppose that y(t0) ∈Int(R(t0, x0)). There must therefore be
a disc of suﬃciently small radius r, D(y(t0), r), that lies entirely within R(t0, x0).
If we now apply the optimal control, u∗, to all of the points within D(y(t0), r),
they will lie in the neighbourhood of y(t1) when t = t1 > t0, and must also lie in
R(t1, x0). Therefore y(t1) ∈Int(R(t1, x0)), so that any trajectory that starts in

14.7 THE TIME-OPTIMAL MAXIMUM PRINCIPLE (TOMP)
437
Fig. 14.15. The reachable set, RT(t, x0).
Int(R(t, x0)) remains there. Since the origin lies in the boundary, ∂RT(t, x0), the
time-optimal trajectory must lie entirely within this boundary.
Theorem 14.11 (The time-optimal maximum principle (TOMP)) The con-
trol u(t) is time-optimal if and only if there exists a nonzero vector, h, such that
for 0 ⩽t ⩽t1,
hT {exp(−At)Bu(t)} = sup
v(t)
hT {exp(−At)Bv(t)} .
The components of the time-optimal control are
ui(t) = sgn
!
hT {exp(−At)B}
"
i
for i = 1, 2, . . . , m
– bang-bang control.
Proof Let u(t) be a control that steers x from x0 when t = 0 to x1 ∈∂R(t1, x0)
when t = t1. The reachable set is convex (this can be proved in the same way that
we proved that the controllable set is convex in Theorem 14.5), so there is a tangent
at x1 with normal n such that
n · x =
sup
y1∈R(t1,x0)
n · y1.
If v(t) is an arbitrary control and y(t) the corresponding solution,
y1 = exp(At1)

x0 +
 t1
0
exp(−Aτ)Bv(τ) dτ
	
,

438
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
and y1 = x1 when v = u. This means that
n · exp(At1)x0 + n ·

exp(At1)
 t1
0
exp(−Aτ)Bu(τ) dτ
	
= sup
v

n · exp(At1)x0 + n ·

exp(At1)
 t1
0
exp(−Aτ)Bv(τ) dτ
	
,
and hence

nT exp(At1)
  t1
0
exp(−Aτ)Bu(τ) dτ
= sup
v

nT exp(At1)
  t1
0
exp(−Aτ)Bv(τ) dτ.
If we let hT = nT exp(At1), we ﬁnd that
 t1
0
hT exp(−Aτ)Bu(τ) dτ = sup
v
 t1
0
hT exp(−Aτ)Bv(τ) dτ
	
.
(14.21)
Note that h is nonzero, because exp(At) is a nonsingular matrix since it always has
an inverse, exp(−At). Since (14.21) holds for all t1 > 0, we must have
hT exp(−Aτ)Bu(τ) = sup
v

hT exp(−Aτ)Bv(τ)

.
(14.22)
All of the steps of this argument can be reversed, and we have therefore proved the
ﬁrst part of the theorem.
To obtain the maximum value of the right hand side of (14.22), we must take
vi(t) = sgn
!
hT exp(−At)B
"
i
for 0 ⩽t ⩽t1.
In other words, each component of the time-optimal control must take one of its
extreme values, 1 or −1, and change when
!
hT exp(−At)B
"
i changes sign – bang-
bang control.
We shall see that this is the main result that we need to solve time-optimal control
problems in the phase plane.
Example: The positioning problem
For this problem
A =
 0
1
0
0

,
B =
 0
1

.
Since A2 = 0,
exp(−At) = I −At =
 1
−t
0
1

,
and
exp(−At)B =
 −t
1

.

14.7 THE TIME-OPTIMAL MAXIMUM PRINCIPLE (TOMP)
439
Therefore, if we write hT = (α, β), we ﬁnd that
hT exp(−At)B = β −αt.
This means that the time-optimal, bang-bang control changes sign at most once.
The TOMP does not tell us when, or even whether, this change in sign occurs.
However, with x1(0) = x0
1 and x2(0) = 0, an initially stationary particle, we have
seen that the control must change sign at least once if the solution is to reach
the origin. We also constructed the unique solution with bang-bang control that
changes sign just once. The TOMP now shows that this is indeed the time-optimal
solution.
Let’s now consider what happens if the particle is initially moving, with x1(0) =
x0
1 and x2(0) = x0
2. Rather than just solving the equations, let’s think about what
the time-optimal solution will look like in the phase plane.
We know that the
time-optimal control is bang-bang with u = ±1, so the integral paths are given by
dx1
dx2
= ±x2,
and hence are parabolas, with
x1 = k ± 1
2x2
2
for some constant of integration k. The only two time-optimal paths that reach the
origin are therefore the appropriate branches of x1 = ± 1
2x2
2, which are labelled as
S± in Figure 14.16. On S+ we need u = 1, whilst on S−we need u = −1. Any initial
conditions that lie on these curves can be controlled to the origin without changing
the sign of u.
For any other initial conditions, the system must be controlled
onto one of the curves S±, when the control must change sign, as illustrated in
Figure 14.16.
Example: Controlling a linear oscillator
For a linear oscillator with unit angular frequency,
exp(−At) =
 cos t
−sin t
sin t
cos t

,
and
exp(−At)B =
 −sin t
cos t

.
With hT = (α, β),
hT exp(−At)B = β cos t −α sin t.
The TOMP therefore shows that the time-optimal control is
u(t) = sgn(β cos t −α sin t) = sgn {a cos(t + b)} ,
for some constants a and b. Since cos(t + b) changes sign at intervals of π, so must
u(t). The ﬁrst change of sign occurs when t = T0 with 0 < T0 ⩽π, and must

440
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
Fig. 14.16. The time-optimal trajectories for the positioning problem.
subsequently change when t = Tn = T0 + nπ, for n = 0, 1, 2, . . . . Let’s consider the
solution when u = ±1 in the phase plane. The governing equations, (14.16), show
that
(x1 ∓1)dx1
dt + x2
dx2
dt = 0,
and hence that
(x1 ∓1)2 + x2
2 = k2,
for some constant of integration, k. These trajectories are circles, centred on (±1, 0),
as shown in Figure 14.17. Proceeding as we did for the positioning problem, we
can see that only the circles marked S± in Figure 14.17, given by (x∓1)2 +x2
2 = 1,
enter the origin. We can now construct the time-optimal solution by considering
the solutions that meet S±, with the control changing sign there. A typical example
is shown in Figure 14.18. The sign of the control must change with period π, so the
time-optimal solution consists of part of S+ or S−, and a succession of semicircles
of increasing radius with centres alternating between (1, 0) and (−1, 0). Intuitively,
we would perhaps have expected the control to change sign when x2 = 0. Thinking
in terms of stopping a child on a swing, we might have expected to push in the
opposite direction to the motion. In fact, the time-optimal solution is out of phase
with this intuitive solution in order to allow the velocity to be zero precisely when
the swing is vertical.

14.7 THE TIME-OPTIMAL MAXIMUM PRINCIPLE (TOMP)
441
Fig. 14.17. The bang-bang trajectories for the problem of controlling a linear oscillator.
Example: The positioning problem with two controls
For this problem
exp(−At)B =
 1
−t
0
1

.
With hT = (α, β),
hT exp(−At)B = (α, β −αt).
The TOMP therefore says that the time-optimal control u2 = sgn(β −αt) changes
sign at most once, as was the case for the positioning problem with just one control
variable. In contrast, u1 = sgn(α) does not change sign in the time-optimal control,
provided that α ̸= 0. If, however, α = 0, u2 does not change sign and u1 is unde-
termined. Let’s begin by considering this case, with u2 = −1. This immediately
gives us
x2 = x0
2 −t,
which is therefore only appropriate when x0
2 > 0.
The optimal control time is
therefore t1 = x0
2. If we now integrate the equation for x1, we obtain
x1 = x0
1 + x0
2t −1
2t2 +
 t1
0
u1(τ) dτ.

442
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
x1
x2
S¯
x
x
Fig. 14.18. A typical time-optimal trajectory for the problem of controlling a linear oscil-
lator.
Since x1(t1) = 0, we have a time-optimal trajectory for any control u1 such that
 x0
2
0
u1(τ) dτ = −x0
1 −1
2(x0
2)2.
However, since |u1| ⩽1, this can only be achieved when
−x0
1 −1
2(x0
2)2
 ⩽x0
2,
and hence when
−x0
2 −1
2(x0
2)2 ⩽x0
1 ⩽x0
2 −1
2(x0
2)2.
This is marked as region B in Figure 14.19. Similarly, when x0
2 < 0 and u2 = 1 we
have nonunique time-optimal paths in region A, where
x0
2 + 1
2(x0
2)2 ⩽x0
1 ⩽−x0
2 + 1
2(x0
2)2.
Outside these two regions, the time-optimal control is unique, with u1 = u2 = −1 in
region C and u1 = u2 = 1 in region D, as marked in Figure 14.19. In region C, each
time-optimal path meets the boundary of region A, where the sign of u1 changes,
and then follows the boundary of region A to the origin. Similarly in region D.
Note that, since t1 = |x0
2| in regions A and B where the time-optimal solution

EXERCISES
443
Fig. 14.19. The regions A, B, C and D, and some time-optimal solutions for the positioning
problem with two controls.
is not unique, the boundaries of C(t1), the reachable set in time t1, are straight
lines in these two regions. This means that C(t1) is not strictly convex. Although
we will not discuss this further, controllable sets that are not strictly convex are
always associated with nonunique time-optimal solutions.
In practice, the diﬃculty with applying bang-bang control lies in determining
when the control needs to change by making measurements of the state of the
system. This is known as measurement–action lag (see, for example, Marlin,
1995).
Exercises
14.1
We have seen that the tightrope walker system, ˙x = x+u, is not completely
controllable.
Show, by solving the governing equations, that the other
two qualitatively diﬀerent, ﬁrst order, linear, constant coeﬃcient systems,
˙x = −x + u and ˙x = u, are completely controllable.
14.2
For the tightrope walker system, ˙x = x + u(t) with |u| ⩽1, ﬁnd the
reachable set from x = 2 in time t1, R(t1, 2) and the set controllable to
x = 2 in time t1, C(t1, 2), and sketch them. Show that R(t1, 2) ̸⊂R(t2, 2)
and C(t1, 2) ̸⊂C(t2, 2) when t1 < t2.

444
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.3
Consider the system of ordinary diﬀerential equations
dx
dt = Ax + b(t),
subject to x(0) = x0 = (x0
1, x0
2)T, with b(t) = (b1(t), b2(t)). Determine the
exponential matrices, exp(At) and exp(−At), and hence write down both
the general solution and the solution when b is independent of t when A =
(i)
 0
0
1
0

(ii)
 1
0
0
2

(iii)

1
0
−q
0

.
14.4
Show that
exp(At) exp(Bt) = exp((A + B)t),
if and only if the matrices A and B commute.
14.5
Classify each of the following sets of points in the plane ﬁrstly as either
open, closed, or neither open nor closed, and secondly as either strictly
convex, convex but not strictly convex, or not convex.
(a)

(x1, x2) | (x1 −1)2 + x2
2 < 1

∪

(x1, x2) | x2
1 + x2
2 < 1

,
(b)

(x1, x2) | (x1 −1)2 + x2
2 ⩽1

∩

(x1, x2) | x2
1 + x2
2 ⩽1

,
(c)

(x1, x2) | x2
1 + x2
2 ⩽1

∩{(x1, x2) | x1 ⩾0} ,
(d)

(x1, x2) | x2
1 + x2
2 ⩽1

∩{(x1, x2) | x1 > 0} .
14.6
Use the controllability matrix to show that Examples 2, 3 and 4 given in
Section 14.4 are completely controllable systems.
14.7
If ˙x = Ax+Bu with u the vector of bounded controls, construct the control-
lability matrix, M, and hence determine whether the system is completely
controllable when
(a)
A =
 0
1
0
2

,
B =
 1
1

,
(b)
A =
 −1
1
0
0

,
B =
 1
1

,
(c)
A =
 −1
1
0
−1

,
B =
 1
0
0
1

.
14.8
Consider the positioning problem. Show that for initial conditions lying
above the curves S± the switching time is
T = x0
2 +
*
x0
1 + 1
2(x0
2)2,
and that the optimal control time is
t1 = x0
2 + 2
*
x0
1 + 1
2(x0
2)2.
Determine C(t1), the controllable set in time t1.

EXERCISES
445
14.9
Consider the steering problem. Show that
exp(−At) =

1
1
q (1 −eqt)
0
eqt

.
Use the TOMP to show that the time-optimal control has at most one
change of sign. If x1(0) = x0
1 and x2(0) = 0, show that the optimal control
time is
t1 = 2
q log

exp
1
2q2x0
1

+
$
exp (q2x0
1) −1
	
.
Show that the integral paths for u = ±1 are
x1 = k −1
q2 {qx2 ± log (1 ∓qx2)} ,
with k a constant. Sketch the time-optimal trajectories and the controllable
set in time t1 in the (x1, x2)-phase plane.
14.10
The population of a pest is increasing exponentially. To control the pest,
a genetically engineered, sterile, predatory beetle is introduced into the
environment.
Since the beetles are harmful to crops, it is desirable to
remove them as soon as possible.
Let the populations of the pest and the beetle be denoted by x1(t) and
x2(t) creatures per square metre respectively. Initially, x1 = X > 0 and
x2 = 0, and the target is to reduce both populations to zero simultaneously
and in the shortest possible time. Model equations for the two populations
are
˙x1 = x1 −x2,
˙x2 = −x2 + u(t),
with |u| ⩽1 representing the rate at which beetles are released into the
environment. Use the time-optimal maximum principle to show that the
optimal control changes sign just once.
Show that the optimal control time is
t1 = log

X + 1 +
√
X2 + 4X
2X −1

.
What is the upper limit on X below which the system is controllable?
14.11
Consider the system
˙x1 = x1 + x2,
˙x2 = −x2 + u(t),
subject to x1(0) = x0
1, x2(0) = x0
2, with |u(t)| ⩽1. Use the time-optimal
maximum principle to show that the sign of the optimal control changes
at most once. Show that when x2 is initially positive, the time-optimal
solution on which the control does not change sign has
x0
1 = −
(x0
2)2
2(1 + x0
2).

446
TIME-OPTIMAL CONTROL IN THE PHASE PLANE
14.12
Determine the matrices exp(−At) and exp(At) when
A =
1
√
2
 1
1
1
−1

.
Consider the second order system of ordinary diﬀerential equations
dx
dt = Ax + Bu,
where x is the vector of state variables, u the vector of bounded control
variables with |ui(t)| ⩽1, and
B =
 1
0

.
Use the time-optimal maximum principle to show that the sign of u(t)
can change no more than once in the time-optimal control. Show that the
time-optimal solutions on which the control does not change sign have
x0
1 = ±

sinh t1 −1
√
2 cosh t1 + 1
√
2

,
x0
2 = ±

−1
√
2 cosh t1 + 1
√
2

,
where t1 is the time taken to control the system to the origin.

448
AN INTRODUCTION TO CHAOTIC SYSTEMS
If we now deﬁne the dimensionless variables
x =
+
l
2a2(l −a)Y,
ˆt =
*
µ(l −a)
ma
t,
we obtain
¨x + ϵδ ˙x −x + x3 = F(ˆt ),
(15.2)
where
ϵδ = k
*
ma
µ(l −a),
F(ˆt ) = −

¨φ + k ˙φ
 +
2m2a4
µ2l(l −a).
The reason for our rather odd choice of dimensionless parameters will become clear
later. Equation (15.2) with F = 0 is known as Duﬃng’s equation, and arises
in many other contexts in mechanics. With F ̸= 0, (15.2) is the forced Duﬃng
equation.† We will assume that the forcing takes the form F = ϵγ cos ωt (dropping
the hat on the time variable for convenience), so that (15.2) becomes
¨x + ϵδ ˙x −x + x3 = ϵγ cos ωt.
(15.3)
a
y(t)
mky
m
φ(t)
.
Fig. 15.1. A mechanical system whose small amplitude oscillations are governed by the
forced Duﬃng equation, (15.3).
Figure 15.2 shows the solution, calculated numerically using MATLAB (see Sec-
tion 9.3.4), when ϵδ = 1
2, ϵγ = 3
5 and y = dy/dt = 0 when t = 0. This solution
† For more on the dynamics of the forced Duﬃng equation, see Arrowsmith and Place (1990)
and references therein.

CHAPTER FIFTEEN
An Introduction to Chaotic Systems
In order to introduce the idea of a chaotic solution, we will begin by studying
three simple chaotic systems that arise in diﬀerent physical contexts. We then look
at some examples of mappings, which are important because ordinary diﬀerential
equations can be related to mappings through the Poincar´e return map.
After
investigating homoclinic tangles in Poincar´e return maps, which contain chaotic
solutions, we investigate how their existence can be established by examining the
zeros of the Mel’nikov function. Finally, we discuss the computation of the Lya-
punov spectrum of a diﬀerential equation, from which a quantitative measure of
chaos can be obtained.
15.1
Three Simple Chaotic Systems
15.1.1
A Mechanical Oscillator
Consider the mechanical system that consists of two rings of mass m threaded
onto two horizontal wires a distance a apart, as shown in Figure 15.1. The rings
are joined by a spring of natural length l > a that obeys Hooke’s law with elastic
constant µ. If we move the upper ring, what happens to the lower ring? We denote
the displacement of the upper ring from a ﬁxed vertical line by φ(t), and that of
the lower ring by y(t). On the assumption that a frictional force of magnitude
mk ˙y opposes the motion of the lower ring, Newton’s second law in the horizontal
direction shows that
−µ
#
a2 + (y −φ)2 −l

(y −φ)
#
a2 + (y −φ)2
−mk ˙y = m¨y,
(15.1)
where a dot denotes d/dt. Let’s assume that the relative displacement of the two
rings, Y = y −φ, is much less than the distance, a, between the wires, so that
#
a2 + (y −φ)2 = a
*
1 + Y 2
a2 ∼a + Y 2
2a .
In terms of Y , (15.1) becomes
¨Y + k ˙Y −µ(l −a)
ma
Y +
µl
2ma3 Y 3 ∼−¨φ −k ˙φ.

15.1 THREE SIMPLE CHAOTIC SYSTEMS
449
behaves erratically. Indeed, it is tempting to think of it as ‘random’ in some sense.
However, we know from Chapter 8 that the solution of (15.3) exists and is unique.
Figure 15.3 superimposes another solution, this time with y = 10−4, dy/dt = 0
when t = 0. The two solutions begin close to each other, but, as time increases,
drift further apart, and soon diverge completely. We say that the system exhibits
sensitive dependence upon initial conditions. In practice, we can only know
the initial state of a physical system with a ﬁnite degree of accuracy. After a suf-
ﬁcient time has elapsed, solutions with diﬀerent initial conditions, but which are
close enough together that, in practice, they are indistinguishable, will diverge in a
chaotic system.
Fig. 15.2. The solution of the forced Duﬃng equation, (15.3), when ϵδ = 1
2, ϵγ = 3
5 and
y = dy/dt = 0 when t = 0.
We can now give an informal deﬁnition of a chaotic solution as a bounded,
aperiodic, recurrent solution, that has a random aspect due to its sensitive depen-
dence on initial conditions. Adjacent chaotic solutions diverge exponentially fast,
a property that we will later measure using the Lyapunov spectrum, and remain
in a bounded region, where they undergo repeated folding, and are, in practice,
unpredictable in the long term.

450
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.3. The solution of the forced Duﬃng equation, (15.3), when ϵδ = 1
2, ϵγ = 3
5 and
y = dy/dt = 0 when t = 0 (solid line) and y = 10−4, dy/dt = 0 when t = 0 (broken line).
15.1.2
A Chemical Oscillator
The next example that we will consider is a well-stirred system of reacting chem-
icals, known as the cubic crosscatalator, which has at its heart the cubic auto-
catalytic step that we studied in Chapter 13. The reaction scheme is
P →A rate k0p, precursor decay,
(15.4)
P + C →A + C rate k1pc, catalysis of precursor decay,
(15.5)
A →B rate kua, uncatalyzed conversion,
(15.6)
A + 2B →3B rate k1ab2, cubic autocatalysis,
(15.7)
B →C rate k2b, autocatalyst decay,
(15.8)
C →D rate k3c, catalyst decay.
(15.9)
In addition to the reactant, A, and autocatalyst, B, there is a precursor, P, which
decays to produce A, and a catalyst, C, which accelerates the decay of the pre-
cursor, and is produced by the decay of the autocatalyst. The action of C both at

15.1 THREE SIMPLE CHAOTIC SYSTEMS
451
the start of the reaction cascade, catalyzing the decay of the precursor, and as a
product at the end of the sequence P →A →B →C is the essential ingredient that
leads to complex behaviour. The reactant A also decays spontaneously to produce
B, and C is itself unstable, decaying to the inert product D.
Under the assumption that the precursor, P, is in large excess and decays slowly,
we can derive the dimensionless governing equations (see Exercise 15.1)
˙α
=
κ (1 + ηγ) −αβ2 −ϵα,
˙β
=
αβ2 −β + ϵα,
(15.10)
˙γ
=
β −χγ,
where α, β and γ are the dimensionless concentrations of A, B and C, and κ,
η, ϵ and χ are dimensionless constants.
Although this system possesses a very
complicated set of diﬀerent types of behavior (see Petrov, Scott and Showalter
1992), we will focus on a single chaotic solution. Figure 15.4 shows the solution
when κ = 0.71, η = 0.054, ϵ = 0.005 and χ = 0.25. The single equilibrium point
has a one-dimensional stable manifold and a two-dimensional unstable manifold
associated with complex eigenvalue. The solution is continually attracted towards
the equilibrium point close to the stable manifold, and then spirals away close to
the unstable manifold before the process begins again.
15.1.3
The Lorenz Equations
Our third example is the system of three autonomous diﬀerential equations,
˙x
=
−8
3x + yz,
˙y
=
−10(y −z),
(15.11)
˙z
=
−xy + 28y −z,
known as the Lorenz equations. Equations (15.11) were derived by Lorenz (1963)
as the leading order approximation to the behaviour of an idealized model of the
Earth’s atmosphere. To claim that these simple equations model the weather is
perhaps going a little too far, but they certainly have very interesting dynamics.
There are three equilibrium points, one at the origin and two at x = 27,
y = z = ±6
√
2, each of which is unstable.
The two equilibrium points away
from the origin each have a two-dimensional unstable manifold, associated with
complex eigenvalues, and hence oscillatory behaviour, and a one-dimensional sta-
ble manifold. The system is rather like two copies of the cubic crosscatalator system
interacting with each other. Typical solutions bounce back and forth between the
two equilibrium points away from the origin, continually being attracted towards
an equilibrium point along a trajectory close to the stable manifold, and then spi-
ralling away close to the unstable manifold, as shown in Figure 15.5 for the solution
with x = y = z = 1 when t = 0.
The best way to get a feel for the dynamics of the Lorenz equations, and an
indication of their iconic status as the ﬁrst chaotic system to be discovered, is to
type lorenz in MATLAB, which runs an animated simulation.

452
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.4. The solution of the cubic crosscatalator equations, (15.10), with κ = 0.71,
η = 0.054, ϵ = 0.005, χ = 0.25 and α(0) = β(0) = γ(0) = 0.
15.2
Mappings
Although this book is about diﬀerential equations, we will see later that it is often
helpful to relate the solutions of diﬀerential equations to those of mappings, some-
times known as diﬀerence equations. Before we give some basic deﬁnitions, let’s
consider three examples of nonlinear mappings, which illustrate how complicated
the solutions of these deceptively simple systems can be.
Example: A shift map
The shift map is deﬁned by
x →ax|1,
(15.12)
which maps [0, 1) to itself, where b|c means the remainder when b is divided by
c. Let’s focus on the case a = 10. The shift map is then equivalent to shifting
the decimal point one place to the right and throwing away the integer part. For
example,
1
8 = 0.125 maps to 0.25, which maps to 0.5, which maps to zero, an
equilibrium, or ﬁxed point of the map. Another equilibrium point of the map
is
1
9 = 0.11111 . . . . The map also has many periodic solutions, for example,
1
11 = 0.09090909 . . . →10
11 = 0.909090 . . . →
1
11 →. . . , which has period 2.

15.2 MAPPINGS
453
Fig. 15.5. The solution of the Lorenz equations, (15.11), with x(0) = y(0) = z(0) = 1.
Every rational number has either a ﬁnite decimal expansion, and therefore is
eventually mapped to zero, for example 1
8, or has a repeated decimal expansion,
in which case it is part of a periodic solution, for example
1
11. Irrational numbers,
for example
√
2, π and e, have decimal expansions that do not repeat themselves
and have no pattern, for example
√
2 −1 ≈0.4142135 . . . →0.1421356 . . . →
0.4213562 →0.2135623 →. . . . In addition, two irrational numbers may be arbi-
trarily close to each other, but the corresponding solutions eventually diverge. For
example, consider x = x1 =
√
2 −1 and x = x2 =
√
2 −1 + 10−5π ≈0.4142449 . . . .
These diﬀer by just 10−5π, but after four iterations of (15.12), x1 maps to 0.135 . . .
whilst x2 maps to 0.449 . . . . This is a simple example of sensitive dependence on
initial conditions. Each solution initially at an irrational number therefore satisﬁes
our conditions to be called a chaotic solution, since they behave in an apparently
random manner, and initially close solutions diverge. The chaotic solutions, which
correspond to the irrational numbers, are dense in [0, 1), as are the periodic solu-
tions, which correspond to rational numbers with no ﬁnite decimal representation.

454
AN INTRODUCTION TO CHAOTIC SYSTEMS
Example: The logistic map
Consider the logistic map
xn+1 = rxn(1 −xn),
(15.13)
where r is a constant, with 0 < r ⩽4 and n an integer. The logistic map is a simple
model for the growth of the population of a single species. Starting from an initial
population, x0, (15.13) gives a measure of the population in subsequent generations.
The state xn = 0 represents the complete absence of the species, and for xn ≪1,
xn+1 ∼rxn, so that the next generation grows by a factor of r. When xn is not
small, the factor (1−xn), which models the eﬀect of overcrowding and competition
for resources, is no longer close to unity, and the full, nonlinear equation, (15.13),
determines the size of the next generation. Note that if 0 ⩽x0 ⩽1, then 0 ⩽xn ⩽1
for n ⩾0. The interval [0, 1] is also the physically meaningful range for this map.
Let’s begin by trying to ﬁnd the ﬁxed points of (15.13). These satisfy xn+1 = xn,
and hence xn = rxn(1−xn). The ﬁxed points are therefore x = 0 and x = (r−1)/r.
The nontrivial ﬁxed point lies in the meaningful range only if r > 1. Let’s now
determine whether there are any solutions of period 2, or 2-cycles. These satisfy
xn+1 = rxn(1 −xn),
xn+2 = xn = rxn+1(1 −xn+1).
By eliminating xn+1, we obtain the equation for xn,
xn

xn −r −1
r
 
r2x2
n −r(1 + r)xn + (1 + r)

= 0.
This equation is easy to factorize, since we know that it must also be satisﬁed by
the two ﬁxed points. The discriminant of the quadratic factor is r2(r2 −2r −3),
which is positive provided r > 3. This means there are points of period 2 for r > 3.
In fact, it can be shown that the nontrivial ﬁxed point is stable for r < 3, but loses
stability in a bifurcation† at r = 3, where the points of period 2 emerge and are
stable. Similarly, as r increases, the points of period 2 eventually lose stability, and
a stable 4-cycle emerges. This process is known as period doubling, and, as r
increases, eventually leads to chaotic solutions. We will not go into the details of
this process, since we want to concentrate on maps relevant to diﬀerential equations.
The interested reader is referred to Arrowsmith and Place (1990). Figure 15.6 shows
the period doubling process as a bifurcation diagram. This was produced using the
MATLAB script
† a ﬂip bifurcation

15.2 MAPPINGS
455
'
&
$
%
for r = 0:0.005:4
x = rand(1);
for j = 1:100
x = r*x*(1-x);
end
xout = [];
for j = 1:400
x = r*x*(1-x); xout = [xout x];
end
plot(r*ones(size(xout)),xout,’.’,’MarkerSize’,3)
axis([0 4 0 1]), hold on, pause(0.01)
end
This iterates the map 100 times, starting from randomly generated initial conditions
(x = rand(1)), and then saves the next 400 iterates of the map before plotting
them. Figure 15.7 shows 100 iterates of the logistic map with r = 4 for two initial
conditions separated by just 10−16. The apparently random nature of the solution
can be seen, as can the fact that these initially very close solutions have completely
diverged after about 50 iterations of the map.
Fig. 15.6. The bifurcation diagram for the logistic map.

456
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.7. A sequence of 100 iterates of the logistic map with r = 4 and x0 = 0.1 and
x0 = 0.1 + 10−16.
We will also discuss another phenomenon that can occur in the logistic map,
known as intermittency.
A system that behaves in an intermittent manner
exhibits bursts of chaotic behaviour interspersed with simpler, more regular be-
haviour.
Many other systems, both maps and ﬂows, can exhibit intermittency,
which often begins to occur as a parameter is changed, and is a prelude to full
chaos. For example, the ﬂow of water down a pipe is smooth and steady, or lami-
nar, at suﬃciently low ﬂow rates.† At suﬃciently high ﬂow rates, the ﬂow becomes
unsteady and chaotic, or turbulent.
However, at intermediate ﬂow rates, tur-
bulence can appear in intermittent, spatially localized bursts (see, for example,
Mathieu and Scott, 2000).
Consider the ﬁfth iterate of the logistic map, f (5)(x), which is shown in Fig-
ure 15.8 for r = 3.7.
Note that there are four points where the curve is close
to the straight line that represents the identity mapping, f(x) = x, but does not
touch it.
We will focus on the point close to x = 0.65, the image of which is
shown in Figure 15.8, and look for the nearby values of x and r at which f (5)(x)
actually touches the straight line. This is then a ﬁxed point of f (5)(x), and hence
part of a periodic solution of the logistic map of period 5. It is straightforward
† Strictly speaking, at suﬃciently low Reynolds numbers.

15.2 MAPPINGS
457
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
f5(x)
Fig. 15.8. The ﬁfth iterate of the logistic map for r = 3.7. Also shown is the image of the
point x = 0.65.
to show, using MATLAB, that g(x) = f (5)(x) −x and its derivative are zero when
r = rc ≈3.73817237526634 and x = xc ≈0.66045050397608. Figure 15.9 shows
f (5)(x) in the neighbourhood of this point when r = rc and for two values of r
slightly above and below rc. We can see that for r > rc there are two ﬁxed points,
one stable, one unstable, whilst for r < rc, locally there are no ﬁxed points. Clearly,
r = rc is a bifurcation point of the map f (5), analogous to the saddle–node bifurca-
tion that we discussed in Section 13.3.1, and is known as a tangent bifurcation.
For values of r slightly greater that rc, points initially close to xc are attracted
to the ﬁxed point, and therefore the solution of the logistic map is attracted to
a stable periodic solution of period 5. For r slightly less than rc, we can see in
Figure 15.9 that f (5)(x) is very close to the straight line, and that iterates of the
map can be trapped close to x = xc before moving away, and also close to the
other points where the curve is close to the straight line. The solution therefore
exhibits almost steady behaviour before moving away and behaving irregularly, as
shown in Figure 15.10. This is intermittency. The equivalent solution of the logistic
map displays almost periodic behaviour interrupted by chaotic bursts, which is also
shown in Figure 15.10. For further examples of intermittency, see Guckenheimer
and Holmes (1983).

458
AN INTRODUCTION TO CHAOTIC SYSTEMS
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
x
f5(x)
(a) r = rc ≈ 3.73817237526634.
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
x
f5(x)
(b) r = rc − 1.5 × 10−3
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
x
f5(x)
(c) r = rc + 1.5 × 10−3
Fig. 15.9. The function f 5(x) at rc and rc ± 1.5 × 10−3. The iterates of a point in this
neighbourhood are also shown.

15.2 MAPPINGS
459
1000
2000
3000
4000
5000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
xn+1 = f(xn) = rx(1−x)
n
xn
0
200
400
600
800
1000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
xn+1 = f(5)(xn)
n
xn
Fig. 15.10. The action of the logistic map and its ﬁfth iterate with x1 = xc and r =
rc −1 × 10−6.
Example: The H´enon map
The H´enon map is a nonlinear, two-dimensional map, which is deﬁned by
xn+1 = xn cos θ −yn sin θ + x2
n sin θ,
yn+1 = xn sin θ + yn cos θ −x2
n cos θ.
This is the most simple area-preserving, quadratically nonlinear map with linear
part a rotation through the angle θ about the origin. We can see that it preserves
areas, since the determinant of its Jacobian, the deﬁnition of which we will discuss
in more detail below, is

∂xn+1
∂xn
∂xn+1
∂yn
∂yn+1
∂xn
∂yn+1
∂yn

=

cos θ + 2xn sin θ
−sin θ
sin θ −2xn cos θ
cos θ
 = 1.
In Figure 15.11 we show the results of an iteration using the MATLAB script

460
AN INTRODUCTION TO CHAOTIC SYSTEMS
'
&
$
%
cosa=0.24; sina=sqrt(1-cosa^2);
ii=0;
for st=-0.5:0.05:0.5
x=st; y=st;
for its=1:1000
ii=ii+1;
xn=cosa*x-sina*y+x^2*sina;
yn=sina*x+cosa*y-x^2*cosa;
x=xn; y=yn; po(ii)=x+i*y;
if ((abs(x)>10)|(abs(y)>10))
break
end
end
end
plot(po,’.’,’MarkerSize’,2)
axis equal, axis([-1 1 -1 1])
Note that we have used MATLAB’s ability to plot complex numbers, and that |
is the logical or operator. In addition, it is necessary to conﬁne the iterates by
stopping the calculation once the points have left the domain of interest.
Fig. 15.11. Iterates of the H´enon map for cos θ = 0.24 and cos θ = 0.3.
It is clear that this apparently simple map gives rise to extremely complex be-
haviour. The regions where there are concentric sets of points, or islands, owe their
appearance to the existence of periodic solutions. In particular, for cos θ = 0.24
we see evidence of a period 5 structure. In fact, there is a 5-cycle consisting of the
ﬁve points at the centre of the chain of ﬁve islands, separated by ﬁve further un-
stable equilibrium points. Chaotic solutions exist near these points. If we consider

15.2 MAPPINGS
461
other values of cos θ, other periodic structures become evident (for instance, when
cos θ = 0.34 we ﬁnd a structure of period 11). We also note that if we consider
certain regions in more detail, we can see structures with even longer periods (see,
for example, Figure 15.12, which we obtained from Figure 15.11 using MATLAB’s
ability to zoom in on a small region of an existing ﬁgure).
Fig. 15.12. Iterates of the H´enon map for cos θ = 0.24 in the neighbourhood of one of the
hyperbolic ﬁxed points.
15.2.1
Fixed and Periodic Points of Maps
Consider the map
xn+1 = g(xn),
(15.14)
where g : Rn →Rn is a diﬀeomorphism; that is g is a bijection and is dif-
ferentiable with a diﬀerentiable inverse†. A ﬁxed point, ¯x, of the map satisﬁes
¯x = g(¯x). A point of period k, x∗, satisﬁes x∗= gk(x∗), and x∗̸= gm(x∗) for
all positive integer m < k.
† Note that, of the maps that we consider in this section, only the H´enon map is a diﬀeomorphism,
since the logistic, tent and horseshoe maps are not bijective and have discontinuous derivatives.

462
AN INTRODUCTION TO CHAOTIC SYSTEMS
Let’s now consider the linearization of (15.14) about a ﬁxed point, ¯x. If we let
xn = ¯x + ˆxn we ﬁnd that, at leading order,
ˆxn+1 = Dg(¯x)ˆxn,
(15.15)
where Dg(¯x) is the Jacobian matrix associated with g at x = ¯x. For one-dimensional
maps, the solution of xn+1 = g′(0)xn is xn = (g′(0))n x0, and hence the stability
of x = 0 depends upon whether |g′(0)| is greater than or less than unity. Similarly,
(15.15) has solution
ˆxn = (Dg(¯x))n x0,
(15.16)
and the stability of the ﬁxed point depends upon the size of the moduli of the n
eigenvalues. We say that the equilibrium point is hyperbolic if none of the eigen-
values of its Jacobian have modulus unity, and that it is nonhyperbolic otherwise.
In the same way as we found for diﬀerential systems, in the neighbourhood of an
equilibrium point of a map, there exist three invariant manifolds.
(i) The local stable manifold, ωs
loc, of dimension s, is spanned by the eigen-
vectors of Dg(¯x) whose eigenvalues have modulus less than unity.
(ii) The local unstable manifold, ωu
loc, of dimension u, is spanned by the
eigenvectors of Dg(¯x) whose eigenvalues have modulus greater than unity.
(iii) The local centre manifold, ωc
loc, of dimension c, is spanned by the eigen-
vectors of Dg(¯x) whose eigenvalues have modulus equal to unity.
These local manifolds exist as global manifolds when we consider the full, nonlinear
system, just as they do for systems of diﬀerential equations.
Theorem 15.1 Consider the diﬀeomorphism (15.14).
(i) The stable manifolds of diﬀerent equilibrium points cannot intersect.
(ii) The unstable manifolds of diﬀerent equilibrium points cannot intersect.
(iii) The stable manifold of an equilibrium point cannot intersect itself.
(iv) The unstable manifold of an equilibrium point cannot intersect itself.
Proof Recall that every point x ∈Rn has a unique image and preimage, since g is
a diﬀeomorphism.
(i) Since a point in the stable manifold of an equilibrium point, ¯x, must asymp-
tote to ¯x as n →∞, it cannot also asymptote to a diﬀerent equilibrium
point as n →∞, which proves (i).
(ii) As for case (i) but with n →−∞.
(iii) Consider a point, x∗, that is mapped to a point of intersection of the stable
manifold with itself, g(x∗). Points on the stable manifold in the neighbour-
hood of x∗must map to points on the stable manifold in the neighbourhood
of g(x∗). Since g is continuous, this is not possible.
(iv) As for case (iii), but on the unstable manifold.

15.2 MAPPINGS
463
Although this theorem eliminates several possibilities, it is possible for a stable
manifold to intersect an unstable manifold, either of the same equilibrium point or
a diﬀerent one. This will prove to be crucial later.
15.2.2
Tents and Horseshoes
Before we return to consider systems of diﬀerential equations, we will examine
two more examples of maps, which will prove to be of direct relevance later.
Example: A tent map
A tent map is a map x →f(x) where f : R →R and
f(x) =

sx
for x ⩽1
2,
s(1 −x)
for x ⩾1
2.
We will concentrate on the case s = 3, when the function f is as shown in Fig-
ure 15.13. Note that f(x) = 1 when x = 1
3 or x = 2
3 and that the only equilibrium
Fig. 15.13. The function f(x) when s = 3.
point of the map is zero. Let’s now consider where this tent map sends various sets
of points.
(i) If x < 0, x →3x < x. Clearly, f n(x) →−∞as n →∞for x ∈(−∞, 0).
(ii) If x > 1, x →3(1 −x) < 0. After one iteration, all points with x > 1 are
therefore mapped to a point with x < 0, and case (i) applies for subsequent
iterations, with f n(x) →−∞as n →∞for x ∈(1, ∞).

464
AN INTRODUCTION TO CHAOTIC SYSTEMS
(iii) If x ∈
 1
3, 2
3

, x →f(x) > 1. After one iteration, all points with 1
3 < x <
2
3 are therefore mapped to a point with x > 1, and case (ii) applies for
subsequent iterations, with f n(x) →−∞as n →∞.
(iv) Now consider points with x ∈[0, 1
3] ∪[ 2
3, 1], which are mapped to [0, 1].
Only points in the set [0, 1
9] ∪[ 2
9, 3
9] ∪[ 6
9, 7
9] ∪[ 8
9, 1] have images in the set
[0, 1
3] ∪[ 2
3, 1], so the remaining points have f n(x) →−∞as n →∞.
If we continue this process, we can see that the set of points, K, that are not expelled
from [0, 1] as n →∞can be constructed in an iterative way by deleting the middle
third from K0 = [0, 1] to leave K1 = [0, 1
3] ∪[ 2
3, 1], then deleting the middle third
from each of the remaining intervals to leave K2 = [0, 1
9] ∪[ 2
9, 3
9] ∪[ 6
9, 7
9] ∪[ 8
9, 1],
and so on, with Kn the union of the 2n closed subintervals [r/3n, (r + 1)/3n], each
of which has length 3−n. The index r takes the values in the set Rn, which can be
generated iteratively using
R0 = {0} ,
Rn = {Rn−1, 3n −1 −Rn−1}
for n ⩾1.
We then have
K =
∞
?
n=0
Kn,
which is known as Cantor’s middle-third set and was ﬁrst constructed by Cantor
in 1883 as an example of an inﬁnite, completely disconnected set. All points
initially in K remain in K as n →∞, so that K is a positively invariant set for the
tent map with s = 3. Note that the length of Kn is 2n/3n, which tends to zero as
n →∞, and hence the length of K is zero.
A more convenient deﬁnition of K is provided by looking at the numbers in
[0, 1] expressed in ternary, or base three form. We can write all points in K as
x = 0.d1d2d3 . . . = d1/3+d2/32 +d3/33 +· · · , with di = 0 or 2 for n = 1, 2, . . . . To
see this, note that each point in K1 has ternary form 0.0x2x3 . . . if it lies in [0, 1
3] =
[0, 0.0222 . . . ] and 0.2x2x3 . . . if it lies in [ 2
3, 1] = [0.2, 0.222 . . . ]. Similarly, each
x in K2 has ternary form 0.00x3x4 . . . , 0.02x3x4 . . . , 0.20x3x4 . . . , or 0.22x3x4 . . .
according to which of the four subintervals of K2 it lies in, and so on. We can
see that excluding the middle third of each successive subinterval is equivalent to
excluding numbers that have di = 1 for i = 1, 2, . . . .
Lemma 15.1 Cantor’s middle-third set, K, has uncountably many points.
Proof Suppose that K is countable, so that all the members of K can be ordered as
x1 = 0.x11x12x13 . . . < x2 = 0.x21x22x23 . . . , in ternary form. We can now deﬁne
y = 0.y1y2y3 . . . , with ym = 0 if xmm = 2 and ym = 2 if xmm = 0. Then 0 < y < 1,
y ∈K and y ̸= xn for all n – a contradiction. The elements of K cannot therefore
be ordered, and must be uncountable.†
† This is just a variation of Cantor’s famous diagonalization proof that the real numbers are
uncountable.

15.2 MAPPINGS
465
Nonetheless, by construction, K contains no subinterval, however small, of [0, 1],
and [0, 1] contains inﬁnitely many subintervals that do not intersect K.
Other Cantor sets can be constructed by successively removing diﬀerent parts
of [0, 1]. An invariant set that is also a Cantor set is, as we shall see, typical of a
chaotic system.
Example: The Smale horseshoe map
Consider a two-dimensional map from the unit square,
D =

(x, y) ∈R2 | 0 ⩽x ⩽1, 0 ⩽y ⩽1

,
to itself, f : D →D. The function f contracts the square in the horizontal direction
and expands it in the vertical direction, and then folds the resulting strip back on
itself, as shown in Figure 15.14. The map is only deﬁned on the unit square, D,
and points that are mapped out of D are discarded. This is the Smale horseshoe
map. The inverse map, which can be visualized in terms of stretching and folding
the unit square in the opposite way, is shown in Figure 15.15.
Fig. 15.14. The Smale horseshoe map.
The invariant set, Λ, which is both positively and negatively invariant, is the
intersection of the image of D under any number of forward or backward iterations,
Λ =
· · · ∩f −2(D) ∩f −1(D) ∩D ∩f(D) ∩f 2(D) ∩· · · .
As we can see, each application of the map removes the middle third of each re-
maining strip in each direction, so we conclude that Λ is a two-dimensional Cantor
set given by the set of points (x, y) such that x ∈Kx and y ∈Ky, where Kx and
Ky are the Cantor middle-third sets in each direction.
It is now helpful to show that each point in the invariant set can be uniquely
labelled by a sequence of 0’s and 1’s. After each forward iteration of the map, we
append ak to the right of the symbol sequence, where
ak =
0
for points in the left half of D,
1
for points in the right half of D,

466
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.15. The inverse Smale horseshoe map.
Fig. 15.16. The construction of the left half of the bi-inﬁnite sequence.

15.3 THE POINCAR´E RETURN MAP
467
as illustrated in Figure 15.16. After each backward iteration of the map we append
ak to the left of the symbol sequence, where
ak =
0
for points in the bottom half of D,
1
for points in the top half of D,
as shown in Figure 15.17. We can then represent any point in Λ as the bi-inﬁnite
Fig. 15.17. The construction of the right half of the bi-inﬁnite sequence.
sequence of symbols,
a =
. . . a2a1a0.a−1a−2 . . . ,
as shown in Figure 15.18. The digits to the right of the decimal point reﬂect the
vertical location and to the left the horizontal location. For any point in Λ, the
action of the horseshoe map is simply to shift the decimal point in the bi-inﬁnite
sequence one place to the right. The map is therefore equivalent to the shift map
x →2x|1, which is known as the Bernoulli shift map. For the same reasons that
the shift map that we studied as our ﬁrst example had chaotic solutions, so does
the Bernoulli shift map, and hence the horseshoe map.
15.3
The Poincar´e Return Map
Now that we have seen several examples of maps, we will demonstrate how the
solutions of a system of ordinary diﬀerential equations can be related to the solutions
of a map, the Poincar´e map, that is easier to work with than the original system,
and then develop a technique for deciding whether this map is chaotic.
Consider the autonomous system of ordinary diﬀerential equations
dx
dt = f(x),
(15.17)
where f : Rn →Rn and x ∈Rn. We introduce the ﬂow evolution operator or
ﬂow operator, φt1,t0, which maps the point x0 ∈Rn and an interval (t0, t1) ∈R

468
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.18. The points in Λ, the invariant set.
to the point x1 ∈Rn, where x1 = x(t1) and x is the solution of (15.17) subject
to x = x0 when t = t0.
That is x(t0) = x0 evolves to x(t1) = x1 in the n-
dimensional phase space during the time interval (t0, t1). Note that φt0,t0(x0) = x0
and φt2,t1 · φt1,t0 ≡φt2,t0 for all t1 ∈(t0, t2).
In addition, since (15.17) is an
autonomous system, we can express the ﬂow evolution operator in terms of the
length of the time interval alone, and we write
φt1,t0(x0) = φt1−t0(x0).
Consequently we have that φ0 is the identity transformation, φt · φs ≡φt+s and
(φt)−1 = φ−t, so that φ is a group operator (see Chapter 10).
Example: The logistic equation
Let’s construct the ﬂow operator for the logistic equation,
dx
dt = x(1 −x).
This is the continuous version of the logistic map, which we studied earlier. The
logistic equation is separable, and we can therefore integrate it subject to the initial

15.3 THE POINCAR´E RETURN MAP
469
condition that x = x0 when t = 0 to obtain the solution, and hence the evolution
operator,
x(t) = φt(x0) =
x0et
x0et −x0 + 1.
Note that solutions of the logistic equation all have x →1 as t →∞, in complete
contrast to the solutions of the logistic map. This shows that choosing to use a
continuous rather than a discrete system can have dramatic consequences for the
behaviour of the solutions.
We can now verify that φ0(x0) = x0 and
φt · φs(x0) = φt(φs(x0)) =
φs(x0)et
φs(x0)et −φs(x0) + 1,
and, since φs(x0) = x0es/(x0es −x0 + 1),
φt · φs(x0) =
x0eset
x0eset −x0es + (x0es −x0 + 1) =
x0es+t
x0es+t −x0 + 1,
which is equal to φs+t(x0), as expected.
Equilibrium points of (15.17) satisfy φt(x∗) = x∗for all t ∈R (or equivalently
f(x∗) = 0). Periodic solutions of (15.17) with period T satisfy x∗(t) = x∗(t + T)
for all t ∈R, and can be written in terms of the ﬂow operator as φt+T (x∗) = φt(x∗)
for all t ∈R.
One way of obtaining a map from the system (15.17) is to sample the solution
with period τ, so that φn(x0) = x(t0 + nτ) for n ∈Z. This allows us to track
the trajectory of particles in a stroboscopic way. A more useful map, which we
can use to analyze the behaviour of integral paths close to a periodic solution, is
the Poincar´e return map. Let γ be a trajectory of (15.17), and consider the
intersections of γ with Σ ⊂Rn such that
(i) Σ has dimension n −1,
(ii) Σ is transverse (nowhere parallel) to the integral paths,
(iii) all solutions in the neighbourhood of γ pass through Σ.
If γ intersects Σ at x = p, and its next intersection with Σ is at x = q, then
the Poincar´e return map, P : Σ →Σ, maps the point p to the point q. This is
illustrated in Figure 15.19 for n = 3 where Σ is a plane. Note that an equilibrium
point of the Poincar´e return map corresponds to a limit cycle that intersects Σ
once, and a periodic solution of period k corresponds to a limit cycle that intersects
Σ k times.
Example
Let’s try to construct the Poincar´e return map for (15.17) with n = 2 when
f(x) =
 4x + 4y −x(x2 + y2)
4y −4x −y(x2 + y2)

,

470
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.19. The deﬁnition of the Poincar´e return map.
and Σ is the positive x axis. It is easier to write this system in terms of plane polar
coordinates, and, using (9.22) and (9.23), we ﬁnd that ˙r = r(4 −r2) and ˙θ = −4.
These expressions can be integrated with initial conditions in Σ, the positive x-axis,
so that r(0) = x0 and θ(0) = 0, to obtain
r =
+
4x2
0e8t
4 −x2
0 + x2
0e8t ,
θ = −4t.
The orbit next returns to this axis when θ is an integer multiple of 2π, that is when
t = π/2, so that
x1 = P(x0) =
+
4x2
0e4π
4 −x2
0 + x2
0e4π .
The solution for x0 = 10 is shown in Figure 15.20, along with the Poincar´e return
map, which rapidly asymptotes to x = 2.
Example: The Lorenz equations
Solutions of the Lorenz equations repeatedly cross the plane y = −z, which sepa-
rates the two equilibrium points at x = 27, y = z = ±6
√
2. Indeed the equations
are symmetric about this plane, since they are unchanged by the transformation
y →−y, z →−z. We can therefore use the plane y = −z as Σ, and hence de-
ﬁne a Poincar´e return map. In order to investigate this map, we must proceed
numerically. In MATLAB, we need to deﬁne an event function

15.3 THE POINCAR´E RETURN MAP
471
Fig. 15.20. The solution when x0 = 10 and the corresponding Poincar´e return map.




function [value, isterminal, direction] = lorenzevent(t,y)
value = y(2)+y(3); isterminal = 0; direction = 1;
This function returns the value zero when the solution intersects Σ (when y + z =
y(2)+y(3) = 0), but only as the solution approaches Σ with y + z increasing
(direction = 1), and allows the integration to continue (isterminal = 0). We
can then use the commands




options = odeset(’Events’,@lorenzevent);
solution = ode45(@lor,[0 500], [1 1 1], options);
This integrates the Lorenz equations, which are in the function lor
#
"
 
!
function dy = lor(t,y)
dy(1) = -8*y(1)/3+y(2)*y(3);
dy(2) = 10*(y(3)-y(2));
dy(3) = -y(2)*y(1)+28*y(2)-y(3);
dy=dy’;
for 0 ⩽t ⩽500, starting from x = y = z = 1 when t = 0. Note that the function
odeset allows us to create a variable options that we can pass to ode45 as an
argument, which controls its execution. In this case, we tell ode45 to detect the

472
AN INTRODUCTION TO CHAOTIC SYSTEMS
event coded for in lorenzevent. The variable† solution then contains the points
where the solution crosses Σ, in solution.ye, at times solution.te. The Poincar´e
return map is shown in Figure 15.21. As you can see, the map is eﬀectively one-
dimensional, since the points where the solution meets Σ lie on a simple curve.
The dynamics are, however, apparently chaotic (see Sparrow, 1982, for a detailed
discussion of the Lorenz equations).
Fig. 15.21. A Poincar´e return map for the Lorenz equations.
15.4
Homoclinic Tangles
Let’s now consider a two-dimensional Poincar´e return map, with f : Σ →Σ and Σ ⊂
R2, associated with a three-dimensional system, (15.17). Each limit cycle solution
of (15.17) is associated with an equilibrium point, ¯x, of the map. If this equilibrium
point is stable, the limit cycle is stable. Let’s assume that a particular equilibrium
point, ¯x of the Poincar´e return map, which corresponds to a limit cycle solution
† The variable solution, produced by ode45, is a structure, and eﬀectively contains more than
one type of data in its deﬁnition.

15.4 HOMOCLINIC TANGLES
473
of (15.17), has a one-dimensional stable manifold, ωs(¯x), and a one-dimensional
unstable manifold, ωu(¯x). As we saw earlier, it is possible for these manifolds to
intersect. As we shall see, if this intersection is transverse, the manifolds become
tangled, intersecting inﬁnitely often, as shown in Figure 15.22. The manifolds then
contain embedded horseshoe maps, and thus have chaotic solutions. We will now
discuss why this should be so.
Although we will proceed through lemmas and
theorems, our approach is fairly informal, and should not be read as a rigorous
proof.
Fig. 15.22. A homoclinic tangle.
A homoclinic point is a point x ̸= ¯x that lies in the set ωs(¯x) ∩ωu(¯x), the
intersection of the stable and unstable manifolds of ¯x. Such a point asymptotes to ¯x
as n →±∞. Under successive applications of the map and its inverse, a homoclinic
point is mapped to a homoclinic orbit, a discrete set of points that is a subset of
the stable and unstable manifolds.
We will now assume that the map f is area-preserving, so that, for any set
D ⊂Σ, the area of f(D) is equal to that of D. This assumption greatly simpliﬁes
the following discussion, but is not actually necessary (see Wiggins, 1988).
Lemma 15.2 If x0 is a transverse homoclinic point, then all positive and negative
iterates of x0 have the same orientation, either US or SU, as shown in Figure 15.23.
Proof
The Poincar´e return map is associated with a ﬂow.
Figure 15.24 shows
the trajectory through the points x0 and f(x0).
Since the stable and unstable
manifolds associated with this trajectory vary continuously, their orientations can
at most rotate and cannot ﬂip during their passage from one side of Σ to the other.

474
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.23. US and SU orientation at a homoclinic point. Remember that the manifolds
are stable or unstable with regard to the equilibrium point ¯x, not x0.
x0
f (x0)
s
s
u
s
u
u
∑
Fig. 15.24. The stable and unstable manifolds of the limit cycle.
Lemma 15.3 If x0 is a transverse homoclinic point, then there must be another
transverse homoclinic point between x0 and f(x0). In other words, the next trans-
verse homoclinic point along say ωs after x0 cannot be f(x0).

15.4 HOMOCLINIC TANGLES
475
Proof If the orientation associated with x0 is US, then the next homoclinic point
along ωs, z, has SU orientation, and by Lemma 15.2, z ̸= f(x0), as shown in
Figure 15.25.
Note that this means that there are at least two transverse homoclinic orbits, one
associated with x0 and one associated with z.
Fig. 15.25. The orientation of successive homoclinic points.
Lemma 15.4 The existence of a single transverse homoclinic point ensures the
existence of an inﬁnite number of transverse homoclinic points.
Proof We consider the image of the points within the lobe L0, which is bounded
by the stable and unstable manifolds through x0 and z, as shown in Figure 15.26.
The image of L0 is the lobe L1, which is bounded by the portions of ωu and ωs
between f(x0) and f(z). Similarly if x ∈L0 then f j(x) ∈Lj which is bounded
by the portions of ωu and ωs between f j(x0) and f j(z).
Since we know that
f n(x0) →¯x and f n(z) →¯x along ωs as n →∞, the distance between these
images must decrease. However, the distance between the points along ωu must
increase. This leads to long thin lobes, bounded by short sections of ωs and long
sections of ωu. We therefore have a ﬁnite area covered by an inﬁnite set of lobes
of ﬁnite area equal to the area of A0 (recall that we are assuming that the map
preserves areas). Consequently, these lobes must overlap. We can assume, without
loss of generality, that they overlap between z and f(x0). There are, therefore,
two further transverse intersection points, which we label a and b. We can repeat
this argument indeﬁnitely, and conclude that there must be an inﬁnite number of
homoclinic points.
Theorem 15.2 (Smale–Birkhoﬀ) Let f : R2 →R2 be a diﬀeomorphism with a
hyperbolic equilibrium point ¯x. If ωs(¯x) and ωu(¯x) intersect transversally at a point
other than ¯x, in other words, if there is a homoclinic tangle, then the map has a
horseshoe map embedded within it.
Note that, since horseshoe maps have chaotic orbits, this theorem shows that the

476
AN INTRODUCTION TO CHAOTIC SYSTEMS
f(x0)
L1
L0
b
a
z
x0
f j(z)
f j(x0)
ωu
f(z)
Fig. 15.26. The homoclinic points a and b.
existence of a homoclinic tangle implies the existence of chaotic orbits.
As we
shall see later, there is an algebraic test, Mel’nikov’s method, that can be used to
determine whether a system has a transverse homoclinic point.
Proof
We will simply give an outline of the proof here. A more detailed proof,
which uses the idea of Markov partitioning, is given in Guckenheimer and Holmes
(1983). Our aim here is simply to convince you that the forward and backward
maps of the region D, which we deﬁne below, intersect.
Consider a region D that contains a transverse homoclinic point, x0, associated
with an equilibrium point, ¯x of a map f : R2 →R2, as shown in Figure 15.27. Then
(i) f k(D) contains the iterates of f k(x0) for k ∈Z.
(ii) For k < 0, f k(D) is stretched along the stable manifold and contracted along
the unstable manifold.
(iii) For k > 0, f k(D) is stretched along the unstable manifold and contracted
along the stable manifold.
(iv) For some backward iterate −q, f −q(D) = D −will have a horseshoe shape
and intersect with f p(D) = D + for some forward iterate p.
(v) The map f −(p+q)(D +) = D −, which maps the region D + into a horseshoe
shaped region, D −, with overlap between D −and D +, is a horseshoe map.

15.4 HOMOCLINIC TANGLES
477
x0
ωu
D−
D+
ωs
x
D
Fig. 15.27. A horseshoe map arising from the dynamics in a homoclinic tangle.
15.4.1
Mel’nikov Theory
We will now describe an algebraic test with which we can determine whether a
system has a homoclinic tangle, and hence, by Theorem 15.2, chaotic solutions. We
will focus on two-dimensional Hamiltonian systems perturbed by a periodic function
of time only, although the method can also be used for perturbed non-Hamiltonian
systems (see Wiggins, 1988).
We consider a Hamiltonian system (see Section 9.3.9) with Hamiltonian H =
H(x, y) and associated diﬀerential equations
˙x = f(x)
where f = (−∂H/∂y, ∂H/∂x)T or, written in component form,
dx
dt = −∂H
∂y ,
dy
dt = ∂H
∂x .
(15.18)
We will consider this system under the inﬂuence of a small perturbation that is a
function of space and periodic in time, in the form
˙x = f(x) + ϵg(x, t),
(15.19)
with g(x, t) = g(x, t+T) for some period T > 0, and ϵ ≪1. We start by considering
the unperturbed system.

478
AN INTRODUCTION TO CHAOTIC SYSTEMS
15.4.2
Unperturbed System (ϵ = 0)
As we saw in Section 9.3.9, this system is area-preserving, and its equilibrium
points are either nonlinear centres or saddles. We suppose that ¯x0 is a saddle point
and that there is a homoclinic orbit x0(t) such that x0(t) →¯x0 as t →±∞. The
interior of the homoclinic orbit must contain concentric limit cycles surrounding a
centre at x∗, as shown in Figure 15.28.
x = x0(t)
x∗
x0
Fig. 15.28. A homoclinic orbit of the unperturbed Hamiltonian system.
15.4.3
Perturbed System (0 < ϵ ≪1)
We can think of the perturbed system as autonomous in the three-dimensional
(x, y, t)-phase space. We can deﬁne an associated map by stroboscopically sampling
the ﬂow at t = nT for n ∈Z. Since g(x, t) = g(x, t + nT), this is equivalent to a
Poincar´e return map with Σ the plane t = 0. Note that the equilibrium point, ¯x0,
of the unperturbed system of diﬀerential equations is also an equilibrium point of
the associated map, and that the stable and unstable manifolds of ¯x0 are the same
for the unperturbed system of diﬀerential equations and the associated map.
We assume that the inﬂuence of the perturbation is to modify the equilibrium
point of the map to the point xϵ, with, since ϵ ≪1, |xϵ−x0| ≪1. For the perturbed
map, the stable and unstable manifolds of the saddle point do not necessarily
connect smoothly, as shown in Figure 15.29.
We would now like to ﬁnd some
way of distinguishing between the two cases – either disjoint or tangled manifolds.
Let t0 denote the time at which we wish to consider the fate of the solutions. Since
the unperturbed system is autonomous, its orbits are invariant under arbitrary
transformations in time, so that the orbits x0(t) and x0(t −t0) are the same. This
is not true for the solutions of the perturbed system. In Figure 15.30, we show the
orbit x0(t) as a dashed line and the stable and unstable manifolds of the saddle
point of the perturbed system as solid lines. We will now introduce the idea of the
distance between points on the stable and unstable manifolds of the saddle point.
The idea is to ﬁnd an expression for this distance and determine whether it can be

15.4 HOMOCLINIC TANGLES
479
Fig. 15.29. The perturbed stable and unstable manifolds of the associated map.
zero. We can do this by deﬁning the Mel’nikov function, which then provides an
algebraic suﬃcient condition for the existence of transverse homoclinic intersections
and hence chaotic dynamics.
Fig. 15.30. The perturbed (solid lines) and unperturbed (broken lines) stable and unstable
manifolds of the saddle point.
The Mel’nikov function, D(t, t0), is proportional to the component of the
distance between corresponding points on the stable and unstable manifolds in the
direction normal to the unperturbed homoclinic orbit, and is given by
D(t, t0) = N(t, t0) · d(t, t0).
Here N(t, t0) is a normal to the unperturbed orbit and d(t, t0) connects correspond-
ing points on the stable and unstable manifolds. If D(t, t0) has simple zeros, then
there must be transverse homoclinic intersections, and we can conclude that the
system has chaotic solutions.
Firstly, we construct the normal to the unperturbed homoclinic orbit, x0(t). The
tangent to the orbit is f, and hence N(t, t0) · f(x0(t −t0)) = 0. In component form,

480
AN INTRODUCTION TO CHAOTIC SYSTEMS
N1f1 + N2f2 = 0, where f = (f1, f2), so we can take
N = (−f2(x0(t −t0)), f1(x0(t −t0))).
Note that N is not a unit normal. The vector displacement between correspond-
ing points on orbits xs(t) and xu(t) in the stable and unstable manifolds of xϵ is
d(t, t0) = xs(t, t0) −xu(t, t0), so the Mel’nikov function is
D(t, t0) = N(t, t0) · d(t, t0) = −f2d1 + f1d2,
where d(t, t0) = (d1(t, t0), d2(t, t0))T. For notational convenience, we now introduce
the binary operation ∧such that u ∧v = u1v2 −v1u2, so that D ≡f ∧d. We can
now use perturbation theory to obtain an expression for D(t, t0).
We assume that points on the orbits associated with xϵ remain close to points
on the homoclinic orbit x0(t −t0), so that we can write their locations as a posi-
tion on the homoclinic orbit plus a small perturbation proportional to ϵ. We also
introduce the superscript s,u so that we can discuss the stable and unstable cases
simultaneously, writing
xs,u(t, t0) ∼x0(t −t0) + ϵxs,u
1 (t, t0),
(15.20)
and hence
d(t, t0) ∼ϵ (xs
1(t, t0) −xu
1(t, t0))
and
D ∼ϵ (f ∧xs
1 −f ∧xu
1) .
It is now convenient to introduce two subsidiary Mel’nikov functions, D s(t, t0) and
D u(t, t0), such that
D ∼ϵD s(t, t0) −ϵD u(t, t0),
where D s,u(t, t0) = f ∧xs,u
1 . We now substitute (15.20) into the perturbed system,
(15.19), and obtain
˙x0 + ϵ˙xs,u
1
= f(x0 + ϵxs,u
1 ) + ϵg(x0 + ϵxs,u
1 , t)
= f(x0) + ϵDf(x0)xs,u
1
+ ϵg(x0, t) + O(ϵ2),
where Df(x0) is the Jacobian of the unperturbed system. This equation is au-
tomatically satisﬁed at leading order, since x0 is a solution of the unperturbed
equation, whilst at O(ϵ),
˙xs,u
1 (t, t0) = Df(x0(t −t0))xs,u
1 (t, t0) + g(x0(t −t0), t).
(15.21)
Now, diﬀerentiating the functions D s,u(t, t0), we obtain
˙D s,u(t, t0) = ˙f ∧xs,u
1
+ f ∧˙xs,u
1 ,
since there is a product rule associated with the ∧operator. However, we note that
˙f = d
dtf(x0(t −t0)) = Df(x0(t −t0))˙x0(t −t0),

15.4 HOMOCLINIC TANGLES
481
which implies that ˙f = Dff, and hence
˙D s,u(t, t0) = (Dff) ∧xs,u
1
+ f ∧˙xs,u
1 .
Substituting from (15.21) into this equation gives
˙D s,u(t, t0) = (Dff) ∧xs,u
1
+ f ∧Dfxs,u
1
+ f ∧g.
If we now use the identity
Dff ∧x + f ∧Dfx ≡(∇· f)f ∧x,
and note that ∇· f = 0 since the unperturbed system is Hamiltonian, we ﬁnd that
˙D s,u(t, t0) = f ∧g.
Considering the unstable manifold ﬁrst, we integrate from −∞to t0 to obtain
 t0
−∞
˙D u(t, t0) dt =
 t0
−∞
f ∧g dt,
which gives
D u(t0, t0) −D u(−∞, t0) =
 t0
−∞
f ∧g dt.
(15.22)
Now consider the stable case integrated from t0 to +∞, which gives
 ∞
t0
˙D s(t, t0) dt =
 ∞
t0
f ∧g dt,
and hence
−D s(t0, t0) + D s(∞, t0) =
 ∞
t0
f ∧g dt.
(15.23)
We note that as t →−∞along the unstable orbit and as t →∞along the stable
orbit, both solutions tend to xϵ, so that D u(−∞, t0) = D s(∞, t0). Adding (15.22)
and (15.23) and changing notation slightly so that we replace (t0, t0) with (t0), we
have
D(t0) ∼ϵ(D s(t0) −D u(t0)) = −ϵ
 ∞
−∞
f ∧g dt
= −ϵ
 ∞
−∞
!
f(x0(t −t0)) ∧g(x0(t −t0), t)
"
dt.
Note that
(i) If D(t0) has simple zeros for suﬃciently small ϵ, then ωu and ωs intersect
transversally, and hence by Theorem 15.2, there are chaotic solutions.
(ii) If D(t0) is bounded away from zero, there are no transverse homoclinic
intersections.
(iii) If we can ﬁnd one zero of a Mel’nikov function, the dynamics of the system
ensure that there will be inﬁnitely many zeros.

482
AN INTRODUCTION TO CHAOTIC SYSTEMS
Example
Consider the perturbed Hamiltonian system given by the forced Duﬃng equation,
(15.3),
¨x = x −x3 −ϵ(δ ˙x −γ cos ωt),
when ϵ ≪1. For what values of δ, γ and ω are there chaotic solutions?
We begin by rewriting the system as
d
dt
 x
y

=

y
x −x3 −ϵ(δy −γ cos ωt)

.
(15.24)
The unperturbed system is
d
dt
 x
y

=

y
x −x3

.
This is a Hamiltonian system with
d
dt
 x
y

=

∂H/∂y
−∂H/∂x

.
and Hamiltonian
H = 1
2y2 + 1
4x4 −1
2x2.
The unperturbed system has equilibrium points at (x, y) = (0, 0) and (x, y) =
(±1, 0), and it is straightforward to show that the origin is a saddle point and that
the other two equilibria are centres. The unperturbed phase portrait is shown in
Figure 15.31. We now need to determine the equation of the homoclinic orbit. This
orbit must pass through the origin, where H = 0. However, H is a constant on the
orbit, so the homoclinic orbit is given by
y2 = x2 −1
2x4.
Setting y equal to zero, we ﬁnd that the homoclinic orbit also meets the x-axis
where x = ±
√
2. We will take y(0) = 0 and x(0) =
√
2. We can then solve the
diﬀerential equation for the homoclinic orbit and ﬁnd that x0(t) =
√
2secht. This
gives us
x0(t −t0) =
√
2sech(t −t0),
y0(t −t0) = −
√
2sech(t −t0) tanh(t −t0).
We can now construct the Mel’nikov function. The vectors f and g are
f(x0(t −t0)) =

y0(t −t0)
x0(t −t0) −(x0(t −t0))3

and
g(x0(t −t0)) =

0
γ cos ωt −δy0(t −t0)

,
so that their wedge product is
f ∧g = y0(t −t0)(γ cos ωt −δy0(t −t0)).

15.4 HOMOCLINIC TANGLES
483
Fig. 15.31. The phase portrait of (15.24) with ϵ = 0.
Now, using the deﬁnition of the Mel’nikov function,
D(t0) = −ϵ
 ∞
−∞
−
√
2sech(t −t0) tanh(t −t0)
×
(
cos ωt + δ
√
2sech(t −t0) tanh(t −t0)
)
dt.
This can be integrated to give
D(t0) = −ϵ

−
√
2πγωsech
πω
2

sin ωt0 + 4δ
3

.
This has simple zeros when
δ = 3
√
2π
4
γωsech
πω
2

sin ωt0,
provided that
δ < 3
√
2π
4
γωsech
πω
2

.
This means that there are transverse homoclinic points when this condition is sat-
isﬁed, and we can infer that the system is chaotic. Figure 15.32 shows the Poincar´e
return map when ϵ = 0.1, δ = 0, γ = 1 and ω = 1
3.

484
AN INTRODUCTION TO CHAOTIC SYSTEMS
−4
−3
−2
−1
0
1
2
3
4
−10
−5
0
5
10
Points in the Poincaré return map
x
dx/dt
0
100
200
300
400
500
600
700
800
900
1000
−4
−2
0
2
4
t
x
Successive iterations of the Poincaré return map
Fig. 15.32. The Poincar´e return map for the forced Duﬃng equation when ϵ = 0.1, δ = 0,
γ = 1 and ω = 1
3.
15.5
Quantifying Chaos: Lyapunov Exponents and the Lyapunov
Spectrum
We have now seen how to determine analytically when chaotic solutions exist for
weakly, periodically perturbed Hamiltonian systems. What can we say about other
nonlinear systems of diﬀerential equations? If we can solve such a system numeri-
cally, and it appears to have chaotic solutions, can we characterize and quantify the
chaos? In this ﬁnal section, we will introduce the ideas of the maximum Lyapunov
exponent and the Lyapunov spectrum, which we can use for this purpose.
15.5.1
Lyapunov Exponents of Systems of Ordinary Diﬀerential
Equations
In a chaotic system, we have seen that neighbouring trajectories diverge.
In
fact this divergence is usually exponentially fast. If we can quantify this rate of

15.5 LYAPUNOV EXPONENTS AND THE LYAPUNOV SPECTRUM
485
divergence, we can quantify the chaotic solutions. Consider the system
dx
dt = f(x),
(15.25)
where f : Rn →Rn. Let ¯x(t) be a reference trajectory, and consider a neighbour-
ing trajectory, y(t), that has y(0) = ¯x(0) + ∆x(0). As t →∞, and the trajectories
diverge, we expect that ∆x(t) = y(t) −¯x(t) ∼∆x(0)eλt, as shown in Figure 15.33.
If λ < 0, the trajectories actually converge, whilst if λ > 0 the trajectories diverge,
and λ gives a measure of the rate of divergence.
Fig. 15.33. Neighbouring trajectories diverge exponentially fast.
Formally, we deﬁne the maximum Lyapunov exponent with respect to a
reference trajectory of a ﬂow as
λmax =
lim
||∆x(0)||→0
t→∞
1
t log ||∆x(t)||
||∆x(0)||,
(15.26)
where ||x|| =
# 
i x2
i is the Euclidean norm (see Section A1.2). This gives us
the basic numerical recipe for computing λmax from two neighbouring solutions.
Recall that the idea which is central to this deﬁnition is linearization about the
trajectory ¯x(t), and consequently we need to ensure that we can indeed linearize by
considering neighbouring trajectories (∆x(0) →0). In this limit, ∆x(t) is governed
by the linearized equation
d∆x
dt
= Df(¯x(t))∆x.
(15.27)
Formally ∆x(t) = φt
L(∆x(0)) where φt
L(.) is a linear evolution operator, so that
φt
L(α∆x) = αφt
L(∆x). If λmax is positive, numerical integration of (15.27) will lead
to exponentially growing solutions. This can be avoided in a nonlinear system by
renormalizing. The idea is to integrate forward in time until the two trajectories
become a given distance apart, and then scale ∆x, so that the calculation can
continue with a trajectory that is within a small distance of the reference orbit, as
shown in Figure 15.34. One way of doing this is to renormalize after regular time
intervals τ. The maximum Lyapunov exponent is then given by the average of the

486
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.34. Renormalization of diverging trajectories.
exponent calculated between each renormalization,
λmax = lim
n→∞
1
(n + 1)τ
n

j=0
log ||∆xj(τ)||
||∆xj(0)||,
(15.28)
provided that we deﬁne our renormalization to be
∆xn(0) = δ ∆xn−1(τ)
||∆xn−1(τ)|| for n = 1, 2, . . . ,
where δ ≪1 and we note that ||xn(0)|| = δ.
Example: The forced Duﬃng equation
Consider the forced Duﬃng equation, (15.3), with ϵδ = 1
2 and ϵγ = 3
5, a solution
of which is shown in Figure 15.2. We can now construct the maximum Lyapunov
exponent using the MATLAB function

15.5 LYAPUNOV EXPONENTS AND THE LYAPUNOV SPECTRUM
487
'
&
$
%
function avls = lyapunov(n,tau,del0,y,eqn)
t=0; ls=zeros(1,n); avls=ls;
del=del0*[1 1]/sqrt(2);
for i=1:n
tspan = [t t+tau];
[tout1 yout1]=ode45(eqn,tspan, y);
[tout2 yout2]=ode45(eqn,tspan, y+del);
delxe= [yout1(end,:)-yout2(end,:)];
nd=norm(delxe);
ls(i) = log(nd/del0); avls(i) = sum(ls)/i/tau;
del = del0*delxe/nd;
y = yout1(end,:); t = t+tau;
end
The arguments of the function lyapunov are n, the number of separate evaluations
of the maximum Lyapunov exponent, tau = τ, del0 = δ, y, the vector of initial
data, and eqn, a handle containing the name of the equation to be integrated. The
built-in function norm(delxe) calculates the Euclidean norm of the vector delxe.
The command
plot(lyapunov(1000,1,0.01,[0 0],@duffing))
produces Figure 15.35, which shows how the average maximum Lyapunov exponent
converges to a value of about 0.1. Since this is positive, neighbouring trajectories
diverge exponentially fast – an indication that the solution has sensitive dependence
upon initial conditions, and hence is chaotic.
One diﬃculty associated with calculating the maximum Lyapunov exponent in
this way is that the choice of the direction of the initial displacement, ∆x0, can
have an eﬀect. We will see how to overcome this problem in the next section.
15.5.2
The Lyapunov Spectrum
Although we now have a way of characterizing the rate of divergence of neigh-
bouring trajectories, this is rather a blunt tool, and can depend upon the direction of
the initial displacement from the reference trajectory. For an n-dimensional system
of diﬀerential equations, we can overcome these problems by deﬁning n quantities
that characterize the growth of line†, area, volume and hypervolume elements.
Consider the system (15.25).
As we saw above, the time evolution of small
variations ∆x(t) about the reference trajectory, ¯x(t), is governed by
d∆x
dt
= Df(¯x(t))∆x.
We can write the solution of this system as
∆x(t) = M(t)∆x(0),
† This is just the maximum Lyapunov exponent.

488
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.35. The maximum Lyapunov exponent of the forced Duﬃng equation, estimated
over 1000 iterations.
where the evolution operator M(t) is the fundamental matrix. We can construct
M numerically. The ﬁrst column of M is ∆x(1)(t), the solution subject to the initial
condition ∆x(1)(0) = (1, 0, . . . , 0)T. The jth column is ∆x(j) subject to the initial
condition ∆x(j)(0) = (0, . . . , 1, 0, . . . , 0)T where the one is in the jth position. By
using this construction we ﬁnd that
∆x(j)(t) = M(t)∆x(j)(0),
with
M(t) = (∆x(1)(t), ∆x(2)(t), . . . , ∆x(n)(t)).
The fundamental matrix M(t) has n eigenvalues {mi(t)} and the Lyapunov
exponents are deﬁned as
λi = lim
t→∞
1
t log |mi(t)| for i = 1, 2, . . . , n.
(15.29)
We say that the set of n Lyapunov exponents is the Lyapunov spectrum. The
exponents can be ordered as λ1 ⩾λ2 ⩾· · · ⩾λn, and λ1 = λmax, the maximum
Lyapunov exponent. As before, if one of the exponents is positive, this is a sign of

15.5 LYAPUNOV EXPONENTS AND THE LYAPUNOV SPECTRUM
489
sensitive dependence upon initial conditions, and we need to use a renormalization
scheme to calculate the Lyapunov spectrum.
Example: The cubic crosscatalator
We can calculate the Lyapunov spectrum of the cubic crosscatalator equations,
(15.10) using the MATLAB function
'
&
$
%
function avlambda = lyapunovspectrum(n,tau,del0,y,eqn)
t=0; avlambda = zeros(3,n); lambda = avlambda;
ex = [1 0 0]; ey = [0 1 0]; ez = [0 0 1];
for i=1:n
tspan = [t t+tau];
[tout yout]=ode45(eqn,tspan, y);
[toutx youtx]=ode45(eqn,tspan, y+del0*ex);
[touty youty]=ode45(eqn,tspan, y+del0*ey);
[toutz youtz]=ode45(eqn,tspan, y+del0*ez);
delx= [youtx(end,:)-yout(end,:)]/del0;
dely= [youty(end,:)-yout(end,:)]/del0;
delz= [youtz(end,:)-yout(end,:)]/del0;
m = eig([delx; dely; delz]);
lambda(:,i) = log(abs(m));
avlambda(:,i) = sum(lambda,2)/i/tau;
y = yout(end,:); t = t+tau;
end
The built-in function eig(A) calculates the eigenvalues of the matrix A. Figure 15.36
shows the estimates of the three Lyapunov exponents converging over 1000 itera-
tions. One of these is positive, which indicates that the solution depends sensitively
upon initial conditions. Since the other two elements of the Lyapunov spectrum
are negative, this indicates that neighbouring solutions diverge in one direction and
approach each other in the two perpendicular directions.
We should note that there are some diﬃculties associated with calculating the
Lyapunov exponents numerically. There may be diﬀerent sets of exponents in dif-
ferent regions of phase space. This means that the Lyapunov spectrum may depend
upon the starting condition. It is also notoriously hard to obtain convergence for
Lyapunov exponents, especially where there are regions characterized by rotation,
such as in the neighbourhood of centres.
There are several further points that we can make about the Lyapunov spectrum.

490
AN INTRODUCTION TO CHAOTIC SYSTEMS
Fig. 15.36. The Lyapunov spectrum of the cubic crosscatalator equations.
— We can also deﬁne the N-dimensional Lyapunov exponent,
ΛN =
N

j=1
λj
This allows us to consider the growth of various elements, so that line elements
grow like eΛ1t, area elements grow like eΛ2t, and so on. For the cubic crosscata-
lator, we can see that Λ1 and Λ2 are positive, so that line and area elements
expand, but that Λ3 is negative, so that volume elements contract.
— The Lyapunov exponents of an equilibrium point are just the real parts of its
eigenvalues. To see this, note that, in the neighbourhood of an equilibrium point,
x = x∗,
d∆x
dt
= Df(x∗)∆x,
with Df(x∗) a constant matrix.
This means that the fundamental matrix is
M = exp (Df(x∗)t) (see Section 14.3.2). If µi are the eigenvalues of Df(x∗),
then the eigenvalues of M are eµit. On substituting these into the deﬁnition
(15.29), we obtain λi = Re(µi).

EXERCISES
491
— Every point in the basin of attraction of an attractor has the same Lyapunov
spectrum.
— Lyapunov exponents characterize average rates of expansion (λi > 0) and con-
traction (λi < 0) in phase space. For conservative systems,  n
i=1 λi = 0, since
the determinant of the Jacobian is unity, which means that the product of its
eigenvalues is unity, and hence the sum of the Lyapunov exponents is zero. A
dissipative system, for which volume elements contract, has  n
i=1 λi < 0.
Finally, we note that if a dynamical system is chaotic then at least one Lyapunov
exponent is positive.
Exercises
15.1
Write down the equations that govern the concentrations of the chemicals
involved in the cubic crosscatalator scheme, (15.4) to (15.9). After deﬁning
suitable dimensionless variables, derive (15.10), noting carefully the con-
ditions on the initial concentration of P and the rate at which P decays
under which the equations are a good approximation.
15.2
Determine the period 2 points of the map
f : x →4x|1.
15.3
Consider the map
H(x) =

3x,
for x ∈
!
0, 1
2
"
,
−2 + 3x,
for x ∈
 1
2, 1
"
.
Prove that the only points that remain in [0, 1] have the form
x =
∞

n=1
an
3n ,
with an ∈{0, 2}.
If a(x) = a1a2a3 . . . , with an ∈{0, 2}, show that
a(H(x)) = σ(a(x)),
where σa = b if bn = an+1. For what value of x is a(x) = 002002002002 . . .?
Show that this point is periodic of period 3 under H.
15.4
In order to determine the nth roots of a, we can try to determine the zeros
of f(x, a) = xn −a using the Newton–Raphson iteration. This is given by
xi+1 = xi −f(xi)
f ′(xi).
(a) Write down the map for the iterates in the determination of the
roots, and show for n = 2 that
xi+1 = 1
2

xi + a
xi

.

492
AN INTRODUCTION TO CHAOTIC SYSTEMS
(b) Show that, for general n, the ﬁxed points of the map are a1/n, and
determine the stability of these points by considering xi+ϵ for ϵ ≪1.
Do you think that this is a good way of determining the nth roots
of a?
15.5
Determine the ﬁxed point of the map
yn+1 =
1
1 + yn
for yn > 0,
and show that with yn = xn/xn+1, xn are the Fibonacci numbers.
15.6
Consider two maps f and g such that f(x) : x →Ax and g(x) : x →Bx,
where x = (x, y)T, and A, B are real 2 × 2 matrices.
(a) Determine a condition for f(x) to have a ﬁxed point other than
the trivial one. Comment on the ﬁxed points of the composition
of f(g(x)) and g(f(x)), stating a condition for which these are the
same set of points.
(b) If det(A) = det(B) = 1, and hence the corresponding maps are area-
preserving, comment on the properties of the composition f(g(x)).
Discuss the diﬀerent options for f and g given that the maps are
area-preserving.
(c) Consider
A =
 1
0
0
−1

,
B =
 1
1
0
1

,
and determine the unstable manifolds, where applicable, of the maps
f, g and f(g).
15.7
Express the system
˙x = x3 + xy2 −x −2y,
˙y = yx2 + y3 −y + 2x
in terms of the polar coordinates (r, θ) and hence calculate the Poincar´e
return map P : R →R as the map of successive intersections of the orbit
x(t) with the positive y-axis. Show that orbits starting on the y-axis outside
the circle r =
#
e2π/ (e2π −1) never return to the y-axis.
15.8
After deﬁning a suitable plane Σ, use MATLAB to calculate a Poincar´e
return map for (i) the forced Duﬃng equation and (ii) the cubic crosscata-
lator equations.
15.9
The equation of motion of a forced simple pendulum is
d 2θ
dt2 + sin θ = ϵ(α + γ cos ωt),
where α, γ and ϵ are positive constants.
Show that if ϵ = 0 then there is a pair of heteroclinic orbits connecting
saddle points of (±π, 0) in the (θ, φ)-plane, where φ = dθ/dt. Deduce that
one of the orbits is given by
θ0(t) = 2 tan−1(sinh t),
φ0(t) = 2sech(t).

EXERCISES
493
Show that the Mel’nikov function for the perturbation problem of inter-
section near (0, 2) of the unstable manifold from (−π, 0) and the stable
manifold to (π, 0) can be expressed as
M(t0) =
∞

−∞
φ0(t −t0)(α + γ cos ωt) dt,
and deduce that
M(t0) = 2π

α + γsech
πω
2

cos(ωt0)

.
Hence show that there is chaos for small ϵ if γ > α cosh( 1
2πω).
15.10
Arnol’d’s cat map maps the torus T2 = R2/Z2 to itself, and is given by
xn+1 = f(xn), where xn = (xn, yn) and
f(x, y) = (x + y modulo 1, x + 2y modulo 1).
Show that this map is area-preserving. Find its Lyapunov exponents.
15.11
Calculate the Lyapunov spectrum of the Lorenz equations.
15.12
Project The two-dimensional motion of a particle in a ﬂow with stream
function ψ = ψ(x, y, t) is governed by the equations
˙x = ∂ψ
∂y ,
˙y = −∂ψ
∂x .
(E15.1)
Consider the motion of a particle under the inﬂuence of an impulsive Stokes
ﬂow, for which momentum is negligibly small, and ψ = ψ∗(x, y)δ(t −t0).
We can integrate (E15.1) to show that
x1 −x0 = ∂ψ∗
∂y

(x0,y0)
,
y1 −y0 = −∂ψ∗
∂x

(x0,y0)
,
(E15.2)
where (x0, y0) is the position of the particle before the impulse and (x1, y1)
its position after the impulse.
(a) Show that the map deﬁned in (E15.2) is not area-preserving, but
that the map
x1 −x0 = ∂ψ∗
∂y

(x0,y1)
y1 −y0 = −∂ψ∗
∂x

(x0,y1)
,
(E15.3)
is area-preserving.
(b) Write a MATLAB script that iterates points forward under the in-
ﬂuence of the pulsed ﬂow
ψ(x, y, t) =
∞

n=0
ψ∗(x, y)δ(t −n),
where the stream function is that associated with a point force at

494
AN INTRODUCTION TO CHAOTIC SYSTEMS
(x, y) = (d, h) above a solid plane at y = 0. This is known as a
Stokeslet, and its stream function is given by
ψ∗
d,h(x, y) =
α(x −d)
1
2 log
(x −d)2 + (y + h)2
(x −d)2 + (y −h)2
	
−
2hy
(x −d)2 + (y + h)2

.
Here α is the strength parameter (see Otto, Yannacopolous and
Blake, 2001, for more details). Use the area-preserving map (E15.3).
This will give a stroboscopic plot of the trajectory of the point.
(c) Consider the eﬀect of alternating Stokeslets at (0, 1
2) and (0, 3
2) with
the same strengths. Show that this leads to chaotic dynamics. This
is similar to Aref’s blinking vortex which leads to chaotic advection
(Aref, 1984).
(d) Now consider the ﬂow associated with Stokeslets that are not on a
vertical line, for instance (−1
2, 1
2) and ( 1
2, 1
2) or (−1
2, 1
2) and ( 1
2, 3
2).
(e) By constructing the Jacobian associated with the ﬂow, determine
the nature of any ﬁxed or periodic points, and determine the Lya-
punov exponents associated with the ﬂow.
(f) This model can be extended to other ﬂows (see Ottino, 1989). In-
vestigate the combination of other fundamental solutions of Stokes
ﬂow.

APPENDIX 1
Linear Algebra
A1.1
Vector Spaces Over the Real Numbers
Let V be a set of objects called vectors, of the form V = {. . . , x, y, z, . . . }, and
let R denote the real numbers, or scalars. The set V forms a vector space over
R if, for all x, y, z ∈V and α, β ∈R,
(i) x + y = y + x,
(ii) (x + y) + z = x + (y + z),
(iii) x + 0 = x,
(iv) x + (−x) = 0,
(v) α(x + y) = αx + αy,
(vi) (α + β)x = αx + βx,
(vii) (αβ)x = α(βx),
(viii) 1x = x.
These conditions are the familiar laws of commutativity, associativity and distribu-
tivity for the vectors and scalars, together with the existence of inverses and iden-
tities for the scalars and vectors.
Examples
(i) V = Rn, so that
x =





x1
x2
...
xn




,
y =





y1
y2
...
yn




,
are n-dimensional vectors that can be written in terms of their coordinates.
If we then deﬁne vector addition and scalar multiplication by
x + y =





x1 + y1
x2 + y2
...
xn + yn




,
αx =





αx1
αx2
...
αxn




,
it is straightforward to verify that Rn is a vector space over R.

496
LINEAR ALGEBRA
(ii) V = Pn =

anxn + an−1xn−1 + · · · + a1x + a0 | an ∈R, x ∈[α, β]

, so
that V is the set of polynomials of degree n with domain x ∈[α, β]. A
typical member of V would be an object of the form 6x3 −2x + 1. If we
deﬁne vector addition and scalar multiplication by
(f + g)(x) = f(x) + g(x),
(αf)(x) = αf(x),
for x ∈[α, β], and the zero function by 0(x) = 0, it is again easy to verify
that V is a vector space over R.
A subset B = {b1, b2, . . . , bn} of a vector space V is said to be linearly inde-
pendent if α1b1 + α2b2 + · · · + αnbn = 0 implies that α1 = α2 = · · · = αn = 0.
If α1x1 + α2x2 + · · · + αnxn = 0, and the αi are not all zero, we say that the set
of vectors x1, x2, . . . , xn is linearly dependent.
The subset B forms a basis for V if, for every x ∈V , we can write x as a linear
combination of the elements of B, so that x = α1b1 + α2b2 + · · · + αnbn, for
some αi ∈R. The set of all linear combinations of b1, . . . , bn is called the span of
these vectors. If span (b1, . . . , bn) = V , then b1, . . . , bn form a basis for V .
A vector space V is ﬁnite dimensional if it has a basis with a ﬁnite number of
elements. If it is not ﬁnite dimensional, and it has a basis with an inﬁnite number
of elements, it is said to be inﬁnite dimensional.
Examples
(i) Consider V = Rn. The subset
B =














1
0
...
0




,





0
1
...
0




, . . . ,





0
0
...
1














= {b1, b2, . . . , bn} ,
forms a basis, since





x1
x2
...
xn




= x1b1 + x2b2 + · · · + xnbn.
There are other bases for Rn, but this is the simplest. All other bases also
have n elements.
(ii) Consider the vector space
V =
 ∞

n=0
an cos nx
 an ∈R, x ∈[−π, π]

,
which consists of convergent Fourier series deﬁned on −π ⩽x ⩽π. A basis
for V is B = {1, cos x, cos 2x, . . . }, which contains an inﬁnite number of
elements. This shows that V is inﬁnite dimensional.

A1.2 INNER PRODUCT SPACES
497
A1.2
Inner Product Spaces
The inner or dot product of two elements, x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn),
of Rn is deﬁned to be
⟨x, y⟩= x1y1 + x2y2 + · · · + xnyn.
For a general vector space, V over R, the inner product is a mapping, ⟨. , . ⟩:
V × V →R, with the three properties
(i) ⟨x, x⟩⩾0,
(ii) ⟨x, x⟩= 0 if and only if x = 0,
(iii) ⟨αx + βy, z⟩= α⟨x, z⟩+ β⟨y, z⟩,
for x, y, z ∈V , α, β ∈R.
A vector space with an inner product deﬁned on it is called an inner product
space. An important example is the space of all real-valued functions, C(I), deﬁned
on an interval I = [a, b]. It is straightforward to conﬁrm, using the properties of
the Riemann integral, that
⟨f, g⟩=
 b
a
f(x) g(x) dx
is an inner product. For example, if I = [−1, 1], f(x) = x and g(x) = x3, then
⟨f, g⟩=
' 1
−1 x4 dx = 2
5.
Two nonzero vectors, x and y, in an inner product space are said to be orthog-
onal if ⟨x, y⟩= 0. A set of nonzero vectors in an inner product space, {xi} for
i ⩾1, whose members are mutually orthogonal is necessarily linearly independent.
To see this, suppose that α1x1 + α2x2 + · · · + αnxn = 0 for α1, α2, . . . , αn ∈R.
This means that α1⟨x1, xj⟩+ α2⟨x2, xj⟩+ · · · + αn⟨xn, xj⟩= αj⟨xj, xj⟩= 0, and
hence, by property (ii) above, αj = 0 for j ⩾0.
A norm, ||x||, of a vector x must have the four properties
(i) ||x|| is a non-negative real number,
(ii) ||x|| = 0 if and only if x = 0,
(iii) ||kx|| = |k| ||x|| for all real k,
(iv) ||x + y|| ⩽||x|| + ||y||, the triangle inequality.
The Euclidean norm of a vector is deﬁned to be ||x|| =
#
⟨x, x⟩, and gives the
size or length of x. This is familiar in R3, with ⟨x, y⟩= x1y1 + x2y2 + x3y3, so that
||x|| =
#
x2
1 + x2
2 + x2
3. In C(I), there are diﬀerent types of norm. Using the inner
product discussed above, we can deﬁne a norm
||f|| =
+ b
a
f 2(x) dx.
A useful relationship between the inner product and this norm is the Cauchy–
Schwartz inequality,
|⟨x, y⟩| ⩽||x|| ||y||.

498
LINEAR ALGEBRA
We can also deﬁne the sup norm through
||f|| = sup {f(x) | x ∈I} .
The function space C(I) is complete under the sup norm.† This can be useful
when proving rigorous results about diﬀerential equations.
A1.3
Linear Transformations and Matrices
A transformation, T, from a vector space, V , into itself, denoted by T : V →V , is
linear if
(i) T(x + y) = T(x) + T(y) ∀x, y ∈V ,
(ii) T(λx) = λT(x) ∀λ ∈R, x ∈V .
It follows immediately from this deﬁnition that T(λx + µy) = λT(x) + µT(y) and
T(0) = 0.
Examples
(i) If V = R3 and T : R3 →R3 is deﬁned by
T


x1
x2
x3

=


x2
x3
x1

,
then
T(x + y) = T




x1 + y1
x2 + y2
x3 + y3



=


x2 + y2
x3 + y3
x1 + y1

= T(x) + T(y),
T(λx) = T




λx1
λx2
λx3



=


λx2
λx3
λx1

= λT(x),
so that T is a linear transformation.
(ii) If V = Pn, the vector space of polynomials of degree n, and
T(anxn + · · · + a1x + a0) = nanxn−1 + · · · + a1,
T is a linear transformation, and can be identiﬁed with the operation of
diﬀerentiation.
Linear transformations can be represented by considering their eﬀect on the
basis vectors. If T(bj) = α1jb1 +α2jb2 +· · ·+αnjbn, and we take a general vector
x = λ1b1 + λ2b2 + · · · λnbn, then
T(x) = λ1T(b1) + λ2T(b2) + · · · + λnT(bn),
= λ1(α11b1 + α21b2 + · · · + αn1bn) + · · · + λn(α1nb1 + α2nb2 + · · · + αnnbn),
† See Kreider, Kuller, Ostberg and Perkins (1966) for a discussion of completeness.

A1.4 THE EIGENVALUES AND EIGENVECTORS OF A MATRIX
499
which, using the standard deﬁnition of matrix multiplication, we can write as
T





λ1
λ2
...
λn




=





α11
α12
. . .
α1n
α21
α22
. . .
α2n
...
...
...
αn1
αn2
. . .
αnn










λ1
λ2
...
λn




.
We say that the matrix
A =





α11
α12
. . .
α1n
α21
α22
. . .
α2n
...
...
...
αn1
αn2
. . .
αnn





is a representation of the transformation. In example (i) above,
T


x1
x2
x3

=


x2
x3
x1

,
so that
A =


0
1
0
0
0
1
1
0
0

.
A1.4
The Eigenvalues and Eigenvectors of a Matrix
If A is an n × n matrix and x is an n × 1 column vector, the eigenvalues of A
are deﬁned as those values of λ for which the equation Ax = λx has a nontrivial
solution. For each of these values of λ, the eigenvectors of A are the corresponding
values of x. This deﬁning equation can be rearranged into the form (A −λI)x = 0,
where I is the n×n identity matrix, for which the condition for nontrivial solutions
is det(A−λI) = 0. This is known as the characteristic equation associated with
the matrix A.
Example
Find the eigenvalues and eigenvectors of the matrix
A =
 4
−1
2
1

.
The eigenvalues satisfy
det
 4 −λ
−1
2
1 −λ

= 0.
This gives λ2 −5λ + 6 = 0 so that λ = 2 or 3. The eigenvectors satisfy
 4
−1
2
1
  x
y

= λ
 x
y

.

500
LINEAR ALGEBRA
For λ = 2, 2x −y = 0, so that the eigenvector is α (1, 2)T for any nonzero real α.
Here, the superscript T denotes the transpose of the vector. For λ = 3, x −y = 0
so that the eigenvector is β (1, 1)T for any nonzero real β. The eigenvectors are
linearly independent and span R2.
Example
Find the eigenvalues and eigenvectors of the matrix
A =


0
0
1
0
−1
0
2
2
1

.
The eigenvalues satisfy
det


−λ
0
1
0
−1 −λ
0
2
2
1 −λ

= 0,
so that (λ −2)(λ + 1)2 = 0, and hence λ = 2 or −1. The associated eigenvectors
are α(1, 0, 2)T corresponding to λ = 2, and β(1, 0, −1)T corresponding to λ = −1.
These eigenvectors span a two-dimensional subspace of R3.
A particularly useful result concerning eigenvalues is that, if an n × n matrix
A has n distinct eigenvalues λ1, λ2, . . . , λn, there exists a nonsingular matrix B,
whose columns are the eigenvectors of A, such that
B−1AB = diag(λ1, λ2, . . . , λn) =





λ1
0
. . .
0
0
λ2
0
. . .
...
0
0
. . .
λn




.
The only nonzero entries of this matrix are on its diagonal, and are the eigenvalues
of A.
Consider the system of diﬀerential equations ˙x = Ax, where x = (x1, x2, . . . , xn)T
and a dot denotes diﬀerentiation.
If we write x = By, the system changes to
˙y = B−1ABy, which is considerably easier to analyze if B−1AB is diagonal, since
the diﬀerential equations are all decoupled.
Example
Let’s try to ﬁnd a simpliﬁcation of the system of diﬀerential equations
˙x1
=
x3,
˙x2
=
x1,
˙x3
=
2x1 + x2.
We can write this in matrix form as ˙x = Ax with
A =


0
0
1
1
0
0
2
1
0

.

A1.4 THE EIGENVALUES AND EIGENVECTORS OF A MATRIX
501
We need to simplify the structure of the matrix A. To do this we ﬁrstly ﬁnd the
eigenvalues, λ = −1, 1
2(1+
√
5) and 1
2(1−
√
5), and the corresponding eigenvectors,


1
−1
−1

,


1
−1
2(1 −
√
5)
1
2(1 +
√
5)

,


1
−1
2(1 +
√
5)
1
2(1 −
√
5)

.
These are orthogonal, and hence linearly independent, and form a basis for R3.
After choosing our matrix B to be
B =


1
1
1
−1
−1
2(1 −
√
5)
−1
2(1 +
√
5)
−1
1
2(1 +
√
5)
1
2(1 −
√
5)

,
with inverse
B−1 =



1
1
−1
1
√
5
1
2
√
5(3 −
√
5)
−
1
2
√
5(1 −
√
5)
1
√
5
−
1
2
√
5(3 +
√
5)
1
2
√
5(1 +
√
5)


,
we ﬁnd that
B−1AB =


−1
0
0
0
1
2(1 +
√
5)
0
0
0
1
2(1 −
√
5)

= diag(−1, 1
2(1 +
√
5), 1
2(1 −
√
5))
is the representation of A with respect to the basis of eigenvectors. The transformed
system of diﬀerential equations therefore takes the form
˙y1 = −y1,
˙y2 = 1
2(1 +
√
5)y2,
˙y3 = 1
2(1 −
√
5)y3.
These have the simple solutions y1 = c1 exp(−t), y2 = c2 exp{ 1
2(1 +
√
5)t}, y3 =
c3 exp{ 1
2(1 −
√
5)t}. Finally, the solution is x1 = y1 + y2 + y3, x2 = −y1 −1
2(1 −
√
5)y2 −1
2(1 +
√
5)y3 and x3 = −y1 + 1
2(1 +
√
5)y2 + 1
2(1 −
√
5)y3, in terms of the
original variables.
Finally, we will often make use of the Cayley–Hamilton theorem.
Theorem A1.1 (Cayley–Hamilton) Every square matrix A satisﬁes its own
characteristic equation, det(A −λI) = 0.
The proof of this theorem is rather involved, and we will not consider it here (see
Morris, 1982). The Cayley–Hamilton theorem shows that Ak, with k ⩾n, can be
written as a linear combination of I, A, A2, . . . , An−1.

APPENDIX 2
Continuity and Diﬀerentiability
Let f be a real-valued function deﬁned on some open interval that contains the
point x = c. We say that
f is continuous at x = c if and only if lim
x→c f(x) = f(c).
The function f is continuous on the interval (a, b) if and only if it is continuous for
all x ∈(a, b). The set of all continuous functions f : (a, b) →R (R denotes the set
of real numbers) forms a vector space denoted by C(a, b) or C0(a, b).
A function f is continuous on [a, b] if it is continuous on (a, b) and
lim
x→a+ f(x) = f(a),
lim
x→b−f(x) = f(b).
In other words, there is no singular behaviour at the ends of the interval. We will
use C[a, b] to denote the vector space of continuous functions f : [a, b] →R.
An alternative, but equivalent, deﬁnition of continuity is that f is continuous
at a point x = x0 if, for every ϵ > 0, there exists a function δ(x0, ϵ) such that
|f(x) −f(x0)| < ϵ when |x −x0| < δ. If δ depends upon ϵ but not upon x0 in some
interval I, we say that f is uniformly continuous on I. It can be shown that, if
a function is continuous on a closed interval, then it is also uniformly continuous
there.
If limx→c+ f(x) ̸= limx→c−f(x), f(x) is not continuous at x = c. There is an
important class of functions of this form, for which there are a ﬁnite number of
discontinuities, at each of which f(x) jumps by a ﬁnite amount. These are known
as piecewise continuous functions, and, like the continuous functions, form a
vector space, which we denote by PC(a, b).
If the limit of {f(x) −f(c)} /(x −c) exists as x →c, we say that f is diﬀeren-
tiable at x = c, and denote its derivative by
f ′(c) = lim
x→c
f(x) −f(c)
x −c
	
.
The set of all once-diﬀerentiable functions on (a, b) forms a vector space which we
denote by C1(a, b). We can make similar deﬁnitions of C2(a, b), C3(a, b), . . . , Cn(a, b),
. . . , C∞(a, b), in which the higher derivatives are deﬁned in terms of limits of deriva-
tives of one order lower.
Examples of functions that live in these spaces are:
(i) f : (0, 2π) →R : f(x) = sin x. Since f ′(x) = cos x and f ′′(x) = −sin x =

CONTINUITY AND DIFFERENTIABILITY
503
−f(x), this function can be diﬀerentiated as many times as we like and the
result will be a continuous function. We conclude that f ∈C∞(0, 2π).
(ii) f : (−1, 1) →R : f(x) = x2 for x ⩾0, f(x) = 0 for x < 0. This function
can be diﬀerentiated twice, but, since f ′′(x) = 0 for x < 0 and f ′′(x) = 2 for
x ⩾0, no more than twice. We conclude that f ∈C2(−1, 1).
(iii) f : R →R : f(x) =  ∞
n=1
1
n! sin (n!)2 x. This function, which is illustrated
in Figure A2.1, is continuous on R but nowhere diﬀerentiable, so that f ∈
C0(R), but f /∈Cn(R) for any n > 0.
Fig. A2.1. The continuous, nowhere diﬀerentiable function f(x) =  ∞
n=1
1
n! sin (n!)2 x.
(iv) f : [0, 3] →R :
f(x) =



x2
for 0 ⩽x < 1,
cos x
for 1 ⩽x ⩽2,
e−x
for 2 < x ⩽3,
(A2.1)
which is plotted in Figure A2.2. This function is discontinuous at x = 1 and
x = 2, and therefore f ∈PC[0, 3].

504
CONTINUITY AND DIFFERENTIABILITY
Fig. A2.2. The piecewise continuous function given as example (iv). A cross indicates the
value of the function at a point of discontinuity.
If f is diﬀerentiable at c ∈(a, b), it is continuous at c (but the converse of this is false,
see example (iii) above) so that C0(a, b) ⊃C1(a, b) ⊃C2(a, b) · · · ⊃Cn(a, b) · · · .
We conclude this section with two useful theorems that you should recall from
your ﬁrst course in real analysis.
Theorem A2.1 If f : [a, b] →R and f ∈C[a, b], then ∃K such that |f(x)| <
K ∀x ∈[a, b]. In words, a continuous function on a closed, bounded interval is
bounded.
Theorem A2.2 (The mean value theorem) If f : (a, b) →R and f ∈C1(a, b),
then ∃c ∈(a, b) such that f(b) −f(a) = (b −a)f ′(c).

APPENDIX 3
Power Series
Many commonly encountered functions can be expressed as power series, for exam-
ple,
sin x = x −x3
3! + x5
5! −· · · =
∞

n=0
(−1)n
x2n+1
(2n + 1)!,
cos x = 1 −x2
2! + x4
4! −· · · =
∞

n=0
(−1)n x2n
(2n)!.
(A3.1)
In general, we can develop a power series representation for a suﬃciently diﬀeren-
tiable function, provided that it involves no singularities, fractional powers of x or
logarithms.
A3.1
Maclaurin Series
The Maclaurin series is a power series about x = 0. If f ∈C∞(a, b) for some
a < 0 < b, the series takes the form
f(x) = f(0) + xf ′(0) + x2
2! f ′′(0) + · · · + xn
n! f (n)(0) + · · ·.
Here, a prime denotes a derivative with respect to x and f (n)(x) is the nth derivative.
Example
Let’s determine the Maclaurin series expansion for the function f(x) = 1/(1 + x).
Since
f(x) =
1
1 + x,
f(0) = 1,
f ′(x) = −
1
(1 + x)2 ,
f ′(0) = −1,
f ′′(x) =
2
(1 + x)3 ,
f ′′(0) = 2,
f (3)(x) =
−3 · 2
(1 + x)4 ,
f (3)(0) = −3!,
...
f (n)(x) =
(−1)nn!
(1 + x)n+1 ,
f (n)(0) = (−1)nn!,

506
POWER SERIES
the Maclaurin series is
f(x) = 1 + (−1)x + 2x2
2! + (−1)3!x3
3! + · · · + (−1)nn!xn
n! + · · ·
= 1 −x + x2 −x3 + · · · + (−1)nxn + · · · =
∞

n=0
(−1)nxn.
A3.2
Taylor Series
The Maclaurin series is a special case of the Taylor series, which is a power series
about x = x0. Its existence also requires reasonable behaviour of the function at
x = x0, and for a C∞function takes the form,
f(x) = f(x0) + (x −x0)f ′(x0) + (x −x0)2
2!
f ′′(x0) + · · · + (x −x0)
n!
n
f (n)(x0) + · · · .
Example
Let’s determine the Taylor series of the function f(x) = 1/(1 + x) at the point
x0 = 1. Since
f(x) =
1
1 + x,
f(x0) = 1
2,
f ′(x) = −
1
(1 + x)2 ,
f ′(x0) = −1
22 ,
f ′′(x) =
2
(1 + x)3 ,
f ′′(x0) = 2
23 ,
f (3)(x) =
−3 · 2
(1 + x)4 ,
f (3)(x0) = −3!
24 ,
...
f (n)(x) =
(−1)nn!
(1 + x)n+1 ,
f (n)(x0) = (−1)n n!
2n+1 ,
the Taylor series is
f(x) =
∞

n=0
(x −1)n
n!
(−1)nn!
2n+1
= 1
2
∞

n=0
(x −1)n(−1)n
2n
.
(A3.2)
A3.3
Convergence of Power Series
In order to determine for what values of x a power series is convergent, we can
usually use the ratio test, which says that
the series
∞
 
n=0
an



converges if limn→∞|an+1/an| < 1,
diverges if limn→∞|an+1/an| > 1.

A3.4 TAYLOR SERIES FOR FUNCTIONS OF TWO VARIABLES
507
In the previous example, (A3.2),
an = (x −1)n(−1)n
2n+1
,
an+1 = (x −1)n+1(−1)n+1
2n+2
,
so that, for convergence, we need
lim
n→∞

an+1
an
 =

x −1
2
 < 1.
This means that the series converges when −1 < x < 3. This deﬁnes the domain
of convergence of the power series (A3.2). This is often written as |x −1| < 2,
which deﬁnes the radius of convergence of the series as two units from the point
x = 1. Notice that the function f(x) is singular at x = −1, and will not have a
Taylor series about this point. It is worth pointing out that the Taylor series of a
polynomial is a terminating series and hence has an inﬁnite radius of convergence.
A useful composite result from the theory of Taylor series is that if f(x) =
 ∞
n=0 an(x −x0)n is a Taylor series with radius of convergence R (it converges for
|x −x0| < R), f is diﬀerentiable for all x such that |x −x0| < R, and f ′(x) =
 ∞
n=1 nan(x −x0)n−1 has the same radius for convergence as the series for f(x).
This ‘term by term’ diﬀerentiation result allows us to develop a method for solving
diﬀerential equations using power series – the method of Frobenius, which we discuss
in Chapter 1.
Note that another useful test for the convergence of a series is the comparison
test, which says that
if  ∞
n=0 an and  ∞
n=0 bn are series of positive terms then
(i) if  ∞
n=0 an converges and bk ⩽ak for k > k0,  ∞
n=0 bn also converges,
(ii) if  ∞
n=0 an diverges and bk ⩾ak for k > k0,  ∞
n=0 bn also diverges.
A3.4
Taylor Series for Functions of Two Variables
If f = f(x, y) is a scalar ﬁeld that has suﬃcient partial derivatives at the point
(x0, y0), the Taylor series about this point is
f(x, y) = f(x0, y0) + (x −x0) ∂f
∂x

(x0,y0)
+ (y −y0)∂f
∂y

(x0,y0)
+ (x −x0)2
2!
∂2f
∂x2

(x0,y0)
+ (x −x0)(y −y0) ∂2f
∂x∂y

(x0,y0)
+ (y −y0)2
2!
∂2f
∂y2

(x0,y0)
+ · · · .
Example
Let’s ﬁnd the ﬁrst two terms in the Taylor series of e−(x2+y2) about the point (0, 0).
∂f
∂x

(0,0)
= ∂f
∂y

(0,0)
= 0,

508
POWER SERIES
∂2f
∂x2

(0,0)
= ∂2f
∂y2

(0,0)
= −2,
∂2f
∂xdy

(0,0)
= 0,
so that e−(x2+y2) = 1 −x2 −y2 + · · · .
We can also generalize the Taylor series from scalar- to vector-valued functions
of two variables. Let’s deﬁne a vector ﬁeld,
f(x) =
f(x)
g(x)

,
x =
x
y

.
Using the Taylor series for each component of this vector we can write
f(x) = f(x0) + J(f)(x0)(x −x0) + · · · ,
where
J(f)(x0) =




∂f
∂x(x0)
∂f
∂y (x0)
∂g
∂x(x0)
∂g
∂y (x0)




is called the Jacobian matrix. This result proves to be very useful in Chapter 9,
where we study nonlinear diﬀerential equations. Note that the deﬁnition of the
Jacobian can easily be generalized to higher dimensional vector ﬁelds.
Example
Let’s ﬁnd the linear approximation to the vector ﬁeld
f =
 ex −1
(1 −y)ex

about the point x0 =
 0
1

. Since f(x0) = 0, the Taylor series takes the form
f(x) = J(f)
0
1
 
x −
0
1
	
+ · · · .
After calculating the Jacobian, this gives
f(x) =
 1
0
0
−1
 
x
y −1

+ · · · =

x
1 −y

+ · · · .

APPENDIX 4
Sequences of Functions
The concepts that we brieﬂy describe in this section are used in Chapter 5, where
we discuss the convergence of Fourier series. Consider a sequence of functions,
{fk(x)} for k = 1, 2, . . . , deﬁned on some interval of the real line, I. We say that
{fk(x)} converges pointwise to f(x) on I if limk→∞fk(x0) exists for all x0 ∈I,
and fk(x0) →f(x0). For example, consider the sequence {fk(x)} = {x + 1/k}
on R. This sequence converges pointwise to f(x) = x as k →∞. The functions
fk(x) and their limit, f(x) = x, are continuous. In contrast, consider the sequence
{fk(x)} = {xk} on [0, 1]. Although each of these functions is continuous, as k →∞,
fk(x) converges pointwise to
f(x) =
 0
for 0 ⩽x < 1,
1
for x = 1,
which is not continuous. As a ﬁnal example, consider the sequence fk(x), deﬁned
on x ⩾0, with
fk(x) =
 k2x(1 −kx)
for 0 ⩽x ⩽1/k,
0
for x ⩾1/k,
(A4.1)
which is illustrated in Figure A4.1. Since fk(x) = 0 for x ⩾1/k, fk(x0) →0 as
k →∞for all x0 > 0. Moreover, fk(0) = 0 for all k, so we conclude that {fk(x)}
converges pointwise to f(x) = 0, a continuous function. However, the individual
members of the sequence don’t really provide a good approximation to the pointwise
limit, ﬁrstly because the maximum value of fk(x) is k/4, which is unbounded as
k →∞, and secondly because
 ∞
0
fk(x) dx = 1/6,
 ∞
0
f(x) dx = 0.
In order to eliminate this sort of behaviour, we need to introduce the concept of
uniform convergence. A sequence of functions {fk(x)} is uniformly convergent
to a function f(x) on an interval of the real line, I, as k →∞if for every ϵ > 0 there
exists a positive integer, K(ϵ), which is not a function of x, such that |fk(x)−f(x)| <
ϵ for all k ⩾K(ϵ) and x ∈I. This means that, for suﬃciently large k, we can make
fk(x) arbitrarily close to f(x) over the entire interval I. For example, to see that
the sequence deﬁned by (A4.1) does not converge uniformly to zero, note that, for a
given value of ϵ, we can only guarantee that |fk(x)−f(x)| = fk(x) < ϵ for k ⩾1/x,
which is not independent of x.

510
SEQUENCES OF FUNCTIONS
Fig. A4.1. The sequence of functions given by (A4.1).
Theorem A4.1 If {fk(x)} is a sequence of continuous functions deﬁned on an
interval of the real line, I = [a, b], that converges uniformly to f(x), then
(i) f(x) is continuous
(ii) for every x ∈I
lim
k→∞
 x
a
fk(s) ds =
 x
a
f(s) ds.
A proof of this theorem can be found in any textbook on analysis.

APPENDIX 5
Ordinary Diﬀerential Equations
Chapters 1 to 4 are concerned with the solution of linear second order diﬀerential
equations with variable coeﬃcients. In this appendix, we will review simple methods
for solving ﬁrst order diﬀerential equations, and second order diﬀerential equations
with constant coeﬃcients.
A5.1
Variables Separable
We will start by looking at a class of equations that can be rewritten in the form
g(y)dy
dt = f(t).
(A5.1)
These equations can be integrated with respect to t to give

g(y)dy
dt dt =

f(t) dt,
and, using the chain rule,

g(y) dy =

f(t) dt.
Example
Determine the solution of the diﬀerential equation
dy
dt + ey cos t = 0,
subject to the condition that y(0) = 1. Firstly, we convert the equation to the form
(A5.1), so that
e−y dy
dt = −cos t.
Integrating with respect to t we have
−e−y = −sin t + C.
Using the boundary condition we ﬁnd that C = −e−1, and rearranging gives the
solution
y(t) = log

1
e−1 + sin t

.

512
ORDINARY DIFFERENTIAL EQUATIONS
A5.2
Integrating Factors
We now consider the linear, ﬁrst order diﬀerential equation
dy
dx + P(x)y = Q(x).
You should recognize this as a type of equation that can be solved by ﬁnding an
integrating factor. If we multiply through by exp

' x P(t)dt

, where t is a dummy
variable for the integration, we have
exp
 x
P(t)dt
	 dy
dx + exp
 x
P(t)dt
	
P(x)y = Q(x) exp
 x
P(t)dt
	
.
We can immediately see that the left hand side of this is d
dx

exp
 x
P(t)dt
	
y
	
,
so we have
d
dx

exp
 x
P(t)dt
	
y
	
= Q(x) exp
 x
P(t)dt
	
.
This can be directly integrated to give
y = A exp

−
 x
P(t)dt
	
+ exp

−
 x
P(t)dt
	  x
Q(s) exp
 s
P(t)dt
	
ds,
where A is a constant of integration. Here it is important to notice the structure
of the solution as y = yh + yp where
yh = A exp

−
 x
P(t)dt
	
is the solution of the homogeneous diﬀerential equation,
dyh
dx + P(x)yh = 0,
and
yp = exp

−
 x
P(t)dt
	  x
Q(s) exp
 s
P(t)dt
	
ds
is the particular integral solution of the inhomogeneous diﬀerential equa-
tion
dyp
dx + P(x)yp = Q(x).
The key idea of an integrating factor used here may seem rather like pulling a rabbit
out of a hat. In fact, the derivation can be performed systematically using the idea
of a Lie group (see Chapter 10).
Example
Let’s ﬁnd the solution of the diﬀerential equation
dy
dx −y = ex,

A5.3 SECOND ORDER EQUATIONS WITH CONSTANT COEFFICIENTS
513
subject to the condition that y(0) = 0. The integrating factor is
exp
 x
−1 dx

= e−x,
so that we have
e−x dy
dx −e−xy = d
dx

e−xy

= 1.
We can integrate this to obtain e−xy = x+c, where c is a constant. Since y(0) = 0,
we must have c = 0 and hence y = xex.
A5.3
Second Order Equations with Constant Coeﬃcients
We will now remind you how to solve second order ordinary diﬀerential equations
with constant coeﬃcients through a series of examples.
Example
Solve the second order ordinary diﬀerential equation
d 2y
dx2 + 3dy
dx + 2y = 0,
subject to the boundary conditions that y(0) = 0 and y′(0) = 1.
We can solve constant coeﬃcient equations by seeking a solution of the form
y = emx, which we substitute into the equation in order to determine m. This gives
d 2y
dx2 + 3dy
dx + 2y =

m2 + 3m + 2

emx = 0.
Since emx is never zero, we require m2 + 3m + 2 = (m + 2)(m + 1) = 0. This gives
m = −1 or m = −2. The general solution of the equation is therefore
y(x) = Ae−x + Be−2x,
with A and B constants. The ﬁrst boundary condition, y(0) = 0, yields
A + B = 0,
(A5.2)
whilst the second, y′(0) = 1, gives
−A −2B = 1.
(A5.3)
We now need to solve (A5.2) and (A5.3), which gives us A = 1 and B = −1, so
that the solution is
y(x) = e−x −e−2x.
It is possible to solve this equation by direct integration, but the method we have
presented above is far simpler. Notice that it is not necessary that both boundary
conditions are imposed at the same place. Boundary conditions at diﬀerent values
of x will just lead to slightly more complicated simultaneous equations.

514
ORDINARY DIFFERENTIAL EQUATIONS
Example: Equal roots
Solve the diﬀerential equation
d 2y
dx2 + 2dy
dx + y = 0,
subject to the boundary conditions y(0) = 1 and y′(0) = 0.
Again we can look for a solution of the form y = emx. We ﬁnd that m satisﬁes
the quadratic equation m2 + 2m + 1 = (m + 1)2 = 0, so that m = −1 is a repeated
root, and we have only determined one solution. In fact, the full solution has the
form
y(x) = Ae−x + Bxe−x.
(A5.4)
The boundary condition y(0) = 1 gives A = 1. Since
y′(x) = −Ae−x + B(−x + 1)e−x,
y′(0) = 0 gives −A + B = 0, and hence B = 1. The solution is therefore
y(x) = e−x + xe−x = (x + 1)e−x.
Example: Imaginary roots
Solve the diﬀerential equation
d 2y
dx2 + 4y = 0,
subject to the boundary conditions y(0) = 1 and y′(0) = 3.
As usual, we seek a solution of the form y = emx, and ﬁnd that m satisﬁes
m2 + 4 = 0. This means that m = ±2i. The general solution is therefore
y(x) = Ae2ix + Be−2ix.
(A5.5)
We could proceed with the solution in this form. However, it may be better to use
eiα = cos α + i sin α. Substituting this into (A5.5) gives
y(x) = A(cos 2x + i sin 2x) + B(cos 2x −i sin 2x)
= (A + B) cos 2x + i(A −B) sin 2x = ˜A cos 2x + ˜B sin 2x.
We have introduced new constants ˜A and ˜B, which we determine from the boundary
conditions to be ˜A = 1 and ˜B = 3/2. The solution is therefore
y(x) = cos 2x + 3
2 sin 2x.
Example: An inhomogeneous equation
Solve the diﬀerential equation
d 2y
dx2 + 3dy
dx + 2y = ex,
subject to the boundary conditions y(0) = y′(0) = 1.

A5.3 SECOND ORDER EQUATIONS WITH CONSTANT COEFFICIENTS
515
Initially we consider just the homogeneous part of the equation and solve
d 2yh
dx2 + 3dyh
dx + 2yh = 0.
As we saw earlier, the general solution of this is
yh(x) = Ae−x + Be−2x.
We now have to ﬁnd a function f(x), the particular integral, which, when substi-
tuted into the inhomogeneous diﬀerential equation, yields the right hand side. We
notice that when we diﬀerentiate ex we get it back again, so we postulate that f(x)
takes the form αex. Substituting y = f(x) = αex into the diﬀerential equation
gives
d 2f
dx2 + 3 df
dx + 2f = αex + 3αex + 2αex = ex.
From this expression we ﬁnd that we need α = 1/6. The solution of the inhomoge-
neous equation is therefore
y(x) = Ae−x + Be−2x + 1
6ex.
It is at this point that we impose the boundary conditions, which show that A = 5/2
and B = −5/3, and hence the solution is
y(x) = 5
2e−x −5
3e−2x + 1
6ex.
Example: Right hand side a solution of the homogeneous equation
Solve the diﬀerential equation equation
d 2y
dx2 + 3dy
dx + 2y = e−x,
subject to the boundary conditions y(0) = y′(0) = 1.
Following the previous
example, we need to ﬁnd a function which, when substituted into the left hand side
of the equation, yields the right hand side. We could try y = αe−x, but, since e−x
is a solution of the equation, substituting it into the left hand side just gives us
zero. Taking a lead from the case of repeated roots, we try a solution of the form
y = f(x) = αxe−x. Since
f(x) = αxe−x,
f ′(x) = α(e−x −xe−x) = α(1 −x)e−x,
f ′′(x) = α(−1 −(1 −x))e−x = α(x −2)e−x,
on substituting into the diﬀerential equation we obtain
α(x −2)e−x + 3α(1 −x)e−x + 2αxe−x = 3αe−x = e−x,
which gives α = 1/3. The solution is therefore
y(x) = Ae−x + Be−2x + 1
3xe−x.

516
ORDINARY DIFFERENTIAL EQUATIONS
We now need to satisfy the boundary conditions, which give A = 8/3 and B = −5/3,
and hence
y(x) = 8
3e−x −5
3e−2x + 1
3xe−x.
This technique is usually referred to as the trial solution method. Clearly it is not
completely satisfactory, as it requires an element of guesswork. We present a more
systematic method of solution in Chapter 1.

APPENDIX 6
Complex Variables
The aim of this brief appendix is to provide a reminder of the results that are needed
in order to be able to invert Laplace transforms using the Bromwich inversion
integral (6.5), and to understand the material on the asymptotic evaluation of
complex integrals in Section 11.2. We have had to be selective in what we have
presented, and note that this appendix is no substitute for a proper course on
complex variables! A good textbook is Ablowitz and Fokas (1997).
A6.1
Analyticity and the Cauchy–Riemann Equations
Consider a complex-valued function of a complex variable, f(s), with s = x + iy.
The natural way to deﬁne the derivative of f is
df
ds = lim
∆s→0
f(s + ∆s) −f(s)
∆s
,
provided that this limit is independent of the way that ∆s = ∆x + i∆y tends to
zero. A function f(s) is said to be analytic in some region, R, of the complex
s-plane if df/ds exists and is unique in R.
Theorem A6.1 If the complex-valued function f(s) = u(x, y) + iv(x, y), where
s = x + iy and u, v, x and y are real, is analytic in a region, R, of the complex
s-plane, then
∂u
∂x = ∂v
∂y ,
∂v
∂x = −∂u
∂y .
(A6.1)
These are known as the Cauchy–Riemann equations.
Proof By deﬁnition,
df
ds = lim
∆s→0
f(s + ∆s) −f(s)
∆s
= lim
∆x→0
∆y→0
u(x + ∆x, y + ∆y) + iv(x + ∆x, y + ∆y) −u(x, y) −iv(x, y)
∆x + i∆y
.
If ∆y = 0,
df
ds = lim
∆x→0
u(x + ∆x, y) −u(x, y) + i {v(x + ∆x, y) −v(x, y)}
∆x
= ∂u
∂x + i∂v
∂x,

518
COMPLEX VARIABLES
whilst if ∆x = 0,
df
ds = lim
∆y→0
u(x, y + ∆y) −u(x, y) + i {v(x, y + ∆y) −v(x, y)}
i∆y
= ∂v
∂y −i∂u
∂y .
These are clearly not equal unless the Cauchy–Riemann equations hold.
Theorem A6.2 If the Cauchy–Riemann equations, (A6.1), hold in a region, R,
of the complex s-plane for some complex-valued function f(s) = u(x, y) + iv(x, y),
where s = x + iy and u, v, x and y are real, then, provided that all of the partial
derivatives in (A6.1) are continuous in R, f(s) is analytic in R.
We will not give a proof of this here.
A6.2
Cauchy’s Theorem, Cauchy’s Integral Formula and Taylor’s
Theorem
The contour integral of a complex-valued function f(s) along some contour C in
the complex s-plane is deﬁned to be

C
f(s) ds =
 b
a
f(s(t))ds
dt (t) dt,
where s(t) for a ⩽t ⩽b is a parametric representation of the contour C. By con-
vention, the integral around a closed contour is taken in the anticlockwise direction.
Theorem A6.3 (Cauchy’s theorem) If f(s) is a single-valued, analytic function
in a simply-connected domain D in the complex s-plane, then, along any simple
closed contour, C, in D,

C
f(s) ds = 0.
Proof If f(s) = u + iv and ds = dx + i dy,

C
f(s) ds =

C
(u dx −v dy) + i

C
(v dx + u dy).
If df/ds is continuous in D, then u and v have continuous partial derivatives there.
We can therefore use Green’s theorem in the plane to write

C
f(s) ds = −
 
D
∂v
∂x + ∂u
∂y

dx dy + i
 
D
∂u
∂x −∂v
∂y

dx dy.
Since f is analytic in D, the Cauchy–Riemann equations hold, and the result is
proved. If df/ds is not continuous in D, the proof is rather more technical, and we
will not give it here.

A6.2 CAUCHY’S THEOREM AND TAYLOR’S THEOREM
519
Theorem A6.4 (Cauchy’s integral formula) If f(s) is a single-valued, analytic
function in a simply-connected domain D in the complex s-plane, then
f(z) =
1
2πi

C
f(s)
(s −z) ds,
where C is any simple closed contour in D that encloses the point s = z.
Proof
Let Cδ be a small circle of radius δ, centred on the point s = z. Using
Cauchy’s theorem,

C
f(s)
(s −z) ds =

Cδ
f(s)
(s −z) ds,
which we can rewrite as

C
f(s)
(s −z) ds = f(z)

Cδ
ds
(s −z) +

Cδ
f(s) −f(z)
(s −z)
ds.
By writing the ﬁrst of these two integrals in terms of polar coordinates, s = z+δeiθ,
we ﬁnd that

Cδ
ds
(s −z) =
 2π
0
iδeiθ
δeiθ dθ = 2πi.
We can deal with the second integral by noting that, since f(s) is continuous,
|f(s) −f(z)| < ϵ for suﬃciently small |s −z| = δ. This means that


Cδ
f(s) −f(z)
(s −z)
ds
 ⩽

Cδ
|f(s) −f(z)|
|s −z|
|ds| ⩽ϵ
δ

Cδ
|ds| = 2πϵ.
Since ϵ →0 as δ →0, the result is proved.
An extension of Cauchy’s integral formula is
Theorem A6.5 If f(s) is a single-valued, analytic function in a simply-connected
domain D in the complex s-plane, then all the derivatives of f exist in D, and are
given by
f (n)(z) = n!
2πi

C
f(s)
(s −z)n+1 ds,
(A6.2)
where C is any simple closed contour in D that encloses the point s = z.
The proof of this is similar to that of Cauchy’s integral formula.
Theorem A6.6 (Taylor’s theorem) If f(z) is analytic and single-valued inside
and on a simple closed contour, C, and z0 is a point inside C, then
f(z) = f(z0) + (z −z0)f ′(z0) + 1
2! (z −z0)2f ′′(z0) + · · · =
∞

n=0
1
n!(z −z0)nf (n)(z0).
(A6.3)

520
COMPLEX VARIABLES
Proof Let δ be the minimum distance from z0 to the curve C, and let γ be any circle
centred on z0 with radius ρ < δ, so that, for any point z inside γ, |z −z0| < ρ < δ.
By Cauchy’s integral formula,
f(z) =
1
2πi

C
f(w)
w −z dw =
1
2πi

γ
f(w)
w −z dw.
We also have
1
w −z =
1
w −z0 −z + z0
=
1
w −z0
1
1 −z−z0
w−z0
=
1
w −z0

1 + z −z0
w −z0
+ (z −z0)2
(w −z0)2 + · · ·

=
1
w −z0
+
z −z0
(w −z0)2 + · · · +
(z −z0)n
(w −z0)n+1 + · · · ,
and this series is uniformly convergent inside γ, since |z −z0| / |w −z0| < 1. This
means that
f(z) =
1
2πi

γ
f(w)
w −z dw =
1
2πi

γ
f(w)
w −z0
dw + (z −z0) 1
2πi

γ
f(w)
(w −z0)2 dw + · · ·
+ (z −z0)n
1
2πi

γ
f(w)
(w −z0)n+1 dw + · · · .
Equation (A6.2) then gives us (A6.3). This series converges in any circle centred
on z0 with radius less than δ. If z0 = 0 we get the simpler form
f(z) = f(0) + zf ′(0) + 1
2!z2f ′′(z) + · · · .
Theorem A6.7 (Cauchy’s inequality) If f(z) is analytic for |z| < R with
f(z) =  ∞
n=0 anzn, then, if Mr is the supremum of |f(z)| on the circle |z| = r < R,
we have |an| < Mr/rn.
Proof By Taylor’s theorem,
an =
1
2πi

|z|=r
f(z)
zn+1 dz,
so that
|an| ⩽1
2π

|z|=r
|f(z)|
rn+1 |dz| ⩽1
2π
Mr
rn+1 2πr = Mr
rn .

A6.3 THE LAURENT SERIES AND RESIDUE CALCULUS
521
A6.3
The Laurent Series and Residue Calculus
Theorem A6.8 Any function f(s) that is single-valued and analytic in an annulus
r1 ⩽|s −s0| ⩽r2 for r1 < r2 real and positive has a series expansion
f(s) =
∞

n=−∞
cn(s −s0)n,
which is convergent for r1 < |s −s0| < r2. This is known as the Laurent series,
and its coeﬃcients are given by
cn =
1
2πi

C
f(s)
(s −s0)n+1 ds
for any simple closed contour that encloses s = s0 and lies in the annulus r1 ⩽
|s −s0| ⩽r2.
Although we quote this important theorem, which underlies the techniques of
residue calculus, we will not give a proof here. We do note however that

C
f(s) ds = 2πic−1.
For an analytic function, c−1 = 0, and we have Cauchy’s theorem. If f is not ana-
lytic within C, this result shows that we can calculate the integral by determining
the coeﬃcient of 1/(s −s0) in the Laurent series. The coeﬃcient c−1 is called the
residue of f(s) at s = s0.
Theorem A6.9 (The residue theorem) Let C be a simple, closed contour, and
let f(s) be a complex-valued function that is single-valued and analytic on and within
C, except at n isolated singular points, s = s1, s2, . . . , sn. Then

C
f(s) ds = 2πi
n

j=1
aj,
where aj is the residue of f(s) at s = sj.
Proof Consider the closed contour ˆC, shown in Figure A6.1, which is the contour
C deformed into small circles around each singular point and joined to the original
position of C by straight contours. Since f is analytic within ˆC,

ˆ
C
f(s) ds = 0,
by Cauchy’s theorem. The integrals along the straight parts of ˆC cancel out in the
limit as they approach each other, so we conclude that

ˆ
C
f(s) ds =

C
f(s) ds −
n

j=1

Cj
f(s) ds = 0,

522
COMPLEX VARIABLES
and hence

C
f(s) ds = 2πi
n

j=1
aj.
x
x
x
C2
C1
Cn
C
•
•
•
•
Fig. A6.1. The contours C and Cj.
The residue theorem simply states that the integral around a simple, closed
contour is equal to 2πi times the sum of the residues enclosed by C. This can
be used in a variety of ways, in particular to evaluate the integral in the Laplace
inversion formula, (6.5).
Note that, for a function with a pole of order m at s = s0, and Laurent series
expansion
f(s) =
a−m
(s −s0)m +
a−(m−1)
(s −s0)m−1 + · · · ,
we can write
f(s) =
1
(s −s0)m g(s),
where
g(s) = a−m + a−(m−1)(s −s0) + · · · + a−1(s −s0)m−1 + · · · ,
and hence
d m−1g
dsm−1 = (m −1)!a−1 + m! a0(s −s0) + · · · ,

A6.4 JORDAN’S LEMMA
523
which shows that the residue of f(s) at s = s0 is
a−1 =
1
(m −1)!
d m−1
dsm−1 {(s −s0)m f(s)}

s=s0
.
In particular, at a simple pole, for which m = 1, a−1 = (s −s0)f(s0), and at a
double pole, for which m = 2,
a−1 = d
ds

(s −s0)2 f(s)

s=s0
.
A6.4
Jordan’s Lemma
Jordan’s lemma allows us to neglect an integral that often arises when evaluating
inverse Laplace transforms of functions F(s), provided that |F(s)| →0 uniformly
as |s| →∞in the left half plane.
Lemma A6.1 (Jordan) Let CJ be a semi-circular arc of radius R centred at s = s0
in the left half of the complex s-plane. If F(s) is a complex-valued function of s
and |F(s)| →0 uniformly on CJ as R →∞, then
lim
R→∞

CJ
estF(s) ds = 0,
for t a positive constant.
Fig. A6.2. sin θ ⩾2θ/π.

524
COMPLEX VARIABLES
Proof On CJ, s = s0 + Reiθ, so
I =

CJ
estF(s) ds =
 3π/2
π/2
ets0+tReiθF(s0 + Reiθ)iReiθ dθ,
and hence
|I| ⩽R
 3π/2
π/2
ets0+tReiθF(s0 + Reiθ)
 dθ
= R
 3π/2
π/2
ets0 etR cos θ+itR sin θF(s0 + Reiθ)
 dθ
= R
ets0
 3π/2
π/2
etR cos θ F(s0 + Reiθ)
 dθ
= R
ets0
 π
0
e−tR sin θ′ F

s0 + Rei(θ′+ π
2 ) dθ′,
after making the change of variable θ = θ′ + π/2. Since |F(s0 + Reiθ)| →0 as
R →∞, |F(s0 + Reiθ)| < K(R) for some real-valued, positive function K(R),
such that K(R) →0 as R →∞. In addition, since sin θ ⩾2θ/π, as shown in
Figure A6.2,
|I| ⩽R
ets0 K(R)
 π
0
e−2tRθ′/π dθ′ = π |ets0| K(R)
2t

1 −e−2tR
→0 as R →∞.
A6.5
Linear Ordinary Diﬀerential Equations in the Complex Plane
Linear ordinary diﬀerential equations in the complex variable z = x + iy can be
studied in much the same way as those in a real variable, although their solutions
tend to have a richer structure due to the appearence of branch cuts, singularities
and other features associated with the complex plane. In particular, the solution
of linear second order equations by power series works in the same way as the real
variable case, with the proviso that there is a circle of convergence rather than an
interval of convergence. In this section, we will prove that power series solutions of
w′′ + q(z)w′ + r(z)w = 0,
(A6.4)
with zq(z) and z2r(z) analytic at z = 0, are convergent. Note that these are pre-
cisely the conditions for Theorem 1.3, the convergence theorem for a real diﬀerential
equation at a regular singular point.
If we assume a series solution,
w =
∞

n=0
anzn+c,

A6.5 ORDINARY DIFFERENTIAL EQUATIONS IN THE COMPLEX PLANE
525
and expansions
zq(z) = q0 + q1z + · · · ,
z2r(z) = r0 + r1z + · · · ,
then the indicial equation is
a0 {c(c −1) + cq0 + r0} = 0,
and, in general,
anf(c + n) +
n−1

s=0
as {(c + s)qn−s + rn−s} = 0,
(A6.5)
where f(c) = c(c−1)+cq0 +r0. The indicial equation always has one root that will
produce a well-deﬁned series solution. We will proceed assuming that we are not
dealing with a diﬃcult case in which f(c + n) vanishes, when special care would be
needed.
We can rewrite (A6.5) in the form
ann (n + c1 −c2) = −
n−1

s=0
as {(c1 + s) qn−s + rn−s} ,
(A6.6)
where c1 and c2 are the solutions of the indicial equation, and we have used the
fact that c1 + c2 = 1 −q0. Since zq(z) and z2r(z) are analytic at z = 0, they each
have Taylor expansions that converge in some disc centred on z = 0. Let R be the
smaller of these two radii. Then, by Cauchy’s inequality (Theorem A6.7), there
exist r and K = K(r) such that
|qn| ⩽K
rn ,
|rn| ⩽K
rn
for r < R and n = 0, 1, 2, . . . .
If we take the modulus of (A6.6), we can now see that
n |an| |n + c1 −c2| ⩽K
n−1

s=0
|as||c1| + s + 1
rn−s
.
Writing |c1 −c2| = λ and |c1| = µ, we can deﬁne a sequence of coeﬃcients An with
|an| ⩽An using
An = |an|
for 0 ⩽n < λ,
n(n −λ)An = K  n−1
s=0 An (µ + s + 1) /rn−s
for n ⩾λ.
The deﬁnitions of An−1 and An for n ⩾λ show that
n(n −λ)An −(n −1)(n −1 −λ)An−1
r
= K(µ + n)An−1
r
.
If we now divide through by n(n−λ)An−1 and let n →∞, we ﬁnd that An/An−1 →
1/r. This shows that the radius of convergence of  ∞
n=0 Anzn is r. But, from our
deﬁnition, |an| ⩽An, so that the radius of convergence of  ∞
n=0 anzn is at least r
for all r < R. Hence  ∞
n=0 anzn converges for |z| < R as we claimed.

APPENDIX 7
A Short Introduction to MATLAB
MATLAB† is a programming language and environment that is both powerful and
easy to use. It contains built-in functions that perform most of the tasks that we
need in order to illustrate the material in this book, for example, the numerical inte-
gration of ordinary diﬀerential equations‡, the graphical display of data, and much
more. In particular, MATLAB was originally developed to provide an easy way to
access programs that act on matrices, and contains extremely eﬃcient routines, for
example, to solve systems of linear equations and to extract the eigenvalues of a
matrix§.
In this appendix, our aim is to show complete newcomers to MATLAB enough
for them to be able understand the material in the book that uses MATLAB. We
introduce several other MATLAB functions in the main part of the text. For a
more extensive guide to the power of MATLAB, see Higham and Higham (2000).
A7.1
Getting Started
MATLAB can be used in two ways.
As we shall see, we can save functions,
which have arguments and return values, and scripts, which are just sequences of
MATLAB commands, as ﬁles, and call them from MATLAB. Alternatively, we can
simply type commands into MATLAB at the command prompt (>>), and obtain
results immediately. For example, MATLAB has all the functionality of a scientiﬁc
calculator.
>> (4+5-6)*14/5, exp(0.4), gamma(0.5)/sqrt(pi)
ans = 8.4000
ans = 1.4918
ans = 1.0000
Note that pi is a built-in approximation to π and gamma is the gamma function,
Γ(x). The three expressions separated by commas are evaluated successively, and
the calculated answers displayed. The default is to display ﬁve signiﬁcant digits of
the answer. This can be changed using the format command. For example,
† MATrix LABoratory
‡ We will often use the built-in subroutine ode45 to solve ordinary diﬀerential equations. An
explanation of this is given in Section 9.3.4.
§ See Trefethen and Bau (1997) for an introduction to numerical linear algebra that uses MAT-
LAB extensively to illustrate the material.

A7.2 VARIABLES, VECTORS AND MATRICES
527
>> format long
>> (4+5-6)*14/5, exp(0.4), gamma(0.5)/sqrt(pi)
ans = 8.40000000000000
ans = 1.49182469764127
ans = 1.00000000000000
MATLAB has comprehensive online documentation, which can be accessed with
the command help. Typing help format at the command prompt lists the many
other formats that are available.
It should be noted that MATLAB, like any programming language, can only
perform calculations with a ﬁnite precision, since numbers can only be represented
digitally with a ﬁnite precision. In particular, each calculation is subject to an error
of the order of eps, the machine precision.
>> eps,sin(pi)
ans = 2.2204e-016
ans = 1.2246e-016
This tells us that the machine precision is about 10−16†, which corresponds to
double precision arithmetic. A calculation of sin π = 0 leads to an error comparable
in magnitude to eps.
A7.2
Variables, Vectors and Matrices
As well as being able to perform calculations directly, MATLAB allows the use of
variables. Most programming languages require the user to declare the type of
each variable, for example a double precision scalar or a single precision complex
array. This is not necessary in MATLAB, as storage is allocated as it is needed.
For example,
>> A = 1, B = 1:3, C = 0:0.1:0.45, D = linspace(1,2,5)’...
E = [1 2 3; 4 5 6], F = [1+i; 2+2*i]
A = 1
B = 1
2
3
C = 0
0.1000
0.2000
0.3000
0.4000
D = 1.0000
1.2500
1.5000
1.7500
2.0000
E = 1
2
3
4
5
6
F = 1.0000 + 1.0000i
2.0000 + 2.0000i
† The precise value of eps varies depending upon the system on which MATLAB is installed.

528
A SHORT INTRODUCTION TO MATLAB
Note that ... allows the current command to overﬂow onto a new line. We also
need to remember that MATLAB variables are case-sensitive. For example, X
and x denote distinct variables.
— A is a 1 × 1 matrix with the single, real entry, 1. Of course, we can just treat this
as a scalar.
— B is a 1×3 matrix (a row vector). The colon notation, a:b, just denotes a vector
with entries running from a to b at unit intervals.
— C is another row vector. In the notation a:c:b, c denotes the spacing as the
vector runs from a to b.
— D is a 5 × 1 matrix (a column vector). The function linspace(a,b,n) generates
a row vector running from a to b with n evenly spaced points. Note that A’ is
the transpose of A when A is real, and its adjoint if A is complex.
— E is a 2 × 3 matrix. The semicolon is used to denote a new row.
— F is a complex column vector. Note that i is the square root of −1.
MATLAB has a rich variety of functions that operate upon matrices (type help
elmat), which we do not really need for the purposes of this book. However, as an
example, matrix multiplication takes the obvious form,
>> E’*F
ans = 9.0000 +
9.0000i
12.0000 + 12.0000i
15.0000 + 15.0000i
>> E*F
??? Error using ==> *
Inner matrix dimensions must agree.
Remember, an m1 × n1 matrix can only be multiplied by an m2 × n2 matrix if
n1 = m2.
It is also possible to operate on matrices element by element, for example
>> B.*C
ans = 0
0.2000
0.6000
1.2000
2.0000
This takes each element of B and multiplies it by the corresponding element of C.
The matrices B and C must be of the same size. The use of a full stop in, as another
example, B./C, always denotes element by element calculation. Matrix addition
and subtraction take the obvious form. For example,
>> C + D’
ans = 1.0000
1.3500
1.7000
2.0500
2.4000
>> C - D
??? Error using ==> -
Matrix dimensions must agree.
The one exception to the rule that matrices must have the same dimensions comes
when we want to add a scalar to each element of a matrix. For example,

A7.3 USER-DEFINED FUNCTIONS
529
>> C + 1
ans = 1.0000
1.1000
1.2000
1.3000
1.4000
This adds 1 to each element of C, even though 1 is a scalar.
It is often useful to be able to extract rows or columns of a matrix. For example,
>> E(:,3), E(2,:)
ans = 3
6
ans = 4
5
6
extracts the third column and second row of E.
Most of MATLAB’s built-in functions can also take matrix arguments.
For
example
>> sin(C)
ans = 0
0.0998
0.1987
0.2955
0.3894
>> E.^2
ans = 1
4
9
16
25
36
A7.3
User-Deﬁned Functions
We can add to MATLAB’s set of built-in functions by deﬁning our own functions.
For example, if we save the commands




function a = x2(x)
a = x.^2;
in a ﬁle named x2.m, we will be able to access this function, which simply squares
each element of x, from the command prompt. Note that the semicolon suppresses
the output of a. For example,
>> x2(0:-0.1:-0.5)
ans = 0
0.0100
0.0400
0.0900
0.1600
0.2500
It is always prudent to write functions in such a way that they can take a matrix
argument.
For example, if we had written x2 using a = x^2, MATLAB would
return an error unless x were a square matrix, when the result would be the matrix
x*x.
A7.4
Graphics
One of the most useful features of MATLAB is its wide range of diﬀerent methods
of displaying data graphically (type help graphics). The most useful of these is
the plot command. The basic idea is that plot(x,y) plots the data contained in
the vector y against that contained in the vector x, which must, of course have the
same dimensions. For example,

530
A SHORT INTRODUCTION TO MATLAB
>> x = linspace(1,10,500); y = log(gamma(x)); plot(x,y)
produces a plot of the logarithm of the gamma function with 1 ⩽x ⩽10. We can
add axis labels and titles with
>> xlabel(’x’), ylabel(’log \Gamma(x)’)
>> title(’The logarithm of the gamma function’)
This produces the plot shown in Figure A7.1. Note that MATLAB automatically
picks an appropriate range for the y axis. We can override this using, for exam-
ple, YLim([a b]), to reset the limits of the y axis. There are many other ways of
controlling the axes (type help axis).
Fig. A7.1. A plot of log Γ(x), produced by MATLAB.
The command plot can produce graphs of more than one function. For example,
plot(x1,y1,x2,y2,x3,y3) produces three lines in the obvious way. By default,
the lines are produced in diﬀerent colours to distinguish between them, and the
command legend(’label 1’, ’label 2’, ’label 3’) adds a legend that names
each line. The style of each line can be controlled, and individual points plotted if
necessary. For example, plot(x1,y1,’--’,x2,y2,’x’,x3,y3,’o-’) plots the ﬁrst
data set as a dashed line, the second as discrete crosses, and the third as a solid
line with circles at the discrete data points. For a comprehensive list of options,
type help plot.
As we shall see in the main part of the book, there is also an easy way of
plotting functions to get an idea of what they look like. For example, ezplot(@sin)

A7.5 PROGRAMMING IN MATLAB
531
produces a plot of sin x. We will also use the command ezmesh, which produces
a mesh plot of a function of two variables (see Section 2.6.1 for an explanation).
The quantity @sin is called a function handle. These allow functions to take
other functions as arguments, by passing the function handle as a parameter. This
includes functions supplied by the user, so that, for example, ezplot(@x2) plots
the function x2 that we deﬁned earlier.
A7.5
Programming in MATLAB
All of the control structures that you would expect a programming language to
have are available in MATLAB. The most commonly used of these are FOR loops
and the IF, ELSEIF structure.
FOR Loops
There are many examples of FOR loops in the main text. The basic syntax is
for c = x
statements
end
The statements are executed with c taking as its value the successive columns of
x. For example, if we save the script
#
"
 
!
x = linspace(0,2*pi,500);
for k = 1:4
plot(x,sin(k*x))
pause
end
to a ﬁle sinkx.m, and then type sinkx at the command prompt, the commands
in sinkx.m are executed. This is a MATLAB script. Note that the indentation of
the lines is not necessary, but makes the script more readable. The editor supplied
with MATLAB formats scripts in this way automatically. The command pause
waits for the user to hit any key before continuing, so this script successively plots
sin x, sin 2x, sin 3x and sin 4x for 0 ⩽x ⩽2π.
As a general rule, if a script needs to execute quickly, FOR loops should be
avoided where possible, in favour of matrix and vector operations. For example,
>> tic, A = (1:50000).^2; toc
elapsed_time = 0.0100
>> tic, for k = 1:50000, A(k) = k^2; end, toc
elapsed_time = 0.4010
The command tic sets an internal clock to zero, and the variable toc contains
the time elapsed, in seconds, since the last tic. We can see that allocating k2 to
A(k), the kth entry of A, using element by element squaring of 1:50000, known as
vectorizing the function, executes about forty times faster than performing the

532
A SHORT INTRODUCTION TO MATLAB
same task using a FOR loop. For an in-depth discussion of vectorization, see Van
Loan (1997).
Another way of speeding up a MATLAB script is to preallocate the array. In
the above sequence, the array A already exists, and is of the correct size when the
FOR loop executes. Consider
>>A = [];
>>tic, for k = 1:50000, A(k) = k^2; end,toc
elapsed_time = 94.1160
If we set A to be the empty array, and then allocate k2 to A(k), expecting MATLAB
to successively increase the size of A at each iteration of the loop, we can see that
the execution time becomes huge compared with that when A is already known to
be a vector of length 50000.
The IF, ELSEIF Structure
The basic syntax for an IF, ELSEIF statement is
if expression
statements
elseif expression
statements
elseif expression
statements
.
.
else
statements
end
For example, consider the function
'
&
$
%
function f = f1(x)
f = zeros(size(x));
for k = 1:length(x)
if (x(k)<0)|(x(k)>3)
disp(’x out of range’)
elseif x(k)<1
f(k) = x(k)^2;
elseif x(k)<2
f(k) = cos(x(k));
else
f(k) = exp(-x(k));
end
end

A7.5 PROGRAMMING IN MATLAB
533
This evaluates the function (A2.1), which is plotted in Figure A2.2, at the points
in the vector x, and displays a warning if any of the elements of x lie outside the
range [0, 3], where the function is deﬁned. The logical function or is denoted by
| in MATLAB. The function length(x) calculates the length of a vector x. Note
that we initialize f as a vector of zeros with the same dimensions as x using f =
zeros(size(x)). As we have already discussed, this preallocation of the matrix
signiﬁcantly speeds up the execution of the function. In fact, although this is a
transparent way of programming the function, it is still not as eﬃcient as it could
be. Consider the function
'
&
$
%
function f = f2(x)
if ((x<0)|(x>3)) == zeros(size(x))
f = ((x>=0)&(x<1)).*x.^2 + ((x>=1)&(x<2)).*cos(x)...
+ ((x>=2)&(x<=3)).*exp(-x);
else
disp(’x out of range’)
end
Note that ==, not =, is used to test the equality of two matrices. The vector (x>=0)
is the same size as x, and contains ones where the corresponding element of x is
positive or zero, and zeros elsewhere. The operator & acts in the obvious way as
the logical function and. The function f2 evaluates f(x) in a vectorized manner,
avoiding the use of a FOR loop. Now consider
>> tic, y = f1(x); toc
elapsed_time = 1.4320
>> tic, y = f2(x); toc
elapsed_time = 0.1300
It is clear that the vectorized function f2 evaluates (A2.1) far more eﬃciently than
f1, which uses a FOR loop and an IF, ELSEIF statement.

Bibliography
Ablowitz, M.J. and Fokas, A.S., 1997, Complex Variables, Cambridge University Press.
Acheson, D.J., 1990, Elementary Fluid Dynamics, Oxford University Press.
Aref, H., 1984, ‘Stirring by chaotic advection’, J. Fluid Mech., 143, 1–21.
Arrowsmith, D.K. and Place, C.M., 1990, An Introduction to Dynamical Systems,
Cambridge University Press.
Bakhvalov, N. and Panasenko, G., 1989, Homogenization: Averaging Processes in
Periodic Media, Kluwer.
Billingham, J., 2000, ‘Steady state solutions for strongly exothermic thermal ignition in
symmetric geometries’, IMA J. of Appl. Math., 65, 283–313.
Billingham, J. and King, A.C., 1995, ‘The interaction of a ﬂuid/ﬂuid interface with a ﬂat
plate’, J. Fluid Mech., 296, 325–351.
Billingham, J. and King, A.C., 2001, Wave Motion, Cambridge University Press.
Billingham, J. and Needham, D.J., 1991, ‘A note on the properties of a family of
travelling wave solutions arising in cubic autocatalysis’, Dynamics and Stability of
Systems, 6, 1, 33–49.
Bluman, G.W. and Cole, J.D., 1974, Similarity Methods for Diﬀerential Equations,
Springer-Verlag.
Bourland, F.J. and Haberman, R., 1988, ‘The modulated phase shift for strongly
nonlinear, slowly varying, and weakly damped oscillators’, SIAM J. Appl. Math.,
48, 737–748.
Brauer, F. and Noble, J.A., 1969, Qualitative Theory of Ordinary Diﬀerential Equations,
Benjamin.
Buckmaster, J.D. and Ludford, G.S.S., 1982, Theory of Laminar Flames, Cambridge
University Press.
Byrd, P.F., 1971, Handbook of Elliptic Integrals for Engineers and Scientists,
Springer-Verlag.
Carrier, G.F. and Pearson, C.E., 1988, Partial Diﬀerential Equations: Theory and
Technique, Academic Press.
Coddington, E.A. and Levinson, N., 1955, Theory of Ordinary Diﬀerential Equations,
McGraw-Hill.
Courant, R. and Hilbert, D., 1937, Methods of Mathematical Physics, Interscience.
Dunlop, J. and Smith, D.G., 1977, Telecomunications Engineering, Chapman and Hall.
Flowers, B.H. and Mendoza, E., 1970, Properties of Matter, John Wiley.
Garabedian, P.R., 1964, Partial Diﬀerential Equations, John Wiley.
Gel’fand, I.M., 1963, ‘Some problems in the theory of quasilinear equations’, Amer.
Math. Soc. Translations Series 2, 29, 295–381.
Glendinning, P., 1994, Stability, Instability and Chaos: An Introduction to the Theory of
Nonlinear Diﬀerential Equations, Cambridge University Press.
Grindrod, P., 1991, Patterns and Waves: The Theory and Applications of
Reaction-Diﬀusion Equations, Oxford University Press.
Grobman, D.M., 1959, ‘Homeomorphisms of systems of diﬀerential equations’, Dokl.
Akad. Nauk SSSR, 128, 880.
Guckenheimer, J. and Holmes, P.J., 1983, Nonlinear Oscillations, Dynamical Systems

BIBLIOGRAPHY
535
and Bifurcations of Vector Fields, Springer-Verlag.
Hartman, P., 1960, ‘A lemma in the theory of structural stability of diﬀerential
equations’, Proc. Amer. Math. Soc., 11, 610–620.
Higham, D.J. and Higham, N.J., 2000, MATLAB Guide, SIAM.
Hydon, P.E., 2000, Symmetry Methods for Diﬀerential Equations, Cambridge University
Press.
Ince, E.L., 1956, Ordinary Diﬀerential Equations, Dover.
Kevorkian, J., 1990, Partial Diﬀerential Equations: Analytical Solution Techniques,
Wadsworth and Brooks.
King, A.C., 1988, ‘Periodic approximations to an elliptic function’, Applicable Analysis,
27, 271–278.
King, A.C. and Needham, D.J., 1994, ‘The eﬀects of variable diﬀusivity on the
development of travelling waves in a class of reaction-diﬀusion equations’, Phil.
Trans. R. Soc. Lond. A, 348, 229–260.
K¨orner, T.W., 1988, Fourier Analysis, Cambridge University Press.
Kreider, D.L., Kuller, R.G., Ostberg, D.R. and Perkins, F.W., 1966, An Introduction to
Linear Analysis, Addison-Wesley.
Kuzmak, G.E., 1959, ‘Asymptotic solutions of nonlinear second order diﬀerential
equations with variable coeﬃcients’, J. Appl. Math. Mech., 23, 730–744.
Landau, L.D. and Lifschitz, E.M., 1959, Theory of Elasticity, Pergamon.
Lighthill, M.J., 1958, Introduction to Fourier Analysis and Generalised Functions,
Cambridge University Press.
Lorenz, E.N., 1963 ‘Deterministic non-periodic ﬂows’, J. Atmos. Sci., 20, 130–141.
Lunn, M., 1990, A First Course in Mechanics, Oxford University Press.
Marlin, T.E., 1995, Process Control: Designing Processes and Control Systems for
Dynamic Performance, McGraw-Hill.
Mathieu, J. and Scott, J., 2000, An Introduction to Turbulent Flow, Cambridge
University Press.
Milne-Thompson, L.M., 1952, Theoretical Aerodynamics, Macmillan.
Milne-Thompson, L.M., 1960, Theoretical Hydrodynamics, Macmillan.
Morris, A.O., 1982, Linear Algebra: An Introduction, Van Nostrand Reinhold.
Ottino, J.M., 1989, The Kinematics of Mixing: Stretching, Chaos and Transport,
Cambridge University Press.
Otto, S.R., Yannacopoulos, A. and Blake, J.R., 2001, ‘Transport and mixing in Stokes
ﬂow: the eﬀect of chaotic dynamics on the blinking stokeslet’, J. Fluid Mech., 430,
1–26.
Pedley, T.J., 1980, The Fluid Mechanics of Large Blood Vessels, Cambridge University
Press.
Petrov, V., Scott, S.K. and Showalter, K., 1992, ‘Mixed-mode oscillations in chemical
systems’, J. Chem. Phys., 97, 6191–6198.
Schiﬀ, L.I., 1968, Quantum Mechanics, McGraw-Hill.
Sobey, I., 2000, An Introduction to Interactive Boundary Layer Theory, Oxford
University Press.
Sparrow, C.T., 1982, The Lorentz Equations: Bifurcations, Chaos and Strange
Attractors, Springer-Verlag.
Trefethen, L.N. and Bau, D., III, 1997, Numerical Linear Algebra, SIAM.
Van Dyke, M., 1964, Perturbation Methods in Fluid Mechanics, Academic Press.
Van Loan, C.F., 1997, Introduction to Scientiﬁc Computing, Prentice Hall.
Watson, G.N., 1922, A Treatise on the Theory of Bessel Functions, Cambridge
University Press.
Wiggins, S., 1988, Global Bifurcations and Chaos, Springer-Verlag.
Wiggins, S., 1990, Introduction to Applied Nonlinear Dynamical Systems and Chaos,
Springer-Verlag.

Index
Abel’s formula, 10, 34, 77, 99
aerofoils, 195, 274
Airy’s equation, 79, 214, 350
analyticity, 517
Arrhenius law, 319
asymptotic balance, 305
autocatalysis, 374, 409
bang-bang control, 422, 437
basin of attraction, 223
Bendixson’s negative criterion, 240
Dulac’s extension, 240, 253E
Bernoulli shift map, 467
Bernoulli’s equation, 190
Bessel functions, 42, 58, 293
Fourier–Bessel series, 71, 74
generating function, 64
modiﬁed Bessel functions of ﬁrst and
second kind, 71
orthogonality, 71
recurrence relations, 69
Weber’s, 62
Bessel’s equation, 28, 58, 108
ν = 1, 10
ν = 1
2 , 12, 28E
inhomogeneity, 77
Lommel’s functions, 79
Bessel’s inequality, 114
Bessel–Lommel theorem, 75
bifurcation
codimension two, 397
diagram, 390
ﬂip, 454
global, 408
homoclinic, 408
Hopf, 402
pitchfork, 399, 402
saddle–node, 390, 392, 402, 457
subcritical Hopf, 402
subcritical pitchfork, 399
supercritical Hopf, 402
supercritical pitchfork, 399
tangent, 457
transcritical, 398, 402
bifurcation theory, 388
boundary value problem, 46, 54E, 93, 121E,
183, 370, 376
inhomogeneous, 96
bounded controls, 419
Cantor
diagonalization proof, 464
cantor
middle-third set, 464
Cauchy problem, 180
Cauchy’s integral formula, 519
Cauchy’s theorem, 287, 298, 518
Cauchy–Kowalewski theorem, 180
Cauchy–Riemann equations, 186
Cauchy–Schwartz inequality, 497
Cayley–Hamilton theorem, 425, 434, 501
centre manifold
local, 373, 379, 462
centre manifold theorem, 372
chain rule, 511
chaotic solutions, 449
islands, 460
characteristic equation, 499
characteristic variables, 358
chemical oscillator, 450
comparison theorems, 212
complete elliptic integral
ﬁrst kind, 334
second kind, 336
completely controllable, 420, 444E
completeness, 42, 114, 498
complex analysis
branch cut, 165
branch point, 164
Cauchy–Riemann equations, 186
keyhole contour, 165
poles and residues, 163, 521
composite expansions, 312
conformal mapping, 191
Joukowski transformation, 195
connection problems, 348
conservation law, 271
conservative systems, 218
continuity, 502
piecewise, 502
uniform, 502
contour integral, 518
control
bang-bang, 422
variables, 418
vector, 418
536

INDEX
537
control problem
optimal, 418
time-optimal, 418
controllability matrix, 433
controllable set, 420
convolution, 130, 160, 352
CSTR (continuous ﬂow, stirred tank reactor),
393, 400, 415
cubic autocatalysis, 374
cubic crosscatalator, 450, 489, 492E
D’Alembert’s solution, 180, 357
diﬀeomorphism, 379, 461, 475
smooth, 379
diﬀerentiable, 502
diﬀusion equation
heat, 45, 123
point source solution, 271
diﬀusion problem, 352
dimensionless
groups, 274
variables, 123
Dirac delta function, 135, 271
Dirichlet kernel, 128
Dirichlet problem, 55, 143, 183, 193
domain of attraction, 223
eigenfunctions, 95
expansions, 104
eigensolutions, 369E
eigenvalues, 95, 103, 372, 499
eigenvectors, 103, 499
elastic membrane, 80
electrostatics, 39, 148
equation
Airy, 79, 122E, 214, 280, 350
Bernoulli, 190
Cauchy–Riemann, 186, 290, 517
characteristic (matrices), 425
diﬀusion, 123, 175, 184, 270
Duﬃng’s, 448
Euler, 413E
forced Duﬃng, 448, 482, 486, 492E
forced wave, 151
Fourier, 124
Helmholtz, 351
Laplace, 143, 147, 175, 179, 182, 186, 193
logistic, 468
Lorenz, 451, 470, 493E
modiﬁed Helmholtz, 151, 352
Navier–Stokes, 170
porous medium, 273
reaction–diﬀusion, 185, 370, 375
Ricatti’s, 269
Schr¨odinger’s, 119
Tricomi’s, 177
Volterra integral, 162
wave, 148, 175, 288, 357
equilibrium point
centre, 230
hyperbolic, 224, 475
nonhyperbolic, 224, 384
nonlinear centre, 401
saddle, 227, 294, 378
stable, 220
stable node, 227
stable spiral, focus, 229
unstable, 220
unstable node, 226
unstable spiral, focus, 228
equilibrium solutions, 220
Euclidean norm, 485, 487, 497
exchange of stabilities, 398
existence, 418
local, 204
exponential integral, 276
Floquet theory, 335
ﬂow evolution operator, 467
ﬂuid dynamics, 48
Bernoulli’s equation, 51, 186
boundary conditions, 49
boundary layer, 274
circle theorem, 189
complex potential, 186
ﬂow past a ﬂat plate, 194
ﬂow past an aerofoil, 195
inviscid, irrotational, 186
Kutta condition, 195
Navier-Stokes equations, 170
Reynolds number, 274, 371, 456
stream function, 186
triple deck, 370
turbulence, 456
velocity potential, 49
viscosity, 167
Fourier integral, 133
Fourier series, 68, 125, 496, 509
Bessel functions, 71, 74
cosine, 127
generalised coeﬃcients, 113
Gibbs’ phenomenon, 75, 128
Legendre polynomials, 43
sine, 127
uniform convergence, 132
Fourier theorem, 131
Fourier transform, 133, 280, 289, 352
convolution integral, 140
higher dimensions, 145
inverse, 138
inversion formula, 133
linearity, 138
shifting, 150
Fourier’s equation, 108, 124
Fourier’s law, 44
Fourier–Bessel series, 71, 74, 90E, 126
Fourier–Legendre series, 126
Fredholm alternative, 96
frequency modulation, 87
Friedrichs boundary conditions, 109
functions
φ(n), 23
Airy, 79, 296
Bessel, 280

538
INDEX
functions (cont.)
Bessel (large argument), 346
Bessel of order zero, 293
complementary error, 167, 270, 357
cost (control), 418
Dirac delta, 135
error, 167
gamma, 58, 153, 281
gauge, 275
good, 134
Heaviside, 135
Jacobian elliptic, 251, 314, 331, 333, 368E
Kronecker delta, 74
Laguerre polynomials, 122
Mel’nikov, 479
modiﬁed Bessel, 71, 281, 300E, 302E
negative deﬁnite, 381
of exponential order, 154
parabolic cylinder, 302
positive deﬁnite, 381
sequences of, 509
sign function, 135
unit function, 135
gamma function, 153, 294
gauge functions, 275
generalized function, 134
Dirac delta, 135
Heaviside, 135
multi-dimensional delta function, 145
sign function, 135
unit function, 135
generating functions
Bessel functions, 64
Legendre polynomials, 35
Gibbs’ phenomenon, 75, 128
good functions, 134
Green’s formula, 111
Green’s function, 98, 121E, 141
free space, 142
Gronwall’s inequality, 210, 212, 216E
group
extended, 262
inﬁnitesimal generators, 259
isomorphic, 258
magniﬁcation, 260
one-parameter, 257, 266
PDEs, 270
rotation, 261
two-parameter, 270
group invariants, 256, 261
group theoretic methods, 322, 512
Hamiltonian system, 247, 477
Hartman-Grobman theorem, 230
Heaviside function, 135, 156
Hermite’s equation, 101, 112
Hermitian, 103
heteroclinic
path, 221
solutions, 221
homeomorphism, 379
homoclinic
path, 221
homoclinic point
transverse, 474
homoclinic tangles, 447, 472
Hopf bifurcation theorem, 403, 415E
hypergeometric equation, 29
hysteresis, 397
initial value problem, 3, 181, 288, 357
inner product, 102, 497
complex functions, 111
integral equations, 172E
Volterra, 162
integral path, 219, 225, 373
integrating factors, 512
intermittency, 456
Inverse
Fourier transform, 138
Jacobian, 234, 248, 250, 322, 377, 380, 480,
508
Jordan curve theorem, 243
Jordan’s lemma, 163, 287, 523
Kronecker delta, 113
Kuzmak’s method, 332
Lagrange’s identity, 101
Laguerre polynomials, 122
Laplace transform, 152, 352
Bromwich inversion integral, 163
convolution, 160
ﬁrst shifting theorem, 155, 164
inverse, 155, 280
inversion, 162
linearity, 154
ODEs, 158
of a derivative, 157
second shifting theorem, 156
Laplace’s equation, 45, 54E, 143, 193
Laplace’s method, 281, 300E
Laurent series, 521
leading order solutions, 303
Legendre equation
n = 1, 6
Legendre polynomials, 31, 61, 106
P0(x), P1(x), P2(x), P3(x) and P4(x), 33
associated Legendre polynomials, 52
Fourier-Legendre series, 43, 54E
generating function, 35, 54E
Laplace’s representation, 40
orthogonality, 41
recurrence relations, 38
Rodrigues’ formula, 39
Schl¨aﬂi’s representation, 40
special values, 37
Legendre’s equation, 31, 108
associated, 52, 120
general solution, 32

INDEX
539
generalized, 121
order, 31
Lerch’s theorem, 154
Lie group, 257
Lie series, 259
limit cycles, 221
limit point
ω, 242
linear dependence, 9, 496
linear independence, 9, 496
linear operator
Hermitian, 103
self-adjacency, 103
self-adjoint, 42, 100
linear oscillator
controlling, 428, 439
linear transformations, 498
linearity
Fourier transform, 138
Laplace transform, 154
linearization of nonlinear systems, 221
Liouville’s theorem, 249
Lipschitz condition, 205, 212
lobe, 475
Lommel’s functions, 79
Lyapunov exponents, 484
maximum, 485
spectrum, 487
Lyapunov function, 372, 381
Lyapunov stable, 381
Maclaurin series, 505
manifold, 379
maps, 452
Bernoulli shift, 467
H´enon, 459
horseshoe, 475
logistic, 454
Poincar´e return, 467
shift, 452
Smale horseshoe, 465
tent, 463
matched asymptotic expansions, 310
MATLAB functions
airy, 80
besselj, 63
ceil, 125
contour, 50
eig, 489
ellipj, 314, 318
ellipke, 337
erf, 169
erfc, 169
ezmesh, 48
fzero, 76, 318
gamma, 60
legend, 33
legendre, 33
linspace, 33
lorenz, 451
meshgrid, 50
ode45, 237, 489
plot, 33, 529
polyval, 17
subplot, 63
matrix exponential, 424, 444
mean value theorem, 504
Mel’nikov function, 493E
Mel’nikov theory, 477
method of Frobenius, 11–24, 31, 54E, 60, 507
General Rule I, 15, 27, 32
General Rule II, 19, 61
General Rule III, 23, 62
indicial equation, 13
method of matched asymptotic expansions,
307, 366E
method of multiple scales, 325, 332, 333, 359,
367E
method of reduction of order, 5, 33, 54E, 269
formula, 6
method of stationary phase, 285–290, 301E
method of steepest descents, 290, 350
Method of variation of parameters, 7, 34, 77,
96, 351
formula, 8, 98
modiﬁed Bessel functions, 71
negatively invariant set, 241
Neumann problem, 144, 184
Newton’s
ﬁrst law, 94
second law, 50, 80, 218, 231, 255, 426, 447
nonautonomous, 269, 322, 429, 432
normal form, 390
pitchfork bifurcation, 402, 415E
transcritical bifurcation, 402, 414E
nullclines, 234
ODEs, 511
autonomous, 222
complementary function, 4
coupled systems, 158
existence, 203, 204
integral paths, 225
integrating factors, 512
irregular singular point, 25
nonautonomous, 222
nonsingular, 3, 10
ordinary points, 225
particular integral, 4, 8
regular point, 3
second order equations (constant
coeﬃcients), 513
separable variables, 511
singular point, 3, 24
successive approximations, 204
uniqueness, 203, 210
one-parameter group
horizontal translation, 257
magniﬁcation, 257
rotation, 257
vertical translation, 257

540
INDEX
orthogonality, 98, 103, 111, 497
Bessel functions, 71
Legendre polynomials, 41
orthonormality, 113
particular integral, 512
PDEs
asymptotic methods, 351
canonical form, 175, 177
Cauchy problem, 180
characteristic variables, 83, 177, 358
classiﬁcation, 175
d’Alembert’s solution, 180
elliptic, 175
Fourier transforms, 143
group theoretic methods, 270
hyperbolic, 175
parabolic, 175
separable solution, 124
similarity variable, 270
Peano uniqueness theorem, 216E
period doubling, 454
periodic solutions, 221
perturbation problem
regular, 274, 303
singular, 274, 304
phase plane, 219
phase portrait, 245
plane polar coordinates, 146
planetary motion, 66
Pockhammer symbol, 60
Poincar´e return map, 467, 492E
Poincar´e index, 237, 253E
Poincar´e projection, 245
Poincar´e–Bendixson theorem, 241,
387
population dynamics, 233
positioning problem, 426, 433, 438,
444E
with friction, 427
with two controls, 429, 441
positively invariant set, 241
power series
comparison test, 507
convergence, 506
radius of convergence, 507
ratio test, 506
Pr¨ufer substitution, 115
predator–prey system, 234, 253E,
445E
product rule, 5, 425
quantum mechanics, 118
hydrogen atom, 120
Rayleigh’s problem, 168
reachable set, 420, 421
residue theorem, 521
resonance, 97
Riemann integral, 497
Riemann–Lebesgue lemma, 130
Rodrigues’ formula, 39
saddle point, 227
Schr¨odinger’s equation, 119
secular terms, 327
separatrices, 227, 252E, 323, 378, 408
set
Cantor’s middle-third, 464
closed, 423
completely disconnected, 464
connected, 431
controllable, 429
convex, 422
open, 423
reachable, 421
strictly convex, 423
simple harmonic motion, 217, 233, 331,
425
Smale-Birkhoﬀtheorem, 475
solubility, 97
spherical harmonics, 54
spherical polar coordinates, 39, 53, 146
stability
asymptotic, 381
exchange of, 398
Lyapunov, 381
structurally unstable, 389
stable manifold
global, 380
local, 372, 378, 379, 462
stable node, 227
stable spiral, focus, 229
state variables, 418
state vector, 418
steering problem, 427
Stirling’s formula, 282, 301E
structurally unstable, 389
Sturm comparison theorem, 30
Sturm separation theorem, 11
Sturm–Liouville problems, 42, 107
regular endpoint, 109
singular endpoint, 109
target state, 418
Taylor series, 506
functions of two variables, 507
thermal ignition, 318
tightrope walker, 419, 443E
time-optimal control problem, 418
Time-optimal maximum principle,
436
transformations
identity, 257
inﬁnitesimal, 258
inverse, 257
magniﬁcation, 260
rotation, 261
transversal, 242
travelling wave solution, 375, 409
triangle inequality, 208, 497

INDEX
541
uniqueness, 210, 418
unstable manifold
global, 380
local, 379, 462
unstable node, 227
unstable spiral, focus, 228
van der Pol Oscillator, 329
Van Dyke’s matching principle, 311, 366E
Vector space, 9, 495
basis, 496
Watson’s lemma, 283, 292, 300E
wave equation, 82
d’Alembert’s solution, 83
WKB approximation, 344, 369E
Wronskian, 8, 9, 28E, 54E, 77, 90E,
98


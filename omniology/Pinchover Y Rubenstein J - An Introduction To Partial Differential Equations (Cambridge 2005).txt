
This page intentionally left blank

A N I N T R O D U C T I O N T O P A R T I A L D I F F E R E N T I A L
E Q U A T I O N S
A complete introduction to partial differential equations, this textbook provides a
rigorous yet accessible guide to students in mathematics, physics and engineering.
The presentation is lively and up to date, with particular emphasis on developing
an appreciation of underlying mathematical theory.
Beginning with basic deﬁnitions, properties and derivations of some fundamental
equationsofmathematicalphysicsfrombasicprinciples,thebookstudiesﬁrst-order
equations, the classiﬁcation of second-order equations, and the one-dimensional
wave equation. Two chapters are devoted to the separation of variables, whilst
others concentrate on a wide range of topics including elliptic theory, Green’s
functions, variational and numerical methods.
A rich collection of worked examples and exercises accompany the text, along
with a large number of illustrations and graphs to provide insight into the numerical
examples.
Solutions and hints to selected exercises are included for students whilst extended
solution sets are available to lecturers from solutions@cambridge.org.


AN INTRODUCTION TO PARTIAL
DIFFERENTIAL EQUATIONS
YEHUDA PINCHOVER AND JACOB RUBINSTEIN


Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge , UK
First published in print format
-
----
-
----
-
----
© Cambridge University Press 2005
2005
Information on this title: www.cambridg e.org /9780521848862
This book is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.
-
---
-
---
-
---
Cambridge University Press has no responsibility for the persistence or accuracy of
s for external or third-party internet websites referred to in this book, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
hardback
paperback
paperback
eBook (MyiLibrary)
eBook (MyiLibrary)
hardback

To our parents
The equation of heaven and earth
remains unsolved.
(Yehuda Amichai)


Contents
Preface
page xi
1
Introduction
1
1.1
Preliminaries
1
1.2
Classiﬁcation
3
1.3
Differential operators and the superposition principle
3
1.4
Differential equations as mathematical models
4
1.5
Associated conditions
17
1.6
Simple examples
20
1.7
Exercises
21
2
First-order equations
23
2.1
Introduction
23
2.2
Quasilinear equations
24
2.3
The method of characteristics
25
2.4
Examples of the characteristics method
30
2.5
The existence and uniqueness theorem
36
2.6
The Lagrange method
39
2.7
Conservation laws and shock waves
41
2.8
The eikonal equation
50
2.9
General nonlinear equations
52
2.10
Exercises
58
3
Second-order linear equations in two indenpendent
variables
64
3.1
Introduction
64
3.2
Classiﬁcation
64
3.3
Canonical form of hyperbolic equations
67
3.4
Canonical form of parabolic equations
69
3.5
Canonical form of elliptic equations
70
3.6
Exercises
73
vii

viii
Contents
4
The one-dimensional wave equation
76
4.1
Introduction
76
4.2
Canonical form and general solution
76
4.3
The Cauchy problem and d’Alembert’s formula
78
4.4
Domain of dependence and region of inﬂuence
82
4.5
The Cauchy problem for the nonhomogeneous wave equation
87
4.6
Exercises
93
5
The method of separation of variables
98
5.1
Introduction
98
5.2
Heat equation: homogeneous boundary condition
99
5.3
Separation of variables for the wave equation
109
5.4
Separation of variables for nonhomogeneous equations
114
5.5
The energy method and uniqueness
116
5.6
Further applications of the heat equation
119
5.7
Exercises
124
6
Sturm–Liouville problems and eigenfunction expansions
130
6.1
Introduction
130
6.2
The Sturm–Liouville problem
133
6.3
Inner product spaces and orthonormal systems
136
6.4
The basic properties of Sturm–Liouville eigenfunctions
and eigenvalues
141
6.5
Nonhomogeneous equations
159
6.6
Nonhomogeneous boundary conditions
164
6.7
Exercises
168
7
Elliptic equations
173
7.1
Introduction
173
7.2
Basic properties of elliptic problems
173
7.3
The maximum principle
178
7.4
Applications of the maximum principle
181
7.5
Green’s identities
182
7.6
The maximum principle for the heat equation
184
7.7
Separation of variables for elliptic problems
187
7.8
Poisson’s formula
201
7.9
Exercises
204
8
Green’s functions and integral representations
208
8.1
Introduction
208
8.2
Green’s function for Dirichlet problem in the plane
209
8.3
Neumann’s function in the plane
219
8.4
The heat kernel
221
8.5
Exercises
223

Contents
ix
9
Equations in high dimensions
226
9.1
Introduction
226
9.2
First-order equations
226
9.3
Classiﬁcation of second-order equations
228
9.4
The wave equation in R2 and R3
234
9.5
The eigenvalue problem for the Laplace equation
242
9.6
Separation of variables for the heat equation
258
9.7
Separation of variables for the wave equation
259
9.8
Separation of variables for the Laplace equation
261
9.9
Schr¨odinger equation for the hydrogen atom
263
9.10
Musical instruments
266
9.11
Green’s functions in higher dimensions
269
9.12
Heat kernel in higher dimensions
275
9.13
Exercises
279
10 Variational methods
282
10.1
Calculus of variations
282
10.2
Function spaces and weak formulation
296
10.3
Exercises
306
11 Numerical methods
309
11.1
Introduction
309
11.2
Finite differences
311
11.3
The heat equation: explicit and implicit schemes, stability,
consistency and convergence
312
11.4
Laplace equation
318
11.5
The wave equation
322
11.6
Numerical solutions of large linear algebraic systems
324
11.7
The ﬁnite elements method
329
11.8
Exercises
334
12 Solutions of odd-numbered problems
337
A.1
Trigonometric formulas
361
A.2
Integration formulas
362
A.3
Elementary ODEs
362
A.4
Differential operators in polar coordinates
363
A.5
Differential operators in spherical coordinates
363
References
364
Index
366


Preface
This book presents an introduction to the theory and applications of partial dif-
ferential equations (PDEs). The book is suitable for all types of basic courses on
PDEs, including courses for undergraduate engineering, sciences and mathematics
students, and for ﬁrst-year graduate courses as well.
Having taught courses on PDEs for many years to varied groups of students from
engineering, science and mathematics departments, we felt the need for a textbook
that is concise, clear, motivated by real examples and mathematically rigorous. We
therefore wrote a book that covers the foundations of the theory of PDEs. This
theory has been developed over the last 250 years to solve the most fundamental
problems in engineering, physics and other sciences. Therefore we think that one
should not treat PDEs as an abstract mathematical discipline; rather it is a ﬁeld that
is closely related to real-world problems. For this reason we strongly emphasize
throughout the book the relevance of every bit of theory and every practical tool
to some speciﬁc application. At the same time, we think that the modern engineer
or scientist should understand the basics of PDE theory when attempting to solve
speciﬁc problems that arise in applications. Therefore we took great care to create
a balanced exposition of the theoretical and applied facets of PDEs.
The book is ﬂexible enough to serve as a textbook or a self-study book for a large
class of readers. The ﬁrst seven chapters include the core of a typical one-semester
course. In fact, they also include advanced material that can be used in a graduate
course. Chapters 9 and 11 include additional material that together with the ﬁrst
seven chapters ﬁts into a typical curriculum of a two-semester course. In addition,
Chapters 8 and 10 contain advanced material on Green’s functions and the calculus
of variations. The book covers all the classical subjects, such as the separation of
variables technique and Fourier’s method (Chapters 5, 6, 7, and 9), the method of
characteristics (Chapters 2 and 9), and Green’s function methods (Chapter 8). At
the same time we introduce the basic theorems that guarantee that the problem at
xi

xii
Preface
hand is well deﬁned (Chapters 2–10), and we took care to include modern ideas
such as variational methods (Chapter 10) and numerical methods (Chapter 11).
The ﬁrst eight chapters mainly discuss PDEs in two independent variables.
Chapter 9 shows how the methods of the ﬁrst eight chapters are extended and
enhanced to handle PDEs in higher dimensions. Generalized and weak solutions
are presented in many parts of the book.
Throughout the book we illustrate the mathematical ideas and techniques by
applying them to a large variety of practical problems, including heat conduction,
wave propagation, acoustics, optics, solid and ﬂuid mechanics, quantum mechanics,
communication, image processing, musical instruments, and trafﬁc ﬂow.
We believe that the best way to grasp a new theory is by considering examples
and solving problems. Therefore the book contains hundreds of examples and
problems, most of them at least partially solved. Extended solutions to the problems
are available for course instructors using the book from solutions@cambridge.org.
We also include dozens of drawing and graphs to explain the text better and to
demonstrate visually some of the special features of certain solutions.
It is assumed that the reader is familiar with the calculus of functions in several
variables, with linear algebra and with the basics of ordinary differential equations.
The book is almost entirely self-contained, and in the very few places where we
cannot go into details, a reference is provided.
The book is the culmination of a slow evolutionary process. We wrote it during
several years, and kept changing and adding material in light of our experience in
the classroom. The current text is an expanded version of a book in Hebrew that the
authors published in 2001, which has been used successfully at Israeli universities
and colleges since then.
Our cumulative expertise of over 30 years of teaching PDEs at several univer-
sities, including Stanford University, UCLA, Indiana University and the Technion
– Israel Institute of Technology guided to us to create a text that enhances not just
technical competence but also deep understanding of PDEs. We are grateful to our
many students at these universities with whom we had the pleasure of studying this
fascinating subject. We hope that the readers will also learn to enjoy it.
We gratefully acknowledge the help we received from a number of individuals.
Kristian Jenssen from North Carolina State University, Lydia Peres and Tiferet
Saadon from the Technion – Israel Institute of Technology, and Peter Sternberg from
Indiana University read portions of the draft and made numerous comments and
suggestions for improvement. Raya Rubinstein prepared the drawings, while Yishai
Pinchover and Aviad Rubinstein assisted with the graphs. Despite our best efforts,
we surely did not discover all the mistakes in the draft. Therefore we encourage
observant readers to send us their comments at pincho@techunix.technion.ac.il.
We will maintain a webpage with a list of errata at http://www.math.technion.ac
.il/∼pincho/PDE.pdf.

1
Introduction
1.1 Preliminaries
A partial differential equation (PDE) describes a relation between an unknown
function and its partial derivatives. PDEs appear frequently in all areas of physics
and engineering. Moreover, in recent years we have seen a dramatic increase in the
use of PDEs in areas such as biology, chemistry, computer sciences (particularly in
relation to image processing and graphics) and in economics (ﬁnance). In fact, in
each area where there is an interaction between a number of independent variables,
we attempt to deﬁne functions in these variables and to model a variety of processes
by constructing equations for these functions. When the value of the unknown
function(s) at a certain point depends only on what happens in the vicinity of this
point, we shall, in general, obtain a PDE. The general form of a PDE for a function
u(x1, x2, . . . , xn) is
F(x1, x2, . . . , xn, u, ux1, ux2, . . . , ux11, . . .) = 0,
(1.1)
where x1, x2, . . . , xn are the independent variables, u is the unknown function,
and uxi denotes the partial derivative ∂u/∂xi. The equation is, in general, sup-
plemented by additional conditions such as initial conditions (as we have of-
ten seen in the theory of ordinary differential equations (ODEs)) or boundary
conditions.
The analysis of PDEs has many facets. The classical approach that dominated
the nineteenth century was to develop methods for ﬁnding explicit solutions. Be-
cause of the immense importance of PDEs in the different branches of physics,
every mathematical development that enabled a solution of a new class of PDEs
was accompanied by signiﬁcant progress in physics. Thus, the method of charac-
teristics invented by Hamilton led to major advances in optics and in analytical
mechanics. The Fourier method enabled the solution of heat transfer and wave
1

2
Introduction
propagation, and Green’s method was instrumental in the development of the theory
of electromagnetism. The most dramatic progress in PDEs has been achieved in
the last 50 years with the introduction of numerical methods that allow the use of
computers to solve PDEs of virtually every kind, in general geometries and under
arbitrary external conditions (at least in theory; in practice there are still a large
number of hurdles to be overcome).
The technical advances were followed by theoretical progress aimed at under-
standing the solution’s structure. The goal is to discover some of the solution’s
properties before actually computing it, and sometimes even without a complete
solution. The theoretical analysis of PDEs is not merely of academic interest, but
rather has many applications. It should be stressed that there exist very complex
equations that cannot be solved even with the aid of supercomputers. All we can
do in these cases is to attempt to obtain qualitative information on the solution. In
addition, a deep important question relates to the formulation of the equation and
its associated side conditions. In general, the equation originates from a model of
a physical or engineering problem. It is not automatically obvious that the model
is indeed consistent in the sense that it leads to a solvable PDE. Furthermore, it
is desired in most cases that the solution will be unique, and that it will be stable
under small perturbations of the data. A theoretical understanding of the equation
enables us to check whether these conditions are satisﬁed. As we shall see in what
follows, there are many ways to solve PDEs, each way applicable to a certain class
of equations. Therefore it is important to have a thorough analysis of the equation
before (or during) solving it.
The fundamental theoretical question is whether the problem consisting of the
equation and its associated side conditions is well posed. The French mathematician
Jacques Hadamard (1865–1963) coined the notion of well-posedness. According
to his deﬁnition, a problem is called well-posed if it satisﬁes all of the following
criteria
1. Existence The problem has a solution.
2. Uniqueness There is no more than one solution.
3. Stability A small change in the equation or in the side conditions gives rise to a small
change in the solution.
If one or more of the conditions above does not hold, we say that the problem is
ill-posed. One can fairly say that the fundamental problems of mathematical physics
are all well-posed. However, in certain engineering applications we might tackle
problems that are ill-posed. In practice, such problems are unsolvable. Therefore,
when we face an ill-posed problem, the ﬁrst step should be to modify it appropriately
in order to render it well-posed.

1.3 Differential operators and the superposition principle
3
1.2 Classiﬁcation
We pointed out in the previous section that PDEs are often classiﬁed into different
types. In fact, there exist several such classiﬁcations. Some of them will be de-
scribed here. Other important classiﬁcations will be described in Chapter 3 and in
Chapter 9.
r The order of an equation
The ﬁrst classiﬁcation is according to the order of the equation. The order is deﬁned to be
theorderofthehighestderivativeintheequation.Ifthehighestderivativeisoforderk,then
the equation is said to be of order k. Thus, for example, the equation utt −uxx = f (x, t)
is called a second-order equation, while ut + uxxxx = 0 is called a fourth-order equation.
r Linear equations
Another classiﬁcation is into two groups: linear versus nonlinear equations. An equation is
called linear if in (1.1), F is a linear function of the unknown function u and its derivatives.
Thus, for example, the equation x7ux + exyuy + sin(x2 + y2)u = x3 is a linear equation,
while u2
x + u2
y = 1 is a nonlinear equation. The nonlinear equations are often further
classiﬁed into subclasses according to the type of the nonlinearity. Generally speaking,
the nonlinearity is more pronounced when it appears in a higher derivative. For example,
the following two equations are both nonlinear:
uxx + uyy = u3,
(1.2)
uxx + uyy = |∇u|2u.
(1.3)
Here |∇u| denotes the norm of the gradient of u. While (1.3) is nonlinear, it is still linear
as a function of the highest-order derivative. Such a nonlinearity is called quasilinear. On
the other hand in (1.2) the nonlinearity is only in the unknown function. Such equations
are often called semilinear.
r Scalar equations versus systems of equations
A single PDE with just one unknown function is called a scalar equation. In contrast, a
set of m equations with l unknown functions is called a system of m equations.
1.3 Differential operators and the superposition principle
A function has to be k times differentiable in order to be a solution of an equation
of order k. For this purpose we deﬁne the set Ck(D) to be the set of all functions
that are k times continuously differentiable in D. In particular, we denote the set
of continuous functions in D by C0(D), or C(D). A function in the set Ck that
satisﬁes a PDE of order k, will be called a classical (or strong) solution of the
PDE. It should be stressed that we sometimes also have to deal with solutions that
are not classical. Such solutions are called weak solutions. The possibility of weak
solutions and their physical meaning will be discussed on several occasions later,

4
Introduction
see for example Sections 2.7 and 10.2. Note also that, in general, we are required
to solve a problem that consists of a PDE and associated conditions. In order for
a strong solution of the PDE to also be a strong solution of the full problem, it is
required to satisfy the additional conditions in a smooth way.
Mappings between different function sets are called operators. The operation
of an operator L on a function u will be denoted by L[u]. In particular, we shall
deal in this book with operators deﬁned by partial derivatives of functions. Such
operators, which are in fact mappings between different Ck classes, are called
differential operators.
An operator that satisﬁes a relation of the form
L[a1u1 + a2u2] = a1L[u1] + a2L[u2],
where a1 and a2 are arbitrary constants, and u1 and u2 are arbitrary functions is
called a linear operator. A linear differential equation naturally deﬁnes a linear
operator: the equation can be expressed as L[u] = f , where L is a linear operator
and f is a given function.
A linear differential equation of the form L[u] = 0, where L is a linear operator,
is called a homogeneous equation. For example, deﬁne the operator L = ∂2/∂x2 −
∂2/∂y2. The equation
L[u] = uxx −uyy = 0
is a homogeneous equation, while the equation
L[u] = uxx −uyy = x2
is an example of a nonhomogeneous equation.
Linear operators play a central role in mathematics in general, and in PDE
theory in particular. This results from the important property (which follows at
once from the deﬁnition) that if for 1 ≤i ≤n, the function ui satisﬁes the linear
differential equation L[ui] = fi, then the linear combination v := n
i=1 αiui sat-
isﬁes the equation L[v] = n
i=1 αi fi. In particular, if each of the functions
u1, u2, . . . , un satisﬁes the homogeneous equation L[u] = 0, then every linear com-
bination of them satisﬁes that equation too. This property is called the superposition
principle. It allows the construction of complex solutions through combinations of
simple solutions. In addition, we shall use the superposition principle to prove
uniqueness of solutions to linear PDEs.
1.4 Differential equations as mathematical models
PDEs are woven throughout science and technology. We shall brieﬂy review a
number of canonical equations in different areas of application. The fundamental

1.4 Differential equations as mathematical models
5
laws of physics provide a mathematical description of nature’s phenomena on a
variety of scales of time and space. Thus, for example, very large scale phenomena
(astronomical scales) are controlled by the laws of gravity. The theory of electro-
magnetism controls the scales involved in many daily activities, while quantum
mechanics is used to describe phenomena on the atomic scale. It turns out, how-
ever, that many important problems involve interaction between a large number
of objects, and thus it is difﬁcult to use the basic laws of physics to describe
them. For example, we do not fall to the ﬂoor when we sit on a chair. Why? The
fundamental reason lies in the electric forces between the atoms constituting the
chair. These forces endow the chair with high rigidity. It is clear, though, that it
is not feasible to solve the equations of electromagnetism (Maxwell’s equations)
to describe the interaction between such a vast number of objects. As another
example, consider the ﬂow of a gas. Each molecule obeys Newton’s laws, but
we cannot in practice solve for the evolution of an Avogadro number of individ-
ual molecules. Therefore, it is necessary in many applications to develop simpler
models.
The basic approach towards the derivation of these models is to deﬁne new quan-
tities (temperature, pressure, tension,. . .) that describe average macroscopic values
of the fundamental microscopic quantities, to assume several fundamental princi-
ples, such as conservation of mass, conservation of momentum, conservation of
energy, etc., and to apply the new principles to the macroscopic quantities. We shall
often need some additional ad-hoc assumptions to connect different macroscopic
entities. In the optimal case we would like to start from the fundamental laws and
then average them to achieve simpler models. However, it is often very hard to do
so, and, instead, we shall sometimes use experimental observations to supplement
the basic principles. We shall use x, y, z to denote spatial variables, and t to denote
the time variable.
1.4.1 The heat equation
A common way to encourage scientiﬁc progress is to confer prizes and awards.
Thus, the French Academy used to set up competitions for its prestigious prizes
by presenting speciﬁc problems in mathematics and physics. In 1811 the Academy
chose the problem of heat transfer for its annual prize. The prize was awarded to the
French mathematician Jean Baptiste Joseph Fourier (1768–1830) for two important
contributions. (It is interesting to mention that he was not an active scientist at that
time, but rather the governor of a region in the French Alps – actually a politician!).
He developed, as we shall soon see, an appropriate differential equation, and, in
addition developed, as we shall see in Chapter 5, a novel method for solving this
equation.

6
Introduction
The basic idea that guided Fourier was conservation of energy. For simplicity
we assume that the material density and the heat capacity are constant in space
and time, and we scale them to be 1. We can therefore identify heat energy with
temperature. Let D be a ﬁxed spatial domain, and denote its boundary by ∂D.
Under these conditions we shall write down the change in the energy stored in D
between time t and time t + t:

D
[u(x, y, z, t + t) −u(x, y, z, t)] dV
=
 t+t
t

D
q(x, y, z, t, u)dV dt −
 t+t
t

∂D
⃗B(x, y, z, t) · ˆndSdt,
(1.4)
where u is the temperature, q is the rate of heat production in D, ⃗B is the heat
ﬂux through the boundary, dV and dS are space and surface integration elements,
respectively, and ˆn is a unit vector pointing in the direction of the outward nor-
mal to ∂D. Notice that the heat production can be negative (a refrigerator, an air
conditioner), as can the heat ﬂux.
In general the heat production is determined by external sources that are inde-
pendent of the temperature. In some cases (such as an air conditioner controlled
by a thermostat) it depends on the temperature itself but not on its derivatives.
Hence we assume q = q(x, y, z, t, u). To determine the functional form of the heat
ﬂux, Fourier used the experimental observation that ‘heat ﬂows from hotter places
to colder places’. Recall from calculus that the direction of maximal growth of a
function is given by its gradient. Therefore, Fourier postulated
⃗B = −k(x, y, z) ⃗∇u.
(1.5)
Theformula(1.5)iscalledFourier’slawofheatconduction.The(positive!)function
k is called the heat conduction (or Fourier) coefﬁcient. The value(s) of k depend
on the medium in which the heat diffuses. In a homogeneous domain k is expected
to be constant. The assumptions on the functional dependence of q and ⃗B on u are
called constitutive laws.
We substitute our formula for q and ⃗B into (1.4), approximate the t integrals
using the mean value theorem, divide both sides of the equation by t, and take
the limit t →0. We obtain

D
utdV =

D
q(x, y, z, t, u)dV +

∂D
k(x, y, z) ⃗∇u · ˆndS.
(1.6)
Observe that the integration in the second term on the right hand side is over a
different set than in the other terms. Thus we shall use Gauss’ theorem to convert

1.4 Differential equations as mathematical models
7
the surface integral into a volume integral:

D
[ut −q −⃗∇· (k ⃗∇u)]dV = 0,
(1.7)
where ⃗∇· denotes the divergence operator. The following simple result will be used
several times in the book.
Lemma 1.1 Let h(x, y, z) be a continuous function satisfying

 h(x, y, z)dV = 0
for every domain . Then h ≡0.
Proof Let us assume to the contrary that there exists a point P = (x0, y0, z0) where
h(P) ̸= 0. Assume without loss of generality that h(P) > 0. Since h is continuous,
there exists a domain (maybe very small) D0, containing P and ϵ > 0, such that h >
ϵ > 0 at each point in D0. Therefore

D0 hdV > ϵVol(D0) > 0 which contradicts
the lemma’s assumption.
□
Returning to the energy integral balance (1.7), we notice that it holds for any
domain D. Assuming further that all the functions in the integrand are continuous,
we obtain the PDE
ut = q + ⃗∇· (k ⃗∇u).
(1.8)
In the special (but common) case where the diffusion coefﬁcient is constant, and
there are no heat sources in D itself, we obtain the classical heat equation
ut = ku,
(1.9)
where we use u to denote the important operator uxx + uyy + uzz. Observe that
we have assumed that the solution of the heat equation, and even some of its
derivatives are continuous functions, although we have not solved the equation yet.
Therefore, in principle we have to reexamine our assumptions a posteriori. We shall
see examples later in the book in which solutions of a PDE (or their derivatives) are
not continuous. We shall then consider ways to provide a meaning for the seemingly
absurd process of substituting a discontinuous function into a differential equation.
One of the fundamental ways of doing so is to observe that the integral balance
equation (1.6) provides a more fundamental model than the PDE (1.8).
1.4.2 Hydrodynamics and acoustics
Hydrodynamics is the physical theory of ﬂuid motion. Since almost any conceivable
volume of ﬂuid (whether it is a cup of coffee or the Paciﬁc Ocean) contains a
huge number of molecules, it is not feasible to describe the ﬂuid using the law
of electromagnetism or quantum mechanics. Hence, since the eighteenth century

8
Introduction
scientists have developed models and equations that are appropriate to macroscopic
entities such as temperature, pressure, effective velocity, etc. As explained above,
these equations are based on conservation laws.
The simplest description of a ﬂuid consists of three functions describing its state
at any point in space-time:
r the density (mass per unit of volume) ρ(x, y, z, t);
r the velocity ⃗u(x, y, z, t);
r the pressure p(x, y, z, t).
To be precise, we must also include the temperature ﬁeld in the ﬂuid. But to
simplify matters, it will be assumed here that the temperature is a known constant.
We start with conservation of mass. Consider a ﬂuid element occupying an arbitrary
spatial domain D. We assume that matter neither is created nor disappears in D.
Thus the total mass in D does not change:
∂
∂t

D
ρdV = 0.
(1.10)
The motion of the ﬂuid boundary is given by the component of the velocity ⃗u in
the direction orthogonal to the boundary ∂D. Thus we can write

D
∂
∂t ρdV +

∂D
ρ⃗u · ˆndS = 0,
(1.11)
where we denoted the unit external normal to ∂D by ˆn. Using Gauss’ theorem we
obtain

D
[ρt + ⃗∇· (ρ⃗u)]dV = 0.
(1.12)
Since D is an arbitrary domain we can use again Lemma 1.1 to obtain the mass
transport equation
ρt + ⃗∇· (ρ⃗u) = 0.
(1.13)
Next we require the ﬂuid to satisfy the momentum conservation law. The forces
acting on the ﬂuid in D are gravity, acting on each point in the ﬂuid, and the pressure
applied at the boundary of D by the rest of the ﬂuid outside D. We denote the
density per unit mass of the gravitational force by ⃗g. For simplicity we neglect the
friction forces between adjacent ﬂuid molecules. Newton’s law of motion implies
an equality between the change in the ﬂuid momentum and the total forces acting
on the ﬂuid. Thus
∂
∂t

D
ρ⃗udV = −

∂D
pˆnds +

D
ρ⃗gdV.
(1.14)

1.4 Differential equations as mathematical models
9
Let us interchange again the t differentiation with the spatial integration, and use
(1.13) to obtain the integral balance

D
[ρ⃗ut + ρ(⃗u · ⃗∇)⃗u]dV =

D
(−⃗∇p + ρ⃗g)dV.
(1.15)
From this balance we deduce the PDE
⃗ut + (⃗u · ⃗∇)⃗u = −1
ρ
⃗∇p + ⃗g.
(1.16)
So far we have developed two PDEs for three unknown functions (ρ, ⃗u, p). We
therefore need a third equation to complete the system. Notice that conservation of
energy has already been accounted for by assuming that the temperature is ﬁxed.
In fact, the additional equation does not follow from a conservation law, rather one
imposes a constitutive relation (like Fourier’s law from the previous subsection).
Speciﬁcally, we postulate a relation of the form
p = f (ρ),
(1.17)
where the function f is determined by the speciﬁc ﬂuid (or gas). The full system
comprising (1.13), (1.16) and (1.17) is called the Euler ﬂuid ﬂow equations. These
equations were derived in 1755 by the Swiss mathematician Leonhard Euler (1707–
1783).
If one takes into account the friction between the ﬂuid molecules, the equations
acquire an additional term. This friction is called viscosity. The special case of
viscous ﬂuids where the density is essentially constant is of particular importance.
It characterizes, for example, most phenomena involving the ﬂow of water. This
case was analyzed ﬁrst in 1822 by the French engineer Claude Navier (1785–1836),
and then studied further by the British mathematician George Gabriel Stokes (1819–
1903). They derived the following set of equations:
ρ(⃗ut + (⃗u · ⃗∇)⃗u) = µ⃗u −⃗∇p,
(1.18)
⃗∇· ⃗u = 0.
(1.19)
The parameter µ is called the ﬂuid’s viscosity. Notice that (1.18)–(1.19) form a
quasilinear system of equations. The Navier–Stokes system lies at the foundation of
hydrodynamics. Enormous computational efforts are invested in solving them under
a variety of conditions and in a plurality of applications, including, for example, the
design of airplanes and ships, the design of vehicles, the ﬂow of blood in arteries,
the ﬂow of ink in a printer, the locomotion of birds and ﬁsh, and so forth. Therefore
it is astonishing that the well-posedness of the Navier–Stokes equations has not
yet been established. Proving or disproving their well-posedness is one of the most

10
Introduction
important open problems in mathematics. A prize of one million dollars awaits the
person who solves it.
An important phenomenon described by the Euler equations is the propagation
of sound waves. In order to construct a simple model for sound waves, let us look
at the Euler equations for a gas at rest. For simplicity we neglect gravity. It is easy
to check that the equations have a solution of the form
⃗u = 0,
ρ = ρ0,
p = p0 = f (ρ0),
(1.20)
where ρ0 and p0 are constants describing uniform pressure and density. Let us
perturb the gas by creating a localized pressure (for example by producing a
sound out of our throats, or by playing a musical instrument). Assume that the
perturbation is small compared with the original pressure p0. One can therefore
write
⃗u = ϵ⃗u1,
ρ = ρ0 + ϵρ1,
(1.21)
p = p0 + ϵp1 = f (ρ0) + ϵ f ′(ρ0)ρ1,
where we denoted the perturbation to the density, velocity and pressure by ⃗u1, ρ1,
and p1, respectively, ϵ denotes a small positive parameter, and we used (1.17).
Substituting the expansion (1.21) into the Euler equations, and retaining only the
terms that are linear in ϵ, we ﬁnd
ρ1
t + ρo ⃗∇· ⃗u1 = 0,
(1.22)
⃗u1
t + 1
ρ0 ⃗∇p1 = 0.
Applying the operator ⃗∇· to the second equation in (1.22), and substituting the
result into the time derivative of the ﬁrst equation leads to
ρ1
tt −f ′(ρ0)ρ1 = 0.
(1.23)
Alternatively we can use the linear relation between p1 and ρ1 to write a similar
equation for the pressure
p1
tt −f ′(ρ0)p1 = 0.
(1.24)
The equation we have obtained is called a wave equation. We shall see later that this
equationindeeddescribeswavespropagatingwithspeedc =

f ′(ρ0).Inparticular,
inthecaseofwavesinalongnarrowtube,orinalongandnarrowtunnel,thepressure

1.4 Differential equations as mathematical models
11
only depends on time and on a single spatial coordinate x along the tube. We then
obtain the one-dimensional wave equation
p1
tt −c2 p1
xx = 0.
(1.25)
Remark 1.2 Many problems in chemistry, biology and ecology involve the spread
of some substrate being convected by a given velocity ﬁeld. Denoting the con-
centration of the substrate by C(x, y, z, t), and assuming that the ﬂuid’s ve-
locity does not depend on the concentration itself, we ﬁnd that (1.13) in the
formulation
Ct + ⃗∇· (C⃗u) = 0
(1.26)
describes the spread of the substrate. This equation is naturally called the convection
equation. In Chapter 2 we shall develop solution methods for it.
1.4.3 Vibrations of a string
Many different phenomena are associated with the vibrations of elastic bodies.
For example, recall the wave equation derived in the previous subsection for the
propagation of sound waves. The generation of sound waves also involves a wave
equation – for example the vibration of the sound chords, or the vibration of a string
or a membrane in a musical instrument.
Consider a uniform string undergoing transversal motion whose amplitude is
denoted by u(x, t), where x is the spatial coordinate, and t denotes time. We
also use ρ to denote the mass density per unit length of the string. We shall
assume that ρ is constant. Consider further a small interval (−δ, δ). Just as in
the previous subsection, we shall consider two forces acting on the string: an
external given force (e.g. gravity) acting only in the transversal (y) direction,
whose density is denoted by f (x, t), and an internal force acting between adja-
cent string elements. This internal force is called tension. It will be denoted by
⃗T . The tension acts on the string element under consideration at its two ends.
A tension ⃗T + acts at the right hand end, and a tension ⃗T −acts at the left hand
end. We assume that the tension is in the direction tangent to the string, and that
it is proportional to the string’s elongation. Namely, we assume the constitutive
law
⃗T = d

1 + u2x ˆeτ,
(1.27)
where d is a constant depending on the material of which the string is made, and
ˆeτ is a unit vector in the direction of the string’s tangent. It is an empirical law, i.e.
it stems from experimental observations. Projecting the momentum conservation

12
Introduction
equation (Newton’s second law) along the y direction we ﬁnd:
 δ
−δ
ρuttdl =
 δ
−δ
f (x, t)dl + ˆe2 · ( ⃗T + −⃗T −) =
 δ
−δ
f (x, t)dl +
 δ
−δ
(ˆe2 · ⃗T )xdx,
where dl denotes a length element, and ˆe2 = (0, 1). Using the constitutive law for
the tension and the following formula for the tangent vector ˆeτ = (1, ux)/

1 + u2x,
we can write
ˆe2 · ⃗T = d

1 + u2x ˆe2 · ˆeτ = dux.
Substituting this equation into the momentum equation we obtain the integral bal-
ance
 δ
−δ
ρutt

1 + u2xdx =
 δ
−δ

f

1 + u2x + duxx

dx.
Since this equation holds for arbitrary intervals, we can use Lemma 1.1 once again
to obtain
utt −
c2

1 + u2x
uxx = f (x, t)
ρ
,
(1.28)
where the wave speed is given by c = √d/ρ. A different string model will be
derived in Chapter 10. The two models are compared in Remark 10.5.
In the case of weak vibrations the slopes of the amplitude are small, and we
can make the simplifying assumption |ux| ≪1. We can then write an approximate
equation:
utt −c2uxx = 1
ρ f (x, t).
(1.29)
Thus, the wave equation developed earlier for sound waves is also applicable to
describe certain elastic waves. Equation (1.29) was proposed as early as 1752 by
the French mathematician Jean d’Alembert (1717–1783). We shall see in Chapter 4
how d’Alembert solved it.
Remark 1.3 We have derived an equation for the transversal vibrations of a string.
What about its longitudinal vibrations? To answer this question, project the mo-
mentum equation along the tangential direction, and again use the constitutive law.
We ﬁnd that the density of the tension force in the longitudinal direction is given by
∂
∂x

d

1 + u2x

1 + u2x

= 0.
This implies that the constitutive law we used is equivalent to assuming the string
does not undergo longitudinal vibrations!

1.4 Differential equations as mathematical models
13
1.4.4 Random motion
Random motion of minute particles was ﬁrst described in 1827 by the British
biologist Robert Brown (1773–1858). Hence this motion is called Brownian motion.
The ﬁrst mathematical model to describe this motion was developed by Einstein in
1905. He proposed a model in which a particle at a point (x, y) in the plane jumps
during a small time interval δt to a nearby point from the set (x ± δx, y ± δx).
Einstein showed that under a suitable assumption on δx and δt, the probability that
the particle will be found at a point (x, y) at time t satisﬁes the heat equation. His
model has found many applications in physics, biology, chemistry, economics etc.
We shall demonstrate now how to obtain a PDE from a typical problem in the theory
of Brownian motion.
Consider a particle in a two-dimensional domain D. For simplicity we shall
limit ourselves to the case where D is the unit square. Divide the square into N 2
identical little squares, and denote their vertices by {(xi, y j)}. The size of each edge
of a small square will be denoted by δx. A particle located at an internal vertex
(xi, y j) jumps during a time interval δt to one of its nearest neighbors with equal
probability. When the particle reaches a boundary point it dies.
Question What is the life expectancy u(x, y) of a particle that starts its life at a
point (x, y) in the limit
δx →0, δt →0,
(δx)2
2δt
= k?
(1.30)
We shall answer the question using an intuitive notion of the expectancy. Obvi-
ously a particle starting its life at a boundary point dies at once. Thus
u(x, y) = 0,
(x, y) ∈∂D.
(1.31)
Consider now an internal point (x, y). A particle must have reached this point
from one of its four nearest neighbors with equal probability for each neighbor. In
addition, the trip from the neighboring point lasted a time interval δt. Therefore u
satisﬁes the difference equation
u(x, y) = δt + 1
4[u(x −δx, y) + u(x + δx, y) + u(x, y −δx) + u(x, y + δx)].
(1.32)
We expand all functions on the right hand side into a Taylor series, assuming u ∈C4.
Dividing by δt and taking the limit (1.30) we obtain (see also Chapter 11)
u = −1
k ,
(x, y) ∈D.
(1.33)
An equation of the type (1.33) is called a Poisson equation. We shall elaborate on
such equations in Chapter 7.

14
Introduction
The model we just investigated has many applications. One of them relates to the
analysis of variations in stock prices. Many models in the stock market are based
on assuming that stocks prices vary randomly. Assume for example that a broker
buys a stock at a certain price m. She decides in advance to sell it if its price reaches
an upper bound m2 (in order to cash in her proﬁt) or a lower bound m1 (to minimize
losses in case the stock dives). How much time on average will the broker hold
the stock, assuming that the stock price performs a Brownian motion? This is a
one-dimensional version of the model we derived. The equation and the associated
boundary conditions are
ku′′(m) = −1,
u(m1) = u(m2) = 0.
(1.34)
The reader will be asked to solve the equation in Exercise 1.6.
1.4.5 Geometrical optics
We have seen two derivations of the wave equation – one for sound waves, and
another one for elastic waves. Yet there are many other physical phenomena
controlled by wave propagation. Two notable examples are electromagnetic waves
and water waves. Although there exist many analytic methods for solving wave
equations (we shall learn some of them later), it is not easy to apply them in
complex geometries. One might be tempted to proceed in such cases to numerical
methods (see Chapter 11). The problem is that in many applications the waves
are of very high frequency (or, equivalently, of very small wavelength). To
describe such waves we need a resolution that is considerably smaller than a single
wavelength. Consider for example optical phenomena. They are described by a
wave equation; a typical wavelength for the visible light part of the spectrum is
about half a micron. Assuming that we use ﬁve points per wavelength to describe
the wave, and that we deal with a three-dimensional domain with linear dimension
of 10−1 meters, we conclude that we need altogether about 1017 points! Even
storing the data is a difﬁcult task, not to mention the formidable complexity of
solving equations with so many unknowns (Chapter 11).
Fortunately it is possible to turn the problem around and actually use the short
wavelength to derive approximate equations that are much simpler to solve, and,
yet, provide a fair description of optics. Consider for this purpose the wave equation
in R3:
vtt −c2(⃗x)v = 0.
(1.35)
Notice that the wave’s speed need not be constant. We expect solutions that are
oscillatory in time (see Chapter 5). Therefore we seek solutions of the form
v(x, y, z, t) = eiωtψ(x, y, z).

1.4 Differential equations as mathematical models
15
It is convenient to introduce at this stage the notation k = ω/c0 and n = c0/c(x),
where c0 is an average wave velocity in the medium. Substituting v into (1.35)
yields
ψ + k2n2(⃗x)ψ = 0.
(1.36)
The function n(x) is called the refraction index. The parameter k is called the wave
number. It is easy to see that k−1 has the dimension of length. In fact, the wavelength
is given by 2πk−1. As was explained above, the wavelength is often much smaller
than any other length scale in the problem. For example, spectacle lenses involve
scales such as 5 mm (thickness), 60 mm (radius of curvature) or 40 mm (frame
size), all of them far greater than half a micron which is a typical wavelength. We
therefore assume that the problem is scaled with respect to one of the large scales,
and hence k is a very large number. To use this fact we seek a solution to (1.36) of
the form:
ψ(x, y, z) = A(x, y, z; k)eikS(x,y,z).
(1.37)
Substituting (1.37) into (1.36), and assuming that A is bounded with respect to k,
we get
A[| ⃗∇S|2 −n2(⃗x)] = O
	1
k

.
Thus the function S satisﬁes the eikonal equation
| ⃗∇S| = n(⃗x).
(1.38)
Thisequation,postulatedin1827bytheIrishmathematicianWilliamRowanHamil-
ton (1805–1865), provides the foundation for geometrical optics. It is extremely
useful in many applications in optics, such as radar, contact lenses, projectors,
mirrors, etc. In Chapter 2 we shall develop a method for solving eikonal equa-
tions. Later, in Chapter 9, we shall encounter the eikonal equation from a different
perspective.
1.4.6 Further real world equations
r The Laplace equation
Many of the models we have examined so far have something in common – they involve
the operator
u = ∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 .

16
Introduction
This operator is called the Laplacian. Probably the ‘most important’ PDE is the Laplace
equation
u = 0.
(1.39)
The equation, which is a special case of the Poisson equation we introduced earlier, was
proposed in 1780 by the French mathematician Pierre-Simon Laplace (1749–1827) in
his work on gravity. Solutions of the Laplace equation are called harmonic functions.
Laplace’s equation can be found everywhere. For example, in the heat conduction prob-
lems that were introduced earlier, the temperature ﬁeld is harmonic when temporal equi-
librium is achieved. The equation is also fundamental in mechanics, electromagnetism,
probability, quantum mechanics, gravity, biology, etc.
r The minimal surface equation
When we dip a narrow wire in a soap bath, and then lift the wire gently out of the bath, we
can observe a thin membrane spanning the wire. The French mathematician Joseph-Louis
Lagrange (1736–1813) showed in 1760 that the surface area of the membrane is smaller
than the surface area of any other surface that is a small perturbation of it. Such special
surfaces are called minimal surfaces. Lagrange further demonstrated that the graph of a
minimal surface satisﬁes the following second-order nonlinear PDE:
(1 + u2
y)uxx −2uxuyuxy + (1 + u2
x)uyy = 0.
(1.40)
When the slopes of the minimal surface are small, i.e. ux, uy ≪1, we see at once that
(1.40) can be approximated by the Laplace equation. We shall return to the minimal
surface equation in Chapter 10.
r The biharmonic equation
The equilibrium state of a thin elastic plate is provided by its amplitude function u(x, y),
which describes the deviation of the plate from its horizontal position. It can be shown
that the unknown function u satisﬁes the equation
2u = (u) = uxxxx + 2uxxyy + uyyyy = 0.
(1.41)
For an obvious reason this equation is called the biharmonic equation. Notice that in
contrast to all the examples we have seen so far, it is a fourth-order equation. We fur-
ther point out that almost all the equations we have seen here, and also other important
equations such as Maxwell’s equations, the Schr¨odinger equation and Newton’s equation
for the gravitational ﬁeld are of second order. We shall return to the plate equation in
Chapter 10.
r The Schr¨odinger equation
One of the fundamental equations of quantum mechanics, derived in 1926 by the Austrian
physicist Erwin Schr¨odinger (1887–1961), governs the evolution of the wave function u
of a particle in a potential ﬁeld V :
iℏ∂u
∂t = −ℏ
2m u + V u.
(1.42)

1.5 Associated conditions
17
Here V is a known function (potential), m is the particle’s mass, and ℏis Planck’s constant
divided by 2π. We shall consider the Schr¨odinger equation for the special case of an
electron in the hydrogen atom in Chapter 9.
r Other equations
There are many other PDEs that are central to the study of different problems in science
and technology. For example we mention: the Maxwell equations of electromagnetism;
reaction–diffusion equations that model chemical reactions; the equations of elasticity;
the Korteweg–de Vries equation for solitary waves; the nonlinear Schr¨odinger equation in
nonlinear optics and in superﬂuids; the Ginzburg–Landau equations of superconductivity;
Einstein’s equations of general relativity, and many more.
1.5 Associated conditions
PDEs have in general inﬁnitely many solutions. In order to obtain a unique solution
one must supplement the equation with additional conditions. What kind of condi-
tions should be supplied? It turns out that the answer depends on the type of PDE
under consideration. In this section we brieﬂy review the common conditions, and
explain through examples their physical signiﬁcance.
1.5.1 Initial conditions
Let us consider the transport equation (1.26) in one spatial dimension as a prototype
for equations of ﬁrst order. The unknown function C(x, t) is a surface deﬁned over
the (x, t) plane. It is natural to formulate a problem in which one supplies the con-
centration at a given time t0, and then to deduce from the equation the concentration
at later times. Namely, we solve the problem consisting of the convection equation
Ct + ⃗∇· (C ⃗u) = 0,
and the condition
C(x, t0) = C0(x).
(1.43)
This problem is called an initial value problem. Geometrically speaking, condition
(1.43) determines a curve through which the solution surface must pass. We can
generalize (1.43) by imposing a curve  that must lie on the solution surface, so
that the projection of  on the (x, t) plane is not necessarily the x axis. In Chapter 2
we shall show that under suitable assumptions on the equation and , there indeed
exists a unique solution.
Another case where it is natural to impose initial conditions is the heat equation
(1.9). Here we provide the temperature distribution at some initial time (say t = 0),

18
Introduction
and solve for its distribution at later times, namely, the initial condition for (1.9) is
of the form u(x, y, z, 0) = u0(x, y, z).
The last two examples involve PDEs with just a ﬁrst derivative with respect
to t. In analogy with the theory of initial value problems for ODEs, we expect
that equations that involve second derivatives with respect to t will require two
initial conditions. Indeed, let us look at the wave equation (1.29). As explained in
the previous section, this equation is nothing but Newton’s second law, equating
the mass times the acceleration and the forces acting on the string. Therefore it is
natural to supply two initial conditions, one for the initial location of the string, and
one for its initial velocity:
u(x, 0) = u0(x),
ut(x, 0) = u1(x).
(1.44)
We shall indeed prove in Chapter 4 that these conditions, together with the wave
equation lead to a well-posed problem.
1.5.2 Boundary conditions
Another type of constraint for PDEs that appears in many applications is called
boundary conditions. As the name indicates, these are conditions on the behavior
of the solution (or its derivative) at the boundary of the domain under consideration.
As a ﬁrst example, consider again the heat equation; this time, however, we limit
ourselves to a given spatial domain :
ut = ku
(x, y, z) ∈, t > 0.
(1.45)
We shall assume in general that  is bounded. It turns out that in order to obtain a
unique solution, one should provide (in addition to initial conditions) information
on the behavior of u on the boundary ∂. Excluding rare exceptions, we encounter
in applications three kinds of boundary conditions. The ﬁrst kind, where the values
of the temperature on the boundary are supplied, i.e.
u(x, y, z, t) = f (x, y, z, t)
(x, y, z) ∈∂, t > 0,
(1.46)
is called a Dirichlet condition in honor of the German mathematician Johann
Lejeune Dirichlet (1805–1859). For example, this condition is used when the
boundary temperature is given through measurements, or when the temperature
distribution is examined under a variety of external heat conditions.
Alternatively one can supply the normal derivative of the temperature on the
boundary; namely, we impose (as usual we use here the notation ∂n to denote the
outward normal derivative at ∂)
∂nu(x, y, z, t) = f (x, y, z, t)
(x, y, z) ∈∂, t > 0.
(1.47)

1.5 Associated conditions
19
This condition is called a Neumann condition after the German mathematician Carl
Neumann (1832–1925). We have seen that the normal derivative ∂nu describes the
ﬂux through the boundary. For example, an insulating boundary is modeled by
condition (1.47) with f = 0.
A third kind of boundary condition involves a relation between the boundary
values of u and its normal derivative:
α(x, y, z)∂nu(x, y, z, t) + u(x, y, z, t) = f (x, y, z, t)
(x, y, z) ∈∂D, t > 0.
(1.48)
Such a condition is called a condition of the third kind. Sometimes it is also called
the Robin condition.
Although the three types of boundary conditions deﬁned above are by far the
most common conditions seen in applications, there are exceptions. For example,
we can supply the values of u at some parts of the boundary, and the values of
its normal derivative at the rest of the boundary. This is called a mixed boundary
condition. Another possibility is to generalize the condition of the third kind and
replace the normal derivative by a (smoothly dependent) directional derivative of
u in any direction that is not tangent to the boundary. This is called an oblique
boundary condition. Also, one can provide a nonlocal boundary condition. For
example, one can provide a boundary condition relating the heat ﬂux at each point
on the boundary to the integral of the temperature over the whole boundary.
To illustrate further the physical meaning of boundary conditions, let us consider
again the wave equation for a string:
utt −c2uxx = f (x, t)
a < x < b, t > 0.
(1.49)
When the locations of the end points of the string are known, we supply Dirichlet
boundary conditions (Figure 1.1(a)):
u(a, t) = β1(t),
u(b, t) = β2(t), t > 0.
(1.50)
Another possibility is that the tension at the end points is given. From our deriva-
tion of the string equation in Subsection 1.4.3 it follows that this case involves a
b
a
b
a
(a)
(b)
Figure 1.1 Illustrating boundary conditions for a string.

20
Introduction
Neumann condition:
ux(a, t) = β1(t),
ux(b, t) = β2(t), t > 0.
(1.51)
Thus, for example, when the end points are free to move in the transversal direction
(Figure 1.1(b)), we shall use a homogeneous Neumann condition, i.e. β1 = β2 = 0.
1.6 Simple examples
Before proceeding to develop general solution methods, let us warm up with a few
very simple examples.
Example 1.4 Solve the equation uxx = 0 for an unknown function u(x, y). We can
consider the equation as an ODE in x, with y being a parameter. Thus the general
solution is u(x, y) = A(y)x + B(y). Notice that the solution space is huge, since
A(y) and B(y) are arbitrary functions.
Example 1.5 Solve the equation uxy + ux = 0. We can transform the problem
into an ODE by setting v = ux. The new function v(x, y) satisﬁes the equation
vy + v = 0. Treating x as a parameter, we obtain v(x, y) = C(x)e−y. Integrating v
we construct the solution to the original problem: u(x, y) = D(x)e−y + E(y).
Example 1.6 Find a solution of the wave equation utt −4uxx = sin t + x2000. No-
tice that we are asked to ﬁnd a solution, and not the most general solution. We shall
exploit the linearity of the wave equation. According to the superposition principle,
we can split u = v + w, such that v and w are solutions of
vtt −4vxx = sin t,
(1.52)
wtt −4wxx = x2000.
(1.53)
The advantage gained by this step is that solutions for each of these equations can
be easily obtained:
v(x, t) = −sin t,
w(x, t) = −
1
4 × 2001 × 2002x2002.
Thus
u(x, t) = −sin t −
1
4 × 2001 × 2002x2002.
There are many other solutions. For example, it is easy to check that if we add
to the solution above a function of the form f (x −2t), where f (s) is an arbitrary
twice differentiable function, a new solution is obtained.

1.7 Exercises
21
Unfortunately one rarely encounters real problems described by such simple equa-
tions. Nevertheless, we can draw a few useful conclusions from these examples.
For instance, a commonly used method is to seek a transformation from the original
variables to new variables in which the equation takes a simpler form. Also, the
superposition principle, which enables us to decompose a problem into a set of far
simpler problems, is quite general.
1.7 Exercises
1.1 Show that each of the following equations has a solution of the form u(x, y) = f (ax +
by) for a proper choice of constants a, b. Find the constants for each example.
(a) ux + 3uy = 0.
(b) 3ux −7uy = 0.
(c) 2ux + πuy = 0.
1.2 Show that each of the following equations has a solution of the form u(x, y) = eαx+βy.
Find the constants α, β for each example.
(a) ux + 3uy + u = 0.
(b) uxx + uyy = 5ex−2y.
(c) uxxxx + uyyyy + 2uxxyy = 0.
1.3 (a) Show that there exists a unique solution for the system
ux = 3x2y + y,
uy = x3 + x,
(1.54)
together with the initial condition u(0, 0) = 0.
(b) Prove that the system
ux = 2.999999x2y + y,
uy = x3 + x
(1.55)
has no solution at all.
1.4 Let u(x, y) = h(

x2 + y2) be a solution of the minimal surface equation.
(a) Show that h(r) satisﬁes the ODE
rh′′ + h′(1 + (h′)2) = 0.
(b) What is the general solution to the equation of part (a)?
1.5 Let p : R →R be a differentiable function. Prove that the equation
ut = p(u)ux
t > 0
has a solution satisfying the functional relation u = f (x + p(u)t), where f is a differ-
entiable function. In particular ﬁnd such solutions for the following equations:

22
Introduction
(a) ut = kux.
(b) ut = uux.
(c) ut = u sin(u)ux.
1.6 Solve (1.34), and compute the average time for which the broker holds the stock.
Analyze the result in light of the ﬁnancial interpretation of the parameters (m1, m2, k).
1.7 (a) Consider the equation uxx + 2uxy + uyy = 0. Write the equation in the coordinates
s = x, t = x −y.
(b) Find the general solution of the equation.
(c)Considertheequationuxx −2uxy + 5uyy = 0.Writeitinthecoordinatess = x + y,
t = 2x.

2
First-order equations
2.1 Introduction
A ﬁrst-order PDE for an unknown function u(x1, x2, . . . , xn) has the following
general form:
F(x1, x2, . . . , xn, u, ux1, ux2, . . . , uxn) = 0,
(2.1)
where F is a given function of 2n + 1 variables. First-order equations appear in
a variety of physical and engineering processes, such as the transport of material
in a ﬂuid ﬂow and propagation of wavefronts in optics. Nevertheless they appear
less frequently than second-order equations. For simplicity we shall limit the pre-
sentation in this chapter to functions in two variables. The reason for this is not
just to simplify the algebra. As we shall soon observe, the solution method is
based on the geometrical interpretation of u as a surface in an (n + 1)-dimensional
space. The results will be generalized to equations in any number of variables in
Chapter 9.
We thus consider a surface in R3 whose graph is given by u(x, y). The surface
satisﬁes an equation of the form
F(x, y, u, ux, uy) = 0.
(2.2)
Equation (2.2) is still quite general. In many practical situations we deal with
equations with a special structure that simpliﬁes the solution process. Therefore
we shall progress from very simple equations to more complex ones. There is a
common thread to all types of equations – the geometrical approach. The basic idea
is that since u(x, y) is a surface in R3, and since the normal to the surface is given by
the vector (ux, uy, −1), the PDE (2.2) can be considered as an equation relating the
surface to its normal (or alternatively its tangent plane). Indeed the main solution
method will be a direct construction of the solution surface.
23

24
First-order equations
2.2 Quasilinear equations
We consider ﬁrst a special class of nonlinear equations where the nonlinearity is
conﬁned to the unknown function u. The derivatives of u appear in the equation
linearly. Such equations are called quasilinear. The general form of a quasilinear
equation is
a(x, y, u)ux + b(x, y, u)uy = c(x, y, u).
(2.3)
An important special case of quasilinear equations is that of linear equations:
a(x, y)ux + b(x, y)uy = c0(x, y)u + c1(x, y),
(2.4)
where a, b, c0, c1 are given functions of (x, y).
Before developing the general theory for quasilinear equations, let us warm up
with a simple example.
Example 2.1
ux = c0u + c1.
(2.5)
In this example we set a = 1, b = 0, c0 is a constant, and c1 = c1(x, y). Since (2.5)
contains no derivative with respect to the y variable, we can regard this variable
as a parameter. Recall from the theory of ODEs that in order to obtain a unique
solution we must supply an additional condition. We saw in Chapter 1 that there
are many ways to supply additional conditions to a PDE. The natural condition
for a ﬁrst-order PDE is a curve lying on the solution surface. We shall refer to
such a condition as an initial condition, and the problem will be called an initial
value problem or a Cauchy problem in honor of the French mathematician Augustin
Louis Cauchy (1789–1857). For example, we can supplement (2.5) with the initial
condition
u(0, y) = y.
(2.6)
Since we are actually dealing with an ODE, the solution is immediate:
u(x, y) = ec0x
 x
0
e−c0ξc1(ξ, y)dξ + y

.
(2.7)
A basic approach for solving the general case is to seek special variables in which
the equation is simpliﬁed (actually, similar to (2.5)). Before doing so, let us draw a
few conclusions from this simple example.
(1) Notice that we integrated along the x direction (see Figure 2.1) from each point on the
y axis where the initial condition was given, i.e. we actually solved an inﬁnite set of
ODEs.

2.3 The method of characteristics
25
y
x
Figure 2.1 Integration of (2.5).
(2) Is there always a solution to (2.5) and an initial condition? At a ﬁrst sight the answer
seems positive; we can write a general solution for (2.5) in the form
u(x, y) = ec0x
 x
0
e−c0ξc1(ξ, y)dξ + T (y)

,
(2.8)
where the function T (y) is determined by the initial condition. There are examples,
however, where such a function does not exist at all! For instance, consider the special
case of (2.5) in which c1 ≡0. The solution (2.8) now becomes u(x, y) = ec0xT (y).
Replace the initial condition (2.6) with the condition
u(x, 0) = 2x.
(2.9)
Now T (y) must satisfy T (0) = 2xe−c0x, which is of course impossible.
(3) We have seen so far an example in which a problem had a unique solution, and an exam-
ple where there was no solution at all. It turns out that an equation might have inﬁnitely
many solutions. To demonstrate this possibility, let us return to the last example, and
replace the initial condition (2.6) by
u(x, 0) = 2ec0x.
(2.10)
Now T (y) should satisfy T (0) = 2. Thus every function T (y) satisfying T (0) = 2 will
provide a solution for the equation together with the initial condition. Therefore, (2.5)
with c1 = 0 has inﬁnitely many solutions under the initial condition (2.10).
We conclude from Example 2.1 that the solution process must include the step
of checking for existence and uniqueness. This is an example of the well-posedness
issue that was introduced in Chapter 1.
2.3 The method of characteristics
We solve ﬁrst-order PDEs by the method of characteristics. This method was de-
veloped in the middle of the nineteenth century by Hamilton. Hamilton investigated
the propagation of light. He sought to derive the rules governing this propagation

26
First-order equations
from a purely geometric theory, akin to Euclidean geometry. Hamilton was well
aware of the wave theory of light, which was proposed by the Dutch physicist
Christian Huygens (1629–1695) and advanced early in the nineteenth century by
the English scientist Thomas Young (1773–1829) and the French physicist Au-
gustin Fresnel (1788–1827). Yet, he chose to base his theory on the principle of
least time that was proposed in 1657 by the French scientist (and lawyer!) Pierre
de Fermat (1601–1665). Fermat proposed a uniﬁed principle, according to which
light rays travel from a point A to a point B in an orbit that takes the least amount
of time. Hamilton showed that this principle can serve as a foundation of a dy-
namical theory of rays. He thus derived an axiomatic theory that provided equa-
tions of motion for light rays. The main building block in the theory is a function
that completely characterizes any given optical medium. Hamilton called it the
characteristic function. He showed that Fermat’s principle implies that his char-
acteristic function must satisfy a certain ﬁrst-order nonlinear PDE. Hamilton’s
characteristic function and characteristic equation are now called the eikonal func-
tion and eikonal equation after the Greek word ϵικων (or ϵικoν) which means
“an image”.
Hamilton discovered that the eikonal equation can be solved by integrating it
along special curves that he called characteristics. Furthermore, he showed that in a
uniform medium, these curves are exactly the straight light rays whose existence has
been assumed since ancient times. In 1911 it was shown by the German physicists
Arnold Sommerfeld (1868–1951) and Carl Runge (1856–1927) that the eikonal
equation, proposed by Hamilton from his geometric theory, can be derived as a
small wavelength limit of the wave equation, as was shown in Chapter 1. Notice
that although the eikonal equation is of ﬁrst order, it is in fact fully nonlinear and
not quasilinear. We shall treat it separately later.
We shall ﬁrst develop the method of characteristics heuristically. Later we shall
present a precise theorem that guarantees that, under suitable assumptions, the equa-
tion together with its associated condition has a unique solution. The characteristics
method is based on ‘knitting’ the solution surface with a one-parameter family of
curves that intersect a given curve in space. Consider the general linear equation
(2.4), and write the initial condition parameterically:
 = (s) = (x0(s), y0(s), u0(s)), s ∈I = (α, β).
(2.11)
The curve  will be called the initial curve.
The linear equation (2.4) can be rewritten as
(a, b, c0u + c1) · (ux, uy, −1) = 0.
(2.12)
Since (ux, uy, −1) is normal to the surface u, the vector (a, b, c0u + c1) is in the

2.3 The method of characteristics
27
tangent plane. Hence, the system of equations
dx
dt (t) = a(x(t), y(t)),
dy
dt (t) = b(x(t), y(t)),
(2.13)
du
dt (t) = c(x(t), y(t)))u(t) + c1(x(t), y(t))
deﬁnes spatial curves lying on the solution surface (conditioned so that the curves
start on the surface). This is a system of ﬁrst-order ODEs. They are called the system
of characteristic equations or, for short, the characteristic equations. The solutions
are called characteristic curves of the equation. Notice that equations (2.13) are
autonomous, i.e. there is no explicit dependence upon the parameter t. In order
to determine a characteristic curve we need an initial condition. We shall require
the initial point to lie on the initial curve . Since each curve (x(t), y(t), u(t))
emanates from a different point (s), we shall explicitly write the curves in the
form (x(t, s), y(t, s), u(t, s)). The initial conditions are written as:
x(0, s) = x0(s), y(0, s) = y0(s), u(0, s) = u0(s).
(2.14)
Notice that we selected the parameter t such that the characteristic curve is located
on  when t = 0. One may, of course, select any other parameterization. We also
notice that, in general, the parameterization (x(t, s), y(t, s), u(t, s)) represents a
surface in R3.
One can readily verify that the method of characteristics applies to the quasilinear
equation (2.3) as well. Namely, each point on the initial curve  is a starting point
for a characteristic curve. The characteristic equations are now
xt(t) = a(x, y, u),
yt(t) = b(x, y, u),
ut(t) = c(x, y, u),
(2.15)
supplemented by the initial condition
x(0, s) = x0(s), y(0, s) = y0(s), u(0, s) = u0(s).
(2.16)
The problem consisting of (2.3) and initial conditions (2.16) is called the Cauchy
problem for quasilinear equations.
The main difference between the characteristic equations (2.13) derived for the
linear equation, and the set (2.15) is that in the former case the ﬁrst two equations of
(2.13) are independent of the third equation and of the initial conditions. We shall
observe later the special role played by the projection of the characteristic curves
on the (x, y) plane. Therefore, we write (for the linear case) the equation for this

28
First-order equations
y
u
x
characteristic
curve
initial curve
Figure 2.2 Sketch of the method of characteristics.
projection separately:
xt = a(x, y), yt = b(x, y).
(2.17)
In the quasilinear case, this uncoupling of the characteristic equations is no longer
possible, since the coefﬁcients a and b depend upon u. We also point out that in the
linear case, the equation for u is always linear, and thus it is guaranteed to have a
global solution (provided that the solutions x(t) and y(t) exist globally).
To summarize the preliminary presentation of the method of characteristics, let
us consult Figure 2.2. In the ﬁrst step we identify the initial curve . In the second
step we select a point s on  and solve the characteristic equations (2.13) (or (2.15)),
using the point we selected on  as an initial point. After performing these steps for
all points on  we obtain a portion of the solution surface (also called the integral
surface) that consists of the union of the characteristic curves. Philosophically
speaking, one might say that the characteristic curves take with them an initial piece
of information from , and propagate it with them. Furthermore, each characteristic
curve propagates independently of the other characteristic curves.
Let us demonstrate the method for a very simple case.
Example 2.2 Solve the equation
ux + uy = 2
subject to the initial condition u(x, 0) = x2.
The characteristic equations and the parametric initial conditions are
xt(t, s) = 1, yt(t, s) = 1, ut(t, s) = 2,
x(0, s) = s, y(0, s) = 0, u(0, s) = s2.
It is a simple matter to solve for the characteristic curves:
x(t, s) = t + f1(s), y(t, s) = t + f2(s), u(t, s) = 2t + f3(s).

2.3 The method of characteristics
29
Upon substituting into the initial conditions, we ﬁnd
x(t, s) = t + s, y(t, s) = t, u(t, s) = 2t + s2.
We have thus obtained a parametric representation of the integral surface. To ﬁnd
an explicit representation of the surface u as a function of x and y we need to invert
the transformation (x(t, s), y(t, s)), and to express it in the form (t(x, y), s(x, y)),
namely, we have to solve for (t, s) as functions of (x, y). In the current example the
inversion is easy to perform:
t = y, s = x −y.
Thus the explicit representation of the integral surface is given by
u(x, y) = 2y + (x −y)2 .
This simple example might lead us to think that each initial value problem for a
ﬁrst-order PDE possesses a unique solution. But we have already seen that this is
not the case. What, therefore, are the obstacles we might face? Is (2.3) equipped
with initial conditions (2.14) well-posed? For simplicity we shall discuss in this
chapter two aspects of well-posedness: existence and uniqueness. Thus the question
is whether there exists a unique integral surface for (2.3) that contains the initial
curve.
(1) Notice that even if the PDE is linear, the characteristic equations are nonlinear! We
know from the theory of ODEs that in general one can only establish local existence of
a unique solution (assuming that the coefﬁcients of the equation are smooth functions).
In other words, the solutions of nonlinear ODEs might develop singularities within a
short distance from the initial point even if the equation is very smooth. It follows that
one can expect at most a local existence theorem for a ﬁrst-order PDE, even if the PDE
is linear.
(2) The parametric representation of the integral surface might hide further difﬁculties. We
shall demonstrate this in the sequel by obtaining naive-looking parametric representa-
tions of singular surfaces. The difﬁculty lies in the inversion of the transformation from
the plane (t, s) to the plane (x, y). Recall that the implicit function theorem implies
that such a transformation is invertible if the Jacobian J = ∂(x, y)/∂(t, s) ̸= 0. But
we observe that while the dependence of the characteristic curves on the variable t is
derived from the PDE itself, the dependence on the variable s is derived from the initial
condition. Since the equation and the initial condition do not depend upon each other,
it follows that for any given equation there exist initial curves for which the Jacobian
vanishes, and the implicit function theorem does not hold.
Thefunctionalproblemwejustdescribedhasanimportantgeometricalinterpretation.
An explicit computation of the Jacobian at points located on the initial curve , using

30
First-order equations
the characteristic equations, gives
J = ∂x
∂t
∂y
∂s −∂x
∂s
∂y
∂t =

a
b
(x0)s
(y0)s
 = (y0)sa −(x0)sb,
(2.18)
where (x0)s = dx0/ds. Thus the Jacobian vanishes at some point if and only if the
vectors (a, b) and ((x0)s, (y0)s) are linearly dependent. Hence the geometrical meaning
of a vanishing Jacobian is that the projection of  on the (x, y) plane is tangent at this
point to the projection of the characteristic curve on that plane. As a rule, in order for
a ﬁrst-order quasilinear PDE to have a unique solution near the initial curve, we must
have J ̸= 0. This condition is called the transversality condition.
(3) So far we have discussed local problems. One can also encounter global problems. For
example, a characteristic curve might intersect the initial curve more than once. Since
the characteristic equation is well-posed for a single initial condition, then in such a
situation the solution will, in general, develop a singularity. We can think about this
situation in the following way. Recall that a characteristic curve ‘carries’ with it along
its orbit a charge of information from its intersection point with . If a characteristic
curve intersects  more than once, these two ‘information charges’ might be in conﬂict.
A similar global problem is the intersection of the projection on the (x, y) plane of
different characteristic curves with each other. Such an intersection is problematic for
the same reason as the intersection of a characteristic curve with the initial curve. Each
characteristic curve carries with it a different information charge, and a conﬂict might
arise at such an intersection.
(4) Another potential problem relates to a lack of uniqueness of the solution to the char-
acteristic equation. We should not worry about this possibility if the coefﬁcients of the
equations are smooth (Lipschitz continuous, to be precise). But when considering a
nonsmooth problem, we should pay attention to this issue. We shall demonstrate such
a case below.
In Section 2.5 we shall formulate and prove a precise theorem (Theorem 2.10)
that will include all the problems discussed above. Before doing so, let us examine
a few examples.
2.4 Examples of the characteristics method
Example 2.3 Solve the equation ux = 1 subject to the initial condition u(0, y) =
g(y).
The characteristic equations and the associated initial conditions are given by
xt = 1, yt = 0, ut = 1,
(2.19)
x(0, s) = 0, y(0, s) = s, u(0, s) = g(s),
(2.20)
respectively. The parametric integral surface is (x(t, s), y(t, s), u(t, s)) = (t, s, t +
g(s)). It is easy to deduce from here the explicit solution u(x, y) = x + g(y).

2.4 Examples of the characteristics method
31
On the other hand, if we keep the equation unchanged, but modify the initial
conditions into u(x, 0) = h(x), the picture changes dramatically. In this case the
parametric solution is
(x(t, s), y(t, s), u(t, s)) = (t + s, 0, t + h(s)).
Now, however, the transformation (x(t, s), y(t, s)) cannot be inverted. Geometri-
cally speaking, the reason is simple: the projection of the initial curve is precisely
the x axis, but this is also the projection of a characteristic curve. In the special
case where h(x) = x + c for some constant c, we obtain u(t, s) = s + t + c. Then
it is not necessary to invert the mapping (x(t, s), y(t, s)), since we ﬁnd at once
u = x + c + f (y) for every differentiable function f (y) that vanishes at the origin.
But for any other choice of h the problem has no solution at all.
We note that for the initial conditions u(x, 0) = h(x) we could have foreseen the
problem through a direct computation of the Jacobian:
J =

a
b
(x0)s
(y0)s
 =

1
0
1
0
 = 0.
(2.21)
Whenever the Jacobian vanishes along an interval (like in the example we are
considering), the problem will, in general, have no solution at all. If a solution does
exist, we shall see that this implies the existence of inﬁnitely many solutions.
Because of the special role played by the projection of the characteristic curves
on the (x, y) plane we shall use the term characteristics to denote them for short.
There are several ways to compute the characteristics. One of them is to solve the
full characteristic equations, and then to project the solution on the (x, y) plane.
We note that the projection of a characteristic curve is given by the condition
s = constant. Substituting this condition into the equation s = s(x, y) determines
an explicit equation for the characteristics. An alternative method is valid whenever
the PDE is linear. The linearity implies that the ﬁrst two characteristic equations are
independent of u. Thus they can be solved directly for the characteristics themselves
without solving ﬁrst for the parametric integral surface. Furthermore, since the
characteristic equations are autonomous (i.e. they do not explicitly include the
variable t), it follows that the equations for the characteristics can be written simply
as the ﬁrst-order ODE
dy
dx = b(x, y)
a(x, y).
Example 2.4 The current example will be useful for us in Chapter 3, where we
shall need to solve linear equations of the form
a(x, y)ux + b(x, y)uy = 0.
(2.22)

32
First-order equations
The equations for the characteristic curves
dx
dt = a,
dy
dt = b,
du
dt = 0
imply at once that the solutionu is constant on the characteristics that are determined
by
dy
dx = b(x, y)
a(x, y) .
(2.23)
For instance, when a = 1, b = √−x (see Example 3.7) we obtain that u is
constant along the lines 3
2 y + (−x)3/2 = constant.
Example 2.5 Solve the equation ux + uy + u = 1, subject to the initial condition
u = sin x, on y = x + x2, x > 0.
The characteristic equations and the associated initial conditions are given by
xt = 1, yt = 1, ut + u = 1,
(2.24)
x(0, s) = s, y(0, s) = s + s2, u(0, s) = sin s,
(2.25)
respectively. Let us compute ﬁrst the Jacobian along the initial curve:
J =

1
1
1
1 + 2s
 = 2s.
(2.26)
Thus we anticipate a unique solution at each point where s ̸= 0. Since we are limited
to the regime x > 0 we indeed expect a unique solution.
The parametric integral surface is given by
(x(t, s), y(t, s), u(t, s)) = (s + t, s + s2 + t, 1 −(1 −sin s)e−t).
In order to invert the mapping (x(t, s), y(t, s)), we substitute the equation for x into
the equation for y to obtain s = (y −x)1/2. The sign of the square root was selected
according to the condition x > 0. Now it is easy to ﬁnd t = x −(y −x)
1
2 , whence
the explicit representation of the integral surface
u(x, y) = 1 −[1 −sin(y −x)
1
2 ]e−x+(y−x)
1
2 .
Notice that the solution exists only in the domain
D = {(x, y) | 0 < x < y} ∪{(x, y) | x ≤0 and x + x2 < y},
and in particular it is not differentiable at the origin of the (x, y) plane. To see the
geometrical reason for this, consult Figure 2.3. We seethattheslopeofcharacteristic
passing through the origin equals 1, which is exactly the slope of the projection of

2.4 Examples of the characteristics method
33
char.
y
x
projection
of Γ
Figure 2.3 The characteristics and projection of  for Example 2.5.
the initial curve there. Namely, the transversality condition does not hold there (a
fact we already expected from our computation of the Jacobian above). Indeed the
violation of the transversality condition led to nonuniqueness of the solution near
the curve
{(x, y) | x < 0
and
y = x + x2},
which is manifested in the ambiguity of the sign of the square root.
Example 2.6 Solve the equation −yux + xuy = u subject to the initial condition
u(x, 0) = ψ(x).
The characteristic equations and the associated initial conditions are given by
xt = −y, yt = x, ut = u,
(2.27)
x(0, s) = s, y(0, s) = 0, u(0, s) = ψ(s).
(2.28)
Let us examine the transversality condition:
J =

0
s
1
0
 = −s.
(2.29)
Thus we expect a unique solution (at least locally) near each point on the initial
curve, except, perhaps, the point x = 0.
The solution of the characteristic equations is given by
(x(t, s), y(t, s), u(t, s))
= ( f1(s) cos t + f2(s) sin t, f1(s) sin t −f2(s) cos t, et f3(s)).
Substituting the initial condition into the solution above leads to the parametric
integral surface
(x(t, s), y(t, s), u(t, s)) = (s cos t, s sin t, etψ(s)).

34
First-order equations
y
x
projection of Γ
char.
Figure 2.4 The characteristics and projection of  for Example 2.6.
Isolating s and t we obtain the explicit representation
u(x, y)=ψ(

x2+y2) exp

arctan
 y
x

.
It can be readily veriﬁed that the characteristics form a one-parameter family of
circles around the origin (see Figure 2.4). Therefore, each one of them intersects
the projection of the initial curve (the x axis) twice. We also saw that the Jacobian
vanishes at the origin. So how is it that we seem to have obtained a unique solution?
The mystery is easily resolved by observing that in choosing the positive sign for
the square root in the argument of ψ, we effectively reduced the solution to the ray
{x > 0}. Indeed, in this region a characteristic intersects the projection of the initial
curve only once.
Example 2.7 Solve the equation ux + 3y2/3uy = 2 subject to the initial condition
u(x, 1) = 1 + x.
The characteristic equations and the associated initial conditions are given by
xt = 1, yt = 3y2/3, ut = 2,
(2.30)
x(0, s) = s, y(0, s) = 1, u(0, s) = 1 + s.
(2.31)
In this example we expect a unique solution in a neighborhood of the initial curve
since the transversality condition holds:
J =

1
3
1
0
 = −3 ̸= 0.
(2.32)
The parametric integral surface is given by
x(t, s) = s + t, y(t, s) = (t + 1)3, u(t, s) = 2t + 1 + s.
Before proceeding to compute an explicit solution, let us ﬁnd the characteristics. For
this purpose recall that each characteristic curve passes through a speciﬁc s value.
Therefore, we isolate t from the equation for x, and substitute it into the expression

2.4 Examples of the characteristics method
35
y
x
1
projection of Γ
char.
char.
char.
Figure 2.5 Self-intersection of characteristics.
for y. We obtain y = (x + 1 −s)3, and, thus, for each ﬁxed s this is an equation
for a characteristic. A number of characteristics and their intersection with the
projection of the initial curve y = 1 are sketched in Figure 2.5. While the picture
indicates no problems, we were not careful enough in solving the characteristic
equations, since the function y2/3 is not Lipschitz continuous at the origin. Thus the
characteristic equations might not have a unique solution there! In fact, it can be
easily veriﬁed that y = 0 is also a solution of yt = 3y2/3. But, as can be seen from
Figure 2.5, the well behaved characteristics near the projection of the initial curve
y = 1 intersect at some point the extra characteristic y = 0. Thus we can anticipate
irregular behavior near y = 0. Inverting the mapping (x(t, s), y(t, s)) we obtain
t = y1/3 −1, s = x + 1 −y1/3.
Hence the explicit solution to the PDE is u(x, y) = x + y1/3, which is indeed
singular on the x axis.
Example 2.8 Solve the equation (y + u)ux + yuy = x −y subject to the initial
conditions u(x, 1) = 1 + x.
This is an example of a quasilinear equation. The characteristic equations and the
initial data are:
(i) xt = y + u,
(ii) yt = y,
(iii) ut = x −y,
x(0, s) = s, y(0, s) = 1, u(0, s) = 1 + s.
Let us examine the transversality condition. Notice that while u is yet to be found,
the transversality condition only involves the values of u on the initial curve . It
is easy to verify that on  we have a = 2 + s, b = 1. It follows that the tangent
to the characteristic has a nonzero component in the direction of the y axis. Thus
it is nowhere tangent to the projection of the initial curve (the x axis, in this case).

36
First-order equations
Alternatively, we can compute the Jacobian directly:
J =

2 + s
1
1
0
 = −1 ̸= 0.
(2.33)
We conclude that there exists an integral surface at least at the vicinity of . From the
characteristic equation (ii) and the associated initial condition we ﬁnd y(t, s) = et.
Adding the characteristic equations (i) and (iii) we get (x + u)t = x + u. There-
fore, u + x = (1 + 2s)et. Returning to (i) we obtain x(t, s) = (1 + s)et −e−t
and u(t, s) = set + e−t. Observing that x −y = set −e−t, we ﬁnally get u =
2/y + (x −y). The solution is not global (it becomes singular on the x axis),
but it is well deﬁned near the initial curve.
2.5 The existence and uniqueness theorem
We shall summarize the discussion on linear and quasilinear equations into a general
theorem. For this purpose we need the following deﬁnition.
Deﬁnition 2.9 Consider a quasilinear equation (2.3) with initial conditions (2.16)
deﬁning an initial curve for the integral surface. We say that the equation and the
initial curve satisfy the transversality condition at a point s on , if the characteristic
emanating from the projection of (s) intersects the projection of  nontangentially,
i.e.
J |t=0 = xt(0, s)ys(0, s) −yt(0, s)xs(0, s) =

a
b
(x0)s
(y0)s
 ̸= 0.
Theorem 2.10 Assume that the coefﬁcients of the quasilinear equation (2.3) are
smooth functions of their variables in a neighborhood of the initial curve (2.16).
Assume further that the transversality condition holds at each point s in the interval
(s0 −2δ, s0 + 2δ) on the initial curve. Then the Cauchy problem (2.3), (2.16) has a
unique solution in the neighborhood (t, s) ∈(−ϵ, ϵ) × (s0 −δ, s0 + δ) of the initial
curve. If the transversality condition does not hold for an interval of s values, then
the Cauchy problem (2.3), (2.16) has either no solution at all, or it has inﬁnitely
many solutions.
Proof The existence and uniqueness theorem for ODEs, applied to (2.15) together
with the initial data (2.16), guarantees the existence of a unique characteristic
curve for each point on the initial curve. The family of characteristic curves forms
a parametric representation of a surface. The transversality condition implies that
the parametric representation provides a smooth surface. Let us verify now that the

2.5 The existence and uniqueness theorem
37
surface thus constructed indeed satisﬁes the PDE (2.3). We write
˜u = ˜u(x, y) = u(t(x, y), s(x, y)),
and compute
a ˜ux + b˜uy = a(uttx + ussx) + b(utty + ussy) = ut(atx + bty) + us(asx + bsy).
But the characteristic equations and the chain rule imply
1 = tt = atx + bty, 0 = st = asx + bsy.
Hence a ˜ux + b˜uy = ut = c, i.e. ˜u satisﬁes (2.3).
To show that there are no further integral surfaces, we prove that the characteristic
curves we constructed must lie on an integral surface. Since the characteristic curve
starts on the integral surface, we only have to show that it remains there. This is
intuitively clear, since the characteristic curve is, by deﬁnition, orthogonal at every
point to the surface normal. On the other hand, clearly for a curve starting on some
surface to leave the surface, its tangent must at some point have a nonzero projection
on the normal to the surface. This simple geometrical reasoning can be supported
through an explicit computation; for this purpose we write a given integral surface
in the form u = f (x, y). Let (x(t), y(t), u(t)) be a characteristic curve. We assume
u(0) = f (x(0), y(0)). Deﬁne the function
(t) = u(t) −f (x(t), y(t)).
Differentiating by t we write
t = ut −fx(x, y)xt −fy(x, y)yt.
Substituting the system (2.15) into the above equations for t we obtain
t = c(x, y,  + f ) −fx(x, y)a(x, y,  + f ) −fy(x, y)b(x, y,  + f ).
(2.34)
But the initial condition implies (0) = 0. It is easy to check (using (2.3)) that
(t) ≡0 solves the ODE (2.34). Since that equation has smooth coefﬁcients, it has
a unique solution. Thus  ≡0 is the only solution, and the curve (x(t), y(t), u(t))
indeed lies on the integral surface. Therefore the integral surface we constructed
earlier through the parametric representation inducedbythecharacteristicequations
is unique.
When the transversality condition does not hold along an interval of s values,
the characteristic there is the same as the projection of . If the solution of the
characteristic equation is a curve that is not identical to the initial curve, then
the tangent (vector) to the initial curve at some point cannot be at that point tan-
gential to any integral surface. In other words, the initial condition contradicts

38
First-order equations
the equation and thus there can be no solution to the Cauchy problem. If, on the
other hand, the characteristic curve agrees with the initial curve at that point, there
are inﬁnitely many ways to extend it into a compatible integral surface that contains
it. Therefore, in this case we have inﬁnitely many solutions to the Cauchy problem.
We now present a method for constructing this family of solutions. Select an arbi-
trary point P0 = (x0, y0, u0) on . Construct a new initial curve ′, passing through
P0, which is not tangent to  at P0. Solve the new Cauchy problem consisting of
(2.3) with ′ as initial curve. Since, by construction, the transversality condition
holds now, the ﬁrst part of the theorem guarantees a unique solution. Since there are
inﬁnitely many ways of selecting such an initial curve ′, we obtain inﬁnitely many
solutions.
The following simple example demonstrates the case where the transversality
condition fails along some interval.
Example 2.11 Consider the Cauchy problem
ux + uy = 1,
u(x, x) = x.
Show that it has inﬁnitely many solutions.
The transversality condition is violated identically. However the characteristic di-
rection is (1, 1, 1), and so is the direction of the initial curve. Hence, the initial
curve is itself a characteristic curve. Thus there exist inﬁnitely many solutions. To
ﬁnd these solutions, set the problem
ux + uy = 1,
u(x, 0) = f (x),
for an arbitrary f satisfying f (0) = 0. The solution is easily found to be u(x, y) =
y + f (x −y).
Notice that the Cauchy problem
ux + uy = 1,
u(x, x) = 1,
ontheotherhand,isnotsolvable.Toseethisobservethatthetransversalitycondition
fails again, but now the initial curve is not a characteristic curve. Thus there is no
solution.
Remark 2.12 There is one additional possibility not covered by Theorem 2.10.
This is the case where the transversality condition does not hold on isolated points
(as was indeed the case in some of the preceding examples). It is difﬁcult to
formulate universal statements here. Instead, each such case has to be analyzed
separately.

2.6 The Lagrange method
39
2.6 The Lagrange method
First-order quasilinear equations were in fact studied by Lagrange even before
Hamilton. Lagrange developed a solution method that is also geometric in nature,
albeit less general than Hamilton’s method. The main advantage of Lagrange’s
method is that it provides general solutions for the equation, regardless of the
initial data.
Let us reconsider (2.15). The set of all solutions to this system forms a two-
parameter set of curves. To justify this assertion, notice that since the system (2.15)
is autonomous, it is equivalent to the system
yx = b(x, y, u)/a(x, y, u), ux = c(x, y, u)/a(x, y, u).
(2.35)
Since (2.35) is a system of two ﬁrst-order ODEs in the (y, u) plane, where x is a
parameter, it follows that the set of solutions is determined by two initial conditions.
Lagrange assumed that the two-parameter set of solution curves for (2.15) can
be represented by the intersection of two families of integral surfaces
ψ(x, y, u) = α, φ(x, y, u) = β.
(2.36)
When we vary the parameters α and β we obtain (through intersecting the surfaces
ψ and φ) the two-parameter set of curves that are generated by the intersection.
Recall that a solution surface of (2.3) passing through an initial curve is obtained
from a one-parameter family of curves solving the characteristic equation (2.15).
Each such one-parameter subfamily describes a curve in the parameter space (α, β).
Since such a curve can be expressed in the form F(α, β) = 0, it follows that every
solution of (2.3) and (2.16) is given by
F(ψ(x, y, u), φ(x, y, u)) = 0.
(2.37)
Since the surfaces ψ and φ were determined by the equation itself, (2.37) deﬁnes
a general solution to the PDE. When we solve for a particular initial curve, we just
have to use this curve to determine the speciﬁc functional form of F associated
with that initial curve.
We are still left with one “little” problem: how to ﬁnd the surfaces ψ and φ.
In the theory of ODEs one solves ﬁrst-order equations by the method of inte-
gration factors. While this method is always feasible in theory, it involves great
technical difﬁculties. In fact, it is possible to ﬁnd integration factors only in special
cases. In a sense, the Lagrange method is a generalization of the integration fac-
tor method for ODEs, as we have to ﬁnd solution surfaces for the two ﬁrst-order
ODEs (2.35). Hence it is not surprising that the method is applicable only in special
cases.

40
First-order equations
We proceed by introducing a method for computing the surfaces ψ and φ, and
then apply the method to a speciﬁc example.
Example 2.13 Recall that by deﬁnition, the surfaces ψ = α, φ = β contain the
characteristic curves. Assume there exist two independent vector ﬁelds ⃗P1 =
(a1, b1, c1) and ⃗P2 = (a2, b2, c2) (i.e. they are nowhere tangent to each other) that
are both orthogonal to the vector ⃗P = (a, b, c) (the vector deﬁning the characteris-
tic equations). This means aa1 + bb1 + cc1 = 0 = aa2 + bb2 + cc2. Let us assume
further that the vector ﬁelds ⃗P1 and ⃗P2 are exact, i.e. ∇× ⃗P1 = 0 = ∇× ⃗P2. This
implies that there exist two potentials ψ and φ satisfying ∇ψ = ⃗P1 and ∇φ = ⃗P2.
By construction it follows that dψ = ∇ψ · ⃗P = 0 = ∇φ · ⃗P = dφ, namely, ψ and
φ are constant on every characteristic curve, and form the requested two-parameter
integral surfaces.
Let us apply this method to ﬁnd the general solution to the equation
−yux + xuy = 0.
(2.38)
The characteristic equations are
xt = −y, yt = x, ut = 0.
In this example ⃗P = (−y, x, 0). It is easy to guess orthogonal vector ﬁelds ⃗P1 =
(x, y, 0) and ⃗P2 = (0, 0, 1). The reader can verify that they are indeed exact vector
ﬁelds. The associated potentials are
ψ(x, y, u) = 1
2(x2 + y2), φ(x, y, u) = (0, 0, u).
Therefore, the general solution of (2.38) is given by
F(x2 + y2, u) = 0,
(2.39)
or
u = g(x2 + y2).
(2.40)
To ﬁnd the speciﬁc solution of (2.38) that satisﬁes a given initial condition, we
shall use that condition to eliminate g. For example, let us compute the solution
of (2.38) satisfying u(x, 0) = sin x for x > 0. Substituting the initial condition
into (2.40) yields g(ξ) = sin √ξ; hence u(x, y) = sin

x2 + y2 is the required
solution.
While the Lagrange method has an advantage over the characteristics method, since
it provides a general solution to the equation, valid for all initial conditions, it also
has a number of disadvantages.

2.7 Conservation laws and shock waves
41
(1) We have already explained that ψ and φ can only be found under special circumstances.
Many tricks for this purpose have been developed since Lagrange’s days, yet, only a
limited number of equations can be solved in this way.
(2) It is difﬁcult to deduce from the Lagrange method any potential problems arising from
the interaction between the equation and the initial data.
(3) The Lagrange method is limited to quasilinear equations. Its generalization to arbitrary
nonlinearities is very difﬁcult. On the other hand, as we shall soon see, the characteristics
method can be naturally extended to a method that is applicable to general nonlinear
PDEs.
It would be fair to say that the main value of the Lagrange method is historical,
and in supplying general solutions to certain canonical equations (such as in the
example above).
2.7 Conservation laws and shock waves
The existence theorem for quasilinear equations only guarantees (under suitable
conditions) the existence of a local solution. Nevertheless, there are cases of interest
where we need to compute the solution of a physical problem beyond the point
where the solution breaks down. In this section we shall discuss such a situation.
For simplicity we shall perform the analysis in some detail for a canonical prototype
of quasilinear equations given by
uy + uux = 0.
(2.41)
This equation plays an important role in hydrodynamics. It models the ﬂow of mass
with concentration u, where the speed of the ﬂow depends on the concentration. The
variable y has the physical interpretation of time. We shall show that the solutions
to this equation often develop a special singularity that is called a shock wave. In
hydrodynamics the equation is called the Euler equation (cf. Chapter 1; the reader
may be bafﬂed by now by the multitude of differential equations that are called
after Euler. We have to bear in mind that Euler was a highly proliﬁc mathematician
who published over 800 papers and books). Towards the end of the section we shall
generalize the analysis that is performed for (2.41) to a larger family of equations,
and in particular, we shall apply the theory to study trafﬁc ﬂow.
As a warm-up we start with the simple linear equation
uy + cux = 0.
(2.42)
The difference between this equation and (2.41), is that in (2.42) the ﬂow speed is
given by the positive constant c. The initial condition
u(x, 0) = h(x)
(2.43)

42
First-order equations
will be used for both equations. Solving the characteristic equations for the linear
equation (2.42) we get
(x, y, u) = (s + ct, t, h(s)).
Eliminating s and t yields the explicit solution u = h(x −cy). The solution im-
plies that the initial proﬁle does not change; it merely moves with speed c along
the positive x axis, namely, we have a ﬁxed wave, moving with a speed c while
preserving the initial shape.
Euler’s equation (2.41) is solved similarly. The characteristic equations are
xt = u, yt = 1, ut = 0,
and their solution is
(x, y, u) = (s + h(s)t, t, h(s)),
where we used the parameterization x(0, s) = s, y(0, s) = 0, u(0, s) = h(s) for
the initial data. Therefore, the solution of the PDE is
u = h(x −uy),
(2.44)
except that this time this solution is actuallyimplicit.Inordertoanalyzethissolution
further we eliminate the y variable from the equations for the characteristics (the
projection of the characteristic curves on the (x, y) plane):
x = s + h(s)y.
(2.45)
The third characteristic equation implies that for each ﬁxed s, i.e. along each char-
acteristic, u preserves its initial value u = h(s). The other characteristic equations
imply, then, that the characteristics are straight lines.
Since different characteristics have different slopes that are determined by the
initial values of u, they might intersect. Such an intersection has an obvious physical
interpretation that can be seen from (2.45): The initial data h(s) determine the speed
of the characteristic emanating from a given s. Therefore, if a characteristic leaving
the point s1 has a higher speed than a characteristic leaving the point s2, and if
s1 < s2, then after some (positive) time the faster characteristic will overtake the
slower one.
As we explained above, the solution is not well deﬁned at points where character-
istic curves intersect. To see the resulting difﬁculty from an algebraic perspective,
we differentiate the implicit solution with respect to x to get ux = h′(1 −yux),
implying
ux =
h′
1 + yh′ .
(2.46)

2.7 Conservation laws and shock waves
43
Recalling that physically the variable y stands for time, we consider the ray y > 0.
We conclude that the solution’s derivative blows up at the critical time
yc = −
1
h′(s).
(2.47)
Hence the classical solution is not deﬁned for y > yc. This conclusion is consistent
with the heuristic physical interpretation presented above. Indeed a necessary con-
dition for a singularity formation is that h′(s) < 0 at least at one point, such that a
faster characteristic will start from a point behind a slower characteristic. If h(s) is
never decreasing, there will be no singularity; however, such data are exceptional.
Observe that the solution becomes singular at the ﬁrst time y that satisﬁes (2.47);
such a value is achieved for the value s, where h′(s) is minimal.
Equation (2.41) arises in the investigation of a fundamental physical problem.
Thus we cannot end our analysis when a singularity forms. In other words, while
the solution becomes singular at the critical time (2.47), the ﬂuid described by the
equations keeps ﬂowing unaware of our mathematical troubles! Therefore we must
ﬁnd a means of extending the solution beyond yc.
Extending singular solutions is not a simple matter. There are several ways
to construct such extensions, and we must select a method that conforms with
fundamental physical principles. The basic idea is to deﬁne a new problem. This
new problem is formulated so as to be satisﬁed by each classical solution of the Euler
equation,andsuchthateachcontinuouslydifferentiablesolutionofthenewproblem
will satisfy the Euler equation. Yet, the new problem will also have nonsmooth
solutions. A solution of the new extended problem is called a weak solution, and
the new problem itself is called the weak formulation of the original PDE. We shall
see that sometimes there exist more than one weak solution, and this will require
upgrading the weak formulation to include a selection principle.
We choose to formulate the weak problem by replacing the differential equation
with an integral balance. In fact, we have already discussed in Chapter 1 the connec-
tion between an integral balance and the associated differential relation emerging
from it. We explained that the integral balance is more fundamental, and can only be
transformed into a differential relation when the functions involved are sufﬁciently
smooth. To apply the integral balance method we rewrite (2.41) in the form
∂yu + 1
2
∂
∂x (u2) = 0,
(2.48)
and integrate (with respect to x, and for a ﬁxed y) over an arbitrary interval [a, b]
to obtain
∂y
 b
a
u(ξ, y)dξ + 1
2[u2(b, y) −u2(a, y)] = 0.
(2.49)

44
First-order equations
It is clear that every solution of the PDE satisﬁes the integral relation (2.49) as
well. Also, since a and b are arbitrary, any function u ∈C1 that satisﬁes (2.49)
would also satisfy the PDE. Nevertheless, the integral balance is also well deﬁned
for functions not in C1; actually, (2.49) is even deﬁned for functions with ﬁnitely
many discontinuities.
We now demonstrate the construction of a weak solution that is a smooth function
(continuously differentiable) except for discontinuities along a curve x = γ (y).
Since the solution is smooth on both sides of γ , it satisﬁes the equation there. It
remains to compute γ . For this purpose we write the weak formulation in the form
∂y
 γ (y)
a
u(ξ, y)dξ +
 b
γ (y)
u(ξ, y)dξ

+ 1
2[(u2(b, y) −u2(a, y)] = 0.
Differentiating the integrals with respect to y and using the PDE itself leads to
γy(y)u−−γy(y)u+ −1
2
 γ (y)
a
(u2(ξ, y))ξdξ +
 b
γ (y)
(u2(ξ, y))ξdξ

+ 1
2[u2(b, y) −u2(a, y)]=0.
Here we used u−and u+ to denote the values of u when we approach the curve γ
from the left and from the right, respectively. Performing the integration we obtain
γy(y) = 1
2(u−+ u+),
(2.50)
namely, the curve γ moves at a speed that is the average of the speeds on the left
and right ends of it.
Example 2.14 Consider the Euler equation (2.41) with the initial conditions
u(x, 0) = h(x) =



1
x ≤0,
1 −x/α
0 < x < α,
0
x ≥α.
(2.51)
Since h(x) is not monotone increasing, the solution will develop a singularity
at some ﬁnite (positive) time. Formula (2.47) implies yc = α. For all y < α the
(smooth) solution is given by
u(x, y) =



1
x ≤y,
x −α
y −α
y < x < α,
0
x ≥α.
(2.52)
After the critical time yc when the solution becomes singular we need to deﬁne
a weak solution. We seek a solution with a single discontinuity. Formula (2.50)

2.7 Conservation laws and shock waves
45
u
x
1
a
u
x
1
a
u
x
1
a
u
x
1
a
t = 0
a  > t >0
t > a
t = a
Figure 2.6 Several snapshots in the development of a shock wave.
implies that the discontinuity moves with a speed 1
2. Therefore the following weak
solution is compatible with the integral balance even for y > α:
u(x, y) =

1
x < α + 1
2(y −α),
0
x > α + 1
2(y −α).
(2.53)
The solution thus constructed has the structure of a moving jump discontinuity. It
describes a step function moving at a constant speed. Such a solution is called a
shock wave. Several snapshots of the formation and propagation of a shock wave
are depicted in Figure 2.6.
Strictly speaking, the solution is not continuously differentiable even at time
y = 0; however, this is a minor complication, since it can be shown that the formula
for the classical solution is valid even when the derivative of the initial data has
ﬁnitely many discontinuities as long as it is bounded.
Example 2.15 We now consider the opposite case where the initial data are in-
creasing:
u(x, 0) =



0
x ≤0,
x/α
0 < x < α,
1
x ≥α.
(2.54)
Since this time h′ ≥0, there is no critical (positive) time where the characteristics
intersect. On the contrary, the characteristics diverge. This situation is called an
expansion wave, in contrast to the wave in the previous example which is called a

46
First-order equations
compression wave. We use the classical solution formula to obtain
u(x, y) =



0
x ≤0,
x
α + y
0 < x < α + y,
1
x ≥α + y.
(2.55)
It is useful to consider for both examples the limiting case where α →0. In Example
2.14 the initial data are the same as the shock weak solution (2.53), and therefore this
solution is already valid at y = 0. In contrast, in Example 2.15 the characteristics
expand, the singularity is smoothed out at once, and the solution is
u(x, y) =



0
x ≤0,
x
y
0 < x < y,
1
x ≥y.
(2.56)
We notice, though, that we could in principle write in the expansion wave case
a weak solution that has a shock wave structure:
u(x, y) =

0
x < α + 1
2(y −α),
1
x > α + 1
2(y −α).
(2.57)
We see that the weak formulation by itself does not have a unique solution! Ad-
ditional arguments are needed to pick out the correct solution among the several
options. In the case we consider here it is intuitively clear that the shock solution
(2.57) is not adequate, since slower characteristics starting from the ray x < 0 can-
not overtake the faster characteristics that start from the ray x > 0. Another, more
physical, approach to this problem will be described now.
The theory of weak solutions and shock waves is quite difﬁcult. We therefore
present essentially the same ideas we developed above from a somewhat different
perspective. Instead of looking at the speciﬁc canonical equation (2.48) with general
initial conditions, we look at a more general PDE with canonical initial conditions.
Speciﬁcally we consider the following ﬁrst-order quasilinear PDE
uy + ∂
∂x F(u) = 0.
(2.58)
Equations of this kind are called conservation laws. To understand the name, let
us recall the derivation of the heat equation in Section 1.4.1. The energy balance
(1.8) is actually of the form (2.58), where F denotes ﬂux. In the canonical example
(2.48), the ﬂux is F = 1
2u2, and u is typically interpreted as mass density.
Equation (2.58) is supplemented with the initial condition
u(x, 0) =
u−
x < 0,
u+
x > 0.
(2.59)

2.7 Conservation laws and shock waves
47
To write the weak formulation for (2.58), we assume that the solution takes the
shape of a shock wave of the form
u(x, y) =
u−
x < γ (y),
u+
x > γ (y).
(2.60)
It remains therefore to ﬁnd the shock orbit x = γ (y). We ﬁnd γ by integrating
(2.58) with respect to x along the interval (x1, x2), with x1 < γ, x2 > γ . Taking
(2.60) into account, we get
∂
∂y

[x2 −γ (y)]u+ + [γ (y) −x1]u−
= F(u+) −F(u−).
(2.61)
The last equation implies
γy(y) = F(u+) −F(u−)
u+ −u−
:= [F]
[u] ,
(2.62)
where we used the notation [·] to denote the change (jump) of a quantity across the
shock.
Conservation laws appear in many areas of continuum mechanics (including
hydrodynamics, gas dynamics, combustion, etc.), where the jump equation (2.62)
is called the Rankine–Hugoniot condition.
Notice that in the special case of F = 1
2u2 that we considered earlier, the rule
(2.62) reduces to (2.50). We also point out that we integrated (2.58) along an
arbitrary ﬁnite interval, although the values of x1 and x2 do not appear in the ﬁnal
conclusion. The reason for introducing this artiﬁcial interval is that the integral
of (2.58) over the real line is not bounded. Since we interpret u as a density of a
physical quantity (such as mass), our model is really artiﬁcial, and a realistic model
will have to take into account the effects of ﬁnite boundaries.
Our analysis of the general conservation law (2.58) is not yet complete. From our
study of Example 2.15 we expect that shock would only occur if the characteristics
collide. In the case of general conservation laws, this condition is expressed as:
The entropy condition Characteristics must enter the shock curve, and are not
allowed to emanate from it.
The motivation for the entropy condition is rooted in gas dynamics and the
second law of thermodynamics. In order not to stray too much away from the
theory of PDEs, we shall give a heuristic reasoning, based on the interpretation of
entropy as minus the amount of “information” stored in a given physical system.
We thus phrase the second law of thermodynamics as stating that in a closed system
information is only lost as time y increases, and cannot be created. Now, we have
shownthatcharacteristicscarrywiththeminformationonthesolutionof a ﬁrst-order

48
First-order equations
PDE. Therefore the emergence of a characteristic from a shock is interpreted as a
creation of information which should be forbidden. To give the entropy condition an
algebraic form, we write the conservation law (2.58) as uy + Fuux = 0. Therefore
the characteristic speed is given by Fu, and the entropy condition can be expressed as
Fu(u−) > γy > Fu(u+).
(2.63)
Applying this rule to the special case F(u) = 1
2u2 and using (2.62) we obtain that
the shock solution is valid only if u−> u+, a conclusion we reached earlier from
different considerations.
The theory of conservation laws has a nice application to the real-world problem
of trafﬁc ﬂow. We therefore end this section by a qualitative analysis of this problem.
Consider the ﬂow of cars along one direction in a road. Although cars are discrete
entities, we model them as a continuum, and denote by u(x, y) the car density at a
point x and time y.
A great deal of research has been devoted to the question of how to model the
ﬂux term F(u). Clearly the ﬂux is very low in bumper-to-bumper trafﬁc, where
each car barely moves. It may be surprising at ﬁrst sight, but the ﬂux is also low
when the trafﬁc is very light. In this case drivers tend to drive fast, and maintain a
large distance between each other (at least this is what they ought to do when they
drive fast. . . ). Therefore the ﬂux, which counts the total number of cars crossing a
given point per time, is low. If we assume that a car occupies on average a length
of 5 m, then the highest density is ub = 200 cars/km. It was found experimentally
that the maximal ﬂux is about Fmax = 1500 cars/hour, and is achieved at a speed of
about 35 km/hour (20 miles/hour) (see [22] for a detailed discussion of trafﬁc ﬂow
in the current context). Therefore the optimal density (if one wants to maximize the
ﬂux) is umax ∼43 cars/km. The concave shape of F(u) is depicted in Figure 2.7.
Let us look at some practical implications of the model. Suppose that at time y =
0thereisatrafﬁcjamatsomepoint x = xj.Thiscouldbecausedbyanaccident,ared
trafﬁc light, a policeman directing the trafﬁc, etc. Assume further that there is a line
F
u
1500
200
43
Figure 2.7 The trafﬁc ﬂux F as a function of the density u.

2.7 Conservation laws and shock waves
49
u(x,0)
x
200
xs
xj
Figure 2.8 The car density at a red trafﬁc light.
u
x
L
Figure 2.9 Trafﬁc ﬂow through a sequence of trafﬁc lights.
ofstationarycarsextendingfrom xs to xj (with xj > xs).Carsapproachthetrafﬁcjam
from x < xs. At some point the drivers slow down, as they reach the regime where
the car density is greater than umax. Therefore the density u just before xs is as shown
schematically in Figure 2.8. The Rankine–Hugoniot condition (2.62) implies that
the shock speed is negative. Although the curve u(x, 0) is increasing, the derivative
Fu is now negative, and therefore the entropy condition holds. We conclude that a
shock wave will propagate from xs backwards. Indeed as drivers approach a trafﬁc
jam there is a stage when they enter the shock and have to decelerate rapidly. The
opposite occurs as we leave the point xj. The density is decreasing and the entropy
condition is violated. Therefore we have an expansion wave.
Our analysis can be applied to the design of trafﬁc lights timing. Assume there
are several consecutive trafﬁc lights, separated by a distance L (see Figure 2.9).
When the trafﬁc approaches one of the trafﬁc lights, a shock propagates backward.
To estimate the speed of the shock we assume that behind the shock the density is
optimal and so the ﬂux is Fmax. Then γy = −Fmax/(ub −umax). Therefore the time
it takes the shock to reach the previous trafﬁc light is
Ts = L(ub −umax)
Fmax
.

50
First-order equations
If the red light is maintained over a period exceeding Ts, the high density proﬁle
will extend throughout the road, and trafﬁc will come to a complete stop.
2.8 The eikonal equation
Before proceeding to the general nonlinear case, let us analyze in detail the special
case of the eikonal equation (see Chapter 1). We shall see that this equation can
also be solved by characteristics. The two-dimensional eikonal equation takes the
form
u2
x + u2
y = n2,
(2.64)
where the surfaces u = c (where c is some constant) are the wavefronts, and n is
the refraction index of the medium. The initial conditions are given in the form of
an initial curve .
To write the characteristic equations, notice that the eikonal equation can be
expressed as
(ux, uy, n2) · (ux, uy, −1) = 0.
Thus the vector (ux, uy, n2) describes a direction tangent to the solution (integral)
surface. To verify this argument algebraically, write equations for the x and y com-
ponents of the characteristic curve, and check that the equation for the u component
is consistent with (2.64).
We thus set
dx
dt = ux,
dy
dt = uy,
du
dt = n2.
(2.65)
Since ux and uy are unknown at this stage, we compute
d2x
dt2 = d
dt (ux) = uxx
dx
dt + uxy
dy
dt = uxxux + uxyuy
= 1
2(u2
x + u2
y)x = 1
2(n2(x, y))x,
(2.66)
and similarly
d2y
dt2 = 1
2(n2(x, y))y.
(2.67)
Towritethesolutionoftheeikonalequation,noticethatitfollowsfromthedeﬁnition
of the characteristic curves that
du
dt = ux
dx
dt + uy
dy
dt = u2
x + u2
y = n2.
(2.68)

2.8 The eikonal equation
51
Integrating the last equation leads to a formula that determines u at the point
(x(t), y(t)) in terms of the initial value of u and the values of the refraction index
along the integration path:
u(x(t), y(t)) = u(x(0), y(0)) +
 t
0
n2(x(τ), y(τ))dτ,
(2.69)
where (x(t), y(t)) is a solution of (2.66) and (2.67).
Before solving speciﬁc examples, we should clarify an important point regarding
the initial conditions for the characteristic equations. Since the original equation
(2.65) involves the derivatives of u that are not known at this stage, we eliminated
these derivatives by differentiating the characteristic equations once more with re-
spect to the parameter t. Indeed the equations we obtained ((2.66) and (2.67)) no
longer depend on u itself; however these are second-order equations! Therefore, it is
not enough to provide a single initial condition (such as the initial point of the char-
acteristic curve on the initial curve ), but, rather, we must provide the derivatives
(xt, yt) too. Equivalently, we should provide the vector tangent to the characteristic
at the initial point. For this purpose we shall use the fact that the required vector
is precisely the gradient (ux, uy) of u. From the eikonal equation itself we know
that the size of that vector is n(x, y), and from the initial condition we can ﬁnd its
projection at each point of  in the direction tangent to . But obviously the size
of a planar vector and its projection along a given direction determine the vector
uniquely. Hence we obtain the additional initial condition.
Example 2.16 Solve the eikonal equation (2.64) for a medium with a constant
refraction index n = n0, and initial condition u(x, 2x) = 1.
The physical meaning of the initial condition is that the wavefront is a straight line.
The characteristic equations are d2x/dt2 = 0 = d2y/dt2. Thus, the characteristics
are straight lines, emanating from the initial line y = 2x. Since u is constant on
such a line, the gradient of u is orthogonal to it. Hence the second initial condition
for the characteristic is
dx
dt (0) =
2
√
5
n0,
dy
dt (0) = −1
√
5
n0.
We thus obtain:
x(t, s) =
2
√
5
n0t + x0(s), y(t, s) = −1
√
5
n0t + y0(s), u(t, s) = n2
0t + u0(s).
(2.70)

52
First-order equations
In order to ﬁnd x0(s) and y0(s), we write the initial curve parameterically as
(s, 2s, 1). Substituting the initial curve into (2.70) leads to the integral surface
(x, y, u) =
	 2
√
5
n0t + s, −1
√
5
n0t + 2s, n2
0t + 1

.
(2.71)
Eliminating t = (2x −y) /
√
5n0, we obtain the explicit solution
u(x, y) = 1 + n0
√
5
(2x −y).
The solution we have obtained has a simple physical interpretation: in a homoge-
neous medium the characteristic curves are straight lines (classical light rays), and
an initial planar wavefront propagates in the direction orthogonal to them. Therefore
all wavefronts are planar.
Example 2.17 Compute the function u(x, y) satisfying the eikonal equation u2
x +
u2
y = n2 and the initial condition u(x, 1) = n
√
1 + x2 (n is a constant parameter).
Write the initial conditions parametrically in the form (x, y, u) = (s, 1, n
√
1 + s2).
This condition implies xt(0, s) = ux = ns/
√
1 + s2. Substituting the last expres-
sion into the eikonal equation gives yt(0, s) = uy = n/
√
1 + s2. Integrating the
characteristic equations we obtain
(x(t, s), y(t, s), u(t, s)) =
	
ns
√
1 + s2 t + s,
n
√
1 + s2 t + 1, n(nt +

1 + s2)

.
In order to write an explicit solution, observe the identity
x2 + y2 = (nt +

1 + s2)2
satisﬁed by the integral surface. Therefore, the solution is u = n

x2 + y2. This
solution represents a spherical wave starting from a single point at the origin of
coordinates.
2.9 General nonlinear equations
The general ﬁrst-order nonlinear equation takes the form
F(x, y, u, ux, uy) = 0.
(2.72)
We shall develop a solution method for such equations. The method is an extension
of the method of characteristics. To simplify the presentation we shall use the
notation p = ux, q = uy.

2.9 General nonlinear equations
53
Consider a point (x0, y0, u0) on the integral surface. We want to ﬁnd the slope
of a (characteristic) curve on the integral surface passing through this point. In the
quasilinear case the equation determined directly the slope of a speciﬁc curve on the
integral surface. We shall now construct that curve in a somewhat different manner.
Let us write for this purpose the equation of the tangent plane to the integral surface
through (x0, y0, u0):
p(x −x0) + q(y −y0) −(u −u0) = 0.
(2.73)
Notice that the derivatives p and q at (x0, y0, u0) are not independent. Equation
(2.72) imposes the relation
F(x0, y0, u0, p, q) = 0.
(2.74)
The last two equations deﬁne a one-parameter family of tangent planes. Such a
family spreads out a cone. To honor the French geometer Gaspard Monge (1746–
1818) this cone is called the Monge cone. The natural candidate for the curve
we seek in the tangent plane (deﬁning for us the direction of the characteristic
curve) is, therefore, the generator of the Monge cone. To compute the generator we
differentiate (2.73) by the parameter p:
(x −x0) + dq
dp(y −y0) = 0.
(2.75)
Assume that F is not degenerate, i.e. F2
p + F2
q never vanishes. Without loss of
generality assume Fq ̸= 0. Then it follows from (2.74) and the implicit function
theorem that
q′(p) = −Fp
Fq
.
(2.76)
Substituting (2.76) into (2.75) we obtain the equation for the Monge cone generator:
x −x0
Fp
= y −y0
Fq
.
(2.77)
Equations (2.73) and (2.77) imply three differential equations for the characteristic
curves:
xt = Fp(x, y, u, p, q),
yt = Fq(x, y, u, p, q),
(2.78)
ut = pFp(x, y, u, p, q) + qFq(x, y, u, p, q).
It is easy to verify that equations (2.78) coincide in the quasilinear case with
the characteristic equations (2.15). However in the fully nonlinear case the char-
acteristic equations (2.78) do not form a closed system. They contain the hitherto

54
First-order equations
unknown functions p and q. In other words, the characteristic curves carry with
them a tangent plane that has to be found as part of the solution. To derive equations
for p and q we write
pt = uxxxt + uxyyt = uxx Fp + uxy Fq,
(2.79)
and similarly
qt = uyx Fp + uyy Fq.
(2.80)
In order to eliminate uxx, uxy and uyy, recall that the equation F = 0 holds along
the characteristic curves. We therefore obtain upon differentiation
Fx + pFu + px Fp + qx Fq = 0,
(2.81)
Fy + qFu + py Fp + qy Fq = 0.
(2.82)
Substituting (2.81) and (2.82) into (2.79) and (2.80) leads to
pt = −Fx −pFu,
qt = −Fy −qFu.
To summarize we write the entire set of characteristic equations:
xt = Fp(x, y, u, p, q),
yt = Fq(x, y, u, p, q),
ut = pFp(x, y, u, p, q) + qFq(x, y, u, p, q),
(2.83)
pt = −Fx(x, y, u, p, q) −pFu(x, y, u, p, q),
qt = −Fy(x, y, u, p, q) −qFu(x, y, u, p, q).
A simple computation of Ft, using (2.83), indeed veriﬁes that the PDE holds at all
points along a characteristic curve. The main addition to the theory we presented
earlier for the quasilinear case is that the characteristic curves have been replaced
by more complex geometric structures. Since each characteristic curve now drags
with it a tangent plane, we call these structures characteristic strips, and equations
(2.83) are called the strip equations.
We are now ready to formulate the general Cauchy problem for ﬁrst-order PDEs.
Consider (2.72) with the initial condition given by an initial curve  ∈C1:
x = x0(s), y = y0(s), u = u0(s).
(2.84)
We shall show in the proof of the next theorem how to derive initial conditions
also for p and q in order to obtain a complete initial value problem for the system
(2.83). We do not expect every Cauchy problem to be solvable. Clearly some form

2.9 General nonlinear equations
55
of transversality condition must be imposed. It turns out, however, that slightly
more than that is required:
Deﬁnition 2.18 Let a point P0 =(x0(s0), y0(s0), u0(s0), p0(s0), q0(s0)) satisfy the
compatibility conditions
F(P0) = 0,
u′
0(s0) = p0(s0)x′
0(s0) + q0(s0)y′
0(s0).
(2.85)
If, in addition,
x′
0(s0)Fq(P0) −y′
0(s0)Fp(P0) ̸= 0
(2.86)
is satisﬁed, then we say that the Cauchy problem (2.72), (2.84) satisﬁes the gener-
alized transversality condition at the point P0.
Theorem 2.19 Consider the Cauchy problem (2.72), (2.84). Assume that the gen-
eralized transversality condition (2.85)–(2.86) holds at P0. Then there exists ε > 0
and a unique solution (x(t, s), y(t, s), u(t, s), p(t, s), q(t, s)) for the Cauchy prob-
lem which is deﬁned for |s −so| + |t| < ε. Moreover, the parametric representation
deﬁnes a smooth integral surface u = u(x, y).
Proof We start by deriving full initial conditions for the system (2.83). Actually
the Cauchy problem has already provided three conditions for (x, y, u):
x(0, s) = x0(s), y(0, s) = y0(s), u(0, s) = u0(s).
(2.87)
We are left with the task of ﬁnding initial conditions
p(0, s) = p0(s), q(0, s) = q0(s),
(2.88)
for p(t, s) and q(t, s). Clearly p0(s) and q0(s) must satisfy at every point s the
differential condition
u′
0(s) = p0(s)x′
0(s) + q0(s)y′
0(s),
(2.89)
and the equation itself
F(x0(s), y0(s), u0(s), p0(s), q0(s)) = 0.
(2.90)
However (2.85) guarantees that these two requirements indeed hold at s = s0. The
transversality condition (2.86) ensures that the Jacobian of the system (2.89)–(2.90)
(with respect to the variables p0 and q0) does not vanish at s0. Therefore, the implicit
function theorem implies that one can derive from (2.89)–(2.90) the required initial
conditions for p0 and q0. Hence the characteristic equations have a full set of initial
conditions in a neighborhood |s −s0| < δ. Since the system of ODEs (2.83) is

56
First-order equations
well-posed, the existence of a unique smooth solution
(x(t, s), y(t, s), u(t, s), p(t, s), q(t, s))
is guaranteed for (t, s) in a neighborhood of (0, s0). As in the quasilinear case, one
can verify that (2.83) and (2.90) imply that
F(x(t, s), y(t, s), u(t, s), p(t, s), q(t, s)) = 0
∀|s −s0| < δ, |t| < ε.
(2.91)
In order that the parametric representation for (x, y, u) will deﬁne a smooth sur-
face u(x, y), we must show that the mapping (x(t, s), y(t, s)) can be inverted
to a smooth mapping (t(x, y), s(x, y)). Such an inversion exists if the Jacobian
J = ∂(x, y)/∂(t, s) does not vanish. But the characteristic equations imply
J |(0,s0) = ∂(x, y)
∂(s, t)

(0,s0)
= xs(0, s0)yt(0, s0) −xt(0, s0)ys(0, s0) ̸= 0,
(2.92)
where the last inequality follows from the transversality condition (2.86).
We have thus constructed a smooth function u(x, y). Does it satisfy the Cauchy
problem for the nonlinear PDE? This requires that the relations
ut(t, s) = p(t, s)xt(t, s) + q(t, s)yt(t, s)
(2.93)
and
us(t, s) = p(t, s)xs(t, s) + q(t, s)ys(t, s)
(2.94)
would hold. Condition (2.93) is clearly valid since the characteristic equations were,
in fact, constructed to satisfy it. The compatibility condition (2.94) holds on the
initial curve, i.e. at t = 0. It remains, though, to check this condition also for values
of t other than zero. We therefore deﬁne the auxiliary function
R(t, s) = us −pxs −qys.
We have to show that R(t, s) = 0. As we have already argued, the initial data for p
and q imply
R(0, s) = 0.
To check that R also vanishes for other values of t we compute
Rt = ust −ptxs −pxst −qt ys −qyst
= ∂
∂s (ut −pxt −qyt) + psxt + qs yt −ptxs −qt ys.

2.9 General nonlinear equations
57
Using (2.93) and the characteristic equations we get
Rt = ps Fp + qs Fq + xs(Fx + pFu) + ys(Fy + qFu)
= Fu(pxs + qys) + ps Fp + qs Fq + xs Fx + ys Fy.
Adding and subtracting Fuus to the last expression we ﬁnd
Rt = Fs −Fu R = −Fu R,
where we used the fact that Fs = 0 which follows from (2.91). Since the ini-
tial condition for the linear homogeneous ODE Rt = −Fu R is homogeneous too
(R(0, s) = 0), it follows that R(t, s) ≡0.
To demonstrate the method we just developed, let us solve again the Cauchy
problem from Example 2.16.
Example 2.20 We write the eikonal equation in the form
F(x, y, u, p, q) = p2 + q2 −n2
0 = 0.
(2.95)
Hence the characteristic equations are
xt = 2p,
yt = 2q,
ut = 2n2
0,
pt = 0,
qt = 0.
(2.96)
The initial conditions for (x, y, u) are given by
x(0, s) = s, y(0, s) = 2s, u(0, s) = 1.
(2.97)
We use (2.89)–(2.90) to derive the initial data for p and q:
p(0, s) + 2q(0, s) = 0,
p2(0, s) + q2(0, s) −n2
0 = 0.
Solving these equation we obtain
p(0, s) = 2n0
√
5
, q(0, s) = −n0
√
5
.
(2.98)
It is an easy matter to solve the full characteristic equations:
(x, y, u, p, q) =
	
s + 2n0
√
5
t, 2s −n0
√
5
t, 1 + 2n2
0t, 2n0
√
5
, −n0
√
5

,
(2.99)

58
First-order equations
After eliminating t we ﬁnally obtain
u(x, y) = 1 + n0
√
5
(2x −y).
Notice that the parametric representation obtained in the current example is different
from the one we derived in Example 2.16, since the parameter t we used here is
half the parameter used in Example 2.16.
2.10 Exercises
2.1 Consider the equation ux + uy = 1, with the initial condition u(x, 0) = f (x).
(a) What are the projections of the characteristic curves on the (x, y) plane?
(b) Solve the equation.
2.2 Solve the equation xux + (x + y)uy = 1 with the initial conditions u(1, y) = y. Is the
solution deﬁned everywhere?
2.3 Let p be a real number. Consider the PDEs
xux + yuy = pu
−∞< x < ∞,
−∞< y < ∞.
(a) Find the characteristic curves for the equations.
(b) Let p = 4. Find an explicit solution that satisﬁes u = 1 on the circle x2 + y2 = 1.
(c) Let p = 2. Find two solutions that satisfy u(x, 0) = x2, for every x > 0.
(d) Explain why the result in (c) does not contradict the existence–uniqueness
theorem.
2.4 Consider the equation yux −xuy = 0 (y > 0). Check for each of the following initial
conditions whether the problem is solvable. If it is solvable, ﬁnd a solution. If it is not,
explain why.
(a) u(x, 0) = x2.
(b) u(x, 0) = x.
(c) u(x, 0) = x, x > 0.
2.5 Let u(x, y) be an integral surface of the equation
a(x, y)ux + b(x, y)uy + u = 0,
where a(x, y) and b(x, y) are positive differentiable functions in the entire plane.
Deﬁne
D = {(x, y), |x| < 1, |y| < 1}.
(a) Prove that the projection on the (x, y) plane of each characteristic curve passing
through a point in D intersects the boundary of D at exactly two points.
(b) Show that if u is positive on the boundary of D, then it is positive at every point
in D.

2.10 Exercises
59
(c) Suppose that u attains a local minimum (maximum) at a point (x0, y0) ∈D. Eval-
uate u(x0, y0).
(d) Denote by m the minimal value of u on the boundary of D. Assume m > 0. Show
that u(x, y) ≥m for all (x, y) ∈D.
Remark This is an atypical example of a ﬁrst-order PDE for which a maximum
principle holds true. Maximum principles are important tools in the study of PDEs, and
they are valid typically for second-order elliptic and parabolic PDEs (see Chapter 7).
2.6 The equation xux + (x2 + y)uy + (y/x −x)u = 1 is given along with the initial con-
dition u(1, y) = 0.
(a) Solve the problem for x > 0. Compute u(3, 6).
(b) Is the solution deﬁned for the entire ray x > 0?
2.7 Solve the Cauchy problem ux + uy = u2, u(x, 0) = 1.
2.8 (a) Solve the equation xuux + yuuy = u2 −1 for the ray x > 0 under the initial
condition u(x, x2) = x3.
(b) Is there a unique solution for the Cauchy problem over the entire real line −∞<
x < ∞?
2.9 Consider the equation
uux + uy = −1
2u.
(a) Show that there is a unique integral surface in a neighborhood of the curve
1 = {(s, 0, sin s) | −∞< s < ∞}.
(b) Find the parametric representation x = x(t, s), y = y(t, s), u = u(t, s) of the in-
tegral surface S for initial condition of part (a).
(c) Find an integral surface S1 of the same PDE passing through the initial curve
1 = {(s, s, 0) | −∞< s < ∞}.
(d) Find a parametric representation of the intersection curves of the surfaces S and S1.
Hint Try to characterize that curve relative to the PDE.
2.10 A river is deﬁned by the domain
D = {(x, y)|
|y| < 1, −∞< x < ∞}.
A factory spills a contaminant into the river. The contaminant is further spread and
convected by the ﬂow in the river. The velocity ﬁeld of the ﬂuid in the river is only in
the x direction. The concentration of the contaminant at a point (x, y) in the river and
at time τ is denoted by u(x, y, τ). Conservation of matter and momentum implies
that u satisﬁes the ﬁrst-order PDE
uτ −(y2 −1)ux = 0.
The initial condition is u(x, y, 0) = eye−x2.

60
First-order equations
(a) Find the concentration u for all (x, y, τ).
(b) A ﬁsh lives near the point (x, y) = (2, 0) at the river. The ﬁsh can tolerate
contaminant concentration levels up to 0.5. If the concentration exceeds this level,
the ﬁsh will die at once. Will the ﬁsh survive? If yes, explain why. If no, ﬁnd the time
in which the ﬁsh will die.
Hint Notice that y appears in the PDE just as a parameter.
2.11 Solve the equation (y2 + u)ux + yuy = 0 in the domain y > 0, under the initial
condition u = 0 on the planar curve x = y2/2.
2.12 Solve the equation uy + u2ux = 0 in the ray x > 0 under the initial condition
u(x, 0) = √x. What is the domain of existence of the solution?
2.13 Consider the equation uux + xuy = 1, with the initial condition
 1
2s2 + 1, 1
6s3 + s, s

.
Find a solution. Are there other solutions? If not, explain why; if there are further
solutions, ﬁnd at least two of them, and explain the lack of uniqueness.
2.14 Consider the equation xux + yuy = 1/cos u.
(a) Find a solution to the equation that satisﬁes the condition u(s2, sin s) = 0 (you
can write down the solution in the implicit form F(x, y, u) = 0).
(b) Find some domain of s values for which there exists a unique solution.
2.15 (a) Find a function u(x, y) that solves the Cauchy problem
(x + y2)ux + yuy +
	x
y −y

u = 1 ,
u(x, 1) = 0
x ∈R.
(b) Check whether the transversality condition holds.
(c) Draw the projections on the (x, y) plane of the initial condition and the
characteristic curves emanating from the points (2, 1, 0) and (0, 1, 0).
(d) Is the solution you obtained in (a) deﬁned at the origin (x, y) = (0, 0)? Explain
your answer in light of the existence–uniqueness theorem.
2.16 Solve the Cauchy problem
xux + yuy = −u, u(cos s, sin s) = 1 0 ≤s ≤π.
Is the solution deﬁned everywhere?
2.17 Consider the equation
xux + uy = 1.
(a) Find a characteristic curve passing through the point (1, 1, 1).
(b) Show that there exists a unique integral surface u(x, y) satisfying u(x, 0) = sin x.
(c) Is the solution deﬁned for all x and y?
2.18 Consider the equation uux + uy = −1
2u.
(a) Find a solution satisfying u(x, 2x) = x2.
(b) Is the solution unique?
2.19 (a) Find a function u(x, y) that solves the Cauchy problem
x2ux + y2uy = u2,
u(x, 2x) = x2
x ∈R.
(b) Check whether the transversality condition holds.

2.10 Exercises
61
(c) Draw the projections on the (x, y) plane of the initial curve and the characteristic
curves that start at the points (1, 2, 1) and (0, 0, 0).
(d) Is the solution you found in part (a) deﬁned for all x and y?
2.20 Consider the equation
yux −uuy = x.
(a) Write a parametric representation of the characteristic curves.
(b) Solve the Cauchy problem
yux −uuy = x,
u(s, s) = −2s
−∞< s < ∞.
(c) Is the following Cauchy problem solvable:
yux −uuy = x,
u(s, s) = s
−∞< s < ∞?
(d) Set
w1 = x + y + u,
w2 = x2 + y2 + u2,
w3 = xy + xu + yu.
Show that w1(w2 −w3) is constant along each characteristic curve.
2.21 (a) Find a function u(x, y) that solves the Cauchy problem
xux −yuy = u + xy ,
u(x, x) = x2
1 ≤x ≤2.
(b) Check whether the transversality condition holds.
(c) Draw the projections on the (x, y) plane of the initial curve and the characteristic
curves emanating from the points (1, 1, 1) and (2, 2, 4).
(d) Is the solution you found in (a) well deﬁned in the entire plane?
2.22 Solve the Cauchy problem u2
x + uy = 0, u(x, 0) = x.
2.23 Let u(x, t) be the solution to the Cauchy problem
ut + cux + u2 = 0,
u(x, 0) = x,
where c is a constant, t denotes time, and x denotes a space coordinate.
(a) Solve the problem.
(b) A person leaves the point x0 at time t = 0, and moves in the positive x direction
with a velocity c (i.e. the quantity x −ct is ﬁxed for him). Show that if x0 > 0, then
the solution as seen by the person approaches zero as t →∞.
(c) What will be observed by such a person if x0 < 0, or if x0 = 0?
2.24 (a) Solve the problem
xux −uuy = y,
u(1, y) = y
−∞< y < ∞.
(b) Is the solution unique? What is the maximal domain where it is deﬁned?

62
First-order equations
2.25 Find at least ﬁve solutions for the Cauchy problem
ux + uy = 1,
u(x, x) = x.
2.26 (a) Solve the problem
xuy −yux + u = 0,
u(x, 0) = 1
x > 0.
(b) Is the solution unique? What is the maximal domain where it is deﬁned?
2.27 (a) Use the Lagrange method to ﬁnd a function u(x, y) that solves the problem
uux + uy = 1
(2.100)
u(3x, 0) = −x
−∞< x < ∞.
(2.101)
(b) Show that the curve {(3x, 2, 4 −3x)| −∞< x < ∞} is contained in the solution
surface u(x, y).
(c) Solve
uux + uy = 1
u(3x, 2) = 4 −3x
−∞< x < ∞.
2.28 Analyze the following problems using the Lagrange method. For each problem
determine whether there exists a unique solution, inﬁnitely many solutions or no
solution at all. If there is a unique solution, ﬁnd it; if there are inﬁnitely many
solutions, ﬁnd at least two of them. Present all solutions explicitly.
(a)
xuux + yuuy = x2 + y2
x > 0, y > 0,
u(x, 1) =

x2 + 1.
(b)
xuux + yuuy = x2 + y2
x > 0, y > 0,
u(x, x) =
√
2x.
2.29 Consider the equation
xux + (1 + y)uy = x(1 + y) + xu.
(a) Find the general solution.
(b) Assume an initial condition of the form u(x, 6x −1) = φ(x). Find a necessary
and sufﬁcient condition for φ that guarantees the existence of a solution to the
problem. Solve the problem for the appropriate φ that you found.
(c) Assume an initial condition of the form u(−1, y) = ψ(y). Find a necessary and
sufﬁcient condition for ψ that guarantees the existence of a solution to the problem.
Solve the problem for the appropriate ψ that you found.

2.10 Exercises
63
(d) Explain the differences between (b) and (c).
2.30 (a) Find a compatibility condition for the Cauchy problem
u2
x + u2
y = 1,
u(cos s, sin s) = 0
0 ≤s ≤2π.
(b) Solve the above Cauchy problem.
(c) Is the solution uniquely deﬁned?

3
Second-order linear equations in two
independent variables
3.1 Introduction
In this chapter we classify the family of second-order linear equations for func-
tions in two independent variables into three distinct types: hyperbolic (e.g., the
wave equation), parabolic (e.g., the heat equation), and elliptic equations (e.g., the
Laplace equation). It turns out that solutions of equations of the same type share
many exclusive qualitative properties. We show that by a certain change of variables
any equation of a particular type can be transformed into a canonical form which
is associated with its type.
3.2 Classiﬁcation
We concentrate in this chapter on second-order linear equations for functions in
two independent variables x, y. Such an equation has the form
L[u] = auxx + 2buxy + cuyy + dux + euy + f u = g,
(3.1)
where a, b, . . . , f, g are given functions of x, y, and u(x, y) is the unknown func-
tion. We introduced the factor 2 in front of the coefﬁcient b for convenience. We
assume that the coefﬁcients a, b, c do not vanish simultaneously.
The operator
L0[u] = auxx + 2buxy + cuyy
that consists of the second-(highest-)order terms of the operator L is called the
principal part of L. It turns out that many fundamental properties of the solutions
of (3.1) are determined by its principal part, and, more precisely, by the sign of the
discriminant δ(L) := b2 −ac of the equation. We classify the equation according
to the sign of δ(L).
64

3.2 Classiﬁcation
65
Deﬁnition 3.1 Equation (3.1) is said to be hyperbolic at a point (x, y) if
δ(L)(x, y) = b(x, y)2 −a(x, y)c(x, y) > 0,
it is said to be parabolic at (x, y) if δ(L)(x, y) = 0, and it is said to be elliptic at
(x, y) if δ(L)(x, y) < 0.
Let  be a domain in R2 (i.e.  is an open connected set). The equation is
hyperbolic (resp., parabolic, elliptic) in , if it is hyperbolic (resp., parabolic,
elliptic) at all points (x, y) ∈.
Deﬁnition 3.2 The transformation (ξ, η)=(ξ(x, y), η(x, y)) is called a change of
coordinates (or a nonsingular transformation) if the Jacobian J := ξxηy −ξyηx of
the transformation does not vanish at any point (x, y).
Lemma 3.3 The type of a linear second-order PDE in two variables is invariant
under a change of coordinates. In other words, the type of the equation is an
intrinsic property of the equation and is independent of the particular coordinate
system used.
Proof Let
L[u] = auxx + 2buxy + cuyy + dux + euy + f u = g,
(3.2)
and let (ξ, η)=(ξ(x, y), η(x, y)) be a nonsingular transformation. Write w(ξ, η) =
u(x(ξ, η), y(ξ, η)). We claim that w is a solution of a second-order equation of the
same type. Using the chain rule one ﬁnds that
ux = wξξx + wηηx,
uy = wξξy + wηηy,
uxx = wξξξ 2
x + 2wξηξxηx + wηηη2
x + wξξxx + wηηxx,
uxy = wξξξxξy + wξη(ξxηy + ξyηx) + wηηηxηy + wξξxy + wηηxy,
uyy = wξξξ 2
y + 2wξηξyηy + wηηη2
y + wξξyy + wηηyy.
Substituting these formulas into (3.2), we see that w satisﬁes the following linear
equation:
ℓ[w] := Awξξ + 2Bwξη + Cwηη + Dwξ + Ewη + Fw = G,
where the coefﬁcients of the principal part of the linear operator ℓare given by
A(ξ, η) = aξ 2
x + 2bξxξy + cξ 2
y,
B(ξ, η) = aξxηx + b(ξxηy + ξyηx) + cξyηy,
C(ξ, η) = aη2
x + 2bηxηy + cη2
y.

66
Second-order linear equations
Notice that we do not need to compute the coefﬁcients of the lower-order derivatives
(D, E, F) since the type of the equation is determined only by its principal part (i.e.
by the coefﬁcients of the second-order terms). An elementary calculation shows
that these coefﬁcients satisfy the following matrix equation:
	 A
B
B
C

=
	 ξx
ξy
ηx
ηy

 	a
b
b
c

 	ξx
ηx
ξy
ηy

.
Denote by J the Jacobian of the transformation. Taking the determinant of the two
sides of the above matrix equation, we ﬁnd
−δ(ℓ) = AC −B2 = J 2(ac −b2) = −J 2δ(L).
Therefore, the type of the equation is invariant under nonsingular transformations.
□
In Chapter 1 we encountered the three (so called) fundamental equations of math-
ematical physics: the heat equation, the wave equations and the Laplace equation.
All of them are linear second-order equations. One can easily verify that the wave
equation is hyperbolic, the heat equation is parabolic, and the Laplace equation is
elliptic.
We shall show in the next sections that if (3.1) is hyperbolic (resp., parabolic,
elliptic) in a domain D, then one can ﬁnd a coordinate system in which the equation
has a simpler form that we call the canonical form of the equation. Moreover, in
such a case the principal part of the canonical form is equal to the principal part of
the fundamental equation of mathematical physics of the same type. This is one of
the reasons for studying these fundamental equations.
Deﬁnition 3.4 The canonical form of a hyperbolic equation is
ℓ[w] = wξη + ℓ1[w] = G(ξ, η),
where ℓ1 is a ﬁrst-order linear differential operator, and G is a function.
Similarly, the canonical form of a parabolic equation is
ℓ[w] = wξξ + ℓ1[w] = G(ξ, η),
and the canonical form of an elliptic equation is
ℓ[w] = wξξ + wηη + ℓ1[w] = G(ξ, η).
Note that the principal part of the canonical form of a hyperbolic equation is
not equal to the wave operator. We shall show in Section 4.2 that a simple (linear)
change of coordinates transforms the wave equation into the equation wξη = 0.

3.3 Canonical form of hyperbolic equations
67
3.3 Canonical form of hyperbolic equations
Theorem 3.5 Suppose that (3.1) is hyperbolic in a domain D. There exists a coor-
dinate system (ξ, η) in which the equation has the canonical form
wξη + ℓ1[w] = G(ξ, η),
where w(ξ, η) = u(x(ξ, η), y(ξ, η)), ℓ1 is a ﬁrst-order linear differential operator,
and G is a function which depends on (3.1).
Proof Without loss of generality, we may assume that a(x, y) ̸= 0 for all
(x, y) ∈D. We need to ﬁnd two functions ξ = ξ(x, y), η = η(x, y) such that
A(ξ, η) = aξ 2
x + 2bξxξy + cξ 2
y = 0,
C(ξ, η) = aη2
x + 2bηxηy + cη2
y = 0.
The equation that was obtained for the function η is actually the same equation as
for ξ; therefore, we need to solve only one equation. It is a ﬁrst-order equation that
is not quasilinear; but as a quadratic form in ξ it is possible to write it as a product
of two linear terms
1
a

aξx + (b −
√
b2 −ac)ξy
 
aξx + (b +
√
b2 −ac)ξy

= 0.
Therefore, we need to solve the following linear equations:
aξx + (b +
√
b2 −ac)ξy = 0,
(3.3)
aξx + (b −
√
b2 −ac)ξy = 0.
(3.4)
In order to obtain a nonsingular transformation (ξ(x, y), η(x, y)) we choose ξ to
be a solution of (3.3) and η to be a solution of (3.4).
These equations are a special case of Example 2.4. The characteristic equations
for (3.3) are
dx
dt = a,
dy
dt = b +
√
b2 −ac,
dξ
dt = 0.
Therefore, ξ is constant on each characteristic. The characteristics are solutions of
the equation
dy
dx = b +
√
b2 −ac
a
.
(3.5)
The function η is constant on the characteristic determined by
dy
dx = b −
√
b2 −ac
a
.
(3.6)
□

68
Second-order linear equations
Deﬁnition 3.6 The solutions of (3.5) and (3.6) are called the two families of the
characteristics (or characteristic projections) of the equation L[u] = g.
Example 3.7 Consider the Tricomi equation:
uxx + xuyy = 0
x < 0.
(3.7)
Find a mapping q =q(x, y), r =r(x, y) that transforms the equation into its canon-
ical form, and present the equation in this coordinate system.
The characteristic equations are
dy±
dx = ±√−x,
and their solutions are 3
2 y± ± (−x)3/2 = constant. Thus, the new independent vari-
ables are
q(x, y) = 3
2 y + (−x)3/2,
r(x, y) = 3
2 y −(−x)3/2.
Clearly,
qx = −rx = −3
2(−x)1/2,
qy = ry = 3
2.
Deﬁne v(q,r) = u(x, y). By the chain rule
ux = −3
2 (−x)1/2vq + 3
2(−x)1/2vr,
uy = 3
2(vq + vr),
uxx = −9
4xvqq −9
4xvrr + 29
4xvqr + 3
4(−x)−1/2(vq −vr),
uyy = −9
4(vqq + 2vqr + vrr).
Substituting these expressions into the Tricomi equation we obtain
uxx + xuyy = −9(q −r)2/3

vqr + vq −vr
6(q −r)

= 0.
Example 3.8 Consider the equation
uxx −2 sin x uxy −cos2x uyy −cos x uy = 0.
(3.8)
Find a coordinate system s = s(x, y), t = t(x, y) that transforms the equation into
its canonical form. Show that in this coordinate system the equation has the form
vst = 0, and ﬁnd the general solution.

3.4 Canonical form of parabolic equations
69
The characteristic equations are
dy±
dx = −sin x ±

sin2 x + cos2 x = −sin x ± 1.
Consequently, the solutions are y± = cos x ± x+ constant. The requested transfor-
mation is
s(x, y) = cos x + x −y,
t(x, y) = cos x −x −y.
Consider now the function v(s, t) = u(x, y) and substitute it into (3.8). We get

vss(−sin x + 1)2 + 2vst(−sin x + 1)(−sin x −1) + vtt(−sin x −1)2
+ vs(−cos x) + vt(−cos x)] −2 sin x [vss(sin x −1) + vst(sin x −1)
+ vst(sin x + 1)+vtt(sin x + 1)]−cos2 x [vss+2vst + vtt]
−cos x(−vs −vt) = 0.
Thus, −4vst = 0, and the canonical form is
vst = 0.
It is easily checked that its general solution is v(s, t) = F(s) + G(t), for every
F, G ∈C2(R). Therefore, the general solution of (3.8) is
u(x, y) = F(cos x + x −y) + G(cos x −x −y).
3.4 Canonical form of parabolic equations
Theorem 3.9 Suppose that (3.1) is parabolic in a domain D. There exists a coor-
dinate system (ξ, η) where the equation has the canonical form
wξξ + ℓ1[w] = G(ξ, η),
where w(ξ, η) = u(x(ξ, η), y(ξ, η)), ℓ1 is a ﬁrst-order linear differential operator,
and G is a function which depends on (3.1).
Proof Since b2 −ac = 0, we may assume that a(x, y) ̸= 0 for all (x, y) ∈D. We
needtoﬁndtwofunctionsξ = ξ(x, y), η = η(x, y)suchthat B(ξ, η) = C(ξ, η) =0
for all (x, y) ∈D. It is enough to make C = 0, since the parabolicity of the equation
will then imply that B = 0. Therefore, we need to ﬁnd a function η that is a solution
of the equation
C(ξ, η) = aη2
x + 2bηxηy + cη2
y = 1
a (aηx + bηy)2 = 0.
From this it follows that η is a solution of the ﬁrst-order linear equation
aηx + bηy = 0.
(3.9)

70
Second-order linear equations
Hence, the solution η is constant on each characteristic, i.e., on a curve that is a
solution of the equation
dy
dx = b
a .
(3.10)
Now, the only constraint on the second independent variable ξ, is that the Jacobian
of the transformation should not vanish in D, and we may take any such function ξ.
Note that a parabolic equation admits only one family of characteristics while for
hyperbolic equations we have two families.
□
Example 3.10 Prove that the equation
x2uxx −2xyuxy + y2uyy + xux + yuy = 0
(3.11)
is parabolic and ﬁnd its canonical form; ﬁnd the general solution on the half-plane
x > 0.
We identify a = x2, 2b = −2xy, c = y2; therefore, b2 −ac = x2y2 −x2y2 = 0
and the equation is parabolic. The equation for the characteristics is
dy
dx = −y
x ,
and the solution is xy = constant. Therefore, we deﬁne η(x, y) = xy. The second
variable can be simply chosen as ξ(x, y) = x. Let v(ξ, η) = u(x, y). Substituting
the new coordinates ξ and η into (3.11), we obtain
x2(y2vηη + 2yvξη + vξξ) −2xy(vη + xyvηη + xvξη)
+ x2vηη + xyvη + xvξ + xyvξ = 0.
Thus,
ξ 2vξξ + ξvξ = 0,
or vξξ + (1/ξ)vξ = 0, and this is the desired canonical form.
Setting w = vξ, we arrive at the ﬁrst-order ODE wξ + (1/ξ)w = 0. The solution
is ln w = −ln ξ + ˜f (η), or w = f (η)/ξ. Hence, v satisﬁes
v =

vξdξ =

wdξ =

f (η)
ξ
dξ = f (η) ln ξ + g(η).
Therefore, the general solution u(x, y) of (3.11) is u(x, y) = f (xy) ln x + g(xy),
where f, g ∈C2(R) are arbitrary real functions.
3.5 Canonical form of elliptic equations
The computation of a canonical coordinate system for the elliptic case is somewhat
more subtle than in the hyperbolic case or in the parabolic case. Nevertheless,

3.5 Canonical form of elliptic equations
71
under the additional assumption that the coefﬁcients of the principal part of the
equation are real analytic functions, the procedure for determining the canonical
transformation is quite similar to the one for the hyperbolic case.
Deﬁnition 3.11 Let D a planar domain. A function f : D →R is said to be real
analytic in D if for each point (x0, y0) ∈D, we have a convergent power series
expansion
f (x, y) =
∞

k=0
k

j=0
a j,k−j(x −x0) j(y −y0)k−j,
valid in some neighborhood N of (x0, y0).
Theorem 3.12 Suppose that (3.1) is elliptic in a planar domain D. Assume further
that the coefﬁcients a, b, c are real analytic functions in D. Then there exists a
coordinate system (ξ, η) in which the equation has the canonical form
wξξ + wηη + ℓ1[w] = G(ξ, η),
where ℓ1 is a ﬁrst-order linear differential operator, and G is a function which
depends on (3.1).
Proof Without loss of generality we may assume that a(x, y) ̸= 0 for all (x, y) ∈D.
We are looking for two functions ξ = ξ(x, y), η = η(x, y) that satisfy the equations
A(ξ, η) = aξ 2
x + 2bξxξy + cξ 2
y = C(ξ, η) = aη2
x + 2bηxηy + cη2
y, (3.12)
B(ξ, η) = aξxηx + b(ξxηy + ξyηx) + cξyηy = 0.
(3.13)
This is a system of two nonlinear ﬁrst-order equations. The main difﬁculty in the
elliptic case is that (3.12)–(3.13) are coupled. In order to decouple these equations,
we shall use the complex plane and the analyticity assumption. We may write the
system (3.12)–(3.13) in the following form:
a(ξ 2
x −η2
x) + 2b(ξxξy −ηxηy) + c(ξ 2
y −η2
y) = 0,
(3.14)
aξxiηx + b(ξxiηy + ξyiηx) + cξyiηy = 0,
(3.15)
where i = √
−1. Deﬁne the complex function φ = ξ + iη. The system (3.14)–
(3.15) is equivalent to the complex valued equation
aφ2
x + 2bφxφy + cφ2
y = 0.
Surprisingly, we have arrived at the same equation as in the hyperbolic case. But in
the elliptic case the equation does not admit any real solution, or, in other words,
elliptic equations do not have characteristics. As in the hyperbolic case, we factor
out the above quadratic PDE, and obtain two linear equations, but now these are

72
Second-order linear equations
complex valued differential equations (where x, y are complex variables!). The
nontrivial question of the existence and uniqueness of solutions immediately arises.
Fortunately, it is known that if the coefﬁcients of these ﬁrst-order linear equations
are real analytic then it is possible to solve them using the same procedure as in the
real case. Moreover, the solutions of the two equations are complex conjugates.
So, we need to solve the equations
aφx + (b ± i
√
ac −b2)φy = 0.
(3.16)
Asbefore,thesolutionsφ, ψ areconstantonthe“characteristics”(whicharedeﬁned
on the complex plane):
dy
dx = b ± i
√
ac −b2
a
.
(3.17)
As in the hyperbolic case, the equation in the new coordinates system has the form
4vφψ + · · · = 0.
This is still not the elliptic canonical form with real coefﬁcients. We return to our
real variables ξ and η using the linear transformation
ξ = Re φ, η = Im φ.
Since ξ and η are solutions of the system (3.12)–(3.13), it follows that in the
variables ξ and η the equation has the canonical form. In Exercise 3.9 the reader
will be asked to prove that the Jacobian of the canonical transformations in the
elliptic case and in the hyperbolic case do not vanish.
□
Example 3.13 Consider the Tricomi equation:
uxx + xuyy = 0,
x > 0.
(3.18)
Find a canonical transformation q =q(x, y), r =r(x, y) and the corresponding
canonical form.
The differential equations for the “characteristics” are dy/dx = ±√−x, and
their solutions are 3
2 y ± i(x)3/2 = constant. Therefore, the canonical variables are
q(x, y) = 3
2 y and r(x, y) = −(x)3/2. Clearly,
qx = 0, qy = 3
2
rx = −3
2(x)1/2, ry = 0.

3.6 Exercises
73
Set v(q,r) = u(x, y). Hence,
ux = −3
2(x)1/2vr ,
uy = 3
2vq ,
uxx = 9
4xvrr −3
4(x)−1/2vr ,
uyy = 9
4vqq .
Substituting these into the Tricomi equation we obtain the canonical form
1
x uxx + uyy = 9
4
	
vqq + vrr + 1
3r vr

= 0.
3.6 Exercises
3.1 Consider the equation
uxx −6uxy + 9uyy = xy2.
(a) Find a coordinates system (s, t) in which the equation has the form:
9vtt = 1
3(s −t)t2.
(b) Find the general solution u(x, y).
(c) Find a solution of the equation which satisﬁes the initial conditions u(x, 0) =
sin x, uy(x, 0) = cos x for all x ∈R.
3.2 (a) Show that the following equation is hyperbolic:
uxx + 6uxy −16uyy = 0.
(b) Find the canonical form of the equation.
(c) Find the general solution u(x, y).
(d) Find a solution u(x, y) that satisﬁes u(−x, 2x) = x and u(x, 0) = sin 2x.
3.3 Consider the equation
uxx + 4uxy + ux = 0.
(a) Bring the equation to a canonical form.
(b) Find the general solution u(x, y) and check by substituting back into the equation
that your solution is indeed correct.
(c) Find a speciﬁc solution satisfying
u(x, 8x) = 0,
ux(x, 8x) = 4e−2x.
3.4 Consider the equation
y5uxx −yuyy + 2uy = 0,
y > 0.
(a) Find the canonical form of the equation.
(b) Find the general solution u(x, y) of the equation.

74
Second-order linear equations
(c) Find the solution u(x, y) which satisﬁes u(0, y) = 8y3, and ux(0, y) = 6, for all
y > 0.
3.5 Consider the equation
xuxx −yuyy + 1
2(ux −uy) = 0.
(a) Find the domain where the equation iselliptic,andthedomainwhereitishyperbolic
(b) For each of the above two domains, ﬁnd the corresponding canonical transforma-
tion.
3.6 Consider the equation
uxx + (1 + y2)2uyy −2y(1 + y2)uy = 0.
(a) Find the canonical form of the equation.
(b) Find the general solution u(x, y) of the equation.
(c) Find the solution u(x, y) which satisﬁes u(x, 0) = g(x), and uy(x, 0) = f (x),
where f, g ∈C2(R).
(d) Find the solution u(x, y) for f (x) = −2x, and g(x) = x.
3.7 Consider the equation
uxx + 2uxy + [1 −q(y)]uyy = 0,
where
q(y) =



−1
y < −1,
0
|y| ≤1,
1
y > 1.
(a) Find the domains where the equation is hyperbolic, parabolic, and elliptic.
(b) For each of the above three domains, ﬁnd the corresponding canonical transfor-
mation and the canonical form.
(c) Draw the characteristics for the hyperbolic case.
3.8 Consider the equation
4y2uxx + 2(1 −y2)uxy −uyy −
2y
1 + y2 (2ux −uy) = 0.
(a) Find the canonical form of the equation.
(b) Find the general solution u(x, y) of the equation.
(c) Find the solution u(x, y) which satisﬁes u(x, 0) = g(x), and uy(x, 0) = f (x),
where f, g ∈C2(R) are arbitrary functions.
3.9 (a) Prove that in the hyperbolic case the canonical transformation is nonsingular
(J ̸= 0).
(b) Prove that in the elliptic case the canonical transformation is nonsingular (J ̸= 0).
3.10 Consider the equation
uxx −2uxy + 4ey = 0.

3.6 Exercises
75
(a) Find the canonical form of the equation.
(b) Find the solution u(x, y) which satisﬁes u(0, y) = f (y), and ux(0, y) = g(y).
3.11 In continuation of Example 3.8, consider the equation
uxx −2 sin x uxy −cos2x uyy −cos x uy = 0.
(a) Find a solution of the equation which satisﬁes u(0, y) = f (y), ux(0, y) = g(y),
where f , g are given functions.
(b) Find conditions on f and g such that the solution u(x, y) of part (a) is a classical
solution.
3.12 Consider the equation
uxx + yuyy = 0.
Find the canonical forms of the equation for the domain where the equation is hyper-
bolic, and for the domain where it is elliptic.

4
The one-dimensional wave equation
4.1 Introduction
In this chapter we study the one-dimensional wave equation on the real line. The
canonical form of the wave equation will be used to show that the Cauchy problem
is well-posed. Moreover, we shall derive simple explicit formulas for the solutions.
We also discuss some important properties of the solutions of the wave equation
which are typical for more general hyperbolic problems as well.
4.2 Canonical form and general solution
The homogeneous wave equation in one (spatial) dimension has the form
utt −c2uxx = 0
−∞≤a < x < b ≤∞, t > 0,
(4.1)
where c ∈R is called the wave speed, a terminology that will be justiﬁed in the
discussion below.
To obtain the canonical form of the wave equation, deﬁne the new variables
ξ = x + ct
η = x −ct,
and set w(ξ, η)=u(x(ξ, η), t(ξ, η)) (see Section 3.3 for the method to obtain
this canonical transformation). Using the chain rule for the function u(x, t)=
w(ξ(x, t), η(x, t)), we obtain
ut = wξξt + wηηt = c(wξ −wη),
ux = wξξx + wηηx = wξ + wη,
and
utt = c2(wξξ −2wξη + wηη),
uxx = wξξ + 2wξη + wηη.
Hence,
utt −c2uxx = −4c2wξη = 0.
76

4.2 General solution
77
This is the canonical form for the wave equation. Since (wξ)η = 0, it follows that
wξ = f (ξ), and then w =

f (ξ) dξ + G(η). Therefore, the general solution of the
equation wξη = 0 has the form
w(ξ, η) = F(ξ) + G(η),
where F, G ∈C2(R) are two arbitrary functions. Thus, in the original variables,
the general solution of the wave equation is
u(x, t) = F(x + ct) + G(x −ct).
(4.2)
In other words, if u is a solution of the one-dimensional wave equation, then there
exist two real functions F, G ∈C2 such that (4.2) holds. Conversely, any two
functions F, G ∈C2 deﬁne a solution of the wave equation via formula (4.2).
For a ﬁxed t0 > 0, the graph of the function G(x −ct0) has the same shape as
the graph of the function G(x), except that it is shifted to the right by a distance
ct0. Therefore, the function G(x −ct) represents a wave moving to the right with
velocity c, and it is called a forward wave. The function F(x + ct) is a wave
traveling to the left with the same speed, and it is called a backward wave. Indeed
c can be called the wave speed.
Equation (4.2) demonstrates that any solution of the wave equation is the sum
of two such traveling waves. This observation will enable us to obtain graphical
representations of the solutions (the graphical method).
We would like to extend the validity of (4.2). Observe that for any two real
piecewise continuous functions F, G, (4.2) deﬁnes a piecewise continuous function
u thatisasuperpositionofaforwardwaveandabackwardwavetravelinginopposite
directions with speed c. Moreover, it is possible to ﬁnd two sequences of smooth
functions, {Fn(s)}, {Gn(s)}, converging at any point to F and G, respectively, which
converge uniformly to these functions in any bounded and closed interval that does
not contain points of discontinuity. The function
un(x, t) = Fn(x + ct) + Gn(x −ct)
is a proper solution of the wave equation, but the limiting function u(x, t) = F(x +
ct) + G(x −ct) is not necessarily twice differentiable, and therefore might not be
a solution. We call a function u(x, t) that satisﬁes (4.2) with piecewise continuous
functions F, G a generalized solution of the wave equation.
Let us further discuss the general solution (4.2). Consider the (x, t) plane. The
following two families of lines
x −ct = constant,
x + ct = constant,
are called the characteristics of the wave equation (see Section 3.3). For the wave
equation, the characteristics are straight lines in the (x, t) plane with slopes ±1/c.

78
The one-dimensional wave equation
It turns out that as for ﬁrst-order PDEs, the “information” is transferred via these
curves.
We arrive now at one of the most important properties of the characteristics.
Assume that for a ﬁxed time t0, the solution u is a smooth function except at one
point (x0, t0). Clearly, either F is not smooth at x0 + ct0, and/or the function G is
not smooth at x0 −ct0. There are two characteristics that pass through the point
(x0, t0); these are the lines
x −ct = x0 −ct0,
x + ct = x0 + ct0.
Consequently, for any time t1 ̸= t0 the solution u is smooth except at one or two
points x± that satisfy
x−−ct1 = x0 −ct0,
x+ + ct1 = x0 + ct0.
Therefore, the singularities (nonsmoothness) of solutions of the wave equation
are traveling only along characteristics. This phenomenon is typical of hyperbolic
equations in general: a singularity is not smoothed out; rather it travels at a ﬁnite
speed. This is in contrast to parabolic and elliptic equations, where, as will be shown
in the following chapters, singularities are immediately smoothed out.
Example 4.1 Let u(x, t) be a solution of the wave equation
utt −c2uxx = 0,
whichisdeﬁnedinthewholeplane.Assumethatu isconstantontheline x = 2 + ct.
Prove that ut + cux = 0.
The solution u(x, t) has the form u(x, t) = F(x + ct) + G(x −ct). Since u(2 +
ct, t) = constant, it follows that
F(2 + 2ct) + G(2) = constant.
Set s = 2 + 2ct, we have F(s) = constant. Consequently u(x, t) = G(x −ct).
Computing now the expression ut + cux, we obtain
ut + cux = −cG′(x −ct) + cG′(x −ct) = 0.
4.3 The Cauchy problem and d’Alembert’s formula
The Cauchy problem for the one-dimensional homogeneous wave equation is given
by
utt −c2uxx = 0
−∞< x < ∞, t > 0,
(4.3)
u(x, 0) = f (x),
ut(x, 0) = g(x),
−∞< x < ∞.
(4.4)

4.3 The Cauchy problem and d’Alembert’s formula
79
A solution of this problem can be interpreted as the amplitude of a sound wave
propagating in a very long and narrow pipe, which in practice can be considered as
a one-dimensional inﬁnite medium. This system also represents the vibration of an
inﬁnite (ideal) string. The initial conditions f, g are given functions that represent
the amplitude u, and the velocity ut of the string at time t = 0.
A classical (proper) solution of the Cauchy problem (4.3)–(4.4) is a function
u that is continuously twice differentiable for all t > 0, such that u and ut are
continuous in the half-space t ≥0, and such that (4.3)–(4.4) are satisﬁed. Generally
speaking, classical solutions should have the minimal smoothness properties in
order to satisfy continuously all the given conditions in the classical sense.
Recall that the general solution of the wave equation is of the form
u(x, t) = F(x + ct) + G(x −ct).
(4.5)
Our aim is to ﬁnd F and G such that the initial conditions of (4.4) are satisﬁed.
Substituting t = 0 into (4.5) we obtain
u(x, 0) = F(x) + G(x) = f (x).
(4.6)
Differentiating (4.5) with respect to t and substituting t = 0, we have
ut(x, 0) = cF′(x) −cG′(x) = g(x).
(4.7)
Integration of (4.7) over the integral [0, x] yields
F(x) −G(x) = 1
c
 x
0
g(s) ds + C,
(4.8)
where C = F(0) −G(0). Equations (4.6) and (4.8) are two linear algebraic equa-
tions for F(x) and G(x). The solution of this system of equations is given by
F(x) = 1
2 f (x) + 1
2c
 x
0
g(s) ds + C
2 ,
(4.9)
G(x) = 1
2 f (x) −1
2c
 x
0
g(s) ds −C
2 .
(4.10)
By substituting these expressions for F and G into the general solution (4.5),
we obtain the formula
u(x, t) = f (x + ct) + f (x −ct)
2
+ 1
2c
 x+ct
x−ct
g(s) ds,
(4.11)
which is called d’Alembert’s formula. Note that sometimes (4.9)–(4.10) are also
useful, as they give us explicit formulas for the forward and the backward waves.
The following examples illustrate the use of d’Alembert’s formula.

80
The one-dimensional wave equation
Example 4.2 Consider the Cauchy problem
utt −uxx = 0
−∞< x < ∞, t > 0,
u(x, 0) = f (x) =



0
−∞< x < −1,
x + 1
−1 ≤x ≤0,
1 −x
0 ≤x ≤1,
0
1 < x < ∞,
ut(x, 0) = g(x) =



0
−∞< x < −1,
1
−1 ≤x ≤1,
0
1 < x < ∞.
(a) Evaluate u at the point (1, 1
2).
(b) Discuss the smoothness of the solution u.
(a) Using d’Alembert’s formula, we ﬁnd that
u(1, 1
2) =
f ( 3
2 )+ f ( 1
2 )
2
+ 1
2
 3
2
1
2 g(s) ds.
Since 3
2 > 1 it follows that f ( 3
2) = 0. On the other hand, 0 ≤1
2 ≤1; therefore,
f ( 1
2) = 1
2. Evidently,
 3
2
1
2 g(s)ds =
 1
1
2 1ds = 1
2. Thus, u(1, 1
2) = 1
2.
(b) The solution is not classical, since u ̸∈C1. Yet u is a generalized solution of
the problem. Note that although g is not continuous, nevertheless the solution u
is a continuous function. The singularities of the solution propagate along charac-
teristics that intersect the initial line t = 0 at the singularities of the initial condi-
tions. These are exactly the characteristics x ± t = −1, 0, 1. Therefore, the solu-
tion is smooth in a neighborhood of the point (1, 1
2) which does not intersect these
characteristics.
Example 4.3 Let u(x, t) be the solution of the Cauchy problem
utt −9uxx = 0
−∞< x < ∞, t > 0,
u(x, 0) = f (x) =
1
|x| ≤2,
0
|x| > 2,
ut(x, 0) = g(x) =
1
|x| ≤2,
0
|x| > 2.
(a) Find u(0, 1
6).
(b) Discuss the large time behavior of the solution.

4.3 The Cauchy problem and d’Alembert’s formula
81
(c) Find the maximal value of u(x, t), and the points where this maximum is achieved.
(d) Find all the points where u ∈C2.
(a) Since
u(x, t) = f (x + 3t) + f (x −3t)
2
+ 1
6
 x+3t
x−3t
g(s)ds,
it follows that for x = 0 and t = 1
6, we have
u(0, 1
6) = f ( 1
2) + f (−1
2)
2
+ 1
6

1
2
−1
2
g(s) ds = 1 + 1
2
+ 1
6

1
2
−1
2
1ds = 7
6.
(b) Fix ξ ∈R and compute limt→∞u(ξ, t). Clearly,
lim
t→∞f (ξ + 3t) = 0,
lim
t→∞f (ξ −3t) = 0,
lim
t→∞
 ξ+3t
ξ−3t
g(s) ds =
 2
−2
1ds = 4.
Therefore, limt→∞u(ξ, t) = 2
3.
(c) Recall that for any real functions f, g,
max{ f (x) + g(x)} ≤max f (x) + max g(x).
It turns out that in our special case there exists a point (x, t), where all the terms in (4.11)
attain their maximal value simultaneously, and therefore at such a point the maximum
of u is attained.
Indeed, max{ f (x + 3t)} = 1 which is attained on the strip −2 ≤x + 3t ≤2. Sim-
ilarly, max{ f (x −3t)} = 1 which is attained on the strip −2 ≤x −3t ≤2, while
max{
 x+3t
x−3t g(s) ds =
 2
−2 1ds = 4, and it is attained on the intersection of the half-
planes x + 3t ≥2 and x −3t ≤−2. The intersection of all these sets is the set of all
points that satisfy the two equations
x + 3t = 2,
x −3t = −2.
This system has a unique solution at (x, t) = (0, 2
3). Thus, the solution u achieves its
maximum at the point (0, 2
3), where u(0, 2
3) = 5
3.
(d) The initial conditions are smooth except at the points x = ±2. Therefore, the solution
is smooth at all points that are not on the characteristics
x ± 3t = −2,
x ± 3t = 2.
The function u is a generalized solution that is piecewise continuous for any ﬁxed time
t > 0.
The well-posedness of the Cauchy problem follows from the d’Alembert
formula.
Theorem 4.4 Fix T > 0. The Cauchy problem (4.3)–(4.4) in the domain −∞<
x < ∞, 0 ≤t ≤T is well-posed for f ∈C2(R), g ∈C1(R).

82
The one-dimensional wave equation
Proof The existence and uniqueness follow directly from the d’Alembert formula.
Indeed, this formula provides us with a solution, and we have shown that any
solution of the Cauchy problem is necessarily equal to the d’Alembert solution.
Note that from our smoothness assumption ( f ∈C2(R), g ∈C1(R)), it follows that
u ∈C2(R × (0, ∞)) ∩C1(R × [0, ∞)), and therefore, the d’Alembert solution is a
classical solution. On the other hand, for f ∈C(R) and g that is locally integrable,
the d’Alembert solution is a generalized solution.
It remains to prove the stability of the Cauchy problem, i.e. we need to show that
small changes in the initial conditions give rise to a small change in the solution.
Let ui be two solutions of the Cauchy problem with initial conditions fi, gi, where
i = 1, 2. Now, if
| f1(x) −f2(x)| < δ,
|g1(x) −g2(x)| < δ,
for all x ∈R, then for all x ∈R and 0 ≤t ≤T we have
|u1(x, t) −u2(x, t)| ≤| f1(x + ct) −f2(x + ct)|
2
+ | f1(x −ct) −f2(x −ct)|
2
+ 1
2c
 x+ct
x−ct
|g1(s) −g2(s)| ds < 1
2(δ + δ) + 1
2c2ctδ ≤(1 + T )δ.
Therefore, for a given ε > 0, we take δ < ε/(1 + T ). Then for all x ∈R and 0 ≤
t ≤T we have
|u1(x, t) −u2(x, t)| < ε.
□
Remark 4.5 (1) The Cauchy problem is ill-posed on the domain −∞<x <
∞, t ≥0.
(2) The d’Alembert formula is also valid for −∞< x < ∞, T < t ≤0, and the
Cauchy problem is also well-posed in this domain. The physical interpretation is
that the process is reversible.
4.4 Domain of dependence and region of inﬂuence
Let us return to the Cauchy problem (4.3)–(4.4), and examine what is the informa-
tion that actually determines the solution u at a ﬁxed point (x0, t0). Consider the
(x, t) plane and the two characteristics passing through the point (x0, t0):
x −ct = x0 −ct0,
x + ct = x0 + ct0.
These straight lines intersect the x axis at the points (x0 −ct0, 0) and (x0 + ct0, 0),
respectively. The triangle formed by the these characteristics and the interval [x0 −
ct0, x0 + ct0] is called a characteristic triangle (see Figure 4.1).

4.4 Domain of dependence and region of inﬂuence
83
t
x
L
x0 + ct0
x0 − ct0
(x0,t0)
R
B
x + ct = x0 + ct0
x − ct = x0 −ct0
∆
Figure 4.1 Domain of dependence.
By the d’Alembert formula
u(x0, t0) = f (x0 + ct0) + f (x0 −ct0)
2
+ 1
2c
 x0+ct0
x0−ct0
g(s) ds.
(4.12)
Therefore, the value of u at the point (x0, t0) is determined by the values of f at
the vertices of the characteristic base and by the values of g along this base. Thus,
u(x0, t0) depends only on the part of the initial data that is given on the interval
[x0 −ct0, x0 + ct0]. Therefore, this interval is called domain of dependence of u
at the point (x0, t0). If we change the initial data at points outside this interval,
the value of the solution u at the point (x0, t0) will not change. Information on
a change in the data travels with speed c along the characteristics, and therefore
such information is not available for t ≤t0 at the point x0. The change will ﬁnally
inﬂuence the solution at the point x0 at a later time. Hence, for every point (x, t) in
a ﬁxed characteristic triangle, u(x, t) is determined only by the initial data that are
given on (part of) the characteristic base (see Figure 4.1). Furthermore, if the initial
data are smooth on this base, then the solution is smooth in the whole triangle.
We may ask now the opposite question: which are the points on the half-plane
t > 0 that are inﬂuenced by the initial data on a ﬁxed interval [a, b]? The set of all
such points is called the region of inﬂuence of the interval [a, b]. It follows from the
discussion above that the points of this interval inﬂuence the value of the solution
u at a point (x0, t0) if and only if [x0 −ct0, x0 + ct0] ∩[a, b] ̸= ∅. Hence the initial
data along the interval [a, b] inﬂuence only points (x, t) satisfying
x −ct ≤b, and x + ct ≥a.
These are the points inside the forward (truncated) characteristic cone that is deﬁned
bythebase[a, b]andtheedges x + ct = a, x −ct = b (itistheunionoftheregions
I–IV of Figure 4.2).
Assume, for instance, that the initial data f, g vanish outside the interval [a, b].
Then the amplitude of the vibrating string is zero at every point outside the inﬂuence

84
The one-dimensional wave equation
x
t
x + ct = a
a
b
x + ct = b
x − ct = a
x − ct = b
II
III
I
IV
Figure 4.2 Region of inﬂuence.
region of this interval. On the other hand, for a ﬁxed point x0 on the string, the effect
of the perturbation (from the zero data) along the interval [a, b] will be felt after a
time t0 ≥0, and eventually, for t large enough, the solution takes the constant value
u(x0, t) = (1/2c)
 b
a g(s) ds. This occurs precisely at points (x0, t) that are inside
the cone
x0 −ct ≤a, and x0 + ct ≥b,
(see region IV in Figure 4.2).
Using these observations, we demonstrate in the following example the so-called
graphical method for solving the Cauchy problem for the wave equation.
Example 4.6 Consider the Cauchy problem
utt −c2uxx = 0
−∞< x < ∞, t > 0,
u(x, 0) = f (x) =
2
|x| ≤a,
0
|x| > a,
ut(x, 0) = g(x) = 0
−∞< x < ∞.
Draw the graphs of the solution u(x, t) at times ti = ia/2c, where i = 0, 1, 2, 3.
Using d’Alembert’s formula, we write the solution u as a sum of backward and
forward waves
u(x, t) = f (x + ct) + f (x −ct)
2
.
Since these waves are piecewise constant functions, it is clear that for each t,
the solution u is also a piecewise constant function of x with values u = 0,
1, 2.
Consider the (x, t) plane. We draw the characteristic lines that pass through the
special points on the initial line t = 0 where the initial data are not smooth. In

4.4 Domain of dependence and region of inﬂuence
85
t,u
x
3a
−5a
−2a
2a
−3a
2
−a
2
a
2
3a
2
−a
2
a
5a
2
2
−a
a
2c
2
t3 =
a
t2 = c
a
t1 = 2c
t0 = 0
Figure 4.3 The graphical method.
the present problem these are the points x = ±a. We also draw the lines t = ti
that will serve us as the abscissas (x axes) for the graphs of the functions u(x, ti).
Note that the ordinate of the coordinate system is used as the t and the u axes (see
Figure 4.3).
Consider the time t = t1. The forward wave has traveled a/2 units to the right,
and the backward wave has traveled a/2 units to the left. The support (the set
of points where the function is not zero) of the forward wave at time t1 is the
interval [−a/2, 3a/2], while [−3a/2, a/2] is the support of the backward wave at
t1. Therefore, the support of the solution at t1 is the interval [−3a/2, 3a/2], i.e. the
region of inﬂuence of [−a, a] at t = t1. Now, at the intersection of the supports of
the two waves (the interval [−a/2, a/2]) u takes the value 1 + 1 = 2, while on the
intervals [−3a/2, −a/2), (a/2, 3a/2], where the supports do not intersect, u takes
the value 1. Obviously, u = 0 at all other points.
Consider the time t2 = a/c. The support of the forward (backward) wave is
[0, 2a] ([−2a, 0], respectively). Consequently, the support of the solution u is
[−2a, 2a], i.e. the region of inﬂuence of [−a, a] at t = t2. The intersection of
the supports of the two waves is the point x = 0, where u takes the value 2. On the
intervals [−2a, 0), (0, 2a], u is 1. Obviously, u = 0 at all other points.
At the time t3 = 3a/2c, the support of the forward (backward) wave is
[a/2, 5a/2] ([−5a/2, −a/2], respectively), and there is no interaction between
the waves. Therefore, the solution at these intervals equals 1, and it equals zero
otherwise.
To conclude, the ﬁrst step of the graphical method is to compute and to draw the
graphs of the forward and backward waves. Then, for a given time t, we shift these

86
The one-dimensional wave equation
two shapes to the right and, respectively, to the left by ct units. Finally, we add the
two graphs.
In the next example we use the graphical method to investigate the inﬂuence of
the initial velocity on the solution.
Example 4.7 Findthegraphsofthesolutionu(x, ti), ti = i, i = 1, 4fortheprob-
lem
utt −uxx = 0
−∞< x < ∞, t > 0,
u(x, 0) = f (x) = 0,
−∞< x < ∞,
ut(x, 0) = g(x) =
0
x < 0,
1
x ≥0.
We apply d’Alembert’s formula to write the solution as the sum of forward and
backward waves:
u(x, t) = 1
2
 0
x−t
g(s) ds + 1
2
 x+t
0
g(s) ds = −max{0, x −t}
2
+ max{0, x + t}
2
.
Since both the forward and backward waves are piecewise linear functions, the
solution u(·, t) for all times t is a piecewise linear function of x.
We draw in the plane (x, t) the characteristics emanating from the points where
the initial condition is nonsmooth. In our case this happens at just one point, namely
x = 0. We also depict the lines t = ti that form the abscissas for the graph of u(x, ti)
(see Figure 4.4).
t = 1
1
4
t = 4
−4
−1
x = t
x
x = −t
t,u
t = 0
Figure 4.4 The graphical solution for Example 4.7.

4.5 Nonhomogeneous wave equation
87
At time t1, the forward wave has moved one unit to the right, and the backward
wave has moved one unit to the left. The forward wave is supported on the interval
[1, ∞), while the backward wave is supported on [−1, ∞). Therefore the solution
is supported on [−1, ∞). It is clear that on the interval [−1, 1] the solution forms
a linear function thanks to the backward wave. Speciﬁcally, u = (x + 1)/2 there.
In the interval [1, ∞) the forward wave is a linear function with slope −1/2, while
the backward wave is a linear function with slope 1/2. Therefore the solution itself,
being a superposition of these two waves, is constant u = 1 there.
A similar consideration can be used to draw the graph for t = 4. Actually, the
solution can be written explicitly as
u(x, t) =



0
x < −t,
x + t
2
−t ≤x ≤t,
t
x > t.
Notice that for each ﬁxed x0 ∈R, the solution u(x0, t) (considered as a function of
t) is not bounded.
4.5 The Cauchy problem for the nonhomogeneous wave equation
Consider the following Cauchy problem
utt −c2uxx = F(x, t)
−∞< x < ∞, t > 0,
(4.13)
u(x, 0) = f (x),
ut(x, 0) = g(x)
−∞< x < ∞.
(4.14)
This problem models, for example, the vibration of a very long string in the presence
of an external force F. As in the homogeneous case f, g are given functions that
represent the shape and the vertical velocity of the string at time t = 0.
As in every linear problem, the uniqueness for the homogeneous problem implies
the uniqueness for the nonhomogeneous problem.
Proposition 4.8 The Cauchy problem (4.13)–(4.14) admits at most one solution.
Proof Assume that u1, u2 are solutions of problem (4.13)–(4.14). We should prove
that u1 = u2. The function u = u1 −u2 is a solution of the homogeneous problem
utt −c2uxx = 0
−∞, < x < ∞, t > 0,
(4.15)
u(x, 0) = 0,
ut(x, 0) = 0
−∞< x < ∞.
(4.16)
On the other hand, v(x, t) = 0 is also a solution of the same (homogeneous) prob-
lem. By Theorem 4.4, u = v = 0, hence, u1 = u2.
□

88
The one-dimensional wave equation
Next, using an explicit formula, we prove, as in the homogeneous case, the
existence of a solution of the Cauchy problem (4.13)–(4.14). For this purpose,
recall Green’s formula for a pair of functions P, Q in a planar domain  with a
piecewise smooth boundary :
 

[Q(x, t)x −P(x, t)t] dx dt =


[P(x, t) dx + Q(x, t) dt].
Let u(x, t) be a solution of problem (4.13)–(4.14). Integrate the two sides of the
PDE (4.13) over a characteristic triangle △with a ﬁxed upper vertex (x0, t0). The
three edges of this triangle (base, right and left edges) will be denoted by B, R, L,
respectively (see Figure 4.1). We have
−
 
△
F(x, t) dx dt =
 
△
(c2uxx −utt) dx dt.
Using Green’s formula with Q = c2ux and P = ut, we obtain
−
 
△
F(x, t) dx dt =


(ut dx + c2ux dt) =

B
+

R
+

L
(ut dx + c2ux dt).
On the base B we have dt = 0; therefore, using the initial conditions, we get

B
(ut dx + c2ux dt) =
 x0+ct0
x0−ct0
ut(x, 0) dx =
 x0+ct0
x0−ct0
g(x) dx.
On the right edge R, x + ct = x0 + ct0, hence dx = −cdt. Consequently,

R
(ut dx + c2ux dt) = −c

R
(ut dt + ux dx) = −c

R
du
= −c[u(x0, t0) −u(x0 + ct0, 0)] = −c[u(x0, t0) −f (x0 + ct0)].
Similarly, on the left edge L, x −ct = x0 −ct0, implying dx = cdt, and

L
(ut dx + c2ux dt) = c

L
(ut dt + ux dx) = c

L
du
= c[u(x0 −ct0, 0) −u(x0, t0)] = c[ f (x0 −ct0) −u(x0, t0)].
Therefore,
−
 
△
F(x, t) dx dt =
 x0+ct0
x0−ct0
g(x) dx+c[ f (x0 −ct0)+ f (x0 + ct0)−2u(x0, t0)].
Solving for u gives
u(x0, t0)= f (x0 + ct0)+ f (x0 −ct0)
2
+ 1
2c
 x0+ct0
x0−ct0
g(x) dx+ 1
2c

△
F(x, t) dx dt.

4.5 Nonhomogeneous wave equation
89
We ﬁnally obtain an explicit formula for the solution at an arbitrary point (x, t):
u(x, t) = f (x + ct) + f (x −ct)
2
+ 1
2c
 x+ct
x−ct
g(s) ds + 1
2c
 
△
F(ξ, τ) dξ dτ.
(4.17)
This formula is also called d’Alembert’s formula.
Remark 4.9 (1) Note that for F = 0 the two d’Alembert’s formulas coincide, and actu-
ally, we have obtained another proof of the d’Alembert formula (4.11).
(2) The value of u at a point (x0, t0) is determined by the values of the given data on the
whole characteristic triangle whose upper vertex is the point (x0, t0). This is the domain
of dependence for the nonhomogeneous Cauchy problem.
It remains to prove that the function u in (4.17) is indeed a solution of the Cauchy
problem. From the superposition principle it follows that u in (4.17) is the desired
solution, if and only if the function
v(x, t) = 1
2c
 
△
F(ξ, τ) dξ dτ = 1
2c
 t
0
 x+c(t−τ)
x−c(t−τ)
F(ξ, τ) dξ dτ
is a solution of the Cauchy problem
utt −c2uxx = F(x, t)
−∞< x < ∞, t > 0,
(4.18)
u(x, 0) = 0,
ut(x, 0) = 0
−∞< x < ∞.
(4.19)
We shall prove that v is a solution of the initial value problem (4.18)–(4.19) under
the assumption that F and Fx are continuous. Clearly,
v(x, 0) = 0.
In order to take derivatives, we shall use the formula
∂
∂t
 b(t)
a(t)
G(ξ, t) dξ = G(b(t), t)b′(t) −G(a(t), t)a′(t) +
 b(t)
a(t)
∂
∂t G(ξ, t) dξ.
Hence,
vt(x, t) = 1
2c
 x
x
F(ξ, t) dξ + 1
2
 t
0
[F(x + c(t −τ), τ) + F(x −c(t −τ), τ)] dτ
= 1
2
 t
0
[F(x + c(t −τ), τ) + F(x −c(t −τ), τ)] dτ.
In particular,
vt(x, 0) = 0.

90
The one-dimensional wave equation
By taking the second derivative with respect to t, we have
vtt(x, t) = F(x, t) + c
2
 t
0
[Fx(x + c(t −τ), τ) −Fx(x −c(t −τ), τ)] dτ.
Similarly,
vx(x, t) = 1
2c
 t
0
[F(x + c(t −τ), τ) −F(x −c(t −τ), τ)] dτ,
vxx(x, t) = 1
2c
 t
0
[Fx(x + c(t −τ), τ) −Fx(x −c(t −τ), τ)] dτ.
Therefore, v(x, t) is a solution of the nonhomogeneous wave equation (4.18), and
the homogeneous initial conditions (4.19) are satisﬁed. Note that all the above
differentiations are justiﬁed provided that F, Fx ∈C(R2).
Theorem 4.10 Fix T > 0.TheCauchyproblem(4.13)–(4.14)inthedomain−∞<
x < ∞, 0 ≤t ≤T is well-posed for F, Fx ∈C(R2), f ∈C2(R), g ∈C1(R).
Proof Recallthattheuniquenesshasalreadybeenproved,andtheexistencefollows
from d’Alembert’s formula. It remains to prove stability, i.e. we need to show that
small changes in the initial conditions and the external force give rise to a small
change in the solution. For i = 1, 2, let ui be the solution of the Cauchy problem
with the corresponding function Fi, and the initial conditions fi, gi. Now, if
|F1(x, t) −F2(x, t)| < δ,
| f1(x) −f2(x)| < δ,
|g1(x) −g2(x)| < δ,
for all x ∈R, 0 ≤t ≤T , then for all x ∈R and 0 ≤t ≤T we have
|u1(x, t) −u2(x, t)| ≤| f1(x + ct) −f2(x + ct)|
2
+ | f1(x −ct) −f2(x −ct)|
2
+ 1
2c
 x+ct
x−ct
|g1(s) −g2(s)| ds + 1
2c
 
△
|F1(ξ, τ) −F2(ξ, τ)| dξ dτ
< 1
2(δ + δ) + 1
2c2ctδ + 1
2cct2δ ≤(1 + T + T 2/2)δ.
Therefore, for a given ε > 0, we choose δ < ε/(1 + T + T 2/2). Thus, for all x ∈R
and 0 ≤t ≤T , we have
|u1(x, t) −u2(x, t)| < ε.
Note that δ does not depend on the wave speed c.
□
Corollary 4.11 Suppose that f, g are even functions, and for every t ≥0 the func-
tion F(·, t) is even too. Then for every t ≥0 the solution u(·, t) of the Cauchy
problem (4.13)–(4.14) is also even. Similarly, the solution is an odd function or a

4.5 Nonhomogeneous wave equation
91
periodic function with a period L (as a function of x) if the data are odd functions
or periodic functions with a period L.
Proof We prove the ﬁrst part of the corollary. The other parts can be shown simi-
larly. Let u be the solution of the problem and deﬁne the function v(x, t) = u(−x, t).
Clearly,
vx(x, t) = −ux(−x, t),
vt(x, t) = ut(−x, t)
and
vxx(x, t) = uxx(−x, t),
vtt(x, t) = utt(−x, t).
Therefore,
vtt(x, t)−c2vxx(x, t) = utt(−x, t)−c2uxx(−x, t)
= F(−x, t)= F(x, t)
−∞<x <∞, t >0.
Thus, v is a solution of the nonhomogeneous wave equation (4.13). Furthermore,
v(x, 0) = u(−x, 0) = f (−x) = f (x),
vt(x, 0) = ut(−x, 0) = g(−x) = g(x).
It means that v is also a solution of the initial value problem (4.13)–(4.14). Since
the solution of this problem is unique, we have v(x, t) = u(x, t), which implies
u(−x, t) = u(x, t).
□
Example 4.12 Solve the following Cauchy problem
utt −9uxx = ex −e−x
−∞< x < ∞, t > 0,
u(x, 0) = x
−∞< x < ∞,
ut(x, 0) = sin x
−∞< x < ∞.
Using the d’Alembert formula, we have
u(x, t) = 1
2[ f (x + ct) + f (x −ct)] + 1
2c
 x+ct
x−ct
g(s)ds
+ 1
2c
 τ=t
τ=0
 ξ=x+c(t−τ)
ξ=x−c(t−τ)
F(ξ, τ) dξdτ.
Hence,
u(x, t) = 1
2[x + 3t + x −3t] + 1
6
 x+3t
x−3t
sin sds
+1
6
 τ=t
τ=0
 ξ=x+3(t−τ)
ξ=x−3(t−τ)
(eξ −e−ξ) dξdτ
= x + 1
3 sin x sin 3t −2
9 sinh x + 2
9 sinh x cosh 3t.
As expected, for all t ≥0, the solution u is an odd function of x.

92
The one-dimensional wave equation
Remark 4.13 In many cases it is possible to reduce a nonhomogeneous problem to
a homogeneous problem if we can ﬁnd a particular solution v of the given nonho-
mogeneous equation. This will eliminate the need to perform the double integration
which appears in the d’Alembert formula (4.17). The technique is particularly use-
ful when F has a simple form, for example, when F = F(x) or F = F(t). Suppose
that such a particular solution v is found, and consider the function w = u −v. By
the superposition principle, w should solve the following homogeneous Cauchy
problem:
wtt −wxx = 0
−∞< x < ∞, t > 0,
w(x, 0) = f (x) −v(x, 0)
−∞< x < ∞,
wt(x, 0) = g(x) −vt(x, 0)
−∞< x < ∞.
Hence,w canbefoundusingthed’Alembertformulaforthehomogeneousequation.
Then u = v + w is the solution of our original problem.
We illustrate this idea through the following example.
Example 4.14 Solve the problem
utt −uxx = t7
−∞< x < ∞, t > 0,
u(x, 0) = 2x + sin x
−∞< x < ∞,
ut(x, 0) = 0
−∞< x < ∞.
Because of the special form of the nonhomogeneous equation, we look for a partic-
ular solution of the form v = v(t). Indeed it can be easily veriﬁed that v(x, t) =
1
72t9
is such a solution. Consequently, we need to solve the homogeneous problem
wtt −wxx = 0
−∞< x < ∞, t > 0,
w(x, 0) = f (x) −v(x, 0) = 2x + sin x
−∞< x < ∞,
wt(x, 0) = g(x) −vt(x, 0) = 0
−∞< x < ∞.
Using d’Alembert’s formula for the homogeneous equation, we have
w(x, t) = 2x + 1
2 sin(x + t) + 1
2 sin(x −t),
and the solution of the original problem is given by
u(x, t) = 2x + sin x cos(t) + t9
72.

4.6 Exercises
93
4.6 Exercises
4.1 Complete the proof of Corollary 4.11.
4.2 Solve the problem
utt −uxx = 0
0 < x < ∞, t > 0,
u(0, t) = t2
t > 0,
u(x, 0) = x2
0 ≤x < ∞,
ut(x, 0) = 6x
0 ≤x < ∞,
and evaluate u(4, 1) and u(1, 4).
4.3 Consider the problem
utt −4uxx = 0
−∞< x < ∞, t > 0,
u(x, 0) =
 1 −x2
|x| ≤1,
0
otherwise,
ut(x, 0) =
 4
1 ≤x ≤2,
0
otherwise.
(a) Using the graphical method, ﬁnd u(x, 1).
(b) Find limt→∞u(5, t).
(c) Find the set of all points where the solution is singular (nonclassical).
(d) Find the set of all points where the solution is not continuous.
4.4 (a) Solve the following initial boundary value problem for a vibrating semi-inﬁnite
string which is ﬁxed at x = 0:
utt −uxx = 0
0 < x < ∞, t > 0,
u(0, t) = 0
t > 0,
u(x, 0) = f (x)
0 ≤x < ∞,
ut(x, 0) = g(x)
0 ≤x < ∞,
where f ∈C2([0, ∞)) and g ∈C1([0, ∞)) satisfy the compatibility conditions
f (0) = f ′′(0) = g(0) = 0.
Hint Extend the functions f and g as odd functions ˜f and ˜g over the real line. Solve
the Cauchy problem with initial data ˜f and ˜g, and show that the restriction of this
solution to the half-plane x ≥0 is a solution of the problem. Recall that the solution
of the Cauchy problem with odd data is odd. In particular, the solution with odd data
is zero for x = 0 and all t ≥0.
(b) Solve the problem with f (x) = x3 + x6, and g(x) = sin2x, and evaluate u(1, i)
for i = 1, 2, 3. Is the solution classical?
4.5 Consider the problem
utt −uxx = 0
−∞< x < ∞, t > 0,
u(x, 0) =
 8x −2x2
0 ≤x ≤4,
0
otherwise,
ut(x, 0) =
 16
0 ≤x ≤4,
0
otherwise.

94
The one-dimensional wave equation
t
x
B+
A−
B−
A+
D
Figure 4.5 A drawing for the parallelogram identity.
(a) Find a formula for the forward and backward waves.
(b) Using the graphical method, draw the graph of u(x, i) for i = 4, 8, 12.
(c) Find u(±5, 2), u(±3, 4).
(d) Find limt→∞u(5, t).
4.6 (a) Solve the following initial boundary value problem for a vibrating semi-inﬁnite
string with a free boundary condition:
utt −uxx = 0
0 < x < ∞, t > 0,
ux(0, t) = 0
t > 0,
u(x, 0) = f (x)
0 ≤x < ∞,
ut(x, 0) = g(x)
0 ≤x < ∞,
where f ∈C2([0, ∞)) and g ∈C1([0, ∞)) satisfy the compatibility conditions
f ′
+(0) = g′
+(0) = 0.
Hint Extend the functions f and g as even functions ˜f and ˜g on the line. Solve the
Cauchy problem with initial data ˜f and ˜g, and show that the restriction of this solution
to the half-plane x ≥0 is a solution of the problem.
(b) Solve the problem with f (x) = x3 + x6, g(x) = sin3x, and evaluate u(1, i) for
i = 1, 2, 3. Is the solution classical?
4.7 (a)Letu(x, t)beasolutionofthewaveequationutt −uxx = 0inadomain D ⊂R2. Let
a, b be real numbers such that the parallelogram with vertices A± = (x0 ± a, t0 ± b),
B± = (x0 ± b, t0 ± a) is contained in D (see Figure 4.5). Prove the parallelogram
identity:
u(x0 −a, t0 −b) + u(x0 + a, t0 + b) = u(x0 −b, t0 −a) + u(x0 + b, t0 + a).
(b) Derive the corresponding identity when the wave speed c ̸= 1.
(c)Usingtheparallelogramidentity,solvethefollowinginitialboundaryvalueproblem
for a vibrating semi-inﬁnite string with a nonhomogeneous boundary condition:
utt −uxx = 0
0 < x < ∞, t > 0,
u(0, t) = h(t)
t > 0,
u(x, 0) = f (x)
0 ≤x < ∞,
ut(x, 0) = g(x)
0 ≤x < ∞,
where f, g, h ∈C2([0, ∞)).

4.6 Exercises
95
Hint Distinguish between the cases x −t > 0 and x −t ≤0.
(d) From the explicit formula that was obtained in part (c), derive the corresponding
compatibility conditions, and prove that the problem is well-posed.
(e) Derive an explicit formula for the solution and deduce the corresponding compat-
ibility conditions for the case c ̸= 1.
4.8 Solve the following initial boundary value problem using the parallelogram identity
utt −uxx = 0
0 < x < ∞, 0 < t < 2x,
u(x, 0) = f (x)
0 ≤x < ∞,
ut(x, 0) = g(x)
0 ≤x < ∞,
u(x, 2x) = h(x)
x ≥0,
where f, g, h ∈C2([0, ∞)).
4.9 Solve the problem
utt −uxx = 1
−∞< x < ∞, t > 0,
u(x, 0) = x2
−∞< x < ∞,
ut(x, 0) = 1
−∞< x < ∞.
4.10 (a) Solve the Darboux problem:
utt −uxx = 0
t > max{−x, x}, t ≥0,
u(x, t) =
 φ(t)
x = t, t ≥0,
ψ(t)
x = −t, t ≥0,
where φ, ψ ∈C2([0, ∞) satisﬁes φ(0) = ψ(0).
(b) Prove that the problem is well posed.
4.11 A pressure wave generated as a result of an explosion satisﬁes the equation
Ptt −16Pxx = 0
in the domain {(x, t) | −∞< x < ∞, t > 0}, where P(x, t) is the pressure at the
point x and time t. The initial conditions at the explosion time t = 0 are
P(x, 0) =
 10
|x| ≤1,
0
|x| > 1,
Pt(x, 0) =
 1
|x| ≤1,
0
|x| > 1.
A building is located at the point x0 = 10. The engineer who designed the building
determined that it will sustain a pressure up to P = 6. Find the time t0 when the
pressure at the building is maximal. Will the building collapse?
4.12 (a) Solve the problem
utt −uxx = 0
0 < x < ∞, 0 < t,
u(0, t) =
t
1 + t
0 ≤t,
u(x, 0) = ut(x, 0) = 0
0 ≤x < ∞.

96
The one-dimensional wave equation
(b) Show that the limit
lim
x→∞u(cx, x) := φ(c)
exists for all c > 0. What is the limit?
4.13 Consider the Cauchy problem
utt −4uxx = F(x, t)
−∞< x < ∞, t > 0,
u(x, 0) = f (x),
ut(x, 0) = g(x)
−∞< x < ∞,
where
f (x) =



x
0 < x < 1,
1
1 < x < 2,
3 −x
2 < x < 3,
0
x > 3, x < 0,
g(x) =
 1 −x2
|x| < 1,
0
|x| > 1,
and F(x, t) = −4ex on t > 0, −∞< x < ∞.
(a) Is the d’Alembert solution of the problem a classical solution? If your answer is
negative, ﬁnd all the points where the solution is singular.
(b) Evaluate the solution at (1, 1).
4.14 Solve the problem
utt −4uxx = ex + sin t
−∞< x < ∞, t > 0,
u(x, 0)
= 0
−∞< x < ∞,
ut(x, 0)
=
1
1 + x2
−∞< x < ∞.
4.15 Find the general solution of the problem
uttx −uxxx = 0, ux(x, 0) = 0, uxt(x, 0) = sin x,
in the domain {(x, t) | −∞< x < ∞, t > 0}.
4.16 Solve the problem
utt −uxx = xt
−∞< x < ∞, t > 0,
u(x, 0)
= 0
−∞< x < ∞,
ut(x, 0)
= ex
−∞< x < ∞.
4.17 (a) Without using the d’Alembert formula ﬁnd a solution u(x, t) of the problem
utt −uxx = cos(x + t)
−∞< x < ∞, t > 0,
u(x, 0) = x , ut(x, 0) = sin x
−∞< x < ∞.
(b) Without using the d’Alembert formula ﬁnd v(x, t) that is a solution of the problem
vtt −vxx = cos(x + t)
−∞< x < ∞, t > 0,
v(x, 0) = 0 , vt(x, 0) = 0
−∞< x < ∞.

4.6 Exercises
97
(c) Find the PDE and initial conditions that are satisﬁed by the function w := u −v.
(d) Which of the functions u, v, w (as a function of x) is even? Odd? Periodic?
(e) Evaluate v(2π, π), w(0, π).
4.18 Solve the problem
utt −4uxx = 6t
−∞< x < ∞, t > 0,
u(x, 0) = x
−∞< x < ∞,
ut(x, 0) = 0
−∞< x < ∞,
without using the d’Alembert formula.
4.19 Let u(x, t) be a solution of the equation utt −uxx = 0 in the whole plane. Suppose
that ux(x, t) is constant on the line x = 1 + t. Assume also that u(x, 0) = 1 and
u(1, 1) = 3. Find such a solution u(x, t). Is this solution uniquely determined?

5
The method of separation of variables
5.1 Introduction
We examined in Chapter 1 Fourier’s work on heat conduction. In addition to de-
veloping a general theory for heat ﬂow, Fourier discovered a method for solving
the initial boundary value problem he derived. His solution led him to propose
the bold idea that any real valued function deﬁned on a closed interval can be
represented as a series of trigonometric functions. This is known today as the
Fourier expansion. D’Alembert and the Swiss mathematician Daniel Bernoulli
(1700–1782) had actually proposed a similar idea before Fourier. They claimed
that the vibrations of a ﬁnite string can be formally represented as an inﬁnite series
involving sinusoidal functions. They failed, however, to see the generality of their
observation.
Fourier’s method for solving the heat equation provides a convenient method
that can be applied to many other important linear problems. The method also en-
ables us to deduce several properties of the solutions, such as asymptotic behavior,
smoothness, and well-posedness. Historically, Fourier’s idea was a breakthrough
which paved the way for new developments in science and technology. For exam-
ple, Fourier analysis found many applications in pure mathematics (number theory,
approximation theory, etc.). Several fundamental theories in physics (quantum me-
chanics in particular) are heavily based on Fourier’s idea, and the entire theory of
signal processing is based on Fourier’s method and its generalizations.
Nevertheless, Fourier’s method cannot always be applied for solving linear dif-
ferential problems. The method is applicable only for problems with an appropriate
symmetry. Moreover, the equation and the domain should share the same symmetry,
and in most cases the domain should be bounded. Another drawback follows from
the representation of the solution as an inﬁnite series. In many cases it is not easy
to prove that the formal solution given by this method is indeed a proper solution.
Finally, even in the case when one can prove that the series converges to a classical
98

5.2 Heat equation
99
solution, it might happen that the rate of convergence is very slow. Therefore, such
a representation of the solution may not always be practical.
Fourier’s method for solving linear PDEs is based on the technique of separation
of variables. Let us outline the main steps of this technique. First we search for
solutions of the homogeneous PDE that are called product solutions (or separated
solutions). These solutions have the special form
u(x, t) = X(x)T (t),
and in general they should satisfy certain additional conditions. In many cases, these
additional conditions are just homogeneous boundary conditions. It turns out that
X and T should be solutions of linear ODEs that are easily derived from the given
PDE. In the second step, we use a generalization of the superposition principle
to generate out of the separated solutions a more general solution of the PDE, in
the form of an inﬁnite series of product solutions. In the last step we compute the
coefﬁcients of this series.
Since the separation of variables method relies on several deep ideas and also
involves several technical steps, we present in the current chapter the technique for
solving several relatively simple problems without much theoretical justiﬁcation.
The theoretical study is postponed to Chapter 6. Since Fourier’s method is based on
constructing solutions of a speciﬁc type, we introduce towards the end of the chapter
the energy method, which is used to prove that the solutions we have constructed
are indeed unique.
5.2 Heat equation: homogeneous boundary conditions
Consider the following heat conduction problem in a ﬁnite interval:
ut −kuxx = 0
0 < x < L, t > 0,
(5.1)
u(0, t) = u(L, t) = 0
t ≥0,
(5.2)
u(x, 0) = f (x)
0 ≤x ≤L,
(5.3)
where f is a given initial condition, and k is a positive constant. In order to make
(5.2) consistent with (5.3), we assume the compatibility condition
f (0) = f (L) = 0.
The equation and the domain are drawn schematically in Figure 5.1
The problem deﬁned above corresponds to the evolution of the temperature
u(x, t) in a homogeneous one-dimensional heat conducting rod of length L (i.e. the
rod is narrow and is laterally insulated) whose initial temperature (at time t = 0)
is known and is such that its two ends are immersed in a zero temperature bath.

100
The method of separation of variables
u(x,0) = f(x)
L
t
x
ut − kuxx = 0
u = 0
u = 0
Figure 5.1 The initial boundary value problem for the heat equation together with
the domain.
We assume that there is no internal source that heats (or cools) the system. Note
that the problem (5.1)–(5.3) is an initial boundary value problem that is linear and
homogeneous. Recall also that the boundary condition (5.2) is called the Dirichlet
condition. At the end of the present section, we shall also discuss other boundary
conditions.
We start by looking for solutions of the PDE (5.1) that satisfy the boundary
conditions (5.2), and have the special form
u(x, t) = X(x)T (t),
(5.4)
where X and T are functions of the variables x and t, respectively. At this step we
do not take into account the initial condition (5.3). Obviously, we are not interested
in the zero solution u(x, t) = 0. Therefore, we seek functions X and T that do not
vanish identically.
Differentiate the separated solution (5.4) once with respect to t and twice with
respect to x and substitute these derivatives into the PDE. We then obtain
XTt = kXxxT.
Now, we carry out a simple but decisive step – the separation of variables step.
We move to one side of the PDE all the functions that depend only on x and to the
other side the functions that depend only on t. We thus write
Tt
kT = Xxx
X .
(5.5)
Since x and t are independent variables, differentiating (5.5) with respect to t
implies that there exists a constant denoted by λ (which is called the separation
constant) such that
Tt
kT = Xxx
X
= −λ.
(5.6)

5.2 Heat equation
101
Equation (5.6) leads to the following system of ODEs:
d2X
dx2 = −λX
0 < x < L,
(5.7)
dT
dt = −λkT
t > 0,
(5.8)
which are coupled only by the separation constant λ. The function u satisﬁes the
boundary conditions (5.2) if and only if
u(0, t) = X(0)T (t) = 0,
u(L, t) = X(L)T (t) = 0.
Since u is not the trivial solution u = 0, it follows that
X(0) = X(L) = 0.
Therefore, the function X should be a solution of the boundary value problem
d2X
dx2 + λX = 0
0 < x < L,
(5.9)
X(0) = X(L) = 0.
(5.10)
Consider the system (5.9)–(5.10). A nontrivial solution of this system is called
an eigenfunction of the problem with an eigenvalue λ. The problem (5.9)–(5.10)
is called an eigenvalue problem. The boundary condition (5.10) is called (as in the
PDE case) the Dirichlet boundary condition.
Note that the problem (5.9)–(5.10) is not an initial boundary problem for an ODE
(for which it is known that there exists a unique solution). Rather, it is a boundary
value problem for an ODE. It is not clear a priori that there exists a solution for any
value of λ. On the other hand, if we can write the general solution of the ODE for
every λ, then we need only to check for which λ there exists a solution that also
satisﬁes the boundary conditions.
Fortunately, (5.9) is quite elementary. It is a second-order linear ODE with con-
stant coefﬁcients, and its general solution (which depends on λ) has the following
form:
1. if λ < 0, then X(x) = αe
√−λx + βe−√−λx,
2. if λ = 0, then X(x) = α + βx,
3. if λ > 0, then X(x) = α cos(
√
λx) + β sin(
√
λx),
where α, β are arbitrary real numbers.
We implicitly assume that λ is real, and we do not consider the complex case
(although this case can, in fact, be treated similarly). In Chapter 6, we show that
the system (5.9)–(5.10) does not admit a solution with a nonreal λ. In other words,
all the eigenvalues of the problem are real numbers.

102
The method of separation of variables
Negative eigenvalue (λ < 0) The general solution can be written in a more conve-
nient form: instead of choosing the two exponential functions as the fundamental
system of solutions, we use the basis {sinh(√−λx), cosh(√−λx)}. In this basis,
the general solution for λ < 0 has the form
X(x) = ˜α cosh(
√
−λx) + ˜β sinh(
√
−λx).
(5.11)
The function sinh s has a unique root at s = 0, while cosh s is a strictly positive
function. Since X(x) should satisfy X(0) = 0, it follows ˜α = 0. The second bound-
ary condition X(L) = 0 implies that ˜β = 0. Hence, X(x) ≡0 is the trivial solution.
In other words, the system (5.9)–(5.10) does not admit a negative eigenvalue.
Zero eigenvalue (λ = 0) We claim that λ = 0 is also not an eigenvalue. Indeed,
in this case the general solution is a linear function X(x) = α + βx that (in the
nontrivial case X ̸= 0) vanishes at most at one point; thus it cannot satisfy the
boundary conditions (5.10).
Positive eigenvalue (λ > 0) The general solution for λ > 0 is
X(x) = α cos(
√
λx) + β sin(
√
λx).
(5.12)
Substituting this solution into the boundary condition X(0) = 0, we obtain α = 0.
The boundary condition X(L) = 0 implies sin(
√
λL) = 0. Therefore,
√
λL = nπ,
where n a positive integer. We do not have to consider the case n < 0, since it
corresponds to the same set of eigenvalues and eigenfunctions. Hence, λ is an
eigenvalue if and only if
λ =
nπ
L
2
n = 1, 2, 3, . . . .
The corresponding eigenfunctions are
X(x) = sin nπx
L ,
and they are uniquely deﬁned up to a multiplicative constant.
In conclusion, the set of all solutions of problem (5.9)–(5.10) is an inﬁnite
sequence of eigenfunctions, each associated with a positive eigenvalue. It is con-
venient to use the notation
Xn(x) = sin nπx
L ,
λn =
nπ
L
2
n = 1, 2, 3, . . . .
Recall from linear algebra that an eigenvalue has multiplicity m if the space
consisting of its eigenvectors is m-dimensional. An eigenvalue with multiplicity 1
is called simple. Using the same terminology, we see that the eigenvalues λn for the
eigenvalue problem (5.9)–(5.10) are all simple.

5.2 Heat equation
103
Let us deal now with the ODE (5.8). The general solution has the form
T (t) = Be−kλt.
Substituting λn, we obtain
Tn(t) = Bne−k( nπ
L )2t
n = 1, 2, 3, . . . .
(5.13)
From the physical point of view it is clear that the solution of (5.8) must decay in
time, hence, we must have λ > 0. Therefore, we could have guessed a priori that
the problem (5.9)–(5.10) would admit only positive eigenvalues.
We have thus obtained the following sequence of separated solutions
un(x, t) = Xn(x)Tn(t) = Bn sin nπx
L e−k( nπ
L )2t
n = 1, 2, 3, . . . .
(5.14)
The superposition principle implies that any linear combination
u(x, t) =
N

n=1
Bn sin nπx
L e−k( nπ
L )2t
(5.15)
ofseparatedsolutionsisalsoasolutionoftheheatequationthatsatisﬁestheDirichlet
boundary conditions.
Consider now the initial condition. Suppose it has the form
f (x) =
N

n=1
Bn sin nπx
L ,
i.e. it is a linear combination of the eigenfunctions. Then a solution of the heat
problem (5.1)–(5.3) is given by
u(x, t) =
N

n=1
Bn sin nπx
L e−k( nπ
L )2t.
Hence, we are able to solve the problem for a certain family of initial conditions.
It is natural to ask at this point how to solve for more general initial conditions?
The brilliant (although not fully justiﬁed at that time) idea of Fourier was that it is
possible to represent an arbitrary function f that satisﬁes the boundary conditions
(5.2) as a unique inﬁnite “linear combination” of the eigenfunctions sin(nπx/L).
In other words, it is possible to ﬁnd constants Bn such that
f (x) =
∞

n=1
Bn sin nπx
L .
(5.16)
Such a series is called a (generalized) Fourier series (or expansion) of the function
f with respect to the eigenfunctions of the problem, and Bn, n = 1, 2 . . . are called
the (generalized) Fourier coefﬁcients of the series.

104
The method of separation of variables
The last ingredient that is needed for solving the problem is called the generalized
superposition principle. We generalize the superposition principle and apply it also
to an inﬁnite series of separated solutions. We call such a series a generalized
solution of the PDE if the series is uniformly converging in every subrectangle that
is contained in the domain where the solution is deﬁned. This deﬁnition is similar
to the deﬁnition of generalized solutions of the wave equation that was given in
Chapter 4.
In our case the generalized superposition principle implies that the formal ex-
pression
u(x, t) =
∞

n=1
Bn sin nπx
L e−k( nπ
L )2t
(5.17)
is a natural candidate for a generalized solution of problem (5.1)–(5.3). By a ‘formal
solution’ we mean that if we ignore questions concerning convergence, continuity,
and smoothness, and carry out term-by-term differentiations and substitutions, then
we see that all the required conditions of the problem (5.1)–(5.3) are satisﬁed.
Before proving that under certain conditions (5.17) is indeed a solution, we need
to explain how to represent an ‘arbitrary’ function f as a Fourier series. In other
words, we need a method of ﬁnding the Fourier coefﬁcients of a given function f .
Surprisingly, this question can easily be answered under the assumption that the
Fourier series of f converges uniformly. Fix m ∈N, multiply the Fourier expansion
(5.16) by the eigenfunction sin(mπx/L), and then integrate the equation term-by-
term over [0, L]. We get
 L
0
sin mπx
L
f (x) dx =
∞

n=1
Bn
 L
0
sin mπx
L
sin nπx
L
dx.
(5.18)
It is easily checked (see Section A.1) that
 L
0
sin mπx
L
sin nπx
L
dx =
0
m ̸= n,
L/2 m = n.
(5.19)
Therefore, the Fourier coefﬁcients are given by
Bm =
 L
0 sin(mπx/L) f (x) dx
 L
0 sin2(mπx/L) dx
= 2
L
 L
0
sin mπx
L
f (x) dx ,
m = 1, 2, . . . .
(5.20)
In particular, it follows that the Fourier coefﬁcients and the Fourier expansion
of f are uniquely determined. Therefore, (5.17) together with (5.20) provides an
explicit formula for a (formal) solution of the heat problem. Notice that we have

5.2 Heat equation
105
developed a powerful tool! For a given initial condition f , one only has to compute
the corresponding Fourier coefﬁcients in order to obtain an explicit solution.
Example 5.1 Consider the problem:
ut −uxx = 0
0 < x < π, t > 0,
(5.21)
u(0, t) = u(π, t) = 0
t ≥0,
(5.22)
u(x, 0) = f (x) =
 x
0 ≤x ≤π/2,
π −x
π/2 ≤x ≤π.
(5.23)
The formal solution is
u(x, t) =
∞

m=1
Bm sin mxe−m2t,
(5.24)
where
Bm = 2
π
 π
0
f (x) sin mx dx
= 2
π
 π/2
0
x sin mx dx + 2
π
 π
π/2
(π −x) sin mx dx
= 2
π
−x cos mx
m
+ sin mx
m2
π/2
0
+ 2
π
−(π −x) cos mx
m
−sin mx
m2
π
π/2
=
4
πm2 sin mπ
2 .
But
sin mπ
2
=
0
m = 2n,
(−1)n+1
m = 2n −1,
(5.25)
where n = 1, 2, . . .. Therefore, the formal solution is
u(x, t) =
∞

n=1
un(x, t) = 4
π
∞

n=1
(−1)n+1
(2n −1)2 sin[(2n −1)x]e−(2n−1)2t.
(5.26)
We claim that under the assumption that the Fourier expansion converges to f ,
the series (5.26) is indeed a classical solution. To verify this statement we assume
f (x) = 4
π
∞

n=1
(−1)n+1
(2n −1)2 sin[(2n −1)x].
(5.27)
The functions obtained by summing only ﬁnitely many terms in the Fourier series
are depicted in Figure 5.2.

106
The method of separation of variables
1.3
1.4
1.5
1.6
1.7
1.8
1.3
1.35
1.4
1.45
1.5
1.55
1.6
1.65
f
x
1.25
Figure 5.2 The function obtained by summing 100 terms (solid line) and 7 terms
(dotted line) for the Fourier expansion of f (x) (5.27). We concentrate the region
near the point x = π/2, where f (x) is not differentiable. If we take just a few
terms in the expansion, the actual singularity is smoothed out.
Since
|un(x, t)| = 4
π

(−1)n+1
(2n −1)2 sin[(2n −1)x]e−(2n−1)2t
 ≤
4
π(2n −1)2 ,
it follows by the Weierstrass M-test that the series (5.26) converges uniformly to a
continuous function in the region
{(x, t) | 0 ≤x ≤π, t ≥0}.
Substituting u into the initial and boundary conditions, and using the assumption
that the Fourier expansion of f converges to f , we obtain that these conditions are
indeed satisﬁed.
It remains to show that the series (5.26) is differentiable with respect to t, twice
differentiable with respect to x, and satisﬁes the heat equation in the domain
D := {(x, t) | 0 < x < π, t > 0}.
Fix ε > 0. We ﬁrst show that the series (5.26) is differentiable with respect
to t, twice differentiable with respect to x, and satisﬁes the heat equation in the
subdomain
Dε := {(x, t) | 0 < x < π, t > ε}.

5.2 Heat equation
107
For instance, we show that (5.26) can be differentiated with respect to t for t > ε.
Indeed, by differentiating un(x, t) with respect to t, we obtain that
|(un(x, t))t|=

4(2n −1)2
π(2n −1)2 sin[(2n −1)x]e−(2n−1)2t
≤4
π e−(2n−1)2ε.
Since the series (4/π)  e−(2n−1)2ε converges, it follows by the Weierstrass M-
test that for every ε > 0 the series (un(x, t))t converges to ut uniformly in Dε.
Similarly, it can be shown that u has a continuous second-order derivative with
respect to x that is obtained by two term-by-term differentiations. Hence,
ut −uxx =
∞

n=1
(un)t −
∞

n=1
(un)xx =
∞

n=1
{(un)t −(un)xx} = 0,
where in the last step we used the property that each separated solution un(x, t) is
a solution of the heat equation. Thus, u is a solution of the PDE in Dε. Since ε is
an arbitrary positive number, it follows that u is a solution of the heat equation in
the domain D.
Because the general term un decays exponentially in Dε, it is possible to differ-
entiate (5.26) term-by-term to any order with respect to x and t. The corresponding
series converges uniformly in Dε to the appropriate derivative. Note that k differ-
entiations with respect to x and ℓdifferentiations with respect to t contribute to the
general term of the series a factor of order O(nk+2ℓ), but because of the exponential
term, the corresponding series is converging.
The important conclusion is that even for nonsmooth initial condition f , the
solution has inﬁnitely many derivatives with respect to x and t and it is smooth
in the strip D. The nonsmoothness of the initial data disappears immediately (see
Figure 5.3). This smoothing effect is known to hold also in more general parabolic
problems, in contrast with the hyperbolic case, where singularities propagate along
characteristics and in general persist over time.
Another qualitative result that can be deduced from our representation, concerns
the large time behavior of the solution (i.e. the behavior in the limit t →∞). This
behavior is directly inﬂuenced by the boundary conditions. In particular, it depends
on the minimal eigenvalue of the corresponding eigenvalue problem. In our case, all
the eigenvalues are strictly positive, and from (5.17) and the uniform convergence
in Dε it follows that
lim
t→∞u(x, t) = 0
∀0 ≤x ≤L.
Hence the temperature along the rod converges to the temperature that is imposed
at the end points.

108
The method of separation of variables
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
u(x,t)
x
t = 0 
t = 0.1
t = 1
Figure 5.3 The function u(x, t) of (5.26) for t = 0, t = 0.1, and t = 1. Notice that
the singularity at t = 0 is quickly smoothed out. The graphs were generated with
200 terms in the Fourier expansion. Actually just three or four terms are needed
to capture u correctly even for t = 0.1.
We conclude this section by mentioning other boundary conditions that appear
frequently in heat conduction problems (see Chapter 1). Speciﬁcally, we distinguish
between two types of boundary conditions:
(a) Separated boundary conditions These boundary conditions can be written as
B0[u] = αu(0, t) + βux(0, t) = 0,
BL[u] = γ u(L, t) + δux(L, t) = 0
t ≥0,
where
α, β, γ, δ ∈R,
|α| + |β| > 0,
|γ | + |δ| > 0.
This type of boundary condition includes for α = γ = 1, β = δ = 0 the Dirichlet
boundary condition
u(0, t) = u(L, t) = 0
t ≥0,
which is also called a boundary condition of the ﬁrst kind. Also, for α = γ =
0, β = δ = 1 we obtain
ux(0, t) = ux(L, t) = 0
t ≥0,
which is called the Neumann condition or a boundary condition of the second kind.
Recall that the physical interpretation of the Neumann boundary condition for heat

5.3 Wave equation
109
problems is that there is no heat ﬂow through the boundary. In our case it means that
the rod is insulated. If we impose a Dirichlet condition at one end, and a Neumann
condition at the other hand, then the boundary condition is called mixed. In the gen-
eral case, where α, β, γ, δ are nonzero, the boundary condition is called a boundary
condition of the third kind (or the Robin condition). The physical interpretation is
that the heat ﬂow at the boundary depends linearly on the temperature.
(b) Periodic boundary condition This boundary condition is imposed for example
in the case of heat evolution along a circular wire of length L. Clearly, in this case
the temperature u(x, t) and all its derivatives are periodic (as a function of x) with a
period L. In addition u satisﬁes the heat equation on (0, L). The boundary conditions
for this problem are
u(0, t) = u(L, t),
ux(0, t) = ux(L, t)
∀t ≥0.
The periodicity of all the higher-order derivatives follows from the PDE and the
boundary conditions presented above.
5.3 Separation of variables for the wave equation
We now apply the method of separation of variables to solve the problem of a
vibrating string without external forces and with two clamped but free ends. Let
u(x, t) be the amplitude of the string at the point x and time t, and let f and g
be the amplitude and the velocity of the string at time t = 0 (see the discussion in
Chapter 1 and, in particular, Figure 1.1). We need to solve the problem
utt −c2uxx = 0
0 < x < L, t > 0,
(5.28)
ux(0, t) = ux(L, t) = 0
t ≥0,
(5.29)
u(x, 0) = f (x)
0 ≤x ≤L,
(5.30)
ut(x, 0) = g(x)
0 ≤x ≤L,
(5.31)
where f, g are given functions and c is a positive constant. The compatibility
conditions are given by
f ′(0) = f ′(L) = g′(0) = g′(L) = 0.
The problem (5.28)–(5.31) is a linear homogeneous initial boundary value
problem. As mentioned above, the conditions (5.29) are called Neumann boundary
conditions.
Recall that at the ﬁrst stage of the method, we compute nontrivial separated
solutions of the PDE (5.28), i.e. solutions of the form
u(x, t) = X(x)T (t),
(5.32)

110
The method of separation of variables
that also satisfy the boundary conditions (5.29). Here, as usual, X, T are functions
of the variables x and t respectively. At this stage, we do not take into account the
initial conditions (5.30)–(5.31).
Differentiating the separated solution (5.32) twice in x and twice in t, and then
substituting these derivatives into the wave equation, we infer
XTtt = c2XxxT.
By separating the variables, we see that
Ttt
c2T = Xxx
X .
(5.33)
It follows that there exists a constant λ such that
Ttt
c2T = Xxx
X
= −λ.
(5.34)
Equation (5.34) implies
d2X
dx2 = −λX
0 < x < L,
(5.35)
d2T
dt2 = −λc2T
t > 0.
(5.36)
The boundary conditions (5.29) for u imply
ux(0, t) = dX
dx (0)T (t) = 0,
ux(L, t) = dX
dx (L)T (t) = 0.
Since u is nontrivial it follows that
dX
dx (0) = dX
dx (L) = 0.
Therefore, the function X should be a solution of the eigenvalue problem
d2X
dx2 + λX = 0
0 < x < L,
(5.37)
dX
dx (0) = dX
dx (L) = 0.
(5.38)
This eigenvalue problem is also called the Neumann problem.
We have already written the general solution of the ODE (5.37):
1. if λ < 0, then X(x) = α cosh(√−λx) + β sinh(√−λx),
2. if λ = 0, then X(x) = α + βx,
3. if λ > 0, then X(x) = α cos(
√
λx) + β sin(
√
λx),
where α, β are arbitrary real numbers.

5.3 Wave equation
111
Negative eigenvalue (λ < 0) The ﬁrst boundary condition (dX/dx)(0) = 0 im-
plies that β = 0. Then (dX/dx)(L) = 0 implies that sinh(√−λL) = 0. Therefore,
X(x) ≡0 and the eigenvalue problem (5.37)–(5.38) does not admit negative eigen-
values.
Zero eigenvalue (λ = 0) The general solution is a linear function X(x) = α + βx.
Substituting this solution into the boundary conditions (5.38) implies that λ = 0 is
an eigenvalue with a unique eigenfunction X0(x) ≡1 (the eigenfunction is unique
up to a multiplicative factor).
Positive eigenvalue (λ > 0) The general solution for λ > 0 has the form
X(x) = α cos(
√
λx) + β sin(
√
λx).
(5.39)
Substituting it in (dX/dx)(0) = 0, we obtain β = 0. The boundary condition
(dX/dx)(L) = 0 implies now that sin(
√
λL) = 0. Thus
√
λL = nπ, where n ∈N.
Consequently, λ > 0 is an eigenvalue if and only if:
λ =
nπ
L
2
n = 1, 2, 3, . . . .
The associated eigenfunction is
X(x) = cos nπx
L ,
and it is uniquely determined up to a multiplicative factor.
Therefore, the solution of the eigenvalue problem (5.37)–(5.38) is an inﬁnite
sequence of nonnegative simple eigenvalues and their associated eigenfunctions.
We use the convenient notation:
Xn(x) = cos nπx
L ,
λn =
nπ
L
2
n = 0, 1, 2, . . . .
Consider now the ODE (5.36) for λ = λn. The solutions are
T0(t) = γ0 + δ0t,
(5.40)
Tn(t) = γn cos(

λnc2 t) + δn sin(

λnc2 t)
n = 1, 2, 3, . . . .
(5.41)
Thus, the product solutions of the initial boundary value problem are given by
u0(x, t) = X0(x)T0(t)= A0+B0t
2
,
(5.42)
un(x, t) = Xn(x)Tn(t)=cos nπx
L
	
An cos cπnt
L
+Bn sin cπnt
L

, n =1, 2, 3, . . . .
(5.43)

112
The method of separation of variables
Applying the (generalized) superposition principle, the expression
u(x, t) = A0 + B0t
2
+
∞

n=1
	
An cos cπnt
L
+ Bn sin cπnt
L

cos nπx
L
(5.44)
is a (generalized, or at least formal) solution of the problem (5.28)–(5.31). In
Exercise 5.2 we show that the solution (5.44) can be represented as a superpo-
sition of forward and backward waves. In other words, solution (5.44) is also a
generalized solution of the wave equation in the sense deﬁned in Chapter 4.
It remains to ﬁnd the coefﬁcients An, Bn in solution (5.44). Here we use the
initial conditions. Assume that the initial data f, g can be expanded into generalized
Fourier series with respect to the sequence of the eigenfunctions of the problem,
and that these series are uniformly converging. That is,
f (x) = a0
2 +
∞

n=1
an cos nπx
L ,
(5.45)
g(x) = ˜a0
2 +
∞

n=1
˜an cos nπx
L .
(5.46)
Again, the (generalized) Fourier coefﬁcients of f and g can easily be determined;
for m ≥0, we multiply (5.45) by the eigenfunction cos(mπx/L), and then we
integrate over [0, L]. We obtain
 L
0
cos mπx
L
f (x) dx = a0
2
 L
0
cos mπx
L
dx +
∞

n=1
an
 L
0
cos mπx
L
cos nπx
L
dx.
(5.47)
It is easily checked (see Section A.1) that
 L
0
cos mπx
L
cos nπx
L
dx =



0
m ̸= n,
L/2
m = n ̸= 0,
L
m = n = 0.
(5.48)
Therefore, the Fourier coefﬁcients of f with respect to the system of eigenfunctions
are
a0 = 2
 L
0 f (x) dx
 L
0 1 dx
= 2
L
 L
0
f (x) dx,
(5.49)
am =
 L
0 cos(mπx/L) f (x) dx
 L
0 cos2(mπx/L) dx
= 2
L
 L
0
cos mπx
L
f (x) dx
m = 1, 2, . . . .
(5.50)

5.3 Wave equation
113
The Fourier coefﬁcients ˜an of g can be computed similarly. Substituting t = 0 into
(5.44), and assuming that the corresponding series converges uniformly, we obtain
u(x, 0) = A0
2 +
∞

n=1
An cos nπx
L
= f (x) = a0
2 +
∞

n=1
an cos nπx
L .
Recallthatthe(generalized)Fouriercoefﬁcientsareuniquelydetermined,andhence
An = an for all n ≥0. In order to compute Bn, we differentiate (5.44) formally
(term-by-term) with respect to t and then substitute t = 0. We have
ut(x, 0) = B0
2 +
∞

n=1
Bn
cπn
L
cos nπx
L
= g(x) = ˜a0
2 +
∞

n=1
˜an cos nπx
L .
Therefore, Bn = ˜anL/cπn for all n ≥1. Similarly, B0 = ˜a0. Thus, the problem is
formally solved. The uniqueness issue will be discussed at the end of this chapter.
There is a signiﬁcant difference between the solution (5.17) of the heat problem
and the formal solution (5.44). Each term of the solution (5.17) of the heat equation
has a decaying exponential factor which is responsible for the smoothing effect
for t > 0. In (5.44) we have instead a (nondecaying) trigonometric factor. This is
related to the fact that hyperbolic equations preserve the singularities of the given
data since the rate of the decay of the generalized Fourier coefﬁcients to zero usually
depends on the smoothness of the given function (under the assumption that this
function satisﬁes the prescribed boundary conditions). The precise decay rate of
the Fourier coefﬁcients is provided for the classical Fourier system by the general
theory of Fourier analysis [13].
Example 5.2 Solve the problem
utt −4uxx = 0
0 < x < 1, t > 0,
ux(0, t) = ux(1, t) = 0
t ≥0,
u(x, 0) = f (x) = cos2 πx
0 ≤x ≤1,
ut(x, 0) = g(x) = sin2 πx cos πx
0 ≤x ≤1.
(5.51)
The solution of (5.51) was shown to have the form
u(x, t) = A0 + B0t
2
+
∞

n=1
(An cos 2nπt + Bn sin 2nπt) cos nπx.
(5.52)

114
The method of separation of variables
Substituting f into (5.52) implies
u(x, 0) = A0
2 +
∞

n=1
An cos nπx = cos2 πx.
(5.53)
The Fourier expansion of f is easily obtained using the trigonometric identity
cos2 πx = 1
2 + 1
2 cos 2πx. Since the Fourier coefﬁcients are uniquely determined,
it follows that
A0 = 1, A2 = 1
2, An = 0
∀n ̸= 0, 2.
(5.54)
By differentiating the solution with respect to t, and substituting ut(x, 0) into the
second initial condition, we obtain
ut(x, 0) = B0
2 +
∞

n=1
Bn2nπ cos nπx = sin2 πx cos πx.
(5.55)
Similarly, the Fourier expansion of g is obtained using the trigonometric identity
sin2 πx cos πx = 1
4 cos πx −1
4 cos 3πx. From the uniqueness of the expansion it
follows that
B1 = 1
8π , B3 = −1
24π , Bn = 0
∀n ̸= 1, 3.
Therefore,
u(x, t) = 1
2 + 1
8π sin 2πt cos πx + 1
2 cos 4πt cos 2πx −
1
24π sin 6πt cos 3πx.
(5.56)
Since (5.56) contains only a ﬁnite number of (smooth) terms, it is veriﬁed directly
that u is a classical solution of the problem.
5.4 Separation of variables for nonhomogeneous equations
It is possible to upgrade the method of separationof variablestoamethodforsolving
nonhomogeneous PDEs. This technique is called also the method of eigenfunction
expansion. For example, consider the problem
utt −uxx = cos 2πx cos 2πt
0 < x < 1, t > 0,
ux(0, t) = ux(1, t) = 0
t ≥0,
u(x, 0) = f (x) = cos2 πx
0 ≤x ≤1,
ut(x, 0) = g(x) = 2 cos 2πx
0 ≤x ≤1.
(5.57)
In the previous section we found the system of all eigenfunctions and the corre-
sponding eigenvalues of the homogeneous problem. They are
Xn(x) = cos nπx,
λn = (nπ)2
n = 0, 1, 2, . . . .

5.4 Nonhomogeneous equations
115
Recall Fourier’s claim (to be justiﬁed in the next chapter) that any reasonable
function satisfying the boundary conditions can be uniquely expanded into (gen-
eralized) Fourier series with respect to the system of the eigenfunctions of the
problem. Since the solution u(x, t) of the problem (5.57) is a twice differentiable
function satisfying the boundary conditions, it follows that for a ﬁxed t the solution
u can be represented as
u(x, t) = 1
2T0(t) +
∞

n=1
Tn(t) cos nπx,
(5.58)
where Tn(t) are the (time dependent) Fourier coefﬁcients of the function u(·, t).
Hence, we need to ﬁnd these coefﬁcients.
Substituting (5.58) into the wave equation (5.57) and differentiating the series
term-by-term implies that
1
2T0
′′ +
∞

n=1
(T ′′
n + n2π2Tn) cos nπx = cos 2πt cos 2πx.
(5.59)
Note that in the current example, the right hand side of the equation is already given
in the form of a Fourier series. The uniqueness of the Fourier expansion implies
that the Fourier coefﬁcients of the series of the left hand side of (5.59) are equal to
the Fourier coefﬁcients of the series of the right hand side. In particular, for n = 0
we obtain the ODE:
T ′′
0 = 0,
(5.60)
whose general solution is T0(t) = A0 + B0t. Similarly we obtain for n = 2
T ′′
2 + 4π2T2 = cos 2πt.
(5.61)
The general solution of this linear nonhomogeneous second-order ODE is
T2(t) = A2 cos 2πt + B2 sin 2πt + t
4π sin 2πt.
For n ̸= 0, 2, we have
T ′′
n + n2π2Tn = 0
∀n ̸= 0, 2.
(5.62)
The solution is Tn(t) = An cos nπt + Bn sin nπt for all n ̸= 0, 2. Substituting the
solutions of (5.60), (5.61), and (5.62) into (5.58) implies that the solution of the

116
The method of separation of variables
problem is of the form
u(x, t) = A0 + B0t
2
+ t
4π sin 2πt cos 2πx
+
∞

n=1
(An cos nπt + Bn sin nπt) cos nπx.
(5.63)
Substituting (5.63) into the ﬁrst initial condition (5.57), we get
u(x, 0) = A0
2 +
∞

n=1
An cos nπx = cos2 πx = 1
2 + 1
2 cos 2πx,
therefore,
A0 = 1,
A2 = 1
2,
An = 0 ∀n ̸= 0, 2.
By differentiating (term-by-term) the solution u with respect to t and substituting
ut(x, 0) into the second initial condition of (5.57), we ﬁnd
ut(x, 0) = B0
2 +
∞

n=1
nπ Bn cos nπx = 2 cos 2πx,
Hence,
B2 = 1
π , Bn = 0 ∀n ̸= 2.
Finally
u(x, t) = 1
2 +
	1
2 cos 2πt + t + 4
4π
sin 2πt

cos 2πx.
It is clear that this solution is classical, since the (generalized) Fourier series of
the solution has only a ﬁnite number of nonzero smooth terms, and therefore all
the formal operations are justiﬁed. Note that the amplitude of the vibrating string
grows linearly in t and it is unbounded as t →∞. This remarkable phenomenon
will be discusses further in Chapter 6.
5.5 The energy method and uniqueness
The energy method is a fundamental tool in the theory of PDEs. One of its main
applications is in proving the uniqueness of the solution of initial boundary value
problems. The method is based on the physical principle of energy conservation,
although in some applications the object we refer to mathematically as an ‘energy’
is not necessarily the actual energy of a physical system.

5.5 The energy method and uniqueness
117
Recall that in order to prove the uniqueness of solutions for a linear differential
problem, it is enough to show that the solution of the corresponding homoge-
neous PDE with homogeneous initial and boundary conditions is necessarily the
zero solution. This basic principle has already been used in Chapter 4 and will be
demonstrated again below.
Let us outline the energy method. For certain homogeneous problems it is possi-
ble to deﬁne an energy integral that is nonnegative and is a nonincreasing function
of the time t. In addition, for t = 0 the energy is zero and therefore, the energy
is zero for all t ≥0. Due to the positivity of the energy, and the zero initial and
boundary conditions it will follow that the solution is zero.
We demonstrate the energy method for the problems that have been studied in
the present chapter.
Example 5.3 Consider the Neumann problem for the vibrating string
utt −c2uxx = F(x, t)
0 < x < L, t > 0,
(5.64)
ux(0, t) = a(t),
ux(L, t) = b(t)
t ≥0,
(5.65)
u(x, 0) = f (x)
0 ≤x ≤L,
(5.66)
ut(x, 0) = g(x)
0 ≤x ≤L.
(5.67)
Let u1, u2 be two solutions of the problem. By the superposition principle, the
function w := u1 −u2 is a solution of the problem
wtt −c2wxx = 0
0 < x < L, t > 0,
(5.68)
wx(0, t) = 0,
wx(L, t) = 0
t ≥0,
(5.69)
w(x, 0) = 0
0 ≤x ≤L,
(5.70)
wt(x, 0) = 0
0 ≤x ≤L.
(5.71)
Deﬁne the total energy of the solution w at time t as
E(t) := 1
2
 L
0
(w2
t + c2w2
x) dx.
(5.72)
The ﬁrst term represents the total kinetic energy of the string, while the second term
is the total potential energy. Clearly, E′ is given by
E′(t) = d
dt
1
2
 L
0
(w2
t + c2w2
x) dx

=
 L
0
(wtwtt + c2wxwxt) dx.
(5.73)
But
c2wxwxt = c2
 ∂
∂x (wxwt) −wxxwt

= c2 ∂
∂x (wxwt) −wttwt.

118
The method of separation of variables
Substituting this identity into (5.73) and using the fundamental theorem of calculus,
we have
E′(t) = c2
 L
0
∂
∂x (wxwt) dx = c2(wxwt)|L
0 .
(5.74)
The boundary condition (5.69) implies that E′(t) = 0, hence, E(t) = constant and
the energy is conserved.
Ontheotherhand,sincefor t = 0wehavew(x, 0) = 0,itfollowsthat wx(x, 0) =
0. Moreover, we have also wt(x, 0) = 0. Therefore, the energy at time t = 0 is zero.
Thus, E(t) ≡0.
Since e(x, t) := w2
t + c2w2
x ≥0, and since its integral over [0, L] is zero, it
follows that w2
t + c2w2
x ≡0, which implies that wt(x, t) = wx(x, t) ≡0. Conse-
quently, w(x, t) ≡constant. By the initial conditions w(x, 0) = 0, hence w(x, t) ≡
0. This completes the proof of the uniqueness of the problem (5.64)–(5.67).
Example 5.4 Let us modify the previous problem a little, and instead of the (non-
homogeneous) Neumann problem, consider the Dirichlet boundary conditions:
u(0, t) = a(t),
u(L, t) = b(t)
t ≥0.
We use the same energy integral and follow the same steps. We obtain for the
function w
E′(t) = c2(wxwt)|L
0 .
(5.75)
Since w(0, t) = w(L, t) = 0, it follows that wt(0, t) = wt(L, t) = 0; therefore,
E′(t) = 0 and in this case too the energy is conserved. The rest of the proof is
exactly the same as in the previous example.
Example 5.5 The energy method can also be applied to heat conduction problems.
Consider the Dirichlet problem
ut −kuxx = F(x, t)
0 < x < L, t > 0,
(5.76)
u(0, t) = a(t),
u(L, t) = b(t)
t ≥0,
(5.77)
u(x, 0) = f (x)
0 ≤x ≤L.
(5.78)
As we explained above, we need to prove that if w is a solution of the homogeneous
problem with zero initial and boundary conditions, then w = 0. In the present case,
we deﬁne the energy to be:
E(t) := 1
2
 L
0
w2 dx.
(5.79)

5.6 Further applications of the heat equation
119
The time derivative E′ is given by
E′(t) = d
dt
	1
2
 L
0
w2 dx

=
 L
0
wwt dx =
 L
0
kwwxx dx.
(5.80)
Integrating by parts and substituting the boundary conditions, we have
E′(t) = kwwx|L
0 −
 L
0
k(wx)2 dx = −
 L
0
k(wx)2 dx ≤0,
therefore, the energy is not increasing. Since E(0) = 0 and E(t) ≥0, it follows
that E ≡0. Consequently, for all t ≥0 we have w(·, t) ≡0 and the uniqueness is
proved. The same proof can also be used for the Neumann problem and even for
the boundary condition of the third kind:
u(0, t) −αux(0, t) = a(t).
u(L, t) + βux(L, t) = b(t)
t ≥0,
provided that α, β ≥0.
5.6 Further applications of the heat equation
We have seen that the underlying property of the wave equation is to propagate
waves, while the heat equation smoothes out oscillations and discontinuities. In
this section we shall consider two speciﬁc applications of the heat equation that
concern signal propagation. In the ﬁrst application we shall show that a diffusion
mechanism can still transmit (to some extent) oscillatory data. In fact, diffusion
effects play an important role in one of the most important communication systems.
In the second example the goal will be to use the smoothing property of the heat
equation to dampen oscillations in the data.
5.6.1 The cable equation
The great success of the telegraph prompted businessmen and governments to lay an
underwater cable between France and Britain in 1850. It was realized, however, that
the transmission rate through this cable was very low. The British scientist William
Thomson(1824–1907)soughttoexplainthisphenomenon.Hismathematicalmodel
showed that the cable’s electrical capacity has a major effect on signal transmission.
We shall derive the equation for signal transmission in a cable, solve it, and then
explain Thomson’s analysis.
A cross section of the cable is shown in Figure 5.4. The cable is modeled as a
system of outer and inner conductors separated by an insulating layer. To simplify
the analysis we shall consider a two-dimensional model, using x to denote the

120
The method of separation of variables
outer conductor
inner conductor
insulator
Figure 5.4 The cross section of the cable.
Vo(x)
lo(x)
lsdx
cs
li(x)
ridx
li(x + dx)
Vi(x + dx)
Vi(x)
cs
rs
rs
lo(x + dx)
Vo(x + dx)
x
x
outer conductor
inner conductor
rodx
Figure 5.5 A longitudinal cross section of the cable.
longitudinal direction. A small segment of the longitudinal cross section is shown
in Figure 5.5. In this segment we see the local resistivity (ri in the inner conductor,
and ro in the outer conductor), while the insulator is modeled by a capacitor Cs and
a resistor rs in parallel. The transversal current in a horizontal element of length dx
is Isdx. Ohm’s law for the segment (x, x + dx) implies
Vi(x + dx) −Vi(x) = −Ii(x)ridx,
Vo(x + dx) −Vo(x) = −Io(x)rodx. (5.81)
In the limit dx →0 this becomes
∂Vi
∂x = −riIi(x),
∂Vo
∂x = −roIo(x).
(5.82)
Having balanced the voltage drop in the longitudinal direction, we proceed to
write the current conservation equation (Kirchhoff’s law). We have
Ii(x + dx) = Ii(x) + Is(x)dx,
Io(x + dx) = Io(x) −Is(x)dx.
(5.83)

5.6 Further applications of the heat equation
121
Again letting dx →0 we obtain
Is = ∂Ii
∂x = −∂Io
∂x .
(5.84)
Introducing the transinsulator potential V = Vi −Vo, we conclude from (5.82) that
−∂V
∂x = riIi −roIo.
(5.85)
Differentiating (5.85) by x, and using (5.84) we get
−Is =
1
ri + ro
∂2V
∂x2 .
(5.86)
It remains to understand the current Is. The contribution of the resistor rs is
−(1/rs)V . The current through a capacitor is given by [10] −Cs∂V /∂t, where Cs
denotes the capacitance. Therefore we ﬁnally obtain the (passive) cable equation
∂V
∂t = D ∂2V
∂x2 −βV
D =
1
Cs(ri + re),
β =
1
rsCs
.
(5.87)
Note that the capacitor gave rise to a diffusion-like term in the transport equation.
Equation (5.87) can be solved in a ﬁnite x interval by the separation of variables
method (see, for example, Exercise 5.10). In order to understand its use in commu-
nication, we shall assume that the transmitter is located at x = 0, and the receiver
is at an arbitrary location x up the line. Therefore we solve the cable equation for
a semi-inﬁnite interval. To ﬁx ideas, we formulate the following problem:
Vt = DVxx −βV
0 < x < ∞, −∞< t < ∞,
(5.88)
V (0, t) = A cos ωt
−∞< t < ∞,
(5.89)
V (x, t) →0
x →∞.
(5.90)
The problem (5.88)–(5.90) can be solved by a variant of the separation of variables
method. Our motivation is to seek a solution that will have propagation and oscil-
lation properties as in a wave equation, but also decay properties that are typical of

122
The method of separation of variables
a heat equation. Therefore we seek a solution of the form
V (x, t) = Av(x) cos(ωt −kx).
(5.91)
Substituting (5.91) into (5.88), and deﬁning φ := ωt −kx, we get
−ωv sin φ = D

vxx cos φ + 2kvx sin φ −k2v cos φ

−βv cos φ.
We ﬁrst equate the coefﬁcients of the cos φ term. This implies
vxx −
	
k2 + β
D

v = 0.
The boundary conditions (5.89)–(5.90) imply v(0) = 1, v(∞) = 0. Therefore
v(x) = exp[−

k2 + (β/D) x]. Equating now the coefﬁcients of the term sin φ,
using the solution that was found for v, we ﬁnd that (5.91) is indeed the desired
solution if k, ω, and D satisfy the dispersion relation
ω = 2Dk

k2 + β
D .
(5.92)
We now analyze this solution in light of the cable transmission issue. The param-
eter β represents the loss of energy due to the transinsulator resistivity. Increasing
the resistivity will decrease β. We therefore proceed to consider an ideal situation
where β = 0. In this case the solution (5.91) and the dispersion relation (5.92)
become
V (x, t) = Ae−kx cos(ωt −kx),
ω = 2Dk2.
(5.93)
The frequency ω can be interpreted as the rate of transmission. Similarly, we in-
terpret 1/k as the distance L between the transmitter and the receiver. Therefore
ω = 2DL−2. This formula enabled Thomson to predict that with the parameters
of the materials used for the cable, i.e. Cs,ri,ro that determine D, and in light of
the distance L, the transmission rate would be far below the expected rate. His
prediction was indeed fulﬁlled. Following the great success of his mathematical
analysis, Thomson was asked to consult in the next major attempt to lay an un-
derwater communication cable, this time in 1865 between Britain and the USA.
The great improvement in production control allowed the manufacture of a high
quality cable, and the enterprise met with high technical and ﬁnancial success. To
honor him for his contributions to the transatlantic cable, Thomson was created
Lord Kelvin in 1866.
Interest in the cable equation was renewed in the middle of the twentieth century
when it was discovered to be an adequate model for signal transmission in biological
cells in general, and for neurons in particular. The insulating layer in this case is the
cell’s membrane. The currents consist of ions, mainly potassium and sodium ions.

5.6 Further applications of the heat equation
123
In the biological applications, however, one needs to replace the passive resistor
rs with a nonlinear electrical element. The reason is that the current through the
cell’s membrane ﬂows in special channels with a complex gate mechanism that
was deciphered by Hodgkin and Huxley [8]. Moreover, we need to supplement in
this case the resulting active cable equation with further dynamical rules for the
channel gates.
As another example of the energy method we shall prove that the solution we
found for the cable equation is unique. More precisely, we shall prove that the
problem (5.88)–(5.90) has a unique solution in the class of functions for which the
energy
Ew(t) = E(t) := 1
2
 ∞
0
[w(x, t)]2 dx
(5.94)
is bounded. Namely, we assume that for each solution w of the problem there exists
a constant Mw > 0 such that Ew(t) ≤Mw. We need to prove that if w is a solution of
the homogeneous problem with zero boundary conditions, then w = 0. We obtain
as for the heat equation that
E′(t) = d
dt
	1
2
 ∞
0
w2 dx

=
 ∞
0
wwt dx =
 ∞
0
(Dwwxx −βw2) dx, (5.95)
where we have assumed that all the above integrals are ﬁnite. Integrating by parts
and substituting the boundary conditions, we have
E′(t) ≤−
 ∞
0
D(wx)2 dx −βE(t) ≤−βE(t).
Fixing T ∈R and integrating the above differential inequality from t to T > t, we
obtain the estimate
E(T ) ≤E(t)e−β(T −t) ≤Me−βT eβt.
Letting t →−∞it follows that E(T ) = 0. Therefore, E ≡0 which implies that
w = 0.
5.6.2 Wine cellars
Most types of foodstuff require good temperature control. A well-known example
is wine, which is stored in underground wine cellars. The idea is that a good layer of
soil will shield the wine from temperature ﬂuctuations with the seasons (and even
daily ﬂuctuations). Clearly very deep cellars will do this, but such cellars are costly
to build, and inconvenient to use and maintain. Therefore we shall use the solution
we found in the previous section for the heat equation in a semi-inﬁnite strip to

124
The method of separation of variables
estimate an adequate depth for a wine cellar. We consider the following model:
ut = Duxx
0 < x < ∞, −∞< t < ∞,
(5.96)
u(0, t) = T0 + A cos ωt
−∞< t < ∞,
(5.97)
u(x, t) →T0
x →∞.
(5.98)
Here the x coordinate measures the distance towards the earth center, where x = 0
is the earth’s surface, D is the earth’s diffusion coefﬁcient, and ω represents the
ground temperature ﬂuctuations about a ﬁxed temperature T0. For example, one
can take one year as the basic period, which implies ω = 0.19 × 10−6 s−1. Thanks
to the superposition principle and formula (5.93), we obtain the solution:
V (x, t) = T0 + Ae−kx cos(ωt −kx),
ω = 2Dk2.
(5.99)
How should formula (5.99) be used to choose the depth of the cellar? We have
already determined ω. The diffusion coefﬁcient D depends on the nature of the soil.
It can vary by a factor of 5 or more between dry soil and wet soil and rocks. For the
purpose of our model we shall assume an average value of 0.0025 cm2 s−1. The
ground temperature can ﬂuctuate by 20 ◦C. If we want to minimize the ﬂuctuation
in the cellar to less than 2 ◦C, say, we need to use a depth L such that e−kL = 0.1,
i.e. L = 3.7 m. A smarter choice for the depth L would be the criterion kL = π,
i.e. L = 5 m. This will provide two advantages. First, it gives a reduction in the
amplitude by a factor of 23, i.e. the ﬂuctuation will be less than 1 ◦C. Second, the
phase at this depth would be exactly opposite to the phase at zero ground level (with
respect to the ﬁxed temperature T0). This effect is desirable, since other mechanisms
of heat transfer, such as opening the door to the cellar, convection of heat by water,
etc. would then drive the temperature in the cellar further towards T0.
5.7 Exercises
5.1 Solve the equation
ut = 17uxx
0 < x < π, t > 0,
with the boundary conditions
u(0, t) = u(π, t) = 0
t ≥0,
and the initial conditions
u(x, 0) =
 0
0 ≤x ≤π/2,
2
π/2 < x ≤π.
5.2 Prove that the solution we found by separation of variables for the vibration of a free
string can be represented as a superposition of a forward and a backward wave.

5.7 Exercises
125
5.3 (a) Using the separation of variables method ﬁnd a (formal) solution of a vibrating
string with ﬁxed ends:
utt −c2uxx = 0
0 < x < L, 0 < t,
u(0, t) = u(L, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤L,
ut(x, 0) = g(x)
0 ≤x ≤L.
(b) Prove that the above solution can be represented as a superposition of a forward
and a backward wave.
5.4 (a) Find a formal solution of the problem
utt = uxx
0 < x < π, t > 0,
u(0, t) = u(π, t) = 0
t ≥0,
u(x, 0) = sin3 x
0 ≤x ≤π,
ut(x, 0) = sin 2x
0 ≤x ≤π.
(b) Show that the above solution is classical.
5.5 (a) Using the method of separation of variables, ﬁnd a (formal) solution of the problem
ut −kuxx = 0
0 < x < L, t > 0,
ux(0, t) = ux(L, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤L,
describing the heat evolution of an insulated one-dimensional rod (Neumann problem).
(b) Solve the heat equation ut = 12uxx in 0 < x < π, t > 0 subject to the following
boundary and initial conditions:
ux(0, t) = ux(π, t) = 0
t ≥0,
u(x, 0) = 1 + sin3 x
0 ≤x ≤π.
(c) Find limt→∞u(x, t) for all 0 < x < π, and explain the physical interpretation of
your result.
5.6 (a) Using the separation of variables method ﬁnd a (formal) solution of the following
periodic heat problem:
ut −kuxx = 0
0 < x < 2π, t > 0,
u(0, t) = u(2π, t), ux(0, t) = ux(2π, t)
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤2π,
where f is a smooth periodic function. This system describes the heat evolution on a
circular insulated wire of length 2π.
(b) Find limt→∞u(x, t) for all 0 < x < 2π, and explain the physical interpretation of
your result.

126
The method of separation of variables
(c) Show that if v is an arbitrary partial derivative of the solution u, then v(0, t) =
v(2π, t) for all t ≥0.
5.7 Solve the following heat problem:
ut −kuxx = A cos αt
0 < x < 1, t > 0,
ux(0, t) = ux(1, t) = 0
t ≥0,
u(x, 0) = 1 + cos2 πx
0 ≤x ≤1.
5.8 Consider the problem
ut −uxx = e−t sin 3x
0 < x < π , t > 0,
u(0, t) = u(π, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤π.
(a) Solve the problem using the method of eigenfunction expansion.
(b) Find u(x, t) for f (x) = x sin x.
(c) Show that the solution u(x, t) is indeed a solution of the equation
ut −uxx = e−t sin 3x
0 < x < π , t > 0.
5.9 Consider the problem
ut −uxx −hu = 0
0 < x < π , t > 0,
u(0, t) = u(π, t) = 0
t ≥0,
u(x, 0) = x(π −x)
0 ≤x ≤π,
where h is a real constant.
(a) Solve the problem using the method of eigenfunction expansion.
(b) Does limt→∞u(x, t) exist for all 0 < x < π?
Hint Distinguish between the following cases:
(i)
h < 1,
(ii)
h = 1,
(iii)
h > 1.
5.10 Consider the problem
ut = uxx + αu
0 < x < 1, t > 0,
u(0, t) = u(1, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤1, f ∈C([0, 1]).
(a) Assume that α = −1 and f (x) = x and solve the problem.
(b) Prove that for all α ≤0 and all f , the solution u satisﬁes limt→∞u(x, t) = 0.
(c) Assume now that π2 < α < 4π2. Does limt→∞u(x, t) exist for all f ? If your
answer is no, ﬁnd a necessary and sufﬁcient condition on f which ensures the existence
of this limit.

5.7 Exercises
127
5.11 Consider the following problem:
utt −uxx = 0
0 < x < 1, t > 0,
ux(0, t) = ux(1, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤1,
ut(x, 0) = 0
0 ≤x ≤1.
(a) Draw (on the (x, t) plane) the domain of dependence of the point ( 1
3, 1
10).
(b) Suppose that f (x) = (x −1
2)3. Evaluate u( 1
3, 1
10).
(c) Solve the problem with f (x) = 2 sin2 2πx.
5.12 (a) Solve the problem
ut −uxx −9u
4 = 0
0 < x < π, t > 0,
u(0, t) = ux(π, t) = 0
t ≥0,
u(x, 0) = sin(3x/2) + sin(9x/2)
0 ≤x ≤π.
(b) Compute φ(x) := limt→∞u(x, t) for x ∈[0, π].
5.13 Solve the problem
ut = uxx −u
0 < x < 1, t > 0,
u(0, t) = ux(1, t) = 0
t ≥0,
u(x, 0) = x(2 −x)
0 ≤x ≤1.
5.14 Prove Duhamel’s principle: for s ≥0, let v(x, t, s) be the solution of the following
initial-boundary problem (which depends on the parameter s):
vt −vxx = 0
0 < x < L, t > s,
v(0, t, s) = v(L, t, s) = 0
t ≥s,
v(x, s, s) = F(x, s)
0 ≤x ≤L.
Prove that the function
u(x, t) =
 t
0
v(x, t, s) ds
is a solution of the nonhomogeneous problem
ut −uxx = F(x, t)
0 < x < L, t > 0,
u(0, t) = u(L, t) = 0
t ≥0,
u(x, 0) = 0
0 ≤x ≤L.

128
The method of separation of variables
5.15 Using the energy method, prove the uniqueness for the problem
utt −c2uxx = F(x, t)
0 < x < L, t > 0,
ux(0, t) = t2,
u(L, t) = −t
t ≥0,
u(x, 0) = x2 −L2
0 ≤x ≤L,
ut(x, 0) = sin2 πx
L
0 ≤x ≤L.
5.16 Consider the following telegraph problem:
utt + ut −c2uxx = 0
a < x < b, t > 0,
u(a, t) = ux(b, t) = 0
t ≥0,
u(x, 0) = f (x),
a ≤x ≤b,
ut(x, 0) = g(x),
a ≤x ≤b.
(5.100)
Use the energy method to prove that the problem has a unique solution.
5.17 Using the energy method, prove uniqueness for the problem
utt −c2uxx + hu = F(x, t)
−∞< x < ∞, t > 0,
lim
x→±∞u(x, t) =
lim
x→±∞ux(x, t) =
lim
x→±∞ut(x, t) = 0
t ≥0,
 ∞
−∞
(u2
t + c2u2
x + hu2) dx < ∞
t ≥0,
u(x, 0) = f (x)
−∞< x < ∞,
ut(x, 0) = g(x)
−∞< x < ∞,
where h is a positive constant.
Hint Use the energy integral
E(t) = 1
2
 ∞
−∞
(w2
t + c2w2
x + hw2) dx.
5.18 Let α, β ≥0, k > 0. Using the energy method, prove uniqueness for the problem
ut −kuxx = F(x, t)
0 < x < L, t > 0,
u(0, t) −αux(0, t) = a(t),
u(L, t) + βux(L, t) = b(t)
t ≥0,
u(x, 0) = f (x)
0 < x < L.
5.19 (a) Prove the following identity:
u

(y2ux)x + (x2uy)y

= div

y2uux, x2uuy

−

(yux)2 + (xuy)2
.
(5.101)
(b) Let D be a planar bounded domain with a smooth boundary  which does not
intersect the lines x = 0 and y = 0. Using the energy method, prove uniqueness for
the elliptic problem
(y2ux)x + (x2uy)y = F(x, t)
(x, y) ∈D,
u(x, y) = f (x, y)
(x, y) ∈.
Hint Use the divergence theorem

D div w dxdy =

∂D w · n dσ and (5.101).

5.7 Exercises
129
5.20 Similarity variables for the heat equation: the purpose of this exercise is to derive
an important canonical solution for the heat equation and to introduce the method of
similarity variables.
(a) Consider the heat equation
ut −uxx = 0
x ∈R, t ≥0.
(5.102)
Set
u(x, t) = φ(λ(x, t)),
where
λ(x, t) =
x
2√t .
Show that u is a solution of (5.102) if and only if φ(λ) is a solution of the ODE
φ′′ + 2λφ′ = 0, where ′ = d/dλ.
(b) Integrate the ODE and show that the function
u(x, t) = erf
	 x
2√t

is a solution of (5.102), where erf(s) is the error function deﬁned by
erf(s) :=
2
√π
 s
0
e−r2dr.
(c) The complementary error function is deﬁned by
erfc(s) :=
2
√π
 ∞
s
e−r2dr = 1 −erf(s).
Show that
u(x, t) = erfc
	 x
2√t

is a solution of (5.102).
(d) Differentiating erf (x/2√t), show that
K(x, t) =
1
√
4πt
exp
	
−x2
4t

is a solution of (5.102). K is called the heat kernel. We shall consider heat kernels in
detail in Chapter 8.

6
Sturm–Liouville problems and eigenfunction expansion
6.1 Introduction
In the preceding chapter we presented several examples of initial boundary value
problems that can be solved by the method of separation of variables. In this chapter
we shall discuss the theoretical foundation of this method.
We consider two basic initial boundary value problems for which the method of
separation of variables is applicable. The ﬁrst problem is parabolic and concerns
heat ﬂow in a nonhomogeneous rod. The corresponding PDE is a generalization of
the heat equation. We seek a function u(x, t) that is a solution of the problem
ut −
1
r(x)m(t) [(p(x)ux)x + q(x)u] = 0
a < x < b, t > 0,
(6.1)
Ba[u] = αu(a, t) + βux(a, t) = 0
t ≥0,
(6.2)
Bb[u] = γ u(b, t) + δux(b, t) = 0
t ≥0,
(6.3)
u(x, 0) = f (x)
a ≤x ≤b.
(6.4)
The second problem is hyperbolic. It models the vibrations of a nonhomogeneous
string. The corresponding PDE is a generalization of the wave equation:
utt −
1
r(x)m(t) [(p(x)ux)x + q(x)u] = 0
a < x < b, t > 0,
(6.5)
Ba[u] = αu(a, t) + βux(a, t) = 0
t ≥0,
(6.6)
Bb[u] = γ u(b, t) + δux(b, t) = 0
t ≥0,
(6.7)
u(x, 0) = f (x),
ut(x, 0) = g(x)
a ≤x ≤b.
(6.8)
We assume that the coefﬁcients of these PDEs are real functions that satisfy
p, p′, q,r ∈C([a, b]), p(x),r(x) > 0, ∀x ∈[a, b],
m ∈C([0, ∞)), m(t) > 0, ∀t ≥0.
130

6.1 Introduction
131
We also assume that
α, β, γ, δ ∈R,
|α| + |β| > 0,
|γ | + |δ| > 0.
Note that these boundary conditions include in particular the Dirichlet boundary
condition (α = γ = 1, β = δ = 0) and the Neumann boundary condition (α =
γ = 0, β = δ = 1).
We concentrate on the parabolic problem; the hyperbolic problem can be dealt
with similarly. To apply the method of separation of variables we seek nontrivial
separated solutions of (6.1) that satisfy the boundary conditions (6.2)–(6.3) and
have the form
u(x, t) = X(x)T (t),
(6.9)
where X and T are functions of one variable, x and t, respectively. Substituting
such a product solution into the PDE and separating the variables we obtain
mTt
T
= (pXx)x + q X
r X
.
(6.10)
The left hand side depends solely on t, while the right hand side is a function of x.
Therefore, there exists a constant λ such that
mTt
T
= (pXx)x + q X
r X
= −λ.
(6.11)
Thus, (6.11) is equivalent to the following system of ODEs
(pX′)′ + q X + λr X = 0
a < x < b,
(6.12)
m dT
dt = −λT
t > 0.
(6.13)
By our assumption u ̸= 0. Since u must satisfy the boundary conditions (6.2)–(6.3),
it follows that
Ba[X] = 0,
Bb[X] = 0.
In other words, the function X should be a solution of the boundary value problem
(pv′)′ + qv + λrv = 0
a < x < b,
(6.14)
Ba[v] = Bb[v] = 0.
(6.15)
The main part of the present chapter is devoted to the solution of the system
(6.14)–(6.15). A nontrivial solution of this system is called an eigenfunction of the
problem associated with the eigenvalue λ. The problem (6.14)–(6.15) is called a
Sturm–Liouville eigenvalue problem in honor of the French mathematicians Jacques
Charles Sturm (1803–1855) and Joseph Liouville (1809–1882). The differential

132
Sturm–Liouville problems
operator L[v] := (pv′)′ + qv is said to be a Sturm–Liouville operator. The function
r is called a weight function.
The notions eigenfunction and eigenvalue are familiar to the reader from a basic
course in linear algebra. Let A be a linear operator acting on a vector space V , and
let λ ∈C. A vector v ̸= 0 is an eigenvector of the operator A with an eigenvalue
λ, if A[v] = λv. The set of all vectors satisfying A[v] = λv is a linear subspace of
V , and its dimension is the multiplicity of λ. An eigenvalue with multiplicity 1 is
called simple.
In our (Sturm–Liouville) eigenvalue problem, the corresponding linear operator
is the differential operator −L, which acts on the space of twice differentiable
functions satisfying the corresponding boundary conditions.
Example 6.1 In Chapter 5 we solved the following Sturm–Liouville problem:
d2v
dx2 + λv = 0
0 < x < L,
(6.16)
v(0) = v(L) = 0.
(6.17)
Here p = r = 1, q = 0, and the boundary condition is of the ﬁrst kind (Dirichlet).
The eigenfunctions and eigenvalues of the problem are:
vn(x) = sin nπx
L ,
λn =
nπ
L
2
n = 1, 2, 3, . . . .
Example 6.2 We also solved the Sturm–Liouville problem
d2v
dx2 + λv = 0
0 < x < L,
(6.18)
v′(0) = v′(L) = 0.
(6.19)
Here we are dealing with the Neumann boundary condition. The eigenfunctions
and eigenvalues of the problem are:
vn(x) = cos nπx
L ,
λn =
nπ
L
2
n = 0, 1, 2, . . . .
In the following sections we show that the essential properties of the eigenfunc-
tions and eigenvalues of these simple problems are also satisﬁed in the case of a
general Sturm–Liouville problem. We then use these properties to solve the general
initial boundary value problems that were presented at the beginning of the current
section.

6.2 The Sturm–Liouville problem
133
6.2 The Sturm–Liouville problem
Consider the Sturm–Liouville eigenvalue problem
(p(x)v′)′ + q(x)v + λr(x)v = 0
a < x < b,
(6.20)
Ba[v] := αv(a) + βv′(a) = 0,
Bb[v] := γ v(b) + δv′(b) = 0.
(6.21)
The ﬁrst equation is a linear second-order ODE. We assume that the coefﬁcients of
this ODE are real functions satisfying
p, p′, q,r ∈C([a, b]),
p(x),r(x) > 0,
∀x ∈[a, b].
We also assume that
α, β, γ, δ ∈R,
|α| + |β| > 0,
|γ | + |δ| > 0.
Under these assumptions the eigenvalue problem (6.20)–(6.21) is called a regular
Sturm–Liouville problem. If either of the functions p orr vanishes at least at one end
point, or is discontinuous there, or if the problem is deﬁned on an inﬁnite interval,
then the Sturm–Liouville problem is said to be singular.
Remark 6.3 It is always possible to transform a general linear second–order ODE
into an ODE of the Sturm–Liouville form:
L[v] := (p(x)v′)′ + q(x)v = f.
Indeed, suppose that
M[v] := A(x)v′′ + B(x)v′ + C(x)v = F(x)
(6.22)
isanarbitrarylinearsecond-orderODEsuchthat A isapositivecontinuousfunction.
We denote by p the integration factor p(x) := exp {

[B(x)/A(x)] dx}. Multiplying
(6.22) by p(x)/A(x) we obtain
L[v] := p(x)
A(x) M[v] = p(x)v′′ + p′(x)v′ + p(x)
A(x)C(x)v
= (p(x)v′)′ + q(x)v = f,
and we see that the operator M is equivalent to a Sturm–Liouville operator L, where
q(x) = [p(x)/A(x)]C(x), and f (x) = [p(x)/A(x)]F(x).
Example 6.4 Let ν ∈R, a > 0. The equation
r2w′′(r) + rw′(r) + (r2 −ν2)w(r) = 0
r > 0
(6.23)
is called a Bessel equation of order ν. Dividing (6.23) by r, using the transformation
x = r/
√
λ, and limiting our attention to a ﬁnite interval, we obtain the following

134
Sturm–Liouville problems
singular Sturm–Liouville problem:
(xv′(x))′ +
	
λx −ν2
x

v(x) = 0
0 < x < a,
v(a) = 0,
|v(0)| < ∞.
Here p(x) = r(x) = x, q(x) = −ν2/x. We shall study this equation in some detail
in Chapter 9.
In our study of the Sturm–Liouville theory, we shall also deal with the periodic
Sturm–Liouville problem:
(p(x)v′)′ + q(x)v + λr(x)v = 0
a < x < b,
(6.24)
v(a) = v(b),
v′(a) = v′(b),
(6.25)
where the coefﬁcients p, q,r are periodic functions of a period (b −a), and
p, p′, q,r ∈C(R),
p(x),r(x) > 0
∀x ∈R.
The periodic boundary conditions (6.25) and the ODE (6.24) imply that an eigen-
function can be extended to a periodic function on the real line. This periodic
function is a twice differentiable (periodic) function, except possibly at the points
a + k(b −a), k ∈Z, where a singularity of the second derivative may occur.
Example 6.5 Consider the following periodic Sturm–Liouville problem:
d2v
dx2 + λv = 0
0 < x < L,
(6.26)
v(0) = v(L),
v′(0) = v′(L).
(6.27)
Here p = r = 1, q = 0. Recall that the general solution of the ODE (6.26) is of
the form:
1. if λ < 0, then v(x) = α cosh(√−λx) + β sinh(√−λx),
2. if λ = 0, then v(x) = α + βx,
3. if λ > 0, then v(x) = α cos(
√
λx) + β sin(
√
λx),
where α, β are arbitrary real numbers. Note that we assume again that λ is real.
We shall prove later that all the eigenvalues of regular or periodic Sturm–Liouville
problems are real.
Negative eigenvalues (λ < 0) In this case any nontrivial solution of the corre-
sponding ODE is an unbounded function on R. In particular, there is no periodic
nontrivial solution for this equation. In other words, the system (6.26)–(6.27) does
not admit negative eigenvalues.
Zero eigenvalue (λ = 0) A linear function is periodic if and only if it is a constant.
Therefore, λ = 0 is an eigenvalue with an eigenfunction 1.

6.2 The Sturm–Liouville problem
135
Positive eigenfunctions (λ > 0) The general solution for the case λ > 0 is of the
form
v(x) = α cos(
√
λx) + β sin(
√
λx).
(6.28)
Substituting the boundary conditions (6.27) into (6.28) we arrive at a system of
algebraic linear equations
α cos(
√
λL) + β sin(
√
λL) = α,
(6.29)
√
λ[−α sin(
√
λL) + β cos(
√
λL)] =
√
λβ.
(6.30)
If α or β equals zero, but |α| + |β| ̸= 0, then obviously λ = (2nπ/L)2, where
n ∈N. Otherwise, multiplying (6.29) by β, and (6.30) by α/
√
λ implies again that
λ = (2nπ/L)2.
Therefore, the system (6.29)–(6.30) has a nontrivial solution if and only if
λn =
	2nπ
L

2
n = 1, 2, 3, . . . .
These eigenvalues have eigenfunctions of the form
vn(x) = αn cos
	2nπx
L

+ βn sin
	2nπx
L

.
(6.31)
It is convenient to select {cos(2nπx/L), sin(2nπx/L)} as a basis for the eigenspace
corresponding to λn.
Therefore, positive eigenvalues of the periodic problem (6.26)–(6.27) are of
multiplicity 2. Recall that in the other examples of the Sturm–Liouville problem
that we have encountered so far all the eigenvalues are simple (i.e. of multiplicity
1). In the sequel, we prove that this is a general property of regular Sturm–Liouville
problems. Moreover, it turns out that this is the only essential property of a regular
Sturm–Liouville problem that does not hold in the periodic case. Note that the
maximal multiplicity of an eigenvalue of a Sturm–Liouville problem is 2, since
the space of all solutions of the ODE (6.20) (without imposing any boundary
conditions) is two-dimensional.
In conclusion, the solution of the periodic Sturm–Liouville eigenvalue problem
(6.26)–(6.27) is the following inﬁnite sequence of eigenvalues and eigenfunctions
λ0 = 0,
u0(x) = 1,
(6.32)
λn =
	2nπ
L

2
, un(x) = cos 2nπx
L
, vn(x) = sin 2nπx
L
n = 1, 2, . . . .
(6.33)
This system is called the classical Fourier system on the interval [0, L].

136
Sturm–Liouville problems
6.3 Inner product spaces and orthonormal systems
To prepare the ground for the Sturm–Liouville theory we survey basic notions and
properties of real inner product spaces. We omit proofs which can be found in
standard textbooks on Fourier analysis [13].
Deﬁnition 6.6 A real linear space V is said to be a (real) inner product space if
for any two vectors u, v ∈V there is a real number ⟨u, v⟩∈R, which is called the
inner product of u and v, such that the following properties are satisﬁed:
1. ⟨u, v⟩= ⟨v, u⟩for all u, v ∈V .
2. ⟨u + v, w⟩= ⟨u, w⟩+ ⟨v, w⟩for all u, v, w ∈V .
3. ⟨αu, v⟩= α⟨u, v⟩for all u, v ∈V , and α ∈R.
4. ⟨v, v⟩≥0 for all v ∈V , moreover, ⟨v, v⟩> 0 for all v ̸= 0.
In the context of Sturm–Liouville problems the following natural inner product
plays an important role:
Deﬁnition 6.7 (a) Let f be a real function deﬁned on [a, b] except, possibly, for
ﬁnitely many points. f is called piecewise continuous on [a, b] if it has at most
ﬁnitely many points of discontinuity, and if at any such point f admits left and right
limits (such a discontinuity is called a jump (or step) discontinuity).
(b)Twopiecewisecontinuousfunctionswhichtakethesamevaluesatallpointsin
[a, b]except,possibly,forﬁnitelymanypointsarecalledequivalent.Thespaceofall
(equivalent classes of) piecewise continuous functions on [a, b] will be denoted by
E(a, b).
(c) If f and f ′ are piecewise continuous functions, we say that f is piecewise
differentiable.
(d) Let r(x) be a positive continuous weight function on [a, b]. We deﬁne the
following inner product on the space E(a, b):
⟨u, v⟩r =
 b
a
u(x)v(x)r(x) dx,
u, v ∈E(a, b).
The corresponding inner product space is denoted by Er(a, b). To simplify the
notation we shall use E(a, b) for E1(a, b).
Each inner product induces a norm deﬁned by ∥v∥:= ⟨v, v⟩1/2, which satisﬁes
the usual norm properties:
(1) ∥αu∥= |α| ∥u∥for all u ∈V , and α ∈R.
(2) The triangle inequality: ∥u + v∥≤∥u∥+ ∥v∥for all u, v ∈V .
(3) ∥v∥≥0 for all v ∈V , moreover, ∥v∥> 0 for all v ̸= 0.

6.3 Inner product spaces
137
In addition this induced norm satisﬁes the Cauchy–Schwartz inequality
|⟨u, v⟩| ≤∥u∥∥v∥.
Deﬁnition 6.8 Let (V, ⟨·, ·⟩) be an inner product space.
(1) A sequence {vn}∞
n=1 converges to v in the mean (or in norm), if
lim
n→∞∥vn −v∥= 0 .
(2) Two vectors u, v ∈V are called orthogonal if ⟨u, v⟩= 0.
(3) The sequence {vn} ⊂V is said to be orthogonal if vn ̸= 0 for all n ∈N, and ⟨vn, vm⟩= 0
for all n ̸= m.
(4) The sequence {vn} ⊂V is said to be orthonormal if
⟨vn, vm⟩=
 0
m ̸= n,
1
m = n.
(6.34)
Remark 6.9 Consider the inner product space Er(a, b). Then convergence in the
mean does not imply pointwise convergence on [a, b], and vice versa, a point-
wise convergence does not imply convergence in the mean. If, however, [a, b] is a
bounded closed interval, then uniform convergence on [a, b] implies convergence
in the mean.
As an example, consider the interval [0, ∞) and the weight function r(x) = 1.
The function
χ[α,β](x) =
1
x ∈[α, β],
0
x ̸∈[α, β]
(6.35)
is called the characteristic function of the interval [α, β].
The sequence of functions vn = χ[n,n+1], n = 1, 2, . . . converges pointwise to
zero on [0, ∞), but since ∥vn∥= 1, this sequence does not converge in norm to zero.
On the other hand, consider the interval [0, 1], and let {[an, bn]} be a sequence
of intervals such that each x ∈[0, 1] belongs, and also does not belong, to
inﬁnitely many intervals [an, bn], and such that bn−an =2−k(n), where {k(n)} is a
nondecreasing sequence satisfying limn→∞k(n) = ∞. Since
∥χ[an,bn]∥2 =
 1
0
[χ[an,bn](x)]2 dx = 2−k(n) →0,
it follows that the sequence {χ[an,bn]} tends to the zero function in the mean. On
the other hand {χ[an,bn]} does not converge at any point of [0, 1] since for a ﬁxed
0 ≤x0 ≤1, the sequence {χ[an,bn](x0)} attains inﬁnitely many times the value 0 and
also inﬁnitely many times the value 1.
Remark 6.10 One can easily modify any orthogonal sequence {vn} to obtain an
orthonormal sequence {˜vn}, using the normalization process ˜vn := (1/∥vn∥)vn.

138
Sturm–Liouville problems
Using an orthonormal sequence, one can ﬁnd the orthogonal projection of a
vector v ∈V into a subspace VN of V , which is the closest vector to v in VN.
Theorem 6.11 (a) Let {vn}N
n=1 be a ﬁnite orthonormal sequence, and set VN :=
span{v1, . . . , vN}. Let v ∈V , and deﬁne
u :=
N

n=1
⟨v, vn⟩vn .
Then
∥v −u∥= min
w∈VN{∥v −w∥} =

 
 
!∥v∥2 −
N

n=1
⟨v, vn⟩2.
(6.36)
In other words, u is the orthogonal projection of v into VN.
(b) Let {vn}N
n=1 (N ≤∞) be a ﬁnite or inﬁnite orthonormal sequence, and let v ∈V . Then
the following inequality holds:
N

n=1
⟨v, vn⟩2 ≤∥v∥2 .
(6.37)
In particular,
lim
n→∞⟨v, vn⟩= 0.
(6.38)
Deﬁnition 6.12 (1) The last claim of Theorem 6.11 (i.e. (6.38)) is called the Riemann–
Lebesgue lemma.
(2) The coefﬁcients ⟨v, vn⟩are called generalized Fourier coefﬁcients (or simply, Fourier
coefﬁcients) of the function v with respect the orthonormal sequence {vn}N
n=1, where
N ≤∞.
(3) The inequality (6.37) is called the Bessel inequality. Note that the Bessel inequality
(6.37) follows easily from (6.36).
(4) The orthonormal sequence {vn}N
n=1 is said to be complete in V , if for every v ∈V we
have equality in the Bessel inequality. In this case the equality is call the Parseval
identity.
The following proposition follows from (6.36).
Proposition 6.13 Let {vn}∞
n=1 be an inﬁnite orthonormal sequence. The following
propositions are equivalent:
(1) {vn}∞
n=1 is a complete orthonormal sequence.
(2)
lim
k→∞∥v −
k

n=1
⟨v, vn⟩vn∥= 0 ,
for all v ∈V .

6.3 Inner product spaces
139
Deﬁnition 6.14 If limk→∞∥v −k
n=1⟨v, vn⟩vn∥= 0 exists, we write
v =
∞

n=1
⟨v, vn⟩vn,
and we say that the Fourier expansion of v converges in norm (or on average, or in
the mean) to v. More generally, the series
∞

n=1
⟨v, vn⟩vn
is called the generalized Fourier expansion (or for short, Fourier expansion) of v
with respect to the orthonormal system {vn}∞
n=1.
Remark 6.15 The notion of convergence in the mean may seem initially to be
an abstract mathematical idea. We shall see, however, that in fact it provides the
right framework for Fourier’s theory of representing a function as a series of an
orthonormal sequence.
We end this section with two examples of Fourier expansion.
Example 6.16 Let E(0, π) be the inner product space (of equivalent classes) of
all piecewise continuous functions in the interval [0, π] equipped with the inner
product ⟨u, v⟩=
 π
0 u(x)v(x) dx. Consider the sequence
un(x) = cos nx,
n = 0, 1, 2, 3, . . .
and recall that in Example 6.2 we computed directly for m, n = 0, 1, 2, . . .
 π
0
cos mx cos nx dx =



0
m ̸= n,
π/2
m = n ̸= 0,
π
m = n = 0.
(6.39)
Consequently, the sequence {√1/π } ∪{√2/π cos nx}∞
n=1 is orthonormal in the
space E(0, π). We shall see in the next section that it is, in fact, a complete or-
thonormal sequence in E(0, π).
We proceed to compute the Fourier expansion of u(x) = x with respect to that
orthonormal sequence. We write the expansion as
A0

1
π +
∞

n=1
An

2
π cos nx,
where
A0 =

1
π
 π
0
u(x)dx,
An =

2
π
 π
0
u(x) cos nxdx
n ≥1.

140
Sturm–Liouville problems
Therefore,
A0 =

1
π
 π
0
x dx = √π π
2 ,
An =

2
π
 π
0
x cos nx dx =
π
2
2π
n2π2 [(−1)n −1].
It follows that the Fourier expansion of u in this orthonormal sequence is given by
the series
π
2 −4
π
∞

m=1
1
(2m −1)2 cos(2m −1)x,
which converges uniformly on [0, π].
Example 6.17 Let E0(0, π) be the subspace of E(0, π) (of equivalent classes) of
all piecewise continuous functions in the interval [0, π] that vanish at the interval’s
end points. In particular, E0(0, π) is an inner product space with respect to ⟨u, v⟩=
 π
0 u(x)v(x) dx . Consider the sequence of functions
vn(x) = sin nx
n = 1, 2, 3, . . .
in this space.
The orthogonality of the sequence {vn(x)}∞
n=1 in the space E0(0, π) has already
been established in Example 6.1. Speciﬁcally, we found that for m, n = 1, 2, 3, . . .
 π
0
sin mx sin nx dx =
0
m ̸= n,
π/2
m = n.
(6.40)
Therefore, {√2/π sin nx}∞
n=1 is indeed an orthonormal (and, as will be shown soon,
even a complete orthonormal) sequence in E0(0, π).
The Fourier expansion of v(x) = x sin x in the current sequence is given by
∞

n=1
Bn

2
π sin nx,
where Bn =

2
π
 π
0
v(x) sin nxdx.
We use the identity
sin x sin nx = 1
2 [cos(n −1)x −cos(n + 1)x]
to ﬁnd
Bn =

1
2π
 π
0
x[cos(n −1)x −cos(n + 1)x]dx.

6.4 Eigenfunctions and eigenvalues: basic properties
141
An integration by parts leads to
B1 =
π
2
3/2
,
Bn =
π
2
4n[(−1)n+1 −1]
π(n + 1)2(n −1)2
n > 1.
We therefore obtain that the Fourier expansion for v in this orthonormal sequence
is given by the series
π
2 sin x +
∞

n=2
4n[(−1)n+1 −1]
π(n+1)2(n−1)2 sin nx,
which converges uniformly on [0, π].
6.4 The basic properties of Sturm–Liouville
eigenfunctions and eigenvalues
We now present the essential properties of the eigenvalues and eigenfunctions of
regular and periodic Sturm–Liouville problems. We shall point out some properties
which are still valid in the irregular case.
We start with an algebraic characterization of λ as an eigenvalue.
Proposition 6.18 Consider the following regular Sturm–Liouville problem
L[v] + λrv = 0
a < x < b,
(6.41)
Ba[v] = Bb[v] = 0.
(6.42)
Assume that the pair of functions uλ, vλ is a basis of the linear space of all solutions
of the ODE (6.41). Then λ is an eigenvalue of the Sturm–Liouville problem if and
only if

Ba[uλ]
Ba[vλ]
Bb[uλ]
Bb[vλ]
 = 0.
(6.43)
Proof A function w is a nontrivial solution of (6.41) if and only if there exist
c, d ∈R such that |c| + |d| > 0 and such that
w(x) = cuλ(x) + dvλ(x).
The function w is an eigenfunction with eigenvalue λ if and only if w also satisﬁes
the boundary conditions
Ba[w] = cBa[uλ] + d Ba[vλ] = 0,
Bb[w] = cBb[uλ] + d Bb[vλ] = 0.

142
Sturm–Liouville problems
In other words, the vector (c, d) ̸= 0 is a nontrivial solution of a 2 × 2 linear ho-
mogeneous algebraic system with the coefﬁcients matrix
	 Ba[uλ]
Ba[vλ]
Bb[uλ]
Bb[vλ]

.
This system has a nontrivial solution if and only if condition (6.43) is satisﬁed. □
Example 6.19 Let us check the criterion that we just derived for the Sturm–
Liouville problem
v′′ + λv = 0
0 < x < L,
(6.44)
v(0) = v′(L) = 0.
(6.45)
For λ > 0, the pair of functions
uλ(x) = sin
√
λx,
vλ(x) = cos
√
λx
forms a basis for the linear space of all solutions of the corresponding ODE. There-
fore, λ is an eigenvalue of the problem if and only if

sin 0
cos 0
√
λ cos
√
λL −
√
λ sin
√
λL
 = −
√
λ cos
√
λL = 0.
(6.46)
Hence,
λ =
	(2n −1)π
2L

2
n = 1, 2, . . . .
We proceed to list general properties of Sturm–Liouville problems.
1 Symmetry Let L be a Sturm–Liouville operator of the form
L[u] = (p(x)u′)′ + q(x)u,
and consider the expression uL[v] −vL[u] for u, v ∈C2([a, b]). Using the Leib-
nitz product rule we have
uL[v] −vL[u] = u(pv′)′ + uqv −v(pu′)′ −vqu
= (upv′)′ −u′ pv′ −(vpu′)′ + u′ pv′.
We thus obtain the Lagrange identity:
uL[v] −vL[u] =

p

uv′ −vu′′ .
(6.47)
Integrating the Lagrange identity over the interval [a, b] implies the identity
 b
a
(uL[v] −vL[u]) dx = p

uv′ −vu′b
a ,
(6.48)

6.4 Eigenfunctions and eigenvalues: basic properties
143
which is called Green’s formula. Assume that u and v satisfy the boundary condi-
tions (6.21) in the regular case, or (6.25) in the periodic case. Then it can be seen
that
p

uv′ −vu′b
a = 0.
(6.49)
Therefore, for such u and v we have
 b
a
(uL[v] −vL[u]) dx = 0.
(6.50)
The algebraic interpretation of the above formula is that the operator L is a sym-
metric operator on the space of twice differentiable functions that satisfy either
the regular boundary conditions (6.21), or the periodic boundary conditions (6.25),
with respect to the inner product
⟨u, v⟩=
 b
a
u(x)v(x) dx.
Although the formal deﬁnition of a symmetric operator will not be given here,
the analogy with the case of symmetric matrices acting on the vector space Rk
(equipped with the standard inner product) is evident.
We point out that the operator L is symmetric in many singular cases. For
example, if a = −∞, b = ∞and limx→±∞p(x) = 0, then L is symmetric on
the space of smooth bounded functions with bounded derivatives.
2 Orthogonality The following property also has a well-known analog in the case
of symmetric matrices.
Proposition 6.20 Eigenfunctions which belong to distinct eigenvalues of a regular
Sturm–Liouville problem are orthogonal relative to the inner product
⟨u, v⟩r =
 b
a
u(x)v(x)r(x) dx.
Moreover, this property also holds in the periodic case and in fact also in many
singular cases.
Proof Let vn, vm be two eigenfunctions belonging to the eigenvalues λn ̸= λm,
respectively. Hence,
−L[vn] = λnrvn,
(6.51)
−L[vm] = λmrvm.
(6.52)
Moreover, vn, vm satisfy the boundary conditions (6.21).

144
Sturm–Liouville problems
Multiplying (6.51) by vm, and (6.52) by vn, integrating over [a, b], and then
taking the difference between the two equations thus obtained, we ﬁnd
−
 b
a
(vmL[vn] −vnL[vm]) dx = (λn −λm)
 b
a
vnvmr dx.
(6.53)
Since vn, vm satisfy the boundary conditions (6.21), we may use Green’s formula
(6.50) to infer that
(λn −λm)
 b
a
vnvmr dx = 0.
But λn ̸= λm, thus ⟨vn, vm⟩r = 0.
□
Recall that for the Sturm–Liouville problem of Example 6.1, the orthogonality
of the corresponding eigenfunctions was already shown, since we checked that for
m, n = 1, 2, 3, . . .
 L
0
sin mπx
L
sin nπx
L
dx =
0
m ̸= n,
L/2
m = n.
(6.54)
In other words, the sequence {√2/L sin(nπx/L)}∞
n=1 is an orthonormal system of
all the eigenfunctions of this problem.
Similarly, in Example 6.2 we found that for m, n = 0, 1, 2, . . .
 L
0
cos mπx
L
cos nπx
L
dx =



0
m ̸= n,
L/2
m = n ̸= 0,
L
m = n = 0.
(6.55)
Therefore, {√1/L} ∪{√2/L cos(nπx/L)}∞
n=1 is an orthonormal system of all the
eigenfunctions of the corresponding problem.
Consider now the periodic problem of Example 6.5. From (6.54) we have for
m, n = 1, 2, 3, . . .
 L
0
sin 2mπx
L
sin 2nπx
L
dx =
0
m ̸= n,
L/2
m = n.
(6.56)
From (6.55) we see that for m, n = 0, 1, 2, . . .
 L
0
cos 2mπx
L
cos 2nπx
L
dx =



0
m ̸= n,
L/2
m = n ̸= 0,
L
m = n = 0.
(6.57)
In addition, for m = 1, 2, 3, . . . , n = 0, 1, 2, . . .
 L
0
sin 2mπx
L
cos 2nπx
L
dx = 0.
(6.58)

6.4 Eigenfunctions and eigenvalues: basic properties
145
It follows that our system of all eigenfunctions of the periodic problem is indeed
orthogonal, including the orthogonality of eigenfunctions with the same eigenvalue.
Moreover, the system

1
L
"
∪

2
L cos 2nπx
L
"∞
n=1
∪

2
L sin 2nπx
L
"∞
n=1
is an orthonormal system of all the eigenfunctions of this periodic problem.
Note that the functions
un = sin 2nπx
L
,
wn = sin 2nπx
L
+ cos 2nπx
L
are two linearly independent eigenfunctions belonging to the same eigenvalue; yet
they are not orthogonal. But in such a case of nonsimple eigenvalue, one can carry
out the Gram–Schmidt orthogonalization process to obtain an orthonormal system
of all the eigenfunctions of the problem.
3 Real eigenvalues
Proposition 6.21 The eigenvalues of a regular Sturm–Liouville problem are all
real. Moreover, this property holds in the periodic case and also in many singular
cases.
Proof Assume that λ ∈C is a nonreal eigenvalue with an eigenfunction v. Then
L[v] + λrv = (pv′)′ + qv + λrv = 0,
(6.59)
Ba[v] = αv(a) + βv′(a) = 0,
Bb[v] = γ v(b) + δv′(b) = 0.
(6.60)
Recall that the coefﬁcients of (6.59)–(6.60) are all real. By forming the complex
conjugate of (6.59)–(6.60), and interchanging the order of conjugation and differ-
entiation, we obtain
L[v] + λrv = L[v] + λrv = 0,
(6.61)
Ba[v] = αv(a) + βv ′(a) = 0,
Bb[v] = γ v(b) + δv ′(b) = 0.
(6.62)
Therefore, v is an eigenfunction with eigenvalue λ. By our assumption λ ̸= λ, and
by Proposition 6.20 we have
0 = ⟨v, v⟩r =
 b
a
v(x)v(x)r(x) dx =
 b
a
|v(x)|2r(x) dx.
On the other hand, since v ̸= 0 and r(x) > 0 on [a, b], it follows that
 b
a |v(x)|2r(x)dx > 0, which leads to a contradiction.
□

146
Sturm–Liouville problems
4 Real eigenfunctions Let λ be an eigenvalue with eigenfunction v. Since for
every complex number C ̸= 0, the function Cv is also an eigenfunction with the
same eigenvalue λ, it is not true that all the eigenfunctions are real. Moreover, for
n = 1, 2, . . . , the complex valued functions exp(±2nπix/L), which are not scalar
multiples of real eigenfunctions, are eigenfunctions of the periodic problem of
Example 6.5. We can prove, however, the following result.
Proposition 6.22 Let λ be an eigenvalue of a regular or a periodic Sturm–Liouville
problem, and denote by Vλ the subspace spanned by all the eigenfunctions with
eigenvalue λ. Then Vλ admits an orthonormal basis of real valued functions.
Proof Let v be an eigenfunction with eigenvalue λ. Recall that λ is a real number.
By separating the real and the imaginary parts of (6.59)–(6.60), it can be checked
that both Re v and Im v are solutions of the ODE (6.59) that satisfy the boundary
conditions (6.60). Since at least one of these two functions is not zero, it follows
that at least one of them is an eigenfunction. If λ is simple, then we now have a
real basis for Vλ. On the other hand, if the multiplicity of λ is 2, we can consider
the real and imaginary parts of two linearly independent eigenfunctions in Vλ. By
a simple dimensional consideration, it follows that out of these four real functions,
one can extract at least one pair of linearly independent functions. Then one ap-
plies the Gram–Schmidt process on such a pair of real eigenfunctions to obtain an
orthonormal basis for Vλ.
□
5 Simple eigenvalues
Proposition 6.23 The eigenvalues of a regular Sturm–Liouville problem are all
simple.
Proof Let v1, v2 be two eigenfunctions belonging to the same eigenvalue λ. Then
L[v1] = −λrv1,
(6.63)
L[v2] = −λrv2.
(6.64)
Therefore,
v2L[v1] −v1L[v2] = 0.
Recall that by the Lagrange identity
v2L[v1] −v1L[v2] =

p

v2v′
1 −v1v′
2
′.
(6.65)
Hence,
Q(x) := p

v2v′
1 −v1v′
2

= constant.

6.4 Eigenfunctions and eigenvalues: basic properties
147
On the other hand, we have shown that two functions that satisfy the same regular
boundary conditions also satisfy Q(a) = Q(b) = 0. Since p is a positive function
on the entire closed interval [a, b], it follows that the Wronskian
W := v2v′
1 −v1v′
2
vanishes at the end points. Recall that v1, v2 are solutions of the same linear ODE,
and therefore, the Wronskian is identically zero. Consequently, the functions v1, v2
are linearly dependent.
□
Remark 6.24 For the periodic eigenvalue problem of Example 6.5, we have shown
that except for the ﬁrst eigenvalue all the other eigenvalues are not simple.
6 Existence of an inﬁnite sequence of eigenvalues The standard proof of the
existence of an eigenvalue for matrices uses the characteristic polynomial and
therefore cannot be generalized to the Sturm–Liouville case. Actually, it is not
clear at all that a Sturm–Liouville problem admits even one eigenvalue; in fact, in
1836 both Sturm and Liouville published papers in the same journal where they
independently asked exactly this particular question.
Example 6.25 It can be checked that the following singular Sturm–Liouville prob-
lem does not admit an eigenvalue.
v′′ + λ v = 0
x ∈R,
limx→−∞v(x) = limx→∞v(x) = 0.
(6.66)
On the other hand, if we change the boundary conditions slightly:
v′′ + λ v = 0
x ∈R,
supx∈R |v(x)| < ∞,
(6.67)
then the set of all eigenvalues of the problem is the half-line [0, ∞). Indeed, for
λ > 0 the eigenfunctions are sin
√
λx, cos
√
λx, while for λ = 0 the corresponding
eigenfunction equals 1. This set of eigenfunctions is not an orthogonal system with
respect to the natural inner product ⟨u, v⟩=
 ∞
−∞u(x)v(x) dx, since for such a
function v we have ∥v∥2 = ∞, and hence v does not belong to the corresponding
inner product space.
The following proposition demonstrates that for regular problems the picture is
simpler (the proof is beyond the scope of this book; see for example [6]).
Proposition 6.26 The set of all eigenvalues of a regular Sturm–Liouville problem
forms an unbounded strictly monotone sequence. We denote this sequence by
λ0 < λ1 < λ2 < · · · < λn < λn+1 < · · ·.
In particular, there are inﬁnitely many eigenvalues, and limn→∞λn = ∞.

148
Sturm–Liouville problems
Moreover, the above statements are also valid in the periodic case, ex-
cept that the sequence {λn}∞
n=0 is only nondecreasing (repeated eigenvalues are
allowed).
Corollary 6.27 (1) A regular or periodic Sturm–Liouville problem admits an inﬁ-
nite orthonormal sequence of real eigenfunctions in Er(a, b).
(2) The sequence of all eigenvalues is an unbounded subset of the real line that is
bounded from below.
7 Completeness, and convergence of the Fourier expansion The separation of
variables method (and the justiﬁcation of Fourier’s idea) relies on the following
convergence theorems; the proofs will not be given here (see for example [6]).
Proposition 6.28 The orthonormal system {vn}∞
n=0 of all eigenfunctions of a reg-
ular (or periodic) Sturm–Liouville problem is complete in the inner product space
Er(a, b).
Deﬁnition 6.29 The generalized Fourier expansion of a function v with respect to
the orthonormal system {vn}∞
n=0 of all eigenfunctions of a Sturm–Liouville problem
is called the eigenfunction expansion of v.
Proposition 6.28 implies that the eigenfunction expansion is converging in the mean
(in norm). In fact, for every function such that
 b
a u2(x)r(x) dx < ∞the eigenfunc-
tion expansion of u converges in norm. If we assume further that the function u is
smoother we arrive at a stronger convergence result.
Proposition 6.30 Let {vn}∞
n=0 be an orthonormal system of all eigenfunctions of a
regular (or periodic) Sturm–Liouville problem.
(1) Let f be a piecewise differentiable function on [a, b]. Then for all x ∈(a, b)
the eigenfunction expansion of f with respect to the system {vn}∞
n=0 converges to
[ f (x+) + f (x−)]/2 (i.e. the average of the two one-side limits of f at x).
(2) If f is a continuous and piecewise differentiable function that satisﬁes the boundary
conditions of the given Sturm–Liouville problem, then the eigenfunction expansion of
f with respect to the system {vn}∞
n=0 converges uniformly to f on the interval [a, b].
In the following three examples we demonstrate Proposition 6.28 and Proposition
6.30 for three different eigenfunctions systems.
Example 6.31 Find the eigenfunction expansion of the function f = 1 with respect
to the orthonormal system {√2/L sin(nπx/L)}∞
n=1 of the eigenfunctions of the
Sturm–Liouville problem of Example 6.1.

6.4 Eigenfunctions and eigenvalues: basic properties
149
The Fourier coefﬁcients are given by
bn =
#
f,

2
L sin nπx
L
$
=

2
L
 L
0
sin nπx
L
dx =−

2
L
L
nπ cos nπx
L

L
0
=
√
2L
nπ [1 −(−1)n].
Therefore, the series
2
π
∞

n=1
[1 −(−1)n]
n
sin nπx
L
= 4
π
∞

k=0
1
2k + 1 sin (2k + 1)πx
L
(6.68)
is the eigenfunction expansion of f . While it converges to 1 for all x ∈(0, L),
it does not converges uniformly on [0, L] since f does not satisfy the Dirichlet
boundary conditions at the end points.
Example 6.32 Find the eigenfunction expansion of the function f (x) = x with
respect to the orthonormal system {√1/L} ∪{√2/L cos(nπx/L)}∞
n=1 of all the
eigenfunctions of the Sturm–Liouville problem of Example 6.2.
For n = 0, we have
a0 =
#
f,

1
L
$
=

1
L
 L
0
x dx = (L)3/2
2
.
For n ̸= 0, we have
an =
#
f,

2
L cos nπx
L
$
=

2
L
 L
0
x cos nπx
L
dx = −L
√
2L
(nπ)2 [1 −(−1)n].
Therefore, the series
L
2 −2L
π2
∞

n=1
[1 −(−1)n]
n2
cos nπx
L
= L
2 −4L
π2
∞

k=0
1
(2k + 1)2 cos (2k + 1)πx
L
is the eigenfunction expansion of f that converges to x for all x ∈(0, L). This
expansion converges uniformly on [0, L], although the expansion theorem does not
ensure this.
Example 6.33 Find the eigenfunction expansion of the function
f (x) =
 x
0 ≤x ≤1,
1
1 ≤x ≤2
(6.69)
with respect to the (classical Fourier) orthonormal system

1
2
"
∪{cos nπx}∞
n=1 ∪{sin nπx}∞
n=1,

150
Sturm–Liouville problems
the eigenfunctions of the periodic Sturm–Liouville problem of Example 6.5 on
[0, 2].
For n = 0, we obtain
a0 =
#
f,

1
2
$
=
3
2
√
2
.
For n ̸= 0, we have
an = ⟨f, cos nπx⟩=
 1
0
x cos nπx dx +
 2
1
cos nπx dx = −[1 −(−1)n]
(nπ)2
.
In addition,
bn = ⟨f, sin nπx⟩=
 1
0
x sin nπx dx +
 2
1
sin nπx dx = −1
nπ .
Therefore, the series
3
4 +
∞

n=1
[(−1)n −1]
n2π2
cos nπx −1
nπ sin nπx

is the corresponding eigenfunction expansion of f that converges to f for all
x ∈(0, 2). This expansion does not converge uniformly on [0, 2], since f does not
satisfy the periodic boundary conditions.
Although the eigenfunction expansion for a piecewise differentiable function
may not converge uniformly, it frequently happens that the expansion converges
uniformly on any subinterval that does not contain the end points and jump discon-
tinuities. Recall that at a jump discontinuity, the eigenfunction expansion converges
to the average of the two one-sided limits of f . When one draws the graphs of the
sums of the ﬁrst N terms of this eigenfunction expansion, one notices oscillations
that appear near the jump points. The oscillations persist even as the number of
terms in the expansion is increased. These oscillations (which appear only for ﬁnite
sums) are called the Gibbs phenomenon after the American scientist Josiah Willard
Gibbs (1839–1903) who discovered them. We demonstrate the Gibbs phenomenon
in the following example.
Example 6.34 Consider the following 2π-periodic function:
f (x) =

−1
−π < x < 0,
1
0 < x < π,
and f (x + 2π) = f (x), which is sometimes called a square wave. This function is
discontinuous at integer multiples of π. The eigenfunction expansion with respect

6.4 Eigenfunctions and eigenvalues: basic properties
151
Figure 6.1 The Gibbs phenomenon for the square wave function: the partial sums
for (a) N = 8 and (b) N = 24.
to the classical Fourier series is given by
f (x) = 4
π
∞

k=0
sin(2k + 1)x
2k + 1
.
(6.70)
Note that the eigenfunction expansions (6.68) for L = π and (6.70) look the same.
Clearly, the series (6.70) does not converge uniformly on R. Consider the partial
sum
fN(x) := 4
π
N

k=0
sin(2k + 1)x
2k + 1
.
In Figure 6.1 the graphs of f8 and f24 are illustrated. It can be seen that while
adding terms improves the approximation, no matter how many terms are added,
there is always a ﬂuctuation near the jump at x = 0 (overshoot before the jump and
undershoot after it). To see the oscillation better, we concentrate the graphs on the
interval (−π/2, π/2). The graph of f is drawn (dashed line) in the background for
comparison.
8 Rayleigh quotients An important problem that arises frequently in chemistry
and physics is how to compute the spectrum of a quantum system. The system is
modeled by a Schr¨odinger operator. In the one-dimensional case such operators
are of the Sturm–Liouville type. For instance, the information from the spectrum
of the Schr¨odinger operator enables us to determine the discrete frequencies of
the radiation from excited atoms (We shall present an explicit computation of the
spectral lines of the hydrogen atom in Chapter 9.) In addition, using the information
from the spectrum, one can understand the stability of atoms and molecules. We

152
Sturm–Liouville problems
do not present here a precise deﬁnition of the spectrum of a given linear operator,
but roughly speaking, the (point) spectrum of a quantum system is given by the
eigenvalues of the corresponding Schr¨odinger operator. It is particularly important
to ﬁnd the ﬁrst (minimal) eigenvalue, or at least a good approximation of it.
Remark 6.35 In the periodic case and in many other important cases, the minimal
eigenvalue is simple (as for any eigenvalue in the regular case).
Deﬁnition 6.36 The minimal eigenvalue of a Sturm–Liouville problem is called the
principal eigenvalue (or the ground state energy), and the corresponding eigenfunc-
tion is called the principal eigenfunction (or the ground state). The British scientist
John William Strutt (Lord Rayleigh) (1842–1919) observed that the expression
R(u) = −
 b
a uL[u] dx
 b
a u2r dx
playsanimportantroleinthiscontext.Therefore R(u)iscalledtheRayleighquotient
of u.
Most of the numerical methods for computing the eigenvalues of a symmet-
ric operator are based on the following variational principle, which is called the
Rayleigh–Ritz formula.
Proposition 6.37 The principal eigenvalue λ0 of a regular Sturm–Liouville prob-
lem satisﬁes the following variational principle:
λ0 = inf
u∈V R(u) = inf
u∈V −
 b
a uL[u] dx
 b
a u2r dx
,
(6.71)
where
V = {u ∈C2([a, b]) | Ba[u] = Bb[u] = 0, u ̸= 0}.
Moreover, the inﬁmum of the Rayleigh quotient is attained only by the principal
eigenfunction.
For a periodic Sturm–Liouville problem, (6.71) holds true with
V = {u ∈C2([a, b]) | v(a) = v(b), v′(a) = v′(b), v ̸= 0}.
Proof The following proof is not complete since it relies on some auxiliary lem-
mas which we do not prove here. Let {λn}∞
n=0 be the increasing sequence of all
eigenvalues of the given problem, and let {vn}∞
n=0 be the orthonormal system of the
corresponding eigenfunctions.

6.4 Eigenfunctions and eigenvalues: basic properties
153
If u ∈V , then the eigenfunction expansion of u converges uniformly to u, i.e.
u(x) =
∞

n=0
anvn(x).
Without a rigorous justiﬁcation, let us exchange the order of summation and dif-
ferentiation. This implies that
L[u] =
∞

n=0
anL[vn(x)] = −
∞

n=0
anλnr(x)vn(x).
We substitute the above expression into the numerator of the Rayleigh quotient, and
integrate term by term (again, without a rigorous justiﬁcation), using the orthogo-
nality relations. For the denominator of the Rayleigh quotient, we use the Parseval
identity. We obtain
R(u) = −
 b
a uL[u] dx
 b
a u2r dx
=
 b
a
∞
m=0
∞
n=0 amanλnr(x)vm(x)vn(x)

dx
∞
m=0 a2n
=
∞
m=0
∞
n=0 amanλn
 b
a r(x)vm(x)vn(x) dx
∞
m=0 a2n
=
∞
n=0 a2
nλn
∞
m=0 a2n
≥
∞
n=0 a2
nλ0
∞
m=0 a2n
= λ0.
Therefore, R(u) ≥λ0 for all u ∈V , and thus, infu∈V R(u) ≥λ0. It is easily
veriﬁed that equality holds if and only if u = Cv0 (recall that λ0 is always a simple
eigenvalue), and the proposition is proved.
□
Remark 6.38 The following alternative method for computing the principal eigen-
value can be derived from the Rayleigh–Ritz formula through an integration by parts
of (6.71)
λ0 = inf
u∈V
 b
a

p(u′)2 −qu2
dx −puu′|b
a
 b
a u2r dx
,
(6.72)
where
V = {u ∈C2([a, b]) | Ba[u] = Bb[u] = 0, u ̸= 0}.
Actually, (6.72) is more useful than (6.71) since it does not involve second deriva-
tives. In particular, for the Dirichlet (or Neumann, or periodic) problem, we have
λ0 = inf
u∈V
 b
a

p(u′)2 −qu2
dx
 b
a u2r dx
.
(6.73)

154
Sturm–Liouville problems
Corollary 6.39 If q ≤0, and if puu′|b
a ≤0 for all functions u ∈V , then all the
eigenvalues of the Sturm–Liouville problem are nonnegative. In particular, for the
Dirichlet (or Neumann, or periodic) problem, if q ≤0, then all the eigenvalues of
the problem are nonnegative.
Example 6.40 Consider the following Sturm–Liouville problem:
d2v
dx2 + λv = 0
0 < x < 1,
(6.74)
v(0) = v(1) = 0.
(6.75)
We already know that the principal eigenfunction is v0(x) = sin πx, with the prin-
cipal eigenvalue λ0 = π2. If we use the test function u(x) = x −x2 in the Rayleigh
quotient, we obtain the bound
R(u) = 10 ≥π2 ≈9.86.
This bound is a surprisingly good approximation for λ0.
In general, it is not possible to explicitly compute the eigenfunctions and
the eigenvalues λn. But the Rayleigh–Ritz formula has a useful generaliza-
tion for λn with n ≥1. In fact, using Rayleigh quotients with appropriate
test functions, one can obtain good approximations for the eigenvalues of the
problem.
9 Zeros of eigenfunctions The following beautiful result holds (for a proof see
Volume 1 of [4]).
Proposition 6.41 Consider a regular Sturm–Liouville problem on the interval
(a, b). Let {λn}∞
n=0 be the increasing sequence of all eigenvalues, and {vn}∞
n=0 be the
corresponding complete orthonormal sequence of eigenfunctions. Then vn admits
exactly n roots on the interval (a, b), for n = 0, 1, 2 . . . . In particular, the principal
eigenfunction v0 does not change its sign in (a, b).
The reader can check the proposition for the orthonormal systems of Examples 6.1
and 6.2.
10 Asymptotic behavior of high eigenvalues and eigenfunctions As was men-
tioned above, the eigenvalues λn of a regular Sturm–Liouville problem cannot, in
general, be computed precisely; using the (generalized) Rayleigh–Ritz formula,
however, we can obtain a good approximation for λn. It turns out that for large n
there is no need to use a numerical method, since the asymptotic behavior of large
eigenvalues is given by the following formula discovered by the German–American

6.4 Eigenfunctions and eigenvalues: basic properties
155
mathematician Herman Weyl (1885–1955). Write
ℓ:=
 b
a
%
r(x)
p(x) dx,
then
λn ∼
nπ
ℓ
2
.
(6.76)
The symbol ∼in the preceding formula means an asymptotic relation, i.e.
lim
n→∞
ℓ2λn
(nπ)2 = 1.
Furthermore,itisknownthatthegeneralsolutionoftheequation L[u] + λru = 0
forlargeλbehavesasalinearcombinationofcosandsin.Moreprecisely,thegeneral
solution of the above equation for large λ takes the form
u(x)∼[r(x)p(x)]−1/4

α cos
&
√
λ
 x
a
%
r(s)
p(s) ds
'
+β sin
&
√
λ
 x
a
%
r(s)
p(s) ds
'"
.
It follows that the orthonormal sequence {vn(x)} of all eigenfunctions is uni-
formly bounded. Moreover, for large n, we have the asymptotic estimates (Vol I of
[4], [9]):
|vn(x)| ≤C0,
dvn(x)
dx
 ≤C1

λn,
d2vn(x)
d2x
 ≤C2λn ≤C3n2.
(6.77)
Example 6.42 Let h > 0 be a ﬁxed number. Consider the following mixed eigen-
value problem
u′′ + λu = 0
0 < x < 1,
u(0) = 0, hu(1) + u′(1) = 0.
(6.78)
If λ < 0, then a solution of the ODE above that satisﬁes the ﬁrst boundary
condition is a function of the form uλ(x) = C sinh √−λx. The boundary condition
at x = 1 implies that 0 < tanh √−λ = −√−λ/h < 0 which is impossible. This
means that there are no negative eigenvalues.
If λ = 0, then a nontrivial solution of the corresponding ODE is a linear function
of the form u0(x) = cx + d where |c| + |d| > 0. But such a function cannot satisfy
both boundary conditions, since our boundary conditions clearly imply that d = 0
and c(1 + h) = 0.
If λ > 0, then an eigenfunction has the form uλ(x) = C sin
√
λx, where λ is a
solution of the transcendental equation
tan
√
λ = −
√
λ/h.
(6.79)

156
Sturm–Liouville problems
0
1
2
3
4
5
6
7
8
−20
−15
−10
−5
0
5
10
15
20
tan s = −s/h
h > 0
Figure 6.2 The graphical solution of (6.79).
Equation (6.79) cannot be solved analytically. Using the intermediate value theo-
rem, however, we can verify that this transcendental equation has inﬁnitely many
roots λn that satisfy (n −1/2)π < √λn < nπ (see Figure 6.2). Therefore, all the
eigenvalues are positive and simple. Note that using Corollary 6.39, we could have
concluded directly that there are no negative eigenvalues, since in our case q = 0,
and for all u ∈V we have
puu′|1
0 = −h(u′(1))2 ≤0.
Let us check the asymptotic behavior of the eigenvalues as a function of n,
and also as a function of h. Denote the sequence of the eigenvalues of the above
Sturm–Liouville problem by {λ(h)
n }∞
n=1. The nth eigenvalue satisﬁes
(n −1/2)π <
(
λ(h)
n
< nπ
n = 1, 2, . . . .
Using our graphical solution (Figure 6.2), we verify that as n →∞the asymp-
totic formula (6.76) is satisﬁed, namely,
lim
n→∞
λ(h)
n
n2π2 = 1.
Let h →0+. The slope of the straight line −s/h tends to −∞. Therefore, this
line intersects the graph of the function tan s closer and closer to the negative

6.4 Eigenfunctions and eigenvalues: basic properties
157
asymptotes of tan s. Hence,
lim
h→0 λ(h)
n
=
(2n −1)π
2
2
.
Indeed, for h = 0 we have the Sturm–Liouville problem
u′′ + λu = 0
0 < x < 1,
u(0) = 0, u′(1) = 0,
(see Example 6.19), and the eigenvalues and eigenfunctions of this problem are
λn =
(2n −1)π
2
2
,
un(x) = Cn sin (2n −1)πx
2
,
n = 1, 2, . . . .
Similarly, we have
lim
h→∞λ(h)
n
= (nπ)2,
which are the eigenvalues of the limit problem:
u′′ + λu = 0
0 < x < 1,
u(0) = 0, u(1) = 0,
(see Example 6.1). The reader should check as an exercise that for h > 0, the nth
eigenfunction admits exactly (n −1) zeros in (0, 1), where n = 1, 2 . . . .
Example 6.43 Assume now that h is a ﬁxed negative number. Consider again the
mixed eigenvalue problem
u′′ + λu = 0
0 < x < 1,
u(0) = 0, hu(1) + u′(1) = 0.
If λ < 0, then a solution of the ODE that satisﬁes the boundary condition at
x = 0 is of the form uλ(x) = C sinh √−λx. The boundary condition at x = 1
implies tanh √−λ = −√−λ/h. Since the function tanh s is a concave increasing
function on [0, ∞) that satisﬁes
tanh(0) = 0,
(tanh)′(0) = 1,
lim
s→∞tanh s = 1,
it follows that the equation tanh s = −s/h has a positive solution if and only if h <
−1. Moreover, under this condition there is exactly one solution (see Figure 6.3).
Thisisanecessaryandsufﬁcientconditionfortheexistenceofanegativeeigenvalue.
The corresponding eigenfunction of the unique negative eigenvalue λ0 is of the form
u0(x) = sinh √−λ0x that indeed does not vanish on (0, 1).

158
Sturm–Liouville problems
0
1
2
3
4
5
−0.5
0
0. 5
1
1. 5
tanh (s) = −s/h
h > 0 
−1 < h < 0
h < −1
h = −1
Figure 6.3 The graphical solution method for negative eigenvalues in Example 6.43.
If λ = 0, then a solution of the corresponding ODE is of the form u0(x) =
cx + d. The ﬁrst boundary condition implies d = 0, and from the second boundary
condition we have c(1 + h) = 0. Consequently, λ = 0 is an eigenvalue if and only
if h = −1. If h = −1, then λ0 = 0 is the minimal eigenvalue, and corresponding
eigenfunction is u0(x) = x (notice that this function does not vanish on (0, 1)).
If λ > 0, then an eigenfunction has the form uλ(x) = C sin
√
λx, where λ is a
solution of the transcendental equation
tan
√
λ = −
√
λ/h.
This equation has inﬁnitely many solutions: If h ≤−1, then the solutions of this
equation satisfy nπ < √λn < (n + 1/2)π, where n ≥1. On the other hand, if
−1 < h < 0, then the minimal eigenvalue satisﬁes 0 < √λ0 < π/2, while nπ <
√λn < (n + 1/2)π for all n ≥1 (see Figure 6.4). Note that for n ≥0, the function
λ(h)
n
is an increasing function of h.
The asymptotic behavior of the eigenvalues of the present example as a function
of h or n is similar to the behavior in the preceding example (h > 0).
The reader should check as an exercise the number of roots of the nth eigenfunc-
tion for h < 0. Here one should distinguish between the following three cases:
(1) The problem admits a negative eigenvalue (h < −1).
(2) The principal eigenvalue is zero (h = −1).
(3) All the eigenvalues are positive (h > −1).

6.5 Nonhomogeneous equations
159
0
1
2
3
4
5
−10
−5
0
5
10
15
20
tan s = −s/h
h < 0 
−1 < h < 0 
h = −1
h < −1
Figure 6.4 The graphical solution method for positive eigenvalues in Example 6.43.
6.5 Nonhomogeneous equations
We turn our attention to the case of nonhomogeneous problems in the general
context of the Sturm–Liouville theory.
Consider ﬁrst a general nonhomogeneous parabolic initial boundary value prob-
lem with homogeneous boundary conditions. One can solve a general hyperbolic
problem similarly, and we leave this to the reader as an exercise.
We seek a function u(x, t) that is a solution of the problem
r(x)m(t)ut −[(p(x)ux)x + q(x)u] = F(x, t)
a < x < b, t > 0,
Ba[u] = αu(a, t) + βux(a, t) = 0
t ≥0,
Bb[u] = γ u(b, t) + δux(b, t) = 0
t ≥0,
u(x, 0) = f (x)
a ≤x ≤b.
(6.80)
One can also deal with periodic boundary conditions by the same method.
The related Sturm–Liouville eigenvalue problem that is derived from the homo-
geneous problem using the method of separation of variables is of the form
L[v] + λrv = (pv′)′ + qv + λrv = 0
a < x < b,
(6.81)
Ba[v] = Bb[v] = 0.
(6.82)
Let{vn}∞
n=0 bethecompleteorthonormalsequenceofeigenfunctionsoftheproblem,
and let the corresponding sequence of eigenvalues be denoted by {λn}∞
n=0, where

160
Sturm–Liouville problems
the eigenvalues are in nondecreasing order (repeated eigenvalues are allowed, since
we may consider also the periodic case).
Suppose that the functions f (x) and F(x, t)/r(x) (for all t ≥0) are continuous,
piecewise differentiable functions that satisfy the boundary conditions. It follows
that the eigenfunction expansions of f (x) and F(x, t)/r(x) (for all t ≥0) converge
uniformly on [a, b]. In particular,
f (x) =
∞

n=0
fnvn(x),
F(x, t)
r(x)
=
∞

n=0
Fn(t)vn(x),
where
fn =
 b
a
f (x)vn(x)r(x) dx,
Fn(t) =
 b
a
F(x, t)vn(x) dx.
Let u(x, t) be a solution of our problem. For t ≥0, the function u(·, t) is continu-
ous (and even twice differentiable for t > 0) and satisﬁes the boundary conditions.
Therefore,thegeneralizedFourierseriesof u withrespecttotheorthonormalsystem
{vn}∞
n=0 converges (and uniformly so for t > 0) and has the form
u(x, t) =
∞

n=0
an(t)vn(x),
where
an(t) =
 b
a
u(x, t)vn(x)r(x) dx.
Let us ﬁx n ≥0. Substituting the time derivative of an into the PDE leads to
m(t)a′
n(t)=
 b
a
m(t)r(x)∂u(x, t)
∂t
vn(x) dx
=
 b
a
L[u(x, t)]vn(x) dx+
 b
a
F(x, t)vn(x) dx.
Green’s formula with respect to the functions vn(x) and u(x, t) implies that
m(t)a′
n(t) =
 b
a
u(x, t)L[vn(x)] dx +
 b
a
F(x, t)vn(x) dx
= −λn
 b
a
u(x, t)vn(x)r(x) dx +
 b
a
F(x, t)vn(x) dx
= −λnan(t) + Fn(t).
Therefore, the function an is a solution of the ODE
m(t)a′
n + λnan = Fn(t).

6.5 Nonhomogeneous equations
161
The solution of this ﬁrst-order linear ODE is given by
an(t) = an(0)e−λn
 t
0
1
m(s) ds + e−λn
 t
0
1
m(s) ds
 t
0
Fn(τ)
m(τ) eλn
 τ
0
1
m(s) ds dτ
(see Formula (1) of Section A.3). The continuity of u at t = 0, and the initial
condition u(x, 0) = f (x) imply that
an(0) = fn.
Thus, we propose a solution of the form
u(x, t) =
∞

n=0
fnvn(x)e−λn
 t
0
1
m(s) ds +
∞

n=0
vn(x)e−λn
 t
0
1
m(s) ds
 t
0
Fn(τ)
m(τ) eλn
 τ
0
1
m(s) ds dτ.
We need to show that u is indeed a classical solution. For this purpose we estimate
the general term of the series and its derivatives. Since m is a positive continuous
function on [0, T ], there exist constants 0 < c1 ≤c2 such that
c−1
2
≤m(t) ≤c−1
1 ,
and hence,
c1t ≤
 t
0
1
m(s) ds ≤c2t.
Consequently, for all 0 ≤t ≤T we have
e−c2λnt ≤e−λn
 t
0
1
m(s) ds ≤e−c1λnt.
Furthermore,
 eλn
 τ
0
1
m(s) ds
m(τ)
dτ = eλn
 τ
0
1
m(s) ds
λn
+ C.
Thus
e−λn
 t
0
1
m(s) ds
 t
0
Fn(τ)eλn
 τ
0
1
m(s) ds
m(τ)
dτ

≤e−λn
 t
0
1
m(s) ds max
0≤t≤T |Fn(t)|
 t
0
eλn
 τ
0
1
m(s) ds
m(τ)
dτ
= 1
λn
max
0≤t≤T |Fn(t)|(1 −e−λn
 t
0
1
m(s) ds) ≤1
λn
max
0≤t≤T |Fn(t)|.
Moreover, by the asymptotic estimates (6.77):
|vn(x)| ≤C0,
|dvn(x)
dx
| ≤C1

λn,
|d2vn(x)
d2x
| ≤C2λn ≤C3n2.

162
Sturm–Liouville problems
Since |vn(x)| ≤C0, it follows that | fn| and |Fn(t)| are uniformly bounded. We
assume further that the series  fn and  Fn(t) converge absolutely and uniformly
(on 0 ≤t ≤T ). This assumption can be veriﬁed directly in many cases where f
and F are twice differentiable and satisfy the boundary conditions. In other words,
we assume that
∞

n=0
[| fn| + max
0≤t≤T |Fn(t)|] < ∞.
We are now ready to prove that the proposed series is a classical solution. First
we show that the eigenfunction expansion of u(x, t) satisﬁes the parabolic PDE
r(x)m(t)ut −[(p(x)ux)x + q(x)u] = F(x, t)
a < x < b, t > 0.
Differentiate (term by term) the series of u, twice with respect to x and once with
respect to t for 0 < ε ≤t ≤T . We claim that the obtained series is uniformly
converging. Indeed, using the asymptotic estimates, an(t)vn(x) (the general term of
the series of u), its ﬁrst- and second-order derivatives with respect to x, and its ﬁrst
derivative with respect to t are all bounded by
Cλne−c1λnε + C max
0≤t≤T |Fn(t)| ≤C1n2e−C2n2ε + C max
0≤t≤T |Fn(t)|.
By the Weierstrass M-test for uniform convergence, the corresponding series of the
function u and its derivatives (up to second order in x and ﬁrst order in t) converge
uniformly in the rectangle
{(x, t) | a ≤x ≤b, ε ≤t ≤T }.
Consequently, we may differentiate the series of u term by term, twice with re-
spect to x or once with respect to t, and therefore, u is evidently a solution of the
nonhomogeneous PDE and satisﬁes the boundary conditions for t > 0.
For 0 ≤x ≤π , 0 ≤t ≤T the general term of the series of u is bounded by
C

| fn| + max0≤t≤T |Fn(t)|
λn

.
By the Weierstrass M-test, the series of u converges uniformly on the strip
{(x, t) | 0 ≤x ≤π , 0 ≤t ≤T }.
Thus, the solution u is continuous on this strip and, in particular, at t = 0 we have
u(x, 0) = f (x). Hence, u is a classical solution.
Remark 6.44 In the hyperbolic case, the ODE for the coefﬁcient an is of second
order and has the form
m(t)a′′
n + λnan = Fn(t).

6.5 Nonhomogeneous equations
163
Example 6.45 Let m ∈N and let ω ∈R, and assume ﬁrst that ω2 ̸= m2π2. Solve
the following wave problem:
utt −uxx = sin mπx sin ωt
0 < x < 1, t > 0,
u(0, t) = u(1, t) = 0
t ≥0,
u(x, 0) = 0
0 ≤x ≤1,
ut(x, 0) = 0
0 ≤x ≤1.
(6.83)
The related Sturm–Liouville problem is of the form
v′′ + λ v = 0,
v(0) = v(1) = 0.
(6.84)
The eigenvalues are given by λn = n2π2, and the corresponding eigenfunctions are
vn(x) = sin nπx, where n = 1, 2, . . . . Therefore, the eigenfunction expansion of
the solution u of (6.83) is
u(x, t) =
∞

n=1
Tn(t) sin nπx.
(6.85)
In order to compute the coefﬁcients Tn(t), we (formally) substitute the series (6.85)
into (6.83) and differentiate term by term. We ﬁnd
∞

n=1
(T ′′
n + n2π2 Tn) sin nπx = sin mπx sin ωt.
(6.86)
Thus, for n = m we need to solve the nonhomogeneous equation
T ′′
m + m2π2Tm = sin ωt.
(6.87)
The corresponding initial conditions are zero. Therefore, the solution of this initial
value problem is given by
Tm(t) =
1
ω2 −m2π2
 ω
mπ sin mπt −sin ωt

.
(6.88)
For n ̸= m the corresponding ODE is
T ′′
n + n2π2Tn = 0,
(6.89)
and again the initial conditions are zero. We conclude that Tn(t) = 0 for n ̸= m,
and
u(x, t) =
1
ω2 −m2π2
 ω
mπ sin mπt −sin ωt

sin mπx.
(6.90)
As can be easily veriﬁed, u is a classical solution.

164
Sturm–Liouville problems
Assume now that ω2 = m2π2. Comparing the new problem with the previous
one, we see that in solving the new problem the only difference occurs in (6.87). A
simple way to derive the solution for ω2 = m2π2 from (6.90) is by letting ω →mπ.
Using L’Hospital’s rule, we obtain
u(x, t) =
1
mπ
	sin mπt
mπ
−t cos mπt

sin mπx.
(6.91)
Let us discuss the important result that we have just obtained. Recall that the
natural frequencies of the free string (without forcing) are nπ for n = 1, 2, . . . .
If the forcing frequency is not equal to one of the natural frequencies, the vibra-
tion of the string is a superposition of vibrations in the natural frequencies and in
the forcing frequency, and the amplitude of the vibration is bounded. The energy
provided by the external force to the string is divided between these two types of
motion.
On the other hand, when the forcing frequency is equal to one of the natural
frequencies, the amplitude of the vibrating string grows linearly in t and it is
unbounded as t →∞. Of course, at some point the string will be ripped apart.
The energy that is given to the string by the external force concentrates around
one natural frequency and causes its amplitude to grow. This phenomenon is called
resonance. It can partly explain certain cases where structures such as bridges and
buildings collapse (see also Example 9.27). Note that the resonance phenomenon
does not occur in the heat equation (see Exercise 5.7).
6.6 Nonhomogeneous boundary conditions
We now consider a general, one-dimensional, nonhomogeneous, parabolic initial
boundary problem with nonhomogeneous boundary conditions (the hyperbolic case
can be treated similarly). Let u(x, t) be a solution of the problem
r(x)m(t)ut −[(p(x)ux)x + q(x)u] = F(x, t) a < x < b, t > 0,
Ba[u] = αu(a, t) + βux(a, t) = a(t)
t ≥0,
Bb[u] = γ u(b, t) + δux(b, t) = b(t)
t ≥0,
u(x, 0) = f (x)
a ≤x ≤b.
(6.92)
We already know how to use the eigenfunction expansion method to solve for
homogeneous boundary conditions. We describe a simple technique for reducing
the nonhomogeneous boundary conditions to the homogeneous case.
First we look for an auxiliary simple smooth function w(x, t) satisfying (only)
the given nonhomogeneous boundary conditions. In fact, we can always ﬁnd such

6.6 Nonhomogeneous boundary conditions
165
Table 6.1.
Boundary condition
w(x, t)
Dirichlet: u(0, t)=a(t),
u(L, t)=b(t)
w(x, t)=a(t)+ x
L [b(t)−a(t)]
Neumann: ux(0, t)=a(t),
ux(L, t)=b(t)
w(x, t)=xa(t)+ x2
2L [b(t)−a(t)]
Mixed: u(0, t)=a(t),
ux(L, t)=b(t)
w(x, t)=a(t)+xb(t)
Mixed: ux(0, t)=a(t),
u(L, t)=b(t)
w(x, t)=(x −L)a(t)+ b(t)
a function that has the form
w(x, t) = (A1 + B1x + C1x2)a(t) + (A2 + B2x + C2x2)b(t).
(6.93)
Clearly, the function v(x, t)=u(x, t) −w(x, t) should satisfy the homogeneous
boundary conditions Ba[v] = Bb[v] = 0.
In the second step we check what are the PDE and the initial condition that v
should satisfy in order for u to be a solution of the problem. By the superposition
principle, it follows that v should be a solution of the following initial boundary
problem
r(x)m(t)vt −((p(x)vx)x + q(x)v) = ˜F(x, t)
a < x < b, t > 0,
Ba[v] = αv(a, t) + βvx(a, t) = 0
t ≥0,
Bb[v] = γ v(b, t) + δvx(b, t) = 0
t ≥0,
v(x, 0) = ˜f (x)
a ≤x ≤b,
where
˜F(x, t)= F(x, t)−r(x)m(t)wt +[(p(x)wx)x +q(x)w] ,
˜f (x)= f (x)−w(x, 0).
Since this is exactly the kind of problem that was solved in the preceding section,
we can proceed just as was explained there. To assist the reader in solving non-
homogeneous equations we present Table 6.1 where we list appropriate auxiliary
functions w for the various boundary value problems.
We conclude this section with a ﬁnal example in which we solve a nonhomoge-
neous heat problem.
Example 6.46 Consider the problem
ut −uxx = e−t sin 3x
0 < x < π , t > 0,
u(0, t) = 0 , u(π, t) = 1
t ≥0 ,
u(x, 0) = f (x)
0 ≤x ≤π.

166
Sturm–Liouville problems
We shall solve the problem by the method of separation of variables. We shall also
show that, under some regularity assumptions on f , the solution u is classical.
Recall that the eigenvalues and the corresponding eigenfunctions of the related
Sturm–Liouville problem are of the form: {λn =n2, vn(x)=√2/π sin nx }∞
n=1. In
the ﬁrst step, we reduce the problem to one with homogeneous boundary conditions.
Using Table 6.1 we select the auxiliary function w(x, t) = x/π that satisﬁes the
given boundary conditions. Setting v(x, t) = u(x, t) −w(x, t), then v(x, t) is a
solution of the problem:
vt −vxx = e−t sin 3x
0 < x < π , t > 0,
v(0, t) = 0 , v(π, t) = 0
t ≥0 ,
v(x, 0) = f (x) −x/π
0 ≤x ≤π.
We assume that v is a classical solution that is a smooth function for t > 0. In
particular, for a ﬁxed t > 0 the eigenfunction expansion of v(x, t) with respect the
eigenfunctions of the related Sturm–Liouville problem converges uniformly and is
of the form
v(x, t) =
∞

n=1
an(t) sin nx,
where
an(t) = 2
π
 π
0
v(x, t) sin nx dx.
Since by our assumption v is smooth, we can differentiate the function an with
respect to t and then substitute the expansions for the derivatives into the PDE. We
obtain
a′
n(t) = 2
π
 π
0
(vxx(x, t) + e−t sin 3x) sin nx dx.
By Green’s identity, with the functions sin nx and v(x, t), and the operator L[u] =
∂2u/∂x2 we have
a′
n(t) = 2
π
 π
0
(−n2v(x, t) + e−t sin 3x) sin nx dx
= −n2an(t) + 2e−t
π
 π
0
sin 3x sin nx dx.
Consequently, an is a solution of the ODE
a′
n + n2an =

0
n ̸= 3,
e−t
n = 3.

6.6 Nonhomogeneous boundary conditions
167
The solution of this ODE is given by:
an(t) =

an(0)e−n2t
n ̸= 3,
1/8e−t + [a3(0) −1/8]e−9t
n = 3.
The continuity of v at t = 0 and the initial condition f imply that
an(0) = 2
π
 π
0
[ f (x) −x/π] sin nx dx,
and the proposed solution is
u(x, t) = x
π + 1
8(e−t −e−9t) sin 3x +
∞

n=1
an(0) sin nxe−n2t.
(6.94)
It remains to show that u is indeed a classical solution. For this purpose, we
assume further that f (x) ∈C2([0, π]) and satisﬁes the compatibility condition
f (0) = 0, f (π) = 1. Under these assumptions, it follows from the convergence
theorems that the eigenfunction expansion of the function of f (x) −x/π converges
uniformly to f (x) −x/π. Moreover, for the orthogonal system {sin nx} it is known
[13] from classical Fourier analysis that, under the above conditions, the series
 |an(0)| converges.
We ﬁrst prove that u(x, t) satisﬁes the nonhomogeneous heat equation
ut −uxx = e−t sin 3x
0 < x < π , t > 0.
Since f is bounded on [0, π], the Fourier coefﬁcients an(0) are bounded. For
t > ε > 0, we formally differentiate the general term of the series of u twice with
respect to x or once with respect to t. The obtained terms are bounded by Cn2e−n2ε.
Consequently, by the Weierstrass M-test, the corresponding series converges uni-
formly on the strip
{(x, t) | 0 ≤x ≤π , ε ≤t}.
Therefore, the term-by-term differentiation of the series of u is justiﬁed, and by our
construction, u is a solution of the PDE.
For 0 ≤x ≤π , t ≥0, the general term of u is bounded by |an(0)|. Hence, by
the Weierstrass M-test, the series of u converges uniformly on the strip
{(x, t) | 0 ≤x ≤π , t ≥0}.
It follows that the series representing u is continuous there, and by substituting
x = 0, π, we see that u satisﬁes the boundary conditions.

168
Sturm–Liouville problems
Similarly, substituting t = 0 implies that
u(x, 0) −x
π =
∞

n=0
an(0) sin nx,
which is the eigenfunction expansion of ( f (x) −x/π). By Proposition 6.30, the ex-
pansion converges uniformly to ( f (x) −x/π). In particular, u(x, 0) = f (x). Thus,
u is a classical solution.
In the special case where f (x) = (x/π)2, then
an(0) = 2
π
 π
0
[(x/π)2 −x/π] sin nx dx =



−8
π3(2k + 1)3
n = 2k + 1,
0
n = 2k.
Substituting an(0) into (6.94) implies the solution
u(x, t) = x
π + 1
8(e−t −e−9t) sin 3x −8
π3
∞

k=0
sin(2k + 1)x
(2k + 1)3
e−(2k+1)2t
which is indeed a classical solution.
6.7 Exercises
6.1 Consider the following Sturm–Liouville problem
u′′ + λu = 0
0 < x < 1,
u(0) −u′(0) = 0,
u(1) + u′(1) = 0.
(a) Show that all the eigenvalues are positive.
(b) Solve the problem.
(c) Obtain an asymptotic estimate for large eigenvalues.
6.2 (a) Solve the Sturm–Liouville problem
(xu′)′ + λ
x u = 0
1 < x < e,
u(1) = u′(e) = 0.
(b) Show directly that the sequence of eigenfunctions is orthogonal with respect the
related inner product.
6.3 (a) Consider the Sturm–Liouville problem
(x2v′)
′ + λv = 0
1 < x < b,
v(1) = v(b) = 0,
(b > 1).
Find the eigenvalues and eigenfunctions of the problem.
Hint Show that the function
v(x) = x−1/2 sin(α ln x)
is a solution of the ODE and satisﬁes the boundary condition v(1) = 0.

6.7 Exercises
169
(b) Write a formal solution of the following heat problem
ut = (x2ux)x
1 < x < b, t > 0,
(6.95)
u(1, t) = u(b, t) = 0
t ≥0,
(6.96)
u(x, 0) = f (x)
1 ≤x ≤b.
(6.97)
6.4 Use the Rayleigh quotient to ﬁnd a good approximation for the principal eigenvalue
of the Sturm–Liouville problem
u′′ + (λ −x2)u = 0
0 < x < 1,
u′(0) = u(1) = 0.
6.5 (a) Solve the Sturm–Liouville problem
((1 + x)2u′)′ + λu = 0
0 < x < 1,
u(0) = u(1) = 0.
(b) Show directly that the sequence of eigenfunctions is orthogonal with respect the
related inner product.
6.6 Prove that all the eigenvalues of the following Sturm–Liouville problem are positive.
u′′ + (λ −x2)u = 0
0 < x < 1,
u′(0) = u′(1) = 0.
6.7 (a) Solve the Sturm–Liouville problem
x2u′′ + 2xu′ + λu = 0
1 < x < e,
u(1) = u(e) = 0.
(b) Show directly that the sequence of eigenfunctions is orthogonal with respect the
related inner product.
6.8 Prove that all the eigenfunctions of the following Sturm–Liouville problem are
positive.
u′′ + (λ −x2)u = 0
0 < x < ∞,
u′(0) = lim
x→∞u(x) = 0.
6.9 Consider the eigenvalue problem
u′′ + λu = 0
−1 < x < 1,
(6.98)
u(1) + u(−1) = 0,
u′(1) + u′(−1) = 0.
(6.99)
(a) Prove that for u, v ∈C2([−1, 1]) that satisfy the boundary conditions (6.99) we
have
 1
−1
[u′′(x)v(x) −v′′(x)u(x)] dx = 0.
(b) Show that all the eigenvalues are real.

170
Sturm–Liouville problems
(c) Find the eigenvalues and eigenfunctions of the problem.
(d) Determine the multiplicity of the eigenvalues.
(e) Explain if and how your answer for part (d) complies with the Sturm–Liouville
theory.
6.10 Show that for n ≥0, the eigenfunction of the nth eigenvalue of the Sturm–Liouville
problem (6.78) has exactly n roots in (0, 1).
6.11 Solve the problem
ut −uxx + u = 2t + 15 cos 2x
0 < x < π/2, t > 0,
ux(0, t) = ux(π/2, t) = 0
t ≥0,
u(x, 0) = 1 +
10

n=1
3n cos 2nx
0 ≤x ≤π/2.
6.12 The hyperbolic equation utt + ut −uxx = 0 describes wave propagation along tele-
graphlines.Solvethetelegraphequationon0 < x < 2, t > 0withtheinitialboundary
conditions
u(0, t) = u(2, t) = 0
t ≥0,
u(x, 0) = 0,
ut(x, 0) = x
0 ≤x ≤2.
6.13 Solve the problem
ut −uxx = x(1 + πt)
π
0 < x < π, t > 0,
u(0, t) = 2,
u(π, t) = t
t ≥0,
u(x, 0) = 2
	
1 −x2
π2

0 ≤x ≤π.
6.14 (a) Solve the problem
ut = uxx −4u
0 < x < π, t > 0,
ux(0, t) = u(π, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤π,
for f (x) = x2 −π2.
(b) Solve the same problem for f (x) = x −cos x.
(c) Are the solutions you found in (a) and (b) classical?
6.15 (a) Solve the problem
ut −uxx = 2t + (9t + 31) sin 3x
2
0 < x < π, t > 0,
u(0, t) = t2,
ux(π, t) = 1
t ≥0,
u(x, 0) = x + 3π
0 ≤x ≤π.
(b) Is the solution classical?

6.7 Exercises
171
6.16 (a) Solve the following periodic problem:
ut −uxx = 0
−π < x < π, t > 0,
u(−π, t) = u(π, t),
ux(−π, t) = ux(π, t)
t ≥0 ,
u(x, 0) =

1
−π ≤x ≤0,
0
0 ≤x ≤π.
(b) Is the solution classical?
6.17 Solve the problem
ut −uxx = 1 + x cos t
0 < x < 1, t > 0,
ux(0, t) = ux(1, t) = sin t
t ≥0,
u(x, 0) = 1 + cos(2πx)
0 ≤x ≤1.
6.18 (a) Solve the problem
utt + ut −uxx = 0
0 < x < 2, t > 0,
u(0, t) = u(2, t) = 0
t ≥0,
u(x, 0) = 0,
ut(x, 0) = x
0 ≤x ≤2.
(b) Is the solution classical?
6.19 Let h > 0. Solve the problem
ut −uxx + hu = 0
0 < x < π, t > 0,
u(0, t) = 0, u(π, t) = 1
t ≥0,
u(x, 0) = 0
0 ≤x ≤π.
6.20 (a) Solve the problem
utt −4uxx = (1 −x) cos t
0 < x < π, t > 0,
ux(0, t) = cos t −1,
ux(π, t) = cos t
t ≥0,
u(x, 0) = x2
2π
0 ≤x ≤π,
ut(x, 0) = cos 3x
0 ≤x ≤π.
(b) Is the solution classical?
6.21 Solve the nonhomogeneous heat problem
ut −uxx = t cos(2001x)
0 < x < π, t > 0,
ux(0, t) = ux(π, t) = 0
t ≥0,
u(x, 0) = π cos 2x
0 ≤x ≤π.

172
Sturm–Liouville problems
6.22 Solve the nonhomogeneous heat problem
ut = 13uxx
0 < x < 1, t > 0,
ux(0, t) = 0, ux(1, t) = 1
t ≥0,
u(x, 0) = 1
2x2 + x
0 ≤x ≤1.
6.23 Consider the heat problem
ut −uxx = g(x, t)
0 < x < 1, t > 0,
ux(0, t) = ux(1, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤1.
(a) Solve the problem for f (x) = 3 cos(42πx), g(x, t) = e3t cos(17πx).
(b) Find limt→∞u(x, t) for
g(x, t) = 0,
f (x) =
1
1 + x2 .
6.24 Solve the nonhomogeneous wave problem
utt −uxx = cos 2t cos 3x
0 < x < π, t > 0,
ux(0, t) = ux(π, t) = 0
t ≥0,
u(x, 0) = cos2 x,
ut(x, 0) = 1
0 ≤x ≤π.
6.25 Solve the heat problem
ut = kuxx + α cos ωt
0 < x < L, t > 0,
ux(L, t) = ux(L, t) = 0
t ≥0,
u(x, 0) = x
0 ≤x ≤L.
6.26 Solve the wave problem
utt = c2uxx
0 < x < 1, t > 0,
u(0, t) = 1, u(1, t) = 2π
t ≥0,
u(x, 0) = x + π,
ut(x, 0) = 0
0 ≤x ≤1.
6.27 Solve the radial problem
∂u
∂t = 1
r2
∂
∂r
	
r2 ∂u
∂r

0 < r < a, t > 0,
u(a, t) = a,
|u(0, t)| < ∞
t ≥0,
u(r, 0) = r
0 ≤r ≤a.
Hint Use the substitution ρ(r) = r R(r) to solve the related Sturm–Liouville problem.
6.28 Show that for the initial boundary value problem (6.92) it is possible to ﬁnd an auxiliary
function w which satisﬁes the boundary conditions and has the form of (6.93).

7
Elliptic equations
7.1 Introduction
We mentioned in Chapter 1 the central role played by the Laplace operator in the
theory of PDEs. In this chapter we shall concentrate on elliptic equations, and,
in particular, on the main prototype for elliptic equations, which is the Laplace
equation itself:
u = 0.
(7.1)
We start by reviewing a few basic properties of elliptic problems. We then introduce
the maximum principle, and also formulate a similar principle for the heat equation.
We prove the uniqueness and stability of solutions to the Laplace equation in two
ways. One approach is based on the maximum principle, and the other approach
uses the method of Green’s identities. The simplest solution method for the Laplace
equation is the method of separation of variables. Indeed, this method is only appli-
cable in simple domains, such as rectangles, disks, rings, etc., but these domains are
often encountered in applications. Moreover, explicit solutions in simple domains
provide an insight into the solution’s structure in more general domains. Towards
the end of the chapter we shall introduce Poisson’s kernel formula.
7.2 Basic properties of elliptic problems
We limit the discussion in this chapter to functions u(x, y) in two independent vari-
ables, although most of the analysis can be readily generalized to higher dimensions
(see Chapter 9). We further limit the discussion to the case where the equation con-
tains only the principal part, and this part is in a canonical form. Nevertheless, we
allow for a nonhomogeneous term in the equation. We denote by D a planar domain
(i.e. a nonempty connected and open set in R2). The Laplace equation is given by
u := uxx + uyy = 0
(x, y) ∈D.
(7.2)
A function u satisfying (7.2) is called a harmonic function.
173

174
Elliptic equations
The Laplace equation is a special case of a more general equation:
u = F(x, y),
(7.3)
where F is a given function. Equation (7.3) was used by the French mathematician
Simeon Poisson (1781–1840) in his studies of diverse problems in mechanics,
gravitation, electricity, and magnetism. Therefore it is called Poisson’s equation.
In order to obtain a heuristic understanding of the results to be derived below, it is
useful to provide Poisson’s equation with a simple physical interpretation. For this
purpose we recall from the discussion in Chapter 1 that the solution of Poisson’s
equation represents the distribution of temperature u in a domain D at equilibrium.
The nonhomogeneous term F describes (up to a change of sign) the rate of heat
production in D. For the beneﬁt of readers who are familiar with the theory of
electromagnetism, we point out that u could also be interpreted as the electric
potential in the presence of a charge density −F.
In order to obtain a unique temperature distribution, we must provide conditions
for the temperature (or temperature ﬂux) at the boundary ∂D. There are several
basic boundary conditions (see the discussion in Chapter 1).
Deﬁnition 7.1 The problem deﬁned by Poisson’s equation and the Dirichlet bound-
ary condition
u(x, y) = g(x, y)
(x, y) ∈∂D,
(7.4)
for a given function g, is called the Dirichlet problem. In Figure 7.1 we depict the
problem schematically.
Deﬁnition 7.2 The problem deﬁned by Poisson’s equation and the Neumann
boundary condition
∂nu(x, y) = g(x, y)
(x, y) ∈∂D,
(7.5)
D
u=g
∆u=F
∂D
Figure 7.1 A schematic drawing for the Poisson equation with Dirichlet boundary
conditions.

7.2 Basic properties of elliptic problems
175
where g is a given function, ˆn denotes the unit outward normal to ∂D, and ∂n
denotes a differentiation in the direction of ˆn (i.e. ∂n = ˆn · ∇), is called the Neumann
problem.
Deﬁnition 7.3 The problem deﬁned by Poisson’s equation and the boundary con-
dition of the third kind
u(x, y) + α(x, y)∂nu(x, y) = g(x, y)
(x, y) ∈∂D,
(7.6)
where α and g are given functions, is called a problem of the third kind (it is also
sometimes called the Robin problem).
The ﬁrst question we have to address is whether there exists a solution to each
one of the problems we just deﬁned. This question is not at all easy. It has been con-
sidered by many great mathematicians since the middle of the nineteenth century.
It was discovered that when the domain D is bounded and ‘sufﬁciently smooth’,
then the Dirichlet problem, for example, does indeed have a solution. The precise
deﬁnition of smoothness in this context and the general existence proof are beyond
the scope of this book, and we refer the interested reader to [11]. It is interesting
to point out that in applications one frequently encounters domains with corners
(rectangles, for example). Near a corner the boundary is not differentiable; thus, we
cannot always expect the solutions to be as smooth as we would like. In this chap-
ter, we only consider classical solutions, i.e. the solutions are in the class C2(D).
Some of the analysis we present requires further conditions on the behavior of the
solutions near the boundary. For example, we sometimes have to limit ourselves to
solutions in the class C1( ¯D).
Consider now the Neumann problem. Since the temperature is in equilibrium,
the heat ﬂux through the boundary must be balanced by the temperature produc-
tion inside the domain. This simple argument is the physical manifestation of the
following statement.
Lemma 7.4 A necessary condition for the existence of a solution to the Neumann
problem is

∂D
g(x(s), y(s))ds =

D
F(x, y)dxdy,
(7.7)
where (x(s), y(s)) is a parameterization of ∂D.
Proof Let us ﬁrst recall the vector identity u = ⃗∇· ⃗∇u. Therefore we can write
Poisson’s equation as
⃗∇· ⃗∇u = F.
(7.8)

176
Elliptic equations
Integrating both sides of the equation over D, and using Gauss’ theorem, we obtain

∂D
⃗∇u · ˆnds =

D
Fdxdy.
The lemma now follows from the deﬁnition of the directional derivative and from
the boundary conditions.
□
For future reference it is useful to observe that for harmonic functions, i.e.
solutions of the Laplace equation (F = 0), we have


∂nuds = 0
(7.9)
for any closed curve  that is fully contained in D.
Notice that we supplied just a single boundary condition for each one of the
three problems we presented (Dirichlet, Neumann, third kind). Although we are
dealing with second-order equations, the boundary conditions are quite different
from the conditions we supplied in the hyperbolic case. There we provided two
conditions (one on the solution and one on its derivative with respect to t) for each
point on the line t = 0. The following example (due to Hadamard) demonstrates the
difference between elliptic and hyperbolic equations on the upper half-plane. Con-
sider Laplace’s equation in the domain −∞< x < ∞, y > 0, under the Cauchy
conditions
un(x, 0) = 0,
un
y(x, 0) = sin nx
n
−∞< x < ∞,
(7.10)
where n is a positive integer. It is easy to check that
un(x, y) = 1
n2 sin nx sinh ny
is a harmonic function satisfying (7.10). Choosing n to be a very large number, the
initial conditions describe an arbitrarily small perturbation of the trivial solution
u = 0. On the other hand, the solution is not bounded at all in the half-plane y > 0.
In fact, for any y > 0, the value of supx∈R |un(x, y)| grows exponentially fast as
n →∞. Thus the Cauchy problem for the Laplace equation is not stable and hence
is not well posed with respect to the initial conditions (7.10).
Before developing a general theory, let us compute some special harmonic func-
tions deﬁned over the entire plane (except, maybe, for certain isolated points). We
deﬁne a harmonic polynomial of degree n to be a harmonic function Pn(x, y) of
the form
Pn(x, y) =

0≤i+ j≤n
ai, jxi y j.

7.2 Basic properties of elliptic problems
177
10
5
0
−5
−10
−15
−20
y
x
2
2
1.5
1.5
0.5
0.5
0
0
1
1
u
Figure 7.2 The surface of the harmonic polynomial u(x, y) = x3 −3xy2 −y.
Harmonic functions often have a saddle-like shape. This is a consequence of the
maximum principle that we prove in Theorem 7.5.
For example, the functions x −y, x2 −y2 + 2x, x3 −3xy2 −y are harmonic poly-
nomials of degree 1, 2 and 3 respectively. The graph of the harmonic polynomial
u(x, y) = x3 −3xy2 −y is depicted in Figure 7.2. The subclass Vn of harmonic
polynomials PH
n of the form
PH
n =

i+ j=n
ai, jxi y j
is called the set of homogeneous harmonic polynomials of order n. In Exercise 7.9
we show that (somewhat surprisingly) for each n > 0 the dimension of the space
Vn is exactly 2 (this result holds only in R2).
The most important solution of the Laplace equation over the plane is the solution
that is symmetric about the origin (the radial solution). To ﬁnd this solution it is
convenient to use polar coordinates. We denote the polar variables by (r, θ), and
the harmonic function by w(r, θ) = u(x(r, θ), y(r, θ)). In Exercise 7.7 (a) we show
that the Laplace equation takes the following form in polar coordinates:
w = wrr + 1
r wr + 1
r2 wθθ = 0.
(7.11)
Therefore the radial symmetric solution w(r) satisﬁes
w′′ + 1
r w′ = 0,
(7.12)

178
Elliptic equations
which is Euler (equidimensional) second-order ODE (see Section A.3). One solu-
tion is the constant function (a harmonic polynomial of degree 0, in fact), and the
other solution is given by
w(r) = −1
2π lnr.
(7.13)
The solution w(r) in (7.13) is called the fundamental solution of the Laplace equa-
tion. We shall use this solution extensively in Chapter 8, where the title ‘fun-
damental’ will be justiﬁed. We shall also see there the reason for including the
multiplicative constant −1/2π. Notice that the fundamental solution is not deﬁned
at the origin. The fundamental solution describes the electric potential due to a
point-like electric charge at the origin.
It is interesting to note that the Laplace equation is symmetric with respect to
coordinate shift: i.e. if u(x, y) is a harmonic function, then so is u(x −a, y −b) for
any constants a and b. There are other symmetries as well; for example, the equation
is symmetric with respect to rotations of the coordinate system, i.e. if w(r, θ) is
harmonic, then w(r, θ + γ ) is harmonic too for every constant γ . Another important
symmetry concerns dilation of the coordinate system: if u(x, y) is harmonic, then
u(x/δ, y/δ) is also harmonic for every positive constant δ.
7.3 The maximum principle
Oneofthecentraltoolsinthetheoryof(second-order)ellipticPDEsisthemaximum
principle. We ﬁrst present a ‘weak’ form of this principle.
Theorem 7.5 (The weak maximum principle) Let D be a bounded domain, and
let u(x, y) ∈C2(D) ∩C( ¯D) be a harmonic function in D. Then the maximum of u
in ¯D is achieved on the boundary ∂D.
Proof Consider a function v(x, y) ∈C2(D) ∩C( ¯D) satisfying v > 0 in D. We
argue that v cannot have a local maximum point in D. To see why, recall from calcu-
lus that if (x0, y0) ∈D is a local maximum point of v, then v ≤0, in contradiction
to our assumption.
Since u is harmonic, the function v(x, y)=u(x, y)+ε(x2 + y2) satisﬁes v > 0
for any ε > 0. Set M = max∂D u, and L = max∂D(x2 + y2). From our argument
about v it follows that v ≤M + εL in D. Since u = v −ε(x2 + y2), it now follows
that u ≤M + εL in D. Because ε can be made arbitrarily small, we obtain u ≤M
in D.
□
Remark 7.6 If u is harmonic in D, then −u is harmonic there too. But for any set
A and for any function u we have
min
A u = −max
A (−u).

7.3 The maximum principle
179
Therefore the minimum of a harmonic function u is also obtained on the boundary
∂D.
The theorem we have just proved still does not exclude the possibility that the
maximum (or minimum) of u is also attained at an internal point. We shall now
prove a stronger result that asserts that if u is not constant, then the maximum (and
minimum) cannot, in fact, be obtained at any interior point. For this purpose we
need ﬁrst to establish one of the marvelous properties of harmonic functions.
Theorem 7.7 (The mean value principle) Let D be a planar domain, let u be a
harmonic function there and let (x0, y0) be a point in D. Assume that BR is a disk
of radius R centered at (x0, y0), fully contained in D. For any r > 0 set Cr = ∂Br.
Then the value of u at (x0, y0) is the average of the values of u on the circle CR:
u(x0, y0) =
1
2π R

CR
u(x(s), y(s))ds
= 1
2π
 2π
0
u(x0 + R cos θ, y0 + R sin θ)dθ.
(7.14)
Proof Let 0 < r ≤R. We write v(r, θ) = u(x0 + r cos θ, y0 + r sin θ). We also
deﬁne the integral of v with respect to θ:
V (r) =
1
2πr

Cr
vds = 1
2π
 2π
0
v(r, θ)dθ.
Differentiating with respect to r we obtain
Vr(r) = 1
2π
 2π
0
vr(r, θ)dθ = 1
2π
 2π
0
∂
∂r u(x0 + r cos θ, y0 + r sin θ)dθ
=
1
2πr

Cr
∂nuds = 0,
where in the last equality we used (7.9). Hence V (r) does not depend on r, and thus
u(x0, y0) = V (0) = lim
ρ→0 V (ρ) = V (r) =
1
2πr

Cr
u(x(s), y(s))ds
for all 0 < r ≤R.
□
Remark 7.8 It is interesting to note that the reverse statement is also true, i.e. a
continuous function that satisﬁes the mean value property in some domain D is
harmonic in D.
We prove next a slightly weaker result.
Theorem 7.9 Let u be a function in C2(D) satisfying the mean value property at
every point in D. Then u is harmonic in D.

180
Elliptic equations
Proof Assume by contradiction that there is a point (x0, y0) in D where
u(x0, y0) ̸= 0. Without loss of generality assume u(x0, y0) > 0. Since u(x, y)
is a continuous function, then for a sufﬁciently small R > 0 there exists in D a disk
BR of radius R, centered at (x0, y0), such that u > 0 at each point in BR. Denote
the boundary of this disk by CR. It follows that
0 < 1
2π

BR
udxdy = 1
2π

CR
∂nuds
= R
2π
 2π
0
∂
∂R u(x0 + R cos θ, y0 + R sin θ)dθ
= R
2π
∂
∂R
 2π
0
u(x0 + R cos θ, y0 + R sin θ)dθ
= R ∂
∂R [u(x0, y0)] = 0,
(7.15)
where in the fourth equality in (7.15) we used the assumption that u satisﬁes the
mean value property.
□
As a corollary of the mean value theorem, we shall prove another maximum
principle for harmonic functions.
Theorem 7.10 (The strong maximum principle) Let u be a harmonic function
in a domain D (here we also allow for unbounded D). If u attains it maximum
(minimum) at an interior point of D, then u is constant.
Proof Assume by contradiction that u obtains its maximum at some interior point
q0. Let q ̸= q0 be an arbitrary point in D. Denote byl a smooth orbit in D connecting
q0 and q (see Figure 7.3). In addition, denote by dl the distance between l and ∂D.
q
D
q0
q1
Figure 7.3 A construction for the proof of the strong maximum principle.

7.4 Applications of the maximum principle
181
Consider a disk B0 of radius dl/2 around q0. From the deﬁnition of dl and from the
mean value theorem, we infer that u is constant in B0 (since the average of a set
cannot be greater than all the objects of the set). Select now a point q1 in l ∩B0, and
denote by B1 the disk of radius dl/2 centered at q1. From our construction it follows
that u also reaches its maximal value at q1. Thus we obtain that u is constant also
in B1.
We continue in this way until we reach a disk that includes the point q. We
conclude u(q) = u(q0), and since q is arbitrary, it follows that u is constant in D.
Notice that we may choose the points q0, q1, . . . , such that the process involves a
ﬁnite number of disks B0, B1, . . . , Bnl because the length of l is ﬁnite, and because
all the disks have the same radius.
□
Remark 7.11 The strong maximum theorem indeed guarantees that nonconstant
harmonic functions cannot obtain their maximum or minimum in D. Notice that
in unbounded domains the maximum (minimum) of u is not necessarily obtained
in ¯D. For example, the function log(x2 + y2) is harmonic and positive outside the
unit disk, and it vanishes on the domain’s boundary. We also point out that the ﬁrst
proof of the maximum principle can be readily generalized to a large class of elliptic
problems, while the mean value principle holds only for harmonic functions.
7.4 Applications of the maximum principle
We shall illustrate the importance of the maximum principle by using it to prove
the uniqueness and stability of the solution to the Dirichlet problem.
Theorem 7.12 Consider the Dirichlet problem in a bounded domain:
u = f (x, y)
(x, y) ∈D,
u(x, y) = g(x, y)
(x, y) ∈∂D.
The problem has at most one solution in C2(D) ∩C( ¯D).
Proof Assume by contradiction that there exist two solutions u1 and u2. Denote
their difference by v = u1 −u2. The problem’s linearity implies that v is harmonic
in D, and that it vanishes on ∂D. The weak maximum principle implies, then,
0 ≤v ≤0. Thus v ≡0.
□
We note that the boundedness of D is essential. Consider, for instance, the
following Dirichlet problem:
u = 0
x2 + y2 > 4,
(7.16)
u(x, y) = 1
x2 + y2 = 4.
(7.17)

182
Elliptic equations
It is easy to verify that the functions u1 ≡1 and u2(x, y) = (ln

x2 + y2)/ln 2 both
solve the problem.
Theorem 7.13 Let D be a bounded domain, and let u1 and u2 be functions in
C2(D) ∩C( ¯D) that are solutions of the Poisson equation u = f with the Dirichlet
conditions g1 and g2, respectively. Set Mg = max∂D |g1(x, y) −g2(x, y)|. Then
max
D |u1(x, y) −u2(x, y)| ≤Mg.
Proof Deﬁne v = u1 −u2. The construction implies that v is harmonic in D sat-
isfying, v = g1 −g2 on ∂D. Therefore the maximum (and minimum) principle
implies
min
∂D (g1 −g2) ≤v(x, y) ≤max
∂D (g1 −g2)
∀(x, y) ∈D,
and the theorem follows.
□
7.5 Green’s identities
We now develop another important tool for the analysis of elliptic problems –
Green’s identities. We shall use this tool to provide an alternative uniqueness proof
for the Dirichlet problem, and, in addition, we shall prove the uniqueness of so-
lutions to the Neumann problem and to problems of the third kind. The Green’s
identities method is similar to the energy method we used in Chapter 5, and to
Green’s formula, which we introduced in Chapter 6.
Our starting point is Gauss’ (the divergence) theorem:

D
⃗∇· ⃗ψ(x, y) dxdy =

∂D
⃗ψ(x(s), y(s)) · ˆnds.
This theorem holds for any vector ﬁeld ⃗ψ ∈C1(D) ∩C( ¯D) and any bounded piece-
wise smooth domain D. Let u and v be two arbitrary functions in C2(D) ∩C1( ¯D).
We consider several options for ψ in Gauss’ theorem. Selecting
⃗ψ = ⃗∇u,
we obtain (as we veriﬁed earlier)

D
u dxdy =

∂D
∂nu ds.
(7.18)
The selection ⃗ψ = v ⃗∇u −u ⃗∇v leads to

D
(vu −uv) dxdy =

∂D
(v∂nu −u∂nv) ds.
(7.19)

7.5 Green’s identities
183
A third Green’s identity

D
⃗∇u · ⃗∇v dxdy =

∂D
v∂nuds −

D
vu dxdy,
(7.20)
is given as an exercise (see Exercise 7.1).
We applied the ﬁrst Green’s identity (7.18) to prove the mean value principle.
We next apply the third Green’s identity (7.20) to establish the general uniqueness
theorem for Poisson’s equation.
Theorem 7.14 Let D be a smooth domain.
(a) The Dirichlet problem has at most one solution.
(b) If α ≥0, then the problem of the third kind has at most one solution.
(c) If u solves the Neumann problem, then any other solution is of the form v = u + c,
where c ∈R.
Proof We start with part (b) (part (a) is a special case of part (b)). Suppose u1
and u2 are two solutions of the problem of the third kind. Set v = u1 −u2. It is
easy to see that v is a harmonic function in D, satisfying on ∂D the boundary
condition
v + α∂nv = 0.
Substituting v = u in the third Green’s identity (7.20), we obtain

D
| ⃗∇v|2dxdy = −

∂D
α (∂nv)2 ds.
(7.21)
Since the left hand side of (7.21) is nonnegative, and the right hand side is nonposi-
tive, it follows that both sides must vanish. Hence ∇v = 0 in D and α∂nv = −v = 0
on ∂D. Therefore v is constant in D and it vanishes on ∂D. Thus v ≡0, and
u1 ≡u2.
The proof of part (c) is similar. We ﬁrst notice that one cannot expect uniqueness
in the sense of parts (a) and (b), since if u is a solution to the Neumann problem,
then u + c is a solution too for any constant c. Indeed, we now obtain from the
identity (7.20)

D
| ⃗∇v|2 dxdy = 0,
implying that v is constant. On the other hand, since we have no constraint on the
value of v on ∂D, we cannot determine the constant. We thus obtain
u1 −u2 = constant.
□

184
Elliptic equations
7.6 The maximum principle for the heat equation
The maximum principle also holds for parabolic equations. Consider the heat equa-
tion for a function u(x, y, z, t) in a three-dimensional bounded domain D:
ut = ku
(x, y, z) ∈D
t > 0,
(7.22)
where here we write u = uxx + uyy + uzz. To formulate the maximum principle
we deﬁne the domain
QT = {(x, y, z, t) | (x, y, z) ∈D, 0 < t ≤T }.
Notice that the time interval (0, T ) is arbitrary. It is convenient at this stage to deﬁne
the parabolic boundary of QT :
∂P QT = {D × {0}} ∪{∂D × [0, T ]},
that is the boundary of QT , save for the top cover D × {T }. We also denote by CH
the class of functions that are twice differentiable in QT with respect to (x, y, z),
once differentiable with respect to t, and continuous in ¯QT . We can now state the
(weak) maximum principle for the heat equation.
Theorem 7.15 Let u ∈CH be a solution to the heat equation (7.22) in QT . Then
u achieves its maximum (minimum) on ∂P QT .
Proof We prove the statement with respect to the maximum of u. The proof with
respect to the minimum of u follows at once, since if u satisﬁes the heat equation,
so does −u. It is convenient to start with the following proposition.
Proposition 7.16 Let v be a function in CH satisfying vt −kv < 0 in QT . Then
v has no local maximum in QT . Moreover, v achieves its maximum in ∂P QT .
Proof of the proposition If v has a local maximum at some q ∈QT , then vt(q) = 0,
implying v(q) > 0, which contradicts the assumption. Since v is continuous in
the closed domain ¯QT , its maximum is achieved somewhere on the boundary
∂(QT ). If the maximum is achieved at a point q = (x0, y0, z0, T ) on the top cover
D × {T }, then we must have vt(q) ≥0, and thus v(q) > 0. Again this contradicts
the assumption on v, since (x0, y0, z0) is a local maximum in D.
Returning to the maximum principle, we deﬁne (for ε > 0)
v(x, y, z, t) = u(x, y, z, t) −εt.
Obviously
max
∂P QT v ≤M := max
∂P QT u.
(7.23)

7.6 Heat equation: maximum principle
185
Since u satisﬁes the heat equation, it follows that vt −v < 0 in QT . Propo-
sition 7.14 and (7.23) imply that v ≤M, hence for all points in QT we have
u ≤M + εT . Because ε can be made arbitrarily small, we obtain u ≤M.
□
As a direct consequence of the maximum principle we prove the following the-
orem, which guarantees the uniqueness and stability of the solution to the Dirichlet
problem for the heat equation.
Theorem 7.17 Let u1 and u2 be two solutions of the heat equation
ut −ku = F(x, t)
(x, y, z) ∈D
0 < t < T,
(7.24)
with initial conditions ui(x, y, z, 0) = fi(x, y, z), and boundary conditions
ui(x, y, z, t) = hi(x, y, z, t)
(x, y, z) ∈∂D
0 < t < T,
respectively. Set
δ = max
D | f1 −f2| +
max
∂D×{t>0} |h1 −h2|.
Then
|u1 −u2| ≤δ
(x, y, z, t) ∈¯QT .
Proof Writing w = u1 −u2, the proof is the same as for the corresponding theorem
for Poisson’s equation. The special case f1 = f2, h1 = h2 implies at once the
uniqueness part of the theorem.
□
Corollary 7.18 Let
u(x, t) =
∞

n=1
Bn sin nπx
L e−k( nπ
L )2t
(7.25)
be the formal solution of the heat problem
ut −kuxx = 0
0 < x < L, t > 0,
(7.26)
u(0, t) = u(L, t) = 0
t ≥0,
(7.27)
u(x, 0) = f (x)
0 ≤x ≤L.
(7.28)
If the series
f (x) =
∞

n=1
Bn sin nπx
L
converges uniformly on [0, L], then the series (7.25) converges uniformly on
[0, L] × [0, T ], and u is a classical solution.

186
Elliptic equations
Proof Let ε > 0. By the Cauchy criterion for uniform convergence there exists Nε
such that for all Nε ≤k ≤l we have

l
n=k
Bn sin nπx
L
 < ε
∀x ∈[0, L].
Note that
l
n=k
Bn sin nπx
L e−k( nπ
L )2t
is a classical solution of the heat equation; hence by the maximum principle

l
n=k
Bn sin nπx
L e−k( nπ
L )2t
 < ε
∀(x, t) ∈[0, L] × [0, T ].
Invoking again the Cauchy criterion for uniform convergence, we infer that the
series of (7.25) converges uniformly on [0, L] × [0, T ] to the continuous function
u. In particular, u satisﬁes the initial and boundary conditions. Since u satisﬁes the
heat equation on [0, L] × (0, T ], it follows that u is a classical solution.
□
We saw in Chapters 5 and 6 that the heat equation ‘smoothes out’ the initial
conditions in the sense even if the initial data are not smooth, the solution is in class
C∞for all t > 0. We examine now another aspect of the smoothing property of the
heat equation. For simplicity let us return to the case of a single spatial variable x,
so that our domain consists of an interval: D = (a, b).
Proposition 7.19 Let u(x, t) be the solution of
ut = kuxx
a < x < b
t > 0
(7.29)
with Neumann’s initial boundary value problem
u(x, 0) = f (x)
x ∈(a, b),
ux(a, t) = ux(b, t) = 0
0 < t < T.
Assume f ∈C1([a, b]) satisfying fx(a) = fx(b) = 0. Set QT = (a, b) × (0, T ] for
some T > 0. Then u satisﬁes
max
QT |ux| ≤max
(a,b) | f ′(x)|.
Proof We differentiate u with respect to x and deﬁne w(x, t) = ux(x, t). One can
readily verify that w satisﬁes the heat equation with Dirichlet initial boundary
conditions:
w(x, 0) = f ′(x)
x ∈(a, b),
w(a, t) = w(b, t) = 0
0 < t < T.

7.7 Separation of variables for elliptic problems
187
Therefore Proposition 7.19 follows from the maximum principle for the heat
equation.
□
7.7 Separation of variables for elliptic problems
The method of separation of variables, introduced in Chapter 5 for the heat equation
and for the wave equation, can in some cases also be applied to elliptic equations.
Applying the method requires certain symmetries to hold, both for the equation and
for the domain under study. We shall demonstrate how to use the method to solve
the Laplace and Poisson equations in rectangles, in disks, and in circular sectors.
Additional domains, such as an exterior of a disk or a ring will be dealt with in the
exercises (see Exercises 7.11 and 7.20).
First, we prove the following general result.
Proposition 7.20 Consider the Dirichlet problem
u = 0
(x, y) ∈D,
u(x, y) = g(x, y)
(x, y) ∈∂D,
in a bounded domain D. Let
u(x, y) =
∞

n=1
un(x, y)
(7.30)
be a formal solution of the problem, such that each un(x, y) is a harmonic function
in D and continuous in ¯D. If the series (7.30) converges uniformly on ∂D to g, then
it converges uniformly on ¯D and u is a classical solution of the problem.
Proof Let ε > 0. By the Cauchy criterion for uniform convergence, there exists
Nε such that for all Nε ≤k ≤l we have

l
n=k
un(x, y)
 < ε
for all (x, y) ∈∂D. By the weak maximum principle

l
n=k
un(x, y)
 < ε
for all (x, y) ∈∂D. Invoking again the Cauchy criterion for uniform convergence,
we infer that the series of (7.30) converges uniformly on ¯D to the continuous
function u. In particular, u satisﬁes the boundary condition. Since each un satisﬁes
the mean value property, the uniform convergence implies that u also satisﬁes the
mean value property, and by Remark 7.8 u is harmonic in D.
□

188
Elliptic equations
y
x
c
a
b
d
∆u=0
∆u1=0
k
f
h
g
y
x
c
a
b
d
k
0
h
y
x
c
a
b
d
g
∆u2=0 0
0
f
0
Figure 7.4 Separation of variables in rectangles.
7.7.1 Rectangles
Letu bethesolutiontotheDirichletprobleminarectangulardomain D (Figure7.4):
u = 0
a < x < b, c < y < d,
(7.31)
with the boundary conditions
u(a, y) = f (y), u(b, y) = g(y), u(x, c) = h(x), u(x, d) = k(x).
(7.32)
We recall that the method of separation of variables is based on constructing an
appropriate eigenvalue (Sturm–Liouville) problem. This, in turn, requires homo-
geneous boundary conditions. We thus split u into u = u1 + u2, where u1 and u2
are both harmonic in D, and where u1 and u2 satisfy the boundary conditions (see
Figure 7.4)
u1(a, y) = f (y), u1(b, y) = g(y), u1(x, c) = 0, u1(x, d) = 0
(7.33)
and
u2(a, y) = 0, u2(b, y) = 0, u2(x, c) = h(x), u2(x, d) = k(x).
(7.34)
We assume at this stage that the compatibility condition
f (c) = f (d) = g(c) = g(d) = h(a) = h(b) = k(a) = k(b) = 0
(7.35)
holds. Since u1 + u2 satisfy (7.31) and the boundary conditions (7.32), the unique-
ness theorem guarantees that u = u1 + u2.
The advantage of splitting the problem into two problems is that each one of
the two new problems can be solved by separation of variables. Consider, for
example, the problem for u1. We shall seek a solution in the form of a sum of
separated (nonzero) functions U(x, y) = X(x)Y(y). Substituting such a solution
into the Laplace equation (7.31), we obtain
X′′(x) −λX(x) = 0
a < x < b,
(7.36)
Y ′′(y) + λY(y) = 0
c < y < d.
(7.37)

7.7 Separation of variables for elliptic problems
189
The homogeneous boundary conditions imply that
Y(c) = Y(d) = 0.
(7.38)
Thus, we obtain a Sturm–Liouville problem for Y(y). Solving (7.37)–(7.38), we
derive a sequence of eigenvalues λn and eigenfunctions Yn(y). We can then substi-
tute the sequence λn in (7.36) and obtain an associated sequence Xn(x). The general
solution u1 is written formally as
u1(x, y) =

n
Xn(x)Yn(y).
The remaining boundary conditions for u1 will be used to eliminate the two free
parameters associated with Xn for each n, just as was done in Chapter 5. Instead
of writing a general formula, we ﬁnd it simpler to demonstrate the method via an
example.
Example 7.21 Solve the Laplace equation in the rectangle 0 < x < b, 0 < y < d,
subject to the Dirichlet boundary conditions
u(0, y) = f (y), u(b, y) = g(y), u(x, 0) = 0, u(x, d) = 0.
(7.39)
Recalling the notation u1 and u2 we introduced above, this problem gives rise to a
Laplace equation with zero Dirichlet conditions for u2. Therefore the uniqueness
theorem implies u2 ≡0. For u1 we construct a solution consisting of an inﬁnite
combination of functions of the form w(x, y) = X(x)Y(y). We thus obtain for Y(y)
the following Sturm–Liouville problem:
Y ′′(y) + λY(y) = 0
0 < y < d, Y(0) = Y(d) = 0.
(7.40)
This problem was solved in Chapter 5. The eigenvalues and eigenfunctions are
λn =
nπ
d
2
, Yn(y) = sin nπ
d y n = 1, 2, . . . .
(7.41)
The equation for the x-dependent factor is
X′′(x) −
nπ
d
2
X(x) = 0 0 < x < b.
(7.42)
To facilitate the expansion of the boundary condition into a Fourier series, we select
for (7.42) the fundamental system of solutions {sinh[(nπ/d)x], sinh[(nπ/d)(x −
b)]}. Thus we write for u1
u1(x, y) =
∞

n=1
sin nπy
d

An sinh nπ
d x + Bn sinh nπ
d (x −b)

.
(7.43)

190
Elliptic equations
Substituting the expansion (7.43) into the nonhomogeneous boundary conditions
of (7.39) we obtain:
g(y) =
∞

n=1
An sinh nπb
d
sin nπy
d
,
f (y) =
∞

n=1
Bn sinh −nπb
d
sin nπy
d
.
To evaluate the sequences {An}, {Bn}, expand f (y) and g(y) into generalized
Fourier series:
f (y) =
∞

n=1
αn sin nπy
d , αn = 2
d
 d
0
f (y) sin nπy
d dy,
g(y) =
∞

n=1
βn sin nπy
d , βn = 2
d
 d
0
g(y) sin nπy
d dy.
This implies
An =
βn
sinh nπb
d
,
Bn = −
αn
sinh nπb
d
.
We saw in Chapter 5 that the generalized Fourier series representing the solution
to the heat equations converges exponentially fast for all t > 0. Moreover, the
series for the derivatives of all orders converges too. On the other hand, the rate of
convergence for the series representing the solution to the wave equation depends on
the smoothness of the initial data, and singularities in the initial data are preserved
by the solution. What is the convergence rate for the formal series representing the
solution of the Laplace equation? Do we obtain a classical solution?
To answer these questions, let us consider the general term in the series (7.43). We
assume that the functions f (y) and g(y) are piecewise differentiable, and that they
satisfy the homogeneous Dirichlet conditions at the end points y = 0, d. Then the
coefﬁcients αn and βn satisfy |αn| < C1, |βn| < C2, where C1 and C2 are constants
that do not depend on n (actually, one can establish a far stronger result on the decay
of αn and βn to zero as n →∞, but we do not need this result here). Consider a
speciﬁc term
αn
sinh nπb
d
sinh nπ
d (x −b) sin nπy
d
in the series that represents u1, where (x, y) is some interior point in D. This term
is of the order of O(e−nπ
d x) for large values of n. The same argument implies
βn
sinh nπb
d
sinh nπ
d x sin nπy
d
= O(e
nπ
d (x−b)).

7.7 Separation of variables for elliptic problems
191
Thus all the terms in the series (7.43) decay exponentially fast as n →∞. Similarly
the series of derivatives of all orders also converges exponentially fast, since the
kth derivative introduces an algebraic factor nk into the nth term in the series, but
this factor is negligible (for large n) in comparison with the exponentially decaying
term. We point out, though, that the rate of convergence slows down as we approach
the domain’s boundary.
Example 7.22 Solve the Laplace equation in the square 0 < x, y < π subject to
the Dirichlet condition
u(x, 0) = 1984, u(x, π) = u(0, y) = u(π, y) = 0.
The problem involves homogeneous boundary conditions on the two boundaries
parallel to the y axis, and a single nonhomogeneous condition on the boundary
y = 0. Therefore we write the formal solution in the form
u(x, y) =
∞

n=1
An sin nx sinh n(y −π).
Substituting the boundary condition u(x, 0) = 1984 we obtain
−
∞

1
An sin nx sinh nπ = 1984.
The generalized Fourier series of the constant function f (x) = 1984 is 1984 =
∞
n=1 αn sin nx, where the coefﬁcients {αn} are given by
αn = 2 × 1984
π
 π
0
sin nxdx = 3968
nπ (cos 0 −cos nπ).
We thus obtain
An =



−
7936
(2k −1)π sinh(2k −1)π
n = 2k −1,
0
n = 2k.
(7.44)
The solution to the problem is given formally by
u(x, y) = 7936
∞

n=1
sin(2n −1)x sinh(2n −1)(π −y)
(2n −1)π sinh(2n −1)π
.
(7.45)
The observant reader might have noticed that in the last example we violated the
compatibility condition (7.35). This is the condition that guarantees that each of
the problems we solve by separation of variables has a continuous solution. Nev-
ertheless, we obtained a formal solution, and following our discussion above, we
know that the solution converges at every interior point in the square. Furthermore,
the convergence of the series, and all its derivatives at every interior point is

192
Elliptic equations
exponentially fast. Why should we be bothered, then, by the violation of the com-
patibility condition?
The answer lies in the Gibbs phenomenon that we mentioned in Chapter 6.
If we compute the solution near the problematic points ((0, 0), (1, 0) in the last
example) by summing up ﬁnitely many terms in the series (7.45), we observe
high frequency oscillations. The difﬁculty we describe here is relevant not only
to analytical solutions, but also to numerical solutions. We emphasize that even
if the boundary condition to the original problem leads to a continuous solution,
the process of breaking the problem into several subproblems for the purpose of
separating variables might introduce discontinuities into the subproblems!
We therefore present a method for transforming a Dirichlet problem with con-
tinuous boundary data that does not satisfy the compatibility condition (7.35)
into another Dirichlet problem (with continuous boundary data) that does satisfy
(7.35). Denote the harmonic function we seek by u(x, y), and the Dirichlet bound-
ary condition on the rectangle’s boundary by g. We write u as a combination:
u(x, y) = v(x, y) −P2(x, y), where P2 is a second-order appropriate harmonic
polynomial (that we still have to ﬁnd), while v is a harmonic function. We con-
struct the harmonic polynomial in such a way that v satisﬁes the compatibility
condition (7.35) (i.e. v vanishes at the square’s vertices). Denote the restriction of
v and P2 to the square’s boundary by g1 and g2, respectively. We select P2 so that
the incompatibility of g at the square’s vertices is included in g2. Thus we obtain
for v a compatible Dirichlet problem. To construct P2 as just described, we write
the general form of a second-order harmonic polynomial:
P2(x, y) = a1(x2 −y2) + a2xy + a3x + a4y + a5.
(7.46)
For simplicity, and without loss of generality, consider the square 0 < x < 1, 0 <
y < 1. Requiring g1 to vanish at all the vertices leads to four equations for the ﬁve
unknown coefﬁcients of P2:
g(0, 0) + a5 = 0,
g(1, 0) + a1 + a3 + a5 = 0,
g(0, 1) −a1 + a4 + a5 = 0,
g(1, 1) + a2 + a3 + a4 + a5 = 0.
(7.47)
We choose arbitrarily a1 = 0 and obtain the solution:
a1 = 0,
a2 = −g(1, 1) −g(0, 0) + g(1, 0) + g(0, 1),
a3 = g(0, 0) −g(1, 0),
a4 = g(0, 0) −g(0, 1),
a5 = −g(0, 0).
(7.48)

7.7 Separation of variables for elliptic problems
193
Having thrown all the incompatibilities into the (easy to compute) harmonic polyno-
mial, it remains to ﬁnd a harmonic function v that satisﬁes the compatible boundary
conditions g1 = g + g2.
Example 7.23 Let u(x, y) be the harmonic function in the unit square satisfying
the Dirichlet conditions
u(x, 0) = 1 + sin πx, u(x, 1) = 2, u(0, y) = u(1, y) = 1 + y.
Represent u as a sum of a harmonic polynomial, and a harmonic function v(x, y)
that satisﬁes the compatibility condition (7.35).
We compute the appropriate harmonic polynomial. Solving the algebraic system
(7.48) we get
a1 = a2 = a3 = 0, a4 = −1, a5 = −1.
Hence the harmonic polynomial is P2(x, y) = −1 −y. Deﬁne now
v(x, y) = u(x, y) −(1 + y).
Our construction implies that v is the harmonic function satisfying the Dirichlet
data
v(x, 0) = sin πx, v(x, 1) = v(0, y) = v(1, y) = 0.
Indeed, the compatibility condition holds for v. Finally, we obtain that v(x, y) =
sin πxsinh(π −y)/sinh π, and therefore
u(x, y) = sin πx sinh(π −y)
sinh π
+ 1 + y.
We end this section by solving a Neumann problem in a square.
Example 7.24 Find a harmonic function u(x, y) in the square 0 < x, y < π satis-
fying the Neumann boundary conditions
uy(x, π) = x −π/2, ux(0, y) = ux(π, y) = uy(x, 0) = 0.
(7.49)
The ﬁrst step in solving a Neumann problem is to verify that the necessary condition
for existence holds. In the current case, the integral

∂D ∂nuds is equal to
 π
0 (x −
π/2)dx, which indeed vanishes.
The nature of the boundary conditions implies separated solutions of the form
Un(x, y) = cos nx cosh ny, where n = 0, 1, 2 . . . . Thus the formal solution is
u(x, y) = A0 +
∞

n=1
An cos nx cosh ny.
(7.50)
The function u represented by (7.50) formally satisﬁes the equation and
all the homogeneous boundary conditions. Substituting this solution into the

194
Elliptic equations
nonhomogeneous boundary conditions on the edge y = π leads to
∞

n=1
nAn sinh nπ cos nx = x −π/2
0 < x < π.
We therefore expand x −π/2 into the following generalized Fourier series:
x−π/2=
∞

n=1
βn cos nx,
βn = 2
π
 π
0
(x−π/2) cos nxdx =

−4/πn2
n =1, 3, 5, . . .
0
n =0, 2, 4, . . .
.
Thus
u(x, y) = A0 −4
π
∞

n=1
cos(2n −1)x cosh(2n −1)y
(2n −1)3 sinh(2n −1)π
.
(7.51)
The graph of u is depicted in Figure 7.5. Notice that the additive constant A0 is not
determined by the problem’s conditions. This was expected due to the nonunique-
ness of the Neumann problem as was discussed in Section 7.5.
Remark 7.25 When we considered the Dirichlet problem earlier, we some-
times had to divide the problem into two subproblems, each of which involved
homogeneous Dirichlet conditions on two opposite edges of the rectangle. A sim-
ilar division is sometimes needed for the Neumann problem. Here, however, a
1.5
1
0.5
0
−0.5
−1
−1.5
3
3
y
x
u
2.5
2.5
2
2
1.5
1.5
0.5
0.5
0
0
1
1
Figure 7.5 The graph of u(x, y) from (7.51). Observe that in spite of the intricate
form of the Fourier series, the actual shape of the surface is very smooth. We also
see that u achieves its maximum and minimum on the boundary.

7.7 Separation of variables for elliptic problems
195
fundamental difﬁculty might arise: while the original problem presumably satisﬁes
the necessary existence condition (otherwise the problem is not solvable at all!),
it is not guaranteed that each of the subproblems will satisfy this condition! To
demonstrate the difﬁculty and to propose a remedy for it, we look at it in some
detail.
Consider the Neumann problem for the Laplace equation:
u = 0
x ∈,
(7.52)
∂nu = g
x ∈∂.
(7.53)
We assume, of course, that the condition

∂ gds = 0 holds. Split the boundary
of  into two parts ∂ = ∂1 ∪∂2. Deﬁne u = u1 + u2, where u1, u2 are both
harmonic in  and satisfy the boundary conditions
∂nu1 =
 g
x ∈∂1,
0
x ∈∂2.
∂nu2 =
0
x ∈∂1,
g
x ∈∂2.
The difﬁculty is that now the existence condition may not hold separately for u1 and
u2. We overcome this by the same method we used earlier to take care of the Gibbs
phenomenon.Weaddto(andsubtractfrom)thesolutionaharmonicpolynomial.We
use a harmonic polynomial P(x, y) that satisﬁes

∂1 ∂n P(x, y) ds ̸= 0. Assume,
for example, that the harmonic polynomial x2 −y2 satisﬁes this condition. We
then search for harmonic functions v1 and v2 that satisfy the following Neumann
conditions:
∂nv1 =
 g + a∂n(x2 −y2)
x ∈∂1,
0
x ∈∂2,
∂nv2 =
0
x ∈∂1,
g + a∂n(x2 −y2)
x ∈∂2.
We choose the parameter a such that the solvability condition holds for v1. Since
the original problem is assumed to be solvable and also the harmonic polynomial,
by its very existence, satisﬁes the compatibility condition, it follows that v2 must
satisfy that condition too. Finally, we observe that u = v1 + v2 −a(x2 −y2).
7.7.2 Circular domains
Another important domain where the Laplace equation can be solved by separation
of variables is a disk. Let Ba be a disk of radius a around the origin. We want to
ﬁnd the function u(x, y) that solves the Dirichlet problem
u = 0
(x, y) ∈Ba,
(7.54)
u(x, y) = g(x, y)
(x, y) ∈∂Ba.
(7.55)

196
Elliptic equations
It is convenient to solve the equation in polar coordinates in order to use the symme-
try of the domain. We thus denote the polar coordinates by (r, θ), and the unknown
function is written as w(r, θ) = u(x(r, θ), y(r, θ)). We mentioned in Section 7.2
that w satisﬁes the equation
w = wrr + 1
r wr + 1
r2 wθθ = 0.
(7.56)
Consequently, we have to solve (7.56) in the domain
Ba = {(r, θ)| 0 < r < a, 0 ≤θ ≤2π}.
The PDE is subject to the boundary condition
w(a, θ) = h(θ) = g(x(a, θ), y(a, θ)),
(7.57)
and to the additional obvious requirement that limr→0 w(r, θ) exists and is ﬁnite
(the origin needs special attention, since it is a singular point in polar coordinates).
We seek a solution of the form w(r, θ) = R(r)(θ). Substituting this function
into (7.56), and using the usual arguments from Chapter 5, we obtain a pair of
equations for R and :
r2R′′(r) + r R′(r) −λR(r) = 0
0 < r < a,
(7.58)
′′(θ) + λ(θ) = 0.
(7.59)
The equation for  holds at the interval (0, 2π). In order that the solution w(r, θ)
be of class C2, we need to impose two periodicity conditions:
(0) = (2π), ′(0) = ′(2π).
(7.60)
Notice that (7.59) and (7.60) together also imply the periodicity of the second
derivative with respect to θ.
The general solution to the Sturm–Liouville problem (7.59)–(7.60) is given (see
Chapter 6) by the sequence
n(θ) = An cos nθ + Bn sin nθ, λn = n2, 0, 1, 2, . . . .
(7.61)
Substituting the eigenvalues λn into (7.58) yields a second-order Euler (equidimen-
sional) ODE for R (see Subsection A.3):
r2R′′
n + r R′
n −n2Rn = 0.
(7.62)
The solutions of these equations are given (except for n = 0) by appropriate powers
of the independent variable r:
Rn(r) = Cnrn + Dnr−n, n = 1, 2, . . . .
(7.63)
In the special case n = 0 we obtain
R0(r) = C0 + D0 lnr.
(7.64)

7.7 Separation of variables for elliptic problems
197
Observe that the functionsr−n, n = 1, 2, . . . and the function lnr are singular at the
origin (r = 0). Since we only consider smooth solutions, we impose the condition
Dn = 0
n = 0, 1, 2, . . . .
We still have to satisfy the boundary condition (7.57). For this purpose we form the
superposition
w(r, θ) = α0
2 +
∞

n=1
rn(αn cos nθ + βn sin nθ).
(7.65)
Formally differentiating this series term-by-term, we verify that (7.65) is indeed
harmonic. Imposing the boundary condition (7.57), and using the classical Fourier
formula (Chapter 6), we obtain
α0 = 1
π
 2π
0
h(ϕ)dϕ,
αn =
1
πan
 2π
0
h(ϕ) cos nϕdϕ, βn =
1
πan
 2π
0
h(ϕ) sin nϕdϕ n ≥1.
(7.66)
Example 7.26 Solve the Laplace equation in the unit disk subject to the boundary
conditions w(r, θ) = y2 on r = 1.
Observe that on the boundary y2 = sin2 θ. All we have to do is to compute the clas-
sical Fourier expansion of the function sin2 θ. This expansion is readily performed
in light of the identity sin2 θ = 1
2(1 −cos 2θ). Thus the Fourier series is ﬁnite, and
the required harmonic function is w(r, θ) = 1
2(1 −r2 cos 2θ), or, upon returning to
Cartesian coordinates, u(x, y) = 1
2(1 −x2 + y2).
Before proceeding to other examples, it is worthwhile to examine the conver-
gence properties of the formal Fourier series we constructed in (7.65). We write
M = (1/π)
 2π
0
|h(θ)|dθ. The Fourier formulas (7.66) imply the inequalities
|αn|, |βn| ≤Ma−n.
Hence the nth term in the Fourier series (7.65) is bounded by 2M(r/a)n; thus
the series converges for all r < a, and even converges uniformly in any disk of
radius ˜a < a. In fact, we can use the same argument to show also that the series
of derivatives of any order converges uniformly in any disk of radius ˜a < a to the
appropriate derivative of the solution. Moreover, if h(θ) is a periodic piecewise dif-
ferentiable and continuous function, then by Proposition 6.30, its Fourier expansion
converges uniformly. It follows from Proposition 7.20 that w is a classical solution.
Observe, however, that the rate of convergence deteriorates when we approach the
boundary.

198
Elliptic equations
The method of separation of variables can be used for other domains with a sym-
metric polar shape. For example, in Exercise 7.20 we solve the Dirichlet problem
in the domain bounded by concentric circles. Interestingly, one can also separate
variables in the domain bounded by two nonconcentric circles, but this requires the
introduction of a special coordinate system, called bipolar, and the computations
are somewhat more involved. In Exercise 7.11 the Dirichlet problem in the exterior
of a disk is solved. We now demonstrate the solution of the Dirichlet problem in a
circular sector.
Example 7.27 Find the harmonic function w(r, θ) in the sector
Dγ = {(r, θ)| 0 < r < a, 0 < θ < γ }
that satisﬁes on the sector’s boundary the Dirichlet condition
w(a, θ) = g(θ) 0 ≤θ ≤γ,
w(r, 0) = w(r, γ ) = 0 0 ≤r ≤a.
(7.67)
The process of obtaining separated solutions and an appropriate eigenvalue problem
is similar to the previous case of the Laplace equation in the entire disk. Namely,
we again seek solutions of the form R(r)(θ), where the Sturm–Liouville equation
is again (7.59) for , and the equation for the radial component is again (7.58).
The difference is in the boundary condition for the  equation. Unlike the periodic
boundary conditions that we encountered for the problem in the full disk, we now
have Dirichlet boundary conditions (0) = (γ ) = 0. Therefore the sequences of
eigenfunctions and eigenvalues are now given by
n(θ) = An sin nπ
γ θ,
λn =
	nπ
γ

2
n = 1, 2, . . . .
Substituting the eigenvalues λn into (7.58), and keeping only the solutions that are
bounded in the origin, we obtain
wn(r, θ) = sin nπθ
γ
rnπ/γ .
Hence, the formal solution is given by the series
w(r, θ) =
∞

n=1
αn sin nπθ
γ
rnπ/γ .
(7.68)
On r = a, 0 < θ < γ we have
g(θ) =
∞

n=1
αnanπ/γ sin nπθ
γ
,

7.7 Separation of variables for elliptic problems
199
therefore,
αn = 2a−nπ/γ
γ
 γ
0
g(ϕ) sin nπϕ
γ
dϕ.
Remark 7.28 Consider the special case in which γ = 2π, and write explicitly the
solution (7.68):
w(r, θ) =
∞

n=1
αn sin nθ
2 rn/2.
(7.69)
Observe that even though the sector now consists of the entire disk, the solution
(7.69) is completely different from the solution we found earlier for the Dirichlet
problem in the disk. The reason for the difference is that the boundary condition
(7.67) on the sector’s boundary is fundamentally different from the periodic bound-
ary condition. The condition (7.67) singles out a speciﬁc curve in the disk, and,
thus, breaks the disk’s symmetry.
Remark 7.29 We observe that, in general, some derivatives (in fact, most of them)
of the solution (7.68) are singular at the origin (the sector’s vertex). We cannot
require the solution to be as smooth there as we wish. This singularity has important
physical signiﬁcance. The Laplace equation in a sector is used to model cracks in
the theory of elasticity. The singularity we observe indicates a concentration of
large stresses at the vertex.
We end this section by demonstrating the separation of variables method for the
Poisson equation in a disk with Dirichlet boundary conditions. The problem is thus
to ﬁnd a function w(r, θ), satisfying
w = F(r, θ)
0 < r < a, 0 ≤θ ≤2π,
together with the boundary condition w(a, θ) = g(θ). In light of the general tech-
nique we developed in Chapter 6 to solve nonhomogeneous equations, we seek a
solution in the form
w(r, θ) = f0(r)
2
+
∞

n=1
[ fn(r) cos nθ + gn(r) sin nθ] .
Similarly we expand F into a Fourier series
F(r, θ) = δ0(r)
2
+
∞

n=1
[δn(r) cos nθ + ϵn(r) sin nθ] .

200
Elliptic equations
Substituting these two Fourier series into the Poisson equation, and comparing the
associated coefﬁcients, we ﬁnd
f ′′
n + 1
r f ′
n −n2
r2 fn = δn(r)
n = 0, 1, . . . ,
(7.70)
g′′
n + 1
r g′
n −n2
r2 gn = ϵn(r)
n = 1, 2, . . . .
(7.71)
The general solutions of these equations can be written as
fn(r) = Anrn + ˜f n(r), gn(r) = Bnrn + ˜gn(r),
where ˜f n and ˜gn are particular solutions of the appropriate nonhomogeneous equa-
tions. In fact, it is shown in Exercise 7.18 that the solutions of (7.70)–(7.71), that
satisfy the homogeneous boundary conditions ˜f n(a) = ˜gn(a) = 0, and that are
bounded at the origin, can be written as
˜f n(r) =
 r
0
K (n)
1 (r, a, ρ)δn(ρ)ρ dρ +
 a
r
K (n)
2 (r, a, ρ)δn(ρ)ρ dρ,
(7.72)
˜gn(r) =
 r
0
K (n)
1 (r, a, ρ)ϵn(ρ)ρ dρ +
 a
r
K (n)
2 (r, a, ρ)ϵn(ρ)ρ dρ,
(7.73)
where
K (0)
1
= log r
a ,
K (0)
2
= log ρ
a ,
(7.74)
K (n)
1
= 1
2n
r
a
n
−
a
r
nρ
a
n
, K (n)
2 = 1
2n
ρ
a
n
−
	a
ρ

nr
a
n
n ≥1.
(7.75)
We thus constructed a solution of the form
w(r, θ) = A0 + ˜f 0(r)
2
+
∞

n=1

[Anrn + ˜f n(r)] cos nθ + [Bnrn + ˜gn(r)] sin nθ

.
(7.76)
To ﬁnd the coefﬁcients {An, Bn} we substitute this solution into the boundary
conditions
w(a, θ) = A0 + ˜f 0(a)
2
+
∞

n=1

[Anan + ˜f n(a)] cos nθ + [Bnan + ˜gn(a)] sin nθ

= α0
2 +
∞

n=1
(αn cos nθ + βn sin nθ) = g(θ).
The required coefﬁcients can be written as
An = αn−.2 ˜fn(a)
an
= αn
an
n = 0, 1, . . . ,
Bn = βn−˜gn(a)
an
= βn
an
n = 1, 2, . . . .

7.8 Poisson’s formula
201
Example 7.30 Solve the Poisson equation
w = 8r cos θ
0 ≤r < 1, 0 ≤θ ≤2π,
subject to the boundary conditions w(1, θ) = cos2 θ.
One can verify that
˜f n(r) =
r3
n = 1,
0
n ̸= 1,
(7.77)
and ˜gn(r) = 0 for every n, are particular solutions to the nonhomogeneous equations
(7.70)–(7.71). Therefore the general solution can be written as
w(r, θ) = A0
2 + (A1r + r3) cos θ + B1r sin θ +
∞

n=2
(Anrn cos nθ + Bnrn sin nθ).
We use the identity cos2 θ = 1
2(1 + cos 2θ) to obtain the expansion of the boundary
condition into a Fourier series. Therefore,
A0 = 1, A1 =−1,
A2 = 1
2,
An =0
∀n ̸= 0, 1, 2,
Bn =0
∀n =1, 2, 3 . . . ,
and the solution is
w(r, θ) = 1
2 + (r3 −1) cos θ + r2
2 cos 2θ.
7.8 Poisson’s formula
One of the important tools in the theory of PDEs is the integral representation
of solutions. An integral representation is a formula for the solution of a problem
in terms of an integral depending on a kernel function. We need to compute the
kernel function just once for a given equation, a given domain, and a given type of
boundary condition. We demonstrate now an integral representation for the Laplace
equation in a disk of radius a with Dirichlet boundary conditions (7.54)–(7.55). We
start by rewriting the solution as a Fourier series (see (7.65)), using (7.66):
w(r, θ) = 1
2π
 2π
0
h(ϕ)dϕ
+ 1
π
∞

n=1
r
a
n  2π
0
h(ϕ)(cos nϕ cos nθ + sin nϕ sin nθ)dϕ.
(7.78)

202
Elliptic equations
Considerr < ˜a < a.Sincetheseriesconvergesuniformlythere,wecaninterchange
the order of summation and integration, and obtain
w(r, θ) = 1
π
 2π
0
h(ϕ)
&
1
2 +
∞

n=1
r
a
n
cos n(θ −ϕ)
'
dϕ.
(7.79)
The summation of the inﬁnite series
1
2 +
∞

n=1
r
a
n
cos n(θ −ϕ)
requires a little side calculation. Deﬁne for this purpose z = ρeiα and evaluate (for
ρ < 1) the geometric sum
1
2 +
∞

1
zn = 1
2 +
z
1 −z =
1 −ρ2 + 2iρ sin α
2(1 −2ρ cos α + ρ2).
Since zn = ρn(cos nα + i sin nα), we conclude upon separating the real and imag-
inary parts that
1
2 +
∞

1
ρn cos nα =
1 −ρ2
2(1 −2ρ cos α + ρ2).
(7.80)
Returning to (7.79) using ρ = r/a, α = θ −ϕ, we obtain the Poisson formula
w(r, θ) = 1
2π
 2π
0
K(r, θ; a, ϕ)h(ϕ)dϕ,
(7.81)
where the kernel K, given by
K(r, θ; a, ϕ) =
a2 −r2
a2 −2ar cos(θ −ϕ) + r2 ,
(7.82)
is called Poisson’s kernel. This is a very useful formula. The kernel describes a
universal solution for the Laplace equation in a disk. All we have to do (at least
in theory), is to substitute the boundary condition into (7.81) and carry out the
integration. Moreover, the formula is valid for any integrable function h. It turns
out that one can derive similar representations not just in disks, but also in arbitrary
smooth domains. We shall elaborate on this issue in Chapter 8.
As another example of an integral representation for harmonic functions, we
derive the Poisson formula for the Neumann problem in a disk. Let w(r, θ) be a
harmonic function in the diskr < a, satisfying on the disk’s boundary the Neumann
condition ∂w(a, θ)/∂r = g(θ). We assume, of course, that the solvability condition
 2π
0
g(θ)dθ = 0 holds. Recall that the general form of a harmonic function in the

7.8 Poisson’s formula
203
disk is
w(r, θ) = α0
2 +
∞

n=1
rn(αn cos nθ + βn sin nθ).
Note that the coefﬁcient α0 is arbitrary, and cannot be retrieved from the boundary
conditions (cf. the uniqueness theorem for the Neumann problem). To ﬁnd the
coefﬁcients {αn, βn}, substitute the solution into the boundary conditions and obtain
αn =
1
nπan−1
 2π
0
g(ϕ) cos nϕ dϕ, βn =
1
nπan−1
 2π
0
g(ϕ) sin nϕ dϕ
n =1, 2, . . . .
(7.83)
Hence
w(r, θ) = α0
2 + a
π
 2π
0
KN(r, θ; a, ϕ)g(ϕ)dϕ,
(7.84)
where
K(r, θ; a, ϕ) =
∞

n=1
1
n
r
a
n
cos n(θ −ϕ).
(7.85)
Because of the 1/n factor we cannot use the summation formula (7.80) directly.
Instead we perform another quick side calculation. Notice that by a process like the
one leading to (7.80) one can derive
∞

n=1
ρn−1 cos nα =
cos α −ρ
1 −2ρ cos α + ρ2 .
Therefore after an integration with respect to ρ we obtain the Poisson kernel for
the Neumann problem in a disk:
KN(r, θ; a, ϕ) =
∞

n=1
1
n
r
a
n
cos n(θ −ϕ)
= −1
2 ln

1 −2r
a cos(θ −ϕ) +
r
a
2
.
(7.86)
Remark 7.31 It is interesting to note that both Poisson’s kernels that we have
computed have a special dependency on the angular variable θ. In both cases we
have
K(r, θ; a, ϕ) = ˜K(r, a, θ −ϕ),
namely, the dependency is only through the difference between θ and ϕ. This
property is a consequence of the symmetry of the Laplace equation and the circular
domain with respect to rotations.

204
Elliptic equations
Remark 7.32 Poisson’s formula provides, as a by-product, another proof for the
mean value principle. In fact, the formula is valid with respect to any circle around
any point in a given domain (provided that the circle is fully contained in the
domain). Indeed, if we substitute r = 0 into (7.81), we obtain at once the mean
value principle.
When we solved the Laplace equation in a rectangle or in a disk, we saw that the
solution is in C∞(D). That is, the solution is differentiable inﬁnitely many times at
any interior point. Let us prove this property for any domain.
Theorem 7.33 (Smoothness of harmonic functions) Let u(x, y) be a harmonic
function in D. Then u ∈C∞(D).
Proof Denote by p an interior point in D, and construct a coordinate system
centered at p. Let Ba be a disk of radius a centered at p, fully contained in D.
Write Poisson’s formula for an arbitrary point (x, y) in Ba. We can differentiate
under the integral sign arbitrarily many times with respect to r or with respect to
θ, and thus establish the theorem.
□
7.9 Exercises
7.1 Prove Green’s identity (7.20).
7.2 Prove uniqueness for the Dirichlet and Neumann problems for the reduced Helmholtz
equation
u −ku = 0
in a bounded planar domain D, where k is a positive constant.
7.3 Find the solution u(x, y) of the reduced Helmholtz equation u −ku = 0 (k is a pos-
itive parameter) in the square 0 < x, y < π, where u satisﬁes the boundary condition
u(0, y) = 1, u(π, y) = u(x, 0) = u(x, π) = 0.
7.4 Solve the Laplace equation u = 0 in the square 0 < x, y < π, subject to the bound-
ary condition
u(x, 0) = u(x, π) = 1, u(0, y) = u(π, y) = 0.
7.5 Let u(x, y) be a nonconstant harmonic function in the disk x2 + y2 < R2. Deﬁne for
each 0 < r < R
M(r) =
max
x2+y2=r2 u(x, y).
Prove that M(r) is a monotone increasing function in the interval (0, R)
7.6 Verify that the solution of the Dirichlet problem deﬁned in Example 7.23 is classical.

7.9 Exercises
205
7.7 (a) Compute the Laplace equation in a polar coordinate system.
(b) Find a function u, harmonic in the disk x2 + y2 < 6, and satisfying u(x, y) =
y + y2 on the disk’s boundary. Write your answer in a Cartesian coordinate system.
7.8 (a) Solve the problem
u = 0
0 < x < π, 0 < y < π,
u(x, 0) = u(x, π) = 0
0 ≤x ≤π,
u(0, y) = 0
0 ≤y ≤π,
u(π, y) = sin y
0 ≤y ≤π.
(b) Is there a point (x, y) ∈{(x, y) | 0 < x < π, 0 < y < π} such that u(x, y) = 0?
7.9 A harmonic function of the form
Pn(x, y) =

i+ j=n
ai, jxi y j
is called a homogeneous harmonic polynomial of degree n. Denote the space of ho-
mogeneous harmonic polynomials of degree n by Vn. What is the dimension of Vn?
Hint Use the polar form of the Laplace equation.
7.10 Consider the Laplace equation u = 0 in the domain 0 < x, y < π with the boundary
condition
uy(x, π) = x2 −a, ux(0, y) = ux(π, y) = uy(x, 0) = 0.
Find all the values of the parameter a for which the problem is solvable. Solve the
problem for these values of a.
7.11 SolvetheLaplaceequationu = 0inthedomain x2 + y2 > 4,subjecttotheboundary
condition u(x, y) = y on x2 + y2 = 4, and the decay condition lim|x|+|y|→∞u(x, y) =
0.
7.12 Solve the problem
uxx + uyy = 0
0 < x < 2π, −1 < y < 1,
u(x, −1) = 0,
u(x, 1) = 1 + sin 2x
0 ≤x ≤2π,
ux(0, y) = ux(2π, y) = 0
−1 < y < 1.
7.13 Prove that every nonnegative harmonic function in the disk of radius a satisﬁes
a −r
a + r u(0, 0) ≤u(r, θ) ≤a + r
a −r u(0, 0).
Remark This result is called the Harnack inequality.
7.14 Let D be the domain D = {(x, y) | x2 + y2 < 4}. Consider the Neumann problem
u = 0
(x, y) ∈D,
∂nu = αx2 + βy + γ
(x, y) ∈∂D,
where α, β, and γ are real constants.
(a) Find the values of α, β, γ for which the problem is not solvable.
(b) Solve the problem for those values of α, β, γ for which a solution does exist.

206
Elliptic equations
7.15 Let D = {(x, y) | 0 < x < π, 0 < y < π}. Denote its boundary by ∂D.
(a) Assume vxx +vyy+xvx +yvy >0 in D. Prove that v has no local maximum in D.
(b) Consider the problem
uxx + uyy + xux + yuy = 0
(x, y) ∈D,
u(x, y) = f (x, y)
(x, y) ∈∂D ,
where f is a given continuous function. Show that if u is a solution, then the maximum
of u is achieved on the boundary ∂D .
Hint Use the auxiliary function vε(x, y) = u(x, y) + εx2.
(c) Show that the problem formulated in (b) has at most one solution.
7.16 Let u(x, y) be a smooth solution for the Dirichlet problem
u + ⃗V · ∇u = F
(x, y) ∈D,
u(x, y) = g(x, y)
(x, y) ∈∂D,
where F > 0 in D, g < 0 on ∂D and ⃗V (x, y) is a smooth vector ﬁeld in D. Show that
u(x, y) < 0 in D.
7.17 (a) Solve the equation ut = 2uxx in the domain 0 < x < π, t > 0 under the initial
boundary value conditions
u(0, t) = u(π, t) = 0, u(x, 0) = f (x) = x(x2 −π2).
(b) Use the maximum principle to prove that the solution in (a) is a classical solution.
7.18 Prove that the formulas (7.72)–(7.75) describe solutions of (7.70)–(7.71) that are
bounded at the origin and vanish at r = a.
7.19 Let u(r, θ) be a harmonic function in the disk
D = {(r, θ) | 0 ≤r < R, −π < θ ≤π},
such that u is continuous in the closed disk ¯D and satisﬁes
u(R, θ) =

sin2 2θ
|θ| ≤π/2,
0
π/2 < |θ| ≤π.
(a) Evaluate u(0, 0) without solving the PDE.
(b) Show that the inequality 0 < u(r, θ) < 1 holds at each point (r, θ) in the disk.
7.20 Find a function u(r, θ) harmonic in {2 < r < 4, 0 ≤θ ≤2π}, satisfying the boundary
condition
u(2, θ) = 0, u(4, θ) = sin θ.
7.21 Let u(x, t) be a solution of the problem
ut −uxx = 0
QT = {(x, t) | 0 < x < π, 0 < t ≤T } ,
u(0, t) = u(π, t) = 0
0 ≤t ≤T ,
u(x, 0) = sin2(x)
0 ≤x ≤π .
Use the maximum principle to prove that 0 ≤u(x, t) ≤e−t sin x in the rectangle QT .

7.9 Exercises
207
7.22 Let u(x, y) be the harmonic function in D = {(x, y) | x2 + y2 < 36} which satisﬁes
on ∂D the Dirichlet boundary condition
u(x, y) =

x
x < 0
0
otherwise.
(a) Prove that u(x, y) < min{x, 0} in D.
Hint Prove that u(x, y) < x and that u(x, y) < 0 in D.
(b) Evaluate u(0, 0) using the mean value principle.
(c) Using Poisson’s formula evaluate u(0, y) for 0 ≤y < 6.
(d) Using the separation of variables method, ﬁnd the solution u in D.
(e) Is the solution u classical?

8
Green’s functions and integral representations
8.1 Introduction
Integral representations play a central role in various ﬁelds of pure and applied
mathematics, theoretical physics, and engineering. Many boundary value problems
and initial boundary value problems can be solved using integral kernels. In such
a case, we usually have an explicit formula for the solution as a (ﬁnite) sum of
integrals involving integral kernels and the associated side conditions (which are
the given data). The integral kernel depends on the differential operator, the type
of given boundary condition, and the domain. It should be computed just once for
any given type of problem. Hence, given an integral representation for a differential
problem, we can ﬁnd the solution for a speciﬁc choice of associated conditions by
carrying out just a small number of integrations.
A typical example of an integral representation is the Poisson formula (7.81)
which is an explicit integral representation for solutions of the Dirichlet problem for
the Laplace equation in a disk. Note that the d’Alembert formula (4.17) for solving
the (one-dimensional) Cauchy problem for the nonhomogeneous wave equation
with zero initial conditions is also an integral representation.
In this chapter, we present some further examples of integral representations for
the Laplace operator and for the heat equation. The integral kernel for the Laplace
operator is called Green’s function in honor of the great English mathematician
George Green (1793–1841)1.
Even for these equations, the corresponding integral kernels can be computed
explicitly only in a few special cases. Nevertheless, we can obtain many qualitative
properties of the kernels for a large number of important problems. These prop-
erties can teach us a great deal about the qualitative properties of the solutions of
1 George Green is best known for his famous formula and for his discovery of the Green function. He was an
autodidact with only four terms of elementary school education [1]. Green achieved some of his great results
while working full time as a miller. At the age of 40 he left his mill and was accepted as an undergraduate
student at Cambridge University. Green died before the importance of his results was recognized.
208

8.2 Green’s function (Dirichlet)
209
the problem (their structure, regularity, asymptotic behavior, positivity, stability,
etc.).
The current chapter is devoted to two-dimensional elliptic and parabolic prob-
lems. In Chapter 9, we extend the discussion to integral representations for elliptic
and parabolic problems in higher dimensions, and to the wave equation in the
Euclidean plane and space.
8.2 Green’s function for Dirichlet problem in the plane
Consider the Dirichlet problem for the Poisson equation
u = f
D,
u = g
∂D,
(8.1)
where D is a bounded planar domain with a smooth boundary ∂D. The fundamental
solution of the Laplace equation plays an important role in our discussion. Recall
that this fundamental solution is deﬁned by
(x, y) = −1
2π lnr = −1
4π ln(x2 + y2).
(8.2)
The fundamental solution is harmonic in the punctured plane, and it is a radially
symmetric function with a singularity at the origin. Fix a point (ξ, η) ∈R2. Note
that if u(x, y) is harmonic, then u(x −ξ, y −η) is also harmonic for every ﬁxed
pair (ξ, η). We use the notation
(x, y; ξ, η) := (x −ξ, y −η).
We call (x, y; ξ, η) the fundamental solution of the Laplace equation with a pole
at (ξ, η). The reader can check that

∂
∂x (x, y; ξ, η)
 +

∂
∂y (x, y; ξ, η)
 ≤
C

(x −ξ)2 + (y −η)2 ,
(8.3)

∂2
∂x2 (x, y; ξ, η)
+

∂2
∂x∂y (x, y; ξ, η)
+

∂2
∂y2 (x, y; ξ, η)
 ≤
C
(x −ξ)2 + (y −η)2 .
(8.4)
The function (x, y; ξ, η) is harmonic for any point (x, y) in the plane such that
(x, y) ̸= (ξ, η). For ε > 0, set
Bε := {(x, y) ∈D |

(x −ξ)2 + (y −η)2 < ε},
Dε := D \ Bε.

210
Green’s functions and integral representations
n
n
(x,h)
De
Be
^
^
Figure 8.1 A drawing for the construction of Green’s function.
Let u ∈C2( ¯D). We use the second Green identity (7.19) in the domain Dε where
the function v(x, y) = (x, y; ξ, η) is harmonic to obtain

Dε
(u −u)dxdy =

∂Dε
(∂nu −u∂n)ds.
Therefore,

Dε
u dxdy =

∂D
(∂nu −u∂n)ds +

∂Bε
(∂nu −u∂n)ds.
Let ε tend to zero, recalling that the outward normal derivative (with respect to the
domain Dε) on the boundary of Bε is the inner radial derivative pointing towards
the pole (ξ, η) (see Figure 8.1).
Using estimates (8.3)–(8.4) we obtain


∂Bε
∂nu ds
 ≤Cε| ln ε| →0
as ε →0,

∂Bε
u∂n ds =
1
2πε

∂Bε
u ds →u(ξ, η)
as ε →0.
Therefore,
u(ξ, η) =

∂D
[(x −ξ, y −η) ∂nu −u∂n  (x −ξ, y −η)]ds
−

D
(x −ξ, y −η)u dxdy.
(8.5)

8.2 Green’s function (Dirichlet)
211
Formula (8.5) is called Green’s representation formula, and the function
[ f ](ξ, η) := −

D
(x −ξ, y −η) f (x, y) dxdy
is called the Newtonian potential of f .
The following corollary has already been proved using another approach.
Corollary 8.1 If u is harmonic in a domain D, then u is inﬁnitely differentiable in
D.
Proof By Green’s representation formula
u(ξ, η)=

∂D
[(x−ξ, y−η)∂nu−u∂n(x−ξ, y−η)]ds.
The integrand is an inﬁnitely differentiable function of ξ and η inside D. Inter-
changing the order of integration and differentiation we obtain the claim.
□
Corollary 8.2 Let u ∈C2(R2) be a function that vanishes identically outside a disk
(in other words, u has a compact support in R2). Then
u(ξ, η) = −

R2 (x −ξ, y −η)u(x, y) dxdy.
(8.6)
Let us discuss in some detail the nature of the function (x −ξ, y −η). It is
clear that (x −ξ, y −η) = 0 for all (x, y) ̸= (ξ, η). On the other hand, if we
(formally) carry out integration by parts of (8.6) for u with a compact support, we
obtain
u(ξ, η) = −

R2 (x −ξ, y −η)u(x, y) dxdy.
(8.7)
Therefore, the “function”
δ(x −ξ, y −η) := −(x −ξ, y −η)
vanishes at all points (x, y) ̸= (ξ, η), but its integral against any smooth function u
is not zero; rather it reproduces the value of u at the point (ξ, η). For the particular
case (ξ, η) = (0, 0), we write
−(x, y) = δ(x, y).
It is clear that δ is not a function in the classical sense. It is a mathematical object
called a distribution. The distribution δ (which in the folklore is often termed
the delta function) is called the Dirac distribution, and it is characterized by the

212
Green’s functions and integral representations
following formal expression:
u(ξ, η) =

R2 δ(x −ξ, y −η)u(x, y) dxdy
(8.8)
for any smooth function u with a compact support in R2.
Wemaycharacterize the delta functionasthe limitofcertainsequencesofsmooth
functions with a compact support. For example, consider a smooth nonnegative
function ρ on R2, vanishing outside the unit ball, and satisfying

R2 ρ(⃗x) d⃗x = 1.
Fix ⃗y ∈R2 and let ε > 0. Deﬁne the function
ρε(⃗x) := ε−2ρ
	 ⃗x −⃗y
ε

.
Note that ρε is supported in a ball of radius ε around ⃗y and satisﬁes

R2 ρε(⃗x) d⃗x = 1.
For any smooth function u with a compact support in R2 we have
lim
ε→0+

R2 ρε(⃗x)u(⃗x) d⃗x = u(⃗y) =

R2 δ(⃗x −⃗y)u(⃗x) d⃗x.
(8.9)
We say that ρε converges in the sense of distribution to the delta function at ⃗y as
ε →0, and ρε is called an approximation of the delta function.
A standard example of such an approximation of the delta function is given by
ρ(⃗x) :=



c exp
	
1
|⃗x|2 −1

|⃗x| ≤1,
0
otherwise,
(8.10)
where c is a positive constant (see Exercise 8.7).
Thereadermayrecallfromlinearalgebrathenotionsofadjointmatrixandadjoint
of a linear operator. We introduce now the deﬁnition of the adjoint operator of a
given differential operator L. The relation to the algebraic notion will be explained
later. We also give below the general deﬁnition of a fundamental solution.
Deﬁnition 8.3 Let
L[u] =

0≤i+ j≤m
ai j(x, y)∂i
x∂j
yu
be a linear differential operator of order m with smooth coefﬁcients ai j that is
deﬁned on R2.

8.2 Green’s function (Dirichlet)
213
(a) The operator
L∗[v] =

0≤i+ j≤m
(−1)i+ j∂i
x∂j
y (ai j(x, y)v)
is called the formal adjoint operator of L.
(b) Fix (ξ, η) ∈R2. Suppose that a function v(x, y; ξ, η) satisﬁes
L[v] = δ(x −ξ, y −η)
in the sense of distributions, which means that
φ(ξ, η) =

R2 v(x, y; ξ, η)L∗[φ(x, y)] dxdy
for any smooth function φ with a compact support. Then v is called the fundamental
solution of the equation L[u] = 0 with a pole at (ξ, η).
Example 8.4 (a) The formal adjoint of the Laplace operator L[u] = u is given
by L∗[u]=u = L[u]. If L∗= L, the operator L is said to be formally selfadjoint.
(b) It can be checked that a Sturm–Liouville operator on an interval I, the wave
operator L[u] = utt −c2u, and the biharmonic operator L[u] = 2u are also
formally selfadjoint.
(c) The formal adjoint of the heat operator L[u] = ut −u is the backward heat
operator L∗[u] = −ut −u.
Remark 8.5 (a) Let u, v ∈C∞
0 (R2), where C∞
0 (D) is the space of all smooth (in-
ﬁnitely differentiable) functions with a compact support in D. Integrating by parts,
one can verify that

R2 L[u(x, y)]v(x, y) dxdy =

R2 u(x, y)L∗[v(x, y)] dxdy.
This means that the operator L∗is the (algebraic) adjoint of the operator L on the
space C∞
0 (R2) with respect to the inner product ⟨u, v⟩=

R2 u(x, y)v(x, y) dxdy.
(b) The fundamental solution is not uniquely deﬁned. If v is a fundamental solu-
tion, then v + w is also fundamental solution for any w that solves the homogeneous
equation L[u] = 0.
(c) If the operator L is a linear operator with constant coefﬁcients, and if v(x, y)
is a fundamental solution with a pole at (0, 0), then v(x −ξ, y −η) is a fundamental
solution with a pole at (ξ, η).
(d) We showed above that  is a fundamental solution of the Laplace operator−
on R2, and the signiﬁcance of the factor −1/2π in the deﬁnition of  is now clear.
Consider again the Dirichlet problem (8.1). Green’s representation formula (8.5)
enables us to compute the value of u(ξ, η) for all (ξ, η) ∈D if we know u in D,
and the values of u and ∂nu on the boundary of D. But for the Dirichlet problem

214
Green’s functions and integral representations
for the Poisson equation the values of ∂nu are not given on ∂D. Therefore, in order
to obtain an integral representation for the Dirichlet problem, we have to modify
(8.5). Let h(x, y; ξ, η) be a solution (that depends on the parameter (ξ, η) ∈D) of
the following Dirichlet problem:
h(x, y; ξ, η) = 0
(x, y) ∈D,
h(x, y; ξ, η) = (x, y; ξ, η)
(x, y) ∈∂D.
(8.11)
By the second Green’s identity (7.19),
−

D
h(x, y; ξ, η)u(x, y) dxdy
=

∂D
[u(x, y)∂nh(x, y; ξ, η) −h(x, y; ξ, η)∂nu(x, y)]ds
=

∂D
[u(x, y)∂nh(x, y; ξ, η) −(x−ξ, y−η)∂nu(x, y)]ds.
(8.12)
We introduce now the following important deﬁnition.
Deﬁnition 8.6 The Green function of the domain D for the Laplace operator and
the Dirichlet boundary condition is given by
G(x, y; ξ, η) := (x, y; ξ, η) −h(x, y; ξ, η)
(x, y), (ξ, η) ∈D, (x, y) ̸= (ξ, η),
(8.13)
where h is the solution of (8.11)
It follows that the Green function satisﬁes
G(x, y; ξ, η) = −δ(x −ξ, y −η)
(x, y) ∈D,
G(x, y; ξ, η) = 0
(x, y) ∈∂D.
(8.14)
We now add (8.12) and (8.5) to obtain
u(ξ, η)=−

∂D
∂nG(x, y; ξ, η)u(x, y) ds−

D
G(x, y; ξ, η)u(x, y) dxdy.
(8.15)
Substituting the given data into (8.15), we ﬁnally arrive at the following integral rep-
resentation formula for solutions of the Dirichlet problem for the Poisson equation.
Theorem 8.7 Let D be a smooth bounded domain, f ∈C( ¯D), and g ∈C(∂D).
Let u ∈C2( ¯D) be a solution of the Dirichlet problem
u = f
(x, y) ∈D,
u = g
(x, y) ∈∂D.
(8.16)
Then
u(ξ, η)=−

∂D
∂nG(x, y; ξ, η)g(x, y) ds−

D
G(x, y; ξ, η) f (x, y) dxdy. (8.17)

8.2 Green’s function (Dirichlet)
215
The representation formula (8.17) involves two integral kernels:
(1) The Green function G(x, y; ξ, η), which is deﬁned for all (x, y), (ξ, η) ∈D,
(x, y) ̸= (ξ, η).
(2) K(x, y; ξ, η) := −∂nG(x, y; ξ, η), which is the inward normal derivative of the Green
function on the boundary of the domain D. Therefore, the kernel K is deﬁned for
(x, y) ∈∂D, (ξ, η) ∈D.
Deﬁnition 8.8 The functionK(x, y; ξ, η) is called the Poisson kernel of the Laplace
operator and the Dirichlet problem on D.
Remark 8.9 The reader should show as an exercise that the Poisson kernel that
was obtained in Section 7.8 (for the special case of a disk) is indeed the normal
derivative of the corresponding Green function.
Theorem 8.7 enables us to solve the Dirichlet problem in a domain D provided
that the Green function is known, and that it is a priori known that the solution is
in C2(D) ∩C1( ¯D). This additional regularity is indeed ensured if f, g, and ∂D are
sufﬁciently smooth.
The Green function can be computed explicitly only for a small number of
domains. Some examples of such domains will be presented below and in the
exercises. Nevertheless, the Green function is a very useful tool in the study of the
Dirichlet problem, and therefore we present now its main properties.
The uniqueness of the Green function follows directly from the uniqueness of
the function h, i.e. from the uniqueness of the solution of the Dirichlet problem
for the Laplace equation in D (Theorem 7.12). On the other hand, the existence
of the Green function for a domain D follows from the existence of a solution of
the Dirichlet problem for the Laplace equation in the domain D. The study of the
existence theorem for a smooth bounded domain D is outside the scope of this
book, but the standard proof relies heavily on the existence of a solution for the
special case of a disk. Recall that the existence theorem for the disk was proved
independently in Section 7.8 using the Poisson formula. It follows that the existence
of the Green function is not based on a circular argumentation.
Theorem 8.10 The Green function for the Dirichlet problem is symmetric in the
sense that
G(x, y; ξ, η) = G(ξ, η; x, y)
for all (x, y), (ξ, η) ∈D such that (x, y) ̸= (ξ, η).
Proof Fix two points (x, y), (ξ, η) ∈D such that (x, y) ̸= (ξ, η), and let
v(σ, τ) := G(σ, τ; x, y),
w(σ, τ) := G(σ, τ; ξ, η).

216
Green’s functions and integral representations
The functions v and w are harmonic in D \ {(x, y), (ξ, η)} and vanish on ∂D. We
again use the second Green identity (7.19) for the domain ˜Dε which contains all
the points in D such that their distances from the poles (x, y) and (ξ, η) are larger
than ε. We have

∂B((x,y);ε)
(w∂nv −v∂nw)ds(σ, τ) =

∂B((ξ,η);ε)
(v∂nw −w∂nv)ds(σ, τ).
(8.18)
Using the estimates (8.3)–(8.4) we infer that
lim
ε→0

∂B((x,y);ε)
|v∂nw|ds(σ, τ) = lim
ε→0

∂B((ξ,η);ε)
|w∂nv|ds(σ, τ) = 0;
(8.19)
but
lim
ε→0

∂B((x,y);ε)
w∂nv ds(σ, τ) = w(x, y),
lim
ε→0

∂B((ξ,η);ε)
v∂nw ds(σ, τ) = v(ξ, η).
(8.20)
Letting ε →0 in (8.18) and using (8.19) and (8.20), we obtain
G(x, y; ξ, η) = w(x, y) = v(ξ, η) = G(ξ, η; x, y).
□
Theorem 8.11 (a) Fix (x, y) ∈D. The Green function G(x, y; ξ, η), considered
as a function of (ξ, η), is a positive harmonic function in the domain D \ {(x, y)}
which vanishes on ∂D.
(b) Fix (x, y) ∈∂D. The Poisson kernel K(x, y; ξ, η), considered as a function of
(ξ, η), is a positive harmonic function in the domain D which vanishes on ∂D\
{(x, y)}.
Proof We only sketch the proof.
(a) The fact that G, as a function of (ξ, η), is harmonic and vanishes on the boundary follows
directly from the symmetry of G. Since G is positive near the pole and vanishes on the
boundary, the weak maximum principle implies that for ε > 0 sufﬁciently small, the
function G is positive also in D \ Bε, where Bε is the open disk of radius ε with a center
on the pole (x, y).
(b) Since G vanishes on the boundary and is positive on D, it follows that on the boundary
its inward normal derivative (i.e. K) is nonnegative. The proof of the strict positivity
of K will not be given here. The Poisson kernel is a derivative of a harmonic function.
In other word, K is a limit of a family of harmonic functions which implies that it is
harmonic.
□
Corollary 8.12 Let D be a smooth bounded domain. Let f be a nonpositive con-
tinuous function in D, and let g be a nonnegative continuous function on ∂D, such

8.2 Green’s function (Dirichlet)
217
y
x
R
(x,y)
(x~,y)
~
Figure 8.2 The inverse of a point with respect to the circle.
that at least one of these two functions is not identically zero. Then the solution u
of the Dirichlet problem (8.1) is a positive function in D.
Proof The proof follows directly from Theorems 8.7 and 8.11.
□
Proposition 8.13 Let D1, D2 be (planar) smooth bounded domains such that D1 ⊂
D2. Let Gi be the Green function of the domain Di, where i = 1, 2. Then
0 ≤G1(x, y; ξ, η) ≤G2(x, y; ξ, η)
(x, y), (ξ, η) ∈D1.
Proof Fix (ξ, η) ∈D1, and let Bε be the open disk of radius ε centered at (ξ, η).
Since
lim
(x,y)→(ξ,η)
G1(x, y; ξ, η)
G2(x, y; ξ, η) = 1,
it follows that for any δ > 1 there exists ε > 0 such that 0 ≤G1(x, y; ξ, η) ≤
δG2(x, y; ξ, η) in a disk Bε. Theorem 8.11 and the weak maximum principle in the
domain D1 \ Bε imply that 0 ≤G1(x, y; ξ, η) ≤δG2(x, y; ξ, η) on D1\Bε. Letting
δ →1, it follows that
0 ≤G1(x, y; ξ, η) ≤G2(x, y; ξ, η)
(x, y), (ξ, η) ∈D1.
□
Example 8.14 Let BR be the disk of radius R centered at the origin. We want to
compute the Green function of BR, and derive from it the Poisson kernel. We use
the reﬂection principle.
Let (x, y) ∈BR. The point
(˜x, ˜y) :=
R2
x2 + y2 (x, y)
is called the inverse point of (x, y) with respect to the circle ∂BR (see Figure 8.2).
It is convenient to deﬁne the ideal point ∞as the inverse of the origin. Deﬁne

218
Green’s functions and integral representations
G R(x, y; ξ, η):=



(x −ξ, y −η)−

ξ 2 + η2
R
(x −˜ξ, y −˜η)

(ξ, η) ̸= (0, 0),
(x, y) + 1
2π ln R
(ξ, η) = (0, 0),
(8.21)
and set
r =

(x −ξ)2 + (y −η)2, r∗=
%
(x −R2
ρ2 ξ)2 + (y −R2
ρ2 η)2, ρ =

ξ 2 + η2.
An elementary calculation implies that
G R(x, y; ξ, η) =



−1
2π ln Rr
ρr∗
(ξ, η) ̸= (0, 0),
−1
2π ln r
R
(ξ, η) = (0, 0),
(8.22)
and that G R satisﬁes all the properties of the Green function. Moreover, it can be
checked that the radial derivative of G R on the circle of radius R is the Poisson
kernel, which was calculated using a completely different approach in Section 7.8
(see Exercise 8.1).
Example 8.15 Denote by R2
+ := {(x, y) | y > 0} the open upper half-plane. Al-
though this is an unbounded domain, it is possible to use the reﬂection principle
again to obtain the corresponding Green function. Let (x, y) ∈R2
+. The point
(˜x, ˜y) := (x, −y)
is called the inverse point of (x, y) with respect to the real line. It can be readily
veriﬁed that the function
G(x, y; ξ, η) : = (x −ξ, y −η) −(x −˜ξ, y −˜η)
= −1
2π ln

(x −ξ)2 + (y −η)2

(x −ξ)2 + (y + η)2
(8.23)
satisﬁes all the properties of the Green function, and its derivative in the y direction
on the boundary (y = 0) of R2
+ is given by
K(x, 0; ξ, η) :=
η
π[(x −ξ)2 + η2]
(x, 0) ∈∂R2
+, (ξ, η) ∈R2
+
(8.24)
(see Exercise 8.5).

8.3 Neumann’s function in the plane
219
8.3 Neumann’s function in the plane
We move on to present an integral representation for solutions of the Neumann
problem for the Poisson equation:
u = f
D,
∂nu = g
∂D,
(8.25)
where D is a smooth bounded domain. The ﬁrst difﬁculty that arises is the
nonuniqueness of the problem, which implies that it is impossible to ﬁnd a unique
integral formula. Furthermore, we should recall the solvability condition (7.9) for
the Neumann problem. Nevertheless, the derivation of the integral representation
for the Neumann problem is basically similar to the procedure for the Dirichlet
problem.
Recall that the Green representation formula (8.5) enables us to reproduce the
value of an arbitrary smooth function u at any point (ξ, η) in D provided that u
is given in D, and u and ∂nu are given on ∂D. For the Neumann problem, u is
not known on ∂D. We proceed now with almost the same idea that was used for
the Dirichlet problem. Let h(x, y; ξ, η) be a solution (depending on the parameter
(ξ, η)) of the following Neumann problem:
h(x, y; ξ, η) = 0
(x, y) ∈D,
∂nh(x, y; ξ, η) = ∂n(x, y; ξ, η) + 1/L
(x, y) ∈∂D,
(8.26)
where L is the length of ∂D. Substituting u = 1 into the Green representation
formula (8.5) implies that

∂D
∂n(x, y; ξ, η)ds = −1.
(8.27)
Therefore, the boundary condition in (8.26) satisﬁes the solvability condition (7.9).
It is known that (7.9) is not only a necessary condition but also a sufﬁcient condition
for the solvability of the problem.
Deﬁnition 8.16 A Neumann function for a domain D and the Laplace operator is
the function
N(x, y; ξ, η) := (x, y; ξ, η) −h(x, y; ξ, η)
(x, y), (ξ, η) ∈D, (x, y) ̸= (ξ, η),
(8.28)
where h(x, y; ξ, η) is a solution of (8.26).

220
Green’s functions and integral representations
In other words, a Neumann function satisﬁes
N(x, y; ξ, η) = −δ(x −ξ, y −η)
(x, y) ∈D,
∂nN(x, y; ξ, η) = −1
L
(x, y) ∈∂D.
(8.29)
Therefore,
u(ξ, η) =

∂D
N(x, y; ξ, η)∂nu(x, y) ds
−

D
N(x, y; ξ, η)u(x, y) dxdy + 1
L

∂D
u ds.
(8.30)
Substituting the given data into (8.30), we obtain the following representation for-
mula for solutions of the Neumann problem.
Theorem 8.17 Suppose that u ∈C2( ¯D) is a solution of the Neumann problem
u = f
D,
∂nu = g
∂D.
(8.31)
Then
u(ξ, η)=

∂D
N(x, y; ξ, η)g(x, y) ds−

D
N(x, y; ξ, η) f (x, y) dxdy + 1
L

∂D
u ds.
(8.32)
Remark 8.18 (a) The kernel N is not called the Green function of the problem,
since N doesnotsatisfythecorrespondinghomogeneousboundarycondition.There
is no kernel function that satisﬁes
G(x, y; ξ, η) = −δ(x −ξ, y −η)
(x, y) ∈D,
∂nN(x, y; ξ, η) = 0
(x, y) ∈∂D.
(8.33)
(b) The Neumann function is determined up to an additive constant. In order to
uniquely deﬁne N it is convenient to use the normalization

∂D
N(x, y; ξ, η) ds = 0.
(8.34)
(c) The third term in the representation formula (8.32) is (1/L)

∂D u ds, the
averageofu ontheboundary,whichisnotgiven.Butsincethesolutionisdetermined
up to an additive constant, it is convenient to add the condition

∂D
u(x, y) ds = 0,
(8.35)

8.4 The heat kernel
221
and then the problem is uniquely solved, and the corresponding integral represen-
tation uniquely determines the solution.
8.4 The heat kernel
Consider (again) the homogeneous heat problem with the Dirichlet condition
ut −kuxx = 0
0 < x < L, t > 0,
u(0, t) = u(L, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤L,
(8.36)
that was solved in Section 5.2. Using the separation of variables method, we found
that the solution of the problem is of the form
u(x, t) =
∞

n=1
Bn sin nπx
L e−k( nπ
L )2t,
(8.37)
where Bn are the Fourier coefﬁcients
Bn = 2
L
 L
0
sin nπy
L
f (y) dy
n = 1, 2, . . . .
(8.38)
For ﬁxed t > ε > 0 and 0 < x < L, the series
2
L
∞

n=1

e−k( nπ
L )2t sin nπx
L

sin nπy
L
f (y)
converges uniformly (as a function of y). Therefore, we may integrate term by term
and hence
u(x, t) =
∞

n=1
e−k( nπ
L )2t sin nπx
L
	 2
L
 L
0
sin nπy
L
f (y) dy

=
 L
0

2
L
∞

n=1
e−k( nπ
L )2t sin nπx
L
sin nπy
L

f (y) dy.
The function
K(x, y, t) := 2
L
∞

n=1
e−k( nπ
L )2t sin nπx
L
sin nπy
L
(8.39)
is called the heat kernel of the initial boundary condition (8.36). The reader can
verify that for every ﬁxed y the kernel K is a solution of the heat equation and
that it satisﬁes the Dirichlet conditions for t > 0. In addition, K is symmetric, i.e.
K(x, y, t) = K(y, x, t).

222
Green’s functions and integral representations
To summarize, we have obtained the following simple integral representation:
u(x, t) =
 L
0
K(x, y, t) f (y) dy,
(8.40)
for the solution of the initial boundary value problem (8.36).
Consider now the nonhomogeneous problem
ut −kuxx = F(x, t)
0 < x < L, t > 0,
u(0, t) = u(L, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤L.
(8.41)
We apply the Duhamel principle (see Exercise 5.14). Let v(x, t, s) be the so-
lution of the following initial homogeneous problem (which depends on the
parameter s)
vt −kvxx = 0
0 < x < L, t > s,
v(0, t, s) = v(L, t, s) = 0
t ≥s,
v(x, s, s) = F(x, s)
0 ≤x ≤1.
Using the integral representation (8.40), we can express v(x, t, s) in the form
v(x, t, s) =
 L
0
K(x, y, t −s)F(y, s) dy.
Therefore, by the Duhamel principle and the superposition principle, the solution
of problem (8.41) is given by the integral representation
u(x, t) =
 L
0
K(x, y, t) f (y) dy +
 t
0
 L
0
K(x, y, t −s)F(y, s) dy ds.
(8.42)
The signiﬁcance of (8.39) and (8.42) is that they are valid in a much broader
context (see also Section 9.12).
Remark 8.19 From the exponential decay in (8.39), it follows that the heat kernel
is a smooth function for 0 < x < L, t > 0. On the other hand, the heat kernel is
singular at t = 0, for x = y. As for the Green function, the precise character of this
singularity is explained rigorously by the theory of distributions. It turns out that
for a ﬁxed 0 < y < L, the heat kernel K(x, y, t) is a distribution with a support at
 = [0, L] × R that solves the problem
Kt −kKxx = δ(x −y)δ(t)
0 < x < L, −∞< t < ∞,
K(x, y, t) = 0
t < 0,
K(0, y, t) = K(L, y, t) = 0
t > 0.
(8.43)

8.5 Exercises
223
In other words, for any smooth function ϕ with a compact support in , we have
ϕ(y, 0) =


K(x, y, t)[−∂tϕ(x, t) −∂xxϕ(x, t)] dx dt,
and for any smooth function ψ with a compact support in {(x, t) | 0 ≤x ≤L, t <
0}, we have


K(x, y, t)ψ(x, t) dx dt = 0.
The following is an alternative but equivalent characterization of the heat kernel.
For any ﬁxed 0 < y < L and t > 0, the heat kernel K(x, y, t) is a distribution with
a compact support in [0, L] that satisﬁes
Kt −kK = 0
0 < x < L, 0 < t,
K(x, y, 0) = δ(x −y),
K(0, y, t) = K(L, y, t) = 0
t > 0.
In the latter formulation, t is considered as a parameter, and the precise meaning is
that for any smooth function φ(x) with a compact support in [0, L] we have



∂
∂t
L

0
K(x, y, t)φ(x) dx −
L

0
K(x, y, t)∂xxφ(x) dx = 0
∀t > 0,
lim
t→0+
L

0
K(x, y, t)φ(x) dx = φ(y).
8.5 Exercises
8.1 (a) Show that the function that is deﬁned in (8.22) is indeed the Green function in BR,
and that its radial derivative on the circle is the Poisson kernel which was derived in
Section 7.8.
(b) Evaluate limR→∞G R(x, y; ξ, η).
8.2 Prove that the Neumann function for the Poisson equation is symmetric, i.e.
N(x, y; ξ, η) = N(ξ, η; x, y),
for all (x, y), (ξ, η) ∈D such that (x, y) ̸= (ξ, η).
Hint The proof is similar to the proof of Theorem 8.10.
8.3 (a) Derive an explicit formula for the Green function of a disk as an inﬁnite series,
using (7.76) which is a formula for the solution of the Dirichlet problem for the Poisson
equation.
(b) Calculate the sum of the above series and obtain the explicit formula (8.22) for the
Green function of the disk.

224
Green’s functions and integral representations
8.4 (a) Write the Green function of (8.22) in polar coordinates.
(b) Using a reﬂection principle and part (a) ﬁnd the Green function of half of a disk.
8.5 (a) Show that the function which is deﬁned in (8.23) is indeed the Green function in
R2
+, and that its derivative in the y direction for y = 0 is the Poisson kernel which is
given by (8.24).
(b) Using a reﬂection principle and part (a) ﬁnd the Green function of the positive
quarter plane x > 0, y > 0.
8.6 Let R2
+ be the upper half-plane. Find the Neumann function of R2
+.
8.7 (a) Prove (8.9).
(b) Find the constant c in (8.10), and verify directly that ρε is an approximation of the
delta function.
8.8 Let k ̸= 0. Show that the function Gk(x, ξ) = e−k|x−ξ|/2k is a fundamental solution
of the equation
−u′′ + k2u = 0
−∞< x < ∞.
Hint Use one of Green’s identities.
8.9 Show that the Gaussian kernel
K(x, y, t) :=



1
(4πkt)1/2 e−(x−y)2
4kt
t > 0,
0
t < 0.
(8.44)
is the heat kernel for the Cauchy problem
ut −kuxx = 0
−∞< x < ∞, t > 0,
u(x, 0) = f (x)
−∞< x < ∞,
where f is a bounded continuous function on R.
8.10 Use a reﬂection principle and the (Gaussian) heat kernel (8.44) to obtain the heat
kernel for the problem
ut −kuxx = 0
0 < x < ∞, t > 0,
u(0, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤∞.
8.11 Let DR := R2 \ BR be the exterior of the disk with radius R centered at the origin.
Find the (Dirichlet) Green function of DR.
8.12 (a) Use a reﬂection principle and the (Gaussian) heat kernel (8.44) to obtain the
following alternative representation of the heat kernel for the initial boundary value
problem (8.36):
K(x, y, t) =
1
(4πkt)1/2
n=∞

n=−∞

e−(x−y−2Ln)2
4kt
−e−(x+y−2Ln)2
4kt

t > 0.
(8.45)

8.5 Exercises
225
(b) Use (8.45) to show that the exact short time behavior (t →0+, x ̸= y) of the heat
kernel for the problem (8.36) is given by (8.44).
(c) Use (8.39) to show that the exact large time behavior (t →∞) of the heat kernel
for the problem (8.36) is given by
K(x, y, t) ≈2
L e−k( π
L )2t sin πx
L sin πy
L .
8.13 Let BR be the disk with radius R centered at the origin. Find the Neumann function
of BR.

9
Equations in high dimensions
9.1 Introduction
To simplify the presentation we have concentrated so far mainly on equations in-
volving two independent variables. In this chapter we shall extend the discussion to
equations in higher dimensions. A considerable part of the theoretical and practical
aspects that we studied for equations in two variables can be extended at once to
higher dimensions. Nevertheless, we shall see that there are sometimes signiﬁcant
differences between problems in different dimensions.
9.2 First-order equations
The general ﬁrst-order quasilinear equation for a function u in n variables is
n

i=1
ai(x1, x2, . . . , xn, u)uxi = c(x1, x2, . . . , xn, u).
(9.1)
The method of characteristics that we developed in Chapter 2 is also valid for (9.1).
The initial condition for (9.1) is an (n −1)-dimensional surface  in the Euclidean
space Rn+1. We write  parameterically:
x0, i = x0, i(s1, s2, . . . , sn−1)
i = 1, 2, . . . , n,
(9.2)
u0 = u0(s1, s2, . . . , sn−1).
(9.3)
Similarly to the two-dimensional case we write the characteristic equations
∂xi
∂t = ai(x1, x2, . . . , xn, u)
i = 1, 2, . . . , n,
(9.4)
∂u
∂t = c(x1, x2, . . . , xn, u).
(9.5)
226

9.2 First-order equations
227
Solving the system of ODEs (9.4)–(9.5) with the initial data (9.2)–(9.3) at t = 0,
we generate the solution u(x1, x2, . . . , xn) of (9.1) as a parametric n-dimensional
hypersurface
xi = xi(t, s1, s2, . . . , sn−1)
i = 1, 2, . . . , n,
u = u(t, s1, s2, . . . , sn−1).
The transversality condition that we introduced in Chapter 2 takes now the form
J

 =


∂x0,1
∂s1
∂x0,2
∂s1
· · ·
∂x0,n
∂s1
∂x0,1
∂s2
∂x0,2
∂s2
· · ·
∂x0,n
∂s2
...
...
...
∂x0,1
∂sn−1
∂x0,2
∂sn−1
· · ·
∂x0,n
∂sn−1
a1
a2
· · ·
an


̸= 0.
(9.6)
When this condition holds, the parametric representation we obtained indeed
provides the (locally) unique solution to (9.1). Generalizing the existence and
uniqueness statement of Theorem 2.10 and the discussion that follows it to the
n-dimensional case is straightforward.
Example 9.1 Solve the linear equation
xux + yuy + zuz = 4u,
subject to the initial condition u(x, y, 1) = xy.
The characteristic equations are
xt = x, yt = y, zt = z, ut = 4u,
and the initial conditions can be written parametrically as
x(0, s1, s2) = s1, y(0, s1, s2) = s2, z(0, s1, s2) = 1, u(0, s1, s2) = s1s2.
The transversality condition can easily be seen to hold. Solving the characteristic
equations and substituting in the initial condition yields:
x = s1et, y = s2et, z = et, u = s1s2e4t.
Therefore, the solution is given by u(x, y, z) = xyz2.
Consider now the general ﬁrst-order equation in n independent variables:
F(x1, x2, . . . , xn, u, ux1, ux2, . . . , uxn) = 0.
(9.7)

228
Equations in high dimensions
The associated Cauchy problem consists of (9.7) and the initial condition provided
asan(n −1)-dimensionalsurface intheEuclideanspaceRn+1,asdescribedabove
in (9.2)–(9.3). The method of characteristic strips that we developed in Chapter 2
is also valid in the higher-dimensional case. The strip equations are given by
∂xi
∂t = ∂F
∂pi
i = 1, 2, . . . , n,
∂u
∂t
=
n

i=1
pi
∂F
∂pi
,
∂pi
∂t = −∂F
∂xi
−pi
∂F
∂u
i = 1, 2, . . . , n,
(9.8)
where we used the notation pi = ∂u/∂xi. To obtain a unique solution we must
supply appropriate initial conditions. One such condition is given by the initial
surface . The additional conditions (for pi) are determined in a similar way to
(2.89)–(2.90). We therefore write the initial conditions for pi as
pi(0, s1, . . . , sn−1) = p0,i(s1, s2, . . . , sn−1).
The functions p0,i are determined from the equations
∂u0
∂si
=
n

i=1
p0,i
∂x0,i
∂si
i = 1, 2, . . . , n −1,
(9.9)
and
F(x0,1, x0,2, . . . , x0,n, u0, p0,1, p0,2, . . . , p0,n) = 0,
(9.10)
provided that an appropriate transversality condition holds true.
9.3 Classiﬁcation of second-order equations
In Chapter 3 we classiﬁed second-order equations in two independent variables into
three categories. A similar classiﬁcation in higher dimensions is more intricate. Let
u(x1, x2, . . . , xn) be a function satisfying a second-order equation whose principal
part is of the form
L0[u] =
n

i, j=1
ai j
∂2u
∂xi∂x j
,
(9.11)
where ai j = ai j(x1, x2, . . . , xn). Since the mixed derivatives are invariant under a
change in the order of differentiation, we can assume without loss of generality
that the coefﬁcient matrix A = (ai j) is symmetric. Thus the principal part can be
considered as
L0 = ( ⃗∇x)tA ⃗∇x,
(9.12)

9.3 Classiﬁcation of second-order equations
229
where ⃗∇x denotes the gradient operator with respect to the variables (x1, . . . , xn). In
the special case considered earlier in Chapter 3 we had A =
	 A B
B C

. In the process
of changing the notation (9.11) to the notation (9.12) we may have generated ﬁrst-
order derivatives of u, but such terms have no effect on the principal part.
In order to obtain a classiﬁcation scheme for equations in arbitrarily many vari-
ables, it is beneﬁcial to review some of the fundamental issues that we saw during
our analysis of equations in two variables. We recall the deﬁnition of characteristic
curves provided in Chapter 2. A fundamental property of these curves is that when
the initial condition is provided on such a curve, the associated Cauchy problem
does not have a unique solution. Actually a characteristic curve can be deﬁned
as a curve satisfying this property. Later, in Chapter 3, we saw another important
property of these curves: they are exactly the curves along which singularities prop-
agate. It turns out that these two properties of the characteristic curves are related
to each other. We shall analyze this relation, and elaborate on its signiﬁcance to the
classiﬁcation of equations in n dimensions.
We start by deﬁning the Cauchy problem for an equation in n variables.
Deﬁnition 9.2 Cauchy problem Find a function u(x1, x2, . . . , xn) in the space C2
satisfying a given second-order PDE whose principal part is given by (9.11), such
that u and all its ﬁrst derivatives are provided on a hypersurface  that is given
parametrically by φ(x1, x2, . . . , xn) = 0.
A necessary condition for a solution for a Cauchy problem to exist is that the mixed
derivatives be compatible (in the sense that the mixed derivative does not depend
on the order of differentiation). We assume that this condition holds (otherwise the
problem is not meaningful). Formally, to ﬁnd the solution of a Cauchy problem in a
neighborhood of the initial surface we have to compute the second-order derivatives
of u from the PDE itself and from the initial data. Differentiating the equation will
then enable us to ﬁnd the third-order derivatives and so forth. This process fails if
we cannot eliminate some second-order derivative from the condition of the Cauchy
problem. We thus deﬁne characteristic surfaces as follows.
Deﬁnition 9.3 A surface  will be called a characteristic surface with respect to
a second-order PDE if it is not possible to eliminate at least one second derivative
of u from the conditions of the Cauchy problem.
Example 9.4 Consider the hyperbolic equation in two variables
uη1η2 = 0.
(9.13)

230
Equations in high dimensions
We try to solve the Cauchy problem consisting of (9.13) and the initial data
u(η1, 0) = f (η1),
uη2(η1, 0) = g(η1).
(9.14)
Recalling that the general solution to (9.13) is of the form u(η1, η2) = F(η1) +
G(η2), we see at once that the problem cannot, in general, be solved. The reason
is that the problem does not contain a term involving the second derivative with
respect to η2, and, therefore, it is not possible to “get off” the initial surface (the η1
axis) in the normal direction.
Having used the formulation of the Cauchy problem to deﬁne characteristic
surfaces, we shall show that these are the only surfaces along which the solution
can be singular.
Lemma 9.5 Let u(x1, x2, . . . , xn) be a solution of a Cauchy problem. Assume that
u ∈C1 in some domain , and, furthermore, u ∈C2 in  except for a surface .
Then  is a characteristic surface.
Proof Suppose by contradiction that  is not characteristic. Then knowing the
values of u and its derivatives on  and using the PDE, we can eliminate the
second derivatives of u on both sides of . But then the continuity of u and its ﬁrst
derivatives imply that the second derivatives are also continuous, which contradicts
the assumptions.
□
We shall use Deﬁnition 9.3 to derive an analytic criterion for the existence of
characteristic surfaces, and even to compute such surfaces. In Example 9.4 we could
have veriﬁed that the surface η2 = 0 is characteristic since the equation had no sec-
ond derivative with respect to η2. In general, we have to ﬁnd out whether there exist
surfaces such that the equation effectively has no second derivatives in the direction
normal to them. For this purpose, let  be a surface (our candidate for a charac-
teristic surface) described parametrically as φ1(x1, x2, . . . , xn) = 0. Consider an
invertible change of variables from (x1, x2, . . . , xn) to (η1, η2, . . . , ηn) given by
ηi = φi(x1, x2, . . . , xn)
i = 1, 2, . . . , n.
To express the principal part (9.11) in terms of the new variables, let us write
u(x1, x2, . . . , xn) = w(φ1(x1, x2, . . . , xn), . . . , φn(x1, x2, . . . , xn)).
We thus obtain
L0 =
n

i, j=1
αi j
∂2w
∂ηi∂η j
,
αi j =
n

k,l=1
akl
∂φi
∂xk
∂φ j
∂xl
.
(9.15)
The condition for  to be a characteristic surface is therefore equivalent to asking
that the coefﬁcient of ∂2w/∂η2
1 should vanish. In other words, the quadratic form

9.3 Classiﬁcation of second-order equations
231
deﬁned by the matrix A should vanish for the vector ⃗∇xφ1:
α11 = 0 ⇒( ⃗∇xφ1)tA ⃗∇xφ1 = 0.
(9.16)
There are degenerate cases where one of the variables, say, x1, does not appear
at all in the principal part, namely a1 j = 0 for all j. Obviously, in such a case the
surface x1 = c, for some constant c, is a characteristic surface in the sense deﬁned
above. However this is an “uninteresting” case, since in this case we cannot provide
the ﬁrst derivative with respect to x1 on , and thus the Cauchy problem should be
reformulated. We therefore deﬁne the classiﬁcation of second-order equations in
the following way:
Deﬁnition 9.6 A PDE is called elliptic if it has no characteristic surfaces; it is
called parabolic if there exists a coordinate system, such that at least one of the
independent variables does not appear at all in the principal part of the operator,
and the principal part is elliptic relative to the variables that do appear in it; all other
equations are called hyperbolic.
Let us reexamine the transformation (9.15) for the principal part. If the prin-
cipal part has no mixed derivatives we shall say that it is a canonical form.
Notice that this deﬁnition is somehow different from the one we introduced in
Chapter 3, but, in fact, it is equivalent to it. We saw in Chapter 3 that in addition
to the classiﬁcation scheme, any equation can be transformed to an appropriate
canonical form. In the elliptic case, for instance, we converted the principal part
into the form ∂2/∂x2
1 + ∂2/∂x2
2, while in the hyperbolic case the principal part was
converted into the form ∂2/∂x2
1 −∂2/∂x2
2. It is remarkable that while the classiﬁca-
tion we just described is valid in any dimension, it is not always possible to convert
a given equation into a canonical form. The reason is basically combinatoric. A
transformation into a canonical form requires equating all the mixed derivatives
to zero. However as the dimension grows linearly, the number of mixed deriva-
tives grows quadratically. Thus, when the dimension is 3, we have three functions
φi, i = 1, 2, 3 at our disposal to set three terms (the three mixed derivatives) to
zero. In dimension 4, however, the mission is, in general, impossible, since we are
to set to zero the six coefﬁcients of the mixed derivatives using only four degrees
of freedom (φi, i = 1, 2, 3, 4). The surplus of equations over unknowns becomes
even worse with increasing dimension.
Fortunately, in the special but frequent case of equations with constant coefﬁ-
cients we can transform the equation into a canonical form regardless of the dimen-
sion. To consider this case in some detail, we assume that A is a constant matrix.
Notice that the principal part is, in fact, expressed as a quadratic form relative to A.
To study the quadratic form observe that since A is symmetric it is diagonalizable.

232
Equations in high dimensions
We thus write
QtAQ = D =


λ1
0
·
·
·
0
0
λ2
0
·
·
0
·
·
·
·
·
·
0
·
·
·
0
λn

,
(9.17)
where Q is the diagonalizing matrix of A, and {λi} are the real eigenvalues of A.
The classiﬁcation scheme we introduced earlier can now be readily implemented
with respect to the quadratic form (9.17). For example, an equation is elliptic if the
quadratic form is strictly positive or strictly negative. More generally, we write the
full classiﬁcation scheme in terms of the spectrum of A.
Deﬁnition 9.7 LetAbethe(constant)matrixformingtheprincipalpartofasecond-
order PDE. The equation is called hyperbolic if at least one of the eigenvalues is
positive and one is negative; it is called elliptic if all the eigenvalues are of the same
sign; it is called parabolic if at least one eigenvalue vanishes, and all the eigenvalues
that do not vanish are of the same sign.
The spectral decomposition induced by Q provides us with a natural tool for trans-
forming the principal part to a canonical form. Denote the ith column of Q by ⃗qi,
and deﬁne the canonical variables
⃗ξi = ⃗qi
t · ⃗x.
(9.18)
The new variables satisfy ⃗∇ξ = Qt ⃗∇x; thus it follows from (9.15) that the principal
part relative to the variables ξ takes the form
L0[u] =
n

i=1
λi
∂2v
∂ξ 2
i
,
where we used the notation
u(x1, x2, . . . , xn) = v(ξ1(x1, x2, . . . , xn), . . . , ξn(x1, x2, . . . , xn)).
Example 9.8 Consider the Poisson equation in R3:
u = ux1x1 + ux2x2 + ux3x3 = F(x1, x2, x3).
The matrix A corresponding to the principal part is the identity matrix in R3.
Therefore the equation is elliptic. Alternatively, using (9.16) the equation for the
characteristic surface is
φ2
x1 + φ2
x2 + φ2
x3 = 0.
Clearly, this equation has no nontrivial solution.

9.3 Classiﬁcation of second-order equations
233
Example 9.9 The heat equation in a three-dimensional spatial domain is given by
ut = ku.
The variable t does not show up at all in the principal part, while the reduction
of the principal part to the other variables (x1, x2, x3) is elliptic according to the
previous example. Thus the equation is parabolic.
Example 9.10 The Klein–Gordon equation for a function u(x1, x2, x3, t) in four-
dimensional space-time has the form
utt −c2(ux1x1 + ux2x2 + ux3x3) = V (x1, x2, x3, u).
(9.19)
This is one of the fundamental equations of mathematical physics. Although the
equation is nonlinear, we shall classify it according to the criteria we developed
above, since the principal part is linear, and it is this part that determines the nature
of the equation. The matrix associated with the principal part is
A =


−c2
0
0
0
0
−c2
0
0
0
0
−c2
0
0
0
0
1

.
(9.20)
Therefore the equation is hyperbolic. The equation for the characteristic surfaces
is
φ2
t = c2(φ2
x1 + φ2
x2 + φ2
x3).
(9.21)
This is a generalization of the eikonal equation that we discussed in Chapters 1
and 2. As a matter of fact, we are interested in the level sets φ = constant. If we
write the level sets as
ωt = kS(x1, x2, x3),
we ﬁnd that S satisﬁes the same eikonal equation derived in Chapter 1. We point
out, though, that there is a fundamental difference between the derivation of the
eikonal equation in Chapter 1 and the one given in this chapter. In Chapter 1 we
derived the eikonal equation as an asymptotic limit for large wave numbers; here,
on the other hand, we obtained it as the exact equation for the characteristic surfaces
of the wave operator!
Example 9.11 In dimension 4 or more there exist equations of types that we have
not (fortunately...) encountered yet. For example, consider the equation
ux1x1 + ux2x2 −ux3x3 −ux4x4 = 0.
Heuristically speaking, this is a wave equation where the dimension of “time” is 2!

234
Equations in high dimensions
9.4 The wave equation in R2 and R3
We developed in Chapter 4 the d’Alembert formula for the solution u(x, t) of
the wave equation in dimensions 1 + 1 (i.e. one space dimension and one time
dimension). We also studied the way in which waves propagate according to this
formula. In particular we observed two basic phenomena:
(1) Suppose that the initial data u(x, 0) have a compact support. Then the support propagates
with the speed of the wave while preserving its initial shape. This seems to contradict
our daily experience that indicates that waves decay as they propagate.
(2) When the initial velocity ut(x, 0) is different from zero we observed an even more
bizarre effect. Suppose that ut(x, 0) is compactly supported, and assume for simplicity
that u(x, 0) = 0. Let x0 be an arbitrary point along the x axis. Denote by l the distance
between x0 and the farthest point in the support of ut(x, 0). D’Alembert’s formula
implies that u(x0, t) = (1/2c)
 ∞
−∞ut(x, 0)dx for all t > l/c, where c is the speed of
the wave. Had we been living in a world in which sound waves behaved in this manner,
we would be subjected to an unbearable noise!
Our experience shows, however, that there are here and there calm places
and quiet moments in our turbulent world. Therefore the waves described by
d’Alembert’s formula do not provide a realistic description of actual waves. It
turns out that the source of the difﬁculty is in the reduction to one space dimension.
We shall demonstrate in this section that the wave equation in three space dimen-
sions does not suffer from any of the difﬁculties we just pointed out. It is remarkable
that three is a magical number in this respect. It is the only (!) dimension in which
waves propagate while maintaining their original shape on the one hand, but decay
in amplitude and do not leave a trace behind them on the other hand. In other words,
it is the only dimension in which it is possible to use waves to transmit meaningful
information. Is it a coincidence that we happen to live in such a world?
9.4.1 Radially symmetric solutions
The case of radially symmetric problems in dimension 3 + 1 turns out to be partic-
ularly simple. We seek solutions u(x1, x2, x3, t) to the wave equation
utt −c2u = 0
(x1, x2, x3) ∈R3, −∞< t < ∞,
(9.22)
that are of the form u = u(r, t), where r =
(
x2
1 + x2
2 + x2
3. In Exercise 9.4 the
reader will show that the radial part of the Laplace operator in three dimensions is
∂2
∂r2 + 2
r
∂
∂r

9.4 The wave equation in R2 and R3
235
(see also Subsection A.5). Thus u(r, t) satisﬁes the equation
utt −c2
	∂2u
∂r2 + 2
r
∂u
∂r

= 0.
(9.23)
Deﬁning v(r, t) = ru(r, t), we observe that v satisﬁes vtt −c2vrr = 0. This is ex-
actly the one-dimensional wave equation! Therefore the general radial solution for
(9.23) can be written as
u(r, t) = 1
r [F(r + ct) + G(r −ct)].
(9.24)
Moreover, we can use the same strategy to solve the Cauchy problem that consists
of (9.23) for t > 0 and the initial conditions
u(r, 0) = f (r),
ut(r, 0) = g(r)
0 ≤r ≤∞.
(9.25)
In light of the equation for the auxiliary function v that we deﬁned above, we can use
d’Alembert’s formula to write down an explicit solution for u. There is one obstacle,
though; the initial conditions are only given along the ray r ≥0, and not for all
values ofr. To resolve this difﬁculty we observe that if a radial function h(r) is of the
class C1, then it must satisfy h′(0) = 0. In order for u to be a classical solution of the
problem we shall assume that indeed f and g are continuously differentiable. Thus
f ′(0) = g′(0) = 0. We can therefore apply the method we introduced in Chapter 4
(see Exercise 4.4) to solve the one-dimensional wave equation over the ray r > 0.
For this purpose we extend f and g to the whole line −∞< r < ∞by deﬁning
them to be the even extensions of the given f and g. Hence, the initial conditions
for v are odd functions, and therefore the solution v(r, t) is odd, which implies that
u is an even function. We thus obtain the following radially symmetric solution for
the three-dimensional (radial) wave equation:
u(r, t) = 1
2r

(r + ct) ˜f (r + ct) + (r −ct) ˜f (r −ct)

+ 1
2cr
 r+ct
r−ct
s ˜g(s)ds,
(9.26)
where ˜f and ˜g are the even extensions of f and g, respectively.
In spite of the similarity between (9.26) and the one-dimensional d’Alembert
formula that was introduced in Chapter 4, they are, in fact, quite different from each
other. Let us consider a few examples to demonstrate these differences.
Example 9.12 Let u(r, t) be the radial solution to the Cauchy problem (9.22) for
c = 1 and the initial conditions
u(r, 0) = 0,
ut(r, 0) =
1
r ≤1,
0
r > 1.
Compute u(2, 1
2), u(2, 3
2), and u(2, 4).

236
Equations in high dimensions
Substituting the problem’s data into (9.26) and using the even extension principle
of the initial data, we obtain:
u(2, 1
2) = 1
4
 5/2
3/2
s ˜g(s)ds = 0,
u(2, 3
2) = 1
4
 7/2
1/2
s ˜g(s)ds = 1
4
 1
1/2
sds =
3
32,
u(2, 4) = 1
4
 6
−2
s ˜g(s)ds = 1
4
 1
−1
sds = 0.
More generally, one can verify that for short time intervals the perturbation originat-
ing in the domain r ≤1 does not inﬂuence the sphere r = 2 at all. After one unit of
time the perturbation does reach that sphere, and after two units of time it reaches
its maximum there. After this time the wave on the sphere r = 2 decays, and it
vanishes completely after some ﬁnite time. This picture should be contrasted with
the one-dimensional case, where we saw that the inﬂuence of initial data consisting
of a compactly supported wave’s speed never disappears.
Example 9.13 Let u(r, t) be the radial solution of the Cauchy problem (9.22) with
c = 1 and the initial data
u(r, 0) = f (r) =
1
r ≤1
0
r > 1,
ut(r, 0) = 0.
Let us compute u(r, t) for a sphere of radius r > 1. We obtain
u(r, t) = 1
2r (r −t) ˜f (r −t).
Notice that the solution is zero outside the shell t −1 ≤r ≤t + 1; moreover,
max{r>0} |u(r, t)| decays like 1/r (see Figure 9.1).
9.4.2 The Cauchy problem for the wave equations in three-dimensional space
Consider the general Cauchy problem in 3 + 1 dimensions consisting of
utt −c2u = 0
(x1, x2, x3) ∈R3, 0 < t < ∞,
(9.27)
together with the initial conditions
u(x1, x2, x3, 0)= f (x1, x2, x3), ut(x1, x2, x3, 0)=g(x1, x2, x3)
(x1, x2, x3)∈R3.
(9.28)
Weshallﬁrstshowthatitisenoughtosolveasimplerprobleminwhich f (⃗x) ≡0.
This simpliﬁcation is a consequence of the following claim and the superposition
principle.

9.4 The wave equation in R2 and R3
237
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
r
u
t = 0
t = 0.3 
t = 0.85 
Figure 9.1 The solution of the problem in Example 9.13 for t = 0, 0.3, 0.85. We
observethepropagationofthewavetotheright(thedomainr > 1),thereductionof
the amplitude of the forward propagating wave, and the approach to the singularity
at r = 0 that will occur at t = 1.
Proposition 9.14 Let u(⃗x, t) be the solution of the Cauchy problem (9.27)–(9.28)
with the initial data
u(x1, x2, x3, 0) = 0,
ut(x1, x2, x3, 0) = g(x1, x2, x3).
(9.29)
Then v(⃗x, t) := ut(⃗x, t) solves the Cauchy problem (9.27) with the initial data
v(x1, x2, x3, 0) = g(x1, x2, x3),
vt(x1, x2, x3, 0) = 0.
(9.30)
Proof Since (9.27) is an equation with constant coefﬁcients, it is clear that if u is
a solution, then so is v = ut. Hence v solves the Cauchy problem (9.27) with the
initial data
v(x1, x2, x3, 0) = ut(x1, x2, x3, 0) = g(x1, x2, x3),
vt(x1, x2, x3, 0) = utt(x1, x2, x3, 0) = c2uxx(x1, x2, x3, 0) = 0.
□
We shall use an interesting observation due to the French mathematician Gaston
Darboux (1842–1917) to solve the Cauchy problem (9.27) and (9.29). Let h be a
differentiable function in R3. We deﬁne its spherical mean Mh(a) over the sphere

238
Equations in high dimensions
of radius a around the point ⃗x to be
Mh(a, ⃗x) =
1
4πa2

|⃗ξ−⃗x|=a
h(⃗ξ)dsξ.
(9.31)
Darboux discovered that Mh(a, ⃗x) satisﬁes the differential equation
	 ∂2
∂a2 + 2
a
∂
∂a

Mh(a, ⃗x) = x Mh(a, ⃗x).
(9.32)
For an obvious reason we name this equation after Darboux himself. We leave the
derivation of the Darboux equation as an exercise (see Exercise 9.7).
Considering (9.31) as a transformation h →Mh, we notice that the inverse trans-
formation is obvious:
h(⃗x) = Mh(0, ⃗x).
(9.33)
Just as in the previous subsection we shall construct the even extension of Mh to
negative values of a, such that the extended function is smooth. We thus require
∂
∂a Mh(0, ⃗x) = 0.
(9.34)
This extension conforms with the deﬁnition of Mh: write Mh as
Mh(a, ⃗x) = 1
4π

|⃗η|=1
h(⃗x + a⃗η)dsη,
(9.35)
where we have applied the change of variables ⃗ξ = ⃗x + a⃗η, and ⃗η varies over
the unit sphere. The symmetry of the unit sphere now implies that Mh is an even
function of a.
Equations (9.33) and (9.34) provide “initial” conditions for the Darboux equa-
tion. To connect the notion of spherical means and the wave equation, set Mu to be
the spherical mean of u(⃗x, t), where u is the solution of the Cauchy problem (9.27)
and (9.29). We prove the following statement.
Proposition 9.15 Mu(a, ⃗x, t)satisﬁestheradiallysymmetricwaveequation(9.23).
Proof The Darboux equation, the representation (9.31) and the wave equation
imply
c2
	 ∂2
∂a2 + 2
a
∂
∂a

Mu(a, ⃗x, t) = c2x Mu(a, ⃗x, t)
= 1
4π

|⃗η|=1
c2xu(⃗x + a⃗η)dsη = ∂2
∂t2 Mu(a, ⃗x, t).

9.4 The wave equation in R2 and R3
239
Notice that the variables (x1, x2, x3) are merely parameters in this equation. The
initial conditions are
Mu(a, ⃗x, 0) = M f (a, ⃗x) = 0,
∂
∂t Mu(a, ⃗x, 0) = Mg(a, ⃗x).
□
Using the formula we derived in the previous subsection for the radial solution
of the wave equation in three space dimensions we infer
Mu(a, ⃗x, t) =
1
2ca
 a+ct
a−ct
sMg(s, ⃗x)ds =
1
2ca
 ct+a
ct−a
sMg(s, ⃗x)ds,
(9.36)
where in the last equality we used the evenness of M. To eliminate u(⃗x, t) we let a
approach zero in (9.36). We obtain
u(⃗x, t) = t Mg(ct, ⃗x).
(9.37)
Thanks to formula (9.37) and to Proposition 9.14, we can now write a formula
for the general solution of the Cauchy problem:
u(⃗x, t) = t Mg(ct, ⃗x) + ∂
∂t [t M f (ct, ⃗x)],
(9.38)
or, upon substituting the formula for the spherical means,
u(⃗x, t) =
1
4πc2t

|⃗ξ−⃗x|=ct
g(⃗ξ)dsξ + ∂
∂t

1
4πc2t

|⃗ξ−⃗x|=ct
f (⃗ξ)dsξ

.
(9.39)
To understand the signiﬁcance of the representation (9.39) we shall analyze
separately the contributions of f and of g. Assume ﬁrst that both f and g are
compactly supported. The contribution to the ﬁrst term in (9.39) is only from the
sphere |⃗x −⃗ξ| = ct. Let ⃗x be outside the support of g. For sufﬁciently small times
there is no contribution to the solution at ⃗x, since the sphere is fully outside the
support. There is a ﬁrst time t0 at which the sphere |⃗x −⃗ξ| = ct intersects the
support of g. Then we shall, in general, get a contribution to u(⃗x). On the other
hand, when t is sufﬁciently large, the sphere |⃗x −⃗ξ| = ct has expanded so much
that it no longer intersects the support of g, and from that time on g will have
no impact on the value of u(⃗x). This behavior is in marked contrast to the bizarre
phenomenon we mentioned above in the one-dimensional case. The contribution
of f to the solution at a point ⃗x outside the support of f is also felt only after an
initial time period (the distance between ⃗x and the support of f divided by c), and,
here, too, the perturbation proceeds without leaving a trace in ⃗x. Hadamard called
such a phenomenon Huygens’ principle in the narrow sense. Huygens’ principle is
graphically depicted in Figure 9.2.

240
Equations in high dimensions
x
→
x
→ 
−  x
Æ
 = ct1
x
Æ
−x
→
 = ct2
x
→
−x
→
 = ct3
support of g
→
Figure 9.2 Wave propagation in R3.
Another feature of the wave equation that distinguishes the three-dimensional
case from the one-dimensional case is the loss of regularity. We proved in Chapter 4
that if the initial data satisfy f ∈C2 and g∈C1, then the solution is classical, namely,
u ∈C2. Moreover, even when the initial data do have singular points, such as points
of nondifferentiability, or even discontinuity, the solution is singular in exactly
the same way, and the singularity propagates (while preserving its nature) along
the characteristic curves. The situation is different in the three-dimensional case,
as smooth initial data might develop singularities in ﬁnite time. To analyze this
phenomenon, let us reexamine the radial case. We assume that the initial condition
g vanishes identically and compute the solution at the origin. In considering the
limit r →0 in (9.26) we recall that f is an even function, and, therefore, f ′ is odd.
We obtain
u(0, t) = f (ct) + ct f ′(ct),
(9.40)
Indeed the expression for u(0, t) depends not only on f itself, but also on its
derivative. Therefore even if f ∈C2, the solution may not be classical at the origin.
Moreover, if f has discontinuities, the solution may be even unbounded. For ex-
ample, let us look again at Example 9.13. Formula (9.40) implies that the solution
blows up at t = 1, which is exactly the time it takes the singularity to travel from its
original location r = 1 to the origin. The reason behind the spontaneous creation
of singularities is geometric. The initial data in Example 9.13 are discontinuous on
the unit sphere. As the wave propagates towards the origin (cf. Figure 9.1) it shrinks
until it collapses at the origin to a point. In other words, the singularity that started
its life as a two-dimensional object (a sphere) later turned into a zero-dimensional

9.4 The wave equation in R2 and R3
241
object (a point). This shrinking implies that the singularity concentrates (or focuses)
and its nature worsens.
9.4.3 The Cauchy problem for the wave equation in two-dimensional space
Equipped with the solution of the wave equation in n spatial dimensions, we can
solve the equation in a smaller number of dimensions by freezing one of the vari-
ables.Weshallnowdemonstratethismethod,calledHadamard’smethodofdescent,
to derive a formula for the solution of the Cauchy problem for the wave equation
in 2 + 1 dimensions:
vtt −c2(vx1x1 + vx2x2) = 0
(x1, x2) ∈R2, t > 0,
(9.41)
v(x1, x2, 0) = f (x1, x2),
vt(x1, x2, 0) = g(x1, x2)
(x1, x2) ∈R2. (9.42)
Wesubstitutetheinitialconditionsinto(9.39).Sincetheproblemdoesnotdepend
on the variable x3, we shall evaluate the solution at a point (x1, x2, 0) in the (x1, x2)
plane. To compute the surface integral we use the relation
ξ3 =

(ct)2 −(ξ1 −x1)2 −(ξ2 −x2)2
to express the integral in terms of (ξ1, ξ2), where these two variables vary over the
disk (ξ1 −x)2 + (ξ2 −x)2 ≤(ct)2, i.e. the projection of the sphere over the plane.
Note that the point (ξ1, ξ2, −ξ3) contributes to the integral the same as (ξ1, ξ2, ξ3).
Using the formula dsξ = |ct/ξ3|dξ1dξ2 for the surface element of the sphere, ex-
pressed in Cartesian coordinates, we obtain
v(x1, x2, 0, t) =
1
2πc

r≤ct
g(ξ1, ξ2)

(ct)2 −r2 dξ1dξ2
+ ∂
∂t
&
1
2πc

r≤ct
f (ξ1, ξ2)

(ct)2 −r2 dξ1dξ2
'
,
(9.43)
where
we
have
written
r =

(ξ1 −x1)2 + (ξ2 −x2)2.
By
construction,
v(x1, x2, 0, t) is a solutions the Cauchy problem in the plane. Thus we can omit
x3 from the list of variables of v.
There is a fundamental difference between the solution in two dimensions (9.43)
and that in three dimensions (9.39). In the former case the integration is over a
planar domain, while in the latter case the integration is over a boundary of a three-
dimensional domain. Therefore, in the two-dimensional case even if the initial data
have a compact support, once the initial perturbation has reached a planar point ⃗x
outside the support, it will leave some trace there for all later times, since if t2 > t1,

242
Equations in high dimensions
then the domain of integration at time t2 includes the domain of integration at
time t1.
9.5 The eigenvalue problem for the Laplace equation
We have applied in Chapters 5–7 the method of separation of variables to solve
a variety of canonical problems. The basic tool employed in this method was the
solution of an appropriate eigenvalue problem. For example, when we dealt with
equations with constant coefﬁcients, we typically solved Sturm–Liouville problems
like (5.9)–(5.10). The main difference between the method of separation of variables
for equations in two variables and equations in more than two variables is that in
the latter case the eigenvalue problem itself might be a PDE. We point out that the
method is not circular. The PDE one needs to solve as part of the eigenvalue problem
is of a lower dimension (it involves a smaller number of variables), and thus is
simpler,thantheunderlyingPDEwearesolving.SincewecansolvePDEsexplicitly
only for a small number of simple canonical domains, and since these domains occur
in a large variety of canonical problems, we shall limit the discussion to rectangles,
prisms, disks, balls and cylinders. Nevertheless, we start with a general discussion
on the eigenvalues of the Laplace operator that applies to any smooth domain.
Let  be a bounded domain in R2 or in R3. We deﬁne the following inner product
in the space of continuous functions in ¯:
⟨u, v⟩=


u(⃗x)v(⃗x) d⃗x.
(9.44)
The following problem generalizes the Sturm–Liouville problem (5.9)–(5.10):
−u = λu
⃗x ∈,
(9.45)
u = 0
⃗x ∈∂.
(9.46)
We call this problem a Dirichlet eigenvalue problem. The set of eigenvalues λ is
called the spectrum of the Dirichlet problem. It can be shown that under certain
smoothness assumptions on the domain , there exists a discrete inﬁnite sequence
of eigenvalues {λn} and eigenfunctions {un(⃗x)} solving (9.45)–(9.46). We show in
the next subsection that many of the properties we presented in Chapter 6 for the
eigenvaluesandeigenfunctionsoftheSturm–Liouvilleproblemarealsovalidforthe
problem (9.45)–(9.46). We then proceed to compute the spectrum of the Laplacian
in several canonical domains. One can similarly formulate the eigenvalue problem
for the Laplace operator under the Neumann boundary condition
∂nu = 0
⃗x ∈∂,
(9.47)
or even for the problem of the third kind.

9.5 The eigenvalue problem for the Laplace equation
243
9.5.1 Properties of the eigenfunctions and eigenvalues
of the Dirichlet problem
We review the ten properties that were presented in Chapter 6 for the Sturm–
Liouville problem, and examine the analogous properties in the case of (9.45)–
(9.46). We assume throughout that  is a sufﬁciently smooth (bounded) domain
such that the eigenfunctions belong the class C2( ¯) there. We also refer to the scalar
product deﬁned in (9.44).
1 Symmetry Using an integration by parts (Green’s formula) we see that for any
two functions satisfying the Dirichlet boundary conditions


vu d⃗x = −


⃗∇v · ⃗∇u d⃗x =


uv d⃗x.
This veriﬁes the symmetry of the Laplace operator.
2 Orthogonality
Proposition 9.16 Eigenfunctions associated with different eigenvalues are orthog-
onal to each other.
Proof Let vn, vm be two eigenfunctions associated with the eigenvalues λn ̸= λm,
respectively; namely,
−vn = λnvn,
(9.48)
−vm = λmvm.
(9.49)
The symmetry property implies
(λn −λm)


vnvm d⃗x = 0,
hence the orthogonality.
□
3 The eigenvalues are real The proof is the same as the proof of Proposition 6.21.
4 The eigenfunctions are real Here the claim is identical to Proposition 6.22 and
the related discussion in Chapter 6.
5 Multiplicity of the eigenvalues One of the main differences between the one-
dimensional Sturm–Liouville problem and the multi-dimensional case we consider
here involves multiplicity. In the multi-dimensional case (9.45)–(9.46) the multi-
plicity might be larger than 1 (but it is always ﬁnite!). This fact is of great physical
signiﬁcance. We shall demonstrate this property in the sequel through speciﬁc
examples.

244
Equations in high dimensions
6 There exists a sequence of eigenvalues converging to ∞We formulate the
following proposition.
Proposition 9.17 (a) The set of eigenvalues for the problem (9.45)–(9.46) consists
of a monotone nondecreasing sequence converging to ∞.
(b) The eigenvalues are all positive and have ﬁnite multiplicity.
Proof We only prove the statement that all the eigenvalues are positive. In the
process of doing so, we shall discover an important formula for the characterization
of the eigenvalues. Multiply (9.45) by u and integrate by parts over . We obtain
λ =

 |∇u|2 d⃗x

 u2 d⃗x
.
(9.50)
Since the function u = constant is not an eigenfunction, it follows that λ > 0.
□
7 Generalized Fourier series Let {λn} be the eigenvalue sequence for the Dirichlet
problem, written in a nondecreasing order. Denote by Vn the subspace spanned by
the eigenfunctions associated with the eigenvalue λn. We have shown that eigen-
functions belonging to different subspaces Vn are orthogonal to each other. We now
select for each eigenspace Vn an orthonormal basis. We have thus constructed an
orthonormal set of eigenfunctions {vn(⃗x)}. It is known that the sequence is complete
with respect to the norm induced by the inner product (9.44). Thus we can formally
expand smooth functions deﬁned in  into a generalized Fourier series
f (⃗x) =
∞

m=0
αmvm(⃗x).
(9.51)
Due to the completeness of the orthonormal system {vm}, the series is converging
on average, and the generalized Fourier coefﬁcients are given by
αm = ⟨f (⃗x), vm(⃗x)⟩.
(9.52)
We shall demonstrate several such Fourier expansions in the next few subsections,
although we shall not analyze their convergence in detail.
8 An optimization problem for the ﬁrst eigenfunction We developed in (9.50)
an integral formula for the eigenvalues. Denote the smallest eigenvalue (called the
principal eigenvalue) by λ0.Using a proof that is similar to that for Proposition 6.37,
the following proposition can be shown.
Proposition 9.18 The Rayleigh–Ritz formula
λ0 = inf
v∈V

 |∇v|2dx

 v2dx
,
(9.53)

9.5 The eigenvalue problem for the Laplace equation
245
where
V = {v ∈C2() ∩C( ¯) | v ̸= 0, v |∂ = 0}.
Moreover, λ0 is a simple eigenvalue, and the inﬁmum is only achieved for the
associated eigenfunction.
9 Zeros of the eigenfunctions The zero set of a scalar function is generically a
codimension one manifold (lines in the plane; surfaces in space). These sets are
called nodal surfaces. The nodal surfaces can take quite intricate shapes. An in-
teresting application of the shape of the nodal surfaces of the eigenfunctions for
the Laplace operator is in the theory of Turing instability. This theory, proposed by
the British mathematician Alan Mathison Turing (1912–1954) explains the spon-
taneous creation of patterns in chemical and biological systems. It is argued, for
example, that the speciﬁc patterns of the zebra’s stripes or the giraffe’s spots can
be explained with the aid of the nodal surfaces of certain eigenfunctions of the
Laplacian [12].
10 Asymptotic behavior of the eigenvalues λn when n →∞It can be shown
in analogy to formula (6.76) that for  ⊆R j the nth eigenvalue associated with
(9.45)–(9.46) has the following asymptotic behavior in the limit n →∞:
λn ∼4π2
	
n
ω j||

 2
j
j = 1, 2, 3 . . . .
(9.54)
This formula is called Weyl’s asymptotic formula. We have used here the notation ω j
to denote the volume of the unit ball in R j. For example, ω1 = 2, ω2 = π, w3 =
4π/3.
9.5.2 The eigenvalue problem in a rectangle
Let  be the rectangle {0 < x < a, 0 < y < b}. We want to compute the eigen-
values of the Laplace operator in :
uxx + uyy = −λu
0 < x < a, 0 < y < b,
u(0, y) = 0, u(a, y) = 0
0 < y < b,
u(x, 0) = 0, u(x, b) = 0
0 < x < a.
(9.55)
We use the symmetry of the rectangle to construct separable solutions of the form
u(x, y) = X(x)Y(y). We obtain two Sturm–Liouville problems
Y ′′(y) + µY(y) = 0,
(9.56)
Y(0) = Y(b) = 0,
(9.57)

246
Equations in high dimensions
1
0.5
0
−0.5
−1
1
0.8
0.6
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1
Figure 9.3 The (7,2) mode u7,2(x, y) = sin(7πx) sin(2πy).
and
X′′(x) + (λ −µ)X(x) = 0,
(9.58)
X(0) = X(a) = 0.
(9.59)
We have already solved such systems in Chapter 6. The solutions are
λn,m = π2
	n2
a2 + m2
b2

m, n = 1, 2, . . . , (9.60)
un,m(x, y) = Xn(x)Ym(y) = sin nπx
a
sin mπy
b
m, n = 1, 2, . . . .
(9.61)
The graph of u7,2 is depicted in Figure 9.3. Notice that the eigenvalue µ that appears
in (9.56)–(9.59) is merely a tool in the computation, and it does not show up in the
ﬁnal answer.
The generalized Fourier expansion of a function in two variables f (x, y) in the
rectangle  by the system {un,m} can be written as
f (x, y) =
∞

n,m=1
An,m sin nπx
a
sin mπy
b
,
(9.62)
where the generalized Fourier coefﬁcients are given by
An,m = 4
ab


f (x, y) sin nπx
a
sin mπy
b
dxdy.
(9.63)

9.5 The eigenvalue problem for the Laplace equation
247
It is straightforward to ﬁnd the corresponding eigenvalues and eigenfunctions for
the Neumann problem in a rectangle. This is left to the reader as an exercise.
One of the important issues in the analysis of eigenvalues is their multiplicity. We
saw in Chapter 6 that all the eigenvalues in a regular Sturm–Liouville problem are
simple. In higher dimensions, though, some eigenvalues might have a multiplicity
larger than 1. When this happens, we say that the problem has degenerate states.
We prove now that the eigenvalue problem for Laplace equation in the unit square
is degenerate.
Proposition 9.19 There are inﬁnitely many eigenvalues for the Dirichlet problem
in the unit square that are not simple.
Proof An eigenvalue λ is degenerate if there are two different pairs of positive
integers (m, n) and (p, q) such that
p2 + q2 = m2 + n2.
Equations of this type appear frequently in number theory, where they are called
Diophantic equations. To prove that this Diophantic equation has inﬁnitely many
two pairs of solutions we choose p = m + 1. The equation takes the form
2m + 1 = n2 −q2,
namely, there exists a solution for each choice of n and q, provided they have a
different parity. There also exist ‘trivial’ solutions such as (m, n) = (q, p); further-
more, if a pair of solutions is multiplied by an integer, one obtains a new solution.
□
9.5.3 The eigenvalue problem in a disk
Let  be the disk {0 ≤r < a, 0 ≤θ ≤2π}. We want to compute the eigenvalues
and eigenfunctions of the Laplace equation there. Using a polar coordinate system
the problem is written as:
urr + 1
r ur + 1
r2 uθθ = −λu
0 < r < a, 0 ≤θ ≤2π,
(9.64)
u(a, θ) = 0
0 ≤θ ≤2π.
(9.65)
Just like in Chapter 7 we construct separable solutions of the form u(r, θ) =
R(r)(θ). We use the standard arguments to obtain two systems of Sturm–Liouville
problems:
′′(θ) + µ(θ) = 0
0 ≤θ ≤2π,
(9.66)
(0) = (2π),
′(0) = ′(2π),
(9.67)

248
Equations in high dimensions
and
R′′(r) + 1
r R′(r) +

λ −µ
r2

R(r) = 0
0 < r < a,
(9.68)
| lim
r→0 R(r)| < ∞,
R(a) = 0.
(9.69)
The solution to (9.66)–(9.67) is (see Chapter 7)
n(θ) = An cos nθ + Bn sin nθ,
µn = n2
n = 0, 1, 2, . . . .
(9.70)
Therefore, the radial problem (9.68)–(9.69) becomes
R′′(r)+ 1
r R′(r)+
	
λ−n2
r2

R(r)=0
0 < r < a, | lim
r→0 R(r)| < ∞, R(a) = 0.
(9.71)
Applying the change of variables s =
√
λr, (9.71) is transformed into the canonical
form
ψ′′(s) + 1
s ψ′(s) +
	
1 −n2
s2

ψ(s) = 0
0 < s <
√
λa,
(9.72)
together with the boundary conditions
| lim
s→0 ψ(s)| < ∞,
ψ(
√
λa) = 0,
(9.73)
where we write R(r) = ψ(
√
λr). The system (9.72)–(9.73) forms a singular Sturm–
Liouville problem, Indeed, we can also write (9.72) in the form (see Chapter 6, and
in particular (6.24) there):
(sψ′)
′ +
	
s −n2
s

ψ = 0
0 < s <
√
λa.
We call (9.72) a Bessel equation of order n. Equations of this type can be solved
by the Frobenius–Fuchs method (expansion into a power series). It is easy to verify
that the point s = 0 is a regular singular point for all Bessel equations. Moreover,
one of the independent solutions is singular at s = 0, while the other one is regular
there. Since we are looking for regular solutions to (9.64)–(9.65), we shall ignore
the singular solution. The regular solution for the Bessel equation is called the
Bessel function of order n of the ﬁrst kind. It is denoted by Jn in honor of the
German mathematician and astronomer Friedrich Wilhelm Bessel (1784–1846)
who was among the ﬁrst to study these functions. There is also a singular solution
Yn for the Bessel equation that is called the Bessel function of order n of the
second kind. There exist several voluminous books such as [21] summarizing the
rich knowledge accumulated over the years on the many fascinating properties of
Bessel functions.

9.5 The eigenvalue problem for the Laplace equation
249
We list here some of these properties that are of particular relevance to our study
of the eigenvalues in a disk.
(1) For every nonnegative integer n the zeros of the Bessel function Jn form a
sequence of real positive numbers αn,m that diverge to ∞as m →∞.
(2) The difference between two consecutive zeros converges to π in the limit
m →∞. A full proof of this interesting property is difﬁcult; instead we present
the following heuristic argument. For large n the eigenvalues are determined by the
form of the solution for large values of s (since
√
λa ≫1). To estimate the behavior
of the solution ψ of (9.72) at large s, it is useful write ψ = s−1/2χ. A little algebra
shows that χ satisﬁes the equation
χ′′ + χ + s−2
	1
4 −n2

χ = 0.
Therefore we expect that for large argument the Bessel function will be approxi-
mately proportional to s−1/2 cos(s + γ ), where γ is an appropriate constant. It can
be shown that this indeed is the asymptotic behavior of the Bessel functions, and
that γ = −1
2nπ −1
4π, where n is the order of the function. This justiﬁes our claim
about the difference between consecutive zeros.
(3) We pointed out that (9.72) possesses only one solution that is not singular
at the origin. We shall select a certain normalization for that solution. In the case
n = 0 it is convenient to select the normalization J0(0) = 1. When n > 0, however,
it follows from the series expansion of the solution to (9.72) that Jn(0) = 0. We thus
search for another normalization. An elegant way to select a normalization is to
construct an integral representation for Jn. For this purpose consider the differential
equation
v + v = ( + 1)v = 0.
(9.74)
Clearly the function v(y) = eiy = eir sin θ satisﬁes this equation. Let us expand this
function into a classical Fourier series in the variable θ:
eir sin θ =
∞

n=−∞
n(r)einθ.
(9.75)
Operating over (9.75) with  + 1 we ﬁnd
0 =
∞

n=−∞

′′
n + 1
r ′
n +
	
1 −n2
r2



einθ.
Therefore we can identify the coefﬁcients n in (9.75) with the Bessel functions Jn.
The Fourier formulas now provide the important integral representation for Bessel

250
Equations in high dimensions
Figure 9.4 The Bessel functions J0 (solid line) and J1 (dashed line).
functions:
Jn(x) = 1
2π
 2π
0
eix sin θe−inθ dθ.
(9.76)
Indeed the normalization we selected satisﬁes J0(0) = 1. One of the applications
of the integral representation (9.76) is the recursive formula
s Jn+1(s) = nJn(s) −s J ′
n(s).
(9.77)
We leave the proof of the recursive formula to Exercise 9.11. Notice that according
to this formula it is enough to compute J0, and then use this function to evaluate Jn
for n > 0. The Bessel functions J0 and J1 are depicted in Figure 9.4.
(4) The following proposition is particularly useful for the expansion of functions
deﬁned over the disk in terms of Bessel functions.
Proposition 9.20 Let n be a nonnegative integer. Then for all m = 1, 2, . . . we
have
 a
0
r J 2
n
αn,m
a r

dr = a2
2 J 2
n+1(αn,m),
(9.78)
where {αn,m} are the zeros of Jn.

9.5 The eigenvalue problem for the Laplace equation
251
Proof Consider (9.72) for some eigenvalue λn,m. Multiplying the equation by s2J ′
n
and integrating from 0 to

λn,ma, one obtains
 √
λn,ma
0

s J ′
n(s J ′
n)
′ +
	
1 −n2
s2

s2Jn J ′
n

ds = 0.
Some of the terms in the integrand are complete derivatives. Performing the inte-
grations we ﬁnd
2
 √
λn,ma
0
s J 2
n ds = λn,ma2[J ′
n(

λn,ma)]2.
Returning to the variable r we end up with
 s
0
r J 2
n (

λn,mr) dr = a2
2 [J ′
n(

λn,ma)]2.
Observe that if the argument s in the recurrence formula (9.77) is a zero of Jn, then
the formula reduces to J ′
n(s) = Jn+1(s). Since by assumption λn,m is an eigenvalue,
then

λn,ma is indeed a zero of Jn, and the claim follows.
□
The eigenvalues of the Dirichlet problem in a disk are therefore given by the
double index sequence
λn,m =
αn,m
a
2
n = 0, 1, 2, . . . ,
m = 1, 2, . . . ,
(9.79)
while the eigenfunctions are
un,m = Jn
αn,m
a r

(An,m cos nθ +Bn,m sin nθ)
n =0, 1, 2, . . . , m =1, 2, . . . .
(9.80)
This sequence forms a complete orthogonal system for the space of continuous
functions in the disk of radius a with respect to the inner product
⟨f, g⟩=
 2π
0
 a
0
f (r, θ)g(r, θ)r drdθ.
(9.81)
The Fourier–Bessel expansion for a function h(r, θ) over that disk is given by
h(r, θ) =
∞

n=0
∞

m=1
Jn
αn,m
a r

(An,m cos nθ + Bn,m sin nθ),
(9.82)

252
Equations in high dimensions
x
y
r
z
q
f
Figure 9.5 Our notation for the spherical coordinate system.
where according to Proposition 9.20 the Fourier–Bessel coefﬁcients are
An,m =
2
πa2Jn+1(αn,m)
 2π
0
 a
0
h(r, θ)Jn
αn,m
a r

cos nθ r drdθ,
(9.83)
Bn,m =
2
πa2Jn+1(αn,m)
 2π
0
 a
0
h(r, θ)Jn
αn,m
a r

sin nθ r drdθ.
(9.84)
We end this subsection by pointing out that each eigenvalue (except for the case
n = 0) is of multiplicity 2.
9.5.4 The eigenvalue problem in a ball
We solved the eigenvalue problem in a rectangle by writing the rectangle as a
product of two intervals. Similarly we computed the eigenvalues in a disk using the
observation that in polar coordinates the disk too can be written as a product of two
intervals. We thus separated the eigenvalue problem in the disk into one eigenvalue
problem on the unit circle (9.66)–(9.67), and another problem in the radial direction.
Proceeding similarly in the case of the ball we deﬁne a spherical coordinate system
{(r, φ, θ)| r > 0, 0 ≤φ ≤π, 0 ≤θ ≤2π} (see Figure 9.5), given by
x = r sin φ cos θ,
y = r sin φ sin θ,
z = r cos φ.
(9.85)
The reader will compute the Laplace operator in spherical coordinates in
Exercise 9.4. Write
Ba := {0 < r < a, 0 < φ < π, 0 ≤θ ≤2π},
S2 := {0 ≤φ ≤π, 0 ≤θ ≤2π}.

9.5 The eigenvalue problem for the Laplace equation
253
The eigenvalue problem in a ball of radius a is given by
1
r2
∂
∂r
	
r2 ∂u
∂r

+ 1
r2

1
sin φ
∂
∂φ
	
sin φ ∂u
∂φ

+
1
sin2 φ
∂2u
∂θ2

= −λu
(r, φ, θ) ∈Ba,
(9.86)
u(a, θ, φ) = 0
(φ, θ) ∈S2,
(9.87)
plus certain compatibility conditions to be presented later.
Writing u in the separable form u(r, θ, φ) = R(r)Y(φ, θ), we obtain a system of
two eigenvalue problems. One of them, deﬁned over the unit sphere S2, takes the
form
1
sin φ
∂
∂φ
	
sin φ ∂Y
∂φ

+
1
sin2 φ
∂2Y
∂θ2 = −µY
(φ, θ) ∈S2.
(9.88)
Equation (9.88) is subject to two conditions. The ﬁrst condition is that the solution
is periodic with respect to the variable θ:
Y(φ, 0) = Y(φ, 2π), Yθ(φ, 0) = Yθ(φ, 2π).
(9.89)
The other condition is that Y is bounded everywhere on the unit sphere, and, in
particular in the two poles φ = 0 and φ = π, where the coefﬁcients of (9.88) are
not bounded.
The second problem for the radial function R(r) consists of the equation
1
r2
∂
∂r
	
r2 ∂R
∂r

=
 µ
r2 −λ

R
0 < r < a,
(9.90)
the boundary condition
R(a) = 0,
(9.91)
and the requirement that R is bounded at the origin (where (9.90) is singular).
We shall perform an extensive analysis of the eigenvalue problem (9.88)–(9.89).
We seek eigenfunctions Y in a separable form Y(φ, θ) = (φ)(θ). Substituting
this form of Y into (9.88) gives rise to two equations:
′′(θ) + ν(θ) = 0
0 < θ < 2π,
(9.92)
sin φ ∂
∂φ

sin φ ∂(φ)
∂φ

+ (µ sin2 φ −ν)(φ) = 0
0 < φ < π.
(9.93)
The periodicity condition (9.89) implies the following eigenfunctions and eigen-
values for (9.92):
νm = m2,
m(θ) = Am cos mθ + Bm sin mθ
m = 0, 1, 2, . . . .
(9.94)

254
Equations in high dimensions
Substituting the eigenvalues νm into (9.93), performing the change of variables
t = cos φ, using sin φd/dφ = −sin2 φd/dt, and setting P(t) = (φ(t)), we obtain
for P(t) a sequence of eigenvalue problems
(1 −t2) d
dt

(1 −t2)dP
dt

+

(1 −t2)µ −m2
P = 0
−1 < t < 1,
m = 0, 1, 2, . . . .
(9.95)
Equation (9.95) is a linear second-order ODE. It is a regular equation except at the
end points t = ±1 which are regular singular points. We recall that we are looking
for solutions that are bounded everywhere, including the singular points (the poles
of the original unit sphere). It it convenient to consider ﬁrst the case m = 0, and to
proceed later to the cases where m > 0.
When m = 0 we obtain
d
dt

(1 −t2)dP
dt

+ µP = 0
−1 < t < 1.
(9.96)
This equation is called the Legendre equation after the French mathematician
Adrien-Marie Legendre (1752–1833). The problem of ﬁnding bounded solutions
to this equation is called the Legendre eigenvalue problem. It is a singular Sturm–
Liouville problem. Since the eigenvalue problem for the Laplace equation in a ball
is important in many applications such as electromagnetism, quantum mechanics,
gravitation, hydrodynamics, etc., the Legendre equation has been studied exten-
sively. The following property of it is very useful for our purposes.
Proposition 9.21 A solution of the Legendre equation is bounded at the end points
t = ±1 if and only if the eigenvalues are µn = n(n + 1) for n = 0, 1, . . . . More-
over, in this case the solution for µn is a polynomial of degree n that is called the
Legendre polynomial. We denote this polynomial by Pn(t).
Proof We outline the main steps in the proof. The Legendre equation is solved by
the series expansion (Frobenius–Fuchs) method. For example, we expand around
the regular singular point t = 1. The solution takes the form
P = (t −1)γ
∞

k=0
ak(t −1)k.
Substituting the series into the equation, we ﬁnd that the indicial equation for γ is
γ 2 = 0. Thus γ = 0 is a double root. Hence there exists one solution that is regular
at t = 1, while the other solution has a logarithmic singularity there. We observe
that if P(t) is a solution, then P(−t) is a solution too. This implies that also at
t = −1 there is one regular solution and one singular solution. Therefore we need

9.5 The eigenvalue problem for the Laplace equation
255
to check whether the regular solution at t = 1 connects to the regular solution at
t = −1.
For this purpose we compute the recursive formula for the coefﬁcients ak:
ak+1
ak
= µ −k(k + 1)
2(k + 1)2
.
Therefore, if µ is not of the form k(k + 1), the ratio between two consecutive terms
in the series at t = −1 satisﬁes [k(k + 1) −µ]/(k + 1)2 = O(1 −1/k), and thus
the series diverges there, i.e. the regular solution at t = 1 is in fact singular at
t = −1.
It follows that the only way to obtain a solution that is regular at the two end points
is to impose that the series is not inﬁnite but rather terminates at some point and the
solution is then a polynomial. This requires µ = k(k + 1) for some positive integer
k (an alternative proof of this result will be outlined in Exercise 9.13). Furthermore,
the recurrence formula we wrote can be integrated to provide an explicit formula
for the Legendre polynomial (the regular solution is normalized by Pn(1) = 1):
Pn(t) =
n

k=0
(n + k)!
(n −k)!(k!)22k (t −1)k.
(9.97)
For example, the ﬁrst few polynomials are
P0(t) = 1,
(9.98)
P1(t) = t,
(9.99)
P2(t) = 3
2t2 −1
2.
(9.100)
□
Let us return now to the general case in which m > 0. Equation (9.95) is called
the associated Legendre equation of order m. The structure of the eigenvalues and
eigenfunctions of the associated Legendre equation is provided by the following
proposition.
Proposition 9.22 Fix m ∈N. The associated Legendre equation (9.95) has solu-
tions that are bounded everywhere if and only if the eigenvalues µ are of the form
µn = n(n + 1) for n = 0, 1, . . . . Moreover, the eigenfunction Pm
n (t) associated
with such an eigenvalue µn can be expressed as
Pm
n (t) = (1 −t2)m/2 dm Pn
dtm .
(9.101)
Proof We ﬁrst verify that indeed (9.101) satisﬁes (9.95). For this purpose we let
P be some solution of the Legendre equation (9.96). Differentiating the equation

256
Equations in high dimensions
m times we obtain
(1 −t2)dm+2P
dtm+2 −2(m + 1)t dm+1P
dtm+1 + [µ −m(m + 1)] dm P
dtm = 0.
Substituting
L(t) = (1 −t2)m/2 dm P(t)
dtm
,
(9.102)
we observe that L satisﬁes
(1 −t2) d
dt

(1 −t2)dL
dt

+ [(1 −t2)µ −m2]L = 0.
(9.103)
It follows that each solution of the associated Legendre equation is of the form
(9.102). Clearly, if we now select µ = n(n + 1) for a positive integer n, we shall
obtain a solution to (9.95) that is bounded in both end points (since in this case
P is a polynomial). We have thus shown that each function of the form (9.101) is
indeed a valid solution of our problem. It remains to show that there are no further
solutions.
Since each solution of the associated Legendre equation is of the form (9.102),
we have to show that if µ ̸= n(n + 1), then L is singular at least at one end point.
This can be proved by the same method as in the proof of Proposition 9.21; namely,
if µ ̸= n(n + 1), then the solution L(t) that is regular at t = 1 is singular at t = −1,
and the solution that is regular at t = −1 is singular at t = 1.
□
Since Pn is a polynomial of degree n, Pm
n ≡0 for m > n. We have thus estab-
lished that the eigenvalues and eigenfunctions for the problem (9.88)–(9.89) are
given by
µn = n(n + 1)
n =0, 1, . . . , (9.104)
Yn,m(φ, θ)={cos mθ Pm
n (cos φ), sin mθ Pm
n (cos φ)}
n =0, 1, . . . ,
m =0, 1, . . . , n.
(9.105)
In particular, µn is an eigenvalue with a multiplicity n + 1. The functions Yn,m are
called spherical harmonics of order n. They can also be written in a complex form:
Yn,m(φ, θ) = eimθ Pm
n (cos φ)
n = 0, 1, . . . , m = −n, −n + 1, . . . , n −1, n.
We turn our attention to the radial problem (9.90)–(9.91). Fix a nonnegative
integer n. Let us substitute µn = n(n + 1), and R(r) = ρ(r)/√r. Equation (9.90)
now becomes a Bessel equation of order n + 1
2:
ρ′′
n(r) + 1
r ρ′
n(r) +
&
λ −(n + 1
2)2
r2
'
ρn(r) = 0.
(9.106)

9.5 The eigenvalue problem for the Laplace equation
257
Under the change of variables s =
√
λr (see the previous section) we obtain from
the boundary condition (9.91) (similarly to (9.79) and (9.80)) that the radial solution
is of the form
Rn,l(r) =
Jn+ 1
2 (

λn,lr)
√r
,
λn,l =
αn,l
a
2
l = 1, 2, . . . ,
where αn,l denote the zeros of the Bessel function Jn+ 1
2 . We have thus shown that
the eigenfunctions of the Dirichlet problem in the ball are
Un,m,l(r, φ, θ) = 1
√r Jn+ 1
2
αn,lr
a

Yn,m(φ, θ)
n = 0, 1, . . . ,
m = 0, 1, . . . , n, l = 1, 2, . . . ,
(9.107)
while the eigenvalues are
λn,l =
αn,l
a
2
n = 0, 1, . . . , l = 1, 2, . . . .
(9.108)
Two important conclusions stemming from the calculations performed in this
section are worthwhile mentioning.
Corollary 9.23 The eigenvalue problem in a rectangle may or may not be degen-
erate. The eigenvalue problem in a disk is always degenerate, and the multiplicity
is exactly 2. The degeneracy of the eigenvalue problem in the ball is even greater.
For n ≥0 and l ≥1, the eigenvalue λn,l has a multiplicity of 2n + 1, since each
such eigenvalue is associated with 2n + 1 spherical harmonics.
Corollary 9.24 Let Qn(x, y, z) be a homogeneous harmonic polynomial of degree
n in R3, i.e.
Qn(x, y, z) =

p+q+s=n
αp,q,sx pyqzs.
Expressing Qn in the spherical coordinate system we obtain Qn = rnF(φ, θ). If
we substitute Qn into (9.86), we ﬁnd that F is a spherical harmonic of degree n.
Conversely, every function of the form rnYn,m(φ, θ) is a homogeneous harmonic
polynomial (the proof is given as an exercise; see Exercise 9.15). It follows that the
dimension of the space of all homogeneous harmonic polynomials of degree n in
R3 is 2n + 1.
One of the important applications of eigenfunctions is as a means for expanding
functions into generalized Fourier series. For instance, the classical Fourier series
can be derived as an expansion in terms of the eigenfunctions of the Laplacian
on the unit circle. Similarly we use spherical harmonics, i.e. the eigenfunctions of
the Laplacian on the unit sphere S2 ((9.88) and the conditions that followed it), to

258
Equations in high dimensions
expand functions f depending on the spherical variables φ, θ. We thus consider the
space C(S2) of continuous functions over the unit sphere. For each pair of functions
f and g in this space we deﬁne an inner product:
⟨f, g⟩=
 2π
0
 π
0
f (φ, θ)g(φ, θ) sin φ dφdθ.
We write the following expansion for any function f ∈C(S2)
f (φ, θ) =
∞

n=0
1
2 An,0Pn(cos φ) +
n

m=1

An,m cos mθ Pm
n (cos φ)
+ Bn,m sin mθ Pm
n (cos φ)
 /
.
(9.109)
To ﬁnd the coefﬁcients An,m and Bn,m we need to compute the inner product between
each pair of spherical harmonics. From the construction of the spherical harmonics,
and from the general properties of the eigenfunctions of the Laplacian it follows
that different spherical harmonics are orthogonal to each other. It remains to ﬁnd
the norms of the spherical harmonics.
Proposition 9.25 The associated Legendre functions satisfy the identity
 π
0

Pm
n (cos φ)
2 sin φ dφ =
2
2n + 1
(n + m)!
(n −m)!.
(9.110)
The proof is relegated to Exercise 9.17.
We thus obtain the following formulas for the coefﬁcients of the expansion
(9.109):
An,m = (2n + 1)(n −m)!
2π(n + m)!
 2π
0
 π
0
f (φ, θ) cos mθ Pm
n (cos φ) sin φ dφdθ, (9.111)
Bn,m = (2n + 1)(n −m)!
2π(n + m)!
 2π
0
 π
0
f (φ, θ) sin mθ Pm
n (cos φ) sin φ dφdθ.
(9.112)
9.6 Separation of variables for the heat equation
Let  be a bounded domain in Rn, and let u(⃗x, t) be the solution to the heat problem
ut −u = F(⃗x, t)
⃗x ∈,
t > 0,
(9.113)
u(⃗x, t) = 0
⃗x ∈∂,
(9.114)
u(⃗x, 0) = f (⃗x)
⃗x ∈.
(9.115)

9.7 Wave equation: separation of variables
259
Denote by {λm, vm(⃗x)}∞
m=1 the spectrum of the Laplace equation (Dirichlet problem)
in . We solve the problem (9.113)–(9.115) by expanding u, F, and f into a formal
series of eigenfunctions (see (9.51)–(9.52)):
u =
∞

m=1
Tm(t)vm(⃗x),
F =
∞

m=1
Fm(t)vm(⃗x),
f =
∞

m=1
fmvm(⃗x).
(9.116)
Substituting the expansion (9.116) into (9.113)–(9.115) we obtain a system of ODEs
for {Tn(t)}:
T ′
m(t) + λmTm(t) = Fm(t),
T (0) = fm
m = 1, 2, . . . .
(9.117)
Example 9.26 Solve the following heat problem for u(x, y, t):
ut =u
0<x, y <π, t >0, (9.118)
u(0, y, t)=u(π, y, t)=u(x, 0, t)=u(x, π, t)=0
0≤x, y ≤π, t ≥0,
(9.119)
u(x, y, 0)=1
0≤x, y ≤π.
(9.120)
The eigenvalues and eigenfunctions of the Laplacian in this rectangle are given by
{m2 + n2, sin mx sin ny}. Therefore, the solution of (9.118), subject to the boundary
condition (9.119), is of the form:
u(x, y, t) =
∞

n,m=1
An,m sin mx sin ny e−(m2+n2)t .
Substituting the initial conditions and computing the generalized Fourier
coefﬁcients An,m, we obtain
An,m = 8
π2



1
nm
m = 2k + 1, n = 2l + 1,
0
otherwise.
Therefore the solution can be written as
u(x, y, t)= 8
π2
∞

k,l=0
1
(2k+1)(2l+1) sin[(2k+1)x] sin[(2l+1)y]e−[(2k+1)2+(2l+1)2]t .
9.7 Separation of variables for the wave equation
The basic structure of the solution of the wave equation in a bounded do-
main  in Rn is similar to the corresponding solution of the heat equation that
we presented in the preceding section. Let u(⃗x, t) be the solution of the wave

260
Equations in high dimensions
problem
utt −c2u = F(⃗x, t)
⃗x ∈, t > 0,
(9.121)
u(⃗x, t) = 0
⃗x ∈∂, t > 0,
(9.122)
u(⃗x, 0) = f (⃗x), ut(⃗x, 0) = g(⃗x)
⃗x ∈.
(9.123)
Denote again the spectrum of the Dirichlet problem for the Laplace equation in 
by {λm, vm(⃗x)}∞
m=1. We expand the solution, the initial condition, and the forcing
term F into a generalized Fourier series in {vm}, just like in (9.116). Similarly to
(9.117) we obtain
T ′′
m(t) + c2λmTm(t) = Fm(t),
T (0) = fm, T ′(0) = gm
m = 1, 2, . . ..
(9.124)
Example 9.27 Vibration of a circular membrane Denote by u(r, θ, t) the am-
plitude of a membrane with a circular cross section. Then u satisﬁes the following
problem:
∂2u
∂t2 −c2
	∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2

= F(r, θ, t)
0<r <a, 0≤θ ≤2π, t >0,
(9.125)
u(r, θ, 0)= f (r, θ),
∂u
∂t (r, θ, 0)=g(r, θ)
0<r <a, 0≤θ ≤2π,
(9.126)
u(a, θ, t)=0
0≤θ ≤2π, t ≥0.
(9.127)
The system (9.125)–(9.127) models, for example, the vibrations of a drum or of a
trampoline, where the forcing term F is determined by the beating of the drummer
or by the forces exerted by the people jumping on the trampoline.
To solve (9.125)–(9.127) we expand u, F, f , and g into the eigenfunctions of the
Laplaceequationinadisk.Toﬁxideas,weshallconsideraspeciﬁcphysicalproblem
in which the trampoline starts from a horizontal rest position (i.e. f = g = 0), and
the people jumping on it do so rhythmically with a constant frequency, namely,
F(r, θ, t) = F0(r, θ) sin ωt. We use (9.79) and (9.80) to expand
u(r, θ, t) =
∞

n=0
∞

m=1
Jn
αn,m
a r

[An,m(t) cos nθ + Bn,m(t) sin nθ],
(9.128)
F(r, θ, t) = sin ωt
∞

n=0
∞

m=1
Jn
αn,m
a r

(Cn,m cos nθ + Dn,m sin nθ). (9.129)

9.8 Laplace equation: separation of variables
261
Substituting the expansion (9.129) into (9.125) provides a set of ODEs for the
coefﬁcients {An,m, Bn,m}:
A′′
n,m(t) + c2λn,m An,m(t) = Cn,m sin ωt,
An,m(0) = A′
n,m(0) = 0, (9.130)
B′′
n,m(t) + c2λn,m Bn,m(t) = Dn,m sin ωt,
Bn,m(0) = B′
n,m(0) = 0. (9.131)
We obtain
u =
∞

n=0,m=1
(c2λn,m −ω2)−1Jn
αn,m
a r

(Cn,m cos nθ + Dn,m sin nθ)
×

sin ωt −ω sin c

λn,m t
c

λn,m

.
(9.132)
Clearly the solution (9.132) is valid only if we are careful to stay away from the
resonance condition
ω2 −c2λn,m = 0.
(9.133)
Notice that, in general, obtaining an equality between two real numbers is unlikely.
However even if condition (9.133) holds only approximately for some values of
n, m, then one of the terms in the series (9.132) would have a very small denomina-
tor. When the system is in a state of resonance, or near a resonance, the trampoline’s
amplitude becomes very large, and it might collapse. We comment, though, that
when the amplitude is large the linear model (9.125)–(9.127) is no larger valid.
9.8 Separation of variables for the Laplace equation
We present in this section two additional examples for solving the Laplace equation
in multi-dimensional domains.
Example 9.28 Laplace equation in a cylinder Let u be a harmonic function in
a cylinder with radius a and height h that is a solution of the Dirichlet problem
(we employ a cylindrical coordinate system (r, θ, z)):
∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2 + ∂2u
∂z2 = 0
0 < r < a, 0<θ <2π, 0 < z < h,
(9.134)
u(r, θ, 0) = u(r, θ, h) = 0
0≤θ ≤2π, 0 < r < a,
(9.135)
u(a, θ, z) = f (θ, z)
0≤θ ≤2π, 0 < z < h.
(9.136)
The cylinder  is the product of a disk and an interval:
 = {0 < r < a, 0 ≤θ ≤2π} × {0 < z < h}.

262
Equations in high dimensions
We construct accordingly separated solutions of the form u = R(r)(θ)Z(z). We
obtain two eigenvalue problems for Z and for :
Z′′(z) + µZ(z) = 0,
Z(0) = Z(h) = 0,
(9.137)
′′(θ) + ν = 0,
(0) = (2π), ′(0) = ′(2π).
(9.138)
These problems are by now well known to us. The solutions are given by the
sequences
Zn(z) = sin nπz
h ,
µn =
	nπ
h

2
n = 1, 2, . . . , (9.139)
m(θ) = am cos mθ + bm sin mθ,
νm = m2
m = 0, 1, 2, . . . .
(9.140)
The equation for the doubly indexed radial component Rn,m(r) is
R′′
n,m + 1
r R′
n,m −
	m2
r2 + n2

Rn,m = 0.
(9.141)
Under the transformation r →inr (9.141) becomes a Bessel equation. The Bessel
function Jm with a complex argument is often denoted by Im, i.e. Rn,m =
An,m Im(nr). In order to satisfy the Dirichlet condition on the cylinder’s envelope
we write as usual a formal eigenfunction expansion. The structure of the series will
be studied in Exercise 9.18.
Example 9.29 Laplace equation in a ball The Dirichlet problem in a ball Ba =
{(r, φ, θ)| 0 < r < a, 0 ≤φ ≤π, 0 ≤θ ≤2π} is written as
1
r2
∂
∂r
	
r2 ∂u
∂r

+ 1
r2

1
sin φ
∂
∂φ
	
sin φ ∂u
∂φ

+
1
sin2 φ
∂2u
∂θ2

=0
(r, φ, θ) ∈Ba,
(9.142)
u(a, φ, θ)= f (φ, θ)
(φ, θ) ∈∂Ba.
(9.143)
It can easily be checked that the separated solutions for the problem are of the form
u(r, φ, θ) = rnYn,m(φ, θ)
n = 0, 1, . . . , m = 0, 1, . . . ,
where Yn,m are spherical harmonics. Therefore, Corollary 9.24 implies that the
solutionof(9.142)–(9.143)isa(maybeinﬁnite)linearcombinationofhomogeneous
harmonic polynomials. To facilitate the process of eliminating the coefﬁcients of
this linear combination, we write it in the form
u =
∞

n=0
r
a
n
&
An,0
2
Pn(cos φ) +
n

m=1
Pm
n (cos φ)(An,m cos mθ + Bn,m sin mθ)
'
.
(9.144)
The coefﬁcients {An,m, Bn,m} are given by the Fourier formula (9.111)–(9.112).

9.9 Schr¨odinger equation for the hydrogen atom
263
WehavepointedoutonseveraloccasionstheanalogybetweensolvingtheDirich-
let problems in the ball and in the disk. We saw in Chapter 7 that the generalized
Fourier series in a disk can be summed up in the Poisson integral representation.
A similar representation can also be obtained in the case of the ball. Instead of
deriving it as the inﬁnite sum of harmonic polynomials, we shall construct it later
in this chapter using the theory of Green’s functions.
9.9 Schr¨odinger equation for the hydrogen atom
In the second half of the nineteenth century it was already realized by scientists that
the spectrum of the hydrogen atom (and other elements) consists of many discrete
lines (values). The Swiss scientist Johann Jacob Balmer (1825–1898) discovered
in 1885 that many of the spectral lines in the visible part of the spectrum obey the
formula
νk = R(2−2 −k−2),
(9.145)
where νk is the frequency, k = 3, 4, . . . , and R is a constant called the Rydberg
constant. Balmer then made the bold guess that (9.145) is, in fact, a special case of
a more general rule of the form
νk1,k2 = R(k−2
1
−k−2
2 ),
(9.146)
where k1 and k2 are certain integers. Indeed such spectral lines were discovered in
the invisible part of the spectrum! One of the earliest conﬁrmations of the young
quantummechanicsdevelopedbytheGermanphysicistsWernerHeisenberg(1901–
1976) and Max Born (1882–1970) and by Schr¨odinger was the derivation of the
Balmer’s spectral formula from theoretical principles (we point out, though, that
the formula was also derived earlier by Bohr using his ‘old’ formulation of quantum
mechanics).
The hydrogen atom consists of a nucleus and an electron. Therefore, we can
split its motion into a part related to the motion of the center of mass, and a part
related to the relative motion between the electron and the nucleus. We shall ignore
the motion of the center of mass of the atom, and concentrate on the energy levels
resulting from the electric attraction between the nucleus and the electron. The
Schr¨odinger equation for the motion of an electron in an electric ﬁeld generated by
a nucleus with exactly one proton takes the form [17]
−
	 ℏ
2m  + e2
r

u = Eu.
(9.147)
Here ℏis the Planck constant divided by 2π, m is the reduced mass of the atom (i.e.
m = m1m2/(m1 + m2), where m1 is the electron’s mass, and m2 is the nucleus’s

264
Equations in high dimensions
E2
E1
Ephoton = hu = E2 − E1
e
Figure 9.6 A sketch for the emission spectral lines of an atom. As an electron
moves from the energy level E2 to the energy level E1, it emits a photon. The
photon’s energy is determined by the difference E2 −E1. Since the energy levels
are discrete (as will be explained in the text), only discrete values are observed
for the photon’s energy. These photon’s energy values determine the observed
emission spectral lines.
mass; since m2 ≫m1, in practice m ∼m1), r is the distance between the electron
and the center of the nucleus, e is the electron’s charge, E is the energy of the
system, and u(x, y, z) is the wave function. The standard physical interpretation
of the wave function is that if we normalize u such that the integral of its square
over the entire space is 1, then the square of u at any given point is the probability
density for the electron to be at that point.
Equation (9.147) has the general form L[u] = Eu, i.e. it is an eigenvalue prob-
lem. Unlike the eigenvalue problems we have encountered so far, it is not formulated
in a bounded domain with boundary conditions. Rather it is written for the entire
space R3 under the condition that |u|2 is integrable. It turns out that there is a
discrete inﬁnite set of negative eigenvalues. These are the bound states of the hy-
drogen. We remark that there is also a continuous part to the spectrum (see Chapter
6), but we shall not treat it here. The discrete eigenvalues are all negative, and the
continuous part has no negative component. Note also that we keep the equation in
a dimensional form to obtain a comparison between the theoretical results and the
experimental observations mentioned above.
We exploit the radial symmetry of the Schr¨odinger equation (9.147) to seek
solutions in the form u(r, θ, φ) = R(r)Yl,m(θ, φ), where Yl,m are the spherical har-
monics. Substituting this form into (9.147), and using our results on the Laplacian’s
spectrum on the unit sphere, we obtain for R:
(r2R′)
′ + 2mr2
ℏ2
	
E + e2
r

R −n(n + 1)R = 0, n = 0, 1, . . . .
(9.148)
Just as in our discussion of the Laplacian’s spectrum in a disk, it is convenient
to scale the eigenvalue E out of the problem. We thus introduce the new variable
ρ = αr, where α = 8m|E|/ℏ2. Equation (9.148) is now written as
1
ρ2 (ρ2R′)
′ +
λ
ρ −1
4 −n(n + 1)
ρ2

R = 0,
(9.149)

9.9 Schr¨odinger equation for the hydrogen atom
265
where
λ = e2√m
ℏ√2|E|,
(9.150)
and we use Rn = Rn(ρ) = R(ρ), in spite of the mild abuse of notation.
To be able to normalize u, we look only for solutions that decay to zero as
ρ →∞. Since the leading order terms (for large ρ) in (9.149) are R′′ −1
4 R = 0,
we write R as R = Fn(ρ)e−1
2 ρ. We obtain that Fn satisﬁes
F′′
n +
	 2
ρ −1

F′
n +
λ −1
ρ
−n(n + 1)
ρ2

Fn = 0.
(9.151)
Applying the Frobenius–Fuchs theory, the solutions of (9.151) are of the form
Fn(ρ) = ρµn ∞
k=0 an
k ρk. The indicial equation for µn is
µn(µn + 1) −n(n + 1) = 0.
The only solution leading to a wave function that is bounded in the origin is µn = n.
It is convenient to write at this point Fn(ρ) = ρnLn(ρ). Under this transformation,
the function Ln satisﬁes the Laguerre equation
ρL′′
n + [2(n + 1) −ρ] L′
n + (λ −n −1)Ln = 0.
(9.152)
From the analysis above, we know that (9.152) has an analytic solution of the
form Ln(ρ) = ∞
k=0 an
k ρk. Indeed we obtain by substitution a recursive equation
of the form
ak+1 =
k + n + 1 −λ
(k + 1)(k + 2n + 2)ak.
(9.153)
A delicate analysis of the ODE (9.152) shows that the solutions for which the power
series does not terminate after ﬁnitely many terms grow at ∞at the rate of eρ. It
follows that a necessary condition for the normalization of the wave function u is
that the power series will be, in fact, a polynomial.
Anecessaryandsufﬁcientconditionforthepowerseriestoterminateafterﬁnitely
many terms is λ = k + n + 1. We can thus derive from (9.150) the discrete energy
levels:
E j = −me4
2ℏ2 j2
j = 1, 2, . . . .
(9.154)
To convert the formula for the energy levels into the observed spectral lines, we
recall Planck’s quantization rule stating that a wave with frequency ν carries an
energy quanta of hν. Moreover, a hydrogen atom emits radiation when an electron
‘jumps’ from one bound state to another. In the transition process the electron emits
radiation (photons). The frequency of the radiated energy is the difference between

266
Equations in high dimensions
the energies in the states before and after the transition (see Figure 9.6). Therefore
one expects to observe spectral lines with frequencies of the form (9.146) with
R = 2π2me4
h3
.
(9.155)
Remark 9.30 (1) The 2n + 1 degeneracy of the spherical harmonics implies that
each energy state of the hydrogen atom is associated with actual 2n + 1 distinct
eigenfunctions (or eigenstates). The different 2n + 1 states can be distinguished
from each other by subjecting the atom to an external magnetic ﬁeld. This is pre-
cisely the Zeeman effect.
(2) To appreciate the quantities involved in the calculation we carried out, we re-
call that ℏ= 1.054 × 10−27 erg, e = 4.8 × 10−10 esu, m = 0.911 × 10−27 g, and
R ≈3.3 × 1015 s−1.
(3) It is interesting to note that Balmer was not a ‘professional’ physicist. He was
a mathematics teacher in a girls’ school in Basel. His interest was in ﬁnding simple
geometrical principles in the sciences and humanities. For example, he wrote an
article on architectural interpretation of Ezekiel’s prophecy. His belief in nature’s
harmony led him to argue that the spectral lines of the elements satisfy beautiful
arithmetic relations. It seems at ﬁrst sight that the spectral lines of the hydrogen atom
prove his belief that the world is based on harmonic arithmetic rules. It turns out,
however, that (9.146) is only approximately correct. The Schr¨odinger equation we
wrote neglects relativistic effects. When one considers the relativistic Schr¨odinger
equation (or the Dirac equation), it is discovered that each energy level we found
has an inner structure (called the ﬁne structure). The actual formula for the spectral
lines is then found to be more involved and not as elegant and integral as Balmer
believed.
9.10 Musical instruments
Musical instruments differ in shape, size, and mode of operation. Yet, they all share
the same basic principle – they emit sound waves. We shall analyze a number of
instruments in light of the eigenvalue theory we have developed. In particular, we
consider instruments based on strings, air compression, and membranes. Some less
common instruments, such as the gong, are based on plates (see the next chapter)
and will not be studied here. We emphasize that our models are crude, and they aim
only to capture basic mechanisms. Much more extensive analysis can be found in
[5] and numerous websites.
String instruments Let us recall our solution (5.44) for the wave equation in a
ﬁnite interval with homogeneous boundary conditions. To understand the physical

9.10 Musical instruments
267
interpretation of this solution, we notice that the vibration of the string is a superpo-
sition of fundamental vibrations that are called normal modes (or standing waves, or
just harmonics) with natural frequencies ωn = cπn/L. The name harmonics (and
hence also the notions of harmonic functions and harmonic analysis) was coined
by the ancient Greeks, who discovered that the basic frequencies of a string are all
integral multiples of a single fundamental frequency ω1 = cπ/L. They noted that
this phenomenon is pleasing to the ear, or, rather, to the brain.
Thus, the harmonics produced by a string instrument, such as a violin or a guitar,
are determined by two parameters: c and L. The speed of sound c depends, as was
explained in Chapter 1, upon the material composing the string; it is a ﬁxed property
of the instrument. The string’s length L is adjusted by the musician, and provides
a dynamical control of the basic tones.
The explanation above for the performance of a string instrument is oversim-
pliﬁed. Let us look, for example, at the guitar. In reality the motion of the guitar’s
string is too weak actually to move a large enough mass of air to create a sound
wave that could be detected by our ears (to demonstrate this fact, one can try to play
an electric guitar without an ampliﬁer). What the string’s oscillations really do is
activate a complex vibrating system that consists also of the top plate of the guitar’s
sound box and of the air inside the sound box. We therefore have a coupled system
of vibrating bodies: the string (modeled by the one-dimensional wave equation of
Chapter 5), the top plate (whose wave equation is based on the energy associated
with plates; see Chapter 10), and a mass of air (whose wave equation was derived
in Chapter 1). The ﬁnal outcome of this complex system is an air pressure wave of
considerable amplitude that is radiated from the sound box through its sound hole.
Another interesting point is that a guitar string actually generates a combination of
several basic modes. The combination depends not only on the string’s length and
material, but also on the playing technique. When the guitarist strikes the string, we
should consider the wave equation with zero initial amplitude and some nonzero
initial velocity; on the other hand, when the guitarist plucks the string, the initial
velocity is zero, but the initial amplitude is different from zero. Different initial
conditions give rise, of course, to different linear combinations of the basic modes.
Wind instruments In a wind instrument the sound wave is generated directly. To
create a musical sound, the player blows air into the instrument. Therefore we use
the acoustics equations (1.22). We consider ﬁrst long and narrow instruments with
a uniform cross section, such as the ﬂute or the clarinet. Because of their geometry,
we can assume for simplicity that the pressure depends only on the longitudinal
direction that we denote by z. We thus write the wave equation for the pressure:
ptt −c2 pzz = 0
0 < z < L,
t > 0,
(9.156)

268
Equations in high dimensions
where L is the length of the instrument. The pressure p in (9.156) is actually the
deviation of the pressure in the instrument from the bulk pressure in air. Therefore,
in the ﬂute, which is open to the outside air at both ends, we use the Dirichlet
boundary condition p = 0 at z = 0, L. In the clarinet, for example, one end z = 0
is open, and we also use the homogeneous Dirichlet condition there, but the other
end z = L is closed. Hence the air velocity is constant (zero) there. Therefore the
second equation of the pair (1.22) implies that we should use there a homogeneous
Neumann condition, i.e. pz(L) = 0. From our discussion of the wave equation in
Chapter 5 we conclude (at least in our crude model) that the basic frequencies of
the ﬂute are determined by the eigenvalues of the Sturm–Liouville problem (5.9)–
(5.10), while the basic frequencies of the clarinet are determined by the eigenvalues
of the problem (6.44)–(6.45). It follows that the fundamental frequencies of the ﬂute
include all multiples of ω = cπ/L, while in the clarinet we expect to obtain only
the odd multiples of ω. The ﬂute and the clarinet have basically a pipe geometry.
The organ consists of many pipes. Typically some of them will be open at both
ends, and some only open at one end. Finally, the basic harmonics generated by
the ﬂute depend on the location of the open holes. Basically, the holes shorten the
length of the ﬂute, thus increasing the frequency.
We proceed to analyze elongated wind instruments with a varying cross section
such as the horn or the oboe. We still use a one-dimensional wave equation like
(9.156), but we need to take into account also the variations in the cross section
S(z). It can be shown that the appropriate model for waves in a narrow elongated
structure with a varying cross section is given by
S(z)ptt −c2 (S(z)pz)z = 0
0 < z < L, t > 0.
(9.157)
Equation (9.157) is known in the scientiﬁc music literature as Webster’s horn equa-
tion. Separating variables, as in Chapter 5, we obtain that the basic frequencies are
cλn, where λn are the eigenvalues of the Sturm–Liouville problem

S(z)v′(z)
′
+ λS(z)v(z) = 0
0 < z < L.
(9.158)
We have not speciﬁed the boundary conditions, since they depend on the speciﬁc
instrument.
So far we have considered an arbitrary cross section S(z). In general there is no
closed form solution to problem (9.158). Fortunately, the proﬁle of many instru-
ments can be well approximated by the formula
S(z) = b(z −z0)−γ ,
(9.159)
where γ is a parameter, called ﬂare in musical jargon. For example, the horn roughly
corresponds to γ = 2, while the oboe corresponds to γ = 7. Obviously, the ﬂute

9.11 Green’s functions in higher dimensions
269
and the clarinet correspond to γ = 0. For this special family of cross sections,
(9.158) becomes
z2v′′ −γ zv′ + λz2v = 0
L1 < z < L2,
(9.160)
where we have shifted the origin to simplify the equation and to ﬁt the actual shape
of the instrument. Applying the transformation v(z) = z(1+γ )/2u(z), one can check
that u satisﬁes the Bessel equation
u′′(z) + 1
z u′(z) +

λ −[(γ + 1)/2]2
z2
/
u(z) = 0
L1 < z < L2.
(9.161)
The fundamental system of solutions of (9.161) is the pair

J(γ +1)/2(
√
λz), Y(γ +1)/2(
√
λz)

,
where J(γ +1)/2 is the Bessel function of the ﬁrst kind of order (γ + 1)/2, and Y(γ +1)/2
is the Bessel function of the second kind (which is the singular solution to the Bessel
equation of order (γ + 1)/2). The eigenvalues λn can now be computed by the
algebraicmethodofProposition6.18,oncetheboundaryconditionsattheendpoints
have been speciﬁed. The freedom introduced by incorporating a proﬁle function
S(z) can be used for design purposes to generate better acoustical instruments and
devices.
Drums The drum is modeled as a circular membrane attached at its bound-
ary to a ﬁxed frame. We have already solved the equation for the vibrations of
such membranes, and found that the normal modes are Jn(αn,mr/a) cos nθ and
Jn(αn,mr/a) sin nθ, where αn,m is the mth root of the Bessel function Jn, and a is
the drum’s radius. The basic frequencies of the membrane are ωn,m = cαn,m/a. In
a typical drum, though, the membrane vibrates over a bounded domain in space.
Therefore the assumption of free vibrations is not accurate. This assumption is
particularly problematic for the ﬁrst mode ω0,0, since this mode (and only this
mode!) corresponds to an eigenfunction with a constant sign. Thus, the integral
under the membrane surface is not zero, implying that a considerable compression
or expansion of the air inside the drum is required. Therefore this mode is often not
heard.
9.11 Green’s functions in higher dimensions
In this section we study (Dirichlet) Green’s function for the Laplace equation
u = f
D,
u = g
∂D,
(9.162)

270
Equations in high dimensions
where D is a smooth bounded domain in RN, and N ≥3. We show that most
of the properties of Green’s function for a planar domain are valid also for
higher dimensions. As in the two-dimensional case, the fundamental solution
of the Laplace equation plays an important role in the study of Green’s func-
tion. The essential difference, which has many implications, is that in the two-
dimensional case the fundamental solution changes sign and does not decay at
inﬁnity, while for N ≥3 the fundamental solution is positive and decays to zero at
inﬁnity.
Just like in the two-dimensional case, the fundamental solution in higher di-
mensions is a radial symmetric harmonic function that is singular at the origin.
Set
⃗x = (x1, . . . , xN),
r = |⃗x| =
 N

n=1
x2
i
1/2
.
The fundamental solution (⃗x; 0) = (|⃗x|) with a pole at the origin satisﬁes the
following Euler (equidimensional) equation:
urr + N −1
r
ur = 0
r > 0.
(9.163)
Therefore, for N ≥3
(⃗x, 0) =
1
N(N −2)ωN
|x|2−N,
(9.164)
where ωN is the volume of a unit ball N in RN. Hence, the fundamental solution
with a pole at ⃗y is given by
(⃗x; ⃗y) = (|⃗x −⃗y|).
The fact that  is indeed a fundamental solution of the Laplace equation will be
proved later (see Corollary 9.31). One can verify that

∂
∂xi
(⃗x; ⃗y)
 ≤CN|⃗x −⃗y|1−N
1 ≤i ≤N,
(9.165)

∂2
∂xi∂x j
(⃗x; ⃗y)
 ≤CN|⃗x −⃗y|−N
1 ≤i, j ≤N.
(9.166)
Fixing ⃗y, the function (⃗x; ⃗y) is harmonic as a function of ⃗x for all ⃗x ̸= ⃗y. For
ε > 0, we use the notation
Bε := {⃗x | |⃗x −⃗y| < ε},
Dε := D \ Bε.

9.11 Green’s functions in higher dimensions
271
Let u ∈C2( ¯D). Use the second Green identity (7.19) on the domain Dε where
v(⃗x) = (⃗x; ⃗y) is harmonic to derive

Dε
(u −u)d⃗x =

∂Dε
(∂nu −u∂n)dσ.
Therefore,

Dε
u d⃗x =

∂D
(∂nu −u∂n)dσ +

∂Bε
(∂nu −u∂n)dσ.
Letting ε →0, and recalling that the outward normal derivative on ∂Bε is the
(inward) radial derivative (see Figure 8.1), we ﬁnd


∂Bε
∂nu dσ
 ≤Cε sup |∇u| →0
where ε →0,

∂Bε
u∂n ds =
1
NωN εN−1

∂Bε
u dσ →u(⃗y)
where ε →0.
Thus,
u(⃗y)=

∂D

(⃗x−⃗y)∂nu−u∂n(⃗x−⃗y)

dσ −

D
(⃗x−⃗y)u d⃗x.
(9.167)
Equation (9.167) is called Green’s representation formula, and the function
[ f ](⃗y) := −

D
(|⃗x −⃗y|) f (⃗x) d⃗x
is called the Newtonian potential of f .
As in the planar case, it follows that harmonic functions are smooth. Moreover,
we have the following corollary.
Corollary 9.31 If u ∈C2
0(RN) (i.e. u has a compact support), then
u(⃗y) = −

RN (⃗x; ⃗y)u(⃗x) d⃗x.
(9.168)
In other words, −(⃗x; ⃗y) = δ(⃗x −⃗y), so  is a fundamental solution of the
Laplace equation on RN.
Consider again the Dirichlet problem (9.162). Let h(⃗x; ⃗y) be a solution (which
depends on the parameter ⃗y) of the following Dirichlet problem:
h(⃗x; ⃗y) = 0
⃗x ∈D,
h(⃗x; ⃗y) = (⃗x −⃗y)
⃗x ∈∂D.
(9.169)
We suppose that there exists a solution for (9.169), and use the following deﬁnition.

272
Equations in high dimensions
Deﬁnition 9.32 (Dirichlet) Green’s function for the Laplace equation on D is given
by
G(⃗x; ⃗y) := (⃗x; ⃗y) −h(⃗x; ⃗y)
⃗x, ⃗y ∈D, ⃗x ̸= ⃗y.
(9.170)
Hence, the Green function satisﬁes
G(⃗x; ⃗y) = −δ(⃗x −⃗y)
⃗x ∈D,
G(⃗x; ⃗y) = 0
⃗x ∈∂D.
(9.171)
As in the two-dimensional case, we can write
u(⃗y)=−

∂D
∂nG(⃗x; ⃗y)u(⃗x) dσ −

D
G(⃗x; ⃗y)u(⃗x) d⃗x.
(9.172)
Substituting the given data into (9.172), we ﬁnally arrive at following integral
representation formula for solutions of the Dirichlet problem for the Poisson
equation.
Theorem 9.33 Let u ∈C2( ¯D) be a solution of the Dirichlet problem
u = f
D,
u = g
∂D.
(9.173)
Then
u(⃗y)=−

∂D
∂nG(⃗x; ⃗y)g(⃗x) dσ −

D
G(⃗x; ⃗y) f (⃗x) d⃗x.
(9.174)
Theorem 9.33 enables us to solve the Dirichlet problem in a domain D provided
that Green’s function is known, and that it is a priori known that the solution is in
C2(D) ∩C1( ¯D). This additional regularity is indeed ensured if f, g, and ∂D are
sufﬁciently smooth.
There are two integral kernels in the representation formula (9.174):
(1) Green’s function G(⃗x; ⃗y), which is deﬁned for all distinct points ⃗x, ⃗y ∈D.
(2) K(⃗x; ⃗y) := −∂nG(⃗x; ⃗y), which is the outward normal derivative of Green’s function on
the boundary of the domain D. This kernel, which is naturally called the Poisson kernel
on D, is deﬁned for ⃗x ∈∂D and ⃗y ∈D.
It turns out that Green’s function can be represented as an inﬁnite series using
the eigenfunction expansion. More precisely, let D be a smooth bounded domain
in RN. Appealing to the results of Section 9.5 we recall that the Laplace operator
with the Dirichlet boundary condition admits a complete orthonormal system (with
respect to the inner product ⟨u, v⟩=

D u(⃗x)v(⃗x) d⃗x) of eigenfunctions {φn(⃗x)}∞
n=0.
The corresponding eigenvalues {λn}∞
n=0 are listed in a nondecreasing order and are
repeated according to their multiplicity.

9.11 Green’s functions in higher dimensions
273
Using the eigenfunction expansion, it (formally) follows that the solution of the
Dirichlet problem
u = f (⃗x)
⃗x ∈D,
u(⃗x) = 0
⃗x ∈∂D
(9.175)
is given by
u(⃗x) =
∞

n=0
Bnφn(⃗x),
(9.176)
where Bn are the (generalized) Fourier coefﬁcients
Bn = −1
λn

D
φn(⃗y) f (⃗y) d⃗y
n = 0, 1, . . . .
(9.177)
Interchanging (formally) the order of summation and integration, we obtain
u(⃗x) = −
∞

n=0
 1
λn

D
φn(⃗y) f (⃗y) d⃗y

φn(⃗x)
= −

D
& ∞

n=0
φn(⃗x)φn(⃗y)
λn
'
f (⃗y) d⃗y.
Therefore,
G(⃗x, ⃗y) :=
∞

n=0
φn(⃗x)φn(⃗y)
λn
.
(9.178)
The following theorem summarizes some essential properties of (Dirichlet)
Green’s function.
Theorem 9.34 Let D be a smooth bounded domain in RN, N ≥3. Then
(a) The Laplace operator has a unique (Dirichlet) Green function on D.
(b) Green’s function is symmetric, i.e.
G(⃗x; ⃗y) = G(⃗y; ⃗x),
for all ⃗x, ⃗y ∈D such that ⃗x ̸= ⃗y.
(c) For a ﬁxed ⃗x ∈D, the function G(⃗x; ⃗y) is a positive harmonic function in D \ {⃗x} and
vanishes on ∂D.
(d) Fix ⃗x ∈∂D. Then as a function of ⃗y, the Poisson kernel K(⃗x; ⃗y) is a positive harmonic
function in D that vanishes on ∂D \ {⃗x}.
(e) Let D1, D2 be smooth bounded domains in RN, N ≥3 such that D1 ⊂D2. Then
0 ≤G1(⃗x; ⃗y) ≤G2(⃗x; ⃗y) ≤(|⃗x −⃗y|)
⃗x, ⃗y ∈D1,
where Gi is the Green function in Di for i = 1, 2.

274
Equations in high dimensions
Proof The proof of the theorem is similar to the proof for the two-dimensional
case and therefore is left for the reader (see Exercise 9.20). Note that by part (e)
Green’s function is pointwise bounded from above by the fundamental solution .
In particular at each point there is an upper bound for the Green function that is
independent of D. This property is not valid on the plane, where the sequence {Gn}
of Green’s functions for Bn, the disks of radius n and a center at the origin, tends
to inﬁnity as n →∞.
□
Example 9.35 Let BR be an open ball with radius R and center at the origin. We
want to calculate Green’s function and the Poisson kernel for BR. For ⃗x ∈BR, the
point ˜x := (R2/|⃗x|2)⃗x is said to be the inverse point of ⃗x with respect to the sphere
∂BR. It is convenient to deﬁne the ideal point ∞as the inverse of the origin. Set
G(⃗x; ⃗y) : =



(|⃗x −⃗y|) −
	|⃗y|
R |⃗x −˜y|

⃗y ̸= 0,
(|⃗x|) −(R)
⃗y = 0.
= (

|⃗x|2 + |⃗y|2 −2⃗x · ⃗y) −(
(
|⃗x||⃗y|/R
2 + R2 −2⃗x · ⃗y ).
(9.179)
An elementary calculation implies that G is indeed Green’s function on BR, and
that the Poisson kernel is given by
K(⃗x; ⃗y) = R2 −|⃗y|2
NωN R |⃗x −⃗y|−N.
(9.180)
As an immediate consequence of the explicit expression (9.180) for the Poisson
kernel in a ball, we can derive the mean value principle (see Exercise 9.21).
Theorem 9.36 The mean value principle Let D be a domain in RN, N ≥3, and
let u be a harmonic function in D. Let ⃗x0 be a point in D and let BR be a ball
of radius R around ⃗x0 that is fully included in D. Then the value of u at ⃗x0 is the
average of the values of u on the sphere ∂BR.
Just like in the two-dimensional case, the mean value principle implies the strong
and weak maximum principles.
Theorem 9.37 (a) Let u be a harmonic function in a domain D (here we also
allow for unbounded D ⊂RN). If u attains it maximum (minimum) at an interior
point of D, then u is constant.
(b) Suppose that D is a bounded domain, and let u ∈C2(D) ∩C( ¯D) be a har-
monic function in D. Then the maximum (minimum) of u in ¯D is achieved on the
boundary ∂D.

9.12 Heat kernel in higher dimensions
275
Example 9.38 Let RN
+ be the upper half-space. As in Example 8.15, one can use
the reﬂection principle to obtain the corresponding Green’s function and Poisson’s
kernel (see Exercise 9.23).
9.12 Heat kernel in higher dimensions
In this section we give a short survey of the generalization to higher dimensions of
the results of Section 8.4 concerning the heat kernel and integral representations
for the heat equation. For simplicity, we assume in the following discussion that
the heat conduction coefﬁcient k equals 1.
Let D ⊂RN be a smooth bounded domain in RN, N ≥2. Recall from Sec-
tion 9.5 that the Laplace operator with the Dirichlet boundary condition ad-
mits a complete orthonormal system (with respect to the inner product ⟨u, v⟩=

D u(⃗x)v(⃗x) d⃗x) of eigenfunctions {φn(⃗x)}∞
n=0, and eigenvalues {λn}∞
n=0. The eigen-
values are listed in a nondecreasing order and are repeated according to their
multiplicity.
Using the eigenfunction expansion, it (formally) follows that the solution of the
initial boundary value problem
ut −u = 0
⃗x ∈D, t > 0,
u(⃗x, t) = 0
⃗x ∈∂D, t ≥0,
u(⃗x, 0) = f (⃗x)
⃗x ∈D
(9.181)
is given by
u(⃗x, t) =
∞

n=0
Bnφn(⃗x)e−λnt,
(9.182)
where Bn are the (generalized) Fourier coefﬁcients
Bn =

D
φn(⃗y) f (⃗y) d⃗y
n = 0, 1, . . . .
(9.183)
It turns out that the above series converges uniformly for t > ε > 0; therefore we
may interchange the order of summation and integration, and hence
u(⃗x, t) =
∞

n=0

D
φn(⃗y) f (⃗y) d⃗y

φn(⃗x)e−λnt
=

D
& ∞

n=0
e−λntφn(⃗x)φn(⃗y)
'
f (⃗y) d⃗y.

276
Equations in high dimensions
We have derived the following integral representation:
u(⃗x, t) =

D
K(⃗x, ⃗y, t) f (⃗y) d⃗y,
(9.184)
where K is the heat kernel:
K(⃗x, ⃗y, t) :=
∞

n=0
e−λntφn(⃗x)φn(⃗y).
(9.185)
By the Duhamel principle, it follows that the solution of the initial boundary
value problem



ut −u = F(⃗x, t)
⃗x ∈D, t > 0,
u(⃗x, t) = 0
⃗x ∈∂D, t ≥0,
u(⃗x, 0) = f (⃗x)
⃗x ∈D
(9.186)
is given by the following representation formula:
u(⃗x, t) =

D
K(⃗x, ⃗y, t) f (⃗y) d⃗y +
 t
0

D
K(⃗x, ⃗y, t −s)F(⃗y, s) d⃗yds.
(9.187)
The main properties of the heat kernel for the one-dimensional case are also
valid in higher dimensions. The following theorem summarizes these properties
and some other properties that were not stated for the one-dimensional case.
Theorem 9.39 Let K(⃗x, ⃗y, t) be the heat kernel of problem (9.186). Then
(a) The heat kernel is symmetric, i.e. K(⃗x, ⃗y, t) = K(⃗y, ⃗x, t).
(b) For a ﬁxed ⃗y (or a ﬁxed ⃗x), the heat kernel K as a function of t and ⃗x (or ⃗y) solves the
heat equation for t >0, and satisﬁes the Dirichlet boundary conditions.
(c) K(⃗x, ⃗y, t) ≥0.
(d) Suppose that D1 ⊂D2, and let Ki be the heat kernel in Di, i = 1, 2. Then K1(⃗x, ⃗y, t) ≤
K2(⃗x, ⃗y, t) for all ⃗x, ⃗y ∈D1 and t > 0.
(e)

D
K(⃗x, ⃗y, t) d⃗y ≤1.
(f) For all t, s > 0 the heat kernel satisﬁes the following semigroup property:
K(⃗x, ⃗y, t + s) =

D
K(⃗x, ⃗z, t)K(⃗z, ⃗y, s) d⃗z.
(g) The following trace formula holds:

D
K(⃗x, ⃗x, t) d⃗x =
∞

n=0
e−λnt.

9.12 Heat kernel in higher dimensions
277
(h) Let G(⃗x, ⃗y)be(Dirichlet)Green’sfunctionoftheLaplaceequationonasmoothbounded
domain D. Then
G(⃗x, ⃗y) =
 ∞
0
K(⃗x, ⃗y, t) dt.
Proof (a),(b) Formally these parts follow directly from (9.185), which deﬁnes the
heat kernel. In order to justify the convergence and term-by-term differentiations
one should use the exponential decay of the terms e−λnt and the bounds (which may
depend on n) on the eigenfunctions and their derivatives.
(c) Suppose on the contrary that there exists (⃗x0, ⃗y0, t0), ⃗x0, ⃗y0 ∈D, t0 > 0
such that K(⃗x0, ⃗y0, t0) < 0. Then K(⃗x, ⃗y, t) is negative in some neighborhood of
(⃗x0, ⃗y0, t0). Let u be a solution of problem (9.186) with F = 0 and with f (⃗y), which
is a nonnegative function that is strictly positive only for ⃗y in a small neighborhood
of ⃗y0. The representation formula (9.187) implies that u is negative at (⃗x0, t0), but
this contradicts the maximum principle for the heat equation.
(d) Let f be an arbitrary nonnegative smooth function with a compact support
in D1. For i = 1, 2 the functions
ui(⃗x, t) =

Di
Ki(⃗x, ⃗y, t) f (⃗y) d⃗y
solve the problems
(ui)t −ui = 0
⃗x ∈Di, t > 0,
ui(⃗x, t) = 0
⃗x ∈∂Di, t ≥0,
ui(⃗x, 0) = f (⃗x)
⃗x ∈Di.
On the other hand, by the maximum principle 0 ≤u1(⃗x, t) ≤u2(⃗x, t). Therefore,

D1
[K2(⃗x, ⃗y, t) −K1(⃗x, ⃗y, t)] f (⃗y) d⃗y ≥0.
Now, since f ≥0 is an arbitrary function, a similar argument to the one used in the
proof of part (c) shows that K1(⃗x, ⃗y, t) ≤K2(⃗x, ⃗y, t).
(e) Let 0 ≤fn ≤1 be a sequence of compactly supported smooth functions that
converge monotonically to the function 1. Then
un(⃗x, t) :=

D
K(⃗x, ⃗y, t) fn(⃗y)d⃗y
→
u(⃗x, t) :=

D
K(⃗x, ⃗y, t)d⃗y.
Now, un is a monotone sequence of solutions to the heat problem
ut −u = 0
⃗x ∈D, t > 0,
u(⃗x, t) = 0
⃗x ∈∂D, t ≥0,
u(⃗x, 0) = fn(⃗x)
⃗x ∈D.

278
Equations in high dimensions
On the other hand, v(⃗x, t) = 1 solves the heat problem
vt −v = 0
⃗x ∈D, t > 0,
v(⃗x, t) = 1
⃗x ∈∂D, t ≥0,
v(⃗x, 0) = 1
⃗x ∈D.
The maximum principle implies that un(⃗x, t)≤v(⃗x, t), and therefore,
u(⃗x, t) =

D
K(⃗x, ⃗y, t) d⃗y ≤v(⃗x, t) = 1.
(f ) Fix s > 0, and let f be a smooth function. Set
v(⃗x, t) :=

D
K(⃗x, ⃗y, t + s) f (⃗y) d⃗y.
The function
u(⃗x, t) : =

D

D
K(⃗x, ⃗z, t)K(⃗z, ⃗y, s)d⃗z

f (⃗y)d⃗y
=

D
K(⃗x, ⃗z, t)

D
K(⃗z, ⃗y, s) f (y)d⃗y

d⃗z
is a solution of the problem
ut −u = 0
⃗x ∈D, t > 0,
u(⃗x, t) = 0
⃗x ∈∂D, t ≥0,
u(⃗x, 0) = v(⃗x, 0)
⃗x ∈D.
On the other hand, v(⃗x, t) is also a solution of the same problem. Thanks to the
uniqueness theorem u = v, hence

D

D
K(⃗x, ⃗z, t)K(⃗z, ⃗y, s) d⃗z −K(⃗x, ⃗y, t + s)

f (⃗y) d⃗y = 0 .
Since f is an arbitrary function, it follows that
K(⃗x, ⃗y, t + s) =

D
K(⃗x, ⃗z, t)K(⃗z, ⃗y, s) d⃗z.
(g) The trace formula follows directly from the orthonormality of the sequence
{φn(⃗x)}∞
n=0 of all eigenfunctions (see Exercise 9.22). The proof of the Weyl asymp-
totic formula (9.54) relies on this trace formula.
(h) Follows from the expansion formulas (9.185) and (9.178), and integration
with respect to t.
□

9.13 Exercises
279
9.13 Exercises
9.1 (a) Generalize the characteristic method for the eikonal equation (see Chapter 2) to
solve the eikonal equation in three space dimensions.
(b) Let u(x, y, z) be a solution to the eikonal equation in R3 (homogeneous medium).
Assume
u(0, 0, 0) = ux(0, 0, 0) = uy(0, 0, 0) = 0.
Show that (∂nu/∂zn)|(0,0,0) = 0, for all n ≥2.
9.2 Solve the equation u2
x + u2
y + u2
z = 4 subject to the initial condition u(x, y,
1 −x −y) = 3.
9.3 Prove formula (9.26).
9.4 Derive the formulation of the Laplace equation in a spherical coordinate system
(r, θ, φ).
9.5 Find the radial solution to the Cauchy problem (9.22) under the initial conditions
u(r, 0) = 2,
ut(r, 0) = 1 + r2.
9.6 Find the radial solution to the Cauchy problem (9.22), with c = 1 subject to the initial
conditions
u(r, 0) = ae−r2,
ut(r, 0) = be−r2.
9.7 Derive the Darboux equation (9.32).
9.8 Find the eigenfunctions, eigenvalues, and the generalized Fourier formula for the
Laplace operator in the rectangle 0 < x < a, 0 < y < b subject to Neumann bound-
ary conditions.
9.9 Find the eigenfunctions, eigenvalues, and the generalized Fourier formula for the
Laplace operator in the three-dimensional box
0 < x < a, 0 < y < b, 0 < z < c
under the Dirichlet boundary conditions.
9.10 (a) Prove that the Dirichlet eigenvalue problem for the Laplace equation in the unit
square has inﬁnitely many eigenvalues with multiplicity three or more.
(b) Let  be a rectangle with sides a and b, such that the ratio a2/b2 is not a rational
number. Show that the Dirichlet eigenvalue problem in  is not degenerate.
9.11 Use the representation (9.76) to derive the recurrence formula (9.77).
9.12 The Bessel functions share many common properties with the classical trigonometric
functions sin nx and cos nx. Some of these properties were discussed in Subsection
9.5.3. Let us consider two additional properties:
(a) Show that, like sin nx and sin(n + 1)x, the Bessel functions Jn(x) and Jn+1(x) do
not vanish at the same point.
(b) The formula relating sin(α + β) with sin α, cos α, sin β and cos β is in general
taught in high school trigonometry classes. Show that for Bessel functions there exist

280
Equations in high dimensions
similar formulas, except that they now involve an inﬁnite series:
Jn(α + β) =
∞

m=−∞
Jm(α)Jn−m(β).
9.13 (a) Let v1 and v2 be two smooth solutions to the Legendre equation (9.96) in [−1,1]
associated with different coefﬁcients µ1 and µ2. Show that
 1
−1 v1(s)v2(s)ds = 0.
(b) Use the result in part (a), the Weierstrass approximation theorem and the Legendre
polynomials constructed in this chapter to prove that if µ is not of the form µ =
k(k + 1) for some integer k, then the Legendre equation has no smooth solutions on
[−1, 1].
9.14 Find the general solution of the wave equation in a cube under the Dirichlet boundary
value conditions.
9.15 Prove that every function of the form Q(r, φ, θ)=rnYn,m(φ, θ) is a homogeneous
harmonic polynomial.
9.16 Find the general solution of the heat equation in a disk under the Neumann boundary
conditions.
9.17 (a) Prove the Rodriguez formula:
Pn(t) =
1
2nn!
dn
dtn (t2 −1)n.
(9.188)
(b) Prove Proposition 9.25.
9.18 Write the solution of the Dirichlet problem for the Laplace equation on a cylinder as
a generalized Fourier series, and ﬁnd the corresponding formula for the generalized
Fourier coefﬁcients.
9.19 Prove formulas (9.179)–(9.180) for the Green function and Poisson kernel in the ball
BR.
9.20 Complete the proof of Theorem 9.34 (see the corresponding proof for the two-
dimensional case).
9.21 (a) Use the explicit formula for the Poisson kernel in a ball (formula (9.180)) to prove
the mean value principle (Theorem 9.36).
(b) Provide an alternative proof of the same theorem that relies on the proof method
of Theorem 7.7.
(c) Prove the strong and weak maximum principles for harmonic functions on a domain
D in RN.
9.22 Complete the proof of Theorem 9.39.
9.23 Using the reﬂection method, ﬁnd explicit formulas for the Green function and Poisson
kernel in the half space RN
+.
Hint See Example 8.15.
9.24 Let D ⊂RN beasmoothboundeddomain.Let{φn(⃗x)}∞
n=0 and{λn}∞
n=0 betheorthonor-
mal sequence of eigenfunctions and the corresponding eigenvalues for the Laplace
operator with the Dirichlet boundary condition. Let λ ̸= λn be a real number.

9.13 Exercises
281
(a) Find the eigenfunction expansion of the Green function Gλ(⃗x; ⃗y) for the Dirichlet
problem in D for the Helmholtz equation u + λu = 0. So, Gλ satisﬁes
Gλ(⃗x; ⃗y) + λGλ(⃗x; ⃗y) = −δ(⃗x −⃗y)
⃗x ∈D,
Gλ(⃗x; ⃗y) = 0
⃗x ∈∂D.
(b) Calculate limλ→λ0(λ0 −λ)Gλ(⃗x, ⃗y) .
(c) Find the large time asymptotic formula for the heat kernel.
Hint Calculate limt→∞eλ0t K(⃗x, ⃗y, t).
(d) Compare your answers to parts (b) and (c), and try to explain your result.
9.25 (a) Find the eigenfunction expansion of the (Dirichlet) Green function in the rectangle
{(x, y) | 0 < x < a, 0 < y < b}.
(b) Find the eigenfunction expansion of the (Dirichlet) Green function in the disk
{(x, y) | x2 + y2 < R2}.

10
Variational methods
The PDEs we have considered so far were derived by modeling a variety of phe-
nomena in physics, engineering, etc. In this chapter we shall derive PDEs from a
new perspective. We shall show that many PDEs are related to optimization prob-
lems. The theory that associates optimization with PDEs is called the calculus of
variations [20]. It is an extremely useful theory. On the one hand, we shall be
able to solve many optimization problems by solving the corresponding PDEs. On
the other hand, sometimes it is simpler to study (and solve) certain optimization
problems than to study (and solve) the related PDE. In such cases, the calculus of
variations is an indispensable theoretical and practical tool in the study of PDEs.
The calculus of variations can be used for both static problems and dynamic prob-
lems. The dynamical aspects of this theory are based on the Hamilton principle that
we shall derive below. In particular, we shall show how to apply this principle for
wave propagation in strings, membranes, etc.
We shall see that the connection between optimization problems and the associ-
ated PDEs is based on the a priori assumption that the solution to the optimization
problem is smooth enough for the PDE to make sense. Can we justify this assump-
tion? In many cases we can. Moreover, even if the solution is not smooth, we would
like to deﬁne an appropriate concept of weak solutions as we already did earlier in
this book in different contexts. How should we deﬁne them? To answer these ques-
tions we need to introduce special inner product function spaces (see Chapter 6).
After introducing these spaces, we shall be able to deﬁne a natural notion of weak
solutions for a large variety of PDEs.
10.1 Calculus of variations
Let  be a simple closed curve in R3. A surface whose boundary is  is said to
be spanned by . To deﬁne the concept of minimal surfaces let us consider for the
moment a surface S = S(u), characterized by a graph of a function u(x, y) deﬁned
282

10.1 Calculus of variations
283
over a region D in R2, such that the boundary ∂D is mapped by u to ∂S =  (in
particular S(u) is spanned by ). The surface area A of S is given by
E(u) := A(S(u)) =

D
(
1 + u2x + u2y dxdy.
(10.1)
A surface S is called a (local) minimal if its surface area is smaller than the surface
area of all other surfaces spanned by  that are close to S in an appropriate sense.
More precisely, a function v is called admissible if the surface S(v) is spanned by ,
v is continuously differentiable in D (to guarantee that the local surface element is
deﬁned), and v is continuous in the closure of D. A function u is a (local) minimizer
for the surface area problem if u is an admissible function and
E(u) ≤E(v),
(10.2)
for every admissible functions v (that are close to u).
The problem of characterizing and computing minimal surfaces has been con-
sidered by many mathematicians since the middle of the eighteenth century, with
major contributions to the subject provided by Lagrange and Laplace. The rich-
ness of the problem was not realized, however, until the soap ﬁlm experiments
performed by the Belgian physicist Joseph Antoine Plateau (1801–1883) around
1870.1
The problem of minimizing E is analogous to the problem of minimizing a dif-
ferentiable function f : R →R. As we recall from calculus, a necessary condition
for a point x ∈R to be a local minimizer is that the derivative of f is zero at x.
The function E(u) deﬁned above is a mapping that associates a real number with
a function u. Such objects are called functionals. There is a trick that enables us to
use the theory of optimizing real functions to optimize functionals. The idea is to
consider a ﬁxed function u which is our candidate for a minimizer. We then intro-
duce a real parameter ε and represent any admissible function v as v = u + εψ.
This construction implies that ψ must belong to the space of functions
A = {ψ ∈C1(D) ∩C( ¯D), ψ(x, y) = 0 for (x, y) ∈∂D}.
(10.3)
We rewrite (10.2) in the form E(u) ≤E(u + εψ) for small |ε| and for all ψ ∈A.
Considering E(u + εψ) as a real function of ε (with u and ψ ﬁxed), we apply the
standard argument from calculus to require the necessary condition
d
dε E(u + εψ)

ε=0
= 0.
(10.4)
1 Incidentally, while such experiments are now frequently performed by children in science museums around the
world, Plateau himself did not see a single minimal surface! He was blinded early in his scientiﬁc career as a
result of looking directly at the sun while performing optical experiments.

284
Variational methods
The expression on the left hand side of (10.4) is called the ﬁrst variation of E at
u. It is denoted by δE(u)(ψ). The somewhat unusual notation indicates that the
ﬁrst variation depends on u, and it is a functional (in fact, a linear functional) of
ψ. We shall demonstrate in the sequel explicit computations of ﬁrst variations. To
avoid too cumbersome notation, we shall often denote the ﬁrst variation for short
by δE(u).
Before demonstrating the implications of (10.4) for our model problem of min-
imal surfaces, let us look at a simpler problem. If we assume that the minimal
surface u has small derivatives, we can approximate the functional E(u) by a
simpler functional. Using the approximation √1 + x ∼1 + 1
2x + · · · , we expand
E(u) = |D| + 1
2

D

u2
x + u2
y

dxdy + · · · , where |D| denotes the area of D. Ne-
glecting high order terms, we replace the problem of minimizing E(u) with the
problem of minimizing the functional
G(u) = 1
2

D

u2
x + u2
y

dxdy = 1
2

D
|∇u|2 dxdy.
(10.5)
The functional G is called the Dirichlet functional or Dirichlet integral. It plays a
prominent role in many branches of science and engineering.
We are now ready to use (10.4) to derive an equation for the local minimizers of
G. Let us compute in detail the differentiation in (10.4): it is easy to check that
G(u + εψ) = G(u) + ε

∇u · ∇ψ dxdy + ε2G(ψ).
(10.6)
Thus,
δG(u) = d
dε G(u + εψ)

ε=0
=

D
∇u · ∇ψ dxdy.
(10.7)
Therefore, a necessary condition for u to be a local minimizer is that it satisﬁes

D
∇u · ∇ψ dxdy = 0
∀ψ ∈A.
(10.8)
To derive an explicit equation for u we integrate the last integral by parts. Using
Green’s identity (7.20) and the condition on ψ at the boundary ∂D, we obtain

D
uψ dxdy = 0
∀ψ ∈A.
(10.9)
At this point we invoke Lemma 1.1. Thanks to this lemma we conclude that if u
is continuous, then (10.9) implies
u = 0
in D.
(10.10)

10.1 Calculus of variations
285
By construction, u must satisfy the boundary condition
u(x, y) = g(x, y)
(x, y) ∈∂D,
(10.11)
where g is the (given) graph of u over ∂D. We have therefore proved that a necessary
condition for a smooth function u to minimize the Dirichlet functional G is that u is a
solution of the Dirichlet problem for the Laplace equation. The PDE that is obtained
by equating the ﬁrst variation of a functional to zero is called the Euler–Lagrange
equation. We can therefore say that “Laplace = Euler–Lagrange of Dirichlet” . . . .
We now return to our original problem of minimal surfaces. It is convenient to
derive the minimal surface equation as a special case of a more general equation that
is valid for any functional that depends on a function and its derivatives. Consider
for this purpose a function F(x1, x2, . . . , xn, L1(u), L2(u), . . . , Lm(u)), where Li
is a linear operator (such as a differential operator or the identity operator). For
instance, the integrand in the Dirichlet integral is
F =
	 ∂
∂x u

2
+
	 ∂
∂y u

2
.
To compute the ﬁrst variation of
K(u) =

D
F dx1dx2 . . . dxn,
(10.12)
we expand F into a Taylor series around a base function u. Using
F(u + εψ) = F(u) + ε
m

i=1
∂F
∂Li
(u)Li(ψ) + O(ε2),
(10.13)
and equating the ﬁrst variation to zero, we obtain the Euler–Lagrange equation
δK(u) =

D
m

i=1
∂F
∂Li
(u)Li(ψ) dx1 . . . dxn = 0.
(10.14)
The reader is encouraged to use (10.14) for an alternative derivation of (10.7).
We now use (10.14) to derive a number of further examples of Euler–Lagrange
equations:
Example 10.1 The minimal surface equation In the minimal surface problem,
F(u) =
(
1 + u2x + u2y. Therefore,
δA(S(u)) =

D
1
(
1 + u2x + u2y
∇u · ∇ψ dxdy.
(10.15)

286
Variational methods
Figure 10.1 The helicoid.
Figure 10.2 The catenoid.
Integrating by parts with the aid of the divergence theorem, we obtain the minimal
surface equation:
∇·


1
(
1 + u2x + u2y
∇u

= ∂
∂x


ux
(
1 + u2x + u2y

+ ∂
∂y


uy
(
1 + u2x + u2y

=0.
(10.16)
Examples of minimal surfaces are depicted in Figure 10.1 (the helicoid) and in
Figure 10.2 (the catenoid). Notice, though, that the surfaces in these examples (as
well as the examples in Figure 10.3) cannot be represented as global graphs; rather
they can be written explicitly in a parametric form. For example, the helicoid is

10.1 Calculus of variations
287
expressed as
x = ρ cos θ, y = ρ sin θ, z = dρ,
(10.17)
where θ ∈[0, 2π], and ρ ∈(a, b) and a, b, d are some ﬁxed parameters. Similarly,
the catenoid is written as
x = d cosh ρ
d cos θ, y = d cosh ρ
d sin θ, z = ρ.
(10.18)
In the two examples we have analyzed so far, the values of the unknown function
u were known at the boundary. We consider now the problem of minimizing func-
tionals without constraints at the boundary. The calculation of the ﬁrst variation is
the same as in the case of problems with boundary constraints. The difference is in
the last step where the ﬁrst variation is used to obtain a PDE. We demonstrate the
derivation of the Euler–Lagrange equation in such a case through a variant of the
Dirichlet integral.
Example 10.2 Reconstruction of a function from its gradient Many applica-
tions in optics and other image analysis problems require a surface u(x, y) to be
computed from measurements of its gradient. This procedure is particularly useful
in determining the phase of light waves or sound waves. If the measurement is ex-
act, the solution is straightforward. Since, however, there is always an experimental
error, the measurement can be considered at best as an approximation of the gradi-
ent. Denote the measured vector that approximates the gradient by ⃗f t = ( f1, f2).
Typically, a given vector ﬁeld is not a gradient of a scalar function u. To be a gradi-
ent ⃗f must satisfy the compatibility condition ∂f1/∂y = ∂f2/∂x. If this condition
indeed holds, we can ﬁnd u (locally) through a simple integration. Since we expect
generically that measurement errors will corrupt the compatibility condition, we
seek other means for estimating the phase u. One such estimate is provided by the
least squares approximation:
min K(u) :=

D
|∇u −⃗f |2 dxdy,
(10.19)
where D is the domain where the gradient is measured. To apply (10.14), we write
F(ux, uy) = |∇u −⃗f |2 = |∇u|2 −2∇u · ⃗f + | ⃗f |2.
The differentiation is simple to perform and we obtain
δK(u) = 2

D

∇u −⃗f

· ∇ψ dxdy = 0.
(10.20)

288
Variational methods
Integrating (as usual) by parts, we get
1
2δK(u) =

D

−u + ⃗∇· ⃗f

ψ dxdy +

∂D

∂nu −⃗f · ˆn

ψ ds = 0,
(10.21)
where ˆn is the unit outer normal vector to ∂D. Since the ﬁrst variation must vanish,
in particular for functions ψ that are identically zero at ∂D, we must equate the
ﬁrst integral (10.21) to zero to obtain the Euler–Lagrange equation
u = ⃗∇· ⃗f
(x, y) ∈D.
(10.22)
Then (10.21) reduces to

∂D

∂nu −⃗f · ˆn

ψ ds = 0.
(10.23)
Now, taking advantage of the fact that this relation holds for ψ that are nonzero on
∂D as well, we obtain
∂nu = ⃗f · ˆn
(x, y) ∈∂D.
(10.24)
We have demonstrated how to deduce appropriate boundary conditions from the op-
timization problem. Such boundary conditions, which are inherent to the variational
problem (in contrast to being supplied from outside), are called natural boundary
conditions.
Physical systems in equilibrium are often characterized by a function that is a
local minimum of the potential energy of the system. This is one of the reasons
for the great value of variational methods. In the next examples we consider two
classical problems from the theory of elasticity.
Example 10.3 Equilibrium shape of a membrane under load Consider a thin
membrane occupying a domain D ⊂R2 when at a horizontal rest position, and
denote its vertical displacement by u(x, y). Assume that the membrane is subjected
to a transverse force (called in elasticity load) l(x, y) and constrained to satisfy
u(x, y) = g(x, y), for (x, y) ∈∂D. Since the membrane is assumed to be in equi-
librium, its potential energy must be at a minimum. The potential energy consists
of the energy stored in the stretching of the membrane and the work done by mem-
brane against the load l. The local stretching of the membrane from its horizontal
rest shape is given by d(
(
1 + u2x + u2y −1), where d is the elasticity constant of
the membrane. Assuming that the membrane’s slopes are small, we approximate
the local stretching by 1
2d(u2
x + u2
y). The work against the load is −lu. Therefore,
we have to minimize
Q(u) =

D
	d
2

u2
x + u2
y

−lu

dxdy.
(10.25)

10.1 Calculus of variations
289
The ﬁrst variation is
δQ(u) =

D
(d∇u · ∇ψ −lψ) dxdy.
(10.26)
Integrating the ﬁrst term by parts, using the boundary condition ψ = 0 on ∂D, and
equating the ﬁrst variation to zero we obtain

D
(du + l) ψ dxdy = 0.
Therefore, the Euler–Lagrange equation for the membrane is the Poisson equation:
du = −l(x, y) (x, y) ∈D,
u(x, y) = g(x, y)
(x, y) ∈∂D.
(10.27)
Example 10.4 The plate equation Consider a thin plate under a load l whose
amplitude with respect to a planar domain D is given by u(x, y). Integration of the
equations of elasticity leads to the following expression for the plate’s energy:
P(u) =

D
d
2

(u)2 + 2(1 −λ)−1 
u2
xy −uxxuyy

−lu
/
dxdy,
(10.28)
where λ is called the Poisson ratio and d is called the ﬂexural rigidity of the plate.
The Poisson ratio is a characteristic of the medium composing the plate. It measures
the transversal compression of an element of the plate when it is stretched longitu-
dinally. For example, λ ≈0.5 for rubber, and λ ≈0.27 for steel. The parameter d
depends not only on the material constituting the plate, but also on its thickness.
To ﬁnd the Euler–Lagrange equations for the plate we compute the ﬁrst variation
of P. To simplify the calculations we assume that the plate is clamped, i.e. both
u and ∂u/∂n are given on ∂D. Computing the ﬁrst variation of the ﬁrst and third
terms in (10.28) is straightforward:
δ1 =

D
(duψ −lψ) dxdy,
(10.29)
where ψ is the variation, and the boundary conditions imply that ψ and ∂ψ/∂n = 0
on ∂D. Integrating by parts twice the ﬁrst integral in (10.29) and using the boundary
conditions on ψ we obtain
δ1 =

D

d2u −l

ψ dxdy,
(10.30)
where
2 = ∂4
∂4x + 2
∂4
∂2x∂2y + ∂4
∂4y
is called for obvious reasons the biharmonic operator.

290
Variational methods
We proceed to compute the ﬁrst variation of the middle term in (10.28):
δ2 =
d
1 −λ

D
(2uxyψxy −uxxψyy −uyyψxx) dxdy.
(10.31)
The computation is facilitated by the important observation that the integrand in
(10.31) is the divergence of a certain vector:
2uxyψxy −uxxψyy −uyyψxx = ∂
∂x (uxyψy −uyyψx) + ∂
∂y (uxyψx −uxxψy).
(10.32)
Thanks to this identity and to the divergence theorem we can convert the variation
δ2 into a boundary integral. This integral involves the ﬁrst derivatives of ψ. Since
∂ψ/∂n = ψ = 0 at the boundary, both the normal and the tangential derivatives
of ψ vanish there. Therefore, ∂ψ/∂x = ∂ψ/∂y = 0 on the boundary ∂D, and the
boundary integral we derived for the variation δ2 is identically zero. We ﬁnally
obtain
δP(u) =

D

d2u −l

ψ dxdy = 0,
(10.33)
which implies that the Euler–Lagrange equation for thin plates is given by
d2u = l.
(10.34)
Another way to see that the middle term in the integral in (10.28) does not contribute
to the Euler–Lagrange equation is to observe that the corresponding integrand is
the Hessian uxxuyy −u2
xy, which is a divergence of a vector ﬁeld; i.e. it equals
∇· (uxuyy, −uxuxy).
Notice that the Poisson ratio does not play a role in the ﬁnal equation! This does
not mean, though, that clamped rubber plates and clamped steel plates bend in the
same way under the same load: the coefﬁcient d in (10.34) does depend on the
material (in addition to its dependence on the plate’s thickness). We can conclude
the surprising fact that for any given steel plate there is a rubber plate that bends in
exactly the same way. Equation (10.34) is the ﬁrst fourth-order equation we have
encountered so far in this book. As it turns out, fourth-order equations are rare
in applications; among the exceptions are the plate equation we just derived, the
equation for the vibrations of rods that we derive later in this chapter, and certain
equations in lens design.
10.1.1 Second variation
It is well known that equating the ﬁrst derivative of a real (scalar) function f (x)
to zero only provides a necessary condition for potential minimizers of f . To

10.1 Calculus of variations
291
determine whether a stationary point x0 (where f ′(x0) = 0) is indeed a local mini-
mizer, we have to examine higher derivatives of f . For example, if f ′′(x0) > 0, we
can conclude that indeed x0 is a local minimizer.
Similarly, to verify that a function u is a local minimum of some functional, we
must compute the second variation of the functional, and evaluate it at u. When
considering a general functional Q(u), the ﬁrst variation was deﬁned as
δQ(u)(ψ) := d
dε Q(u + εψ)

ε=0
for ψ in an appropriate function space. Similarly, if the ﬁrst variation of Q at u is
zero, we deﬁne the second variation of Q there through
δ2Q(u)(ψ) := d2
dε2 Q(u + εψ)

ε=0
.
(10.35)
Just like the case of the ﬁrst variation, the second variation is a functional of ψ that
depends on u.
For example, we consider the second variation of the Dirichlet functional G.
From (10.6) it follows at once that δ2G(v)(ψ) = G(ψ) > 0 for any functions v and
ψ. Therefore, the harmonic function u that we identiﬁed above as a candidate for a
minimizer is indeed a local minimizer. In fact, it can be shown that u is the unique
minimizer of G. Notice, however, that the association between minimizers of G
and the harmonic function was contingent on the harmonic function being a smooth
function!
A functional Q such that δ2Q(v)(ψ) > 0 for all appropriate v and ψ is called
(strictly) convex. Such functionals are particularly useful to identify since they have
a unique minimizer.
Is there always a unique minimum? This question has far reaching implications
in many branches of science and technology. In fact, it is also raised in unex-
pected disciplines such as philosophy and even theology. In contrast to the ethical
monotheism of the Prophets of Israel, the Hellenic monotheism was based on logi-
cal arguments, basically claiming that since God is the best, i.e. optimal, and since
the best must be unique, then there is only one god. This argument did not convince
the ancient Greeks (were they aware of the possibility of many local extrema?),
who stuck to their belief in a plurality of gods.
Indeed one of the intriguing questions raised by Plateau and many mathemati-
cians after him was whether the minimal surface problem has a unique solution for
any given spanning curve . The answer is no. In Figure 10.3 we depict an example
of a spanning curve  for which there exist more than one minimal surface.

292
Variational methods
Figure 10.3 An example of two distinct minimal surfaces spanned by the same curve.
10.1.2 Hamiltonians and Lagrangians
Newton founded his theory of mechanics in the second part of the seventeenth
century. The theory was based upon three laws postulated by him. The laws provided
a set of tools for computing the motion of bodies, given their initial positions and
initial velocities, by calculating the forces they exert on each other, and relating
these forces to the acceleration of the bodies. Motivated by the introduction of
steam machines towards the end of the eighteenth century and the beginning of the
nineteenth century, scientists developed the theory of thermodynamics, and with
it the important concept of energy. Then, in 1824 Hamilton started his systematic
derivation of an axiomatic geometric theory of light. He realized that his theory
is equivalent to a variational principle, called the Fermat principle, which states
that light propagates so as to travel between two arbitrary points in minimal time.
For example, the eikonal equation that we studied in Chapter 2 is related to the
Euler–Lagrange equation associated with this principle. During his optics research,
Hamilton observed that apparently different notions such as optical travel time and
energy are in fact related by another physical object called action. Moreover, he
showed that the entire theory of Newtonian mechanics can be formulated in terms of
actions and energies, instead of in terms of forces and acceleration. Hamilton’s new
theory, now called Hamilton’s principle, enabled the use of variational methods to
study not just static equilibria, but also dynamical problems.
We ﬁrst demonstrate Hamilton’s principle by applying it to a standard one-
dimensional problem in classical mechanics. Consider a discrete system of n in-
teracting particles whose location at time t is given by (x1(t), x2(t), . . . , xn(t)).
The kinetic energy is given by Ek(x1, . . . , xn) = 1
2
n
1 mi(dxi/dt)2, while the

10.1 Calculus of variations
293
potential energy is given by E p(x1, x2, . . . , xn). Since the force acting on parti-
cle i is Fi = −∇xi E p(x1, . . . , xn), Newton’s second law takes the form
d
dt
	
mi
dxi
dt

= −∇xi E p(x1, . . . , xn)
i = 1, 2, . . . , n.
(10.36)
To derive Hamilton’s principle we deﬁne the total energy of the system (called
the Hamiltonian) E = Ek + E p. We also deﬁne the Lagrangian of the system
L = Ek −E p. The action in Hamilton’s formalism is deﬁned as
J =
 t2
t1
Ldt,
(10.37)
where t1 and t2 are two arbitrary points along the time axis. Hamilton postulated
that a mechanical system evolves such that δJ = 0, where the variation is taken
with respect to all orbits (y1(t), . . . , yn(t)) such that
yi(t1) = xi(t1), yi(t2) = xi(t2),
i = 1, 2, . . . , n.
(10.38)
Computing the ﬁrst variation using (10.14), noting that the Lagrangian is a function
of the form
L = L
	
x1(t), . . . , xn(t), dx1
dt , dx2
dt , . . . , dxm
dt

,
we write
δJ =
 t2
t1
& n

1
	
mi
dxi
dt
dϕi
dt −∂E p
∂xi
ϕi

'
dt =
 t2
t1
&
−
n

1
	
mi
d2xi
dt2 −∂E p
∂xi

ϕi
'
dt =0,
(10.39)
where ϕi is the variation with respect to the particle xi, and we have used the fact
that the end point constraints (10.38) imply that ϕi(t1) = ϕi(t2) = 0. We have thus
found that the Newton equations (10.36) are the Euler–Lagrange equations for the
action functional.
The concept of the Lagrangian seems a bit odd at ﬁrst sight. The sum of the
kinetic and potential energies is the total energy, which is an intuitively natural
physical object. But why should we consider their difference? To give an intuitive
meaning to the difference Ek −E p it is useful to look a bit closer at the historical
development of mechanics. Although Newton wrote clear laws for the dynamics
of bodies, he and many other scientists looked for metaphysical principles behind
them. As the mainstream philosophy of the eighteenth century was based on the
idea of a single God, it was natural to assume that such a God would create a
world that is ‘perfect’ in some sense. This prompted the French scientist Pierre de
Maupertuis (1698–1759) to deﬁne the notion of action of a moving body. According

294
Variational methods
to Maupertuis, the action of a body moving from a to b is A =
 b
a p dx, where p is
the particle’s momentum. He then formulated his principle of least action, stating
that the world is such that action is always minimized. Converting this deﬁnition
of action to energy-related terms we write
A =
 b
a
p dx =
 b
a
m dx
dt dx =
 t2
t1
m
	dx
dt

2
dt = 2
 t2
t1
Ek dt.
Here t1 and t2 are the initial and terminal times for the particle’s path. The difﬁculty
with this formula is that it only includes the kinetic energy, while the motion is
determined by both the kinetic energy and the potential energy. Therefore, Lagrange
used the identity 2Ek = E + L to write the action as
A =
 t2
t1
(E + L) dt.
Since the energy is a constant of the motion, extremizing
 t2
t1 L dt is the same as
extremizing
 t2
t1 (E + L) dt.
We proceed to demonstrate Hamilton’s principle for a continuum. For this pur-
pose we return to the problem of the elastic string. We consider a string clamped at
the end points a and b, say u(a, t) = ua, u(b, t) = ub, where u(x, t) is the string’s
deviation from the horizontal rest position. The kinetic energy of the string is given
by Ek = 1
2
 b
a ρu2
t ds, where ρ(x, t) is the mass density, and ds =

1 + u2x dx is a
unit length element. The potential energy consists of the sum of the energy due to
the stretching of the string, and the work done against a load l(x, t):
Ep =
 b
a

d

1 + u2x −1

−lu

1 + u2x

dx,
where d(x, t) is the string’s elastic coefﬁcient, and l(x, t) is the load on the string.
Notice that we allow the density, the elastic coefﬁcient, and the load to depend on
x and t. The action is thus given by
J =
 t2
t1
 b
a
1
2

1 + u2x ρu2
t −d

1 + u2x −1

+ lu

1 + u2x

dxdt. (10.40)
Consider variations u + εψ such that ψ vanishes at the string’s end points a and b,
and also at the initial and terminal time points t1 and t2. Neglecting the term that is
cubic in the derivatives ux, ut we get for the ﬁrst variation:
δJ =
 t2
t1
 b
a

1 + u2x ρutψt −d(1 + u2
x)−1/2uxψx + l

1 + u2xψ

dxdt.
(10.41)

10.1 Calculus of variations
295
We further integrate by parts the terms utψt (with respect to the t variable) and
uxψx (with respect to the x variable). The boundary conditions speciﬁed above
for the variation ψ imply that all the boundary terms (both spatial and temporal)
vanish. Therefore, equating the ﬁrst variation to zero and integrating by parts we
obtain
δJ =
 t2
t1
 b
a
0
(−

1 + u2x ρut)t + [d(1 + u2
x)−1/2ux]x + l

1 + u2x
1
ψ dxdt =0.
(10.42)
The last equation implies the dynamical equation for the string’s vibrations
(ρut)t −(1 + u2
x)−1/2[d(1 + u2
x)−1/2ux]x −l = 0.
(10.43)
If we assume that ρ and d are constants, and use the small slope approximation
|ux| ≪1, we obtain again the one-dimensional wave equation.
Remark 10.5 The observant reader may have noticed that our nonlinear string
model (10.43) is different from the string model (1.28) that we derived in Chapter 1.
One difference is that in the current model we have allowed for variation of the
mass density ρ and the elasticity coefﬁcient d in space and time. A more subtle
difference is in the form of the nonlinearity. The string model in Chapter 1 is based
on the constitutive law (1.27). The model in this section is based on the action
(10.40). It can be shown that this action is equivalent to a constitutive law of the
form
⃗T = ˆeτ,
(10.44)
i.e. the tension is assumed to be uniform across the string. The model (1.27) can be
called a “spring-like” string, while the model (10.44) can be called an “inextensible”
spring.
Example 10.6 Vibrations of rods The potential energy of the string is stored in
its stretching i.e. a string resists being stretched. We deﬁne a rod as an elastic body
that also resists being bent. This means that we have to add to the elastic energy of
the string a term that penalizes bending. The amount of bending of a curve f (x) is
measured by its curvature:
κ(x) =
fxx
(1 + f 2x )3/2 .
Therefore, the Lagrangian for a rod under a load l can be written as
L =
 b
a
1
2

1 + u2xρu2
t −d1
2
u2
xx
(1 + u2x)2 −d2

1 + u2x −1

+ lu

1 + u2x

dx.
(10.45)

296
Variational methods
To simplify the computation in this example we introduce the small slopes (|ux| ≪
1) assumption at the outset. We thus approximate the action by
J =
 t2
t1
 b
a
	1
2ρu2
t −d1
2 u2
xx −d2
2 u2
x + lu

dxdt.
(10.46)
Computing the ﬁrst variation we ﬁnd
δJ =
 t2
t1
 b
a
(ρutψt −d1uxxψxx −d2uxψx + lψ) dxdt.
(10.47)
In order to obtain the Euler–Lagrange equation we need to integrate the last integral
by parts. Just as in the case of the plate, we assume that the rod is clamped, i.e. we
specify u and ux at the end points a and b. Therefore, the variation ψ vanishes at
the spatial and temporal end points, and in addition, ψx vanishes at a and b. We
thus obtain that the vibrations of rods are determined by the equation
(ρut)t −(d2ux)x + (d1uxx)xx −l = 0.
(10.48)
In Exercise 10.9 the reader will use the separation of variables method to solve the
initial boundary value problem for equation (10.48).
10.2 Function spaces and weak formulation
In Chapter 6 we deﬁned the notion of inner product spaces. We also introduced
there concepts such as norms, complete orthonormal sets, and generalized Fourier
expansions. In this section we take a few further steps in this direction. We shall
introduce the concept of Hilbert spaces and show how to use it in the analysis of
optimization problems and PDEs. We warn the reader that this is a small section
dealing with an extensive subject. One of the main difﬁculties is that we are dealing
with function spaces. Not only do these spaces turn out to be of inﬁnite dimension,
but the very meaning of “function” and “integral” must be very carefully examined.
Therefore, our presentation will be minimal and conﬁned to the basic facts that are
essential in applications. We recommend [7] for more extensive exposition of the
subject.
Let V be a (real) inner product space of functions deﬁned over a domain D ⊂Rn.
We have already seen in Chapter 9 that in such a space there is a well-deﬁned
(induced) norm, ∥f ∥:= ⟨f, f ⟩1/2 for f ∈V . We used the norm to deﬁne (Deﬁ-
nition 6.9) convergence in the mean. We shall need later to deﬁne an alternative
notion of convergence that is called weak convergence. To better distinguish be-
tween the different types of convergence, we shall replace ‘convergence in the
mean’ by the title strong convergence. So, a sequence {vn}∞
n=1 converges strongly
to v, if limn→∞∥vn −v∥= 0.

10.2 Function spaces and weak formulation
297
A natural example for an inner product space is the space of all functions
in a bounded domain D that are continuous in ¯D, equipped with the inner
product
⟨f, g⟩=

D
f (⃗x)g(⃗x) d⃗x.
(10.49)
So far the deﬁnitions of an inner product space and the norms induced by the
inner product are quite similar to the same notions in linear algebra. It is tempting,
therefore, to proceed and borrow further ideas from linear algebra in developing
the theory of function spaces. One of the most useful objects in linear algebra is
a basis for a vector space. Indeed, intuitively, the generalized Fourier series we
wrote in Chapter 6 looks like an expansion with respect to a basis that consists
of the system of eigenfunctions of the given Sturm–Liouville problem. In order
actually to deﬁne a basis in a function space we must overcome a serious obstacle,
namely that the space must be ‘complete’ in an appropriate sense. To explain what
we have in mind, recall again the example of the function space V consisting
of the continuous functions over ¯D under the inner product (10.49). Can we say
that the eigenfunctions of the Laplace operator in D form a basis for this space?
As a more concrete example, consider the function space VE consisting of all
continuous functions deﬁned on [0, π] that vanish at the end points. Can we say
that the sequence {sin nx} is a basis for this space? To answer this question, recall
from linear algebra that if B is a basis of an n-dimensional vector space V , then it
spans V and, in particular, every linear combination of vectors in B is an element in
V . Since we expect, from the examples above, that in the case of function spaces a
basis will consist of inﬁnitely many terms, we have to be slightly more careful and
require that B be a linearly independent set that ‘spans’ V , and that each sequence in
V whose terms are inﬁnitesimally close (in norm) to each other, strongly converges
to a vector in V . Note the latter condition is the completeness condition on the
space V .
As it turns out, however, if we consider the space VE above, and the candidate for
a basis BE = {sin nx}, then there exist such linear combinations of functions in BE
that do not converge to a continuous function. In fact, this is not a surprise for us;
we computed in Chapters 5 and 6 examples in which the Fourier series converged
to discontinuous functions. This means that the function space VE is not complete
in some sense. Therefore, we now set about completing it.
For this purpose we recall from calculus the concept of a Cauchy sequence.
A sequence of functions { fn} in an inner product space V is said to be a Cauchy
sequence if for each ε > 0 there exists N = N(ε) such that ∥fn −fm∥< ε when-
ever n, m > N. We note that every (strongly) converging sequence in V is a Cauchy
sequence.

298
Variational methods
We proceed to construct out of an inner product space V a new space that consists
of all the Cauchy sequences of vectors (functions) in V. Note that this space contains
V since for any f ∈V , the constant sequence { f } is a Cauchy sequence. It turns out
that the resulting space H, the completion of V , is an inner product space that has
the property that every Cauchy sequence in H has a limit which is also an element
of H. We can now introduce the following deﬁnition.
Deﬁnition 10.7 An inner product space in which every Cauchy sequence converges
is said to be complete. Complete inner product spaces are called Hilbert spaces in
honor of the German mathematician David Hilbert (1862–1943).2
In our example above, we constructed a Hilbert space out of the space VE. The
Hilbert space thus constructed is denoted L2[0, π] (or just L2 if the domain under
consideration is clear from the context). The construction implies in particular that
every function in L2 is either continuous or can be approximated (in the sense of
strong convergence) to arbitrary accuracy by a continuous function.
Deﬁnition 10.8 A set W of functions in a Hilbert space H with the property that
for every f ∈H and for every ε > 0 there exists a function fε ∈W such that
∥f −fε∥< ε is called a dense set.
Thus, the set of continuous functions on [0, π] is dense in L2[0, π]. Our dis-
cussion on the construction of Hilbert spaces has been heuristic. In particular it is
not obvious that the space we completed out of VE is still an inner product space.
Nevertheless, it can be shown that essentially every inner product space VI can be
extended uniquely into a Hilbert space HI such that VI is dense in HI, and such that
the inner product ⟨f, g⟩VI of VI is extended into an inner product ⟨φ, ψ⟩HI for the
elements of HI, such that if f, g ∈VI, we have
⟨f, g⟩VI = ⟨f, g⟩HI.
We are now ready to deﬁne a basis in a Hilbert space.
Deﬁnition 10.9 A set B of functions in a Hilbert space H is said to be a basis of
H if its vectors are linearly independent and the set of ﬁnite linear combinations of
functions from B is dense in H.
2 If you ﬁnd the concept of Hilbert space hard to grasp, then you are not alone. Hilbert was a professor at G¨ottingen
University which was a center of mathematics research from the days of Gauss and Riemann up until the mid-
1930s. The Hungarian–American mathematician John von Neumann (1903–1957), one of the founders of the
modern theory of function spaces, visited G¨ottingen in the mid-1920s to give a lecture on his work. The legend
is that shortly into the lecture Hilbert raised his hand and asked, “Herr von Neumann, could you explain to us
again what a Hilbert space is?”

10.2 Function spaces and weak formulation
299
Example 10.10 In Chapter 6, we stated below Proposition 6.30 that for a given
regular (or periodic) Sturm–Liouville problem on [a, b], the eigenfunction ex-
pansion of a function u that satisﬁes
 b
a u2(x)r(x) dx < ∞(r is the correspond-
ing weight function) converges strongly to u. In other words, the orthonormal
system of all eigenfunctions of a given regular (or periodic) Sturm–Liouville
problem forms a basis in the Hilbert space of all functions such that the norm
∥u∥r := (
 b
a u2(x)r(x) dx)1/2 is ﬁnite. In particular, the system {sin nx}∞
n=1 (or
{cos nx}∞
n=0) is a basis of L2[0, π].
Remark 10.11 Since any orthonormal sequence is a linearly independent set,
Proposition 6.13 implies that a complete orthonormal sequence in a Hilbert space
is a basis.
As another example of a Hilbert space that is particularly useful in the theory of
PDEs we consider the space C1(D) equipped with the inner product
⟨u, v⟩=

D
(uv + ∇u · ∇v) d⃗x.
(10.50)
The special Hilbert space obtained from the completion process of the space
above is called a Sobolev space after the Russian mathematician Sergei Sobolev
(1908–1989). It is denoted by H1(D). Just like the case of the space L2, the set
of continuously differentiable functions in D is dense in H1(D). Other examples
of Hilbert spaces are obtained for functions with special boundary behavior, for
instance functions that vanish on the boundary of D.
What is the theory of Hilbert space we elaborated on good for? We shall now
consider several applications of it.
10.2.1 Compactness
When we studied in calculus the problem of minimizing real valued functions,
we had at our disposal a theorem that guaranteed that a continuous function in a
closed bounded set K must achieve its maximum and minimum in K. Establishing
a priori the existence of a minimizer for a functional is much harder. To understand
the difﬁculty involved, let us recall from calculus that if A is a set of real numbers
bounded from below, then it has a well-deﬁned inﬁmum. Moreover, there exists
at least one sequence an ⊂A that converges to the inﬁmum. Consider now, for
example, the Dirichlet integral G(u) deﬁned over the functions in
B = {u ∈C1(D) ∩C( ¯D), u = g
⃗x ∈∂D}

300
Variational methods
for some domain D. Clearly G is bounded from below by 0. Therefore, there exists
a sequence {uk} such that
lim
k→∞G(uk) = inf
u∈B G(u).
Such a sequence {uk} is called a minimizing sequence.
The trouble is that a priori it is not clear that the inﬁmum is achieved, and in fact,
it is not even clear that the minimizing sequence uk has convergent subsequences
in B. Achieving the inﬁmum is not always possible even for a sequence of numbers
(for example if they are deﬁned over an open interval), but we do like to retain some
sort of convergence. In Rn we know that any bounded sequence has at least one
convergent subsequence. This is the compactness property of bounded sets in Rn.
Is it also true for the space B? The answer is no. We have seen examples in
which a Fourier series converges strongly to a discontinuous function. This is
a case in which a sequence of functions in B – the partial sums of the Fourier
series – does not have any subsequence converging to a function in B. In Exer-
cise 10.11 the reader will show that any orthonormal (inﬁnite) sequence in a given
inﬁnite-dimensional Hilbert space H is bounded, but does not admit any subse-
quence converging strongly to a function in H.
It turns out that, if we consider inﬁnite bounded sequences of functions in Hilbert
spaces, we can still maintain to some extent the property of compactness. Unfortu-
nately we have to weaken the meaning of convergence.
Deﬁnition 10.12 A sequence of functions { fn} in a Hilbert space H is said to
converge weakly to a function f in H if
lim
n→∞⟨fn, g⟩= ⟨f, g⟩
∀g ∈H.
(10.51)
Note that by the Riemann–Lebesgue lemma (see (6.38)), any (inﬁnite) orthonormal
sequence in a given inﬁnite-dimensional inner product space converges weakly
to 0.
The following theorem explains why we call the property (10.51) weak conver-
gence, and also provides the fundamental compactness property of Hilbert spaces.
Theorem 10.13 Let H be a Hilbert space. The following statements hold:
(a) Every strongly convergent sequence {un} in H also converges weakly. The converse is
not necessarily true.
(b) If {un} converges weakly to u, then
∥u∥≤lim inf
n→∞∥un∥.
(10.52)
(c) Every sequence {un} in H that is bounded (in the sense that ∥un∥H ≤C) has at least
one weakly convergent subsequence.

10.2 Function spaces and weak formulation
301
(d) Every weakly convergence sequence in H is bounded.
Proof We only prove parts (a) and (b):
(a) We need to show that if ∥un −u∥→0 in H, then {un} converges weakly to u. For this
purpose we write for an arbitrary function f ∈H
|⟨un, f ⟩−⟨u, f ⟩| = |⟨un −u, f ⟩| ≤∥un −u∥1/2∥f ∥1/2,
(10.53)
where the last step follows from the Cauchy–Schwartz inequality (see (6.8)).
The second part of (a) follows from a counterexample. Consider the sequence
{sin nx} ⊂L2([0, π]). Then by the Riemann–Lebesgue lemma {sin nx} converges
weakly to 0, while ∥sin nx∥= √π/2 and therefore, {sin nx} does not converge strongly
to 0.
(b) If {un} converges weakly to u, then, in particular, ⟨un, u⟩→∥u∥2. By the Cauchy–
Schwartz inequality, it follows that
∥u∥2 = lim
n→∞|⟨un, u⟩| ≤∥u∥lim inf
n→∞∥un∥.
(10.54)
□
We have therefore shown that it is useful to work in Hilbert spaces to guarantee
compactness in some sense. It still remains to show in applications that a given
minimizing sequence is indeed bounded and thus admits a weakly convergence
subsequence, and that at least one of its limits indeed achieves the inﬁmum for the
underlying functional. We shall demonstrate all of this through an example below.
10.2.2 The Ritz method
Consider the problem of minimizing a functional G(u), where u is taken from
some Hilbert space H. The Ritz method is based on selecting a basis B (preferably
orthonormal) for H, and expressing the unknown minimizer u in terms of the
elements φn of B:
u =
∞

n=1
αnφn.
(10.55)
The functional minimization problem has been transformed to an algebraic (albeit
inﬁnite-dimensional) minimization problem in the unknown coefﬁcients αn. This
process is similar to our discussion after the introduction of the Rayleigh quotient
in Chapters 6 and 9.
Practically, we can use the fact that since the series expansion for u is convergent,
we expect the coefﬁcients to decay as n →∞. We can therefore truncate the

302
Variational methods
expansion at some ﬁnite term N and write
u ≈
N

n=1
αnφn.
(10.56)
This approximation leads to a ﬁnite-dimensional algebraic system that can be han-
dled by a variety of numerical tools as discussed in Chapter 11.
Remark 10.14 A very interesting question is: what would be an optimal basis?
It is clear that some bases are superior to others. For example, the series (10.55)
might converge much faster in one basis than in another basis. In fact, the series
might even be ﬁnite if we are fortunate (or clever). For instance, suppose that we
happened to choose a basis that contains the minimizing function u itself. Then the
series expansion would consist of just one term!
At the other extreme, we might face the problem of not having any obvious
candidate for a basis. This would happen when we consider a Hilbert space of
functions deﬁned over a general domain that has no symmetries. We shall address
this question in Chapter 11.
Example 10.15 To demonstrate the Ritz method we return to the problem of phase
reconstruction (Example 10.2). In typical applications D is the unit disk. We shall
seek the minimizer of K(u) in the space H1(D). What would be a good basis for
this space? The ﬁrst candidate that comes to mind is the basis
0
Jn
αn,m
a r

cos nθ
1
∪
0
Jn
αn,m
a r

sin nθ
1
that we constructed in (9.80). While this basis would certainly do the work, it turns
out that in practice physicists use another basis. Phase reconstruction is an impor-
tant step in a process called adaptive optics, in which astronomers correct images
obtained by telescopes. These images are corrupted by atmospheric turbulence (this
is similar to scintillation of stars when they are observed by a naked eye). Thus
astronomers measure the phase and use these measurements to adjust ﬂexible mir-
rors to correct the image. The Dutch physicist Frits Zernike (1888–1966) proposed
in 1934 to expand the phase in a basis in which he replaced the Bessel functions
above by radial functions that are polynomials in r.
The Zernike basis for the space L2 over the unit disk consists of functions that
have the same angular form as the Bessel basis above. The radial Bessel functions,
though, are replaced by orthogonal polynomials. Using complex number notation,
we write the Zernike functions as
Zm
n (r, θ) = Rm
n (r) eimθ,
(10.57)

10.2 Function spaces and weak formulation
303
where the polynomials Rm
n are orthogonal over the interval (0, 1) with respect
to the inner product ⟨f (r), g(r)⟩:=
 1
0 f (r)g(r)r dr. For some reason Zernike
did not choose the polynomials to be orthonormal, but rather set ⟨Rm
n , Rm
n′⟩=
[1/2(n + 1)]δn,n′. In fact, one can write the polynomials explicitly (they are only
deﬁned for n ≥|m| ≥0):
Rm
n (r)=



(n−|m|)/2
l=0
(−1)l(n −l)!
l!
 1
2(n + |m|) −l

!
 1
2(n −|m|) −l

! rn−2l for n −|m| even,
0
for n −|m| odd.
(10.58)
The phase is expanded in the form
u(r, θ) =

n,m
αn,m Zm
n (r, θ).
Wethensubstitute this expansion into the minimizationproblem(10.19)toobtainan
inﬁnite-dimensional quadratic minimization problem for the unknown coefﬁcients
{αn,m}. In practice the series is truncated at some ﬁnite term, and then, since the
functional is quadratic in the unknown coefﬁcients, the minimization problem is
reduced to solving a system of linear algebraic equations. Notice that this method
has a fundamental practical ﬂaw: since the functional involves derivatives of u, and
the derivatives of the Zernike functions are not orthogonal, we need to evaluate
all the inner products of these derivatives. Moreover, this implies that the matrix
associated with the linear algebraic system we mentioned above is generically full;
in contrast we shall show in Chapter 11 that if we select a clever basis, we can obtain
linear algebraic systems that are associated with sparse matrices, whose solution
can be computed much faster.
10.2.3 Weak solutions and the Galerkin method
We shall use the following example to illustrate some of the ideas that have been
developed in this chapter and also to introduce the concept of weak formulation.
Example 10.16 Consider the minimization problem
min Y(u) =

D
	1
2|∇u|2 + 1
2u2 + f u

d⃗x,
(10.59)
where D is a (bounded) domain in Rn and f is a given continuous function satis-
fying without loss of generality | f | ≤1 in D. The ﬁrst variation is easily found to
be
δY(u) =

D
(∇u · ∇ψ + uψ + f ψ) d⃗x.
(10.60)

304
Variational methods
We seek a minimizer in the space H1(D), and take the variation ψ also to belong
to this space. Therefore, the condition on the minimizer u is

D
(∇u · ∇ψ + uψ + f ψ) d⃗x = 0
∀ψ ∈H1(D).
(10.61)
If we assume that the minimizer u is a smooth function (i.e. in the class C2( ¯D))
and that D has a smooth boundary, then we can integrate (10.61) by parts in the
usual way and obtain the Euler–Lagrange equation
−u + u = −f
⃗x ∈D,
∂nu = 0
⃗x ∈∂D.
(10.62)
Equation (10.61), however, is more general than (10.62) since it also holds under
the weaker assumption that u is only once continuously differentiable, or at least is a
suitablelimitoffunctionsinC1(D).Therefore,wecall(10.61)theweakformulation
of (10.62). We prove the following statement.
Theorem 10.17 The weak formulation (10.61) has a unique solution u∗. Moreover,
u∗is a minimizer of (10.59).
Proof Since | f | ≤1, then 1
2u2 + u f ≥−1
2 f 2 ≥−1
2 for all ⃗x ∈D. Therefore,
Y(u) ≥−1
2|D| and thus the functional is bounded from below. Let {un} be a mini-
mizing sequence, i.e.
lim
n→∞Y(un) = I :=
inf
u∈H1(D) Y(u).
The Cauchy–Schwartz inequality implies that |

D f ud⃗x| ≤|D|1/2∥u∥L2(D).
Since it sufﬁces to consider un such that Y(un) < Y(0) = 0, it follows that
1
2∥un∥2
L2(D) ≤1
2∥un∥2
H1(D) ≤


D
f un d⃗x
 ≤|D|1/2∥un∥L2(D).
Therefore, ∥un∥L2(D) < C := 2|D|1/2, which in turn implies that ∥un∥H1(D) < C.
Thus, Theorem 10.13 implies that {un} has at least one weakly convergent subse-
quence {unk} in H1(D). We denote its weak limit by u∗.
Using (10.52) and the fact that weak convergence in H1(D) implies weak con-
vergence in L2(D) (see Exercise 10.12), it follows that
Y(u∗) = 1
2∥u∗∥2
H1(D) +

D
u∗f d⃗x
≤1
2 lim inf
n→∞∥un∥2 + lim
n→∞

D
un f d⃗x = lim
n→∞Y(un) = I ≤Y(u∗).
Therefore, u∗is a minimizer of the problem.

10.2 Function spaces and weak formulation
305
Now ﬁx ψ ∈H1. Then
g(ε) := Y(u∗+ εψ) = Y(u∗) + ε2
2 ∥ψ∥2
H1(D) + ε⟨u∗, ψ⟩H1(D) + ε

D
ψ f d⃗x
has a minimum at ε = 0, therefore, g′(0) = 0. Hence
⟨u∗, ψ⟩H1(D) = −

D
f ψ d⃗x.
(10.63)
Since (10.63) holds for all ψ ∈H1(D), we have established the existence of a
solution of the weak formulation. To prove the uniqueness of the solution, we
assume by contradiction that there exist two solutions u∗
1 and u∗
2. We then form
their difference v∗= u∗
1 −u∗
2, and obtain for v∗:
⟨v∗, ψ⟩H1(D) = 0
∀ψ ∈H1(D).
(10.64)
In particular, we can choose ψ = v∗, and then (10.64) reduces to ∥v∗∥H1(D) = 0,
implying v∗≡0.
□
If we can prove that v∗is in C2(D) ∩C1( ¯D), then Theorem 10.17 implies the
existence of a classical solution to the elliptic boundary value problem (10.62).
Although we have proved that the weak formulation has a unique solution, the
proof was not constructive. The limit u∗was identiﬁed as a limit of an as yet un-
known sequence. We therefore introduce now a practical method for computing
the solution. The idea is to construct a chain of subspaces H (1), H (2), . . . , H (k), . . .
with the property that H (k) ⊂H (k+1), and dim H (k) = k, such that their union ex-
hausts the full H1(D), i.e. there exists a basis {φk} of H1(D) with φk ∈H (k). In each
subspace H (k), we select a basis φk
1, φk
2, . . . , φk
k. We write the weak formulation in
H (k) as
⟨vk, φk
i ⟩H1(D) = −

D
f φk
i d⃗x
i = 1, 2, . . . , k.
(10.65)
If we further express the unknown function vk in terms of the basis φk, i.e. vk =
k
j=1 αk
jφk
j, we obtain for the unknown coefﬁcient vector ⃗αk the algebraic equations
k

j=1
K k
i jαk
j = di
i = 1, 2, . . . , k,
(10.66)
where
K k
i j = ⟨φk
i , φk
j⟩H1(D),
and
di = −

D
f φk
i d⃗x.
(10.67)
It can be shown (although we shall not do it here) that the system (10.66) has a
unique solution for all k, and that the sequence vk converges strongly to u∗.

306
Variational methods
The practical method we presented for computing u∗is called the Galerkin
method after the Russian engineer Boris Galerkin (1871–1945). In Exercise 10.10
the reader will show that for the minimization problem at hand the Galerkin method
is identical to the Ritz method introduced earlier. This is why these methods are
often confused (or maybe just fused. . .) with each other and go together under the
title the Galerkin–Ritz method. We point out, however, that the Galerkin method
is more general than the Ritz method in the sense that it is not limited to problems
where the weak formulation is derived from a variation of a functional. In fact,
given any PDE of the abstract form L[u] = f , where L is a linear or nonlinear
operator, we can apply the Galerkin method by writing the equation in the form
⟨L[u] −f, ψ⟩= 0
∀ψ ∈H,
where H is a suitable Hilbert space. Sometimes, we can then integrate the left hand
side by parts and throw some derivatives of u to ψ and thus obtain a formulation
that requires less regularity for its solution.
There still remains the important question of how to choose the subspaces H k
that we used in the Galerkin method. A very important class of such subspaces
forms a numerical method called ﬁnite elements, which will be discussed in more
detail in Chapter 11.
10.3 Exercises
10.1 Consider the variational problem
min K(y) :=
 1
0
[1 + y′(t)2] dt,
(10.68)
where y ∈C1([0, 1]) satisﬁes y(0) = 0, y(1) = 1. Find the Euler–Lagrange equa-
tion, the boundary conditions, and the minimizer for this problem. Is the minimizer
unique?
10.2 Consider the variational problem
min K(u) :=

D
1
2|∇u|2 + αuxxuyy + (1 −α)u2
xy

dxdy,
(10.69)
where α is a real constant. Find the Euler–Lagrange equation, and the natural bound-
ary conditions for the problem.
10.3 Consider the variational problem
min K(u) :=

D
	
|∇u|2 + 1
2gu4

dxdy,
(10.70)
where D ⊂R2, and g(x, y) is a given positive function. Find the Euler–Lagrange
equation and the natural boundary conditions for this problem.

10.3 Exercises
307
10.4 Can you guess a third minimal surface spanned by the spatial curve of Figure 10.3?
10.5 A canonical physical model concerns a system whose kinetic energy and potential
energy are of the forms
Ek = 1
2

D
u2
t d⃗x
E p =

D
1
2|∇u|2 + V (u)

d⃗x.
Here u(⃗x, t) is a function that characterizes the system, D is a domain in R3, and V
is a known function.
(a) Write the Lagrangian and the action for the system.
(b) Equating the ﬁrst variation of the action to zero, ﬁnd the dynamical PDE obtained
by Hamilton’s principle.
Comment The PDE that you ﬁnd in (b) is called the Klein–Gordon equation (see
(9.19)).
10.6 Suppose that
p, p′, q,r ∈C([a, b]),
p(x),r(x) > 0,
∀x ∈[a, b].
(a) Write the Euler–Lagrange equation for the following constrained optimization
problem:
min
 b
a

p(x)y′(x)2 −q(x)y(x)

dx,
subject to
 b
a
r(x)y(x)2 dx = 1,
(10.71)
where y satisﬁes the boundary conditions y(a) = y(b) = 0.
Hint Use a Lagrange multiplier method to replace (10.71) with a minimization prob-
lem without constraints.
(b) What is the relation between the calculation you performed in (a) and the Rayleigh
quotient (6.71)?
10.7 (a) Let D be a domain in R2. Write the Euler–Lagrange equation for the following
constrained optimization problem:
min

D
|∇u|2 dxdy,
subject to

D
u2 dxdy = 1,
u = 0 x ∈∂D.
(10.72)
(b) What is the relation between the calculation you performed in (a) and the
Rayleigh–Ritz formula (9.53)?
10.8 Use the Hamilton principle and the energy functional for the membrane (see Example
10.3) to compute the equation for the vibration of a membrane with an elasticity
constant d and a ﬁxed density ρ in the small slope approximation. What are the
eigenfrequencies of the membrane in the rectangle [0, 1] × [0, 8]?
10.9 Consider the vibrations of a rod (equation (10.48)) clamped at x = 0 and x = b, with
d2 = 0, d1 = d for some constant d, and ρ = 1.
(a) Write separated solutions of the form u(x, t) = X(x)T (t). Denote the eigenvalues
of the eigenvalue problem for X by λn. Write explicitly the eigenvalue problem.
(b) Show that all eigenvalues are positive.

308
Variational methods
(c) Show that the eigenvalues are the solutions of the transcendental equation
cosh αb cos αb = 1,
(10.73)
where α = λ1/4. What is the asymptotic behavior of the nth eigenvalue as n →∞?
10.10 Analyze the minimization problem (10.59) by the Ritz method. Use the same bases
{φk} as in the Galerkin method, and show that the Ritz method and the Galerkin
method give rise to the same algebraic equation.
10.11 Let {vn} be an orthonormal inﬁnite sequence in a given inﬁnite-dimensional Hilbert
space H.
(a) Show that {vn} is bounded.
(b) Show that {vn} converges weakly to 0.
(c) Show that {vn} does not admit any subsequence converging strongly to a function
in H.
Hint Use the Riemann–Lebesgue lemma (see (6.38)).
10.12 Prove that if {un} is a weakly converging sequence in H1(D), then {un} weakly
converges in L2(D).

11
Numerical methods
11.1 Introduction
In the previous chapters we studied a variety of solution methods for a large number
of PDEs. We point out, though, that the applicability of these methods is limited to
canonical equations in simple domains. Equations with nonconstant coefﬁcients,
equations in complicated domains, and nonlinear equations cannot, in general, be
solved analytically. Even when we can produce an ‘exact’ analytical solution, it
is often in the form of an inﬁnite series. Worse than that, the computation of each
term in the series, although feasible in principle, might be tedious in practice, and,
in addition, the series might converge very slowly. We shall therefore present in
this chapter an entirely different approach to solving PDEs. The method is based
on replacing the continuous variables by discrete variables. Thus the continuum
problem represented by the PDE is transformed into a discrete problem in ﬁnitely
many variables. Naturally we pay a price for this simpliﬁcation: we can only obtain
an approximation to the exact answer, and even this approximation is only obtained
at the discrete values taken by the variables.
The discipline of numerical solution of PDEs is rather young. The ﬁrst analysis
(and,infact,alsotheﬁrstformulation)ofadiscreteapproachtoaPDEwaspresented
in 1929 by the German-American mathematicians Richard Courant (1888–1972),
Kurt Otto Friedrichs (1901–1982), and Hans Lewy (1905–1988) for the special
case of the wave equation. Incidentally, they were not interested in the numerical
solution of the PDE (their work preceded the era of electronic computers by almost
two decades), but rather they formulated the discrete problem as a means for a
theoretical analysis of the wave equation. The Second World War witnessed the
introduction of the ﬁrst computers that were built to solve problems in continuum
mechanics. Following the war and the rapid progress in the computational power
of computers, it was argued by many scientists that soon people would be able to
solve numerically any PDE. Thus, von Neumann envisioned the ability to obtain
309

310
Numerical methods
long-term weather prediction by modeling the hydrodynamical behavior of the
atmosphere. These expectations turned out to be too optimistic for several reasons:
(1) Many nonlinear PDEs suffer from inherent instabilities; a small error in estimating the
equation’s coefﬁcients, the initial conditions, or the boundary conditions may lead to
a large deviation of the solution. Such difﬁculties are currently investigated under the
title ‘chaos theory’.
(2) Discretizing a PDE turns out to be a nontrivial task. It was discovered that equations
of different types should be handled numerically differently. This problem led to the
creation a new branch in mathematics: numerical analysis.
(3) Each new generation of computers brings an increase in computational power and has
been accompanied by an increased demand for accuracy. At the same time scientists
develop more and more sophisticated physical models. These factors result in a never-
ending race for improved numerical methods.
We pointed out earlier that a numerical solution provides only an approximation
to the exact solution. In fact, this is not such a severe limitation. In many situations
there is no need to know the solution with inﬁnite accuracy. For example, when
solving a heat conduction problem it is rarely required to obtain an answer with
an accuracy better than a hundredth of a degree. In other words, an exact answer
provides more information than is actually required. Moreover, even if we can write
an exact answer in terms of trigonometric functions or special functions, we can
only evaluate these functions to some ﬁnite accuracy.
As we stated above, the main idea of a numerical method is to replace the PDE,
formulated for one unknown real valued function, by a discrete equation in ﬁnitely
many unknowns. The discrete problem is called a numerical scheme. Thus a PDE
is replaced by an algebraic equation. When the original PDE is linear, we obtain,
in general, a system of linear algebraic equations. We shall demonstrate below
that the accuracy of the solution depends on the number of discrete variables, or,
alternatively, on the number of algebraic equations. Therefore, seeking an accurate
approximation requires us to solve large algebraic systems.
There are several techniques for converting a PDE into a discrete problem. We
have already mentioned in Chapter 10 the Ritz method that is suitable for equations
arising from optimization problems. The main difﬁculty in the Ritz method is
in ﬁnding a good basis for problems in domains that are not simple (where, for
example, the eigenfunctions for the Laplacian are not easy to calculate). The most
popular numerical methods are the ﬁnite difference method (FDM) and the ﬁnite
elements method (FEM). Both methods can be used for most problems, including
equations with constant or nonconstant coefﬁcients, equations in general domains,
and even nonlinear equations. Because of the limited scope of the discussion in this
book we shall only introduce the basic ideas behind these two methods. There is an

11.2 Finite differences
311
on-going debate on whether one of the methods is superior to the other. Our view is
that the FDM is simpler to describe and to program (at least for simple equations).
The FEM, on the other hand, is somehow ‘deeper’ from the mathematical point of
view, and is more ﬂexible when solving equations in complex geometries.
We end this section by noting that we shall discuss the prototypes of second-order
equations (heat, Laplace, wave). In addition we choose simple domains in order to
simplify the presentation. Nevertheless, unlike the analytical methods introduced
in the preceding chapters, the numerical methods provided here are not limited to
symmetric domains and they can be applied in far more complex situations.
11.2 Finite differences
To present the principle of the ﬁnite difference approximation, consider a smooth
function in two variables u(x, y), deﬁned over the rectangle D = [0, a] × [0, b].
We further deﬁne a discrete grid (mesh; net) of points in D:
(xi, y j) = (ix, jy) 0 ≤i ≤N −1,
0 ≤j ≤M −1,
(11.1)
where x = a/(N −1), y = b/(M −1). Since we are interested in the values
taken by u at these points, it is convenient to write
Ui, j = u(xi, y j).
We use the Taylor expansion of u around the point (xi, y j) to compute u(xi+1, y j)
(see Figure 11.1). We obtain
u(xi+1, y j) = u(xi, y j) + ∂xu(xi, y j)x
+ 1
2∂2
xu(xi, y j)(x)2 + 1
6∂3
xu(xi, y j)(x)3+ · · · ·
(11.2)
It follows that
∂xu(xi, y j) = Ui+1, j −Ui, j
x
+ O(x).
(11.3)
We obtained the following approximation for the partial derivative of u with respect
to x which is called a forward difference formula:
∂xu(xi, y j) ∼Ui+1, j −Ui, j
x
.
(11.4)
i
i+1
i-1
Figure 11.1 A one-dimensional grid.

312
Numerical methods
Similarly, one can derive a backward difference formula
∂xu(xi, y j) ∼Ui, j −Ui−1, j
x
.
(11.5)
The error induced by the approximation (11.4) is called a truncation error. To
minimize the truncation error, and thus to obtain a more faithful approximation for
the derivative, we need x to be very small. Since x = O(1/N), this requirement
implies that N should be very big. We shall see below that working with large values
of N (very ﬁne grids) is expensive in terms of computational time as well as in terms
of memory requirements. We therefore seek a ﬁnite difference approximation for ux
that is more accurate than (11.4). For this purpose write also the Taylor expansion
for u(xi−1, y j), and subtract the two Taylor expansions to obtain
∂xu(xi, y j) = Ui+1, j −Ui−1, j
2x
+ O((x)2).
(11.6)
The approximation
∂xu(xi, y j) ∼Ui+1, j −Ui−1, j
2x
(11.7)
is called a central ﬁnite difference or, for short, a central difference. For obvious
reasons we say that it is a second-order approximation for ux. Similarly we obtain
the central difference for uy:
∂yu(xi, y j) ∼Ui, j+1 −Ui, j−1
2y
.
(11.8)
Using a similar method one can also derive second-order central differences for the
second derivatives of u:
∂xxu = Ui−1, j −2Ui, j + Ui+1, j
(x)2
+ O((x)2)
(11.9)
and
∂yyu = Ui, j−1 −2Ui, j + Ui, j+1
(y)2
+ O((y)2).
(11.10)
The computation of a second-order ﬁnite difference approximation for the mixed
derivative ∂xyu is left for an exercise.
11.3 The heat equation: explicit and implicit schemes, stability,
consistency and convergence
The reader might infer from the discussion in the previous section that the con-
struction of a numerical scheme, i.e. converting a PDE to a discrete problem, is a

11.3 The heat equation
313
simple task: all one has to do is to replace each derivative by a ﬁnite difference
approximation, and voil`a a numerical scheme pops up. It turns out, however, that
the matter is not so simple! One should seriously consider several difﬁculties that
frequently arise during the process, and generate an appropriate numerical scheme
for each differential problem. In this section we shall present some basic terms and
ideas related to the construction of numerical schemes and apply them to derive a
variety of schemes for the heat equation.
Consider the problem
ut = kuxx
0 < x < π, t > 0,
(11.11)
u(0, t) = u(π, t) = 0
t ≥0,
u(x, 0) = f (x)
0 ≤x ≤π,
(11.12)
where we assume f (0) = f (π) = 0.
Fix an integer N > 2 and a positive number t, and set x := π/(N −1). We
deﬁne a grid {xi = ix} on the interval [0, π] and another grid {tn = nt} on
the interval [0, T ]. We further use the notation Ui,n = u(xi, tn). A simple reason-
able difference scheme for the problem (11.11)–(11.12) is based on a ﬁrst-order
difference for the time derivative, and a central difference for the spatial derivative:
Ui,n+1 −Ui,n
t
= k Ui+1,n −2Ui,n + Ui−1,n
(x)2
1 ≤i ≤N −2,
n ≥0.
(11.13)
Notice that the boundary values are determined by the boundary conditions
(11.12), i.e.
U0,n = UN−1,n = 0
n ≥0.
Using simple algebraic manipulations, (11.13) can be written in the form of an
explicit expression for the discrete solution at each point at time n + 1 in terms of
the solution at time n:
Ui,n+1 = Ui,n + α(Ui+1,n −2Ui,n + Ui−1,n),
(11.14)
where α = kt/(x)2. The initial condition for the PDE becomes an initial con-
dition for the difference equation (11.13):
Ui,0 = f (xi).
(11.15)
We have derived a simple algorithm for a numerical solution of the heat equation.
Were we to attempt to apply it, however, we would probably obtain meaningless
results! The problem is that, unless we are careful in our choice for the differences
t and x, the difference scheme (11.14) is not stable. This means that a small
perturbation to the initial condition will grow (very fast) in time. Recalling that the
representation of numbers in the computer is always ﬁnite, we realize that every
numerical solution inevitably includes some round-off error. Hence a necessary

314
Numerical methods
condition for the validity of a numerical scheme is its stability against small per-
turbations, including round-off errors.
Let us deﬁne more precisely the notion of stability for a numerical scheme (this
is analogous to the notion of stability we saw in Chapter 1 in the context of well-
posedness). For this purpose, let us denote the vector of unknowns by V . It consists
of the approximations to the values of u (the solution of the original PDE) at the
grid points where the values of u are not known. Notice that the solution u is known
at the grid points where the boundary or initial conditions are given. We write the
discrete problem in the form T (V ) = F, where the vector F contains the known
parameters of the problem (e.g. initial condition or boundary conditions). When
the PDE is linear, so is the numerical scheme. In this case we can write the scheme
as AV = F, where we denote the appropriate matrix by A. We shall demonstrate
such a matrix notation in the next section.
Deﬁnition 11.1 Let T (V ) = F be a numerical scheme. Let V i be two solutions,
i.e. T (V i) = Fi, for i = 1, 2. We shall say that the scheme is stable if for each
ε > 0 there exist δ(ε) such that |F1 −F2| < δ implies |V 1 −V 2| < ε. In other
words, a small change in the problem’s data implies a small change in the solution.
We shall demonstrate the stability notion we just deﬁned for the scheme we
proposed for the heat equation. The external conditions are given here by the initial
conditions f (x) at the grid points. To examine the stability of the scheme for an
arbitrary perturbation, we choose an initial condition of the form f (x) = sin Lx for
some positive integer L. Since any solution of the heat equation under consideration
is determined by a combination of such solutions, it follows that stability with
respect to any such initial condition implies stability for arbitrary perturbations.
Conversely, instability even with respect to a single L implies instability with
respect to random perturbations. In light of the form of the solution of the heat
equation that we found in Chapter 5, we seek a solution for (11.14) in the form
Ui,n = A(n) sin Lix. Substituting this function into (11.12) leads to a difference
equation for the sequence A(n):
A(n + 1) = A(n)

1 + αsin[L(i + 1)x] −2 sin Lix + sin[L(i −1)x]
sin Lix
/
.
(11.16)
We use the identity
sin[L(i + 1)x] −2 sin Lix + sin[L(i −1)x]
sin Lix
= −4 sin2
	 Lx
2

(11.17)

11.3 The heat equation
315
to simplify the equation for A(n), n ≥1:
A(n + 1) =

1 −4α sin2
	 Lx
2


A(n).
Therefore {A(n)} is a geometric sequence and
A(n) =

1 −4α sin2
	 Lx
2

n
A(0).
Consequently,
Ui,n =

1 −4α sin2
	 Lx
2

n
sin(Lix).
Since 1 −4α sin2 Lx < 1, it follows that a necessary and sufﬁcient condition for
the stability of the difference equation (11.14) is
1 −4α sin2
	 Lx
2

> −1.
Therefore, we cannot choose t and x arbitrarily; they must satisfy the stability
condition
t ≤1
2k (x)2.
(11.18)
Recall that we normally select a small value for x in order to obtain a ﬁnite
difference formula that is faithful to the analytic derivative. Thus the stability con-
dition is bad news for us. Although the difference scheme we developed is simple
and easy to program, the stability requirement implies that t must be very small;
we have to cover a very large number of time steps to compute the solution at
some ﬁnite positive time. For this reason many people have tried to upgrade the
scheme (11.14) into a faster scheme. However, before considering these alternative
schemes, let us examine two further important theoretical aspects: consistency and
convergence of a scheme.
Deﬁnition 11.2 A numerical scheme is said to be consistent if the solution of the
PDE satisﬁes the scheme in the limit where the grid size tends to zero. For example,
in the case of the heat equation a scheme will be called consistent if the solution of
the heat equation satisﬁes the scheme in the limit where x →0 and t →0.
To examine the consistency of the scheme (11.13) we deﬁne for each function
v(x, t)
R[v] = v(xi, tn+1) −v(xi, tn)
t
−k v(xi+1, tn) −2v(xi, tn) + v(xi−1, tn)
(x)2
. (11.19)

316
Numerical methods
Substituting a solution of the heat equation u(x, t) into the expression R, we obtain
(using the ﬁnite difference formula from the previous section)
R[u] = 1
2t ¯utt −1
12k(x)2 ¯uxxxx,
(11.20)
where ¯u denotes the value of u or its derivative at a point near the grid point (xi, tn).
Hence, if u is sufﬁciently smooth we have limx,t→0 R[u] = 0, and the scheme
is indeed consistent.
Properties such as stability and consistency are, of course, necessary conditions
for an acceptable numerical scheme. The main question in numerical analysis of a
PDE is, however, the convergence problem:
Deﬁnition 11.3 We say that a numerical scheme converges to a given differential
problem (a PDE with suitable initial or boundary conditions) if the solution of the
discrete numerical problem converges in the limit x, t →0 to the solution of
the original differential problem.
It is clear that consistency and stability are necessary conditions for convergence;
it turns out that they are also sufﬁcient.
Theorem 11.4 Any consistent and stable numerical scheme for the problem
(11.11)–(11.12) is convergent.
The usefulness of this theorem stems from the fact that consistency is, in general,
easy to verify, while stability is a property of the (discrete) numerical scheme
alone (and does not depend on the PDE itself). Therefore, it is easier to check for
stability than directly for convergence. We shall skip the proof of Theorem 11.4. We
comment that similar theorems hold for other PDEs, such as the Laplace equation
and the wave equation.
We have thus derived a convergent numerical scheme for the heat equation with
the unfortunate limitation of tiny time steps (if we wish to retain high accuracy). One
might suspect that the blame lies with the ﬁrst-order time difference. To examine
this conjecture, and to provide further examples of the notions we have deﬁned, let
us consider a scheme that is similar to (11.14), except that the time difference is
now of second order:
Ui,n+1 −Ui,n−1
2t
= k Ui+1,n −2Ui,n + Ui−1,n
(x)2
1 ≤i ≤N −2, n ≥0.
(11.21)
We face immediately a technical obstacle: the values of Ui,n−1 are not deﬁned at all
for n = 0. We can easily overcome this problem, however, by using our previous
scheme (11.13) for just the ﬁrst time step, and then proceeding further in time with
(11.21).

11.3 The heat equation
317
Surprisingly enough it turns out that the promising scheme (11.21) would be
terrible to work with; it is unstable for any choice of the time step! To see that, let
us construct, as we did for the scheme (11.14), a product solution for the differ-
ence equation of the form Ui,n = A(n) sin Lix. The sequence A(n) satisﬁes the
difference equation
A(n + 1) = A(n −1) −8α sin2
	 Lx
2

A(n).
The solution is given by A(n) = A(0)rn, where r is a solution of the quadratic
equation
r2 + 8α sin2
	 Lx
2

r −1 = 0.
Since one of the roots of this quadratic satisﬁesr >1, the scheme is always unstable.
The problem of ﬁnding an efﬁcient stable scheme for (11.13) attracted intense
activity in the middle of the twentieth century. One of the popular schemes from
this period was proposed by Crank and Nicolson:
Ui,n+1−Ui,n
t
= k
	Ui+1,n−2Ui,n+Ui−1,n
2(x)2
+ Ui+1,n+1−2Ui,n+1+Ui−1,n+1
2(x)2

.
(11.22)
To examine the stability of the Crank–Nicolson scheme, let us substitute into it the
product solution Ui,n = A(n) sin Lix. We obtain the difference equation
A(n + 1)

1 + 4α sin2(Lx/2)

=

1 −4α sin2(Lx/2)

A(n).
The solution is of the form A(n) = rn, where
r = 1 −4α sin2(Lx/2)
1 + 4α sin2(Lx/2) ,
implying that the scheme is stable for any choice of t and x. The reader will
examine the consistency of the Crank–Nicolson scheme in Exercise 11.3.
It is important to notice a fundamental difference between the scheme (11.22)
and the ﬁrst scheme we presented, (11.13). While (11.13) can be written as an
explicit expression for Ui,n+1 in terms of Un, this is not the case for (11.22). A
scheme of the former character is called an explicit scheme, while a scheme of the
latter character is called an implicit scheme. A rule of thumb in numerical analysis
is that implicit schemes are better behaved than explicit schemes (although one
should not conclude that implicit schemes are always valid). The better behavior
manifests itself, for example, in higher efﬁciency or higher accuracy.
The advantages of implicit schemes are counterbalanced by one major deﬁ-
ciency: at each time step we need to solve an algebraic system in N −2 unknowns.

318
Numerical methods
Therefore, a major theme in numerical analysis of PDEs is to derive efﬁcient meth-
ods for solving large algebraic systems. Even if the PDE is nonlinear (and, therefore,
so is the algebraic problem), the solution is often obtained iteratively (e.g. with the
Newton method), such that at each iteration one solves a linear system. In Sec-
tion 11.6, we shall consider solution methods for large algebraic systems and their
applications.
We conclude this section by examining yet another numerical scheme for the heat
equation that is based on a second-order difference formula for the time variable.
Although our ﬁrst naive approach failed, it was discovered that a slight modiﬁcation
of (11.21) provides a convergent scheme. We thus consider the following scheme
proposed by Du-Fort and Frankel:
Ui,n+1 −Ui,n−1
2t
=k Ui+1,n −Ui,n−1 −Ui.n+1 + Ui−1,n
(x)2
1 ≤i ≤N −2, n ≥0.
(11.23)
The stability and consistency of this scheme (and hence also its convergence) will
be proved in Exercise 11.4.
11.4 Laplace equation
We move on to discuss the numerical solution of the Laplace (or, more generally,
the Poisson) equation. Let  be the rectangle  = (0, a) × (0, b). We are looking
for a function u(x, y) that solves the Dirichlet problem
u(x, y) = f (x, y)
(x, y) ∈,
u(x, y) = g(x, y)
(x, y) ∈∂,
(11.24)
or the Neumann problem
u(x, y) = f (x, y)
(x, y) ∈,
∂nu(x, y) = g(x, y)
(x, y) ∈∂.
(11.25)
Notice that it is possible to solve (11.24) or (11.25) by the separation of variables
method; yet applying this method could be technically difﬁcult since it involves the
computation of the Fourier coefﬁcients of the given function f (or g). Moreover,
the Fourier series might converge slowly near the boundary.
We deﬁne instead a grid over the rectangle  (see Figure 11.2):
{(xi, y j) = (ix, jy) i = 0, 1, . . ., N −1, j = 0, 1, . . . , M −1},
(11.26)
where x = a/(N −1), y = b/(M −1). The derivatives of u will be approx-
imated by ﬁnite differences of the values of u on the grid’s points. We shall use

11.4 Laplace equation
319
i,j
N-1,j
0,j
Figure 11.2 The grid for the Laplace equation.
central difference approximations to write a scheme for the Dirichlet problem as
Ui−1, j −2Ui, j + Ui+1, j
(x)2
+ Ui, j−1 −2Ui, j + Ui, j+1
(y)2
= Fi, j := f (xi, y j),
(11.27)
i = 1, 2, . . ., N −2, j = 1, 2, . . ., M −2,
together with the boundary conditions:
U0, j := G0, j ≡g(0, y j)
j = 0, 1, . . ., M −1,
UN−1, j := G N−1, j ≡g(a, y j)
j = 0, 1, . . . , M −1,
(11.28)
Ui,0 := Gi,0 ≡g(xi, 0)
i = 0, 1, . . ., N −1,
Ui,M−1 := Gi,M−1 ≡g(xi, b)
i = 0, 1, . . ., N −1.
Observe that (11.27) determines the value of Ui, j in terms of the values of U
at the four nearest neighbors of the point (i, j), and the value of f at (i, j). We
further point out that the differential problem has been replaced by a linear algebraic
system of size (N −2) × (M −2).
Before approaching the question of how the linear system is to be solved, we have
to address a number of fundamental theoretical questions. Is the system we obtained
solvable? Is the solution unique? Is it stable? To answer these questions we shall
prove that the difference scheme we have formulated satisﬁes a strong maximum
principle that is similar to the one we proved in Chapter 7 for the continuous
problem:
Theorem 11.5 (The strong maximum principle) Let Ui, j be the solution of the
homogeneous system
Ui−1, j −2Ui, j + Ui+1, j
(x)2
+ Ui, j−1 −2Ui, j + Ui, j+1
(y)2
= 0,
(11.29)
i = 1, 2, . . ., N −2, j = 1, 2, . . ., M −2,

320
Numerical methods
with speciﬁed boundary values. If U attains its maximum (minimum) value at an
interior point of the rectangle, then U is constant.
Proof We assume for simplicity and without loss of generality that x = y.
Notice that
Ui, j = Ui−1, j + Ui+1, j + Ui, j−1 + Ui, j+1
4
;
namely, Ui, j is the arithmetic average of its nearest neighbors. Clearly if an average
of a set of numbers is greater than or equal to each of the numbers in the set, then
all the numbers in the set equal the average. Therefore, if U attains a maximum at
some interior point, then the same maximum is also attained by each neighbor of
this point. We can continue this process until we cover all the points in the rectangle.
Thus U is constant.
□
An important consequence of the maximum principle is the following theorem.
Theorem 11.6 The difference system (11.27)–(11.28) has a unique solution.
Proof Let U (1), U (2) be two solutions of the system. Then U = U (1) −U (2) is a
solution of the same system with homogeneous boundary conditions and zero right
hand side. If U ̸= 0, then U achieves a maximum or a minimum somewhere inside
the rectangle. By the strong maximum principle (Theorem 11.5), U is constant.
Since U vanishes on the boundary, it must vanish everywhere. Thus, U (1) = U (2).
The system (11.27) consists of (N −2) × (M−2) equations in (N −2) × (M−2)
unknowns. A well-known theorem in linear algebra states that if a homogeneous
equation possesses only the trivial solution, then nonhomogeneous equation has a
unique solution.
□
Another important question is whether the solution of the numerical scheme
converges to the solution of the PDE in the limit where x and y tend to zero.
The answer is positive under certain assumptions on the boundary conditions, but
a detailed discussion is beyond the scope of the book.
Since (11.27) is a linear system, it can be written in a matrix form. For this
purpose we concatenate the two-dimensional arrays Ui, j and Fi, j into vectors that
we denote by V and G, respectively. The components Vk and Gk of these vectors
are given by
V( j−1)(N−2)+i := Ui, j
i, j = 1, 2, . . ., N −2,
(11.30)
G( j−1)(N−2)+i := (x)2Fi, j
i, j = 1, 2, . . ., N −2.

11.4 Laplace equation
321
1
0
1
7
8
9
1
3
2
4
6
5
Figure 11.3.
Notice that the vector V consists only of interior points. The system (11.27) can be
written now as AV = b, where the vector b is determined by G and by the boundary
conditions. We demonstrate the matrix notation through the following example.
Example 11.7 Write an explicit matrix equation for the discrete approximation of
the Poisson problem
u = 1 (x, y) ∈,
u(x, y) = 0 (x, y) ∈∂,
(11.31)
for a grid of 3 × 3 of interior points. The grid’s structure, together with the num-
bering of the interior points is depicted in Figure 11.3. Concatenating the discrete
system (11.27) gives rise to the matrix equation


−4
1
0
1
0
0
0
0
0
1
−4
1
0
1
0
0
0
0
0
1
−4
0
0
1
0
0
0
1
0
0
−4
1
0
1
0
0
0
1
0
1
−4
1
0
1
0
0
0
1
0
1
−4
0
0
1
0
0
0
1
0
0
−4
1
0
0
0
0
0
1
0
1
−4
1
0
0
0
0
0
1
0
1
−4




V1
V2
V3
V4
V5
V6
V7
V8
V9


=


1
1
1
1
1
1
1
1
1


.
(11.32)
A quick inspection of (11.32) teaches us that the matrix A has some special
features:
(1) It is sparse (most of its entries vanish).
(2) The entries that do not vanish concentrate near the diagonal.
(3) The diagonal entry in every row is equal to or greater (in absolute value) than the
sum of all the other terms in that row. A matrix with this property is called diagonally
dominated matrix.

322
Numerical methods
These properties are typical of many linear systems obtained as approximations
to PDEs. The sparseness stems from the fact that the differentiation operator is local;
it relates the value of a function at some point to the values at near-by points. We
conclude that while numerical schemes for PDEs lead to large algebraic systems,
these systems have a special structure. We shall see later that this structure enables
us to construct efﬁcient algorithms for solving the algebraic systems.
11.5 The wave equation
The numerical solution of hyperbolic equations, such as the wave equation, is more
involved than the solution of parabolic and elliptic equations. The reason is that so-
lutions of hyperbolic equations might have singularities. Since the ﬁnite difference
schemes we presented above are valid only for smooth functions, they may not be
adequate for use in hyperbolic equations without some modiﬁcation. We empha-
size that the existence of characteristic surfaces where the solution of hyperbolic
equations might be singular is not a mathematical artifact. On the contrary, it is an
important aspect of many problems in many scientiﬁc and engineering disciplines.
It is impossible to analyze the very important and difﬁcult problem of the nu-
merical solution of PDEs with singularities within our limited framework. Instead
we shall brieﬂy consider a ﬁnite difference scheme for the wave equation in cases
where the solution is smooth. Consider, therefore, the wave equation
utt −c2uxx = 0
0 < x < π, t > 0,
(11.33)
with the initial boundary conditions
u(0, t) = u(π, t) = 0
t > 0,
u(x, 0) = f (x), ut(x, 0) = g(x)
0 ≤x ≤π.
(11.34)
We construct for (11.33) a second-order explicit ﬁnite difference scheme. For this
purpose, ﬁx an integer N > 2 and a positive number t, and set x := π/(N −1).
We deﬁne a grid {xi = ix} on the interval [0, π], and a grid {tn = nt} on the
interval [0, T ]. We further introduce the notation Ui,n = u(xi, tn), and write
Ui,n+1 −2Ui,n + Ui,n−1
(t)2
= c2Ui+1,n −2Ui,n + Ui−1,n
(x)2
1 ≤i ≤N −2, n ≥0.
(11.35)
The boundary values are determined by (11.34):
U0,n = UN−1,n = 0
n ≥0.

11.5 The wave equation
323
Let us rewrite the scheme as an explicit expression for the solution at the discrete
time n + 1 in terms of the solution at times n and n −1:
Ui,n+1 = 2(1 −α)Ui,n −Ui,n−1 + α(Ui−1,n + Ui+1,n),
(11.36)
where
α = c2
	 t
x

2
.
Since the system involves three time steps, we have to compute Ui,−1 in order to
initiate it. For this purpose we use the initial condition ut and express it by a central
second-order difference:
Ui,1 −Ui,−1 = 2tg(xi).
(11.37)
Solving for Ui,−1 from (11.37), and using the additional initial condition Ui,0 =
f (xi), we now have at our disposal all the data required for the difference equation
(11.36).
It is straightforward to check that if the solution of the PDE is sufﬁciently smooth,
then the scheme (11.36) is consistent. Is it also stable? We examine the stability
of the scheme by the same method we introduced above for the heat equation.
For this purpose we analyze the evolution of a fundamental sinusoidal initial wave
sin Lix in the course of the discrete time argument n. We express the solution of
the discrete problem in the form Ui,n = A(n) sin Lix. Substituting this expression
into (11.36), we obtain a difference equation for A(n):
A(n + 1) = 2(1 −α)A(n) −A(n −1) + 2α cos(Lx)A(n).
(11.38)
We seek solutions to (11.38) of the form A(n) = rn. We ﬁnd that r must satisfy the
quadratic equation
r2 −2r

1 −2α sin2
	 Lx
2


+ 1 = 0.
Since the product of the roots equals 1, a necessary and sufﬁcient condition that a
solution A(n) does not grow as a function of n is that the solution of the quadratic
equation would be complex, i.e.

1 −2α sin2
	 Lx
2

2
−1 ≤0.
We thus obtain
x
t ≥c.
(11.39)

324
Numerical methods
t
x
xi-1
xi
xi+1
ti+1
ti
Figure 11.4 The discrete domain of inﬂuence.
This is called the CFL condition after Courant, Friedrichs, and Lewy. Since the
CFL condition enforces values for t that are of the order of x, it is not as limiting
as the corresponding condition for the heat equation. We could have anticipated the
condition (11.39) from theoretical (and physical) grounds even without the stability
analysis of the discrete scheme. To see that, it is useful to consult Figure 11.4. The
triangle bounded between the dashed lines in the drawing is the region of inﬂuence
of the interval [(xi−1, ti), (xi+1, ti)]. The CFL condition guarantees that the point
(xi, ti+1) would be within this triangle, as indeed must be the case following our
discussion in Chapter 4.
11.6 Numerical solution of large linear algebraic systems
We saw in the previous sections that many numerical schemes give rise to systems
of linear algebraic equations (see Example 11.7). Furthermore, the systems we
obtained were inherently large and sparse. We shall therefore consider in this section
special efﬁcient methods for solving such systems. It is important to realize that
numerical linear algebra is a fast growing mathematical discipline; we shall limit
ourselves to a brief exposition of some of the basic ideas and classical methods.
Linear systems can be solved, of course, by the Gauss elimination method. The
drawback of this method is its high complexity. The complexity of a numerical
calculation is deﬁned here as the number of multiplications involved in it. (Truly,
this is a somewhat outdated deﬁnition that was more relevant to older computers;
nevertheless, it is a convenient deﬁnition and we shall stick to it.) A direct solution
by the Gauss elimination method of a system in K unknowns requires O(K 3)
multiplications. Since we normally consider equations with many unknowns (to
ensure a good numerical approximation), it is desirable to construct more efﬁcient
algorithms. The main idea behind the algorithms we present below is to exploit the
special structure of the systems arising in the discrete approximation of PDEs.

11.6 Large linear algebraic systems
325
Most of the methods that are used for large sparsesystemsareiterative.(Although
we immediately mention that there are also some popular direct methods. We refer
the reader to basic linear algebra books for an exposition of matrix decomposition
methods.) We shall demonstrate below three iterative methods. The simplest of
them all is the Jacobi method. A considerable improvement of it is achieved by the
Gauss–Seidel method, and a far more signiﬁcant improvement is obtained by the
successive over relaxation (SOR) method. Although we shall verify that the SOR
method is far superior to the Jacobi or to the Gauss–Seidel method, we prefer to
start by presenting the simpler methods that are easier to understand. Moreover, it
turns out that sometimes the complexity is not the main consideration; there are
some applications for which the Gauss–Seidel method is preferred over the SOR
method for deep reasons that we cannot discuss here. We emphasize again that
the methods we are about to present do not necessarily work for any matrix! We
consider them here for a special class of matrices that are sparse and have a certain
relation between the diagonal terms and the off-diagonal terms.
We start with the Jacobi method. To ﬁx ideas, we shall use as a prototype the
Crank–Nicolson scheme for the heat equation. Rewrite (11.22) in the form
Ui,n+1 = α
2 (Ui+1,n+1 −2Ui,n+1 + Ui−1,n+1) + ri,n,
(11.40)
where
ri,n = α
2 (Ui+1,n −2Ui,n + Ui−1,n) + Ui,n.
The values of ri,n are known at the nth step for all i, and the unknowns are the
values of Ui,n+1 for all relevant indices i (i.e. for 1 ≤i ≤N −2). We ﬁx n, and
solve (11.40) iteratively. The solution at the pth iteration will be denoted by V p
i,n+1.
The process starts at some guess V 0
i,n+1. For example, we can choose V 0
i,n+1 = Ui,n.
In the Jacobi method, we update at each step V p+1
i,n+1 by solving (11.40), using the
values of V (p)
i−1,n+1 and V (p)
i+1,n+1 (which are known from the previous iteration). We
therefore obtain the following recursive equation:
V (p+1)
i,n+1 =
α
2α + 2(V (p)
i−1,n+1 + V (p)
i+1,n+1) +
1
α + 1ri,n.
(11.41)
A close inspection of (11.41) reveals that while scanning the vector V (p+1)
n+1 , we
update V (p+1)
i,n+1 using V (p)
i−1,n+1, although at this stage we already know the updated
value V (p+1)
i−1,n+1. We therefore intuitively expect an improvement in the convergence
if we incorporate in the iterative process a more updated value. We thus write
V (p+1)
i,n+1 =
α
2α + 2(V (p+1)
i−1,n+1 + V (p)
i+1,n+1) +
1
α + 1ri,n.
(11.42)

326
Numerical methods
This is known as the Gauss–Seidel formula. We shall verify below that the Gauss–
Seidel method is twice as fast as the Jacobi method. Furthermore, in the Gauss–
Seidel algorithm there is no need to use two vectors to describe two successive
steps in the iteration: it is possible now to update V (p)
n+1 in a single vector. Hence the
Gauss–Seidel method is superior to the Jacobi method from all perspectives.
As we noted above, one can improve upon the Gauss–Seidel method by a method
called SOR . To formulate this algorithm, it is convenient to rewrite the Gauss–
Seidel formula as
V (p+1)
i,n+1 = V (p)
i,n+1 +

α
2α + 2(V (p+1)
i−1,n+1 + V (p)
i+1,n+1) +
1
α + 1ri,n −V (p)
i,n+1

.
(11.43)
The meaning of this notation is that the term in the square brackets is the change
obtained in passing from V (p)
i,n+1 to V (p+1)
i,n+1 . In the SOR method we multiply this term
by a relaxation parameter ω:
V (p+1)
i,n+1 = V (p)
i,n+1 + ω

α
2α + 2(V (p+1)
i−1,n+1 + V (p)
i+1,n+1) +
1
α + 1ri,n −V (p)
i,n+1

.
(11.44)
In the special case where ω = 1 we recover the Gauss–Seidel method. Surprisingly,
it turns out that for a clever choice of the parameter ω in the interval (1, 2) the scheme
(11.44) converges much faster than the Gauss–Seidel scheme.
To analyze the iterative methods we have presented, we shall verify that they
indeed converge (under suitable conditions), and examine their rate of convergence
(so that we can select an efﬁcient method). For this purpose it is convenient to write
the equations in a matrix form
AV = b.
(11.45)
The Crank–Nicolson method, for example, can be written (for the special choice
N = 7) as a system of the type (11.45), where
Vi = Ui,n+1,
bi = ri,n
i = 1, 2, . . ., 5
(11.46)
and
A =


1 + α
−α/2
0
0
0
−α/2
1 + α
−α/2
0
0
0
−α/2
1 + α
−α/2
0
0
0
−α/2
1 + α
−α/2
0
0
0
−α/2
1 + α


.
(11.47)

11.6 Large linear algebraic systems
327
To write the iterative methods presented above, we express A as
A = L + D + S,
where L, D, and S are matrices whose nonzero entries are below the diagonal, on the
diagonal, and above the diagonal, respectively. Observing that the Jacobi method
can be written as
AiiV (p+1)
i
= −

j̸=i
Ai jV (p)
j
+ bi,
we obtain
V (p+1) = −D−1(L + S)V (p) + D−1b.
(11.48)
Similarly, the Gauss–Seidel method is equivalent to

j≤i
Ai jV (p+1)
j
= −

j>i
Ai jV (p)
j
+ bi,
or in a matrix formulation
V (p+1) = −(D + L)−1SV (p) + (D + L)−1b.
(11.49)
The reader will be asked to show in Exercise 11.8 that the SOR method is equivalent
to
V (p+1) = −(D + ωL)−1 
[(1 −ω)D −ωS] V (p) + ωb

.
(11.50)
The iterative process for each of the methods we have presented is of the general
form
V (p+1) = MV (p) + Qb.
(11.51)
Obviously the solution V of (11.45) satisﬁes (11.51), namely
V = MV + Qb.
To investigate the convergence of each method we deﬁne (p+1) to be the difference
between the (p + 1)th iteration and the exact solution V ; we further construct for
 a difference equation:
(p+1) = V (p+1) −V = MV (p) + Qb −MV −Qb = M(V (p) −V ) = M(p).
(11.52)
Itremainstoexaminewhetherthesequence{(p)},deﬁnedthroughthedifference
equation
(p+1) = M(p),

328
Numerical methods
indeed converges to zero, and, if so, to ﬁnd the rate of convergence. For simplicity
we assume here that the matrix M is diagonalizable. We denote its eigenvalues by
λi and its eigenvectors by wi. We expand {(p)} by these eigenvectors, and obtain
(0) =

i
βiwi
for the initial condition, and (using (11.52))
(p) =

i
βiλp
i wi
(11.53)
for the subsequent terms.
Deﬁne the spectral radius of the matrix M:
λ(M) = max
i
|λi|.
It is readily seen from (11.53) that the iterative scheme converges if λ(M) < 1.
Moreover, the rate of convergence itself is also determined by λ(M). We provide
now an example for the computation of the spectral radius for a particular equation.
Example 11.8 Compute the spectral radius λ(M), where M is the matrix associated
with the Jacobi method for the Crank–Nicolson scheme. The matrix is given by
M = −D−1(L + S) =
−1
1 + α


0
−α/2
·
·
·
−α/2
0
−α/2
·
·
·
·
·
·
·
·
·
−α/2
0
−α/2
·
·
·
−α/2
0


.
(11.54)
Let w be an eigenvector of −(L + S). The entries of w satisfy the equation
α
2 w j−1 + α
2 w j+1 = λw j
j = 1, 2, . . ., K,
(11.55)
where we extend the natural K components by setting w0 = wK+1 = 0. Equation
(11.55) has solutions wk of the form
wk
j = sin kjx
j, k = 1, 2, . . ., K,
corresponding to the eigenvalue λk = α cos kx. Since D is a diagonal matrix, we
obtain that the spectral radius is
λJacobi =
α
1 + α cos x ∼
α
1 + α

1 −(x)2
2

.
(11.56)

11.7 The ﬁnite elements method
329
Similarly, one can show that the Gauss–Seidel spectral radius for the same scheme
is
λGauss−Seidel ∼
α
1 + α[1 −(x)2].
The spectral radius for the SOR method depends on the parameter ω. It can be
shown that for the special case of the Crank–Nicolson scheme for the heat equation
the optimal choice is ω ≈1.8. For this value one obtains
λSOR ∼
α
1 + α(1 −2x).
One can derive similar results for elliptic PDEs. The main difference is that the
term α/(1 + α) is absent in the elliptic case. Thus, one can show that the spectral
radii for second-order difference schemes for the Laplace equation are
λJacobi ∼1 −γ1
K ,
λGauss−Seidel ∼1 −2γ1
K ,
and, for an appropriate choice of ω,
λSOR ∼1 −γ2
√
K
,
where the constants γ1, γ2 depend on the domain .
We ﬁnally remark that there exist several sophisticated methods (for example the
multi-grid method) that accelerate the solution process well beyond the methods
we presented here.
11.7 The ﬁnite elements method
The ﬁnite elements method (FEM) is a special case of the Galerkin method that
was presented in Chapter 10. To recall this method and to introduce the essentials
of the FEM, we shall demonstrate the theory for a canonical elliptic problem:
−u = f
⃗x ∈D,
u = 0
⃗x ∈∂D,
(11.57)
where f is a given function and D is a domain in R2. Multiplying both sides by a
test function ψ and integrating by parts we obtain

D
∇u · ∇ψ d⃗x =

D
f ψ d⃗x.
(11.58)
When we use a weak formulation we should specify our function spaces. The
natural space for u is the Hilbert space that is obtained from the completion of the

330
Numerical methods
C1 functions in D that have a compact support there (i.e. that vanish on ∂D). The
condition on the support of u comes from the homogeneous Dirichlet boundary
conditions. This space is denoted by ˙H1. The test function ψ is selected in some
suitable Hilbert space. For example, we can select ψ also to lie in ˙H1. To proceed
with the Galerkin method we should deﬁne a sequence of Hilbert spaces H (k), select
a basis φ1, φ2, . . ., φk in each one of them and write u = k
i=1 αiφi. As we saw in
Chapter 10 this leads to the linear algebraic equation (10.66) for the unknowns αi,
where the entries of the matrix K and the vector d are given (similarly to (10.67))
by
Ki j =

D
∇φi · ∇φ j d⃗x,
di =

D
f φi d⃗x.
(11.59)
Remark 11.9 The FEM was invented by Courant in 1943. It was later extensively
developed by mechanical engineers to solve problems in structural design. This is
why the matrix K is called the stiffness matrix and the vector d is called the force
vector. The mathematical justiﬁcation in terms of the general Galerkin method
came later.
The special feature of the FEM lies in the choice of the family φi. The idea
is to localize the test functions φi to facilitate the computation of the stiffness
matrix. There are many variants of the FEM, and we only describe one of them
(the most popular one) here. Similar to the discretization of the domain we used
in the FDM, we divide D into many smaller regions. We use triangles for this
division. This provides a great deal of geometric ﬂexibility that makes the FEM
a powerful tool for solving PDEs in complex geometries. In Figure 11.5 we have
drawn two examples of triangulations. The initial step in the triangulation involves
the numbering of the triangles Tj and the vertices Vi. The numbering is arbitrary
in principle, but, as will become clear shortly, a clever numbering can be important
in practice.
7
      8               9             9              10             11              12
4
      5               6             5               6               7               8
1
      2               3             1               2               3               4
5
1
6
2
3
7
4
8
7
1
8
2
3
9
4
10
5
11
6
12
(a)
(b)
Figure 11.5 Two examples of triangulation.

11.7 The ﬁnite elements method
331
Each test function is constructed to be linear in each triangle and continuous at the
vertices. The shape taken by the test functions in the triangles is called an element.
In principle we can choose other more complex elements than linear functions. This
will make the problem harder to solve, but may yield higher accuracy (somewhat
similar to writing high-order ﬁnite difference schemes). To determine the actual
linear shape of φi in each triangle we impose the conditions
φi(Vj) =
1
i = j,
0
i ̸= j.
(11.60)
Since the general linear function in the plane has three coefﬁcients, and since these
coefﬁcients are uniquely determined in each triangle in terms of the value of the
function at the vertices, the set of conditions (11.60) determines each φi uniquely.
Obviously, if T is a triangle that does not have Vi as a vertex, then φi is identically
zero there. This implies that when the number of vertices is large the stiffness matrix
K isquitesparse.Wealsoseethatifwenumbertheverticesinareasonableway,then
the nonzero entries of K will not be far from the diagonal. This will considerably
simplify the complexity and stability of the algebraic system Kα = d.
Another important consequence of our choice of test functions is that if we use
U(Vi) to denote the numerical approximation of the exact solution u at Vi, then we
obtain at once the identiﬁcation αi = U(Vi) := Ui; namely the unknowns αi in the
expansion of u are exactly the values of the approximant U at the vertices.
It remains to compute the matrix K. While we could use the deﬁnition (11.59),
this would require us to compute φi in all the triangles where it does not vanish.
Instead we shall employ a popular quicker way that uses the variational charac-
terization of the problem (11.57). We have already seen in Chapter 10 examples
where the Galerkin method and the Ritz method yield the same algebraic equation.
It is easy to cast (11.57) as a variational problem; it is exactly the Euler–Lagrange
equation for minimizing the functional
F(u) =

D
	1
2|∇u|2 −f u

d⃗x
(11.61)
over all functions u that vanish on ∂D. Expressing the approximate minimizer as
u = k
i=1 Uiφi, the functional F is converted to
Fk = 1
2U t KU −U t · d,
(11.62)
where K and d are given in (11.59), and a · b is the standard inner product of the
vectors a and b in Rk. The minimization problem is now k-dimensional.
Now, since each of the φi is a linear function over each triangle Tj, it follows that
the approximant u = k
i=1 Uiφi is also a linear function. Therefore, we can easily

332
Numerical methods
 3
1               2
Te
∆x
∆x
Figure 11.6 A representative triangle.
compute the integrals in (11.61) directly in terms of the unknowns Ui. Notice that
the choice of linear elements leads to particularly simple computations since the
gradient of u is constant in each element.
The computation of K and d is straightforward to program; to demonstrate it
explicitly, we shall restrict ourselves further to cases where D is a square or a rect-
angle partitioned into identical isosceles right triangles. The length of each of the
equal sides of the triangle is x. We start the computation with a canonical repre-
sentative triangle Te (Figure 11.6). Denoting the value of the linear approximation
of u at the vertices Ue1, Ue2, Ue3 we obtain

Te
|∇U|2 d⃗x = 1
2

(Ue2 −Ue1)2 + (Ue3 −Ue1)2
.
(11.63)
Notice that in the formula above we actually had two factors of (x)2. One of them,
due to the numerical integration, is in the numerator, while the other one, due to the
gradient, is in the denominator. Therefore, these factors cancel each other. Formula
(11.63) can also be written as a quadratic form
1
2

Te
|∇u|2 d⃗x = 1
2U t
e KeUe, where
Ke =


1
−1
2
−1
2
−1
2
1
2
0
−1
2
0
1
2

.
(11.64)
Similarly we integrate the second term in the integrand of (11.61). In general
even in a simple domain such as the unit square, and even in the case of identical
triangles we need to perform the integration

Te f u d⃗x numerically. There are many
numerical integration schemes for this purpose. One simple integration formula is

Te
f u d⃗x ≈(x)2 f (Ce)
6
(Ue1 + Ue2 + Ue3),
(11.65)
where Ce is the center of gravity of the triangle Te. It remains to perform integrations
such as (11.63) and (11.65) for every triangle and to assemble the results into one
big matrix K and one big vector d.
We ﬁrst demonstrate the assembly for the triangulation (a) in Figure 11.5. Since
there is only one internal vertex (vertex 5 in the drawing), there is only one un-
known – U5. Therefore, there is only one test function φ5. We use the canonical

11.7 The ﬁnite elements method
333
formula (11.64) for all the triangles that have V5 as a vertex. This means that
triangles 5 and 4 do not participate in the computation. We write

D
|∇u|2d⃗x ≈


U4
U1
U5


t
Ke


U4
U1
U5

+


U2
U1
U5


t
Ke


U2
U1
U5

+


U5
U2
U6


t
Ke


U5
U2
U6


(11.66)
+


U5
U4
U8


t
Ke


U5
U4
U8

+


U8
U5
U9


t
Ke


U8
U5
U9

+


U6
U5
U9


t
Ke


U6
U5
U9

.
Since the boundary conditions imply that Ui = 0 for i ̸= 5, we obtain K = (4).
The computation of the force vector d is based on (11.65). Suppose that f is
constant (say 1), then we obtain at once that each of the relevant six triangles
contributes exactly (x)2/6 to the entry d5 of d which is the only nonzero entry.
Therefore, d5 = (x)2U5. Thus, after optimization, we obtain 4U5 = (x)2, and
since x = 1
2, the numerical solution in this triangulation is U5 = 1/16.
We proceed to the more “interesting” triangulation depicted in Figure 11.5(b).
Now there are two internal vertices (6 and 7); thus there are two unknowns U6 and
U7, and K is a 2 × 2 matrix. A little algebra gives
K =
	 4
−1
−1
4

,
d = (x2)
	1
1

.
(11.67)
Finally we look at the general case of a rectangle divided into identical isosceles
right triangles. Instead of writing the full matrix K, we derive the equation for Ui, j –
the numerical solution at the vertex (i, j) (see Figure 11.7). We use the computation
in (11.63). The vertex (i, j) appears in six of the triangles in the drawing. Summing
all the contributions to the term

D |∇u|2 d⃗x in energy that involve Ui, j gives
4U 2
i, j −Ui, j(Ui, j−1 + Ui−1, j + Ui+1, j + Ui, j+1).
i,j+1
i-1,j
i,j
i+1,j
i,j-1
Figure 11.7 Two relevant triangles for a given vertex.

334
Numerical methods
Similarly, the term

D f u d⃗x in the energy contributes (x)2Ui, j. Therefore, the
equation for the minimizer Ui, j is
4Ui, j −Ui, j−1 −Ui−1, j −Ui+1, j −Ui, j+1 = (x)2.
(11.68)
The observant reader will realize that this is exactly the equation we obtained
in (11.27) by the FDM. Does it mean that the two methods are the same? They
certainly give rise to the same algebraic system for the PDE we are considering
in the current example and for the present triangulation. This fact should boost
our conﬁdence in these algebraic equations! While there are indeed equations and
domains for which both methods yield the same discrete equations, this is certainly
not always the case. Even for the present domain and triangulation, we would have
obtained different discrete equations had we solved the equation −u = f (x, y),
where f is not constant (see Exercise 11.15).
11.8 Exercises
11.1 Consider the rectangular grid (11.1) and assume x = y. Find a second-order
difference scheme for uxy.
11.2 Prove (11.9) and (11.10).
11.3 Prove that the Crank–Nicolson scheme is consistent.
11.4 Prove that, under suitable conditions, the Du-Fort–Frankel scheme is stable and
consistent.
11.5 Consider the heat equation
ut = uxx
0 < x < π, t > 0,
(11.69)
u(0, t) = u(π, t) = 0,
u(x, 0) = x(π −x).
(11.70)
(a) Solve (11.69)–(11.70) numerically (in spatial grids of 25, 61, and 101 points)
using the Crank–Nicolson scheme. Compute for each one of the grids the solution
at the point (x, t) = (π/4, 2).
(b) Solve the same problem analytically using 2, 7, and 20 Fourier terms. Construct a
table to compare the analytic solution at the point (x, t) = (π/4, 2) with the numerical
solutions found in part (a).
11.6 (a) Write an explicit ﬁnite difference scheme for the problem
ut = uxx
0 < x < 1, t > 0,
(11.71)
u(0, t) = ux(1, t) = 0,
u(x, 0) = f (x).
(11.72)
(b) Write an implicit ﬁnite difference scheme for problem
ut = uxx
0 < x < 1, t > 0,
(11.73)
u(0, t) = ux(1, t) = 0,
u(x, 0) = f (x).
(11.74)

11.8 Exercises
335
11.7 Solve the problem
ut = uxx + 5t4
0 ≤x ≤1, t > 0,
(11.75)
u(0, t) = u(1, t) = t5,
u(x, 0) = 0.
(11.76)
using scheme (11.13). Use x = t = 0.1. Compute u( 1
2, 3). Compare your answer
with the analytical solution of the same equation and explain what you observe.
11.8 Derive (11.50).
11.9 Show that if Fi, j is positive at all the grid points, then the solution Ui, j of (11.27)
cannot attain a positive maximal value at an interior point.
11.10 (a) Let D be the unit rectangle D = {(x, y)| 0 < x < 1, 0 < y < 1}. Solve
u(x, y) = 1
(x, y) ∈D,
u(x, y) = 0
(x, y) ∈∂D
(11.77)
for N = 3, 4 manually, and for N = 11, 41, 91 using a computer (write the code
for this problem). For each choice of grid ﬁnd an approximation to u( 1
2, 1
2) and to
u( 1
10, 1
10).
(b) Solve the problem of part (a) by the method of separation of variables. Evaluate
the solution at the points u( 1
2, 1
2), and u( 1
10, 1
10), using a Fourier series with 2, 5,
and 20 coefﬁcients. Compare the numerical solution you found in part (a) with the
analytical solution of part (b).
11.11 Let D be the unit square. Solve
u(x, y) = 0
(x, y) ∈D,
u(x, y) = 1 + 1
5 sin x
(x, y) ∈∂D,
(11.78)
for N = 3, 11, 41, 80. In each case ﬁnd an approximation for u( 1
2, 1
2)
11.12 Write a ﬁnite difference scheme for the equation u = 1 in the rectangle T =
{(x, y)| 0 < x < 3, 0 < y < 2} under the Dirichlet condition u = 0 on the bound-
ary of T . Use a discrete grid with step size x = y = 1. Solve the algebraic equa-
tions without using a computer, and ﬁnd an approximation for u(1, 1) and u(2, 1).
11.13 Consider the discrete Dirichlet problem for the Laplace equation on a rectangular
uniform N × N grid; there are (N −2)2 unknowns, and 4(N −2) boundary points.
Prove that the space of all solutions to the discrete Dirichlet problem is of dimension
4(N −2).
11.14 Let w(x, y) be a smooth given vector ﬁeld in the unit square D. Generalize the
scheme (11.27) to write a second-order ﬁnite difference scheme for the equation
u + w · ∇u = 0
under the Dirichlet conditions u = 1 on the boundary of D.
11.15 Consider problem (11.57), where f (x, y) = x2 + x2y and D is the unit square.
(a) Write explicitly an FEM scheme in which the triangulation consists of identical
isosceles right triangles with 16 vertices.
(b) Now write for the same set of vertices an FDM scheme. Is it the same as the
scheme you obtained in (a)?
(c) Solve the equations you derived in (a).

336
Numerical methods
11.16 Consider the ODE
u′′(x) = f (x) x < 0 < L,
x(0) = 1
x(L) = 0.
(11.79)
Divide the interval (0, L) into N identical subintervals with vertices x0 =
0, x1, . . ., xN+1 = L. Consider the basis functions φi where φi is linear at each
subinterval (xi−1, xi), with
φi(x j) =
 1
i = j,
0
i ̸= j.
(11.80)
(a) Use one of the methods we introduced above to construct an FEM scheme for the
Poisson equation in a rectangle to construct an FEM scheme for the ODE (11.79).
Compute explicitly the stiffness matrix K.
(b) Solve the ODE analytically and numerically for
f (x) = sin 2x and for
N = 4, 10, 20, 40. Discuss the error in each of the numerical solutions compared
with the exact solution.

12
Solutions of odd-numbered problems
Here we give numerical solutions and hints for most of the odd-numbered problems.
Extended solutions to the problems are available for course instructors using the
book from solutions@cambridge.org.
Chapter 1
1.1 (a) Write ux = af ′, uy = bf ′. Therefore a and b can be any constants such
that a + 3b = 0.
1.3 (a) Integrate the ﬁrst equation with respect to x to get u(x, y) = x3y + xy +
F(y), where F(y) is still undetermined. Differentiate this solution with respect to
y and compare with the equation for uy to conclude that F is a constant function.
Finally, using the initial condition u(0, 0) = 0, obtain F(y) = 0.
(b) The compatibility condition uxy = uyx does not hold. Therefore there does not
exist a function u satisfying both equations.
1.5 (a) u(x, t) = f (x + kt) for any differentiable function f .
(b), (c). Equations (b) and (c) do not have such explicit solutions. Nevertheless, if
selecting f (s) = s, then (b) is solved by u = x + ut that can be written explicitly
as u = x/(1 −t), which is well deﬁned if t ̸= 1.
1.7 (a) Substitute v(s, t) = u(x, y), and use the chain rule to get
ux = vs + vt
uy = −vt,
and
uxx = vss + vtt + 2vst
uxy = −vtt −vst
uyy = vtt.
Therefore, uxx + 2uxy + uyy = vss, and the equation becomes vss = 0.
(b) The general solution is u(x, y) = f (x −y) + xg(x −y).
(c) Proceeding similarly, obtain for v(s, t) = u(x, y) the equation vss + vtt = 0.
337

338
Solutions of odd-numbered problems
Chapter 2
2.1 (a) The characteristics are y = x + c.
(b) The solution is u(x, y) = f (x −y) + y.
2.3 (a) The parametric solution is
x(t) = x0et
y(t) = y0et
u(t) = u0ept,
and the characteristics are the curves x/y = constant.
(b) u(x, y) = (x2 + y2)2 is the unique solution.
(c) The initial curve (s, 0, s2) is a characteristic curve (see the characteristic equa-
tions). Thus, there exist inﬁnitely many solutions: u(x, y) = x2 + ky2
∀k ∈R.
2.5 (a) The projection on the (x, y) plane of each characteristic curve has a positive
direction and it propagates with a strictly positive speed in the square.
(b) On each characteristic line u equals u(t) = f (s)e−t, therefore u preserves its
sign along characteristics.
(c) Since ∇u(x0, y0) = 0 at critical points, it follows from the PDE that u(x0, y0) =
0.
(d) Follows from part (c) and (b).
2.7 The parametric solution is
(x(t, s), y(t, s), u(t, s)) =
	
t + s, t,
1
1 −t

,
implying u = 1/(1 −y).
2.9 (a) The transversality condition holds, implying a unique solution near the initial
curve.
(b) The solution of the characteristic equations is
x(t, s) = s −2 sin s

e−t/2 −1

y(t, s) = t
u(t, s) = sin se−t/2.
(c) The solution passing through 1 is
x(t, s) = s
y(t, s) = s + t
u(t, s) = 0,
namely, u(x, y) = 0.
(d) Such a curve must be a characteristic curve. It follows that it can be represented
as {(nπ, t, 0) | t ∈R}, where n ∈Z.
2.11 The Jacobian satisﬁes J ≡0. Since u ≡0 is a solution of the problem, there
exist inﬁnitely many solutions. To compute other solutions, deﬁne a new Cauchy
problem such as
(y2 + u)ux + yuy = 0,
u(x, 1) = x −1
2.

Chapter 2
339
Now the Jacobian satisﬁes J ≡1. The parametric form of the solution is
x(t, s) = (s −1
2)t + 1
2e2t + s −1
2,
y(t, s) = et,
u(t, s) = s −1
2.
It is convenient in this case to express the solution as a graph of the form
x(y, u) = y2
2 + u ln y + u.
2.13 The transversality condition is violated for all s. “Guess” a solution of the
form u = u(x), to ﬁnd u = √2(x −1). This means that there are inﬁnitely many
solutions. To ﬁnd them, deﬁne a new Cauchy problem; for instance, select the
problem
uux + xuy = 1,
u

x + 3
2, 7
6

= 1.
The parametric representation of the solution to the new problem is
x(t, d) = 1
2t2 + t + d + 3
2,
y(t, d) = 1
6t3 + 1
2t2 + (d + 3
2)t + 7
6,
u(t, d) = t + 1.
Finally,
y(x, u) = (u −1)3
6
+ (u −1)2
2
+ (u −1)

x −(u −1)2
2
−(u −1)

+ 7
6.
2.15 (a)
u(x, y) = y 1 −yx/y−y
x −y2
.
(b),(d) The transversality condition holds everywhere. The explicit solution shows
that u is not deﬁned at the origin. This does not contradict the local existence
theorem, since this theorem only guarantees a solution in a neighborhood of the
original curve (y = 1).
2.17 (a) The parametric surface representation is
x = x0et
y = y0 + t
u = u0 + t,
and the characteristic curve passing through the point (1, 1, 1) is (et, 1 + t, 1 + t).

340
Solutions of odd-numbered problems
(b) The direction of the projection of the initial curve on the (x, y) plane is (1, 0). The
direction of the projection of the characteristic curve is (s, 1). Since the directions
are not parallel, there exists a unique solution.
(c) u(x, y) = sin(x/ey) + y. It is deﬁned for all x and y.
2.19 (a) u(x, y) = x2y2/[4(y −x)2 −xy(y −2x)].
(b) The projection of the initial curve on the (x, y) plane is in the direction (1, 2).
The direction of the projection of the characteristic curve (for points on the initial
curve) is s2(1, 4). The directions are not parallel, except at the origin where the
characteristic direction is degenerate.
(c) The characteristic that starts at the points (0, 0, 0) is degenerate.
(d) The solution is not deﬁned on the curve 4(y −x)2 = xy(y −2x) that passes
through the origin.
2.21 (a) u(x, y) = 2x3/2y1/2 −xy.
(b) The transversality condition holds. The solution is deﬁned only for y > 0.
2.23 (a) u = (x −ct)/[1 + t(x −ct)].
(b), (c) The observer that starts at a point x0 > 0 sees the solution u(x0 + ct, t) =
x0/(1 + x0t). Therefore, if x0 > 0, the observed solution decays, while if x0 < 0
the solution explodes in a ﬁnite time. If x0 = 0 the solution is 0.
2.25 The transversality condition is violated identically. However, the characteristic
direction is (1, 1, 1), and so is the direction of the initial curve. Therefore the initial
curve is itself a characteristic curve, and there exist inﬁnitely many solutions. To
ﬁnd solutions, consider the problem ux + uy = 1, u(x, 0) = f (x), for an arbitrary
f satisfying f (0) = 0. The solution is u(x, y) = y + f (x −y). It remains to ﬁx
ﬁve choices for f .
2.27 (a) u(x, y) = (6y −y2 −2x)/[2(3 −y)].
(b) A straightforward calculation veriﬁes u(3x, 2) = 4 −3x.
(c) The transversality condition holds in this case. Therefore the problem has a
unique solution, and from (b) the solution is the same as in (a).
Chapter 3
3.1 (a) The equation is parabolic. The required transformation is
y = t,
x = s −t
3
.
(b)
u(x, y) = (3x + y)y4
324
−y5
540 + yφ(3x + y) + ψ(3x + y).

Chapter 3
341
(c)
u(x, y)= (3x + y)y4
324
−y5
540 +y

cos(x + y
3)−1
3 cos(x + y
3)

+sin(x + y/3).
3.3 (a) Writing w(s, t) = u(x, y), the canonical form is wst + 1
4wt = 0.
(b) Using W := wt, the general solution is u(x, y) = f (y −4x)e−y/4 + g(y), for
arbitrary functions f, g ∈C2(R).
(c) u(x, y) = (−y/2 + 4x)e−y/4.
3.5 (a) The equation is hyperbolic when xy > 0, elliptic when xy < 0, and parabolic
when xy = 0 (but this is not a domain!).
(b) The characteristic equation is y′2 = y/x.
(1) When xy > 0 there are two real roots y′ = ±√y/x. Suppose for instance that x, y > 0.
Then the solution is √y ± √x = constant. Deﬁne the new variables s(x, y) = √y +
√x and t(x, y) = √y −√x.
(2) When xy < 0 there are two complex roots y′ = ±i√|y/x|. Choose y′ = i√|y/x|. The
solution of the ODE is 2sign(y)√|y| = i2sign(x)√|x| + constant. Divide by 2sign(y) =
−2sign(x) to obtain √|y| + i√|x| = constant. Deﬁne the new variables s(x, y) = √|x|
and t(x, y) = √|y|.
3.7 (a) The equation is hyperbolic for q > 0, i.e. for y > 1. The equation is elliptic
for q < 0, i.e. for y < −1. The equation is parabolic for q = 0, i.e. for |y| ≤1.
(b) The characteristics equation is (y′)2 −2y′ + (1 −q) = 0; its roots are y′
1,2 =
−1 ± √q.
(1) The hyperbolic regime y > 1. There are two real roots y′
1,2 = 1 ± 1. The solutions
of the ODEs are y1 = constant,
y2 = 2x + constant. Hence the new variables are
s(x, y) = y and t(x, y) = y −2x.
(2) The elliptic regime y < −1. The two roots are imaginary: y′
1,2 = 1 ± i. Choose one of
them, y′ = 1 + i, to obtain y = (1 + i)x + constant. The new variables are s(x, y) =
y −x, t(x, y) = x.
(3) The parabolic regime |y| ≤1. There is a single real root y′ = 1; The solution of the
resulting ODE is y = x + constant. The new variables are s(x, y) = x, t(x, y) = x −y
3.11 (a)
u(x, y)= 1
2 [ f (1−cos x−x+y)+ f (1−cos x+x+y)]+ 1
2
	 1−cos x+x+y
1−cos x−x+y
g(s)ds

.
(b) The solution is classic if it is twice differentiable. Thus, one should require that
f would be twice differentiable, and that g would be differentiable.

342
Solutions of odd-numbered problems
Chapter 4
4.3 (a)
u(x, 1) =



0
x < −3,
1 −(x + 2)2
2
−3 ≤x ≤−1,
x + 1
−1 ≤x ≤0,
1
0 ≤x ≤1,
1 −(x −2)2
2
+ 1
1 ≤x ≤3,
4 −x
3 ≤x ≤4,
0
x > 4.
(b) limt→∞u(5, t) = 1.
(c) The solution is singular at the lines: x ± 2t = ±1, 2.
(d) The solution is continuous at all points.
4.5 (a) The backward wave is
ur(x, t) =



12(x + t) −(x + t)2
0 ≤x + t ≤4,
0
x + t < 0,
32
x + t > 4.
and the forward wave is
u p(x, t) =



−4(x −t) −(x −t)2
0 ≤x −t ≤4,
0
x −t < 0,
−32
x −t > 4.
(d) The explicit representation formulas for the backward and forward waves of (a)
imply that the limit is 32, since for t large enough 5 + t > 4 and 5 −t < 0.
4.7 (a) Consider a forward wave u = u p(x, t) = ψ(x −t). Then
u p(x0−a, t0−b) + u p(x0 + a, t0 + b)=ψ(x0−t0−a + b) + ψ(x0−t0 + a−b)
=u p(x0−b, t0−a) + u p(x0 + b, t0 + a).
A similar equality is obtained for a backward wave u = ur(x, t) = φ(x + t). Since
every solution of the wave equation is a linear combination of forward and backward
waves, the statement follows.
(b) u(x0−ca, t0−b) + u(x0 + ca, t0 + b)=u(x0−cb, t0−a)+u(x0 + cb, t0 + a).

Chapter 4
343
(c)
u(x, t) =



f (x + t) + f (x −t)
2
+ 1
2
 x+t
x−t
g(s) ds
t ≤x,
f (x + t) −f (t −x)
2
+ 1
2
 x+t
t−x
g(s) ds + h(t −x)
t ≥x.
(d) h(0) = f (0), h′(0) = g(0), h′′(0) = f ′′(0). If these conditions are not satisﬁed
the solution is singular along the line x −t = 0.
(e)
u(x, t) =



f (x + ct) + f (x −ct)
2
+ 1
2c
 x+ct
x−ct
g(s) ds
ct ≤x,
f (x + ct) −f (ct −x)
2
+ 1
2c
 x+ct
ct−x
g(s) ds + h

t −x
c

ct ≥x.
The
corresponding
compatibility
conditions
are
h(0) = f (0), h′(0) =
g(0), h′′(0) = c2 f ′′(0). If these conditions are not satisﬁed the solution is
singular along the line x −ct = 0.
4.9 u(x, t) = x2 + t + 3t2/2.
4.11 D’Alembert’s formula implies
P(x, t) = 1
2 [ f (x + 4t) + f (x −4t)] + 1
8 [H(x + 4t) −H(x −4t)] ,
where H(x) =
 x
0 g(s)ds. Hence
H(x) =



x
|x| ≤1,
1
x > 1,
−1
x < −1.
Notice that at x0 = 10:
f (10 + 4t) = 0,
f (10 −4t) ≤10, |H(t)| ≤1, t > 0.
Therefore, P(10, t) ≤5 + 1
4 = 21
4 < 6, and the structure will not collapse.
4.13 (a) The solution is not classical when x ± 2t = −1, 0, 1, 2, 3.
(b) u(1, 1) = 1/3 + e −e3/2 −e−1/2.
4.15 u(x, t)=

v(x, t)dx + f (t)= 1
2 [sin(x −t) −sin(x + t)] + f (t), where f (t)
is an arbitrary function.
4.17 (a) u(x, t) = x + 1
2tsin(x + t) + 1
4cos(x −t) −1
4cos(x + t).
(b) v(x, t) = 1
2tsin(x + t) + 1
4cos(x + t) −1
4cos(x −t).
(c) The function w = 1
2cos(x + t) −1
2 cos(x −t) −x solves the homogeneous
wave equation wtt −wxx = 0, and satisﬁes the initial conditions w(x, 0) = x,
wt(x, 0) = sin x.
(d) w is an odd function of x.

344
Solutions of odd-numbered problems
4.19 The unique solution is u(x, t) = 1 + 2t.
Chapter 5
5.1 u(x, t) = (4/π) ∞
n=1 (1/n)

cos
 1
2nπ

−(−1)n
e−17n2t sin nx.
5.3 (a)
u(x, t) =
∞

n=1
	
An cos cπnt
L
+ Bn sin cπnt
L

sin nπx
L
,
An = 2
L
L

0
f (x) sin
nπx
L

dx
n ≥1,
Bn =
2
cnπ
L

0
g(x) sin
nπx
L

dx
n ≥1.
5.5 (a)
u(x, t) = A0
2 +
∞

n=1
Ane−kπ2n2t/L2 cos
nπx
L

,
where
An = 2
L
 L
0
f (x) cos
nπx
L

dx
n ≥0.
(c) The obtained function is a classical solution of the equation for all t > 0, since
if f is continuous the exponential decay implies that for every ε > 0 the series and
all its derivatives converge uniformly for all t > ε > 0. For the same reason, the
series (without A0/2) converges uniformly to zero (as a function of x) in the limit
t →∞. Thus,
lim
t→∞u(x, t) = A0
2 .
It is instructive to compute A0 by an alternative method. Notice that
d
dt
 L
0
u(x, t)dx =
 L
0
ut(x, t)dx = k
 L
0
uxx(x, t)dx
= k [ux(L, t) −ux(0, t)] = 0,

Chapter 5
345
where the last equality follows from the Neumann boundary condition. Hence,
 L
0
u(x, t)dx =
 L
0
u(x, 0)dx =
 L
0
f (x)dx
holds for all t > 0. Since the uniform convergence of the series implies the conver-
gence of the integral series, you can infer
A0
2 =
 L
0 f (x)dx
L
.
A physical interpretation The quantity
 L
0 u(x, t)dx was shown to be conserved
in a one-dimensional insulated rod. The quantity kux(x, t) measures the heat ﬂux
at a point x and time t. The homogeneous Neumann condition amounts to stating
that there is zero ﬂux at the rod’s ends. Since there are no heat sources either
(the equation is homogeneous), the temperature tends to equalize its gradient, and
therefore it converges to a constant temperature, such that the total stored energy is
the same as the initial energy.
5.7 To obtain a homogeneous equation write u = v + w, where w = w(t) satisﬁes
wt −kwxx = A cos αt,
w(x, 0) ≡0.
Therefore,
w(t) = A
α sin αt .
Solving for v, the complete solution is
u(x, t)=3/2 + 1/2 cos 2πx e−4kπ2t + A
α sin αt.
5.9 (a)
u(x, t) =
∞

n=1
Bne(−n2+h)t sin nx,
where
Bn = 2
π
 π
0
x(π −x) sin nxdx = −4[(−1)n −1]
πn3
.
(b) The limit limt→∞u(x, t) exists if and only if h ≤1. When h < 1 the series
converges uniformly to 0. If h = 1, the series converges to B1 sin x.
5.11 (a) The domain of dependence is the interval [ 1
3 −1
10, 1
3 + 1
10] along the x axis.
(b) Part (a) implies that the domain of dependence does not include the boundary.
Therefore, D’Alembert’s formula can be used to compute u( 1
3, 1
10) = −1
2
65
153 =
−13
1350.

346
Solutions of odd-numbered problems
(c)
u(x, t) = 1 −cos 4πx cos 4πt.
5.13
u(x, t) = e−t
∞

n=0
Bne−(2n+1)2tπ2/4 sin
	2n + 1
2
πx

,
where
Bn = 2
 1
0
x(2 −x) sin
	2n + 1
2
πx

dx =
32
(2n + 1)3π3 .
5.17 Let u1 and u2 be a pair of solutions. Set v = u1 −u2. We need to show that
v ≡0. Thanks to the superposition principle v solves the homogeneous system
vtt −c2vxx + hv = 0
−∞< x < ∞, t > 0,
lim
x→±∞v(x, t) =
lim
x→±∞vx(x, t) =
lim
x→±∞vt(x, t) = 0
t ≥0,
v(x, 0) = vt(x, 0) = 0
−∞< x < ∞.
Let E(t) be as suggested in the problem. The initial conditions imply E(0) = 0.
Formally differentiating E(t) by t we write
dE
dt =
 ∞
−∞

vtvtt + c2vxvxt + hvvt

dx,
assuming that all the integrals converge (we ought to be careful since the integration
is over the entire real line).
We compute
 ∞
−∞
vxvxtdx = −
 ∞
−∞
vtvxxdx +
 ∞
−∞
∂(vxvt)
∂x
dx.
Using the homogeneous boundary conditions
 ∞
−∞
∂(vxvt)
∂x
dx = lim
x→∞vx(x, t)vt(x, t) −lim
x→−∞vx(x, t)vt(x, t) = 0,
hence,
 ∞
−∞vxvxtdx = −
 ∞
−∞vxxvtdx. Conclusion:
dE
dt =
 ∞
−∞
vt

vtt −c2vxx + hv

dx = 0.
We have veriﬁed that E(t) = E(0) = 0 for all t. The positivity of h implies that
v ≡0.

Chapter 6
347
5.19 (b) We consider the homogeneous equation
(y2vx)x + (x2vy)y = 0
(x, y) ∈D,
v(x, y) = 0
(x, y) ∈.
Multiply the equation by v and integrate over D:
 
D
v

(y2vx)x + (x2vy)y

dxdy = 0.
Using the identity of part (a):
 
D
v

(y2vx)x + (x2vy)y

dxdy = −
 
D

(yvx)2 + (xvy)2
+
 
D
div

y2vvx, x2vvy

dxdy.
Using further the divergence theorem (see Formula (2) in Section A.2):
 
D
div

vy2vx, x2vvy

dxdy =


vy2vxdy −vx2vydx = 0,
where in the last equality we used the homogeneous boundary condition v ≡0 on
. We infer that the energy integral satisﬁes
E :=
 
D

(yvx)2 + (xvy)2
dxdy = 0,
hence vx = vy = 0 in D. We conclude that v(x, y) is constant in D, and then the
homogeneous boundary condition implies that this constant must vanish.
Chapter 6
6.1 (b) Use part (a) to set λ = µ2 and write
u(x) = A sin µx + B cos µx.
The boundary conditions lead to the transcendental equation
2µ
µ2 −1 = tan µ.
(c) In the limit λ →∞(or µ →∞), µn satisﬁes the asymptotic relation µn ∼nπ
(where nπ is the root of the nth branch of tan µ). Therefore, λn ≈n2π2 when
n →∞.
6.3 (a) The eigenvalues are
λn =
 nπ
ln b
2
+ 1
4 > 1
4
n = 1, 2, 3. . . . ,

348
Solutions of odd-numbered problems
and the eigenfunctions are
vn(x) = x−1/2 sin
 nπ
ln b ln x

n = 1, 2, 3, . . .
(b)
u(x, t) =
∞

n=1
Cne−λntx−1/2 sin
 nπ
ln b ln x

.
The constants Cn are determined by the initial data:
u(x, 0) = f (x) =
∞

n=1
Cnx−1/2 sin
 nπ
ln b ln x

.
This is a generalized Fourier series expansion for f (x), and
Cn = ⟨f, vn⟩
⟨vn, vn⟩,
where ⟨· , · ⟩denotes the appropriate inner product.
6.5 (a)
un(x) = (x + 1)−1/2 sin
nπ ln(x + 1)
ln 2

,
λn = n2π2
ln2 2 + 1/4
n = 1, 2, . . . .
6.7 (a) Verify ﬁrst that all eigenvalues are greater than 1/4. Then ﬁnd
un(x) = x−1/2 sin(nπ ln x),
λn = n2π2 + 1/4
n = 1, 2, 3, . . . .
6.9 (a) Perform two integration by parts for the expression
 1
−1u′′v dx, and use the
boundary conditions to handle the boundary terms.
(b) Let u be an eigenfunction associated with the eigenvalue λ. Write the equation
that is conjugate to the one satisﬁed by u:
¯u′′ + ¯λ¯u = 0.
Obviously ¯u satisﬁes the same boundary conditions as u. Multiply respectively by
¯u and by u, and integrate over the interval [−1, 1]. Use part (a) to get
λ
 1
−1
|u(x)|2 dx = ¯λ
 1
−1
|u(x)|2 dx.
Hence λ is real.
(c) Verify ﬁrst that all the eigenvalues are positive. Then, the eigenvalues are λn =

(n + 1
2)π
2 and the eigenfunctions are
un(x) = an cos

n + 1
2

πx + bn sin

n + 1
2

πx.

Chapter 6
349
(d) It follows from part (c) that the multiplicity is 2, and a basis for the eigenspace
is

cos

n + 1
2

πx, sin

n + 1
2

πx

.
(e) Indeed, the multiplicity is not 1, but this is not a regular Sturm–Liouville prob-
lem!
6.11
u(x, t) = e−t +
10

n=2
3ne(−4n2−1)t cos 2nx +

2t −2 + 2e−t + 3 cos 2x

.
This is a ﬁnite sum of elementary smooth functions, and therefore it is a classical
solution.
6.13 To obtain a homogeneous problem, write
u = v + xt
π + 2
	
1 −x2
π2

.
v is a solution for the system



vt −vxx = xt −4
π2
0 < x < π, t > 0,
v(0, t) = v(π, t) = 0
t ≥0,
v(x, 0) = 0
0 ≤x ≤π.
Solving for v obtain u:
u(x, t) =
∞

n=1

−(2π3 + 8)(−1)n+1 + 8
n5π3

1 −e−n2t
+
2(−1)n+1
n3

t
/
sin(nx) + xt
π + 2
	
1 −x2
π2

.
6.15 (a) To generate a homogeneous boundary condition write u(x, t) = v(x, t) +
x + t2. The initial-boundary value problem for v is
vt −vxx = (9t + 31) sin(3x/2)
0 < x < π,
v(0, t) = vx(π, t) = 0
t ≥0,
v(x, 0) = 3π
0 ≤x ≤π/2.

350
Solutions of odd-numbered problems
Its solution is
v(x, t) =
∞

n=0
12
2n + 1e−(n+1/2)2t sin[(n + 1/2)x]
+

9e−9t/4
&
4
9
	
t −4
9

e9t/4 +
	4
9

2'
+ 31 × 4
9

1 −e−9t/4
"
sin
	3x
2

.
Finally,
u(x, t) = x + t2 + v(x, t).
(b) The solution is classical in the domain (0, π) × (0, ∞). On the other hand,
the initial condition does not hold at x = 0, t = 0 since it conﬂicts there with the
boundary condition.
6.17 The solution u is given by
u(x, t) = x sin(t) + 1 + t + e−4π2t cos 2πx.
It is clearly classical.
6.19 To obtain a homogeneous boundary condition write u = w + x/π, and obtain
for w:
wt −wxx + hw = −hx
π
0 < x < π, t > 0,
w(0, t) = w(π, t) = 0
t ≥0,
w(x, 0) = u(x, 0) −v(x) = −x
π
0 ≤x ≤π.
The solution for w is
w(x, t) =
∞

n=1
2(−1)n
nπ
	
1 −
h
n2 + h

e−(n2+h)t +
h
n2 + h

sin nx.
This solution is not classical at t = 0, since the sine series does not converge
to −x/π in the closed interval [0, 1].
6.21
u(x, t)=πe−4t cos 2x+
1
20014 e−20012t cos 2001x+
	
t
20012 −
1
20014

cos 2001x.
6.23 (a)
u(x, t)=−e−172π2t cos 17πx
3+172π2
+3e−422π2t cos 42πx+ e3t cos 17πx
3+172π2 .
(b) The general solution has the form
u(x, t) = A0 +
∞

n=1
Ane−n2π2t cos nπx.

Chapter 7
351
The function f (x) = 1/(1 + x2) is continuous in [0, 1], implying that An are all
bounded. Therefore, the series converges uniformly for all t > t0 > 0, and
lim
t→∞u(x, t) = A0 =
 π
0
dx
1 + x2 = π
4 .
6.25
u = L
2 −4L
π2
∞

m=1
e−k (2m−1)2π2
L2
t
(2m −1)2 cos (2m −1)πx
L
+ α
ω sin ωt.
6.27 The PDE is equivalent to
rut = rurr + 2ur.
Set
w(r, t) := u(r, t) −a,
and obtain for w:



rwt = rwrr + 2wr
0 < r < a, t > 0,
w(a, t) = 0
t ≥0,
w(r, 0) = r −a
0 ≤r ≤a.
Solve for w by the method of separation of variables and obtain
w(r, t) =
∞

n=1
An e−n2π2t
a2
1
r sin nπr
a .
The initial conditions then imply
w(r, 0) =
∞

n=1
An sin nπr
a
= r (r −a).
Therefore, An are the (generalized) Fourier coefﬁcients of r(r −a), i.e.
An = 2
a
 a
0
r (r −a) sin nπr
a
dr = −4 a2
n3 π3 [1 −(−1)n].
Chapter 7
7.1 Select ⃗ψ = v ⃗∇u in Gauss’ theorem

D
⃗∇· ⃗ψ(x, y) dxdy =

∂D
⃗ψ(x(s), y(s)) · ˆnds.
7.3
u(x, y) = 4
π
∞

l=1
sin [(2l −1)y]
sinh

k + (2l −1)2 (π −x)

(2l −1) sinh

k + (2l −1)2 π
.

352
Solutions of odd-numbered problems
7.5 It needs to be shown that
M(r1) < M(r2)
∀0 < r1 < r2 < R.
Let Br = {(x, y) | x2 + y2 ≤r2} be a disk of radius r. Choose arbitrary 0 < r1 <
r2 < R. Since u(x, y) is a nonconstant harmonic function in BR, it must be a
nonconstant harmonic function in each subdisk. The strong maximum principle
implies that the maximal value of u in the disk Br2 is obtained only on the disk’s
boundary. since all the points in Br1 are internal to Br2,
u(x, y) <
max
(x,y)∈∂Br2
u(x, y) = M(r2),
∀(x, y) ∈Br1.
In particular,
M(r1) =
max
(x,y)∈∂Br1
u(x, y) < M(r2).
7.7 (b)
u(r, θ) = 3 −r2 cos2 θ + r2
2 + r sin θ,
or, in Cartesian coordinates,
u(x, y) = 3 + y + 1
2(y2 −x2).
7.9 n = 0: A homogeneous harmonic polynomial is of the form P0(x, y) = c and
the dimension of V0 is 1.
n ≥1: A homogeneous harmonic polynomial has the following form in polar co-
ordinates:
Pn(r, θ) = rn(An cos nθ + Bn sin nθ).
Therefore the homogeneous harmonic polynomials of order n ≥1 are spanned by
two basis functions:
v1(r, θ) = rn cos nθ ;
v2(r, θ) = rn sin nθ,
and the dimension of Vn (for n ≥1) is 2.
7.11
u(r, θ) = 4
r sin θ,
or, in Cartesian coordinates,
u(x, y) =
4y
x2 + y2 .

Chapter 7
353
7.13 Outline of the proof:
(i) Motivated by the Poisson formula, deﬁne the function
g(ϕ) =
a2 −r2
a2 −2 a r cos(θ −ϕ) + r2
in the interval [−π, π].
(ii) Prove
a −r
a + r ≤g(ϕ) ≤a + r
a −r .
(12.1)
(iii) Use (12.1) to show
	a −r
a + r

1
2π
 π
−π
f (ϕ) dϕ ≤u(r, θ) ≤
	a + r
a −r

1
2π
 π
−π
f (ϕ) dϕ.
(iv) The result now follows from the mean value theorem.
7.17 (a)
u(x, t) =
∞

n=1
Bne−2n2t sin nx,
where
Bn = 2
π
 π
0
f (x) sin nxdx = 12(−1)n
n3
.
(b) Use Corollary 7.18.
7.19 (a) The mean value theorem for harmonic functions implies
u(0, 0) = 1
2π
 π
−π
u(R, θ)dθ = 1
2π
 π/2
−π/2
sin2 2θdθ = 1
4 .
(b) This is an immediate consequence of the strong maximum principle. The prin-
ciple implies
u(r, θ) ≤
max
ψ∈[−π/2,π/2) u(R, ψ) = 1
for all r < R, and the equality holds if and only if u is constant. Clearly the solution
isnotaconstantfunction,andthereforeu < 1in D.Theinequalityu > 0isobtained
from the strong maximum principle applied to −u.
7.21 The function w(x, t) = e−t sin x is a solution of the problem
wt −wxx = 0
(x, t) ∈QT ,
w(0, t) = w(π, t) = 0
0 ≤t ≤T,
w(x, 0) = sin x
0 ≤x ≤π.

354
Solutions of odd-numbered problems
On the parabolic boundary 0≤u(x, t)≤w(x, t), and therefore, from the maximum
principle 0≤u(x, t)≤w(x, t) in the entire rectangle QT .
Chapter 8
8.1 (a) Use polar coordinates (r, θ) for (x, y), and (R, φ) for (ξ, η), to obtain
∂G R(x, y; ξ, η)
∂ξ
=
ξ(1 −r2/R2)
2π[R2 −2Rr cos(θ −φ) + r2],
and similarly for ∂/∂η. The exterior unit normal at a point (ξ, η) on the sphere is
(ξ, η)/R, therefore,
∂G R(x, y; ξ, η)
∂r
=
R2 −r2
2π R[R2 −2Rr cos(θ −φ) + r2].
(b) limR→∞G R(x, y; ξ, η) = ∞.
8.3 (a) The solution for the Poisson equation with zero Dirichlet boundary condition
is known from Chapter 7 to be
w(r, θ) =
˜f0(r)
2
+
∞

n=1
[ ˜fn(r) cos nθ + ˜gn(r) sin nθ].
(12.2)
Substituting the coefﬁcients ˜fn(r), ˜gn(r) into (12.2), we obtain
w(r, θ) = 1
2
 r
0
K (0)
1 (r, a, ρ)δ0(ρ)ρ dρ + 1
2
 a
r
K (0)
2 (r, a, ρ)δ0(r)ρ dρ
+
∞

n=1
	 r
0
K (n)
1 (r, a, ρ)[δn(ρ) cos nθ + εn(r) sin nθ]ρ dρ

+
∞

n=1
	 a
r
K (n)
2 (r, a, ρ)[δn(r) cos nθ + εn(r) sin nθ]ρ dρ

.
Recall that the coefﬁcients δn(ρ), εn(r) are the Fourier coefﬁcients of the Function
F, hence
δn(ρ) = 1
π
 2π
0
F(ρ, ϕ) cos nϕ dϕ,
εn(r) = 1
π
 2π
0
F(ρ, ϕ) sin nϕ dϕ.
Substitute these coefﬁcients, and interchange the order of summation and integra-
tion to obtain
w(r, θ) =
 a
0
 2π
0
G(r, θ; ρ, ϕ)F(ρ, ϕ) dϕρ dρ,

Chapter 8
355
where G is given by
G(r, θ; ρ, ϕ) = 1
2π



log r
a +
∞

n=1
1
n
r
a
n
−
a
r
nρ
a
n
cos n(θ −ϕ) if ρ < r,
log ρ
a +
∞

n=1
1
n
ρ
a
n
−
	a
ρ

nr
a
n
cos n(θ −ϕ) if ρ > r.
(b) To calculate the sum of the above series use the identities
∞

n=1
1
n zn cos nα =
 z
0
∞

n=1
ζ n−1 cos nα dζ
=
 z
0
cos α −ζ
1 + ζ 2 −2ζ cos α dζ = −1
2 log(1 + z2 −2z cos α).
8.5 (a) On the boundary of R2
+ the exterior normal derivative is ∂/∂y. Therefore,
∂G(x, y; ξ, η)
∂y

y=0
=
η
π[(x −ξ)2 + η2]
x ∈R, (ξ, η) ∈R2
+ .
(b) The function
G(x, y; ξ, η) = −1
4π ln

(x −ξ)2 + (y −η)2 
(x + ξ)2 + (y + η)2

(x −ξ)2 + (y + η)2 
(x + ξ)2 + (y −η)2
"
satisﬁes all the required properties.
8.7 (b) Since
2π
 1
0
exp
	
1
|r|2 −1

rdr ≈0.4665,
the normalization constant c is approximately 2.1436.
8.9 By Exercise 5.20, the kernel K (as a function of (x, t)) is a solution of the heat
equation for t > 0.
Set ρ(x) := (1/√π)e−x2, and consider
ρε(x) := ε−1ρ
	x −y
ε

.
By Exercise 8.7, ρε approximates the delta function as ε →0+.
Take ε =
√
4kt, then ρε(x) = K(x, y, t). Thus, K(x, y, 0) = δ(x −y).
8.11 Hint For (x, y) ∈DR, let
(˜x, ˜y) :=
R2
x2 + y2 (x, y)

356
Solutions of odd-numbered problems
be the inverse point of (x, y) with respect to the circle ∂BR, and set
r =

(x −ξ)2 + (y −η)2, r∗=
%	
x −R2
ρ2 ξ

2
+
	
y −R2
ρ2 η

2
, ρ =

ξ 2 + η2.
Finally, verify (as was done in Exercise 8.1) that the function
G R(x, y; ξ, η) = −1
2π ln Rr
ρr∗
(ξ, η) ̸= (x, y)
is the Green function in DR.
8.13 Fix (ξ, η) ∈BR, and deﬁne for (x, y) ∈BR \ (ξ, η)
NR(x, y; ξ, η) =



−1
2π ln rr∗ρ
R3
(ξ, η) ̸= (0, 0),
−1
2π ln r
R
(ξ, η) = (0, 0),
where
r =

(x −ξ)2 + (y −η)2, r∗=
%	
x −R2
ρ2 ξ

2
+
	
y −R2
ρ2 η

2
, ρ =

ξ 2 + η2.
Verify that
NR(x, y; ξ, η) = −δ(x −ξ, y −η),
and that NR satisﬁes the boundary condition
∂NR(x, y; ξ, η)
∂r
=
1
2π R .
Finally, check that NR satisﬁes the normalization (8.34).
Chapter 9
9.1
(b)
From
the
eikonal
equation
itself
uz(0, 0, 0) =
±
(
1 −u2x(0, 0, 0) −u2y(0, 0, 0) = ±1, where the sign ambiguity means that
there are two possible waves, one propagating into z > 0, and one into z < 0.
The characteristic curves (light rays) for the equations are straight lines perpen-
dicular to the wavefront. Therefore the ray that passes through (0, 0, 0) is in the
direction (0, 0, 1). This implies ux(0, 0, z) = uy(0, 0, z) = 0 for all z, and hence
uxz(0, 0, z) = uyz(0, 0, z) = 0. Differentiating the eikonal equation by z and using
the last identity implies uzz(0, 0, 0) = 0. The result for the higher derivatives is
obtained similarly by further differentiation.

Chapter 9
357
9.3 Hint Verify that the proposed solution (9.26) indeed satisﬁes (9.23) and (9.25),
and that ur(0, t) = 0.
9.5 u(r, t) = 2 + (1 + r2 + c2t2)t.
9.7 The representation (9.35) for the spherical mean makes it easier to interchange
the order of integration. For instance,
∂
∂a Mh(a, ⃗x) = 1
4π

|⃗η|=1
∇h(⃗x + a⃗η) · ⃗ηdsη.
Use Gauss’ theorem (recall that the radius vector is orthogonal to the sphere) to
express the last term as
a
4π

|η|<1
xh(⃗x + a⃗η) d⃗η.
To return to a surface integral notation rewrite the last expression as
a−2
4π x

|⃗x−⃗ξ|<a
h(⃗ξ)d⃗ξ = a−2
4π x
 a
0
dα

|⃗x−⃗ξ|=α
h(⃗ξ)dsξ
= a−2x
 a
0
α2Mh(α, ⃗x)dα.
Multiply the two sides by a2 and differentiate again with respect to the variable a
to obtain the Darboux equation.
9.9 Use the same method as in Subsection 9.5.2 to ﬁnd
λl,n,m = π2
	 l2
a2 + n2
b2 + m2
c2

, ul,n,m(x, y, z)=sin lπx
a
sin nπy
b
sin mπz
c
,
for l, n, m = 1, 2, . . . .
9.11 Hint Differentiate (9.76) with respect to r to obtain one recursion formula, and
differentiate with respect to θ to obtain another recursion formula. Combining the
two recursion formulas leads to (9.77).
9.13 (a) Multiply Legendre equations for vi by v j, i ̸= j, subtract and integrate
over [−1, 1]. It follows that
 1
−1
0
v2

(1 −t2)v′
1
′
−v1

(1 −t2)v′
2
′1
ds = (µ2 −µ1)
 1
−1
v1(s)v2(s) ds.
Integrating the left hand side by parts implies (µ2 −µ1)
 1
−1 v1(s)v2(s) ds = 0.
Since µ1 ̸= µ2, it follows that
 1
−1 v1(s)v2(s) ds = 0.
(b) Suppose that the Legendre equation admits a smooth solution v on [−1, 1]
with µ ̸= k(k + 1). By part (a), v is orthogonal to the space of all polynomials.
Weierstrass’ approximation theorem implies that v is orthogonal to the space
E(−1, 1). This implies v = 0.

358
Solutions of odd-numbered problems
9.15 Write the general homogeneous harmonic polynomial as in Corollary 9.24,
and express it in the form Q = rnF(φ, θ). Substitute Q into the spherical form of
the Laplace equation (see (9.85)) to get that F satisﬁes
1
sin φ
∂
∂φ
	
sin φ ∂F
∂φ

+
1
sin2 φ
∂2F
∂θ2 = −n(n + 1)F.
Therefore F is a spherical harmonic (or a combination of spherical harmonics).
9.17 (a) By Exercise 9.13, each Legendre polynomial Pn is an n-degree polynomial
whichisorthogonalto Pm forn ̸= m.Thesetogetherwiththenormalization Pn(1) =
1 determine the Legendre polynomials uniquely.
Set
Qn(t) :=
1
2nn!
dn
dtn (t2 −1)n.
Clearly, Qn is an n-degree polynomial. Repeatedly integrating by parts implies the
orthogonality of Qn. Since Qn(1) = 1 it follows that Pn = Qn.
(b) Compute
 1
−1
Pn(t)2dt =
1
22nn!2
 1
−1
 dn
dtn (t2−1)n
2
dt
(12.3)
= (2n!)
22nn!2
 1
−1
(t2−1)n dt =
2
2n+1.
The general case of associated Legendre functions can be proved similarly using
(9.100) and (12.3).
9.19 Hint Note that for y ∈BR the function (

(|⃗x||⃗y|/R)2 + R2 −2⃗x · ⃗y ) is
harmonic in BR, and that on ∂BR the identity ∂/∂n = ∂/∂r holds.
9.21 Hints (a) Substitute ⃗y = ⃗0 into the Poisson integral formula.
(b) Prove that U(r) := (1/NωNr N−1)

∂Br u(⃗x) dσ⃗x is the constant function.
(c) The proof of the strong maximum principle for domains in RN is exactly the
same as for planar domains, and the weak maximum principle is a direct conse-
quence of it.
9.23 Hints (i) Write ⃗x = (x′, xN), and let ˜x := (x′, −xN). Then (˜x; ⃗y) is harmonic
as a function of ⃗x in R2
+, while ⃗x(⃗x; ⃗y) = −δ(⃗x −⃗y).
(ii) Notice that for ⃗y ∈∂RN
+, the identity ∂/∂n = ∂/∂yN holds.
9.25 Hint The general formula for the eigenfunction expansion is (9.178). The spe-
ciﬁc cases of the rectangle and the disk are solved in (9.61) and (9.80) respectively.

Chapter 10
359
Chapter 10
10.1 The ﬁrst variation is δK = 2
 1
0 y′ψ′dt, where ψ is the variation function.
Therefore the Euler–Lagrange equation is y′′ = 0, and the solution is yM(t) =
t. Expand the functional with respect to the variation ψ about y = yM, to get
K(uM + ψ) = K(uM) +
 1
0 (ψ′)2dt. This shows that yM is a minimizer, and it is
indeed unique.
10.3 The Euler–Lagrange equation is u −gu3 = 0, x ∈D, while u satisﬁes the
natural boundary conditions ∂nu = 0 on ∂D
10.5 Hints The action is
J =
 t2
t1

D
1
2u2
t −1
2|∇u|2 −V (u)

dx.
The Euler–Lagrange equation is utt −u + V ′(u) = 0.
10.7 (a) Introduce a Lagrange multiplier λ, and solve the minimization problem
min

D
|∇u|2dxdy + λ
	
1 −

d
u2dxdy

,
for all u that vanish on ∂D. Equate the ﬁrst variation to zero to ﬁnd the Euler–
Lagrange equation
u = −λu
x ∈D,
u = 0
x ∈∂D.
10.9 Hints The eigenvalue problem is
X(iv)(x) −λX(x) = 0,
X(0) = X′(0) = X(b) = X′(b) = 0.
Multiply both sides by X and integrate over (0, b). Perform two integrations by
parts and use the boundary conditions to derive
 b
0

X′′2 dx = λ
 b
0
X2dx.
Therefore λ > 0.
The solution satisfying the boundary conditions at x = 0 is
X(x) = A (cosh αx −cos αx) + B (sinh αx −sin αx) .
Enforcing the boundary condition at x = b implies that a necessary and sufﬁcient
condition for a nontrivial solution is indeed given by condition (10.73).
10.11 Hints (i) ∥vn∥= 1.
(ii) By the Riemann–Lebesgue lemma, limn→∞⟨vn, v⟩= 0.
(iii) Note that strong convergence implies ∥vn∥→∥v∥.

360
Solutions of odd-numbered problems
Chapter 11
11.1
∂xyu(xi, y j) = Ui+1, j+1 −Ui−1, j+1 −Ui+1, j−1 + Ui−1, j−1
4xy
.
11.5 Hints The analytic solution is
u(x, t) = 8
π
∞

m=1
e−(2m−1)2t
(2m −1)3 sin(2m −1)x.
In comparing the numerical solution and analytic solution at (x, t) = (π/4, 2),
observe that very few Fourier terms are needed to capture the right answer.
11.7 The analytic solution is u(x, t) = t5. The numerical procedure blows up be-
cause of the violation of the stability condition.
11.9 Let (i, j) be the index of an internal maximum point. Both terms on the left
hand side of (11.27) are dominated by Ui, j. Therefore, if Ui, j is positive, the left
hand side is negative which is a contradiction.
11.13 Let pi, i = 1, . . . , 4(N −2) be the set of boundary point. For each i deﬁne
the harmonic function Ti, such that Ti(pi) = 1, while Ti(p j) = 0 if j ̸= i. Clearly
the set {Ti} spans all solutions to the Laplace equation in the grid. It also follows
directly from the construction that the set {Ti} is linearly independent.

Appendix: Useful formulas
A.1 Trigonometric formulas
(1)

xn sin ax dx = −xn
a cos ax + n
a

xn−1 cos ax dx.
(2)

xn cos ax dx = xn
a sin ax −n
a

xn−1 sin ax dx.
(3)

eax sin bx dx =
eax
a2 + b2 (a sin bx −b cos bx).
(4)

eax cos bx dx =
eax
a2 + b2 (a cos bx + b sin bx).
(5) sin(α ± β) = sin α cos β ± sin β cos α.
(6) cos(α ± β) = cos α cos β ∓sin α sin β.
(7) cos α cos β = 1
2[cos(α + β) + cos(α −β)].
(8) sin α sin β = 1
2[cos(α −β) −cos(α + β)].
(9) sin α cos β = 1
2[sin(α + β) + sin(α −β)].
(10) sin α + sin β = 2 sin α + β
2
cos α −β
2
.
(11) cos α + cos β = 2 cos α + β
2
cos α −β
2
.
(12) tan α + tan β = sin(α + β)
cos α cos β .
(13) sin3 α = 1
4(3 sin α −sin 3α).
(14) cos3 α = 1
4(3 cos α + cos 3α).
361

362
Appendix: Useful formulas
A.2 Integration formulas
(1)
 
D
∇· w dxdy =

∂D
w · n dσ.
(2)
 
D
(Qx −Py) dxdy =

(Pdx + Qdy).
(3)
 
D
(vu + ∇u · ∇v) dxdy =

∂D
v ∂u
∂n dσ.
(4)
 
D
(vu −uv) dxdy =

∂D
	
v ∂u
∂n −u ∂v
∂n

dσ.
(5) ∂
∂t
 b(t)
a(t)
G(ξ, t) dξ = G(b(t), t)b′(t) −G(a(t), t)a′(t) +
 b(t)
a(t)
∂
∂t G(ξ, t) dξ.
A.3 Elementary ODEs
(1) The general solution of the linear ODE y′ + P(x)y = Q(x) is given by
y(x) = e−

P(x) dx
	
Q(x)e

P(x) dx dx + c

.
(2) The general solution of the ODE
y′′ + λy = 0
λ ∈R,
is given by
y(x) =



αe
√−λx + βe−√−λx = ˜α cosh(√−λx) + ˜β sinh(√−λx)
λ < 0,
α + βx
λ = 0,
α cos(
√
λx) + β sin(
√
λx)
λ > 0,
where α, β, ˜α, ˜β are arbitrary real numbers.
(3) Let A, B, C ∈R, and let r1,r2 be the roots of the (quadratic) indicial equation Ar(r −
1) + Br + C = 0. Then the general solution of the Euler (equidimensional) equation:
Ax2y′′ + Bxy′ + Cy = 0,
is given by
y(x) =



αxr1 + βxr2
r1,r2 ∈R, r1 ̸= r2,
αxr1 + βxr1 log x
r1,r2 ∈R, r1 = r2,
αxλ cos(µ log x) + βxλ sin(µ log x)
r1 = λ + iµ ∈C,
where α, β are arbitrary real numbers.

A.5 Differential operators in spherical coordinates
363
A.4 Differential operators in polar coordinates
We use the notation er and eθ to denote unit vectors in the radial and angular
direction, respectively, and ez to denote a unit vector in the z direction. A vector ⃗u
is expressed as ⃗u = u1er + u2eθ. We also use V (r, θ) to denote a scalar function.
∇V = ∂V
∂r er + 1
r
∂V
∂θ eθ.
∇· ⃗u = 1
r
∂(ru1)
∂r
+ 1
r
∂u2
∂θ .
∇× ⃗u =
1
r
∂(ru2)
∂r
−1
r
∂u1
∂θ

ez.
V = ⃗∇· ⃗∇V = Vrr + 1
r Vr + 1
r2 Vθθ.
A.5 Differential operators in spherical coordinates
We use the notation er, eθ, and eφ to denote unit vectors in the radial, vertical angular
direction, and horizontal angular direction, respectively. A vector ⃗u is expressed as
⃗u = u1er + u2eθ + u3eφ. We also use V (r, θ, φ) to denote a scalar function.
∇V = ∂V
∂r er + 1
r
∂V
∂θ eθ +
1
r sin θ
∂V
∂φ eφ.
∇· ⃗u = 1
r2
∂(r2u1)
∂r
+ 1
r
∂(sin θ u2)
∂θ
+
1
r sin θ
∂u3
∂φ .
∇× ⃗u =
1
r sin θ
∂(sin θ u3)
∂θ
−∂u2
∂φ

er + 1
r
 1
sin θ
∂u1
∂φ −∂(ru3)
∂r

eθ
+ 1
r
∂(ru2)
∂r
−∂u1
∂θ

eφ.
V = ⃗∇· ⃗∇V = 1
r2
∂
∂r
	
r2 ∂V
∂r

+ 1
r2

1
sin φ
∂
∂φ
	
sin φ ∂V
∂φ

+
1
sin2 φ
∂2V
∂θ2

.

References
[1] D.M. Cannell, George Green; Mathematician and Physicist 1793–1841, second
edition. Philadelphia, PA: Society for Industrial and Applied Mathematics (SIAM),
2001.
[2] G.F. Carrier and C.E. Pearson, Partial Differential Equations, Theory and Technique,
second edition. Boston, MA: Academic Press, 1988.
[3] W. Cheney and D. Kincaid, Numerical Mathematics and Computing. Paciﬁc Grove,
CA: Brooks Cole, Monterey, 1985.
[4] R. Courant and D. Hilbert, Methods of Mathematical Physics, Vols. I,II, New York,
NY: John Wiley & Sons, 1996.
[5] N.H. Fletcher and T.D. Rossing, The Physics of Musical Instruments. New York, NY:
Springer-Verlag, 1998.
[6] I. Gohberg and S. Goldberg, Basic Operator Theory. Boston, MA: Birkh¨auser,
2001.
[7] P.R. Halmos, Introduction to Hilbert Space and the Theory of Spectral Multiplicity.
Providence, RI: American Mathematical Society – Chelsea Publications, 1998.
[8] A.L. Hodgkin and A.F. Huxley, “A quantitative description of membrane current and
its application to conduction and excitation in nerve”, Journal of Physiology 117,
500–544, 1952.
[9] E.L. Ince, Ordinary Differential Equations. Mineda, NY: Dover, 1944.
[10] D. Jackson, Classical Electrodynamics, second edition. New York, NY: Wiley, 1975.
[11] F. John, Partial Differential Equations, reprint of the fourth edition, Applied
Mathematical Sciences Vol. 1. Berlin: Springer-Verlag, 1991.
[12] J.D. Murray, Mathematical Biology, second edition. Berlin: Springer-Verlag, 1993.
[13] A. Pinkus and S. Zafrani, Fourier Series and Integral Transforms. Cambridge:
Cambridge University Press, 1997.
[14] M.H. Protter and H.F. Weinberger, Maximum Principles in Differential Equations,
corrected reprint of the 1967 original. New York, NY: Springer-Verlag, 1984.
[15] R.D. Richtmyer and K.W. Morton, Difference Methods for Initial Value Problems,
reprint of the second edition. Malabar, FL: Robert E. Krieger, 1994.
[16] M. Schatzman, Numerical Analysis – A Mathematical Introduction. Oxford: Oxford
University Press, 2002.
[17] L. Schiff, Quantum Mechanics. Tokyo, Mcgraw-Hill, 1968.
[18] G.D. Smith, Numerical Solutions of Partial Differential Equations, Finite Difference
Methods, third edition, Oxford Applied Mathematics and Computing Science Series.
New York, NY: Oxford University Press, 1985.
364

References
365
[19] I.N. Sneddon, Elements of Partial Differential Equations. New York, NY:
McGraw-Hill, 1957.
[20] J.L. Troutman, Variational Calculus and Optimal Control, second edition.
Undergraduate Texts in Mathematics. New York, NY: Springer-Verlag, 1996.
[21] G.N. Watson, A Treatise on the Theory of Bessel Functions. Cambridge: Cambridge
University Press, 1966.
[22] G.B. Whitham, Linear and Nonlinear Waves. New York, NY: John Wiley, 1974.
[23] E. Zauderer, Partial Differential Equations of Applied Mathematics, second edition.
New York, NY: John Wiley & Sons, 1989.

Index
acoustics, 7–11
action, 292, 293
adjoint operator, 213
admissible surface, 283
asymptotic behavior
Bessel function, 249
eigenvalue, 154, 245
solution, 155
backward
difference, 312
heat operator, 213
wave, 77
Balmer, Johann Jacob, 263, 266
basis, 297, 299, 301, 302, 305, 310, 330, 336
Zernike, 302
Bernoulli, Daniel, 98
Bessel equation, 133, 248, 257, 262, 269
Bessel function, 248
asymptotic, 249
properties, 249
Bessel inequality, 138
Bessel, Friedrich Wilhelm, 248
biharmonic
equation, 16
operator, 213, 290
Born, Max, 263
boundary conditions, 18–20
Dirichlet, 18, 108, 174
ﬁrst kind, 108
mixed, 19, 109
natural, 288, 307
Neumann, 19, 108, 109, 175
nonhomogeneous, 164
nonlocal, 19
oblique, 19
periodic, 109
Robin, 19, 109, 175
second kind, 108
separated, 108, 130, 133
third kind, 19, 109
Brown, Robert, 13
Brownian motion, 13–14
cable equation, 119–123
semi-inﬁnite, 121
calculus of variations, 282–308
canonical form, 66, 231
elliptic, 66, 70–73
hyperbolic, 66–69
parabolic, 66, 69–70
wave equation, 76
Cauchy problem, 24, 27, 55, 76, 78, 176, 224, 229,
236, 241
Cauchy sequence, 298
Cauchy, Augustin Louis, 24
Cauchy–Schwartz inequality, 137
central difference, 312
CFL condition, 324
change of coordinates, 65
characteristic curve, 27, 229
characteristic equations, 27, 67, 226
characteristic function, 137
characteristic projection, 68
characteristic strip, 54, 228
characteristic surface, 229
characteristic triangle, 82
characteristics, 31, 67, 68, 77
method, 25–63, 226
clarinet, 267–269
classical Fourier system, 135
classical solution, 3, 43, 79, 175
classiﬁcation of PDE, 3, 64–75, 228–234
compact support, 211, 234, 330
compatibility condition, 55, 99, 109, 167, 188, 252,
287
complementary error function, 129
complete orthonormal sequence, 138, 299
compression wave, 46
conservation laws, 8, 9, 41–50
consistent numerical scheme, 315
constitutive law, 6, 11, 12, 295
convection equation, 11, 17
convergence
in distribution sense, 212
in norm, 137
in the mean, 137
366

Index
367
convergence (cont.)
numerical scheme, 316
strong, 297
weak, 300
convex functional, 291
Courant, Richard, 309
Crank–Nicolson method, 317
curvature, 295
δ function, 211, 224, 272
d’Alembert formula, 79–97, 208, 234
d’Alembert, Jean, 12, 98
Darboux equation, 238, 279
Darboux problem, 95
Darboux, Gaston, 237, 238
degenerate states, 247, 257, 266
delta function, 211, 224, 272
diagonally dominated matrix, 321
difference equation, 13, 313, 314
difference scheme, 13, 313, 319, 322, 329, 331
differential operator, 4
diffusion coefﬁcient, 7, 124
diophantic equations, 247
dirac distribution, 211, 224, 272
Dirichlet condition, 18, 100
Dirichlet functional, 284, 285
Dirichlet integral, 284, 285, 287, 300
Dirichlet problem, 174, 209–218, 285
ball, 257, 262
cylinder, 261
disk, 195
eigenfunction expansion, 273
eigenvalue, 243
exterior domain, 198
numerical solution, 318–322
rectangle, 188
sector, 198
spectrum, 242
stability, 182
uniqueness, 181, 183
Dirichlet, Johann Lejeune, 18
dispersion relation, 122
distribution, 211, 223
convergence, 212
Dirac, 211, 224, 272
divergence theorem, 7, 8, 182, 362
domain of dependence, 83, 89
drum, 260, 269
Du-Fort–Frankel method, 318
Duhamel principle, 127, 222, 276
eigenfunction, 101, 131
expansion, 114, 130–172
orthogonality, 143, 243
principal, 152
properties, 243–245
real, 146, 243
zeros, 154, 245
eigenvalue, 131
asymptotic behavior, 154, 245
existence, 147, 244
multiplicity, 102, 132, 135, 146, 243, 244, 247, 257
principal, 152, 244
problem, 101, 131, 242–258
properties, 243–245
real, 145, 243
simple, 102, 132, 146, 245
eigenvector, 132
eikonal equation, 15, 26, 50–52, 57, 233, 292
element (for FEM), 331
elliptic equation, 65, 173–183, 209, 231, 232, 305, 329
elliptic operator, 65
energy integral, 116–119
energy level, 263
energy method, 116–119, 182
entropy condition, 47
equation
elliptic, 65, 209, 231, 232, 305, 329
homogeneous, 4
hyperbolic, 65, 231, 232
Klein–Gordon, 233
nonhomogeneous, 4
parabolic, 65, 209, 231, 232
error function, 129
complementary, 129
Euler equation, 41
Euler ﬂuid equations, 9
Euler equidimensional equation, 362
Euler, Leonhard, 9, 41
Euler–Lagrange equation, 285, 331
even extension, 94, 235, 238
expansion wave, 45
explicit numerical scheme, 317
FDM, 310–324
FEM, 306, 310, 329–334
element, 331
Fermat principle, 292
Fermat, Pierre, 26
ﬁnite difference method (FDM), 310–324
ﬁnite differences, 311–312
ﬁnite elements method (FEM), 306, 310, 329–334
element, 331
ﬁrst variation, 284
ﬁrst-order equations, 23–63
existence, 36–38
high dimension, 226–228
Lagrange method, 39–41, 62
linear, 24
nonlinear, 52–58
uniqeness, 36–38
ﬂare, 268
ﬂexural rigidity, 289
ﬂute, 267, 268
formally selfadjoint operator, 213
formula
Poisson, 202
Rayleigh-Ritz, 152, 244, 308
forward difference, 311
forward wave, 77
Fourier classical system, 135
Fourier coefﬁcients, 103, 138

368
Index
Fourier expansion, 98, 139
convergence, 148, 244
convergence on average, 139
convergence in norm, 139
convergence in the mean, 139
generalized, 139, 244
Fourier law, 6
Fourier series, 103, 258
Fourier, Jean Baptiste Joseph, 5, 98, 103, 139, 148
Fourier–Bessel coefﬁcients, 252
Fourier–Bessel series, 251
Fresnel, Augustin, 26
Friedrichs, Kurt Otto, 309
Frobenius–Fuchs method, 248, 254, 265
function
characteristic, 137
error, 129
harmonic
mean value, 179–180, 274, 280
piecewise continuous, 136
piecewise differentiable, 136
real analytic, 71
functional, 283
bounded below, 304
convex, 291
Dirichlet, 284, 285
ﬁrst variation, 284
linear, 284
second variation, 291
fundamental equations of mathematical physics, 66
fundamental solution, 213, 224, 270
Laplace, 178, 209, 271
uniqueness, 213
Galerkin method, 303–306
Galerkin, Boris, 306
Gauss theorem, 7, 8, 176, 182, 362
Gauss–Seidel method, 325, 326
Gaussian kernel, 224
general solution, 40, 76–78, 230, 239, 362
generalized Fourier coefﬁcients, 103, 138
generalized Fourier expansion, 139
generalized Fourier series, 103, 258
generalized solution, 77, 104, 112
geometrical optics, 14–15, 287
Gibbs phenomenon, 150, 192, 195
Gibbs, Josiah Willard, 150
Green’s formula, 88, 143, 144, 160, 182, 210, 243,
271, 284
Green’s identity, 182, 210, 271, 284
Green’s representation formula, 211, 219, 271
Green’s function, 209–221, 272
ball, 274
deﬁnition, 214
Dirichlet problem, 209–218
disk, 217, 224
exterior of disk, 225
half-plane, 218, 224
half-space, 275
higher dimensions, 269–275
monotonicity, 217, 273
Neumann problem, 219–221
positivity, 216, 273
properties, 273
rectangle, 281
symmetry, 215, 273
uniqueness, 273
Green, George, 208
grid, 311
ground state, 152
energy, 152
guitar, 267
Hadamard example, 176
Hadamard method of descent, 241
Hadamard, Jacques, 2, 176, 239
Hamilton characteristic function, 26
Hamilton principle, 292
Hamilton, William Rowan, 1, 15, 25, 26, 39, 292,
293
Hamiltonian, 292–296
harmonic function, 173
harmonic polynomial
homogeneous, 177, 205
Harnack inequality, 205
heat equation
Dirichlet problem, 185
uniqueness, 118
maximum principle, 184
numerical solution, 312–318
separation of variables, 99–109, 259
stability, 185
heat ﬂow, 6, 109, 130
heat ﬂux, 6, 19, 175
heat kernel, 129, 221–224, 275–278, 281
properties, 276–278
Heisenberg, Werner, 263
Helmholtz equation, 204, 281
Hilbert space, 298
Hilbert, David, 298
homogeneous equation, 4
Huygens’ principle, 239
Huygens, Christian, 26
hydrodynamics, 7–11
hydrogen atom, 263–266
hyperbolic equation, 65, 231, 232
hyperbolic operator, 65
ill-posed problem, 2, 82, 176
implicit numerical scheme, 317
induced norm, 136
inequality
Bessel, 138
Cauchy–Schwartz, 137
Harnack, 205
triangle, 136
initial condition, 1, 24, 79, 99, 226, 229, 313
initial curve, 26
initial value problem, 17, 24, 89
inner product, 136
induced norm, 136
space, 136

Index
369
insulate, 99
insulated boundary condition, 109, 125
integral surface, 28
inverse point
circle, 217, 356
line, 218
sphere, 274
iteration, 318, 325–327
Jacobi method, 325
jump discontinuity, 136
Kelvin, Lord, 122
Lagrange identity, 142, 146
Lagrange method, 39–41, 62
Lagrange multiplier, 307, 359
Lagrange, Joseph-Louis, 16, 39, 283, 294
Lagrangian, 292–296
Laguerre equation, 265
Laplace equation, 15, 173–206
ball, 262
cylinder, 261
eigenvalue problem, 242–258
fundamental solution, 178
Green’s function, 209–218
higher dimension, 269–275
maximum principle, 178–181
numerical solution, 318–322
polar coordinates, 177
separation of variables, 187–201, 245–258,
261–263
Laplace, Pierre-Simon, 16, 283
Laplacian, 16
cylindrical coordinates, 261
polar coordinates, 363
spectrum, 245
ball, 257
disk, 251
rectangle, 245
spherical coordinates, 363
least squares approximation, 287
Legendre associated equation, 255
Legendre equation, 254
Legendre polynomial, 254
Legendre, Adrien-Marie, 254
Lewy, Hans, 309
linear equation, 3
ﬁrst-order, 24
linear functional, 284
linear operator, 4
linear PDE, 3
Liouville, Joseph, 131, 147
Maupertuis, Pierre, 294
maximum principle
heat equation, 184
numerical scheme, 319
strong, 180, 274
weak, 178
mean value principle, 179–180, 204, 274, 280
membrane, 11, 16, 122, 260–261, 266, 269, 288, 289,
308
mesh, 311
minimal surface, 16, 282–287
equation, 16, 285, 286
minimizer, 283, 284, 304, 331
existence, 299
uniqueness, 291
minimizing sequence, 300
modes of vibration, 267, 269
Monge, Gaspard, 53
multiplicity, 102, 132, 135, 243, 244, 247, 257
musical instruments, 266–269
natural boundary conditions, 288, 307
Navier, Claude, 9
net, 311
Neumann boundary conditions, 19, 108, 109, 131,
174, 193, 242
Neumann function, 219–221, 224
Neumann problem, 110, 125, 175, 183, 195, 203,
219–221, 318
Neumann, Carl, 19
Newtonian potential, 211, 271
nodal lines, 245
nodal surfaces, 245
nonhomogeneous boundary conditions, 164
nonhomogeneous equation, 4, 114–116,
159–164
norm, 136
normal modes, 267, 269
numerical methods, 309–336
linear systems, 324–329
numerical scheme, 310
consistent, 315
convergence, 316
explicit, 317
implicit, 317
stability, 314
stability condition, 315
odd extension, 93
operator, 4
elliptic, 231
formally self-adjoint, 213
hyperbolic, 231
parabolic, 231
symmetric, 143
order of PDE, 3
organ, 268
orthogonal projection, 138
orthogonal sequence, 137
orthogonal vectors, 137
orthonormal sequence, 137
complete, 138, 148, 299
orthonormal system
complete, 244
outward normal vector, 6
parabolic boundary, 184
parabolic equation, 65, 209, 231, 232

370
Index
parabolic operator, 65
parallelogram identity, 94
Parseval identity, 138
PDE, 1
classiﬁcation, 3, 64–75, 228–234
linear, 3
order, 3
quasilinear, 3, 9, 24–50
semilinear, 3
system, 3
periodic eigenvalue problem, 134, 196, 253
periodic problem, 171
periodic solution, 91
periodic Sturm–Liouville problem, 134, 196, 253
piecewise continuous function, 136
piecewise differentiable function, 136
pipes
closed, 268
open, 268
Planck constant, 17, 263
Planck quantization rule, 265
plate equation, 289
Plateau, Joseph Antoine, 283
Poisson equation, 13, 174, 219, 289
separation of variables, 199
Poisson formula, 201–204
Poisson kernel, 202, 215, 223, 224, 272–274,
280
Neumann, 203
Poisson ratio, 289
Poisson, Simeon, 174
principal eigenfunction, 152
principal eigenvalue, 152, 244
principal part, 64, 228
product solutions, 99, 100
quasilinear equation, 3, 9, 24–50
random motion, 13–14
Rankine–Hugoniot condition, 47, 49
Rayleigh quotient, 151–154, 244, 302
Rayleigh, Lord, 152
Rayleigh–Ritz formula, 152, 244, 308
real analytic function, 71
reﬂection principle, 217, 218, 224, 225, 275, 280
refraction index, 15, 50
region of inﬂuence, 83, 324
regular Sturm–Liouville problem, 133
resonance, 164, 261
Riemann–Lebesgue lemma, 138
Ritz method, 301–303
Rodriguez formula, 280
round-off error, 313
Runge, Carl, 26
Rydberg constant, 263
scalar equation, 3
Schr¨odinger equation, 16
hydrogen atom, 263–266
Schr¨odinger operator, 151, 152
Schr¨odinger, Erwin, 16, 263
second variation, 291
second-order equation
Cauchy problem, 229–234
classiﬁcation, 64–75, 228–234
semi-inﬁnite cable, 121
semi-inﬁnite string, 93, 94
semilinear equation, 3
separated solutions, 99, 100
separation of variables, 98–172, 245–263
shock wave, 41–50
similarity solution, 129
simple eigenvalue, 102, 132, 146, 245
singular Sturm–Liouville problem, 133, 254
soap ﬁlm, 283
Sobolev space, 299
Sobolev, Sergei, 299
solution
classical, 3, 79
even, 91
general, 40, 76–78, 230, 239, 362
generalized, 77, 104, 112
odd, 91
periodic, 91
strong, 3
weak, 3, 41–50, 296–301
Sommerfeld, Arnold, 26
SOR method, 325, 326
spectral radius, 328
spectrum, 151, 152, 242
hydrogen atom, 263–266
Laplacian
ball, 257
disk, 251
rectangle, 245
spherical harmonics, 256, 262, 266
spherical mean, 237
square wave, 150
stability
CFL condition, 324
Dirichlet problem, 182
heat equation, 185
numerical scheme, 314
wave equation, 90
stiffness matrix, 330
Stokes, George Gabriel, 9
string, 11–12, 19, 79, 87, 98, 109, 117, 130, 164,
266–267, 294
semi-inﬁnite, 93, 94
strip equations, 54, 228
strong convergence, 297
strong maximum principle, 180, 274
strong solution, 3
Sturm, Jacques Charles, 131, 147
Sturm–Liouville asymptotic behavior
eigenvalue, 154
solution, 155
Sturm–Liouville eigenfunctions, 141–158
Sturm–Liouville eigenvalues, 141–158
Sturm–Liouville operator, 132
Sturm–Liouville problem, 131, 133–135, 141–158
periodic, 134, 196, 253

Index
371
Sturm–Liouville problem (cont.)
regular, 133
singular, 133, 254
superposition principle, 4, 89, 92, 99, 103
generalized, 104
support, 85
symmetric operator, 143
system, 3
telegraph equation, 128, 170
temperature, 6, 8, 18, 99, 123–124
equilibrium, 16, 174
tension, 11, 295
test function, 154, 329–331
Thomson, William, 119, 122
trace formula, 276
transcendental equation, 155
transport equation, 8, 17, 121
transversality condition, 30, 227, 228
generalized, 55
traveling waves, 77
triangle inequality, 136
Tricomi equation, 68
truncation error, 312
Turing, Alan Mathison, 245
uniqueness, 36–38, 82, 87, 182, 291
Dirichlet problem, 181
energy method, 116–119
Fourier expansion, 115
variational methods, 282–308
viscosity, 9
von Neumann, John, 298
wave compression, 46
wave equation, 10–12, 14, 26, 76–97, 266, 295,
309
Cauchy problem, 78–82
domain of dependence, 83, 89
general solution, 76–78
graphical method, 84
nonhomogeneous, 87–92
numerical solution, 322–324
parallelogram identity, 94
radial solution, 234–236
region of inﬂuence, 83, 324
separation of variables, 109–114, 260–261
stability, 90
three-dimensional, 234–241
two-dimensional, 241–242
wave expansion, 45
wave number, 15, 233
wave speed, 12, 76, 77
weak convergence, 300
weak solution, 3, 41–50, 296–301
Webster’s horn equation, 268
weight function, 132
well-posedness, 2, 9, 30, 81, 82, 90,
176
Weyl formula, 155, 245
Weyl, Herman, 155
wine cellars, 123–124
Young, Thomas, 26
Zeeman effect, 266
Zernike basis, 302
Zernike, Frits, 302


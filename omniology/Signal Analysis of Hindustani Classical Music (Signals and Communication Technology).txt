Signals and Communication Technology
Signal Analysis 
of Hindustani 
Classical Music
Asoke Kumar Datta · Sandeep Singh Solanki
Ranjan Sengupta · Soubhik Chakraborty
Kartik Mahto · Anirban Patranabis

Signals and Communication Technology

More information about this series at http://www.springer.com/series/4748

Asoke Kumar Datta
• Sandeep Singh Solanki
Ranjan Sengupta
• Soubhik Chakraborty
Kartik Mahto
• Anirban Patranabis
Signal Analysis of Hindustani
Classical Music
123

Asoke Kumar Datta
Sir C V Raman Centre for Physics
and Music
Jadavpur University
Kolkata, West Bengal
India
Sandeep Singh Solanki
Department of Electronics
and Communication Engineering
Birla Institute of Technology, Mesra
Ranchi, Jharkhand
India
Ranjan Sengupta
Sir C V Raman Centre for Physics
and Music
Jadavpur University
Kolkata, West Bengal
India
Soubhik Chakraborty
Department of Mathematics
Birla Institute of Technology, Mesra
Ranchi, Jharkhand
India
Kartik Mahto
Department of Electronics
and Communication Engineering
Birla Institute of Technology, Mesra
Ranchi, Jharkhand
India
Anirban Patranabis
Sir C V Raman Centre for Physics
and Music
Jadavpur University
Kolkata, West Bengal
India
ISSN 1860-4862
ISSN 1860-4870
(electronic)
Signals and Communication Technology
ISBN 978-981-10-3958-4
ISBN 978-981-10-3959-1
(eBook)
DOI 10.1007/978-981-10-3959-1
Library of Congress Control Number: 2017932638
© Springer Science+Business Media Singapore 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore

Prologue
This book looks at music through the eyes of science. Music is subjective, and
science is objective. It is thus an attempt to translate subjectivity into objectivity to
the extent possible. The reason is not too difﬁcult to guess. One argument is that
anything in modern times gets an aura of credibility if it is vetted by science. But a
more important aspect is that science with its paradigm makes it possible to gain
intricate knowledge of the interplay of different objective elements which together
create the subjectivity. It is possible because these two seemingly disjoint issues are,
in reality, joined seamlessly by what we may call the ‘public subjectivity’. For
example, ‘the Sun rises in the east’ is subjective. But it is common to many people
and therefore is a public subjectivity. And it becomes truth. In fact, all objective
truths are sublimated common subjectivity unless they are axiomatic. Thus, it
stands to reason to approach music from the objective rationality of science. In fact,
one can say science as a paradigm of converting subjectivity to objectivity and
technology as the paradigm of converting objectivity to subjectivity. And this lens
of science revealed the genesis of human music and its evolution from a social
necessity of the early ages to the aesthetic need of modern era. This book contains a
glimpse of it.
Of the different classes of music, the genre called Hindustani music, to which
this book is devoted, belongs to the class of melodic music. However, the basic
features of the other two classes namely chordal music and rhythmic music are also
present to an extent required to blossom the melody of Hindustani music. Melody is
horizontal sonority of stationary states, notes and microtones, embellished by
various transitional movements, meends (glides). All the italicized words above are
of course subjective. This book attempts to bring out the rationality of them to this
genre by ﬁnding the objective elements such as fundamental frequency, harmonics,
anharmonics, resonances, and psychological objectivities such as consonance and
dissonance, in short, of quasi-stationary states on one hand. On the other hand, it
also examines the dynamic nature of the sound in between these states to classify
them for an understanding of their role in building up of the aesthetics of music.
The scientiﬁc probe into the horizontal sonority led to the formulation of the
psycho-perceptual theory of musical scales, and the rationale behind the existence
v

of 22 shrutis, the nuances like that of deviating from exact ratio values leading often
to different pitch value states in the same quasi-stationary states. The theoretical
approaches have been supplemented by large-scale data analysis from the perfor-
mances of contemporary musicians. The details of the technological processes
along with the results form one fundamental contribution to the Hindustani music
and are dealt with due emphasis in this book.
A formidable portion of this book presents the essences of scientiﬁc investiga-
tions conducted in the ITC Sangeet Research Academy (ITC SRA) during the last
three decades. This book provides an opportunity to preserve and make available to
the researchers in associated ﬁelds; these activities spanned over a wide spectrum of
music particularly in the north Indian context. The rest of the book has contributions
from another, relatively younger, scientiﬁc music research team at Birla Institute of
Technology, Mesra, Ranchi. This second team has shown a lot of promise having
attained the double distinction of publishing the ﬁrst book on computational musi-
cology in Hindustani music (see the reference Chakraborty et al. 2014 in Chap. 1 of
this book) and also contributing signiﬁcantly in the area of music and medicine with
the help of a medical team at Rajendra Institute of Medical Sciences (RIMS), Ranchi
(see the references Singh et al. 2013, 2016 in Chap. 1 of this book). This book, which
focuses on musical signal processing of Hindustani classical music, resulted from the
combined effort of these two music research teams and provides, with a nice blend of
youth and experience, a wide variety of topics that will be of immense value to any
scientiﬁc researcher in Indian music. The activities cover extensive studies on
Hindustani music particularly the in-depth study of vocal performances of con-
temporary artists and are reported in a chapter. One chapter includes acoustic studies
of Indian string, wind and percussion instruments. A whole chapter is devoted on
Tanpura, its material structure and special musical properties as the essential drone
instrument for classical music. This chapter also includes relevant historical and
musicological information in brief. Acoustics of voice including singing voice has
also been presented. A chapter is devoted on the objective and cognitive studies on
meend. The cognitive studies include nasality, random perturbation and emotions
generated by different ragas. A notable and somewhat rare characteristic of
Hindustani music is that there is no script to follow. It is purely improvised in nature;
the performer is the composer. Thus, in the context of making a music piece, it
somewhat stands alone in the world of music and therefore calls for a rational
examination of its principle vis-à-vis others. There are chapters that cover music
information retrieval (MIR), tonic detection, raga identiﬁcation, Gharana identiﬁ-
cation and Vadi-Samvadi controversy.
It is great to see a mainstream and reputed publisher like Springer accepting the
challenge to publish this book. All the authors wholeheartedly thank the very
friendly Springer editors and other staff for helping them to select the appropriate
book series and title for the book and for helping resolving the issues coming up at
editing and production stage. In particular, we thank Ronan Nugent, Ramesh
Premnath and Chew Juan Low. All the authors thank their respective family
members, friends, colleagues and other well wishers for the moral support. We
vi
Prologue

thank the artists whose recordings have been used for research purpose throughout
the book.
It is hoped that the book will go a long way in narrowing the gap between
musicians and scientists.
It will deﬁnitely promote scientiﬁc research in Indian music, especially
Hindustani music. We are dedicating this book to both the communities—scientiﬁc
and musical—in anticipation that perhaps someday we shall see music has been
made compulsory in all educational institutes around the world with scientists and
musicians working together everywhere!
Kolkata, India
Asoke Kumar Datta
Ranchi, India
Sandeep Singh Solanki
Kolkata, India
Ranjan Sengupta
Ranchi, India
Soubhik Chakraborty
Ranchi, India
Kartik Mahto
Kolkata, India
Anirban Patranabis
October 2016
Prologue
vii

Contents
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
What Is Music? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Origin of Music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Indian Music. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3.1
Notes (Swara) in Indian Music . . . . . . . . . . . . . . . . . . .
5
1.3.2
Importance of the Tonic (Sa). . . . . . . . . . . . . . . . . . . . .
6
1.4
Basic Elements of Music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.5
Uniqueness of Indian Classical Music . . . . . . . . . . . . . . . . . . . .
8
1.6
Different Forms of Indian Classical Music . . . . . . . . . . . . . . . . .
9
1.7
Raga—The Soul of Indian Classical Music . . . . . . . . . . . . . . . .
12
1.8
Scientiﬁc Research in Indian Music . . . . . . . . . . . . . . . . . . . . . .
13
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2
Music Information Retrieval. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Feature Extraction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.1
Process of Feature Extraction . . . . . . . . . . . . . . . . . . . .
19
2.2.2
Selection of Features . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.3
Conclusions and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3
Scales and Shruti Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.1
Views on Shruti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2
Ancient Period . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.3
Modern Period . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.3.1
Divisive Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.3.2
Cyclic Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.3.3
Vedic Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
ix

3.4
Musical Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.4.1
Objective Modeling of Musical Scale . . . . . . . . . . . . . .
46
3.4.2
Relevant Psycho-Perceptual Concepts . . . . . . . . . . . . . .
47
3.4.3
Hypothesis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.4.4
Construction of Shrutis from Hypothesis. . . . . . . . . . . .
50
3.4.5
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4
Tonic Detection and Shruti Analysis from Raga
Performance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.2
Relevant Signal Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.2.1
Pitch Period Extraction from Signal . . . . . . . . . . . . . . .
61
4.2.2
Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.2.3
Steady State Detection. . . . . . . . . . . . . . . . . . . . . . . . . .
63
4.3
Determination of Tonic (Sa) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
4.3.1
Data Base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.3.2
Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.3.3
Results and Discussions. . . . . . . . . . . . . . . . . . . . . . . . .
67
4.4
Swara-Shruti Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.5
Ratio-Intervals for Steady States. . . . . . . . . . . . . . . . . . . . . . . . .
69
4.5.1
Data Base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.5.2
Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.5.3
Results and Discussions. . . . . . . . . . . . . . . . . . . . . . . . .
72
4.6
Shruti Positions in Contemporary Performances . . . . . . . . . . . . .
73
4.6.1
Clustering Methodology . . . . . . . . . . . . . . . . . . . . . . . .
74
4.6.2
Algorithm (K-Means) . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.6.3
Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.7
Approach of Heuristic Search . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.7.1
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.7.2
Results and Discussions. . . . . . . . . . . . . . . . . . . . . . . . .
78
4.8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5
Pitch Transition and Pitch Stability. . . . . . . . . . . . . . . . . . . . . . . . . .
83
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
5.2
Extraction of Meends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.3
Algorithmic Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.4
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
5.4.1
Objective Categorisation of Meends . . . . . . . . . . . . . . .
89
5.4.2
Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
5.4.3
More Details on Intonation . . . . . . . . . . . . . . . . . . . . . .
91
5.4.4
Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
5.4.5
Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
x
Contents

5.5
On Perceptibility of Transitory Movements . . . . . . . . . . . . . . . .
94
5.5.1
Experimental Procedure. . . . . . . . . . . . . . . . . . . . . . . . .
94
5.6
Results and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
5.8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
6
Raga Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
6.2
Swars or Notes (To Be Used in Ragas) . . . . . . . . . . . . . . . . . . .
101
6.2.1
Raga Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
6.2.2
Quantiﬁed Features of Raga . . . . . . . . . . . . . . . . . . . . .
102
6.3
Identiﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
6.3.1
Process of Feature Extraction and Database
Building . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
6.4
Recognition of Ragas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
6.5
Experiments and Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
6.5.1
Experimental Parameters . . . . . . . . . . . . . . . . . . . . . . . .
112
6.5.2
Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
6.5.3
Identiﬁcation Accuracy . . . . . . . . . . . . . . . . . . . . . . . . .
112
6.5.4
Identiﬁcation Versus Accuracy . . . . . . . . . . . . . . . . . . .
113
6.6
Raga Similarity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
6.6.1
Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
6.6.2
Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
6.7
Identiﬁcation of Raga . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
6.7.1
Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
6.7.2
Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . .
122
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
7
Gharana Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
7.1.1
What Is Gharana? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
7.1.2
Gharana Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . .
126
7.2
Audio Feature Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
7.2.1
Timbral Texture Features. . . . . . . . . . . . . . . . . . . . . . . .
129
7.2.2
Rhythmic Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
7.3
Projection Pursuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
7.4
Feature Database Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
7.5
Experimental Results and Discussions . . . . . . . . . . . . . . . . . . . .
136
7.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
8
Production, Perception and Cognition . . . . . . . . . . . . . . . . . . . . . . . .
143
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
8.2
Perception. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
Contents
xi

8.3
Cognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
8.3.1
Signiﬁcance of Cognition . . . . . . . . . . . . . . . . . . . . . . .
149
8.3.2
Some Experiments in Aural Cognition . . . . . . . . . . . . .
150
8.3.3
Emotion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
8.4
Making Music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
9
Automatic Musical Instrument Recognition . . . . . . . . . . . . . . . . . . .
167
9.1
Musical Instruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
9.1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
9.1.2
Indian Musical Instruments . . . . . . . . . . . . . . . . . . . . . .
168
9.1.3
Tanpura . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
9.1.4
Sarod. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
9.1.5
Flute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
9.1.6
Harmonium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
9.1.7
Tabla . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
9.2
Acoustical Analysis for the Sound of Indian Musical
Instruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
9.2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
9.2.2
Timbre Parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
9.2.3
Perceptual Features . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
9.2.4
Spectral Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
9.2.5
Wavelet Analysis (Transform) . . . . . . . . . . . . . . . . . . . .
181
9.2.6
Shimmer and Jitter . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
9.2.7
Analysis of Acoustic Characteristics of Musical
Instrument from Their Sound Signals . . . . . . . . . . . . . .
185
9.2.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
9.3
Identiﬁcation of Indian Musical Instruments . . . . . . . . . . . . . . . .
215
9.3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
9.3.2
Sound Source Recognition by Human Brain . . . . . . . . .
216
9.3.3
Constraints. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
9.3.4
Important Features for Musical Instrument
Recognition Systems . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
9.3.5
Temporal Envelope Estimation . . . . . . . . . . . . . . . . . . .
219
9.3.6
Timbre Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
9.3.7
A Practical Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
9.3.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
9.3.9
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
10
Vadi-Samvadi Controversy and Statistics . . . . . . . . . . . . . . . . . . . . .
233
10.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
10.2
Methodology. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
10.2.1
Krumhansl’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
xii
Contents

10.3
Experimental Results and Discussions . . . . . . . . . . . . . . . . . . . .
236
10.3.1
Chi-Square Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
10.3.2
Ranking of Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
242
10.4
Chapter Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
Epilogue. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
Contents
xiii

Chapter 1
Introduction
1.1
What Is Music?
Murmur of trees, singing of the birds, tinkling streams, patter of rain drops on tin
roof, strumming the strings, ringing of cow bells of returning herds under the
evening sun—all these soothe us. Add to these products of nature, the art of humans
producing songs and instrumental tunes. We all perceive these as music. Natural
sounds existed before human beings appeared on earth. Was it music then or was it
just mere sounds? Without an appreciative mind, these sounds are meaningless. So
music has meaning and music needs a mind to appreciate it. Deﬁning music is not
easy. Some say that music is organized sound (and silence). But speech, mating
calls of animals are also organized sounds. Some others say that a sequence of
sound the meaning of which the recipient experiences with his mind, feelings,
senses, will, and metabolism is music. But so is the speech or even the sounds of a
lightning or a roar of a tiger nearby.
A good deﬁnition must be simple and relatively unambiguous. Let us deﬁne
music as an acoustical emotive and co-operative communication, universal in
nature. The simultaneity of the three elements acoustics, emotion, and universality
is integral to the deﬁnition. The universality implies that it is not culture speciﬁc.
Tigre’s roar is not co-operatively emotive. Speech, mating calls are very culture
speciﬁc and therefore not music. Speech has the disadvantage in that, unless the
language is known to the receiver, everything will be Greek and Latin. On the other
hand, music is universal in nature. Music is a form of auditory communication
between the creator and the recipient. One of the differences with speech is that
speech primarily transmits message. The emotion comes out later on the semantic
interpretation of the message. In contrast, music primarily communicates emotion.
Music is also relative and subjective. What is music to one person may be noise to
another. Speech, however, has one advantage over music. When speech is trans-
lated from one language to another, it is possible to preserve the meaning. But when
music of one region is adapted and used in another, often the context and meaning
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_1
1

change (Patel 2010). It should be understood that the term “meaning” in music
refers to its emotional interpretation. From the point of view of signal processing, a
music signal differs from a speech signal in that although both are continuous
functions of time, a speech signal is a continuously varying functions of time while
a music signal remains stable for the duration of a note and then it jumps to the
(fundamental) frequency of the next note (Dorrell 2005). Thus musical notes can be
easily identiﬁed simply by looking for stability of the corresponding fundamental
frequencies (which determine the pitch). Another point of difference between the
two signals lies is rhythm which, in general, is found to be more dominant, formal
and repetitive in music than in speech. Rhythm arises when anything occurs
periodically and to detect it one needs to take a close look at the inter onset interval
(IOI) graph of the phenomenon, be it musical notes or beats in a percussion
instrument or words in a poem. We conclude this section with a quote:
Words make you think a thought.
Music makes you feel a feeling.
A song makes you feel a thought.
—EY Harburg.
1.2
Origin of Music
The word music comes from the Greek word mousikê meaning any of the arts or
sciences governed by the Muses. Of the nine Muses, daughters of the God Zeus,
only Euterpe, Terpsichore and Polyhymnia can be related to what we broadly
understand as music. In India Sangeet leads to the word ‘Samgeet’ which means
singing together. Narada the Hindu idol of music used Vina as an accompaniment.
In Vedas music began as a choral chant while older communities each had their
own music. If one looks at the full panorama of music in the world the solo
performance would constitute only a miniscule. A signiﬁcantly larger portion is
occupied by the collective performances.
Those who look at the origin of music as an evolutionary necessity suggests four
primary needs e.g. survival, territorial, sexual and motherese (mother-infant rela-
tionship). This last one may push back the origin of music to the lullaby of
Neanderthal mothers. About 300,000 years ago, Neanderthals possibly have had
proto-musical ability. Some argue that music and speech have evolved by differ-
entiation of early proto-human voice sounds “Hmmm” (Holistic, multi-modal,
manipulative and musical). The development was facilitated by vertical posture and
walking, which required sophisticated sensorimotor control, a sense of rhythm, and
possibly including the ability for dancing. Instrumental music dates back at least
around 37,000 years (which is the age of the oldest uncontested bone ﬂute found at
Geissenklösterle in Germany). It is likely though that instrumental music is much
2
1
Introduction

older, considering that instruments such as drums or rattles were made of perishable
materials and the archaeological evidences are difﬁcult.
Another way to look back in the past into the origin of music is to fall back on
the theory of intentionality of music. Some say that the ability to reﬂect about the
past and the future is a necessary ingredient of intentionality and the age when
human being manifested this ability is between 60,000 and 30,000 years ago when
they started creating art forms. If we take a cue for survival strategy from the theory
of evolution of life every culture has to adapt to the changing environment, change
brought about through interaction with other powerful cultures. These intercourses
need not be seen with suspicion. In fact, this is the only way to evolve and therefore
to survive. For evolving and survival, it is very necessary to analyse transparently
and without bias. For this modern civilization provides a plethora of knowledge
tools in disciplines from psychology to information technology. As of now these
evolutionary necessities are extinct. Now music has only one common primary
need, entertainment. This need is very culture speciﬁc. In this era of globalization,
every culture is under a considerable strain to survive. India being a country with
extensive diversity, every community has its own culture and own music. All of
them, starting from Bhadur Gaan from a Bengal village to Classical Music in India,
are ﬁghting to survive.
1.3
Indian Music
The earliest evidence of music in India can be traced back to old cites of Indus
civilisation. Earliest ﬁnds are two and three stringed instruments dated about 2000
BC from Lothal located in the Bhāl region of the modern state of Gujarātand. The
Indus Valley civilization died with the arrival of the Aryans, who descended into
India from the northwest in the ﬁrst half of the 2nd millennium BC. An important
aspect of Aryan religious life was the bard-priest who composed hymns in praise of
the gods, to be sung or chanted at sacriﬁces. This tradition was continued in the
Aryans’ new home in northern India until a sizable body of oral religious poetry
had been composed.
This body of chanted poetry grew to massive proportions, and the best of the
poems were compiled as an anthology called Rigveda. The Rigveda came into being
between 1500 BC and 500 BC. The hymns of the Rigveda, the oldest Veda, are
addressed to the elements of nature personiﬁed as deities, and are prayers for
protection from calamities and for attainment of prosperity—material as well as
spiritual.
The Yajurveda and the Samaveda were composed after the Rigveda. The
Yajurveda, with portions in prose, is a manual, describing the procedures to be
followed in the sacriﬁce. The Samaveda contains hymns to be sung by those who
did the chanting. It is the Samaveda which is speciﬁcally connected with music in
India. A fourth Veda, the Atharvaveda, replete with magical chants and incanta-
tions, was accepted as a Veda considerably later and is quite unrelated to the other
1.2
Origin of Music
3

three. The Samveda was chanted in deﬁnite musical patterns. Vedic hymns were
sung in plain melody, using only 3 notes (Sa, Re and Ni). An example of this chant
is: Ni, Sa, Re, Sa/Sa, Re, Ni, Sa/Ni, Re, Sa/Sa, Re, Ni, Sa, Re/Ni, Sa, Re, Sa, Re,
Sa/Sa, Re, Sa, Ni, Sa/Sa, Sa, Re/Re, Ni, Sa, Ni, Ni, Sa/Ni, Ni, Re, Sa, and so on …
The present system of Indian music stands on two important pillars: raga and
taal. Raga is the melodic form while taal is the rhythm underlying music. Together,
raga and taal distinguish Indian music from many other musical systems of the
world. The rhythm of music is explored through beats in time. Melody evolved as
the raga through several processes; the taal resulted from a similar evolution in
rhythm.
It took a long time for music to come to the form found in present-day India. The
most important development in music took place between the 14th and 18th cen-
turies. During this period, the music sung in the north came in contact with Persian
music and assimilated it, through the Pathans and the Mughals. It is then that two
schools of music resulted, the Hindustani (also called North Indian) and the
Carnatic (also called South Indian). Hindustani music adopted a scale of shudha
swara saptaka (octave of natural notes). During this period, different styles of
classical compositions such as dhrupad, dhamar, khayal, and others were con-
tributed to Hindustani music, along with many exquisite hymns, bhajans, kirtans.
Bharat’s Natyashastra, is the ﬁrst treatise known for laying down the fundamental
principles of drama, dance and music. Pandit Vishnu Narayan Bhatkhande (1887)
consolidated the musical structures of Hindustani classical music into 10 thaats
(classes based on allowable sequence of notes). These ten thaats are named as
Bilawal, Kalyan, Khamaj, Bhairav, Poorvi, Marwa, Kaﬁ, Asavari, Bhairavi and
Todi; if one were to pick a raga at random, it should be possible to ﬁnd that it is
based on one or the other of these thaats. A raga more precisely is a melodic
structure with ﬁxed notes and a set of rules which characterize a particular mood
conveyed by performance (Chakraborty et al. 2009). Commonality and diversity are
both eminent here, e.g., the ragas Shivranjani and Bageshree both belonging to the
Kaﬁthaat evoke sadness or karun rasa (commonality) and yet the individual
emotional characteristics of Bageshree which we may call “Bageshree rasa”
(Chakraborty et al. 2014) are quite different from those of Shivranjani (which we
may call “Shivranjani rasa”). The “Bageshree rasa” of Bageshree which makes it
different from the “Shivranjani rasa” of Shivranjani depict the diversity in the two
ragas concerned. This is analogous to saying that a mango tastes differently from a
guava (diversity) though both the fruits may be sweet (commonality) when ripe. It
should be remembered that music being a work of art, the special effects outweigh
the common effects or, in other words, diversity is more important than com-
monality. Two ragas using even the same notes can evoke not only different
emotions but sometimes even opposite emotions like sad and happy (e.g. raga Kaﬁ
evokes joy in a romantic sense and Bageshree evokes sadness; the raga Kaﬁshould
not be confused with the thaat Kaﬁ).
The Indian system of music is an individualistic, subjective, and spiritual art,
aiming not at symphonic elaborations but at personal harmony with one’s own
being. Indian music reﬂects the delightful blend of the knowledge of the Vedic
4
1
Introduction

Aryans, with the emotions, prevailing in the sub-continent’s music for millennia
before their arrival. Spontaneous and intuitive improvisations in melody as well as
rhythm constitute the sublime underpinning of the
Indianness characteristic in Indian music.
While the western classical music system insists on machine-like precision in
determining fundamental frequencies for its notes, its Indian counterpart, shows a
liberal and human approach while dealing with the rendering of swaras in raga
elaboration. According to a deﬁnition of raga by Matanga (the seventh century
author whose work Brihaddesi described raga in its technical sense for the ﬁrst
time), what is pleasant alone can be a raga. Raga is built by a choice of a minimum
of ﬁve and maximum of seven swaras in an octave. Sa being the tonic is always
present and between Ma and Pa at least one must be present in a raga.
Melody and rhythm are the common grounds for music, be it Western or Indian.
Indian music is essentially monophonic (single melody format) while Western
music can be polyphonic (multiple notes played or sung in harmonised unison),
monophonic or a combination of both. Western classical music is based upon the
equal tempered scale, and rests upon melody, rhythm, harmony and counterpart
while swara and taal are the two basic components of Indian classical music.
Swaras are the twelve notes, while a taal is a cycle of beats, starting with a stress
point called the sam and with a release point called the khali. It is this (the sam &
khali) that brings life to a taal. Chakraborty et al. (2014) have provided a com-
parative study between Indian and Western music in chapter 1 of their book.
1.3.1
Notes (Swara) in Indian Music
The seven notes of the scale (swaras) in Indian music are named shadja, rishabh,
gandhar, madhyam, pancham, dhaivat and nishad, usually shortened to Sa, Re, Ga,
Ma, Pa, Dha, and Ni (the western counterpart being Do, Re, Mi, Fa, Sol, La, and
Si) and written S, R, G, M, P, D, N. Collectively these notes are known as the
sargam (the word is an acronym of the consonants of the ﬁrst four swaras). ‘Riyaz
Sargam’ is a technique in Indian music which provides a good exercise for music
learners by singing different combinations of notes, popularly known as ‘paltas’.
Sargam is practised against a drone produced by a stringed instrument called the
tanpura.
Sa in Indian music is not associated with any particular pitch. In the Western
system, the tonic (ﬁrst note of a musical scale) of a piece has a particular pitch. The
Indian musical scale is said to have evolved from 3 notes to a scale of 7 primary
notes subdivided into 22 intervals. In Indian music the 7 notes of the scale do not
have equal intervals between them. The ﬁrst and ﬁfth notes (Sa and Pa) do not alter
their positions on this scale. The other 5 notes can change their positions in the
scale, leading to different ragas. Each shuddha swara or natural note (i.e., Sa, Re,
Ga, Ma, Pa, Dha and Ni) is traditionally believed to have originated in the sound of
1.3
Indian Music
5

a different animal, and some have additional meanings of their own. Also, each
swara is associated with one of the seven chakras of the body.
It appears that the ancient system conceived swara not merely as a sound of
ﬁxed pitch position, but as the entire tonal range between itself and its previous
swara. Though this interval can be theoretically divided into inﬁnitesimal parts,
they believed that only a limited number not exceeding four sounds could be
distinctly cognized by the ear in a swara-interval. These cognizable sounds are
known as shrutis (the 22 micro-tonal intervals placed in one octave-space) and the
interval, which separated one swara from the next, was measured in terms of
shrutis. Shruti was thought of both as the least audible interval between two sounds,
as well as the sounds themselves, which were separated, by such an interval. The
total number of shrutis was ﬁxed unambiguously at 22 in ancient treatises. However
the two different distributions of shruti for swara intervals for different gramas
(Ancient music scales—Shadaja, Madhyama, and Gandhar Gramas) were reported
in the ancient treatises. A measure of a shruti, considering it as an interval, is also
prescribed in ancient treatises. There is no indication that the ancient authors
considered shruti to be a signiﬁcant concept in performance. At least one author
speciﬁcally states that it is needed for the purpose of classiﬁcation. Though the
ancient treatises do not reveal anything about the objective measure of swaras,
some interval ratios can be built up using the hints in Nātyashāstra (Bharata 1929).
It may be noted here that in the Indian system the octave is known as Saptak as it
contains only seven major expressive intervals, called Swaras. This is so because
the Indian system implicitly relies upon the intervals not on the boundaries (the
pitch representing the Musical Notes) of the major intervals. Also since music is a
mode of communication it must have a linguistic structure. A linguistic structure
needs alphabets, which in the case of music are categories in the pitch scale. Again
the need of expression as well as appreciation of Indian Music requires smaller
standard intervals. A very long period of development of Indian music (over 4
millennia) gave rise to a unique scale based on a large number of basic microtonal
intervals called shrutis. Thus a musician can choose any of the shrutis, which
supports the interval when he is required to use a swara.
1.3.2
Importance of the Tonic (Sa)
The Indian musical system is modal. In a modal system, relations between suc-
cessive notes as well as those between any note and the tonic (ﬁrst note of a musical
scale—Sa) are of paramount importance. Here each note leaves an impression in the
mind and the idea is revealed through the cumulative effect of successive notes.
Furthermore, the tonic is not ﬁxed in terms of a set of speciﬁed frequencies but can
be deﬁned by the performer anywhere according to his/her own choice. The basis of
scale in the Indian musical system, therefore, has to be considered in terms of the
relation between successive notes. Sensation of pitch, which corresponds primarily
to that of fundamental frequency of vibration, is the most important property of
6
1
Introduction

musical sounds. The whole audible range of pitch is available for musical
expression and it is therefore necessary to have a scale with a tonic suited to a
particular musical system for interpreting, analyzing as well as expressing musical
ideas.
1.4
Basic Elements of Music
The material essence of music lies in its melody, harmony, and rhythm. Melody
gives music the soul, while rhythm blends the expression of harmony and dynamics
with the tempo of the passage. Melody, a linear sequence of musical tones is
perceived as a single entity. On the other hand harmony uses a vertical, i.e.
simultaneous use of tones of different pitches. Rhythm is the temporal pattern
successive melodic or harmonic patterns or weak/strong elements to create an
expression. The origin of the word is Greek, meaning “ﬂow.” Rhythm is indeed the
embodiment of timely ﬂow. Rhythm organizes music in much the same way as
meter regulates and pulsates a poem.
It is interesting to note that the generally accepted three basic classes (a) melodic,
(b) chordal, and (c) rhythmic music can be correlated to these basic features. In
melodic music the aesthetics is built upon the succession of different consonant
pitch values. Jazz, Rock, Western folk, Indian music, etc. belongs to this class. The
quality of sound or timbre plays the most important role as the source of aesthetic
pleasure in chordal music like modern western, West African and Oceanian.
Rhythmic music uses patterns of regulated succession of strong and weak elements,
or of opposite or different conditions to create expression. Most of the ethnic
African Music belongs to this class. Though this grouping uses the different pri-
mary acoustical parameters all music uses all these elements for full aesthetic
development, only the importance varies. One may note an interesting correlation
between this classiﬁcation and the evolutionary forces mentioned earlier. Melodic
class may be related to motherese; Chordal class may be related to territorial and
rhythmic to sexual.
The physical reality of music, the acoustic phenomena, is interpreted by the
mind as music through a transformation into ideational entities by the perceptual
and cognitive mechanism. Like any other language of communication, music also
has its own basic units. The foundation of these units lies on two primary sources of
knowledge, the acoustic and the semiotic. Acoustic based units are notes, scales,
timbre, laya, tala etc. The semiotic units are phrases, vadi-samvadi relations (vadi
refers to the most important note in a raga; samvadi refers to the second most
important one), syntax, grammar, pragmatics, semantics etc. These units, though
ideational are not completely personal. They have objectivity in their apparent
subjectivity. One interesting branch of scientiﬁc study is to ﬁnd the objective
correlates of these musical units. These will be discussed later in different chapters.
In addition to the many elements mentioned earlier the role of lyrics, which
contains also some semantic pointers to feelings, need not be underestimated. In
1.3
Indian Music
7

Indian genre even instrumental music is said to be primarily based on some lyrical
composition. Thus lyrics, the choice of which is purely subjective, plays an
important role in the making of music. Broadly these elements may be divided into
two groups, acoustics and semiotics. Musical texture is the overall sound of a
passage or complete composition. This may be described according to the number
of and relationship between parts of music: monophony, heterophony, polyphony,
homophony, or monody. Texture is the way the melodic, rhythmic, and harmonic
elements are combined in a composition, thus determining the perceptual overall
quality of the sound. Musical expression is created by nuances of any phenomena
of sound including timbre, variation of pitch, tempo, volume, etc. It is of particular
interest in the making of music. Expression can be closely related to breath, and the
voice’s natural ability to express feelings, sentiment, deep emotions.
1.5
Uniqueness of Indian Classical Music
What is unique about India’s classical music? Compared to other forms of art, all
music is universal in nature, as it uses the human voice or sound which is not
speciﬁc to territories and cultures. Poetry, for example, is closely cultural since it
uses language which is nothing but a repository of such thoughts and experiences as
have been shared in a particular culture over the ages. The fact remains, however,
that music too, somewhat indirectly, is an expression of a given cultural tradition.
The subtle relationship of music and culture is the ﬁeld of the social anthropologist.
The musician would be able to indicate certain features of Indian music which are
unique to it. For example, India’s music works on the principle of melody, rather
than harmony as in Western music. Indian music has developed in the direction of
cosmic one-ness of creation. Convergence, rather than duality of opposites, has
been unique to the Indian mind—hence melody. Sequences are of importance, not
chords. The concept of Taal is an exercise in dynamics of time sequence. The
concept of Time is the framework of Indian classical music. Time is broken down
into minutest units making up a discipline within which the performer must stay.
Different dimensions of the music are expressed in numbers. There are 7 swars; 22
imperceptible sounds, shruti in each octave; 3 saptaks (mandra, madhyam, and
taar); 4 stages in dhrupad singing, sthayi (stable), antaraa (rising) sanchari (free
moving section) and abhog (similar to the antaraa); 2 stages in all Khayal singing,
sthayi (stable) and antaraa (rising); 3 motions, laya (vilambit laya, madhya laya, and
drut laya); and so on. Finally, the hallmark of Indian music is the limitless ﬁeld it
offers for improvisation and virtuosity. It is not orchestrated, and is not written
down, unlike Western music. While the framework imposes great discipline, within
the area the performer can bring his or her genius into full play. Here the performer
endeavors to gain a spiritual experience, a tryst with God. Indeed, sometimes the
improvisations presented by great masters are so inspirational that they themselves
would ﬁnd it hard to repeat them!
8
1
Introduction

The use of music brings out the importance given to it in human life. Twenty
centuries ago, the essential role of Indian music was predominant in festivals and
other social occasions. Music as entertainment is supposed to have evolved much
later. Another part of Indian music is folk music. Indian classical music is said to
have evolved out of the mixture of these. It is presumed that folk music existed long
before the Aryans came to India; the Dravidians had their own as well as the
multitudes of tribes abounding all over India. The art of music practised in India has
a special signiﬁcance, as it has developed from ritualistic music in association with
folk music and other musical expressions of neighbouring nations, developing into
its own characteristic art. Matured through “thought, experience and expression”,
Indian classical music has become unique in the world.
Rabindranath Tagore who was acquainted with both the Indian and Western
systems put it thus: “The world by day is like European music—a ﬂowing concourse
of vast harmony, composed of concord and discord and many disconnected frag-
ments. And the night world is our Indian music: purely deep and tender raga. They
both stir us, yet the two are contradictory in spirit. But that cannot be helped. At the
very root, nature is divided into two, day and night, unity and variety, ﬁnite and
inﬁnite. We, the people of India, live in the realm of night; we are overpowered by
the sense of the One and Inﬁnite. Our music draws the listener away beyond the
limits of every day human joys and sorrows and takes us to the lonely region of
renunciation which lies at the root of the universe, while European music leads us to
a variegated dance through the endless rise and fall of human grief and joy.” (Gobind
Singh Mansukhani, Indian Classical Music And Sikh Kirtan, http://fateh.sikhnet.
com/sikhnet/gurbani.nsf/d9c75ce4db27be328725639a0063aecc/085885984cfaafcb
872565bc004de79f!OpenDocument).
Basically Indian music evokes a spiritual sentiment and discipline—a longing
for realisation of the self—salvation.
By this process individual consciousness can be elevated to a realm of awareness
where the revelation of the true meaning of the universe—its eternal and
unchanging essence—can be joyfully experienced. Our ragas are the vehicles by
which this essence can be perceived.
The music of India is a pervasive inﬂuence in Indian life. It pervades the big and
small events of Indian life, from child birth to death, religious rites and seasonal
festivals. Originally, not all developments of music were reduced to writing. To
keep their traditional integrity, they were imparted orally from teacher to pupil—the
Guru-Shishya tradition. In the past, there used to be a system of Gurukul Ashram
where teachers imparted knowledge to deserving students.
1.6
Different Forms of Indian Classical Music
The Hindustani music system uses different musical forms like the Dhrupad,
Khayal, Thumri, Dadra, Tarana and so on. Of these, the important ones are
Dhrupad, Khayal and Thumri. While the Dhrupad is the strictest form in terms of
1.5
Uniqueness of Indian Classical Music
9

grammar and presentation format, the Khayal permits more liberty. The Thumri is
the most ﬂexible compared to the other two. But what separates one form from the
other? How can one identify and differentiate the forms? Let’s examine this.
Dhrupad: The Dhrupad is considered one of the oldest forms of Hindustani
vocal music. The performance of Dhrupad consists of two parts, namely, the
exposition section or Alap and the ﬁxed composition or Bandish. There are four
styles of performing Dhrupad. They are Gaurahari (Gobarahara), Nauhari, Dagari
and Khandari. Of these, only the Dagari style seems to exist today. Dhrupads have
their basis in raga form and are set to Taal. They exist both as vocal and instru-
mental forms. Performances on the Bin (Vina) are now rare. The Dhrupad usually
covers themes such as religion, philosophy, devotion and praise of deities, patrons,
and celebration of seasons. The distinctive feature of Dhrupad is the strongly
systematized arrangement of its musical parts. This emphasis extends to raga
development and adherence to the grammar of the raga and meticulous exploration
of rhythmic possibilities, which makes this style serious and sombre with an appeal
that is restricted to a relatively smaller audience. Sometimes before improvisation
the words are engaged in a kind of rhythmic play known as ‘bol-banth’, in order to
bring out the rhythm. The Alap or exposition broadly follows the three stages of
Vilambit, Madhya and Drut (slow, medium and fast tempo respectively). The lyric
(bandish) generally consists of four segments, Sthayi, Antara, Sanchari and Abhog.
Dhrupad is always accompanied by a Pakhawaj, a certain percussion instrument.
Dhamar: This is generally coupled with the Dhrupad, in terms of the style of
singing and raga development. However, the lyrics are of greater erotic content and
the style allows greater freedom in presentation. For the dhrupad exponent it is the
equivalent of the thumri of the exponent of khayal. The form is almost invariably
sung to the Dhamar Taal, a cycle of 14 beats. Dhamar songs often pertain to Holi
(the festival of colours) and the antics of Lord Krishna, although lyrics with other
subjects are also presented.
Khayal: This word comes from the Persian word meaning idea, thought, con-
ception or imagination. This form is more free and ﬂowery when compared to the
sombre Dhrupad. Khayal covers diverse topics such as divine love, separation of
lovers, seasons, praise of kings, patrons and the pranks of Lord Krishna. The style
and presentation of singing a Khayal greatly differs from that of Dhrupad. In fact,
the subtle nuances and embellishments are also different. The use of Taan (the
permutations and combinations of note-sequences) in Khayal gayaki is one of the
major features that distinguish it from Dhrupad. In addition, there is a greater use of
ornamentation in the Khayal form. The lyric (bandish) of the Khayal only has two
sections, Sthayi and Anthara.
The performance of Khayal allows a great deal of freedom in elaboration, the
style depending largely on the gayakee or stylistic pattern adopted by the performer.
The composition may be presented at different tempo, namely vilambit laya,
madhya laya and drut laya, often determined by the nature of the composition, that
is, whether it is a bada khayal (sung in vilambit laya) or a chhota khayal (sung in
madhya laya or drut laya). Master performers have been known to experiment with
10
1
Introduction

compositions to enhance their aesthetic appeal. However, generally a bada khayal is
generally followed by a chhota khayal.
Tarana: This is a vocal composition that is usually sung in a fast tempo using
syllables such as na, ta, re, da, ni, odani, tanom, yalali, yalalam, etc. Sometimes,
Pakhawaj bols or Sargams are also used. The difference between the Drut Khayal
and Tarana lies in the text. In the Khayal, the fast type is usually a meaningful poem
while in a Tarana, the emphasis is on producing rhythmic patterns. The Tarana is set
to a raga and Taal. Tarana singing requires specialization and skill in rhythmic
manipulation.
Tappa: This musical composition is characterized by its very quick phrases and
short Taans without any elaboration. These songs are usually composed in Punjabi
and were supposed to have been sung by camel drivers in the North-West, before
they were adapted into classical music. The songs have an extremely short text and
do not consist of sections. They, however, use the Taals employed for Khayals.
These are composed only in a few ragas like Kaﬁ, Peelu, Jhinjhoti, Gara, Barva,
Manjh Khamaj and so on. The predominant rasa or emotion of these songs are
romantic-erotic. This is a semi-classical form, which can be classiﬁed with Thumri.
Tappa is a form of Indian semi-classical vocal music whose specialty is its
rolling pace based on fast, subtle, knotty construction. It originated from the folk
songs of the camel riders of Punjab and was developed as a form of classical music
by Mian Ghulam Nabi Shori or Shori Mian, a court singer for Asaf-Ud-Dowlah, the
Nawab of Awadh. “Nidhubabur Tappa”, or tappas sung by Nidhu Babu were very
popular in eighteenth and nineteenth century Bengal. Among the living performers
of this style are Laxmanrao Pandit, Shamma Khurana, Manvalkar, Girija Devi,
Ishwarchandra Karkare, and Jayant Khot. Tappa is catchy to the ear, due to its
unusual aspect of bounce and re-bounce of musical notes. Tappa, understood to
have been the staple diction of the erstwhile camel drivers, has since come to a
ripened age, by being nurtured in the hands of some of the legendary masters in this
genre. The word tappa stands for jumping, bouncing and skipping, implying the
extraordinary rule of unremitting attempts made by a singer on the musical notes,
not stopping or taking a pause for once. This outstanding formation is unique to
tappa only, absent in the other Hindustani classical forms.
http://www.jagranjosh.com/general-knowledge/types-of-music-compositions-
tappa-1344414719-1.
Thumri: These are considered as light classical as their melodies do not strictly
adhere to the raga structure, providing greater freedom of improvisation. This
musical form exists in both its vocal and instrumental forms, providing a light and
enjoyable fare, and is frequently employed for concluding music concerts. The texts
of the Thumris are composed mainly in Braj Bhasha (an old Indian language) and
the themes are predominantly romantic, particularly focussing on the separation of
lovers and the pranks of Lord Krishna as a playful lover. Thumris are usually
composed only in certain ragas which belong to a group of ragas that are closely
associated with folk music and also employs Taals that are similar to those of folk
rhythms. The structure of Thumri consists of a Sthayi and Antara. In this case also,
1.6
Different Forms of Indian Classical Music
11

the Mukhda, i.e., the portion of the ﬁrst line concluding on the Sam, is the most
important part of the composition.
Dadra: This is another light-classical form closely related to Thumri. Dadras are
generally sung in Dadra Taal or Kaharva Taal. Dadra pieces have a faster tempo
compared to Thumri with greater emphasis on rhythm. These are light fast songs in
Urdu or Braj Basha. The text in Dadra deals with more mundane topics. Dadra texts
appear to be derived from folk sources.
1.7
Raga—The Soul of Indian Classical Music
The combination of several notes woven into a composition in a way which is
pleasing to the ear, is called a raga. More elaborately, as stated earlier, a raga is a
melodic structure with ﬁxed notes and a set of rules which characterizes a particular
mood conveyed by performance (Chakraborty et al. 2009). Each raga creates an
atmosphere, which is associated with feelings and sentiments. Any stray combi-
nation of notes cannot be called a raga. The raga is the nucleus of Indian classical
music. A raga is based on the principle of a combination of notes selected out of all
notes in an octave. A performer with sufﬁcient training and knowledge alone can
create the desired emotions, through the combination of shrutis and notes. Though
Indian music is modal in character, ragas should not be mistaken as modes that one
hears in the music of the Middle and Far Eastern countries, nor be understood to be
a scale, melody per se, a composition, or a key. A raga is a scientiﬁc, subtle and
aesthetic melodic form with its own peculiar ascending and descending movement
consisting of either a full seven note octave, or a series of six or ﬁve notes (or a
combination of any of these) in a rising or falling structure called the Arohana and
Avarohana. It is the subtle difference in the order of notes, an omission of a
dissonant note, an emphasis on a particular note, the slide from one note to another,
and the use of microtones together with other subtleties that demarcate one raga
from the other. There is a saying in Sanskrit—“Ranjayathi iti Ragah”—, which
means, “that which colours the mind is a raga.” For a raga to truly colour the mind
of the listener, its effect must be created not only through the notes and the
embellishments, but also by the presentation of the speciﬁc emotion or mood
characteristic of each raga. Thus through rich melodies in our music, every human
emotion, every subtle feeling in man and nature can be musically expressed and
experienced. The performance is set to a melodic pattern called a raga (also spelled
as raga) characterized in part by speciﬁc ascent (Arohana) and descent (Avarohana)
sequences, which may not be identical. Other characteristics include King (Vadi)
and Queen (Samavadi) notes and characteristic phrases (Pakad). In addition each
raga has its natural register (Ambit) and glissando (Meend) rules, as well as features
speciﬁc to different styles and compositions within the raga structure. Performances
are usually marked by considerable improvisation within these norms. The three
primary pitch ranges (saptak) of Indian classical music are Mandra, Madhya and
Tara. A typical rendition of a Hindustani raga involves two stages:
12
1
Introduction

• Alap: a rhythmically free improvisation on the rules for the raga in order to give
life to the raga and shape out its characteristics. The alap is followed by the jod
and jhala in instrumental music, while khayal singers sometimes introduce the
raga with an aochar, which is a brief introduction.
• Bandish or Gat: a ﬁxed, melodic composition set in a speciﬁc raga, performed
with rhythmic accompaniment by a tabla or pakhavaj. There are different ways
of systematizing the parts of a composition. For example:
Sthaayi: The initial body phrase or line of a ﬁxed, melodic composition.
Antara: The second body phrase or line of a ﬁxed, melodic composition.
Sanchaari: The third body phrase or line of a ﬁxed, melodic composition, seen
more typically in Dhrupad Bandishes.
Aabhog: The fourth and concluding body phrase or line of a ﬁxed, melodic
composition, seen more typically in Dhrupad Bandishes.
1.8
Scientiﬁc Research in Indian Music
Scientiﬁc research in music has been viewed from the view point of hard sciences.
This branch of science is primarily of western origin. We may trace its origin to the
famous deﬁnition of reality by Democritus of Adbera. He said that reality is atom
and void and thence the strict material objectivity of the modern science. The part of
the natural world which represented the conscious wilful activity of the life forms
and was subjective was thrown out of the window for good.
Scientiﬁc research on music has been initiated in ancient times. In Greece,
Pythagoras, in India, Bharat had speculated on the rational and scientiﬁc basis of
music to elucidate its fundamental structures. Music was ﬁrst given numbers (the
simple ratios of octave, perfect ﬁfth and perfect fourth) by Archimedes. The works
of eminent scientists like Pythagoras, Helmholtz in the past and Raman, Kar fol-
lowed by Rossing and Sundberg later on, threw a ﬂood of light on many scientiﬁc
aspects of music.
Raman’s Work on Musical Instruments
In India, Sir C.V. Raman, working at Kolkata did some pioneer research on
Indian musical instruments. Raman worked on the acoustics of musical instruments
from 1909 to 1935 and regularly published his research work on musical instru-
ments in reputed journals like Nature (London), journal of Dept. of science,
University of Calcutta, Philosophical magazine, Indian association for cultivation of
science etc. and also in various proceedings of national and international repute. He
worked out the theory of transverse vibration of bowed strings, on the basis of
superposition velocities. He was also the ﬁrst to investigate the harmonic nature of
the sound of the Indian drums such as the tabla and the mridangam. He had some
pioneer work on violin family and ektara. “On the wolf note of the violin and
cello”, “The kinematics of bowed string”, “The musical instruments and their
1.7
Raga—The Soul of Indian Classical Music
13

tones”, “Musical drums with harmonic overtones” etc. are some examples of his
published work. Raman was fascinated by waves and sounds and always carried in
his mind the memory of reading Helmholtz’s book on ‘The Sensations of Tone’ in
his school days. His work on musical instruments is the biggest motivation of
research in the area. For further information, see Raman (1909, 1910, 1921, 1922
and 1935) and Raman and Kumar (1920).
After Sir C.V. Raman, research work on musical instruments was carried for-
ward by S. Kumar, K.C. Kar, B.S. Ramakrishna (see Ramakrishna 1957) and B.M.
Banerji (see Banerjee and Nag 1991) during mid and late of nineteenth century.
After that there was a big void in the research of Indian musical instrument sound.
The early scientiﬁc research on music remained primarily involved in physics of
sound and vibration. Some of the examples are the mathematical analysis of string,
wind and percussion instruments. The cognitive and experiential aspects of music
were not taken much note of. The emergence of the subject Artiﬁcial Intelligence
appears to be co-incidental with the taking up of cognitive aspects of music into the
ambit of modern scientiﬁc research. Thus we had two disjoint realities, objective
and subjective. The causal relationship between these two realities gave birth to the
cognitive science. Yet the two remain disjoint. Of late some AI students began to
toy with a new idea to pass seamlessly from one reality to the other. Consider the
statement ‘the sun rises in the east’. There is no ‘pointer reading’. This is a sub-
jective reality common to many men. Such commonness gives rise to the concept of
public subjectivity. This ‘public subjectivity’ bridges the two sets, subjective and
objective. It might transform some subjective realities into what we call objective
reality. We believe, most of the so-called objective realities, we know of, belong to
this class, unless they are axiomatic.
Thus the scientiﬁc research in music may be broadly classiﬁed into
A. Strictly objective: Mechanics, Acoustics
Mechanics (Production of sounds from instruments including vocal organs)
Acoustics (Physical measures like, fundamental frequency, amplitude, com-
plexity., etc.)
B. Subjective: Cognition
Aesthetics, emotion in music, their measures, universality vs speciﬁcity,
physiological and psychological manifestations
C. Publicly Subjective: AI approach to musical elements
Understanding of physical elements along with their time series, different
cognitive processes in music, musicological units and their structures.
In the aforesaid context, the chapters of this book reﬂect the contributions by two
different teams, one at Kolkata and the other at Ranchi. The Kolkata team, although
now associated with Sir C.V. Raman Centre of Physics and Music at Jadavpur
university, actually conducted these scientiﬁc experiments on music at the ITC
Sangeet Research Academy (ITC SRA), Kolkata during the last three decades. This
team comprises of experts in artiﬁcial intelligence, signal processing and physics. In
contrast, the relatively younger team at Birla Institute of Technology, Mesra,
14
1
Introduction

Ranchi comprises of experts in signal processing and statistics. The Ranchi team
also has a medical unit at Rajendra Institute of Medical Sciences (RIMS) where a
team of doctors and another statistician are working on music—medicine. The ﬁrst
book on computational musicology in Hindustani music (Chakraborty et al. 2014)
is credited to the Ranchi team which has also contributed signiﬁcantly in the area of
music—medicine (Singh et al. 2013, 2016).
It is hoped that the combined effort of the two teams will deﬁnitely provide every
scientiﬁc researcher in music, especially Indian music, sufﬁcient food for thought.
Additional bibliography
Further literature in Hindustani music can be found in Jairazbhoy (1995),
Sarangadeva, (1972), Levy (1982), Kolinski (1961), Ranade (1997), Wade (1983,
1984) and Gautam (1980). See also Datta et al. (2006), Fox-Strageways (1965),
Daniloue (1979), Deva (1974, 1980, 1981), Sairam (2006), Pragananada (1963),
Sarmadee (2003), Mushalgaonkar (1995), Singh (1994) and Chatterjee (1996).
A useful reference book is by Kauffman (1968). However, for a comprehensive
treatise on Indian music, we strongly recommend the reader is to The Oxford
Encyclopedia of the music of India by Mahabharati (2011) in three volumes.
References
Banerjee BM, Nag D (1991) The acoustical character of sounds from Indian twin drums. Acustica
75:206–208
Bharata (1929) Natyashastra”. Vidya Vilas Palace, Banaras
Bhatkhande Bishnunarayan B (1887) Hindushani Sangeet Paddhati, vol 1–7. In: Roy D,
Chattopadhyaya AK (eds) Deepayan, Kolkata
Chakraborty S, Ranganayakulu R, Chauhan S, Solanki SS, Mahto K (2009) A statistical analysis
of Raga Ahir Bhairav. J. Music Meaning 8, sec. 4, Winter 2009. http://www.musicandmeaning.
net/issues/showArticle.php?artID=8.4. Accessed 16 Sept 2016
Chakraborty S, Mazzola G, Tewari S, Patra M (2014) Computational musicology in Hindustani
music. Springer
Chatterjee Chhaya (1996) Sastriya Sangita and music culture of Bengal through the ages, vol 1.
Sharada Publihing House, Delhi
Daniloue A (1979) Introduction to the study of musical scales. Oriential Books Reprint
Corporation, New Delhi
Datta AK, Sengupta R, Dey N, Nag D (2006) Experimental analysis of Shrutis from performances
in Hindustani music. Scientiﬁc Research Department, ITC Sangeet Research Academy,
Kolkata
Deva BC (1974) Music and science. J Indian Musicol Soc 5(1):19–23
Deva BC (1980) Indian music. Indian Council for Cultural Relations, New Delhi
Deva BC (1981) The music of India: a scientiﬁc study. Munshiram Manoharlal Publishers Pvt.
Ltd., New Delhi
Dorrell P (2005) What is music? Solving a scientiﬁc Mystery, © 2005. Philip Dorrell. http://
whatismusic.info/
Fox-Strangeways AH (1965) “The Music of Hindustan”, 1914, Rpt. Clarendon Press, Oxford
Gautam MR (1980) The musical heritage of India. Abhinav Publications, New Delhi
Jairazbhoy NA (1995) The rags of North Indian music: their structure and evolution. Popular
Prakashan, Bombay
1.8
Scientiﬁc Research in Indian Music
15

Kauffman W (1968) The ragas of North India. Indiana University Press
Kolinski M (1961) The origin of Indian twenty-two tone system. In: Kolinski M (eds) Studies in
ethnomusicology, vol 1. Folkways Records and Service Corporation Inc., NewYork
Levy M (1982) Intonation in North Indian music: a select comparison of theories with
contemporary practice. Biblia Impex, New Delhi
Mahabharati S (2011) The Oxford encyclopedia of the music of India, vol 1–3. Oxford University
Press, Oxford
Mushalgaonkar Vimla (1995) Bhartiya Sangeet-Shastra ka Darshanparak Anushilan. ITC Sangeet
Research Academy, Kolkata (in Hindi)
Nur-Ratnakar by Shahab Sarmadee, Ed. Premlata Sharma and Francoise ‘Nalini’ Delvoye, ITC
Sangeet Research Academy, Kolkata, 2003
Patel AD (2010) Music, language and the brain. Oxford University Press, USA
Praganananda Swami (1963) A history of Indian music, vol 1. Ramkrishna Vedanta Math,
Calcutta
Ramakrishna BS (1957) Modes of vibration of the Indian Drum Dugga or left hand Thabala.
J Acoust Soc Am 29:234–238
Raman CV (1909) The Ectara. J. Indian Math Club 170
Raman CV (1910) Escalations of the stretched strings. J Indian Math Club U
Raman CV (1921) On some Indian stringed instruments. Proc Indian Assoc Cultiv Sci 7:29
Raman CV (1922) The acoustical knowledge of the ancient Hindus. Asutosh Mookerjee, Silver
Jubilee Volume 2:179
Raman CV (1935) The Indian musical drums. Proc Indian Acad Sci A 1:179–188
Raman CV, Kumar S (1920) Musical drums with harmonic overtones. Nature 104:500
Ranade AD (1997) “Hindustani music”, 1st edn. National Book Trust, New Delhi
Sairam TV (2006) Melody and rhythm—‘Indianness’ in Indian music and music. Music Ther
Today VII(4):876–891
Sarangadeva (1972) “Sangitratnakar” (trans: Bengali by Bandopadhyay SC). Rabindra Bharati
University
Singh TJ (1994) Bhatiya Sangeet ka Itihaas. ITC Sangeet Research Academy, Kolkata (in Hindi)
Singh SB, Chakraborty S, Jha KM, Chandra S, Prakash S (2013) Impact of Hindustani ragas on
visual acuity, spatial orientation and cognitive functions in patients with cerebrovascular
accident and diffuse head injury. Music Med 5(2):67–75
Singh SB, Chakraborty S, Jha KM, Chandra S, Prakash S, Tewari S (2016) Healing brain injury
through ragas. CBH Publications
Wade BC (ed) (1983) Performing arts in India: essays on music, dance, and drama, no. 21. Center
for south and Southeast Asia studies. University of California, Berkeley
Wade BC (1984) Khyal: creativity within North India’s classical music tradition., Cambridge
studies in ethnomusicologyCambridge University Press, Cambridge
16
1
Introduction

Chapter 2
Music Information Retrieval
2.1
Introduction
Music is a very interesting topic in our society as almost everyone enjoys listening
to it and many wants to create. Broadly speaking, the research in Music Information
Retrieval (MIR) is one of the upcoming research interest with the extraction and
inference of meaningful features from music (from the audio signal), indexing of
music using these features, and the development of deferent search and retrieval
schemes (for instance, content-based search, music recommendation systems, or
user interfaces for browsing large music collections), as deﬁned by Downie (2003).
As a consequence, MIR aims at making the world’s vast store of music available
to individuals (Downie 2003). To this end, deferent representations of music-related
subjects (e.g., songwriters, composers, performers, consumer) and items (music
pieces, albums, video clips, etc.) are considered. A key problem in MIR is classi-
ﬁcation, which assigns labels to each song based on genre, mood, artists, etc. Music
classiﬁcation is an interesting topic with many potential applications. It provides
important functionalities for music retrieval. This is because most end users may
only be interested in certain types of music. Thus, a classiﬁcation system would
enable them to search for the music they are interested in. On the other hand,
different music types have different properties. We can manage them more effec-
tively and efﬁciently once they are categorized into different groups.
The following tasks are directly related to music classiﬁcation:
• Genre Classiﬁcation (Tzanetakis and Cook 2002; Li 2003; Lidy and Rauber
2005; Meng and Shawe-Taylor 2005; Mierswa and Morik 2005; Scaringella and
Zoia 2005; Turnbull and Elkan 2005; West and Cox 2005; Bergstra et al. 2006;
Morchen et al. 2006; Mochen et al. 2006; Liand and Ogihara 2006; Shen et al.
2006; Lidy et al. 2007; Meng et al. 2007; Panagakis et al. 2008, 2009; Song and
Zhang 2008; Lin et al. 2009)
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_2
17

• Mood Classiﬁcation (Liand and Ogihara 2003, 2006; Feng et al. 2003; Yang and
Lee 2003; Korhonen and Clausi 2006; Yang et al. 2006, 2008; Lu et al. 2006;
Cheung and Lu 2008; Mion and Poli 2008; Trohidis et al. 2008; Laurier et al.
2009; Whitman et al. 2001; Berenzweig and Ellis 2001)
• Artist Identiﬁcation (Berenzweig et al. 2002; Kim and Whitman 2002; Mandel
and Ellis 2005; Mandel et al. 2006; Tsai and Wang 2006; Nwe and Li 2007;
Shen et al. 2009; Marques and Moreno 1999; Brown 1999)
• Instrument Recognition (Agostini et al. 2003; Essid et al. 2004, 2006a, b;
Kitahara et al. 2007; Leveau et al. 2007; Little and Pardo 2008; Fuhrmann et al.
2009; Hamel et al. 2009; Heittola et al. 2009; Slaney 2007; Levy and Sandler
2007)
• Music Annotation (Turnbull et al. 2007, 2008, 2009; Barrington et al. 2008;
Chen et al. 2008; Bertin Mahieux et al. 2008; Chechik et al. 2008; Levy and
Sandler 2009; Knees et al. 2009; Wang et al. 2009; Tomasik et al. 2009;
Hoffman et al. 2009; Kim et al. 2009; Scaringella et al. 2006; Weihs et al. 2007).
2.2
Feature Extraction
The key components of a classiﬁcation system are feature extraction and classiﬁer
learning (Duda and Hart 2000). Feature extraction addresses the problem of how to
represent the examples to be classiﬁed in terms of feature vectors or pair wise
similarities. The purpose of classiﬁer learning is to ﬁnd a mapping from the feature
space to the output labels so as to minimize the prediction error. We focus on music
classiﬁcation based on audio signals.
From the perspective of music understanding, we can divide audio features into
two levels, low-level and mid-level features. Low level features can be further
divided into two classes of timbre and temporal features as shown in Fig. 2.1.
Timbre features capture the tonal quality of sound that is related to different
instrumentation, whereas temporal features capture the variation and evolution of
timbre over time. Low-level features are obtained directly from various signal
processing techniques like Fourier transform, spectral/cepstral analysis, autore-
gressive modelling, etc. Low-level features have been used predominantly in music
classiﬁcation, due to the simple procedures to obtain them and their good perfor-
mance. However, they are not closely related to the intrinsic properties of music as
perceived by human listeners. Mid-level features provide a closer relationship and
include mainly three classes of features, namely rhythm, pitch, and harmony. These
features are usually extracted on top of low-level ones. At the top level, semantic
labels provide information on how humans understand and interpret music, like
genre, mood, style, etc. This is an abstract level as the labels cannot be readily
obtained from lower level features as indicated by the semantic gap between
mid-level features and labels. The purpose of content-based music classiﬁcation is
to bridge the semantic gap by inferring the labels from low-/mid-level features.
From a different perspective, audio features can also be categorized into short-term
18
2
Music Information Retrieval

features and long-term features, as illustrated by Fig. 2.1. Short-term features like
timbre features usually capture the characteristics of the audio signal in frames with
10–100 ms duration, whereas long-term features like temporal and rhythm features
capture the long-term effect and interaction of the signal and are normally extracted
from local windows with longer durations. Hence, the main difference here is the
length of local windows used for feature extraction.
2.2.1
Process of Feature Extraction
Time and frequency domain representation techniques for the automatic description
of music recordings are based on the computation of time and frequency repre-
sentations of audio signals. We summarize here the main concepts and procedures
to obtain such representations. The frequency of a simple sinusoid is deﬁned as the
number of times that a cycle is repeated per second, and it is usually measured in
cycles per second, or Hertz (Hz). As an example, a sinusoidal wave with a fre-
quency f = 440 Hz performs 440 cycles per second. The inverse of the frequency f
is called the period T (f = 1/T), which is measured in seconds and indicates the
temporal duration of one oscillation of the sinusoidal signal. In time domain, analog
signals x(t) are sampled each Ts seconds to obtain digital signal representations
Top Level labels
Genre – Pop, Rock, Blue, Classical etc. 
Mood – Happy, Sad, Angry, Jolly etc. 
Instrument – Piano, Guitar, Violin, Drum etc. 
More – Artist, Style, Similar Song etc. 
Middle Level Labels
Rhythm 
Pitch 
            Harmony 
Low Level Labels               Timbre – ZCR (Zero Crossing Rate), SC (Spectral Centroid), SR  
         (Spectral Rolloff), SF (Spectral Flux), SB (Spectral Bandwidth),  
          SFM (Spectral Flatness Measure), SCF (Spectral Crest Factor),  
          ASE (Amplitude Spectral Envelop), WT (Wavelet Transform),  
          MFCC (Mel-fequency Cepstrum Coefficient), LPCC (Linear  
          Predictive Cepstrum Coefficient), etc. 
Temporal – SM (Statistical Moments), AM (Amplitude 
modulation), ARM (Auto- Regressive Modelling) 
Fig. 2.1 Audio features
2.2
Feature Extraction
19

x[n], where n = i  Ts, i = 0, 1, 2, … and fs = 1, Ts is the sampling rate in samples
per second(Hz). According to the Nyquist-Shannon sampling theorem, a given
audio signal should be at least sampled to the double of its maximum frequency to
avoid the so-called aliasing, i.e. the introduction of artifacts during the sampling
process. Time-domain representations, illustrated in Fig. 2.2, are suitable to extract
descriptors related to the temporal evolution of the waveform x[n], such as the
location of major changes in signal properties.
The frequency spectrum of a time-domain signal is a representation of that signal
in the frequency domain. It can be generated via the Fourier Transform (FT) of the
signal, and the resulting values are usually presented as amplitude and phase, both
plotted versus frequency, as illustrated in Fig. 2.3.
For sampled signals x[n] we use the Discrete version of the Fourier Transform
(DFT). Spectrum analysis is usually carried out in short segments of the sound
signal (called frames), in order to capture the variations in frequency content along
time (Short Time Fourier Transform-STFT). This is mathematically expressed by
multiplying the discrete signal x[n] by a window function w[n], which typically has
a bell-shaped form and is zero-valued outside of the considered interval as illus-
trated in Fig. 2.4.
The audio signal is described using various numerical values extracted from the
signal. These are called as features of the signal. Feature extraction is an important
audio analysis step. In general, feature extraction is an essential processing step in
machine learning tasks and classiﬁcation tasks. The aim is to extract a set of features
from the dataset of interest. These features are more informative with respect to the
desired properties of the original data i.e. the audio signal. Feature extraction can
also be viewed as a data rate reduction procedure because analysis algorithms to be
based on a relatively small number of features. In case, the original data, i.e. the
audio signal, is voluminous and as such, it is hard to process directly in any analysis
task. Therefore it needs to be transformed into the initial data representation to a
more suitable one, by extracting audio features that represent the properties of the
Fig. 2.2 Time domain
(Amplitude vs. Time) of Input
audio sample
20
2
Music Information Retrieval

original signals while reducing the volume of data. In order to achieve this goal, it is
important to have a good knowledge of the application domain, so that we can
decide the best features.
Fig. 2.3 Frequency domain (Fourier Transform) of Input audio
Fig. 2.4 Short time Fourier Transform of Input audio
2.2
Feature Extraction
21

2.2.2
Selection of Features
In this section, some of the essential features are presented. These features are
widely adopted in the work for instrument classiﬁcation approach. Right set of
feature selection plays important roles in the classiﬁcation.
For example, Instrument classiﬁcation requires the following features to be used:
(a) Temporal features: zero crossing rate, low energy.
(b) Spectral shape features: features (instantaneous) computed from the Short Time
Fourier transform (STFT) of the signal. These include spectral centroid, spectral
roll-off, spectral irregularity, spectral entropy, spectral skewness, spectral kur-
tosis, and spectral spread.
(c) Cepstral features: Mel Frequency Cepstral Coefﬁcients (MFCC) as cepstral
feature.
The features extracted are explained below.
(a) Temporal Features
Temporal feature is calculated by analysing temporal distribution.
Low Energy
The temporal distribution of energy is given by energy curve, to check its behaviour
throughout the signal whether it remains constant, or if some frames are more
contrastive than others. Low energy is the percentage of frames showing
less-than-average energy.
Zero-crossing rate (ZCR)
ZCR gives the number of sign change rate within a frame of time domain wave-
form. It is the indicator for the noisiness of the signal. Zero Crossing is calculated
by counting the number of times that the time domain signal crosses zero within a
given window as shown in Fig. 2.5.
Zt ¼ 1
2
X
N
n¼1
jsign x n½ 
ð
Þ  sign x n  1
½

ð
Þj
ð2:1Þ
Fig. 2.5 Zero Crossing Rate
22
2
Music Information Retrieval

‘Crossing zero’ is deﬁned as (xn−1 < 0 and xn > 0) or (xn−1 > 0 and xn < 0) or
(xn−1 6¼ 0 and xn = 0).
(b) Spectral Features
Spectral features (Nedeljkovic 1994) are calculated from the short time Fourier
transform (STFT) and are calculated for every short-time frame of the audio signal.
The spectral centroid (l)
Spectral centroid is a measure used to characterize a spectrum. It indicates where
the “centre of mass” of the spectrum as shown in Fig. 2.6. It is calculated as the
weighted mean of the frequencies present in the signal, which is determined by
using Fourier transform, with their magnitudes as the weights. The centroid mea-
sures the spectral shape. Higher centroid values indicate higher frequencies.
For the time-domain signal x(t):
Y f
ð Þ ¼ F x tð Þ
½

j
j
ð2:2Þ
Spectral centroid,
l ¼
Z
f :pðf Þ:df
ð2:3Þ
where
pðf Þ ¼
Yðf Þ
P
f
Yðf Þ
ð2:4Þ
Spectral Roll-off
Spectral roll-off is another measure of spectral shape. It is the point where frequency
that is below some percentage (usually at 95%) of the power spectrum resides. It is
one of the ways to estimate the amount of high frequency in the signal consists in
ﬁnding the frequency such that a certain fraction of the total energy is contained
below that frequency.
Fig. 2.6 Spectral centroid
2.2
Feature Extraction
23

The spectral roll-off point (fc) is the frequency for which 95% of the signal
energy is below this frequency.
Using the amplitude Y(f):
X
fc
0
Y2 f
ð Þ ¼ 0:95
X
fny
0
Y2 f
ð Þ
ð2:5Þ
Spectral Irregularity
Spectral irregularity of a spectrum is the degree of variation of the successive peaks
of the spectrum. It is the sum of the amplitude minus the mean of preceding, same
and next amplitude.
X
N1
k¼2
ak  ak1 þ ak þ ak þ 1
3


ð2:6Þ
Spectral entropy
In information theory, we use the Shannon entropy which is based on the following
equation
HðxÞ ¼ 
X
N
i¼1
pðxiÞ log2pðxiÞ
ð2:7Þ
Spectral entropy provides information of the signal and indicates whether it
contains predominant peaks or not.
Spectral skewness (c1)
In probability theory and statistics, skewness is a measure of the asymmetry of the
probability distribution of a real valued random variable. The skewness value can
be positive or negative, or even undeﬁned. Qualitatively, a negative skew indicates
that the tail on the left side of the probability density function is longer than the
right side and the bulk of the values (possibly including the median) lie to the right
of the mean. A positive skew indicates that the tail on the right side is longer than
the left side and the bulk of the values lie to the left of the mean. A zero value
indicates that the values are relatively evenly distributed on both sides of the mean,
typically but not necessarily implying a symmetric distribution.
Thus, the spectral skewness is a measure of the asymmetry of the distribution
around the mean value l. The skewness ðc1Þ is calculated from the 3rd order
moment, m3 as:
24
2
Music Information Retrieval

m3 ¼
Z
ðf  lÞ3:pðf Þdf
ð2:8Þ
c1 ¼ m3
r3
ð2:9Þ
Spectral kurtosis (c2)
Spectral kurtosis indicates the ﬂatness of the energy distribution. Higher kurtosis
means more of the variance is the result of infrequent extreme deviations, as
opposed to frequent modestly sized deviations. It is calculated from the 4th order
moment, m4, using the value of l as:
m4 ¼
Z
f  l
ð
Þ4:pðf Þdf
ð2:10Þ
c2 ¼ m4
r4
ð2:11Þ
If kurtosis c2 ¼ 3, then it indicates a normal (Gaussian) distribution.
Spectra with c2\3 are ﬂatter and conversely spectra with c2 [ 3 have a more
deﬁned, sharper peak.
The spectral spread (r)
Spectral spread is a measure of variance (or spread) of the spectrum around the
mean value l calculated in equation:
r2 ¼
Z
ðf  lÞ2:pðf Þdf
ð2:12Þ
(c) Cepstral Features
In sound processing, Cepstral feature is widely used. Among various cepstral
features, MFCC (Chandwadkar and Sutaone 2012; Jensen et al. 2006; Gupta et al.
2013) is the most famous one. MFC is a representation of the short-term power
spectrum of a sound, based on a linear cosine transform of a log power spectrum on
a nonlinear Mel scale of frequency. The Coefﬁcients of MFC are collectively
referred to as Mel-frequency cepstral coefﬁcients (MFCCs). MFCCs are cepstral
coefﬁcients used for representing audio in a way that mimics the physiological
properties of the human auditory system.
Block diagram of MFCC is shown in Fig. 2.7. Steps involved in its calculation
are as follows:
1. The Fourier transform of (a windowed excerpt of) a signal is calculated.
2. Triangular overlapping windows are used to map the powers of the spectrum
obtained above onto the Mel scale.
2.2
Feature Extraction
25

3. Now, take the logs of the powers at each of the Mel frequencies.
4. Take the discrete cosine transform (DCT) of the list of Mel log powers, as if it
were a signal.
5. The MFCCs are the amplitudes of the resulting spectrum. First 13 coefﬁcients
are saved.
Conversion from the linear frequency scale to the Mel scale frequency mf is
achieved using the following equation
mf ¼ 1127:01048 logeð1 þ
f
700Þ
ð2:13Þ
where f is frequency in hertz in linear scale.
2.3
Conclusions and Discussion
A very concise study on audio signal processing is covered for the reader. To build
robust music signal analysis techniques for audio signal in a useful way, the reader
must examine the other different feature extraction techniques also.
In upcoming chapters different problems related with Indian music are attempted
with other feature extraction techniques, which will give some insight knowledge of
the feature extraction technique related to applications. There is much audio pro-
cessing software available which is very useful in music research, some of the
software names are:
SOLO EXPLORER
This software is the oldest software that was developed for the audio processing
technique. It converts wav audio ﬁle to its MIDI (Musical Instrument Digital
Interface) form. It has an automatic music transcription (recognition) system. In
other words, it can identify the instrument being played in the sample.
Audio 
Spectrum
Mel-
frequency
Log
DCT 
MFCC
Fig. 2.7 Block diagram of MFCC
26
2
Music Information Retrieval

SONIC VISUALISER
This software is the improvised form of Solo Explorer software. It opens MP3,
wmv and WAV audio ﬁle.
2.3
Conclusions and Discussion
27

It has proven itself to be very useful for the music researcher in many aspects.
For the input audio record, it gives combined channels view, spectrogram, melodic
range, peak frequency and spectrum views, Multiple channel views (Separate,
Mean, Butterﬂy), scale options (dB, Linear, Meter), gain, pan and balance tweaks,
amongst others. It also provides beat trackers; tempo and key estimators; pitch,
tempo and note trackers; harmony and chord extraction; timbre and rhythmic
similarity estimators.
GOLDWAVE
This software is a pre-processing tool. It supports various audio ﬁle formats,
including WAV, MP3, WMA, etc. It can also record sounds (live recording) and
then processes it to make it in standard form. This standard audio can be saved in
wav format. GoldWave is widely used by researchers during data collection for
speech and music processing work. It has added quality of having real-time graphic
visuals. It can edit multiple ﬁles at once. It possess processing tools including ﬁlters
such as noise reduction, compressor/expander, volume shaping, volume matcher,
pitch, reverb, resampling, parametric EQ, etc. One can preview the effect of the
applied tool simultaneously.
28
2
Music Information Retrieval

SOUND FORGE
Sound Forge is digital audio editing software that includes a powerful set of audio
processes, tools, and effects for manipulating audio. It contains tools for Spectrum
analysis; White, pink, brown and ﬁltered noise generators; DTMF/MF tone syn-
thesis etc.
It allows one to edit, record, encode, and master nearly any form of digital audio
including WAV, AIFF, and MP3 including mp4 also. Mp4 videos can be converted
to wav ﬁle by this software. This software is also widely used for converting one
audio form to another audio form. It has a function for Batch conversion.
AUDACITY
Audacity is the most famous audio processing software among all. It can edit and
process polyphonic music also. That is why; it is a multi-track audio editor. It can
also copy, combine, cut and splice tracks like GoldWave and Sound Forge
software.
It can add ﬁlters, alter pitch and speed, eliminate or reduce the presence of
background music like other audio processing tools. But the main difference is that
it can isolate vocals. And user can work on the isolated vocals for the respective
works.
2.3
Conclusions and Discussion
29

Main drawback of Audacity software is that, it is difﬁcult to use. Only learned
user can utilize the full power of this software. Other drawback includes poor
documentation and unavailability of conversion from audio formats to MIDI or
MIDI to audio.
The above software download links are http://solo-explorer.en.softonic.com/,
http://www.downloadcrew.com/article/32152-sonic_visualiser/,
http://download.
cnet.com/GoldWave/3000-2170_4-10001099.html,
http://www.sonycreativesoft
ware.com/download/trials/soundforgepro,
http://download.cnet.com/Audacity/
3000-2170_4-10058117.html.
References
Agostini G, Longari M, Pollastri E (2003) Musical instrument timbres classiﬁcation with spectral
features. EURASIP J. Appl Sig Process 2003(1):5–14
Barrington L, Yazdani M, Turnbull D, Lanckriet G (2008) Combining feature Kernels for semantic
music retrieval. In: Proceedings of the International Conference on Music Information
Retrieval
Berenzweig AL, Ellis DPW (2001) Locating singing voice segments within music signals. In:
Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and
Acoustics
30
2
Music Information Retrieval

Berenzweig AL, Ellis DPW, Lawrence S (2002) Using voice segments to improve artist
classiﬁcation of music. In: Proceedings of the International Conference on Virtual, Synthetic
and Entertainment Audio
Bergstra J, Casagrande N, Erhan D, Eck D, Kegl B (2006) Aggregate features and adaboost for
music classiﬁcation. Mach Learn 65(2–3):473–484
Bertin Mahieux T, Eck D, Maillet F, Lamere P (2008) Autotagger: a model for predicting social
tags from acoustic features on large music databases. J New Music Res 37(2):115–135
Brown JC (1999) Computer identiﬁcation of musical instruments using pattern recognition with
cepstral coefﬁcients as features. J Acoust Soc Amer 105:1933–1941
Chandwadkar DM, Sutaone MS (2012) Role of features and classiﬁers on accuracy of
identiﬁcation of musical instruments. In: 2nd National Conference on Computational
Intelligence and Signal Processing (CISP), pp 66–70
Chechik G, Le E, Rehn M, Bengio S, Lyon D (2008) Large scale content based audio retrieval
from text queries. In: Proceedings of the ACM Multimedia Information Retrieval
Chen ZS, Zen JM, Jang JS (2008) Music annotation and retrieval system using anti-models. In
Proceedings of the Audio Engineering Society
Cheung W-L, Lu G (2008) Music emotion annotation by machine learning. In: Proceedings of the
International Workshop on Multimedia Signal Processing
Downie JS (2003) Music information retrieval. Annu Rev Inf Sci Technol 37:295–340
Duda RO, Hart PE (2000) Pattern Classiﬁcation, 2nd edn. Wiley, NewYork
Essid S, Richard G, David B (2004) Musical instrument recognition based on class pairwise
feature selection. In: Proceedings of the International Conference on Music Information
Retrieval
Essid S, Richard G, David B (2006a) Music and instrument recognition by pairwise classiﬁcation
strategies. IEEE Trans Audio Speech Lang Process 14(4):1401–1412
Essid S, Richard G, David B (2006b) Instrument recognition in polyphonic music based on
automatic taxonomies. IEEE Trans Audio Speech Lang Process 14(1):68–80
Feng Y, Zhuang Y, Pan Y (2003) Music retrieval by detecting mood via computational media
aesthetics. In: Proceedings of the International Conference Web Intelligence
Fuhrmann F, Haro M, Herrera P (2009) Scalability, generality and temporal aspects in automatic
recognition of predominant musical instruments in polyphonic music. In: Proceedings of the
International Conference on Music Information Retrieval
Gupta S, Jaafar J, Fatimah wan Ahmad W, Bansal A (2013) Feature extraction using MFCC. SIPIJ
4(4)
Hamel P, Wood S, Eck D (2009) Automatic identiﬁcation of instrument classes in polyphonic and
poly-instrument audio. In: Proceedings of the International Conference on Music Information
Retrieval
Heittola T, Klapuri A, Virtanen T (2009) Musical instrument recognition in polyphonic audio
using source-ﬁlter model for sound separation. In: Proceedings of the International Conference
on Music Information Retrieval
Hoffman M, Blei D, Cook P (2009) Easy as CBA: a simple probabilistic model for tagging music.
In: Proceedings of the International Conference on Music Information Retrieval
Jensen JH, Christensen MG, Murthi M, Jensen SH (2006) Evaluation of MFCC estimation
techniques for music similarity. In: Proceedings of the European Signal Processing Conference,
pp 926–930
Kim YE, Whitman B (2002) Singer identiﬁcation in popular music recordings using voice coding
features. In: Proceedings of the International Conference on Music Information Retrieval
Kim JH, Tomasik B, Turnbull D (2009) Using artist similarity to propagate semantic information.
In: Proceedings of the International Conference on Music Information Retrieval
Kitahara T, Goto M, Komatani K, Ogata T, Okuno H (2007) Instrument identiﬁcation in
polyphonic music: feature weighting to minimize inﬂuence of sound overlaps. EURASIP J
Appl Signal Process 2007(1):155
References
31

Knees P, Pohle T, Schedl M, Schnitzer D, Seyerlehner K, Widmer G (2009) Augmenting
text-based music retrieval with audio similarity. In: Proceedings of the International
Conference on Music Information Retrieval
Korhonen MD, Clausi MJDA (2006) Modeling emotional content of music using system
identiﬁcation. IEEE Trans Syst Man Cybern 36(3):588–599
Laurier C, Sordo M, Serra J, Herrera P (2009) Music mood representations from social tags. In:
Proceedings of the International Conference on Music Information Retrieval
Leveau P, Sodoyer D, Daudet L (2007) Automatic instrument recognition in a polyphonic mixture
using sparse representation. In: Proceedings of the International Conference on Music
Information Retrieval
Levy M, Sandler M (2007) A semantic space for music derived from social tags. In: Proceedings
of the International Conference on Music Information Retrieval
Levy M, Sandler M (2009) Music information retrieval using social tags and audio. IEEE Trans
Multimedia 11(3):383–395
Li T, Ogihara M, Li, Q (2003) A comparative study of content-based music genre classiﬁcation.
In: Proceedings of the SIGIR
Liand T, Ogihara M (2003) Detecting emotionin music. In: Proceedings of the International
Conference on Music Information Retrieval
Liand T, Ogihara M (2006) Toward intelligent music information retrieval. IEEE Trans
Multimedia 8(3):564–574
Lidy T, Rauber A (2005) Evaluation of feature extractors and psychoacoustic transformations for
music genre classiﬁcation. In: Proceedings of the International Conference on Music
Information Retrieval
Lidy T, Rauber A, Pertusa A, Inesta J (2007) Improving genre classiﬁcation by combination of
audio and symbolic descriptors using a transcription system. In: Proceedings of the
International Conference on Music Information Retrieval
Lin C-H, Shih J-L, Yu K-M, Lin H-S (2009) Automatic music genre classiﬁcation based on
modulation spectral analysis of spectral and cepstral features. IEEE Trans Multimedia 11
(4):670–682
Little D, Pardo B (2008) Learning musical instruments from mixtures of audio with weak labels.
In: Proceedings of the International Conference on Music Information Retrieval
Lu L, Liu D, Zhang H-J (2006) Automatic mood detection and tracking of music audio signals.
IEEE Trans Speech Audio Process 14(1):5–18
Mandel M, Ellis D (2005) Song-level features and SVMs for music classiﬁcation. In: Proceedings
of the International Conference on Music Information Retrieval
Mandel M, Poliner G, Ellis D (2006) Support vector machine active learning for music retrieval.
Multimedia Syst 12(1):3–13
Marques J, Moreno PJ (1999) A study of musical instrument classiﬁcation using gaussian mixture
models and support vector machines. Technical Report, Cambridge Research Lab, Cambridge
Meng A, Shawe-Taylor J (2005) An investigation of feature models for music genre classiﬁcation
using the support vector classiﬁer. In: Proceedings of the International Conference on Music
Information Retrieval
Meng A, Ahrendt P, Larsen J (2007) Temporal feature integration for music genre classiﬁcation.
IEEE Trans Audio Speech Lang Process 15(5):1654–1664
Mierswa I, Morik K (2005) Automatic feature extraction for classifying audio data. Mach Learn
58:127–149
Mion L, Poli GD (2008) Score-independent audio features for description of music expression.
IEEE Trans Audio Speech Lang Process 16(2):458–466
Mochen F, Mierswa I, Ultsch A (2006) Understandable models of music collections based on
exhaustive feature generation with temporal statistics. In: Proceedings of the ACM SIGKDD
Morchen F, Ultsch A, Thies M, Lohken I (2006) Modeling timbre distance with temporal statistics
from polyphonic music. IEEE Trans Audio Speech Lang Process 14(1):81–90
Nedeljkovic I (1994) Image classiﬁcation based on fuzzy logic. Int Arch Photogrammetry
34(part XXX)
32
2
Music Information Retrieval

Nwe T-L, Li H (2007) Exploring vibrato-motivated acoustic features for singer identiﬁcation.
IEEE Trans Audio Speech Lang Process 15(2):519–530
Panagakis I, Benetos E, Kotropoulos C (2008) Music genre classiﬁcation: a multilinear approach.
In: Proceedings of the International Conference on Music Information Retrieval
Panagakis I, Kotropoulos C, Arce GR (2009) Music genre classiﬁcation using locality preserving
non-negative tensor factorization and sparse representations. In: Proceedings of the
International Conference on Music Information Retrieval
Scaringella N, Zoia G (2005) On the modelling of time information for automatic genre
recognition systems in audio signals. In: Proceedings of the International Conference on Music
Information Retrieval
Scaringella N, Zoia G, Mlynek D (2006) Automatic genre classiﬁcation of music content—a
survey. IEEE Sig Process Mag 23(2):133–141
Shen J, Shepherd J, Ngu A (2006) Towards effective content-based music retrieval with multiple
acoustic feature combination. IEEE Trans Multimedia 8(6):1179–1189
Shen J, Shepherd J, Cui B, Tan K-L (2009) An ovel frame work for efﬁcient automated singer
identiﬁcation in large music databases. ACM Trans Inf Syst 27(3):1–31
Slaney M (2007) Semantic-audio retrieval. In: Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing
Song Y, Zhang C (2008) Content based information fusion for semisupervised music genre
classiﬁcation. IEEE Trans Multimedia 10(1):145–152
Tomasik B, Kim JH, Ladlow M, Augat M, Tingle D, Wicentowski R, Turnbull D (2009) Using
regression to combine data sources for semantic music discovery. In: Proceedings of the
International Conference on Music Information Retrieval
Trohidis K, Tsoumakas G, Kalliris G, Vlahavas I (2008) Multi-label classiﬁcation of music into
emotions. In: Proceedings of the International Conference on Music Information Retrieval
Tsai W-H, Wang H-M (2006) Automatic singer recognition of popular music recordings via
estimation and modeling of solo vocal signals. IEEE Trans Audio Speech Lang Process 14
(1):330–341
Turnbull D, Elkan C (2005) Fast recognition of musical genres using RBF networks. IEEE Trans
Knowl Data Eng 17(4):580–584
Turnbull D, Barrington L, Torres D, Lanckriet G (2007) Towards musical query by semantic
description using the cal500 dataset. In: Proceedings of the ACM Information Retrieval
Turnbull D, Barrington L, Torres D, Lanckriet G (2008) Semantic annotation and retrieval of
music and sound effects. IEEE Trans Audio Speech Lang Process 16(2):467–476
Turnbull D, Barrington L, Yazdani M, Lanckriet G (2009) Combining audio content and social
context for semantic music discovery. In: Proceedings of the ACM Information Retrieval
Tzanetakis G, Cook P (2002) Musical genre classiﬁcation of audio signals. IEEE Trans Speech
Audio Process 10(5):293–302
Wang F, Wang X, Shao B, Li T, Ogihara M (2009) Tag integrated multi-label music style
classiﬁcation with hypergraph. In: Proceedings of the International Conference on Music
Information Retrieval
Weihs C, Ligges U, Morchen F, Mullensiefen D (2007) Classiﬁcation in music research. Adv Data
Anal Classif 1(3):255–291
West K, Cox S (2005) Finding an optimal segmentation for audio genre classiﬁcation. In:
Proceedings of the International Conference on Music Information Retrieval
Whitman B, Flake G, Lawrence S (2001) Artist detection in music with minnow match. In:
Proceedings of the IEEE Workshop on Neural Networks for Signal Processing
Yang D, Lee W (2003) Disambiguating music emotion using software agents. In: Proceedings of
the International Conference on Music Information Retrieval
Yang Y-H, Liu C-H, Chen HH (2006) Music emotion classiﬁcation: a fuzzy approach. In:
Proceedings of the ACM Multimedia
Yang Y-H, Lin Y-C, Su Y-F, Chen HH (2008) A regression approach to music emotion
recognition. IEEE Trans Audio Speech Lang Process 16(2):448–457
References
33

Chapter 3
Scales and Shruti Concept
3.1
Views on Shruti
Music of modern days transcended the simplicity of early human music and its
practical needs. The artistic and the aesthetic aspects began to emerge through its
entertainment potential during the medieval times. Its beauty and serenity along
with its potential to touch the emotional chord evoked intense interest among the
earliest thinkers in every civilization and India is no exception. The recorded evi-
dence of musical activities in India dates back to more than 2000 years. There have
been attempts to a sort of metaphysical rationalization in music in early days
followed by attempts, at times, to formalize music including structures in some
domains of it. A deﬁnite predominant direction in the paradigm of analysis seemed
to emerge universally. This pertained to the perception of pitch. The other cognitive
phenomena of attention, among others, were loudness, timbre and rhythm. The
continuous domain of pitch began to be conceived in music mostly not as a con-
tinuum but a repetitive set of discrete intervals. Notably this discretization of
continuum has striking similarity in terms of numbers, measures and repeatability in
the music of completely different origin in different parts of the world. This simi-
larity includes even songs of the song-birds (Kumar et al. 1993). As the develop-
ment of music continued, number of intervals grew and ultimately stabilized in a
reasonably small numbers. The big intervals later on began to be divided into
smaller sub-intervals because of the need to understand ﬁner aesthetic perception.
In Indian musical structures swaras (intervals) and shrutis (microtonal intervals)
emerged some 2000 years back. The ancient period witnessed intense debate and
theorization by different philosophers of music. The basic premise on which these
debates relied is the acute sense of perception simply because of the absence of
appropriate tools for objective measurement of the related matters. In recent times
we have seen a great interest of modern science interacting with this highly emotive
and experiential phenomenon of music.
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_3
35

Yet an orthodox question “Why should there be a scientiﬁc research in music?
What is the use?” is oft-repeated. “I wonder and wonder and feel sad.” said Deva
(1974). “What is the use of historical research in music? …. I wonder whether
Newton ever thought of the use of falling apples. Whether J.C. Bose ever asked
about the use of his biological research…what is the use of Bharata’s theory of
shrutis?” We human beings have an inquisitive mind. Questions lead to research.
The modern science offers an objective basis of research with a plethora of tools
and robust methodologies supposed to be independent of users. It appears that the
public reposes faith in the modern scientiﬁc approaches.
This modern science, which is of western origin, is generally traced back to 5th
century BC by the philosophers Leucippus and Democritus (Lloyd 2006; Wilber
1984). The elements of reality were reduced to two, being and non-being, full and
void, atom and space. A reality was born which was strictly and coarsely material,
the subject, the ‘I’ went out in the wilderness. The reality was assumed to be strictly
causal. This concept of science continued till the discovery of quantum theory in the
20th century. The realization that physics has de-humanized science so much that a
man has become an impotent ‘observer symbol’ started agitating the minds of
scientists in the late 20th century. It is strange that none bothered about the strange
indifference of this science towards the important half of the creation, the conscious
world.
Unfortunately, we tend “to forget that the best in Indian musicology was sci-
entiﬁc. …… The experiment on the shruti veena described by Sarangadeva is an
excellent one in psychoacoustics; but with a mind heavily burdened by a stagnating
inferiority, we prefer to call it musicology and the same experiment by Ellis in the
19th Century in England as Science” (Deva 1974).
There was another science, the science of the ancient East. This is often referred
to as mysticism. This was holistic in nature combining both the experiential and
experimental realities. Of course the experiments were then very rudimentary
compared to the extreme sophistication of modern science. The lack of this
sophistication was somewhat compensated by experiential data from various natural
phenomena integrated in the thought experiments of those mystics to discover the
truth. This paradigm of knowing yielded in olden times some surprising truths,
which very closely conform to the ﬁndings of modern science (Chowdhuri and
Datta 1998). A relevant case in point is the theory on nature and propagation of
sound. Sabar Muni in 57 BC (Nayaratna 1889) held that the physical sound con-
stituted a series of conjunction and disjunction of air particles with rarefaction and
condensation. We would like to emphasize that there is absolutely no evidence of
the existence of experimental infrastructure in material science available at that
period to quantify or measure pressure variation in air. Prastapada (4th century AD)
(Seal 1985) described the ﬁrst sound as giving of a second, the second a third and so
on expanding akasa in the same way as waves propagate themselves in water
(bicitaranganyaya). In the 6th century AD Udyoktakara 1940 opined that the ﬁrst
sound created gave of not one sound in a circle but an inﬁnite number of sounds in
all directions (kadambakorakanyaya). Each of these again gives of another set of
sounds in all directions (compare Huygens’s principle) (Rayleigh 1945). The Nyaya
36
3
Scales and Shruti Concept

thinkers consider that each sound wave is destroyed by its successor. (Compare
Fresnel’s correction, 1815) (Rayleigh 1945). This historical evidence is mentioned
only to emphasize that the holistic approach, like reductionism of modern science is
quite effective in garnering truth.
It will be seen that the Indian thinkers analyzed and discussed, in great depth,
different structures on the continuum of pitch primarily from a perceptual analytical
angle though had taken recourse to some experiments with strings to give some
numerical representations. It appears that they were not unduly concerned with
mathematisation of music. One of the noted musicologists of recent times, (Tagore
1874), observed “…(it) mystiﬁes the subject by enveloping it in a cloud of math-
ematicism…We hold that it is quite possible to build a rational theory of music
without the help of numbers”. This skepticism may be understandable when one
sees theories developed on prime numbers like 2, 3, 5, 7 etc. or bringing down p to
substantiate 22 shrutis and 7 intervals without explaining the physical or psy-
chophysical rationality behind such premises. These arithmetical manipulations
were not substantiated with either a ﬁrm physical or a psychological theory or
objective analysis of experimental data.
Before the advent of Artiﬁcial Intelligence, the scientiﬁc research on music in
India remained primarily involved in the physics of sound and vibration. Some of
the examples are the mathematical analysis of string, wind and percussion instru-
ments (Raman 1921, 1935; Raman and Kumar 1920). The cognitive and experi-
ential aspects of music were not taken much note of. The emergence of the subject
Artiﬁcial Intelligence appears to be co-incidental with the taking up of cognitive
aspects of music into the ambit of modern scientiﬁc research. A scientiﬁc under-
standing of music must begin by taking into account how minds act in the ambience
of music and how this ambience is correlated with the physical reality of the sound
of music. Music like speech is also a mode of communication between human
beings. The communicator endeavors to communicate certain messages, be it mood,
feelings, expression and the like. Through this he creates a story, a sort of ambience
for the audience. In a sense music appears to be a more universal phenomenon than
speech. In speech the listener has to know the language of the speaker to get the
message. In creating music an artist uses an objective material called sound. The
most interesting point to note is that this sound contains only about 40% of the
content of the music. The objective contents are embedded in fundamental fre-
quency, amplitude, complexity and duration. The other 60%, called semiotics,
reside in the mind of the listener. The semiotics in music consists of lexicon
(chalan, pakad), syntax (raga), pragmatics (thaat, gharana) and semantics (mood,
feeling, emotion). Thus if science has to probe music it has to take into account
these semiotics also, the cognitive processes taking place in the mind along with the
acoustics of it. As in the case of a language, the semiotics here is also dependent on
the speciﬁc genre of music. This needs to be borne in mind when one takes up a
particular music for study. A comprehensive scientiﬁc approach therefore needs to
address the physical reality of acoustics and the cognitive realities of semiotics.
To our mind, it is this ultimate reality, where matter and mind play equal roles,
the science of music should and must graze. The formation of a linkage between the
3.1
Views on Shruti
37

natural processes of progress from sensory perception to concept formation inherent
in human mind is to be included in the scheme of science. It is useful to recollect
here the caution given by Pauli (Hiesenberg 1974) “… pure logic is fundamentally
incapable of constructing such a linkage. The most satisfactory course, it seems, is
to introduce at this point a postulate of an order …, the relation between sense
perception and Idea remains a congruence of the fact that both the soul and what is
known as perception are subject to an order objectively conceived.” This order is to
be born and this is the real challenge of science in music.
The ancient Indian thinkers placed a signiﬁcant importance to shruti in the
structure of music. There has been a lot of ambiguity till modern times as to a
precise deﬁnition of shrutis, their numbers, locations, and relations to swaras as well
as their role in the totality of the theme of a particular musical piece. Exhaustive and
painstaking psycho-perceptual experiments are needed to understand the nature of
these ambiguities, which in turn are likely to resolve some of them. A theoretical
model of musical scale including shrutis based on psycho perceptual and physical
theories must resolve inter alia the following questions:
(1) Is there some rational (causal) basis of the concepts of swaras and shrutis in an
octave? if so,
(2) What are their number and lengths (usually given in ratios)?
(3) How are the shrutis related to the swaras?
Before we attend these questions it would be pertinent to review the already
expressed opinions on shrutis and swaras.
3.2
Ancient Period
We do not come across any evidence of the use of microtones in the primitive,
prehistoric and Vedic periods. However, some musicologists hold that the use of
microtones may be assigned to the beginning of the classical period in 600–500 BC,
when vedic music sāmagāna was falling out of practice and laukika gandharva type
of systematic-cum-scientiﬁc music was gaining ascendancy over it (Praganananda
1963). It was made perfect by Nārada of Sikshā (1st century AD) and Bharata of
Nātyashāstra (2nd Century BC) (Kavi Ramakrishna and Pade 1964). During the
Vedic period archikā-gāyana/ekesvara-gāyana (sung with single tune), gāthā-
gāyana (chanting in two tune and sāmika-gāyana (singing hymns with three tunes)
were the earliest known method of systematic singing. Sāmagānas were sung with
basic notes udātta (rishava—10/9), anudātta (nishāda—18/91) and svarita (sajda-1).
Two to three stringed instruments were found at Lothal, which dates back to 2000
BC. Gradually these developed into quadratonic scale with the addition of gāndhara
(32/27). Four string lutes were found at the site of Rupar datable to 200 BC. The
tones pa and dha were added successively.
38
3
Scales and Shruti Concept

One of the earliest written documents on musicology in India is Nātyasāstra, the
original portions of this treatise were written by Bharata Muni (200 BC). Muni
Bharata’s Nātyashāstra is believed to be a collection of earlier Nātyashāstras.
Bharata describes a music system based on jāti-s (modes) that is murcchanā (scales)
based on the successive notes of two heptatonic scales, sadjagrāma (a scale based
on the note, sadjā) and madhymagrāma (a scale based on the note, madhyamā). The
only difference between these two parent scales was the placement of one shruti.
Bharata also speaks of a microtonal interval: the shruti, which is heard (SrUyanta iti
Srutayāh). Intervals of three sizes—4, 3, or 2 shrutis—formed the basis for ancient
scales (sloka. 28.22–23).
Bharata describes the classical 22-shruti scale by giving the names of seven
swaras, Sadja, Risabha, Gāndhara, Madhyamā, Pañcamā, Dhaivata, and Nisāda
(sloka 28.21) (Sastri Sambasiva 1930). What is an Amsa anywhere is vadi. Two
notes separated by nine or thirteen shrutis are called samvadi and by two shrutis are
called vivadi. A note as it follows another note is called anuvadi. The ancient
musicologists used the term shruti to denote the sounds themselves, which are thus
distinguished from one another (Sastri Sambasiva 1928; Sastri Subrahmanya 1992;
Sarangadeva 1972). While for Dattila and Sarangadeva shruti is primarily a pitch
position, it appears that Bharata viewed shruti mainly as an interval, the smallest
possible one that distinguishes one sound from another as lower or higher in pitch
(sloka 28, 26–27) (Sastri Sambasiva 1930).
Shruti was the unit of measure of swara’s. It is also the basis on which swara
structures were classiﬁed into two grama-s (sloka18, 5–6) (Sastri Sambasiva 1930).
In fact, Abhinavagupta believed that it was for the purpose of classiﬁcation that the
concept of shruti was formulated; there was no reference to their existence in
performance (sloka 1.11) (Sastri Sambasiva 1930).
Bharata said that the measure (pramana) of shruti is the intervallic difference
between the panchama of shadjagrama and the panchama of madhyamagrAma.
Pramana is not the value of any particular kind of shruti such as `pramana-shruti’
(Sastri Sambasiva 1930). Abhinava further states that augmenting or diminishing a
sound should be perceivable by the ear. He is aware that, though theoretically,
augmenting and diminishing can be negotiated through very minute or literally
atomic intervals, such inﬁnitesimal intervals cannot be grasped by the ear (Sastri
Sambasiva 1930).
Bharata has also explicitly stated the distribution of shrutis on the two scales. In
the Sadjagrama, Sadja includes four shrutis, Rsabha three, Gandhara two,
Madhyama four, Pañcama four, Dhaivata three, and Nisada two (sloka 28.25–26)
(Sastri Sambasiva 1930). In the Madhyama Grama, Madhyama consists of four
shrutis, Pañchama three, Dhaivata four, Nisada two, Sadja four, Risabha three, and
Gandhara two shrutis (sloka. 27–28) (Sastri Sambasiva 1930). It may be noted that
Bharata speaks of a total of 22 shrutis in the Saptak.
Dattalingam (Sastri Sambasiva 1928) also refers to the same distribution of
shrutis on different swaras. The sound (dhwani), which is indicated by the term
Sadja, is the starting point in the Sadjagrama. In this Grama, Sadja includes three,
Risabha two, Gandhara four, Madhyama four, Pañchama three, Dhaivata two, and
3.2
Ancient Period
39

Nisada four. From this one the third (shruti) upwards is Risabha. From Risabha the
second shruti is Gandhara, from Gandhara the fourth shruti is Madhyama. From
Madhyama in the same way Pañchama; from this one the third shruti is Dhaivata.
From this one the second (shruti) is Nisada; from this one the fourth (shruti) is
Sadja. In the Madhyamagrama, Pañchama is the third (shruti) from Madhyama.
It appears from above that the ancient system conceived swara not merely as a
sound of ﬁxed pitch position, but as the entire tonal range between itself and its
previous swara. Though this interval can be theoretically divided into inﬁnitesimal
parts, they believed that only a limited number not exceeding four sounds could be
distinctly cognized by the ear in a swara-interval. These cognizable sounds are
known as shrutis and the interval, which separated one swara from the next, was
measured in terms of shrutis. Shruti was thought of both as the least audible interval
between two sounds, as well as the sounds themselves, which were separated, by
such an interval. The total number of shrutis was ﬁxed unambiguously at 22 since
the time of natyasastra. However, the two different distributions of shruti for swara
intervals for different gramas were reported in the ancient treaties. A measure of a
shruti, considering it as an interval, is also prescribed in ancient treatise. There is no
indication that the ancient authors considered shruti to be a signiﬁcant concept in
performance. At least one author speciﬁcally states that it is needed for the purpose
of classiﬁcation.
Though the ancient treatises do not reveal anything about the objective measure
of swaras, some interval ratios can be built up using the hints in Nātyashāstra. For
example Bharata has talked about consonant intervals of nine and thirteen shrutis.
We know that most consonant intervals are 4/3 and 3/2. Let us consider 4/3 as a
nine-shruti interval and 3/2 as the thirteen-shruti intervals. Noting the shruti dis-
tribution 3, 2, 4, 4, 3, 2 and 4 for swaras Sa, Ri, Ga, Ma, Pa, Dha and Ni
respectively some reasonable ratio intervals may be derived for all notes except Ri
and Dha (Fig. 3.1). Let us assume Ma and Pa to be 4/3 and 3/2 respectively. Then
as Ni is nine-shruti interval from Ma its ratio would be 16/9. Then Ga would be half
of 16/9  4/3 i.e., 32/27. It is also obvious that a four-shruti interval would be 3/2
 4/3 = 9/8. Ri being only 3 shruti up from Sa we have to assume a suitable
consonant ratio less than 9/8 for a three-shruti interval. 10/9 is such a ratio, which is
about 22 cents below 9/8. If we take Ri to be 10/9 then Dha becomes
10/9  3/2 = 5/3.
Fig. 3.1 Construction of shruti intervals following Bharata
40
3
Scales and Shruti Concept

Experiment of Sarangadeva with two veenas (Fox-Strangeways 1914):
Two veenas with twenty-two strings were used. The strings of the one veena were
so tuned that each of the string was tuned at slightly higher frequency from the
previous one so that the note was just differentiable. The fourth string would be
tuned to Sa, 7th to Ri, 9th to Ga, 13th to Ma 17th to Pa, 20th to dha and 22nd or the
last string to Ni. All the strings of the other veena were also tuned in unison with the
ﬁrst one. The ﬁrst one was the ﬁxed veena. The second was the variable one. The
strings of the variable veena were retuned in each successive step so that each time
the variable veena strings were tuned to one string higher of the ﬁxed veena.
Figure 3.2 schematically represents the experiment of Sarangadeva.
On the ﬁrst movement none of the same two strings of the two veenas when
sounded together would produce consonant sounds. On the second movement only
7th and 20th strings when played simultaneously would produce harmonious
sounds. This showed that Ga and Ni had two shrutis each. At the third movement
only 4th and 17th strings would produce consonant sounds indicating that Ri and
Dha have three shrutis each. On the ninth movement the 9th, 13th and 22nd strings
would produce consonant sounds showing that Ma, Pa and Sa had four shrutis each.
The tuning of the twenty strings of the two veenas indicated that Sarangadeva
considered shrutis to be equal in measure.
3.3
Modern Period
Although the awareness of discrepancy between the ancient treatises and contem-
porary practices had been growing at least from the mid-17th century, it was only
the latter half of the twentieth century that theories of intonation based on empirical
research emerged. The western bias for discreet pitches is not only fed by the
prominence of keyboard instruments, it also has a long history of obsession with
ratios. From Pythagoras to Helmholtz, western thinking about melody is marked by
a reduction of the continuous tonal space into a limited series of points. Although
Indian music uses a similar abstraction by deﬁning seven notes, twelve semi-tones
and twenty two shrutis, it must be stressed that since from early times these may
have been considered regions rather than points.
Fig. 3.2 Schematic representation of the experiment of Sarangadeva
3.2
Ancient Period
41

3.3.1
Divisive Theory
Fox Strangeways (Kolinski 1961) in 1914 hypothesized that shrutis evolved from
various combinations of the intervals of just intonation. He believes that shrutis are
of three different sizes, namely, Bharata’s pramana shruti of 22 cents (81/80), small
semitone of 24 cents (25/24) and Pythagorean limma 90 cents (256/243).
3.3.2
Cyclic Theory
Kolinsky (Bake 1957), one of the main proponents of the cyclic theory, projects a
series of ascending ﬁfths and descending fourths into a single octave, forming what
he calls the quintal chromatic scale. He describes the four shrutis as a cycle of
differences of apotome (114 cents) and limma (90 cents). The four shrutis are
respectively, limma, apotome, (apotome—limma) and limma—(apotome—limma).
3.3.3
Vedic Theory
Bake (Jairazbhoy 1975) and Jairazbhoy (Deval 1910) believe that the shrutis were
considered by Bharata to have been equal in size. He believes that 22 shrutis cannot
be equal in practice assuming that the concept of consonant fourths and ﬁfths was
recognized.
K.B. Deval (Clements 1912)
Using a dichord in a series of acoustical experiments, Deval supposedly measured
the pitches sung by well-known vocalists, obtaining frequency ratios for intervals
used by these performers. A summary of his ﬁndings is that a 2-shruti interval
makes a just semi-tone (112 cents), a 3 shruti-interval a minor tone (182 cents) and
4 shruti-interval a major tone (204 cents). He agreed to the ancient division of the
shuddha scale namely 3, 2, 4, 4, 3, 2, and 4.
Earnest Clements (Clements 1912)
Clements states that his own measurements of many North Indian ragas have
matched the corresponding intervals on Deval’s shruti harmonium. Like Deval,
Clements unfortunately provides no details or data corresponding to pitch mea-
surements. He proposed eight different shruti sizes: 20, 22, 27, 63, 70, 85, 90 and
92 cents.
Alan Danielou (Daniloue 1979)
He believes the pramana shruti of Bharata to be the comma 81/80 (22 cents)
(Danielou 1949). He does not believe that the ancient 22 shrutis are all equal
(Danielou 1943). He constructed 53 shrutis through an elaborate process of
42
3
Scales and Shruti Concept

considering seven pitch series. He believes that the modern singers and instru-
mentalists are perfectly consistent in their use of 53 shrutis (Danielou 1969).
Danielou states that all his conclusions concerning the role of shrutis in perfor-
mances are based on laboratory pitch measurements. However, he provides no
details concerning his laboratory procedure or data.
H.V. Modak (Modak 1967a, b)
He assumes all 22 shrutis of Bharata were exactly equal in size.
Van der Meer and Rao Suvarnalata (Rao and Van der Meer 2004)
They hold that “even when the note is steady for a moment the various measured
pitch levels do not conform to the pitch schemes that have been developed by a
number of scholars who tried to equate Bharata’s system with the major (cha-
tushruti) and minor (trishruti) whole tone scheme…. As such, the idea that the
number of shrutis is inﬁnite seems to be the only correct description.” On the other
hand, explaining contemporary intonation on the basis of the ancient 22-shruti
systems was shown to be a meaningless endeavour by Van der Meer (2000).
Jairazbhoy and Stone (Jairazbhoy and Stone 1963)
They used the modern methods of measuring pitches and came to completely reject
the very notion of shrutis playing a role in intonation. Their ﬁndings were con-
ﬁrmed by Jairazbhoy’s disciple Levy who performed more experiments. ……ex-
plaining contemporary intonation on the basis of the ancient 22 shruti system was
shown to be a meaningless endeavour (Van der Meer 2000).
To cite Ranade (1957), “…one need not be surprised if such [sound curves]
photographs reveal that the so-called ﬁxed notes of a raga scale either develop
enharmonic forms or often oscillate between certain pitch limits and undergo many
other changes, in the course of the different stages of the development of one and
the same raga.”
Mark Levy (Levy 1982)
In 1976–77 he conducted laboratory experiment by measuring pitch from recorded
performances of well-known north Indian classical singers. He has used an
instrument called Strobotuner for ﬁnding pitch in cents. Somewhat detail descrip-
tion of his experiment is available. According to the author reliable pitch mea-
surements can be made only with note duration of at least 500 ms, the reliability
being ±4 cents. It seems that the laboratory data reﬂects mean values of pitch for a
relatively long period. His data consisted of pitch measurements of 10 performances
of very well known Hindustani singers on raga marwa, malkaus, darbari kannada,
bagesri, todi and abhogi. The mean pitch values for different examples are given
only for the twelve notes. No serious attempt is noticed for searching shruti posi-
tions. All Pitch data has been pulled with respect to the twelve note positions.
Comments of Levy: “A widening gap, however, becomes evident between the old
theory and constantly changing musical practices. Shruti differentiation seems to
lose meaning they may have once had, and a system of twelve semitones gradually
3.3
Modern Period
43

emerges. All attempts to relate the ancient shruti system to later practice appeared
contrived and unconvincing.”
Narendra Kumar Bose (Bose 1960)
He considers that in the old Sanskrit treaties the shrutis theoretically divided the
octave into twenty-two equal parts. He describes the ancient “scale of origin” as a
diatonic major scale in just intonation. He believes that the shrutis were primarily of
theoretical interest only and there is very little evidence of its practical use.
According to him the shrutis gradually lost their signiﬁcance due to the inﬂuence of
foreign musical elements.
O. Goswami (Goswami 1957)
According to Goswami, Bharata’s pramana shruti evolved from intonational dif-
ferences between vocal and instrumental music. According to him the vocal music
during Bharata’s time was in Pythagorian tuning and instrumental music (based on
simple divisions of a vibrating string) in just tuning. He agrees with Danielou with
respect to the existence of 66 shrutis and the ability of singers to reproduce them
accurately in performance. According to him accurate manipulation of the shrutis
makes all the difference between good and bad Indian music and also between one
raga and another.
It seems that the views in the modern period ranges from complete negation of
practical relevance or even existence of shrutis to the existence of different numbers
of them. However, the majority of authors on shruti adopt the view that in con-
temporary music shrutis exist and form the basis of the twelve notes, but there is
little agreement on the distribution particularly the distribution referred to in
Bharata’s Nātyashāstra.
3.4
Musical Scale
Sensation of pitch, which corresponds primarily to that of fundamental frequency of
vibration, is the most important property of musical sounds. In a sense the whole
audible range of pitch is available for musical expression and it is therefore nec-
essary to have a scale suited to a particular musical system for interpreting, ana-
lyzing as well as expressing musical ideas. Like all other musical systems octave
(pitch ratio 2:1) is the most fundamental unit in the Indian system. It also happens to
be the most consistent interval (keeping aside the small amount of perceptual
stretching of octave under certain constraints) (Datta 1989). It may be noted here
that in the Indian system the octave is known as Saptak as it contains only seven
major expressive intervals, called Swaras. This is so because the Indian system
implicitly relies upon the intervals not on the boundaries (the pitch representing the
so called Musical Notes) of the major intervals. Again the need of expression as
well as appreciation of Indian Music requires smaller standard intervals. A very
long period of development of Indian music (over 4 millennia) (Datta et al. 1983)
44
3
Scales and Shruti Concept

gave rise to a unique scale seemingly based on a large number of basic microtonal
intervals called shrutis. Thus a musician can choose any of the shrutis, which
supports the interval when he is required to use a swara.
According to ancient deﬁnition shruti is perceived as an interval of sound (Sastri
Sambasiva 1930; Coomarswamy 1935). Again the music is similarly deﬁned as a
particular arrangement of sounds, which is pleasing. Thus following the ancients,
shruti may be deﬁned as the separately identiﬁable intervals of pleasing sounds. The
swara is understood by the expression created in the mind of the listener through the
resonance of a perceived shruti. As we have already noticed in Sect. 3.3, there had
been various opinions about the number (66, 53) (Coomarswamy 1935) of shrutis
during the span of development. In recent times it seems that the number of shrutis
is broadly agreed upon as 22 (Tagore 1874). Table 3.2, presented later, summarizes
the present position. It is impossible to sing a complete scale of shrutis in suc-
cession but they are said to be sung with perfect accuracy when they are embodied
in expressive scales. However there still exists some controversy over the numbers
and the exact ratios of the shruti intervals supporting each swara.
All shrutis are by no means equal. As it is reported earlier according to the size
of the intervals three different shrutis are mentioned Pramana, Nyuna and Purna
shruti (Sarangadeva 1897). There is confusion regarding the measure of these
intervals. There have been attempts to use western intervals like comma,
Pythagorean limma etc. (Pandit Abhola; Sarangadeva 1897) to measure them. It is
already discussed earlier that though very sophisticated equipments were available,
serious systematic experimental study of these intervals from recorded songs of
established musicians for resolving this vexed problem are taken up only recently.
The usage of shrutis is plentiful in the practice of North Indian raga music. All
ragas are broadly categorized into ten ‘thaat’s (Lentz 1961) which are actually
common scales for the corresponding groups. Shrutis are again grouped into ﬁve
families (Jati) (Somnath 1910) namely, Dipta, Mridu, Ayata, Madhya and Karuna
according to the feelings they express. A broad correlation is thus expected between
the ‘thaat’ and the ‘jati’ as the ragas depict feelings through the expressions
embodied in the shrutis.
A very simple experiment (Datta 1989) conducted in the scientiﬁc department of
ITCSRA primarily for seeing deformation of vowel in singing at the limits of the
range of a singer in Hindustani music provided an interesting insight to shrutis.
Eight senior research scholars of music were asked to sing vowel /a/, /e/ and /o/
consecutively both in ascending and descending order on all notes for the full
stretch possible for each of them (in the accompanying environment of tanpura
drone). Pitches are determined at the steady states of the rendered note for each
singer for each vowel and were divided by the frequency of the tonic. All such
ratios are then folded back onto the middle octave. The continuous curve in Fig. 3.3
presents the frequency distribution of the data pooled for all singers. The vertical
axis represents the frequency of occurrence for the ratios of the steady states rep-
resented by the horizontal axis. The small well-separated sharp peaks (represented
by red arrows) indicate discrete systematic selection of frequency by the singers.
They are also well separated.
3.4
Musical Scale
45

It may be observed that the note positions in the octave represented by green bars
correspond to relatively prominent peaks. In between these there are also small
peaks indicating that the singers has also used these frequency positions quite often.
Had these been caused by the random straying of the singers from note positions
these would present a wider bell shaped curve. The sharp nature of the peaks
indicates some rationale behind the choice of the relevant frequency. It is note-
worthy that the number of these peaks happens to be 22.
Let us now consider the ﬁne structure presented by these smaller peaks. Blue
arrows give the shruti positions calculated on the basis of the ratios given by Lentz
(Sarangadeva 1897). It may be observed that fourteen of the twenty-two signiﬁcant
peaks indicated by the arrows at the bottom are almost in full agreement with the
deﬁned shruti positions.
The aforesaid ﬁnding of shrutis from analysis of plain rendering of notes form an
experiment with an objective quite different from investigating shrutis is quite
surprising. It is generally held that shrutis are not manifested while doing plain
sargams, they manifest themselves only in raga performance. Unfortunately this
data revealed otherwise. However one explanation may emerge from the fact that
perception of pitch in music mode is categorical (Whitﬁeld 1970). The learned
categories are the intervals of seven pure notes and the ﬁve altered notes. As the
notes are intervals not ﬁxed positions the singers are free to choose any frequency
for a note in a particular interval according to his/her own perception of pleas-
antness. There is really no obligation to sing an exact frequency dictated by the
so-called ratios.
3.4.1
Objective Modeling of Musical Scale
The Indian musical system is modal where relations between successive sounds as
well as those between any sound and a ﬁxed tonic are of paramount importance.
Fig. 3.3 Distribution of fundamental frequency in sung vowels
46
3
Scales and Shruti Concept

Even in the absence of the tonic, an isolated sound can convey an expression, of
course, with respect to a memorized tonic or a previous sound. Here each sound
leaves an impression in the mind and the idea is revealed through the cumulative
effect of successive sounds (Goswami 1957). Furthermore, the tonic is not ﬁxed in
terms of a set of speciﬁed frequencies of vibration but can be deﬁned by the
performer anywhere according to his/her own choice.
The basis of scale in the Indian musical system, therefore, has to be considered
in terms of the relation between successive sounds and the tonic. Music is a phe-
nomenon in the ideational world and hence has its primary basis in the psychology
of perception of sound. Though the ancient Indian approach for understanding and
production of it was metaphysical and was given the aura of divinity, it was not
completely devoid of attempts to relate it to the keen sense of perception along with
some real experiments. In the last few decades there have been considerable
advances in the psychophysiology of hearing. Experimental data on the pitch dis-
crimination and in human perception are available. There had been theories, backed
by experimental data, on the perception of pitch, timbre and loudness and on the
consonance/dissonance phenomena as well as musicality of sound. There is a need
to examine the Indian Musical scale, which is said to be primarily based on
‘shrutis’, in the light of these advances keeping in mind its modal nature.
The Indian musical system, like all other old ones, has an ancient basis that is
primarily cognitive. Though some measurements are reported on the basis of
comparison of string lengths these are of quite recent origin (17th century A.D.)
(Saraswati 1941). These are believed by the Indian theorists to be less accurate and
practical than the perception on the feelings (Abhola 1884). There exists a physical
reality of music i.e. the reality of the physical phenomena, which is interpreted by
the mind as music; it is transformed into ideational entities by the perceptual and
cognitive mechanism, which often appears arbitrary (Daniloue 1979). A sound
theory must, therefore, take into account these properties of transformation while
interpreting an ideational function in terms of material processes.
3.4.2
Relevant Psycho-Perceptual Concepts
It is suggested that the perception of complex tones begins at a later stage of
development of the foetus. The ﬂuid in which a fetus is immersed transmits the
throbbing of the mother’s heart, activating continuously the auditory processes. The
concept of partials is developed at that stage. It is an experimental fact that when
musical notes, which are in simple, integral ratios, are sounded together they sound
pleasant. This does not always hold true for pure tones (Sundberg 1994). One theory
is that when some upper partials of the two tones match consonance results. In fact,
Helmholtz (Plomp and Levett 1965) in 1862 explained the dimension of consonance
in terms of the coincidence and proximity of the overtones and difference tones.
These tones arise when simultaneously sounded complex tones excite real non-linear
physical resonators, e.g. the human ear. To the extent that an interval’s most
3.4
Musical Scale
47

powerful secondary tones exactly coincide, it is consonant or sweet sounding. To the
extent that any of its secondary tones are separated in frequency by a small enough
difference to “beat” at a rate, which he put at around 33 c/s, it is dissonant, or harsh.
Same effects are observed even when these two sounds are not simultaneous but in
succession. A complex wave would give rise to a series of active groups of nerve
ﬁbres separated by groups of inactive ﬁbres. The active ﬁbres correspond to partials
(Somnath 1910). Recent studies revealed prints of harmonic structures in the audi-
tory cortex A1 (Helmholtz 1862). The other theory of preference of pair of tones
relies on the similarity of the temporal patterns of neural discharge for tones having
simple frequency ratios (Wang and Shamma 1995).
The average human ear has a frequency range approximately from 10 to 20 kHz.
Of this region the ear is very sensitive up to 5 kHz. The range of frequency over
which the human voice spectra has relevance is approximately up to 8 kHz.
Figure 3.4 presents the Difference Limens (DL) against frequency of the tone. The
parameter of the tone pulses is duration (in milliseconds). It may be seen that the ear
is most sensitive to frequency discrimination, generally referred to as DL, near
2 kHz. There is a sudden decrease in the discrimination capability in the region 5–
6.3 kHz. The upper partials play an important role both in pitch perception and
musicality of sequence of notes (Bloomsliter and Creel 1961). Assuming the
average fundamental for a female singer at 200 Hz, up to about 30th partials would
fall on the sensitive region of the ear. Also in the slow tempo (vilambit laya) the
minimum duration is, in general, around 200 ms DL is around 0.3 Hz or a fun-
damental of around 100 Hz under very controlled experimental conditions. This
may be referred to as discrimination percentage (DP) of 0.003. DP is DL expressed
in percentage of the pitch of the higher tone, Moreover near the end of the most
sensitive region at 5 kHz the DL is about 22 Hz. About 50th partial for the same
note would fall in this region and this will be reﬂected as a DP of about 0.004 for
Fig. 3.4 Relative frequency
DLs, Df/f, as a function of
frequency
48
3
Scales and Shruti Concept

the fundamental. This means that notes separated by a lower DP would have
partials in this region which is likely to be non separable and therefore cause beats
resulting in unpleasantness. The DP for a fundamental of 150 Hz is lower than that
of 100 Hz. It is the 33rd partial for this frequency that falls near 5 kHz and the DP
at 150 Hz would approximately 0.007. Including some margin, in consideration of
the departure from the experimental laboratory listening environment, the values of
DP in normal public auditoriums may be taken as 0.005 between 100 and 150 Hz
and 0.0075 between 150 and 200 Hz for considering the psycho-physical
hypothesis below for explaining scales and shrutis.
3.4.3
Hypothesis
The hypothesis postulated here for explaining the shrutis and the consequent Indian
musical scale uses the two signiﬁcant virtues that emerge from the deﬁnition of
shrutis reported in Sect. 3.2. These are:
(1) Shrutis are perceptually differentiable and
(2) They are pleasant.
Two more assumptions which need to be made for a good scale system may
further be included:
(3) Shrutis should be distributed over the saptak as evenly as possible and
(4) They should be pleasant among themselves as numerously as possible.
The foundation for (1) has already been laid in the last section. As for (2) it was
seen in the same section that the pleasantness of two sounds requires small, integral
ratios. To answer the question how small it should be, we might fall back upon the
consonance theory, which requires some of the upper partials of the two sounds to
match (see Javel 1980 for neurological decoding of signals). Now if the funda-
mental of the two sounds are in the ratio m/n where m and n are prime to each other
and m > n then the ﬁrst matching would be for the mth partial of the lower note
with the nth partial of the higher note. Taking 100 Hz as the tonic at best only two
partials of the pair of tones would match within the sensitive range of the human ear
if m lies between 25 and 30. We shall see in the next section that 27 may be taken
as the maximum limit for m.
As for (3) the requirement of shruti for Indian music is two-fold. First it allows
the musician to express various feelings and for that he would need maneuverability
in all the major intervals. Second a large space devoid of any shruti would be
useless as far as expression is concerned. At the same time an overcrowding would
be wasteful. It may be seen from Table 3.2 below that the present distribution of
shruti follows this evenness.
The fourth assumption is made with the idea that because of the melodic nature
of Indian music, consecutive notes could be almost any pair of the twelve notes.
3.4
Musical Scale
49

A shruti, therefore, would be good if it produces pleasantness with a large number
of other shrutis.
3.4.4
Construction of Shrutis from Hypothesis
It has been observed in the earlier sections that the ratios representing shrutis need
be small simple fractions. A computation of all ratios m/n for m  30 and m > n
shows that there exist only 137 different ratios (barring the ratio 1:1). These only are
the possible candidates for shrutis. The notes corresponding to all of these (with
100 Hz as the tonic) are not perceptually separable. The ratios of each pair of these
candidates, 18,906 in number, are computed to the form m/n using a special
numerical algorithm that can directly and arithmetically manipulate ratios in this
form. These form a symmetric square matrix of order 137. For each column the
number of ratios for which m  30 is then computed. Only such ratios are pleasant
according to aforesaid theory of consonance. This number divided by the rank of
the matrix shall be referred to as the measure of acceptability (MA) for the can-
didate represented by the column. MA = 1 for a particular element in the matrix
indicates that it has m  30 when its ratios with all other elements are computed in
the form m/n. These ratios, therefore, according to the present hypothesis, are
pleasant in the immediate neighborhood of all other ratios to the degree indicated by
the MA value. Thus MA for a ratio can be used to determine how extensively it can
produce a pleasant pair in combination with the other ratios of the set. For this set of
137 ratios the highest value of MA is found to be 0.526 for the ratio 3/2. Figure 3.5
represents MA against these 137 ratios.
One interesting point to note from the Fig. 3.5 is that, in general, not only do the
ratios representing the twelve notes have MA larger than the surrounding ones but
they also appear to have some free spaces on both the sides. They seem to stand
distinctly apart.
As has been pointed out already, all these 137 ratios are not perceptually dif-
ferentiable. The next task is to ﬁnd out the subset of these pleasant ratios that are
perceptually separable. To avoid well acceptable notes being ﬁltered out it is
Fig. 3.5 MA for all the 137 ratios
50
3
Scales and Shruti Concept

decided to select all ratios having MA  0.15 ﬁrst and then use DP in the
descending order from the already selected ratios at the end of the list and work
upwards. This process of ﬁltering out is done through computer programming. The
resulting selection indicated clearly that the upper value of m could be ﬁxed at 27
without any signiﬁcant adverse effect. Table 3.1 shows a list of all ratios (after just
one ﬁltering) having m  27 and hence consonant with the tonic. Their number is
65 and including the tonic this number fully conforms to the number of shrutis
referred to in some literature. Column 4 gives the value of each interval in cents.
The perceptually differentiable ratios are marked with an asterisk against the serial
number. These ratios are also pleasant in conjunction with other ratios to the extent
indicated by the corresponding MA given in column 3.
Figure 3.6 presents three different sets of shrutis obtained through the iterations
described above along with the corresponding MAs. The distribution of the 66
shrutis reveals clusters separated by vacant regions. It is interesting to note that the
bars representing the 22 shruti positions are placed in these vacant areas. One may
notice that MA generally increases as the number of shrutis decrease. In fact the
MA for 22 shrutis show a jump compared to those for 53 shrutis.
Let us consider the distribution of these shrutis in the 12 major intervals of the
octave. One may notice from Fig. 3.6 that the shrutis are more crowded towards the
beginning. While rejecting ratios care should be taken to do that ﬁrst from the
region, where they are more crowded. Obviously the intervals with least MA should
be rejected ﬁrst. It may be noted here that MA refers to the acceptability only with
reference to the ratios in the current set. Therefore if a large number of notes are
discarded at a single step there is a risk of those ratios being discarded, which are
more acceptable with respect to the new set. So, for each iteration an appropriate
threshold is selected such that not more than 10% of these ratios are ﬁltered out in
forming a subset. After only two iterations the number of shrutis was reduced to 53
(including the tonic). These are marked with asterisks against the serial number in
Table 3.1. This number conforms again with some ancient literatures. The distri-
bution with respect to the note intervals is now slightly better (3, 5, 5, 6, 5, 5, 3, 5,
4, 4, 5, 2). If this iteration is further continued 8 times we get the shrutis given in
bold ratios in the table. The numbers of these are 22 including the tonic, which
conforms to the number of shrutis at present.
Table 3.2 gives the ratios of these 22 shrutis along with their values in cent and
the MAs. Some of the ratios available in literature are also listed here for com-
parison. In the last row the root mean square error of these ratios from the
hypothesis is given. It may be seen that the ratios given by Nagoji Row, comes
closest to those predicted from the hypothesis followed by those given by Lentz and
Deval in the order of closeness (Datta and Kundu 1991).
The predicted shrutis which are within an error of one cent from one of the
available ratios are atikomal and tivra rishava, komal and shuddha gandhar,
shuddha maddhyam, panchama, komal and suddhya dhaibat, komal and shuddha
nishad. It is to be noted that all notes except komal rishava and tivra maddhyam are
predicted accurately. The shrutis for which error just exceeds 20 cents are komal
rishav, atikomal gandhar, tivra gandhar, ekashruti and tivratara madhyam, tivra
3.4
Musical Scale
51

Table 3.1 Set of 66 shrutis
Sr. No.
Ratio
Cents
MA
♪
♫
Sr. No.
Ratio
Cents
MA
♪
♫
1*
24/23
73.7
0.15
33*
4/3
498.0
0.54
¶¶
M
2*
21/20
84.5
0.22
¶¶
34
23/17
523.3
0.14
A
3*
19/18
93.6
0.2
35*
15.11
536.9
0.23
D
4
17/16
104.9
0.15
36*
11/8
551.3
0.18
D
5*
15/14
119.4
0.26
37*
25/18
568.7
0.28
¶
H
6
14/13
128.3
0.15
38*
7/5
582.5
0.32
S
Y
7*
13/12
138.6
0.25
¶
R
39*
24/17
596.9
0.18
A
8*
12/11
150.6
0.23
I
40
27/19
608.3
0.14
M
9*
11/10
165.0
0.22
S
41*
10/7
617.4
0.32
A
10*
10/9
182.4
0.29
S
H
42*
13/9
636.6
0.28
#
11
19/17
192.5
0.12
A
43
19/13
656.9
0.14
12*
9/8
203.9
0.26
#
V
44*
25/17
667.6
0.17
13*
25/22
221.3
0.18
A
45*
3/2
701.9
0.17
PA
14*
8/7
231.2
0.31
46*
23/15
740.0
0.22
15
23/20
241.9
0.17
47*
17/11
753.6
0.17
16
22/19
253.8
0.09
48*
14/9
764.9
0.26
¶¶
D
17*
7/6
266.9
0.30
¶¶
G
49*
19/12
795.5
0.20
H
18
27/23
277.6
0.12
A
50*
8/5
813.6
0.38
¶
I
19*
13/11
289.2
0.22
N
51*
13/8
840.6
0.23
B
20*
25/21
301.8
0.22
D
52*
23/14
859.4
0.25
A
21*
6/5
315.6
0.43
¶
H
53*
5/3
884.3
0.55
S
T
22*
17/14
336.1
0.23
A
54*
17/10
918.6
0.23
23*
11/9
347.4
0.22
R
55*
12/7
933.1
0.32
24
16/13
359.4
0.12
A
56*
26/15
952.2
0.26
#
25*
26/21
369.7
0.22
57*
7/4
968.8
0.34
¶¶
N
26*
5/4
386.3
0.45
S
58
23/13
987.7
0.15
I
27*
19/15
409.2
0.2
59*
25/14
1003.7
0.29
S
28*
23/18
424.3
0.22
60*
9/5
1017.5
0.40
¶
H
29*
9/7
435.1
0.28
61*
20/11
1034.9
0.22
A
30*
13/10
454.2
0.28
#
62*
13/7
1071.6
0.29
D
31
17/13
464.4
0.14
63*
15/8
1088.2
0.32
S
A
32
25/19
475.1
0.15
64*
21/11
1119.4
0.23
#
65*
27/14
1137.0
0.23
Legends:
¶¶ Atikomal (shuddha for Madhyama)
¶ Komal (ekashruti for Madhyama)
S Shuddha (tivra for Madhyama)
# Tivra (tivratara for Madhyama)
♫Swara
♪Shruti
52
3
Scales and Shruti Concept

dhaibat and atikomal nishad. Compared to 22 shrutis given in the list the predicted
ones are almost same for 10 and quite close to 5 ratios. Only in seven cases
discrepancies are signiﬁcant.
Table 3.3 gives a distribution of the length of the predicted shrutis. The smallest
shruti is about 14 cents and the largest is 85 cents. These values may be compared
with the measure of pramana shruti (70 cents), nyuna shruti (22 cents) and purna
shruti (90 cents) as given in western literatures (Sarangadeva 1897). It may be noted
here that the shruti intervals are basically unequal. The largest number of shrutis
covered by a single length, i.e. 70.7 cents is only 4. This gives some justiﬁcation for
referring this shruti as pramana or standard in the ancient literatures.
Table 3.4 at the end gives the pleasantness matrix wherein an asterisk (*)
indicates that the pair of shrutis represented by the column and the row would
sound pleasant in succession. These have ratios of frequencies with m  27. The
positions marked with ‘!’ have ratios with m  100 and are considered to sound
positively unpleasant in succession. The positions marked with ‘*’ have m lying
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
100
200
300
400
500
600
Ratio in cents
MA
66
53
22
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
600
700
800
900
1000
1100
1200
Ratio in cents
MA
66
53
22
Fig. 3.6 Three sets of shrutis generated by algorithm
3.4
Musical Scale
53

between 27 and 36. They are expected to sound pleasant in vibrato. The vibrato is
known to reduce unpleasantness because it prevents the sensation of beats caused
by close proximity of some of the partials of the two different tones (Pandither
1984). The following points may be noted from this table—Pa has the highest MA
closely followed successively by Dha, Ma and ga, indicating that these notes would
sound pleasant in the context of most of the other shrutis. Among the swaras, komal
Dhaibat has the lowest MA.
Table 3.2 Comparison of derived and existing shrutis
Shruti name
Present hypothesis
Lentz
Deval
Nagogji row
Ratio
MA
Cents
Cents
Cents
Cents
Sadja
1
1.00
0
0
0
0
Atikomal Rishava
21/20
0.59
84.5
89.4
84.5
70.7
Komal Rishava
13/12
0.55
138.6
112.3
111.8
111.8
Shuddha Rishava
10/9
0.55
182.4
182.2
182.5
182.5
Tivra Rishava
9/8
0.45
203.9
203.9
204
204
Atikomal gandhar
7/6
0.68
266.9
293.9
294.2
294.2
Komal gandhar
6/5
0.68
315.6
315.6
315.7
315.7
Shuddha gandhar
5/4
0.68
386.3
386.3
386.4
386.4
Tivra gandhar
13/10
0.55
454.2
408.3
407.9
427.4
Shuddha madhyam
4/3
0.77
498.0
497.6
470.8
498.1
Ekashruti madhyam
25/18
0.45
568.7
519.5
498.1
519.6
Tivra madhyam
7/5
0.68
582.5
589.9
590.3
590.3
Tivratara madhyam
13/9
0.55
636.6
609.5
609.8
631.3
Panchama
3/2
0.91
701.9
702.0
702
702
Atikomal dhaibat
14/9
0.45
764.9
791.9
786.5
772.7
Komal dhaibat
8/5
0.45
813.6
813.7
813.7
813.7
Shuddha dhaibat
5/3
0.82
884.3
884.7
884.4
884.4
Tivra dhaibat
26/15
0.50
952.2
906.4
905.9
905.9
Atikomal nishad
7/4
0.64
968.8
996.3
996.1
996.1
Komal nishad
9/5
0.59
1017.5
1017.6
1017.6
1017.6
Shuddha nishad
15/8
0.55
1088.2
1088.3
1088.3
1088.3
Tivra nishad
21/11
0.36
1119.4
1109.4
1109.8
1129.4
Error from hypothesis (in cent)
21.789
24.825
17.854
Table 3.3 Measure of shrutis
Cents
Number
<20
2
20–40
2
40–60
7
60–80
9
>80
1
54
3
Scales and Shruti Concept

Table 3.4 Pleasantness matrix
S
R1
R2
R3
R4
G1
G2
G3
G4
M1
M2
M3
M4
P
D1
D2
D3
D4
N1
N2
N3
N4
MA
S
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
1.00
R1
*
!
*
*
*
*
*
!
*
!
*
*
!
!
*
*
*
*
0.59
R2
*
*
*
*
*
*
*
*
*
*
*
!
!
!
0.55
R3
*
*
*
*
!
*
*
*
*
*
*
*
*
*
!
0.55
R4
*
*
*
*
*
!
!
*
!
!
*
*
*
0.50
G1
*
*
*
*
*
*
*
*
*
*
*
*
*
0.68
G2
*
*
*
*
!
*
*
*
*
*
*
*
*
*
*
0.73
G3
*
*
*
*
*
*
*
*
!
*
*
*
0.68
G4
*
!
*
*
*
!
*
*
*
*
!
0.55
M1
*
*
*
*
*
*
*
*
*
*
0.73
M2
*
!
*
*
*
!
*
!
!
*
!
0.45
M3
*
*
*
*
*
*
*
*
*
0.68
M4
*
*
*
*
*
!
!
0.55
P
*
*
*
*
*
*
*
*
0.91
D1
*
*
*
*
*
!
*
0.45
D2
*
*
*
*
*
!
0.45
D3
*
*
*
*
*
0.82
D4
*
!
*
!
!
0.50
N1
*
*
*
*
0.64
N2
*
*
*
0.55
N3
*
0.55
N4
*
0.36
3.4
Musical Scale
55

3.4.5
Conclusion
Views on shrutis expressed in musicological literatures are many and ambiguous.
While attempts of mathematical formulation to ﬁt musical scales have been noticed
since the era of Greek civilization a rational objective basis for the scale has been
elusive. The general universality observed in the frequency ratios obtained in scales
evolved in musical systems developed independently in different cultures suggested
psycho-physical causes behind this. It is only late in 20th century that a psycho-
perceptual model has come out. The simplicity of this model based on psycho-
perceptual phenomena of consonance and differentiability appears to be attractive.
The shruti positions arrived at through the hypothesis seems to be quite satisfactory
as a large number of them are quite close to the empirical scales suggested by some
eminent musicologists. The structure of shrutis clearly indicated the position of
notes with the corresponding measures of interval completely in consonance of the
generally accepted values. We shall see later in Chap. 4 that the positions are also
corroborated from the analysis of large volume of recorded contemporary perfor-
mances on classical Hindustani music.
References
Abhola P (1884) Sangita Parijata. Kalivara Vedanta-vagisa (ed), Calcutta edition
Bake AA (1957) Bharatas experiments with two Vinas. Bull Sch Orient Afr Stud 20:61–67
Bloomsliter P, Creel W (1961) The long pattern hypothesis in harmony and hearing. J Music
Theory 5:2–31
Bose NK (1960) Melodic types of Hindustan. Jaico Publishing House, Bombay
Chowdhuri L, Datta AK (1998) Consonance between physics and philosophy regarding nature and
propagation of sound. J Acoust Soc India XXVI(3–4):508–513
Clements E (1912) Introduction to the study of Indian music. Kitab Mahal, Allahabad
Coomarswamy AK (1935) The transformation of nature in arts. Harvard University Press,
Cambridge
Danielou A (1943) Introduction to the study of musical scales. The India Society, London
Danielou A (1949) Northern Indian music. vol 1: Theory and Technique, Christopher Johnson,
London
Danielou A (1969) Northern Indian music. Frederick A, Praeger
Daniloue A (1979) Introduction to the study of musical scales. Oriential Books Reprint
Corporation, New Delhi
Datta AK (1989) Machine Emulation of Audition. J Acoust Soc India. XVII(3–4):1–9
Datta AK, Kundu R (1991) A psycho-perceptual hypothesis for Indian musical scales. Proceedings
of the International Workshop in Speech, Music and Allied Signal Processing, m31–m39,
Delhi, India, December, 1991
Datta AK, Ganguly NR, Dattamajumder DD (1983) Speech and music—a review on quantitative
studies. J Sangeet Res Acad 4(1):77–91
Deva BC (1974) Music and science. J Ind Musicol Soc 5(1):19 23
Deval KB (1910) The Hindu musical scale and the twenty-two shrutis. Ayabhushan Press, Pune
Fox-Strangeways AH (1914) The music of Hindustan, Rpt. Clarendon Press, Oxford, 1965
Goswami O (1957) The story of Indian music. Asia Publishing House, Bombay
Helmholtz H (1862) Die Lehre von dem Tonempﬁndungen. Vieweg, Braunschweig
56
3
Scales and Shruti Concept

Hiesenberg W (1974) Wolfgang Pauli’s philosophical outlook. In: Chapter 3 of Across the
frontiers. Harper and Row, New York
Jairazbhoy NA (1975) An interpretation of the twenty-two shrutis. Asian Music 6:38–59
Jairazbhoy NA, Stone AW (1963) Intonation in present-day North Indian classical music. Bull Sch
Orient Afr Stud 26:119–132
Javel E (1980) Coding of AM tones in chinchilla auditory nerves: implication for the pitch of
complex tones. JASA 68:133–166
Kavi Ramakrishna YM, Pade JS (eds) (1964) Natyasastra of Bharatamuni with Commentary of
Abhinavabharati of Abhinavagupta in the Gaekwad’s Oriental Series by Oriental Institute, vol
IV. Baroda
Kolinski M (1961) The origin of Indian twenty-two tone system. In: Kolinski M (ed) Studies in
ethnomusicology, vol 1. Folkways Records and Service Corpn., Inc., New York
Kumar C, Datta AK, Mukherjee B (1993) The song of songbirds. Environment 6(4):10–15 (Cec)
Lentz DA (1961) Tones and intervals of Hindu classical music. University of Nebraska Studies,
New Series No. 24, University of Lincoln
Levy M (1982) Intonation in North Indian music. Biblia Impex Pvt. Ltd., New Delhi
Lloyd G (2006) Encyclopedia of philosophy (Copyright 2006 Thomson Gale)
Modak HV (1967a) Propriety of dividing an octave into twenty-two shrutis. J Music Acad Madras
38:151–159
Modak HV (1967b) Septimal frequency ratios and the twenty-two shruti scale. J Music Acad
Madras 38:160–164
Nayaratna M (ed) (1889) Sabara Mimansasutrabhasya. Bibliotheca Indica, 45, Asiatic Society,
Calcutta
Pandither MARS (1984) Karunamirtha Sagaram—on shrutis. Asian Educational Services, New
Delhi
Plomp R, Levett WJM (1965) Tonal resonance critical bandwidth. JASA 38:548–560
Praganananda S (1963) A history of Indian music, vol 1. Ramkrishna Vedanta Math, Calcutta
Raman CV (1921) On some Indian stringed instrument. Proc Ind Assoc Cultiv Sci 7:29–33
Raman CV (1935) The Indian musical drums. Proc Ind Acad Sci A1:179–188
Raman CV, Kumar S (1920) Musical drums with harmonic overtones. Nature 104(2620):500
Ranade GH (1957) Eternal paradox in Indian music: the shrutis. In: Aspects of Indian music.
Publications Division, Ministry of Information and Broadcasting, New Delhi, p 33
Rao S, Van der Meer W (2004) Shruti in contemporary Hindustani music. Proc. FRSM-2004,
Annamalai University, January 8–9, 2004
Rayleigh JWS (1945) The theory of sound, vol 1. Dover Publications
Sarangadeva (1897) Sangeet Ratnakar with comments of Kalinath. Anandasram edition
Sarangadeva (1972) Sangitratnakar (trans: Suresh Chandra Bandopadhyay in Bengali). Rabindra
Bharati University
Saraswati SH (1941) Sabda and Artha. Siddhant I:45
Sastri Sambasiva K (ed) (1928) Brhaddesi of Matangamuni. Anantasayanam Samskrta Series, no.
94, Trivandrum
Sastri Sambasiva K (ed) (1930) Dattilam of Dattilamuni. Trivandrum Sanskrit Series, Trivandrum
Sastri Subrahmanya SP (ed) (1992) SangItaratnAkara of Sarngadeva with the commentaries
Kalanidhi of Kallinatha and sudhakara of simhabhupala, vol I (revised by S. Sarada). Adyar
Library Series by Adyar Library, Madras
Seal BN (1985) The positive sciences of the ancient Hindu. Motilal Banarasidass, New Delhi
Somnath (1910) Raga Vibodha (1610AD). Lahore
Sundberg J (1994) Perceptual aspect of singing. J Voice 8(2)
Tagore SM (1874) Hindu music. Hindu Patriot, Sept 7, 1874
Udyoktakara (1940) Nyaya-vartika. Bibliotheka Indica Edn., Dvivedi VP (ed). Chowkhamba,
Varanasi
Van der Meer W (2000) Theory and Practice of intonation in Hindustani Music. In: Barlow C
(ed) The ratio book. Feedback Papers, Koln, pp 50–71
References
57

Wang K, Shamma SA (1995) Spectral shape analysis in the central auditory system. IEEE Trans
Speech Audio Process 1(5):382–435
Whitﬁeld IC (1970) Central nervous processing in relation to speciﬁc temporal discrimination of
auditory patterns. In: Plomp R, Smoorenburg F (eds) Frequency analysis and periodicity
detection in hearing. Sijthoff, Leiden
Wilber K (ed) (1984) Quantum questions. Shambala Publications Inc., London
58
3
Scales and Shruti Concept

Chapter 4
Tonic Detection and Shruti Analysis
from Raga Performance
4.1
Introduction
The technological advances of modern times make it possible to have an in-depth
analysis of songs of renowned singers of North Indian Classical music to objec-
tively assess the vexing questions related to shrutis and swaras. Not only this, the
issues related to musical scales in India are many, and to say the least, not simple.
Much of the historical commentaries reported in literature (see Chap. 3) leave us as
bafﬂed as ever. Confusion becomes even more confounded by the unending
arithmetical calculations of ratios with little experimental support (Basant 1980).
The shrutis, in which, the conventional scale is subdivided, producing many
musical intervals, which may or may not (according to various opinions) corre-
spond to simple ratios (Moore 1982). Scholarly opinions differ over whether or not
there is something inherently preferable about simple frequency ratios for shrutis. In
fact, it is a common notion that individual differences and cultural backgrounds can
signiﬁcantly inﬂuence the musical combinations that are judged to be ‘pleasant’ or
otherwise. Indeed, the microtones, and the various different scales, which can be
composed from them, is believed to add considerably to the richness of the music
and to the variety of moods, which it can create.
Bhatkhande (1887) has subjectively rationalized Shruti distributions with respect
to major notes, though there has been another variant (Danielou 1943). However,
both the systems lack systematic experimental back up. A psycho-acoustical
explanation (Datta and Kundu 1991; Bharata 1929) of shrutis and to the size of the
corresponding intervals in Indian scale based on the theory of consonance or
vaditya is given in Chap. 3. The best-known Indian experiments in this ﬁeld are by
Bharata (Sarangadeva 1897), Sarangadeva (Pandit 1884) (his deﬁnition of Shruti, as
just noticeable difference in pitch), Abhola (length of strings) (Clements 1913).
Later on Deval and Clements (Tagore 1874) conducted experiments with a
Sonometer in collaboration with Ustad Abdul Karim Khan.
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_4
59

According to some theoreticians, these ratios and intervals are less accurate and
practical than the perception on the basis of feelings (Deva 1970). Factually, there
exists a physical reality of music, i.e., the reality of the physical phenomena that is
interpreted by the mind as music through a transformation by the cognitive
mechanisms. Such transformations are often arbitrary. While the feelings are the
real deciders for these ratios, the long period of stabilization of this cognitive
process in a particular system (in the present case Hindustani music) through an
interaction between the performers and the listeners is bound to put their consistent
and indelible mark on the physical reality, which is the performed music. It is
possible now to get an accurate and minute description of the pitch proﬁle from
performed music and the time associated with it. Even small micro-perturbations
can be clearly measured. Therefore, there is a need to study the exact ratios,
intervals and interval boundaries etc. used by the present-day musicians while
rendering a raga. According to Deva (Sundberg 1994), with the advent of newer
mathematics and modern scientiﬁc tools, experimental work on Shruti is an abso-
lute necessity. It is an area where the least experimental work has been done. The
position of the ratios for different shrutis with respect to the shruti intervals, i.e.,
whether the ratio indicates the beginning or end or the middle of an interval, is not
quite clear.
The ancient texts (Bhatkhande 1887) suggest that the number of shrutis holding
swaras is not equal for all swaras. There are four shrutis each on sadja, madhyama
and panchama, two each for gandhar and nishada and three each for rishabha and
dhaibata. While ancient musicologist used to place the pure notes on the last shruti
i.e., sa, re, ga, ma, pa, dha and ni on 4th, 7th, 9th, 13th, 17th, 20th and 22nd shrutis,
the comparatively modern Indian musicologists place them on the beginning i.e., on
1st, 5th, 8th, 10th, 14th, 18th and 21st respectively. Western musicologist has also
investigated the swara-shruti distribution deeply and according to their view the
distribution is 1, 4, 4, 4, 1, 4 and 4 instead of 4, 3, 2, 4, 4, 3 and 2 (Danielou 1943).
The different graphs and text used in literature to deﬁne shruti intervals and ratios
suggests that a shruti interval is represented by the ratio at its left/lower end
(Sarangadeva 1897). However, what is actually happening is worth investigation
since pitch perception in music is categorical. The position of the magic limit with
respect to which (Datta et al. 1996) the interval perception suddenly changes is also
worth investigating. The basic questions, noting that shrutis are said to be intervals,
need to be addressed are:
1. Whether shrutis have real existence? If so:
2. What is the relationship of the ratios representing shrutis with the intervals?
3. What is the distribution of these shrutis with respect to the major notes i.e., the
swaras?
4. Of the various ratios proposed for different shrutis in the literature, which one is
used in present days?
We shall endeavor to answer these questions from an objective empirical
investigation using recorded raga performances of established eminent singers of
60
4
Tonic Detection and Shruti Analysis from Raga Performance

Hindustani music. A pilot study (Datta 1996) in this vein is reported using only raga
Yaman. The methodology copiously uses signal processing as well as mathematical
modeling for achieving different goals. One important problem is the automatic
detection of the tonic.
The primary objective parameter on which the analysis for shrutis or scale rests
is the fundamental frequency which has to be extracted from actual performances.
The tonic plays an important role in determining the ratios and intervals. Large
scale investigation on recorded performances is needed to arrive at meaningful
statistically signiﬁcant resolution of the mysteries surrounding musical scale and
shrutis. For this computational techniques have to be used, even developed.
4.2
Relevant Signal Processing
The digitisation of the song signal of the recorded performances is done at the rate
of 22,050 samples/sec (16 bits/sample). Figure 4.1 shows time-displacement curve
of the raw signal of a portion of one of the songs. Necessary pre-processing of the
digitized song signals before the analysis of shrutis are: (1) extraction of pitch
contour, (2) error correction and smoothing, (3) detection of steady states repre-
senting notes and (4) detection of tonic.
4.2.1
Pitch Period Extraction from Signal
A method based on Phase-Space Analysis (PSA) (Datta 1996) is used for extracting
pitch periods. The pitch pattern ﬁles, extracted from all the 116 signal ﬁles using the
above method, are referred to as ‘.cep ﬁle’. Figure 4.2 presents a portion of a ‘cep’
ﬁle. The horizontal-axis represents the time and the vertical axis pitch in Hz. PSA
extract pitch period by period instead of average pitch for a ﬁxed window as is done
in usual standard software packages. The overshoots and under shoots indicate
Fig. 4.1 Portion of the signal corresponding to one song
4.1
Introduction
61

where the pitch values are incorrect reﬂected by extra-ordinarily large sudden
changes in pitch values. Though the pitch proﬁle is highly compressed (more that
30,000 pitch values in the Fig. 4.2), one could yet sense the continuous variation in
pitch values. This makes the extraction of steady states a non-trivial task.
4.2.2
Smoothing
The .cep ﬁle contains pitch extracted only in the quasi-periodic region of the signal.
The pitch detection algorithm above uses a predeﬁned speciﬁc range for possible
pitch values e.g., in the present case the default range is ﬁxed between 70 and
700 Hz. The smoothing operation consists of detection of errors in pitch data and
suitably correcting them. Three types of errors are observed. One type is that the
determined pitch value is approximately equal to half or doubles of the actual pitch
value. This error is typical for PSA though usually a rare and isolate occurrence.
The other is pitch occurring outside the range. Third one is spike in pitch sequence.
The removal of the ﬁrst type of error is simply detection of it and doubling or
halving the value as required. For the second type the anomalous values are
replaced using linear interpolation from the neighbouring valid data. To remove
the third error we simply replace the (i + 1)th pitch xi+1 by the ith pitch xi when
|(xi+1−xi)| > xi*0.1 & xi+1 > 0. The resulting pitch proﬁles are put into 118 ‘.pit
ﬁles’. Figure 4.3 presents the portion of the Pit ﬁle roughly corresponding to the
part of the signal presented in Fig. 4.1. A visual examination reveals that the major
overshoots and undershoots have been effectively removed. The other errors
mentioned above are also removed satisfactorily.
Fig. 4.2 Portion of the .cep ﬁles showing raw pitch proﬁle
62
4
Tonic Detection and Shruti Analysis from Raga Performance

4.2.3
Steady State Detection
The deﬁnition of steady state is not a trivial problem. Even in a perceptible steady
note all the pitch values are never identical. There are always some involuntary
small variations in pitch, which has no relevance in the perception of the corre-
sponding value of the note. We need to segment the steady states from the con-
tinuous pitch contour as it is where the notes, shrutis manifest themselves.
From p-ﬁle steady state sequences are created with all consecutive pitch in the .
pit ﬁles. We deﬁne a steady state in the pitch ﬁle as that subsequence of pitch data
where all the pitch values lie within a predetermined band around the mean value.
A preliminary estimate of a steady state sequences is created from smoothed
pitch data with all consecutive pitch in a sequence, which is terminated when
|xi+1−M| > M/30 where M = (R xi.)/i. For each of these sequences, the standard
deviation (SD) of the pitch values in a window of length of approximately 60 ms is
examined. If the value of SD is less than a threshold value h the pitch contour is
taken to be steady and all pitch values in the window is replace by M. The value of
60 ms for the window length is another threshold, which means a comfortably
perceptible steady state for a note. h = 1 roughly correspond to a width of ±10
cents. With the aforesaid values the std ﬁle presents the sequences of steady states
giving the mean pitch values obtained from the pitch contour extracted from the
song. Figure 4.4 presents the std ﬁle corresponding to the pit ﬁle presented in
Fig. 4.3 for h = 1. It may be noticed that steady states less than 250 ms are rejected
in the std ﬁle.
This being a very compressed ﬁgure the steady states may be visualized well in
Fig. 4.5, which represents only ﬁrst 10s of the pitch proﬁle.
Fig. 4.3 Portion of the .pit ﬁles showing raw pitch proﬁle
4.2
Relevant Signal Processing
63

4.3
Determination of Tonic (Sa)
A musician can usually identify the base tonic (Sa) in a singing easily. However
how they do it is an inexplicit knowledge. Presumably he/she tries to imitate the
note sequences in his/her own voice either loudly or silently before deciding which
one may be the tonic. We, therefore, presume this to be some sort of error-feedback
mechanism. Consequently we decided to use an error minimizing technique for the
purpose. The basic approach is to try out a large number of pitch values in a given
range as a possible tonic and to ﬁnd the deviations of the actual steady states from
Fig. 4.4 Portion of the .std ﬁles showing raw pitch proﬁle
Fig. 4.5 Expanded view of Std ﬁle
64
4
Tonic Detection and Shruti Analysis from Raga Performance

the closest note predicted by the selected tonic. It is reasonable to assume that for an
incorrect trial tonic the sum of these deviations would be large, since the musical
intervals are known to be non-uniform. In fact, the sum of these deviations for each
trial tonic is seen to be a function of the value of the trial tonic. We shall see later
that this function is generally a continuous single valued function in the given
range. We assume that the minimum in this function will indicate the actual tonic.
4.3.1
Data Base
Forty one singers (Thirty four male and seven female) of Hindustani music were
asked to render four ragas namely Bhairav (That—Bhairav), Darbari Kannada (That
—Asavari), Mian-ki-Malhar (That—Kaﬁ) and Todi (That—Todi), which included
aalap, vistar, taan and gamaka. In most cases the notes extended on both sides of the
middle octave. The F0 range for male and female singers was 55–600 Hz and 100–
800 Hz respectively. Direct digital recording was done in a noise proof studio
having a reverberation time of 0.1s. via standard sound card (full Duplex PnP) in P
IV PC (2.6 GHz.) to avoid possible phase distortions present in most of the analog
recording (Datta et al. 2003). The digitization of the signal was done at the rate of
22,050 samples/sec (16 bits/sample). Only the voice of the singer was recorded.
The tanpura drone was fed to the singer through headphone. For our analysis only
the aalap part of each singer was selected from each raga. Pieces of aalap for each
singer for a raga were taken out from the complete aalap deleting the bandish
part. These constituted the aalap signal ﬁles for a singer for each raga. For each raga
a singer had one aalap signal of *2–3 min. Total 118 aalap signal ﬁles (97 for male
and 21 for female) out of 164 songs were selected as suitable for analysis. These
constitute our database.
4.3.2
Experimental Details
For extraction of tonic The .std ﬁles obtained from the songs are used. Two separate
ranges are selected 95–175 Hz and 185–255 Hz respectively for male and female
singers. Within these ranges each pitch value at an interval of 0.01 Hz was tried as a
tentative tonic. For each of such tentative tonic corresponding note intervals are
constructed using each of the 3 ratio systems having 12 ratios each, they are
(i) equi-tempered (ET) (Datta et al. 2004), (ii) ratio system compiled by Lenz and
Danielou (W) and (iii) a new ratio system extracted from analyzing (Datta 1997)
actual songs (NS). NS is constructed by manually measuring the 12 peaks as shown
in the Fig. 4.6, which depicts the frequency distribution of the duration of pitch
(folded to the middle octave). Table 4.1 presents the above stated peak values.
Frequency ratios of the 12 peaks in Fig. 4.1 henceforth referred as new ratio system
(NS).
4.3
Determination of Tonic (Sa)
65

The error for each of the steady states was determined in relation to one of the
aforesaid note structures. The deviation of each of the steady states from the closest
ratio values of the selected note structure is then determined. The sum of these
deviations constitutes the error corresponding to the tentative tonic. The errors for
all values of the tentative tonic are then calculated. This constitutes the error
function with respect to the tentative values of the tonic. Two different kinds of
error were calculated for each paradigm. One is plain error e1 and the other is the
weighted error e2 (weighted by the duration of the pitch).
e1 ¼ 1=m 
X
m
k¼1
X
n
l¼1
ðfk  NlÞ2
(
)1=2
e2 ¼ 1=
X
k
wk 
X
m
k¼1
X
n
l¼1
ðfk  NlÞ  w2
k
(
)1=2
where
fk
is the frequency of the kth steady state,
Nl
is the frequency of the closest note,
wk
is the duration of the
l
runs over all the n number of notes
and k
runs over all m number of steady states.
Two paradigms for error functions were used. The ﬁrst one is done without using
restricted grammar (see Table 4.5) and the other using restricted grammar. The use
of restricted grammar means that while calculating error we do not use all notes but
limit the notes to only those dictated by the grammar for the raga of the selected
Fig. 4.6 Distribution of duration in 1200 bins for signals pooled together
Table 4.1 Frequency ratios of the 12 peaks in Fig. 4.6
Sa
re
Re
ga
Ga
Ma
MA
Pa
dha
Dha
ni
Ni
1.
1.054
1.125
1.186
1.253
1.333
1.451
1.5
1.58
1.691
1.778
1.893
66
4
Tonic Detection and Shruti Analysis from Raga Performance

song. For each of these four cases two different error minimization approaches were
investigated for ﬁnding the tonic. One corresponds to least error in the error
sequence (Method 1). The other one ﬁrst assumes the local minima in the error
sequence as valid notes then calculate the least error corresponding to these minima
(Method 2). Average value of the deviations were calculated ratio system-wise (3
ratio systems) and sex-wise (male and female). Figures 4.7 and 4.8 below present
one example each respectively from a male and a female singer. In the legend
RWL_NS represent the curve for weighted error using grammar and WWL_NS that
without using grammar for method 1 in new scale.
It may be seen that the pitch proﬁle rarely presents a smooth steady length of
region. The zeroes generally indicate short interruption due to consonantal
obstructions. The x-axis in the error proﬁle is the value of the trial tonic and y-axis
gives the corresponding error. The error proﬁles happen to be relatively smooth
oscillating curves with prominent extrema. The lowest minimum indicated by the
arrow gives the required tonic.
4.3.3
Results and Discussions
Table 4.2 presents averages of absolute deviations of tonic detected using all the 8
procedures, as outlined in earlier. As is seen, the data is pooled sex-wise (female
and male) and ratio system-wise (3 ratio systems, as discussed earlier). The ﬁgures
in the brackets indicates the count of corresponding signal data ﬁles after rejecting
Fig. 4.7 Example of pitch contour and error proﬁle for a male singer
4.3
Determination of Tonic (Sa)
67

those showing absolute deviation of error greater than sum of mean and S.D. in the
respective category. The average detection of the tonic is about 83% for all the
methods and songs. Consequently, the data in the table is actually the modiﬁed
average after rejecting the above-stated outliers. Average in the last row takes into
account of these counts.
Fig. 4.8 Example of pitch contour and error proﬁle for a female singer
Table 4.2 Averages of absolute deviations of tonic detected from actual tonic
Sex
Method
Without grammar
Using grammar
Plain error
Weighted error
Plain error
Weighted error
1
2
1
2
1
2
1
2
F
ET
11.89
(17)
16.18
(18)
11.5
(18)
15.52
(18)
0.66
(20)
13.97
(17)
0.66
(20)
13.57
(17)
W
9.24
(17)
7.55
(17)
9.87
(16)
8.27
(18)
0.78
(20)
9.37
(17)
0.75
(20)
7.84
(17)
NS
11.29
(18)
7.94
(18)
11.29
(18)
6.95
(17)
0.59
(20)
11.11
(17)
0.58
(20)
11.21
(17)
M
ET
12.51
(77)
19.58
(80)
12.08
(76)
19.37
(80)
2.27
(84)
16.13
(80)
2.46
(81)
16.15
(79)
W
15.68
(79)
11.15
(78)
15.44
(76)
11.29
(78)
2.72
(80)
15.64
(81)
2.58
(78)
12.61
(78)
NS
14.37
(76)
19.88
(79)
14.65
(78)
20.13
(82)
2.37
(83)
17.22
(81)
2.23
(80)
17.39
(81)
Average
13.58
(284)
15.75
(290)
13.5
(282)
15.79
(293)
2.11
(307)
15.49
(293)
2.07
(299)
14.6
(289)
68
4
Tonic Detection and Shruti Analysis from Raga Performance

As is observed from the table, weighted error using restricted grammar gives the
best results. With this approach 85% of the tonics could be detected with an
accuracy of about 2 Hz. However the best result is for the female singers. It is 95%
with an average accuracy of 0.58 Hz. Although the ratio system NS is the best,
others give very close results. Method 1, which uses the least error, gives appre-
ciably better results compared to Method 2. The results in the table indicate that
weighted error in combination with method 1 using restricted grammar is best
suited for automatic tonic detection.
4.4
Swara-Shruti Relation
As already mentioned in Sect. 4.0 the problems that need to be addressed are:
1. Ascertaining the position of the ratios representing the interval with respect to
the intervals themselves.
2. Finding which of the two distributions of shrutis with respect to swaras (ancient
4, 3, 2, 4, 4, 3, 2 or modern 1, 4, 4, 4, 1, 4, 4) is being followed in contemporary
singing.
3. Ascertaining scale amongst those available in literature which ﬁts best the
contemporary performances.
Table 4.3 presents the ratios of scales used in Hindustani music obtained from
available literatures.
Legend:
• C—Altered Consonance*
• D—Deval
• E—Equi-tempered
• N—Nagoji Row
• SB—Sahasra Budhi
• SM—Sourindra Mohan
• W—Western compilation*
• *—Refer Chap. 3.
4.5
Ratio-Intervals for Steady States
Shrutis are believed to be the sub-intervals, which hold the twelve swaras. At least
two different distributions are generally reported. Table 4.4 gives the distribution of
shrutis under the two systems namely, modern and ancient. Shuddha swaras are
indicated by capitalizing the ﬁrst letter. Sharper notes are indicated by # and ﬂatter
ones by ¶.
4.3
Determination of Tonic (Sa)
69

For the analysis one needs to use a grammatical structure of a raga. Towards this
end, the basic grammatical structures chosen for the four ragas namely, Bhairav,
Darbari Kannada, Mian-ki-Malhar and Todi selected for the present task, is given in
Table 4.3 Shrutis from different sources
Shruti
A
C
D
E
N
SB
SM
W
S1
1/1
1/1
1/1
1
1/1
1/1
1/1
1/1
R1
27/26
21/20
21/20
1.032
25/24
44/43
36/35
256/243
R2
14/13
13/12
16/15
1.065
16/15
44/42
36/34
16/15
R3
10/9
10/9
10/9
1.099
10/9
44/41
36/33
10/9
R4
8/7
9/8
9/8
1.134
9/8
44/40
36/32
9/8
G1
7/6
7/6
32/27
1.171
32/27
44/39
36/31
32/27
G2
6/5
6/5
6/5
1.208
6/5
44/38
36/30
6/5
G3
5/4
5/4
5/4
1.247
5/4
44/37
36/29
5/4
G4
9/7
13/10
81/64
1.287
32/25
44/36
36/28
81/64
M1
4/3
4/3
21/16
1.328
4/3
44/35
36/27
4/3
M2
7/5
25/18
4/3
1.37
27/20
44/34
52/38
27/20
M3
10/7
7/5
45/32
1.414
45/32
44/33
52/37
45/32
M4
13/9
13/9
64/45
1.459
36/25
44/32
52/36
64/45
P1
3/2
3/2
3/2
1.506
3/2
44/31
52/35
3/2
D1
26/17
14/9
63/40
1.554
25/16
44/30
52/34
128/81
D2
19/12
8/5
8/5
1.604
8/5
44/29
52/33
8/5
D3
5/3
5/3
5/3
1.656
5/3
44/28
52/32
5/3
D4
26/15
26/15
27/16
1.708
27/16
44/27
52/31
27/16
N1
25/14
7/4
16/9
1.763
16/9
44/26
52/30
16/9
N2
9/5
9/5
9/5
1.82
9/5
44/25
52/29
9/5
N3
15/8
15/8
15/8
1.878
15/8
44/24
52/28
15/8
N4
25/13
21/11
243/128
1.938
48/25
44/23
52/27
243/128
Table 4.4 Distribution of shrutis with respect to swaras
Name of Shruti
Ancient
Modern
Name of Shruti
Ancient
Modern
Tivra
Sa
Sa
priti
Ma#
ma
Kumudvati
Sa
re¶
marjani
Ma##
ma#
Manda
Sa
re
kshiti
Pa
Pa
Chandovati
Sa
Re
rakta
Pa
dha¶
Dayavati
re¶
Re#
sandipini
Pa
dha
Ranjani
re
ga¶
alapini
Pa
Dha
Raktika
Re
ga
madanti
dha¶
Dha#
Roudri
ga
Ga
rohini
dha
n¶
Krodhi
Ga
Ga#
ramya
Dha
ni
Bajrika
ma
Ma¶
ugra
ni
Ni
Prasarini
Ma
Ma
kshobhini
Ni
N¶
70
4
Tonic Detection and Shruti Analysis from Raga Performance

Table 4.5. It may be noted that the subtle variations of the nature of notes between
ascent and descent, if any, are not used. This is because automatic detection of the
ascending and descending phrases is again a complex artiﬁcial intelligence problem
and yet awaits a reasonably robust solution.
4.5.1
Data Base
116 songs on four ragas namely Bhairav (Thaat—Bhairav), Darbari Kannada
(Thaat—Asavari), Mian-ki-Malhar (Thaat—Kaﬁ) and Todi (Thaat—Todi), sung by
twenty three (23) singers from ﬁve different gharanas of Hindustani music, namely
Agra, Kirana, Patiala, Gwalior and Mixed are taken for analysis. Only the alap part
from these songs is used as it has slow tempo and therefore detection of steady
states and fundamental frequencies are easier. The Mixed gharana constitutes those
contemporary singers who do not belong to a particular traditional gharana. They
are known to receive training from teachers of different traditional gharanas. Total
signal data consists of 7 h of recording.
4.5.2
Analysis
The ﬁrst task is to decide the position of the ratio with regard to the musical
interval. One could use an inﬁnite number of representations of an interval. Let r1,
r2, r3 are the three successive ratios, and r is the ratio of any pitch with respect to the
tonic. Let also Iri represent the interval corresponding to ri. We shall consider for
the analysis only ﬁve basic representations of the intervals by the representative
ratios, namely boundary at left end, right end, arithmetic mean, geometric mean,
harmonic mean. Then conditions (i) to (v) shown below deﬁnes the belongingness
of r to an interval. If:
(i)
r  r1\r2 then r belongs to Ir1 (when ratio indicates the right boundary)
(ii)
r  r1; r\r2 then r belongs to Ir1 (when ratio indicates left boundary)
(iii)
r  r1 þ r2
ð
Þ=2\ r2 þ r3
ð
Þ=2 then r belongs to Ir2 (when ratio indicates
arithmetic mean)
(iv)
r  pr1r2\pr2r3 then r belongs to Ir2 (when ratio indicates geometric mean)
(v)
r  2 r1r2= r1 þ r2
ð
Þ\2 r2r3= r2 þ r3
ð
Þ then r belongs to Ir2 (when ratio indi-
cates harmonic mean).
Table 4.5 Basic
grammatical structures of
selected ragas
Raga
Shuddha Swaras
Vikrit Swaras
Bhairav
Ga, Ma, Ni
re, dha
Darbari Kanada
Re, Ma
ga, dha, ni
Mian-ki-Malhar
Re, Ma, Dha, Ni
ga, ni
Todi
Ni
re, ga, ma, dha
4.5
Ratio-Intervals for Steady States
71

Figure 4.9 schematically presents the relationship of the ratios with the corre-
sponding intervals. The colours red, blue and green represent the interval for the
ratio r2 when the boundary respectively is right, left or A.M.
We have two different alternatives for swara-shruti relations, ancient and modern
and ﬁve alternatives relating to the position of the interval boundary. As they are
interrelated we have a total of ten alternative hypotheses, ﬁve each for the two
swara-shruti relations, to test. Again each of the hypotheses is to be tested for each
of the eight-shruti ratio systems given in Table 4.3.
For the purpose of analysis the notes found in the data set for all songs for each
of the raga are divided into two groups. One group (G) consists of those notes
which follow the prescription in Table 4.5 under the hypothesis being tested; the
other being the group (g), which consists of notes in contradiction to the Table 4.3.
A score si in percentage is deﬁned by the relation si = 100  MG/(MG + Mg),
where MG and Mg are the total duration of notes in group G and g respectively in a
raga when the ith hypothesis of the ten aforesaid hypothesis is tested. Si = Psi
where i runs over all ragas represent the score for the ith hypothesis. Obviously a
higher value of Si would indicate greater acceptability for a particular hypothesis.
4.5.3
Results and Discussions
Table 4.6 presents score Si for data on all songs pooled together using ﬁve
boundary conditions with the two possible swara-shruti distributions. In row 2 am,
gm and hm indicate the interval boundaries respectively at arithmetic mean, geo-
metric mean and harmonic mean. Scores are computed for the seven different ratio
Fig. 4.9 Deﬁnition of
interval boundaries
Table 4.6 Distribution of score S for swara-shruti relations, intervals and ratio-scales
Scales
Ancient
Modern
Left
Right
am
gm
hm
Left
Right
am
gm
hm
D
78.88
85.14
85
84.98
84.99
86.5
79.92
94.21
94.24
94.22
A
80.45
86.41
86.91
86.92
86.9
85.32
79.49
92.87
92.88
92.85
W
78.30
86.84
84.97
84.94
84.95
85.88
81.76
94.23
94.25
94.22
ET
75.32
85.83
82.95
82.96
82.95
87.89
83.49
94.76
94.76
94.74
NR
75.44
86.86
84.6
84.63
84.65
86.16
82.03
95.22
95.25
95.25
SB
78.85
87.28
84.47
84.5
84.5
78.34
63.67
76.27
76.23
76.20
SM
78.96
86.97
83.39
83.44
83.47
89.17
75.07
92.28
92.25
92.23
Average
78.49
86.33
84.64
84.65
84.65
85.88
78.49
91.8
91.8
91.77
72
4
Tonic Detection and Shruti Analysis from Raga Performance

systems we could collect from available literature (Table 4.3). The highest value in
each row is given in bold letters. It may be seen from the Table 4.3 that the value of
S for the modern view on swara-shruti distribution signiﬁcantly exceeds the dis-
tribution proposed in the ancient treatises in all the cases except one i.e., for the
scale proposed by Sahasra Buddhi. That the highest score for this scale is signiﬁ-
cantly lower than those of other scales, this result may be kept out of further
considerations. The score Si averaged over all the interval hypotheses respectively
for ancient and modern swara-shruti distribution are 84 and 88 approximately. This
strongly indicates that the performers of recent times use the modern swara- shruti
distribution, i.e., 1, 4, 4, 4, 1, 4 and 4. In the context of the present analysis a score
of above 90% appears to be quite good as it means that less than 10% of the sung
notes deviate from the grammar. These deviations include some possible errors in
processing as well as the deviations of the singers uses deliberately in exceptional
cases, may be for aesthetic considerations.
The average score further reveals the boundary deﬁnitions at the middle region
of the interval far outweigh other deﬁnitions. It may be noted that the scores for the
three mid positions namely arithmetic, geometric and harmonic means are almost
the same. This is expected as in the real sense shruti intervals being small these
mean values are not signiﬁcantly different. The signiﬁcantly larger scores for the
hypothesis that intervals are represented not by the end points but by the middle and
that it is the modern swara-shruti distribution as against the ancient ones which is
used by these 41 singers indicates the modern trend in Hindustani singing. The
highest score of 95.25% is observed for the scale proposed by Nagoji Row. The
next comparable scales are those of equi-tempered, western compilation and
Deval’s in descending order. It may be noted that the score for these scales are so
close that it is difﬁcult to say conclusively at this point which one is really used in
the performances.
4.6
Shruti Positions in Contemporary Performances
The following sections are devoted to the objective analysis of performances of
established singers of Hindustani music ﬁrst to obtain the 22 shruti positions from
the analysed data and then to examine each of the independent scales enumerated in
earlier sections to ﬁnd which amongst them ﬁt these observed positions best. It has
been seen in earlier sections that the distribution of shrutis with respect to swaras, in
modern day performances, follow 1, 4, 4, 4, 1, 4 and 4. We have also seen that the
ratios represent the geometric mean of the intervals. These will be the basis of
analysis in following sections.
Technological advancement makes it feasible to provide objective assessment of
the intervals and usage of the shrutis (Datta et al. 2000, 2004; Deva 1974;
Jairazbhoy 1975; Deval 1910; Modak 1967a; Rao and Van der Meer 2004;
4.5
Ratio-Intervals for Steady States
73

Van der Meer 2000; Jairazbhoy and Stone 1963; Levy 1982; Weisstein 2004)
directly from performed music. For a proper understanding of use of shruti in
contemporary Hindustani music it is of prime importance to know the objective and
standard shruti position for every note performers employ in different ragas. For this
purpose, an objective analysis of aalap from 142 songs performed by 42 eminent
musicians and scholars covering 21 different ragas will be presented. The total time
of singing analysed is nearly 8.5 h. The audio samples are taken from recorded
performances. The experiment handled 1.35 GB of digitized song signal data as
detailed above to extract the steady pitch states. For this objective assessment, two
different approaches for the purpose, namely clustering and peak-picking from
frequency distribution shall be presented.
4.6.1
Clustering Methodology
One is a clustering approach, a procedure which automatically ﬁnds out different
concentrations in the data ﬁeld. As a large number of songs are analyzed there
would be a large number of pitch values. If the 22-shruti positions were used
meaningfully by the singers one would expect the data to be clustered around the
positions. The larger the volume of data the more prominent and stable would be
the cluster. The clustering algorithm works in a recursive mode and ﬁnally con-
verges to the best possible solution. The other approach is a heuristic search for
these concentrations through construction of bins and then ﬁnding out which bins
contain signiﬁcant amounts of data. The manipulation of bin size is painstaking and
starts with smaller bin size followed by the increasing of the bin size progressively
until the desired number of different concentrations is obtained. The detailed pro-
cedures for preparation and extraction of data in the form of frequency ratios folded
into one octave from signals has been described in earlier sections. For the heuristic
approach the data so obtained for all the 141 songs is pooled together and then put
into 1200 bins of one cent each covering the octave. Let rij be an element in ith bin
of jth song and let pij be the corresponding pitch in Hz in the original sequence of
smoothed pitch data of the jth song. Then P
j 1

pij is the total duration of all data in
the ith bin.
Figure 4.10 shows the 3D spectra (top section), compressed waveform (middle
section) of a song-sample (Mian-ki-Malhar) along with raw pitch proﬁle (bottom
section) extracted by different algorithms.
Figure 4.11 shows the frequency distribution of one song of raga Bhairav spread
over 1200 bins of 1-cent interval. The peaks can be easily identiﬁed with the ratios,
which correspond to shruti positions R1, R2, G4, M1, D1 and N4, utilized by the
artist in this song.
74
4
Tonic Detection and Shruti Analysis from Raga Performance

4.6.2
Algorithm (K-Means)
For a clustering algorithm the task becomes simpler if the number of such con-
centrations is known beforehand. The number of shrutis for each of the notes is
known. It is now necessary only to locate the main swara positions from the
distribution of duration across the bins. Fortunately as we shall see later it is an easy
task. To begin with a set of arbitrarily selected positions, called the seed points each
representing a shruti, for the 22-shrutis are given. The algorithm checks each ele-
ment of the population and assigns it to one of the clusters. This point is assigned to
that cluster for which the seed point is at the minimum distance. The cluster
position is recalculated every time a component is added to a cluster and this
Fig. 4.10 3D spectra, waveform and raw pitch proﬁle for a song in raga Mian-ki Malhar
Fig. 4.11 Distribution of the duration (sec.) in 1200 bin of a song-sample (Bhairav)
4.6
Shruti Positions in Contemporary Performances
75

continues until all the components are grouped into the ﬁnal required number of
clusters. This process modiﬁes the position of the representative points. This pro-
cess continues recursively till the best set of seed points are obtained. This recursive
process is an algorithm (Hartigan and Wang 1979) for partitioning (or clustering) N
data points into K disjoint subsets Sj containing Nj data points so as to minimize the
sum-of-squares criterion
J ¼
X
K
j¼1
X
n2Sj xn  lj

2
where xn is the value
P
j 1
.
pij


representing the nth data point and lj is the
geometric centroid of the data points in Sj.
4.6.3
Results
Though for individual songs of each artist peak positions revealed the shruti
positions used in the songs (Fig. 4.11), the individual prominences for each shruti
position get lost when the steady states of all the 142 songs are pooled together and
put into the 1200 bins. As the notes are said to be the receptacle of the shrutis, one
could assign points representing shrutis on both the sides of a note representing
each hill. The peak of distribution of Pa is another initial seed point. The clustering
algorithm described in the last section uses these as the initial seed points and ﬁnds
the appropriate clusters for each of these points from the data pooled for all songs
distributed over the 1200 bins (Fig. 4.12). The distribution now shows peaks at the
12 note ratios.
The centroids of these clusters are used as the new seed points for the next
iteration. The iteration continues till the seed points stabilize. The open circles in
Fig. 4.3 represent these stabilised values of the seed points, which are taken as the
shruti positions extracted using the clustering algorithm. These ratio values are
given in column 10 of Table 4.7.
Fig. 4.12 Distribution of the duration in 1200 bin for all data (142 songs) pooled together
76
4
Tonic Detection and Shruti Analysis from Raga Performance

4.7
Approach of Heuristic Search
As mentioned earlier the search for shruti position starts with a narrow bin of one
cent width. The manipulation of bin sizes is done by progressively increasing the
bin size in steps for each song so that well-deﬁned separate clusters appear. It has
been seen that a bin size corresponding to approximately 27 cents, which makes 44
bins in an octave, is best suited for visualization of the clusters.
4.7.1
Methodology
The frequencies of the steady states from a song are now distributed in 1200 bins of
one-cent width each. The peaks of these distributions for each song are purported to
be indicative of the shruti positions for that song. Figure 4.13 shows the distribution
for one song. One could see that the distribution consists of closely packed clusters
in the form of hills strewn along the ratio axis. The peaks are indicated by red dots.
Table 4.7 Shruti positions obtained from distribution of steady states
Sl.
no.
Shruti
Peaks from different groups
Interval from
Diff. in
cents
Bhai
Dar
Mia
Todi
Singer
A
Singer
B
Peaks
Cluster
1.
r1
1.048
1.048
1.047
1.65
2.
r2
1.065
1.065
1.065
1.065
1.065
1.071
9.73
3.
r3
1.117
1.111
9.32
4.
r4
1.134
1.134
1.134
1.135
1.53
5.
g1
1.189
1.189
1.189
1.173
23.45
6.
g2
1.208
1.208
1.208
1.208
1.206
2.87
7.
g3
1.224
1.227
1.243
22.43
8.
g4
1.267
1.267
1.267
1.267
1.267
1.267
1.271
5.46
9.
m1
1.328
1.328
1.328
1.317
14.4
10.
m2
1.349
1.349
1.349
1.349
1.349
1.349
1.349
0
11.
m3
1.399
1.399
1.399
0
12.
m4
1.428
1.428
1.428
1.428
1.437
1.429
1.428
1.21
13.
pa
1.506
1.506
1.506
1.506
1.506
1.506
1.506
1.51
4.59
14.
d1
1.579
1.579
1.579
1.579
1.573
6.59
15.
d2
1.604
1.604
1.604
1.604
1.613
9.69
16.
d3
1.63
1.63
1.63
1.668
39.9
17.
d4
1.682
1.682
1.682
1.682
1.682
1.709
27.57
18.
n1
1.727
1.7265
1.763
1.739
1.759
19.8
19.
n2
1.791
1.791
1.791
1.791
1.803
11.56
20.
n3
1.824
1.824
1.824
1.869
42.19
21.
n4
1.908
1.908
1.908
1.908
1.908
1.908
1.908
1.914
5.44
4.7
Approach of Heuristic Search
77

These peaks for different songs when compared are found to be slightly displaced in
one-cent bins. These peaks are pooled together in six different groups, four
raga-wise and two singer-wise.
These pooled peaks from 1200 bin-data for each group are then re-distributed in
44 bins of equal size in cents as mentioned in the last section. The peaks from these
distributions for each group are considered to be the shruti positions as exhibited for
the group.
4.7.2
Results and Discussions
4.7.2.1
Shruti Positions
Figure 4.14 shows the distribution of shrutis in 44 bins obtained from the distri-
bution of steady states of the pitch proﬁle of different songs in raga Bhairav
(BHAI), Darbari Kannada (DAR), Mian-ki-Malhar (MIA) and Todi (TODI). The
broken vertical lines (21 in number excluding Sa) represent the shrutis obtained
0
10
20
30
40
50
60
70
80
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
Ratio of bin
No. of occurences
Fig. 4.13 Distribution of peaks in 1200 bins for one song
Fig. 4.14 Distribution of shrutis in raga Bhairav, Darbari, Mian-ki-Malhar and Todi
78
4
Tonic Detection and Shruti Analysis from Raga Performance

through the clustering algorithm (sub-sections of Sect. 4.5). It is interesting to note
that though the data is pooled from 142 songs from 42 performers, the distribution
of shruti positions is signiﬁcantly peaky. Some of the peaks cluster closely together.
One could see 9 closely packed clusters of them atikomal and tivra Rishav, tivra
Gandhar, tvratara Madhyam, tivra Nishad are clearly indicated shrutis.
Figure 4.15 shows those for different ragas sung by two most eminent vocalists
(A and B). The peaks obviously indicate the concentration of samples in a bin and
therefore may be taken as tentative shruti positions used by the performers in
corresponding groups. Visual inspection of the ﬁgures shows that a number of
peaks are in quite good agreement with the shruti positions determined from the
clustering algorithm. The ragas included in A’s list are Ahir Bhairav, Basant Bahar,
Bairagi Bhairav, Bilaskhani Todi, Basant Mukhari, Gurjari Todi, Komal Rishav
Ashavari, Lalit, Marwa, Nat Bhairav, Puriya Kalyan, Puriya, Ramkali, Sree and
Todi. For B, the list consists of Bhankar Todi, Darbari Kannada, Lalit, Lalit Bhatiar,
Lalita Gauri, Marwa, Mian ki Malhar, Multani, Puriya Kalyan, Puriya, Ramkeli and
Todi.
Table 4.7 gives the values of shrutis represented by the peaks in Figs. 4.14 and
4.15 as well as those obtained from the clustering algorithm. Column 9 and 10
respectively present the values of shrutis obtained through peak picking and the
clustering approaches described in earlier sections. The peaks, in column 9, indicate
that in performances these are the preferred positions for the performers. The dif-
ference in cents between the shruti intervals obtained by the two methods is given
in column 10. The difference is generally low, average difference being 12 cents.
There is very good agreement for shrutis r1, r4, m2, m3, m4, in both the algorithms.
For g1, g2, d3, d4, n1 and n3 difference is noticeably large.
Table 4.8 gives the difference (in cents) of shruti ratios obtained by clustering
and peak-picking algorithms from the ﬁve shruti ratios selected from eight shruti
ratios obtained from literature (Table 4.3) and the theoretical scale obtained from
consonance hypothesis (vide Chap. 3). Last row gives the average values for each
of them which shows clustering algorithm seem to be closer to all the ﬁve shruti
ratios.
Fig. 4.15 Distribution of shrutis in different ragas by vocalists A and B
4.7
Approach of Heuristic Search
79

The average value of error, around 10 cents in these cases, may be considered as
reasonably low. The results may be taken as validation of the existence of the
twenty-two shrutis directly from the analysis of performances of a large number of
singers as well as an adequate number of different ragas.
The ratio given by Deval and the western compilation are close to both the
experimentally obtained ratios. The average error of less than 10 cents for Deval’s
shruti ratios from those obtained through clustering algorithm is noteworthy. Only
G1, M2, D4 and N1 of Deval’s shruti ratios have an error over 15 cents. It may be
noted that for these four shrutis the difference between the values of the two
methods are also quite large.
Scale Legend:
• c—Consonance (Theoretical model)
• d—Deval
• et—Equi-tempered
• nr—Nagoji Row
• w—Western compilation.
Table 4.8 Difference of shruti ratios of selected scales from observed ones (in cents)
Shruti
Clustering algorithm
Peak-picking algorithm
nr
et
w
d
c
nr
et
w
d
c
R1
8.36
24.49
11.19
5.43
13.7
10.49
26.62
9.06
3.30
15.82
R2
6.63
9.27
6.63
6.63
9.93
2.71
0.07
2.71
2.71
19.27
R3
0.11
18.88
0.11
0.11
0.11
9.15
27.92
9.15
9.15
9.15
R4
14.85
0.58
14.85
14.85
12.41
13.79
0.48
13.79
13.79
13.47
G1
17.73
3.68
17.73
17.73
9.54
5.56
26.97
5.56
5.56
32.83
G2
8.01
3.62
8.01
8.01
8.01
11.50
0.13
11.50
11.50
11.50
G3
9.19
4.69
9.19
9.19
9.19
32.15
27.66
32.15
32.15
32.15
G4
12.02
21.01
7.54
7.54
19.73
17.67
26.66
1.88
1.88
25.38
M1
21.78
14.64
21.78
5.49
21.78
6.94
0.20
6.94
20.33
6.94
M2
1.80
27.71
1.80
19.70
64.76
1.28
27.19
1.28
20.22
64.24
M3
9.38
19.16
9.38
9.38
36.65
8.95
18.73
8.95
8.95
36.21
M4
14.48
37.75
7.02
7.02
19.82
13.28
36.54
8.23
8.23
18.61
P
11.55
4.41
11.55
11.55
11.55
6.91
0.23
6.91
6.91
6.911
D1
11.65
20.64
7.91
2.15
48.7
18.19
27.18
1.37
4.39
55.24
D2
13.59
9.10
13.59
13.59
31.72
4.32
0.17
4.32
4.32
22.45
D3
1.19
12.82
1.19
1.19
1.19
38.51
26.88
38.51
38.51
38.51
D4
22.15
0.75
22.15
22.15
5.11
5.65
27.06
5.65
5.65
32.92
N1
18.80
4.53
18.80
18.80
8.47
38.18
23.91
38.18
38.18
10.92
N2
3.34
15.42
3.34
3.34
3.35
8.68
27.45
8.68
8.68
8.68
N3
5.52
8.17
5.52
5.52
5.53
47.74
50.38
47.74
47.74
47.74
N4
5.55
21.68
14.00
14.00
8.32
10.85
26.98
8.70
8.70
13.63
Average
10.37
13.48
10.16
9.69
16.65
14.88
20.45
12.92
14.33
24.89
80
4
Tonic Detection and Shruti Analysis from Raga Performance

Column 6 and 11 of Table 4.8 gives the difference in cents between ratios
respectively from clustering and peak-picking algorithms and those obtained from
the theoretical shruti ratios obtained in Table 4.3. The average error of 16.65 for the
theoretical shruti ratios from the ratios obtained from the ﬁrst algorithm appears to
be somewhat high compared to the four other ratios. It may be seen that differences
for m2 and d1 is abnormally high. The average difference without these two shrutis
comes down to 12 cents which is quite comparable with other ratios. Out of 21
theoretical predicted shruti ratios, 11 are quite close (within 10 cents) to the
experimental ratios. Of the rest ﬁve ratios differ from the experimental value by
more than 20 cents. One may thus hold that the hypothesis of consonance as the
basis for the psychophysical theory of musical scales presented in Chap. 3 rea-
sonably conforms with the empirical shruti positions.
4.8
Conclusion
Large scale objective analysis of pitch data extracted from 116 songs on four ragas
namely Bhairav, Darbari Kannada, Mian-ki-Malhar and Todi, sung by twenty-three
(23) singers conforms strongly to:
• Shruti distribution of 1, 4, 4, 4, 1, 4 and 4 respectively for swaras Sa, Re, Ga,
Ma, Pa, Dha and Ni.
• Interval boundary being at the middle, particularly at geometric mean of the
interval
The pitch data from 142 songs performed by 42 eminent musicians and scholars
covering 21 different ragas in north Indian classical music reveal use of 22-shrutis
in all the swaras except the achal swaras Sa and Pa. In general, the number of
shrutis, i.e., four, for each note is vindicated. The peak-picking method reveals
poorer conformity with known scales than the clustering algorithm. However, this
need not indicate that clustering algorithm provides the more correct picture.
Clustering algorithm is more automated than the peak-picking method. In the latter
approach, careful inspection in every step is very important. The shruti positions
extracted from the performances is in consonance with those predicted by the
psycho-perceptual hypothesis of musical scale proposed in Chap. 3.
References
Abhola P (1884) “Sangeet Parijata” Kalivara Vedanta—Vagisa (ed), Calcutta Edition
Basant (1980) “Sangeet Visharad”, pub. Sangeet Karyalaya, Hatras (UP)
Bharata (1929) “Natyashastra”, Vidya Vilas Palace, Banaras
Bhatkhande B (1887) “Hindustani Sangeet Paddhati-vol. 7”, In: Roy D, Chattopadhyaya AK (eds).
Deepayan, Kolkata, pp 19–38
4.7
Approach of Heuristic Search
81

Clements E (1913) Introduction to the study of Indian Music. Longmans, Green & Company,
London
Danielou A (1943) Introduction to the study of musical scales. The India Society, London
Datta AK (1996) Generation of musical notations from song using state-phase for pitch detection
algorithm. J Acoust Soc India XXIV
Datta AK, Kundu R (1991) “A psycho-perceptual hypothesis for Indian musical scale”,
Pre-proceedings of the Int. Workshop on recent trends in Speech Music & Allied Signal
Processing, Delhi, Dec 9–11
Datta AK et al. (2003) “Study of srutis in Indian musical scales and relevance of schools in recent
times using signal processing techniques”, Proc. FRSM-2003, IIT Kanpur, Kanpur, Feb 15–16
Datta AK et al. (2004) Srutis from objective analysis of the performances of Hindustani Music
using Clustering Algorithm. J Acoust Soc Ind 32
Datta AK, Sengupta R, Dey N, Banerjee BM, Nag D (1997) Pitch analysis of recorded vocal
performances in Hindustani Music: evidence of a personal scale. J Acoust Soc India XXV
Datta AK, Sengupta R, Dey N, Nag D (2000) “On Scientiﬁc Approaches to the Study of Vaditya
in Indian Music”, Proc. Fifth Int. Workshop on Recent Trends in Speech, Music and Allied
Signal Processing, 14–15 December, Thiruvanathapuram, India
Datta AK, Sengupta R, Dey N, Nag D (1996) Ratios and intervals used in North Indian classical
music—a study based on recorded performance. J Acoust Soc Ind XXIV, III-4.1
Datta A , Sengupta R, Dey N, Nag D, Mukerjee A (2004) “Shruti usuage by old and contemporary
singers in khyal: an objective approach”, Proc. Frontiers of Research on Speech and Music
(FRSM-2004), 8–9 January, Annamalai University, Tamilnadu
Deva BC (1970) Some problems in Science & Music. J Sangeet Natak Acad 16 Apr–June
Deva BC (1974) Music and science. J Ind Musicol Soc 5(1):19, 23
Deval KB (1910) The Hindu musical scale and the twenty-two shrutis. Ayabhushan Press, Pune
Hartigan J, Wang M (1979) A K-means clustering algorithm. Appl Stat 28:100–108
Jairazbhoy NA, Stone AW (1963) Intonation in present-day North Indian classical music. Bull Sch
Orient Afr Stud 26:119–132
Jairazbhoy NA (1975) An interpretation of the twenty-two shrutis. Asian Music 6:38–59
Levy M (1982) Intonation North Indian music. Biblia Impex Pvt Ltd, New Delhi
Modak HV (1967a) Propriety of dividing an octave into twenty-two shrutis. J Music Acad Madras
38:151–159
Modak HV (1967b) Septimal frequency ratios and the twenty-two shruti scale. J Music Acad
Madras 38:160–164
Moore BCJ (1982) An introduction to the psychology of hearing. Academic Press, London
Rao S, van der Meer W (2004) “Shruti in contemporary Hindustani music. Proc. FRSM-2004,
Annamalai University, Jan 8–9
Sarangadeva “Sangeet Ratnakar with comments of Kalinath” Edition, 1897, Anandasram
Sundberg J (1994) Perceptual aspect of singing. J Voice 8(2), N.Y
Tagore SM (1874) “Hindu Music. Hindu Patriot Sept 7
Van der Meer W (2000) Theory and practice of intonation in Hindustani music. In: Barlow C
(ed) The ratio book. Feedback Papers, Koln, pp 50–71
Weisstein EW (2004) K-means clustering algorithm. From MathWorld–a wolfram web resource.
http://mathworld.wolfram.com/K-MeansClusteringAlgorithm.html
82
4
Tonic Detection and Shruti Analysis from Raga Performance

Chapter 5
Pitch Transition and Pitch Stability
5.1
Introduction
The pitch is the fundamental parameter for understanding objectively various aspect
of melodic music. A pitch contour describes a series of relative pitch transitions
adjoining the abstractions of a sequence of steady states called notes. While the
quasi stationary states in the dynamics of pitch in Indian music have been elabo-
rately discussed in other chapters it is also necessary to pay attention to the tran-
sitory movements. The connoisseurs of the Hindustani music hold that the role of
continuous pitch movements including those leading to the establishment of a
particular note in a particular environment cannot be neglected for the full devel-
opment of the aesthetics of a raga (http://www.itcsra.org/alankar/alankar.html).
According to Strawn (1985), a transition “…includes the ending part of the
decay [or release] of one note and, the beginning and possibly all of the attack of
the next note, and whatever connects the two notes.” It has been found to be more
signiﬁcant to listeners in determining melodic similarity, and it also includes
rhythm information. This type of description is also found in Nettheim (1992) and
Lemstrom and Laine (1998), where both pitch and duration sequences are used to
deﬁne the melody. The pitch movements in music are also seen generally as
ornamental embellishments (alankar). This implies a role of the dynamics of pitch
transition in the proper development of the raga in addition to its role as alankar. In
Indian music, Alankar or Alamkara means ornaments or adornments. In the context
of Indian classical music, the application of an alankar is essentially to embellish or
enhance the inherent beauty of the music. Note transitions play other important
roles in the domain of Indian Classical Music. Style, emotions, gharana, raga, and
even the personal characteristics might be embedded in these transitions. Therefore,
it is very important to objectively categorize these transitions to understand their
cognitive implications. With the advent of modern measuring methodologies and
tools these tasks are doable now-a-days. Some studies about such cognitive pro-
cesses in relation to pitch movement in speech domain which categorizes intonation
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_5
83

patterns into different classes like ﬂat, rising, falling, hat, valley, etc. are already
reported elsewhere (Sengupta et al. 2002; Hart et al. 1990).
Analysis and synthesis of transitions between musical notes are open-ended
problems in computer music. While much research has been done on the proper
analysis and synthesis of musical timbres, sequence of notes, etc., less attention has
been paid to what occurs between successively played notes.
Meends are transitional pitch movements between two notes. These are obvi-
ously a subset of all kinds of pitch movements, which appear in Indian music. This
is a musical term that refers to a continuous sliding pitch from one melodic note to
another. It is likely that meends play a substantive aesthetic role in the class known
as melodic music, such as Indian music. Not much reliable information is available
on different kinds of meends in literature. As transitory pitch movements these
meends may range from a simple span of two notes to a whole octave. Similarly,
one may expect the basic ones to be straightforward, smooth and uni-directional i.e.
either ascending or descending. However, a complex one may be a combination of
both. A second type of meend may be perceived to have the touch of a note in
between. This may be caused either by a slight pause on one or more intermediate
notes or even a small stretch of low rate of pitch change. The undulating meend has
an up down or wave-like movement. The available information on meends relate to
the cognitive domain. To understand the relation between cognition and the
physical events triggering them one needs to analyse a large volume of pitch
contours from actual performances. An analytical study in this area requires the
development of processing algorithms, which are non-trivial to say the least. The
detail of an algorithm which extracts meends directly from acoustic signal of
performed singing is included later. The literatures cite the following types of
meends:
• Meends with rests on Intermediate notes
This type of Meend employs a slight pause on one or more intermediate notes
within the Basic Meend. The duration of the rest may vary from one raga to
another.
• Undulating Meends
Meends can be ascending, descending or a combination of both. The third type
sometimes has an undulating or wave-like effect and may be referred to as the
Undulating Meend.
• Ghaseet
Ghaseet is a technique of plucking a note and then, within the resonance gen-
erated, gliding the other hand along the string over the frets (on the sitar) and
along the steel plate (in the case of the sarod). A Ghaseet is fast paced and differs
only in speed from other alankars in the Meend category.
• Soonth
The Soonth is a fast paced (basic) meend employed by vocalists. This usually
comes into play in the latter part of the alaap-vistaar, after the pace picks up.
84
5
Pitch Transition and Pitch Stability

It is surprising to notice that in spite of the fact that performers often verbally or
through gestures refer to the intense importance of different kinds of movements in
meends, the literature on the subject is almost silent. Unfortunately, it is certainly
very difﬁcult to extract explicit knowledge from the ruminations of musicians or the
connoisseurs of music. Added to this is the fact that the west, where the science of
music is well practiced, is not much concerned about glissando as a very important
embellishment. The onus of investigation in this area therefore rests on the
investigators of India.
5.2
Extraction of Meends
116 songs in raga Bhairavi, Todi, Darbari Kanada and Mian-ki-Malhar sung by 41
eminent singers of Hindustani music were taken for analysis. Only the alap portions
are used in the present experiment. The digitization of the signal was done at the
rate of 22,050 samples/sec (16 bits/sample).
5.3
Algorithmic Procedure
For the purpose of the present study meends are deﬁned as transitory movement of
pitch between two different valid notes greater than 300 ms in duration (as it was
felt that in alap shorter transitory movements may not generally be perceived as
meends). Furthermore, very complex pitch movements are also kept outside the
purview of the present study.
For extraction of such meends from the song signals following procedures are
used:
1. Extraction of pitch from song signal by phase space analysis Chap. 4 Smoothing
of pitch proﬁle Chap. 4.
2. Steady Pitch sequences representing sung notes are removed Chap. 4.
3. After this, small note sequences of pitch values (<0.2 s.), which have already
been removed, are copied back from the original. This reﬁlls gaps introduced by
step 3 in transitions.
4. Transitions for which start pitch (if non-zero) and end pitch (if non-zero) belong
to different valid note intervals, are retained; all other transitions are removed.
5. For each transitional sequence thus obtained, ratio of path length to chord length
is calculated. Those sequences are retained for which this ratio is less than or
equal to 5.
6. R-square values are found out for the best ﬁt 2nd degree equation to each of the
retained sequences. Sequences for which R-square values are greater than or
equal to 0.6 are retained as the ﬁnal meend sequences.
5.1
Introduction
85

Pitch is extracted from the signal ﬁles using state phase analysis. This and the
ﬁrst three steps are already presented in Chap. 4 in detail. The top section (a) of
Fig. 5.1 shows the signal and the bottom curves show examples of different phases
of operations starting from this extracted pitch (b). There are usually some misses in
the pitch extraction. This is corrected in the following manner. If two non-zero pitch
sequences are separated by a maximum of ﬁve zeroes, these are replaced through
linear interpolation (vide middle of the right half of (c) in Fig. 5.1).
Then range of pitch values falling within note intervals appropriate to the raga
rendered are zeroed; for this purpose, the pitch of the sa of the artist and the
corresponding ratios are used. For each note ±2% of the pitch is used as the interval
width for each ratio. The resulting pitch sequence is exempliﬁed in (d) of Fig. 5.1.
This process will unfortunately remove the pitch sequences which are part of the
meends simply because they happen to have values corresponding to the note
intervals. These need to be put back.
For this, small note sequences of pitch values (<200 ms) which have already
been zeroed and have meend at least at one end are copied back from the original
sequence. The resulting pitch sequence is shown in Fig. 5.2.
Fig. 5.1 Extraction of pitch from signal and some of the initial processing
86
5
Pitch Transition and Pitch Stability

Then small sequences of transitions (<300 ms) are removed assuming that in
aalap portions transitory movements less than 300 ms may not be perceived as a
meend. The effect may be visualized by comparing Fig. 5.2 with Fig. 5.3.
Further, as the gliding perceptible pitch movement between two different notes
are considered as meends, the transitions for which start pitch and end pitch belong
to different valid note intervals, are retained; all other transitions are removed
compare (a) and (b) in Fig. 5.4.
To avoid very complex transitory pitch movement like the one shown in
sequence 3 of frame (a) in Fig. 5.4 two measures have been used. The ﬁrst is the
ratio of path length to chord length. Those sequences for which this ratio is above
the predetermined threshold value of 5 are retained. The second measure relates to
the closeness of ﬁtting of the sequence to a second-degree equation. R-square
values are used for the purpose. The threshold value for R-square used is 0.6. It may
be seen from (c) of Fig. 5.4 that the ﬁrst two sequences of (b) were rejected.
5.4
Results
The total numbers of meends, extracted from all these songs, are 3328. These
exclude all transitions of durations less than 300 ms. The size of the data may be
considered as adequate. For checking the algorithm, the pitch proﬁle of a song was
printed on continuous charts with note region marked thereon. The meend regions
are manually marked there. A visual comparison of this with the output of the
Fig. 5.2 Restoration of false pitch-removal in a meend segment
Fig. 5.3 Removal of small meends
5.3
Algorithmic Procedure
87

algorithm was used to evaluate the algorithm. Table 5.1 gives the summarized
results of analysis. Though results are given separately for different ragas too much
emphasis need not be given on their variability with respect to ragas. The interesting
point to note is the overall correct identiﬁcation rate of about 84%. The smallness
(about 5%) of the false inclusion is also noticeable. It appears that the algorithm is
useful for extraction of meends from actual singing performances for the purpose of
objective categorization and subsequent contextual analysis.
Figure 5.5 shows examples different meends extracted from songs. Numbers 1,
2, 9 and 10 are simple meends both ascending and descending. Numbers 3 and 4 are
meends with touch notes. Numbers 5, 6 and 11 are meends with undulations at
ends. Number 7 is an undulated meend. Number 8 is a complex meend. No 11 may
represent a ‘dana’.
Fig. 5.4 Removal of non-meend transitional sequences
Table 5.1 Ragawise distribution of identiﬁcation of meends
Rejection
(%)
False inclusion
(%)
Correct identiﬁcation
(%)
Total
Bhairavi
21.82
3.64
78.18
893
Darbari Kanada
9.80
5.88
90.20
745
Mian-ki-Malhar
17.74
8.06
82.26
830
Todi
15.58
2.60
84.42
860
Overall
16.33
4.90
83.67
3328
88
5
Pitch Transition and Pitch Stability

5.4.1
Objective Categorisation of Meends
For this the same data base and pre-processing methods as were used in the study
presented under Sect. 8.4.1 is used except that in the present study lower durational
limit of transitory movements has been brought down to 200 ms.
Before the actual categorization is done the ﬁrst step is to subdivide a meend into
sequences of ascending, descending and ﬂat pitch movements. To do this a
non-overlapping window of 40 ms is used for determining the slope using the
method of best line ﬁt. Each of these subsequences is tested to see if they are
perceptually ﬂat by using Eq. 5.1 (Hart et al. 1990)
w ¼ abs 12 log10ðf1=f2Þ= log10ð2Þ
f
g
ð5:1Þ
where f1 and f2 are the pitches corresponding to two extrema of a subsequence.
Then if w < log10(0.04/t2), where t is the duration in seconds of the subse-
quence, this is taken as ﬂat (N). For all non-ﬂat pitch contours the method of best
line ﬁt is used for determining the slope. If the slope is +ve then the window is
labeled as R (rise) otherwise the label is F (fall). All single occurrences of a
particular label are removed. This is done as it is felt that a tone movement of less
than 80 ms may not be cognitively signiﬁcant. The label N at the beginning and at
the end of a meend, if any, is also removed. Consecutive occurrences of same labels
are merged. These operations produce a sequence of letters N, F and R. There can
only be 10 possible permutations for all sequences of length 3. The sequences of
length more than 3 are grouped under the pattern complex.
5.4.2
Results
Figure 5.6 shows examples of different types of meends occurring in the analysed
songs. The horizontal axis represents the sequence of pitch and the vertical axis
represents pitch in Hz. It may be noticed that small non-cognizable perturbations
Fig. 5.5 Examples of different types of Meends
5.4
Results
89

have been ignored while categorizing different parts of the pitch proﬁle. As such a
general rise, fall or ﬂat region may contain small irregularities of other types of
duration less than 80 ms.
Table 5.2 shows the number of occurrences of patterns of different categories
occurring in all the 116 songs. Descending meends (F) are seen to be remarkably
more abundant than the ascending ones (R) (more than 3:1). Meends of the types
hat (RF) and valley (FR) are of almost equal abundance. RFR and FRF constitute
the undulating meends and they seem to be quite abundant.
Fig. 5.6 Examples of meends of different types
Table 5.2 Number of meends for different categories of patterns
RNR
RNF
RFR
RF
R
FRF
FNR
FNF
FR
F
Complex
4
3
146
463
289
232
6
6
378
923
379
90
5
Pitch Transition and Pitch Stability

Figure 5.7 shows the frequency of occurrences of different categories of simple
meends for different duration. It may be noted that most of the meends have a
duration less than 600 ms. For most of the patterns the mode is within 300–500 ms.
There is hardly any sequence of duration less than 200 ms. In fact the occurrences
of such small sequences are less than 1% of the total number of transitory pitch
movements.
In summary one may note:
• Descending meends (F) are found to be remarkably more abundant.
• The duration of most of the meends are found to be within 300–500 ms.
• Less than 1% of the total number of meends have a duration less than 200 ms.
5.4.3
More Details on Intonation
As it is revealed in the study of actual performances in Hindustani music the rising
and falling intonation is by far the largest type of meends used. It therefore seems
relevant to investigate further details in these two categories. For this purpose ten
songs sung by two eminent singers of Hindustani music are taken for analysis. The
database constitutes of 65 min of recording ofthese songs. The database for the
analysis includes one song each in raga Bahar, Mian ki Mallhar, Basant, Bhairav,
and 2 songs each in Darbari Kannada, Jaunpuri, and Purbi. The extraction of
meends using procedure described earlier yielded altogether 1224 total transitional
movements. An algorithm which compares subsequent values of pitch is used to
classify sequences into ﬁve different categories (rise, fall, hat, valley and mixed).
Figures 5.8, 5.9, 5.10 and 5.11 gives some examples of these categories.
Fig. 5.7 Frequency of occurrences of different categories of simple meends for different duration
5.4
Results
91

5.4.4
Results
The numbers of sequences in each of the ﬁve broad categories are mentioned in
Table 5.3.
All these categories exist in signiﬁcant numbers. Each of these categories may be
subdivided further, rise and fall may be classiﬁed into three categories, and namely
concave, convex and linear (Figs. 5.8 and 5.9). Similarly the hat and valley like
transitions (Fig. 5.10) may be subcategorized by the broad categories of shape like
the asymmetry and sharpness.
Fig. 5.8 Examples of rising intonation
Fig. 5.9 Examples of falling intonation
Fig. 5.10 Examples of hats and valleys
Fig. 5.11 Examples of
mixed transition
92
5
Pitch Transition and Pitch Stability

For all patterns falling under the categories of rise and fall the sequences are
examined to decide the three subcategories namely concave, convex and linear. For
the categories of hat and valley skewness and kurtosis may be used for making
further subcategories.
Thus each of these categories is subdivided into four subcategories namely,
high/deep, low/shallow, and steep at left steep at right. Tables 5.4 and 5.5 present
the relative numbers/percentages of various subcategories. All subcategories have
signiﬁcant population. In rise and fall a large number of the transitions are linear in
nature.
5.4.5
Discussions
The categorizations of transitions done above are based solely on the shape of the
pitch proﬁle. It appears that a simple algorithmic approach based on signal pro-
cessing techniques alone can be used effectively to categorize the transitions
between notes. It however remains to be seen whether such categories have any
cognitive relevance in the musical sense. For this one needs to use the paradigm of
analysis by synthesis. These separated transitions of each type may be used to
produce sound signals preferably using the vowel /a/ and be judged aurally by a
group of musicians. The different subcategories for simple transitions like rise and
Table 5.3 Distribution of
transitions for different
categories
Category
Number
Rise
700
Fall
670
Hat
550
Valley
451
Mixed
458
Total
2829
Table 5.4 Distribution of
segments rise and fall into
subcategories
Category
Rise
Fall
Number
%
Number
%
Concave
59
19.47
64
19.51
Convex
46
15.18
83
25.30
Linear
198
65.35
143
43.60
Table 5.5 Distribution of
segments hat and valley into
subcategories
Category
Left
Right
Low/shallow
High/deep
Valley
31
69
78
22
Hat
18
82
23
77
5.4
Results
93

fall are expected to give different colors to the intonation. To what extent this
happens can be seen only after performing the actual audition experiment.
The other complex transitions under the categories of hat, valley and mixed
could again be perceived either as a single entity or as different entities separated by
the extrema, the extrema being perceived as a touch note. All these require per-
ception experiment involving active participation of musicians.
5.5
On Perceptibility of Transitory Movements
The aforesaid objective studies on meend need to be buttressed by subjective
cognitive studies on the meaningfulness of these objective classes. It seems perti-
nent in this connection to mention the results and implication of such a study
conducted recently. This study concentrated on the use of expressive dynamics in a
particular type of note-to-note transitions in Hindustani vocal music. It is necessary
to know what exactly constitutes the cognition of pitch movement as against that of
a note. There exists some knowledge about such cognitive processes in relation to
pitch movement in speech domain which categorizes intonation patterns into dif-
ferent classes like ﬂat, rising, falling, hat, valley, etc. These relate to the nature of
the utterance, the mood, emphasis, etc. It is necessary to understand to what extent
such categories or some similar ones are cognitively relevant in establishing the
aesthetics of raga in Hindusthani music (Sengupta et al. 2006). Unfortunately, we
do not even know whether these different types of transition are perceptible or not.
This section presents a synopsis of such a preliminary study.
Expressions in performance of Indian music have been a topic of research for
quite some time now. Most recently the works of Clarke, Palmer and Repp gave
some understanding of the performance and perception of music in general (Clarke
1985; Palmer 1992; Repp 1995a, b). The present work concentrates on the use of
expressive dynamics in a particular type of note-to-note transitions (meend) in
Hindustani vocal music. The availability of modern signal processing technology
made it relatively easy to measure and to quantify performance information. The
study uses analysis through synthesis paradigm. Seven different chosen categories
of pitch movement are generated for each of falling and rising intonation. Of these
contours one is a linear movement the other two are convex and concave types each
of three different degrees. These are used to synthesize singing of vowel /a/ between
two steady pitch states. 12 subjects (listeners) are selected for the perception test.
5.5.1
Experimental Procedure
The objective of the perception test is to know to what extent differently shaped
pitch contours simulating actual meends are differentiated in isolation by the lis-
tener who is a musician himself. To do this pre-designed shaped pitch contours are
94
5
Pitch Transition and Pitch Stability

put between a pair of notes. The pair of notes are kept identical in terms of pitch,
intensity and duration for all contours. A speech synthesizer (Datta et al. 1990; Dan
et al. 1993) is used to synthesize these pitch movements on vowel /a/ produced by a
male informant.
Three different degrees of concavity and convexity are generated between two
steady pitch states of 150 and 170 Hz for both falling and rising intonation using
second-degree equations of the form y = ax2 + bx + c. The different values of the
coefﬁcients a, b and c are presented in Table 5.6. The graphical nature of the
movements is shown in Figs. 5.12, 5.13, 5.14 and 5.15. Two other movements,
linear rise and fall, are also generated (Fig. 5.16). About 60 ms of steady states are
added to both ends of the aforesaid 15 pitch movements.
A song synthesizer (Sengupta et al. 2006; Datta et al. 1990) is used to synthesize
these pitch movements on a male vowel /a/. Two sets of signal pairs, one for rising
movement and the other for falling movement, are formed using all possible
combinations. Each set consists of seven different intonation patterns described
earlier for each of the two categories namely rise and fall. The signals in each pair
are separated by a gap of two seconds. There are twenty-eight pairs in each of the
two categories namely rise and fall. All these signals are used in the perceptual
evaluation. The listeners are presented with three repetitions of each pair. The task
for the listener is to judge if they can distinguish the two signals. They are required
to put their opinions in the score sheets immediately after listening to a pair using a
four-point scale. The four grades are equal (E), nearly equal (*), distinct (N) and
confusing (indicated by blank). The confusion occurs when the listener cannot
decide on any of the ﬁrst three values even after listening to three repetitions.
Before the actual listening experiment, a test for consistency and reliability of the
listeners is performed. For this a new set of signal pairs are constructed wherein
some pairs containing same signal are randomly inserted. These signals were
presented to 12 listeners. Of these 12 listeners 2 were rejected because of their poor
assessment of pairs, which contained same signals. Out of the remaining 10, 2 are
non-musicians.
Table 5.6 Coefﬁcients of the equation generating different curves
Degree 1
Degree 2
Degree 3
Co-efﬁcients
a
b
c
a
b
c
a
b
c
Concave fall
0.004
−0.585
171.89
0.006
−0.713
171.72
0.009
−0.864
172.02
Convex fall
−0.002
−0.146
170.21
−0.005
−0.005
170.95
−0.007
0.151
171.16
Linear fall
0
−0.247
170.25
Concave rise
0.0082
−0.161
150.79
0.006
−0.069
151.61
0.004
0.05
152.06
Convex rise
−0.002
0.413
150.6
−0.004
0.594
150.26
−0.006
0.736
150.56
Linear rise
0
0.247
149.75
5.5
On Perceptibility of Transitory Movements
95

5.6
Results and Discussions
As has already been mentioned purpose of the study is to ﬁnd out to what extent
different transitory movements are perceptually differentiated by the listeners. The
results of perceptual evaluations are collated in the form of upper triangular
matrices for each listener in tabular format. Table 5.7 presents one such table. For
each of the ten selected listeners there are two matrices one for rise and the other for
Fig. 5.12 Different degrees
of concave fall movements
Fig. 5.13 Different degrees
of convex fall movements
Fig. 5.14 Different degrees
of concave rise movements
96
5
Pitch Transition and Pitch Stability

fall. In the matrices v and x indicate concave and convex pitch curves respectively.
The associated numerals indicate the degrees of curvature. ‘l’ indicates linear
movements.
For the analysis, the perceived data for all the informants are pooled to form
Tables 5.8 and 5.9 respectively for falling and rising intonations. These again are
upper triangular matrices. The contents of the cells represent the number of listeners
who could distinguish between the indicated pairs. If a data sheet for pair indicates
that the listener has marked it as either ‘*’ or ‘N’ it is considered as a differentiated
Fig. 5.15 Different degrees
of convex rise movements
Fig. 5.16 Linear movements
Table 5.7 Perceptual
evaluation (Listener 1)
Fall
2nd signal
1st signal
l
v1
v2
v3
x1
x2
x3
l
E
E
*
*
N
N
N
v1
E
*
*
N
N
N
v2
E
*
N
N
N
v3
E
N
N
N
x1
E
E
*
x2
E
E
x3
*
5.6
Results and Discussions
97

pair. As the number of selected listeners is 10 a score of 6 and above is considered
signiﬁcant and the entries are made bold.
In total 280 pairs of signals each for the two intonations namely rise and fall. Of
these the number of signals those have been differentiated are 154 for falling
intonation and 142 for the rising intonation. Thus grossly the differentiation is
almost chance directed. However meaningful differentiability begins to peep out
when we go for detail analysis.
It may be interesting to note that the linear transitions are not generally differ-
entiated from the concave type of movement be it rise or fall. However these could
be mostly separable from convex type of movements. It is seen that the concave
movements and convex movements are well differentiated. It may be seen that there
is very little differentiations between different degrees of concave intonations.
Convex movement of degree three seem to be well separated from that of degree
one. For falling concave intonations degree three seems to be separable from degree
one for majority of listeners.
5.7
Summary
The study, within the limitation of small number of subjects and a non-musical
context, throws some interesting cognitive behaviour. There are two distinctly
separable intonation patterns, namely convex and concave ones. Linear intonations
Table 5.8 Discrimination
matrix for rising intonation
Fall
2nd signal
1st signal
v3
v2
v1
l
x1
x2
x3
v3
2
3
6
7
9
9
9
v2
2
5
3
7
9
10
v1
2
2
6
9
9
l
2
8
9
9
x1
0
3
6
x2
4
1
x3
3
Table 5.9 Discrimination
matrix for falling intonation
Rise
2nd signal
1st signal
v3
v2
v1
v0
x1
x2
x3
v3
1
3
1
2
6
8
10
v2
2
2
1
10
10
9
v1
1
1
10
10
9
v0
2
3
10
10
x1
1
4
9
x2
1
4
x3
2
98
5
Pitch Transition and Pitch Stability

are indistinguishable from concave ones. Only two extreme degrees of curvatures in
convex intonation show some perceptual differentiability. This is a preliminary
study. It only considers two basic simple natures of pitch movements.
It is technically possible to analyse large amount of songs to extract meends and
categorise them into different pattern groups. However to what extent such cate-
gorisation is meaningful in cognitive sense requires elaborate well designed per-
ception experiments. The last experiment indicates the possibility of designing
more elaborate perception experiments to empirically understand the role of various
pattern groups of meends in the aesthetics of Hindustani music.
5.8
Conclusion
Meends (glissando) are integral to Indian classical music as embellishments to
enhance the aesthetics. Whatever is presented in this chapter can at best be termed
as an attempt to outline the objective approach of understanding pure objective
nature of the movements of pitch associated with the phenomena. However this is
only a part of the whole story. To what extent the objective classes we can dis-
tinguish are relevant to the aesthetics of music constitute the other subjective
part. We have also presented a pilot study to show how one can proceed to examine
the subjective relevance in terms of cognizability of such objective classes. The
main task of establishing the aesthetic relevance of different major types of meends
as found from the analysis of large song data base remains unexplored. One hopes
that this will be taken up in the future. However one may hold the view that all the
nuances of the classical music including meends has been time-tested through
decades of appreciation of the listeners and therefore the features revealed con-
sistently in large databases are meaningful themselves and their analysis is also
meaningful.
References
Clarke EF (1985) Structure and expression in rhythmic performance. In: Howell P, Cross I, West R
(eds) Musical structure and cognition. Academic, London, pp 209–36
Dan T, Mukherjee B, Datta AK (1993) Temporal approach for synthesis of singing (SopranoI). In:
Proceedings of the StockholmMusic acoustics conference (SMAC93), pp 282–287
Datta AK, Ganguly NR, Mukherjee B (1990) Intonation in segment-concatenated speech. In:
Proceedings of the ESCA workshop on speech synthesis. Autrans, France, pp 153–156,
September 1990
Hart JT, Collier R, Cohen A (1990) A perceptual study of intonation, an experimental phonetic
approach to speech melody. Cambridge Studies in Speech Science and Communication
ITCSRA. http://www.itcsra.org/alankar/alankar.html
Lemstrom K, Laine P (1998) Musical information retrieval using musical parameters. In:
Proceedings of the international computer music conference, Ann Arbor, USA, pp 341–348
Nettheim N (1992) On the spectral analysis of melody. J New Music Res 21:135–1481
5.7
Summary
99

Palmer C (1992) The role of interpretive preferences in music performance. In: Jones MR,
Holleran S (eds) Cognitive bases of musical communication. American Psychological
Association, Washington, DC, pp 249–62
Repp BH (1995a) Detectability of duration and intensity increments in melody tones: a partial
connection between music perception and performance. Percept Psychophys 57:1217–1232
Repp BH (1995b) Quantitative effects of global tempo on expressive timing in music performance:
some perceptual evidence. MusicPercept 13:39–57
Sengupta R, Dey N, Nag D, Datta AK (2006) Extraction and relevance of transitory pitch
movements in Hindusthani music. In: National symposium on acoustics, New Delhi,
November, 2006
Sengupta R, Dey N, Nag D, Datta AK (2002) Perceptual evaluation of tanpura from the sound
signals and its objective quantiﬁcation using spectral features. J Acoust Soc India 30
Strawn J (1985) Modeling musical transitions. Ph.D. dissertation, Stanford University, Stanford, CA
100
5
Pitch Transition and Pitch Stability

Chapter 6
Raga Identiﬁcation
6.1
Introduction
The deﬁnition of a raga is somewhat nebulous and verbose. In Indian musical
system, raga is deﬁned as a melodic structure with ﬁxed notes and a set of rules that
characterizes a particular mood conveyed by performance (see Chakraborty et al.
2014; Menon 2007; Priyamvada 2007; Jones 2016). Ragas consist of a ﬁxed set of
ﬁve or more musical notes. Ragas (in Sanskrit it is known as colour or passion), are
supposed to evoke various moods in the listener. In Hindustani music, there are
certain ragas which are speciﬁc to different season or time of the day. The monsoon
ragas belongs to the Malhar group and are mainly performed during the rains,
while we have morning ragas, such as Bibhas, Bhairav and night ragas, such as
Kedar, Malkauns. However, not all musicians accept the time theory of ragas.
The Hindustani and Carnatic classical music systems usually have different
ragas. There are some ragas which are similar but use different names in both
systems. Others have similar names, but different in their actual form. Also,
Hindustani classical music classiﬁes ragas into ten parent raga groups (or thaats),
as organized by Vishnu Narayan Bhatkhande in the early 1900s (Bhatkhande
1995). The Carnatic system, on the other hand, depends on an older classiﬁcation
having 72 parent raga groups called melakarta.
6.2
Swars or Notes (To Be Used in Ragas)
A small letter indicates a komal swara (ﬂat) as komal Re is r etc. and a capital letter
represents a sudh swara (natural) as sudh Re or R. Sa the tonic and Pa are always
sudh swars abbreviated as S and P respectively. But there is the only exception of
M representing sudh Ma and m as tivra (sharp) Ma. Further, a note in italics,
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_6
101

normal and bold font stands for the notes in lower octave, middle octave and
higher octave respectively. Therefore the notes notations are grouped as:
Shudh notes: SA, RE, GA, MA, PA, DHA, NI notated as S, R, G, M, P, D, N in the
middle octave.
Komal and Tivra Notes: Komal Re, Komal Ga, Komal Dha and Komal Ni notated
as r, g, d, n and Teevra Ma notated as m in the middle octave.
Remark: In case the raga makes use of a microtone or a shruti swar such as the ati
komal Ga of raga Darbari or the ati komal Re of Bhairav, we shall use the notation
for komal swar only i.e. small letter remembering that note in the raga is actually ati
komal and not komal. This means if Sa is taken at natural C, then ati komal Re is a
note ﬂatter than D ﬂat (komal Re) and ati komal Ga is a note ﬂatter than E ﬂat
(komal Ga).
6.2.1
Raga Structure
In the context of raga identiﬁcation, Chordia and Rae (2007) have used pitch-class
distribution (PDC) and pitch-class dyad distributions (PDCC) for feature extraction
and classiﬁcation was performed using support vector machines. Another method of
raga identiﬁcation is to use Hidden Markov Model (Shahin 2004). In our work, the
music transcription software Solo Explorer 1.0 has been used to plot the musical
transcriptions with time versus fundamental frequencies of music notes in Western
notation. The ragas Todi, Yaman, Rageshree, Marwa and Ahir-bhairava are
studied practically for their automatic identiﬁcation.
Statistical features refer to onset, duration, pitch and amplitude of notes. These
features form the numerical data. Notes are characterized by pitch. Note duration
helps in knowing the stay of notes. Onset helps in getting the note sequences while
amplitude gives the loudness. Solo Explorer helps in analyzing the melodic
structures of ragas by generating the note sequences. But for melody analysis of
performance [140], RUBATO is a software developed by the Swiss mathematician
and musicologist professor Guerino Mazzola described in Chap. 3 of the book
Chakraborty et al. (2014) and is used to compare Bhairav and Bihag in Chap. 8 of
the same book. The general features in the description of the above-mentioned
ragas are given in Table 6.1.
6.2.2
Quantiﬁed Features of Raga
The study of above ragas deﬁnes the following features that are sufﬁcient to
identify the structure or performance of the raga. The quantiﬁed features that are
considered in this section of study are as follows:
102
6
Raga Identiﬁcation

Count of notes
This feature is deﬁned as the number of times a note arrives in the signal. This
feature is selected because it gives some idea about the Vadi and Samvadi and
anuvadi notes as various notes do not have the same level of signiﬁcance. Vadi is
Table 6.1 Raga features
Raga
features
Name of the Raga
Todi
Yaman raga
Rageshree raga
Marwa
raga
Ahir-Bhairav
raga
Thaat
Todi
Kalyan
Khambaj
Marwa
Not speciﬁc,
generally
taken as
“Mishra
Bhairav”
Aroh
S, r, g, m, P, d,
N, S
N R G, m P,
D, N, S
S G M D N S
S r G m
D N D S
S r G M P D
n S
Awaro
S, N, d, P, m, g,
r, g, r, S
S N D, P,
mG,RS
{S n D M G}
{M R} S
S N D m
G r S
S n D P M G
r S
Jati
Sampooorna-
Sampoorna
Sampooorna-
Sampoorna
Aurabh-Sarabh
(ﬁve distinct
notes used in
ascent; six
distinct notes
used in descent)
Sarabh -
Sarabh
Sampooorna-
Sampoorna
Vadi Swar
D
G
M (some say G)
r
M
Samvadi
Swar
G
N
S (some say N)
D
S
Prakriti
Restful and
serious
Restful
Restful
Restless
Restful
Pakad
d n S, r, g, rS,
mg, rg, rS
N R G R,
N R S
{G M R S}, {n
D} {N S} G M
D m G r
G m G r
S
S, r G M, G
M r, n D, n r
S
Stay notes
r, g, d
R G N P
G N M S
r G D
M and S
Peshkash
Second phase of
morning (9
AM-12
O’clock)
First phase of
night (6 PM
to 9 PM)
Second phase of
night (9 PM to
12 PM)
Third
phase of
morning
(12 noon
to 3 PM)
First phase of
morning
(6 AM to
9 AM)
Additional
information
of the raga
Pa (P) is a weak
note. This raga.
is also called
Mian ki Todi
created by Mian
Tansen
This raga
was created
by Amir
Khusru
6.2
Swars or Notes (To Be Used in Ragas)
103

the most important note in a raga and samvadi is the second most important note in
a raga. Anuvadi is an important note in the raga which is different from Vadi and
Samvadi.
Average duration of notes
The second feature that is extracted from the raga is the average time duration
for a note in the raga. It gives the idea about the raga structure. Since every raga
will have a ﬁxed set of notes in it, so to completely express a raga these funda-
mental notes must be present. From the note duration we get the idea about the
notes most stayed in the raga and the notes present in the raga. See also Lentz
(1961), Datta and Kundu (1991).
Onset of notes
This is one of the most important features, which gives the time instance when a
note is detected in the signal and therefore helps in obtaining the sequence of the
notes. The note onset gives the idea about the aroh (ascent) and awaroh (descent)
of the raga.
6.3
Identiﬁcation
The task of a raga identiﬁcation system is to identify the given unknown raga into
the type of raga. This book presents three approach for raga identiﬁcation. The
statistical framework allows the problem of raga identiﬁer to be decomposed into a
set of well deﬁned sub-tasks, namely the extraction of the relevant features from the
acoustic signal, the statistical modelling of the acoustic and linguistic knowledge,
and the combination of all information in an efﬁcient search for the most probable
type of raga. The two most critical building blocks are the feature extraction model
and the search-engine (Identiﬁer). See also Chordia and Rae (2007).
6.3.1
Process of Feature Extraction and Database Building
The main software for feature extraction is Solo Explorer 1.0 which gives the time
and fundamental frequency data. Figure 6.1a shows the western musical notation
on time versus fundamental frequency, when basic notes (S, r, R, g, G, M, m, P, d,
D, n, N) are played on harmonium, sitar and ﬂute.
From the above Figs. 6.1a–c, it is observed that the fundamental frequency
increases as higher notes are played. Further it is clear that, the fundamental fre-
quency of a note is not exactly the same for different instruments. Therefore a
104
6
Raga Identiﬁcation

Fig. 6.1 a Basic notes played on Harmonium. b Basic notes played on Sitar. c Basic notes played
on ﬂute
6.3
Identiﬁcation
105

common scale has to be selected for every instrument played. A program is
developed to ﬁnd the frequency component present in the signal with respect to its
time. All the samples taken from the instrument (Harmonium) are set to ‘C’ scale as
‘Sa’ because of the following reasons:
– Ragas don’t depend on where the tonic (base note i.e. SA) is taken so there is no
loss of generality.
– The twelve notes Sa to sudh Ni are assigned pitch values 0–11 respectively for
the middle octave and this is done assuming Sa is at C, these pitch values can be
used for statistical analysis of melody (Chakraborty et al. 2008; Chakraborty
et al. 2014).
The frequencies of all the swaras (notes) are noted in Table 6.2.
There are several steps involved in the process of Feature Extraction and
Database building. Figure 6.2 shows the procedure for feature extraction.
Count of notes
With standard note fundamental frequencies, the notes are detected and hence
their count. Figure 6.3a, b shows the count of notes for sixty-second performance
graphs of Ahir-bhairav and Ragheshree ragas; this graph shows the overall distri-
bution of notes and whether the distribution is maintained in parts, as shown by
ﬁrst, middle and last 30 s graphs. Similarly the data for other ragas are obtained.
Table 6.2 Notes frequency table
Notes
Lower octave frequency
(in Hz)
Middle octave frequency
(in Hz)
Upper octave frequency
(in Hz)
Sa (S)
120
240
480
Komal Re (r)
129.6
259.2
518.4
Sudh Re (R)
135
270
540
Komal Ga (g)
144
288
576
Sudh Ga (G)
150.5
301
602
Sudh Ma (M)
160
320
640
Tivra Ma (m)
172.035
344.07
688.14
Pa
180
360
720
Komal Dha
194.4
388.8
777.6
Sudh Dha
202.5
405
810
Komal Ni
216
432
864
Sudh Ni
226
452
904
106
6
Raga Identiﬁcation

Note Onset
The onset of the note refers to its point of arrival in the performance.
Figure 6.4a, b show the comparison between the ﬁrst note arrival to second note
arrival and so on, which clearly indicates the difference in rendering of the notes.
Inter onset interval or IOI refers to the time difference between arrivals of two
successive notes. Notes are in rhythm if their IOI are equal.
Average duration of notes
The average duration is calculated by taking the sum of difference between the
note arrival time and note departure time of a note rendered in the whole perfor-
mance divided by the total count of that note. Figure 6.5a, b below shows the
duration of each note occurrences in the whole performance. The stay note will
have more number of higher peaks compared to non-stay notes.
After calculating all these features a matrix is created. This creates a database for
training and testing. Other than solo explorer software, one can also use the
algorithms developed by other researchers (see e.g. Datta 1996, 1997).
Fig. 6.2 Feature extraction process
6.3
Identiﬁcation
107

6.4
Recognition of Ragas
The ragas can be recognised from the features extracted. Similar to speech samples,
music samples are dynamic in behaviour, but the features are not always giving the
same value on each samples. From a previous study on speech recognition, it is
observed that this is a problem of deﬁning a class in a group, as this is attempted
earlier with the classiﬁcation technique using PNN (Probabilistic Neural Network)
and is found to be most suitable (if memory space is not a criteria). PNN is a four
Fig. 6.3 a Count of notes in Ahir-bhairav raga. b Count of Notes in Ragheshwari raga
108
6
Raga Identiﬁcation

layer structure. The number of the neurons in the input layer depends on the size of
feature vector. In a music sample, three features (count of a note, onset of a note and
average duration of a note) for each note are computed. There is a maximum of 12
notes (Sa to Ni) in each raga sample (irrespective of the octave). Hence the size of
the input feature vector is 36 considering lower, middle and higher octaves. If a
particular note is not present in that raga then all the three features are taken as zero.
The training feature vector sets are stored in the pattern layer. The experiment is
performed for ﬁve different ragas and 8 samples per class is taken as the training set.
Hence number of neurons in the pattern layer will be 8 * 5 = 40 and the size of the
neuron is same as that in the input layer. The summation results as shown in Eq. 6.1
Fig. 6.4 a Onset of notes in Ahir-bhiarav raga. b Onset of Notes in Rageshree raga
6.4
Recognition of Ragas
109

of each class are stored in summation layer. Hence the number of neuron in the
summation layer is 5. Maximum value of the neuron is transferred to the output
layer. A particular input will fall in that class when summation value is maximum.
giðXÞ ¼ 1
ni
X
ni
k¼1
e
XXik
k
k2
r2
ð6:1Þ
Fig. 6.5 a Average duration of notes in Ahir-bhairav raga. b Average duration of notes in
Ragheshwari raga
110
6
Raga Identiﬁcation

where
X
unknown (input)
Xik
“kth” sample
n
number of training inputs
r
smoothing parameter
The following Table 6.3 gives the architectural details of PNN.
Fig. 6.5 (continued)
6.4
Recognition of Ragas
111

6.5
Experiments and Results
6.5.1
Experimental Parameters
In this work the ragas todi, yaman, ragheshari, ahir-bhairav and marwa are played
on Sitar by two musicians in different time and place.
The following speciﬁcations are maintained during the experiment
• Time duration 60 s.
• Sampling rate 10000 samples/s.
6.5.2
Results
Application software is developed as a graphical user interface (GUI) for ease
access. This GUI allows the user to select the unknown raga from an input ﬁle for
testing and training purposes as shown in Fig. 6.6. This software is very useful for
the person who has no idea about understanding of the raga. For the identiﬁcation
of a raga, a user has to select the unknown raga ﬁle and run the GUI.
6.5.3
Identiﬁcation Accuracy
Feature versus accuracy analysis
This section deals with the training of the PNN. Figure 6.7 states that as the
number of feature increases the amount of accuracy also increases. But it doesn’t
mean that if we increase some more features the accuracy will necessarily increase
further. This is because, as mentioned earlier, the accuracy is also dependent on the
training data. Hence the conclusion is that the selection of features as well as the
amount of training data samples are both crucial determining the accuracy of
identiﬁcation. Therefore it has to be selected very carefully.
Table 6.3 Structure of PNN
Input
layer
Pattern layer
Summation
layer
Output
layer
36
Neurons
5  8
Neurons
5 Neurons
1 Neurons
Size of each neurons
36
8
1
1
112
6
Raga Identiﬁcation

6.5.4
Identiﬁcation Versus Accuracy
Here the accuracy varies according to the raga. It happens as the ragas are taken
only from two different musicians. Therefore it requires more samples to train the
Fig. 6.6 GUI (Graphical user interface) application program for Hindustani raga identiﬁcation
Fig. 6.7 Features versus average identiﬁcation accuracy
6.5
Experiments and Results
113

system. It can also be improved by increasing more features, which imposes
ﬁne-tuning of the system as shown in Fig. 6.8.
This work demonstrated the use of PNN for automatic identiﬁcation of ragas
which is attempted for the ﬁrst time in music theory and when compared with other
researchers work (e.g. Chordia and Rae 2007), it is found that this technique gives
better average accuracy as 96%. The technique is efﬁcient enough to identify the
ragas. It can be employed to any type of audio signal for ﬁnding out the frequency
value at each and every instance of time. PNN employed in this work makes it
possible to build the database of ragas and makes the system quick and robust.
There are many type of studies performed to understand the characteristics of ragas
and observed many interesting results. The statistical study on the raga gives the
idea of the emotion among the raga, which is a factor to recognize raga due to its
great variable nature not selected as a feature. Here the software reference taken
from software named Solo Explorer for obtaining frequency and time of each
occurrence of notes and it was observed that the system is much more ﬂexible to the
input. It can classify input taken into noisy environment. This model sets an
example of how intelligent techniques can be used in identiﬁcation. The system is
very conductive to improvement and can be ﬁne-tuned to improve over time when
more efﬁcient parameters become available.
It is observed that this system is only 50% efﬁcient if only one feature i.e. count
of notes is used. In case of two features it gives 70% accuracy and with three
features it gives 96% accuracy. This efﬁciency entirely depends upon the proﬁ-
ciency of the system designer and the feature selected.
Fig. 6.8 Identiﬁcation accuracy
114
6
Raga Identiﬁcation

6.6
Raga Similarity
Very little work has taken place in the area of applying techniques from compu-
tational musicology and artiﬁcial intelligence to the realm of Indian classical music.
Of special interest to us is the work done by Sahasrabuddhe et al. (Sahasrabuddhe
1994; Upadhye 1992). Hidden Markov Models (Rabiner 1989) are now widely
used to model signals whose functions are not known. A raga too can be considered
to be a class of signals and can be modelled by HMM. A pakad is a catch-phrase of
the raga, with each raga having a different pakad. Most people claim that they
identify the raga by identifying the pakad of the raga. However, it is not necessary
for a pakad be sung without any adverse effect in a raga performance. Approximate
string matching algorithms designed speciﬁcally for computer musicology, such as
the one by Iliopoulos and Kurokawa (2002) for musical melodic recognition with
scope for gaps between independent pieces of music, seems more relevant. Other
relevant works which deserve mention here are the ones on query by humming
(Ghias et al. 1995) and Music Genre Classiﬁcation (Tzanetakis et al. 2001;
Deshpande et al. 2001). A raga is characterised by several attributes, like its vaadi
samvaadi, aaroha-avroha and pakad, besides the sequence of notes, which denotes
it. It is important to note here that no two classical performances of the same raga,
even two performances by the same artist, will be identical in the sense that same
sequences of notes with same duration are used. However it is often said that a
musician can always identify it simply from the image it creates. What image?
Different musician’s gives different verbal, gestural information often associated
with a nice story. Does such an image have any common objectivity? We shall see
that a some objective form can be possibly deﬁned using the relative abundance of
different notes, which loosely relates to vadi, samvadi and anuvadi relations.
6.6.1
Experimental Details
Altogether four ragas namely, Bhairav, Darbari-Kannada, Mian-ki-Malhar and Todi
were selected (Datta et al. 2012). The ragas were sung by 8 maestros. The rendered
ragas were digitized at 22050 samples/s, 16 bits per sample. For our analysis the
aalap part of each singer was selected from each raga. Pieces of aalap for each
singer for a raga were taken out from the complete aalap deleting the bandish
part. These constituted the aalap signal ﬁles for a singer for each raga. For each
raga a singer had one aalap signal of *3–5 min. Pre-processing of the signal
consists of standard noise cleaning and amplitude normalization. The pitch proﬁle is
extracted using state-phase approach (Datta 1996), which gives pitch values (in
Hz.) for each pitch period and were stored as 32 ‘cep ﬁles’. The smoothing oper-
ation (Datta et al. 2003) was done on the above pitch pattern ﬁles. The resulting
pitch proﬁles were put into 32 ‘pit ﬁles’. A skilled musician listened to the signal
ﬁles one after another to perceptually detect the position of tonic (Sa) in the ﬁle.
6.6
Raga Similarity
115

By FFT analysis, using a standard software package, at the detected region, Sa for
the respective signal ﬁle was calculated. All pitch values in each pitch proﬁle is
divided by the Sa of the song and is folded back to middle octave (1 to less than 2),
if necessary. The inverse of pitch period converted in millisecond and is used as
durational values. The whole octave is divided into 1200 bins of width 1 cent.
Frequency distribution of these ratios in a raga uses the total time corresponding to
the pitch values in each bin. Figure 6.9 shows one example of such distribution
from raga Bhairav.
6.6.2
Results
There are in all 32 pitch proﬁles corresponding to 8 singers and 4 ragas. As already
mentioned these proﬁles are normalized with respect to the Sa of each ﬁle and
folded back on to the middle octave to get the ﬁnal pitch proﬁles for the present
study. One way to test the similarities and the dissimilarities is to test the correlation
between the frequency distributions. As the concept of raga portrayed by the notes
alone, not other elements like embellishments etc., is the primary object of the
present study it must be noted that the frequency distribution should ideally have
been taken for the sung note positions only. There is some adulteration in the
frequency distributions as the raw pitch proﬁles contain pitch values pertaining to
transition between notes also. This adulteration requires that normal value of 0.75
recommended for goodness of correlation and that of 0.25 for bad correlation need
to be changed. Considering this the threshold value of 0.5 has been taken for the
present study. The total number of correlation between the pitch proﬁles are 496 of
which 112 pertains to those between same ragas sung by different singers. It is
interesting to note that 102 out 112 exhibited correlation? 0.5 Which means that in
91% cases these ragas shows similarity between them? On the other hand out of the
total number of 384 cross correlations i.e., correlations between pitch proﬁles of
different ragas, 253 showed poor correlation (<0.5). This comes to about 66%. This
being not encouraging requires a further investigation. Table 6.4 below shows
pair-wise distribution of the unexpected similarities. The number of cross correla-
tion between the songs of two ragas in a pair is 64. The erroneous similarity occurs
Fig. 6.9 Example of frequency distribution from raga Bhairav
116
6
Raga Identiﬁcation

when one song of a raga shows a correlation 0.5 with a song of another raga. The
number such errors divided by 64 expressed in percentage is given in column 3 of
the table. Ideally this should be zero for a pair.
The erroneous similarity reﬂected by correlation is highest between Todi and
Bhairav. It is second highest between Darbari-Kannada and Mian-ki-Malhar. The
same is, however less than 20% between Mian-ki-Malhar on one hand and Bhairav
and Todi on the other hand. Figure 6.10 presents the distributions of time a note is
sung in the selected pieces of songs for each of the 4 different ragas. An average of
all the songs is also included to facilitate comparison. The prominences may be
visualised from the peaks in the curves. The symbols used for the pure notes begin
Table 6.4 Pair-wise
percentage of erroneous
similarities
Raga pair
Error in %
Bhairav
Todi
67.20
Darbari-Kannada
Mian-ki-Malhar
42.18
Bhairav
Darbari-Kannada
34.39
Darbari-Kannada
Todi
28.09
Bhairav
Mian-ki-Malhar
17.15
Mian-ki-Malhar
Todi
15.66
Fig. 6.10 Time distributions of notes sung for each of the 4 ragas
6.6
Raga Similarity
117

with a capital letter and the altered notes with small letters. For example, most
prominent note in Bhairav generally appears to be ‘Pa’ and the next position is
occupied by ‘Ma’ and ‘re’. The notable exception is for singer 2 who used ‘re’ most
prominently. For Mian-ki-Malhar the most prominent note is ‘Ni’ and the second
position is occupied by ‘Pa’ and ‘Re’. The notable exception is for singer 3, the
prominence being shifted to ‘Pa’. For Darbari-Kannada however the most promi-
nent note is ‘Re’ followed by ‘Pa’. Here too the exceptions are for singer 3 and 4
where the prominence is again shifted to ‘Pa’. In the case of Todi the most
prominent note seems to be’re’ followed by ‘Pa’. It appears that ‘Pa’ always
occupies some position of prominence. Only when an examination of all major
ragas is made, one can decide the efﬁcacy of ‘Pa’ as a note for discrimination
(Fig. 6.11).
Bhairav presents the very distinctive hill covering ‘Ga’ and ‘Ma’ whereas Todi
hill covers ‘ma - dha’.While Darbari-Kannada and Mian-ki-Malhar have hills in
same positions the only distinctive feature seem to be the valley at ‘Dha’ for the
former and ‘dha’ for the latter. Do all these make a meaning in the subjective sense?
Does it mean that Todi and Bhairav cannot be confused between themselves or with
the other two ragas by listeners having no musical knowledge of ragas? Is it true
that for such listeners Darbari-Kannada and Mian-ki-Malhar get confused? Only
suitably designed listening experiment can answer these questions.
6.7
Identiﬁcation of Raga
For identiﬁcation of raga only the time series of F0 is important. Let us begin with
some deﬁnitions (Pragnananda 1963); ‘thaat’ (a frame of allowed notes), ‘swaras’
(notes), ‘vadi’ (most signiﬁcant note), ‘samwadi’ (immediate next), ‘bibadit’ note
used exceptionally to enhance the portrayal of raga), and ‘varjit’ (note to be
Fig. 6.11 Raga-roop (the form of raga) of four ragas
118
6
Raga Identiﬁcation

avoided). Estimation of all these except the varjit may be obtained from the fre-
quency distribution of occurrences or nyas. Nyas on a swara is indicated when the
artist rest a while on the note. Arohi, aworohi are general tendency of ascent or
descent in terms of swara. ‘Bandish’ here is deﬁned as a set of sequences of notes
sufﬁcient to identify a raga. ‘Swara-sangati’ is particular phrases used frequently in
a raga which make it conspicuous e.g., ga pa da pa is a phrase that is most likely to
be found in the raga Deshkar than Bhupali.
Pre processing of the signal starting from cleaning of noise through extraction of
F0 time-series to extraction of notes have been described in detail in some other
chapters. Feature selection for identiﬁcation of raga involves two hierarchical levels
(Chakraborty et al. 1989).
Level 1 is to identify the ‘thaat’. However, that cannot be used in practice as this
sometimes requires the a priori knowledge of the raga. Therefore a recognition
process is attempted which uses a similar clustering concept, which is practically
estimable and contain more information about a particular raga. The concept
incorporates notes used and not used in a particular raga and comprises of eight
character information and is called raga-mote-identiﬁer (RNI). The conﬁguration
may best be understood from a reference to Fig. 6.12.
R (re) (ga) (ma) (pa) (da) (ni)
The eight letters furnish the usage of notes in the bandish irrespective of the
octave. The crosses in the table indicates the impossible states as the the ﬁfth note,
pancham is invariant. RNI aims at the clustering of ragas only on the basis of the
attributes of the notes used and the identity of the note not used. Typically the RNI
string R211121G furnishes the following information:
(a) This corresponds to a raga where all notes are used and
(b) for this raga rishav and dhaibat are komal and the rest are sudh.
These characteristics indicate the ragas Bhairav and Kalingra. Clearly there is a
considerable departure from the theory of classes proposed by Bhatkhande. Yet it
maintains the essence of classiﬁcation of the raga solely on the basis of the attri-
butes of the swaras, there uses in the raga. It may be noted here that the classes
generated by RNI are not disjoint. Occasionally a particular raga may occur in more
than one class.
  Attribute Swaras
 
re ga ma pa da
ni
sudh
1 
1 
1 
1 
1 
1 
komal
2 
2 
2 
2 
2 
2 
both 
3 
3 
3 
3 
3 
3 
none 
4 
4 
4 
4 
4 
4 
Fig. 6.12 Raga rate identiﬁer
6.7
Identiﬁcation of Raga
119

The second level involves a lexicon of phrases comprising of pakad and swara-
sangatis for each of the included ragas. Detection of these phrases in the bandish
forms the second phase of the identiﬁcation. The construction of the lexicon
involves the expertise of the exponents of music and the need for a knowledge
based approach cannot be under estimated. The present report uses a lexicon which
contains over 200 phrases covering 75 ragas. Each record in the lexicon is phrase
with the name of the raga and the RNI associated with it for the ease of retrieval of
records in the data base when a search is deployed.
6.7.1
Feature Extraction
The notation of a bandish set to a particular raga serves as the input to the system.
In the ﬁrst step the system will interpret the notation deciding upon the timing of
each note relative to a user deﬁned speed (laya) and scale. The choice is made from
the menu supplied at the console. The input is scanned to generate the RNI. The
given notation is then segmented into the arohi and the awarohi parts. This seg-
mentation is done by recognizing a tendency to ascent or descent rather than
considering the consecutive notes for a strictly monotonic sequence. This is nec-
essary because in practical situations small perturbations are found to prevail in
such sequences. Sa ga re ma ga is a typical such example of arohi in Goursarang.
The problem is tackled in the following way. Each of the twelve notes in octave is
assigned a particular numeric value (Fig. 6.13). The numeric coding is required for
determining the increasing or decreasing nature of the sequence.
Notes 
Octave 
Lower 
Middle 
Upper 
Saraj 
‘sa0 
12  sa0 
24
“sa0 
48
Komal rishav 
‘re- 
13  re- 
26
“re- 
52
Sudh  rishav 
‘re+ 
14  re+ 
28
“re+ 
56
Komal gandhar
‘ga- 
15  ga- 
30
“ga- 
60
Sudh gandhar 
‘ga+ 16  ga+
32
“ga+
64
Sudh madhyam
‘ma- 17  ma-
34
“ma-
68
Kori madhyam 
ma+ 18 ma+
36
ma+ 
72
Pancam 
‘pa0 19  pa0 38
“pa0 76
Komal dhaibat 
‘da- 
20  da- 
40
“da- 
80
Sudh dhaibat 
‘da+ 21  da+
42
“da+
84
Komal nishad 
‘ni- 
22  ni- 
44
“ni- 
88
Sudh nishad 
‘ni+ 
23  ni+ 
46
“ni+ 
92
Fig. 6.13 Numeric coding
scheme for notes
120
6
Raga Identiﬁcation

The coding scheme also an explicit octave relation between the three notes of the
three different octaves and to denote the mutual gradation among the notes in terms
of frequency in the same octave. 99 used to indicate a nyas on particular note.
Having transformed each note to the corresponding numerical value we get a long
sequence of numbers of which two passes are required for segmentation. The ﬁrst
one mark the transition and the second pass provide the smoothening and in this
process provide segments of varied lengths—a sequence placing markers ‘U’ and
‘D’ as indicated in Fig. 6.14.
The sequence is scanned and markers are placed by a one note look ahead. To
Indicate the beginning of the list a marker label ‘S’ is placed. In the second pass the
markers which has been placed at a distance of two notes or less are removed taking
into account the repetitions and the nyas. After the ﬁrst phase which provides the
RNI and the sequences of arohi and awrohi the second phase begin with a search of
the lexicon (Fig. 6.15). If the RNI indicates a single member cluster the system
gives out the raga name without going for a lexical search.
Otherwise system enters the lexical matching phase. In this process each of the
segments is considered as a sample. Each of the patterns from the lexicon corre-
sponding to the ragas indicated by the RNI is matched successively with a
sub-string of the sample space. In this matching priorities are given to pakad over
Fig. 6.14 State diagram for ﬁrst pass
Fig. 6.15 Block diagram of the recognition system
6.7
Identiﬁcation of Raga
121

swara-sangati. A match indicates identiﬁcation of the raga. If a match is not found
after exhaustive search of the indicated part of the lexicon the partial match
sub-routine is triggered. In the partial match algorithm repetition of a note in the
sample string is neglected whenever required. Also only for phrases related to
swara-sangati one inclusion and one deletion from the sample string is allowed. As
the Phrases in the lexicon are of varying length and as the procedure also includes
partial matching the phrases ‘should be’ and ‘may be’ is used to indicate the
qualitative estimate of the level of conﬁdence for the identiﬁcation thus obtained.
6.7.2
Results and Discussion
The RNI induced 45 classes in the sample space of 75 ragas of which 25 contains
single element. Bandishes of 36 ragas has been tried for identiﬁcation. RNI clus-
tering is errorless. The lexical matching 26 bandishes belonging to the multiple
element clusters gives 19 correct classiﬁcations. There is ambiguity for 3 cases.
Incorrect classiﬁcation occurred only in 7 cases. An example of the working pro-
cedure for a case is illustrated in Figs. 6.16 and 6.17.
The performance of RNI is found to be satisfactory. An analysis of the wrongly
classiﬁed samples shows that the defect lies mainly in the simple nature of the
matching algorithm.
The supplied notations  :   Raga Marwa 
(‘ni+) (re-) (ga+) (ma+)        (ni+) (da) (=) (ma): 
(ga+) (re-) (ga+) (ma+)        (ga+) (re-) (sa0) (=): 
(‘ni+) (re-) (ni+) (‘da+)        (‘ma+) (da+) (sa0) (=): 
(‘ni+) (re-) (ga+) (ma+)        (ga+) (re-) (sa0) (=): 
(‘ma+) (da+) (ma+) (ga+)    (ga+) (re-) (sa0) (re-): 
(‘ni+) (=) (ga+) (ma+)           (ga+) (re-) (sa0) (=): 
(“re-) (ni+) (da+) (ni+)         (da+) (ma+) (da+) (ma+) 
(ga+) (re-) (ga+) (ma+)        (ga+) (re-) (sa0) (=) 
Raga-note-identifier ====== R212411G 
Nyas Table 
Swar #(42)  Nyas (1);  Swar #(24) Nyas(4); 
Swar #(46)  Nyas (1);  Swar #(48) Nyas(1); 
The segmented Arohi & Abrohi Sequences 
AROHI    23 26 32 36 46 
ABROHI  46 42 99 36 32 26 32 36 32 26 24 99 23 26 
 * 23 21 18 21 24 99 23  
AROHI     23 26 32 36 
ABROHI   36 32 26 24 99 36 42 36 32 
AROHI     32 36 42 48 52 46 99 64 72 
ABROHI   72 64 52 48 99 52 46 42 46 42 36 42 36 32  
* 26 32 36 32 26 24 
Fig. 6.16 Score (Musical
Notations) of Raga Marwa
122
6
Raga Identiﬁcation

References
Bhatkhande VN (1995) Hindustani Sangeet Paddhati (Marathi/Hindi), vol 1–4. Popular Prakashan,
Mumbai
Chakraborty J, Mukherjee B, Datta AK (1989) Some studies on machine recognition of ragas in
indian classical music. J Acoust Soc India XVII(3, 4)
Chakraborty S, Solanki SS, Roy S, Tripathy SS, Mazzola G (2008) A statistical comparison of
performance of two ragas (Dhuns) that use the same notes. In: Proceedings of the international
symposium on frontiers of speech and music, Kolkata, pp 167–171, 20–21 Feb 2008
Chakraborty S, Mazzola G, Tewari S, Patra M (2014) Computational musicology in Hindustani
music. Springer International Publishing, Berlin
Chordia P, Rae A (2007) Raag recognition using pitch-class and pitch-class dyad distributions.
Austrian Computer Society (OCG)
Datta AK (1996) Generation of musical notations from song using state-phase for pitch detection
algorithm. J Acoust Soc India XXIV
Datta AK (1996) Ratios and Intervals used in North Indian Classical Music—a study based on
recorded performance. J Acoust Soc India 14
Datta AK (1997) Pitch analysis of recorded vocal performances in Hindustani music: evidence of a
personal scale. J Acoust Soc India 15
Datta AK, Kundu R (1991) A psycho-perceptual hypothesis for Indian Musical scale. In:
Proceedings of international workshop on recent trends in Speech, Music and Allied Signal
Processing, Delhi, pp 9–11
Datta AK et al (2003) Study of Srutis in Indian musical scales and relevance of schools in recent
times using Signal Processing techniques. In: Proceedings of frontiers of research on speech
and music (FRSM-2003)
RAGA INDICATED BY RNI R212411G  : 
PURIA 
MARWA 
SOHONI 
LEXICAL RECORDS: 
32 36 42 32                            PURIA 
36 42 46 52 46 36 42 48        PURIA 
23 21 23 18 21 26 24             PURIA 
32 23 25 24                            PURIA 
23 26 32 26 32 26 24             PURIA 
32 36 32 36 24                       MARWA 
42 36 32 26                            MARWA 
42 36 32 26 32 36 32 26 24   MARWA 
23 26 99 24                            SOHINI     
48 52 48 52 46 48                  SOHINI 
36 42 36 32 26 24                  SOHINI 
Matched phrases and associated raga name: 
The name of the Raga should be: 
MARWA 
32 36 32 26 24 
The name of the Raga may be: 
MARWA 
42 36 32 26 
The name of the Raga should be: 
MARWA 
42 36 32 2632 36 32 26 24 
Fig. 6.17 Raga indicated by
the proposed identiﬁer
References
123

Datta AK, Sengupta R, Dey N (2012) Capturing the essence of raga from Hindusthani singing: an
objective approach. J Acoust Soc India 39(1)
Deshpande H, Nam U, Singh R (2001) Mugec: automatic music genre classiﬁcation. Technical
Report, Stanford University
Ghias A, Logan J, Chamberlin D, Smith BC (1995) Query by humming—musical information
retrieval in an audio database. In: Proceedings of ACM multimedia, pp 231–236
Iliopoulos CS, Kurokawa M (2002) String matching with gaps for musical melodic recognition. In:
Proceedings of Prague Stringology conference, pp 55–64
Jones CS, Indian classical music, tuning and ragas. http://cnx.org/content/m12459/1.6. Accessed
on 26 Nov 2016
Lentz DA (1961) Tones and intervals of Hindu classical music. Nebraska Univ. Studies, New
Series No. 24, Lincoln University Press, Lincoln
Menon RR (2007) Indian music: the magic of the raga. Manohar Publishers and Distributors, Delhi
Pragnananda S (1963) A history of Indian music, vol 1. Ramakrishna Vedanta Math
Priyamvada A (2007) Encyclopaedia of Indian music. Eastern Book Corporation, Delhi
Rabiner LR (1989) A tutorial on hidden markov models and selected applications, in speech
recognition. Proc IEEE 77:257–286
Sahasrabuddhe HV (1994) Searching for a common language of ragas. In: Proceedings of Indian
music and computers: can ‘mindware’ and software meet?
Shahin I (2004) Enhancing speaker identiﬁcation performance using circular hidden Markov
model. In: Proceedings of the international conference on information and communication
technologies: from theory to applications, vol 1, pp 389–390, 19–23 Apr 2004
Tzanetakis G, Essl G, Cook P (2001) Automatic musical genre classiﬁcation of audio signals. In:
Proceedings international symposium of music information retrieval, pp 205–210
Upadhye R, Sahasrabuddhe HV (1992) On the computational model of Raag music of India. In:
Workshop on AI and Music: 10th European conference on AI, Vienna
124
6
Raga Identiﬁcation

Chapter 7
Gharana Identiﬁcation
7.1
Introduction
Indian classical music resulted from the fusion of various cultural styles. The basic
musical structures or Ragas have been divided into various Thaats traditionally and,
ragas have speciﬁc ascent (arohana) and descent (awarohana) sequences that may
not necessarily be identical. Every raga is assigned a natural register and rules for
pitch transitions (Jairazbhoy 1995). The rules of pitch transitions, like transitory and
non-transitory pitch movements between the notes are taught in different Gharanas
of Indian classical music.
7.1.1
What Is Gharana?
The word Gharana means “family”. In relation to music, Gharana refers to a family
of musicians, a school of music or a musical lineage connected with the name of a
particular person or place. The characteristic feature of a Gharana is its special style
of presentation: the result of the special and extraordinary creativity and innovation
of a highly talented musician. The other musicians of the Gharana may have their
own individual features of presentation, but their training and conditioning in the
distinguishing style of the Gharana is bound to leave indelible and recognizable
stamps on the presentation of the performer.
A Gharana may take the name of
(a) The name of a person, family or group.
(b) A place or region.
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_7
125

Examples of the ﬁrst category are Seni Gharana, Imdadkhani Gharana, Kavval
Gharana, while the example of the second category are Gwalior Gharana, Agra
Gharana.
Table 7.1 gives the names of some Gharana in vocal and instrumental (melodic)
music across India.
Each Gharana has its own special Silsila or style or logic of presentation, within
the general framework of the regional Bani (or, for instrumental music, Baj) which
applied to it. Thus, the rise of the Gharana system resulted in the segmentation of
the different styles of Raga development. This sometimes led to different versions
of the same Raga, specially when comparisons arose between the presentations of
musicians of different Gharana presenting the same Raga. This has had a spin-off to
the present day. For example, Gwalior Gharana musicians use Shuddha Ni in
addition to Komal Ni in Raga Rageshvari but musicians of several other Gharana
use only Komal Ni and not Shuddha Ni.
Table 7.2 gives a brief description of the musical characteristics of three major
Gharana of vocal (esp. Khayal) music that are prevalent in present times—Gwalior,
Agra, and Kirana. Each Gharana has its own special features.
For better appreciation of Khayal, it is important for the listener to understand
and keep in mind these features. In this context, it is also important to know that the
Tabla accompanist must be fully aware of the special features of the Gharana of the
artist he is accompanying. If the accompanist has a less than proper understanding
about these features, he will not be able to provide the correct form of accompa-
niment that is appropriate to the music of the Gharana. Many an otherwise good—
even great—vocal performance has been ruined or all but ruined because the poor
vocalist has had to struggle against the complacently ignorant accompaniment of
the tabla player.
Training of Gharana
Gharana is taught orally, in this process the Shishya (the students) listen carefully
the raga performed by the Guru (the teacher) and tries to follow the same rendering
style (levels and transitions of pitches) of the Guru. However, artists are given
considerable latitude to improvise within allowable norms. Therefore, the mathe-
matical analysis of a raga performance becomes difﬁcult because the same raga
when rendered by artists from different Gharanas, would have a very different
signature in each rendition.
7.1.2
Gharana Identiﬁcation
This chapter discusses the use of projection pursuit techniques to analyze the
similarity between different artists, means to classify the ragas objectively rather
than perceptually. Due to the aesthetic rather than mathematical nature of the Indian
musical tradition, timbral and rhythm features can provide a clearer picture of the
musical pieces rather than just a set of statistical or acoustic features.
126
7
Gharana Identiﬁcation

Table 7.1 Different Gharana of Indian classical music
Vocal music Gharana
Instrumental music Gharana
Dhrupad
Khayal
Tappa
Thumri
Rabab
Been
Sitar
Sarode
Sarangi
Esraj
Senia
Gwalior
Tilmandi
Atrauli
Jaipur
Agra
Banaras
Betiah
Vishnupur
Delhi
Gwalior
Tilmandi
Atrauli
Jaipur
Agra
Banaras
Lucknow
Panjab
Sikanerabad
Rangile
Kirana
Rampur
Seheswan
Bhendibazar
Indore
Mevati
Lucknow
Banaras
Lucknow
Banaras
Panjab
Senia
Senia
Kirana
Gharpure
Jaipur
Senia
Gwalior
Jaipur
Imdadkhani
Senia
Rampur
Kirana
Delhi
Panjab
Gaya
Vishnupur
7.1
Introduction
127

Therefore various sets of statistical measure is performed on audio features
namely Timbral texture features, Rhythm content features and statistical features to
represent the raga performances by the artists from different Gharana. The pro-
jection pursuit techniques are applied to study the similarities and dissimilarities of
the raga performance on the basis of Gharana.
7.2
Audio Feature Set
Feature extraction is the process in which a characteristic of an audio segment is
computed and represented in a compact form. A music audio is characterized by its
timbral and rhythmic features.
Table 7.2 Brief description of the musical characteristics of three major Gharana of vocal music
Gwalior, Agra, and Kirana
Kirana Gharana
Gwalior Gharana
Agra Gharana
A soft and sensitive voice
capable of subtle tonal
manipulation
Alap-Pradhhan Gayaki,
i.e., style heavily relying
upon Alap
Lyrical approach to
“Svara-Lagav” or
articulation of notes
Vilambit Badhat
(development) of every note
of the Raga. This principle
of Svara-Vistara is
diametrically opposed to that
of Raga-Vistara followed
by, say, the Agra and
Gwalior styles
“Chaindar” or serene,
contemplative and restrained
Gayaki. Unhurried, restful
style
A strong Dhrupad base
Open-throated and bold
voice production
Long Auchar or
introductory Alap before the
Bandish
Importance of vowels in
Badhat. Extensive use of
Aakar. Vowels often
coalesced with words. Sharp
vowels like EE and OO
employed in the higher notes
Employment of Gamak
throughout, lending Vazan
to the notes used
Elongation of the Anunasik
Svara (nasals), especially for
staying on the higher notes
A penchant for long
Bandish often containing
Adi Laya syncopation
Use of syncopation during
Vistar, especially in medium
tempo phrases
Systematic phrasewise
development of the Raga
Bold and full-throated voice
production like that of
Gwalior Gharana
Dhrupad based
development of Khayal
Long Nome-Tome Alap
before commencing upon
the Khayal composition as
practised by Dhrupadiya
Articulating the sharp
vowels EE and OO for
lingering on the high notes
Importance of Bol-Alap
Clear and lyrical rendition of
the text of the song
Development on the basis of
the Raga phrases and
rhythm as opposed to the
system of ‘Svara-Badhat’
Laya-based Gayaki.
Indulgence in Sath-Sangat/
Ladant with Tabla
accompaniment at
appropriate placesa
Emphasis on the purity of
the Raga Little use of
“lighter” improvisational
tools e.g. Khatka, Murki,
etc
Moderately fast Gamaka
Tana
128
7
Gharana Identiﬁcation

7.2.1
Timbral Texture Features
Timbre describes those characteristics of audio signals that allow us to differentiate
between audio signals having the same pitch and loudness. It is determined pri-
marily by the harmonic content and the dynamic characteristics of the signal. The
calculated features are based on the short time Fourier transform (STFT) and are
calculated for every short-time frame of sound. Following features are used in this
work to determine timbral characteristics.
Mel Frequency Cepstral Coefﬁcients—The Mel scale is a scale of pitches
perceived by listeners to be equally spaced. Mel-Frequency Cepstral Coefﬁcients
are coefﬁcients that collectively form a representation of the short-term power
spectrum of an audio sample. They are derived from a cepstral representation of the
audio clip. The frequency bands are spaced equally along the Mel scale. This gives
a closer approximation of the human auditory response as compared to a linear
spacing of frequency bands.
The basic steps to be followed in deriving MFCCs are as follows:
• Perform a Fourier transform on a windowed excerpt of a signal.
• Map the spectral powers obtained onto the Mel scale, using triangular over-
lapped windows.
• Take the log of the powers at each Mel frequency.
• Take the discrete cosine transform treating the log powers as a signal.
• The amplitudes of the resultant spectrum are the MFCCs.
Equations (7.1)–(7.8) give various formulae for MFCC features and Eq. (7.9)
gives the formula for Andrews plot.
Conversion from the linear frequency scale to the Mel scale frequency mf is
achieved using the following equation
mf ¼ 1127:01048 logeð1 þ
f
700Þ
ð7:1Þ
where, f is the frequency in hertz in linear scale.
The ﬁrst few coefﬁcients obtained by this method contain most of the signal
energy. Generally First 13 MFCCs are used for speech signal representation but ﬁrst
ﬁve coefﬁcients are taken as features for music signals (Tzanetakis and Cook 2002).
RMS Energy—Root Mean Square (RMS) energy is the quadratic mean of the
amplitude values of the audio signal. It is given by the following equation
xrms ¼
ﬃﬃﬃ
1
n
r X
n
i¼1
x2
i
ð7:2Þ
where
xi
is the magnitude of ith sample
n
is the total number of samples
7.2
Audio Feature Set
129

Spectral Irregularity—Spectral irregularity gives a measure of the degree of
variation between successive peaks.
Ir ¼
PN
k¼1 ðak  ak1Þ2
PN
k¼1 a2
k
ð7:3Þ
where
ak
is the magnitude of kth spectral coefﬁcients
N
is the total number of spectral coefﬁcients
Zero Crossing Rate—Zero crossing rate is a measure of the number of times
the signal amplitude changes its sign. It is used as a measure of noisiness in
complex signals.
Z ¼ 1
T
X
m
i¼mT þ 1
sgnðxiÞ  sgnðxi  1Þ
j
j
2
wðm  iÞ
ð7:4Þ
where
T
is the length of the time window
xi
is the magnitude of the ith time domain sample
w
is a rectangular window
m
is the sample number of ending point of the window
Spectral Centroid—This gives the centre of gravity of the magnitude spectrum
of the STFT. The spectral centroid is calculated as the weighted mean of the
frequencies present in the signal, using their amplitudes as weights.
Ct ¼
PNsc1
k¼0
fkak
PNsc1
k¼0
ak
ð7:5Þ
where
ak
is the magnitude of kth spectral coefﬁcients
fk
is the frequency of kth spectra
Nsc
is the total number of spectral coefﬁcients.
7.2.2
Rhythmic Features
Rhythm refers to a regular recurrence or pattern. These features provide a measure of
the regularity or structure of the audio signal. Three features are used here to give a
measure of the rhythm of the audio signals—event density, tempo and pulse clarity.
130
7
Gharana Identiﬁcation

Event Density—The event density gives the number of note onsets per second.
Notes are deﬁned by three basic concepts—attack, transient and onset. The attack is
the time period when the amplitude increases. The transient is the time period when
an excitation is applied and subsequently damped. The onset is a speciﬁc time
instant chosen to mark the transient and usually corresponds with the start of the
transient or the earliest instant when the transient can be reliably detected.
Tempo—Tempo is a measure of the speed of a musical piece and is deﬁned
terms of the number of beats per minute. Tempo can be calculated by detecting
periodicities from the onset detection curve. Once the onset detection curve is
formed using the detection function as described above, a determination of peri-
odicities in the curve gives us an estimate of the tempo of the musical piece in beats
per minute.
Pulse Clarity—Pulse clarity is an estimate of rhythmic clarity or the strength of
the beats. A beat histogram is a curve describing the beat strength as a function of
beat per minute values. Statistical measures of the beat histogram such as mean,
standard deviation, skewness and kurtosis give an estimate of the pulse clarity. In
this work standard deviation of the beat histogram is taken as the pulse clarity.
Skewness and kurtosis of beat histogram are calculated as—Skewness is the
measure of the symmetry or asymmetry of a signal.
Skewness ¼
PNdp
i¼1 ðYi  YÞ3
ðNdp  1Þs3
ð7:6Þ
where
Yi
is ith element of the signal
Y
is the mean of Y
s
is the standard deviation
Ndp
is the number of data points
Kurtosis is a measure of the noisiness of the signal. It shows whether the data is
peaked or ﬂat relative to a normal distribution.
Kurtosis ¼
PNdp
i¼1 ðYi  YÞ4
ðNdp  1Þs4
ð7:7Þ
where
Yi
is ith element of the signal
Y
is the mean of Y
s
is the standard deviation
Ndp
is the number of data points
Shannon Entropy—Entropy is deﬁned as the measure of the uncertainty of a
random variable. Shannon entropy gives the average unpredictability of a random
7.2
Audio Feature Set
131

variable thereby quantifying its information content. The Shannon entropy H of a
discrete random variable X with possible values {x1, … , xn} and probability mass
function P(X) is given as
HðXÞ ¼ E½IðXÞ ¼ E½ lnðPðXÞÞ
ð7:8Þ
where
E
is the expected value operator,
I
is the information content of X
I(X)
is a random variable.
7.3
Projection Pursuit
In many applications of statistical analysis involving multidimensional data, the
problem arises not from missing information but rather from an excess of infor-
mation. In such cases, projection pursuit techniques are employed to determine the
optimal projections that offer maximum information. The conventional approach to
analyzing multidimensional datasets is to reduce their dimensionality to one, two or
three dimensions, thus enabling easier visualization. This dimensional reduction
provides structural details of the datasets. The scatter plot is one of the most basic
techniques of projection pursuit. It is essentially a two-dimensional display to
indicate data over two selected dimensions at a time. It is quite simple to produce all
(2n) pair-wise scatter plots for N-dimensional space and perform analysis on these.
This method, however, only allows structure across the two plotted dimensions to
be visualized. Manual projection pursuit techniques are limited as they require a
huge amount of time it takes to exhaustively explore a given space. Friedman and
Tukey (1974) automated the task of projection pursuit by characterizing a given
projection using a numerical index that indicates the amount of structure that is
present. This index is then used for a heuristic search to locate the appropriate
projections. Different types of heuristic searches are suggested in Friedman and
Tukey (1974) and in Tukey and Tukey (1981). Once the structure is found, it is
removed from the data. The datasets are then examined for further structures,
which, if found, are also removed. This process continues until no further structures
are detectable within the datasets. Projection pursuit methods are very useful tools
in the area of high-dimensional data analysis. They do however present many
limitations as enumerated by Crawford and Fall (1990). It can be difﬁcult to
determine just what the solutions obtained from automatic projection pursuit
methods might mean. Also, most projection pursuit softwares don’t possess the
ability to make inferences, and so can get false matches for structure. Finally, it is
often difﬁcult, in general to algorithmically specify what constitutes the structure in
132
7
Gharana Identiﬁcation

data. The technique employed here for dimensional reduction is the Andrews plot
method. An Andrews plot (Andrews 1972) or Andrews curve is a method to
visualize multi-dimensional data in two dimensions. Each point x = {x1, x2, … ,
xn} in the dataset denotes a ﬁnite Fourier series.
fxðtÞ ¼ x1ﬃﬃﬃ
2
p
þ x2 sinðtÞ þ x3 cosðtÞ þ x4 sinð2tÞ þ x5 cosð2tÞ þ   
ð7:9Þ
This function is then plotted for −p < t < p. Each point is now a line between
−p and p. Any structure in the dataset is now visible as curves on a two-
dimensional plot.
See
also
http://www.mathworks.in/products/matlab/whatsnew.html.
By
the
property of Andrews’s plots, if any two N-dimensional points are close, the cor-
responding curves in two dimensions are also close accordingly. Therefore, clusters
among the curves are identiﬁed and determines the artists performance closeness to
the same raga. If two artists train under the same teacher, by comparing the per-
formances of the two artists with that of the teacher’s, one can determine which
artist more closely resembles the teacher’s individual style. Artists from the same
Gharana, or school of thought, are more likely to produce curves closer to one
another when they render the same raga. For artists from different Gharanas, the
curves will be further apart. Thus, projection pursuit can help in the classiﬁcation of
renderings of ragas according to the Gharana of the artist responsible for the
particular rendition.
7.4
Feature Database Preparation
The list of artists, their corresponding Gharanas and the ragas used for this analysis
are given in Table 7.3. The fundamental musical features of the ragas selected for
the analysis are given in Table 7.4.
The database of raga Khamaj, Malkauns and Marwa is collected from six artist
belongs to different Gharanas namely Gwalior, Agra and Kirana. The music was in
Table 7.3 Artists, Gharanas and Ragas selected for the analysis
Name of the Gharana
Name of the Artist
and abbreviations
Name of the Ragas
Gwalior
ARK
Abdul Rashid Khan
Khamaj
MR
Malini Rajurkar
Agra
LJR
Lalith J. Rao
Malkauns
SM
Sumati Mutatkar
Kirana
BR
Basavaraj Rajguru
Marwa
MP
Mani Prasad
7.3
Projection Pursuit
133

MP3 format, and Soft div software was used to convert the raga performance from .
mp3 ﬁle to.wav ﬁle. Then from this .wav ﬁle audio samples of 5 s duration from
each stage of the raga performance was created using Adobe audition 3.0. Database
comprises of 36 samples (18 Alaap portion and 18 Gati portions of the raga
performances) of music belonging to three ragas. Figure 7.1 shows the prepro-
cessing and process of feature database preparation from the audio ﬁles of the
ragas. The music signal (raga performance in mp3 format) is fed to the system and
processed to obtain different features output.
Table 7.4 Fundamental musical features of Raga Khamaj, Malkauns and Marwa
Musical feature
Raga Khamaj
Raga Malkauns
Raga Marwa
Thaat
Khamaj
Bhairavi
Marwa
Arohana
S G M P D n S
S g M d n S
S r G m D N D S
Awarohana
S n D P M G R S
S n d M g S
S N D m G r S
Jati
Shadav—Sampoorna
Audav—Audav
Shadav—Shadav
Vadi Swara
G
M
R
Samvadi Swara
N
S
D
Prakriti
Serious
Pakad
S G M P D, G M n D
M, M g, M G d n d
D m G r S, G m G D, r S
Nyas Swara
S
S
S
Time of rendition
10.30–11.30 pm
1.00–4.00 am
5.30–7.00 pm
Fig. 7.1 Process of feature database preparation
134
7
Gharana Identiﬁcation

Table 7.5 Various features of the raga Khamaj performances by the artists of different Gharanas
Features
Gwalior
Gharana
Agra
Gharana
Kirana
Gharana
ARK
MR
LJR
SM
BR
MP
MFCC1
0.77291
0.72311
0.91223
0.89213
0.71776
0.74325
MFCC2
0.69852
0.62141
0.81772
0.78791
0.68725
0.58671
MFCC3
0.56756
0.61723
0.71223
0.69231
0.61661
0.68717
MFCC4
0.61777
0.59487
0.67192
0.71024
0.56716
0.60711
MFCC5
0.45773
0.52751
0.71662
0.87616
0.59817
0.61098
RMS energy
0.047729
0.043325
0.082332744
0.076332744
0.022469413
0.021969413
Event density
1.2197
1.1099
1.2072042
1.1872042
0.285378841
0.282537884
Tempo
135.9892
136.5892
66.498204
65.947204
152.2931561
151.931561
Pulse clarity
0.020941
0.024941
0.12503499
0.12303499
0.026852802
0.01968528
Zero cross rate
180.815
180.815
328.29797
327.39797
11.69933056
12.09933056
Spectral irregularity
0.82068
0.71968
0.97250471
0.898250471
0.868513732
0.859851373
Spectral centroid
1973.8882
1974.6882
1534.618
1535.168
2345.148105
2344.948105
Skewness
3.68822E+11
3.68822E+11
3.63253E+11
3.63253E+11
38638111536
38638111536
Kurtosis
6.80963E+15
6.80963E+15
7.15541E+15
7.15541E+15
3.0659E+14
3.0659E+14
Shannon entropy
0.91058
0.89158
0.87621982
0.86921982
0.899265474
0.919265474
7.4
Feature Database Preparation
135

7.5
Experimental Results and Discussions
Six artists were chosen from across three Gharanas, by selecting two artists from
each Gharana. Their renderings of three ragas namely raga Khamaj, Malkauns and
Marwa were compared to get the results. Testing was done to conﬁrm application
of projection pursuit techniques for classiﬁcation of musical pieces in terms of
Gharanas of the artists. Also, closeness of artists from the same Gharana and
separation between renderings of the same raga by artists of different Gharanas was
determined. The computer used for simulations uses a 64-bit Intel Core i3 processor
at 2.53 GHz, 4 GB of RAM and runs the Windows 8 operating system. Simulations
were carried out using MatLab R2012a. The following speciﬁcations are main-
tained during the experiment:
Time duration ¼ 05 s each for alaap and gati:
Sampling rate ¼ 10000 samples=s:
Tables 7.5 and 7.6 give the characteristics of raga Khamaj, Malkauns and
Marwa respectively as rendered by artists of different gharanas (Table 7.7).
Table 7.6 Various features of the raga Malkauns performances by the artists of different
Gharanas
Features
Gwalior
Gharana
Agra
Gharana
Kirana
Gharana
ARK
MR
LJR
SM
BR
MP
MFCC1
0.87921
0.76311
0.95423
0.92013
0.71216
0.71521
MFCC2
0.76852
0.69101
0.83742
0.78392
0.69823
0.59713
MFCC3
0.67156
0.63714
0.71427
0.69145
0.67174
0.67614
MFCC4
0.71377
0.58937
0.67253
0.72037
0.56395
0.61372
MFCC5
0.72173
0.68781
0.72641
0.86411
0.58913
0.61171
RMS energy
0.037593
0.017593
0.11028
0.19028
0.054507
0.114507
Event density
0.69852
1.69852
2.0521
2.3521
1.8043
2.09043
Tempo
83.8164
80.8164
161.1558
171.1558
118.9472
123.9472
Pulse clarity
0.085671
0.065671
0.048319
0.0548319
0.058645
0.098645
Zero cross rate
100.9782
102.9782
430.678
435.678
160.3516
165.3516
Spectral
irregularity
0.87383
0.97383
0.56756
0.6756
0.79599
1.279599
Spectral
centroid
2174.9354
2214.9354
1326.2469
1336.2469
1549.3103
1552.3103
Skewness
5.36944E+11
5.36944E+11
9.43206E+11
9.43206E+11
1.91548E+11
1.91548E+11
Kurtosis
0.87921
0.76311
0.95423
0.92013
0.71216
0.71521
Shannon
entropy
0.76852
0.69101
0.83742
0.78392
0.69823
0.59713
136
7
Gharana Identiﬁcation

Table 7.7 Various features of the raga Marwa performances by the artists of different Gharanas
Features
Gwalior
Gharana
Agra
Gharana
Kirana
Gharana
ARK
MR
LJR
SM
BR
MP
MFCC1
0.87921
0.76311
0.95423
0.92013
0.71216
0.71521
MFCC2
0.76852
0.69101
0.83742
0.78392
0.69823
0.59713
MFCC3
0.67156
0.63714
0.71427
0.69145
0.67174
0.67614
MFCC4
0.71377
0.58937
0.67253
0.72037
0.56395
0.61372
MFCC5
0.72173
0.68781
0.72641
0.86411
0.58913
0.61171
RMS energy
0.037593
0.017593
0.11028
0.19028
0.054507
0.114507
Event density
0.69852
1.69852
2.0521
2.3521
1.8043
2.09043
Tempo
83.8164
80.8164
161.1558
171.1558
118.9472
123.9472
Pulse clarity
0.085671
0.065671
0.048319
0.0548319
0.058645
0.098645
Zero cross rate
100.9782
102.9782
430.678
435.678
160.3516
165.3516
Spectral irregularity
0.87383
0.97383
0.56756
0.6756
0.79599
1.279599
Spectral centroid
2174.9354
2214.9354
1326.2469
1336.2469
1549.3103
1552.3103
Skewness
5.36944E+11
5.36944E+11
9.43206E+11
9.43206E+11
1.91548E+11
1.91548E+11
Kurtosis
0.87921
0.76311
0.95423
0.92013
0.71216
0.71521
Shannon entropy
0.76852
0.69101
0.83742
0.78392
0.69823
0.59713
7.5
Experimental Results and Discussions
137

The plot in Fig. 7.2 shows the curves for renderings of the raga Khamaj by both
artists of three Gharanas each. The artists are identiﬁed by their initials, as given in
Table 7.3. While separation does exist between curves for each artist, it is more
pronounced for artists of different Gharanas. For artists from the same Gharana,
the separation between the curves denotes the individuality of the artist. This may
come from individual vocal characteristics or even inﬂuence from other styles.
Fig. 7.2 Andrews’s plots of the raga Khamaj performances by various artists of different
Gharana
Fig. 7.3 Andrews’s plots of the raga Malkauns performances by various artists of different
Gharana
138
7
Gharana Identiﬁcation

On the other hand, for artists from different Gharanas, the pronounced difference
comes from basic and substantial differences in musical style, in addition to the
individual characteristics. Figure 7.5 shows the Andrews plot using standardized
PCA (principal component analysis) scores. This gives a clear separation by
Gharana. Figures 7.3 and 7.4 show the same comparison for raga Malkauns and
Marwa. Figures 7.6 and 7.7 show the Andrews plot for raga Malkauns and Marwa
using standardized PCA scores.
Fig. 7.4 Andrews’s plots of the raga Marwa performances by various artists of different Gharana
Fig. 7.5 Andrews’s plots of raga Khamaj performances by various artists of different Gharana
using standardized PCA scores
7.5
Experimental Results and Discussions
139

7.6
Conclusion
The application of projection pursuit techniques to the problem of quantifying the
differences and similarities between different artists and Gharanas of Indian clas-
sical music is a practical solution. The approach for classical musical pieces on the
basis of the Gharanas of the artists would require ﬁrst the comparison of curves to
determine which curves cluster together. These curves would belong to the same
Gharana. Once that is done, further classiﬁcation can be done based on the artist.
Remark: The reader is referred to Mahabharati (2011) for further information
about gharanas. The present work is an extended version of the paper by Mahto
et al. (2014).
Fig. 7.6 Andrews’s plots of raga Malkauns performances by various artists of different Gharana
using standardized PCA scores
Fig. 7.7 Andrews’s plots of raga Marwa performances by various artists of different Gharana
using standardized PCA scores
140
7
Gharana Identiﬁcation

References
Andrews DF (1972) Plots of high-dimensional data. Int Biometric Soc 18(1):125–136. http://
www.mathworks.in/products/matlab/whatsnew.html. Last accessed on 4 Feb 2016
Crawford SL, Fall TC (1990) Projection pursuit techniques for visualizing high-dimensional data
sets. In: Nielson GM, Shriver B (eds) Visualization in scientiﬁc computing. IEEE Computer
Society Press, Washington, pp 94–108
Friedman JH, Tukey JW, (1974) A projection pursuit algorithm for exploratory data analysis.
IEEE Trans on Comput C-23(9):881–889
Jairazbhoy NA, (1995) The Rags of North Indian music: their structure and evolution; Popular
Prakashan. Bombay
Mahabharati S (2011) The oxford encyclopedia of the music of India. vol 1–3, Oxford University
Press, Oxford
Mahto K et al (2014) A study on artist similarity using projection pursuit and MFCC: identiﬁcation
of Gharana from Raga performance. In: Proceedings of the 8th IndiaCom 2014, international
conference on “computing for sustainable global development”, BVICAM, New Delhi, 5–7
Mar 2014 (included in IEEE Xplore, pp 647–653)
Roychaudhary B (2005) The grammar of North Indian ragas. OM Publication, New Delhi
Tukey PA, Tukey JW (1981) Graphical display of data sets in 3 or more dimensions. In: Barnett V
(ed) Interpreting multivariate data. Wiley, New York, pp 189–257
Tzanetakis G, Cook P, (2002) Musical genre classiﬁcation of audio signals. IEEE Trans Speech
Audio Process 10(5):293–302
References
141

Chapter 8
Production, Perception and Cognition
8.1
Introduction
This chapter is devoted to muse over the basic principles behind the making of
music instead of presenting a synopsis of techniques and technologies used in
modern day music production. In the introductory chapter we attempted a com-
prehensive deﬁnition of music. Let us add further that music as an acoustical
emotive (except for the emotion of anger, disgust) communication, generally uni-
versal in nature. The simultaneity of the three elements acoustics, emotion, and
universality (not in the absolute sense) is integral to the deﬁnition.
The early human music was an evolutionary necessity. The theory of evolu-
tionary origin suggests four primary needs e.g. survival, territorial, sexual and
motherese (mother-infant relationship). This last one may push back the origin of
music to the lullaby of Neanderthal mothers about 300,000 years ago. Some argues
that music and speech have evolved by differentiation of early proto-human voice
sounds “Hmmm” (Holistic, multi-modal, manipulative and musical). The devel-
opment was facilitated by vertical posture and walking, which required sophisti-
cated sensorimotor control, a sense of rhythm, and possibly ability for dancing. But
this started changing since the age of intentionality about some 30,000 years back
when aesthetic sense began to evolve in human being. The modern music could
probably have its new direction at this stage entering into the realm of aesthetics
shaking off the shackles of social necessities. In the pre-modern tradition, the art
and beauty of music explored the mathematical and cosmological dimensions of
rhythmic and harmonic organization. In the eighteenth century philosophical focus
shifted to the experience of hearing music. It relates to its beauty and human
enjoyment. The original term aesthetics, meaning sensory perception, now
emphasizes issues besides beauty and enjoyment. For example, music’s capacity to
express emotion has become a central issue. Some issues concerning the aesthetics
of music include lyricism, harmony, emotiveness, temporal dynamics, resonance,
playfulness, and color. The physical reality of music, the acoustic phenomena, is
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_8
143

interpreted by the mind as music through a transformation into ideational entities by
the perceptual and cognitive mechanism. Like any other language of communica-
tion, music also has its own basic units (Bowman 1998; Budd 1985; Adorno 1973,
2002). The foundation of these units lies on two primary sources of knowledge, the
acoustic and the semiotic. Acoustic based units are notes, scales, timbre, laya, tala
etc. The semiotic units are phrases, vadi-samvadi relations, syntax, grammar,
pragmatics, semantics etc. These units, though ideational are not completely per-
sonal. They have objectivity in their apparent subjectivity. One interesting branch
of scientiﬁc study is to ﬁnd the objective correlates of these musical units. These
will be discussed later in sections on perception and cognition.
8.2
Perception
Cognition of signals from the external world involves higher level cortical function
which needs certain pre-processing (Fig. 8.1). Perception is the ﬁrst process of
contact with the external reality as well as the acoustic deconstruction of the aural
reality in the present context. Behavioural processes inside a living organism are
controlled by the bio-electro-chemical processes. The signals from external world
usually use different media. For human beings and for the audio signals ear is the
external organ for transforming (transduction) audio signals into appropriate
bio-chemical and electrical signals. Perception primarily is a pre-cortical process
which alerts the individual of the arrival of the signal. Cognition on the other hand
relates to the understanding of the perceived signal and is a very complex cortical
process which will be dealt with later.
Normally transduction means transformation of the signal from its original
medium to some other selected medium. For example a microphone is a transducer
which converts the air pressure signal to electrical signal. However in the case of
aural signals, between this simple transduction and the cognition in the cortex,
Fig. 8.1 Schematic diagram for the process of auditory cognition
144
8
Production, Perception and Cognition

many complex operations are performed to extract various parameters needed for
this cognition. There is a need to discuss these, at least primarily, under this section
of perception.
Figure 8.2 shows some details of the organ of ear. The role of the outer ear is to
collect and intensify the pressure waves. The primary role of the middle ear is a sort
of acoustic impedance matching between the tympanic membrane and the mem-
brane covering the oval window of the cochlea.
The cochlea of the inner ear is the most important component of human auditory
perception. This coiled tube ﬁlled with ﬂuid is partitioned into three compartments
scala vestibule, scala tympani and scala media (Fig. 8.3). The basilar membrane
separates scala media and scala tympani. The ﬁrst two are joined at the apex.
The organ of Corti (Fig. 8.4) on the basilar membrane constitutes the auditory
transducer. The hair cells in the organ of Corti are the receptors. There are about
25,000 hair cells in a human ear. The hair cells behave like loosely tuned resonators
whose resonance frequency varies from the basal side (high frequency) to the apex
side low frequency.
Fig. 8.2 Organ of ear
Fig. 8.3 Schematic diagram of the uncoiled Cochlea
8.2
Perception
145

These hair cells while converting the pressure function on the membrane into
electro-chemical signal also decomposes the signal simultaneously into a large
number of component frequencies composing the signal. Some neural network
sharpens the tuning curves to facilitate extraction of spectrum at a later stage. The
processed signal from the inner ear goes to the inferior colliculus (IC) and medial
geniculate body (MGB), neural structures before cortex. These neural structures
along with cochlea extracts various parameters particularly related to the spectra
and relay them to the auditory cortex. Both IC and MGB are responsible for
auditory reﬂexes and therefore these are the right candidates to be included in the
section of perception. Efferent ﬁbres from cortex modify neural responses in IC and
MGB in the adaptive learning process. Most of the basic parameters deﬁning a
sound like fundamental frequency (f0), harmonics, formants and loudness are
extracted in auditory nerve ﬁbres. There exist three different theories for extraction
of f0. The place theory which says the place of maximum excitation in cochlea
indicates f0. The frequency theory which says that some neurons get phase locked
in between ﬁrings. The difference of timing between two ﬁrings indicates f0. In the
pattern recognition theory the pattern of responses generated by the upper har-
monics supply the information of the fundamental that would have produced these
harmonics. The last theory is interesting as it give rise to the phenomenon known as
missing fundamental. This may have relevance in understanding the cognitive
phenomena of hearing notes other than the notes of the tuned strings in tanpura with
proper jowari.
Fig. 8.4 Cross section of Cochlea showing organ of Corti
146
8
Production, Perception and Cognition

Even indication of robust detection of consonance and dissonance has been
reported in these neural structures. It is important to note that pre-cognitive or
pre-cortical processing of musical signals is quite elaborate and complex and should
not be identiﬁed with the simple sensing of the signal. Also all these processes does
not involve cortex except in adjusting certain threshold values for ﬁring of these
neutrons. One may thus treat this whole bunch of processing under the heading of
perception.
8.3
Cognition
The word ‘cognition’ originated from the Greek word “cmώrjx”, meaning
learning. Cognition comes from learning, the primary requirement of which is
‘memory’. A very simple objective process of comparison between the signature of
the input signal with that in the memory will allow us to ‘cognize’ i.e. to categorise
the external signal. In our present context the processes of association, concept
formation, and pattern recognition are important for an understanding of cognition.
Human cortex (Fig. 8.5) is not only the most important seat of memory and
therefore the most important element in cognition but also the only mechanism for
performing these processes. The memory organisation is distributive and associa-
tive. Distributive in the sense that at every instance the multimodal information
coming from the external organs are channelled in different areas, referred to as
lobes. These are associative in the sense that for a particular reality all the infor-
mation coming from different modes and going into different lobes are associatively
linked. The concepts or the signiﬁcances are dictated by these associations. The
recall of memory in brain is again associative (content directed), not address
directed as in normal digital computers (Sternberg and Sternberg 2009; Blomberg
2011; Von Eckardt 1996; Matlin 2009).
The important elements in music cognition are identiﬁcation of musical elements
like semantics, emotion, consonance/dissonance, melody, pitch, rhythm etc. The
classiﬁcation and judgement based on previous experience are also integral parts of
cognition. One way to categorize judgment is by comparing it to a similar past
experience within our stored memories. It is done in the pre frontal cortex. This part
of the cortex is highly interconnected with much of the brain, including extensive
connections with other cortical, sub-cortical and brain stem sites and is involved in
classiﬁcation, attention, and analysis.
Pitch
While pitch extraction is primarily a pre cortical process described in some detail
in the section of perception, this tonotopy is in general maintained up to primary
auditory cortex. The right secondary auditory cortex has ﬁner pitch resolution than
the left. As well as ﬁnding superior pitch resolution in the right secondary auditory
cortex, speciﬁc areas found to be involved were the planum temporale (PT) in the
8.2
Perception
147

secondary auditory cortex, and the primary auditory cortex in the medial section of
Heschl’s gyrus (HG).
Melody
Melody (Greek lekῳdίa, meaning, “singing or chanting”) is a linear sequence of
pitches perceived as a single entity. Although rooted in time melodies have their
own temporal structure and phrasing. Melody processing takes place in the sec-
ondary auditory cortex in temporal lobe. The temporal lobe deals with the
Fig. 8.5 Human brain
148
8
Production, Perception and Cognition

recognition and perception of auditory stimuli, memory, and speech. When a
melody was produced activation spread to the superior temporal gyrus (STG) and
planum polare (PP).
Rhythm
The processing of rhythm involves the belt and the parabelt areas of the right
hemisphere. For simple rhythms of regular intervals (1:2 or 1:3) the left frontal
cortex, left parietal cortex, and right cerebellum are all activated. However with
difﬁcult rhythms like 1:2.5 or 1:3.5 more areas in the cerebral cortex and cerebellum
are involved.
Consonance/dissonance
The increased levels of dissonance is known to be correlated with activities in
the right parahippocampal gyrus and right precuneus, while increased musical
consonance is associated with activity in the right orbitofrontal cortex and medial
subcallosal cingulated gyrus. The right para-hippocampal gyrus, which is strongly
activated by dissonant conditions.
Emotion
Emotional response to a musical stimulus is often manifested by changes in heart
rate and respiratory rate. Analysis of cerebral blood ﬂow patterns revealed that the
ventral striatum, midbrain, amygdala, orbitofrontal cortex, and ventral medial
prefrontal cortex are involved in modulation of emotion.
Semantics
The secondary auditory cortex is utilized in sound processing for a wide variety of
purposes, including the possible semantic interpretation of music and linkage of this
interpretation to non-musical constructs. Musical semantics mostly relates to
expression and feeling. It is however not wholly independent of linguistic semantics,
particularly in the case of songs. Even in the case of instrumental music of Indian
Classical genre it has some relation with the linguistic semantics as because of such
compositions are most of the time dependent on lyrical composition.
8.3.1
Signiﬁcance of Cognition
The aforesaid examples indicate that cortical response to external signals is strongly
related to change in the neural activity at different locations in the cortex. However
the questions what cognition means and how do we cognize is not yet explicit.
Cognition is a subjective issue. Therefore even if we understand the material
processes and property changes in the related neural systems for an input the
subjective cognition remains beyond objective formulation. Let us consider the
activation of different region in the cortex corresponding to a particular set of
external signals as a cortical map of the external events. The repeated enervation by
8.3
Cognition
149

similar external inputs would establish a map of associatively linked fuzzy regions.
We shall also see in a later chapter that emotion causes changes in the release of
psychotropic chemicals and consequent changes in certain physiological parame-
ters. As cognition, except the process involved in internal thinking, is related also to
emotional expression it is reasonable to expect physiological manifestation of it.
For example the limbic system consisting of the Hypothalamus, the Amygdala,
and the Hippocampus become active with behavioural manifestations like laughing,
crying etc. (Fig. 8.6).
Cognition may also manifest itself through release of psychotropic substances in
the brain. The neural system responsible for this is called the ascending reticular
activating system. Neurotransmitters like serotonin, norepinephrine, and dopamine
are necessary for normal emotional states and arousal.
We may now possibly venture to deﬁne cognition. Cognition may be deﬁned as
the process of identiﬁcation of the cortical map of enervation due to the advent of
signals from the organs and consequent recall of memories of events, feelings, and
emotions etc., associated with the identiﬁed stored map. The consequent external
behavioural manifestations like laughing, crying etc., or physiological manifesta-
tions like change in pulse rate, skin resistance, blood pressure, respiration rate etc.,
usually accompany cognition. However in case of musing, precipitated by external
organs or the internal organ of mind, cognition may not be associated with the
aforesaid manifestation but could further lead to deeper search of associative links
resulting in creating new links with already memorised events or in strengthening
existing weak links. Such churnings in the subconscious memory may cause what is
known as ‘eureka moments’ (Tan et al. 2010; Thompson 2008; Deutsch 2014;
Gabrielsson 2014; Sloboda 2014).
8.3.2
Some Experiments in Aural Cognition
It may not be out of order to include some cognitive studies on aural signals. It has
already been mentioned that complexities of audio signals are said to be understood
Fig. 8.6 Association of Limbic system in emotional behaviour
150
8
Production, Perception and Cognition

by their spectra. The vowels of a spoken language have different spectral structures.
For at least 70 years the cognition of articulatory positions of vowels are ﬁrmly
believed to depend on spectral structure. These led to the traditional belief that the
cognition processes are based on locating the formant frequencies of the signal.
These formants are resonant structures in the spectra (Fig. 8.7). Vowels are said
to be cognitively differentiated substantively on the basis of ﬁrst two formants
F1 and F2.
There has not been any attempt to question this belief for long. However
question began to appear in 90s in an institute at Kolkata. There signals were
prepared from original speech sounds by suitably doctoring them. The method uses
a cognitive fact that the phonetic quality resides within 2 ms from the epoch in a
period in all voiced speech sounds (shaded part in Fig. 8.8). Keeping the signal
intact for the ﬁrst 2 ms from the epoch, the rest of the waveform is doctored so that
the resonances can be manipulated as required. Such doctored periods are replicated
to produce audible sound signals of required length. This signals so processed are
used for making 13 new vowels segments.
The 13 altered vowel sets are randomly mixed with normal vowel signals and
presented to 30 native educated listeners to identify the vowel in the presented
signal. The result of the listening tests revealed that:
• Even from almost the same values each for F1 and F2 two separate vowels are
distinctly perceived.
• Signals with F2 as high 2500 kHz were perceived as /o/
• All the altered signals are perceived different from the category dictated by the
formant frequencies
• Results were found to be signiﬁcant at 99% level of conﬁdence.
Fig. 8.7 Examples of formants F1, F2, F3 of vowels
Fig. 8.8 Schematic diagram
related to manipulation of
vowel signal
8.3
Cognition
151

These results demonstrate that formant frequencies are neither necessary nor
sufﬁcient for cognition of category of vowels.
The oral/nasal distinction is phonemic in most of the major Indian dialects. The
following parameters all related to the resonances are traditionally believed to be
responsible for cognition of nasality: weakening of F1, strengthening of F2,
increase of F1 and F2, and introduction of nasal formants and anti-formants.
To test the veracity of these traditional assertions 7 minimal pairs of words in
Bangla namely
,
,
,
,
and
spoken by six native speakers were used.
The vowel portions (Fig. 8.9) of the signals of these 84 original words were
doctored by altering one selected harmonic near 400 Hz to convert oral vowel to
nasal and vice versa.
Altogether there were 168 words including both altered and original words
which were randomly ordered and put up for listening tests by nine educated native
listeners. The listeners were instructed to concentrate on oral/nasal distinction. It
must be noted that the original formant/antiformants structures were not changed.
The results of listening tests revealed that:
• Oral vowels converted to nasal were cognised as nasal in 90% cases despite the
contrary evidence of the absence nasal spectral cues.
• The results are signiﬁcant at 99% level of conﬁdence.
For cognition of nasality resonance and anti-resonances cues are neither nec-
essary nor sufﬁcient.
It has been observed that in while buying a string instrument a musician select
one just by hearing the sound by playing a few strokes on the tuned instrument. It
indicates that such an instrument can be efﬁciently graded subjectively by hearing
the sound only (Sengupta et al. 2004, 2005). The objective of the cognitive study
was to see if aural grading of Tanpura by musicians is consistent.
Fig. 8.9 Spectrogram and the harmonic structure of nasality showing nasal/oral distinction
152
8
Production, Perception and Cognition

Twenty three tanpura was taken from the music room of a reputed music
institute and tuned to scales appropriate to their sizes. Four minutes of playing was
recorded by one tanpura player. The listening experiment was conducted with 5
senior scholars, 5 Junior scholars, 6 musicians 3 tanpura players on a four point
scale of gradation: Excellent, Good, Standard, Bad. The listeners were allowed a
free choice towards the number of times he wants to listen for arriving at a ﬁrm
decision. Two separate sessions were conducted at an interval of a fortnight. Of 437
choices 48 shows inconsistency (differing in grading by 2 or more in two different
sittings). Consistency was 88%. Spearman Rank-order Correlation Coefﬁcient
varied from 0.6 to 0.9 for different groups indicating that the ranking by the dif-
ferent informants are signiﬁcantly similar with conﬁdence level 95%. The cognitive
test showed that normal practices of subjective judgement in such practices are
reliable.
Cognition though subjective is not fully idiosyncratic. These studies show that
there are ways in the scientiﬁc domain to assess the extent of generality or pub-
licness of cognitive behaviour of human beings. These also show that in the aural
domain, even in music, cognitive behaviour has credible public reality.
Studies on the nature of complexity of the voice indicated that timbre of voiced
sounds undergo considerable changes from speaking to singing as well as with the
fundamental frequency which undergoes highly systematic meaningful very wide
changes during singing. As early as 1989 the behaviour of the formant frequencies
F1 to F4 of vowels /o/, /ɔ/ and /a/ in singing was studied and reported (Sengupta
et al. 1989a, b). A general trend of rising formants with the fundamental was
reported though speciﬁc relationship could not be observed. The study also reported
a comparison with the corresponding formant structure of spoken vowels
(Fig. 8.10).
Apart from the singer’s formant, acoustic features like resonance balance, center
frequency, bandwidth, asymmetry parameter and spectral energy balance plays
important roles in the study of spectral complexity of singing voice vis-a-vis spoken
voice (Sengupta 1990). Figures 8.11, 8.12 and 8.13 represents such behaviours of
some of them.
Taan
Overall complexity of the voiced signal undergoes signiﬁcant changes (Sengupta
et al. 1989a, b) in producing taan, an embellishment associated with Indian classical
music. This involves very rapid changes in rendering notes. Accordingly, the
spectral structure of formants and the overall musical complexity is expected to
undergo changes. Figure 8.14a shows the change of formant frequencies, including
singer’s formant with the change in fundamental. Figure 8.14b shows a comparison
between the amplitudes of these formants with respect to the fundamental. The
amplitudes of f1 f2 and fs, in general, follows GP with a common ratio of ½.
8.3
Cognition
153

The fast rendering of notes is seen to weaken the singing formants considerably.
It seems that fast transition of notes constrain singers from lowering the larynx
sufﬁciently causing an acoustical mismatch with the vocal tract.
Register
The use of belly (nada), chest and head registers in Indian classical singing have
been studied (Nag et al. 1990). In Fig. 8.15 frequency no 4 denotes transition from
belly to chest and frequency no 8 from chest to head. It is observed that Fs is
strengthened during the change of register (Fig. 8.16).
Musical quality is strengthened during the transition between registers. During
singing an automatic error control mechanism operates to retain quality of voice
during change of register. The inadequacy of this feedback mechanism is respon-
sible for the larger degradation of both phonetic and musical qualities in highest
notes (Banerjee et.al. 1989, Sengupta et.al 1990).
Fig. 8.10 Typical behaviour of formants with respect to F0 for a male and a female singer
154
8
Production, Perception and Cognition

Fig. 8.12 Asymmetry
parameter for vowels sung by
four male and four female
singers
Fig. 8.13 Spectral energy
balance for vowels sung by a
male singer
Fig. 8.11 Singer’s formant for vowel /a/ over complete vocal range
8.3
Cognition
155

8.3.3
Emotion
Hevner in 1936 grouped emotions described by listeners into following groups:
(a) cheerful, gay, happy, (b) fanciful, light, (c) delicate, graceful, (d) dreamy,
Fig. 8.14 a Shows change of formant frequencies with F0. b Amplitude of formant frequencies
with F0
156
8
Production, Perception and Cognition

leisurely, (e) longing, pathetic, (f) dark, depressing, (g) sacred, spiritual, (h) dra-
matic, emphatic, (i) agitated, exciting, (j) frustrated, (k) mysterious, spooky,
(l) passionate, (m) bluesy and represented them in a circle. Emotions recognized in
music can be represented in at two-dimensional space (Fig. 8.17), with valence
(positive vs. negative feelings) and arousal (high–low) as principal axes. These are
the dimensions to describe emotion. Valence refers to the happiness or sadness of
the emotion and arousal is the activeness or passiveness of it. A positive valence
Fig. 8.15 Relationship between F0 and FS For twelve notes
Fig. 8.16 Variation of frequency of singer’s formant against Fundamental frequency
8.3
Cognition
157

corresponds to positive emotions (joy, happiness, relaxing) and a negative valence
with negative emotions (fear, anger and sadness).
The early Indian treaties categorise emotions (rasa) into ten major groups
namely: Heroic (Vira), Anger (Raudra), Serenity (Shanta), Devotion (Bhakti),
Sorrow (Karuna), Romantic (Shringara), repugnant (Bibhatsa) and Horriﬁc
(Bhayankara) of which the last two are not applicable to music. Figure 8.18 shows
the corresponding modiﬁed emotion circle.
Indian treaties also associate ragas with emotions. Ragasare said to be emotion
speciﬁc. A raga speciﬁc emotion study has been reported recently (Sengupta et al.
2012; Wieczorkowska et al. 2010). The test clips were taken from the ‘alaalp’
portion to minimize the role of both timbre and rhythm. The listening tests therein
used 48 raga clips (6 clips from each raga), of 30 s each, randomly extracted from
15 ragas uses responses of 103 naïve listeners. The study also includes examination
of cross-cultural differences between 3 contrasting groups’ namely rural/urban,
science/humanities background and male/female. Pearson’s correlation coefﬁcient
has been used in place of normal ANOVA which uses underlying assumption of
normal distribution of the variable. Result shows speciﬁc signiﬁcant emotional
content of the song clips. No signiﬁcant cross-cultural difference has been observed.
Table 8.1 shows the categorization of the song clips into evaluated groups on the
basis of average value of r (correlation coefﬁcient). In general, the emotional
responses to the song clips are found to be not emotionally confusing. Out of 48
clips responses for only 1 clip each are in the group ‘none’ and ‘small’ correlation.
Fig. 8.17 Possible
descriptions of emotion
Fig. 8.18 Modiﬁed emotion
circle
158
8
Production, Perception and Cognition

In Table 8.2 the number of pairs showing different grades of cohesion is pre-
sented. It may be noted that except Adana, Kedar and Sree all other showed strong
emotional cohesion.
Table 8.1 The categorization of the song clips into evaluated groups
Clip
No
r
Grade
Clip
No
r
Grade
Clip
No
r
Grade
Clip
No
r
Grade
1
0.43
m
13
0.47
m
25
0.59
s
37
0.68
s
2
0.78
s
14
0.65
s
26
0.3
sm
38
0.54
s
3
0.51
s
15
0.95
s
27
0.55
s
39
0.74
s
4
0.94
s
16
0.92
s
28
0.53
s
40
0.64
s
5
0.9
s
17
0.9
s
29
0.86
s
41
0.91
s
6
0.91
s
18
0.58
s
30
0.89
s
42
0.95
s
7
0.84
s
19
0.09
n
31
0.49
m
43
0.88
s
8
0.88
s
20
0.73
s
32
0.73
s
44
0.95
s
9
0.76
s
21
0.8
s
33
0.84
s
45
0.7
s
10
0.77
s
22
0.98
s
34
0.71
s
46
0.39
m
11
0.84
s
23
0.97
s
35
0.71
s
47
0.61
s
12
0.89
s
24
0.4
m
36
0.65
s
48
0.12
n
Legend s strong; m medium; sm small; n none
Table 8.2 The number of pairs showing different grades of correlation
Raga
Grade
Preferred response obtained from
Fig. 8.17
st
m
sm
n
Adana
1
0
1
4
Jogia
6
0
0
0
Devotion, romantic,
serenity
Bhairav
6
0
0
0
Devotion, romantic,
serenity
Kedar
3
0
1
2
Sree
3
0
0
3
Hindol
5
1
0
0
Mian ki Todi
5
1
0
0
Devotion, romantic,
serenity
Mian ki Malhar
6
0
0
0
Devotion, romantic,
serenity
Jayjayanti
6
0
0
0
Serenity
Darbari Kannada
6
0
0
0
Devotion
Chayanat
6
0
0
0
Sorrow/romantic
Basant
NA
NA
NA
NA
Anger
Durga
NA
NA
NA
NA
Heroic, anger
Deepak
NA
NA
NA
NA
Sorrow
Alahiya Dhrupad
NA
NA
NA
NA
Heroic, anger
8.3
Cognition
159

From this study the following conclusions have been reported:
• An oral music segment of only 30 s from the aalap of a raga generally elicit a
speciﬁc emotion.
• All elicited emotions can be speciﬁed into prescribed categories except for ragas
Adana, Kedar and Sree.
• The elicited emotion from different segments from the same raga has strong
speciﬁcity.
• The elicited emotional responses from the segments of a raga rarely correspond
to those given in different treatises.
• The raga Chayanat elicited opposite emotions.
• The elicited response has strong cross-cultural similarity in the following divides
namely rural/urban, science background/humanities-background and male/
female.
8.4
Making Music
We have discussed above some of the fundamental objective and subjective ele-
ments related to the phenomenon music. The basic approach to the making of music
in the aforesaid background may now be ventured into. Music has been, since the
beginning, an important means of acoustical communication. In modern days the
subject of communication in music is primarily the feelings and expressions. Music
is composed on the medium of sound. There are many elements involved in this. In
songs, the lyric, which contains also some semantic pointers to feelings, is one of
them. In Indian genre even instrumental music is said to be primarily based on some
lyrical composition. Thus lyrics, the choice of which is purely subjective, play an
important role in the making of music. Broadly these elements may be divided into
two groups, acoustics and semiotics. The three basic necessary and sufﬁcient
segmental features in acoustics are pitch, timbre and loudness and the sequential
organisation of them. Harmony, rhythm, melody may be grouped into the semi-
otics. Form, performance, and style may be considered as pragmatics.
On acoustics elaborate discussions on pitch, scales and modes, and consonance
and dissonance have been presented in some earlier chapters. Of the three basic
acoustic elements timbre, interestingly, does not have a scale of measure like pitch
or loudness do. This may be deﬁned as the property of the sound by which one can
differentiate two sounds even when there pitch and loudness are the same. This is
closely related to the shape of the wave form and is physically measured by the
spectrum of the sound. Sometimes this prompts to refer timbre as ‘colour’ or ‘tone
colour’ (Erickson 1975). Spectrum of a complex sound is nothing but a list of the
different harmonics (which are simply a series of pure tones with frequencies
multiple of fundamentals present in the signal) and their amplitudes. In Western
music timbre plays an important role in creating musical expression. In Indian
classical music however its role is minimal. However in all other form of music like
160
8
Production, Perception and Cognition

folk and entertainment music, even in this subcontinent its role in creating music
expression is important.
The physical correlate of the loudness or the volume of a sound is the amplitude
of the signal. Its use is said to be quite notable in musical expression and rhythm.
For instance the word dolce indicates a low volume and sweet or tender feeling. Of
the semiotics melody, harmony and rhythm are the most important components.
Melody is what results from playing successive notes of different pitches in an
‘organised’ way. Melodies are very distinguishable and are often singable.
However, just the succession of pitches doesn’t make a melody. The succession
must contain horizontal sonority. The successive notes are generally grouped into
phrases which are melodic in an aesthetic development in a music piece. It is the
pitch relationship of one note to the next which is the signature of a particular
melody. Melody is an essential component of pleasantness.
Rhythm may be deﬁned as the organization of relative durations of sound and
silence (or notes and rests) (Peretz 1990). Of all components of music perception,
rhythm is most fundamentally linked to the movement of time. Each note played
has duration. The relation between durations refers to rhythm. Music has a pulse
(generally synonymous to beats). Even if we do not here it explicitly, it is always
there. The beat happens periodically. But rhythm is not just a constant periodic beat.
Rhythm inhabit the pulse like ﬂesh covers and give forms to its skeleton. Rhythm is
the product of combining notes of different durations. It sometimes coincides with
the beat and sometimes not. The primary role of rhythm seems to deal with the
boredom of monotony. Therefore surprises in rhythm often add to the taste.
Harmony is the study of vertical sonorities in music. Vertical sonority is pro-
duced by the relationships of pitches that occur together, and probably is the most
important element in western music. Ambiguities tend to arise from either aesthetic
considerations (for example the view that only “pleasing” concords may be har-
monious) or from the point of view of musical texture (distinguishing between
“harmonic” (simultaneously sounding pitches) and “contrapuntal” (successively
sounding tones). In general in music the appropriate integration of harmony and
counterpoint is an important element in the armory of a composer. Harmony
comprises not only the ‘vertical’ structure of chords but also their (‘horizontal’)
movement. Like music as a whole, harmony is a process.
Musical texture (Dahlhaus 1979) is the overall sound of a passage or complete
composition. This may be described according to the number of and relationship
between parts of music: monophony, heterophony, polyphony, homophony, or
monody. The perceived texture of a piece can also be affected, inter alia, by the
timbre of the sources of sounds, the number of sources, and the intervallic distance
between each musical line. Its theoretical interest includes its effects on perception,
form, and style. Texture is the way the melodic, rhythmic, and harmonic elements
are combined in a composition, thus determining the perceptual overall quality of
the sound. Texture is also described in terms of the density, and range, or width,
between lowest and highest pitches, as well as to the number of voices, or parts, and
the relationship between these voices.
8.4
Making Music
161

Musical expression (Sorantin 1932) is created by nuances of any phenomena of
sound including timbre, variation of pitch, tempo, volume, etc. It is of particular
interest in the making of music. Expression can be closely related to breath, and the
voice’s natural ability to express feelings, sentiment, deep emotions. Whether these
can somehow be categorized is perhaps the realm of academics, who view
expression as an element of musical performance which embodies a consistently
recognizable emotion, ideally causing a sympathetic emotional response in its
listeners.
So far a brief overview of elements related to the making of music has been
presented. It may be pertinent for discussing the issue of making music to divide the
gamut of music into two broad groups: (a) informal music or ‘free music’ and
(b) formal music or ‘court music’. Folk music, light music, primarily entertainment
focused music is considered under the ﬁrst group, while classical music of various
grades are grouped in the second. As because music involves higher ideational
faculties it may be nicer to use the term ‘create’ instead of ‘make’ in the subsequent
discussions. There exist sharp paradigmatic and structural differences between the
music of the two groups. These differences are very similar to those between spoken
natural language and artiﬁcial textual languages. For example while free speech and
the music of the ﬁrst group has a bottom-up structure, the textual language and the
formal music is a top-down procedure. One could loosely say that free speech and
the music of the ﬁrst group creates grammar whereas grammar creates written texts
and the music of the second group.
The motive or the ‘seed’ of the ﬁrst group of music relate mainly to religious,
ritualistic, occupational and contemporary social issues. All the elements like the
seeds; the sounds (both voice and instruments); the musical elements like notes,
concept of consonance/dissonance, rhythm; the cognitive elements like emotion and
expression etc. are lying like an ocean before the creator. The creation is somewhat
free like the self-assembly of molecules noticed in the evolution of living organism.
The role of creator is that of an assistant to select the appropriate element and judge
the produce by tasting the pudding. The steps involved seem to be:
• Selection of the seed.
• Formation (or selection) of the narrative, generally in the form of a lyric.
• Selection of the sound sources (timbre and pitch of voice or voices, often the
creator himself or herself is the voice source; the accompanying musical
instruments including those for rhythm).
• Selection of expressive note sequences, pauses.
• Selection of rhythm.
• Time alignment of different sound sources.
• Assessment and editing.
It may be noticed that creators need not have formal training; some of them is
known to have only informal musical schooling. The modality of the creation is that
this could be sung by common people even without special musical skills and
would be purposefully absorbed by the common mass. One important characteristic
162
8
Production, Perception and Cognition

of such music is that they do not usually have script and therefore allow high degree
of improvisation. A lacuna associated with the absence of scripting and written
score is that it is almost impossible to objectively visualize the course of devel-
opment of a particular folk music and also the creation process does not have the
beneﬁt of using melodies of somewhat distant past. Since the creation of music in
this group is not constrained by grammars of musical genres it has an immense
potential for diversiﬁcation. In fact it is said that most of the melodies (ragas) of the
formal or court music of India has its origin in some traditional folk songs. This
genre of music will never die and like life it will evolve and acquire different forms,
texture and continue to be a source for entertainment and solace to the common
people for long.
The creation of music of the second group is governed strictly by the grammar of
the particular genre. The performer and/or the creator are profoundly schooled ones.
The music is highly sophisticated made for the elite by the elite. The seeds in this
group are concepts somewhat abstract, more ideational than real or of practical
utility. In the case of Indian genre these mostly relate to interplay of some god,
devotional offerings through music. These concepts sometimes ﬂoat around a single
emotion, as in the case of music of Indian classical genre, or are expressed through
a progression of several emotions leading to the full development of the concept
and a climax, as in the case of western classical. Many of the most esteemed works
of classical music make use of a development process by which a motif is repeated
in different contexts or in altered form. This core composition is then given ﬂesh
through a process usually known as arranging. In the case of western classical
arranging is the task of adapting the vertical sonority through appropriate selection
of sound sources. In the case of western classical use of notation and script seem to
be essential. In the case of Indian classical it is the horizontal sonority which is
paramount. It is generally achieved through the selection of note sequences. There
is no script. Large degree of freedom is given to the performer in terms of pitch,
loudness and duration. However there is strong presence of grammar in terms of
allowable notes and sequences for a raga. In fact performer himself is the composer.
For all kinds of court music, because of the rigidity of adhering to the grammar,
creator has to have exceptional skill and a long period of schooling. All these
discourage diversiﬁcation and growth or evolution. The new produce has a narrow
window. The music of this group ﬁnds it hard to keep pace with the rapid changes
of the modern society.
It is interesting to ﬁnd increasing interest among the elites, both in terms of
participation and creation, towards the ﬁrst group of music. The freedom of for-
aging allows new experiments being undertaken. New forms are created for
appreciation by the crowd keeping the silver line of evolution on for this music. On
the other hand increased pace of the society together with the dearth of innovations
inherent to a doctrinal approach squeezes the appreciation space of the second
group of music. It appears that unless the need to loosen the rigor of the shackles of
classical grammar along with a respect for the appreciation of common mass is
inculcated the music of the second group may not come out of the cloudy future.
8.4
Making Music
163

References
Adorno TW (1973) Philosophy of modern music (trans: Mitchell AG, Blomster WV). Seabury
Press, New York
Adorno TW (2002) Essays on music. In: Leppert R (ed). University of California Press, Berkeley
Banerjee BM, Sengupta S, Nag SD, Dutta AK, Ganguli NR, Mukherjee B, Duttamajumdar D
(1989) Spectral Characteristics of Vowels sung in Hindusthani Sastriya Sangeet. J Acoust Soc
India xvii(3&4) (December)
Blomberg O (2011) Concepts of cognition for cognitive engineering. Int J Aviat Psychol 21(1):
85–104
Bowman WD (1998) Philosophical perspectives on music. Oxford University Press, New York
and Oxford
Budd M (1985) Music and the emotions: the philosophical theories. Routledge & Kegan Paul,
London
Dahlhaus C (1979) Richard Wagner’s Music Dramas. Cambridge University Press, Translated by
Mary Whittall. Cambridge & New York
Deutsch D (2014) Psychology of music, history, antiquity to the 19th century. Grove Music
Online, Oxford Music Online. Oxford University Press. Retrieved 9 Apr 2014
Erickson R (1975) Sound structure in music. University of California, Berkeley and Los Angeles
Gabrielsson A (2014) Psychology of music, history, 1860–1960. Grove Music Online, Oxford
Music Online. Oxford University Press. Retrieved 9 Apr 2014
Matlin M (2009) Cognition. Wiley, Hoboken
Nag D, Banerjee BM, Sengupta R, Dey N (1990) Spectral pattern analysis of vocal registration
during singing. J Acoust Soc India xviii(3&4) (November)
Peretz I (1990) Processing of local and global musical information in unilateral brain-damaged
patients. Brain 113:1185–1205
Sengupta R (1990) Study on some aspects of the singer’s formant in North Indian classical
singing. J Voice 4 (2):129–134
Sengupta R, Dey N, Nag D (1989a) Acoustic comparison of nasal and non-nasal Indian classical
singing. J Acoust Soc India xvii(3&4) (December)
Sengupta R, Dey N, Nag D (1989b) Spectral characteristics of ‘Taan’ in North Indian classical
music. J Acoust Soc India xvii(3&4) (December)
Sengupta R, Dey N, Nag D, Kichlu VK (1990) Changes in the characters and pattern of formants
in spoken and sung vowels of Indian classical singers. In: Proceedings of the national
conference on real time systems, SGSITS, Indore, 1–3 Nov 1990
Sengupta R, Dey N, Nag D, Datta AK (2004) Objective evaluation of tanpura form the sound
signals using spectral features. J Sangeet Res Acad 18:361–381
Sengupta R, Dey N, Datta AK (2005) Assessment of musical quality of tanpura by
fractal-dimensional analysis. Fractals 13(3)
Sengupta R, Guhathakurta T, Ghosh D, Datta AK (2012) Emotion induced by Hindustani Music—
a cross cultural study based on listeners response. In: Proceedings of the international
symposium, frontiers of research on speech and music, Gurgaon, India
Sloboda J (2014) Psychology of music, history, the late 20th century. Grove Music Online, Oxford
Music Online. Oxford University Press. Retrieved 9 Apr 2014
Sorantin E (1932) The problem of musical expression: a philosophical and psychological study.
Marshall & Bruce Company
Sternberg RJ, Sternberg K (2009) Cognitive psychology, 6th edn. Wadsworth, Cengage Learning,
Belmont, CA
Tan S-L et al (2010) Psychology of music: from sound to signiﬁcance. Psychology Press,
New York, p 2
164
8
Production, Perception and Cognition

Thompson WF (2008) Music, thought, and feeling: understanding the psychology of music, 2nd
edn. Oxford University Press, New York, p 320
Von Eckardt B (1996) What is cognitive science?. MIT Press, Massachusetts
Wieczorkowska A et al (2010) On search for emotion in Hindusthani vocal music. Springer edited
volume entitled ‘Advances in Music Information Retrieval’ SCI 274, pp 285–304
References
165

Chapter 9
Automatic Musical Instrument
Recognition
This chapter deals with analysis of musical instruments especially the Indian
musical instruments by analyzing its sound. Sections 9.1, 9.2 and 9.3 concerns the
automatic recognition of musical instruments with the idea that extract the per-
ceptually relevant features from acoustic musical signals that a computer system
“listen” to musical sounds and recognize which instrument is playing. For this,
timbre of the sound of those musical instruments needs to be studied extensively.
Only ﬁve musical instruments which are popularly adopted in Hindustani music
were chosen for study. Among these, two instruments (Tanpura and Sarod) belong
to string category, one instrument each for wood and wind category (Flute), one
reed instrument (Harmonium) and one percussion instrument (Tabla) have been
considered here. Section 9.1 contains a brief description on these musical instru-
ments mainly on their historic and structural perspective.
Section 9.2 provides the identiﬁcation, extraction and in depth study of timbre
parameters by analyzing sound signals from tanpura, sarod, ﬂute, harmonium and
tabla. The results are analyzed and detail discussion for each instruments are done.
This chapter also describe the different timbral and spectral parameters. Among
these parameters, brightness, irregularity, odd and even harmonics, irregularity
among partials, inharmonicity of the signal and spectral centroid were discussed. In
depth analysis of the timbre of these instruments were done leading to an under-
standing of the causes of the timbral changes of these instruments.
Section 9.3 presents the study of musical instrument identiﬁcation by analyzing
the sound signals of above mentioned musical instruments. Identifying musical
instruments by hearing their sound signals can be done by human intelligence
(trained or untrained to music) very easily but it is really difﬁcult to do it artiﬁcially
(by computers). From the time varying harmonic amplitude and frequency data of
those ﬁve musical instruments the spectra of sound were analyzed. Here the spectral
and timbral features were analyzed and resonant frequencies were evaluated from a
dataset of sound samples from these musical instruments. These were utilized to
compare the sound quality of these instruments. These parameters, if properly
utilized, might act as an approach for musical instrument identiﬁcation artiﬁcially.
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_9
167

9.1
Musical Instruments
9.1.1
Introduction
This section contains a brief description on Tanpura, Sarod, harmonium and tabla
both historically and structurally. Tanpura is useful as drone instrument, Sarod is
used in solo performances as a plucking string instrument (as opposed to a bowing
one like violin), harmonium is generally used as an accompanying instrument but
sometimes also in solo performances while tabla is used as a percussion instrument
in both solo and accompaniment.
9.1.2
Indian Musical Instruments
Indian musical instruments are evolved over a long period of time. Each instrument
is special in nature and has its own history and evolutionary phases. In a culture’s
early stages, artifacts, musical Instruments, and lifestyles were simple and basic in
nature. In earlier days all these instruments were very simple and were not capable
of producing many ranges of octaves. They use to produce only the basic rhythm.
As a society progresses, the demands made on musical instruments rise. Thus, most
Indian instruments—although having started in simple forms because of a long
period of evolution—have now become exquisite instruments.
Since ancient period, over hundred types of musical instruments existed in India
due to patronization of different Indian Kings and Nawabs. Some of them were
previously very popular but are obsolete now. Some of the instruments got modiﬁed
with time as per musicians’ perception and changing taste of audience. However,
Indian musical instruments are multifaceted and mostly melodic in nature. In this
chapter we have discussed only ﬁve very popular Indian musical instruments on
which exhaustive work on timbre analysis have been described in Sects. 9.2
and 9.3.
Before entering into the technical discussions, there is a need to know something
about the Indian musical instruments from the historic point of view. It will help to
understand the rich heritage and would serve as a missing link between history and
acoustics. Among the ﬁve Indian musical instruments considered here for analysis,
ﬂute is probably the oldest one. Though there is some controversy about its earliest
existence but it is before Paleolithic period when deer’s bone (with 2–5 holes) was
used for ﬂute making (Buchner 1964). However, through the ages there had been
huge experimentation done in the western music and we have seen a variety of
ﬂutes. The Indian ﬂute which we considered is more or less similar. It has only six
holes made of special type of bamboo and mostly done by the artist himself
(Banerjee 2004–2006).
It is said that Tamburo or Tumburo-vina is the source of modern Tanpura and its
origin is considered to be about as early as 2nd to 7th century. But there is no
168
9
Automatic Musical Instrument Recognition

supportive evidence. As per its nomenclature, there were some musical instruments
in the contemporary music of other countries like Italy etc. But from its simple
structure and applications (drone), it is considered as one of the ancient instrument
(of this genre) in this subcontinent (Banerjee 2004–2006).
Dordur, Dunduvi and Banashpati are known as the source of Tabla creation.
However, fresco and engraved caves and mountains of southern India there are
some depiction of big drum which to some music scholars are the proof of table’s
ancestry in early days. From different evidences one can say that in between early to
middle of 18th century, Tabla was created by Amir Khusru II. He was inspired by
the Pakhawaj (another percussion instrument) during the reign of Allauddin Khilji
(Prajyanananda 1991).
To some, Sarod is a Persian musical instrument which was migrated to India by
the soldiers and it was an accompanying musical instrument and has much simi-
larity with Rabab or Kabli-Rabab. It was during 19th century when Namatullah
Khan improvised and added metal ﬁngerboard and strings to it. Ustad Allauddin
Khan and his brother Ayet Ali Khan are attributed as the makers of modern Sarod
(Banerjee 2010).
Harmonium is comparatively modern and migrated from the West. It was in the
middle of 19th century when French-made hand-pumped Harmonium was brought
into the arena of Indian music by some missionaries. Apart from the historical
evidence it had provided a great impact on the basic Hindustani musical genre;
especially on Hindustani vocal music. Surprisingly, after that it was Indianised and
improvised by some of the great makers of eastern and western part of India
including Dwarka Nath Ghosh proprietor and founder of ‘Dwarkin’. They made it a
scale changer. On the contrary, Harmonium lost its glory and later became obsolete
in western music (Banerjee 2004–2006).
9.1.3
Tanpura
The Tanpura (sometimes also spelled Tampura or Tambura) is a fretless musical
instrument that is played traditionally for accompaniment in Hindustani music
(Matthias et al. 2012). As resonance bodies it has a large gourd and a long volu-
minous neck with four or ﬁve metal strings supported at the lower end by a
meticulously curved bridge made of bone or ivory. The strings are plucked one after
the other in slow cycles of several seconds generating a buzzing drone sound. This
phenomenon is called jwari (pronounced jovari) in musical terms, which means
“life giving”. It is evoked by ﬁne cotton threads that are carefully adjusted between
the bridge and the strings during instrument tuning. The periodic change of length
in the plucked string creates amplitude ﬂuctuations in the higher harmonics so that
the mechanical energy is spread out to very high frequencies. The listener of
Tanpura drone is captivated by its extremely rich harmonic structure. Because there
is a felt resonance in perception, psycho-acoustics of Tanpura drone may provide a
unique window into the human psyche and cognition. The Tanpura is a remarkable
9.1
Musical Instruments
169

drone instrument the sounding of which acts as a canvas in Indian Raga Music and
provides contrast to the tune and melody without introducing rhythmic content of
its own. The jwari phenomenon in Tanpura drones has been found empirically by
musicians and instrument makers in careful contemplation of the nature of sound,
and in classical North Indian Music there is practically no performance without this
drone accompaniment. Drones and jwari became closely related. A drone without
jwari may be as boring as the buzz of a running motor. With jwari the drone is
brought to life and enhances the aesthetic experience of music (Braeunig et al.
2012).
As there is no absolute pitch in the Indian classical music systems so the tanpura
pitch is selected according to the need. The standard tuning is ‘PA-sa-sa-SA’
although various others tuning rule is there in Indian music. Tanpura does not take
part in the melodic part of the music, but it supports and sustains the melody by
providing a colourful and dynamic harmonic resonance ﬁeld based on one precise
tone, the basic note or key note. Also, it is not played to produce any rhythm with
the music. Its tempo is independent of the music it supports, and the speed of
playing may vary throughout a performance or remain relatively constant, at the
discretion of the player.
Classical vocalists and classical musical instrument players of North Indian
classical music use four stringed tanpura to get a drone effect with which they
synchronize the notes they produce accurately in their performances. When the
strings are struck in succession by ﬁngers of an expert player, the resonance
chamber called “Tumba” emits a characteristic melodious sound that creates the
atmosphere for the music. Although the four strings are tuned to three deﬁnite
frequencies, the musician can get the cue of all the notes from harmonics that are
produced strongly and abundantly. Its sound is melodious and it stimulates both the
musician and the audience (Sengupta et al. 2004).
Figure 9.1 shows the schematic diagram of a Tanpura. In a four-stringed
Tanpura, ﬁrst string (made of steel) is tuned to middle note (ma) of the lower
octave. Both the second and third string (made of steel) is tuned to the ﬁrst note of
the middle octave. The fourth string (made of brass wire) is tuned to the ﬁrst note of
the lower octave. The strings go side by side over the bridge, and then along the
neck almost to the top of the Tanpura, ﬁnally over the edge of “Ati” or “Atak”,
which is an ivory strip positioned securely in a slot cut into the convex surface of
the neck, and then through the holes of “Targahan”, yet another strip of ivory or
bone, to the four pegs on which they are twisted and tied. The pegs are held tightly
in holes of the neck, by friction. By twisting the knobs on the pegs, tension of
individual strings can be adjusted separately. Fine adjustment of tension is obtained
by positioning the “Mankas”, through which the strings go, to be held securely in
the wooden block of a triangular cross-section, called Keel, Mongra or Langot,
which is glued to the bottom of “Tumba”, the resonant box. The Tumba is formed
out of a dried shell of class of Indian gourd. This must have the shape of a ﬂattened
sphere, which is slightly re-entrant at the base, but elongated to a cone at the top,
where it is attached to the hollow wooden, nearly semi-cylindrical long neck (called
Dand) which is fretless. About one-fourth of one side of the gourd is cut off to be
170
9
Automatic Musical Instrument Recognition

covered by the slightly convex wooden surface called the “Tabli”. The bridge is a
rectangular strip of deer horn which rest elevated on the wooden blocks, which are
glued to the middle of Tabli (Mukhopadhyay et al. 1998). The resonating shells of
Tumba and Tabli are glued securely to the neck through decorative leaves of thin
wood called “Pattia”. The vibrations of shells of Tabli and Tumba, and to a lesser
extent that of the neck, contribute in major way to the volume, character and
melody of the sound emitted. Periodic excitations by the strings vibrating at stable
and controlled frequencies bestow the emitted sound, its musical characteristics.
Fig. 9.1 Tanpura
9.1
Musical Instruments
171

The tanpura is the only instrument which places a piece of cotton thread between
the string and the bridge, which, when placed correctly, dramatically improves the
volume and quality of sound bestowing it the Tanpura character. This is known as
the adjustment of “jwari” (means strengthening) (Sengupta et al. 2004).
9.1.4
Sarod
The sarod is a stringed musical instrument, used mainly in Indian classical music. It
is among the most popular and prominent instruments in Indian classical music.
The sarod is known for a deep, weighty, introspective sound, in contrast with the
sweet, overtone-rich texture of the sitar, with sympathetic strings that give it a
resonant, reverberant quality. It is a fretless instrument able to produce the con-
tinuous slides between notes known as meend (glissando), which is important to
Indian music. Also the beauty of Hindustani (Indian) Music lies in the production of
meend.
A schematic diagram of Sarod is shown in Fig. 9.2. The conventional sarod is a
17–25-stringed lute-like instrument—four to ﬁve main strings used for playing the
melody, one or two drone strings; two chikari strings and nine to eleven sympa-
thetic strings which are tuned to the micro tones. Sarod is made of teak wood, and a
soundboard made of goat skin stretched across the face of the resonator. Some
artists prefer a polished stainless steel ﬁngerboard for the ease of maintenance while
some uses the conventional chrome or nickel-plated cast steel ﬁngerboard. Visually,
the two variants are similar, with six pegs in the main pegbox, two rounded chikari
pegs and 11–15 pegs for sympathetic strings. Sarod strings are either made of steel
or
phosphor
bronze.
Most
contemporary
sarod
players
use
German
or
American-made strings, such as Roslau (Germany), Pyramid (Germany) and
Precision (USA). The strings are plucked with a triangular plectrum (java) made of
polished coconut shell, ebony, cocobolo wood, horn, cowbone, Delrin, or other
such materials. Early sarod players used plain wire plectrums, which yield a soft,
ringing tone. The lack of frets and the tension of the strings make the sarod a very
demanding instrument to play, as the strings must be pressed hard against the
ﬁngerboard (Patranabis et al. 2008).
There are two approaches to damp the vibrating strings of the sarod. One
involves using the tip of one’s ﬁngernails to stop the string vibration, and the other
uses a combination of the nail and the ﬁngertip to stop the vibration of the strings
against the ﬁngerboard. Fingering techniques and how they are taught depends
largely on the personal preferences of musicians rather than on the basis of school
afﬁliation. Radhika Mohan Maitra, for example, used the index, middle and ring
ﬁnger of his left hand to stop the string, just like followers of Allauddin Khan used
to do. Maitra, however, made much more extensive use of the third ﬁngernail for
slides and hammers. Amjad Ali Khan, while a member of approximately the same
stylistic school as Radhika Mohan, prefers to use just the index and middle ﬁngers
of his left hand.
172
9
Automatic Musical Instrument Recognition

9.1.5
Flute
Flute is a musical instrument of the woodwind family. Unlike woodwind instru-
ments with reeds, a ﬂute is an aero phone or reed less wind instrument that produces
its sound from the ﬂow of air across an opening. Indian ﬂute are mainly played as a
classical instrument and is made of bamboo and is different but simpler than its
western counterpart and are key less. Flute (called Bansuri), has six ﬁnger holes and
one embouchure hole as shown in Fig. 9.3. It is used predominantly in the
Hindustani music of Northern India. The quality of the ﬂute’s sound depends
somewhat on the speciﬁc bamboo used to make it, and it is generally agreed that the
best bamboo grows in the Nagercoil area in South India.
A ﬂute produces sound when a stream of air directed across a hole in the
instrument creates a vibration of air at the hole. The air stream across this hole
Fig. 9.2 Sarod
9.1
Musical Instruments
173

creates a Bernoulli, or Siphon. This excites the air contained in the usually cylin-
drical resonant cavity within the ﬂute. The player changes the pitch of the sound
produced by opening and closing holes in the body of the instrument, thus changing
the effective length of the resonator and its corresponding resonant frequency. By
varying the air pressure, a ﬂute player can also change the pitch of a note by causing
the air in the ﬂute to resonate at a harmonic rather than the fundamental frequency
without opening or closing any holes. To be louder, a ﬂute must use a larger
resonator, a larger air stream, or increased air stream velocity. A ﬂute’s volume can
generally be increased by making its resonator and tone holes larger.
9.1.6
Harmonium
The harmonium is not a native Indian instrument. It is a European instrument which
was imported in India in the 19th century. Although this is a European invention, it
has evolved into a truly bi-cultural instrument. The keyboard is European, but it has a
number of drone reeds which are particularly Indian. European models came in both
hand pumped and foot pumped models. The foot pumped models disappeared in
India many years ago. Also the only advantage of the foot model was that it freed
both hands so that both melody and chords could be played. Indian music has no
chords, so this was no advantage. Although the hand pumped models required one
hand to pump they were more portable and comfortable when played on the ﬂoor.
Indian version of harmonium (as shown in Fig. 9.4) is similar to that of a reed organ
with hand pump. The bellows are the pumps which force the air through the
instrument. There are really two sets of bellows, one internal and one external. The
external bellows are pumped by hand; these are familiar to the average player. The
external bellows then forces the air into the internal bellows. The internal bellows act
as a reservoir for the air. These bellows lay deep inside the instrument and are visible
only by disassembling the instrument. The internal bellows push against a spring; it
is this spring which forces the air over the reeds. The keys, known in India as
“chabi”, are the small wooden controls that the performer ﬁngers to play the music.
There are black keys and white keys. The main stops are a series of valves which
control the way that air ﬂows in the instrument. The main stops control the air
Fig. 9.3 Flute
174
9
Automatic Musical Instrument Recognition

ﬂowing into the various reed chambers. There are usually a minimum of one stop per
reed chamber; however, it is not unusual to ﬁnd more than one per chamber.
Although these extra stops may control special functions, such as tremolo, it is not
unusual to ﬁnd a redundant stop with no special function. This reﬂects the tendency
of Indian musicians to simply open up all the stops, regardless of the function. The
drone stops are the most distinguishing feature of Indian harmoniums. These stops
control the ﬂow of air over un-keyed reeds. They simply drone their particular pitch.
There may be any number of drones set to any pitch; however they tend toward, A
sharp, C sharp, D sharp, F sharp, and G sharp (David Courtney). Although it is a
relatively recent introduction, it has spread throughout the subcontinent. Today, it is
used in virtually every musical genre except the south Indian classical.
9.1.7
Tabla
Besides tanpura, the tabla is the other important and indispensible accompanying
instrument for North Indian classical music. It supports and stimulates the vocalists
and other instrumentalists with the sound of its rhythmic strokes, to help a musician
maintaining the rhythmic cycle which is always present in the music. It can be
tuned to a vocalist’s scale, by adjusting the stretching tension of its membrane.
Indian twin drums mainly bayan and dayan (tabla) are the most important per-
cussion instruments in India popularly used for keeping rhythm Music. It is twin
percussion/drum instruments of which the right hand drum is called dayan and the
left hand drum is called bayan are shown in Fig. 9.5. Tabla strokes are commonly
called as ‘bol’, constitutes a series of syllables. Among the percussion instruments,
‘tabla’ is one of the most important musical instruments in India. Tabla plays an
important role in accompanying vocalists, instrumentalists and dancers in every
style of music from classical to light in India. It is mainly used for keeping rhythm
in Music. The ‘right hand’ drum, called the dayan (also called the dahina, or the
tabla) is a conical (almost cylindrical) drum shell carved out of a solid piece of hard
wood. The ‘open’ end is covered by a composite membrane. The ‘left hand’ drum,
Fig. 9.4 Harmonium
9.1
Musical Instruments
175

called the bayan (also called the duggi) is a hemispherical bowl shaped drum made
of polished copper, brass, bronze, or burnt clay. Both of them have an ‘open’ end,
covered by a composite membrane. The drum head (puri), is unique, and is made of
goatskin, the lao. There is a weight in the middle, the syahi or gub. The syahi is
perfect circle, in the middle of the puri, it is a semi-permanent paste made of coal
dust, iron ﬁllings, and rice paste. Around the outside of the puri, is a ring of thicker
skin, this is called the chanti, this is not attached to the lao. The puri is laced by
buffalo skin straps, baddhi, and tensioned by round wooden ‘chocks’, called gittak.
As an accompanying instrument, Tabla serves the purpose of keeping rhythm in
Music by repeating a theka (beat-pattern) and adorns the vocal/instrumental music
that it is accompanying. In this music, the choice of strokes is precise, each one
functioning like a note in a melody; the timbral and rhythmic structures are equally
important and carefully integrated into a singing line.
Tabla strokes are typically inharmonic in nature but strongly pitched resonant
strokes (Raman 1934). The sounds of most drums are characterized by strongly
inharmonic spectra; however,tablas, especially the dayan are an exception. This was
pointed out as early as 1920 by C.V. Raman and S. Kumar. Raman further reﬁned
the study in a later paper (Ghosh 1922; Raman 1934; Rao 1938). Thereafter several
theoretical and experimental studies were held on the dynamics of the instrument
(Ramakrishna 1957; Sarojini and Rahman 1958; Banerjee and Nag 1991; Courtney
1999). The classical model put forth by Raman represents the sound of tabla-dayan,
as having a spectrum consisting of ﬁve harmonics; these are the fundamental with
its four overtones (Courtney 1999).
Fig. 9.5 Tabla
176
9
Automatic Musical Instrument Recognition

9.2
Acoustical Analysis for the Sound of Indian Musical
Instruments
9.2.1
Introduction
This chapter is mainly about the timbre analysis of the sound signals (solo note) of
tanpura, sarod, ﬂute, harmonium and tabla and is aimed at the identiﬁcation of
timbral characteristics of these musical instruments in a musical progression.
Classiﬁcation of musical instrument sounds is an important issue especially for
Indian musical instruments. The main goal is to maximize the correct classiﬁcation
of the Indian instrument sounds with timbre aspects. Timbre is what enables us to
distinguish sounds from different instruments. Ever since Helmholtz wrote about
timbre perception in the latter half of the 19th century, there have been many
attempts to deﬁne timbre spaces that model the human perception of timbre using
Multidimensional Scaling (MDS). However, currently there is no uniﬁed model that
explains human timbre perception across all instrument families.
Ciglar (2009) in his thesis pointed out a list of different historic timbre
descriptors that were used in music research. American Standards Association made
a deﬁnition of timbre as “Timbre is that attribute of sensation in terms of which a
listener can judge that two sounds having the same loudness and pitch as dissim-
ilar.” Bregman (1990) commented on the ASA deﬁnition as “This is, of course; no
deﬁnition at all … until such time as the dimensions of timbre are clariﬁed, perhaps
it is better to drop the term timbre.” Jensen and Georgios (2001) also commented on
the ASA deﬁnition as “This deﬁnition deﬁnes what timbre is not, not what timbre
is. Timbre is generally assumed to be multidimensional”.
Basic deﬁnition of timbre is still a debatable issue and may cause more confu-
sion than clarity. Here lies the challenge of timbre research. Although timbre is an
important terminology in a most common musical vocabulary as well as it is a
crucial component in the terminology of hearing science, it seems, that a consensus
on a precise scientiﬁc deﬁnition is still due to be achieved.
The focus of this chapter however is on the spectral and temporal character of
timbre. It points out the importance of particular sequences of timbral features as
being a crucial information carrier for the purpose of sound classiﬁcation and
identiﬁcation of Indian musical instruments. The temporal sequence of change is
considered as the timbral structure. The extracted features are important recognition
cues when identifying sounds. The location, strength and the inter-relation of
individual harmonic partials, especially their evolution in the attack and the release
segments, plays an important role for the discrimination of different sounds or
sound sources.
An in-depth analysis of musical sounds using wavelet tool is included. Wavelet
analysis is now a common tool for analyzing localized variations of power within a
time series and to ﬁnd the frequency distribution in time–frequency space.
Distribution of power and behavior of harmonics are the two important features.
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
177

9.2.2
Timbre Parameters
Research, both perceptual, ‘how to perceive a sound’, and signal processing, ‘how
to measure a sound’, gives in principle the same result. Timbre is generally assumed
to be multidimensional, where some of the dimensions have to do with the spectral
envelope, some the time envelope, etc. The difﬁculty of timbre identity research is
often increased by the fact that many timbre parameters are more similar for dif-
ferent instrument sounds with the same pitch, than for sounds from the same
instrument with different pitch. Timbre analysis divides into three feature domains,
that of energy, spectral and harmonic.
9.2.2.1
Energy Features
Energy features are obtained from frequency domain. A harmonic signal consists of
the frequencies that are integer multiples of the fundamental frequency and explain
the content of the digital spectrum. Fundamental frequency can be calculated as the
weighted average of frequencies, normalized over all the harmonics. F0 ¼
P fi=i
ð
Þwi where wi = ai/Pai, is the weight if ith harmonic, ai is the amplitude of
the ith partial, fi is the frequency of the ith partial. For a periodic sound, all the
partials are multiple of the fundamental frequency, that is fi = iF0 (Bregman 1990).
Total Energy estimates the signal power at a given time. It is estimated directly
from the signal frame.
9.2.2.2
Spectral Features
Spectral Centroid of a sound is a concept adapted from psychoacoustics and music
cognition. It measures the average frequency, weighted by amplitude, of a spec-
trum. The standard formula for the (average) spectral centroid (Park 2004) of a
sound is C = PCj/j, where ci is the centroid for one spectral frame, and i is the
number of frames for the sound. The individual centroid of a spectral frame is
deﬁned as the average frequency weighted by amplitudes, divided by the sum of the
amplitudes, or: Cj = P(fjaj)/Paj. Spectral centroid is the centroid of the frequency
of a spectrum.
Spectral spread is the spread of the spectrum around its mean value, i.e. the
variance of the spectral distribution.
Spectral skewness gives a measure of the asymmetry of a distribution around its
mean value.
Spectral Flux is deﬁned as the Euclidean distance between two amplitude spec-
trums of two close frames. SF ¼ P NtðkÞ  Nt1ðkÞ
½
2 where Nt(k) and Nt−1(k) are
respectively the spectral amplitude of frame FFT at instants t and t −1(Park 2004).
178
9
Automatic Musical Instrument Recognition

9.2.2.3
Harmonic Features
Inharmonicity is an attribute to characterize pitched sounds with partial frequencies
deviating from harmonic frequencies. Those are also described as quasi-harmonic,
which implies that the partial frequencies can be either stretched, or compressed.
The frequency of the harmonic partial k can thus be a little higher or lower than k *
fundamental frequency. Inharmonicity is an attribute of stiff strings. This is visu-
alized best by dividing the overtones by their partial tone number/index resulting in
a straight line for perfectly harmonic sounds and in a curve for quasi harmonic
sounds. The inharmonicity is measured as fk ¼ kf0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ bk2


q
, where, f0 stands
for the fundamental frequency while b represents the strength of the inharmonicity.
Inharmonicity can also be measured for each partial and is deﬁned as hk = mod
(fk −n.fo)/(n.fo) where fk is the frequency of the kth partial, k is the number of
partial (Ciglar 2009).
Even-odds energy is a measure for the energy distribution on even and odd
harmonics and is related to the subjective sensation of fullness of a sound, another
important attributes that assist in specifying timbre and it depends upon the ratio of
odd to even numbered partials.
Odd parameters are deﬁned as
odd ¼
X
N=2
k¼2
a2k1
 
!
=
X
N
k¼1
ak;
where ak is the amplitude of kth partial.
To avoid too much correlation between the odd parameter and the tristimulus 1
parameter, the odd parameter is calculated from the third partial. Even parameters
are deﬁned as
even ¼
X
N=2
k¼1
a2k
 
!
=
X
N
k¼1
ak
Tristimulus method of analysis proposed by Pollard and Jansson in 1982, where
the spectral energy is divided into three bands and the energy level for each band is
determined i.e. the tristimulus is also a descriptor for the spectral energy distribution. It
measures the energy in the fundamental-, the next three partials, and the higher partials
in relation to the whole energy. Since the sum of Tristimulus one, -two and -three
equals “1” only two values need to be calculated (Jensen 2002). The tristimulus values
have been introduced as a timbre equivalent to the color attributes in the vision
(Pollard and Jansson 1982). They used it for analyzing the transient behavior of
musical sounds and for classiﬁcation of musical timbre. Tristimulus is deﬁned by the
following three equations:
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
179

tristimulus1 ¼
a1
PN
k¼1 ak
tristimulus2 ¼ a2 þ a3 þ a4
PN
k¼1 ak
tristimulus3 ¼
PN
k¼5 ak
PN
k¼1 ak
where a1 stands for the amplitude of fundamental or 1st harmonic, a2, a3, a4 are the
amplitudes of 2nd, 3rd and 4th harmonics respectively and k is the partial index. So
T1 is the descriptor for the spectral energy distribution of fundamental, T2 is the
descriptor for the spectral energy distribution of next three harmonics and T3 is the
descriptor for the spectral energy distribution of higher partials.
Bregman (1990) remarks that the smoothness of a spectrum is an indicator for
partials belonging to a same sound source and a single higher intensity partial is
more likely to be perceived as an independent sound. It has also been found to be
useful
in
revealing
complex
resonant
structures
of
string
instruments.
Irregularity/spectral smoothness is deﬁned as
irregularity ¼
X
N1
k¼2
a2  ak1 þ ak þ ak þ 1
3


where ak is the amplitude of kth harmonic, ak−1 and ak+1 are the amplitude of
previous and next of kth harmonic. In this chapter, an alternative calculation of the
irregularity is used, where the irregularity is the sum of the square of the difference
in amplitude between adjacent partials,
irregularity ¼
X
N
k¼1
ak  ak þ 1
ð
Þ2
 
!
=
X
N
K¼1
a2
k
and the N + 1 partial is supposed to be zero.
9.2.3
Perceptual Features
Brightness is one of the most important perceptive attributes that assist in specifying
timbre. It is determined by the location of the mean of the energy distribution on the
180
9
Automatic Musical Instrument Recognition

frequency continuum (spectral centroid). Brightness is calculated as the spectral
centroid which is correlated with the subjective quality brightness (Jensen 2002).
The brightness is calculated as
brightness ¼
X
N
k¼1
kak
 
!
=
X
N
K¼1
ak
where k is the partial index, and ak is the amplitude of the kth partial. A closely
related attribute is sharpness. If the partial index multiplication k is replaced with
the frequency of the partial, the brightness is expressed in Hertz. For harmonic
sounds, this is equivalent to multiplying the brightness with the fundamental.
The shimmer (irregularity on the amplitude evolution of the partials) adds a
quality to the sound, which can be both liveliness, and also additive noise. The
shimmer is added as a ﬁltered Gaussian noise to the partial amplitudes. No shimmer
makes the sound dead, unnatural, and excess shimmer increases the noise and hence
the irregularity of the sound.
The jitter (irregularity on the frequencies of the partials) adds a different quality
than the shimmer to the sound. The jitter gives more low-frequency random pitch
variations (with low bandwidth) or it adds roughness (with high bandwidth), going
to an almost screaming quality when the correlation is high. The jitter bandwidth
changes the effect of the sound from low-frequency random pitch variations to
adding roughness to the sound.
9.2.4
Spectral Analysis
Each window of signal can be utilized for the following studies:
1. The difference in amplitude (expressed in dB) between two largest peaks in the
average power spectra for each signals is measured.
2. Similarly, the frequency values for each of these peaks were noted.
3. The r.m.s. value of power of each signal is measured at different frequency
bands (viz. 60 Hz–1 kHz, 1 kHz–3 kHz etc.). Both have to be normalized over
total energy. The ratio of these two energy bands has been used as a discrim-
inatory feature for sound of a musical instrument (Sengupta et al. 2004).
9.2.5
Wavelet Analysis (Transform)
Among non-deterministic approach, wavelet transform is now a common tool for
analyzing localized variations of power within a time series and to ﬁnd the frequency
distribution in time-frequency space. Distribution of power and behavior of har-
monics are the two important features. The whole purpose of wavelet analysis is to see
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
181

the time-scale (frequency) distribution, i.e. how the power changes over time and to
ﬁnd the frequency distribution in time-frequency space. The idea behind these
time-frequency joint representations is to cut the signal of interest into several parts
and then analyze the parts separately. It is clear that analyzing a signal this way will
give more information about the when and where of different frequency components
present (Valens 2004). The continuous wavelet transformer CWT. is written as:
cðs; sÞ ¼
Z
fðtÞw
S;TðtÞdt
where * denotes complex conjugation. This equation shows how a function f(t) is
decomposed into a set of basic functions ws,T(t), called the wavelets. The variables s
and s are scale and translation, are the new dimensions after the wavelet analysis. If
we expand the wavelet transform into the Taylor series at t = 0 until order n (let
t = 0 for simplicity) we get
cðs; OÞ ¼ 1ﬃﬃs
p
f ð0ÞM0s þ f ð1Þð0Þ
1!
M1s2 þ f ð2Þð0Þ
2!
M2s3 þ    þ f ðnÞð0Þ
n!
Mnsn þ 1 þ Oðsn þ 2Þ


where M are moments. From the admissibility condition we already have that the
0th moment M0 = 0 so that the ﬁrst term in the right-hand side of the above
equation is zero. If we now manage to make the other moments up to M0 as well,
then the wavelet transform coefﬁcients (s) will decay as fast as sn+2 for a smooth
signal f(t). This is known as the vanishing moments or approximation order. If a
wavelet has N vanishing moments, then the approximation order of the wavelet
transform is also N. The moments do not have to be exactly zero; a small value is
often good enough. In fact, experimental research suggests that the number of
vanishing moments required depends heavily on the application. Summarizing, the
admissibility condition gives the wave, regularity and vanishing moments gives the
fast decay, and put together they give us the wavelet (Polikar R).
The most important properties of wavelets are the admissibility and the regu-
larity conditions and these are the properties which gave wavelets their name. It can
be shown that square integral functions w(t) satisfying the admissibility condition,
Z
WðxÞ
j
j2
x
j j
dx\ þ 1
This can be used to ﬁrst analyze and then reconstruct a signal without loss of
information. w(x) stands for the Fourier transform of w(t). The admissibility con-
dition implies that the Fourier transform of w(t) vanishes at the zero frequency, i.e.
WðxÞ
j
j2
x¼0
j
¼ 0
182
9
Automatic Musical Instrument Recognition

This means that wavelets must have a band-pass like spectrum (Polikar R). This
is a very important observation, which we have used to build an efﬁcient wavelet
transform. Each signal is passed through a series of high pass ﬁlters to analyze the
high frequencies, and it is passed through a series of low pass ﬁlters to analyze the
low frequencies. A time compression of the wavelet by a factor of 2 will stretch the
frequency spectrum of the wavelet by a factor of 2 and also shift all frequency
components up by a factor of 2.
9.2.5.1
The Sub-band Coding and the Multiresolution Analysis
This split/ﬁltered each of the signal spectrums in two (equal) parts, a low-pass and a
high-pass part. Transition band width is kept minimum to 50 Hz and the offset
information is negligible thus causing no artifact in the split signal. The ﬁrst
high-pass part contains no information. We now have two bands. However, the
low-pass part still contains some details and therefore we split it again and again,
until the last low-pass part contain no information. In this way we have created an
iterated ﬁlter bank. This is done by sub-band coding/pyramidal coding algorithm
using Nyquist’s rule. Why did we split the signal into several frequency bands?
Energy of the signal varies with time. In this process a certain high frequency
component can be located well in time while a low frequency component can be
located better in frequency.
9.2.6
Shimmer and Jitter
In complex musical tone, which as a rule pretend to realize a single frequency value
indicated by pitch (Rasch 1983) of the note in a song, frequency variations are, in
general, always present. The vibrato and trend (slow, long term apparently regular
variations) are distinctly heard as variation in pitch. However, the jitter, shimmer
and complexity perturbation, which on the other hand are short term irregular
variations, do not reﬂect themselves cognitively as pitch or intensity variations. Its
cognitive inﬂuence is the quality of the tone. These random perturbations are
integral phenomenon in natural speech. This is because the vibrating organs are not
rigid bodies but pliant mucosal layers. Its existence in proper measure lends to
naturalness of vocal sound. Also it is known to produce perceptual roughness or
hoarseness to the quality of voice (Sengupta et al. 2001). Listeners are sensitive to
very small amount of jitter. If the relative duration of jitter is increased, then the
listener will evaluate the signal as increasing in roughness. It is said that jitter is the
fundamental property of natural musical sound. The source of jitter in string
instruments are presumed to be again some sort of deviation from the ideal concept
of rigidity, ﬁxed length and tension of strings and the deviation of the shapes of the
ideally sharp bridge of the instrument. The spectral estimation of the fundamental
frequency, which is usually an average over a few pitch periods is extremely steady
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
183

and consistent. The amplitude as well as harmonics shows a regular long term
variation, a sort of waxing and waning. This waxing and waning become shorter at
the higher harmonics. When the adjustment of the position of string of the bridge is
made carefully by trial, the instrument is highly sonorous, giving a tone of ﬁne
musical quality. Each strings of a musical instrument have perturbations and hence
some sort of non-linearity associated with the strings. All such perturbations reveal
variation with fundamental frequency (F0). F0 is calculated using PDA algorithm
(Hess W 1983) and relative jitter and shimmer is calculated using Chicago algo-
rithm (Karnell et al. 1991; Datta et al. 2003).
For jitter and shimmer calculations, cycle period is used for jitter and cycle peak
to peak amplitudes were used for shimmer. In the algorithm the perturbation factor
(PF) is calculated using the expression,
PF ¼
1
ðn  1ÞX
X
n
i¼2
Xi  Xi1
j
j
where Xi is the fundamental frequency or amplitude of the ith period, n is the
number of consecutive cyclic periods and X−is the mean fundamental frequency or
amplitude of n periods.
The
HNR
is
determined
by
the
formula
HNR ¼ n
R T
0 f 2
A s
ð Þdt= Pn
i¼1
R T
0
½fi s
ð Þ  fA s
ð Þdt2 where the resultant wave is fA s
ð Þ ¼ Pn
i¼1
fi s
ð Þ
n and fi s
ð Þ is the signal
for the ith pitch period. So from jitter, shimmer and HNR, a combined coefﬁcient
Mv which is the musicality of voice and is deﬁned as Mv = (µ3 /r3) /{(µ1 /r1) *
(µ2 /r2)} = (µ3 * r1 * r2) /(r3 * µ1 * µ2), where µi and ri represent respectively
the mean and the standard deviation of jitter, shimmer and HNR (Sengupta et al.
1999; Yumoto and Gould 1982; Karnel et al. 1991; Milekovic 1987; Rees 1958;
Rasch 1983).
In usual measures of HNR (Yumoto and Gould 1982), an average waveform is
ﬁrst obtained and the spectral differences of each waveform in the signal from that
of the average waveform is used for arriving at the measure of HNR. The actual
cycle to cycle variation is therefore lost. It is likely that because of this the measure
would be signiﬁcantly less. It has been reported in (Sengupta et al. 1999) that
behavior of HNR with pitch is quite at variance with shimmer and jitter. Also there
is no correlation of HNR with them. These imply that the algorithm used for
measuring HNR is not capturing properly the random pitch to pitch complexity
perturbation. In shimmer and jitter the perturbation is measured locally, in the sense
that differences in contiguous periods are measured and then averaged over the
whole signal. Furthermore this evaluation of HNR also includes noises other than
those caused by random perturbation of complexity originating from the oscillating
folds (in speech). Various researchers cited in (Sengupta et al. 2000) have indicated
the presence of such noises. Also the regular vibrato like undulations observed for
low pitch range for singers indicate some sort of chaotic indeterminacy.
Let yi(t) and yi+1(t) represents the signals for two successive periods Ti and Ti+1
and without any loss of generality let Ti <= Ti+1. Also let Ai and Ai+1 be the
184
9
Automatic Musical Instrument Recognition

respective amplitudes (Sengupta et al. 2000). Then the Complexity perturbation
(CP) for the ith period is deﬁned by
CPi ¼ 1
T
P
Ti
j¼1
yiðtjÞ  Ai
Ai þ 1 yi þ 1ðtjÞ


CP ¼ 1n
P
n
CPi
;
where n runs over the entire signal.
9.2.7
Analysis of Acoustic Characteristics of Musical
Instrument from Their Sound Signals
Let us discuss the general characteristics of timbral features by analyzing short
segments like single notes in the context of sound signals of Tanpura, Sarod, Flute,
Harmonium and Tabla.
9.2.7.1
Tanpura
The Tanpura drone, tuned and plucked by trained musicians, with and without
jwari, was recorded and was analysed. Although the four strings are tuned to only
three frequencies viz., the ﬁrst note of the lower octave, the middle note of the
lower octave and the ﬁrst note of the middle octave, the musician get the cue of all
the seven notes. This is because they are trained to hear the harmonics, and to match
with the harmonics of their own musical tones. Perhaps they also recall these tones
from their memory to help in this matching. This becomes easy due to the abun-
dance and strength of harmonics in the Tanpura tones, which exceed the funda-
mental up to at least 1000–1500 Hz. In this frequency range, the sung tones also
have similarly strong harmonics. Harmonics are very strong in the ﬁrst two for-
mants, and extend to 2000 Hz with large amplitudes (Sengupta et al. 1983).
Some form of regular variation of complexity in tanpura signals has been
noticed. Figures 9.6 and 9.7 shows the amplitude proﬁle of the thick string
Fig. 9.6 Amplitude (dB2)
proﬁle for SA of tanpura
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
185

(SA) and combined stroke of all the strings. Figure 9.15 shows the 3D spectra of a
Tanpura signal and its LTAS. Figure 9.16 shows the average power spectra of a
window showing two largest peaks. Figure 9.17 shows amplitude proﬁle of a
selected frequency component and curves representing the absolute variations of
amplitude. This is only for visual depiction (Datta et al. 1988).
The amplitude shows a regular long-term variation and also been noticed that the
harmonics of tanpura strings exhibit a periodic waxing and waning. Amplitude
ﬂuctuation is a major criterion of tanpura sound signals. It has been observed that
the sound spectrum of each string of tanpura has very regularly occurring bands of
spectral peaks which sustain for long periods (Sengupta et al. 1996a, b). Multiple
decays and the amplitude perturbations can be very well observed. Hence the
signals emitted by a Tanpura are characterized by varying complexity with undu-
lations of intensity of different harmonics with different frequencies as well as
multiple decay. Thus it can be stated that time variation of each frequency com-
ponent plays an important role towards the timbral quality of tanpura. The relative
amplitude perturbation factor (shimmer) is higher within 100 ms of the thick string,
SA. It is the order of 0.04.
In order to study the timbral quality we have taken some timbre parameters
(shown in Table 9.1) to see their behavior for each strings and combined stroke of
all strings. T1, brightness and irregularity of the string 1 differs from the other
strings. A low value of brightness conﬁrms its low energy and a high irregularity
conﬁrms a higher amplitude perturbation. Other three strings show similar timbral
characteristics. Another interesting observation is that the timbre parameters of all
strings (drone) is exactly similar with average of individual four strings except for
irregularity (Fig. 9.12) and centroid (Fig. 9.13). Perhaps this is the reason tanpura is
used as a drone instrument in Indian music. High standard deviation of brightness is
due to poor timbre quality of string 1 (Sengupta et al. 2007).
Each of the sound signals of each string was divided into ten equal time win-
dows each of 0.5 ms. The timbral parameters were studied in each window for each
string and the combined strokes. Figures 9.8, 9.10, 9.12 and 9.14 shows that the
string 1 (Pa) lacks in brightness; its irregularity is high and the tristimulus values
different from other strings. The timbral quality of this string therefore is poor. This
is also corroborated by hearing the sound of the signal. The values of T1, T2, T3
(Figs. 9.8, 9.9, 9.10 and 9.11) and irregularity (Fig. 9.12) shows consistent results
at the initial time (attack region). As time elapses their values get diverged. So a
Fig. 9.7 Amplitude (dB2)
proﬁle for combined strokes
of tanpura
186
9
Automatic Musical Instrument Recognition

Fig. 9.8 Variation of T1 with time for strings of tanpura
Fig. 9.9 Variation of T2 with time for strings of tanpura
Fig. 9.10 Variation of T3 with time for strings of tanpura
Fig. 9.11 Variation of T2 with T3 for strings of tanpura
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
187

uniqueness of timbral quality is observed during attack but defuses during decay.
We see that T1 and T2 increases with time for all the strings but the picture is
reverse in T3; it decreases with the increase of time. Variation of T3 with T2 graphs
(Fig. 9.11) shows the concentration of the points towards the high T3 shows the
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Irregularity
Time (ms)
string 1
string 2
string 3
string 4
all strins
Fig. 9.12 Variation of Irregularity with time for strings of tanpura
Fig. 9.13 Variation of Centroid with time for strings of tanpura
Fig. 9.14 Variation of Brightness with time for strings of tanpura
188
9
Automatic Musical Instrument Recognition

existence of strong high frequency partials. This indicates that the tanpura sound
picks up energy at high frequency partials. The sound persists with good amplitude
and insensible decay over the ﬁrst half and it starts decay in the later half when
frequency clusters are rapidly eliminated.
Correlation coefﬁcients of timbre parameters are shown in Table 9.2. Even
harmonics and irregularity are the two timbre parameters weakly correlated with the
Fig. 9.15 3D spectra of a Tanpura signal and its LTAS. This is only for visual depiction
Fig. 9.16 Average power spectra of a window showing two largest peaks
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
189

other parameters. So brightness, tristimulus, odd harmonics and spectral centroid
are the most important timbre parameters to be considered for tanpura sound
analysis. High correlation of inharmonicity with T2 and T3 implies that tanpura
sound loses its harmonic nature at higher energy partials. The special overtone-rich
sound is achieved by applying the principle of ‘jwari’, which creates a sustained
“buzzing” sound in which particular harmonics will resonate with focused clarity
(Sengupta et al. 2007).
For the study of inharmonicity, we have studied the HNR (Harmonic to Noise
ratio) of the signal. As all the recordings are done in an acoustically treated room,
the HNR, particularly the difference of them for different signals may be attributed
to the inharmonicity inherent in the tanpura signals. The algorithm for calculating
HNR performs acoustic periodicity detection on the basis of a forward cross cor-
relation analysis. Slope of the power spectra in the region of 1–3 kHz (since it was
found that most of the energy concentration remains within this band) was taken as
a special measure of spectral structure. Jwari is a special phenomenon, which is
supposed to be related to the waxing and waning of the harmonics of its sound. It is,
therefore, natural to assume that the extent of the waxing and waning would be a
measure of the amount of jwari. One quick way to get the estimate is to select the
most prominent harmonic and measure the variation of its waxing and waning.
Therefore, for estimation of jwari, the most prominent frequency component in each
signal was ﬁltered out and its amplitude proﬁle was evaluated. Then a nine point
moving average (smoothing) was performed on the amplitude proﬁle. Figure 9.17
shows the amplitude proﬁle of the selected frequency component and its smoothed
version in a signal. The difference of the two above mentioned proﬁles and the same
excluding the outlier gives the measure of Jwari. Sengupta et al. (2003) observed by
analyzing two largest peaks in a window concludes to the fact that the frequency of
1st peak reveals some distinction and information from 1st two peaks categorizes
good and bad tanpuras.
Fig. 9.17 Amplitude proﬁle
of a selected frequency
component and curves
representing the absolute
variations of amplitude
190
9
Automatic Musical Instrument Recognition

Table 9.1 Timbral parameters for each string and the combined stroke of all strings of Tanpura
T1
T2
T3
Brightness
Odd parameter
Even parameter
Irregularity
Centroid
String 1
0.04909
0.03740
0.91350
14.1579
0.51760
0.43330
0.25735
11.73582
String 2
0.02279
0.04326
0.93394
26.39048
0.48676
0.49044
0.05029
11.95129
String 3
0.01359
0.03709
0.94930
29.72608
0.48369
0.50270
0.05713
12.08305
String 4
0.02409
0.04693
0.92896
21.83658
0.49138
0.48451
0.06711
11.98633
All strings
0.01038
0.04385
0.94575
24.89731
0.48737
0.50223
0.05048
8.88564
Average
0.02739
0.04117
0.93143
24.02776
0.49486
0.47774
0.10797
11.93912
Standard deviation
0.01520
0.00430
0.01429
5.89628
0.01382
0.02865
0.09019
1.37144
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
191

In another experiment fourteen tanpuras were considered of which 10 tanpuras
had 4 strings and 4 tanpuras had 6 strings. These were tuned and plucked by an
expert. Shimmer, Jitter and complexity perturbation (CP) parameters were mea-
sured. Figure 9.18 presents the scatter diagram for all parameters and all samples
against pitch. In general, the scatter for jitter and shimmer appears to be quite large
and without any trend with respect to pitch. CP shows low scatter along with low
values. CP becomes a little more prominent at higher pitch values. The scatter also
increases. Also Table 9.3 presents the value of shimmer and jitter for individual
tanpura averaged over all strings existing in that instrument along with the coefﬁ-
cient of variation. The range for average value of jitter over all instruments is
0.004–0.008. The range for shimmer and CP are respectively 0.004–0.016 and
0.002–0.008. It is interesting to note that the CP is insigniﬁcant w.r.t. the other two
parameters. This indicates that all these perturbations are not coming from the same
source. Furthermore, this indicates that there is no signiﬁcant change in the
period-to-period wave shapes and therefore of Fourier structures. This again means
that the signal shaping source remains invariant locally. The absence of complexity
Table 9.2 Correlation coefﬁcient of timbre parameters of tanpura sound
T1
T2
T3
Odd
parameter
Even
parameter
Irregularity
Centroid
Brightness
−0.9849
−0.9656
0.99025
0.8227
−0.5650
−0.496
0.98
T1
0.91876
−0.9638
−0.9024
0.6868
0.4884
−0.94
T2
−0.9908
−0.6590
0.3441
0.6369
−0.94
T3
0.7550
−0.4683
−0.598
0.96
Odd
parameter
−0.9329
−0.235
0.76
Even
parameter
−0.012
−0.5
Irregularity
−0.36
Fig. 9.18 Scatter plot for all parameters against pitch for all samples
192
9
Automatic Musical Instrument Recognition

perturbation indicates that the source of perturbations is not located in the main
resonating structures of the instrument. The vibrating string itself i.e. the string with
its attachment is likely to be the source of perturbations. The coefﬁcient of variation
is an indicator of the variations of the parameters of an individual tanpura over all
the strings. In general shimmer appears to be more coherent with a range between
0.105 and 0.36 and jitter least with the corresponding range between 0.165 and
0.623. Also the coherency does not appear to be some characteristics of a particular
tanpura since there is usually a wide range of discrepancy in the relative value of
perturbation for any one tanpura. This indicates that the perturbation is likely to be
substantially caused by the same mechanism though they may be caused by the
same physical structures.
It has been observed that the perturbations of tanpura sounds are small compared
to human speech. Also tanpura sounds have negligible complexity perturbation and
no correlation of shimmer and jitter is found with the pitch. But interestingly
shimmer and jitter perturbations are almost similar to that of human voice. Such
variation of perturbation is due to the source characteristics and hence the jwari of
tanpura.
9.2.7.2
Sarod
The tonal quality of Sarod differs at different harmonic ranges. Its complexity
allows the musician to diverge the sound and mood and so it remains a leading
musical instrument in India for a long time.
Table 9.3 Average value of Shimmer, Jitter and CP with their ƴ for individual tanpura
Tanpura
number
Scale
Shimmer
Jitter
CP
Average
c
Average
c
Average
c
1
C#
0.0095
0.1053
0.0025
0.4000
0.0028
0.1812
2
D
0.0118
0.1071
0.0043
0.4851
0.0020
0.4062
3
D
0.0155
0.1235
0.0075
0.4422
0.0035
0.3689
4
G
0.0095
0.2183
0.0050
0.5657
0.0052
0.3101
5
G
0.0045
0.1859
0.0052
0.2849
0.0028
0.1441
6
D#
0.0088
0.1323
0.0078
0.5133
0.0025
0.3347
7
A
0.0055
0.1050
0.0070
0.1650
0.0035
0.2657
3
B
0.0060
0.2722
0.0060
0.6236
0.0040
0.4564
9
D#
0.0088
0.1505
0.0040
0.4743
0.0028
0.7666
10
B
0.0095
0.3039
0.0060
0.4907
0.0056
0.3292
11
B
0.0043
0.3529
0.0065
0.3203
0.0030
0.2722
12
G
0.0060
0.3600
0.0203
0.2918
0.0075
0.4929
13
G
0.0070
0.2608
0.0035
0.5471
0.0025
0.2309
14
D#
0.0110
0.1286
0.0070
0.5632
0.0036
0.2553
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
193

Figures 9.19, 9.20, 9.21 and 9.22 shows the amplitude proﬁle of only four
strings one from each type out of 18 strings. Amplitude of drone, chikari and
sympathetic strings show a regular long-term variation with time and it has also
been noticed that the harmonics of these sarod strings exhibit a quasi-random
Fig. 9.19 Amplitude (dB2) proﬁle of main string Da of sarod
Fig. 9.20 Amplitude (dB2) proﬁle of another main string Da (drone) of sarod
Fig. 9.21 Amplitude (dB2) proﬁle chikari string Da of sarod
194
9
Automatic Musical Instrument Recognition

waxing and waning. Amplitude ﬂuctuation is a major criterion of the supporting
strings of sarod sound signals (Figs. 9.21 and 9.22). But the melodic strings show
the reverse characteristics (Figs. 9.19 and 9.20). It has been observed that the sound
spectrum of each string of sarod has very irregularly occurring bands of spectral
peaks which sustain for long periods.
From Figs. 9.23 and 9.26 we can observe that all main strings and 1st chikari
string shows a sharp transition of brightness and centroid among attack, steady and
decay part while the same is negligible for sympathetic and supplementary strings.
That is why these supporting strings (sympathetic and supplementary) are mainly
used for rhythm and drone while main strings are used for playing melody. This is
so because the timbre variations are the least for supporting strings while maximum
for main strings. Among all these strings soft-sympathetic strings produces sound of
Fig. 9.22 Amplitude (dB2) proﬁle of sympathetic string of sarod
Fig. 9.23 Brightness and T1 at attack, decay and steady state of different strings of sarod
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
195

least energy. These strings are tuned to a deﬁnite frequency and they vibrate due to
resonance effect of main strings and gives ﬁller to the sarod sound, making the
sarod sound rich.
From tristimulus graphs (Figs. 9.23 and 9.24) it is observed that the main strings
have high energy in fundamental and 1st three harmonics while low energy in
higher harmonics. But sympathetic strings show reverse characteristics. Energy of
odd harmonics and even harmonics (Fig. 9.25) are equally distributed in the
Fig. 9.24 T2 and T3 at attack, decay and steady state of different strings of sarod
Fig. 9.25 Odd and even parameters at attack, decay and steady state of different strings of sarod
196
9
Automatic Musical Instrument Recognition

waveform of sarod string sounds. The thick string shows wider irregularity
(Fig. 9.26) among its partial.
During attack and steady state, we ﬁnd concentrations of tristimulus values of
main strings at mid frequency ranges (top left corner of T2 vs. T3 graph) which is
not so for decay state. So this conﬁrms the fact that the harmonics of main strings of
sarod are rich in the lower and mid frequency ranges. Concentration of tristimulus
values at the high frequency ranges (low right corner of T2 vs. T3 graph) for
sympathetic strings conﬁrms the fact that they have too weak fundamental. This is
observed in Fig. 9.27. So, in general, sarod sound is rich in texture in lower notes
but higher ranges are less rich and sympathetic strings are tuned to micro notes
(Table 9.4). Legends used in Figs. 9.23, 9.24, 9.25, 9.26 and 9.27 are shown in
Table 9.4.
Table 9.5 shows the correlation coefﬁcient and standard deviation of timbre
parameters of sarod sounds. High standard deviation of inharmonicity conﬁrms the
greater ﬂuctuations of energy of harmonic partials which also correlated with high
correlation coefﬁcient of brightness with T1, T2 and irregularity. Main strings have
more inharmonicity compared to other supporting strings. High correlation of odd
and even harmonics conﬁrms low hollowness of sarod sound. Hence sarod has rich
timbre characteristics which make it popular among Indian listeners.
Fig. 9.26 Irregularity and centroid at attack, decay and steady state of different strings of sarod
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
197

9.2.7.3
Flute
Generally, the quality called timbre or “tone color” varies because the ﬂute can
produce harmonics in different proportions or intensities. Table 9.6 shows the
correlation coefﬁcient and standard deviation of timbre parameters of ﬂute sounds.
From Figs. 9.30, 9.31, 9.32 and 9.33 we may conclude, in general brightness of
ﬂute sound is low and almost uniform for all notes. But centroid increases with the
length, which means increasing energy in the higher partials which again depends
upon the length of the ﬂute because larger space provides more energy to excite the
air’s resonance. So the tone color can be modiﬁed by changing the internal. This is
used for playing melody (Figs. 9.28 and 9.29).
9.2.7.4
Harmonium
From Figs. 9.34, 9.35, 9.36, 9.37, 9.38, 9.39 and 9.40 we can observe that low pitch
sound of harmonium have low brightness and rises at mid pitched sound and again
Fig. 9.27 T2 versus T3 at attack, steady and decay state
Table 9.4 Legends used in Figs. 9.23, 9.24, 9.25, 9.26 and 9.27
198
9
Automatic Musical Instrument Recognition

Table 9.5 Correlation coefﬁcient and standard deviation of timbre parameters of sarod sounds
T1
T2
T3
Odd parameter
Even parameter
Irregularity
Inharmonicity
Centroid
Brightness
−0.244
−0.942
0.925
−0.491
0.4406
−0.8412
0.6237
0.756
T1
0.376
−0.467
0.5040
−0.8287
0.2408
−0.1820
−0.088
T2
−0.995
0.4020
−0.4494
0.9523
−0.5709
−0.720
T3
−0.4379
0.5181
−0.9349
0.5645
0.696
Odd parameter
−0.9011
0.2608
−0.2252
−0.213
Even parameter
−0.2899
0.2373
0.182
Irregularity
−0.5581
−0.729
Inharmonicity
0.962
Standard deviation
0.0183
0.1622
0.1699
0.0236
0.0363
0.0816
4.1991
0.2365
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
199

falls at higher pitched sounds. So the energy of the partials is high at mid frequency
ranges but low at lower and higher frequency ranges. Again fundamental and next
three harmonics are very strong for the partials at mid frequency range but partials
at high and low frequency ranges have strong higher harmonics but weak funda-
mental and next three harmonics. Odd and even partials are equally spaced and
energy distribution is shared equally among them.
Table 9.7 shows the correlation coefﬁcients of timbre parameters of harmoium
sound. Spectra of harmonium sound shows a higher degree of irregularity among its
partials at high frequency ranges but less at mid frequency range. Thus less per-
turbation at the mid frequency ranges are observed. Harmonium sound is highly
inharmonic in nature. Brightness, tristimulus and irregularity are highly correlated
thus conﬁrm the above ﬁndings. A higher value of brightness and inharmonicity
proves the fact that harmonium sounds larger perturbation in energy among partials
and larger deviation from harmonic sound.
9.2.7.5
Tabla
This section reﬂects the timbre and spectral characteristics of nine strokes from each
of ﬁve tablas using Wavelet transform. Membrane of tabla 1, 2 and 3 have diameter
5′′, tabla 4 has a diameter 5.5′′ and tabla 5 has a diameter 6′′. This analysis leads to
categorization of strokes and also the structure of tabla. Tabla playing has a very
well developed formal structure and an underlying “language” for representing its
Fig. 9.28 Brightness of 5 notes (X axis) of ﬂute sound signal
Fig. 9.29 Cendroid (Y axis) of 5 notes (X axis) of ﬂute sound signal
200
9
Automatic Musical Instrument Recognition

Table 9.6 Correlation coefﬁcient and standard deviation of timbre parameters of ﬂute sounds
T1
T2
T3
Odd parameter
Even parameter
Irregularity
Inharmonicity
Centroid
Brightness
−0.9728
−0.737
0.8993
0.4662
0.0139
−0.3259
0.2329
0.703
T1
0.5609
−0.774
−0.255
−0.239
0.1504
−0.3797
−0.637
T2
−0.958
−0.933
0.6593
0.7227
0.2344
−0.616
T3
0.8016
−0.421
−0.6045
−0.0480
0.691
Odd parameter
−0.877
−0.8325
−0.3042
0.450
Even parameter
0.7614
0.4936
−0.136
Irregularity
0.1104
−0.655
Inharmonicity
−0.024
Standard deviation
0.5398
0.0512
0.1133
0.1482
0.1038
0.1034
0.1236
2.4917
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
201

sounds. Tabla ‘bol’s constitutes a series of syllables which correlate to the various
strokes of the tabla. Here we have considered nine tabla strokes namely Ta, Na, Ti,
Teen, Tu, Thun, Ghe, Ge, and Re. Stroke ‘Ta/Na’ is executed by lightly pressing
the ring ﬁnger down in order to mute the sound while index ﬁnger strikes the edge.
Stroke ‘Ti’ is executed by striking the dayan on the 2nd circle with the index ﬁnger
and by keeping the ﬁnger on that position causes more damping but after striking if
the index ﬁnger release quickly to give an open tone it produces ‘Teen’. Stroke
Fig. 9.30 T1 (Y axis) of 5 notes (X axis) of ﬂute sound signal
Fig. 9.31 T2 (Y axis) of 5 notes (X axis) of ﬂute sound signal
Fig. 9.32 Odd and even parameters (Y axis) of 5 notes (X axis) of ﬂute sound signal
202
9
Automatic Musical Instrument Recognition

Fig. 9.33 Irregularity parameters (Y axis) of 5 notes (X axis) of ﬂute sound signal
Fig. 9.34 Brightness of all reeds of harmonium
Fig. 9.35 Tristimulus 1 for all reeds of harmonium
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
203

‘Ghe’ is executed by striking the bayan with middle and index ﬁnger keeping the
wrist on the membrane but after striking if released quickly it produces ‘Ge’. Stroke
‘Thun’ is executed by striking on the centre circle of dayan with index, middle, ring
and little ﬁngers together and by quickly releasing. Stroke ‘Tu’ is executed by
striking at the corner of centre circle of dayan with index ﬁnger only and imme-
diately after striking ﬁnger will lift. Stroke ‘Te’ is executed by striking the dayan
with middle and ring ﬁnger at the centre of the circle. Stroke ‘Re’ is executed by
striking the dayan with index ﬁnger at the centre of the circle and by keeping the
ﬁnger on that position causes more damping.
Fig. 9.36 Tristimulus 2 of all reeds of harmonium
Fig. 9.37 Tristimulus 3 of all reeds of harmonium
204
9
Automatic Musical Instrument Recognition

In the process of sub band coding each of the signals are divided into eight
frequency sub bands of band width 7040–14,080 rad/s referred to as 1st band/DWT
1st level, 3520–7040 rad/s referred to as 2nd band/DWT 2nd level, 1760–
3520 rad/s referred to as 3rd band/DWT 3rd level and so on. From each of these
sub bands number of harmonics, number of DWT coefﬁcients and most prominent
frequency (MPF) is measured. Fidelity factor Q (Polikar R) here is ½. By this
process the scale of each signal gets doubled, reducing the uncertainty in the
frequency by half.
DWT 1st level (7040–14,080 rad/s) and DWT 8th level (55–110 rad/s) reveals
no information. So, all the information of tabla lies within 7 kHz.
Fig. 9.38 Odd and Even parameters of all reeds of harmonium
Fig. 9.39 Irregularity of all reeds of harmonium
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
205

Figure 9.41 reveals that the MPF corresponds to the highest peak of each
sub-band, is uniform during the lower frequencies, but varies in high frequency
bands. Hence maximum harmonic information lies in the range 0.9–2.5 kHz. The
damping effect of Tee against Teen and Ghe against Ge is visible in 440–1760 Hz
although they are similar strokes.
Stroke Ta/Na (where the position of stroke and damping are different) of all
tablas shows that the peak reaches attack very late compared to all other strokes.
Attack peaks of all strokes of 1st tabla delay to reach (found in the 6th of its band
structure). Attack peak of all strokes of 2nd tabla reaches early (found in the 7th of
its band structure). So the players of 1st and 2nd tablas made the strokes consis-
tently. No such consistency is found in achieving attack peak for the rest of the
Fig. 9.40 Inharmonicity of all reeds of harmonium
Table 9.7 Correlation coefﬁcients of timbre parameters of harmoium sound
T1
T2
T3
Odd
parameter
Even
parameter
Irregularity
Inharmonicity
Brightness
−0.8352
−0.8756
0.8932
0.6131
0.2642
−0.8220
−0.5180
T1
0.8392
−0.9096
−0.7253
−0.3285
0.8409
0.1813
T2
−0.9892
−0.6605
−0.2045
0.7148
0.2246
T3
0.6994
0.2445
−0.7719
−0.2203
Odd
parameter
−0.4119
−0.6501
−0.2292
Even
parameter
−0.2211
0.0744
Irregularity
0.4959
Standard
deviation
0.0464
0.1319
0.1727
0.0481
0.0350
0.0706
5.2149
206
9
Automatic Musical Instrument Recognition

three tablas because of their differences in stroke productions. Attack peak also
depend upon the stroke type and position viz. stroke Thunexecuted by striking on
the centre circle with index, middle, ring and little ﬁngers together and by quickly
releasing, reaches the attack peak immediately. The stroke Ta/Na executed by
lightly pressing the ring ﬁnger down in order to mute the sound while index ﬁnger
strikes the edge, reaches the attack peak delayed. It is also found that attack peak
reaches faster in free strokes than damped strokes.
Also a shorter decay is observed for Re, Te, Tu and Na while longer decay for
Ghe, Ge, Tee and Teen. MPF of damped strokes show lesser time of occurrence
compared to free strokes. Calculating the scale (frequency) and time of all the
harmonics, it is observed that lower harmonics in the attack portion are always
lagging. It is also observed that the higher energy bands are of shorter duration than
the lower energy bands. In comparison to other strokes, Teen has longer duration of
energy in all the bands.
From Fig. 9.42, Damped strokes have larger number of harmonics at high fre-
quency ranges (band 3 and 4) for all the tablas compared to free strokes. Comparing
the number of harmonics at higher frequency range we can categorize the stroke of
tablas. For the 1st, 2nd and 3rd tablas, all strokes produce harmonics uniformly and
equally in bands 5 and 6 but at higher frequency bands they show larger number of
Fig. 9.41 Most pominent frequency (rad/s) at different sub-bands
Fig. 9.42 Number of harmonics at different sub-bands
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
207

harmonics. For the 1st, 2nd and 3rd tabla, harmonics are present within band 3 to
band 6. 4th and 5th tabla produces very less number of harmonics for the free
strokes and they are found within band 4 and band 6 but the damped strokes
produces harmonics within band 3 and band 6. So it is observed that tablas with
smaller diameter produce more harmonics than those of larger diameter.
Again Table 9.8 shows the standard deviation and average frequency of har-
monics. It is observed that at low frequency range both the standard deviation and
average frequency remains same but at higher frequency bands these values get
scattered. This conﬁrms that the randomness of energy of the partial increases at
higher frequency bands. Such thing occurs due to differences in structure of the
tabla as well as the intensity of strokes made by the player.
Timbre Analysis of Tabla Strokes
Among deterministic approach, timbre is an important feature for analyzing tabla
and its stroke characteristics. All the sound samples are normalized to 0 dB and of
same length. Among various timbre features we have measured some harmonic and
perceptual features. These are irregularity, tristimulus, odd and even parameters,
spectral centroid and brightness.
From Table 9.9 it is observed that tristimulus 2 (T2) and tristimulus 3 (T3) are
highly correlated while tristimulus 1 (T1) is weakly correlated with T2 and T3. This
concludes the fact that fundamental (corresponds to T1) of tabla stroke is weak
compared to its harmonics. Also mid and higher frequency partials behave simi-
larly. Odd and even harmonics are equally proportionate and are highly correlated
and so tabla strokes are harmonically good to hear. T2 and T3 both have high
correlation with irregularity and spectral centroid. This concludes the fact that high
frequency partials have higher order of irregularity among partials. Also brightness
(i.e. centre of gravity of amplitude) and spectral centroid (i.e. centre of gravity of
frequency) are highly correlated. It is also observed that stroke ‘ta’ and ‘tu’ have
low brightness hence energy for all tablas. Since both strokes execute by striking
index ﬁnger at the edge and such process of stroke cause weak resonance in the
cavity of tabla. Comparing all strokes, it is observed that irregularity among partials
are higher for tabla 3 and 4 compared to other tablas.
Stroke ‘Ge’ of tabla 1 is different from others viz. brightness and centroid both
are low and stronger 2nd, 3rd and 4th harmonics and low irregularity. Other tablas
show uniformity in timbre for the stroke ‘Ge’. Strokes ‘Ghe’, ‘tu’, ‘teen’ and ‘ta’ of
tabla 3 and 4 are different from others viz. brightness and centroid are too low and
stronger 2nd, 3rd and 4th harmonics and higher irregularity, while other three tablas
show uniformity and all these four strokes are free stroke. For stroke ‘thun’, tabla 3
and 4 differ from other tablas only in brightness i.e. its CG of amplitude. Stroke ‘ti’
of tabla 3 is different from others viz. brightness and centroid both are low, stronger
2nd, 3rd and 4th harmonics and higher irregularity. For strokes ‘re’ and ‘te’, tabla 3
and 4 differs from other tablas only in irregularity, in which both the strokes are
made at the centre circle and both are damped strokes. Damped strokes have higher
208
9
Automatic Musical Instrument Recognition

Table 9.8 Standard deviation and mean of tabla strokes at different sub-bands
110–220
220–440
440–880
880–1760
1760–3520
Stdev
Mean
Stdev
Mean
Stdev
Mean
Stdev
Mean
Stdev
Mean
Tabla 1
24.341
178.732
36.927
302.998
124.346
640.864
195.905
1181.595
242.300
2170.102
Tabla 2
31.433
178.149
61.536
315.576
45.133
558.249
236.912
1130.017
245.908
1996.191
Tabla 3
27.200
178.310
23.820
321.220
137.530
565.568
253.916
1307.372
393.874
2228.098
Tabla 4
24.889
306.703
111.122
552.938
165.671
1043.175
261.962
2217.92
Tabla 5
40.109
288.843
157.109
580.582
307.175
1258.977
324.960
1991.757
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
209

Table 9.9 Correlation coefﬁcients of various timbre parameters
Brightness
T1
T2
T3
Odd
Even
Irregularity
Centroid
Brightness
1
−0.1634
−0.6958
0.7314
0.2169
−0.1544
−0.5954
0.8293
T1
1
−0.2661
0.1582
−0.1359
−0.2269
−0.1771
0.0218
T2
1
−0.9939
−0.1634
0.2567
0.8948
−0.8412
T3
1
0.1830
−0.2370
−0.8964
0.8592
Odd
1
−0.9341
−0.4308
0.2121
Even
1
0.4873
−0.2164
Irregularity
1
−0.7318
Centroid
1
210
9
Automatic Musical Instrument Recognition

brightness and spectral centroid than the free strokes. So, one may conclude that
style (nature and intensity) of strokes of player of 1st tabla is different from that of
others. Also style of strokes is similar for the players of tabla 3 and 4.
Spectral Envelope Analysis of Tabla Strokes
By decomposing a time series into time-frequency space, one is able to determine
both the dominant modes of variability and how those modes vary in time. The
spectral envelope, which embodies a wealth of information,
is
really
a
“zoomed-out” view of the power spectrum. What determines the shape of the
envelope is basically the location of dominant peaks on the periodic scale (Torrence
and Gilbert 1997). These dominant peaks are called Most Prominent frequency
(MPF). Most prominent frequency (rad/s) at different sub-bands of all the tabla
strokes are shown in Fig. 9.41 and number of harmonics at different sub-bands are
shown in Fig. 9.42. For the Morlet, which has several smooth oscillations, the
period is a well-deﬁned quantity which measures the approximate Fourier period of
the signal. With the help of Torrence wavelet tool we can locate the individual
locations of high power. We also made contour plot using the Morlet wavelet of the
wavelet power spectrum with white spectrum (with a ﬂat Fourier spectrum) and
colored spectrum (increasing power with decreasing frequency). By this we can
know the time intervals in which certain band of frequencies exist. It involves a
transform from a one-dimensional time series (or frequency spectrum) to a diffuse
two-dimensional time–frequency image with statistical signiﬁcance tests.
We have shown only two wavelet graphs consisting of power spectra out of 45
samples. Table 9.10 shows the power for thick contour of wavelet spectra with 95%
conﬁdence level along with its periodicity and time of occurrences of 45 samples.
Wavelet spectra of two tabla strokes only is shown in Fig. 9.43.
The most prominent power which is the location of dominant peaks in the power
spectrum is not obtained immediately after strokes. This corroborated with the fact
that tabla strokes have weak fundamental but strong mid frequency harmonics.
Only Tabla 1 shows white contour of 95% conﬁdence level while other tabla does
not show any white contour. Strokes of Tabla 1 are different from all others.
Periodicity of change of power shows uniformity for all tablas. Difference in power
and periodicity occur only due to stroke strength. Variance of power with period is
more for free strokes than damped strokes.
9.2.8
Summary
From these studies of timbre characteristics of tanpura, sarod, ﬂute and harmonium
we can summarize that
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
211

Table 9.10 Power in dB2, period in ms and time of occurrence in ms obtained from Torrence
wavelet tool
Stroke
Power (dB2) for thick contour of wavelet
spectrum with 95% conﬁdence level
Period (ms)
Time (ms)
Ge
2.5–23
8–16
8–40
0–2.5
24–32
80–120
Ghe
0.82–17
8–16
40–80
Re
0–3
8–12
20–160
3–32
12–48
80–160
Ta
1.3–25
8–32
100–200
Te
0–2.6
8–16
100–200
0–2.6
32–64
80–160
2.6–28
16–32
45–180
Teen
0.35–7.2
8–32
120–160
Thun
0.15–12
8–32
80–120
Ti
0–1.2
8–16
80–200
1.2–31
16–32
80–160
Tu
0.095–17
8–24
320–480
Ge2
0.0077–7.3
8–24
20–80
Ghe2
0.094–16
8–24
20–80
Re2
0.13–45
8–48
60–160
Ta2
0.028–19
8–24
80–160
Te2
0.12–36
8–48
80–180
Teen2
0.059–22
8–32
40–160
Thun2
0.0058–8.4
8–16
20–160
Ti2
0.084–24
8–32
40–160
Tu2
0.0044–6.1
8–32
60–120
Ge3
0.17–38
8–48
20–160
Ghe3
0.038–29
8–24
20–200
Re3
0.048–18
8–48
40–160
Ta3
0.042–31
8–32
20–160
Te3
0.026–30
8–32
20–160
Teen3
0.0092–14
8–24
40–160
Thun3
0.097–29
8–32
40–160
Ti3
0.092–46
8–32
20–160
Tu3
0.053–30
8–24
20–160
Ge4
0.051–23
8–16
8–60
Ghe4
0.038–29
8–24
40–160
Re4
0.093–29
8–16
20–160
Ta4
0.021–12
8–16
80–120
Te4
0.031–19
8–24
80–160
Teen4
0.0058–7.1
8–16
40–120
(continued)
212
9
Automatic Musical Instrument Recognition

• Timbral characteristics such as tristimulus (T1, T2 and T3) and the odd and even
parameters have been chosen in view of the energy distribution in partials,
whereas spectral brightness, irregularity and inharmonicity are descriptive of the
harmonic content.
• Compared to the other strings, Kharaj (lower octave Sa), the ﬁrst thick string of
tanpura has a poor timbre quality.
• The combined signal of tanpura, made up of all four strings superposed, proﬁts
from contributions of the 3rd and 4th string, where the energy is slightly shifted
to the even partials, so that the overall timbre quality of the drone is better than
the 1st string. The drone effect is due to contribution of kharaj string.
• The uniqueness of tanpura sound lies in the multiple decay of the total energy as
well as the periodic ﬂuctuations of the harmonics. The waxing and waning
stems from multiple decay of the higher harmonics is a specialty of tanpura
sound. This property is precisely responsible for the jwari in Tanpura. The
impulses arising from the grazing touch of the strings with the bridge are
constantly pumping energy to the higher harmonics, leading to a resonant
structure that is very different from classical string instruments.
• Sarod signal has a very short attack but a long decay and exhibit a quasi-random
waxing and waning.
• Amplitude ﬂuctuation is a major criterion of the supporting strings of sarod
sound signals. But the melodic strings show the reverse characteristics. It has
been observed that the sound spectrum of each string of sarod has very irreg-
ularly occurring bands of spectral peaks which sustain for long periods. This is
due to smaller resonating structure and the resonating chamber along with the
bridge fails to supply energy to the higher harmonics.
Table 9.10 (continued)
Stroke
Power (dB2) for thick contour of wavelet
spectrum with 95% conﬁdence level
Period (ms)
Time (ms)
Thun4
0.0073–7.8
8–16
40–80
Ti4
0.31–43
8–24
20–160
Tu4
0.0074–6.7
8–24
40–100
Ge5
0.12–22
8–24
20–100
Ghe5
0.0075–5.2
8–16
40–60
Re5
0.037–25
8–16
20–160
Ta5
0.0096–11
8–24
100–120
Te5
0.013–14
8–24
120–160
Teen5
1.3–86
8–24
0–40
Thun5
3.6–70
8–16
0–40
0–3.6
8–16
40–60
Ti5
0.073–20
8–32
40–160
Tu5
1.4–39
8–16
8–40
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
213

Fig. 9.43 Wavelet spectra of two tabla strokes ge and re
214
9
Automatic Musical Instrument Recognition

• Timbre variation is observed for main strings only and so these strings are used
for playing melody while chikari and sympathetic strings are used as supporting
strings due to their lack in timbre properties.
• Timbre of ﬂute depends upon its length to diameter ratio. The resonating tube
helps the ﬂute sound to pick up energy immediately and distribute to higher
harmonics and so ﬂute sound is more harmonic.
• All the notes of ﬂute have almost similar timbre characteristics.
• Harmonium sound is highly inharmonic and its timbre property varies with the
pitch of the reed. Sound of mid frequency reeds have uniformity in timbre
characteristics but the randomness in timbre increases with the increasing pitch.
Such timbral change of harmonium sound with its reed helps the Indian
musicians to use it as an accompaniment especially for the singers.
Flute sounds are more harmonic compared to that of harmonium sounds.
• With the help of wavelet tools we can precisely distinguish individual strokes
which may lead to
(i)
Categorization of tabla viz. (a) tabla 1 differs from other four tablas;
(b) tablas with smaller diameter produce more harmonics than tablas of
larger diameter; (c) stroke at the edge (3rd circle) produces weak reso-
nance in the cavity of tabla and hence produce low energy sound.
(ii)
Categorization of tabla strokes viz. (a) damped strokes are more powerful
(energetic), higher irregularity in its harmonics and also possess larger
number of harmonics than free strokes; (b) tabla strokes have weak
fundamental. (c) Attack peak reaches faster in free strokes than the
damped strokes. (d) Time to reach MPF (Most Prominent Frequency) is
less for damped strokes compared to free strokes.
(iii)
Style of tabla player viz. (a) players of 1st and 2nd tablas made the strokes
consistently; (b) style of strokes of player of 1st tabla is different than
others. (c) also style of strokes is similar for the players of tabla 3 and 4.
9.3
Identiﬁcation of Indian Musical Instruments
9.3.1
Introduction
Rapid growth of digital media increases the need for an effective data management
with modern music technology. Automatic recognition of musical instruments from
music audio signals is a very important area of the study in music technology.
Human brain (intelligence) irrespective of whether trained or untrained in such
identiﬁcation can very easily identify musical instruments simply by hearing their
sound signals. But it is really a difﬁcult artiﬁcial intelligence problem. During the
last two decades’ various processes to tackle the problem from various perspectives
were attempted, but they are not as competent as the human auditory system. Since
9.2
Acoustical Analysis for the Sound of Indian Musical Instruments
215

very little research has been done so far on identiﬁcation of Indian musical
instruments especially on North Indian ones, there exists ample prospect of research
in this area. Foundation of research in this area relies on studies in psychoacoustics
and acoustics of musical instrument. Human perception in musical applications is
especially important, since musical sounds are designed primarily for human
audition. Finally, the auditory system is very successful in identifying musical
instrument by hearing; thus it operates as a benchmark for sound understanding
systems. If we could imitate the performance of human audition, we would do
extremely well (Eronen 2001b).
Researches so far done in deﬁning the task of musical instrument recognition,
particularly for western musical instruments, have revealed several levels of difﬁ-
culties. Most of the work reported is on monophonic recognition which refers to the
recognition of solo music or solo notes. Polyphonic recognition has received much
fewer attempts. It is not even clear how the problem should be approached. One
way would be to separate the sounds of individual instruments from the mixture and
then classify them individually using algorithms developed for monophonic
recognition. The polyphonic musical instrument recognition problem would cul-
minate into reliable sound separation, and the main task of the recognizer block
would be to cope with possibly corrupted separated sounds. However, the sepa-
ration approach has received some criticism, too. It has been argued than humans do
not separate a single musical instrument from a mixture but more or less consider a
mixture of musical sounds as a whole (Scheirer 2000). Since there exist algorithms
for polyphonic pitch estimation (Klapuri 2001), separation of concurrent harmonic
sounds (Virtanen 2001), and recognition of musical instruments from the separated
tones (Eronen 2001b), it is natural to try the separation approach. This motivated us
to apply similar approach in identifying Indian musical instruments from poly-
phonic sounds.
There are both scientiﬁc and practical reasons for building computer systems that
can recognize and identify the instruments in music. More than a century after
Helmholtz’s groundbreaking research, arguments still abound over the deﬁnition of
musical “timbre,” and over the relative perceptual importance of various acoustic
features of musical instrument sounds. This chapter is simultaneously a scientiﬁc
attempt to understand timbre by quantifying the relevance of various acoustic cues
for instrument identiﬁcation. Also this literature on the perception of musical
instrument sound suggests many potentially salient acoustic features of musical
sound.
9.3.2
Sound Source Recognition by Human Brain
Human being can recognize many events and objects based on the produced sound
alone. “Recognition means that what is currently being heard corresponds in some
way to something that has already been heard in the past”. However, little is known
216
9
Automatic Musical Instrument Recognition

about how the human sound source recognition actually works. The basic problem
in sound source recognition is contextual variation (Martin 1999) i.e. the position of
a source with respect to a listener, and the acoustic characteristics of the environ-
ment affect the sound waves. The listener must use information that is characteristic
to a source and remains constant from one time to another. We call these infor-
mation acoustic invariants (Martin 1999). Sound producing objects have acoustic
properties, which include e.g. the type of excitation, the physical construction, the
materials, and the shape and size of the resonance structures. The type of excitation
varies from instrument to another, and has signiﬁcant inﬂuence on the sound. The
resonance structures affect the spectrum of the resulting sound, the temporal
development of spectral partials, and so on. By using features that are affected by
the invariants, it is possible to move backwards to the invariants themselves, and to
the identity of the sound source (Martin 1999). However, the situation is compli-
cated by a few things. The acoustic properties evolve over time, typically quite
slowly and continuously. “The auditory world is transparent and linear; the sound
waves from different sources add together and form larger sound sources, for
example, the sound of an orchestra is a mixture of the sounds of all the instruments”
(Handel 1995). Recognition requires learned experience. An auditory percept is
evoked by acoustic waves, which are the result of the physical processes of the
source. We humans tend to hear the process that has generated the sound, or “see
through the sound” into the sound generating mechanism. But for the sake of
coping with environmental variation and changes in the production processes, we
need to learn the connection between different acoustic properties and their sources.
We learn, for instance, how different environments affect some sound. Then, the
ﬁnal recognition is obtained by matching the information in the sound heard with
some representation in the long term memory, i.e. a lexicon of sound forms
(McAdams 1993). Cochlea, which is a shell-shaped organ in the inner ear, performs
initial frequency analysis and dynamic compression. Acoustic vibration is trans-
mitted to a membrane inside the Cochlea, namely basilar membrane, of which
different frequencies of the input signal set different parts into motion. From the
basilar membrane, the movement at different points is transduced into neural
impulses that are sent through the auditory nerve to the brain. In the auditory
grouping phase, the stream of input information is then processed into separate
auditory representations, one for each sound source in the environment (Bregman
1990). This means that the components constituting the sound of each source are
segregated from the input information (which describes the whole sound mixture),
and the components belonging to ascertain sound source are integrated into a
group. It is supposed that in this stage, the brain progressively analyzes the per-
ceptual features relevant to listening at a given moment.
By this point, the initial auditory representation has been changed into a group of
abstract properties, characterizing the acoustic invariants of each source, such as the
spectral, temporal or onset characteristics. In the phase of matching with auditory
lexicon, the input representation is matched to classes of similar sound sources and
events in memory, and the stimulus is recognized as the class giving the best match.
9.3
Identiﬁcation of Indian Musical Instruments
217

9.3.3
Constraints
There exists an enormous variety of North Indian musical instruments. The major
constraint in musical instrument identiﬁcation of these instruments is that the timbre
characteristics have not been extensively studied resulting lack of insight into the
information that makes recognition possible.
9.3.4
Important Features for Musical Instrument
Recognition Systems
Eronen (2001a) used feature vectors that included cepstral coefﬁcients and features
relating to the type of excitation, brightness, modulations, asynchrony and funda-
mental frequency of tones, an accuracy of 35% was obtained on a database
including several examples of 29 western instruments. The recognition of the
family of the instrument between six possible classes was successful in 77% of the
cases. A central concept in the study of musical acoustics is the quality of sound,
i.e. what something sounds like. A musical sound is said to have four perceptual
attributes: pitch, loudness, duration and timbre. These four attributes make it pos-
sible for a listener to distinguish musical sounds from each other. Pitch, loudness
and duration are better understood than timbre and they have clear physical
counterparts. For musical sounds, pitch is well deﬁned and is almost equal to the
fundamental frequency. The physical counterpart of loudness is intensity, which is
proportional to the square of the amplitude of the acoustic pressure. The third
dimension, perceived duration, corresponds quite closely to the physical duration
with tones that are not very short. Timbre is the least understood among the four
attributes. According to ANSI73, traditionally, timbre is deﬁned by exclusion: the
quality of a sound by which a listener can tell that two sounds of the same loudness
and
pitch
are
dissimilar.
We
are
fortunate
in
the
sense
that
many
psycho-acousticians have explored the underlying acoustic properties that cause
different sound quality, or timbre sensations.
Western Musical instrument recognition and sound source recognition in general
are essential parts of computational auditory scene analysis (CASA). In this ﬁeld,
the goal is to analyze complex acoustic environments, including the recognition of
overlapping sound events, and thus their sources. In musical synthesis, the model
parameters are often analyzed from an acoustic signal. There might be potential in
combining these two ﬁelds, using physical model synthesis parameters for musical
instrument recognition and bringing new methods for feature extraction from
musical instrument recognition to physical modeling.
A recent multimedia description standard MPEG-7, developed by the Moving
Pictures Expert Group, has two different objectives relating to instrument recog-
nition (Herrera 1999; Peeters et al. 2000). The ﬁrst, music segmentation according
to the played instrument, requires an operating instrument recognition system. The
218
9
Automatic Musical Instrument Recognition

second, segmentation according to perceptual features, means that no universal
labels are assigned to the segments, but the segmentation is accomplished using
some distance metrics, such as distances between feature values measuring per-
ceptually relevant information calculated from the sample. In multimedia applica-
tions, some higher level information is likely to be available, such as structural and
semantic information, temporal data, notes, chords or scales.
9.3.5
Temporal Envelope Estimation
Amplitude envelope can be deﬁned as the variation of the amplitude of a musical
sound. This is often described through four phases called attack, decay, sustain and
release (ADSR). The basic idea to extract the temporal envelope starts with a
low-pass ﬁltering, in order to extract the slow time changing components. This
concept comes from the time representation of energy envelops. This feature is not
always easily achievable because of overlapping notes with noise or with other
notes. This is best applicable for monophonic single note sound signal. Different
instruments show different behavior of ADSR (Martin 1999). Some instrument
shows very low attack but long decay region while some shows long attack but low
decay region. Such behavior of ADSR is the characteristic identiﬁcation of a par-
ticular instrument viz. according to (Sengupta et al. 2007) tanpura sounds have
short but highly energetic attack but a long decay.
9.3.6
Timbre Analysis
A most important feature for instrument identiﬁcation is timbre space (i.e. to cluster
musical sounds into some space having perceptually relevant dimensions) with
techniques attempting to model the human auditory system. These can be consid-
ered as relating to instrument recognition. Acoustic features that may be useful for
instrument identiﬁcation are divided into two categories: spectral and temporal. The
spectral characteristics of musical tones were ﬁrst examined more than a century
ago and have remained the foundation of the conventional-wisdom understanding
of musical sound. More recent research, however, has shown that the temporal
properties of musical-instrument sounds are at least as important as spectral
properties for sound-source recognition.
In the ﬁrst extensive scientiﬁc investigation of musical-tone properties,
Helmholtz in 1954 showed that the relative amplitudes of the harmonic partials that
compose a periodic tone—much more so than their relative phases—are the pri-
mary determinants of the tone’s sound quality. Subsequently, researchers have
identiﬁed several important sub-features of the harmonic magnitude spectrum. The
spectral centroid which is the “balancing point” of the spectrum—correlates
strongly with “brightness,” a primary subjective dimension of timbre (Martin 1998),
9.3
Identiﬁcation of Indian Musical Instruments
219

and for many instruments it varies characteristically with sound intensity. Also, for
example, bowed string tones are inharmonic during both their attack and decay
(Martin 1998). The relative rates of energy buildup in the harmonic partials are a
salient feature of some instrument tones.
9.3.7
A Practical Study
Let us start with a study which is primarily concerned with single-reed woodwind
instruments like clarinet, saxophone and harmonium, wind instrument like ﬂute and
string instruments like tanpura and sarod. Clarinet and saxophones are used in India
as a western instrument without any modiﬁcations in it. Although harmonium is a
European invention, it has evolved into a truly bi-cultural instrument. We are
generally limited to Indian bamboo made ﬂute although ﬂute has wider variations.
Tanpura and sarod are two typical Indian string musical instruments. Sample sig-
nals each of 30 s duration was collected from each instrument. All the sound signals
were from a single instrument of speciﬁc type; no interference of other instruments
was there. Each of the signals was segregated into small window frame of 1 s each
resulting into 30 such frames for each signal. Each of this spectrogram was studied.
The perceptual salience of several outstanding features of quasi-harmonic,
time-variant spectra was investigated in musical instrument sounds. Spectral anal-
ysis of sounds from six musical instruments like clarinet, saxophone, harmonium,
ﬂute, sarod and tanpura produced time-varying harmonic amplitude and frequency
data. In this part of the study we have analyzed the spectral features and evaluated
pitch, formants/resonant frequencies and timbre parameters from the dataset of
sound samples from these six different musical instruments to compare the sound
quality of those instruments. These parameters properly utilized might act as an
approach for musical instrument identiﬁcation.
9.3.7.1
Feature Extraction from Spectral Envelope
Once the pitch has been determined, the height of each partial may be measured as a
function of frequency. The spectral centroid is simply the centre of gravity of the
spectral envelope. Deviations from harmonicity, called inharmonicity in the signal
will be reﬂected as deviations from one partial to the other. In this study, we
extracted various features from each instrument tone, including the spectral cen-
troid, onset asynchrony (both the relative onset times at various frequencies, and
their overall variation) and ratio of odd-to-even harmonic energy.
The sum of the energy in the spectral envelope approximates the instantaneous
loudness of the signal. Tracking this over time leads to simple measures of
amplitude modulation, which can reveal shimmer—and, by correlation with fre-
quency modulation, resonances. As suggested by Beauchamp and Foerster in 1969,
220
9
Automatic Musical Instrument Recognition

the relationship between intensity and the spectral centroid may be an important
perceptual correlate of timbre.
Here we explore the issues about the relationship between acoustic features and
sound properties, justifying their choice in terms of musical relevance, brightness,
inharmonicity, and so forth. Representation of a musical instrument involves the
estimation of the physical parameters that contribute to the perception of pitch,
intensity levels and timbres of all sounds the instrument is capable of producing.
Here the timbre parameters are being calculated from the harmonic spectrum of
sound signals of six instruments. Amplitude and frequency time varying curves of
partials were measured based on Fast Fourier Transformation. Each of the spectral
envelopes was studied to evaluate the attack, decay and steady state timings and the
formants/resonant frequencies too. These features are being compared and also the
sound qualities of those instruments have been compared. We choose to describe
the experimental results into four types of features, which are decided based on
acoustic knowledge of the instruments like the change in energy from frame to
frame, formants or resonant characteristics i.e. frequency and bandwidth of for-
mants, change in sound qualities due to variation of timbre and the spectral analysis.
9.3.7.2
Energy Change
Different instrument behaves differently over time. Some instruments attack quickly
and then decays exponentially. Other instruments attack slowly then decays
quickly. Every instrument has a unique amplitude envelope over time (known as
ADSR). This feature set indicates the time frame of the energy change, whether
there has been a sudden attack, slow decay, sudden decay or slow decay (or some
combination) (Sell et al. 2006). The amplitude envelope after being normalized for
all the sound samples is simply the intensity measured as a function of time. It
carries information about the source excitation and its coupling to the resonant
body. All these six instruments have faster rise time than others (showed in
Table 9.11), indicating tight coupling between source and resonant structure.
Table 9.11 Comparative study of Attack, Steady and Decay time for six instruments
Name of the
instrument
Attack period
Steady period
Decay period
Harmonium
Very short (0–
0.15 s)
Long (0.15–1.3 s)
Long (1.3–3.2 s)
Clarinet
Short (0–0.3 s)
Long (0.3–3.6 s)
Short (3.6–4.2 s)
Flute
Short (0–0.15 s)
Long (0.15–1.08 s)
Short (1.08–1.2 s)
Saxophone
Very short (0–
0.02 s)
Long (0.02–0.8 s)
Short (0.8–0.9 s)
Tanpura
Short (0–0.3 s)
Short (0.3–1.0 s)
Very long (1–7 s)
Sarod
Very short (0–0.05)
Very short (0.05–
0.2 s)
Very long (0.2–
3.4 s)
9.3
Identiﬁcation of Indian Musical Instruments
221

Intensity level for tanpura, saxophone and clarinet are also high while low for ﬂute,
harmonium and sarod.
9.3.7.3
Resonant Frequency
Frequency range and frequency bandwidth are often indicative of an instrument.
Characteristic frequency peaks on the spectral envelope are known as formants (Sell
et al. 2006), are also often present in the spectrum of a certain instrument. From
Table 9.12 it has been observed that the formants/resonant frequency for the
woodwind instruments are stable with time while that for string instruments con-
sidered here are unstable with time. Widths of the ﬁrst characteristic resonance
frequency bands do not differ much among tanpura, clarinet, saxophone and ﬂute.
This is due to strong coupling between string and resonant body. But no similarity
is observed for sarod and harmonium because of the loose coupling between the
vibrating source and the resonant body. Distance between 1st and 2nd formants of
sarod and harmonium is very less compared to the other four instruments.
9.3.7.4
Timbre
The average value of the timbre parameters for the entire signals are shown in
Figs. 9.44, 9.45, 9.46, 9.47, 9.48, 9.49, 9.50 and 9.51 and standard deviation of
these parameters are shown in Figs. 9.52, 9.53, 9.54, 9.55, 9.56, 9.57, 9.58 and
9.59. From the Fig. 9.44 it is observed that the brightness graph shows a low value
for ﬂute while sarod and clarinet shows low but similar brightness. Brightness for
saxophone tones are non-linear but its values are mostly similar with harmonium
and tanpura. So the energy of the partials varies largely for different tones of
saxophone and harmonium. It has been observed that a minimum deviation of
energy among partials from the mean energy are observed for ﬂute and tanpura. So
these two instruments are more harmonic compared to other three. From tristimus
graphs it is observed that the energy of the higher frequency partials are least for
ﬂute but sarod, tanpura, saxophone and harmonium has almost similar energy
Table 9.12 Formant frequencies/characteristic resonant frequencies of six instruments
Name of the instrument
Formant/resonance frequency ranges in Hz
F1
F2
F3
F4
Clarinet
426
1575
2588
3263
Flute
450
4688
2363
3713
Sarod
788
1350
3488
4275
Saxophone
450
1688
2306
3094
Tanpura
506
1856
3263
3994
Harmonium
1153
1800
2981
4962
222
9
Automatic Musical Instrument Recognition

Fig. 9.44 Brightness of the six instruments
Fig. 9.45 Tristimulus 1 of the six instruments
Fig. 9.46 Tristimulus 2 of the six instruments
Fig. 9.47 Tristimulus 3 of the six instruments
9.3
Identiﬁcation of Indian Musical Instruments
223

Fig. 9.48 Odd parameters of the six instruments
Fig. 9.49 Even parameters of the six instruments
Fig. 9.50 Irregularity of the six instruments
Fig. 9.51 Centroid of the six instruments
224
9
Automatic Musical Instrument Recognition

Fig. 9.52 Standard deviation
of brightness of the six
instruments
Fig. 9.53 Standard deviation
of tristimulus 1 of the six
instruments
Fig. 9.54 Standard deviation
of tristimulus 2 of the six
instruments
Fig. 9.55 Standard deviation
of tristimulus 3 of the six
instruments
9.3
Identiﬁcation of Indian Musical Instruments
225

Fig. 9.56 Standard deviation
of odd parameter of the six
instruments
Fig. 9.57 Standard deviation
of even Parameter of the six
instruments
Fig. 9.58 Standard deviation
of irregularity of the six
instruments
Fig. 9.59 Standard deviation
of centroid of the six
instruments
226
9
Automatic Musical Instrument Recognition

distribution in their higher frequency partials. There is a wider gap between fun-
damental and its next three harmonics for all the instruments.
Deviation of fundamentals among different tones of sarod and tanpura are the
least while it is maximum for clarinet, ﬂute and harmonium. Clarinet shows a
non-linearity in energy distribution at all its harmonics while tanpura shows max-
imum linearity in energy distribution among its harmonics. Saxophone also shows
very less dispersion in energy at higher harmonics. Deviation of energy distribution
is moderate for ﬂute and sarod at higher harmonics. All the instruments show an
uniformity in distribution of energy among odd-even harmonic partials except for
clarinet. Clarinet shows a lower of odd parameter. Irregularity among partials are
high for ﬂute and clarinet but less for all other instruments. With increasing
inharmonicity we can arrange the instruments like tanpura, ﬂute, harmonium,
clarinet, saxophone and sarod. So we ﬁnd that clarinet have low complexity and
poor timbre quality while sarod and ﬂute has a rich timbral quality. Although most
musical tones consist of harmonically related partials, deviations from harmonicity
are responsible for the “warmth” of a musical tone and the “bite” of saxophone and
harmonium tones.
It has been found that clarinet exhibit in general much more richness of timbre
variation and rich spectral centroid and hence a rich perceived brightness. Sarod and
clarinet have higher but almost similar values of centroid while the rest have lower
but almost similar values for centroid. Again saxophone, ﬂute and tanpura sounds
are more harmonic in nature. Maximum inharmonicity is observed in case of
harmonium, sarod and clarinet sounds. Since tanpura is more harmonic, tanpura
sound appears as a calm wave.
9.3.7.5
Spectral Analysis
The whole signal is normalized and the following spectral features are considered
for the study. The difference in amplitude (expressed in dB) between two largest
peaks in long term average spectra (LTAS) for all the sound signals was calculated.
The frequency values (expressed in Hz) for each of these peaks are noted. These
two acoustic features are also used to differentiate between the qualities of sound
produced by the musical instruments (Sengupta et al. 2004). The average of the
difference between the amplitude of two largest peaks is as given in Table 9.13.
Closer examination reveals that it is the frequencies of the ﬁrst peaks, which
reveal some distinction. With respect to both the amplitude and frequency values,
the largest peaks of sound signal of saxophone are closely apart while that of ﬂute is
widely apart. Interestingly the ﬁrst peak of all the sound signals of tanpura and
sarod shows almost constant frequency value but for the woodwind instruments it
varies from signal to signal. Again in case of tanpura the differences between the
frequencies are high but the differences between the amplitudes are low
(Table 9.13).
9.3
Identiﬁcation of Indian Musical Instruments
227

9.3.8
Summary
Based on Euclidean distance measurement (distance between two coordinates (a, b)
and (c, d) is distance ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c  a
ð
Þ2 þ d  b
ð
Þ2
h
i
r
) we can make an assessment of
timbre relation among six instruments. Harmonium, clarinet and saxophone are reed
instruments produce melodious and unique sound. The uniqueness lies in the
timbral complexity as well as the energy characteristics of the individual partials. It
has been observed that the sound spectrum of each reed of harmonium has very
regularly occurring bands of spectral peaks which sustain for long periods while
that for clarinet and saxophone sustain for short period. Conventional steady state
sound produced from woodwind reed instrument are quasi periodic, which means
that their spectral components are related to the fundamental frequency by integer
multiples. The non-linear action of the reed promotes the distribution of oscillatory
energy among the higher harmonics of the fundamental mode of vibration.
So we conclude:
1. Features like energy change due to attack, decay etc. give information about the
classiﬁcation of instruments. These features carry information about the source
excitation and its coupling to the resonant body resulting a sustained sound after
a short attack for woodwind instruments while stroke string instruments have a
long decay after a very short attack.
2. Features like formants play an important role. Table 9.12 reveals that the string
instruments have higher ﬁrst, third and fourth formants than the wind
instruments.
3. The variation of timbre parameters helps in classifying musical instruments.
4. The standard deviation is a measure of how widely values are dispersed from the
average values. Inharmonicity plays an important role. Inharmonicity values are
more widely dispersed for woodwind instruments.
5. Most of all timbre parameters are widely dispersed for clarinet while closely
dispersed for tanpura. Since the sound samples are normalized and the corre-
lation between intensities belonging to each sound can be used to identify them
Table 9.13 Average distribution of frequency and amplitude of two largest peaks
Average
frequency
of 1st
largest
peak in Hz
Average
frequency
of 2nd
largest
peak in Hz
Difference in
frequency
(Hz) between
two largest
peak
Average
amplitude
of 1st
largest
peak in dB
Average
amplitude
of 2nd
largest
peak in dB
Difference in
Amplitude
(dB) between
two largest
peak
Clarinet
299.8
880.2
580.4
−19.57
−25.68
6.11
Flute
401.6
1098.6
697
−18.96
−31.9
12.94
Sarod
275.25
739
463.75
−16.825
−22.6
5.775
Saxophone
305.2
476.3
171.1
−19.74
−21.37
1.63
Tanpura
419
932
513
−19.74
−27.95
2.4
Harmonium
195
225
30
−22.5
−25
2.5
228
9
Automatic Musical Instrument Recognition

from the cluster. Such an approach of formation of clusters for timbre param-
eters for different instruments can be used to build an artiﬁcial system for sound
source recognition.
6. Spectral features involving frequency of two strongest harmonics (spectral
peaks) and their difference in amplitude may be useful for comparing sound of
musical instruments.
7. Timbre parameters showed a strong correlation among themselves for all the
sound signals.
8. Figure 9.60 shows the position of the instruments based on the Euclidean dis-
tance measured for brightness, irregularity and centroid. These timbre parame-
ters are important cue for musical instrument identiﬁcation. A close look at the
ﬁgure reveals that harmonium, tanpura and saxophone can be identiﬁed as a
single category, since all these three timbre parameters behave similarly for
these three instruments. Sound of these three instruments are more harmonic
than the others. Clarinet, ﬂute and sarod are better identiﬁed by their inhar-
monicity behavior. Probably this may be the reason that tanpura, saxophone and
harmonium are popularly used as accompanying instrument while the other
three are good for solo.
9. Figure 9.61 also helps in identifying these musical instruments based on ADSR
and formant structures. Flute, clarinet and saxophone have low 1st and 3rd
formant but very high 2nd formant frequencies and also have a longer steady
and decay periods.
The result of this study applied to wider dynamic timbre variation will facilitate
the investigation of the use of timbre differentiation to convey musical
expressiveness.
Fig. 9.60 Euclidean distance measured for different timbre parameters
9.3
Identiﬁcation of Indian Musical Instruments
229

9.3.9
Applications
Automatic sound source recognition plays an important role in developing auto-
matic indexing and database retrieval applications. These applications have
potential in saving the humans from time taking searches through huge amounts of
digital audio material available today. For instance, it would be most useful if we
could ﬁnd sound samples that “sound similar” to a given sound example. Music
content analysis in general has many practical applications, including e.g. structured
coding, automatic musical signal annotation, and musicians’ tools. Automatic
musical instrument recognition is a crucial subtask in solving these difﬁcult prob-
lems, and may also provide useful information in other sound source recognition
areas, such as speaker recognition. However, musical signal analysis has not been
able to attain as much commercial interest as, for instance, speaker and speech
recognition. This is because the topics around speech processing are more readily
commercially applicable, although both areas are considered as being highly
complicated. Through constructing computer systems that “listen”, we may also
gain some new insights into human perception.
References
Banerjee K (2004–2006) A Historical study of string musical instrument makers (since 19th
century). Report of research project under department of culture, Government of India
Banerjee K (2010) The contribution of Ustad Allauddin Khan & his descendants in the
development & internationalization of North Indian Classical Music (in Bengali). Ph.D. thesis,
Rabindra Bharati University, 2010
Fig. 9.61 Positions of instruments based on ADSR and formant structures
230
9
Automatic Musical Instrument Recognition

Banerjee BM, Nag D (1991) The acoustical character of sounds from Indian twin drums. Acustica
75:206–208
Beauchamp J, von Foerster HF (1969) A computer system for time-variant harmonic analysis and
synthesis of musical tones. In: Music by Computers (eds). Wiley, New York, pp 19–62
Braeunig M, Sengupta R, Patranabis A (2012) On Tanpura drone and brain electrical correlates.
Lecture Notes in Computer Science, vol 7172. Springer, Berlin, pp 53–65
Bregman A (1990) Auditory scene analysis: the perceptual organization of sound. MIT Press,
Cambridge
Buchner A (1964) Musical instruments through the ages (trans: Urwin I), 3rd impression 1964.
Spring Books, London
Ciglar M (2009) The temporal character of timbre. Diploma thesis. IEM—Institute of Electronic
Music and Acoustics, University of Music and Performing Arts Graz, Slovenia. www.iem.at
Courtney D (1999) Psychoacoustics of the Musical pitch of Tabla. J Sangeet Res Acad 13(1)
(Calcutta, India)
Datta AK, Chowdhury S (2003) A study of Jitter in continuous speech in Bangla. In: Proceedings
of frontiers of research on speech and music (FRSM), Indian Institute of Technology (IIT),
Kanpur, India, pp 154–159, 15–16 Feb 2003. David Courtney, http://www.chandrakantha.com
Datta AK, Ganguly NR, Mukherjee B et al (1988) Spectrographic study of nasality in sung
vowels. J Acoust Soc India XVI(3&4):265
Eronen A (2001a) Automatic musical instrument recognition. Master of Science thesis, Tampere
University of Technology, Finland
Eronen A (2001b) Comparison of features for musical instrument recognition. In: Proceedings of
the IEEE workshop on applications of signal processing to audio and acoustics
Ghosh RN (1922) Note on musical drums. Phys Rev 20:526–527
Handel S (1995) Timbre perception and auditory object identiﬁcation. In: Moore B (ed) Hearing
Herrera S (1999) A proposal for the description of audio in the context of MPEG-7. In:
Proceedings of the CBMI’99 European workshop on content-based multimedia indexing.
Audiovisual Institute—Pompeu Fabra University, Rambla 31, 08002 Barcelona, Spain. http://
www.iua.upf.es, pherrera@iua.upf.es, xserra@iua.upf.es
Hess W (1983) Pitch Determination of Speech Signals-Algorithm and devices. Springer, ISBN
978-3-642-81926-1
Jensen K (2002) Perceptual and physical aspects of musical sounds. DIKU, University of
Copenhagen, Universitetsparken 1, 2100 København, Denmark
Jensen K, Georgios M (2001) Hybrid perception. 1st Seminar on Auditory Models, Lyngby,
Datalogisk Institute, University of Copenhagen, Universitetsparken 1, Copenhagen, Denmark,
2001
Karnel MP, Scherer RS, Fischer LB (1991) Comparison of acoustic voice perturbation measures
among three independent voice laboratories—a research note. J Speech Hear Res 34:781–790
Klapuri AP (2001) Multipitch estimation and sound separation by the spectral smoothness
principle. In: Proceedings IEEE international conference on acoustics, speech and signal
processing, ICASSP
Martin KD (1998) Towards automatic sound source recognition: identifying musical instruments.
MIT Media Lab Machine Listening Group, Cambridge, MA. NATO Computational Hearing
Advanced Study Institute, Italy
Martin KD (1999) Sound-source recognition: a theory and computational model. Ph.D. thesis, MIT
McAdams S (1993) Recognition of auditory sound sources and events. Thinking in sound: the
cognitive psychology of human audition. Oxford University Press, Oxford
Milekovic P (1987) Lease mean square measures of voice perturbation. J Speech Hear Res 30:529
Mukhopadhyay AK et al (1998) Characterisation of materials for Indian Tanpura. J Acoust Soc
India XXVI(324):372
Park TH (2004) Towards automatic musical instrument timbre recognition. Ph.D. thesis,
Department of Music, Princeton University
References
231

Patranabis A, Sengupta R, Ghosh D, Deb A, Santra AK, Dey N, Datta AK (2008) Study on the
timbral parameters of Sarod signals. In: Proceedings of the international symposium—frontiers
of research on speech and music (FRSM-2008), 20–21 Feb 2008, SCVRCPM, Jadavpur
University, Kolkata, India
Peeters G, McAdams S, Herrera P (2000) Instrument sound description in the context of MPEG-7.
In: Proceedings of the international computer music conference, Berlin
Pollard HF, Jansson EV (1982) A tristimulus method for the speciﬁcation of musical timbre.
Acustica 51:162–171
PolikarRobi. The Wavelet Tutorial. http://users.rowan.edu/*polikar/WAVELETS-/WTtutorial.html
Prajyanananda S (1991) Bharatiya Sangeeter Itihas (2nd part), published by Swami Ashesananda,
Sri Ramkrishna Bedanta Math, 4th edn, Kolkata
Ramakrishna BS (1957) Modes of vibration of the Indian Drum Dugga or left hand Thabala.
J Acoust Soc Am 29:234–238
Raman CV (1934) The Indian musical drums. In: Proc Indian Acad Sci 1A:179–188
Raman CV, Kumar S (1920) Musical drums with harmonic overtones. Nature 104:500
Rao KN (1938) Theory of the Indian musical drums. Proc. Indian Acad Sci 7A:75–84
Rasch RA (1983) Jitter in violin tone. In: Proceedings of the Stockholm music acoustics
conference, Stockholm, 28–Aug 1 July 1983, pp 275
Rees MB (1958) Some variables affecting perceived harshness. J Speech Hear Res 1:115
Sarojini T, Rahman A (1958) Variational method for the vibrations of Indian drums. J Acoust Soc
Am 30:191–196
Scheirer ED (2000) Music-listening systems. Ph.D. dissertation, MIT, April 2000
Sell G, Mysore GJ, Chon SH (2006) Musical instrument detection. Centre for Computer Research
in music and acoustics, Stanford University
Sengupta R et al (1983) Tonal quality of the Indian Tanpura. Sangeet Research Academy, In:
Proceedings of the international symposium—Stockholm music acoustics conference (SMAC).
Speech Transmission Laboratory, Royal Institute of Technology, Sweden, July 1983, pp 333
Sengupta R, Datta AK, Dey N, Nag D (1996a) Generation of musical notations from song using
state-phase for pitch detection algorithm. J Acoust Soc India XXIV
Sengupta R, Datta AK, Dey N, Nag D (1996b) Some studies on Spectral dynamics of Tanpura
Strings with relation to perception of Jwari. J Acoust Soc India XXIV
Sengupta R, Dey N, Nag D (1999) Role of random perturbation of source voice in musical quality
of singing voice. J Acoust Soc India XXVII(1–4):187–190
Sengupta R, Dey N, Nag D, Datta AK (2000) Study on jitter, shimmer and complexity
perturbations in voices of singers and non-singers. J Acoust Soc India XXVIII
Sengupta R, Datta AK, Dey N, Nag D (2001) A comparative study of fractal behavior in
quasi-random and quasi periodic speech wave map. Fractals 9(4):403–414
Sengupta R, Datta AK, Dey N, Nag D (2003) Acoustic cues for the Timbral Goodness of Tanpura.
J Acoust Soc India 31
Sengupta R, Dey N, Nag D, Datta AK, Parui SK (2004) Objective evaluation of Tanpura from the
sound signals using spectral features. J ITC Sangeet Res Acad 18
Sengupta R et al (2007) Study on the acoustic characteristics of Tanpura sound signals. J Acoust
Soc India 34(2&3)77–81
Torrence C, Gilbert CP (1997) A practical guide to wavelet analysis. University of Colorado,
Boulder, Colorado
Valens C (1999–2004) A really friendly guide to wavelets. wavelets@polyvalens.com
Virtanen T, Klapuri A (2001) Separation of harmonic sounds using multipitch analysis and
iterative parameter estimation. In: Proceedings of the IEEE workshop on applications of signal
processing to audio and acoustics
Yumoto E, Gould WJ (1982) Harmonics to noise ratio as an index of the degree of hoarseness.
J Acoust Soc Am 71(6):1544–1550
232
9
Automatic Musical Instrument Recognition

Chapter 10
Vadi-Samvadi Controversy and Statistics
10.1
Introduction
There is a concept of alankar in Indian music, meaning a musical ornament. The
shastras talk about shabdalankars and varnalankars. The varnas include sthayi
(stay on a note), arohi (ascent), awarohi (descent) and sanchari (mixture of ascent
and descent). The rising and falling transitions can be further classiﬁed into convex,
concave and linear. We also have hats and valleys. A hat may be interpreted as an
ascent followed by immediate descent, and a valley as a descent followed by
immediate ascent. By making a count of all the above, we can study the transitory
as well as non-transitory pitch movements between the notes. For more literature on
pitch transition, we refer the reader to Chakraborty et al. (2014) and the link http://
www.itcsra.org/alankar/alankar.html.
Broadly speaking, there are two concepts of pitch stability: psychological and
statistical, the former dealing with the note duration and the latter with the relative
frequency of occurrences of the note. The tonic Sa for example is always a psy-
chologically stable note as it is always a stay note. But in order to be statistically
stable it must maintain its probability of occurrence consistently over the instances
of its realization in the musical piece.
The notes that play the most important and the second most important roles in
expressing a raga are called Vadi and Samvadi swaras (notes) (Chakraborty et al.
2009). In Chakraborty et al. (2011), a strategy for objectively deciding the Vadi
Swara is proposed where it is stated that a note may be classiﬁed as a Vadi Swara, if
it satisﬁes the following conditions.
i. It should exhibit a high relative frequency of occurrence in the entire raga
performance.
ii. In addition to (i), the relative frequency of occurrence of the note should
stabilize in the shortest period.
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1_10
233

The above two conditions implies the Statistical stability of notes in the raga
performance. The ﬁrst condition is necessary for that an important note, whether
Vadi, Smvadi or Anuwadi, cannot afford to have low relative frequency. However
condition (i) for the selection of Vadi Swara cannot be sufﬁcient because a stay note
(Nyas Swara) can also have a high relative frequency without being Vadi or
Samvadi. For example Sudh Re in raga Yaman performance discussed in
(Chakraborty et al. 2011). Again, a raga can have several Nyas Swaras and the
Vadi may itself be a Nyas Swara (e.g. raga Durga of Khambaj Thaat where Vadi:
Sudh Ga; Samvadi: Sudh Ni; Nyas Swaras: Sudh Ni, Sudh Ga, Sudh Ma) or it may
not be (e.g. raga Durga of Bilaval Thaat where Vadi: Sudh Ma; Samvadi: Sa; Nyas
swaras: Sudh Re, Pa and Sudh Dha). Hence, condition (i) can be grossly misleading
without (ii). On the other hand although (ii) is a sufﬁcient condition, it is to be
assessed in addition to the necessary condition (i) and not independently because it
is quite possible that an unimportant note, can be accidentally stable but such a note
will not have a high relative frequency and should be ruled out of the contest
straightaway for being Vadi Swara (Chakraborty et al. 2011). So to identify Vadi
and Samvadi, notes which satisfy (i) are selected and then, among the selected lot,
the note which one satisfy (ii) is taken as Vadi Swara at least by performance.
Now, there are following interesting situations:
1. Vadi Swara of a raga from music theory is known, and the experimental results
conﬁrm it as the raga Pilu performance analyzed in Chakraborty et al. (2011).
2. Vadi Swara of a raga from music theory is known, but the experimental results
contradict it. Although such cases yet to be reported, if it happens, it would not
be correct to throw away the music theory. It is more correct to infer that the
theoretical claim is not reﬂected in the performance. Then analysis of the
recordings of other performers, preferably both vocal and instrumental, would
be taken to check out how many of them contradict the theory. Only then can
some decision be taken.
3. There is a difference of opinion in Vadi-Samvadi selection (Dutta 2006) such as
in Bageshree, Bhairavi, Shankara, Hamir, Kalingra and Rageshree. Using
objective analysis, either the conﬂict will be resolved or both opinions will be
acceptable. Accepting two contrasting opinions in Indian Classical music may
be rational, especially if the opinions are Gharana (school of music) based. But
even then, an opinion that characterizes a speciﬁc Gharana ought to be reﬂected
in the performance of the corresponding artist.
4. The raga is of South Indian Classical origin which has been introduced to the
North Indian Classical music. For such ragas (e.g. Kirwani and Charukeshi),
rules regarding Vadi and Samvadi selection are not clear. This objective analysis
seems to be the only way out. Kirwani has been analyzed in Chakraborty et al.
(2011).
In this chapter, a two-minute vocal recording of raga Rageshree is subjected to a
statistical analysis. As mentioned above, like Bageshree and Shankara, Rageshree
is another controversial raga so far as the choice of Vadi-Samvadi selection is
234
10
Vadi-Samvadi Controversy and Statistics

concerned where there are two different opinions (Dutta 2006). The fundamental
musical features of raga Rageshree are shown in Table 10.1.
10.2
Methodology
Using Solo Explorer 1.0, software for music transcription the fundamental fre-
quencies of the notes are detected by adopting probabilistic approach using
Chebyshev’s inequality as experimented in Mahto et al. (2014). The analysis is
divided into three phases (ﬁrst 60 s, middle 60 s and last 60 s) and distribution of
notes analyzed as discussed in Chakraborty et al. (2013). The notes which satisfy
the condition (i), and (ii) are taken as Vadi and Samvadi swars.
Since the Vadi and Samvadi notes are the stay notes in most of the Hindustani
ragas (Roychaudhary 2005), the next step in this work is to ﬁnd the stay notes
using Krumhansl’s concept (Castellano et al. 1984) psychological stability of notes.
10.2.1
Krumhansl’s Method
This method only detects the stay notes or Nyas Swars by giving ranks to the
notes on the basis of mean duration of the note but not the Vadi and Samvadi
Swars. A stay note will have more mean duration than other (physiological stability
concept) permissible notes in the raga, (Castellano et al. 1984). For detecting Vadi
and Samvadi Swars, statistical stability is a better concept. However, in ragas
where the Vadi and Samvadi are known to be Nyas Swars as in raga Rageshree,
Krumhansl’s technique (Castellano et al. 1984) does help in reducing the search
space. A weighted combination of physiological stability and statistical stability can
be used to decide Vadi Samvadi Swars, given that there are only few ragas in North
Indian Classical Music where Vadi and Samvadi are themselves not Nyas swars
(e.g. raga Durga of Bilaval thaat).
Table 10.1 Fundamental
musical features of raga
Rageshree
Musical feature
raga Rageshree
Thaat
Khambaj
Arohana
S n D M G
Awarohana
{S n D M G} {M R} S
Jati
Aurabh-Sarabh
Vadi Swara
G (some say M)
Samvadi Swara
N (some say S)
Prakriti
Restful
Pakad
{G M R S}, {n D} {N S} G M
Nyas Swaras
G N M S
Time of rendition
2nd phase of night (9 PM–12 PM)
10.1
Introduction
235

10.3
Experimental Results and Discussions
The note sequence detected with their onsets of raga Rageshree performance is
given in Table 10.2.
Count of different notes in whole performance of two-minute and three different
phases (First 60 s, middle 60 s and last 60 s) of the raga Rageshree performances
are tabulated in Table 10.3.
10.3.1
Chi-Square Test
The hypothesis is that the overall distribution (for two minute) of the raga per-
formance is applicable for smaller parts of the piece (First 60 s, middle 60 s, and
last 60 s) as well. The analysis begins with Table 10.3.
From the above table, it is noted that the number of notes detected for the raga
Rageshree is 236. The pooling of notes Sudh Dha, and Komal Ni is required only in
the middle and last phases of the performance.
Remark: Relative frequency of a note fr is deﬁned as follows:
fr = Frequency of the note (F)/Total frequency
The Chi-Square test is performed to test the signiﬁcance of the difference
between observed frequency and expected frequencies for raga Rageshree per-
formance. The results are shown in Tables 10.4, 10.5 and 10.6.
Calculated Chi-Square for ﬁrst 60 s of the raga Rageshree performance is
X
ððO  EÞ2=EÞ ¼ 2:492193
The minimum Chi-Square for rejecting the null hypothesis at 5% level of sig-
niﬁcance is 11.07 as seen from Chi-Square Table 10.7.
Calculated Chi-Square value for 6 degrees of freedom 2.492193 is less than
12.59 and therefore insigniﬁcant at 5% level of signiﬁcance for the ﬁrst 60 s of raga
Rageshree performance.
Calculated Chi-Square (v2) value for middle 60 s of the raga Rageshree per-
formance is
X
ððO  EÞ2=EÞ ¼ 3:72634
After pooling the frequencies of Sudh Dha and Komal Ni in one class, six classes
left for which Chi-Square calculated for 5 degrees of freedom is 3.72634.
The minimum Chi-Square for rejecting the null hypothesis at 5% level of sig-
niﬁcance is 11.07 as seen from Chi-Square Table 10.7. Calculated Chi-Square
236
10
Vadi-Samvadi Controversy and Statistics

Table 10.2 Note sequence with their onsets of raga Rageshree performance
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
1
D
1.53
61
N
26.39
121
S
61.82
181
R
87.85
2
D
1.64
62
S
26.67
122
R
62.05
182
D
91.28
3
n
1.68
63
G
27.05
123
S
62.51
183
N
91.30
4
N
1.72
64
m
27.14
124
S
62.67
184
N
91.40
5
S
1.75
65
G
29.87
125
N
62.73
185
S
91.50
6
S
2.33
66
N
30.76
126
S
62.81
186
S
91.85
7
S
2.42
67
R
30.97
127
R
64.97
187
G
91.94
8
S
4.05
68
G
31.07
128
G
67.04
188
m
92.59
9
S
4.34
69
m
31.12
129
R
67.15
189
R
92.80
10
S
4.49
70
m
33.42
130
R
67.45
190
N
93.01
11
R
4.83
71
G
33.60
131
S
67.52
191
m
93.45
12
R
4.91
72
G
33.67
132
n
67.62
192
N
93.87
13
R
5.04
73
G
33.79
133
S
67.75
193
G
98.11
14
R
5.07
74
G
33.85
134
N
67.78
194
m
98.36
15
R
5.14
75
G
33.95
135
N
68.21
195
m
98.39
16
R
6.59
76
G
34.01
136
N
68.64
196
R
99.80
17
S
8.46
77
G
34.18
137
n
68.76
197
G
100.11
18
N
8.59
78
G
34.30
138
N
68.83
198
m
100.16
19
n
10.04
79
G
34.47
139
S
68.92
199
m
101.96
20
D
10.18
80
G
34.52
140
S
69.20
200
G
102.11
21
S
10.25
81
G
34.66
141
G
69.34
201
G
103
22
S
10.45
82
S
35.49
142
m
71.34
202
G
103.41
23
m
10.62
83
R
35.57
143
G
71.48
203
G
103.71
(continued)
10.3
Experimental Results and Discussions
237

Table 10.2 (continued)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
24
R
10.67
84
S
36.25
144
S
76.28
204
G
103.80
25
S
11.01
85
S
36.36
145
S
76.92
205
R
105.95
26
N
13.54
86
S
36.49
146
S
77.07
206
R
106.65
27
N
14.07
87
S
37.16
147
N
77.17
207
N
106.72
28
N
14.32
88
S
37.39
148
S
77.22
208
S
106.87
29
N
14.76
89
N
40.70
149
N
77.28
209
G
107.03
30
N
15.14
90
N
41.10
150
R
78.20
210
N
107.67
31
N
15.53
91
N
41.97
151
R
78.37
211
N
107.94
32
N
15.70
92
N
41.99
152
R
78.51
212
N
109.30
33
m
16.52
93
S
42.16
153
S
78.94
213
N
109.36
34
G
16.55
94
m
44.56
154
N
79.08
214
m
110.00
35
N
16.84
95
R
44.82
155
N
79.39
215
N
110.14
36
S
16.92
96
D
45.49
156
N
79.58
216
N
110.21
37
S
16.98
97
S
45.61
157
N
79.76
217
S
110.50
38
N
17.04
98
m
45.91
158
N
79.79
218
S
110.62
39
S
17.19
99
N
52.14
159
N
79.91
219
S
110.92
40
R
17.28
100
N
52.15
160
D
80.32
220
N
111.39
41
S
17.42
101
m
52.56
161
R
80.96
221
S
111.47
42
S
17.59
102
D
52.97
162
S
81.10
222
N
111.61
43
S
17.92
103
N
53.03
163
N
81.26
223
R
112.02
44
S
18.93
104
N
53.20
164
S
81.46
224
m
112.91
45
D
20.24
105
N
54.13
165
S
81.77
225
G
115.36
46
D
20.33
106
N
54.28
166
S
81.81
226
R
115.53
(continued)
238
10
Vadi-Samvadi Controversy and Statistics

Table 10.2 (continued)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
Sl. No
Notes
Onset time
(in sec.)
47
N
20.39
107
N
54.31
167
S
81.97
227
N
116.14
48
S
20.89
108
N
54.43
168
S
82.10
228
S
116.22
49
S
21.50
109
D
54.61
169
S
82.20
229
m
116.58
50
N
21.81
110
R
58.43
170
S
82.40
230
m
117.94
51
S
21.91
111
m
58.65
171
S
82.53
231
G
118.03
52
S
22.09
112
G
60.05
172
S
83.38
232
G
118.15
53
R
22.32
113
G
60.21
173
m
84.99
233
G
118.21
54
S
24.88
114
G
60.32
174
G
85.15
234
G
118.79
55
N
25.18
115
G
60.43
175
G
85.80
235
G
118.88
56
N
25.30
116
S
60.94
176
G
86.81
236
G
119.14
57
G
26.17
117
S
60.99
177
R
86.95
58
G
26.19
118
R
61.10
178
N
87.61
59
m
26.21
119
R
61.13
179
N
87.64
60
D
26.27
120
S
61.64
180
R
87.81
10.3
Experimental Results and Discussions
239

Table 10.3 Frequency distribution of notes in raga Rageshree performance
Note
Entire sample
First 60 s
(0–60 s)
Middle 60 s
(30–90 s)
Last 60 s
(60–120 s)
F
Relative
frequency
fr
F
Relative
frequency
fr
F
Relative
frequency
fr
F
Relative
frequency
fr
Sa
64
0.271186
31
0.281818
34
0.295652
33
0.2619047
Sudh Re
32
0.135593
13
0.118181
17
0.147826
19
0.1507936
Sudh Ga
43
0.182203
17
0.154545
22
0.191304
26
0.2063492
Sudh Ma
23
0.097457
10
0.090909
07
0.060869
13
0.1031746
Sudh Dha
11
0.046610
08
0.072727
04
0.034782
03
0.0238095
Komal Ni
16
0.067796
08
0.072727
05
0.043478
08
0.0634920
Sudh Ni
47
0.199152
23
0.209090
26
0.226086
24
0.1904761
Total No. of
notes (Nt)
236
110
115
126
Table 10.4 Chi-Square test for ﬁrst 60 s of the raga Rageshree performance
Note
Observed
frequency (O)
Expected
frequency (E)
(O−E)
(O−E)2
(O−E)2/E
Sa
31
29.830460
1.169540
1.367823
0.045853
Sudh Re
13
14.915230
−1.915230
3.668105
0.245930
Sudh Ga
17
20.042330
−3.042330
9.255771
0.461811
Sudh Ma
10
10.720270
−0.720270
0.373010
0.034794
Sudh Dha
08
5.127100
2.872900
8.253554
1.609789
Komal Ni
08
7.457560
0.542440
0.294241
0.039455
Sudh Ni
23
21.906720
1.093280
1.195261
0.054561
Total
110
Table 10.5 Chi-Square test for middle 60 s of the raga Rageshree performance
Note
Observed
frequency (O)
Expected
frequency (E)
(O−E)
(O−E)2
(O−E)2/E
Sa
34
31.186390
2.813610
7.916401
0.253841
Sudh Re
17
15.593195
1.406805
1.690715
0.108426
Sudh Ga
22
20.953345
1.046655
1.095486
0.052282
Sudh Ma
07
11.207555
−4.207555
17.703519
1.579605
Sudh Dha
Komal Ni
09
(04 + 05)
13.15669
(5.360150
+7.79654)
−4.15669
17.278071
1.313253
Sudh Ni
26
22.902480
3.097520
9.594630
0.418934
Total
115
240
10
Vadi-Samvadi Controversy and Statistics

value for 5 degrees of freedom 3.72634 is less than 11.07 and therefore insigniﬁcant
at 5% level of signiﬁcance for the middle 60 s of the raga Rageshree performance.
Calculated Chi-Square (v2) value for last 60 s of the raga Rageshree perfor-
mance is
X
ððO  EÞ2=EÞ ¼ 1:556915
After pooling the frequencies of Sudh Dha and Komal Ni in one class, 6 classes
left for which Chi-Square calculated for 5 degrees of freedom is 1.556915. The
minimum Chi-Square for rejecting the null hypothesis at 5% level of signiﬁcance is
11.07 as seen from Chi-Square Table 10.7. Calculated Chi-Square value for 5
degrees of freedom 1.556915 is less than 11.07 and therefore insigniﬁcant at 5%
level of signiﬁcance for last 60 s of the raga Rageshree performance.
Table 10.7 Chi-Square table
value at 5% level of
signiﬁcance
Degree of
freedom
v2
value
Degree of
freedom
v2
value
1
3.84
11
19.68
2
5.99
12
21.03
3
7.82
13
22.36
4
9.49
14
23.69
5
11.07
15
25.00
6
12.59
16
26.30
7
14.07
17
27.59
8
15.51
18
28.87
9
16.92
19
30.14
10
18.31
20
31.41
Table 10.6 Chi-Square test for last 60 s of the raga Rageshree Performance
Note
Observed
frequency (O)
Expected
frequency (E)
(O−E)
(O−E)2
(O−E)2/E
Sa
33
34.169436
−1.169436
1.367580
0.0400235
Sudh Re
19
17.084718
1.915282
3.668305
0.2147126
Sudh Ga
26
22.957578
3.042422
9.256331
0.4031926
Sudh Ma
13
12.279582
0.720418
0.5190020
0.0422654
Sudh Dha
Komal Ni
11
(3 + 8)
14.415156
(5.87286
+8.542296)
3.414156
11.663290
0.8090991
Sudh Ni
24
25.093152
1.093152
1.194981
0.0476218
Total
126
10.3
Experimental Results and Discussions
241

From the above tables (Tables 10.4, 10.5 and 10.6), the following observations
are listed
1. The relative frequencies of the overall two-minute performance are best
reﬂected in the performance of the middle 60 s. This observation is, however,
performer dependent.
2. Multinomial model is acceptable at 5% level of signiﬁcance for all three phases
as the calculated chi-Square values are less than the corresponding tabular
values. Hence, entire two minutes performance can be used for Vadi and
Samvadi Swars selection. If Multinomial model breaks down in any phase of the
raga performance then that phase should be excluded for analysis as detecting
Vadi and Samvadi Swars.
3. Next part of analysis in this work is to ﬁnd the rejection of notes as Vadi
Samvadi. Opinions are divided over {Sudh Ga, Suh Ni} and {Sudh Ma, Sa} for
Vadi Samvadi Selection. Applying condition (i) on Table 5.2, Sudh Ma is ruled
out from the contest straightaway due to its low relative frequency. This
automatically rules out Sa from the contest since the importance of Sa is more as
a “tonic” (an origin of reference for realizing other notes), and it could at most
be a Samvadi Swara, if Ma could be endorsed as Vadi. Besides, the stability of
{Sudh Ga, Sudh Ni} cannot be questioned for the ﬁrst two phases with Sudh Ga
settling to where it should be by the second phase. Both are Nyas Swars and
both have high relative frequencies. Why Sudh Ni has a higher relative fre-
quency than Sudh Ga in the ﬁrst two phases is a performer dependent phe-
nomenon, and it could well be the other way round.
10.3.2
Ranking of Notes
Ranking of notes is based on the mean duration of the note in the entire perfor-
mance. Duration of a note is calculated by ﬁnding out the difference between onset
and departure of the note from Solo Explorer 1.0 plots. Figure 10.1 shows a typical
Solo explorer plot sample for raga Rageshree vocal performance. Durations of all
notes used by the artist in the 120 s raga Rageshree performance are given in
Tables 10.8, 10.9, 10.10, 10.11, 10.12, 10.13 and 10.14.
From the duration tables of notes (from Tables 10.8, 10.9, 10.10, 10.11, 10.12,
10.13 and 10.14) mean duration and standard deviations are calculated and tabu-
lated in the summary table of note duration Table 10.15.
Ranks are given on the basis of mean duration of the note. Higher rank (i.e.
lower numerical value; rank 01 is higher than rank 02) is given for higher mean
duration. If mean durations are equal (say up to six places of decimals), a note with
a lower standard deviation will be given a higher rank.
242
10
Vadi-Samvadi Controversy and Statistics

From the above tables result the following observations are listed
1. The Nyas Swars experimentally detected using Krumhansl’s theory (Castellano
et al. 1984) are Sudh Ni, Sa, Sudh Ga and Sudh Re. Theoretically they are Sudh
Ni, Sa, Sudh Ga, and Sudh Ma. Thus, Sudh Ma does not have a good rank to
claim psychological stability or Nyas Swar. Since the Vadi Swar in raga
Rageshree has to be a Nyas Swar, Sudh Ma loses the contest for being Vadi
Swar from psychological consideration of note stability. As Sudh Ma is
already ruled out from being Vadi Swar due to its low relative frequency in
overall performance, both the techniques are conﬁrming the rejection of Ma as
Vadi Swar.
2. Since Ma could not be Vadi Swar, Sa is also rejected for being Samvadi Swar.
This means that Vadi-Samvadi Swar can only be (Sudh Ga, Sudh Ni), and we
select Sudh Ga as Vadi Swar and Sudh Ni as Samvadi Swar from statistical
considerations as stated earlier.
Table 10.16 summarizes the experimental results of Vadi and Samvadi Swars
detection for different raga performances.
Fig. 10.1 Solo explorer 1.0 graph of raga Rageshree sample
10.3
Experimental Results and Discussions
243

Table 10.8 Duration table for the note Sa
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
1
0.02
23
0.4
45
0.86
2
0.95
24
0.5
46
0.04
3
0.33
25
0.51
47
0.1
4
0.05
26
0.01
48
0.08
5
0.08
27
0.02
49
0.01
6
0.02
28
0.22
50
0.3
7
0.01
29
0.03
51
0.11
8
0.18
30
0.01
52
1.61
9
0.08
31
0.06
53
0.03
10
0.02
32
0.31
54
0.02
11
0.03
33
0.41
55
0.01
12
0.02
34
0.3
56
0.3
13
1.6
35
0.06
57
0.06
14
0.26
36
0.29
58
0.02
15
0.32
37
0.30
59
0.21
16
0.20
38
0.02
60
0.3
17
0.06
39
0.02
61
0.41
18
0.04
40
0.21
62
0.12
19
0.07
41
0.02
63
0.04
20
0.22
42
0.51
64
0.01
21
0.02
43
0.02
22
0.04
44
0.05
Table 10.9 Duration table for the note Sudh Re
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
1
0.66
12
0.02
23
0.04
2
1.33
13
0.3
24
0.07
3
0.2
14
0.63
25
0.04
4
0.15
15
0.09
26
0.06
5
0.02
16
0.3
27
0.2
6
0.09
17
0.06
28
0.01
7
0.04
18
0.06
29
0.2
8
0.02
19
0.02
30
0.1
9
0.08
20
0.02
31
0.3
10
0.06
21
0.01
32
0.01
11
0.49
22
0.11
244
10
Vadi-Samvadi Controversy and Statistics

Table 10.10 Duration table for the note Sudh Ga
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
1
0.03
16
0.02
31
0.4
2
0.04
17
0.13
32
0.03
3
0.04
18
0.54
33
0.2
4
0.03
19
0.08
34
1.2
5
0.02
20
0.06
35
0.04
6
1.27
21
0.07
36
0.1
7
0.45
22
0.02
37
0.03
8
0.01
23
0.91
38
0.07
9
0.07
24
0.17
39
0.03
10
0.33
25
0.01
40
0.12
11
0.03
26
0.33
41
0.02
12
0.12
27
0.07
42
0.2
13
0.3
28
0.3
43
0.13
14
0.25
29
0.02
15
0.08
30
0.1
Table 10.11 Duration table for the note Sudh Ma
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
1
0.03
9
0.04
17
0.08
2
0.01
10
0.01
18
0.07
3
0.01
11
0.05
19
0.11
4
0.05
12
0.06
20
0.03
5
0.02
13
0.02
21
0.03
6
0.06
14
0.02
22
0.03
7
0.05
15
0.06
23
0.01
8
0.06
16
0.03
Table 10.12 Duration table for the note Sudh Dha
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
1
0.04
5
0.04
9
0.01
2
0.02
6
0.01
10
0.03
3
0.02
7
0.02
11
0.02
4
0.02
8
0.03
10.3
Experimental Results and Discussions
245

Table 10.13 Duration table for the note Komal Ni
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
1
0.02
7
0.01
13
0.02
2
0.02
8
0.01
14
0.02
3
0.03
9
0.04
15
0.03
4
0.02
10
0.06
16
0.01
5
0.01
11
0.01
6
0.02
12
0.02
Table 10.14 Duration table for the note Sudh Ni
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
Occurrence
of note
Duration of
note in
seconds
1
0.02
17
0.52
33
0.3
2
0.06
18
0.02
34
0.2
3
0.02
19
0.05
35
0.01
4
0.03
20
0.09
36
0.02
5
1.45
21
0.01
37
0.03
6
0.02
22
0.02
38
0.01
7
0.04
23
0.03
39
0.88
8
0.34
24
0.01
40
0.02
9
0.01
25
1.20
41
0.07
10
0.08
26
0.11
42
0.11
11
0.17
27
0.03
43
1.62
12
0.46
28
0.05
44
0.2
13
0.02
29
0.02
45
0.1
14
0.23
30
1.20
46
0.2
15
0.06
31
0.2
47
0.3
16
0.02
32
0.1
Table 10.15 Summary table of note-duration
Note
Mean duration (s)
Standard deviation
Rank based on mean duration
Sa
0.211563
0.320145
02
Sudh Re
0.180938
0.271048
04
Sudh Ga
0.196977
0.291421
03
Sudh Ma
0.040870
0.025746
05
Sudh Dha
0.023636
0.010269
06
Komal Ni
0.021880
0.013276
07
Sudh Ni
0.228936
0.390731
01
246
10
Vadi-Samvadi Controversy and Statistics

10.4
Chapter Conclusion
The test of psychological stability and statistical stability of permissible notes in
the raga can be used to decide which notes are Vadi and Samvadi notes of the raga.
The analysis of raga Rageshree results in Sudh Ga as Vadi and Sudh Ni as
Samvadi notes for raga Rageshree, under a multinomial model setup. However,
Sudh Ma and Sa cannot be ruled out for all situations, this being a case study. In the
present performance of raga Rageshree, the artist can only defend that Gharana
(musical school) which supports Sudh Ga as Vadi swar and Sudh Ni as Samvadi
Swar.
References
Castellano MA, Bharucha JJ, Krumhansl CL (1984) Tonal hierarchies in the music of north India.
J Exp Psychol Gen 113(3):394–412
Chakraborty S, Mazzola G, Tewari S, Patra M (2014) Computational musicology in Hindustani
music, Springer International Publishing
Chakraborty S, Ranganayakulu R, Chauhan S, Solanki SS, Mahto K (2009) “A statistical analysis
of raga Ahir Bhairav”. J Music and Meaning 8(4) Winter
Chakraborty S, Ranganayakulu R, Chauhan S, Solanki SS, Mahto K (2013) “Given a raga
recording, can we scientiﬁcally verify which school of Vadi-Samvadi selection is supported by
the artist?” Intern J Comput Techn 4(2) March–April
Chakraborty S, Solanki SS, Roy S, Chauhan S, Tripathy SS, Mahto K (2011) “A statistical
approach to modeling indian classical music performance”. Int J Comput Cogn 9(4) December
Dutta D (2006) “Sangeet tattwa (pratham khanda)”, Brati Prakashani, 5th edn.
Mahto K, Solanki S.S, Chakraborty S (2014) “Automatic extractions of notes and their onsets from
Indian classical music signal” Int J Appl Eng Res 9(21)
Roychaudhary B (2005) The grammar of North Indian ragas. OM Publication, New Delhi
Table 10.16 Summary table of Vadi and Samvadi Swars detection
Raga name
Vadi and Samvadi Swars given
by music theory
Vadi and Samvadi Swars detected
experimentally
Pilu
Komal Ga and Sudh Ni
Komal Ga and Sudh Ni
Kirwani
Not speciﬁc
Komal Ga and Sudh Ni
Yaman
Sudh Ga and Sudh Ni
Sudh Ga and Sudh Ni
Ahir Bhairav
Sudh Ma and Sa
Sudh Ma and Sa
Rageshree
Sudh Ga and Sudh Ni
(or Sudh Ma and Sa?)
Sudh Ga and Sudh Ni
10.4
Chapter Conclusion
247

Epilogue
In stark contrast with western countries where almost every university or big
academic institutions have departments related to scientiﬁc and technological
pursuit in music, India probably has none. Even the scientists who are also musi-
cians seem generally to have mindset of indifference, if not antagonism, towards
applying their scientiﬁc minds to music. The co-operations of musicians are not
forthcoming to say the least. The academics in the country also seem to be least
interested in the issue. The odds are formidable which is exactly what makes it
challenging. The light is still kept burning through the zeal of a few individuals in
different ﬁelds of science having a personal love for the issue. There is only one
annual conference Frontiers of Research in Speech and Music (FRSM) in the whole
country which provides a platform to these people to interact.
The cultural and characteristic diversity of Indian music is enormous. This book
presents an attempt to use science to investigate music only for a small but an
esoteric area, namely the north Indian or Hindustani classical music, of this huge
ocean. The other areas like the counterpart, the classical music of south India,
namely the Carnatic music; the other regional music including the folk music, the
modern music including ﬁlm music demands the same attention. It is quite possible
and expected that the common view of different classical music genres having
intimate evolutionary relationship with the regional music may have a rational
basis. In the same vein it is also likely that modern music may have a similar root in
the other two classes. The need to rationally investigate them is obvious. It requires
huge effort both in terms of manpower and money.
It may be seen that different ﬁelds of sciences and technologies like physics,
mathematics, physiology, psychology, statistics, signal processing, computer sci-
ence including algorithms etc. play an incisive role in unraveling the mystery of
music. It gives the scientiﬁc mind a new ﬁeld to play with through disproving old
theories and establishing new ones. In the Indian context this book may be looked
upon as a beacon presenting possibly the only sustained dedicated work of more
than three decades of a small group of enthusiasts in a small corner of this vast
country. Admittedly this is only a drop in the ocean of need. However the authors
© Springer Science+Business Media Singapore 2017
A.K. Datta et al., Signal Analysis of Hindustani Classical Music,
Signals and Communication Technology, DOI 10.1007/978-981-10-3959-1
249

feel that at least a model of scientiﬁc investigation inter alia, in this complex area of
simulating human mind, has come into picture. The authors also expect this book,
which is the ﬁrst book on musical signal processing in the context of Hindustani
music, will surely attract scientists, musicologists, technologists and musicians to
this titillating area of ideational adventure. We close hoping there will be many
scientists to take up the challenge.
[Concluded]
250
Epilogue


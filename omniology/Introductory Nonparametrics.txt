J.C.W. Rayner
Introductory Nonparametrics
Download free books at

2
Ôªø
J.C.W. RAYNER
INTRODUCTORY 
NONPARAMETRICS
Download free eBooks at bookboon.com

3
Introductory Nonparametrics 
1st edition
¬© 2016 J.c.w. Rayner & bookboon.com
ISBN 978-87-403-1475-5
Peer review: Dr Paul Rippon, Conjoint Lecturer in Statistics School of Mathematical and 
Physical Sciences, Faculty of Science and Information Technology
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
4
Contents
4
CONTENTS
	
About the author	
6
	
Preface	
7
1	
A First Perspective on Nonparametric Testing	
10
1.1	
What are nonparametric methods?	
10
1.2	
The sign tests	
11
1.3	
Runs tests	
15
1.4	
The median test	
19
1.5	
The Wilcoxon tests	
22
2	
Nonparametric Testing in the Completely Randomised,  
Randomised Blocks and Balanced Incomplete Block Designs	
30
2.1	
Introduction and outline	
31
2.2	
The Kruskal-Wallis test	
31
2.3	
The Friedman test	
33
2.4	
The Durbin test	
34
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

INTRODUCTORY NONPARAMETRICS
5
Contents
2.5	
Relationships of Kruskal-Wallis, Friedman and Durbin tests with  
ANOVA F tests	
35
2.6	
Orthogonal contrasts: Page and umbrella tests	
41
3	
Permutation Testing	
48
3.1	
What is permutation testing and why it is important?	
48
3.2	
Nonparametric multifactor ANOVA when the levels of the factors  
are unordered 	
52
3.3	
Revisiting some previous examples	
58
	
Concluding Remarks	
62
	
References	
63
	
Subject Index	
64
	
Examples Index	
65
	
Exercises	
66
	
Chapter One Exercises	
66
	
Chapter Two Exercises	
67
	
Chapter Three Exercises	
70
	
Solutions	
73
	
Solutions to the Chapter One Exercises	
73
	
Solutions to the Chapter Two Exercises	
81
	
Solutions to the Chapter Three Exercises	
84
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
6
About the author
ABOUT THE AUTHOR
John Rayner is currently Honorary Professorial Fellow at the Centre for Statistical and Survey 
Methodology, School of Mathematics and Applied Statistics, University of Wollongong, 
NSW, Australia and Conjoint Professor of Statistics at the University of Newcastle in NSW, 
Australia. He served as Professor of Statistics and Head of Discipline at the University of 
Newcastle from 2006 to 2011 before retiring from full-time employment. Previously John 
worked full-time at the University of Otago in Dunedin, New Zealand from 1973 to 1992 
and the University of Wollongong in NSW, Australia from 1992 to 2006.
John‚Äôs prime research interests are goodness of fit (assessing statistical models) and 
nonparametric statistics. He is the lead author of Smooth Tests of Goodness of Fit: Using R 
and A Contingency Table Approach to Nonparametric Testing. He has written over 150 research 
articles and books, many with his long-time friend and colleague John Best.
Now in his 71st year, John exercises moderately every day with the aim of participating in 
a weekly parkrun. These are timed 5km runs at venues all over the world. Last year he ran 
under 25 mins several times and, once, under 24 mins!
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
7
Preface
PREFACE
Nonparametric procedures usually feature in undergraduate statistics courses 
1)	in parallel with the corresponding parametric procedures, 
2)	as a module in a larger course, and 
3)	as a sequence of modules, perhaps in a half course or short course. 
In the first option it is common, for example, to treat the one-way analysis of variance in 
tandem with the Kruskal-Wallis test and the two-way analysis of variance in tandem with 
the Friedman test. However, it seems best that an introduction to nonparametric ideas 
should precede such a pairing. 
The modular approach adopted here better serves the second and third options. The first 
module gives a gentle introduction to nonparametric ideas, assuming readers have already met 
the binomial and normal distributions and some parametric methods. The second module 
introduces nonparametric tests for the simpler and most commonly used experimental 
designs. The third focuses on permutation tests, a fundamental nonparametric tool. In 
an undergraduate statistics sequence it would be reasonable to include these modules in 
successive semesters in comprehensive courses or as a half course or short course towards 
the end of a statistics major.
Nonparametric tests should be applied when the parametric assumptions cannot reasonably 
be assumed. However there are users who believe that many parametric tests are so robust 
to their parametric assumptions that they can be applied almost always. Nevertheless there 
remain many parametric procedures, such as the Bartlett test of equality of variance, that 
are known to be highly non-robust. The contrary point of view is that it is always wise to 
use analyses that assume as little as possible. If normality is an assumption in a parametric 
test, how small a p-value should a test of normality be before the conclusions of the 
parametric test be regarded as suspect? In general there is no clear guidance on this. For a 
t-test a small p-value would not deter most analysts, but for Bartlett‚Äôs test the same is not 
true. Perhaps the wisest course, even when dealing with robust procedures, is to apply both 
the parametric and the corresponding nonparametric procedure, if one is available, and to 
investigate further if they disagree.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
8
Preface
The analysis of data is fundamental to statistics courses. Calculations can sometimes be done 
using calculators and tables, although elementary statistical packages would be more common. 
Favourites would be MINITAB, EXCEL, JMP and SPSS. Readers should work through 
the examples and exercises here using whatever software is familiar to them. However R is 
freeware and Dr Paul Rippon has written an R Companion for the material here: Rippon 
(2016). It is strongly recommended that the reader work through the material here and the 
R Companion simultaneously. For those developing their R skills verifying R output using 
the package with which they are most familiar would be a sensible way forward.
We assume that those readers initially without a background in R will acquire appropriate R 
skills over the time they are working through this material. Short courses are often available 
or appropriate resources are available at, for example, http://cran.r-project.org/other-docs.html.
Sometimes different software will produce different outcomes to each other and to hand 
calculations. This isn‚Äôt necessarily wrong. For example there are different ways to treat 
ties and this is not always apparent in the output of various packages. If in doubt most 
packages have Help files that give details. However most users can cheerfully dismiss minor 
discrepancies as ‚Äònoise‚Äô but dig deeper when major discrepancies arise.
The first chapter gives a taste of nonparametric procedures appropriate in an introductory 
statistics course that assumes minimal mathematical background. The material will be covered 
towards the end of the course after the student has become familiar with random variables, 
the binomial and normal distributions and the one and two sample t tests. The appendix 
gives some challenge material that may be omitted at first reading.
The second chapter focuses on what are possibly the three most useful experimental designs: 
the completely randomised, randomised block and balanced incomplete block designs. The 
appropriate nonparametric tests, the Kruskal-Wallis, Friedman and Durbin tests are given, as 
are examples in which p-values are calculated using the asymptotic chi-squared approximations 
to the null distributions of the test statistics. Improved approximations are given using 
ANOVA F tests. Finally trend and umbrella tests are derived as orthogonal contrasts.
Many nonparametric tests find p-values using an asymptotic or approximate distribution 
that, in some circumstances, may be far from satisfactory. Resampling methods like the 
bootstrap or permutation testing may need a little coding, but given that, yield quick and 
accurate results. The third chapter introduces permutation testing. Knowledge of a computing 
language like R is assumed.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
9
Preface
There are exercises for each chapter. Their solutions draw heavily on the R code in Rippon 
(2016). Parts of some of the analyses were done using JMP, the ‚Äòclick and point‚Äô software 
with which I am most familiar nowadays. Readers should also use the software that is most 
familiar to them. Wherever possible the R programs in Rippon (2016) have been modified 
and applied to the exercises.
The following Additional Supplementary Files give some of the R code used in this book.
‚Ä¢	 Herbicide Example
‚Ä¢	 Vanilla flavour scores data entry
‚Ä¢	 Chapter Two Exercise 1 Solution
‚Ä¢	 Chapter Two Exercise 2 Solution
‚Ä¢	 Chapter Two Exercise 3 Solution
‚Ä¢	 Chapter Three Exercise 1 Solution Comprehensive Chocolate Analysis
‚Ä¢	 Chapter Three Exercise 2 Solution Comprehensive Word processors Analysis
‚Ä¢	 Chapter Three Exercise 3 Solution Comprehensive flavour score analysis
This is not all the R code in the text. You will need to type in the remainder.
My good friend and colleague Dr John Best wrote computer programs that supported the 
data analysis throughout this material, and has helped to clarify both my thinking and 
the text. Subsequently Dr Paul Rippon produced the R Companion (Rippon, 2016) that 
accompanies this effort. In doing so he read my text and made many useful suggestions.
My deepest thanks to Paul and John for their contributions.
Reference
RIPPON, Paul (2016). An R Companion to Introductory Nonparametrics.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
10
A First Perspective on Nonparametric Testing
1	
A FIRST PERSPECTIVE ON 
NONPARAMETRIC TESTING
Learning Objectives
After successful completion of the material in this chapter the student will be able to
‚Ä¢	 discuss the nature of nonparametric methods and contrast them to parametric 
methods, and
‚Ä¢	 apply a number of nonparametric tests to appropriate data sets.
1.1	
WHAT ARE NONPARAMETRIC METHODS?
Although there is good agreement on which tests are nonparametric, a definition is hard to 
give. Typically nonparametric tests assume less than their parametric competitors, but there 
is much more to distinguish the two than this.
In most introductory statistics courses most of the tests studied initially are parametric: 
they make reference to specific parameters of the population under study, or they are valid 
only if the population has some specific distribution, such as the normal, or the binomial. 
Consider, for example, the one-sample t-test: it assumes X1, ‚Ä¶,  Xn  are independent and 
normal with mean ¬µ and variance 2. If X  = ‚àëi
i n
X /
	 ¬† and S2 = 
(
) (
)
1
/
2
‚àí
‚àí
‚àë
n
X
X
i
i
  ¬† then 
the one-sample t-test tests for a particular population mean ¬µ0, formally testing H0: ¬µ = ¬µ0  
against one sided (K11: ¬µ < ¬µ0 or K12: ¬µ > ¬µ0)	 ¬† or two-sided d (K2: ¬µ ‚â† ¬µ0)  alternatives, uses the 
test statistic T = ( X ‚Äì ¬µ0)‚àön/S. The test statistic T has the t distribution with n ‚Äì 1  degrees 
of freedom:  tn-1. This is very definitely a parametric test: it is testing for a particular value of 
the parameter ¬µ, and it depends clearly on the assumption of normality. If the normality or 
independence assumptions are dubious, or if the first half of the observations is less variable 
than the second half, then this t-test cannot be validly applied. Then what is needed is a 
test that makes fewer assumptions. Typically a nonparametric test makes fewer assumptions 
than the corresponding parametric test. Sometimes so little is assumed in the scenario of 
interest that it is not reasonable to construct a parametric test. We give examples of both 
these situations.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
11
A First Perspective on Nonparametric Testing
Another reason why a parametric test may be inapplicable is that the data may not be in 
a suitable form. Four scales of measurement can be identified.
i)	 Nominal. Variables differ in kind rather than amount. The data are categorised, 
perhaps into colour, or gender.
ii)	Ordinal. Ordinal scales may be based on qualitative rather than quantitative variables; 
however some ordering is also implied. This frequently involves ranks, as in ranking 
taste preferences. Alternatively, the variables may be categorical; for example, people 
may be categorised as short, medium height or tall.
iii)	Interval and ratio scales. Measurements are quantitative, and the usual arithmetic 
operations can meaningfully be used. In the ratio scale the zero point reflects the 
absence of the attribute being observed. This is not the case with the interval scale. 
Examples are probabilities and temperatures.
Parametric procedures typically involve interval or ratio scales. Nonparametric procedures 
are available for all measurement scales, and are certainly possible for nominal and ordinal 
scales, where parametric procedures are not available. The wider applicability of nonparametric 
procedures may be offset by them having less power and efficiency when the parametric 
assumptions are valid. (We won‚Äôt go into the technical meanings of power and efficiency 
here, but with both, more is good.) This is quite reasonable: if a parametric procedure can 
validly assume more, it is more likely to give deeper insights into the analysis. If some of 
the parametric assumptions are not valid, the nonparametric procedure is valid when the 
parametric is not. 
In the following sections the reader is introduced to a number of nonparametric tests: the 
one and two-sample sign tests, a two-sample runs test and a runs test for randomness, the 
median test, the Wilcoxon signed ranks test and the Wilcoxon two sample test of location.
1.2	
THE SIGN TESTS
1.2.1	 A ONE-SAMPLE SIGN TEST
The one sample t-test is the parametric test for a particular population mean. We now 
develop a test for a particular median. Like the mean, the median of a distribution is 
a measure of central tendency. For the distribution of a random variable X the median 
satisfies P(X < median) = P(X > median). For symmetric distributions, the mean is equal to 
the median. The null hypothesis is H0:  median = m0; the alternative could be one-sided, 
K11: median > m0 (or K12: median < m0)  or two-sided, K2: median ‚â† m0.  The test statistic 
is S, the number of observations greater than m0. The name of the test comes from calling 
an observation greater than m0 ‚Äòpositive‚Äô, and an observation less than n m0  ‚Äònegative‚Äô. The 
test statistic is then the number of positive (or negative) signs.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
12
A First Perspective on Nonparametric Testing
12
Under H0,  a single observation of X is equally likely to be at least or at most n m0 . When 
X is continuous P(X > m0) = 0.5  and under H0 the statistic S is binomially distributed 
with parameters n, the total number of observations, and p = 0.5. For n > 10, p-values may 
be calculated using a normal approximation. When the distribution of X is assumed to 
be continuous observations equal to n m0  are usually discarded. There are many possible 
treatments for ties, that result when continuity cannot be assumed. See, for example, Rayner 
and Best (1999).
Aside. Subsequently we write bin (n, p) for the binomial distribution with n Bernoulli trials 
and probability of success p, while b(n, p, x) = nCx  px  (1‚àíp)n‚àíx  for x = 0, 1, ‚Ä¶ ,  n is the 
binomial probability function.
Monkey Example. Adult female monkeys at a particular site are known to have median 
weight 8.41 kilograms. Can the same conclusion apply to monkeys from another site with 
weights, in kilograms,
8.30, 9.50, 9.60, 8.75, 8.40, 9.10, 9.25, 9.80, 
10.05, 8.15, 10.00, 9.60, 9.80, 9.20, 9.30? 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

INTRODUCTORY NONPARAMETRICS
13
A First Perspective on Nonparametric Testing
Of the 15 observations, 12 are greater than the hypothesised median. Calculation or tables 
give P(S < 11) = b(15, 0.5, 0) + ... + b(15, 0.5, 11) = 0.982. If the alternative specified larger 
monkeys at the new site, our p-value, the probability of observations at least as extreme as 
the observed, is P(S > 12) = 1 ‚Äì 0.982 = 0.018. The null hypothesis can be rejected at the 
0.05 level but not the 0.01 level. Since we were looking for a different median, P(data | 
null hypothesis) = P(S ‚â• 12 or S ‚â§ 3) = 2 P(S ‚â• 12) = 0.036. Again there is evidence, at the 
0.05 level but not the 0.01 level, against the null hypothesis. 
For n > 10  and p not extreme the normal distribution with the same mean and variance as 
the binomial is a good approximation to the binomial. The more extreme the probability 
of success the larger n needs to be for a good approximation. It is counterproductive to be 
more precise.
In this example n > 10  and since this is the sign test p = 0.5. We therefore apply the 
normal approximation: P(S ‚â• 12 | S is bin (15, 0.5))  is approximately equal to P(Z 
> (11.5 ‚Äì 7.5)/‚àö(15/4) = 2.0656|Z  is N(0, 1)) = 0.019. From the above the exact value is 
0.018: the approximation is excellent. This calculation uses the binomial mean np and 
variance np(1 ‚Äì p)  and the continuity correction. Note that whenever a discrete distribution 
is approximated by a continuous one, the continuity correction is required. So if a discrete 
distribution takes the values x1, x2,  and is approximated by a continuous random variable Y 
then P(X = xi)  is approximately P([xi‚Äì1 + xi]/2 < Y < [xi + xi+1]/2). For the normal approximation 
to the binomial P(X > x| X  is bin(n, p))  is approximately P(Y > x ‚Äì 0.5| Y  is normal with 
mean np and variance np(1 ‚Äì p)).
Compared to the t-test, the sign test has asymptotic relative efficiency (ARE) 2/S = 63.66%. 
Roughly this means that when the data are normally distributed, to achieve the same power 
as the sign test does with 100 observations, the t-test asymptotically requires only 63.66 
observations. However the sign test is more generally applicable even if it is generally 
less powerful. When the data are not normal the ARE is greater than 2/S,  and for some 
distributions the ARE is greater than 100%. 
Exercise. Analyse the monkey data above using a convenient and appropriate computer 
package. You should find that normality is a valid assumption. For example the Shapiro-
Wilk test for normality has p-value 0.25. Any convenient test of normality should give a 
p-value well in excess of 0.05. The t-test for H0: Œº = 8.41  against K: Œº ‚â† 8.41 has p-value 
0.000. The signed-ranks test, a nonparametric test we will consider later in this chapter, 
also has p-value 0.000. When normality holds the signed-ranks test is more efficient than 
the sign test and less efficient than the t-test.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
14
A First Perspective on Nonparametric Testing
1.2.2	 A TWO-SAMPLE SIGN TEST
Suppose now we have paired subjects. These might be the same individual with identical 
skin conditions on both forearms. In some studies the paired subjects may be twins, or 
individuals matched on a number of factors. Different treatments are then applied to each 
individual in the pair: treatment X say to one, and treatment Y say to the other. The aim 
is to assess which treatment is preferable.
More formally suppose we have n independent pairs of observations (X1, Y1), ..., (Xn, Yn),, 
and we wish to test that the X‚ÄÜ‚Äôs and Y‚ÄÜ‚Äôs have the same distribution against the alternative 
that they differ in location. The parametric paired t-test could be used if it is valid to assume 
that the differences are normally distributed and all have the same variance.
To perform this sign test first calculate the differences Di = Xi ‚Äì Yi, i = 1, 2, ... , n. Second, 
count S, the number of positive differences, or signs. The distribution of S is binomial l b(n, p),, 
and we wish to test H0: p = 0.5  against one or two sided alternatives such as K: p ‚â† 0.5. 
This test doesn‚Äôt assume normality but it does involve the binomial parameter. It is widely 
regarded as a nonparametric test, but it might more accurately be said to be less parametric 
than the paired t-test. 
Heart Rates Example. In Table 1.1 the heart rates in beats per minute of 10 rats alone and 
in the presence of another are given. We now test at the 0.05 level if togetherness increases 
heart rate. There are eight negative differences or signs and two positive differences or signs. 
We calculate P(S ‚â§ 2|bin(10, 0.5)) = 0.0547. At the 0.05 level the null hypothesis of no 
increase cannot be rejected, but given this borderline acceptance it may well be prudent to 
conduct another study with more than 10 subjects. 
Interestingly the Shapiro-Wilk test for normality has a large p-value and the t-test p-value is 
0.001. As is not uncommon, the parametric test is far more critical of the null hypothesis 
than the nonparametric test. One possible explanation of this phenomenon is that the t-test 
makes more assumptions, which may not be true, and hence is more critical of the data 
and the assumptions. Conversely the nonparametric test doesn‚Äôt make use of assumptions 
that may not be true.
Rat number
1
2
3
4
5
6
7
8
9
10
Rate alone (xi)
463
462
462
456
450
426
418
415
409
402
Rate together (yi)
523
494
461
535
476
454
448
408
470
437
Difference
‚Äì60
‚Äì32
1
‚Äì79
‚Äì26
‚Äì28
‚Äì30
7
‚Äì61
‚Äì35
TABLE 1.1. Rat data
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
15
A First Perspective on Nonparametric Testing
15
1.3	
RUNS TESTS
Suppose we have observations of two types: A and A . These might be defective and non-
defective, male and female or, when candidates are interviewed for a position, university 
graduate and non-graduate. A typical sequence could be
A, A , A , A, A, A, A , A , A. 
Any sequence of like observations, bounded by observations of a different type, is called 
a run. Alternatively a run is defined to be the greatest subsequence of like elements. The 
number of observations in the run is called its length. In the above sequence there are five 
runs, of lengths 1, 2, 3, 2, 1.
Runs tests are given in the next two subsections: both are nonparametric. Although 
approximate distributions are given for the test statistics, no distributional assumptions are 
made about the data.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

INTRODUCTORY NONPARAMETRICS
16
A First Perspective on Nonparametric Testing
1.3.1	 A TWO-SAMPLE RUNS TEST
Suppose we have e X1, ..., Xm, a random sample of size m from the X population, and Y1,, 
..., Yn, an independent random sample of size n from the Y population. These samples are 
combined, ordered, and classified as either an X or a Y. A typical result would be similar 
to the following:
X < X < Y < Y < Y < X < Y < X < Y. 
From such data we can calculate the number of runs. Under the null hypothesis that the X 
and Y populations are identical a moderate or large number of runs could be expected. A 
small number of runs could result under an alternative hypothesis of differences in location 
(such as mainly Xs followed by mainly Ys):
     X   XX   X         X                         YY          Y      Y  
or under an alternative hypothesis of differences in dispersion (such as mainly Xs  between 
clusters of mainly Ys in the tails):
     Y         Y      Y        X     Y    XX  X  X  XX   YX          YY      Y        Y 
The test statistic is therefore taken to be one-tailed.
Flints Example. Four pieces of flint were collected from area A, and five pieces from area B. 
By scratching against each other, the flints were ranked in order of hardness:
A, A, A, B, A, B, B, B, B. 
There are four runs. Is this significantly small? We need the distribution of T, the number 
of runs. No proof is given for the following result.
Probability function of T, the number of runs
Under the null hypothesis that the m‚ÄÜXs and n‚ÄÜYs come from the same population, the 
probability function of T is given by
P(T = 2k) = 2 m‚àí1Ck‚àí1 n‚àí1Ck‚àí1
m+nCn
 and P(T = 2k + 1) = 
m‚àí1Ck  n‚àí1Ck‚àí1 +  m‚àí1Ck‚àí1 n‚àí1Ck
m+nCn
. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
17
A First Perspective on Nonparametric Testing
Flints Example continued. For the flints we have m = 4, n = 5 and t = 4. So with k = 1, 
P(T = 2) = 2 3C0 4C0
9C5
 = 2/126 and P(T = 3) = 
3C1 4C0 +  3C0 4C1
9C5
 = 7/126. 
With k = 2,
P(T = 4) = 2 3C1 4C1
9C5
 = 24/126. 
Thus the probability of observations at least as extreme as the observed is P(T ‚â§ 4) =  33/126 
= 0.2619.
Normal Approximation
Use of the exact formula is tedious for moderate m and n. However if both are greater than 
10 an excellent normal approximation is available. This needs the mean and variance of T:
E[T] = 
n
m
mn
+
+ 2
1
 and var(T) = 
(
)
(
) (
)
1
2
2
2
‚àí
+
+
‚àí
‚àí
n
m
n
m
n
m
mn
mn
. 
Again no proof is given.
Flints Example continued. Direct calculation gives E[T‚Äâ] = 1 + 2‚Äâ√ó‚Äâ4‚Äâ√ó‚Äâ5/9 = 49/9 = 5.4444 and 
var (T) = 2u4u5u(40 ‚Äì 9)/(81u8) = 155/81 = 1.9136 = 1.38332. Using the continuity correction, 
for the flints data
P(T ‚â§ 4 ) = P(Z ‚â§ (4.5 ‚Äì 5.4444)/1.3833| Z is N(0, 1)) 
= P(Z ‚â§ ‚Äì 0.6827) = 0.2474. 
This is remarkable agreement since neither m nor n is greater than 10. However the 
approximation is not as good in the tails, the important region, unless both m and n are 
greater than 10.
1.3.2	 A RUNS TEST FOR RANDOMNESS
The data must be of two types, such as success or failure as in the binomial situation, or 
above or below the median (or upper quartile) with continuous data. Usually, though not 
necessarily, we try to ensure approximately the same numbers of each type, as this improves 
the normal approximation. Unlike the two-sample test described in 1.3.1, the alternative here 
is two-sided. A large number of runs suggests some alternating mechanism (continuously 
overcorrecting) and a small number of runs is consistent with a mechanism in control until 
a fault occurs. The previous formulas apply.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
18
A First Perspective on Nonparametric Testing
18
Examination Example. A true-false examination produces the following sequence of correct 
answers: 
T, F, F, T, F, T, F, T, T, F, T, F, F, T, F, T, F, T, T, F. 
Here we have m = 10, n = 10,  and t = 16.  We have E[T] = 11  and var(T) = 4.7368 , 
so the normal approximation gives P(T ‚â• 16) = P(Z ‚â• (15.5 ‚Äì 11)/2.1764|Z is N(0, 1)) =  
P(Z ‚â• 2.0676) = 0.0193. Since a two-tailed test is called for, the p-value is 0.0386. At the 
0.05 level there is some evidence of non-randomness in the answers.
Speed Example. The (unordered) speeds of every fifth passenger car past a check-point, in 
miles per hour, were:
46, 58, 60, 56, 70, 66, 48, 54, 62, 41, 39, 52, 45, 62, 53, 69, 65, 
65, 67, 76, 52, 52, 59, 59, 67, 51, 46, 61, 40, 43, 42, 77, 67, 63, 
59, 63, 63, 72, 57, 59, 42, 56, 47, 62, 67, 70, 63, 66, 69, 73. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

INTRODUCTORY NONPARAMETRICS
19
A First Perspective on Nonparametric Testing
The median is 59.5 and if observations are classified as either above or below the 
median you should obtain m = n = 25  and t = 20.  Since E[T] = 26, var(T) = 12.2449  a 
p-value (for two tails) of 0.116 results. If speeds above and below 55.1 are assessed, we 
obtain m = 33, n = 17, t = 18,  so that E[T] = 23.44, var(T) = 9.8186  and the p-value is 
2P(T ‚â§ 18) = 2P(Z < ‚Äì1.5765) = 0.115..
Trends of similar speeds could be expected: in high density traffic almost everyone travels 
at the same speed, but on the open road drivers tend to treat the speed limit as both an 
upper and a lower limit. This is equivalent to using one-sided tests; the alternative would 
specify low values of T. This means we are more critical of these data.
1.4	
THE MEDIAN TEST
Experience has shown that the runs test is sensitive to differences both in shape and 
location. The median test is sensitive to differences in location, but not to differences in 
shape. When normality holds both the median test and the runs test have test efficiency  
(ARE) 2/ = 63.66%  compared to their parametric competitors.
From the ith of c populations a random sample of size ni  is drawn. The observations 
are pooled and a predetermined quantile, usually the median, is found. The numbers of 
observations in the ith  sample that are above (Ai)  and below (Bi)  the combined quantile 
are then as in Table 1.2. All row and column totals are known before sighting the data.
Sample
1
2
‚Ä¶
c
Totals
Above
A1
A2
‚Ä¶
Ac
A‚Ä¢  
Below
B1
B2
‚Ä¶
Bc
B‚Ä¢  
Total
n1
n2
‚Ä¶
nc
n‚Ä¢ 
TABLE 1.2. Layout of data for the median test
We assume that all samples are independent random samples, that measurement is at least 
ordinal and that if all populations have the same quantile, then all populations have the same 
probability of an observation exceeding that quantile. Then we may test H: all c populations 
have the same quantile against K: at least two of the populations have different quantiles. 
The probability of the observed table is an extended hypergeometric:
n1CA1
n2CA2... ncCAc / n‚Ä¢CA‚Ä¢ ,  
in which Ai = 0, ... , ni, i = 1, ... , c, A‚Ä¢  = A1 + ... + Ac and n‚Ä¢ = n1 + ... + nc. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
20
A First Perspective on Nonparametric Testing
To test H against K find and sum the probabilities of every table with a value of X2 =  
observed ‚àíexpected
(
)
2 /
all cells
‚àë
 expected at least as large as the observed.
When the quantile chosen is (close to) the median, the distribution of X2  is generally well 
approximated by œác‚àí1
2 . In hand calculations the extended hypergeometric is rarely used.
Corn Example. Four different methods of growing corn were randomly assigned a large 
number of different plots of land, and the yield per acre computed for each plot (Conover, 
1999, p. 173).
Method 1: 83, 91, 94, 89, 89, 96, 91, 92, 90: n1 = 9;  
Method 2: 91, 90, 81, 83, 84, 83, 88, 91, 89, 84: n2 = 10;  
Method 3: 101, 100, 91, 93, 96, 95, 94: n3 = 7;  
Method 4: 78, 82, 81, 77, 79, 81, 80, 81: n4 = 8. 
The median is 89 (three values). In Table 1.3, expected cell values are found using row 
total √ó column total/grand total are given in brackets. We find X2 = 17.54  with a œá3
2 p-value 
of 0.0005. There is very strong evidence that the methods are different.
Method
1
2
3
4
Totals
> 89
6 (4.24)
3 (4.71)
7 (3.29)
0 (3.76)
16
< 89
3 (4.76)
7 (5.29)
0 (3.71)
8 (4.24)
18
Total
9
10
7
8
34
TABLE 1.3. Corn data
Achievement Test Example. An achievement test was given to comparable classes in two 
different schools. The scores were
School 1: 43, 80, 99, 86, 68, 70, 85, 93, 98, 96, 75, 81, 32, 92, 96, 64, 79, 97, 76, 80;
School 2: 76, 65, 73, 95, 77, 99, 55, 35, 72, 83, 70, 65, 86, 60, 62, 90, 71, 65, 89, 71, 
80, 76, 93, 94.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
21
A First Perspective on Nonparametric Testing
21
School
1
2
Totals
< 78
7 (10)
15 (12)
22
> 78
13 (10)
9 (12)
22
Total
20
24
44
TABLE 1.4. Achievement test data
The median is 78, a score not achieved by any student. We find X2 = 3.3 and using the 
2
1œá
approximation results in a p-value of 0.0693. Using the exact distribution,
P(A1 = 7) = 
20C7
24C15 /
44C22 = 0.0482, P(A1 = 6) = 
20C6
24C16/
44C22  = 0.0135, 
P(A1 = 5) = 0.0026, P(A1 = 4) = 0.0003, ... , 
giving P(A1 < 7) = 0.0646. The required p-value is double this, 0.1292. To be sure of the 
meaning of ‚Äòat least as extreme as the observed‚Äô, plot X2  against A1. The 2  approximation 
is poor here because a continuity correction is needed.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

INTRODUCTORY NONPARAMETRICS
22
A First Perspective on Nonparametric Testing
1.5	
THE WILCOXON TESTS
The Wilcoxon tests are the nonparametric equivalents of the t-tests. We consider both one 
and two sample tests of location.
1.5.1	 THE SIGNED RANKS TESTS
Suppose that x1, ... , xn  are observations from a distribution assumed to be symmetric 
and with mean and hence median hypothesised to be m0. In practice symmetry might be 
informally assessed using a histogram, although there are formal tests for symmetry. We 
calculate the xi ‚Äì m0, their absolute values, the ranks of these, and then W‚Äì and W+, the sum 
of the ranks corresponding to the negative and positive xi ‚Äì m0. If there is a tie, the rank 
assigned is the mean of the ranks that would otherwise have been assigned.
A useful check is to note that W+ + W‚Äì = 1 + ... + n = n(n + 1)/2.  
Some books give exact tables of the W+  distribution, but for sample sizes of at least 15 
under the null hypothesis W+ and W‚Äì  are both approximately normal with
mean = 
4
)1
( 
n
n
  and variance =
24
)1
2
)(
1
(


n
n
n
.
Tyre Example. A testing company finds that 16 tyres of a certain make have provided miles 
of service as in Table 1.5. Do the results support the claim that on average this kind of 
tyre provides at least 30,000 miles of service? 
Note that the change of scale in Table 1.5 doesn‚Äôt affect the ranks. We find
W+ = 13.5 + 2 + 7.5 + 11.5 + 3 = 37.5, 
W‚Äì = 5 + 1 + 6 + 11.5 + 10 + 15 + 9 + 13.5 + 16 + 4 + 7.5 = 98.5 and 
W+ + W‚Äì = 37.5 + 98.5 = 136 = 1617/2, verifying the identity W+ + W‚Äì = n(n + 1)/2.
In this example H0: m0 = 30,000 and K: m0 < 30,000. We have 
(
)
(
)(
)
24
1
2
1
4
1
+
+
+
‚àí
=
+
n
n
n
n
n
W
Z
 = 37.5‚àí68
374
 = ‚Äì 1.577. 
This gives a p-value of approximately 0.0574. So at the 0.05 level there is no evidence 
against the null hypothesis of an average of (at least) 30,000 miles of service. However this 
conclusion is marginal. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
23
A First Perspective on Nonparametric Testing
Miles of service (xi )
yi = (xi ‚Äì 30,000)/1,000
ranks of the |yi |
27900
‚Äì2.1
5
35100
5.1
13.5
29800
‚Äì0.2
1
27700
‚Äì2.3
6
26700
‚Äì3.3
11.5
30700
0.7
2
26900
‚Äì3.1
10
32400
2.4
7.5
24800
‚Äì5.2
15
27400
‚Äì2.6
9
24900
‚Äì5.1
13.5
33300
3.3
11.5
31600
1.6
3
24300
‚Äì5.7
16
28300
‚Äì1.7
4
27600
‚Äì2.4
7.5
TABLE 1.5. Type service data
It is interesting to note that of the 16 signs 11 are negative and since P(S > 11|S  is bin
n(16, 0.5)) = 0.105, the one-sided sign test is not significant at the 0.1 level. The t-test has 
p-value 0.062, and since the Shapiro-Wilk test of normality has p-value 0.45, the t-test may 
be validly applied. There is remarkable consistency in the conclusions from these tests, so 
perhaps the message is that the null hypothesis is neither confirmed nor rejected at the usual 
levels of significance, and this suggests a study with a larger sample size could be of value.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
24
A First Perspective on Nonparametric Testing
24
1.5.2	 THE TWO-SAMPLE WILCOXON TEST
Suppose we have a random sample of size m from a population labelled X, and an independent 
random sample of size n from a population labelled Y. We test the null hypothesis that the 
populations are identical against what are called slippage alternatives in the literature. This 
means that the X‚Äôs tend to be smaller/larger/different from the Y‚Äôs. These are sometimes 
equivalent to differences in the medians or the means: E[X] <, >, ‚â† E[Y]. The Wilcoxon 
test in this situation involves a sum of ranks. 
The two samples are combined, ordered and ranked. The sum of the X ranks, WX‚Ää‚Ää‚Ää, and the 
sum of the Y ranks, WY,, are calculated. If there is a tie, each of the tied observations is 
assigned the mean of the ranks they would otherwise have received. The following result 
gives an arithmetic check when calculating the test statistic.
Result: WX + WY = 1 + 2 + ‚Ä¶ + (m + n) = (m + n)(m + n + 1)/2. 
If both m and n are greater than 8 then under the null hypothesis the distribution of WX 
is approximately N(m(m + n + 1)/2, mn(m + n + 1)/12). Similarly under the null hypothesis 
WY  is approximately N(n(m + n + 1)/2, mn(m + n + 1)/12). 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

INTRODUCTORY NONPARAMETRICS
25
A First Perspective on Nonparametric Testing
Flints Example Continued. The combined, ordered and ranked data are as follows:
A
A
A
B
A
B
B
B
B
A ranks
1
2
3
5
WA = 11
B ranks
4
6
7
8
9
WB = 34
TABLE 1.6. Flints data
We have WA = 11, WB = 34.
Check: WA + WB = 45 and (m + n)(m + n + 1)/2 = 910/2 = 45.
We are interested in testing for equality of the A and B distributions. If the alternative is 
that the B ranks tend to be greater than the A ranks, the rejection region is large values of
WB. Since neither m nor n is greater than 8 the normal approximation isn‚Äôt recommended. 
However if it is used, the approximating mean is 25 and variance is 50/3. Applying a 
continuity correction results in a p-value of 
P(WB > 34) = P(Z > (33.5 ‚Äì 25)‚àö(3/50) = 2.0821) = 0.0187. 
Here we won‚Äôt discuss the (exact) small sample distribution of WB‚ÄÜ. However that is available 
through R, which gives a p-value of 0.0159.
Teaching Methods Example. Two groups of students were taught by two different methods. 
We use the Wilcoxon test to see if the methods are equally effective. The combined, ordered 
and ranked scores are as follows.
X
70
72
74
79
80
Y
65
66
68
69
71
73
75
76
78
rank
1
2
3
4
5
6
7
8
9
10
11
12
13
14
X
82
86
91
93
95
Y
84
90
rank
15
16
17
18
19
20
21
TABLE 1.7. Teaching methods data
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
26
A First Perspective on Nonparametric Testing
The sum of the X ranks is 5 + 7 + 9 + 13 + 14 + 15 + 17 + 19 + 20 + 21 = 140, and the sum 
of the Y ranks is 1 + 2 + 3 + 4 + 6 + 8 + 10 + 11 + 12 + 16 + 18 = 91. 
Check: 140 + 91 = 231 = 2122/2.
We calculate




12
1
5.0
2
1






n
m
mn
n
m
m
WX
 = 
3
/
605
110
5.
139

 = 2.07733. 
and 2P(Z > = 2.07733) = 0.0378. Using a two-tailed test the null hypothesis that the two 
methods are equally effective is rejected at the 0.05 level.
The two sample Wilcoxon test is sometimes referred to as the Wilcoxon Mann-Whitney 
Test. At about the same time (the 1940s) as the Wilcoxon test was proposed, so was an 
alternative test, the Mann-Whitney test. Although it is not immediately apparent, the tests 
are equivalent. This is shown in the appendix that can be viewed as challenge material.
Appendix: Wilcoxon Mann-Whitney Tests
As in section 1.5.2 suppose we have a random sample of size m from a population labelled 
X, and an independent random sample of size n from a population labelled Y. To test for 
equality of the X and Y distributions the Wilcoxon test in this situation involves a sum of 
the X (or Y‚ÄÜ‚ÄÜ) ranks. 
As before, suppose the two samples are combined, ordered and ranked. If there is a tie, 
the rank assigned is the mean of the ranks that would otherwise have been assigned. The 
following development assumes there are no ties but can be adjusted to cope with ties. 
For each pair i, j, define hij = 1 if Xi > Yj, 0 otherwise. So hij  counts 1 if Yj  is smaller than 
Xi, 
hij
j
‚àë
 counts the number of Y‚Äôs that are smaller than Xi, and 
hij
j
‚àë
i
‚àë
 = UY  say, 
counts the number of Y‚Äôs that are smaller than the X‚Äôs. Equivalently UY  counts the number 
of X‚Äôs that are bigger than the Y‚Äôs. We may similarly define UX as the number of X‚Äôs that 
are smaller than the Y‚Äôs, or, equivalently, the number of Y‚Äôs that are bigger than the X‚Äôs.
Recall that we have defined the sum of the X ranks to be WX, and the sum of the Y ranks 
to be WY. We now state and prove results relating WX, WY, UX  and UY . 
Result 1: WX + WY = (m + n) (m + n + 1)/2.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
27
A First Perspective on Nonparametric Testing
27
Proof. WX + WY = 1 + 2 + 3 + ... + (m + n) = (m + n) (m + n + 1)/2, using the result for the 
sum of an arithmetic progression.
Result 2. UX + UY = mn.
Proof. Consider any adjacent pair XY in the combined ordered sample. If XY is transposed 
to YX, UX  decreases by 1, while UY  increases by 1. Thus UX + UY  is unaffected 
by such changes. Perform these transpositions until the combined sample becomes  
Y1 Y2 ... YnX1X2 ... Xm. Then UX = 0 and UY = m + m + ... + m (n times) = mn..
Exercise. Verify the result if the transformed sample is X1X2 ... Xm Y1 Y2 ... Yn.
These results are useful to check calculations done by hand. If WX  and WY  are calculated 
independently, then Result 1 should be satisfied. Similarly if UX  and UY  are calculated 
independently, then Result 2 should be satisfied.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

INTRODUCTORY NONPARAMETRICS
28
A First Perspective on Nonparametric Testing
Result 3: WY + UY = mn + n(n + 1)/2.
Proof. Consider transposing XY to YX as in the proof of Result 2. Each such exchange 
reduces WY  by 1 and increases UY  by 1. Thus WY + UY  is unaffected by such changes. 
Perform these transpositions until the combined sample becomes Y1 Y2 ...  YnX1X2 ... Xm.  
Then WY = 1 + 2 + ... + n = n(n + 1)/2  and UY = m + m + ... + m (n  times) = mn. Hence 
the result.
Exercises.	
(i)	
Verify the Result 3 if the transformed sample is X1X2 ... Xm Y1 Y2 ... Yn.
	
	
(ii)	
Using transpositions show that WX + UX = mn + m(m + 1)/2.
	
(iii)	
Using Results 2 and 3 show both algebraically and using transpositions that
WX = UY + m(m + 1)/2 and WY = UX + n(n + 1)/2..
These three results establish that the four statistics WX, WY, UX  and UY  are equivalent in the 
sense that from any one all the others can be calculated. For larger data sets when calculation 
of UX  and UY  can be tedious, we could use the data to calculate WX  and WY  and then
UY = WX ‚Äì m(m + 1)/2 and UX = WY ‚Äì n(n + 1)/2.
Then either UX  or UY  can then be referred to the N(mn/2, mn(m + n + 1)/12)  distribution. 
In addition taking expectations in UY = WX ‚Äì m(m + 1)/2  gives 
E[WX] = E[UY] + m(m + 1)/2 = m(m + n + 1)/2, 
while taking variances gives var(WX) = v varr(UY), so WX  may be referred to the N(m(m  
+ n + 1)/2, mn(m + n + 1)/12)  distribution. Similarly WY  may be referred to the N(n(m  
+ n + 1)/2, mn(m + n + 1)/12)  distribution. Tests based on the normal approximations to 
all of 
(
WX, WY, UX  and UY  will all give the same p-values and conclusions.
Flints Example Continued.
It is routine to show that for these data WA = 11, WB = 34, UB = 1  and UA = 5 + 5 + 5 + 4 
= 19.
Checks:		
(i)	
WA + WB = 45 and (m + n)(m + n + 1)/2 = 9√ó10/2 = 45.
	
	
(ii)	
WA + UA = 30 and mn + m(m + 1)/2  = 20 + 4√ó5/2 = 30..
	
	
(iii)	
WB + UB = 35 and mn + n(n + 1)/2  = 20 + 5√ó6/2 = 35.
	
	
(iv)	
UA + UB = 20 and mn = 20..
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
29
A First Perspective on Nonparametric Testing
Tables of UX  (and UY)) are available in most nonparametric texts. However there is a 
convenient normal approximation, adequate if both m and n are greater than 8. Then both 
UX  and UY  are normal with mean mn/2  and variance mn(m + n + 1)/12. Note we are 
testing for equality of the X and Y means. If the alternative is that larger values of Y are 
expected the rejection region consists of large values of UX.. To analyse the flint data we 
need the (exact) small sample distribution of UX. This is given in some texts, but will not 
be discussed here.
Teaching Methods Example.
Hand calculation of UX  and UY i is tedious, but from previously WX = 140 and WY = 91 and 
incidentally the first result is verified. In addition
UY = WX ‚Äì m(m + 1)/2 = 140 ‚Äì 55 = 85 and UX = WY ‚Äì n(n + 1)/2 = 91 ‚Äì 66 = 25.
Check: UX + UY = 25 + 85 = 110 = 10√ó11.
We calculate Z = (
)
(
) 12
/
1
/
2
/
5.0
+
+
‚àí
‚àí
n
m
mn
mn
UY
 = 2.0773.. Using a two-tailed test and 
the 0.05 significance level, the null hypothesis that the two methods are equally effective is 
rejected (the p-value is 0.0346). This is exactly as in section 1.5.2 using WX. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
30
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
30
2	
NONPARAMETRIC TESTING IN 
THE COMPLETELY RANDOMISED, 
RANDOMISED BLOCKS AND 
BALANCED INCOMPLETE 
BLOCK DESIGNS
Learning Objectives
After successful completion of the material in this chapter the student will be able to
‚Ä¢	 apply the Kruskal-Wallis, Friedman and Durbin tests using both chi-squared and 
F distribution approximations to the appropriate test statistics and 
‚Ä¢	 explain the meaning of component statistics and find linear (Page-type) and 
quadratic (umbrella) components of the Kruskal-Wallis, Friedman and Durbin 
test statistics.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

INTRODUCTORY NONPARAMETRICS
31
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
2.1	
INTRODUCTION AND OUTLINE
Among the elementary statistical designs, probably the most widely used are the completely 
randomised, randomised blocks and balanced incomplete block designs. There exist parametric 
ANOVA F tests for treatment effects for these designs, and these tests are known to be robust: 
the assumptions underpinning the test may not hold but the analysis is hardly affected by 
this. For example a test of the normality of the residuals may be significant at the 0.05 level 
but the p-values found using the F test and a permutation test (to be discussed in the next 
chapter) are often very similar. However if the assumptions for these parametric tests are 
seriously flawed, then the validity of the conclusions will be in doubt. In a similar vein tests 
of equality of variance are known to be very sensitive to the assumption of normality. For 
these tests a small deviation from normality will seriously affect inference. When parametric 
inference is in any way dubious nonparametric tests are required.
In sections 2.2, 2.3 and 2.4 we introduce the Kruskal-Wallis test in the case of the completely 
randomised design, the Friedman test for the randomised block design and the Durbin test 
for the balanced incomplete block design. These nonparametric test statistics have asymptotic 
chi-squared distributions. 
All statistics and analyses in this chapter assume there are no ties. When ties occur the usual 
practice is to give tied data the mean of the ranks they would otherwise have received. This 
introduces an extra element of approximation to the distributions of the test statistics. 
In section 2.5 it is shown that ANOVA F test statistics on the ranks are one to one functions 
of the appropriate nonparametric test statistics. This leads to improved approximations to 
the null distribution of the treatment test statistics.
In section 2.6 orthogonal contrasts are used to decompose the nonparametric test statistics 
into linear, quadratic etc. components. The linear components are the basis of Page or Page-
type tests while the quadratic component is the basis of umbrella tests.
2.2	
THE KRUSKAL-WALLIS TEST
Suppose we have distinct (untied) observations, with yij  being the jth  of ni  observations 
on the ith  of t treatments. The model assumed is the completely randomized design, 
sometimes called the one-way layout and sometimes the one-way analysis of variance. All 
n1 + ‚Ä¶ + nt = n  observations are combined, ordered and ranked. For i = 1, ‚Ä¶, t  the sums 
of the ranks for treatment i, Ri, is calculated. The Kruskal-Wallis test statistic KW is given by
KW =
12
n(n +1)  
Ri
2
ni
‚àí3(n +1)
i=1
t
‚àë
.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
32
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
Under the null hypothesis of no treatment effects the test statistic has the œát‚àí1
2  distribution. 
The alternative hypothesis is sometimes simplistically presented as the treatment medians 
being different. A more general and much more correct presentation of the alternative is that 
the distributions for the different treatments are different. Although the Kruskal-Wallis test is 
well known to be sensitive to location differences, the simplistic presentation above requires 
an additional assumption that the only difference in the distributions is in their medians.
Tomato Example. The data in Table 2.1 are ‚Äòscores‚Äô ‚Äì lengths in millimetres measured 
along a line with one end marked ‚Äòpoor‚Äô and the other ‚Äògood‚Äô. It is assumed there were 24 
independent tasters for each of the four tomato varieties: Floradade, Momotara, Summit 
and Rutgers. The analysis in Rayner and Best (1989, Section 8.2.3) found normality to be 
a marginal assumption.
The one-way ANOVA F test for treatment effects for this data set yields a p-value of 0.4978. 
The null hypothesis of equality of tomato means is accepted at all reasonable significance 
levels, and certainly at the 0.05 level. However when the Shapiro-Wilk test of normality 
is applied to the residuals a p-value of 0.0052 is obtained. The validity of the ANOVA F 
test is questionable, although the test is known to be robust to the normality assumption.
To assess the null hypothesis of no treatment effects using the Kruskal-Wallis test hand 
calculation requires the samples to be combined, ordered and ranked. Here that results 
in rank sums of 1058, 1200, 1303.5 and 1094.5. Of course this is tedious for larger data 
sets such as this, and using R as in Rippon (2016) is to be preferred. Using the formula 
KW = 1.9772  with a œá3
2   p-value of 0.5772. At the 0.05 level the null hypothesis that the 
tomato distributions are similar is accepted. Note that this value of the Kruskal-Wallis test 
statistic ignores the fact that there are a few tied values in the data. Here an adjustment 
for ties makes little difference to the value of the test statistic.
Tomato variety
Flavour scores
Floradade
43, 5, 74, 64, 10, 16, 75, 20, 36, 76, 60, 57, 55, 29, 82, 91, 66, 27, 72, 108, 
84, 50, 82, 39
Momotara
74, 112, 64, 101, 105, 12, 33, 90, 129, 37, 50, 44, 18, 24, 48, 62, 88, 50, 
73, 119, 109, 50, 12, 37
Summit
109, 25, 48, 91, 52, 35, 42, 100, 22, 122, 105, 119, 29, 26, 102, 48, 108, 
53, 57, 82, 105, 108, 13, 74
Rutgers
39, 82, 100, 62, 126, 26, 24, 35, 74, 19, 113, 56, 61, 21, 6, 13, 118, 91, 60, 
88, 15, 32, 134, 29
TABLE 2.1. Tomato data
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
33
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
33
2.3	
THE FRIEDMAN TEST
For the randomised block design suppose we have distinct (untied) observations,  yij,  this 
being the ith  of t treatments on the jth  of b blocks. The observations are ranked within 
each block and Ri‚ÄÜ, the sum of the ranks for treatment i over all blocks is calculated for 
i = 1, ‚Ä¶, t. The Friedman test statistic is
S = 
12
bt(t +1)  
Ri
2 ‚àí3b(t +1)
i=1
t
‚àë
.
Under the null hypothesis of no treatment effects the test statistic has the œát‚àí1
2  distribution. 
Note that here ranking is within blocks; for the Kruskal-Wallis test overall ranking is used. As 
with the Kruskal-Wallis test the Friedman is not testing for equality of means (or medians). 
That would only be the case if initially it could be assumed that the distributions sampled 
differed only in their means (or medians). If that is not the case, testing assesses whether 
or not the treatment distributions are similar. However it is well known that the Friedman 
test is sensitive to location differences.
Lemonade Example. Five lemonades with increasing sugar content are ranked by each of ten 
judges. They were not permitted to give tied outcomes. The results are in Table 2.2 below. 
We wish to assess what, if any, differences there are between the lemonades. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

INTRODUCTORY NONPARAMETRICS
34
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
Routine calculation finds S = 9.04. The œá 4
2  p-value is 0.0601. There is weak evidence, at 
the 0.10 level, of a difference in lemonades. While there is no evidence of a difference at 
the 0.05 level, further investigation would seem to be warranted. We will return to this 
data set later in this and subsequent chapters.
Judge
Product 
Product
1
2
3
4
5
6
7
8
9
10
mean
1
5
3
4
5
3
4
5
3
1
3
3.6
2
2
5
3
2
5
3
2
5
4
1
3.2
3
1
2
2
1
2
2
1
2
2
2
1.7
4
3
1
5
3
1
5
3
1
5
4
3.1
5
4
4
1
4
4
1
4
4
3
5
3.4
TABLE 2.2. Lemonades ranked by ten judges
2.4	
THE DURBIN TEST
In the balanced incomplete block design each of the b blocks contains k experimental units, 
each of the t treatments appears in r blocks, and every treatment appears with every other 
treatment precisely   times. Necessarily 
k < t, r < b, bk = rt, and (t ‚Äì 1) = r(k ‚Äì 1).
Treatments are ranked on each block and Durbin‚Äôs statistic, D, is given by
)
1
(
)
1
)(
1
(
3
)
1
(
)
1
(
12
1
2
2









k
k
t
r
R
k
bk
t
D
t
i
i
 
in which Ri is the sum of the ranks given to treatment i, i = 1, ‚Ä¶, t. Without further 
assumptions the test statistic D tests the null hypothesis of equality of the treatment 
distributions. The asymptotic distribution of D is œát‚àí1
2 . If k = t  then D = S: the design is 
no longer incomplete; it is, in effect, complete.
Ice Cream Example. Conover (1999, p. 390) gave an example of a taste test involving seven 
ice cream varieties, coded S, U, V, W, X, Y and Z, and presented three at a time. Table 2.3 
shows, for each judge, the rank given for each variety.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
35
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
For this design we see that b = t = 7, k = r = 3 and  = 1 . We calculate that  
{R1, R2, R3, R4, R5, R6, R7} = {8, 9, 4, 3, 5, 6, 7} and D = 12. The œá6
2  p-value is 0.0620. 
At the 0.05 level there is no evidence of a difference in the distributions of the ice creams; 
however there is weak evidence, at the 0.10 level, of a difference. It is known that the œát‚àí1
2
approximation to the distribution of D is not particularly accurate, so it would be wise to 
pursue a better approximation in a marginal case like this. See Section 2.5.
Variety
Judge
S
U
V
W
X
Y
Z
1
2
3
1
2
3
1
2
3
2
1
3
4
1
2
3
5
3
1
2
6
3
1
2
7
3
1
2
TABLE 2.3. Ranks of seven judges of seven ice cream varieties
In section 2.6 D will be decomposed into orthogonal contrasts that will show that for these 
data there is evidence of what will be called an umbrella effect. Since D is associated with 
six degrees of freedom, the Durbin test is attempting to detect a quite complex alternative: 
the parameter space is six dimensional. The orthogonal contrast tests are each associated 
with a single degree of freedom, and are seeking to detect simpler effects: the parameter 
spaces are each one dimensional.
2.5	
RELATIONSHIPS OF KRUSKAL-WALLIS, FRIEDMAN AND 
DURBIN TESTS WITH ANOVA F TESTS
In this section we look at the test statistics KW, S and D and their relationships with ANOVA 
F test statistics for their design. These relationships lead to improved approximations to the 
sampling distributions of these test statistics.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
36
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
36
2.5.1	 THE COMPLETELY RANDOMISED DESIGN
Suppose, as in section 2.2, we have distinct observations, yij,  in the completely randomized 
design. Write Ti‚Ä¢ =
yij
j
‚àë
  and 
‚àë
‚àë
=
=
‚Ä¢
‚Ä¢
‚Ä¢
j
i
ij
i i
y
T
T
,
. The ANOVA F test statistic F for this 
design is given by
)
/(
)
1
/(
1
1
2
1
2
1
2
2
t
n
n
T
Y
t
n
T
n
T
F
t
i
t
i
i
i
n
j
ij
t
i
i
i
i
‚àí
‚é≠
‚é¨
‚é´
‚é©
‚é®
‚éß
‚àí
‚àí
‚é≠‚é¨
‚é´
‚é©‚é®
‚éß
‚àí
=
‚àë
‚àë
‚àë
‚àë
=
=
‚Ä¢
=
=
‚Ä¢
‚Ä¢
‚Ä¢
.
If the observations are the ranks then Ti‚Ä¢ = Ri  and if also there are no ties  
then T‚Ä¢‚Ä¢ = 1 + ‚Ä¶ + n = n(n + 1)/2  and 
Yij
2
i, j
‚àë
 = 12 + ‚Ä¶ + n2 = n(n + 1)(2n + 1)/6. 
Moreover, reorganising 
KW =
12
n(n +1)  
Ri
2
ni
‚àí3(n +1)
i=1
t
‚àë
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

INTRODUCTORY NONPARAMETRICS
37
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
gives ‚àëi
i
i
n
R /
2
 = {KW + 3(n + 1)}n(n + 1)/12. Substituting into F gives
{
}
{
}
‚é≠‚é¨‚é´
‚é©‚é®‚éß
‚àí
‚àí
‚é™‚é™
‚é≠
‚é™‚é™
‚é¨
‚é´
‚é™‚é™
‚é©
‚é™‚é™
‚é®
‚éß
+
+
+
‚àí
+
+
+
‚àí
+
+
+
=
)
1
(
)
(
12
)
1
(
)
1
(
3
6
)
1
2
)(
1
(
4
)
1
(
12
)
1
(
)
1
(
3
2
t
t
n
n
n
n
KW
n
n
n
n
n
n
n
n
KW
F
 
= 
KW(n ‚àít)
(n ‚àí1‚àíKW)(t ‚àí1)
 
after simplification. Note that if there are ties this relationship doesn‚Äôt hold. The usual 
procedure for a group of tied observations is to assign to each the mean of the ranks 
they would otherwise be given. Then 
Ri‚Ä¢
i
‚àë
 = n(n + 1)/2  as before, but no longer is 
Yij
2
i, j
‚àë
 = 12 + ‚Ä¶ + n2 = n(n + 1)(2n + 1)/6. It is not possible to treat ties in such a way 
that both the sum of the ranks and the sum of the squares of the ranks are constants known 
before sighting the data.
While not particularly good, the asymptotic chi-squared approximation to the distribution 
of KW is generally superior to the Ft‚Äì1,n‚Äìt  approximation to the distribution of F. However a 
simple adjustment retrieves the situation. If F is referred to the F distribution with  d(t ‚Äì 1) 
and d(n ‚Äì t)  degrees of freedom where 
d = 1 ‚Äì 6(n + 1)/{(n ‚Äì 1)(5n + 6)},
then this approximation is generally superior to the chi-squared approximation to the 
distribution of KW. Of course these degrees of freedom are no longer integers, but this can 
be easily handled by most modern computer packages. See, for example, Spurrier (2003).
Tomato Example. In section 2.2 we found that for the tomato data the Kruskal-Wallis test 
statistic takes the value 1.9772 with a œá3
2  p-value of 0.5772. Using R gives a p-value of 
0.577, using its own ties correction. Using the relationship above, the F test statistic F 
takes the value 0.6518. The F3,92  p-value is 0.5838; using the improved approximation the 
F2.9622,90.8403  p-value is 0.5820. For these data, the p-values are very similar. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
38
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
2.5.2	 THE RANDOMISED BLOCK DESIGN
Suppose, as in section 2.3, that in the completely randomized design we have distinct 
observations, yij.  The observations are ranked within each block and Ri,  the sum of the 
ranks for treatment i over all blocks is calculated for i = 1, ‚Ä¶, t.  The Friedman test statistic is
S = 
12
bt(t +1)  
Ri
2 ‚àí3b(t +1)
i=1
t
‚àë
.
The notation is as before, but additionally T‚Ä¢ j =
yij
i
‚àë
. The ANOVA F test statistic F for 
this design is given by
)
1
)(
1
/(
)1
/(
1
1
1
2
2
2
1
2
1
2
2
‚àí
‚àí
‚é≠
‚é¨
‚é´
‚é©
‚é®
‚éß
+
‚àí
‚àí
‚àí
‚é≠‚é¨
‚é´
‚é©‚é®
‚éß
‚àí
=
‚àë
‚àë
‚àë
‚àë
‚àë
=
=
=
‚Ä¢‚Ä¢
‚Ä¢
‚Ä¢
=
=
‚Ä¢‚Ä¢
‚Ä¢
t
b
bt
T
t
T
b
T
Y
t
bt
T
b
T
F
t
i
t
i
b
j
j
i
b
j
ij
t
i
i
.
If the observations are ranks and if there are no ties then
Ti‚Ä¢ = Ri , T‚Ä¢ j  = 1 + ‚Ä¶ + t = t(t + 1)/2, T‚Ä¢‚Ä¢ = bt(t + 1)/2 and 
Yij
2
i, j
‚àë
 = b(12 + ‚Ä¶ + t2) = bt(t + 1)(2t + 1)/6. 
Moreover, reorganising the equation for S gives ‚àëi
iR2  = bt(t + 1){S + 3b(t + 1)}/12 . 
Substituting and simplifying gives
F =
S(b ‚àí1)
b(t ‚àí1)‚àíS
. 
A slightly less algebraic approach is to note that with no ties every rank appears on each 
block, so there is no block effect, and the block sum of squares is zero. The treatment mean 
square is as in the numerator of the first equation for F above and the error mean square 
is the total sum of squares minus the treatment sum of squares.
It cannot be assumed that the ranks are normally distributed, which is one of the assumptions 
needed for F to have an F distribution. Thus only approximately does the ANOVA F statistic 
have the Ft‚Äì1,(b‚Äì1)(t‚Äì1)  distribution.
Lemonade Example. In the Lemonade example in section 2.3 we found S = 9.04  with a 
œá 4
2   p-value of 0.0601. The F test statistic using the formula above takes the value 2.6279 
with F4,36  p-value 0.0504.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
39
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
39
2.5.3	 THE BALANCED INCOMPLETE BLOCK DESIGN
Suppose, as in section 2.4 above, that in the balanced incomplete block design we have 
distinct observations, yij.  The observations are ranked within each block and Ri, the sum 
of the ranks for treatment i over all blocks is calculated for i = 1, ‚Ä¶, n. The Durbin test 
statistic is
D = 12(t ‚àí1)
rt(k2 ‚àí1)  
{Ri ‚àír(k +1)
2
}2
i=1
t
‚àë
.
Aside. There are different possible ANOVA analyses for balanced incomplete block designs. 
See, for example, Kuehl (2000, section 8.5); note that a balanced incomplete block design 
can be regarded as a block design with missing data. Such designs are not orthogonal and 
so it is possible to calculate treatment sums of squares both adjusted and not adjusted for 
blocks. We use the former.
Here we assume there are no ties and so the ranks assigned on each block are 1, 2, ‚Ä¶, k.. 
There are no block effects, and the block sum of squares, no matter how it is calculated, 
is zero. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

INTRODUCTORY NONPARAMETRICS
40
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
We now show that the ANOVA F test statistic F when the data are the untied ranks is 
related to D by
F =
D
{b(k ‚àí1)‚àíD}
(bk ‚àíb ‚àít +1)
(t ‚àí1)
.
The total sum of squares is SSTot =  
yij
2
i, j
‚àë
‚àíy‚Ä¢‚Ä¢
2 / (bk). As above, the ranks assigned on each 
block are 1, 2, ‚Ä¶, k,  so 
 y‚Ä¢‚Ä¢ = b(1 + ‚Ä¶ + k) = bk(k + 1)/2 and 
yij
2
i, j
‚àë
 = b(12 + ‚Ä¶ + k2) = bk(k + 1)(2k + 1)/6 
giving SSTot = bk(k + 1)(k ‚Äì 1)/12  after simplifying.
Moreover the treatment sum of squares, no matter how it is calculated, is 
SSTreat = k
Œªt  
{yi‚Ä¢ ‚àí
i=1
t
‚àë
 block average across blocks that contain treatment i}2.
Here the 
‚Ä¢iy   are rank sums. For any given i, there are r blocks containing treatment i and 
each block total is k(k + 1)/2, so the block average across blocks that contain treatment i 
is  r(k + 1)/2. It follows that 
SSTreat = k
Œªt  
{Ri ‚àír(k +1)
2
}2
i=1
t
‚àë
 = k(k +1)
12
 D.
This uses ‚àë
+
‚àí
i
i
k
r
R
2}
2
/)1
(
{
 = rt(k2 ‚Äì 1)/{12(t ‚Äì 1)}  from the equation for D and both 
bk = rt, and (t ‚Äì 1) = r(k ‚Äì 1).
The error sum of squares is 
SSError = SSTot ‚Äì SSTreat = bk(k + 1)(k ‚Äì 1)/12 ‚Äì k(k + 1)D/12 
= k(k + 1){b(k ‚Äì 1) ‚Äì D}/12 
on simplifying. Substitution gives the relationship between F and D given above.
The randomised block design is recovered if blocks are ‚Äòcomplete‚Äô; that is, k = t and r = b.
As the observations here are ranks and not normally distributed, only approximately does 
the ANOVA F statistic have the distribution Ft‚Äì1,bk‚Äìb‚Äìt+1.
As with the randomised block design the approximate F distribution of F generally improves 
on the asymptotic 2  distribution of D.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
41
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
Ice Cream Example. In section 2.4 we found the Durbin test statistic took the value 12 with 
œá6
2  p-value 0.0620. Using the formula above the F test statistic takes the value 8 with F6,8 
p-value 0.0049. From not quite significant at the 0.05 level using the œá6
2  approximation 
the F approximation results in significance at the 0.01 level. Which is more precise? We 
will return to this question in the next chapter.
2.6	
ORTHOGONAL CONTRASTS: PAGE AND UMBRELLA TESTS
We now show how to decompose the test statistics KW, S and D into component statistics 
called orthogonal contrasts. These contrasts give focused tests for particular aspects of the 
hypotheses under consideration. They assume there is a meaningful ordering of the treatments 
being compared. The first component is usually described as a test for linear trend, while 
the second is usually described as a test for a quadratic effect. The effects assess whether, as 
the ordered treatments pass from first to last, polynomial effects of order one, two etc. are 
observed in the responses. The component tests typically are associated with a single degree 
of freedom, and seek alternatives in a one dimensional parameter space. Test statistics such 
as KW, S and D are more omnibus, seeking alternatives in a higher dimensional (typically 
t ‚Äì 1) parameter space. The focused tests have higher power than the corresponding omnibus 
tests when the alternative falls within their one dimensional parameter space. But outside 
that space they are totally insensitive, with power close to the test size.
Aside. The test size is the probability of rejecting the null hypothesis when the null hypothesis 
is true. It is usually close to the nominated significance level.
The following treatment requires linear algebra that many readers will not have met yet. 
These readers should focus on the results rather than the mathematical details.
We begin by showing that KW, S and D may be represented in a similar way. Tedious but 
routine algebra shows that the Kruskal-Wallis KW statistic is given by 
 = 
‚àë
=
+
‚àí
+
t
i
i
i
i
n
n
n
R
n
n
1
2}
2
1
{
)1
(
12
 
 
= 
  
KW =
12
n(n +1)  
Ri
2
ni
‚àí3(n +1)
i=1
t
‚àë
(Yi ‚àíE[Yi])2
i=1
t
‚àë
/ var(Yi)
in which Yi = Ri,  the sum of the ranks for treatment i, E[Yi] = (n + 1)ni/2 and var(Yi) = 
ni n(n + 1)/12. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
42
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
42
Similarly it may be shown that the Friedman test statistic S is given by
S = 
 = 
 
= 
  
12
bt(t +1)  
Ri
2 ‚àí3b(t +1)
i=1
t
‚àë
12
bt(t +1)  
{Ri ‚àíb(t +1)
2
}2
i=1
t
‚àë
(Yi ‚àíE[Yi])2
i=1
t
‚àë
/ var(Yi)
in which Yi = Ri, E[Yi] = b(t + 1)/2 and var(Yi) = bt(t + 1)/12.. Note that this uses 
Ri
i
‚àë
  
= b(1 + ‚Ä¶ + t) = bt(t + 1)/2.
Finally the Durbin test statistic D is given by
D = 
 = 
= 
  
12(t ‚àí1)
rt(k2 ‚àí1)  
Ri
2 ‚àí3r(t ‚àí1)(k +1)
(k ‚àí1)
i=1
t
‚àë
12(t ‚àí1)
rt(k2 ‚àí1)  
{Ri ‚àír(k +1)
2
}2
i=1
t
‚àë
(Yi ‚àíE[Yi])2
i=1
t
‚àë
/ var(Yi)
in which Yi = Ri, E[Ri] = r(k + 1)/2  and var(Yi) = rt(k2 ‚Äì 1)/{12(t ‚Äì 1)}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master‚Äôs Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master‚Äôs programmes
‚Ä¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
‚Ä¢ 1st place: MSc International Business
‚Ä¢ 1st place: MSc Financial Economics
‚Ä¢ 2nd place: MSc Management of Learning
‚Ä¢ 2nd place: MSc Economics
‚Ä¢ 2nd place: MSc Econometrics and Operations Research
‚Ä¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‚ÄòBeste Studies‚Äô ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

INTRODUCTORY NONPARAMETRICS
43
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
Thus if Vi = (Yi ‚Äì E[Yi])/
 and V = (Vi) then with T = KW, S
var(Yi)
S or D,
T = 
Vi
2
i=1
t
‚àë
 = VTV. 
It is well known that in all three cases asymptotically T has the œát‚àí1
2  distribution. To give 
this a little substance note that for each i, since Yi  is a rank sum, by the central limit 
theorem it is asymptotically normal. Thus V is multivariate normal. Clearly E[V] = 0 and 
var r(Vi) = 1. However there are linear constraints on the Vi: Vt = 0  for T = S and D, while 
for KW, ‚àë=
+
‚àí
t
i
i
i
i
i
n
n
n
R
n
1
}
2
/)1
(
/
{
 = 0. Thus the Vi  are correlated and  = cov(V)  
is of rank at most t ‚Äì 1. 
Aside. Write  = cov(V).. Since Œ£ is positive semi-definite there exists a matrix  0.5  such 
that 0.5 0.5 = . Define Z by V = 0.5  Z in which the elements Zi  of Z are asymptotically 
independent and standard normal, written IN(0, 1). Then T = VTV = ZTZ. By a well-known 
theorem on quadratic forms ZTZ  asymptotically has the œát‚àí1
2  distribution if and only if   
is idempotent of rank t ‚Äì 1. We will not pursue that further here. 
Suppose now that 1t denotes the t √ó 1 column of units as opposed to It that denotes 
a t √ó t identity matrix. Suppose that H is an orthogonal t √ó t matrix with last row  
t
t /
1T
 and Z = HV. Then 
T = VTV = VTHTHV = ZTZ since HTH = It. 
Thus T = Z1
2 + ‚Ä¶  + Zt
2. If the ith  row of H is denoted by hi
T,  the Zi = hi
TV are our so-
called orthogonal contrasts; orthogonal because the rows of H are orthogonal and contrasts 
because the choice of last row means the elements of every other row sum to zero, using 
the orthogonality hi
T1t = 0. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
44
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
The rows of H may be based on the orthonormal polynomials. The completely randomised 
design with equal treatment numbers, the randomised block and the balanced incomplete 
block designs are all balanced in the sense that the ni  are all equal. For unbalanced designs 
the orthonormal polynomials will vary with {ni}, but for balanced designs the orthonormal 
polynomials can be given explicitly. Thus the order one polynomial and the first row of 
H will consist of 1, 2, ‚Ä¶, t with the mean (t + 1)/2  subtracted from each element and 
then each of the resulting numbers divided by the square root of the sum of their squares, 
which can readily be shown to be t(t2 ‚Äì 1)/12. Thus in Table 2.4 corresponding to the linear 
coefficients for t = 5 we start with 1, 2, 3, 4 and 5, subtract off their mean, 3, giving ‚Äì 
2, ‚Äì1, 0, 1, 2 and then divide by the square root of their sum of squares, ‚àö10. This gives 
the row ‚Äì 2/10, ‚Äì 1/10, 0, 1/10, 2/10. Similarly for the quadratic coefficients start with 
12, 22, ‚Ä¶, t2, subtract their mean and divide by the square root of their sum of squares. For 
t = 3, 4, ‚Ä¶, 7  the elements of h1 and h2, that give the linear and quadratic contrasts, are 
given in Table 2.4.
For the randomised block design the test statistic Z1 is the Page test statistic, while for the 
completely randomised and balanced incomplete blocks designs Z1 could be called Page-type 
test statistics. For the randomised block and balanced incomplete block designs Z1  is of the 
form 
h1i{Ri ‚àíŒº} /œÉ
i
‚àë
 = 
h1iRi /œÉ
i
‚àë
 since 
h1i
i
‚àë
 = 0  using the orthogonality of the first and 
last rows of H. Thus the Page and Page-type test statistics are specifically,
‚Ä¢	
h1i{Ri ‚àí(n +1)ni / 2} /
n(n +1)ni /12
i
‚àë
 for the completely randomised design
‚Ä¢	
g
h1iRi /
bt(t +1) /12
i
‚àë
 for the randomised block design
‚Ä¢	 ‚àë
‚àí
‚àí
i
i
i
t
k
bk
R
h
)]
1
(
12
/[
)1
(
/
2
1
 for the balanced incomplete block design.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
45
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
45
a) Linear Coefficients
t 
h11, h12, ‚Ä¶, h1t 
3 
‚Äì 1/2, 0, 1/2, 
4 
‚Äì 3/20, ‚Äì 1/20, 1/20, 3/20 
5 
‚Äì 2/10, ‚Äì 1/10, 0, 1/10, 2/10 
6 
‚Äì 5/70, ‚Äì 3/70, ‚Äì 1/70, 1/70, 3/70, 5/70 
7 
‚Äì 3/28, ‚Äì 2/28, ‚Äì 1/28, 0, 1/28, 2/28, 3/28 
b) Quadratic Coefficients
t 
h21, h22, ‚Ä¶, h2t 
3 
1/6, ‚Äì 2/6, 1/6,  
4 
1/4, ‚Äì1/4, ‚Äì 1/4, 1/4,  
5 
2/14, ‚Äì1/14, ‚Äì 2/14, ‚Äì 1/14, 2/14,  
6 
5/84, ‚Äì 1/84, ‚Äì 4/84, ‚Äì 4/84, ‚Äì 1/84, 5/84 
7 
5/84, 0, ‚Äì 3/84, ‚Äì 4/84, ‚Äì 3/84, 0, 5/84 
TABLE 2.4. Linear and Quadratic Coefficients for balanced designs
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

INTRODUCTORY NONPARAMETRICS
46
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
The test statistics Z2 are umbrella test statistics. Obviously they take the same form as the 
Z1  statistics, but with h1i  replaced by h2i. 
Although it is possible to calculate and base inference upon all of the Zi it is more usual 
to aggregate Z3, ‚Ä¶, Zt  into a residual, Z3
2 +...+ Zt
2  = T ‚àíZ1
2 ‚àíZ2
2. 
The asymptotic distributions of the contrasts are standard normal. This follows routinely 
because the Ri  are rank sums and being sums, are asymptotically normal by the central 
limit theorem. They are clearly standardised, and so each has mean zero and variance one. 
Thus for all three designs both Z1  and Z2  are asymptotically standard normally distributed. 
For the randomised block and balanced incomplete blocks designs Zt = 0. Note that for 
T = S  or D, Zt  is proportional to 
(Ri ‚àíE[Ri]
i
‚àë
) which is zero because 
Ri
i
‚àë
  is the sum 
of all of the ranks assigned, namely bt(t + 1)/2  for randomised block design, and this is 
just t 
E[Ri]
i
‚àë
. The reasoning for balanced incomplete blocks is similar. The same argument 
does not apply to the completely randomised design because of the factors 
in . 
The discussion above does not prove that asymptotically T has the œát‚àí1
2  distribution, nor 
that the orthogonal contrasts are asymptotically independent. To prove these results requires 
relatively advanced distribution theory and linear algebra that we will not pursue here. 
Tomato Example. For the balanced completely randomised design if ni = m  for all i then 
‚Ä¢	 Z1 = 
‚àë
+
i
i
iR
h
m
n
n
1
 }
)
1
(
/{
12
. 
The rank sums for the four treatments are 1058, 1200, 1303.5 and 1094.5. For these data 
the Page-type test statistic takes the value ‚Äì 0.3490 with a two-sided p-value using the 
normal distribution of 0.7271. The umbrella test statistic Z2  takes the value ‚Äì 1.2860 with 
two-sided p-value 0.1984. There is no statistical evidence of linear or quadratic effects at 
all reasonable levels.
Again, no adjustment for ties has been made as it makes little difference, when, as here, 
there are few ties.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
47
NONPARAMETRIC TESTING IN THE COMPLETELY  
RANDOMISED, RANDOMISED BLOCKS AND  
BALANCED INCOMPLETE BLOCK DESIGNS
Lemonade Example. For this example using the randomised block design the Page test statistic 
takes the value ‚Äì 0.3162 with a two-sided p-value using the normal distribution of 0.7518. 
The umbrella test statistic takes the value 2.2984 with a two-sided p-value using the normal 
distribution of 0.0215. There is evidence of an umbrella effect at the 0.05 level but no 
evidence of a linear trend at the same level. The rank sums for treatments 1 to 5 are 36, 
32, 17, 31 and 34. A by-eye inspection of the data supports the conclusions: there is no 
evidence of linear trend but the rank sums clearly decrease then increase ‚Äì an umbrella effect.
Ice Cream Example. For these data using the balanced incomplete block design the Page-type 
test statistic takes the value ‚Äì 0.9897 with a two-sided p-value using the normal distribution 
of 0.3223. The umbrella test statistic takes the value 2.5714 with a two-sided p-value using 
the normal distribution of 0.0101. There is evidence of an umbrella effect at the 0.05 level 
but no evidence of a linear trend at the 0.05 level.
The rank sums for treatments 1 to 7 are 8, 9, 4, 3, 5, 6 and 7. Again a by-eye inspection 
of these rank sums supports the conclusions: there is no evidence of linear trend but the 
rank sums tend to decrease then increase ‚Äì an umbrella effect.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
48
Permutation Testing
48
3	
PERMUTATION TESTING
Learning Objectives
After successful completion of the material in this chapter the student will be able to
‚Ä¢	 explain to peers the concept of permutation testing of statistical hypotheses
‚Ä¢	 implement permutation tests for designs such as the completely randomised, 
randomised block and balanced incomplete block designs.
3.1	
WHAT IS PERMUTATION TESTING AND WHY IT IS IMPORTANT?
In Chapters 1 and 2 we usually found approximate p-values using the asymptotic distributions 
of the test statistics. Only for some test statistics and for small samples is it possible to find 
exact p-values. However it is possible to find almost exact p-values using permutation tests, 
based on random samples of permutations of the data. To illustrate how to carry out these 
permutation tests below we consider herbicide data from Higgins (2004, pp. 38‚Äì39). Here 
and elsewhere we use our own R code for permutation tests; we will use the almost exact 
p-values based on a random sample of all possible permutations. Random samples are used 
because for larger data sets exact p-values may take too long to calculate.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

INTRODUCTORY NONPARAMETRICS
49
Permutation Testing
Herbicide Example. A study was conducted to assess the damage to strawberry plants caused 
by a particular type of herbicide used for controlling weeds. The dry weight of nine plants 
treated with the herbicide was compared with the dry weights of seven untreated plants. It 
is expected that the untreated plants will have larger dry weights than treated plants. Data 
and ranks are given in Table 3.1. We use the test statistic the rank sum of the untreated 
plants. Since the ranks of the untreated plants are expected to be larger than those of the 
treated plants, an upper-tailed test is used. 
Rank
1
2
3
4
5
6
7
8
9
Untreated plants
0.55
0.63
Treated plants
0.44
0.47
0.51
0.52
0.58
0.59
0.60
Rank
10
11
12
13
14
15
16
Untreated plants
0.67
0.68
0.79
0.81
0.85
Treated plants
0.65
0.66
TABLE 3.1. Dry weights in kg of strawberry plants
The raw data and the combined, ordered and ranked data are in the Table 3.1. The rank 
sum of the untreated plants is 5 + 9 + 12 + 13 + 14 + 15 + 16 = 84; the rank sum of the 
treated plants is 1 + 2 + 3 + 4 + 6 + 7 + 8 + 10 + 11 = 52.
If there is no difference between the treated and untreated strawberry plants then all data 
sets obtained by randomly assigning seven of the 16 ranks to the untreated plants and 
nine to the treated plants would have an equal chance of being observed. In this case the 
average of the ranks for the treated and untreated plants would be similar. If the untreated 
weights were higher though, then the ranks of these weights should be greater, as rank one 
is assigned to the smallest weight and rank 16 to the largest. We will carry out a Wilcoxon 
test with almost exact p-value by seeing if among 10,000 random permutations of the 16 
ranks the rank sum of the untreated weights, namely 84, is one of the largest rank sums. 
Our p-value will be the proportion of the untreated rank sums greater than or equal to 84.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
50
Permutation Testing
We now give some R code to obtain 10,000 random permutations and so 10,000 rank 
sums for seven untreated ranks. Note that in R to get a random permutation we use the 
sample command. The other R commands should be obvious. If the R commands are 
typed into a text editor such as notepad in Windows then they may be copied and pasted 
into the R console window and pressing enter will give the p-value. Possible R code is
d<-c (.55,.67,.63,.79,.81,.85,.68,.65,.59,.44,.6,.47,.58, .66,.52,.51)
r<-rank(d)
nperm<-10000
teststat.obs<-sum(r[c(1:7)]) 
print("test statistic")
teststat.obs 
teststat<-rep(NA,nperm) 
for (i in 1:nperm){
rankperm=sample(r)
rp=rankperm[c(1:7)]
teststat[i]=sum(rp)
}
print("p-value")
sum(teststat>=teststat.obs)/nperm
Press enter after pasting this code into the R console window and get a p-value similar to 
0.0034. This is close to the exact p-value of 0.0039 found using the R routine wilcox.
test. Repeated running of this code will produce a cluster of p-values around 0.0039. 
The number of permutations used is nperm in the R code. If that is changed to 100,000 a 
value similar to 0.004 is obtained almost instantly. A wait of a few seconds is required with 
nperm set to 1,000,000, which returns a value similar to 0.00392. Experiment by running 
the code with different values of nperm.
Note. Alternative R code and a fuller discussion is given in Rippon (2016).
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
51
Permutation Testing
51
The extension of permutation tests for comparing two treatments to studies with k  treatments 
follows in similar fashion. Suppose the number of observations or ranks in the ith treatment 
group is ni  with the total number of observations being 
ni
i=1
k
‚àë
 = n. Form a vector of size 
n containing all the observations and generate random permutations of the elements of this 
vector. For each of these random permutations take the first n1  elements to belong to the 
first treatment group, the second n2  elements to belong to the second treatment group and 
so on, with the last nk  elements belonging to the kth treatment group. For each random 
permutation of the observations calculate the Kruskal-Wallis test statistic and see what 
proportion of these statistics are greater than or equal to the Kruskal-Wallis test statistic 
for the original observations. This is the ‚Äòalmost exact‚Äô p-value. An exact p-value requires 
all possible n!/(n1! n2! ‚Ä¶ nk!)  permutations to be considered but for larger n this becomes 
time consuming. In general it is preferable to calculate almost exact p-values with user 
defined number of permutations. Clearly increasing the number of permutations improves 
the precision of the p-value estimated while increasing the time taken to calculate it.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet‚Äôs 
electricity needs. Already today, SKF‚Äôs innovative know-
how is crucial to running a large proportion of the 
world‚Äôs wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

INTRODUCTORY NONPARAMETRICS
52
Permutation Testing
The permutation test for the Friedman randomised blocks test for the comparison of t 
treatments is calculated in a slightly different manner. There are t ranks in each of the b 
blocks. For each of the b blocks random permutations of the ranks 1,2, ‚Ä¶, t are generated. 
The Friedman statistic is calculated for each such set of n = bt  random permutations 
and the proportion of these statistics greater than or equal to the Friedman statistic for 
the original rankings is found. This is the almost exact p-value. An exact p-value requires 
all (t!)b  permutations to be considered, but again this is too time consuming for a large 
number of rankings. 
We will return to permutation tests in Section 3.3. In Section 3.2 we consider a new topic, 
nonparametric ANOVA. This appears to be very similar to parametric ANOVA but is based 
on very weak assumptions rather than assuming normality of the residuals. P-values may 
be found using F tests, but these are only approximations, albeit very good approximations 
in general. As the true distribution of the test statistics is not known permutation tests are 
needed to find almost exact p-values. These are found and presented in the final section, 
where previous p-values and permutation test p-values are collected and compared. 
3.2	
NONPARAMETRIC MULTIFACTOR ANOVA WHEN THE LEVELS 
OF THE FACTORS ARE UNORDERED 
Nonparametric ANOVA is a technique applicable to multifactor ANOVA. In this section 
we will apply the technique to designs in which the levels of all factors are unordered or 
any ordering is ignored. 
Only a brief sketch of the theory underpinning the technique will be given here. For 
more details see Rayner and Best (2013). A model for a multifactor ANOVA may be 
constructed using product multinomial distributions. These models essentially label cells in 
a relevant table rather than impose strong assumptions that may not be true, such as the 
error distributions all being normally distributed. Then a 1-1 transformation is given that 
transforms the multinomial cell probabilities to ANOVA-like parameters. We may then test 
if each ANOVA-like parameter is zero against the negation of this. The usual ANOVA F 
test statistics can be shown to be appropriate test statistics. Because the residuals are not 
assumed to be normally distributed these statistics do not have F distributions. However 
using permutation tests almost exact p-values can be found and these are remarkably similar 
to p-values obtained from F distributions, even when the residuals are not well approximated 
by normality. There are other options for test statistics, but they do not perform as well as 
the F test statistics.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
53
Permutation Testing
The essence of the technique is to transform the responses using successive order r orthonormal 
polynomials on the responses. We call the ANOVA F tests on the responses transformed by 
the order r orthonormal polynomial order r tests, and these tests assess treatment effects in 
the moments up to order r in the responses. Test statistics of different orders are uncorrelated 
with each other, and so the significance or not of tests for one order does not influence 
significance or not of tests for any other order.
It will rarely be useful to consider effects beyond those of order three. Even so, many tests 
on treatment effects on several levels are being done at the same time, and if testing is done 
at the 0.05 level then we should expect approximately 5% of them will be significant, even 
if there are no effects present in the model. This suggests nonparametric ANOVA should 
be considered to be exploratory data analysis.
To apply the nonparametric ANOVA the first few orthonormal polynomials of a random 
variable are needed. Here we give the orthonormal polynomials of a random variable X up 
to order three. Write   for the mean of X and r, r = 2, 3, ‚Ä¶  for the central moments of 
X. Ambiguity is avoided by setting a0(x) = 1 for all x.  Then
a1(x) = (x ‚Äì )/2, 
 
a2(x) = {(x ‚Äì )2 ‚Äì 3(x ‚Äì )/2 ‚Äì 2}/d  
in which d = 
, and 
 
a3(x) = {(x ‚Äì )3 ‚Äì a(x ‚Äì )2 ‚Äì b(x ‚Äì ) ‚Äì c}/e,  
in which a = (
)/d, 
b = (
)/d, 
c = (
)/d, and  
e = 6 ‚Äì 2a5 + (a2 ‚Äì 2b)4 + 2(ab ‚Äì c)3 + (b2 + 2ac)2 + c2
2
2
2
2
3
4
/






3
2
2
4
3
5
/








2
3
2
5
3
4
2
2
2
4
/
/











5
2
2
3
3
4
3
/
2








Should further orthonormal polynomials be required the recurrence relation in Rayner et 
al. (2008) can be used.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
54
Permutation Testing
54
Given a data set {yij} say, the unordered nonparametric multifactor ANOVA applies the 
intended ANOVA to {ar(yij)} for r = 1, 2, ‚Ä¶ k, say, for some predetermined k:  usually 3. 
For each specified r the ANOVA tests whether or not the E[ar(Yij)]  are consistent across 
the levels of the factors. Thus the ANOVA applied to E[a1(Yij)]  tests whether or not the
E[a1(Yij)] = (E[Yij] ‚Äì )/  are consistent across the levels of the factors. Here the moments 
defining the orthonormal polynomials refer to the distribution of the responses. Since 
ANOVA is location-scale invariant the first order analysis is equivalent to testing whether or 
not the E[Yij]  are consistent across the levels of the factors; this is the traditional ANOVA 
null hypothesis.
The second order ANOVA that tests whether or not the E[a2(Yij)] = E[(Yij ‚Äì )2  ‚Äì 3(Yij ‚Äì )/2 ‚Äì 2]  
are consistent across the levels of the factors. If the first order null hypothesis is accepted, 
E[Yij] =  and E[a2(Yij)] = var(Yij) ‚Äì 2,  and the second order null hypothesis is testing 
whether or not the var(Yij)  are consistent across the levels of the factors. If the first order 
null hypothesis is not accepted the test concerns a measure of dispersion, generally not 
the variance, and assesses whether or not this measure is consistent across levels. The same 
argument applies to the higher order tests.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

INTRODUCTORY NONPARAMETRICS
55
Permutation Testing
Tomato Example. To apply the suggested analysis the orthonormal polynomials are required, 
and these in turn require the moments of the responses. For n responses each response is 
given probability 1/n. For example here the responses 5, 6 and 10 each have probability 
1/96 as do each of the responses 12. To construct the orthonormal polynomials of order 
up to three requires moments up to order six. These are
 = 62.09375, 2 = 1205.24375, 3 = 9539.0866089,  
4 = 2747648.0196, 5 = 51562852.798 and 6 = 8071672926.3.
Note. If the sample variance is used in place of the population variance the result here is 
1192.689 and the subsequent analysis is identical.
Response
Tomato 
variety
First order 
orthonormal 
polynomial
Second order 
orthonormal 
polynomial
Third order 
orthonormal 
polynomial
5
1
-1.6532
2.2583
-2.7114
6
4
-1.6242
2.1498
-2.4677
10
1
-1.5084
1.7339
-1.5893
12
2
-1.4505
1.5367
-1.2059
12
2
-1.4505
1.5367
-1.2059
13
3
-1.4216
1.4408
-1.0275
13
4
-1.4216
1.4408
-1.0275
15
4
-1.3636
1.2544
-0.6968
TABLE 3.2. Eight smallest transformed responses of orders one, two and three
The values taken by the eight smallest responses transformed by the polynomials of order 
one, two and three are given in the Table 3.2. These calculations will enable users to check 
their calculations should they prefer to program in a language other than R.
A summary of the analysis is given in the following Table 3.3. The ANOVA F-tests applied 
to the responses and to the responses transformed by the first order orthonormal polynomial 
are, as discussed above, identical. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
56
Permutation Testing
First order
Second order
Third order
Tomato F test statistic
0.7986
1.4708
0.5025
Tomato F test p-value
0.4978
0.2277
0.6815
Shapiro-Wilk Normality test p-value
0.0052
< 0.0001
0.0014
TABLE 3.3. Summary of the nonparametric unordered analysis for the tomato data
The ANOVA F-tests applied to the responses transformed by the first, second and third 
order orthonormal polynomials all produced large p-values, giving non-significant results at 
all reasonable levels. In line with the discussion above, since first order tomato effects are 
consistent across varieties, the test for second order effects is, in fact, a test for consistency of 
variety variances. Since first and second order effects are consistent across varieties, the test 
for third order effects is, in fact, a test for equality of third order moments across varieties. 
So the tomato varieties have similar first, second and third order moments across varieties. 
The first order p-value here is a little smaller than the Kruskal-Wallis p-value reported in 
Section 5.1 of Chapter 2. However the Kruskal-Wallis test is based on the ranked responses; 
the analysis here is based on the raw data, the unranked responses.
A check on the normality of the residuals for the three ANOVAs using the Shapiro-Wilk test 
of normality gave small p-values in all cases. Since the normality assumption underpinning 
the F-tests is in doubt it is desirable to check the p-values by calculating permutation test 
p-values. This will be done in the next section.
The tests applied here are unable to find any differences between the tomato varieties.
Lemonade Example. The nonparametric ANOVA ignoring order applies the randomised 
blocks ANOVA to the data transformed by the orthonormal polynomials of orders one, 
two and three. The p-values are summarised in the Table 3.4. 
p-value
First order
Second order
Third order
Lemonade F test 
0.0504
0.9232
0.0047
Shapiro-Wilk Normality test
0.0211
< 0.0001
0.0125
TABLE 3.4. Summary of the nonparametric unordered analysis for the lemonade data
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
57
Permutation Testing
57
Although normality is dubious it seems there are first and third order effects ‚Äì roughly 
indicating a mean effect and an effect due to moments up to third order. The p-value here 
for the first order transformation agrees with that given in Section 5.2 of Chapter 2. 
The polynomial means for the lemonade varieties are given in Table 3.5. The first order 
effects are almost significant at the 0.05 level. In section 2.6 the orthogonal components of 
the first order test statistic identified an umbrella effect, so this is the cause of the borderline 
first order significance. The umbrella effect is apparent in Table 3.5. The second order mean 
differences are just natural variation: they are not significant at all reasonable levels. The 
third order effect is significant at the 0.01 level. It is not interesting to decompose this effect 
into linear, quadratic etc. components, although from ‚Äòeyeballing‚Äô the third order means in 
Table 3.5 there is a possible umbrella effect. However it is not clear that such an effect is 
useful in interpreting the data.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

INTRODUCTORY NONPARAMETRICS
58
Permutation Testing
Lemonade variety
1
2
3
4
5
First order mean
0.424
0.141
-0.919
0.071
0.283
Second order mean
-0.120
0.000
-0.060
0.299
-0.120
Third order mean
-0.141
0.424
0.778
-0.141
-0.919
TABLE 3.5. Lemonade polynomial means
Ice Cream Example. Using the usual parametric F test the responses are significant at the 
0.01 level with a p-value of 0.0049. This is as reported in Section 5.3 of Chapter 2. Using 
the Shapiro-Wilk test for normality yields a p-value of 0.4445, so the residuals are consistent 
with normality. The second order orthonormal polynomial responses are not significant with 
a p-value of 0.6118 and a Shapiro-Wilk normality test p-value of 0.5992. There can be no 
third order nonparametric ANOVA analysis as there are only three responses, and so only 
two orthonormal polynomials can be constructed.
It seems there are first order, that is, mean effects, but no order two effects. The polynomial 
means of first and second order are given in Table 3.6. There is no pattern apparent in 
the second order means, the differences being natural variation. However the first order 
means increase, and although the first order effect is of standardised ranks, this reflects the 
significant linear trend discussed in section 2.6.
Ice Cream
1
2
3
4
5
6
First order mean
-1.012
-0.559
-0.106
0.348
0.529
0.801
Second order mean
0.265
0.051
-0.564
-0.578
0.618
0.209
TABLE 3.6. Ice cream polynomial means
3.3	
REVISITING SOME PREVIOUS EXAMPLES
In this section we collect the treatment p-values based on distribution theory for our three 
main examples, the tomato, lemonade and ice cream examples, and compare them with 
permutation test almost exact p-values. Note that the permutation test p-values will be 
different each time they are calculated, but they will cluster around the true value, since 
they are based on the true distribution of the test statistic. P-values based on 2  and F 
distributions are only approximations, relying on the assumption of normality. In some 
cases this assumption is dubious.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
59
Permutation Testing
In particular note that in Tables 3.7, 3.8 and 3.9 the p-values for the ANOVA F and first 
order NP ANOVA are the same. The corresponding permutation test p-values vary slightly 
because different permutations are being generated.
Tomato Example. 
Test Statistic
Distribution theory
Permutation test
Distribution
p-value
p-value
ANOVA F
F3,92
0.4978
0.4966
Kruskal-Wallis
œá3
2  
0.5772
0.5824
F3,92
0.5838
-
F2.9622,90.8403
0.5820
-
Page-type
N(0, 1)
0.7271
0.7284
Umbrella
N(0, 1)
0.1984
0.1994
Cubic
N(0, 1)
0.6535
0.6552
First order NP ANOVA
F3,92
0.4978
0.4968
Second order NP ANOVA
F3,92
0.2277
0.2266
Third order NP ANOVA
F3,92
0.6815
0.6833
TABLE 3.7. Summary of the analyses of the Tomato data
The permutation test p-values for nonparametric ANOVA use method 1 suggested in 
Manly (2007, p. 145). This freely randomises observations and uses F statistics as opposed 
to restricted randomisation and/or using mean squares or other test statistics.
The permutation test p-values are very similar to the p-values based on the nominated 
asymptotic and approximate distributions, confirming the validity of these tests.
Lemonade Example.
The permutation test p-values here and in the next example involve permuting within 
blocks. Again the permutation test p-values are very similar to the p-values based on the 
nominated asymptotic and approximate distributions, confirming the validity of these tests.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
60
Permutation Testing
60
Test Statistic
Distribution theory
Permutation test 
Distribution
p-value
p-value
ANOVA F
F4,36
0.0504
0.0531
Friedman
2
4
œá  
0.0601
0.0554
F4,36
0.0504
-
Page
N(0, 1)
0.7518
0.7771
Umbrella
N(0, 1)
0.0215
0.0204
First order NP ANOVA
F4,36
0.0504
0.0523
Second order NP ANOVA
F4,36
0.9232
0.9175
Third order NP ANOVA
F4,36
0.0047
0.0061
TABLE 3.8. Summary of the analyses of the Lemonade data
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT‚Ä¶
     RUN FASTER.
          RUN LONGER..
                RUN EASIER‚Ä¶
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

INTRODUCTORY NONPARAMETRICS
61
Permutation Testing
Ice Cream Example.
Test Statistic
Distribution theory
Permutation test
Distribution
p-value
p-value
ANOVA F
F6,8
0.0049
0.0031
Durbin
2
6
œá
0.0620
0.0181
F6,8
0.0049
-
Page
N(0, 1)
0.3223
0.3208
Umbrella
N(0, 1)
0.0101
0.0068
First order NP ANOVA
F6,8
0.0049
0.0081
Second order NP ANOVA
F6,8
0.6118
0.6284
TABLE 3.9. Summary of the analyses of the Ice cream data
Note there is no third order NP ANOVA analysis as each judge only assess three ice creams. 
The permutation test p-value for the Durbin test is similar to that for the F6,8 approximation, 
but is somewhat different to that based on the 
2
6
œá  distribution. The other permutation test 
p-values agree well with the nominated asymptotic and approximate distributions.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
62
Permutation Testing
CONCLUDING REMARKS
Introductory Nonparametrics is intended as a gentle introduction to nonparametric methods. 
Having worked through this material the reader should have the ability to apply several 
tests generally acknowledged as ‚Äònonparametric‚Äô. The Kruskal-Wallis, Friedman and Durbin 
tests are important because they arise in experimental designs that are often applied in 
practice. P-values are often calculated using asymptotic distributions of the test statistics. 
However it is possible to improve on using these chi-squared distributions by taking certain 
transformations of the test statistics and using their F distributions. Better still is to use 
permutation tests. Uncritical use of any recipe is poor science. If a user is not in a position 
to calculate permutation test p-values then use the F statistics and check by also using the 
chi-squared statistics; the answers should be similar.
Some of the material here, specifically that on nonparametric multifactor ANOVA, is 
relatively recent research. It should be a comfort to the reader that statistics is a vibrant 
science with better methodology constantly emerging.
My intention is to produce follow-up material in Advanced Nonparametrics. This will focus 
on more advanced methods, with substantial content in recent research papers. It will include 
chapters on correlation and independence, the Cochran-Mantel-Haenszel tests, goodness of 
fit testing and powerful new methods based on probability index models.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
63
References
REFERENCES
CONOVER, W.J. (1999). Practical Nonparametric Statistics (3rd ed.). New York: Wiley. 
HIGGINS, J.J. (2004). Introduction to Modern Nonparametric Statistics. Belmont, CA: 
Duxbury Press. 
KUEHL, R.O. (2000). Design of Experiments: Statistical Principles of Research Design and 
Analysis. Pacific Grove, CA: Duxbury Press.
RAYNER, J.C.W. and BEST, D.J. (1989). Smooth Tests of Goodness of Fit. New York: Oxford 
University Press.
RAYNER, J.C.W. and BEST, D.J. (1999). Modelling ties in the sign test. Biometrics, 55, 
2, 663‚Äì666.
RAYNER, J.C.W. and BEST, D.J. (2013). Extended ANOVA and rank transform procedures. 
Australian and NZ Journal of Statistics, 55(3), 305‚Äì319.
RAYNER, J.C.W., THAS, O. and De BOECK, B. (2008). A generalised Emerson recurrence 
relation. Australian and NZ Journal of Statistics 50(3), 235‚Äì240.
RIPPON, Paul (2016). An R Companion to Introductory Nonparametrics.
SPURRIER, J.D. (2003). On the null distribution of the Kruskal-Wallis statistic, Journal 
of Nonparametric Statistics, 15:6, 685‚Äì691, DOl: 10.1080/10485250310001634719.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
64
Subject Index
SUBJECT INDEX
Balanced incomplete block design	 	
Section 2.4, 2.5.3, 3.2, 3.3
Completely randomised design	
	
Section 2.2, 2.5.1, 3.2, 3.3
Continuity correction	
	
	
Section 1.2
Median test	 	
	
	
	
Section 1.4
Orthonormal polynomials	
	
	
Section 3.2
Randomised block design	
	
	
Section 2.3, 2.5.2, 3.2, 3.3
Runs tests
	
One-sample test	
	
	
Section 1.3.1
	
Randomness test	
	
	
Section 1.3.2
Sign tests
	
One-sample test	
	
	
Section 1.2.1
	
Two-sample test	
	
	
Section 1.2.2
Wilcoxon tests
	
Signed ranks test	
	
	
Section 1.5.1
	
Two-sample test	
	
	
Section 1.5.2
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
65
Examples Index
65
EXAMPLES INDEX
Achievement Test Example	 	
	
Section 1.4
Corn Example	
	
	
	
Section 1.4
Examination Example	
	
	
Section 1.3.2
Flints Example	
	
	
	
Sections 1.3.1, 1.5.2
Heart Rates Example		
	
	
Section 1.2.2
Herbicide Example	 	
	
	
Section 3.1
Ice cream Example	
	
	
	
Sections 2.4, 2.5.3, 2.6, 3.2, 3.3
Japanese chocolate responses		
	
Chapter 2 exercise 1, Chapter 3 exercise 1
Lemonade Example	 	
	
	
Sections 2.3, 2.5.2, 2.6, 3.2, 3.3
Monkey Example	
	
	
	
Section 1.2.1
Potencies of a pharmaceutical product	
Chapter 1 exercise 3
Speed Example	
	
	
	
Section 1.3.2
Teaching methods Example	 	
	
Section 1.5.2
Tomato Example	
	
	
	
Sections 2.2, 2.5.1, 2.6, 3.2, 3.3
Tyre Example		
	
	
	
Section 1.5.1
Vanilla flavour ratings for six ice creams	
Chapter 2 exercise 3
Word Processors	
	
	
	
Chapter 2 exercise 2
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

INTRODUCTORY NONPARAMETRICS
66
Exercises
EXERCISES
CHAPTER ONE EXERCISES
1.	In about 100 words contrast some of the essential features of parametric and 
nonparametric methods.
Questions 2 and 3 don‚Äôt really require the use of R; a standard package such as JMP or 
SPSS will be sufficient. However the R code in Rippon (2016, Chapter 1) may usefully 
be modified. This has been done in the solutions. 
2.	(i)	
Use the sign test to test the hypothesis that zero is the median of the following 
16 observations: 
	
0.3, 6.3, 3.7, 2.8, 5.8, ‚Äì1.4, 1.7, 2.3, ‚Äì1.7, 1.6, ‚Äì1.8, 0.6, 4.5, 1.9, 2.4, 6.8.
(ii)	
An approximate confidence interval for a binomial proportion p based on an 
observed proportion pÀÜ  from a sample of size n is ÀÜp¬± aŒ±
ÀÜp(1‚àíÀÜp) / n . Here aŒ±  is 
the point that gives probability Œ±/2 in each tail of the standard normal distribution. 
Construct approximate 95% and 99% confidence intervals for the proportion of 
negative observations. 
(iii)	 Are your answers in (i) and (ii) consistent? If not, why not?
3.	Coded potencies of a series of lots of a pharmaceutical product as measured by 
two different methods were:
Method I:	
3.3, 2.3, 3.7, 2.8, 2.8, ‚Äì1.4, 1.7, 2.3,
Method II:	
‚Äì4.7, 4.6, ‚Äì1.8, ‚Äì2.6, 4.5, 3.9, 2.4, 6.8.
We wish to assess if the two methods can be regarded as being the same.
(i)	
Analyse the data both parametrically and nonparametrically. Use any convenient 
software with which you are familiar. Comment on the output. In particular 
comment on the assumptions for the pooled t-test.
(ii)	
Combine, order and rank the data. This will make it easier to do the tests following.
(iii)	 Apply the runs test to the data above to assess if there is a difference in methods. 
Find the exact probability of differences at least as extreme as the observed, and 
compare this with the normal approximation. State your conclusion carefully.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
67
Exercises
(iv)	
Apply the median test, classifying the data as above and below both
a)‚ÄÉ the median (show this is 2.6), and
b)‚ÄÉ the lower quartile (show this is 0.15).
In b) calculate the exact p-value using the extended hypergeometric distribution.
(v)	
Apply the two-sample Wilcoxon test to these data. In the process of doing this, 
verify that W1 + W2 = (m + n)(m + n + 1)/2.
(vi)	
Summarise your conclusions.
CHAPTER TWO EXERCISES
1.	At the CSIRO Food Research Laboratory in the early 1990s, the Japan project was 
set up to look at, among other things, how Japanese and Australian consumers rated 
various sweet foods on seven point scales. These scales had the anchors ‚ÄòDislike 
Extremely‚Äô and ‚ÄòLike Extremely‚Äô. The scores shown were assigned later to assist 
with analysis. Note that the comparison is not between products, but between 
consumers from two cultures.
Score
Consumers
1
2
3
4
5
6
7
Australian
2
1
6
1
8
9
6
Japanese
0
1
3
4
15
7
1
Japanese chocolate responses
Analyse these data both parametrically and nonparametrically to see if there are location 
differences. Your parametric analysis should include the pooled t-test, the Welch test, an 
assessment of which is preferred by checking the sample variances for consistency, and 
an assessment of the normality of the residuals. (As there are only two treatments these 
analyses are essentially an orthogonal contrast, so do not attempt to apply the techniques 
of section 2.6.) Discuss your results. We will return to these data in a later chapter.
R Help. As there are different numbers of Australian and Japanese consumers a data frame 
is not appropriate; use the list instead, as in the following:
choc1 <- list(Australian=c(1, 1, 2, 3, 3, 3, 3, 3, 3, 4, 5, 5, 5, 
5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7),
	
	
	
Japanese=c(2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 
5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7))
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
68
Exercises
68
The Wilcoxon test requires ranks rather than scores. As in the following, the rank command 
may be helpful
choc2$r <- rank(choc2$score, ties.method = "average")
2.	Four experts compared five word processors. The data are the time (in minutes) 
taken to prepare a report on each machine. The data come from Freund (2004, 
Exercise 15.30).
Experts
Word Processors
1
2
3
4
A
49.1 (2)
48.2 (4)
52.3 (4)
57.0 (4)
B
47.5 (1)
40.9 (1)
44.6 (1)
49.5 (1)
C
76.2 (5)
46.8 (3)
50.1 (3)
55.3 (3)
D
50.7 (3)
43.4 (2)
47.0 (2)
52.6 (2)
E
55.8 (4)
48.3 (5)
82.6 (5)
57.8 (5)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

INTRODUCTORY NONPARAMETRICS
69
Exercises
This is a randomised blocks design. Analyse the data both parametrically and 
nonparametrically for location effects between word processors. To do the latter modify 
the R code in Rippon (2016). Input the ranks within blocks, given in brackets in the 
table. Remember to load reshape2.
3.	Rayner et al. (2005, p.93) analyse data from the former Dairy Manufacturing 
Department at North Carolina State College, USA, for ice creams rated on a 
six-point scale with one meaning no vanilla flavour and six meaning the highest 
amount of vanilla flavour. Balanced incomplete block (BIB) designs are used in 
sensory evaluation due to palate paralysis or sensory fatigue. Previous experience 
shows that often only four products can reliably be rated at one time by a judge. 
The data below use the ranks of the original data with ties broken at random.
Judge/Ice cream
A
B
C
D
E
F
1
2
4
1
3
2
1
3
2
4
3
1
2
3
4
4
1
2
3
4
5
1
2
3
4
6
1
2
3
4
7
2
3
1
4
8
1
2
3
4
9
1
2
4
3
10
2
3
1
4
11
1
3
4
2
12
1
2
3
4
13
1
2
4
3
14
1
3
4
2
15
4
3
1
2
Vanilla flavour ratings for six ice creams
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
70
Exercises
Analyse the data both parametrically and nonparametrically. For the latter you may 
assume the ice creams are ordered from A to F and calculate the Page-type and umbrella 
statistics. Discuss your findings.
R Help. The data entry is a little tedious; use the text file on the book web page.
CHAPTER THREE EXERCISES
1.	For the Japan project chocolate data from the first exercise for Chapter Two copy 
and complete the following table.
Test Statistic
Distribution theory
Permutation test
Distribution
p-value
p-value
ANOVA F
F1,62
Kruskal-Wallis
2
1
F
Page-type
N(0, 1)
First order NP ANOVA
F1,62
Second order NP ANOVA
F1,62
Third order NP ANOVA
F1,62
Comment.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
71
Exercises
71
2.	For the word processors data from the second exercise for Chapter Two copy and 
complete the following table.
Test Statistic
Distribution theory
Permutation test 
Distribution
p-value
p-value
ANOVA F
F4,12
Friedman
2
4
F  
F4,12
-
Page
N(0, 1)
Umbrella
N(0, 1)
Cubic
N(0, 1)
First order NP ANOVA
F4,12
Second order NP 
ANOVA
F4,12
Third order NP ANOVA
F4,12
Comment.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

INTRODUCTORY NONPARAMETRICS
72
Exercises
3.	For the ice cream data from the third exercise for Chapter Two copy and complete 
the following table.
Test Statistic
Distribution theory
Permutation test
Distribution
p-value
p-value
ANOVA F
F5,40
Durbin
2
5
F
F5,40
-
Page
N(0, 1)
Umbrella
N(0, 1)
Cubic
N(0, 1)
First order NP ANOVA
F5,40
Second order NP ANOVA
F5,40
Third order NP ANOVA
F5,40
Comment.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
73
Solutions
73
SOLUTIONS
SOLUTIONS TO THE CHAPTER ONE EXERCISES
1.	Parametric methods are not available for data on the nominal or ordinal measurement 
scales, only on ratio and interval scales. Nonparametric methods are available on 
all measurement scales.
Nonparametric methods make minimal assumptions whereas parametric methods make 
more assumptions and are more powerful when these assumptions are valid. Nonparametric 
methods are available when the assumptions needed for parametric methods may not be 
valid. When the parametric assumptions do not hold nonparametric methods is usually 
have greater power and efficiency.
Often parametric methods are about distributions (are these data consistent with normality?) 
or parameters of distributions (is the population mean zero?), whereas nonparametric 
methods may be more nebulous (are these data random?).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

INTRODUCTORY NONPARAMETRICS
74
Solutions
2.		(i) We are testing if the 16 observations have median 0. Since there are three 
negative differences, a one-sided p-value would be {13C3 + 13C2 + 13C1 + 13C0}/216 
= 697/216 = 0.011. A two-tailed test is appropriate, so the p-value is double 
this, 0.021. The null hypothesis that the median is zero is rejected at the 0.05 
level but not the 0.01 level; there is some evidence that the median is not zero. 
Here is some R code, modified from Rippon (2016). It supports the calculations above.
# vector of question 2 data
y <- c(0.3,6.3,3.7,2.8,5.8,-1.4,1.7,2.3,-1.7,1.6,-1.8,0.6,4.5,1.9,2.4,6.8)
n <- length(y) # number of measurements
hmed <- 0 # hypothesized median
S <- length(y[y>hmed]) # number of measurements greater than 
hypothesized median
left.tail <- pbinom(q=S-1, size=n, prob=0.5)
right.tail <- pbinom(q=S-1, size=n, prob=0.5, lower.tail=FALSE)
cat("pval(less)=",left.tail, "; pval(greater)=",right.tail, "; 
pval(both)=", 2*right.tail)
(ii)	 Since pÀÜ  = 3/16, the approximate 95% confidence interval is (‚Äì0.004, 0.379), 
or, since p must be non-negative, (0, 0.379). The negative part of the confidence 
interval reflects the fact that the normal approximation to the binomial isn‚Äôt 
adequate in the tails. The 99% confidence interval is (0.066, 0.430).
(iii)	The 95% confidence interval in (ii) excludes 0.5, which is consistent with 
concluding, at the 0.05 level, that p ‚â† 0.5, and that the median is not 0. The 
99% confidence interval also excludes 0.5, so that the test for p = 0.5 would 
also be rejected, now at the 0.01 level. This is in conflict with (i), reflecting the 
fact that the test based on the approximate confidence interval uses the normal 
approximation to the binomial, whereas the test in (i) does not. The two tests 
are making different assumptions, so it is not surprising they come to different 
conclusions. 
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
75
Solutions
3.	(i)‚ÄÇ The Shapiro-Wilk test of normality has p-value 0.0226 for method 1 and 
0.3728 for method 2. Tests assuming normality are therefore dubious. For the 
normal theory tests, tests of equality of variance, such as the Bartlett and the 
Levene, have p-values less than 0.05. Thus the pooled t-test, which has p-value 
0.857, is dubious. The Welch test, that does not assume equality of variances, 
has p-value 0.858.
The Wilcoxon test has p-value 0.713 or 0.674, depending on the approximation used, 
and the median test has p-value 1.0. Of these two tests the Wilcoxon test is the more 
powerful location test. 
However none of these tests gives evidence of a location difference between methods 
at any commonly used significance level.
(ii)	The ordered data, keeping track of the methods and ranks, is
Method 1
‚Äì1.4
1.7
2.3
2.3
2.8
2.8
Method 2
‚Äì4.7
‚Äì2.6
‚Äì1.8
2.4
Rank
1
2
3
4
5
6
7
8
9
10
Method 1
3.3
3.7
Method 2
3.9
4.5
4.6
6.8
Rank
11
12
13
14
15
16
(iii)	There are t = 5 runs, E[T] = 9, and var(T) = 56/15 = 1.9322. We find P(5 or 
fewer runs) = P(Z < (5.5 ‚Äì 9)/1.932 = ‚Äì1.811) = 0.035. Exact calculations give
	
P(T = 2) = 27C0
7C0/16C8, ‚Ä¶, P(T = 5) = 27C2
7C1/16C8 and
	
P(T < 5) = 2(1 + 7 + 49 + 147)/16C8 = 408/16C8 = 0.0317.
There is good agreement between the exact p-value and that based on the normal 
approximation. Both show significance at the 0.05 level but not at the 0.01 level. 
There is some evidence of a difference in methods.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
76
Solutions
76
Here is some R code, modified from Rippon (2016). It supports the calculations above.
y <- c("A", "A", "A", "B", "B", "B", "B", "A", "B", "B", "B", 
"B","A","A","A","A") # data vector
yt <- y == y[1] # convert to TRUE and FALSE, ie 1 and 0
yd <- diff(yt) # non-zero elements of the difference vector 
indicate the end of a run
T <- length(yd[yd != 0])+1
ty <- table(y) # table of counts
M <- ty[1]
N <- ty[2]
PT <- function (t, m, n) {# probability calculations
	
k <- t %/% 2 # note use of the integer division operator
	
if (t %% 2 == 0) # t is even
	
	
prob <- 2 * choose(m-1, k-1) * choose(n-1, k-1) / choose(m+n, n)
else # t is odd
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

INTRODUCTORY NONPARAMETRICS
77
Solutions
	
	
prob <- (choose(m-1, k) * choose(n-1, k-1) + choose(m-1, 
k-1) * choose(n-1, k)
	
	
) / choose(m+n, n)
	
return(prob)
}
cat("p-value = P(T =< ", T, ") = ", PT(t=2,m=M,n=N) + PT(t=3,m=M,n=N) 
+ PT(t=4,m=M,n=N)+ PT(t=5,m=M,n=N), sep="")
(iv)	 (a) The median is (2.4 + 2.8)/2 = 2.6. This gives a table
Method 1
Method 2
Total
Above 2.1
4 (4)
4 (4)
8
Below 2.1
4 (4)
4 (4)
8
8
8
16
Expected values are in parentheses. 
This is exactly as expected, so X2 = 0 with p-value P(X2 > 0) = 1. The data are not significant 
at any level. In fact the agreement with the null hypothesis is suspiciously good. We 
conclude that at all ‚Äòreasonable‚Äô levels, the method medians are consistent.
Again, here is some supporting R code.
# data vectors
m1 <- c(3.3, 2.3, 3.7, 2.8, 2.8, -1.4, 1.7, 2.3)
m2 <- c(-4.7, 4.6, -1.8, -2.6, 4.5, 3.9, 2.4, 6.8)
methods <- data.frame ( )
for (m in paste("m",1:2,sep="")) {
	
methods <- rbind(methods, data.frame(yield=get(m), method=m))
}
om <- median(methods$yield) # observed median of combined methods yields
# use the cut function to create a factor based on whether yield 
# is above or below the median
methods$cut <- cut(methods$yield, breaks=quantile(methods$yield, 
probs=c(0,0.5,1)),
	
	
	
labels=paste(c("<=", ">"), om), include.lowest=TRUE)
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
78
Solutions
# create and print contingency table
methods.xt <- xtabs(~cut+method,data= methods)
methods.xt
# perform chi-squared test
methods.chisq <- chisq.test(methods.xt)
methods.chisq
# show expected values 
methods.chisq$expected
chisq.test(methods.xt, simulate.p.value=TRUE)
(b)	The lower quartile is (‚Äì1.4 + 1.7)/2 = 0.15. This gives the following table.
Method 1
Method 2
Total
Above 0.15
7 (6)
5 (6)
12
Below 0.15
1 (2)
3 (2)
4
8
8
16
Expected values are in parentheses.
X2 = 2(1/6 + 1/2) = 4/3. This is not significant at the 0.1 level as œá1
2(0.1)  = 2.706. To 
find the exact p-value, note that the only table not more extreme than the observed (in 
terms of X2 values), is that with all entries exactly as expected, and this table has probability 
8C2
8C2/16C4 = 0.4308. The exact p-value is thus P(X2 > 4/3) = 1 ‚Äì P(X2 < 4/3) = 1 ‚Äì 0.4308 
= 0.5692. All other tables result X2 values of at least 4/3. We conclude that at the 0.05 
level (and all reasonable levels), the method lower quartiles are consistent.
Using a quantile other than the median requires slight adjustments to the R code. In 
general such code won‚Äôt be given subsequently.
m1 <- c(3.3, 2.3, 3.7, 2.8, 2.8, -1.4, 1.7, 2.3)
m2 <- c(-4.7, 4.6, -1.8, -2.6, 4.5, 3.9, 2.4, 6.8)
methods <- data.frame()
for (m in paste("m",1:2,sep="")) {
	
methods <- rbind(methods, data.frame(yield=get(m), method=m))
}
oq <- quantile(methods$yield, probs=0.25) # observed quantile of 
combined methods yields
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
79
Solutions
79
# use the cut function to create a factor based on whether yield 
# is above or below the observed quantile
methods$cut <- cut(methods$yield, breaks=quantile(methods$yield, 
probs=c(0,0.25,1)),
	
	
	
labels=paste(c("<=", ">"), oq), include.lowest=TRUE)
# create and print contingency table
methods.xt <- xtabs(~cut+method,data= methods)
methods.xt
# perform chi-squared test
methods.chisq <- chisq.test(methods.xt)
methods.chisq
# show expected values 
methods.chisq$expected
chisq.test(methods.xt, simulate.p.value=TRUE)
(v)	Using the ranked data we find W1 = 64, W2 = 72.
Check: W1 + W2 = 1 + ... + 16 = 8*17 = 136 = 64 + 72.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

INTRODUCTORY NONPARAMETRICS
80
Solutions
With m = n = 8 we find E[W2] = 68, var(W2) = 90.6667, giving P(W2 > 72) = P(Z > (71.5 
‚Äì 68)/‚àö90.6666 = 0.3676) = 0.357. Since both large and small W2 values are inconsistent 
with the null hypothesis, the p-value is double this, 0.714. At the 0.05 level, and indeed, 
at all reasonable levels, there is no evidence against the null hypothesis that the methods 
are consistent. The following R code supports the above.
methods <- data.frame(area= c("A", "A", "A", "B", "B", "B", "B", 
"A", "B", "B", "B", "B","A","A","A","A"))
methods$r <- 1:nrow(methods) # ranks
methods
W.A <- sum(methods$r[methods$area=="A"]) # sum of ranks for method 1
W.B <- sum(methods$r[methods$area=="B"]) # sum of ranks for method 2
counts <- table(methods$area)
m <- counts[1] # method 1
n <- counts[2] # method 2
# determine parameters for normal approximation to null 
distribution of W.A
mu.A <- m*(m+n+1)/2
sig2.A <- m*n*(m+n+1)/12
# calculate z score and p value corresponding to W.A (note 
continuity correction)
z <- (W.A - 0.5 - mu.A)/sqrt(sig2.A)
p.val <- 1-pnorm(z)
cat("p-value = P(Z < ", z, ") = ", p.val, sep="")
(vi)	In addition to the p-values reported in 3(i), we have the median test p-values of 
0.57 and 1.0, and the runs test 0.03. 
It is not valid to apply many different tests and take the most or least extreme. Nevertheless 
what seems to be happening here is that the runs test has detected an alternative to the null 
hypothesis that none of the other tests has been able to detect. The runs test is sensitive to 
differences in both location and shape, while the Wilcoxon and median tests are sensitive 
to location differences only. It seems the runs test is picking up dispersion differences 
between the methods that the tests for parametric equality of variances detected. Recall 
that all had low p-values, suggesting the variances were inconsistent. Although the other 
tests of equality of variance are parametric, Levene‚Äôs test does not assume normality, and 
is traditionally labelled as nonparametric.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
81
Solutions
SOLUTIONS TO THE CHAPTER TWO EXERCISES
1.	The two sample t-test gives a p-value of 0.9178. The Welch test gives a p-value of 
0.9167, remarkably consistent, but JMP also gives p-values for tests of consistency 
of the population variances: 0.0098 for the Bartlett test that assumes the data are 
normally distributed and which is known to be sensitive to non-normality. The 
nonparametric Levene test gives a p-value of 0.0097. This is remarkable agreement 
given that the Shapiro-Wilk test for normality of the residuals has p-value 0.0001. 
So in spite of the agreement that the Australian and Japanese consumers have 
consistent means but different variances, none of the tests here are valid as they 
assume normality, and at the 0.001 level the data are not consistent with normality.
Using the asymptotic chi-squared approximation the Kruskal-Wallis test has p-value 
0.4445. Referring F = KW(n ‚Äì t)/{n ‚Äì 1 ‚Äì KW)(t ‚Äì 1)} to the Ft‚Äì1,n‚Äìt distribution gives a 
p-value of 0.4603; using the improved Fd(t‚Äì1),d(n‚Äìt) distribution gives a p-value of 0.4572.
At all reasonable levels there is no evidence of a location difference between the Australian 
and Japanese consumers. However any plot of the data, or even just ‚Äòeyeballing‚Äô the data 
suggests there are no location differences but that the Japanese may be less variable than 
the Australian consumers. This is consistent with the Levene test result, which does not 
assume normality. Tests sensitive to more than differences in location are needed to unlock 
what is happening here.
The following R code gives the Wilcoxon p-value.
rm(list=ls())
choc1 <- list(Australian=c(1, 1, 2, 3, 3, 3, 3, 3, 3, 4, 5, 5, 5, 
5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7), 
Japanese=c(2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 
5,5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7))
choc2 <- stack(choc1)
names(choc2) <- c("score", "nationality")
wilcox.test(score~nationality, data=choc2)
choc2$r <- rank(choc2$score, ties.method = "average")
A <- sum(choc2$r[choc2$nationality=="Australian"]) 
J <- sum(choc2$r[choc2$nationality=="Japanese"])
t <- 2
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
82
Solutions
82
n <- nrow(choc2)
KW <- 12/n/(n+1) * (A^2/33 + J^2/31) - 3*(n+1)
d <- 1 - 6*(n+1)/(n-1)/(5*n+6)
F <- KW / (n-1-KW) * (n-t) / (t-1) 
pval <- pf(F, df1=d*(t-1), df2=d*(n-t), lower.tail=FALSE) # note 
adjusted df
F
pval
2.	A parametric analysis of the raw unranked data yields a p-value of 0.1626 for word 
processors (and 0.2717 for experts, who are blocks). At even the 0.1 level word 
processors are not significantly different. However the Shapiro-Wilk test of normality 
has p-value 0.0083. At the 0.01 level the data are not consistent with normality 
and although ANOVA in general is robust to the assumption of normality, the 
analysis is problematic.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

INTRODUCTORY NONPARAMETRICS
83
Solutions
One way of analysing the data nonparametrically is to analyse the ranks within blocks. The 
randomised blocks analysis of the ranks yields a p-value of 0.0003 for word processors. 
The blocks p-value is 1.0; because there are no ties every block receives ranks 1, 2, 3 and 
4, and hence there are no differences between blocks. Now word processor ranks are 
significant at the 0.001 level, which can be interpreted as there are significant differences 
in the means of the word processor ranks. The Shapiro-Wilk test of normality has p-value 
0.6209, so on this score at least, the inference is valid. One multiple comparisons analysis 
gives the following diagram, with rank means in parentheses:
B (1)‚ÄÉ ‚ÄÉ ‚ÄÉ D (2.25)‚ÄÉ ‚ÄÉ ‚ÄÉ A = C (3.5)‚ÄÉ ‚ÄÉ ‚ÄÉ E (4.75)
               _________
At an overall 0.05 level, there is evidence that the mean ranks for A and C are similar, 
and otherwise the word processor mean ranks are different.
Modifying the R code gives a value of the Friedman statistic of 13 with a c2
4
œá  p-value 
of 0.01128 and a F4,12 p-value 0.0002553. From the latter (which is more reliable) at 
the 0.001 level there is evidence of a difference in the word processor ranks. To assess 
the nature of this difference the orthogonal components are investigated. The Page test 
statistic takes the value 1.5 and p-value 0.1336, while the umbrella statistic takes the value 
2.1129 with p-value 0.0346. The mean ranks in order A, B, C, D and E are 3.5, 1, 3.5, 
2.25 and 4.75. So as we pass from A to E there is evidence (at the 0.05 level but not the 
0.01 level) of an umbrella effect: here that the mean ranks decrease and then increase.
The R code is given in the text file on the book web page.
3.	The ANOVA F test shows that the (ranked and ties broken) responses are significant 
at the 0.001 level with p-value less than 0.0001. Using the Shapiro-Wilk test for 
normality yields a p-value of 0.319, so the residuals are consistent with normality. 
One multiple comparisons analysis at the 0.05 level gives the means for ice creams 
one to six as 1.3, 1.9, 2.4, 2.9, 3.1 and 3.4 respectively, with corresponding diagram.
1.3	
1.9	
2.4	
2.9	
3.1	
3.4
_________
	
________
	
	
_____________
	
	
	
_____________
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
84
Solutions
It seems that at this level ice creams A and B are consistent, as are the ice creams B and 
C, C to E and ice creams D to E and; all others are significantly different.
The nonparametric analysis finds the Durbin statistic takes the value 20.9333 with 
2
5
F  
p-value 0.0008 and F5,40 p-value 0.0001. The linear contrast takes the value 4.5057 
with corresponding p-value 0.0000; the quadratic contrast takes the value ‚Äì0.7066 with 
corresponding p-value 0.4469. 
There is strong evidence, at the 0.001 level, of a treatment effect. This is consistent with 
the F test conclusion, although for the Durbin analysis the conclusion is weaker: that the 
treatment distributions differ. The quadratic contrast is not significant at the 0.05 level 
but the linear contrast is significant at the 0.05 level but not the 0.01 level. Again this is 
consistent with the F test analysis and with simply eyeballing the data: as we pass from 
ice cream one to six that mean ranks strictly increase.
The R code is given in the text file on the book web page.
SOLUTIONS TO THE CHAPTER THREE EXERCISES
1.	Here is the completed table for the Japan project chocolate analysis. Permutation 
test p-values are based on 10,000 permutations and of course will differ slightly 
each time they are calculated.
Test Statistic
Distribution theory
Permutation test
Distribution
p-value
p-value
ANOVA F
F1,62
0.9178
0.9324
Kruskal-Wallis
2
1
F
0.4403
0.4446
Page-type
N(0, 1)
0.4560
0.4446
First order NP ANOVA
F1,62
0.9178
0.9324
Second order NP ANOVA
F1,62
0.0037
0.0030
Third order NP ANOVA
F1,62
0.5079
0.5110
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
85
Solutions
85
Comment. In Exercises 2, question 1 a p-value for the Wilcoxon rank sum test with 
continuity correction is given. This is slightly different from the Kruskal-Wallis p-value 
given here, presumably because R either does not use a continuity correction for the 
Kruskal-Wallis test, or uses one that doesn‚Äôt reduce to that for two treatments. 
The permutation test p-values agree well with the distribution theory p-values. At the 0.05 
level the ANOVA F test reveals no difference in mean scores and the Kruskal-Wallis test, 
known to be sensitive to differences in medians, reveals no difference in distributions. 
As there are only two factors there is only one orthogonal contrast, the linear. It will 
also be sensitive to location differences, and it, too, shows no evidence of same. The 
nonparametric ANOVAs show no evidence of first and third order effects but at the 0.01 
level gives evidence of a second order effect. As there is no evidence of a location effect, 
this is a variance effect.
The R code is given in the text file on the book web page.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

INTRODUCTORY NONPARAMETRICS
86
Solutions
2.	For the word processors ranked data from the second exercise for Chapter Two the 
completed table is as follows.
Test Statistic
Distribution theory
Permutation test 
Distribution
p-value
p-value
ANOVA F
F4,12
0.0003
0.0011
Friedman
2
4
F  
0.0113
0.0012
F4,12
0.0003
0.0011
Page
N(0, 1)
0.1336
0.1438
Umbrella
N(0, 1)
0.0346
0.0321
Cubic
N(0, 1)
0.6171
0.5936
First order NP ANOVA
F4,12
0.0003
0.0010
Second order NP ANOVA
F4,12
0.0088
0.0201
Third order NP ANOVA
F4,12
0.1148
0.1270
Comment. Again the agreement between the distribution theory p-values and the 
permutation test p-values is good. At the 0.001 level the word processor mean ranks are 
significantly different. For word processors A, B, C and D these are are 3.5, 1.0, 3.5, 2.25 
and 4.75 respectively. An LSD analysis at an overall 0.05 level gives
B‚ÄÉ ‚ÄÉ D‚ÄÉ ‚ÄÉ A‚ÄÉ ‚ÄÉ C‚ÄÉ ‚ÄÉ E
________
Word processors A and C have similar mean ranks but otherwise all word processor mean 
ranks are significantly different. At the 0.05 level there is no evidence of a linear trend but 
there is of an umbrella effect. This is consistent with the significance, at the 0.05 level, 
of the first and second order nonparametric ANOVAs.
The R code is given in the text file on the book web page.
Download free eBooks at bookboon.com

INTRODUCTORY NONPARAMETRICS
87
Solutions
3.	For the ice cream flavour data from the third exercise for Chapter Two the completed 
table is as follows.
Test Statistic
Distribution theory
Permutation test
Distribution
p-value
p-value
ANOVA F
F5,40
0.0001
0.0003
Durbin
0.0008
0.0002
F5,40
0.0001
Page
N(0, 1)
0.0000
0.0000
Umbrella
N(0, 1)
0.4469
0.4680
Cubic
N(0, 1)
0.9846
0.9892
First order NP ANOVA
F5,40
0.0001
0.0002
Second order NP ANOVA
F5,40
0.0473
0.0581
Third order NP ANOVA
F5,40
0.0283
0.0318
Comment. At the 0.001 level there is a significant difference in ice cream flavours. For ice 
creams A to F the flavour means are 1.4, 1.9, 2.4, 2.9, 3.1 and 3.4. Using the comparison 
command in R gives the following LSD analysis.
A‚ÄÉ ‚ÄÉ ‚ÄÉ B‚ÄÉ ‚ÄÉ ‚ÄÉ C‚ÄÉ ‚ÄÉ ‚ÄÉ D‚ÄÉ ‚ÄÉ ‚ÄÉ E‚ÄÉ ‚ÄÉ ‚ÄÉ F
	
 ‚ÄÉ ‚ÄÉ   ‚ÄÉ
	
	
   ‚ÄÉ  ‚ÄÉ ‚ÄÉ
	
	
	
     ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ
	
	
	
	
       ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ  
There are five degrees of freedom associated with the location effect. These can be 
decomposed into orthogonal components of degree one to five. Here we only give the 
first three. Of these it seems only the first component, reflecting a linear trend in the 
mean ranks, is important. The Page-type test indicates, at the 0.05 level, that as we pass 
successively through the ice creams from A to F the flavour ranks increase.
The significance of the first order NP ANOVA reflects the location effect. The (near) 
significance of the second order NP ANOVA reflects moment effects up to order two. At 
the 0.05 level there are location effects; there may be second order effects as well.
The R code is given in the text file on the book web page.
2
5
F
Download free eBooks at bookboon.com


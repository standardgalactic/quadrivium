Preface
This Instructor’s Solutions Manual contains solutions for essentially all of the exercises in the
text that are intended to be done by hand.
Solutions to Matlab exercises are not included.
The Student’s Solutions Manual that accompanies this text contains solutions for only selected
odd-numbered exercises, including those exercises whose answers appear in the answer key. The
solutions that appear in the students’ manual are identical to those provided in this manual,
and generally provide a more detailed solution than is available in the answer key. Although no
pattern is strictly adhered to throughout the student manual, the solutions provided there are
primarily to the computational exercises, whereas solutions that involve proof are generally not
included. None of the solutions to the supplementary end-of-chapter exercises are included in the
student manual.

Contents
Preface
iii
1
Matrices and Systems of Equations
1
1.1
Introduction to Matrices and Systems of Linear Equations . . . . . . . . . . . . . .
1
1.2
Echelon Form and Gauss-Jordan Elimination . . . . . . . . . . . . . . . . . . . . .
6
1.3
Consistent Systems of Linear Equations . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.5
Matrix Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.6
Algebraic Properties of Matrix Operations . . . . . . . . . . . . . . . . . . . . . . .
21
1.7
Linear Independence and Nonsing. Matrices . . . . . . . . . . . . . . . . . . . . . .
26
1.8
Data ﬁtting, Numerical Integration . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.9
Matrix Inverses and their Properties . . . . . . . . . . . . . . . . . . . . . . . . . .
32
1.10 Supplementary Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
1.11 Conceptual Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2
Vectors in 2-Space and 3-Space
43
2.1
Vectors in the Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.2
Vectors in Space
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.3
The Dot Product and the Cross Product . . . . . . . . . . . . . . . . . . . . . . . .
48
2.4
Lines and Planes in Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
2.5
Supplementary Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.6
Conceptual Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3
The Vector Space Rn
59
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.2
Vector Space Properties of Rn
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.3
Examples of Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.4
Bases for Subspaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.5
Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.6
Orthogonal Bases for Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
3.7
Linear Transformations from Rn
to Rm . . . . . . . . . . . . . . . . . . . . . . . .
83
v

vi
CONTENTS
3.8
Least-Squares Solutions to Inconsistent Systems . . . . . . . . . . . . . . . . . . . .
89
3.9
Fitting Data and Least Squares Solutions
. . . . . . . . . . . . . . . . . . . . . . .
92
3.10 Supplementary Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
3.11 Conceptual Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
4
The Eigenvalue Problems
99
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
4.2
Determinants and the Eigenvalue Problem . . . . . . . . . . . . . . . . . . . . . . . 101
4.3
Elementary Operations and Determinants . . . . . . . . . . . . . . . . . . . . . . . 104
4.4
Eigenvalues and the Characteristic Polynomial
. . . . . . . . . . . . . . . . . . . . 108
4.5
Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.6
Complex Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.7
Similarity Transformations & Diagonalization . . . . . . . . . . . . . . . . . . . . . 121
4.8
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
4.9
Supplementary Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.10 Conceptual Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
5
Vector Spaces and Linear Transformations
135
5.1
Introduction (No exercises)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5.2
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5.3
Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.4
Linear Independence, Bases, and Coordinates . . . . . . . . . . . . . . . . . . . . . 144
5.5
Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.6
Inner-products
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
5.7
Linear Transformations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
5.8
Operations with Linear Transformations . . . . . . . . . . . . . . . . . . . . . . . . 158
5.9
Matrix Representations for Linear Transformations . . . . . . . . . . . . . . . . . . 161
5.10 Change of Basis and Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . 166
5.11 Supplementary Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
5.12 Conceptual Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
6
Determinants
175
6.1
Introduction (No exercises)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
6.2
Cofactor Expansion of Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . 175
6.3
Elementary Operations and Determinants . . . . . . . . . . . . . . . . . . . . . . . 178
6.4
Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.5
Applications of Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
6.6
Supplementary Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.7
Conceptual Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

7
Eigenvalues and Applications
193
7.1
Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
7.2
Systems of Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
7.3
Transformation to Hessenberg Form
. . . . . . . . . . . . . . . . . . . . . . . . . . 199
7.4
Eigenvalues of Hessenberg Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
7.5
Householder Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
7.6
QR
Factorization & Least-Squares
. . . . . . . . . . . . . . . . . . . . . . . . . . 208
7.7
Matrix Polynomials & The Cayley-Hamilton Theorem . . . . . . . . . . . . . . . . 211
7.8
Generalized Eigenvectors & Diﬀ. Eqns. . . . . . . . . . . . . . . . . . . . . . . . . . 212
7.9
Supplementary Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
7.10 Conceptual Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

Chapter 1
Matrices and Systems of Equations
1.1
Introduction to Matrices and Systems of Linear Equations
1. Linear.
2. Nonlinear.
3. Linear.
4. Nonlinear.
5. Nonlinear.
6. Linear.
7.
x1 + 3x2
=
7
4x1 −x2
=
2
1 + 3 · 2
=
7
4 · 1 −2
=
2
8.
6x1 −x2 + x3
=
14
x1 + 2x2 + 4x3
=
4
6 · 2 −(−1) + 1
=
14
2 + 2 · (−1) + 4 · 1
=
4
9.
x1 + x2
=
0
3x1 + 4x2
=
−1
−x1 + 2x2
=
−3
1 + (−1)
=
0
3 · 1 + 4 · (−1)
=
−1
−1 + 2 · (−1)
=
−3
10.
3x2
=
9,
4x1
=
8,
3 · 3
=
9
4 · 2
=
8
11. Unique solution.
12. No Solution
13. Inﬁnitely many solutions.

2
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
14. No solution.
15.
(a) The planes do not intersect; that is, the planes are parallel.
(b) The planes intersect in a line or the planes are coincident.
16. The planes intersect in the line x = (1 −t)/2, y = 2, z = t.
17. The planes intersect in the line x = 4 −3t, y = 2t −1, z = t.
18. Coincident planes.
19. A =
· 2
1
6
4
3
8
¸
.
20. C =
· 1
2
7
1
2
2
4
3
¸
.
21. Q =


1
4
−3
2
1
1
3
2
1

.
22.
x1
+2x2
+7x3
=
1
2x1
+2x2
+4x3
=
3
23.
2x1
+
x2
=
6
4x1
+
3x2
=
8
;
x1
+
4x2
=
−3
2x1
+
x2
=
1
3x1
+
2x2
=
1
24. A =
· 1
−1
1
1
¸
, B =
· 1
−1
−1
1
1
3
¸
.
25. A =
· 1
1
−1
2
0
−1
¸
, B =
· 1
1
−1
2
2
0
−1
1
¸
.
26. A =


1
3
−1
2
5
1
1
1
1

, B =


1
3
−1
1
2
5
1
5
1
1
1
3

.
27. A =


1
1
2
3
4
−1
−1
1
1

, B =


1
1
2
6
3
4
−1
5
−1
1
1
2

.
28. A =


1
1
−3
1
2
−5
−1
−3
7

, B =


1
1
−3
−1
1
2
−5
−2
−1
−3
7
3

.

1.1. INTRODUCTION TO MATRICES AND SYSTEMS OF LINEAR EQUATIONS
3
29. A =


1
1
1
2
3
1
1
−1
3

, B =


1
1
1
1
2
3
1
2
1
−1
3
2

.
30. Elementary operations on equations:
E2 −2E1 .
Reduced system of equations:
2x1 + 3x2
=
6
−7x2
=
−5 .
Elementary row operations:
R2 −2R1 .
Reduced augmented matrix:
· 2
3
6
0
−7
−5
¸
.
31. Elementary operations on equations:
E2 −E1, E3 + 2E1 .
Reduced system of equations:
x1 + 2x2 −x3
=
1
−x2 + 3x3
=
1
5x2 −2x3
=
6
.
Elementary row operations:
R2 −R1, R3 + 2R1 .
Reduced augmented matrix:


1
2
−1
1
0
−1
3
1
0
5
−2
6

.
32. Elementary operations on equations:
E1 ↔E2, E3 −2E1 .
Reduced system of equations:
x1 −x2 + 2x3
=
1
x2 + x3
=
4
3x2 −5x3
=
4
.
Elementary row operations:
R1 ↔R2, R3 −2R1 .
Reduced augmented matrix:


1
−1
2
1
0
1
1
4
0
3
−5
4

.
33. Elementary operations on equations:
E2 −E1, E3 −3E1 .
Reduced system of equations:
x1 + x2
=
9
−2x2
=
−2
−2x2
=
−21
.
Elementary row operations:
R2 −R1, R3 −3R1 .
Reduced augmented matrix:


1
1
9
0
−2
−2
0
−2
−21

.

4
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
34. Elementary operations on equations:
E2 + E1, E3 + 2E1 .
Reduced system of equations:
x1 + x2 + x3 −x4
=
1
2x2
=
4
3x2 + 3x3 −3x4
=
4
.
Elementary row operations:
R2 + R1, R3 + 2R1 .
Reduced augmented matrix:


1
1
1
−1
1
0
2
0
0
4
0
3
3
−3
4

.
35. Elementary operations on equations:
E2 ↔E1, E3 + E1 .
Reduced system of equations:
x1 + 2x2 −x3 + x4
=
1
x2 + x3 −x4
=
3
3x2 + 6x3
=
1
.
Elementary row operations:
R2 ↔R1, R3 + R1 .
Reduced augmented matrix:


1
2
−1
1
1
0
1
1
−1
3
0
3
6
0
1

.
36. Elementary operations on equations:
E2 −E1, E3 −3E1 .
Reduced system of equations:
x1 + x2
=
0
−2x2
=
0
−2x2
=
0
.
Elementary row operations:
R2 −R1, R3 −3R1 .
Reduced augmented matrix:


1
1
0
0
−2
0
0
−2
0

.
37. (b) In each case, the graph of the resulting equation is a line.
38. Now if a11 = 0 we easily obtain the equivalent system
a21x1 + a22x2
=
b2
a12x2
=
b1
Thus we may suppose that a11 ̸= 0. Then :
a11x1 + a12x2
=
b1
a21x1 + a22x2
=
b2
½ E2 −(a21/a11)E1
=⇒
¾

1.1. INTRODUCTION TO MATRICES AND SYSTEMS OF LINEAR EQUATIONS
5
a11x1 + a12x2
=
b1
((−a21/a11)a12 + a22)x2
=
(−a21/a11)b1 + b2
½ a11E2
=⇒
¾
a11x1 + a12x2
=
b1
(a11a22 −a12a21)x2
=
−a21b1 + a11b2
Each of a11 and (a11a22 −a12a21) is non-zero.
39. Let
A =
½ a11x1 + a12x2 = b1
a21x1 + a22x2 = b2
¾
and let
B =
½
a11x1 + a12x2 = b1
ca21x1 + ca22x2 = cb2
¾
Suppose that x1 = s1, x2 = s2 is a solution to A . Then a11s1 + a12s2 = b1, and a21s1 +
a22s2 = b2. But this means that ca21s1 + ca22s2 = cb2 and so x1 = s1, x2 = s2 is also a
solution to B . Now suppose that x1 = t1, x2 = t2 is a solution to B . Then a11t1+a12t2 = b1
and ca21t1 + ca22t2 = cb2 . Since c ̸= 0 , a21x1 + a22x2 = b2 .
40. Let
A =
½ a11x1 + a12x2
=
b1
a21x1 + a22x2
=
b2
¾
and let
B =
½
a11x1 + a12x2 = b1
(a21 + ca11)x1 + (a22 + ca12)x2 = b2 + cb1 .
¾
Let x1 = s1 and x2 = s2 be a solution to A . Then a11s1+a12s2 = b1 and a21s1+a22s2 = b2 so
a11s1+a12s2 = b1 and (a21+ca11)s1+(a22+ca12)s2 = b2+cb1 as required. Now if x1 = t1 and
x2 = t2 is a solution to B then a11t1+a12t2 = b1 and (a21+ca11)t1+(a22+ca12)t2 = b2+cb1,
so a11t1 + a12t2 = b1 and a21t1 + a12t2 = b2 as required.
41. The proof is very similar to that of 45 and 46.
42. By adding the two equations we obtain: 2x2
1 −2x1 = 4. Then x1 = 2 or x1 = −1 and
substituting these values in the second equation we ﬁnd that there are three solutions:
x1 = −1, x2 = 0 ; x1 = 2, x2 =
√
3, ; x1 = 2, x2 = −
√
3.

6
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
1.2
Echelon Form and Gauss-Jordan Elimination
1. The matrix is in echelon form.
The row operation R2 −2R1 transforms the matrix to
reduced echelon form
· 1
0
0
1
¸
.
2. Echelon form. R2 −2R1 yields reduced row echelon form
· 1
0
−7
0
1
3
¸
.
3. Not in echelon form. (1/2)R1, R2 −4R1, (−1/5)R2 yields echelon form
· 1
3/2
1/2
0
1
2/5
¸
.
4. Not in echelon form. R1 ↔R2 yields echelon form
· 1
2
3
0
1
1
¸
.
5. Not in echelon form.
R1 ↔R2, (1/2)R1, (1/2)R2 yields the echelon form
· 1
0
1/2
2
0
0
1
3/2
¸
.
6. Not in echelon form.
(1/2)R1 yields the echelon form
· 1
0
3/2
1/2
0
0
1
2
¸
.
7. Not in echelon form.
R2 −4R3, R1 −2R3, R1 −3R2 yields the reduced echelon form


1
0
0
5
0
1
0
−2
0
0
1
1

.
8. Not in echelon form. (1/2)R1, (−1/3)R3 yields the echelon form


1
−1/2
3/2
0
1
1
0
0
1

.
9. Not in echelon form. (1/2)R2 yields the echelon form


1
2
−1
−2
0
1
−1
−3/2
0
0
0
1

.
10. Not in echelon form −R1, (1/2)R2 yields the echelon form


1
−4
3
−4
−6
0
1
1/2
−3/2
−3/2
0
0
0
1
2

.
11. x1 = 0, x2 = 0.
12. The system is inconsistent.
13. x1 = −2 + 5x3, x2 = 1 −3x3, x3 is arbitrary.
14. x1 = 1 −2x3, x2 = 0.

1.2. ECHELON FORM AND GAUSS-JORDAN ELIMINATION
7
15. x1 = 0, x2 = 0, x3 = 0.
16. x1 = 0, x2 = 0, x3 = 0.
17. x1 = x3 = x4 = 0, x2 is arbitrary.
18. The system is inconsistent.
19. The system is inconsistent.
20. x1 = 3x4 −5x5 −2, x2 = x4 + x5 −2, x3 = −2x4 −x5 + 2, x4 and x5 are arbitrary.
21. x1 = −1 −(1/2)x2 + (1/2)x4, x3 = 1 −x4, x2 and x4 arbitrary, x5 = 0.
22. x1 = (5 + 3x2)/2, x2 arbitrary.
23. The system is inconsistent.
24. x1 = x3, x2 = −3 + 2x3, x3 arbitrary.
25. x1 = 2 −x2, x2 arbitrary.
26. x1 = 10 + x2, x2 arbitrary, x3 = −6.
27. x1 = 2 −x2 + x3, x2 and x3 arbitrary.
28. x1 = 2x3, x2 = 1, x3 arbitrary.
29. x1 = 3 −2x3, x2 = −2 + 3x3, x3 arbitrary.
30. x1 = −3x4 −6x5, x2 = 1 + 3x4 + 7x5, x3 = −2x4 −5x5, x4 and x5 arbitrary.
31. x1 = 3 −(7x4 −16x5)/2, x2 = (x4 + 2x5)/2, x3 = −2 + (5x4 −12x5)/2, x4 and x5 arbitrary.
32. x1 = 2, x2 = −1.
33. The system is inconsistent.
34. x1 = 1 −2x2, x2 arbitrary.
35. The system is inconsistent.
36.
x1 + 2x2
=
−3
ax1 −2x2
=
5
½ E1 + E2
=⇒
¾
x1 + 2x2
=
−3
(a + 1)x1
=
2
Hence if a = −1 there is no solution.
37.
x1 + 3x2
=
4
2x1 + 6x2
=
a
½ E2 −2E1
=⇒
¾ x1 + 3x2
=
4
0
=
a −8
Thus, if a ̸= 8 there is no solution.

8
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
38.
2x1 + 4x2
=
a
3x1 + 6x2
=
5
½ E2 −(3/2)E1
=⇒
¾ 2x1 + 4x2
=
a
0
=
5 −(3/2)a
Thus, if a ̸= 10/3 there is no solution.
39.
3x1 + ax2
=
3
ax1 + 3x2
=
5
½ E2 −(a/3)E1
=⇒
¾
3x1 + ax2
=
3
(a2/3 −3)x2
=
5 −a
Thus, if a = ±3 there is no solution.
40.
x1 + ax2
=
6
ax1 + 2ax2
=
4
½ E2 −aE1
=⇒
¾
x1 + ax2
=
6
(2a −a2)x2
=
4 −6a
41. cos α = 1/2 and sin β = 1/2, so α = π/3 or α = 5π/3 and β = π/6 or β = 5π/6.
42. cos2 α = 3/4 and sin2 β = 1/2. The choices for α are π/6, 5π/6, 7π/6, and 11π/6. The
choices for β are π/4, 3π/4, 5π/4, and 7π/4.
43. x1 = 1 −2x3, x2 = 2 + x3, x3 arbitrary. (a) x3 = 1/2. (b) In order for x1 ≥0, x2 ≥0, we
must have −2 ≤x3 ≤1/2; for a given x1 and x2, y = −6 −7x3, so the minimum value is
y = 8 at x3 = −2. (c) The minimum value is 20.
44.
· 1
d
c
b
¸ ½ R2 −cR1
=⇒
¾ · 1
d
0
b −cd
¸ 


R1 −(d/(b −cd))R2
( recall b −cd ̸= 0
=⇒



· 1
0
0
b −cd
¸ ½ 1/(b −cd)R2
=⇒
¾ · 1
0
0
1
¸
.
45.
· 1
x
x
0
1
x
¸
,
· 1
x
x
0
0
1
¸
,
· 1
x
x
0
0
0
¸
,
· 0
1
x
0
0
1
¸
· 0
1
x
0
0
0
¸
,
· 0
0
1
0
0
0
¸
,
· 0
0
0
0
0
0
¸
.
46.
(a)


1
x
0
1
0
0

,


1
x
0
0
0
0

,


0
1
0
0
0
0

,


0
0
0
0
0
0

.
(b)


1
x
x
0
1
x
0
0
1

,


1
x
x
0
1
x
0
0
0

,


1
x
x
0
0
1
0
0
0

,


1
x
x
0
0
0
0
0
0

,


0
1
x
0
0
1
0
0
0

,


0
1
x
0
0
0
0
0
0

,


0
0
1
0
0
0
0
0
0

,


0
0
0
0
0
0
0
0
0

.

1.2. ECHELON FORM AND GAUSS-JORDAN ELIMINATION
9
(c)


1
x
x
x
0
1
x
x
0
0
1
x

,


1
x
x
x
0
1
x
x
0
0
0
1

,


1
x
x
x
0
1
x
x
0
0
0
0

,


1
x
x
x
0
0
1
x
0
0
0
1

,


1
x
x
x
0
0
1
x
0
0
0
0

,


1
x
x
x
0
0
0
1
0
0
0
0

.


1
x
x
x
0
0
0
0
0
0
0
0

,


0
1
x
x
0
0
1
x
0
0
0
1

,


0
1
x
x
0
0
0
1
0
0
0
0

,


0
1
x
x
0
0
0
0
0
0
0
0

,


0
0
1
x
0
0
0
1
0
0
0
0

,


0
0
1
x
0
0
0
0
0
0
0
0

,


0
0
0
1
0
0
0
0
0
0
0
0

,


0
0
0
0
0
0
0
0
0
0
0
0

.
47.
· 1
2
2
3
¸ ½ 2R2
=⇒
¾ · 1
2
4
6
¸ ½ R2 −R1
=⇒
¾ · 1
2
3
4
¸
.
48.
· 1
4
3
7
¸ ½ R2 −3R1
=⇒
¾ · 1
4
0
−5
¸ ½ R1 + (2/5)R2
=⇒
¾
· 1
2
0
−5
¸ ½ (3/5)R2
=⇒
¾ · 1
2
0
−3
¸ ½ R2 + 2R1
=⇒
¾ · 1
2
2
1
¸
.
49.
100x1 + 10x2 + x3
=
15(x1 + x2 + x3)
100x3 + 10x2 + x1
=
100x1 + 10x2 + x3 + 396
x3
=
x1 + x2 + 1
x1 = 1, x2 = 3, and x3 = 5, so N = 135.
50.
a −b + c
=
6
a + b + c
=
4
4a + 2b + c
=
9
a = 2, b = −1, c = 3. So y = 2x2 −x + 3.
51. Let x1, x2, x3 be the amounts initially held by players one, two and three, respectively.
Also assume that player one loses the ﬁrst game, player two loses the second game, and
player three loses the third game. Then after three games, the amount of money held by
each player is given by the following table

10
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
Player
Amount of money
1
4x1 −4x2 −4x3 = 24
2
−2x1 + 6x2 −2x3 = 24
3
−x1 −x2 + 7x3 = 24
Solving yields x1 = 39, x2 = 21, and x3 = 12.
52. The resulting system of equations is
x1 + x2 + x3
=
34
x1 + x2
=
7
x2 + x3
=
22
The solution is x1 = 12, x2 = −5, x3 = 27.
53. If x1 is the number of adults, x2 the number of students, and x3 the number of children,
then x1 + x2 + x3 = 79, 6x1 + 3x2 + (1/2)x3 = 207, and for j = 1, 2, 3, xj is an integer such
that 0 ≤xj ≤79. Following is a list of possiblities
Number of Adults
0
5
10
15
20
25
30
Number of Students
67
56
45
34
23
12
1
Number of Children
12
18
24
30
36
42
48
54. The resulting system of equations is
a + b + c + d
=
5
b + 2c + 3d
=
5
a + 2b + 4c + 8d
=
17
b + 4c + 12d
=
21.
The solution is a = 3, b = 1, c = −1, d = 2. So p(x) = 3 + x −x2 + 2x3.
55. By (7), 1 + 2 + 3 + · · · + n = a1n + a2n2. Setting n = 1 and n = 2 gives
a1 + a2
=
1
2a1 + 4a2
=
3
The solution is a1 = a2 = 1/2, so 1 + 2 + 3 + . . . + n = n(n + 1)/2.
56. By (7), 12 + 22 + 32 + · · · + n2 = a1n + a2n2 + a3n3. Setting n = 1, n = 2, n = 3, gives
a1 + a2 + a3
=
1
2a1 + 4a2 + 8a3
=
5
3a1 + 9a2 + 27a3
=
14
The solution is a1 = 1/6, a2 = 1/2 and a3 = 1/3, so 12+22+32+. . .+n2 = n(n+1)(2n+1)/6.

1.3. CONSISTENT SYSTEMS OF LINEAR EQUATIONS
11
57. The system of equations obtained from (7) is
a1 + a2 + a3 + a4 + a5
=
1
2a1 + 4a2 + 8a3 + 16a4 + 32a5
=
17
3a1 + 9a2 + 27a3 + 81a4 + 242a5
=
98
4a1 + 16a2 + 64a3 + 256a4 + 1024a5
=
354
5a1 + 25a2 + 125a3 + 625a4 + 3125a5
=
979
The solution is a1 = −1/30, a2 = 0, a3 = 1/3, a4 = 1/2, a5 = 1/5. Therefore, 14 + 24 +
34 + · · · + n4 = n(n + 1)(2n + 1)(3n2 + 3n −1)/30.
58. 15 + 25 + 35 + · · · + n5 = n2(n + 1)2(2n2 + 2n −1)/12.
1.3
Consistent Systems of Linear Equations
1. The augmented matrix reduces to


1
1
0
5/6
0
0
1
2/3
0
0
0
0
0
0
0
0

.
n = 3, r = 2, x2 is independent.
2. The augmented matrix reduces to


1
0
−3/2
0
1
2
0
0
0

.
n = 2, r = 2.
3. The augmented matrix reduces to


1
0
4
0
13/2
0
1
−1
0
−3/2
0
0
0
1
1/2

.
n = 4, r = 3, x3 is independent.
4. The augmented matrix reduces to


1
2
3
0
1/3
0
0
0
1
1/3
0
0
0
0
0
0
0
0
0
0

.
n = 4, r = 2, x2 and x3 are independent.
5. n = 2 and r ≤2 so r = 0, n −r = 2; r = 1, n −r = 1; r = 2, n −r = 0. There could be a
unique solution.
6. n = 4 and r ≤3 so r = 0, n −r = 4; r = 1, n −r = 3; r = 2, n −r = 2; r = 3, n −r = 1.
By the corollary to Theorem 3, there are inﬁnitely many solutions.
7. Inﬁnitely many solutions.

12
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
8. Inﬁnitely many solutions.
9. Inﬁnitely many solutions, a unique solution or no solution.
10. Inﬁnitely many solutions, a unique solution, or no solution.
11. A unique solution or inﬁnitely many solutions.
12. Inﬁnitely many solutions or a unique solution.
13. Inﬁnitely many solutions.
14. Inﬁnitely many solutions.
15. Inﬁnitely many solutions or a unique solution.
16. Inﬁnitely many solutions or a unique solution.
17. Inﬁnitely many solutions.
18. Inﬁnitely many solutions.
19. There are nontrivial solutions.
20. There are nontrivial solutions.
21. There is only the trivial solution.
22. There is only the trivial solution.
23. If a = −1 then when we reduce the augmented matrix we obtain a row of zeroes and hence
inﬁnitely many nontrivial solutions.
24.
(a) Reduced row echelon form of the augmented matrix is


1
0
2
−2b1 + 3b2
0
1
−1
b1 −b2
0
0
0
b3 −b1 −2b2

.
Hence, if b3 −b1 −2b2 ̸= 0 then the system is inconsistent. Therefore, the system of
equations is consistent if and only if b3 −b1 −2b2 = 0.
(b) (i) The system is consistent. For example, a solution is x1 = −1, x2 = 1 and x3 = 1.
(ii) The system is inconsistent by part (a). (iii) The system is consistent. For example,
a solution is x1 = 1, x2 = 0 and x3 = 1.
25.
(a) B =


∗
x
x
0
∗
x
0
0
∗
0
0
0

.

1.3. CONSISTENT SYSTEMS OF LINEAR EQUATIONS
13
(b) In the third row of the matrix of 25(a) for B, we need 0·x1 +0·x2 = ∗and, in general,
this can’t be.
26. The resulting system of equations is
3a + b + c
=
0
7a + 2b + c
=
0 .
The general solution is a = c, b = -4c.
Thus x −4y + 1
=
0
is an equation for the line.
27. The resulting system of equations is
2a + 8b + c
=
0
4a + b + c
=
0 .
The general solution is a = (-7/30)c, b = (-1/15)c.
Thus −7x −2y + 30
=
0
is an equation for the line.
28. The resulting system of equations is
16a −4d + f
=
0
4a + 4b + 4c −2d −2e + f
=
0
9c + 3e + f
=
0
a + b + c + d + e + f
=
0
16a + 4d + f
=
0
.
The general solution is:
a = (-1/16)f, b = (-71/144)f, c = (1/18)f, d = 0, e = (-1/2)f.
An equation is 9x2 + 71xy −8y2 + 72y −144
=
0 .
29. The resulting system of equations is
16a −4b + c −4d + e + f
=
0
a −2b + 4c −d + 2e + f
=
0
9a + 6b + 4c + 3d + 2e + f
=
0
25a + 5b + c + 5d + e + f
=
0
49a −7b + c + 7d −e + f
=
0
.
The general solution is:
a = (-3/113)f, b = (3/113)f, c = (1/113)f, d = 0, e = (-54/113)f.
An equation is −3x2 + 3xy + y2 −54y + 113
=
0 .
30. Using equation (4), the given points result in a system of 9 equations in 10 unknowns, with
the solution:
a = (−15/16)j,
b = (−1/16)j,
c = (7/8)j,
d = (15/16)j,
e = (−15/16)j,
f = (1/8)j,
g = (15/8)j,
h = (−15/16)j,
i = (−15/8)j.
An equation is:
−15x2 −y2 + 14z2 + 15xy −15xz + 2yz + 30x −15y −15z + 16
=
0 .
31. Omitted
32. The resulting system of equations is:
2a + b + c + d
=
0
5a + 2b + c + d
=
0
13a + 3b + 2c + d
=
0
.

14
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
The general solution is: a = (1/6)d, b = (−1/2)d, c = (−5/6)d.
Thus, x2 + y2 −3x −5y + 6
=
0 , is an equation for the circle.
33. The resulting system of equations is:
25a + 4b + 3c + d
=
0
5a + b + 2c + d
=
0
4a + 2b + d
=
0
.
The general solution is: a = (7/50)d, b = (−39/50)d, c = (−23/50)d.
Thus, 7x2 + 7y2 −39x −23y + 50
=
0 ,
is an equation for the circle.
1.4
Applications
1.
(a)
x1 + x4
=
1200
x1 + x2
=
1000
x3 + x4
=
600
x2 + x4
=
400
The solution is x1 = 1200 −x4, x2 = −200 + x4, x3 = 600 −x4.
(b) x1 = 1100, x2 = −100, x3 = 500.
(c) 200 ≤x4 ≤600 so 600 ≤x1 ≤1000
2.
(a)
x1
=
1200
x1 + x2
=
1000
x3 + x4
=
1000
x2 + x3
=
800
The solution is x1 = 1200 −x4, x2 = −200 + x4, x3 = 1000 −x4.
(b) x1 = 1100, x2 = −100, x3 = 900.
(c) 200 ≤x4 ≤1000 so 200 ≤x1 ≤1000.
3. x2 = 800, x3 = 400, x4 = 200.
4. x2 = 400, x3 = 700, x4 = 300, x5 = 500, x6 = 100.
5. 4I1 + 3I2 = 2, 3I2 + 4I3 = 4, and I1 + I3 = I2. Therefore, I1 = 1/20, I2 = 3/5, and
I3 = 11/20.
6. I1 +I2 = 7, I1 +2I3 = 3, and I1 +I3 = I2. Therefore, I1 = 18/5, I2 = 17/5, and I3 = −1/5.
7. 5/7, 20/7, 15/7
8. 7/4, 15/8, −13/8, 1/8, 1/8, 27/8.

1.5. MATRIX OPERATIONS
15
9. (a)
x1 −x4
=
a1 −a2
x1 −x2
=
−b1 + b2
−x3 + x4
=
d1 −d2
x2 −x3
=
−c1 + c2
10. Let I1, I2, . . . , I5 be the currents ﬂowing through R1, R2, . . . , R5, respectively. If I5 = 0 then
I1 = I2, I3 = I4, I1R1 −I3R3 = 0, and I2R2 −I4R4 = 0. It follows that either all currents
are zero or R1R4 = R2R3.
1.5
Matrix Operations
1. (a)
· 2
0
2
6
¸
, (b)
· 0
4
2
4
¸
, (c)
· 0
−6
6
18
¸
, (d)
· −6
8
4
6
¸
.
2. (a)
· −2
2
2
4
¸
, (b)
· 6
3
3
9
¸
, (c)
· −2
7
3
5
¸
, (d)
· −2
3
1
1
¸
.
3.
· −2
−2
0
0
¸
.
4.
· −2
1
0
−1
¸
.
5.
· −1
−1
0
0
¸
.
6.
·
2
4
−2
−6
¸
.
7.
(a)
·
3
−3
¸
, (b)
· 3
4
¸
, (c)
· 0
0
¸
.
8.
(a)
· 3
1
¸
, (b)
· −11
18
¸
, (c)
· −5
24
¸
.
9.
(a)
· 2
1
¸
, (b)
· 0
1
¸
, (c)
· 17
14
¸
.
10.
(a)
· −4
13
¸
, (b)
· −15
0
¸
, (c)
·
3
−6
¸
.
11.
(a)
· 2
3
¸
, (b)
· 20
16
¸
.

16
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
12.
(a)
· 0
2
¸
, (b)
· −33
−17
¸
.
13.
a1 = 11/3, a2 = −(4/3) .
14.
a1 = 0, a2 = −2.
15.
a1 = −2, a2 = 0.
16.
a1 = 4/11, a2 = 14/11 .
17.
The equation has no solution.
18.
The equation has no solution.
19.
a1 = 4, a2 = −(3/2).
20.
a1 = 9/11, a2 = −(17/11) .
21.
w1 =
· 0
1
¸
, w2 =
· 1
3
¸
, AB =
· 1
1
3
8
¸
, (AB)r =
· 1
3
¸
22.
w1 =
· −13
−1
¸
, w2 =
· −27
−16
¸
, Q =
· −3
7
1
6
¸
, Qs =
· −27
−16
¸
.
23.
w1 =
· −2
1
¸
, w2 =
· −1
1
¸
, w3 =
· −1
2
¸
, Q =
· −1
4
2
17
¸
,
Q r =
· −1
2
¸
.
24.
w1 =
· 2
1
¸
, w2 =
· −1
3
¸
, w3 =
· −3
8
¸
.
Q =
· −3
−4
8
19
¸
, Q r =
· −3
8
¸
.
25.
· −4
6
2
12
¸
.
26.
·
3
−1
15
30
¸
.
27.
· 4
12
4
10
¸
.
28.
· 0
0
0
0
¸
.

1.5. MATRIX OPERATIONS
17
29.
· 0
0
0
0
¸
.
30.
· 0
0
0
0
¸
.
31.
AB =
· 5
16
5
18
¸
, BA =
· 4
11
6
19
¸
.
32.


50
11
16
10
3
−2
28
4

.
33.
A u =
· 11
13
¸
, v A = [8, 22].
34.
uv =
· 2
4
6
12
¸
, vu = 14.
35.
v Bu = 66 .
36.
Bu =
·
7
13
¸
.
37.
CA =


5
10
8
12
15
20
8
17

.
38.
CB =


3
8
4
8
7
12
5
14

.
39.
C(B) u ) =


27
28
43
47

.
40.
(AB)u =
· 53
59
¸
, A(Bu ) =
· 53
59
¸
.
41.
(BA)u =
· 37
63
¸
, B(Au ) =
· 37
63
¸
.

18
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
42.


x1
x2
x3
x4

=


x3 + 2x4
−2x3 −3x4
x3
x4

= x3


1
−2
1
0

+ x4


2
−3
0
1

.
43.


x1
x2
x3

=


−2 + x3
3 −2x3
x3

=


−2
3
0

+ x3


1
−2
1

.
44


x1
x2
x3
x4

=


−1 + x3
1 −2x3
x3
1

=


−1
1
0
1

+ x3


1
−2
1
0

.
45.


x1
x2
x3
x4
x5


=


x3 + x5
−2x3 −x5
x3
−x5
x5


= x3


1
−2
1
0
0


+ x5


1
−1
0
−1
1


.
46.


x1
x2
x3
x4
x5


=


1 + x3 + 2x4 + 3x5
−2x3 −3x4 −4x5
x3
x4
x5


=


1
0
0
0
0


+ x3


1
−2
1
0
0


+ x4


2
−3
0
1
0


+ x5


3
−4
0
0
1


.
47.


x1
x2
x3
x4
x5


=


x3 + 2x4 + 3x5
−2x3 −3x4 −4x5
x3
x4
x5


= x3


1
−2
1
0
0


+ x4


2
−3
0
1
0


+ x5


3
−4
0
0
1


.
48.


x1
x2
x3
x4
x5
x6


=


x3 + x5 + 2x6
−2x3 −x5 −2x6
x3
−x5 −x6
x6


= x3


1
−2
1
0
0
0


+ x5


1
−1
0
−1
1
0


+ x6


2
−2
0
−1
0
1


.

1.5. MATRIX OPERATIONS
19
49.


x1
x2
x3
x4
x5


=


x2 + 2x4
x2
−2x4
x4
0


= x2


1
1
0
0
0


+ x4


2
0
−2
1
0


.
50.
A(Bu ) has 8 multiplications while (AB)u
has 12 multiplications.
51.
C(A(B u )) has 12 multiplications, (CA)(Bu ) has 16 multiplications, [C(AB)](u ) has
20 multiplications, and C[(AB)u ] has 16 multiplications.
52.
(a)
A1 =
· 2
1
¸
, A2 =
· 3
4
¸
, D1 =


2
2
1
1

, D2 =


1
0
−1
3

,
D3 =


3
0
1
1

, D4 =


6
4
−1
2

.
(b)
A1
is in R2, D1
is in R4.
(c)
AB1 =
· 5
5
¸
, AB2 =
· 16
18
¸
, AB =
· 5
16
5
18
¸
.
53.
(a)
AB is a 2 x 4 matrix, BA is not deﬁned.
(b)
AB is not deﬁned, BA is not deﬁned.
(c)
AB is not deﬁned. BA is a 6 x 7 matrix.
(d)
AB is a 2 x 2 matrix, BA is a 3 x 3 matrix.
(e)
AB is a 3 x 1 matrix, BA is not deﬁned.
(f)
A(BC) and (AB)C are 2 x 4 matrices.
(g)
AB is a 4 x 4 matrix. BA is a 1 x 1 matrix.
54.
(AB)(CD) is a 2 x 2 matrix, A(B(CD)) and ((AB)C)D are
2 x 2 matrices.
55. A2 = AA provided A is a square matrix.
56. Since b ̸= 0 is arbitrary in B, the equation has inﬁnitely many solutions.
57.
(a) Px =


135, 000
120, 000
45, 000

is the state vector after one year and P 2x =


126, 000
132, 000
42, 000

is the
state vectore after two years.

20
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
(b) P nx
58.
(a) Setting AB = BA yields the system of equations 3b −2c = 0, 2a + 3b −2d = 0, and
3a + 3c −3d = 0. The solution is a = −c + d and b = 2c/3, so B =
· −c + d
2c/3
c
d
¸
.
(b) B =
· −2
2
3
1
¸
and C =
· 0
0
1
1
¸
are possible choices for B and C.
59. Let A be an (m × n) matrix and B be a (p × r) matrix. Since AB is deﬁned, n = p and
AB is an (m × r) matrix. But AB is a square matrix, so m = r. Thus, B is an (n × m)
matrix, so BA is deﬁned and is an (n × n) matrix.
60. Let B = [B1, B2, . . . , Bs]. Then AB = [AB1, AB2, . . . , ABs].
(a) If Bj = θ then the jth column of AB is ABj = θ.
(b) If Bi = Bj then ABi = ABj.
61. (a)
(i)
A =
· 2
−1
1
1
¸
, x =
· x1
x2
¸
, b =
· 3
3
¸
.
(ii)
A =


1
−3
1
1
−2
1
0
1
−1

, x =


x1
x2
x3

, b =


1
2
−1

.
(b)
(i)
x1
· 2
1
¸
+ x2
· −1
1
¸
=
· 3
3
¸
.
(ii)
x1


1
1
0

+ x2


−3
−2
1

+ x3


1
1
−1

=


1
2
−1

.
(c)
(i)
x1 = 2, x2 = 1, 2A1 +A2 = b .
(ii)
x1 = 2, x2 = 1, x3 = 2, 2A1 +A2 +2A3 = b .
62.
· 1
1
2
1
2
3
¸ ½ R2 −R1
=⇒
¾ · 1
1
2
0
1
1
¸
.
Thus x1 = 1 and x2 = 1 .
63.
(a) We solve each of the systems
(i)
Ax =
· 1
0
¸
,
(ii)
) Ax =
· 0
1
¸
.
(i) x =
·
2
−1
¸
; (ii) x =
· −1
1
¸
.

1.6. ALGEBRAIC PROPERTIES OF MATRIX OPERATIONS
21
(b)
B =
·
2
−1
−1
1
¸
andAB = I = BA.
64.
The ith component of A x
is the Pn
j=1 aijxj. Now the ith components of x1A1 , x2A2
, . . . , xnAn
are x1ai1, x2ai2 . . . , xnain,
respectively. Thus the ith
component of x1A1
+x2A2 + · · · + xnAn
is Pn
j=1 aijxj
as required.
65.
(a) B =
· −1
6
1
0
¸
.
(b) No B exists. (c) B =
·
2
2
−1
−1
¸
.
66.
If A = (aij) and B = (bij) then
AB =


a11b11
a11b12 + a12b22
a11b13 + a12b23 + a13b33
0
a22b22
a22b23 + a23b33
0
0
a33b33

.
67.
Let A = (aij) and B = (bij) be upper triangular (n x n) matrices. Then the ijth entry
of AB equals Pn
k=1 aikbkj. Suppose i > j. If k > j then bkj = 0. If j ≥k then i > k so
aik = 0. Thus the ijth component of AB equals zero.
68.


x1
x2
x3
x4
x5


=


4 −3x2 −22x5
x2
6 −9x5
5 −x5
x5


=


4
0
6
5
0


+ x2


−3
1
0
0
0


+ x5


−22
0
−9
−1
1


.
69.


x1
x2
x3
x4
x5


=


5 + 2x4 −3x5
4 −3x4 −2x5
2 −x4 −x5
x4
x5


=


5
4
2
0
0


+ x4


2
−3
−1
1
0


+ x5


−3
−2
−1
0
1


.
70.


x1
x2
x3
x4
x5
x6


=


2 −x2 −2x6
x2
3 −x6
2 −2x6
3 −5x6
x6


=


2
0
3
2
3
0


+ x2


−1
1
0
0
0
0


+ x6


−2
0
−1
−2
−5
1


.
1.6
Algebraic Properties of Matrix Operations
1. DE =
·
8
15
11
18
¸
, EF =
· 9
9
5
5
¸
, (DE)F = D(EF) =

22
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
· 23
23
29
29
¸
.
2. FE =
· 5
9
5
9
¸
, ED =
· 12
27
7
14
¸
, F(ED) = (FE)D =
· 19
41
19
41
¸
.
3. DE =
·
8
15
11
18
¸
, ED =
· 12
27
7
14
¸
.
4. EF =
· 9
9
5
5
¸
, FE =
· 5
9
5
9
¸
.
5. Fu =
· 0
0
¸
, Fv =
· 0
0
¸
.
6. 3Fu = 3
· 0
0
¸
=
· 0
0
¸
, 7Fv = 7
· 0
0
¸
=
· 0
0
¸
.
7. AT =
· 3
4
2
1
7
6
¸
.
8. DT =
· 2
1
1
4
¸
.
9. ETF =
· 5
5
9
9
¸
.
10. ATC =
· 34
15
28
20
56
32
37
35
¸
.
11. (F v )T =
£
0
0
¤
.
12. (EF) v =
· 0
0
¸
.
13. −6.
14. 0.
15. 36.
16. 0.
17. 2.

1.6. ALGEBRAIC PROPERTIES OF MATRIX OPERATIONS
23
18. 18.
19.
√
2.
20. 3
√
10.
21.
√
29.
22. 4
√
2.
23. 0.
24. 0.
25. 2
√
5
26.
Let A =
· 0
1
0
0
¸
and let B =
· 1
0
0
0
¸
.
Then (A −B)(A + B) =
· −1
−1
0
0
¸
and
A2 −B2 =
· −1
0
0
0
¸
.
27.
Let A =
· 1
0
0
0
¸
and let B =
· 1
0
0
1
¸
.
Then A2 = AB and A ̸= B.
28.
The argument depends upon the ”fact” that if the product of two matrices is O
then
one of the factors must be O.
This is not true. Let A =
· 0
1
0
0
¸
.
and B =
· 1
0
0
0
¸
.
Then A2 = O = AB
and neither of A or B
is O.
29. D
and F
are symmetric.
30.
Let A =
· 1
1
1
0
¸
and let B =
· 0
1
1
1
¸
. Then each of A and B are symmetric and
AB =
· 1
2
0
1
¸
is not symmetric.
31.
If each of A and B are symmetric, then a necessary and suﬃcient condition that AB be
symmetric is that AB = BA.
32. xT Gx =
£
x1
x2
¤ · 2
1
1
1
¸ · x1
x2
¸
= x2
1 + (x1 + x2)2. This term is always greater than
zero whenever x1 and x2 are not simultaneously zero.
33. xT Dx= [x2, x2]
· 2
2
1
4
¸ · x1
x2
¸
= x2
1 + 3x2
2 + (x1 + x2)2. This term is always greater than
zero whenever x1 and x2 are not simultaneously zero.

24
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
34. xT Fx= [x1, x2]
· 1
1
1
1
¸ · x1
x2
¸
= (x1 + x2)2. Then xT Fx= 0 if and only if x1 + x2 = 0.
35.
· −3
3
3
−3
¸
.
36.
· 0
0
0
0
¸
.
37.
· −27
−9
27
9
¸
.
38.
·
9
3
−9
−3
¸
.
39.


−12
18
24
18
−27
−36
24
36
−48

.
40.


−12
18
24
12
−18
−24
24
−36
−48

.
41.
(a) xTa
= 6 means that x1 + 2x2 = 6 and xTb = 2 means that 3x1 + 4x2 = 2. Thus
x1 = −10, x2 = 8 and x =
· −10
8
¸
.
(b) xT(a + b) = 12
and x Ta = 2
yields 4x1 + 6x2 = 12
and x1 + 2x2 = 2.
Thus
x1 = 6, x2 = −2 and x =
·
6
−2
¸
.
42.
(a)
· 1
3
0
1
¸
.
(b)
·
5
11
−3
−7
¸
.
(c) BC1 =
· 14
18
¸
, BT
1 C =
£
6
8
¤
, (BC1)TC2 = 132, ∥CB2 ∥=
2
√
337.
43.
(b) A5u = 25u = 32u =


32
96
64


(c) Anu = 2nu. Property 3 is required. For example, A2u = A(Au) = A(2u) = 2(Au) =
2(2u) = 22u.

1.6. ALGEBRAIC PROPERTIES OF MATRIX OPERATIONS
25
44.
(a) By property (3) there exists an (m × n) matrix O such that A + O = A.
(b) By property (4) there exists an (m × n) matrix D such that C + D = O.
Thus,
A = A + O = A + (C + D).
(c) Since matrix addition is associative (property 2), A = A + (C + D) = (A + C) + D.
Now A + C = B + C by assumption so, by substitution, A = (B + C) + D.
(d) Since matrix addition is associative, this becomes A = B + (C + D).
(e) By choice of D, C + D = O, so A = B + O.
(f) But B + O = B so A = B.
45.
(a) Theorem 9, part(2)
(b) Theorem 8, part(3)
(c) Theorem 9, part(3)
46. Using Theorem 10, it can be seen that yT x = (xT y)T = 0T = 0.
Thus ∥x −y ∥=
q
(x −y)T (x −y) =
p
(xT −yT )(x −y) =
p
xT x −xT y −yT x + yT y=
p
∥x∥+ ∥y∥=
√
2.
47. (A + AT )T = AT + (AT )T = AT + A = A + AT .
49.
(a) QT is a (n x m) matrix, QTQ is a n x n matrix and QQT is a m x m matrix. Now
(QTQ)T = QT(QT)T = QTQ so QTQ is symmetric. A similar argument shows that
QQT is symmetric.
(b) (ABC)T = ((AB)C)T = CT(AB)T = CT(BTAT) = CTBTAT.
50. 0 ≤∥Qx∥2= (Qx)T(Qx ) = x TQTQx.
51.
Property 2. Let A = (aij), B = (bij) and C = (cij). The (ij)th com-
ponent of (A + B) + C is (aij + bij) + cij whereas the (ij)th component of A + (B + C)
is aij + (bij + cij). The two are clearly equal.
Property 3. Let O denote the (m x n) matrix with all zero entries. Clearly A + O= A for
every (m x n) matrix A.
Property 4. If A = (aij) then set P = (−aij). Clearly A + P = O.
52.
Let A = (aij), B = (bij), C = (cij), AB = (dij), and BC = (eij).
The (rs)th entry of
(AB)C
is Pp
k=1 drkcks, where drk = Pn
j=1 arjbjk.
Thus the (rs)th entry of (AB)C
is
Pp
k=1(Pn
j=1 arjbjk)cks =
Pp
k=1
Pn
j=1 arjbjkcks = Pn
j=1 arj(Pp
k=1 bjkcks) = Pn
j=1 arjejs. The last sum is the (rs)th
entry of A(BC) so it follows that (AB)C = A(BC).

26
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
53.
Property 2: If A = (aij) then the (ij)th entry of r(sA) is r(saij).
Similarly the (ij)th entry of (rs)A is (rs)aij. The two are clearly equal.
Property 3: Let A = (aij) and B = (bij). The (ij)th
entry of r(AB) is r Pn
k=1 aikbkj.
The (ij)th
entry of (rA)B
is Pn
k=1(raik)bkj.
Finally, the (ij)th
entry of A(rB)
is
Pn
k=1 aik(rbkj). The three are equal so r(AB) = (rA)B = A(rB).
54.
Property 2: Let A = (aij), B = (bij) and C = (cij). The (rs)th entry of A(B + C) is
Pn
k=1 ark(bks + cks) = Pn
k=1 arkbks + Pn
k=1 arkcks. The last expression in the (rs)th entry
of AB + AC so A(B + C) = AB + AC.
Property 3: The (ij)th entry of (r + s)A is (r + s)aij. The (ij)th
entry of rA + sA is
raij + saij. The entries are equal so (r + s)A = rA + sA.
Property 4: The (ij)th entry of r(A + B) is r(aij + bij). The (ij)th
entry of rA + rB is raij + rbij. Since the entries are equal r(A + B) = rA + rB.
55.
Property 1: Let A = (aij), B = (bij), and A + B = (cij), where cij = aij + bij. The (rs)th
entry of (A + B)T is csr = asr + bsr. But asr
is the (rs)th entry of AT and bsr
is the
(rs)th entry of BT. Thus asr + bsr is the (rs)th entry of AT + BT.
Property 3: Let A = (aij), AT = (dij). and (AT)T = (eij). Thus ers = dsr = ars; that is,
(AT)T = A.
56. n = 2, m = 3.
57. n = 5, m = 7.
58. n = m = 4.
59. n = 4, m = 6.
60. n = 4, m = 2.
61. n = m = 5.
1.7
Linear Independence and Nonsingular
Matrices
1. x1v1 + x2v2 = θ has only the trivial solution so {v1 , v2 }
is linearly independent.
2.
Linearly dependent. v3 = 2v1.
3.
Linearly dependent. v5 = 3v1.
4. x1v2 +x3v3 = θ has only the trivial solution so {v2 , v3 }
is linearly independent.

1.7. LINEAR INDEPENDENCE AND NONSING. MATRICES
27
5.
Linearly dependent. v3 = 2v1 .
6.
Linearly dependent. v3 = 2 v1 −2 v4.
7.
Linearly dependent. u4 = 4 u5 .
8. x1u3 +x2u4 = θ has only the trivial solution. So {u3 , u4 }
is linearly independent.
9. x1u1+x2u2+x3u5= θ has only the trivial solution so {u1, u2, u5} is linearly independent.
10.
Linearly dependent. u4 = 4 u5 .
11.
Linearly dependent. u4 = 4 u5 .
12. x1u1+x2u2+x3u4= θ has only the trivial solution so {u1, u2, u4} is linearly independent.
13.
Linearly dependent. u4 = (16/5)u0 +(12/5)u1 −(4/5)u2 .
14.
Linearly dependent. u4 = (16/5)u0 +(4/5)u2 +(4/5)u3.
15.
Sets 5, 6, 13, and 14 are linearly dependent by inspection.
16. A
is nonsingular.
17. B
is singular, x1 = −2x2.
18. C
is nonsingular.
19. AB
is singular, x1 = −2x2.
20. BA
is singular, 7x1 = −10x2.
21. D
is singular, x1 = x2 = 0, x3 arbitrary.
22. F
is nonsingular.
23. D + F
is nonsingular.
24. E
is singular, x1 arbitrary, x2 = 0 = x3.
25. EF
is singular, x1 arbitrary, x2 = 0 = x3.
26. DE
is singular, x1 arbitrary, x2 = 0 = x3.
27. F T
is nonsingular.
28. {v1 , v2}
is linearly dependent if a = 3/2.
29. a = 6.

28
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
30. {v1, v2, v3}
is linearly dependent if a = 1.
31. {v1, v2, v3}
is linearly dependent if b(a −2) = 4.
32. {v1, v2 }
is linearly dependent if 3a = b.
33. {v1, v2 }
is linearly dependent if c = ab.
34. x =
·
0
1/2
¸
, v1 = (1/2)A2.
35. x =
· 0
1
¸
, v3 = A2 .
36. x =
· −1/2
1/2
¸
, v4 = (−1/2)C1 +(1/2)C2 .
37. x =
· 1/2
1/2
¸
, v2 = (1/2)(C1 + C2) .
38. x =


−2/3
4/3
−1

, u1 = (−2/3)F1 +(4/3)F2 −F3 .
39. x =


−8/3
−2/3
3

u3 = (−8F1 −2F2 +9F3 )/3.
40. 8
· 1
2
¸
−3
· 2
3
¸
=
· 2
7
¸
.
41. −11
· 1
2
¸
+ 7
· 2
3
¸
=
·
3
−1
¸
.
42. 8
· 1
2
¸
−4
· 2
3
¸
=
· 0
4
¸
.
43. 0
· 1
2
¸
+ 0
· 2
3
¸
=
· 0
0
¸
.
44. 1
· 1
2
¸
+ 0
· 2
3
¸
=
· 1
2
¸
.
45. −3
· 1
2
¸
+ 2
· 2
3
¸
=
· 1
0
¸
.

1.8. DATA FITTING, NUMERICAL INTEGRATION
29
46.
(a) Since v2 = −2v1 the set S is linearly dependent for any value of a.
(b) If a = −3 then v3 = v1 −v2.
47.
(a) The set S is linearly dependent for any value of a.
(b) The vector v3 can be written as a linear combination of v1 and v2 for any value of a.
48. A nontrivial solution is: 1v1 +0v2 +0v3 = θ.
49. 0 = θT θ = (a1v1 + a2v2 + a3v3)T (a1v1 + a2v2 + a3v3) = a2
1 ∥v1 ∥2 +a2
2 ∥v2 ∥2 +a2
3 ∥
v3 ∥2, so ai = 0, i = 1, 2, 3
50. If a1v1 + a2v2 + a3v3 = θ, where some ai ̸= 0, then a1v1 + a2v2 + a3v3 + 0v4 = θ.
51. If θ = a1v1 +a2(v1 +v2)+a3(v1 +v2 +v3) then θ = (a1 +a2 +a3)v1 +(a2 +a3)v2 +a3v3.
Since {v1, v2, v3} are linearly iindependent, a1 + a2 + a3 = 0, a2 + a3 = 0, and a3 = 0. It
follows that a1 = a2 = a3 = 0.
52. AB = [AB1, . . . , ABn] = O, so ABi = θ for 1 ≤i ≤n. Since A is nonsingular, Bi = θ for
1 ≤i ≤n, so B = O.
53. If AB = AC then A(B −C) = O. By Exercise 50, B −C = O. Therefore, B = C.
54. Suppose, b = [b1, . . . , bn−1]T . If c = [b1, . . . , bn−1, −1]T then Bc = b1A1 +· · ·+bn−1An−1 −
Ab = Ab −Ab = θ.
55.
If x1
is a nontrivial vector such that Bx1 = θ,
then ABx1 =
Aθ= θ.
56.
By Theorem 12, A = [w1 , w2 ]
is a nonsingular matrix. By Theorem 13, Ax = b
has
a (unique) solution.
57.
Let v
be any vector such that ATv= θ.
By Theorem 13, there exists a vector w,
such
that Aw= v.
Then AT(Aw) = ATv= θ,
and so w T(ATAw) = wT θ = 0.
Then (Aw
)T(Aw ) = w TATAw = 0
and ∥Aw ∥= 0.
Thus Aw =θ,
and since A
is nonsingular,
w = θ.
Thus Aw = A θ = θ.
But then v
had to be θ and AT
is nonsingular.
1.8
Data Fitting, Numerical Integration
& Diﬀerentiation
1. p(t) = (−1/2)t2 + (9/2)t −1.
2. p(t) = t2 −4t + 1.
3. p(t) = 2t + 3.

30
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
4. p(t) = 3t + 2.
5. p(t) = 2t3 −2t2 + 3t + 1.
6. p(t) = t3 + t2 + 1.
7. y = 2e2x + e3x.
8. y = −ex−1 + 2e3(x−1).
9. y = 3e−x + 4ex + e2x.
10. y = 2ex −4e2x + e3x.
11.
R 3h
0
f(t)dt ≈3h
2 [f(h) + f(2h)].
12.
R h
0 f(t)dt ≈h
2[f(0) + f(h)].
13.
R 3h
0
f(t)dt ≈3h
8 [f(0) + 3f(h) + 3f(2h) + f(3h)].
14.
R 4h
0
≈
8h
3 f(h) −4h
3 f(2h) + 8h
3 f(3h).
15.
R h
0 f(t)dt ≈h
2[−f(−h) + 3f(0)].
16.
R h
0 f(t)dt ≈−h
12f(−h) + 2h
3 f(0) + 5h
12f(h).
17. f′(0) ≈−1
h f(0) + 1
hf(h).
18. f′(0) ≈−1
h f(−h) + 1
hf(0).
19. f′(0) ≈−3
2hf(0) + 2
hf(h) + −1
2h f(2h).
20. f′(0) ≈−11
6h f(0) + 3
hf(h) −3
2hf(2h) + 1
3hf(3h).
21. f′′(0) ≈
1
h2 [f(−h) −2f(0) + f(h)].
22. f′′(0) ≈
1
h2 [f(0) −2f(h) + f(2h)].
23.


1
1
1
b −a
a
t
b
(b2 −a2)/2
a2
t2
b2
(b3 −a3)/3




1
1
1
b −a
0
t −a
b −a
(b2 + a2 −2ab)/2
0
t2 −a2
b2 −a2
(b3 −a3 −3(b −a)a2)/3




1
1
1
b −a
0
1
2
b −a
0
t2 −a2
b2 −a2
(b3 −a3 −3(b −a)a2)/3



1.8. DATA FITTING, NUMERICAL INTEGRATION
31


1
1
1
b −a
0
1
2
b −a
0
0
(b −a)2/2
(b −a)3/12




1
1
1
b −a
0
1
2
b −a
0
0
6
b −a


24.


1
1
1
0
a −h
a
a + h
1
(a −h)2
a2
(a + h)2
2a




1
1
1
0
0
h
2h
1
0
h(2a −h)
4ah
2a




1
1
1
0
0
h
2h
1
0
0
2h2
h

.
25.
By Rolle’s Theorem there exist u1
and u2
such that t0 < u1 < t1 < u2 < t2
and
p′(u1) = p′(u2) = 0. Since p′(u1) = p′(u2) = 0, u1 < u2,
and p′(t) = 2at + b,
it follows
that b = 0 = a.
Finally, p(t0) = 0 means c = 0.
26.
Suppose we have seen that a nonzero polynomial of degree n −1 can have at most n −1
distinct real zeros. Now assume that p(t) has n + 1 zeros; that is there exist real numbers
t0, t1, . . . , tn such that t0 < t1 < · · · < tn and p(ti) = 0 for 0 ≤i ≤n. By Rolle’s Theorem
there are real numbers u1, . . . , un
such that ti−1 < ui < ti,
for 1 ≤i ≤n,
and such
that p′(ui) = 0 for each i. Now p′(t) = nantn−1 + · · · + a1
and p′(t) has n zeros. By
assumption p′(t) is the zero polynomial. Thus 0 = a1 = · · · = an. This leaves p(t) = a0;
but p(t0) = 0 so a0 = 0. Therefore p(t) is the zero polynomial.
27.
We must solve the system Lx = b
where L =


0
0
0
1
0
0
1
0
1
1
1
1
3
2
1
0


x =


a
b
c
d

, b =


2
3
8
10

. p(t) = t3 + 2t2 + 3t + 2.
28. p(t) = t2 + 2t + 1.
29. p(t) = t3 + t2 + 4t + 3.
30. p(t) = 2t3 −2t + 3.
31.
By Rolle’s Theorem there exists a number s such that t0 < s < t1 and p′(s) = 0. Thus
p′(t) has three zeros. By Exercise 25, p′(s) is the zero polynomial. It follows that p(t) is
the zero polynomial.
32.
The coeﬃcient matrix of p(t) = at3 + bt2 + ct + d must satisfy
Lx = b

32
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
where L =


t3
0
t2
0
t0
1
3t2
0
2t0
1
0
t3
1
t2
1
t1
1
3t2
1
2t1
1
0

x =


a
b
c
d

b =


y0
s0
y1
s1

.
Suppose Lx0 =θ, where x0 = [a, b, c, d]T. If p(t) = at3 + bt2 + ct + d then p(t0) = p(t1) = 0
and p′(t0) = p′(t1) = 0. By Exercise 31 a = b = c = d = 0; that is x0 = θ. This proves
that L is nonsingular so by Theorem 13, Lx = b
has a unique solution.
33.
First supppose that p(ti) = p′(ti) = 0
for 0 ≤i ≤n.
Since p(ti−1) = 0 = p(ti),
it
follows from Rolle’s Theorem that there is a real number ui such that ti−1 < ui < ti and
p′(ui) = 0. Therefore p′(t) has 2n + 1 zeros, t0, t1, . . . , tn, u1, . . . , un. By Exercise 26, p′(t)
is the zero polynomial and it follows that p(t) is the zero polynomial.
Now set p(t) = P2n+1
k=0 aktk
and assume that p(ti) = yi
and p′(ti) = si
for 0 ≤i ≤n.
These constraints yield a system of equations Lx=b, where
L =


t2n+1
0
t2n
0
. . .
t0
1
(2n + 1)t2n
0
2nt2n−1
0
. . .
1
0
...
t2n+1
n
t2n
n
. . .
tn
1
(2n + 1)t2n
n
2nt2n−1
n
. . .
1
0


,
x = [a2n+1, a2n, . . . , a1, a0]T
and b = [y0, s0, . . . , yn, sn]T.
Suppose Lx0 = θ,
where x0
= [b2n+1, b2n, . . . , b1, b0]T. If we set q(t) =
P2n+1
k=0 bktk then it follows that q(ti) = q′(ti) = 0 for 0 ≤i ≤n. As we have shown above,
this implies that b2n+1 = b2n = · · · = b1 = b0 = 0. In particular x0 = θ and it follows that
L is nonsingular. By Theorem 13 the system Lx=b has a unique solution.
34.
R 5h
0
f(x)dx ≈5h
24[ 19
12f(0) + 25
4 f(h) + 25
6 f(2h) + 25
6 f(3h) + 25
4 f(4h) + 19
12f(h)].
35. f′(a) ≈
1
12h[f(a −2h) −8f(a −h) + 8f(a + h) −f(a + 2h)]
1.9
Matrix Inverses and their Properties
5. (cf. Ex. 2) x = A−1b = Bb =
·
1
−1
−.2
.3
¸· 6
9
¸
=
· −3
1.5
¸
.
6. (cf. Ex. 1) x = A−1b = Bb =
·
3
−4
−5
7
¸· 5
2
¸
=
·
7
−11
¸
.
7. (cf. Ex. 3) x = B−1b = Ab =


−1
−2
11
1
3
−15
0
−1
5




4
2
2

=


14
−20
8

.

1.9. MATRIX INVERSES AND THEIR PROPERTIES
33
8. (cf. Ex. 4) x = B−1b = Ab =


1
0
0
2
1
0
3
4
1




2
3
2

=


2
7
20

.
9.
If B is any 3 x 3 matrix, then the (1, 1)th entry of AB is zero and so AB ̸= I.
10.
The (1, 1)th entry of BA is zero.
11.
Let B = (xij) be a (3 x 3) matrix and suppose that BA = I. Then the (1, 1)th entry of
BA must be one and the (1, 2)th entry of BA be must be zero. But each of these entries
equals 2x11 + x12 + 3x13 and cannot simultaneously be one and zero.
12.
The (1, 1)th and the (2, 1)th entry of AB cannot be simultaneously be zero and one.
13.
·
3
−1
−2
1
¸
.
14.
· −7/4
3/4
3/2
−1/2
¸
.
15.
· −1/3
2/3
2/3
−1/3
¸
.
16.


0
1
3
5
5
4
1
1
1

.
17.


1
0
0
−2
1
0
5
−4
1

.
18.


1
11
−7
0
−7
4
0
2
−1

.
19.


1
−2
0
3
−3
−1
−6
7
2

.
20.


−35
−16
26
−17
−15
−7
11
−7
4
2
−3
2
−2
−2
2
−1

.

34
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
21.


−1/2
−2/3
−1/6
7/6
1
1/3
1/3
−4/3
0
−1/3
−1/3
1/3
−1/2
1
1/2
1/2

.
22.
−1
5
·
1
−2
−1
−3
¸
.
23. ∆= 10 so A−1 =
1
10
·
3
2
−2
2
¸
.
24.
−1
7
·
1
−3
−2
−1
¸
.
25. ∆= 0 so A−1 does not exist.
26. A−1 does not exist.
27. λ ̸= 2, −2
28. λ ̸= 2
29. x = A−1 b =
·
2
−1
−3
2
¸ · 4
2
¸
=
·
6
−8
¸
.
30.
· −4
4
¸
.
31. x = A−1b =
· 4
−1
3
−1
¸ · 5
2
¸
=
· 18
13
¸
.
32.
·
17
−11
¸
.
33. x = A−1b = 1
10
· 3
−1
1
3
¸ · 10
5
¸
=
· 5/2
5/2
¸
.
34. x =
·
6.8
−3.2
¸
.
35. Q−1 = C−1A−1 =
· −3
1
3
5
¸
.
36. Q−1 = A−1C−1 =
· −2
5
2
4
¸
.

1.9. MATRIX INVERSES AND THEIR PROPERTIES
35
37. Q−1 = (A−1)T =
· 3
0
1
2
¸
.
38. Q−1 = C−1(A−1)T =
· −1
1
1
2
¸ · 3
0
1
2
¸
=
· −2
2
5
4
¸
.
39. Q−1 = (A−1)T(C−1)T =
· 3
0
1
2
¸ · −1
1
1
2
¸
=
· −3
3
1
5
¸
.
40. Q−1 = A−1B =
· 5
7
4
2
¸
.
41. Q−1 = BC−1 =
·
1
5
−1
4
¸
.
42. Q−1 = B =
· 1
2
2
1
¸
.
43. Q−1 = 1
2A−1 =
· 3/2
1/2
0
1
¸
.
44. Q−1 = 1
10C−1 =
· −1/10
1/10
1/10
1/5
¸
.
45. Q−1 = B(C−1A−1) =
·
3
11
−3
7
¸
.
46. B = A−1D =
· −4
6
7
3
−4
−4
¸
, C = EA−1 =


8
−3
1
0
−6
3

.
47. B = A−1D =


1
10
15
12
3
3

, C = EA−1, =
· 13
12
8
2
3
5
¸
.
48. a ̸= −1.
49. (AB)−1 = B−1A−1 =


2
35
1
14
35
34
23
12
70


(3A)−1 = (1/3)A−1 =


1/3
2/3
5/3
1
1/3
2
2/3
8/3
1/3



36
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
(AT )−1 = (A−1)T =


1
3
2
2
1
8
5
6
1


50. A2 = AB + 2A = A(B + 2I) so A = B + 2I =


4
1
−1
0
5
2
−1
4
3


51. I
52.
(a) X = I and X = −I
(d) The equation (X −I)(X +I) = O does not require that either X −I = O or X +I = O.
53. ATA =
· u1
u2
v1
v2
¸ · u1
v1
u2
v2
¸
=
· uTu
uTv
uTv
vTv
¸
.
54. AA = (I −uuT )(I −uuT ) = I 2 −2uuT +(uuT )(uuT ) = (I −2uuT +u (uTu )uT = I −2uuT
+uuT =
I −uuT = A.
55. I = AA−1 = (AA)A−1 = A(AA−1) = AI = A.
56. Symmetry: AT = (I −avvT )T = I T −a(vvT )T =
I −a(v T Tv T) = I −avvT = A.
AA = (I −avvT )(I −avvT ) = I −2avvT +a2(vvT )(vvT ) =
I −2avvT +a[2/vvT ](vvT vvT ) =
I −2avvT +(2avvT ) = I.
57. Ax = I x −a(vvT )x = x −av (vTx ) = x −a(vTx )v = x −λv where λ = a(vTx ).
58. ∥Ax∥=
p
(Ax)T(Ax) =
p
(xTAT)(Ax) =
p
xT(ATA)x =
√
xTI x =
√
xTx =∥x∥.
59. A(I −auvT ) = (I + uvT )(I −auvT ) = I2 + uvT −auvT −
a(uvT )(uvT ) = I + uvT −auvT −au(vTu )vT =
I + uvT −auvT (1 + vTu ) = I + uvT −uvT = I.
60. AB = A(A2 −2A + 3I) = A3 −2A2 + 3A −I + I = θ+I = I.
61. AB = A(−1/b0)[A + b1I] = (−1/b0)(A2 + b1A) + (−I + I) =
(−1/b0)(A2 + b1A + b0I) + I = θ+I = I.

1.9. MATRIX INVERSES AND THEIR PROPERTIES
37
62. θ= A2 + b1A + b0I =
·
9
5
25
14
¸
+
· 2b1
b1
5b1
3b1
¸
+
· b0
0
0
b0
¸
=
· 9 + 2b1 + b0
5 + b1
25 + 5b1
14 + 3b1 + b0
¸
.
b0 = 1, b1 = −5.
(−1/b0)[A + b1I ] =
·
3
−1
−5
2
¸
= A−1.
63. θ = A2 + b1A + b0I =
· 11
−4
−2
3
¸
+
· −3b1
2b1
b1
b1
¸
+
· b0
0
0
b0
¸
=
· 11 −3b1 + b0
−4 + 2b1
−2 + b1
3 + b1 + b0
¸
.b0 = −5, b1 = 2.
(−1/b0)[A + b1I ] = −(1/5)
·
1
−2
−1
−3
¸
= A−1.
64. θ= A2 + b1A + b0I =
·
0
−10
10
5
¸
+
· 2b1
−2b1
2b1
3b1
¸
+
· b0
0
0
b0
¸
=
· 2b1 + b0
−10 −2b1
10 + 2b1
5 + 3b1 + b0
¸
.b0 = 10, b1 = −5.
(−1/b0)[A + b1I ] =
1
10
·
3
2
−2
2
¸
= A−1.
65. θ= A2+b1A+b0I =
· 7
0
0
7
¸
+
· −b1
+3b1
2b1
b1
¸
+
· b0
0
0
b0
¸
=
· 7 −b1 + b0
3b1
2b1
7 + b1 + b0
¸
.
b0 = −7, b1 = 0
(−1/b0)[A + b1I] = 1
7
· −1
3
2
1
¸
= A−1.
66.
(a) Ax = b1, has solution x ≈


106.395
−4909.194
4979.886

. Ax = b2 has solution x ≈


107.459
−4958.286
5029.685


(b) A−1 =


25.315
21.316
−59.764
−1100.002
−1028.428
2780.764
1114.405
1044.910
−2820.572


67.
(a) A =
· 1
0
0
1
¸
and B =
· 0
1
1
0
¸
.
(b) A =
· 1
0
0
0
¸
and B =
· 0
0
0
1
¸
.
68. (A−1)T = (AT )−1 = A−1.
69.
(a) (AB = O) =⇒B = A−1AB = A−1O = O.

38
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
(b) B =
·
1
1
−1
−1
¸
.
70. (AB = AC) =⇒B = IB = (A−1A)B = A−1(AB) =
A−1(AC) = C.
71. If either d ̸= 0 or c ̸= 0 then
A
·
d
−c
¸
= θ and so A is singular.
If c = 0 = d, then A =
· a
b
0
0
¸
, and Ax =
· 0
1
¸
does not have a solution so A is singular.
72. (AB)−1 = B−1A−1.
73. The hypothesis of Theorem 17 is not satisﬁed; that is, Theorem 17 assumes that A and B
have inverses.
74.
(a) (Bv = θ) =⇒(A(Bv ) = θ =⇒(AB)v =θ)
(=⇒v = θ) so B is singular.
(b) AB and B−1 are nonsingular so A = (AB)B−is nonsingular.
75.
Suppose each of the systems Ax = ek
is consistent and let bk
be a solution. If B =
[b1, b2, . . . , bn] then AB = [Ab1, Ab2, . . . , Abn] = [e1, e2, . . . , en] = I . Thus B = A−1
and A is nonsingular.
76. Clearly I−1 = I, so by Theorem 1.5, I is non-singular.
77. Since AB is deﬁned, q = r and AB is a (p × s) matrix. Since BA is deﬁned, s = p and BA
is a (r × q) matrix. But AB = BA, so p = r and q = s.
79. Suppose B and C are inverses for A.
Then B = BI = B(AC) = (BA)C = IC = C.
1.10
Supplementary Exercises
1. The augmented matrix for the system reduces to
· 1
0
1
0
(a −1)(a + 2)
(a −3)(a + 2)
¸
There are inﬁnitely many solutions if a = −2, no solution if a = 1, and a unique solution
in which x2 = 0 if a = 3.

1.10. SUPPLEMENTARY EXERCISES
39
2.
(a) The system is consistent if and only if −b1 + 2b2 + b3 = 0 and in this case the solution
is x1 = b2 −b1 −2x3, x2 = b2 −2b1 −3x3, x3 arbitrary.
(b)
(i) inconsistent
(ii) x1 = −3 −2x3, x2 = −8 −3x2, x3 arbitrary
(iii) x1 = −4 −2x3, x2 = −11 −3x2, x3 arbitrary
(iv) inconsistent
3.
(a) Reducing the matrix [A, b1, b2, b3] yields: for b1, x1 = −48, x2 = 11, x3 = 18; for b2,
x1 = 4, x2 = 2, x3 = 1; and for b3, x1 = −3, x2 = 2, x3 = 2.
(b) C =


−48
4
−3
11
2
2
18
1
2


4. Set C = [c1, c2]. Reducing the matrix [A, C] yields solution x1 = 2 −x3, x2 = 1 + 2x3, x3
arbitrary, for the system Ax = c1. Similarly, the system Ax = c2 has solution x1 = −1−x3,
x2 = −3 + 2x3, x3 arbitrary. Therefore, if b1 =


2 −a
1 + 2a
a

and b2 =


−1 −b
−3 + 2b
b

for
arbitrary a, b, then Ab1 = c1 and Ab2 = c2, B = [b1, b2] is the desired matrix since
AB = C.
5. By assumption, A5 + 3A1 + 5A4 + 7A2 + 9A3 = b. Reordering the terms yield 3A1 +
7A2 + 9A3 + 5A4 + A5 = b so Ax = b has solution [3, 7, 9, 5, 1]T .
6.
(a) x1 = 2 + x3, x2 = 3 −3x3, x3 arbitrary.
(b) x1 = x3, x2 = −3x3, x3 nonzero.
7.
(a) x = [2, 1, 0]T is the unique solution.
(b) x = θ is the unique solution.
8. x = [−1, 0, 1]T
9.
(a) A−1 =


−4
1
3
4
−1
−2
−3
1
1


(b) A−1 =
·
cos θ
sin θ
−sin θ
cos θ
¸
10. ∆= (λ −4)(λ −1) + 2 = (λ −2)(λ −3). A is singular when ∆= 0; that is, when λ = 2 or
λ = 3. When A is nonsingular
A−1 =
1
(λ −2)(λ −3)
· λ −1
1
−2
λ −4
¸

40
CHAPTER 1. MATRICES AND SYSTEMS OF EQUATIONS
11. A =
·
1/2
−1/4
−5/4
3/4
¸
12. A =
· 3
4
6
8
¸
and B =
· 1
2
2
2
¸
13. A99 = A; A100 = I
14. x = A−1b = [3 −6 −1]T
15. (AB)−1 = B−1A−1 =


28
−22
−17
27
−1
49
29
8
16


16. (3A)−1 = (1/3)A−1 =


2/3
1
5/3
7/3
2/3
1/3
4/3
−4/3
1


17. (AT B)−1 = B−1(A−1)T =


15
−31
−31
36
52
47
18
21
−1


18. [(A−1B−1)−1A−1B]
−1 = (B−1)2 =


70
−19
5
−39
44
21
11
8
22


1.11
Conceptual Exercises
1. False. If A =
· 1
2
2
3
¸
and B =
· 1
1
1
4
¸
then A and B are symmetric but AB =
· 3
9
5
14
¸
is not symmetric.
2. True. (A + AT )T = AT + (AT )T = AT + A = A + AT .
3. True. A−1 = A and B−1 = B so AB−1 = B−1A−1 = BA.
4. False. If A =
· 1
0
0
1
¸
and B =
· 1
0
0
−1
¸
then A and B are nonsingular, but A + B =
· 2
0
0
0
¸
is singular.
5. False. The system
x1
=
1
x2
=
2
x1 + x2
=
3

1.11. CONCEPTUAL EXERCISES
41
clearly has a unique solution x1 = 1, x2 = 2.
6. True. Suppose A = [A1, A2, . . . , An]. Then for 1 ≤j ≤n, θ = Aej = Aj.
7. False. If {u1, u2} is linearly dependent then so is {Au1, Au2}. (cf. Exercise 12).
8. True. Since AB is deﬁned, n = p and AB is an (m × q) matrix. But AB is square, so
m = q. Thus BA is deﬁned and is an (n × n) matrix.
9. Q−1 = RP.
10. AB = (AB)T = BT AT = BA.
11. First note that uT
i uj = (uT
j ui)T , since uT
j ui is an (1 × 1) matrix. If θ = c1u1 + c2u2 + c3u3
then 0 = θT θ = (c1u1 + c2u2 + c3u3)T (c1u1 + c2u2 + c3u3) = c2
1 ∥u1 ∥2 +c2
2 ∥u2 ∥2 +c2
3 ∥
u3 ∥2. If follows that c1 = c2 = c3 = 0.
12. Suppose c1u1 +c2u2 = θ, where ci ̸= 0 for i = 1 or i = 2. Then θ = Aθ = A(c1u1 +c2u2) =
c1Au1 + c2Au2.
13. ∥Ax∥2= (Ax)T (Ax) = xT AT Ax = xT Ix = xT x =∥x∥2.
14. A2 = AI so A = I.
15. (AB)2 = (AB)(AB) = A(BA)B = A(AB)B = A2B2 = AB.
16.(b) Since Ak−1 ̸= O, there exists a vector b such that Ak−1b ̸= θ (cf. Exercise 6). If c = Ak−1b
then Ac = A(Ak−1b) = Akb = Ob = θ. It follows that A is singular.

Chapter 2
Vectors in 2-Space and 3-Space
2.1
Vectors in the Plane
1. For vector −−→
AB the x−component is −4 −0 = 4 and the y−component is 3 −(−2) = 5. For
vector −−→
CD the x−component is 1 −5 = −4 and the y−component is 4 −(−1) = 5. The
vectors are equal.
2. For vector −−→
AB the x−component is 3 −(−1) = 4 and the y−component is −2 −3 = −5.
For vector −−→
CD the x−component is 1 −5 = −4 and the y−component is 4 −(−1) = 5. The
vectors are not equal.
3. For vector −−→
AB the x−component is 0 −(−4) = 4 and the y−component is 1 −(−2) = 3.
For vector −−→
CD the x−component is 3 −0 = 3 and the y−component is 2 −(−2) = 4. The
vectors are not equal.
4. For vector −−→
AB the x−component is −1 −3 = −4 and the y−component is −1 −1 = −2.
For vector −−→
CD the x−component is −6 −0 = −6 and the y−component is 0 −3 = −3. The
vectors are not equal.
5.
(a) For u: ∥u∥=
p
(2 −(−3))2 + (2 −5)2 = √25 + 9 =
√
34.
For v: ∥v∥=
p
(−2 −3)2 + (7 −4)2 = √25 + 9 =
√
34.
Therefore ∥u∥=∥v∥.
(b) Segment AB has slope: (2 −5)/(2 −(−3)) = −3/5.
Segment CD has slope: (7 −4)/(−2 −3) = 3/(−5).
(c) For vector −−→
AB the x−component is 2 −(−3) = 5 and the y−component is 2 −5 = −3.
For vector −−→
CD the x−component is −2 −3 = −5 and the y−component is 7 −4 = 3.
The vectors are not equal.
6. D = (4, 7)

44
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
7. D = (−2, 5)
8. D = (−1, 6)
9. D = (−1, 1)
11. v1 = 5, v2 = 3
12. v1 = 5, v2 = −4
13. v1 = −6, v2 = 5
14. v1 = −3, v2 = −4
15. B = (3, 3)
16. B = (−1, 2)
17. A = (2, 4)
18. A = (0, 3)
19.
(a) B = (3, 2), C = (5, 0)
20.
(a) B = (−2, 3), C = (1, 4)
21.
(a) Q = (7, 1)
22.
(a) Q = (4, 1)
23.
(a) B = (−1, 4), C = (0, −1)
24.
(a) Q = (3, −2)
25.
(a) B = (3, 3), C = (6, 1)
26.
(a) B = (0, 7), C = (−3, −2)
27.
(a) D = (6, −3)
28.
1
√
5
· −1
2
¸
29. 1
5
· 3
4
¸
30.
1
√
2i + 1
√
2j

2.2. VECTORS IN SPACE
45
31.
3
√
13i −
2
√
13j
32. B = (4, 2)
33. B = (1, −2)
34. B = (0, −5)
35. B = (1/3, −7)
36. u + v =
· 2
3
¸
, u −3v =
· −2
−5
¸
37. u + v =
· 2
4
¸
, u −3v =
· −6
8
¸
38. u + v = 2i + j, u −3v = −2i + 5j
39. u + v = 4i + j, u −3v = −4i −7j
40. Note that given u =
· a
b
¸
with either a ̸= 0 or b ̸= 0, gives ∥u∥=
√
a2 + b2 ̸= 0.
Then w =
1
∥u∥u =
1
∥u∥
· a
b
¸
=
· a/
√
a2 + b2
b/
√
a2 + b2
¸
∥w∥=
r
a2
a2 + b2 +
b2
a2 + b2 =
r
a2 + b2
a2 + b2 =
√
1 = 1
2.2
Vectors in Space
1. d(P, Q) =
p
(0 −1)2 + (2 −2)2 + (2 −1)2 =
√
2
2. d(P, Q) =
p
(0 −1)2 + (0 −1)2 + (1 −0)2 =
√
3
3. d(P, Q) =
p
(0 −1)2 + (0 −0)2 + (1 −0)2 =
√
2
4. d(P, Q) =
p
(0 −1)2 + (0 −1)2 + (0 −1)2 =
√
3
5. M = (1, 4, 4);d(M, 0) =
p
(0 −1)2 + (0 −4)2 + (0 −4)2 =
√
33
6. M = (2, 1, 4);d(M, 0) =
p
(0 −2)2 + (0 −1)2 + (0 −4)2 =
√
21
7. B = (0, 3/2, −3/2), C = (1, 3, 0), D = (2, 9/2, 3/2)
8. plane
9. line

46
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
10. line
11. plane
12. plane
17.
(a) The length of the segment from P to R is
p
(r1 −p1)2 + (r2 −p2)2 + (r3 −p3)2 and
the length of the segment from R to S is
p
(s1 −r1)2 + (s2 −r2)2 + (s3 −r3)2. Let
a and b be the distances from P to R and R to S respectively and c be the distance
from point P to S. Solving the equation a2 + b2 = c2 for c yields the desired equality.
(b) This problem is worked similarly to part (a).
18.
(a) v =


−2
3
−3


(b) D = (−3, 5, −2)
19.
(a) v =


3
2
−3


(b) D = (2, 4, −2)
20.
(a) v =


−3
−2
3


(b) D = (−4, 0, 4)
21.
(a) v =


0
5
−7


(b) D = (−1, 7, −6)
22. A = (4, 0, 0)
23. A = (3, 2, 1)
24. A = (4, 1, 2)
25. A = (−2, 3, −1)
26.
(a) u + 2v =


9
9
14


(b) ∥u −v∥= 5

2.2. VECTORS IN SPACE
47
(c) w =


3/2
0
2


27.
(a) u + 2v =


7
7
10


(b) ∥u −v∥= 3
(c) w =


1
−1/2
1


28.
(a) u + 2v =


11
−3
4


(b) ∥u −v∥=
√
74
(c) w =


−4
3/2
−1/2


29.
(a) u + 2v =


−1
1
2


(b) ∥u −v∥=
√
150
(c) w =


7/2
−5
1/2


30. u = 2v = 2i + 2j
31. u = 2k
32. u = −4v =


−4
0
−4


33. u = −5
3v =


5/3
−10/3
−10/3


34. u =


−1/2
−1
0



48
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
35. u =


−2
−4
−1


2.3
The Dot Product and the Cross Product
1. −2
2. 0
3. 7
4. 0
5. cos (θ) =
11
√
290
6. cos (θ) =
−3
√
130
7. cos (θ) = 1
6
8. cos (θ) = 11
14
9. θ = π
6
10. θ = 2π
3
11. θ = π
2
12. θ = 0
13. u = i + 3j + 4k
14. u = 0i + 0j + 4k
15. u = 3i + 0j + 4k
16. u = 12i + 4j + 3k or u = 12i −4j + 3k
17. u = −1i + 3j + 1k
18. u = 1i + 1j + 2k
19. R = (33/10, 11/10)

2.3. THE DOT PRODUCT AND THE CROSS PRODUCT
49
20. R = (8, 2)
21. R = (−3, −1)
22. R = (0, 0)
u and q are perpendicular so w is the zero vector.
23. u1 =
· 5
5
¸
, u2 =
·
2
−2
¸
24. u1 =
·
2
−2
¸
, u2 =
· 4
4
¸
25. u1 =


2
4
2

, u2 =


4
0
−4


26. u1 =


3
3
3

, u2 =


−1
−2
3


27. If u =


u1
u2
u3

, then u · u = u2
1 + u2
2 + u2
3. Each term of this sum is a real number squared
and therefore greater than or equal to zero, hence the sum is greater than or equal to zero.
28. Note that u · v = u1v1 + u2v2 + u3v3 and v · u = v1u1 + v2u2 + v3u3. By the commutative
property of multiplication, uivi = viui for each value of i and hence u · v = v · u.
29. Note that u·(cv) =


u1
u2
u3

·


c v1
c v2
c v3


= u1 c v1 + u2 c v2 + u3 c v3
= c(u1v1 + u2v2 + u3v3)
= c(u · v)
30.
u·(v + w) =


u1
u2
u3

·


v1 + w1
v2 + w2
v3 + w3


= u1(v1 + w1) + u2(v2 + w2) + u3(v3 + w3)
= u1v1 + u1w1 + u2v2 + u2w2 + u3v3 + u3w3
= (u1v1 + u2v2 + u3v3) + (u1w1 + u2w2 + u3w3)
= u · v + u · w

50
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
31. With the given u and v the left-hand-side of equation (1) yields:
∥u −v∥2 = (u1 −v1)2 + (u2 −v2)2 = u2
1 −2u1v1 + v2
1 + u2
2 −2u2v2 + v2
2
The right-hand-side of equation (1) yields:
∥u∥2+ ∥v∥2 −2 ∥u∥∥v∥cos (θ) = u2
1 + u2
2 + v2
1 + v2
2 −2 ∥u∥∥v∥cos (θ)
Setting these expanded left and right hand sides equal to each other and then cancelling
terms yields:
−2u1v1 −2u2v2 = −2 ∥u∥∥v∥cos (θ)
Dividing the last equation by −2 gives the desired result, equation (2a).
32.


0
0
−11


33.


−2
2
8


34.


0
0
0


35.


3
−1
−5


36. w =


−1
−1
2


37. w =


−2
−5
4


38. w =


1
−1
−1


39. w =


2
−3
1



2.3. THE DOT PRODUCT AND THE CROSS PRODUCT
51
40. w =


−9
−9
−9


41. w =


1
1
−4


42.
√
41 square units
43. 4
√
6 square units
44. 5
√
5
2
square units
45. 6
√
11
2
square units
46. 22 cubic units
47. 24 cubic units
48. coplanar
49. NOT coplanar
50. Substitute the given values of x, y, and z into the left-hand-side of each of the two given
equations and simplify. This will show that for the given values, the left-hand-side of each
equation is zero and therefore the given values are a solution to the system.
51. By ﬁnding the two cross products (i × i) × j =


0
0
0

and i × (i × j) =


0
−1
0

, one can
see that they are not equal.
52.
(a) Expanding and simplifying the right and left hand sides of the equations, one can see
that they are equal.
∥u × v∥2 = (u2v3 −u3v2)2 + (u3v1 −u1v3)2 + (u1v2 −u2v1)2
= u32(v12 + v22) −2u1u3v1v3
−2u2v2(u1v1 + u3v3) + u22(v12 + v32) + u12(v22 + v32)
∥u∥2 ∥v∥2 −(u · v)2 = (u2v3 −u3v2)2 + (u3v1 −u1v3)2 + (u1v2 −u2v1)2
= u32(v12 + v22) −2u1u3v1v3
−2u2v2(u1v1 + u3v3) + u22(v12 + v32) + u12(v22 + v32)

52
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
(b) Substitute ∥u∥∥v∥cos (θ) into the equation given in 10.(a) for u · v:
∥u × v∥2 = ∥u∥2 ∥v∥2−∥u∥2 ∥v∥2 cos2 (θ)
= ∥u∥2 ∥v∥2(1 −cos2 (θ))
= ∥u∥2 ∥v∥2 sin2 (θ)
Therefore, ∥u × v∥=∥u∥∥v∥cos (θ).
53. To see that ∥u∥cos (θ) is the height of the parallelepiped, calculate the length of proj(v×w)u.
Then the volume of the parallelepiped is (area of base)×(height) which is exactly the abso-
lute value of the triple product given.
2.4
Lines and Planes in Space
1.
x = 2 + 3t
y = 4 + 2t
z = −3 + 4t
2.
x = 1 + 2t
y = 1
z = −1 + 3t
3. As a direction vector for L we use u = −−−→
P0P1 =


1 −0
2 −4
4 −1

=


1
−2
3

. Therefore, one set of
parametric equations for L are x = t, y = 4 −2t, z = 1 + 3t.
4. As a direction vector for L we use u = −−−→
P0P1 =


6 −5
6 −1
4 −(−3)

=


1
5
7

. Therefore, one set
of parametric equations for L are x = 5 + t, y = 1 + 5t, z = −3 + 7t.
5. The direction vector for the ﬁrst line is u =


2
−1
3

. The direction vector for the second
line is v =


−4
2
6

= −2u. Since the two direction vectors are scalar multiples of each
other, the lines are parallel.
6. The direction vector for the ﬁrst line is u =


6
4
3

. The direction vector for the second line
is v =


3
2
3

. Since there is no real number k such that u = kv, the two lines are NOT

2.4. LINES AND PLANES IN SPACE
53
parallel.
7. The direction vector for the ﬁrst line is u =


−2
3
−2

. The direction vector for the second
line is v =


2
−3
4

. Since there is no real number k such that u = kv, the two lines are
NOT parallel.
8. The direction vector for the ﬁrst line is u =


−1
2
−3

. The direction vector for the second
line is v =


3
−6
9

= −3u. Since the two direction vectors are scalar multiples of each
other, the lines are parallel.
9. The normal vector to the plane is n =


3
4
−1

. Therefore, the equation of the line is
x = 1 + 3t, y = 2 + 4t, z = 1 −t.
10. The normal vector to the plane is n =


1
−1
2

. Therefore, the equation of the line is
x = 2 + t, y = 0 −t, z = −3 + 2t.
11. By substituting the parametric equations of the line into the equation for the plane, we ﬁnd
that t = −1. Thus P = (−1, 4, 1).
12. When substituting the parametric equations of the line into the equation for the plane, we
ﬁnd a contradiction. Therefore, the line does not intersect the plane at any point. It can
also be noted that the normal vector to the plane and the direction vector of the line are
perpendicular so that the line is at least parallel to the plane. Then since the point P0 is
not in the plane, the line is not in the plane either.
13. By substituting the parametric equations of the line into the equation for the plane, we ﬁnd
that t = −4. Thus P = (−8, −13, 36).
14. By substituting the parametric equations of the line into the equation for the plane, we ﬁnd
that t = −2. Thus P = (−3, 5, −3).
15. 6x + y −z = 16

54
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
16. x −2y + 3z = 9
17. A normal vector to the plane can be found by taking the cross product of the vectors
−−→
PQ =


1
1
2

and −→
PR =


0
4
1

.
n = −−→
PQ × −→
PR = −7i −j + 4k.
Therefore, the equation of the plane is −7x −y + 4z = 5.
18. A normal vector to the plane can be found by taking the cross product of the vectors
−−→
PQ =


1
8
−5

and −→
PR =


2
1
2

.
n = −−→
PQ × −→
PR = 21i −12j −15k.
Therefore, the equation of the plane is 21x −12y −15z = −12
or equivalently 7x −4y −5z = −4.
19. A normal vector to the plane can be found by taking the cross product of the vectors
−−→
PQ =


−2
−1
1

and −→
PR =


3
0
2

.
n = −−→
PQ × −→
PR = −2i + 7j + 3k.
Therefore, the equation of the plane is −2x + 7y + 3z = 17.
20. A normal vector to the plane can be found by taking the cross product of the vectors
−−→
PQ =


−1
1
1

and −→
PR =


1
0
1

.
n = −−→
PQ × −→
PR = 1i + 2j −k.
Therefore, the equation of the plane is x + 2y −z = 6.
21. v =


2/3
1/3
−2/3


22. v =


2/3
−2/3
1/3


23. x + 2y −2z = 17

2.5. SUPPLEMENTARY EXERCISES
55
24. x + y + 3z = 11
25.
x = 4 −t
y = 5 + t
z = t
26.
x = −3 + t
y = 2 −t
z = t
2.5
Supplementary Exercises
1. Solving the system of equations c1u+c2v = x for c1 and c2 we ﬁnd that c1 = 3 and c2 = −2.
Thus x1 = 3u =
· 15
6
¸
and x2 = −2v =
· −14
−2
¸
.
2. The island dock is at (5, 5
√
3). Assuming that the boat travels t miles west and t miles
north to get to the buoy, the buoy is at (5 −t, 5
√
3 + t). The distance from the mainland
dock to the buoy is
q
(5 −t)2 + (5
√
3 + t)2 =
q
2t2 + (10
√
3 −2)t + 100 miles.
3. R = (13, −1)
4.
∥a∥2 = a · a = (2u + 3v) · (2u + 3v)
= 4(u · u) + 12(u · v) + 9(v · v)
= 4 ∥u∥2 + 12(0) + 9 ∥v∥2
= 4 + 9 = 13
Since ∥a∥2 = 13 then ∥a∥=
√
13.
5.
∥a∥2 = a · a = (u + v + w) · (u + v + w)
= (u · u) + 2(u · v) + 2(u · w) + 2(v · w) + (v · v) + (w · w)
= 22 + 2(0) + 2(0) + 2(0) + 12 + 22
= 4 + 1 + 4 = 9
Since ∥a∥2 = 9 then ∥a∥= 3.
6.
(u −v) · (u + v) = (u · u) −(v · v)
= 4 −9 = −5
Therefore (u −v) · (u + v) = −5.
7. Note that since u · v = 0 then u and v are perpendicular. So the angle between u and v,
θ, is π/2.
Therefore, ∥u × v∥=∥u∥∥v∥sin (θ) = 2 · 3 · 1 = 6.

56
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
8. Note that since u and v are perpendicular then ∥u −v∥=∥u + v∥=
√
13.
Then ∥(u −v) × (u + v)∥2 = ∥u −v∥2 ∥u + v∥2 sin2 (θ)
= 169(1 −sin2 (θ)
But cos (θ) = (u −v) · (u + v)
∥u −v∥∥u + v∥and from Exercise 6 we have that (u −v · u + v) = −5.
Therefore, ∥(u −v) × (u + v)∥2 = 169(1 −(−5/13)2) = 169 −25 = 144. Thus ∥(u −v) ×
(u + v)∥= 12.
9. Unit vectors in the xz−plane are of the form v =


x
0
y

where
√
x2 + z2 = 1. For v to be
perpendicular to


3
−2
4

requires that


x
0
y

·


3
−2
4

= 0 or 3x + 4y = 0. Hence, v is
of the form v = k


−4/3
0
1

for some number k. Since v is a unit vector then k = ±3/5.
Therefore v =


−4/5
0
3/5

or v =


4/5
0
−3/5

.
10. Letting Pa = (a, 0, 0), Pb = (0, b, 0) and Pc = (0, 0, c) then −−−→
PaPb =


−a
b
0

and
−−−→
PaPc =


−a
0
c

. Thus a normal vector to the plane is
n = −−−→
PaPb × −−−→
PaPc = (bc)i + (ac)j + (ab)k.
Then the equation of the plane is (ab)x + (ac)y + (ab)z = abc. Dividing through by abc the
equation of the plane can be rewritten as
µ1
a
¶
x +
µ1
b
¶
y +
µ1
c
¶
z = 1.
11. Letting P1 = (1, 1, 2), P2 = (2, 3, 9), P3 = (−2, 1, −1) and P4 = (1, 2, 5), then the equation
of the plane determined by P1, P2 and P3 is −x−3y+z = −2. Since P4 satisﬁes the equation
of this plane, i. e. −1(1) −3(2) + 1(5) = −2 then P4 is also in the plane. Therefore, all four
points lie in the plane −x −3y + z = −2.
12. Consider a circle of radius r in the xy−plane centered at the origin. Letting A = (0, r),
B = (0, −r) and C = (x, y) where x2 + y2 = r2, then the vector −→
AC =
·
x
y −r
¸
and the

2.6. CONCEPTUAL EXERCISES
57
vector −−→
BC =
·
x
y + r
¸
. Since −→
AC · −−→
BC = x2 + (y + r)(y −r) = (x2 + y2) −r2 = r2 −r2 = 0
then the vectors −→
AC and −−→
BC are orthogonal and therefore, the triangle △ABC is a right
triangle.
13. Let the midpoints of the four sides of the quadrilateral ABCD be P1 = (a1 + b1
2
, a2 + b2
2
),
P1 = (b1 + c1
2
, b2 + c2
2
), P1 = (c1 + d1
2
, c2 + d2
2
), and P1 = (d1 + a1
2
, d2 + a2
2
). Then note
that the line segments P1P3 and P2P4 share the same midpoint,
namely (a1 + b1 + c1 + d1
4
, a2 + b2 + c2 + d2
4
).
2.6
Conceptual Exercises
1. False. For example, if u = 2i + 4j and v = −2i + j then u · v = 0 while both u and v are
nonzero.
2. False. For example, if u = 2i + j + 2k and v = 3u = 6i + 3j + 6k then u × v = 0 while both
u and v are nonzero.
3. Let u =


u1
u2
u3

and v =


v1
v2
v3

.
Then ∥u + v∥2+ ∥u −v∥2 =
u2
1 + 2u1v1 + v2
1 + u2
2 + 2u2v2 + v2
2 + u2
3 + 2u3v3 + v2
3
+u2
1 −2u1v1 + v2
1 + u2
2 −2u2v2 + v2
2 + u2
3 −2u3v3 + v2
3
= 2(u2
1 + u2
2 + u2
3) + 2(v2
1 + v2
2 + v2
3)
= 2 ∥u∥2 + 2 ∥v∥2
4. The vectors u and v form the sides of a parallelogram while the vectors u + v and u −v
form the diagonals of the parallelogram. Thus, the parallelogram law states that the sum of
the squares of the sides of a parallelogram equals the sum of the squares of the diagonals. In
other words, the parallelogram law is the application of the law of cosines to a parallelogram.
5. For example, u = i+j+k, v = 2i−j+k, and w = i−2j−k. Then u×(v × w) = −6i+6j
and (u × v) × w = −7i −j −5k. Thus u × (v × w) ̸= (u × v) × w.
6. xi = Ai · b
Ai · Ai
for i = 1, 2, 3.
7. It is equivalent to show that the columns of A form a linearly independent set (see Section
1.7). That is, if c1A1 + c2A2 + c3A3 = 0 then the only solution is c1 = c2 = c3 = 0. But
due to Exercise 6. we have that ci = Ai · 0
Ai · Ai
= 0. Therefore, the columns of A are linearly
independent and so A is nonsingular.

58
CHAPTER 2. VECTORS IN 2-SPACE AND 3-SPACE
8. Due to Exercise 7. the matrix A is nonsingular. Thus the solution of Ax = b can be written
as x = A−1b. From Exercise 6., we have that xi = Ai ·b (since Ai ·Ai = 1 for each i). Then
let A =


a11
a12
a13
a21
a22
a23
a31
a32
a33

so Ai =


ai1
ai2
ai3

. Then xi = Ai · b = A1ib1 + A2ib2 + A3ib3.
This leads to the following system of equations for x1, x2 and x3:
x1 = A11b1 + A21b2 + A31b3
x2 = A12b1 + A22b2 + A32b3
x3 = A13b1 + A23b2 + A33b3
In matrix form these equations are written as x =


a11
a21
a31
a12
a22
a32
a13
a23
a33

b = AT b.
Since we already know that x = A−1b then it must be true that A−1 = AT .
9. It is equivalent to show that ∥Ax ∥2 =∥x ∥2. Recall that the dot product of u and v can
be expressed as (u · v) = uT v.
Then ∥Ax∥2 = (Ax) · (Ax)
= (Ax)T (Ax)
= xT AT Ax
= xT x = x · x =∥x∥2
10. (Au) · (Av) = (Au)T (Av) = uT AT Av = uT v = u · v.
11. To show that the angle θ1 between the vectors u and v equals the angle θ2 between the
vectors Au and Av consider that
cos (θ1) =
u · v
∥u∥∥v∥=
Au · Av
∥Au∥∥Av∥= cos (θ2).
Since cos (θ1) = cos (θ2) it follows that θ1 = θ2.
12. (u −v) · (u + v) = (u · u) −(v · v) =∥u∥2−∥v∥2 = 0 since ∥u∥=∥v∥. Therefore u and
v are orthogonal.

Chapter 3
The Vector Space Rn
3.1
Introduction
12.
Geometrically, W consists of the points in the plane that lie on the line with equation
x + y = 1.
13.
Geometrically, W consists of the points in the plane that lie on the line with equation
x = −3y.
14.
Geometrically, W consists of the points in the plane that lie on the y−axis.
15.
Geometrically, W consists of the points in the plane that have coordinates (x, y) satisfying
x + y ≥0.
16.
Geometrically, W consists of the points in the plane that lie on the line passing through
the origin and the point (1, 3). [Let t = 0 and t = 1, respectively. If (x, y) is a point on the
line then x = t and y = 3t, so the equation for the line is y = 3x.
17.
Geometrically, W consists of the points in the plane that lie on the circle with equation
x2 + y2 = 4.
18.
Geometrically, W consists of the points in three-space that lie on positive x−axis.
19.
Geometrically, W consists of the points in three-space that lie on the plane with equation
x + y + 2z = 0.
20.
Geometrically, W consists of the points in three-space that lie on the line which passes
through the origin and the point with coordinates (2, 0, 1). The line can be represented by
the equations x = 2r, y = 0, z = r.
21.
Geometrically, W consists of the points in three-space that are on or above the xy−plane
and that lie on the sphere with equation x2 + y2 + z2 = 1.

60
CHAPTER 3. THE VECTOR SPACE RN
22. W = {x : x =
· a
b
¸
, a −2b = 1}.
23. W = {x : x =
· a
0
¸
, a
any real number }.
24. W = {x : x =
· a
b
¸
, b > 0}.
25. W = {x : x =
· a
2
¸
, a
any real number }.
26. W = {x : x =
· a
b
¸
, b = a2}.
27. W = {x : x =


x1
x2
x3

, x1 + x2 −2x3 = 0}.
28. W = {x : x = t


2
−3
1

, t
any real number}.
29. W = {x : x =


0
x2
x3

, x, x
any real number }.
30. W = {x : x =


x1
2
x3

, x, x
any real number }.
3.2
Vector Space Properties of Rn
1. Clearly θ is in W . Suppose u and v are in W , where u =
· u1
u2
¸
and v =
· v1
v2
¸
.
Then u1 = 2u2 and v1 = 2v2 . If a is any scalar then u + v =
· u1 + v1
u2 + v2
¸
and
au =
· au1
au2
¸
. But u1 + v1 = 2u2 + 2v2 = 2(u2 + v2) and au1 = a(2u2) = 2(au2), so u + v
and au are in W. By Theorem 2, W is a subspace of R2. Geometrically, W consists of the
points on the line with equation x = 2y.

3.2. VECTOR SPACE PROPERTIES OF RN
61
2. W is not a subspace of R2. Note for example that θ is not in W. Also if u =
· 2
0
¸
and v =
·
0
−2
¸
, then u
and v
are in W, but u + v
is not in W.
3. W
is not a subspace of R2
since, for example, u =
· 1
1
¸
and v =
·
1
−1
¸
are in W
whereas u + v
is not in W. Note that W satisﬁes properties (s1) and (s3) of Theorem 2.
4. W
is not a subspace of R2. For example, u =
· 1
1
¸
is in W, but
√
2 u
is not in W.
Note that W
satisﬁes properties (s1) and (s2) of Theorem 2.
5.
Clearly, θ =
· 0
0
¸
is in W. If u =
· u1
u2
¸
and v =
· v1
v2
¸
are in W, then u1 = v1 = 0.
Thus u + v =
·
0
u2 + v2
¸
is in W. Likewise, if a is any scalar, au =
·
0
au2
¸
is in W.
By Theorem 2, W
is a subspace of R2. Geometrically, W
consists of the points on the
y−axis .
6.
If u =
· u1
u2
¸
is in W, then | u1 | + | u2 |= 0 so u1 = u2 = 0. Thus W = { θ}
and W
is a subspace of R2.
7. W satisﬁes none of the properties (s1) - (s3) of Theorem 2, so W is not a subspace of R2.
Clearly, θ =
· 0
0
¸
is not in W. Also, u =
· 1
0
¸
and v =
· 0
1
¸
are in W whereas u + v
is not in W. Finally,
if a ̸= 1
then av
is not in W.
8. W
is not a subspace of R2. For example u =
· 1
0
¸
and v =
· 0
1
¸
are in W
whereas u + v
is not in W. Note that W
satisﬁes the properties (s1) and (s3)
of Theorem 2.
9.
Clearly θ =


0
0
0

is in W. If u =


u1
u2
u3


and v =


v1
v2
v3

are in W
and a is any
scalar, then u3 = 2u1 −u2 and v3 = 2v1 −v2. Now u3 + v3 = (2u1 −u2) + (2v1 −v2) =
2(u1 + v1) −(u2 + v2) and au3 = a(2u1 −u2) = 2(au1) −au2. Therefore u + v
and au
are in W. By Theorem 2, W is a subspace of R3. Geometrically, W consists of the points
on the plane with equation 2x −y −z = 0.

62
CHAPTER 3. THE VECTOR SPACE RN
10.
Clearly θ
=


0
0
0

is in W. If
u =


u1
u2
u3

and
v =


v1
v2
v3

are
in W, it follows that u2 + v2 = (u3 + v3) + (u1 + v1) and for any scalar a, au2 = au3 + au1.
Thus
u + v
and au
are in W. By Theorem 2, W
is a subspace of R3. Geometrically,
W
consists of the points in the plane with equation x −y + z = 0.
11. W
is not a subspace of R3. For example, u =


1
1
1

and v =


1
0
0


are in W
but u + v
is not in W. Also if a ̸= 1 and a ̸= 0 then au
is not in W.
12.
Clearly θ =


0
0
0

is in W. If u =


u1
u2
u3

and v =


v1
v2
v3

are in W
then u1 = 2u3
and v1 = 2v3. Therefore u1 + v1 = 2(u3 + v3) and, for any scalar a, au1 = 2au3. Thus
u + v
and au
are in W
and by Theorem 2, W is a subspace of R3. Geometrically, W
consists of the points on the plane with equation x −2z = 0.
13. W
is not a subspace of R3. For example u =


1
0
0

and v =


2
2
0


are in W
but u + v
is not in W. Also, if a ̸= 1 and a ̸= 0 then au
is not in W.
14.
Clearly θ is in W. Vectors u
and v
in W
can be written in the form u =


u1
0
u3


and v =


v1
0
v3

. Thus u + v =


u1 + v1
0
u3 + v3


is in W. Likewise, for any scalar a, au
=


au1
0
au3

is in W. Geometrically, W
consists of the points in the xz−plane .
15.
Clearly θ is in W. If u
and v
are vectors in W then u
and v
can be expressed in
the form u =


2a
−a
a

and v =


2b
−b
b

. Then u + v =


2(a + b)
−(a + b)
a + b

. Similarly, for
any scalar c, cu =


2ca
−ca
ca

. By Theorem 2, W
is a subspace of R3. Geometrically W
consists of the points on the line with parametric equations x = 2t, y = −t, z = t.

3.2. VECTOR SPACE PROPERTIES OF RN
63
16.
Clearly θ is in W. Elements u
and v
in W can be expressed in the form u =


a
2a
2a


and v =


b
2b
2b

. Therefore u + v =


a + b
2(a + b)
2(a + b)


is in W
and, for any scalar c, cu =


ca
2ca
2ca

is in W.
By Theorem 2, W
is a subspace of R3. Geometrically, W
consists of the points on the
line with parametric equations x = t, y = 2t,
z = 2t.
17.
Clearly θ is in W. Moreover, any two elements u
and v
in W
can be written in the
form u =


a
0
0

and v =


b
0
0

. Therefore u + v =


a + b
0
0

is in W
and for any
scalar c, cu =


ca
0
0

is in W˚. By Theorem 2, W is a subspace of R3. Geometrically, W
consists of the points on the x−axis.
18.
Clearly aT θ = 0 so θ
is in W˚. If u
and v
are in W
then aTu = 0 and aTv = 0. It
follows that aT(u + v ) = aT u +aT v = 0, so u + v
is in W. If c is a scalar, then aT (cu
) = caTu = 0 and hence cu
is in W. This proves that W
is a subspace of R3.
19.
The vector u =


u1
u2
u3

is in W if and only if 0 = aTu= u1+2u2+u3. Thus geometrically
W
consists of the points in R3 which lie in the plane x + 2y + 3z = 0.
20.
The vector u =


u1
u2
u3

is in W
if and only if 0 = aTu = u1. Thus geometrically W
consists of thepoints in R3 which lie on the yz- plane.
21.
Clearly, a Tθ = bT θ = 0, so θ is in W. Let aTu = bTu = aTv = bTv = 0. It then follows
that aT(u + v)= 0 and bT(u + v)= 0. Therefore u+v is in W. Likewise, aT(cu) = c(aTu
) = 0 and bT (cu ) =
c(bTu ) = 0 for any scalar c. Therefore cu
is in W. Thus W
is a subspace of R3.

64
CHAPTER 3. THE VECTOR SPACE RN
22.
The vector u =


u1
u2
u3

is in W
if and only if 0 = aTu = u1 −u2 + 2u3 and 0 = bTu
= 2u1 −u2 + 3u3. Thus W
is the set of points in R3 formed by the intersecting planes
x −y + 2z = 0 and 2x −y + 3z = 0. The parametric equations for the line are x = −t, y =
t, z = t.
23.
The vector u =


u1
u2
u3

is in W
if and only if 0 = aTu = u1 + 2u2 + 2u3 and 0 = bTu
= u1 + 3u2.
Thus W
is the set of points on the line formed by the intersecting planes
x + 2y + 2z = 0 and x + 3y = 0. Solving yields x = −6z
and y = 2z
so the line has
parametric equations x = −6t, y = 2t, z = t.
24.
The vector u =


u1
u2
u3

is in W
if and only if 0 = aTu = u1 + u2 + u3 and 0 = bTu
= 2u1 + 2u2 + 2u3. Clearly the latter condition is redundant so W
consists of the points
on the plane x + y + z = 0.
25.
The vector u =


u1
u2
u3


is in W
if and only if 0 = aTu = u1 −u3
and 0 = bTu
= −2u1 + 2u3. Clearly the latter condition is redundant so W consists of the points in the
plane x −z = 0.
27. Property (m1) is not satisﬁed. For example 3(2x) = 3
· 4x1
4x2
¸
=
· 24x1
24x2
¸
where 6x =
· 12x1
12x2
¸
. Also, (m4) is not satisﬁed since 1x =
· 2x1
2x2
¸
̸= x.
28. Property (c2) is not satisﬁed. For example, if x =
· 1
1
¸
and a = −1 then x is in W but
ax is not in W. This also illustrates that (a4) is not satisﬁed.
29.
The set of points on the line can be expressed as the set W
= {t


a
b
c

: t
any real
number }. Taking t = 0 we see that θ is in W. If u = r


a
b
c

and v = s


a
b
c

then
u + v = (r + s)


a
b
c

is in W. Likewise, if k is any scalar then ku = kr


a
b
c

is in W.
Therefore W
is a subspace of R3.

3.3. EXAMPLES OF SUBSPACES
65
30.
Since θ is in both U
and V, θ = θ + θ is in U + V. Suppose x
and y
are in U + V
and write x= u1 +v1 , y = u2 +v2 where u1 , u2 are in U and v1 , v2 are in V. Then x
+y = (u1 +u2 ) + (v1 +v2 ) is in U + V. If a is a scalar then ax = au1 +av1
is in U + V.
It follows that U + V
is a subspace of Rn.
31.
Clearly θ is in U ∩V. Suppose x
and y
are in U ∩V. Then x
and y
are in U and
since U
is a subspace, x +y
is in U. Also for any scalar a, ax
is in U. Similarly, x +y
and ax
are in V.
Therefore x +y
and ax
are in U ∩V.
It follows that U ∩V
is a
subspace of Rn.
32.
The vector u =


1
−1
0

is in U
and the vector v =


0
1
1

is in V. Thus u
and v
are in U ∪V
but u +v
is in neither U nor V.
33.
(a)
Clearly θ is in U ∪V. Suppose x
is in U ∪V
and let a be a scalar. If x
is in U,
then ax is also in U. Similarly, if x is in V, then ax is in V. In either case, ax is
in U ∪V.
(b)
Assume that u +v
is in U. Since −u
is in U and U is closed under addition we
see that v = (u +v ) + (−u ) is in U. This contradicts the assumption that v
is not
in U. Similarly, u +v
is not in V.
34.
Since W is non-empty, W contains a vector x. By (s3) the vector 0x= θ
is in W.
By
Theorem 2, W
is a subspace of Rn.
3.3
Examples of Subspaces
1.
By deﬁnition Sp(S) = {t
·
1
−1
¸
: t
any real number }. Thus if x =
· x1
x2
¸
is in R2
, then x
is in Sp(S) if and only if x1 + x2 = 0.
In particular Sp(S)
is the line with
equation x + y = 0.
2.
By deﬁnition Sp(S) = { t
· 2
3
¸
: t
any real number }. If x =
· x1
x2
¸
is in R2 , then
x is in Sp(S) if and only if 3x1 + 2x2 = 0. In particular Sp(S) is the line with equation
3x + 2y = 0.
3. Sp(S) = { t
· 0
0
¸
: t
any real number } = {e}.Sp(S) is the point (0, 0).
4. Sp(S) = {x in R2 : x = k1a + k2b for scalars k1
and k2 }. For an arbitrary vector x
in R2, x =
· x1
x2
¸
, x = k1a + k2b if k1 = 3x1 + 2x2 and k2 = −x1 −x2. It follows that
Sp(S) = R2.

66
CHAPTER 3. THE VECTOR SPACE RN
5. Sp(S) = {x
in R2 : x = k1a + k2d
for scalars k1
and k2}.
For an arbitrary vector x
in R2, x =
· x1
x2
¸
, the equation k1a + k2d = x has augmented matrix
·
1
1
x1
−1
0
x2
¸
.
This matrix reduces to
· 1
1
x1
0
1
x1 + x2
¸
and backsolving yields k1 = −x2,
and k2 = x1 + x2. It follows that
Sp(S) = R2.
6.
If x =
· x1
x2
¸
the equation k1a + k2c = x
has augmented matrix
·
1
−2
x1
−1
2
x2
¸
.
This matrix reduces to
· 1
−2
x1
0
0
x1 + x2
¸
, so the equation has a solution if and only if
x1 + x2 = 0. It follows that Sp(S) = {x : x1 + x2 = 0}˚; that is, Sp(S) is the line with
equation x + y = 0.
7. Sp(S) = { x
in R2 : x = k1b + k2e
for scalars k1
and k2}. But k1b + k2e = k1b so
Sp(S) = Sp({b}). It follows that Sp(S) = { t
·
2
−3
¸
: t any real number }. If x =
· x1
x2
¸
is in R2, x is in Sp(S) if and only if 3x1 + 2x2 = 0. Thus, Sp(S) = { x : 3x1 + 2x2 = 0};
so Sp(S) is the line with equation 3x + 2y = 0.
8.
If x =
· x1
x2
¸
the equation k1a + k2b + k3d = x has augmented matrix
·
1
2
1
x1
−1
−3
0
x2
¸
.
This matrix reduces to
· 1
2
1
x1
0
−1
1
x1 + x2
¸
.
It follows that the system is consistent for arbitrary x , so Sp(S) = R2.
9. Sp(S) = { x in R2 : x = k1b + k2c + k3d for scalars
k1, k2, k3}.
With x =
· x1
x2
¸
,
the equation k1b + k2c + k3d = x has augmented matrix
·
2
−2
1
x1
−3
2
0
x2
¸
. The reduction reveals that the system is consistent for every x , so
Sp(S) = R2.
10.
Since k1a + k2b + k3e = k1a + k2b , Sp(S) = Sp{a, b} = R2.
11.
Since k1a+k2c+k3e = k1a+k2c, Sp(S) = Sp{a, c}. But Sp{a, c} = { x : x1 +x2 = 0}.
Thus Sp(S) is the line with equation x + y = 0. (cf. Exercise 6).
12. Sp(S) = { x : x = tv , t a scalar} so Sp(S) is the line through (0, 0, 0) and (1, 2, 0). The
parametric equations for the line are x = t, y = 2t, z = 0. Equivalently, if x= [x1, x2, x3]T,
then Sp(S) = { x : −2x1 + x2 = 0
and x3 = 0}.
Thus Sp(S)
the line formed by the
intersecting planes −2x + y = 0 and z = 0.

3.3. EXAMPLES OF SUBSPACES
67
13. Sp(S) = { x in R3 : x= tw for some scalar t }.Therefore Sp(S) is the line through (0, 0, 0)
and (0, −1, 1). The parametric equations are x = 0, y = −t, z = t. If x = [x1, x2, x3
]T
then tw = x
has augmented matrix


0
x1
−1
x2
1
x3

which reduces to


1
x3
0
x2 + x3
0
x1

.
The system is consistent if x2 + x3 = 0
and x1 = 0 so we have Sp(S) = { x : x2 + x3 = 0
and x1 = 0}. Therefore Sp(S) is the line formed by the the intersecting planes y + z = 0
and x = 0.
14.
For x = [x1, x2, x3
]T the equation k1v + k2w = x is consistent if and only if −2x1 +
x2 + x3 = 0.
Thus Sp(S) = { x : −2x1 + x2 + x3 = 0}
and is the plane with equation
−2x + y + z = 0.
15. Sp(S) = {u in R3 : u = k1v + k2x}. The equation k1v + k2x = u has augmented matrix


1
1
u1
2
1
u2
0
−1
u3

, which reduces to


1
1
u1
0
1
2u1 −u2
0
0
2u1 −u2 + u3

. A solution exists if and only if 2u1−u2+u3 = 0 , so Sp(S) = { u
: 2u1 −u2 + u3 = 0}. Sp(S) is the plane with equation 2x −y + z = 0.
16.
For arbitrary u in R3 the equation k1v + k2w + k3x = u has a solution, so Sp(S) = R3.
17. Sp(S) = { u
in R3 : u = k1w + k2x + k3z}.
For u = [u1, u2, u3]T
the equation
k1w + k2x + k3z = u has augmented matrix


0
1
1
u1
−1
1
0
u2
1
−1
2
u3

.
This matrix reduces to


1
−1
2
u3
0
1
1
u1
0
0
2
u2 + u3

. Since the system is consistent for every u in R3, Sp(S) = R3.
18.
For u = [u1, u2, u3]T the system of equations k1v + k2w + k3z = u is consistent if and
only if −2u1 + u2 + u3 = 0. Therefore Sp(S) =
{ u : −2u1 + u2 + u3 = 0} and Sp(S) is the plane with equation −2x + y + z = 0.
19.
The matrix


0
1
−2
u1
−1
1
−2
u2
1
−1
2
u3

reduces to

68
CHAPTER 3. THE VECTOR SPACE RN


1
−1
2
u3
0
1
−2
u1
0
0
0
u2 + u3


so the system of equations k1w + k2x + k3y = u is consistent
if and only if u2 + u3 = 0. Therefore Sp(S) = { u : u2 + u3 = 0} and Sp(S) is the plane
with equation y + z = 0.
20.
By Exercise 14, Sp(S) = { x : −2x1 + x2 + x3 = 0}. Then the vectors given in (a), (c)
and (d) are in Sp(S). Moreover when the system of equations k1v+k2w = x is consistent,
the unique solution is k1 = x1,
andk2 = x3.
Thus in (a)
[1, 1, 1]T = v + w;
in (c)
[1, 2, 0]T = v; and in (d), [2, 3, 1]T = 2v + w.
21.
By Exercise 15, Sp(S) = { u in R3 : 2u1 −u2 + u3 = 0}. Thus the vectors given in (b),
(c), and (e) are in Sp(S). From the calculations done in Exercise 15, it follows that when
the system of equations k1v + k2x = u is consistent, the unique solution is k1 = −u1 + u2
and k2 = 2u1 −u2.
Thus in (b), [1, 1, −1]T = x;
in (c), [1, 2, 0]T = v;
and in (e),
[−1, 2, 4]T =
3v
−4x.
22.
The vectors a, c, and e are in N(A).
23.
The vectors d and e are in N(A)
since by direct calculation, Ad = θ and Ae = θ.
24.
The vectors v, w, and z are in N(A) .
25.
The vectors x and y are in N(A)
since, by direct calculation Ax = θ and Ay = θ.
26.
The homogeneous system Ax = θ has solution x1 = 2x2
, where x2
is arbitrary.
Thus N(A)={ x in
R2 : x1 −2x2 = 0}. If b=
· b1
b2
¸
,
then Ax=b is consistent if and only if 3b1 +b2 = 0 , so R(A) = {b in R2 : 3b1 +b2 = 0}.
27.
The matrix [A | b] is row equivalent to the matrix :
· −1
3
b1
0
0
2b1 + b2
¸
.
It follows that the homogeneous system Ax = θ has solution x1 = 3x2 whereas the system
Ax = b is consistent if and only if 2b1 + b2 = 0. Therefore N(A) = { x : x1 −3x2 = 0}
and R(A) =
{ b : 2b1 + b2 = 0}.
28. N(A) = { θ} and R(A) = R2.
29.
The matrix [A | b] reduces to
· 1
1
b1
0
3
−2b1 + b3
¸
. Setting b= θ
and solving yields
x1 = x2 = 0
, so N(A)={θ}.
Since the system Ax= b
is consistent for every b
in
R2, R(A) = R2.

3.3. EXAMPLES OF SUBSPACES
69
30. N(A) = { x
in R3 : x1 = −3x3
and x2 = −x3} and R(A) = R2.
31.
The matrix [A | b] reduces to
· 1
2
1
b1
0
0
1
−3b1 + b2
¸
. Setting b = θ and backsolving
yields x1 = −2x2, x3 = 0
as the solution to Ax= θ. Thus N(A)={ x in R3 : x1 +2x2 = 0
and x3 = 0}. Since Ax=b is consistent for arbitrary b in R2, R(A)=R2.
32.
The homogeneous system Ax =θ
has only the trivial solution so N(A) = { θ}.
The
system Ax = b
is consistent precisely when 3b1 −2b2 + b3 = 0
so R(A) = { b
in
R3 : 3b1 −2b2 + b3 = 0}.
33.
The matrix [A | b] reduces to


0
1
b1
0
0
−2b1 + b2
0
0
−3b1 + b3

. Setting b = θ
yields x2 = 0, x1 arbitrary as the solution to Ax = θ. Thus N(A)= { x in R2 : x2 = 0}.
The system Ax = b is consistent if and only if −2b1 +b2 = 0 and −3b1 +b3 = 0. Therefore
R(A) =
{ b
in R3 : b2 = 2b1 and b3 = 3b1}.
34. N(A) = {x in R3 : x1 = −7x3 and x2 = −3x3 } and R(A) = { b in R3 : 3b1−2b2+b3 = 0}.
35.
The matrix [A | b] reduces to


1
2
3
b1
0
1
−2
−b1 + b2
0
0
0
−4b1 + 2b2 + b3

.
Setting b = θ and backsolving the reduced system yields N(A) = {x
in R3 : x1 = −7x3
and x2 = 2x3}. The system Ax = b
is consistent if and only if −4b1 + 2b2 + b3 = 0 so
R(A) = {b
in R3 : −4b1 + 2b2 + b3 = 0}.
36. N(A) = {θ } and R(A) = R3.
37.
The matrix [A | b] reduces to


1
2
1
b1
0
1
2
−2b1 + b2
0
0
1
b1 −b2 + b3

. Setting b= θ
and solving
yields N(A) = { θ}.
The system Ax = b is consistent for all b
so R(A) = R3.
38.
(a)
The vectors b in (i), (iv), and (vi) are in R(A).
(b)
For (i), x = [1, 0]T
is one choice; for (iv), x = [−2, 0]T
is one choice; for (vi), x
= [0, 0]T
is one choice.
(c)
For (i), b = A1
; for (iv), b = −2A1
, for (vi), b = 0A1 +0A2 .
39.
(a)
From the description of R(A)
obtained in Exercise 27 it follows that the vectors b
in (ii),(v), and (vi) are in R(A).

70
CHAPTER 3. THE VECTOR SPACE RN
(b)
When the system of equations Ax = b
is consistent, the calculations done in
Exercise 27 show that the solution is given by x1 = −b1 +3x2 , where x2 is arbitrary.
Thus for (ii), x = [1, 0]T
is one choice; for (v), x = [0, 1]T
is one choice; for (vi), x
= [0, 0]T
is one choice.
(c)
If Ax = b, where x= [x1, x2]T , then b = x1A1 + x2A2. Therefore for (ii), b = A1
˚; for (v), b = A2; for (vi), b = 0A1 + 0A2.
40.
(a)
The vectors b in (ii), (iii), (iv), and (vi) are in R(A) .
(b)
For (ii), x = [−1, −1, 0]T
is one choice; for (iii), x = [2, −1, 0]T
is one choice; for
(iv), x = [2, 1, 0]T
is one choice; for (vi), x = [0, 0, 0]T
is one choice.
(c)
For (ii), b = −A1 −A2 ; for (iii), b = 2A1 −A2 ;
for (iv), x = 2A1 +A2
; for (vi),
x = 0A1 +0A2 +0A3 .
41.
(a)
From the description of R(A)
obtained in Exercise 35, the vectors b
in (i), (iii),
(v), and (vi), are in R(A) .
(b)
When the system Ax= b is consistent, the solution is given by x1 = 3b1 −2b2 −7x3
and x2 = −b1 + b2 + 2x3 , where x3
is arbitrary. Thus for (i), x = [−1, 1, 0]T is one
choice; for (iii), x = [−2, 3, 0]T is one choice for (v), x = [−2, 1, 0]T is one choice; for
(vi), x = [0, 0, 0]T is one choice.
(c).
If Ax = b , where x= [x1, x2, x3]T , then b = x1A1 + x2A2 + x3A3.
Thus it follows from (b) that for (i), b = −A1 +A2˚; for (iii), b = −2A1 +3A2˚; for
(v), b = −2A1 +A2˚; for (vi), b = 0A1 +0A2 +0A3 ,
42. A =


2
−3
1
−1
4
−2
2
1
4


43. A = [ 3
−4
2 ]
44. A = [v, w, x] =


1
0
1
2
−1
1
0
1
−1


45. A = [w, x, z] =


0
1
1
−1
1
0
1
−1
2


46.
Let A be the (3 x 3) matrix whose columns are the vectors given in S. Then AT reduces
to BT =


1
0
−1
0
2
3
0
0
0

. The nonzero columns of B, w1 = [1, 0, −1]T, and w2 = [0, 2, 3]T,
form a basis for Sp(S).

3.3. EXAMPLES OF SUBSPACES
71
47.
Let A be the (3 x 3) matrix whose columns are the vectors given in S.
Then AT reduces
to BT =


−2
1
3
0
3
2
0
0
0

. The nonzero columns of B, w1 = [−2, 1, 3]T and w2 = [0, 3, 2]T
form a basis for Sp(S).
48. w1 = [1, 0, 1]T
and w2 = [0, 1, 1]T .
49.
Let A
be the (3 x 4) matrix whose columns are the vectors given in S. Then AT reduces
to BT =


1
2
2
0
3
1
0
0
0
0
0
0

. The nonzero columns of B, w1 = [1, 2, 2]T and w2 = [0, 3, 1]T form
a basis for Sp(S).
50.
(a) R(I ) = Rn and N(I )={θ}.
(b)
R(O)={θ}
and N(O)= Rn.
(c)
R(A) = Rn and N(A)={θ}.
51.
Let x
be in N(A) ∩N(B). Then Ax = θ and Bx = θ. Therefore (A + B)x = Ax +Bx
= θ + θ˚= θ. It follows that x
is in N(A + B).
52.
(a)
If Bx = θ then (AB)x = A(Bx ) = Aθ˚= θ.
(b)
Suppose b = (AB)x
for some vector x
in Rn.
Then y = Bx
is in Rr
and b
= A(Bx ) = Ay.
53.
Let θm and θn denote the zero vectors in Rm and Rn respectively. Then θn is in W and
θm = Aθn. Therefore θm is in V. Suppose u
and v
are in V. Then there exist vectors
x
and z
in W
such that u = Ax
and v = Az.
Since x +z
is in W, u +v = Ax +Az
= A(x + z)
is in V. If a
is a scalar then ax
is in W
so au = a(Ax ) = A(ax ) is in V.
Thus V
is a subspace of Rn.
54.
If A
has row vectors a1 , . . . , ak , . . . , am
then B
has row vectors a1 , . . . , cak , . . . , am
. Note that d1a1 + · · · + dkak + · · · + dmam = d1a1 + · · · + (dk/c)cak + · · · + dmam . It
follows that :
Sp{a1 , . . . , ak , . . . , am } = Sp{a1 , . . . , cak , . . . , am }.

72
CHAPTER 3. THE VECTOR SPACE RN
3.4
Bases for Subspaces
1.
Backsolving the given system yields x1 = x3 −x4,
and x2 = x4.
Thus


x1
x2
x3
x4

=


x3 −x4
x4
x3
x4

= x3


1
0
1
0

+ x4


−1
1
0
1

.
As in Example 5,{[1, 0, 1, 0]T, [−1, 1, 0, 1]T} is a basis for W.
2.
Backsolving yields x1 = −x3 −2x4
and x2 = 2x3 + x4.
It follows that {[−1, 2, 1, 0]T, [−2, 1, 0, 1]T}
is a basis for W.
3.
Writing x1 = x2 −x3 + 3x4
we have


x1
x2
x3
x4

=


x2 −x3 + 3x4
x2
x3
x4

= x2


1
1
0
0

+ x3


−1
0
1
0

+ x4


3
0
0
1

.
Thus {[1, 1, 0, 0]T, [−1, 0, 1, 0]T, [3, 0, 0, 1]T}
is the desired basis.
4.
Writing x1 = x2 −x3
and noting that x1, x3 and x4
are unconstrained variables, we
obain {[1, 1, 0, 0]T, [−1, 0, 1, 0]T, [0, 0, 0, 1]T}
as the desired basis.
5.
Since x1 = −x2 we have


x1
x2
x3
x4

=


−x2
x2
x3
x4

= x2


−1
1
0
0

+ x3


0
0
1
0

+ x4


0
0
0
1

.
It follows that {[−1, 1, 0, 0]T, [0, 0, 1, 0]T, [0, 0, 0, 1]T}
is a basis for W.
6.
Backsolving yields x1 = 2x4, x2 = 2x4, x3 = x4.
Thus {[2, 2, 1, 1]T} is a basis for W.
7.
Backsolving yields x1 = −2x3 −x4
and x2 = −x3.
Thus


x1
x2
x3
x4

=


−2x3 −x4
−x3
x3
x4

= x3


−2
−1
1
0

+ x4


−1
0
0
1

.
Therefore {[−2, −1, 1, 0]T, [−1, 0, 0, 1]T}
is a basis for W.

3.4. BASES FOR SUBSPACES
73
8.
Backsolving yields x1 = −x4
and x2 = −x3.
Therefore the set
{[−1, 0, 0, 1]T, [0, −1, 1, 0]T}
is a basis for W.
9. Let {w1, w2} be the basis found in Exercise 1. (a) x = 2w1 + w2 (b) x is not in W. (c)
x = −3w2 (d) x = 2w1.
10. Let {w1, w2} be the basis found in Exercise 2. (a) x = w1 + w2 (b) x = 2w1 −w2 (c) x is
not in W. (d) x = −2w2.
11.
(a)
B =


1
2
3
−1
0
−1
−1
1
0
0
0
0

.
(b)
Backsolving the reduced system Bx = θ
yields the solution x1 = −x3 −x4, x2 =
−x3 + x4
for the homogeneous system Ax = θ .
Thus x = [x1, x2, x3, x4]T
is in
N(A)
if and only if


x1
x2
x3
x4

=


−x3 −x4
−x3 + x4
x3
x4

= x3


−1
−1
1
0

+ x4


−1
1
0
1


It follows that {[−1, −1, 1, 0]T , [−1, 1, 0, 1]T } is a basis for N(A) .
(c)
It follows from (b) that x1A1+x2A2+x3A3+x4A4= θ
if and only if x1 = −x3−x4
and x2 = −x3 + x4. Since x3 and x4 are unconstrained variables {A1, A2} is a basis
for R(A) . Setting x3 = 1
and x4 = 0
yields x1 = −1
and x2 = −1
so −A1 −A2
+A3 = θ .
Therefore A3 = A1 +A2 .
Similarly, setting x3 = 0
and x4 = 1
yields
A4 = A1 −A2 .
(d)
The nonzero rows of B
form a basis for the row space of A
;
that is {[1, 2, 3, −1], [0, −1, −1, 1]} is the desired basis.
12.
(a)
B =


1
1
2
0
1
1
0
0
0

.
(b)
The system Ax= θ
has solution x1 = −x3
and x2 = −x3. Therefore {[−1, −1, 1]T}
is a basis for N(A) .
(c)
{A1 , A2 }
is a basis for R(A)
and A3 = A1 +A2 .
(d) {[1, 1, 2], [0, 1, 1]}
is a basis for the row space of A .
13.
(a) B =


1
2
1
0
0
1
1
−1
0
0
0
0
0
0
0
0

.

74
CHAPTER 3. THE VECTOR SPACE RN
(b)
The homogeneous system Ax = θ
has solution x1
= x3 −2x4, x2 = −x3 + x4.
Thus x = [x1, x2, x3, x4]T
is in
N(A)
if and only if


x1
x2
x3
x4

=


x3 −2x4
−x3 + x4
x3
x4

= x3


1
−1
1
0

+ x4


−2
1
0
1

.
The set {[1, −1, 1, 0]T, [−2, 1, 0, 1]T} is a basis for N(A) .
(c)
It follows from (b) that in the equation x1A1 +x2A2 +x3A3 +x4A4 = θ, x3
and x4
are unconstrained variables. Therefore {A1 , A2 }
is a basis for R(A) . Furthermore
A1 −A2 +A3 = θ,
so A3 = −A1 +A2 .
Likewise −2A1 +A2 +A4 = θ,
so A4 = 2A1
−A2 .
(d)
The nonzero rows of B, [1, 2, 1, 0], [0, 1, 1, −1],
form a basis for the row space of A.
14.
(a) B =


2
2
0
0
−1
1
0
0
1

.
(b) The system Ax = θ
has only the trivial solution so N(A) = {θ}.
(c) It follows from (b) that the columns of A are linearly independent so {A1 , A2 , A3}
is a basis for R(A) .
(d) The set {[2, 2, 0], [0, −1, 1], [0, 0, 1]}
is a basis for the row space of A.
15.
(a)
B =


1
2
1
0
0
−1
0
0
0

.
(b)
The system Ax = θ
has solution x1 = −2x2, x3 = 0.
Thus N(A) = {x : x = [−2x2, x2, 0]T}
and {[−2, 1, 0]T}
is a basis for N(A) .
(c)
In the equation x1A1 +x2A2 +x3A3
= θ ,
x2
is an unconstrained variable, so
{A1 , A3 }
is a basis for R(A) .
Furthermore, −2A1 +A2 =θ ,
so A2 = 2A1 .
(d)
{[1, 2, 1], [0, 0, −1]}
is a basis for the row space of A.
16.
(a)
B =


2
1
2
0
1
−1
0
0
0

.
(b)
x= [x1, x2, x3]T
is in N(A)
if and only if x1 = (−3/2)x3
and x2 = x3.
Therefore
{[−3/2, 1, 1]T}
is a basis for N(A) .

3.4. BASES FOR SUBSPACES
75
(c)
{A1 , A2 }
is a basis for R(A)
and A3 = (3/2)A1 −A2 .
(d)
{[2, 1, 2], [0, 1, −1]}
is a basis for the row space of A.
17.
The matrix AT
is row equivalent to BT =


1
3
1
0
−1
−1
0
0
0
0
0
0

.
The desired basis is
{[1, 3, 1]T, [0, −1, −1]T},
formed by taking the nonzero columns of B.
18.
The matrix AT
is row equivalent to BT =


1
1
2
0
0
1
0
0
0

.
The desired basis is
{[1, 1, 2]T, [0, 0, 1]T},
formed by taking the nonzero columns of B.
19.
The matrix AT
is row equivalent to BT =


1
2
2
0
0
1
−2
1
0
0
0
0
0
0
0
0


so
{[1, 2, 2, 0]T, [0, 1, −2, 1]T}
is a basis for R(A) .
20.
The matrix AT
is row equivalent to BT =


2
2
2
0
−1
1
0
0
1


so
{[2, 2, 2]T, [0, −1, 1]T, [0, 0, 1]T}
is a basis for R(A) .
21.
(a)
For the given vectors u1
and u2
the equation x1u1 +x2u2 = θ
has solution
x1 = −2x2
where x2
is an unconstrained variable. Therefore {u1 }
is a basis for
Sp(S),
where u1 = [1, 2]T.
(b) If A = [u1, u2] then AT is row equivalent to BT =
· 1
2
0
0
¸
. Therefore {[1, 2]T } is a
basis for Sp (S).
22.
(a)
For the given vectors u1 , u2
and u3
the equation x1u1 +x2u2 +x3u3 = θ
has
solution x1 = (−1/3)x3
and x2 = (−4/3)x3,
where x3
is arbitrary. Thus {u1 , u2
}
is a basis for Sp(S),
where u1 = [1, 2]T
and u2 = [2, 1]T.
(b)
If A = [u1 , u2 , u3 ]
then AT
is row equivalent to BT =


1
2
0
−3
0
0

.
Therefore {[1, 2]T, [0, −3]T}
is a basis for Sp(S).
23.
(a)
For the given vectors u1 , u2 , u3 , u4
the equation x1u1 +x2u2 +x3u3 +x4u4 = θ
has solution x1 = −x3 −3x4, x2 = −x3 + x4.
Since x3
and x4
are unconstrained
variables, {u1 , u2 }
is a basis for Sp(S),
where u1 = [1, 2, 1]T
and u2 = [2, 5, 0]T.

76
CHAPTER 3. THE VECTOR SPACE RN
(b)
If A = [u1 , u2 , u3 , u4 ]
then AT
is row equivalent to BT =


1
2
1
0
1
−2
0
0
0
0
0
0

.
Therefore {[1, 2, 1]T, [0, 1, −2]T}
is a basis for Sp(S).
24.
(a)
For the given vectors u1 , u2 , u3 , u4 ,
in the equation x1u1 +x2u2 +x3u3 +x4u4
= θ , x4
is an unconstrained variable. The desired basis is {u1 , u2 , u3 },
where u1
= [1, 2, −1, 3]T, u2 = [−2, 1, 2, −1]T,
and u3 = [−1, −1, 1, −3]T.
(b)
If A = [u1 , u2 , u3 , u4 ],
then AT
reduces to BT =


1
2
−1
3
0
1
0
0
0
0
0
5
0
0
0
0

.
Therefore {[1, 2, −1, 3]T, [0, 1, 0, 0]T,
[0, 0, 0, 5]T}
is a basis for Sp(S).
25.
(a)
Let A
denote the given matrix. The homogeneous system
Ax = θ
has solution x1 = 0, x2
is arbitrary, x3 = 0.
Thus
{[0, 1, 0]T}
is a basis for N(A) .
(b)
Let A
denote the given matrix. The system Ax = θ
has solution x1 = −x2
,
where x2
and x3
are arbitrary. Thus {[−1, 1, 0]T, [0, 0, 1]T}
is a basis for N(A) .
(c)
The system Ax = θ
has solution x1 = −x2, x3 = 0,
where x2
is arbitrary. The
set {[−1, 1, 0]T}
is a basis for N(A) .
26. (a) {[1, 1]T, [0, 1]T}. (b) {[1, 1]T}.
(c) {[1, 1]T, [0, 1]T}.
27.
The equation x1v1 +x2v2 +x3v3 = θ
has solution x1 = −2x3,
x2 = −3x3, x3
arbitrary. In particular, x1 = −2, x2 = −3, x3 = 1
is a nontrivial solution
and the set S
is linearly dependent. Moreover, from −2v1 −3v2 +v3 = θ
we obtain v3
= 2v1 +3v2 . If v
is in Sp(S)
then v = a1v1 +a2v2 +a3v3 = (a1 + 2a3)v1 +(a2 + 3a3)v2
,
so v
is in Sp{v1, v2 }. It follows that Sp{v1 , v2 , v3 } =
Sp{v1 , v2 }.
28. The subsets {v1, v2}, {v1, v3}, {v2, v3} are bases for R2.
29. The subsets are {v1, v2, v3}, {v1, v3, v4}, and {v1, v2, v4}. Note that v4 = 3v2 −v3.
30.
By Theorem 12 of Section 1.8, the matrix V = [v1 , v2 , v3 ] is nonsingular. Thus, by
Theorem 13 of Section 1.8, the system of equations Ax = b
has a solution for each b
in
R3; that is each vector b in R3
can be written in the form x1v1 +x2v2 +x3v3 = b. This
shows that R3 = Sp(B)
so B
is a basis for R3.

3.5. DIMENSION
77
31.
Set V = [v1 , v2 , v3 ].
By assumption the system V x= b
has a solution for every b
in
R3
. By Theorem 13 of Section 1.8, V
is a nonsingular matrix. Therefore, by Theorem
12 of Section 1.8, the set {v1 , v2 , v3 }
is linearly independent.
32.
The set S
is linearly independent so, by Exercise 30, S
is a basis for R3.
33.
The set S
is linearly dependent so S
is not a basis for R3.
34.
The set S
is linearly dependent so S
is not a basis for R3.
35.
If u= [u1, u2, u3]T
then u is in Sp(S)
if and only if 4u1 −2u2 + u3 = 0.
In particular,
Sp(S) ̸= R
and S
is not a basis for R3.
36.
A vector w = [w1, w2, w3]T is in Sp{v1, v2}
if and only if w1 + w3 =
0.
In particular w = [0, 0, 1]T
is not a linear combination of v1
and v2 .
37.
(a)
By Theorem 11 of Section 1.8, any set of three or more vectors in R2
is linearly
dependent and is not a basis for R2.
(b)
Suppose {v }
is a basis for R2.
Then e1 = a1v
and e2 = a2v
for some nonzero
scalars a1
and a2.
But then a2e1 −a1e2 = θ,
contradicting the fact that {e1 , e2 }
is a linearly independent set. We conclude that {v } is not a basis for R2. It follows
that every basis for R2
contains exactly two vectors.
38.
If vT= [x1, x2, . . . , xn]
then the constraints vTui = 0, 1 ≤i ≤p,
yield a homogeneous
system of p
equations in the unknowns x1, x2, . . . , xn.
By Theorem 4 of Section 1.4 the
system has nontrivial solutions.
Suppose v= a1u1+a2u2+ · · ·+apup. Then ∥v∥2 = vTv= vT(a1u1+a2u2+ · · ·+apup) =
a1v Tu1 +a2v Tu2 + · · · + apv Tup = 0,
contradicting that v
is a nonzero vector.
39.
By Theorem 11 of Section 1.8, any set of n + 1
or more vectors in Rn
is linearly
dependent so it is not a basis for Rn. By Exercise 38, any set of less than n
vectors cannot
span Rn. Therefore a basis for Rn
must contain exactly n
vectors.
3.5
Dimension
1. S
contains only one vector and dim(R2) = 2, so by property 2 of Theorem 9, S does not
span R2.
2. S does not span R2
by property 2 of Theorem 9
3. Since S contains three vectors and dim(R2) = 2, S is linearly
dependent by property 1 of Theorem 9.

78
CHAPTER 3. THE VECTOR SPACE RN
4. S is linearly dependent by property 1 of Theorem 9.
5.
Since u4 = θ, S
is a linearly dependent set; for example 0u1 +au4 = θ for any nonzero
scalar a.
Also S does not span R2 since
Sp{u1, u4 } = Sp{u1 }.
6. S is linearly dependent since, for example, 3u1 −u2 = θ.
7. S contains two vectors and dim(R3) = 3 so by property 2 of Theorem 9, S does not span
R3.
8. S does not span R3 by property 2 of Theorem 9.
9. Since S
contains four vectors and dim(R3) = 3, S
is linearly dependent by property 1 of
Theorem 9.
10.
It is easily checked that S
is a linearly independent set. Therefore, by property 3 of
Theorem 9, S is a basis for R2.
11. It is easily checked that S is a linearly independent set. Since S
contains two vectors and dim(R2) = 2 it follows from property 3 of Theorem 9 that S is
a basis for R2.
12.
The set S is linearly independent so, by property 3 of Theorem 9, S is a basis for R3.
13.
It is easily shown by direct calculation that S is a linearly dependent set. Therefore S is
not a basis for R3.
14.
The set S is linearly independent so, by property 3 of Theorem 9, S is a basis for R3.
15.
If we write x1 = 2x2 −x3 + x4 then the procedure described in Example 5 of Section 2.4
yields a basis {[2, 1, 0, 0]T, [−1, 0, 1, 0]T, [1, 0, 0, 1]T}
for W. It follows that dim(W) = 3.
16. dim(W) = 3.
17. Following the procedure used in Example 5 of Section 2.4, we obtain a basis
{[1, −1, 0, 0]T, [2, 0, −1, 0]T}
for W. In particular dim(W) = 2.
18. dim(W) = 2.
19.
The set {[−1, 3, 2, 1]T} is a basis for W, so dim(W) = 1.
20. dim(W) = 1.
21. The homogeneous system Ax = θ
has solution x1 = −2x2.
Therefore {[−2, 1]T}
is a basis for N(A) and nullity (A) = 1. Since 2 = rank (A) +
nullity (A), it follows that rank (A) = 1.

3.5. DIMENSION
79
22. The set {[2, 1, 1]T} is a basis for N(A). Therefore nullity (A) = 1 and rank (A) = 2.
23.
The homogeneous system Ax= θ
has solution x1 = −5x3, x2 = −2x3. Thus {[−5, −2, 1]T}
is a basis for N(A) and nullity (A) = 1. Since 3 = rank (A) + nullity (A), it follows that
rank (A) = 2.
24.
The set {[2, −1, 1, 0]T}
is a basis for N(A). Therefore nullity (A) = 1 and rank (A) = 3.
25. AT
reduces to BT =


1
−1
1
0
2
3
0
0
0

. It follows that {[1, −1, 1]T,
[0, 2, 3]T} is a basis for R(A). Consequently rank (A) = 2. Since 3 = rank (A)+nullity (A),
it follows that nullity (A) = 1.
26.
The matrix AT reduces to BT =


1
2
2
0
2
−1
0
0
0
0
0
0

. Therefore
{[1, 2, 2]T, [0, 2, −1]T}
is a basis for R(A), rank (A) = 2
and nullity (A) = 2.
27.
(a)
Following the methods of Example 7 in Section 2.4, let A =


1
−1
1
2
1
−2
0
−1
−2
3
−1
0

.
Then AT reduces to BT =


1
1
−2
0
−1
1
0
0
1
0
0
0

. It follows that { [1, 1, −2]T, [0, −1, 1]T,
[0, 0, 1]T }
is a basis for W
. In particular dim(W) = 3.
(b)
Following the procedure in (a), we obtain a basis {[1, 2, −1, 1]T,
[0, 1, −1, 1]T, [0, 0, −1, 4]T}
for W. In particular, dim(W) = 3.
28. W = {x
in R4 : x1 + 2x2 −3x3 −x4 = 0}. It follows that dim(W) = 3.
29.
The constraints aTx= 0, bTx= 0
and cTx= 0 yield the homogeneous system of equations
x1 −x2 = 0, x1 −x3 = 0,
and x2 −x3 = 0.
Solving we obtain x1 = x3 and x2 = x3
where x3
and x4
are arbitrary. Thus {[1, 1, 1, 0]T, [0, 0, 0, 1]T}
is a basis for W
and
dim(W) = 2.
30.
Following the procedure described in the hint, suppose we have obtained a linearly in-
dependent subset Sk = {w1 , . . . , wk } of W.
If Sk
spans W
we are done.
If not
there exists a vector wk+1
in W
such that wk+1
is not in Sp(Sk).
Suppose a1w1
+ · · · + akwk +ak+1wk+1 = θ.
Now ak+1 = 0
since otherwise we could solve for wk+1 ,
contradicting that wk+1
is not in Sp(Sk) . Since Sk
is linearly independent, it follows

80
CHAPTER 3. THE VECTOR SPACE RN
that ai = 0, 1 ≤i ≤k.
This shows that the set Sk+1 = {w1 , . . . , wk , wk+1 } is linearly
independent. A linearly independent subset of Rn
contains at most n
vectors, so the
process must eventually stop. That is, there is a linearly independent subset Sm = {w1
, . . . , wm } such that Sm spans W. Thus Sm is a basis for W.
31. Suppose x = a1u1 +a2u2 + · · · + apup
and x = b1u1 +b2u2 + · · · + bpup . Then θ =x −x
= (a1 −b1)u1 +(a2 −b2)u2 + · · · + (ap −bp)up.
Since {u1 , u2 , . . . , up }
is linearly
independent, a1−b1 = 0, a2−b2 = 0, . . . , ap−bp = 0. Therefore a1 = b1, a2 = b2, . . . , ap = bp.
32.
Let B = {u1 , . . . , um }
be a basis for U. Then B is a linearly independent subset of V
so by property 1 of Theorem 9, m ≤dim(V )
Moreover if m = dim(V ) then by property 3 of Theorem 9, B
is also a basis for V. It
follows that V = W.
33.
(a) rank (A) ≤3 and nullity (A) ≥0.
(b) rank (A) ≤3 and nullity (A) ≥1.
(c) rank (A) ≤4 and nullity (A) ≥0.
34. Use Theorem 9, part(1). The columns of A are vectors in R3.
35. Use Theorem 9, part(1). The rows of A, when transposed, are vectors in R3.
36.
Since n = rank (A)+nullity (A), it follows that rank (A) ≤n. Further, R(A)
is a subset
of Rm so, by Exercise 32, rank (A) ≤m.
37.
Clearly 2 = rank (A) ≤rank ([A | b ]). By Exercise 36,
rank ([A | b ]) ≤2.
Therefore rank ([A | b ]) = 2 = rank (A) and,
by Theorem 11, the system Ax = b
is consistent.
38. 4 = rank (A) + nullity (A), so 3 = rank (A) ≤rank ([A | b ]). By Exercise 36, rank ([A | b
]) ≤3. Therefore rank (A) = 3 =
rank ([A | b ]) and, by Theorem 11, the system Ax = b
is consistent.
39. The matrix A is, by deﬁnition, nonsingular if and only if N(A) = { θ }.
40. If x is in N(B) then (AB)x = A(Bx) = Aθ = θ, so x is also in N(AB). Conversely, if x is
in N(AB) then θ = (AB)x = A(Bx). Since A is nonsingular, Bx = θ and x is in N(B).
41.
Suppose c1w1 + · · · + cpwp = θ
where ci ̸= 0.
Then wi = a1w1 + · · · + ai−1wi−1
+ai+1wi+1 + · · · + apwp , where aj = −cj/ci. If w
is any vector in W
then w = b1w1
+ · · · + bpwp
for some scalars b1, . . . , bp.
Substituting for wi
yields w = (b1 + bia1)w1
+ · · · + (bi−1 + biai−1)wi−1 +(bi+1 + biai+1)wi+1 + · · · + (bp + biap)wp .
It follows that W = Sp{w1 , . . . , wi−1 , wi+1 , . . . , wp }.
By Theorem 8 any set of p
vectors in W
is linearly dependent. This contradicts the assumption that dim(W) = p.
We conclude that ci = 0
for each i
so S
is a linearly independent set.

3.6. ORTHOGONAL BASES FOR SUBSPACES
81
42. See the proof given in Exercise 30.
3.6
Orthogonal Bases for Subspaces
1. u1Tu2 = 1(−1) + 1(0) + 1(1) = 0; u1Tu3 = 1(−1) + 1(2) + 1(−1) = 0; u2Tu3 = −1(−1) +
0(2) + 1(−1) = 0.
2. u1Tu2 = u1Tu3 = u2Tu3 = 0.
3. u1Tu2 = 1(2) + 1(0) + 2(−1) = 0; u1Tu3 = 1(1) + 1(−5) + 2(2) = 0; u2Tu3 = 2(1) + 0(−5) +
(−1)2 = 0.
4. u1Tu2 = u1Tu3 = u2Tu3 = 0.
5. 0 = u1Tu3 = a + b + c
and 0 = u2Tu3 = 2a + 2b −4c.
Solving yields a = −b, b arbitrary,
and c = 0.
6. a = (−1/2)c, b = (5/2)c, c
arbitrary.
7. 0 = u1Tu2 = −3 + a; 0 = u1Tu3 = 4 + b + c; 0 = u2Tu3 = −8 −b + ac.
Solving yields
a = 3, b = −5, c = 1.
8. 0 = u1Tu2 = 2a + 2; 0 = u1Tu3 = 2b + 3 −c; 0 = u2Tu3 = ab + 3 −c.
Solving yields
a = −1, , b = 0, c = 3.
9. v = a1u1 +a2u2 +a3u3
where a1 = (u1Tv )/(u1Tu1) = 2/3, a2 =
(u2Tv )/(u2Tu2 ) = −1/2, a3 = (u3Tv )/(u3Tu3 ) = 1/6.
10. v = u1 +u2 .
11. v = a1u1 +a2u2 +a3u3
where a1 = (u1Tv )/(u1Tu1 ) = 9/3 = 3,
a2 = (u2Tv )/(u2Tu2 ) = 0, a3 = (u3Tv )/(u3Tu3 ) = 0.
12. v = (4/3)u1 +(1/3)u3 .
13.
Denote the given vectors by, w1 , w2 , w3 ,
respectively. Then u1 = w1 = [0, 0, 1, 0]T. u2
= w2 −c1u1 ,
where c1 =
(u1Tw2 )/(u1Tu1 ) = 2.
Then u2 = [1, 1, 0, 1]T, u3 = w3 −b1u1 −b2u2
where b1 = (u1Tw3
)/(u1Tu1 ) = 1
and
b2 = (u2Tw3 )/(u2Tu2 ) = 2/3.
Therefore u3 = [1/3, −2/3, 0, 1/3]T.
14. u1 = [1, 0, 1, 2]T, u2 = [1, 1, −1, 0]T, u3 = [1/2, −1, −1/2, 0]T.
15.
Denote the given vectors by w1 ,w2 , w3 , respectively. Then u1 = w1 = [1, 1, 0]T , u2 = w2
−c1u1, where c1 = (u1Tw2 )/(u1Tu1 ) = 2/2 = 1. Thus u2 = [−1, 1, 1]T . u3 = w3 −b1u1
−b2u2 , where b1 = (u1Tw3 )/(u1Tu1 ) = 2/2 = 1 and b2 = (u2Tw3 )/(u2Tu2 ) = 6/3 = 2.
Therefore u3 = [2, −2, 4]T.

82
CHAPTER 3. THE VECTOR SPACE RN
16. u1 = [0, 1, 2]T, u2 = [3, 4, −2]Tu3 = [10, −6, 3]T.
17. Denote the given system by w1, w2, w3, respectively. Then u1 = w1 = [0, 1, 0, 1]T , u2=
w2 −c1u1, where c1 = (u1Tw2 )/(u1Tu1 ) = 2/2 = 1. Thus u2 = [1, 1, 0, −1]T , u3 = w3 −b1
u1 −b2u2, where b1 = (u1Tw3 )/(u1Tu1 ) = 2/2 = 1 and b2 = (u2Tw3 )/(u2Tu2 ) = 2/3.
Therefore u3 = [−2/3, 1/3, 1, −1/3]T .
18. u1 = [1, 1, 0, 2]T, u2 = [−1, 1, 1, 0]T, u3 = [−1/2, −1/6, −1/3, 1/3]T.
19.
If A
denotes the given matrix then the homogeneous system Ax = θ has solution
x1 = −3x3 −x4, x2 = −x3 −3x4,
where x3
and x4
are arbitrary. It follows that
{[−3, −1, 1, 0]T, [−1, −3, 0, 1]T}
is a basis for N(A) and {A1 , A2 }
is a basis for R(A) ,
where A1= [1, 2, 1]T and A2= [−2, 1, −1]T. The Gram-Schmidt process yields orthogonal
bases {[−3, −1, 1, 0]T , [7/11, −27/11, −6/11, 1]T } and {[1, 2, 1]T , [−11/6, 8/6, −5/6]T } for
N(A) and R(A) respectively.
20.
A basis for N(A) is {[−1, −3, 1, 0, 0]T , [−2, −3, 0, 1, 0]T , [−3, −2, 0, 0, 1]T }.
The Gram-Schmidt process yields the orthogonal basis
{[−1, −3, 1, 0, 0]T , [−1, 0, −1, 1, 0]T , [−13/11, 5/11, 2/11, −1, 1]T }.
A basis for R(A) is {[1, −1, 2]T , [3, 2, −1]T } and the Gram-Schmidt process yields the or-
thogonal basis {[1, −1, 2]T , [19/6, 11/6, −4/6]T}.
21.
By Theorem 13 an orthogonal set of nonzero vectors is linearly independent. By property
1 from Theorem 9, Section 2.5, any set of four or more vectors in R3
is linearly dependent.
22. It follows from Theorem 13 of Section 2.6 and Theorem 12 of Section 1.8 that A
is
nonsingular. Note that ATA
is the (3 x 3) matrix [cij]
where cij = uiTuj .
Since S
is
orthogonal cij = uiTuj = 0
if i ̸= j
and cii = uiTui =∥ui ∥2
for i = 1, 2, 3.
For the
vectors given in Exercise 1, ATA =


3
0
0
0
2
0
0
0
6

.
23.
Since v
is in W, 0 = vTv = (∥v∥)2.
Therefore v = θ.
24. Suppose y ̸= θ.
For any scalar c, 0 ≤∥x −cy ∥2= (x −cy )T
(x −cy ) = xTx −cxTy −cyTx +c2yTy = (∥x ∥)2
−2cxTy +c2(∥y ∥)2.
For c = xTy /yTy
this implies that 0 ≤∥x ∥2 −(xTy )2/ ∥y ∥2.
It follows that | xTy |≤∥x ∥∥y ∥.
If y = θ
then | xTy |= ∥x∥∥y∥= 0.
25. ∥x + y∥2 = (x + y )T(x + y ) =∥x∥2 + 2(xTy )+ ∥y∥2 ≤
∥x∥2 + 2 | xTy | + ∥y∥2 ≤∥x∥2 + 2 ∥x∥∥y∥+ ∥y∥2 = (∥x∥+ ∥y∥)2.
26.
Note that ∥y∥=∥x + (y −x)∥≤∥x∥+ ∥y −x∥
so
−(∥x∥−∥y∥) =∥y∥−∥x∥≤∥y −x∥=∥x −y∥.
Similarly, ∥x∥=

3.7. LINEAR TRANSFORMATIONS FROM RN
TO RM
83
∥(x −y) + y ∥≤∥x −y ∥+ ∥y ∥.
Therefore, ∥x ∥−∥y ∥≤∥x −y ∥.
It follows that
| ∥x∥−∥y∥| ≤∥x −y∥.
27.
If W = Sp{wi }p−1
i=1
then {ui }p−1
i=1
is an orthogonal basis for W˚. Since {wi }p
i=1 is linearly
dependent, wp
is in W . Therefore up
is in W
and upTui = 0
for 1 ≤i ≤p −1.
It
follows that upTw = 0
for every vector w
in W .
By Exercise 23, up = θ.
28. ∥v∥2 = (a1u1 + · · · + apup )T(a1u1 + · · · + apup ) =
P
1≤i,j≤p aiajuiTuj = Pp
i=1 a2
i uiTui = Pp
i=1 a2
1
since B
is an orthonormal basis.
3.7
Linear Transformations from Rn
to Rm
1.
(a) T
µ· 0
0
¸¶
=
· 0
0
¸
.
(b) T
µ· 1
1
¸¶
=
· −1
0
¸
.
(c) T
µ· 2
1
¸¶
=
·
1
−1
¸
.
(d) T
µ· −1
0
¸¶
=
· −2
1
¸
.
2.
(a) T
µ· 2
2
¸¶
=
· 0
0
¸
.
(b) T
µ· 3
1
¸¶
=
·
2
−6
¸
.
(c) T
µ· 2
0
¸¶
=
·
2
−6
¸
.
(d) T
µ· 0
0
¸¶
=
· 0
0
¸
.
3. (a), (b), and (d) are in the null space of T.
4. T(x ) = b
requires 2x1 −3x2 = 2
and −x1 + x2 = −2.
Solving yields x1 = 4, x2 = 2,
so x = [4, 2]T.
5.
If b = [b1, b2]T
then T(x ) = b
requires that 2x1 −3x2 = b1
and −x1 + x2 = b2.
Solving yields x1 = −b1 −3b2
and x2 = −b1 −2b2;
that is x = [−b1 −3b2, −b1 −2b2]T.
6. T(x ) = b
if and only if Ax = b .
Solving yields x1 = −2 + x2, x2
arbitrary. For
example, if x = [−2, 0]T
then T(x ) = b .
7.
The system of equations Ax = b
is easily seen to be inconsistent.

84
CHAPTER 3. THE VECTOR SPACE RN
8. Let u = [u1, u2]T
and v = [v1, v2]T.
Then
F(u + v) =
· 2(u1 + v1) −(u2 + v2)
(u1 + v1) + 3(u2 + v2)
¸
=
· 2u1 −u2
u1 + 3u2
¸
+
· 2v1 −v2
v1 + 3v2
¸
= F(u) + F(v).
Also F(au ) =
· 2au1 −au2
au1 + 3au2
¸
= a
· 2u1 −u2
u1 + 3u2
¸
= aF(u ).
This shows that F
is a linear transformation.
9. F
is a linear transformation.
10. F
is not a linear transformation. For example if u = [u1, u2]T
and v = [v1, v2]T
then
F(u + v) =
· (u1 + v1) + (u2 + v2)
1
¸
whereas F(u) + F(v) =
· (u1 + v1) + (u2 + v2)
2
¸
.
Likewise,
F(au ) =
· au1 + au2
1
¸
whereas aF(u ) =
· au1 + au2
a
¸
.
11. F
is not a linear transformation. For example F
µ· 1
2
¸
+
· 2
1
¸¶
= F
µ· 3
3
¸¶
=
· 9
9
¸
whereas F
µ· 1
2
¸¶
+ F
µ· 2
1
¸¶
=
· 1
2
¸
+
· 4
2
¸
=
· 5
4
¸
.
12. If u = [u1, u2, u3]T
and v = [v1, v2, v3]T
then F(u +v ) =
·
(u1 + v1) −(u2 + v2) + (u3 + v3)
−(u1 + v1) + 3(u2 + v2) −2(u3 + v3)
¸
=
·
u1 −u2 + u3
−u1 + 3u2 −2u3
¸
+
·
v1 −v2 + v3
−v1 + 3v2 −2v3
¸
= F(u ) + F(v ).
For any scalar a, F(a u ) =
·
au1 −au2 + au3
−au1 + 3au2 −2au3
¸
=
a
·
u1 −u2 + u3
−u1 + 3u2 −2u3
¸
= a F(u ).
Thus F
is a linear transformation.
13. F
is a linear transformation.
14. If u = [u1, u2]T and v = [v1, v2]T
then F(u +v ) =


(u1 + v1) −(u2 + v2)
−(u1 + v1) + (u2 + v2)
u2 + v2

=


u1 −u2
−u1 + u2
u2

+


v1 −v2
−v1 + v2
v2

=

3.7. LINEAR TRANSFORMATIONS FROM RN
TO RM
85
F(u ) + F(v ). Similarly, F(au ) =


au1 −au2
−au1 + au2
au2

=
a


u1 −u2
−u1 + u2
u2

= aF(u ). It follows that F
is a linear transformation.
15. F is a linear transformation.
16. Let u = [u1, u2]T
and v = [v1, v2]T.
Then F(u + v ) = 2(u1 + v1) + 3(u2 + v2) =
(2u1 + 3u2) + (2v1 + 3v2) = F(u ) + F(v ).
Likewise
F(au ) = 2au1 + 3au2 = a(2u1 + 3u2) = a F(u ).
This means that F
is a linear transfor-
mation.
17. F
is not a linear transformation. For example note that
F(−e1 ) = 1
whereas −F(e1 ) = −1.
18.
The set {e1 }
is an orthonormal basis for W
and T(v ) = (vTe1 )e1 = [a, 0, 0]T.
19.
(a) T
µ· 1
1
¸¶
= T(e1 +e2 ) = T(e1 ) + T(e2 ) = u1 +u2 = [3, 1, −1]T.
(b) T
µ·
2
−1
¸¶
= T(2e1 −e2 ) = 2T(e1 ) −T(e2 ) = 2u1 −u2 = [0, −1, −2]T.
(c) T
µ· 3
2
¸¶
= T(3e1 +2e2 ) = 3T(e1 ) + 2T(e2 ) = 3u1 +2u2 = [7, 2, −3]T.
20.
(a) T
µ· 1
1
¸¶
= T(2v1 −v2 ) = 2u1 −u2 = [−3, 3]T.
(b) T
µ·
2
−1
¸¶
= T(v1 −2v2 ) = u1 −2u2 = [−6, 0]T.
(c) T
µ· 3
2
¸¶
= T(5v1 −3v2 ) = 5u1 −3u2 = [−9, 7]T.
21.
Let u1 = [1, 1]T
and u2 = [1, −1]T.
If x = [x1, x2]T
then x =
[(x1 + x2)/2]u1 +[(x1 −x2)/2]u2 .
Thus T(x ) =
[(x1 + x2)/2]
·
2
−1
¸
+ [(x1 −x2)/2]
· 0
3
¸
=
· x1 + x2
x1 −2x2
¸
.
22. T(x ) =


(x1 + x2)/2
2x1
(3x1 −x2)/2

.

86
CHAPTER 3. THE VECTOR SPACE RN
23.
Let u1 = [1, 0, 1]T, u2 = [0, −1, 1]T, u3 = [1, −1, 0]T.
If x =
[x1, x2, x3]T
then x = c1u1 +c2u2 +c3u3 ,
where c1 =
(x1 + x2 + x3)/2, c2 = (−x1 −x2 + x3)/2 and c3 = (x1 −x2 −x3)/2.
Therefore T(x ) = c1[0, 1]T + c2[1, 0]T + c3[0, 0]T;
that is,
T




x1
x2
x3



=
· (−x1 −x2 + x3)/2
(x1 + x2 + x3)/2
¸
.
24. T




x1
x2
x3



=


−x1 −x2 + x3
−x1 −x2
x1

.
25. A = [T(e1 ), T(e2 )] =
· 1
3
2
1
¸
.
The homogeneous system of equations Ax = θ has only
the trivial solution so N(T) = N(A) = { θ}
and nullity (T) = 0.
Since rank (T) =
2 −nullity (T) = 2,
it follows that R(T) = R2.
26. A =


1
−1
1
1
0
1

; N(T) = N(A) = { θ}; R(T) = R(A) =
{[b1, b2, b3]T : b1 −b2 + 2b3 = 0}; nullity (T) = 0; rank (T) = 2.
27. A = [T(e1 ), T(e2 )] = [3, 2]; N(T) = {x : 3x1 + 2x2 = 0}; R(T) = R1; rank (T) =
1; nullity (T) = 1.
28. A =


1
1
0
0
0
1
0
1
0

; N(T) = N(A) = { θ}; R(T) = R(A) = R3;
rank (T) = 3; nullity (T) = 0.
29. A = [T(e1 ), T(e2 ), T(e3 )] =
· 1
−1
0
0
1
−1
¸
; N(T) = N(A) =
{x : x1 = x3, x2 = x3}; R(T) = R(A) = R2; rank (T) = 2;
nullity (T) = 1.
30. A = [2, −1, 4]; N(T) = {x : 2x1 −x2 + 4x3 = 0}; R(T) = R1;
rank (T) = 1; nullity (T) = 2.
31.
For any x and y in R, f(x + y) = a(x + y) = ax + ay = f(x) + f(y).
If b is any real number then f(bx) = a(bx) = b(ax) = bf(x).
Therefore f
is a linear
transformation.

3.7. LINEAR TRANSFORMATIONS FROM RN
TO RM
87
32.
Since T
is a linear transformation and x can be viewed as a scalar, T(x) = T(x · 1) =
xT(1) = xa = ax for each x in R.
33. T
µ· x1
x2
¸¶
=
·
x1
−x2
¸
34. T
µ· x1
x2
¸¶
=
· x2
x1
¸
35.
For vectors u
and v
in V, [F + G](u +v ) =
F(u +v ) + G(u +v ) = [F(u ) + F(v )]+
[G(u ) + G(v )] = [F(u ) + G(u )] + [F(v ) + G(v )] =
[F + G](u ) + [F + G](v ). Similarly, [F + G](au ) =
F(au ) + G(au ) = aF(u ) + aG(u ) = a[F(u ) + G(u )] =
a[F + G](u )
for every scalar a. This proves that F + G
is a linear transformation.
36.
(a) (F + G)




x1
x2
x3



=
·
x1 + x2 + 3x3
2x1 + 5x2 −2x3
¸
.
(b) A =
· 2
−3
1
4
2
−5
¸
, B =
· −1
4
2
−2
3
3
¸
, C =
·
1
1
3
−2
3
3
¸
37. [aT](u +v ) = a[T(u +v )] = a[T(u ) + T(v )] = aT(u ) + aT(v ) =
[aT](u ) + [aT](v ).
Also [aT](bu ) = a(T(bu )) = a(bT(u )) =
b(aT(u )) = b[aT](u ).
This proves that aT
is a linear transformation.
38.
(a) [3T]




x1
x2
x3



=
· 3x1 −3x2
3x2 −3x3
¸
(b) A =
· 1
−1
0
0
1
−1
¸
, B =
· 3
−3
0
0
3
−3
¸
39. For u1
and u2
in U, [G ◦F](u1 +u2 ) = G(F(u1 + u2 )) =
G(F(u1 ) + F(u2 )) = G(F(u1 )) + G(F(u2 )) = [G ◦F](u1 )+
[G ◦F](u2 ).
If u
is in U and a is any scalar then [G ◦F](au ) =
G(F(au)) = G(aF(u)) = aG(F(u)) = a[G◦F](u). Thus G◦F
is a linear transformation.
40.
(a) [G ◦F]




x1
x2
x3



=


−5x1 −8x2 −6x3
x1 + 16x2 −10x3
3x1 + 3x2 + 5x3



88
CHAPTER 3. THE VECTOR SPACE RN
(b) A =
· −1
2
−4
2
5
1
¸
, B =


1
−2
3
2
−1
1

,


−5
−8
−6
1
16
−10
3
3
5


41. Write B = [B1 , B2 , . . . , Bn ].
Then T(ei ) = Bei = Bi ,
so A =
[T(e1 ), T(e2 ), . . . , T(en )] = B.
42. [G ◦F](x ) = G(F(x )) = G(Ax ) = B(Ax ) = BAx .
By Exercise 41, BA
is the matrix
for G ◦F.
43. A = [e1 , e2 , . . . , en ] = I,
the (n x n)
identity matrix.
44. A = [T(e1 ), T(e2 ), . . . , T(en )] = [ae1 , ae2 , . . . , aen ].
Thus A =


a
0
· · ·
0
0
a
· · ·
0
...
...
...
0
0
· · ·
a

.
45.
(a) A =
· 0
−1
1
0
¸
.
(b) A =
·
1/2
−
√
3/2
√
3/2
1/2
¸
.
(c) A =
· −1/2
−
√
3/2
√
3/2
−1/2
¸
.
46.
(a) θ = 0
so A =
· 1
0
0
−1
¸
; T(e1 ) = e1 ; T(e2 ) = −e2 .
(b) θ = π
so A =
· −1
0
0
1
¸
; T(e1 ) = −e1 ; T(e2 ) = e2 .
(c) θ = π/2 so A =
· 0
1
1
0
¸
; T(e1 ) = e2 ; T(e2 ) = e1 .
(d) θ = 2π/3
so A =
· −1/2
√
3/2
√
3/2
1/2
¸
; T(e1 ) = [−1/2,
√
3/2]T;
T(e2 ) = [
√
3/2, 1/2]T.
47.
Set u1 = T(e1 )
and u2 = T(e2 ).
By assumption ∥u1 ∥=∥u2 ∥= 1
and u1Tu2 = 0. Moreover T(v ) = au1 +bu2
so ∥T(v ) ∥2=
(au1 +bu2 )T(au1 +bu2 ) = a2 + b2 =∥v∥2.
Thus ∥T(v ) ∥=
∥v ∥and T
is orthogonal.

3.8. LEAST-SQUARES SOLUTIONS TO INCONSISTENT SYSTEMS
89
48. Set u1 = T(e1 )
and u2 = T(e2 ).
Then A = [u1 , u2 ] and ATA =
· u1 Tu1
u1 Tu2
u2 Tu1
u2 Tu2
¸
.
It then follows from Theorem 16 that
ATA = I.
49.
(a) ATA =
· A1 TA1
A1 TA2
A2 TA1
A2 TA2
¸
so it follows that A1 TA1 = A2 TA2 = 1
whereas
A1 TA2 = A2 TA1 = 0.
Thus
{A1 , A2 } is an orthonormal set.
(b)
Now ∥T(e1 ) ∥= ∥A1 ∥= 1, ∥T(e2 ) ∥=∥A2 ∥= 1
and
T(e1 )
is perpendicular to T(e2 ). By Theorem 16, T
is orthogonal.
3.8
Least-Squares Solutions to Inconsistent Systems
1. ATA =
· 3
4
4
14
¸
and ATb =
· 1
6
¸
. The system of equations ATAx = ATb has unique
solution x∗=
· −5/13
7/13
¸
.
2. ATA =


6
11
23
11
22
44
23
44
90

and ATb =


1
5
7

. The system of equations ATAx = ATb has
solution x∗=


−3
19/11
0

+ x3


−2
−1
1

, where x3 is arbitrary.
3. ATAx =


11
16
17
16
30
18
17
18
33

and ATb =


10
17
13

. The system of equations ATAx = ATb has
solution x∗=


28/74
27/74
0

+ x3


−3
1
1

where x3 is arbitrary.
4. ATA =


15
24
3
24
39
3
3
3
6

and ATb =


0
1
−3

. The system of equations ATAx = ATb has
solution x∗=


−8/3
5/3
0

+ x3


−5
3
1

where x3 is arbitrary.

90
CHAPTER 3. THE VECTOR SPACE RN
5. ATA =
· 14
28
28
56
¸
and ATb=
·
52
104
¸
. The system of equations ATAx= ATb has solution
x∗=
· 26/7
0
¸
+ x2
· −2
1
¸
, where x2 is arbitrary.
6. ATA =


11
1
1
1
1
1
1
1
1

and ATb =


21
1
1

. The system of equations ATAx = ATb has
solution x∗=


2
−1
0

+ x3


0
−1
1

where x3 is arbitrary.
7. We must obtain the least-squares solution to Ax = b where A =


−1
1
0
1
1
1
2
1

, x =
· m
c
¸
,
and b =


0
1
2
4

. ATA =
· 6
2
2
4
¸
and
ATb=
· 10
7
¸
. The system of equations ATAx= ATb has solution x∗=
· 1.3
1.1
¸
. Therefore
y = (1.3)t + 1.1 is the least-squares linear ﬁt.
8. y = (−19/35)t + 31/35 is the least-squares linear ﬁt.
9. We must obtain the least-squares solution to Ax = b where A =


−1
1
0
1
1
1
2
1

, x =
· m
c
¸
,
and b=


−1
1
2
3

. In this case ATA =
· 6
2
2
4
¸
and ATb=
· 9
5
¸
. The system of equations
ATAx = ATb has solution x∗=
· 13/10
3/5
¸
so y = 2t + 1 is the least-squares linear ﬁt.
10. y = 4t −3/2 is the least-squares linear ﬁt.
11. We must obtain the least-squares solution to Ax= b where A =


1
−2
4
1
−1
1
1
1
1
1
2
4

, x=


a0
a1
a2

,

3.8. LEAST-SQUARES SOLUTIONS TO INCONSISTENT SYSTEMS
91
and b =


2
0
1
2

. In this case ATA =


4
0
10
0
10
0
10
0
34

and ATb =


5
1
17

. The system
of equations ATAx = ATb has solution x∗= [0, 1/10, 1/2]T so y = a0 + a1t + a2t2 =
(1/10)t + (1/2)t2 is the least-squares quadratic ﬁt.
12. y = −1/20 −(1/20)t + (1/4)t2 is the least-squares quadratic ﬁt.
13. We must obtain the least-squares solution to Ax= b where A =


1
−2
4
1
−1
1
1
0
0
1
1
1

, x=


a0
a1
a2

,
and b=


−3
−1
0
3

. In this case ATA =


4
−2
6
−2
6
−8
6
−8
18

and ATb=


−1
10
−10

. The system of
equations ATAx= ATb has solution x∗= [9/20, 43/20, 1/4]T so y = 9/20+(43/20)t+(1/4)t
is the least-squares quadratic ﬁt.
14. y = 31/55 −(4/55)t + (12/11)t2 is the least-squares quadratic ﬁt.
15. Note that Ax−b= [f(t1)−y1, . . . , f(tm)−ym]T so, by deﬁnition, ∥Ax−b∥2= Pm
i=1[f(ti)−
yi]2.
16.
(a) A =


1
−1
2
1
3
−1
4
1

, x =
· a1
a2
¸
, b =


0
2
4
5

.
(b) ATA =
· 30
2
2
4
¸
and ATb =
· 36
3
¸
. The system of equations ATAx = ATb has
solution x∗= [69/58, 9/58]T. Therefore Q(a1, a2) is minimized if f(t) = (69/58)
√
t +
(9/58)cosπt.
17. Suppose that Ax = θ, where x = [a0, a1, . . . , an]T. If p(t) = a0 + a1t + · · · + antn then
p(ti) = 0 for 0 ≤i ≤m; that is, p(t) has m + 1 roots and m + 1 > n. It follows that
a0 = a1 = · · · = an = 0. Thus nullity (A) = 0 and, consequently, rank (A) = n.
18. The matrix reduces to:


1
0
2
0
1
1
0
0
0
0
0
0

.
So, rank A = 2.

92
CHAPTER 3. THE VECTOR SPACE RN
3.9
Fitting Data and Least Squares Solutions
1. If u1 = [2, 1, 0]T and u2 = [−1, 0, 1]T then {u1 , u2 } is a basis for W. For A = [u1 , u2 ] the
system of equations ATAx = ATv is
given by 5x1 −2x2 = 4, −2x1 + 2x2 = 5. Solving we
obtain x1 = 3 and x2 = 11/2. Thus w∗= 3u1 +(11/2)u2 = [1/2, 3, 11/2]T.
2. The system of equations ATAx = ATv has solution x1 = x2 = 2. Thus w∗= 2u1 +2u2
= [2, 2, 2]T.
3. The basis {u1 , u2 } is given in Exercise 1. The system ATAx= ATv is given by 5x1−2x2 =
3, −2x1 + 2x2 = 0. The solution is x1 = x2 = 1, so w∗= u1 +u2 = [1, 1, 1]T = v .
4. R(B) has basis {u1, u2} where u1= [1, 1, 0]T and u2= [2, 1, 1]T. If A = [u1, u2] the system
ATAx = ATv is given by 2x1 + 3x2 = 2, 3x1 + 6x2 = 9. The solution is x1 = −5, x2 = 4.
Thus w∗= −5u1 +4u2 = [3, −1, 4]T.
5. R(B) has basis {u1, u2} where u1= [1, 1, 0]T and u2= [2, 1, 1]T. If A = [u1, u2], the system
ATAx = ATv is given by 2x1 + 3x2 = 6, 3x1 + 6x2 = 12. Solving yields x1 = 0, x2 = 2, so
w∗= 2u2 = [4, 2, 2]T.
6. The system of equations ATAx = ATv has solution x1 = −3, x2 = 3. Thus w∗= −3u1
+3u2 = [3, 0, 3]T = v .
7. R(B) has basis {u1 , u2 } where u1 = [1, −1, 1]T and u2 = [2, 0, 1]T. If A = [u1 , u2 ], the
system ATAx= ATv is given by 3x1 + 3x2 = 6, 3x1 + 5x2 = 8. Solving yields x1 = x2 = 1,
so w∗= u1 +u2 = [3, −1, 2]T.
8. The system of equations ATAx = ATv has solution x1 = −1, x2 = 2 so w∗= −u1 +2u2
= [3, 1, 1]T.
9. W has basis {u } where u = [0, −1, 1]T. If A = [u ] then the system ATAx = ATv is given
by 2x = −2. Thus x = −1 and w∗= −u = [0, 1, −1]T.
10. w∗= −2u = [0, 2, −2]T.
11. An orthogonal basis for W is the set {u1 , u2 } where u1 = [2, 1, 0]T
and u2 = [−1/5, 2/5, 1]T. The vector w∗is given by w∗= au1 +au2 where a1 = u1Tv
/u1Tu1 = 4/5 and a2 = u2Tv /u2Tu2 = 11/2. Thus w∗= [1/2, 3, 11/2]T.
12. w∗= au1 +au2 where u1 and u2 are given in Exercise 11, a= u1Tv /u1Tu1 = 6/5, and
a= u2Tv /u2Tu2 = 2. Thus w∗= [2, 2, 2]T.
13. If u1 = [1, 1, 0]T and u2 = [1/2, −1/2, 1]T then {u1 , u2 } is an orthogonal basis for W. The
vector w∗is given by w∗= au1 +au2
where a= u1Tv /u1Tu1 = 1 and a= u2Tv /u2Tu2 = 4. Thus w∗= [3, −1, 4]T.

3.10. SUPPLEMENTARY EXERCISES
93
14. w∗= au1 +au2 where u1 and u2 are given in Exercise 13, a= u1Tv /u1Tu1 = 3 and
a= u2Tv /u2Tu2 = 2. Thus w∗= [4, 2, 2]T.
15. If u1 = [1, −1, 1]T and u2 = [1, 1, 0]T then {u1 , u2 } is an orthogonal basis for W. The
vector w∗is given by w∗= au1 +au2 where a= u1Tv /u1Tu1 = 2 and a= u2Tv /u2Tu2
= 1. Therefore w∗= [3, −1, 2]T.
16. w∗= au1 +au2 where u1 and u2 are given in Exercise 15, a= u1Tv /u1Tu1 = 1 and
a= u2Tv /u2Tu2 = 2. Therefore w∗= [3, 1, 1]T.
3.10
Supplementary Exercises
1. Clearly θ =
· 0
0
¸
is in W. Suppose x =
· x1
x2
¸
is in W. Then x1x2 = 0. If a is any scalar
then ax =
· ax1
ax2
¸
and (ax1)(ax2) = a2x1x2 = 0, so ax is in W.
Now u =
· 1
0
¸
and v =
· 0
1
¸
are in W, but u + v =
· 1
1
¸
is not in W. Therefore, W
does not satisfy (S2).
2. Clearly θ = [0, 0]T is in W. Let u and v be in W, where u =
· u1
u2
¸
and v =
· v1
v2
¸
. Then
u1 ≥0, u2 ≥0, v1 ≥0, and v2 ≥0. If follows that u1 + v1 ≥0 and u2 + v2 ≥0. Thus
u + v =
· u1 + v1
u2 + v2
¸
is in W. If u =
· 1
1
¸
and a = −1 then u is in W but au is not in W.
Therefore W does not satisfy (S3).
3.
(a) Ax = 3x if and only if (A −3I)x = θ. Thus, W is the null space of the matrix A −3I.
(b) The system of equations (A−3I)x = θ has solution x1 = −x2+x3, x2 and x3 arbitrary.
Therefore





−1
1
0

,


1
0
1




is a basis for W.
4. Let S = {u1, u2}, T = {v1, v2, v3}, and b = [b1, b2, b3]T . Reducing the matrices [u1, u2, b]
and [v1, v2, v3, b] yields


1
0
2b2 −b1
0
1
b1 −b2
0
0
−5b1 + 7b2 + b3

and


1
0
3
b1
0
1
2
b2
0
0
0
−5b1 + 7b2 + b3

, re-
spectively. Thus Sp (S) = Sp (T) = {b : −5b1 + 7b2 + b3 = 0}. Alternatively, reducing
the matrices [u1, u2]T and [v1, v2, v3]T gives
· 1
0
5
0
1
−7
¸
and


1
0
5
0
1
−7
0
0
0

, respectively.
Therefore, {[1, 0, 5]T , [0, 1, −7]T } is a basis for both Sp (S) and Sp (T).

94
CHAPTER 3. THE VECTOR SPACE RN
5.
(a) A reduces to


1
−1
0
7
0
0
1
−2
0
0
0
0

so rank (A) = 2 and nullity (A) = 2.
(b) {[1, −1, 0, 7], [0, 0, 1, −2]}.
(c) {[1, 2, 1]T , [2, 5, 0]T }.
(d) {[1, 2, 1], [2, 5, 0]} is a basis for the row space of AT and {[1, −1, 0, 7]T , [0, 0, 1, −2]T } is
a basis for the column space of AT .
(e) The homogenous system of equations Ax = θ has solution x1 = x2 −7x4, x3 = 2x4,
x2 and x4 arbitrary.A basis for N(A) is {[1, 1, 0, 0]T , [−7, 0, 2, 1]T }.
6.
(a) The matrix A = [v1, v2, v3] reduces to


1
0
1
0
1
2
0
0
0

. If follows that {v1, v2} is a basis
for Sp (S).
(b) AT reduces to


3
0
1
0
3
−2
0
0
0

so {[3, 0, 1]T , [0, 3, −2]T } is a basis for Sp (S).
(c) Let b = [b1, b2, b3]T . The matrix [A, b] reduces to


1
0
1
(2b1 −b2)/3
0
1
2
(b1 + b2)/3
0
0
0
(−b1 + 2b2 −3b3)/3

.
Therefore, Sp (S) = {b : −b1 + 2b2 −3b3 = 0}. If follows that {[2, 1, 0]T , [3, 0, 1]T } is a
basis for Sp (S).
7. The matrix A is row equivalent to the (m × n) martrix


1
1
. . .
1
1
0
1
. . .
n −2
n −1
0
0
. . .
0
0
...
...
...
...
0
0
. . .
0
0


Thus, rank (A) = 2, nullity (A) = n−2, and {[1, 1, . . . , 1, 1], [0, 1, . . . , n−2, n−1]} is a basis
for the row space of A.
8. (a) 1 (b) 1 (c) 0
9. (a) 1 (b) 2 (c) 2
10. T(e1) = T(x1) −2T(x2) =
· −3
3
¸
and T(e2) = 2T(x1) + T(x2) =
· 4
1
¸
so
· −3
4
3
1
¸
is
the matrix of T.

3.10. SUPPLEMENTARY EXERCISES
95
11.
(a) x1 =


−5 −8u
2 + 3u
u

and x2 =


3 −8v
−1 + 3v
v

, where u and v are arbitrary.
(b) x3 =


−8w
3w
w

for any nonzero w.
(c) Taking u = v = 0 and w = 1 in (a) and (b) gives
B = {x1, x2, x3} = {[−5, 2, 0]T , [3, −1, 0]T , [−8, 3, 1]T }.
The set B is linearly independent, so is a basis for R3.
(d) e1 = x1 + 2x2 so T(e1) = T(x1) + 2T(x2) =
· 1
0
¸
+ 2
· 0
1
¸
=
· 1
2
¸
. e2 = 3x1 + 5x2
so T(e2) = 3T(x1) + 5T(x2) = 3
· 1
0
¸
+ 5
· 0
1
¸
=
· 3
5
¸
. e3 = −x1 + x2 + x3 so
T(e3) = −T(x1)+T(x2)+T(x3) = −
· 1
0
¸
+
· 0
1
¸
+
· 0
0
¸
=
· −1
1
¸
. If follows that
A =
· 1
3
−1
2
5
1
¸
.
12. {[1, 0, 2, 0, −3, 1], [0, 1, −1, 0, 2, 2], [0, 0, 0, 1, −1, −2]} is a basis for the row space of A. There-
fore rank (A) = 3 and nullity (A) = 3.
13. b = [a, b, c, d]T is in R(T) if and only if −16a −7b + 9c + d = 0. Therefore, w1, w3, and
w4 are in R(T).
14. For w1, x1 =


1 −2u + 3v −w
u −2v −2w
u
v + 2w
v
w


; for w3, x3 =


4 −2u + 3v −w
7 + u −2v −2w
u
−3 + v + 2w
v
w


; for w4, x4 =


1 −2u + 3v −w
1 + u −2v −2w
u
v + 2w
v
w


In each case, u, v, and w are arbitrary.
15. See the solution to Exercise 14. T(xi) = wi if and only if Axi = wi.
16.
(a) In the solutions to Exercise 14, take u = v = w = 0. This gives w1 = Ax1 = A1
w3 = Ax3 = 4A1 + 7A2 −3A4, and w4 = Ax4 = A1 + A2.
(b) {A1, A2, A4}

96
CHAPTER 3. THE VECTOR SPACE RN
(c) The homogenous system of equations, Ax = θ, has solution x1 = −2x3 + 3x5 −x6,
x2 = x3 −2x5 −2x6, x4 = x5 + 2x6, x3, x5, x6 arbitrary. Setting x3 = 1, x5 = x6 = 0
yields x1 = −2, x2 = 1, and x4 = 0.
If follows that −2A1 + A2 + A3 = θ, so
A3 = 2A1 −A2. Similarly, A5 = −3A1 + 2A2 −A4 and A6 = A1 + 2A2 −2A4.
(d) b is in the column space of A since, by the condition established in the solution to
Exercise 13, b is in R(T) . The system of equations Ax = b has solution x1 = 0,
x2 = −5, x3 = 0, x4 = 2, x5 = 0, x6 = 0, so b = −5A2 + 2A4.
(e) Ax = 2A1+3A2+A3−A4+A5+A6. Substituting for A3, A5, and A6 the expressions
obtained in (c) gives Ax = 2A1 + 6A2 −4A4.
17.
(a) If b = [a, b, c, d]T then R(T) = {b : −16a −7b + 9c + d = 0} (cf. the solution to
Exercise 13.). The set {u1, u2, u3} is the basis for R(T), where u1 = [1, 0, 0, 16]T ,
u2 = [0, 1, 0, 7]T , and u3 = [0, 0, 1, −9]T .
Moreover, if b is in R(T) then b =
[a, b, c, 16a + 7b −9c]T = au1 + bu2 + cu3.
(b) b = u1 + 2u2 + 3u3.
18.
(a) If v1 = [−2, 1, 1, 0, 0, 0]T , v2 = [3, −2, 0, 1, 1, 0]T , and v3 = [−1, −2, 0, 2, 0, 1]T , then
{v1, v2, v3} is a basis for N (T) (cf. the solution to Exercise 16(c)).
Moreover,if x = [x1, x2, x3, x4, x5, x6]T is in N (T) then
x = [−2x3 + 3x5 −x6, x3 −2x5 −2x6, x3, x5 + 2x6, x5, x6]T , where x3, x5, x6 are arb-
itrary. Thus, x = x3v1 + x5v2 + x6v3.
(b) x = v1 + 2v2 −2v3.
3.11
Conceptual Exercises
1. False. In R2 let W = {[a, a]T : a arbitrary }. Then e1 + e2 is in W but neither e1 nor e2 is
in W.
2. True. Since ax is in W, a−1(ax) = x is in W.
3. False. {θ} is a linearly dependent subset of Rn.
4. True. cf. Theorem 9(1).
5. True. cf. Theorem 9(2).
6. False. Consider S = {[0, 0]T , [1, 0]T , [3, 0]T } in R2.
7. False. Consider S1 = {[1, 1]T } and {[1, 0]T , [0, 1]T } in R2.
8. False. The sets {[1, 0]T , [0, 1]T } and {[2, 0]T , [1, 1]T } are both bases for R2.
9. False. A basis for W must contain exactly k vectors but if W ̸= {θ} then W contains
inﬁnitely many vectors.

3.11. CONCEPTUAL EXERCISES
97
10. False. Let B = {[1, 0]T , [0, 1]T }. Then B is a basis for R2 but no subset of B is a basis for
W = {[a, a]T : a arbitrary }.
11. True. A basis for W must also be a basis for Rn.
12. False. B1 = {[1, 1, 0]T , [1, 0, 1]T } is a basis for W1 = {[b + c, b, c]T : b, c arbitrary } and
B2 = {[2, 1, 0]T , [−1, 0, 1]T } is a basis for W2 = {[2b −c, b, c]T : b, c arbitrary }. B1 ∩B2 = ∅
but W1 ∩W2 = {[3c, 2c, c]T : c arbitrary }.
13. No. θ is not in V .
14. B need not be a subset of W.

Chapter 4
The Eigenvalue Problems
4.1
Introduction
1.
The matrix A −λI =
· 1 −λ
0
2
3 −λ
¸
is singular if and only if 0 = (1 −λ)(3 −λ).
Thus λ = 1 and λ = 3 are eigenvalues for A. The eigenvectors corresponding to λ = 1
(A −I)x = θ. Solving yields x1 = −x2, x2 arbitrary. Therefore any vector of the form x
= a
· −1
1
¸
, a ̸= 0,
is an eigenvector for λ = 1. Similarly the eigenvectors corresponding
to λ = 3 are the nontrivial solutions to (A −3I)x= θ. Solving yields x1 = 0, x2 arbitrary,
so any vector of the form x = a
· 0
1
¸
, a ̸= 0, is an eigenvector for λ = 3.
2.
The matrix A −λI =
· 2 −λ
1
0
−1 −λ
¸
is singular if and only if 0 = (2 −λ)(−1 −λ).
Therefore A has eigenvalues λ = 2 and λ = −1. For λ = 2 the corresponding eigenvectors
are x = a
· 1
0
¸
, a ̸= 0. For λ = −1 the corresponding eigenvectors are x = a
· −1
3
¸
,
a ̸= 0.
3.
The matrix A −λI =
· 2 −λ
−1
−1
2 −λ
¸
is singular if and only if 0 = (2 −λ)(2 −λ) −1 =
λ2 −4λ + 3 = (λ −1)(λ −3). Therefore A has eigenvalues λ = 1 and λ = 3˚.
Solving
(A−I)x= θ yields x1 = x2, x2 arbitrary, so any vector of the form x= a
· 1
1
¸
, a ̸= 0, is an
eigenvector corresponding to λ = 1˚. Solving (A −3I)x = θ yields x1 = −x2, x2 arbitrary,
so any vector of the form x = a
· −1
1
¸
, a ̸= 0, is an eigenvector for λ = 3.
4. λ = 2, x = a
· −2
1
¸
, a ̸= 0; λ = 3, x = a
· −1
1
¸
, a ̸= 0.

100
CHAPTER 4. THE EIGENVALUE PROBLEMS
5.
The matrix A −λI =
· 2 −λ
1
1
2 −λ
¸
is singular if and only if 0 = (2 −λ)(2 −λ) −1 =
λ2 −4λ + 3 = (λ −1)(λ −3). Therefore A has eigenvalues λ = 1 and λ = 3 . Solving
(A −I)x= θ
yields x1 = −x2, x2 arbitrary, so any vector of the form x= a
· −1
1
¸
, a ̸= 0,
is an eigenvector for λ = 1.
Solving (A −3I)x = θ yields x1 = x2, x2 arbitrary, so any
vector x = a
· 1
1
¸
, a ̸= 0, is an eigenvector for λ = 3.
6. λ = 2, x = a
· 1
1
¸
, a ̸= 0.
7. The matrix A−λI =
· 1 −λ
0
2
1 −λ
¸
is singular if and only if 0 = (1−λ)2,
so λ = 1
is
the only eigenvalue for A. Solving (A −I)x = θ yields x1 = 0, x2 arbitrary, so any vector
x = a
· 0
1
¸
, a ̸= 0, is an eigenvector for λ = 1.
8. λ = 2, x = a
· 1
0
¸
, a ̸= 0.
9.
The matrix A −λI =
· 2 −λ
2
3
3 −λ
¸
is singular if and only if 0 = (2 −λ)(3 −λ) −6 =
λ2 −5λ = λ(λ −5). Therefore A has eigenvalues λ = 0 and λ = 5. Solving Ax= θ yields
x1 = −x2, x2 arbitrary, so x = a
· −1
1
¸
, a ̸= 0, is an eigenvector for λ = 0. Solving
(A −5I)x = θ yields x1 = (2/3)x2, x2 arbitrary, so x = a
· 2
3
¸
,
a ̸= 0, is an eigenvector for λ = 5.
10. λ = 0, x = a
· −2
1
¸
, a ̸= 0; λ = 9, x = a
· 1
4
¸
, a ̸= 0.
11.
The matrix A −λI =
· 1 −λ
−1
1
3 −λ
¸
is singular if and only if 0 = (1 −λ)(3 −λ) + 1 =
λ2 −4λ + 4 = (λ −2)2. Therefore λ = 2 is the only eigenvalue for A. Solving (A −2I)x
= θ yields x1 = −x2, so x = a
· −1
1
¸
, a ̸= 0
is an eigenvector for λ = 2.
12. λ = 3, x = a
· −1
1
¸
, a ̸= 0.

4.2. DETERMINANTS AND THE EIGENVALUE PROBLEM
101
13.
The matrix A−λI =
· −2 −λ
−1
5
2 −λ
¸
is singular if and only if 0 = (−2−λ)(2−λ)+5 =
λ2 + 1. Solving yields λ = ±i.
14.
The matrix A −λI is singular if and only if λ2 + 1 = 0˚. Solving yields λ = ±i.
15.
The matrix A −λI =
· 2 −λ
−1
1
2 −λ
¸
is singular if and only if 0 = (2 −λ)(2 −λ) + 1 =
λ2 −4λ + 5. Solving we obtain λ = 2 ± i.
16. A −λI is singular if and only if λ2 −2λ + 2 = 0. Solving yields λ = 1 ± i.
17.
The matrix A −λI =
· a −λ
b
b
d −λ
¸
is singular if and only if 0 = (a −λ)(d −λ) −b2 =
λ2 −(a + d)λ + (ad −b2).
Note that (a + d)2 −4(ad −b2) = (a −d)2 + 4b2 ≥0,
so the
equation has real roots.
18.
The matrix A−λI is singular if and only if λ2−2aλ+(a2+b2) = 0. Since (2a)2−4(a2+b2) =
−4b2 < 0 the equation has no real roots.
19.
Let A =
· a
b
c
d
¸
. The matrix AT −λI =
· a −λ
c
b
d −λ
¸
is singular if and only if
0 = (a −λ)(d −λ) −bc = λ2 −(a + d)λ+
(ad −bc). Therefore the eigenvalues of AT are roots of (5)˚, so
coincide with the eigenvalues of A.
4.2
Determinants and the Eigenvalue Problem
1. M11 =


1
3
−1
2
4
1
2
0
−2

.
A11 = det(M11) =
¯¯¯¯
4
1
0
−2
¯¯¯¯ −3
¯¯¯¯
2
1
2
−2
¯¯¯¯ −
¯¯¯¯
2
4
2
0
¯¯¯¯ = 18.
2. M21 =


−1
3
1
2
4
1
2
0
−2

.
A21 = −det(M21) = −18.
3. M31 =


−1
3
1
1
3
−1
2
0
−2

.
A31 = det(M31) =
−
¯¯¯¯
3
−1
0
−2
¯¯¯¯ −3
¯¯¯¯
1
−1
2
−2
¯¯¯¯ +
¯¯¯¯
1
3
2
0
¯¯¯¯ = 0.

102
CHAPTER 4. THE EIGENVALUE PROBLEMS
4. M41 =


−1
3
1
1
3
−1
2
4
1

.
A41 = −det(M41) = 18.
5. M34 =


2
−1
3
4
1
3
2
2
0

.
A34 = −det(M34) =
−2
¯¯¯¯
1
3
2
0
¯¯¯¯ −
¯¯¯¯
4
3
2
0
¯¯¯¯ −3
¯¯¯¯
4
1
2
2
¯¯¯¯ = 0.
6. M43 =


2
−1
1
4
1
−1
6
2
1

.
A43 = −det(M43) = −18.
7. det(A) = 2A11 + 4A21 + 6A31 + 2A41 =
2(18) + 4(−18) + 6(0) + 2(18) = 0.
8. det(A) = 5; A is nonsingular.
9. det(A) = 0; A is singular
10. det(A) = 0; A is singular.
11. det(A) = −1; A is nonsingular.
12. det(A) = 0; A is singular.
13. det(A) = 2
¯¯¯¯
−2
1
1
−1
¯¯¯¯ + 3
¯¯¯¯
−1
1
3
−1
¯¯¯¯ + 2
¯¯¯¯
−1
−2
3
1
¯¯¯¯ = 6; A is nonsingular.
14. det(A) = 0;
A
is singular.
15. det(A) = 2
¯¯¯¯
3
2
1
4
¯¯¯¯= 20;
A is nonsingular.
16. By Theorem 4, det(A) = 2(1)(2) = 4;
A is nonsingular.
17. Expansion along the ﬁrst column of A yields det(A) =
¯¯¯¯¯¯
3
0
0
4
1
2
3
1
4
¯¯¯¯¯¯
.
Now expansion along the ﬁrst row gives det(A) = 3
¯¯¯¯
1
2
1
4
¯¯¯¯ = 6.
A is nonsingular.
18. det(A) = 1; A is nonsingular.

4.2. DETERMINANTS AND THE EIGENVALUE PROBLEM
103
19. Expansion along the ﬁrst column in successive steps yields
det(A) = −3
¯¯¯¯¯¯
0
0
2
0
3
1
2
1
2
¯¯¯¯¯¯
= (−3)(2)
¯¯¯¯
0
2
3
1
¯¯¯¯ = (−3)(2)(−6) = 36. A is nonsingular.
20.
(a) The described algorithm yields
a11a22a33 + a12a23a31 + a13a21a32 −a31a22a13 −a32a23a11 −a33a21a12 which equals
det(A).
(b) Note that for a (4 × 4) matrix, A = (aij), the deﬁnition of det(A) yields a sum of
products with 24 terms, whereas the “basketweave algorithm” yields an expression
with only eight summands. For A =


1
1
1
1
1
2
2
2
1
2
3
3
1
2
3
4

, the basket weave algorithm
gives 7, but det(A) = 1.
21. Det (A) = 4x −2y −2, so A is singular when 4x −2y −2 = 0, that is when y = 2x −1.
22. Det (A) = (x −2)(y + 1), so A is singular if either x = 2 or y = −1.
23. For n = 2,
¯¯¯¯
d
1
1
d
¯¯¯¯ = d2 −1 = (d −1)(d + 1).
For n = 3,
¯¯¯¯¯¯
d
1
1
1
d
1
1
1
d
¯¯¯¯¯¯
= d
¯¯¯¯
d
1
1
d
¯¯¯¯ −
¯¯¯¯
1
1
1
d
¯¯¯¯ +
¯¯¯¯
1
d
1
1
¯¯¯¯ = d(d −1)(d + 1) −(d −1) + (1 −d)
= (d −1)2(d + 2). For n = 4,
¯¯¯¯¯¯¯¯
d
1
1
1
1
d
1
1
1
1
d
1
1
1
1
d
¯¯¯¯¯¯¯¯
= d
¯¯¯¯¯¯
d
1
1
1
d
1
1
1
d
¯¯¯¯¯¯
−
¯¯¯¯¯¯
1
1
1
1
d
1
1
1
d
¯¯¯¯¯¯
+
¯¯¯¯¯¯
1
d
1
1
1
1
1
1
d
¯¯¯¯¯¯
−
¯¯¯¯¯¯
1
d
1
1
1
d
1
1
1
¯¯¯¯¯¯
= d(d −1)2(d + 2) −3(d −1)2 = (d −1)3(d + 3).
24.
(a)
If A is singular then det(A) = 0 so det(AB) = det(A) det(B) = 0. Therefore AB
is singular. Similarly if B is singular then so is AB.
(b)
If AB is singular then 0 = det(AB) = det(A) det(B). Therefore either det(A) = 0
or det(B) = 0; that is either A or B
is singular.
25. 1 = det(I) = det(AA−1) = det(A) det(A−1). Therefore det(A−1) = 1/ det(A).
26. det(AB) = det(A) det(B) = det(B) det(A) = det(BA).
27. Det (ABA−1) = det(A) det(B)/ det(A) = det(B) = 5.

104
CHAPTER 4. THE EIGENVALUE PROBLEMS
28. Det (A2B) = [det(A)]2 det(B) = 325 = 45.
29. Det (A−1B−1A2) = [det(A)]2/[det(A) det(B)] = det(A)/ det(B) = 3/5.
30. Det (AB−1A−1B) = [det(A)/ det(B)][det(B)/ det(A)] = 1.
31.
(a) H(n) = n!/2.
(b) n = 2, 3 secs; n = 5, 3 min; n = 10, 63 days.
32.
If U = [uij] and V = [vij] then, by Theorem 4, det(U) =
u11u22 · · · unn and det(V ) = v11v22 · · · vnn. By Exercise 59, Section 1.6, UV
is an upper
triangular matrix. Moreover UV
is the (n x n) matrix [aij] where a11 = u11v11, . . . , ann =
unnvnn. It follow that det(UV ) = (u11v11)(u22v22) · · · (unnvnn) = det(U) det(V ).
33.
Suppose that V
is a lower triangular matrix with diagonal entries t1, t2, . . . , tn.
Then
V T is an upper triangular matrix with the same diagonal entries so det(V ) = det(V T) =
t1t2 · · · tn.
34.
For n = 2, det(T) =
¯¯¯¯
t11
t12
0
t22
¯¯¯¯ = t11t22. Assume that if T
is a
(k x k) matrix then det(T) = t11t22 · · · tkk.˚. Now assume that T
is a [(k+1) x (k+1)]
matrix. Thus T =


t11
t12
· · ·
t1,k+1
0
t22
· · ·
t2,k+1
...
...
0
0
. . .
tk+1,k+1

.
Expansion along the ﬁrst column yields det(T) =
t11
¯¯¯¯¯¯¯
t22
· · ·
t2,k+1
...
...
0
· · ·
tk+1,k+1
¯¯¯¯¯¯¯
.
Since the result holds for (k x k) matrices, we have det(T) =
t11t22 · · · tk+1,k+1. It follows by induction that for any integer n ≥2, det(T) = t11t22 · · · tnn.
4.3
Elementary Operations and Determinants
1. det(A) =
¯¯¯¯¯¯
1
2
1
3
0
2
−1
1
3
¯¯¯¯¯¯
½ R2 −3R1
R3 + R1
¾
=
¯¯¯¯¯¯
1
2
1
0
−6
−1
0
3
4
¯¯¯¯¯¯
=
¯¯¯¯
−6
−1
3
4
¯¯¯¯ = −21.
2. det(A) = 20.

4.3. ELEMENTARY OPERATIONS AND DETERMINANTS
105
3. det(A) =
¯¯¯¯¯¯
3
6
9
2
0
2
1
2
0
¯¯¯¯¯¯
= (3)(2)
¯¯¯¯¯¯
1
2
3
1
0
1
1
2
0
¯¯¯¯¯¯
½ R2 −R1
R3 −R1
¾
=
(6)
¯¯¯¯¯¯
1
2
3
0
−2
−2
0
0
−3
¯¯¯¯¯¯
= 36.
4. det(A) = −24.
5. det(A) =
¯¯¯¯¯¯
2
4
−3
3
2
5
2
3
4
¯¯¯¯¯¯
= (1/2)
¯¯¯¯¯¯
2
4
−3
6
4
10
2
3
4
¯¯¯¯¯¯
½ R2 −3R1
R3 −R1
¾
=
(1/2)
¯¯¯¯¯¯
2
4
−3
0
−8
19
0
−1
7
¯¯¯¯¯¯
= (2)(1/2)
¯¯¯¯
−8
19
−1
7
¯¯¯¯ = −37.
6. det(A) = −21.
7.
¯¯¯¯¯¯¯¯
1
0
0
0
2
0
0
3
1
1
0
1
1
4
2
2
¯¯¯¯¯¯¯¯
{C3 ↔C4}
=
(−1)
¯¯¯¯¯¯¯¯
1
0
0
0
2
0
3
0
1
1
1
0
1
4
2
2
¯¯¯¯¯¯¯¯
{C2 ↔C3}
=
¯¯¯¯¯¯¯¯
1
0
0
0
2
3
0
0
1
1
1
0
1
2
4
2
¯¯¯¯¯¯¯¯
= (1)(3)(1)(2) = 6.
8.
¯¯¯¯¯¯¯¯
0
0
3
1
2
1
0
1
0
0
0
2
0
2
2
1
¯¯¯¯¯¯¯¯
= (−1)
¯¯¯¯¯¯¯¯
2
1
0
1
0
2
2
1
0
0
3
1
0
0
0
2
¯¯¯¯¯¯¯¯
= −24.
9.
¯¯¯¯¯¯¯¯
0
0
2
0
0
0
1
3
0
4
1
3
2
1
5
6
¯¯¯¯¯¯¯¯
½ C1 ↔C4
C2 ↔C3
¾
=
¯¯¯¯¯¯¯¯
0
2
0
0
3
1
0
0
3
1
4
0
6
5
1
2
¯¯¯¯¯¯¯¯
{C1 ↔C2}
=
(−1)
¯¯¯¯¯¯¯¯
2
0
0
0
1
3
0
0
1
3
4
0
5
6
1
2
¯¯¯¯¯¯¯¯
= (−1)(2)(3)(4)(2) = −48.

106
CHAPTER 4. THE EIGENVALUE PROBLEMS
10.
¯¯¯¯¯¯¯¯
0
0
1
0
1
2
1
3
0
0
0
5
0
3
1
2
¯¯¯¯¯¯¯¯
= (−1)
¯¯¯¯¯¯¯¯
1
2
1
3
0
3
1
2
0
0
1
0
0
0
0
5
¯¯¯¯¯¯¯¯
= −15.
11.
¯¯¯¯¯¯¯¯
0
0
1
0
0
2
6
3
2
4
1
5
0
0
0
4
¯¯¯¯¯¯¯¯
{R1 ↔R3}
=
(−1)
¯¯¯¯¯¯¯¯
2
4
1
5
0
2
6
3
0
0
1
0
0
0
0
4
¯¯¯¯¯¯¯¯
=
(−1)(2)(2)(1)(4) = −16.
12.
¯¯¯¯¯¯¯¯
0
1
0
0
0
2
0
3
2
1
0
6
3
2
2
4
¯¯¯¯¯¯¯¯
= (−1)
¯¯¯¯¯¯¯¯
1
0
0
0
2
3
0
0
1
6
2
0
2
4
3
2
¯¯¯¯¯¯¯¯
= −12.
13. det(B) = 3 det(A) = 6.
14. det(B) = det(A) = 2.
15. det(B) = −det(A) = −2.
16. det(B) = det(A) = 2.
17. det(B) = −2 det(A) = −4.
18. det(B) = det(A) = 2.
19.
¯¯¯¯¯¯¯¯
2
4
2
6
1
3
2
1
2
1
2
3
1
2
1
1
¯¯¯¯¯¯¯¯
{R1 −2R4}
=
¯¯¯¯¯¯¯¯
0
0
0
4
1
3
2
1
2
1
2
3
1
2
1
1
¯¯¯¯¯¯¯¯
= (−4)
¯¯¯¯¯¯
1
3
2
2
1
2
1
2
1
¯¯¯¯¯¯
½ R2 −2R1
R3 −R1
¾
=
(−4)
¯¯¯¯¯¯
1
3
2
0
−5
−2
0
−1
−1
¯¯¯¯¯¯
=
(−4)
¯¯¯¯
−5
−2
−1
−1
¯¯¯¯ = −12.
20. −5.
21.
¯¯¯¯¯¯¯¯
0
4
1
3
0
2
2
1
1
3
1
2
2
2
1
4
¯¯¯¯¯¯¯¯
{R4 −2R3}
=
¯¯¯¯¯¯¯¯
0
4
1
3
0
2
2
1
1
3
1
2
0
−4
−1
0
¯¯¯¯¯¯¯¯
=

4.3. ELEMENTARY OPERATIONS AND DETERMINANTS
107
¯¯¯¯¯¯
4
1
3
2
2
1
−4
−1
0
¯¯¯¯¯¯
{R1 + R3}
=
¯¯¯¯¯¯
0
0
3
2
2
1
−4
−1
0
¯¯¯¯¯¯
= (3)
¯¯¯¯
2
2
−4
−1
¯¯¯¯ = 18.
22. 4.
23.
¯¯¯¯¯¯
1
a
a2
1
b
b2
1
c
c2
¯¯¯¯¯¯
½ R2 −R1
R3 −R1
¾
=
¯¯¯¯¯¯
1
a
a2
0
b −a
b2 −a2
0
c −a
c2 −a2
¯¯¯¯¯¯
= (b −a)(c −a)
¯¯¯¯¯¯
1
a
a2
0
1
b + a
0
1
c + a
¯¯¯¯¯¯
{R3 −R2}
=
(b −a)(c −a)
¯¯¯¯¯¯
1
a
a2
0
1
b + a
0
0
c −b
¯¯¯¯¯¯
= (b −a)(c −a)(c −b)
¯¯¯¯¯¯
1
a
a2
0
1
b + a
0
0
1
¯¯¯¯¯¯
= (b −a)(c −a)(c −b).
24. (b −a)(c −a)(c −b)(d −a)(d −b)(d −c).
25.
Write A =


A1
A2
...
An


where Ai = [ai1, ai2, . . . , ain] is the ith row
of A. Then cA =


cA1
cA2
...
cAn


so, by Theorem 7, det(cA) = cn det(A).
26.
Suppose the ith and jth rows of A
are identical and let B denote the matrix obtained
by interchanging these two rows.
By Theorem 6 det(B) = −det(A). But B = A
so
det(A) = det(B). It follows that
det(A) = −det(A), so det(A) = 0.
27. A =
· 1
0
0
0
¸
and B =
· 0
0
0
1
¸
is one possibility.
28. By Theorem 5, det(A) = det(AT).
But AT = −A
so by Exercise 25,
det(AT) =
(−1)n det(A). Therefore det(A) = (−1)n det(A). In particular if n is odd then det(A) =
−det(A).
It follows that det(A) = 0 and hence A is singular.

108
CHAPTER 4. THE EIGENVALUE PROBLEMS
4.4
Eigenvalues and the Characteristic
Polynomial
1. p(t) = (1−t)(3−t). The eigenvalues are λ = 1 and λ = 3, each with algebraic multiplicity
1.
2. p(t) = (2 −t)(−1 −t).
The eigenvalues are λ = 2 and λ = −1
each with algebraic
multiplicity 1.
3. p(t) =
¯¯¯¯
2 −t
−1
−1
2 −t
¯¯¯¯ = (2 −t)(2 −t) −1 = t2 −4t + 3 = (t −1)(t −3). The eigenvalues
are λ = 1 and λ = 3, each with algebraic multiplicity 1.
4. p(t) = (t −1)2.
The only eigenvalue is λ = 1 and it has algebraic multiplicity 2.
5. p(t) =
¯¯¯¯
1 −t
−1
1
3 −t
¯¯¯¯ = (1 −t)(3 −t) + 1 = t2 −4t + 4 = (t −2)2. The only eigenvalue is
λ = 2 and it has algebraic multiplicity 2.
6. p(t) = t(t −5). The eigenvalues are λ = 0 and λ = 5, each with algebraic multiplicity 1.
7. p(t) =
¯¯¯¯¯¯
−6 −t
−1
2
3
2 −t
0
−14
−2
5 −t
¯¯¯¯¯¯
= −t3 + t2 + t −1 = −(t −1)2(t + 1).
The eigenvalues are
λ = 1 with algebraic multiplicity 2 and λ = −1 with algebraic multiplicity 1.
8. p(t) = −t(t + 1)2. The eigenvalues are λ = 0 with algebraic multiplicity 1, and λ = −1
with algebraic multiplicity 2.
9. p(t) =
¯¯¯¯¯¯
3 −t
−1
−1
−12
−t
5
4
−2
−1 −t
¯¯¯¯¯¯
= −t3 + 2t2 + t −2 =
−(t −2)(t −1)(t + 1). The eigenvalues are λ = 2, λ = 1, and λ = −1 each with algebraic
multiplicity 1.
10. p(t) = −(t −1)3.ÄThe only eigenvalue is λ = 1 and it has algebraic multiplicity 3.
11. p(t) =
¯¯¯¯¯¯
2 −t
4
4
0
1 −t
−1
0
1
3 −t
¯¯¯¯¯¯
= (2 −t)
¯¯¯¯
1 −t
−1
1
3 −t
¯¯¯¯ =
(2−t)(t2−4t+4) = −(t−2)3. The only eigenvalue is λ = 2 and it has algebraic multiplicity
3.
12. p(t) = (t −5)2(t + 1)(t −15).
The eigenvalues are λ = 5
with algebraic multiplicity 2,
λ = −1 with algebraic multiplicity 1, and λ = 15 with algebraic multiplicity 1.

4.4. EIGENVALUES AND THE CHARACTERISTIC POLYNOMIAL
109
13. p(t) =
¯¯¯¯¯¯¯¯
5 −t
4
1
1
4
5 −t
1
1
1
1
4 −t
2
1
1
2
4 −t
¯¯¯¯¯¯¯¯
= t4 −18t3 +97t2 −180t+100 = (t−1)(t−2)(t−5)(t−
10).
The eigenvalues are λ = 1, λ = 2, λ = 5, λ = 10, each with algebraic multiplicity 1.
14. p(t) = (t−2)3(t+2).
The eigenvalues are λ = 2 with algebraic multiplicity 3, and λ = −2
with algebraic multiplicity 1.
15. Let x
be an eigenvector corresponding to λ. Thus x ̸= θ
and Ax = λx . Multiplication
by A−1 yields x = A−1(λx ) = λA−1x . Since A is nonsingular λ ̸= 0 (cf. Theorem 13).
Thus multiplication by λ−1 gives A−1x = λ−1x .
16.
If Ax = λx
then (A + αI)x = Ax +αIx = λx +αx = (λ + α)x .
17.
Let x
be an eigenvector corresponding to λ and suppose Akx = λkx
for some integer
k ≥2. Then Ak+1x = A(Akx ) =
A(λkx ) = λk(Ax ) = λk(λx ) = λk+1x . It follows by the principle of induction that Anx
= λnx for each positive integer n, n ≥2.
18.
(a) q(H)x = (H3 −2H2 −H + 2I)x = H3x −2H2x −Hx +2Ix =
λ3x −2λ2x −λx +2x = (λ3 −2λ2 −λ + 2)x = q(λ)x .
(b)
The eigenvalues for q(A) are q(1) = 0 and q(−1) = 0. The eigenvalues for q(B) are
q(0) = 2 and q(−1) = 0.
19. q(C) = C3 −2C2 −C + 2I =


35
−3
−15
−44
2
19
68
−6
−29

−
2


17
−1
−7
−16
2
7
32
−2
−13

−


3
−1
−1
−12
0
5
4
−2
−1

+


2
0
0
0
2
0
0
0
2

= O.
20. p(t) = t2 −4t + 3. p(A) = A2 −4A + 3I = O.
21. p(t) = t2 −2t + 1. p(A) = A2 −2A + I = O.
22. p(t) = −t3 + 2t2 + t −2. p(A) = −A3 + 2A2 + A −2I = O.
23. p(t) = t4 −18t3 + 97t2 −180t + 100. p(A) = A4 −18A3 + 97A2 −180A + 100I = O.
24.
(a)
Suppose B = [B1 , B2 , B3 ]. Then θ = Be1 = B1 ,θ =
Be2 = B2 , and θ = Be3 = B3 . Thus B = O.
(b)
Since Aui = λiui for i = 1, 2, 3, it follows, as in Exercise 18a, that p(A)ui = p(λi)ui
= (0)ui = θ. By property 3 of Theorem 9 in Section 2.5, {u1 , u2 , u3 } is a basis for

110
CHAPTER 4. THE EIGENVALUE PROBLEMS
R3. Therefore every vector x in R3
can be expressed in the form x = a1u1 +a2u2
+a3u3 . It follows that p(A)x =
a1p(A)u1 +a2p(A)u2 +a3p(A)u3 = θ. By part (a), p(A) = O.
25. p(A) = A2 −(a + d)A + (ad −bc)I =
· a2 + bc
ab + bd
ca + dc
cb + d2
¸
−(a + d)
· a
b
c
d
¸
+ (ad −bc)
· 1
0
0
1
¸
=
· 0
0
0
0
¸
.
26.
Expansion yields p(t) = −[t3 −(a+b+c)t2 +(ab+ac+bc)t−abc]. Similarly the properties
of matrix multiplication imply that −(A −aI)(A −bI)(A −cI) = −[A3 −(a + b + c)A2 +
(ab + ac + bc)A −abcI] = p(A). Therefore
p(A) = −(A −aI)(A −bI)(A −cI) =


0
d
f
0
b −a
e
0
0
c −a




a −b
d
f
0
0
e
0
0
c −b




a −c
d
f
0
b −c
e
0
0
0


= O.
27.
(a)
For n = 2, det(A −tI) =
¯¯¯¯
−a1 −t
−a0
1
−t
¸
= t2 + a1t + a0 =
q(t). For n = 3, det(A −tI) =
¯¯¯¯¯¯
−a2 −t
−a1
−a0
1
−t
0
0
1
−t
¯¯¯¯¯¯
. Expanding along the third
column and applying the case n = 2
yields det(A −tI) = −a0
¯¯¯¯
1
−t
0
1
¯¯¯¯ −t
¯¯¯¯
−a2 −t
−a1
1
−t
¸
=
−t(t2 + a2t + a1) −a0 = −q(t).
(b) A =


−3
1
−2
2
1
0
0
0
0
1
0
0
0
0
1
0

.
The characteristic polynomial for A is det(A −tI) =
¯¯¯¯¯¯¯¯
−3 −t
1
−2
2
1
−t
0
0
0
1
−t
0
0
0
1
−t
¯¯¯¯¯¯¯¯
= q(t).
(c)
For some integer k ≥2, assume that if
A =


−ak−1
−ak−2
· · ·
−a1
−a0
1
0
· · ·
0
0
0
1
· · ·
0
0
...
0
0
· · ·
1
0



4.4. EIGENVALUES AND THE CHARACTERISTIC POLYNOMIAL
111
then det(A −tI) = (−1)k(tk + ak−1tk−1 + · · · + a1t + a0).
If A is the [(k + 1) x
(k + 1)] companion matrix then det(A −tI) =
¯¯¯¯¯¯¯¯¯¯¯
−ak −t
−ak−1
· · ·
−a1
−a0
1
−t
· · ·
0
0
0
1
· · ·
0
0
...
...
0
0
· · ·
1
−t
¯¯¯¯¯¯¯¯¯¯¯
.
Expanding along the
(k + 1)st column and using the case n = k yields det(A −tI) =
(−1)k+1a0
¯¯¯¯¯¯¯¯¯
1
−t
· · ·
0
0
1
· · ·
0
...
...
0
0
· · ·
1
¯¯¯¯¯¯¯¯¯
−t
¯¯¯¯¯¯¯¯¯
−ak −t
−ak−1
· · ·
−a1
1
−t
· · ·
0
...
0
0
· · ·
−t
¯¯¯¯¯¯¯¯¯
=
(−1)k+1a0 −t(−1)k(tk + aktk−1 + · · · + a2t + a1) = (−1)k+1q(t).
By mathematical
induction det(A −tI) = (−1)nq(t) for all n, n ≥2.
28. x0 = [1, 1, 1]T, x1 = [1, −7, 1]T, x2 = [9, −7, 17]T, x3 = [17, −23, 33]T,
x4 = [41, −39, 81]T, x5 = [81, −87, 161]T.β0 = −5/3 ≈
−1.667, β1 = 75/51 ≈1.471, β2 = 875/419 ≈2.088, β3 = 4267/1907
≈2.238, β4 = 19755/9763 ≈2.023.
29.
Note that xj= Ajx0 so by property (a) of Theorem 11,xj= c1λj
1u1+c2λj
2u2+ · · ·+cnλj
nun
. Set aj = λj/λ1, 1 ≤j ≤n. Then a1 = 1 whereas |aj |< 1 for 2 ≤j ≤n. In particular
limk→∞ak
j = 0 if j ̸= 1. It follows that xkTxk+1 = P cicjλk
i λk+1
j
uiTuj
1 ≤i, j ≤n
=
λ2k+1
1
(c2
1u1Tu1 +rk), where rk = P cicjak
i ak+1
j
uiTuj
1 ≤i, j ≤n
(i, j) ̸= (1, 1).
In particular
limk→∞rk = 0. Similarly xkTxk = λ2k
1 (c1u1Tu1 +tk) where
limk→∞tk = 0. Therefore limk→∞βk = λ1.
30.
Let A and AT have characteristic polynomials p(t) and q(t),
respectively. Note that (A −tI)T = AT −tI. It follows that p(t) =
det(A −tI) = det(A −tI)T = det(AT −tI) = q(t).
31.
Suppose p(t) = t2 +a1t+a0. Then p(0) = a0 = det(A−0I) = 4 and p(1) = 1+a1 +a0 =
det(A −I) = 1. Thus we have
a0
=
4
a0 + a1
=
0 .
Solving yields a0 = 4, a1 = −4. Therefore p(t) = t2 −4t + 4.

112
CHAPTER 4. THE EIGENVALUE PROBLEMS
32. p(0) = a0 = det(A −0I) = 0
and p(1) = 1 + a1 + a0 = det(A −I) = −4.
Therefore
a0 = 0, a1 = −4, and p(t) = t2 −5t.
33.
Suppose p(t) = −t3 + a2t2 + a1t + a0. Then p(−1) = 1 + a2 −a1 + a0 = det(A + I) =
0, p(0) = a0 = det(A −0I) = −1, and p(1) = −1 + a2 + a1 + a0 = det(A −I) = 0. It follows
that a0 = −1, a1 = 1, and a2 = 1. Therefore p(t) = −t3 + t2 + t −1.
34. p(−1) = 1+a2−a1+a0 = det(A+I) = 0, p(0) = a0 = det(A−0I) = 0, and p(1) = −1+a2+
a1 + a0 = det(A −I) = −4. Therefore a0 = 0, a1 = −1, a2 = −2, and p(t) = −t3 −2t2 −t.
4.5
Eigenvalues and Eigenvectors
1. (A −3I)x = θ is the system
−x1 −x2 = 0
−x1 −x2 = 0 .
The solution is x1 = −x2, x2
arbitrary, so Eλ
consists of the vectors of the form
x2
· −1
1
¸
.
Thus {[−1, 1]T}
is a basis for Eλ.
The eigenvalue λ = 3 has algebraic and
geometric multiplicity 1.
2.
The system (A −I)x = θ has solution x1 = x2, x2 arbitrary. {[1, 1]T}
is a basis for Eλ. The eigenvalue λ = 1 has algebraic and geometric multiplicity 1.
3. (B −2I)x = θ is the system
−x1 −x2 = 0
x1 + x2 = 0 .
The solution is x1 = −x2, x2 arbitrary, so Eλ consists of the vectors of the form x2
· −1
1
¸
.
Thus {[−1, 1]T} is a basis for Eλ. The eigenvalue λ = 2 has algebraic multiplicity 2 and
geometric multiplicity 1.
4. The system (C −I)x = θ has solution x1 = (1/2)x3, x2 = (−3/2)x3,
x3 arbitrary. The set {[1, −3, 2]T} is a basis for Eλ. The eigenvalue λ = 1 has algebraic
multiplicity 2 and geometric multiplicity 1.
5. (C + I)x = θ is the system
−5x1
−
x2
+
2x3
=
0
3x1
+
3x2
=
0
−14x1
−
2x2
+
6x3
=
0
.

4.5. EIGENVALUES AND EIGENVECTORS
113
The solution is x1 = (1/2)x3, x2 = (−1/2)x3, x3 arbitrary, so Eλ
consists of vectors of the form a


1
−1
2

where a is an arbitrary scalar. Thus {[1, −1, 2]T}
is a basis for Eλ. The eigenvalue λ = −1 has algebraic and geometric multiplicity 1.
6.
The system (D −I)x = θ has solution x1 = (1/2)x2 −(3/8)x3, x2 and x3 arbitrary. The
set {[1, 2, 0]T, [−3, 0, 8]T} is a basis for Eλ. The eigenvalue λ = 1 has algebraic multiplicity
3 and geometric multiplicity 2.
7. (E + I)x = θ is the system
7x1
+
4x2
+
4x3
+
x4
=
0
4x1
+
7x2
+
x3
+
4x4
=
0
4x1
+
x2
+
7x3
+
4x4
=
0
x1
+
4x2
+
4x3
+
7x4
=
0
.
The solution is x1 = x4, x2 = −x4, x3 = −x4, x4 arbitary, so Eλ
consists of vectors of the form x4


1
−1
−1
1

.
Thus {[1, −1, −1, 1]T}
is a basis for Eλ. The eigenvalue λ = −1 has geometric and alge-
braic multiplicity 1.
8. The system (E −5I)x= θ has solution x1 = −x4, x2 = −x3, x3 and x4 arbitrary. The set
{[−1, 0, 0, 1]T, [0, −1, 1, 0]T} is a basis for Eλ.
The eigenvalue λ = 5 has algebraic and geometric multiplicity 2.
9. (E −15I)x = θ is the system
−9x1
+
4x2
+
4x3
+
x4
=
0
4x1
−
9x2
+
x3
+
4x4
=
0
4x1
+
x2
−
9x3
+
4x4
=
0
x1
+
4x2
+
4x3
−
9x4
=
0
.
The solution is x1 = x2 = x3 = x4, x4 arbitrary so Eλ con-
sists of vectors of the form x4


1
1
1
1

. Thus {[1, 1, 1, 1]T} is a basis
for Eλ. The eigenvalue λ = 15 has algebraic and geometric multi-
plicity 1.

114
CHAPTER 4. THE EIGENVALUE PROBLEMS
10. The system (F + 2I)x = θ has solution x1 = x2 = x3 = x4, x4 arbi-
trary. The set {[1, 1, 1, 1]T} is a basis for Eλ. The eigenvalue
λ = −2 has algebraic and geometric multiplicity 1.
11. (F −2I)x = θ is the system
−x1
−
x2
−
x3
−
x4
=
0
−x1
−
x2
−
x3
−
x4
=
0
−x1
−
x2
−
x3
−
x4
=
0
−x1
−
x2
−
x3
−
x4
=
0
.
The solution is x1 = −x2 −x3 −x4, x2, x3, x4 arbitrary so Eλ consists of vectors of the
form x2


−1
1
0
0

+ x3


−1
0
1
0

+ x4


−1
0
0
1

. Thus {[−1, 1, 0, 0]T, [−1, 0, 1, 0]T, [−1, 0, 0, 1]T}
is a basis for Eλ. The
eigenvalue λ = 2 has algebraic and geometric multiplicity 3.
12.
The characteristic equation is p(t) = −(t −1)2(t −2) = 0 so the eigenvalues are λ = 1
and λ = 2.
The eigenvectors for λ = 1
are the nonzero vectors of the form


x1
x3
x3

=
x1


1
0
0

+ x3


0
1
1

. The eigenvectors for λ = 2
are the nonzero vectors of the form


x2
x2
0

= x2


1
1
0

. The matrix is not defective.
13.
The characteristic equation for the given matrix A is 0 =
det(A −tI) =
¯¯¯¯¯¯
2 −t
1
2
0
3 −t
2
0
0
2 −t
¯¯¯¯¯¯
= −(t −2)2(t −3).
The eigenvalues are λ = 2 and λ = 3. The system (A −2I)x = θ is given by
x2 + 2x3 = 0
x2 + 2x3 = 0 .
In the solution x1 is arbitrary, x2 = −2x3, and x3 is arbitrary. The eigenvectors for λ = 2
are the nonzero vectors of the form


x1
−2x3
x3



4.5. EIGENVALUES AND EIGENVECTORS
115
= x1


1
0
0

+ x3


0
−2
1

.
Therefore λ = 2 has algebraic and geometric multiplicity 2.
The system (A −3I)x = θ is given by
−x1 + x2 + 2x3 = 0
2x3 = 0
−x3 = 0
.
The solution is x1 = x2, x2 arbitrary, x3 = 0. The eigenvectors for λ = 3 are the nonzero
vectors of the form


x2
x2
0

= x2


1
1
0

.
Therefore λ = 3 has algebraic and geometric multiplicity 1. The matrix is not defective.
14.
The characteristic polynomial is p(t) = −(t −1)3 so λ = 1 is the only eigenvalue. The
eigenvectors for λ = 1 are the nonzero vectors of the form


x1
0
0

= x1


1
0
0

. Thus λ = 1
has algebraic multiplicity 3 and geometric multiplicity 1. The matrix is defective.
15.
The given matrix A
has characteristic equation 0 = det(A −tI) = −(t −2)2(t −1)
so the eigenvalues are λ = 1
and λ = 2.
The system of equations (A −I)x = θ
has
solution x1 = −3x3, x2 = −x3, x3 arbitrary so the eigenvectors for λ = 1 are the nonzero
vectors of the form x = [−3x3, −x3, x3]T. For the system (A −2I)x = θ, x1
and x2
are
arbitrary and x3 = 0. The eigenvectors for λ = 2 are the nonzero vectors of the form x
= x1


1
0
0

+ x2


0
1
0

.
The matrix is not defective.
16.
The characteristic polynomial is p(t) = −(t −3)2(t + 4) so the eigenvalues are λ = 3 and
λ = −4. The eigenvectors for λ = 3 are the nonzero vectors of the form x= [5x3, 3x3, x3]T.
The eigenvectors for λ = −4 are the nonzero vectors of the form x = [−2x3, (2/3)x3, x3]T.
Since λ = 3 has algebraic multiplicity 2 and geometric multiplicity 1, the matrix is defective.
17.
The given matrix A has characteristic polynomial p(t) =
−(t + 1)(t −1)(t −2) so the eigenvalues for A are λ = −1, λ = 1, λ = 2. The system of
equations (A+I)x= θ has solution x1 = (1/2)x3, x2 = x3, x3 arbitrary so the eigenvectors
for λ = −1
are the nonzero vectors of the form x = [(1/2)x3, x3, x3]T.
The system of
equations (A−I)x= θ has solution x1 = −3x2, x2 arbitrary,x3 = −7x2 so the eigenvectors
for λ = 1 are the nonzero vectors of the
form x = [−3x2, x2, −7x2]T. The system of equations (A −2I)x = θ
has solution x1 = (1/2)x3, x2 = (−1/2)x3 so the eigenvectors for
λ = 2 are the nonzero vectors of the form
x = [(1/2)x3, (−1/2)x3, x3]T. The matrix is not defective.

116
CHAPTER 4. THE EIGENVALUE PROBLEMS
18. u1 = [1, 1]T is an eigenvector for the eigenvalue λ = 2 and u2 = [2, 5]T is an eigenvector
for the eigenvalue λ = −1. Moreover x = −6u1 +3u2
so A10x = −6(2)10u1 +3(−1)10u2
= [−6138, −6129]T.
19.
The characteristic polynomial for A is p(t) = −(t −1)2(t −2) so the eigenvalues for A
are λ = 1 and λ = 2. The vectors u1 = [1, 0, 0]T and u2 = [0, 1, 2]T are the eigenvectors
for λ = 1 and u3 = [1, 2, 3]T is an eigenvector for λ = 2. Moreover x = u1 +2u2 +u3
so
A10x = (1)10u1 +2(1)10u2 +(2)10u3 = [1025, 2050, 3076]T.
20.
Since λ is an eigenvalue for H, nullity (H −λI) ≥1. It follows that rank (H −λI) ≤3.
But a, b and c˚ are nonzero so the ﬁrst three columns of H −λI are linearly independent.
Therefore
rank (H −λI) ≥3. Thus rank (H −λI) = 3 and, hence,
nullity (H −λI) = 1. This proves that λ has geometric multiplicity 1.
21. P = P −1P 2 = P −1P = I.
22.
Suppose Px = λx , x ̸= θ.
Then λ2x = P 2x = Px = λx
so (λ2 −λ)x = θ.
Since x ̸=θ,
0 = λ2 −λ = λ(λ −1). Therefore either λ = 0 or λ = 1.
23. P 2 = (uuT)(uuT) = u(uTu)uT = uuT = P.
24. (I −Q)2 = I2 −IQ −QI + Q2 = I −Q. Also (I −2Q)2 = I2 −2IQ −2QI + 4Q2 = I so
(I −2Q)−1 = I −2Q.
25. P 2 = (uuT + vvT)(uuT + vvT) = u (uTu)uT + u (uTv)vT +
v (vTu)uT + v (vTv)vT = uuT +vvT = P.
26. P(au +bv) = aPu +bPv = a(uuT +vvT)u +b(uuT +vvT)v =
au(uTu) + av (vTu) + bu(uTv) + bv(vTv) = au +bv .
27.
(a) A has eigenvalues λ = 1 and λ = 3 with corresponding eigenvectors
u1 = [1/
√
2, 1/
√
2]T and u2 = [−1/
√
2, 1/
√
2]T, respectively,
where ∥u1 ∥=∥u2 ∥= 1. It is easily checked that u1u1T +3u2u2T = A.
(b) A has eigenvalues λ = −1 and λ = 3 with corresponding eigenvectors
u1 = [−1/
√
2, 1/
√
2]T and u2 = [1/
√
2, 1/
√
2]T, respec-
tively, where ∥u1 ∥=∥u2 ∥= 1. It is easily checked that
−u1u1T +3u2u2T = A.
(c) A
has eigenvalues λ = 4
and λ = −1
with corresponding eigenvectors u1 =
[2/
√
5, 1/
√
5]T and u2 = [1/
√
5, −2/
√
5]T, respec-
tively, where ∥u1 ∥=∥u2 ∥= 1. It is easily checked that
4u1u1T −u2u2T = A.
28. β(uTv) = uT(βv ) = u T(Av ) = (Av )Tu = vTATu = vT(Au ) =
vT(λu ) = λ(vTu ) = λ(uTv ). Since β ̸= λ it follows that uTv = 0.

4.6. COMPLEX EIGENVALUES AND EIGENVECTORS
117
29.
Set B = A −C. For each i, 1 ≤i ≤n, Bui = Aui −Cui = Aui −λ1u1(u1Tui ) −· · · −λiui
(uiTui ) −· · · −λnun (unTui ) = λiui −λiui = θ. If x
is in Rn then x
may be written in
the form x = a1u1 + · · · + anun , so Bx = a1Bu1 + · · · + anBun = θ. In particular Bej = θ
for 1 ≤j ≤n. If B = [B1 , . . . , Bn ] then Bej = Bj , so Bj = θfor 1 ≤j ≤n. This shows
that A −C = O so A = C.
4.6
Complex Eigenvalues and Eigenvectors
1. u = 3 + 2i.
2. z = 1 −i.
3. u + v = 7 −3i.
4. z + w = 3 −2i.
5. u + u = 6.
6. s −s = 4i.
7. vv = 17.
8. uv = 10 −11i.
9. s2 −w = −5 + 5i.
10. z2w = 2 + 4i.
11. uw2 = 17 −6i.
12. s(u2 + v) = 31 + 7i.
13. u/v = (uv)/(vv) = 10/17 −(11/17)i.
14. v/u2 = 8/169 + (53/169)i.
15. s/z = (sz)/(zz) = 3/2 + (1/2)i.
16. (w + v)/u = 22/13 + (6/13)i.
17. w + iz = 1.
18. s −iw = 0.

118
CHAPTER 4. THE EIGENVALUE PROBLEMS
19. For the given matrix A
the characteristic polynomial is p(t) = t2 −8t + 20
and the
eigenvalues are λ = 4 + 2i and λ = 4 −2i. The system of equations (A −(4 + 2i)I)x = θ
is given by
(2 −2i)x1
+
8x2
=
0
−x1
+
(−2 −2i)x2
=
0 .
The solution is x1 = (−2 −2i)x2, x2 arbitrary. Thus the eigenvectors for λ = 4 + 2i are
the nonzero vectors of the form
x = [(−2 −2i)x2, x2]T. By Theorem 16, x
is an eigenvector corresponding to λ.
20.
The characteristic polynomial is p(t) = t2 + 4 so the eigenvalues are λ = 2i and λ = −2i.
The eigenvectors for λ = 2i are the nonzero vectors of the form x = [(−1 −i)x2, x2]T. By
Theorem 16, x is an eigenvector for λ = −2i.
21. The given matrix A
has characteristic polynomial p(t) = t2 + 1
so the eigenvalues are
λ = i and λ = −i. The system (A −iI)x = θ is given by
(−2 −i)x1
−
x2
=
0
5x1
+
(2 −i)x2
=
0 .
The solution is x1 = ((−2 + i)/5)x2, so the eigenvectors for
λ = i are the nonzero vectors of the form x = [((−2 + i)/5)x2, x2]T.
By Theorem 16, x
is an eigenvector for λ = −i.
22.
The characteristic polynomial is p(t) = −(t −2)(t2 −4t + 5).
The eigenvalues are
λ1 = 2, λ2 = 2 + i,
and λ2 = 2 −i.
The eigenvectors for λ1 = 2
have the form x
= [0, −x3, x3]T, x3 ̸= 0. The eigenvectors for λ2 = 2 + i have the form x = [x3, (−2/5 −
(1/5)i)x3, x3]T,
x3 ̸= 0 and x is an eigenvector for λ2 = 2 −i.
23.
The given matrix A has characteristic polynomial p(t) =
−(t −2)(t2 −4t + 13) so the eigenvalues for A are λ1 = 2, λ2 =
2 + 3i, and λ2 = 2 −3i. The system (A −2I)x = θ is given by
−x1
−
4x2
−
x3
=
0
3x1
+
3x3
=
0
x1
+
x2
+
x3
=
0
.
The solution is x1 = −x3, x2 = 0, x3 arbitrary so the eigenvectors corresponding to λ1 = 2
are of the form x = [−x3, 0, x3]T. The system [A −(2 + 3i)I ]x = θ is given by
(−1 −3i)x1
−
4x2
−
x3
=
0
3x1
−
3ix2
+
3x3
=
0
x1
+
x2
+
(1 −3i)x3
=
0
.

4.6. COMPLEX EIGENVALUES AND EIGENVECTORS
119
The solution is x1 = (−5/2 + (3/2)i)x3 and x2 = (3/2 + (3/2)i)x3
so the eigenvectors for λ2 = 2 + 3i
are the nonzero vectors of the form x = [(−5/2 +
(3/2)i)x3, (3/2 + (3/2)i)x3, x3]T. By Theorem 16, x is an eigenvector for λ2 = 2 −3i.
24.
The characteristic polynomial is p(t) = (t2 −2t + 26)(t2 −2t + 5) so the eigenvalues are
λ1 = 1 + 5i, λ1 = 1 −5i, λ2 = 1 + 2i, λ2 = 1 −2i.
Eigenvectors for λ1
are the nonzero
vectors of the form x = [ix2, x2, 0, 0]T and x is an eigenvector for λ1. Eigenvectors for λ2
are the nonzero vectors of the form x = [0, 0, ix4, x4]T and x is an eigenvector for λ2.
25. x = 2 −i, y = 3 −2i.
26. x = i, y = 2.
27. xTx = (1 −i)(1 + i) + 2(2) = 6 so ∥x∥=
√
6.
28. xTx = (3 −i)(3 + i) + (2 + i)(2 −i) = 10 + 5 = 15 so ∥x∥=
√
15.
29. xTx = (1 + 2i)(1 −2i) + (−i)(i) + (3 −i)(3 + i) = 5 + 1 + 10 = 16. Thus ∥x∥=
√
16 = 4.
30. xTx = (−2i)(2i) + (1 + i)(1 −i) + 3(3) = 4 + 2 + 9 = 15 so ∥x∥=
√
15.
31. λ1 = −1.4937 + 1.2616i, x1 =


0.5835 −0.1460i
0.1650 −0.4762i
−0.4369 + 0.4397i

; λ2 = −1.4937 −1.2616i, x2 =


0.5835 + 0.1460i
0.1650 + 0.4762i
−0.4369 −0.4397i

; λ3 = 10.9873, x3 =


−0.4486
−0.7312
−0.5139

. In each case, the eigenvectors
are chosen to have length 1.
32. λ1 = −3.6884 + 2.8416i, x1 =


−0.0558 −0.6977i
−0.4571 + 0.1436i
0.3948 + 0.3532i

; λ2 = −3.6884 −2.8416i, x2 =


−0.0558 + 0.6977i
−0.4571 −0.1436i
0.3948 −0.3532i

; λ3 = 13.3769, x3 =


−0.4184
−0.7889
−0.4501

. For i = 1, 2, 3, ∥xi ∥= 1.
33. λ1 = 1.1857 + 2.6885i, x1 =


−0.0781 −0.6033i
−0.3495 + 0.5754i
0.1199 −0.1125i
0.1963 + 0.3334i

;
λ2 = 1.1857 −2.6885i, x2 =


−0.0781 + 0.6033i
−0.3495 −0.5754i
0.1199 + 0.1125i
0.1963 −0.3334i

;

120
CHAPTER 4. THE EIGENVALUE PROBLEMS
λ3 = 16.8037, x3 =


−0.5484
−0.0550
−0.1746
−0.8160

;
λ4 = 4.8249, x4 =


0.7046
−0.6728
−0.2027
−0.0995

. For i = 1, 2, 3, 4, ∥xi ∥= 1.
34. λ1 = 0.2617 + 2.0076i, x1 =


−0.2742 + 0.1318i
−0.6251 −0.1139i
0.1594 + 0.2495i
0.5871 −0.2672i

;
λ2 = 0.2617 −2.0076i, x2 =


−0.2742 −0.1318i
−0.6251 + 0.1139i
0.1594 −0.2495i
0.5871 + 0.2672i

;
λ3 = 16.6911, x3 =


0.5848
0.5644
0.1660
0.5585

;
λ4 = 3.7856, x4 =


−0.4955
0.6192
0.2253
−0.5659


35.
Let z = a + bi and w = c + di.
(a) z + w = (a + c) + (b + d)i so z + w = (a + c) −(b + d)i = z + w.
(b) zw = (ac −bd) + (ad + bc)i
so zw = (ac −bd) −(ad + bc)i.
Therefore z w =
(a −bi)(c −di) = (ac −bd) −(ad + bc)i = zw.
(c) z + z = (a + bi) + (a −bi) = 2a.
(d) z −z = (a + bi) −(a −bi) = 2bi.
(e) zz = (a + bi)(a −bi) = a2 −b2i2 = a2 + b2.
36.
If A = [aij] and B = [bij] then AB is the (n x p) matrix [cij] where cij = Pn
k=1 aikbkj.
Likewise A B
is the (n x p) matrix [dij]
where dij = Pn
k=1 aikbkj = Pn
k=1 aikbkj =
Pn
k=1 aikbkj = cij. Thus A B = AB. If A is a real matrix and x
is an (n x 1) vector
then Ax = Ax = Ax.
37. (AB)∗= (AB)
T = (A B)T = B
TA
T = B∗A∗.

4.7. SIMILARITY TRANSFORMATIONS & DIAGONALIZATION
121
38.
(a)
First note that for vectors x and y, y∗x = y∗x∗∗=
(x∗y)∗= x∗y. Now suppose that Ax = λx where x ̸= θ. Then λ ∥x ∥2 = λxTx =
λx∗x = x∗(λx) = x∗(Ax) = (Ax)∗x = (x∗A∗)x = x∗(Ax) = x∗(λx) = λx ∗x =
λ∥x∥2 =
λ ∥x∥2 . Since ∥x∥2̸= 0, λ = λ and λ is a real number.
(b) A∗is the (n x n) matrix [bij] where bij = aji. Since A = A∗, aij = aji for 1 ≤i, j ≤n.
In particular, aii = aii for 1 ≤i ≤n, so aii is a real number.
39.
(a)
Since p(r) = 0 we have 0 = 0 = p(r) = a0 + a1r + · · · + anrn = a0+a1 r+· · ·+anrn =
a0 + a1r + · · · + anrn = p(r).
(b)
Write p(t) = c(t −r1)(t −r2)(t −r3) where c is a real number and r1, r2, r3 are the
(not necessarily distinct) roots of p(t). If all three roots are real numbers then there
is nothing to prove, so assume that r1 = a + bi, b ̸= 0. By (a) we may also assume
that r2 = r1 = a−bi. Thus p(t) = c(t−r1)(t−r2)(t−r3) = c[t2−2at+(a2+b2)](t−r3).
Since the coeﬃcients of p(t) are real numbers, it follows that r3 is a real number.
(c)
The characteristic polynomial p(t) = det(A−tI) has degree three and real coeﬃcients.
By (b) p(t) has a real root so A has at least one real eigenvalue.
40.
For any vector x, ∥Ax ∥2= (Ax)
TAx = (Ax)T(Ax) = xTATAx = xTx =∥x ∥2.
In
particular if Ax = λx, x ̸= θ,
then ∥x∥2 =
∥λx∥2= λx
T(λx) = λλ(xTx) = λλ ∥x∥2. Thus λλ = 1.
41.
Let λ be an eigenvalue for A (note that λ is real by Theorem 17) and suppose x
in Rn
is a corresponding eigenvector. Then x ̸= θ and 0 < x TAx = x T(λx) = λxTx = λ ∥x∥2.
Since ∥x∥2 > 0 it follows that λ > 0.
42.
cf. Exercise 40.
4.7
Similarity Transformations and
Diagonalization
1. A has eigenvalues λ = 1 and λ = 3 with corresponding eigenvectors u1 = [1, 1]T and u2
= [−1, 1]T,
respectively. If S = [u1 , u2 ]
then S−1AS = D where D =
· 1
0
0
3
¸
. Now
D5 =
· 1
0
0
243
¸
so A5 = SD5S−1 =
·
122
−121
−121
122
¸
.
2.
For S =
· 1
−1
1
1
¸
, S−1AS =
· 0
0
0
2
¸
. A5 = S
· 0
0
0
25
¸
S−1 =
·
16
−16
−16
16
¸
.

122
CHAPTER 4. THE EIGENVALUE PROBLEMS
3. A
has only one eigenvalue, λ = −1.
The corresponding eigenvectors are the nonzero
vectors of the form x = [x2, x2]T. Since we cannot ﬁnd a linearly independent set {u1 , u2 }
of eigenvectors for A, A is not diagonalizable.
4. A has eigenvalue λ = 1 with associated eigenvectors of the form x = [x1, 0]T, x1 ̸= 0. A is
not diagonalizable,
5. A has eigenvalues λ = 1 and λ = 2 with corresponding eigenvectors u1 = [1, −10]T and
u2 = [0, 1]T, respectively. If S = [u1 , u2 ]
then S−1AS = D where D =
· 1
0
0
2
¸
.
Thus A5 = SD5S−1 =
S
· 1
0
0
32
¸
S−1 =
·
1
0
310
32
¸
.
6.
For S =
· 1
7
0
2
¸
, S−1AS =
· −1
0
0
1
¸
.
Thus A5 =
S
· (−1)5
0
0
(1)5
¸
S−1 =
· −1
7
0
1
¸
= A.
7. A
has eigenvalue λ = 1 with algebraic multiplicity 3. The eigenvectors for λ = 1
have
the form x = [x2 + 2x3, x2, x3]T. In particular we cannot obtain 3 linearly independent
eigenvectors so A is not diagonalizable.
8. A has eigenvalue λ = 1 with algebraic multiplicity 3 and geometric multiplicity 1, so A is
not diagonalizable.
9. A
has eigenvalues 1, 2,
and −1
with corresponding eigenvectors u1 = [−3, 1, −7]T, u2
= [−1, 1, −2]T, u3 = [1, 2, 2]T, respectively. If S = [u1 , u2 , u3 ] then
S−1 =


2
0
−1
16/3
1/3
7/3
5/3
1/3
−2/3

and S−1AS = D, where D =


1
0
0
0
2
0
0
0
−1

.
A5 = SD5S−1 =


163
−11
−71
−172
10
75
324
−22
−141

.
10. A has eigenvalues λ = 1 and λ = 2. For λ = 1, u1 = [1, 0, 0]T and u2 = [0, 1, 1]T are
linearly independent eigenvectors. λ = 2 has corresponding eigenvector u3 = [1, 1, 0]T. If
S = [u1 , u2 , u3 ] then S−1AS =


1
0
0
0
1
0
0
0
2

= D. Moreover A5 = SD5S−1 =


1
31
−31
0
32
−31
0
0
1

.

4.7. SIMILARITY TRANSFORMATIONS & DIAGONALIZATION
123
11. A has eigenvalue λ = 1 with algebraic multiplicity 2 and geometric multiplicity 1 so A is
not diagonalizable.
12.
If S =


1
0
3
0
−1
4
0
1
0

then S−1AS =


1
0
0
0
1
0
0
0
5

. A5 =
S


1
0
0
0
1
0
0
0
55

S−1 =


1
2343
2343
0
3125
3124
0
0
1

.
13. q1Tq2 = 0 and q1Tq1 = q2Tq2 = 1 so Q is orthogonal.
14. Q is orthogonal.
15. q1Tq1 = 5 so Q is not orthogonal.
16. q1Tq1 = 13 so Q is not orthogonal.
17. 0 = q1Tq2 = q1Tq3 = q2Tq3
and 1 = q1Tq1 = q2Tq2 = q3Tq3
so Q is orthogonal.
18. q1Tq1 = 6 so Q is not orthogonal.
19.
If Q is orthogonal then 2α2 = 1, 6β2 = 1, a2+b2+c2 = 1, αa+αc = 0, and βa+2βb−βc = 0.
This implies that α = 1/
√
2, β = 1/
√
6, a = −c and b = c, where c = ±1/
√
3. Thus we
one choice for Q is
Q =


1/
√
2
1/
√
6
−1/
√
3
0
2/
√
6
1/
√
3
1/
√
2
−1/
√
6
1/
√
3

.
20.
If Q
is orthogonal then α = 1/
√
3, β = 1/
√
14, a = (−5/4)c,
and b = (1/4)c
where
c = ±4/
√
42.
21. Q =


−0.8807
0.4332
0.1918
0.1849
0.6870
−.7028
0.4361
0.5835
0.6851

T =


0.5048
2.9498
−1.4966
0.0
8.3443
0.2429
0.0
0.0
−2.8491


22. Q =


0.5892
−0.7155
−0.3754
0.7516
0.6559
−0.0703
−0.2965
0.2407
−0.9242

T =


−0.5223
−7.6134
−8.2480
0.0
−5.6251
0.0735
0.0
0.0
7.1474



124
CHAPTER 4. THE EIGENVALUE PROBLEMS
23. Q =


−0.5276
−0.5463
−0.6406
0.1130
−0.4235
−0.4440
0.6477
−0.4516
−0.3315
0.0292
0.3989
0.8545
−0.6576
0.7096
−0.1044
−0.2306


T =


19.2422
1.4541
−3.5019
0.0983
0.0
−3.8383
−3.4646
−0.5055
0.0
0.0
4.8409
3.5148
0.0
0.0
0.0
0.7552


24. Q =


0.5118
−0.5287
0.6725
−0.0794
0.6815
−0.2030
−0.6454
0.2788
0.3517
0.3331
−0.1083
−0.8681
0.3871
0.7539
0.3457
0.4029


T =


19.1640
5.4524
1.6686
−1.9529
0.0
2.9467
−2.5265
−2.3201
0.0
0.0
−3.8881
0.9528
0.0
0.0
0.0
−2.2226


25.
(a) (S−1AS)2 = (S−1AS)(S−1AS) = S−1A(SS−1)AS = S−1A2S.
(S−1AS)3 = (S−1AS)2(S−1AS) = (S−1A2S)(S−1AS) = S−1A2(SS−1)AS = S−1A3S.
(b)
Suppose (S−1AS)n = S−1AnS
for some integer n ≥1.
Then (S−1AS)n+1 =
(S−1AS)n(S−1AS) = (S−1AnS)(S−1AS) = S−1An(SS−1)AS = S−1An+1S. By in-
duction (S−1AS)k = S−1AkS for any positive integer k.
26.
Suppose S−1AS = D,
where D is a diagonal matrix, and suppose W −1AW = B. If
T = W −1S then T
is invertible and T −1BT = S−1WBW −1S = S−1AS = D. Therefore
B is diagonalizable.
27.
Suppose that S−1AS = B.
(a) S−1(A + αI)S = S−1AS + S−1(αI)S = B + αS−1IS = B + αI.
(b)
Set Q = (S−1)T = (ST)−1. Then Q−1ATQ = STAT(S−1)T = (S−1AS)T = BT.
(c)
A product of nonsingular matrices is nonsingular so if A is nonsingular then so is
B. Moreover B−1 = (S−1AS)−1 =
S−1A−1(S−1)−1 = S−1A−1S. Therefore B−1 is similar to A−1.
28.
For (b) if x , y
are in Rn then (Qx )T(Qy ) = x TQTQy = xTI y = xTy .
For (c) let t = det(Q).
Recall that from Theorem 5 of Section 3.3 that det(QT) =
det(Q).
Since QTQ = I,
Theorem 2 of Section 3.2 gives 1 = det(I) = det(QTQ) =
det(QT) det(Q) = t2. But t2 = 1 implies that t = ±1.

4.7. SIMILARITY TRANSFORMATIONS & DIAGONALIZATION
125
29. First note that QT = (I −2uuT)T = I T −2u TTu T =
I −2u u T = Q, so Q is symmetric. Thus QTQ = QQ =
(I −2uuT )(I −2uuT ) = I 2 −2u u TI −2I uuT + 4u (uTu )u T =
I . Thus Q is orthogonal. Moreover Qu = (I −2uuT)(u) = I u −2u(uTu) = u −2u = −u,
so u s an eigenvector corresponding to the eigenvalue λ = −1.
30. (AB)T(AB) = BT(ATA)B = BTB = I .
31. y = [b/
√
a2 + b2, −a/
√
a2 + b2]T is one choice. −y
is another choice.
32. QTQ =
· uTu
uTv
vTu
vTv
¸
=
· 1
0
0
1
¸
so Q is orthogonal. Now
QTAQ is the product
· uT
vT
¸
A[u , v ] =
· uT
vT
¸
[Au , Av ] =
· uT
vT
¸
[λu , Av ] =
· λuTu
u TAv
λvTu
v TAv
¸
=
· λ
u TAv
0
v TAv
¸
.
33.
If u = [1/
√
2, −1/
√
2]T then Au = 2u
and uTu = 1. If v =
[1/
√
2, 1/
√
2]T then uTv = 0 and vTv = 1. If Q = [u , v ] then QTAQ =
· 2
−2
0
2
¸
.
34.
If u = [1/
√
5, 2/
√
5]T then Au = u
and uTu = 1. If v =
[2/
√
5, −1/
√
5]T
then uTv = 0
and vTv = 1. If Q = [u , v ]
then QTAQ =
· 1
8
0
2
¸
.
Similarly if u = [2/
√
13, 3/
√
13]T and v =
[3/
√
13, −2/
√
13]T
then Au = 2u , uTu = vTv = 1,
and uTv = 0.
If Q = [u , v ]
then
QTAQ =
· 2
8
0
1
¸
.
35.
If u = [1/
√
2, 1/
√
2]T then Au = u
and uTu = 1. If v =
[1/
√
2, −1/
√
2]T then uTv = 0 and vTv = 1. If Q = [u , v ] then
QTAQ =
· 1
0
0
3
¸
.
36.
If Q =
· −1/
√
2
1/
√
2
1/
√
2
1/
√
2
¸
then QTAQ =
· 0
1
0
5
¸
.
Also if Q =
· 2/
√
13
3/
√
13
3/
√
13
−2/
√
13
¸
then QTAQ =
· 5
1
0
0
¸
.

126
CHAPTER 4. THE EIGENVALUE PROBLEMS
37.
Note that ARj
is the jth column of AR and RT
i
is the ith row of RT. Thus RT
i ARj
is the ijth entry of RTAR.
38.
It is an consequence of Exercise 37 that
QTAQ =


uTAu
uTAv
uTAw
vTAu
vTAv
vTAw
wTAu
wTAv
wTAw

.
But Au = λu , uTu = 1, vTu = 0 = wTu , so QTAQ has the desired form.
39.
The matrix B has characteristic polynomial p(t) = det(B −tI).
Expansion along the ﬁrst column gives p(t) = (λ −t) det(A1 −tI) =
(λ−t)q(t) where q(t) is the characteristic polynomial for A1. Since every root of q(t) is also
a root of p(t), each eigenvalue for A1 is also an eigenvalue for B. Since Q is an orthogonal
matrix QT = Q−1 and B is similar to A. Thus A and B have the same eigenvalues. In
particular B has only real eigenvalues. It follows that A1 has only real eigenvalues.
40.
(a)
It is straightforward to show that
RT =


1
|
0
0
−−
+
−−
−−
0
|
ST
0
|


and that
RTR =


1
|
0
0
−−
+
−−
−−
0
|
STS
0
|

= I.
(b) RTQTAQR = RTBR and RTBR has the form


λ
|
a
b
−−
+
−−
−−
0
|
STA1S
0
|

=


λ
|
a
b
−−
+
−−
−−
0
|
T1
0
|

.
Since T1 is upper triangular so is RTBR.

4.7. SIMILARITY TRANSFORMATIONS & DIAGONALIZATION
127
41.
Assume that Theorem 22 is true for any [(k-1) x (k-1)] matrix with only real eigenvalues.
Now let A be a (k x k) matrix with only real eigenvalues and suppose Au = λu
where
uTu = 1. By the Gram-Schmidt process there is an orthonormal basis {u1 , u2 ,
. . . , uk } for Rk such that u1 = u . The matrix Q =
[u1 , u2 , . . . , uk ] is orthogonnal and
QTAQ =


λ
|
u1TAu2
· · ·
u1TAuk
−−
+
−−−−
−−−−
−−−−
0
|
...
|
A1
0
|


,
where A1 is a [(k-1) x (k-1)] matrix. If B = QTAQ
then B has characteristic polynomial
p(t) = det(B −tI). Expanding along the ﬁrst column yields p(t) = (λ −t) det(A1 −tI) =
(λ −t)q(t) where q(t) is the characteristic polynomial for A1. Thus every eigenvalue for
A1 is also an eigenvalue for B. But B is similar to A (since QT = Q−1˚) so B has only
real eigenvalues and, therefore A1 has only real eigenvalues. By assumption there exists a
[(k-1) x (k-1)] orthogonal matrix S such that STA1S = T1 where T1 is upper triangular.
If R is the (k x k) matrix
R =


1
|
0
· · ·
0
−−
+
−−
−−−−
−−
0
|
...
|
S
0
|


then R is orthogonal and P = QR is orthogonal. Furthermore
P TAP = RTQTAQR = RTBR and RTBR has the form


λ
|
c2
· · ·
ck
−−
+
−−
−−−−
−−
0
|
...
|
STA1S
0
|


=


λ
|
c2
· · ·
ck
−−
+
−−
−−−−
−−
0
|
...
|
T1
0
|


.

128
CHAPTER 4. THE EIGENVALUE PROBLEMS
Since T1 is upper triangular so is RTBR. The theorem now follows by induction.
42. Au1= (2−n)u1 while A(e1−ei) = Ae1−Aei= A1−Ai= [2, 0, . . . , 0, −2, 0, . . . , 0]T = 2(e1
−ei ), for 2 ≤i ≤n.
43. Since A is symmetric we have β(uTv ) = uT(βv ) = uTAv =
uTATv = (Au )Tv = (λu )Tv = λuTv . Since β ̸= λ, uTv = 0.
4.8
Applications
1. x1 =
· 4
2
¸
, x2 =
· 2
4
¸
, x3 =
· 4
2
¸
, x4 =
· 2
4
¸
.
2. x1 = x2 = x3 = x4 =
· 12
12
¸
.
3. x1 =
·
80
112
¸
, x2 =
·
68
124
¸
, x3 =
·
65
127
¸
, x4 =
·
64.25
127.75
¸
.
4. x1 =
·
5
−1
¸
, x2 =
· 11
−7
¸
, x3 =
·
29
−25
¸
, x4 =
·
83
−79
¸
.
5. x1 =
· 7
1
¸
, x2 =
· 11
8
¸
, x3 =
· 43
19
¸
, x4 =
· 119
62
¸
.
6. x1 =
· 6
8
¸
, x2 =
· 26
48
¸
, x3 =
· 126
248
¸
, x4 =
·
626
1248
¸
.
7. A has eigenvalues λ1 = 1 and λ2 = −1 with corresponding eigenvectors u1 = [1, 1]T and
u2 = [−1, 1]T, respectively. x0 = 3u1 +u2
so xk = 3(1)ku1 +(−1)ku2 = [3 + (−1)k+1, 3 +
(−1)k]T.
In particular x4 = [2, 4]T = x10 . The sequence {xk }
has no limit but ∥xk ∥
=
√
20 for all k.
8. A has eigenvalues λ1 = 1 and λ2 = 0 with corresponding eigenvectors u1 = [1, 1]T and
u2 = [−1, 1]T,
respectively. x0 = 12u1 −4u2
so xk = 12u1 .
In particular x4 = x10
= [12, 12]T. The limit of the
sequence {xk } is x ∗= [12, 12]T.
9. A
has eigenvalues λ1 = 1
and λ2 = 0.25
with corresponding eigenvectors u1 = [1, 2]T
and u2 = [−1, 1]T,
respectively.
x0 = 64u1 −64u2
so xk = 64(1)ku1 −64(0.25)ku2
= [64 + 64(0.25)k, 128 −64(0.25)k]T. In particular x4 = [64.25, 127.75]T and x10 =
[64.00006, 127.99994]T. The sequence {xk } converges to x∗=
[64, 128]T.

4.8. APPLICATIONS
129
10. A has eigenvalues λ1 = 3 and λ2 = 1 with corresponding eigenvectors u1 = [−1, 1]T and
u2 = [1, 1]T, respectively. x0 = −u1 +2u2 , so xk = −3ku1 +2u2 = [2 + 3k, 2 −3k]T. In
particular x4 = [83, −79]T
and x10 = [59051, −59047]T. The sequence {xk } has no limit and limk→∞∥xk ∥= ∞.
11. A has eigenvalues λ1 = 3 and λ2 = −1 with corresponding eigenvectors u1 = [2, 1]T and
u2 = [−2, 1]T, respectively. x0 =
(3/4)u1 +(5/4)u2
so xk = (3/4)(3)ku1 +(5/4)(−1)ku2 =
(1/4)[2(3)k+1 −10(−1)k, 3k+1 + 5(−1)k]T.
In particular x4 =
[119, 62]T and x10 = [88571, 44288]T. The sequence {xk } has no limit and limk→∞∥xk ∥
= ∞.
12. A has eigenvalues λ1 = 5 and λ2 = 1 with corresponding eigenvectors u1 = [1, 2]T
and
u2 = [1, −2]T,
respectively. x0 = u1 +u2
so xk = 5ku1 +u2 = [5k + 1, 2(5)k −2]T. In
particular, x4 = [626, 1248]T and x10 = [390626, 781248]T. The sequence {xk } has no limit
and
limk→∞∥xk ∥= ∞.
13. A
has eigenvalues λ1 = 2, λ2 = 1, λ3 = −1
with corresponding eigenvectors u1 =
[1, −1, 2]T, u2 = [3, −1, 7]T, and u3 = [1, 2, 2]T, re-
spectively. x0 = 2u1 +2u2 −5u3 , so
xk = 2k+1u1 +2u2 +5(−1)k+1u3;
thus : xk =


2k+1 + 6 + 5(−1)k+1
−2k+1 −2 + 10(−1)k+1
2k+2 + 14 + 10(−1)k+1

.
In particular x4 = [33, −44, 68]T and x10 = [2049, −2060, 4100]T.
The sequence {xk } has no limit and limk→∞∥xk ∥= ∞.
14. A
has eigenvalues λ1 = 4, λ2 = 1, λ3 = −1
with corresponding eigenvectors u1 =
[14, 11, 43]T, u2 = [1, 1, 2]T, and u3 =
[1, −1, 2]T, respectively. x0 = −0.2u1 +3.5u2 +0.3u3
so xk =
−0.2(4)ku1 +3.5u2 +0.3(−1)ku3 ; that is
xk =


(−2.8)4k + 3.5 + 0.3(−1)k
(−2.2)4k + 3.5 −0.3(−1)k
(−8.6)4k + 7 + 0.6(−1)k

.
In particular x4 = [−713, −560, −2195.2]T and x10 =

130
CHAPTER 4. THE EIGENVALUE PROBLEMS
[−2936009, −2306864, −9017746]T.
The sequence {xk } has no limit and limk→∞∥xk ∥
= ∞.
15. If x(t) =
· u(t)
v(t)
¸
then x′(t) = Ax(t) for A =
· 5
−6
3
−4
¸
. Eigenvalues and corresponding
eigenvectors for A are λ1 = 2, u1 =
· 2
1
¸
and λ2 = −1, u2 =
· 1
1
¸
. Setting x(t) =
ae2tu1 + be−tu2 and x(0) =
· 4
1
¸
yields a = 3 and b = −2. Thus, x(t) = 3e2tu1 −2e−tu2.
Equivalently, u(t) = 6e2t −2e−t and v(t) = 3e2t −2e−t.
16. If x(t) =
· u(t)
v(t)
¸
then x′(t) = Ax(t) for A =
· 1
2
2
1
¸
. Eigenvalues and corresponding
eigenvectors for A are λ1 = 3, u1 =
· 1
1
¸
and λ2 = −1, u2 =
· −1
1
¸
. Setting x(t) =
ae3tu1 + be−tu2 and x(0) =
· 1
5
¸
yields a = 3 and b = 2. Thus, x(t) = 3e3tu1 + 2e−tu2.
Equivalently, u(t) = 3e3t −2e−t and v(t) = 3e3t + 2e−t.
17. The matrix A =


1
1
1
0
3
3
−2
1
1

has eigenvalues λ1 = 0, λ2 = 2, λ3 = 3. Corresponding
eigenvectors are u1 = [0, −1, 1]T , u2 = [−2, −3, 1]T , u3 = [1, 2, 0]T , respectively.
The
solution is x(t) = 2u1−e2tu2+e3tu3. Equivalently, u(t) = 2e2t+e3t, v(t) = −2+3e2t+2e3t,
and w(t) = 2 −e2t.
18. The matrix A =


−2
2
−3
2
1
−6
−1
−2
0

has eigenvalues λ1 = 5 and λ2 = −3, where λ2 has
algebraic multiplicity 2. An eigenvector for λ1 is u1 = [−1, −2, 1]T . Further, u2 = [−2, 1, 0]T
and u3 = [3, 0, 1]T are linearly independent eigenvectors for λ2. The solution is x(t) =
e5tu1 + e−3tu2 + 2e−3tu3. Equivalently, u(t) = −e5t + 4e−3t, v(t) = −2e5t + e−3t, w(t) =
e5t + 2e−3t.
19.
(a)
The eigenvectors corresponding to λ = 1 have the form x= [a, 0], a ̸= 0. In particular
λ = 1 has algebraic multiplicity 2 and geometric multiplicity 1. Thus A is defective.
(b)
For k = 0 we have x0 = [1, 1]T = [2(0) + 1, 1]T. Suppose
xm = [2m + 1, 1]T for some integer m ≥0. Then xm+1 =
Axm = [2m + 3, 1]T = [2(m + 1) + 1, 1]T. It follows that
xk = [2k + 1, 1]T for each k ≥0.
20. α = −0.5. For each k ≥0, xk = x0 so limk→∞xk = [1, 1]T.

4.8. APPLICATIONS
131
21.
For α = −0.18 A has eigenvalues λ1 = 1 and λ2 = −0.18 with corresponding eigenvectors
u1 = [3, 10]T
and u2 = [10, −6]T, respectively. Moreover x0 = (16/118)u1 +(7/118)u2 .
Therefore xk = (16/118)u1 +(7/118)(−0.18)ku2 . It follows that limx→∞xk = (16/118)u1 .
22.
Note that u1 = Au0 = Av0 = v1. Suppose we have shown that um = vm for some m ≥1.
Then um+1 = Aum = Avm = vm+1 . It follows by induction that uk = vk for all k.
23. Bw
is the vector [c1, c2, . . . , cn]T where ci = bi1 + bi2 + · · · +
bin = 1.
Therefore Bw = w
and λ = 1
is an eigenvalue for B
with corresponding
eigenvector w .
24.
Suppose u = [u1, u2, . . . , un]T and choose i so that |ui |=
max1≤i≤n{| u1 |, . . . , | un |}. Set α = 1/ui and v = αu . Then v = [v1, v2, . . . , vn]T
where
vi = 1 and | vj |≤1 for 1 ≤j ≤n. Note also that Bv = v . Equating the ith component
yields bi1v1+· · ·+binvn = vi = 1. But bi1+· · ·+bin = 1 so it follows that v1 = · · · = vn = 1.
Thus v = w = αu , so u = α−1w .
25.
Suppose Bu = λu, u ̸= θ. As in Exercise 24, deﬁne v = [v1, . . . , vn]T such that Bv = λv
, vi = 1 for some i, and |vj |≤1 for 1 ≤j ≤n. The ith component of λv is λ whereas
the ith component of Bv is bi1v1 + · · · + binvn. Therefore |λ|=|bi1v1 + · · · + binvn |≤
bi1 |v1 | + · · · + bin |vn |≤bi1 + · · · + bin = 1.
26.
The matrix AT is a stochastic matrix. Moreover A and AT have the same eigenvalues
so we may apply Exercises 23 and 25.
27.
If au +bv = θ then θ = Aθ= A(au +bv ) = aAu +bAv = aλu +
b(λv +u ) = λ(au +bv ) + bu = bu . Since u ̸= θ it follows that b = 0.
Thus au = θ and
a = 0. This proves that {u , v } is a linearly independent set.
28.
The formula holds for k = 1 by Exercise 27. Suppose Amv = λmv +mλm−1u for some
m ≥1.
Then Am+1v = A(Amv ) = A(λmv +mλm−1u ) = λmAv +mλm−1Au = λm(λv
+u ) + mλm−1λu = λm+1v +(m + 1)λmu. By induction the given formula holds for every
integer k ≥1.
29.
(a)
The vector u = [1, 0]T
is an eigenvector for λ = 1 and v = [0, 1/2]T
satisﬁes the
equation (A −I)v = u .
(b) x0 = u +2v .
(c) Akx0 = Ak(u +2v ) = Aku +2Akv = u +2(v +ku ) =
(2k + 1)u +2v .
(d)
It follows immediately from (c) that Akx0 = [2k + 1, 1]T.

132
CHAPTER 4. THE EIGENVALUE PROBLEMS
4.9
Supplementary Exercises
1. Det (A) = x2 −9 so A is singular when x = ±3.
2. x ≥−1/4.
3. [1, 1]T is an eigenvector for the eigenvalue λ = 2.
4.
(a) det(A−1B2) = (det B)2/ det A = 81/2.
(b) det(3A) = 33 det(A) = 54.
(c) det(AB2A−1) = (det B)2 = 81.
5. x ̸= 0.
6. A2 = −3A + I so A2u = −3Au + u = [−5, 0]T . A3 = −3A2 + A so A3u = −3A2u + Au =
[17, 1]T .
7. Suppose Ax = θ. Then A2x = −3Ax + x (cf. Exercise 6), so θ = x.
8. I = A2 + 3A = A(A + 3I) so A−1 = A + 3I. Thus A−1u = Au + 3u = [5, 10]T .
9. A2 = −3A + I; A3 = 10A −3I; A4 = −33A + 10I; A5 = 109A −33I.
10. λ1 = −1, λ2 = −2.
11. λ1 = 2, λ2 = 3.
12. x = 8, λ = −6.
13. x = 0, y = −1.
14. x = 2, y = 3.
4.10
Conceptual Exercises
1. False. A =
· −1
0
0
1
¸
.
2. True. If Ax = λx then A−1x = (1/λ)x.
3. True. Det (A4) = (det A)4.
4. False. A =
· 1
1
0
1
¸
.
5. True. ∥Ax∥2 = (Ax)T (Ax) = xT AT Ax = xT x =∥x∥2.

4.10. CONCEPTUAL EXERCISES
133
6. True. Det (S−1AS −tI) = det[S−1(A −tI)S] = det(A −tI).
7. False. A =
· 1
0
0
2
¸
and B =
· 2
0
0
1
¸
.
8. False. A =
· 1
0
0
0
¸
.
9. (a) Qx (b) QT u.
10. If Ax = λx then A3x = λ3x. But A3 = O.
11. If P −1AP = I then A = PP −1 = I.
12. A−1(AB)A = BA.
13. Suppose S−1AS = B. Then S−1A2S = (S−1AS)(S−1AS) = B2. Similarly, S−1A3S =
(S−1A2S)(S−1AS) = B2B = B3 and S−1A4S = (S−1A3S)(S−1AS) = B3B = B4.
14.
(a) Yes. AT = (I −2uuT )T = IT −2uuT = I −2uuT = A.
(b) Yes. AAT = A2 = (I −2uuT )2 = I −4uuT + 4u(uT u)uT = I.
(c) Au = (I −2uuT )u = u −2u(uT u) = −u.
(d) Aw = (I −2uuT )w = w −2u(uT w) = w.
(e) λ = 1 has geometric multiplicity n −1 and λ = −1 has geometric multiplicity 1.

Chapter 5
Vector Spaces and Linear
Transformations
5.1
Introduction (No exercises)
5.2
Vector Spaces
1. u −2v =
·
0
−7
5
−11
−3
−12
¸
; u −(2v −3w) =
·
12
−22
38
−50
−6
−15
¸
;
−2u −v +3w =
·
7
−21
28
−42
−7
−14
¸
.
2. u −2v = −x2 −4x; u −(2v −3w ) = −x2 + 2x + 3;
−2u −v +3w = −3x2 + 4x + 8.
3. u−2v= ex −2 sin x; u−(2v−3w) = ex −2 sin x+3
√
x2 + 1; −2u−v+3w= −2ex −sin x+
3
√
x2 + 1;
4.
For u, v, and w in Exercise 2 we may take c1 = c3, c2 = −c3, c3 arbitrary. For example,
c1 = 1, c2 = −1, c3 = 1 is one choice. For u, v, and w
in Exercise 1, c1u +c2v +c3w = θ
if and only if c1 = c2 = c3 = 0.
5.
Note that c1u +c2v +c3w = (c1 + c2)x2 + (2c2 + 2c3)x + (−2c1 −c2 + c3). Thus c1u +c2v
+c3w = x2 + 6x + 1 if and only if
c1
+
c2
=
1
2c2
+
2c3
=
0
−2c1
−
c2
+
c3
=
0.
Solving yields c1 = −2 + c3, c2 = 3 −c3, c3 arbitrary. One choice is

136
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
c1 = −2, c2 = 3, c3 = 0 and a direct calculation shows that −2u +
3v = x2 + 6x + 1. Similarly c1u +c2v +c3w = x2 if and only if
c1
+
c2
=
1
2c2
+
2c3
=
6
−2c1
−
c2
+
c3
=
1.
The system is easily seen to be inconsistent.
6. S is a vector space.
7. S
is not a vector space. None of properties (c1), (c2), (a3), and (a4) of Deﬁnition 1 is
satisﬁed. For example v = [1, 0, 0, 0]T and
w = [0, 0, 0, 1]T are in S but u +w
is not in S.
8. P
is a vector space.
9. P
is not a vector space. Properties (c1), (c2), and (a3) of Deﬁnition 1 fail to hold in P.
For example p(x) = 1 + 2x2 and q(x) = x −2x2 are in P
but p(x) + q(x) is not in P.
10. P
is a vector space.
11. P
is not a vector space (cf. Exercise 9).
12. S is a vector space.
13. S is a vector space.
14. S is not a vector space. For example the (3 x 4) zero matrix is not in S.
15. S is not a vector space. Properties (c1), (c2), and (a3) of Deﬁnition 1 fail to hold in S.
16. S is not a vector space. For example if A = [aij] is a nonzero matrix in S then
√
2 A is
not in S. Thus property (c2) of Deﬁnition 1
fails to hold. Note that S satisﬁes the remaining properties of Deﬁni-
tion 1.
17.
Let A =
· 1
0
0
1
¸
and let B =
· 0
1
1
0
¸
. Then A and B are in Q but A + B is not in
Q. Also 0A, the (2 x 2) zero matrix, is not in Q.
18. Q is not a vector space. For example, if A =
· 1
0
0
0
¸
and B =
· 0
0
0
1
¸
then A and B
are in Q but A + B is nonsingular.

5.2. VECTOR SPACES
137
19.
The set of all (2 x 2) matrices is a vector space so axioms (a1), (a2), (m1), (m2), (m3),
and (m4) are satisﬁed by any subset. Now let A and B
be (2 x 2) symmetric matrices;
that is A = AT
and B = BT. Therefore (A + B)T = AT + BT = A + B, so A + B
is
symmetric. This veriﬁes that property (c1) holds. For any scalar c, (cA)T = cAT = cA, so
cA is symmetric and (c2) is satisﬁed. Clearly the (2 x 2) zero matrix is symmetric so (a3)
holds. Finally if A is symmetric then so is −A, so (a4) holds. Therefore Q is a vector
space.
20.
Suppose u, v,
and w
are vectors in a vector space V
such that u+ v=u+w.
By
property (a4) of Deﬁnition 1, V
contains a vector -u such that u+(-u)= θ.
By property
(a1),(-u)+u= θ.
Applying properties (a1), (a3), and (a2) yields v=v+ θ= θ+v=
[(-u)+u]+ v= (-u)+(u+ v)= (-bfu)+(u+w)= [(-u)+u]+w=
θ +w= w+ θ =w.
Similarly, v+u=w+u implies
that v=w.
21.
Let u
and w
be inverses for v .
Thus u + v = v +u = θ
and w + v = v +w = θ.
Therefore u = u + θ = u +(v +w ) = (u + v ) + w = θ +w = w .
22.
Note that 0v + θ = 0v = (0 + 0)v = 0v +0v . By the cancellation laws, θ = 0v .
23.
If a = 0 then we are done, so suppose that a ̸= 0. Then v = 1v =
(a−1a)v = a−1(av) = a−1θ = θ.
24.
We show as illustrations that properties (a2) and (m1) hold.
Thus θ +(θ + θ) = θ + θ = θ = θ + θ = (θ + θ)+ θ, so (a2) holds. If a and b are scalars
then a(bθ) = aθ = θ = (ab)θ so (m1) is satisﬁed.
25. F
is a vector space. Since F
is a subset of C[−1, 1]
and C[−1, 1]
is a vector space,
properties (a1), (a2), (m1), (m2), (m3), and (m4) hold in F. Now let g(x), h(x) be in F;
that is g(x) and h(x) are continuous, g(−1) = g(1) and h(−1) = h(1). It follows that
(g + h)(x) = g(x) + h(x) is continuous and (g + h)(−1) = g(−1) + h(−1) = g(1) + h(1) =
(g + h)(1). Therefore (g + h)(x) is in F
and property (c1) holds. If a is a scalar then
(ag)(x) = ag(x) is continuous and (ag)(−1) = ag(−1) = ag(1) = (ag)(1), so (ag)(x) is in
F. This veriﬁes that (c2) holds. The zero vector in C[−1, 1] is the function θ deﬁned by
θ(x) = 0 for all x, −1 ≤x ≤1. In particular θ(−1) = 0 = θ(1) so θ(x) is in F. Thus
θ(x)
is also the zero vector for F
and (a3) is satisﬁed. Property (a4) is an immediate
consequence of (c2) since −g(x) = (−1)g(x) for g(x) in C[−1, 1]. Since F
satisﬁes the
properties of Deﬁnition 1, F
is a vector space.
26. F
is a vector space.
27. F
is not a vector space. For example set f(x) = 2x −1 and g(x) = 2x2 −1. Then f(x)
and g(x) are in F
whereas f(x) + g(x) = 2x2 + 2x −2 is not.

138
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
28. F
is a vector space.
29. F
is a vector space. As in Exercise 25, it suﬃces to check that properties (c1), (c2), (a3),
and (a4) of Deﬁnition 1 are satisﬁed. To check (c1) for example, let f(x) and g(x) be in
F
. Then
R 1
−1[f(x) + g(x)]dx =
R 1
−1 f(x)dx +
R 1
−1 g(x)dx = 0 + 0 = 0, so f(x) + g(x) is
in F
.
30. If f(x) and g(x) are in C2[a, b] then (f + g)(x) = f(x) + g(x) is continuous on [a, b], (f +
g)′(x) = f ′(x)+g′(x) is continuous on [a, b], and (f +g)′′(x) = f ′′(x)+g′′(x) is continuous
on [a, b].
Similarly if c is a scalar then the functions (cf)(x) = cf(x), (cf)′(x) = cf ′(x),
and (cf)′′(x) = cf ′′(x) are all continuous on [a, b]. It follows that C2[a, b]
is a vector space.
31.
(a) F
is a vector space. Since F
is a subset of C2[−1, 1] and
C2[−1, 1] is a vector space by Exercise 30, properties (a1), (a2), (m1), (m2), m(3), and
(m4) hold in F˚. Now let g(x), h(x) be in F . Thus g′′(x)+g(x) = 0 and h′′(x)+h(x) =
0. It follows that (g + h)′′(x) + (g + h)(x) = [g′′(x) + g(x)] + [h′′(x) + h(x)] = 0 + 0 = 0
for −1 ≤x ≤1. Therefore (g + h)(x) is in F
and property (c1) is satisﬁed. If a is
any scalar then (ag)′′(x) + (ag)(x) = a[g′′(x) + g(x)] = a0 = 0 for −1 ≤x ≤1. Thus
(ag)(x) is in F
and property (c2) is satisﬁed. The zero vector in C2[−1, 1] is the
function θ deﬁned by θ(x) = 0, −1 ≤x ≤1. In particular θ′′(x) + θ(x) = 0 + 0 = 0
for −1 ≤x ≤1 so θ(x) is in F. Therefore θ(x) is the zero vector in F
and (a3) is
satisﬁed. Property (a4) follows from (c2) since −f(x) = (−1)f(x) for every f(x) in
C2[−1, 1].
(b) F is not a vector space. For example suppose that g(x) and h(x) are in F ; that is,
assume that g′′(x)+g(x) = x2 and h′′(x)+h(x) = x2. Then (g+h)′′(x)+(g+h)(x) =
[g′′(x) + g(x)] + [h′′(x) + h(x)] = 2x2. Therefore (g + h)(x) is not in F
.
32. Let p(x)
and q(x)
be in P.
Then we may write p(x) = a0 + a1x + · · · + anxn
and
q(x) = b0 + b1x + · · · + bnxn where for 0 ≤i ≤n, ai and bi are real numbers. (By using
zero coeﬃcients as necessary we may list the same terms for both p(x) and q(x).) Thus
p(x) + q(x) = (a0 + b0) + (a1 + b1)x + · · · + (an + bn)xn
and for any scalar c, cp(x) =
ca0 + ca1x + · · · + canxn. It is now straightforward to verify that P is a vector space.
33.
The proof that F(R) is a vector space requires checking all ten properties of Deﬁnition 1.
We illustrate by verifying properties (a2), (a3) and (m1). If f(x), g(x), h(x) are in F(R)
then [f + (g + h)](x) = f(x) + [g + h](x) = f(x) + [g(x) + h(x)] = [f(x) + g(x)] + h(x) =
[f + g](x) + h(x) = [(f + g)) + h](x). Thus property (a2) holds. Deﬁne θ : R →R
by
θ(x) = 0 for all x in R. If f(x) is in F(R) then (f +θ)(x) = f(x)+θ(x) = f(x); f +θ = f
so θ is the zero of F(R). Therefore property (a3) is satisﬁed. To check (m1) let a and
b be scalars and suppose f(x) is in F(R). Then [a(bf)](x) = a[(bf)(x)] = a[b(f(x))] =
ab(f(x)) = [(ab)f](x).

5.3. SUBSPACES
139
34.
(a)
We will show that the given operations satisfy (a3) and (a4) of Deﬁnition 1. Note
that z = [−1, 1]T is in V
and u +z = u
for every element u
in V.
Thus z
is the
zero of V. If u= [u1, u2]T then w = [−u1 −2, −u2 + 2] is in V
and u+w = z. Thus
w
is an additive inverse for u .
(b)
Note that 2(e1 +e2 ) = [4, 0]T whereas 2e1 +2e2 = [3, 1]T. Simil-
arly, e1 +e1 = [3, −1]T whereas (1 + 1)e1 = 2e1 = [2, 0]T.
35.
To check (m2) as an illustration, note that a(u + v ) = θ = θ+ θ = au +av
for any u , v
in V
and scalar a. If u ̸= θ then 1u = θ and (m4) fails.
36.
The zero of V
is the vector z= [0, 1]T. If u= [u1, u2] is in V
then w = [−u1, 1/u2] is in
V
and u +w = z . Thus w
is the inverse of u . To show that (m2) holds let u = [u1, u2]T
and v = [v1, v2]T be in V
and let a be a scalar. Then
a(u + v) = a
· u1 + v1
u2v2
¸
=
· a(u1 + v1)
(u2v2)a
¸
=
· au1 + av1
ua
2va
2
¸
=
· au1
ua
2
¸
+
· av1
va
1
¸
= au + av.
5.3
Subspaces
1. W
is not a subspace of V.
None of the properties (s1), (s2), and (s3) of Theorem 2 is
satisﬁed. For example, if A =
· 1
0
0
0
0
0
¸
and B =
· 0
0
1
0
0
0
¸
then A and B are in
W
but A + B is not in W.
2. W
is a subspace of V.
3. W is a subspace of V. Clearly the (2 x 3) zero matrix is in W. Let A = [aij] and B = [bij]
be in W. Therefore a11 −a12 = 0, a12 + a13 = 0, a23 = 0, b11 −b12 = 0, b12 + b13 = 0, b23 = 0.
Now A + B is the (2 x 3) matrix A + B = [cij], where cij = aij + bij. Thus c11 −c12 =
(a11 + b11) −(a12 + b12) = (a11 −a12) + (b11 −b12) = 0.Similarly, c12 + c13 = 0 and c23 = 0.
This shows that A + B is in W. If k is a scalar then kA is the (2 x 3) matrix kA = [dij]
where dij = kaij. Consequently d11 −d12 = ka11 −ka12 = k(a11 −a12) = k0 = 0. Likewise
d12 + d13 = 0 and d23 = 0. Therefore kA is in W. It follows from Theorem 2 that W
is
a subspace of V.
4. W is not a subspace of V. For Example if A =
· 1
0
0
0
0
0
¸
and B =
· 0
1
1
0
0
0
¸
then A
and B are in W
but A + B is not in W. Note that properties (s1) and (s3) of Theorem
2 are satisﬁed.

140
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
5. W
is a subspace of P.
If θ(x) = 0 + 0x + 0x2
is the zero polynomial then clearly
θ(0)+θ(2) = 0, so θ(x) is in W. Suppose g(x) and h(x) are in W˚; that is g(0)+g(2) = 0
and h(0) + h(2) = 0.
Then (g + h)(0) = +(g + h)(2) = [g(0) + h(0)] + [g(2) + h(2)] =
[g(0) + g(2)] + [h(0) + h(2)] = 0 + 0 = 0 and it follows that g(x) + h(x) is in W. If c is a
scalar then (cg)(0) + (cg)(2) = c[g(0) + g(2)] = 0 and hence (cg)(x) is in W. By Theorem
2, W
is a subspace of P.
6. W
is a subspace of P.
7. W
is not a subspace of P.
For example if p(x) = x2 + x −2 and q(x) = x2 −9 then
p(x) and q(x) are in W but p(x) + q(x) = 2x2 + x −11 is not in W. Note that properties
(s1) and s(3) of Theorem 2 are satisﬁed.
8. W
is a subspace of P.
9. F is a subspace of C[−1, 1].
First recall that the zero of C[−1, 1] is the function θ deﬁned
by θ(x) = 0 for −1 ≤x ≤1. Since θ(−1) = 0 = −θ(1), θ(x) is in F.
Now assume that
g(x) and h(x) are in F. Thus g(−1) = −g(1) and h(−1) = −h(1). It follows that
(g + h)(−1) = g(−1) + h(−1) = −g(1) −h(1) = −(g + h)(1).
Therefore (g + h)(x) is in F. If c is a scalar then (cg)(−1) =
c(g(−1)) = c(−g(1)) = −(cg)(1)
so (cg)(x) is in F. by Theorem 2, F
is a subspace of
C[−1, 1].
10. F is not a subspace of C[−1, 1].
If f(x) is a nonzero function in F and c < 0 then cf(x)
is not in F
.
Note that properties (s1) and (s2) of Theorem 2 are satisﬁed.
11. F
is not a subspace of C[−1, 1]. None of the properties (s1), s(2), (s3) of Theorem 2 is
satisﬁed. For example if g(x) and h(x) are in F
then (g + h)(−1) = g(−1) + h(−1) =
−2 + (−2) = −4 so (g + h)(x) is not in F
.
12. F
is a subspace of C[−1, 1].
13. F
is a subspace of C2[−1, 1].
If θ(x) is the zero function then θ
′′(x) = θ(x) = 0
for
−1 ≤x ≤1. In particular θ
′′(0) = 0 so θ(x) is in F. Let g(x) and h(x) be in F; that
is g
′′(0) = 0 = h
′′(0). Therefore (g + h)
′′(0) = g
′′(0) + h
′′(0) = 0, so (g + h)(x) is in F
.
If c is a scalar then (cg)
′′(0) = cg
′′(0) = 0 and (cg)(x) is in F .
By Theorem 2, F is a
subspace of C2[−1, 1].
14. F
is a subspace of C2[−1, −].
15. F
is not a subspace of C2[−1, 1]. None of the properties (s1), (s2), s(3) of Theorem 2 is
satisﬁed. For example suppose g(x) and h(x) are in F
. Then g
′′(x) + g(x) = sin x and
h
′′(x)+h(x) = sin x for −1 ≤x ≤1. But (g+h)
′′(x)+(g+h)(x) = 2 sin x for −1 ≤x ≤1.
Therefore (g + h)(x) is not in F
.

5.3. SUBSPACES
141
16. F
is a subspace of C2[−1, 1].
17. Note that c1p1(x) + c2p2(x) + c3p3(x) = (c1 + 2c2 + 3c3) + (2c1 + 5c2 + 8c3)x + (c1 −2c3)x2.
Therefore c1p1(x) + c2p2(x) + c3p3(x) = −1 −3x + 3x2
requires that c1 + 2c2 + 3c3 =
−1, 2c1 + 5c2 + 8c3 = −3, and c1 −2c3 = 3. Solving we obtain c1 = −1, c2 = 3, c3 = −2
and it is easily veriﬁed that p(x) = −p1(x) + 3p2(x) −2p3(x).
18. p(x) = P4
i=1 cipi(x) if and only if c1 = −1 −2c3, c2 = 2 + 3c3, c3 is arbitrary, and c4 = −3.
For example p(x) = −p1(x) + 2p2(x) −3p4(x).
19.
From the matrix equation A = P4
i=1 ciBi we obtain the system of equations
c1
+
c2
−
c3
+
c4
=
−2
c2
−
3c3
+
2c4
=
−4
2c1
+
4c3
−
c4
=
1
c1
+
2c2
−
4c3
+
c4
=
0
.
The solution is c1 = −1 −2c3, c2 = 2 + 3c3, c3 arbitrary, and c4 = −3. Taking c3 = 0 we
see that A = −B1 + 2B2 −3B4.
20. ex = sinh x + cosh x.
21. cos 2x = (−1) sin2 x + (1) cos2 x.
22.
½ · 1
1
0
−1
¸
,
· 0
0
1
0
¸ ¾
.
23.
Let p(x) = a0+a1x+a2x2+a3x3 be in W. The constraints p(1) = p(−1) and p(2) = p(−2)
imply that a0+a1+a2+a3 = a0−a1+a2−a3 and a0+2a1+4a2+8a3 = a0−2a1+4a2−8a3.
This forces a1 = a3 = 0 while a0 and a2 are arbitrary. Thus {1, x2} is a spanning set for
W.
24. {−3 + 2x + x2, 2 −3x + x3}.
25. For Exercise 2, a matrix A = [aij]
is in W
if and only if A
has the form A =
· a12 −2a13
a12
a13
a21
a22
a23
¸
, where a12, a13, a21, a22, a23 are arbitrary. Therefore
W = Sp
½ · 1
1
0
0
0
0
¸
,
· −2
0
1
0
0
0
¸
,
· 0
0
0
1
0
0
¸
,
· 0
0
0
0
1
0
¸
,
· 0
0
0
0
0
1
¸ ¾
.

142
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
For Exercise 3, A = [aij] is in W
if and only if A =
· a11
a11
−a11
a21
a22
0
¸
, where
a11, a21, and a22 are arbitrary.
Therefore
W = Sp
½ · 1
1
−1
0
0
0
¸
,
· 0
0
0
1
0
0
¸
,
· 0
0
0
0
1
0
¸ ¾
.
For Exercise 5 let p(x) = a0 + a1x + a2x2. The condition p(0) + p(2) = 0 implies that
2a0 + 2a1 + 4a2 = 0. Therefore p(x) = (−a1 −2a2) + a1x + a2x2 where a1 and a2 are
arbitrary. It follows that W = Sp{−1 + x, −2 + x2}.
For Exercise 6, W = Sp{ 1, −4x + x2}.
For Exercise 8, W = Sp{ x, 1 −x2}.
26.
That W
is a subspace follows from Theorem 2 and from the properties of the transpose
given in Theorem 10 of Section 1.6.
W = Sp





1
0
0
0
0
0
0
0
0

,


0
0
0
0
1
0
0
0
0

,


0
0
0
0
0
0
0
0
1




,


0
1
0
1
0
0
0
0
0

,


0
0
1
0
0
0
1
0
0

,


0
0
0
0
0
1
0
1
0




.
27.
It is straightforward to show that tr(A + B) = tr(A) + tr(B) and tr(cA) = ctr(A). It
then follows easily from Theorem 2 that W
is a subspace of V. If A is in W
then A
has the form A =


−a22 −a33
a12
a13
a21
a22
a23
a31
a32
a33

.
It follows that W = Sp{B1, B2, E12, E13, E21, E23, E31, E32}
where B1 =


−1
0
0
0
1
0
0
0
0


and B2 =


−1
0
0
0
0
0
0
0
1

.
28. BT = [(1/2)(A + AT)]T = (1/2)(AT + ATT) = (1/2)(A + AT) = B, so B
is symmetric.
Similarly CT = [(1/2)(A −AT)]T =
(1/2)(AT −A) = −C and C is skew-symmetric.
29.
For any (n x n) matrix A, A = B + C where B and C are the matrices given in Exercise
28.

5.3. SUBSPACES
143
30.
The vector space of all (3 x 3) matrices is spanned by the set
{E11, E22, E23, A1, A2, A3, B1, B2, B3} where A1 =


0
1
0
1
0
0
0
0
0

,
A2 =


0
0
1
0
0
0
1
0
0

, A3 =


0
0
0
0
0
1
0
1
0

, B1 =


0
1
0
−1
0
0
0
0
0

,
B2 =


0
0
1
0
0
1
−1
0
0

, B3 =


0
0
0
0
0
1
0
−1
0

.
If A = [aij]
is a (3 x 3) matrix then A =
a11E11 + a22E22 + a33E33 + (1/2)(a12 + a21)A1 + (1/2)(a13 + a31)A2 + (1/2)(a23 + a32)A3 +
(1/2)(a12 −a21)B1 + (1/2)(a13 −a31)B2 + (1/2)(a23 −a32)B3.
31.
(a) A is in W if and only if A
has the form A =


0
a12
a13
0
0
a23
0
0
0

,
where a12, a13, a23
are arbitrary. Thus W = Sp{E12, E13, E23}.
(b) A is in W
if and only if A has form A =


−a22 −a33
−a23
a13
0
a22
a23
0
0
a33

. Therefore
W = Sp





−1
0
0
0
1
0
0
0
0

,


−1
0
0
0
0
0
0
0
1

,


0
−1
0
0
0
1
0
0
0

,


0
0
1
0
0
0
0
0
0




.
(c) A
is in W
if and only if A
has the form A =


a11
a11
a13
0
a22
a13
0
0
a22

..
Therefore
W = Sp





1
1
0
0
0
0
0
0
0

,


0
0
1
0
0
1
0
0
0

,


0
0
0
0
1
0
0
0
1




.
(d) A is in W
if and only if A has the form A =


a11
−a23
a13
0
a11
a23
0
0
a11

.
Therefore W = Sp





1
0
0
0
1
0
0
0
1

,

144
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS


0
−1
0
0
0
1
0
0
0

,


0
0
1
0
0
0
0
0
0




.
32. p(x) = (a0 −a1 + a2)(1) + (a1 −2a2)(x + 1) + a2(x + 1)2. In particular q(x) = 4(1) −5(x +
1) + 2(x + 1)2 and r(x) = 6(1) −5(x + 1) + (x + 1)2.
33.
The equation A = P4
i=1 xiBi implies that
x1
+
2x2
−
x3
+
x4
=
a
x2
+
3x3
+
x4
=
b
x1
+
x2
−
3x3
−
2x4
=
c
−2x1
−
2x2
+
6x3
+
5x4
=
d
.
Solving we obtain x1 = −6a+5b+37c+15d, x2 = 3a−2b−17c−7d, x3 = −a+b+5c+2d, x4 =
2c + d.
Therefore C = −12B1 + 6B2 −B3 −B4 and D = 8B1 −3B2 + B3 + B4.
5.4
Linear Independence, Bases, and Coordinates
1.
If A is in W
then A =
· −b −c −d
b
c
d
¸
= b
· −1
1
0
0
¸
+
c
· −1
0
1
0
¸
+ d
· −1
0
0
1
¸
.
The set
½ · −1
1
0
0
¸
,
· −1
0
1
0
¸
,
· −1
0
0
1
¸ ¾
is a basis for W.
2.
The set
½ · −1
2
−3
1
¸ ¾
is a basis for W.
3.
The set {E12, E21, E22}
is a basis for W.
4.
The set
½ · 1
1
0
2
¸
,
· 0
−1
1
1
¸ ¾
is a basis for W.
5.
For p(x) in W, p(x) = a0 + a1x + (a0 −2a1)x2 = a0(1 + x2)+
a1(x −2x2). The set {1 + x2, x −2x2} is a basis for W.
6.
The set {3 −x + x2} is a basis for W.
7.
The set {x, x2} is a basis for W.
8.
The set {1 −2x + x2} is a basis for W.

5.4. LINEAR INDEPENDENCE, BASES, AND COORDINATES
145
9.
Let p(x) = P4
i=0 aixi be in V. The given constraints are as follows:
p(0) = 0 :
a0 = 0
p
′(1) = 0 :
a1 + 2a2 + 3a3 + 4a4 = 0
p
′′(−1) = 0 :
2a2 −6a3 + 12a4 = 0
.
Solving yields a0 = 0, a1 = −9a3 + 8a4, a2 = 3a3 −6a4, a3
and a4
arbitrary.
Thus
{−9x + 3x2 + x3, 8x −6x2 + x4} is a basis for V.
10.
The set
½ · 1
0
0
0
¸
,
· 0
0
0
1
¸
,
· 0
1
1
0
¸ ¾
is a basis for the subspace of (2 x 2)
symmetric matrices.
11.
If A = [aij] is a (2 x 2) matrix then A = a11E11 + a12E12 + a21E21 + a22E22 so B spans
V. It is easy to see that B is a linearly independent set, so B is a basis for W.
12.
(a) [1, −1, 1]T, (b) [−1, 4, 1]T (c) [5, 2, 0]T.
13.
(a) [2, −1, 3, 2]T (b) [1, 0, −1, 1]T (c) [2, 3, 0, 0]T.
14.
Set p(x) = a0 + a1x + · · · + anxn
and assume p(x) = θ(x).
Then p(n)(x) = θ(x)
so
n!an = 0. It follows that an = 0. Suppose we have seen that am+1 = · · · = an = 0 where
0 ≤m < n. Then p(m)(x) = θ(x) so m!am = 0.
Thus am = 0. By continuing the process
we see that ai = 0, 0 ≤i ≤n.
Therefore the set {1, x, . . . , xn} is linearly independent.
15.
The given matrices have coordinate vectors u1 = [2, 1, 2, 1]T,
u2 = [3, 0, 0, 2]T, u3 = [1, 1, 2, 1]T, respectively. The equation x1u1 +x2u2 +x3u3 = θ has
only the trivial solution so {u1, u2, u3} is a linearly independent subset of R4. By property
(2) of Theorem 5, the set {A1, A2, A3} is a linearly independent subset of the vector space
of (2 x 2) matrices.
16.
The set is linearly dependent. For example −2A1 −A2 + A3 = O.
17.
The given matrices have coordinate vectors u1 = [2, 2, 1, 3]T,
u2 = [1, 4, 0, 5]T, u3 = [4, 10, 1, 13]T, respectively. The set
{u1 , u2 , u3 } is linearly dependent in R4. For example −u1 −2u2 +u3 = θ. It follows that
{A1, A2, A3} is linearly dependent; indeed −A1 −2A2 + A3 = O.
18.
The set is linearly independent.
19.
The polynomials p1(x), p2(x), p3(x), have coordinate vectors
u1 = [−1, 2, 1, 0]T, u2 = [2, −5, 1, 0]T, u3 = [0, −1, 3, 0]T,
respectively.
The set {u1 , u2
, u3 }
is linearly dependent; for example −2u1 −u2 +u3 = θ.
It follows that the set
{p1(x), p2(x), p3(x)} is linearly dependent; indeed −2p1(x) −p2(x) + p3(x) = 0.

146
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
20.
The set is linearly dependent; for example −p1(x) −p2(x) −p3(x) + p4(x) = 0.
21.
The given polynomials have coordinate vectors u1 = [1, 0, 0, 1]T,
u2 = [1, 0, 1, 0]T, u3 = [1, 1, 0, 0]T, u4 = [1, 0, 0, 0]T, respectively.
Since {u1, u2, u3, u4} is a linearly independent subset of R4, the given set of polynomials
is a linearly independent subset of P.
22.
A basis for Sp(S)
is {1 + 2x + x2, x −2x2}.
23.
The given polynomials have coordinate vectors u1 = [1, 2, 1]T,
u2 = [2, 5, 0]T, u3 = [3, 7, 1]T, u4 = [1, 1, 3]T,
respectively. In the equation x1u1 +x2u2
+x3u3 +x4u4 = θ, x3 and x4 are arbitrary. It follows that {u1 , u2 } is a basis for Sp{u1
, u2 , u3 , u4 }. Therefore {p1(x), p2(x)} is a basis for Sp (S).
24. The set
½ ·
1
2
−1
3
¸
,
· 0
1
0
0
¸
,
· 0
0
0
1
¸ ¾
is a basis for Sp (S).
25.
The given matrices have coordinate vectors u1 = [1, 2, −1, 3]T, u2 = [−2, 1, 2, −1]T, u3
= [−1, −1, 1, −3]T, u4 = [−2, 2, 2, 0]T,
respectively. In the equation x1u1 +x2u2 +x3u3
+x4u4 = θ, x4 is arbitrary so {u1 , u2 , u3 } is a basis for Sp{u1 , u2 , u3 , u4 }. It follows
that {A1, A2, A3} is a basis for Sp (S).
26. The coordinate vectors u1 = [−1, 1, 2]T, u2 = [0, 1, 3]T, u3 =
[1, 2, 8]T form a linearly independent set in R3. Since dim(R3) = 3 the set {u1 , u2 , u3 }
is a basis for R3. By the Corollary to Theorem 5, Q is a basis for P.
27. p(x) = −4p1(x) + 11p2(x) −3p3(x) so [p(x)]Q = [−4, 11, −3]T.
28. [p(x)]Q = [−2a0 −3a1 + a2, 4a0 + 10a1 −3a2, −a0 −3a1 + a2]T.
29.
The coordinate vectors for the given matrices are u1 = [1, 0, 0, 0]T,
u2 = [1, −1, 0, 0]T, u3 = [0, 2, 1, 0]T, u4 = [−3, 0, 2, 1]T. It is easily veriﬁed that {u1 , u2 , u3
, u4 } is a basis for R4. It follows from the Corollary to Theorem 5 that Q is a basis for V.
30. [A]Q = [9, −5, −1, −1]T.
31. A = (a + b −2c + 7d)A1 + (−b + 2c −4d)A2 + (c −2d)A3 + dA4 so [A]Q = [a + b −2c +
7d, −b + 2c −4d, c −2d, d]T.
32.
The suggested constraints yield the following system of equations:
p(−1)
=
0 :
a0 −a1 + a2
=
0
p(0)
=
0 :
a0
=
0
p(1)
=
0 :
a0 + a1 + a2
=
0
Solving we obtain a0 = a1 = a2 = 0 as the unique solution.

5.5. DIMENSION
147
33.
Let f(x) = c1 sin x + c2 cos x
and suppose f(x) = θ(x).
Then f(0) = c2 = 0
and
f(π/2) = c1 = 0.
It follows that {sin x, cos x} is a linearly independent set.
34.
The conditions h(0) = h
′(0) = h
′′(0) = h
′′′(0) = 0 yield the system of equations
c1
+
c2
+
c3
+
c4
=
0
c1
+
2c2
+
3c3
+
4c4
=
0
c1
+
4c2
+
9c3
+
16c4
=
0
c1
+
8c2
+
27c3
+
64c4
=
0
The system has only the trivial solution so B
is a linearly independent set.
Since
V = Sp(B) by deﬁnition, B is a basis for V.
35.
Note that [g1(x)]B = [1, 0, 0, −1]T = u1 , [g2(x)]B = [0, 1, 1, 0]T = u2 ,
and [g3(x)]B =
[−1, 0, 1, 1]T = u3. It is easy to verify that {u1, u2, u3} is a linearly independent subset
of R4. By Theorem 5 the set {g1(x), g2(x), g3(x)} is linearly independent in V.
36.
It follows from the note that w ̸= θ. Suppose that a1v1 + · · · + amvm +bw = θ. If b ̸= 0
then we can solve for w , contradicting the assumption that w
is not in Sp(Q). Thus
b = 0.
This leaves a1v1 + · · · + amvm = θ.
But the set Q
is linearly independent so
a1 = · · · = am = 0. This proves that Q ∪{w} is linearly independent.
37.
Suppose a1v1 + · · · + anvn = θ, where ai ̸= 0. Then vi = b1v1 + · · · + bi−1vi−1 +bi+1vi+1
+ · · · + bnvn , where bj = −aj/ai, for 1 ≤j ≤n, j ̸= i.
38.
A set of two vectors is linearly dependent if and only if one of the vectors is a scalar
multiple of the other. The sets given in (a) and (b) are linearly independent whereas the
sets given in (c), (d), and (e) are linearly dependent.
5.5
Dimension
1.
(a)
We show that V1 is a subspace. The proof that V2 is a subspace is similar. Clearly
the (3 x 3) zero matrix is lower-triangular so it is in V1. Now let A and B be in V1,
A =


a11
0
0
a21
a22
0
a31
a32
a33

and B =


b11
0
0
b21
b22
0
b31
b32
b33

.
Then A + B =


a11 + b11
0
0
a21 + b21
a22 + b22
0
a31 + b31
a32 + b32
a33 + b33

,
so A + B is in V1. If c is any scalar then

148
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
cA =


ca11
0
0
ca21
ca22
0
ca31
ca32
ca33

,
so cA is in V1. By Theorem 2 of Section 4.3, V
is a subspace of V1.
(b) The set {E11, E21, E22, E31, E32, E33} is a basis for V1 and
{E11, E12, E13, E22, E23, E33} is a basis for V2.
(c) dim(V ) = 9; dim(V1) = 6; dim(V2) = 6.
2.
Since V1 and V2 are subspaces of V , θ is in V1 ∩V2. Suppose u
and v
are vectors in
V1 ∩V2. Then u
and v
are in V1 and V1 is a vector space. Therefore u + v
is in V1
and for any scalar a, au
is in V1. Similarly,
u + v
and au
are in V2. This shows that u + v
and au
are in V1 ∩V2 so by Theorem
2 of Section 4.3, V1 ∩V2 is a subspace of V.
Set V1 = {[a, 0]T : a any real number } and let V2 = {[0, b]T : b any real number }. Then
V1 and V2 are subspaces of R2 but V1 ∪V2 is not a subspace. For example u = [1, 0]T
and v = [0, 1]T are in V1 ∪V2 but u + v
is not in V1 ∪V2.
3.
Let A = [aij] be in V1 ∩V2. Since A is in V1, aij = 0 for i < j. Likewise A is in V2
so aij = 0 for i > j. Thus aij = 0 if i ̸= j
and V1 ∩V2
is the set of (3 x 3) diagonal
matrices. dim(V1 ∩V2) = 3.
4. dim(W) = 6.
5.
The set





0
1
0
−1
0
0
0
0
0

,


0
0
1
0
0
0
−1
0
0

,


0
0
0
0
0
1
0
−1
0





is a basis for W
so
dim(W) = 3.
6. dim(W) = 2.
7. p(x) = P4
i=0 aixi is in W
if and only if a0 = 4a4, a2 = −5a4,
a1, a3, a4 arbitrary. A basis for W
is {x, x3, 4 −5x2 + x4} so
dim(W) = 3.
8.
The set S does not span V
by property (1) of Theorem 9.
9. S
contains 4 elements and dim(P2) = 3. By property (1) of Theorem 8, S
is linearly
dependent.
10.
The set S is a basis for V
by property (2) of Theorem 8.

5.5. DIMENSION
149
11. S contains only two vectors and dim(V ) = 4. By property (1) of Theorem 9, S does not
span V.
12.
The set S is a basis for V
by property (2) of Theorem 8.
13. The set S contains 5 elements whereas dim(V ) = 4. By property (1) of Theorem 8, S is a
linearly dependent set.
14. dim(W) = 2.
15.
(a)
First note that since V
is not a subset of an already familiar vector space, we must
check all the properties of Deﬁnition 1 given in Section 4.2. The closure properties
(c1) and (c2) are evident. As illustrations we will check (a2), (a3), and (m2).
Let x= {xi}∞
i=1, y = {yi}∞
i=1, and z= {zi}∞
i=1 be in V. Then x+(y +z) = {xi + (yi +
zi)}∞
i=1 = {(xi + yi) + zi}∞
i=1 = (x +y )+
z . Therefore (a2) is satisﬁed. If θ = {θi}∞
i=1, where θi = 0 for each i then θ is the
zero for V
and property (a3) holds. To check (m2) let a be a scalar. Then a(x +y
) = {a(xi + yi)}∞
i=1 = {axi + ayi}∞
i=1 = {axi}∞
i=1 + {ayi}∞
i=1 = ax +ay .
(b)
If x = a1s1 + · · · + ansn
then x
is the sequence {xi}∞
i=1 where xi = ai, 1 ≤i ≤n,
and xi = 0
for i > n. In particular if θ = a1s1 + · · · + ansn (where θ
is the zero
sequence described in (a)) then a1 = · · · = an = 0. It follows that {s1 , s2 , . . . , sn } is
a linearly independent subset of V. Since n is arbitrary, it follows from property (1)
of Theorem 8 that V
has inﬁnite dimension.
16.
Apply property (1) of Theorem 8.
17.
Suppose dim(V ) = n and let w1
be in W, w1 ̸= θ. If {w1} spans W then it is a basis.
If not then by Exercise 36, Section 4.4, there is a vector w2
in W such that {w1 , w2 } is
a linearly independent set. In general suppose we have constructed a linearly independent
subset Sk = {w1 , w2 , . . . , wk } of W. If W = Sp(Sk) then Sk is a basis and we are done.
If Sk
does not span W
then, by Exercise 36, Section 4.4, there is a vector wk+1 in W
such that Sk+1 = {w1 , . . . , wk ,
wk+1 } is linearly independent. This process must stop since, by property (1) of Theorem
8, any set of n+1 vectors in V
is linearly dependent. Therefore there exists an integer m,
1 ≤m ≤n,
and a linearly independent subset Sm = {w1 , . . . , wm }
of W
such that
W = Sp(Sm). Thus Sm is a basis for W
and dim(W) = m ≤n.
18.
The set T = {[u1 ]B, . . . , [uk ]B} contains k vectors in Rp, where k ≥p + 1. Thus T
is a linearly dependent subset of Rp. By Theorem 5, {u1 , . . . uk } is a linearly dependent
subset of V.
19.
Since B is a linearly independent subset of V
containing p vectors and Q is a basis of
V
containing m vectors, Theorem 6 implies that p ≤m. Reversing the roles of B and Q
gives m ≤p. Therefore

150
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
m = p.
20.
Property (1) of Theorem 8 is a direct consequence of Theorem 6. To prove property (2)
let B be a basis for V and let S = {u1, . . . , up} be a linearly independent subset of V. By
property (2) of Theorem 5 the set T = {[u1]B, . . . , [up]B} is a linearly independent subset
of Rp. Thus T
is a basis for Rp. If v is in V
then [v ]B is in Sp(T). By property 1 of
Theorem 5, v
is in Sp(S). It follows that S spans V, so S is a basis for V.
21.
Note that [w ]B = [d1, . . . , dn]T and [w ]C = [c1, . . . , cn]T. Thus
A[w ]C = c1[u1 ]B + · · · + cn[un ]B = [c1u1 + · · · + cnun ]B = [w ]B.
(a) A =


1
−1
1
0
1
−2
0
0
1

.
(b) [p(x)]C = [8, 4, 1]T and [p(x)]B = A[p(x)]C = [5, 2, 1]T.
22.
(a) p(x) = −4+(x+1)+(x+1)2. (b) p(x) = 15−9(x+1)+2(x+1)2. (c) p(x) = 4−(x+1)2.
(d) p(x) = −10 + (x + 1).
23. A−1 =


1
1
1
0
1
2
0
0
1

.
(a) p(x) = 6 + 11x + 7x2. (b) p(x) = 4 + 2x −x2.
(c) p(x) = 5 + x.
(d) p(x) = 8 −2x −x2.
24. A =


1
0
0
0
0
1
1
1
0
0
1
3
0
0
0
1

.
(a) p(x) = −9 + 4x + x(x −1) + x(x −1)(x −2).
(b) p(x) = −2 + 8x + x(x −1).
(c) p(x) = 1 + x + 3x(x −1) + x(x −1)(x −2).
(d) p(x) = 3 + 5x + 5x(x −1) + x(x −1)(x −2).
5.6
Inner-products
1.
(1) <x, x> = 4x2
1 + x2
2 ≥0 and <x, x> = 0 if and only if x1 = x2 = 0.
(2) <x, y> = 4x1y1 + x2y2 = 4y1x1 + y2x2 =<y, x> .
(3) <ax, y> = 4ax1y1 + ax2y2 = a(4x1y1 + x2y2) = a <x, y> .
(4) Let z = [z1, z2]T. Then < x, y + z > = 4x1(y1 + z1) + x2(y2 + z2) = (4x1y1 + x2y2) +
(4x1z1 + x2z2) =<x, y> + <x, z> .

5.6. INNER-PRODUCTS
151
2. (i) <x, x> = Pn
i=1 aix2
i ≥0 with equality if and only if xi = 0 for each i.
(2) <x, y> = Pn
i=1 aixiyi = Pn
i=1 aiyixi =<y, x> .
(3) <ax, y> = Pn
i=1 aiaxiyi = a Pn
i=1 aixiyi = a <x, y> .
(4) <x, y + z> = Pn
i=1 aixi(yi + zi) = Pn
i=1 aixiyi + Pn
i=1 aixizi =
<x, y> + <x, z> .
3.
(1) is immediate since A is positive deﬁnite.
(2) <x, y> = xTAy = (xTAy)T = yTATxTT =
yTAx =<y, x> .
(3) <ax, y> = (ax )TAy = a[xTAy ] = a <x, y> .
(4) <x, y + z> = xTA(y + z ) = xTAy + xTAz =<x, y> +
<x, z> .
4. xTAx = x2
1 + 2x1x2 + 2x2
2 = (x1 + x2)2 + x2
2.
5.
(1) <p, p>= a2
0 + a2
1 + a2
2 ≥0 with equality if and only if ai = 0 for 0 ≤i ≤2.
(2) <p, q>= a0b0 + a1b1 + a2b2 = b0a0 + b1a1 + b2a2 =<q, p> .
(3) <ap, q>= aa0b0 + aa1b1 + aa2b2 = a(a0b0 + a1b1 + a2b2) =
a <p, q> .
(4) Let r(x) = c0 + c1x + c2x2. Then <p, q + r>= a0(b0 + c0) + a1(b1 + c1) + a2(b2 + c2) =
(a0b0 + a1b1 + a2b2) + (a0c0 + a1c1 + a2c2) =
<p, q> + <p, r> .
6.
Clearly < p, p >= p(0)2 + p(1)2 + p(2)2 ≥0.
Suppose 0 =< p, p >= a2
0 + (a0 + a1 +
a2)2 + (a0 + 2a1 + 4a2)2. Then 0 = a0 = a0 + a1 + a2 = a0 + 2a1 + 4a2. It follows that
a0 = a1 = a2 = 0. The remaining properties of Deﬁnition 7 are straightforward to verify.
7.
(1) <A, A>= a2
11 + a2
12 + a2
21 + a2
22 ≥0 with equality if and only if A = O.
(2) <A, B >= a11b11+a12b12+a21b21+a22b22 = b11a11+b12a12+b21a21+b22a22 =<B, A> .
(3) <aA, B >= aa11b11 +aa12b12 +aa21b21 +aa22b22 = a(a11b11 +a12b12 +a21b21 +a22b22) =
a <A, B > .
(4) Let C = [cij]. Then < A, B + C >= a11(b11 + c11) + a12(b12 + c12) + a21(b21 + c21) +
a22(b22 + c22) = (a11b11 + a12b12 + a21b21 + a22b22) + (a11c11 + a12c12 + a21c21 + a22c22) =<
A, B > + <A, C > .
8. <x, y> = −2; ∥x∥= 2
√
2; ∥y∥= 1; ∥x −y∥=
√
13.

152
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
9. <x, y> = [1, −2]
· 1
1
1
2
¸ · 0
1
¸
= −3; ∥x∥2 =<x, x> =
xTAx = 5 so ∥x∥=
√
5; ∥y∥=
√
2; x −y = [1, −3]T
and
∥x −y∥2 = (x −y )TA(x −y ) = 13. Thus ∥x −y∥=
√
13.
10. <p, q>= −1; ∥p∥=
√
6; ∥q∥=
√
6; ∥p −q∥=
√
14.
11. < p, q >= (−1)1 + (2)2 + 7(7) = 52; ∥p ∥2 =< p, p >= (−1)2 + 22 + 72 = 54
so ∥p ∥=
3
√
6; ∥q ∥2=<q, q >= 12 + 22 + 72 = 54 so ∥q ∥= 3
√
6; ∥p −q ∥2=<p −q, p −q >= 22 so
∥p −q∥= 2.
12.
With the inner product deﬁned in Exercise 5, <1, x>=<1, x2 >=
<x, x2 >= 0 so {1, x, x2} is an orthogonal set. With the inner product deﬁned in Exercise
6, <1, x>= 3 so {1, x, x2} is not an orthogonal set.
13.
For <x, y> = xTy
the graph of S is the circle with equation x2 + y2 = 1. For <x, y>
= 4x1y1 + x2y2 the graph of S is the ellipse with equation 4x2 + y2 = 1.
14. u1 = [1, 0]T , u2 = [−1, 1]T .
15. v = a1u1 +a2u2
where a1 =<u1, v> / <u1, u1 > = 7 and
a2 =<u2, v> / <u2, u2 > = 4.
16. p0(x) = 1, p1(x) = x −1, p2(x) = x2 −2x + 1/3.
17. q(x) = a0p0(x) + a1p1(x) + a2p2(x) where a0 =<p0, q > / <p0, p0 >= −5/3, a1 =<p1, q >
/ <p1, p1 >= −5, and
a2 =<p2, q> / <p2, p2 >= −4.
18.
For every scalar c, if p(x) = 2cx −3cx2 + cx3 then <p, p>= 0.
19. p0(x) = 1, p1(x) = x −a0p0 where a0 =<x, p0 >/<p0, p0 >= 0.
Thus p1(x) = x. p2(x) = x2 −b0p0 −b1p1 where b0 =
<x2, p0 >/<p0, p0 > = 2 and b1 = <x2, p1 >/<p1, p1 > = 0. Thus p2(x) = x2 −2. p3(x) =
x3 −c0p0 −c1p1 −c2p2 where c0 =
<x3, p0 >/<p0, p0 > = 0, c1 =<x3, p1 >/<p1, p1 > = 17/5, and c2 =<x3, p2 >/<p2, p2 > =
0. Therefore p3(x) = x3 −(17/5)x.
p4(x) = x4 −d0p0 −d1p1 −d2p2 −d3p3 where d0 =
<x4, p0 > / <p0, p0 > = 34/5, d1 =<x4, p1 > / <p1, p1 >= 0, d2 =
< x4, p2 > / < p2, p2 > = 31/7, d3 = < x4, p3 > / < p3, p3 > = 0.
Therefore p4(x) =
x4 −(31/7)x2 + 72/35.

5.6. INNER-PRODUCTS
153
20. <v,θ>=<v,θ+θ>=<v,θ> + <v,θ> . Therefore <v,θ>= 0.
21.
By assumption ∥u∥2 =<u, u> = 0. Therefore u = θ.
22. ∥av∥= √<av, av> =
p
a2 <v, v> =|a| ∥v∥.
23.
Suppose a1v1 +a2v2 + · · · + akvk = θ. For each i, 1 ≤i ≤k, 0 =
<vi ,θ>=<vi , Pk
j=1 aj vj>= Pk
j=1 aj <vi, vj > =
ai <vi, vi > = ai ∥vi ∥2. Thus ai = 0 and the set is linearly independent.
24.
Suppose u = Pn
j=1 aj vj. Then <vi, u> =<vi , Pn
j=1 aj vj>=
Pn
j=1 aj <vi, vj > = ai <vi, vi > . Therefore ai =
<vi, u> / <vi, vi > .
25.
From Examples 4 and 5,p0(x) = 1, p1(x) = x −(1/2), p2(x) = x2 −x + (1/6), <p0, p0 >=
1, < p1, p1 >= 1/12, and < p2, p2 >= 1/180. Moreover < x3, p0 >= 1/4, < x3, p1 >= 3/40,
and <x3, p2 >= 1/120. By Theorem 13,p∗(x) = (1/4)p0(x)+
(9/10)p1(x) + (3/2)p2(x) = (3/2)x2 −(3/5)x + (1/20).
26. The required constants are < p0, x4 >= 1/5, < p1, x4 >= 1/15,
< p2, x4 >= 1/105, and < p3, x4 >= 1/1400. The remaining constants have already been
calculated in Examples 4, 5, and 7. If follows that p4(x) = x4−2x3+(9/7)x2−(2/7)x+1/70.
27.
With p3(x) as determined in Example 7 and with the calculations done in Example 6 we
obtain p∗(x) ≃0.841471p0(x)−
0.467544p1(x) −0.430920p2(x) + 0.07882p3(x).
28. T0(x) = 1, T1(x) = x, T2(x) = 2x2 −1, T3(x) = 4x3 −3x.
29.
(a)
Clearly T0(cos θ) = 1 = cos (0θ) and T1(cosθ) = cos θ =
cos (1θ). Suppose we have seen that Tk(cosθ) = cos(kθ) for
0 ≤k ≤n, where n ≥1. Then Tn+1(cos θ) = 2 cos θ Tn(cos θ)−
Tn−1(cos θ) = 2 cos θ cos (nθ) −cos (n −1)θ = cos (n + 1)θ
[ since cos (α + β) =
2 cos α cos β −cos (α −β)].
(b) <Ti, Tj >= (2/π)
R 1
−1[Ti(x)Tj(x)/
√
1 −x2]dx =
−(2/π)
R 0
π cos (iθ) cos (jθ)dθ = 0 if i ̸= j.
(c) T0(x) = 1
has degree zero and T1(x) = x
has degree one.
Suppose Tk(x)
has
degree k
for 0 ≤k ≤n,
where n ≥1.
Thus Tn(x) = anxn + · · · + a1x + a0
and Tn−1(x) = bn−1xn−1 + · · · + b1x + b0,
where an ̸= 0.
Using (R) we obtain
Tn+1(x) = 2anxn+1 + · · · + (2a0 −b1)x −b0. In particular, Tn+1(x) has degree n + 1.
It follows by induction that Tk(x) is a polynomial of degree k for each integer k ≥0.

154
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
(d) T2(x) = 2x2 −1, T3(x) = 4x3 −3x, T4(x) = 8x4 −8x2 + 1, T5(x) = 16x6 −20x3 + 5x.
30.
It follows from Exercise 29 that {T0(x), T1(x), . . . , Tn(x)} is an orthogonal basis for Pn.
Moreover < T0, T0 >= 2 whereas < Tj, Tj >= 1 for j ≥1. The given formula is now an
immediate consequence of Theorem 13.
32. By property (2) of Deﬁnition 7, < ei, ej >=< ej, ei > for unit vectors, e1, e2, . . . , en in R2.
But < ei, ej >= eT
i Aej = aij, whereas < ej, ei >= eT
j Aei = aji. It follows that aij = aji,
so A is symmetric. If x is a nonzero vector in Rn then xT Ax =< x, x >> 0 by property
(1) of Deﬁnition 7. Therefore, A is positive deﬁnite.
5.7
Linear Transformations
1.
Let A =
· 1
0
0
0
¸
and B =
· 0
0
0
1
¸
. Then T(A + B) =
det(A + B) = det(I) = 1 whereas T(A) + T(B) = det(A) + det(B) = 0. Therefore T
is
not a linear transformation.
2. T
is a linear transformation.
3. T
is a linear transformation. If A and B are (2 x 2) matrices it is straightforward to see
that tr(A + B) = tr(A) + tr(B); thus
T(A + B) = T(A) + T(B). Likewise if c is a scalar, tr(cA) =
c tr(A) so T(cA) = cT(A).
4. T
is not a linear transformation. For example if A =
· 1
0
0
0
¸
and B =
· 0
1
0
0
¸
then
T(A) = T(B) = 0 whereas T(A + B) = 1.
5.
Let f
and g
be in C[−1, 1]
and let c
be a scalar. Then T(f + g) = (f + g)′(0) =
f′(0) + g′(0) = T(f) + T(g), and T(cf) = (cf)′(0) =
cf′(0) = cT(f). Therefore T
is a linear transformation.
6. T
is a linear transformation.
7. T
is not a linear transformation. For example T(1 + θ(x)) = T(1) = 2 + x + x2 whereas
T(1) + T(θ(x)) = (2 + x + x2) + (1 + x + x2) = 3 + 2x + 2x2.
8. T
is a linear transformation.
9.
(a) T(p) = 3T(1) −2T(x) + 4T(x2) = 3(1 + x2) −2(x2 −x3)+
4(2 + x3) = 11 + x2 + 6x3.

5.7. LINEAR TRANSFORMATIONS
155
(b) T(a0+a1x+a2x2) = a0T(1)+a1T(x)+a2T(x2) = a0(1+x2)+a1(x2−x3)+a2(2+x3) =
(a0 + 2a2) + (a0 + a1)x2 + (−a1 + a2)x3.
10. p(x) = (−5)1 + 3(x + 1) + (x2 + 2x + 1) so T(p(x)) = −5x4+
3(x3 −2x) + x = −5x4 + 3x3 −5x. Similarly, q(x) = (−3)1+
7(x + 1) + (x2 + 2x + 1) so T(q(x)) = −3x4 + 7x3 −13x.
11.
(a) T(A) = −2T(E11) + 2T(E12) + 3T(E21) + 4T(E22) = 8 + 14x −9x2.
(b) T
µ· a
b
c
d
¸¶
= aT(E11) + bT(E12) + cT(E21) + dT(E22) =
(a + b + 2d) + (−a + b + 2c + d)x + (b −c −2d)x2.
12.
(a)
Let A = [aij] and B = [bij] be (2 x 2) matrices. Then
T(A + B) = T([aij + bij]) =
· (a11 + b11) + 2(a22 + b22)
(a12 + b12) −(a21 + b21)
¸
=
· a11 + 2a22
a12 −a21
¸
+
· b11 + 2b22
b12 −b21
¸
= T(A) + T(B).
If c is a scalar then T(cA) = T([caij]) =
· ca11 + ca22
ca12 −ca21
¸
= c
· a11 + a22
a12 −a21
¸
= cT(A).
Therefore T
is a linear transformation.
(b) N(T) =
½· a
b
c
d
¸
: a = −2d, b = c, c and d arbitrary
¾
.
(c)
½· −2
0
0
1
¸
,
· 0
1
1
0
¸¾
is a basis for N(T) .
(d) nullity (T) = 2 and rank (T) = 2.
(e)
is a subspace of R2 and 2 = dim(R2) = rank (T) = dim(R(T) ). Therefore R(T)
= R2.
(f) T(A) = v where A is any matrix of the form
A =
· x −2d
y + c
c
d
¸
, where c and d are arbitrary. For example, A =
· x
y
0
0
¸
is one choice for A.
13.
(a)
By property 1 of Theorem 15, R(T) =
Sp {T(1), T(x), T(x2), T(x3), T(x4)} =Sp{0, 0, 2, 6x, 12x2} =
Sp{2, 6x, 12x2}. It follows that rank (T) = 3. Since R(T) ⊆P2
and dim(P2) = 3, we have R(T) = P2.
(b)
By property 3 of Theorem 15, nullity (T) =
dim(P4) −rank (T) = 5 −3 = 2. Since nullity (T) > 0, T
is not one to one.

156
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
(c)
We wish to determine q(x) = b0 + b1x + b2x2 + b3x3 + b4x4
in P4
such that
a0 + a1x + a2x2 = T(q) = 2b2 + 6b3x + 12b4x2.
Equating coeﬃcients gives b2 = a0/2, b3 = a1/6, b4 = a2/12, b0
and b1 arbitrary. In particular,q(x) = (a0/2)x2+(a1/6)x3+(a2/12)x4
is one choice.
14. R(T)= Sp{1 −x + 2x2 + 3x3, −1 + 3x −3x2 −x3, 2 −2x + 5x2 + 7x3, −1 + 3x −x2 + 2x3, 1 −
x + x2 + 2x3}. Utilizing the spanning set we obtain the basis {1 −x + 2x2 + 3x3, 2x −x2 +
2x3, x2 + x3, x3} for R(T). In particular, rank (T) = 4. Thus nullity (T) = 1 and T
is
not one to one.
15. N(T) = {p(x) = a0 + a1x + a2x2 : a0 + 2a1 + 4a2 = 0}. It
follows that nullity (T) = 2. Consequently rank (T) = 1 and
R(T) = R1.
16. N(T)= {f in C[0, 1] :
R 1
0 f(t) dt = 0}. For any a in R1, T(2ax) =
R 1
0 2atdt = a, so R(T)
= R1.
17.
(a)
Let u, v be vectors in V
and let c be a scalar. Then
I(u + v) = u +v = I(u) + I(v) and I(cu ) = cu = cI(u).
Therefore I is a linear transformation.
(b)
The vector v is in N(I ) if and only if θ= I(v) = v. Thus N(I ) = {θ}. For each v
in V, I(v) = v so R(I ) = V .
18.
(a)
Let u1, u2 be in U and let a be a scalar. Then T(u1 + u2) =
θV = θV + θV = T(u1) + T(u2) and T(au1) = θV = a θV =
aT(u1 ). This proves that T
is a linear transformation.
(b) N(T) = U and R(T) = {θV }.
19.
Recall that 5 = dim(P4) = rank (T) + nullity (T). Moreover
R(T) ⊆P2 so rank (T) ≤3. The possibilities are:
rank (T)
3
2
1
0
nullity (T)
2
3
4
5.
Since nullity (T) ≥2, T
cannot be one to one.
20.
By property 3 of Theorem 15, dim(U) = rank (T) + nullity (T).
But R(T) ⊆V
so
rank (T) ≤dim(V ) < dim(U). It follows that
nullity (T) > 0 and, hence, T
cannot be one to one.

5.7. LINEAR TRANSFORMATIONS
157
21.
Recall that 3 = dim(R3) = rank (T) + nullity (T). Moreover
N(T) ⊆R3 so nullity (T) ≤3. The possibilities are:
rank (T)
3
2
1
0
nullity (T)
0
1
2
3.
Since dim(P3) = 4 and rank (T) < 4, R(T) = P3 is not a possibility.
22.
By property 3 of Theorem 15, dim(U) = rank (T) + nullity (T). In particular, rank (T) ≤
dim(U) < dim(V ) so R(T) = V
is not possible.
23.
It follows from property 1 of Theorem 14 that θV
is in R(T) . Sup-
pose that v1 and v2 are in R(T) ; thus there exist vectors u1 and
u2 in U such that T(u1) = v1 and T(u2) = v2. Therefore v1 +v2 =
T(u1 ) + T(u2 ) = T(u1 +u2 ) so v1 +v2
is in R(T) . If a is a scalar
then av1 = aT(u1 ) = T(au1 ), and av1
is in R(T) . This proves
that R(T)
is a subspace of V.
24.
Suppose T
is one to one and let u
be in N(T). It follows from property 1 of Theorem
14 that T(u) = θV = T(θU). Since T
is one to one, u = θU so N(T) = { θU}.
25.
(a)
If rank (T) = p then, in the notation of Theorem 15, C =
{T(u1 ), . . . , T(up )} is a basis for R(T) [ cf. property 2 of Theorem 9 in Section 4.5
]. In particular the set C is linearly independent. By property 2 of Theorem 15, T is
one to one so, by property 4 of Theorem 14, nullity (T) = 0.
(b)
If rank (T) = 0 then R(T) = {θV } and T is the zero linear transformation deﬁned
by T(u ) = θV
for all u
in U. Thus N(T) = U and nullity (T) = dim(U) = p.
26. T
is one to one if and only if N(T) = {θ}.
Thus T
is one to one if and only if Ax = θ
has only the trivial solution, that is, if and only if A is nonsingular.
27.
(a)
Let A
and B
be (2 x 2) matrices. Then T(A + B) = (A + B)T = AT + BT =
T(A) + T(B). If c is a scalar then T(cA) = (cA)T = cAT = cT(A). This proves that
T
is a linear transformation.
(b) If A is in N(T) then T(A) = AT = O. It follows that A = O and that nullity (T) = 0.
Therefore rank (T) = 4. Consequently T
is one to one and R(T) = V.
(c)
Let B be in V
and set C = BT. Then T(C) = CT = BTT = B. Therefore R(T)
= V.

158
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
5.8
Operations with Linear Transformations
1. (S + T)(p) = S(p) + T(p) = p′(0) + (x + 2)p(x). In particular, (S + T)(x) = 1 + (x + 2)x =
x2 + 2x + 1 and (S + T)(x2) = 0 + (x + 2)x2 = x3 + 2x2.
2. (2T)(p) = 2[T(p)] = 2(x + 2)p(x) = (2x + 4)p(x).
Therefore
(2T)(x) = 2x2 + 4x.
3. (H ◦T)(p) = H(T(p)) = H((x+2)p(x)) = [(x+2)p(x)]′+2p(0) = (x+2)p′(x)+p(x)+2p(0).
The domain for H ◦T
is P3 and
(H ◦T)(x) = 2x + 2.
4. (T ◦H)(p) = (x + 2)[p′(x) + p(0)]; T ◦H has domain P4; (T ◦H)(x) = x + 2.
5.
(a)
If p(x) = P3
i=0 aixi then T(p) = 2a0+(a0+2a1)x+(a1+2a2)x2+(a2+2a3)x3+a3x4.
In particular T(p) = θ(x) if and only if p(x) = θ(x). Therefore T is one to one. Now
rank (T) = dim(P3) −nullity (T) = 4.
Since R(T) ⊆P4
and dim(P4) = 5, R(T)
̸= P4; that is, T
is not onto.
(b)
It is easy to verify that T(p) = x is impossible. Therefore
T −1(x) is not deﬁned.
6.
(a) R(H ) = Sp {H(1), H(x), H(x2), H(x3), H(x4)} =
Sp{ 1, 1, 2x, 3x2, 4x3} = Sp{ 1, 2x, 3x2, 4x3}. It follows that
rank (H) = 4 and nullity (H) = 1. Therefore H is onto but not one to one.
(b)
Note that H((1/2)x2) = x = H((1/2)x2 + x −1). Therefore H−1(x) is not uniquely
determined.
7.
Let p(x) = aex + be2x + ce3x
be in V.
Then T(p(x)) = p′(x) = aex + 2be2x + 3ce3x.
Since B = {ex, e2x, e3x}
is a linearly independent set, it follows that T(p(x)) = θ(x)
if
and only if a = b = c = 0. Thus N(T) = {θ(x)} and T
is one to one. The set B
is a
basis for V
so dim(V ) = 3. Thus rank (T) = dim(V ) −nullity (T) = 3. It follows that
T
is onto. Therefore T
is invertible. Moreover T −1(ex) = ex, T −1(e2x) = (1/2)e2x, and
T −1(e3x) = (1/3)e3x. This implies that T −1(aex + be2x + ce3x) = aex + (b/2)e2x + (c/3)e3x.
8. T −1(sin x) = −cos x, T −1(cos x) = sin x, and T −1(e−x) = −e−x.
Therefore T −1(a sin x + b cos x + ce−x) = −a cos x + b sin x −ce−x.
9. If A is in N(T) then T(A) = AT = O. Therefore A = O, so N(T)= {O}. It follows that
T
is one to one. Further rank (T) = dim(V ) −nullity (T) = 4 so T
is onto. Therefore T
is invertible. In fact T −1 = T
since (AT )T = A.
10. T −1(A) = QAQ−1.
11.
(a)
Since dim(V ) = 4, V
is isomorphic to R4 by Theorem 17.

5.8. OPERATIONS WITH LINEAR TRANSFORMATIONS
159
(b)
Since dim(P3) = 4 = dim(V ), V and P3 are isomorphic by the corollary to Theorem
17.
(c)
It is easily shown that T : V →P3 deﬁned by T
µ· a
b
c
d
¸¶
= a + bx + cx2 + dx3
is an isomorphism.
12.
(a)
Note that dim(U) = 3.
(b) dim(U) = 3 = dim(P2).
(c)
Deﬁne T : U →P2 by T
µ· a
b
b
c
¸¶
= a + bx + cx2.
13.
If u and w are in U then S(u + w) = T1(u + w) + T2(u + w) =
T1(u) + T1(w) + T2(u) + T2(w) = T1(u) + T2(u) + T1(w)+
T2(w) = S(u) + S(w).
If c is a scalar then S(cu) = T1(cu)+
T2(cu) = cT1(u) + cT2(u) = c[T1(u) + T2(u)] = cS(u).
This proves that S is a linear transformation.
14.
Let u and w be in U. [aT](u + w) = a[T(u + w)] =
a[T(u ) + T(w )] = a[T(u )] + a[T(w )] = [aT](u ) + [aT](w ). If c is a scalar then
[aT](cu ) = a[T(cu )] = a[cT(u )] = c[aT(u )] = c[aT](u ). There-
fore aT
is a linear transformation.
15.
Suppose that T −1(v ) = u . Then T(u ) = v
and T(cu ) =
cT(u ) = cv . Therefore T −1(cv ) = cu = cT −1(v ).
16.
By formula 1, (T −1)−1(u ) = v
where v
is chosen so that T −1(v) =
u. But T −1(v ) = u
precisely when T(u ) = v . Therefore (T −1)−1
= T.
17.
Let u
be in U and set T(u ) = v . Then T −1(v ) = u
so
(T −1 ◦T)(u ) = T −1(T(u )) = T −1(v ) = u . It follows that
T −1 ◦T = IU. Likewise (T ◦T −1)(v) = T(T−1(v )) = T(u ) = v ,
so
T ◦T −1 = IV .
18.
(a)
Suppose S and T
are both one to one and let u
be in N(T ◦S).
Thus θW = (T ◦S)(u) = T(S(u)). Since T is one to one it follows that S(u) = θV .
But S is also one to one so u = θU.

160
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
(b)
Suppose that S and T
are onto and let w be in W. By assumption there exists v
in V
such that T(v ) = w . Likewise, S is onto
so there exists u in U such that S(u) = v. Therefore (T ◦S)(u)
= T(S(u)) = T(v) = w. It follows that T ◦S is onto.
(c) S and T
are both one to one and both onto so by (a) and (b) T ◦S is one to one
and onto. Therefore T ◦S is invertible. To see that (T ◦S)−1 = S−1 ◦T −1, let w be
in W. Since T ◦S is onto there exists u in U such that (T ◦S)(u) = w . Therefore
(T ◦S)−1(w ) = u . Now set v = S(u ). Then T(v ) = w
so
T −1(w ) = v
and S−1(v ) = u . Therefore (S−1 ◦T −1)(w ) =
S−1(T −1(w )) = S−1(v ) = u = (T ◦S)−1(w ).
19.
Let S : U →V
be an isomorphism and let T : V →W
be an isomorphism. By Exercise
18, T ◦S : U →W
is an isomorphism.
20.
(a)
Since T is one to one, nullity (T) = 0. Therefore rank (T) = n −nullity (T) = n so
R(T) = V. Since T
is onto, T is invertible.
(b)
By assumption rank (T) = n. Thus nullity (T) = n −rank (T) = 0 and T is one to
one. Therefore T
is invertible.
21.
It is easy to show that T(p(x)) = θ(x) if and only if p(x) = θ(x).
Thus N(T)= {θ(x)} and T is one to one. Clearly there exists no polynomial p(x) in P
such that T(p(x)) = 1. Therefore T is not onto.
This does not contradict Exercise 20(a)
since P has inﬁnite dimension.
22.
Let q(x) = b0+b1x+· · ·+bnxn be in P and set p(x) = b0x+(b1/2)x2+· · ·+[bn/(n+1)]xn+1.
Then S(p) = p′(x) = q(x)
so S
is onto. Note that N(T)
is the set of all constant
polynomials. In particular, N(T)̸= {θ(x)} so T is not one to one. This does not contradict
Exercise 20(b) since P has inﬁnite dimension.
23.
If u
is in N(S) then S(u) = θV . Therefore (T ◦S)(u ) =
T(S(u)) = T(θV ) = θW and u is in N(T ◦S). If T ◦S is one to one then N(T ◦S) = {θU}.
Therefore N(S) = {θU} and S is one to one.
24.
If w
is in R(T ◦S) then there exists u
in U such that w =
(T ◦S)(u ). Set v = S(u ). Then v
is in V
and T(v ) = T(S(u )) =
(T ◦S)(u) = w . This shows that w is in R(T). If T ◦S is onto we have R(T ◦S) = V
and R(T ◦S) ⊆R(T) ⊆V. It follows that R(T) = V
and T
is onto.
25.
Assume that T ◦S is invertible. Then T ◦S is one to one so, by Exercise 23, S is one
to one. Exercise 20(a) now implies that S is invertible. Since T ◦S is also onto, Exercise
24 implies that T
is onto. Exercise 20(b) now implies that T
is invertible.

5.9. MATRIX REPRESENTATIONS FOR LINEAR TRANSFORMATIONS
161
26.
Deﬁne S : Rn →Rp
by S(x ) = Bx
and deﬁne T : Rp →Rm by T(y ) = Ay
. Then T ◦S : Rn →Rm is deﬁned by (T ◦S)(x ) = ABx.
Therefore nullity (B) =
nullity (S), nullity (AB) =
nullity (T ◦S), rank (A) = rank (T), and rank (AB) = rank (T ◦S).
Now apply Exercises 23 and 24.
27
It follows from Exercise 20(a) that T
is invertible if and only if T
is one to one. Now
apply Exercise 26 of Section 4.7.
28.
Deﬁne S : Rn →Rnby S(x ) = B(x ) and deﬁne T : Rn →Rn by T(x) = Ax. Then
T ◦S : Rn →Rn is deﬁned by (T ◦S)(x) = ABx. If AB is nonsingular then T ◦S is
invertible by Exercise 27. By Exercise 25, both T and S are invertible. Applying Exercise
27 again, we see that A and B are nonsingular.
29.
To prove that L(U, V ) is a vector space requires checking all ten properties of Deﬁnition
1 in Section 4.2. We shall verify only properties (c1), (c2), (a2), (a3), (a4), and (m2).
If S and T are in L(U, V ) and c is a scalar then S + T and cT are in L(U, V ) by Exercises
13 and 14. Thus properties (c1) and (c2) hold. Now let R be in L(U, V ). To show that
R+(S+T) = (R+S)+T we must show that each of the transformations has the same action
on any vector u in U. But addition is associative in V so [R + (S + T)](u) = R(u) + (S +
T)(u) = R(u)+(S(u)+T(u)) = (R(u)+S(u))+T(u) = (R+S)(u)+T(u) = [(R+S)+T](u).
Recall that the zero linear transformation T0 : U →V is deﬁned by T0(u) =θV for every u
in U. Thus (T + T0)(u) = T(u) + T0(u) = T(u)+θV = T(u). It follows that T + T0 = T,
so T0 is the zero of L(U, V ). For T in L(U, V ) it is easily seen that T + (−1)T = T0, so
(−1)T = −T and property (a4) of Deﬁnition 1 is satisﬁed,
To check (m2) let a be a scalar and let S and T be in L(U, V ). For any u in U [a(S+T)](u) =
a[(S + T)(u)] = a[S(u) + T(u)] = aS(u) + aT(u) = [aS](u) + [aT](u) = [aS + aT](u). It
follows that a(S + T) = aS + aT.
5.9
Matrix Representations for Linear Transformations
1. S(1) = 0, S(x) = 1, S(x2) = 0,
and S(x3) = 0.
Thus [S(1)]C = [S(x2)]C = [S(x3)]C =
[0, 0, 0, 0, 0]T, while [S(x)]C = [1, 0, 0, 0, 0]T.
The matrix for S is


0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0


.

162
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
2.
The matrix for T
is


2
0
0
0
1
2
0
0
0
1
2
0
0
0
1
2
0
0
0
1


.
3.
(a) (S + T)(1) = 2 + x, (S + T)(x) = 1 + 2x + x2, (S + T)(x2) = 2x2 + x3, (S + T)(x3) =
2x3 + x4. Therefore [(S + T)(1)]C = [2, 1, 0, 0, 0]T, [(S + T)(x)]C = [1, 2, 1, 0, 0]T, [(S +
T)](x2)]C = [0, 0, 2, 1, 0]T, and [(S + T)(x3)]C = [0, 0, 0, 2, 1]T.
The matrix for S + T
is the matrix


2
1
0
0
1
2
0
0
0
1
2
0
0
0
1
2
0
0
0
1


.
(b)
By Theorem 19 the matrix for S + T
is the sum of matrices for S and T. This is
easily veriﬁed.
4.
The matrix for 2T
is


4
0
0
0
2
4
0
0
0
2
4
0
0
0
2
4
0
0
0
2


.
5. H(1) = 1, H(x) = 1, H(x2) = 2x, H(x3) = 3x2, and H(x4) = 4x3. Therefore [H(1)]B =
[H(x)]B = [1, 0, 0, 0]T, [H(x2)]B
= [0, 2, 0, 0]T, [H(x3)]B = [0, 0, 3, 0]T, and [H(x4)]B = [0, 0, 0, 4]T.
The matrix for H is the matrix


1
1
0
0
0
0
0
2
0
0
0
0
0
3
0
0
0
0
0
4

.
6.
(a)
The matrix for H ◦T
is the matrix


3
2
0
0
0
2
4
0
0
0
3
6
0
0
0
4

.
(b)
Denote by D, E, and F
the matrices in Exercises 5,2, and 6(a), respectively. By
Theorem 20, F = DE and it is easily veriﬁed that this is the case.
7.
(a) (T ◦H)(1) = 2 + x, (T ◦H)(x) = 2 + x, (T ◦H)(x2) = 4x + 2x2, (T ◦H)(x3) =
6x2 + 3x3, (T ◦H)(x4) = 8x3 + 4x4. Therefore
[(T ◦H)(1)]C = [(T ◦H)(x)]C = [2, 1, 0, 0, 0]T, [(T ◦H)(x2)]C
= [0, 4, 2, 0, 0]T, [(T ◦H)(x3)]C = [0, 0, 6, 3, 0]T, and

5.9. MATRIX REPRESENTATIONS FOR LINEAR TRANSFORMATIONS
163
[(T ◦H)(x4)]C = [0, 0, 0, 8, 4]T. Thus the matrix for T ◦H is the matrix


2
2
0
0
0
1
1
4
0
0
0
0
2
6
0
0
0
0
3
8
0
0
0
0
4


.
(b) Let D, E, and F denote the matrices for T, H, and T ◦H, respectively (cf. Exercises
2, 5, and 7(a)). By Theorem 20, F = DE and it is easily veriﬁed that this the case.
8.
(a) [p]B = [a0, a1, a2, a3]T, S(p)=a1
so [S(p)]C = [a1, 0, 0, 0, 0]T.
9.
(a) [p]B = [a0, a1, a2, a3]T; T(p) = 2a0 + (a0 + 2a1)x + (a1 + 2a2)x2 + (a2 + 2a3)x3 + a3x4
so [T(p)]C = [2a0, a0 + 2a1, a1 + 2a2, a2 + 2a3, a3]T.
10. [q]C = [a0, a1, a3, a4]T; H(q) = (a0 + a1) + 2a2x + 3a3x2 + 4a4x3
so [H(q)]B = [a0 +
a1, 2a2, 3a3, 4a4]T. It is easily seen that N[q]C = [H(q)]B.
11.
(a) Q =


1
0
0
0
2
0
0
0
3

.
(b) P =


1
0
0
0
1/2
0
0
0
1/3

.
(c)
Clearly P = Q−1.
12.
(a) Q =


0
−1
0
1
0
0
0
0
−1

.
(b) P =


0
1
0
−1
0
0
0
0
−1

.
(c)
Clearly P = Q−1.
13.
(a) T(E11) = E11, T(E12) = E21, T(E21) = E12,
and T(E22) = E22.
Therefore Q =


1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1

.
(b)
If A = [aij] is a (2 x 2) matrix then [A]B = [a11, a12, a21, a22]T
whereas [AT]B = [a11, a21, a12, a22]T. Clearly Q[A]B = [AT]B.

164
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
14.


3
0
0
0
3
0
0
−1
3
0
0
0

.
15. S(x + 1) = 3 + 3x −x2, S(x + 2) = 6 + 3x −x2, and S(x2) = 3x2. Therefore [S(x + 1)]C =
[3, 3, −1, 0]T, [S(x + 2)]C = [6, 3, −1, 0]T,
and [S(x2)]C = [0, 0, 3, 0]T and the matrix representation for S
is


3
6
0
3
3
0
−1
−1
3
0
0
0

.
16.


1
0
0
0
1
0
0
0
1
0
0
0

.
17. T(1) =


1
0
0

, T(x) =


0
3
1

,
and T(x2) =


0
6
4

,
so the matrix for T
is given by


1
0
0
0
3
6
0
1
4

.
18.


0
−2
−2
−1
1
4
1
2
2

.
19. T(v1) = 0v1+1v2+0v3+0v4, T(v2) = 0v1+0v2+1v3+0v4, T(v3) = 1v1+1v2+0v3+0v4,
and T(v4) = 1v1 + 0v2 + 0v3 + 3v4, so the matrix of T
is


0
0
1
1
1
0
1
0
0
1
0
0
0
0
0
3

.
20. T(e1 ) = Ae1 =
· 1
3
¸
, T(e2 ) = Ae2 =
· 2
0
¸
and T(e3 ) = Ae3 =
· 1
4
¸
. Therefore the
matrix for T
is the given matrix A.
21. T(1) = −4 + 3x −x2, T(x) = −2 + 3x + 2x2, and T(x2) = 3x2
so the matrix for T
is


−4
−2
0
3
3
0
−1
2
3

.

5.9. MATRIX REPRESENTATIONS FOR LINEAR TRANSFORMATIONS
165
22. [p]B = [a0, a1, a2]T and [T(p)]B = [−4a0 −2a1, 3a0 + 3a1, −a0 + 2a1 + 3a2]T. It is easily
veriﬁed that Q[p]B = [T(p)]B.
23. T(1 −3x + 7x2) = 2(1 −3x + 7x2), T(6 −3x + 2x2) = −3(6 −3x + 2x2), and T(x2) = 3x2.
Therefore the matrix of T
is


2
0
0
0
−3
0
0
0
3

.
24.
Let P = [P1 , P2 , . . . , Pk ] be a matrix such that P[u]B = [T(u)]C
for every vector u
in U. Since [u ]B is in Rn and [T(u) ]C
is in Rm,
P
is necessarily an (m x n) matrix. Therefore k = n. Suppose Q =
[Q1 , Q2 , . . . , Qn ] and assume that B = {u1 , u2 , . . . , un }. Then for 1 ≤j ≤n, Pj = Pej
= P[uj ]B = [T(uj )]C = Q[uj ]B = Qej = Qj. It follows that P = Q.
25.
Let B = {u1 , u2 , . . . , un } and C = {v1 , v2 , . . . , vm }. Suppose
T1(u1)
=
a11v1 + a21v2 + · · · + am1vm
T1(u2)
=
a12v1 + a22v2 + · · · + am2vm
...
...
T1(un)
=
a1nv1 + a2nv2 + · · · + amnvm
.
Also assume that
T2(u1)
=
b11v1 + b21v2 + · · · + bm1vm
T2(u2)
=
b12v1 + b22v2 + · · · + bm2vm
...
...
T2(un)
=
b1nv1 + b2nv2 + · · · + bmnvm
.
Then T1 and T2 are represented by the (m x n) matrices Q1 = [aij]
and Q2 = [bij], respectively. To obtain the matrix for T1 + T2 note that
(T1 + T2)(u1 ) = T1(u1 ) + T2(u1 ) =
(a11 + b11)v1 +(a21 + b21)v2 + · · · + (am1 + bm1)vm
(T1 + T2)(u2 ) = T1(u2 ) + T2(u2 ) =
(a12 + b12)v1 +(a22 + b22)v2 + · · · + (am2 + bm2)vm
...
...
(T1 + T2)(un ) = T1(un ) + T2(un ) =
(a1n + b1n)v1 +(a2n + b2n)v2 + · · · + (amn + bmn)vm
Therefore the matrix for T1 + T2 is the (m x n) matrix
[aij + bij] = Q1 + Q2.

166
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
26.
Let B = {u1 , . . . , un } and C = {v1 , . . . , vm }. If T(uj ) =
Pm
i=1 qijvi
for 1 ≤j ≤n, then the matrix for T
is the (m x n)
matrix Q = [qij].
Moreover [aT](uj ) = a Pm
i=1 qijvi = Pm
i=1(aqij)vi
for 1 ≤j ≤n.
Therefore the matrix for aT
is the (m x n) matrix [aqij] = aQ.
27. By assumption Q[u]B = [T(u)]C for every vector u in U. If P is the matrix for aT then P is
the unique matrix such that P[u]B= [(aT)(u)]C for every vector u in U. But (aQ)[u]B =
a(Q[u]B) = a[T(u)]C = [aT(u)]C = [(aT)(u)]C. It follows that P = aQ.
28.
If B = {v1 , v2 , . . . , vn } is a basis for V
then IV (vj ) = vj
for 1 ≤j ≤n. Therefore
[IV (vj )]B = ej
and the matrix representation for IV
is [e1 , e2 , . . . , en ] = I.
29.
If B = {v1 , v2 , . . . , vn } is a basis for V
then T0(vj ) = θV = 0v1 +0v2 + · · · + 0vn
for
1 ≤j ≤n. Thus [T0(vj )]B = θ
(the zero vector in Rn
) and the matrix for T0
is the
(n x n) zero matrix.
30.
It is an immediate consequence of Theorem 20 and Exercise 28 that PQ = I and QP = I.
Therefore P = Q−1.
31.
If Q is the matrix for T then T(1) = 1−x2, T(x) = x+x2, T(x2) = 2, and T(x3) = x−x2.
Therefore T(a0 +a1x+a2x2 +a3x3) = a0T(1)+a1(T(x)+a2T(x2)+a3T(x3) = (a0 +2a2)+
(a1 + a3)x + (−a0 + a1 −a3)x2.
32. S(a0 + a1x + a2x2) =
· a0 −a2
a1 + a2
2a0
a1 −a2
¸
.
33.
To see that ψ is one to one let T : U →V
be a linear transformation and assume that
T
is in N(ψ); that is ψ(T) is the (m x n) zero matrix.
Let B = {u1 , u2 , . . . , un } and C = {v1 , v2 , . . . , vm } be the given bases for U and V,
respectively. By assumption, T(uj) = Pm
i=1 0vi = θV
for each vector uj
in B, 1 ≤j ≤n.
It follows that T(u ) = θV
for each u in U.
Therefore T = T0,
where T0
is the zero
transformation from U
to V
(cf. Exercise 29). But T0
is the zero vector in the vector
space L(U, V ), and N(ψ) = {T0}. This proves that ψ is one to one.
5.10
Change of Basis and Diagonalization
1. T(u1) = u1 and T(u2) = 3u2. Therefore u1 and u2 are eigenvectors for T corresponding
to the eigenvalues λ1 = 1 and λ2 = 3, respectively. The matrix of T with respect to C is
· 1
0
0
3
¸
.
2.
The matrix for T
with respect to C is


2
0
0
0
1
0
0
0
1

.

5.10. CHANGE OF BASIS AND DIAGONALIZATION
167
3. T(A1) = 2A1, T(A2) = −2A2, T(A3) = 3A3, and T(A4) = −3A4.
Therefore A1, A2, A3
and A4
are eigenvectors for T
corresponding to the eigenvalues
λ1 = 2, λ2 = −2, λ3 = 3, and λ4 = −3, respectively. The matrix for T
with respect to C
is given by


2
0
0
0
0
−2
0
0
0
0
3
0
0
0
0
−3

.
4.
The transition matrix is the matrix P =
· −1/2
1/2
1/2
1/2
¸
. Note that Pa = [−1, 3]T = [a]C.
Therefore a= −u1 +3u2 . Similarly b= u1 −u2 , c= −2u1 +7u2 , and d= (−a/2+b/2)u1
+(a/2 + b/2)u2 .
5.
The transition matrix is the matrix P =


1
−1
−1
1
−1
0
−1
2
1

.
Now [p(x)]B = [2, 1, 0]T
and P[p(x)]B = [1, 1, 0]T = [p(x)]C. Denote the polynomials
in C
by g1(x), g2(x), g3(x), respectively. It follows that p(x) = g1(x) + g2(x). Similarly
s(x) = −2g1(x) −g2(x) + 2g3(x), q(x) = −5g1(x) −3g2(x) + 7g3(x), and r(x) =
(a0 −a1 −a2)g1(x) + (a0 −a1)g2(x) + (−a0 + 2a1 + a2)g3(x).
6.
The transition matrix is P =


0
0
0
1
0
0
1
0
0
1
−1
0
1
0
0
−1

. A = 4A1 + 3A2 −A3 −3A4; B =
3A1 + A3 −4A4; C = dA1 + cA2 + (b −c)A3 + (a −d)A4.
7.
Since u1 = (1/3)w1 +(1/3)w2
and u2 = (5/3)w1 −(1/3)w2 , the transition matrix is
P =
· 1/3
5/3
1/3
−1/3
¸
.
8. P =
·
5/3
2/3
−4/3
−1/3
¸
.
9.
The transition matrix is P =


−1
1
2
3
1
0
0
−3
0
0
1
0
0
0
0
1

.
Since [p(x)]B = [2, −7, 1, 0]T, P[p(x)]B = [−7, 2, 1, 0]T = [p(x)]C.
Let the polynomials in
C
be denoted by g1(x), g2(x), g3(x),
and g4(x),
respectively.
It follows that p(x) =
−7g1(x) + 2g2(x) + g3(x). Similarly q(x) = 13g1(x) −4g2(x) + g4(x) and r(x) = −7g1(x) +
3g2(x) −2g3(x) + g4(x).

168
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
10.
The transition matrix is P =


1
0
0
0
1
1
0
0
1

. p(x) = −3 + 6x+
x(x −1); q(x) = 8 −4x + 2x(x −1); r(x) = −5 + x + x(x −1).
11.
Note that T(e1 ) = [2, 1]T = 2e1 +e2
and T(e2 ) = [1, 2]T = e1 +2e2 . Therefore the
matrix of T
with respect to B is the matrix Q1 =
· 2
1
1
2
¸
. The transition matrix from
B
to C
is the matrix P =
· −1
1
1
1
¸
and P −1 =
· −1/2
1/2
1/2
1/2
¸
. By Theorem 24 the
matrix of T
with respect to C is the matrix Q2 given by Q2 = P −1Q1P =
· 1
0
0
3
¸
.
12.
The matrix for T
with respect to B is Q1 =


2
−1
−1
1
0
−1
−1
1
2

. The transition matrix
from C to B is P =


1
1
1
1
0
1
−1
1
0

and P −1 =


1
−1
−1
1
−1
0
−1
2
1

. By Theorem 24 the
matrix of T
with respect to C is the matrix Q2 = P −1Q1P =


2
0
0
0
1
0
0
0
1

.
13.
The matrix of T
with respect to B is Q1 =


−3
0
0
5
0
3
−5
0
0
0
−2
0
0
0
0
2

. The transition matrix
from C
to B
is P =


1
0
0
1
0
1
1
0
0
1
0
0
1
0
0
0

and P −1 =


0
0
0
1
0
0
1
0
0
1
−1
0
1
0
0
−1

. By Theorem 24
the matrix of T
with respect to C is the matrix Q2 = P −1Q1P =


2
0
0
0
0
−2
0
0
0
0
3
0
0
0
0
−3

.
14.
(a) Q =
·
4
3
−2
−3
¸
.
(b)
If S =
· −3
1
1
−2
¸
then S−1QS = R where R =
· 3
0
0
−2
¸
.
(c) C = {−3 + x, 1 −2x}.

5.10. CHANGE OF BASIS AND DIAGONALIZATION
169
(d) P =
· −2/5
−1/5
−1/5
−3/5
¸
.
(e) [w1 ]B = [2, 3]T so [w1 ]C = P[w1 ]B = [−7/5, −11/5]T. There-
fore [T(w1 )]C = R[w1 ]C = [−21/5, 22/5]T. It follows that T(w1 ) = (−21/5)[−3 +
x] + (22/5)[1 −2x] = 17 −13x.
Similarly [T(w2 )]C = [3/5, 4/5]T
so T(w2 ) =
(3/5)[−3 + x] + (4/5)[1 −2x] = −1 −x. Finally, [T(w3 )]C = [−3/5, 6/5]T so T(w3
) = (−3/5)[−3 + x] + (6/5)[1 −2x] = 3 −3x.
15.
(a) T(1) = 1, T(x) = 1 + 2x, and T(x2) = 4x + 3x2.
Therefore Q =


1
1
0
0
2
4
0
0
3

.
(b) Q has characteristic polynomial p(t) = −(t −1)(t −2)(t −3).
Therefore Q has eigenvalues λ1 = 1, λ2 = 2, λ3 = 3. The corresponding eigenvectors
are u1 = [1, 0, 0]T, u2 = [1, 1, 0]T and u3 = [2, 4, 1]T, respectively. If S = [u1 , u2 , u3 ]
then S−1QS = R
where R =


1
0
0
0
2
0
0
0
3

.
(c) C = {v1 , v2 , v3 } where [vi ]B = ui , the ith column of S. Thus [v1 ]B = [1, 0, 0]T
so v1 = 1; [v2 ]B = [1, 1, 0]T so v2 =
1 + x; [v3 ]B = [2, 4, 1]T so v3 = 2 + 4x + x2.
(d) 1 = v1 , x = −v1 +v2 ,
and x2 = 2v1 −4v2 +v3
so the transition matrix is
P =


1
−1
2
0
1
−4
0
0
1

.
(e) [w1 ]B = [−8, 7, 1]T and [w1 ]C = P[w1 ]B = [−13, 3, 1]T. There-
fore [T(w1 )]C = R[w1 ]C = [−13, 6, 3]T. It follows that T(w1) =
−13v1 +6v2 +3v3 = −1 + 18x + 3x2. Similarly [T(w2 )]C =
[7, −8, 3]T so T(w2 ) = 7v1 −8v2 +3v3 = 5 + 4x + 3x2.
Finally, [T(w3)]C =
[11, −22, 6]T so T(w3 ) =
11v1 −22v2 +6v3 = 1 + 2x + 6x2.
16.
(a) Q =


1
−1
0
0
0
2
−2
0
0
0
5
−3
0
0
0
10

.
(b)
If S =


1
−1
1
−3
0
1
−4
27
0
0
6
−108
0
0
0
180

then S−1QS = R where R =


1
0
0
0
0
2
0
0
0
0
5
0
0
0
0
10

.

170
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
(c) C =
½ · 1
0
0
0
¸
,
· −1
1
0
0
¸
,
· 1
−4
6
0
¸
,
·
−3
27
−108
180
¸ ¾
.
(d) P =


1
1
1/2
1/6
0
1
2/3
1/4
0
0
1/6
1/10
0
0
0
1/180

.
(e) T(w1 ) =
· −3
6
−3
10
¸
; T(w2 ) =
· 5
−8
5
0
¸
;
T(w3 ) =
· 15
−14
−6
20
¸
.
17.
Let Q be the matrix of IV
with respect to C and B. Since P is the matrix of IV
with
respect to B and C it follows from Theorem 20 in Section 4.9 that PQ is the matrix of
IV ◦IV = IV
with respect to C. Thus PQ = I. Similarly, QP
is the matix of IV
with
respect to B, so QP = I. Therefore Q = P −1 and P
is nonsingular.
18.
(a)
By assumption T(v ) = λv . Therefore Q[v ]B = [T(v )]B =
[λv ]B = λ[v ]B.
(b)
By assumption Qx = λx
and x = [v ]B. Thus Qx = Q[v ]B = [T(v )]B. It follows
that [T(v )]B = λx = λ[v ]B = [λv ]B.
Therefore T(v ) = λv .
19. Let v be an eigenvector for T corresponding to λ. Then T 2(v) = T(T(v)) = T(λ(v) =
λT(v) = λ(λv) = λ2v.
20. Suppose T is one to one and let v be a vector in V such that T(v) = 0v. Then T(v) = θ
so v is in N(T). But N(T) = {θ} so v = θ. Therefore 0 is not an eigenvalue for T (since
eigenvectors must be nonzero). Next assume that 0 is not an eigenvalue for T and let u
be in N(T). Then T(u) = θ = 0u. Since 0 is not an eigenvalue, it must be the case that
u = θ. Therefore N(T) = {θ} and T is one to one.
21. Suppose v is an eigenvector for T corresponding to λ. Thus T(v) = λv and T −1(λv) = v.
But T −1(λv) = λT −1(v) so it follows that T −1(v) = λ−1v.

5.11. SUPPLEMENTARY EXERCISES
171
5.11
Supplementary Exercises
1. V is not a vector space. For example, if A =
· 1
1
1
1
¸
then 1A ̸= A.
3.
(a) For arbitrary c, −2cA1 −3cA2 + cA3 = O. In particular, with c = 1, A3 = 2A1 + 3A2.
(b) As in (a), p3(x) = 2p1(x) + 3p2(x)
(c) v3 = 2v1 + 3v2.
4.
(a) B =
½· 2
1
0
0
¸
,
· −3
0
1
0
¸
,
· −1
0
0
1
¸¾
is one basis for W.
(b) Set A = 2
· 2
1
0
0
¸
+
· −3
0
1
0
¸
−2
· −1
0
1
1
¸
=
· 3
2
1
−2
¸
.
5.
(a) Sp (S) = {a + bx + cx2 : 7a −3b −5c = 0}.
(b) q1(x), q3(x), q4(x) are in Sp (S).
(c) A polynomial p(x) = a + bx + cx2 is in Sp (S) if and only if c = (7/5)a −(3/5)b. Thus,
p(x) = a[1 + (7/5)x2] + b[x −(3/5)x2]. The set B = {1 + (7/5)x2, x −(3/5)x2} is one
choice of a basis for Sp (S). Moreover, for this choice of B, [p(x)]B = [a, b]T .
(d) [q1(x)]B = [5, 5]T ; [q3(x)]B = [0, −5]T ; [q4(x)]B = [5, 0]T
6.
(a) The dependence relation x1A1 + x2A2 + x3A3 + x4A4 + x5A5 = O has solution x1 =
−x3 −2x5, x2 = x3 −3x5, x4 = −4x5, x3 and x5 arbitrary. {A1, A2, A4} is a basis
for Sp (S). Setting x3 = 1 and x5 = 0 yields −A1 + A2 + A3 = O, so A3 = A1 −A2.
Setting, x3 = 0 and x5 = 1 gives −2A1−3A2−4A4+A5 = O, so A5 = 2A1+3A2+4A4.
(b) Using the same calculations as in (a), {p1(x), p2(x), p4(x)} is a basis for Sp (S), p3(x) =
p1(x) −p2(x), and p5(x) = 2p1(x) + 3p2(x) + 4p4(x).
(c) {f1(x), f2(x), f3(x)} is a basis for Sp (S), f3(x) = f1(x) −f2(x) and f5(x) = 2f1(x) +
3f2(x) + 4f4(x).
7.
½· 1
0
2
3
0
−1
¸
,
· 0
1
−1
2
0
3
¸
,
· 0
0
0
0
1
2
¸¾
8. {p1(x), p2(x), p5(x)}
9. A polynomial p(x) = a + bx + cx2 + dx3 is in Sp (S) if and only if a −3b −c + d = 0 and in
this case q(x) = (4a −3b −2c)p1(x) + (−3a + 3b + c)p2(x) + (−2a + 2b + c)p5(x). Therefore,
q(x) is in Sp (S) and q(x) = 2p2(x) + p5(x).
10. Sp (S) =
½· a
b
c
d
¸
: a −3b −c + d = 0
¾
.
½· 3
1
0
0
¸
,
· 1
0
1
0
¸
,
· −1
0
0
1
¸
,
¾
, is a basis
for Sp (S).

172
CHAPTER 5. VECTOR SPACES AND LINEAR TRANSFORMATIONS
11.
(a) The matrix of T is A.
(b) rank (T) = 3 and nullity (T) = 3.
(c) R(T) =
©
p(x) = a + bx + cx2 + dx3 : a −3b −c + d = 0
ª
.
The set
©
3 + x, 1 + x2, −1 + x3ª
is a bais for R(T) . (cf Exercise 10).
(d) If B =
· 0
2
0
0
1
0
¸
then T(B) = q(x) (cf. Exercise 9).
(e) T
µ· a11
a12
a13
a21
a22
a23
¸¶
= θ(x) if and only if a11 = −2a13 −3a21 + a23, a12 = a13 −
2a21 −3a23, a22 = −2a23, a13, a21, a23 arbitrary.
Therefore,
½· −2
1
1
0
0
0
¸
,
· −3
−2
0
1
0
0
¸
,
· 1
−3
0
0
−2
1
¸¾
is a basis for N(T) .
12.
· a
b
¸
= (b + a)
· 0
1
¸
−a
· −1
1
¸
, so T
µ· a
b
¸¶
= (b + a)T
µ· 0
1
¸¶
−aT
µ· −1
1
¸¶
=
(b + a)(1 + 2x + x2) −a(2 −x) = (b −a) + (3a + 2b)x + (a + b)x2.
13. If T(1) =
· u1
u2
¸
then T(a + bx + cx2) = aT(1) + bT(x) + cT(x2) =
· au1 + b
bu2 + c
¸
.
In
particular, T(a + bx + cx2) =
· b
c
¸
is one such linear transformation.
14.
(a)


1
−1
1
−4
0
1
1
3
1
0
2
−1


(b) R(T) = {u + vx + wx2 : u + v −w = 0}.
Moreover, if q(x) = u + vx + wx2 is
in R(T) then T
µ· u + v −2c + d
v −c −3d
c
d
¸¶
= q(x) for arbitrary c and d. If
q1(x) = 1 + x2 and q2(x) = x + x2, then S = {q1(x), q2(x)} is a basis for R(T) .
(c) If A1 =
· 1
0
0
0
¸
and A2 =
· 1
1
0
0
¸
then T(A1) = q1(x) and T(A2) = q2(x).
(d) N(T) =
½· a
b
c
d
¸
: a = −2c + d and b = −c −3d, c and d arbitrary
¾
. Therefore, if
A3 =
· −2
−1
1
0
¸
and A4 =
· 1
−3
0
1
¸
then B2 = {A3, A4} is a basis for N (T).

5.12. CONCEPTUAL EXERCISES
173
5.12
Conceptual Exercises
1. True. u = a−1(au) = a−1(av) = v.
2. True. (a −b)v = θ and v ̸= θ, so a −b = 0.
3. False. Each vector v in V has a unique inverse −v in V , but as v varies, so does −v.
4. False. If n = 1 then p(x) = 1 −x and q(x) = 1 + x are in V but p(x) + q(x) is not in V .
5. True. Every basis for W is also a basis for V .
6. True. If dim(W) = k then a basis B for W is a linearly independent susbset of V containing
k vectors. Therefore, k ≤n.
7. True. aθ = θ for every nonzero scalar a.
8. True.
9. False. In R2 let S1 = {[1, 0]T , [0, 1]T } and S2{[1, 0]T , [0, 1]T , [1, 1]T }.
10. True. Since V = Sp (S1), dim(V ) ≤k. Since S2 is a linearly independent subset of V ,
l ≤dim(V ).
11. u = (1/2)(u + v) + (1/2)(u −v) and v = (1/2)(u + v) −(1/2)(u −v)

Chapter 6
Determinants
6.1
Introduction (No exercises)
6.2
Cofactor Expansion of Determinants
1. det(A) = 1(1) −3(2) = −5.
2. det(A) = −31.
3. det(A) = 2(8) −4(4) = 0; x = a
· −2
1
¸
, a ̸= 0.
4. det(A) = 2.
5. det(A) = 4(7) −3(1) = 25.
6. det(A) = 3.
7. det(A) = 4(1) −1(−2) = 6.
8. det(A) = 0; x = a
· −3
1
¸
, a ̸= 0.
9. A11 = (−1)2
¯¯¯¯
1
3
1
1
¯¯¯¯ = −2; A12 = (−1)3
¯¯¯¯
0
3
2
1
¯¯¯¯ = 6;
A13 = (−1)4
¯¯¯¯
0
1
2
1
¯¯¯¯ = −2; A33 = (−1)6
¯¯¯¯
1
2
0
1
¯¯¯¯ = 1.
10. A11 = −2; A12 = 4; A13 = 1, A33 = −4.
11. A11 =
¯¯¯¯
2
2
2
1
¯¯¯¯ = −2; A12 = −
¯¯¯¯
−1
2
3
1
¯¯¯¯ = 7;

176
CHAPTER 6. DETERMINANTS
A13 =
¯¯¯¯
−1
2
3
2
¯¯¯¯ = −8; A33 =
¯¯¯¯
2
−1
−1
2
¯¯¯¯ = 3.
12. A11 = −1; A12 = 3; A13 = −1; A33 = 0.
13. A11 =
¯¯¯¯
1
0
1
3
¯¯¯¯ = 3; A12 = −
¯¯¯¯
2
0
0
3
¯¯¯¯ = −6;
A13 =
¯¯¯¯
2
1
0
1
¯¯¯¯ = 2; A33 =
¯¯¯¯
−1
1
2
1
¯¯¯¯ = −3.
14. A11 = 6; A12 = −8; A13 = 0; A33 = 4.
15. det(A) = A11 + 2A12 + A13 = −2 + 2(6) + (−2) = 8.
16. det(A) = 14.
17. det(A) = 2A11 −A12 + 3A13 = 2(−2) −7 + 3(−8) = −35.
18. det(A) = 1.
19. det(A) = −A11 + A12 −A13 = −3 −6 −2 = −11.
20. det(A) = 8.
21. det(A) = 2
¯¯¯¯¯¯
0
0
1
1
2
0
1
1
2
¯¯¯¯¯¯
+ (−1)
¯¯¯¯¯¯
3
0
1
2
2
0
3
1
2
¯¯¯¯¯¯
+ (−1)
¯¯¯¯¯¯
3
0
1
2
1
0
3
1
2
¯¯¯¯¯¯
+
2(−1)
¯¯¯¯¯¯
3
0
0
2
1
2
3
1
1
¯¯¯¯¯¯
= 2
¯¯¯¯
1
2
1
1
¯¯¯¯ + (−1)
·
3
¯¯¯¯
2
0
1
2
¯¯¯¯ +
¯¯¯¯
2
2
3
1
¯¯¯¯
¸
+
(−1)
·
3
¯¯¯¯
1
0
1
2
¯¯¯¯ +
¯¯¯¯
2
1
3
1
¯¯¯¯
¸
+ (−2)(3)
¯¯¯¯
1
2
1
1
¯¯¯¯ = −9.
22. det(A) = 2.
23. det(A) = 2
¯¯¯¯¯¯
3
1
2
1
2
1
3
1
4
¯¯¯¯¯¯
+ 2
¯¯¯¯¯¯
1
3
2
0
1
1
0
3
4
¯¯¯¯¯¯
=
2
·
3
¯¯¯¯
2
1
1
4
¯¯¯¯ −
¯¯¯¯
1
1
3
4
¯¯¯¯ + 2
¯¯¯¯
1
2
3
1
¯¯¯¯
¸
+
2
· ¯¯¯¯
1
1
3
4
¯¯¯¯ −3
¯¯¯¯
0
1
0
4
¯¯¯¯ + 2
¯¯¯¯
0
1
0
3
¯¯¯¯
¸
= 22.
24. det(A) = 4.

6.2. COFACTOR EXPANSION OF DETERMINANTS
177
25. det(A) = a11A11 + a12A12 + a13A13 = (1)(10) + (3)(5) + (2)(−10) = 5; a21A21 + a22A22 +
a23A23 = (−1)(−5)+(4)(−1)+(1)(4) = 5; a31A31 +a32A32 +a33A33 = (2)(−5)+(2)(−3)+
(3)(7) = 5.
26. det(A) = a11A11 + a12A12 + a13A13 = (2)(−7) + (4)(0) + (1)(7) = −7; a21A21 + a22A22 +
a23A23 = (3)(−5) + (1)(2) + (3)(2) = −7; a31A31 + a32A32 + a33A33 = (2)(11) + (3)(−3) +
(2)(−10) = −7.
27. a11A21 + a12A22 + a13A23 = (1)(−5) + (3)(−1) + (2)(4) = 0; a11A31 + a12A32 + a13A33 =
(1)(−5) + (3)(−1) + (2)(7) = 0.
28. a11A21 + a12A22 + a13A23 = (2)(−5) + (4)(2) + (1)(2) = 0; a11A31 + a12A32 + a13A33 =
(2)(11) + (4)(−3) + (1)(−10) = 0.
29. C =


10
5
−10
−5
−1
4
−5
−3
7

, CT A =


5
0
0
0
5
0
0
0
5

= [det(A)]I.
So A−1 = (1/5)CT = [1/ det(A)]CT .
30. C =


−7
0
7
−5
2
2
11
−3
−10

, CT A =


−7
0
0
0
−7
0
0
0
−7

= [det(A)]I. So A−1 = −(1/7)CT =
[1/ det(A)]CT .
31. det(A) = −a12
¯¯¯¯
0
a23
0
a33
¯¯¯¯ + a13
¯¯¯¯
0
a22
0
a32
¯¯¯¯ = 0.
32. det(U) = u11
¯¯¯¯¯¯
u22
u23
u24
0
u33
a34
0
0
u44
¯¯¯¯¯¯
−u12
¯¯¯¯¯¯
0
u23
u24
0
u33
u34
0
0
u44
¯¯¯¯¯¯
+
u13
¯¯¯¯¯¯
0
u22
u24
0
0
u34
0
0
u44
¯¯¯¯¯¯
−u14
¯¯¯¯¯¯
0
u22
u23
0
0
u33
0
0
0
¯¯¯¯¯¯
=
u11
¯¯¯¯¯¯
u22
u23
u24
0
u33
u34
0
0
u44
¯¯¯¯¯¯
= u11u22
¯¯¯¯
u33
u34
0
u44
¯¯¯¯ = u11u22u33u44.
33. AT =
· a11
a21
a12
a22
¸
so det(AT) = a11a22 −a21a12 = det(A).
34.
(a)
If A is positive deﬁnite then 0 < e1TAe1= a11. If x= [u, v]T and x̸= θ then 0 < xTAx
= a11u2 + 2a12uv + a22v2 (since A is symmetric a12 = a21
). In particular if u = a12
and v = −a11 Then 0 < a11a2
12 −2a2
12a11 + a22a2
11 = a11(a11a22 −a2
12) = a11 det(A).
It follows that det(A) > 0.

178
CHAPTER 6. DETERMINANTS
(b)
Suppose a11 > 0 and det(A) > 0. For x = [u, v]T we have
a11(xTAx ) = a2
11u2 + 2a11a12uv + a11a22v2 = (a11u + a12v)2+
v2(a11a22 −a2
12) = (a11u + a12v)2 + v2 det(A). For x ̸=θ it follows
that xTAx > 0.
35.
(a)
For n = 3 and n = 4, H(n) = n!/2. For some integer k ≥4 suppose we have seen
that H(k) = k!/2. If A is a ((k + 1) x (k + 1)) matrix then det(A) can be obtained
by evaluating k + 1 (k x k) determinants. Thus the number of (2 x 2) determinants
in the expansion of det(A) is (k + 1)H(k) = (k + 1)!/2. It follows by induction that
H(n) = n!/2 for every positive integer n, n ≥2.
(b)
Note that evaluating a single (2 x 2) determinant requires 3 operations, two multi-
plications and one subtraction.
n
H(n)
Time required
−−
−−−−−−
−−−−−−−
2
1
3 seconds
5
60
3 minutes
10
1, 814, 400
1512 hours.
6.3
Elementary Operations and Determinants
1.
¯¯¯¯¯¯
1
2
1
2
0
1
1
−1
1
¯¯¯¯¯¯
C2 −2C1
C3 −C1
=
¯¯¯¯¯¯
1
0
0
2
−4
−1
1
−3
0
¯¯¯¯¯¯
=
¯¯¯¯
−4
−1
−3
0
¯¯¯¯ = −3.
2.
¯¯¯¯¯¯
2
4
−2
0
2
3
1
1
2
¯¯¯¯¯¯
C2 −2C1
C3 + C1
=
¯¯¯¯¯¯
2
0
0
0
2
3
1
−1
3
¯¯¯¯¯¯
= 2
¯¯¯¯
2
3
−1
3
¯¯¯¯ = 18.
3.
¯¯¯¯¯¯
0
1
2
3
1
2
2
0
3
¯¯¯¯¯¯
C1 ↔C2
=
−
¯¯¯¯¯¯
1
0
2
1
3
2
0
2
3
¯¯¯¯¯¯
C3 −2C1
=
−
¯¯¯¯¯¯
1
0
0
1
3
0
0
2
3
¯¯¯¯¯¯
=
−
¯¯¯¯
3
0
2
3
¯¯¯¯ = −9.
4.
¯¯¯¯¯¯
2
2
4
1
0
1
2
1
2
¯¯¯¯¯¯
=
¯¯¯¯¯¯
2
0
0
1
−1
−1
2
−1
−2
¯¯¯¯¯¯
= 2.
5.
¯¯¯¯¯¯
0
1
3
2
1
2
1
1
2
¯¯¯¯¯¯
C1 ↔C2
=
−
¯¯¯¯¯¯
1
0
3
1
2
2
1
1
2
¯¯¯¯¯¯
C3 −3C1
=

6.3. ELEMENTARY OPERATIONS AND DETERMINANTS
179
−
¯¯¯¯¯¯
1
0
0
1
2
−1
1
1
−1
¯¯¯¯¯¯
= −
¯¯¯¯
2
−1
1
−1
¯¯¯¯ = 1.
6.
¯¯¯¯¯¯
1
1
1
2
1
2
3
0
2
¯¯¯¯¯¯
C2 −C1
C3 −C1
=
¯¯¯¯¯¯
1
0
0
2
−1
0
3
−3
−1
¯¯¯¯¯¯
= 1.
7. det(B) = −2 det(A) = −6.
8. det(B) = −6 det(A) = −18.
9. det(B) = det(A) = 3.
10. det(B) = 2 det(A) = 6.
11. det(B) = det(A) = 3.
12. det(B) = 4 det(A) = 12.
13.
¯¯¯¯¯¯¯¯
1
0
0
0
2
0
0
3
1
1
0
1
1
4
2
2
¯¯¯¯¯¯¯¯
C2 ↔C4
=
−
¯¯¯¯¯¯¯¯
1
0
0
0
2
3
0
0
1
1
0
1
1
2
2
4
¯¯¯¯¯¯¯¯
C3 ↔C4
=
¯¯¯¯¯¯¯¯
1
0
0
0
2
3
0
0
1
1
1
0
1
2
4
2
¯¯¯¯¯¯¯¯
= 6.
14.
¯¯¯¯¯¯¯¯
0
0
2
0
0
0
1
3
0
4
1
3
2
1
5
6
¯¯¯¯¯¯¯¯
= −
¯¯¯¯¯¯¯¯
2
0
0
0
1
3
0
0
1
3
4
0
5
6
1
2
¯¯¯¯¯¯¯¯
= −48.
15.
¯¯¯¯¯¯¯¯
0
1
0
0
0
2
0
3
2
1
0
6
3
2
2
4
¯¯¯¯¯¯¯¯
C1 ↔C2
C3 ↔C4
=
¯¯¯¯¯¯¯¯
1
0
0
0
2
0
3
0
1
2
6
0
2
3
4
2
¯¯¯¯¯¯¯¯
C2 ↔C3
=
−
¯¯¯¯¯¯¯¯
1
0
0
0
2
3
0
0
1
6
2
0
2
4
3
2
¯¯¯¯¯¯¯¯
= −12.

180
CHAPTER 6. DETERMINANTS
16.
¯¯¯¯¯¯¯¯
1
2
0
3
2
5
1
1
2
0
4
3
0
1
6
2
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
0
0
2
1
0
0
2
−4
8
−23
0
1
5
7
¯¯¯¯¯¯¯¯
=
¯¯¯¯
8
−23
5
7
¯¯¯¯ = 171.
17.
¯¯¯¯¯¯¯¯
2
4
−2
−2
1
3
1
2
1
3
1
3
−1
2
1
2
¯¯¯¯¯¯¯¯
C2 −2C1
C3 + C2
C4 + C1
=
¯¯¯¯¯¯¯¯
2
0
0
0
1
1
2
3
1
1
2
4
−1
4
0
1
¯¯¯¯¯¯¯¯
C3 −2C2
C4 −3C2
=
¯¯¯¯¯¯¯¯
2
0
0
0
1
1
0
0
1
1
0
1
−1
4
−8
−11
¯¯¯¯¯¯¯¯
= 2
¯¯¯¯
0
1
−8
−11
¯¯¯¯ = 16.
18.
¯¯¯¯¯¯¯¯
1
1
2
1
0
1
4
1
2
1
3
0
2
2
1
2
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
0
0
0
1
0
0
2
−1
3
−1
2
0
−3
0
¯¯¯¯¯¯¯¯
= −3.
19.
¯¯¯¯¯¯¯¯
1
2
0
3
2
5
1
1
2
0
4
3
0
1
6
2
¯¯¯¯¯¯¯¯
R2 −2R1
R3 −2R1
=
¯¯¯¯¯¯¯¯
1
2
0
3
0
1
1
−5
0
−4
4
−3
0
1
6
2
¯¯¯¯¯¯¯¯
R3 + 4R4
R4 −R2
=
¯¯¯¯¯¯¯¯
1
2
0
3
0
1
1
−5
0
0
8
−23
0
0
5
7
¯¯¯¯¯¯¯¯
=
¯¯¯¯
8
−23
5
7
¯¯¯¯ = 171.
20.
¯¯¯¯¯¯¯¯
2
4
−2
−2
1
3
1
2
1
3
1
3
−1
2
1
2
¯¯¯¯¯¯¯¯
= 2
¯¯¯¯¯¯¯¯
1
2
−1
−1
1
3
1
2
1
3
1
3
−1
2
1
2
¯¯¯¯¯¯¯¯
=
2
¯¯¯¯¯¯¯¯
1
2
−1
−1
0
1
2
3
0
0
0
1
0
0
−8
−11
¯¯¯¯¯¯¯¯
= 16.
21.
¯¯¯¯¯¯¯¯
1
1
2
1
0
1
4
1
2
1
3
0
2
2
1
2
¯¯¯¯¯¯¯¯
R3 −2R1
R4 −2R1
=
¯¯¯¯¯¯¯¯
1
1
2
1
0
1
4
1
0
−1
−1
−2
0
0
−3
0
¯¯¯¯¯¯¯¯
R3 + R2
=

6.3. ELEMENTARY OPERATIONS AND DETERMINANTS
181
¯¯¯¯¯¯¯¯
1
1
2
1
0
1
4
1
0
0
3
−1
0
0
−3
0
¯¯¯¯¯¯¯¯
=
¯¯¯¯
3
−1
−3
0
¯¯¯¯ = −3.
22.
If A =
· 1
0
0
0
¯¯¯¯
and B =
· 0
0
0
1
¸
then det(A) = det(B) = 0 whereas det(A+B) = 1.
If A =
· 1
0
0
0
¸
and B =
· 0
1
0
0
¸
then det(A) = det(B) = 0 and det(A + B) = 0 =
det(A) + det(B).
23.
¯¯¯¯¯¯
a + 1
a + 4
a + 7
a + 2
a + 5
a + 8
a + 3
a + 6
a + 9
¯¯¯¯¯¯



C3 −C2
=
C2 −C1



¯¯¯¯¯¯
a + 1
3
3
a + 2
3
3
a + 3
3
3
¯¯¯¯¯¯
= 0;
¯¯¯¯¯¯
a
4a
7a
2a
5a
8a
3a
6a
9a
¯¯¯¯¯¯



C3 −C2
=
C2 −C1



¯¯¯¯¯¯
a
3a
3a
2a
3a
3a
3a
3a
3a
¯¯¯¯¯¯
= 0;
¯¯¯¯¯¯
a
a4
a7
a2
a5
a8
a3
a6
a9
¯¯¯¯¯¯
= (a)(a4)(a7)
¯¯¯¯¯¯
1
1
1
a
a
a
a2
a2
a2
¯¯¯¯¯¯
= 0.
24.
(a) Set B = [B1, B2, B3]. Then AB = [AB1, AB2, AB3] where AB1 = A


2
3
1

= 2A1 +
3A2 + A3, AB2 = A


0
−1
3

= −A2 + 3A3, AB3 = A


0
0
4

= 4A3.
(b)
det(AB)
=
det[2A1 + 3A2 + A3, −A2 + 3A3, 4A3]
(C3/4) :
=
(4) det[2A1 + 3A2 + A3, −A2 + 3A3, A3]
(C2 −3C3) :
=
(4) det[2A1 + 3A2 + A3, −A2, A3]
(−C2) :
=
(−4) det[2A1 + 3A2 + A3, A2, A3]
(C1 −C3) :
=
(−4) det[2A1 + 3A2, A2, A3]
(C1 −3C2) :
=
(−4) det[2A1, A2, A3]
(C1/2) :
=
(−8) det[A1, A2, A3] = (−8) det(A).
(c) det(B) = −8, so det(AB) = det(A) det(B).
25.
It follows from Theorem 3 (with c = 0 ) that if A has a zero column then det(A) = 0.
Since the ﬁrst column of U1j contains only zeros for 2 ≤j ≤n, det(U1j) = 0.

182
CHAPTER 6. DETERMINANTS
26.
If U
is a (2 x 2) upper triangular matrix then U =
· u11
u12
0
u22
¸
and det(U) = u11u22.
Suppose we have seen that det(U) = u11u22 · · · ukk
for a (k x k) upper triangular matrix U. If U is a ((k + 1) x (k + 1))
upper triangular matrix then det(U) = u11U11 + u12U12 + · · · +
u1,k+1U1,k+1 = u11U11. But U11 =
¯¯¯¯¯¯¯¯¯
u22
u23
· · ·
u2,k+1
0
u33
· · ·
u3,k+1
...
0
0
· · ·
uk+1,k+1
¯¯¯¯¯¯¯¯¯
=
u22u33 · · · uk+1,k+1 by assumption. Thus det(U) =
u11u22 · · · uk+1,k+1.
It follows by induction that if U = (uij) is an (n x n) upper triangular
matrix, n ≥2, then det(U) = u11u22 · · · unn.
27.
First note that
¯¯¯¯¯¯
x
y
1
x1
y1
1
x2
y2
1
¯¯¯¯¯¯
= 0
is a linear equation in x
and y.
It follows from
Theorem 5 that x = x1, y = y1
and x = x2, y = y2
are solutions. Consequently the
equation describes the line through the points (x1, y1) and (x2, y2).
28.
Consider the case represented by the ﬁgure below.
Clearly area (ABC) = area (ADEC) + area (CEFB)−
area(ADFB). Therefore area (ABC) = (1/2)(x3 −x1)(y1 + y3)+ (1/2)(x2 −x3)(y2 + y3) −
(1/2)(x2 −x1)(y1 + y2) =
(1/2)[x1y2 −x1y3 −x2y1 + x2y3 + x3y1 −x3y2] = (1/2)
¯¯¯¯¯¯
x1
y1
1
x2
y2
1
x3
y3
1
¯¯¯¯¯¯
.
29.
Let x = [x1, x2, x3]T and y = [y1, y2, y3]T and let B = xyT =
[B1, B2, B3] where Bj = [xjy1, xjy2, xjy3]T. Then A =
[B1 + e1, B2 + e2, B3 + e3]T.
Repeated applications of Theorem 4 yield; det(A) =
det[B1, B2, B3] + det[B1, B2, e3] +

6.4. CRAMER’S RULE
183
det[B1, e2, B3 ] + det[e1, B2, B3 ] + det[B1, e2, e3 ] +
det[e1, B2, e3 ] + det[e1, e2, B3 ] + det[e1, e2, e3 ].
Since each Bj, 1 ≤j ≤3, is a scalar multiple of y, Theorem 5 implies that
det(A) = det[B1, e2 , e3] + det[e1, B2, e3 ] + det[e1, e2, B3 ]+
det[e1, e2, e3 ] = x1y1 + x2y2 + x3y3 + 1 = 1 + yTx .
30.
¯¯¯¯¯¯
1
a
a2
1
b
b2
1
c
c2
¯¯¯¯¯¯
C3 −aC2
C2 −aC1
=
¯¯¯¯¯¯
1
0
0
1
b −a
b(b −a)
1
c −a
c(c −a)
¯¯¯¯¯¯
=
(b −a)(c −a)
¯¯¯¯
1
b
1
c
¯¯¯¯ = (b −a)(c −a)(c −b).
31.
¯¯¯¯¯¯¯¯
1
a
a2
a3
1
b
b2
b3
1
c
c2
c3
1
d
d2
d3
¯¯¯¯¯¯¯¯
C4 −aC3
C3 −aC2
C2 −aC1
=
¯¯¯¯¯¯¯¯
1
0
0
0
1
b −a
b(b −a)
b2(b −a)
1
c −a
c(c −a)
c2(c −a)
1
d −a
d(d −a)
d2(d −a)
¯¯¯¯¯¯¯¯
=
(b −a)(c −a)(d −a)
¯¯¯¯¯¯
1
b
b2
1
c
c2
1
d
d2
¯¯¯¯¯¯
=
(b −a)(c −a)(d −a)(c −b)(d −b)(d −c).
32. Write A = [A1, A2, . . . , An]. Then cA = [cA1, cA2, . . . , cAn].
By Theorem 3, det(cA) = c det[A1, cA2, . . . , cAn] =
c2 det[A1, A2, . . . , cAn] = · · · = cn det[A1, A2, . . . , An] =
cn det(A).
6.4
Cramer’s Rule
1.
¯¯¯¯¯¯
0
1
3
1
2
1
3
4
1
¯¯¯¯¯¯
{C1 ↔C2}
=
−
¯¯¯¯¯¯
1
0
3
2
1
1
4
3
1
¯¯¯¯¯¯
{C3 −3C1}
=
−
¯¯¯¯¯¯
1
0
0
2
1
−5
4
3
−11
¯¯¯¯¯¯
{C3 + 5C2}
=
−
¯¯¯¯¯¯
1
0
0
2
1
0
4
3
4
¯¯¯¯¯¯
= −4.
2.
¯¯¯¯¯¯
1
2
1
2
4
3
2
1
3
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
1
0
0
2
1
0
2
1
−3
¯¯¯¯¯¯
= 3.

184
CHAPTER 6. DETERMINANTS
3.
¯¯¯¯¯¯
2
2
4
1
3
4
−1
2
1
¯¯¯¯¯¯
½ C2 −C1
C3 −2C1
¾
=
¯¯¯¯¯¯
2
0
0
1
2
2
−1
3
3
¯¯¯¯¯¯
{C3 −C2}
=
¯¯¯¯¯¯
2
0
0
1
2
0
−1
3
0
¯¯¯¯¯¯
= 0.
4.
¯¯¯¯¯¯
1
0
1
2
1
1
1
2
1
¯¯¯¯¯¯
= 2
¯¯¯¯¯¯
1
0
0
0
1
0
0
0
1
¯¯¯¯¯¯
= 2.
5.
¯¯¯¯¯¯
1
0
−2
3
1
3
0
1
2
¯¯¯¯¯¯
{C3 + 2C1}
=
¯¯¯¯¯¯
1
0
0
3
1
9
0
1
2
¯¯¯¯¯¯
{C3 −9C2}
=
¯¯¯¯¯¯
1
0
0
3
1
0
0
1
−7
¯¯¯¯¯¯
{(−1/7)C3}
=
−7
¯¯¯¯¯¯
1
0
0
3
1
0
0
1
1
¯¯¯¯¯¯
{C2 −C3}
=
−7
¯¯¯¯¯¯
1
0
0
3
1
0
0
0
1
¯¯¯¯¯¯
{C1 −3C2}
=
−7
¯¯¯¯¯¯
1
0
0
0
1
0
0
0
1
¯¯¯¯¯¯
= −7.
6. A is singular.
7. (a) det(AB) = det(A) det(B) = (2)(3) = 6
(b) det(AB2) = det(A)[det(B)]2 = (2)(9) = 18
(c) det(A−1B) = det(B)/ det(A) = 3/2
(d) det(2A−1) = 8 det(A−1) = 8/ det(A) = 4
(e) det (2A)−1 = det((1/2)A−1) = (1/8) det(A−1) = 1/[8 det(A)] = 1/16.
8. Both matrices have determinant sin2 θ + cos2 θ = 1, so the matrices are nonsingular for all
values of θ.
9. det(B(λ)) = 2λ −λ2 = λ(2 −λ); B(λ) is singular provided λ = 0 or λ = 2.
10. det(B(λ)) = λ2 −1; B(λ) is singular for λ = ±1.
11. det(B(λ)) = 4 −λ2; B(λ) is singular for λ = ±2.
12. det(B(λ)) = 2(1 −λ)(3 −λ); B(λ) is singular provided λ = 1 or λ = 3.
13. det(B(λ)) = (λ −1)2(λ + 2); B(λ) is singular provided λ = 1 or λ = −2.
14. det(B(λ)) = λ(λ −3)(λ + 1); B(λ) is singular provided λ = 0, λ = 3, or λ = −1.
15. det(A) =
¯¯¯¯
1
1
1
−1
¯¯¯¯ = −2; det(B1) =
¯¯¯¯
3
1
−1
−1
¯¯¯¯ = −2;

6.4. CRAMER’S RULE
185
det(B2) =
¯¯¯¯
1
3
1
−1
¯¯¯¯ = −4.
x1 = det(B1)/ det(A) = 1; x2 = det(B2)/ det(A) = 2.
16. x1 = x2 = 1.
17. det(A) =
¯¯¯¯¯¯
1
−2
1
1
0
1
1
−2
0
¯¯¯¯¯¯
= −2; det(B1) =
¯¯¯¯¯¯
−1
−2
1
3
0
1
0
−2
0
¯¯¯¯¯¯
= −8;
det(B2) =
¯¯¯¯¯¯
1
−1
1
1
3
1
1
0
0
¯¯¯¯¯¯
= −4; det(B3) =
¯¯¯¯¯¯
1
−2
−1
1
0
3
1
−2
0
¯¯¯¯¯¯
= 2.
x1 = det(B1)/ det(A) = 4; x2 = det(B2)/ det(A) = 2;
x3 = det(B3)/ det(A) = −1.
18. x1 = −1, x2 = 0, x3 = 3.
19. det(A) =
¯¯¯¯¯¯¯¯
1
1
1
−1
0
1
−1
1
0
0
1
−1
0
0
1
2
¯¯¯¯¯¯¯¯
= 3; det(B1) =
¯¯¯¯¯¯¯¯
2
1
1
−1
1
1
−1
1
0
0
1
−1
3
0
1
2
¯¯¯¯¯¯¯¯
= 3;
det(B2)
¯¯¯¯¯¯¯¯
1
2
1
−1
0
1
−1
1
0
0
1
−1
0
3
1
2
¯¯¯¯¯¯¯¯
= 3; det(B3) =
¯¯¯¯¯¯¯¯
1
1
2
−1
0
1
1
1
0
0
0
−1
0
0
3
2
¯¯¯¯¯¯¯¯
= 3;
det(B4) =
¯¯¯¯¯¯¯¯
1
1
1
2
0
1
−1
1
0
0
1
0
0
0
1
3
¯¯¯¯¯¯¯¯
= 3.
x1 = det(B1)/ det(A) = 1; x2 = det(B2)/ det(A) = 1;
x3 = det(B3)/ det(A) = 1; x4 = det(B4)/ det(A) = 1.
20. x1 = 2, x2 = 1, x3 = 0.
21. det(A) =
¯¯¯¯¯¯
1
1
1
0
1
1
0
0
1
¯¯¯¯¯¯
= 1; det(B1) =
¯¯¯¯¯¯
a
1
1
b
1
1
c
0
1
¯¯¯¯¯¯
= a −b;
det(B2) =
¯¯¯¯¯¯
1
a
1
0
b
1
0
c
1
¯¯¯¯¯¯
= b −c; det(B3) =
¯¯¯¯¯¯
1
1
a
0
1
b
0
0
c
¯¯¯¯¯¯
= c.
x1 = a −b, x2 = b −c, x3 = c.

186
CHAPTER 6. DETERMINANTS
22. det(A)2 = det(A2) = det(I) = 1 so det(A) = ±1.
23.
Suppose ˆB
is produced by interchanging the ith
and the jth
columns of B.
Thus
A ˆB = A[B1, . . . , Bj, . . . , Bi, . . . , Bn] =
[AB1 , . . . , ABj , . . . , ABi . . . . , ABn ] = ˆC
where ˆC
is obtained by interchanging the ith
and jth columns of C = AB.
Suppose ˆB is produced by replacing Bi
with Bi +aBj . Then
A ˆB = [AB1 , . . . , ABi +aABj , . . . , ABn ] = ˆC where C = AB and ˆC is obtained by adding
a times the jth column of C to the ith column of C.
Finally suppose ˆB is produced by replacing Bi
with aBi . Then A ˆB = [AB1 , . . . , aABi
, . . . , ABn ] = ˆC where C = AB and ˆC is obtained by multiplying the ith column of C
by a..
24. det(AB) = det(A) det(B) = det(B) det(A) = det(BA).
25. det(B) = det(SAS−1) = det(S) det(A) det(S−1) =
det(S) det(S)−1 det(A) = det(A).
26.
Either det(A) = 0 or det(A) = 1.
27. det(A5) = det(A)5 = 35 = 243.
28. Set x = det(A). Then x−1 = det(A−1). Moreover, since A and A−1 both have only integer
entries, both det(A) and det(A−1) are integers. It follows that x = ±1.
29. det(Q) = −33 = (−3)(11) =
¯¯¯¯
1
2
2
1
¯¯¯¯
¯¯¯¯¯¯
1
2
2
3
5
1
1
4
1
¯¯¯¯¯¯
.
30. If A =
· 1
2
2
1
¸
and B =


1
2
2
3
5
1
1
4
1

then det(A) det(B) = (−3)(11) = −33 = det(Q).
6.5
Applications of Determinants
1.
¯¯¯¯¯¯
1
2
1
2
3
2
−1
4
1
¯¯¯¯¯¯
½ R2 −2R1
R3 + R1
¾
=
¯¯¯¯¯¯
1
2
1
0
−1
0
0
6
2
¯¯¯¯¯¯
{R3 + 6R2}
=
¯¯¯¯¯¯
1
2
1
0
−1
0
0
0
2
¯¯¯¯¯¯
= −2.

6.5. APPLICATIONS OF DETERMINANTS
187
2.
¯¯¯¯¯¯
0
3
1
1
2
1
2
−2
2
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
1
2
1
0
3
1
0
0
2
¯¯¯¯¯¯
= −6.
3.
¯¯¯¯¯¯
0
1
3
1
2
2
3
1
0
¯¯¯¯¯¯
{R1 ↔R2}
=
−
¯¯¯¯¯¯
1
2
2
0
1
3
3
1
0
¯¯¯¯¯¯
{R3 −3R1}
=
−
¯¯¯¯¯¯
1
2
2
0
1
3
0
−5
−6
¯¯¯¯¯¯
{R3 + 5R2}
=
¯¯¯¯¯¯
1
2
2
0
1
3
0
0
9
¯¯¯¯¯¯
= −9.
4.
¯¯¯¯¯¯
1
0
1
0
2
4
3
2
1
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
0
1
0
2
4
0
0
−6
¯¯¯¯¯¯
= −12.
5. adj (A) =
·
4
−2
−3
1
¸
; det(A) = −2; A−1 = (−1/2)
·
4
−2
−3
1
¸
.
6. adj (A) =
·
d
−b
−c
a
¸
; det(A) = ad −bc;
A−1 = [1/(ad −bc)]
·
d
−b
−c
a
¸
.
7. adj (A) =


0
1
−1
−2
1
0
1
−1
1

; det(A) = 1; A−1 = adj (A).
8. adj (A) =


−1
−1
1
−3
2
−2
3
−2
−3

; det(A) = −5;
A−1 = (−1/5)


−1
−1
1
−3
2
−2
3
−2
−3

.
9. adj (A) =


−4
2
0
1
0
−1
1
−2
1

; det(A) = −2;
A−1 = (−1/2)


−4
2
0
1
0
−1
1
−2
1

.

188
CHAPTER 6. DETERMINANTS
10. adj (A) =


1
−2
1
0
1
−2
0
0
1

; det(A) = 1; A−1 = adj (A).
11. W(x) =
¯¯¯¯¯¯
1
x
x2
0
1
2x
0
0
2
¯¯¯¯¯¯
= 2. Since W(0) = 2 the given set of
functions is linearly independent.
12. W(x) =
¯¯¯¯¯¯
ex
e2x
e3x
ex
2e2x
3e3x
ex
4e2x
9e3x
¯¯¯¯¯¯
= e6x
¯¯¯¯¯¯
1
1
1
1
2
3
1
4
9
¯¯¯¯¯¯
= 2e6x.
Since W(0) = 6 ̸= 0,
the set of
functions is linearly independent.
13. W(x) =
¯¯¯¯¯¯
1
cos2 x
sin2 x
0
−2 cos x sin x
2 cos x sin x
0
2 sin2 x −2 cos2 x
2 cos2 x −2 sin2 x
¯¯¯¯¯¯
=
4 cos x sin x cos 2x
¯¯¯¯¯¯
1
cos2 x
sin2 x
0
−1
1
0
−1
1
¯¯¯¯¯¯
= 0. The Wronskian gives no information, but 1 −
cos2 x −sin2 x = 0 so the set is linearly dependent.
14. W(x) = 4 sin x cos 2x −2 sin 2x cos x.W(π/4) ̸= 0 so the set of functions is linearly inde-
pendent.
15.
Note that x | x |= x2
for x ≥0
and x | x |= −x2
for x < 0.
Therefore W(x) =
¯¯¯¯
x2
x2
2x
2x
¯¯¯¯ = 0 if x ≥0 and W(x) =
¯¯¯¯
x2
−x2
2x
−2x
¯¯¯¯ when x < 0. Since W(x) = 0 for all
x, −1 ≤x ≤1, the Wronskian test is inconclusive. Thus suppose that c1x2 + c2x | x |= 0
for all x, −1 ≤x ≤1. Then for x = 1 we have c1 + c2 = 0 and for x = −1 we obtain
c1 −c2 = 0. It follows that c1 = c2 = 0 and the set {x2, x |x|} is linearly independent.
16. W(x) = 0 for all x, −1 ≤x ≤1. The set is linearly dependent since 3x2 −2(1 + x2) + (2 −
x2) = 0.
17.
The column operations C1 ↔C2, C3 −3C1, C3 + 2C2
reduce A
to the matrix L =


1
0
0
2
1
0
2
2
−1

. Therefore Q = E1E2E3 where E1 =


0
1
0
1
0
0
0
0
1

, E2 =


1
0
−3
0
1
0
0
0
1

,
and E3 =


1
0
0
0
1
2
0
0
1

.

6.5. APPLICATIONS OF DETERMINANTS
189
Multiplication yields Q =


0
1
2
1
0
−3
0
0
1

. It is easily seen that
det(Q) = det(QT) = −1.
18. E1 =


0
1
0
1
0
0
0
0
1

; E2 =


1
0
2
0
1
0
0
0
1

; E3 =


1
0
0
0
1
−5
0
0
1

;
Q = E1E2E3 =


0
1
−5
1
0
2
0
0
1

; AQ = L =


−1
0
0
3
1
0
2
1
0

;
det(Q) = det(QT) = −1.
19.
The column operations C2−2C1, C3+C1, C3+4C4 transform A to L =


1
0
0
3
−1
0
4
−8
−26

.
If E1 =


1
−2
0
0
1
0
0
0
1

, E2 =


1
0
1
0
1
0
0
0
1

,
E3 =


1
0
0
0
1
4
0
0
1

,
and Q = E1E2E3 =


1
−2
−7
0
1
4
0
0
1


then AQ = L and det(Q) = det(QT) = 1.
20. E1 =


1
−2
0
0
1
0
0
0
1

; E2 =


1
0
3
0
1
0
0
0
1

; E3 =


1
0
0
0
1
4
0
0
1

;
Q = E1E2E3 =


1
−2
−5
0
1
4
0
0
1

; AQ = L =


2
0
0
1
−1
0
3
−4
−6

;
det(Q) = det(QT) = 1.
21. det(A(x)) = x2 + 1 > 0
for all real x. adj (A(x)) =
· x
−1
1
x
¸
so A−1 = [1/(x2 +
1)]
· x
−1
1
x
¸
.
22. det(A(x)) = x2 + 2 > 0 for all real x. A−1 = [1/(x2 + 2)]
· 2
−x
x
1
¸
.

190
CHAPTER 6. DETERMINANTS
23. det(A(x)) = 4x2 + 8 > 0 for all real x. adj (A) =


x2 + 4
−2x
x2
2x
4
−2x
x2
2x
x2 + 4

so A−1 = [1/(4x2 + 8)] adj (A).
24. det(A(x)) = 1 for all x.A−1 = adj (A) =


sin x
0
−cos x
0
1
0
cos x
0
sin x

.
25. det(L) = 1
so L−1 = adj (L) =


1
0
0
−a
1
0
ac −b
−c
1

. det(U) = 1
so U −1 = adj (U) =


1
−a
ac −b
0
1
−c
0
0
1

.
26.
Let L = [lij] be a (4 x 4) nonsingular, lower-triangular matrix. Direct calculations show
that adj (L) is also a lower-triangular matrix. By Theorem 14, L−1 is lower-triangular.
27.
Clearly each cofactor, Aij of A is an integer. Therefore A−1 =
adj (A) contains only integer entries.
28.
(a)
Let A = [A1, . . . , An ]. By assumption
E = [e1, . . . , ej, . . . , ei, . . . , en]. Therefore
AE = [Ae1 . . . , Aej, . . . , Aei, . . . , Aen] =
[A1, . . . , Aj, . . . , Ai, . . . , An].
(b)
Let E = [ers]. If suﬃces to note that ers = esr when s ̸= r. If (r, s) ̸= (i, j) and
(r, s) ̸= (j, i) then ers = esr = 0. But eij = eji = 1 so E is symmetric.
29.
If A is an (n x n) skew symmetric matrix then det(AT) = det(−A) = (−1)n det(A). But
det(A) = det(AT)
so det(A) −det(AT) = det(A) −(−1)n det(A) = .
For n
odd this
implies that 2 det(A) = 0. Therefore det(A) = 0 and A is singular.
30. Let x = det(A). Then x = det(AT ) = det(A−1) = 1/x. Thus, x2 = 1 and it follows that
x = ±1.
31. Set c = det(A). Since A is nonsingular, c ̸= 0. Moreover, Adj (A) = det(A)A−1 = cA−1, so
det[Adj (A)] = det(cA−1) = cn det(A−1) = cn/ det(A) = cn/c = cn−1.
32.
(a) I = AA−1 = A[
1
det(A) Adj (A)] = [
1
det(A)A] Adj (A).
Therefore, [Adj (A)]−1 = [1/ det(A)]A.
(b) A = (A−1)−1 =
£
1/ det(A−1)
¤
Adj (A−1) = det(A) Adj (A−1). Therefore, Adj (A−1) =
[1/ det(A)]A.

6.6. SUPPLEMENTARY EXERCISES
191
6.6
Supplementary Exercises
1.
¯¯¯¯
a11
a12
a21
a22
¯¯¯¯ +
¯¯¯¯
a11
a12
b21
b22
¯¯¯¯ +
¯¯¯¯
b11
b12
a21
a22
¯¯¯¯ +
¯¯¯¯
b11
b12
b21
b22
¯¯¯¯.
2. Note that if [An−2, . . . , A1] can be obtained from [A1, . . . , An−2] with m column inter-
changes, then [A1, An−1, . . . , A2, An] can be obtained from [A1, A2, . . . , An] with m col-
umn interchanges. Thus, m + 1 column interchanges yields [An, An−1, . . . , A1]. Therefore,
if n = 4k or n = 4k + 1 then an even number of column interchanges is required and
det(B) = det(A). If n = 2k or n = 2k + 1, where k is odd, then an odd number of column
interchanges is required and det(B) = −det(A).
3. If x = det(A) then x3 = x so x = 0, x = 1, or x = −1.
4. Let x = det(A). Then x ̸= 0, x = det(AT ), and if A is a (2 × 2) matrix, det(cA) = c2x.
From x = c2x it follows that c = ±1. Similarly, if A is a (3 × 3) matrix, c3 = 1 so c = 1.
5. AB =


0
0
2
0
2
0
2
0
0


6. Det (A) = a11A11 + a12A12 + a13A13 = −1 and A−1 = (1/ det(A)) Adj (A) = −CT . There-
fore, A = (−CT )−1 =


1
2
−1
3
1
4
2
2
1

.
7. Det (B + I) = det[b + e1, b + e2, . . . , b + en] = Pn
i=1 det(Ai) + det(I) = b1 + · · · + bn + 1.
8. (x5 + x3)ex.
6.7
Conceptual Exercises
1. True. A is nonsingular so B = A−1(AB) = A−1(AC) = C.
2. True. Det (AB) = det(A) det(B) = det(B) det(A) = det(AB).
3. False. If A = In then det(cIn −A) = (c −1)n.
4. False. Det (cA) = cn det(A).
5. True. 0 = det(Ak) = (det(A))k, so det(A) = 0.
6. True. 0 ̸= det(B) = det(A1) · · · det(Am), so det(Ai) ̸= 0 for 1 ≤i ≤m.
7. True. If C = [Aij] is the cofactor matrix for A then C is symmetric.

192
CHAPTER 6. DETERMINANTS
8. True. A−1 = Adj (A).
9. If x = det(A) and A2 = −I then x2 = −1, which is not possible.
10. A is nonsingular (cf. Exercise 6) so A−1 exists. Thus I = A−1IA = A−1(AB)A = BA.
11. AT −cI = (A −cI)T .
12.
(a) Det (B−1AB −cI) = det[B−1(A−cI)B] = det(B−1) det(A−cI) det(B) = det(A−cI).
(b) Det (AB −cI) = det(B−1(BA)B −cI) = det(BA −cI).
13. A[Adj (A)] = (det(A))I so, by Exercise 6, Adj (A) is nonsingular.
14.
(a) If A is nonsingular then B = IB = A−1(AB) = A−1O = O.
Similarly, if B is
nonsingular, then A = O. If follows that A and B are both singular.
(b) If A is singular then det(A) = 0 so A[Adj (A)] = O. By (a), Adj (A) is singular.
15. AT = A−1 = (1/ det(A)) Adj (A) so Adj (A)T = (det(A))A.

Chapter 7
Eigenvalues and Applications
7.1
Quadratic Forms
1. A =
· 2
2
2
−3
¸
.
2. A =
· −1
3
3
1
¸
.
3. A =


1
1
−3
1
−4
4
−3
4
3

.
4. A =


1
1
5
−2
1
0
2
−1
5
2
4
3
−2
−1
3
−1

.
5. A =
· 2
2
2
1
¸
.
6. A =


1
4
2
4
2
3
2
3
1

.
7. q(x) = xTAx where A =
· 2
3
3
2
¸
. A has eigenvalues λ1 = 5, λ2 = −1 with corresponding
eigenvectors a
· 1
1
¸
, b
·
1
−1
¸
,
respectively, where a ̸= 0 and b ̸= 0. In particular Q =
(1/
√
2)
· 1
1
1
−1
¸
.
The form is indeﬁnite.

194
CHAPTER 7. EIGENVALUES AND APPLICATIONS
8. q(x) = xTAx
for A =
·
5
−2
−2
5
¸
. A has eigenvalues λ1 = 7 and λ2 = 3. We may take
Q = (1/
√
2)
· −1
1
1
1
¸
.
The form is positive deﬁnite.
9. q(x) = xTAx
for A =


1
2
2
2
1
2
2
2
1

.
The eigenvalues for A are λ1 = 5 and λ2 = −1
(algebraic multiplicity 2). An eigenvector for λ1 = 5
is u1 = [1, 1, 1]T.
The vectors w2
= [−1, 1, 0]T
and w3 = [−1, 0, 1]T
are eigenvectors for λ2 = −1. The Gram-Schmidt process yields orthogonal eigenvectors
u2 = w2 = [−1, 1, 0]T and u3 = [−1, −1, 2]T.
We form Q by normalizing the set {u1 , u2 , u3 } of eigenvectors;
Q =


1/
√
3
−1/
√
2
−1/
√
6
1/
√
3
1/
√
2
−1/
√
6
1/
√
3
0
2/
√
6

.
The form is indeﬁnite.
10. q(x) = xTAx
for A =


1
1
1
1
1
1
1
1
1

. A has eigenvalues λ1 = 3 and λ2 = 0
(algebraic
multiplicity 2). One choice for Q is
Q =


1/
√
3
−1/
√
2
−1/
√
6
1/
√
3
1/
√
2
−1/
√
6
1/
√
3
0
2/
√
6

.
The form is positive semideﬁnite.
11. q(x) = xTAx
where A =
·
3
−1
−1
3
¸
. A
has eigenvalues λ1 = 2
and λ2 = 4
with
corresponding eigenvectors u1 = [1, 1]T and u2 =
[−1, 1]T,
respectively. The set {u1 , u2 }
is orthogonal. We normalize u1
and u2
to
obtain Q; Q = (1/
√
2)
· 1
−1
1
1
¸
.
The form is positive deﬁnite.
12. q(x) = xTAx where A =


1
−1
−1
−1
−1
1
−1
−1
−1
−1
1
−1
−1
−1
−1
1

. A has eigenvalues λ1 = 2 (with algebraic

7.1. QUADRATIC FORMS
195
multiplicity 3) and λ2 = −2. We may take Q =


−1/
√
2
−1/
√
6
−1/2
√
3
1/2
1/
√
2
−1/
√
6
−1/2
√
3
1/2
0
2/
√
6
−1/2
√
3
1/2
0
0
3/2
√
3
1/2

. The
form is indeﬁnite.
13.
Set q(x) = 2x2 +
√
3 xy + y2. Then q(x) = xTAx
for A =
·
2
√
3/2
√
3/2
1
¸
. A
has eigenvalues λ1 = 1/2, λ2 = 5/2 with corresponding eigenvectors
u1 = [−1,
√
3]T and u2 = [
√
3, 1]T, respect- ively. Since {u1 , u2 } is an orthogonal set
we may normalize and
obtainQ =
· −1/2
√
3/2
√
3/2
1/2
¸
.
The substitution x = Qy
yields
q(x ) = (1/2)u2 + (5/2)v2 = 10. The graph corresponds to the ellipse
u2/20 + v2/4 = 1.
14. Q = (1/
√
2)
· 1
−1
1
1
¸
and the graph corresponds to the ellipse
u2/2 + v2/4 = 1.
15.
Set q(x) = x2 + 6xy −7y2. Then q(x) = xTAx
where A =
· 1
3
3
−7
¸
.
A has eigenvalues λ1 = −8 and λ2 = 2 with corresponding eigenvectors u1 = [−1, 3]T
and u2 = [3, 1]T, respectively. Since {u1 , u2 } is an orthogonal set we may normalize to
obtain Q = (1/
√
10)
· −1
3
3
1
¸
.
The substitution x = Qy
yields q(x) = −8u2 + 2v2 = 8. The graph corresponds to the
hyperbola v2/4 −u2 = 1.
16. Q = (1/
√
2)
· 1
−1
1
1
¸
and the graph corresponds to the ellipse u2 + v2/5 = 1.
17.
If q(x) = xy then q(x) = xTAy where A =
· 0
1
1
0
¸
. The mat- rix A is diagonalized by
Q = (1/
√
2)
· 1
−1
1
1
¸
and QTAQ = D =
· 1
0
0
−1
¸
.
The substitution x = Qy
yields
q(x) = u2 −v2 = 4. The graph corresponds to the hyperbola u2/4 −v2/4 = 1.
18. Q =
· √
3/2
1/2
1/2
−
√
3/2
¸
.
The transformed equation is the parabola v = −2u2 −
√
3 u + 2.

196
CHAPTER 7. EIGENVALUES AND APPLICATIONS
19.
If q(x) = 3x2 −2xy + 3y2 then q(x) = xTAx
where A =
·
3
−1
−1
3
¸
.
A is diagonalized by Q = (1/
√
2)
· −1
1
1
1
¸
and QTAQ = D where D =
· 4
0
0
2
¸
.
The
transformed equation is the ellipse 4u2 + 2v2 = 16, or u2/4 + v2/8 = 1.
20. Q = (1/
√
2)
· 1
−1
1
1
¸
and the transformed equation is 2u2 = −1.
21.
Note that aii = eiTAei = eiTCei = cii for 1 ≤i ≤n. For r ̸= s set x = er + es. Then
xTAx = arr + ars + asr + ass = arr + 2ars + ass,
since A is symmetric. Similarly xTCx
= crr + 2crs + css it follows that ars = crs.
22.
(a)
Suppose that Ax = λix
where x ̸=θ. Then 0 < xTAx = λixTx =
λi ∥x∥2. It follows that λi > 0.
(b)
By Equation (3) q(x) = λ1y2
1 + λ2y2
2 + · · · + λny2
n
where y = [y1, . . . , yn]T
and x
= Qy . If x ̸=θ then y ̸=θ so q(x) > 0.
23.
See Exercise 22.
24.
See Exercise 22.
25. If q(x) = xTAx
is indeﬁnite then it follows from properties (a) - (d) of Theorem 2 that
A has positive and negative eigenvalues. Conversely assume that A has eigenvalues λ1 > 0
and λ2 < 0 and let x1 and x2 be corresponding eigenvectors, respectively. Then x1TAx1 =
λ1 ∥x∥2
1> 0 and x2TAx2 = λ2 ∥x∥2
2< 0. This shows that q(x) is indeﬁnite.
26.
Following the hint we obtain R(x ) = (Pn
i=1 a2
1λi)/(Pn
i=1 a2
i ). There-
fore λ1 = (λ1
Pn
i=1 a2
i )/(Pn
i=1 a2
i ) ≤(Pn
i=1 a2
i λi)/(Pn
i=1 a2
i ) =
R(x) ≤(λn
Pn
i=1 a2
i )/(Pn
i=1 a2
i ) = λn.
27.
If ∥x∥= 1 then R(x) = xTAx= q(x) [cf. Exercise 26]. By Exercise 26, λ1 ≤R(x) ≤λn.
Let u1
and un
be eigenvectors corresponding to λ1 and λn, respectively, where ∥u1 ∥
=∥un ∥= 1.
Then R(u1) = u1TAu1 = λ1 and, similarly, R(un) = λn.
28.
(a) BT = (STAS)T = STATSTT = STAS = B.
(b)
Suppose that q1
is positive deﬁnite and assume x ̸=θ.
Since S
is nonsingular y
= Sx̸=θ. Therefore q2(x) = xTBx= x TSTASx= (Sx)TA(Sx) = yTAy = q1(y) > 0.
This shows that q2 is positive deﬁnite. The reverse argument is similar.

7.2. SYSTEMS OF DIFFERENTIAL EQUATIONS
197
7.2
Systems of Diﬀerential Equations
1.
The given system has matrix equation x
′(t) = Ax (t) where
x (t) = [u(t), v(t)]T and A =
· 5
−2
6
−2
¸
. The eigenvalues for A
are λ1 = 1 and λ2 = 2 and the corresponding eigenvectors are u1 = [1, 2]T, u2 = [2, 3]T.
Thus x1 (t) = etu1
and x2 (t) = e2tu2
are solutions. The general solution is given by x
(t) = b1x1 (t)+
b2x2 (t); that is x (t) = b1et
· 1
2
¸
+ b2et
· 2
3
¸
. It is easily seen that
x0 = u1 +2u2
so the solution x (t) = et
· 1
2
¸
+ 2e2t
· 2
3
¸
=
· et + 4e2t
2et + 6e2t
¸
satisﬁes the
initial condition.
2. A =
·
2
−1
−1
2
¸
.
The general solution is x(t) = b1et
· 1
1
¸
+ b2e3t
· −1
1
¸
.
The solution
that satisﬁes the initial condition is x (t) = (1/2)et
· 1
1
¸
−(3/2)e3t
· −1
1
¸
.
3.
The system is x
′(t) = Ax (t) where A =
· 1
1
2
2
¸
. A has eigen-
values λ1 = 0 and
λ2 = 3 with corresponding eigenvectors u1 =
[−1, 1]T and u2 = [1, 2]T. The general solution is given by x (t) =
b1
· −1
1
¸
+ b2e3t
· 1
2
¸
. The particular solution that satisﬁes the
initial condition is x (t) = −3
· −1
1
¸
+ 2e3t
· 1
2
¸
=
·
3 + 2e3t
−3 + 4e3t
¸
.
4. A =
· 5
−6
3
−4
¸
. The general solution is x (t) = b1e2t
· 2
1
¸
+
b2e−t
· 1
1
¸
. The solution that satisﬁes the initial condition is
x (t) = e2t
· 2
1
¸
+ e−t
· 1
1
¸
=
· 2e2t + e−t
e2t + e−t
¸
.
5.
The system is x
′(t) = Ax(t) where A =
·
0.5
0.5
−0.5
0.5
¸
. A has eigenvalues λ1 = 0.5+0.5i
and λ2 = 0.5 −0.5i with corresponding eigenvectors u1 = [−i, 1]T and u2 = [i, 1]T. The
general solution is x (t) = b1eλ1tu1 +b2eλ2tu2
where eλ1t = e(0.5+0.5i)t =
et/2[cos (t/2)+i sin (t/2)]
and eλ2t = e(0.5−0.5i)t = et/2[cos (t/2)−i sin (t/2)]. The equation

198
CHAPTER 7. EIGENVALUES AND APPLICATIONS
x0 = b1u1 +b2u2
has solution b1 = 2 + 2i
and b2 = 2 −2i so the particular solution that
satisﬁes the initial
condition is x (t) = 4e(t/2)
· cos (t/2) + sin (t/2)
cos (t/2) −sin (t/2)
¸
.
6. A =
·
6
8
−1
2
¸
. The general solution is
x(t) = b1e(4+2i)t
· −2 −2i
1
¸
+b2e(4−2i)t
· −2 + 2i
1
¸
. The solution that satisﬁes the initial
condition is x (t) = 2ie(4+2i)t
· −2 −2i
1
¸
−
2ie(4−2i)t
· −2 + 2i
1
¸
= 4e4t
· 2 cos 2t + 4 sin 2t
−sin 2t
¸
.
7.
The system is x
′(t) = Ax(t) where A =


4
0
1
−2
1
0
−2
0
1

. A has eigenvalues λ1 = 1, λ2 = 2,
and λ3 = 3
with corresponding eigenvectors u1 = [0, 1, 0]T, u2 = [1, −2, −2]T,
and u3
= [−1, 1, 1]T, respectively. Therefore the general solution is x (t) =
b1et


0
1
0

+ b2e2t


1
−2
−2

+ b3e3t


−1
1
1

.
Since x0 = u1 +u2 +2u3
the solution x
(t) = et


0
1
0

+ e2t


1
−2
−2

+ 2e3t


−1
1
1


satisﬁes the initial condition.
8. A =


3
1
−2
−1
2
1
4
1
−3

. The general solution is
x (t) = b1et


1
0
1

+ b2e−t


7
−2
13

+ b3e2t


1
1
1

. The solution x (t) = 3et


1
0
1

−
e−t


7
−2
13

+ 2e2t


1
1
1


satisﬁes the initial condition.
9.
(a)
The system is x
′(t) = Ax (t) for A =
· 1
−1
1
3
¸
. A has only one eigenvalue, λ = 2,
with corresponding eigenvector u = [1, −1]T.
Therefore x1 (t) = e2t
·
1
−1
¸
is a solution for the system.

7.3. TRANSFORMATION TO HESSENBERG FORM
199
(b)
Set x2 (t) = teλtu +eλty0 . Then x2
′(t) = eλt(tλu +u + λy0)
whereas Ax2 (t) = eλt(tλu +Ay0 ). Therefore we require that
Ay0 = u +λy0 ; that is (A −λI)y0 = u .
One choice is y0 =
[−2, 1]T.
Thus x2 (t) = te2t
·
1
−1
¸
+ e2t
· −2
1
¸
is a solution.
(c)
If y(t) = c1x1 (t)+c2x2 (t) note that y(0) = c1u+c2y0 . Since {u, y0 } is a linearly
independent set for every x0 in R2 we may ﬁnd c1 and c2 such that x0 = c1u+c2y0
.
10.
(a) A =
· 2
−1
4
6
¸
and x1 (t) = e4t
·
1
−2
¸
.
(b)
One choice for y0
is y0 = [0, −1]T. In this case x2 (t) =
te4t
·
1
−2
¸
+ e4t
·
0
−1
¸
.
(c)
The solution x (t) = x1 (t) −3x2 (t) =
· e4t(1 −3t)
e4t(1 + 6t)
¸
satisﬁes the initial condition.
7.3
Transformation to Hessenberg Form
1.
The desired elementary row operation is R3 −4R2.
Performing this operation on the (3
x 3) identity matrix yields Q1 =


1
0
0
0
1
0
0
−4
1

.
Q−1
1
=


1
0
0
0
1
0
0
4
1

and Q1AQ−1
1
= H =


−7
16
3
8
9
3
0
1
1

.
2. Q1 =


1
0
0
0
1
0
0
2
1

; H = Q1AQ−1
1


−6
31
−14
−1
6
−2
0
2
1

.
3.
Let Q1 denote the permutation matrix Q1 =


1
0
0
0
0
1
0
1
0

. Then Q1 = Q−1
1
and Q1A
interchanges the second and third rows of A.
Further (Q1A)Q1 interchanges the second
and third columns of Q1A.
Therefore H = Q1AQ−1
1
=


1
1
3
1
3
1
0
4
2

.
4. Q1 =


1
0
0
0
1
0
0
2
1

and H = Q1AQ−1
1
=


1
4
−1
3
0
1
0
−5
5

.

200
CHAPTER 7. EIGENVALUES AND APPLICATIONS
5.
The desired elementary row operation is R3 + 3R2. Performing this operation on the (3
x 3) identity matrix yields Q1 =


1
0
0
0
1
0
0
3
1

. Q−1
1
=


1
0
0
0
1
0
0
−3
1

and H = Q1AQ−1
1
=


3
2
−1
4
5
−2
0
20
−6

.
6. Q1 =


1
0
0
0
0
1
0
1
0

and H = Q1AQ−1
1
=


4
3
0
3
1
2
0
2
1

.
7.
Performing the elementary row operations R3 −R2 and R4 −R2 on the (4 x 4) identity
matrix yields Q1 =


1
0
0
0
0
1
0
0
0
−1
1
0
0
−1
0
1

. Q−1
1
=


1
0
0
0
0
1
0
0
0
1
1
0
0
1
0
1


and H = Q1AQ−1
1
=


1
−3
−1
−1
−1
−1
−1
−1
0
0
2
0
0
0
0
2

.
8.
Let Q1 =


1
0
0
0
0
1
0
0
0
−4
1
0
0
−4
0
1

and Q2 =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
−1
1

.
Then H = Q2Q1AQ−1
1 Q−1
2
=


6
33
8
4
1
38
8
4
0
−120
−25
−15
0
0
0
5

.
9.
If Q1 =


1
0
0
0
0
0
0
1
0
0
1
0
0
1
0
0


then Q1 = Q−1
1
and Q1A
interchanges
the second and
fourth rows of A whereas (Q1A)Q1 interchanges the
second and fourth columns 0f Q1A.
Therefore
Q1AQ1 =


1
3
1
2
1
2
0
2
0
1
1
3
0
2
1
1

.

7.3. TRANSFORMATION TO HESSENBERG FORM
201
Now the desired elementary row operation is R4 −2R3
so set Q2 =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
−2
1

.
Then Q−1
2
=


1
0
0
0
0
1
0
0
0
0
1
0
0
0
2
1

and
H = Q2Q1AQ−1
1 Q−1
2
=


1
3
5
2
1
2
4
2
0
1
7
3
0
0
−11
−5

.
10.
If Q1 =


1
0
0
0
0
1
0
0
0
2
1
0
0
1
0
1

and Q2 =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
−5/3
1

Then
H = Q2Q1AQ−1
1 Q−1
2
=


2
−1
−5/3
−1
−1
2
−1/3
1
0
0
7
6
0
0
0
0

.
11.
Since λ is an eigenvalue for H, the matrix H −λI is singular; that is nullity (H −λI) ≥1.
It follows that rank (H −λI) ≤3. But H −λI =


a1 −λ
b1
c1
d1
a2
b2 −λ
c2
d2
0
b3
c3 −λ
d3
0
0
c4
d4 −λ

and
clearly the ﬁrst three columns of H −λI are linearly independent. Therefore
rank (H −λI) ≥3. Thus rank (H −λI) = 3 and so, nullity (H −λI) = 1. Hence, λ has
geometric multiplicity equal to 1.
12.
Since H
is similar to a symmetric matrix, H
is diagonizable. Therefore the algebraic
multiplicity for λ equals the geometric multiplicity. Now apply Exercise 11.
13. p(t) = (t −2)3(t + 2) is the characteristic polynomial for H. Since H and A are similar,
p(t) is also the characteristic polynomial for A. Therefore A has eigenvalues λ1 = 2 and
λ2 = −2 and λ1 = 2 has algebraic (and hence geometric) multiplicity 3 (cf. Exercise 12).
14. p(t) = (t −5)2(t −15)(t + 1) so A has eigenvalues λ1 = 5, λ2 = 15, and λ3 = −1.
15. [e1, e2, e3], [e1, e3, e2], [e2, e1, e3], [e2, e3, e1], [e3, e1, e2],
[e3, e2, e1].

202
CHAPTER 7. EIGENVALUES AND APPLICATIONS
16. [e1, e2, e3, e4], [e1, e2, e4, e3], [e1, e3, e2, e4], [e1, e3, e4, e2],
[e1, e4, e2, e3], [e1, e4, e3, e2], [e2, e1, e3, e4], [e2, e1, e4, e3],
[e2, e3, e1, e4], [e2, e3, e4, e1], [e2, e4, e1, e3], [e2, e4, e3, e1],
[e3, e1, e2, e4], [e3, e1, e4, e2], [e3, e2, e1, e4], [e3, e2, e4, e1],
[e3, e4, e1, e2], [e3, e4, e2, e1], [e4, e1, e2, e3], [e4, e1, e3, e2],
[e4, e2, e1, e3], [e4, e2, e3, e1], [e4, e3, e1, e2], [e4, e3, e2, e1].
17.
There are n! (n x n) permutation matrices.
18.
Since the columns of P
are some ordering of e1, e2, . . . , en, they form an orthonormal
set.
19. AP = A[ei, ej, ek, . . . , er] = [Aei , Aej , Aek , . . . , Aer ] =
[Ai, Aj, Ak, . . . , Ar].
20.
Let A =


a1
a2
...
an

where aj is the jth row of A.
By Exercise 19, ATP = [aiT, ajT, akT, . . . , arT].
Therefore P TA = (ATP)T =


ai
aj
ak
...
ar


.
21.
Apply Exercise 19.
22.
By Exercise 21 each of the matrices P 1, P 2, P 3, . . . is a permutation matrix. By Exercise
17 there are n!
distinct (n x n)
permutation matrices. Therefore there exists integers r
and s such that r > s and P r = P s. Since P
is nonsingular this implies that P r−s = I.
7.4
Eigenvalues of Hessenberg Matrices
1.
Note that the given matrix H
is in unreduced Hessenberg form.
We have w0 = e1
= [1, 0]T, w1 = Hw0 = [2, 1]T, and w2 = Hw1 = [4, 3]T. The vector equation a0w0 +a1w1
= −w2 is equivalent to the system
a0
+
2a1
=
−4
a1
=
−3 .
The system has solution a0 = 2, a1 = −3 so p(t) = 2 −3t + t2.

7.4. EIGENVALUES OF HESSENBERG MATRICES
203
2. w0 = e1 = [1, 0]T; w1 = [0, 3]T; w2 = [0, 0]T,
The vector equation a0w0 +a1w1 = −w2
has solution a0 = a1 = 0, so p(t) = t2.
3.
Note that the given matrix H
is in unreduced
Hessenberg form. We have w0 = e1
= [1, 0, 0]T, w1 = Hw0 = [1, 2, 0]T, w2 = Hw1 = [1, 4, 2]T, and w3 = Hw2 = [3, 6, 8]T. The
vector equation
a0w0 + a1w1 + a2w2 = −w3 is equivalent to the system of equations
a0
+
a1
+
a2
=
−3
2a1
+
4a2
=
−6
2a2
=
−8
.
The system has unique solution a0 = −4, a1 = 5, a2 = −4 so p(t) = −4 + 5t −4t2 + t3.
4. w0 = e1 = [1, 0, 0]T; w1 = [1, 1, 0]T; w2 = [3, 4, 1]T; w3 = [12, 14, 6]T.
The vector equation a0w0 +a1w1 +a2w2 = −w3
has solution
a0 = −4, a1 = 10, a2 = −6, so p(t) = −4 + 10t −6t2 + t3.
5.
Note that the given matrix H
is in unreduced Hessenberg form. We have w0 = e1
= [1, 0, 0]T, w1 = Hw0 = [2, 1, 0]T, w2 = Hw1 = [8, 3, 1]T, and w3 = Hw2 = [29, 14, 8]T.
The vector equation a0w0 +a1w1 +a2w2 = −w3 is equivalent to the system of equations
a0
+
2a1
+
8a2
=
−29
a1
+
3a2
=
−14
a2
=
−8
.
The system has unique solution a0 = 15, a1 = 10, a2 = −8 so p(t) = 15 + 10t −8t2 + t3.
6. w0 = e1; w1 = e2; w2 = e3, w3 = e1. The vector equation a0w0 + +a1w1 + a2w2 = −w3
has solution a0 = −1, a1 = a2 = 0. Therefore p(t) = −1 + t3.
7.
Note that the given matrix H
is in unreduced
Hessenberg form. We have w0 = e1
= [1, 0, 0, 0]T, w1 = Hw0 = [0, 1, 0, 0]T,
w2 = Hw1 = [1, 2, 1, 0]T, w3 = Hw2 = [2, 6, 2, 2]T, and w4 =
Hw3= [8, 18, 8, 6]T. The vector equation a0w0+a1w1+a2w2+a3w3 = −w4 is equivalent
to the system of equations
a1
+
a2
+
2a3
=
−8
a1
+
2a2
+
6a3
=
−18
a2
+
2a3
=
−8
2a3
=
−6
.
The system has unique solution a0 = 0, a1 = 4, a2 = −2, a3 = −3, so p(t) = 4t−2t2−3t3+t4.

204
CHAPTER 7. EIGENVALUES AND APPLICATIONS
8. w0 = e1 = [1, 0, 0, 0]T, w1 = [0, 1, 0, 0]T, w2 = [2, 0, 2, 0]T, w3 = [2, 4, 0, 2]T,
and w4 =
[12, 0, 12, 2]T. The vector equation a0w0 +
a1w1 +a2w2 +a3w3 = −w4
has solution a0 = 2, a1 = 4, a2 = −6,
a3 = −1. Therefore p(t) = 2 + 4t −6t2 −t3 + t4.
9. H =
· B11
B12
O
B22
¸
where B11 =
· 1
−1
1
3
¸
, B12 =
·
1
4
−2
1
¸
and B22 =
·
2
−1
−1
2
¸
.
B11 has eigenvalue λ1 = 2 (with algebraic multiplicity 2) with corresponding eigenvector
u1 = [−1, 1]T.B22 has eigenvalues λ2 = 1, λ3 = 3 with corresponding eigenvectors
v2= [1, 1]T and v3= [−1, 1]T, respectively. Thus H has eigenvalues λ1 = 2, λ2 = 1, λ3 = 3.
The vector x1 =
· u1
θ
¸
= [−1, 1, 0, 0]T
is an eigenvector for H
corresponding to
λ1 = 2.
The system of equations (B11 −I)u = −B12v2
has solution u2 = [−9, 5]T,
so
x2 =
· u2
v2
¸
=
[−9, 5, 1, 1]T is an eigenvector of H corresponding to λ2 = 1. Similarly
(B11 −3I)u = −B12v3 has solution u3 = [−3, 9]T so x3 =
· u3
v3
¸
= [−3, 9, −1, 1]T is an
eigenvector of H corresponding to λ3 = 3.
10. B11 =
· 1
1
1
1
¸
and B22 =
· 3
0
1
4
¸
. B11
has eigenvalues λ1 = 0 and λ2 = 2 and B22
has eigenvalues λ3 = 3, λ4 = 4. The corresponding eigenvectors are x1 = [−1, 1, 0, 0]T, x2
= [1, 1, 0, 0]T, x3 = [0, 1, −1, 1]T
and x4 = [3/4, 5/4, 0, 1]T.
11. H =
· B11
B12
O
B22
¸
where B11 =


−2
0
−2
−1
1
−2
0
1
−1

, B12 =


1
3
−2

,
and B22 = [2]. B11 has eigenvalues λ1 = 0 and λ2 = −1 (algebraic multiplicity 2) with
corresponding eigenvectors u1 = [−1, 1, 1]T and u2 = [−2, 0, 1]T.B22 has eigenvalue λ3 = 2
with corresponding eigenvector v3 = [1].
Thus H has eigenvalues λ1 = 0, λ2 = −1, and
λ3 = 2.
The vectors x1 =
· u1
θ
¸
= [−1, 1, 1, 0]T
and x2 =
· u2
θ
¸
= [−2, 0, 1, 0]T
are
eigenvectors for H
corresponding to λ1 = 0 and λ2 = −1, respectively. The system of
equations (B11 −2I)u1 = −B12v3 has solution u3 = [1/6, 15/6, 1/6]T so x3 =
· u3
v3
¸
=
[1/6, 15/6, 1/6, 1]T is an eigenvector for H corresponding to λ3 = 2.
12. B11 =
· 2
3
3
2
¸
and B22 =
· 3
0
1
3
¸
.
B11
has eigenvalues λ1 = 5 and λ2 = −1 and
B22 has eigenvalue λ3 = 3. The corresponding eigenvectors for H are x1 = [1, 1, 0, 0]T, x2
= [−1, 1, 0, 0]T, and x3 = [−7/8, −13/8, 0, 1]T.

7.4. EIGENVALUES OF HESSENBERG MATRICES
205
13. det(B) = afwz −afyx −ebwz + ebyx = (af −eb)(wz −yx) = det(B11) det(B22).
14. det(H) =
¯¯¯¯
1
−1
1
3
¯¯¯¯ x
¯¯¯¯
2
−1
−1
2
¯¯¯¯ = (4)(3) = 12.
15. P = [e2 , e3 , e1 ].
16. P = [e2 , e3 , e4 , e1 ].
17. P = [e2 , e3 , . . . , en , e1 ].
18.
Write P = [P1, P2, . . . , Pn] where, as shown in Exercise 17,
P1 = e2, P2 = e3, . . . , Pn−1 = en, and Pn = e1. Thus w0 = e1, w1 = Pw0 = Pe1 =
P1 = e2, w2 = Pw1 = Pe2 = P2 = e3, . . . , wn−1 = Pwn−2 = Pen−1 = Pn−1 = en,
and wn = Pwn−1 = Pen = Pn = e1.
Obviously the vector equation a0w0 + a1w1+
· · ·+an−1wn−1 = −wn has solution a0 = −1, a1 = · · · = an−1 = 0. Therefore p(t) = tn−1.
19.
Let H = [hij] and let λ be an eigenvalue for H, Then
H −λI =


h11 −λ
h12
· · ·
h1,n−1
h1n
h21
h22 −λ
h2,n−1
h2n
0
h32
h3,n−1
h3n
...
...
...
...
0
0
hn−1,n−1 −λ
hn−1,n
0
0
hn,n−1
hnn −λ


.
Since h21, h32, . . . , hn,n−1 are nonzero, the ﬁrst n −1 columns of
H −λI form a linearly independent set. Therefore rank (H −λI) ≥
n −1. It follows that nullity (H −λI) ≤1. Since λ is an eigenvalue for H it follows that
nullity (H −λI) = 1; that is, λ has geometric multiplicity 1.
20.
Since H is symmetric, it is diagonalizable. Therefore the algebraic multiplicity of λ equals
the geometric multiplicity. It follows from Exercise 19 that λ
has algebraic multiplicity 1.
Therefore H neces-
sarily has n distinct eigenvalues.
21.
If H is unreduced then b ̸= 0. Thus p(t) = t2 −(a + c)t −b2. The eigenvalues for H are
λ = [(a + c) ±
p
(a + c)2 + 4b2]/2. Since
(a + c)2 + 4b2 > 0, H has two distinct eigenvalues.
22.
Set u = [u1, u2, . . . , un]T and assume that un = 0. Set H = [h1, h2,
. . . , hn]. Thus λu = H u = u1h1 + u2h2 + · · · + un−1hn−1.
There-

206
CHAPTER 7. EIGENVALUES AND APPLICATIONS
fore the nth
component of H u is un−1hn,n−1. Since hn,n−1 ̸= 0 it
follows that un−1 = 0. Repetition of this argument yields
u1 = u2 = · · · = un = 0, so u = θ. Therefore if u ̸= θ then un ̸= 0.
23.
Let k be an integer, 1 ≤k ≤n, and suppose we have shown that wk−1
has the form
wk−1 = [a1 . . . , ak, 0, . . . , 0]T,
where ak ̸= 0.
If H = [h1 , . . . , hn ]
then wk = H wk−1
= a0h1 + · · · + akhk . But H is in Hessenberg form so hij = 0 when i > j + 1. Therefore
the k + 1 component of wk
is akhk+1,k and is nonzero since H is unreduced. Thus wk
has the form wk = [b1, . . . , bk+1, 0, . . . , 0]T, where
bk+1 ̸= 0.
7.5
Householder Transformations
1. Qx = x −γu
where γ = 2uTx /uTu = (−2)(2)/4 = −1. Thus Qx = [4, 1, 6, 7]T.
2. Qx = [4, −3, 5, 4[T.
3.
Set γ1 = 2uTA1 /uTu = −1
and γ2 = 2uTA2 /uTu = −2.
Then QA1 = A1 −γ1u
= [3, 5, 5, 1]T and QA2 = A2 −γ2u = [3, 1, 4, 2]T.
Therefore QA =


3
3
5
1
5
4
1
2

.
4. QA =


2
3
1
0
0
2
3
6
2
1
5
3

.
5.
Set γ = 2uTx /uTu = −1. Then Qx = x −γu = x +u = [4, 1, 3, 4]T.
Thus xTQ = (Qx )T = [4, 1, 3, 4].
6. xTQ =
[2, 2, 3, 1].
7.
Set x = [2, 1, 2, 1]T and y = [1, 0, 1, 4]T. Then QAT = Q[x , y ] =
[Qx , Qy ] =


1
2
2
−1
1
2
2
3

. Therefore AQ = (QAT)T =
· 1
2
1
2
2
−1
2
3
¸
.

7.5. HOUSEHOLDER TRANSFORMATIONS
207
8. BQ =


1/2
5/2
7/2
5/2
6
0
−1
−1
−7/2
13/2
11/2
5/2
3
−7
1
5

.
9.
Set u1 = 0.
If a = −√4 + 4 + 1 = −3
then u2 = v2 −a = 2 + 3 = 5.
Finally take
u3 = v3 = 2 and u4 = v4 = 1. Thus u = [0, 5, 2, 1]T.
10. a = −2, u = [3, 1, 1, 1]T.
11. a = −
√
42 + 32 = −5; u1 = u2 = 0; u3 = v3 −a = 4 + 5 = 9; u4 = v4 = 3. Therefore u
= [0, 0, 9, 3]T.
12. a = 3; u = [0, 0, −5, 2, 1]T.
13. a =
p
(−3)2 + 42 = 5; u1 = u2 = u3 = 0; u4 = v4 −a = −8; u5 = v5 = 4.
Therefore u
= [0, 0, 0, −8, 4]T.
14. a = −4; u = [0, 0, 8, 0, 0]T.
15.
We want QA1 = [1, a, 0]T.
Therefore a = −
√
32 + 42 = −5 and
u = [u1, u2, u3]T where u1 = 0, u2 = 3 −(−5) = 8, and u3 = 4.
Then u = [0, 8, 4]T.
16. u = [0, −5, 5]T.
17.
We want QA1 =


0
a
0

. Therefore a =
p
(−4)2 + 32 = 5 and
u = [u1, u2, u3]T
where u1 = 0, u2 = −4 −5 = −9, and u3 = 3.
Thus u = [0, −9, 3]T.
18. u = [0, 0, 8, 4]T.
19.
We want QA2 =


1
4
a
0

so a =
p
(−3)2 + 42 = 5. u =
[u1, u2, u3, u4]T where u1 = u2 = 0, u3 = −3 −5 = −8, and u4 = 4.
Thus u = [0, 0, −8, 4]T.
20. u = [0, 0, −1, 1]T.
21.
QT = (I −buuT)T = IT −(buuT)T = I −buTTu T = I −buuT = Q.

208
CHAPTER 7. EIGENVALUES AND APPLICATIONS
22.
Set b = 2/uTu . Then Qu = (I −buuT)u = Iu −(buuT )u =
u −bu (uTu ) = u −2u = −u . If uTv = 0 then Qv = (I −buuT)v =
Iv −bu (uTv ) = v .
23.
Let {u, w2, . . . , wn} be as given in the hint. By Exercise 22, Qu = −u
and Qwi = wi
for 2 ≤i ≤n.
Thus Rn
has a basis consisting of eigenvectors for Q;
that is Q
is
diagonalizable. Moreover Q
is similar to the (n x n)
diagonal matrix D
with diagonal
entries d11 = 1, d22 = · · · = dnn = −1. Since Q and D have the same eigenvalues, Q has
eigenvalues 1 and −1.
24.
To prove (a) note that Q−1 = (Qn−2 · · · Q2Q1)−1 =
Q−1
1 Q−1
2
· · · Q−1
n−2 = QT
1 QT
2 · · · QT
n−2 = (Qn−2 · · · Q2Q1)T = QT. To
prove (b) assume that A is symmetric. Thus HT = (QAQT)T =
QTTATQT = QAQT = H and H is symmetric.
25.
(a)
Set BT = [v1, v2, v3, v4]. Then QBT = [Qv1, Qv2, Qv3, Qv4],
and for 1 ≤j ≤4, Qvj = vj −γju , where γj is a constant. Since u = [0, a, b, c]T it
follows that Qvj and vj have the same ﬁrst coordinate. Thus BT
and QBT have
the same ﬁrst row. It follows that B and BQ = (QBT)T have the same ﬁrst column.
(b)
It follows from (a) that x = [b11, b12, 0, 0]T is the ﬁrst column of BQ. Thus Qx
is
the ﬁrst column of QBQ. But Qx = x −γu
where γ = 2uTx /uTu = 0; that is Qx
= x .
7.6
QR
Factorization & Least-Squares
1. x∗
is the unique solution to Rx = c , where R =
· 1
2
0
1
¸
and
c =
· 3
1
¸
. Thus x∗= [1, 1]T.
2. x∗= [2, −1]T.
3. x∗
is the unique solution to Rx = c
where R =


1
2
1
0
1
3
0
0
2

and
c =


6
7
4

.
Thus x∗= [2, 1, 2]T.
4. x∗= [1, 2, 1]T.

7.6. QR
FACTORIZATION & LEAST-SQUARES
209
5.
We require that SA1 = [a, 0]T. Therefore a = ±
p
a2
11 + a2
21 =
−5, u1 = a11 −a = 8, and u2 = a21 = 4. Consequently u = [8, 4]T and SA1 = A1 −u
= [−5, 0]T. SA2 = A2 −2u = [−11, 2]T, so SA = R =
· −5
−11
0
2
¸
.
6. u = [1, 1]T; R =
· −1
−5
0
−3
¸
.
7.
We require that SA1 =
· a
0
¸
so take a = −
p
a2
11 + a2
21 = −4,
u1 = a11 −a = 4, and u2 = a21 = 4. Thus u = [4, 4]T and SA1 =
A1 −u = [−4, 0]T. Also SA2 = A2 −2u = [−6, −2]T. Therefore SA = R =
· −4
−6
0
−2
¸
.
8. u = [−22, 4]T; R =
· 5
−22
0
4
¸
.
9.
We require that SA2 = [2, a, 0]T so set a = −
p
a2
22 + a2
32 = −1; u1 = 0, u2 = a22 −a = 1,
and u3 = a32 = 1. Therefore u = [0, 1, 1]T, SA1 = A1 , SA2 = A2 −u = [2, −1, 0]T, and
SA3 = A3 −14u = [1, −8, −6]T. Consequently SA = R =


1
2
1
0
−1
−8
0
0
−6

.
10. u = [0, 8, 4]T; R =


3
1
2
0
−5
−11
0
0
2

.
11.
We ﬁrst require Q1 such that Q1A1 = [a, 0, 0, 0]T so take u1 = [6, 2, 2, 4]T. Then Q1A1
= A1 −u1 = [−5, 0, 0, 0]T and Q1A2 =
A2 −3u = [−59/3, 6, 2, 3]T. We now require Q2 such that
Q2(Q1A2 ) = [−59/3, a, 0, 0]T. Thus set u2 = [0, 13, 2, 3]T. Then
Q2(Q1A1 ) = Q1A1
and Q2(Q1A2 ) = Q1A2 −u2 =
[−59/3, −7, 0, 0]T. Therefore Q2Q1A =


−5
−59/3
0
−7
0
0
0
0

.
12. u1 = [3, 1, 1, 1]T and Q1A =


−2
−7
0
0
0
0
0
3

. u2 = [0, 3, 0, 3]T and

210
CHAPTER 7. EIGENVALUES AND APPLICATIONS
Q2Q1A =


−2
−7
0
−3
0
0
0
0

.
13.
We require a matrix Q such that QA2 = [4, a, 0, 0]T. Thus u = [0, 8, 0, 4]T and QA =


2
4
0
−5
0
0
0
0

.
14. u = [0, 5, 1, 2]T and QA =


3
5
0
−3
0
0
0
0

.
15.
Let Q1, u1 , Q2, u2
be as in Exercise 11. Then Q1b = b −u1 = [−5, 8, −2, −3]T
and
Q2(Q1b ) = Q1b −u2 = [−5, −5, −4, −6]T.
The least-squares solution is the unique solution x∗to Rx= c where R =
· −5
−59/3
0
−7
¸
and c = [−5, −5]T. Thus x∗=
[−38/21, 15/21]T.
16. Q2Q1b= [−4, 2, −1, 3]. x∗
is the unique solution to Rx= c where R =
· −2
−7
0
−3
¸
and
c = [−4, 2]T. Thus x∗= [13/3, −2/3]T.
17.
With Q and u as in Exercise 13, Qb = b −(12/5)u =
[2, −56/5, 16, −8/5]T. Therefore x∗
is the unique solution to
Rx = c
where R =
· 2
4
0
−5
¸
and c = [2, −56/5]T. Solving
yields x∗= [−87/25, 56/25]T.
18. Qb = [5, −10/3, −19/3, −8/15]T.
x∗
is the unique solution to Rx = c
where R =
· 3
5
0
−3
¸
and c = [5, −10/3]T. Solving yields
x∗= [−5/27, 10/9]T.
19.
Write [A1, A2, . . . , An], where {A1, A2, . . . , An} is a linearly independent subset of Rn.
Now SA = [SA1, SA2, . . . , SAn]. Suppose c1, c2, . . . , cn are scalars such that θ=c1SA1 +
c2SA2 + · · · + cnSAn.

7.7. MATRIX POLYNOMIALS & THE CAYLEY-HAMILTON THEOREM
211
Then θ= S(c1A1 + c2A2 + · · · + cnAn) and S is nonsingular. There-
fore θ= c1A1 + c2A2 + · · · + cnAn. It follows that c1 = c2 = · · · = cn = 0 and hence, the
set {SA1, SA2, . . . , SAn} is linearly independent. For each j, 1 ≤j ≤n, SAj =
· Rj
θ
¸
where Rj
is the jth column of R and θ is in Rm−n. Therefore the set {R1, R2, . . . ,
Rn} is linearly independent in Rn and the matrix R is nonsingular.
7.7
Matrix Polynomials & The Cayley-Hamilton Theorem
1. q(A) = A2 −4A + 3I =
· −1
0
0
−1
¸
; q(B) = B2 −4B + 3I =
· 0
0
0
0
¸
; q(C) = C2 −4C +
3I =


15
−2
14
5
−2
10
−1
−4
6

.
2.
(a) p(A) = (A −I)3 = O; p(B) = (B −I)3 = O;
p(C) = (C −I)3 = O; p(I) = (I −I)3 = O3 = O
(b)
Set q(t) = (t −1)2 = t2 −2t + 1.
3.
(a) q(t) = s(t)p(t) + r(t) where s(t) = t3 + t −1 and r(t) = t + 2.
(b) q(B) = s(B)p(B)+r(B) = r(B) since p(B) = O. Thus q(B) = B+2I =
·
4
−1
−1
4
¸
.
4. H11 =
· 2
3
5
7
¸
, H22 =


4
1
3
6
1
2
0
4
1

and H33 =
· 6
5
7
3
¸
. Using Algorthim 1 we obtain
p1(t) = t2 −9t −1, p2(t) = t3 −6t2 −5t −38, and p3(t) = t2 −9t −17.
5.
Note that H2 = (SAS−1)(SAS−1) = (SA2S−1). For some positive integer k ≥2 sup-
pose we have shown that Hk = SAkS−1.
Then Hk+1 = HkH = (SAkS−1)(SAS−1) =
SAk+1S−1. It follows by induction that Hn = SAnS−1 for each positive integer n. Now
let q(t) = antn+an−1tn−1+· · ·+a1t+a0. Then q(H) = anHn+an−1Hn−1+· · ·+a1H+a0I =
anSAnS−1 + an−1SAn−1S−1 + · · · + a1SAS−1 + a0SIS−1 = S(anAn + an−1An−1 + · · · +
a1A + a0I)S−1 = Sq(A)S−1.
6.
Since u1TAu2 is a (1 x 1) matrix, u1TAu2 = (u1TAu2)T =
u2TATu1TT = u2TAu1. Now u1TAu2 = λ2u1Tu2 whereas
u2TAu1 = λ1u2Tu1 = λ1u1Tu2. Therefore λ1u1Tu2 = λ2u1Tu2.
Since λ1 ̸= λ2 it follows that u1Tu2 = 0.

212
CHAPTER 7. EIGENVALUES AND APPLICATIONS
7.
(a)
By assumption, Ax0 is in W. For some positive integer k, suppose we have shown
that Akx0 is in W. Then Ak+1x0 = A(Akx0) is in W
by the assumed property of
A. By induction, Anx0 is in W
for each positive integer n.
(b) θ= m(A)x0 = (A −rI)s(A)x0. Since s(t) has degree k −1,
S(A)x0 ̸= θ.
Thus if u = S(A)x0
then (A −rI)u = θ.
It follows that u
is an
eigenvector of A corresponding to the eigenvalue r. Now if s(t) = bk−1tk−1+· · ·+b1t+b0
then S(A)x0 = bk−1Ak−1x0 + · · · + b1Ax0 + b0x0. Since r is an eigenvalue for A, r
is real. It follows that b0, . . . , bk−1 are real. Thus S(A)x0 is a linear combination of
the vectors Ak−1x0,. . ., Ax0, x0 in W.
Hence S(A)x0 is in W.
8.
(a)
Let x and y be in W; that is, xuiT = 0 and yuiT = 0 for 1 ≤i ≤k. Therefore
(x + y)uiT = xuiT + yuiT = 0 + 0 = 0 for 1 ≤i ≤k. Consequently, x+y is in W.
Likewise if c is a scalar then (cx)uiT = c(xuiT) = c0 = 0, for 1 ≤i ≤k, so cx is in
W. Certainly θ is in W, so W
is a subspace of Rn.
(b)
Suppose that Aui = λiui, 1 ≤i ≤k, and let x be in W. Thus xTui= 0 for 1 ≤i ≤k.
Now (Ax)Tui = xTATui = xTAui = xT(λiui) = λi(xTui) = 0 for 1 ≤i ≤k. Therefore
Ax is in W. It now follows from Exercise 7 that A has an eigenvector uk+1 in W.
By deﬁnition of W, {u1, u2, . . . , uk, uk+1} is an orthogonal set of eigenvectors for A.
7.8
Generalized Eigenvectors & Diﬀerential
Equations
1.
(a)
The given matrix H has characteristic polynomial p(t) =
(t −2)2,
so λ = 2
is the only eigenvalue and it has algebraic multiplicity 2. The
vector v1 = [1, −1]T is an eigenvector corresponding to λ = 2. If we solve the system
of equations (H −2I)x = v1
we see that x = [−1 −a, a]T,
where a
is arbitrary.
Taking a = 0 we obtain a generalized eigenvector v2 = [−1, 0]T.
(b)
The given matrix H has characteristic polynomial p(t) =
t(t + 1)2.
The eigenvalue λ = −1
has corresponding eigenvector v1 = [−2, 0, 1]T.
Solving the system (H −(−1)I)x = v1 yields x= [2 −2a, 1, a]T where a is arbitrary.
Thus v2 = [0, 1, 1]T
is a generalized eigenvector for λ = −1. The eigenvalue λ = 0
has corresponding eigenvector w1 = [−1, 1, 1]T.
(c)
The given matrix H has characteristic polynomial p(t) =
(t −1)2(t + 1). The eigenvalue λ = 1 has corresponding eigenvector v1 = [−2, 0, 1]T.
Solving (H −I)x = v1 yields x = [(5/2) −2a, 1/2, a]T, where a is arbitrary. Thus
v2 = [5/2, 1/2, 0]T is a generalized eigenvector of λ = 1. The eigenvalue λ = −1 has
corresponding eigenvector w1 = [−9, −1, 1]T.

7.8. GENERALIZED EIGENVECTORS & DIFF. EQNS.
213
2.
For A, λ = 1
is the only eigenvalue.
Corresponding generalized eigenvectors are v1
= [0, 0, 0, 1]T, v2 = [0, 0, 1, 0]T, v3 = [0, 1, 0, 0]T,
and v4 = [1, 0, 0, 0]T.
For B generalized eigenvectors are v1 = [−3, −5, −1, 2]T, v2 =
[0, 0, 0, 1]T, v3 = [0, 1/2, 0, 1/2]T, and v4 = [1/4, 1/4, 0, 1/4]T.
3.
(a)
If Q =


1
0
0
0
1
0
0
3
1

then Q−1 =


1
0
0
0
1
0
0
−3
1

and H =
QAQ−1 =


8
−69
21
1
−10
3
0
−4
1

is in unreduced Hessenberg form. H
has characteristic
polynomial p(t) = (t+1)2(t−1). The eigenvalue λ = −1 has corresponding eigenvector
v1 = [3, 1, 2]T.
Solving the system (H −(−1)I)u = v1 yields u =
[−(7/2) + (3/2)a, (−1/2) + (1/2)a, a]T, where a is arbitrary.
Therefore v2 = [−2, 0, 1]T
is a generalized eigenvector for λ = −1.
The eigenvalue
λ = 1
has corresponding eigenvector w1 = [−3, 0, 1]T.
Set y(t) = Qx(t)
and
y0 = Qx0 = [−1, −1, −2]T.
The system y
′ =H y
has general solution y(t) = c1e−tv1+
c2e−t(v2 + tv1) + c3etw1 and y0 = y (0) = c1v1 +c2v2 +c3w1 .
Solving we obtain
c1 = −1, c2 = 2, c3 = −2, so y(t) =


e−t(6t −7)
+
6et
e−t(2t −1)
e−t(4t)
−
2et

.
Therefore
x(t) = Q−1y(t) =


e−t(6t −7) + 6et
e−t(2t −1)
e−t(−2t + 3) −2et

.
(b)
If Q =


1
0
0
0
1
0
0
3
1

then Q−1 =


1
0
0
0
1
0
0
−3
1

and
H = QAQ−1 =


2
4
−1
−3
−4
1
0
3
−1

is in unreduced Hessenberg form. H has character-
istic polynomial p(t) = (t+1)3. The eigenvalue λ = −1 has corresponding eigenvector
v1 = [1, 0, 3]T. The system (H −(−1)I)u = v1 has solution u = [−1 + (1/3)a, 1, a]T,
where a is arbitrary. Therefore v2 = [0, 1, 3]T is a generalized eigenvector of order 2
for λ = −1. The system (H −(−1)I)u = v2 has solution u= [(−4/3) + (1/3)a, 1, a]T
so v3 = [0, 1, 4]T is a generalized eigenvector of order 3 for λ = −1.

214
CHAPTER 7. EIGENVALUES AND APPLICATIONS
Set y (t) = Qx (t)
and y0 = Qx0 = [−1, −1, −2]T.
The system y
′ = H y
has
general solution y(t) = c1e−tv1 + c2e−t(v1 + tv1) + c3e−t(v3 + tv2 + (t2/2)v1) and
y0 = y(0) = c1v1 + c2v2 + c3v3. Solving we obtain c1 = −1, c2 = −5, c3 = 4, so y
(t) =


e−t(2t2 −5t −1)
e−t(4t −1)
e−t(6t2 −3t −2)

.
Therefore x (t) = Q−1y (t) =


e−t(2t2 −5t −1)
e−t(4t −1)
e−t(6t2 −15t + 1)

.
(c)
If Q =


1
0
0
0
1
0
0
3
1

then Q−1 =


1
0
0
0
1
0
0
−3
1

and H =
QAQ−1 =


1
4
−1
−3
−5
1
0
3
−2

is in unreduced Hessenberg form. H has characteristic
polynomial p(t) = (t + 2)3. The eigenvalue λ = −2 has eigenvector v1 = [1, 0, 3]T,
generalized eigenvector v2 = [0, 1, 3]T
of order two, and generalized eigenvector v3 =
[0, 1, 4]T of order 3.
Set y(t) = Qx(t)
and y0 = Qx0 = [−1, −1, −2]T.
The system y
′ = H y
has
general solution y(t) = e−2t[c1v1 + c2(v2 + tv1) + c3(v3 + tv2 + (t2/2)v1)]
and
y0 = y(0) = c1v1 + c2v2 + c3v3. Solving yields c1 = −1, c2 = −5, and c3 = 4, so
y(t) =


e−2t(2t2 −5t −1)
e−2t(4t −1)
e−2t(6t2 −3t −2)

.
Therefore
x (t) = Q−1y (t) =


e−2t(2t2 −5t −1)
e−2t(4t −1)
e−2t(6t2 −15t + 1)

.
4. x (t) =


etc4
et(c4t + c3)
et(c4t2/2 + c3t + c2)
et(c4t3/6 + c3t2/2 + c2t + c1)

.
5.
We see that from part(c) of Exercise 1 that x(t) =
c1etv1 +c2et(v2 +tv1 ) + c3e−tw1 =
c1et


−2
0
1

+ c2et




5/2
1/2
0

+ t


−2
0
1



+ c3e−t


−9
−1
1

.
6.
Note that (H −λI)v2 ̸= θ since v1 ̸=θ. But (H −λI)2v2 =
(H −λI)v1 =θ,
so v2
is a generalized eigenvector of order 2. Suppose we have seen
that vj
is a generalized eigenvector of order j
for 1 ≤j ≤k
where 1 ≤k < m. Then
(H −λI)kvk+1 = (H −λI )k−1vk ̸=θ whereas (H −λI)k+1vk+1 = (H −λI )kvk =θ.

7.8. GENERALIZED EIGENVECTORS & DIFF. EQNS.
215
Therefore vk+1 is a generalized eigenvector of order k + 1. It follows by induction that
vr is a generalized eigenvector of order r for 1 ≤r ≤m.
Clearly the set {v1}
is linearly independent since v1 ̸=θ.
Suppose we have seen that
the set {v1, . . . , vk}
is linearly independent for some k, 1 ≤k < m.
Now assume that
c1v1 + · · · + ckvk+
ck+1vk+1 =θ.
Note that (H −λI)kvj = θ for 1 ≤j ≤k whereas
(H −λI)kvk+1 = v1. It follows that θ =
(H −λI)kθ=(H −λI)k(c1v1 + · · · + ckvk + ck+1vk+1) = ck+1v1.
Therefore ck+1 = 0.
Since the set {v1, . . . , vk} is linearly independent, c1 = · · · = ck = 0. This proves that
{v1, . . . , vk, vk+1} is a linearly independent set. It follows by induction that {v1, . . . , vm}
is linearly independent.
7.
Note that Hvj = λvj + vj−1 for 2 ≤j ≤r whereas Hv1 = λv1. It is straightforward to
see that xr
′(t) = Hxr(t).
8.
First note that q(H)(H −λ1I)m1−1 = (H −λ1I)m1−1q(H). It follows from the equations
in (5) that if Eq.(9) is multiplied by
(H −λ1I)m1−1 then we obtain am1q(H)v1 =θ. Since v1 is an eigenvector corresponding
to λ1, q(H)v1 = q(λ1)v1 ̸=θ since v1 ̸=θ and q(λ1) ̸= 0. Therefore am1 = 0. By a similar
argument, multiplication of (9) by (H −λ1I)m1−2 shows that am1−1 = 0. We may continue
the process to show that aj = 0 for each j, 1 ≤j ≤m1.

216
CHAPTER 7. EIGENVALUES AND APPLICATIONS
7.9
Supplementary Exercises
1. A =
·
1
a
3 −a
1
¸
, a arbitrary.
2. a = 0 or a = −6.
3.
(a) If x = [1, −1]T then q(x) = −2.
(b) The matrix B is not symmetric.
5. (a) L =
· 2
0
3
1
¸
(b) L =


1
0
0
3
2
0
1
2
1


7.10
Conceptual Exercises
1. Let A have characterstic polynomial p(t) = t3 + ut2 + vt + w. Since A is nonsingular, λ = 0
is not an eigenvalue for A. Therefore, w ̸= 0. Since A3 + uA2 + vA + wI = O, it follows
that [(−1/w)A2 −(u/w)A −(v/w)I]A = I.
2. If B = P −1AP then Bn = (P −1AP)n = P −1AnP. It follows that p(B) = p(P −1AP) =
P −1p(A)P.
3. For 1 ≤i ≤n, aii = eT
i Aei > 0.


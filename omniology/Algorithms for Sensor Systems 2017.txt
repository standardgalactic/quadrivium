Antonio Fernández Anta
Tomasz Jurdzinski
Miguel A. Mosteiro 
Yanyong Zhang (Eds.)
 123
LNCS 10718
13th International Symposium on Algorithms and Experiments 
for Wireless Sensor Networks, ALGOSENSORS 2017  
Vienna, Austria, September 7–8, 2017, Revised Selected Papers
Algorithms  
for Sensor Systems
www.ebook3000.com

Lecture Notes in Computer Science
10718
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7411
www.ebook3000.com

Antonio Fernández Anta
• Tomasz Jurdzinski
Miguel A. Mosteiro
• Yanyong Zhang (Eds.)
Algorithms
for Sensor Systems
13th International Symposium on Algorithms and Experiments
for Wireless Sensor Networks, ALGOSENSORS 2017
Vienna, Austria, September 7–8, 2017
Revised Selected Papers
123

Editors
Antonio Fernández Anta
IMDEA Networks Institute
Leganés
Spain
Tomasz Jurdzinski
University of Wrocław
Wroclaw
Poland
Miguel A. Mosteiro
Pace University
New York
USA
Yanyong Zhang
Rutgers University
North Brunswick, NJ
USA
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-72750-9
ISBN 978-3-319-72751-6
(eBook)
https://doi.org/10.1007/978-3-319-72751-6
Library of Congress Control Number: 2017962890
LNCS Sublibrary: SL5 – Computer Communication Networks and Telecommunications
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
www.ebook3000.com

Preface
ALGOSENSORS, the International Symposium on Algorithms and Experiments for
Wireless Sensor Networks, is an international conference dedicated to algorithmic
aspects of networks of restricted devices. The 13th edition of ALGOSENSORS was
held during September 7–8, 2017, in Vienna, Austria, as a part of the ALGO 2017
event.
While ALGOSENSORS was created to focus on sensor networks, in recent years it
has broadened its scope to topics around the common theme of wireless networks of
computational entities. For example, networks of (static or mobile) sensors,
cyber-physical systems, mobile robots, Internet of Things (IoT) devices, and drones.
Aspects explored include optimization, security and privacy, energy management,
localization, coordination and pattern formation, data collection and aggregation, and
fault tolerance. This year, ALGOSENSORS had two tracks: Algorithms and Theory,
and Experiments and Applications.
For ALGOSENSORS 2017, we received 30 submissions from more than 20
countries. These manuscripts were rigorously reviewed by our Program Committee of
25 members and some external reviewers. Each submission had three or more reviews.
As a result, 17 papers were accepted and presented at the conference. The program was
completed with the keynote presentation by Jie Gao (Stony Brook University), to
whom we are very grateful for delivering an excellent talk. This volume contains the
technical details of the papers presented at the conference.
This year, for the ﬁrst time, one paper was selected to receive the ALGOSENSORS
Best Paper Award in the Algorithms and Theory Track. The paper selected was
“Parameterized Algorithms for Power-Efﬁcient Connected Symmetric Wireless Sensor
Networks” by Matthias Bentert, René van Bevern, André Nichterlein, and Rolf Nie-
dermeier. Congratulations to the authors.
We want to thank all the Program Committee members, and their external reviewers,
for their efforts in selecting the best papers. The strong ﬁnal program of ALGO-
SENSORS 2017 is a reﬂection of their excellent work. We also want to thank the
Organizing Committee of ALGO 2017 for the great coordination of the ALGO event,
and facilitating the hosting of ALGOSENSORS 2017 in such a nice environment. We
are particularly grateful to the Organizing Committee chair, Stefan Szeider, who was
always there to help. Finally, we want to thank the Steering Committee of ALGO-
SENSORS for trusting us with the task of driving ALGOSENSORS 2017, and in
particular the Steering Committee chair, Sotiris Nikoletseas, for all the help and
guidance he provided.
November 2017
Miguel A. Mosteiro
Antonio Fernández Anta
Tomasz Jurdzinski
Yanyong Zhang

Organization
Program Committee
Ashwin Ashok
Georgia State University, USA
Evangelos Bampas
LIF, Aix-Marseille University and CNRS, France
Amotz Bar-Noy
City University of New York, USA
Fernando Boavida
University of Coimbra, Portugal
Jacek Cichon
Wroclaw University of Technology, Poland
Gianluca de Marco
University of Salerno, Italy
Robert Elsasser
University of Salzburg, Austria
Martin Farach-Colton
Rutgers University, USA
Antonio Fernández Anta
IMDEA Networks Institute, Spain
Ben Firner
NVIDIA, USA
Leszek Gasieniec
University of Liverpool, UK
James Gross
RWTH Aachen University, Germany
Tomasz Jurdzinski
University of Wroclaw, Poland
Evangelos Kranakis
Carleton University, Canada
Joseph S. B. Mitchell
Stony Brook University, USA
Miguel A. Mosteiro
Pace University, USA
Jorge Ortiz
IBM, USA
Hui Pan
The Hong Kong University of Science
and Technology, SAR China
Gopal Pandurangan
University of Houston, USA
Dror Rawitz
Bar-Ilan University, Israel
Guiling Wang
New Jersey Institute of Technology, USA
Dongxiao Yu
Huazhong University of Science and Technology,
China
Lan Zhang
Tsinghua University, China
Yanyong Zhang
Rutgers University, USA
Rong Zheng
McMaster University, Canada
Additional Reviewers
Augustine, John
Chatterjee, Soumyottam
Chlebus, Bogdan
Even, Guy
Karousatou, Christina
Molla, Anisur Rahaman
Montangero, Manuela
Rescigno, Adele
Robinson, Peter
Scquizzato, Michele
www.ebook3000.com

Contents
Collaborative Delivery by Energy-Sharing Low-Power Mobile Robots. . . . . .
1
Evangelos Bampas, Shantanu Das, Dariusz Dereniowski,
and Christina Karousatou
Data Collection in Population Protocols with Non-uniformly
Random Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
Joffroy Beauquier, Janna Burman, Shay Kutten, Thomas Nowak,
and Chuan Xu
Parameterized Algorithms for Power-Efficient Connected Symmetric
Wireless Sensor Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
Matthias Bentert, René van Bevern, André Nichterlein,
and Rolf Niedermeier
Fast Distributed Approximation for Max-Cut . . . . . . . . . . . . . . . . . . . . . . .
41
Keren Censor-Hillel, Rina Levy, and Hadas Shachnai
Barrier Coverage with Uniform Radii in 2D . . . . . . . . . . . . . . . . . . . . . . . .
57
Andrew Cherry, Joachim Gudmundsson, and Julián Mestre
Rendezvous on a Line by Location-Aware Robots Despite the Presence
of Byzantine Faults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
Huda Chuangpishit, Jurek Czyzowicz, Evangelos Kranakis,
and Danny Krizanc
Querying with Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
Huda Chuangpishit, Kostantinos Georgiou, and Evangelos Kranakis
Energy-Optimal Broadcast in a Tree with Mobile Agents. . . . . . . . . . . . . . .
98
Jerzy Czyzowicz, Krzysztof Diks, Jean Moussi, and Wojciech Rytter
Searching for a Non-adversarial, Uncooperative Agent on a Cycle . . . . . . . .
114
Jurek Czyzowicz, Stefan Dobrev, Maxime Godon, Evangelos Kranakis,
Toshinori Sakai, and Jorge Urrutia
Improved Leader Election for Self-organizing Programmable Matter . . . . . . .
127
Joshua J. Daymude, Robert Gmyr, Andréa W. Richa,
Christian Scheideler, and Thim Strothmann
Conflict-Free Data Aggregation on a Square Grid When Transmission
Distance is Not Less Than 3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
Adil Erzin and Roman Plotnikov

Uniform Dispersal of Robots with Minimum Visibility Range . . . . . . . . . . .
155
Attila Hideg and Tamás Lukovszki
Gathering Anonymous, Oblivious Robots on a Grid . . . . . . . . . . . . . . . . . .
168
Matthias Fischer, Daniel Jung, and Friedhelm Meyer auf der Heide
A Continuous Strategy for Collisionless Gathering . . . . . . . . . . . . . . . . . . .
182
Shouwei Li, Christine Markarian, Friedhelm Meyer auf der Heide,
and Pavel Podlipyan
Maximizing Barrier Coverage Lifetime with Static Sensors . . . . . . . . . . . . .
198
Menachem Poss and Dror Rawitz
Independent Sets in Restricted Line of Sight Networks . . . . . . . . . . . . . . . .
211
Pavan Sangha, Prudence W. H. Wong, and Michele Zito
Braid Chain Radio Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
Jacek Cichoń, Mirosław Kutyłowski, and Kamil Wolny
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
X
Contents
www.ebook3000.com

Collaborative Delivery by Energy-Sharing
Low-Power Mobile Robots
Evangelos Bampas1, Shantanu Das1(B), Dariusz Dereniowski2,
and Christina Karousatou1
1 LIF, Aix-Marseille University and CNRS, Marseille, France
{evangelos.bampas,shantanu.das,christina.karousatou}@lif.univ-mrs.fr
2 Faculty of Electronics, Telecommunications and Informatics,
Gda´nsk University of Technology, Gda´nsk, Poland
deren@eti.pg.edu.pl
Abstract. We study two variants of delivery problems for mobile robots
sharing energy. Each mobile robot can store at any given moment at most
two units of energy, and whenever two robots are at the same location,
they can transfer energy between each other, respecting the maximum
capacity. The robots operate in a simple graph and initially each robot
has two units of energy. A single edge traversal by an robot reduces its
energy by one unit and the robot can only perform such move initially
having at least one unit of energy. There are two distinguished nodes s
and t in the graph and the goal for the robots is to deliver the package
initially present on s to the node t. The package can be passed from
one robot to another when they are colocated. In the ﬁrst problem we
study, the robots are initially placed at some given nodes of the graph
and the question is whether the delivery is feasible. We prove that this
problem is NP-complete. In the second problem, the initial positions of
the robots are not ﬁxed but a subset of nodes H of the graph is given as
input together with an integer k, and the question is as follows: is there
a placement of k robots at nodes in H such that the delivery is possible?
We prove that this problem can be solved in polynomial time.
Keywords: Computational complexity · Energy sharing · Delivery
Mobile robots · Power-aware
1
Introduction
We consider algorithms for coordinated tasks performed by swarms of small
inexpensive robots. There has been a lot of research interest on designing teams
of simple robots that can perform a given task in collaborative fashion. The task
we consider is the basic operation of moving an object or a package from its
source to its target destination by one or more mobile robots. For example, the
package could be a sample collected by a robotic sensor that needs to be delivered
Partially supported by National Science Centre (Poland) grant number 2015/17/B/
ST6/01887 and the project ANR-ANCOR (anr-14-CE36-0002-01).
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 1–12, 2017.
https://doi.org/10.1007/978-3-319-72751-6_1

2
E. Bampas et al.
to a base station for analysis. One can imagine an automated postal delivery
system where packages need to be delivered between sources and destinations
using teams of robots or drones. We can model the sources and destinations as
nodes of a graph and the Delivery problem consists of moving a single package
from its source node to the target node. The main issue when using small robots
is that they have a restricted supply of energy (e.g. a battery) and thus, a
robot can move only a limited distance before running out of power. However,
many small robots can cooperate to deliver a package from source to destination.
Assuming that the robots start from diﬀerent nodes of the graph, scheduling the
moves of the robots for collaborative delivery is known to be a challenging task.
Indeed, it was shown by Chalopin et al. [9] that even if the graph is a tree of
n nodes, collaborative delivery from single source to a single destination using
k robots having energy B each is NP-hard. Czyzowicz et al. [10] studied the
problem when the robots can share their energy, i.e. a robot may give its unused
energy to another robot. They showed that the problem can be easily solved
in trees in polynomial time, but it remains NP-hard in general graphs. For all
the above results, the parameters k and B have arbitrary (non-constant) values.
Note that collaborative delivery for a constant value of k can be trivially solved
by brute force manner in constant time. In this paper, we consider the problem
when B is a small constant but k can have arbitrary values. This corresponds
to the case of many small robots each operating with low power batteries such
that each robot can move only for a constant number of steps in the graph. A
robot with depleted energy can gather the unused energy of any other robot
that it meets. However, no robot can have more than B units of energy at
any time. Surprisingly, we show that even when B = 2, the smallest constant for
which the problem is non-trivial, collaborative delivery is still NP-hard in general
graphs. On the other hand, we provide an optimal polynomial-time algorithm
for collaborative delivery with robots having B = 2 if we are allowed to choose
the initial placement of robots among designated homebase nodes in the graph.
Note that if we are allowed to place robots on any node of the graph, then there
is a trivial optimal solution where all robots are placed on the shortest s-t path
at intervals of distance 2. At the other extreme, if there is only one homebase
node, then the solution is non-trivial.
Our Contributions. We completely solve the problem of collaborative deliv-
ery for B = 2 when energy sharing is allowed. We deﬁne two versions of the
problem. In the ﬁrst version of the problem called CollaborativeDelivery
with Fixed Placement, the initial placement of robots (i.e. the energy distribu-
tion is given as part of the problem). In the second version of the problem called
CollaborativeDelivery with Chosen Placement, a set of homebase nodes is
given and the algorithm may choose the distribution of robots among the home-
base nodes. We show that the ﬁrst version of the problem is strongly NP-hard,
while the second version of the problem admits a polynomial time solution and
we present such a solution strategy. Proofs are omitted due to space constraints
and will appear in the full version of the paper.
www.ebook3000.com

Collaborative Delivery by Energy-Sharing Low-Power Mobile Robots
3
Related Work. Betke et al. [6] considered for the ﬁrst time energy-constrained
robots in the context of exploration of grid graphs by a robot who can return
to its starting node for refueling. Awerbuch et al. [2] studied the same problem
for general graphs. Duncan et al. [16] studied a similar model where the robot is
tethered to its starting position with a rope of ﬁxed length and they optimized
the exploration time. Several other papers have considered robots with limited
energy, or the goal of minimizing spent energy or maximum displacement of
robots, e.g. in the context of exploration [12,14,17–20], formation [7,13], cover-
age [11,15], and broadcast/convergecast [1,10] problems.
In what concerns speciﬁcally collaborative delivery by energy-aware mobile
robots, the problem has been considered in some recent works under various
assumptions. In [8], the authors assume robots with limited energy that is con-
sumed as they move. They prove that the problem of deciding whether delivery
is feasible is NP-hard even if the robots are initially collocated, and they provide
a 2-approximation algorithm for the optimization version of ﬁnding the min-
imum initial energy that can be given to all robots so that delivery becomes
feasible, as well as exact, approximation, and resource-augmented algorithms
for variants of the problem. In [9], the authors show that the problem is weakly
NP-hard even on the line (with initially dispersed robots), and provide a quasi-,
pseudo-polynomial algorithm under the assumption of integer numerical values
in the problem instance. In [3], the authors consider the variant of the delivery
probem in which the robots have to return to their respective starting positions
and they prove that this problem is NP-hard for planar graphs but can be solved
eﬃciently on trees and lines, in contrast to the non-returning version which is
NP-hard on lines. They also give resource-augmented algorithms for returning
delivery in general graphs and prove tight lower bounds on the resource augmen-
tation for both the returning and the non-returning variant. In [4], the robots do
not have a limited energy source, but instead they have diﬀerent rates of energy
consumption and the goal is to ﬁnd a delivery schedule that minimizes the total
energy spent. Moreover, there are several messages that need to be delivered
from their respective source to their respective target. The authors study sep-
arately three subtasks that need to be solved in order to compute the optimal
solution (collaboration of diﬀerent robots on the same message, planning for a
robot that works on multiple messages, and assignment of messages to robots)
and they provide a polynomial-time (nonconstant) approximation algorithm for
the problem. In this setting, [5] studies the design of truthful mechanisms in
a game-theoretic model where the rate of energy consumption is information
private to each robot.
The only previous work that considers energy sharing by mobile robots is [10],
where this feature is introduced in the context of the delivery and convergecast
problems. The authors show that both problems can be solved eﬃciently in trees,
whereas they are NP-complete in general undirected and directed graphs. It is
important to note that, in the model of [10], a robot may store an unlimited
amount of energy as a result of receiving energy from other robots it encounters.

4
E. Bampas et al.
In other words, there is no battery capacity constraint for the robots, in contrast
to the model that we study in the present paper.
2
Preliminaries
We now deﬁne precisely the collaborative delivery problem for a collection of
energy sharing mobile robots. Given a simple undirected graph G = (V, E), with
two special nodes s (source) and t (target), and a collection of k mobile robots
located initially in speciﬁc nodes of the graph, the objective is to decide whether
there is schedule of robot moves that can deliver a package from s to t. Each
robot has a constant energy budget and we denote its value by B. Traversing
each edge consumes one unit of energy, thus, a fully charged robot can move a
distance of B before running out of energy. When two robots are at the same
node, one robot can transfer to the other robot, any integral part of its energy,
with the only constraint that no robot can have more than B units of energy at
any time.
In this paper, B = 2 for all robots. There is a unique package initially at
node s that needs to be moved to node t. To simplify the discussion, we will
assume that the system is synchronous (any synchronous strategy can also be
implemented in an asynchronous system using appropriate waits). A robot r
located at a node v at time j and having some positive energy, can perform any
subset of the following actions:
– Pick up the package, if the package is present at v at time j.
– Transfer one unit of energy to another robot r′ that is located at v at time j,
if r′ is not fully charged.
– Move to a neighboring node u, consuming one unit of energy and arriving at
u at time j + 1.
A solution strategy is a sequence of steps as above, such that after the last step,
the package is located at node t.
In the general version of the problem described below, the position of the
robots is given by an adversary.
Problem 1 (CollaborativeDelivery with Fixed Placement). CDX
Instance: ⟨G, s, t, k, h⟩, where G
=
(V, E) is a simple undirected graph,
s, t ∈V are, respectively, the source and target nodes, h : {1, . . . , k} →V is
the placement function that speciﬁes the initial positions of the k ≥1 robots.
Question: Does there exist a solution strategy for moving the package from s
to t, when each robot start with B = 2 units of energy?
If the placement of robots among homebase nodes can be chosen by the
algorithm, then we have the following version of the problem:
Problem 2 (CollaborativeDelivery with Chosen Placement). CDC
www.ebook3000.com

Collaborative Delivery by Energy-Sharing Low-Power Mobile Robots
5
Instance: ⟨G, s, t, k, H⟩, where G = (V, E) is a simple undirected graph, s, t ∈V
are, respectively, the source and target nodes, k ≥1 is the number of mobile
robots and H ⊂V is the set of homebase nodes.
Question: Does there exist a placement function p : {1, . . . , k} →H and a
corresponding solution strategy for moving the package from s to t, when the
i-th robot starts at node p(i) with B = 2 units of energy?
A path with nodes v1, . . . , vn and edges {vi, vi+1}, i ∈{1, . . . , n −1}, is
denoted by (v1, . . . , vn). The path graph with n vertices and n −1 edges is
denoted by Pn. The length of the shortest path between two nodes u, v of a
graph G is denoted by dG(u, v), or simply d(u, v) when there is no potential for
confusion. If a robot is initially placed on a node v, then we write a(v) to refer
to this robot at any point of a strategy.
We obtain the following regarding the hardness of problem CDX:
Theorem 1. CDX is NP-complete in the class of graphs with degree bounded by
5 and with each node being initially occupied by at most one robot.
Theorem 2. CDX is NP-complete in the class of graphs with diameter at most
42 with each node being initially occupied by at most one robot.
3
An Eﬃcient Algorithm for COLLABORATIVEDELIVERY
with Chosen Placement
Let I = ⟨G, s, t, H⟩be an instance of CDC, with G = (V, E) and H ⊆V . Recall
that a solution to I is a strategy that enables a group of energy-exchanging
robots starting from some or all of the nodes in H with battery capacity B = 2
to transfer the package from s to t. The cost of a solution is the total initial
energy of the robots that are placed on nodes in H. If u ∈V , we denote by hu
the node in H that is closest to u, breaking ties arbitrarily, and we denote by
z(u) the distance from u to hu in G, i.e., z(u) = dG(hu, u). Let ˆG be a weighted
complete digraph with vertex set V and arc set E′, and let the weight of an
arc e = (u, v) ∈E′ be w(e) = 2z(u)+dG(u,v)−1. If P is a directed path in ˆG, the
weight of P is the sum of the weights of the arcs in P. This section is devoted
to the proof of the following theorem:
Theorem 3. An optimal solution to I = ⟨G, s, t, H⟩has cost d ˆ
G(s, t).
It suﬃces to show that there exists a directed s-t path in ˆG with weight
smaller than or equal to the cost of the optimal solution for I and, additionally,
that every directed s-t path in ˆG corresponds to some solution for I with cost
equal to the weight of the path. The latter claim is given in the following lemma:
Lemma 1. For every directed s-t path in ˆG, there exists a solution for I with
cost equal to the weight of the path.

6
E. Bampas et al.
In the rest of this section, we derive some structural properties of optimal
solutions for I, which permit us to prove the former claim (cf. Lemma 5). In
Sect. 3.1 we introduce our main tool in the analysis: an energy ﬂow hypergraph
that provides a way of presenting solutions to CDC. Then, Sects. 3.2 and 3.3
give a series of properties of this hypergraph, allowing us to ﬁnish the proof
of Theorem 3 in Sect. 3.4. We assume that s ̸= t, otherwise Theorem 3 holds
trivially.
3.1
The Energy Flow Hypergraph
Given a solution S for I with cost X > 0, we represent S by a triple S = (V, E, ˜E),
where (V, E) = H is a directed hypergraph that represents the ﬂow and eventual
consumption of energy units (cf. Deﬁnition 1 below) and ˜E ⊆E corresponds to
the package moves under S (cf. Deﬁnition 2 below). The nodes of H are the
energy arrival and extinction events of S, as speciﬁed below.
We assume that the units of energy that are initially present at nodes in H
receive distinct identities from 1 to X. We distinguish two types of events during
the delivery under S. An arrival event occurs whenever a robot with two units
of energy i, j with i < j moves from some node u of G to a neighbor v at time
step t. We say that the unit of energy j is wasted by i during the event and that
the unit of energy i arrives at v at time t+1. We denote this event as (i, t+1, v).
An extinction event occurs whenever a robot with one unit of energy i moves
from some node u of G to a neighbor v at time step t. We say that the unit of
energy i wastes itself during the event. We denote this event as (⊥i, t + 1, v).
We also consider as arrival events the appearance of the X units of energy
at the homebases at time 0, and we denote them as (i, 0, hi), for 1 ≤i ≤X,
where hi ∈H is the homebase where energy unit i was placed.
We are now ready to deﬁne the energy ﬂow hypergraph H = (V, E) in terms
of the arrival and extinction events as follows:
Deﬁnition 1 (Energy ﬂow hypergraph). The vertex set V contains all of
the arrival and extinction events, as speciﬁed above. The hyperarc set E con-
tains ({(i, t1, u), (j, t2, u)}, {(i, t, v)}) if t ≥1, the unit of energy i came from
node u during the event (i, t, v), j > i is the unit of energy consumed during
the event (i, t, v), and in addition t1 < t, t2 < t, and i (resp. j) is not involved
in any other events between times t1 (resp. t2) and t. Furthermore, E contains
({(i, t1, u)}, {(⊥i, t, v)}) if t1 < t and i is not involved in any other events between
times t1 and t.
Deﬁnition 2 (Item moves).
The set ˜E is deﬁned as the subset of E that
contains all of the hyperarcs that correspond to package moves. More precisely,
a hyperarc ({(i, t1, u), (j, t2, v)}, {(i, t, v)}) ∈˜E if the robot that arrived with
energy unit i at v at time t was carrying the package. Similarly, a hyperarc
({(i, t1, u)}, {(⊥i, t, v)}) ∈˜E if the robot that arrived with zero energy at v at
time t (having wasted energy unit i) was carrying the package.
www.ebook3000.com

Collaborative Delivery by Energy-Sharing Low-Power Mobile Robots
7
Fig. 1. An energy ﬂow hypergraph constructed for a graph G shown on the left. The
hypergraph consists of two components (the hyperarcs that correspond to package
moves are highlighted): the ﬁrst component dictates two robots to move from H to s
and then one of those robots picks up the package and travels along path (s, a, b); the
second component makes two robots to move from H to t and then one of them goes
to b, picks up the package and returns to t.
We illustrate the energy ﬂow hypergraph H corresponding to a simple solu-
tion for a CDC instance in Fig. 1. Note that the cost of S is given by the total
number of energy units that “arrive” at nodes in H at time 0, which corresponds
to the number of nodes of the form (·, 0, ·) in H.
By construction, there is no cycle in H. Moreover, the head of every hyperarc
of H has size 1 and every node of H is contained in at most one hyperarc head and
in at most one hyperarc tail. Therefore, H consists of a number of independent
components H1, . . . , Hσ, each of which has a tree-like structure, as in Fig. 1.
Notation. If e ∈E, we denote by head(e) the unique node that is in the head
of e. If v ∈V, we denote by g(v) the node of G that is involved in the event v.
If e ∈E, then we denote by gtail(e) the node of G that is involved in the events
in the tail of e (recall that, by deﬁnition of the hypergraph H, all events in the
tail of e must involve the same node of G), and by ghead(e) the node of G that
is involved in the unique event in the head of e (i.e., ghead(e) = g(head(e))).
If v ∈V, let Δv denote the subgraph of H induced by the ancestors of v
and v itself. Let height(v) denote the number of hyperarcs in the longest path
that terminates at v. If e ∈E, we abuse the notation slightly and we denote by
height(e) the height of head(e). If v, v′ ∈V, we write v ≺v′ if v is an ancestor
of v′ and we write v ⊏v′ if v precedes v′ temporally, i.e., v = (i, t, x) and
v′ = (i′, t′, x′) with t < t′. Note that v ≺v′ implies v ⊏v′. As above, we extend
the notation to arcs and we write e ≺e′ if head(e) ≺head(e′) and e ⊏e′ if
head(e) ⊏head(e′).
If Δv contains x nodes of the form (·, 0, ·), then we say that Δv incurs a cost
of x. This represents the energy units used by the solution in order to generate
the event v. We also say that a component Hi incurs a cost equal to the cost
incurred by its maximal node (under ≺). The cost of S is the sum of the costs
incurred by the components of H.

8
E. Bampas et al.
3.2
Properties of Optimal Solutions
The goal of this section is to prove a property of optimal solutions that can be
informally stated as follows: every component of the hypergraph corresponding
to the solution contains exactly one chain of item moves and the last hyperarc
of this chain is an extinction event of the component (Fig. 2).
Proposition 1. For every solution S = (V, E, ˜E) in which there exist arcs f, g ∈
˜E with f ≺g, there exists a solution S′ = (V, E, ˜E′) with ˜E′ = ˜E\{e : f ⊏e ⊏g}∪
{e : f ≺e ≺g}.
Fig. 2. Illustration of Proposition 1. Triangles represent components of the solution
hypergraph. The solution S is shown on the left (in which some arcs of the path
from f to g are not package moves—those package moves can be possibly in diﬀerent
components; the dotted arrows represent the time succession between package moves
that take the package from f to g) and the corresponding S′ is shown on the right.
By repeated application of Proposition 1, we obtain the following:
Corollary 1. For every solution S = (V, E, ˜E), there exists a solution S′ =
(V, E, ˜E′) and a partition of ˜E′ into sets ˜E′
1, . . . , ˜E′
τ such that, for every i, the
hyperarcs of ˜E′
i form a chain in H and, for every e ∈˜E′
i and e′ ∈˜E′
j with i < j,
we have e ⊏e′, e ̸≺e′, and e′ ̸≺e.
Note that Proposition 1 and Corollary 1 apply to any solution (not necessar-
ily an optimal one). Furthermore, in both statements, the obtained solution S′
has the same energy ﬂow hypergraph as S, and therefore it has the same cost
as S.
Lemma 2. For every optimal solution S = (V, E, ˜E) and for every component
Hi of H = (V, E) with maximum (under ≺) hyperarc ri, we have ri ∈˜E and
head(ri) is an extinction event.
By applying Corollary 1 to an arbitrary optimal solution, we obtain the
following corollary in view of Lemma 2:
Corollary 2. There exists an optimal solution S = (V, E, ˜E) such that every
component Hi of H = (V, E) with maximum (under ≺) hyperarc ri contains
exactly one chain of package moves whose last hyperarc is ri and, in addition,
head(ri) is an extinction event.
www.ebook3000.com

Collaborative Delivery by Energy-Sharing Low-Power Mobile Robots
9
3.3
Canonical Nodes
Given a solution S = (V, E, ˜E) for I, let v ∈V such that Δv does not contain any
hyperarc in ˜E. Intuitively, if v is not an extinction event, then the sole function
of Δv in the solution is to bring one unit of energy to g(v). It thus makes
sense that, if S is optimal, then the energy units that participate in the events
of Δv travel along shortest paths from their respective homebases to g(v). If v
satisﬁes these conditions, then we say that v is canonical. The following deﬁnition
captures this notion:
Deﬁnition 3 (Canonical nodes). Given a solution S = (V, E, ˜E), a node
v ∈V is called canonical if either height(v) = 0, or height(v) = h + 1 for
some h ≥0 (in this case, v = head(e) for some e ∈E) and all of the following
hold: (i) for every node u in the tail of e, u is canonical and height(u) = h,
(ii) e /∈˜E, and (iii) z(g(v)) = 1 + z(gtail(e)).
The two propositions below follow easily by induction on the height of v.
Recall that, by deﬁnition of H, if height(v) = 0 then g(v) ∈H.
Proposition 2. If v is canonical, then z(g(v)) = height(v).
Proposition 3. If v is canonical and it is not an extinction event, then the cost
incurred by Δv is 2height(v).
We can now prove that, in every optimal solution, every node v whose Δv
does not contain any package move is canonical.
Lemma 3. For every optimal solution S = (V, E, ˜E) and for every v ∈V, v is
canonical or Δv contains a hyperarc in ˜E.
3.4
Completing the Proof of Theorem 3
In the following, let S⋆= (V, E, ˜E) be an optimal solution as guaranteed by
Corollary 2, with the maximum number σ of components of H = (V, E). Let
(Hi)i=1,...,σ be an enumeration of the components of H in temporal order of
their extinction events. For i ∈{1, . . . , σ}, component Hi is responsible for
moving the package along a path Pi = (ui,0, ui,1, . . . , ui,ρi) in G, where u1,0 = s,
uσ,ρσ = t, and ui,ρi = ui+1,0 (for i < σ).
Lemma 4. For every i, j in the ranges 1 ≤i ≤σ and 0 ≤j < ρi−1, z(ui,j+1) =
1 + z(ui,j).
Corollary 3. The cost incurred by component Hi is 2z(ui,0)+ρi−1.
Let Q be the directed s-t path in ˆG that consists of the arcs (u1,0, u1,ρ1),
(u2,0, u2,ρ2),. . . , (uσ,0, uσ,ρσ). By deﬁnition of ˆG, the i-th arc of Q has weight
2z(ui,0)+dG(ui,0,ui,ρi)−1. However, Pi is a path in G from ui,0 to ui,ρi and its
length is ρi. Therefore, dG(ui,0, ui,ρi) ≤ρi. In view of Corollary 3, we conclude
that the weight of the i-th arc of Q is at most equal to the cost of Hi and thus
the total weight of the arcs of Q is at most equal to the cost of S⋆. We have
proved the following lemma, which concludes the proof of Theorem 3:

10
E. Bampas et al.
Lemma 5. There exists a directed s-t path in ˆG with weight at most equal to
the cost of an optimal solution to I.
4
Concluding Remarks
Our work reveals an interesting diﬀerentiation in the complexity of the collab-
orative delivery problem by robots with battery capacity B = 2, depending on
whether the energy allocation to the homebases (starting nodes of robots) is
given as part of the input on the one hand, or the allocation can be chosen as
part of the solution on the other hand.
As we showed, the problem with ﬁxed allocation of energy units to the home-
bases is NP-complete. However, we proved in Sect. 3 that the delivery problem
in which one is given the total available energy and is asked if it is possible to
distribute this energy to robots at the homebases in order to achieve delivery
is solvable in polynomial time. In fact, what we proved is that the underlying
optimization problem, i.e., ﬁnding the minimum amount of energy that can be
distributed to the homebases so that delivery is feasible, is solvable in polyno-
mial time by reduction to a shortest path computation in a complete directed
graph.
A natural question is how to handle greater battery capacities B ≥3. While
we expect that our NP-completeness reduction generalized to B ≥3, the situa-
tion is less clear when it comes to the question of computing the energy allocation
to the homebases as part of the solution. A straightforward adaptation of our
algorithm from Sect. 3 would be to reduce the problem to computing the short-
est s-t path in a directed graph ˆG similar to the one we construct in Sect. 3,
except that the weight of an arc (u, v) would be equal to the minimum amount
of energy required by robots with capacity B to traverse the path hu ⇝u ⇝v,
where hu is the nearest homebase to u. Unfortunately, this algorithm is no longer
guaranteed to produce an optimal solution for B ≥3 (see Fig. 3).
h
s
t
u
v
Fig. 3. An example of a graph in which the straightforward adaptation of our algorithm
from Sect. 3 to B = 3 does not give an optimal solution. Here, H = {h} and the shortest
s-t path in ˆG is s →v →t, with each arc having a weight of 41 for a total cost of 82.
However, the optimal solution has a cost of 81: 27 fully charged robots start from h
and they reach u with 16 remaining energy units in total. At u, the robots split into
two groups with 8 units of energy each. The ﬁrst group goes to s and then to v from
the top branch, picking up the package from s on the way. The second group goes to v
from the bottom branch, picks up the package, and continues until t.
The reason is that several nice properties of the optimal solutions for B = 2
no longer hold for B ≥3. In particular, since the hyperarcs in the energy ﬂow
www.ebook3000.com

Collaborative Delivery by Energy-Sharing Low-Power Mobile Robots
11
hypergraph can now have up to two nodes in their heads, each component can
now have more than one bottommost nodes and it can contain more than one
chains of package moves. This is exactly the case in the example of Fig. 3, where
the energy ﬂow hypergraph of the optimal solution has only one component,
which contains two chains of package moves. Furthermore, it is no longer the
case that all of the nodes in the tail of a given hyperarc have the same height.
This can be seen even in cases where the optimal solution consists of only one
component with only one chain of package moves (see Fig. 4).
Fig. 4. An example of a graph (left, with H = {h1, h2}) in which the energy ﬂow
hypergraph of the optimal solution (right) contains a hyperarc with nodes of diﬀerent
heights in its tail.
References
1. Anaya, J., Chalopin, J., Czyzowicz, J., Labourel, A., Pelc, A., Vax`es, Y.: Converge-
cast and broadcast by power-aware mobile agents. Algorithmica 74(1), 117–155
(2016)
2. Awerbuch, B., Betke, M., Rivest, R.L., Singh, M.: Piecemeal graph exploration by
a mobile robot. Inf. Comput. 152(2), 155–172 (1999)
3. B¨artschi, A., Chalopin, J., Das, S., Disser, Y., Geissmann, B., Graf, D., Labourel,
A., Mihal´ak, M.: Collaborative delivery with energy-constrained mobile robots. In:
Suomela, J. (ed.) SIROCCO 2016. LNCS, vol. 9988, pp. 258–274. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-48314-6 17
4. B¨artschi, A., Chalopin, J., Das, S., Disser, Y., Graf, D., Hackfeld, J., Penna, P.:
Energy-eﬃcient delivery by heterogeneous mobile agents. In: Vollmer, H., Vall´ee, B.
(eds.) 34th Symposium on Theoretical Aspects of Computer Science, STACS 2017.
LIPIcs, 8–11 March 2017, Hannover, Germany, vol. 66, pp. 10:1–10:14. Schloss
Dagstuhl - Leibniz-Zentrum fuer Informatik (2017)
5. B¨artschi, A., Graf, D., Penna, P.: Truthful mechanisms for delivery with mobile
agents. CoRR abs/1702.07665 (2017)
6. Betke, M., Rivest, R.L., Singh, M.: Piecemeal learning of an unknown environment.
Mach. Learn. 18(2), 231–254 (1995)

12
E. Bampas et al.
7. Bil´o, D., Disser, Y., Gual´a, L., Mihal’´ak, M., Proietti, G., Widmayer, P.: Polygon-
constrained motion planning problems. In: Flocchini, P., Gao, J., Kranakis, E.,
Meyer auf der Heide, F. (eds.) ALGOSENSORS 2013. LNCS, vol. 8243, pp. 67–82.
Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-45346-5 6
8. Chalopin, J., Das, S., Mihal’´ak, M., Penna, P., Widmayer, P.: Data delivery by
energy-constrained mobile agents. In: Flocchini, P., Gao, J., Kranakis, E., Meyer
auf der Heide, F. (eds.) ALGOSENSORS 2013. LNCS, vol. 8243, pp. 111–122.
Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-45346-5 9
9. Chalopin, J., Jacob, R., Mihal´ak, M., Widmayer, P.: Data delivery by energy-
constrained mobile agents on a line. In: Esparza, J., Fraigniaud, P., Husfeldt,
T., Koutsoupias, E. (eds.) ICALP 2014 Part II. LNCS, vol. 8573, pp. 423–434.
Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-43951-7 36
10. Czyzowicz, J., Diks, K., Moussi, J., Rytter, W.: Communication problems for
mobile agents exchanging energy. In: Suomela, J. (ed.) SIROCCO 2016. LNCS,
vol. 9988, pp. 275–288. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
48314-6 18
11. Czyzowicz, J., Kranakis, E., Krizanc, D., Lambadaris, I., Narayanan, L.,
Opatrny, J., Stacho, L., Urrutia, J., Yazdani, M.: On minimizing the maxi-
mum sensor movement for barrier coverage of a line segment. In: Ruiz, P.M.,
Garcia-Luna-Aceves, J.J. (eds.) ADHOC-NOW 2009. LNCS, vol. 5793, pp. 194–
212. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-04383-3 15
12. Das, S., Dereniowski, D., Karousatou, C.: Collaborative exploration by energy-
constrained mobile robots. In: Scheideler, C. (ed.) Structural Information and
Communication Complexity. LNCS, vol. 9439, pp. 357–369. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-25258-2 25
13. Demaine, E.D., Hajiaghayi, M., Mahini, H., Sayedi-Roshkhar, A.S., Oveisgharan,
S., Zadimoghaddam, M.: Minimizing movement. ACM Trans. Algorithms 5(3),
1–30 (2009)
14. Dereniowski, D., Disser, Y., Kosowski, A., Paj ak, D., Uzna´nski, P.: Fast collabo-
rative graph exploration. Inf. Comput. 243, 37–49 (2015)
15. Dobrev, S., Durocher, S., Hesari, M.E., Georgiou, K., Kranakis, E., Krizanc, D.,
Narayanan, L., Opatrny, J., Shende, S.M., Urrutia, J.: Complexity of barrier cov-
erage with relocatable sensors in the plane. Theoret. Comput. Sci. 579, 64–73
(2015)
16. Duncan, C.A., Kobourov, S.G., Kumar, V.S.A.: Optimal constrained graph explo-
ration. In: 12th ACM Symposium on Discrete Algorithms, SODA 2001, pp. 807–814
(2001)
17. Dynia, M., Korzeniowski, M., Schindelhauer, C.: Power-aware collective tree explo-
ration. In: Grass, W., Sick, B., Waldschmidt, K. (eds.) ARCS 2006. LNCS,
vol. 3894, pp. 341–351. Springer, Heidelberg (2006). https://doi.org/10.1007/
11682127 24
18. Dynia, M., Lopusza´nski, J., Schindelhauer, C.: Why robots need maps. In:
Prencipe, G., Zaks, S. (eds.) SIROCCO 2007. LNCS, vol. 4474, pp. 41–50. Springer,
Heidelberg (2007). https://doi.org/10.1007/978-3-540-72951-8 5
19. Fraigniaud, P., G¸asieniec, L., Kowalski, D.R., Pelc, A.: Collective tree exploration.
Networks 48(3), 166–177 (2006)
20. Ortolf, C., Schindelhauer, C.: Online multi-robot exploration of grid graphs with
rectangular obstacles. In: 24th ACM Symposium on Parallelism in Algorithms and
Architectures, SPAA 2012, pp. 27–36 (2012)
www.ebook3000.com

Data Collection in Population Protocols
with Non-uniformly Random Scheduler
Joﬀroy Beauquier1, Janna Burman1(B), Shay Kutten2, Thomas Nowak1,
and Chuan Xu1(B)
1 LRI, Universit´e Paris Sud, CNRS, Universit´e Paris Saclay, Orsay, France
{joffroy.beauquier,janna.burman,thomas.nowak,chuan.xu}@lri.fr
2 Technion - Israel Institute of Technology, Haifa, Israel
kutten@ie.technion.ac.il
Abstract. Contrary to many previous studies on population protocols
using the uniformly random scheduler, we consider a more general non-
uniform case. Here, pair-wise interactions between agents (moving and
communicating devices) are assumed to be drawn non-uniformly at ran-
dom. While such a scheduler is known to be relevant for modeling many
practical networks, it is also known to make the formal analysis more
diﬃcult.
This study concerns data collection, a fundamental problem in mobile
sensor networks (one of the target networks of population protocols).
In this problem, pieces of information given to the agents (e.g., sensed
values) should be delivered eventually to a predeﬁned sink node without
loss or duplication. Following an idea of the known deterministic proto-
col TTF solving this problem, we propose an adapted version of it and
perform a complete formal analysis of execution times in expectation and
with high probability (w.h.p.).
We further investigate the non-uniform model and address the impor-
tant issue of energy consumption. The goal is to improve TTF in terms of
energy complexity, while still keeping good time complexities (in expec-
tation and w.h.p.). Namely, we propose a new parametrized protocol
for data collection, called lazy TTF, and present a study showing that a
good choice of the protocol parameters can improve energy performances
(compared to TTF), at a slight expense of time performance.
1
Introduction
Population protocols have been introduced in [1] as a model for passively mobile
sensor networks (cf. the journal version [2]). In this model, tiny indistinguishable
agents with bounded memory move unpredictably and interact in pairs. That is,
when two agents are suﬃciently close to each other, they can communicate (i.e.,
interact). During an interaction, they exchange and update their respective states
according to a transition function (the protocol). Such successive interactions
contribute to the realization of some global task.
The fact that agent moves are unpredictable is usually modeled by assuming
the uniformly random scheduler [2–5]. That is, the interactions between any
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 13–25, 2017.
https://doi.org/10.1007/978-3-319-72751-6_2

14
J. Beauquier et al.
two agents are drawn uniformly at random. However, for some practical sensor
networks, this assumption may be unrealistic. Consider, for instance, agents
moving at diﬀerent speeds. In this case, an agent interacts more frequently with
a faster agent than with a slower one. In other networks, certain agents may
be frequently prevented from communicating with some others, because they
move in diﬀerent limited areas, or disfunction from time to time, etc. In all these
examples, the interactions are clearly not uniformly random. There are thus
strong arguments for enhancing the basic model.
This paper initiates the study of non-uniform schedulers in the context of
population protocols. Considering the scheduler as the generator of sequences of
pairwise interactions, non-uniform means that the next interacting pair (i, j) is
chosen with a non-uniform probability Pi,j, depending on i and j.
As a supplementary justiﬁcation for studying a non-uniform scheduler, notice
that many experimental and analytical studies of diﬀerent (ﬁnite boundary)
mobile sensor networks show and exploit (respectively) the assumption that
the inter-contact time of two agents (the time period between two successive
interactions of the same two mobile agents) is distributed exponentially (cf.
[6–9]). Similarly, under a non-uniformly random scheduler, it appears that the
inter-contact time Ti,j, of any two agents i and j, follows a geometric distribution
(P[Ti,j = t] = (1−Pi,j)t−1Pi,j), which is the discrete analogue of the exponential
case (observed in practical mobile networks).
The counterpart of considering a non-uniform scheduler is a more complex
analysis. Though, it remains feasible in certain cases, as it is shown in this
paper. To illustrate this point, consider a fundamental task for mobile sensors,
data collection (or data gathering). In this task, each agent has initially an input
value (for instance, a sensed value). Each value must be gathered exactly once (as
a multi-set) by a special agent which we call the base station. In the context of
population protocols (assuming non-random schedulers), several data collection
protocols have been proposed and their complexity in time has been studied [10].
Notice that the analysis there was only for the worst case. We are not aware of
any previous results concerning the average complexity of these protocols. The
current paper presents protocols that basically use the simple ideas of the TTF
(Transfer To the Faster) protocol of [10]. The new protocols are adapted to a
non-uniform scheduler and improve energy consumption, as explained further.
First, consider the original version of TTF. It uses a deterministic parameter
called cover time, which is an upper bound on the time, counted in the number
of global interactions, for an agent to interact with all the others. The data
transfer between the agents in TTF depends on the comparison of cover times
of two interacting agents. Here we follow this idea. However, as the scheduler
is probabilistic, we adapt the corresponding deﬁnition of the cover time to be
the expected (instead of the maximum) number of interactions for an agent to
interact with every other agent (see Sect. 2).
The complexity analysis starts with the proofs of two lower bounds on the
expected convergence time of any protocol solving data collection (Sect. 3). Then,
an analysis of execution times in expectation and with high probability (w.h.p.),
www.ebook3000.com

Data Collection in Population Protocols
15
for the new version of TTF, is given (Sect. 4). The complexity in expectation
indicates how the protocol is good in average, while the complexity w.h.p. tells
how it is good almost all the time. We obtain explicit bounds, thus justifying
the relevance of the enhanced model in protocol analysis and its operability.
We further investigate the non-uniform model by addressing also energy com-
plexity, which is known to be a crucial issue for sensor networks. The goal is to
improve energy consumption of TTF, while keeping good time complexity. For
that, we propose a new parametrized protocol, called lazy TTF (Sect. 5). As
opposed to TTF, it does not execute necessarily the transition of TTF result-
ing from an interaction. Instead, during an interaction (i, j), TTF is executed
with probability pi (depending on agent i, playing the role of initiator in the
interaction). Analysis and the corresponding numerical study show that a good
choice of the parameters pi results in lower energy consumption. To ﬁnd such
parameters, we formulate and solve a polynomial-time optimization program.
The resulting optimized lazy TTF is compared to TTF in respect with time
and energy complexity (Sect. 6). For this analysis, we adopt the energy scheme
proposed for population protocols in [11].
Due to the lack of space, most of the proofs and the survey on additional
related work have been moved to the report [12].
2
Model and Deﬁnitions
Population Protocols. The system is represented by an interaction graph G =
(A, E), a table T of transition rules and a scheduler S(P). All are deﬁned below.
A set A consists of n anonymous agents and is also called a population. An
agent i ∈A represents a ﬁnite state sensing and communicating mobile device,
which can be seen as a ﬁnite state machine. The size of the population n is
unknown to the agents. Among the agents, there is a distinguishable one called
the base station (BST), which can be as powerful as needed, in contrast with
the resource-limited agents. The non-BST agents are also called mobile. Each
agent has a state that is taken from a ﬁnite set of states which is the same for
all mobile agents, but possibly diﬀerent for the base station.
A directed edge (i, j) ∈E intuitively represents a possible interaction between
two agents. That is, if such an edge exists, then the scheduler (see below) is
allowed to schedule an event, called interaction, between i, called then the ini-
tiator, and j, called the responder for that event. In this work, we consider
only complete interaction graphs. What happens in the interaction event is now
described.
When two agents i, in state p, and j, in state q, interact (meet), they execute
a transition (p, q) →(p′, q′). As a result, i changes its state from p to p′ and
j from q to q′. The table T of all the transition rules deﬁnes the population
protocol. A protocol (respectively, its transition rules) are called deterministic,
if for every pair of states (p, q), there is exactly one (p′, q′) such that (p, q) →
(p′, q′). Otherwise, they are non-deterministic. Note that, as interactions are
supposed to be asymmetric (with one agent acting as the initiator and the other
as the responder), the transition rules for (p, q) and (q, p) may be diﬀerent.

16
J. Beauquier et al.
A conﬁguration of the system is deﬁned by the vector of agents’ states. If,
in a given conﬁguration C, a conﬁguration C′ can be obtained by executing
one transition of the protocol (between two interacting agents), it is denoted by
C →C′. An execution of a protocol is a sequence of conﬁgurations C0, C1, C2, . . .
such that C0 is the initial conﬁguration and for each i ≥0, Ci →Ci+1. We
consider the number of interactions in an execution as the time reference, i.e.,
each interaction adds one time unit to the global time. This is similar to the
step complexity, a common measure in population protocols (cf. [2,13]) and in
distributed computing in general [14].
The sequence of the corresponding interactions in an execution is provided
by an external entity called scheduler.
Non-uniformly Random Scheduler. Such a scheduler, denoted by S(P),
is deﬁned by a matrix of probabilities P ∈Rn×n. During an execution, S(P)
chooses the next pair of agents (i, j) to interact (taking i as initiator and j as
responder) with the probability Pi,j. Notice that, in the case of the matrix with
entries Pi,j = 1/n(n −1) for i ̸= j, and Pi,i = 0, the scheduler chooses each pair
of agents uniformly at random for each next interaction (i.e., the scheduler is
uniformly random).
The matrix P satisﬁes
n
i=1
n
j=1
Pi,j = 1 and ∀i ∈{1, . . . , n}, Pi,i = 0, since
interactions are pairwise. Moreover, for any edge (i, j) in the interaction graph
G, Pi,j > 0. As the considered here G is complete, every pair of agents is chosen
inﬁnitely often with probability 1.
For a given P, one can compute the expected (ﬁnite) time for a given agent i
to meet all the others. We call it cover time of agent i and denote it by cvi. By
resolving the coupon collector’s problem with a non-uniform distribution [15],
we obtain the cover time of each agent: cvi =
 ∞
0 (1−
j̸=i(1−e−(Pi,j+Pj,i)t))dt.
Similarly to [10], for two agents i and j, if cvi < cvj, we say that i is faster than
j, and j is slower than i. If cvi = cvj, i and j are said to be in the same category
of cover times. We denote by m the number of diﬀerent categories of cover times.
We emphasize that agents are not assumed to know their cvs (to conform with
the ﬁnite state population protocol model). Instead, we do assume that two
interacting agents can compare their respective cvs. For instance, this can be
implemented by comparing categories instead of cvs, in applications where the
overall number of categories is likely to be uniformly bounded.
Data Collection. Each agent, except the base station, owns initially a constant
input value. Eventually, every input value has to be delivered to the base station,
and exactly once (as a multi-set). When this happens, we say that a terminal
conﬁguration or simply termination has been reached. A protocol is said to solve
data collection if termination is reached in every execution of the protocol.
In the sequel, when describing or analyzing a protocol, the term “transfer an
input value (or token) from agent i to j” means copy it to j’s memory and erase
it from the memory of i. In particular, this prevents loss or duplication of input
values. Moreover, in this preliminary study, we make the assumption that every
www.ebook3000.com

Data Collection in Population Protocols
17
agent has enough memory to store n values. This assumption is common in the
literature [4,16].
Time Complexity Measures. The convergence time of a data collection pro-
tocol P can be evaluated in two ways: ﬁrst, in terms of expected time until
termination, denoted by TE(P), and second, in terms of time until termination
w.h.p.1, denoted by Twhp(P).
Remark 1. The notion of parallel time, which is common when considering the
uniformly random scheduler (cf. [3,17]), is not used in this paper. When using
this measure of time, it is assumed that each agent participates in an expected
number Θ(1) of interactions per time unit. With the uniformly random scheduler,
this time measure is asymptotically equal to the number of interactions divided
by n. However, with non-uniformly random scheduler, this is no more true.
3
Lower Bounds on the Expected Convergence Time
We now give two nontrivial lower bounds on the expected convergence time of
data collection protocols. The ﬁrst one (Theorem 1) only depends on the num-
ber of agents. The second one (Theorem 2) depends on the speciﬁc values of the
probability matrix P used by the scheduler. The bounds are incomparable in
general. To obtain the bounds, we observe that, for performing data collection,
each agent has to interact at least once (otherwise, its value simply won’t be
delivered), and we compute the expected time ensuring that. The proof of The-
orem 1 uses an analogy with a generalization of the classical coupon collector’s
problem, which we introduce next.
Let k be a positive integer. Given a probability distribution (p1, . . . , pk) on
[k] = {1, . . . , k}, the corresponding k-coupon collector’s problem is deﬁned by its
coupon sequence (X1, X2, . . . ) of independent and identically distributed (i.i.d.)
random variables with P(Xt = i) = pi for all i ∈[k] and all t ≥0. The k-coupon
collector’s problem’s expected time is the expectation of the earliest time T such
that {X1, . . . , XT } = [k], i.e., all coupons were collected at least once.
More generally, given a set A of subsets of [k] such that 
A∈A A = [k],
and a probability distribution (pA) on A, the corresponding A-group k-coupon
collector’s problem is deﬁned by its coupon group sequence (X1, X2, . . . ) of i.i.d.
random variables with P(Xt = A) = pA for all A ∈A and all t ≥0. Its expected
time is the expectation of the earliest time T such that T
t=1 Xt = [k], i.e., all
coupons were collected in at least one coupon group.
Given an integer 1 ≤g ≤k, the g-group k-coupon collector’s problem is the
A-group k-coupon collector’s problem where A =

A ⊆[k] | |A| = g

. This gen-
eralization of the classical coupon collector’s problem has been studied, among
others, by Stadje [18], Adler and Ross [19], and Ferrante and Saltalamacchia [20].
The following lemma characterizes the probability distributions that lead to
a minimal expected time for the group coupon collector’s problem. To the best
1 An event Ξ is said to occur w.h.p., if P(Ξ) ≥1 −
1
nc , where c ≥1.

18
J. Beauquier et al.
of our knowledge, this is a new result which generalizes the characterization in
the classical coupon collector’s problem [15,21], for which it is known that the
uniform distribution leads to the minimal expected time.
Lemma 1. The expected time of any A-group k-coupon collector’s problem is
greater than or equal to the B-group k-coupon collectors problem with uniform
probabilities where B ⊆A is of minimal cardinality such that  B = [k].
In particular, the expected time of any g-group k-coupon collector’s problem
is Ω(k log k) for every constant g ≥1.
Theorem 1. The expected convergence time of any protocol solving data collec-
tion with non-uniformly random scheduler is Ω(n log n).
Theorem 2. The expected convergence time of any protocol solving data collec-
tion with random scheduler S(P), is Ω(max
i
1
n
j=1(Pi,j+Pj,i)).
The next corollary considers a very simple protocol solving the data collection
problem. In this protocol, agents transfer their values only when they interact
with the base station. We consider it as a reference, to compare with other
proposed protocols. The corollary follows from Theorem 2.
Corollary 1. With random scheduler S(P), the expected convergence time of
the protocol solving data collection and where each agent transfers its value only
to the base station is Ω(max
i
1/(Pi,BST + PBST,i)).
4
Protocol “Transfer to the Faster” (TTF)
Corollary 1 formalizes the straightforward observation that, if the only trans-
fers performed by the agents are towards the base station, the convergence time
depends on the slowest agent i. It can be very large, e.g. if Pi,BST + PBST,i ≪
1/n2. Therefore, to obtain better time performances, we propose to study
another data collection protocol based on the idea of the TTF protocol of [10].
In the sequel, the studied protocol is called TTF too, since its strategy is the
same and there is no risk of ambiguity. The only diﬀerence is on the deﬁnition
of the cover time parameter (Sect. 2) used by this strategy (as explained in the
introduction).
The strategy of TTF is easy. When agent i meets a faster agent j, i transfers
to j all the values it has in its memory (recall that transfer means to copy to
the memory of the other and erase from its own). The intuition behind is that
the faster agent j is more likely to meet the base station before i. Of course,
whenever any agent i meets the base station, it transfers all the values it (still)
has in its memory at that time to the base station. As a matter of fact, no
transition depends on the actual value held by the agents. It depends only on
the comparison between cover times, which are constants. Thus, the input values
can be seen as tokens and the states of every agent can be represented by the
number of tokens it currently holds. Recall, that in this study, it is assumed that
www.ebook3000.com

Data Collection in Population Protocols
19
each agent has enough memory for storing the tokens (i.e., an O(n) memory),
and each pair of agents interacts inﬁnitely often (i.e., the interaction graph is
complete).
The sequel concerns analytical results on the time performance of TTF.
Firstly, we associate to each conﬁguration a vector of non-negative integers rep-
resenting the number of tokens held by each agent. Then, it is shown that the
evolution of such vectors during executions can be expressed by a stochastic
linear system. Next, Twhp(TTF) is expressed in terms of distances between the
conﬁguration vectors (Theorem 3) and, by applying stochastic matrix theory [22–
24] an upper bound on Twhp(TTF) is obtained (Theorem 4). Finally, using this
result, we obtain also an upper bound on the convergence time in expectation,
TE(TTF) (Theorem 5).
Formally, we represent a conﬁguration by a non-negative integer vector x ∈
Nn that satisﬁes n
i=1 xi = n −1. By abusing the terminology, we sometimes
call such a vector a conﬁguration. We denote the conﬁguration vectors’ space by
V. By convention, the ﬁrst element of x is the number of tokens held by the base
station. Since, at the beginning of an execution, every mobile agent owns exactly
one token and no token is held by the base station, the initial conﬁguration is
xinit = 1 −e1, where ei =

0, . . . , 0, 1, 0, . . . , 0
	T is the n × 1 unit vector with
the ith component equal to 1. The terminal conﬁguration is xend = (n −1)e1.
Let x(t) ∈V be the discrete random integer vector that represents the
conﬁguration just after tth interaction in executions of TTF. We can see that
P(x(0) = xinit) = 1, and since the base station never transfers tokens to others,
P(x(t + 1) = xend) ≥P(x(t) = xend). Moreover, since at any moment there
is a positive probability for delivering any of the tokens to the base station,
limt→∞P(x(t) = xend) = 1. Furthermore, the time complexities of TTF can be
formalized using x(t) by TE(TTF) = ∞
t=1 t · (P(x(t) = xend ∧x(t −1) ̸= xend))
and Twhp(TTF) = inf

t | P(x(t) = xend) ≥1 −1
n

.
To evaluate these time complexities, we study the evolution of x(t) during
executions of TTF. Given time t, consider a transition rule applicable from a
conﬁguration represented by a vector vt and resulting in a conﬁguration with
vector vt+1. Suppose that at time t, the interaction (i, j) is chosen by the sched-
uler. If neither i nor j are the base station and i is faster than j (cvi < cvj),
agent j transfers all its tokens to i. Thus, vt+1
i
= vt
i + vt
j and vt+1
j
= 0. The rela-
tion between vt and vt+1, in this case, can be expressed by the linear equation
vt+1 = W(t + 1)vt, where W(t + 1) = I + eieT
j −ejeT
j ∈{0, 1}n×n. If cvi = cvj,
no token is transferred and vt+1 = vt. We still have vt+1 = W(t + 1)vt, but
with W(t + 1) = I. On the other hand, if j is the base station, W(t + 1) =
I + eieT
j −ejeT
j , as agent i transfers all of its tokens to the base station.
As the pair of agents is chosen independently with respect to P, W(t + 1)
can be seen as a random matrix such that with probability Pi,j + Pj,i:
W(t + 1) =

I + eieT
j −ejeT
j
if cvi < cvj or i = 1 or j = 1
I
if cvi = cvj
(1)

20
J. Beauquier et al.
By comparing the resulting probability distributions, we readily verify that
the relation between x(t) and x(t+1), i.e., x(t+1) = W(t+1)x(t), is a stochastic
linear system with the matrices speciﬁed in (1).
Distance. Consider a function dγ(x) : V →R. It associates any x in V to
a real number representing a “weighted” Euclidian norm distance between the
conﬁguration vector x and the vector representing a terminal conﬁguration. That
is, dγ(x) = ||(x −xend) ◦γ||2, where γ ∈Rn is a real vector, ◦the entry-
wise product, and || · ||2 the Euclidean norm. The vector γ can be viewed as a
weight vector. We choose γ in such a way that, if there is a transfer of tokens in
interaction t+1, conﬁguration vt, then dγ(vt+1) is smaller than dγ(vt). Intuitively
this means that, when a transfer is performed, the resulting conﬁguration is closer
to termination.
Lemma 2. Let i and j be two agents with cvi < cvj. Consider an interaction
between i and j in a conﬁguration represented by vt and resulting in vt+1. If
γj/γi ≥√2n −3, then dγ(vt+1) ≤dγ(vt).
Theorem 3. The convergence time with high probability of TTF, Twhp(TTF), is
equal to inf

t | P

dγ(x(t))
dγ(xinit) < (2n)
−(m−1)
2

≥1 −1/n

if γBST = 0 and γj/γi ≥
√
2n whenever cvi < cvj. Recall that m ≤n denotes the number of cover time
categories (Sect. 2).
We are now ready to state and prove our main upper bound on the conver-
gence time of TTF, Twhp(TTF) (Theorem 4). To prove it, we apply stochastic
matrix theory to the stochastic linear system deﬁned above for x(t).
Without loss of generality, we assume that cv2 ≤cv3 ≤· · · ≤cvn. We
choose γ ∈Rn by setting γ1 = 0, γ2 = 1, and γi+1 = γi, if cvi+1 = cvi, and
γi+1 = γi
√
2n, if cvi+1 > cvi. In particular, γn = (2n)(m−1)/2.
Theorem 4. With a non-uniformly random scheduler S(P), the convergence
time of TTF is at most
m log 2n
log λ2( ˜
W )−1 with high probability, where γ is deﬁned
above. Γi,j = γi/γj, ˜W =

i<j∧cvi<cvj
(Pi,j + Pj,i)W Γ 2
ij +

i<j∧cvi=cvj
(Pi,j + Pj,i)I,
W Γ 2
ij
= I + Γi,j(eieT
j + ejeT
i ) + (Γ 2
i,j −1)ejeT
j , and λ2(A) denotes the modulus
of the second largest eigenvalue of matrix A.
Now, we study the performance of TTF with respect to the convergence time
in expectation, i.e. TE(TTF).
Theorem 5. The
expected
convergence
time
of
the
TTF
protocol
is
O

m log n
log λ2( ˜
W )−1

where ˜W is the matrix deﬁned in Theorem 4.
5
Lazy TTF
The strategy of TTF may result in a long execution when an input value is
transferred many times before being ﬁnally delivered to the base station. These
www.ebook3000.com

Data Collection in Population Protocols
21
transfers are certainly energy consuming. Then a natural issue is to transform
TTF in order to save energy, while keeping the time complexity as low as possi-
ble. The idea is to prevent certain data transfers, for example, when it is more
likely to meet soon a faster agent and thus possibly make fewer transfers in
overall. We propose a simple protocol based on TTF, called lazy TTF. In con-
trast with TTF, lazy TTF does not necessarily execute the transition resulting
from an interaction. It chooses randomly to execute it or not. Formally, dur-
ing an interaction (i, j), with agent i acting as initiator, TTF is executed with
probability pi, where p ∈Rn is a vector of probabilities.
Notice that the choice of executing TTF depends uniquely on the initia-
tor i. In practical terms, an initiator represents an agent that, by sensing the
environment, has detected another agent j. At this moment i takes the random
decision (with probability pi) whether a TTF transition should be executed and
the interaction itself should take place, or not. In the latter case, not only the
energy for the eventual data transfer is saved, but also the energy for establishing
the interaction.
Observe that when p is the vector of all ones, lazy TTF behaves as TTF and
its energy consumption is the same as for TTF. However, when p is the vector of
all zeros, lazy TTF does not solve the problem of data collection as no value is
ever transferred to the base station, but no energy is consumed for transferring
of data or establishing interactions. Depending on p, time complexities of lazy
TTF can be worse than of TTF, given the same scheduler. At the same time,
longer executions of lazy TTF may be more energy eﬃcient. Thus, there is a
trade-oﬀbetween time and energy performance depending on the values of p.
We investigate the choice of p for obtaining good time/energy trade-oﬀ. Firstly,
we give upper bounds on the time complexities of lazy TTF. Then, we introduce
an optimization problem that takes p as a variable. Finally, numerical results in
Sect. 6 demonstrate energy eﬃciency of lazy TTF, given the optimal p.
5.1
Convergence Time of Lazy TTF
To obtain an upper bound on the convergence time of lazy TTF, we show a par-
ticular equivalence of lazy TTF under scheduler S(P) with TTF under scheduler
S(P ◦(p · 1T )), where 1 is the vector of all ones and ◦presents the entry-wise
product. This equivalence is on the level of distribution of conﬁgurations of the
two protocols. Precisely, as we show below, the random vector x(t) for these
two protocols is exactly the same, allowing to use Theorem 4 to obtain a time
complexity upper bound for lazy TTF.
Let us express x(t) in case of lazy TTF in a similar way as we did before
for TTF in Sect. 4. First, P(x(0) = xinit) = 1 is the same as for TTF. Then,
x(t + 1) = W(t + 1)x(t) and W(t + 1) can be seen as a random matrix such
that, with probability Pi,j × pi + Pj,i × pj, W(t + 1) is as in Eq. 1. Notice that
x(t) in case of TTF under S(P ◦(p · 1T )) is expressed exactly in the same way
(Sect. 4). Thus, by applying Theorem 4 for TTF under S(P ◦(p·1T )), we obtain
the upper bound on Twhp(lazy TTF(p)).

22
J. Beauquier et al.
Theorem 6. With a non-uniformly random scheduler S(P), the convergence
time with high probability of lazy TTF is at most
m log 2n
log λ2( ˜
W )−1 ,
where ˜
W =

cvi<cvj
(Pi,jpi + Pj,ipj)W Γ 2
ij
+

cvi<cvj
(Pi,j(1 −pi) + Pj,i(1 −pj)I
+

cvi=cvj
(Pi,j + Pji)I, and W Γ 2
ij
= I + Γi,j(eieT
j + ejeT
i ) + (Γ 2
i,j −1)ejeT
j .
(2)
Then, the upper bound on TE(lazy TTF(p)) can be obtained in the same way as
in Th. 5.
To summarize, note that, as executions of lazy TTF are equivalent to those
of TTF under S(P ◦(p · 1T )) in the sense explained above, one can imagine
that lazy TTF transforms the matrix of interaction probabilities “on the ﬂy”
(during executions). It can be also seen as if it transforms the interaction graph
itself. Indeed, certain vectors p may make some pairs of agents to interact with
extremely small probability (or not interact at all), thus eﬀectively remove these
pairs from the graph. This is illustrated in the report [12]. Next, we are looking for
vectors p, optimizing an upper bound on the time performance of lazy TTF(p) to
ensure a good time energy trade-oﬀ. Equivalently, we are looking for schedulers
(matrices P) for which the original TTF is eﬃcient in this sense.
Thus, the goal is to ﬁnd a vector p minimizing the upper bound on
Twhp(lazy TTF(p)) (Theorem 6). To that end, an optimization program OP1,
taking p as a variable, is proposed as follows:
OP1 :
min
p∈Rn λ2( ˜W) s.t Eq. 2, 0 ≤pi ≤1.
By Theorem 6, minimizing the upper bound of Twhp(lazy TTF(p)) is equiva-
lent to minimizing the second largest eigenvalue of ˜W. Then, we reformulate
OP1 as a semi-deﬁnite program [25,26] OP2 which is convex and can be solved
in polynomial time.
OP2 :
min
p∈Rn,s s s.t sI −˜W ⪰0, Eq. 2, 0 ≤pi ≤1.
Let ˆp be the optimal solution of OP2. We can see that if ˆp is all ones vector,
lazy TTF(ˆp) performs as TTF. Otherwise, lazy TTF(ˆp) outperforms TTF in
terms of the upper bounds on time. This optimized upper bound ensures that
lazy TTF(ˆp) converges in a reasonable time. In the next section, by the numerical
results obtained for diﬀerent small examples, we demonstrate the eﬃciency of
lazy TTF(ˆp), in terms of energy consumption.
6
Numerical Results
6.1
The Relation Between Twhp(TTF) and Its Upper Bound
The goal of this section is to justify the relevance of the method used here to
obtain the optimal probability vector p for lazy TTF. To justify this, we show
by simulations that the time upper bound value for TTF is well correlated with
www.ebook3000.com

Data Collection in Population Protocols
23
the exact value of its time complexity (calculated by Markov chains, for small
systems). This implies the same correlation for lazy TTF, because the bounds
in Theorems 4 and 6 are obviously well correlated too (one is obtained from the
other; see Sect. 5). That is why the optimal probability vector p for the upper
bound of lazy TTF is close to the optimal vector for the real (tight) convergence
time.
From Theorem 4, we have an upper bound on time w.h.p. for TTF, denoted
here by Tupp(TTF). In this section, we show the relation between Tupp(TTF)
and Twhp(TTF). In our experiment, two systems of size 4 and 5 are considered
and 100 schedulers are generated randomly for each system. Since the system
is of small size, for each scheduler s, the exact value of T s
whp(TTF) can be
obtained by constructing the corresponding Markov Chain. The upper bound,
T s
upp(TTF), can be calculated by Theorem 4. Then, for every generated s, we
plot T s
whp(TTF) and T s
upp(TTF) on the ﬁgure with x-axis for Twhp(TTF) and
y-axis for Tupp(TTF).
Fig. 1.
Relation
between
Twhp(TTF)
and
Tupp(TTF).
From Fig. 1, we can see that
Tupp(TTF) has a nearly lin-
ear relation with Twhp(TTF). It
means that Tupp(TTF) in The-
orem 4 captures well the rela-
tion of the scheduler’s behav-
ior to the time performance
of TTF in most of the cases.
Moreover, it demonstrates that,
for
lazy
TTF,
minimizing
Twhp(lazy TTF(p)) in Sect. 5, is
reasonable for improving the
energy performance.
6.2
Gaps on Time and Energy Between TTF and Lazy TTF(ˆp)
For energy consumption analysis, we consider the energy model of [11] proposed
for population protocols. In this model, an agent senses its vicinity by proximity
sensor, consuming a negligible amount of energy [27]. Once the interaction is
established, each participant consumes a ﬁxed amount of energy Ewkp (mainly
for switching on its radio, which is known to be very energy consuming; cf. [28]).
Now, recall that, with lazy TTF, the choice of executing TTF depends on the
probability pi of the initiator i. If TTF should not be executed, the initiator
does not proceed to establish the interaction neither (i.e., Ewkp is not spent), as
explained in Sect. 5.
We study the expectation of the total energy consumption of a protocol P,
denoted E(P). According to the energy scheme explained above, E(P) is eval-
uated by the expected total energy spent for establishing all the interactions

24
J. Beauquier et al.
till convergence. It is proportional to the time expectation TE(P). In partic-
ular, E(TTF) = 2TE(TTF) · Ewkp and E(lazyTTF(p)) = 2TE(lazy TTF(p)) ×

i

j(Pi,jpi + Pj,ipj) × Ewkp.
For the systems of small size with a scheduler s, the exact values of T s
E(TTF)
and T s
E(lazy TTF(ˆps)) can be calculated by constructing the corresponding
Markov Chain. In the experiments, systems of size 4, 5, 6, 7 and 8 are considered
and for each size n, 10000 diﬀerent schedulers are generated randomly. Denote
by S(n) the set of these schedulers. For each scheduler s ∈S(n), T s
E(TTF),
ˆps, T s
E(lazy TTF(ˆps)), Es(TTF) and Es(lazy TTF(ˆps)) are evaluated. Then, the
gaps on time and on energy between lazy TTF(ˆps) and TTFs are denoted by
Gap(TE, n) and Gap(E, n), respectively, and are computed as follows.
Gap(TE, n) =

s∈S(n)
T s
E(lazy TTF(ˆps))−T s
E(TTF)
T s
E(TTF)

/10000 and
Gap(E, n) =

s∈S(n)
Es(lazy TTF(ˆps))−Es(TTF)
Es(TTF)

/10000.
Table 1. Gaps on time and energy.
Size n
Gap(TE, n)
Gap(E, n)
4
11.60%
−15.32%
5
17.10%
−23.60%
6
22.04%
−30.79%
7
26.31%
−36.99%
8
27.41%
−39.07%
Results appear in Table 1. In column 3,
it can be seen that lazy TTF consumes less
energy than TTF for all systems. Lazy TTF
saves at least 15% of energy. The counter-
part is (a slight) increase in the execution
time, as shown in column 2.
References
1. Angluin, D., Aspnes, J., Diamadi, Z., Fischer, M.J., Peralta, R.: Computation in
networks of passively mobile ﬁnite-state sensors. In: Proceedings of 23rd Annual
ACM Symposium on Principles of Distributed Computing, pp. 290–299 (2004)
2. Angluin, D., Aspnes, J., Diamadi, Z., Fischer, M.J., Peralta, R.: Computation in
networks of passively mobile ﬁnite-state sensors. Distrib. Comput. 18(4), 235–253
(2006)
3. Angluin, D., Aspnes, J., Eisenstat, D.: Fast computation by population protocols
with a leader. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 61–75. Springer,
Heidelberg (2006). https://doi.org/10.1007/11864219 5
4. Alistarh, D., Gelashvili, R., Vojnovi´c, M.: Fast and exact majority in population
protocols. In: Proceedings of 2015 ACM Symposium on Principles of Distributed
Computing, pp. 47–56. ACM (2015)
5. Aspnes, J., Beauquier, J., Burman, J., Sohier, D.: Time and space optimal counting
in population protocols. In: OPODIS 2016, pp. 13:1–13:17 (2016)
6. Sharma, G., Mazumdar, R.R.: Scaling laws for capacity and delay in wireless ad
hoc networks with random mobility. In: 2004 IEEE International Conference on
Communications, vol. 7, pp. 3869–3873 (2004)
7. Cai, H., Eun, D.Y.: Crossing over the bounded domain: from exponential to power-
law inter-meeting time in manet. In: Proceedings of 13th Annual ACM Interna-
tional Conference on Mobile Computing and Networking, pp. 159–170 (2007)
8. Zhu, H., Fu, L., Xue, G., Zhu, Y., Li, M., Ni, L.M.: Recognizing exponential inter-
contact time in vanets. In: 2010 Proceedings of INFOCOM, pp. 1–5 (2010)
www.ebook3000.com

Data Collection in Population Protocols
25
9. Gao, W., Cao, G.: User-centric data dissemination in disruption tolerant networks.
In: Proceedings of IEEE INFOCOM 2011, pp. 3119–3127 (2011)
10. Beauquier, J., Burman, J., Clement, J., Kutten, S.: On utilizing speed in networks
of mobile agents. In: Proceedings of 29th ACM SIGACT-SIGOPS Symposium on
Principles of Distributed Computing, pp. 305–314 (2010)
11. Xu, C., Burman, J., Beauquier, J.: Power-aware population protocols. In: 2017
IEEE 37th International Conference on Distributed Computing Systems (ICDCS),
pp. 2067–2074, June 2017
12. Beauquier, J., Burman, J., Kutten, S., Nowak, T., Xu, C.: Data collection in popu-
lation protocols with non-uniformly random scheduler. Research report, July 2017.
https://hal.archives-ouvertes.fr/hal-01567322
13. Alistarh, D., Aspnes, J., Eisenstat, D., Gelashvili, R., Rivest, R.L.: Time-space
trade-oﬀs in population protocols. In: Proceedings of 28th Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2017, pp. 2560–2579 (2017)
14. Tel, G.: Introduction to Distributed Algorithms, 2nd edn. Cambridge University
Press, Cambridge (2000). https://doi.org/10.1017/CBO9781139168724
15. Flajolet, P., Gardy, D., Thimonier, L.: Birthday paradox, coupon collectors,
caching algorithms and self-organizing search. Discret. Appl. Math. 39(3), 207–
229 (1992)
16. Guerraoui, R., Ruppert, E.: Names trump malice: tiny mobile agents can tolerate
byzantine failures. In: Albers, S., Marchetti-Spaccamela, A., Matias, Y., Nikolet-
seas, S., Thomas, W. (eds.) ICALP 2009. LNCS, vol. 5556, pp. 484–495. Springer,
Heidelberg (2009). https://doi.org/10.1007/978-3-642-02930-1 40
17. Angluin, D., Aspnes, J., Eisenstat, D.: A simple population protocol for fast robust
approximate majority. Distrib. Comput. 21, 87–102 (2008)
18. Stadje, W.: The collector’s problem with group drawings. Adv. Appl. Probab.
22(4), 866–882 (1990)
19. Adler, I., Ross, S.M.: The coupon subset collection problem. J. Appl. Probab. 38,
737–746 (2001)
20. Ferrante, M., Saltalamacchia, M.: The coupon collector’s problem. Mater.
Matem`atics 2014(2), 35 (2014)
21. Nakata, T.: Coupon collector’s problem with unlike probabilities (2008, preprint)
22. Tsitsiklis, J.N., Bertsekas, D.P., Athans, M.: Distributed asynchronous determin-
istic and stochastic gradient optimization algorithms. In: American Control Con-
ference 1984, pp. 484–489 (1984)
23. Jadbabaie, A., Lin, J., Morse, A.S.: Coordination of groups of mobile autonomous
agents using nearest neighbor rules. IEEE Trans. Autom. Control 48(6), 988–1001
(2003)
24. Ren, W., Beard, R.W., et al.: Consensus seeking in multiagent systems under
dynamically changing interaction topologies. IEEE Trans. Autom. Control 50(5),
655–661 (2005)
25. Vandenberghe, L., Boyd, S.: Semideﬁnite programming. SIAM Rev. 38(1), 49–95
(1996)
26. Helmberg, C., Rendl, F., Vanderbei, R.J., Wolkowicz, H.: An interior-point method
for semideﬁnite programming. SIAM J. Optim. 6(2), 342–361 (1996)
27. Razzaque, M.A., Dobson, S.: Energy-eﬃcient sensing in wireless sensor networks
using compressed sensing. Sensors 14(2), 2822–2859 (2014)
28. Rajendran, V., Obraczka, K., Garcia-Luna-Aceves, J.J.: Energy-eﬃcient, collision-
free medium access control for wireless sensor networks. Wirel. Netw. 12(1), 63–78
(2006)

Parameterized Algorithms for Power-Eﬃcient
Connected Symmetric Wireless Sensor Networks
Matthias Bentert1(B), Ren´e van Bevern2,3,
Andr´e Nichterlein1, and Rolf Niedermeier1
1 Institut f¨ur Softwaretechnik und Theoretische Informatik,
TU Berlin, Berlin, Germany
{matthias.bentert,andre.nichterlein,rolf.niedermeier}@tu-berlin.de
2 Novosibirsk State University, Novosibirsk, Russian Federation
rvb@nsu.ru
3 Sobolev Institute of Mathematics, Siberian Branch of the Russian
Academy of Sciences, Novosibirsk, Russian Federation
Abstract. We study an NP-hard problem motivated by energy-
eﬃciently maintaining the connectivity of a symmetric wireless sensor
communication network. Given an edge-weighted n-vertex graph, ﬁnd a
connected spanning subgraph of minimum cost, where the cost is deter-
mined by letting each vertex pay the most expensive edge incident to it in
the subgraph. We provide an algorithm that works in polynomial time if
one can ﬁnd a set of obligatory edges that yield a spanning subgraph with
O(log n) connected components. We also provide a linear-time algorithm
that reduces any input graph that consists of a tree together with g addi-
tional edges to an equivalent graph with O(g) vertices. Based on this, we
obtain a polynomial-time algorithm for g ∈O(log n). On the negative
side, we show that o(log n)-approximating the diﬀerence d between the
optimal solution cost and a natural lower bound is NP-hard and that
there are presumably no exact algorithms running in 2o(n) time or in
f(d) · nO(1) time for any computable function f.
Keywords: Monitoring areas and backbones
Parameterized complexity · Color coding · Data reduction
Parameterization above lower bounds · Approximation hardness
Spanning trees
1
Introduction
We consider a well-studied graph problem arising in the context of saving power
in maintaining the connectivity of symmetric wireless sensor communication
networks. Our problem, which falls into the category of survivable network
design [22], is formally deﬁned as follows (see Fig. 1 for an example).
Problem 1.1. Min-Power Symmetric Connectivity (MinPSC)
Input: A connected undirected graph G = (V, E) with n vertices, m edges, and
edge weights (costs) w: E →N.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 26–40, 2017.
https://doi.org/10.1007/978-3-319-72751-6_3
www.ebook3000.com

Parameterized Algorithms for Power-Eﬃcient
27
v1
5
v2
6
v3
6
v4
5
v5
1
v6
3
5
6
5
4
3
1
Fig. 1. A graph with positive edge weights and an optimal solution (bold edges). Each
vertex pays the most expensive edge incident to it in the solution (the numbers next
to the vertices). The cost of the solution is the sum of the costs paid by each vertex.
Note that the optimal solution has cost 26 while a minimum spanning tree (using
edge {v2, v5} instead of {v2, v3}) has cost 27 (as a MinPSC solution).
Goal: Find a connected spanning subgraph T = (V, F) of G that minimizes

v∈V
max
{u,v}∈F w({u, v}).
We denote the minimum cost of a solution to an MinPSC instance I = (G, w)
by Opt(I). Throughout this work, weights always refer to edges and costs refer
to vertices or subgraphs. For showing hardness results, we will also consider the
decision version of MinPSC, which we call k-PSC. Herein the problem is to
decide whether an input instance I = (G, w) satisﬁes Opt(I) ≤k.
Figure 1 reveals that computing a minimum-cost spanning tree may not yield
an optimal solution for MinPSC (also see Erzin et al. [12] for further discus-
sion concerning the relationship to minimum-cost spanning trees). In this work,
we provide a reﬁned computational complexity analysis by initiating parameter-
ized complexity studies of MinPSC (and its decision version). In this way, we
complement previous ﬁndings mostly concerning polynomial-time approximabil-
ity [2,9,12], heuristics and integer linear programming [2,13,21], and computa-
tional complexity analysis for special cases [8,9,12,17].
Our Contributions. Our work is driven by asking when small input-speciﬁc
parameter values allow for fast (exact) solutions in practically relevant special
cases. Our two fundamental “use case scenarios” herein are monitoring areas
and infrastructure backbones. Performing a parameterized complexity analysis,
we obtain new encouraging exact algorithms together with new hardness results,
all summarized in Table 1.
In Sect. 2, we provide an (exact) algorithm for MinPSC that works in poly-
nomial time if one can ﬁnd a set of obligatory edges that can be added to any
optimal solution and yield a spanning subgraph with O(log n) connected com-
ponents. In particular, this means that we show ﬁxed-parameter tractability for
MinPSC with respect to the parameter “number c of connected components
in the spanning subgraph consisting of obligatory edges”. Cases with small c
occur, for example, in grid-like sensor arrangements, which arise when monitor-
ing areas [28,29].

28
M. Bentert et al.
Table 1. Overview on our results, using the following terminology: n—number of
vertices, m—number of edges, g—size of a minimum feedback edge set, d—diﬀerence
between optimal solution cost and a lower bound (see Problem 4.1), c—number of
connected components of subgraph consisting of obligatory edges (see Deﬁnition 2.3).
MinPSC-AL is the problem of computing the minimum value of d (see Problem 4.1),
d-PSC-AL is the corresponding decision problem.
Problem
Result
Reference
Section 2
MinPSC
Solvable in O(ln(1/ε) · (36e2/
√
2π)c · n4/√c)
time with error probability at most ε
Theorem 2.5
MinPSC
Solvable in cO(c log c) · nO(1) time
Theorem 2.5
MinPSC
Solvable in O(3n · (n + m)) time
Proposition 2.7
Section 3
MinPSC
Linear-time data reduction algorithm that
guarantees at most 40g −26 vertices and
41g −27 edges
Theorem 3.1
Section 4
MinPSC-AL
NP-hard to approximate within a factor
of o(log n)
Theorem 4.2(i)
d-PSC-AL
W[2]-hard when parameterized by d
Theorem 4.2(ii)
k-PSC
Not solvable in 2o(n) time unless ETH fails
Theorem 4.2(iii)
In Sect. 3, we provide a linear-time algorithm that reduces any input graph
consisting of a tree with g additional edges to an equivalent graph with O(g) ver-
tices and edges (a partial kernel in terms of parameterized complexity, since the
edge weights remain unbounded). Combined with the previous result, this yields
ﬁxed-parameter tractability with respect to the parameter g (also known as the
feedback edge number of a graph), and, in particular, a polynomial-time algo-
rithm for g ∈O(log n). Such tree-like graphs occur when monitoring backbone
infrastructure or pollution levels along waterways.
We provide some negative (that is, intractability) results in Sect. 4: We show
that o(log n)-approximating the diﬀerence d between the minimum solution cost
and a natural lower bound is NP-hard. Moreover, we prove W[2]-hardness with
respect to the parameter d, that is, there is presumably no algorithm running
in f(d) · nO(1) time for any computable function f. Finally, assuming the Expo-
nential Time Hypothesis (ETH), we show that there is no 2o(n)-time algorithm
for MinPSC.
Due to space constraints, all proofs are deferred to a full version1.
2
Parameterizing by the Number of Connected
Components Induced by Obligatory Edges
This section presents an algorithm that solves MinPSC eﬃciently if we can ﬁnd
obligatory edges that can be added to any optimal solution and yield a spanning
subgraph with few connected components. This is the case, for example, when
1 https://arxiv.org/abs/1706.03177.
www.ebook3000.com

Parameterized Algorithms for Power-Eﬃcient
29
sensors are arranged in a grid-like manner, which saves energy when monitoring
areas [28,29]. To ﬁnd obligatory edges, we use a lower bound ℓ(v) on the cost
paid by each vertex v in the goal function of MinPSC (Problem 1.1).
Deﬁnition 2.1 (vertex lower bounds). Vertex lower bounds are given by a
function ℓ: V →N such that, for any solution T = (V, F) of MinPSC and any
vertex v ∈V , it holds that
max
{u,v}∈F w({u, v}) ≥ℓ(v).
Example 2.2. A trivial vertex lower bound ℓ(v) is given by the weight of the
lightest edge incident to v because v has to be connected to some vertex in
any solution. Moreover, since any edge {u, v} incident to a degree-one vertex u
will be part of any solution, one can choose ℓso that ℓ(u) = w({u, v}) and
ℓ(v) ≥w({u, v}).
Clearly, coming up with good vertex lower bounds is a challenge on its own.
Once we have vertex lower bounds, we can compute an obligatory subgraph,
whose edges we can add to any solution without increasing its cost:
Deﬁnition 2.3 (obligatory subgraph). The obligatory subgraph Gℓinduced
by vertex lower bounds ℓ: V →N for a graph G = (V, E) consists of all vertices
of G and all obligatory edges {u, v} such that min{ℓ(u), ℓ(v)} ≥w({u, v}).
The better the vertex lower bounds ℓ, the more obligatory edges they potentially
induce, thus reducing the number c of connected components of Gℓ. Yet already
the simple vertex lower bounds in Example 2.2 may yield obligatory subgraphs
with only a few connected components in some applications:
Example 2.4. Consider the vertex lower bounds ℓfrom Example 2.2. If we
arrange sensors in a grid, which is the most energy-eﬃcient arrangement of
sensors for monitoring areas [28,29], then Gℓhas only one connected compo-
nent. The number of connected components may increase due to sensor defects
that disconnect the grid or due to varying sensor distances within the grid. The
worst case is if the sensors have pairwise distinct distances. Then, Gℓhas only
one edge and n −1 connected components.
The number c of connected components in Gℓcan easily be exploited in
an exact O(n2c)-time algorithm for MinPSC,2 which runs in polynomial time
for constant c, yet is ineﬃcient already for small values of c. We will show,
among other things, a randomized algorithm that runs in polynomial time for
c ∈O(log n):
2 To connect the c components of Gℓ, one has to add c −1 edges. These have at most
2c −2 end points. One can try all n2c−2 possibilities for choosing these end points
and check each resulting graph for connectivity in O(n + m) ⊆O(n2) time.

30
M. Bentert et al.
Theorem 2.5. MinPSC with vertex lower bounds ℓis solvable
(i) in O(ln 1/ε · (36e2/
√
2π)c · 1/√c · n4) time by a randomized algorithm with
error probability at most ε for any given ε ∈(0, 1), and
(ii) in cO(c log c) · nO(1) time by a deterministic algorithm,
where c is the number of connected components of the obligatory subgraph Gℓ.
Remark 2.6. The deterministic algorithm in Theorem 2.5(ii) is primarily of theo-
retical interest, because it classiﬁes MinPSC as ﬁxed-parameter tractable param-
eterized by c. Practically, the randomized algorithm in Theorem 2.5(i) seems
more promising.
The number of connected components of obligatory subgraphs has recently
also been exploited in ﬁxed-parameter algorithms for problems of servicing links
in transportation networks [6,15,25,26], which led to practical results.
The rest of this section outlines the proof of Theorem 2.5. The proof also
yields the following deterministic algorithm for MinPSC, which will be inter-
esting in combination with the data reduction algorithm in Sect. 3. It is much
faster than the trivial algorithm enumerating all of the possibly nn−2 spanning
trees:
Proposition 2.7. MinPSC can be solved in O(3n · (m + n)) time.
Like some known approximation algorithms for MinPSC [2,17], our algorithms
in Theorem 2.5 work by adding edges to Gℓin order to connect its c connected
components. In contrast to these approximation algorithms, our algorithms will
ﬁnd an optimal set of edges to add. To this end, they work on a padded version G•
ℓ
of the input graph G, in which each connected component of Gℓis turned into a
clique. Then, it is suﬃcient to search for connected subgraphs of G•
ℓthat contain
at least one vertex of each connected component of Gℓ. We can always add the
edges in Gℓto such subgraphs in order to obtain a connected spanning subgraph
of G.
Deﬁnition 2.8 (padded graph, components). Let ℓ: V →N be vertex lower
bounds for a graph G = (V, E). We denote the c connected components of the
obligatory subgraph Gℓby G1
ℓ, G2
ℓ, . . . , Gc
ℓ.
The padded graph G•
ℓ= (V, E•
ℓ) with edge weights w•
ℓ: E•
ℓ→N is obtained
from G with edge weights w: E →N by adding zero-weight edges between each
pair of non-adjacent vertices in Gi
ℓfor each i ∈{1, . . . , c}.
To solve a MinPSC instance (G, w) with vertex lower bounds ℓ: V →N, we
have to add c −1 edges to Gℓin order to connect its c connected components.
These edges have at most 2c−2 endpoints. Thus, we need to ﬁnd a minimum-cost
connected subgraph in G•
ℓthat
– contains at most 2c −2 vertices,
– contains at least one vertex of each connected component of Gℓ,
– such that each of its vertices v pays at least the cost ℓ(v) that it would pay
in any optimal solution to the MinPSC instance (G, w).
www.ebook3000.com

Parameterized Algorithms for Power-Eﬃcient
31
We will do this using the color coding technique introduced by Alon et al. [1]:
randomly color the vertices of G•
ℓusing at most 2c −2 colors and then search
for connected subgraphs of G•
ℓthat contain exactly one vertex of each color.
Formally, we will solve the following auxiliary problem on G•
ℓ.
Problem 2.9. Min-Power Colorful Connected Subgraph (MinPCCS)
Input: A connected undirected graph G = (V, E), edge weights w: E →N,
vertex colors col: V →N, a function ℓ: V →N and a color subset C ⊆N.
Goal: Compute a connected subgraph T = (W, F) of G such that col is a bijection
between W and C and such that T minimizes

v∈W
max

ℓ(v), max
{u,v}∈F w({u, v})

.
Note that, in the deﬁnition of MinPCCS, the function ℓ: V →N does not
necessarily give vertex lower bounds, but makes sure that each vertex v ∈V
pays at least ℓ(v) in any feasible solution to MinPCCS.
In contrast to the usual way of applying color coding, we cannot simply
color the vertices of our input graph G completely randomly and then apply an
algorithm for MinPCCS: One component of Gℓcould contain all colors and,
thus, a connected subgraph containing all colors does not necessarily connect
the components of Gℓ. Instead, we employ a trick that was previously applied
mainly heuristically in algorithm engineering in order to increase the success
probability of color coding algorithms [4,7,10]. Since we know that our sought
subgraph contains at least one vertex of each connected component of Gℓ, we
color the connected components of Gℓusing pairwise disjoint color sets. Herein,
we ﬁrst “guess” how many vertices ci of each connected component Gi
ℓof Gℓthe
sought subgraph will contain and use ci colors to color each component Gi
ℓ. We
thus arrive at the following algorithm for MinPSC:
Algorithm 2.10 (for MinPSC).
Input: A MinPSC instance I = (G, w), vertex lower bounds ℓ: V →N for G =
(V, E), an upper bound ε ∈(0, 1) on the error probability.
Output: A solution for I that is optimal with probability at least 1 −ε.
1. c ←number of connected components of the obligatory subgraph Gℓ.
2. for each c1, c2, . . . , cc ∈N+ such that c
i=1 ci ≤2c −2 do
3.
choose pairwise disjoint Ci ⊆{1, . . . , 2c−2} with |Ci| = ci for i ∈{1, . . . , c}.
4.
repeat t := ln ε/ ln(1 −c
i=1 ci!/cci
i ) times
5.
for i ∈{1, . . . , c}, randomly color the vertices of component Gi
ℓof Gℓ
using colors from Ci, let the resulting coloring be col: V →N.
6.
Solve MinPCCS instance (G•
ℓ, w•
ℓ, col, ℓ, C) using dynamic program-
ming.
7. let T = (W, F) be the best MinPCCS solution found in any of the repetitions.
8. return T ′ = (V, (F ∩E) ∪Eℓ).

32
M. Bentert et al.
The main ingredient of Algorithm 2.10 is the following O(2|C| · (n + m)2 + 3|C| ·
(m + n))-time dynamic programming algorithm for MinPCCS used in line 6.
It is inspired by an algorithm for ﬁnding signalling pathways in biological net-
works [24]. However, our case is complicated by the non-standard goal function
in MinPCCS.
Algorithm 2.11 (for MinPCCS). Let I := (G, w, col, ℓ, C) be an instance of
MinPCCS, where G = (V, E). Assume w(e) = ∞for any e /∈E and min ∅= ∞.
For any color set C′ ⊆C and any pair of vertices {v, q} ⊆V , let D(v, q, C′) be the
cost of a feasible solution T = (W, F) to the MinPCCS instance (G, w, col, ℓ, C′)
that minimizes
Φ(v, q, T) := max{ℓ(v), w({v, q})} +

v′∈W \{v}
max

ℓ(v′),
max
{u,v′}∈F w({u, v′})

under the constraints that v ∈W and
max{ℓ(v), w({v, q})} ≥max

ℓ(v), max
{u,v}∈F w({u, v})

(such a solution might not exist for some choices of v and q). Note that the only
diﬀerence between Φ(v, q, T) and the goal function of MinPCCS (Problem 2.9)
is that the vertex v pays exactly max{ℓ(v), w({v, q})}.
The cost min{v,q} ⊆V D[v, q, C] of an optimal solution to I can then be
computed as follows. Obviously, D[v, q, C′]
=
∞if col(v)
/∈
C′. More-
over, D[v, q, {col(v)}] = max{ℓ(v), w({v, q})}. Finally, compute D[v, q, C′] with
|C′| ≥2 for subsets C′ ⊆C of increasing cardinality, storing intermediate results,
via
D[v, q, C′] = min
⎧
⎨
⎩
D[v, q, C1] + D[v, q, C2] −max{ℓ(v), w({v, q})
for all C1 ⊊C′ and C2 ⊊C′
such that C1 ∪C2 = C′ and C1 ∩C2 = {col(v)}
⎫
⎬
⎭
∪
⎧
⎪
⎪
⎨
⎪
⎪
⎩
D[u, q′, C′ \ {col(v)}] + max{ℓ(v), w({v, q})
for all u ∈N(v) and q′ ∈N(u)
such that w({u, q′}) ≥w({u, v})
and w({v, q}) ≥w({u, v})
⎫
⎪
⎪
⎬
⎪
⎪
⎭
.
3
Parameterizing by the Feedback Edge Number
This section studies the complexity of MinPSC parameterized by the feedback
edge number—the minimum number of edges one has to delete in order to turn
a graph into a tree. This parameter can be computed in linear time by com-
puting a spanning tree and counting the remaining edges. The motivation for
studying this parameter is twofold. From a theoretical point of view, the cost of
any spanning tree of the input graph gives an upper bound on the cost of an
optimal solution. The remaining edges are a feedback edge set whose size lim-
its the freedom for improving this upper bound. In practice, graphs with small
www.ebook3000.com

Parameterized Algorithms for Power-Eﬃcient
33
feedback edge number may appear when monitoring backbone infrastructure or
waterways (for example, when deleting canals, the remaining, natural waterways
usually form a forest [14]). Hence, it is natural to ask whether MinPSC is sig-
niﬁcantly easier on graphs with small feedback edge number than on general
graphs. In this section, we answer this question in the aﬃrmative by proving the
following theorem.
Theorem 3.1. In linear time, one can transform any instance I = (G, w) of
MinPSC with feedback edge number g into an instance I′ = (G′, w′) and compute
a value a ∈N such that G′ has at most 40g −26 vertices, 41g −27 edges,
and Opt(I) = Opt(I′) + a.
In terms of parameterized complexity theory, Theorem 3.1 gives a partial
kernel [5] of linear size for k-PSC parameterized by the feedback edge number,
leaving k and the edge weights unbounded.
By applying ﬁrst Theorem 3.1 to shrink a MinPSC instance and then apply-
ing Proposition 2.7 to solve the shrunk instance, we can solve MinPSC in poly-
nomial time on graphs with feedback edge number g ∈O(log n):
Corollary 3.2. MinPSC is solvable in 2O(g) + O(n + m) time.
We point out that, in practice, it seems more promising to solve the shrunk
instance using Theorem 2.5 instead of Proposition 2.7, although it will not give
a provably better bound than Corollary 3.2.
To prove Theorem 3.1, we ﬁrst present data reduction rules for the following
intermediate variant of MinPSC, which allows for annotating each vertex v with
the minimum cost ℓ(v) it has to pay in any optimal solution. We will then show
how to transform an instance of this variant to the original problem.
Problem 3.3. Annotated MinPSC
Input: A connected undirected graph G = (V, E), edge weights w: E →N, and
vertex annotations ℓ: V →N.
Goal: Find a connected spanning subgraph T = (V, F) of G that minimizes

v∈V
max

ℓ(v), max
{u,v}∈F w({u, v})

.
Note that Annotated MinPSC can be seen as special case of MinPCCS
(Problem 2.9) where each vertex is assigned a distinct color. Each instance of
MinPSC (Problem 1.1) can be transformed into an equivalent instance of Anno-
tated MinPSC with ℓ(v) = 0 for each vertex v ∈V .
Our data reduction rules shrink the input graph and, at the same time, com-
pute the value a as speciﬁed in Theorem 3.1. Initially, a = 0. The general app-
roach is common to many results that upper-bound the size of the graph in terms
of its feedback edge number [3,16,20,27]. To this end, it is suﬃcient to reduce
the number of degree-one vertices and the lengths of paths of degree-two ver-
tices. We will see that the second part—shrinking paths—is the challenging one.

34
M. Bentert et al.
v1
v2
v3
v4
v5
· · ·
1
4
5
4
· · ·
Fig. 2. A path where the most beneﬁcial edge is not the heaviest edge. We assume
that v1 and v5 are connected to the rest of the graph in such a way that v1 has to pay
at least 1 and v5 has to pay at least 4. Omitting {v3, v4} (the heaviest edge) results in
the following optimal assignments: v2 : 4, v3 : 4, v4 : 4 ( = 12) and omitting {v2, v3}
results in a better solution: v2 : 1, v3 : 5, v4 : 5 ( = 11).
Like our dynamic programming algorithm for MinPCCS (see Algorithm 2.11),
it is complicated by the nonlinear goal function of Annotated MinPSC: even
if we knew that an optimal solution does not contain all edges of a path, it is not
obvious which edge of the path the optimal solution will skip (we will see that it
is not always the heaviest edge). Our ﬁrst data reduction rule for Annotated
MinPSC removes degree-one vertices.
Reduction Rule 3.4. Let v be a vertex with exactly one neighbor u.
Then,
set
ℓ(u)
:=
max{ℓ(u), w({u, v})},
delete
v,
and
increase
a
by
max{w({u, v}), ℓ(v)}.
Henceforth, we assume that no degree-one vertices are left. Our second data
reduction rule upper-bounds the length of paths of degree-two vertices. Let P =
(v0, v1, . . . , vh) be such a path with h > 8 and all inner vertices having degree
two, that is, deg(vi) = 2 for 1 ≤i ≤h −1. Observe that at most one edge of the
path is not in a solution—a connected spanning subgraph of G. Thus, there are
two cases: either all edges of P or all but one edge of P are in the solution. We
can encode this in a shorter path containing the edge that yields the highest
beneﬁt when omitted in a connected spanning subgraph. Remarkably, this is not
necessarily the heaviest edge, as shown in Fig. 2. Besides such a most beneﬁcial
edge, we also need to keep the ﬁrst and last edge in the path as the beneﬁt of
omitting them depends on the rest of the solution. We formalize this as follows.
Deﬁnition 3.5 (representative path, most beneﬁcial edge).
Let P =
(v0, v1, . . . , vh) with h > 8 be a path such that deg(v0) > 2, deg(vh) > 2, and
deg(vi) = 2 for i ∈{1, 2, . . . , h −1}. Let β : E →N be a function deﬁned by
β({vj, vj+1}) := max

0, w({vi, vi+1}) −max

ℓ(vi), w({vi−1, vi})

+
+ max

0, w({vi, vi+1}) −max

ℓ(vi+1), w({vi+1, vi+2})

.
If β({vj, vj+1}) ≥β({vk, vk+1}), then we say that {vj, vj+1} is more beneﬁcial.
For any most beneﬁcial edge {vi, vi+1} on P, that is, an edge maximizing β,
we deﬁne the representative rep(P) = (v0, v1, u1, vi, vi+1, u2, vh−1, vh) of P as a
path with new vertices u1 and u2,
www.ebook3000.com

Parameterized Algorithms for Power-Eﬃcient
35
ℓ(u1) := max

w({v1, v2}),
w({vi−1, vi}),
ℓ(u2) := max

w({vi+1, vi+2}),
w({vh−2, vh−1}),
and new incident edges of weights
w({v1, u1}) := w({v1, v2}),
w({u2, vh−1}) := w({vh−2, vh−1}),
w({u1, vi}) := w({vi−1, vi}),
and
w({vi+1, u2}) := w({vi+1, vi+2}).
We will replace P by rep(P). At the same time, we have to adjust the value
of a for Theorem 3.1. To this end, observe that each vertex vj with j ∈{2, . . . h−
2} \ {i, i + 1} pays max{ℓ(vj), w({vj−1,vj}), w({vj, vj+1})} before replacement
and is deleted after replacement. Moreover, the new vertices u1 and u2 pay
exactly ℓ(u1) and ℓ(u2), respectively, after replacement and are not part of the
instance before replacement. Thus, we increase a by
adj(rep(P)) := −ℓ(u1) −ℓ(u2) +

2≤j≤h−2
j /∈{i,i+1}
max
⎧
⎪
⎨
⎪
⎩
ℓ(vj)
w({vj−1, vj})
w({vj, vj+1})
and the second data reduction rule works as follows.
Reduction Rule 3.6. Let P = (v0, v1, . . . , vh) be a path with h > 8 such that
deg(v0) > 2, deg(vh) > 2, and deg(vi) = 2 for i ∈{1, 2, . . . , h −1}. Replace P
by rep(P) and increase a by adj(rep(P)).
So far, our data reduction rules turn an instance I of MinPSC into an
instance IA of Annotated MinPSC. Next, we show a result that any instance IA
of Annotated MinPSC can be transformed back into an instance I′ of
MinPSC so that Opt(IA) + 
v∈V ℓ(v) = Opt(I′).
Lemma 3.7. In linear time, one can transform any instance IA of Anno-
tated MinPSC into an instance I′ of MinPSC so that Opt(IA)+
v∈V ℓ(v) =
Opt(I′). The instance I′ contains at most 2n vertices and at most n + m edges.
We now give an upper bound on the size of graphs reduced by Reduction
Rules 3.4 and 3.6. Combined with Lemma 3.7, this yields a proof of Theorem 3.1.
Proposition 3.8. Let I = (G, w, ℓ) be an instance of Annotated MinPSC
and let g be the feedback edge number of G. If Reduction Rules 3.4 and 3.6 cannot
be applied to I, then G contains at most 20g −13 vertices and 21g −14 edges.
Theorem 3.1 can be proven by exhaustively applying Reduction Rules 3.4 and
3.6 and transforming the resulting instance into an instance of MinPSC using
Lemma 3.7.

36
M. Bentert et al.
4
Parameterized Hardness and Inapproximability
In Sect. 2, we algorithmically exploited vertex lower bounds—lower bounds on
the cost that each vertex has to pay in any optimal solution to MinPSC (see
Deﬁnition 2.1). Vertex lower bounds immediately yield lower bounds on the total
cost of optimal solutions. In this section, we show that the latter are much harder
to exploit algorithmically.
For example, if the weights w: E →N of the edges in a graph G =
(V, E) are at least one, then the vertex lower bounds given by ℓ(v) :=
min{u,v}∈E w({u, v}) ≥1 for each vertex v ∈V (see Example 2.2) immedi-
ately yield a “large” lower bound of at least n on the cost of an optimal solution.
This implies that even constant-factor approximation algorithms (e. g. the one by
Althaus et al. [2]) can return solutions that are, in absolute terms, quite far away
from the optimum. Furthermore, it follows that Proposition 2.7 already yields
ﬁxed-parameter tractability for k-PSC parameterized by the solution cost k.
A more desirable and stronger result would be a constant-factor approxima-
tion of the diﬀerence d between the optimal solution cost and a lower bound or a
ﬁxed-parameter tractability result with respect to the parameter d. However, we
show that such algorithms (presumably) do not exist. Herein, we base some of
our hardness results on the Exponential Time Hypothesis (ETH) as introduced
by Impagliazzo and Paturi [18] and on the W-hierarchy from parameterized com-
plexity theory [11]. The ETH is that 3-Sat cannot be solved in 2o(n+m) time,
where n and m are the number of variables and clauses in the input formula,
respectively. Moreover, proving that a parameterized problem is W-hard with
respect to some parameter k shows that there is (presumably) no f(k)·nO(1)-time
algorithm, where n denotes the input size.
To state our hardness results, we use the following problem variant, which
incorporates the lower bound.
Problem 4.1. MinPSC Above Lower Bound (MinPSC-AL)
Input: A connected undirected graph G = (V, E) and edge weights w: E →N.
Goal: Find a connected spanning subgraph T = (V, F) of G that minimizes

v∈V
max
{u,v}∈Fw({u, v}) −min
{u,v}∈Ew({u, v}).
(4.1)
For a MinPSC-AL instance I = (G, w), we denote by Opt(I) the minimum
value of (4.1) (we also refer to Opt(I) as the margin of I). For showing hardness
results, we will also consider the decision version of the problem: By d-PSC-AL,
we denote the problem of deciding whether an MinPSC-AL instance I = (G, w)
satisﬁes Opt(I) ≤d.
Theorem 4.2.
(i) MinPSC-AL is NP-hard to approximate within a factor of o(log n).
(ii) d-PSC-AL is W[2]-hard when parameterized by d.
(iii) Unless the Exponential Time Hypothesis fails, k-PSC and d-PSC-AL are
not solvable in 2o(n) time.
www.ebook3000.com

Parameterized Algorithms for Power-Eﬃcient
37
Theorem 4.2(ii) shows that d-PSC-AL is presumably not solvable in
f(d) · nO(1) time, whereas it is easily solvable in O(2d · nd+2) time, that is, in
polynomial time for any ﬁxed d.3 Moreover, Theorem 4.2(iii) shows that our
algorithm in Proposition 2.7 is asymptotically optimal.
We prove Theorem 4.2 using a reduction from Minimum Set Cover to
MinPSC due to Erzin et al. [12].
Problem 4.3. Minimum Set Cover
Input: A universe U = {u1, . . . , un} and a set family F = {S1, . . . , Sm} contain-
ing sets Si ⊆U.
Goal: Find a minimum cardinality set cover F′ ⊆F (that is, 
S∈F′ S = U).
Again, in the corresponding decision version k-Set Cover, we are given
a k ∈N and want to decide whether there exists a set cover of size at most k.
Minimum Set Cover is NP-hard to approximate within a factor of o(log n) [23].
Furthermore, k-Set Cover is W[2]-complete parameterized by the solution
size k [11] and cannot be solved in 2o(n+m) time unless the ETH fails [19].
We now present a slightly modiﬁed version of the reduction by Erzin et al.
[12] from which Theorem 4.2 follows. Note that Erzin et al. [12] used edge weights
zero and one in their reduction whereas we use positive integers as edge weights,
namely one and two.
Transformation 4.4. Given an instance I = (U, F) of Minimum Set Cover,
construct an instance I′ = (G, w) of MinPSC-AL as follows. The graph G con-
sists of a special vertex s, a vertex vu for each element u ∈U, and a vertex vS
for each set S ∈F. Denote the respective vertex sets by VU and VF, that is,
V := {s} ⊎VU ⊎VF. There is an edge between each pair of vertices vu, vS ∈V
if the set S ∈F contains u ∈U. Moreover, there is an edge {s, vSi} for
each i ∈{1, . . . , m}. The edge weights w are as follows. All edges incident to s
get weight one. The remaining edges (between VU and VF) get weight two.
5
Conclusion
We believe that both our randomized ﬁxed-parameter algorithm exploiting ver-
tex lower bounds and our data reduction rules (a partial kernelization for the
parameter “feedback edge number”) are worth implementing and testing. Empir-
ical work is ongoing work; however, we believe that our algorithms are less suited
for random test data (as typically used in published work so far) because our
algorithms make explicit use of structure in the input which presumably occurs
in some real-world monitoring instances.
3 At most d vertices can pay more than their vertex lower bound. We can try all
possibilities for choosing i ≤d vertices, all
d
i

possibilities to increase their total
cost by at most d, and check whether the graph of the “paid” edges is connected.
The algorithm runs in d
i=1
n
i
d
i

· O(n + m) ⊆O(2d · nd+2) time.

38
M. Bentert et al.
An important theoretical challenge is to ﬁnd good vertex lower bounds for
exploitation in Sect. 2. This goes hand in hand with identifying scenarios where
(more) obligatory edges are given by the application (e.g., this may be the case
in communication networks with designated hub nodes). Finally, we identiﬁed
positive results for two natural network parameters; thus, the search for further
useful parameterizations is a generic but nevertheless promising undertaking.
Ideally, this should be driven by using data from real-world applications and
analyzing their structural properties.
Acknowledgments. RvB was supported by the Russian Science
Foundation,
grant 16-11-10041, while working on Sect. 2. The results in Sects. 3 and 4 were obtained
during a research stay of RvB at TU Berlin, jointly supported by TU Berlin, by the
Russian Foundation for Basic Research under grant 16-31-60007 mol a dk, and by the
Ministry of Science and Education of the Russian Federation under the 5-100 Excel-
lence Programme.
References
1. Alon, N., Yuster, R., Zwick, U.: Color-coding. J. ACM 42(4), 844–856 (1995)
2. Althaus, E., C˘alinescu, G., Mandoiu, I.I., Prasad, S.K., Tchervenski, N.,
Zelikovsky, A.: Power eﬃcient range assignment for symmetric connectivity
in static ad hoc wireless networks. Wirel. Netw. 12(3), 287–299 (2006)
3. Bentert, M., Fluschnik, T., Nichterlein, A., Niedermeier, R.: Parameterized
aspects of triangle enumeration. In: Klasing, R., Zeitoun, M. (eds.) FCT
2017. LNCS, vol. 10472, pp. 96–110. Springer, Heidelberg (2017). https://
doi.org/10.1007/978-3-662-55751-8 9
4. Betzler, N., van Bevern, R., Fellows, M.R., Komusiewicz, C., Niedermeier,
R.: Parameterized algorithmics for ﬁnding connected motifs in biological
networks. IEEE/ACM Trans. Comput. Biol. 8(5), 1296–1308 (2011)
5. Betzler, N., Guo, J., Komusiewicz, C., Niedermeier, R.: Average parameter-
ization and partial kernelization for computing medians. J. Comput. Syst.
Sci. 77(4), 774–789 (2011)
6. van Bevern, R., Komusiewicz, C., Sorge, M.: A parameterized approximation
algorithm for the mixed and windy capacitated arc routing problem: theory
and experiments. Networks (2017, in press)
7. Bruckner, S., H¨uﬀner, F., Karp, R.M., Shamir, R., Sharan, R.: Topology-free
querying of protein interaction networks. J. Comput. Biol. 17(3), 237–252
(2010)
8. Carmi, P., Katz, M.J.: Power assignment in radio networks with two power
levels. Algorithmica 47(2), 183–201 (2007)
9. Clementi, A.E., Penna, P., Silvestri, R.: On the power assignment problem
in radio networks. Mob. Netw. Appl. 9(2), 125–140 (2004)
10. Dost, B., Shlomi, T., Gupta, N., Ruppin, E., Bafna, V., Sharan, R.: Qnet:
a tool for querying protein interaction networks. J. Comput. Biol. 15(7),
913–925 (2008)
www.ebook3000.com

Parameterized Algorithms for Power-Eﬃcient
39
11. Downey, R.G., Fellows, M.R.: Fundamentals of Parameterized Complexity.
Texts in Computer Science. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-1-4471-5559-1
12. Erzin, A.I., Plotnikov, R.V., Shamardin, Y.V.: O nekotorykh polino-
mial’no razreshimykh sluchayakh i priblizh¨ennykh algoritmakh dlya zadachi
postroyeniya optimal’nogo kommunikatsionnogo dereva. Diskretn. Anal.
Issled. Oper. 20(1), 12–27 (2013)
13. Erzin, A.I., Mladenovic, N., Plotnikov, R.V.: Variable neighborhood search
variants for min-power symmetric connectivity problem. Comput. Oper. Res.
78, 557–563 (2017)
14. Giacometti, A.: River networks. In: Complex Networks, Encyclopedia of
Life Support Systems (EOLSS), pp. 155–180. EOLSS Publishers/UNESCO
(2010)
15. Gutin, G., Wahlstr¨om, M., Yeo, A.: Rural postman parameterized by the
number of components of required edges. J. Comput. Syst. Sci. 83(1), 121–
131 (2017)
16. Hartung, S., Komusiewicz, C., Nichterlein, A.: Parameterized algorithmics
and computational experiments for ﬁnding 2-clubs. J. Graph Algorithms
Appl. 19(1), 155–190 (2015)
17. Hoﬀmann, S., Wanke, E.: Minimum power range assignment for sym-
metric connectivity in sensor networks with two power levels (2016).
arXiv:1605.01752
18. Impagliazzo, R., Paturi, R.: On the complexity of k-SAT. J. Comput. Syst.
Sci. 62(2), 367–375 (2001)
19. Impagliazzo, R., Paturi, R., Zane, F.: Which problems have strongly expo-
nential complexity? J. Comput. Syst. Sci. 63(4), 512–530 (2001)
20. Mertzios, G.B., Nichterlein, A., Niedermeier, R.: Linear-time algorithm for
maximum-cardinality matching on cocomparability graphs. In: MFCS 2017.
LIPIcs, vol. 83, pp. 46:1–46:14, Schloss Dagstuhl – Leibniz-Zentrum fuer
Informatik (2017)
21. Montemanni, R., Gambardella, L.: Exact algorithms for the minimum power
symmetric connectivity problem in wireless networks. Comput. Oper. Res.
32(11), 2891–2904 (2005)
22. Panigrahi, D.: Survivable network design problems in wireless networks. In:
Proceedings of 22nd SODA, pp. 1014–1027. SIAM (2011)
23. Raz, R., Safra, S.: A sub-constant error-probability low-degree test, and a
sub-constant error-probability PCP characterization of NP. In: Proceedings
of 29th STOC, pp. 475–484. ACM (1997)
24. Scott, J., Ideker, T., Karp, R.M., Sharan, R.: Eﬃcient algorithms for detect-
ing signaling pathways in protein interaction networks. J. Comput. Biol.
13(2), 133–144 (2006)
25. Sorge, M., van Bevern, R., Niedermeier, R., Weller, M.: From few compo-
nents to an Eulerian graph by adding arcs. In: Kolman, P., Kratochv´ıl, J.
(eds.) WG 2011. LNCS, vol. 6986, pp. 307–318. Springer, Heidelberg (2011).
https://doi.org/10.1007/978-3-642-25870-1 28

40
M. Bentert et al.
26. Sorge, M., van Bevern, R., Niedermeier, R., Weller, M.: A new view on rural
postman based on Eulerian extension and matching. J. Discrete Alg. 16,
12–33 (2012)
27. Uhlmann, J., Weller, M.: Two-layer planarization parameterized by feedback
edge set. Theoret. Comput. Sci. 494, 99–111 (2013)
28. Zalyubovskiy, V.V., Erzin, A.I., Astrakov, S.N., Choo, H.: Energy-eﬃcient
area coverage by sensors with adjustable ranges. Sensors 9(4), 2446–2460
(2009)
29. Zhang, H., Hou, J.C.: Maintaining sensing coverage and connectivity in large
sensor networks. Ad Hoc Sens. Wirel. Netw. 1(1–2), 89–124 (2005)
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
Keren Censor-Hillel(B), Rina Levy, and Hadas Shachnai
Computer Science Department, Technion, 3200003 Haifa, Israel
{ckeren,rinalevy,hadas}@cs.technion.ac.il
Abstract. Finding a maximum cut is a fundamental task in many
computational settings, with a central application in wireless networks.
Surprisingly, Max-Cut has been insuﬃciently studied in the classic dis-
tributed settings, where vertices communicate by synchronously sending
messages to their neighbors according to the underlying graph, known as
the LOCAL or CONGEST models. We amend this by obtaining almost
optimal algorithms for Max-Cut on a wide class of graphs in these mod-
els. In particular, for any ϵ > 0, we develop randomized approximation
algorithms achieving a ratio of (1 −ε) to the optimum for Max-Cut on
bipartite graphs in the CONGEST model, and on general graphs in the
LOCAL model.
We further present eﬃcient deterministic algorithms, including a 1/3-
approximation for Max-Dicut in our models, thus improving the best
known (randomized) ratio of 1/4. Our algorithms make non-trivial use
of the greedy approach of Buchbinder et al. (SIAM Journal Computing
44:1384–1402, 2015) for maximizing an unconstrained (non-monotone)
submodular function, which may be of independent interest.
1
Introduction
Max-Cut is one of the fundamental problems in theoretical computer science.
A cut in an undirected graph is a bipartition of the vertices, whose size is the
number of edges crossing the bipartition. Finding cuts of maximum size in a given
graph is among Karp’s famous 21 NP-complete problems [25]. Since then, Max-
Cut has received considerable attention, in approximation algorithms [17,19,
42,45], parallel computation [44], parameterized complexity [43], and streaming
algorithms (see, e.g., [24]).
Max-Cut has a central application in wireless mesh networks (WMNs). The
capacity of WMNs that operate over a single frequency can be increased sig-
niﬁcantly by enhancing each router with multiple transmit (Tx) or receive (Rx)
(MTR) capability. Thus, a node will not experience collision when two or more
neighbors transmit to it. Yet, interference occurs if a node transmits and receives
simultaneously. This is known as the no mix-tx-rx constraint. The set of links
activated in each time slot, deﬁning the capacity of an MTR WMN, is governed
K. Censor-Hillel—The research is supported in part by the Israel Science Foundation
(grant 1696/14).
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 41–56, 2017.
https://doi.org/10.1007/978-3-319-72751-6_4

42
K. Censor-Hillel et al.
by a link scheduler. As shown in [10], link scheduling is equivalent to ﬁnding
Max-Cut in each time slot. A maximum cut contains the set of non-conﬂicting
links that can be activated at the same time, i.e., they adhere to the no mix-tx-
rx constraint. The induced bipartition of the vertices at each time slot deﬁnes a
set of transmitters and a set of receivers in this slot. Link scheduling algorithms
based on approximating Max-Cut, and other applications in wireless networks,
can be found in [27,48–51].1
Surprisingly, Max-Cut has been insuﬃciently studied in the classic dis-
tributed settings, where vertices communicate by synchronously sending mes-
sages to their neighbors according to the underlying graph, known as the LOCAL
or CONGEST models. Indeed, there are known distributed algorithms for Max-
Cut using MapReduce techniques [5,35,36]. In this setting, the algorithms par-
tition the ground set among m machines and obtain a solution using all the
outputs. However, despite a seemingly similar title, our distributed setting is
completely diﬀerent.
In this paper we address Max-Cut in the classic distributed network models,
where the graph represents a synchronous communication network. At the end
of the computation, each vertex decides locally whether it joins the subset S or
¯S, and outputs 1 or 0, respectively, so as to obtain a cut of largest possible size.
It is well known that choosing a random cut, i.e., assigning each vertex to
S or ¯S with probability 1/2, yields a 1
2-approximation for Max-Cut, and a 1
4-
approximation for Max-Dicut, deﬁned on directed graphs (see, e.g., [37,38]).2
Thus, a local algorithm, where each vertex outputs 0 or 1 with probability 1/2,
yields the above approximation factors with no communication required. On the
other hand, we note that a single vertex can ﬁnd an optimal solution, once it has
learned the underlying graph. However, this requires a number of communication
rounds that depends linearly on global network parameters (depending on the
exact model considered). This deﬁnes a tradeoﬀbetween time complexity and the
approximation ratio obtained by distributed Max-Cut algorithms. The huge gap
between the above results raises the following natural questions: How well can
Max-Cut be approximated in the distributed setting, using a bounded number of
communication rounds? Or, more precisely: How many communication rounds
are required for obtaining an approximation ratio strictly larger than half, or
even a deterministic 1
2-approximation for Max-Cut?
To the best of our knowledge, these questions have been studied in our
distributed network models only for a restricted graph class. Speciﬁcally, the
paper [22] suggests a distributed algorithm for Max-Cut on d-regular triangle-
free graphs, that requires a single communication round and provides a (1/2 +
0.28125/
√
d)-approximation.
The key contribution of this paper is in developing two main techniques
for approximating Max-Cut and Max-Dicut in distributed networks, with any
1 Max-Cut naturally arises also in VLSI [9], statistical physics [4] and machine learning
[47].
2 In Max-Dicut we seek the maximum size edge-set crossing from S to ¯S.
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
43
communication graph. Below we detail the challenges we face, and our methods
for overcoming them.
1.1
The Challenge
In the LOCAL model, where message sizes and the local computation power are
unlimited, every standard graph problem can be solved in O(n) communication
rounds. For Max-Cut it also holds that ﬁnding an optimal solution requires
Ω(n) communication rounds. This lower bound follows from Linial’s seminal
lower bound [31, Theorem 2.2] for ﬁnding a 2-coloring of an even-sized cycle. In
an even cycle, the maximum cut contains all edges. Therefore, ﬁnding a Max-Cut
is equivalent to ﬁnding a 2-coloring of the graph.
An approach that proved successful in many computational settings −in
tackling hard problems −is to relax the optimality requirement and settle for
approximate solutions. Indeed, in the distributed setting, many approximation
algorithms have been devised to overcome the costs of ﬁnding exact solutions
(see, e.g., [1–3,16,21,28–30,32,39], and the survey of Elkin [11]). Our work can
be viewed as part of this general approach. However, we face crucial hurdles
attempting to use the known sequential toolbox for approximating Max-Cut in
the distributed setting.
As mentioned above, a 1
2-approximation for Max-Cut can be obtained easily
with no communication. While this holds in all of the above models, improv-
ing the ratio of 1/2 is much more complicated. In the sequential setting, an
approximation factor strictly larger than 1/2 was obtained in the mid-1990’s
using semideﬁnite programming [17] (see Sect. 1.3). Almost two decades later,
the technique was applied by [44] to obtain a parallel randomized algorithm for
Max-Cut, achieving a ratio of (1−ϵ)0.878 to the optimum, for any ε > 0. Adapt-
ing this algorithm to our distributed setting seems non-trivial, as it relies heavily
on global computation. Trying to apply other techniques, such as local search,
unfortunately leads to linear running time, because of the need to compare values
of global solutions.
Another obstacle that lies ahead is the lack of locality in Max-Cut, due to
strong dependence between the vertices. The existence of an edge in the cut
depends on the assignment of both of its endpoints. This results in a chain of
dependencies and raises the question whether cutting the chain can still guar-
antee a good approximation ratio.
1.2
Our Contribution
We develop two main techniques for approximating Max-Cut, as well as Max-
Dicut. Our ﬁrst technique relies on the crucial observation that the cut value is
additive for edge-disjoint sets of vertices. Exploiting this property, we design
clustering-based algorithms, in which we decompose the graph into small-
diameter clusters, ﬁnd an optimal solution within each cluster, and prove that the
remaining edges still allow the ﬁnal solution to meet the desired approximation
ratio. An essential component in our algorithms is eﬃcient graph decomposition

44
K. Censor-Hillel et al.
to such small-diameter clusters connected by few edges (also known as a padded
partition), inspired by a parallel algorithm of [34] (see also [12,13]).
For general graphs, this gives (1 −ϵ)-approximation algorithms for Max-Cut
and Max-Dicut, requiring O( log n
ϵ ) communication rounds in the LOCAL model.
For the special case of a bipartite graph, we take advantage of the graph struc-
ture to obtain an improved clustering-based algorithm, which does not require
large messages. The algorithm achieves a (1 −ϵ)-approximation for Max-Cut in
O( log n
ϵ ) rounds, in the more restricted CONGEST model.
For our second technique, we observe that the contribution of a speciﬁc ver-
tex to the cut depends only on the vertex itself and its immediate neighbors.
We leverage this fact to make multiple decisions in parallel by independent
sets of vertices. We ﬁnd such sets using distributed coloring algorithms. Our
coloring-based technique, which makes non-trivial use of the greedy approach
of [7] for maximizing an unconstrained submodular function, yields determin-
istic 1
2-approximation and 1
3-approximation algorithms for Max-Cut and Max-
Dicut, respectively, and a randomized 1
2-approximation algorithm for Max-Dicut.
Each of these algorithms requires ˜O(Δ + log∗n) communication rounds in the
CONGEST model, where Δ is the maximal degree of the graph, and ˜O ignores
polylogarithmic factors in Δ.
Finally, we present LOCAL algorithms which combine both of our tech-
niques. Applying the coloring-based technique to low-degree vertices, and the
clustering-based technique to high-degree vertices, allows as to design faster
deterministic algorithms with approximation ratios of 1
2 and 1
3 for Max-Cut and
Max-Dicut, respectively, requiring min{ ˜O(Δ + log∗n), O(√n)} communication
rounds (Table 1).3
Table 1. A summary of our results.
1.3
Background and Related Work
The weighted version of Max-Cut is one of Karp’s NP-complete problems [25].
The unweighted version that we study here is also known to be NP-complete [15].
While there are graph families, such as planar and bipartite graphs, in which a
maximum cut can be found in polynomial time [18,19], in general graphs, even
3 Due to space constraints, some of the results are omitted. A detailed version of this
paper can be found in [8].
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
45
approximating the problem is NP-hard. In the sequential setting, one cannot
obtain an approximation ratio better than 16
17 for Max-Cut, or an approximation
ratio better than 12
13 for Max-Dicut, unless P = NP [20,46].
Choosing a random cut, i.e., assigning each vertex to S or ¯S with probabil-
ity 1/2, yields a 1
2-approximation for Max-Cut, and 1
4-approximation for Max-
Dicut. In the sequential setting there are also deterministic algorithms yielding
the above approximation ratios [40,42]. For 20 years there was no progress in
improving the 1/2 approximation ratio for Max-Cut, until (in 1995) Goemans
and Williamson [17] achieved the currently best known ratio, using semideﬁnite
programming. They present a 0.878-approximation algorithm, which is optimal
assuming the Unique Games Conjecture holds [26]. In the same paper, Goemans
and Williamson also give a 0.796-approximation algorithm for Max-Dicut. This
ratio was improved later by Matuura et al. [33], to 0.863. Using spectral tech-
niques, a 0.53-approximation algorithm for Max-Cut was given by Trevisan [45].
In [23] Kale and Seshadhri present a combinatorial approximation algorithm for
Max-Cut using random walks, which gives a (0.5 + δ)-approximation, where δ
is some positive constant that appears also in the running time of the algo-
rithm. In particular, for ˜O(n1.6), ˜O(n2) and ˜O(n3) times, the algorithm achieves
approximation factors of 0.5051, 0.5155 and 0.5727, respectively.
Max-Cut and Max-Dicut can also be viewed as special cases of submodular
maximization, which has been widely studied. It is known that choosing a solu-
tion set S uniformly at random yields a 1
4-approximation, and a 1
2-approximation
for a general and for symmetric submodular function, respectively [14]. These
results imply the known random approximation ratios for Max-Cut and Max-
Dicut. Buchbinder et al. [7] present determinstic 1
2-approximation algorithms for
both symmetric and asymmetric submodular functions. These algorithms assume
that the submodular function is accessible through a black box returning f(S)
for any given set S (known as the value oracle model).
In recent years, there is an ongoing eﬀort to develop distributed algorithms
for submodular maximization problems, using MapReduce techniques [5,35,36].
Often, the inputs consist of large data sets, for which a sequential algorithm may
be ineﬃcient. The main idea behind these algorithms is to partition the ground
set among m machines, and have each machine solve the problem optimally
independently of others. After all machines have completed their computations,
they share their solutions. A ﬁnal solution is obtained by solving the problem
once again over a union of the partial solutions. The algorithms achieve perfor-
mance guarantees close to the sequential algorithms while decreasing the running
time, where the running time is the number of communication rounds among
the machines. As mentioned above, these algorithms do not apply to our classic
distributed settings.
2
Preliminaries
The Max-Cut problem is deﬁned as follows. Given an undirected graph G =
(V, E), one needs to divide the vertices into two subsets, S ⊂V and ¯S = V \ S,

46
K. Censor-Hillel et al.
such that the size of the cut, i.e., the number of edges between S and the com-
plementary subset ¯S, is as large as possible. In the Max-Dicut problem, the given
graph G = (V, E) is directed, and the cut is deﬁned only as the edges which are
directed from S to ¯S. As in the Max-Cut problem, the goal is to obtain the largest
cut.
Max-Cut and Max-Dicut can be described as the problem of maximizing the
submodular function f(S) = |E(S, ¯S)|, where for Max-Dicut f(S) counts only
the edges directed from S to ¯S. Given a ﬁnite set X, let 2X denote the power set
of X. A function f : 2X →R is submodular if it satisﬁes the following equivalent
conditions:
(i) For any S, T ⊆X: f(S ∪T) + f(S ∩T) ≤f(S) + f(T).
(ii) For any A ⊆B ⊆X and x ∈X\B: f(B∪{x})−f(B) ≤f(A ∪{x}) −f(A).
For Max-Cut and Max-Dicut, the submodular function also satisﬁes: for any
pair of disjoint sets S, T ⊆X such that ES×T = {(u, v)|u ∈S, v ∈T} = ∅,
f(S)+f(T) = f(S ∪T). Note that for Max-Cut, the function is also symmetric,
i.e., f(S) = f( ¯S).
Model: We consider a distributed system, modeled by a graph G = (V, E), in
which the vertices represent the computational entities, and the edges represent
the communication channels between them. We assume that each vertex v has
a unique identiﬁer id(v) of size O(log n), where n = |V |.
The communication between the entities is synchronous, i.e., the time is
divided into rounds. In each round, the vertices send messages simultaneously
to all of their neighbors and make a local computation based on the information
gained so far. This is the classic LOCAL model [41], which focuses on analyzing
how locality aﬀects the distributed computation. Therefore, message sizes and
local computation power are unlimited, and the complexity is measured by the
number of communication rounds needed to obtain a solution. It is also impor-
tant to study what can be done in the more restricted CONGEST model [41],
in which message sizes are O(log n).
We assume that each vertex has preliminary information including the size
of the network n = |V |, its neighbors, and the maximal degree of the graph Δ.4
Each vertex runs a local algorithm to solve the Max-Cut problem. Along the
algorithm, each vertex decides locally whether it joins S or ¯S, and outputs 1 or
0, respectively. We deﬁne the solution of the algorithm as the set of all outputs.
Note that individual vertices do not hold the entire solution, but only their local
information. The solution value is deﬁned as the size of the cut induced by the
solution. We show that this value approximates the size of the maximum cut.
3
Clustering-Based Algorithms
In this section we present clustering-based algorithms for Max-Cut and Max-
Dicut. Our technique relies on the observation that Max-Cut is a collection of
4 This assumption is needed only for the (Δ+1)-coloring algorithm [6] used in Sect. 4;
it can be omitted (see [6]), increasing the running time by a constant factor.
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
47
edges having their endpoints in diﬀerent sets; therefore, it can be viewed as the
union of cuts in the disjoint parts of the graph.
Given a graph G = (V, E), we ﬁrst eliminate a small fraction of edges to
obtain small-diameter connected components. Then, the problem is solved opti-
mally within each connected component. For general graphs, this is done by
gathering the topology of the component at a single vertex. For the special case
of a bipartite graph, we can use the graph structure to propagate less informa-
tion. Since the ﬁnal solution, consisting of the local decisions of all vertices, is at
least as good as the sum of the optimal solutions in the components, and since
the fraction of eliminated edges is small, we prove that the technique yields a
(1 −ϵ)-approximation.
3.1
A Randomized Distributed Graph Decomposition
We start by presenting the randomized distributed graph decomposition algo-
rithm. The algorithm is inspired by a parallel graph decomposition by Miller
et al. [34] that we adapt to the distributed.5 The PRAM algorithm of [34] gener-
ates a strong padded partition of a given graph, namely, a partition into connected
components with strong diameter O( log n
β ), for some β ≤1/2, such that the frac-
tion of edges that cross between diﬀerent clusters of the partition is at most
β. As we show below, the distributed version guarantees the same properties
with high probability and requires only O( log n
β ) communication rounds in the
CONGEST model.
The distributed version of the graph decomposition algorithm proceeds as
follows: Let δv be a random value chosen by vertex v from an exponential dis-
tribution with parameter β. Deﬁne the shifted distance from vertex v to vertex
u as distδ(u, v) = dist(u, v) −δu. Along the algorithm, each vertex v ﬁnds a
vertex u within its k log n
β
-neighborhood that minimizes distδ(u, v), where k is
a constant. We deﬁne this vertex as v’s center. This step implies the diﬀerence
between the parallel and the distributed decomposition. Indeed, in the paral-
lel algorithm, each vertex chooses its center from the entire ground set V . We
show that our modiﬁed process still generates a decomposition with the desired
properties. Furthermore, w.h.p. the distributed algorithm outputs a decomposi-
tion identical to the one created by the parallel algorithm. A pseudocode of the
algorithm is given in Algorithm 1.
We prove that the fraction of edges between diﬀerent components is small. In
order to do so, we bound the probability of an edge to be between components,
i.e., the probability that the endpoints of the edge choose diﬀerent centers. We
consider two cases for an edge e = (u, v). In the ﬁrst case, we assume that both
u and v choose the center that minimizes their shifted distance, distδ, over all
the vertices in the graph. In other words, if the algorithm allowed each vertex
to learn the entire graph, they would choose the same center as they did in our
5 Our algorithm can be viewed as one phase of the distributed algorithm presented by
Elkin et al. in [12] with some necessary changes.

48
K. Censor-Hillel et al.
Algorithm 1. Distributed Decomposition, code for vertex v
1: 0 < β < 1, k > 2.
2: choose δv at random from Exp(β)
3: center = id(v)
4: distδmin = −δv
5: for k log n
β
iterations do
6:
send (distδmin, center)
7:
for every (dist
′
δmin, center
′) received from u ∈N(v) do
8:
if

dist
′
δmin +1 < distδmin

OR

(dist
′
δmin +1 = distδmin) AND (center
′ <
center)

then
9:
center ←center
′
10:
distδmin ←dist
′
δmin + 1
11:
end if
12:
end for
13: end for
14: output center
algorithm. In the second case, we assume that at least one of u and v chooses
diﬀerently if given a larger neighborhood.
Deﬁne the ideal center of a vertex v as argminw∈V distδ(w, v). In the next
lemma, we upper bound the probability that a vertex does not choose its ideal
center.
Lemma 3.1. Let v′ be the ideal center of vertex v, then the probability that
dist(v′, v) > k log n
β
, i.e., vertex v does not join its ideal center, is at most
1
nk .
Proof. Since v′ is the ideal center of vertex v, we have that distδ(v′, v) ≤
distδ(v, v). Therefore, dist(v′, v) −δv′ ≤dist(v, v) −δv = −δv ≤0, which
implies that dist(v′, v) ≤δv′. That is, the distance between each vertex v to
its ideal center v′ is upper bounded by δv′, and hence Pr

dist(v′, v) > k log n
β

≤
Pr

δv′ > k log n
β

. Using the cumulative exponential distribution, we have that
Pr

δv′ > k log n
β

= exp

−k·β log n
β

= exp (−k log n) ≤
1
nk .
⊓⊔
Corollary 3.2. The Distributed Decomposition algorithm generates a decom-
position identical to the decomposition generated by the parallel decomposition
algorithm with probability at least 1 −
1
nk−1
Deﬁne an exterior edge as an edge connecting diﬀerent vertex components, and
let F denote the set of exterior edges. Let Au,v denote the event that both u
and v choose their ideal centers.
Lemma 3.3. The probability that an edge e = (u, v) is an exterior edge, given
that u and v choose their ideal centers, is at most β.
The lemma follows directly from [34], where indeed the algorithm assigns to
each vertex its ideal center. We can now bound the probability of any edge to
be an exterior edge.
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
49
Lemma 3.4. The probability that an edge e = (u, v) is in F is at most β +
2
nk .
We can now prove the performance guarantees of the Distributed Decomposition
algorithm. Recall that the weak diameter of a set S = {u1, u2, ...ul} is deﬁned
as max(ui,uj)∈S dist(ui, uj).
Theorem 3.5. The Distributed Decomposition algorithm requires O( log n
β ) com-
munication rounds in the CONGEST model, and partitions the graph into com-
ponents, such that in expectation there are O(βm) exterior edges. Each of the
components is of weak diameter O( log n
β ), and with high probability also of strong
diameter O( log n
β ).
Proof. Clearly, as each vertex chooses a center from its k log n
β
-neighborhood, the
distance between two vertices that choose the same center, i.e., belong to the
same component, over the graph G, is at most O( log n
β ). Therefore, the weak
diameter of every component is at most O( log n
β ). By Corollary 3.2, with prob-
ability at least 1 −
1
nk−1 , the algorithm outputs a partition identical to the one
output by the parallel algorithm, and therefore with the same properties, which
implies that the strong diameter of every component is at most O( log n
β ) as well.
Using the linearity of expectation and Lemma 3.4, we have that E [|F|] ≤

e∈E

β +
2
nk

= βm + 2m
nk . Since m ≤n2, for any k > 2, E [|F|] ≤O(βm).
Finally, as can be seen from the code, the algorithm requires O( log n
β ) communi-
cation rounds.
⊓⊔
3.2
A Randomized (1 −ϵ)-Approximation Algorithm for Max-Cut
on a Bipartite Graph
Clearly, in a bipartite graph, the maximum cut contains all of the edges. Such a
cut can be found by selecting arbitrarily a root vertex, and then simply putting
all the vertices of odd depth in one set and all the vertices of even depth in
the complementary set. However, this would require a large computational time
in our model, that depends on the diameter of the graph. We overcome this
by using the above decomposition, and ﬁnding an optimal solution within each
connected component. In each component C, we ﬁnd an optimal solution in
O(Dc) communication rounds, where Dc is the diameter of C. First, the vertices
in each component search for the vertex with the lowest id.6 Then, the vertex
with the lowest id joins S or ¯S with equal probability and sends its decision to
its neighbors. When a vertex receives a message from one of its neighbors, it
joins the opposite set, outputs its decision, and sends it to its neighbors. Since
ﬁnding the optimal solution within each component does not require learning
the entire component topology, the algorithm is applicable in the more restricted
6 This can be done by running a BFS in parallel from all vertices. Each vertex propa-
gates the information from the root with lowest id it knows so far, and joins its tree.
Thus, at the end of the process, we have a BFS tree rooted at the vertex with the
lowest id.

50
K. Censor-Hillel et al.
CONGEST model. The algorithm yields a (1 −ϵ)-approximation for the Max-
Cut problem on a bipartite graph in O( log n
ϵ ) communication rounds with high
probability.
Theorem 3.6. Bipartite Max-Cut is a randomized (1 −ϵ)-approximation for
Max-Cut, requiring O( log n
ϵ ) communication rounds in the CONGEST model
w.h.p.
Algorithm 2. Bipartite Max-Cut
1: G=(V,E)
2: apply Distributed Decomposition to G, with β = ϵ, k > 2
3: for each component C obtained by the decomposition do
4:
build a BFS tree from the vertex v with the lowest id
5:
assign v to S or ¯S with equal probability, assign the rest of the vertices to
alternating sides
6: end for
3.3
A Randomized (1 −ϵ)-Approximation Algorithm for General
Graphs
We present below a (1 −ϵ)-approximation algorithm for Max-Cut in general
graphs, using O( log n
ϵ ) communication rounds. As before, the algorithm consists
of a decomposition phase and a solution phase. While the decomposition phase
works in the CONGEST model, the algorithm suits for the LOCAL model,
since for general graphs, the generated components are not necessarily sparse,
and learning the components topology is expensive in the CONGEST model.
Algorithm 3. Decomposition-Based Max-Cut
1: G=(V,E)
2: apply Distributed Decomposition on G, with β = ϵ/2, k > 2
3: for each component C obtained by the decomposition do
4:
gather the component topology at the vertex v ∈C with the lowest id.
5:
let v ﬁnd an optimal solution and determine the value output by the compo-
nent’s vertices.
6: end for
Theorem 3.7. Decomposition-Based
Max-Cut
is
a
randomized
(1 −ϵ)-
approximation for Max-Cut, requiring O( log n
ϵ ) communication rounds in the
LOCAL model.
Proof. Let OPT(G) be the set of edges that belong to some maximum cut in G,
and let ALG(G) be the set of edges in the cut obtained by Decomposition-Based
Max-Cut. Let Su be the component induced by the vertices which choose u as
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
51
their center, and denote by S the set of components that algorithm Distributed
Decomposition constructs. Then E [|ALG(G)|] ≥E
	
Su∈S |OPT(Su)|

≥
|OPT(G)|−βm ≥|OPT(G)|−2β|OPT(G)| = (1−ϵ)|OPT(G)|. The last inequal-
ity follows from the fact that for every graph G it holds that |OPT(G)| ≥m
2 .
The graph decomposition requires O( log n
ϵ ) communication rounds, and out-
puts components with weak diameter at most O( log n
ϵ ). Therefore, ﬁnding the
optimal solution within each component takes O( log n
ϵ ) as well. The time bound
follows.
⊓⊔
By taking β = ϵ/4, one can now obtain a (1 −ϵ)-approximation algorithm
for Max-Dicut. The diﬀerence comes from the fact that for Max-Dicut it holds
that |OPT(G)| ≥m
4 for every graph G. The rest of the analysis is similar to the
analysis for Max-Cut. Hence, we have
Theorem 3.8. Decomposition-Based Max-Dicut is a randomized (1 −ϵ)-
approximation for Max-Dicut, requiring O( log n
ϵ ) communication rounds in the
LOCAL model.
4
Coloring-Based Algorithms
Many of the sequential approximation algorithms for Max-Cut perform n iter-
ations. Each vertex, in its turn, makes a greedy decision so as to maximize
the solution value. We present distributed greedy algorithms that achieve the
approximation ratios of the sequential algorithms much faster. We ﬁrst prove
that the greedy decisions of vertices can be done locally, depending only on their
immediate neighbors. Then we show how to parallelize the decision process, such
that in each iteration an independent set of vertices completes. The independent
sets are generated using (Δ+1)-coloring; then, for (Δ+1) iterations, all vertices
of the relevant color make their parallel independent decisions. All algorithms
run in the CONGEST model (see [8]).
5
A Deterministic LOCAL Algorithm
Our coloring-based algorithms may become ineﬃcient for high degree graphs,
due to the strong dependence on Δ. Consider a clique in this model. The above
algorithms require a linear number of communication rounds, while learning the
entire graph and ﬁnding an optimal solution requires only O(1) communication
rounds in the LOCAL model. Indeed, there is a tradeoﬀbetween the graph
diameter and the average degree of its vertices. Based on this tradeoﬀ, we propose
a faster, two-step, deterministic algorithm for Max-Cut that requires min{ ˜O(Δ+
log∗n), O(√n)} communication rounds in the LOCAL model. The pseudocode
is given in Algorithm 4.
We call a vertex v a low-degree vertex, if deg(v) < √n, and a high-degree
vertex, if deg(v) ≥√n. Deﬁne Glow, and Ghigh as the graphs induced by the

52
K. Censor-Hillel et al.
low-degree vertices and the high-degree vertices, respectively. The idea is to solve
the problem separately for Glow and for Ghigh.
In the ﬁrst step, the algorithm deletes every high-degree vertex, if there are
any, and its adjacent edges, creating Glow. The deletion means that the low-
degree vertices ignore the edges that connect them to high-degree vertices and
do not communicate over them. Then, the algorithm approximates the Max-Cut
on Glow, using one of the coloring-based algorithms described in Sect. 4.
In the second step, the problem is solved optimally within each connected
component in Ghigh. However, the high-degree vertices are allowed to commu-
nicate over edges which are not in Ghigh. As we prove next, the distance in the
original graph G between any two vertices which are connected in Ghigh is upper
bounded by O(√n). Hence, the number of rounds needed for this part of the
algorithm is O(√n).
Algorithm 4. Fast Distributed Greedy Max-Cut
1: run Distributed Greedy Max-Cut on Glow
2: for each connected component in Ghigh do
3:
learn the component topology in G, including all its adjacent edges
4:
let the vertex with the lowest id ﬁnd an optimal solution, and determine the
output for each vertex in its component
5: end for
6: output the vertices decisions
Lemma 5.1. Assume u, v are connected in Ghigh, then the distance between u
and v in the original graph G is at most 3√n.
Theorem 5.2. Fast Distributed Greedy Max-Cut yields a
1
2-approximation to
Max-Cut, using min{ ˜O(Δ + log∗n), O(√n)} communication rounds in the
LOCAL model.
Proof. We ﬁrst prove the approximation ratio. Since Distributed Greedy Max-
Cut is applied on Glow, at least half of the edges of Glow are in the cut. Given
the decisions of vertices in Glow, the algorithm ﬁnds an optimal solution for all
vertices in Ghigh. Note that running Distributed Greedy Max-Cut on the high-
degree vertices of G, would give at least half of the remaining edges. This is due
to the fact that the algorithm makes sequential greedy decisions. Therefore, an
optimal solution for the high-degree vertices guarantees at least half of the edges
in G \ Glow, implying the approximation ratio.
Applying Distributed Greedy Max-Cut on Glow requires ˜O(Δlow + log∗n)
communication rounds, where Δlow = min{Δ, √n}. Using Lemma 5.1 we have
that each high degree vertex can communicate with every high-degree vertex
connected to it in Ghigh, using at most O(√n) communication rounds. Hence,
Steps 2. −4. of the algorithm take O(√n) communication rounds. We note that
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
53
when Δ < √n, the algorithm terminates after the ﬁrst step. Thus, the algorithm
requires min{ ˜O(Δ + log∗n), O(√n)} communication rounds.
⊓⊔
Using the above technique, we obtain a fast, deterministic algorithm for the
Max-Dicut problem, by replacing the call to Distributed Greedy Max-Cut in
Step 1 with a call to Distributed Greedy Max-Dicut. Using the same arguments
as in the analysis for the Max-Cut algorithm, we have:
Theorem 5.3. Fast Distributed Greedy Max-Dicut yields a 1
3-approximation to
Max-Dicut, using min{ ˜O(Δ + log∗n), O(√n)} communication rounds in the
LOCAL model.
Acknowledgements. We thank Roy Schwartz and Shay Kutten for stimulating dis-
cussions and for helpful comments on the paper.
References
1. ˚Astrand, M., Flor´een, P., Polishchuk, V., Rybicki, J., Suomela, J., Uitto, J.: A local
2-approximation algorithm for the vertex cover problem. In: Keidar, I. (ed.) DISC
2009. LNCS, vol. 5805, pp. 191–205. Springer, Heidelberg (2009). https://doi.org/
10.1007/978-3-642-04355-0 21
2. ˚Astrand, M., Suomela, J.: Fast distributed approximation algorithms for vertex
cover and set cover in anonymous networks. In: Proceedings of the Twenty-Second
Annual ACM Symposium on Parallelism in Algorithms and Architectures, pp.
294–302. ACM (2010)
3. Bar-Yehuda, R., Censor-Hillel, K., Schwartzman, G.: A distributed (2+ϵ)-
approximation for vertex cover in O(logΔ/ϵ log log Δ) rounds. In: Proceedings
of the 2016 ACM Symposium on Principles of Distributed Computing, PODC
2016, Chicago, IL, USA, 25–28 July 2016, pp. 3–8 (2016)
4. Barahona, F., Gr¨otschel, M., J¨unger, M., Reinelt, G.: An application of combi-
natorial optimization to statistical physics and circuit layout design. Oper. Res.
36(3), 493–513 (1988)
5. da Ponte Barbosa, R., Ene, A., Nguyen, H.L., Ward, J.: A new framework for
distributed submodular maximization. arXiv preprint http://arxiv.org/abs/1507.
03719 (2015)
6. Barenboim, L.: Deterministic (δ+ 1)-coloring in sublinear (in δ) time in static,
dynamic and faulty networks. In: Proceedings of the 2015 ACM Symposium on
Principles of Distributed Computing, pp. 345–354. ACM (2015)
7. Buchbinder, N., Feldman, M., Naor, J., Schwartz, R.: A tight linear time (1/2)-
approximation for unconstrained submodular maximization. SIAM J. Comput.
44(5), 1384–1402 (2015)
8. Censor-Hillel, K., Levy, R., Shachnai, H.: Fast distributed approximation for max-
cut. arXiv preprint http://arxiv.org/abs/1707.08496 (2017)
9. Chang, K., Du, D.C.: Eﬃcient algorithms for layer assignment problem. IEEE
Trans. Comput.-Aided Des. Integr. Circuits Syst. 6(1), 67–78 (1987)
10. Chin, K.W., Soh, S., Meng, C.: Novel scheduling algorithms for concurrent trans-
mit/receive wireless mesh networks. Comput. Netw. 56(4), 1200–1214 (2012)

54
K. Censor-Hillel et al.
11. Elkin, M.: Distributed approximation: a survey. ACM SIGACT News 35(4), 40–57
(2004)
12. Elkin, M., Neiman, O.: Distributed strong diameter network decomposition. In:
Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing,
pp. 211–216. ACM (2016)
13. Elkin, M., Neiman, O.: Eﬃcient algorithms for constructing very sparse spanners
and emulators. In: Proceedings of the Twenty-Eighth Annual ACM-SIAM Sym-
posium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira,
16–19 January, pp. 652–669 (2017)
14. Feige, U., Mirrokni, V.S., Vondr´ak, J.: Maximizing non-monotone submodular
functions. SIAM J. Comput. 40(4), 1133–1153 (2011)
15. Garey, M.R., Johnson, D.S., Stockmeyer, L.: Some simpliﬁed NP-complete graph
problems. Theoret. Comput. Sci. 1(3), 237–267 (1976)
16. Ghaﬀari, M., Kuhn, F.: Distributed minimum cut approximation. In: Afek, Y. (ed.)
DISC 2013. LNCS, vol. 8205, pp. 1–15. Springer, Heidelberg (2013). https://doi.
org/10.1007/978-3-642-41527-2 1
17. Goemans, M.X., Williamson, D.P.: Improved approximation algorithms for max-
imum cut and satisﬁability problems using semideﬁnite programming. J. ACM
42(6), 1115–1145 (1995)
18. Gr¨otschel, M., Pulleyblank, W.R.: Weakly bipartite graphs and the max-cut prob-
lem. Oper. Res. Lett. 1(1), 23–27 (1981)
19. Hadlock, F.: Finding a maximum cut of a planar graph in polynomial time. SIAM
J. Comput. 4(3), 221–225 (1975)
20. H˚astad, J.: Some optimal inapproximability results. J. ACM (JACM) 48(4), 798–
859 (2001)
21. Henzinger, M., Krinninger, S., Nanongkai, D.: A deterministic almost-tight dis-
tributed algorithm for approximating single-source shortest paths. In: Proceedings
of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pp. 489–
498. ACM (2016)
22. Hirvonen, J., Rybicki, J., Schmid, S., Suomela, J.: Large cuts with local algorithms
on triangle-free graphs. arXiv preprint arXiv:1402.2543 (2014)
23. Kale, S., Seshadhri, C.: Combinatorial approximation algorithms for maxcut using
random walks. arXiv preprint arXiv:1008.3938 (2010)
24. Kapralov, M., Khanna, S., Sudan, M.: Streaming lower bounds for approximating
max-cut. In: Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on
Discrete Algorithms, pp. 1263–1282. SIAM (2015)
25. Karp, R.M.: Reducibility among combinatorial problems. In: Miller, R.E.,
Thatcher, J.W., Bohlinger, J.D. (eds.) Complexity of Computer Computations, pp.
85–103. Springer, Heidelberg (1972). https://doi.org/10.1007/978-1-4684-2001-2 9
26. Khot, S., Kindler, G., Mossel, E., O’Donnell, R.: Optimal inapproximability results
for max-cut and other 2-variable CSPs? SIAM J. Comput. 37(1), 319–357 (2007)
27. Komurlu, C., Bilgic, M.: Active inference and dynamic Gaussian Bayesian networks
for battery optimization in wireless sensor networks. In: AI for Smart Grids and
Smart Buildings, Papers from the 2016 AAAI Workshop, Phoenix, Arizona, USA
(2016)
28. Kuhn, F., Moscibroda, T.: Distributed approximation of capacitated dominating
sets. Theory Comput. Syst. 47(4), 811–836 (2010)
29. Kuhn, F., Moscibroda, T., Wattenhofer, R.: Local computation: lower and upper
bounds. J. ACM (JACM) 63(2), 17 (2016)
www.ebook3000.com

Fast Distributed Approximation for Max-Cut
55
30. Lenzen, C., Pignolet, Y.A., Wattenhofer, R.: Distributed minimum dominating set
approximations in restricted families of graphs. Distrib. Comput. 26(2), 119–137
(2013)
31. Linial, N.: Locality in distributed graph algorithms. SIAM J. Comput. 21(1), 193–
201 (1992)
32. Lotker, Z., Patt-Shamir, B., Pettie, S.: Improved distributed approximate match-
ing. In: Proceedings of the Twentieth Annual Symposium on Parallelism in Algo-
rithms and Architectures, pp. 129–136. ACM (2008)
33. Matuura, S., Matsui, T.: 0.863-approximation algorithm for MAX DICUT. In:
Goemans, M., Jansen, K., Rolim, J.D.P., Trevisan, L. (eds.) APPROX/RANDOM
-2001. LNCS, vol. 2129, pp. 138–146. Springer, Heidelberg (2001). https://doi.org/
10.1007/3-540-44666-4 17
34. Miller, G.L., Peng, R., Xu, S.C.: Parallel graph decompositions using random shifts.
In: Proceedings of the Twenty-Fifth Annual ACM Symposium on Parallelism in
Algorithms and Architectures, pp. 196–203. ACM (2013)
35. Mirrokni, V., Zadimoghaddam, M.: Randomized composable core-sets for dis-
tributed submodular maximization. In: Proceedings of the Forty-Seventh Annual
ACM on Symposium on Theory of Computing, pp. 153–162. ACM (2015)
36. Mirzasoleiman, B., Karbasi, A., Sarkar, R., Krause, A.: Distributed submodular
maximization: identifying representative elements in massive data. In: Advances
in Neural Information Processing Systems, pp. 2049–2057 (2013)
37. Mitzenmacher, M., Upfal, E.: Probability and Computing: Randomized Algorithms
and Probabilistic Analysis. Cambridge University Press, Cambridge (2005)
38. Motwani, R., Raghavan, P.: Randomized Algorithms. Chapman & Hall/CRC,
London (2010)
39. Nanongkai, D.: Distributed approximation algorithms for weighted shortest paths.
In: Proceedings of the 46th Annual ACM Symposium on Theory of Computing,
pp. 565–573. ACM (2014)
40. Papadimitriou, C., Yannakakis, M.: Optimization, approximation, and complexity
classes. In: Proceedings of the Twentieth Annual ACM Symposium on Theory of
Computing, pp. 229–234. ACM (1988)
41. Peleg, D.: Distributed Computing. SIAM Monographs on Discrete Mathematics
and Applications, vol. 5 (2000)
42. Sahni, S., Gonzalez, T.: P-complete approximation problems. J. ACM (JACM)
23(3), 555–565 (1976)
43. Saurabh, S., Zehavi, M.: (k, n −k)-Max-Cut: an O∗(2p)-time algorithm and a
polynomial kernel. In: Kranakis, E., Navarro, G., Ch´avez, E. (eds.) LATIN 2016.
LNCS, vol. 9644, pp. 686–699. Springer, Heidelberg (2016). https://doi.org/10.
1007/978-3-662-49529-2 51
44. Tangwongsan, K.: Eﬃcient parallel approximation algorithms. Ph.D. thesis, School
of Computer Science, Carnegie Mellon University (2011)
45. Trevisan, L.: Max cut and the smallest eigenvalue. SIAM J. Comput. 41(6), 1769–
1786 (2012)
46. Trevisan, L., Sorkin, G.B., Sudan, M., Williamson, D.P.: Gadgets, approximation,
and linear programming. SIAM J. Comput. 29(6), 2074–2097 (2000)
47. Wang, J., Jebara, T., Chang, S.F.: Semi-supervised learning using greedy max-cut.
J. Mach. Learn. Res. 14(Mar), 771–800 (2013)
48. Wang, L., Chin, K., Soh, S.: Joint routing and scheduling in multi-Tx/Rx wireless
mesh networks with random demands. Comput. Netw. 98, 44–56 (2016)

56
K. Censor-Hillel et al.
49. Wang, W., Liu, B., Yang, M., Luo, J., Shen, X.: Max-cut based overlapping channel
assignment for 802.11 multi-radio wireless mesh networks. In: 2013 IEEE 17th
International Conference on Computer Supported Cooperative Work in Design
(CSCWD), pp. 662–667 (2013)
50. Xu, Y., Chin, K., Raad, R., Soh, S.: A novel distributed max-weight link sched-
uler for multi-transmit/receive wireless mesh networks. IEEE Trans. Veh. Technol.
65(11), 9345–9357 (2016)
51. Xue, G., He, Q., Zhu, H., He, T., Liu, Y.: Sociality-aware access point selection in
enterprise wireless LANs. IEEE Trans. Parallel Distrib. Syst. 24(10), 2069–2078
(2013)
www.ebook3000.com

Barrier Coverage with Uniform Radii in 2D
Andrew Cherry1, Joachim Gudmundsson1(B), and Juli´an Mestre1,2
1 School of Information Technologies, The University of Sydney, Sydney, Australia
joachim.gudmundsson@sydney.edu.au
2 Facebook Inc., Menlo Park, USA
Abstract. Given a set B of disjoint line segments in the plane, the so-
called barriers, and a set of n sensors with uniform range in the plane,
the barrier coverage problem is to move the sensors so that they cover
the segments in B, while minimizing the total movement of the sensors.
In the 1D case when B contains a single barrier and all the sensors lie on
B then the problem can be solved in O(n log n) time. In 2D very little is
known about the complexity of the problem.
We consider the 2D setting and give a
√
2-approximation algorithm
when B contains a single barrier, or a set of parallel barriers. We also give
an approximation algorithm for arbitrarily oriented disjoint barriers.
1
Introduction
The problem of covering a set of barriers using sensors was originally motivated
by intrusion detection and has been studied in numerous research papers [1,3,
5,7,9] and in two surveys [11,12]. A sensor can detect an intruder in a circular
region of ﬁxed range with center at the sensor, and the goal is to guard a region in
the plane. Two cases have been considered in the literature. The area coverage
case where the sensors have to monitor an entire region [6,8] and the barrier
coverage case where only the perimeter of the region is monitored [1,3–5,7,9].
In this paper we will focus our attention on the barrier coverage problem.
As input we are given a set B = {b1, . . . , bk} of (closed) disjoint line segments,
the so-called barriers, together with a set S = {1, . . . , n} of sensors and a location
assignment p : S →R2, where pi = (xi, yi) for each i ∈S. Each sensor i ∈S is
modelled as a unit disk centered at pi. Following the model deﬁned by Dobrev et
al. [5], we say that a sensor located at q covers a barrier bj ∈B if and only if
q ∈bj. An assignment ˆp : S →R2 is called a covering assignment if all barriers
in B are completely covered; that is, for all bj ∈B, we have bj ⊆∪i:ˆpi∈bjD(ˆpi),
where D(q) is the unit disk centered at q. Figure 1(a) depicts an example of a
covering assignment.
The barrier coverage problem is to ﬁnd a covering assignment ˆp of minimum
cost, where the cost is equal to the sum of sensor movements, namely,
cost(ˆp) =
n

i=1
dist2(pi, ˆpi),
where dist2 is the usual Euclidean distance.
This
work
was
supported
by
ARCs
Discovery
Projects
funding
scheme
(DP150101134).
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 57–69, 2017.
https://doi.org/10.1007/978-3-319-72751-6_5

58
A. Cherry et al.
1.1
Background
For the barrier coverage problem three diﬀerent optimisation criteria have been
considered in the literature: minimise the sum of movements (min-sum), mini-
mize the maximum movement (min-max) and, minimize the number of sensors
that move (min-num). Most of the existing research has focused on the one-
dimensional setting where B contains a single barrier and all the sensors lie on
the line containing the barrier.
Min-Sum model in 1D. In this case the problem can be solved optimally in
O(n log n) time when the sensors all have the same radius. If the sensors have
diﬀerent radius the problem is known to be NP-hard to approximate within a
constant factor [4].
Gaspers et al. [9] strengthen this result and proved that no polynomial
time ρ1−ε-approximation algorithm exists unless P = NP, where ρ is the ratio
between the largest radius and the smallest radius. Even if the number of inter-
vals required to move in an optimal solution is small the problem turns out
to be W[1]-hard. On the positive side they showed that a ((2 + ε)ρ + O(1/ε))-
approximation can be computed in O(n3/ε2) time.
The Min-Max and Min-Num models in 1D. Czyzowicz et al. [4] considered
the min-max version of the problem, where the aim is to minimize the maximum
movement. If the sensors have unit radius they gave an O(n2) time algorithm.
Chen et al. [3] improved the bound to O(n log n). In the same paper they pre-
sented an O(n2 log n) time algorithm for the case when the sensors have diﬀerent
radius. For the min-num version Mehrandish et al. [8] showed that the problem
can be solved in polynomial time using dynamic programming if the sensor radii
are uniform, otherwise the problem is NP-hard.
The Min-Max and Min-Sum models in 2D. To the best of our knowledge
there are only a few papers that consider the case when the sensors are originally
placed anywhere in the plane.
Dobrev et al. [5] studied the min-sum and min-max versions with sensors
of arbitrary ranges, initially located at arbitrary locations in the plane. They
showed that the problem is NP-complete even for two barriers and that the min-
max problem is NP-complete even for a single barrier (follows from results in [4]).
In the same paper Dobrev et al. also gave algorithms for the restricted setting
when the movement of the sensors have to be perpendicular to the barriers,
that is, a sensor has to move to the closest point on a line containing a barrier.
Also, the ﬁnal positions of sensors have to be on the line containing the bar-
rier. Using this model in the case of k parallel barriers they gave an O(knk+1)
time algorithm. In the case when the input contains one vertical barrier and
one horizontal barrier the problem becomes NP-hard even when restricted to
perpendicular movement. They also showed an O(n1.5) time algorithm for per-
pendicular movement when the problem is further restricted to the case when
all sensors are located at integer positions and the sensors have unit radius.
www.ebook3000.com

Barrier Coverage with Uniform Radii in 2D
59
Circular barriers in the plane were studied in [2,11]. Bhattacharya et al. [2]
considered moving n sensors to the boundary of the circular barrier and gave an
O(n3.5 log n) time for the min-max version. This bound was later improved to
O(n2.5 log n) by Tan and Wu [10]. They also gave an O(n4) time algorithm that
solves the min-sum version when the initial positions of the sensors are on the
boundary of the circular barrier.
1.2
Our Results
In this paper we present approximation algorithms for the barrier coverage prob-
lem on the plane.
Theorem 1. There is an O(n4) time
√
2-approximation for the setting where
we have a single barrier on the plane.
Theorem 2. There is an O(kn2k+2) time
√
2-approximation for the setting
where we have k parallel barriers on the plane.
Theorem 3. There is an O(kn2k+2) time 2
√
2(1 + 2 tan α)-approximation for
the setting where we have k disjoint barriers on the plane, where α is the largest
angle between a barrier and the x-axis.
2
A Single Barrier
This section deals with the case where B consists of a single line segment b. For
ease of presentation we assume without loss of generality that b is the horizontal
segment between (0, 0) and some point (L, 0) (this can always be achieved by
rotating and translating the input point set p). Furthermore, we assume that the
sensors S = {1, . . . , n} are ordered from left to right (ties are broken arbitrarily).
pi
pi+1
b
ˆpi+1
ˆpi
(a)
(b)
Fig. 1. (a) An instance of the covering assignment problem with seven sensors. The
grey disks represent the new locations of sensors that move. The original location (pi)
and the new location (ˆpi) are connected with a directed arrow. (b) An instance where
the dotted lines represent an order preserving assignment, while the solid lines represent
an optimal assignment.

60
A. Cherry et al.
We say that a covering assignment ˆp is order-preserving if for every ˆpi = (ˆxi, ˆyi) ∈
B we have ˆxi < ˆxj if and only if i < j.
If we knew that there is always an optimal solution that is order-preserving,
perhaps we could exploit this fact to design a dynamic programming formulation
for the problem. Unfortunately there are instances where an optimal covering
assignment is not order-preserving. A simple example is shown in Fig. 1(b). The
interesting question to ponder then is, how bad can an order-preserving covering
assignment be in the worst case? The main result of this section is to show that
there exists an order-preserving covering that is at most a factor
√
2 worse than
an optimal solution and that we can compute such a solution eﬃciently.
To prove this we need to study the Manhattan distance cost of a covering
assignment ˆp, namely,
cost1(ˆp) =

i∈B
dist1(ˆpi, pi).
The main reason to consider the dist1-cost is that unlike the dist2 case, there is
always an order-preserving covering assignment that is optimal under dist1.
Our algorithm and its correctness rests on the following key lemmas.
Lemma 1. In the single-barrier case any cost1-optimal order-preserving cover-
ing is
√
2-approximate under the dist2-cost.
Lemma 2. For the single-barrier case there exists a set D of O(n2) discrete
points on the barrier such that there exists a cost1-optimal order-preserving cov-
ering assignment where sensors are only allowed to be placed in locations in D.
Lemma 3. For the single-barrier case there is an O(n4) time algorithm for
ﬁnding a cost1-optimal order-preserving covering assignment.
We can combine these lemmas in a straightforward way to get a proof of Theo-
rem 1, namely, an O(n4) time
√
2-approximation for the barrier coverage problem
with one barrier. The rest of this section is devoted to proving these lemmas.
2.1
Order-Preserving Covering Assignments
We start with a simple observation on the structure of the cost1-optimal solution.
Observation 1. There is a cost1-optimal covering assignment that is order-
preserving.
Proof. Assume we have a cost1-optimal covering assignment ˆp that is not order-
preserving. Then there exists two sensors i, j ∈S such that xi ≤xj and ˆxi > ˆxj.
The contribution of i and j to cost1(ˆp) is |yi| + |ˆxi −xi| + |yj| + |ˆxj −xj|.
Now consider a new covering assignment ˆp′ where sensors i and j are reversed,
that is, ˆp′
i = ˆpj and ˆp′
j = ˆpi. Note that the order of i and j is preserved under ˆp′.
The contribution of i and j to cost1(ˆp′) is |yi| + |ˆxj −xi| + |yj| + |ˆxi −xj|, which
is equal to their contribution under ˆp when both ˆxi and ˆxj lie to the right or to
www.ebook3000.com

Barrier Coverage with Uniform Radii in 2D
61
(a)
(b)
ˆpi
ˆpj
b
pi
pj
ˆpi
ˆpj
b
pi
pj
Fig. 2. Sensors in an dist1-optimal covering.
the left of both xi and xj and less than otherwise. Figure 2 shows and example
of both cases.
Therefore, cost1(ˆp′) ≤cost1(ˆp). Repeating this process until complete order
has been restored yields an order preserving covering that is optimal with respect
to cost1.
□
Using the above observation we can now give a proof of Lemma 1, namely
that a cost1-optimal order-preserving solution is
√
2-approximate under the dist2
metric.
Proof (Proof of Lemma 1). Let opt1 and opt2 be optimal covering assignments
minimizing the dist1- and dist2-cost, respectively. To diﬀerentiate the two we
talk of cost1(·) and cost2(·). These two objectives are related as follows:
cost2(opt1) ≤cost1(opt1) ≤cost1(opt2) ≤
√
2 · cost2(opt2),
where the ﬁrst inequality follows from the fact that dist1 dominates dist2, the
second from the optimality of opt1 under cost1, and the third from the well-
known relation between dist2 and dist1 in R2.
□
In the next two sections we will show how to compute a dist1-optimal order-
preserving covering assignment in time O(n4). First we need to impose more
structure on such solutions.
2.2
Discrete Sensor Locations
Let ˆp be an order preserving covering assignment. Consider two consecutive
sensors i, j ∈S are said to be neighbors in a covering ˆp if both cover the barrier
(i.e., ˆpi, ˆpj ∈b) and there is no other sensor covering the barrier between i and j.
Furthermore, two neighbouring sensors are said to overlap if the distance between
them is less than 2, otherwise they are non-overlapping. Notice that other than
the leftmost and rightmost sensors, every sensor has exactly one neighbor to the
left and one neighbor to the right.
Consider a sensor i covering the barrier in the solution ˆp. We say that the
sensor’s position ˆpi is an anchor if either xi = ˆxi or pi is the leftmost or rightmost

62
A. Cherry et al.
point in b. Furthermore, we say a sensor i moves left if xi > ˆxi and move right
if xi < ˆxi.
Observation 2. Let ˆp be a dist1-optimal order-preserving covering assignment.
For any sensor i covering the barrier in ˆp, then
1. if i is not placed at (0, 0) and is right-moving, it will not overlap its right
neighbour, and
2. if i is not placed at (L, 0) and is left-moving, it will not overlap its left neigh-
bour.
Proof. If ˆpi ̸= (0, 0) and pi lies to the right of pi then the cost of moving sensor
i is |yi| + (ˆxi −xi). If it is overlapping with its right neighbour j in ˆp then one
could move i to the left by a distance min(ˆxi −xi, 2 −(ˆxj −ˆxi)). This would not
aﬀect the coverage of the barrier but would reduce the cost of the movement of
sensor i, contradicting the optimality of ˆp. Hence, a right-moving sensor, not at
barrier position (0, 0), does not overlap its right neighbour in ˆp.
The case when a sensor moves left and ˆpi ̸= (L, 0) is symmetric.
□
This observation can be generalized to a chain of non-overlapping sensors.
Observation 3. Let ˆp be a dist1-optimal order-preserving covering assignment.
Then for any sequence of consecutive non-overlapping sensors that cover the
barrier in ˆp,
1. if the sequence does not include a sensor at (0, 0) and has more right-moving
than left-moving sensors, it will not overlap with the neighboring sensor to its
right.
2. if the sequence does not include a sensor at (L, 0) and has more left-moving
than right-moving sensors, it will not overlap with the neighboring sensor to
its left.
Proof. Given a sequence of consecutive non-overlapping sensors S′ that cover
the barrier, let S′
L ⊂S′ be the sensors in S′ that moved left and let S′
R ⊂S′
be the sensors in S′ that moved right. That is, S′
L = {i ∈S′ : xi > ˆxi} and
S′
R = {i ∈S′ : xi < ˆxi}.
If |S′
L| < |S′
R| and there is an overlap between the right-most sensor in the
sequence and its neighbour to the right then moving the entire sequence to
the left a small distance such that the barrier is still covered will decrease the
total cost which contradicts the assumption that ˆP is a minimum cost covering
assignment. Hence, if |S′
L| < |S′
R| the sequence cannot overlap with its right
neighbour.
The symmetric argument can be used for the case when |S′
L| > |S′
R|.
□
Note that the sensors in a sequence of neighboring non-overlapping sensors
in ˆp are separated by discrete distances. That is, if the leftmost sensor of the
sequence is positioned at ˆxi then the following sensors’ locations in the sequence
are given by (ˆxi + 2k, 0), for k = 1, 2, . . .. Below we will show key properties of
an optimal order-preserving covering assignment ˆp under dist1.
www.ebook3000.com

Barrier Coverage with Uniform Radii in 2D
63
non-overlapping sensor sequence
anchor points
Fig. 3. A sequence of non-overlapping sensors in ˆP together with three anchor points.
Observation 4. There is a dist1-optimal order-preserving covering assignment
in which every interval between two consecutive anchors along b is covered by
either
Type 1: a sequence of non-overlapping sensors that includes at least one of the
anchors, or
Type 2: two sequences of non-overlapping sensors with the left sequence includ-
ing the left anchor and the right sequence including the right anchor.
Proof. Let ˆp be a dist1-optimal order-preserving covering assignment with the
maximum number of anchor points. Let ˆpi and ˆpj be two consecutive anchors
along b. If the number of sensors in ˆp between ˆpi and ˆpj is zero then the sequence
is trivially of type 1. If the number of sensors is at least one then we will have
the ﬁve cases listed below. As above let S′
L be the sensors in the sequence that
moved left and let S′
R be the sensors in the sequence that moved right.
Case 1: If the sensors are all right-moving then, according to Observation 2,
the sequence will not overlap internally or with ˆpj, hence it will be of Type 1.
Case 2: If the sensors are all left-moving then, according to Observation 2, the
sequence will not overlap internally or with ˆpi, hence it will be of Type 1.
Case 3: If the sequence contains no internal overlapping sensors and |S′
R| ̸=
|S′
L| then, from Observation 3, we know that the sequence cannot overlap ˆpj
(|S′
R| > |S′
L|) or the sequence cannot overlap ˆpi (|S′
R| < |S′
L|). Both situations
result Type 1 sequences.
Case 4: If the sequence contains no internally overlapping sensors and |S′
R| =
|S′
L| then the sequence can be moved either left or right until one of the
sensors in the sequence becomes an anchor which contradicts the assumption
that ˆp is an optimal solution with a maximum number of anchor points, or
until the sequence does not overlap with either ˆpi or ˆpj. This is a sequence of
Type 1.
Case 5: If the sequence contains overlapping sensors then let ˆpi = pi1, . . . , pik =
pj be the sensor positions in the sequence ordered from left to right along b.

64
A. Cherry et al.
First consider the pairs of overlapping sensors in the sequence, and let ˆpib and
ˆpib+1 be the positions of the leftmost sensors that overlap in the sequence, see
Fig. 4. If there is a second such overlap, then let ˆpih and ˆpih+1 be the leftmost
sensors that overlap in the sequence to the right of ˆpib+1. Note that one can
move the subsequence ˆpib+1, . . . , ˆpih either to the left or to the right without
increasing the cost of the solution until (a) one sensor position becomes an
anchor, or until (b) either ˆpib and ˆpib+1 or ˆpih and ˆpih+1 are non-overlapping.
Since outcome (a) contradicts the assumption that the covering assignment
has a maximum number of anchors only outcome (b) is possible. As a result
the resulting sequence has one fewer overlap than the starting conﬁguration.
This argument can be used iteratively, with the result that one either gets case
3 or 4 (type 1), or a type 2 scenario where two sequences of non-overlapping
sensors overlap in the intersection with the left sequence including the left
anchor and the right sequence including the right anchor.
□
ˆpib
ˆpib+1
ˆpih+1
ˆpih
ˆpib
ˆpih+1
Fig. 4. (top) Illustrating the initial conﬁguration for Case 5 in the proof of Obser-
vation 4. (bottom) The sequence after moving the sensors ˆpib+1, . . . , ˆpih to the right
without increasing the cost of the covering assignment, resulting in one less overlap.
Let A = {xi | 0 ≤xi ≤L, i ∈S} ∪{0, 1, L −1, L} and let D = {ai ± 2 · c | 0 ≤
ai ±2·c ≤L, ai ∈A, c ∈Z}. Note that the number of points in D is bounded by
O(n2), since L ≤2n. Everything is in place to give the proof of Lemma 2, which
states that there is a cost1-optimal solution that places sensors only in locations
in D.
Proof (Proof of Lemma 2). The set of potential sensor locations D is a ﬁnite set
of locations on the barrier so the barrier can clearly be partitioned into intervals
with a subset of elements of D as endpoints. By Observation 4, each interval will
contain a (possibly empty) sequence of non-overlapping sensors that includes
one or both of the anchors and so every sensor in the covering is located at 2 · c,
c ∈Z, from some anchor sensor also in the covering.
□
2.3
Dynamic Programming Formulation for a Single Barrier
In this section we develop a dynamic programming algorithm for ﬁnding a cost1-
optimal order-preserving covering assignment where each sensor lies on a point
in D.
www.ebook3000.com

Barrier Coverage with Uniform Radii in 2D
65
We follow the standard approach for a dynamic programming algorithm.
Order the coordinates in D from left to right as c1, . . . , cm, where m = O(n2).
Let M[i, j] denote the cost of an optimal order-preserving covering assignment
of the part of b between 0 and cj using only the sensors 1, . . . , i. For simplicity we
will assume that a single point interval on a barrier does not have to be covered,
hence, M[0, 0] = 0.
Assume that the algorithm has computed all M[i′, j′]-values for all subprob-
lems smaller than M[i, j]. We can now calculate M[i, j] as follows:
M[i, j] =
min
cr∈D∩[cj−2,cj+2)
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
M[i −1, j]
M[i −1, r] + dist1(pi, (cj −1, 0))
if xi < cj −1
M[i −1, r] + dist1(pi, (cr + 1, 0))
if xi > cr + 1
M[i −1, r] + dist1(pi, (xi, 0))
if cj −1 ≤xi ≤cr + 1
The dynamic programming checks all possible placements of sensor i in an
attempt to ﬁnd an optimal order-preserving covering assignment to the sub-
barrier [0, cj]. The ﬁrst case is if the sensor i is not used in an optimal covering.
For the second case the dynamic programming considers all possible solutions
when pi moves right, and hence will move a minimum distance to the right which
is ˆxi = cj −1. For the third case, the sensor i is assumed to move left a minimum
distance, that is to position ˆxi = cr + 1. Finally, the last case considers the case
when i becomes an anchor and xi = ˆxi.
We are ready to give the proof of Lemma 3, which states that we can compute
a cost1-optimal order-preserving covering assignment in O(n4) time.
Proof (Proof of Lemma 3). For the algorithm we use the above dynamic pro-
gramming formulations. It is clear that the formulation computes a cost1-optimal
order preserving covering assignment with the additional constraint that sensors
must be placed on locations in D. From Lemma 2 and Observation 1, we know
that such a solution is also cost1-optimal among all covering assignments, so the
correctness of the algorithm follows.
For the running time, note that the algorithm considers O(n3) subproblems.
Each subproblem can be solved in time proportional to the number of points of
D within the interval [cj−2, cj+2), which is O(n). This yields an overall running
time of O(n4).
□
3
Parallel Barriers
In this section we relax the single barrier assumption to allow multiple parallel
barriers B = {b1, . . . , bk}. Without loss of generality we orient the plane as
for the single-barrier case, choosing the left-most barrier endpoint to coincide
with the origin, making this barrier and the x-axis collinear. Each barrier bt =
((x(t), y(t)), (x′(t), y(t))) in B, 1 ≤t ≤k, is now represented by a line segment
parallel to the x-axis. All sensors assigned covering a given barrier will have the
same y-value.

66
A. Cherry et al.
As in the single barrier case we assume that the sensors S = {1, . . . , n} are
ordered from left to right (ties are broken arbitrarily). We now generalize our
deﬁnition of order-preserving to multi-barrier. We say a covering assignment ˆp is
order-preserving if for each barrier bt the subset of sensors covering bt are placed
along bt in the same order they appear in the input; that is, for any sensors
i, j ∈S such that ˆpi, ˆpj ∈bt then it must be the case that if i < j then ˆxi ≤ˆxj.
The results from the single-barrier case can be generalized to the multiple-
barrier case as follows.
Lemma 4. In the parallel-barrier case any cost1-optimal order-preserving cov-
ering is
√
2-approximate under the dist2-cost.
Lemma 5. For the parallel-barrier case there exists a set D of O(kn2) discrete
points on the barrier such that there exists a cost1-optimal order-preserving cov-
ering assignment where sensors are only allowed to be placed in locations in D.
Lemma 6. For the parallel-barrier case there is an O(kn2k+2) time algorithm
for ﬁnding a cost1-optimal order-preserving covering assignment.
Lemma 4 follows directly from Lemma 1 and the new deﬁnition of order-
preserving to the parallel-barrier case. Lemma 5 follows by applying Lemma 2 to
each of the k barriers in the instance. Finally, in the next subsection we show
how to adapt the proof of the Lemma 3 to show Lemma 6.
Given these three lemmas, it is straightforward to combine them to get a
proof of Theorem 2, namely, an O(kn2k+2) time
√
2-approximation for the barrier
coverage problem with k parallel barriers.
3.1
Dynamic Programming Formulation for Parallel Barriers
We extend the dynamic programming formulation presented in Sect. 2.3 to
accommodate problem instances with multiple parallel barriers. The dynamic
program is very similar to the single barrier case, instead of considering all pos-
sible solutions for a sub-barrier of a single barrier the dynamic program has to
consider all possible sub-barriers of all the barriers.
Let A(t) = {xi | x(t) ≤xi ≤x′(t), i ∈S}∪{x(t), x(t)+1, x′(t)−1, x′(t)} and
let D(t) = {ai ± 2 · c | x(t) ≤ai ± 2 · c ≤x′(t), ai ∈A(t), c ∈Z}.
Let M[i, j1, . . . , jk] be the cost of an optimal covering assignment of the part
of each barrier Bt ∈B in the x-interval [x(t), ct,jt], 1 ≤t ≤k, using only the
sensors 1, . . . , i. As for the single barrier case we will assume that a single point
interval on a barrier does not have to be covered, hence, M[0, x(1), . . . , x(t)] = 0.
Assume that the algorithm has computed all M[i′, j′
1, . . . , j′
k] for all subprob-
lems smaller than M[i, j1, . . . , jk].
M[i, j1, . . . , jk] =
min
cr∈∪k
t=1D(t)∩[ct,jt−2,ct,jt+2)
www.ebook3000.com

Barrier Coverage with Uniform Radii in 2D
67
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
M[i −1, j1, . . . , jk]
M[i −1, cr, . . . , jk] + dist1(pi, (c1,j1 −1, y(t)))
if cr ∈A(1) and xi < c1,j1 −1
...
...
M[i −1, j1, . . . , cr] + dist1(pi, (ck,jk −1, y(k)))
if cr ∈A(k) and xi < ck,jk −1
M[i −1, cr, . . . , jk] + dist1(pi, (r + 1, y(1)))
if cr ∈A(1) and xi > r + 1
...
...
M[i −1, j1, . . . , cr] + dist1(pi, (r + 1, y(k)))
if cr ∈A(k) and xi > r + 1
M[i −1, cr, . . . , jk] + dist1(pi, (xi, y(1)))
if cr ∈A(1) and cj −1 ≤xi ≤r + 1
...
...
M[i −1, j1, . . . , cr] + dist1(pi, (xi, y(k)))
if cr ∈A(k) and cj −1 ≤xi ≤r + 1
The dynamic programming checks all possible placements of sensor i in an
attempt to ﬁnd an optimal order-preserving covering assignment. The ﬁrst case
is if the sensor i is not used in an optimal covering. For the second set of cases
the dynamic programming considers all possible solutions when pi moves right,
and hence will move a minimum distance to the right for each barrier. For the
third set of cases, the sensor i is assumed to move left a minimum distance for
each barrier. Finally, the last set of cases considers the case when i becomes an
anchor on a barrier and xi = ˆxi.
We are ready to give the proof of Lemma 6, which states that we can compute
a cost1-optimal order-preserving covering assignment in O(kn2k+2) time.
Proof (Proof of Lemma 6). For the algorithm we use the above dynamic pro-
gramming formulation. The correctness of the algorithm follows immediately
from Lemmas 4 and 5.
For the running time, note that the algorithm considers O(n · (n2)k) sub-
problems. Each subproblem can be solved in O(kn), similar to the single barrier
case. Summing up the running time of the algorithm is O(kn2k+2).
□
4
Non-parallel Barriers
Consider a set of non-parallel disjoint barriers B. Instead of using the dist1
measure we will need to use a Manhattan distance measure for each of the bar-
riers, which will be denoted the dist⊥measure. That is, we consider a form of
rectilinear distance in which a sensor moves to a barrier using only movements
perpendicular and parallel to that barrier. In the special case where a barrier
is parallel to either axis the distance is the familiar Manhattan distance but
note that, unlike the Manhattan distance, the dist⊥measure is invariant under
rotation. As with the multiple parallel barrier case, we preserve the left-to-right
order on each barrier but not necessarily across all barriers. Note that the deﬁni-
tion of an order-preserving covering assignment is still valid for the non-parallel
case.
Consider the minimum angle each barrier makes with the x-axis. Rotate the
plane so that the largest of these angles is minimised, denote this angle by α.
For example, for vertical and horizontal segments α would be π/4.

68
A. Cherry et al.
Lemma 7. In the disjoint multiple barrier case any cost⊥-optimal order-
preserving covering is 2
√
2(1 + 2 tan α)-approximate under the dist2-cost.
Lemma 8. For the disjoint multiple barrier case there exists a set D of O(kn2)
discrete points along the barrier such that there exists a cost⊥-optimal order-
preserving covering assignment where sensors are only allowed to be placed in
locations in D.
Lemma 9. For the disjoint multiple barrier case there is an O(kn2k+2) time
algorithm for ﬁnding a cost⊥-optimal order-preserving covering assignment.
Lemma 8 follows by applying Lemma 2 to each of the k barriers in the instance
using the dist⊥. The algorithm is analogous to that for computing an order-
preserving covering assignment for multiple parallel barriers from Sect. 3, hence,
Lemma 9 is identical to Lemma 6. It remains to prove Lemma 7 which will be
done below using the following lemma (proof omitted):
Lemma 10. Let B be a barrier with a minimum angle of α to the horizontal
line. If two sensors in a dist⊥-optimal covering assignment O of B are order-
reversed then swapping their positions in the covering assignment increases their
cost by at most a factor (1 + 2 tan α).
Given a dist⊥-optimal covering O, an order-preserving covering assignment
can be created by exchanging sensors until a left-to-right order is obtained on
each barrier.
Lemma 10 states that swap increases the sensors’ cost by a factor of at most
(1 + 2 tan α). In the worst case every sensor in O would need to be relocated. If
the swaps were performed in sequential order – that is, move a sensor to its ﬁnal
location then move the sensor it dislocates and so on – until all are in order every
sensor is handled at most twice and so we have that the total cost is at most
2(1 + 2 tan α) times the cost of O. Finally, we apply Lemma 1 which completes
the proof of Lemma 7, and hence also Theorem 3.
Remark 1. The results for disjoint multiple barriers can be modiﬁed to work for
simple polygons such as squares. However, extra care has to be taken at the
endpoints of the barriers, e.g., by modifying the dynamic programming.
References
1. Arora, A., Ramnath, R., Ertin, E., Sinha, P., Bapat, S., Naik, V., Kulathumani, V.,
Zhang, H., Cao, H., Sridharan, M., Kumar, S., Seddon, N., Anderson, C., Herman,
T., Trivedi, N., Zhang, C., Nesterenko, M., Shah, R., Kulkarni, S.S., Aramugam,
M., Wang, L., Gouda, M.G., Choi, Y.-R., Culler, D.E., Dutta, P., Sharp, C., Tolle,
G., Grimmer, M., Ferriera, B., Parker, K.: ExScal: elements of an extreme scale
wireless sensor network. In: 11th IEEE International Conference on Embedded and
Real-Time Computing Systems and Applications (RTCSA), pp. 102–108 (2005)
2. Bhattacharya, B.K., Burmester, B., Hu, Y., Kranakis, E., Shi, Q., Wiese, A.: Opti-
mal movement of mobile sensors for barrier coverage of a planar region. Theor.
Comput. Sci. 410(52), 5515–5528 (2009)
www.ebook3000.com

Barrier Coverage with Uniform Radii in 2D
69
3. Chen, A., Kumar, S., Lai, T.: Local barrier coverage in wireless sensor networks.
IEEE Trans. Mob. Comput. 9(4), 491–504 (2010)
4. Czyzowicz, J., Kranakis, E., Krizanc, D., Lambadaris, I., Narayanan, L., Opa-
trny, J., Stacho, L., Urrutia, J., Yazdani, M.: On minimizing the sum of sensor
movements for barrier coverage of a line segment. In: Nikolaidis, I., Wu, K. (eds.)
ADHOC-NOW 2010. LNCS, vol. 6288, pp. 29–42. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-14785-2 3
5. Dobrev, S., Durocher, S., Eftekhari, M., Georgiou, K., Kranakis, E., Krizanc, D.,
Narayanan, L., Opatrny, J., Shende, S., Urrutia, J.: Complexity of barrier coverage
with relocatable sensors in the plane. Theor. Comput. Sci. 579, 64–73 (2015)
6. Huang, C.-F., Tseng, Y.-C.: The coverage problem in a wireless sensor network.
Mob. Netw. Appl. 10(4), 519–528 (2005)
7. Kumar, S., Lai, T.-H., Arora, A.: Barrier coverage with wireless sensors. In: Pro-
ceedings of the 11th Annual International Conference on Mobile Computing and
Networking (MOBICOM), MobiCom 2005, pp. 284–298. ACM (2005)
8. Meguerdichian, S., Koushanfar, F., Potkonjak, M., Srivastava. M.B.: Coverage
problems in wireless ad-hoc sensor networks. In: Proceedings of the 20th Annual
Joint Conference of the IEEE Computer and Communications Societies INFO-
COM, vol. 3, pp. 1380–1387 (2001)
9. Mestre, J., Gaspers, S., Gudmundsson, J., R¨ummele, S.: Barrier coverage with non-
uniform length to minimize aggregate movements (2017, submitted manuscript)
10. Tan, X., Wu, G.: New Algorithms for Barrier Coverage with Mobile Sensors.
Springer, Heidelberg (2010). pp. 327–338
11. Tao, D., Wu, T.Y.: A survey on barrier coverage problem in directional sensor
networks. IEEE Sens. J. 15(2), 876–885 (2015)
12. Wu, F., Gui, Y., Wang, Z., Gao, X., Chen, G.: A survey on barrier coverage with
sensors. Front. Comput. Sci. 10(6), 968–984 (2016)

Rendezvous on a Line by Location-Aware
Robots Despite the Presence of Byzantine Faults
Huda Chuangpishit1,2, Jurek Czyzowicz1,
Evangelos Kranakis2(B), and Danny Krizanc3
1 D´epartemant d’informatique, Universit´e du Qu´ebec en Outaouais,
Gatineau, QC, Canada
2 School of Computer Science, Carleton University, Ottawa, ON, Canada
kranakis@scs.carleton.ca
3 Department of Mathematics and Computer Science, Wesleyan University,
Middletown, CT, USA
Abstract. A set of mobile robots is placed at points of an inﬁnite line.
The robots are equipped with GPS devices and they may communicate
their positions on the line to a central authority. The collection contains
an unknown subset of “spies”, i.e., byzantine robots, which are indistin-
guishable from the non-faulty ones. The set of the non-faulty robots need
to rendezvous in the shortest possible time in order to perform some task,
while the byzantine robots may try to delay their rendezvous for as long
as possible. The problem facing a central authority is to determine trajec-
tories for all robots so as to minimize the time until the non-faulty robots
have rendezvoused. The trajectories must be determined without knowl-
edge of which robots are faulty. Our goal is to minimize the competitive
ratio between the time required to achieve the ﬁrst rendezvous of the
non-faulty robots and the time required for such a rendezvous to occur
under the assumption that the faulty robots are known at the start. We
provide a bounded competitive ratio algorithm, where the central author-
ity is informed only of the set of initial robot positions, without knowing
which ones or how many of them are faulty. When an upper bound on
the number of byzantine robots is known to the central authority, we
provide algorithms with better competitive ratios. In some instances we
are able to show these algorithms are optimal.
Keywords: Competitive ratio · Faulty · GPS · Line · Rendezvous
Robot
1
Introduction
Rendezvous is useful for cooperative control in a distributed system, either when
communication between distributed entities is restricted by range limitations or
when it is required to speed up information exchanges in a distributed system.
It is often presented as a consensus problem in which the agents have to agree
J. Czyzowicz and E. Kranakis—Research supported in part by NSERC Discovery
grant.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 70–83, 2017.
https://doi.org/10.1007/978-3-319-72751-6_6
www.ebook3000.com

Rendezvous on a Line by Location-Aware Robots Despite the Presence
71
on the meeting point and time (see [28]) where by consensus we mean reaching
an agreement regarding a certain quantity of interest that depends on the state
of all the agents.
In this paper we consider the following version of the rendezvous problem.
A population of mobile robots is distributed at points of an inﬁnite line. The
robots are equipped with GPS devices and are able to communicate their initial
positions to a central authority. In order to perform some task, that the central
authority shall assign to the robots, all of the non-faulty robots need to ren-
dezvous (meet at the same point of the line). For this reason, the robots send
to the central authority the coordinates of their positions on the line and the
central authority assigns to each of them a route which eventually results in the
rendezvous of all robots. A group of robots may attempt the task at any time
in order to determine if all of the non-faulty robots have been brought together.
Unfortunately, an adversary has infected the population with “spies” - a
collection of byzantine faulty robots, indistinguishable from the original ones, in
order to delay the performance of the task for as long as possible. A byzantine
robot may fail to report its position, report a wrong position or it may fail
to follow its assigned route. Furthermore, a faulty robot may fail to help in
performing the required task. As the central authority does not know the identity
of the faulty robots it broadcasts travel instructions to all the robots.
We would like to deﬁne the strategy resulting in the smallest possible time of
the rendezvous of all non-faulty robots. Our goal is to minimize the competitive
ratio between the time required to achieve this ﬁrst rendezvous of the non-faulty
robots and the time required for such a rendezvous to occur under the assumption
that the faulty robots are known at the start.
1.1
Our Model
A collection of n anonymous robots travel along a Cartesian line with maximum
unit speed. Robots are equipped with GPS devices, so each of them is aware
of the coordinate of its current position on the line. An unknown subset of f
robots may turn out to be faulty. At some point in time, a task is identiﬁed that
requires the coming together of all of the non-faulty robots at the same point
on the line and this fact is broadcast to the robots by a central authority (CA).
The robots stop what they are doing and report their positions to the CA. The
CA computes trajectories for each of the robots and instructs them how to time
their movement.
At this point the robots follow the trajectories provided. The movement of
the robots continues until such time as all of the non-faulty robots meet for
the ﬁrst time and are able to perform the task, which ends the algorithm. We
assume the time required to attempt the task is negligible in comparison to the
time required for the robots to move between points. (As an example, imagine
that the robots have chip cards, that are used to open a container carried by all
robots. Using a secret-sharing scheme, the container is set to open only if n −f
or more of the keys are valid.) A failed attempt at the task may or may not
identify those robots that are faulty (caused the attempt to fail). If identiﬁed as
faulty, a robot need not continue on its trajectory. A successful attempt at the

72
H. Chuangpishit et al.
task means that all non-faulty robots are present and this is recognized by them
and the central authority.
As stated, we assume that the robots report their correct locations at the
beginning of the algorithm. We note that this need only be true of the non-faulty
robots as in the worst case the robots could be anywhere and the algorithm must
bring together all of them. It is possible that faulty robots may report initial
locations that are incorrect and potentially adversely eﬀect the lengths of the
trajectories. Of course, this may result in their receiving trajectories that they
cannot complete without being detected as faulty by the other robots. But as long
as all non-faulty robots complete their trajectories the algorithm must ensure
that they meet.
The message to the CA about a robot’s position contains the robot’s unique
identity. We assume that the faulty robots cannot lie about their identity. Con-
sequently, each faulty robot can send only one message about its position, oth-
erwise it will be identiﬁed as faulty and ignored. Observe that, as the robot’s
identity, the CA could use the position communicated by the robots, and thus
our approach could be extended to anonymous robots. This would require some
extra conditions on the model (e.g., message uniqueness), so, for simplicity, we
assume that our robots have unique identiﬁers.
We also assume that after the initial reporting of their positions, until the
reporting of success with the task, there is no further communication between
the robots themselves or the robots and the central authority. Again, this need
only be true of the non-faulty robots. Any communication by robots during
the execution of the trajectories is assumed to come from faulty robots and is
ignored.
We note that the requirement of a central authority may be removed by
allowing the robots to broadcast their initial positions to all other robots and
each computing the same set of trajectories using the same algorithm.
A rendezvous algorithm speciﬁes the trajectories of the robots as a function of
time. We assume the robots have suﬃcient memory to carry out the instructions
of the rendezvous algorithm. The competitive ratio of a given algorithm is the
ratio of the time it takes the algorithm to enable rendezvous of all non-faulty
robots divided by the time it takes the best oﬀ-line algorithm, with knowledge
of which robots are faulty, to accomplish the same. Note: the time of the oﬄine
algorithm equals D/2, where D is the minimum diameter of the set of non-faulty
robots. Indeed, these non-faulty robots could then meet at the mid point between
the most distant ones in the set.
We assume that the task is such that n −f non-faulty robots are necessary
and suﬃcient to perform the task. Under this assumption, the task can be used
to determine if all of the non-faulty robots are together. If a group of robots
attempts the task and it succeeds, it contains all non-faulty robots. If it fails,
then there exist more non-faulty robots outside the group.
Below we present algorithms which have no knowledge of f as well as oth-
ers where an upper bound on f is provided. Depending upon that knowledge,
diﬀerent algorithms can achieve a better competitive ratio in diﬀerent situations.
We restrict our attention to the nontrivial case where at least two robots must
rendezvous, i.e., f ≤n −2.
www.ebook3000.com

Rendezvous on a Line by Location-Aware Robots Despite the Presence
73
1.2
Related Work
The mobile agent rendezvous problem has been studied extensively in many
topologies (or domains) and under various assumptions on system synchronicity
and capabilities of the agents [12,14,16,24] both as a dynamic symmetry break-
ing problem [30] as well as in operations research [2] in order to understand
the limitations of search theory. A critical distinction in the models is whether
the agents must all run the same algorithm, which is generally known as the
symmetric rendezvous problem [3]. If agents can execute diﬀerent algorithms,
generally known as the asymmetric rendezvous problem, then the problem is
typically much easier, though not always trivial.
Closely related to our research is the work of [9,10]. In [9] the authors study
rendezvous of two anonymous agents, where each agent knows its own initial
position in the environment, and the environment is a ﬁnite or inﬁnite graph
or a Euclidean space. They show that in the line and trees as well as in multi-
dimensional Euclidean spaces and grids the agents can rendezvous in time O(d),
where d is the distance between the initial positions of the agents. In [10] the
authors study eﬃcient rendezvous of two mobile agents moving asynchronously
in the Euclidean 2d-space. Each agent has limited visibility, permitting it to
see its neighborhood at unit range from its current location. Moreover, it is
assumed that each agent knows its own initial position in the plane given by its
coordinates. The agents, however, are not aware of each other’s position. Also
worth mentioning is the work of [4] which studies the rendezvous problem of
location-aware agents in the asynchronous case and whose proposed algorithm
provides a route, leading to rendezvous.
The underlying domain which is traversed by the robots is a continuous
curve (in our case an inﬁnite line) and the robots may exploit a particular char-
acteristic, e.g., diﬀerent identiﬁers, speeds, or their initial location, to achieve
rendezvous. For example, in several papers the robots make use of the fact that
they have diﬀerent speeds, as in the paper [18], as well as in the work on prob-
abilistic rendezvous on a cycle [23]. Rendezvous on a cycle for multiple robots
with diﬀerent speeds is studied in [20], and rendezvous in arbitrary graphs for
two robots with diﬀerent speeds in [25].
There is also related work on gathering a collection of identical memoryless,
mobile robots in one node of an anonymous ring whereby robots start from
diﬀerent nodes of the ring and operate in Look-Compute-Move cycles and have
to end up in the same node [22], as well as oblivious mobile robots in the same
location of the plane when the robots have limited visibility [19].
Fault tolerance has been extensively studied in distributed computing,
though failures were usually related to static elements of the environment, like
network nodes or links (e.g., see [26,27]), rather than to the mobile components.
The unreliability of robots has been studied with respect to inaccurate robots’
sensing or mobility devices (cf. [8,21,29]). Problems concerning faulty robots
operating in a line environment have been studied in the context of searching
in [13] and patrolling [11]. The questions of convergence or gathering involving
faulty robots were investigated in [1,5,7,15,17]. To the best of our knowledge

74
H. Chuangpishit et al.
the rendezvous problem for location aware robots some of which may be faulty
has never been considered by the research community in the past.
1.3
Our Results
Here is an outline of the results of the paper. In Sect. 2 we consider two gen-
eral rendezvous algorithms for n > 2 robots with f ≤n −2 faulty ones. Both
algorithms assume no knowledge of the actual value of f and the second algo-
rithm stops as soon as suﬃciently many robots are available to perform the task.
The competitive ratios of these algorithms are f + 1 and 12, respectively. We
also prove a lower bound of 2 on the competitive ratio for arbitrary n > 2 and
1 ≤f ≤n −2. In Sect. 3 we provide algorithms for the case where the central
authority possesses some knowledge concerning the number of faulty robots. For
the case where the ratio of the number of faulty robots to the total number of
robots is strictly less than 1/2 we provide an optimal algorithm and when this
number is strictly less than 2/3 we give an algorithm that beats the general case
algorithms above unless f is known to be less than 5. Next we provide optimal
algorithms for the particular cases where f ∈{1, 2} in Sect. 4. The main result
here is the case of n = 4 and f = 2 where we show the exact value of the com-
petitive ratio is 1 + φ, where φ is the golden ratio. We end with a discussion of
open problems. A full version of the paper can be found in [6].
2
General Results
In this section we present a rendezvous algorithm for n robots f of which are
faulty with a competitive ratio of at most min{f + 1, 12}. Neither of these algo-
rithms require prior knowledge of f. We also show that the competitive ratio of
any rendezvous algorithm is at least 2. We ﬁrst observe that the assumptions of
our model allow us to severely restrict the potential algorithms available to the
CA. We can show the following lemma:
Lemma 1. Consider a rendezvous algorithm A for n robots f of which are faulty
with competitive ratio α. There exists a rendezvous algorithm B such that during
the execution of B the movement of the robots follow these rules:
(1) A robot does not change direction between meetings with other robots.
(2) The robots always move at full speed.
Moreover, the competitive ratio of B is less or equal to α.
We assume throughout the paper that the movement of the robots in any
rendezvous algorithm follows rules (1) and (2) of Lemma 1.
2.1
Upper Bounds
The ﬁrst rendezvous algorithm we present has a competitive ratio which is
bounded above by the number of faulty robots plus one. It is interesting to note,
www.ebook3000.com

Rendezvous on a Line by Location-Aware Robots Despite the Presence
75
that to obtain such competitive ratio no knowledge of the number of faulty
robots is necessary. The idea of the algorithm can be summarized as follows.
Consider the distances between consecutive robots on the line. The algorithm
shrinks the shortest interval (between consecutive robots) in that the two robots
at its endpoints meet at its midpoint while the rest of the robots “follow the
shrinkage” depending on their location until all non-faulty robots meet (or suf-
ﬁciently many of them in the case where the task does not require all non-faulty
robots to be together to be performed). We prove the following theorem.
Theorem 1. There is a rendezvous algorithm for n > 2 robots at most f of
which are faulty whose competitive ratio is at most f + 1, where f ≤n −2.
We now describe a second general approach for rendezvous of n robots, which
also works for any number f of faulty robots. Unlike the previous one, this
algorithm has a competitive ratio independent of f, (it equals 12). The core of
our approach is the Algorithm 1, presented in [9], which guarantees rendezvous
of any two robots, at initial integer positions at distance d on the line, in time of
at most 6d. The idea of the algorithm is the following. Each robot gets an integer
label corresponding to its initial position. The algorithm consists of a sequence of
rounds, each round containing two stages. In the ﬁrst round, odd-labelled robots
move distance 1/2 to the right in the ﬁrst stage and then distance 1 to the left
in the second stage. The even-labelled robots move distance 1/2 to the left in
the ﬁrst stage and then distance 1 to the right in the second stage. Observe that
each odd-label robot would meet its right neighbour at initial distance 1 in the
ﬁrst stage and its left neighbour at distance 1 in the second stage. At the end of
the ﬁrst round robots are in groups that from now on will travel together.
Algorithm 1. Rendezvous on the inﬁnite line
1: Set ℓ= 1
2.
2: for all agents a do
3:
Set label(a) = position of a on the line.
4: for all i = 1, 2, 3, . . . do
5:
for all agent a do
6:
Stage 1.
7:
if odd(label(a)) then
8:
move right distance ℓ
9:
else
10:
move left distance ℓ.
11:
Stage 2.
12:
if odd(label(a)) then
13:
move left distance 2ℓ.
14:
else
15:
move right distance 2ℓ.
16:
ℓ= 2ℓ
17:
label(a) = ⌊label(a)
2
⌋

76
H. Chuangpishit et al.
All groups are then at even distances. In round two, the conﬁguration of such
groups on the line is scaled up by the factor of two and each group of robots meet
neighbouring groups at distance 2 in the two corresponding stages. The process
continues inductively and after round i, the groups are at integer positions being
multiples of 2i. It is possible to show that during round i, in its ﬁrst stage meet
all robots initially placed in any interval [(2k −1)2i, (2k −1)2i), for some integer
k, and in its second stage meet all the robots initially placed in any interval
[(2k)2i, (2k + 2)2i), for some integer k. Let D be minimum diameter of the set
of non-faulty robots required to rendezvous, and i∗= ⌈log2 D⌉. It easy to see
that all the non-faulty robots must meet in the ﬁrst or the second stage of round
i∗+ 1. Moreover, the total distance travelled by each robot is linear in D. In [9]
they show the following:
Theorem 2 ([9]). For two agents a1, a2 starting at distance d (and at integer
points) on the line, Algorithm 1 permits rendezvous within at most 6d time.
The following lemma is an immediate consequence of Theorem 2.
Lemma 2. Let a1 and a2 be two robots on the real line with integer starting
positions at distance d. Then the rendezvous time of a1 and a2 in Algorithm 1 is
at most 6d.
We now have all the required results to prove an upper bound of 12 on the
competitive ratio of rendezvous of n robots f of which are faulty. Our approach
is to approximate the initial positions of all robots by other ones which are at
rational coordinates. Then the obtained conﬁguration may be scaled up so that
all initial robot positions are integers and Algorithm 1 may be applied. We show
that for any ϵ > 0 we can choose an approximation ﬁne enough so that the
competitive ratio does not exceed 12 + ϵ. We have the following theorem.
Theorem 3. There exists a rendezvous algorithm for n > 2 robots, at most
f ≤n −2 of which are faulty, which guarantees a competitive ratio less than
12 + ϵ, for any ϵ > 0.
As a corollary of Theorems 1 and 3 we can state the following.
Corollary 1. There is a rendezvous algorithm for n > 2 robots at most f ≤n−2
of which are faulty, with competitive ratio at most min{12 + ϵ, f + 1}, for any
ϵ > 0.
2.2
Lower Bound
Next we show that any rendezvous algorithm for n robots, which include at least
one which is faulty, must have a competitive ratio of at least 2.
Theorem 4. For any n > 2 robots, any 1 ≤f ≤n −2 of which are faulty,
the competitive ratio of any algorithm that achieves rendezvous of at least n −f
non-faulty robots is at least 2.
www.ebook3000.com

Rendezvous on a Line by Location-Aware Robots Despite the Presence
77
Proof. (Theorem 4) Consider the following arrangement of the robots where n−f
robots are required to perform rendezvous: ⌈f+1
2 ⌉are located at position −1,
n −f −1 are located at the origin and ⌊f+1
2 ⌋are located at position 1. By
Lemma 1, we can assume there is an optimal rendezvous algorithm in which all
robots move at speed 1 for the ﬁrst 1/2 time unit. At that time, at least one of
the robots, say r, starting at the origin must be at −1/2 or 1/2. Wlog, assume
it is at −1/2. Make all of robots starting at the origin non-faulty, one of the
robots, say r′, starting at 1 non-faulty, and the remaining f robots faulty. In
order for n −f non-faulty robots to meet, r and r′ must meet which requires
at least another 1/2 time unit, i.e., the competitive ratio of the algorithm is at
least 2.
⊓⊔
3
Bounded Number of Faults
In the previous section we proposed algorithms, whose competitive ratio did not
depend on the knowledge of the number f of faulty robots. However, employing
Corollary 1 to get the competitive ratio which is the best between the values 12
and f +1 (cf. Theorems 1 and 3), we need to have knowledge of an upper bound
on f. In this section we show, that having more precise knowledge on an upper
bound on f allows us to obtain algorithms with more attractive competitive
ratios. More exactly, we provide upper bounds for the competitive ratio of ren-
dezvous algorithms where the number of faulty robots is known to be bounded
by a fraction of the total number of robots.
The following theorem shows that if the majority of the robots are non-faulty
then there is a rendezvous algorithm whose competitive ratio is at most 2. By
Theorem 4, this is optimal.
Theorem 5. Suppose that n ≥3 and the number of faulty robots is f ≤n−1
2 .
Then there is a rendezvous algorithm with competitive ratio at most 2.
As a consequence of this result and Theorem 1 we get the following corollary:
Corollary 2. If the number of faulty robots is strictly less than the number of
non-faulty robots then the competitive ratio for solving the rendezvous problem
is exactly 2.
In the sequel we consider the case
n−1
2
< f <
2
3(n −1) and provide an
algorithm that has a better guarantee than the general algorithm as long as our
upper bound on f is greater than 4.
Theorem 6. Suppose that n ≥3 and there are at most f faulty robots. If f ≤
2
3(n −1) then there is a rendezvous algorithm with competitive ratio at most 5.
In the sequel, we present the proof of Theorem 6. First note that if n ≤8.
Then f ≤
2
3(8 −1) =
14
3 , and so f ≤4. Therefore by Theorem 1, there is a
rendezvous algorithm with competitive ratio 5. Thus, without loss of generality
we can assume that n ≥9.

78
H. Chuangpishit et al.
Lemma 3. Let n ≥9 and f < 2
3(n −1) then there is a partition of the robots
into three groups GL, GM, and GR such that at least two of the groups GL, GM,
and GR contain a non-faulty robot.
We are now ready to prove Theorem 6. We present a rendezvous algorithm
for the case f < 2
3(n −1) whose competitive ratio is 5.
Proof. (Theorem 6) Let f < 2
3(n −1). As we discussed earlier we may assume
that n ≥9, as for the case n ≤8 we obtain a competitive ratio of 5 by Theorem 1.
Therefore we can use Lemma 3 to split the robots into three groups Gl, GM, and
GR. Consider the following rendezvous algorithm:
Algorithm 2.
1: The robots broadcast their coordinates, and split into three groups as follows.
2:
GL contains the ⌊n
2 ⌋−k −1 leftmost robots
3:
GR contains the ⌈n
2 ⌉−k −1 rightmost robots.
4:
GM contains the 2k + 2 middle robots.
5: Let Al and Ar be the leftmost and the rightmost robots of GM, respectively. More-
over let ml and mr be the initial positions of Al and Ar, respectively. For the robot
Al, sequence all the other robots based on their distances to Al such that the robots
with shorter distances appear earlier in the sequence, denote the sequence by Sl.
Do the same for Ar, and let Sr denotes its corresponding sequence.
6: while the rendezvous has not occurred do
7:
The robots in GL move at full speed to the right, and when they meet Al stick
to Al.
8:
The robots in GR move at full speed to the left, and when they meet Ar stick
to Ar.
9:
The robots in interval [ml, ml+mr
2
) move towards Al and when they meet Al
stick to it.
10:
The robots in interval [ ml+mr
2
, mr] move towards Ar, and when they meet Ar
stick to it.
11:
The robot Al moves to the robot next in the sequences Sl, until it meets Ar.
Then it sticks to Ar.
12:
The robot Ar moves to the robot next in the sequences Sr, until it meets Al.
Then it sticks to Al.
13:
When Al and Ar meet they stick to each other. Then they sequence the robots
based on their distances to the location of their meeting in such a way that the
robots closer to the meeting point appear earlier in the sequence. Denote the
sequence by S. The robots Al ∪Ar move to the next robot in the sequence S.
We now analyze the competitive ratio of the above algorithm. As seen in
Fig. 1, deﬁne
– Bl: the rightmost robot in [ml, ml+mr
2
).
– Br: the leftmost robot in [ ml+mr
2
, mr].
– Cl: the last robot in GL that Al meets before Al moves to visit Br.
www.ebook3000.com

Rendezvous on a Line by Location-Aware Robots Despite the Presence
79
– Cr: the last robot in GR that Ar meets before Ar moves to visit Bl.
– d1: the distance between Al and Bl.
– d2: the distance between Ar and Br.
– d3: the distance between Al and Cl.
– d4: the distance between Ar and Cr.
– x: the distance between Al and Ar.
Fig. 1. Robot movement and the analysis of Algorithm 2.
The following inequalities follow immediately.
(1) Al meets Cl before Br: d3 ≤d1 + x.
(2) Ar meets Cr before Bl: d4 ≤d2 + x.
(3) Without loss of generality assume that d1 ≤d2.
Let Ml be the group of the robots which stick to Al before a meeting with
Ar, see Fig. 2. More precisely Ml contains Cl and all the robots to the right of Cl,
and Bl and all the robots to the left of Bl. Similarly deﬁne Mr to be the group
of the robots which stick to Ar before a meeting with Al. Then Mr contains Cr
and all the robots to its left, and Br and all the robots to its right.
Fig. 2. Robot movement and the analysis of Algorithm 2.
Consider the following three cases:
Case 1. The rendezvous occurs among Ml or Mr: Without loss of generality
assume that the rendezvous occurs among ML. This implies that the non-faulty

80
H. Chuangpishit et al.
robots belong to GL and the interval [ml, ml+mr
2
). Let al be the leftmost non-
faulty robot of GL and ar be the rightmost non-faulty robot of [ml, ml+mr
2
).
The rendezvous of Algorithm 2 occurs when Al meets both ar and al. Suppose
that δ1 and δ2 are the distances between Al, ar and Al, al respectively. Since
Al moves towards the closest robots then the rendezvous occurs at the time at
most 3
2max{δ1, δ2}. Moreover max{δ1, δ2} is bounded above by the diameter of
non-faulty robots. Therefore in this case the competitive ratio is at most 3.
Case 2. The rendezvous occurs at the time of the meeting of Ml and
Mr: The meeting of Ml and Mr occurs when the robots Cl and Cr meet. The
robot Cl moves to the right and the robot Cr moves to the left, and thus their
meeting occurs at time
d1 + d2 + d3 + d4 + x
2
.
By Inequalities (1), (2) and (3) we have
d1 + d2 + d3 + d4 + x ≤2d1 + 2d2 + 3x
= 4d1 + 2x1 + 3x
= 4(d1 + x) −x + 2x1
By Lemma 3 we know that at least two of GL, GM, and GR contain non-faulty
robots. This implies that the diameter of the non-faulty robots, D, is at least
min{d1 +x+z, d2 +x+y}. By Inequality (3) we have that D ≥d1 +x. Therefore
CR ≤4(d1 + x) −x + 2x1
d1 + x
= 4 + x1 −x2
d1 + x ≤5
Case 3. The rendezvous occurs after the meeting of Ml and Mr: This
case occurs if there are non-faulty robots either to the left of Cl or to the right
of Cr. First assume that there are non-faulty robots both to the left of Cl and to
the right of Cr. Then the rendezvous occurs in at most two times the diameter
of non-faulty robots. Therefore the competitive ratio is at most 2. Now suppose
without loss of generality that there are only non-faulty robots to the right of Cr,
and the rightmost non-faulty robot is R at distance δ from Cr. Then at the time
of the meeting of Ml and Mr the distance between the robots in Ml ∪Mr and
R is δ. So it takes at most 3
2δ for Ml ∪Mr to meet R. Therefore the rendezvous
occurs at time
d1 + d2 + d3 + d4 + x
2
+ 3δ
2
while the optimal time is at least d4+δ
2
.
Note that Ar meets Al before R, and thus d1 + d2 + x ≤d4 + δ. Moreover
d3 ≤d2 + x. Therefore
d1 + d2 + d3 + d4 + x
2
+ 3δ
2 ≤3(d4 + δ)
2
This implies that the competitive ratio in this case is at most 3.
⊓⊔
www.ebook3000.com

Rendezvous on a Line by Location-Aware Robots Despite the Presence
81
4
Optimal Rendezvous Algorithms for at Most Two
Faulty Robots
This section is dedicated to the study of optimal rendezvous algorithms when
the number of faulty robots is small, i.e., for f ∈{1, 2}.
The next theorem yields the competitive ratio for f = 1 fault and is an
immediate consequence of Theorems 1 and 4.
Theorem 7. For n > 2 robots with f = 1 faulty, the competitive ratio of the
algorithm which shrinks the shortest interval is 2, and this is optimal.
It remains to consider the competitive ratio for f = 2 faulty robots. By
Corollary 2, the competitive ratio of the rendezvous problem for n ≥5 robots
with two faulty is exactly 2. Therefore the only unknown case concerning two
faulty robots is when n = 4. In this section we present a rendezvous algorithm
with optimal competitive ratio 1 + φ, where φ = 1+
√
5
2
is the golden ratio. We
summarize the main result in the following two theorems. For the lower bound
we prove:
Theorem 8. Consider four robots exactly two of which are faulty. No ren-
dezvous algorithm can have competitive ratio less than 1 + φ, where φ is the
golden ratio.
The proof of Theorem 8 is based on an exhaustive analysis and considers the
competitive ratio of any potential algorithm solving the rendezvous problem for
the four robots. For the upper bound we prove:
Theorem 9. Consider four robots exactly two of which are faulty. There is a
rendezvous algorithm for four robots two of which are faulty with competitive
ratio at most 1 + φ, where φ is the golden ratio.
The proof of Theorem 9 is a continuation of the proof of Theorem 8 leading to a
speciﬁc algorithm whose competitive ratio is optimal for the rendezvous problem
considered.
5
Conclusion
In this paper we considered the rendezvous problem for n > 2 robots on a line
with 1 ≤f ≤n−2 among them byzantine faulty. The robots were equipped with
GPS devices and they could communicate their positions to a central authority.
We designed several rendezvous algorithms and considered their competitive
ratio depending on the knowledge the central authority has about the number
of faulty robots. An interesting question remaining might be to improve the
competitive of the algorithms presented. Another question concerns the model
presented here which ignores any communication beyond the broadcasting of the
initial positions of the robots. It might be of interest to consider algorithms in
a “richer” communication model where the robots may broadcast information
as they follow their trajectories. For example, one could consider a model where
the faulty robots may crash and non-faulty robots may report not meeting them
when expected.

82
H. Chuangpishit et al.
References
1. Agmon, N., Peleg, D.: Fault-tolerant gathering algorithms for autonomous mobile
robots. SIAM J. Comput. 36(1), 56–82 (2006)
2. Alpern, S.: The rendezvous search problem. SIAM J. Control Optim. 33(3), 673–
683 (1995)
3. Alpern, S.: Rendezvous search: a personal perspective. Oper. Res. 50(5), 772–795
(2002)
4. Bampas, E., Czyzowicz, J., G asieniec, L., Ilcinkas, D., Labourel, A.: Almost opti-
mal asynchronous rendezvous in inﬁnite multidimensional grids. In: Lynch, N.A.,
Shvartsman, A.A. (eds.) DISC 2010. LNCS, vol. 6343, pp. 297–311. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-15763-9 28
5. Bouzid, Z., Potop-Butucaru, M.G., Tixeuil, S.: Optimal byzantine-rezilient conver-
gence in uni-dimensional robot network. Theor. Comput. Sci. 411(34–36), 3154–
3168 (2010)
6. Chuangpishit, H., Czyzowicz, J., Kranakis, E., Krizanc, D.: Rendezvous on a line
by location-aware robots despite the presence of byzantine faults (2017). https://
arxiv.org/pdf/1707.06776.pdf
7. Cohen, R., Peleg, D.: Convergence properties of the gravitational algorithm in
asynchronous robot systems. SIAM J. Comput. 41(1), 1516–1528 (2005)
8. Cohen, R., Peleg, D.: Convergence of autonomous mobile robots with inaccurate
sensors and movements. SIAM J. Comput. 38(1), 276–302 (2008)
9. Collins, A., Czyzowicz, J., G asieniec, L., Kosowski, A., Martin, R.: Synchronous
rendezvous for location-aware agents. In: Peleg, D. (ed.) DISC 2011. LNCS, vol.
6950, pp. 447–459. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-
642-24100-0 42
10. Collins, A., Czyzowicz, J., G asieniec, L., Labourel, A.: Tell me where I am so
I can meet you sooner. In: Abramsky, S., Gavoille, C., Kirchner, C., Meyer auf
der Heide, F., Spirakis, P.G. (eds.) ICALP 2010. LNCS, vol. 6199, pp. 502–514.
Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14162-1 42
11. Czyzowicz, J., G asieniec, L., Kosowski, A., Kranakis, E., Krizanc, D., Taleb, N.:
When patrolmen become corrupted: monitoring a graph using faulty mobile robots.
In: Elbassioni, K., Makino, K. (eds.) ISAAC 2015. LNCS, vol. 9472, pp. 343–354.
Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48971-0 30
12. Czyzowicz, J., Kosowski, A., Pelc, A.: Deterministic rendezvous of asynchronous
bounded-memory agents in polygonal terrains. Theory Comput. Syst. 52(2), 179–
199 (2013)
13. Czyzowicz, J., Kranakis, E., Krizanc, D., Narayanan, L., Opatrny, J.: Search on a
line with faulty robots. In: Proceedings of the 2016 ACM Symposium on Principles
of Distributed Computing, PODC 2016, Chicago, IL, USA, 25–28 July 2016, pp.
405–414 (2016)
14. De Marco, G., Gargano, L., Kranakis, E., Krizanc, D., Pelc, A., Vaccaro, U.: Asyn-
chronous deterministic rendezvous in graphs. Theor. Comput. Sci. 355(3), 315–326
(2006)
15. D´efago, X., Gradinariu, M., Messika, S., Raipin-Parv´edy, P.: Fault-tolerant and
self-stabilizing mobile robots gathering. In: Dolev, S. (ed.) DISC 2006. LNCS, vol.
4167, pp. 46–60. Springer, Heidelberg (2006). https://doi.org/10.1007/11864219 4
16. Dessmark, A., Fraigniaud, P., Kowalski, D., Pelc, A.: Deterministic rendezvous in
graphs. Algorithmica 46, 69–96 (2006)
www.ebook3000.com

Rendezvous on a Line by Location-Aware Robots Despite the Presence
83
17. Dieudonn´e, Y., Pelc, A., Peleg, D.: Gathering despite mischief. ACM Trans. Algo-
rithms (TALG) 11(1), 1 (2014)
18. Feinerman, O., Korman, A., Kutten, S., Rodeh, Y.: Fast rendezvous on a cycle by
agents with diﬀerent speeds. In: Chatterjee, M., Cao, J., Kothapalli, K., Rajsbaum,
S. (eds.) ICDCN 2014. LNCS, vol. 8314, pp. 1–13. Springer, Heidelberg (2014).
https://doi.org/10.1007/978-3-642-45249-9 1
19. Flocchini, P., Prencipe, G., Santoro, N., Widmayer, P.: Gathering of asynchronous
robots with limited visibility. Theor. Comput. Sci. 337(1), 147–168 (2005)
20. Huus, E., Kranakis, E.: Rendezvous of many agents with diﬀerent speeds in a cycle.
In: Papavassiliou, S., Ruehrup, S. (eds.) ADHOC-NOW 2015. LNCS, vol. 9143, pp.
195–209. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-19662-6 14
21. Izumi, T., Souissi, S., Katayama, Y., Inuzuka, N., D´efago, X., Wada, K., Yamashita,
M.: The gathering problem for two oblivious robots with unreliable compasses.
SIAM J. Comput. 41(1), 26–46 (2012)
22. Klasing, R., Markou, E., Pelc, A.: Gathering asynchronous oblivious mobile robots
in a ring. Theoret. Comput. Sci. 390(1), 27–39 (2008)
23. Kranakis, E., Krizanc, D., MacQuarrie, F., Shende, S.: Randomized rendezvous
algorithms for agents on a ring with diﬀerent speeds. In: Proceedings of the 2015
International Conference on Distributed Computing and Networking, ICDCN 2015,
Goa, India, 4–7 January 2015, pp. 9:1–9:10 (2015)
24. Kranakis, E., Krizanc, D., Markou, E.: The Mobile Agent Rendezvous Problem in
the Ring: An Introduction. Synthesis Lectures on Distributed Computing Theory
Series. Morgan & Claypool Publishers, San Rafael (2010)
25. Kranakis, E., Krizanc, D., Markou, E., Pagourtzis, A., Ram´ırez, F.: Diﬀerent speeds
suﬃce for rendezvous of two agents on arbitrary graphs. In: Steﬀen, B., Baier, C.,
van den Brand, M., Eder, J., Hinchey, M., Margaria, T. (eds.) SOFSEM 2017.
LNCS, vol. 10139, pp. 79–90. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-51963-0 7
26. Lamport, L., Shostak, R., Pease, M.: The byzantine generals problem. ACM Trans.
Program. Lang. Syst. (TOPLAS) 4(3), 382–401 (1982)
27. Lynch, N.A.: Distributed Algorithms. Morgan Kaufmann, Burlington (1996)
28. Olfati-Saber, R., Fax, J.A., Murray, R.M.: Consensus and cooperation in networked
multi-agent systems. Proc. IEEE 95(1), 215–233 (2007)
29. Souissi, S., D´efago, X., Yamashita, M.: Gathering asynchronous mobile robots with
inaccurate compasses. In: Shvartsman, M.M.A.A. (ed.) OPODIS 2006. LNCS,
vol. 4305, pp. 333–349. Springer, Heidelberg (2006). https://doi.org/10.1007/
11945529 24
30. Yu, X., Yung, M.: Agent rendezvous: a dynamic symmetry-breaking problem. In:
Meyer, F., Monien, B. (eds.) ICALP 1996. LNCS, vol. 1099, pp. 610–621. Springer,
Heidelberg (1996). https://doi.org/10.1007/3-540-61440-0 163

Querying with Uncertainty
Huda Chuangpishit1, Kostantinos Georgiou1, and Evangelos Kranakis2(B)
1 Department of Mathematics, Ryerson University, Toronto, ON, Canada
2 School of Computer Science, Carleton University, Ottawa, ON, Canada
kranakis@scs.carleton.ca
Abstract. We introduce and study a new optimization problem on
querying with uncertainty. k robots are required to locate a hidden item
that is placed uniformly at random in one of n diﬀerent locations, each
associated with a probability pi, i = 1, . . . , n. If the item is placed in
location i, a query trial by any of the robots reveals the item with prob-
ability pi. Each robot j is assigned a subset Aj of the locations, and is
allowed to perform a random walk among them, each time step querying
the current location (being visited) for the item. We are interested in
determining sets {Aj}j=1,...,k so as to minimize the expected discovery
time of the item. We measure the cost by the number of queries, while
there is no cost for hopping from node to node.
Our ﬁrst contribution is to prove a closed formula for the expected
number of steps until the treasure is found when the robots execute
unanimous queries. Then we focus on querying problems where the sets
Aj are restricted to be either pairwise disjoint or identical. Our ﬁndings
allow us to obtain optimal solutions, when sets Aj are exclusively pair-
wise disjoint, requiring time nO(k). In our second contribution, we devise
an optimal polynomial time algorithm for querying with k = 2 robots
even when the sets A1, A2 are allowed to overlap. All our algorithms are
based on special concavity-type properties of the expected termination
time when the robots execute unanimous queries, thus inducing special
structural properties of optimal solutions for the general problem.
Keywords: Searching · Querying · Random walk · Partition
Assignment · Optimization
1
Introduction
Search in computer science aims to retrieve information which is stored in a
given data structure. It involves an algorithm which describes the “trajectories”
of each of the searchers (in the present paper they will be autonomous robots)
within the data structure and a target representing the information and whose
location–although unknown to the searcher(s)–can be recognized by the searchers
K. Georgiou and E. Kranakis—Research supported in part by NSERC Discovery
grant.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 84–97, 2017.
https://doi.org/10.1007/978-3-319-72751-6_7
www.ebook3000.com

Querying with Uncertainty
85
when located (by a robot). There is vast literature on search algorithms (both
deterministic and randomized) depending on the type of data structure or search
domain, capabilities of the robots, and properties of the target of which it is
worth mentioning the books [2,4,21].
In search, there are numerous situations when the searchers (be that humans
or machines) have to make decisions whose outcomes may not be under the
searcher’s control. For example, when a drone needs to make routing decisions
(in military strategy planning), or a guidance system must consult a forwarding
database. These kinds of problems have been considered in artiﬁcial intelligence
and typically involve some form of combinatorial search in which one is look-
ing for a speciﬁc sub-structure of the given discrete structure which achieves a
solution of the search problem.
In this paper we investigate the structure of search in an environment involv-
ing uncertainties both in the actual location of the target as well as in recognizing
the target. Assume there are n diﬀerent “black” boxes and that a treasure is hid-
den uniformly at random at one of them. Searchers may query the boxes to ﬁnd
the treasure that will be revealed with a certain probability (which depends on
the box) but only if it is located there. We are interested in designing search
algorithms for ﬁnding the treasure. As it is standard practice in this area, when
uncertainty is involved a natural approach is to look for algorithms that have
optimal behaviour, where this is measured either by the probability of success or
the expected number of steps until the target is identiﬁed. In order to make the
model more interesting, we also assume that each robot may only be assigned
a speciﬁc subset of the boxes that it will be investigating (which will be part of
the algorithmic decisions toward solving the underlying optimization problem).
As we want to keep robots capabilities down to minimal, we also assume that
robots may only perform random walks over the assigned subset of boxes.
1.1
Search Model
We describe in the sequel the essential aspects of our search model involving the
domain being searched, the searchers, and the target.
There are n nodes (boxes) labeled {1, . . . , n} = [n] and a treasure is hid-
den uniformly at random at one of them. These labels uniquely identify the
nodes which can be distinguished as such by the robots. We are concerned with
querying nodes using robots. There are k identical robots and each robot j is
assigned a subset of the nodes Aj, called query domain, and this subset will
usually be speciﬁed by the search algorithm. Note that the robots are allowed
to share (i.e., search) the same query domain or part thereof. The agents query
the nodes (as speciﬁed by their query domain) synchronously, independently and
uniformly at random. For this, we assume that robots have identical clocks, and
computation proceeds in rounds. Each round of querying consists of three steps:
a robot (a) visits a node, (b) queries the node for the treasure, and (c) jumps
to a new node. All that counts as one unit. Robots are also allowed to query
the same node simultaneously at one unit time. Success is accomplished when

86
H. Chuangpishit et al.
one robot ﬁnds the treasure. Throughout this paper we use interchangeably the
terms autonomous agents, robots, and searchers.
We distinguish three basic types of query strategies depending on the allowed
intersection of the domain of nodes being queried by the robots.
– Unanimous query: A query strategy in which the query domain of all the
agents consists of the entire set [n] of nodes.
– Partition query: A query strategy in which every two query domains either
coincide or are disjoint, and therefore the maximal collection of pairwise dis-
joint domains forms a partition of the set [n]. We further distinguish two
more subtypes. When all domains are pairwise disjoint, we call the strat-
egy pure-partition query. When all domains are of the same size, say n/t,
and the same number of robots (k/t) perform in each domain, we call the
strategy t-uniform. We refer to the collection of all t-uniform strategies as
uniform-partition.
– Overlap query: A query strategy in which the union of the query domains of all
the agents contains the entire set [n] of nodes and query domains are allowed
to have non empty intersection, (i.e. domain intersections and symmetric
diﬀerences may be non-empty).
Note that the condition ∪k
i=1Ai = [n] only guarantees that the expected ter-
mination time is ﬁnite. We also adopt a model in which there is uncertainty not
only in locating but also in identifying the target. Depending on the algorithm
being considered, searches at a node may be executed by alternate robots. As
such, robots may have to query a given node repeatedly so as to ﬁnd (or iden-
tify) the target, note however that success is assured (at an unknown number
of steps) only if the target is located at the node being queried. This gives rise
to the probability of success at node i: Given that the treasure is at node i, the
probability of success is denoted by pi and it is equal to the probability that a
robot successfully ﬁnds the treasure at the node i. The probabilities pi are given
as input to our problem. Moreover, the expected search time is the expected
number of queries the robots perform until the treasure is found by any of the
robots.
An interesting feature of our model is the uncertainty inherent in the fact
that a query at a node, say i, may fail not only because the treasure is not located
at node i but also because the probability that a query succeeds is only a certain
probability pi > 0 which depends on the node i. It follows that a robot cannot
obtain any advantage in locating the treasure only by knowing the label of a
node unless it can possibly have additional knowledge of the relative magnitude
of the probability of success pi at node i with respect to the probabilities pj,
for all other nodes j ̸= i. Therefore a natural search approach is for the robots
to execute random walks in which this type of knowledge may be exploited; for
additional details see Sects. 3 and 4.
On input p1, . . . , pn, any feasible solution to our search problem will be deter-
mined by the query domains A1, . . . , Ak (also referred to as the search strategy)
for the k robots, either in the unanimous, or the partition or the overlap vari-
ation of the problem. Finally, an optimal solution to our search problem is a
www.ebook3000.com

Querying with Uncertainty
87
search strategy A for querying n nodes with k robots such that the expected
time until the treasure is found (for the ﬁrst time) is minimized. Note that in
our model the robots incur costs only for querying (with each query costing one
unit); thus there is no cost when hopping from node to node.
1.2
Related Work
Search involves ﬁnding a target which is placed at an unknown location of a
search domain while searchers (autonomous mobile robots) can move with cer-
tain maximum speed within the domain. One usually wants to minimize the
search time required so that a robot ﬁnds the target. Searching has been studied
extensively both in graph theoretic [15] as well as geometric settings [13].
Various search strategies have been studied involving static or moving tar-
gets, in discrete and continuous environments with or without knowledge of the
location of one or several targets. A large number of such problems under various
models has been considered and analyzed in the seminal book [2]. An important
variant of the search problem is when searching for a target in an inﬁnite line
(known as linear search) [5]. Linear search is also known as a single-lane cow-
path problem, as opposed to the cow-path problem where the target may be
located in one of many possible paths. Optimal randomized algorithms for the
cow-path problem can be found in [18,19]. Additional stochastic and game the-
oretic approaches to the search problem can be found in [3,7] as well as in the
seminal papers of Beck [6] and Bellman [8] (and subsequent papers thereof) in
which the authors attempted to minimize the competitive ratio in a stochastic
setting.
A new line of research is emerging in which searchers may cooperate by
exchanging messages and variants of linear search are being studied in a dis-
tributed setting. Two communication models that have been considered so far
include wireless and face-to-face (F2F). In the former, the robots can commu-
nicate anywhere and anytime regardless of their distance, while in the latter
they can communicate only if they are at the same location, at the same time.
For example, [10] considered evacuation (this is linear group search, when the
process is completed when the target is reached by the last robot visiting it) in
the F2F communication model and proved that the competitive ratio of search
is still bounded from below by 9. More interestingly, linear search for a team of
cooperating robots where some fraction of the robots may exhibit either crash
faults or where some robots may exhibit Byzantine faults have recently been
studied in [12] and [11], respectively.
Most related to our current paper involves studies on searching with uncer-
tainty. This type of search usually involves making decisions under an adversary
(usually random) who is not under the searchers’ control; this has been studied in
various search domains and under various memory requirements for the robots.
For example, in [20] the location of the target in the network is unknown, but
information about its whereabouts can be obtained by querying the nodes (of
the network), while [17] investigates memoryless search algorithms in a network

88
H. Chuangpishit et al.
with faulty advice. In [16] the authors study the problem of ﬁnding a destina-
tion node by a mobile agent in an unreliable network having the structure of an
unweighted graph in which nodes of the network are able to give advice. Fur-
ther, [1] studies nearest-neighbor queries in a probabilistic framework in which
the location of each input node is speciﬁed as a probability distribution function.
Finally, [14] studies memory lower bounds for randomized collaborative search
and implications for biology (in the context of the Ants Nearby Treasure Search).
1.3
Outline and Results of the Paper
In Sect. 2 we are concerned with robots executing unanimous queries; (a) we
prove an estimate on the number of queries to succeed with high probability
(Theorem 1), and (b) prove a closed formula for the expected number of steps
until the treasure is found (Theorem 2). This formula will prove important in
our later analysis which involves partition queries in Sect. 3 whereby the search
domains assigned to the robots form a partition of the entire domain. In Sub-
sect. 3.1 we further restrict our attention on uniform-partition queries. The main
results of this section are as follows; (c) a monotonicity result regarding uniform
query strategies that indicates a divide-and-conquer phenomenon according to
which, at a high level, it is more beneﬁcial to partition the collection of nodes
to as small search domains as possible (Theorem 3). The latter implies also a
sublinear algorithm for determining the optimal uniform query strategy (Corol-
lary 1), and an interesting structural property of optimal solutions to optimal
partition query strategies (Corollary 2). Then in Subsect. 3.2 we study pure-
partition query strategies, and we show that (d) optimal pure-partition strate-
gies of n nodes with k robots can be obtained eﬃciently for every constant k
(Theorem 4). As a corollary, we also show that (e) optimal partition strategies of
even many nodes and 2 robots can be obtained in polynomial time (Theorem 6).
In Sect. 4 we allow the robots to perform queries with overlapping domains, and
we show (f) an optimal overlap query strategy of n nodes and 2 robots (Theo-
rem 7). In Sect. 5 we brieﬂy comment on the challenges toward generalizing our
results. Finally, in Sect. 6 we provide the conclusion and discuss possible open
problems for further research.
2
Unanimous Querying
In this section we are concerned with robots executing unanimous queries. First,
we determine in Theorem 1 the number of such queries which are needed so that
the treasure will be found by anyone of the k robots with high probability while
in Theorem 2 we ﬁnd an exact formula for the expected number of steps until
the treasure is found.
Lemma 1. Consider the problem of querying n nodes with k robots. Let r be
a positive integer. If k robots perform unanimous querying for a time at least
(r −1 + 3n ln n)/k then every node will be queried at least r times, with high
probability, i.e. at least 1 −1
n.
www.ebook3000.com

Querying with Uncertainty
89
Proof (Lemma 1). Deﬁne m := ⌈(r −1 + 3n ln n)/k⌉and consider the ﬁrst m
steps of k synchronous robots, performing independent, random queries of the
nodes. Let Ei be the event that node i has been queried less than r times.
Deﬁne the event E that at the end of the m-th step of the k querying robots,
some node i, where 1 ≤i ≤n, has been queried less than r times. We estimate
the probability of the event E. Using the union bound, we obtain that Pr[E] =
Pr [n
i=1 Ei] ≤n
i=1 Pr[Ei]. Consider the event Ei. Observe that k robots in
m steps query the nodes at least mk times (not necessarily diﬀerent nodes),
where r −1 + 3n ln n ≤mk ≤r −1 + 3n ln n + k. Let q := 1 −1/n. The
probability that a given node i has been queried less than r times in mk queries
of the robots is equal to r−1
j=0 qmk−j(1 −q)j = qmk−r+1 r−1
j=0 qr−1−j(1 −q)j ≤
qmk−r+1 r−1
j=0 qr−1−j = qmk−r+1 1−qr
1−q . It follows that
Pr[Ei] ≤

1 −1
n
mk−r+1 1 −qr
1 −q ≤n

1 −1
n
mk−r+1
≤n

1 −1
n
3n ln n
since mk −r + 1 ≥3n ln n. Thus, Pr[E] ≤n2 
1 −1
n
3n ln n ≤1
n, for every n ≥1.
As a consequence, we conclude that k robots querying the n nodes independently,
synchronously and randomly m times will query every node at least r times, with
high probability (≥1 −1
n).
⊓⊔
Using Lemma 1 we can prove the following theorem concerning the search
time by k robots to ﬁnd the treasure.
Theorem 1. Consider the problem of querying n nodes with k robots. If k robots
perform a unanimous query at least
1
k

3n ln n −1 + max
i=1,...,n

ln n
−ln(1 −pi)
	
(1)
times then the treasure will be found by one of the robots with high probability,
i.e., at least 1 −1
n.
Proof (Theorem 1). Let Ni be the random variable that counts the number of
queries that the k robots make to node i before the treasure is found, where
1 ≤i ≤k. As a consequence of Lemma 1 we obtain that if k robots query m
times and mk ≥r −1 + ⌈3n ln n⌉then Pr[∀i(Ni ≥r)] ≥1 −1
n.
Let the treasure be placed at a node uniformly at random with probability
1/n. If the treasure is located at node i, then the probability that a query
succeeds is pi. The probability that a robot ﬁnds the treasure in at most r queries
is r−1
j=0 pi(1 −pi)j = 1 −(1 −pi)r. To ensure that the treasure is found at node
i with high probability, we require that 1 −(1 −pi)r ≥1 −1/n. This means that
we must guarantee that r ≥
ln n
−ln(1−pi). By selecting r ≥maxi=1,...,n

ln n
−ln(1−pi)

we can ensure that the treasure will be found with high probability regardless
of where it is placed.
⊓⊔
Next we compute the expected search time for the treasure to be found by a
robot. The resulting formula will prove vital to our subsequent investigations.

90
H. Chuangpishit et al.
Theorem 2. Assume that k robots perform a unanimous query on n nodes.
Then the expected number of steps until the treasure is found is given by the
formula
1
n
n

i=1
1
1 −(1 −pi/n)k ,
(2)
where for each node i, pi is the probability that a query (by a robot) at node i
will succeed to ﬁnd the treasure.
Proof (Theorem 2). Let Ei denote the event that the treasure is located at node
i. It is clear that Pr[Ei] = 1/n. Let the random variable Tl count the number
of steps until the l-th robot ﬁnds the treasure and deﬁne T := minl=1,...,k Tl to
be the number of steps until the ﬁrst robot (among the k) ﬁnds the treasure.
Observe that E[T] = 1
n
n
i=1 E[T|Ei]. Also, observe that Pr[Tl > s|Ei] is equal
to the probability that the l-th robot does not ﬁnd treasure after s attempts,
given that the treasure is at node i. Therefore we have that
Pr[Tl > s|Ei] = Pr[l-th robot does not ﬁnd treasure after s attempts|Ei]
=
s

j=0
Pr[l-th robot has visited treasure j times unsuccessfully|Ei]
=
s

j=0

s
j
 1 −pi
n
j 
1 −1
n
s−j
=

1 −1
n
s
s

j=0

s
j
 1 −pi
n −1
j
=

1 −1
n
s 
1 + 1 −pi
n −1
s
= (1 −pi/n)s .
It is easy to see that in the summation above, the visitation to the treasure
can be realized in
s
j

diﬀerent ways, and each of these j times happens and is
unsuccessful with probability (1−pi)/n, while for the remaining of the steps you
do not visit the treasure, that happens with probability (1 −1/n).
E[T|Ei] =

s≥0
Pr[T > s|Ei] =

s≥0
Pr[min{Tl : 1 ≤l ≤k} > s|Ei]
=

s≥0
(Pr[T1 > s|Ei])k =

s≥0
(1 −pi/n)sk =
1
1 −(1 −pi/n)k ,
where the last equation follows from the fact that the random variables Tl (1 ≤
l ≤k) are independent and identical. The formula resulting after combining the
identities above is exactly as claimed in (2).
⊓⊔
3
Partition Querying
In this section we study partition solutions for the problem of querying a set of n
nodes with k robots. Notably, even when k is a constant, there are exponentially
many (in n) possible uniform or pure-partition search strategies. Nevertheless, we
show in this section that the best uniform partition and the best pure-partition
strategies can both be computed eﬃciently (the latter, only when k is a constant).
www.ebook3000.com

Querying with Uncertainty
91
3.1
Uniform-Partition Querying
Recall that in a partition query, the domain [n] is partitioned into a number,
say t, of sets S1, . . . , St and Si is the query domain of ki many robots such
that k1 + · · · + kt = k. Whenever t|k and t|n, in every t-partition query, the
query domains are of size n/t, and exactly k/t robots search in each domain. We
show in the next theorem that the expected search time in a t-uniform query is
independent of the partitioning, for any ﬁxed t.
Theorem 3. The expected search time of querying n nodes with k robots using
any t-uniform query strategy is
Et[T] = 1
n
n

i=1
1
1 −(1 −tpi/n)
k
t .
(3)
Moreover, Et[T] is strictly decreasing with respect to t.
Proof (Theorem 3). Consider a t−uniform querying of the n nodes induced by
the partition S1, . . . , St. recall that |Si| = n
t , for i = 1, . . . , t. Given that the
treasure is in Si, and by Formula (2), the expected termination time is E[Ti] =
t
n

r∈Si
1
1−(1−tpr/n)
k
t . The treasure is uniformly placed at one of the nodes, and
thus the probability that the treasure is in Si is 1
t . Therefore
Et[T] =
t

i=1
1
t E[Ti] =
t

i=1
1
t
t
n

r∈Si
1
1 −(1 −tpr
n )
k
t = 1
n
n

r=1
1
1 −(1 −tpr
n )
k
t ,
where the last equality follows since the Si’s form a partition of [n].
To prove the monotonicity of Et[T] we show that each of the summands
is decreasing in t, where 1 ≤t ≤n. Note that each of the summands
is a function of the form f(t) :=
1
1−(1−at)b/t , where a, b are positive con-
stants (depended on the index of each summand), independent of t. We have
df(t)
dt
=
(1−at)b/t(−b log(1−at)
t2
−
ab
t(1−at))
(1−(1−at)b/t)
2
. For the i-th summand we have a = pi/n,
and since 1 ≤t ≤n, it follows that (1 −at)b/t above is positive. Hence, the sign
of df(t)
dt
is fully determined by −log(1−at)
t
−
a
1−at, which we show shortly that is
negative. Indeed, consider function g(x) = ln(1 −x) +
x
1−x
for
x ∈(0, 1). The
derivative of g′(x) = −
1
1−x +
1
(1−x)2 , which is positive since 0 < x < 1. Therefore
for all x ∈(0, 1), we have g(x) > limx→0 g(x) = 0. For the i-th summand we
have a = pi/n, and since 1 ≤t ≤n we see that ax ∈(0, 1). But then g(ax) > 0,
which is equivalent to the assertion −log(1−at)
t
−
a
1−at < 0, as promised.
⊓⊔
There are two immediate implications of Theorem 3.
Corollary 1. The optimal uniform querying strategy of n nodes with k-robots
is a t-uniform querying strategy where t = gcd(n, k). Hence, the optimal uni-
form querying strategy can be found in time O(log n), and the optimal expected
querying time is given by (3).

92
H. Chuangpishit et al.
Corollary 2. Consider an optimal partition querying strategy of n nodes with k
robots, where the search domains are S1, . . . , St, and k1, . . . , kt many robots are
assigned to each domain, respectively. Then for each i = 1, . . . , t, we have that
ki does not divide |Si|, unless ki = 1.
3.2
Pure-Partition Querying
For any ﬁxed k, we show in this section how to ﬁnd the optimal pure-partition
query strategy of n nodes and k robots. Consider some pure-partition strategy
{Si}i=1,...,k. Let the random variable X count the number of steps until the
treasure is found. Further, let Bi denote the event that the treasure is in one
of the vertices of some set Si, and so Pr[Bi] = |Si|
n . Using Theorem 3 and since
E[X] = k
i=1 Pr[Bi]E[X|Bi], the best pure-partition strategy is the one with
query domains S1, . . . , Sk that minimizes
E[X] =
k

i=1
|Si|
n

j∈Si
1
pj
(4)
In the language of [9], we just showed that determining the optimal pure-
partition query is a set partition problem with an additive objective (SPAO). In
such problems a set [n] has to be partitioned into k (ﬁxed) many sets Si so as
to minimize the expression k
i=1 g(Si), given some cost-function g : 2n →R.
A special family of cost-functions g will play a critical role in our arguments.
Suppose that the elements in [n] are each associated with real numbers ri, and
assume w.l.o.g. that r1 < r2 < . . . < rn.
Deﬁnition 1. Cost-function g : 2n →R is called concave in the subset sum for
ﬁxed cardinality of the subset if there are concave functions f1, . . . , fn such that
for any B ⊂A with |B| = i we have g(B) = fi

j∈B rj

.
Clearly, by Equation (4), we see that determining the optimal pure-partition
query strategy is a SPAO with a cost-function that is concave in the subset sum
for ﬁxed cardinality of the subset. In fact, our functions are linear in the sum of
rj = 1/pj. We invoke the following known result.
Theorem 4 [9]. In every SPAO with cost-function that is concave in the subset
sum for ﬁxed cardinality of the subset, there is an optimal partition S1, . . . , Sk
in which the elements of each Si are consecutive integers.
We are now ready to present an eﬃcient algorithm for determining optimal
pure-partition strategies.
Theorem 5. Consider the problem of querying n nodes with k robots. An opti-
mal pure-partition strategy S1, . . . , Sk can be found in time nO(k), and the optimal
expected querying time is given by Eq. (4).
www.ebook3000.com

Querying with Uncertainty
93
Proof. By Eq. (4), determining the optimal pure-partition query is a SPAO with
a cost-function that is concave in the subset sum for ﬁxed cardinality of the
subset. Relabel nodes [n] so that p1 ≥p2 ≥· · · ≥pn. According to Theorem 5,
there is an optimal partition S1, . . . , Sk in which each Si ⊆[n] is composed of
consecutive integers. Such “consecutive” partitions are as many as the number
of positive solutions to the linear Diophantine equation x1 + · · · + xk = n, which
is
n−1
k−1

. By exhaustively checking them all, we can ﬁnd the one minimizing the
Formula in (4). Note that once we ﬁx the partition, we can arbitrarily assign
each domain to any of the robots.
⊓⊔
As an immediate corollary, we see how to solve the general partition query
problem of even many nodes and 2 robots.
Theorem 6. Consider the problem of querying 2n nodes with 2 robots. An opti-
mal partition strategy S1, S2 can be found in time O(n2), and the optimal expected
querying time is given by Eq. (4).
Proof. In partition querying, sets S1, S2 either coincide (unanimous querying) or
they form a partition of [2n]. By Corollary 1, the 2-uniform querying strategy is
better than the unanimous. But the 2-uniform querying strategy is just a pure-
partition strategy, and using Theorem 5 we can determine the best among them
in time O(n2).
4
Overlap Querying with 2 Robots
In this section we consider the problem of querying n nodes with two robots
using overlap query strategies and we show the following result.
Theorem 7. An optimal overlap query strategy of n nodes with 2 robots can be
found in O(n2) steps.
Assume that nodes [n] are relabeled so that p1 ≤p2 ≤. . . ≤pn. The main
idea behind the proof of Theorem 7 is to show that there is always an optimal
strategy S1, S2 with two special properties; (a) either S1, S2 are disjoint or S1 ∩
S2 = {1, . . . , s}, for some 1 ≤s ≤n (Lemma 2) and (b) any nodes explored by
any of the robots but not the other have consecutive indices (Lemma 3).
Lemma 2. Consider an optimal overlap query S1, S2 of n nodes with two robots.
If s ∈S1 ∩S2, then {1, . . . , s} ⊆S1 ∩S2.
Proof (Lemma 2). Suppose that S1, S2 is an optimal overlapping query with 2
robots, where n1 = |S1| and n2 = |S2|. Then the expected search time is
E[T] = 1
n
 
i∈S1
E[T|i] +

i∈S2
E[T|i] +

i∈S1∩S2
E[T|i]

,
(5)

94
H. Chuangpishit et al.
where E[T|i] is the expected search time given the treasure is at the node i. For
nodes i ∈(S1 ∪S2) \ (S1 ∩S2) only one robot queries the node i. Therefore for
i ∈Sj \ (S1 ∩S2), j ∈{1, 2}, we have
E[T|i] =
1
1 −(1 −pi
nj ).
(6)
The nodes i ∈S1 ∩S2 are queried by both robots, and so for the nodes inside
the overlap we have E[T|i] =
1
1−(1−pi
n1 )(1−pi
n2 ).
Clearly, if S1 = S2 or if S1 ∩S2 = ∅, the statement of the Lemma is true.
So, we may assume that S2 \ S1 ̸= ∅and that S1, S2 are not disjoint. Our goal
will be to compare two query strategies which diﬀer only in nodes x, y for which
px ≤py. More speciﬁcally, we show that if y ∈S1 ∩S2 and x ∈S2 \ S1, then by
switching x, y, the expected termination time does not increase.
So let Ey[T] be the expected search time when y ∈S1 ∩S2 and x ∈S2 \ S1,
and Ex[t] be the expected search time when x ∈S1 ∩S2 and y ∈S2 \ S1. Then
Ex[T] −Ey[T] = 1
n

1
1 −(1 −py
n2 ) +
1
1 −(1 −px
n1 )(1 −px
n2 )
−
1
1 −(1 −px
n2 ) −
1
(1 −py
n1 )(1 −py
n2 )

Consider the function f(t) =
1
1−t −
1
1−rt, where 0 < r, t < 1. Then f ′(t) =
1
(1−t)2 −
r
(1−rt)2 . Since 1 −t < 1 −rt we conclude that f ′(t) > 0 and thus f(t) is
an increasing function. Now note that 0 <

1 −pi
nj

< 1 for 1 ≤i ≤n and 1 ≤
j ≤2. This together with the fact that f(t) is an increasing function and px ≤py
implies that Ex[T] −Ey[T] ≤0.
⊓⊔
Lemma 3. Consider the problem of querying n nodes with two robots. Let the
nodes 1, . . . , n be labeled such that p1 < · · · < pn. Suppose that S1, S2 is an
optimal solution among overlapping and partitioning queries. Then S1 ∩S2, S1 \
(S1 ∩S2) and S2 \ (S1 ∩S2) are consecutive.
Proof (Lemma 3). If the optimal solution is a partition solution, i.e. S1 ∩S2 = ∅
then, by Theorem 4, we know that S1 and S2 are consecutive. So suppose that the
optimal solution is an overlapping solution i.e. S1∩S2 ̸= ∅. Then by Lemma 2 we
know that the nodes inside the overlap, i.e. S1∩S2, are the nodes with the smaller
probability of success. Therefore the nodes inside the overlap are consecutive
nodes. The formula for the expected search time of overlapping strategy is given
by (5).
Consider the nodes outside of the overlapping set i.e. B = (S1∪S2)\(S1∩S2).
We want to ﬁnd a partitioning query of B with two robots whose expected search
time is minimum. If in an optimal solution, one of the sets S1 \ (S1 ∩S2) and
S2 \ (S1 ∩S2) is empty we are done. Otherwise, the optimal solution is obtained
by partitioning the nodes [n] \ (S1 ∩S2) into two non-empty sets, and assigning
each of them to some robot.
www.ebook3000.com

Querying with Uncertainty
95
We think of the following artiﬁcial optimization problem. Assuming that
robots will share some ﬁxed collection of nodes [n] \ B, partition the remaining
nodes B so as to minimize the expected termination time. Conditioning on that
the treasure is in B, the contribution to the cost is the same as if the two robots
were performing pure-partition strategies, where the size of each of their domains
is still |S1| and |S2| see (5) and (6). In particular, this artiﬁcial optimization
problem is still a SPAO with a cost-function that is concave in the subset sum for
ﬁxed cardinality of the subset. So by Theorem 4, there is an optimal partitioning
B into two consecutive sets minimizing the expected search time.
⊓⊔
We are now ready to prove Theorem 7.
Proof (Theorem 7). Let S1, S2 presents an optimal query among partitioning
and overlapping solutions. Suppose that the nodes 1, . . . , n are ordered such
that p1 < · · · < pn. By Lemma 3 we know that S1 ∩S2, S1 \ (S1 ∩S2) and
S2 \ (S1 ∩S2) are consecutive sets. Therefore there are
n+1
2

ways to choose
S1 ∩S2, S1 \ (S1 ∩S2) and S2 \ (S1 ∩S2). By exhaustively checking all of them,
we can determine the optimal solution in O(n2) many steps.
5
Brief Discussion on Generalizations
Notably, Corollary 2 leaves open the possibility that either unanimous or pure-
partition strategies are optimal strategies for the partition query problem.
Indeed, when all n = 3 nodes are associated with the same probability p, then
the expected termination time of unanimous querying is, by Theorem 3, equal
to
1
1−(1−p/3)2 . On the other hand, the other possible partition strategy in which
a robot searches two nodes, and the other robot searches the remaining node
has cost
5
3p (see (4)). Which of the two is better is dependent on the value of p,
indicating that the structure of optimal solutions in partition querying might be
in general challenging to determine.
Some of our results of Sect. 4 also generalize to querying with 3 (or more)
robots. We can show that in an optimal solution, the nodes (if any) that are
searched by all robots form a set of consecutive integers, and again this set
should include the nodes with the smallest probabilities pi. However, it is not
true necessarily that indices of nodes searched by r robots are smaller than
indices of nodes searched by l robots, whenever r > l. This is another indication
that optimal solutions to overlap querying might be challenging to obtain.
6
Conclusion
In this paper we introduced a new search problem about querying with uncer-
tainty; k robots try to locate a hidden treasure that is placed uniformly at
random in one of n locations, each associated with a probability pi, i = 1, . . . , n.
A query by a robot in location i may succeed only if the treasure is located in
location i, in which case the treasure will be revealed with probability pi.

96
H. Chuangpishit et al.
Based on a new closed formula which we proved for the expected number
of steps until the treasure is found by a robot in unanimous querying, we were
also able to analyze more complex querying such as partition and overlapping
and establish the expected number of steps until the treasure is found by a
robot in these cases as well. We analyzed in detail querying by two robots, but
the general case of overlapping queries with multiple robots still remains open.
Further, throughout our analysis we made the assumption that the underlying
graph is the complete graph Kn and there was no cost associated either with
the movement of the robots or the underlying graph. An interesting variant of
our model is considering other graphs, e.g., lines, rings, trees, as well as when
the robots incur costs in their movement from node to node, e.g., when there is
an underlying, possibly either edge or vertex weighted graph.
References
1. Agarwal, P.K., Aronov, B., Har-Peled, S., Phillips, J.M., Yi, K., Zhang, W.: Near-
est neighbor searching under uncertainty ii. In: Proceedings of the 32nd ACM
SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, pp.
115–126. ACM (2013)
2. Ahlswede, R., Wegener, I.: Search Problems. Wiley-Interscience, Hoboken (1987)
3. Alpern, S., Gal, S.: The Theory of Search Games and Rendezvous, vol. 55. Kluwer
Academic Publishers, Dordrecht (2002)
4. Alpern, S., Gal, S.: The Theory of Search Games and Rendezvous. Springer, Hei-
delberg (2003). https://doi.org/10.1007/b100809
5. Baeza Yates, R., Culberson, J., Rawlins, G.: Searching in the plane. Inf. Comput.
106(2), 234–252 (1993)
6. Beck, A.: On the linear search problem. Isr. J. Math. 2(4), 221–228 (1964)
7. Beck, A., Warren, P.: The return of the linear search problem. Isr. J. Math. 14(2),
169–183 (1973)
8. Bellman, R.: An optimal search. SIAM Rev. 5(3), 274 (1963)
9. Chakravarty, A.K., Orlin, J.B., Rothblum, U.G.: Technical note–a partitioning
problem with additive objective with an application to optimal inventory groupings
for joint replenishment. Oper. Res. 30(5), 1018–1022 (1982)
10. Chrobak, M., G asieniec, L., Gorry, T., Martin, R.: Group search on the line. In: Ital-
iano, G.F., Margaria-Steﬀen, T., Pokorn´y, J., Quisquater, J.-J., Wattenhofer, R.
(eds.) SOFSEM 2015. LNCS, vol. 8939, pp. 164–176. Springer, Heidelberg (2015).
https://doi.org/10.1007/978-3-662-46078-8 14
11. Czyzowicz, J., Georgiou, K., Kranakis, E., Krizanc, D., Narayanan, L., Opatrny,
J., Shende, S.: Search on a line with byzantine robots. In: ISAAC. LIPCS (2016)
12. Czyzowicz, J., Kranakis, E., Krizanc, D., Narayanan, L., Opatrny, J.: Search on a
line with faulty robots. In: Proceedings of the 2016 ACM Symposium on Principles
of Distributed Computing, pp. 405–414. ACM (2016)
13. Deng, X., Kameda, T., Papadimitriou, C.: How to learn an unknown environment.
In: FOCS, pp. 298–303. IEEE (1991)
14. Feinerman, O., Korman, A.: Memory lower bounds for randomized collaborative
search and implications for biology. In: Aguilera, M.K. (ed.) DISC 2012. LNCS,
vol. 7611, pp. 61–75. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-
642-33651-5 5
www.ebook3000.com

Querying with Uncertainty
97
15. Fomin, F.V., Thilikos, D.M.: An annotated bibliography on guaranteed graph
searching. Theoret. Comput. Sci. 399(3), 236–245 (2008)
16. Hanusse, N., Ilcinkas, D., Kosowski, A., Nisse, N.: Locating a target with an agent
guided by unreliable local advice: how to beat the random walk when you have a
clock? In: Proceedings of the 29th ACM SIGACT-SIGOPS Symposium on Princi-
ples of Distributed Computing, pp. 355–364. ACM (2010)
17. Hanusse, N., Kavvadias, D.J., Kranakis, E., Krizanc, D.: Memoryless search algo-
rithms in a network with faulty advice. TCS 402(2–3), 190–198 (2008)
18. Kao, M.-Y., Ma, Y., Sipser, M., Yin, Y.: Optimal constructions of hybrid algo-
rithms. J. Algorithms 29(1), 142–164 (1998)
19. Kao, M.-Y., Reif, J.H., Tate, S.R.: Searching in an unknown environment: an opti-
mal randomized algorithm for the cow-path problem. Inf. Comput. 131(1), 63–79
(1996)
20. Kranakis, E., Krizanc, D.: Searching with uncertainty. In: 6th International Collo-
quium on Structural Information & Communication Complexity, SIROCCO 1999,
Lacanau-Ocean, France, 1–3 July 1999, pp. 194–203 (1999)
21. Stone, L.: Theory of Optimal Search. Academic Press, New York (1975)

Energy-Optimal Broadcast in a Tree
with Mobile Agents
Jerzy Czyzowicz2(B), Krzysztof Diks1, Jean Moussi2, and Wojciech Rytter1
1 D´epartement d’informatique, Universit´e du Qu´ebec en Outaouais,
Gatineau, QC, Canada
{diks,rytter}@mimuw.edu.pl
2 Faculty of Mathematics, Informatics and Mechanics,
University of Warsaw, Warsaw, Poland
{jurek,Jean.Moussi}@uqo.ca
Abstract. A set of k mobile agents is deployed at the root r of a weighted,
n-node tree T. The weight of each tree edge represents the distance
between the corresponding nodes along the edge. One node of the tree, the
source s, possesses a piece of information which has to be communicated
(broadcasted) to all other nodes using mobile agents. An agent visiting a
node, which already possesses the information, automatically acquires it
and communicates it to all nodes subsequently visited by this agent. The
process ﬁnishes when the information is transferred to all nodes of the tree.
The agents spend energy proportionally to the distance traversed. The
problem considered in this paper consists in ﬁnding the minimal total
energy, used by all agents, needed to complete the broadcasting. We give
an O(n log n) time algorithm solving the problem. If the number of agents
is suﬃciently large (at least equal to the number of leaves of T), then our
approach results in an O(n) time algorithm.
When the source of information s is initially at the root r, our algorithm
solves the problem of searching the tree (exploring it) by a set of agents
using minimal energy. It is known that, even if the tree is a line, the broad-
casting problem and the search problem are NP-complete when the agents
may be initially placed at possibly many distinct arbitrary positions.
Keywords: Mobile agents · Tree · Data delivery · Broadcast · Search
1
Introduction
A packet of information available at the source node of a network must be
disseminated to all other nodes. The task needs to be performed by a collection of
mobile agents. Given a network, what is the minimal amount of energy needed for
the information to be delivered to all nodes. If the agents are initially distributed
J. Czyzowicz—Supported by NSERC grant of Canada.
K. Diks and W. Rytter—Supported by the grant NCN2014/13/B/ST6/00770 of the
Polish Science Center.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 98–113, 2017.
https://doi.org/10.1007/978-3-319-72751-6_8
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
99
along the network, this problem turns out to be NP-complete, even if the original
network is a line (see [13]). In this paper we solve the problem for tree networks,
when all agents start at the same node.
Similar problems, studied in operations research (particularly in vehicle rout-
ing), sometimes assume limited capacities of robots and quantities of product
to be transported. In our case, the amount of product to be carried by a robot
is irrelevant. Consequently, similarly to [13], we categorize our problem as data
delivery or, more exactly as broadcasting using mobile agents.
Moreover, in the special case when the data source and the starting position
of the robots are the same, our solution produces the optimal search schedule
- the movement of the collection of agents searching all nodes of the tree using
minimal total energy. It is a folklore knowledge that the same problem when the
time of the schedule is to be optimized (i.e. the time of arrival to its destination
of the last robot), the problem is NP-complete.
1.1
Preliminaries and the Problem Statement
A set of k mobile agents is placed at the root r of an edge-weighted tree T,
where edge weight represents distance between edge endpoints. One node of
the tree, that we call source node, initially possesses a packet of information,
which eventually needs to reach every other node of the tree. The information is
transported by mobile agents, which use energy proportionally to the distance
travelled. We conservatively assume, that if an agent previously acquired the
source information packet, then if such agent is visiting a node, it implicitly
leaves a copy of the packet at that node. Any agent later visiting such node
automatically copies the packet to its memory and it may then distribute it to
other, subsequently visited nodes. We consider the following problem:
Data broadcasting:
Let T be an edge-weighted tree with two distinguished nodes s, r. The node
s is a source node and r is the initial location of k mobile agents. The source
node possesses a packet of information, which needs to be transported to all
other nodes by mobile agents.
What is the minimal amount of energy, denoted by MinCost(T, k), that
the agents need for their travel so that the source packet is successfully
distributed?
By a (global) schedule we mean a set of functions f1, f2, . . . , fk, such that
fi(t) is a position at time t of agent i in the tree. The agent i knows the data
packet at time t∗, if fi(t) = s for some t ≤t∗, or fi(t1) = v, fj(t2) = v and
t2 ≤t1 ≤t∗for some agent j, which knew the packet at time t2.
We call a schedule optimal if it results in the smallest possible usage of energy
by mobile agents.

100
J. Czyzowicz et al.
1.2
Our Results
We present almost linear-time, greedy algorithms solving data broadcasting.
Our problem can be solved in O(n log n) time, independently of the number k of
available agents. This complexity is reduced to O(n) time in case of unlimited
number of agents, or when the number of agents is at least equal to the number
of leaves of T.
In the special case when the root, from which all agents start, is also
the source node, our approach solves the search problem, when the collec-
tion of agents need to search the tree optimally, i.e. using the smallest total
energy. Surprisingly, according to our knowledge, this natural setting of the
search/exploration problem has not been studied before.
The missing proofs will appear in the full version of the paper.
1.3
Related Work
Recent development of the network technology fuelled the research in mobile
agents computing. Several applications involve physical mobile devices, software
agents, migrating in a network, or living beings: humans (e.g. soldiers or disas-
ter relief personnel) or animals (e.g. ants). Most important questions for mobile
agents concern environment search or exploration (cf. [3,9,15–17]). Some ques-
tions involving mobile agents are related to problems from operations research,
especially vehicle routing (e.g., see [19]).
Searching and exploration have been extensively investigated in numerous
settings. Using collections of mobile agents for the tree environment, the previous
papers attempted to optimize the usage of various resources, e.g., the time for
exploration (the maximal time used among all agents) [6,16], memory used by
agents [4], the number of agents [15], etc. Many papers assumed no knowledge of
the tree, which leads to distribute algorithms optimizing the competitive ratio
(e.g. [15,17]). In the centralized setting, the minimization of maximal time used
by mobile agents is an NP-hard problem, even if the tree is a star and the
collection contains only two robots (cf. [6,16]).
The task of broadcast is useful, e.g., when a designated leader needs to share
its information with collaborating agents in order to perform together some
future tasks. The broadcast problem for stationary processors has been exten-
sively studied both in the case of the message passing model, (e.g. [7]), and for
the wireless model, (see [10]).
The question of energy awareness has been investigated in diﬀerent contexts.
Paper [2] studied power management of (not necessarily mobile) devices. Several
methods have been proposed to reduce energy consumption of computer systems
including power-down strategies (see [2,5,18]) or speed scaling (cf. [20]). How-
ever, most research related to energy eﬃciency attempts to optimize the total
power used in the entire system. When the power assignments concern the indi-
vidual system components (as is the case of our model), the related optimization
questions (e.g. see [8]) have a ﬂavour of load balancing.
The communication problem by mobile agents has been studied in [1]. The
agents of [1] perform eﬃcient convergecast and broadcast in line networks. All
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
101
agents of [1] have energy sources of the same size, allowing to travel the same
distance. However, in the case of tree networks, the problems of convergecast
and broadcast are proven to be strongly NP-complete in [1].
In the case, where agents may have diﬀerent initial energy levels, [13] investi-
gated a simpler communication problem of data delivery, when the information
has to be transmitted between two given network nodes. This problem is proven
to be NP-complete in [13] already for line networks. [11] showed that the situa-
tion is quite diﬀerent when the agents are required to return to original locations.
[11] gave a polynomial solution for data delivery in trees by returning agents.
The problem of energy-eﬃcient data delivery between given set of pairs of graph
nodes was investigated in [12]. The data delivery and convergecast for trees with
energy-exchanging agents were studied in [14], where linear-time solutions for
both problems were proposed.
2
Agents Starting from the Source Node
We start with an easier case when the root r is the same as the source node s,
which contains the initial data packet. Observe that, even if we have unlimited
number of agents in r, the problem is nontrivial. In the proposed solution, every
agent i initially takes an exact amount of energy needed to traverse some subtree
T(i) and its traversal of T(i) must be optimal. The union of all subtrees must
sum up to the entire tree T and the choice of the subtrees must minimize the
total energy needed to traverse them.
We consider separately cases of limited and unlimited number of agents. We
will show that not all the agents are always activated, i.e. in some cases making
walk too many agents would result in a suboptimal algorithm. We say that
an agent is activated if it is used for walking (consumes a non-zero amount of
energy), copies a data packet to its memory, when arriving to a node at which
a copy of data packet is present, and subsequently disseminates it to all nodes
visited afterwards.
Denote by Tv the subtree of T rooted at v.
Lemma 1. Suppose that the source node s is the same as the root r. In every
optimal broadcast algorithm, each activated agent should terminate its walk at a
leaf of T.
Proof. The proof goes by contradiction. Suppose that, in an optimal broadcast,
some agent i terminates its walk in a non-leaf node v, traversing some edge (w, v)
as the last edge of its route. Two cases are possible:
Case 1: The traversal of the last edge (w, v) does not coincide with the ﬁrst
visit of node v by agent i. In this case we can remove the traversal of the
last edge (w, v) from the route of agent i and the tree explored by agent i
remains the same. However, such shortening of the route of agent i reduces its
energy cost by weight(w, v), which contradicts the optimality of the original
traversal.

102
J. Czyzowicz et al.
Case 2: The traversal of the last edge (w, v) by agent i coincides with its ﬁrst
visit of node v. In such a case, agent i could not previously enter the subtree
Tv (otherwise this would imply the second visit of v). Consequently, as Tv
contains at least one leaf, unvisited by agent i, it must be visited by some
other agent j. However, to reach any leaf of Tv from the starting position r,
agent j must visit v on its route. As v does not need to be visited by two
diﬀerent agents, we can then again shorten the route of i by the last edge
(w, v), reducing its cost. This contradicts optimality of its original route.
⊓⊔
The subset of leaves of T, at which the activated agents of an optimal algo-
rithm terminate their paths, are called critical leaves. Each path from root r to
a critical leaf is called a critical path. The union of all critical paths forms a tree,
rooted at r, that we call the frame of the algorithm.
2.1
Scheduling Agent Movements When Critical Leaves are Known
We start with the presentation of an algorithm which designs the movements of
the agents once the set of critical leaves has been obtained. As agents possess the
information about the packet from the start, it is suﬃcient to generate the tra-
jectories of all robots, disregarding synchronization between actual movements
of diﬀerent agents.
Consider a subset L of critical leaves of tree T. Deﬁne frame(L) as the union
of all critical paths, i.e. the subtree of T induced by L and all its ancestors, see
Fig. 1. By |frame(L)| we understand the sum of weights of all edges of frame(L).
Observe that the edges T \ frame(L) form a set of subtrees rooted at the nodes
of frame(L). We call them hanging subtrees and we denote the set of hanging
subtrees by H(L).
Once we know the optimal set of critical leaves L, then an optimal schedule
is easy to construct. Below we give the algorithm ConstructSchedule, which con-
structs the optimal schedule for the given set of k agents. In fact we concentrate
later only on computing the optimal L (needed in line 1 of the algorithm Con-
structSchedule). Our main result is the computation of minimum cost in almost
linear time, which also implies computing the optimal set L.
Algorithm ConstructSchedule(k);
1. Compute the set of critical leaves L, such that |L| ≤k, which maximizes Δ(L).
2. Assign to every critical leaf li a diﬀerent agent i which will terminate its walk at li.
3. Assign arbitrarily each subtree T ′ ∈H(L) to a single critical leaf L(T ′),
such that T ′ has the root on the critical path from r to L(T ′).
4. for each leaf li ∈L do
4.1. Agent i follows the critical path from r to the critical leaf li,
4.2. On the way to its assigned critical leaf li the agent i makes a full DFS
traversal of each hanging subtree T ′ ∈H(L) such that L(T ′) = li.
Observe that the total number of edge traversals, generated by the algorithm
ConstructSchedule, can be quadratic.
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
103
Denote by path(u, v) the set of nodes on the simple path between u and v
(including u, v) and let |path(u, w)| denote the distance (sum of edge weights)
from node u to w in tree T. We denote also depth(w) = |path(r, w)|.
We deﬁne below a function Δ(L) which measures the eﬃciency of the broad-
casting algorithm having L as its critical leaves.
Δ(L) = 2|frame(L)| −

w∈L
depth(w)
(1)
The following lemma shows what is the value of MinCost(T, k) - the energy
cost of the schedule produced by the algorithm ConstructSchedule for k agents
starting at the root of tree T. The energy depends on the choice of the set of
critical leaves L. The construction of the set L minimizing the energy cost will
be discussed in the subsequent sections.
Lemma 2. Assume k agents are placed initially in the source node r = s of T.
Then
MinCost(T, k) = 2|E| −Δ(L),
where L is a subset of leaves maximizing Δ(L) over |L| ≤k.
Proof. The algorithm has enough agents, so that to every critical leaf li cor-
responds a diﬀerent agent i, which terminates its walk at li. The edges of all
hanging subtrees H(L), i.e. all edges of T \ frame(L), are traversed twice in
step 4.2 of the algorithm. Moreover each edge of a critical path is traversed in
step 4.1 as many times as there are critical paths containing this edge. Con-
sequently, the total cost of such traversal of T is twice the sum of lengths of
edges belonging to T \frame(L), and the sum of the critical path lengths of the
frame(L). Hence the total cost equals
2|T|\frame(L)|+

w∈L
depth(w) = 2|E|−(2·|frame(L)|−

w∈L
depth(w)) = 2|E|−Δ(L)
(2)
By Lemma 1, each optimal algorithm using at most k agents, corresponds to
frame(L) for some L. Therefore, the cost represented in Eq. 2 is minimized for
maximal Δ(L).
⊓⊔
Consequently, the broadcasting problem reduces in this case to the compu-
tation of L which maximizes Δ(L) with |L| ≤k. The set L will be computed
incrementally and in a greedy way. We conclude this section with some obser-
vations needed for the incremental construction of the optimal set of critical
leaves.
Assume L is a set of leaves and consider a leaf w /∈L. Denote by LCA(w, L)
the lowest common ancestor of w and some leaf from L (i.e. the lowest node
belonging to path(w, r) and frame(L)). Deﬁne LCA(w, ∅) = r. Let

104
J. Czyzowicz et al.
δ(w, L) = |path(u, w)| −|path(r, u)|,
(3)
where u = LCA(w, L). Equivalently we have
δ(w, L) = depth(w) −2 · depth(LCA(w, L))
(4)
Observation 1. For L1, L2 such that L1 ⊆L2 and for any leaf w we have
δ(w, L1) ≥δ(w, L2).
Indeed the statement of the Observation 1 follows from the fact that L1 ⊆L2
implies depth(LCA(w, L1)) ≤depth(LCA(w, L2)).
Lemma 3. For a given subset of leaves L and a leaf ˜w /∈L:
Δ(L ∪{ ˜w}) = Δ(L) + δ( ˜w, L).
(5)
Proof. If we add ˜w to L, the new path between LCA( ˜w, L) and ˜w is added to
frame(L) (cf. Fig, 1). Hence, according to formula 1 we have
Δ(L ∪{ ˜w}) −Δ(L) = 2|frame(L ∪{ ˜w})| −2|frame(L)|
−

w∈(L∪{ ˜
w})
depth(w) +

w∈L
depth(w)
= 2|path(LCA( ˜w, L), ˜w)| −depth( ˜w) = depth( ˜w)
−2 · depth(LCA( ˜w, L) = δ( ˜w, L)
⊓⊔
2.2
A Schematic Algorithm Computing the Minimal Cost
The formula (5) from Lemma 3 is used to design our algorithm Schematic-
MinCost. The idea of the algorithm may be viewed as an incremental, greedy
construction of the optimal set of critical leaves L, by adding them, one by one.
At each step we have a current version of the frame, which is augmented by a
new subpath when a new leaf is added to L. Consider frame(L) obtained from
the leaves L assigned to the ﬁrst i−1 agents (that terminate their paths at i−1
leaves of L). In the i-th iteration of the main loop we try to decide what is the
best use of the next available agent. The i-th agent will terminate its traversal
at some leaf wi of T, not yet present in frame(L). Therefore, frame(L ∪{wi})
will contain some new subpath, disjoint with frame(L), starting at some vertex
of LCA(wi, L) and ending at wi. Observe, that the usage of agent i, permits
the subpath from LCA(wi, L) to wi to be traversed once (by a new agent)
rather than twice (by some other agent which would need to perform a complete
traversal of some subtree containing this path), which results in some energy
gain. However such energy beneﬁt is at the expense of bringing the agent from
the root r to LCA(wi, L). The main loop executions continue as long as such
gain is possible (i.e. beneﬁt minus expense is positive) and there are still available
agents to be used. Such beneﬁt is represented by the function δ(wi, L) and our
algorithm chooses the leaf oﬀering the largest beneﬁt. We prove later that this
greedy approach results in construction of the best possible set of critical leaves.
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
105
Algorithm Schematic-MinCost(T, k);
1.
L := ∅;
2.
while |L| ≤k and ∃( w /∈L) δ(w, L) > 0 do
3.
choose a leaf w /∈L with maximum δ(w, L);
4.
L := L ∪{w};
5.
return |2E| −Δ(L);
Example 1. Figure 1 illustrates the execution of one step of the algorithm. The
set L contains leaves w1, w2. The value of Δ(L) = 36 −19 = 17, cf. formula 1.
Among the remaining leaves, the maximal beneﬁt is obtained by including w4 in
the set of critical leaves as δ(w4, L) = 3. Then Δ({w1, w2, w4}) = 20. As for the
remaining leaves the values of δ are not positive, only three agents are activated
(even if more are available) and, by Lemma 2, the cost of the optimal algorithm
equals
2|E| −Δ({w1, w2, w4}) = 56 −20 = 36.
=⇒
1
8
2
4
3
2
3
3
2
r
w1
w2
w3
w4
w5
1
8
2
4
3
2
3
3
2
r
w1
w2
w3
w4
w5
LCA({w1, w2}, w4)
Fig. 1. The iteration which starts with a set of leaves L = {w1, w2}, then w4 is added
to L. Bold edges belong to the frames (subtrees frame(L) of paths from the root to
set L of critical leaves) before and after inclusion of w4. We have Δ({w1, w2, w4}) =
Δ({w1, w2}) + δ(w4, {w1, w2})
Observe that algorithm Schematic-MinCost is in fact non-deterministic as it
is possible that more than one leaf having the same value of δ may be chosen in
line 3. Moreover, among optimal broadcasting algorithms, it is possible that the
number of agents used may be diﬀerent. This is possible if we activate an agent
terminating at a leaf w for which δ(w, L) = 0.
Lemma 4. Assume that in the algorithm Schematic-MinCost we insert the
sequence w1, w2, . . . wm of leaves into L and there is a set L′ maximizing Δ(L′)
such that {w1, w2, . . . wt} ⊆L′, t < m. Then there exists a set L′′, also maxi-
mizing Δ(L′′), which contains {w1, w2, . . . wt, wt+1}.

106
J. Czyzowicz et al.
Sketch of the Proof. The idea of the proof is to show that the set L′ must con-
tain some leaf w∗, such that w∗̸= wi, for i = 1, 2, . . . , t + 1 and that exchanging
w∗by wt+1 in the set L” will not increase the cost of the corresponding broad-
casting algorithm.
Lemma 5.
(a) The set L computed by the algorithm Schematic-MinCost maximizes Δ(L).
(b) The value 2|E| −Δ(L), output by the algorithm Schematic-MinCost, is the
minimum amount of energy needed for broadcasting using k agents initially
placed in the source r = s.
Proof.
(a) Using inductively Lemma 4 we prove that the entire set w1, w2, . . . wm
belongs to a set of critical leaves used by an optimal algorithm. By the exit
condition of the while loop at line 2 of the algorithm Schematic-MinCost,
there is no other leaf which may be added to such critical set of leaves
improving the cost of the algorithm.
(b) This point follows directly from (a) and Lemma 2.
⊓⊔
Observe that every agent possesses the information about the packet at the
very beginning of the algorithm. Then, as observed before, once the trajectories
of each activated agent are determined, the timing of the travel of each agent is
independent of the timing of the travel of any other agent. We conclude then by
the following observation, which will be useful in the next section.
Observation 2. Any energy-optimal schedule may be designed in such a way
that the time intervals, during which agents perform their travel, are pairwise
disjoint. In particular, we can choose any agent and make this agent complete
its walk before any other agent starts walking.
2.3
Eﬃcient Implementation of Algorithm Schematic-MinCost
Eﬃciency of Schematic-MinCost depends on the cost of computing on-line the
best δ(w, L). We replace it by introducing a more eﬃcient function Gain(v)
which does not depend on L and can be computed oﬀ-line in linear time. The
algorithm Schematic-MinCost subsequently adds leaves to the set L, each time
choosing the leaf w oﬀering the largest gain, i.e. the largest reduction δ(w, L)
in the cost of the broadcasting schedule. The values of function δ for any leaf
w, which does not yet belongs to L, may change with subsequent modiﬁcations
of frame(L). In order to avoid recalculations of the function δ we propose the
following solution.
Consider the moment when the leaf w is being added to the current set L.
Let v be a child of LCA(w, L), which belongs to the path from LCA(w, L) to
w. Let maxpath(v) be the longest path starting at v (and going away from the
root). If there is more than one such path, we choose any one of them arbitrarily.
We denote by leaf(maxpath(v)) the last node on such path. Observe that, at
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
107
the moment when w is being added to L, we have |maxpath(v)| = |path(v, w)|.
For any node v ̸= r we deﬁne
Gain(v) = |maxpath(v)| + weight(parent(v), v) −|path(r, parent(v))|
(6)
By convention, we also set Gain(r) = |maxpath(r)|.
Observation 3. Assume L is a set of leaves. It follows from Eq. 3, that
max{Gain(v) : v /∈frame(L)} = max{δ(w, L) : w /∈L}
Following the above Observation, in our algorithm we will be looking for nodes
v, which are not in the current frame(L).
Algorithm MinCostLimited(T, k);
1.
X : = {v ∈V : Gain(v) > 0};
2.
Sort X with respect to Gain(v) in non-increasing order;
3.
Δ := 0 ; L := ∅;
4.
while X ̸= ∅and |L| ≤k do
5.
choose v ∈X with maximum Gain(v);
6.
Δ := Δ + Gain(v);
7.
remove from X all nodes belonging to maxpath(v);
8.
L := L ∪leaf(maxpath(v));
9.
return 2 · |E| −Δ
/* |L| equals the number of activated agents */
Theorem 1. The
algorithm
MinCostLimited(T, k)
correctly
computes
in
O(n log n) time the minimal amount of energy, which is needed to perform the
data broadcast by k agents.
Proof. We prove, by induction on the iteration of the while loop from line 4, that
the node v chosen in line 5 does not belong to the current frame(L). Indeed,
in the ﬁrst iteration of the while loop from line 4, frame(L) is empty. In every
other iteration, because of the leaf added to L in line 8, frame(L) is augmented
by the nodes of maxpath(v), but all these nodes are then removed from set X in
line 7. Therefore, in each execution of line 5 no node of X belongs to frame(L).
Consequently, by Observation 3, every value of Gain chosen in line 5 of algo-
rithm MinCostLimited is the same as the value of δ from the corresponding
iteration of line 3 of algorithm Schematic-MinCost. Moreover, the same leaf is
added to the set of critical set of leafs L in the corresponding iterations of both
algorithms. The ﬁnal critical set of leaves is then the same for both algorithms.
In the variable Δ is accumulated the sum of the values of function Gain for all
nodes chosen in all iterations of the while loop. By Observation 3, after exiting
the while loop, Δ equals the sum of values of function δ for all leafs from the

108
J. Czyzowicz et al.
ﬁnal critical set L. By Lemma 3, this sum equals Δ(L) and the ﬁnal value of the
computed cost equals 2|E| −Δ(L). By Lemma 5 this proves the correctness of
algorithm MinCostLimited.
We consider now the time eﬃciency of the algorithm. Observe ﬁrst,
that in the preprocessing, the values of Gain(v) can be computed in lin-
ear time. Recall that, by formula 6, we need to compute the values of
|maxpath(v)|, weight(parent(v), v) and |path(r, parent(v))| Observe, that all
these values may be computed using depth-ﬁrst-search traversal (DFS) of T.
Indeed weight(parent(v), v) and |path(r, parent(v))| may be obtained when DFS
enters node v from its parent. On the other hand, |maxpath(v)| is obtained when
DFS visits v for the last time (arriving from its last child).
The amortized complexity of line 7 is also linear. Assume that X is imple-
mented as a bidirectional list and each node v of the tree T contains a pointer to
the element of X corresponding to Gain(v). Then the removal operation in line 7
takes constant time for each considered node v, hence the O(n) time overall. As
each other instruction inside the while loop takes constant time, the complexity
of all lines of the algorithm, except line 2, is O(n). The overall complexity is
then dominated by the O(n log n) sorting in line 2.
⊓⊔
2.4
Unlimited Number of Agents in the Source
For a set of nodes Y denote by children(Y ) the set of all children of nodes in Y .
Algorithm MinCostUnlimited(T);
/* The number of agents is unlimited */
1. X := {r}; Δ := 0; L := ∅;
2. while X ̸= ∅do
3.
v := Extract any element of X;
4.
Add to X each x /∈maxpath(v) such that
parent(x) ∈maxpath(v) and Gain(x) > 0;
5.
Δ := Δ + Gain(v); L := L ∪{leaf(maxpath(v)};
6.
/ * X = { v ∈children(frame(L)) : v /∈frame(L) } */
7.
return 2 · |E| −Δ
Theorem 2. Assume that the number of agents initially placed at the source
node is at least equal to the number of leaves of T. Then algorithm
MinCostUnlimited(T) correctly computes in O(n) time the minimal amount of
energy needed for the broadcast in T.
The idea of the proof (see the Appendix) is based on the fact that the set X
is now restricted only to the children of the current frame and we choose each
of them at some time. Since any two nodes, which are present in X at a same
moment, never interfere (i.e. choosing one of them does never aﬀect the Gain
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
109
function of the other one), they may be treated in any order (as the number of
available agents is suﬃcient for taking each of them at some time). This allows to
avoid sorting and the time of the entire treatment is proportional to the number
of edges of T.
3
All Agents Start from the Same Node r Diﬀerent
from the Source s
In this section we extend the consideration to the case when the initial position
of the packet is not at the root of the tree. We show that this setting may be
reduced to the case studied in the previous sections.
It would be helpful if we design the schedule, so that a robot moves along
its trajectory independently from the timing of the motion of any other robot.
By Observation 2, this was possible when the robots were initially placed at the
source node s. However, in the current setting, at every time moment the robots
executing an optimal schedule can be divided into two categories: the robots
which already know the packet and the robots that do not. Clearly, the former
category of robots are not restricted by their movement. On the other hand, the
robots not knowing the packet might need to delay their movement as they may
have to visit a node after the packet is deposited there.
The path between the root r and the source s we call the backbone of tree T
and we denote it by B. We start with the following lemma.
Lemma 6. There exists an optimal broadcasting algorithm in which the ﬁrst
activated agent starts moving towards the source node s, eventually returning to
r, before any other agent is activated.
Proof. The packet initially present at the source node s needs to be transported
to all other nodes of the tree, including root r. Therefore, there must exist an
agent which travels from r to s to pick up the packet. After that, a copy of the
packet must be transported along the backbone B, starting at s and ending at r.
During this travel of the packet along B, it may be transported by divers agents.
However, when the packet is left by some agent i at a point p of B and picked
later by some agent j, we can make agent i wait at point p until the arrival of
agent j. At that moment, as agents are identical, we could exchange the roles of
agents i and j and it is still agent i which continues to transport the packet. We
conclude, by induction, that the packet is transported all the way by the same
agent.
Observe as well, that the remaining agents that were exchanging roles with
the agent i, in fact, do not need to start their travel before agent i reaches r.
Indeed, they may wait at r until the packet is brought there by agent i and start
their respective routes afterwards.
⊓⊔
We now construct the reduction from the setting where r ̸= s to the case
r = s. For every instance I of the problem for r ̸= s we create an instance I′

110
J. Czyzowicz et al.
r
s
k agents
Instance I
T
Instance I′
r′ = s′
W
w0
(k + 1) agents
T ′
Fig. 2. On the left: the trajectory of the ﬁrst activated agent, when the algorithm
MinCostLimited is run for instance I′. The ﬁrst agent terminates its walk in w0. On
the right: trajectory of the ﬁrst activated agent for instance I. The ﬁrst agent traverses
the same nodes (except w0) but returns to r to be reused later.
of the problem where r = s. We show that to solve I it is suﬃcient to solve I′,
where we use the results from the previous sections.
Let I be a given instance of the broadcast problem, in which we have tree T
with k agents 1, 2, . . . , k initially placed at root r and the source s ̸= r.
We describe an instance I′ of the broadcast problem with s = r and k + 1
agents. We construct the tree T ′ by adding to T one extra leaf w0 and an edge
from node s to w0 of weight W, where W equals the sum of weights of all edges
of T. We deﬁne its root r′ = r in which we place k′ = k + 1 mobile agents,
represented by the integers 0, 1, . . . , k. We also set the source s′ = r′ (see Fig. 2).
Lemma 7 [Reduction-Lemma]. If s ̸= r in T then
MinCost(T, k) = MinCost(T ′, k + 1) + |path(r, s)| −W,
where in T ′ the source node s′ equals the initial location of k + 1 agents.
Proof. Consider ﬁrst an optimal solution to the instance I′ produced by algo-
rithm ConstructSchedule. The weight of the edge incoming to node w0 is so large
that the leaf w0 must belong to the set of critical leaves L and some agent 0
must terminate its walk in w0. By Observation 2, we can suppose that agent
0 is the very ﬁrst agent activated and the remaining agents didn’t start before
agent 0 reaches node w0. Denote by TH the set of all edges, traversed by agent
0, outside the simple path from r to w0. By algorithm ConstructSchedule, TH
forms a subset of hanging subtrees.
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
111
Consider now an optimal solution to the instance I, which veriﬁes Lemma 6.
In this solution, the ﬁrst activated agent 1, starting at r, travels along the back-
bone B to the source s (without any detour) and then continues its walk, even-
tually returning to r (bringing the packet), before any other agent starts moving.
Obviously, on its way back along the backbone (i.e. from s to r) agent 1 may
visit some nodes outside the backbone before returning to r.
Assume then, that agent 1, during its return from s to r along the backbone,
traverses exactly the subtrees formed by the edges TH (cf. Fig. 2). Consider the
time moment in instance I′ where agent 0 arrives at leaf w0 and the time moment
when in instance I agent 1 returns to root r. In both cases we have k agents
and the packet available at the root r and the part of the tree that still needs
to be explored equal to T \ (TH ∪B). Therefore, if we use the trajectories of
the remaining k agents 1, 2, . . . , k from the optimal solution of instance I′ to
complete the instance I the obtained solution of I is also optimal.
Observe that the assumption that agent 1 visited the subtrees formed by
the edges of TH may be dropped. Indeed, all subtrees of TH are the hanging
subtrees of the optimal solution and each of them is DFS traversed by some agent.
Assigning any such subtree to agent 1 or any other agent visiting its root does
not change the cost of the solution (recall line 3 of algorithm ConstructSchedule).
As from the moments when the situations in instances I′ and I are identical
all agents walk along the same trajectories in T and T ′, respectively, the cost of
the solution of instance I diﬀers from the solution of instance I′ by the diﬀerence
in the amounts of energy spent by the ﬁrst agents of each instance, respectively.
As this diﬀerence is W −|path(r, s)|, we have
MinCost(T, k) = MinCost(T ′, k + 1) + |path(r, s)| −W
⊓⊔
Theorem 3. Suppose that in the tree T the root r is diﬀerent from the source
s. We can solve the limited broadcast problem in O(n log n) time. If k is at least
equal to the number of leaves in T we solve the broadcast problem in O(n) time.
Proof. Due to Lemma 7 limited broadcast reduces in linear time to the case when
the source is the same as starting location of agents. In the unlimited case we
can use Lemma 7 with k = n.
Hence the time complexity is asymptotically of the same order as that of
the algorithm MinCostLimited(T, k), which is O(n log n). The case of unlimited
broadcast can be done similarly in O(n) time, by reduction to the algorithm
MinCostUnlimited.
⊓⊔
4
Final Remarks
There are several open questions related to the communication problems and
data delivery for mobile agents. One possible extension is to try to perform
broadcast from a single source having mobile agents initially distributed in some
nodes of the tree.

112
J. Czyzowicz et al.
When the initial amounts of energy are a priori assigned to the agents, most
communication problems for mobile agents are shown to be NP-complete (cf.
[1,13]). However, when the assignment of energy levels to the agents is left to
the algorithm, minimization of total energy used for the communication problems
remains open.
Another variation consists in broadcasting from a set of source nodes rather
than a single one. If such set of sources involve all the tree nodes the problem
becomes gossiping, in which the union of initial information of all nodes must
reach every node of the network.
More general open question, which generalizes all communication protocols,
concern the delivery of the union of information of a given set of nodes to another
set of nodes. Finally, we can consider delivery from a set of speciﬁc sources to the
respective speciﬁc target nodes. We believe that some variants of the question
will lead to NP-hard problems for trees.
References
1. Anaya, J., Chalopin, J., Czyzowicz, J., Labourel, A., Pelc, A., Vax`es, Y.: Collecting
information by power-aware mobile agents. In: Aguilera, M.K. (ed.) DISC 2012.
LNCS, vol. 7611, pp. 46–60. Springer, Heidelberg (2012). https://doi.org/10.1007/
978-3-642-33651-5 4
2. Albers, S.: Energy-eﬃcient algorithms. Commun. ACM 53(5), 86–96 (2010)
3. Albers, S., Henzinger, M.R.: Exploring unknown environments. SIAM J. Comput.
29(4), 1164–1188 (2000)
4. Amb¨uhl, C., Gasieniec, L., Pelc, A., Radzik, T., Zhang, X.: Tree exploration with
logarithmic memory. ACM Trans. Algorithms 7(4), 1–21 (2011)
5. Augustine, J., Irani, S., Swamy, C.: Optimal powerdown strategies. SIAM J. Com-
put. 37, 1499–1516 (2008)
6. Averbakh, I., Berman, O.: A heuristic with worst-case analysis for minimax routing
of two traveling salesmen on a tree. Discret. Appl. Math. 68, 17–32 (1996)
7. Awerbuch, B., Goldreich, O., Peleg, D., Vainish, R.: A trade-oﬀbetween informa-
tion and communication in broadcast protocols. J. ACM 37(2), 238–256 (1990)
8. Azar, Y.: On-line load balancing. In: Fiat, A., Woeginger, G.J. (eds.) Online Algo-
rithms. LNCS, vol. 1442, pp. 178–195. Springer, Heidelberg (1998). https://doi.
org/10.1007/BFb0029569
9. Baeza-Yates, R.A., Schott, R.: Parallel searching in the plane. Comput. Geom. 5,
143–154 (1995)
10. Bar-Yehuda, R., Goldreich, O., Itai, A.: On the time-complexity of broadcast in
multi-hop radio networks: an exponential gap between determinism and random-
ization. J. Comput. Syst. Sci. 45(1), 104–126 (1992)
11. B¨artschi, A., Chalopin, J., Das, S., Disser, Y., Geissmann, B., Graf, D., Labourel,
A., Mihal´ak, M.: Collaborative delivery with energy-constrained mobile robots. In:
Suomela, J. (ed.) SIROCCO 2016. LNCS, vol. 9988, pp. 258–274. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-48314-6 17
12. B¨artschi, A., Chalopin, J., Das, S., Disser, Y., Graf, D., Hackfeld, J., Penna,
P.: Energy-eﬃcient delivery by heterogeneous mobile agents. In: Proceedings of
STACS, pp. 10:1–10:14 (2017)
www.ebook3000.com

Energy-Optimal Broadcast in a Tree with Mobile Agents
113
13. Chalopin, J., Jacob, R., Mihal´ak, M., Widmayer, P.: Data delivery by energy-
constrained mobile agents on a line. In: Esparza, J., Fraigniaud, P., Husfeldt, T.,
Koutsoupias, E. (eds.) ICALP 2014. LNCS, vol. 8573, pp. 423–434. Springer, Hei-
delberg (2014). https://doi.org/10.1007/978-3-662-43951-7 36
14. Czyzowicz, J., Diks, K., Moussi, J., Rytter, W.: Communication problems for
mobile agents exchanging energy. In: Suomela, J. (ed.) SIROCCO 2016. LNCS,
vol. 9988, pp. 275–288. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
48314-6 18
15. Das, S., Dereniowski, D., Karousatou, C.: Collaborative exploration by energy-
constrained mobile robots. In: Scheideler, C. (ed.) Structural Information and
Communication Complexity. LNCS, vol. 9439, pp. 357–369. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-25258-2 25
16. Dynia, M., Korzeniowski, M., Schindelhauer, C.: Power-aware collective tree explo-
ration. In: Grass, W., Sick, B., Waldschmidt, K. (eds.) ARCS 2006. LNCS,
vol. 3894, pp. 341–351. Springer, Heidelberg (2006). https://doi.org/10.1007/
11682127 24
17. Fraigniaud, P., Gasieniec, L., Kowalski, D.R., Pelc, A.: Collective tree exploration.
In: Farach-Colton, M. (ed.) LATIN 2004. LNCS, vol. 2976, pp. 141–151. Springer,
Heidelberg (2004). https://doi.org/10.1007/978-3-540-24698-5 18
18. Irani, S., Shukla, S.K., Gupta, R.: Algorithms for power savings. ACM Trans.
Algorithms 3(4), 41 (2007)
19. Toth, P., Vigo, D.: Vehicle routing: problems, methods, and applications. SIAM
(2014)
20. Yao, F.F., Demers, A.J., Shenker, S.: A scheduling model for reduced CPU energy.
In: Proceedings of 36th FOCS, pp. 374–382 (1995)

Searching for a Non-adversarial, Uncooperative
Agent on a Cycle
Jurek Czyzowicz1, Stefan Dobrev2, Maxime Godon1, Evangelos Kranakis3(B),
Toshinori Sakai4, and Jorge Urrutia5
1 D´ep. d’informatique, Universit´e du Qu´ebec en Outaouais, Gatineau, Canada
2 Institute of Mathematics, Slovak Academy of Sciences, Bratislava, Slovak Republic
3 School of Computer Science, Carleton University, Ottawa, ON, Canada
kranakis@scs.carleton.ca
4 Research Institute of Educational Development, Tokai University, Tokyo, Japan
5 Instituto de Matematicas, UNAM, 04510 Mexico D.F., Mexico
Abstract. Assume k robots are placed on a cycle–the perimeter of a
unit (radius) disk–at a position of our choosing and can move on the
cycle with maximum speed 1. A non-adversarial, uncooperative agent,
called bus, is moving with constant speed s along the perimeter of the
cycle. The robots are searching for the moving bus but do not know
its exact location; during the search they can move anywhere on the
perimeter of the cycle. We give algorithms which minimize the worst-
case search time required for at least one of the robots to ﬁnd the bus.
The following results are obtained for one robot. (1) If the robot knows
the speed s of the bus but does not know its direction of movement then
the optimal search time is shown to be exactly (1a) 2π/s, if s ≥1, (1b)
4π/(s+1), if 1/3 ≤s ≤1, and (1c) 2π/(1−s), if s ≤1/3. (2) If the robot
does not know neither the speed nor the direction of movement of the
bus then the optimal search time is shown to be 2π(1 +
1
s+1). Moreover,
for all ϵ > 0 there exists a speed s such that any algorithm knowing
neither the bus speed nor its direction will need time at least 4π −ϵ to
meet the bus.
These results are also generalized to k ≥2 robots and analogous tight
upper and lower bounds are proved depending on the knowledge the
robots have about the speed and direction of movement of the bus.
Keywords: Cycle · Direction of movement · Moving bus
Robot · Search · Speed
1
Introduction
Due to their fundamental nature, the problems of searching and exploration have
been investigated in many areas of mathematics and computer science, e.g., by
J. Czyzowicz and E. Kranakis—Research supported in part by NSERC Discovery
grant.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 114–126, 2017.
https://doi.org/10.1007/978-3-319-72751-6_9
www.ebook3000.com

Searching for a Non-adversarial, Uncooperative Agent on a Cycle
115
addressing fundamental perspectives in robotics and autonomous mobile agent
computing. The robots move with certain speeds (not necessarily the same)
and the objective of the search is to ﬁnd a (usually static) target placed at
an unknown location of the domain in a (provably) optimal time. This search
problem was ﬁrst proposed by Bellman [5] and independently by Beck [4].
In this paper we consider a similar search problem concerning k mobile
autonomous robots which are initially located on a cycle–the perimeter of a
unit disk–and which can move on this cycle with maximum speed 1. Unlike pre-
vious research which considers a static target, in our work the robots are aware
that a bus (non-adversarial, uncooperative agent) is moving with constant speed,
say s, along this cycle but do not know its exact location and may or may not
know its direction of movement. We assume that during their search the robots
can move in any direction anywhere on this cycle.
More speciﬁcally, we are interested in investigating the following search prob-
lem: Give an algorithm which places the robots on the perimeter of the cycle and
minimizes the search time, i.e. the time it takes for at least one of the robots to
“catch the bus”. By the “robot catching the bus” we mean that the robot and
the bus are at the same location at the same time.
1.1
Preliminaries and Model of Computation
The robots are assumed to traverse a cycle (always assumed to be the perime-
ter of a disk of unit radius). Furthermore, there is a bus which is traveling
around the cycle at constant linear speed s and its location is unknown to the
robots. The robots can move at speed at most 1 on the perimeter of the disk
and can change direction at will at any time during the search depending on the
speciﬁcations of the algorithm. An algorithm speciﬁes the initial position and
trajectories of the robots. For k robots, their movement is speciﬁed by a k-tuple
(f1(t), f2(t), . . . , fk(t)) of k functions such that fi(t) gives the precise location
of the i-th robot on the cycle at time t, where i = 1, 2, . . . , k. Without loss of
generality we may assume that the robots start at the same time while the bus
is always in motion around the cycle. The search algorithm succeeds when at
least one of the k robots catches the bus. Note that in our model no wireless
communication between the robots and the bus is required, rather the robots
can recognize the bus when they cross its path. Throughout we use the abbre-
viations CW and CCW to denote clockwise and counter clockwise movement,
respectively.
1.2
Related Work
Our problem can be seen as a rendezvous/meeting problem with an uncoop-
erative, but not adversarial agent, a middle case between rendezvous and cops
and robbers. In the standard rendezvous model, all agents fully cooperate to
the common meeting goal. Indeed, this is the case in the related paper [11] on
rendezvous of two robots with diﬀerent speeds in a cycle; our problem is dif-
ferent in that one of the two vehicles—namely the bus—has a ﬁxed speed and

116
J. Czyzowicz et al.
cannot change direction. At the other extreme, in cops and robbers problems
(e.g., see [6]), the cops have the same goal of meeting the robber, but the robber
is adversarial and actively tries to avoid meeting. Here (at least for search), we
are also trying to meet with an agent. However, that agent does not cooperate,
but goes doing its own business, not caring whether it is met or not.
The underlying domain which is traversed by the robots is a continuous curve
(in our case the perimeter of a disk of unit radius). In this setting, in addition to
the rendezvous paper [11], related to our rendezvous problem is the work on prob-
abilistic rendezvous on a cycle for robots with diﬀerent speeds [15], rendezvous
on a cycle for multiple robots with diﬀerent speeds in [13], and rendezvous in
arbitrary graphs for two robots with diﬀerent speeds [16].
Related is also the literature on search on a line involving a robot and a static
exit in the seminal papers [3–5] as well as extensive discussions and models in
the books on search problems [1], on the theory of rendezvous games [2], and
on the game of cops and robbers [6]. More recently, there is research on robot
evacuation which is like search but measures the quality of search by the time
it takes the last robot to ﬁnd an exit; this has been investigated in the wireless
model as well as in the face-to-face model [8]. Related papers on robot evacuation
include two robots in the face-to-face model [7,9] and [10] in the wireless model
when the underlying domain is a triangle or a square.
There is also related work on gathering a collection of identical memoryless,
mobile robots in one node of an anonymous unoriented ring. Robots start from
diﬀerent nodes of the ring and operate in Look-Compute-Move cycles and have
to end up in the same node [14], as well as oblivious mobile robots in the same
location of the plane when the robots have limited visibility [12].
1.3
Outline and Results of the Paper
In this paper we determine the search time when the robots (1) do not know the
direction of movement but know the speed of the bus, and (2) know neither the
direction of movement nor the speed of the bus. We note that if the robot knows
the direction of movement of the bus, then it follows from the proof in [11] that
2π/(s + 1) is a tight bound on the search time. In Sect. 2 we provide tight upper
Table 1. Optimal search time for a single robot of maximum speed 1. The column
“Speed” refers to what the robot knows about the speed of the bus, while the last
column indicates the theorem where the result is proved.
Direction
Speed
Optimal search time Theorem
Known
s
2π/(s + 1)
Theorem 1
Unknown s ≥1
2π/s
Theorem 2
Unknown 1/3 ≤s ≤1 4π/(s + 1)
Theorem 2
Unknown s ≤1/3
2π/(1 −s)
Theorem 2
Unknown Unknown
4π
Theorem 3
www.ebook3000.com

Searching for a Non-adversarial, Uncooperative Agent on a Cycle
117
and lower bounds for single robot search, while in Sect. 3 tight upper and lower
bounds for multiple robot search. In both sections we consider the impact of
knowing the direction of movement of the bus. Table 1 summarizes the results
of Sect. 2 for a single robot and Table 2 the results of Sect. 3 concerning multiple
robots.
Table 2. Optimal search time for k robots of maximum speed 1. The column “Speed”
refers to what the robots know about the speed of the bus, while the last column
indicates the theorem where the result is proved (if not OPEN).
Direction
Speed
k
Search time (U.B.)
Lower bound
Theorem
Note
Known
s
all
2π/k(s + 1)
2π/k(s + 1)
Theorem 4
optimal
Unknown s ≥1
all
2π/ks
2π/ks
Theorem 5
optimal
Unknown s ≤1
even
2π/k
2π/k
Theorem 5
optimal
Unknown s ≤1
odd
min(2π/ks, 2π/(k −s))
2π/k
Theorem 5
OPEN
Unknown Unknown even
2π/k
2π/k
Theorem 6
optimal
Unknown Unknown odd
2π/(k −1)
2π/k
Theorem 6
OPEN
2
One Robot
Consider the case of a single robot R and let B denote the bus, and P the
path followed by the robot. Throughout this section we assume that the bus
is moving at constant speed s and cannot change direction, while the robot is
moving with speed 1. Our analysis is divided into three subsections depending
on the knowledge the robot has about the bus. In Subsect. 2.1 we assume only
that the robot knows the direction of movement of the bus, in Subsect. 2.2 the
robot does not know the direction of movement of the bus but knows its speed,
while in Subsect. 2.3 the robot knows neither the direction nor the speed s of
the bus. Table 1 summarizes the results of Sect. 2 for a single robot.
2.1
Known Direction of Movement of the Bus
Assume that the robot knows only the direction of movement of the bus. The
following theorem was ﬁrst proved in [11] but we state it here for completeness.
Theorem 1 [11]. If the robot knows the direction of movement of the bus then
2π
s+1 is the worst-case optimal search time.
⊓⊔
Observe that the upper bound stated in Theorem 1 is obvious.
2.2
Unknown Direction of Movement but Known Speed of the Bus
Assume that the robot does not know the direction of movement but knows the
speed s of the bus. We show that for small bus speeds the robot should walk at
its maximal speed, eventually catching the bus (head on or from behind). For

118
J. Czyzowicz et al.
large bus speeds, the best strategy for the robot is to wait until the bus arrives.
Finally for intermediate bus speeds, the robot should walk at full speed in one
direction, then reverse the direction at some point and continue until the bus is
met.
Theorem 2. If the robot knows the speed s of the bus but does not know its
direction of movement then the optimal search time is exactly
1. 2π/s
if s ≥1.
2. 4π/(s + 1) if 1
3 ≤s ≤1.
3. 2π/(1 −s) if s ≤1
3.
Proof (Theorem 2, upper bounds). The upper bounds are relatively simple; we
present below three algorithms, one for each case.
To prove Statement 1 assume that s ≥1. In the search algorithm below, the
robot stays put and waits for the bus to arrive.
Algorithm Wait (Direction Unknown: s ≥1).
1. Robot waits for the bus to arrive.
The upper bound 2π
s in Statement 1 is immediate since the bus travels with
speed s and the robot is at distance at most 2π from the bus. To prove State-
ment 2, assume that 1
3 ≤s ≤1 and consider the following search algorithm.
Algorithm ZigZag (Direction Unknown: 1/3 ≤s ≤1).
1. Robot chooses a direction and walks at full speed for time
2π
s+1;
2. If bus not found then changes direction and walks until bus is met;
The upper bound
4π
s+1 in Statement 2 is easy since in the ﬁrst part of the
algorithm the robot walks for time
2π
s+1. If it did not meet the bus by this time
it is because the bus is moving in the same direction as the robot. Therefore at
the moment the robot changes direction, in the second part of the algorithm,
it is certain that it is moving against the bus. Therefore it will meet the bus in
additional time
2π
s+1. To prove Statement 3, assume that s ≤1
3 and consider the
following search algorithm.
Algorithm GoStraight (Direction Unknown: s ≤1/3).
1. Robot chooses an arbitrary direction and walks until bus is found;
The upper bound
2π
1−s in Statement 3 is easy since in the worst case the bus
and the robot are moving in the same direction with initial distance less than
2π. This proves the upper bounds.
⊓⊔
Lower bounds. Let us introduce a visualization that will be used in the other
lower bounds as well. The x-axis represents time and the y-axis represents posi-
tions in the circle. The bus trajectory is then represented by a line passing
though the initial position of the bus, with the slope determined by the speed
and direction of the bus (let down-slope mean counterclockwise direction).
www.ebook3000.com

Searching for a Non-adversarial, Uncooperative Agent on a Cycle
119
The robot’s trajectory from time 0 until time T will be represented by a con-
tiguous curve P (possibly consisting of straight line segments) in this time-space
diagram. Let ps and pe denote the start and end-points of P. In order for the robot
to catch the bus, its trajectory will have to cross the bus lines corresponding to all
possible initial positions, directions (and possibly speeds, if the speed is unknown)
of the bus. Such a robot trajectory will be called a valid one. Note that the validity
of the trajectory depends on the assumptions/knowledge about the bus’s speed s,
i.e. a trajectory valid for a given s might not be valid for diﬀerent s.
Fig. 1. Trajectory and other concepts: u, v, u′ and v′ are key points. ta, t′, qq and q′
are support lines.
Consider ﬁrst the case where the speed of the bus is known to be s, but its
direction is unknown. For a ﬁxed P, let ts
a and ts
b denote tangents of slope s
touching P from above and below, respectively. Since here we deal with ﬁxed
s, we will omit superscripts, and use shorthands qa and qb for t−s
a
and t−s
b ,
respectively (refer to Fig. 1).
Let z(x) denote the y-coordinate of line z at time x. Hence, ta(0) and tb(0)
represent the starting positions of the buses moving at speed s that touch the
trajectory of the robot from above and from below.
Lemma 1. If the bus speed s is known but its direction is unknown, then P is
valid if and only if ta(0) −tb(0) ≥2π and qa(0) −qb(0) ≥2π
Proof (Lemma 1). As the y axis represents the unfolded perimeter of the cycle,
every point of the cycle is represented in any segment of y-axis of length at least
2π. In particular, the segment ⟨tb(0), ta(0)⟩. Note that since P is contiguous,
every line of slope s lying between tb and ta represents a bus starting at this
segment intersecting P. As the same argument holds for direction −s, P is valid.
If (without loss of generality) ta(0) −tb(0) < 2π, there is a point in the circle
not covered by the segment ⟨tb(0), ta(0)⟩. A bus line of slope s crossing this point
does not intersect P, therefore P is not valid.
⊓⊔
Suppose that P is valid. Let u be the earliest (in time) of the intersections
of ta or tb with P. Let t′ be a line of slope s at vertical distance exactly 2π from
u and lying between ta and tb. Let u′ be the earliest intersection of t′ with P.
Note that since t′ is between ta and tb, it is ensured to intersect P, hence u′ is
well deﬁned. Deﬁne v and v′ for the slope −s analogously (see Fig. 1). Let us
call points u, v, u′, v′ the key points of P.

120
J. Czyzowicz et al.
Let P ′ be a trajectory starting at the earliest of the key points, following
P and ﬁnishing at the latest key point (v to u′ in Fig. 1). We will call such
trajectory a pruned trajectory. By Lemma 1 and its construction, P ′ is also a
valid trajectory for s. (Note that translating a trajectory does not change its
validity status, as translations correspond to diﬀerent starting time and position
of the robot.)
Fig. 2. The case s ≥1.
We are now ready to prove Statement 1 of The-
orem 2:
Proof (Theorem 2, Statement 1 lower bound). Since
s ≥1, we immediately get that u = v = ps. By
Lemma 1, in order for P to be valid, it must touch
both t′ and q′ (see Fig. 2). Note that the robot can
do so in time 2π/s by simply waiting at the starting
location. Assume, on the contrary, that it moves and
ﬁrst touches (without loss of generality) q′ at point
v′. Then the earliest it can touch t′ is by moving
towards it (counterclockwise). However, it will not
be able to reach t′ sooner than q′ (which is also mov-
ing counterclockwise, but at speed s ≥1), which
reaches t′ at time exactly 2π/s.
⊓⊔
Consider now the case s < 1. The span of a robot trajectory is simply the
diﬀerence between its starting time and its end time. We know that we can prune
the trajectory without increasing its span or breaking its validity. In fact, the
following Lemma tells us that it is suﬃcient to consider only trajectories with
robot moving at full speed at all time:
Lemma 2. Let s < 1 and let P ′ be a valid pruned trajectory of span T. Then
there exists a valid trajectory P ′′ of span at most T in which the robot always
travels at full speed.
Proof (Lemma 2). Let a, b, c and d be the key points of P ′, ordered by time.
Let ra, rb, rc and rd be the corresponding support lines. P ′ will be modiﬁed as
follows (see Fig. 3):
Fig. 3. Speeding up pruned trajectory.
www.ebook3000.com

Searching for a Non-adversarial, Uncooperative Agent on a Cycle
121
– consider the robot moving from c to rd at full speed and let d′ be the point
where it reaches rd
– consider the robot moving from b to ra at full speed back in time and let
a′ be the point where it reaches ra (alternatively, a′ is the point on ra from
where a robot moving at full speed towards rb reaches b)
– ﬁnally, let c′ be the point where the robot moving at full speed from b to
rc reaches rc and let d′′ be the point where the robot moving from c′ at full
speed towards rd reaches it
As the support lines did not change, the trajectory a′bc′d′′ is still valid. Note
that a′ is not earlier than a and d′ is not later than d. Also, as c′ is not later than
c, then d′′ is not later than d′ and hence the resulting span a′d′′, as depicted in
Fig. 3, has not increased.
⊓⊔
Hence, it is suﬃcient to consider only trajectories consisting of at most three
line segments of slope 1 and −1. In fact, it is suﬃcient to consider only one- and
two-segment trajectories:
Lemma 3. Let P be a valid pruned full-speed trajectory consisting of three seg-
ments of alternating directions. Then there is a valid pruned full-speed trajectory
of at most two segments with smaller span.
Fig. 4. Optimizing a three-segment trajectory.
Proof (Lemma 3). As P has three segments, it must change direction in each
of its interior key points and each key point touches exactly one support line.
Without loss of generality the ﬁrst segment is of speed 1. As P changes direction
whenever it touches a support line, it can’t leave the quadrangle into which
it started. Since P is valid, it touches all four support lines, hence the only
possibility is shown in Fig. 4.
Consider now the trajectory a′b′c′. It touches all four support lines and hence
is valid, however its span is smaller than then the span of P : |bc| = |b′c′|, but
|ab| + |cd| > |a′b′|, because s < 1 and hence the distance of c from the support
line b′d is greater than the distance of b from it.
⊓⊔

122
J. Czyzowicz et al.
Now we are ready to prove Statements 2 and 3.
Proof (Theorem 2, Statements 2 and 3 lower bounds). By Lemma 3, we need to
consider only two strategies:
– One segment: robot travels in one direction until it meets the bus, or
– Two segments: robot travels in one direction for distance 2π/(1 + s) (any
two segment trajectory needs to cross from the starting support line to the
opposing one; if it reverses sooner, it can only achieve the needed 2π separa-
tion between support lines by travelling 2π/(1 −s) since reversal, at which
point one segment strategy is better), then reverses and travels until it meets
the bus
In the one segment strategy, the worst case time to meet the bus is 2π/(1 −s),
in the two segments strategy, it is 4π(1 + s). The ﬁrst is better for s < 1
3, the
latter for s ∈⟨1
3, 1⟩, which proves Statements 2 and 3. This completes the lower
bound in all three cases and proves Theorem 2.
⊓⊔
2.3
Robot Knows Neither the Direction nor the Speed of the Bus
Now assume the robot knows neither the direction of movement nor the speed
of the bus.
Theorem 3. If the robot knows neither the speed s of the bus being used nor its
direction of movement then the search time is 2π

1 +
1
s+1

. Moreover, for all
ϵ > 0 there exists a bus speed s such that any algorithm knowing neither the bus
speed nor its direction will need time at least 4π −ϵ to meet the bus.
Observe that the upper and the lower bounds from Theorem 3 converge to
the same value 4π when s approaches zero.
3
Multiple Robots
In this section we consider the case of a collection of k > 1 robots. Through-
out we assume that the bus is moving at constant speed s and cannot change
direction, while the robots are moving with speed 1. Our analysis is divided into
three subsections depending on the knowledge the robots have about the bus.
In Subsect. 3.1 we assume only that the robots know the direction of movement
of the bus, in Subsect. 3.2 the robots do not know the direction of movement of
the bus but know its speed, while in Subsect. 3.3 the robots know neither the
direction nor the speed s of the bus.
3.1
Known Direction of Movement of the Bus
Now assume the robots know the direction of movement of the bus.
Theorem 4. If the robots know the direction of movement of the bus, then the
optimal search time is
2π
k(s+1).
www.ebook3000.com

Searching for a Non-adversarial, Uncooperative Agent on a Cycle
123
3.2
Unknown Direction of Movement but Known Speed of the Bus
Now assume the robots do not know the direction of movement but know the
speed s of the bus.
Theorem 5. If the robots know the speed s of the bus but do not know its direc-
tion of movement then the search time is the following:
1. If s ≥1, then 2π/ks is the lower and the upper bound.
2. If s ≤1 and k is even, then 2π/k is the lower and the upper bound.
3. Furthermore, if s ≤1 and k is odd, then 2π/k is the lower bound and the
upper bound is
(a)
2π
ks for s ∈

k
k+1, 1

(b)
2π
k−s for s ≤
k
k+1
Proof (Theorem 5). We prove separately the upper and lower bounds.
Upper bounds. For Statement 1, assume s ≥1.
Search Algorithm (Direction Unknown, Speed Known s ≥1).
1. The robots are initially placed on the perimeter of the cycle at distance
2π
k from each other and wait motionless for the bus to arrive.
It is clear that the bus will meet one of the robots in time at most 2π
ks . This
upper bound is valid regardless of the parity of k.
Next consider Statement 2. Recall that in this case k is even. Assume s ≤1.
Search Algorithm (Direction Unknown, Speed Known s ≤1: k even).
1. The robots are initially placed in pairs on the perimeter of the cycle
at distance 2π
k from each other;
2. The robots in each pair move in opposite direction
For k even, the resulting distance between pairs is exactly
2π
k/2 = 4π
k . Observe
that the bus is located between two robots moving against each other. Since
these two robots will meet no later than in time 4π
2k = 2π
k , the resulting running
time for this algorithm will be 2π
k .
Finally, consider the case of k odd. We evaluate two algorithms and choose
the best, depending on s. The ﬁrst option is to use the algorithms for speed
larger than 1, i.e. spread the agents evenly and wait until the bus meets a robot.
In this case, the meeting time is 2π/ks.
The second option is to use the following algorithm:
Search Algorithm (Direction Unknown, Speed Known s ≤1: k odd).
1. Let X = 2π(1−s)
k−s
and Y = 2(2π−X)
k−1
2. k + 1 robots are initially placed in pairs on the perimeter of the cycle
at distance Y from each other; One robot at a node u neighbouring
a segment of length X is then removed to bring down the number
of robots used to k (see Fig. 5)
3. The robots in each pair move in opposite direction, the lone robot
moves away from the X segment

124
J. Czyzowicz et al.
Observe that if the bus started in a Y segment, two robots will be traveling
towards each other from the opposite ends of this segment and will meet it at
time at most Y/2. If the bus started in the X segment, the lone robot crossing
the X segment will catch it in time at most
X
1−s. X was selected so that these two
times are equal: T = Y
2 = 2π−X
k−1 =
2π(k−s)−2π(1−s)
k−s
k−1
=
2π(k−1)
(k−s)(k−1) =
2π
k−s =
X
1−s
Fig. 5. The algorithm for odd number of agents. Here k = 5.
For s >
k
k+1, the bound 2π/k of the waiting algorithm is better, while for
s <
k
k+1, this algorithm yields a better bound of
2π
k−s.
Fig. 6. Sizes of excluded regions. Left: case s ≥1. Right: case s < 1.
Lower bounds. Let vi(t) denote the speed of the i-th robot at time t as it is search-
ing for the bus. Further, consider the movement of the robot at an inﬁnitesimal
time dt. Let us express how much of the possible initial bus positions can the
robot exclude (cover) in the time interval dt: These are the initial bus positions
from which the bus crosses the trajectory of the robot in the interval dt, i.e.
if the robot does not meet the bus at time interval dt, then the bus could not
have started at those positions (see Fig. 6). As dt is inﬁnitesimal, the robot’s
trajectory can be seen as a line segment of horizontal size dt and vertical size
vi(t)dt. Hence, in the interval dt it covers a segment of starting bus positions of
size (s + vi(t))dt among the busses with opposite direction as the robot, and of
size |s −vi(t)|dt (where || here means absolute value) among the buses with the
same direction as the robot. Let us deﬁne f(t) = s + vt(t) + |s −vi(t)|.
www.ebook3000.com

Searching for a Non-adversarial, Uncooperative Agent on a Cycle
125
Let us ﬁrst consider the lower bound in Statement 1, i.e. s ≥1 ≥vi(t).
In this case, s + vi(t) and s −vi(t) are always positive, hence we can ignore
the absolute value and sum up the regions for both directions directly: f(t) =
(s + vi(t)) + (s −vi(t)) = 2s.
Let T be the time it takes for at least one of the robots to ﬁnd the bus
according to the execution of an optimal search algorithm. Therefore in time T,
the i-th robot can cover at most (if at every time moment it excluded diﬀerent
regions) length
 T
0 f(t)dt = 2sdt = 2sT. Thus, all k robots taken together can
cover at most a length of 2Tks, and only if all of them cover diﬀerent areas.
However, this last quantity must be at least 4π (2π for clockwise and another 2π
for counterclockwise bus directions), otherwise there is a non-excluded starting
bus position and direction which would escape the robot’s search. It follows that
2ksT ≥4π, which yields T ≥2π/ks. This proves the lower bound in Statement 1.
Now consider the lower bound in Statement 2. i.e. s ≤1. If s ≥vi(t), then
f(t) = 2s, as shown before. Otherwise |s −vi(t)| = vi(t) −s and f(t) = (s +
vi(t)) + (vi(t) −s) = 2vi(t). In any case, as both s and vi(t) do not exceed
1, we have f(t) ≥2. Hence, in time T, the i-th robot covers length at most
 T
0 2f(t)dt ≤
 T
0 2dt = 2T. It follows that k robots can cover a length of at
most 2kT. However, this last quantity must be at least 4π (otherwise there is a
trajectory of the bus which will escape the robots’ search). Therefore, 2kT ≥4π,
which yields T ≥2π/k. This proves the lower bound in Statements 2 and 3, as
the proof does not rely on the parity of k.
⊓⊔
3.3
Robots Know Neither the Direction nor the Speed of the Bus
Assume the robots know neither direction of movement nor speed s of the bus.
Theorem 6. If the robots know neither direction of movement nor speed of the
bus then the search can be completed in time 2π
k for k even, and
2π
k−1 for kodd.
Moreover, the lower bound of 2π
k is valid regardless of the parity of k.
The exact answer is not known for k odd.
4
Conclusion
Several interesting questions arise from our research. One possibility is to look
more closely to the case of non-constant speed (i.e. known only upper and lower
bound). In fact, one can consider also the case of known movement function
(e.g. f(t), where f(t) gives the speed of the bus at time t), but unknown initial
location and direction (determined by the sign at f(t)), and perhaps also the time
shift (e.g. knowing that the agent moves according to f(t + t0), for some t0, but
not knowing t0). Similarly one could consider the case of robots with diﬀerent
possibly non-constant speeds. The possibility of exploring other domains, like
trees or arbitrary graphs is quite challenging, and also communication models
like wireless (e.g., limited visibility, or face-to-face) could be considered.

126
J. Czyzowicz et al.
References
1. Ahlswede, R., Wegener, I.: Search Problems. Wiley-Interscience, Hoboken (1987)
2. Alpern, S., Gal, S.: The Theory of Search Games and Rendezvous. Springer, Boston
(2003). https://doi.org/10.1007/b100809
3. Baeza Yates, R., Culberson, J., Rawlins, G.: Searching in the plane. Inf. Comput.
106(2), 234–252 (1993)
4. Beck, A.: On the linear search problem. Isr. J. Math. 2(4), 221–228 (1964)
5. Bellman, R.: An optimal search. Siam Rev. 5(3), 274 (1963)
6. Bonato, A., Nowakowski, R.: The Game of Cops and Robbers on Graphs. AMS,
Providence (2011)
7. Brandt, S., Laufenberg, F., Lv, Y., Stolz, D., Wattenhofer, R.: Collaboration with-
out communication: evacuating two robots from a disk. In: Fotakis, D., Pagourtzis,
A., Paschos, V.T. (eds.) CIAC 2017. LNCS, vol. 10236, pp. 104–115. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-57586-5 10
8. Czyzowicz, J., G asieniec, L., Gorry, T., Kranakis, E., Martin, R., Pajak, D.: Evac-
uating robots via unknown exit in a disk. In: Kuhn, F. (ed.) DISC 2014. LNCS,
vol. 8784, pp. 122–136. Springer, Heidelberg (2014). https://doi.org/10.1007/978-
3-662-45174-8 9
9. Czyzowicz, J., Georgiou, K., Kranakis, E., Narayanan, L., Opatrny, J., Vogtenhu-
ber, B.: Evacuating robots from a disk using face-to-face communication (Extended
Abstract). In: Paschos, V.T., Widmayer, P. (eds.) CIAC 2015. LNCS, vol. 9079, pp.
140–152. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-18173-8 10.
CoRR abs/1501.04985
10. Czyzowicz, J., Kranakis, E., Krizanc, D., Narayanan, L., Opatrny, J., Shende, S.:
Wireless autonomous robot evacuation from equilateral triangles and squares. In:
Papavassiliou, S., Ruehrup, S. (eds.) ADHOC-NOW 2015. LNCS, vol. 9143, pp.
181–194. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-19662-6 13
11. Feinerman, O., Korman, A., Kutten, S., Rodeh, Y.: Fast rendezvous on a cycle by
agents with diﬀerent speeds. In: Chatterjee, M., Cao, J., Kothapalli, K., Rajsbaum,
S. (eds.) ICDCN 2014. LNCS, vol. 8314, pp. 1–13. Springer, Heidelberg (2014).
https://doi.org/10.1007/978-3-642-45249-9 1
12. Flocchini, P., Prencipe, G., Santoro, N., Widmayer, P.: Gathering of asynchronous
robots with limited visibility. Theor. Comput. Sci. 337(1), 147–168 (2005)
13. Huus, E., Kranakis, E.: Rendezvous of many agents with diﬀerent speeds in a cycle.
In: Papavassiliou, S., Ruehrup, S. (eds.) ADHOC-NOW 2015. LNCS, vol. 9143, pp.
195–209. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-19662-6 14
14. Klasing, R., Markou, E., Pelc, A.: Gathering asynchronous oblivious mobile robots
in a ring. Theor. Comput. Sci. 390(1), 27–39 (2008)
15. Kranakis, E., Krizanc, D., MacQuarrie, F., Shende, S.: Randomized rendezvous
algorithms for agents on a ring with diﬀerent speeds. In: ICDCN, Goa, India, 4–7
January, pp. 9:1–9:10 (2015)
16. Kranakis, E., Krizanc, D., Markou, E., Pagourtzis, A., Ram´ırez, F.: Diﬀerent speeds
suﬃce for rendezvous of two agents on arbitrary graphs. In: Steﬀen, B., Baier, C.,
van den Brand, M., Eder, J., Hinchey, M., Margaria, T. (eds.) SOFSEM 2017.
LNCS, vol. 10139, pp. 79–90. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-51963-0 7
www.ebook3000.com

Improved Leader Election for Self-organizing
Programmable Matter
Joshua J. Daymude1(B), Robert Gmyr2, Andr´ea W. Richa1,
Christian Scheideler2, and Thim Strothmann2
1 Computer Science, CIDSE, Arizona State University, Tempe, AZ, USA
{jdaymude,aricha}@asu.edu
2 Department of Computer Science, Paderborn University, Paderborn, Germany
{gmyr,scheidel,thim}@mail.upb.de
Abstract. We consider programmable matter that consists of compu-
tationally limited devices (called particles) that are able to self-organize
in order to achieve some collective goal without the need for central con-
trol or external intervention. We use the geometric amoebot model to
describe such self-organizing particle systems, which deﬁnes how parti-
cles can actively move and communicate with one another. In this paper,
we present an eﬃcient local-control algorithm which solves the leader
election problem in O(n) asynchronous rounds with high probability,
where n is the number of particles in the system. Our algorithm relies
only on local information — particles do not have unique identiﬁers, any
knowledge of n, or any sort of global coordinate system — and requires
only constant memory per particle.
1
Introduction
The vision for programmable matter is to create some material or substance
that can change its physical properties like shape, density, conductivity, or color
in a programmable fashion based on either user input or autonomous sensing
of its environment. Many realizations of programmable matter have been pro-
posed — including DNA tiles, shape-changing molecules, synthetic cells, and
reconﬁguring modular robots — each of which is pursuing solutions applica-
ble to its own situation, subject to domain-speciﬁc capabilities and constraints.
We envision programmable matter as a more abstract system of computation-
ally limited devices (which we refer to as particles) which can move, bond, and
exchange information in order to collectively reach a given goal without any out-
side intervention. Leader election is a central and classical problem in distributed
computing that is very interesting for programmable matter; e.g., most known
shape formation techniques for programmable matter suppose the existence of
J. J. Daymude and A. W. Richa—Supported in part by NSF awards CCF-1422603
and CCF-1637393.
R. Gmyr, C. Scheideler and T. Strothmann—Supported in part by DFG grant SCHE
1592/3-1.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 127–140, 2017.
https://doi.org/10.1007/978-3-319-72751-6_10

128
J. J. Daymude et al.
a leader/seed particle (examples can be found in [23] for the nubot model, [20]
for the abstract tile self assembly model and [12,13] for the amoebot model).
In this paper, we present a fully asynchronous local-control protocol for the
leader election problem, improving our previous algorithm for leader election
in [14] which was only described at a high level, lacking speciﬁc rules for each
particle’s execution. Moreover, while the analysis in [14] used a simpliﬁed, syn-
chronous setting and only achieved its linear runtime bound in expectation, here
we prove with high probability1 correctness and runtime guarantees for the full
local-control protocol2. Finally, as this algorithm is both conceptually simpler
than that of [14] and presented directly from the point-of-view of an individual
particle, it is more easily understood and implemented.
1.1
Amoebot Model
We represent any structure the particle system can form as a subgraph of the
inﬁnite graph G = (V, E), where V represents all possible positions the parti-
cles can occupy relative to their structure, and E represents all possible atomic
movements a particle can perform as well as all places where neighboring par-
ticles can bond to each other. In the geometric amoebot model, we assume that
G = Geqt, where Geqt is the inﬁnite regular triangular grid graph. We recall the
properties of the geometric amoebot model necessary for this algorithm; a full
description can be found in [14].
Each particle occupies either a single node (i.e., it is contracted) or a pair
of adjacent nodes in Geqt (i.e., it is expanded), and every node can be occupied
by at most one particle. Particles move through expansions and contractions;
however, as our leader election algorithm does not require particles to move, we
omit a detailed description of these movement mechanisms.
Particles are anonymous; they have no unique identiﬁers. Instead, each par-
ticle has a collection of ports — one for each edge incident to the node(s) the
particle occupies — that have unique labels from the particle’s local perspective.
We assume that the particles have a common chirality (i.e., a shared notion of
clockwise direction), which allows each particle to label its ports in clockwise
order. However, particles do not have a common sense of global orientation and
may have diﬀerent oﬀsets for their port labels.
Two particles occupying adjacent nodes are connected by a bond, and we
refer to such particles as neighbors. Neighboring particles establish bonds via
the ports facing each other. The bonds not only ensure that the particle system
forms a connected structure, but also are used for exchanging information. Each
particle has a constant-size local memory that can be read and written to by
any neighboring particle. Particles exchange information with their neighbors by
1 An event occurs with high probability (w.h.p.), if the probability of success is at least
1 −n−c, where c > 1 is a constant; in our context, n is the number of particles.
2 An astute reader may note that a w.h.p. guarantee on correctness is weaker than the
absolute guarantee given for the algorithm in [14], but the latter was given without
considering the necessary particle-level execution details.
www.ebook3000.com

Improved Leader Election for Programmable Matter
129
simply writing into their memory. Due to the constant-size memory constraint,
particles know neither the total number of particles in the system nor any esti-
mate of this number.
We assume the standard asynchronous model, wherein particles execute an
algorithm concurrently and no assumptions are made about individual particles’
activation rates or computation speeds. A classical result under this model is
that for any asynchronous concurrent execution of atomic particle activations,
there exists a sequential ordering of the activations which produces the same
end conﬁguration, provided conﬂicts which arise from the concurrent execution
are resolved (namely, only conﬂicts of shared memory writes can happen in our
algorithm; we simply assume that an arbitrary particle wins). Thus, it suﬃces to
view particle system progress as a sequence of particle activations; i.e., only one
particle is active at a time. Whenever a particle is activated, it can perform an
arbitrary, bounded amount of computation involving its local memory and the
memories of its neighbors and can perform at most one movement. We deﬁne
an asynchronous round to be complete once each particle has been activated at
least once.
1.2
Related Work
A variety of work related to programmable matter has recently been proposed
and investigated. One can distinguish between active and passive systems. In
passive systems, the computational units either have no intelligence (moving and
bonding is based only on their structural properties or interactions with their
environment), or have limited computational capabilities but cannot control their
movements. Examples of research on passive systems are DNA computing [1,3,
7,21], tile self-assembly systems (e.g., the surveys in [15,19,22]), and population
protocols [2]. We will not describe these models in detail as they are of little
relevance to our approach. Active systems, on the other hand, are composed of
computational units which can control the way they act and move in order to
solve a speciﬁc task. Prominent examples of active systems are swarm robotics
(see, e.g., [17,18]), modular self-reconﬁgurable robotic systems (e.g., [16,24]) —
especially metamorphic robots [8] — and the nubot model [5,6,23] by Woods
et al. For an in depth discussion of these models and how they relate to our
amoebot model, we refer the reader to the full version of this paper [9].
The amoebot model [10] is a model for self-organizing programmable matter
that aims to provide a framework for rigorous algorithmic research for nano-
scale systems. In [14], the authors describe a leader election algorithm for an
abstract (synchronous) version of the amoebot model that decides the problem
in expected linear time. Recently, a universal shape formation algorithm [13], a
universal coating algorithm [11] and a Markov chain algorithm for the compres-
sion problem [4] were introduced, showing that there is potential to investigate
a wide variety of problems under this model.

130
J. J. Daymude et al.
1.3
Problem Description
We consider the classical problem of leader election. An algorithm is said to solve
the leader election problem if for any connected particle system of initially con-
tracted particles with empty memories, eventually a single particle irreversibly
declares itself the leader (e.g., by setting a dedicated bit in its memory) and no
other particle ever declares itself to be the leader. We deﬁne the running time
of a leader election algorithm to be the number of asynchronous rounds until a
leader is declared. Note that we do not require the algorithm to terminate for
particles other than the leader.
2
Algorithm
Before we describe the leader election algorithm in detail, we give a short high-
level overview. The algorithm consists of six phases. These phases are not strictly
synchronized among each other, i.e., at any point in time, diﬀerent parts of the
particle system may execute diﬀerent phases. Furthermore, a particle can be
involved in the execution of multiple phases at the same time. The ﬁrst phase
is boundary setup (Sect. 2.1). In this phase, each particle locally checks whether
it is part of a boundary of the particle system. Only the particles on a bound-
ary participate in the leader election. Particles occupying a common boundary
organize themselves into a directed cycle. The remaining phases operate on each
boundary independently. In the segment setup phase (Sect. 2.2), the boundaries
are subdivided into segments: each particle ﬂips a fair coin. Particles that ﬂip
heads become candidates and compete for leadership whereas particles that ﬂip
tails become non-candidates and assist the candidates in their competition. A
segment consists of a candidate and all subsequent non-candidates along the
boundary up to the next candidate. The identiﬁer setup phase (Sect. 2.3) assigns
a random identiﬁer to each candidate. The identiﬁer of a candidate is stored dis-
tributively among the particles of its segment. In the identiﬁer comparison phase
(Sect. 2.4), the candidates compete for leadership by comparing their identiﬁers
using a token passing scheme. Whenever a candidate sees an identiﬁer that is
higher than its own, it revokes its candidacy. Whenever a candidate sees its own
identiﬁer, the solitude veriﬁcation phase (Sect. 2.5) is triggered. In this phase,
the candidate checks whether it is the last remaining candidate on the bound-
ary. If so, it initiates the boundary identiﬁcation phase (Sect. 2.6) to determine
whether it occupies the unique outer boundary of the system. In that case, it
becomes the leader; otherwise, it revokes its candidacy.
2.1
Boundary Setup
The boundary setup phase organizes the particle system into a set of boundaries.
This approach is directly adopted from [14], but we give a full description here
to introduce important notation. Let A ⊂V be the set of nodes in Geqt = (V, E)
that are occupied by particles. According to the problem deﬁnition, the subgraph
www.ebook3000.com

Improved Leader Election for Programmable Matter
131
Geqt|A of Geqt induced by A is connected. Consider the graph Geqt|V \A induced
by the unoccupied nodes in Geqt. We call a connected component R of Geqt|V \A
an empty region. Let N(R) be the neighborhood of an empty region R in Geqt;
that is, N(R) = {u ∈V \ R : ∃v ∈R such that (u, v) ∈E}. Note that by
deﬁnition, all nodes in N(R) are occupied by particles. We refer to N(R) as
the boundary of the particle system corresponding to R. Since Geqt|A is a ﬁnite
graph, exactly one empty region has inﬁnite size while the remaining empty
regions have ﬁnite size. We deﬁne the boundary corresponding to the inﬁnite
empty region to be the unique outer boundary and refer to a boundary that
corresponds to a ﬁnite empty region as an inner boundary.
For each boundary of the particle system, we organize the particles occu-
pying that boundary into a directed cycle. Upon ﬁrst activation, each particle
p instantly determines its place in these cycles using only local information as
follows. First, p checks for two special cases. If p has no neighbors, it must be
the only particle in the particle system since the particle system is connected.
Thus, it immediately declares itself the leader and terminates. If all neighboring
nodes of p are occupied, p is not part of any boundary and terminates without
participating in the leader election process any further.
If these special cases do not apply, then p has at least one occupied node
and one unoccupied node in its neighborhood. Interpret the neighborhood of
p as a directed ring of six nodes that is oriented clockwise around p. Consider
all maximal sequences of unoccupied nodes (v1, . . . vk) in this ring; call such a
sequence an empty sequence. Such a sequence is part of some empty region and
hence corresponds to a boundary that includes p. Let v0 be the node before v1
and let vk+1 be the node after vk in the ring. Note that we might have v0 = vk+1.
By deﬁnition, v0 and vk+1 are occupied. Particle p implicitly arranges itself as
part of a directed cycle spanning the aforementioned boundary by considering
the particle occupying v0 to be its predecessor and the particle occupying vk+1
to be its successor on that boundary. It repeats this process for each empty
sequence in its neighborhood.
A particle can have up to three empty sequences in its neighborhood, and
consequently can be part of up to three distinct boundaries. However, a parti-
cle cannot locally decide whether two distinct empty sequences belong to two
distinct empty regions or to the same empty region. To guarantee that the exe-
cutions on distinct boundaries are isolated, we let the particles treat each empty
sequence as a distinct empty region. For each such sequence, a particle acts as a
distinct agent which executes an independent instance of the algorithm encom-
passing the remaining ﬁve phases of the leader election algorithm. Whenever a
particle is activated, it sequentially executes the independent instances of the
algorithm for each of its agents in an arbitrary order, i.e., whenever a particle is
activated also its agents are activated. Each agent a is assigned the predecessor
and successor — denoted a.pred and a.succ, respectively — that was determined
by the particle for its corresponding empty sequence. This organizes the set of
all agents into disjoint cycles spanning the boundaries of the particle system (see
Fig. 1). As consequence of this approach, a particle can occur up to three times

132
J. J. Daymude et al.
on the same boundary as diﬀerent agents. While we can ignore this property for
most of the remaining phases, it will remain a cause for special consideration in
the solitude veriﬁcation phase (Sect. 2.5).
Fig. 1. Boundaries and agents. Particles are depicted as gray circles and the agents
of a particle are depicted as black dots inside of the corresponding circle. After the
boundary setup phase, the agents form disjoint cycles that span the boundaries of the
particle system. The solid arrows represent the unique outer boundary and the dashed
arrows represent the two inner boundaries.
2.2
Segment Setup
All remaining phases (including this one) operate exclusively on boundaries, and
furthermore execute on each boundary independently. Therefore, we only con-
sider a single boundary for the remainder of the algorithm description. The goal
of the segment setup phase is to divide the boundary into disjoint “segments”.
Each agent ﬂips a fair coin. The agents which ﬂip heads become candidates and
the agents which ﬂip tails become non-candidates. In the following phases, candi-
dates compete for leadership while non-candidates assist the candidates in their
competition. A segment is a maximal sequence of agents (a1, a2, . . . , ak) such
that a1 is a candidate, ai is a non-candidate for i > 1, and ai = ai−1.succ for
i > 1. Note that the maximality condition implies that the successor of ak is a
candidate. We refer to the segment starting at a candidate c as c.seg and call it
the segment of c. In the following phases, each candidate uses its segment as a
distributed memory.
2.3
Identiﬁer Setup
After the segments have been set up, each candidate generates a random iden-
tiﬁer by assigning a random digit to each agent in its segment. The candidates
use these identiﬁers in the next phase to engage in a competition in which all but
one candidate on the boundary are eliminated. Note that the term identiﬁer is
slightly misleading in that two distinct candidates can have the same identiﬁer.
www.ebook3000.com

Improved Leader Election for Programmable Matter
133
Nevertheless, we hope that the reader agrees that the way these values are used
makes this term an appropriate choice.
To generate a random identiﬁer, a candidate c sends a token along its segment
in the direction of the boundary. A token is simply a constant-size piece of
information that is passed from one agent to the next by writing it to the memory
of a neighboring particle. While the token traverses the segment, it assigns a
value chosen uniformly at random from [0, r −1] to each visited agent where
r is a constant that is ﬁxed in the analysis. The identiﬁer generated in this
way is a number with radix r consisting of |c.seg| digits where c holds the most
signiﬁcant digit and the last agent of c.seg holds the least signiﬁcant digit. We
refer to the identiﬁer of a candidate c as c.id. The competition in the next phase
of the algorithm is based on comparing identiﬁers. When comparing identiﬁers
of diﬀerent lengths, we deﬁne the shorter identifer to be lower than the longer
identiﬁer.
After generating its random identiﬁer, each candidate creates a copy of its
identiﬁer that is stored in reversed digit order in its segment. This step is required
as a preparation for the next phase. To achieve this, we use a single token
that moves back and forth along the segment and copies one digit at a time.
More speciﬁcally, we reuse the token described above that generated the random
identiﬁer. Once this token reaches the end of the segment, it starts copying the
identiﬁer by reading the digit of the last agent of the segment and moving to the
beginning of the segment. There, it stores a copy of that digit in the candidate
c. It then reads the digit of c and moves back to the end of the segment where
it stores a copy of that digit in the last agent of the segment. It proceeds in a
similar way with the second and the second to last agent and so on until the
identiﬁer is completely copied. Afterwards, the token moves back to c to inform
the candidate that the identiﬁer setup is complete.
Note that for ease of presentation we deliberately opted for simplicity over
speed when creating a reversed copy of the identiﬁer. As we will show in Sect. 3.2,
the running time of this simple algorithm is dominated by the running time of
the next phase so that the overall asymptotic running time of the leader election
algorithm does not suﬀer.
2.4
Identiﬁer Comparison
During the identiﬁer comparison phase the agents use their identiﬁers to compete
with each other. Each candidate compares its own identiﬁer with the identiﬁer of
every other candidate on the boundary. A candidate with the highest identiﬁer
eventually progresses to the solitude veriﬁcation phase, described in the next
section, while any candidate with a lower identiﬁer withdraws its candidacy. To
achieve the comparison, the non-reversed copies of the identiﬁers remain stored
in their respective segments while the reversed copies move backwards along
the boundary as a sequence of tokens. More speciﬁcally, a digit token is created
for each digit of a reversed identiﬁer. A digit token created by the last agent
of a segment is marked as a delimiter token. Once created, the digit tokens
traverse the boundary against the direction of the cycle spanning it. Each agent

134
J. J. Daymude et al.
is allowed to hold at most two tokens at a time, which gives the tokens some
space to move along the boundary. The tokens are not allowed to overtake each
other, so whenever an agent stores two tokens, it keeps track of the order they
were received in and forwards them accordingly. An agent forwards at most one
token per activation. Furthermore, an agent can only receive a token after it
creates its own digit token. We deﬁne the token sequence of a candidate c as
the sequence of digit tokens created by the agents in c.seg. Note that according
to the rules for forwarding tokens, the token sequences of distinct candidates
remain separated and the tokens within a token sequence maintain their relative
order along the boundary.
Whenever a token sequence traverses a segment c.seg of a candidate c, the
agents in c.seg cooperate with the tokens of the token sequence to compare the
identiﬁer c.id with the identiﬁer stored in the token sequence. This comparison
has three possible outcomes: (i) the token sequence is longer than c.seg or the
lengths are equal and the token sequence stores an identiﬁer that is strictly
greater than c.id, (ii) the token sequence is shorter than c.seg or the lengths are
equal and the token sequence stores an identiﬁer that is strictly smaller than
c.id, or (iii) the lengths are equal and the identiﬁers are equal. In the ﬁrst case,
c does not have the highest identiﬁer and withdraws its candidacy. In the second
case, c might be a candidate with the highest identiﬁer and therefore remains a
candidate. Finally, in the third case, c initiates the solitude veriﬁcation phase,
which is then executed in parallel to the identiﬁer comparison phase. Solitude
veriﬁcation might be triggered quite frequently, especially for candidates with
short segments; we describe how this is handled in the next section. Due to space
constraints, we omit the exact token passing scheme for identiﬁer comparison
and refer to the full version of this paper [9].
2.5
Solitude Veriﬁcation
The goal of the solitude veriﬁcation phase is for a candidate c to check whether it
is the last remaining candidate on its boundary. Solitude veriﬁcation is triggered
during the identiﬁer comparison phase whenever a candidate detects equality
between its own identiﬁer and the identiﬁer of a token sequence that traversed
its segment. Note that such a token sequence can either be the token sequence
created by c itself or the token sequence created by some other candidate that
generated the same identiﬁer. Once the solitude veriﬁcation phase is started, it
runs in parallel to the identiﬁer comparison phase and does not interfere with it.
This phase is based on the idea of solitude veriﬁcation given in [14], but greatly
simpliﬁes many of the original ideas to obtain a more easily understood protocol.
A candidate c can check whether it is the last remaining candidate on its
boundary by determining whether or not the next candidate in direction of the
cycle is c itself. To achieve this, the solitude veriﬁcation phase has to span not
only c.seg but also all subsequent segments of former candidates that already
withdrew their candidacy during the identiﬁer comparison phase. We refer to
the union of these segments as the extended segment of c. The basic idea of
the algorithm is the following. We treat the edges that connect the agents on
www.ebook3000.com

Improved Leader Election for Programmable Matter
135
the boundary as vectors in the two-dimensional Euclidean plane. If c is the last
remaining candidate on its boundary, the vectors corresponding to the directed
edges of the boundary cycle in the extended segment of c and the next edge
(connecting the extended segment of c to the next candidate) sum to the zero
vector, implying that the next candidate and c occupy the same node. To perform
this summation in a local manner, c locally deﬁnes a two-dimensional coordinate
system (e.g., by choosing two consecutive ports as the x and y axes, respectively)
and uses two token passing schemes to generate and sum the x and y coordinates
of these vectors in parallel. Again, due to space constraints, the details of this
token passing scheme for summing x or y vector coordinates is detailed in the
full version of this paper [9].
Using the token passing scheme, a candidate c can decide whether the next
candidate along the boundary is itself. However, this is not suﬃcient to decide
whether c is the last remaining candidate on the boundary. As described in
Sect. 2.1, a particle can occur up to three times as diﬀerent agents on the same
boundary. Therefore, there can be distinct agents on the same boundary that
occupy the same node of Geqt. If an extended segment reaches from one of these
agents to another, the vectors induced by the extended segment sum up to the
zero vector even though there are at least two agents left on the boundary.
To handle this case, each particle assigns a locally unique agent identiﬁer from
{1, 2, 3} to each of its agents in an arbitrary way. The token passing scheme then
additionally checks that the agent identiﬁer of the last agent in the extended
segment matches that of c, ensuring that c is the last remaining candidate on
its boundary.
Finally, we must address the interaction between the solitude veriﬁcation
phase and the identiﬁer comparison phase. As noted in the previous section,
solitude veriﬁcation may be triggered quite frequently. Therefore, it may occur
that solitude veriﬁcation is triggered for a candidate c while c is still performing
a previously triggered execution of solitude veriﬁcation. In this case, c simply
continues with the already ongoing execution and ignores the request for another
execution. Furthermore, c might be eliminated by the identiﬁer comparison phase
while it is performing solitude veriﬁcation. In this case, c waits for the ongoing
solitude veriﬁcation to ﬁnish and only then withdraws its candidacy.
2.6
Boundary Identiﬁcation
Once a candidate c determines that it is the only remaining candidate on its
boundary, it initiates the boundary identiﬁcation phase to check whether or not
it lies on the unique outer boundary of the particle system. If it lies on the outer
boundary, the particle acting as candidate agent c declares itself the leader. Oth-
erwise, c revokes its candidacy. To achieve this, we make use of the observation
that the outer boundary is oriented clockwise while an inner boundary is ori-
ented counter-clockwise (see Fig. 1), a property resulting directly from the way
the an agent’s predecessor and successor are deﬁned in Sect. 2.1.
A candidate c can distinguish between clockwise and counter-clockwise ori-
ented boundaries using a simple token passing scheme introduced in [14]. It

136
J. J. Daymude et al.
sends a token along the boundary that sums up the angles of the turns it takes
according to Fig. 2, storing the results in a counter α. When the token returns
to c, the absolute value |α| represents the external angle of the polygon induced
by the boundary. It is well known that the external angle of a polygon in the
Euclidean plane is |α| = 360◦. Since the outer boundary is oriented clockwise
and an inner boundary is oriented counter-clockwise, we have α = 360◦for the
outer boundary and α = −360◦for an inner boundary. The token can encode
α as an integer k such that α = k · 60◦. To distinguish the two possible ﬁnal
values of k it is suﬃcient to store k modulo 5 so that we have k = 1 for the outer
boundary and k = 4 for an inner boundary. Therefore, the token only needs
three bits of memory.
Fig. 2. Determining the external angle α. The incoming and outgoing arrows represent
the directions in which the token enters and leaves an agent, respectively. Only the angle
between the arrows is relevant; the absolute global direction of the arrows cannot be
detected by the agents since they do not posses a common compass.
3
Analysis
We now turn to the analysis of the leader election algorithm. We ﬁrst show its
correctness in Sect. 3.1 and then analyze its running time in Sect. 3.2. Due to
space constraints, some of the supporting lemmas and their proofs are omitted;
they can be found in the analysis section of the full version of this paper [9].
3.1
Correctness
To show the correctness of the algorithm we must prove that eventually a single
particle irreversibly declares itself to be the leader of the particle system and
no other particle ever declares itself to be the leader. Any agent on an inner
boundary can never cause its particle to become the leader; even if the algo-
rithm reaches the point at which there is exactly one candidate c on some inner
boundary, c will withdraw its candidacy in the boundary identiﬁcation phase.
Therefore, we can focus exclusively on the behavior of the algorithm on the
unique outer boundary. We focus only on the major theorem here.
Theorem 1. The algorithm solves the leader election problem, w.h.p.
Proof. We must show that eventually a single particle irreversibly declares itself
to be the leader of the particle system and no other particle ever declares itself to
www.ebook3000.com

Improved Leader Election for Programmable Matter
137
be the leader. Again, we consider only the agents on the outer boundary as agents
on an inner boundary will never cause their particles to declare themselves as
leaders. Once every particle has ﬁnished the boundary setup phase, every agent
has ﬁnished the segment setup phase, and every candidate has ﬁnished the iden-
tiﬁer setup phase, with high probability3 there is a unique candidate c∗that has
the highest identiﬁer on the outer boundary. Since c∗has the highest identiﬁer, it
does not withdraw its candidacy during the identiﬁer comparison phase. In con-
trast, every other candidate c ̸= c∗eventually withdraws its candidacy because
the token sequence of c∗eventually traverses c.seg. Therefore, such an agent c
cannot cause its particle to become the leader. Once c∗is the last remaining
candidate on the outer boundary, it eventually triggers the solitude veriﬁcation
phase because the token sequence of c∗eventually traverses c∗.seg while c∗is
not already performing solitude veriﬁcation. After verifying that it is the last
remaining candidate, c∗executes the boundary identiﬁcation phase and deter-
mines that it lies on the outer boundary. It then instructs its particle to declare
itself the leader of the particle system.
⊓⊔
3.2
Running Time
Recall from Sect. 1.3 that the running time of an algorithm for leader election is
deﬁned as the number of asynchronous rounds until a leader is declared. Since
the given algorithm always establishes a leader on the outer boundary, we can
limit our attention to that boundary. Let n be the number of particles in the
system and L be the number of agents on the outer boundary.
The ﬁrst two phases of the algorithm, namely the boundary setup and seg-
ment setup phases, consist entirely of computations based on local neighborhood
information. Therefore, these phases can be completed instantly by each particle
upon its ﬁrst activation. Since each particle is activated at least once in every
round, every particle completes these ﬁrst two phases after a single round. When
an agent becomes a candidate, it initiates the identiﬁer setup phase. We have
the following lemma.
Lemma 1. All candidates on the outer boundary complete the identiﬁer setup
phase after O(log2 n) rounds, w.h.p.
After the identiﬁers have been generated, they are compared in the identiﬁer
comparison phase. In this phase, a set of digit tokens, one for each agent on the
boundary, traverses the boundary against the direction of the cycle spanning it.
Each agent can store at most two tokens. The tokens are not allowed to overtake
each other, so agents maintain the order of the tokens when forwarding them.
Note that a token is never delayed unless it is blocked by tokens in front of it.
3 This w.h.p. guarantee results from there being a small but nonzero probability that
either (a) all agents ﬂip tails and become non-candidates in the segment setup phase,
or (b) more than one candidate generates the same highest identiﬁer in the identiﬁer
setup phase. See [9] for more details.

138
J. J. Daymude et al.
Therefore, an agent a forwards a token whenever a.pred can hold an additional
token. Finally, an agent forwards at most one token for each activation.
We deﬁne the number of steps a token has taken as the number of times
it’s been forwarded from one agent to the next since its creation. Let T be the
earliest round such that at its beginning every agent on the outer boundary has
created its digit token. We have the following lemma.
Lemma 2. At the beginning of round T + i for i ∈N, each digit token on the
outer boundary has taken at least i steps.
Next, the following lemma provides an upper bound on the running time of
the solitude veriﬁcation phase.
Lemma 3. For an extended segment of length ℓ, the solitude veriﬁcation phase
takes O(ℓ) rounds.
The boundary identiﬁcation phase is only executed once a candidate deter-
mines that it is the last remaining candidate on the boundary. The following
lemma provides an upper bound for the running time of this phase.
Lemma 4. The boundary identiﬁcation phase on the outer boundary takes O(L)
rounds.
Finally, we can show the following runtime bound.
Theorem 2. The algorithm solves the leader election problem in O(L) rounds,
w.h.p.
Theorem 2 speciﬁes the running time of the leader election algorithm in terms
of the number of agents on the outer boundary. Let C be the number of particles
on the outer boundary. Since each particle on the outer boundary corresponds to
at most three agents on the outer boundary, we have that the algorithm solves
the leader election problem in O(C) rounds, w.h.p.. Moreover, the number of
particles on the outer boundary is obviously at most n; thus, the runtime bound
can also be formulated as O(n) rounds, w.h.p.. Note that compared to the O(C)
bound, the O(n) bound is quite pessimistic since the number of particles on the
outer boundary can much lower than n. For example, a solid square of n particles
only has C = O(√n) particles on its outer boundary.
4
Conclusion
In this paper we presented a randomized leader election algorithm for pro-
grammable matter which requires O(n) asynchronous rounds with high prob-
ability. The main idea of this algorithm is to use coin ﬂips to set up random
identiﬁers for each leader candidate in such a way that at least one candidate
has an identiﬁer of logarithmic length, leading to a unique leader w.h.p.. In the
full version of this paper [9], we consider several variants of the leader election
problem and detail how our algorithm can be modiﬁed to solve them. These
www.ebook3000.com

Improved Leader Election for Programmable Matter
139
variants include allowing particle systems to contain both expanded and con-
tracted particles, enforcing that all particles terminate their executions of the
algorithm (instead of requiring only the leader to terminate), and improving
the with high probability guarantee on electing a leader to a with probability 1
guarantee without changing the O(L), w.h.p. runtime bound.
References
1. Adleman, L.M.: Molecular computation of solutions to combinatorial problems.
Science 266(11), 1021–1024 (1994)
2. Angluin, D., Aspnes, J., Diamadi, Z., Fischer, M.J., Peralta, R.: Computation in
networks of passively mobile ﬁnite-state sensors. Distrib. Comput. 18(4), 235–253
(2006)
3. Boneh, D., Dunworth, C., Lipton, R.J., Sgall, J.: On the computational power of
DNA. Discrete Appl. Math. 71, 79–94 (1996)
4. Cannon, S., Daymude, J.J., Randall, D., Richa, A.W.: A Markov chain algorithm
for compression in self-organizing particle systems. In: Proceedings of the 2016
ACM Symposium on Principles of Distributed Computing, PODC 2016, Chicago,
IL, USA, 25–28 July 2016, pp. 279–288 (2016)
5. Chen, H.-L., Doty, D., Holden, D., Thachuk, C., Woods, D., Yang, C.-T.: Fast
algorithmic self-assembly of simple shapes using random agitation. In: Murata,
S., Kobayashi, S. (eds.) DNA 2014. LNCS, vol. 8727, pp. 20–36. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-11295-4 2
6. Chen, M., Xin, D., Woods, D.: Parallel computation using active self-assembly. In:
Soloveichik, D., Yurke, B. (eds.) DNA 2013. LNCS, vol. 8141, pp. 16–30. Springer,
Cham (2013). https://doi.org/10.1007/978-3-319-01928-4 2
7. Cheung, K.C., Demaine, E.D., Bachrach, J.R., Griﬃth, S.: Programmable assem-
bly with universally foldable strings (moteins). IEEE Trans. Rob. 27(4), 718–729
(2011)
8. Chirikjian, G.: Kinematics of a metamorphic robotic system. In: Proceedings of the
1994 IEEE International Conference on Robotics and Automation, IRCA 1994, vol.
1, pp. 449–455 (1994)
9. Daymude, J.J., Gmyr, R., Richa, A.W., Scheideler, C., Strothmann, T.: Improved
leader election for self-organizing programmable matter. CoRR, abs/1701.03616
(2017)
10. Derakhshandeh, Z., Dolev, S., Gmyr, R., Richa, A.W., Scheideler, C., Strothmann,
T.: Brief announcement: amoebot - a new model for programmable matter. In: 26th
ACM Symposium on Parallelism in Algorithms and Architectures, SPAA 2014,
Prague, Czech Republic, 23–25 June 2014, pp. 220–222 (2014)
11. Derakhshandeh, Z., Gmyr, R., Porter, A., Richa, A.W., Scheideler, C., Strothmann,
T.: On the runtime of universal coating for programmable matter. In: Rondelez,
Y., Woods, D. (eds.) DNA 2016. LNCS, vol. 9818, pp. 148–164. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-43994-5 10
12. Derakhshandeh, Z., Gmyr, R., Richa, A.W., Scheideler, C., Strothmann, T.: An
algorithmic framework for shape formation problems in self-organizing particle sys-
tems. In: Proceedings of the Second Annual International Conference on Nanoscale
Computing and Communication, NANOCOM 2015, Boston, MA, USA, 21–22
September 2015, pp. 21:1–21:2 (2015)

140
J. J. Daymude et al.
13. Derakhshandeh, Z., Gmyr, R., Richa, A.W., Scheideler, C., Strothmann, T.: Uni-
versal shape formation for programmable matter. In: Proceedings of the 28th ACM
Symposium on Parallelism in Algorithms and Architectures, SPAA 2016, Asilomar
State Beach/Paciﬁc Grove, CA, USA, 11–13 July 2016, pp. 289–299 (2016)
14. Derakhshandeh, Z., Gmyr, R., Strothmann, T., Bazzi, R., Richa, A.W., Scheideler,
C.: Leader election and shape formation with self-organizing programmable matter.
In: Phillips, A., Yin, P. (eds.) DNA 2015. LNCS, vol. 9211, pp. 117–132. Springer,
Cham (2015). https://doi.org/10.1007/978-3-319-21999-8 8
15. Doty, D.: Theory of algorithmic self-assembly. Commun. ACM 55(12), 78–88
(2012)
16. Fukuda, T., Nakagawa, S., Kawauchi, Y., Buss, M.: Self organizing robots based
on cell structures - CEBOT. In: Proceedings of the 1988 IEEE International Con-
ference on Intelligent Robots and Systems, IROS 1988, pp. 145–150 (1988)
17. Kernbach, S. (ed.): Handbook of Collective Robotics - Fundamentals and Chal-
langes. Pan Stanford Publishing, Singapore (2012)
18. McLurkin, J.: Analysis and implementation of distributed algorithms for multi-
robot systems. Ph.D. thesis, Massachusetts Institute of Technology (2008)
19. Patitz, M.J.: An introduction to tile-based self-assembly and a survey of recent
results. Nat. Comput. 13(2), 195–224 (2014)
20. Rothemund, P.W.K., Winfree, E.: The program-size complexity of self-assembled
squares (extended abstract). In: Proceedings of the Thirty-Second Annual ACM
Symposium on Theory of Computing, Portland, OR, USA, 21–23 May 2000, pp.
459–468 (2000)
21. Winfree, E., Liu, F., Wenzler, L.A., Seeman, N.C.: Design and self-assembly of
two-dimensional DNA crystals. Nature 394(6693), 539–544 (1998)
22. Woods, D.: Intrinsic universality and the computational power of self-assembly. In:
Proceedings of MCU 2013, pp. 16–22 (2013)
23. Woods, D., Chen, H.-L., Goodfriend, S., Dabby, N., Winfree, E., Yin, P.: Active
self-assembly of algorithmic shapes and patterns in polylogarithmic time. In: Pro-
ceedings of the 4th Conference on Innovations in Theoretical Computer Science,
ITCS 2013, pp. 353–354 (2013)
24. Yim, M., Shen, W.-M., Salemi, B., Rus, D., Moll, M., Lipson, H., Klavins, E.,
Chirikjian, G.S.: Modular self-reconﬁgurable robot systems. IEEE Robot. Autom.
Mag. 14(1), 43–52 (2007)
www.ebook3000.com

Conﬂict-Free Data Aggregation on a Square Grid
When Transmission Distance is Not Less Than 3
Adil Erzin1,2(B) and Roman Plotnikov1
1 Sobolev Institute of Mathematics, Novosibirsk, Russia
{adilerzin,prv}@math.nsc.ru
2 Novosibirsk State University, Novosibirsk, Russia
Abstract. In this paper a Convergecast Scheduling Problem on a unit
square grid, in each node of which there is a sensor with transmission
distance d which is not less than 3, is considered. For the cases d = 1 and
d = 2, polynomial algorithms, which construct the optimal solution to
the problem, are known. For an arbitrary d, an approximate algorithm is
proposed, the application of which gives an upper bound on the length
of the conﬂict-free data aggregation schedule, depending on d. We con-
ducted a priori and a posteriori analysis of the accuracy of this algorithm
for various d comparing either with the optimal length of the schedule,
or with a lower bound, the value of which we improved.
Keywords: Wireless sensor networks · Data aggregation
Min-length conﬂict-free scheduling · Convergecast Scheduling Problem
Grid graph
1
Introduction
The elements of the wireless sensor network (WSN) must regularly transmit the
collected data to the base station (BS). If the collected data is insigniﬁcantly
related to a speciﬁc sensor, then it is enough to deliver aggregated information to
the BS (e.g. min, max, mean, etc.). It is often assumed that the time of packet
transfer along the edge of the communication graph is one time slot. Sensors, as
usual, share common radio frequency to transmit messages. Therefore, if more
than one transmitter is operating in the receiving area, the receiver cannot get
the data packet intended for it because of such a phenomenon as interference.
This situation is called a conﬂict or collision. Moreover, in half-duplex commu-
nication systems, the sensor cannot receive and transmit, and also receive more
than one packet at a time. And, ﬁnally, for reason of energy saving, each sen-
sor transmits a data packet only once during the data aggregation session. This
means that in the communication network it is necessary to deﬁne the spanning
aggregation tree (AT) with the root in the BS along the edges of which the pack-
ets are transmitted [6]. Obviously, any tree node cannot send a packet before
receiving messages from all of its children.
In the Convergecast Scheduling Problem (CSP), it is required to ﬁnd the AT,
as well as the min-length schedule for the conﬂict-free aggregation of data [2,7].
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 141–154, 2017.
https://doi.org/10.1007/978-3-319-72751-6_11

142
A. Erzin and R. Plotnikov
CSP is NP-hard even in the case of a given AT [3]. However, in the case when
the communication graph is represented by a square unit grid, in each node
of which there is a sensor (this is a unit disk graph in the L1 metric, which
naturally arises in the construction of regular covers, which use square tiles [9]),
and the transmission distance is 1, the problem is polynomially solvable [5]. In
[3] the case when the transmission distance is greater than 1 is considered and an
approximate algorithm is proposed to construct a feasible conﬂict-free schedule.
Later it was proved that when a transmission range is 2, the proposed in [3]
algorithm constructs the optimal solution to the CSP on the grid graph [4].
In this paper, we use the Integer Programming Problem (IPP) from [8] to con-
struct optimal schedules for the conﬂict-free aggregation of data on a grid with
an arbitrary transmission distance and compare it with solutions constructed
by the approximate algorithm. Unfortunately, the IPP can only be solved with
small dimensions, so new lower bounds on the length of the schedule were found.
The rest of the paper is organized as follows. The mathematical formulation
of the problem is given in Sect. 2. In Sect. 3, a linear graph is considered as
a communication network. A CSP on a square grid is considered in Sect. 4.
Simulation results are presented in Sect. 5, and the paper is concluded in Sect. 6.
2
Problem Formulation
We consider a WSN consisting of stationary sensor nodes with one sink – base
station. We assume correct reception of a message if and only if there is no
simultaneous transmission within proximity of the receiver. We assume that the
interference range is equal to the transmission range. Then the WSN with sink
node 0 can be represented as a graph G = (V, E), where V denotes all the sensor
nodes, and edge (i, j) ∈E if and only if the distance between the nodes i and j
is within the transmission range.
The CSP considered in this paper is deﬁned as follows. Given a connected
undirected graph G = (V, E) and a sink node 0 ∈V , ﬁnd the minimum length
conﬂict-free schedule of data aggregation from all the vertices of V \ {0} to 0
under the following conditions:
– at the same time slot any vertex can either receive or send a message;
– each vertex can receive at most one message during one time slot;
– each vertex can send a message only once.
In the next sections we will specify the communication graph as linear graph
or grid graph.
3
Linear Graph
In a linear graph the vertices V = {0, 1, . . . , n} are on the line and the distance
between the neighbouring vertices i and i + 1, i = 0, . . . , n −1, is 1. The edge
between vertices i and j exists if and only if the distance |i −j| between them
does not exceed d, where d ≥3 is the transmission distance.
www.ebook3000.com

Conﬂict-Free Data Aggregation
143
Suppose ﬁrst that n = Nd, where N is a positive integer. We call the vertices
with numbers d multiple red, and all others blue. The idea of the algorithm is
as follows. At each time slot, if at least the two most remote vertices are red,
then we send the packet from the outermost red vertex to the left by a distance
d, if this does not lead to a conﬂict. Then we look for the most remote blue
vertex and send the packet from it to the most remote vertex, the transmission
to which does not lead to a conﬂict (taking into account the transmissions on
the right). We repeat the last procedure as long as possible. If there are no
valid transmissions, then delete all the vertices from which the packets were
sent, increase the time and repeat the procedure for the remaining vertices. The
pseudocode of the algorithm is given below.
Algorithm Al.
Step 0. Set t = 0 and W = {1, . . . , n}.
Step 1. Set t = t + 1 and S = ∅. (% S is the set of transmitting vertices)
Step 1.1. If there are no blue vertices left between the two outermost red
vertices of W, then the packet transfer from the outermost red vertex i to the
left by a distance d (to the vertex i −d). Set W = W \ {i} and S = S ∪{i}.
Step 1.2. Find the outermost blue vertex j ∈W transmission from which
does not lead to conﬂicts taking into account transfers from vertices of the set
S. Send the packet from j to the most remote vertex from W, if this does not
lead to a conﬂict taking into account transfers from the vertices of the set S. If
such node j exists, then set W = W \ {j} and S = S ∪{j}. If there is no such
vertex j and W ̸= ∅, then go to Step 1.
Step 2. Aggregation time is Th(n, d) = t. Stop.
Fig. 1. Example of conﬂict-free data aggregation in the linear graph (d = 5). (Color
ﬁgure online)
Lemma 1. If n = Nd+r, 0 ≤r ≤d−1, then for N ≥2 algorithm Al constructs
a feasible schedule of length at most N + d + r.
Proof. Let n = Nd. After the time slot d −1, one red vertex has already sent
the packet and the outermost red vertex has the number (N −1)d. It cannot
send a packet at time d −1. One more additional time slot is needed to provide

144
A. Erzin and R. Plotnikov
the possibility of transfer to a distance d. We will show that d + 1 time slots are
suﬃcient for all blue vertices to transmit packets to the nearest red vertices.
In algorithm Al at the ﬁrst time slot, blue vertices, distances between which
are not less than d, transmit their packets. Assume that the distance between
blue vertices with numbers less than (N −1)d, that transmit packets during the
ﬁrst time round (let’s call these vertices green), is d + 2 or (if the next vertex
is red) d + 3 (Fig. 1). Such a change in the set of vertices that transmit at the
ﬁrst time slot does not reduce the length of the schedule. Therefore, if we prove
a statement for such a slightly modiﬁed algorithm, then the statement is true
for the original algorithm.
The elements of each group of blue vertices located between adjacent green
vertices are transmitted in turn, beginning with the rightmost vertex. In each
group there are d vertices. They transmit at 2, 3, . . . , d + 1, d moments of time.
Therefore, after the (d + 1)-th moment, all the blue vertices will send packets.
If r > 0, then it suﬃces to add r time slots to obtain the situation described
above.
Lemma 2. If n = Nd, then the length of the conﬂict-free aggregation schedule
in the linear graph with n nodes cannot be less than N + d −1, where d is the
transmission distance.
Proof. Assume the opposite, i.e. there is a schedule whose length is less than
N + d −1. Let D be the distance from vertex 0 to the most remote vertex that
did not transmit data during the ﬁrst d moments in such a schedule.
If D ≥(N −1)d, then the vertex D needs at least N −1 time slots for
transmission, and therefore the length of the schedule cannot be less than N +
d −1. A contradiction.
If D < (N −1)d, then during d time slots, all d + 1 most remote vertices
must transmit. Therefore, during the ﬁrst d time slots, the two vertices from the
last d + 1 must transmit the packets simultaneously. But such transfer cannot
be carried out without conﬂicts. A contradiction.
Remark 1. The lower bound proved in the Lemma 2 is tight. In order to verify
this, it is suﬃcient to refer to Fig. 2.
4
Grid Graph
Suppose that communication graph is given as a square (n + 1) × (m + 1) grid
with unit distance between the neighbour nodes. At each node of grid (x, y),
x = 0, 1, . . . , n, y = 0, 1, . . . , m, except the origin (0, 0), there is a sensor with
the transmission distance d ≥3 in L1 metric.
In this section we describe the novel algorithm, which constructs an approxi-
mate solution to the considered problem on a grid. Also we give an upper bound
on the length of a schedule constructed by the algorithm.
www.ebook3000.com

Conﬂict-Free Data Aggregation
145
Fig. 2. An example of the tightness of the lower bound (d = 5). (Color ﬁgure online)
To further clarity the exposition, let us color the nodes, whose ordinate
is multiple of d, in red, and let us color the other nodes in blue. Let M =
⌊m/d⌋, ry = m −Md. Let the i-th row be the set of nodes which ordinate
equals i, where i = 0, . . . , m. Let the j-th strip stands for the set of rows
(j −1)d + 1, . . . , jd, j = 1, . . . , ⌊m/d⌋. Let us call the distance between rows
i and j, i, j = 0, . . . , m, i ̸= j the value of |i −j|.
The algorithm Asq of aggregation on square grid consists of two sequential
stages: vertical aggregation and horizontal aggregation. At the ﬁrst stage, in ver-
tical aggregation, the data transmission is performed only upward or downward
until the moment when the data from all vertices is transmitted to the elements
of 0-th row. After that, at the second stage, the horizontal aggregation is per-
formed with algorithm Al described above. The algorithm Av, which performs
vertical aggregation, is described below.
For each time slot the algorithm Av chooses the pairs of rows, each pair con-
tains the row of sending vertices (a row-transmitter) and the row of receiving
vertices (a row-receiver). The rows are chosen sequently from top to bottom.
Note, that vertical transmission of a row at larger distance is more preferable,
because in this case more vertices from row-transmitter are able to send simulta-
neously their packets without conﬂicts. Each time, the highest allowable blue row
is selected as row-transmitter and then the row at largest distance not exceeding
d from it is selected as row-receiver. If there are two candidates for a row-receiver,
then the highest one is chosen. The transmission between the chosen pair of rows
should not lead to a conﬂict with previously deﬁned transmissions at the same
time slot. The red row is chosen as a row-transmitter only in a case when all
vertices above and d−1 blue rows below have already sent their packets. Given a
row-transmitter and a row-receiver the method TransmitRow chooses a subset
of vertices to transmit without conﬂicts. The pseudocodes of the algorithms Av
and TransmitRow (due to lack of space) are presented in Appendix section.
Lemma 3. If rx ≤d−1, ry ≤d−1, N ≥2, and M ≥4 all positive integers, then
algorithm Asq constructs a feasible schedule on a grid (Nd+rx+1)×(Md+ry+1)
of length at most M + N + rx + ry + ⌊d2/4⌋+ 2d + max{1, ⌊(d −1)/2⌋} −2.

146
A. Erzin and R. Plotnikov
Proof. The length of a schedule constructed by the algorithm Asq on a grid
(Nd + rx + 1) × (Md + ry + 1) is equal to a sum of the length of a schedule
of vertical aggregation obtained by the Av on a grid (Nd + rx + 1) × (Md +
ry + 1) and the length of a schedule obtained by the Al on a linear graph with
Nd + rx + 1 nodes. As it follows from Lemma 1, the number of time slots of
horizontal aggregation does not exceed N + rx + d. According to the algorithm
Av, in each of the ﬁrst ry time slots the highest row from those, which did not
transmit data yet, transfers data downwards by d units. It remains to prove that
M + ⌊d2/4⌋+ d + max{1, ⌊(d −1)/2⌋} −2 time slots are suﬃcient to transfer the
data to the vertices on the 0-th row from all other vertices.
First, we will prove four propositions. Then we will show that the trueness
of Lemma 3 follows from these propositions.
Proposition 1 [3]. All blue vertices of the M-th strip transmit the data during
the ﬁrst d + ⌊d2/4⌋−1 time slots.
Proposition 2. All blue vertices of the (M−1)-th strip transmit the data during
the ﬁrst d −1 time slots.
Proof. The blue vertices of the (M −1)-th strip are able to transmit the packets
downwards at a distance d without conﬂicts, while the blue vertices of the M-th
strip transmit their packets up to the Md-th row. According to the Step 1.3
of Av, the blue vertices of the M-th strip transmit upwards during the ﬁrst
⌈(d+1)/2⌉
t=2
t time slots. Let us show that d −1 ≤⌈(d+1)/2⌉
t=2
t for any integer
d > 0. Consider two cases:
1. d = 2l −1:
⌈(d+1)/2⌉
t=2
t −(d −1) = l
t=2 i −2l + 2 = 0.5(l −1)(l + 2) −2l + 2 =
0.2l2 −1.5l + 1 ≥0 ∀l ∈(−∞; 1] ∪[2; +∞);
2. d = 2l :
⌈(d+1)/2⌉
t=2
t−(d−1) = l+1
t=2 i−2l+1 = 0.5l(l+3)−2l+1 = 0.5l2−0.5l+1 > 0
∀l ∈(−∞; +∞).
In both cases, the inequality d −1 ≤⌈(d+1)/2⌉
t=2
t holds for any integer value
of d.
Proposition 3. All blue vertices of the (M−2)-th strip transmit the data during
the ﬁrst d + ⌊d2/4⌋+ ⌊(d −1)/2⌋−2 time slots.
Proof. According to Proposition 2, during the ﬁrst d−1 time slots all blue vertices
of the (M −1)-th strip transmit their packets downwards to the blue vertices
of the (M −2)-th strip at the distance d. At the time slot t (1 ≤t ≤d −1)
the ((M −2)d −t)-th row of the (M −2)-th strip receive packets from the
((M −1)d −t)-th row of the (M −1)-th strip. At the same time slot either
((M −2)d −t −1)-th row of the (M −3)-th strip (when t < d −1) or the
((M −4)d −1)-th row of the (M −4)-th strip (when t = d −1) transmits its
packets downwards. This process is shown in Fig. 3 (the vertices, which already
sent their packets, are marked grey).
www.ebook3000.com

Conﬂict-Free Data Aggregation
147
After the end of the (d −1)-th time slot all blue vertices of the (M −1)-th
strip and all blue vertices of the (M −3)-th strip, except the row ((M −3)d−1),
have already sent their data, unlike the vertices of (M −2)-th strip and the
((M −3)d−1)-th row. As it is shown in Fig. 4, during the next ⌈(d−1)/2⌉
t=1
t time
slots the highest ⌈(d −1)/2⌉rows of the (M −2)-th strip transmit their packets
downwards to the row ((M −3)d−1). After that the remained ⌊(d−1)/2⌋rows of
the (M −2)-th strip transmit their packets upwards to the (M −2)-th row during
the next ⌊(d−1)/2⌋+1
t=2
t time slots. In total, all blue vertices of the (M −2)-th
strip transmit their packets during the ﬁrst d−1+⌈(d−1)/2⌉
t=1
t+⌊(d−1)/2⌋+1
t=2
t =
d + ⌊d2/4⌋+ ⌊(d −1)/2⌋−2 time slots.
Fig. 3. Vertical data aggregation during the ﬁrst d−1 time slots on the highest 4 strips
in the grid graph with transmission range d. (Color ﬁgure online)
Proposition 4. All blue vertices below the (M −2)-th strip transmit the data
during the ﬁrst d + ⌊d2/4⌋+ ⌈(d −1)/2⌉+ 1 time slots.
Proof. In order to facilitate understanding of this proof, the reader is referred
to the Fig. 5 where the illustration of vertical aggregation on a grid with d = 4
and M = 8 is presented. As it is shown above, all the vertices of (M −3)-th row
transmit their packets earlier than the vertices of (M −2)-th row, i.e., within
d + ⌊d2/4⌋+ ⌊(d −1)/2⌋−2 time slots. Let us prove that the vertices below the
(M −3)-th strip transmit the data during the ﬁrst d + ⌊d2/4⌋+ ⌈(d −1)/2⌉+ 1
time slots. At the ﬁrst time slot the data transmission below the (M −3)-th
strip is performed in the following regular way. The row i transmits the packets
downwards to the row i−d, the row j = i−2d−1 transmits the packets upwards

148
A. Erzin and R. Plotnikov
Fig. 4. Vertical data aggregation on the (M −2)-th strip of the grid graph with trans-
mission range d. (Color ﬁgure online)
at the distance d (or j = i−2d−2 if i−2d−1-th row is red), the highest blue row
below j transmits downwards at the distance d and so on. Note, that all rows
transmitting data at the ﬁrst time slot can be divided into pairs in such way
that there is either one red row or no any rows between the elements of a pair,
and there is exactly 2d −2 blue rows and 2 or 3 red rows between neighboring
pairs. The distance between two neighboring transmitting pairs of rows is either
2d + 1 or 2d + 2.
The highest pair of rows, which transmit the packets at the ﬁrst time slot
below the (M −3)-th strip, consists of rows (M −3)d −2 and (M −3)d −3,
because the row (M −1)d −1 transmits the packets to the row (M −2)d −1.
If d > 3, then at the second time slot the two highest blue rows below the
((M −3)d −3)-th row transmit their packets at the distance d (one — upwards,
another — downwards), and all the rows below (maybe except one row in the
ﬁrst strip) transmit the data by pairs at the distance d. If d > 5 then the data
transmission at the third time slot is performed in a similar way again. Note, that
the rows below the (M −3)-th strip transmit the data in mentioned way during
the ﬁrst ⌈(d −1)/2⌉time slots, because in that period there exist a guaranteed
receivers at the distance d for all transmitting vertices. Notice that after the
⌈(d −1)/2⌉-th time slot all the blue rows, which did not transmit packets yet,
can be grouped in such way that each group contains either d or d+1 sequential
blue rows (depending on the parity of d), and the distance between two groups is
greater than d. Obviously, elements in one group are not able to receive a packet
from elements of another group. Next, the highest rows of each group transmit
www.ebook3000.com

Conﬂict-Free Data Aggregation
149
the packets downwards at the distance d while the number of blue rows in group
exceeds d −1 — this is done by at maximum 2 time slots.
Fig. 5. Example of conﬂict-free vertical data aggregation in the grid graph (d = 4).
(Color ﬁgure online)
It remains only to show that duration of data transmission of the remaining
d −1 rows in a group does not exceed d + ⌊d2/4⌋−1 time slots. In a worst
case, at ﬁrst, the highest ⌈(d −1)/2⌉blue rows transmit the data downwards
to the lowest row of a group. Note that in this case the packet can be sent
downwards only if the receiver is red or if there exists one red row between
the transmitter and receiver, because otherwise transmission to the red row is
preferable. Therefore, the highest ⌈(d −1)/2⌉blue rows transmit their packets
not longer than ⌈(d+1)/2⌉
t=2
t time slots. After that the remaining ⌊(d −1)/2⌋
blue rows sequentially transmit data to the most remote red row. Notice that
the conﬂicts with other groups are not possible at this step. Therefore, the
remaining ⌊(d −1)/2⌋rows transmit their packets not longer that ⌊(d+1)/2⌋
t=2
t
time slots. We have ⌈(d+1)/2⌉
t=2
t + ⌊(d+1)/2⌋
t=2
t = d + ⌊d2/4⌋−1.
In total, the duration of data transmission from all blue rows below the
(M −2)-th strip can not exceed ⌈(d −1)/2⌉+ 2 + d + ⌊d2/4⌋−1 = d + ⌊d2/4⌋+
⌈(d −1)/2⌉+ 1.
Let us return to the proof of the Lemma 3. Note that each red row transmits
data downward at the distance d as soon as all rows above and d −1 blue rows
below have already transmitted their packets. Therefore, the upper bound of a
schedule length obtained by the algorithm Av is maximum of the next three
values:

150
A. Erzin and R. Plotnikov
(1) M + d + ⌊d2/4⌋−1 (this estimate follows from Proposition 1)
(2) M + d + ⌊d2/4⌋+ ⌊(d −1)/2⌋−2 (this estimate follows from Proposition 3)
(3) M + d + ⌊d2/4⌋+ ⌈(d −1)/2⌉−3 (this estimate follows from Proposition 4)
Remark 2. In [3] the less accurate upper bound for the minimum length of a
convergecast schedule on a square grid is found for the case when rx = ry = 0.
The upper bound obtained above in Lemma 3 is less than the previous upper
bound from [3] by min{d, ⌈(d −1)/2⌉+ 2}.
5
Simulation
All the proposed algorithms have been implemented in C++ using the Visual
Studio Integrated Development Environment. We have run a simulation in order
to compare the solutions obtained by the proposed algorithms with optimal solu-
tions or, in cases when an optimal solution is not known, to check the proximity
of the objective to the upper and lower bounds. We used CPLEX with IP for-
mulation from [8] to obtain an optimal solution. In Table 1 the results of the
experiment on linear graph are presented. The values marked italic stand for
the cases when CPLEX failed to ﬁnd an optimal solution within 1000 s, and the
best feasible solution was taken. The two useful points should be noticed: (a)
Table 1. The length of a schedule obtained by Al on a linear graph with n+1 vertices
compared with CPLEX solution, lower and upper bounds.
n
d = 3
d = 4
d = 5
d = 6
CPLEX Al LB UB CPLEX Al LB UB CPLEX Al LB UB CPLEX Al LB UB
2
2
2
-
2
2
2
-
2
2
2
-
2
2
2
-
2
3
3
3
3
4
3
3
-
3
3
3
-
3
3
3
-
3
4
4
4
-
5
4
4
4
5
4
4
-
4
4
4
-
4
5
4
5
-
6
5
5
-
6
5
5
5
6
5
5
-
5
6
4
4
4
5
5
6
-
7
6
6
-
7
6
6
6
7
7
5
5
-
6
5
6
-
8
6
7
-
8
7
7
-
8
8
5
6
-
7
5
5
5
6
6
7
-
9
7
8
-
9
9
6
6
5
6
6
6
-
7
6
7
-
10
7
8
-
10
10
6
6
-
7
6
7
-
8
6
6
6
7
7
8
-
11
11
6
7
-
8
7
7
-
9
7
7
-
8
7
8
-
12
12
7
7
6
7
7
7
6
7
7
8
-
9
7
7
7
8
13
7
7
-
8
7
8
-
8
8
8
-
10
8
8
-
9
14
7
8
-
9
10
8
-
9
8
8
-
11
8
9
-
10
15
8
8
7
8
8
8
-
10
-
8
7
8
10
9
-
11
16
8
8
-
9
8
8
7
8
-
9
-
9
9
9
-
12
17
10
9
-
10
8
9
-
9
-
9
-
10
14
9
-
13
18
13
9
8
9
9
9
-
10
-
9
-
11
13
9
8
9
www.ebook3000.com

Conﬂict-Free Data Aggregation
151
in cases when n = Nd algorithm Al always yielded an optimal solution; (b) in
cases when n = Nd and N ≤2 the objective of optimal solution equals to the
lower bound N + d −1, but in cases when n = Nd and N > 2 the objective of
optimal solution equals to the upper bound N + d. We believe that these two
properties hold for any integer N ≥1 and d ≥3, but unfortunately we could
not prove this theoretically.
Table 2 represents the results of Asq on a square compared with the upper
bound which equals to the sum of the upper bounds of Al and Av. Unfortunately
the optimal solutions are not known for these cases because CPLEX failed to
solve the problem on a square in acceptable time. The rather good lower bounds
Table 2. The length of a schedule obtained by Asq on a grid graph with (n+1)×(n+1)
vertices compared with upper bound.
n
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
d = 3 Asq
15 17 15 16 18 17 18 20 19 20 22 21 22 24 23 24 26 25 26 28 27
UB
-
-
15 17 19 17 19 21 19 21 23 21 23 25 23 25 27 25 27 29 27
d = 4 Asq
20 21 19 20 22 23 19 20 22 23 21 22 24 25 23 24 26 27 25 26 28
UB
-
-
-
-
-
-
19 21 23 25 21 23 25 27 23 25 27 29 25 27 29
d = 5 Asq
23 25 27 29 29 24 25 27 28 29 24 25 27 28 29 26 27 29 30 31 28
UB
-
-
-
-
-
-
-
-
-
-
24 26 28 30 32 26 28 30 32 34 28
d = 6 Asq
27 28 28 30 32 33 34 35 30 31 33 34 35 36 29 30 32 33 34 35 31
UB
-
-
-
-
-
-
-
-
-
-
-
-
-
-
29 31 33 35 37 39 31
Fig. 6. Objective of solution obtained by the algorithm Asq compared with upper
bound for the CSP on a square grid 101 × 101 with diﬀerent d. (Color ﬁgure online)

152
A. Erzin and R. Plotnikov
are not known as well: the best one is (n+m)/d+d+1 which is rather far from the
upper bound. The upper bounds were calculated for the cases when n = Nd + r
and N ≥4 because of the conditions of Lemma 3. It should be noticed, that the
upper bound appeared to be tight for the cases when n is multiple of d. In Fig. 6
the objectives obtained by the algorithm Asq and the upper bounds for a case
when n = m = 100 in dependency of d are presented. The length of obtained
schedule is minimum when d = 5. If n = m = 1000, then the minimum schedule
length is 248, when d = 13. In the general case, the function of the upper bound
of length of the schedule, depending on d, is convex.
6
Conclusion
In this paper, we present a new algorithm for approximate solution of the conﬂict-
free convergecast scheduling problem on a unit square grid. We found a guar-
anteed upper and lower bounds on objective value in linear case and an upper
bound in general case. The experiment showed, that the proposed algorithm
allows to obtain an optimal solution on a linear graph in cases when the number
of nodes except the base station is multiple of the transmission distance. Also,
according to the experiment results, the obtained upper bound in general case
is tight.
In general, CSP is NP-hard even when the aggregation tree is given [3]. We
have considered a special case of the problem that is interesting not only from
the theoretical point of view, but can also be used to assess the quality of the
functioning of the WSN. For example, in regular covers, the sensors are located
at the vertices of a regular grid, each cell (tile) of which is either a triangle, a
square, or a hexagon [9]. When the sensors are placed arbitrarily, each sensor
can be assigned to one of the grid nodes. This will make it possible to select
the transmission distance and estimate the data aggregation time in the original
sensor network [1].
In future we plan to deﬁne a rather accurate lower bound for vertical aggre-
gation in order to estimate quality of the proposed algorithm. Also we plan to
implement and run the best of known heuristics for the conﬂict-free convergecast
scheduling problem and compare them with our approach.
Acknowledgments. The research of A. Erzin is partly supported by the Russian
Foundation for Basic Research (grants 16-07-00552 and 17-51-45125) and by the Min-
istry of Science and Education of the Russian Federation under the 5–100 Excellence
Programme. The research of R. Plotnikov is partly supported by the Russian Founda-
tion for Basic Research (grant 16-37-60006).
A
Appendix
Algorithm Av.
Step 0. Set t = 0, W = {0, . . . , n}m, S = ∅. (% W is arrays of vertices which
did not transmit data yet, grouped by rows, S is a schedule stored as an array
www.ebook3000.com

Conﬂict-Free Data Aggregation
153
of arrays of pairs of vertices: each pair contains the sender and the receiver, the
number of the array where the pair is placed stands for the time slot)
Step 1. Set s = max{i : W[i] ̸= ∅}.
If s = 0 then Stop. The vertical aggregation schedule is S = {S1, . . . , St}. The
length of the schedule is t;
Set t = t + 1; Set St = ∅;
Step 1.1. If s > Md, then set Ss
t
= TransmitRow(W, s, s −d), set
St = St ∪Ss
t and go to Step 1. Otherwise, go to Step 1.2.
Step
1.2. If s
>
0, s is multiple of d and each element from
{Ws−d+1, . . . , Ws−1} is empty, then set Ss
t = TransmitRow(W, s, s −d), set
St = St ∪Ss
t and go to Step 1.4. Otherwise, go to Step 1.3.
Step 1.3. If s > (M −1)d, then do the following.
Set s = min{i : i > (M −1)d and Wi ̸= ∅};
Set r = M if s −(M −1) < ⌈(d −1)/2⌉or set r = M −1 otherwise;
Set Ss
t = TransmitRow(W, s, r);
Set St = St ∪Ss
t ;
Set sprev = s;
Set s = min{s −1, r −d −1};
Step 1.4. While s > 0 do the following.
If the s-th row is red or Ws is empty, then set s = s −1 and go to Step
1.4.
Find the most remote blue row r from the row s, such that |s−r| < d+1,
r < sprev −d and Wr is not empty. If there are two appropriate rows, then take
the maximum of them. If there is no any appropriate row, then set s = s −1
and go to Step 1.4.
Set Ss
t = TransmitRow(W, s, s −d);
Set St = St ∪Ss
t ;
Set sprev = s;
Set s = min{s −1, r −d −1};
Algorithm TransmitRow(W, s, r).
Set p = 0; A = ∅;
while p < n do
Add a pair of vertices ((Ws,p, s), (Ws,p, r)) to A;
Set pnext = p + 1;
while pnext < n and Ws,pnext −Ws,p < d + 1 −|s −r| do
p = p + 1;
Set Ws = Ws \ Ws,p;
Set p = pnext;
return A.

154
A. Erzin and R. Plotnikov
References
1. Aldyn-ool, T.A., Erzin, A.I., Zalyubovskiy, V.V.: The coverage of a planar region
by randomly deployed sensors. Vestn. Novosib. Gos. Univ. Ser. Mat. Mekh. Inform.
10(4), 7–25 (2010)
2. De Souza, E., Nikolaidis, I.: An exploration of aggregation convergecast scheduling.
Ad Hoc Netw. 11, 2391–2407 (2013)
3. Erzin, A., Pyatkin, A.: Convergecast scheduling problem in case of given aggre-
gation tree. The complexity status and some special cases. In: 10th International
Symposium on Communication Systems, Networks and Digital Signal Processing,
article 16, 6 p. IEEE-Xplore, Prague (2016)
4. Erzin, A.: Solution of the convergecast scheduling problem on a square unit grid
when the transmission range is 2. In: Battiti, R., Kvasov, D.E., Sergeyev, Y.D.
(eds.) LION 2017. LNCS, vol. 10556, pp. 50–63. Springer, Cham (2017). https://
doi.org/10.1007/978-3-319-69404-7 4
5. Gagnon, J., Narayanan, L.: Minimum latency aggregation scheduling in wireless
sensor networks. In: Gao, J., Efrat, A., Fekete, S.P., Zhang, Y. (eds.) ALGOSEN-
SORS 2014. LNCS, vol. 8847, pp. 152–168. Springer, Heidelberg (2015). https://
doi.org/10.1007/978-3-662-46018-4 10
6. Incel, O.D., Ghosh, A., Krishnamachari, B., Chintalapudi, K.: Fast data collection
in tree-based wireless sensor networks. IEEE Trans. Mob. Comput. 11(1), 86–99
(2012)
7. Malhotra, B., Nikolaidis, I., Nascimento, M.A.: Aggregation convergecast scheduling
in wireless sensor networks. Wirel. Netw. 17, 319–335 (2011)
8. Tian, C.: Neither shortest path nor dominating set: aggregation scheduling by greedy
growing tree in multihop wireless sensor networks. IEEE Trans. Veh. Technol. 60(7),
3462–3472 (2011)
9. Zalyubovskiy, V., Erzin, A., Astrakov, S., Choo, H.: Energy-eﬃcient area coverage
by sensors with adjustable ranges. Sensors 9(4), 2446–2460 (2009)
www.ebook3000.com

Uniform Dispersal of Robots with Minimum
Visibility Range
Attila Hideg1(B) and Tam´as Lukovszki2
1 Department of Automation and Applied Informatics,
Budapest University of Technology and Economics, Budapest, Hungary
attila.hideg@aut.bme.hu
2 Faculty of Informatics, E¨otv¨os Lor´and University, Budapest, Hungary
lukovszki@inf.elte.hu
Abstract. We consider the ﬁlling problem, in which autonomous mobile
robots enter a connected orthogonal area from several entry points
and have to disperse in order to reach full coverage. The entry points
are called doors. The area is decomposed into cells. The robots are
autonomous, anonymous, they have a limited visibility range of one unit,
and do not use explicit communication. Collision of the robots is not
allowed. First we describe an algorithm solving the ﬁlling problem for
the single door case in O(n) time steps in the synchronous model, where
n is the number of cells in the area. This algorithm is optimal in terms
of visibility range, and asymptotically optimal in running time and size
of persistent memory used by the robots. Moreover, we show that our
algorithm solves the multiple door ﬁlling problem in O(n) time, as well.
For the multiple door case, our algorithm is asymptotically worst-case
optimal, and its running time is at most k times the running time of the
optimal algorithm for any input, where k is the number of doors.
1
Introduction
In swarm robotics a huge number of simple, cheap, tiny robots can perform com-
plex tasks collectively. The greatest advantages of such systems are scalability,
reliability, and fault tolerance. Contrary to a single-robot system, which requires
complex, expensive hardware and software components with redundancy, the
same attributes can be achieved by simply adding more robots to a multi-robot
system. In case of mobile robots, the spatial distribution of the robots has huge
advantage in problems related to exploration, coverage, demining, toxic waste
cleanup, etc. In this paper we study the uniform dispersal (or ﬁlling) of syn-
chronous robots in an unknown, connected area.
The area is decomposed into cells and the robots are injected one by one into
the area through an entry point, which is called the door. The robots have to
reach full coverage by occupying all cells. This problem is called ﬁlling, and was
introduced by Hsiang et al. [5]. When more than one door is present in the area
the problem is called multiple door ﬁlling or k-door ﬁlling.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 155–167, 2017.
https://doi.org/10.1007/978-3-319-72751-6_12

156
A. Hideg and T. Lukovszki
In [5] the goal was to achieve a rapid ﬁlling, minimizing the make-span (time
to reach full coverage) of the algorithm. Their solution required 2n −1 cycles to
reach full coverage, where n was the number of cells in the area.
Barrameda et al. [1] investigated the minimum hardware requirements and
the possibilities of solving the ﬁlling problem by robots with constant visibility
radius, communication range, and constant number of bits of persistent memory.
The algorithms in [1,2,5] used the Leader-Follower method, where one robot
is elected as a leader and the rest of the robots follow it until the leader is
blocked, then the leadership is transferred to another robot. The leader-follower
method results in a DFS-like dispersion in the area. Collisions are prevented
through the property that the leader explores new cells (cells that were never
visited before) and the followers simply moving towards their predecessor (the
robot they are following).
In case of the multiple door ﬁlling problem the robots enter through multiple
doors and there are several leaders in the area. In [1] the robots were colored
according to the door they entered, and the robots required to have their color
visible to other robots within the visibility range. In [3] Das et al. showed that
allowing visible colors or lights yields a more powerful computational model than
allowing inﬁnite visibility range but no lights.
In this paper a fundamental question is, whether it is possible to reduce the
hardware requirements of the robots and still ﬁll an unknown connected orthog-
onal area, even in presence of holes, and maintain O(n) runtime. These hardware
requirements are: visibility range, size of persistent memory, and avoidance of
explicit communication and the usage of lights.
1.1
Our Contribution
We present a method for ﬁlling an unknown, connected orthogonal region S
consisting of |S| = n square shaped cells by a set of n autonomous anonymous
robots with a visibility radius of 1 hop in O(n) time in the synchronous compu-
tational model. The robots require O(1) bits of persistent memory and cannot
communicate (they do not use explicit communication, nor colors or lights). The
only precondition is that they require a common coordinate system.
First, we consider the single door case, and present an algorithm which solves
the problem without collisions and terminates in O(n) time. Then, we show that
the presented approach solves both the single door and multiple door ﬁlling
problem in orthogonal areas.
Regarding this model our algorithm is optimal in terms of visibility range
and asymptotically optimal in the size of the memory. Moreover, it is asymptot-
ically optimal in running time in the single door case, asymptotically worst-case
optimal in the multiple door case, and its running time is at most k times the run-
ning time of the optimal algorithm for any input. The optimality regarding the
visibility range follows from the fact that with a visibility range less than 1 the
robots cannot even distinguish between occupied and unoccupied neighboring
cells. The asymptotic optimality of the memory size O(1) follows from the result
by Barrameda et al. [1]; they proved that oblivious (memoryless) robots cannot
www.ebook3000.com

Uniform Dispersal of Robots with Minimum Visibility Range
157
deterministically solve the problem. The asymptotic optimality of the running
time O(n) follows from the fact that we can place one robot per round in the
single door case and n robots must be placed. For the asymptotically worst-case
optimality for the k-door case, we show inputs where almost all robots must pass
through a single cell to get into a large component of the area. This single cell
behaves similarly to a single door, and the dispersion must take Ω(n) time.
Organization: In Sect. 2 we deﬁne our model. In Sect. 3 we present previous
results on the ﬁlling problem and related problems. Then in Sect. 4 we describe
and analyze our algorithm for the single door and for the multiple door cases.
Finally, Sect. 5 summarizes the paper.
2
Model
We are given an orthogonal area, i.e. polygonal with sides either parallel or per-
pendicular to one another, which is decomposed into equal sized, square shaped
cells (see [2]). The size of each cell allows only one robot to occupy it at any
given time. Each cell has at most four adjacent cells in ﬁxed directions: North,
South, East, and West.
The robots’ actions are divided into three phases: Look, Compute, and Move.
During the Look phase, the robots take a snapshot of their surroundings, in the
Compute phase they perform their computations (e.g. which action should they
perform), and during the Move phase they move there. This is called the Look-
Compute-Move (LCM) model, which is commonly used in distributed robotics.
When the robots perform their LCM cycles at the same time, the model
is called fully synchronous (FSYNC). In the FSYNC model, each robot takes
snapshots at the same time, compute, and move at the same time based on a
global tick.
The robots are anonymous, i.e. they cannot distinguish each other, and are
equipped with limited hardware. They have a visibility range of 1 hop, i.e. each
robot can ’see’ only the cells which they are occupying and the cells adjacent to
it. In one LCM cycle a robot can move to one of its neighboring cells or stay
at place. The robots are silent, i.e. they cannot communicate at all. They are
ﬁnite-state robots, i.e. they have a constant number of bits of persistent memory.
The robots have a common notion of North, South, East, and West.
The entry points, called doors are always occupied by a robot. Whenever a
robot moves from a door cell, a new one is placed there. The doors cannot be
distinguished from other cells by the robots, moreover, the robots do not know
which door they used to enter the area.
3
Related Work
Hsiang et al. [5] investigated the make-span (i.e. the time to reach full coverage)
of ﬁlling of a connected orthogonal region measured in rounds. They assumed
that robots have a limited ability to communicate with nearby robots, i.e. a

158
A. Hideg and T. Lukovszki
robot is able to exchange a constant-size message. They proposed two solutions,
BFLF and DFLF, both modeling generally known algorithms: BFS and DFS. In
DFLF, the method maintained a distance of 2 hops between the robot and its
successor. As a consequence the method only required visibility range of 2 hops.
However, the robots had to be able to detect the orientation of each other.
Barrameda et al. [1] assumed common top-down and left-right directions for
the robots and showed that robots with visibility range of 1 hop and 2 bits of
persistent memory can solve the problem in an orthogonal area if the area does
not contain holes, without using explicit communication. Holes are cells in the
area which cannot be occupied by robots (e.g. obstacles).
In [2] Barrameda et al. presented two methods for ﬁlling an unknown orthog-
onal area in presence of obstacles (holes). Their ﬁrst method, called TALK,
requires a visibility range of 1 hop if the robots have explicit communication.
The other method, called MUTE, do not use explicit communication between
the robots, but it requires visibility range of 6. Both methods need O(1) bits of
persistent memory.
For the multiple door case, in [1] a method, called MULTIPLE, has been
presented. It solves the problem for robots with visibility range of 2 hops, no
explicit communication and a O(1) bits of persistent memory. In this solution
the doors are colored with diﬀerent colors and the robots are colored according
to the door they enter. The color of the robots is visible to other robots within
the visibility range.
A summary of these previous results and a comparison to our contribution
is presented in Table 1.
Table 1. Summary of the requirements of Filling algorithms.
Method
Visibility
range (hops)a
Comm. range
(hops)a
Memory (bits) Area
DFLF [5]
2
2
2
Arbitrary
TALK [2]
2
2
4
Orthogonal
MUTE [2]
6
0
9
Orthogonal
MULTIPLE [1] 3
0
4
Orthogonal
Our method: (here)
Single door
1
0
13 bits
Orthogonal
Multiple doors
1
0
13 bits
Orthogonal
aIn [2], grid cells sharing a common edge or a common corner with the current cell c
of the robot are assumed within the visibility range of the robot. In our model a cell
sharing only one corner with c has a hop distance of two. Thus, it is outside of the
visibility range. Only the cells sharing a common edge have hop distance of one.
In [6] a related problem, the pattern formation problem has been investigated
in the FSYNC model. In the pattern formation problem n robots are placed
arbitrarily in the 2 dimensional grid and they have to form a given connected
www.ebook3000.com

Uniform Dispersal of Robots with Minimum Visibility Range
159
pattern F, known for all robots. In [6] a common coordinate system for the robots
has been assumed. The presented solution requires no explicit communication,
a visibility range of 2 hops, and 2 bits of persistent memory. The robots are
gathered in a certain point. They reach this point one-by-one and then start
to traverse a spanning tree of F and ﬁll the tree by this traversal. The pattern
formation algorithm needs O(|F| + d) rounds, where d is the diameter of the
initial conﬁguration. Only considering the ﬁlling phase of this algorithm, it needs
O(|F|) rounds.
For an excellent overview of distributed mobile robotics, we refer to the book
by Flocchini et al. [4].
4
Method
In this section we describe the algorithm for the ﬁlling problem, i.e. we deﬁne
the states and state-transitions of the robots. Then we prove that the algorithm
solves both the single door and multiple door ﬁlling problem without collisions.
4.1
Concept
Our algorithm is based on the leader-follower approach, which means a certain
robot is the leader while the rest are following it. This is a common approach to
eliminate collisions, as the leader is the only robot which is capable of moving
to cells, that were never occupied before. Such cells are called free cells. Each
follower has a predecessor which is the robot it is following during the dispersion,
and each robot, except the one at the door, has a successor which follows it.
Each robot stores its state, which can be one of the following: None, Leader,
Follower, Stopped. The state of the robot is not visible for other robots. The
transitions between these states are shown in Fig. 1. The robots placed at the
door are initialized with None state. They can switch to either Leader or Follower
state. In Follower state, the robot can switch to Leader state if and only if its
predecessor was the Leader and that Leader switched to Stopped state. This
ensures that the number of Leaders does not increase.
The robots repeatedly perform their LCM cycles. We label each cycle with
a direction and call it a step. There are four possible directions: North, East,
West, South; the steps are labeled correspondingly N-step, E-step, S-step, W-
step. In each step the robots can only move toward the cell in the corresponding
direction. A round is a sequence of four consecutive steps starting with an N-
step, followed by an E-step, followed by an S-step, ending with a W-step (see
Fig. 2). The rounds and the steps start synchronously at the robots.
After a robot r is placed, in the ﬁrst round (and then in every odd round) it
always observes its surroundings, then it can only move during the second round
(and during every even round), irrespective of its current state. Note: odd and
even rounds are relative to the starting round for r.

160
A. Hideg and T. Lukovszki
None
start
Leader
Follower
Stopped
(has a robot to follow)
(no robot to follow)
(predecessor did not move)
(cannot move)
Fig. 1. States and state-transitions of the robots. The edges are labeled with the con-
dition for the transition.
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
R1
R2
R3
N-step E-step
S-step W-step N-step E-step
S-step W-step N-step E-step
S-step W-step
Fig. 2. The structure of the rounds. Three rounds (denoted by R1, R2, R3), each
consists of four consecutive steps in ﬁxed order: step s1 is an N-step, s2 is an E-step
and so forth.
Our algorithm mimics a DFS exploration of the unknown region simultane-
ously started from the doors. A current Leader deﬁnes a path in the DFS tree
from the root (door) it entered, which path is traversed by it. When the Leader
switches to Stopped state it is in a leaf of the DFS-tree. The Follower robots
will ﬁll the path segment between the last branch vertex and the Leader (in the
leaf). Then the leadership transfers backwards until we reach a branch vertex.
Then the robot on that branch vertex becomes a new Leader and traverses to the
next branch. As the destination selection is deterministic, the robots on branch
vertices are checking the neighbors in the same order, even without the explicit
s1
s2
s3
s4
N-step E-step
S-step W-step
r
r′
s1
s2
s3
s4
N-step E-step
S-step W-step
r
r′
Fig. 3. Robot r following r′.
www.ebook3000.com

Uniform Dispersal of Robots with Minimum Visibility Range
161
knowledge of the DFS-trees, or if the predecessor becomes Stopped. Therefore,
they will move in the direction of the ﬁrst free branch.
The main diﬃculty to realize this concept is that it must be ensured during
the algorithm that the Leaders are able to determine which neighboring cells
are free and the Followers must know where their predecessors are, despite the
fact that the predecessors can be outside of the 1 hop visibility range. Now we
describe the exact behavior of the robots in the diﬀerent states.
Stopped: If a robot r is in Stopped state it does not move anymore. Other
robots will treat it like it would be an obstacle.
Follower: If a robot r is a Follower it follows its predecessor r′ until r′ stops
for two consecutive rounds (r′ is not able to move in its even round). Then the
leadership will be transfered from r′ to r. In the odd rounds of r it checks the
occupancy of the cell where its predecessor is. When r takes a snapshot in the
Look phase in its odd rounds it always sees its predecessor in the N-step. If r′
moves in a certain step, then r will not see r′ in the next step. E.g., if r′ moves
to north in the N-step, then after the Look phase of the E-step r does not see r′,
but it knows that r′ moved to the north. Therefore, r knows where r′ is, even if
the distance between them is 2 and r does not have r′ in its visibility range. In
the next round r occupies the former cell of r′ and r′ becomes visible again. In
every even round r moves toward its predecessor, occupying the previous cell of
r′ in the step corresponding to the direction r moves. As a result, the successor
of r also knows which direction r moves to and follows r.
There might be a case during the odd round of r, that r′ does not move
in that round. It is only possible if r′ does not have any cells it can move to,
either because it is surrounded by obstacles or other robots. It implies that r′
switches to Stopped state and r can switch to Leader state after this round
(more precisely, after the Look phase of the N-step of the next round, when r
recognizes that r′ did not move in the odd round of r). We refer to this event
as the transfer of the leadership. It guarantees that r can only switch to Leader
state after its predecessor r′ switched to Stopped state. Therefore, the invariant
that there can only be one Leader at any time (or k Leaders in the multiply door
case) is fulﬁlled. Note: r switches to Leader state only in the N-step of its even
round. Then in that round r performs the actions of robots in Leader state.
Leader: If r is a Leader it moves to a free cell in every second (even) round.
In each step of every odd round r checks each of its neighboring cells if it is
occupied. Occupied cells can not be free and they can be excluded from the set
of potential moving directions in the next (even) round. In the next (even) round
in each step r checks the neighboring cell c corresponding to the direction of the
step again. If c is unoccupied and it has been unoccupied in the previous (odd)
round, r moves to c and r does not check the remaining directions in that round.
If r cannot move in any direction it switches to Stopped state and terminates
its actions. Note: r can switch to Stopped state in the same round it switched
to Leader state.

162
A. Hideg and T. Lukovszki
None: After r has been placed at a door in round Ri, its initial state is None.
It only has to know, which step is currently performed by the system, i.e. an N-,
E-, S-, or W-step. If the current step is an N-step and no robots are visible, then
r is the ﬁrst robot placed at that door and it has to switch to Leader state. In
every other scenario, its predecessor robot r′ is in the direction corresponding
to the previous step, e.g., if the current step is an E-step then the predecessor
r′ leaved in the previous step, which is an N-step. Therefore, it must be in the
northern neighbor cell. Let Ri′ be the round in which r′ has moved from the
door. Note that i′ = i −1 if r is placed in an N-step, and i′ = i otherwise. If
i′ = i then round Ri is a shortened round for the newly placed robot r and r
does not perform any actions in this round. Then the ﬁrst round of r starts with
the next N-step.
As the predecessor r′ of the newly placed robot r has moved during round
Ri′, round Ri′+2 is the next even round for r′ (i.e. the next round r′ moves). We
have to ensure that Ri′+2 will be an odd round round for r. Therefore, the only
action r performs in Ri′+1 is switching in Follower state at the end of the round.
The ﬁrst round when r performs its actions is Ri′+2, which is an odd round for
r, and it acts as a Follower. Note: in Ri′+3, robot r can switch to Leader state
if it recognizes that r′ is in Stopped state, and r can switch even to Stopped if
r cannot ﬁnd any free cells.
4.2
Single Door Case
First, we show that the described algorithm ﬁlls the area without collisions in
the case, when the robots are entering in a single door.
Lemma 1. No collisions can occur during the dispersion.
Proof. In each step each cell can only be occupied from one sole direction. Even
if two robots would move to the same cell during the same round, only one will
move there, depending on the direction it wants to enter.
⊓⊔
Lemma 2. During each step each Follower knows where its predecessor is.
Proof. Consider a Follower r. Let r′ be its predecessor. Based on the timing of
the movement, i.e. in which step r′ moved, r knows the target cell of r′. With
other words, when a robot r sees its predecessor r′ on cell c during step si,
but if r detects c is unoccupied in si+1, it implies that r′ has moved during si.
The robot r′ can only move to one direction which corresponds to the direction
of si. Consequently, after the Look phase of si+1 the robot r knows where its
predecessor r′ is, even if r′ is outside of its visibility range. In the next round r
moves to c and r′ becomes visible again.
The argument above does not apply when r′ moves from the door, as r has
not been placed yet. However, r knows, in which step it has been placed. Let si
be the step in which r′ has moved from the door and si+1 the step, in which r
has been placed. Then r′ had to move in the direction corresponding to step si.
Therefore, r knows where r′ is.
⊓⊔
www.ebook3000.com

Uniform Dispersal of Robots with Minimum Visibility Range
163
Lemma 3. The Leader only moves to free cells.
Proof. The leader has to move to free cells to increase the coverage with each
of its movement and to prevent inﬁnite loops. Based on one single snapshot the
Leader r can only distinguish between occupied and unoccupied cells. Let Ri
be the round in which r has moved (its even round), then Ri+1 will be its odd
round. In Ri+1, if a neighboring cell is occupied at any time during Ri+1, r
recognizes it as not being free. In the next (even) round Ri+2 in each step r
checks the neighboring cell c corresponding to the direction of the step again. If
c is unoccupied and it has been unoccupied in the previous round, r recognizes
c as a free cell and moves to c. Then r does not check the remaining directions
in that round.
r′
r′′
r
a)
c
r′
r′′
r
b)
r′
r′′
r
c)
Fig. 4. A non-free cell c and its neighborhood in three consecutive rounds.
Assume r would like to determine whether a neighboring cell c is free in Ri,
and assume c is not a free cell. There are two cases: c contains a robot at the
beginning of Ri (Fig. 4(a) and (c)) or not. If c is occupied, then r recognizes it as
not being free and will not try to move there. If c is unoccupied at the beginning
of Ri and it is not a free cell (Fig. 4(b)), there was a robot r′ in c at the beginning
of Ri−1 and moved from it to a neighbor of c which is not visible by r. Then,
r′′ (the successor of r′) will enter c in Ri to follow r′. At the beginning of the
next round (Ri+1), c will be occupied by r′′ (as in Fig. 4(c)) and r will not move
there.
As a result two consecutive rounds, Ri and Ri+1, are suﬃcient for r to identify
c as a non-free cell. In other case, c is identiﬁed as a free cell and r can move to
c in Ri+1.
⊓⊔
Lemma 4. At most one Leader is present in the area.
Proof. Recall the event transfer of the leadership. The Leader r switches to
Stopped state during Ri. Its successor r′ can only switch to Leader state once
it has detected that r is in Stopped state, which will be in the ﬁrst step
of Ri+1.
⊓⊔
Lemma 5. The proposed method ﬁlls the area.

164
A. Hideg and T. Lukovszki
Proof. For contradiction, assume a cell c is left unoccupied after the algorithm
is terminated. There are two cases: (i) c is unoccupied but not free (some robots
visited c already but c left it unoccupied), (ii) c is a free cell.
For (i), assume r is the last robot which left the cell c. The successor of r
must have followed r and occupied c, which is a contradiction.
For (ii), consider a cell c′ which is a closest occupied cell to c. There is a path
between c and c′ consisting only of unoccupied cells (or c′ is a neighbor of c).
By (i) all unoccupied cells are free cells, as well. Let r be the robot in c′, which
is in Stopped state as the algorithm has been already terminated. However, r
has a free neighboring cell, therefore, r can move there. This prevents r from
switching to Stopped state, which is a contradiction.
⊓⊔
Theorem 1. By using the presented algorithm a connected orthogonal area with
a single door is ﬁlled in O(n) rounds without collisions by robots with visibility
range of 1 hop and O(1) bits of persistent memory.
Proof. According to the previous lemmas, the algorithm ﬁlls the area and pre-
vents collisions. The robots move in their even rounds. Whenever a robot r is
placed at the door, its ﬁrst odd round is started with the ﬁrst N-step, i.e. if r is
placed in a E-, S-, or W-step, it waits for the next N-step. Then in the odd round
it just observes and in the next even round it moves, allowing the placement of
a new robot. Therefore, a new robot can be placed in every third round. Since
the number of cells in the area is n, placing n robots requires 3n rounds, which
takes 12n LCM cycles.
The persistent memory of the robots must store the state of the robots (2
bits), the parity of the current round (1 bit), and the step of the current round
(2 bits). Leaders must additionally store information about the occupancy of the
neighboring cells observed in the odd round (4 bits). Followers must store the
direction they will move in their next even round (2 bits), which is the direction
their predecessor is in their odd round. Additionally, Followers must store where
the predecessor moves in their odd round (2 bits) and the information about the
occupancy of the neighboring cells for the case when the Follower has to switch
to Leader state.
⊓⊔
4.3
Multiple Door Case
In the multiple door case, also called k-door ﬁlling, the robots are entering the
area through k ≥1 doors. The k = 1 case is the single door case. In the k-door
ﬁlling we assume that each door has enough robot, thus, when a robot leaves a
door, a new one can be placed there.
During the analysis of the multiple door case, we examine how multiple
Leaders and the set of Followers following them interact with each other. We
prove that robots entering from distinct doors cannot collide or block each other.
An invariant of the algorithm is that each Follower only follows its predecessor
or becomes a Leader. First, we have to prove that robots in Leader state are not
www.ebook3000.com

Uniform Dispersal of Robots with Minimum Visibility Range
165
colliding with other robots. Then, we have to show that the paths of the Leaders
do not cross each other (for the single door case, we have already shown that
the leader does not cross its own path).
Lemma 6. A Leader cannot collide with a Follower.
Proof. Lemma 3 still holds: each robot in Leader state can determine which cells
are free within two rounds. Therefore, they will not go to cells where a Follower
would.
⊓⊔
c′
r1
r3
r2
c
Fig. 5. Leaders can not collide. The Leader r1 occupies c in the N-step. In the E-step
r2 sees c as an occupied cell. An important aspect is that in this scenario the robot r2
still has to move to a cell c′ diﬀerent from c, if it can. Then it remains a Leader. If c′
is not free, then r2 cannot move and in step s4 and switches to Stopped state.
Lemma 7. A Leader cannot collide with another Leader.
Proof. Assume a scenario where two leaders r1 and r2 would move to the same
cell c (see Fig. 5). Furthermore, assume both r1 and r2 are in their even round
and recognized c as free cell. Let Ri be this round, consisting of steps s1 . . . s4,
and as Ri+1 is their even round, they both will try to move there. Since c is
visible by r1 and r2 from diﬀerent directions, therefore they try to move to c
in diﬀerent steps, which means one of the robots (w.l.o.g. r1) will move and
the other one (r2) will be prohibited to move to c after it has been occupied
by r1.
⊓⊔
Lemma 8. Paths of diﬀerent Leaders cannot cross each other.
Proof. Lemma 3 still holds for the multiple door case. It means that the Leader
only moves to free cells, which implies that a Leader will not cross the path,
where Followers are already moving. In Lemma 7 we have shown that two Leaders
cannot collide (and will not cross the path of each other). Therefore, the path
of the Leaders will not cross each other during the dispersion.
Theorem 2. By using the presented algorithm a connected orthogonal area with
multiple doors is ﬁlled in O(n) rounds without collisions by robots with visibility
range of 1 hop and O(1) bits of persistent memory.
Proof. As in the single door case, the robots do not collide with each other.
Furthermore, the paths of the Leaders cannot cross, they do not block robots
entering from other doors.
⊓⊔

166
A. Hideg and T. Lukovszki
Lemma 9. The algorithm is asymptotically worst-case optimal in the multiple
door case.
Proof. Consider a region A, as depicted in Fig. 6(b). Removing the cell c from
A disconnects the region, such that all doors are separated from the major
connected component A′ of A\c. Therefore, all robots must enter to A′ through
c. Since collision is not allowed, only one robot per round can enter to A′ in any
algorithm. If A′ contains Ω(n) cells, then each algorithm requires Ω(n) rounds.
Our algorithm ﬁlls A in O(n) time.
⊓⊔
D1
D2
D3
D1
D2
D3
c
A’
Fig. 6. Best-case (left) and worst-case (right) examples for the make-span of multiple
door ﬁlling. The best-case is, when the robots entering at disjoint doors ﬁll equal sized
partitions of the area. A worst case input, when single door is used to ﬁll almost
the whole area. D1, D2, D3 are the doors. Red represents the path of the ﬁrst Leader
entering through the given door. (Color ﬁgure online)
For any input, even in the best-case, at most k robots per round can be
placed in the area. Therefore, Ω(n/k) is a lower bound on the ﬁlling time of any
algorithm for each input. Consequently, for each input the O(n) running time of
our algorithm is at most k times the running time of the optimal algorithm.
Note: for certain inputs our algorithm also reaches k speedup (see Fig. 6(a)),
When robots entering from diﬀerent doors ﬁll equal sized partitions of the region,
the runtime is O(n/k).
5
Summary
We have considered the ﬁlling problem in an unknown, connected, orthogonal
region, where the robots enter the region through several doors and they have to
disperse in order to reach full coverage. The robots are autonomous, anonymous,
silent, they have a limited visibility range of 1 hop, and use O(1) bits of persistent
memory. They have a common notion of North, South, East, and West. Collision
of the robots is not allowed. We have presented an algorithm solving the ﬁlling
problem with multiply doors in O(n) time steps in the synchronous model, where
n is the number of cells in the area. This algorithm is optimal in terms of visibility
www.ebook3000.com

Uniform Dispersal of Robots with Minimum Visibility Range
167
range, and asymptotically optimal in size of persistent memory used by the
robots. The running time is asymptotically optimal for the single door case. For
the multiple door case, our algorithm is asymptotically worst-case optimal, and
its running time is at most k times the running time of the optimal algorithm
for any input, where k is he number of doors.
References
1. Barrameda, E.M., Das, S., Santoro, N.: Deployment of asynchronous robotic sensors
in unknown orthogonal environments. In: Fekete, S.P. (ed.) ALGOSENSORS 2008.
LNCS, vol. 5389, pp. 125–140. Springer, Heidelberg (2008). https://doi.org/10.1007/
978-3-540-92862-1 11
2. Barrameda, E.M., Das, S., Santoro, N.: Uniform dispersal of asynchronous ﬁnite-
state mobile robots in presence of holes. In: Flocchini, P., Gao, J., Kranakis, E.,
Meyer auf der Heide, F. (eds.) ALGOSENSORS 2013. LNCS, vol. 8243, pp. 228–
243. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-45346-5 17
3. Das, S., Flocchini, P., Prencipe, G., Santoro, N., Yamashita, M.: Autonomous mobile
robots with lights. Theor. Comput. Sci. 609, 171–184 (2016)
4. Flocchini, P., Prencipe, G., Santoro, N.: Distributed Computing by Oblivious Mobile
Robots: Synthesis Lectures on Distributed Computing Theory. Morgan & Claypool
Publishers, San Rafael (2012)
5. Hsiang, T.-R., Arkin, E.M., Bender, M.A., Fekete, S.P., Mitchell, J.S.B.: Algorithms
for rapidly dispersing robot swarms in unknown environments. In: Boissonnat, J.-D.,
Burdick, J., Goldberg, K., Hutchinson, S. (eds.) Algorithmic Foundations of
Robotics V. STAR, vol. 7, pp. 77–93. Springer, Heidelberg (2004). https://doi.org/
10.1007/978-3-540-45058-0 6
6. Lukovszki, T., Meyer auf der Heide, F.: Fast collisionless pattern formation by
anonymous, position-aware robots. In: Aguilera, M.K., Querzoni, L., Shapiro, M.
(eds.) OPODIS 2014. LNCS, vol. 8878, pp. 248–262. Springer, Cham (2014). https://
doi.org/10.1007/978-3-319-14472-6 17

Gathering Anonymous, Oblivious Robots
on a Grid
Matthias Fischer, Daniel Jung(B), and Friedhelm Meyer auf der Heide
Computer Science Department, Heinz Nixdorf Institute, Paderborn University,
F¨urstenallee 11, 33102 Paderborn, Germany
{mafi,daniel.jung,fmadh}@uni-paderborn.de
Abstract. We consider a swarm of n autonomous mobile robots, dis-
tributed on a 2-dimensional grid. A basic task for such a swarm is the
gathering process: All robots have to gather at one (not predeﬁned) place.
A common local model for extremely simple robots is the following: The
robots do not have a common compass, only have a constant viewing
radius, are autonomous and indistinguishable, can move at most a con-
stant distance in each step, cannot communicate, are oblivious and do
not have ﬂags or states. The only gathering algorithm under this robot
model, with known runtime bounds, needs O(n2) rounds and works in
the Euclidean plane. The underlying time model for the algorithm is the
fully synchronous FSYNC model. On the other side, in the case of the
2-dimensional grid, the only known gathering algorithms for the same
time and a similar local model additionally require a constant memory,
states and “ﬂags” to communicate these states to neighbors in viewing
range. They gather in time O(n).
In this paper we contribute the (to the best of our knowledge) ﬁrst
gathering algorithm on the grid that works under the same simple local
model as the above mentioned Euclidean plane strategy, i.e., without
memory (oblivious), “ﬂags” and states. We prove its correctness and an
O(n2) time bound in the fully synchronous FSYNC time model. This
time bound matches the time bound of the best known algorithm for the
Euclidean plane mentioned above. We say gathering is done if all robots
are located within a 2 × 2 square, because in FSYNC such conﬁgura-
tions cannot be solved.
Keywords: Gathering problem · Autonomous robots
Distributed algorithms · Local algorithms · Mobile agents
Runtime bound · Swarm formation problems
1
Introduction
Swarm robotics considers large swarms of relatively simple mobile robots
deployed to some two- or three-dimensional area. These robots have very limited
This work was partially supported by the German Research Foundation (DFG)
within the Collaborative Research Centre “On-The-Fly Computing” (SFB 901).
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 168–181, 2017.
https://doi.org/10.1007/978-3-319-72751-6_13
www.ebook3000.com

Gathering Anonymous, Oblivious Robots on a Grid
169
sensor capabilities; typically they can only observe aspects of their local environ-
ment. The objective of swarm robotics is to understand which global behavior of
a swarm is induced by local strategies, simultaneously executed by the individ-
ual robots. Typically, the decisions of the individual robots are based on local
information only.
In order to formally argue about the impact of such local decisions of the
robots on the overall behavior of the swarm, many simple models of robots,
their local algorithms, the space they live in, and underlying time models are
proposed. For a survey see the book [12] by Flocchini et al.
A basic desired global behavior of such a swarm is the gathering process: All
robots have to gather at one (not predeﬁned) place. Local algorithms for this
process are deﬁned and analysed for a variety of models [1,3,5–7,12].
A common local model for extremely simple robots is the following: There
is no global coordinate system. The robots do not have a common compass,
only have a constant viewing radius, are autonomous and indistinguishable, can
move at most a constant distance in each step, cannot communicate, are fully
oblivious and do not have ﬂags or lights to communicate a state to others. In this
very restricted robot model, a robot’s decision about its next action can only be
based on the current relative positions of the otherwise indistinguishable other
robots in its constant sized viewing range, and independent on past decisions or
information (oblivious).
The only gathering algorithm under this robot model, with known runtime
bounds, needs O(n2) rounds and works in the Euclidean plane. The underlying
time model for the algorithm is the fully synchronous FSYNC model (see [2,12]).
In FSYNC, all robots are always active and do everything synchronously. Time
is subdivided into equally sized rounds of constant lengths. In every round all
robots simultaneously execute their operations in the common look-compute-
move model [4] (Sect. 3).
In the discretization of the Euclidean plane, the two-dimensional grid, under
the same time and robot model, no runtime bounds for gathering are known.
The concept of the Euclidean algorithm [6] cannot be transferred to the grid,
because it must be able to compute the center of the minimum enclosing circle of
the robots in its viewing range (and then move to this position) and furthermore
move arbitrary small distances. This clearly is impossible on the grid. Instead,
completely diﬀerent approaches are needed.
In the only known gathering algorithms on the grid under the same time
and a similar robot model, the robots need states (so-called runs) and ﬂags to
communicate these states to neighbors, and have to be able to memorize a ﬁxed
number of steps [1,5]. There, a robot with an active run state can further move
this state to a neighboring robot. This allows coordinated robot operations over
several consecutive rounds. In [1,5], these operations are crucial for total running
time proof (O(n) rounds).
In the current submission, we drop the additional robot capabilities memory
and ﬂags or lights to communicate a state to others. Then, analogously to the
Euclidean strategy [6], explained above, a robot’s decision about its next action

170
M. Fischer et al.
can only be based on the current relative positions of the otherwise indistin-
guishable other robots in its constant sized viewing range, and independent on
past decisions or information (oblivious). Especially coordinated robot opera-
tions over several consecutive rounds that are used in [1,5] cannot be performed
under this more restricted model.
To the best of our knowledge we present the ﬁrst strategy under this
restricted model on the grid and prove a total running time of O(n2) rounds
which complies with the best known running time for the Euclidean strate-
gies in this model [6]. More precisely, the running time of our strategy depends
quadratically on the outer boundary length of the swarm. The outer boundary
is the seamless sequence of neighboring robots that encloses all the others robots
inside.
We conjecture that Ω(n2) is a lower bound for the number of rounds
needed for our algorithm and, more generally, even for any algorithm within
our restricted model. At least for our algorithm, we conjecture that a worst
case instance is a conﬁguration with robots on the boundary of an axis-parallel
square. Experiments support this conjecture. Full version of our paper: [11].
2
Related Work
There is vast literature on robot problems researching how speciﬁc coordination
problems can be solved by a swarm of robots given a certain limited set of
abilities. The robots are usually point-shaped (hence collisions are neglected)
and positioned in the Euclidean plane. They can be equipped with a memory
or are oblivious, i.e., the robots do not remember anything from the past and
perform their actions only on their current views. If robots are anonymous, they
do not carry any IDs and cannot be distinguished by their neighbors. Another
type of constraint is the compass model: If all robots have the same coordinate
system, some tasks are easier to solve than if all robots’ coordinate systems are
distorted. In [13,14] a classiﬁcation of these two and also of dynamically changing
compass models is considered, as well as their eﬀects regarding the gathering
problem in the Euclidean plane. The operation of a robot is considered in the
look-compute-move model [4]. How the steps of several robots are aligned is given
by the time model, which can range from an asynchronous ASYNC model (e.g.,
see [4]), where even the single steps of the robots’ steps may be interleaved, to
a fully synchronous FSYNC model (e.g., see [2]), where all steps are performed
simultaneously. A collection of recent algorithmic results concerning distributed
solving of basic problems like gathering and pattern formation, using robots with
very limited capabilities, can be found in the book [12] by Flocchini et al.
One of the most natural problems is to gather a swarm of robots in a single
point. Usually, the swarm consists of point-shaped, oblivious, and anonymous
robots. The problem is widely studied in the Euclidean plane. Having point-
shaped robots, collisions are understood as merges/fusions of robots and inter-
preted as gathering progress [5–7]. In [3] the ﬁrst gathering algorithm for the
ASYNC time model with multiplicity detection (i.e., when a robot can detect if
www.ebook3000.com

Gathering Anonymous, Oblivious Robots on a Grid
171
other robots are also located at its own position) and global views is provided.
Gathering in the local setting was studied in [2]. In [18] situations when no
gathering is possible are studied. The question of gathering on graphs instead of
gathering in the plane was considered in [8,15,17]. In [20] the authors assume
global vision, the ASYNC time model and furthermore allow unbounded (ﬁnite)
movements. They show optimal bounds concerning the number of robot move-
ments for special graph topologies such as trees and rings.
Concerning the gathering on grids, in [10] it is shown that multiplicity detec-
tion is not needed and the authors further provide a characterization of solvable
gathering conﬁgurations on ﬁnite grids. In [21], these results are extended to inﬁ-
nite grids, assuming global vision. The authors characterize gatherable grid con-
ﬁgurations concerning exact gathering in a single point. Under their robot model
and the ASYNC time model, the authors present an algorithm which gathers
gatherable conﬁgurations optimally concerning the total number of movements.
Assuming only local capabilities of the robots, esp. only local vision and no
compass, makes gathering challenging. For example, a given global vision, the
robots could compute the center of the globally smallest enclosing square or circle
and just move to this point. For gathering with presence of a global compass, the
authors in [19] provide a simple gathering algorithm: The robots from the left
and right swarm boundaries keep moving towards the swarm’s inside. In some
kind of degenerated cases, instead the robots on the top and bottom boundaries
do this.
In the FSYNC time model, the total running time is a quality measure
of an algorithm. In this time model there exist several results that prove run-
time bounds [1,5,6,9,16]. For local robot models, the locality strongly restricts
the robot capabilities: no global control, no unique IDs, no compass, only local
vision (i.e., they can only see other robots up to a constant distance) and no
(global) communication. But even under this strongly local model, the presence
of remaining local capabilities such as allowing a constant number of states,
constant memory or locally visible states (ﬂags, lights), can drastically change
running times by even more than the factor n [1,5,16]. The price for this improve-
ment then is many more complicated strategies.
One example are strategies that maintain and shorten a communication
chain between an explorer and a base camp. The Hopper and Manhattan Hop-
per strategies [16] solve this problem in time O(n), in the Euclidean plane and
on the grid, respectively, using robots with a constant number of states, a con-
stant memory and the capability to communicate states to local neighbors (ﬂags,
lights). Without these additional robot capabilities, the simple Euclidean Go-
To-The-Middle strategy [9] needs notably more time O(n2 log(n)) for solving
the same problem. Concerning the gathering under this restricted model, the
simple, Euclidean Go-To-The-Center strategy [6] needs time O(n2). (A faster
strategy for the Euclidean plane does not exist, yet, and it is still unknown if this
bound is tight.) On the grid, two asymptotically optimal O(n) strategies exist
that, solve the gathering of an arbitrary connected swarm [5] and the gathering
of a closed chain of robots [1], respectively. Like the above communication chain

172
M. Fischer et al.
strategies, they require more complex robots with a const. number of states,
a const. memory and the capability to communicate states to local neighbors
(ﬂags, lights). Strategies without these additional capabilities do not exist, yet.
In the present paper, we deliver such an algorithm that uses the same strongly
restricted model as the Euclidean Go-To-The-Center gathering strategy [6]. Our
strategy gathers in time O(|outer boundary|2) ⊆O(n2), where |outer boundary|
denotes the length of the swarm’s outer boundary (cf. Fig. 4(i)) which naturally
is ∈O(n). This is comparable to the O(n2) bound for the Euclidean gathering
[6]. We conjecture that O(|outer boundary|2) is also tight on the grid.
3
Our Local Model
Our mobile robots need very few and simple capabilities: A robot moves on a
two-dimensional grid and can change its position to one of its eight horizontal,
vertical or diagonal neighboring grid cells. It can see other robots only within
a constant viewing radius of 7 (measured in L1-distance). We call the range of
visible robots the viewing range. Within this viewing range, a robot can only see
the relative positions of the viewable robots. The robots have no compass, no
global control, and no IDs. They cannot communicate, do not have any states
(no ﬂags, lights) and are oblivious.
Our algorithm uses the fully synchronous time model FSYNC, in that all
robots are always active and do everything synchronously. Time is subdivided
into equally sized rounds of constant lengths. In every round all robots simulta-
neously execute their operations in the common look-compute-move model [4],
which divides one operation into three steps. Every round contains only one cycle
of these steps: In the look step, the robot gets a snapshot of the current scenario
from its own perspective, restricted to its constant-sized viewing range. During
the compute step, the robot computes its action, and eventually performs it in
the move step. If a robot has moved to an occupied grid cell, the robots from
then on behave like one robot. We say they merge and remove one of them.
We say gathering is done if all robots are located within a 2 × 2 square,
because such conﬁgurations cannot be solved in our time model.
The swarm must be connected. In our model, two robots are connected if
they are located in horizontal or vertical neighboring grid cells. The operations
of our algorithm do not destroy this connectivity.
4
The Algorithm
A robot decides to hop on one of its 8 neighboring grid cells only dependent
on the current robot positions within its viewing range. We distinguish diag-
onal (Sect. 4.1) and horizontal/vertical (Sect. 4.2) hops. The hops are intended
to achieve the gathering progress by modifying the swarm’s outer boundary.
Figure 4(i) deﬁnes the swarm’s boundaries: Black and hatched robots are bound-
ary robots. The boundary on which the black robots are located borders the
swarm and is called the swarm’s outer boundary. In the ﬁgure, all other robots
are colored grey. White cells are empty.
www.ebook3000.com

Gathering Anonymous, Oblivious Robots on a Grid
173
4.1
Diagonal Hops
If a robot r (marked black in Figs. 1 and 2) checks whether it can execute a
diagonal hop, it compares the patterns of Figs. 1 and 2 to the robot positions
in its viewing range: Robot r checks if one of the Diag-{A, B} Hop patterns
matches the current scenario from its own perspective. Patterns that are created
by an arbitrary horizontal and vertical mirroring and an arbitrary 90 ◦rotation
of the three patterns of Fig. 1 are also valid and have to be checked.
(i)
(iii)
Diag-A
Diag-B
(ii)
D
mirror axis
upper
lower
left
right
Fig. 1. Hop patterns: One of the Diag-A or Diag-B Hop patterns must match the
relative robot positions within the black robot’s viewing range. This is the hop criterion,
necessary for allowing the black robot to perform the depicted diagonal hop. In this
paper, the notation Diag-{A, B} means “Diag-A or Diag-B”.
Depending on the matching Hop pattern the robot does the following:
1. If a Diag-A pattern matches, then robot r checks if, using the same rotation
and mirroring, any of the Inhibit patterns match the upper-right area of its
viewing range. If at least one Inhibit pattern matches, then the Diag-A hop
of robot r is not executed. Otherwise, if none of the Inhibit patterns match,
the robot r hops according to the matching Diag-A pattern.
2. If the Diag-B pattern matches, then the robot r checks if, using the same
rotation and mirroring, also any of the Inhibit patterns match the upper-right
area and the lower-left area of its viewing range. However, in case of the lower-
left area, the Inhibit pattern has to be mirrored at the diagonal mirroring axis
D shown in Fig. 1(iii). If for both areas matching Inhibit patterns have been
found, then the Diag-B hop of robot r is not executed. Otherwise the robot
r hops according to the matching Diag-B pattern.
upper
lower
left
right
Inhibit 1:
Inhibit 2:
Inhibit 3:
Fig. 2. Inhibit patterns: Patterns that, in case they match, inhibit the black robot’s
hop.

174
M. Fischer et al.
4.2
Horizontal and Vertical Hops (HV Hops)
Robots can also hop in vertical or horizontal direction (HV hop). We allow these
hops for length 1 and 2 (cf. Fig. 3). For length 2, horizontal or vertical hops,
respectively, are a joint operation of two neighbouring robots. If for a robot
a horizontal and a vertical HV hop apply at the same time (see b⋆of Fig. 3),
then it instead performs a diagonal hop as shown in the ﬁgure. After a HV hop,
every target cell contains at least two robots. We let these robots merge: i.e., we
remove all but one of the robots at the according cell.
before:
after:
length 1:
length 2:
a)
b)
b⋆)
before:
after:
before:
after:
length 2:
Fig. 3. The black robots simultaneously hop downwards. Afterwards, robots that are
located at the same position merge.
Diag-{A, B} and HV hops are executed simultaneously in the same step of
the algorithm. For the pseudocode, see [11].
5
Measuring the Gathering Progress
The gathering progress measures that we will use for the analysis of our strategy
are heavily dependent on the length and shape of the swarm’s outer boundary. In
order to analyze these measures, we need the terms boundary, outer boundary,
length, as well as convex and concave vertices.
Swarm’s boundary. The swarm’s boundary is the set of all robots that have
at least one empty adjacent cell in a horizontal, vertical, or diagonal direction.
Figure 4(i) shows an example: Black and hatched robots are boundary robots.
The empty cells contain no robot and are colored in white. When speaking about
a subboundary, we mean a connected sequence of robots of some boundary.
Swarm’s outer boundary. The swarm’s outer boundary is the boundary that
borders on the outside of the swarm. In Fig. 4(i) the black robots belong to
outer boundary. All other robots are not part of the boundaries, i.e., they have
an adjacent robot in all directions (horizontal, vertical, and diagonal). In Fig. 4(i)
they are colored grey.
Outer boundary’s length. We measure the outer boundary’s length as fol-
lows: We start at a cell of the outer boundary and perform a complete walk
along this boundary while we deﬁne the length as the total number of steps
that we performed during this walk. This means that if the swarm is hourglass-
or cross-shaped, for example, some robots are counted multiple (up to four)
www.ebook3000.com

Gathering Anonymous, Oblivious Robots on a Grid
175
INSIDE
outer boundary
(i)
(ii)
concave
convex
(v)
outer boundary
∗
±180◦
(iv)
∗∗∗∗
(vi)
(iii)
∗
outer
boundary
Fig. 4. (i): Deﬁnition of the (outer) boundary. Outer boundary: black robots; (ii):
Deﬁnition of convex and concave vertex. Convex vertices: fat curves; (iii): Cross shape.
The “∗” marked robot is counted four times; (iv): Hourglass shape: The “∗” marked
robots are counted twice; (v): ±180◦rotation. The “∗” marked robot is counted twice.;
(vi): During the gathering, inner bubbles can be developed.
times (cf. Fig. 4(iii, iv)). Furthermore, robots on which a turn by ±180◦is per-
formed during the walk are counted twice (cf. Fig. 4(v)). We denote this length
by |outer boundary|.
Convex and concave vertices. On the boundary we further distinguish convex
and concave vertices. A vertex of the boundary is a robot that looks like a corner.
In Fig. 4(ii) fat curves mark convex vertices of the swarm’s outer boundary, while
the thin curves mark the concave vertices of the swarm’s outer boundary.
Outline of the running time proof—How we get gathering progress. We
distinguish three kinds of progress measures that help us to prove the quadratic
running time.
Boundary: Length of the swarm’s outer boundary.
Convex:
Diﬀerence between the number of convex vertices on the swarm’s
outer boundary and its maximum value.
Area:
Included area.
We have designed the hops in such a way that the length of the outer boundary
(Boundary) never increases. But it can remain unchanged over several rounds.
Then, instead, we measure the progress by Convex. As we draw robots as squares,
the total number of convex vertices on the swarms’ outer boundary is naturally
upper bounded by its maximum value 4|outer boundary|. Convex is the diﬀer-
ence between this maximum value and the actual number of convex vertices on
the outer boundary. We will show that also Convex never increases.
In rounds in which both Boundary and Convex do not achieve progress, we
instead measure the gathering progress by Area. We measure Area as the number
of robots on the swarm’s outer boundary plus the number of inside cells (occupied
as well as empty ones). In contrast to the other progress measures, Area does not

176
M. Fischer et al.
decrease monotonically in general, but we show that it decreases monotonically
in rounds without Boundary and Convex progress. We upper bound the size by
that the Area can instead be increased during other rounds and show that this
makes the total running time worse at most by a constant factor.
All three measures depend only on the length of the swarm’s outer boundary.
While Boundary and Convex are linear, Area is quadratic. This then leads us
to a total running time O(|outer boundary|2).
6
Correctness and Running Time
In this section, we formally prove the correctness of the progress measures and
ﬁnally the total running time (Theorem 1).
6.1
Progress Measure Boundary
Lemma 1. During the whole gathering, Boundary is monotonically decreasing.
Proof. As the deﬁnition of HV hops requires that the robots hop onto occupied
cells, such hops naturally cannot increase the number of robots on the outer
boundary. So we consider Diag-{A, B} hops in which robots hop towards the
swarm’s outside. In order to increase the number of robots on the outer boundary,
the target cell of such hops must be empty. But then, the hopped robot has also
been part of the outer boundary before the hop, so that the boundary length
did not increase.
⊓⊔
6.2
Impact of Inhibit Patterns: Collisions
For the proofs of the progress measures Convex and Area, we need a deeper
insight why certain robot hops are inhibited by Inhibit patterns (cf. Fig. 2).
When proving that the Convex progress is monotonically decreasing (Lemma 2),
we analyze the change of the total number of convex vertices that is induced by
the robot hops. This is only possible if certain simultaneously hopping robots
are not too close together. Inhibit patterns ensure this minimum distance.
Cf. Figs. 1 and 2. From a more global point of view, the Inhibit patterns
ensure that the black robot only performs its Diag-A or Diag-B hop, respectively,
if the next robot(s) at distance 2 along the boundary does (do) not perform a
hop in the opposite direction. If the hop of the black robot is blocked by Inhibit
patterns, we will say the robots collide. This will be used in the proofs of our
progress measures. Figure 5 shows signiﬁcant collision examples: (i): For both, r
and r′ Diag-A matches. Without inhibition patterns, r would hop to the lower
right, while r′ would hop in the opposite direction to the upper left. But the
Inhibit 1 pattern inhibits the hop of r: r collides with r′. Analogously also the
hop of r′ can be inhibited. (ii): If Diag-B matches for r, then the hop is inhibited
if concerning both r′ and r′′ an inhibition pattern matches. In this example, for
r′ a Diag-A pattern and for r′′ a Diag-B pattern could else enable hops in the
www.ebook3000.com

Gathering Anonymous, Oblivious Robots on a Grid
177
opposite direction than the hop of r, but the matching Inhibit 1 and 2 patterns
inhibit the hop of r: r collides with r′, r′′. A more detailed analysis of collisions
is provided in the proofs for Lemma 3.
(i)
r
r′
Inhibit 1
Diag-A
r′
r′′
r
Diag-B
Inhibit 3
Inhibit 1
(ii)
Fig. 5. Collisions. (i): For r, Diag-A and Inhibit 1 matches. r does not hop. (ii): For
r, Diag-B and Inhibit 1 and 3 match. r does not hop.
6.3
Progress Measure Convex
Lemma 2. During the whole gathering, Convex is monotonically decreasing.
Proof. If we say “convex/concave vertices”, we consider only the outer boundary.
First, we analyze the HV hops. Here, a HV hop can reduce the number of convex
vertices by at most 2. At the same time, the outer boundary becomes shorter by
at least 2. Then, Convex either remains unchanged or decreases.
Concerning the Diag-{A, B} hops, we look at Fig. 6: The ﬁgure shows how
the diagonal hops can (locally) change the number of convex vertices on the
outer boundary. (In the ﬁgure, (ii) shows the hops from (i), but for switched
INSIDE and OUTSIDE.) In all cases, the Inhibit patterns ensure that the robots
a, a′ do not move (cf. Sect. 6.2). In the ﬁgure, we distinguish Diag-A and Diag-B
hops, while A, A⋆refer to Diag-A and B, B⋆to Diag-B. We distinguish the case
that the robot b does not hop (A, B) and the other case, that it performs a hop
(A⋆, B⋆). The result of the case distinction is that in column (i) the number of
convex vertices never decreases. In column (ii), this is also the case if the white
marked cells s, t, u are empty.
If instead not all of s, t, u are empty, the number of convex vertices might also
become smaller. But even in this case, still Convex progress does not increase:
We now show that then also |outer boundary| becomes smaller as well as the
maximum value for the total number of convex vertices, so that by deﬁnition
Convex progress is not increased, i.e., it still behaves monotonically: Fig. 7 shows
the relevant cases. With reference to Fig. 6(ii), all hops are performed towards
the swarms’ outside. In column (i), only the cell s contains a robot. There we
see, that in all cases the outer boundary becomes shorter by at least 2. In case
of A⋆, this can be even more than 2 for the case that after the hop the cell below
b (in Fig. 6(ii), this was the cell u) also contains a robot. Column (ii) shows the
cases where cell t is (also) occupied. Because the swarm is always connected, the
hatched robot must be connected to the rest of the swarm. This can be either
via the subboundary α or β. The hop shortens the outer boundary by forming
inner bubbles (Fig. 4(vi)).
⊓⊔

178
M. Fischer et al.
OUTSIDE
INSIDE
a
b
C
ﬁxed
OUTSIDE
INSIDE
a
b
C
ﬁxed
a
b
C′
INSIDE
OUTSIDE
C′
C′′
OUTSIDE
INSIDE
OUTSIDE
INSIDE
b
ﬁxed
a
b
a
b
C
ﬁxed
C′
a
b
C
C′
OUTSIDE
INSIDE
C
C′
C′′
C′′
C′′′
C⋆
OUTSIDE
INSIDE
C
b
a
a
ﬁxed
ﬁxed
a
b
C′
C′′
b
round i
round i + 1
round i
round i + 1
INSIDE
OUTSIDE
b
a
ﬁxed
b
A)
A⋆)
B)
B⋆)
C
C′
C′′
ﬁxed
a
b
a
b
C′
t
s
t
s
u
t
s
u
s
u
s
(i)
(ii)
a′
a′
a′
a′
a′
a′
a′
a′
#conv(i + 1) = #conv(i)
#conv(i + 1) = #conv(i)
#conv(i + 1) = #conv(i) + 1
#conv(i + 1) = #conv(i) + 1
#conv(i + 1) = #conv(i) + 1
#conv(i + 1) = #conv(i) + 1
#conv(i + 1) = #conv(i) + 1
#conv(i + 1) = #conv(i) + 1
Fig. 6. Local eﬀect of all kinds of hops on the number of convex vertices on the outer
boundary. C, C′, C′′ denote the counted convex vertices. #conv(i) denotes the number
of convex vertices in round i.
6.4
Progress Measure Area
The third progress measure Area does not behave monotonically. It can be
increased during rounds where we get Boundary or Convex progress. But we
use it for estimating the number of the remaining rounds (Lemma 3). And in the
proof of Theorem 1 we show that the increased amount of the Area progress mea-
sure does not worsen the asymptotic running time. For the proof of Lemma 3,
see [11].
Lemma 3. If in a step of the gathering process neither Boundary nor Convex
has progress, then instead Area has progress by at least −8.
6.5
Total Running Time
Now we can combine all three progress measures Boundary, Convex and Area
for the running time proof (Theorem 1).
Theorem 1. A connected swarm of n robots on a grid can be gathered in
O(|outer boundary|2) ⊆O(n2) many rounds.
Proof. Let B be the initial length of the swarm’s outer boundary. We know from
Lemma 1 that Boundary decreases monotonously. Then, progress in Boundary
happens at most B times. By Lemma 2, Convex also decreases monotonously.
As every robot on the swarm’s outer boundary can provide at most 4 convex
vertices, Convex progress happens at most 4B times.
www.ebook3000.com

Gathering Anonymous, Oblivious Robots on a Grid
179
OUTSIDE
INSIDE
OUTSIDE
INSIDE
a
b
C
a
b
INSIDE
round i
round i + 1
INSIDE
OUTSIDE
b
a
b
A)
A⋆)
B)
B⋆)
C
a
b
a
b
OUTSIDE
INSIDE
a
b
OUTSIDE
INSIDE
a
b
a
b
C′′
a
b
round i + 1
b
C′′
a
b
|bndry(i + 1)| = |bndry(i)| −2
a
b
C′′
|bndry(i + 1)| ≤|bndry(i)| −2
a
b
INSIDE
INSIDE
|bndry(i + 1)| = |bndry(i)| −2
a
b
C
ﬁxed
a
b
ﬁxed
C
ﬁxed
ﬁxed
C
ﬁxed
ﬁxed
OUTSIDE
round i
round i + 1
round i + 1
|bndry(i + 1)| = |bndry(i)| −2
α
β
α
β
s
t
t
s
t
t
u
s
(i)
(ii)
a′
a′
a′
a′
a′
a′
|bndry(i + 1)| < |bndry(i)|
|bndry(i + 1)| < |bndry(i)|
Fig. 7. Diagonal hops can change the outer boundary’s length. C, C′, C′′ denote rele-
vant convex vertices. #bndry(i) denotes the outer boundary length in round i.
We estimate the rounds without Boundary and Convex progress via the size
of the included area, i.e., the Area progress. By Lemma 3, we know that in
every round without Boundary and Convex progress, the area becomes smaller
by at least −8. But, Area is not a monotone progress measure, in rounds with
Boundary or Convex progress, the included area can increase: While HV hops
cannot increase the included area, Diag-{A, B} hops can. First, we assume that
the according Diag-{A, B} hops do not change the outer boundary length. Then,
every time Convex has progress, the area can become larger by at most B,
because every robot hop on the outer boundary can increase the area by at most
1. As Convex happens at most 4B times, this in total is upper bounded by 4B2.
If the outer boundary length changes, i.e., becomes shorter, then the included
area can increase (cf. proof of Lemma 2 and Fig. 4(vi)). Then, a Boundary
progress by ℓcan also increase the included area by ΔA ≤ℓ2 ≤ℓB. But as
Boundary progress is monotonically decreasing, the sum of all these ΔA is upper
bounded by B2.
Summing it up, during the whole process of the gathering, the area can be
increased by at most (4 + 1)B2 = 5B2. Together with the initial area of at most
B2, Area progress happens at most 6B2. Then, the gathering is done after at
most B + 4B + 6B2 rounds.
⊓⊔

180
M. Fischer et al.
References
1. Abshoﬀ, S., Cord-Landwehr, A., Fischer, M., Jung, D., Meyer auf der Heide, F.:
Gathering a closed chain of robots on a grid. In: IPDPS 2016, pp. 689–699 (2016).
https://doi.org/10.1109/IPDPS.2016.51
2. Ando, H., Suzuki, Y., Yamashita, M.: Formation and agreement problems for syn-
chronous mobile robots with limited visibility. In: ISIC 1995, pp. 453–460, August
1995
3. Cieliebak, M., Flocchini, P., Prencipe, G., Santoro, N.: Solving the robots gath-
ering problem. In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J.
(eds.) ICALP 2003. LNCS, vol. 2719, pp. 1181–1196. Springer, Heidelberg (2003).
https://doi.org/10.1007/3-540-45061-0 90
4. Cohen, R., Peleg, D.: Robot convergence via center-of-gravity algorithms. In:
Kr´alovi˘c, R., S´ykora, O. (eds.) SIROCCO 2004. LNCS, vol. 3104, pp. 79–88.
Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-27796-5 8
5. Cord-Landwehr, A., Fischer, M., Jung, D., Meyer auf der Heide, F.: Asymp-
totically optimal gathering on a grid. In: SPAA 2016, pp. 301–312 (2016).
http://doi.acm.org/10.1145/2935764.2935789
6. Degener, B., Kempkes, B., Langner, T., Meyer auf der Heide, F., Pietrzyk, P.,
Wattenhofer, R.: A tight runtime bound for synchronous gathering of autonomous
robots with limited visibility. In: SPAA 2011, pp. 139–148 (2011)
7. Degener, B., Kempkes, B., Meyer auf der Heide, F.: A local O(n2) gathering algo-
rithm. In: SPAA 2010, pp. 217–223 (2010)
8. Dessmark, A., Fraigniaud, P., Kowalski, D.R., Pelc, A.: Deterministic rendezvous
in graphs. Algorithmica 46(1), 69–96 (2006)
9. Dynia, M., Kutylowski, J., Lorek, P., Meyer auf der Heide, F.: Maintaining com-
munication between an explorer and a base station. In: IFIP TC10, pp. 137–146,
1 January 2006
10. D’Angelo, G., Di Stefano, G., Klasing, R., Navarra, A.: Gathering of robots on
anonymous grids without multiplicity detection. In: Even, G., Halld´orsson, M.M.
(eds.) SIROCCO 2012. LNCS, vol. 7355, pp. 327–338. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-31104-8 28
11. Fischer, M., Jung, D., Meyer auf der Heide, F.: Gathering anonymous, oblivious
robots on a grid. CoRR abs/1702.03400 (2017). http://arxiv.org/abs/1702.03400
12. Flocchini, P., Prencipe, G., Santoro, N.: Distributed Computing by Oblivious
Mobile Robots. Synthesis Lectures on Distributed Computing Theory. Morgan &
Claypool, San Rafael (2012)
13. Izumi, T., Souissi, S., Katayama, Y., Inuzuka, N., D´efago, X., Wada, K., Yamashita,
M.: The gathering problem for two oblivious robots with unreliable compasses.
SICOMP 41(1), 26–46 (2012)
14. Katayama, Y., Tomida, Y., Imazu, H., Inuzuka, N., Wada, K.: Dynamic compass
models and gathering algorithms for autonomous mobile robots. In: Prencipe, G.,
Zaks, S. (eds.) SIROCCO 2007. LNCS, vol. 4474, pp. 274–288. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-72951-8 22
15. Klasing, R., Markou, E., Pelc, A.: Gathering asynchronous oblivious mobile robots
in a ring. TCS 390(1), 27–39 (2008)
16. Kutylowski, J., Meyer auf der Heide, F.: Optimal strategies for maintaining a chain
of relays between an explorer and a base camp. TCS 410(36), 3391–3405 (2009)
17. Martˆınez, S.: Practical multiagent rendezvous through modiﬁed circumcenter algo-
rithms. Automatica 45(9), 2010–2017 (2009)
www.ebook3000.com

Gathering Anonymous, Oblivious Robots on a Grid
181
18. Prencipe, G.: Impossibility of gathering by a set of autonomous mobile robots.
TCS 384(2–3), 222–231 (2007)
19. Saadatmand, S., Moazzami, D., Moeini, A.: A cellular automaton based algorithm
for mobile sensor gathering. JAC 47(1), 93–99 (2016)
20. Di Stefano, G., Navarra, A.: Optimal gathering of oblivious robots in anonymous
graphs. In: Moscibroda, T., Rescigno, A.A. (eds.) SIROCCO 2013. LNCS, vol.
8179, pp. 213–224. Springer, Cham (2013). https://doi.org/10.1007/978-3-319-
03578-9 18
21. Di Stefano, G., Navarra, A.: Optimal gathering on inﬁnite grids. In: Felber, P.,
Garg, V. (eds.) SSS 2014. LNCS, vol. 8756, pp. 211–225. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-11764-5 15

A Continuous Strategy for Collisionless
Gathering
Shouwei Li, Christine Markarian,
Friedhelm Meyer auf der Heide, and Pavel Podlipyan(B)
Department of Computer Science, Heinz Nixdorf Institute, Paderborn University,
F¨urstenallee 11, 33102 Paderborn, Germany
{shouwei.li,christine.markarian,fmadh,pavel.podlipyan}@upb.de
Abstract. We consider continuous strategies for swarms of robots in the
Euclidean plane. In such a strategy, each robot continuously observes its
local neighborhood, and continuously adapts speed and direction follow-
ing a local rule. We present two main results. The ﬁrst deﬁnes a class
of strategies, the contracting strategies, that perform gathering in time
O(nd), where d is a diameter of the initial conﬁguration. Several well-
known strategies belong to this class. Our second result is about collisions
in such strategies. We present a contracting strategy which ensures that
no collisions occur. This strategy needs the robots to have some addi-
tional capabilities.
1
Introduction
The study of gathering swarms of robots has been ongoing since decades. The
goal here is to ﬁnd provably fast strategies that put together the robots, initially
distributed on the Euclidean plane, into a single predeﬁned point.
Given a group of n autonomous, dimensionless, deterministic, and anonymous
robots, with bounded viewing range, a Gathering algorithm is an algorithm that
eventually gathers all n robots into one point. Gathering algorithms studied thus
far have been analyzed in two diﬀerent time models: discrete and continuous.
While most of these consider the discrete time model in which robots act in
rounds (see [12] for a comprehensive survey), only few of them are analyzed
in the continuous time model. In the continuous model, robots continuously
adjust their speed and direction to the current relative positions of the robots
within their viewing range, while obeying a speed limit normalized to 1. The ﬁrst
Gathering algorithm for the continuous time model was presented by Gordon et
al. in [14] and then analyzed by Kempkes et al. in [15].
In many existing approaches (e.g., in [1,6,9–11]), a collision, i.e., a merge
or fusion during gathering, is often interpreted as a success for the Gathering
algorithm since it represents progress in gathering. In this paper, motivated by
applications in which collisions must be avoided, we provide Gathering algo-
rithms that can gather robots without collisions as well.
This work was partially supported by the German Research Foundation (DFG)
within the Collaborative Research Center “On-The-Fly Computing” (SFB 901) and
the International Graduate School “Dynamic Intelligent Systems”.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 182–197, 2017.
https://doi.org/10.1007/978-3-319-72751-6_14
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
183
In the discrete time model, Lukovszki et al. in [18] proposed collisionless
algorithms for a variant of the Gathering problem in which all robots know the
gathering point. In fact, the need to avoid collisions becomes clearer when robots
gain extent. Robots with an extent were ﬁrst studied by Czyzowicz et al. in [7].
In this work, robots are represented by unit disks instead of points. The aim of
gathering is to form a conﬁguration for which the union of all disks representing
the robots is connected. For the same model as in [7], a general solution for more
than four robots is presented by Agathangelou et al. in [2]. Furthermore, Pagli
et al. in [20] study a discrete (near-)Gathering problem with an asynchronous
activation model. This problem is solved for the point like chiral robots that
agree on common direction (i.e. agree on common coordinate system).
In the continuous time model, the ﬁrst attempts towards collisionless gather-
ing were given by Li et al. in [17] where we introduce and evaluate the continuous
Go-To-The-Gabriel-Center algorithm that gathers robots in a one-dimensional
Euclidean space without collisions in time O(n).
Our Contribution. In this paper, we study the Gathering problem in the
continuous time model and introduce a simple convergence criterion along with
a corresponding class of algorithms that perform gathering in time O(nd), where
d is a diameter of initial conﬁguration.
Moreover, we show that known algorithms belong to this class, and propose
two new ones, namely Go-To-The-Relative-Center algorithm (GTRC) and Safe-
Go-To-The-Relative-Center algorithm (S-GTRC). Both are contracting and thus
gather in quadratic time. The second one is proven to perform no collision. It
uses slightly more complex robots: They are non oblivious, chiral, and luminous
(i.e. they have visible external memory, proposed by Das et al. in [8]).
Our techniques are inspired by ideas from a number of previously known
approaches. GTRC extends the Go-To-The-Center algorithm (GTC) introduced
in [3], studied in the discrete time model, to the continuous time model variant.
We use the relative neighborhood graph proposed by Toussaint in [22] to modify
the original algorithm. Our modiﬁed algorithm considers only the neighbors
of a robots w.r.t. the relative neighborhood subgraph of the visibility graph.
Due to space constraints, the proofs are omitted and all Figures are moved to
Appendix A.3. The full version of the paper can be found in [16].
2
Problem Description
We consider the common robot model used in most previous approaches (e.g.,
[3,14,15,17]). We are given a set R = {r1, . . . , rn} of n autonomous mobile robots
with viewing range of 1. We denote by ri(t) ∈R2 the position of robot ri at time
t. Robots agree on the unit distance. Each robot has its own local coordinate
system. Robots are oblivious, meaning that they act depending only on the
information about the current point of time. They are anonymous, meaning that
they do not have IDs. They are silent, meaning that they do not communicate.
The Euclidean distance between two robots ri and rj at time t is represented
by |ri(t), rj(t)|. Two robots are open unit disk graph (open UDG) or unit disk

184
S. Li et al.
graph (UDG) neighbors at time t, if |ri(t), rj(t)| < 1. The set of robots that
consists of the robot ri itself and all its UDG neighbors at time t is called UDG
neighborhood of ri and denoted by UDGt(ri). The UDG deﬁned on all robots
at some point in time t is denoted by UDGt(R) = (R, Et), where (ri, rj) ∈Et
iﬀ|ri(t), rj(t)| < 1. We skip t in the notation of UDG neighborhood and UDG
unless it needs to be mentioned explicitly.
The disposition of robots at some point in time t on the plane is called
a conﬁguration. The disposition of robots at time t = 0 is called the initial
conﬁguration. Initial conﬁgurations are arbitrary except that the UDG over all
robots at time t = 0 (UDG0(R)) is connected and all robots have distinct
positions. The goal is to gather all robots at one point, which is not predeﬁned.
We consider the continuous time model, ﬁrst introduced by Gordon et al.
in [14] and later studied by Kempkes et al. in [15]. The velocity of the robot
depends solely on the relative positions of neighboring robots at the current
point of time. It may change in a non-continuous manner since robots measure
the relative positions of their neighbors without delay and instantly adjust their
own movement with respect to the measurements. The maximum speed of the
robot is assumed to be 1.
3
Contracting Algorithms
In this section, for continuous time model we introduce a class of algorithms
that perform gathering on the Euclidean plane in time O(nd), where d is the
diameter of the initial conﬁguration.
Let us ﬁrst deﬁne the progress measure. Let Ht(R) ⊂R2 be the closed convex
hull around the positions of all robots at time t. We are particularly interested
in robots that are corners of a convex hull. Namely, we consider the set of robots
CHt(R) = {ci ∈Ht(R) : αi(t) ∈[0, π), i ∈[1, k], k ≤n}, where n is the total
number of robots and k is the number of robots that belong to the boundary of
the convex hull and have internal angle αi(t) ∈[0, π). We refer to CHt(R) as
the corner set of the convex hull Ht(R). Unless explicitly needed, we skip t in
the notation of the convex hull and corner set.
Deﬁnition 1 (Contracting algorithm). In continuous time model a Gath-
ering algorithm for n robots on the Euclidean plane is a contracting if for every
time t such that cardinality of CHt(R) is strictly greater than 1, every robot from
CHt(R) moves with speed 1 in the direction that points into Ht(R).
If an algorithm is contracting, then we can bound the speed with which the
length of the convex hull is decreasing. Let l(t) be the length of the convex hull
boundary at time t. Using the corner set we express the length as follows:
l(t) =
k

i=1
ci, c(i mod k)+1
 ,
(1)
where k is the number of robots in the corner set. Let l′(t) be the speed with
which the length l(t) of the convex hull boundary changes. The length of the
convex hull boundary will be the progress measure for contracting algorithms.
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
185
Lemma 1. If a group of n robots executes a contracting Gathering algorithm
and the robots are not yet gathered (i.e. cardinality of CHt(R) is strictly greater
than 1), then the length l(t) of the convex hull boundary at any point in time t
decreases with speed l′(t) ≥8/n.
The proof of Lemma 1 is easily derived from the runtime analysis in [17]. It
is well known that the length of the convex hull boundary is O(d), where d is the
diameter of the initial conﬁguration. Hence, the theorem below follows directly.
Theorem 1. Every contracting Gathering algorithm solves the Gathering prob-
lem in time O(nd), where d is the diameter of an initial conﬁguration.
The main steps of a gathering algorithm are described as follows. Each robot
r continuously computes the target point T(r) and moves with speed 1 towards
it. If it reaches the target point, it stays there (follows the target point) until
the position of the target point changes discontinuously (e.g., target point may
“jump” due to new neighbors).
There are many ways to calculate the target point on the Euclidean plane.
Consider, for example, the Go-On-Bisector algorithm (GOB) proposed by
Gordon et al. in [14] and later studied by Kempkes et al. in [15]. In the Go-
On-Bisector algorithm, robots that are on the boundary of the local convex hull
(around their UDG neighborhood) move inside the local convex hull along the
bisector of the internal angle. It is not diﬃcult to see that if robots are not yet
gathered, then every robot r ∈CH(R) moves inside the convex hull H(R) with
speed 1. In other words, GOB is a contracting algorithm.
We can also calculate the target point by the average of the positions of
all robots in the neighborhood. This algorithm is known as the Go-To-The-
Gravity-Center algorithm (GTGrC), due to Cohen et al. in [5]. It is easy to see
that GTGrC is a contracting algorithm, as well.
Another way to calculate the target point is by the center of the minimum
enclosing circle around the UDG neighborhood of the robot. This algorithm is
known as the Go-To-The-Center algorithm (GTC), due to Ando et al. in [3]. The
continuous version of GTC is considered in [17] by Li et al. Using argumentation
from [17], we can easily prove that GTC is a contracting algorithm. Moreover, we
can calculate a minimum enclosing circle with respect to the unit Gabriel graph
due to Gabriel et al. in [13]. Consequently, the algorithm Go-To-The-Gabriel-
Center (GTGC) proposed in [17], that considers a unit Gabriel graph, is also a
contracting algorithm.
Note that the diameter of a connected UDG with n vertices is at most n −1.
Therefore, Theorem 1 and the above discussion yield the following.
Corollary 1. Given an initial conﬁguration that is a connected UDG, a con-
tracting Gathering algorithm solves the Gathering problem in time O(n2).
In the next section, we are going to present one more contracting algorithm,
namely GTRC. This is modiﬁcation of GTC proposed by Ando et al. in [3]. In
this algorithm, instead of the UDG neighborhood we calculate minimum enclos-
ing circle with respect to the unit Relative neighborhood graph proposed by
Toussaint in [22].

186
S. Li et al.
4
Go-To-The-Relative-Center Algorithm (GTRC)
We take another viewpoint on the well-known Go-To-The-Center algorithm due
to Ando et al. in [3]. In the Go-To-The-Center algorithm, robots move towards
the center Tt(r) of the minimum enclosing circle Ct(r), which is the smallest
circle that contains all the robots in UDGt(r).
Let us consider a robot r ∈R at some point in time t and its UDG
neighborhood UDGt(r). It is shown in [4] that either (I) there are two robots
m1, m2 ∈UDGt(r) on the circumference of Ct(r) such that the line segment
(m1m2) is the diameter of Ct(r) or (II) there are three points m1, m2, m3 ∈
UDGt(r) such that Ct(r) circumscribes △m1m2m3 and the center of Ct(r) is
inside △m1m2m3. Accordingly, we refer to the set MECt(r) = {m1, m2} or
MECt(r) = {m1, m2, m3} as the minimum enclosing set of robot r at time t.
We say that the robots of MECt(r) form the minimum enclosing circle Ct(r) of
robot r at time t. In fact, robot r might belong to its own minimum enclosing
set (e.g., MECt(r) = {m1, r}).
Note that the minimum enclosing circle of a point set is unique [4] and can
be found in linear time in the Euclidean space of any constant dimension [19].
In case there is more than one minimum enclosing set MECt(r) that may form
Ct(r), then we assume that the robot selects one of them arbitrarily. Unless
explicitly needed, we skip t in the notation of minimum enclosing set and circle.
Instead of calculating C(r) with respect to UDG(r), we use the Relative
neighborhood graph proposed in [22]. The latter is deﬁned in the two-dimensional
Euclidean space as follows:
Deﬁnition 2 (Relative neighborhood graph criterion). Any two robots u,
v are connected iﬀthere does not exist any robot w ∈R satisfying |u, w| < |u, v|
and |v, w| < |u, v|.
We denote by RNGt(r) (at time t) the subgraph obtained from the UDG
neighborhood UDGt(r) by applying the Relative neighborhood graph criterion.
We call RNGt(r) the unit Relative graph neighborhood of robot r. The Rela-
tive neighborhood graph (RNG) deﬁned on all robots at time t is denoted by
RNGt(R).
The set of points on the Euclidean plane that corresponds to the intersection
of open unit disks of all robots in RNG(r) is denoted by Q(r). The set of points
Q(r) is open and convex since it is the intersection of open unit disks that
are convex [21]. The circle CQ(r) with center at r inscribed into Q(r) is the
connectivity circle. The radius of connectivity circle is denoted by ρQ. Unless
needed explicitly, we skip t in the notation of the Relative neighborhood graph,
neighborhoods, etc.
Now we are ready to present the GTRC. Its pseudo-code can be found as
Algorithm 2 in Appendix A.2.
4.1
Correctness and Runtime Analysis of GTRC
We begin this section with a simple proposition and then move to the connec-
tivity property of GTRC.
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
187
Proposition 1. For every robot r ∈R at any ﬁxed point in time, T(r) ∈Q(r).
Proposition 1 holds since the radius of C(r) is less than 1 and C(r) encircles
all the robots in RNG(r). The argumentation of the connectivity property is
similar to the one used in [17] for Go-To-The-Center algorithm (GTC) and Go-
To-The-Gabriel-Center algorithm (GTGC). The proof of Lemma 2 is based on
contradiction by assuming that connectivity does not hold.
Lemma 2. Given a group of robots R on the Euclidean plane executing GTRC.
If {u, w} is an edge in the open Relative neighborhood graph RNG(R) at time
0, then there is a path from u to w in RNG(R), ∀t ≥0.
Using the argumentation in [17], we can show that, at any point of time t
before the robots gather, the robots of CH(R) move only inside the convex hull,
with speed 1. Therefore GTRC is a contracting algorithm. Moreover, the length
of the convex hull boundary around the initial conﬁguration is shown, in [17],
to be not greater than 2(n −1), where n is the number of robots. Hence, we
conclude the following theorem.
Theorem 2. A group of n robots executing GTRC gathers in time O(n2).
4.2
Collisions During Gathering with GTRC
In this section, we show that GTRC performs gathering with collisions. Recall
that if two or more robots have the same position at the same point in time,
then there is collision. We refer to the set of robots in the minimum enclosing set
MEC(r) of a robot r ∈R as a crash point. We are able to show that collisions
in GTRC take place only at crash points.
Deﬁnition 3 (Crash point). For any robot r with minimum enclosing set
MEC(r), the midpoint between any two robots m1m2 ∈MEC(r) is a crash
point p(r). We say that robots m1 and m2 deﬁne the crash point p(r).
Note that if a minimum enclosing set consists of three robots MEC(r) =
{m1, m2, m3}, then the midpoint of every edge in △m1m2m3 is a crash point.
The set of Relative neighborhood edges adjacent to robot r is denoted as
ERNG(r). The set of all edges of the Relative neighborhood graph is denoted by
ERNG(R). We deﬁne, in the same way, the set EUDG(r) of the unit disk graph
edges adjacent to robot r and the set EMEC(r) of the Relative neighborhood
edges between robot r and the members of the minimum enclosing set MEC(r).
For the Relative neighborhood graph RNG(R) = (R, E), we deﬁne the set-
valued map R : X ⇝Y , where X ⊂E and Y ⊂R2 relate the RNG edges to the
area occupied by the corresponding RNG lenses. For example, R({r, u}, {r, w})
is the union of the RNG lenses that correspond to the RNG edges {r, u}, {r, w}.
Deﬁnition 4 (Minimum enclosing set cover). The convex hull around robot
r and the members of MEC(r) form the minimum enclosing set cover K(r) of
robot r ∈R.

188
S. Li et al.
Let us now consider a robot r ∈R and its minimum enclosing sets. We show
that, in any case, the minimum enclosing set cover is a subset of the union of the
RNG lenses that correspond to the RNG edges between robot w and the robots
of MEC(w), namely K(r) ⊂R(EMEC(r)). Using basic geometric arguments,
we conclude Lemma 3.
Lemma 3. If MEC(r) = {m1, m2, m3} is the minimum enclosing set over
RNG(r), then △m1m2m3 is completely covered by the RNG lenses that cor-
respond to the RNG edges between r and the robots of MEC(r).
We can show similar results as to Lemma 3 for the minimum enclosing sets
with diﬀerent structures.
Lemma 4. If MEC(r)
=
{m1, m2} is the minimum enclosing set over
RNG(r), then △rm1m2 (Fig. 1b) is completely covered by the RNG lenses
between r and the robots of MEC(r).
Lemma 5. If MEC(r) = {m1, m2, r} is the minimum enclosing set over
RNG(r), then △rm1m2 is completely covered by the RNG lenses between r and
the robots of MEC(r).
It remains to consider the case where MEC(r) = {m1, r} is the minimum
enclosing circle over RNG(r). We observe that the minimum enclosing circle
C(r) is a proper subset of the RNG lens between m1 and r. This observation
together with Lemmata 3, 4, and 5 yield the following corollary.
Corollary 2. The union of the RNG lenses between robot r and the robots of
MEC(r) is a superset of the minimum enclosing set cover K(r), i.e. K(r) ⊂
R(EMEC(r)).
Note that the RNG lenses between the unit disk graph neighbors that satisfy
the RNG criterion do not contain any other robots. If there is a robot inside one
of the RNG lenses, then the corresponding part of the RNG will change.
Let us now consider, in details, the collisions that occur between the robots
executing GTRC. The set of robots M ⊂R that has a collision at time t∗is
represented by a single robot u for any t ≥t∗. We call u the representative of M.
Observation 1 (Early collision). The set of robots M ⊂R had a collision
at time t∗if the minimum enclosing circle around the RNG neighborhood of the
representative u has a diameter greater than zero.
Note that the opposite of early collision a ﬁnal collision. In other words,
assume that the set of robots M ⊆R had a collision at time t∗and the min-
imum enclosing circle around the RNG neighborhood of the representative u
has diameter zero. This implies that, after the ﬁnal collision, there are no other
robots in the unit disk graph neighborhood of the representative u. There are
also no robots other than those in the unit disk graph neighborhood of robot
u due to the connectivity property of S-GTRC (shown in Lemma 2). Thus, if
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
189
ﬁnal collision takes place, then M = R. Obviously, there can only be one ﬁnal
collision. We say that the gathering is collisionless if there are no early collisions.
In the next Lemma 6, we utilize Corollary 2 to show that a robot w can have
early collision only at the crash point p(w). The proof is a result of checking
carefully all possible dispositions of w and MEC(w).
Lemma 6. Let us consider robot w during arbitrary time interval [a, b], such
that during this interval, diameter dw of the minimum enclosing circle of robot
w is strictly greater than zero. If robot w collides with some other robots M ⊂R
during [a, b], then:
1. collision takes place at the crash point p(w) of robot w;
2. robot w does not belong to its own minimum enclosing set MEC(w).
5
Collisionless Gathering with Extended Robot Model
In this section, we show that it is possible to perform gathering without collision
by allowing robots to have some additional capabilities.
We extend the robot model and design the Safe-Go-To-The-Relative-Center
algorithm (S-GTRC) using the contracting conditions from Lemma 1 and the
structural properties from Lemma 6. The goal of S-GTRC is to gather without
early collisions all the robots at one not predeﬁned point for any initial conﬁgu-
ration. The initial conﬁguration is arbitrary except that UDG0(R) is connected
and all the robots have distinct positions.
The extended robot model is described as follows. The viewing range of
a robot is 2. This will be needed to avoid collisions. Thus, a robot can see the
UDG neighbors of its UDG neighbors. Note that S-GTRC preserves connectivity
with respect to UDG. Open two-unit disk graph is deﬁned analogously to open
unit disk graph. For all robots in R, we deﬁne 2-UDG(R) = (R, 2-Et), where
(ri, rj) ∈2-Et iﬀfor ri and rj, it holds that |ri(t), rj(t)| < 2.
As in the common model, robots are anonymous and each robot has a local
coordinate system that is not aligned with the coordinate systems of other
robots. Unlike in the common model, robots here are chiral, i.e., they all agree
either on left- or right-hand orientation. Robots are equipped with synchronized
clocks. Robots are luminous, i.e. they have one bit of visible external memory
like in [8] by Das et al. The maximum speed of the robot is assumed to be s ≥1.
Next, we describe S-GTRC and show that it performs gathering in the con-
tinuous time model for the extended robot model without early collisions, in
time O(n2).
The main idea of S-GTRC is described as follows. Robots are separated into
two groups/states: regular and safe. In the regular group, robots execute GTRC
and do not take into account the robots in the safe group. If some robots in
the regular group are about to collide, at least one of them switches to the safe
state and independently from other robots, moves towards some speciﬁc, closely
situated ﬁxed point. This point is selected in such a way that collision is not
possible. The state of the robot is automatically visible to its neighbors via the
visible external memory.

190
S. Li et al.
From Lemma 6 we know that robot r collides only at the crash point of some
other robot w. Therefore, shortly before collision, both robots r and w together
with their target points T(w) and T(r) are inside the relatively small ball B with
the center at the crash point p(w) and radius 1/2m|m1, m2|, where m is a positive
parameter. Ball B is depicted in Fig. 3. In order to avoid collision, we let at least
one of the robots move independently from the other robots. For a short period
of time, the target point of the robot will be selected from the circle D with
radius 1/k|w, r| centered at the position of r, where k > 1 is a positive parameter
that will help us preserve connectivity. Circle D together with B are depicted
in Fig. 3. Circle D is depicted in Fig. 2. We refer to the arc A of D as the target
arc. In the safe state, robots move towards the midpoint of the target arc. We
construct the target arc in such a way that robots in the safe state satisfy the
contracting conditions of Lemma 1. This is shown in Lemma 8.
The target arc is constructed as follows. Let us ﬁrst consider arc B ⊂D such
that the central angle ∠arc = 3π/4 and the bisector of ∠arc coincide with the
bisector of ∠m1rm2. Then, we draw the line rg perpendicular to m1m2. With
respect to this line, we either take left or right, depending on the chirality part
of B, i.e. BL ⊂B. Finally we subtract from BL, the arc that corresponds to the
central angle ∠grb = π/20. What is left is target arc A, that we depict in Fig. 2.
In order to select the suitable moment for the independent motion of robot
r, we check whether r is close to the crash point p(w) of some robot w. Namely,
robot r checks if ∃w : w, T(w), MEC(r) ∈B1/2m|m1,m2|(p(w)), where robots
m1, m2 deﬁne p(w) AND |w, r| = minu∈UDG(r)\r{|r, u|}, where UDG(r) consists
of robots in both states (regular and safe) AND |w, r| ≤ρQ AND r is the
leftmost (rightmost, depending on chirality) robot with respect to the direction
towards p(w).
We show in Lemma 9 that there exists a point in time, shortly before collision
takes place, at which this condition is satisﬁed. We refer to the logical expression
above as the safety condition, denoted by the function Sr : X →Z2, where
X ⊂R2, i.e. Sr(w) = true means that robot w at its current position violates
the safety condition with respect to the crash point p(w) of robot w.
At every point in time t, each robot can read the positions and states of its
neighboring robots in 2-UDG(R). Every robot has access to the synchronized
clock. Besides that, each robot can read and write into the two variables. These
will be used to store either the position on the Euclidean plane, denoted by M(r),
or the point in time Δ(r) together with an additional state L(r). The variable of
robot r that represents its state is called S(r). This is set by default to regular.
Our algorithm has three positive parameters, used by all of the robots: s, m
and k. Parameter m deﬁnes how close to the crash point we check the safety
condition. Parameter s tells us the ratio between the speed of the robots in
the safe and regular state. Robots in the safe mode are assumed to be faster.
Parameter k tells us what portion of the minimum distance to other robots
does the robot cover during the motion in the safe state. The initial state at
time 0 of every robot is regular. The initial value of the memory slots that
correspond to M(r), Δ(r) and L(r) are undeﬁned. The Safe-Go-To-The-Relative-
Center algorithm is presented in pseudo-code as Algorithm 1 in Appendix A.1.
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
191
The main diﬀerence between S-GTRC and GTRC lies in the states: regular
and safe. In regular state, a robot moves according to GTRC. Early collisions
may take place only in the regular branch of the algorithm. The safe branch
is designed to avoid early collisions. The collision that takes place in S-GTRC
without safe branch is called potential early collision. Next, we show that the
safe branch avoids potential early collisions.
First, we show that in the safe state, the robots preserve connectivity. Then,
we consider the runtime and show that the robots in the safe state move only
inside the convex hull, with speed s. In Lemma 7, we show that if parameter k
is big enough (i.e. the radius of circle D is small enough), then at the end of the
independent motion in the safe state, the robot will still have the same unit disk
graph neighbors.
Lemma 7. If robot w is in UDG(r) at time 0, when robot r switches to safe
state, then w is still in UDG(r) at time t > 0, when robot r switches back to
regular state.
From Lemmas 2 and 7, we conclude that S-GTRC preserves connectivity.
Corollary 3. Let us consider a group of robots R on the Euclidean plane exe-
cuting S-GTRC. If {u, w} is an edge in the open Relative neighborhood graph
RNG(R) at time 0, then {u, w} is an edge in RNG(R) at ∀t ≥0 or there is a
path from u to v in RNG(R), ∀t ≥0.
Next, we analyze the runtime. We know that the robots in the regular state
(i.e. those executing GTRC) satisfy the contracting conditions of Lemma 1. It
remains to show that in the safe state the robots also satisfy the contracting
conditions. In Lemma 8, we show that the target arc is constructed in such way
that its middle point is always inside the local and consequently also a global
convex hull.
Lemma 8. If robot r is in the safe state, then the target point M(r) is inside
the convex hull H(R) over all robots, and it does not coincide with the position
of the robot r(t) at least until it switches into regular state again.
Lemma 8 implies that in the safe state any robot r moves with speed s ≥1
inside the convex hull. This means that S-GTRC is a contracting algorithm.
Besides that, the length of the convex hull boundary around the initial conﬁg-
uration is not greater then 2(n −1), where n is a number of robots, as shown
in [17].
Theorem 3. The group of n robots executing S-GTRC gathers in time O(n2).
Next, we investigate the collisions of robots executing S-GTRC. Robots that
execute S-GTRC can be in one of the three states: regular, locked (regular) or
safe, where a locked robot is a robot r that is positioned at its own crash point
p(r) and ∃w : Sw(r) = true. It actually is in the regular state but the presence
of w in the proximity of its p(r) prevents r from switching to the safe state.

192
S. Li et al.
Proposition 2. If robot r is in the safe state, then it does not collide with any
other robot.
This proposition holds since according to S-GTRC, during the motion in the safe
state, a robot covers at most a distance 1/k|w, r| = 1/k minu∈UDG(r)\r{|r, u|}. Due
to the speed limit s, all other robots during the safe motion of r can cover at
most the same distance. For k ≥3, the position of r will never coincide with the
position of some other robot. Similar argumentation works in the Proposition 3
for the locked regular state.
Proposition 3. If robot r is in the locked state, then it does not collide with
any other robot.
In order to have a collision, according to Corollary 2, robot r needs to reach
the crash point p(w) of some other robot w. An example of disposition of robots
r and w shortly before a collision at p(w) is depicted in Fig. 4.
Robot r needs to pass through a shrinking gap between two lenses corre-
sponding to the RNG edges between w and m1, m2. Next, in Lemma 9, we show
that the safe move always triggers before the potential collision by carefully ana-
lyzing the safety condition. In Lemma 10, we show that at the end of the motion
in the safe sate, a robot avoids potential collision by showing that during the
run, a robot cannot reach the crash point of any other robot.
Lemma 9. If a set of robots M ⊂R has a potential collision with robot w at
time t∗in p(w), then there exists robot r ∈M and point in time t < t∗such that
Sr(w) = true.
Lemma 10. If a set of robots M ⊂R has a potential collision with robot w
at time t∗in p(w), then there exists r ∈M such that the safety condition is
triggered by p(w) at time t1 < t∗and for k = 4, m ≥500, s ≥10 at the end of
the safe motion at time t2 ∈(t1, t∗) the crash point p(w) does not exist.
Using Propositions 2 and 3 together with Lemmas 6, 9, 10 and Theorem 3,
we prove one of our main results, Theorem 4.
Theorem 4. For s ≥10, m ≥500, and k = 4, the Safe-Go-To-The-Relative-
Center algorithm performs gathering in O(n2) without collisions.
6
Future Work
In this paper, we propose algorithms that can gather robots with and without
collisions. In fact, collisions may be the outcome of certain start conﬁgurations
of swarms and so cannot be avoided for some instances. Within this context, we
conjecture the following: “Start conﬁgurations of swarms yielding unavoidable
collisions are singular in a sense that a random perturbation of robot posi-
tions within arbitrarily small circles around their original positions results in a
collision-free conﬁguration with probability one”.
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
193
A
Appendix
A.1
Safe-Go-To-The-Relative-Center algorithm
Algorithm 1. S-GTRC
Require: Initial conﬁguration, parameters m and k, velocity s.
1: Robot r observes the positions of all its neighbors (regular and safe) in 2-UDG(r):
2: if
maxa,b∈2-UDG(r) |a, b| ≥1 then
3:
Robot r observes the positions of its regular neighbors in UDG(r) and calculates
RNG(r).
4:
Robot r computes the minimum circle C(r) enclosing RNG(r). The center T(r) of C(r)
is the target point of r.
5:
Robot r observes the positions of all regular robots in 2-UDG(r). and calculates crash
points for every robot in UDG(r).
6:
Robot r checks:
7:
if Robot r is at its own crash point, i.e. r = p(r) AND ∃w : Sw(r) = true AND
S(w) = safe then
8:
Set states: L(r) := locked, S(r) := regular,
9:
Δ(r) := |r,M(r)|
s
+ #clock.
10:
else
11:
if ∃w : Sr(w) = true then
12:
if M(r) = undeﬁned
then
13:
S(r) := safe,
14:
M(r) := mid point of target arc A ⊂D, where D is the circle with center at r
and radius radius 1
k |w, r|.
15:
else
16:
S(r) := regular
17:
if L(r) = locked AND #clock = Δ(r) then
18:
Set states: S(r) := undeﬁned, Δ(r) := undeﬁned.
19:
Robot r moves:
20:
if S(r) = regular then
21:
if
r is already at T(r) then
22:
Robot r remains at T(r) and moves in the same way as the target point does.
23:
else
24:
Robot r moves with maximum speed 1 towards T(r).
25:
else
26:
if
r is already at M(r) then
27:
Set states: S(r) := regular, M(r) := undeﬁned.
28:
else
29:
Robot r moves with maximum speed s towards M(r).

194
S. Li et al.
A.2
Go-To-The-Relative-Center algorithm
Algorithm 2. GTRC
Require: Initial conﬁguration
1: Robot r observes the positions of all neighbors in UDG(r) and checks whether:
2: Robot r observes the positions of its unit disk graph neighbors and calculates
RNG(r).
3: Robot r computes the minimum circle C(r) enclosing RNG(r). The center T(r) of
C(r) is the target point of r.
4: Robot r moves:
5: if r is already at T(r) then
6:
Robot r remains at T(r) and moves in the same way as the target point does.
7: else
8:
Robot r moves with speed 1 towards T(r).
A.3
Figures
(a)
Robot
r
together
with
its
minimum
enclosing
set
MEC(r) and RNG lenses, that
correspond to the RNG edges
between
r
and
members
of
MEC(r).
(b) Robot r together with a crash point p(r)
and the robots m1, m2 that deﬁne this crash
point. Besides that here we depict RNG
lenses that correspond to the RNG edges
{m1, r} and {m2, r}.
Fig. 1. Lenses of the Relative neighborhood graph
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
195
Fig. 2. The construction of target arc A ⊂D.
Fig. 3. Robot r in the safe state moves towards target point M(r) the mid point of
the target arc A ⊂D. Black arcs C1, C2 are the parts of according RNG lenses.
Fig. 4. Robots w with RNG lenses that correspond to RNG edges {w, m1} and {w, m2}.
Both robots are inside of the circle B with the center at the crash point p(w) and radius
1/2m|m1, m2|. If robot r performs safe move, then the target point during the safe move
belongs to the circle D.
References
1. Abshoﬀ, S., Cord-Landwehr, A., Fischer, M., Jung, D., Meyer auf der Heide, F.:
Gathering a closed chain of robots on a grid. In: Proceedings of the 30th Inter-
national Parallel and Distributed Processing Symposium (IPDPS), pp. 689–699.
IEEE, May 2016
2. Agathangelou, C., Georgiou, C., Mavronicolas, M.: A distributed algorithm for
gathering many fat mobile robots in the plane. In: Proceedings of the 2013 ACM
Symposium on Principles of Distributed Computing, PODC 2013. ACM, New
York, pp. 250–259 (2013)
3. Ando, H., Suzuki, I., Yamashita, M.: Formation and agreement problems for syn-
chronous mobile robots with limited visibility. In: Proceedings of the 1995 IEEE
International Symposium on Intelligent Control, 1995, pp. 453–460 (1995)
4. Chrystal, G.: On the problem to construct the minimum circle enclosing n given
points in a plane. In: Proceedings of the Edinburgh Mathematical Society, Third
Meeting, pp. 30–35 (1885)

196
S. Li et al.
5. Cohen, R., Peleg, D.: Convergence properties of the gravitational algorithm in
asynchronous robot systems. In: Albers, S., Radzik, T. (eds.) ESA 2004. LNCS,
vol. 3221, pp. 228–239. Springer, Heidelberg (2004). https://doi.org/10.1007/978-
3-540-30140-0 22
6. Cord-Landwehr, A., Fischer, M., Jung, D., Meyer auf der Heide, F.: Asymptotically
optimal gathering on a grid. In: Proceedings of the 28th ACM Symposium on
Parallelism in Algorithms and Architectures (SPAA), pp. 301–312. ACM, July
2016
7. Czyzowicz, J., G asieniec, L., Pelc, A.: Gathering few fat mobile robots in the plane.
In: Shvartsman, M.M.A.A. (ed.) OPODIS 2006. LNCS, vol. 4305, pp. 350–364.
Springer, Heidelberg (2006). https://doi.org/10.1007/11945529 25
8. Das, S., Flocchini, P., Prencipe, G., Santoro, N., Yamashita, M.: The power of
lights: synchronizing asynchronous robots using visible bits. In: 2012 IEEE 32nd
International Conference on Distributed Computing Systems, Macau, China, June
18–21, pp. 506–515 (2012)
9. Degener, B., Kempkes, B., Meyer auf der Heide, F.: A local O(n2) gathering algo-
rithm. In: Proceedings of the 22nd ACM Symposium on Parallelism in Algorithms
and Architectures, SPAA 2010, pp. 217–223. ACM, New York (2010)
10. Degener, B., Kempkes, B., Langner, T., Meyer auf der Heide, F., Pietrzyk, P.,
Wattenhofer, R.: A tight runtime bound for synchronous gathering of autonomous
robots with limited visibility. In: Proceedings of the 23rd ACM Symposium on
Parallelism in Algorithms and Architectures, SPAA 2011. ACM, New York, pp.
139–148 (2011)
11. Fischer, M., Jung, D., Meyer auf der Heide, F.: Gathering anonymous, oblivious
robots on a grid. CoRR, abs/1702.03400 (2017)
12. Flocchini, P.: Distributed Computing by Oblivious Mobile Robots. Synthesis Lec-
tures on Distributed Computing Theory, no. 10. Morgan and Claypool, San Rafael
(2012)
13. Gabriel, R.K., Sokal, R.R.: A new statistical approach to geographic variation
analysis. Syst. Biol. 18(3), 259–278 (1969)
14. Gordon, N., Wagner, I.A., Bruckstein, A.M.: Gathering multiple robotic a(ge)nts
with limited sensing capabilities. In: Dorigo, M., Birattari, M., Blum, C., Gam-
bardella, L.M., Mondada, F., St¨utzle, T. (eds.) ANTS 2004. LNCS, vol. 3172, pp.
142–153. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-28646-
2 13
15. Kempkes, B., Kling, P., Meyer auf der Heide, F.: Optimal and competitive runtime
bounds for continuous, local gathering of mobile robots. In: Proceedinbgs of the
24th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA
2012. ACM, New York, pp. 18–26 (2012)
16. Li, S., Markarian, C., Meyer auf der Heide, F., Podlipyan, P.: A continuous strat-
egy for collisionless gathering. Full version, March 2017. https://www.hni.uni-
paderborn.de/pub/9531
17. Li, S., Meyer auf der Heide, F., Podlipyan, P.: The impact of the gabriel subgraph
of the visibility graph on the gathering of mobile autonomous robots. In: Chrobak,
M., Fern´andez Anta, A., G asieniec, L., Klasing, R. (eds.) ALGOSENSORS 2016.
LNCS, vol. 10050, pp. 62–79. Springer, Cham (2017). https://doi.org/10.1007/978-
3-319-53058-1 5
18. Lukovszki, T., Meyer auf der Heide, F.: Fast collisionless pattern formation by
anonymous, position-aware robots. In: Aguilera, M.K., Querzoni, L., Shapiro, M.
(eds.) OPODIS 2014. LNCS, vol. 8878, pp. 248–262. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-14472-6 17
www.ebook3000.com

A Continuous Strategy for Collisionless Gathering
197
19. Megiddo, N.: Linear-time algorithms for linear programming in R3 and related
problems. SIAM J. Comput. 12(4), 759–776 (1983)
20. Pagli, L., Prencipe, G., Viglietta, G.: Getting close without touching. In: Even, G.,
Halld´orsson, M.M. (eds.) SIROCCO 2012. LNCS, vol. 7355, pp. 315–326. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31104-8 27
21. Singer, I.: Abstract Convex Analysis. Wiley, Hoboken (1997)
22. Toussaint, G.T.: The relative neighbourhood graph of a ﬁnite planar set. Pattern
Recogn. 12, 261–268 (1980)

Maximizing Barrier Coverage Lifetime
with Static Sensors
Menachem Poss and Dror Rawitz(B)
Faculty of Engineering, Bar-Ilan University, 52900 Ramat Gan, Israel
menachemposs@gmail.com, dror.rawitz@biu.ac.il
Abstract. We study variants of the Strip Cover problem (SC) in
which sensors with limited battery power are deployed on a line barrier,
and the goal is to cover the barrier as long as possible. The energy con-
sumption of a sensor depends on its sensing radius: energy is drained
in inverse proportion to the sensor radius raised to a constant exponent
α ≥1. In the Set Once Strip Cover (OnceSC) the radius of each
sensor can be set once, and the sensor can be activated at any time. SCk
and OnceSCk are variants of SC and OnceSC, resp., in which each
sensor is associated with a set of at most k predetermined radii.
It was previously known that OnceSC is NP-hard when α = 1, and
the complexity of the case where α > 1 remained open. We extend the
above mentioned NP-hardness result in two ways: we show that OnceSC
is NP-hard for every α > 1 and that OnceSC is strongly NP-hard for
α = 1. In addition, we show that OnceSCk, for k ≥2, is NP-hard, for
any α ≥1, even for uniform radii sets. On the positive side, we present
(i) a 5γα-approximation algorithm for OnceSCk, for k ≥1, where γ is
the maximum ratio between two radii associated with the same sensor;
(ii) a 5-approximation algorithm for SCk, for every k ≥1; and (iii) a
5 · 2α-approximation algorithm for Strip Cover. Finally, we present an
O(n log n)-time algorithm for a variant of OnceSCk in which all sensors
must be activated at the same time.
1
Introduction
Sensor networks are used in a broad range of applications related to surveillance,
military, health care, and environmental monitoring. One of the main challenges
in designing and operating such networks is coping with the limited resources of
the sensors. In this paper we focus on one of these limited resources – energy.
More speciﬁcally, we study a network of static sensors with limited battery power
that is supposed to cover a line barrier, where the goal is to maximize network
lifetime, namely to provide coverage as long as possible.
More formally, we study the setting where there are n static sensors located
on a barrier represented by the interval [0, N], where N ∈N. The input consist
of battery charges b ∈Nn and locations x ∈([0, N] ∩N)n. We consider the set
once model [2] in which we can set the sensing radius ρi ∈N and the activation
D. Rawitz—Supported by the Israel Science Foundation (grant no. 497/14).
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 198–210, 2017.
https://doi.org/10.1007/978-3-319-72751-6_15
www.ebook3000.com

Maximizing Barrier Coverage Lifetime with Static Sensors
199
0
N
t
τi
τi + bi
ρα
i
ρi
ρi
xi
xi −ρi
xi + ρi
Fig. 1. Sensor i covers the interval [xi −ρi, xi + ρi] during [τi, τi + bi/ρα
i ].
time τi ∈R+ (where R+ = R ∩[0, ∞)) of each sensor i only once, namely the
radius of a sensor remains the same once activated. The energy consumption rate
of sensor i with a covering radius of ρi is proportional to ρα
i , where α ≥1 is a
path-loss exponent [9]. Thus sensor i covers the region [xi −ρi, xi +ρi] during the
time interval [τi, τi + bi/ρα
i ]. It is common to assume a super-linear dependence
of the consumption rate on the radius, i.e., to assume that α > 1. However, the
case of α = 1 is still interesting, since there are special settings where a linear
dependence may be appropriate [10]. The goal is to ﬁnd a solution (ρ, τ) for any
input (x, b), such that the barrier is completely covered for as long as possible.
An example of the coverage of one sensor is given in Fig. 1.
We consider two problems that diﬀer in the types of radial assignments. In
the variable radii case the radius of a sensor i can be any integer, while in the
ﬁxed radii case each sensor i is associated with a set Ri of possible values given
in the input. In the variable radii case the problem is called Set Once Strip
Cover (abbreviated OnceSC), and in the ﬁxed case, if all radii sets are of size
at most k the problem is denoted by OnceSCk. If all radii sets are the same,
i.e., Ri = Rj, for every i ̸= j, we refer to the instance as having uniform radii
sets. Such an instance can be obtained if we use uniform sensors (say, from the
same manufacturer) that previously took part in other tasks, and therefore may
have non-uniform battery powers. We also consider the Strip Cover problem
(SC) in which the radius of each sensor can be set any ﬁnite number of times.
In this case, a solution is a vector of functions where the ith function determines
the radius of sensor i at any given moment. SCk is the ﬁxed radii variant of SC.
Related work. Buchsbaum et al. [3] considered the Restricted Strip Cover
problem (RSC) where the input consists of a set of rectangles that can be moved
vertically but not horizontally, and the goal is to maximize the height at which an
interval region is fully covered. This problem is equivalent to OnceSC1. Notice
that since there is only one radius per sensor in OnceSC1, the value of α is
irrelevant. Buchsbaum et al. [3] gave an O(log log log n)-approximation algorithm
for RSC and a (2+ε)-approximation algorithm for the uniform radii case. They
also showed that it is strongly NP-hard using a reduction from the Dynamic
Storage Allocation problem. Stockmeyer (see [6, Problem SR2]) showed
that Dynamic Storage Allocation is strongly NP-hard using a reduction

200
M. Poss and D. Rawitz
from 3-partition. (The reduction is given in [4].) In fact, in the above reduction
implies that RSC is strongly NP-hard even if all rectangles are of height either
1 or 2. Later, a 5-approximation algorithm for RSC was given by Gibson and
Varadarajan [7]. It is important to note that their analysis is made in comparison
to the minimum load which is a lower bound on the optimal height (or lifetime).
Bar-Noy and Baumer [1] studied Strip Cover. Assuming unit batteries and
α = 1, they showed that the approximation ratio of the Round Robin algorithm,
which simply forces each sensor to successively cover the barrier for as long as
possible by itself, is between
3
2 and 1.825. Bar-Noy et al. [2] deﬁned the set
once model. They proved that OnceSC is NP-hard when α = 1. They analyzed
Round Robin for the case where α = 1 and showed that its approximation ratio is
3
2, even for Strip Cover, thus closing the gap left by [1]. In addition, they gave
an algorithm for a simpler variant, called Set Radius Strip Cover, where all
sensors must be activated at the same time.
Lev-Tov and Peleg [8] explored the Disk Cover problem in which the input
consists of a set of points and a set of sensor locations on the plane, and the
goal is to cover the points so as to minimize the sum of radii (i.e., α = 1). For
the one-dimensional case with variable radii they provided a polynomial time
algorithm and a linear-time 4-approximation algorithm. The former algorithm
works for any α ∈N. They also presented two PTASs for Disk Cover, one for
variable radii, and the other for ﬁxed radii, assuming that the set of points is a
subset of the set of sensor locations. Li et al. [5] studied barrier coverage with
static sensors, where the objective is to minimize the total energy consumption
rate, namely the goal is to ﬁnd a radii assignment ρ that minimizes 
i ρα
i .
They presented a polynomial time algorithm for the ﬁxed multiple radii case,
a 2-approximation algorithm for the variable radii case. They also presented an
FPTAS and an NP-hardness result for variable radii and α = 1.
Our results. We provide NP-hardness results in Sect. 3. We show that OnceSCk
is NP-hard, for any k ≥2 and α ≥1, even for the case of uniform radii sets.
This result is obtained using a reduction from Partition. We use the same
construction, but with a more elaborate analysis, to show that OnceSC is NP-
hard, for any α ≥1. We use a reduction from 3-partition to show that OnceSC
is strongly NP-hard in the case where α = 1. The last two results improve the
NP-hardness result for OnceSC with α = 1 from [2].
Several algorithms are given in Sect. 4. We present a 5γα-approximation algo-
rithm for OnceSCk, for k ≥1, where γ is the maximum ratio between two radii
that belong to the same sensor, i.e., γ
△= maxi {ri,ki/ri,1}, where ki = |Ri|.
Observe that if γ = O(1), then the approximation ratio is also O(1). We present
a 5-approximation algorithm for SCk, for every k ≥1. In addition, we give a
5 · 2α-approximation algorithm for Strip Cover. The algorithms are based on
the 5-approximation algorithm RSC (OnceSC1) by Gibson and Varadarajan [7].
Finally, we consider the ﬁxed radii version of Set Radius Strip Cover,
for which we give an O(n log n)-time algorithm for every k ≥1. (Recall that in
this problem all sensors must be activated at the same time.) This section was
removed for lack of space.
www.ebook3000.com

Maximizing Barrier Coverage Lifetime with Static Sensors
201
2
Preliminaries
The Set Once Strip Cover problem (abbreviated OnceSC) is deﬁned as
follows. Let U = {0, . . . , N}, where N ∈N, be the discrete barrier that we aim
to cover. A OnceSC instance consists of a vector x ∈U n of n sensor locations on
the barrier and a corresponding vector b ∈Nn of battery charges. We assume that
xi ≤xi+1 for every i ∈{1, . . . , n −1}. A solution is an assignment of radii and
activation times to sensors. More speciﬁcally, a solution is a pair (ρ, τ) ∈Nn×Rn
+,
where ρi is the sensing radius of sensor i and τi is the activation time of sensor
i. We assume that the radius of each sensor cannot be reset, therefore sensor i
becomes active at time τi, covers the interval [xi −ρi, xi + ρi] for bi/ρα
i time
units, and then becomes inactive since it has exhausted its battery.
Any solution can be visualized by a space-time diagram in which each cov-
erage assignment can be represented by a rectangle. It is customary in such
diagrams to view the sensor locations as forming the horizontal axis, with time
extending upwards vertically. In this case, the coverage of a sensor located at xi
and assigned the radius ρi beginning at time τi is depicted by a rectangle with
lower-left corner (xi −ρi, τi) and upper-right corner (xi + ρi, τi + bi/ρα
i ). Let the
set of all points contained in this rectangle be denoted as Rect(ρi, τi). A point
(u, t) in space-time is covered by a solution (ρ, τ) if (u, t) ∈
i Rect(ρi, τi). The
lifetime of the network in a solution (ρ, τ) is the maximum value T such that
every point (u, t) ∈[0, N] × [0, T] is covered. We denote this value by T(ρ, τ). In
OnceSC the goal is to ﬁnd a solution (ρ, τ) that maximizes the lifetime. Given
an instance (x, b), the optimal lifetime is denoted by opt(x, b) (or by opt).
The OnceSCk problem is a variant of OnceSC in which the instance also
contains a set Ri = {ri,1, . . . , ri,ki}, for every sensor i, containing ki ≤k radii
that are possible for sensor i. That is, it is required that ρi ∈Ri ∪{0}, for every
i. We assume that ri,j < ri,j+1, for every j. Observe that OnceSC1 is equivalent
to Restricted Strip Cover. RadSCk is variant of OnceSCk in which all
sensors become active at together, namely where τ = 0. Hence, a solution is
simply a radial assignment ρ.
In Strip Cover (SC) the radius of a sensor may change any ﬁnite number
of times, hence a solution is a vector of functions ρ, where ρi : R+ →[0, N] ∩N
is a piecewise constant function that determines the radius of sensor i at any
given moment. In this case the lifetime of a solution ρ is denoted by T(ρ). SCk
is the ﬁxed radii variant of SC. (Notice that SC1 is not the same as OnceSC1).
3
Hardness Results
We provide several hardness results. We show that OnceSC2 is NP-hard for
any α ≥1, even for uniform radii sets, using a reduction from Partition. The
same reduction, but with a more careful analysis, is used to show that OnceSC
is NP-hard. A reduction from 3-partition is used to show that OnceSC is
strongly NP-hard in the case where α = 1. We start the section by examining
the structure of an optimal solution.

202
M. Poss and D. Rawitz
3.1
Structure of an Optimal Solution
We show that that there exists an optimal solution that has a certain structure.
More speciﬁcally, given a radii assignment, we show that if there are sensors that
cover the whole barrier by themselves, then we can change the activation times
such that these sensors would ﬁrst cover the barrier one by one, and then the
rest of the sensors would cover the barrier. Moreover, this can be done without
decreasing the lifetime of the network.
Given a radii assignment ρ, let W(ρ) be the set of sensors that cover the
whole barrier, namely deﬁne W(ρ)
△= {i : [0, N] ⊆[xi −ρi, xi + ρi]}. Also deﬁne
tW
△= 
i∈W (ρ) bi/ρα
i . We ﬁrst show that a sensor in W(ρ) can be moved to the
beginning of the schedule.
Lemma 1. Let (b, x) be a OnceSC (or a OnceSCk) instance, and let (ρ, τ)
be a solution. Also, i be a sensor that covers the whole barrier, i.e., [0, N] ⊆
[xi −ρi, xi + ρi]. Then, there exists a vector τ ′, such that (i) (ρ, τ ′) is a feasible
solution and T(ρ, τ ′) ≥T(ρ, τ); and (ii) τ ′
i = 0 and τ ′
j ≥bi
ρα
i , for every j ̸= i.
Proof. The idea is to activate sensor i at time 0. This creates a coverage gap,
and this gap is closed by activating sensors that were originally activated before
i after i’s battery is depleted. Formally, given (ρ, τ) we deﬁne τ ′ as follows:
τ ′
j =
⎧
⎪
⎨
⎪
⎩
0
i = j,
max {τj, bi/ρα
i }
τj ≥τi,
τj + bi/ρα
i
τj < τi.
First observe that τ ′
i = 0 and τ ′
j ≥bi
ρα
i , for every j ̸= i, by construction.
To prove that T(ρ, τ ′) ≥T(ρ, τ), we show that any point (u, t) ∈[0, N] ×
[0, T(ρ, τ)] is covered in the solution (ρ, τ ′). We consider three cases.
– First, if t < bi/ρα
i , then (u, t) is covered by sensor i.
– The second case is when bi/ρα
i ≤t < τi + bi/ρα
i . Since t −bi/ρα
i ≥0, the
point (u, t −bi/ρα
i ) must be covered in the solution (ρ, τ) by some sensor j.
Moreover, since t −bi/ρα
i ≤τi, it must be that j ̸= i and τj < τi. It follows
that τ ′
j = τj + bi/ρα
i , and thus (u, t) is covered in (ρ, τ ′).
– If t ≥τi + bi/ρα
i , then the point (u, t) is covered in (ρ, τ) by a sensor j ̸= i.
Hence t ∈[τj, τj + bj/ρα
j ]. We claim that (u, t) is covered in (ρ, τ ′) by the
same sensor j. Since τ ′
j ≥τj, for any j ̸= i, it is enough to show that t ≥τ ′
j.
First, if τj < τi we have that τ ′
j = τj + bi/ρα
i < τi + bi/ρα
i , and it follows
that t > τ ′
j. The other option is that τj ≥τi, and in this case we have that
τ ′
j = max {τj, bi/ρα
i }. Since t ≥τj and t > τi + bi/ρα
i we are done.
The lemma follows.
⊓⊔
The following result is obtained using induction and Lemma 1.
Lemma 2. Let (b, x) be a OnceSC (or a OnceSCk) instance, and let (ρ, τ) be
a solution. Then, there is a vector τ ′ such that (i) (ρ, τ ′) is a feasible solution,
(ii) T(ρ, τ ′) ≥T(ρ, τ), and (iii) if i ∈W(ρ), then τ ′
i + bi
ρα
i ≤tW , and otherwise,
τ ′
i ≥tW .
www.ebook3000.com

Maximizing Barrier Coverage Lifetime with Static Sensors
203
3.2
Multiple Fixed Radii
In this section we show that OnceSC2 is NP-hard even in the case of uniform
radii sets. This is done using a reduction from Partition.
Theorem 1. OnceSC2 is NP-hard, even for uniform radii sets.
Proof. Consider a Partition instance A = {a1, . . . , an}. Deﬁne B
△= 1
2

i ai.
We construct a OnceSC2 instance as follows. First assume that N is a multiple
of 6. The instance contains n + 2 sensors, where b1 = bn+2 = B and bi+1 = ai,
for every i. The locations are: x1 = N
6 , xi = N
2 , for every i ∈{2, . . . , n + 1}, and
xn+2 = 5N
6 . The radii subsets are: Ri =
 N
6 , N
2
	
, for every i.
We claim that A ∈Partition if and only if opt(b, x) = B · 6α+2α
Nα .
Consider an optimal solution (ρ, τ) to such an instance. Observe that W(ρ) =
{i : ρi = N/2 and i ̸= 1, n + 2}. By Lemma 2 we may assume that any sensor
i ∈W(ρ) works alone when covering the barrier. The remaining sensors are
activated not earlier than tW .
Observe that the remaining sensors that are located at N
2 cannot cover 0
with a radius of N
6 . Thus if ρ1 = N
2 , sensor 1 cannot be helped by sensors that
are located at
N
2 . Moreover, it must get help from sensor n + 2, and in this
case the network lifetime is: T(ρ, τ) = 
i∈W (ρ)
ai
(N/2)α +
B
(N/2)α . This value is
maximized when w(ρ) = {2, . . . , n + 2}, and in this case T(ρ, τ) = B · 3·2α
Nα . A
similar argument can be used if ρn+2 = N
2 .
Now consider the case where ρ1 = N
6 and ρn+2 = N
6 . Sensors 1 and n + 2
cannot cover the barrier by themselves, since they do not cover the interval
[ N
3 , 2N
3 ]. However, sensors in {2, . . . , n + 1} \ W(ρ) can do that, and the best
way to do that is with no overlaps. It follows that
T(ρ, τ) =

i∈W (ρ)
ai
(N/2)α + min

B
(N/6)α ,
C
(N/6)α

= 2B −C
(N/2)α + min {B, C}
(N/6)α
,
where C = 
i∈{2,...,n+1}\W (ρ) ai. If C ≤B, we have that T(ρ, τ) = 2B · 2α
Nα +
C · ( 6α
Nα −2α
Nα ), which is an increasing function of C. If C ≥B, then T(ρ, τ) =
B · (2 2α
Nα + 6α
Nα ) −C · 2α
Nα , which is decreasing function of C. It follows that the
maximum value is obtain when C = B and in this case T(ρ, τ) = B·( 2α
Nα + 6α
Nα ) =
B · 2α+6α
Nα .
Since 3 · 2α < 2α + 6α, for α ≥1, it follows that the best we could hope for
is B · 2α+6α
Nα .
If A ∈Partition, there exists I such that 
i∈I ai = B. We can obtain a
lifetime of B · 2α+6α
Nα
by assigning ρi+1 = N
2 if i ∈I, and ρi = N
6 otherwise. On
the other hand, if a lifetime of B · 2α+6α
Nα
is attainable, then the set W(ρ) induces
a partition. More speciﬁcally, 
i∈I ai = B for I = {i : i + 1 ∈W(ρ)}.
⊓⊔
This result can be extended to any k > 2.
Theorem 2. OnceSCk is NP-hard, for any k > 2, even for uniform radii sets.

204
M. Poss and D. Rawitz
Proof. We use a similar reduction to the one from the proof of Theorem 1, where
the only diﬀerence is that Ri =
 N
2 , N
6 , k −2, . . . , 1
	
(assuming that k < N
6 ).
Since the smaller radii cannot be used to cover the barrier, they are redundant.
The theorem follows.
⊓⊔
3.3
Variable Radii
We show that OnceSC is NP-hard for any α ≥1. This is done using the
reduction from Partition that was deﬁned in the proof of Theorem 1.
Theorem 3. OnceSC is NP-hard, for any α ≥1.
Proof. We use the same construction (not including the Ris) that was used in
the proof of Theorem 1, and again we claim that A ∈Partition if and only if
opt(b, x) = B · 2α+6α
Nα .
Consider an optimal solution (ρ, τ) to such an instance. First notice that we
may assume without loss of generality that ρ1, ρn+2 ≤
5
N and that ρi ≤N
2 , for
every i ∈{2, . . . , n + 1}. Observe that W(ρ) =

i : ρi ≥N
2
	
. By Lemma 2 we
may assume that any sensor i ∈W(ρ) works alone when covering the barrier.
We examine the sensors 1 and n + 2. There are four cases corresponding to
whether or not they belong to W(ρ).
1. First consider the case where 1, n + 2 ∈W(ρ). By Lemma 2 we may assume
that both sensors cover the whole barrier by themselves, each during a time
interval of length
B
(5N/6)α =
6αB
5αNα . Observe that if they work together, each
with radius N
3 they cover the barrier for
B
(N/3)α = 3αB
Nα time. This increases
the lifetime, since 2·6α
5α < 3α, for α ≥1.
2. Next we consider the case where 1 ̸∈W(ρ) and n+2 ∈W(ρ). Since 1 ̸∈W(ρ),
it follows that ρ1 < 5N
6 which means that 1 does not cover the right endpoint
of the barrier, namely the point N. Since n + 2 ∈W(ρ), sensors n + 2 never
collaborates with 1. Hence, to cover the barrier, sensor 1 needs help from
another sensor i ̸∈W(ρ) located at N
2 . Assuming that this happens, i needs
to cover the right endpoint, and we have that ρi ≥
N
2 , which means that
i ∈W(ρ) and we get a contradiction.
3. Similarly we may eliminated the option, where 1 ∈W(ρ) and n + 2 ̸∈W(ρ).
It follows that we may assume that 1, n + 2 ̸∈W(ρ).
Now consider the sensors that are not in W(ρ). We know that ρi < N
2 , for
every i ∈{2, . . . , n + 1} \ W(ρ). Hence, any such sensor i cannot reach both
barrier end points, and therefore it must rely on the help of both 1 and n + 2.
We also know that ρ1, ρn+2 < 5N
6 , and this means that the two sensors must
work together to obtain coverage. It follows that after tW the barrier is covered
only if both 1 and n + 2 are activated. Also, we may assume that ρ1, ρn+2 ≥N
6 ,
since otherwise the endpoints remain uncovered.
www.ebook3000.com

Maximizing Barrier Coverage Lifetime with Static Sensors
205
If ρ1 + ρn+2 ≥2N
3 , both sensors cover the barrier by themselves, and they
achieve maximum lifetime when ρ1 = ρn+2 = N3. In this case we get
T(ρ, τ) =

i∈W (ρ)
ai
(N/2)α +
B
(N/3)α = 2B −C
(N/2)α +
B
(N/3)α = (2 · 2α + 3α)B −2αC
N α
where C = 
i∈{2,...,n+1}\W (ρ) ai. This function decreasing in C and therefore
the maximum is obtained when C = 0. In this case
T(ρ, τ) = B · 2 · 2α + 3α
N α
.
(1)
Next we consider the case where ρ1 + ρn+2 < 2N
3 . In this case sensors 1 and
n + 2 need the help of sensors located at N
2 . Let S be the set of sensors that
collaborate with 1 and n+2. Since ρ1, ρn+2 ≥N
6 , we may assume that ρi ≤N
6 , for
every i ∈S. Let ℓ= mini∈S ρi. We may assume that ρ1, ρn+2 = N
3 −ℓ≥N
6 ≥ℓ.
Hence the lifetime that is obtained is:
T(ρ, τ) =

i∈W (ρ)
ai
(N/2)α + min
 C
ℓα ,
B
(N/3 −ℓ)α

= (2B −C) · 2α
N α + min
 C
ℓα ,
B
(N/3 −ℓ)α

If C ≥B, then
C
ℓα ≥
B
(N/3−ℓ)α , since ℓ≤N
3 −ℓ. Hence, T(ρ, τ) = (2B −
C) · 2α
Nα +
B
(N/3−ℓ)α . This expression is maximized when ℓ= N
6 , and we get that
T(ρ, τ) = B· 2α+6α
Nα
−C· 2α
Nα . This function is decreasing in C, and it is maximized
when C = B. The lifetime in this case is T(ρ, τ) = B · 2α+6α
Nα .
On the other hand, if C ≤B, then ρ1 and ρn+2 will increase to extend the
lifetime. Assume that ℓ∈R is allowed. In this case the maximum is obtained
when
C
ℓα =
B
(N/3−ℓ)α . It follows that ( N
3ℓ−1)α = B
C or ℓ=
N
3( α√
B/α+1). Hence,
T(ρ, τ) = (2B −C) · 2α
N α + C
ℓα = 2B · 2α
N α + C ·

3α( α
B/C + 1)α −2α
N α

.
The derivative is strictly positive, since
∂T(ρ, τ)
∂C
=
1
N α

3α(
α
B/C + 1)α −2α
−C · 3αα(
α
B/C + 1)α−1
α
B/C
αC

=

[3α(
α
B/C + 1)α −2α] −3α(
α
B/C + 1)α−1 ·
α
B/C

/N α
=

3α(
α
B/C + 1)α−1 
(
α
B/C + 1) −
α
B/C

−2α
/N α
=

3α(
α
B/C + 1)α−1 −2α
/N α
≥(3α2α−1 −2α)/N α,

206
M. Poss and D. Rawitz
where the inequality is implied by C ≤B and α ≥1. It follows that the function
is increasing and reaches its maximum when C = B. As mentioned above, the
lifetime in this case is T(ρ, τ) = B · 2α+6α
Nα .
It remains to compare the above mentioned value to (1). Since α ≥1, we
have that B · 2·2α+3α
Nα
< B · 2α+6α
Nα . Hence, B · 2α+6α
Nα
is the best we can hope for.
To ﬁnalize the proof, if A ∈Partition, there exists I such that 
i∈I ai = B.
A lifetime of B· 2α+6α
Nα
can be achieved by assigning ρi+1 = N
2 if i ∈I, and ρi = N
6
otherwise. On the other hand, if a lifetime of B · 2α+6α
Nα
is attainable, then the
set W(ρ) induces a partition.
⊓⊔
3.4
Strong NP-Hardness When α = 1
We show that OnceSC with α = 1 is strongly NP-hard using a reduction from
3-partition. To do that we need the following observation that was given in [2].
Observation 1 [2].
The lifetime of a OnceSC instance (x, b) is at most
2 
i bi/N. Moreover, if opt = 2 
i bi/N, then there are no coverage overlaps
or coverage outside the barrier.
Theorem 4. OnceSC with α = 1 is strongly NP-hard.
Proof. Consider a 3-partition instance A = {a1, . . . , an}, where n = 3m and
1
m

i ai = Q. (Recall that 3-partition is NP-hard even if ai ∈( Q
4 , Q
2 ), for
every i, and Q is polynomial in n.) Deﬁne an instance of OnceSC as follows.
First, let N = 8m. The instance contains n + 2m = 5m sensors, where
xi =
⎧
⎪
⎨
⎪
⎩
m + i −1
i ∈[1, m],
4m
i ∈[m + 1, 4m],
2m + i
i ∈[4m + 1, 5m],
bi =
⎧
⎪
⎨
⎪
⎩
Q ·
m+i−1
2m−2i+2
i ∈[1, m],
ai−m
i ∈[m + 1, 4m],
Q · 6m−i
2i−8m
i ∈[4m + 1, 5m].
We claim that A ∈3-partition if and only if opt = 2 
i bi/N = Q
2 ·Hm, where
Hm is the ith Harmonic number.
Supposed A ∈3-partition, i.e., there exists a partition A1, . . . , Am of A into
m triples such that 
a∈Aj a = Q. We construct a solution (ρi, τi) as follows:
ρi =
⎧
⎪
⎨
⎪
⎩
m + i −1
i ∈[1, m],
2j
i ∈[m + 1, 4m]and ai−m ∈Aj,
6m −i
i ∈[4m + 1, 5m],
and
τi =
⎧
⎪
⎨
⎪
⎩
Q
2 · Hm−i
i ∈[1, m],
Q
2 · Hj−1 +
d−1
q=0 bi
ρi
i ∈[m + 1, 4m]and i −m = ij,d,
Q
2 · Hi−4m−1
i ∈[4m + 1, 5m],
where Aj =

aij,0, aij,1, aij,2
	
. Observe that there are ﬁve sensors that are acti-
vated in the time interval [Q/2·Hℓ−1, Q/2·Hℓ), for ℓ≤m. First, sensor m−ℓ+1
and sensor 4m + ℓare activated exactly at Q/2 · Hℓ−1. Also, the three sensors
that correspond to Aℓare also activated in this interval:
www.ebook3000.com

Maximizing Barrier Coverage Lifetime with Static Sensors
207
– Sensor m + iℓ,0 is activated at Q/2 · Hℓ−1.
– Sensor m + iℓ,1 is activated at Q/2 · Hℓ−1 + biℓ,0/ρiℓ,0.
– Sensor m + iℓ,2 is activated at Q/2 · Hi−1 + biℓ,0/ρiℓ,0 + biℓ,1/ρiℓ,1.
Observe that ρm−ℓ+1 = ρ4m+ℓ= 2m −ℓand ρiℓ,d = 2ℓ, for d ∈{0, 1, 2}.
In addition, notice that sensor m + iℓ,1 is activated exactly when the battery
of sensor m + iℓ,0 is depleted, and sensor m + iℓ,2 is activated exactly when
the battery of sensor m + iℓ,1 is depleted. Moreover, since xm−ℓ+1 = 2m −ℓ,
x4m+ℓ= 6m + ℓ, and xij,d = 4m, for d ∈{0, 1, 2}, the barrier is covered with
no overlaps by the ﬁve sensors during the time interval [Q/2 · Hℓ−1, Q/2 · Hℓ).
If follows that there are no overlaps, and hence opt = 2 
i bi/N = Q
2 · Hm.
Now, assume that there exists an optimal solution (ρ′, τ ′) without excess
coverage whose lifetime is 2 
i bi/N =
Q
2 · Hm. Consider a sensors i, where
i ≤m. If ρ′
i > ρi, then energy is wasted for coverage beyond the left end-point
of the barrier. If ρ′
i < ρi, then there are several options. If sensor i is active after
the solution lifetime, then energy is wasted. Otherwise, since sensor i fails to
cover the left end-point of the barrier, the endpoint must be covered by another
sensor i′ and hence their coverage intervals must intersect, which again means
a waste of energy. It follows that ρ′
i = ρi, for every i ≤m. By a symmetric
argument we have that ρ′
i = ρi, for every i ≥4m + 1.
Since the rest of the sensors are located at 4m, the sensors in the two sets
{1, . . . , m} and {4m + 1, . . . , 5m} must work in the same pairs as in (ρ, τ), since
otherwise energy is wasted due to overlaps. More speciﬁcally, if at some time a
sensor i ≤m, does not work with sensor 5m −i + 1, then if it covers the barrier
only with a sensor located at 4m, then i is redundant and there is a coverage
overlap. If it covers the barrier with a sensor i′ ≥4m+1, such that i′ ̸= 5m−i+1,
then they need help from a sensor located at 4m, and its covering interval will
overlap with the interval of i or with the interval of i′.
It follows that the solution can be partitioned into m temporal strips each
corresponding to a sensors pair i and 5m −i + 1. In each such strip, sensors
located at 4m ﬁll the gap in coverage [2m + 2i −2, 6m −2i + 2] that lasts
bi
ρi =
Q
2m−2i+2 time units left by the ith sensor pair. Since each such gap must
be ﬁlled exactly by sensors located at 4m, there is a set of sensors ﬁlling the
gap with radii 2m −2i + 2. Since they need to cover the gap for
Q
2m−2i+2 time,
they combined batteries should add up to Q. Hence, the strips induce a partition
of the sensors into m subsets, such that the total power of a subset is Q. The
partition induces a partition of A, and hence A ∈3-partition.
⊓⊔
4
Approximation Algorithms
We present several algorithmic results. We provide a 5γα-approximation algo-
rithm for OnceSCk, where γ is the maximum ratio between two radii associated
with the same sensor. In addition, we give a 5-approximation algorithm for SCk,
for every k ≥1, and a 5 · 2α-approximation algorithm for Strip Cover. The
algorithms are based on the 5-approximation algorithm RSC from [7]. Most
proofs of this section were omitted for lack of space.

208
M. Poss and D. Rawitz
4.1
Algorithm for Set Once Strip Cover with Fixed Radii
We start by presenting a 5γα-approximation algorithm for OnceSCk, for k ≥1,
where γ is the maximum ratio between two radii that belong to the same sensor.
That is γ
△= maxi {ri,ki/ri,1}, where ki = |Ri|.
The next lemma bounds the performance of an assignment that uses only
the largest radii. The idea is to shrink the lifetime such that the batteries would
be able to support the larger radii.
Lemma 3. Given an instance of OnceSCk, there exists a γα-approximate solu-
tion (ρ, τ), where ρi ∈{0, ri,ki}, for every i.
It follows that we can use an algorithm for RSC with the largest radii to
obtain an approximate solution for OnceSCk.
Theorem 5. There is a polynomial time 5γα-approximation algorithm for
OnceSCk, for k ≥2.
4.2
Strip Cover with Multiple Fixed Radii
Next we provide a 5-approximation algorithm for SCk, for every k ≥1, that is
based on the 5-approximation algorithm RSC by Gibson and Varadarajan [7].
Given an RSC (i.e., OnceSC1) instance, deﬁne Ii
△= [xi −ri, xi + ri] and
deﬁne the load of a sensor i by ℓi
△= bi/ri. Also, deﬁne load(p)
△= 
i:p∈Ii ℓi, for
every p ∈U.
Observation 2 [7]. Given an RSC instance, opt ≤minp∈U load(p).
Given an SCk instance, deﬁne ℓi,j
△= bi/rα
i,j and Ii,j
△= [xi −ri,j, xi + ri,j].
Also, let P = 
i,j {xi −ri,j, xi + ri,j}, and let p1, . . . , p|P | be a non decreasing
ordering of the point in P. Deﬁne
I
△=

[0, p1], [p|P |, N]
	
∪
|P |−1

j=1
{[pj, pj+1]} .
Observe that |P| , |I| = O(kn). We are now ready to present an integer program
for the problem of ﬁnding the maximum load:
max
L
s.t.

i,j:I⊆Ii,j ℓi,jzi,j ≥L
∀I ∈I

j zi,j ≤1
∀i
zi,j ∈{0, 1}
∀i, j
(2)
where zi,j stands for whether i is used with radius ri,j. The ﬁrst set of constraints
ensure that the barrier is covered, while the second set makes sure that sensors
are not over used. An LP-relaxation is obtained by replacing the intergrality
constraints by zi,j ≥0, for every i and j.
www.ebook3000.com

Maximizing Barrier Coverage Lifetime with Static Sensors
209
Observation 3. Given an SCk instance, we have that L∗≥opt, where (z∗, L∗)
is an optimal fraction solution of (2).
Proof. An optimal solution induces a solution (z, L) such that opt ≤L.
⊓⊔
We use an optimal solution to (2) to obtain an approximate solution for SCk.
Theorem 6. There exists a polynomial time 5-approximation algorithm for
SCk, for every k ≥1. Moreover, each sensor uses each of its radii at most
once in the computed solution.
4.3
Strip Cover
Observe that an Strip Cover instance can be view as a SCk instance with k =
N and Ri = {1, . . . , N}, for every i. Hence, Theorem 6 implies a 5-approximation
algorithm if N is polynomial in the input size.
Theorem 7. There exists a 5-approximation algorithm for Strip Cover whose
running time is polynomial in the input size and in N. Moreover, each sensor
uses each of its radii at most once in the computed solution.
The next step is to remove the dependency on N.
Lemma 4. Let ρ be a solution for SCk where k = N and Ri = {1, . . . , N}, for
every i. Then, there exists a solution ρ′ such that ρ′(t) ∈

2j : j = 1, . . . , log N
	
and T(ρ′) ≥2αT(ρ).
Using Lemma 4 one may reduce the running time at the expense of the
approximation ratio.
Theorem 8. There exists a 5 · 2α-approximation algorithm for Strip Cover
whose running time is polynomial in the input size and in log N. Moreover, only
radii from the set

2j : j = 1, . . . , log N
	
are used, and each sensor uses each of
these radii is used at most once.
5
Discussion and Open Questions
Bar-Noy et al. [2] showed that OnceSC with α = 1 is NP-hard and that it has
a 3
2-approximation algorithm. We show that, when α = 1, OnceSC is strongly
NP-hard. We also prove that OnceSC is NP-hard if α > 1. Presenting an
approximation algorithm for the case where α > 1 remains an open question.
We showed that OnceSCk, for any k ≥2 and α ≥1, is NP-hard, even for the
case of uniform radii sets. On the other hand, we presented a 5γα-approximation
algorithm for OnceSCk, for k ≥1. If γ = O(1), the approximation ratio is O(1).
It would be interesting to come up with an approximation algorithm whose ratio
is independent of γ and/or of α.
As for Strip Cover, there is a
3
2-approximation algorithm for the case
where α = 1 [2], and we presented a 5 · 2α-approximation algorithm for α > 1.
In addition, we gave a 5-approximation algorithm for SCk, for every k ≥1.
However, it is unclear whether either one of these problems is NP-hard.

210
M. Poss and D. Rawitz
References
1. Bar-Noy, A., Baumer, B.: Average case network lifetime on an interval with
adjustable sensing ranges. Algorithmica 72(1), 148–166 (2015)
2. Bar-Noy, A., Baumer, B., Rawitz, D.: Brief announcement: set it and forget it -
approximating the set once strip cover problem. In: 25th ACM Symposium on
Parallelism in Algorithms and Architectures, pp. 105–107 (2013). To appear in
Algorithmica
3. Buchsbaum, A.L., Efrat, A., Jain, S., Venkatasubramanian, S., Yi, K.: Restricted
strip covering and the sensor cover problem. In: 18th Annual ACM-SIAM Sympo-
sium on Discrete Algorithms, pp. 1056–1063 (2007)
4. Buchsbaum, A.L., Efrat, A., Jain, S., Venkatasubramanian, S., Yi, K.: Restricted
strip covering and the sensor cover problem. Technical report arXiv:cs/0605102v1,
CoRR (2008)
5. Fan, H., Li, M., Sun, X., Wan, P., Zhao, Y.: Barrier coverage by sensors with
adjustable ranges. ACM Trans. Sens. Netw. 11(1), 14:1–14:20 (2014)
6. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman and Company, New York (1979)
7. Gibson, M., Varadarajan, K.: Decomposing coverings and the planar sensor cover
problem. In: 50th Annual IEEE Symposium on Foundations of Computer Science,
pp. 159–168 (2009)
8. Lev-Tov, N., Peleg, D.: Polynomial time approximation schemes for base station
coverage with minimum total radii. Comput. Netw. 47(4), 489–501 (2005)
9. Moscibroda, T., Wattenhofer, R., Zollinger, A.: Topology control meets SINR: the
scheduling complexity of arbitrary topologies. In: 7th ACM Interational Sympo-
sium on Mobile Ad Hoc Networking and Computing, pp. 310–321 (2006)
10. Wan, P.-J., Chen, D., Dai, G., Wang, Z., Yao, F.F.: Maximizing capacity with
power control under physical interference model in duplex mode. In: 31st Annual
IEEE International Conference on Computer Communications, pp. 415–423 (2012)
www.ebook3000.com

Independent Sets in Restricted Line
of Sight Networks
Pavan Sangha(B), Prudence W. H. Wong, and Michele Zito
Department of Computer Science, University of Liverpool, Liverpool, UK
{p.sangha2,pwong,michele}@liverpool.ac.uk
Abstract. Line of Sight (LoS) networks were designed to model wireless
networks in settings which may contain obstacles restricting visibility of
sensors. A graph G = (V, E) is a 2-dimensional LoS network if it can be
embedded in an n × k rectangular point set such that a pair of vertices
in V are adjacent if and only if the embedded vertices are placed on the
same row or column and are at a distance less than ω. We study the
Maximum Independent Set (MIS) problem in restricted LoS networks
where k is a constant. It has been shown in the unrestricted case when
n = k and n →∞that the MIS problem is NP-hard when ω > 2 is ﬁxed
or when ω = O(n1−ϵ) grows as a function of n for ﬁxed 0 < ϵ < 1. In this
paper we develop a dynamic programming (DP) algorithm which shows
that in the restricted case the MIS problem is solvable in polynomial time
for all ω. We then generalise the DP algorithm to solve three additional
problems which involve two versions of the Maximum Weighted Indepen-
dent Set (MWIS) problem and a scheduling problem which exhibits LoS
properties in one dimension. We use the initial DP algorithm to develop
an eﬃcient polynomial time approximation scheme (EPTAS) for the MIS
problem in restricted LoS networks. This has important applications, as
it provides a semi-online solution to a particular instance of the schedul-
ing problem. Finally we extend the EPTAS result to the MWIS problem.
1
Introduction
LoS network. A wireless network typically consists of wireless devices that use
data connections to communicate wirelessly. Geometric graphs often provide a
good model for such networks with vertices representing wireless devices, and
edges representing communication between pairs of devices. Various types of
geometric graphs have been proposed to model wireless sensor networks. The
disk intersection model [5] is a commonly used one. Vertices are placed in some
physical space with the communication range for a vertex represented by a cir-
cle of some prescribed radius. Edges are formed between pairs of vertices whose
circles overlap. A beneﬁt of this model is its ability to capture the constraint
of communication range restriction. This restriction implies that vertices should
be close in distance in order to communicate. Another constraint exhibited by
real world wireless networks are line of sight restrictions, often due to the pres-
ence of a large number of obstacles, like those often found in urban settings
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 211–222, 2017.
https://doi.org/10.1007/978-3-319-72751-6_16

212
P. Sangha et al.
for example. With this restriction vertices can only communicate if they are
both close in distance and also share a direct line of sight. While the presence
of obstacles can be diﬃcult to model, it is clear that a good model of wireless
network should ideally incorporate both communication range restrictions and
line of sight restrictions. Frieze et al. [9] introduced the notion of random Line of
Sight networks to provide a model that incorporates both. Their work focused
on structural properties of the model focusing on connectivity. Since then con-
nectivity in higher dimensions, percolation and communication problems within
the same model have been studied [3,6,8].
For positive integers k and n let Zn,k = {(x, y) : x ∈{1, . . . , n}, y ∈
{1, . . . , k}} denote the underlying point set. For two points p1 = (x1, y1), p2 =
(x2, y2) ∈Zn,k, let d(p1, p2) = |x1 −x2| + |y1 −y2| denote the Manhattan dis-
tance between p1 and p2. The distance metric we use diﬀers from the one used
in [9] which relies on the underlying point set being toroidal to ease calculations.
We say points p1 and p2 share a line of sight if x1 = x2 or y1 = y2. A graph
G = (V, E) is said to be a Line of Sight (LoS) network with parameters n, k
and ω if there exists an embedding fG : V →Zn,k such that {u, v} ∈E if and
only if fG(u) and fG(v) share a line of sight and d(fG(u), fG(v)) < ω. We refer
to ω as the range parameter which is used to model the communication range
restriction. The range parameter can be a constant or grow monotonically as a
function of the parameters n or k.
MIS. We focus on the Maximum Independent Set (MIS) problem [7] in LoS
networks. The MIS in a graph can be seen as a measure of network dispersion
and independent sets share links with other important graph properties like
vertex covers [7]. For n × k line of sight networks, large independent sets could
help in covering scenarios like the following: “New York has many more streets
than avenues. On parade days the police may want to dominate all junctions by
also maximising presence”. In general graphs ﬁnding the largest independent set
is NP-hard [10] and even ﬁnding good approximation solutions are diﬃcult [11].
For LoS networks in particular, if ω = 2 or n, it is not diﬃcult to see that the
problem can be solved optimally. Sangha and Zito [12] showed on the other hand
for ω = O(n1−ϵ) where 0 < ϵ < 1 is ﬁxed the problem is NP-hard. They also
provide approximation results, showing the problem admits a 2-approximation
for any ω and an eﬃcient polynomial time approximation scheme (EPTAS) [4]
for constant ω.
Additional applications. The abstract problem we study also ﬁnds application
[2] in the following scheduling problems. Suppose an advertisement company
manages advertisements from some number of clients (cf. k) over a long period
of discrete time (cf. n). At any time advertisements of some subset of clients are
available to be aired but the company can only select a certain number (cf. l) of
them to advertise due to resource limitation. In addition some “advertisement
diversity” policy requires that advertisements from the same client cannot be
aired more than once in a given period of time (cf. ω). The goal of the company
is to schedule the airing of these advertisements satisfying the constraints and
maximise the number of advertisements aired (cf. MIS). This problem has one
www.ebook3000.com

Independent Sets in Restricted Line of Sight Networks
213
slight diﬀerence from the LoS problem in the sense that the restriction with ω
only applies on one dimension (the time dimension) but not the other (the client
dimension). Nevertheless, as to be showed later, the solution we develop can be
adapted to solve this problem.
Our contribution. In this paper we study the MIS problem on restricted LoS
networks. We focus on the restricted case that parameter k is a constant and
k < ω, in which case we show in Sect. 3 that the problem becomes polynomial
time solvable via a dynamic programming (DP) algorithm. We also show in
Sect. 4 that the DP algorithm can be extended to solve (i) the maximum weighted
independent set problem (MWIS) on restricted LoS networks, (ii) MWIS on
LoS networks with k > ω but both k and ω being constants, and (iii) the
advertisement scheduling problem mentioned above. We then in Sect. 5 apply
the DP algorithm to develop an EPTAS which improves the EPTAS in [12] by
showing that when k is restricted to being constant the algorithm no longer
requires ω to be constant. In addition we show that the EPTAS leads to a semi-
online algorithm [1] with performance ratio (1 + ϵ), which requires a look-ahead
distance dependent on ϵ. This gives a semi-online algorithm for the scheduling
problem in the case where k < ω and l = 1.
2
Problem Deﬁnitions and Preliminaries
The dynamic programming algorithm uses arrays consisting of 0, 1 elements. An
array of size x × y consists of x rows and y columns. We start by introducing
some notations.
– For any array A, a sub-array A[i..j] contains columns i, i + 1, · · · , j.
– For any two arrays A1, A2 of size x×y, we say that A1 agrees with A2, denoted
by A1 ≤a A2, if A1[i, j] ≤A2[i, j] for all 1 ≤i ≤x and 1 ≤j ≤y.
– For any array A, we denote by h(A) the “head” subarray of A containing all
but the last column of A; and t(A) the “tail” subarray of A containing all but
the ﬁrst column of A. That is, h(A) and t(A) have y −1 columns if A has y
columns.
– A1 is said to be tail-aligned with A2 if t(A1) is the same as h(A2), i.e.,
t(A1) ≡h(A2). In this case, we say A2 is head-aligned with A1.
– Let the column sum of an array A at column y be cs(A, y) = 
x A[x, y].
Given a LoS network G in Zn,k let array(G) satisﬁes array(G)[i, j] = 1 if
and only if location (j, i) ∈Zn,k in the LoS embedding of G contains a vertex,
otherwise array(G)[i, j] = 0 Fig. 1 provides an example. Given array(G) of size
k × p where ω ≤p, an independent array I of array(G) is any array of size k × p
satisfying (i) I ≤a array(G) and (ii) I contains at most one 1 in each column and
for any row i and distinct columns j1, j2, if I[i, j1] = 1 and I[i, j2] = 1 it must
be the case that |j1 −j2| ≥ω. We refer an independent array W speciﬁcally of
size k × ω as a “feasible array”. Since any feasible array has exactly ω columns
it contains at most one 1 per column but also at most one 1 per row. We denote
by F the set of all feasible of arrays of size k × ω.

214
P. Sangha et al.
Fig. 1. Figure (i) is a graph G and Figure (ii) is its LoS embedding in Z8,4 with ω = 4.
Figure (iii) represents the array layout of G and Figure(iv) is an independent array of
largest array sum, corresponding to the largest independent set in the graph G.
There is a clear connection between an independent array I of array(G) and
an independent set I of the LoS network G. More precisely given a set I ⊆V
of a LoS network G = (V, E) embedded in Zn,k consider the array I satisfying
I[i, j] = 1 if and only if location (j, i) ∈Zn,k contains a vertex in I. We observe
the following:
1. I is an independent set of G if and only if I is an independent array of the
array G.
2. |I| = k
i=1
n
j=1 I[i, j].
We refer to the quantity k
i=1
n
j=1 I[i, j] as the array sum of I which we
denote by as(I). Using the two observations above it follows that ﬁnding the
maximum independent set in a LoS network G embedded in Zn,k is equivalent
to ﬁnding the independent array of the array G with the largest array sum, we
refer to such an array as a largest independent array. In Sect. 3 we show how our
DP algorithm ﬁnds the largest independent array of array(G) by scanning the
feasible arrays of array(G).
3
Dynamic Programming
3.1
Algorithm
In this section we present a dynamic programming (DP) algorithm for ﬁnding
a largest independent array. For simplicity we refer to array(G) as G. It will be
clear from the context when referring to the LoS network G instead of the array
G. Given the array G of size k × n, for the purposes of the DP algorithm we
prepend G with ω columns consisting entirely of 0’s. We index these columns
from −(ω −1), . . . , 0. Thus the input array G is of size k × (ω + n). The DP
algorithm works by sequentially computing the array sums of G[−(ω −1)..j]
in G. The process keeps track of various sums MIS(j, W) depending on the
independent set choices in the right-most column of G[−(ω −1)..j]. For j =
0, . . . , n let FG,j ⊆F be the set of feasible arrays W satisfying W ≤a G[j −
ω + 1..j]. Note that in particular any independent array I of G[−(ω −1)..j] for
1 ≤j ≤n satisﬁes I[j −ω + 1..j] ≡W for some W ∈FG,j.
Algorithm 1 describes how to compute the array sum on an independent array
of G[−(ω −1)..j] from information about G[−(ω −1)..(j −1)]. More speciﬁcally
www.ebook3000.com

Independent Sets in Restricted Line of Sight Networks
215
Algorithm 1. Computing the size of the largest independent array in G
1: Initialise: MIS(0,⃗0) = 0, where ⃗0 is the k × ω array of all 0’s.
2: for j = 1, . . . , n do
3:
for W ∈F do
4:
if W ∈FG,j then
5:
MIS(j, W) = maxW ′∈FG,j−1:t(W ′)≡h(W ) MIS(j −1, W ′) + cs(W, ω)
6:
else
7:
MIS(j, W) = 0
8:
end if
9:
end for
10:
MIS(j) = maxW ∈FG,j MIS(j, W)
11: end for
if W ∈FG,j we try to extend the independent arrays in G[−(ω −1)..(j −1)]
to independent arrays in G[−(ω −1)..j]. Let I′ be an independent array in
G[−(ω −1)..(j −1)] such that I[(j −ω)..(j −1)] ≡W ′ for some W ′ ∈FG,j−1
and t(W ′) ≡h(W). By considering the next column of G, we extend I′ to an
independent array I of G[−(ω −1)..j] which satisﬁes I[j −ω + 1..j] ≡W. There
are two cases for the sum cs(W, ω):
1. cs(W, ω) is 0 meaning that there is no entry in the last column of W. Then
as(I) = as(I′) and hence the array sum for our independent set does not
increase;
2. cs(W, ω) is 1 meaning that we can increase the array sum of our independent
array by 1; note that since W is feasible, cs(W, ω) is at most 1.
The new independent array can be obtained by extending the one for G[−(ω−
1)..(j −1)] by appending the ω-th column of W. Figure 2 shows an example.
Fig. 2. Figure (i) shows the ﬁrst 8 columns of an array G and the independent array
I′ of G[1..8] has the largest array sum satisfying I′[6..8] ≡W ′. In Figure (ii) the
independent array I is the independent array of G[1..9] which has the largest array
sum satisfying I[7..9] ≡W. Note t(W ′) ≡h(W) and that I can be obtained from I′
by appending the last column of W to I′.
3.2
Correctness
In this section we prove the correctness of Algorithm 1. We ﬁrst prove in Lemma 1
that it is suﬃcient to consider FG,j and then the main result in Theorem 2.

216
P. Sangha et al.
Lemma 1. For j = 0, . . . , n −1 for each W ′
1 ∈FG,j there exists W1 ∈FG,j+1
such that t(W ′
1) ≡h(W1) and for each W2 ∈FG,j+1 there exists W ′
2 ∈FG,j such
that t(W ′
2) ≡h(W2).
Proof. Take W ′
1 ∈FG,j and consider the simple feasible array W1 satisfying (i)
t(W ′
1) ≡h(W1) and (ii) the last column of W1 consists entirely of 0’s. Combining
(i) and (ii) with the fact that W ′
1 ≤a G[j−ω+1..j] we conclude W1 ≤a G[j−ω+
2..j+1] and thus W1 ∈FG,j+1. Similarly given W2 ∈FG,j+1 consider the feasible
array W ′
2 satisfying (i) t(W ′
2) ≡h(W2) and (ii) the ﬁrst column consists entirely
of 0’s. Using similar reasoning to the ﬁrst case we can conclude W ′
2 ∈FG,j.
⊓⊔
Theorem 2. For 1 ≤j ≤n, MIS(j) computed by Algorithm 1 gives the size of
the maximum independent set in the graph G[j] induced by the vertices of the
ﬁrst j columns in Zn,k.
Proof. Recall that MIS(j) = maxW ∈FG,j MIS(j, W) for all 1 ≤j ≤n and let
OPT(j) denote the size of the maximum independent set in the graph G[j]
induced by the vertices of the ﬁrst j columns of Zn,k for all 1 ≤j ≤n. We
prove the theorem by induction on j. Firstly Lemma 1 proves that for each
W ∈FG,j there exists W ′ such that maxW ′∈FG,j−1:t(W ′)≡h(W ) MIS(j −1, W ′) is
well deﬁned. Next consider the case j = 0, since the ﬁrst ω columns (indexed
from −(ω−1), . . . , 0) consist entirely of 0’s it implies that MIS(0) = 0. In addition
let ⃗0 ∈F denote the feasible array consisting entirely 0’s then FG,0 = {⃗0} and
consequently MIS(0) = MIS(0,⃗0) = 0.
Base case: If cs(G[−(ω −1)..1], 1) = 0 then OPT(1) = 0. FG,1 = {⃗0} and
clearly MIS(1,⃗0) = 0. Using the facts that (i) FG,0 = {⃗0}, (ii) MIS(0,⃗0) = 0, (iii)
cs(⃗0, ω) = 0 and (iv) t(⃗0) = h(⃗0) it follows that MIS(1,⃗0) = MIS(0,⃗0) + cs(⃗0, ω).
For all W ̸∈FG,1, MIS(1, W) = 0 as it must be the case that W ≤a G[−(ω −
2)..1]. This means that MIS(1) = 0, which equals to OPT(1).
Otherwise cs(G[−(ω−1)..1], 1) > 0 meaning OPT(1) = 1. In this case FG,1 ̸=
{⃗0}. Thus there exists W ∈FG,1 satisfying t(⃗0) = h(W) and W ̸= ⃗0, and thus
cs(W, ω) = 1. Since W it head-aligned with ⃗0 and contains a 1 in its ﬁnal
column we conclude MIS(1, W) = 1. Finally since FG,0 = {⃗0} and MIS(0,⃗0) = 0
it follows that MIS(1, W) = MIS(0,⃗0) + cs(W, ω). Again for all W ̸∈FG,1,
MIS(1, W) = 0. Then MIS(1) equals to 1, which equals OPT(1).
Inductive step: Suppose that the result holds for all columns of G indexed
from 1, . . . , j −1, i.e., MIS(i) equals to OPT(i) for all 1 ≤i < j. We show that
this implies MIS(j) equals to OPT(j). Assume on the contrary that there exists
an independent array I in G[−(ω −1)..j] satisfying as(I) > MIS(j) and suppose
W ∗∈FG,j satisﬁes W ∗≡I[j −ω + 1..j]. Then as(I) > MIS(j) ≥MIS(j, W ∗)
and thus
as(I) > maxW ′∈FG,j−1:t(W ′)≡h(W ∗)MIS(j −1, W ′) + cs(W ∗, ω).
(1)
Consider the independent array I′ in G[−(ω−1)..j−1] obtained by removing
the last column of I. Then it follows that as(I′) = as(I) −cs(I, j). Furthermore
www.ebook3000.com

Independent Sets in Restricted Line of Sight Networks
217
consider the simple feasible array W ′′ ∈FG,j−1 satisfying I′[j −ω..j −1] ≡W ′′,
then t(W ′′) ≡h(W ∗). In addition cs(I, j) = cs(W ∗, ω) since the last column
of I is the same as the ωth column of W ∗. Thus as(I′) = as(I) −cs(W ∗, ω),
substituting this into Inequality (1) we obtain
as(I′) > maxW ′∈FG,j−1:t(W ′)≡h(W ∗)MIS(j −1, W ′).
Since I′[j −2 −ω..j −1] ≡W ′′ and W ′′ ∈FG,j−1 satisfying t(W ′′) ≡h(W ∗)
this is a contradiction to the optimality of max MIS(j −1, W ′) for W ′ ∈FG,j−1
satisfying t(W ′) ≡h(W ∗).
⊓⊔
3.3
Time Complexity
In this section we provide an upper bound on the worst case time complexity
of the DP algorithm. We describe how the use of n separate bipartite graphs
allow us to compute MIS(j) for 1 ≤j ≤n. Each bipartite graph consists of
two sets of feasible arrays namely those which agree with G[−(ω −1)..(j −1)]
and G[−(ω −1)..j] respectively. Edges between classes represent pairs of arrays
which are tail-head aligned.
For an array G and 0 ≤j ≤n −1 Bj = (FG,j, FG,j+1, E) is a directed
bipartite graph to model the tail-head alignment of arrays with classes FG,j
and FG,j+1. For a pair of simple feasible arrays W ∈FG,j and W ′ ∈FG,j+1,
(W, W ′) ∈E if and only if t(W) ≡h(W ′).
For a directed graph G = (V, E) and vertex v ∈V let N −(v) = {u ∈V :
(u, v) ∈E} and d−(u) = |N −(u)|. Similarly let N +(v) = {u ∈V : (v, u) ∈E}
and d+(u) = |N +(u)|. Let Δ+(G) = maxv∈V d+(v) denote the maximum out-
degree of G. The bipartite graph Bj is used to compute MIS(j + 1, W) for each
W ∈FG,j+1 by considering N −(W) and selecting the array W ′ ∈N −(W) for
which MIS(j, W ′) is maximised.
An important piece of information required for the time complexity is to
obtain an upper bound on |FG,j| for all 0 ≤j ≤n. We do this by obtaining
an upper bound on |F|. We show that |F| = Θ(ωk) through the following two
observations. Firstly |F| ≤(ω + 1)k since each simple feasible array contains at
most one 1 per row, or not contain a 1 at all. Secondly
(ω)!
(ω−k)! ≤|F| since there
are precisely ω(ω −1)(ω −2) · · · (ω −(k −1)) simple feasible arrays with exactly
one 1 in each row. Thus |FG,j| = O(ωk). We make use of the following lemma
in the calculation of the worst case running time of the algorithm.
Lemma 3. For each Bj, the maximum out degree Δ+(Bj) ≤k + 1
Proof. For each array in W ∈FG,j there are at most k arrays W ′ ∈FG,j+1
with a single 1 in the ﬁnal column satisfying t(W) ≡h(W ′) (one in each of the
k possible locations). Combining this with the array consisting of entirely 0’s in
its ﬁnal column gives us a k + 1 possible feasible arrays.
⊓⊔
Theorem 4. The worst case running time of the DP algorithm is O(nkωk).

218
P. Sangha et al.
Proof. For each W ∈FG,j+1 using Algorithm 1 we obtain MIS(j + 1, W) by
comparing MIS(j, W ′) for each W ′ ∈N −(W). Using Lemma 3 and the fact that
|FG,j| = O(ωk) it can be seen that there at most O(kωk) computations per
bipartite graph Bj. Finally given that there are n bipartite graphs Bj we obtain
a worst case running time of O(nkωk).
⊓⊔
4
Extensions
In this section we extend our DP algorithm in Sect. 3 taking weight into account
and show how it provides solutions to the following three problems: (i) The
maximum weighted independent set problem for k < ω for constant k, (ii) The
maximum weighted independent set problem for k > ω for constant k and ω,
and (iii) A weighted version of the scheduling problem with parameter 1 ≤l ≤k
for constant k.
Framework. W.l.o.g., we normalise the weight such that the minimum non-zero
weight is 1, in other words, G[i, j] = 0 or G[i, j] ≥1 for all [i, j]. Let W be the set
of all k ×ω arrays with 0, 1 entries. A basis set F ⊆W satisﬁes the followings (i)
{⃗0} ∈F, (ii) If W ∈F then ∃W ′ ∈F where t(W) ≡h(W ′) and the last column
of W ′ is all 0’s and (iii) if W ∈F then ∃W ′′ ∈F where t(W ′′) ≡h(W) and the
ﬁrst column of W ′′ is all 0’s. We extend some notations of Sect. 2 to account for
the array generalisation. Given arrays G and I of the same size I ≤a G if (i)
G[i, j] = 0 ⇒I[i, j] = 0 and (ii) 1 ≤I[i, j] ≤G[i, j] for all G[i, j] ̸= 0. Since I
may contain values greater than 1 and W contains only 0, 1 we introduce an
additional notion of equivalence, denoted by ≡a. Given an array I of size k × ω,
we say that I ≡a W provided I[i, j] = 0 if and only if W[i, j] = 0.
We extend G to an array of size k × (ω + n) with columns indexed from
−(ω−1), . . . , n and the ﬁrst ω columns consisting of entirely 0’s. Given a basis set
F let FG,j denote the set of feasible arrays W ∈F satisfying W ≤a G[j−ω+1..j].
The goal is to ﬁnd an array I of maximum array sum satisfying I ≤a G and
I[j −ω + 1..j] ≡a W for some W ∈FG,j for all 1 ≤j ≤n. It is important
to note that ⃗0 ∈FG,j for all 1 ≤j ≤n and so the array of size k × (ω + n)
consisting entirely of 0’s satisﬁes the required conditions proving such an array
always exists. Let OPT(j) denote the array sum of the largest array satisfying
the conditions for G[−(ω −1)..j] for all 1 ≤j ≤n. We are required to compute
OPT(n).
Algorithm 1 can be extended by modifying the main recurrence as follows:
F(0,⃗0) = 0 and for each W ∈F and 1 ≤j ≤n let
F(j, W) =
⎧
⎨
⎩
max
W ′∈Fj−1:t(W ′)≡h(W ) F(j −1, W ′) + W[ω]T · G[j]
if W ∈FG,j,
0
otherwise.
Note W[ω]T · G[j] denotes the dot product of the ωth column of W and the
jth column of G. Let F(j) = maxW ∈FG,j F(j, W), we use the following theorem
to calculate OPT(n).
www.ebook3000.com

Independent Sets in Restricted Line of Sight Networks
219
In the full paper we prove that the recurrence and the associated dynamic
programming algorithm gives an optimal value.
Theorem 5. F(j) is equal to OPT(j) for all 1 ≤j ≤n.
We analyse the worst case running time of the algorithm in a similar way to
the case for the maximum independent set problem let Bj = (FG,j, FG,j+1, E)
for 0 ≤j ≤n −1. Let Δ+(Bj) denote the largest out-degree of an array in Bj
for 0 ≤j ≤n −1 and let Δ+(F) = maxj(Δ+(Bj)). We then prove the following
lemma regarding the running time.
Lemma 6. The
worst
case
running
time
of
the
generalised
DP
is
O(nΔ+(F)|F|).
Proof. Given Bj the number of computations required to compute F(j + 1, W)
for W ∈Fj+1 is proportional to the in-degree d−(W). Thus the total number of
computations required for Bj is proportional to the number of edges which is at
most Δ+(F)|F|, since there are n bipartite graphs, the result follows.
⊓⊔
Applications of the extension. We now show how we solve the three problems
introduced at the start of the section by choosing the basis set F corresponding
to the problem.
Weighted independent set. We consider the maximum weighted independent
set problem in LoS networks with k < ω, where each weight assigned to a vertex
has a value least 1. G[i, j] is the weight assigned to the vertex in location [i, j]
of the LoS embedding of G. Since this is just the weighted version our initial
problem we keep the same basis F which consists of all arrays with at most one
1 in each column and row.
Weighted independent set for larger k. We consider the maximum weighted
independent set problem in LoS networks with parameter k > ω where k ∈N
is a constant. In this case an independent set I can have more than one 1 in a
column. In particular I satisﬁes that (i) for distinct columns j1, j2 if I[i, j1] = 1
and I[i, j2] = 1 then |j1 −j2| ≥ω and (ii) for distinct rows i1, i2 if I[i1, j] = 1
and I[i2, j] = 1 then |i1 −i2| ≥ω. Thus the basis set F represents the set of
k × ω arrays having at most one 1 per row and for each column the distance
between any pair of 1’s needs to be at least ω.
Weighted scheduling problem. We consider the scheduling problem where the
parameter 1 ≤l ≤k with the addition that prices charged have diﬀerent weights
that are at least the value 1. Thus G[i, j] contains the price charged to client i
on day j. In this problem the basis set F is the set of all k × ω arrays which
consist of at most one 1 in each row and at most l, 1’s in each column.
We use Lemma 6 to calculate the worst-case running times are O(nkωk) for
the ﬁrst problem, O(ntt(ω)(t+1)ω) for the second where t =
 k
ω

, and O(nklωk)
for the third. Full details are provided in the full paper.

220
P. Sangha et al.
5
EPTAS
The DP algorithms in Sects. 3 and 4 give optimal solutions to an oﬄine ver-
sion of the MIS problem in LoS networks and scheduling problem where the
entire input is known in advance. This is unrealistic for example the duration
in the scheduling problem is large possibly spanning a year or more then it is
likely the input evolves over time. It is desirable to take a more online app-
roach with a good approximation performance. We improve the running time of
the EPTAS in [12] based on the DP algorithms, providing a solution which is
semi-online; in particular we assume we are allowed to observe the input up to a
certain “look-ahead” distance. We show how the look-ahead distance inﬂuences
the approximation ratio achieved.
Given a LoS network G, let Gj and Gj = G\Gj denote the induced subgraph
of G consisting of vertices which are embedded in the region with x-coordinates
from 1 to jω and from jω+1 to n, respectively. Let Ir be a maximum independent
set in Gr. We determine a value r∗which is the “stopping point” of the overhead
distance necessary to achieve (1 + ϵ)-approximation. Precisely, we let r∗be the
smallest integer such that |Ir∗+1| < (1+ϵ)|Ir∗|. This means that for any 1 < r ≤
r∗, |Ir| ≥(1 + ϵ)|Ir−1| (note that |Ir| ≤kr due to the structural properties of a
LoS network embedding). We ﬁrst show an upper bound on r∗(proof is deferred
to the full paper).
Lemma 7. r∗≤

1+√
1+4ln(1+ϵ)ln(k)
2ln(1+ϵ)
	2
To obtain a (1+ϵ)-approximation to the maximum independent set once r∗is
obtained we remove Gr∗+1 from the graph G and apply the procedure iteratively
to the graph Gr∗+1. If I′ is the independent set obtained from applying the
procedure to Gr∗+1 then we show that Ir∗∪I′ is a (1 + ϵ)-approximation to the
maximum independent set in G.
Lemma 8. Suppose that I′ is a (1 + ϵ)-approximate independent set in Gr∗+1,
then I ≡Ir∗∪I′ is (1 + ϵ)-approximate for G.
Proof. Recall that Ir∗+1 is the largest independent set in Gr∗+1, since |Ir∗+1| <
(1+ϵ)|Ir∗| it follows that Ir∗is a (1+ϵ)-approximate independent set on Gr∗+1.
Using the properties of LoS networks, for any vertex v ∈Ir∗the distance between
v and a neighbour u ∈N(v) is at most ω. Thus the neighbourhood ∪v∈Ir∗N(v)
belongs to Gr∗+1 and we can conclude that Ir∗∪I′ is an independent set in G. We
denote by α the independence number. Finally, α(G) ≤α(Gr∗+1)+α(Gr∗+1)) ≤
(1 + ϵ)|Ir∗| + (1 + ϵ)|I′| = (1 + ϵ)|Ir∗∪I′|, giving us the required result.
⊓⊔
Suppose we deﬁne one iteration of the algorithm as the process of reaching the
ﬁrst stopping point and second iteration as the next process of reaching the
second stopping point and so on. Given r∗= O(ω) using Theorem 4 we can
deduce computing Ir using the DP algorithm has a worst case running time of
O(ωkωk). Since we repeat this calculation r∗times in each iteration, the worst
www.ebook3000.com

Independent Sets in Restricted Line of Sight Networks
221
case running time of an iteration is O(kωk+2). Finally since there are at most n
iterations the EPTAS has a worst case running time of O(nkωk+2).
We now turn our attention to the uses of the EPTAS for the scheduling
application in the case where k is constant, k < ω and l = 1, note under these
restrictions the goal of the scheduling problem is equivalent to the MIS problem.
Suppose the advertisement company does not have a full schedule but would like
to start processing a schedule given some partial information. The EPTAS shows
a look-ahead distance of c1ω where c1 =

1+√
1+4ln(1+ϵ)ln(k)
2ln(1+ϵ)
	2
is suﬃcient.
Once the we have computed the ﬁrst stopping point r∗we can process the
schedule up to r∗(ω+1) with a (1+ϵ)-approximation guarantee. We then repeat
this process when the next stopping point is computed and so on. A portion of
the schedule of length c1ω is suﬃcient to guarantee a stopping point is found.
Hence we say that the EPTAS uses a c1ω look-ahead distance.
Theorem 9. For any ϵ > 0, we have an EPTAS of time complexity O(nkωk+2)
provided we have a look-ahead of c1ω, where c1 =

1+√
1+4ln(1+ϵ)ln(k)
2ln(1+ϵ)
	2
.
Extensions. Similar to Sect. 4, we show that with some small modiﬁcations our
EPTAS can be used for the maximum weighted independent set problem. We
assume however that it is known a priori that there exists some global parameter
wmax > 1 which is constant such that for each vertex v in our graph W(v) ≤
wmax.
Ir is deﬁned as the largest weighted independent set in Gr. We deﬁne the
weight of Ir as W(Ir) = 
v∈Ir W(v). Then r∗is deﬁned as the smallest integer
such that W(Ir∗+1) < (1 + ϵ)W(Ir∗). I.e., for any 1 < r ≤r∗, W(Ir) ≥(1 +
ϵ)W(Ir−1) and W(Ir) ≤krwmax. The proof of the following lemma follows from
Lemma 7 by setting k′ = kwmax.
Lemma 10. r∗≤

1+√
1+4ln(1+ϵ)ln(k′)
2ln(1+ϵ)
	2
The proof of the following theorem is left for the full paper.
Theorem 11. Suppose that I′ is a (1 + ϵ)-approximate weighted independent
set in Gr∗+1, then I ≡Ir∗∪I′ is (1 + ϵ)-approximate for G.
6
Conclusions
In this paper we study the WMIS problem on restricted LoS networks where
parameter k is a constant, and propose a polynomial time dynamic programming
algorithm for the problem. We also use the DP algorithm to develop an EPTAS
that applies to a semi-online algorithm with performance ratio (1 + ϵ) and a
look-ahead distance dependent on ϵ, for any ϵ > 0.

222
P. Sangha et al.
For future work there are various avenues to explore. One immediate direction
is to study the LoS network with diﬀerent ranges of various parameters. It is
interesting to determine the complexity of the problem (polynomial time solvable
or NP-hard) given diﬀerent values of the parameters. We can also extend the
problem deﬁnition such that the distance restriction ω may take two diﬀerent
values ω1 and ω2 for each of the two dimensions. Another direction is to study
other optmisation problems on LoS networks. Furthermore, we can explore other
scheduling problems with constraints that can modeled in a similar way as a LoS
network and adapt solutions to these scheduling problems.
References
1. Albers, S.: Online algorithms: a survey. Math. Program. 97(1–2), 3–26 (2003)
2. Bellman, R., Esogbue, A.O., Nabeshima, I.: Mathematical Aspects of Scheduling
and Applications. Elsevier, Amsterdam (2014)
3. Bollob´as, B., Janson, S., Riordan, O.: Line-of-sight percolation. Comb. Probab.
Comput. 18(1–2), 83–106 (2009)
4. Cesati, M., Trevisan, L.: On the eﬃciency of polynomial time approximation
schemes. Inf. Process. Lett. 64(4), 165–171 (1997)
5. Chiu, S.N., Stoyan, D., Kendall, W.S., Mecke, J.: Stochastic Geometry and Its
Applications. Wiley, Hoboken (2013)
6. Czumaj, A., Wang, X.: Communication problems in random line-of-sight ad-hoc
radio networks. In: Hromkoviˇc, J., Kr´aloviˇc, R., Nunkesser, M., Widmayer, P. (eds.)
SAGA 2007. LNCS, vol. 4665, pp. 70–81. Springer, Heidelberg (2007). https://doi.
org/10.1007/978-3-540-74871-7 7
7. Diestel, R.: Graph Theory. Springer, New York (2000)
8. Farczadi, L., Devroye, L.: Connectivity for line-of-sight networks in higher dimen-
sions. Discret. Math. Theor. Comput. Sci. 15 (2013)
9. Frieze, A., Kleinberg, J., Ravi, R., Debany, W.: Line-of-sight networks. Comb.
Probab. Comput. 18(1–2), 145–163 (2009)
10. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness (1979)
11. H˚astad, J.: Clique is hard to approximate within n1−ε. Acta Math. 182, 105–142
(1999)
12. Sangha, P., Zito, M.: Finding large independent sets in line of sight networks.
In: Gaur, D., Narayanaswamy, N.S. (eds.) CALDAM 2017. LNCS, vol. 10156, pp.
332–343. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-53007-9 29
www.ebook3000.com

Braid Chain Radio Communication
Jacek Cicho´n, Miroslaw Kutylowski(B), and Kamil Wolny
Faculty of Fundamental Problems of Technology,
Wroclaw University of Science and Technology, Wroclaw, Poland
{jacek.cichon,miroslaw.kutylowski,kamil.wolny}@pwr.edu.pl
Abstract. We consider data transmission in a wireless multi-hop net-
work, where node failures may occur and it is risky to send over a single
path. As the stations may transmit at the same time, we apply the Cai-
Lu-Wang collision avoidance scheme. We assume that the nodes know
only their neighbors and there is no global coordination. The routing
strategy is to transmits to all nodes that are in some sense closer to the
destination node.
We analyze propagation strategy for the case that the nodes know only
the nodes in their propagation range and there is no global coordination
of the network. Instead, a node transmits to all nodes that appear in
some sense closer to the destination node. We investigate data propaga-
tion speed in such networks, namely the delay due to conﬂict resolution.
In order to understand the phenomena arising there we focus on a fun-
damental case called a braid chain. We prove that for a braid chain of n
layers the normalized delay due to anti-collision mechanism is ≈0.28n·Δ
(while the expected time for a chain of single stations is 0.5n · Δ), where
Δ stands for the time interval length of Cai-Lu-Wang scheme. We also
show that the behavior of the braid chain rapidly converges to its sta-
tionary distribution with the length of the chain. For these results we
develop analytical methods that can be applied for other networks, e.g.
for the case when the forwarding delays are chosen with exponential
distribution.
1
Introduction
Transmitting data over a multi-hop ad hoc radio network creates a number of
problems concerning reliability. In such an environment (e.g. in a sensor ﬁeld)
the nodes might be likely to fail or become controlled by an adversary. So sending
a message via a single path could be risky – a failure of every single node disrupts
the communication channel.
On the other hand, if the data are sent over multiple nodes within the same
transmission range we are risking that the nodes may transmit data at the same
time, create radio channel collisions and interrupt the communication as well.
We may attempt to orchestrate the transmission times in a careful way, however
Supported by National Science Centre, project HARMONIA, decision DEC-
2013/08/M/ST6/00928.
c
⃝Springer International Publishing AG 2017
A. Fern´andez Anta et al. (Eds.): ALGOSENSORS 2017, LNCS 10718, pp. 223–235, 2017.
https://doi.org/10.1007/978-3-319-72751-6_17

224
J. Cicho´n et al.
any detailed conﬁguration by a central coordinator is problematic in case of
ad hoc networks. In principle, the nodes should work autonomously without a
detailed pre-conﬁguration. This is particularly helpful if the network state may
change due to the fact that the nodes join and leave the network at unpredictable
moments.
Communication Model. We assume that each node knows its geographical
coordinates (e.g. from its GPS receiver) as well as its neighbors and their coor-
dinates.
The nodes communicate within their transmission range, however we assume
that if nodes A and B are considered as neighbors, then there is a bidirectional
connection between them (similarly as in the Unit Disk Graph model).
We assume also that the network assigns a separate input channel to each
node (deﬁned via a separate frequency band or time multiplexing). Since we
are talking about a multi-hop network, each channel can be reused. We have
to assure only that the nodes in the same transmission range are assigned to
diﬀerent channels. We assume that the input channels are assigned to the nodes
in a distributed way – e.g. via distributed graph coloring algorithms. The crucial
problem to address here are collisions: if more than one neighbor of a node A
sends a message to A at the same time, then a collision occurs. In this case A
might be unable to receive any of these message.
Cai-Lu-Wang Anti-Collision Mechanism designed and analyzed in [2], is
based on carrier sensing and random delays. A node that intends to send a mes-
sage, chooses t, the starting moment of a transmission, uniformly at random
from the interval [T, T + Δ], where T is the current time and Δ is the proto-
col parameter. At time t, just before starting a transmission, the node checks
whether the shared communication channel is already busy. If the carrier signal
is not detected, it starts its own transmission. Otherwise, it tries to transmit
later. Clearly, the Cai-Lu-Wang mechanism may fail, as there is a slight time
delay δ between the time when a node observes that the channel is free and the
time when it starts the transmission. So if another station decides to monitor the
channel within the time interval [T + t −δ, T + t + δ], then a collision will occur.
Therefore the choice of Δ has to take into account acceptable level of collision
probability on one side and average transmission delay 1
2Δ on the other side.
In order to simplify the notation from now on we assume that Δ = 1. In
other words, we assume that the time is expressed in the number of Δ intervals.
Communication and Routing Strategy. We consider point-to-point com-
munication. If a node A aims to deliver a message to a node B, then we assume
that A knows the coordinates of B and can attach them to the header of the
message. The simplest routing strategy would be to forward the message to a
neighbor that is the closest one to the target destination. If the network is rela-
tively dense and without holes, then this leads to fairly good results. However,
a single node can cut the connection – due to either a fault or misbehavior.
Another strategy would be to forward the message to all neighbors that are
www.ebook3000.com

Braid Chain Radio Communication
225
closer to the destination. This improves resilience to faults, but may ﬂood the
network with a large number of transmissions. So we need a kind of compromise.
An intermediate solution could be to deﬁne a kind of virtual route between
the source A and destination B. Namely, as intermediate nodes we could take all
nodes that are at distance at most D from the line connecting A and B. In this
way we prevent ﬂooding, while on the other hand we should enable multiple paths.
Note that this overall strategy communication strategy is quite simplistic and
lacks any global coordination. This is one of the main arguments why it might
be interesting for ad hoc networks – simplicity is one of the main quality criteria.
Braid Chain Communication. In order to understand the behavior of the
proposed propagation strategy we investigate a special case of such an architec-
ture that we call a braid chain. A braid chain of length n consists of 2n nodes
which are arranged in n layers, with two nodes per layer, say Ui and Di on the
layer i. Each node from layer i can send a message to both nodes of layer i + 1
(see Fig. 1). For receiving, each node uses its private input channel. As two nodes
from layer i send messages to a node of layer i + 1, the Cai-Wang anti-collision
mechanism is used.
Fig. 1. Braid channel architecture
Now let us describe the communication strategy in a more formal way. First,
the source node delivers the message to U1 and D1 at the same time. Let Si
denote a station at layer i - it can be Ui or Di. It executes the following algorithm
based on the Cai-Lu-Wang anti-collision mechanism (random(0, 1) stands for
choosing x ∈[0, 1] uniformly at random):
Braid Line Algorithm
1: start clock
2: loop
3:
if station Si receives a message M at time T then
4:
TU ←T + random(0, 1)
5:
TD ←T + random(0, 1)
6:
while time not later than max{TU, TD} do
7:
if the current time is TU then
8:
if channel for Ui+1 is free then
9:
transmit message M to Ui+1
10:
if the current time is TD then
11:
if channel for Di+1 is free then
12:
transmit message M to Di+1

226
J. Cicho´n et al.
Let ζ denote the probability of collision during the transmission to a single
node and let p stand for the probability of a node to fail. In general, the prob-
ability that the communication in a braid channel is not disconnected turn out
to be higher than (1 −(p2 + ζ2 −(pζ)2))n. If we disregard the probabilities of
a failure due to collision on communication lines, then a braid chain communi-
cation channel is not disconnected with the probability (1 −p2)n. On the other
hand, if we have two independent transmission lines, then the communication
does not fail with the probability
1 −(1 −(1 −p)n)2 = (1 −p)n(2 −(1 −p)n).
Note that for p = .1 and n = 100, the braid chain communication channel is
not disconnected with probability ≈1
e, while in the second case it happens with
probability ≈1
e10 . Even if we increase the number of independent lines to 4, it
will not help much as the probability of successful delivery would be ≈4
e10 as it
equals
1 −(1 −(1 −p)n)4 = (1 −p)n
2 −(1 −p)n
−2(1 −p)n + (1 −p)2n + 2

.
The diﬀerence is still striking, if we assume that the failure probability in a
braid chain increases to, say, 2p due to Cai-Lu-Wang anti-collision mechanism
failure. Then the success probability of the braid chain decreases to (1 −(2p)2)n
which is approximately
1
e4 for our example. Let us note that in both cases there
might be collisions which are due to other transmissions. If we increase p to take
them into account, then the advantage of the braid channel is even higher.
We may also consider the same architecture as for the braid chain, but for
a single radio channel. Then instead of probability p of a failure of receiving by
a node due to transmission collision, we have to consider the failure probability
p/2 as we may double the length of the interval for the Cai-Lu-Wang method.
Then the probability of a successful delivery now becomes ≈(1 −p2)n · (1 −p
2)n
and it is signiﬁcantly lower than (1−(2p)2)n. E.g. for n = 100, p = .1 and a single
radio channel we get (1 −p2)n · (1 −p
2)n ≈1
e · 1
e5 =
1
e6 while (1 −(2p)2)n ≈
1
e4 .
Propagation Delay Problem. Since on each layer the nodes have to apply
a random delay, the Cai-Lu-Wang anti-collision mechanism inﬂuences the data
propagation speed over the braid chain. Our main goal is to investigate this
speed, i.e. to learn what is the extra time necessary for a message sent from the
source node to arrive at the sink node when transferred via a braid chain of the
length n. By extra time we mean the time resulting from application of Cai-Lu-
Wang anti-collision mechanism. Certainly, at each level this delay is between 0
and Δ (recall that we normalize time scale to get Δ = 1), so the total delay
during each successful execution is between 0 and n.
Unfortunately, despite simplicity of the protocol the underlying stochastic
process turns out to be fairly diﬃcult to analyze.
www.ebook3000.com

Braid Chain Radio Communication
227
Related Work. Routing strategies and propagation of messages in multi-hop
radio networks is a challenging topic. There are many papers dealing with routing
strategies that have to be both reliable and eﬃcient. Several message complexity
problems for many diﬀerent models connected with radio communication were
presented and analyzed in [4]. Related problems of extreme propagation were
also raised in [1]. Paper [9] presents a randomized gossiping protocol for infor-
mation spreading. In [5] authors analyze the fault tolerant data propagation
scheme where retransmitions are being performed when acknowledgment is not
received. A somewhat similar channel architecture to the braid chain concept,
but from a perspective of percolation problem, was investigated in [3]. In this
paper several results for the quality of the transmission in such braid chain model
were obtained with the presence of random failures.
Although many of these papers contain deep analytic results on algorithmic
features of the considered protocols - most conclusions and details are related
to common scenarios or “single step” cases. This is due to the fact that a time
analysis for a process running on a multi-hop network usually requires deep
understanding and eﬀective mathematical tools for analyzing complex stochastic
processes. Such tools are frequently unavailable.
Results Overview. The key problem to understand the propagation delay of
braid channel communication is to understand the distribution of the time dif-
ference between the times when the nodes of a given layer learn the transmitted
message – so called layer delay. In Sect. 2 we derive moments of this distribution
and show that these distributions converge rapidly to a stationary distribution.
In Sect. 3 we analyze diﬀerence between times when a message arrives at lay-
ers i and i+1 based on the assumption that the layer delay at layer i is distributed
according to the stationary distribution. Thereby we obtain the expected time
for message delivery over a braid chain with a given number of layers.
Our main theoretical contribution is the way to extract constants by itera-
tive integration (Sect. 3) and application of rapid mixing (Sect. 2). Apart from
the analytic results on the braid chain communication, the proof techniques,
especially from Sect. 3, might be useful elsewhere due to their relative simplicity
and eﬀectiveness (see, e.g. Sect. 4). They apply immediately for diﬀerent network
architectures, the ﬂooding algorithms, etc.
2
Layer Delays
Let us consider a transmission of a message M over the braid channel.
Deﬁnition 1. Let rUi and rDi denote the times when Ui, and respectively Di
receive a message M for the ﬁrst time. Then by the delay at layer i we mean
di = |rUi −rDi|.
Note that di ≤1. Indeed, let us consider the layer i −1 and assume that
V ∈{Ui−1, Di−1} is the ﬁrst node at layer i −1 that receives M. Then M
cannot be delivered to the ﬁrst node of layer i before time r(V ), while on the
other hand V transmits M to both Ui and Di before time r(V ) + 1.

228
J. Cicho´n et al.
Let us observe that the value of di inﬂuences the time needed to transmit
M from the layer i to the layer i + 1. Indeed, initially for time di only one node
of layer i is ready for forwarding the message M to the layer i + 1. After this
initial time period di, the second node at layer i becomes ready for sending and
the probability of starting a transmission to a node of layer i + 1 increases. So
we see that small values of di increase the propagation speed.
If di = 1, then the messages arriving at layer i + 1 originate from the same
node. The time to deliver the message from layer i to layer i+1 is then described
by the minimum of two random variables distributed uniformly at random over
the interval [0, 1], with the expected time 1
3. As this case is the most pessimistic
one, we get a rough estimation of the propagation speed. Below we will get a
precise result.
2.1
Modelling the Layer Delays
In this section we present some theoretical points, strictly connected with the
further analysis of the layer delays. We consider the following model. Let x, y, u, v
be independent random variables with the uniform distribution on the interval
[0, 1]. Let d ∈[0, 1]. For a given choice of number d we consider random variables
Xd, Yd and Zd deﬁned as follows:
Xd = min(x, d + y),
Yd = min(u, d + v),
Zd = |Xd −Yd|.
Lemma 1. Xd and Yd are independent random variables with the same distri-
bution with density function fd given by the following formula:
fd(z) =

1
if z ∈[0, d],
2 + d −2z
if z ∈[d, 1].
Proof. Notice that the condition min(x, d+y) ⩾z is equivalent to (x ⩾z)∧(y ⩾
z −d). Since x and y are independent, Pr(X > z) = (1 −z)(1 −max(0, z −d)).
So:
Pr(X < z) =
z
if z ∈[0, d],
(2 + d)z −d −z2
if z ∈[d, 1].
By diﬀerentiating this cumulative distribution function we get the formula for
fd.
⊓⊔
Let us consider the random variable Ud = Xd −Yd. The density of Ud can be
computed as the convolution of fd and fd. That is,
fU,d(x) =

fd(u)fd(u −x) du.
As Xd, Yd are independent and have the same probability distribution, fU,d is
symmetric. Therefore, the density function kd of the random variable Zd equals
kd(x) =

2fU,d(x)
if x ∈[0, 1],
0
otherwise.
www.ebook3000.com

Braid Chain Radio Communication
229
For x ⩾0 after some tedious calculations we get the following values for kd(x):
2
3(4 −3d + 3d2 −d3 −3x + 3dx −3d2x −3x2 + 2x3)
for d ∈(0, 1
2] ∧x ∈[0, d]
2
3

−d3 −3d2x + 3d2 + 2x3 −6x + 4

for d ∈(0, 1
2) ∧x ∈[d, 1 −d]
2 + 2d −4x −2dx + 2x2
for d ∈(0, 1
2) ∧x ∈[1 −d, 1]
2
3(2 −3x + x3)
for d = 0
2
3(4 −3d + 3d2 −d3 −3x + 3dx −3d2x −3x2 + 2x3) for d(∈1
2, 1) ∧x ∈[0, 1 −d]
2(1 −x)
for d ∈( 1
2, 1) ∧x ∈[1 −d, d]
2 + 2d −4x −2dx + 2x2
for d ∈( 1
2, 1) ∧x ∈[d, 1]
0
otherwise
Let Z be a random variable with density function kd. We will need exact
expressions for the moments of Z. The following lemma provides us a compact
answer.
Lemma 2. Let Z be a random variable with the density function kd(z). Then
the expected value for Zn for n∈N equals:
E [Zn] =
2
(n + 1)(n + 2)

d + d(1−d)3+n + d2+n(d−1)+
2 + 2(1−d)3+n(1−2d) −2d3+n
n+3
−4(1−d)4+n
(n+3)(n+4)

Proof (Sketch). In order to prove Lemma 2 we have to use the tools of analytical
combinatorics. A reader may refer to [7] for details.
Let MZ(t) denote the moment-generating function of the distribution with
the density function kd(z), that is,
MZ(t) =
 +∞
−∞eztkd(z) dz.
Then
MZ(t) = ∞
n=0
tn
n! E [Zn] .
Using the properties of the moment-generating function, we only have to retrieve
the coeﬃcient [tn]MZ(t). Then E[Zn] = n![tn]MZ(t). After tedious computa-
tions, using the formula for kd, we get the formula stated in Lemma 2.1
⊓⊔
By Lemma 2 and linearity of expected value we get in particular:
E [Z] = 1
15(d5−5d3+5d2+4)
E

Z5−5Z3+5Z2	
=
1
378(10d9−63d8+153d7−252d6+
378d5−315d4−21d3+144d2+110)
1 A reader interested in deriving them is advised to use a symbolic computations
program.

230
J. Cicho´n et al.
2.2
Stabilization of Probability Distribution of Layer Delays
In the previous subsection we have analyzed layer delay di+1 given a speciﬁc
value d for di. Our goal now is to check how the probability distribution of di
evolves with i. Note that d1 = 0, since the source node transmits the message
to U1 and D1 at the same time. Indeed, there is no need for the Cai-Lu-Wang
anti-collision mechanism, as there is no other node in the starting layer sending
to U1 and D1. For di with i > 1 the situation gets complicated.
We consider a stochastic process (di)i=0,1,.... As probability distribution of
di+1 depends only on the value of di, it is a Markov chain. The function k(d, x) =
kd(x) computed in Sect. 2.1 is the stochastic density kernel of this chain, that is:
Pr[di+1 ∈A|di = d] =

A
kd(x) dx.
Stationary Distribution. First observe that kd(x) > 0 for each d ∈[0, 1]
and x ∈[0, 1]. Since every strictly positive kernel on a bounded closed interval
is ergodic (see [6]) we deduce that the Markov Chain (di)i=0,1,... is ergodic.
Therefore we conclude that.
Lemma 3. There exists a probability distribution μ on the interval [0, 1] such
that for any probability distribution π, the sequence of iterates (πn) deﬁned by
π1 = π and πn+1(x) =
 1
0 πn(t)k(t, x) dt converges uniformly to μ.
Thereby the distribution μ from Lemma 3 is the stationary probability distribu-
tion for the Markov chain (di)i=0,1,....
Theorem 1. E [μ] = 0.286067 + ϵ, where 0 ⩽ϵ ⩽0.006,
Var[μ] = 0.126981 + δ −(0.286067 + ϵ)2, where 0 ⩽δ ⩽0.0005.
Proof. For the stationary distribution μ, after making a single transition we get
again the stationary distribution. That is, μ(x) =
 1
0 μ(t)k(t, x) dt. Note that
k(t, x) is a piecewise polynomial function. Therefore we are able to make the
following computations:
E[μ] =
 1
0 xμ(x) dx =
 1
0 x
  1
0 μ(t)k(t, x) dt

dx
=
 1
0 μ(t)
  1
0 xk(t, x) dx

dt
=
1
15
 1
0 (t5 −5t3 + 5t2 + 4)μ(t) dt
=
4
15 + 1
15
 1
0 (t5 −5t3 + 5t2)μ(t) dt
(1)
=
4
15 + 1
15
 1
0 (t5 −5t3 + 5t2)
  1
0 μ(s)k(s, t) ds

dt
(2)
=
4
15 + 1
15
 1
0 μ(s)
  1
0 (t5 −5t3 + 5t2)k(s, t) dt

ds
=
4
15 + 1
15
110
378 + 1
15
1
378
 1
0 μ(s)w(s) ds
(3)
where w(s) = 10s9 −63s8 + 153s7 −252s6 + 378s5 −315s4 −21s3 + 144s2. It is
easy to check that 0 ⩽w(s) ⩽34 for s ∈[0, 1]. Therefore
www.ebook3000.com

Braid Chain Radio Communication
231
E[μ] = 4
15 + 1
15
110
378 + ϵ,
(4)
where 0 ⩽ϵ ⩽0.00599647. So ﬁnally, E[μ] ≈0.28.
Remark 1. The main trick here was transition from (1) to (2) based on the fact
that μ is the stationary distribution. As we can compute the integrals of the
form
 1
0 tik(s, t) dt for a ﬁxed i, in line (3) we get back an integral of the form
 1
0 μ(s)w(s) ds, where w(s) is a polynomial, with some constant in front of the
integral. Note that if we need a better precision, we can repeat the same trick to
(3). So by repeating the same trick again and again, we can get an estimation
of E[μ] with any required precision via a simple numeric computation.
One can notice that method above may also be used for the computations of
variance. For this purpose we use Var[μ] = E[μ2] −(E[μ])2, the above result on
E[μ] and perform analogous calculations for E[μ2]. Instead of using an explicit
formula for
 1
0 xk(t, x) dx we make calculations for
 1
0 x2k(t, x) dx.
Rapid Mixing. Now we aim to show that the Markov chain (di)i=0,1,... rapidly
approaches the stationary distribution μ. We show the following result:
Theorem 2. For t > (−log ε+1)
2−log 3
the variation distance between the distributions
μ and dt is at most ε. That is 1
2
 1
0 |μ(x) −dt(x)|dx < ε.
Proof. Let us recall the Coupling Lemma - a powerful and easy tool for showing
convergence of Markov chains [8]. In order to estimate the convergence of a given
process (di)i=0,1,... to its stationary distribution we create two coupled processes,
say (ci, c′
i)i=0,1,... so that:
– the process (ci)i=0,1,... (respectively, (c′
i)i=0,1,...) treated alone has the same
transition probabilities as (di)i=0,1,...,
– there are dependencies between the choices of the processes ci and c′
i; due
to these dependencies the processes ci and c′
i should converge and reach the
same state.
The Coupling Lemma says that if Pr(ct ̸= c′
t) ≤p, then the variation distance
between the distribution of ct and the stationary distribution of this process is
at most 2p. The Coupling Lemma is usually formulated and applied for discrete
state Markov chains. However, we shall use it for non-discrete Markov chains
(where normally it is impossible to construct a good coupling).
As in Sect. 2.1, in order to describe a step of the chain (di)i=0,1,..., we consider
4 random variables: x, y, u, v, where x, y denote delays chosen for transmitting
to Ui+1 and u, v the delays for transmitting to Di+1, while x, u denote the delays
chosen by the station from layer i that is ﬁrst ready for starting a transmission.
Let us consider two copies of the Markov process (di)i=0,1,...,. Let the ﬁrst
process be in state d, the second one in state d′. Deﬁning a coupling for these
processes is surprisingly easy: for both process choose exactly the same values
for x, y, u, v.

232
J. Cicho´n et al.
Lemma 4. With probability at least 1
4 after a coupling step both processes reach
the same state.
Proof. W.l.o.g. assume that d ≤d′. Let us consider the ﬁrst process. With
probability at least 0.5 we have x = min(x, y + d). Similarly, u = min(u, v + d)
with probability at least 0.5. Since x, y are stochastically independent from u, v,
with probability at least 1
4 the layer delay for the ﬁrst process after performing
the step is |x −u|.
For the second process note that if x = min(x, y + d) and u = min(u, v + d),
then x = min(x, y + d′) and u = min(u, v + d′) as well, as d ≤d′. It follows that
in this case the second process reaches the layer delay |x −u| as well.
⊓⊔
By Lemma 4, after T steps the probability that the processes still diﬀer is at
most ( 3
4)T . Hence, by Coupling Lemma, for t > (−log ε+1)
2−log 3
the variation distance
between the distribution of dt and the stationary distribution μ is at most
2 · (3
4)t ≤2 · 3
4
(−log ε+1)
2−log 3
= 2 · (2log 3/4)
(−log ε+1)
log(4/3)
= 2 · 2log ε−1 = ε
and Theorem 2 follows.
⊓⊔
For comparison, let ki denote the density of probability distribution of di. We
have not derived a closed expression for ki, however it is possible to estimate
numerically the total variation distance between ki and ki+1 separately for each
single i. For instance, we get
1
2
 1
0
|k3(x) −k2(x)| dx < 0.001
1
2
 1
0
|k4(x) −k3(x)| dx < 0.0001
1
2
 1
0
|k5(x) −k4(x)| dx < 0.00001
This conﬁrms the general observation from Theorem 2.
3
Propagation Time
Our previous calculations were limited to the delays within a layer. Now, given
probability distribution of these delays we aim to ﬁnd the expected time of
message delivery from layer i to layer i + 1.
Propagation Time for a Fixed Delay. For a moment let us ﬁx the value
of a layer delay to d. Let ˆTd denote the time that elapses between the moment
when the ﬁrst node at the current layer gets the message and the time when the
ﬁrst node of the next layer gets the message. According to the notation used in
Sect. 2, ˆTd = min{Xd, Yd}. Hence,
ˆTd = min

min{x, d + y}, min{u, d + v}

.
www.ebook3000.com

Braid Chain Radio Communication
233
In order to compute cumulative density function of the random variable ˆTd we
use the similar argument as in Sect. 2. Notice that the condition ˆTd ⩾z is
equivalent to (x ⩾z) ∧(y ⩾z −d) ∧(u ⩾z) ∧(v ⩾z −d). Since x, y, u, v are
stochastically independent and uniformly distributed over [0, 1], we thereby get
Pr(Z > z) = (1 −z)2(1 −max(0, z −d))2
Hence,
Pr(Z < z) =
1 −(1 −z)2
if z ∈[0, d],
1 −(1 −z)2
1 −(z −d)
2
if z ∈[d, 1].
By diﬀerentiating this cumulative density function we get the following formula
for the density function f ˆ
Td:
f ˆ
Td(z) =

2 −2z
if z ∈[0, d],
D(d, z)
if z ∈[d, 1],
where D(d, z) = 4 + 6d + 2d2 −12z −12dz −2d2z + 12z2 + 6dz2 −4z3. Let d be
a ﬁxed delay. We aim to compute the expected time
E[ ˆTd] =
 1
0 zf ˆ
Td(z) dz
After some straightforward operations we get E[ ˆTd] = W(d), where W(d) is the
following polynomial:
W(d) =
1
30

6 + 15d −20d2 + 10d3 −d5
(5)
Propagation Time for Stationary Distribution. As before, let μ(d) denote
the stationary distribution for the layer delay. If the initial layer delay occurs
according to the distribution μ(d), then the expected propagation time ˆT for
one layer can be computed as follows:
E[ ˆT] =
 1
0 W(z)µ(z) dz =
 1
0 W(z)
  1
0 µ(t)k(t, z) dt

dz =
 1
0 µ(t)
  1
0 W(z)k(t, z) dz

dt
=
 1
0
µ(t)
  1
0
1
30

6 + 15z −20z2 + 10z3 −z5
k(t, z) dz

dt
By Lemma 2 we obtain
E[ ˆT] =
787
2835 +
 1
0 μ(t)
  1
0 R(z)k(t, z) dz

dt
=
787
2835 +
162949
48648600 +
 1
0 μ(t)
  1
0 S(z)k(t, z) dz

dt
=
787
2835 +
162949
48648600 +
315286
1206079875 + ϵ ≈0.28
where ϵ ⩽0.00086 and R(z), S(z) are polynomials given by the formulas
R(z) = 11z2
126 −26z3
135 + 7z4
36 −z5
10 + z6
45 −2z7
315 + z8
180 −
z9
1134
S(z) = 263z2
62370 −
z3
1215 −11z4
756 + 529z5
18900 −233z6
8100 + 94z7
4725 −
3z8
280 + 173z9
34020 −19z10
11340 +
29z11
155925 +
z12
28350 −
z13
90090

234
J. Cicho´n et al.
The integral
 1
0
1
30

6 + 15z −20z2 + 10z3 −z5
k(t, z) dz is computed separately
on two intervals, where k(t, z) is a polynomial. So we compute two integrals of
polynomial functions. The result is an expression with the values of 2 polynomials
at respectively, 2 points for each polynomial. In each case the result is a constant
plus a polynomial on d without a free term. As the result, say R, is used for
computing the integral
 1
0 μ(t)R(t) dt, the constant term can be placed in front
of the integral. After that we perform the same procedure again and again. We
retrieve the constants step by step to achieve a better precision and at each step
we obtain an integral of the polynomial with a higher degree, so we only have
to compute more and more moments, to achieve a better approximation. Using
three iterations of that method we conclude that E

ˆT

= 0.281212 + ϵ, where
|ϵ| < 10−3. Therefore, by linearity of expectation
limn→∞
E [Tn]
n
= E

ˆT

≈0.28
(6)
The method presented above may also be used for the computations of a pre-
cise approximation of the variance of Tn. We have shown earlier the convergence
to stationary distribution μ(x) of the Markov Chain. If we assume that delays
occur with μ(x), then the random variables for the time execution between two
frames can be considered as independent. Having the expected value for the time
of delivery in a single layer, what we need to obtain is the second moment of the
random variable denoting the time delivery with an assumption that the delay
d is ﬁxed. Finally, we conclude that
limn→∞
Var[TN]
n
≈0.035
(7)
4
Final Remarks
The results obtained in this paper can be generalized for other propagation
models. For instance, we may modify the braid chain protocol in the way that
the delay to send a message to the next node in the chain is chosen according
to the exponential distribution (which is a standard model in many areas of
telecommunication). Using the proof techniques presented above we can show
that there is |ϵ| ≤0.006
λ2
such that
limn→∞
E [Tn]
n
= 313
800 · 1
λ −ϵ,
where E [Tn] is the expected value of time needed to pass through n levels.
The key technical point of our analysis is the technique used in Sect. 3. The
approach based on polynomials and iterative integrating leads to surprisingly
precise results in a conceptually easy way. We feel that this approach can be
used in many diﬀerent occasions, substantially simplifying the analysis.
www.ebook3000.com

Braid Chain Radio Communication
235
References
1. Baquero, C., Almeida, P.S., Menezes, R.: Fast estimation of aggregates in unstruc-
tured networks. In: ICAS, pp. 88–93. IEEE Computer Society (2009)
2. Cai, Z., Lu, M., Wang, X.: Distributed initialization algorithms for single-hop ad
hoc networks with minislotted carrier sensing. IEEE Trans. Parallel Distrib. Syst.
14(5), 516–528 (2003)
3. Cicho´n, J., Klonowski, M.: On ﬂooding in the presence of random faults. Fundam.
Inform. 123(3), 273–287 (2013)
4. Cicho´n, J., Lemiesz, J., Zawada, M.: On message complexity of extrema propagation
techniques. In: Li, X.-Y., Papavassiliou, S., Ruehrup, S. (eds.) ADHOC-NOW 2012.
LNCS, vol. 7363, pp. 1–13. Springer, Heidelberg (2012). https://doi.org/10.1007/
978-3-642-31638-8 1
5. Cicho´n, J., Gebala, M., Zawada, M.: Fault tolerant protocol for data collecting
in wireless sensor networks. In: The 22nd IEEE Symposium on Computers and
Communications, July 2017
6. Feller, W.: An Introduction to Probability Theory and Its Applications, vol. 2, 2nd
edn. Wiley, New York (1971)
7. Flajolet, P., Sedgewick, R.: Analytic Combinatorics, 1st edn. Cambridge University
Press, New York (2009)
8. Lindvall, T.: Lectures on the Coupling Method. A Wiley-Interscience Publication,
New York (1992)
9. Mosk-Aoyama, D., Shah, D.: Computing separable functions via gossip. In: ACM
PODC 2006, pp. 113–122. ACM (2006)

Author Index
Bampas, Evangelos
1
Beauquier, Joffroy
13
Bentert, Matthias
26
Burman, Janna
13
Censor-Hillel, Keren
41
Cherry, Andrew
57
Chuangpishit, Huda
70, 84
Cichoń, Jacek
223
Czyzowicz, Jerzy
98
Czyzowicz, Jurek
70, 114
Das, Shantanu
1
Daymude, Joshua J.
127
Dereniowski, Dariusz
1
Diks, Krzysztof
98
Dobrev, Stefan
114
Erzin, Adil
141
Fischer, Matthias
168
Georgiou, Kostantinos
84
Gmyr, Robert
127
Godon, Maxime
114
Gudmundsson, Joachim
57
Hideg, Attila
155
Jung, Daniel
168
Karousatou, Christina
1
Kranakis, Evangelos
70, 84, 114
Krizanc, Danny
70
Kutten, Shay
13
Kutyłowski, Mirosław
223
Levy, Rina
41
Li, Shouwei
182
Lukovszki, Tamás
155
Markarian, Christine
182
Mestre, Julián
57
Meyer auf der Heide, Friedhelm
168, 182
Moussi, Jean
98
Nichterlein, André
26
Niedermeier, Rolf
26
Nowak, Thomas
13
Plotnikov, Roman
141
Podlipyan, Pavel
182
Poss, Menachem
198
Rawitz, Dror
198
Richa, Andréa W.
127
Rytter, Wojciech
98
Sakai, Toshinori
114
Sangha, Pavan
211
Scheideler, Christian
127
Shachnai, Hadas
41
Strothmann, Thim
127
Urrutia, Jorge
114
van Bevern, René
26
Wolny, Kamil
223
Wong, Prudence W. H.
211
Xu, Chuan
13
Zito, Michele
211
www.ebook3000.com


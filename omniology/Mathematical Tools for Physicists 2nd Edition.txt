

Mathematical Tools for Physicists
Edited by
Michael Grinfeld

Related Titles
Goodson, D.Z.
Mathematical Methods for
Physical and Analytical
Chemistry
2011
ISBN: 978-0-470-47354-2
Also available in digital formats
Trigg, G.L. (ed.)
Encyclopedia of Applied Physics
The Classic Softcover Edition
2004
ISBN: 978-3-527-40478-0
Gl√§ser, M., Kochsiek, M. (eds.)
Handbook of Metrology
2010
ISBN: 978-3-527-40666-1
Stock, R. (ed.)
Encyclopedia of Applied High
Energy and Particle Physics
2009
ISBN: 978-3-527-40691-3
Bohr, H.G. (ed.)
Handbook of Molecular
Biophysics
Methods and Applications
2009
ISBN: 978-3-527-40702-6
Andrews, D.L. (ed.)
Encyclopedia of Applied
Spectroscopy
2009
ISBN: 978-3-527-40773-6
Masujima, M.
Applied Mathematical
Methods in Theoretical
Physics
2nd Edition
2009
ISBN: 978-3-527-40936-5
Also available in digital formats
Willatzen, M., Lew Yan Voon, L.C.
Separable Boundary-Value
Problems in Physics
2011
ISBN: 978-3-527-41020-0
Also available in digital formats

Mathematical Tools for Physicists
Edited by
Michael Grinfeld
Second Edition

The Editor
Dr. Michael Grinfeld
University of Strathclyde
Mathematics and Statistics
Glasgow
United Kingdom
m.grinfeld@strath.ac.uk
Cover
‚ÄúSkeleton and Pore Partition‚Äù, x-ray computed
tomography image of a limestone core from
Mt Gambier in South Australia. Courtesy of
Olaf Delgado-Friedrichs
All books published by Wiley-VCH are
carefully produced. Nevertheless, authors,
editors, and publisher do not warrant the
information contained in these books,
including this book, to be free of errors.
Readers are advised to keep in mind that
statements, data, illustrations, procedural
details or other items may inadvertently be
inaccurate.
Library of Congress Card No.: applied for
British Library Cataloguing-in-Publication
Data
A catalogue record for this book is available
from the British Library.
Bibliographic information published by the
Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek
lists this publication in the Deutsche
NationalbibliograÔ¨Åe; detailed bibliographic
data are available on the Internet at
<http://dnb.d-nb.de>.
¬© 2015 Wiley-VCH Verlag GmbH & Co.
KGaA, Boschstr. 12, 69469 Weinheim,
Germany
All rights reserved (including those of
translation into other languages). No part of
this book may be reproduced in any
form ‚Äì by photoprinting, microÔ¨Ålm, or any
other means ‚Äì nor transmitted or translated
into a machine language without written
permission from the publishers. Registered
names, trademarks, etc. used in this book,
even when not speciÔ¨Åcally marked as such,
are not to be considered unprotected by law.
Print ISBN: 978-3-527-41188-7
ePDF ISBN: 978-3-527-68426-7
ePub ISBN: 978-3-527-68427-4
Mobi ISBN: 978-3-527-68425-0
Cover-Design GraÔ¨Åk-Design Schulz,
Fu√üg√∂nheim, Germany
Typesetting Laserwords Private Limited,
Chennai, India
Printing and Binding Markono Print Media
Pte Ltd, Singapore
Printed on acid-free paper

V
Contents
List of Contributors
XXI
Preface
XXV
Part I: Probability
1
1
Stochastic Processes
3
James R. Cruise, Ostap O. Hryniv, and Andrew R. Wade
1.1
Introduction
3
1.2
Generating Functions and Integral Transforms
4
1.2.1
Generating Functions
4
1.2.2
Example: Branching Processes
7
1.2.3
Other Transforms
9
1.2.3.1
Moment Generating Functions
9
1.2.3.2
Laplace Transforms
10
1.2.3.3
Characteristic Functions
10
1.3
Markov Chains in Discrete Time
10
1.3.1
What is a Markov Chain?
10
1.3.2
Some Examples
11
1.3.3
Stationary Distribution
12
1.3.4
The Strong Markov Property
12
1.3.5
The One-Step Method
13
1.3.6
Further Computational Methods
14
1.3.7
Long-term Behavior; Irreducibility; Periodicity
15
1.3.8
Recurrence and Transience
16
1.3.9
Remarks on General State Spaces
17
1.3.10
Example: Bak‚ÄìSneppen and Related Models
17
1.4
Random Walks
18
1.4.1
Simple Symmetric Random Walk
18
1.4.2
P√≥lya‚Äôs Recurrence Theorem
19
1.4.3
One-dimensional Case; ReÔ¨Çection Principle
19
1.4.4
Large Deviations and Maxima of Random Walks
21
1.5
Markov Chains in Continuous Time
22

VI
Contents
1.5.1
Markov Property, Transition Function, and Chapman‚ÄìKolmogorov
Relation
22
1.5.2
InÔ¨Ånitesimal Rates and Q-matrices
23
1.5.3
Kolmogorov DiÔ¨Äerential Equations
24
1.5.4
Exponential Holding-Time Construction; ‚ÄúGillespie‚Äôs Algorithm‚Äù
25
1.5.5
Resolvent Computations
26
1.5.6
Example: A Model of Deposition, DiÔ¨Äusion, and Adsorption
27
1.5.6.1
N = 1
28
1.5.6.2
N = 3
28
1.6
Gibbs and Markov Random Fields
29
1.6.1
Gibbs Random Field
29
1.6.2
Markov Random Field
30
1.6.3
Connection Between Gibbs and Markov Random Fields
31
1.6.4
Simulation Using Markov Chain Monte Carlo
31
1.7
Percolation
31
1.8
Further Reading
33
1.A
Appendix: Some Results from Probability Theory
33
1.A.1
Set Theory Notation
33
1.A.2
Probability Spaces
34
1.A.3
Conditional Probability and Independence of Events
35
1.A.4
Random Variables and Expectation
35
1.A.5
Conditional Expectation
36
References
37
2
Monte-Carlo Methods
39
Kurt Binder
2.1
Introduction and Overview
39
2.2
Random-Number Generation
40
2.2.1
General Introduction
40
2.2.2
Properties That a Random-Number Generator (RNG) Should Have
40
2.2.3
Comments about a Few Frequently Used Generators
41
2.3
Simple Sampling of Probability Distributions Using Random Numbers
42
2.3.1
Numerical Estimation of Known Probability Distributions
42
2.3.2
‚ÄúImportance Sampling‚Äù versus ‚ÄúSimple Sampling‚Äù
42
2.3.3
Monte-Carlo as a Method of Integration
43
2.3.4
InÔ¨Ånite Integration Space
43
2.3.5
Random Selection of Lattice Sites
44
2.3.6
The Self-Avoiding Walk Problem
44
2.3.7
Simple Sampling versus Biased Sampling: the Example of SAWs
Continued
45
2.4
Survey of Applications to Simulation of Transport Processes
46
2.4.1
The ‚ÄúShielding Problem‚Äù
46
2.4.2
DiÔ¨Äusion-Limited Aggregation (DLA)
46
2.5
Monte-Carlo Methods in Statistical Thermodynamics: Importance
Sampling
47

Contents
VII
2.5.1
The General Idea of the Metropolis Importance-Sampling Method
47
2.5.2
Comments on the Formulation of a Monte-Carlo Algorithm
48
2.5.3
The Dynamic Interpretation of the Monte-Carlo Method
50
2.5.4
Monte-Carlo Study of the Dynamics of Fluctuations Near Equilibrium and of
the Approach toward Equilibrium
51
2.5.5
The Choice of Statistical Ensembles
52
2.6
Accuracy Problems: Finite-Size Problems, Dynamic Correlation of Errors,
Boundary Conditions
52
2.6.1
Finite-Size‚ÄìInduced Rounding and Shifting of Phase Transitions
52
2.6.2
DiÔ¨Äerent Boundary Conditions: Simulation of Surfaces and Interfaces
54
2.6.3
Estimation of Statistical Errors
55
2.7
Sampling of Free Energies and Free Energy Barriers
56
2.7.1
Bulk Free Energies
56
2.7.2
Interfacial Free Energies
57
2.7.3
Transition Path Sampling
57
2.8
Quantum Monte-Carlo Techniques
57
2.8.1
General Remarks
57
2.8.2
Path-Integral Monte-Carlo Methods
58
2.8.3
A Classical Application: the Momentum Distribution of Fluid 4He
59
2.8.4
A Few Qualitative Comments on Fermion Problems
59
2.9
Lattice Gauge Theory
61
2.9.1
Some Basic Ideas of Lattice Gauge Theory
61
2.9.2
A Famous Application
62
2.10
Selected Applications in Classical Statistical Mechanics of Condensed
Matter
62
2.10.1
Metallurgy and Materials Science
63
2.10.2
Polymer Science
63
2.10.3
Surface Physics
67
2.11
Concluding Remarks
67
Glossary
68
References
69
Further Reading
71
3
Stochastic DiÔ¨Äerential Equations
73
Gabriel J. Lord
3.1
Introduction
73
3.2
Brownian Motion / Wiener Process
75
3.2.1
White and Colored Noise
77
3.2.2
Approximation of a Brownian Motion
80
3.3
Stochastic Integrals
80
3.3.1
ItÃÇo Integral
82
3.4
ItÃÇo SDEs
84
3.4.1
ItÃÇo Formula and Exact Solutions
86
3.5
Stratonovich Integral and SDEs
90
3.6
SDEs and Numerical Methods
92

VIII
Contents
3.6.1
Numerical Approximation of ItÃÇo SDEs
93
3.6.2
Numerical Approximation of Stratonovich SDEs
95
3.6.3
Multilevel Monte Carlo
97
3.7
SDEs and PDEs
98
3.7.1
Fokker‚ÄìPlanck Equation
99
3.7.2
Backward Fokker‚ÄìPlanck Equation
102
3.7.2.1
Sketch of Derivation.
102
3.7.2.2
Boundary Conditions
103
3.7.3
Filtering
103
Further Reading
104
Glossary
104
References
105
Part II: Discrete Mathematics, Geometry, Topology
109
4
Graph and Network Theory
111
Ernesto Estrada
4.1
Introduction
111
4.2
The Language of Graphs and Networks
112
4.2.1
Graph Operators
113
4.2.2
General Graph Concepts
114
4.2.3
Types of Graphs
115
4.3
Graphs in Condensed Matter Physics
115
4.3.1
Tight-Binding Models
115
4.3.1.1
Nullity and Zero-Energy States
117
4.3.2
Hubbard Model
118
4.4
Graphs in Statistical Physics
120
4.5
Feynman Graphs
124
4.5.1
Symanzik Polynomials and Spanning Trees
125
4.5.2
Symanzik Polynomials and the Laplacian Matrix
126
4.5.3
Symanzik Polynomials and Edge Deletion/Contraction
128
4.6
Graphs and Electrical Networks
129
4.7
Graphs and Vibrations
130
4.7.1
Graph Vibrational Hamiltonians
131
4.7.2
Network of Classical Oscillators
131
4.7.3
Network of Quantum Oscillators
133
4.8
Random Graphs
134
4.9
Introducing Complex Networks
137
4.10
Small-World Networks
138
4.11
Degree Distributions
139
4.11.1
‚ÄúScale-Free‚Äù Networks
141
4.12
Network Motifs
142
4.13
Centrality Measures
143
4.14
Statistical Mechanics of Networks
146
4.14.1
Communicability in Networks
147

Contents
IX
4.15
Communities in Networks
148
4.16
Dynamical Processes on Networks
150
4.16.1
Consensus
150
4.16.2
Synchronization in Networks
151
4.16.3
Epidemics on Networks
153
Glossary
154
References
155
Further Reading
157
5
Group Theory
159
Robert Gilmore
5.1
Introduction
159
5.2
Precursors to Group Theory
160
5.2.1
Classical Geometry
161
5.2.2
Dimensional Analysis
161
5.2.3
Scaling
162
5.2.4
Dynamical Similarity
163
5.3
Groups: DeÔ¨Ånitions
164
5.3.1
Group Axioms
165
5.3.2
Isomorphisms and Homomorphisms
166
5.4
Examples of Discrete Groups
166
5.4.1
Finite Groups
166
5.4.1.1
The Two-Element Group Z2
166
5.4.1.2
Group of Equilateral Triangle C3v
167
5.4.1.3
Cyclic Groups Cn
168
5.4.1.4
Permutation Groups Sn
168
5.4.1.5
Generators and Relations
169
5.4.2
InÔ¨Ånite Discrete Groups
169
5.4.2.1
Translation Groups: One Dimension
169
5.4.2.2
Translation Groups: Two Dimensions
170
5.4.2.3
Space Groups
170
5.5
Examples of Matrix Groups
170
5.5.1
Translation Groups
170
5.5.2
Heisenberg Group H3
171
5.5.3
Rotation Group SO(3)
171
5.5.4
Lorentz Group SO(3, 1)
172
5.6
Lie Groups
173
5.7
Lie Algebras
175
5.7.1
Structure Constants
175
5.7.2
Constructing Lie Algebras by Linearization
175
5.7.3
Constructing Lie Groups by Exponentiation
177
5.7.4
Cartan Metric
178
5.7.5
Operator Realizations of Lie Algebras
179
5.7.6
Disentangling Results
180
5.8
Riemannian Symmetric Spaces
181

X
Contents
5.9
Applications in Classical Physics
182
5.9.1
Principle of Relativity
182
5.9.2
Making Mechanics and Electrodynamics Compatible
182
5.9.3
Gravitation
184
5.9.4
ReÔ¨Çections
185
5.10
Linear Representations
185
5.10.1
Maps to Matrices
185
5.10.2
Group Element‚ÄìMatrix Element Duality
186
5.10.3
Classes and Characters
187
5.10.4
Fourier Analysis on Groups
187
5.10.4.1
Remark on Terminology
188
5.10.5
Irreps of SU(2)
188
5.10.6
Crystal Field Theory
190
5.11
Symmetry Groups
190
5.12
Dynamical Groups
194
5.12.1
Conformal Symmetry
194
5.12.2
Atomic Shell Structure
195
5.12.3
Nuclear Shell Structure
195
5.12.4
Dynamical Models
198
5.13
Gauge Theory
199
5.14
Group Theory and Special Functions
202
5.14.1
Summary of Some Properties
202
5.14.2
Relation with Lie Groups
202
5.14.3
Spherical Harmonics and SO(3)
203
5.14.4
DiÔ¨Äerential and Recursion Relations
204
5.14.5
DiÔ¨Äerential Equation
205
5.14.6
Addition Theorems
206
5.14.7
Generating Functions
206
5.15
Summary
207
Glossary
208
References
210
6
Algebraic Topology
211
Vanessa Robins
6.1
Introduction
211
6.2
Homotopy Theory
212
6.2.1
Homotopy of Paths
212
6.2.2
The Fundamental Group
213
6.2.3
Homotopy of Spaces
215
6.2.4
Examples
215
6.2.5
Covering Spaces
216
6.2.6
Extensions and Applications
217
6.3
Homology
218
6.3.1
Simplicial Complexes
219
6.3.2
Simplicial Homology Groups
219

Contents
XI
6.3.3
Basic Properties of Homology Groups
221
6.3.4
Homological Algebra
222
6.3.5
Other Homology Theories
224
6.4
Cohomology
224
6.4.1
De Rham Cohomology
226
6.5
Morse Theory
226
6.5.1
Basic Results
226
6.5.2
Extensions and Applications
228
6.5.3
Forman‚Äôs Discrete Morse Theory
229
6.6
Computational Topology
230
6.6.1
The Fundamental Group of a Simplicial Complex
230
6.6.2
Smith Normal form for Homology
231
6.6.3
Persistent Homology
232
6.6.4
Cell Complexes from Data
234
Further Reading
236
References
236
7
Special Functions
239
Chris Athorne
7.1
Introduction
239
7.2
Discrete Symmetry
241
7.2.1
Symmetries
241
7.2.2
Coxeter Groups
243
7.2.3
Symmetric Functions
244
7.2.4
Invariants of Coxeter Groups
246
7.2.5
Fuchsian Equations
247
7.3
Continuous Symmetry
250
7.3.1
Lie Groups and Lie Algebras
250
7.3.2
Representations
251
7.3.3
The Laplace Operator
253
7.3.4
Spherical Harmonics
255
7.3.5
Separation of Variables
256
7.3.6
Bessel Functions
256
7.3.7
Addition Laws
258
7.3.8
The Hypergeometric Equation
258
7.3.9
Orthogonality
260
7.3.10
Orthogonal Polynomials
260
7.4
Factorization
261
7.4.1
The Bessel Equation
262
7.4.2
Hermite
262
7.4.3
Legendre
263
7.4.4
‚ÄúFactorization‚Äù of PDEs
265
7.4.5
Dunkl Operators
267
7.5
Special Functions Without Symmetry
268
7.5.1
Airy Functions
268

XII
Contents
7.5.1.1
Stokes Phenomenon
269
7.5.2
Liouville Theory
269
7.5.3
DiÔ¨Äerential Galois Theory
271
7.6
Nonlinear Special Functions
272
7.6.1
Weierstra√ü Elliptic Functions
272
7.6.1.1
Lam√© Equations
277
7.6.2
Jacobian Elliptic Functions
277
7.6.3
Theta Functions
278
7.6.4
Painlev√© Transcendents
280
7.7
Discrete Special Functions
283
7.7.1
ùúï‚ÄìùõøTheory
283
7.7.2
Quantum Groups
283
7.7.3
DiÔ¨Äerence Operators
284
7.7.4
q-Hermite Polynomials
285
7.7.5
Discrete Painlev√© Equations
286
References
286
8
Computer Algebra
291
James H. Davenport
8.1
Introduction
291
8.2
Computer Algebra Systems
292
8.3
‚ÄúElementary‚Äù Algorithms
292
8.3.1
Representation of Polynomials
292
8.3.2
Greatest Common Divisors
294
8.3.2.1
Intermediate Expression Swell
294
8.3.2.2
Sparsity
295
8.3.3
Square-free Decomposition
295
8.3.4
Extended Euclidean Algorithm
296
8.4
Advanced Algorithms
296
8.4.1
Modular Algorithms‚ÄìInteger
296
8.4.2
Modular Algorithms‚ÄìPolynomial
297
8.4.3
The Challenge of Factorization
298
8.4.4
p-adic Algorithms‚ÄìInteger
300
8.4.5
p-adic Algorithms‚ÄìPolynomial
301
8.5
Solving Polynomial Systems
301
8.5.1
Solving One Polynomial
301
8.5.2
Real Roots
303
8.5.3
Linear Systems
304
8.5.4
Multivariate Systems
304
8.5.5
Gr√∂bner Bases
305
8.5.6
Regular Chains
308
8.6
Integration
308
8.6.1
Rational Functions
308
8.6.2
More Complicated Functions
310
8.6.3
Linear Ordinary DiÔ¨Äerential Equations
311

Contents
XIII
8.7
Interpreting Formulae as Functions
312
8.7.1
Fundamental Theorem of Calculus Revisited
312
8.7.2
SimpliÔ¨Åcation of Functions
313
8.7.3
Real Problems
314
8.8
Conclusion
315
References
315
9
DiÔ¨Äerentiable Manifolds
319
Marcelo Epstein
9.1
Introduction
319
9.2
Topological Spaces
319
9.2.1
DeÔ¨Ånition
319
9.2.2
Continuity
320
9.2.3
Further Topological Notions
320
9.3
Topological Manifolds
320
9.3.1
Motivation
320
9.3.2
DeÔ¨Ånition
321
9.3.3
Coordinate Charts
321
9.3.4
Maps and Their Representations
321
9.3.5
A Physical Application
322
9.3.6
Topological Manifolds with Boundary
323
9.4
DiÔ¨Äerentiable Manifolds
323
9.4.1
Motivation
323
9.4.2
DeÔ¨Ånition
323
9.4.3
DiÔ¨Äerentiable Maps
324
9.4.4
Tangent Vectors
324
9.4.5
Brief Review of Vector Spaces
325
9.4.5.1
DeÔ¨Ånition
325
9.4.5.2
Linear Independence and Dimension
325
9.4.5.3
The Dual Space
326
9.4.6
Tangent and Cotangent Spaces
326
9.4.7
The Tangent and Cotangent Bundles
327
9.4.8
A Physical Interpretation
328
9.4.9
The DiÔ¨Äerential of a Map
328
9.5
Vector Fields and the Lie Bracket
330
9.5.1
Vector Fields
330
9.5.2
The Lie Bracket
330
9.5.3
A Physical Interpretation: Continuous Dislocations
331
9.5.4
Pushforwards
334
9.6
Review of Tensor Algebra
334
9.6.1
Linear Operators and the Tensor Product
334
9.6.2
Symmetry and Skew Symmetry
337
9.6.3
The Algebra of Tensors on a Vector Space
337
9.6.4
Exterior Algebra
339
9.7
Forms and General Tensor Fields
341

XIV
Contents
9.7.1
1-Forms
341
9.7.2
Pullbacks
341
9.7.3
Tensor Bundles
342
9.7.4
The Exterior Derivative
343
9.8
Symplectic Geometry
345
9.8.1
Symplectic Vector Spaces
345
9.8.2
Symplectic Manifolds
345
9.8.3
Hamiltonian Systems
346
9.9
The Lie Derivative
347
9.9.1
The Flow of a Vector Field
347
9.9.2
One-parameter Groups of Transformations Generated by Flows
348
9.9.3
The Lie Derivative
348
9.9.3.1
The Lie Derivative of a Scalar
349
9.9.3.2
The Lie Derivative of a Vector Field
350
9.9.3.3
The Lie Derivative of a 1-form
350
9.9.3.4
The Lie Derivative of Arbitrary Tensor Fields
350
9.9.3.5
The Lie Derivative in Components
351
Further Reading
351
10
Topics in DiÔ¨Äerential Geometry
353
Marcelo Epstein
10.1
Integration
353
10.1.1
Integration of n-Forms in ‚Ñùn
353
10.1.2
Integration of Forms on Oriented Manifolds
354
10.1.3
Stokes‚Äô Theorem
355
10.2
Fluxes in Continuum Physics
355
10.2.1
Extensive-Property Densities
355
10.2.2
Balance Laws, Flux Densities, and Sources
356
10.2.3
Flux Forms and Cauchy‚Äôs Formula
356
10.2.4
DiÔ¨Äerential Expression of the Balance Law
357
10.3
Lie Groups
358
10.3.1
DeÔ¨Ånition
358
10.3.2
Group Actions
358
10.3.3
One-Parameter Subgroups
360
10.3.4
Left- and Right-Invariant Vector Fields on a Lie Group
361
10.4
Fiber Bundles
361
10.4.1
Introduction
361
10.4.2
DeÔ¨Ånition
361
10.4.3
Simultaneity in Classical Mechanics
363
10.4.4
Adapted Coordinate Systems
363
10.4.5
The Bundle of Linear Frames
363
10.4.6
Bodies with Microstructure
364
10.4.7
Principal Bundles
364
10.4.8
Associated Bundles
365
10.5
Connections
367

Contents
XV
10.5.1
Introduction
367
10.5.2
Ehresmann Connection
368
10.5.3
Parallel Transport along a Curve
368
10.5.4
Connections in Principal Bundles
369
10.5.5
Distributions and the Theorem of Frobenius
370
10.5.6
Curvature
371
10.5.7
Cartan‚Äôs Structural Equation
372
10.5.8
Bianchi Identities
372
10.5.9
Linear Connections
372
10.5.10
The Canonical 1-Form
373
10.5.11
The ChristoÔ¨Äel Symbols
374
10.5.12
Parallel Transport and the Covariant Derivative
374
10.5.13
Curvature and Torsion
375
10.6
Riemannian Manifolds
377
10.6.1
Inner-Product Spaces
377
10.6.2
Riemannian Manifolds
379
10.6.3
Riemannian Connections
380
Further Reading
380
Part III: Analysis
383
11
Dynamical Systems
385
David A.W. Barton
11.1
Introduction
385
11.1.1
DeÔ¨Ånition of a Dynamical System
385
11.1.2
Invariant Sets
386
11.2
Equilibria
386
11.2.1
DeÔ¨Ånition and Calculation
386
11.2.2
Stability
387
11.2.3
Linearization
387
11.2.4
Lyapunov Functions
388
11.2.5
Topological Equivalence
389
11.2.6
Manifolds
390
11.2.7
Local Bifurcations
391
11.2.8
Saddle-Node Bifurcation
392
11.2.9
Hopf Bifurcation
393
11.2.10
Pitchfork Bifurcation
396
11.2.11
Center Manifolds
398
11.3
Limit Cycles
399
11.3.1
DeÔ¨Ånition and Calculation
399
11.3.1.1
Harmonic Balance Method
399
11.3.1.2
Numerical Shooting
400
11.3.1.3
Collocation
401
11.3.2
Linearization
402
11.3.3
Topological Equivalence
403

XVI
Contents
11.3.4
Manifolds
403
11.3.5
Local Bifurcations
404
11.3.6
Period-Doubling Bifurcation
404
11.4
Numerical Continuation
405
11.4.1
Natural Parameter Continuation
405
11.4.2
Pseudo-Arc-Length Continuation
407
11.4.3
Continuation of Bifurcations
407
References
408
12
Perturbation Methods
411
James Murdock
12.1
Introduction
411
12.2
Basic Concepts
412
12.2.1
Perturbation Methods versus Numerical Methods
412
12.2.2
Perturbation Parameters
413
12.2.3
Perturbation Series
415
12.2.4
Uniformity
416
12.3
Nonlinear Oscillations and Dynamical Systems
418
12.3.1
Rest Points and Regular Perturbations
418
12.3.2
Simple Nonlinear Oscillators and Lindstedt‚Äôs Method
419
12.3.3
Averaging Method for Single-Frequency Systems
422
12.3.4
Multifrequency Systems and Hamiltonian Systems
424
12.3.5
Multiple-Scale Method
426
12.3.6
Normal Forms
427
12.3.7
Perturbation of Invariant Manifolds; Melnikov Functions
429
12.4
Initial and Boundary Layers
429
12.4.1
Multiple-Scale Method for Initial Layer Problems
429
12.4.2
Matching for Initial Layer Problems
431
12.4.3
Slow‚ÄìFast Systems
432
12.4.4
Boundary Layer Problems
433
12.4.5
WKB Method
434
12.4.6
Fluid Flow
434
12.5
The ‚ÄúRenormalization Group‚Äù Method
435
12.5.1
Initial and Boundary Layer Problems
436
12.5.2
Nonlinear Oscillations
439
12.5.3
WKB Problems
441
12.6
Perturbations of Matrices and Spectra
442
Glossary
444
References
446
Further Reading
447
13
Functional Analysis
449
Pavel Exner
13.1
Banach Space and Operators on Them
449
13.1.1
Vector and Normed Spaces
449

Contents
XVII
13.1.2
Operators on Banach Spaces
450
13.1.3
Spectra of Closed Operators
452
13.2
Hilbert Spaces
452
13.2.1
Hilbert-Space Geometry
452
13.2.2
Direct Sums and Tensor Products
454
13.3
Bounded Operators on Hilbert Spaces
454
13.3.1
Hermitean Operators
454
13.3.2
Unitary Operators
455
13.3.3
Compact Operators
456
13.3.4
Schatten Classes
456
13.4
Unbounded Operators
457
13.4.1
Operator Adjoint and Closure
457
13.4.2
Normal and Self-Adjoint Operators
458
13.4.3
Tensor Products of Operators
460
13.4.4
Self-Adjoint Extensions
460
13.5
Spectral Theory of Self-Adjoint Operators
462
13.5.1
Functional Calculus
462
13.5.2
Spectral Theorem
463
13.5.3
More about Spectral Properties
465
13.5.4
Groups of Unitary Operators
467
13.6
Some Applications in Quantum Mechanics
468
13.6.1
Schr√∂dinger Operators
468
13.6.2
Scattering Theory
470
Glossary
472
References
473
14
Numerical Analysis
475
Lyonell Boulton
14.1
Introduction
475
14.2
Algebraic Equations
476
14.2.1
Nonlinear Scalar Equations
476
14.2.2
Nonlinear Systems
477
14.2.3
Numerical Minimization
479
14.3
Finite-Dimensional Linear Systems
480
14.3.1
Direct Methods and Matrix Factorization
480
14.3.2
Iteration Methods for Linear Problems
482
14.3.3
Computing Eigenvalues of Finite Matrices
485
14.4
Approximation of Continuous Data
487
14.4.1
Lagrange Interpolation
487
14.4.2
The Interpolation Error
487
14.4.3
Hermite Interpolation
488
14.4.4
Piecewise Polynomial Interpolation
489
14.5
Initial Value Problems
491
14.5.1
One-Step Methods
491
14.5.2
Multistep Methods
492

XVIII
Contents
14.5.3
Runge‚ÄìKutta Methods
494
14.5.4
Stability and Global Stability
495
14.6
Spectral Problems
496
14.6.1
The InÔ¨Ånite-Dimensional min‚Äìmax Principle
497
14.6.2
Systems ConÔ¨Åned to a Box
498
14.6.3
The Case of UnconÔ¨Åned Systems
499
Further Reading
499
References
500
15
Mathematical Transformations
503
Des McGhee, Rainer Picard, Sascha TrostorÔ¨Ä, and Marcus Waurick
15.1
What are Transformations and Why are They Useful?
503
15.2
The Fourier Series Transformations
506
15.2.1
The Abstract Fourier Series
506
15.2.2
The Classical Fourier Series
507
15.2.3
The Fourier Series Transformation in L2 (S‚ÑÇ(0, 1)
)
509
15.3
The z-Transformation
509
15.4
The Fourier‚ÄìLaplace Transformation
510
15.4.1
Convolutions as Functions of ùúïùúö
512
15.4.1.1
Functions of ùúïùúö
512
15.4.1.2
Convolutions
513
15.4.2
The Fourier‚ÄìPlancherel Transformation
515
15.5
The Fourier‚ÄìLaplace Transformation and Distributions
515
15.5.1
Impulse Response
517
15.5.2
Shannon‚Äôs Sampling Theorem
517
15.6
The Fourier-Sine and Fourier-Cosine Transformations
518
15.7
The Hartley Transformations H¬±
519
15.8
The Mellin Transformation
520
15.9
Higher-Dimensional Transformations
521
15.10
Some Other Important Transformations
522
15.10.1
The Hadamard Transformation
522
15.10.2
The Hankel Transformation
523
15.10.3
The Radon Transformation
523
References
525
16
Partial DiÔ¨Äerential Equations
527
Des McGhee, Rainer Picard, Sascha TrostorÔ¨Ä, and Marcus Waurick
16.1
What are Partial DiÔ¨Äerential Equations?
527
16.2
Partial DiÔ¨Äerential Equations in ‚Ñùn+1, n ‚àà‚Ñï, with Constant
CoeÔ¨Écients
529
16.2.1
Evolutionarity
530
16.2.2
An Outline of Distribution Theory
531
16.2.3
Integral Transformation Methods as a Solution Tool
533
16.3
Partial DiÔ¨Äerential Equations of Mathematical Physics
537
16.4
Initial-Boundary Value Problems of Mathematical Physics
540

Contents
XIX
16.4.1
Maxwell‚Äôs Equations
542
16.4.2
Viscoelastic Solids
543
16.4.2.1
The Kelvin‚ÄìVoigt Model
543
16.4.2.2
The Poynting‚ÄìThomson Model (The Linear Standard Model)
544
16.5
Coupled Systems
544
16.5.1
Thermoelasticity
545
16.5.2
Piezoelectromagnetism
546
16.5.3
The Extended Maxwell System and its Uses
547
References
548
17
Calculus of Variations
551
Tom√°≈° Roub√≠Àácek
17.1
Introduction
551
17.2
Abstract Variational Problems
551
17.2.1
Smooth (DiÔ¨Äerentiable) Case
552
17.2.2
Nonsmooth Case
554
17.2.3
Constrained Problems
555
17.2.4
Evolutionary Problems
556
17.2.4.1
Variational Principles
556
17.2.4.2
Evolution Variational Inequalities
559
17.2.4.3
Recursive Variational Problems Arising by Discretization in Time
560
17.3
Variational Problems on SpeciÔ¨Åc Function Spaces
562
17.3.1
Sobolev Spaces
562
17.3.2
Steady-State Problems
563
17.3.2.1
Second Order Systems of Equations
564
17.3.2.2
Fourth Order Systems
568
17.3.2.3
Variational Inequalities
570
17.3.3
Some Examples
571
17.3.3.1
Nonlinear Heat-Transfer Problem
571
17.3.3.2
Elasticity at Large Strains
572
17.3.3.3
Small-Strain Elasticity, Lam√© System, Signorini Contact
573
17.3.3.4
Sphere-Valued Harmonic Maps
574
17.3.3.5
Saddle-Point-Type Problems
575
17.3.4
Evolutionary Problems
575
17.4
Miscellaneous
576
17.4.1
Numerical Approximation
576
17.4.2
Extension of Variational Problems
577
17.4.3
Œì-Convergence
581
Glossary
582
Further Reading
584
References
586
Index
589


XXI
List of Contributors
Chris Athorne
University of Glasgow
School of Mathematics and Statistics
15 University Gardens
Glasgow G12 8QW
UK
David A.W. Barton
University of Bristol
Department of Engineering Mathematics
Merchant Venturers Building
Woodland Road
Bristol BS8 1UB
UK
Kurt Binder
Johannes Gutenberg University of Mainz
Department of Physics, Mathematics and
Informatics
Institute of Physics
Staudingerweg 7
55128 Mainz
Germany
Lyonell Boulton
Heriot-Watt University
Department of Mathematics
School of Mathematical and Computer
Sciences
Colin MacLaurin Building
Riccarton
Edinburgh EH14 4AS
UK
James R. Cruise
Heriot-Watt University
Department of Actuarial Mathematics
and Statistics
and the Maxwell Institute for
Mathematical Sciences
Edinburgh EH14 4AS
UK
James H. Davenport
University of Bath
Department of Computer Science
Claverton Down
Bath
Bath BA2 7AY
UK
Marcelo Epstein
University of Calgary
Department of Mechanical and
Manufacturing
Engineering
Schulich
School of Engineering
2500 University Drive NW
Calgary
Alberta, T2N 1N4
Canada
Ernesto Estrada
University of Strathclyde
Department of Mathematics and
Statistics
26 Richmond Street
Glasgow G1 1XQ
UK

XXII
List of Contributors
Pavel Exner
Czech Technical University
Doppler Institute
BÀárehov√° 7
11519 Prague
Czech Republic
and
Department of Theoretical Physics
Nuclear Physics Institute
Academy of Sciences of the
Czech Republic
Hlavn¬¥ƒ± 130
25068 ÀáRe≈æ
Czech Republic
Robert Gilmore
Drexel University
Department of Physics
3141 Chestnut Street
Philadelphia
PA 19104
USA
Ostap Hryniv
Durham University
Department of Mathematical Sciences
South Road
Durham DH1 3LE
UK
Gabriel J. Lord
Heriot-Watt University
Department of Mathematics
Maxwell Institute for Mathematical
Sciences
Edinburgh EH14 4AS
UK
James Murdock
Iowa State University
Department of Mathematics
478 Carver Hall
Ames
IA 50011
USA
Des McGhee
University of Strathclyde
Department of Mathematics and
Statistics
26 Richmond Street
Glasgow G1 1XH
Scotland
UK
Rainer Picard
Technische Universit√§t Dresden
FR Mathematik
Institut f√ºr Analysis
Willersbau
Zellescher Weg 12-14
01069 Dresden
Germany
Vanessa Robins
The Australian National University
Department of Applied Mathematics
Research School of Physics and
Engineering
Canberra ACT 0200
Australia
Tom√°≈° Roub√≠Àácek
Charles University
Mathematical Institute
Sokolovsk√° 83
Prague 186 75
Czech Republic

List of Contributors
XXIII
and
Institute of Thermomechanics
of the ASCR
Prague
Czech Republic
Sascha Trostorff
Technische Universit√§t Dresden
FR Mathematik
Institut f√ºr Analysis
Willersbau
Zellescher Weg 12-14
01069 Dresden
Germany
Andrew R. Wade
Durham University
Department of Mathematical Sciences
South Road
Durham DH1 3LE
UK
Marcus Waurick
Technische Universit√§t Dresden
FR Mathematik
Institut f√ºr Analysis
Willersbau
Zellescher Weg 12-14
01069 Dresden
Germany


XXV
Preface
The intensely fruitful symbiosis between physics and mathematics is nothing short of
miraculous. It is not a symmetric interaction; however, physical laws, which involve
relations between variables, are always in need of rules and techniques for manipulating
these relations: hence, the role of mathematics in providing tools for physics, and hence,
the need for books such as this one. Physics, in its turn, supplies motivation for the
development of mathematically sound techniques and concepts; it is enough to mention
the importance of celestial mechanics in the evolution of perturbation methods and the
impetus Dirac‚Äôs delta ‚Äúfunction‚Äù gave to the work on generalized functions. So perhaps a
sister volume to the present compendium would be Physical Insights for Mathematicians.
(From the above, one might get the impression that physics and mathematics are two
separate, even opposing, domains. This is of course not true; there is a well-established
Ô¨Åeld of mathematical physics and many mathematicians working in Ô¨Çuid and continuum
mechanics would be hard-pressed to separate the mathematics and the physics elements
of their seamless activity.)
The present book is intended for the use of advanced undergraduates, graduate stu-
dents, and researchers in physics, who are aware of the usefulness of a particular math-
ematical approach and need a quick point of entry into its vocabulary, main results,
and the literature. However, one cannot cleanly single out parts of mathematics that are
useful for physics and ones that are not; for example, while the theory of operators in
Hilbert spaces is undoubtedly indispensable in quantum mechanics, an area as abstruse
as category theory is becoming increasingly popular in cosmology and has found appli-
cations in developmental biology.1) Hence, the concept of ‚Äúmathematics useful in physics‚Äù
arguably covers the same area as mathematics tout court, and anyone embarking on the
publication of a book such as the present one does so in the certainty that no single
book can do justice to the intricate interpenetration of mathematics and physics. It is
quite possible to write a second, and perhaps a third volume of an encyclopedia such as
ours.
Let us quickly mention signiÔ¨Åcant areas that are not being covered here. These
include combinatorics, deterministic chaos, fractals, nonlinear partial diÔ¨Äerential
1) The exact relation between physics and biology is still shrouded in mystery, but certainly objects of
interest in biology are physical objects. The time for a truly useful Mathematical Tools for Biologists has
not yet arrived, but is awaited eagerly.

XXVI
Preface
equations, and symplectic geometry. It was also felt that a separate chapter on context-
free modeling was not necessary as there is an ample literature on modeling case
studies.
What this book oÔ¨Äers is an attractive mix of classical areas of applications of mathe-
matics to physics and of areas that have only come to prominence recently. Thus, we have
substantive chapters on asymptotic methods, calculus of variations, diÔ¨Äerential geometry
and topology of manifolds, dynamical systems theory, functional analysis, group theory,
numerical methods, partial diÔ¨Äerential equations of mathematical physics, special func-
tions, and transform methods. All these are up-to-date surveys; for example, the chapter
on asymptotic methods discusses recent renormalized group-based approaches, while the
chapter on variational methods considers examples where no smooth minimizers exist.
These chapters appear side by side with a decidedly modern computational take on alge-
braic topology and in-depth reviews of such increasingly important areas as graph and net-
work theory, Monte Carlo simulations, stochastic diÔ¨Äerential equations, and algorithms
in symbolic computation, an important complement to analytic and numerical problem-
solving approaches.
It is hoped that the layout of the text allows for easy cross-referencing between chapters,
and that by the end of a chapter, the reader will have a clear view of the area under
discussion and will be know where to go to learn more. Bon voyage!

1
Part I
Probability
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.


3
1
Stochastic Processes
James R. Cruise, Ostap O. Hryniv, and Andrew R. Wade
1.1
Introduction
Basic probability theory deals, among
other things, with random variables and
their properties. A random variable is the
mathematical abstraction of the following
concept: we make a measurement on some
physical system, subject to randomness
or uncertainty, and observe the value. We
can, for example, construct a mathematical
model for the system and try to predict the
behavior of our random observable, per-
haps through its distribution, or at least its
average value (mean). Even in the simplest
applications, however, we are confronted
by systems that change over time. Now we
do not have a single random variable, but a
family of random variables. The nature of
the physical system that we are modeling
determines the structure of dependencies
of the variables.
A stochastic (or random) process is the
mathematical abstraction of these systems
that change randomly over time. Formally,
a stochastic process is a family of random
variables (Xt)t‚ààT, where T is some index set
representing time. The two main examples
are T = {0, 1, 2, ‚Ä¶} (discrete time) and
T = [0, ‚àû)
(continuous
time);
diÔ¨Äerent
applications will favor one or other of
these. Interesting classes of processes are
obtained by imposing additional structure
on the family Xt, as we shall see.
The aim of this chapter is to give a tour of
some of the highlights of stochastic process
theory and its applications in the physical
sciences. In line with the intentions of this
volume, our emphasis is on tools. However,
the combination of a powerful tool and an
unsteady grip is a hazardous one, so we
have attempted to maintain mathematical
accuracy. For reasons of space, the pre-
sentation is necessarily concise. While we
cover several important topics, we omit
many more. We include references for fur-
ther reading on the topics that we do cover
throughout the text and in Section 1.8.
The tools that we exhibit include gener-
ating functions and other transforms, and
renewal structure, including the Markov
property, which can be viewed loosely as a
notion of statistical self-similarity.
In the next section, we discuss some
of the tools that we will use, with some
examples. The basic notions of probability
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

4
1 Stochastic Processes
theory that we use are summarized in
Section 1.A.
1.2
Generating Functions and Integral
Transforms
1.2.1
Generating Functions
Given a sequence (ak)k‚â•0 of real numbers,
the function
G(s) = Ga(s) =
‚àë
k‚â•0
aksk
(1.1)
is called the generating function of (ak)k‚â•0.
When Ga(s) is Ô¨Ånite for some s ‚â†0, the
series (1.1) converges in the disc {z ‚àà‚ÑÇ‚à∂
|z| < |s|} and thus its coeÔ¨Écients ak can be
recovered via diÔ¨Äerentiation
ak = 1
k!
( d
ds
)k
Ga(s)
||||s=0
,
(1.2)
or using the Cauchy integral formula
ak =
1
2ùúãi ‚àÆ|z|=r
Ga(z)
zk+1 dz,
(i2 = ‚àí1), (1.3)
with properly chosen r > 0. This obser-
vation is often referred to as the unique-
ness property: if two generating functions,
say Ga(s) and Gb(s), are Ô¨Ånite and coincide
in some open neighborhood of the origin,
then ak = bk for all k ‚â•0. In particular, one
can identify the sequence from its generat-
ing function.
The generating function is one of many
transforms very useful in applications.
We will discuss some further examples in
Section 1.2.3.
Example 1.1 Imagine one needs to pay the
sum of n pence using only one pence and
two pence coins. In how many ways can this
be done?1)
(a)
If the order matters, that is, when
1 + 2 and 2 + 1 are two distinct ways
of paying 3 pence, the question is
about counting the number an of
monomer/dimer
conÔ¨Ågurations
on
the interval of length n. One easily
sees that a0 = a1 = 1, a2 = 2, and
in general an = an‚àí1 + an‚àí2, n ‚â•2,
because the leftmost monomer can
be extended to a full conÔ¨Åguration
in an‚àí1 ways, whereas the leftmost
dimer is compatible with exactly an‚àí2
conÔ¨Ågurations on the remainder of
the interval. A straightforward com-
putation using the recurrence relation
above now gives
Ga(s) = 1 + s +
‚àë
n‚â•2
(an‚àí1 + an‚àí2
)sn
= 1 + sGa(s) + s2Ga(s),
so
that
Ga(s) = (1 ‚àís ‚àís2)‚àí1.
The
coeÔ¨Écient an can now be recovered
using (1.2), (1.3), or partial fractions
and then power series expansion.
(b)
If the order does not matter, that is,
when 1 + 2 and 2 + 1 are regarded
as identical ways of paying 3 pence,
the question above boils down to cal-
culating the number bn of nonnega-
tive integer solutions to the equation
n1 + 2n2 = n. One easily sees that b0 =
b1 = 1, b2 = b3 = 2, and a straightfor-
ward induction shows that bn is the
coeÔ¨Écient of sk in the product
1) Early use of generating functions was often in
this enumerative vein, going back to de Moivre
and exempliÔ¨Åed by Laplace in his Th√©orie ana-
lytique des probabilit√©s of 1812. The full power
of generating functions in the theory of stochas-
tic processes emerged later with work of P√≥lya,
Feller, and others.

1.2 Generating Functions and Integral Transforms
5
(1 + s + s2 + s3 + s4 + ¬∑ ¬∑ ¬∑)
√ó (1 + s2 + s4 + s6 + ¬∑ ¬∑ ¬∑).
In other words, Gb(s) = ‚àë
n‚â•0 bnsn =
(1 ‚àís)‚àí1(1 ‚àís2)‚àí1.
In this chapter, we will make use of
generating functions for various sequences
with probabilistic meaning. In particular,
given a ‚Ñ§+-valued2) random variable X,
we can consider the corresponding prob-
ability generating function, which is the
generating function of the sequence (pk),
where pk = ‚Ñô[X = k], k ‚àà‚Ñ§+, describes
the probability mass function of X. Thus,
the probability generating function of X
is given by (writing ùîºfor expectation: see
Section 1.A)
G(s) = GX(s) =
‚àë
k‚â•0
sk‚Ñô[X = k] = ùîº[sX].
(1.4)
Example 1.2
(a)
If Y ‚àºBe(p) and X ‚àºBin(n, p) (see
Example 1.22), then GY(s) = (1 ‚àíp) +
ps and, by the binomial theorem,
GX(s) = ‚àë
k‚â•0
(n
k
)(sp)k(1 ‚àíp)n‚àík =
((1 ‚àíp) + ps)n.
(b)
If X ‚àºPo(ùúÜ) (see Example 1.23), then
Taylor‚Äôs formula implies
GX(s) =
‚àë
k‚â•0
sk ùúÜk
k! e‚àíùúÜ= eùúÜ(s‚àí1).
Notice that GX(1) = ‚Ñô[X < ‚àû], and thus
for |s| ‚â§1, |GX(s)| ‚â§GX(1) ‚â§1, implying
that the power series GX(s) can be diÔ¨Äer-
entiated in the disk |s| < 1 any number of
times. As a result,
G(k)
X (s)=
( d
ds
)k
GX(s) = ùîº[(X)ksX‚àík],
|s| < 1,
2) Throughout we use the notation ‚Ñ§+ =
{0, 1, 2, ‚Ä¶} and ‚Ñï= {1, 2, ‚Ä¶}.
where
(x)k =
x!
(x‚àík)! = x(x ‚àí1) ¬∑ ¬∑ ¬∑ (x ‚àík +
1). Taking the limit s ‚Üó1 we obtain the kth
factorial moment of X, ùîº[(X)k] = G(k)
X (1‚àí),
where the last expression denotes the value
of the kth left derivative of GX(‚ãÖ) at 1.
Example 1.3 If X ‚àºPo(ùúÜ), from Example
1.2b, we deduce ùîº[(X)k] = ùúÜk.
Theorem 1.1 If X and Y are independent
‚Ñ§+-valued random variables, then the sum
Z = X + Y has generating function GZ(s) =
GX(s)GY(s).
Proof. As X and Y are independent, so
are sX and sY; consequently, the expec-
tation factorizes, ùîº[sXsY] = ùîº[sX]ùîº[sY], by
Theorem 1.18.
Example 1.4
(a)
If X ‚àºPo(ùúÜ) and Y ‚àºPo(ùúá) are inde-
pendent,
then
X + Y ‚àºPo(ùúÜ+ ùúá).
Indeed, by Theorem 1.1, GX+Y(s) =
e(ùúÜ+ùúá)(s‚àí1), which is the generating
function of the Po(ùúÜ+ ùúá) distribution;
the result now follows by uniqueness.
This fact is known as the additive
property of Poisson distributions.
(b)
If
‚Ñ§+-valued
random
variables
X1, ‚Ä¶ , Xn
are
independent
and
identically distributed (i.i.d.), then
Sn = X1 + ¬∑ ¬∑ ¬∑ + Xn
has
generat-
ing
function
GSn(s) = (GX(s))n.
If
Sn ‚àºBin(n, p),
then
Sn
is
a
sum
of
n
independent
Bernoulli
variables
with
parameter
p,
and
so
GSn(s) = ((1 ‚àíp) + ps)n
(see
Example 1.22).
The following example takes this idea
further.
Lemma 1.1 Let
X1, X2, ‚Ä¶
be
i.i.d.
‚Ñ§+-valued
random
variables,
and
let

6
1 Stochastic Processes
N
be
a
‚Ñ§+-valued
random
variable
independent of the Xi. Then the random
sum SN = X1 + ¬∑ ¬∑ ¬∑ + XN
has generating
function GSN (s) = GN
(GX(s)).
Proof. The claim follows from the partition
theorem for expectations (see Section 1.A):
ùîº[sSN ] =
‚àë
n‚â•0
ùîº[sSN ‚à£N = n]‚Ñô[N = n]
=
‚àë
n‚â•0
(GX(s))n‚Ñô[N = n].
Example 1.5 If (Xk)k‚â•1 are independent
Be(p) random variables and if N ‚àºPo(ùúÜ) is
independent of (Xk)k‚â•1, then
GSN (s) = GN(GX(s)) = eùúÜ(GX(s)‚àí1) = eùúÜp(s‚àí1),
that is, SN ‚àºPo(ùúÜp). This result has the
following important interpretation: if each
of N ‚àºPo(ùúÜ) objects is independently
selected with probability p, then the sample
contains SN ‚àºPo(ùúÜp) objects. This fact is
known as the thinning property of Poisson
distributions.
A
further
important
application
of
Lemma 1.1 is discussed in Section 1.2.2.
Example 1.6 [Renewals] A diligent janitor
replaces a light bulb on the same day as
it burns out. Suppose the Ô¨Årst bulb is put
in on day 0, and let Xi be the lifetime of
the ith bulb. Suppose that the Xi are i.i.d.
random variables with values in ‚Ñïand
common generating function Gf (s). DeÔ¨Åne
rn = ‚Ñô[a bulb was replaced on day n]
and
fk = ‚Ñô[the Ô¨Årst bulb was replaced on day k].
Then r0 = 1, f0 = 0, and for n ‚â•1,
rn = f1rn‚àí1 + f2rn‚àí2 + ¬∑ ¬∑ ¬∑ + fnr0 =
n
‚àë
k=1
fkrn‚àík.
(1.5)
Proceeding as in Example 1.1, we deduce
Gr(s) = 1 + Gf (s) Gr(s) for all |s| ‚â§1, so
that
Gr(s) =
1
1 ‚àíGf (s).
(1.6)
Remark 1.1 The ideas behind Example 1.6
have a vast area of applicability. For
example,
the
hitting
probabilities
of
discrete-time Markov chains have prop-
erty
similar
to
the
Ornstein‚ÄìZernike
relation (1.5), see, for example, Lemma 1.3.
Notice also that by repeatedly expanding
each ri on the right-hand side of (1.5), one
can rewrite the latter as
rn =
‚àë
ùìÅ‚â•1
‚àë
{k1,k2,‚Ä¶,kùìÅ}
ùìÅ
‚àè
j=1
fkj,
(1.7)
where
the
middle
sum
runs
over
all
decompositions of the integer n into ùìÅ
positive integer parts k1, k2, ‚Ä¶ , kùìÅ
(cf.
Example 1.1a).
The
decomposition (1.7)
is an example of the so-called polymer
representation; it arises in many models of
statistical mechanics.
The convolution of sequences (an)n‚â•0 and
(bn)n‚â•0 is (cn)n‚â•0 given by
cn =
n
‚àë
k=0
akbn‚àík,
(n ‚â•0);
(1.8)
we write c = a ‚ãÜb. A key property of con-
volutions is as follows.
Theorem 1.2 (Convolution theorem) If
c = a ‚ãÜb, then the associated generating
functions Gc(s), Ga(s), and Gb(s) satisfy
Gc(s) = Ga(s)Gb(s).
Proof. Compare the coeÔ¨Écients of sn on
both sides of the equality.
The convolution appears very often in
probability theory because the probability

1.2 Generating Functions and Integral Transforms
7
mass function of the sum X + Y of indepen-
dent variables X and Y is the convolution of
their respective probability mass functions.
In other words, Theorem 1.1 is a particular
case of Theorem 1.2.
Remark 1.2 The
sequence
(bn)n‚â•0
in
Example 1.1b
is
a
convolution
of
the
sequence 1, 1, 1, 1, ‚Ä¶
and the sequence
1, 0, 1, 0, 1, 0, ‚Ä¶ .
The uniqueness property of generating
functions aÔ¨Äords them an important role
in studying convergence of probability
distributions.
Theorem 1.3 For every Ô¨Åxed n ‚â•1, let
the
sequence
an,0, an,1, ‚Ä¶
be
a
prob-
ability
distribution
on
‚Ñ§+,
that
is,
an,k ‚â•0
and
‚àë
k‚â•0 an,k = 1.
Moreover,
let Gn(s) = ‚àë
k‚â•0 an,ksk be the generating
function of the sequence (an,k)k‚â•0. Then
lim
n‚Üí‚àûan,k = ak, for all k ‚â•0
‚áê‚áí
lim
n‚Üí‚àûGn(s) = G(s), for all s ‚àà[0, 1),
where G(s) is the generating function of the
limiting sequence (ak)k‚â•0.
Example 1.7 [Law
of
rare
events]
Let
(Xn)n‚â•1 be random variables such that
Xn ‚àºBin(n, pn). If n ‚ãÖpn ‚ÜíùúÜas n ‚Üí‚àû,
then the distribution of Xn converges to
that of X ‚àºPo(ùúÜ). Indeed, for every Ô¨Åxed
s ‚àà[0, 1), we have, as n ‚Üí‚àû,
GXn(s) = (1 + pn(s ‚àí1))n ‚Üíexp{ùúÜ(s ‚àí1)},
which is the generating function of a Po(ùúÜ)
random variable.
1.2.2
Example: Branching Processes
Let (Zn,k), n ‚àà‚Ñï, k ‚àà‚Ñï, be a family of i.i.d.
‚Ñ§+-valued random variables with common
probability mass function (pk)k‚â•0 and Ô¨Ånite
mean. The corresponding branching pro-
cess (Zn)n‚â•0 is deÔ¨Åned via Z0 = 1, and, for
n ‚â•1,
Zn = Zn,1 + Zn,2 + ¬∑ ¬∑ ¬∑ + Zn,Zn‚àí1,
(1.9)
where an empty sum is interpreted as zero.
The interpretation is that Zn is the num-
ber of individuals in the nth generation of
a population that evolves via a sequence of
such generations, in which each individual
produces oÔ¨Äspring according to (pk)k‚â•0,
independently of all other individuals in the
same generation; generation n + 1 consists
of all the oÔ¨Äspring of individuals in gener-
ation n. The process starts (generation 0)
with a single individual and the population
persists as long as the generations are
successful in producing oÔ¨Äspring, or until
extinction
occurs.
Branching
processes
appear naturally when modeling chain
reactions, growth of bacteria, epidemics,
and other similar phenomena3): a cru-
cial characteristic of the process is the
probability of extinction.
Let ùúën(s) ‚à∂= ùîº[sZn] be the generating
function of Zn; for simplicity, we write ùúë(s)
instead of ùúë1(s) = ùîº[sZ1]. Then ùúë0(s) = s,
and a straightforward induction based on
Lemma 1.1 implies
ùúën(s) = ùúën‚àí1(ùúë(s)),
(n > 1).
(1.10)
Equation (1.10) can be used to determine
the distribution of Zn for any n ‚â•0. In par-
ticular, one easily deduces that ùîº[Zn] = mn,
where m = ùîº[Z1] is the expected num-
ber of oÔ¨Äspring of a single individual.
3) The simplest branching processes, as discussed
here, are known as Galton‚ÄìWatson processes
after F. Galton and H. Watson‚Äôs work on the
propagation of human surnames; work on
branching processes in the context of nuclear
Ô¨Åssion, by S. Ulam and others, emerged out of
the Manhattan project.

8
1 Stochastic Processes
The long-term behavior of a branching
process is determined by the expected
value m: the process can be subcritical
(m < 1), critical (m = 1), or supercritical
(m > 1).
Remark 1.3 By Markov‚Äôs inequality (see
Section 1.A),
‚Ñô[Zn > 0] = ‚Ñô[Zn ‚â•1] ‚â§
ùîº[Zn] = mn. Hence, in the subcritical case,
‚Ñô[Zn > 0] ‚Üí0 as n ‚Üí‚àû(i.e., Zn ‚Üí0 in
probability). Moreover, the average total
population in this case is Ô¨Ånite, because
ùîº[‚àë
n‚â•0 Zn] = ‚àë
n‚â•0 mn = (1 ‚àím)‚àí1 < ‚àû.
It
follows
that,
with
probability
1,
‚àë
n‚â•0 Zn < ‚àû,
which
entails
Zn ‚Üí0
almost
surely.
This
last
statement
can
also
be
deduced
from
the
fact
that
‚àë
n‚â•0 ‚Ñô[Zn > 0] < ‚àû
using
the
Borel‚ÄìCantelli lemma (see, e.g., [1, 2]).
Extinction is the event Óà±= ‚à™‚àû
n=1{Zn =
0}.
As
{Zn = 0} ‚äÜ{Zn+1 = 0}
for
all
n ‚â•0,
the
extinction
probability
ùúå= ‚Ñô[Óà±] is well deÔ¨Åned (by continuity
of probability measure: see Section 1.A)
via
ùúå= limn‚Üí‚àû‚Ñô[Zn = 0],
where
‚Ñô[Zn = 0] = ùúën(0) is the probability of
extinction before the (n + 1)th generation.
Theorem 1.4 If
0 < p0 < 1,
then
the
extinction probability ùúåis given by the
smallest positive solution to the equation
s = ùúë(s).
(1.11)
In particular, if m = ùîº[Z] ‚â§1, then ùúå= 1;
otherwise, we have 0 < ùúå< 1.
Remark 1.4 The relation (1.11) has a clear
probabilistic sense. Indeed, by indepen-
dence, ‚Ñô[Óà±‚à£Z1 = k] = ùúåk
(as extinction
only occurs if each of the independent
branching processes associated with the
k individuals dies out). Then, by the
law of total probability (see Section 1.A),
we get4)
ùúå= ‚Ñô[Óà±] =
‚àë
k‚â•0
‚Ñô[Óà±‚à£Z1 = k]‚Ñô[Z1 = k]
=
‚àë
k‚â•0
ùúåk‚Ñô[Z1 = k] = ùúë(ùúå).
Notice that Theorem 1.4 characterizes
the extinction probability without the
necessity to compute ùúën( ‚ãÖ). The excluded
values p0 ‚àà{0, 1} are trivial: if p0 = 0, then
Zn ‚â•1 for all n ‚â•0 so that ùúå= 0; if p0 = 1,
then ‚Ñô[Z1 = 0] = ùúå= 1.
Proof of Theorem 1.4 Denote ùúån = ‚Ñô[Zn =
0] = ùúën(0). By continuity and strict mono-
tonicity of the generating function ùúë( ‚ãÖ), we
have (recall (1.10))
0 < ùúå1 =ùúë(0) < ùúå2 = ùúë(ùúå1) < ¬∑ ¬∑ ¬∑ < 1=ùúë(1) ,
so that ùúån ‚Üóùúå‚àà(0, 1] with ùúå= ùúë(ùúå).
Now if ùúåis another Ô¨Åxed point of ùúë( ‚ãÖ) in
[0, 1], that is, ùúå= ùúë(ùúå), then, by induction,
0 < ùúå1 = ùúë(0) < ùúå2 < ¬∑ ¬∑ ¬∑ < ùúë(ùúå) = ùúå,
so that ùúå= limn‚Üí‚àûùúån ‚â§ùúå, that is, ùúåis the
smallest positive solution to (1.11).
Finally, by continuity and convexity of
ùúë( ‚ãÖ) together with the fact ùúë(1) = 1, the
condition m = ùúë‚Ä≤(1) ‚â§1 implies ùúå= 1 and
the condition m = ùúë‚Ä≤(1) > 1 implies that ùúå
is the unique solution in (0, 1) to the Ô¨Åxed
point equation (1.11).
We thus see that the branching process
exhibits a phase transition: in the subcriti-
cal or critical regimes (m ‚â§1), the process
dies out with probability 1, whereas in the
supercritical case (m > 1) it survives for-
ever with positive probability 1 ‚àíùúå.
4) In Section 1.3, we will see this calculation as
exploiting the Markov property of the branching
process.

1.2 Generating Functions and Integral Transforms
9
1.2.3
Other Transforms
1.2.3.1
Moment Generating Functions
The moment generating function of a real-
valued random variable X is deÔ¨Åned by
MX(t) = ùîº[etX]. When Ô¨Ånite for t in some
neighborhood of 0, MX(t) behaves simi-
larly to the generating function GX(s) in
that it possesses the uniqueness prop-
erty
(identifying
the
corresponding
distribution),
maps
convolutions
(i.e.,
distributions
of
sums
of
independent
variables) into products, and can be used
to
establish
convergence
in
distribu-
tion.
If X is ‚Ñ§+-valued and its generating
function GX(s) is Ô¨Ånite for some s > 1, then
MX(t) is also Ô¨Ånite for some t ‚â†0, and
the two are related by MX(t) = GX(et). For
example, if X ‚àºBin(n, p), then MX(t) =
(1 + p(et ‚àí1))n (see Example 1.2a). The
terminology arises from the fact that if
MX(t) is diÔ¨Äerentiable at 0, then the kth
derivative of MX(t) evaluated at 0 gives
ùîº[Xk], the kth moment of X.
Example 1.8
In the case of a continuous
distribution
X
with
probability
den-
sity f , the moment generating function
MX(t) becomes the integral transform
‚à´etxf (x)dx.
(a)
Let ùúÜ> 0. If X has density f (x) = ùúÜe‚àíùúÜx
for x > 0, and 0 elsewhere, then X
has the exponential distribution with
parameter ùúÜand
MX(t)=‚à´
‚àû
0
ùúÜe‚àí(ùúÜ‚àít)xdx=
ùúÜ
ùúÜ‚àít , (t < ùúÜ).
(b)
If X has density f (x) = e‚àíx2‚àï2‚àï
‚àö
2ùúã,
x ‚àà‚Ñù, then X has the standard nor-
mal Óà∫(0, 1) or Gaussian distribution
and
MX(t) =
1
‚àö
2ùúã‚à´
‚àû
‚àí‚àû
et2‚àï2e‚àí(x‚àít)2‚àï2dx
= et2‚àï2,
(t ‚àà‚Ñù).
The normal distribution was in part so
named5) for its ubiquity in real data. It
is also very common in probability and
mathematical statistics, owing to a large
extent to results of the following kind.
Theorem 1.5 (de Moivre‚ÄìLaplace
central limit theorem) Let Xn ‚àºBin(n, p)
with
Ô¨Åxed
p ‚àà(0, 1).
Denote
X‚àó
n =
(Xn ‚àíùîº[Xn])‚àï
‚àö
ùïçar[Xn].
Then
for
any
t ‚àà‚Ñù,
MX‚àón(t) ‚Üíet2‚àï2, as n ‚Üí‚àû;
in other words, the distribution of X‚àó
n con-
verges to that of Óà∫(0, 1).
Proof. Recall that Xn can be written as
a sum Y1 + Y2 + ¬∑ ¬∑ ¬∑ + Yn of independent
Be(p) random variables.
In particular,
ùîº[Xn] = np and ùïçar[Xn] = np(1 ‚àíp) (see
Example 1.25). Using the simple relation
MaZ+b(t) = ebtMZ(at), we deduce that the
random variable ÃÇY = (Y ‚àíp)‚àï
‚àö
n has the
moment generating function (with Ô¨Åxed t)
e‚àítp‚àï
‚àö
n(
1 + p(et‚àï
‚àö
n ‚àí1))
= 1 + t2
2np(1 ‚àíp) + O
(
t3
‚àö
n
)
.
5) The term normal (in the sense of ‚Äúusual‚Äù) was
apparently attached to the distribution by Fran-
cis Galton and others and popularized by Karl
Pearson. The distribution arose in the work of
Gauss and Laplace on least squares and errors
of measurement, and also in Maxwell‚Äôs work
on statistical physics. Perhaps its Ô¨Årst tentative
appearance, however, is in the work of Abraham
de Moivre for whom Theorem 1.5 is named. See
[3] for a discussion.

10
1 Stochastic Processes
Noticing that MX‚àó
n(t)=(MÃÇY(t‚àï
‚àö
p(1 ‚àíp)))n
‚Üíet2‚àï2 as n ‚Üí‚àû, we deduce the result.
One other application of moment gener-
ating functions that we will see is to large
deviations: see Section 1.4.4.
1.2.3.2
Laplace Transforms
In general, MX(t) might be inÔ¨Ånite for
all t ‚â†0. However, for nonnegative vari-
ables X ‚â•0, we have MX(t) = ùîº[etX] ‚â§1,
for all t ‚â§0; in particular, MX(t) is an
analytic function at every point of the
complex plane with negative real part. In
this case, MX(t) behaves very similarly to
generating functions and inherits the main
properties described above. In such a sit-
uation, the function MX(t) (or, sometimes
MX(‚àít)) is called the Laplace transform
of the variable X. See Chapter 15 in the
present volume for background on Laplace
transforms.
1.2.3.3
Characteristic Functions
Unlike the moment generating function
MX(t), which might be inÔ¨Ånite for real
t ‚â†0, the characteristic function ùúìX(t) =
ùîº[eitX] (where i2 = ‚àí1) always exists and
uniquely identiÔ¨Åes the distribution, hence
the name.6) The characteristic functions
inherit all nice properties of (moment)
generating
functions,
though
inverting
them is not always straightforward.
Characteristic functions are the standard
tool of choice for proving results such as the
following generalization of Theorem 1.5.
The proof is similar to that of the previous
theorem, based on a Taylor-type formula: if
ùîº[X2n] < ‚àû, then
6) The term characteristic function is traditional
in the probabilistic context for what elsewhere
might be called the Fourier transform: see
Chapter 15 of the present volume.
ùúìX(t) =
2n
‚àë
ùìÅ=0
(it)ùìÅ
ùìÅ! ùîº[XùìÅ] + o(t2n).
Theorem 1.6 (Central limit theorem)
Let Xn = Y1 + Y2 + ¬∑ ¬∑ ¬∑ + Yn, where Yi are
i.i.d. random variables with ùîº[Y 2
i ] < ‚àû.
Then,
as
n ‚Üí‚àû,
the
distribution
of
X‚àó
n = (Xn ‚àíùîº[Xn])‚àï
‚àö
ùïçar[Xn] converges to
the standard normal, Óà∫(0, 1).
1.3
Markov Chains in Discrete Time
1.3.1
What is a Markov Chain?
Our tour begins with stochastic processes
in discrete time. Here, we will write our
process as X0, X1, X2, ‚Ä¶. A fundamental
class is constituted by the Markov pro-
cesses in which, roughly speaking, given
the present, the future is independent of
the past. In this section, we treat the case
where the Xn take values in a discrete (i.e.,
Ô¨Ånite or countably inÔ¨Ånite) state space
S. In this case, the general term Markov
process is often specialized to a Markov
chain, although the usage is not universally
consistent.
The process X = (Xn) taking values in the
discrete set S satisÔ¨Åes the Markov property
if, for any n and any i, j, ‚ààS,7)
‚Ñô[Xn+1 = j ‚à£Xn = i, Xn‚àí1 = in‚àí1, ‚Ä¶ ,
X0 = i0] = pij,
(1.12)
for all previous histories i0, ‚Ä¶ , in‚àí1 ‚ààS.
The pij = ‚Ñô[Xn+1 = j ‚à£Xn = i] are the
one-step transition probabilities for X, and
they satisfy the obvious conditions pij ‚â•0
for all i, j, and ‚àë
j‚ààS pij = 1 for all i. It is
convenient to arrange the pij as a matrix
P = (pij)i,j‚ààS with nonnegative entries and
7) In (1.12) and elsewhere, we indicate intersec-
tions of events by commas for readability.

1.3 Markov Chains in Discrete Time
11
whose rows all sum to 1: these properties
deÔ¨Åne a stochastic matrix. The Markov
property speciÔ¨Åes the step-by-step evolu-
tion of the Markov chain. One can imagine
a particle moving at random on S, from
state i selecting its next location according
to distribution (pij)j‚ààS. It is an exercise in
conditional probability to deduce from
(1.12) that
‚Ñô[X1 = i1, ‚Ä¶ , Xn = in ‚à£X0 = i0]
= pi0i1 ¬∑ ¬∑ ¬∑ pin‚àí1in.
It may be that X0 is itself random, in which
case, in order to assign a probability to
any particular (Ô¨Ånite) sequence of moves
for the particle, in addition to the Markov
property, we also need to know where the
particle starts: this is speciÔ¨Åed by the initial
distribution ‚Ñô[X0 = i] = wi, i ‚ààS.
Now, to compute the probability of get-
ting from i to k in two steps, we sum over
the j-partition to get
‚Ñô[X2 = k ‚à£X0 = i]
=
‚àë
j‚ààS
‚Ñô[X1 = j, X2 = k ‚à£X0 = i]
=
‚àë
j‚ààS
pijpjk = (P2)ik,
the (i, k) entry in P2 = P ‚ãÖP; this matrix
multiplication yields the two-step transi-
tion probability. More generally, the n-step
transition probabilities are
p(n)
ij ‚à∂= ‚Ñô[Xn = j ‚à£X0 = i] = (Pn)ij.
A similar argument shows the following:
p(n+m)
ij
=
‚àë
k‚ààS
p(n)
ik p(m)
kj .
(1.13)
Remark 1.5 We restrict ourselves to the
case of time-homogeneous Markov chains,
by stipulating that (1.12) should hold simul-
taneously for all n. One may relax this, and
allow pij to depend also on n.
Remark 1.6 An equivalent deÔ¨Ånition of a
Markov chain is as a randomized dynam-
ical system: Xn+1 = f (Xn; Un+1) where f ‚à∂
S √ó [0, 1] ‚ÜíS is a Ô¨Åxed update function,
and U1, U2, ‚Ä¶ are independent U[0, 1] ran-
dom variables.8) Monte Carlo simulation of
a Markov chain uses such a scheme.
We make a Ô¨Ånal remark on notation: for
pij, and similar expressions, we sometimes
write pi,j if otherwise the subscripts may be
ambiguous.
1.3.2
Some Examples
Example 1.9 [The Ehrenfest model] In
1907, Ehrenfest and Ehrenfest [4] intro-
duced this simple model of diÔ¨Äusion. There
are N particles in a container that has two
chambers separated by a permeable par-
tition. At each step, a particle is chosen
uniformly at random and moved across the
partition. The state of the Markov chain at
each time will be the number of particles
in the Ô¨Årst chamber, say, so S = {0, ‚Ä¶ , N}.
The one-step transition probabilities are,
for i ‚àà{1, ‚Ä¶ , N ‚àí1},
pi,i+1 = N ‚àíi
N
,
pi,i‚àí1 = i
N ,
and p0,1 = pN,N‚àí1 = 1.
After a long time, what is the distribution
of the particles? See Section 1.3.6.
Example 1.10 [One-dimensional
simple
random walk] A particle moves at random
on the state space S = ‚Ñ§+. From position
i ‚â†0, the particle jumps one step to the
left with probability pi and one step to the
right with probability 1 ‚àípi. With partial
reÔ¨Çection at 0, we can describe this random
8) That is, uniform on [0, 1], having density f (x) =
1 for x ‚àà[0, 1] and f (x) = 0 elsewhere.

12
1 Stochastic Processes
walk by a Markov chain with one-step
transition probabilities p0,0 = p0 ‚àà(0, 1),
p0,1 = q0 ‚à∂= 1 ‚àíp0, and for i ‚â•1,
pi,i‚àí1 = pi ‚àà(0, 1),
pi,i+1 = qi ‚à∂= 1 ‚àípi.
1.3.3
Stationary Distribution
We use the compact notation ‚Ñôi for the
(conditional) probability associated with
the Markov chain started from state i ‚ààS,
that is, ‚Ñôi[ ‚ãÖ] = ‚Ñô[ ‚ãÖ‚à£X0 = i]. More gener-
ally, if w = (wi)i‚ààS is a distribution on S (i.e.,
wi ‚â•0, ‚àë
i wi = 1), then we write ‚Ñôw for
the Markov chain started from the initial
distribution w, that is, ‚Ñôw[ ‚ãÖ] = ‚àë
i wi‚Ñôi[ ‚ãÖ].
A distribution ùúã= (ùúãi)i‚ààS is a stationary
distribution for a Markov chain X if
‚Ñôùúã[X1 = i] = ùúãi, for all i ‚ààS.
(1.14)
Viewing a stationary distribution ùúãas
a row vector, (1.14) is equivalent to the
matrix-vector equation ùúãP = ùúã, that is, ùúãis
a left eigenvector of P corresponding to the
eigenvalue 1. The nomenclature arises from
the fact that (1.14) implies that ‚Ñôùúã[Xn =
i] = ùúãi for all times n, so the distribution of
the Markov chain started according to ùúãis
stationary in time.
Example 1.11 [A three-state chain] Con-
sider a Markov chain (Xn) with the state
space {1, 2, 3} and transition matrix
P =
‚éõ
‚éú
‚éú
‚éú‚éù
1
2
1
2
0
1
3
1
3
1
3
0
1
2
1
2
‚éû
‚éü
‚éü
‚éü‚é†
.
(1.15)
We look for a stationary distribution
ùúã= (ùúã1, ùúã2, ùúã3). Now ùúãP = ùúã
with the
fact that ùúã1 + ùúã2 + ùúã3 = 1 gives a sys-
tem of equations with unique solution
ùúã= ( 2
7, 3
7, 2
7).
A Markov chain with transition probabil-
ities pij is reversible with respect to a dis-
tribution ùúã= (ùúãi)i‚ààS if the detailed balance
equations hold:
ùúãipij = ùúãjpji, for all i, j ‚ààS.
(1.16)
Not every Markov chain is reversible. Any
distribution ùúãsatisfying (1.16) is necessar-
ily a stationary distribution, because then,
for all j, ‚àë
i ùúãipij = ‚àë
i ùúãjpji = ùúãj. If the chain
is reversible, then the system of equations
(1.16) is often simpler to solve than the
equations ùúãP = ùúã: see Example 1.12. The
‚Äúphysical‚Äù interpretation of reversibility is
that, in equilibrium, the Markov chain is
statistically indistinguishable from a copy
of the chain running backward in time.
Example 1.12 [Random
walk]
Con-
sider Example 1.10. We seek a solution
ùúã= (ùúãi)
to
(1.16),
which
now
reads
ùúãiqi = ùúãi+1pi+1 for all i ‚â•0. The solution
is ùúãi = ùúã0
‚àèi‚àí1
j=0(qj‚àïpj+1). This describes a
proper distribution if ‚àë
i ùúãi = 1, that is, if
‚àû
‚àë
i=0
i‚àí1
‚àè
j=0
qj
pj+1
< ‚àû.
(1.17)
If (1.17) holds, then
ùúãi =
‚àèi‚àí1
j=0
qj
pj+1
‚àë‚àû
i=0
‚àèi‚àí1
j=0
qj
pj+1
.
For example, if pi = p ‚àà(0, 1) for all i, then
(1.17) holds if and only if p > 1‚àï2, in which
case ùúãi = (q‚àï(1 ‚àí2p))(q‚àïp)i, where q = 1 ‚àí
p, an exponentially decaying stationary dis-
tribution.
1.3.4
The Strong Markov Property
One way of stating the Markov property
is to say that (Xn)n‚â•m, conditional on

1.3 Markov Chains in Discrete Time
13
{Xm = i}, is distributed as the Markov
chain (Xn)n‚â•0 with initial state X0 = i. It is
often desirable to extend such a statement
from deterministic times m to random
times T. An important class of random
times are the Ô¨Årst passage times,9)
Ti ‚à∂= min{n ‚â•1 ‚à∂Xn = i}, i ‚ààS.
(1.18)
The Markov property cannot hold at
every
random
time.
For
instance,
if
T‚Ä≤ = Ti ‚àí1, then the Ô¨Årst transition of
the process XT‚Ä≤, XT‚Ä≤+1, ‚Ä¶ is always from
XT‚Ä≤ to XT‚Ä≤+1 = i, regardless of the original
transition matrix.
The following strong Markov property
clariÔ¨Åes these issues. A random time T ‚àà
‚Ñ§+ ‚à™{‚àû} is a stopping time with respect to
(Xn) if, for any n, the event {T ‚â§n} depends
only on X0, ‚Ä¶ , Xn (and not on the future
evolution of the chain). The passage times
Ti are stopping times, but T‚Ä≤ described
above is not a stopping time.
Lemma 1.2 Suppose that T is a stopping
time for (Xn). Then, given T < ‚àûand XT =
i, (XT+n)n‚â•0 has the same distribution as
(Xn)n‚â•0 started from X0 = i.
Sketch of proof
Partition over the possi-
ble values of T. Suppose that T = m and
XT = Xm = i; this is a condition only on
X0, ‚Ä¶ , Xm, because T is a stopping time.
Now apply the usual Markov property at
the deterministic time m.
1.3.5
The One-Step Method
In problems involving Markov chains,
often quantities of interest are hitting prob-
abilities and expected hitting times. One
approach to computing these is via the
9) Here and elsewhere, the convention min ‚àÖ= ‚àû
is in force.
powerful one-step method, which makes
essential use of the Markov property.
Recall the deÔ¨Ånition of the passage times
Ti from (1.18). The expected hitting time of
state j starting from state i is ùîºi[Tj] for i ‚â†j;
if i = j this is the expected return time to i.
Also of interest is ‚Ñôi[Tj < Tk], the probabil-
ity of reaching state j before state k, starting
from i. We illustrate the one-step method
by some examples.
Example 1.13 [Three-state
chain]
We
return to Example 1.11. We partition over
the Ô¨Årst step of the process to obtain, via the
law of total probability (see Section 1.A),
‚Ñô2[T1 < T3] =
3
‚àë
k=1
‚Ñô2[{T1 < T3} ‚à©{X1 = k}]
= p2,1 ‚ãÖ1 + p2,2 ‚ãÖ‚Ñô2[T1 < T3]
+ p2,3 ‚ãÖ0,
by the Markov property. This gives ‚Ñô2[T1 <
T3] = 1‚àï2.
What about ùîº2[T1]? Set zi = ùîºi[T1].
Again we condition on the Ô¨Årst step,
and now use the partition theorem for
expectations (see Section 1.A):
ùîº2[T1] = 1 + ùîº2[T1 ‚àí1]
= 1 +
3
‚àë
k=1
p2,kùîº2[T1 ‚àí1 ‚à£X1 = k].
Now applying the Markov property at time
1, we see that T1 ‚àí1, given X0 ‚â†1 and
X1 = k ‚â†1, has the same distribution as
T1 given X0 = k ‚â†1 in the original chain,
and, in particular, has expected value zk.
On the other hand, if X1 = k = 1, then
T1 ‚àí1 = 0. So we get z2 = 1 + 1
3z2 + 1
3z3. A
similar argument starting from state 3 gives
z3 = 1 + 1
2z2 + 1
2z3. This system of linear
equations is easily solved to give z2 = 5 and
z3 = 7.

14
1 Stochastic Processes
Example 1.14 [Random
walk;
gambler‚Äôs
ruin] Recall Example 1.10. Fix n ‚àà‚Ñïand
for i ‚àà{1, ‚Ä¶ , n ‚àí1}, let ui = ‚Ñôi[Tn < T0].
The one-step method gives
ui = piui‚àí1 + qiui+1,
(1 ‚â§i ‚â§n ‚àí1),
with boundary conditions u0 = 0, un = 1.
The standard method to solve this sys-
tem of equations is to rewrite it in
terms of the diÔ¨Äerences Œîi = ui+1 ‚àíui
to get Œîi = Œîi‚àí1(pi‚àïqi) for 1 ‚â§i ‚â§n ‚àí1,
which yields Œîj = Œî0
‚àèj
k=1(pk‚àïqk). Then
ui = ‚àëi‚àí1
j=0 Œîj, using the boundary condition
at 0. Using the boundary condition at n to
Ô¨Åx Œî0, the solution obtained is
ui =
‚àëi‚àí1
j=0
‚àèj
k=1
pk
qk
‚àën‚àí1
j=0
‚àèj
k=1
pk
qk
.
(1.19)
In the special case where pi = qi = 1‚àï2 for
all i, we have the elegant formula ui = i‚àïn.
If we imagine that the state of the Markov
chain is the wealth of a gambler with initial
wealth i who plays a sequence of fair games,
each time either gaining or losing a unit of
wealth, 1 ‚àíui is the ruin probability (and
ui is the probability that the gambler makes
his fortune).
1.3.6
Further Computational Methods
We present by example some additional
techniques.
Example 1.15 [Matrix diagonalization] In
many situations, we want to compute the
n-step transition probability p(n)
ij , that is, an
entry in the matrix power Pn. To calcu-
late Pn, we try to diagonalize P to obtain
P = TŒõT‚àí1 for an invertible matrix T and
a diagonal matrix Œõ. The usefulness of this
representation is that Pn = TŒõnT‚àí1 and Œõn
is easy to write down, because Œõ is diagonal.
A suÔ¨Écient condition for P to be diagonal-
izable is that all its eigenvalues be distinct.
Consider again the three-state chain with
transition matrix given by (1.15); we have
three eigenvalues, ùúÜ1, ùúÜ2, ùúÜ3, say. As P is a
stochastic matrix, 1 is always an eigenvalue:
ùúÜ1 = 1, say. Then because tr P = ùúÜ1 + ùúÜ2 +
ùúÜ3 and det P = ùúÜ1ùúÜ2ùúÜ3, we Ô¨Ånd ùúÜ2 = 1‚àï2
and ùúÜ3 = ‚àí1‚àï6, say.
It follows from the diagonalized repre-
sentation that
Pn = ùúÜn
1U1 + ùúÜn
2U2 + ùúÜn
3U3
= U1 +
(1
2
)n
U2 +
(
‚àí1
6
)n
U3,
(1.20)
where U1, U2, U3 are 3 √ó 3 matrices to be
determined. One can solve the simultane-
ous matrix equations arising from the cases
n ‚àà{0, 1, 2} of (1.20) to obtain U1, U2, and
U3, and hence,
Pn =
‚éõ
‚éú
‚éú
‚éú‚éù
2
7
3
7
2
7
2
7
3
7
2
7
2
7
3
7
2
7
‚éû
‚éü
‚éü
‚éü‚é†
+
(1
2
)n ‚éõ
‚éú
‚éú
‚éú‚éù
1
2
0
‚àí1
2
0
0
0
‚àí1
2
0
1
2
‚éû
‚éü
‚éü
‚éü‚é†
+
(
‚àí1
6
)n ‚éõ
‚éú
‚éú
‚éú‚éù
3
14
‚àí3
7
3
14
‚àí2
7
4
7
‚àí2
7
3
14
‚àí3
7
3
14
‚éû
‚éü
‚éü
‚éü‚é†
.
It follows that limn‚Üí‚àûp(n)
ij
exists, does
not depend on i, and is equal to ùúãj, the
component of the stationary distribu-
tion that we calculated in Example 1.11.
After a long time, the chain ‚Äúforgets‚Äù its
starting state and approaches a stochastic
equilibrium described by the stationary
distribution. This is an example of a gen-
eral phenomenon to which we return in
Section 1.3.7.
Example 1.16 [Generating functions] We
sketch the use of generating functions to

1.3 Markov Chains in Discrete Time
15
evaluate stationary distributions. Consider
the
Ehrenfest
model
of
Example 1.9.
Suppose that ùúã= (ùúã0, ‚Ä¶ , ùúãN) is a station-
ary distribution for the Markov chain, with
generating function
ÃÇùúã(s) = ‚àëN
i=0 ùúãisi. In
this case, the equation ùúãP = ùúãreads
ùúãj‚àí1
N ‚àí(j ‚àí1)
N
+ ùúãj+1
j + 1
N
= ùúãj,
which is valid for all j ‚àà{0, ‚Ä¶ , N}, pro-
vided we set ùúã‚àí1 = ùúãN+1 = 0. Now multiply
through by sj and sum from j = 0 to N. After
some algebra, we obtain
ÃÇùúã(s) = 1 ‚àís2
N
ÃÇùúã‚Ä≤(s) + s ÃÇùúã(s),
so that d‚àïds log ÃÇùúã(s) = ÃÇùúã‚Ä≤(s)‚àïÃÇùúã(s) = N‚àï(1 +
s). Integrating with respect to s and using
the fact that ÃÇùúã(1) = 1, we obtain
ÃÇùúã(s) =
(1 + s
2
)N
.
The binomial theorem now enables us to
identify ùúãi = 2‚àíN(N
i
).
1.3.7
Long-term Behavior; Irreducibility;
Periodicity
We saw in Example 1.15 a Markov chain for
which
lim
n‚Üí‚àû‚Ñôi[Xn = j] = lim
n‚Üí‚àûp(n)
ij = ùúãj,
(1.21)
for all i, j ‚ààS, where ùúãj is from a stationary
distribution. For which Markov chains does
such a result hold? There are (at least) three
obstacles:
(a)
There might be no solutions to ùúãP =
ùúã, and hence, no right-hand side in
(1.21).
(b)
There might be multiple solutions
to ùúãP = ùúã, and so no uniqueness
in (1.21). For example, consider the
Markov chain on the state space
{0, 1, 2} with p00 = 1, p22 = 1 (0
and 2 are absorbing states) and
p10 = p12 = 1‚àï2. Then p(n)
i2 = i‚àï2 for
all n ‚â•1, that is, the limit on the
left-hand side of (1.21) depends on
the starting state i. Note that here
ùúã= (ùõº, 0, 1 ‚àíùõº) is stationary for any
ùõº‚àà[0, 1].
(c)
In
the
Ehrenfest
model
of
Example 1.9, there is a parity eÔ¨Äect,
because
p(n)
00 = 0
for
odd
n,
for
instance. This phenomenon is an
example
of
periodicity,
which
is
another obstacle to (1.21).
Cases (b) and (c) here can be dealt with
after some additional concepts are intro-
duced. A state i ‚ààS has period d if d is the
greatest common divisor of {n ‚â•1 ‚à∂p(n)
ii >
0}. For example, all states in the Ehrenfest
model have period 2.
A Markov chain is irreducible if, for all
i, j ‚ààS, there exist Ô¨Ånite m and n for which
p(n)
ij > 0 and p(m)
ji
> 0, that is, it is possible
to get between any two states in a Ô¨Ånite
number of steps. For the rest of this section,
we will assume that we have an irreducible
Markov chain. We do not discuss the case
of nonirreducible (reducible) chains in a
systematic way, but Section 1.3.10 provides
an illustrative example.
For an irreducible chain, it can be shown
that all states have the same period, in
which case one can speak about the period
of the chain itself. If all states have period 1,
the chain is called aperiodic.
Recall the deÔ¨Ånition of Ti from (1.18):
ùîºi[Ti] is the expected return time to i. The
following result answers our question on
the limiting behavior of p(n)
ij .
Theorem 1.7 For an irreducible Markov
chain, the following are equivalent.

16
1 Stochastic Processes
‚Ä¢ There exists a unique stationary
distribution ùúã.
‚Ä¢ For some i ‚ààS, ùîºi[Ti] < ‚àû.
‚Ä¢ For all i ‚ààS, ùîºi[Ti] < ‚àû.
If these conditions hold, the Markov chain
is called positive recurrent. For a positive-
recurrent chain, the following hold.
‚Ä¢ For all i ‚ààS, ùúãi = 1‚àïùîºi[Ti].
‚Ä¢ If the chain is aperiodic, then
‚Ñôi[Xn = j] ‚Üíùúãj for all i, j ‚ààS.
In particular, we have the following
result.
Theorem 1.8 An irreducible Markov chain
on a Ô¨Ånite state space is positive recurrent.
Proofs of these results can be found in
[5, 6], for instance.
1.3.8
Recurrence and Transience
Recall the deÔ¨Ånition of Ti from (1.18). A
state i ‚ààS is called recurrent if ‚Ñôi[Ti <
‚àû] = 1 or transient if ‚Ñôi[Ti = ‚àû] > 0. A
Markov chain will return inÔ¨Ånitely often to
a recurrent state, but will visit a transient
state only Ô¨Ånitely often. If a Markov chain is
irreducible (see Section 1.3.7), then either
all states are recurrent, or none is, and so
we can speak of recurrence or transience of
the chain itself.
If an irreducible chain is positive recur-
rent (see Theorem 1.7), then it is neces-
sarily recurrent. A chain that is recurrent
but not positive recurrent is null recurrent,
in which case, for all i, ‚Ñôi[Ti < ‚àû] = 1 but
ùîºi[Ti] = ‚àû(equivalently, it is recurrent but
no stationary distribution exists). Because
of Theorem 1.8, we know that to observe
null recurrence or transience we must look
at inÔ¨Ånite state spaces.
Example 1.17 [One-dimensional random
walk] We return to Example 1.10. Consider
‚Ñô0[T0 = ‚àû]. In order for the walk to never
return to 0, the Ô¨Årst step must be to 1, and
then, starting from 1, the walk must reach
n before 0 for every n ‚â•2. Thus
‚Ñô0[T0 = ‚àû] = q0‚Ñô1[T0 = ‚àû]
= q0‚Ñô1[‚à©n‚â•2{Tn < T0}].
Note
{Tn+1 < T0} ‚äÜ{Tn < T0},
so
the
intersection here is over a decreasing
sequence of events. Thus by continuity
of probability measures (see Section 1.A),
‚Ñô0[T0 = ‚àû] = q0 limn‚Üí‚àû‚Ñô1[Tn < T0].
Here ‚Ñô1[Tn < T0] = 1 ‚àíu1 where u1 is
given by (1.19). So we obtain
‚Ñô0[T0 = ‚àû] > 0 if and only if
‚àû
‚àë
j=0
j‚àè
k=1
(pk
qk
)
< ‚àû.
(For further discussion, see [7, pp. 65‚Äì71])
In particular, if pk = p ‚àà(0, 1) for all k,
the walk is transient if p < 1‚àï2 and recur-
rent if p ‚â•1‚àï2. The phase transition can
be probed more precisely by taking pk =
1‚àï2 + c‚àïk; in this case, the walk is transient
if and only if c < ‚àí1‚àï4, a result due to Har-
ris and greatly generalized by Lamperti [8].
We give one criterion for recurrence that
we will use in Section 1.4.
Lemma 1.3 For any i ‚ààS, ‚Ñôi[Ti < ‚àû] = 1
if and only if ‚àë‚àû
n=0 p(n)
ii = ‚àû.
We give a proof via generating functions.
Write f (n)
i
= ‚Ñôi[Ti = n], the probability that
the Ô¨Årst return to i occurs at time n; here
f (0)
i
= 0; note that ‚àë
n f (n)
i
may be less than
1. Denote the corresponding generating
function by ùúôi(s) = ‚àë‚àû
n=0 f (n)
i
sn. Also deÔ¨Åne
ùúìi(s) = ‚àë‚àû
n=0 p(n)
ii sn, where p(n)
ii = ‚Ñôi[Xn = i]
(so
p(0)
ii = 1).
By
conditioning
on
the

1.3 Markov Chains in Discrete Time
17
value of Ti, the strong Markov property
gives
p(n)
ii =
n
‚àë
m=0
f (m)
i
p(n‚àím)
ii
, (n ‚â•1).
Treating the case n = 0 carefully, it follows
that
ùúìi(s) = 1 +
‚àû
‚àë
n=0
n
‚àë
m=0
f (m)
i
p(n‚àím)
ii
sn.
The Ô¨Ånal term here is a discrete con-
volution
of
the
generating
function
(cf. Theorem 1.2),
so
we
deduce
the
important renewal relation
ùúìi(s) = 1 + ùúôi(s)ùúìi(s).
(1.22)
Sketch of proof of Lemma 1.3
We have
‚Ñôi[Ti < ‚àû] = lims‚Üë1 ùúôi(s),
and
(1.22)
implies that the latter limit is 1 if and
only if lims‚Üë1 ùúìi(s) = ‚àû.
1.3.9
Remarks on General State Spaces
In the case of discrete state spaces, (1.13)
corresponds to the trivial matrix equation
Pn+m = Pn ‚ãÖPm, which one could describe,
rather grandly, as the semigroup property
of matrix multiplication. More generally,
(1.13) is an instance of the fundamental
Chapman‚ÄìKolmogorov relation, and the
connection
to
semigroup
theory
runs
deep.
In a general state space, the analogue of
the transition probability pij is a transition
kernel p(x; A) given by p(x; A) = ‚Ñô[Xn+1 ‚àà
A ‚à£Xn = x]. This immediately introduces
technical issues that can only be addressed
in the context of measure theory. We refer
to [2, 9], for example.
1.3.10
Example: Bak‚ÄìSneppen and Related
Models
Bak and Sneppen [10] introduced a sim-
ple stochastic model of evolution that
initiated a considerable body of research
by
physicists
and
mathematicians.
In
the original model, N sites are arranged
in a ring. Each site, corresponding to a
species in the evolution model, is initially
assigned an independent U[0, 1] random
variable representing a ‚ÄúÔ¨Åtness‚Äù value for
the species. The Bak‚ÄìSneppen model is
a discrete-time Markov process, where at
each step the minimal Ô¨Åtness value and
the values at the two neighboring sites
are replaced by three independent U[0, 1]
random variables.
This process is a Markov process on
the continuous state space [0, 1]N, and
its behavior is still not fully understood,
despite a large physics literature devoted
to these models: see the thesis [11] for an
overview of the mathematical results.
Here we treat a much simpler model, fol-
lowing [12]. The state space of our pro-
cess (Xn) will be the ‚Äúsimplex‚Äù of ranked
sequence of N Ô¨Åtness values
ŒîN ‚à∂= {(x(1), ‚Ä¶ , x(N)) ‚àà[0, 1]N ‚à∂
x(1) ‚â§¬∑ ¬∑ ¬∑ ‚â§x(n)}.
Fix a parameter k ‚àà{1, ‚Ä¶ , N}. We start
with N independent U[0, 1] values: rank
these to get X0. Given Xn, discard the
kth-ranked value X(k)
n
and replace it by a
new independent U[0, 1] random variable;
rerank to get Xn+1.
For example, if k = 1, we replace the min-
imal value at each step. It is natural to antic-
ipate that Xn should approach (as n ‚Üí‚àû)
a limiting (stationary) distribution; observe
that the value of the second-ranked Ô¨Åtness
cannot decrease. A candidate limit is not

18
1 Stochastic Processes
hard to come by: the distribution of the ran-
dom vector (U, 1, 1, 1, ‚Ä¶ , 1) (a U[0, 1] vari-
able followed by N ‚àí1 units) is invariant
under the evolution of the Markov chain.
We show the following result.
Proposition 1.1 Let
N ‚àà‚Ñï
and
k ‚àà{1, 2, ‚Ä¶ , N}. If at each step we replace
the kth-ranked value by an independent
U[0, 1] value, then, as n ‚Üí‚àû,
(X(1)
n , X(2)
n , ‚Ä¶ , X(N)
n ) ‚Üí(0, ‚Ä¶ , 0, U, 1, ‚Ä¶ , 1),
in distribution,10) where the kth coordinate
of the limit vector U ‚àºU[0, 1].
The process Xn lives on a continuous
state space, and it might seem that some
fairly sophisticated argument would be
needed to show that it has a unique sta-
tionary distribution. In fact, we can reduce
the problem to a simpler problem on a
Ô¨Ånite state space as follows.
Sketch of proof of Proposition 1.1
We
sketch the argument from [12]. For each
s ‚àà[0, 1], deÔ¨Åne the counting function11)
Cn(s) ‚à∂= ‚àëN
i=1 1{X(i)
n ‚â§s},
the
number
of Ô¨Åtnesses of value at most s at time
n. Then Cn(s) is a Markov chain on
{0, 1, 2, ‚Ä¶ , N}. The transition probabil-
ities px,y = ‚Ñô[Cn+1(s) = y ‚à£Cn(s) = x] are
given for x ‚àà{0, ‚Ä¶ , k ‚àí1} by px,x = 1 ‚àís
and px,x+1 = s, and for x ‚àà{k, ‚Ä¶ , N} by
px,x = s and px,x‚àí1 = 1 ‚àís. For s ‚àà(0, 1),
the Markov chain is reducible and all
states are transient apart from those in
the recurrent class Sk = {k ‚àí1, k}. The
chain will eventually enter Sk and then
never exit. So the problem reduces to
10) That is, for any x1, ‚Ä¶ , xN ‚àà[0, 1],
‚Ñô[X(1)
n
‚â§x1, ‚Ä¶ , X(k)
n
‚â§xk, ‚Ä¶ , X(N)
n
‚â§xN] ‚Üíxk if
xk+1 ¬∑ ¬∑ ¬∑ xN = 1 and 0 otherwise.
11) ‚Äú1{ ‚ãÖ}‚Äù is the indicator random variable of the
appended event: see Section 1.A.
that of the two-state restricted chain on
Sk. It is easy to compute the stationary
distribution and for s ‚àà(0, 1), analogously
to Theorem 1.7,
lim
n‚Üí‚àû‚Ñô[Cn(s) = x]=
‚éß
‚é™
‚é®
‚é™‚é©
1 ‚àís if x = k ‚àí1
s
if x = k
0
if n ‚àâ{k ‚àí1, k}
.
In particular, for s ‚àà(0, 1),
lim
n‚Üí‚àû‚Ñô[X(k)
n
‚â§s] = lim
n‚Üí‚àû‚Ñô[Cn(s) ‚â•k] = s.
That is, X(k)
n
converges in distribution to
a U[0, 1] variable. Moreover, if k > 1, for
any
s ‚àà(0, 1),
‚Ñô[X(k‚àí1)
n
‚â§s] = ‚Ñô[Cn(s) ‚â•
k ‚àí1] ‚Üí1,
which
implies
that
X(k‚àí1)
n
converges in probability to 0. Similarly,
if k < N, for any s ‚àà(0, 1), ‚Ñô[X(k+1)
n
‚â§
s] = ‚Ñô[Cn(s) ‚â•k + 1] ‚Üí0, which implies
that X(k+1)
n
converges in probability to 1.
Combining these marginal results, an addi-
tional technical step gives the claimed joint
convergence: we refer to [12] for details.
1.4
Random Walks
A drunk man will eventually Ô¨Ånd his
way home, but a drunk bird may get
lost for ever.
‚Äì S. Kakutani‚Äôs rendering of P√≥lya‚Äôs
theorem [1, p. 191].
1.4.1
Simple Symmetric Random Walk
The term random walk can refer to many
diÔ¨Äerent models or classes of models.
Although random walks in one dimen-
sion had been studied in the context of
games of chance, serious study of random

1.4 Random Walks
19
walks as stochastic processes emerged in
pioneering works in several branches of
science around 1900: Lord Rayleigh‚Äôs [13]
theory of sound developed from about
1880, Bachelier‚Äôs [14] 1900 model of stock
prices, Pearson and Blakeman‚Äôs [15] 1906
theory of random migration of species, and
Einstein‚Äôs [16] theory of Brownian motion
developed during 1905‚Äì1908.
In this section, we restrict attention to
simple symmetric random walk on the inte-
ger lattice ‚Ñ§d. This model had been consid-
ered by Lord Rayleigh, but the preeminent
early contribution came from George P√≥lya
[17]: we describe his recurrence theorem
in the following text. The phrase ‚Äúrandom
walk‚Äù was Ô¨Årst applied by statistical pio-
neer Pearson [18] to a diÔ¨Äerent model in a
1905 letter to Nature. We refer to [19] for
an overview of a variety of random walk
models.
Let e1, ‚Ä¶ , ed be the standard orthonor-
mal lattice basis vectors for ‚Ñ§d. Let
X1, X2, ‚Ä¶ be i.i.d. random vectors with
‚Ñô[X1 = ei] = ‚Ñô[X1 = ‚àíei]
= 1
2d , for i ‚àà{1, ‚Ä¶ , d}.
Let S0 = 0 and Sn = ‚àën
i=1 Xi. Then (Sn)n‚àà‚Ñ§+
is a simple symmetric random walk on ‚Ñ§d,
started from 0; ‚Äúsimple‚Äù refers to the fact
that the jumps are of size 1.
1.4.2
P√≥lya‚Äôs Recurrence Theorem
Clearly (Sn) is a Markov chain; a fundamen-
tal question is whether it is recurrent or
transient (see Section 1.3.8). P√≥lya [17] pro-
vided the answer in 1921.
Theorem 1.9 (P√≥lya‚Äôs theorem) A simple
symmetric random walk on ‚Ñ§d is recurrent
if d = 1 or 2 but transient if d ‚â•3.
A basic component in the proof is a com-
binatorial statement.
Lemma 1.4 For d ‚àà‚Ñïand any n ‚àà‚Ñ§+, we
have
‚Ñô[S2n = 0] = (2d)‚àí2n
(
2n
n
)
‚àë
n1+¬∑¬∑¬∑+nd=n
(
n!
n1! ¬∑ ¬∑ ¬∑ nd!
)2
,
where the sum is over d-tuples of nonnega-
tive integers n1, ‚Ä¶ , nd that sum to n.
Proof. Each path of length 2n (i.e., the pos-
sible trajectory for S0, S1, ‚Ä¶ , S2n) has prob-
ability (2d)‚àí2n. Any such path that Ô¨Ånishes
at its starting point must, in each coordi-
nate i, take the same number ni steps in the
positive and negative directions. Enumerat-
ing all such paths, we obtain
‚Ñô[S2n = 0] = (2d)‚àí2n
‚àë
n1+¬∑¬∑¬∑+nd=n
(2n)!
(n1! ¬∑ ¬∑ ¬∑ nd!)2 ,
from which the given formula follows.
Lemma 1.4 and a careful asymptotic
analysis using Stirling‚Äôs formula for n!
yields the following result.
Lemma 1.5 For d ‚àà‚Ñï, as n ‚Üí‚àû,
nd‚àï2‚Ñô[S2n = 0] ‚Üí
(
d
4ùúã
)d‚àï2
.
Proof of Theorem 1.9 Apply the criterion in
Lemma 1.3 with Lemma 1.5.
1.4.3
One-dimensional Case; ReÔ¨Çection
Principle
We consider in more detail the case
d = 1.
Let
Ta ‚à∂= min{n ‚â•1 ‚à∂Sn = a}.
Theorem 1.9 says that ‚Ñô[T0 < ‚àû] = 1. The
next result gives the distribution of T0.

20
1 Stochastic Processes
Theorem 1.10
(i)
For
any
n ‚àà‚Ñ§+,
‚Ñô[T0 = 2n] = (1‚àï(2n ‚àí1))(2n
n
)2‚àí2n.
(ii) ùîº[Tùõº
0 ] < ‚àûif and only if ùõº< 1‚àï2.
We proceed by counting sample paths,
following the classic treatment by Feller
[20, chap. 3]. By an n-path we mean
a sequence of integers s0, ‚Ä¶ , sn where
|si+1 ‚àísi| = 1; for an n-path from a to b we
add the requirement that s0 = a and sn = b.
We view paths as space‚Äìtime trajectories
(0, s0), (1, s1), ‚Ä¶ , (n, sn).
Let Nn(a, b) denote the number of n-
paths from a to b. Let N0
n(a, b) be the num-
ber of such paths that visit 0. An n-path
from a to b must take (n + b ‚àía)‚àï2 posi-
tive steps and (n + a ‚àíb)‚àï2 negative steps,
so
Nn(a, b) =
(
n
1
2(n + b ‚àía)
)
,
(1.23)
where we interpret (n
y
) as 0 if y is not an
integer in the range 0 to n.
Lemma 1.6 (ReÔ¨Çection principle) If
a, b > 0, then N0
n(a, b) = Nn(‚àía, b).
Proof. Each n-path from ‚àía to b must visit
0 for the Ô¨Årst time at some c ‚àà{1, ‚Ä¶ , n ‚àí
1}. ReÔ¨Çect in the horizontal (time) axis the
segment of this path over [0, c] to obtain
an n-path from a to b which visits 0. This
reÔ¨Çection is one-to-one.
Theorem 1.11 (Ballot theorem) If b > 0,
then the number of n-paths from 0 to b
which do not revisit 0 is b
nNn(0, b).
Proof. The Ô¨Årst step of such a path must
be 1, so their number is Nn‚àí1(1, b) ‚àí
N0
n‚àí1(1, b) = Nn‚àí1(1, b) ‚àíNn‚àí1(‚àí1, b),
by
Lemma 1.6. Now use (1.23).
Theorem 1.12 If b ‚â†0 and n ‚â•1, ‚Ñô[T0 >
n, Sn = b] = |b|
n ‚Ñô[Sn = b].
Proof. Suppose b > 0. The event in ques-
tion occurs if and only if the walk does not
visit 0 during [1, n], and Sn = b. By the bal-
lot theorem, the number of such paths is
b
nNn(0, b). Similarly for b < 0.
At this point, we are ready to prove
Theorem 1.10, but Ô¨Årst we take a slight
detour to illustrate one further variation on
‚ÄúreÔ¨Çection.‚Äù
Theorem 1.13 For
a ‚â†0
and
n ‚â•1,
‚Ñô[Ta = n] = |a|
n ‚Ñô[Sn = a].
Proof via time reversal. Fix n. If the trajec-
tory of the original walk up to time n is
(S0, S1, S2, ‚Ä¶ , Sn)
=
(
0, X1, X1 + X2, ‚Ä¶ ,
n
‚àë
i=1
Xi
)
,
then the trajectory of the reversed walk is
(R0, R1, R2, ‚Ä¶ , Rn)
=
(
0, Xn, Xn + Xn‚àí1, ‚Ä¶ ,
n
‚àë
i=1
Xi
)
,
that is, the increments are taken in reverse
order. The reversed walk has the same
distribution as the original walk, because
the Xi are i.i.d.
Suppose a > 0. The original walk has
Sn = a and T0 > n if and only if the reversed
walk has Rn = a and Rn ‚àíRn‚àíi = X1 + ¬∑ ¬∑ ¬∑ +
Xi > 0 for all i ‚â•1, that is, the Ô¨Årst visit of
the reversed walk to a happens at time n. So
‚Ñô[Ta = n] = ‚Ñô[T0 > n, Sn = a]. Now apply
Theorem 1.12.
Proof of Theorem 1.10
If T0 = 2n, then
S2n‚àí1 = ¬±1. Thus
‚Ñô[T0 = 2n] = ‚Ñô[T = 2n, S2n‚àí1 = 1]
+ ‚Ñô[T = 2n, S2n‚àí1 = ‚àí1]

1.4 Random Walks
21
= 1
2‚Ñô[T0 > 2n ‚àí1, S2n‚àí1 = 1]
+1
2‚Ñô[T0 >2n ‚àí1, S2n‚àí1 =‚àí1].
Now by Theorem 1.12,
‚Ñô[T0 = 2n] = 1
2 ‚ãÖ
1
2n ‚àí1
(‚Ñô[S2n‚àí1 = 1] + ‚Ñô[S2n‚àí1 = ‚àí1]) ,
and part (i) of the theorem follows from
(1.23), after simpliÔ¨Åcation. For part (ii), we
have that ùîº[Tùõº
0 ] = ‚àë‚àû
n=1(2n)ùõº‚Ñô[T0 = 2n],
and Stirling‚Äôs formula shows that the sum-
mand here is asymptotically a constant
times nùõº‚àí(3‚àï2).
Remark 1.7
(i)
An alternative approach
to Theorem 1.10 is via the remarkable
identity
‚Ñô[T0 > 2n] = ‚Ñô[S2n = 0],
which can be veriÔ¨Åed by a direct but
more
sophisticated
combinatorial
argument: see, for example, [21].
(ii) Yet another approach uses generating
functions. For Sn,
ùúì(s) ‚à∂= ùîº[sSn] =
‚àû
‚àë
n=0
s2n
(
2n
n
)
2‚àí2n
=
1
‚àö
1 ‚àís2
,
by
(1.23)
and
then
Maclaurin‚Äôs
theorem.
Then
if
ùúô
is
the
gen-
erating
function
for
T0,
we
can
exploit the renewal relation ùúì(t) =
1 + ùúô(t)ùúì(t) (see (1.22)) to obtain
ùúô(s) = 1 ‚àí
‚àö
1 ‚àís2, from which we
can deduce Theorem 1.10 once more.
1.4.4
Large Deviations and Maxima of Random
Walks
In this section, we consider more general
one-dimensional random walks in order
to illustrate some further concepts. Again
we take Sn = ‚àën
i=1 Xi where the Xi are
i.i.d., but now the distribution of Xi will
be arbitrary subject to the existence of
the mean ùîº[Xi] = ùúá. Suppose that ùúá< 0.
The strong law of large numbers shows
that n‚àí1Sn ‚Üíùúá, almost surely, as n ‚Üí‚àû.
So if ùúá< 0, then Sn will tend to ‚àí‚àû, and
in particular the maximum of the walk
M = maxn>0 Sn is well deÔ¨Åned.
There are many applications for the
study of M, for example, the modeling
of queues (see [22]). What properties
does the random variable M possess?
We might want to Ô¨Ånd ‚Ñô[M > x], for
any x, but it is often diÔ¨Écult to obtain
exact
results;
instead
we
attempt
to
understand the asymptotic behavior as
x ‚Üí‚àû.
Let ùúë(t) = ùîº[etX1] be the moment gen-
erating function of the increments. It can
be shown that the behavior of ‚Ñô[M > x]
depends on the form of ùúë. Here we con-
sider only the classical (light-tailed) case in
which there exists ùõæ> 0 such that ùúë(ùõæ) =
1 and ùúë‚Ä≤(ùõæ) < ‚àû. For details of the other
cases, see [23].
First, Boole‚Äôs inequality (see Section 1.A)
gives
‚Ñô[M > x]=‚Ñô[‚à™‚àû
n=1{Sn >x}] ‚â§
‚àû
‚àë
n=1
‚Ñô[Sn > x].
(1.24)
Now the ChernoÔ¨Äbound (see Section 1.A)
implies that, for any ùúÉ‚àà[0, ùõæ],
‚Ñô[Sn > x] ‚â§e‚àíùúÉxùîº[eùúÉSn] = e‚àíùúÉx(ùúë(ùúÉ))n;
cf. Example 1.4b. We substitute this into
(1.24) to obtain
‚Ñô[M > x] ‚â§e‚àíùúÉx
‚àû
‚àë
n=1
(ùúë(ùúÉ))n = e‚àíùúÉx
ùúë(ùúÉ)
1 ‚àíùúë(ùúÉ),
provided ùúë(ùúÉ) < 1, which is the case if ùúÉ‚àà
(0, ùõæ). For any such ùúÉ, we get

22
1 Stochastic Processes
lim sup
x‚Üí‚àû
1
x log ‚Ñô[M > x]
‚â§‚àíùúÉ‚àílim
x‚Üí‚àû
1
x log(1 ‚àíùúë(ùúÉ)) = ‚àíùúÉ.
As ùúÉ< ùõæwas arbitrary, we obtain the
sharpest bound on letting ùúÉ‚Üóùõæ. The
matching lower bound can also be proved
(see [22]), to conclude that
lim
x‚Üí‚àû
1
x log ‚Ñô[M > x] = ‚àíùõæ.
This is an example of a general class of
results referred to as large deviations: fur-
ther details of the general theory can be
found in [24], for example. These tech-
niques have found use in many application
areas including statistical physics: see, for
example, [25].
1.5
Markov Chains in Continuous Time
1.5.1
Markov Property, Transition Function, and
Chapman‚ÄìKolmogorov Relation
In many applications, it is natural to work
in continuous time rather than the discrete
time of Section 1.3. As before, we assume
that we have a discrete state space S, but
now our Markov chains X = (X(t)) have
a continuous- time parameter t ‚àà[0, ‚àû).
Continuous time introduces analytical dif-
Ô¨Åculties, which we will not dwell on in this
presentation.
As in the discrete-time case, we concen-
trate on time-homogeneous chains, and we
will specify the law of (X(t)) in line with the
Markovian idea that ‚Äúgiven the present, the
future is independent of the past.‚Äù
The process (X(t)) satisÔ¨Åes the Markov
property in continuous time if, for all t, h ‚â•
0, all i, j ‚ààS, all 0 ‚â§t0 < t1 < ¬∑ ¬∑ ¬∑ < tn < t,
and all i1, ‚Ä¶ , in ‚ààS,
‚Ñô[X(t + h) = j ‚à£X(t) = i, X(tn) = in, ‚Ä¶ ,
X(t1) = i1] = pij(h).
Here pij( ‚ãÖ) = ‚Ñô[X(t + ‚ãÖ) = j ‚à£X(t) = i] is
the transition function of the Markov chain.
As in the discrete-time case, it is convenient
to use matrix notation:
P(t) = (pij(t))i,j‚ààS given by
pij(t) = ‚Ñôi[X(t) = j],
where again a subscript on ‚Ñôindicates an
initial state, that is, ‚Ñôi[ ‚ãÖ] = ‚Ñô[ ‚ãÖ‚à£X(0) =
i]. We can obtain full information on the
law of the Markov chain, analogously to
the discrete-time case. For example, for 0 <
t1 < ¬∑ ¬∑ ¬∑ < tn and j1, ‚Ä¶ , jn ‚ààS,
‚Ñôi[X(t1) = j1, ‚Ä¶ , X(tn) = jn]
= pij1(t1)pj1j2(t2 ‚àít1) ¬∑ ¬∑ ¬∑ pjn‚àí1jn(tn ‚àítn‚àí1).
To this we add information about the ini-
tial distribution: for instance, ‚Ñô[X(t) = j] =
‚àë
i‚ààS ‚Ñô[X(0) = i]pij(t). Here we must have
pij(0) = ùõøij ‚à∂=
{
1
if i = j
0
if i ‚â†j
.
We also assume that the transition func-
tions satisfy, for each Ô¨Åxed t, pij(t) ‚â•0 for
all i, j and ‚àë
j‚ààS pij(t) = 1 for all i.
To describe our Markov chain now
seems a formidable task: we must specify
the family of functions P(t). However, we
will see in the next section that the Markov
property enables a local (inÔ¨Ånitesimal)
description. First we state a global conse-
quence of the Markov property, namely,
Chapman‚ÄìKolmogorov relation
For any s, t ‚â•0, P(s + t) = P(s)P(t). (1.25)
The fundamental Markovian relation
(1.25) is a special case of the relation

1.5 Markov Chains in Continuous Time
23
known
to
probabilists
as
the
Chap-
man‚ÄìKolmogorov equation, and which,
in its most general form, is often taken as
the starting point of the general theory
of Markov processes. Physicists refer to a
relation such as (1.25) as a master equation.
The derivation of (1.25) in our setting is
direct from the Markov property:
‚Ñôi[X(s + t) = j]
=
‚àë
k‚ààS
‚Ñôi[X(s) = k, X(s + t) = j]
=
‚àë
k‚ààS
‚Ñôi[X(s)=k]‚Ñôi[X(s + t)=j ‚à£X(s)=k]
=
‚àë
k‚ààS
‚Ñôi[X(s) = k]‚Ñôk[X(t) = j],
which is the equality for the (i, j) entry in the
matrix equation (1.25).
1.5.2
InÔ¨Ånitesimal Rates and Q-matrices
In continuous time, there is no smallest
time step and so no concept of a one-step
transition. Often, however, one can encap-
sulate the information in the functions pij(t)
in a single fundamental matrix associated
with the Markov chain, which will serve as
an analogue to the P-matrix in the discrete
theory. This is the Q-matrix.
To proceed, we need to assume some
regularity. We call the chain standard if the
transition probabilities are continuous at 0,
that is, if pij(t) ‚Üípij(0) = ùõøij as t ‚Üì0.
Lemma 1.7 Suppose that X is a standard
Markov chain with transition functions
pij(t). Then for each i, j, pij(t) is a contin-
uous and diÔ¨Äerentiable function of t. The
derivatives p‚Ä≤
ij(t) evaluated at t = 0 we
denote by qij ‚à∂= p‚Ä≤
ij(0); then 0 ‚â§qij < ‚àûfor
i ‚â†j and 0 ‚â§‚àíqii ‚â§‚àû.
The proof of this result relies on the
Chapman‚ÄìKolmogorov relation, but is
somewhat
involved:
see,
for
example,
[26,
Section 14.1].
A
Taylor‚Äôs
formula
expansion now reads
pij(h) = ‚Ñô[X(t + h) = j ‚à£X(t) = i]
= pij(0) + qijh + o(h)
(1.26)
= ùõøij + qijh + o(h), as h ‚Üì0.
(1.27)
So, for i ‚â†j, qij is the (instantaneous) tran-
sition rate of the process from state i to state
j. It is convenient to deÔ¨Åne qi ‚à∂= ‚àíqii for all
i (so qi ‚â•0). Then qi is the rate of departure
from state i.
We further assume that the chain is con-
servative, meaning
‚àë
j‚â†i
qij = qi < ‚àû, for all i.
(1.28)
Note that ‚àë
j‚â†i pij(t) = 1 ‚àípii(t), so, for
example, if S is Ô¨Ånite we can diÔ¨Äerentiate
to immediately get the equality in (1.28),
and then ‚àë
j‚â†i qij < ‚àûby Lemma 1.7, so a
Ô¨Ånite Markov chain is always conservative.
Note that (1.28) implies that ‚àë
j qij = 0
and qi = ‚àë
j‚â†i qij, so the rows of Q sum to
zero.
The matrix Q = (qij)i,j‚ààS is called the
transition
rate
matrix,
the
generator
matrix, or simply the Q-matrix of the
Markov chain; it eÔ¨Äectively describes the
chain‚Äôs dynamics. In particular, under rea-
sonable conditions (see the following text)
the functions pij(‚ãÖ) are uniquely determined
by Q. Thus, in applications, a Markov pro-
cess is often deÔ¨Åned via a Q-matrix and an
initial distribution.12)
Conversely, given a matrix Q = (qij)i,j‚ààS
with nonpositive diagonal entries and non-
negative entries elsewhere for which (1.28)
holds, there always exists a Markov process
with Q as transition rate matrix. This fact
12) This is also essentially the approach taken in
[6].

24
1 Stochastic Processes
can be proved by actually constructing the
paths of such a process: see Section 1.5.4.
Example 1.18 [Birth-and-death process]
Here S = ‚Ñ§+ and X(t) represents a pop-
ulation size at time t. The size of the
population increases on a birth or decreases
on a death. The nonzero entries in the
Q-matrix are
qi,i+1 = ùúÜi, i ‚â•0 (birth rate in state i),
qi,i‚àí1 = ùúái, i ‚â•1 (mortality rate in state i),
q0,0 = ‚àíùúÜ0 and qi,i = ‚àí(ùúÜi + ùúái), i ‚â•1.
In a linear process, ùúÜi = ùúÜi and ùúái = ùúái,
so ùúÜand ùúácan be interpreted as per
individual rates of birth and mortality,
respectively.
1.5.3
Kolmogorov DiÔ¨Äerential Equations
We
now
consider
some
diÔ¨Äerential
equations which, given Q, can be used
to determine the functions pij(‚ãÖ). The start-
ing point is the Chapman‚ÄìKolmogorov
relation
pij(s + t) = ‚àë
k‚ààS pik(s)pkj(t).
If
S is Ô¨Ånite, say, then it is legitimate to
diÔ¨Äerentiate with respect to s to get
p‚Ä≤
ij(s + t) =
‚àë
k‚ààS
p‚Ä≤
ik(s)pkj(t).
Now setting s = 0, we obtain
p‚Ä≤
ij(t) =
‚àë
k‚ààS
qikpkj(t),
which
is
the
Kolmogorov
backward
equation. If instead, we diÔ¨Äerentiate with
respect to t and then put t = 0, we obtain
(after a change of variable)
p‚Ä≤
ij(t) =
‚àë
k‚ààS
pik(t)qkj,
which is the Kolmogorov forward equation.
These diÔ¨Äerential equations are particu-
larly compact in matrix form.
Theorem 1.14 Given Q satisfying (1.28),
we have
P‚Ä≤(t) = P(t)Q,
(Kolmogorov forward equation);
P‚Ä≤(t) = QP(t),
(Kolmogorov backward equation).
We sketched the derivation in the case
where S is Ô¨Ånite. In the general case, a
proof can be found in, for example, [26,
Section 14.2].
Remark 1.8 Suitable versions of the Kol-
mogorov diÔ¨Äerential equations also apply
to processes on continuous state spaces,
such as diÔ¨Äusions, where they take the
form of partial diÔ¨Äerential equations. In
this context, the forward equation can
be framed as a Fokker‚ÄìPlanck equation
for the evolution of a probability den-
sity. The connections among diÔ¨Äusions,
boundary-value problems, and potential
theory are explored, for example, in [2]; an
approach to Fokker‚ÄìPlanck equations from
a more physical perspective can be found
in [27].
We give one example of how to use
the Kolmogorov equations, together with
generating functions, to compute P(t)
from Q.
Example 1.19 [Homogeneous birth process]
We consider a special case of Example 1.18
with only births, where, for all i, ùúÜi = ùúÜ> 0
and ùúái = 0. So qi,i = ‚àíùúÜand qi,i+1 = ùúÜ. The
Kolmogorov forward equation in this case
gives
p‚Ä≤
i,j(t) = ‚àíùúÜpi,j(t) + ùúÜpi,j‚àí1(t),

1.5 Markov Chains in Continuous Time
25
where we interpret pi,‚àí1(t) as 0. In particu-
lar, if i = 0, we have
p‚Ä≤
0,j(t) = ‚àíùúÜp0,j(t) + ùúÜp0,j‚àí1(t).
(1.29)
The initial conditions are assumed to be
p0,0(0) = 1 and p0,i(0) = 0 for i ‚â•1, so that
the process starts in state 0. Consider the
generating function
ùúôt(u) = ùîº[uX(t)] =
‚àû
‚àë
j=0
p0,j(t)uj, |u| < 1.
Multiplying both sides of (1.29) by sj and
summing over j we get
ùúï
ùúït ùúôt(u) = ‚àíùúÜùúôt(u) + ùúÜuùúôt(u)
= ‚àíùúÜ(1 ‚àíu)ùúôt(u).
It follows that ùúôt(u) = A(u)e‚àí(1‚àíu)ùúÜt. The
initial conditions imply that ùúô0(u) = 1, so
in fact A(u) = 1 here, and ùúôt(u) = e‚àí(1‚àíu)ùúÜt,
which is the probability generating func-
tion of a Poisson distribution with mean
ùúÜt (see Example 1.2). Hence, X(t) ‚àºPo(ùúÜt).
In fact X(t) is an example of a Pois-
son process: see, for example, [2, 5, 6,
26].
By analogy with the scalar case, under
suitable conditions one can deÔ¨Åne the
matrix exponential
exp{Qt} =
‚àû
‚àë
k=0
Qktk
k! ,
with
Q0 = I
(identity).
Then
P(t) =
exp{Qt} is a formal solution to both
the
Kolmogorov
forward
and
back-
ward
equations.
In
analytic
terminol-
ogy, Q is the generator of the semi-
group P.
1.5.4
Exponential Holding-Time Construction;
‚ÄúGillespie‚Äôs Algorithm‚Äù
Given the Q-matrix one can construct sam-
ple paths of a continuous- time Markov
chain. The following scheme also tells you
how to simulate a continuous-time Markov
chain.
Suppose the chain starts in a Ô¨Åxed state
X(0) = i for i ‚ààS. Let ùúè0 = 0 and deÔ¨Åne
recursively for n ‚â•0,
ùúèn+1 = inf{t ‚â•ùúèn ‚à∂X(t) ‚â†X(ùúèn)}.
Thus ùúèn is the nth jump time of X, that is,
the nth time at which the process changes
its state.
How long does the chain stay in a partic-
ular state? We have
‚Ñôi[ùúè1 > t + h ‚à£ùúè1 > t]
= ‚Ñôi[ùúè1 > t + h ‚à£ùúè1 > t, X(t) = i]
= ‚Ñôi[ùúè1 > h],
by the Markov property. This mermory-
less property is indicative of the exponential
distribution (see Example 1.8a). Recall that
Y ‚àºexp(ùúÜ) if ‚Ñô[Y > t] = e‚àíùúÜt, t ‚â•0. A cal-
culation shows that
‚Ñô[Y > t + h ‚à£Y > t] = ‚Ñô[Y > h]
= e‚àíùúÜh = 1 ‚àíùúÜh + o(h),
(1.30)
as h ‚Üí0.
In fact, the exponential distribution
is essentially the only distribution with
this property. So it turns out that ùúè1 is
exponential. A heuristic calculation, which
can be justiÔ¨Åed, suggests that ‚Ñôi[ùúè1 >
h] ‚àº‚Ñôi[X(h) = i] = pii(h) = 1 ‚àíqih + o(h).
A comparison with (1.30) suggests that
ùúè1 ‚àºexp(qi).
When the chain does jump, where
does it go? Now, for j ‚â†i, ‚Ñô[X(t + h) = j

26
1 Stochastic Processes
‚à£X(t) = i] = pij(h) = qijh + o(h),
while
‚Ñô[X(t + h) ‚â†i ‚à£X(t) = i] = qih + o(h), so a
conditional probability calculations gives
‚Ñô[X(t + h) = j ‚à£X(t) = i, X(t + h) ‚â†i]
=
qij
qi
+ o(1).
Careful argument along these lines (see,
e.g., [26, Section 14.3]) gives the next
result.13)
Theorem 1.15 Under the law ‚Ñôi of the
Markov chain started in X(0) = i, the
random
variables
ùúè1
and
X(ùúè1)
are
independent.
The
distribution
of
ùúè1
is exponential with rate qi. Moreover,
‚Ñôi[X(ùúè1) = j] = qij‚àïqi.
Perhaps the most striking aspect of this
result is that the holding time and the jump
destination are independent. Theorem 1.15
tells us how to construct the Markov chain,
by iterating the following procedure.
‚Ä¢ Given ùúèn and X(ùúèn) = i, generate an
exp(qi) random variable Yn (this is easily
done via Yn = ‚àíq‚àí1
i
log Un, where
Un ‚àºU[0, 1]). Set ùúèn+1 = ùúèn + Yn.
‚Ä¢ Select the next state X(ùúèn+1) according to
the distribution qij‚àïqi.
Although this standard construction of
Markov chain sample paths goes back to
classical work of Doeblin, Doob, Feller,
and others in the 1940s, and was even
implemented by Kendall and Bartlett in
pioneering computer simulations in the
early 1950s, the scheme is known in certain
applied circles as ‚ÄúGillespie‚Äôs algorithm‚Äù
after Gillespie‚Äôs 1977 paper that rederived
the construction in the context of chemical
reaction modeling.
13) Actually Theorem 1.15 assumes that we are
working with the minimal version of the pro-
cess: see, for example, [6, 26] for details of this
technical point.
Remark 1.9 Let
X‚àó
n = X(ùúèn).
Then
(X‚àó
n)n‚àà‚Ñ§+ deÔ¨Ånes a discrete-time Markov
chain, called the jump chain associated
with X(t), with one-step transitions
p‚àó
ij =
{ qij
qi
if i ‚â†j,
0
if i = j.
,
as long as qi > 0. If qi = 0, then p‚àó
ii = 1, that
is, i is an absorbing state.
1.5.5
Resolvent Computations
Consider a Markov chain with transition
functions pij(t) determined by its generator
matrix Q. The Laplace transform of pij is rij
given by
rij(ùúÜ) = ‚à´
‚àû
0
e‚àíùúÜtpij(t)dt.
(1.31)
Then R(ùúÜ) = (rij(ùúÜ))i,j‚ààS is the resolvent
matrix of the chain. A formal calculation,
which can be justiÔ¨Åed under the conditions
in force in this section, shows that R(ùúÜ)
can be expressed as the matrix inverse
R(ùúÜ) = (ùúÜI ‚àíQ)‚àí1, where I is the identity.
See Chapter 15 of the present volume for
background on Laplace transforms.
Let ùúèbe an exp(ùúÜ) random variable, inde-
pendent of the Markov chain. Then
ùúÜrij(ùúÜ) = ‚à´
‚àû
0
ùúÜe‚àíùúÜtpij(t)dt
= ‚à´
‚àû
0
‚Ñô[ùúè‚ààdt]‚Ñôi[X(t) = j ‚à£ùúè= t],
which is just ‚Ñôi[X(ùúè) = j], the probability
that, starting from state i, the chain is in
state j at the random time ùúè.
Resolvents play an important role in the
theoretical development of Markov pro-
cesses, and in particular in the abstract
semigroup approach to the theory. Here,
however, we view the resolvent as a com-
putational tool, which enables, in principle,

1.5 Markov Chains in Continuous Time
27
calculation of probabilities and hitting-time
distributions.
SpeciÔ¨Åcally, (1.31) implies that pij(t)
can be recovered by inverting its Laplace
transform
ÃÇpij(ùúÜ) = rij(ùúÜ).
Moreover,
let
Ti ‚à∂= inf{t ‚â•0 ‚à∂X(t) = i}, the Ô¨Årst hitting
time of state i. Write Fij(t) ‚à∂= ‚Ñôi[Tj ‚â§t],
and set fij(t) = F‚Ä≤
ij(t) for the density of
the hitting-time distribution. We proceed
analogously
to
the
discrete
argument
for the proof of Lemma 1.3. An applica-
tion of the strong Markov property for
continuous-time chains (cf Section 1.3.4)
gives,
pij(t) = ‚à´
t
0
fij(s)pjj(t ‚àís)ds.
The
convolution
theorem
for
Laplace
transforms (see Chapter 15) implies that
the Laplace transform of fij is given by
ÃÇfij(ùúÜ) =
rij(ùúÜ)
rjj(ùúÜ).
(1.32)
In the next section, we give some examples
of using resolvent ideas in computations.
1.5.6
Example: A Model of Deposition,
DiÔ¨Äusion, and Adsorption
We describe a continuous-time Markov
model of deposition of particles that sub-
sequently
perform
random
walks
and
interact to form barriers according to an
occupation criterion, inspired by models
of submonolayer Ô¨Ålm growth [28, 29].
Particles arrive randomly one by one on a
one-dimensional substrate SN ‚à∂= {0, 1, ‚Ä¶ ,
N + 1} and diÔ¨Äuse until M ‚â•2 particles
end up at the same site, when they clump
together (‚Äúnucleate‚Äù) to form an ‚Äúisland.‚Äù
Islands
form
absorbing
barriers
with
respect to the diÔ¨Äusion of other particles.
We assume that initially, sites 0 and N + 1
are occupied by M particles (so are already
islands) but all other sites are empty.
The Markov dynamics are as follows.
‚Ä¢ At each site x ‚ààSN, new particles arrive
independently at rate ùúå> 0.
‚Ä¢ If at any time a site is occupied by M or
more particles, all those particles are
held in place and are inactive. Particles
that are not inactive are active.
‚Ä¢ Each active particle independently
performs a symmetric simple random
walk at rate 1, that is, from x it jumps to
x + 1 or x ‚àí1 each at rate 1‚àï2.
A state ùúîof the Markov process is a
vector of the occupancies of the sites
1, ‚Ä¶ , N (it is not necessary to keep track
of the occupancies of 0 or N + 1): ùúî(x)
is the number of particles at site x. We
can simulate the process via the expo-
nential
holding-time
construction
of
Section 1.5.4. To do so, we need to keep
track of T(ùúî) = ‚àë
1‚â§x‚â§N ùúî(x)1{ùúî(x) < M},
the total number of active particles in state
ùúî. The waiting time in a state ùúîis then
exponential with parameter T(ùúî) + Nùúå;
at the end of this time, with probabil-
ity T(ùúî)‚àï(T(ùúî) + Nùúå), one of the active
particles jumps (chosen uniformly from
all active particles, and equally likely to
be a jump left or right), else a new par-
ticle arrives at a uniform random site in
{1, ‚Ä¶ , N}.
An analysis of the general model just
described would be interesting but is
beyond the scope of this presentation. We
use small examples (in terms of M and N) to
illustrate the resolvent methods described
in Section 1.5.5. For simplicity, we take
M = 2 and stop the process the Ô¨Årst time
that two particles occupy any internal site.
ConÔ¨Ågurations can be viewed as elements
of {0, 1, 2}{1,2,‚Ä¶,N}, but symmetry can be
used to further reduce the state space for
our questions of interest.

28
1 Stochastic Processes
1.5.6.1
N = 1
Take N = 1. The state space for our Markov
chain X(t) is {0, 1, 2}, the number of parti-
cles in position 1, with X(0) = 0, and 2 as
the absorbing state. Clearly X(t) = 2 even-
tually; the only question is how long we
have to wait for absorption. The answer is
not trivial, even in this minimal example.
The generator matrix for the Markov
chain is
Q =
‚éõ
‚éú
‚éú‚éù
‚àíùúå
ùúå
0
1
‚àí1 ‚àíùúå
ùúå
0
0
0
‚éû
‚éü
‚éü‚é†
, and so
ùúÜI ‚àíQ =
‚éõ
‚éú
‚éú‚éù
ùúÜ+ ùúå
‚àíùúå
0
‚àí1
1 + ùúÜ+ ùúå
‚àíùúå
0
0
ùúÜ
‚éû
‚éü
‚éü‚é†
.
To work out p02(t), we compute r02(ùúÜ):
r02(ùúÜ) = (ùúÜI ‚àíQ)‚àí1
02 =
det
(
‚àíùúå
0
1 + ùúÜ+ ùúå
‚àíùúå
)
det(ùúÜI ‚àíQ)
=
ùúå2
ùúÜ((ùúÜ+ ùúå)2 + ùúÜ).
Inverting the Laplace transform, we obtain
1‚àíp02(t)=e‚àí((1+2ùúå)‚àï(2))t
(
cosh
(( t
2
) ‚àö
1+4ùúå
)
+ 1+2ùúå
‚àö
1+4ùúå
sinh
(( t
2
)‚àö
1+4ùúå
))
Q =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
‚àí3ùúå
2ùúå
ùúå
0
0
0
0
0
1
2
‚àí1 ‚àí3ùúå
1
2
ùúå
ùúå
0
ùúå
0
0
1
‚àí1 ‚àí3ùúå
2ùúå
0
0
0
ùúå
0
0
1
2
‚àí2 ‚àí3ùúå
1
2
ùúå
1
2 + ùúå
1
2 + ùúå
0
1
0
1
‚àí2 ‚àí3ùúå
ùúå
2ùúå
0
0
0
0
1
0
‚àí3 ‚àí3ùúå
1 + 2ùúå
1 + ùúå
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
.
‚àº1
2
(
1 +
1 + 2ùúå
‚àö
1 + 4ùúå
)
exp
{
‚àít
2
(
1 + 2ùúå‚àí
‚àö
1 + 4ùúå
)}
,
as t ‚Üí‚àû.
For example, if ùúå= 3‚àï4, the exact expres-
sion simpliÔ¨Åes to
p02(t) = 1 ‚àí9
8e‚àít‚àï4 + 1
8e‚àí9t‚àï4.
As in this case, 2 is absorbing, ‚Ñô[T2 ‚â§t] =
‚Ñô[X(t) = 2] = p02(t), so
f02(t) = d
dt p02(t) = 9
32(e‚àít‚àï4 ‚àíe‚àí9t‚àï4),
in the ùúå= 3‚àï4 example. The same answer
can be obtained using (1.32).
1.5.6.2
N = 3
For the problem of the distribution of the
point of the Ô¨Årst collision, the Ô¨Årst nontriv-
ial case is N = 3. Making use of the symme-
try, we can now describe the Markov chain
with the 8 states
(0, 0, 0), (1, 0, 0), (0, 1, 0), (1, 1, 0),
(1, 0, 1), (1, 1, 1), (2, ‚àó, ‚àó), (‚àó, 2, ‚àó),
in that order, where each ‚àóindicates either
a 0 or a 1; so for example (1, 0, 0) stands for
(1, 0, 0) or (0, 0, 1). The generator matrix is
now

1.6 Gibbs and Markov Random Fields
29
The probability of the Ô¨Årst collision
occurring at the midpoint is
z(ùúå) = ‚Ñô[ lim
t‚Üí‚àûX(t) = (‚àó, 2, ‚àó)].
According to MAPLE, inverting the appro-
priate Laplace transform gives
z(ùúå)= 1
9 ‚ãÖ486ùúå4+1354ùúå3+1375ùúå2+598ùúå+87
162ùúå4+414ùúå3+392ùúå2+160ùúå+ 23 .
In particular, if deposition dominates diÔ¨Äu-
sion,
lim
ùúå‚Üí‚àûz(ùúå) = 1
9 ‚ãÖ486
162 = 1
3,
which is as it should be!
1.6
Gibbs and Markov Random Fields
We have so far focused on stochastic
processes that vary through time. In this
section and in Section 1.7, we take a detour
into spatial processes. The role of space
in our models is played by an underlying
graph structure, describing vertices V and
the edges E that connect them. This section
is devoted to random Ô¨Åelds, that is, ensem-
bles of random variables associated with
the vertices subject to certain constraints
imposed by the edges.
Let G = (V, E) be an undirected Ô¨Ånite
graph with vertex set V and edge set
E consisting of pairs (i, j) where i, j ‚ààV;
(i, j) ‚ààE indicates an edge between vertices
i and j.14) With this graph, we associate
random variables {Xi} for i ‚ààV; to keep
things simple, we take Xi ‚àà{‚àí1, 1}.
We consider two ways of specifying these
random variables ‚Äì as Gibbs or Markov
14) Our graphs are undirected, which means that
(i, j) = (j, i) is an unordered pair.
random Ô¨Åelds
‚Äì
and the relationship
between the two. Finally, we will consider
how to use ideas from Markov chains
(Section 1.3) to simulate a Markov random
Ô¨Åeld.
1.6.1
Gibbs Random Field
To deÔ¨Åne a Gibbs random Ô¨Åeld, we need the
concept of a clique. A clique in G = (V, E) is
a subset K of V such that E contains all the
possible edges between members of K; we
include the set of no vertices ‚àÖas a clique.
Let Óà∑be the set of all cliques of graph G.
Let x = (x1, ‚Ä¶ , x|V|) ‚àà{‚àí1, 1}V denote
an assignment of a value ¬±1 to each ver-
tex of G. For K ‚äÜV we write x|K for the
restriction of x to K. The random variables
{Xi}i‚ààV on G constitute a Gibbs random
Ô¨Åeld if, for all x ‚àà{‚àí1, 1}V,
‚Ñô[Xi = xi for all i ‚ààV]
= 1
Z exp
{ ‚àë
K‚ààÓà∑
fK(x|K)
}
,
(1.33)
where fK ‚à∂{‚àí1, 1}|K| ‚Üí‚Ñùfor each clique
K, and Z is a normalization:
Z =
‚àë
x‚àà{‚àí1,1}V
exp
{ ‚àë
K‚ààÓà∑
fK(x|K)
}
.
To see why this is a natural deÔ¨Ånition
for the law of {Xi} in many situations,
consider
associating
an
energy
func-
tion
Óà±‚à∂{‚àí1, 1}V ‚Üí‚Ñù
to
the
states.
We seek the maximum entropy distri-
bution for {Xi}i‚ààV
for a given mean
energy. So we want to Ô¨Ånd the proba-
bility mass function, f , on {‚àí1, 1}V that
maximizes ‚àí‚àë
x‚àà{‚àí1,1}V f (x) log f (x) sub-
ject to ‚àë
x‚àà{‚àí1,1}V f (x)Óà±(x) = const. One
can show this is achieved by
f (x) ‚àùexp{‚àíùõΩÓà±(x)},

30
1 Stochastic Processes
where ùõΩ‚àà(0, ‚àû) is chosen to obtain the
required energy. (The analogy here is
between ùõΩand the inverse temperature
1‚àï(kT) in thermodynamics.) Now if Óà±can
be decomposed as a sum over the cliques
of the graph, we recover (1.33).
Example 1.20 [Ising model] Consider the
N √ó N grid in two dimensions. We label
the vertices by members of the set LN =
{1, ‚Ä¶ , N} √ó {1, ‚Ä¶ , N}. We put an edge
between i = (i1, i2) and j = (j1, j2) if |i1 ‚àí
j1| + |i2 ‚àíj2| = 1; this adjacency condition
we write as i ‚àºj. Here the clique set Óà∑is
made up of the empty set, singleton nodes,
and pairs of nodes that are distance one
apart in the lattice.
Given
constants
ùõΩ> 0,
J > 0,
and
h ‚àà‚Ñùwe consider the Gibbs random
Ô¨Åeld
with
probability
mass
function
f (x) = Z‚àí1 exp{‚àíùõΩÓà±(x)}, where
Óà±(x) = ‚àíJ
‚àë
i,j‚ààLN ‚à∂i‚àºj
XiXj ‚àíh
‚àë
i‚ààLN
Xi.
The sum over pairs of nodes (the interac-
tion term) means that neighboring nodes
have a propensity to be in the same state.
The second (external Ô¨Åeld) term leads
to nodes more likely to be in either of
the states 1 or ‚àí1, depending on the sign
of h.
This model has been studied widely as a
model for ferromagnetism and was initially
proposed by Ising [30] under the guidance
of Lenz. The Potts model is a generaliza-
tion with q-valued states and more general
interactions: see [31].
1.6.2
Markov Random Field
We now consider a second speciÔ¨Åcation
of random Ô¨Åeld that adapts the Markov
property to spatial processes. For a given
subset W ‚äÜV, we deÔ¨Åne its boundary as
ùúïW = {v ‚ààV ‚ßµW ‚à∂(v, w) ‚ààE
for some w ‚ààW}.
The concept of Markov random Ô¨Åeld
extends the temporal Markov property
(1.12), which said that, conditional on the
previous states of the process, the future
depends on the past only through the
present, to a spatial (or topological) one.
This ‚ÄúMarkov property‚Äù will say that the
state of nodes in some set of vertices W
conditioned on the state of all the other
vertices only depends on the state of the
vertices in ùúïW.
The random variables {Xi}i‚ààV on G con-
stitute a Markov random Ô¨Åeld if
‚Ä¢ they have a positive probability mass
function,
‚Ñô[{Xi}i‚ààV = x] > 0,
for all x ‚àà{‚àí1, 1}V,
‚Ä¢ and obey the global Markov property: for
all W ‚äÜV,
‚Ñô[{Xi}i‚ààW =x|W ‚à£{Xi}i‚ààV‚ßµW =x|V‚ßµW]
= ‚Ñô[{Xi}i‚ààW =x|W ‚à£{Xi}i‚ààùúïW =x|ùúïW].
Example 1.21
As in Example 1.20, con-
sider a random Ô¨Åeld taking values ¬±1 on the
vertices of the N √ó N lattice. We specify the
(conditional) probability that a vertex, i, is
in state 1, given the states of its neighbors
to be
eùõΩ(h+Jyi)
e‚àíùõΩ(h+Jyi) + eùõΩ(h+Jyi) , where yi =
‚àë
j‚àºi
xj.
(1.34)
Here ùõΩ> 0, J > 0, and h ‚àà‚Ñùare param-
eters. The larger yi, which is the number
of neighbours of i with spin +1 minus the

1.7 Percolation
31
number with spin ‚àí1, the greater the prob-
ability that vertex i will itself have state 1.
1.6.3
Connection Between Gibbs and Markov
Random Fields
In Examples 1.20 and 1.21, we have used
the same notation for the parameters. In
fact, both speciÔ¨Åcations (one Gibbs, the
other Markov) deÔ¨Åne the same probability
measure on {‚àí1, 1}V. This is an example of
the following result.
Theorem 1.16 (Hammersley‚ÄìCliÔ¨Äord
theorem) The ensemble of random vari-
ables {Xi}i‚ààV on G is a Markov random Ô¨Åeld
if and only if it is a Gibbs random Ô¨Åeld with
a positive probability mass function.
A proof can be found in [31]. From this
point forward, we will use the terms Gibbs
random Ô¨Åeld and Markov random Ô¨Åeld
interchangeably.
1.6.4
Simulation Using Markov Chain Monte
Carlo
Direct simulation of a Gibbs random Ô¨Åeld
on a graph is computationally diÔ¨Écult
because the calculation of the normalizing
constant, Z, requires a sum over all the pos-
sible conÔ¨Ågurations. In many situations,
this is impractical. Here we consider an
alternative way to simulate a Gibbs random
Ô¨Åeld making use of Markov chains.
We saw in Section 1.3.7 that an irre-
ducible, aperiodic Markov chain converges
to its stationary distribution. The idea now
is to design a Markov chain on the state
space {‚àí1, 1}V whose stationary distribu-
tion coincides with the desired Gibbs ran-
dom Ô¨Åeld. We simulate the Markov chain
for a long time to obtain what should be a
distribution close to stationarity, and hence,
a good approximation to a realization of the
Gibbs random Ô¨Åeld.
We initialize the Markov chain with any
initial state ùúé‚àà{‚àí1, 1}V. To update the
state of the chain, we randomly select a ver-
tex uniformly from all vertices in the graph.
We will update the state associated with
this vertex by randomly selecting a new
state using the conditional probabilities
given, subject to the neighboring vertices‚Äô
states, taking advantage of the Markov
random Ô¨Åeld description. For example, in
Example 1.21, we set the node state to 1
with probability given by (1.34) and to ‚àí1,
otherwise.
It is easy to check that this Markov
chain is irreducible, aperiodic, and has the
required stationary distribution. This is an
example of a more general methodology of
using a Markov chain with simple update
steps to simulate from a distribution that
is computationally diÔ¨Écult to evaluate
directly, called Markov chain Monte Carlo.
SpeciÔ¨Åcally, we have used a Gibbs sampler
here, but there are many other schemes for
creating a Markov chain with the correct
stationary distribution. Many of these
techniques have been developed within the
setting of Bayesian statistics but have appli-
cations in many other Ô¨Åelds, including spin
glass models and theoretical chemistry.
1.7
Percolation
Consider the inÔ¨Ånite square lattice ‚Ñ§2.
Independently, for each edge in the lattice,
the edge is declared open with probability
p; else (with probability 1 ‚àíp) it is closed.
This model is called bond percolation on
the square lattice.
For two vertices x, y ‚àà‚Ñ§2 write x ‚Üê‚Üíy
if x and y are joined by a path consisting of
open edges in the percolation model on ‚Ñ§2.

32
1 Stochastic Processes
The open cluster containing vertex x, C(x),
is the (random) set of all vertices joined to
x by an open path
C(x) ‚à∂= {y ‚àà‚Ñ§2 ‚à∂y ‚Üê‚Üíx}.
A fundamental question in percolation
regards the nature of C(0), the open cluster
at 0, and how its (statistical) properties
depend on the parameter p.
We write ‚Ñôp for the probability associ-
ated with percolation with parameter p.
Write |C(0)| for the number of vertices in
the open cluster at 0. The percolation prob-
ability is
ùúÉ(p) = ‚Ñôp[|C(0)| = ‚àû].
By translation invariance, ùúÉ(p) ‚àà[0, 1] is
not speciÔ¨Åc to the origin: ‚Ñôp[|C(x)| = ‚àû] =
ùúÉ(p) for any x.
Let H‚àûbe the event that |C(x)| = ‚àûfor
some x. It is not hard to show that
ùúÉ(p) = 0 =‚áí‚Ñôp[H‚àû] = 0
ùúÉ(p) > 0 =‚áí‚Ñôp[H‚àû] = 1.
We state a fundamental result that may
seem obvious; the proof we give, due to
Hammersley, demonstrates the eÔ¨Äective-
ness of another probabilistic tool: coupling.
Lemma 1.8 ùúÉ(p) is nondecreasing as a
function of p.
Proof. List the edges of the lattice ‚Ñ§2 in
some order as e1, e2, ‚Ä¶. Let U1, U2, ‚Ä¶ be
independent uniform random variables on
[0, 1]. Assign Ui to edge ei.
Let Ep = {ei ‚à∂Ui ‚â§p}. Then Ep is the
set of open edges in bond percolation
with parameter p. This construction cou-
ples bond percolation models for every
p ‚àà[0, 1] in a monotone way: if ei ‚ààEp
then ei ‚ààEq for all q ‚â•p.
Let
Cp(0)
denote
the
cluster
con-
taining 0 using edges in Ep. If p ‚â§q,
then by construction Cp(0) ‚äÜCq(0). So
{|Cp(0)| = ‚àû} ‚äÜ{|Cq(0)| = ‚àû},
and
hence, ùúÉ(p) ‚â§ùúÉ(q).
As ùúÉ(p) is nondecreasing, and clearly
ùúÉ(0) = 0 and ùúÉ(1) = 1, there must be some
threshold value
pc ‚à∂= inf{p ‚àà[0, 1] ‚à∂ùúÉ(p) > 0}.
So for p < pc, ‚Ñôp[H‚àû] = 0, while for p > pc,
‚Ñôp[H‚àû] = 1.
The Ô¨Årst question is this: is there a non-
trivial phase transition, that is, is 0 < pc <
1? This question was answered by Broad-
bent and Hammersley in the late 1950s.
Proposition 1.2 1‚àï3 ‚â§pc ‚â§2‚àï3.
Proof of pc ‚â•1‚àï3 Let An be the event that
there exists a self-avoiding open path start-
ing at 0 of length n. Then
A1 ‚äáA2 ‚äáA3 ¬∑ ¬∑ ¬∑
and
‚àû
‚ãÇ
n=1
An = {|C(0)| = ‚àû}.
So
ùúÉ(p) = ‚Ñôp[|C(0)| = ‚àû] = limn‚Üí‚àû‚Ñôp
[An], by continuity of probability measure
(see Section 1.A). Let Œìn be the set of all
possible self-avoiding paths of length n
starting at 0. Then
‚Ñôp[An] = ‚Ñôp
‚ãÉ
ùõæ‚ààŒìn
{ùõæis open}
‚â§
‚àë
ùõæ‚ààŒìn
‚Ñôp[ùõæis open] = |Œìn|pn
‚â§4 ‚ãÖ3n‚àí1 ‚ãÖpn,
which tends to 0 if p < 1‚àï3. So pc ‚â•1‚àï3.
On the basis of pioneering Monte Carlo
simulations, Hammersley conjectured that

1.A Appendix: Some Results from Probability Theory
33
pc was 1‚àï2. Harris proved in 1960 that
ùúÉ(1‚àï2) = 0, which implies that pc ‚â•1‚àï2. It
was not until 1980 that a seminal paper of
Kesten settled things.
Theorem 1.17 (Harris 1960, Kesten
1980) pc = 1‚àï2.
Harris‚Äôs result ùúÉ(1‚àï2) = 0 thus means
that ùúÉ(pc) = 0; this is conjectured to be the
case in many percolation models (e.g., on
‚Ñ§d, it is proved for d = 2 and d ‚â•19, but is
conjectured to hold for all d ‚â•2). In recent
years, there has been much interest in the
detailed structure of percolation when
p = pc. The Schramm‚ÄìLoewner evolution
has provided an important new mathemat-
ical tool to investigate physical predictions,
which often originated in conformal Ô¨Åeld
theory; see [32].
1.8
Further Reading
A wealth of information on stochastic
processes and the tools that we have
introduced here can be found in [2, 5, 9,
20, 26], for example. All of those books
cover Markov chains. The general theory
of Markov processes can be found in [2,
9], which also cover the connection to
semigroup theory. Feller gives a masterly
presentation of random walks and generat-
ing functions [20] and Laplace transforms,
characteristic functions, and their appli-
cations [9]. Branching processes can be
found in [5, 20]. A thorough treatment of
percolation is presented in [33]. We have
said almost nothing here about Brownian
motion or diÔ¨Äusions, for which we refer the
reader to Chapter 3 of this volume as well as
[2, 5, 9, 26]. Physicists and mathematicians
alike Ô¨Ånd it hard not to be struck by the
beauty of the connection between random
walks and electrical networks, as exposited
in [34]. Applications of stochastic processes
in physics and related Ô¨Åelds are speciÔ¨Åcally
treated in [27, 35]; the array of applications
of random walks alone is indicated in
[19, 36, 37].
1.A
Appendix: Some Results from Probability
Theory
There is no other simple mathematical
theory that is so badly taught to physi-
cists as probability.
‚Äì R. F. Streater [38, p. 19].
Essentially, the theory of probability
is nothing but good common sense
reduced to mathematics.
‚Äì P.-S. de Laplace, Essai philosophique
sur les probabilit√©s, 1813.
Kolmogorov‚Äôs 1933 axiomatization of
probability on the mathematical founda-
tion of measure theory was fundamental
to the development of the subject and is
essential for understanding the modern
theory. Many excellent textbook treat-
ments are available. Here we emphasize a
few points directly relevant for the rest of
this chapter.
1.A.1
Set Theory Notation
A set is a collection of elements. The
set of no elements is the empty set ‚àÖ.
Finite nonempty sets can be listed as
S = {a1, ‚Ä¶ , an}. If a set S contains an ele-
ment a, we write a ‚ààS. A set R is a subset
of a set S, written R ‚äÜS, if every a ‚ààR also
satisÔ¨Åes a ‚ààS. For two sets S and T, their
intersection is S ‚à©T, the set of elements
that are in both A and B, and their union is

34
1 Stochastic Processes
S ‚à™T, the set of elements in at least one of
S or T. For two sets S and T, ‚ÄúS minus T‚Äù is
the set S ‚ßµT = {a ‚ààS ‚à∂a ‚àâT}, the set of
elements that are in S but not in T.
Note that S ‚à©‚àÖ= ‚àÖ, S ‚à™‚àÖ= S, and S ‚ßµ
‚àÖ= S.
1.A.2
Probability Spaces
Suppose we perform an experiment that
gives a random outcome. Let Œ© denote the
set of all possible outcomes: the sample
space. To start with, we take Œ© to be dis-
crete, which means it is Ô¨Ånite or countably
inÔ¨Ånite. This means that we can write Œ© as
a (possibly inÔ¨Ånite) list:
Œ© = {ùúî1, ùúî2, ùúî3, ‚Ä¶},
where ùúî1, ùúî2, ùúî3, ‚Ä¶ are the possible out-
comes to our experiment.
A set A ‚äÜŒ© is called an event. Given
events A, B ‚äÜŒ©, we can build new events
using the operations of set theory:
‚Ä¢ A ‚à™B (‚ÄúA or B‚Äù), the event that A
happens, or B happens, or both.
‚Ä¢ A ‚à©B (‚ÄúA and B‚Äù), the event that A and B
both happen.
Two events A and B are called disjoint or
mutually exclusive if A ‚à©B = ‚àÖ.
We want to assign probabilities to events.
Let Œ© be a nonempty discrete sample space.
A function ‚Ñôthat gives a value ‚Ñô[A] ‚àà[0, 1]
for every subset A ‚äÜŒ© is called a discrete
probability measure on Œ© if
(P1) ‚Ñô[‚àÖ] = 0 and ‚Ñô[Œ©] = 1;
(P2) For any A1, A2, ‚Ä¶ , pairwise disjoint
subsets of Œ© (so Ai ‚à©Aj = ‚àÖfor i ‚â†j),
‚Ñô
[ ‚àû
‚ãÉ
i=1
Ai
]
=
‚àû
‚àë
i=1
‚Ñô[Ai] (ùúé‚àíadditivity).
Given Œ© and a probability measure ‚Ñô, we
call (Œ©, ‚Ñô) a discrete probability space.
Remark 1.10 For
nondiscrete
sample
spaces, we may not be able to assign prob-
abilities to all subsets of Œ© in a sensible
way, and so smaller collections of events
are required. In this appendix, we treat the
discrete case only, as is suÔ¨Écient for most
(but not all) of the discussion in this chapter.
For the more general case, which requires
a deeper understanding of measure theory,
there are many excellent treatments, such
as [1, 2, 39, 40].
For an event A ‚äÜŒ©, we deÔ¨Åne its com-
plement, denoted Ac and read ‚Äúnot A‚Äù, to
be Ac ‚à∂= Œ© ‚ßµA = {ùúî‚ààŒ© ‚à∂ùúî‚àâA}. Note
that (Ac)c = A, A ‚à©Ac = ‚àÖ, and A ‚à™Ac = Œ©.
If (Œ©, ‚Ñô) is a discrete probability space,
then
‚Ä¢ For A ‚äÜŒ©, ‚Ñô[Ac] = 1 ‚àí‚Ñô[A];
‚Ä¢ If A, B ‚äÜŒ© and A ‚äÜB, then ‚Ñô[A] ‚â§‚Ñô[B]
(monotonicity);
‚Ä¢ If A, B ‚äÜŒ©, then
‚Ñô[A ‚à™B] = ‚Ñô[A] + ‚Ñô[B] ‚àí‚Ñô[A ‚à©B]. In
particular, if A ‚à©B = ‚àÖ,
‚Ñô[A ‚à™B] = ‚Ñô[A] + ‚Ñô[B].
It follows from this last statement that
‚Ñô[A ‚à™B] ‚â§‚Ñô[A] + ‚Ñô[B]; more generally,
we have the elementary but useful Boole‚Äôs
inequality: ‚Ñô[‚à™nAn] ‚â§‚àë
n ‚Ñô[An].
At several points we use the continuity
property of probability measures:
‚Ä¢ If A1 ‚äÜA2 ‚äÜA3 ‚äÜ¬∑ ¬∑ ¬∑ are events, then
‚Ñô[‚à™‚àû
i=1Ai] = limn‚Üí‚àû‚Ñô[An].
‚Ä¢ If A1 ‚äáA2 ‚äáA3 ‚äá¬∑ ¬∑ ¬∑ are events, then
‚Ñô[‚à©‚àû
i=1Ai] = limn‚Üí‚àû‚Ñô[An].
To see the Ô¨Årst statement, we can write
‚à™‚àû
i=1Ai = ‚à™‚àû
i=1(Ai ‚ßµAi‚àí1), where we set A0 =

1.A Appendix: Some Results from Probability Theory
35
‚àÖ, and the latter union is over pairwise dis-
joint events. So, by ùúé-additivity,
‚Ñô[‚à™‚àû
i=1Ai] =
‚àû
‚àë
i=1
‚Ñô[Ai ‚ßµAi‚àí1]
= lim
n‚Üí‚àû
n
‚àë
i=1
‚Ñô[Ai ‚ßµAi‚àí1].
But
‚àën
i=1 ‚Ñô[Ai ‚ßµAi‚àí1] = ‚Ñô[‚à™n
i=1(Ai ‚ßµ
Ai‚àí1)] = ‚Ñô[An],
giving
the
result.
The
second statement is analogous.
1.A.3
Conditional Probability and Independence
of Events
If A and B are events with ‚Ñô[B] > 0 then the
conditional probability ‚Ñô[A ‚à£B] of A given
B is deÔ¨Åned by
‚Ñô[A ‚à£B] ‚à∂= ‚Ñô[A ‚à©B]
‚Ñô[B]
.
A
countable
collection
of
events
E1, E2, ‚Ä¶ is called a partition of Œ© if
(a)
for all i, Ei ‚äÜŒ© and Ei ‚â†‚àÖ;
(b)
for i ‚â†j, Ei ‚à©Ej = ‚àÖ(the events are
disjoint);
(c)
‚ãÉ
i Ei = Œ© (the events Ô¨Åll the sample
space).
Let E1, E2, ‚Ä¶ be a partition of Œ©. Follow-
ing from the deÔ¨Ånitions is the basic law of
total probability, which states that for all
A ‚äÜŒ©,
‚Ñô[A] =
‚àë
i
‚Ñô[Ei]‚Ñô[A ‚à£Ei].
A
countable
collection
(Ai, i ‚ààI)
of
events is called independent if, for every
Ô¨Ånite subset J ‚äÜI,
‚Ñô
[ ‚ãÇ
j‚ààJ
Aj
]
=
‚àè
j‚ààJ
‚Ñô[Aj].
In particular, two events A and B are inde-
pendent if ‚Ñô[A ‚à©B] = ‚Ñô[A]‚Ñô[B] (i.e., if
‚Ñô[B] > 0, ‚Ñô[A ‚à£B] = ‚Ñô[A]).
1.A.4
Random Variables and Expectation
Let (Œ©, ‚Ñô) be a discrete probability space. A
function X ‚à∂Œ© ‚Üí‚Ñùis a random variable.
So each ùúî‚ààŒ© is mapped to a real number
X(ùúî). The set of possible values for X
is
X(Œ©) = {X(ùúî) ‚à∂ùúî‚ààŒ©} ‚äÇ‚Ñù.
Notice
that because Œ© is discrete, X(Œ©) must be
also.
If X and Y are two random variables
on (Œ©, ‚Ñô), then X + Y, XY, and so on, are
also random variables. For example, (X +
Y)(ùúî) = X(ùúî) + Y(ùúî). In some cases (such
as the expected hitting times deÔ¨Åned at
(1.18)), we extend the domain of a random
variable from ‚Ñùto ‚Ñù‚à™{‚àû}.
The probability mass function of a dis-
crete random variable X is the collection
‚Ñô[X = x] for all x ‚ààX(Œ©). The distribution
function of X is F ‚à∂‚Ñù‚Üí[0, 1] given by
F(x) = ‚Ñô[X ‚â§x].
Example 1.22 [Bernoulli and binomial dis-
tributions] Let n be a positive integer and
p ‚àà[0, 1]. We say X has a binomial distri-
bution with parameters (n, p), written X ‚àº
Bin(n, p), if ‚Ñô[X = k] = (n
k
)pk(1 ‚àíp)n‚àík for
k ‚àà{0, 1, ‚Ä¶ , n}. The case Y ‚àºBin(1, p) is
the Bernoulli distribution; here ‚Ñô[Y = 1] =
p = 1 ‚àí‚Ñô[Y = 0] and we write Y ‚àºBe(p).
The binomial distribution has the following
interpretation: Perform n independent ‚Äútri-
als‚Äù (e.g., coin tosses) each with probability
p of ‚Äúsuccess‚Äù (e.g., ‚Äúheads‚Äù), and count the
total number of successes.
Example 1.23 [Poisson distribution] Let
ùúÜ> 0 and pk ‚à∂= e‚àíùúÜ(ùúÜk‚àïk!) for k ‚àà‚Ñ§+.
If ‚Ñô[X = k] = pk, X is a Poisson random
variable with parameter ùúÜ.

36
1 Stochastic Processes
Let X be a discrete random variable. The
expectation, expected value, or mean of X is
given by
ùîº[X] =
‚àë
x‚ààX(Œ©)
x‚Ñô[X = x],
provided the sum is Ô¨Ånite. The variance
of X is ùïçar[X] = ùîº[(X ‚àíùîº[X])2] = ùîº[X2] ‚àí
(ùîº[X])2.
Example 1.24 [Indicator variables.] Let A
be an event. Let 1A denote the indicator
random variable of A, that is, 1A ‚à∂Œ© ‚Üí
{0, 1} given by
1A(ùúî) ‚à∂=
{ 1 if ùúî‚ààA
0 if ùúî‚àâA
So 1A is 1 if A happens and 0 if not. Then
ùîº[1A] = 1 ‚ãÖ‚Ñô[1A = 1] + 0 ‚ãÖ‚Ñô[1A = 0]
= ‚Ñô[1A = 1] = ‚Ñô[A].
Expectation has the following basic prop-
erties. For X and Y random variables with
well-deÔ¨Åned expectations and a, b ‚àà‚Ñù,
(a)
ùîº[aX + bY] = aùîº[X] + bùîº[Y]
(lin-
earity).
(b)
If ‚Ñô[X ‚â§Y] = 1, then ùîº[X] ‚â§ùîº[Y]
(monotonicity).
(c)
|ùîº[X]| ‚â§ùîº[|X|] (triangle inequality).
(d)
If
h ‚à∂X(Œ©) ‚Üí‚Ñù,
then
ùîº[h(X)] =
‚àë
x‚ààX(Œ©) h(x)‚Ñô[X = x] (‚Äúlaw of the
unconscious statistician‚Äù).
Let X be a nonnegative random vari-
able. Then, for any x > 0, x1{X ‚â•x} ‚â§X
holds with probability 1. Taking expec-
tations and using monotonicity yields
‚Ñô[X ‚â•x] ‚â§x‚àí1ùîº[X]. This is usually known
as Markov‚Äôs inequality, although it is also
sometimes
referred
to
as
Chebyshev‚Äôs
inequality.15) Applying Markov‚Äôs inequality
to eùúÉX, ùúÉ> 0, gives
‚Ñô[X ‚â•x] = ‚Ñô[eùúÉX ‚â•eùúÉx] ‚â§e‚àíùúÉxùîº[eùúÉX],
which is sometimes known as ChernoÔ¨Ä‚Äôs
inequality.
Let (Œ©, ‚Ñô) be a discrete probability space.
A family (Xi, i ‚ààI) of random variables is
called independent if for any Ô¨Ånite subset
J ‚äÜI and all xj ‚ààXj(Œ©),
‚Ñô
( ‚ãÇ
j‚ààJ
{Xj = xj}
)
=
‚àè
j‚ààJ
‚Ñô(Xj = xj).
In particular, random variables X and
Y are independent if ‚Ñô[X = x, Y = y] =
‚Ñô[X = x]‚Ñô[Y = y] for all x and y.
Theorem 1.18 If X and Y are independent,
then ùîº[XY] = ùîº[X]ùîº[Y].
A consequence of Theorem 1.18 is that
if X and Y are independent, then ùïçar[X +
Y] = ùïçar[X] + ùïçar[Y].
Example 1.25
(a)
If Y ‚àºBe(p) then ùîº[Y] = ùîº[Y 2] =
p ‚ãÖ1 + (1 ‚àíp) ‚ãÖ0 = p,
so
ùïçar[Y] =
p ‚àíp2 = p(1 ‚àíp).
(b)
If X ‚àºBin(n, p) then we can write
X = ‚àën
i=1 Yi
where
Yi ‚àºBe(p)
are
independent. By linearity of expec-
tation,
ùîº[X] = ‚àën
i=1 ùîº[Yi] = np.
Also,
by
independence,
ùïçar[X] =
‚àën
i=1 ùïçar[Yi] = np(1 ‚àíp).
1.A.5
Conditional Expectation
On a discrete probability space (Œ©, ‚Ñô), let
B be an event with ‚Ñô[B] > 0 and let X be
15) Chebyshev‚Äôs inequality is the name more com-
monly associated with Markov‚Äôs inequality
applied to the random variable (X ‚àíùîº[X])2, to
give ‚Ñô[|X ‚àíùîº[X]| ‚â•x] ‚â§x‚àí2ùïçar[X].

References
37
a random variable. The conditional expec-
tation of X given B is
ùîº[X ‚à£B] =
‚àë
x‚ààX(Œ©)
x‚Ñô[X = x ‚à£B].
So ùîº[X ‚à£B] can be thought of as expecta-
tion with respect to the conditional proba-
bility measure ‚Ñô[ ‚ãÖ‚à£B]. An alternative is
ùîº[X ‚à£B] = ùîº[X1B]
‚Ñô[B] ,
(1.35)
where 1B is the indicator random variable
of B. The proof of (1.35) is an exercise in
interchanging summations. First,
ùîº[X ‚à£B] =
‚àë
x‚ààX(Œ©)
x‚Ñô[X = x ‚à£B]
=
‚àë
x‚ààX(Œ©)
x‚Ñô[{X = x} ‚à©B]
‚Ñô[B]
.
(1.36)
On the other hand, the random variable
1BX takes values x ‚â†0 with
‚Ñô[1BX = x] =
‚àë
ùúî‚ààŒ©‚à∂ùúî‚ààB‚à©{X=x}
‚Ñô[{ùúî}]
= ‚Ñô[{X = x} ‚à©B],
so by comparison we see that the Ô¨Ånal
expression in (1.36) is indeed ùîº[1BX]‚àï‚Ñô[B].
Let (Ei, i ‚ààI) be a partition of Œ©, so
‚àë
i‚ààI 1Ei = 1. Hence,
ùîº[X] = ùîº
[
X
‚àë
i‚ààI
1Ei
]
= ùîº
[ ‚àë
i‚ààI
X1Ei
]
=
‚àë
i‚ààI
ùîº[X1Ei],
by linearity. By (1.35), ùîº[X1Ei] = ùîº[X ‚à£
Ei]‚Ñô[Ei]. Thus we verify the partition
theorem for expectations:
ùîº[X] =
‚àë
i‚ààI
ùîº[X ‚à£Ei]‚Ñô[Ei].
Given two discrete random variables X
and Y, the conditional expectation of X
given Y, denoted ùîº[X ‚à£Y], is the random
variable
ùîº[X ‚à£Y](ùúî) = ùîº[X ‚à£Y = Y(ùúî)],
which
takes
values
ùîº[X ‚à£Y = y]
with
probabilities ‚Ñô[Y = y].
References
1. Durrett, R. (2010) Probability: Theory and
Examples, 4th edn, Cambridge University
Press, Cambridge.
2. Kallenberg, O. (2002) Foundations of
Modern Probability, 2nd edn,
Springer-Verlag, New York.
3. Fischer, H. (2011) A History of the Central
Limit Theorem, Springer-Verlag, New York.
4. Ehrenfest, P. and Ehrenfest, T. (1907) √úber
zwei bekannte Einw√§nde gegen das
Boltzmannsche H-Theorem. Phys. Z., 8,
311‚Äì314.
5. Karlin, S. and Taylor, H.M. (1975) A First
Course in Stochastic Processes, 2nd edn,
Academic Press.
6. Norris, J.R. (1997) Markov Chains,
Cambridge University Press, Cambridge.
7. Chung, K.L. (1967) Markov Chains with
Stationary Transition Probabilities, 2nd edn,
Springer-Verlag, Berlin.
8. Lamperti, J. (1960) Criteria for the
recurrence or transience of stochastic
process. I. J. Math. Anal. Appl., 1, 314‚Äì330.
9. Feller, W. (1971) An Introduction to
Probability Theory and its Applications, Vol.
II, 2nd edn, John Wiley & Sons, Inc., New
York.
10. Bak, P. and Sneppen, K. (1993) Punctuated
equilibrium and criticality in a simple model
of evolution. Phys. Rev. Lett., 71, 4083‚Äì4086.
11.
Gillett, A.J. (2007) Phase Transitions in
Bak‚ÄìSneppen Avalanches and in a
Continuum Percolation Model. PhD
dissertation, Vrije Universiteit, Amsterdam.
12. Grinfeld, M., Knight, P.A., and Wade, A.R.
(2012) Rank-driven Markov processes. J.
Stat. Phys., 146, 378‚Äì407.
13. Rayleigh, L. (1880) On the resultant of a large
number of vibrations of the same pitch and
of arbitrary phase. Philos. Mag., 10, 73‚Äì78.
14. Bachelier, L. (1900) Th√©orie de la
sp√©culation. Ann. Sci. √âcole Norm. Sup., 17,
21‚Äì86.

38
1 Stochastic Processes
15. Pearson, K. and Blakeman, J. (1906) A
Mathematical Theory of Random Migration,
Drapers‚Äô Company Research Memoirs
Biometric Series, Dulau and co., London.
16. Einstein, A. (1956) Investigations on the
Theory of the Brownian Movement, Dover
Publications Inc., New York.
17. P√≥lya, G. (1921) √úber eine Aufgabe der
Wahrscheinlichkeitsrechnung betreÔ¨Äend die
Irrfahrt im Stra√üennetz. Math. Ann., 84,
149‚Äì160.
18. Pearson, K. (1905) The problem of the
random walk. Nature, 72, 342.
19. Hughes, B.D. (1995) Random Walks and
Random Environments, vol. 1, Oxford
University Press, New York.
20. Feller, W. (1968) An Introduction to
Probability Theory and its Applications, Vol.
I, 3rd edn, John Wiley & Sons, Inc., New
York.
21. Doherty, M. (1975) An Amusing Proof in
Fluctuation Theory, Combinatorial
Mathematics III, vol. 452, Springer,
101‚Äì104.
22. Ganesh, A., O‚ÄôConnell, N., and Wischik, D.
(2004) Big Queues, Springer, Berlin.
23. Foss, S., Korshunov, D., and Zachary, S.
(2011) An Introduction to Heavy-Tailed and
Subexponential Distributions,
Springer-Verlag.
24. Dembo, A. and Zeitouni, O. (1998) Large
Deviations Techniques and Applications,
Springer, New York.
25. Touchette, H. (2009) The large deviation
approach to statistical mechanics. Phys. Rep.,
478, 1‚Äì69.
26. Karlin, S. and Taylor, H.M. (1981) A Second
Course in Stochastic Processes, Academic
Press.
27. Lax, M., Cai, W., and Xu, M. (2006) Random
Processes in Physics and Finance, Oxford
University Press.
28. Blackman, J.A. and Mulheran, P.A. (1996)
Scaling behaviour in submonolayer Ô¨Ålm
growth: a one-dimensional model. Phys. Rev.
B, 54, 11 681.
29. O‚ÄôNeill, K.P., Grinfeld, M., Lamb, W., and
Mulheran, P.A. (2012) Gap-size and
capture-zone distributions in
one-dimensional point-island nucleation and
growth simulations: Asymptotics and
models. Phys. Rev. E, 85, 21 601.
30. Ising, E. (1925) Beitrag zur Theorie des
Ferromagnetismus. Z. Phys., 31,
253‚Äì258.
31. Grimmett, G. (2010) Probability on Graphs,
Cambridge University Press.
32. Lawler, G.F. (2005) Conformally Invariant
Processes in the Plane, American
Mathematical Society.
33. Grimmett, G. (1999) Percolation, 2nd edn,
Springer-Verlag, Berlin.
34. Doyle, P.G. and Snell, J.L. (1984) Random
Walks and Electric Networks, Mathematical
Association of America, Washington, DC.
35. Kac, M. (1959) Probability and Related
Topics in Physical Sciences, vol. 1957,
Interscience Publishers, London and New
York.
36. Shlesinger, M.F. and West, B.J. (1984)
Random Walks and Their Applications on
the Physical and Biological Sciences,
American Institute of Physics, New York.
37.
Weiss, G.H. and Rubin, R.J. (1983) Random
walks: theory and selected applications. Adv.
Chem. Phys., 52, 363‚Äì505.
38. Streater, R.F. (2007) Lost Causes in and
Beyond Physics, Springer-Verlag.
39. Billingsley, P. (1995) Probability and
Measure, 3rd edn, John Wiley & Sons, Inc.
New York.
40. Chung, K.L. (2001) A Course in Probability
Theory, 3rd edn, Academic Press Inc., San
Diego, CA.

39
2
Monte-Carlo Methods
Kurt Binder
2.1
Introduction and Overview
Many
problems
in
science
are
very
complex: for example, statistical ther-
modynamics considers thermal properties
of matter resulting from the interplay of a
large number of particles. A deterministic
description in terms of the equations of
motion of all these particles would make
no sense and a probabilistic description
is required. A probabilistic description
may even be intrinsically implied by the
quantum-mechanical nature of the basic
processes (e.g., emission of neutrons in
radioactive decay) or because the prob-
lem is incompletely characterized, only
some degrees of freedom being considered
explicitly, while the others act as a kind of
background causing random noise. While
thus the concept of probability distribu-
tions is ubiquitous in physics, often it is
not possible to compute these probabil-
ity distribution functions analytically in
explicit form, because of the complexity
of the problem. For example, interac-
tions between atoms in a Ô¨Çuid produce
strong and nontrivial correlations between
atomic positions, and, hence, it is not
possible to calculate these correlations
analytically.
Monte-Carlo methods now aim at a
numerical estimation of probability dis-
tributions (as well as of averages that can
be calculated from them), making use of
(pseudo) random numbers. By ‚Äúpseudo-
random numbers‚Äù one means a sequence
of numbers produced on a computer with
a deterministic procedure from a suitable
‚Äúseed.‚Äù Hence, this sequence is not truly
random: See Section 2.2 for a discussion of
this problem.
The outline of the present article is
as follows. As all Monte-Carlo methods
heavily rely on the use of random num-
bers, we brieÔ¨Çy review random-number
generation in Section 2.2. In Section 2.3,
we then elaborate on the discussion of
‚Äúsimple sampling,‚Äù that is, problems where
a straightforward generation of probabil-
ity distributions using random numbers
is possible. Section 2.4 brieÔ¨Çy mentions
some applications to transport problems,
such as radiation shielding, and growth
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

40
2 Monte-Carlo Methods
phenomena,
such
as
‚ÄúdiÔ¨Äusion-limited
aggregation‚Äù (DLA).
Section 2.5
then
considers
the
importance-sampling
methods
of
sta-
tistical
thermodynamics,
including
the
use of diÔ¨Äerent thermodynamic ensem-
bles. This chapter emphasizes applications
in
statistical
mechanics
of
condensed
matter, because this is the Ô¨Åeld where
most activity with Monte-Carlo methods
occurs.
Some more practical aspects important
for the implementation of algorithms and
the judgment of the tractability of sim-
ulation approaches to physical problems
are then considered in Section 2.6: eÔ¨Äects
resulting from the Ô¨Ånite size of simulation
boxes, eÔ¨Äects of choosing various bound-
ary conditions, dynamic correlation of
errors, and the application to studies of
the dynamics of thermal Ô¨Çuctuations. In
addition, methods for the sampling of free
energies and of free energy barriers in con-
Ô¨Åguration space will be brieÔ¨Çy discussed
(Section 2.7).
The extension to quantum-mechanical
problems is mentioned in Section 2.8 and
the application to elementary particle
theory (lattice gauge theory) in Section 2.9.
Section 2.10
then
illustrates
some
of
the general concepts with a variety of
applications taken from condensed mat-
ter physics, while Section 2.11 contains
concluding remarks.
We do not discuss problems of applied
mathematics such as applications to the
solution
of
linear
operator
equations
(Fredholm integral equations, the Dirich-
let boundary-value problem, eigenvalue
problems, etc.); for a concise discussion
of such problems, see, for example, Ham-
mersley and Handscomb [1]. Nor do we
discuss simulations of chemical kinetics,
such as polymerization processes (see, e.g.,
[2]).
2.2
Random-Number Generation
2.2.1
General Introduction
The precise deÔ¨Ånition of ‚Äúrandomness‚Äù is a
problem in itself (see, e.g., [3]) and is out-
side the scope of this chapter. Truly random
numbers are unpredictable in advance and
must be produced by an appropriate phys-
ical process such as radioactive decay, but
are not useful for computer simulation in
practice.
Pseudorandom numbers are produced in
the computer by one of various algorithms,
some of which will be discussed below, and
thus are predictable, as their sequence is
exactly reproducible. (This reproducibility,
of course, is desirable, as it allows detailed
checks of Monte-Carlo simulation pro-
grams.) They are thus not truly random,
but they have statistical properties (nearly
uniform
distribution,
nearly
vanishing
correlation coeÔ¨Écients, etc.) that are very
similar to the statistical properties of truly
random numbers. Thus, a given sequence
of
(pseudo)random
numbers
appears
‚Äúrandom‚Äù for many practical purposes. In
the following, the preÔ¨Åx ‚Äúpseudo‚Äù will be
omitted throughout.
2.2.2
Properties That a Random-Number
Generator (RNG) Should Have
What one needs are numbers that are uni-
formly distributed in the interval [0,1] and
that are uncorrelated. By ‚Äúuncorrelated‚Äù we
not only mean vanishing pair correlations
for arbitrary distances along the random-
number
sequence,
but
also
vanishing
triplet and higher correlations. No algo-
rithm exists that satisÔ¨Åes these desirable
requirements fully, of course; the extent to

2.2 Random-Number Generation
41
which the remaining correlations between
the generated random numbers lead to
erroneous results of Monte-Carlo simula-
tions has been a matter of long-standing
concern
[4‚Äì6];
even
random-number
generators (RNGs) that have passed all
common statistical tests and have been
used successfully for years may fail for a
new application, in particular, if it involves
a new type of Monte-Carlo algorithm.
Therefore, the testing of RNGs is a Ô¨Åeld of
research in itself (see, e.g., [7‚Äì9]).
A limitation due to the Ô¨Ånite word
length of computers is the Ô¨Ånite period:
Every generator begins after a long but
Ô¨Ånite period to produce exactly the same
sequence
again.
For
example,
simple
generators for 32-bit computers have a
maximum period of 230 (‚âà109) numbers
only. This is not enough for recent high-
quality applications! Of course, one can
get around this problem [4, 5], but, at
the same time, one likes the code rep-
resenting the RNG to be ‚Äúportable‚Äù (i.e.,
in a high-level programming language
such as FORTRAN usable for computers
from diÔ¨Äerent manufacturers) and ‚ÄúeÔ¨É-
cient‚Äù (i.e., extremely fast so it does not
unduly slow down the simulation pro-
gram as a whole). Thus, inventing new
RNGs that are in certain respects a better
compromise between these partially con-
Ô¨Çicting requirements is still of interest (e.g.,
[10]).
2.2.3
Comments about a Few Frequently Used
Generators
The best known among the frequently used
RNG is the linear multiplicative algorithm
[11], which produces random integers Xi
recursively from the formula
xi = aXi‚àí1 + c
(modulo
m),
(2.1)
which means that m is added when the
result is negative, but multiples of m are
subtracted when the result is larger than
m. For 32-bit computers, m = 231 ‚àí1 (the
largest integer that can be used for that
computer). The integer constants a, c, and
X0 (the starting value of the recursion, the
so-called ‚Äúseed‚Äù) need to be appropriately
chosen (e.g., a = 16 807, c = 0, X0 odd).
Obviously, the ‚Äúrandomness‚Äù of the Xi
results because, after a few multiplications
with a, the result would exceed m and
hence is truncated, and so the leading
digits of Xi are more or less random. But
there are severe correlations: if d-tuples of
such numbers are used to represent points
in d-dimensional space having a lattice
structure, they lie on a certain number of
hyperplanes [12].
Equation (2.1) produces random num-
bers between 0 and m. Converting them
into real numbers and dividing by m yields
random numbers in the interval [0,1], as
desired.
More popular now are shift-register gen-
erators [13, 14] based on the formula
Xi = Xi‚àíp ‚ãÖXOR ‚ãÖXi‚àíq,
(2.2)
where ‚ãÖXOR‚ãÖis the bitwise ‚Äúexclusive or‚Äù
operation, and the ‚Äúlags‚Äù p, q have to be
properly chosen (the popular ‚ÄúR250‚Äù [14]
uses p = 109, q = 250 and thus needs 250
initializing integers). ‚ÄúGood‚Äù generators
based on (2.2) have fewer correlations
between random numbers than those
resulting from (2.1), and much larger
period.
A third type of generators is based on the
Fibonacci series and is also recommended
in the literature [4, 5, 15]. But a general
recommendation is that users of random
numbers should not rely on their quality
blindly and should perform their own tests
in the context of the application.

42
2 Monte-Carlo Methods
2.3
Simple Sampling of Probability
Distributions Using Random Numbers
In this section, we give a few simple
examples
of
the
use
of
Monte-Carlo
methods that will be useful for the under-
standing of later sections. More material
on this subject can be found in standard
textbooks such as Koonin [16] and Gould
and Tobochnik [17].
2.3.1
Numerical Estimation of Known
Probability Distributions
A known probability distribution in which
a (discrete) state i, 1 ‚â§i ‚â§n, occurs with
probability pi with ‚àën
i=1 pi = 1, is numeri-
cally realized using random numbers uni-
formly distributed in the interval from zero
to unity as follows: deÔ¨Åning Pi = ‚àëi
j=1 pi, we
choose state i if the random number ùúÅsatis-
Ô¨Åes Pi‚àí1 < ùúÅ< Pi, with P0 = 0. In the limit
of a large number (M) of trials, the gen-
erated distribution approximates pi, with
errors of order 1‚àï
‚àö
M.
Monte-Carlo
methods
in
statistical
mechanics can be viewed as an extension
of this simple concept to the situation
in which the probability that a point X
in phase space occurs is given by the
Boltzmann probability
Peq(X) = 1
Z exp
[‚àí‚Ñã(X)
kBT
]
,
kB being Boltzmann‚Äôs constant, T the abso-
lute temperature, and Z = ‚àëexp
[
‚àí‚Ñã(X)
kBT
]
the partition function, although in general
neither Z nor Peq(X) can be written explic-
itly (as function of the variables of interest,
such as T, particle number N, volume V,
etc.). The term ‚Ñã(X) denotes the Hamilto-
nian of the (classical) system.
2.3.2
‚ÄúImportance Sampling‚Äù versus ‚ÄúSimple
Sampling‚Äù
The sampling of the Boltzmann probability
Peq(X) by Monte-Carlo methods is not
completely
straightforward:
One
must
not choose the points X in phase space
completely at random, because Peq(X)
is extremely sharply peaked. Thus, one
needs
‚Äúimportance-sampling‚Äù
methods
that generate points X preferably from the
‚Äúimportant‚Äù region of space where this
narrow peak occurs.
Before we treat this problem of statisti-
cal mechanics in more detail, we empha-
size the more straightforward applications
of ‚Äúsimple sampling‚Äù techniques. In the fol-
lowing, we list a few problems where sim-
ple sampling is useful. Suppose one wishes
to generate a conÔ¨Åguration of a randomly
mixed crystal of a given lattice structure,
for example, a binary mixture of compo-
sition AxB1‚àíx. Again, one uses pseudoran-
dom numbers ùúÅuniformly distributed in
[0,1] to choose the occupancy of lattice sites
{j}: If ùúÅj < x, the site j is taken by an A atom,
else by a B atom. Such conÔ¨Ågurations now
can be used as starting point for a numerical
study of the dynamical matrix if one is inter-
ested in the phonon spectrum of mixed
crystals. One can study the distribution of
sizes of ‚Äúclusters‚Äù formed by neighboring A
atoms if one is interested in the ‚Äúsite perco-
lation problem‚Äù [18], and so on.
If one is interested in simulating trans-
port processes such as diÔ¨Äusion, a basic
approach is the generation of simple ran-
dom walks (RWs). Such RWs, resulting
from addition of vectors whose orientation
is random, can be generated both on lattices
and in the continuum. Such simulations are
desirable if one wishes to consider compli-
cated geometries or boundary conditions
of the medium where the diÔ¨Äusion takes

2.3 Simple Sampling of Probability Distributions Using Random Numbers
43
place. In addition, it is straightforward
to include competing processes (e.g., in
a reactor, diÔ¨Äusion of neutrons in the
moderator competes with loss of neutrons
due to nuclear reactions, radiation going
to the outside, etc., or gain due to Ô¨Åssion
events). Actually, this problem of reactor
criticality (and related problems for nuclear
weapons!) was the starting point for the
Ô¨Årst large-scale applications of Monte-
Carlo methods by Fermi, von Neumann,
Ulam, and their coworkers [1].
2.3.3
Monte-Carlo as a Method of Integration
Many Monte-Carlo computations may be
viewed as attempts to estimate the value
of (multiple) integrals. To give the Ô¨Çavor of
this idea, we discuss the one-dimensional
integral
I =
1
‚à´
0
f (x) dx
‚â°
1
‚à´
0
1
‚à´
0
g (x, y) dxdy; with
(2.3)
g(x, y) =
{0 if
f (x) < y,
1 if
f (x) ‚â•y,
}
as an example (suppose, for simplicity, that
also 0 ‚â§f (x) ‚â§1 for 0 ‚â§x ‚â§1). Then I sim-
ply is interpreted as the fraction of the
unit square 0 ‚â§x, y ‚â§1 lying underneath
the curve y = f (x). Now a straightforward
(though often not very eÔ¨Écient) Monte-
Carlo estimation of (2.3) is the ‚Äúhit-or-miss‚Äù
method: We take n points (ùúÅx, ùúÅy) uniformly
distributed in the unit square 0 ‚â§ùúÅx ‚â§1,
0 ‚â§ùúÅy ‚â§1. Then I is estimated by the aver-
age g of g(x,y),
g = 1
n
n
‚àë
i=1
g (ùúÅxi, ùúÅyi
) = n‚àó
n ,
(2.4)
n* being the number of points for which
f (ùúÅxi) ‚â•ùúÅyi. Thus, we count the fraction of
points that lie underneath the curve y =
f (x). Of course, such Monte-Carlo meth-
ods are inferior to many other techniques
of numerical integration, if the integration
space is low dimensional, but the situa-
tion for standard integration methods is
worse for high-dimensional spaces: For any
method using a regular grid of points for
which the integrand needs to be evaluated,
the number of points sampled along each
coordinate is M1‚àïd in d dimensions, which
is small for any reasonable sample size M if
d is very large.
2.3.4
InÔ¨Ånite Integration Space
Not always is the integration space limited
to a bounded interval in space. For example,
the ùúô4 model of Ô¨Åeld theory considers a Ô¨Åeld
variable ùúô(x), where x is drawn from a d-
dimensional space and ùúô(x) is a real variable
with distribution
P(ùúô) ‚àùexp
[
‚àíùõº
(
‚àí1
2ùúô2 + 1
4ùúô4)]
;
ùõº> 0.
(2.5a)
While ‚àí‚àû< ùúô< +‚àû, the distribution
P‚Ä≤(y)
P‚Ä≤(y) =
‚à´
y
‚àí‚àûP(ùúô)dùúô
‚à´
+‚àû
‚àí‚àûP(ùúô)dùúô
(2.5b)
varies in the unit interval, 0 ‚â§P‚Ä≤ ‚â§1.
Hence, deÔ¨Åning Y = Y(P‚Ä≤) as the inverse
function of P‚Ä≤(y), we can choose a ran-
dom
number
ùúÅ
uniformly
distributed
between zero and one, to obtain ùúô= Y(ùúÅ)
distributed according to the chosen dis-
tribution P(ùúô). Of course, this method
works not only for the example chosen in
(2.5a) but for any distribution of interest.
Often it will not be possible to obtain Y(P‚Ä≤)
analytically, but then one can compute

44
2 Monte-Carlo Methods
numerically a table before the start of the
sampling [19].
2.3.5
Random Selection of Lattice Sites
A problem that occurs very frequently (e.g.,
in solid-state physics) is one that considers
a large lattice (e.g., a model of a simple
cubic crystal with N = Lx √ó Ly √ó Lz sites),
and one wishes to select a lattice site (nx,
ny, nz) at random. This is trivially done
using the integer arithmetics of standard
computers, converting a uniformly dis-
tributed random number ùúÅx(0 ‚â§ùúÅx < 1)
to an integer nx with 1 ‚â§nx ‚â§Lx via
the statement nx = int(ùúÅxLx + 1). This is
already an example where one must be
careful, however, when three successive
pseudorandom numbers drawn from an
RNG are used for this purpose: If one
uses an RNG with bad statistical qualities,
the frequency with which individual sites
are visited may deviate distinctly from a
truly random choice. In unfavorable cases,
successive pseudorandom numbers are so
strongly correlated that certain lattice sites
would never be visited.
2.3.6
The Self-Avoiding Walk Problem
As an example of the straightforward use
of simple sampling techniques, we now
discuss the study of self-avoiding walks
(SAWs) on lattices (which may be con-
sidered as a simple model for polymer
chains in good solvents; see [20]). Suppose
one considers a square or simple cubic
lattice with coordination number (number
of nearest neighbors) z. Then, for a RW
with N steps, we would have ZRW = zN
conÔ¨Ågurations, but many of these RWs
intersect themselves and thus would not be
self-avoiding. For SAWs, one only expects
of the order of ZSAW = const. √ó Nùõæ‚àí1zN
eÔ¨Ä
conÔ¨Ågurations, where ùõæ> 1 is a character-
istic exponent (which is not known exactly
for d = 3 dimensions), and zeÔ¨Ä‚â§z ‚àí1 is
an eÔ¨Äective coordination number, which
also is not known exactly. But it is already
obvious that an exact enumeration of
all conÔ¨Ågurations would be possible for
rather small N only, while most questions
of interest refer to the behavior for large
N; for example, one wishes to study the
end-to-end distance of the SAW,
‚ü®R2‚ü©
SAW =
1
ZSAW
‚àë
X
[R(X)]2,
(2.6)
the sum being extended over all conÔ¨Ågura-
tions of SAWs which we denote formally as
points X in phase space. One expects that
‚ü®R2‚ü©
SAW ‚àùN2ùúà, where v is another charac-
teristic exponent. A Monte-Carlo estima-
tion of
‚ü®
R2‚ü©
SAW now is based on generating
a sample of only M
‚â™
ZSAW conÔ¨Ågura-
tions Xl, that is,
R2 = 1
M
M
‚àë
l=1
[R (Xl
)]2 ‚âà‚ü®R2‚ü©
SAW .
(2.7)
If the M conÔ¨Ågurations are statistically
independent,
standard
error
analysis
applies, and we expect that the relative
error behaves as
(ùõøR2)2
(
R2
)2 ‚âà
1
M ‚àí1
[‚ü®
R4‚ü©
SAW
‚ü®R2‚ü©2
SAW
‚àí1
]
.
(2.8)
While the law of large numbers then
implies that R2 is Gaussian distributed
around ‚ü®R2‚ü©
SAW with a variance deter-
mined by (2.8), one should note that the
variance does not decrease with increas-
ing N. Statistical mechanics tells us that
Ô¨Çuctuations decrease with increasing the
number of degrees of freedom N; that is,
one equilibrium conÔ¨Åguration diÔ¨Äers in its

2.3 Simple Sampling of Probability Distributions Using Random Numbers
45
energy E(x) from the average ‚ü®E‚ü©only by
an amount of order 1‚àï
‚àö
N. This property
is called self-averaging. Obviously, such
a property is not true for ‚ü®R2‚ü©
SAW. This
‚Äúlack of self-averaging‚Äù is easy to show for
ordinary RWs [21].
2.3.7
Simple Sampling versus Biased Sampling:
the Example of SAWs Continued
Apart from this problem, that the accu-
racy of the estimation of ‚ü®R2‚ü©does
not increase with the number of steps
of the walk, it is also not easy to gen-
erate a large sample of conÔ¨Ågurations
of SAWs for large N. Suppose we do
this at each step by choosing one of
z ‚àí1
neighbors
at
random
(eliminat-
ing, from the start, immediate reversals,
which would violate the SAW condi-
tion). Whenever the chosen lattice site
is
already
taken,
the
attempted
walk
must be terminated. Now the fraction
of walks that will continue successfully
for N steps will only be of the order of
ZSAW‚àï(z ‚àí1)N
‚àù[zeÔ¨Ä‚àï(z ‚àí1)]NNùõæ‚àí1,
which decreases to zero exponentially
(‚àùexp(‚àíNùúÖ) with ùúÖ= ln[(z ‚àí1)‚àïzeÔ¨Ä] for
large N); this failure of success in gen-
erating long SAWs is called the attrition
problem.
The obvious recipe, to select at each step
only from among the lattice sites that do
not violate the SAW restriction, does not
give equal statistical weight for each con-
Ô¨Åguration generated, of course, and so the
average would not be the averaging that one
needs in (2.6). One Ô¨Ånds that this method
would create a ‚Äúbias‚Äù toward more com-
pact conÔ¨Ågurations of the walk. But one
can calculate the weights of conÔ¨Ågurations
w(X) that result in this so-called ‚Äúinversely
restricted sampling‚Äù [22], and, in this way,
correct for the bias and estimate the SAW
averages as
R2 =
{ M
‚àë
l=1
[w (Xl
)]‚àí1
}‚àí1
√ó
M
‚àë
l=1
[w (Xl
)]‚àí1 [R (Xl
)]2 .
(2.9)
However, error analysis of this biased sam-
pling is delicate [20].
A popular alternative to overcome the
above attrition problem is the ‚Äúenrich-
ment technique,‚Äù founded on the principle
‚ÄúHold fast to that which is good.‚Äù Namely,
whenever a walk attains a length that is
a multiple of s steps without intersecting
itself, n independent attempts to con-
tinue it (rather than a single attempt) are
made. The numbers n, s are Ô¨Åxed, and, if
we choose n ‚âàexp(ùúÖs), the numbers of
walks of various lengths generated will
be approximately equal. Enrichment has
the advantage over inversely restricted
sampling that all walks of a given length
have equal weights, while the weights in
(2.9) vary over many orders of magnitude
for large N. But the linear dimensions of
the walks are highly correlated, because
some of them have many steps in common.
For these reasons, simple sampling and
its extensions are useful only for a few
problems in polymer science; ‚Äúimportance
sampling‚Äù (Section 2.5) is much more
used. But we emphasize that related prob-
lems are encountered for the sampling of
‚Äúrandom surfaces‚Äù (this problem arises in
the Ô¨Åeld theory of quantum gravity), in
path-integral Monte-Carlo treatments of
quantum problems, and in many other
contexts.
A variant of such techniques that still
is widely used to simulate single polymer
chains is the PERM algorithm (‚Äúpruned-
enriched Rosenbluth method,‚Äù [23, 24]).
In this method, a population of chains

46
2 Monte-Carlo Methods
growing in their chain length step by step is
generated and their statistical weights are
considered. From time to time, members
of the population with low weight are
removed (‚Äúpruning‚Äù) and members with
high weight are enriched.
2.4
Survey of Applications to Simulation of
Transport Processes
There are many possibilities to simulate the
random motion of particles. Therefore, it
is diÔ¨Écult to comment about such prob-
lems in general. Thus, we only discuss a few
examples that illustrate the spirit of such
approaches.
2.4.1
The ‚ÄúShielding Problem‚Äù
A thick shield of absorbing material is
exposed to ùõæradiation (energetic pho-
tons), of speciÔ¨Åed distribution of energy
and angle of incidence. We want to know
the
intensity
and
energy
distribution
of
the
radiation
that
penetrates
that
shield.
The description is here that one gen-
erates a lot of ‚Äúhistories‚Äù of those parti-
cles traveling through the medium. The
paths of these ùõæparticles between scat-
tering events are straight lines and diÔ¨Äer-
ent ùõæparticles do not interact with each
other. A particle with energy E, instan-
taneously at the point r and traveling in
the direction of the unit vector w, contin-
ues to travel in the same direction with
the same energy, until a scattering event
with an atom of the medium occurs. The
standard assumption is that these atoms
are distributed randomly in space. Then
the total probability that the particle col-
lides with an atom while traveling a length
ùõøs of its path is ùúéc(E)ùõøs, ùúéc(E) being the
cross section. In a region of space where
ùúéc(E) is constant, the probability that a
particle travels without collision a distance
s is Fc(s) = 1 ‚àíexp[‚àíùúéc(E)s]. If a collision
occurs, it leads to absorption or scattering,
and the cross sections for these types of
events are assumed to be known.
A Monte-Carlo solution now simply
involves the tracking of simulated parti-
cles from collision to collision, generating
the distances s that the particles travel
without collision from the exponential
distribution quoted above. Particles leaving
a collision point are sampled from the
appropriate conditional probabilities as
determined from the respective diÔ¨Äerential
cross sections. For increasing sampling
eÔ¨Éciency, many obvious tricks are known.
For example, one may avoid losing particles
by absorption events: If the absorption
probability (i.e., the conditional probability
that absorption occurs given that a collision
has occurred) is ùõº, one may replace ùúéc(E)
by ùúéc(E)(1 ‚àíùõº), and allow only scattering
to take place with the appropriate rela-
tive probability. Special methods for the
shielding problem have been extensively
developed already and have been reviewed
by Hammersley and Handscomb [1].
2.4.2
DiÔ¨Äusion-Limited Aggregation (DLA)
DLA is a model for the irreversible for-
mation of random aggregates by diÔ¨Äusion
of particles that get stuck at random posi-
tions on the already formed object if they
hit its surface in the course of their dif-
fusion (see Vicsek [25], Meakin [26], and
Herrmann [19] for detailed reviews of this
problem and related phenomena). Many
structures (shapes of snowÔ¨Çakes, size dis-
tribution of asteroids, roughness of crack
surfaces, etc.) can be understood as the

2.5 Monte-Carlo Methods in Statistical Thermodynamics: Importance Sampling
47
end product of similar random irreversible
growth processes. DLA is just one example
of them. It is simulated by iterating the
following steps: From a randomly selected
position on a spherical surface of radius Rm
that encloses the aggregate (grown in the
previous steps, its center of gravity being
in the center of the sphere), a particle of
unit mass is launched to start a simple
RW trajectory. If it touches the aggregate,
it sticks irreversibly on its surface. After
the particle has either stuck or moved a
distance Rf from the center of the aggre-
gate such that it is unlikely that it will hit
in the future, a new particle is launched.
Ideally, one would like to have Rf ‚Üí‚àû
but, in practice, Rf = 2Rm suÔ¨Éces. By this
irreversible aggregation of particles, one
forms fractal clusters. This means that the
dimension df characterizing the relation
between the mass of the grown object and
its (gyration) radius R, M ‚àùRdf, is less than
the dimension d of space in which the
growth takes place. Again there are some
tricks to make such simulations more eÔ¨É-
cient: For example, one may allow the par-
ticles to jump over larger steps when they
travel in empty regions. From such studies,
researchers have found that df = 1.715 ¬±
0.004 for DLA in d = 2, while df = 2.485 ¬±
0.005 in d = 3 [27].
2.5
Monte-Carlo Methods in Statistical
Thermodynamics: Importance Sampling
2.5.1
The General Idea of the Metropolis
Importance-Sampling Method
In the canonical ensemble, the average of an
observable A(X) takes the form
‚ü®A‚ü©= 1
Z ‚à´Œ©
dkXA(X)exp
[
‚àí‚Ñã(X)
kBT
]
, (2.10)
where Z is the partition function,
Z = ‚à´Œ©
dkXexp
[
‚àí‚Ñã(X)
kBT
]
,
(2.11)
Œ© denoting the (k-dimensional) volume of
phase space {X} over which the integra-
tion is extended, ‚Ñã(X) being the (classical)
Hamiltonian. For this problem, a simple
sampling analogue to Section 2.3 would not
work: The probability distribution p(X) =
(1‚àïZ) exp[‚àí‚Ñã(X)‚àïkBT] has a very sharp
peak in phase space where all extensive
variables A(X) are close to their average val-
ues ‚ü®A‚ü©. This peak may be approximated
by a Gaussian centered at ‚ü®A‚ü©, with a rela-
tive half-width of order 1‚àï
‚àö
N only, if we
consider a system of N particles. Hence,
for a practically useful method, one can-
not sample the phase space uniformly, but
the points Xùúàmust be chosen preferentially
from the important region of phase space,
that is, from the vicinity of the peak of p(X).
This goal is achieved by the importance-
sampling method [28]: Starting from some
initial conÔ¨Åguration X1, one constructs a
sequence of conÔ¨Ågurations XùúàdeÔ¨Åned in
terms of a transition probability W(Xùúà‚Üí
X‚Ä≤
ùúà) that rules stochastic ‚Äúmoves‚Äù from an
old state Xùúàto a new state X‚Ä≤
ùúà, and, hence,
one creates a ‚ÄúRW through phase space.‚Äù
The idea is to choose W(X ‚ÜíX‚Ä≤) such that
the probability with which a point X is cho-
sen converges toward the canonical proba-
bility
Peq(X) =
( 1
Z
)
exp
[
‚àí‚Ñã(X)
kBT
]
in the limit where the number M of states
X generated goes to inÔ¨Ånity. A condition
suÔ¨Écient to ensure this convergence is the
so-called principle of detailed balance,
Peq(X)W(X ‚ÜíX‚Ä≤) = Peq(X‚Ä≤)W(X‚Ä≤ ‚àíX).
(2.12)

48
2 Monte-Carlo Methods
For a justiÔ¨Åcation that (2.12) actually yields
this desired convergence, we refer to Ham-
mersley and Handscomb [1], Binder [29],
Binder and Heermann [30], and Kalos
and Whitlock [31]. In this importance-
sampling technique, the average (2.10)
then is estimated in terms of a simple
arithmetic average,
A =
1
M ‚àíM0
M
‚àë
v = M0+1
A(Xv).
(2.13)
Here it is anticipated that it is advantageous
to eliminate the residual inÔ¨Çuence of the
initial conÔ¨Åguration X1 by eliminating a
large enough number M0 of states from
the average. (The judgment of what is
‚Äúlarge enough‚Äù is often diÔ¨Écult; see Binder
[29] and Section 2.5.3 below.) Note that
this Metropolis method can be used for
sampling any distribution P(X): One sim-
ply must choose a transition probability
W(X‚ÜíX‚Ä≤) that satisÔ¨Åes a detailed bal-
ance condition with P(X) rather than with
Peq(X).
2.5.2
Comments on the Formulation of a
Monte-Carlo Algorithm
What is now meant in practice by the tran-
sition X‚ÜíX‚Ä≤? Again, there is no general
answer to this question; the choice of the
process may depend both on the model
studied and the purpose of the study. As
(2.12) implies that W(X ‚àíX‚Ä≤)‚àïW(X‚Ä≤ ‚Üí
X) = exp(‚àíùõø‚Ñã‚àïkBT),
ùõø‚Ñã
being
the
energy change caused by the move from
X‚ÜíX‚Ä≤, typically it is necessary to con-
sider small changes of the state X only.
Otherwise, |ùõø‚Ñã| would be rather large,
and then either W(X‚ÜíX‚Ä≤) or W(X‚Ä≤ ‚ÜíX)
would be very small, and the procedure
would be poorly convergent. For example,
in the lattice gas model at constant particle
number, a transition X‚ÜíX‚Ä≤ may consist
of moving one particle to a randomly
chosen neighboring site. In the lattice
gas at constant chemical potential, one
removes (or adds) just one particle at
a time, which is isomorphic to single
Ô¨Çips in the Ising model of anisotropic
magnets.
Another arbitrariness concerns the order
in which the particles are selected for con-
sidering a move. Often, one selects them
in the order of their labels (in the simula-
tion of a Ô¨Çuid at constant particle number)
or to go through the lattice in a regu-
lar typewriter-type manner (in the case of
spin models, for instance). For lattice sys-
tems, it may be convenient to use sub-
lattices (e.g., the ‚Äúcheckerboard algorithm,‚Äù
where the white and black sublattices are
updated in alternation, for the sake of an
eÔ¨Écient ‚Äúvectorization‚Äù of the program; see
[32]). An alternative is to choose the lat-
tice sites (or particle numbers) randomly.
The latter procedure is somewhat more
time consuming, but it is a more faithful
representation of a dynamic time evolu-
tion of the model described by a master
equation (see below).
It is also helpful to realize that often
the transition probability W(X‚ÜíX‚Ä≤) can
be written as a product of an ‚Äúattempt
frequency‚Äù
times
an
‚Äúacceptance
fre-
quency.‚Äù By clever choice of the attempt
frequency, one sometimes can attempt
large moves and still have a high accep-
tance, making the computations more
eÔ¨Écient.
For spin models on lattices, such as
Ising or Potts models, XY, and Heisen-
berg ferromagnets, and so on, algorithms
have been devised where one does not
update single spins in the move X‚ÜíX‚Ä≤,
but, rather, one updates specially con-
structed clusters of spins (see [33], for a

2.5 Monte-Carlo Methods in Statistical Thermodynamics: Importance Sampling
49
review). These algorithms have the merit
that they reduce critical slowing down,
which hampers the eÔ¨Éciency of Monte-
Carlo
simulations
near
second-order
phase transitions. ‚ÄúCritical slowing down‚Äù
means a dramatic increase of relaxation
times at the critical point of such tran-
sitions and these relaxation times also
control statistical errors in simulations;
see Section 2.6. As these ‚Äúcluster algo-
rithms‚Äù work for rather special models
only, they will not be discussed further
here.
There is also some arbitrariness in
the choice of the transition probability
W(X‚ÜíX‚Ä≤) itself. The original choice of
Metropolis et al. [28] is
W(X ‚ÜíX‚Ä≤) =
{
exp
(
‚àíùõø‚Ñã
kBT
)
if ùõø‚Ñã> 0,
1
otherwise.
(2.14)
An alternative choice is the so-called ‚Äúheat-
bath method.‚Äù There one assigns the new
value ùõº‚Ä≤
i of the ith local degree of freedom in
the move from X to X‚Ä≤ irrespective of what
the old value ùõºi was. Considering the local
energy ‚Ñãi(ùõº‚Ä≤
i), one chooses the state ùõº‚Ä≤
i with
probability
exp
[
‚àí‚Ñãi
(
ùõº‚Ä≤
i
)
kBT
]
‚àë
{ùõº‚Ä≤‚Ä≤
i }
exp
[
‚àí‚Ñãi
(
ùõº‚Ä≤‚Ä≤
i
)
kBT
].
We now outline the realization of the
sequence of states X with chosen transition
probability W. At each step of the proce-
dure, one performs a trial move ùõºi ‚Üíùõº‚Ä≤
i,
computes W(X‚ÜíX‚Ä≤) for this trial move,
and compares it with a random number
ùúÖ, uniformly distributed in the interval
[0,1]. If W < ùúÖ, the trial move is rejected,
and the old state (with ùõºi) is counted once
more in the average, (2.13). Then another
trial is made. If W > ùúÖ, on the other hand,
the trial move is accepted, and the new
conÔ¨Åguration thus generated is taken into
account in the average, (2.13). It serves
then also as a starting point of the next
step.
As subsequent states Xùúàin this Markov
chain diÔ¨Äer by the coordinate ùõºi of one
particle only (if they diÔ¨Äer at all), they
are highly correlated. Therefore, it is not
straightforward to estimate the error of
the average, (2.13). Let us assume for
the moment that, after n steps, these
correlations
have
died
out.
Then
we
may estimate
the statistical
error
ùõøA
of the estimate A from the standard
formula,
(ùõøA)2 =
1
m(m ‚àí1)
m + ùúá0‚àí1
‚àë
ùúá=ùúá0
[A(Xùúá) ‚àíA]2,
m ‚â´1,
(2.15)
where the integers ùúá0, ùúá, m are deÔ¨Åned
by m = (M ‚àíM0)/n, ùúá0 labels the state
ùúà= M0+ 1, ùúá= ùúá0 + 1 labels the state
ùúà= M0+ 1 +n, and so on. Then, for consis-
tency, A should be taken as
A = 1
m
m + ùúá0‚àí1
‚àë
ùúá=ùúá0
A(Xùúá).
(2.16)
If the computational eÔ¨Äort of carrying
out the ‚Äúmeasurement‚Äù of A(Xùúá) in the
simulation is rather small, it is advanta-
geous to keep taking measurements every
Monte-Carlo step (MCS) per degree of
freedom but to construct block averages
over n successive measurements, varying
n until uncorrelated block averages are
obtained. Details on error estimation for
Monte-Carlo simulations can be found in
Berg [34].

50
2 Monte-Carlo Methods
2.5.3
The Dynamic Interpretation of the
Monte-Carlo Method
It is not always easy to estimate the number
of states M0 after which the correlations to
the initial state X1, which typically is a state
far from equilibrium, have died out, nor is
it easy to estimate the number n between
steps after which correlations in equilib-
rium have died out. A formal answer to this
problem, in terms of relaxation times of
the associated master equation describing
the Monte-Carlo process, is discussed
in Section 2.5.4. This interpretation of
Monte-Carlo sampling in terms of master
equations is also the basis for Monte-Carlo
studies of the dynamics of Ô¨Çuctuations near
thermal equilibrium, and is discussed now.
One introduces the probability P(X,t) that
a state X occurs at time t. This probability
then decreases by all moves X ‚ÜíX‚Ä≤, where
the system reaches a neighboring state X‚Ä≤;
inverse processes X‚Ä≤ ‚ÜíX lead to a gain of
probability. Thus, one can write down a
rate equation, similar to chemical kinetics,
considering the balance of all gain and loss
processes:
d
dt P(X, t) = ‚àí
‚àë
X‚Ä≤
W(X ‚ÜíX‚Ä≤)P(X, t)
+
‚àë
X‚Ä≤
W(X‚Ä≤ ‚ÜíX)P(X‚Ä≤, t). (2.17)
The
Monte-Carlo
sampling
(i.e.,
the
sequence of generated states X1 ‚ÜíX2 ‚Üí
¬∑ ¬∑ ¬∑ ‚ÜíXùúà‚Üí¬∑ ¬∑ ¬∑) can hence be interpreted
as a numerical realization of the master
equation, (2.17), and then a ‚Äútime‚Äù t is
associated with the index v of subsequent
conÔ¨Ågurations. In a system with N parti-
cles, we may normalize the ‚Äútime unit‚Äù such
that N single-particle moves are attempted
per unit time. This is often called a sweep
step or 1 Monte-Carlo step (MCS).
For the thermal equilibrium distribution
Peq(X), because of the detailed balance
principle, (2.12), there is no change of
probability
with
time,
dP(X, t)‚àïdt = 0;
thus, thermal equilibrium arises as the
stationary solution of the master equation,
(2.17). Thus, it is also plausible that Markov
processes described by (2.17) describe a
relaxation that always leads toward thermal
equilibrium, as desired.
Now, for a physical system (whose trajec-
tory in phase space, according to classical
statistical mechanics, follows from New-
ton‚Äôs laws of motion), it is clear that the
stochastic trajectory through phase space
that is described by (2.17) in general has
nothing to do with the actual dynamics. For
example, (2.17) never describes any propa-
gating waves (such as spin waves in a mag-
net, or sound waves in a Ô¨Çuid, etc.).
In spite of this observation, the dynamics
of the Monte-Carlo ‚Äútrajectory‚Äù described
by (2.17) sometimes does have physical sig-
niÔ¨Åcance. In many situations, one does not
consider the full set of dynamical variables
of the system, but rather a subset only:
For instance, in modeling the diÔ¨Äusion pro-
cesses in an interstitial alloy, the diÔ¨Äusion
of the interstitials may be described by a
stochastic hopping between the available
lattice sites. As the mean time between
two successive jumps is orders of magni-
tude larger than the time scale of atomic
vibrations in the solid, the phonons can be
approximated as a heat bath, as far as the
diÔ¨Äusion is concerned.
There are many examples where such
a separation of time scales for diÔ¨Äerent
degrees of freedom occurs: For example,
describing the Brownian motion of poly-
mer chains in polymer melts, the fast
bond-angle and bond-length vibrations
may be treated as heat bath, and so on. As
a rule of thumb, any very slow relaxation
phenomena (kinetics of nucleation, decay

2.5 Monte-Carlo Methods in Statistical Thermodynamics: Importance Sampling
51
of remnant magnetization in spin glasses,
growth of ordered domains in adsorbed
monolayers at surfaces, etc.) can be mod-
eled by Monte-Carlo methods. Of course,
one must build in relevant conservation
laws into the model properly (e.g., in an
interstitial alloy, the overall concentra-
tion of interstitials is conserved; in a spin
glass, the magnetization is not conserved)
and choose microscopically reasonable
elementary steps for the move X ‚ÜíX‚Ä≤.
The great Ô¨Çexibility of the Monte-Carlo
method, where one can choose the level
of the modeling appropriately for the sys-
tem at hand and identify the degrees of
freedom that one wishes to consider, as
well as the type and nature of transitions
between them, is a great advantage and
thus allows complementary applications
to more atomistically realistic simula-
tion approaches such as the molecular
dynamics (MD) method where one numer-
ically integrates Newton‚Äôs equations of
motion [35].
2.5.4
Monte-Carlo Study of the Dynamics of
Fluctuations Near Equilibrium and of the
Approach toward Equilibrium
Accepting (2.17), the average in (2.13) then
is interpreted as a time average along the
stochastic trajectory in phase space,
A =
1
tM ‚àítM0 ‚à´
tM
tM0
A(t)dt,
tM = M
N ,
tM0 = M0
N .
(2.18)
It is thus no surprise that, for importance-
sampling
Monte-Carlo,
one
needs
to
consider carefully the problem of ergod-
icity: Time averages need not agree with
ensemble averages. For example, near
Ô¨Årst-order phase transitions, there may be
long-lived metastable states. Sometimes,
the considered moves do not allow one to
reach all conÔ¨Ågurations (e.g., in dynamic
Monte-Carlo
methods
for
SAWs;
see
[20]).
One can also deÔ¨Åne time-displaced cor-
relation functions: ‚ü®A (t) B (0)‚ü©, where A, B
stand symbolically for any physical observ-
ables, is estimated by
A(t)B(0) =
1
tM ‚àít ‚àítM0 ‚à´
tM‚àít
tM0
A(t + t‚Ä≤)
B(t‚Ä≤)dt‚Ä≤,
tM ‚àít > tM0.
(2.19)
Equation (2.19) refers to a case where tM0 is
chosen large enough such that the system
has relaxed toward equilibrium during the
time tM0; then the pair correlation depends
only on t and not the two individual times
t‚Ä≤, t‚Ä≤ + t separately.
However, it is also interesting to study the
nonequilibrium process by which equilib-
rium is approached. In this region, A(t) ‚àíA
depends systematically on the observation
time t, and an ensemble average ‚ü®A (t)‚ü©T ‚àí
‚ü®A (‚àû)‚ü©T (limt‚Üí‚àûA = ‚ü®A‚ü©T ‚â°‚ü®A (‚àû)‚ü©T if
the system is ergodic, that is, the time aver-
age agrees with the ensemble average at
the chosen temperature T) is nonzero. One
may deÔ¨Åne
‚ü®A(t)‚ü©T =
‚àë
{X}
P(X, t)A(X)
=
‚àë
{X}
P(X, 0)A(X(t)).
(2.20)
In the second step of (2.20), the fact was
used that the ensemble average involved is
actually an average weighted by P(X, 0) over
an ensemble of initial states X(t = 0), which
then evolve as described by the master
equation, (2.17). In practice, (2.20) means
an average over a large number nrun ‚â´1
statistically independent runs,
[
A(t)
]
av =
1
nrun
nrun
‚àë
l=1
A(t, l),
(2.21)

52
2 Monte-Carlo Methods
where A(t, l) is the observable A observed at
time t in the lth run of this nonequilibrium
Monte-Carlo averaging.
Many concepts of nonequilibrium statis-
tical mechanics can immediately be used
in such simulations. For instance, one can
introduce ‚ÄúÔ¨Åelds‚Äù that can be switched oÔ¨Ä
to study the dynamic response functions,
for both linear and nonlinear responses
[36].
2.5.5
The Choice of Statistical Ensembles
While so far the discussion has been
(implicitly) restricted to the case of the
canonical ensemble (NVT ensemble, for
the case of a Ô¨Çuid, particle number N,
volume V, and temperature T being held
Ô¨Åxed), it is sometimes useful to use other
statistical ensembles. Particularly useful
is the grand canonical ensemble (ùúáVT),
where the chemical potential ùúárather than
the particle number N is Ô¨Åxed. In addition
to moves where the conÔ¨Åguration of par-
ticles in the box relaxes, one has moves
where one attempts to add or remove a
particle from the box.
In the case of binary (AB) mixtures, a use-
ful variation is the ‚Äúsemi‚Äìgrand canonical‚Äù
ensemble, where Œîùúá= ùúáA‚àíùúáB is held Ô¨Åxed
and moves where an A particle is converted
into a B particle (or vice versa) are consid-
ered, in an otherwise identical system con-
Ô¨Åguration.
The isothermal-isobaric (NpT) ensemble,
on the other hand, Ô¨Åxes the pressure and
then volume changes V ‚ÜíV ‚Ä≤ = V + ŒîV
need to be considered (rescaling properly
the positions of the particles).
It also is possible to deÔ¨Åne artiÔ¨Åcial
ensembles that are not in the textbooks
on statistical mechanics. An example is
the
Gaussian
ensemble
(interpolating
between the canonical and microcanonical
ensemble, useful for the study of Ô¨Årst-order
phase transitions, such as the so-called
‚Äúmulticanonical ensemble‚Äù). Particularly
useful is the ‚ÄúGibbs ensemble,‚Äù where one
considers the equilibrium between two
simulation boxes (one containing liquid,
the other gas), which can exchange both
volume ŒîV and particles (ŒîN), while the
total volume and total particle number
contained in the two boxes are held Ô¨Åxed.
The Gibbs ensemble is useful for the sim-
ulation of gas-Ô¨Çuid coexistence, avoiding
interfaces [37, 38].
A simulation at a given state point (NVT)
contains information not only on averages
at that point but also on neighboring states
(NVT‚Ä≤), via suitable reweighting of the
energy distribution PN(E) with a factor
exp(E/kBT)exp(‚àíE/kBT‚Ä≤). Such ‚Äúhistogram
methods‚Äù
are
particularly
useful
near
critical points [33].
2.6
Accuracy Problems: Finite-Size Problems,
Dynamic Correlation of Errors, Boundary
Conditions
2.6.1
Finite-Size‚ÄìInduced Rounding and
Shifting of Phase Transitions
A prominent application of Monte-Carlo
simulation in statistical thermodynamics
and lattice theory is the study of phase
transitions. it is well known now in statis-
tical physics that sharp phase transitions
can occur in the thermodynamic limit only,
N ‚Üí‚àû. This is no practical problem in
everyday life ‚Äì even a small water droplet
freezing into a snowÔ¨Çake contains about
N = 1018H2O molecules, and, thus, the
rounding and shifting of the freezing are on
a relative scale of
1
‚àö
N = 10‚àí9 and thus com-
pletely negligible. But the situation diÔ¨Äers

2.6 Accuracy Problems: Finite-Size Problems, Dynamic Correlation of Errors, Boundary Conditions
53
for simulations, which often consider very
small systems (e.g., a d-dimensional box
with linear dimensions L, V = Ld, and
periodic boundary conditions), where only
N ‚àº103 ‚àí106 particles are involved.
In such small systems, phase transi-
tions are strongly rounded and shifted
[39‚Äì42]. Thus, care needs to be applied
when simulated systems indicate phase
changes. It turns out, however, that these
Ô¨Ånite-size eÔ¨Äects can be used as a valu-
able tool to infer properties of the inÔ¨Ånite
system from the Ô¨Ånite-size behavior. As a
typical example, we discuss the phase tran-
sition of an Ising ferromagnet (Figure 2.1),
which has a second-order phase tran-
sition at a critical temperature Tc. For
L ‚Üíùõº, the spontaneous magnetization
Mspont vanishes according to a power law,
Mspont = B(1 ‚àíT/Tc)ùõΩ, B being a critical
amplitude and ùõΩa critical exponent [43],
and the susceptibility ùúíand correlation
length ùúâdiverge,
ùúí‚àù
||||
1 ‚àíT
Tc
||||
‚àíùõæ
, ùúâ‚àù
||||
1 ‚àíT
Tc
||||
‚àíùúà
(2.22)
where ùõæand ùúàare critical exponents. In
a Ô¨Ånite system, ùúâcannot exceed L, hence,
these singularities are smeared out.
Now Ô¨Ånite-size scaling theory [39, 42]
implies that these Ô¨Ånite-size eÔ¨Äects are
understood from the principle that ‚ÄúL
scales with ùúâ‚Äù; that is, the order-parameter
probability
distribution
PL(M)
can
be
written [40, 41]
PL = LùõΩ‚àïv ÃÉP
(
L
ùúâ, ML
ùõΩ
v
)
,
(2.23)
where ÃÉP is a ‚Äúscaling function.‚Äù From (2.23),
one immediately obtains the Ô¨Ånite-size
scaling relations for order parameter ‚ü®|M|‚ü©
and the susceptibility (deÔ¨Åned from a Ô¨Çuc-
tuation relation) by taking the moments of
the distribution PL:
‚ü®|M|‚ü©= L‚àíùõΩ‚àïv ÃÉM
(
L
ùúâ
)
,
(2.24)
kBTùúí‚Ä≤ = Ld (‚ü®M2‚ü©‚àí‚ü®|M|‚ü©2)=Lùõæ‚àïv ÃÉX
(
L
ùúâ
)
,
(2.25)
where ÃÉM, ÃÉùúíare scaling functions that fol-
low from ÃÉP in (2.23) At Tc where ùúâ‚Üí‚àû, we
thus have ùúí‚Ä≤ ‚àùLùõæ‚àïùúà; from this variation of
ùúí‚Ä≤ with L, hence, the exponent ùõæ‚àïùúàcan be
extracted.
The fourth-order cumulant UL is a func-
tion of L/ùúâonly,
UL ‚â°1 ‚àí
‚ü®M4‚ü©
(3 ‚ü®M2‚ü©2) = ÃÉU
(
L
ùúâ
)
.
(2.26)
Here UL ‚Üí0 for a Gaussian centered at
M = 0, that is, for T > Tc; UL ‚Üí2/3 for
the double-Gaussian distribution, that is,
for T < Tc; while UL = ÃÉU(0) is a universal
nontrivial constant for T = Tc. Cumulants
for diÔ¨Äerent system sizes hence intersect
at Tc, and this can be used to locate Tc
precisely [40, 41].
A simple discussion of Ô¨Ånite-size eÔ¨Äects
at Ô¨Årst-order transitions is similarly possi-
ble [41]: one describes the various phases
that coexist at the Ô¨Årst-order transition in
terms of Gaussians if L ‚â´ùúâ(note that ùúâ
stays Ô¨Ånite at the Ô¨Årst-order transition).
In a Ô¨Ånite system, these phases coexist
not only right at the transition but over
a Ô¨Ånite parameter region. The weights of
the respective peaks are given in terms of
the free-energy diÔ¨Äerence of the various
phases. From this description, energy and
order-parameter distributions and their
moments can be worked out. Of course,
this description applies only for long
enough runs where the system jumps from
one phase to the other many times, while
for short runs where the systems stay in a
single phase, one observes hysteresis.

54
2 Monte-Carlo Methods
PL (M)
PL (M)
PL (M)
M
M
L
L finite
Mspont = B(1‚àíT/Tc)Œ≤
0
0
+ Mspont
‚àí Mspont
0
0
Tc
T  > Tc
T = Tc
T < Tc
T
Tc
UL
Tc
T
T
‚àû
<IMI>
Ld ( <M2> ‚àí <IMI>2 )
L
L
L‚Ä≤ > L
‚àû
L
‚àû
kBTX / L¬∞
‚àùL‚àíŒ≤/ŒΩ
‚àù|I-T/Tc|‚àíY
2/3
Figure 2.1
(a‚Äìf) Schematic evolution of
the order-parameter probability distribution
PL(M) from T > Tc to T < Tc (from above to
below, left part), for an Ising ferromagnet
(where M is the magnetization) in a box of
volume V = Ld. The right part shows the cor-
responding temperature dependence of the
mean order parameter ‚ü®|M|‚ü©, the susceptibility
kBTùúí‚Ä≤ = Ld (‚ü®M2‚ü©‚àí‚ü®|M|‚ü©2), and the reduced
fourth-order cumulant UL = 1 ‚àí‚ü®M4‚ü©‚àï[3‚ü®M2‚ü©2].
Dash-dotted curves indicate the singular vari-
ation that results in the thermodynamic limit,
L ‚Üí‚àû.
2.6.2
DiÔ¨Äerent Boundary Conditions: Simulation
of Surfaces and Interfaces
We now brieÔ¨Çy discuss the eÔ¨Äect of various
boundary conditions. Typically, one uses
periodic boundary conditions to study
bulk properties of systems not obscured
by surface eÔ¨Äects. However, it also is
possible to choose boundary conditions
to study surface eÔ¨Äects deliberately; for
example, one may simulate thin Ô¨Ålms in
an L √ó L √ó D geometry with two free L √ó L
surfaces and periodic boundary condi-
tions otherwise. If the Ô¨Ålm thickness D
is large enough, the two surfaces do not
inÔ¨Çuence each other, and one can infer the
properties of a semi-inÔ¨Ånite system. One

2.6 Accuracy Problems: Finite-Size Problems, Dynamic Correlation of Errors, Boundary Conditions
55
may choose special interactions near the
free surfaces, apply surface ‚ÄúÔ¨Åelds‚Äù (even if
they cannot be applied in the laboratory,
it may nevertheless be useful to study the
response to them in the simulation), and
so on.
Sometimes, the boundary conditions
may stabilize interfaces in the system (e.g.,
in an Ising model for T < Tc a domain wall
between phases with opposite magneti-
zation will be present, if we apply strong
enough surface Ô¨Åelds of opposite sign).
Such interfaces also are often the object
of study. It may be desirable to simulate
interfaces without having the systems
disturbed by free surfaces. In an Ising
system, this may simply be done by choos-
ing
antiperiodic
boundary
conditions.
Combining
antiperiodic
and
staggered
periodic boundary conditions, even tilted
interfaces may be stabilized in the system.
In all such simulations of systems con-
taining interfaces, one must keep in mind,
however, that because of capillary-wave
excitations, interfaces usually are very
slowly relaxing objects, and often a major
computing eÔ¨Äort is needed to equilibrate
them. A further diÔ¨Éculty (when one is
interested in interfacial proÔ¨Åles) is the fact
that the center of the interface is typically
delocalized.
2.6.3
Estimation of Statistical Errors
We now return to the problem of judg-
ing the time needed for having reasonably
small errors in Monte-Carlo sampling. If
the subsequent conÔ¨Ågurations used were
uncorrelated, we could use (2.15), but in the
case of correlations we have rather (here
the index ùúáshould not be confused with
the chemical potential of Section 2.5.5, of
course)
‚ü®(ùõøA)2‚ü©=
‚ü®[
1
n
n
‚àë
ùúá=1
Aùúá‚àí‚ü®A‚ü©
]2‚ü©
= 1
n
[
‚ü®A2‚ü©‚àí‚ü®A‚ü©2 + 2
n
‚àë
ùúá=1
(
1 ‚àíùúá
n
)
√ó
(‚ü®A0Aùúá‚ü©‚àí‚ü®A‚ü©2)
]
.
(2.27)
Now we remember that a time tùúá= ùúáùõøt is
associated with the Monte-Carlo process,
ùõøt being the time interval between two suc-
cessive observations Aùúá, Aùúá+1. Transform-
ing the summation to a time integration
yields
‚ü®(ùõøA)2‚ü©= 1
n
(‚ü®A2‚ü©‚àí‚ü®A‚ü©2)
√ó
[
1+ 2
ùõøt ‚à´
tn
0
(
1‚àít
tn
)
ùúôA(t)dt
]
,
(2.28)
where
ùúôA(t) ‚â°‚ü®A(0)A(t)‚ü©‚àí‚ü®A‚ü©2
‚ü®A2‚ü©‚àí‚ü®A‚ü©2
.
DeÔ¨Åning a relaxation time ùúèA = ‚à´‚àû
0 dtùúôA(t),
one obtains for ùúèA ‚â™nùõøt = ùúèobs (the obser-
vation time)
‚ü®(ùõøA)2‚ü©= 1
n[‚ü®A2‚ü©‚àí‚ü®A‚ü©2]
(
1 + 2ùúèA
ùõøt
)
‚âà2
( ùúèA
ùúèobs
)
[‚ü®A2‚ü©‚àí‚ü®A‚ü©2].
(2.29)
In comparison with (2.15), the dynamic
correlations inherent in a Monte-Carlo
sampling as described by the master
equation, (2.17), lead to an enhancement
of the expected statistical error ‚ü®(ùõøA)2‚ü©by
a ‚Äúdynamic factor‚Äù 1 + 2ùúèA/ùõøt (sometimes
also called the statistical ineÔ¨Éciency).
This dynamic factor is particularly cum-
bersome near second-order phase transi-
tions (ùúèA diverges: critical slowing down)
and near Ô¨Årst-order phase transitions (ùúèA
diverges at phase coexistence, because of

56
2 Monte-Carlo Methods
the large lifetime of metastable states).
Thus, even if one is interested only in
static quantities in a Monte-Carlo simula-
tion, understanding the dynamics may be
useful for estimating errors. In addition,
the question of how many conÔ¨Ågurations
(M0) must be omitted at the start of the
averaging for the sake of equilibrium (2.18)
can be formally answered in terms of a
nonlinear relaxation function
ùúô(nl)(t) = ‚ü®A(t)‚ü©T ‚àí‚ü®A(‚àû)‚ü©T
‚ü®A(0)‚ü©T ‚àí‚ü®A(‚àû)‚ü©T
and its associated time ùúè(nl)
A
= ‚à´‚àû
0 ùúô(nl)
A (t)dt
by the condition tM0 ‚â´ùúè(nl)
A . Here tM0 is the
time corresponding to the number of states
M0 omitted.
2.7
Sampling of Free Energies and Free Energy
Barriers
2.7.1
Bulk Free Energies
Carrying out the step from (2.10) to (2.13),
the explicit knowledge on the partition
function Z of the system, and hence its
free energy F = ‚àíkBT ln Z is lost. In many
cases, however, knowledge of F would be
very useful: for example, at a Ô¨Årst-order
phase transition the free energies F1, F2 of
two distinct phases 1, 2 of a system become
equal.
There are many ways how this drawback
of importance-sampling Monte-Carlo can
be overcome (see [30, 36]). We describe
here very brieÔ¨Çy the most popular method,
Wang-Landau sampling [44]. It starts from
the concept that Z can be expressed in
terms of the energy density of states g(E) as
Z = ‚à´dEg (E) exp
(
‚àíE
kBT
)
.
Suppose g(E) is known and one deÔ¨Ånes a
Markov process via a transition probability
p(E ‚ÜíE‚Ä≤) as
p
(
E ‚ÜíE
‚Ä≤)
= min
{
g (E)
g (E
‚Ä≤), 1
}
This process would generate a Ô¨Çat his-
togram H(E) = const. As g(E) is not known
beforehand, we can use the process deÔ¨Åned
by the above equation to construct an
iteration that yields an estimate for g(E):
when we know g(E), Z and hence F are also
known.
In the absence of any a priori knowledge
on g(E), the iteration starts with g(E) ‚â°1
for all E (for simplicity, we consider here
an energy spectrum that is discrete and
bounded, Emin ‚â§E ‚â§Emax). Now Monte-
Carlo moves (e.g., spin Ô¨Çips in an Ising
model) are carried out to accumulate a his-
togram H(E), starting out with H(E) = 0 for
all E and replacing H(E) by H(E) + 1 when-
ever E is visited. We also replace g(E) by
g(E)f where the modiÔ¨Åcation factor f ini-
tially is large, for example, f = e1. The sam-
pling of the histogram is continued, until
H(E) is reasonably ‚ÄúÔ¨Çat‚Äù (e.g., nowhere less
than 80% of its maximum value).
Then H(E) is reset to H(E) = 0 for all E,
f is replaced by
‚àö
f , and the process con-
tinues: in the ith step of the iteration fi =
‚àö
fi‚àí1, so f tends to unity; that is, we have
reached the above p(E ‚ÜíE‚Ä≤), as desired,
and can use the corresponding g(E) to com-
pute the desired averages.
This algorithm is robust and widely
used (see Landau and Binder [36] for
examples). We stress, however, that many
other schemes for obtaining g(E) exist,
such as the so-called ‚Äúumbrella sampling,‚Äù
‚Äúmulticanonical Monte-Carlo,‚Äù ‚Äútransition
matrix Monte-Carlo,‚Äù and so on. Often the
density of states is required as a function
of several variable (e.g., g(E,M) where M is

2.8 Quantum Monte-Carlo Techniques
57
the magnetization, for an Ising model). We
refer to the more specialized literature [36]
for more details on these techniques.
2.7.2
Interfacial Free Energies
When in the volume taken by a system
several phases coexist, the interfaces cause
excess contribution to the free energy. The
simplest case again is provided by the Ising
model, considering again the distribution
PL(M) of the magnetization M in a system
of Ô¨Ånite linear dimension L, cf. Figure 2.1,
but now for T ‚â™Tc (Figure 2.1): A state
with M ‚âà0 actually is characterized by
two-phase coexistence in the simulation
box: a domain with M ‚âà+Mspont is sepa-
rated from a (roughly equally large) domain
with M ‚âà‚àíMspont by two planar interfaces
(of area Ld‚àí1 if the volume is Ld; note the
periodic boundary conditions). As a result,
one can argue that PL(M ‚âà0)‚àïPL(M =
Mspont) ‚âàexp(‚àí2Ld‚àí1
fint‚àïkBT), with fint
the interface free-energy density between
the coexisting phases.
Accurate sampling of PL(M) near its
minimum (which can be done by umbrella
sampling, for instance) in fact is a use-
ful method for estimating fint in various
systems. Of course, there exist many alter-
native techniques, and also methods to
estimate the excess free energies due to
external walls, and related quantities such
as contact angles of droplets, line tensions,
and so on. This subject is still a very active
area of research [45, 46].
2.7.3
Transition Path Sampling
Suppose we consider a system with a com-
plex free-energy landscape, where the sys-
tem can stay in some minimum (denoted
as A) for a long time, and in a rare event
may pass through some saddle point region
to another minimum (denoted as B). Being
interested in the rates of such process, one
must consider the possibility that many tra-
jectories from A to B in conÔ¨Åguration space
may compete with each other. Transition
path sampling [47] is a technique to explore
the dominating trajectories and thus the
rate of such rare events. Starting out from
one trial trajectory that leads from A to B, a
sampling of stochastic variations of this tra-
jectory is performed. Again, the technique
and its theoretical foundations are rather
involved, and still under development.
2.8
Quantum Monte-Carlo Techniques
2.8.1
General Remarks
Development
of
Monte-Carlo
tech-
niques
to
study
ground-state
and
Ô¨Ånite-temperature
properties
of
inter-
acting quantum many-body systems is an
active area of research (for reviews, see
[48‚Äì56]). These methods are of interest for
problems such as the structure of nuclei
[57] and elementary particles [58], super-
Ô¨Çuidity of 3He and 4He [53, 54], high-Tc
superconductivity (e.g., [59]), magnetism
(e.g., [60, 61]), surface physics [62], and so
on. Despite this widespread interest, much
of this research has the character of ‚Äúwork
in progress‚Äù and hence cannot feature
more prominently in this chapter. Besides,
there is not just one quantum Monte-Carlo
method, but many variants that exist:
variational Monte-Carlo (VMC), Green‚Äôs-
function Monte-Carlo (GFMC), projector
Monte-Carlo (PMC), path-integral Monte-
Carlo (PIMC), grand canonical quantum
Monte-Carlo (GCMC), world-line quan-
tum Monte-Carlo (WLQMC), and so on.

58
2 Monte-Carlo Methods
Some of these (such as VMC, GFMC)
address
ground-state
properties,
oth-
ers (such as PIMC) Ô¨Ånite temperatures.
Here only the PIMC technique will be
brieÔ¨Çy
sketched,
following
Gillan
and
Christodoulos [63].
2.8.2
Path-Integral Monte-Carlo Methods
We wish to calculate thermal averages for
a quantum system and thus rewrite (2.10)
and (2.11) appropriately,
‚ü®A‚ü©= 1
Z Tr exp
(
‚àí
ÃÇ
‚Ñã
kBT
)
ÃÇA,
Z = Tr exp
(
‚àí
ÃÇ
‚Ñã
kBT
)
,
(2.30)
using a notation that emphasizes the oper-
ator character of the Hamiltonian ÃÇ
‚Ñãand of
the quantity √Ç associated with the variable
A that we consider. For simplicity, we con-
sider Ô¨Årst a system of a single particle in one
dimension acted on by a potential V(x). Its
Hamiltonian is
ÃÇ
‚Ñã= ‚àí‚Ñè2
2m
d2
dx2 + V(x).
(2.31)
Expressing the trace in the position repre-
sentation, the partition function becomes
Z = ‚à´dx
‚ü®
x
|||||
exp
(
‚àíÃÇ
‚Ñã
kBT
)|||||
x
‚ü©
,
(2.32)
where |x‚ü©is an eigenvector of the position
operator. Writing exp(‚àíÃÇ
‚Ñã‚àïkBT) formally
as [exp(‚àíÃÇ
‚Ñã‚àïkBTP)]P, where P is a posi-
tive integer, we can insert a complete set of
states between the factors:
Z = ‚à´dx1 ¬∑ ¬∑ ¬∑ ‚à´dxP‚ü®x1|exp
(
‚àíÃÇ
‚Ñã
kBTP
)
|x2‚ü©
‚ü®x2| ¬∑ ¬∑ ¬∑ |xp‚ü©‚ü®xP|exp
(
‚àíÃÇ
‚Ñã
kBTP
)
|x1‚ü©.
(2.33)
For large P, it is a good approximation to
ignore the fact that kinetic and potential
energy do not commute. Hence, one gets
‚ü®x|exp
(
‚àíÃÇ
‚Ñã
kBTP
)
|x‚Ä≤‚ü©‚âà
(kBTmP
2ùúã‚Ñè2
)1‚àï2
√ó exp
[‚àíkBTmP
2ùúã‚Ñè2
(x ‚àíx‚Ä≤)2
]
√ó exp
{
‚àí1
2kBTP
[
V (x) + V
(
x‚Ä≤)]}
(2.34)
and
Z ‚âà
(kBTmP
2ùúã‚Ñè2
)P‚àï2
‚à´dx1 ¬∑ ¬∑ ¬∑ ‚à´dxP
√ó exp
{
‚àí1
kBT
[
1
2
P
‚àë
s=1
ùúÖ(xs ‚àíxs+1
)2
]
+ P‚àí1
P
‚àë
s=1
V(xs)
}
,
(2.35)
where
ùúÖ‚â°
(kBT
‚Ñè
)2
mP.
(2.36)
In the limit P ‚Üí‚àû, (2.35) becomes exact.
Apart from the prefactor, (2.35) is precisely
the conÔ¨Ågurational partition function of a
classical system, a ring polymer consisting
of P beads coupled by harmonic springs
with spring constant ùúÖ. Each bead is in a
potential V(x)/P.
This approach is straightforwardly gener-
alized to a system of N interacting quantum
particles: one gets a system of N classical
cyclic ‚Äúpolymer‚Äù chains. As a result of this
isomorphism, the Monte-Carlo method
for simulating classical systems can be
carried over to such quantum-mechanical
problems too. It is also easy to see that
the system always behaves classically at
high temperatures: ùúÖgets very large, and
then the cyclic chains contract essentially
to a point, while at low temperatures,

2.8 Quantum Monte-Carlo Techniques
59
they are spread out, representing zero-
point motion. However, PIMC becomes
increasingly diÔ¨Écult at low temperatures,
because P has to be the larger the lower
that T is: If ùúéis a characteristic distance
over which the potential V(x) changes,
one must have ‚Ñè2‚àïmùúé2 ‚â™kBTP in order
that two neighbors along the ‚Äúpolymer
chain‚Äù are at a distance much smaller than
ùúé. In PIMC simulations, one empirically
determines and uses that P beyond which
the thermodynamic properties do not
eÔ¨Äectively change.
This
approach
can
be
general-
ized to the density matrix ùúå(x ‚àíx‚Ä≤) =
‚ü®x| exp(‚àíÃÇ
‚Ñã‚àïkBT)|x‚Ä≤‚ü©,
while
there
are
problems
with
time-displaced
corre-
lation functions ‚ü®A(t)B(0)‚ü©, where t is
now the true time (associated with the
time evolution of states following from
the Schr√∂dinger equation, rather than the
‚Äútime‚Äù of Section 2.5.3 related to the master
equation).
The step leading to (2.34) can be viewed
as a special case of the Suzuki‚ÄìTrotter for-
mula [51]
exp ( ÃÇA + ÃÇB)= lim
P‚Üí‚àû
[
exp
( ÃÇA
P
)
exp
( ÃÇB
P
)]P
,
(2.37)
which
is
also
used
for
mapping
d-
dimensional quantum problems on lattices
to equivalent classical problems (in d+ 1
dimensions, because of the ‚ÄúTrotter direc-
tion‚Äù corresponding to the imaginary time
direction of the path integral).
2.8.3
A Classical Application: the Momentum
Distribution of Fluid 4He
We now consider the dynamic structure
factor S(k, ùúî), which is the Fourier trans-
form of a time-displaced pair correlation
function of the density at a point r1 at
time t1 and the density at point r2 at time
t2 (‚Ñèk being the momentum transfer and
‚Ñèùúîthe energy transfer of an inelastic
scattering experiment by which one can
measure S(k, ùúî)). In the ‚Äúimpulse approx-
imation,‚Äù S(k, ùúî) can be related to the
Fourier transform of the single-particle
density matrix ùúå1(r), which for 4He can
be written in terms of the wave function
ùúì(r) as ùúå1 (r) = ‚ü®ùúì+ (r‚Ä≤ + r) ùúì(r)
‚ü©. This
relation is
S(k, ùúî) ‚àùJ(Y) = 1
ùúã‚à´
‚àû
0
ùúå1(r)cos(Yr)dr,
where Y ‚â°m(ùúî‚àík2/2m)/k [64]. As S(k, ùúî)
has been measured via neutron scatter-
ing [65], a comparison between experiment
and simulation can be performed without
adjustable parameters (Figure 2.2). Thus,
the PIMC method yields accurate data in
good agreement with experiment.
The studies of 4He have also yielded qual-
itative evidence for superÔ¨Çuidity [53]. For
a quantitative analysis of the ùúÜtransition,
a careful assessment of Ô¨Ånite-size eÔ¨Äects
(Figure 2.1) is needed because one works
with very small particle numbers (of the
order of 102 4He atoms only). This has not
been possible so far.
2.8.4
A Few Qualitative Comments on Fermion
Problems
Particles obeying Fermi‚ÄìDirac statistics
(such as electrons or 3He, for instance, see
Ceperley [54]) pose particular challenges
to Monte-Carlo simulation. If one tries to
solve the Schr√∂dinger equation of a many-
body system by Monte-Carlo methods,
one exploits its analogy with a diÔ¨Äusion
equation [48, 49]. DiÔ¨Äusion processes cor-
respond to RWs and are hence accessible to
Monte-Carlo simulation. However, while
the diÔ¨Äusion equation (for one particle)

60
2 Monte-Carlo Methods
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
‚àí0.05
‚àí1
‚àí2
‚àí3
‚àí4
1
2
3
4
J(Y)
0
0
Y (A‚àí1)
Figure 2.2
The measured momentum distribution J(Y)
of 4He at T = 3.3 K (circles, from [65]) compared with the
PIMC result of Ceperley and Pollock [66] (solid line). (Source:
From Schmidt and Ceperley [67].)
considers the probability P(r, t) that a
particle starting at time t = 0 at the ori-
gin has reached the position r at time t,
the wave function ùúìin the Schr√∂dinger
equation is not positive deÔ¨Ånite. This fact
creates severe problems for wave functions
of many-fermion systems, because these
wave functions must be antisymmetric,
and the ‚Äúnodal surface‚Äù in conÔ¨Åguration
space (where ùúìchanges sign) is unknown.
Formally,
the
diÔ¨Éculty
of
applying
importance-sampling techniques to dis-
tributions ùúå(r) that are not always positive
can be overcome by splitting ùúå(r) into its
sign, s = sign(ùúå), and its absolute value,
ùúå= s|ùúå|, and one can use ÃÉùúå(r) =
|ùúå(r)|
‚à´|ùúå(r)|d3r
as a probability density for importance
sampling, and absorb the sign of ùúå(r) in the
quantity to be measured. Symbolically, the
average of an observable A is obtained as
‚ü®A‚ü©=
‚à´d3rA(r)s(r) ÃÉùúå(r)
‚à´s(r) ÃÉùúå(r)d3r
=
‚ü®As‚ü©ÃÉùúå
‚ü®s‚ü©ÃÉp
,
where ‚ü®‚Ä¶‚ü©ÃÉùúåmeans averaging with ÃÉùúåas
weight function. However, as is not unex-
pected, using ÃÉùúåone predominantly samples
unimportant regions in phase space; there-
fore, in sampling the sign ‚ü®s‚ü©ÃÉùúå, one has large
cancellations from regions where the sign
is negative, and, for N degrees of freedom,
one gets ‚ü®s‚ü©ÃÉùúå‚àùexp(‚àíconst. √ó N). This dif-
Ô¨Åculty is known as the minus-sign problem
and still hampers applications to fermion
problems signiÔ¨Åcantly.
Sometimes it is possible to start with
a trial wave function where nodes are a
reasonable Ô¨Årst approximation to the actual
nodes, and, starting with the population

2.9 Lattice Gauge Theory
61
of RWs from this Ô¨Åxed-node approxima-
tion given by the trial function, one now
admits walks that cross this nodal surface
and sample the sign as indicated above. In
this way, it has been possible to estimate the
exchange-correlation energy of the homo-
geneous electron gas [68] over a wide range
of densities very well.
2.9
Lattice Gauge Theory
Monte-Carlo simulation has become the
primary tool for nonperturbative quantum
chromodynamics, the Ô¨Åeld theory of quarks
and hadrons and other elementary particles
(e.g., [58, 69]). In this section, we Ô¨Årst stress
the basic problem, to make the analogy with
the calculations of statistical mechanics
clear. Then we very brieÔ¨Çy highlight some
of the results that have been obtained so far.
2.9.1
Some Basic Ideas of Lattice Gauge Theory
The theory of elementary particles is a Ô¨Åeld
theory of gauge Ô¨Åelds and matter Ô¨Åelds.
Choice of a lattice is useful to provide a cut-
oÔ¨Äthat removes the ultraviolet divergences
that would otherwise occur in these quan-
tum Ô¨Åeld theories. The Ô¨Årst step, hence, is
the appropriate translation from the four-
dimensional continuum (3 space + 1 time
dimensions) to the lattice.
The generating functional (analogous to
the partition function in statistical mechan-
ics) is
Z=‚à´DADùúìDùúìexp [‚àíSg(A, ùúì, ùúì)] , (2.38)
where A represents the gauge Ô¨Åelds, ùúìand
ùúìrepresent the (fermionic) matter Ô¨Åeld, Sg
is the action of the theory (containing a
coupling constant g, which corresponds to
inverse temperature in statistical mechan-
ics as const./g2 ‚Üí1/kBT), and the sym-
bols ‚à´D stand for functional integration.
The action of the gauge Ô¨Åeld itself is, using
the summation convention that indices that
appear twice are summed over,
SG = 1
4 ‚à´d4xFùõº
ùúáv(x)Fùúáv
ùõº(x),
(2.39)
Fùõº
ùúáv being the Ô¨Åelds that derive from the
vector potential Aùõº
ùúá(x). These are
Fùõº
ùúáv(x)=ùúïùúáAùõº
v(x) ‚àíùúïvAùõº
ùúá(x)+gf ùõº
ùõΩùõæAùõΩ
ùúá(x)Aùõæ
v(x),
(2.40)
f ùõº
ùõΩùõæbeing the structure constants of the
gauge group, and g a coupling constant.
The variables that one then introduces
are elements Uùúá(x) of the gauge group G,
which are associated with the links of the
four-dimensional lattice, connecting x and
a nearest-neighbor point x+ ùúá:
Uùúá(x) = exp
[
igaTùõºAùõº
ùúá(x)
]
,
[Um(x + m)]‚Ä† = Um(x),
(2.41)
where a is the lattice spacing and Tùõºa group
generator. Here U+ denotes the Hermitean
conjugate of U. Wilson [70] invented a lat-
tice action that reduces in the continuum
limit to (2.39), namely,
SU
kBT = 1
g2
‚àë
n
‚àë
ùúá>v
Re TrUùúá(n) √ó Uv(n + ùúá)
U‚Ä†
ùúá(n + v)U‚Ä†
v (n),
(2.42)
where the links in (2.42) form a closed con-
tour along an elementary plaquette of the
lattice.
Using (2.42) in (2.38), which amounts
to the study of a ‚Äúpure‚Äù gauge theory (no
matter Ô¨Åelds), we recognize that the prob-
lem is equivalent to a statistical mechan-
ics problem (such as spins on a lattice),

62
2 Monte-Carlo Methods
the diÔ¨Äerence being that now the dynami-
cal variables are the gauge group elements
Uùúá(n). Thus importance-sampling Monte-
Carlo algorithms can be put to work, just
as in statistical mechanics.
To include matter Ô¨Åelds, one starts from
a partition function of the form
Z = ‚à´DUDùúìDùúìexp
{
‚àíSU
kBT +
nf
‚àë
i=1
ùúìMùúì
}
= ‚à´DU(det M)nf exp
(
‚àíSU
kBT
)
,
(2.43)
where we have assumed fermions with nf
degenerate ‚ÄúÔ¨Çavors.‚Äù It has also been indi-
cated that the fermion Ô¨Åelds can be inte-
grated out analytically, but the price is that
one has to deal with the ‚Äúfermion determi-
nant‚Äù of the matrix M. In principle, for any
change of the U‚Äôs this determinant needs to
be recomputed; together with the fact that
one needs to work on rather large lattices in
four dimensions, in order to reproduce the
continuum limit, this problem is responsi-
ble for the huge requirement of computing
resources in this Ô¨Åeld.
It is clear that lattice gauge theory cannot
be explained in depth on two pages ‚Äì we
only intend to give a vague idea of what
these calculations are about to a reader who
is not familiar with this subject.
2.9.2
A Famous Application
Among the many Monte-Carlo studies of
various problems (which include problems
in cosmology, such as the phase transition
from the quark-gluon plasma to hadronic
matter in the early universe), we focus here
on the problem of predicting the masses of
elementary particles. Butler et al. [71] used
a new parallel supercomputer with 480 pro-
cessors (‚ÄúGF11‚Äù) exclusively for one year
to run lattice sizes ranging from 83 √ó 32 to
243 √ó 32, 243 √ó 36, and 30 √ó 322 √ó 40. Their
program executed at a speed of more than
5 GÔ¨Çops (Giga Ô¨Çoating point operations
per second), and the rather good statistics
reached allowed a meaningful elimination
of Ô¨Ånite-size eÔ¨Äects by an extrapolation to
the inÔ¨Ånite-volume limit. This problem is
important, because the wave function of
a hadron is spread out over many lattice
sites.
Even with this impressive eÔ¨Äort, several
approximations were necessary.
The fermion determinant mentioned
above is neglected (this is called quenched
approximation). One cannot work at the
(physically
relevant)
very
small
quark
mass mq, but rather data on the hadron
masses were taken for a range of quark
masses and extrapolated (Figure 2.3). After
a double extrapolation (mq ‚Üí0, lattice
spacing at Ô¨Åxed volume ‚Üí0), one obtained
mass ratios that are in satisfactory agree-
ment with experiment. For example, for
the nucleon the mass ratio for the Ô¨Ånite
volume is mN/m√¶ = 1.285 ¬± 0.070, extrap-
olated to inÔ¨Ånite volume 1.219 ¬± 0.105,
the experimental value being 1.222 (all
masses in units of the mass m√¶ of a rho
meson).
Fifteen years later [72], it became possi-
ble to avoid the ‚Äúquenched approximation‚Äù
and other limitations of the early work and
calculate the light hadron masses, obtain-
ing very good agreement with experiment,
using supercomputers that were a million
times faster.
2.10
Selected Applications in Classical
Statistical Mechanics of Condensed Matter
In this section, we mention a few applica-
tions, to give the Ô¨Çavor of the type of work
that is done and the kind of questions that

2.10 Selected Applications in Classical Statistical Mechanics of Condensed Matter
63
0.0
0.5
1.0
1.5
2.0
2.5
3.0
2.0
4.0
6.0
0
0
1
2
3
œÄ
œÅ
Œî
N
[mœÅ,mN,mŒî] / mœÅ(mn)
mœÄ
2 / mœÅ(mn)2
mq / ms
Figure 2.3
For a 30 √ó 322√ó 40 lattice at (kBT)‚àí1 =
6.17, m2
√ü, m√¶, mN, and m‚Ç¨ in units of the physical rho
meson mass m√¶(mn), as functions of the quark mass mq
in units of the strange quark mass ms. Particles studied are
pion, rho meson, nucleon, and delta baryon, respectively.
(Source: From Butler et al. [71].)
are answered by Monte-Carlo simulations.
More extensive reviews can be found in the
literature [29, 36, 73, 74].
2.10.1
Metallurgy and Materials Science
A widespread application of Monte-Carlo
simulation in this area is the study of
order-disorder phenomena in alloys: One
tests analytical approximations to calculate
phase diagrams, such as the cluster varia-
tion (CV) method, and one tests to what
extent a simple model can describe the
properties of complicated materials.
An example (Figure 2.4) shows the order
parameter for long-range order (LRO)
and short-range order (SRO, for nearest
neighbors) as function of temperature for
a model of Cu3Au alloys on the fcc lattice.
An Ising model with antiferromagnetic
interactions between nearest neighbors is
studied, and the Monte-Carlo data (Ô¨Ålled
symbols and symbols with crosses) are
compared to CV calculations (broken
curve) and other analytical calculations
(dash-dotted curve) and to experiment
(open circles). The simulation shows that
the analytical approximations describe the
ordering of the model only qualitatively.
Of course, there is no perfect agreement
with the experiment either; this is to be
expected, of course, because in real alloys
the interaction range is considerably larger
than just extending to nearest neighbors
only.
2.10.2
Polymer Science
One can study phase transitions not only
for models of magnets or alloys, but also
for complex systems such as mixtures
of Ô¨Çexible polymers. A question heavily
debated in the literature is the dependence
of the critical temperature of unmixing
of a symmetric polymer mixture (both
constituents have the same degree of

64
2 Monte-Carlo Methods
1.0
0.5
0.6
0.8
1.0
1.2
0.0
T / Tc
œà
SRO
LRO
CV
Kittler and Falicov
Cu3Au
A3B
Cu3Au
MC
Figure 2.4
Long-range order parameter (LRO)
and absolute value of nearest-neighbor short-
range order parameter (SRO) plotted versus
temperature T (in units of the temperature Tc
where the Ô¨Årst-order transition occurs) for a
nearest-neighbor model of binary alloys on
the face-centered cubic lattice with A3B struc-
ture. Open circles: experimental data for Cu3Au;
broken and dash-dotted curves: results of ana-
lytical theories. Full dots: Monte-Carlo results
obtained in the semi-grand canonical ensemble
(chemical potential diÔ¨Äerence between A and B
atoms treated as independent variable); circles
with crosses: values obtained from a canonical
ensemble simulation (concentration of B atoms
Ô¨Åxed at 25%). (Source: From Binder et al. [75].)
polymerization NA = NB = N) on chain
length N. The classical Flory‚ÄìHuggins
theory predicted Tc ‚àùN, while an inte-
gral equation theory predicted Tc ‚àù
‚àö
N
[76]. This law would lead in the plot of
Figure 2.5 to a straight line through the
origin. Obviously, the data seem to rule
out this behavior, and are rather quali-
tatively consistent with Flory‚ÄìHuggins
theory
(though
the
latter
signiÔ¨Åcantly
overestimates the prefactor in the relation
Tc ‚àùN).
Polymer physics provides examples for
many questions where simulations could
contribute signiÔ¨Åcantly to provide a better
understanding. Figure 2.6 provides one
more example [78]. The problem is to
provide truly microscopic evidence for
the reptation concept [79]. This concept
implies that, as a result of ‚Äúentanglements‚Äù
between chains in a dense melt, each chain
moves snakelike along its own contour.
This behavior leads to a special behavior of
mean square displacements: After a charac-
teristic time ùúèe, one should see a crossover
from a law g1(t) ‚â°‚ü®[ri(t) ‚àíri(0)]2‚ü©‚àùt1‚àï2
(Rouse model) to a law g1(t) ‚àùt1‚àï4, and, at a
still later time (ùúèR), one should see another
crossover to g1(t) ‚àùt1‚àï2 again. At the same
time, the center of gravity displacement
should also show an intermediate regime
of anomalously slow diÔ¨Äusion, g3(t) ‚àùt1‚àï2.
Figure 2.6 provides qualitative evidence for
these predictions, although the eÔ¨Äective
exponents indicated do not quite have the
expected values.
While this is an example where dynamic
Monte-Carlo simulations are used to check
theories and pose further theoretical ques-
tions, one can also compare to experiment
if one uses data in suitably normalized
form. In Figure 2.7, the diÔ¨Äusion constant
D of the chains is normalized by its value
in the Rouse regime (limit for small N)
and plotted versus N/Ne where the char-
acteristic ‚Äúentanglement chain length‚Äù Ne
is extracted from ùúèe shown in Figure 2.6

2.10 Selected Applications in Classical Statistical Mechanics of Condensed Matter
65
1.0
1.5
2.0
2.5
3.0
0
0
0.1
0.15
0.2
N‚àí1/2
0.3
0.25
0.05
0.5
kBTc / N…õ
256 128
32
16
64
Figure 2.5
Normalized critical temperature
kBTc‚àïNùúÄof a symmetric polymer mixture (N is
the chain length, ùúÄis the energy parameter
describing the repulsive interaction between
A‚ÄìB pairs of monomers) plotted versus N‚àí1‚àï2.
Data are results of simulations for the bond-
Ô¨Çuctuation model, 16 ‚â§N ‚â§256. The data
are consistent with an asymptotic extrapo-
lation kBTc‚àïùúÄ‚âà2.15 N, while Flory‚ÄìHuggins
theory (in the present units) would yield
kBTc‚àïùúÄ‚âà7N, and the integral equation the-
ory kBTc
ùúñ
‚àù
‚àö
N. (Source: From Deutsch and
Binder [77].)
100 000
10 000 000
10 00 000
t (MCS)
g3 (t)
g1 (t)
t0.8
œÑR
œÑe
t0.3
t0.9
t0.62
t0.5
t0.5
1000
500
200
100
50
20
10
5
Figure 2.6
Log-log plot of the mean square
displacements of inner monomers [g1(t)] and
of the center of gravity of the chain [g3(t)] ver-
sus time t (measured in units of Monte-Carlo
steps, while lengths are measured in units of
the lattice spacing). Straight lines show various
power laws as indicated; various characteristic
times are indicated by arrows (see text). Data
refer to the bond-Ô¨Çuctuation model on the sim-
ple cubic lattice, for an athermal model of a
polymer melt with chain length N = 200 and
a volume fraction ùúô= 0.5 of occupied lattice
sites. (Source: From Paul et al. [78].)

66
2 Monte-Carlo Methods
1
0.1
0.1
0.2
0.5
10
1.0
N / Ne
Ne
D / DRouse
Symbol
Method
35
96
30
40
MD - simulation
PE NMR data
This work √ò = 0.5
This work √ò = 0.4
Figure 2.7
Log-log plot of the self-diÔ¨Äusion
constant D of polymer chains, normalized by
the Rouse diÔ¨Äusivity, versus N/Ne (Ne is the
entanglement chain length, estimated inde-
pendently, and indicated in the inset). Circles,
from Monte-Carlo simulations of Paul et al. [80];
squares, from molecular dynamics simulations
[81]; triangles, experimental data [82]. (Source:
From [80].)
1500
1000
500
0
0.90
0.95
1.00
1.05
1.10
q / œÄ
20
40
63
100
140
200
300
400
S (q, t)
Figure 2.8
Structure factor S(q, t) versus wavevector q
at various times t (in units MCS per lattice site), after
the system was started in a completely random conÔ¨Åg-
uration. Temperature (measured in units of the nearest-
neighbor repulsion Wnn) is 1.33 (Tc ‚âà2.07 in these
units), and coverage ùúÉ= 1‚àï2. (Source: From Sadiq and
Binder [83].)

2.11 Concluding Remarks
67
(see [78, 80], for details). The Monte-Carlo
data presented in this scaled form agree
with results from both a MD simulation
[81]
and
experiment
on
polyethylene
(PE) [82].
This example also shows that, for slow
diÔ¨Äusive motions, Monte-Carlo simula-
tion is competitive to MD, although it
does not describe the fast atomic motions
realistically.
2.10.3
Surface Physics
Our last example considers phenomena
far from thermal equilibrium. Studying
the ordering behavior of superstructures,
we treat the problem where initially the
adatoms are adsorbed at random, and one
gradually follows the formation of ordered
domains out of initially disordered conÔ¨Åg-
urations. In a scattering experiment, one
sees this by the gradual growth of a peak
at the Bragg position qB. Figure 2.8 shows
a simulation for the case of the (2 √ó 1)
structure on the square lattice, where the
Bragg position is at the Brillouin-zone
boundary (qB = ùúãif lengths are measured
in units of the lattice spacing). Here a lattice
gas with repulsive interactions between
nearest and next-nearest neighbors (of
equal strength) was used [83], using a
single-spin-Ô¨Çip kinetics (if the lattice gas is
translated to an Ising spin model). This is
appropriate for a description of a mono-
layer in equilibrium with surrounding gas,
the random ‚Äúspin Ô¨Çips‚Äù then correspond to
random evaporation-condensation events.
Figure 2.9 presents evidence that these data
on the kinetics of ordering satisfy a scaling
hypothesis, namely,
S(q, t) = [L(t)]2ÃÉS(|q ‚àíqB|L(t)),
(2.44)
where ÃÉS is a scaling function. This hypoth-
esis, (2.44), was Ô¨Årst proposed using sim-
ulations, and later it was established to
describe experimental data as well.
2.11
Concluding Remarks
In this chapter, the basic features of the
most widely used numerical techniques
that fall into the category of Monte-Carlo
calculations were described. There is a
vast literature on the subject; the author
estimates the number of papers using
Monte-Carlo methods in condensed mat-
ter physics of the order of 105, in lattice
gauge theory of the order of 104. Thus many
important variants of algorithms could not
be treated here and interesting applications
(e.g., the study of neural-network models)
were completely omitted.
There
also
exist
other
techniques
for the numerical simulation of com-
plex systems, which sometimes are an
alternative
approach
to
Monte-Carlo
simulation. The MD method (numerical
integration of Newton‚Äôs equations) has
already been mentioned in the text, and
there exist combinations of both meth-
ods
(‚Äúhybrid
Monte-Carlo,‚Äù
‚ÄúBrownian
dynamics,‚Äù etc.). A combination of MD
with the local-density approximation of
quantum mechanics is the basis of the
Car‚ÄìParrinello method.
Problems
such
as
that
shown
in
Figures 2.8 and 2.9 can also be formu-
lated in terms of numerically solving
appropriate diÔ¨Äerential equations, which
may in turn even be discretized to cellular
automata. When planning a Monte-Carlo
simulation, hence, some thought to the
question ‚ÄúWhen which method?‚Äù should
be given.

68
2 Monte-Carlo Methods
‚àí0.4
‚àí0.2
0.0
+0.2
+0.4
(q/œÄ ‚àí 1) / œÉ(t)
S (q, t) / S (œÄ, t)
1.0
0.75
0.50
0.25
0.0
Symbol
t
400
300
200
140
100
Figure 2.9
Structure factor of Figure 2.8 plotted in scaled
form, normalizing S(q, t) by its peak value S(ùúã, t) and
normalizing q‚àïùúã‚àí1 by the half-width ùúé(t) = L‚àí1(t), where
L(t) thus deÔ¨Åned is the characteristic domain size. (Source:
From Sadiq and Binder [83].)
Glossary
Critical slowing down: Divergence of the
relaxation time of dynamic models of sta-
tistical mechanics at a second-order phase
transition (critical point).
Detailed balance principle: Relation link-
ing the transition probability for a move and
the transition probability for the inverse
move to the ratio of the probability for
the occurrence of these states in thermal
equilibrium. This condition is suÔ¨Écient for
a Markov process to tend toward thermal
equilibrium.
Ergodicity: Property that ensures equal-
ity of statistical ensemble averages (such as
the ‚Äúcanonic ensemble‚Äù) and time averages
along the trajectory of the system through
phase space.
Finite-size scaling: Theory that describes
the Ô¨Ånite-size-induced rounding of singu-
larities that would occur at phase transi-
tions in the thermodynamic limit.
Heat-bath method: Choice of transition
probability where the probability to ‚Äúdraw‚Äù
a trial value for a degree of freedom does
not depend on its previous value.
Importance-sampling:
Monte-Carlo
method that chooses the states that are gen-
erated according to the desired probability
distribution. For example, for statistical
mechanics, states are chosen with weights
proportional to the Boltzmann factor.
Lattice gauge theory: Field theory of
quarks and gluons in which space and time

References
69
are discretized into a four-dimensional lat-
tice, gauge Ô¨Åeld variables being associated
to the links of the lattice.
Master equation: Rate equation describ-
ing the ‚Äútime‚Äù evolution of the probabil-
ity that a state occurs as a function of
a ‚Äútime‚Äù coordinate labeling the sequence
of states (in the context of importance-
sampling Monte-Carlo methods).
Molecular dynamics method: Simulation
method for interacting many-body systems
based on numerical integration of the New-
tonian equations of motion.
Monte-Carlo step: Unit of (pseudo) time
in (dynamically interpreted) importance
sampling where, on the average, each
degree of freedom in the system gets one
chance to be changed (or ‚Äúupdated‚Äù).
Random-number
generator
(RNG):
Computer subroutine to produce pseudo-
random numbers that are approximately
not correlated with each other and approx-
imately
uniformly
distributed
in
the
interval from zero to one. RNGs typically
are strictly periodic, but the period is large
enough that, for practical applications, this
periodicity does not matter.
Simple sampling: Monte-Carlo method
that
chooses
states
uniformly
and
at
random from the available phase space.
Transition probability: Probability that
controls the move from one state to the
next one in a Monte-Carlo process.
References
1. Hammersley, J. M., and Handscomb, D. C.
(1964) Monte-Carlo Methods, Chapman &
Hall, London.
2. Bruns, W., Motoc, I., and O‚ÄôDriscoll, K. F.
(1981) Monte-Carlo Applications in Polymer
Science, Springer, Berlin.
3. Compagner, A. (1991) Am. J. Phys. 59,
700‚Äì705.
4. Knuth, D. (1969) The Art of Computer
Programming, Vol. 2, Addison-Wesley,
Reading, MA.
5. James, F. (1990) Comput. Phys. Commun. 60,
329‚Äì344.
6. Mascagni, M. and Srinivasan, A. (2000)
ACM Trans. Math. Software 26, 436‚Äì461.
7. Marsaglia, G. A. (1985) in: L. Billard Ed.
Computer Science and Statistics: The
Interface, Chapter 1, Elsevier, Amsterdam.
8. Compagner, A. and Hoogland, A. (1987) J.
Comput. Phys. 71, 391‚Äì428.
9. Coddington, P. D. (1994) Int. J. Mod. Phys.
C5, 547.
10. Marsaglia, G. A., Narasumhan, B., and
Zaman, A. (1990) Comput. Phys. Commun.
60, 345‚Äì349.
11. Lehmer, D. H. (1951) Proceedings of the 2nd
Symposium on Large-Scale Digital
Computing Machinery, Harvard University,
Cambridge, pp. 142‚Äì145.
12. Marsaglia, G. A. (1986), Proc. Natl. Acad.
Sci. U.S.A. 61, 25‚Äì28.
13. Tausworthe, R. C. (1965) Math. Comput. 19,
201‚Äì208.
14. Kirkpatrick, S. and Stoll, E. (1981) J. Comput.
Phys. 40, 517‚Äì526.
15. Ahrens, J. H. and Dieter, U. (1979) Pseudo
Random Numbers, John Wiley & Sons, Inc.,
New York.
16. Koonin, S. E. (1981) Computational Physics,
Benjamin, Reading, MA.
17. Gould, H. and Tobochnik, J. (1988) An
Introduction to Computer Simulation
Methods/Applications to Physical Systems,
Parts 1 and 2, Addison-Wesley, Reading,
MA.
18. StauÔ¨Äer, D. and Aharony, A. (1994) An
Introduction to Percolation Theory, Taylor &
Francis, London.
19. Herrmann, H. J. (1986) Phys. Rep. 136,
153‚Äì227.
20. Kremer, K. and Binder, K. (1988) Comput.
Phys. Rep. 7, 259‚Äì310.
21. Milchev, A., Binder, K., and Heermann,
D. W. (1986) Z. Phys. B 63, 521‚Äì535.
22. Rosenbluth, M. N. and Rosenbluth, A. W.
(1955) J. Chem. Phys. 23, 356‚Äì362.
23. Grassberger, P. (1997) Phys. Rev. E56,
3682‚Äì3693.
24. Hsu, H.-P., and Grassberger, P. (2011) J. Stat.
Phys. 144, 597‚Äì637.
25. Vicsek, T. (1989) Fractal Growth
Phenomena, World ScientiÔ¨Åc, Singapore.

70
2 Monte-Carlo Methods
26. Meakin, P. (1998) Fractals, Scaling and
Growth Far From Equilibrium. Cambridge
University Press, Cambridge.
27. Tolman, S. and Meakin, P. (1989) Phys. Rev.
A40, 428‚Äì437.
28. Metropolis, N., Rosenbluth, A. W.,
Rosenbluth, M. N., Teller, A.M., and Teller,
E. (1953) J. Chem. Phys. 21, 1087‚Äì1092.
29. Binder, K. (1976) in: C. Domb, M.S. Green
Eds. Phase Transitions and Critical
Phenomena, Vol. 5b, p. 1, Academic Press,
New York.
30. Binder, K. and Heermann, D. W. (2010)
Monte-Carlo Simulation in Statistical
Physics: An Introduction, 5th edn, Springer,
Berlin.
31. Kalos, M. H. and Whitlock, P. A. (1986)
Monte-Carlo Methods, Vol. 1, John Wiley &
Sons, Inc., New York.
32. Landau, D. P. (1992) in: K. Binder Ed. The
Monte-Carlo Method in Condensed Matter
Physics, Chapter 2, Springer, Berlin.
33. Swendsen, R. H., Wang, J. S., and Ferrenberg,
A. M. (1992), in: K. Binder Ed., The
Monte-Carlo Method in Condensed Matter
Physics, Chapter 4, Springer, Berlin.
34. Berg, B. A. (2004) Markov Chain
Monte-Carlo Simulations and Their
Statistical Analysis. World ScientiÔ¨Åc,
Singapore.
35. Rapaport, D. C. (2004) The Art of Molecular
Dynamics Simulation, 2nd edn, Cambridge
University Press, Cambridge.
36. Landau, D. P. and Binder, K. (2009) A Guide
to Monte-Carlo Simulation in Statistical
Physics, 3rd edn., Cambridge University
Press Cambridge.
37. Panagiotopoulos, A. Z. (1992) Mol. Simul. 9,
1‚Äì23.
38. Frenkel, D. and Smit, B. (2002)
Understanding Molecular Simulation: From
Algorithms to Applications, 2nd edn.,
Academic Press, San Diego, CA.
39. Barber, M. N. (1983), in: C. Domb, and J. L.
Lebowitz Eds, Phase Transitions and Critical
Phenomena, Chapter 2, Vol. 8, Academic
Press, New York.
40. Binder, K. (1987) Ferroelectrics 73, 43‚Äì67.
41. Binder K. (1997) Rep. Progr. Phys. 60,
487‚Äì559.
42. Privman, V. Ed. (1990) Finite Size Scaling
and Numerical Simulation of Statistical
Systems, World ScientiÔ¨Åc, Singapore.
43. Stanley, H. E. (1971) An Introduction to
Phase Transitions and Critical Phenomena,
Oxford University Press, Oxford.
44. Wang, F. and Landau, D. P. (2001) Phys. Rev.
E 64, 056101.
45. Binder, K., Block, B., Das, S. K., Virnau, P.,
and Winter, D. (2011) J. Stat. Phys. 144,
690‚Äì729.
46. Grzelak, E. M. and Errington, J. R. (2008) J.
Chem. Phys. 128, 014710.
47. Bolhuis, P. G., Chandler, D., Dellago, C., and
Geissler, P. L. (2002) Ann. Rev. Phys. Chem.
53, 291.
48. Ceperley, D. M. and Kalos, M. H. (1979) in:
K. Binder Ed. Monte-Carlo Methods in
Statistical Physics, Springer, Berlin, pp.
145‚Äì194.
49. Kalos, M. H. Ed. (1984) Monte-Carlo
Methods in Quantum Problems, Reidel,
Dordrecht.
50. Berne, B. J. and Thirumalai, D. (1986) Annu.
Rev. Phys. Chem. 37, 401.
51. Suzuki, M. Ed. (1986) Quantum
Monte-Carlo Methods, Springer, Berlin.
52. Schmidt, K. E. and Ceperley, D. M. (1992) in
K. Binder Ed., The Monte-Carlo Methods in
Condensed Matter Physics, Springer, Berlin,
pp. 105‚Äì148.
53. Ceperley, D. M. (1995) Rev. Mod. Phys. 67,
279‚Äì355.
54. Ceperley, D. M. (1996) in K. Binder and G.
Ciccotti Eds., Monte-Carlo and Molecular
Dynamics of Condensed Matter Systems,
Societa Italiana di Fisica, Bologna, pp.
443‚Äì482.
55. Rothman, S. ed. (2002) Recent Advances in
Quantum Monte-Carlo Methods II, World
ScientiÔ¨Åc, Singapore.
56. Trebst, S. and Troyer M. (2006) In: M.
Ferrario, G. Ciccotti, and K. Binder eds.
Computer Simulations in Condensed Matter:
From Materials to Chemical Biology,
Vol. 1,Springer, Berlin, pp. 591‚Äì640.
57. Carlson, J. (1988) Phys. Rev. C 38,
1879‚Äì1885.
58. De Grand, T. (1992) in: H. Gausterer and C.
B. Lang Eds., Computational Methods in
Field Theory, Springer, Berlin, pp. 159‚Äì203.
59. Frick, M., Pattnaik, P. C., Morgenstern, I.,
Newns, D. M., von der Linden, W. (1990)
Phys. Rev. B 42, 2665‚Äì2668.
60. Reger, J. D. and Young, A. P. (1988) Phys. Rev.
B 37, 5978‚Äì5981.

Further Reading
71
61. Sachdev, S. (2000) Quantum Phase
Transitions, Cambridge University Press,
Cambridge.
62. Marx, D., Opitz, O., Nielaba, P., and Binder,
K. (1993) Phys. Rev. Lett. 70, 2908‚Äì2911.
63. Gillan, M. J. and Christodoulos, F. (1993) Int.
J. Mod. Phys. C4, 287‚Äì297.
64. West, G. B. (1975), Phys. Rep. 18C, 263‚Äì323.
65. Sokol, P. E., Sosnick, T. R., and Snow, W. M.
(1989), in R. E. Silver and P. E. Sokol Eds.
Momentum Distributions, Plenum Press,
New York.
66. Ceperley and Pollock (1987) Can. J. Phy. 65,
1416‚Äì1423.
67. Schmidt, K. E. and Ceperley, D. M. (1992),
in: K. Binder Ed. The Monte-Carlo Method in
Condensed Matter Physics, pp. 203‚Äì248,
Springer, Berlin.
68. Ceperley, D. M., Alder, B. J. (1980) Phys. Rev.
Lett. 45, 566‚Äì569.
69. Rebbi, C. (1984), in: K. Binder Ed.
Application of the Monte-Carlo Method in
Statistical Physics, Springer, Berlin, p. 277.
70. Wilson, K. (1974) Phys. Rev. D10,
2445‚Äì2453.
71. Butler, F., Chen, H., Sexton, J., Vaccarino, A.,
and Weingarten, D. (1993) Phys. Rev. Lett.
70, 2849‚Äì2852.
72. D√ºrr, S. et al. (2008) Science 322,
1224‚Äì1227.
73. Binder, K. Ed. (1979) Monte-Carlo Methods
in Statistical Physics, Springer, Berlin.
74. Binder, K. Ed. (1995) The Monte-Carlo
Method in Condensed Matter Physics,
Springer, Berlin.
75. Binder, K., Lebowitz, J. L., Phani, M. K., and
Kalos, M. H. (1981) Acta Metall. 29,
1655‚Äì1665.
76. Schweizer, K. S. and Curro, J. G. (1990)
Chem. Phys. 149, 105‚Äì127.
77. Deutsch, H. P. and Binder, K. (1992)
Macromolecules 25, 6214‚Äì6230.
78. Paul, W., Binder, K., Heermann, D. W., and
Kremer, K. (1991) J. Chem. Phys. 95,
7726‚Äì7740.
79. Doi, M. and Edwards, S. F. (1986) Theory of
Polymer Dynamics, Clarendon Press,
Oxford.
80. Paul, W., Binder, K., Heermann, D. W., and
Kremer, K. (1991) J. Phys. (Paris) 111,
37‚Äì60.
81. Kremer, K. and Grest, G. S. (1990) J. Chem.
Phys. 92, 5057‚Äì5086.
82. Pearson, D. S., Verstrate, G., von Meerwall,
E., and Schilling, F. C. (1987)
Macromolecules 20, 113‚Äì1139.
83. Sadiq, A. and Binder, K. (1984) J. Stat. Phys.
35, 517‚Äì585.
Further Reading
A textbook describing for the beginner how
to learn to write Monte-Carlo programs
and to analyze the output generated by
them has been written by Binder, K., Heer-
mann, D.W. (2010), Monte-Carlo Simula-
tion in Statistical Physics: An Introduction,
5th edn, Berlin: Springer. This book empha-
sizes applications of statistical mechanics
such as random walks, percolation, and the
Ising model.
More extensive and systematic textbooks
have been written by D. Frenkel and B. Smit
(2002), Understanding Molecular Simula-
tion: From Algorithms to Applications, San
Diego, CA: Academic Press, and by D. P.
Landau and K. Binder A Guide to Monte-
Carlo Simulations in Statistical Physics,
Cambridge: Cambridge University Press
Practical Considerations are emphasized
in B. A. Berg (2004) Markov Chain Monte-
Carlo Simulations and Their Statistical
Analysis, Singapore: World ScientiÔ¨Åc.
A useful book that gives much weight to
applications outside of statistical mechan-
ics is Kalos, M. H., Whitlock, P. A. (1986),
Monte-Carlo Methods, vol. 1, New York:
John Wiley & Sons, Inc.
A more general but pedagogic introduc-
tion to computer simulation is presented
in Gould, H., Tobochnik, J. (1988), An
Introduction
to
Computer
Simulation
Methods/Applications to Physical Systems,
Parts 1 and 2, Reading, MA: Addison-
Wesley.


73
3
Stochastic DiÔ¨Äerential Equations
Gabriel J. Lord
3.1
Introduction
The consequence of inclusion of random
eÔ¨Äects in ordinary diÔ¨Äerential equations
(ODEs) falls in to two broad classes ‚Äì one
in which the solution has a smooth path
and one where the path is not smooth
(i.e., it is not diÔ¨Äerentiable). If the random
eÔ¨Äect is included as a random choice of
a parameter or as a random initial condi-
tion, u0, then we have random diÔ¨Äerentiable
equations. Each realization (i.e., each pick
of the parameter from a random set) can be
solved using standard deterministic meth-
ods (see Chapter 14). If we need to solve
a large number of times to get averaged
properties using Monte Carlo techniques
(see Chapter 2), then eÔ¨Écient schemes are
required.
If the forcing is rough, then the solu-
tions are not diÔ¨Äerentiable and new tech-
niques are required (in fact, a new calculus
is required). Here we concentrate on diÔ¨Äer-
ential equations that are coupled to some
random forcing ùúÅ(t) that is Gaussian white
noise, so the random forces have mean zero
and are uncorrelated at diÔ¨Äerent times t.
When ùúÅ(t) is white, it can be characterized
as the derivative of Brownian motion W(t),
so that ‚ÄúùúÅ(t) = dW‚àïdt.‚Äù Ordinary diÔ¨Äer-
ential equations with this sort of forcing
are called stochastic diÔ¨Äerential equations
(SDEs).
Example 3.1 [Ornstein‚ÄìUhlenbeck (OU)
process] Consider a particle of unit mass
moving with momentum p(t) at time t in
a liquid. The dynamics of the particle can
be modeled by a dissipative force ‚àíùúÜp(t),
where ùúÜ> 0 is known as the dissipation
constant. In the absence of any external
forcing, we have
dp
dt = ‚àíùúÜp,
given p(0) = p0 ‚àà‚Ñù
(3.1)
and the particle will converge to the rest
state p = 0 as t ‚Üí‚àû. This is not physi-
cal as the particle is subject to irregular
bombardment by molecules in the liquid
which can be modeled as a Ô¨Çuctuating force
ùúéùúÅ(t), where ùúÅ(t) is white noise and ùúé> 0
is called the diÔ¨Äusion constant. Newton‚Äôs
second law of motion gives the acceleration
dp‚àïdt as the sum of the two forces. We have
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

74
3 Stochastic DiÔ¨Äerential Equations
dp
dt = ‚àíùúÜp + ùúéùúÅ(t),
(3.2)
which is a linear SDE with additive noise
ùúÅ(t) = dW‚àïdt. It is also written as
dp = ‚àíùúÜp dt + ùúédW(t).
(3.3)
The solution p(t) is known as the Orn-
stein‚ÄìUhlenbeck (OU) process. We will see
that dW‚àïdt does not strictly speaking make
sense as a function and we need to inter-
pret the SDE as an integral equation. In
Figure 3.1, we show two sample realizations
of (3.3) computed numerically and we also
plot, for reference, the solution of the cor-
responding deterministic equation (3.1), all
with the same initial data.
Note that (3.2) can also be interpreted as
a model for thermal noise in an electrical
resistor.
Our aim is to make sense of equations
such as (3.3) and to introduce some of
the techniques for looking at them. Con-
sider the general ODE du‚àïdt = f (u) with
initial data u(0) = u0 and recall that it can
be integrated to get an equivalent integral
equation
u(t) = u(0) + ‚à´
t
0
f (s, u(s))ds.
When we include the eÔ¨Äects of a stochastic
forcing term, ùúÅ(t), we have the SDE
du
dt = f (u) + g(u)ùúÅ(t).
(3.4)
When ‚ÄúùúÅ(t) = dW‚àïdt,‚Äù where W is Brown-
ian motion, we will understand this as the
integral equation
u(t) = u0+‚à´
t
0
f (u(s))ds + ‚à´
t
0
g(u(s))dW(s).
(3.5)
Equation (3.4) and the integral equation
above are often written in shorthand as the
SDE
du = f (u)dt + g(u)dW.
(3.6)
In (3.6), f (u) is called the drift term and g(u)
is called the diÔ¨Äusion term . When g is inde-
pendent of u, such as in (3.3) where g = ùúé,
0
(a)
(b)
10
20
30
40
50
60
70
80
90 100
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
t
u(t)
‚àí2 ‚àí1.5 ‚àí1 ‚àí0.5
0
0.5
1
1.5
2
0
50
100
150
200
250
300
p
Frequency
Figure 3.1
(a) Two numerical solutions of the
SDE (3.3) on the interval [0, T] with ùúÜ= 0.5
and ùúé= 0.5. The initial data is the same in
each case, u(0) = 1. With each realization of
the noise we obtain a diÔ¨Äerent solution path.
Also shown is the solution of the determin-
istic ODE (3.1). In (b), we examine the distri-
bution at t = 100 showing a histogram from
2000 diÔ¨Äerent realizations; note that as t ‚Üí‚àû
p(t) ‚ÜíN(0, ùúé2‚àï2ùúÜ).

3.2 Brownian Motion / Wiener Process
75
the SDE is said to have additive (or extrin-
sic) noise. When g(u) varies with u, then the
SDE has multiplicative (or intrinsic) noise.
Remark 3.1 Often in the probabilistic and
stochastic literature, time dependence of a
variable u is denoted by ut, so that u(t) ‚â°ut,
W(t) ‚â°Wt. We have not used that notation
here as it can lead to confusion with partial
derivatives.
3.2
Brownian Motion / Wiener Process
Although there are many diÔ¨Äerent types of
stochastic processes that could be taken,
we look at one type of noise (probably the
most common) that arises from Brownian
motion. A Scottish botanist, Robert Brown,
observed that a small particle such as a
seed in a liquid or gas moves about in
a seemingly random way. This is because
it is being hit continuously by the gas or
liquid molecules. Later, Norbert Wiener,
an American mathematician, gave a math-
ematical description of the process and
proved its existence. In the literature, both
the term Brownian motion and Wiener
process are widely used; here we will use
Brownian motion and denote it by W.
DeÔ¨Ånition 3.1 A scalar standard Brown-
ian motion or standard Wiener process is
a collection of random variables W(t) that
satisfy
1. W(0) = 0 with probability 1.
2. For 0 ‚â§s ‚â§t, the random variable given
by the increment W(t) ‚àíW(s) is
normally distributed with mean zero
and variance t ‚àís. Equivalently,
W(t) ‚àíW(s) ‚àº
‚àö
t ‚àís N(0, 1).
3. For 0 ‚â§s < t ‚â§u < v the increments
W(t) ‚àíW(s) and W(v) ‚àíW(u) are
independent.
4. W(t) is continuous in t.
Brownian motion is often used to model
random eÔ¨Äects that are, on a microscale,
small, random, independent, and additive,
where a random behavior is seen on a
larger scale. The central limit theorem (see
Chapter 1) then gives that the sum of these
small contributions converges to incre-
ments that are normally distributed on the
large scale. We recall that the covariance
is a measure of how a random variable X
changes against a diÔ¨Äerent random variable
Y and
Cov (X, Y) ‚à∂= E[(X ‚àíE[X])(Y ‚àíE[Y])]
= E[XY] ‚àíE[X]E[Y].
Thus the variance of a random variable
is given by Var (X) = Cov (X, X). To look
at the correlation in time of a random
variable X and Y, we consider the covari-
ance function c(s, t) ‚à∂= Cov (X(s), Y(t)), for
s, t ‚àà[0, T]. If X = Y, then this is called the
autocovariance. When c(s, t) = c(s ‚àít), the
covariance is said to be stationary.
The following properties of W
are
straightforward to see.
Lemma 3.1 Let W(t), t ‚â•0, be a standard
Brownian motion. Then,
1. E[W(t)] = 0 and E[(W(t))2] = t.
2. Cov (W(s), W(t)) = min(s, t).
3. the process Y(t) = ‚àíW(t) is also a
standard Brownian motion.
Proof. As
W(0) = 0
and
W(t) =
W(t) ‚àíW(0) ‚àºN(0, t), we Ô¨Ånd that
E[W(t)] = 0
E[(W(t))2] = t.
The
covariance
Cov (W(s), W(t)) =
E[W(t)W(s)] because E[W(t)] = 0. Let us
take 0 ‚â§s ‚â§t. Then
E[W(t)W(s)] = E[(W(s) + W(t)
‚àíW(s))W(s)]

76
3 Stochastic DiÔ¨Äerential Equations
and so
E[W(t)W(s)] = E[(W(s))2]
+ E[(W(t) ‚àíW(s))W(s)].
From
DeÔ¨Ånition 3.1,
the
increments
W(t) ‚àíW(s) and W(s) ‚àíW(0) are inde-
pendent, so
E[W(t)W(s)] = s + E[(W(t) ‚àíW(s))]
E[W(s)] = s.
Thus
E[W(t)W(s)] = min(s, t),
for all s, t ‚â•0.
To show that Y(t) = ‚àíW(t) is also a
Brownian motion, it is straightforward to
verify the conditions of DeÔ¨Ånition 3.1.
The increment W(t) ‚àíW(s) of a Brow-
nian motion over the interval [s, t] has
distribution N(0, t ‚àís) and hence the dis-
tribution of W(t) ‚àíW(s) is the same as
that of W(t + h) ‚àíW(s + h) for any h > 0;
that is, the distribution of the increments
is independent of translations and we say
that Brownian motion has stationary incre-
ments (recall the stationary covariance).
Another important property is that the
Brownian motion is self-similar, that is,
by a suitable scaling, we obtain another
Brownian motion.
Lemma 3.2 (self-similarity) If W(t) is a
standard Brownian motion then for ùõº>
0, Y(t) ‚à∂= ùõº1‚àï2W(t‚àïùõº) is also a Brownian
motion.
Proof. To show that Y is Brownian motion,
we need to check the conditions of DeÔ¨Åni-
tion 3.1.
1. Y(0) = W(0) = 0 with probability 1
2. Consider s < t and increment
Y(t) ‚àíY(s) = ùõº1‚àï2(W(t‚àïùõº) ‚àíW(s‚àïùõº)).
This has distribution
ùõº1‚àï2N(0, ùõº‚àí1(t ‚àís)) = N(0, (t ‚àís)).
3. Increments are independent. Consider
Y(t) ‚àíY(s) = ùõº1‚àï2(W(t‚àïùõº) ‚àíW(s‚àïùõº))
and
Y(v) ‚àíY(u) = ùõº1‚àï2(W(v‚àïùõº) ‚àíW(u‚àïùõº))
for 0 ‚â§s ‚â§t ‚â§u ‚â§v ‚â§T, because
(W(t‚àïùõº) ‚àíW(s‚àïùõº)) and
(W(v‚àïùõº) ‚àíW(u‚àïùõº)) are independent.
4. Continuity of Y follows from the
continuity of W.
Hence we have shown Y(t) is also a Brow-
nian motion.
We now examine how rough Brownian
motion is. To do this, we will need to con-
sider convergence of random variables. We
say that the random variables Xj converge
to X in mean square if
E[‚ÄñXj ‚àíX‚Äñ2] ‚Üí0,
as
j ‚Üí‚àû
and they converge in root mean square if
{E[‚ÄñXj ‚àíX‚Äñ2]}1‚àï2 ‚Üí0,
as
j ‚Üí‚àû.
We need to specify a norm. For vectors,
we will take the standard Euclidean norm
‚Äñ ‚Äñ2 on ‚Ñùd with inner product ‚ü®xxx,yyy‚ü©for xxx,
yyy ‚àà‚Ñùd, so ‚Äñx‚Äñ2
2 = ‚ü®xxx,xxx‚ü©. For a matrix X ‚àà
‚Ñùd√óm, we will use the Frobenius norm
‚ÄñX‚ÄñF =
( d
‚àë
i=1
m
‚àë
j=1
|xij|2
)1‚àï2
=
‚àö
trace(X‚àóX),
where X‚àóis the conjugate transpose.
We start by examining the quadratic
variation,
‚àëN
j=0(W(tj+1) ‚àíW(tj))2
as
N ‚Üí‚àû. The following lemma will also be
useful for developing stochastic integrals
later.

3.2 Brownian Motion / Wiener Process
77
Lemma 3.3 (Quadratic variation) Let
0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN = T
be
a
parti-
tion of [0, T] where it is understood that
Œît ‚à∂= max1‚â§j‚â§N |tj+1 ‚àítj| ‚Üí0 as N ‚Üí‚àû.
Let
ŒîWj ‚à∂= (W(tj+1) ‚àíW(tj)).
Then
‚àëN
j=0(ŒîWj)2 ‚ÜíT
in
mean
square
as
N ‚Üí‚àû.
Proof. Let
Œîtj ‚à∂= tj+1 ‚àítj,
VN ‚à∂=
‚àëN
j=0(ŒîWj)2 and note that
VN ‚àíT =
N‚àí1
‚àë
j=0
[(ŒîWj)2 ‚àíŒîtj
] .
The terms (ŒîWj)2 ‚àíŒîtj are independent
and have mean 0. Therefore, because the
cross terms are 0,
E[(VN ‚àíT)2] =
N‚àí1
‚àë
j=0
E
[((ŒîWj)2 ‚àíŒîtj
)2]
,
and
(
(ŒîWj)2 ‚àíŒîtj
)2
= (ŒîWj)4 ‚àí2Œîtj(ŒîWj)2 + Œît2
j
=
(
(ŒîWj)4
Œît2
j
‚àí2
(ŒîWj)2
Œîtj
+ 1
)
Œît2
j
= (X2
j ‚àí1)2Œît2
j ,
where Xj ‚à∂= ŒîWj‚àï‚àöŒîtj ‚àºN(0, 1). Thus,
for some C > 0, we have
[(ŒîWj)2 ‚àíŒîtj
]2 ‚â§C
N‚àí1
‚àë
j=0
Œît2
j ‚â§CTŒît.
Hence as N ‚Üí‚àû, E
[
(VN ‚àíT)2] ‚Üí0.
We can now show that, although the
quadratic variation is Ô¨Ånite, the total vari-
ation of Brownian paths is unbounded.
Lemma 3.4 (Total variation) Let
0 = t0
< t1 < ¬∑ ¬∑ ¬∑ < tN = T
be
a
partition
of
[0, T]
as
in
Lemma 3.3.
Then
‚àëN
j=0 |W(tj+1) ‚àíW(tj)| ‚Üí‚àûas N ‚Üí‚àû.
Proof. The proof is by contradiction and we
outline the idea here. Note that
N
‚àë
j=0
(W(tj+1) ‚àíW(tj))2 ‚â§max
1‚â§j‚â§N |W(tj+1)
‚àíW(tj)|
N
‚àë
j=0
|W(tj+1) ‚àíW(tj)|.
Now Brownian motion is continuous, so
as N ‚Üí‚àû, we have max1‚â§j‚â§N |W(tj+1) ‚àí
W(tj)| ‚Üí0. If ‚àë|W(tj+1) ‚àíW(tj)| is Ô¨Ånite
then ‚àëN
j=0(W(tj+1) ‚àíW(tj))2 ‚Üí0, but this
contradicts Lemma 3.3.
The fact that ‚àëN
j=0 |W(tj+1) ‚àíW(tj)| ‚Üí
‚àûhints that Brownian motion, even if it
is continuous, may in fact be very rough.
In Section 3.2.1 we argue that Brownian
motion is not diÔ¨Äerentiable. First, however,
we extend the deÔ¨Ånition of a Brown-
ian motion on ‚Ñùto ‚Ñùd. We can easily
extend DeÔ¨Ånition 3.1 to processes W
W
W(t) =
(W1(t), W2(2), ‚Ä¶ , Wd(t))T ‚àà‚Ñùd
where
increments W
W
W(t) ‚àíW
W
W(s) ‚àº
‚àö
t ‚àís N(0, I).
Here N(0, I) is the d-dimensional normal
distribution with covariance matrix I, the
‚Ñùd√ód identity matrix. It is straightforward
to show the following lemma.
Lemma 3.5 The
process
W
W
W(t) =
(W1(t), W2(2), ‚Ä¶ , Wd(t))T ‚àà‚Ñùd
is
a
Brownian motion (or a Wiener process) if
and only if each of the components Wi(t) is
a Brownian motion.
3.2.1
White and Colored Noise
Despite
the nice properties of Brown-
ian motion, a well-known theorem of
Dvoretzky, Erd√∂s, and Kakutani states that
sample paths of Brownian motion are not
diÔ¨Äerentiable anywhere. To see why this
might be the case, note that from DeÔ¨Åni-
tion 3.1, the increment W(t + h) ‚àíW(t),

78
3 Stochastic DiÔ¨Äerential Equations
for h > 0, is a normal random variable
with mean 0 and variance h. Now con-
sider Y(t) = (W(t + h) ‚àíW(t))‚àïh. This is
a normal random variable with mean 0
and variance h‚àí1. For Y to approximate
a derivative of W, we need to look at
the limit as h ‚Üí0. We see that the vari-
ance of the random variable Y goes to
inÔ¨Ånity as h ‚Üí0 and, in the limit, Y is
not well behaved. The numerical deriva-
tive of two diÔ¨Äerent Brownian paths is
plotted in Figure 3.2. As Œît ‚Üí0 these
numerical derivatives do not converge to a
well-deÔ¨Åned function.
Although we have just argued that
Brownian motion W(t) is not diÔ¨Äeren-
tiable, often in applications, one will see
that dW(t)‚àïdt used. To understand this
properly, we will need to develop some
stochastic integration theory. Before we do
this, let us Ô¨Årst relate the term dW(t)‚àïdt
to ‚Äúwhite noise‚Äù with covariance (or the
autocorrelation function), which is the
Dirac delta function, so that
E
[dW(t)
dt
dW(s)
ds
]
= ùõø(s ‚àít).
(3.7)
Recall that the Dirac delta satisÔ¨Åes the fol-
lowing properties:
ùõø(s) = 0, for s ‚â†0,
‚à´
‚àû
‚àí‚àû
ùõø(s)ùúô(s)ds = ùúô(0)
for any continuous function ùúô(s). In partic-
ular, if ùúô(s) ‚â°1, then ‚à´‚àû
‚àí‚àûùõø(s)ds = 1.
To see why (3.7) might be true, let us Ô¨Åx s
and t and consider
d(s)‚à∂=E
[(W(t+h)‚àíW(t))
h
(W(s+h)‚àíW(s))
h
]
.
For small h, this should approximate the
Dirac delta. We can simplify d(s) to get
d(s) = 1
h2
(E[W(t + h)W(s + h)]
‚àíE
[
W(t + h)W(s)
]
‚àíE[W(t)W(s + h)] + E[W(t)W(s)])
and use Lemma 3.1 on the Brownian
motion to obtain
d(s) = 1
h2 (min(t + h, s + h) ‚àímin(t + h, s)
‚àímin(t, s + h) + min(t, s)).
We see that d is a piecewise linear function
with nodes at s = t ‚àíh, t, t + h and
d(s) =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™‚é©
0
for s ‚â§t ‚àíh
(s‚àít+h)
h2
for t ‚àíh < s < t
(t+h‚àís)
h2
for t < s < t + h
0
for s ‚â•t + h
.
Furthermore, ‚à´‚àû
‚àí‚àûd(s)ds = 1. Hence, we
see d approximates ùõø(s ‚àít) for small h,
which suggests (3.7) holds.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
‚àí100
‚àí50
0
50
100
dW1/dt
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
‚àí100
‚àí50
0
50
100
dW2/dt
t
Figure 3.2
Numerical derivatives of W1(t)
and W2(t) shown in Figure 3.3(a).

3.2 Brownian Motion / Wiener Process
79
We brieÔ¨Çy discuss two ways to see that
dW(t)‚àïdt can be interpreted as white noise
and what this means. The Ô¨Årst of these
is based on the Fourier transform of the
covariance function. The spectral density
of a stochastic process X with stationary
covariance function c (i.e., c(s, t) = c(s ‚àít))
is deÔ¨Åned for ùúÜ‚àà‚Ñùby
f (ùúÜ) ‚à∂= 1
2ùúã‚à´
‚àû
‚àí‚àû
e‚àíiùúÜtc(t)dt.
When c(t) = ùõø(t) we Ô¨Ånd for all frequencies
ùúÜ,
f (ùúÜ) = 1
2ùúã‚à´
‚àû
‚àí‚àû
e‚àíiùúÜtùõø(t)dt = 1
2ùúã.
Therefore, by analogy with white light, all
frequencies contribute equally.
Remark 3.2 Note
that
the
covariance
function can also be found from the inverse
Fourier transform of the spectrum. One
practical application of this is to estimate
the correlation function from numerical
data using the Fourier transform.
The precise statement is given in the fol-
lowing theorem.
Theorem 3.1 (Wiener‚ÄìKhintchine) The
following statements are equivalent.
1. There exists a mean square continuous
stationary process {X(t)‚à∂t ‚àà‚Ñù} with
stationary covariance c(t).
2. The function c ‚à∂‚Ñù‚Üí‚Ñùis such that
c(t) = ‚à´‚Ñù
eiùúàtf (ùúà) dùúà.
A second way to illustrate why dW(t)‚àïdt
and the covariance (3.7) are called white
noise is to consider t ‚àà[0, 1] and let
{ùúôj(t)}‚àû
j=1 be an orthonormal basis, for
example, ùúôj(t) =
‚àö
2 sin(jùúãt). Now con-
struct ùúÅ(t) by the random series with
t ‚àà[0, 1]
ùúÅ(t) ‚à∂=
‚àû
‚àë
j=1
ùúÅjùúôj(t),
where each ùúÅj ‚àºN(0, 1) and are indepen-
dent of each other. Then we have formed a
random ùúÅ(t) that has a homogeneous mix
of the diÔ¨Äerent basis functions ùúôj ‚Äì just like
white light consists of a homogeneous mix
of wavelengths. Let us look at the covari-
ance function for ùúÅ(t),
Cov (ùúÅ(s), ùúÅ(t)) =
‚àû
‚àë
j,k=1
Cov (ùúÅj, ùúÅk
)ùúôj(s)ùúôk(t)
=
‚àû
‚àë
j
ùúôj(s)ùúôk(t).
Although the Dirac delta ùõøis not strictly
speaking a function, formally we can write
ùõø(s) =
‚àû
‚àë
j=1
‚ü®ùõø, ùúôj(s)‚ü©ùúôj(s) =
‚àû
‚àë
j=1
ùúôj(0)ùúôj(s)
and so the covariance function for ùúÅ(t)
is Cov (ùúÅ(s), ùúÅ(t)) = c(s, t) = ùõø(s ‚àít); that is,
ùúÅ(t) is a stochastic process with a homoge-
neous mix of the basis functions that has
the Dirac delta as covariance function and
the noise is uncorrelated.
Colored noise, as the name suggests, has
a heterogeneous mix of the basis func-
tions. For example, consider the stochastic
process
ùúà(t) =
‚àû
‚àë
j=1
‚àöùúàjùúÅjùúôj(t),
(3.8)
where each ùúÅj ‚àºN(0, 1) and are indepen-
dent and ùúàj ‚â•0. As we vary the ùúàj with
j, the process ùúà(t) is said to be colored
noise and the random variables ùúà(t) and

80
3 Stochastic DiÔ¨Äerential Equations
ùúà(s) are correlated. Expansions of the form
(3.8) are a useful way to examine many
stochastic processes. When the basis func-
tions are chosen as the eigenfunctions of
the covariance function, this is termed the
Karhunen‚ÄìLo√®ve expansion.
Theorem 3.2 (Karhunen‚ÄìLo√®ve)
Consider a stochastic process {X(t)‚à∂t ‚àà
[0, T]} and suppose that E[‚à´T
0 (X(s))2ds] <
‚àû. Then,
X(t) = ùúá(t) +
‚àû
‚àë
j=1
‚àöùúàjùúôj(t)ùúâj,
ùúâj ‚à∂=
1
‚àöùúàj ‚à´
T
0
(X(s) ‚àíùúá(s))ùúôj(s)ds
(3.9)
and {ùúàj, ùúôj} denote the eigenvalues and
eigenfunctions of the covariance operator
so that ‚à´T
0 c(s, t)ùúôj(s)ds = ùúàjùúôj(t). The sum
in (3.9) converges in a mean square sense.
The random variables ùúâj have mean 0,
unit variance, and are pairwise uncor-
related. Furthermore, if the process is
Gaussian, then ùúâj ‚àºN(0, 1) independent
and identically distributed.
Another way to generate colored noise is
to solve a stochastic SDE, see Section 3.4,
whose
solution
has
a
particular
fre-
quency
distribution.
The
OU
process
of Example 3.1 is often used to generate
colored noise. For more information on
colored noise, see, for example, [1]. The
Wiener‚ÄìKhintchine theorem is covered
in a wide range of books, including [2‚Äì4].
The Karhunen‚ÄìLo√®ve expansion is widely
used to construct random Ô¨Åelds as well as
in data analysis and signal processing; see
also [4, 5].
3.2.2
Approximation of a Brownian Motion
We can use DeÔ¨Ånition 3.1 directly to con-
struct a numerical approximation Wn to a
Brownian motion W(t) at times tn, where
0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN. From (1) of DeÔ¨Åni-
tion 3.1 we have that W0 = W(0) = 0. We
construct the approximation by noting that
W(tn+1) = W(tn) + (W(tn+1) ‚àíW(tn)) and
from (2) of DeÔ¨Ånition 3.1 we know how
increments are distributed.
Letting Wn ‚âàW(tn), we get that
Wn+1 = Wn + dWn,
n = 1, 2, ‚Ä¶ , N,
where
dWn ‚àº
‚àö
ŒîtN(0, 1).
This
is
described in Algorithm 3.1 and two typ-
ical results of this process are shown
in Figure 3.3a. In the Ô¨Ågure, we use a
piecewise linear approximation to W(t)
for t ‚àà[tn, tn+1]. To approximate a d-
dimensional W
W
W by Lemma 3.5, we can
simply use Algorithm 3.1 for each com-
ponent; the result of this is shown in
Figure 3.3b.
Algorithm 3.1 Brownian motion in one
dimension. We assume t0 = 0.
INPUT : vector t = [t0, t1, ‚Ä¶ , tN]
OUTPUT: vector W such that component
Wn = W(tn).
1: W0 = 0
2: for n = 1 ‚à∂N do
3:
Œît = tn ‚àítn‚àí1
4:
Ô¨Ånd z ‚àºN(0, 1)
5:
dWn =
‚àö
Œîtz
6:
Wn = Wn‚àí1 + dWn
7: end for
3.3
Stochastic Integrals
Before we consider a stochastic integral, let
us brieÔ¨Çy recall how the standard deter-
ministic Riemann integral can be deÔ¨Åned.
Given a bounded function g ‚à∂[0, T] ‚Üí‚Ñù,

3.3 Stochastic Integrals
81
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
W1(t)
0
(a)
(b)
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
W2(t)
t
‚àí1
‚àí0.5
0
0.5
1
1.5
‚àí2
‚àí1
0
1
2
3
4
5
W1(t)
W2(t)
Figure 3.3
(a) Two discretized Brownian motions W1(t),
W2(t) constructed over [0, 5] with N = 5000, so Œît = 0.001.
(b) Brownian motion W1(t) plotted against W2(t). The
paths start at (0, 0) and the Ô¨Ånal point at t = 5 is marked
with a ‚ãÜ.
we can deÔ¨Åne the Riemann integral using
the Riemann sum
‚à´
T
0
g(t)dt = lim
Œît‚Üí0
N‚àí1
‚àë
j=0
g(zj)(tj+1 ‚àítj)
(3.10)
where 0 = t0 < t1 < ‚Ä¶ < tN is a partition of
the interval [0, T], Œît = max1‚â§j‚â§N |tj+1 ‚àítj|
and we take any zj ‚àà[tj, tj+1]. The key
elements are to say for which functions
the integral holds, deÔ¨Åne the partition,
and examine the limit as more points
are added to the partition (and so that
Œît ‚Üí0).
In the stochastic case, we will need to say
what type of functions we can integrate and
how we take the limit. It will also turn out
that the choice of zj will also become impor-
tant when we try and integrate a stochastic
process.
It is standard to illustrate the ideas of a
stochastic integral by considering
I = ‚à´
T
0
W(s)dW(s) ‚à∂= lim
N‚Üí‚àûSN,
SN ‚à∂=
N‚àí1
‚àë
j=0
W(zj)(W(tj+1) ‚àíW(tj)),
(3.11)
where zj = (1 ‚àíùõº)tj + ùõºtj+1 and the limit as
N ‚Üí‚àûreÔ¨Ånes the partition on [0, T]. The
idea of deÔ¨Åning I in (3.11) follows by anal-
ogy with the deterministic integral (3.10).
For the stochastic integrals, we examine
these limits in a mean square sense. Note
that for ùõº= 0, zj = tj (this will correspond
to the It√¥ integral) and for ùõº= 1‚àï2, zj =
(tj + tj+1)‚àï2 (which will correspond to a
Stratonovich integral).
Lemma 3.6 Let
0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN =
T be a partition of [0, T] where it is under-
stood that Œît ‚à∂= max1‚â§j‚â§N |tj+1 ‚àítj| ‚Üí0
as N ‚Üí‚àû. Then in mean square we have
I = lim
N‚Üí‚àûSN = (W(T))2
2
+
(
ùõº‚àí1
2
)
T.
Proof. Let ŒîWj ‚à∂= W(tj+1) ‚àíW(tj). Then
SN = 1
2
N‚àí1
‚àë
j=0
(W(tj+1)2 ‚àíW(tj)2)
‚àí1
2
N‚àí1
‚àë
j=0
(ŒîWj)2 +
N‚àí1
‚àë
j=0
(W(zj) ‚àíW(tj))2

82
3 Stochastic DiÔ¨Äerential Equations
+
N‚àí1
‚àë
j=0
(W(tj+1)‚àíW(zj))(W(zj)‚àíW(tj))
=‚à∂A + B + C + D.
Now the Ô¨Årst sum A is a telescoping sum
so that A = 1‚àï2(W(T))2. By Lemma 3.3, the
second sum B ‚ÜíT‚àï2 as N ‚Üí‚àûand a sim-
ilar lemma gives that C ‚ÜíùõºT. For D, using
the fact that the increments in the sum are
independent, it can be shown that D ‚Üí0 as
N ‚Üí‚àû.
Lemma 3.6 shows that for ‚à´t
0 W(s)dW(s),
the choice of ùõºfor zj is important.
‚Ä¢ When ùõº= 1‚àï2, an average is taken of W
over the time interval Œîtj = tj+1 ‚àítj and
gives rise to the Stratonovich integral,
which is denoted by ‚àò, and we have
‚à´
T
0
W(s) ‚àòdW(s) = 1
2(W(T))2.
(3.12)
This follows directly from Lemma 3.6
with ùõº= 1‚àï2. The result is similar to the
deterministic integral where
‚à´T
0 sds = s2‚àï2.
‚Ä¢ When ùõº= 0 on the interval, we have the
important case of the It√¥ integral. From
Lemma 3.6, with ùõº= 0, we Ô¨Ånd the It√¥
integral
‚à´
T
0
W(s)dW(s) = 1
2(W(T))2 ‚àíT
2 .
(3.13)
Compared to the deterministic integral,
we have an additional term. On the
interval Œîtj, we only use information at
tj and no information in the future, and
this gives the It√¥ integral some special
and useful properties. Because the It√¥
integral does not anticipate the future, it
is widely used in Ô¨Ånancial modeling.
We now discuss in detail the It√¥ version
of the stochastic integral.
3.3.1
It√¥ Integral
Now let us extend the deÔ¨Ånition of the
It√¥ integral (3.11) (where ùõº= 0) to a wider
class of functions. We deÔ¨Åne the It√¥ inte-
gral through the mean square limit as fol-
lows
‚à´
T
0
X(t)dW(t) ‚à∂= lim
N‚Üí‚àû
N‚àí1
‚àë
j=0
X(tj)
(W(tj+1) ‚àíW(tj)) ,
where 0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN = T is a parti-
tion as in Lemma 3.6. We need to say some-
thing about the sort of X that we can inte-
grate, as we have seen X may be a stochas-
tic process. Most importantly, for the It√¥
integral, we do not want X to see into
the future ‚Äì the ideas required to make
this precise are described in the following
paragraph.
We have the idea that the stochastic pro-
cess, X(t), is a model of something that
evolves in time and we can therefore ask at
time t questions about the past s < t (X(t)
is known from observations of the past)
and the future s > t. A probability space
(Œ©, Óà≤, P) consists of a sample space Œ©, a
set of events Óà≤, and a probability measure
P. A Ô¨Åltered probability space consists of
(Œ©, Óà≤, Óà≤t, P) where Óà≤t is a Ô¨Åltration of Óà≤.
The Ô¨Åltration Óà≤t is a way of denoting the
events that are observable by time t and so,
as t increases, Óà≤t contains more and more
events. If X(t), t ‚àà[0, T] is nonanticipating
(or Óà≤t adapted), then X(t) is Óà≤t measurable
for all t ‚àà[0, T] (roughly X(t) does not see
into the future). Finally, X(t) is predictable
if it is nonanticipating and can be approxi-
mated by a sequence X(sj) ‚ÜíX(s) if sj ‚Üís
for all s ‚àà[0, T], sj < s.
Theorem 3.3 (It√¥ integral) Let
X(t),
t ‚àà[0, T]
be
a
predictable
stochastic

3.3 Stochastic Integrals
83
process such that
(
E
[
‚à´
t
0
|X(s)|2ds
])1‚àï2
< ‚àû.
Then
1. ‚à´t
0 X(s) dW(s) for t ‚àà[0, T] has
continuous sample paths.
2. The It√¥ integral has the martingale
property . That is, best predictions of
‚à´t
0 X(s) dW(s) based on information up
to time r is ‚à´r
0 X(s) dW(s). In particular,
the integral has mean 0 :
E
[
‚à´
t
0
X(s) dW(s)
]
= 0.
3. We have the It√¥ isometry
E
[
||| ‚à´
t
0
X(s) dW(s)|||
2]
= ‚à´
t
0
E[|X(s)|2]
ds, t ‚àà[0, T]. (3.14)
Example 3.2
Reconsider the stochastic
integral
I(t) = ‚à´
t
0
W(s) dW(s),
t ‚â•0.
The martingale property gives E[I(t)] = 0
and the It√¥ isometry (3.14) gives
E[I(t)2] =‚à´
t
0
E[(W(s))2] ds =‚à´
t
0
s ds = 1
2t2.
(3.15)
Note that using (3.13), we can write I(t)
as I(t) = 1
2(W(t))2 ‚àít
2, or rearranged as
1
2(W(t))2 = 1
2t + I(t). This is often written
in shorthand as
d
(1
2(W(t)
)2
= 1
2 dt + W(t) dW(t). (3.16)
The It√¥ integral can be extended to be
vector-valued.
DeÔ¨Ånition 3.2 Let W
W
W(t) = (W1(t), W2(2),
‚Ä¶ , Wm(t))T ‚àà‚Ñùm and let X ‚àà‚Ñùd√óm be
such that each Xij is a predictable stochastic
process such that
(
E
[
‚à´
t
0
|Xij(s)|2ds
])1‚àï2
< ‚àû.
(3.17)
Then ‚à´t
0 X(s)dW(s) is the random variable
in ‚Ñùd with ith component
m
‚àë
j=1 ‚à´
t
0
Xij(s)dWj(s).
It can be shown that the following prop-
erties hold for the vector-valued It√¥ inte-
gral.
1. The integral ‚à´t
0 X(s) dW
W
W(s) for t ‚àà[0, T]
is a predictable process.
2. The martingale property holds and the
It√¥ integral has mean 0:
E
[
‚à´
t
0
X(s) dW
W
W(s)
]
= 0.
3. Given two stochastically integrable
processes X and Y, that is, X and Y are
predictable and (3.17) holds, then if
ÃÇt ‚à∂= min(t1, t2),
E
[‚ü®
‚à´
t1
0
X(s) dW
W
W(s), ‚à´
t2
0
Y(s) dW
W
W(s)
‚ü©]
= ‚à´
ÃÇt
0
m
‚àë
i=1
E
[‚ü®
XXXi(s),YYY i(s)
‚ü©]
ds,
where XXXi,YYY i denote the ith column of X
and Y and ‚ü®‚ãÖ, ‚ãÖ‚ü©is the ‚Ñùd inner product.
From this, the It√¥ isometry follows
E
[
‚Äñ ‚à´
t
0
X(s) dW
W
W(s)‚Äñ2
2
]
= ‚à´
t
0
E[‚ÄñX(s)‚Äñ2
F
] ds, t ‚àà[0, T].
(3.18)

84
3 Stochastic DiÔ¨Äerential Equations
In fact, the class of processes for which
the stochastic integral can be developed
is wider, see, for example, [5, 6]. With this
deÔ¨Ånition, we can examine systems of It√¥
SDEs.
3.4
ItÃÇo SDEs
We described in the introduction that (3.6)
is shorthand for the integral equation (3.5)
and in Section 3.2.1 we saw that Brownian
motion is not diÔ¨Äerentiable.
We
now
consider
the
It√¥
SDEs
where
uuu ‚àà‚Ñùd
and
we
are
given
a
process
with
drift
fff (uuu) ‚à∂‚Ñùd ‚Üí‚Ñùd,
diÔ¨Äusion
G(uuu) ‚à∂‚Ñùd ‚Üí‚Ñùd√óm,
and
W
W
W(t) = (W1(t), W2(2), ‚Ä¶ , Wm(t))T ‚àà‚Ñùm.
Instead of writing
duuu
dt = fff (uuu) + G(uuu)dW
W
W(t)
dt
,
we realize that W is not diÔ¨Äerentiable and
hence write
duuu = fff (uuu)dt + G(uuu)dW
W
W(t),
(3.19)
which we interpret as an It√¥ stochastic inte-
gral equation
uuu(t) = uuu(0) + ‚à´
t
0
fff (uuu(s))ds
+ ‚à´
t
0
G(uuu(s))dW
W
W(s).
(3.20)
The last integral in (3.20) is the stochastic
integral from DeÔ¨Ånition 3.2 and G(uuu) is a
d √ó m matrix.
Example 3.3 Consider the system of SDEs
with two independent Brownian motions
W1(t) and W2(t)
du1 = f1(u1, u2) dt + g11(u1, u2)dW1
+ g12(u1, u2)dW2
du2 = f2(u1, u2) dt + g21(u1, u2)dW1
+ g22(u1, u2)dW2.
(3.21)
We can write (3.21) in the form of (3.19)
with d = m = 2 by taking uuu = (u1, u2)T and
f ‚à∂‚Ñù2 ‚Üí‚Ñù2, G ‚à∂‚Ñù2 ‚Üí‚Ñù2√ó2 given by
fff (uuu) =
(f1(u1, u2)
f2(u1, u2)
)
,
G(uuu) =
(g11(u1, u2)
g12(u1, u2)
g21(u1, u2)
g22(u1, u2)
)
.
If the following conditions on the drift
fff and diÔ¨Äusion G are satisÔ¨Åed, it can be
shown that a solution exists to the SDE
(3.19).
Assumption 3.1 (linear growth/
Lipschitz condition) There exists a con-
stant L > 0 such that the linear growth con-
dition holds for uuu ‚àà‚Ñùd,
‚Äñ‚Äñfff (uuu)‚Äñ‚Äñ
2 ‚â§L(1 + ‚Äñuuu‚Äñ2),
‚ÄñG(uuu)‚Äñ2 ‚â§L(1 + ‚Äñuuu‚Äñ2),
uuu ‚àà‚Ñùd,
and the global Lipschitz condition holds for
uuu1,uuu2 ‚àà‚Ñùd,
‚Äñ‚Äñfff (uuu1) ‚àífff (uuu2)‚Äñ‚Äñ ‚â§L ‚Äñ‚Äñuuu1 ‚àíuuu2‚Äñ‚Äñ ,
‚Äñ‚ÄñG(uuu1) ‚àíG(uuu2)‚Äñ‚Äñ ‚â§L ‚Äñ‚Äñuuu1 ‚àíuuu2‚Äñ‚Äñ .
Theorem 3.4 (existence and uniqueness
for
SDEs)
Suppose
that
Assump-
tion 3.1
holds
and
that
W(t)
is
a
Brownian
motion.
For
each
T > 0
and uuu0 ‚àà‚Ñùd, there is a unique uuu with
supt‚àà[0,T]
(E[‚Äñuuu(t)‚Äñ2])1‚àï2 < ‚àû
such
that
for t ‚àà[0, T]
uuu(t)=uuu0 +‚à´
t
0
fff (uuu(s)) ds +‚à´
t
0
G(uuu(s)) dW
W
W(s).
(3.22)

3.4 ItÃÇo SDEs
85
The notion of solution that we have taken
is a stochastic process that is determined
for any Brownian path ‚Äì this is called a
strong solution. We could also ask for solu-
tions that are valid only for particular Brow-
nian paths, in which case the Brownian path
also becomes part of the solution. This is
the idea of a weak solution. We do not con-
sider this further. In Example 3.1, we gave a
simple example of an SDE for which f (u) =
‚àíùúÜu and G(u) = ùúé. As G is independent of
u, the noise is additive and (3.3) is equiva-
lent to the It√¥ integral equation
p(t) = p(0) ‚àíùúÜ‚à´
t
0
p(s)ds + ùúé‚à´
s
0
dW(s).
We now present two further SDEs. The Ô¨Årst
describes the dynamics of a particle subject
to noise. The second models the Ô¨Çuctuation
of an asset price on a stock market.
Example 3.4 [Langevin equation] Denote
by p the momentum, q the position, and
by H(q, p) the system energy of a particle.
If the particle in Example 3.1 has potential
energy V(q) at position q ‚àà‚Ñù, its dynam-
ics are described by the following SDE for
(q(t), p(t))T
dq = p dt
dp = ‚àíùúÜp dt ‚àíV ‚Ä≤(q) dt + ùúédW(t)
(3.23)
for parameters ùúÜ, ùúé> 0. Here d = 2, m = 1,
and for uuu = (q, p)T
fff (uuu) =
(
p
‚àíùúÜp ‚àíV ‚Ä≤(q)
)
,
G(uuu) =
(0
ùúé
)
.
The noise is again additive and acts directly
only on the momentum p.
In Figure 3.4, we plot a solution of (3.23)
for one sample path of the noise. The
potential V(q) = (q2 ‚àí1)2‚àï4 is a double-
well potential and we take ùúÜ= 0.1, ùúé= 0.2.
In the absence of noise, there are three
Ô¨Åxed points (0, 0)T (which is unstable)
and (¬±1, 0)T (which are stable). With the
stochastic forcing we see that the particle
no longer comes to rest and can cross
the energy barrier between (‚àí1, 0)T and
(1, 0)T.
The SDEs of Examples 3.1 and 3.4 both
have additive noise. The following example
has multiplicative noise.
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
p(t)
q(t)
0
20
40
60
80 100 120 140 160 180 200
‚àí1
0
1
q(t)
0
(a)
(b)
20
40
60
80 100 120 140 160 180 200
‚àí1
‚àí0.5
0
0.5
1
t
p(t)
Figure 3.4
Solution of Langevin equation (3.23) with
V(q) = (q2 ‚àí1)2‚àï4, ùúÜ= 0.1, ùúé= 0.2. In (a) we show p(t)
and q(t) for a sample path of the noise. In (b) we plot the
phase portrait.

86
3 Stochastic DiÔ¨Äerential Equations
Example 3.5 [geometric
Brownian
mo-
tion] In Ô¨Ånancial modeling, the price u(t) at
time t of a risk-free asset with interest rate
r obeys a diÔ¨Äerential equation du‚àïdt = ru.
On the stock market, stock prices Ô¨Çuctuate
rapidly and the Ô¨Çuctuations are modeled by
replacing the risk-free interest rate r by a
stochastic process r + ùúédW‚àïdt, where ùúéis
the volatility and dW‚àïdt is white noise. We
obtain the SDE with multiplicative noise
du = r u dt + ùúéu dW(t),
u(0) = u0,
(3.24)
where u0 ‚â•0 is the price at time t = 0.
Here d = m = 1, the drift f (u) = ru, and
the diÔ¨Äusion G(u) = ùúéu. The solution u(t)
depends on the interpretation of the inte-
gral ‚à´t
0 u(s) dW(s). The It√¥ SDE is chosen
to model stock prices, because the cur-
rent price u(t) of the stock is supposed to
be determined by past events and is inde-
pendent of the current Ô¨Çuctuations dW(t).
The solution process u(t) of (3.24) is called
geometric Brownian motion. In Figure 3.5a
we plot Ô¨Åve sample realizations of (3.24)
computed numerically with r = 0.5 and ùúé=
0.5 and all with initial data u(0) = 1. In
Figure 3.5a we also plot E[u(t)] found from
the exact solution, see Example 3.7.
Note that although we have described
(3.24) from a mathematical Ô¨Ånance point of
view, it could also be considered as a basic
population growth model where exponen-
tial population growth is modulated with
random Ô¨Çuctuations.
3.4.1
It√¥ Formula and Exact Solutions
The chain rule of ordinary calculus changes
in the It√¥ calculus to the so-called It√¥ for-
mula. We now discuss the It√¥ formula (also
called It√¥‚Äôs Lemma) for (3.19) with d = m =
1 (so, in one dimension). We apply the
It√¥ formula to geometric Brownian motion
(see Example 3.5) and the mean reverting
OU process of which Example 3.1 is one
case. Finally, we quote a general form of the
It√¥ formula.
Lemma 3.7 (one-dimensional It√¥
formula)
Let Œ¶ ‚à∂[0, T] √ó ‚Ñù‚Üí‚Ñùhave
continuous
partial
derivatives
ùúïŒ¶‚àïùúït,
ùúïŒ¶‚àïùúïu and ùúï2Œ¶‚àïùúïu2. Let u satisfy the It√¥
SDE (3.19) with d = m = 1, so
du = f (u)dt + G(u)dW(t),
u(0) = u0
and suppose Assumption 3.1 holds. Then,
almost surely,
dŒ¶ = ùúïŒ¶
ùúït dt + ùúïŒ¶
ùúïu du + 1
2
ùúï2Œ¶
ùúïu2 G2 dt
(3.25)
or written in full
Œ¶(t, u(t)) = Œ¶(0, u0) + ‚à´
t
0
ùúïŒ¶
ùúït (s, u(s))
+ùúïŒ¶
ùúïu (s, u(s))f (u(s))
+1
2
ùúï2Œ¶
ùúïu2 (s, u(s))G(u(s))2 ds
+ ‚à´
t
0
ùúïŒ¶
ùúïu (s, u(s))G(u(s)) dW(s).
To interpret Lemma 3.7, consider the
solution u(t) of the deterministic ODE
du‚àïdt = ùúÜ
and
let
ùúô(u) = 1‚àï2u2.
The
standard chain rule implies that
dùúô(u)
dt
= u du
dt = ùúÜu(t).
However,
if
u(t)
satisÔ¨Åes
du = ùúÜdt +
ùúédW(t), the It√¥ formula (3.25) says that
dùúô(u) = u du + ùúé2
2 dt
(3.26)
and we pick up an unexpected extra term
ùúé2‚àï2dt. This shows that the It√¥ calculus is
diÔ¨Äerent from the standard deterministic

3.4 ItÃÇo SDEs
87
0
(a)
(b)
1
2
3
4
5
6
7
8
9
10
20
40
60
80
100
120
t
u(t)
0
1
2
3
4
5
6
7
8
9
10
0
100
200
300
400
0
1
2
3
4
5
6
7
8
9
10
‚àí5
0
5
10
15
t
Error
Figure 3.5
(a) Five numerical solutions of the
SDE (3.24) for geometric Brownian motion on
the interval [0, 10] with r = 0.5 and ùúé= 0.5.
The initial data is the same in each case, u(0) =
1. With each realization of the noise we obtain
a diÔ¨Äerent solution path. The solid smooth line
shows E[u(t)] from (3.28). In (b) (top) we plot
the exact solution and approximate numerical
solution + together on the interval [0, 10]. In
(b) (bottom) we plot the error.
calculus. We can expand (3.26) using the
SDE to get
d
(1
2u2)
= u (ùúÜudt + ùúédW) + ùúé2
2 dt
=
(
ùúÜu2 + ùúé2
2
)
dt + ùúéudW. (3.27)
When ùúÜ= 0 and ùúé= 1, the SDE is simply
du = dW and u(t) = W(t). In which case,
(3.27) becomes
d
(1
2W 2)
= 1
2dt + WdW,
and we have recovered (3.16) using the It√¥
formula.
Example 3.6 [Integration by parts] Recon-
sider the simple SDE du = dW and let
ùúô(t, u) = tu. Then by the It√¥ formula,
dùúô= udt + tdu
and
hence
d(tW) =
Wdt + tdW. Written in integral form,
we have tW(t) = ‚à´t
0 W(s)ds + ‚à´t
0 sdW(s) or
‚à´
t
0
sdW(s) = tW(t) ‚àí‚à´
t
0
W(s)ds,
which is a formula for integration by parts.
Remark 3.3 The
integral
Y(T) =
‚à´T
0 W(s)ds is the area under the path
W and because W is stochastic so is the
value of the integral. In fact, for Ô¨Åxed T,
Y ‚àºN(0, T3‚àï3).
This
follows
from
the
deÔ¨Ånition of the Riemann integral. For a
partition of [0, T] with T = NŒît we have
Y = lim
N‚Üí‚àû
N
‚àë
n=1
ŒîtW(tn)
= lim
Œît‚Üí0
[NW(t1) + (N ‚àí1)(W(t2) ‚àíW(t1))
+ ¬∑ ¬∑ ¬∑ + (W(tN) ‚àíW(tN‚àí1))] .
On the right, we have a sum of indepen-
dent normal random variables and clearly
E[Y] = 0. For the variance, we have
Var (Y) = lim
Œît‚Üí0
[Œît2 Var (NW(t1)) + ¬∑ ¬∑ ¬∑
+ Œît2 Var (W(tN) ‚àíW(tN‚àí1))]
= lim
Œît‚Üí0
[Œît3(N2 + ¬∑ ¬∑ ¬∑ + 1)]
= lim
Œît‚Üí0
[
Œît3 1
6N(N + 1)(2N + 1)
]
= T3
3 .

88
3 Stochastic DiÔ¨Äerential Equations
We now look at two applications of the It√¥
formula to solve for geometric Brownian
motion (Example 3.5) and the OU process
(Example 3.1).
Example 3.7 [geometric Brownian mot-
ion] We show the solution of the geometric
Brownian motion SDE (3.24) is given by
u(t) = exp
((
r ‚àíùúé2
2
)
t + ùúéW(t)
)
u0. (3.28)
For u(0) = 0, u(t) = 0 is clearly the solution
to (3.24). For u > 0, let ùúô(u) = log u, so that
ùúô‚Ä≤(u) = u‚àí1 and ùúô‚Ä≤‚Ä≤(u) = ‚àíu‚àí2. By the It√¥
formula with Œ¶(t, u) = ùúô(u),
dùúô(u) = r dt + ùúédW(t) ‚àí1
2ùúé2 dt.
Hence,
ùúô(u(t)) = ùúô(u0) + ‚à´
t
0
(
r ‚àíùúé2
2
)
ds
+ ‚à´
t
0
ùúédW(s)
and
log u(t) = log(u0) + (r ‚àí(ùúé2‚àï2))t +
ùúéW(t). Taking the exponential, we Ô¨Ånd
(3.28). It is clear that, when u0 ‚â•0, the
solution u(t) ‚â•0 for all t ‚â•0. This is desir-
able in Ô¨Ånancial modeling, where stock
prices are nonnegative and also for mod-
els of populations where populations are
positive.
From
(3.28),
we
see
that
because
E[W(t)] = 0, we have E[u(t)] = exp((r ‚àí
ùúé2‚àï2)t)u0. This is plotted in Figure 3.5a
along with Ô¨Åve sample paths of the SDE
(3.24).
Example 3.8 [mean reverting OU process]
We consider the following generalization of
(3.3)
du = ùúÜ(ùúá‚àíu)dt + ùúédW(t),
u(0) = u0,
for ùúÜ, ùúá, ùúé‚àà‚Ñù. We solve this SDE by using
the It√¥ formula (3.25) with Œ¶(t, u) = eùúÜt u.
Then,
dŒ¶(t, u) = ùúÜeùúÜtu dt
+ eùúÜt(ùúÜ(ùúá‚àíu)dt + ùúédW(t)) + 0
and
Œ¶(t, u(t)) ‚àíŒ¶(0, u0) = eùúÜtu(t) ‚àíu0
= ùúÜùúá‚à´
t
0
eùúÜs ds + ùúé‚à´
t
0
eùúÜs dW(s).
After evaluating the deterministic integral,
we Ô¨Ånd
u(t)=e‚àíùúÜtu0+ùúá(
1 ‚àíe‚àíùúÜt)
+ùúé‚à´
t
0
eùúÜ(s‚àít)dW(s)
(3.29)
and this is known as the variation of
constants solution. Notice that u(t) is a
Gaussian process and can be speciÔ¨Åed
by its mean ùúá(t) = E[u(t)] and covari-
ance
c(s, t) = Cov (u(s), u(t)).
Using
the
mean zero property of the It√¥ integral
(Theorem 3.3), the mean is
ùúá(t) = E[u(t)] = e‚àíùúÜtu(0) + ùúá(1 ‚àíe‚àíùúÜt)
(3.30)
so that ùúá(t) ‚Üíùúáas t ‚Üí‚àûand the process
is ‚Äúmean reverting.‚Äù For the covariance, Ô¨Årst
note that
c(s, t) = Cov (u(t), u(s))
= E[(u(s) ‚àíE[u(s)]
) (u(t) ‚àíE[u(t)]
)]
= E
[
‚à´
s
0
ùúéeùúÜ(r‚àís)dW(r) ‚à´
t
0
ùúéeùúÜ(r‚àít)dW(r)
]
= ùúé2e‚àíùúÜ(s+t)E
[
‚à´
s
0
eùúÜrdW(r) ‚à´
t
0
eùúÜrdW(r)
]
.
Then, using the It√¥ isometry, property (3)
from DeÔ¨Ånition 3.2,
c(s, t) = ùúé2
2ùúÜe‚àíùúÜ(s+t)(e2ùúÜmin(s,t) ‚àí1).

3.4 ItÃÇo SDEs
89
In particular, the variance Var (u(t)) =
ùúé2(1 ‚àíe‚àí2ùúÜt)‚àï2ùúÜso that Var (u(t)) ‚Üíùúé2‚àï2ùúÜ
and u(t) ‚ÜíN(ùúá, ùúé2‚àï2ùúÜ) in the distribution
as t ‚Üí‚àû.
Reconsider Example 3.1, which is the
mean reverting OU process with ùúá= 0.
Example 3.9 For
dp = ‚àíùúÜpdt + ùúédW(t)
the variation of constants (3.29) gives
p(t) = e‚àíùúÜtp0 + ùúé‚à´
t
0
e‚àíùúÜ(t‚àís) dW(s).
We can use this to derive an important
relationship in statistical physics. By def-
inition, the expected kinetic energy per
degree of freedom of a system at temper-
ature T is given by kBT‚àï2, where kB is the
Boltzmann constant. The temperature of
a system of particles in thermal equilib-
rium can be determined from ùúÜand ùúé. For
the OU model, the expected kinetic energy
E[p(t)2‚àï2] is easily calculated
E[p(t)2]
= e‚àí2ùúÜtp2
0 + ùúé2
‚à´
t
0
e‚àí2ùúÜ(t‚àís) ds
= e‚àí2ùúÜtp2
0 + ùúé2
2ùúÜ(1 ‚àíe‚àí2ùúÜt)
‚Üíùúé2
2ùúÜ
as t ‚Üí‚àû.
We see that the expected kinetic energy
E[1‚àï2p(t)2] converges to ùúé2‚àï4ùúÜas t ‚Üí‚àû.
Thus, ùúé2‚àï4ùúÜis the equilibrium kinetic
energy and the equilibrium temperature is
given by kBT = ùúé2‚àï2ùúÜ, which is known as
the Ô¨Çuctuation-dissipation relation.
We saw in Example 3.8 that p(t) ‚Üí
N(0, ùúé2‚àï2ùúÜ)
in
distribution.
This
is
illustrated
by
numerical
simulations
in
Figure 3.1.
This
limiting
distribu-
tion is known as the Gibbs canonical
distribution,
often
written
N(0, kBT)
with
probability
density
function
p(q, p) = e‚àíùõΩH(q,p)‚àïZ,
where
ùõΩ= 1‚àïkBT
is the inverse temperature, Z is a normal-
ization constant, and H(q, p) is the system
energy for a particle with position q and
momentum p. In this case, H = p2‚àï2.
We now state a more general form of It√¥‚Äôs
formula.
Lemma 3.8 (It√¥ formula) If Œ¶(t,uuu0) is a
continuously diÔ¨Äerentiable function of t ‚àà
[0, T] and twice continuously diÔ¨Äerentiable
functions of uuu0 ‚àà‚Ñùd and uuu(t) denotes the
solution of
duuu = fff (uuu)dt + G(uuu)dW
W
W(t)
under Assumption 3.1, then
Œ¶(t,uuu(t))
= Œ¶(0,uuu0) + ‚à´
t
0
(
ùúï
ùúït + Óà∏
)
Œ¶(s,uuu(s)) ds
+
m
‚àë
k=1 ‚à´
t
0
Óà∏kŒ¶(s,uuu(s)) dWk(s),
(3.31)
where
Óà∏Œ¶(t,uuu) ‚à∂= fff (uuu)T‚àáŒ¶(t,uuu)
+1
2
m
‚àë
k=1
gggk(uuu)T‚àá2Œ¶ (t,uuu)gggk(uuu),
Óà∏kŒ¶(t,uuu) ‚à∂= ‚àáŒ¶(t,uuu)Tgggk(uuu),
(3.32)
for uuu ‚àà‚Ñùd and t > 0. Here gggk denotes the
kth column of the diÔ¨Äusion matrix G. ‚àáŒ¶
is the gradient and ‚àá2Œ¶ the Hessian matrix
of second partial derivatives of Œ¶(t,xxx) with
respect to xxx.
It√¥‚Äôs formula can be generalized fur-
ther, for example, see [6]. With the It√¥
formula (3.31) it is possible to generalize
the variation of constants formula (3.29).
Consider the semilinear It√¥ SDE
duuu =
[
‚àíAuuu + fff (uuu)
]
dt + G(uuu) dW
W
W(t),
uuu(0) = uuu0 ‚àà‚Ñùd

90
3 Stochastic DiÔ¨Äerential Equations
Then,
by
the
same
techniques
as
in
Example 3.8 we have
uuu(t) = e‚àítAuuu0 + ‚à´
t
0
e‚àí(t‚àís)Afff (uuu(s)) ds
+ ‚à´
t
0
e‚àí(t‚àís)AG(uuu(s)) dW
W
W(s).
In Section 3.7 the It√¥ formula is used to
obtain deterministic equations from the
SDEs.
3.5
Stratonovich Integral and SDEs
Let
W(t)
be
the
Brownian
motion
and
let
g ‚à∂[0, T] ‚Üí‚Ñù
be
such
that
(E[‚à´t
0 |g(s)|2ds])1‚àï2 < ‚àû. Then the one-
dimensional
Stratonovich
integral
is
deÔ¨Åned through the mean square limit as
follows
‚à´
T
0
g(t)dW(t) ‚à∂= lim
N‚Üí‚àû
N‚àí1
‚àë
j=0
g
((tj + tj+1)
2
)
(
W(tj+1) ‚àíW(tj)) , (3.33)
where
0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN = T
is
a
partition as in Lemma 3.6. The multidi-
mensional Stratonovich integral is deÔ¨Åned
analogously to DeÔ¨Ånition 3.2. We saw that
the It√¥ integral has mean 0; however, this is
not the case for the Stratonovich one. We
can illustrate this for the stochastic integral
I = ‚à´T
0 W(s) ‚àòdW(s).
Lemma 3.6
shows
that I = 1
2(W(T))2 and
E[I] = 1
2E[(W(T))2] = T
2 .
We
can
transform
between
the
Stratonovich and It√¥ integrals by ensur-
ing the mean zero property. This is often
called a drift correction. Provided g is
diÔ¨Äerentiable, we have
‚à´
t
0
g(W(s)) ‚àòdW(s)
= ‚à´
t
0
g(W(s)) dW(s)
+ 1
2 ‚à´
t
0
g‚Ä≤(W(s)) ds.
(3.34)
Example 3.10 Consider the case where
g(W) = W. Then from (3.34), we have
using (3.13)
‚à´
t
0
W(s) ‚àòdW(s) = ‚à´
t
0
W(s) dW(s)+ 1
2 ‚à´
t
0
ds
= (W(T))2
2
,
and we have recovered (3.12) exactly.
We write the Stratonovich SDE with drift
f and diÔ¨Äusion g as
dv = f (v) dt + g(v) ‚àòdW(t),
v(0) = v0,
(3.35)
which we interpret as the integral equation
v(t) = v0 + ‚à´
t
0
f (v(s)) ds + ‚à´
t
0
g(v(s)) ‚àòW(s)
(3.36)
and the stochastic integral is a Stratonovich
integral.
We can replace the Stratonovich integral
using the transformation to Ô¨Ånd that v(t)
satisÔ¨Åes the It√¥ SDE
v(t) = v0 + ‚à´
t
0
[
f (v(s)) + 1
2
dg(v(s))
dv
g(v(s))
]
ds
+ ‚à´
t
0
g(v(s))dW(s).
Example 3.11 Consider the SDE
du = rudt + ùúéu ‚àòdW,
given
u(0) = u0.
(3.37)
This looks like the geometric Brownian
motion SDE, only here the stochastic forc-
ing is interpreted in the Stratonovich sense.

3.5 Stratonovich Integral and SDEs
91
0
(a)
(b)
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
1
2
3
4
5
6
t
u(t)
Stratonovich
Ito
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
1
2
3
4
5
6
t
u(t)
Figure 3.6
In both (a) and (b) the sample path for the
Brownian motion is Ô¨Åxed. In (a), we compare the It√¥ SDE
(3.24) and Stratonovich SDEs (3.37), computed using the
Heun method. In (b), we plot the solution of the equivalent
It√¥ SDE, (3.38) to (3.37) and see that these agree.
In Figure 3.6a, we Ô¨Åx a sample path for the
Brownian motion and plot a sample path
of (3.37) and compare it to a sample path
of (3.24). It is clear the solutions of the two
SDEs do not agree. Equation (3.37) is equiv-
alent to the following It√¥ SDE
du =
(
ru + 1
2ùúé2u
)
dt + ùúéudW,
given
u(0) = u0.
(3.38)
In Figure 3.6b, we plot the solution of this
SDE using the same Brownian motion
as in Figure 3.6a. We observe that this
now agrees with the Stratonovich path in
Figure 3.6b.
Example 3.12
Now let us start with an
It√¥ SDE and convert to an equivalent
Stratonovich form. Let us consider the geo-
metric Brownian motion of (3.24), this can
be rewritten as the equivalent Stratonovich
SDE
du =
(
ru ‚àí1
2ùúé2u
)
dt + ùúéu ‚àòdW,
given
u(0) = u0.
For a system of Stratonovich SDEs
in dimension d > 1 with forcing by m
Brownian motions, we have
dvvv = fff (vvv) dt + G(vvv) ‚àòdW
W
W(t),
vvv(0) = vvv0,
(3.39)
for f ‚à∂‚Ñùd ‚Üí‚Ñùd, G‚à∂‚Ñùd ‚Üí‚Ñùd√óm, and
W
W
W(t) = (W1(t), ‚Ä¶ , Wm(t))T
for
inde-
pendent Brownian motions Wi(t). Then,
vvv(t) = (v1(t), ‚Ä¶ , vd(t))T
is
a
solution
to
(3.39) if the components vi(t) for
i = 1, ‚Ä¶ , d satisfy
vi(t) = v0,i + ‚à´
t
0
fi(vvv(s)) ds
+
m
‚àë
j=1 ‚à´
t
0
gij(vvv(s)) ‚àòdWj(s),
t > 0,
where the last term is the Stratonovich inte-
gral deÔ¨Åned as in (3.33).
Provided G is diÔ¨Äerentiable, we are
able to convert between the It√¥ and
Stratonovich interpretations of the SDEs.
We write (3.39) as an It√¥ SDE. DeÔ¨Åne
ÃÉf (vvv) = (ÃÉf1(vvv), ‚Ä¶ , ÃÉfd(vvv))T, where

92
3 Stochastic DiÔ¨Äerential Equations
ÃÉfi(vvv) = fi(vvv) + 1
2
d
‚àë
k=1
m
‚àë
j=1
gkj(vvv)
ùúïgij(vvv)
ùúïvk
,
i = 1, ‚Ä¶ , d,
vvv ‚àà‚Ñùd.
(3.40)
Then the solution vvv(t) of (3.39) satisÔ¨Åes the
equivalent It√¥ SDE
dvvv = ÃÉfff (vvv) dt + G(vvv) dW
W
W(t),
vvv(0) = vvv0.
Note that it is clear that for additive
noise the It√¥ and Stratonovich interpre-
tations coincide as ùúïgij‚àïùúïvk = 0 for all
i, j. Finally, we quote the corresponding
Stratonovich version of the It√¥ formula
of Lemma 3.8. This shows that the chain
rule for Stratonovich calculus resembles
the deterministic version of the chain
rule.
Lemma 3.9 (Stratonovich formula)
Suppose that Œ¶ is as in Lemma 3.8 and vvv(t)
is the solution of (3.39). Then
Œ¶(t,vvv(t))
= Œ¶(0,vvv0) +‚à´
t
0
(
ùúï
ùúït + Óà∏strat
)
Œ¶(s,vvv(s)) ds
+
m
‚àë
k=1 ‚à´
t
0
Óà∏kŒ¶(s,vvv(s)) ‚àòdWk(s), (3.41)
where Óà∏stratŒ¶ = fff T‚àáŒ¶ and Óà∏kŒ¶ ‚à∂= ‚àáŒ¶Tgggk
is deÔ¨Åned by (3.32).
Remark 3.4 One
advantage
of
the
Stratonovich integral is that it follows
the standard rules of calculus. The It√¥
integral is better suited to modeling Ô¨Ånan-
cial applications and for analysis. Which
form of integral is appropriate is essen-
tially a modeling question. However, the
integrals are the same if the integrand is
not stochastic and we can convert between
them provided the diÔ¨Äusion term G is
diÔ¨Äerentiable.
3.6
SDEs and Numerical Methods
We construct approximations uuun to the
solution uuu(tn) of a system of SDEs where
tn = nŒît and we are given initial data
uuu(0) = uuu0. As we have seen the solution
depends on the sample path of the noise
that is taken and so convergence of numeri-
cal methods for SDEs has to be approached
with care.
If we are worried about approximating
the sample path of the solution uuu(t) for a
given sample path of the noise, then we
need a strong approximation and we are
interested in strong convergence, that is we
look at the root mean square error
sup
0‚â§tn‚â§T
(E[‚Äñuuu(tn) ‚àíuuun‚Äñ])1‚àï2 .
Using the Borel‚ÄìCantelli lemma, it is pos-
sible to obtain from the strong conver-
gence, a pathwise error sup0‚â§tn‚â§T ‚Äñuuu(tn) ‚àí
uuun‚Äñ, example, see [7‚Äì9].
In contrast, in many situations it is aver-
age quantities that are of interest and not
the individual sample paths. For example,
we may be actually interested in E[ùúô(uuu(T))]
for
some
given
function
ùúô‚à∂‚Ñùd ‚Üí‚Ñù,
where uuu(T) is the solution of an SDE at
time T. This leads to the notion of weak
convergence where we examine
sup
0‚â§tn‚â§T
|E[ùúô(uuu(tn))] ‚àíE[ùúô(uuun))]|.
We note that if strong convergence can
be established, then weak convergence fol-
lows, for example, see [10].
In practice, to approximate E[ùúô(uuu(T))],
we need a combination of Monte Carlo and
numerical methods for SDEs. We need to
generate M independent samplesuuuj
N for j =
1, ‚Ä¶ , M of an approximationuuuN touuu(T) for
tN = T and approximate E[ùúô(uuu(T))] by the

3.6 SDEs and Numerical Methods
93
sample average
ùúáM ‚à∂= 1
M
M
‚àë
j=1
ùúô(uuuj
N).
(3.42)
It can be shown that the Monte Carlo error
from only taking M samples satisÔ¨Åes
P
‚éõ
‚éú
‚éú
‚éú‚éù
|||E[ùúô(uuuN)] ‚àíùúáM||| <
2
‚àö
Var (ùúô(uuuN))
‚àö
M
‚éû
‚éü
‚éü
‚éü‚é†
> 0.95 + O(M‚àí1‚àï2),
(see Chapter 2). The total error made in
approximating E[ùúô(uuu(tn))] can then be
divided as the sum of the weak discretiza-
tion error because of approximating uuu(T)
by uuuN and the Monte Carlo error because
of taking M samples,
E[ùúô(uuu(T))] ‚àíùúáM
=
[
E[ùúô(uuu(T))] ‚àíE[ùúô(uuuN)]]
‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü
weak discretization error
+
[
E[ùúô(uuuN)] ‚àíùúáM
]
.
‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü
Monte Carlo error
(3.43)
3.6.1
Numerical Approximation of It√¥ SDEs
Consider the It√¥ SDE (3.19) that we can
write as the integral equation (3.20). We
now want to approximate the solution of
the SDE over a Ô¨Åxed time interval [0, T]. For
convenience, we take a Ô¨Åxed step Œît, so that
NŒît = T. The simplest of such methods
is the Euler‚ÄìMaruyama method and we
now outline its derivation. We have from
(3.20),
uuu(tn+1) = uuu0 + ‚à´
tn+1
0
fff (uuu(s)) ds
+ ‚à´
tn+1
0
G(uuu(s)) dW
W
W(s),
and
uuu(tn) = uuu0 + ‚à´
tn
0
fff (uuu(s)) ds
+ ‚à´
tn
0
G(uuu(s)) dW
W
W(s).
Subtracting, we obtain
uuu(tn+1) = uuu(tn) + ‚à´
tn+1
tn
fff (uuu(s)) ds
+ ‚à´
tn+1
tn
G(uuu(s)) dW
W
W(s).
(3.44)
We approximate both the integrands fff
and G by a constant over [tn, tn+1) on
the basis of the value at tn to obtain the
Euler‚ÄìMaruyama method
uuun+1 = uuun + fff (uuun) Œît + G(uuun)ŒîW
W
W n,
ŒîW
W
W n ‚à∂= W
W
W(tn+1) ‚àíW
W
W(tn).
(3.45)
As on each subinterval, G is evaluated at
the left-hand end point, the approxima-
tion is consistent with our deÔ¨Ånition of the
It√¥ integral. The noise increments ŒîW
W
W n ‚à∂=
W
W
W(tn+1) ‚àíW
W
W(tn) have mean 0 and variance
Œît. In implementation, we can usually get
standard normal random variables, that is,
ùúÅn ‚àºN(0, 1); thus, to form the increments
ŒîW
W
W n, we need to use these. To do this, the
Euler‚ÄìMaruyama scheme is rewritten as
uuun+1 = uuun + fff (uuun) Œît +
‚àö
ŒîtG(uuun)ùúÅùúÅùúÅn,
where ùúÅùúÅùúÅn ‚àºN(0, I) (see Algorithm 3.2).
This scheme only gives values at the dis-
crete points tn = nŒît. If values are required
at intermediate points, linear interpolation
can be used [5].
Example 3.13
Apply
the
Euler‚Äì
Maruyama
scheme
to
the
geometric
Brownian motion SDE (3.24), to Ô¨Ånd

94
3 Stochastic DiÔ¨Äerential Equations
Algorithm
3.2
Euler‚ÄìMaruyama
to
approximate
solution
of
SDE
duuu = fff (uuu)dt + G(uuu)dW
W
W.
INPUT : Initial data u0, Final time T, N num-
ber of steps to take.
OUTPUT: Vector t of time and vector u such
that un ‚âàu(tn).
1: Œît = T‚àïN, t0 = 0
2: for n = 1 ‚à∂N do
3:
tn = nŒît
4:
Ô¨Ånd z ‚àºN(0, 1)
5:
dWn =
‚àö
Œîtz
6:
un = un‚àí1 + Œîtf (un‚àí1) +
G(un‚àí1)dWn
7: end for
un+1 ‚âàu(tn+1) given u0 = u(0), from
un+1 = un + runŒît + ùúéunŒîWn,
ŒîWn ‚à∂= W(tn+1) ‚àíW(tn).
Figure 3.5b top shows a numerical solu-
tion and an explicit solution given from
(3.28), and in Figure 3.5b (bottom), we plot
the error u(tn) ‚àíun. We took r = 0.5, ùúé=
0.5, and Œît = 0.01 and we see a reason-
able agreement between the numerical and
exact solution.
For
deterministic
ODEs
numerical
methods, we often need to impose a
timestep restriction to get the correct
long-time
dynamics,
for
example,
if
u(t) ‚Üí0 and t ‚Üí‚àû, we ask that un ‚Üí0
as n ‚Üí‚àû. In the context of SDEs, such
stability questions are treated in a mean
square sense, that is if E[‚Äñuuu(t)‚Äñ2] ‚Üí0 as
t ‚Üí‚àû, we ask E[‚Äñuuun‚Äñ2] ‚Üí0 as n ‚Üí‚àû.
To counter stability constraints on the
step size Œît, the drift term can be approx-
imated at fff (uuun+1). This gives an implicit
method
uuun+1 = uuun + fff (uuun+1) Œît +
‚àö
ŒîtG(uuun)ùúÅùúÅùúÅn.
Note
that
the
noise
term
is
still
approximated at the left-hand end point
and is consistent with the deÔ¨Ånition of
the It√¥ integral. It can be shown that for
stability of the Euler‚ÄìMaruyama approx-
imation
in
Example 3.13,
we
require
0 < Œît < ‚àí2(r + ùúé2‚àï2)‚àïr2.
For
stabil-
ity of numerical methods for SDEs, see
[11‚Äì14].
Theorem 3.5 (strong convergence) Let
uuu(t) be the solution of (3.19) and uuun be
the Euler‚ÄìMaruyama approximation to
uuu(tn). Then the Euler‚ÄìMaruyama method
converges strongly with order Œît1‚àï2, that is,
there is a C > 0, independent of Œît such
that
sup
0‚â§tn‚â§T
(E[‚Äñuuu(tn) ‚àíuuun‚Äñ2])1‚àï2 ‚â§CŒît1‚àï2.
If G(uuu) is independent of uuu, then the rate of
convergence is improved and
sup
0‚â§tn‚â§T
(E[‚Äñuuu(tn) ‚àíuuun‚Äñ2])1‚àï2 ‚â§CŒît.
The Œît1‚àï2 rate of strong convergence
for the general case is very slow, and
much slower than that for additive noise.
Higher-order numerical schemes can be
constructed, and the most popular of these
is the Milstein scheme, which is convergent
with order Œît (see Figure 3.7a). In the case
of additive noise, the Euler‚ÄìMaruyama
and Milstein are equivalent (see (3.46))
and hence we have the improved rate
of convergence in Theorem 3.5 in this
case.
We examine the Milstein method for the
SDE (3.19) with d = m = 1 only. In (3.44),
instead of approximating G at the left-hand
point we now use a Taylor expansion
G(u(t)) = G(u(tn) + u(t) ‚àíu(tn)) ‚âàG(u(tn))
+ G‚Ä≤(u(tn))(u(t) ‚àíu(tn)).

3.6 SDEs and Numerical Methods
95
From
the
Euler‚ÄìMaruyama
approx-
imation, neglecting
the drift,
we can
approximate u(t) ‚àíu(tn) for t ‚àà[tn, tn+1]
by G(u(tn))(W(t) ‚àíW(tn)). Thus,
‚à´
tn+1
tn
G(u(s)) dW(s)
‚âà‚à´
tn+1
tn
G(u(tn))dW(s)
+ G‚Ä≤(u(tn))G(u(tn))(W(s)
‚àíW(tn)) dW(s).
Now using the integral (3.16), we Ô¨Ånd
‚à´
tn+1
tn
G(u(s)) dW(s)
‚âàG(u(tn)) (W(tn+1) ‚àíW(tn))
+1
2G‚Ä≤(u(tn))G(u(tn))
[(W(tn+1) ‚àíW(tn))2 ‚àíŒît
]
.
(3.46)
Combined with (3.44) and approximating f
as before, (3.46) gives the Milstein method
for SDEs in ‚Ñù
un = un‚àí1 + Œîtf (un‚àí1) + G(un‚àí1)dWn
+ 1
2G‚Ä≤(un‚àí1)G(un‚àí1)(ŒîW 2
n ‚àíŒît),
(see Algorithm 3.3). The scheme can be
extended to general SDEs in ‚Ñùd with
noise in ‚Ñùm. In general, however, such
higher-order methods require the non-
trivial approximation of iterated stochastic
integrals (see [5]). Numerically, we observe
in Figure 3.7 a strong rate of convergence
of order Œît for the Milstein method.
The
following
theorem
states
the
rate
of
weak
convergence
for
the
Euler‚ÄìMaruyama and Milstein methods.
Theorem 3.6 (weak convergence)
Suppose that fff and G are C‚àûfunctions
and all derivatives of fff and G are bounded.
Suppose
ùúô‚à∂‚Ñùd ‚Üí‚Ñù
is
Lipschitz.
Let
Algorithm
3.3
Milstein
method
to
approximate
solution
of
SDE
du = f (u)dt + G(u)dW.
INPUT : Initial data u0, Final time T, N num-
ber of steps to take.
OUTPUT: Vector t of time and vector u such
that un ‚âàu(tn).
1: Œît = T‚àïN, t0 = 0
2: for n = 1 ‚à∂N do
3:
tn = nŒît
4:
Ô¨Ånd z ‚àºN(0, 1)
5:
dWn =
‚àö
Œîtz
6:
un = un‚àí1 + Œîtf (un‚àí1) +
G(un‚àí1)dWn +
1
2G‚Ä≤(un‚àí1)G(un‚àí1)(dW 2
n ‚àíŒît)
7: end for
uuu(t) be the solution of (3.19) and uuun be
either the Euler‚ÄìMaruyama or Milstein
approximation to uuu(tn). Then there exists
C > 0,independent of Œît, such that
|||E[ùúô(uuu(T))] ‚àíE
[
ùúô(uuuN)]||| ‚â§CŒît.
In Figure 3.7, we observe from numerical
simulations, the weak convergence rate of
O(Œît) for the scheme applied to geometric
Brownian motion (3.24) with ùúô(u) = u and
T = 1.
3.6.2
Numerical Approximation of Stratonovich
SDEs
The numerical solution of the Stratonovich
SDE (3.35) can be achieved by trans-
forming the equation to an It√¥ SDE
with a modiÔ¨Åed drift ÃÉfff and applying, for
example, the Euler‚ÄìMaruyama method.
It is also possible to approximate solu-
tions of Stratonovich SDEs directly. Heun‚Äôs
method is the natural equivalent of the
Euler‚ÄìMaruyama method to Stratonovich
SDEs. Let us consider the Stratonovich SDE
(3.39) and approximate this over a Ô¨Åxed

96
3 Stochastic DiÔ¨Äerential Equations
10‚àí4
10‚àí3
10‚àí5
(a)
(b)
10‚àí4
10‚àí3
10‚àí2
Œît
Strong error
10‚àí4
10‚àí3
10‚àí2
10‚àí4
10‚àí3
10‚àí2
Œît
Weak error
Figure 3.7
(a) a loglog plot illustrating strong
convergence of the Euler‚ÄìMaruyama method
for (3.24) marked by o and for the Milstein
scheme, marked by x. The Euler‚ÄìMaruyama
method converges with order Œît1‚àï2, whereas
the Milstein scheme is of order Œît. We also
plot the solution of (3.3) with additive noise
marked by + solved with the Euler‚ÄìMaruyama
method. Other lines on the plot are reference
lines with slopes 0.5 and 1. We took 1000
realizations to estimate the expectations. In
(b), we illustrate weak convergence of the
Euler‚ÄìMaruyama method for (3.24) and esti-
mate E[u(1)]. We see an improvement in the
rate of convergence over the strong conver-
gence in (a) for (3.24). We took 10 000 realiza-
tions to estimate the expectations.
time interval [0, T] with Ô¨Åxed step Œît, so
that NŒît = T. We outline the derivation,
uuu(tn+1) = uuu0 + ‚à´
tn+1
0
fff (uuu(s)) ds
+ ‚à´
tn+1
0
G(uuu(s)) ‚àòdW
W
W(s),
and
uuu(tn) = uuu0 + ‚à´
tn
0
fff (uuu(s)) ds
+ ‚à´
tn
0
G(uuu(s)) ‚àòdW
W
W(s).
Subtracting, we see that
uuu(tn+1) = uuu(tn) + ‚à´
tn+1
tn
fff (uuu(s)) ds
+ ‚à´
tn+1
tn
G(uuu(s)) ‚àòdW
W
W(s).
Taking both
fff
and G constant over
[tn, tn+1), we obtain the Heun method. This
time, we evaluate G at the midpoint to be
consistent with the Stratonovich calculus,
that is, we examine
uuun+1 = uuun + fff (uuun) Œît + G(uuun+1‚àï2)ŒîW
W
W n,
ŒîW
W
W n ‚à∂= W
W
W(tn+1) ‚àíW
W
W(tn).
To implement this, we need, however, an
estimate of G(uuun+1‚àï2). Let us consider this
with d = m = 1. By Taylor‚Äôs theorem on G,
we have
G(un+1‚àï2) ‚âàG(u(tn)) + Œît
2 G‚Ä≤(u(tn))
and by a Ô¨Ånite diÔ¨Äerence approximation G‚Ä≤,
we get
G(un+1‚àï2) ‚âà1
2
(
G(u(tn)) + G(u(tn+1))
)
.
Using the Euler‚ÄìMaruyama to approxi-
mate u(tn+1), we can get a prediction v for

3.6 SDEs and Numerical Methods
97
u(tn+1):
v = un + f (un)Œît + G(un)ŒîWn
and then
un+1 =un+f (un)Œît+ 1
2
(
G(un) + G(v)
)
ŒîWn.
Note that the Brownian increment ŒîWn is
the same for the predicting step to get v
and the update step to get un+1. The incre-
ments ŒîWn ‚àºN(0, Œît). We can rewrite the
scheme in terms of ùúÅn ‚àºN(0, 1) as follows:
v = un + f (un)Œît +
‚àö
ŒîtG(un)ùúÅn
and
un+1 =un+f (un)Œît+ 1
2(G(un) + G(v))
‚àö
ŒîtùúÅn.
This gives the Heun method to approximate
the Stratonovich SDE, which, in general,
becomes
vvv = uuun + fff (uuun)Œît +
‚àö
ŒîtG(uuun)ùúÅùúÅùúÅn
and
uuun+1 =uuun+fff (uuun)Œît+ 1
2(G(uuun)+G(vvv))
‚àö
ŒîtùúÅùúÅùúÅn,
where ùúÅùúÅùúÅn ‚àºN(0, I) and is given in Algo-
rithm 3.4.
Theorem 3.7 (convergence) Let uuu(t) be
the solution of (3.39) and uuun be the Heun
approximation touuu(tn). Then, in general, the
Heun method converges strongly with order
Œît1‚àï2, that is, there is a C > 0, independent
of Œît, such that
sup
0‚â§tn‚â§T
(E[‚Äñuuu(tn) ‚àíuuun‚Äñ2])1‚àï2 ‚â§CŒît1‚àï2.
Suppose that fff and G are C‚àûfunctions and
all derivatives of fff and G are bounded. Sup-
pose ùúô‚à∂‚Ñùd ‚Üí‚Ñùis Lipschitz. Then, there
Algorithm
3.4
Heun
method
to
approximate
solution
of
SDE
du = f (u)dt + G(u) ‚àòdW.
Input : Initial data u0, Final time T, N num-
ber of steps to take.
Output: Vector t of time and vector u such
that un ‚âàu(tn).
1: Œît = T‚àïN, t0 = 0
2: for n = 1 ‚à∂N do
3:
tn = nŒît
4:
Ô¨Ånd z ‚àºN(0, 1)
5:
dWn =
‚àö
Œîtz
6:
v = un‚àí1 + Œîtf (un‚àí1) + G(un‚àí1)dWn
7:
un = un‚àí1 + Œîtf (un‚àí1) +
1
2(G(un‚àí1) + G(v))dWn
8: end for
exists C > 0, independent of Œît, such that
|||E[ùúô(uuu(T))] ‚àíE[ùúô(uuuN)]||| ‚â§CŒît.
The Heun method is the Stratonovich
equivalent
of
the
Euler‚ÄìMaruyama
method. As with the It√¥ case, we can
also derive a Stratonovich Milstein and
other higher-order methods [5].
3.6.3
Multilevel Monte Carlo
Suppose
we
need
to
approximate
E[ùúô(uuu(T))] to a given accuracy ùúñ. To
achieve this, both the weak discretization
error and the Monte Carlo sampling error
should be O(ùúñ) in (3.43). We have seen
that the weak discretization error for the
Heun and Euler‚ÄìMaruyama methods is
O(Œît), so we require Œît = O(ùúñ). The Monte
Carlo error is O(1‚àï
‚àö
M), so we require
M = O(ùúñ‚àí2).
We can measure the computational cost
by counting the total number of steps taken
by the numerical method. Finding one sam-
ple ofuuuN requires T‚àïŒît steps and Ô¨Ånding M

98
3 Stochastic DiÔ¨Äerential Equations
samples requires MT‚àïŒît steps. Thus, with
M = O(ùúñ‚àí2) and Œît = O(ùúñ), the total cost to
obtain a result with accuracy ùúñis
cost(ùúáM) = MT
Œît = O(ùúñ‚àí3).
(3.47)
A
more
eÔ¨Écient
method,
in
many
cases,
is
the
multilevel
Monte
Carlo
method, which is essentially a form of
variance
reduction.
This
method
uses
a hierarchy of time discretizations ŒîtùìÅ
to compute uuuùìÅ‚âàuuu(tn) on diÔ¨Äerent lev-
els ùìÅ= L, L ‚àí1, ‚Ä¶ , ùìÅ0, so, for example,
ŒîtùìÅ= ŒîtùìÅ‚àí1‚àï2.
The starting point of the multilevel
Monte Carlo method is the telescoping
sum
E[ùúô(uuuL)] = E[ùúô(uuuùìÅ0)]
+
L
‚àë
ùìÅ=ùìÅ0+1
E[ùúô(uuuùìÅ) ‚àíùúô(uuuùìÅ‚àí1)].
(3.48)
Thus, E[ùúô(uuuL)], the expected value with the
smallest time step ŒîtL, is rewritten into
E
[
ùúô(uuuùìÅ0)
]
, computed using the largest step
size Œît0 and a sum of correction terms. The
sample average is used to independently
estimate the expectations with a diÔ¨Äerent
number of samples on each level.
To estimate E
[
ùúô(uuuùìÅ0)] at level ùìÅ0, we use
the sample average ùúáùìÅ0 with MùìÅ0 indepen-
dent samples, given by
ùúáùìÅ0 ‚à∂=
1
MùìÅ0
MùìÅ0
‚àë
j0=1
ùúô(uuu
j0
ùìÅ0).
(3.49)
Similarly, to estimate the correction at level
ùìÅ= ùìÅ0 + 1, ‚Ä¶ , L, we use the sample aver-
age ùúáùìÅwith MùìÅsamples, deÔ¨Åned by
ùúáùìÅ‚à∂=
1
MùìÅ
MùìÅ
‚àë
jùìÅ=1
(
ùúô(uuu
jùìÅ
ùìÅ) ‚àíùúô(uuu
jùìÅ
ùìÅ‚àí1
))
.
(3.50)
Note that ùúô(uuuk
ùìÅ) ‚àíùúô(uuuk
ùìÅ‚àí1) is computed
using two diÔ¨Äerent time steps ŒîtùìÅand
ŒîtùìÅ‚àí1 but using the same Brownian path
for uuuk
ùìÅand uuuk
ùìÅ‚àí1. Independent sample paths
are used for each k and for each level ùìÅ.
Finally, E[ùúô(uuu(T))] is estimated by
ùúáML ‚à∂=
L
‚àë
ùìÅ=ùìÅ0
ùúáùìÅ.
(3.51)
Because of strong convergence of the
numerical method, the correction terms
should become small and the variance of
these terms decay.
As reappearing in the literature in [15],
the multilevel Monte Carlo method has
received a great deal of interest. It can read-
ily be extended to Milstein or other meth-
ods and also to other noise processes [16,
17]. It has been examined for nonglobal
Lipschitz functions ùúô[18] and combined
with other variance reduction techniques
[19].
3.7
SDEs and PDEs
The solutions of SDEs are closely related to
some partial diÔ¨Äerential equations (PDEs)
and we investigate these now. Suppose we
have that u satisÔ¨Åes the It√¥ SDE (3.19). We
could estimate statistics on the solution
u(t) by direct simulations and using Monte
Carlo techniques (see Chapter 2). Here,
we examine some alternative approaches.
There are two basic types of question we
can ask.
1. Given knowledge of the initial data,
what is the probability of being in a
particular state at time t. Here we are
looking forward.
2. What is the expected value of some
function of the solution at time t as a

3.7 SDEs and PDEs
99
function of the initial data. Here we are
looking backward in time.
We can obtain deterministic PDEs that
help answer these two questions. The main
tool is to use the It√¥ formula and the mean
zero property of the It√¥ integrals.
The solution of the SDE (3.19) is a Markov
process (see Chapter 1). This roughly says
the future evolution of u given what has
happened up to time t is the same as when
starting at u(t).
P(u(t + s) ‚ààA|u(r) ‚à∂0 ‚â§r ‚â§t)
= P(u(t + s) ‚ààA|u(t)).
The
probability
P(u(t) ‚ààA|u(s) = v)
is
called
the
transition
probability
and
the time-dependent probability density
function or transition density p(u, t; v, s)
satisÔ¨Åes
P(u(t) ‚ààA|u(s) = v) = ‚à´A
p(u, t; v, s)du.
To be a Markov process, the transition
density
needs
to
satisfy
the
Chap-
man‚ÄìKolmogorov equation for s < r < t,
p(u, t; v, s) = ‚à´‚Ñù
p(z, r; v, s)p(z, r; u, t)dz.
This states that we can get from v at time s to
u at time t by passing through z at interme-
diate times r. The Chapman‚ÄìKolmogorov
equation gives p(u, t; v, s) by integrating
over all possible intermediate z. For further
details, see [2, 6, 20, 21].
3.7.1
Fokker‚ÄìPlanck Equation
We start by looking at the forward problem
where we know the system at time 0 and we
want to have information about the system
at time t > 0. We develop ideas for an It√¥
SDE (3.19) with d = m = 1 and show that
p(u, t; u0, 0) satisÔ¨Åes the following PDE
ùúï
ùúït p(u, t; u0, 0)
= ‚àíùúï
ùúïu
(f (u(t))p(u, t; u0, 0)
)
+1
2
ùúï2
ùúïu2
((G(u(t)))2p(u, t; u0, 0)) .
This
is
called
the
(forward)
Fokker‚ÄìPlanck
equation
or
forward
Chapman‚ÄìKolmogorov equation.
Sketch of derivation. Given any smooth
function ùúôwe examine Eu0[ùúô(u(t))] ‚à∂=
E[ùúô(u(t))|u(0) = u0
], that is, the expecta-
tion conditional on being at u0 at time 0,
so
Eu0[ùúô(u(t))] = ‚à´‚Ñù
ùúô(u(t))p(u, t; u0, 0)du.
From the It√¥ formula (3.25) and using
that It√¥ integrals have mean zero (see DeÔ¨Å-
nition 3.2), we Ô¨Ånd that
Eu0[ùúô(u(t))] = Eu0[ùúô(u(s))]
+ Eu0
[
‚à´
t
0
ùúïùúô
ùúïu f (u(s))
+ 1
2
ùúï2ùúô
ùúïu2 (G(u(s)))2ds
]
.
We diÔ¨Äerentiate with respect to t to get
d
dt Eu0[ùúô(u(t))] = Eu0
[ùúïùúô
ùúïu f (u(t))
+ 1
2
ùúïùúô
ùúïu2 (G(u(t)))2
]
.
Now use the deÔ¨Ånition of the expectation to
get
‚à´‚Ñù
ùúô(u) ùúï
ùúït p(u, t; u0, 0)du
= ‚à´‚Ñù
(ùúïùúô
ùúïu f (u(t))
+ 1
2
ùúï2ùúô
ùúïu2 (G(u(t)))2
)
p(u, t; u0, 0)du.

100
3 Stochastic DiÔ¨Äerential Equations
If we make assumptions on p ‚Üí0 as u ‚Üí
‚àû, we can use integration by parts once on
ùúïùúô‚àïùúïu fp and twice on ùúï2ùúô‚àïùúïu2Gp to get
‚à´‚Ñù
ùúô(u) ùúï
ùúït p(u, t; u0, 0)du
= ‚à´‚Ñù
ùúô(u)
(
‚àíùúï
ùúïu(f (u(t))p(u, t; u0, 0))
+ 1
2
ùúï2
ùúïu2
((G(u(t)))2p(u, t; u0, 0)))
du.
The above relation is true for any smooth ùúô.
Hence, we must have that
ùúï
ùúït p(u, t; u0, 0)
= ‚àíùúï
ùúïu(f (u(t))p(u, t; u0, 0))
+ 1
2
ùúï2
ùúïu2
((G(u(t)))2p(u, t; u0, 0)) . (3.52)
As initial data, we need p(u, 0; u0, 0) =
ùõø(u0). The solution of the SDE u takes any
value in ‚Ñùand so the PDE (3.52) is posed
on ‚Ñù. Sometimes, (3.52) is rewritten with
p = p(u, 0; u0, 0) as
ùúïp
ùúït = ùúïJ
ùúïx,
where J ‚à∂= fp + 1
2
ùúï
ùúïu(G2p) is a ‚Äúprobability
Ô¨Çux.‚Äù
Example 3.14 [Brownian motion] For the
SDE du = dW, f = 0 and G = 1. Hence the
Fokker‚ÄìPlanck equation for the probability
density p satisÔ¨Åes the heat equation
ùúï
ùúït p = 1
2
ùúï2
ùúïu2 p,
with u ‚àà‚Ñù. If the initial condition is
p(u, 0) = ùõø(u0), the solution is given by
p(u, t) = 1‚àï
‚àö
2ùúãt e‚àíu2‚àï(2t).
We can use the Fokker‚ÄìPlanck equation
to examine moments deÔ¨Åned by
Mn ‚à∂= ‚à´‚Ñù
unp(u, t)dx
for
n = 0, 1, 2, ‚Ä¶ .
Example 3.15 [OU process] For the SDE
du = ‚àíùúÜu + ùúédW, f = ‚àíùúÜu and G = ùúé.
Here the Fokker‚ÄìPlanck equation for the
probability density p satisÔ¨Åes the advection
diÔ¨Äusion equation
ùúï
ùúït p = ùúÜùúï
ùúïu(up) + ùúé2
2
ùúï2
ùúïu2 p
u ‚àà‚Ñù. Let us use this to look at the Ô¨Årst
few moments. For n = 0, we integrate the
Fokker‚ÄìPlanck equation over ‚Ñù
‚à´‚Ñù
ùúï
ùúït pdu = ùúÜ‚à´‚Ñù
ùúï
ùúïu(up)du + ‚à´‚Ñù
ùúé2
2
ùúï2
ùúïu2 pdu.
Integration by parts and using the decay
of p as u ‚Üí¬±‚àûgives d‚àïdt M0 = 0 and so
M0(t) = M0(0) = 1. This is a statement that
probability is conserved.
For n = 1,
‚à´‚Ñù
u ùúï
ùúït pdu = ùúÜ‚à´‚Ñù
u ùúï
ùúïu(up)du
+ ‚à´‚Ñù
uùúé2
2
ùúï2
ùúïu2 pdu.
Once again, integration by parts and decay
of p as u ‚Üí¬±‚àûgives d‚àïdt M1 = ‚àíùúÜM1 and
so the Ô¨Årst moment decays exponentially
fast, M1(t) = e‚àíùúÜtM1(0). We have already
seen this in (3.30).
For n ‚â•2, we can proceed in the same
way and get a diÔ¨Äerential equation for Mn,
d
dt Mn = ‚àíùúÜnMn + ùúé2
2 n(n ‚àí1)Mn‚àí2,
which
has
solution
by
variation
of
constants

3.7 SDEs and PDEs
101
Mn(t) = e‚àíùúÜntMn(0) + ùúé2
2 n(n ‚àí1)
√ó ‚à´
t
0
e‚àíùúÜn(t‚àís)Mn‚àí2(s)ds.
This method of obtaining equations for
moments can be applied to SDEs in general.
These equations for the moments can then
be solved or simulated to obtain statistics
on the solutions. To obtain a Ô¨Ånite-sized
system, the inÔ¨Ånite system of equations for
the moments needs to be approximated by
a Ô¨Ånite set. This is often called moment
closure.
We now state the general form for the
Fokker‚ÄìPlanck equation.
Theorem 3.8 (Fokker‚ÄìPlanck) Consider
the It√¥ SDE (3.19)
duuu = fff (uuu)dt + G(uuu)dW
W
W(t).
Then the transition probability density p =
p(u, t; u0, 0) satisÔ¨Åes
ùúïp
ùúït = L‚àóp,
where
L‚àóp(u) = ‚àí
d
‚àë
i=1
ùúï
ùúïu
(fi(u)p(u))
+ 1
2
d
‚àë
i,j=1
ùúï2
ùúïuiùúïuj
((G(u)G(u)T)ijp(u)) .
If the stochastic process u(t) has an invari-
ant density p0, then it satisÔ¨Åes L‚àóp0 = 0.
Another
application
of
the
Fokker‚ÄìPlanck equation is to examine the
long-term time behavior of the transition
density. The SDE is said to have a invari-
ant density if p‚àû(u) = limt‚Üí‚àûp(u, t; u0, 0)
converges for all initial data u0. For Brow-
nian motion of Example 3.14, we see that
p‚àû= 0. For the OU process, we see from
Example 3.15
that
p‚àû(u) = Ce‚àíùúÜu2‚àïùúé2,
where C is such that ‚à´‚Ñùp‚àû(u)du = 1, that
is, the invariant density of the OU process
is a Gaussian.
Example 3.16 Consider the SDE
du = ùúï
ùúïu log(q(u))dt +
‚àö
2dW.
This SDE has an invariant density p‚àû= q.
Let us look at the transition density for this
SDE. From (3.52), we Ô¨Ånd
ùúï
ùúït p = ‚àíùúï
ùúïu
(
ùúï
ùúïuq1
qp
)
+ ùúï2
ùúïu2 p
and thus if p = q, we have ùúïp‚àïùúït = 0. This
idea can be used for Monte Carlo Markov
Chain methods and drawing samples from
given distributions q for Metropolis Hast-
ings algorithms (see, for example, [22]).
Example 3.17 [Langevin equations] Con-
sider a particle in a potential V(u) subject
to some white noise
du = ‚àíùúÜùúïV
ùúïu dt +
‚àö
2ùúédW
for ùúÜ, ùúé> 0. Assume that V is twice dif-
ferentiable. The Fokker‚ÄìPlanck equation is
given by
ùúïp
ùúït = ùúÜùúï
ùúïu
(ùúïV
ùúïu p
)
+ ùúéùúï2p
ùúïu2 .
For the steady-state solution, we need the
Ô¨Çux ùúïV‚àïùúïu p + ùúéùúïp‚àïùúïu to be constant. As
p is assumed to decay to zero at inÔ¨Ånity, we
can argue that
ùúïV
ùúïu p + ùúéùúïp
ùúïu = 0,
which is solved by p‚àû(u) = Ce‚àíùúÜV(u)‚àïùúé2.

102
3 Stochastic DiÔ¨Äerential Equations
Now reconsider the Example 3.4 and the
Langevin equation (3.23). It can be shown
using the Fokker‚ÄìPlanck equation that
the equilibrium distribution has probabil-
ity density function p(q, p) = e‚àíùõΩH(q,p)‚àïZ
for
H(q, p) = p2‚àï2 + V(q),
normaliza-
tion constant Z, and inverse temperature
ùõΩ= 1‚àïkBT = 2ùúÜ‚àïùúé2.
3.7.2
Backward Fokker‚ÄìPlanck Equation
Now
we
are
interested
in
E[Œ®(u(T))|u(t) = v],
that
is,
we
want
the expected value of some function Œ® of
u(T) at T > t, given that u(t) = v. Let us
consider a simple case. Suppose u satisÔ¨Åes
the It√¥ SDE (3.19) with d = m = 1, so
du = f (u)dt + G(u)dW.
We show that p(u, t)‚à∂=E[Œ®(u(T)|u(t)=v]
satisÔ¨Åes the PDE
ùúï
ùúït p(u, t) = ‚àíf (u) ùúï
ùúïup(u, t) ‚àí1
2
ùúï2
ùúïu2 p(u, t),
for
t < T,
p(u, T) = Œ®(u(T)).
(3.53)
3.7.2.1
Sketch of Derivation.
The techniques are essentially the same as
for the forward Fokker‚ÄìPlanck equation.
Let ùúô(u(t), t) be the solution of (3.53). We
show that ùúô(u(t), t) = E[Œ®(u(T)|u(t) = v].
We apply the It√¥ formula (3.25) to
ùúô(u(t), t) to get
d(ùúô(u(s), s))
= ùúï
ùúïuùúôdu + 1
2
ùúï2
ùúïu2 ùúôdu + ùúï
ùúïsùúôds
=
(
ùúï
ùúïsùúô+ f (u) ùúï
ùúïuùúô+ 1
2(G(u))2 ùúï2
ùúïu2 ùúô
)
dt
+ G(u) ùúï
ùúïuùúôdW.
Thus, because ùúô(u(t), t) is solution of (3.53),
we have
ùúô(u(T), T)=ùúô(u(t), t)+‚à´
T
t
G(u) ùúï
ùúïuùúôdW(s).
Taking the expectation and using the prop-
erty that the It√¥ integral has mean value 0,
we get
E[ùúô(u(T), T)|u(t) = v] = ùúô(u(t), t).
As the solution of (3.53) is unique, we have
the result.
Example 3.18 [Black‚ÄìScholes PDE] Con-
sider the geometric Brownian motion SDE
(3.24). Then p(u, t) = E[ùúô(u(T), T)] satis-
Ô¨Åes the advection diÔ¨Äusion equation
ùúï
ùúït p = ‚àíru ùúï
ùúïu(p) + ùúé2
2 u2 ùúï2
ùúïu2 (p),
u ‚àà‚Ñù, t ‚àà[0, T], and p(u, T) = Œ¶(u(T)).
This is of interest in Ô¨Ånance as it used for
pricing of options at t = 0 with the so-called
payoÔ¨ÄŒ® at time T.
In fact, the backward Fokker‚ÄìPlanck can
also be written for transition probabilities.
Theorem 3.9 (backward Fokker‚ÄìPlanck)
Consider the It√¥ SDE
duuu = fff (uuu)dt + G(uuu)dW
W
W(t).
The
transition
probability
density
p(u, t; v, s) satisÔ¨Åes
ùúïp
ùúït = Lp,
where
Lp(u) = ‚àí
d
‚àë
i=1
fi(u) ùúï
ùúïu (p(u))
+ 1
2
d
‚àë
i,j=1
(G(u)G(u)T)ij
ùúï2
ùúïuiùúïuj
(p(u)) .

3.7 SDEs and PDEs
103
Remark 3.5 Suppose the SDE
duuu = fff (uuu)dt + G(uuu)dW
W
W(t)
has initial data u(0) = v. Let ùúô‚à∂‚Ñùd ‚Üí‚Ñù
be a bounded (measurable) function. We
can associate with the SDE a family of linear
operators S(t), t ‚â•0 such that
S(t)ùúô(u) = E[ùúô(u(t)].
The generator of the semigroup S is the oper-
ator L such that
Lùúô(u) = lim
t‚Üí0
S(t)ùúô(u) ‚àíùúô(u)
t
.
The operator L‚àóin Theorem 3.8 is the
adjoint of L.
3.7.2.2
Boundary Conditions
In our discussion of the Fokker‚ÄìPlanck
equations, we assumed that u ‚àà‚Ñùand we
imposed decay conditions on p(u, t; u0, 0)
as u ‚Üí¬±‚àû. We could also consider impos-
ing some boundaries on u and hence pose
the Fokker‚ÄìPlanck equations on a Ô¨Ånite
(or semi-inÔ¨Ånite) domain. Consider, for
example, the Langevin equation for the
motion of a particle in Example 3.17. We
may be interested in the particle arriving
at a certain part of phase space and in
particular the Ô¨Årst time that it arrives
there. This gives the idea of Ô¨Årst passage
time.
If the particle is absorbed as it reaches
the boundary, then we get an absorbing
boundary condition so that p(u, t) = 0
on the boundary. For example, if we
have u ‚àà[a, b] ‚äÇ‚Ñùand the particle gets
absorbed when it reaches either a or b,
then p(a, t) = p(b, t) = 0 (also called a zero
Dirichlet boundary condition). If the parti-
cle is reÔ¨Çected, we have reÔ¨Çecting boundary
conditions for the PDE and the outward
normal derivative of p is zero at the bound-
ary (n ‚ãÖ‚àáP = 0). In one dimension, with
reÔ¨Çecting boundaries at a and b, we have
ùúïp‚àïùúïu|u=a = ùúïp‚àïùúïu|u=b = 0.
The backward Fokker‚ÄìPlanck equation
can be used to obtain expressions for the
mean Ô¨Årst passage times. This is of par-
ticular interest for Langevin-type dynam-
ics of a particle moving in a potential V.
We can seek the Ô¨Årst exit time from some
domain Óà∞. For the one-dimensional SDE
in Example 3.17, it can be shown that the
expected Ô¨Årst passage is of the order e2R‚àïùúé2
where R is the minimal potential diÔ¨Äer-
ence required to leave Óà∞. This is known as
Arrhenius‚Äô law. There are a large number of
results in this area (see, for example, [2, 20,
21]). An alternative approach is based on
large deviation theory. Although the sam-
ple paths of the solution of an SDE are
not diÔ¨Äerentiable, the idea is that for small
noise they should be close to some smooth
curves that are often taken from the deter-
ministic system. Suppose we have a smooth
curve ùúì‚à∂[0 ‚à∂T] ‚Üí‚Ñùd. A rate function J
is introduced that measures the cost of the
sample path u(t) being close to the curve
ùúôover the interval [0, T]. The theory of
Wentzell‚ÄìFreidlin then gives estimates on
staying close to the mean path. For a review
of large deviation theory, mean Ô¨Årst passage
times, and Kramer‚Äôs method see [2, 3, 6, 21,
23, 24].
3.7.3
Filtering
Filtering is a classic problem of trying to
estimate a function of the state of a noisy
system X(t) at time t where you are only
given noisy observations Y(s), s ‚â§t up to
time t.
The Kalman‚ÄìBucy model assumes that
X and Y are determined by linear SDEs so
that, in one dimension, we have the signal

104
3 Stochastic DiÔ¨Äerential Equations
X is the solution of
dX = ùúÜ(t)X(t)dt + ùúé(t)dW1(t),
X(0) = X0,
and the observations satisfy
dY = ùúá(t)X(t)dt + dW2(t),
Y(0) = Y0.
It is assumed that ùúÜ(t), ùúá(t), and ùúé(t) sat-
isfy ‚à´T
0 |a(t)|dt < ‚àû, a ‚àà{ùúÜ, ùúá, ùúé}. W1 and
W2 are assumed to be independent Brow-
nian motions. The idea is to estimate the
expected value of X given the observations
Y, m(t) ‚à∂= E[X(t)|Y], and the mean square
error z(t) ‚à∂= E[(X(t) ‚àím(t))2]
Theorem 3.10 (Kalman‚ÄìBucy) The
estimate m(t) satisÔ¨Åes the SDE
dm = (ùúÜ(t) ‚àíz(t)(ùúá(t))2)m(t)dt + z(t)ùúá(t)dY,
m(0) = E[X0|Y0
],
where z(t) satisÔ¨Åes the deterministic Ricatti
equation
dz
dt = 2ùúÜ(t)z(t) ‚àí(z(t))2(ùúá(t))2 + (ùúé(t))2,
z(0) = E[X0 ‚àím(0)].
Furthermore, the solution of the equations is
unique.
For further examples on SDEs and Ô¨Ål-
tering and related problems, see [6] and
reviews [25‚Äì28].
Further Reading
SDEs and their mathematical theory are
described in many texts, including [6,
29‚Äì31] and from a physical perspective
in [2, 21, 32, 33]. Numerical methods
and their analysis are covered in [5, 34].
The book [4] covers much of the mate-
rial and also considers both random and
stochastic
PDEs
and
their
numerical
simulation.
In place of a Brownian motion W
W
W(t),
other types of stochastic process can be
used to force the SDE and this is currently
an active research topic. For example, we
may consider forcing by a fractional Brow-
nian motion BH(t) in place of W(t). See,
for example, [35] and numerical methods
for such equations in [36]. More gener-
ally, Lyon‚Äôs theory of rough paths [37, 38]
allows very general driving terms to be
considered.
This article has only covered some of the
basic ideas related to SDEs. It has not dis-
cussed, for example, the dynamics related
to SDEs. There has been a lot of interest in
phenomena such as stochastic resonance
where the presence of a small amount of
noise in a system can lead to larger scale
eÔ¨Äects. This is often through the eÔ¨Äects
of noise when the deterministic system is
close to a bifurcation. For more general
random dynamical systems, see [39]. A
good book on noise-induced phenomena
for slow‚Äìfast systems, including stochastic
resonance, is [3].
There is a growing interest in stochas-
tically forced partial diÔ¨Äerential equations
(SPDEs). The classic theoretical reference is
[40]. More recent expositions on the theory
include [41, 42].
Glossary
expectation Let p(x) be the pdf of X. Then
the expected value of X is given by E[X] =
‚à´‚Ñùxp(x). This can be approximated by the
sample average.
Gaussian
A
random
variable
X
follows
the
Gaussian
or
normal
dis-
tribution
(X ‚àºN(ùúá, ùúé2))
if
its
pdf
is
p(x) = 1‚àï
‚àö
2ùúãùúé2 exp(‚àí(x ‚àíùúá)2‚àï(2ùúé2)).

References
105
independent
If X and Y are real values
with densities pX and pY they are inde-
pendent if and only if the joint probability
density function pX,Y(x, y) = pX(x), pY(y),
x, y ‚àà‚Ñù. Independent random variables
are uncorrelated.
multivariate Gaussian
X ‚àà‚Ñùd is a mul-
tivariate Gaussian if it follows the distri-
bution N(ùúáùúáùúá, C) where C is a covariance
matrix. Gaussian processes are uniquely
determined by their mean and their covari-
ance functions.
probability density function (pdf)
is
the function such that P(B) = ‚à´B p(x)dx, so
P(X ‚àà(a, b)) = ‚à´b
a p(x)dx.
probability space
(Œ©, Óà≤, P) consists of a
sample space Œ©, a set of events Óà≤and a
probability measure P.
sample path/realisation
For a Ô¨Åxed ùúî‚àà
Œ© a X(t, ùúî) is sample path.
stochastic process is a collection of ran-
dom variables X(t) that represents the evo-
lution of a system over time that depends on
the probability space (Œ©, Óà≤, P). To empha-
size dependence on the probability space
X(t) is sometimes written as X(t, ùúî), ùúî‚ààŒ©.
uncorrelated
If Cov (X, Y) = 0 then the
random variables X and Y are uncorrelated.
References
1. H√§nggi, P. and Jung, P. (1995) Colored noise
in dynamical systems. Adv. Chem. Phys.,
239‚Äì326.
2. Gardiner, C. (2009) Stochastic Methods: A
Handbook for the Natural and Social
Sciences, Springer Series in Synergetics, 4th
edn, Springer, Berlin.
3. Berglund, N. and Gentz, B. (2006)
Noise-Induced Phenomena in Slow-Fast
Dynamical Systems. Probability and its
Applications (New York). Springer-Verlag
London Ltd, London, A sample-paths
approach.
4. Lord, G.J., Powell, C.E., and Shardlow, T.
(2014) Introduction to Computational
Stochastic Partial DiÔ¨Äerential Equations,
CUP.
5. Peter, E. and Eckhard Platen, K. (1992)
Numerical Solution of Stochastic DiÔ¨Äerential
Equations, Applications of Mathematics, vol.
23, Springer.
6. √òksendal, B. (2003) Stochastic DiÔ¨Äerential
Equations, 6th edn, Universitext Springer,
Berlin.
7. Kloeden, P.E. and Neuenkirch, A. (2007) The
pathwise convergence of approximation
schemes for stochastic diÔ¨Äerential equations.
LMS J. Comput. Math., 10, 235‚Äì253.
8. Jentzen, A., Kloeden, P.E., and Neuenkirch,
A. (2008) Pathwise convergence of
numerical schemes for random and
stochastic diÔ¨Äerential equations, in
Foundations of Computational Mathematics,
London Mathematical Society Lecture Note
Series, vol. 363 (ed. H. Kong), Cambridge
University Press, pp. 140‚Äì161.
9. Jentzen, A., Kloeden, P.E., and Neuenkirch,
A. (2009) Pathwise approximation of
stochastic diÔ¨Äerential equations on domains:
higher order convergence rates without
global Lipschitz coeÔ¨Écients. Numer. Math.,
112(1), 41‚Äì64, doi: 10.1007/s00211-008-
0200-8. URL http://dx.doi.org/10.1007/
s00211-008-0200-8.
10. Williams, D. (1991) Probability with
Martingales, Cambridge University Press.
11. Buckwar, E. and Sickenberger, T. (2011) A
comparative linear mean-square stability
analysis of Maruyama- and Milstein-type
methods. Math. Comput. Simulat., 81(6),
1110‚Äì1127.
12. Higham, D.J., Mao, X., and Stuart, A.M.
(2003) Exponential mean-square stability of
numerical solutions to stochastic diÔ¨Äerential
equations. London Math. Soc. J. Comput.
Math., 6, 297‚Äì313.
13. Burrage, K., Burrage, P., and Mitsui, T. (2000)
Numerical solutions of stochastic diÔ¨Äerential
equations ‚Äî implementation and stability
issues. J. Comput. Appl. Math., 125(1‚Äì2),
171‚Äì182, doi: 10.1016/S0377-0427(00)
00467-2. URL http://dx.doi.org/10.1016/
S0377-0427(00)00467-2.
14. Higham, D.J. (2000) Mean-square and
asymptotic stability of the stochastic theta
method. SIAM J. Numer. Anal., 38(3),
753‚Äì769, doi: 10.1137/S003614299834736X.

106
3 Stochastic DiÔ¨Äerential Equations
URL http://dx.doi.org/10.1137/S0036142
99834736X.
15. Giles, M.B. (2008) Multilevel Monte Carlo
path simulation. Oper. Res., 56(3), 607‚Äì617,
doi: 10.1287/opre.1070.0496. URL
http://dx.doi.org/10.1287/opre.1070.0496.
16. Giles, M.B. (2008) Improved multilevel
Monte Carlo convergence using the Milstein
scheme, in Monte Carlo and Quasi-Monte
Carlo Methods 2006, Springer, Berlin, pp.
343‚Äì358, doi: 10.1007/978-3-540-74496-
2_20. URL http://dx.doi.org/10.1007/978-3-
540-74496-2_20.
17. Dereich, S. (2011) Multilevel Monte Carlo
algorithms for L√©vy-driven SDEs with
Gaussian correction. Ann. Appl. Probab.,
21(1), 283‚Äì311, doi: 10.1214/10-AAP695.
URL http://dx.doi.org/10.1214/10-AAP695.
18. Giles, M.B., Higham, D.J., and Mao, X.
(2009) Analysing multi-level Monte Carlo
for options with non-globally Lipschitz
payoÔ¨Ä. Finance Stoch., 13(3), 403‚Äì413, doi:
10.1007/s00780-009-0092-1. URL
http://dx.doi.org/10.1007/
s00780-009-0092-1.
19. Giles, M.B. and Waterhouse, B.J. (2009)
Multilevel quasi-Monte Carlo path
simulation, in Advanced Financial
Modelling, Radon Series on Computational
and Applied Mathematics, vol. 8 (eds H.
Abrecher, W.J. Runggaldier, and W.
Schachermayer), Walter de Gruyter, Berlin,
pp. 165‚Äì181, doi: 10.1515/9783110213
140.165. URL http://dx.doi.org/10.1515/
9783110213140.165.
20. Risken, H. (1996) The Fokker-Planck
Equation: Methods of Solution and
Applications, Lecture Notes in Mathematics,
Springer-Verlag.
21. van Kampen, N.G. (1997) Stochastic
Processes in Physics and Chemistry, 2nd edn,
North‚ÄìHolland.
22. Leli√®vre, T., Stoltz, G., and Rousset, M.
(2010) Free Energy Computations: A
Mathematical Perspective, Imperial College
Press.
23. Freidlin, I. and Wentzell, A.D. (1998)
Random Perturbations of Dynamical
Systems, Die Grundlehren der
mathematischen Wissenschaften,
Springer-Verlag.
24. Varadhan, S.R.S. (1984) Large Deviations
and Applications, CBMS-NSF Regional
Conference Series in Applied Mathematics,
Society for Industrial and Applied
Mathematics.
25. RozovskiÀòƒ±, B.L. (1990) Stochastic Evolution
Systems, Mathematics and its Applications
(Soviet Series), vol. 35, Kluwer Academic
Publishers Group, Dordrecht.
26. Bain, A. and Crisan, D. (2009) Fundamentals
of Stochastic Filtering, Stochastic Modelling
and Applied Probability, vol. 60, Springer,
New York.
27. Stuart, A.M. (2010) Inverse problems: a
Bayesian perspective. Acta Numer., 19,
451‚Äì559, doi: 10.1017/S0962492910000061.
URL http://dx.doi.org/10.1017/S09624
92910000061.
28. Crisan, D. and RozovskiÀòƒ±, B. (2011)
Introduction, in The Oxford Handbook of
Nonlinear Filtering, Oxford University Press,
Oxford, pp. 1‚Äì15.
29. Karatzas, I. and Shreve, S.E. (1991) Brownian
Motion and Stochastic Calculus, Graduate
Texts in Mathematics, vol. 113, 2nd edn,
Springer, New York.
30. Mao, X. (2008) Stochastic DiÔ¨Äerential
Equations and Applications, 2nd edn,
Horwood.
31. Protter, P.E. (2005) Stochastic Integration
and DiÔ¨Äerential Equations, Stochastic
Modelling and Applied Probability, vol. 21,
2nd edn, Springer.
32. √ñttinger, H.C. (1996) Stochastic Processes in
Polymeric Fluids, Springer, Berlin.
33. Nelson, E. (1967) Dynamical Theories of
Brownian Motion, Princeton University
Press.
34. Milstein, G.N. and Tretyakov, M.V. (2004)
Stochastic Numerics for Mathematical
Physics, Springer, Berlin.
35. Mishura, Y.S. (2008) Stochastic Calculus for
Fractional Brownian Motion and Related
Processes, Lecture Notes in Mathematics, vol.
1929, Springer, Berlin, doi: 10.1007/978-3-
540-75873-0. URL http://dx.doi.org/10.1007/
978-3-540-75873-0.
36. Neuenkirch, A. (2008) Optimal pointwise
approximation of stochastic diÔ¨Äerential
equations driven by fractional Brownian
motion. Stochastic Process. Appl., 118(12),
2294‚Äì2333, doi: 10.1016/j.spa.2008.01.002.
URL http://dx.doi.org/10.1016/j.spa.
2008.01.002.
37. Friz, P.K. and Victoir, N.B. (2010)
Multidimensional Stochastic Processes as
Rough Paths, Cambridge Studies in

References
107
Advanced Mathematics, vol. 120, Cambridge
University Press.
38. Davie, A.M. (2007) DiÔ¨Äerential equations
driven by rough paths: an approach via
discrete approximation. Appl. Math. Res.
Express., 2, 40.
39. Arnold, L. (1998) Random Dynamical
Systems, Monographs in Mathematics,
Springer.
40. Da Prato, G. and Zabczyk, J. (1992)
Stochastic Equations in InÔ¨Ånite Dimensions,
Encyclopedia of Mathematics and its
Applications, vol. 44, Cambridge University
Press.
41. Pr√©v√¥t, C. and R√∂ckner, M. (2007) A Concise
Course on Stochastic Partial DiÔ¨Äerential
Equations, Lecture Notes in Mathematics,
vol. 1905, Springer, Berlin.
42. Chow, P.-L. (2007) Stochastic Partial
DiÔ¨Äerential Equations, Chapman &
Hall/CRC, Boca Raton, FL.


109
Part II
Discrete Mathematics, Geometry, Topology
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.


111
4
Graph and Network Theory
Ernesto Estrada
4.1
Introduction
Graph Theory was born in 1736 when
Leonhard Euler [1] published ‚ÄúSolutio
problematic as geometriam situs pertinen-
tis‚Äù (The solution of a problem relating to
the theory of position). This history is well
documented [2] and widely available in
any textbook of graph or network theory.
However, the word graph appeared for the
Ô¨Årst time in the context of natural sciences
in 1878, when the English mathemati-
cian James J. Sylvester [3] wrote a paper
entitled ‚ÄúChemistry and Algebra‚Äù which
was published in Nature, where he wrote
that ‚ÄúEvery invariant and covariant thus
becomes expressible by a graph precisely
identical with a Kekulean diagram or
chemicograph.‚Äù The use of graph theory in
condensed matter physics, pioneered by
many chemical and physical graph theo-
rists [4, 5], is today well established; it has
become even more popular after the recent
discovery of graphene.
There are few, if any, areas of physics in
the twenty-Ô¨Årst century in which graphs
and networks are not involved directly or
indirectly. Hence it is impossible to cover
all of them in this chapter. Thus I owe the
reader an apology for the incompleteness
of this chapter and a promise to write
a more complete treatise. For instance,
quantum graphs are not considered in
this chapter and the reader is referred to
a recent introductory monograph on this
topic for details [6]. In this chapter, we
will cover some of the most important
areas of applications of graph theory in
physics. These include condensed mat-
ter physics, statistical physics, quantum
electrodynamics,
electrical
networks,
and vibrational problems. In the second
part, we summarize some of the most
important aspects of the study of com-
plex networks. This is an interdisciplinary
area that has emerged with tremendous
impetus in the twenty-Ô¨Årst century and
that studies networks appearing in com-
plex systems. These systems range from
molecular and biological to ecological,
social, and technological systems. Thus
graph theory and network theory have
helped to broaden the horizons of physics
to embrace the study of new complex
systems.
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

112
4 Graph and Network Theory
We hope this chapter motivates the
reader to Ô¨Ånd more about the connec-
tions between graph/network theory and
physics, consolidating this discipline as an
important part of the curriculum for the
physicists of the twenty-Ô¨Årst century.
4.2
The Language of Graphs and Networks
The Ô¨Årst thing that needs to be clariÔ¨Åed
is that the terms graphs and networks are
used indistinctly in the literature. In this
chapter, we will reserve the term graph for
the abstract mathematical concept, in gen-
eral referred to small, artiÔ¨Åcial formations
of nodes and edges. The term network is
then reserved for the graphs representing
real-world objects in which the nodes rep-
resent entities of the system and the edges
represent the relationships among them.
Therefore, it is clear that we will refer to the
system of individuals and their interactions
as a ‚Äúsocial network‚Äù and not as a ‚Äúsocial
graph.‚Äù However, they should mean exactly
the same.
For the basic concepts of graph theory,
the reader is recommended to consult
the introductory book by Harary [7]. We
start by deÔ¨Åning a graph formally. Let us
consider a Ô¨Ånite set V = {v1, v2, ‚Ä¶ , vn
}
of unspeciÔ¨Åed elements and let V ‚äóV be
the set of all ordered pairs [vi, vj
] of the
elements of V. A relation on the set V is
any subset E ‚äÜV ‚äóV. The relation E is
symmetric if [vi, vj
] ‚ààE implies [vj, vi
] ‚ààE
and it is reÔ¨Çexive if ‚àÄv ‚ààV, [v, v] ‚ààE. The
relation E is antireÔ¨Çexive if [vi, vj
] ‚ààE
implies vi ‚â†vj. Now we can deÔ¨Åne a simple
graph as the pair G = (V, E), where V is a
Ô¨Ånite set of nodes, vertices, or points and
E is a symmetric and antireÔ¨Çexive relation
on V, whose elements are known as the
edges or links of the graph. In a directed
graph, the relation E is nonsymmetric. In
many physical applications, the edges of
the graphs are required to support weights,
that is, real numbers indicating a speciÔ¨Åc
property of the edge. In this case, the
following more general deÔ¨Ånition is conve-
nient. A weighted graph is the quadruple
G = (V, E, W, f ) where V is a Ô¨Ånite set
of
nodes,
E ‚äÜV ‚äóV = {e1, e2, ‚Ä¶ , em
}
is a set of edges, W = {w1, w2, ‚Ä¶ , wr
}
is a set of weights such that wi ‚àà‚Ñù, and
f ‚à∂E ‚ÜíW is a surjective mapping that
assigns a weight to each edge. If the weights
are natural numbers, then the resulting
graph is a multigraph in which there could
be multiple edges between pairs of vertices;
that is, if the weight between nodes p and
q is k ‚ààN, it means that there are k links
between the two nodes.
In an undirected graph, we say that two
nodes p and q are adjacent if they are joined
by an edge e = {p, q}. In this case, we say
that the nodes p and q are incident to the
link e, and the link e is incident to the
nodes p and q. The two nodes are called
the end nodes of the edge. Two edges e1 =
{p, q} and e2 = {r, s} are adjacent if they are
both incident to at least one node. A sim-
ple but important characteristic of a node
is its degree, which is deÔ¨Åned as the num-
ber of edges that are incident to it or equiv-
alently as the number of nodes adjacent
to it. Slightly diÔ¨Äerent deÔ¨Ånitions apply to
directed graphs. The node p is adjacent to
node q if there is a directed link from p to
q, e = (p, q). We also say that a link from p
to q is incident from p and incident to q;
p is incident to e and q is incident from e.
Consequently, we have two diÔ¨Äerent kinds
of degrees in directed graphs. The in-degree
of a node is the number of links incident to
it and its out-degree is the number of links
incident from it.

4.2 The Language of Graphs and Networks
113
4.2.1
Graph Operators
The incidence and adjacency relations in
graphs allow us to deÔ¨Åne the following
graph operators. We consider an undi-
rected graph for which we construct its
incidence matrix with an arbitrary ori-
entation of its entries. This is necessary
to consider that the incidence matrix is
a discrete analogue of the gradient; that
is, for every edge {p, q}, p is the positive
(head) and q the negative (tail) end of the
oriented link. Let the links of the graph be
labeled as e1, e2, ‚Ä¶ , em. Hence the oriented
incidence matrix ‚àá(G) is
‚àáij(G) =
‚éß
‚é™
‚é®
‚é™‚é©
+1
node vi is the head of link ej
‚àí1
node vi is the tail of link ej
0
otherwise
.
We remark that the results obtained below
are independent of the orientation of the
links but assume that once the links are ori-
ented, this orientation is not changed. Let
the vertex LV and edge LE spaces be the
vector spaces of all real-valued functions
deÔ¨Åned on V and E, respectively. The inci-
dence operator of the graph is then deÔ¨Åned
as
‚àá(G) ‚à∂LV ‚ÜíLE,
(4.1)
such that for an arbitrary function f ‚à∂V ‚Üí
‚Ñù, ‚àá(G)f ‚à∂E ‚Üí‚Ñúis given by
(
‚àá(G)f
)
(e) = f (p) ‚àíf (q),
(4.2)
where p are the starting (head) and q the
ending (tail) points of the oriented link e.
Here we consider that f is a real or vector-
valued function on the graph with |f | being
ùúá-measurable for certain measure ùúáon the
graph.
On the other hand, let H be a Hilbert
space with scalar product ‚ü®‚ãÖ, ‚ãÖ‚ü©and norm
‚Äñ‚ãÖ‚Äñ. Let G = (V, E) be a simple graph. The
adjacency operator is an operator acting on
the Hilbert space H ‚à∂= l2 (V) deÔ¨Åned as
(Af )
(p) ‚à∂=
‚àë
u,v‚ààE
f (q), f ‚ààH, i ‚ààV.
(4.3)
The adjacency operator of an undirected
network is a self-adjoint operator, which
is bounded on l2 (V). We recall that l2
is the Hilbert space of square summable
sequences with inner product, and that
an operator is self-adjoint if its matrix is
equal to its own conjugate transpose, that
is, it is Hermitian. It is worth pointing
out here that the adjacency operator of a
directed network might not be self-adjoint.
The matrix representation of this operator
is the adjacency matrix A, which, for a sim-
ple graph, is deÔ¨Åned as
Aij =
{ 1
if i, j ‚ààE
0
otherwise.
(4.4)
A third operator related to the previous
two, which plays a fundamental role in the
applications of graph theory in physics is
the Laplacian operator. This operator is
deÔ¨Åned by
L(G)f = ‚àí‚àá‚ãÖ(‚àáf ) ,
(4.5)
and it is the graph version of the Laplacian
operator
Œîf = ùúï2f
ùúïx12 + ùúï2f
ùúïx22 + ¬∑ ¬∑ ¬∑ + ùúï2f
ùúïxn2 .
(4.6)
The negative sign in (4.5) is used by conven-
tion. Then the Laplacian operator acting on
the function f previously deÔ¨Åned is given by
(
L(G)f
)
(u) =
‚àë
{u,v}‚ààE
[
f (u) ‚àíf (v)
]
,
(4.7)

114
4 Graph and Network Theory
which in matrix form is given by
Luv(G) =
‚àë
e‚ààE
‚àáeu‚àáev =
‚éß
‚é™
‚é®
‚é™‚é©
‚àí1
ku
0
if uv ‚ààE,
if u = v,
otherwise.
(4.8)
Using the degree matrix K, which is a diago-
nal matrix of the degrees of the nodes in the
graph, the Laplacian and adjacency matri-
ces of a graph are related by
L = K ‚àíA.
(4.9)
4.2.2
General Graph Concepts
Other important general concepts of graph
theory that are fundamental for the study
of graphs and networks in physics are the
following. Two graphs G1 and G2 are iso-
morphic if there is a one-to-one correspon-
dence between the nodes of G1 and those
of G2, such as, the number of edges joining
each pair of nodes in G1 is equal to that join-
ing the corresponding pair of nodes in G2.
If the graphs are directed, the edges must
coincide not only in number but also in
direction. The graph S = (V ‚Ä≤, E‚Ä≤) is a sub-
graph of a graph G = (V, E) if, and only if,
V ‚Ä≤ ‚äÜV and E‚Ä≤ ‚äÜE. The clique is a partic-
ular kind of subgraph, which is a maximal
complete subgraph of a graph. A complete
graph is one in which every pair of nodes
are connected. A (directed) walk of length L
from v1 to vL+1 is any sequence of (not nec-
essarily diÔ¨Äerent) nodes v1, v2, ‚Ä¶ , vL, vL+1
such that for each i = 1, 2, ‚Ä¶ , L there is link
from vi to vi+1. A walk is closed (CW) if
vL+1 = v1. A particular kind of walk is the
path of length L, which is a walk of length
L in which all the nodes (and all the edges)
are distinct. A trial has all the links diÔ¨Äerent
but not necessarily all the nodes. A cycle is a
closed walk in which all the edges and all the
nodes (except the Ô¨Årst and last) are distinct.
The girth of the graph is the size (number of
nodes) of the minimum cycle in the graph.
A graph is connected if there is a path
between any pair of nodes in the graph.
Otherwise, it is disconnected. Every con-
nected subgraph is a connected component
of the graph. The analogous concept in a
directed graph is that of a strongly con-
nected graph. A directed graph is strongly
connected if there is a directed path
between each pair of nodes. The strongly
connected
components
of
a
directed
graph are its maximal strongly connected
subgraphs.
In an undirected graph, the shortest-path
distance d (p, q) = dpq is the number of
edges in the shortest path between the
nodes p and q in the graph. If p and q
are in diÔ¨Äerent connected components of
the graph the distance between them is
set to inÔ¨Ånite, d (p, q) ‚à∂= ‚àû. In a directed
graph, it is typical to consider the directed
distance ‚Éód (p, q) between a pair of nodes p
and q as the length of the directed shortest
path from p to q. However, in general
‚Éód (p, q) ‚â†‚Éód (q, p), which violates the sym-
metry property of a metric, so that ‚Éód (p, q)
is not a distance but a pseudo-distance or
a pseudo-metric. The distance between all
pairs of nodes in a graph can be arranged in
a distance matrix D, which, for undirected
graphs, is a square symmetric matrix. The
maximum entry for a given row/column
of the distance matrix of an undirected
(strongly connected directed) graph is
known as the eccentricity e (p) of the node
p, e (p) = max
x‚ààV(G) {d (p, x)}. The maximum
eccentricity among the nodes of a graph
is the diameter of the graph, which is
diam(G) = max
x,y‚ààV(G)
{d (x, y)}. The average
path length l of a graph with n nodes is
l =
1
n (n ‚àí1)
‚àë
x,y
d (x, y).
(4.10)

4.3 Graphs in Condensed Matter Physics
115
An important measure for the study of
networks was introduced by Watts and
Strogatz [8] as a way of quantifying how
clustered a node is. For a given node i,
the clustering coeÔ¨Écient is the number of
triangles connected to this node ||C3(i)||
divided by the number of triples centered
on it
Ci =
2 ||C3(i)||
ki
(
ki ‚àí1
),
(4.11)
where ki is the degree of the node. The
average value of the clustering for all nodes
in a network C
C = 1
n
n
‚àë
i=1
Ci
(4.12)
has been extensively used in the analysis of
complex networks (see Section 4.9 of this
chapter).
A second clustering coeÔ¨Écient has been
introduced as a global characterization of
network cliquishness [9]. This index, which
is also known as network transitivity, is
deÔ¨Åned as the ratio of three times the num-
ber of triangles divided by the number of
connected triples (two paths):
C = 3 ||C3||
||P2||
.
(4.13)
4.2.3
Types of Graphs
The simplest type of graph is the tree. A tree
of n nodes is a graph that is connected and
has no cycles. The simplest tree is the path
Pn. The path (also known as linear path
or chain) is the tree of n nodes, n ‚àí2 of
which have degree 2 and two nodes have
degree 1. For any kind of graph, we can
Ô¨Ånd a spanning tree, which is a subgraph
of this graph that includes every node and
is a tree. A forest is a disconnected graph
in which every connected component is a
tree. A spanning forest is a subgraph of the
graph that includes every node and is a
forest.
An r-regular graph is a graph with rn‚àï2
edges in which all nodes have degree r.
A particular case of a regular graph is
the complete graph previously deÔ¨Åned.
Another type of regular graph is the cycle,
which is a regular graph of degree 2, that
is, a 2-regular graph, denoted by Cn. The
complement of a graph G is the graph G
with the same set of nodes as G but two
nodes in G are connected if, and only if,
they are not connected in G. An empty or
trivial graph is a graph with no links. It is
denoted as Kn as it is the complement of
the complete graph.
A graph is bipartite if its nodes can be
split into two disjoint (nonempty) subsets
V1 ‚äÇV (V1 ‚â†ùúô) and V2 ‚äÇV (V2 ‚â†ùúô) and
V1 ‚à™V2 = V, such that each edge joins a
node in V1 and a node in V2. Bipartite
graphs do not contain cycles of odd length.
If all nodes in V1 are connected to all nodes
in V2 the graph is known as a complete
bipartite graph, denoted by Kn1,n2, where
n1 = ||V1|| and n2 = ||V2|| are the number of
nodes in V1 and V2, respectively. Finally, a
graph is planar if it can be drawn in a plane
in such a way that no two edges intersect
except at a node with which they are both
incident.
4.3
Graphs in Condensed Matter Physics
4.3.1
Tight-Binding Models
In condensed matter physics, it is usual to
describe solid-state and molecular systems
by considering the interaction between N
electrons whose behavior is determined by

116
4 Graph and Network Theory
a Hamiltonian of the following form:
H=
N
‚àë
n=1
[
‚àí
‚Ñè2‚àá2
n
2m +U(rn
)+ 1
2
‚àë
m‚â†n
V (rn‚àírm
)
]
,
(4.14)
where U (rn
) is an external potential and
V (rn ‚àírm
) is the potential describing the
interactions between electrons. Using the
second quantization formalism of quantum
mechanics, this Hamiltonian can be written
as
ÃÇH = ‚àí
‚àë
ij
tijÃÇc‚Ä†
i ÃÇcj + 1
2
‚àë
ijkl
VijklÃÇc‚Ä†
i ÃÇc‚Ä†
kÃÇclÃÇcj, (4.15)
where ÃÇc‚Ä†
i and ÃÇci are ‚Äúladder operators,‚Äù tij
and Vijkl are integrals that control the hop-
ping of an electron from one site to another
and the interaction between electrons,
respectively. They are usually calculated
directly from Ô¨Ånite basis sets [10].
In the tight-binding approach for study-
ing solids and certain classes of molecules,
the
interaction
between
electrons
is
neglected
and
Vijkl = 0, ‚àÄi, j, k, l.
This
method, which is known as the H√ºckel
molecular orbital method in chemistry,
can be seen as very drastic in its approx-
imation, but let us think of the physical
picture behind it [11, 12]. We concentrate
our discussion on alternant conjugated
molecules in which single and double
bonds alternate. Consider a molecule like
benzene in which every carbon atom has an
sp2 hybridization. The frontal overlapping
sp2 ‚àísp2 of adjacent carbon atoms creates
very stable ≈ì-bonds, while the lateral
overlapping p‚Äìp between adjacent carbon
atoms creates very labile √ü-bonds. Thus it
is clear from the reactivity of this molecule
that a ≈ì ‚àí√ü separation is plausible and we
can consider that our basis set consists of
orbitals centered on the particular carbon
atoms in such a way that there is only one
orbital per spin state at each site. Then we
can write the Hamiltonian of the system
as
ÃÇHtb = ‚àí
‚àë
ij
tijÃÇc‚Ä†
iùúåÃÇciùúå,
(4.16)
where ÃÇc(‚Ä†)
iùúåcreates (annihilates) an electron
with spin ùúåin a √ü (or other) orbital centered
at the atom i. We can now separate the in-
site energy ùõºi from the transfer energy ùõΩij
and write the Hamiltonian as
ÃÇHtb =
‚àë
ij
ùõºiÃÇc‚Ä†
iùúåÃÇciùúå+
‚àë
‚ü®ij‚ü©ùúå
ùõΩijÃÇc‚Ä†
iùúåÃÇciùúå,
(4.17)
where the second sum is carried out over all
pairs of nearest neighbors. Consequently,
in a molecule or solid with N atoms the
Hamiltonian equation (4.16) is reduced to
an N √ó N matrix,
Hij =
‚éß
‚é™
‚é®
‚é™‚é©
ùõºi
if
i = j
ùõΩij
if
i isconnectedto j
0
otherwise.
(4.18)
Owing to the homogeneous geometrical
and electronic conÔ¨Åguration of many sys-
tems analyzed by this method, we may
take ùõºi = ùõº, ‚àÄi (Fermi energy) and ùõΩij = ùõΩ‚âà
‚àí2.70eV for all pairs of connected atoms.
Thus,
H = ùõºI + ùõΩA,
(4.19)
where I is the identity matrix and A is the
adjacency matrix of the graph representing
the carbon skeleton of the molecule. The
Hamiltonian and the adjacency matrix of
the graph have the same eigenfunctions ùúëj
and their eigenvalues are simply related by
Hùúëj =EjA, Aùúëj = ùúÜjHEj = ùõº+ ùõΩùúÜj.
(4.20)
Hence, everything we have to do in the
analysis of the electronic structure of
molecules or solids that can be represented
by a tight-binding Hamiltonian is to study

4.3 Graphs in Condensed Matter Physics
117
the spectra of the graphs associated with
them. The study of spectral properties of
graphs represents an entire area of research
in algebraic graph theory. The spectrum
of a matrix is the set of eigenvalues of the
matrix together with their multiplicities.
For the case of the adjacency matrix, let
ùúÜ1(A) ‚â•ùúÜ2(A) ‚â•¬∑ ¬∑ ¬∑ ‚â•ùúÜn(A) be the dis-
tinct eigenvalues of A and let m(ùúÜ1(A)),
m(ùúÜ2(A)), ‚Ä¶ , m(ùúÜn(A)) be their algebraic
multiplicities, that is, the number of times
each of them appears as an eigenvalue
of A. Then the spectrum of A can be
written as
SpA =
(
ùúÜ1 (A)
ùúÜ2 (A)
¬∑ ¬∑ ¬∑
ùúÜn (A)
m (ùúÜ1 (A)
)
m (ùúÜ2 (A)
)
¬∑ ¬∑ ¬∑
m (ùúÜn (A)
)
)
.
(4.21)
The total √ü (molecular) energy is given by
E = ùõºne + ùõΩ
n
‚àë
j=1
gjùúÜj,
(4.22)
where ne is the number of √ü-electrons in
the molecule and gj is the occupation num-
ber of the jth molecular orbital. For neutral
conjugated systems in their ground state,
we have [13]
E=
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™‚é©
2
n‚àï2
‚àë
j=1
ùúÜj
n even,
2
(n+1)‚àï2
‚àë
j=1
ùúÜj + ùúÜ(j+1)‚àï2
n odd.
(4.23)
Because an alternant conjugated hydro-
carbon has a bipartite molecular graph,
ùúÜj = ‚àíùúÜn‚àíj+1 for all j = 1, 2, ‚Ä¶ , n. In a few
molecular systems, the spectrum of the
adjacency matrix is known. For instance
[11], we have
1. Polyenes CnHn+2
ùúÜj (A) = 2 cos
( ùúãj
n + 1
)
,
j = 1, ‚Ä¶ , n,
(4.24)
2. Cyclic polyenes CnHn
ùúÜj (A) = 2 cos
(2ùúãj
n
)
,
j = 1, ‚Ä¶ , n, ùúÜj = ùúÜn‚àíj
(4.25)
3. Polyacenes
N = 1
N = 2
N = 3
ùúÜr (A) = 1; ùúÜs (A) = ‚àí1;
ùúÜk (A) = ¬±1
2
{
1 ¬± 9 + 8 cos
kùúã
N + 1
}
,
k = 1, ‚Ä¶ , N
(4.26)
A few bounds exist for the total energy
of systems represented by graphs with n
vertices and m edges. For instance,
‚àö
2m + n (n ‚àí1) (det A)n‚àï2 ‚â§E ‚â§
‚àö
mn
(4.27)
and if G is a bipartite graph with n vertices
and m edges, then
E ‚â§4m
n +
‚àö
(n ‚àí2)
(
2m ‚àí8m2
n2
)
. (4.28)
4.3.1.1
Nullity and Zero-Energy States
Another characteristic of a graph that is
related to an important molecular property
is the nullity. The nullity of a graph, denoted
by ùúÇ= ùúÇ(G), is the algebraic multiplicity
of the zero eigenvalue in the spectrum of

118
4 Graph and Network Theory
the adjacency matrix of the graph [14].
This property is very relevant for the sta-
bility of alternant unsaturated conjugated
hydrocarbons. An alternant unsaturated
conjugated hydrocarbon with ùúÇ= 0 is
predicted to have a closed-shell electron
conÔ¨Åguration. Otherwise, the respective
molecule is predicted to have an open-shell
electron conÔ¨Åguration, that is, when ùúÇ> 0,
the molecule has unpaired electrons in
the form of radicals that are relevant for
several electronic and magnetic properties
of materials. In a molecule with an even
number of atoms, ùúÇis either zero or it is an
even positive integer.
The following are a few important facts
about the nullity of graphs. Let M = M(G)
be the size of the maximum matching of
a graph, that is, the maximum number of
mutually nonadjacent edges of G. Let T be
a tree with n ‚â•1 vertices. Then,
ùúÇ(T) = n ‚àí2M.
(4.29)
If G is a bipartite graph with n ‚â•1 vertices
and no cycle of length 4s (s = 1, 2, ‚Ä¶), then
ùúÇ(G) = n ‚àí2M.
(4.30)
Also for a bipartite graph G with incidence
matrix ‚àá, ùúÇ(G) = n ‚àí2r (‚àá), where r (‚àá) is
the rank of ‚àá= ‚àá(G). In the particular case
of benzenoid graphs Bz, which may contain
cycles of length 4s, the nullity is given by
ùúÇ(Bz) = n ‚àí2M.
(4.31)
The following are some known bounds for
the nullity of graphs [15]. Let G be a graph
with n vertices and at least one cycle,
ùúÇ(G)‚â§
{n‚àí2g(G) + 2g(G) ‚â°0
(mod 4),
n‚àí2g(G)
otherwise,
(4.32)
where g(G) is the girth of the graph.
If there is a path of length d (p, q) between
the vertices p and q of G
ùúÇ(G) ‚â§
{n ‚àíd(p, q)
if d (p, q) is even,
n ‚àíd (p, q) ‚àí1
otherwise.
(4.33)
Let G be a simple connected graph of diam-
eter D. Then
ùúÇ(G) ‚â§
{ n ‚àíD
if D is even,
n ‚àíD ‚àí1
otherwise.
(4.34)
4.3.2
Hubbard Model
Let us now consider one of the most
important models in theoretical physics:
the Hubbard model. This model accounts
for the quantum-mechanical motion of
electrons in a solid or conjugated hydro-
carbon and includes nonlinear repulsive
interactions between electrons. In brief,
the interest in this model is due to the fact
that it exhibits various interesting phenom-
ena including metal‚Äìinsulator transition,
antiferromagnetism, ferrimagnetism, fer-
romagnetism, Tomonaga‚ÄìLuttinger liquid,
and superconductivity [16].
The Hubbard model can be seen as an
extension of the tight-binding Hamiltonian
we have studied in the previous section in
which we introduce the electron‚Äìelectron
interactions. To keep things simple, we
allow onsite interactions only, that is, we
consider one orbital per site and Vijkl ‚â†0
in (4.15) if, and only if, i, j, k, and l all
refer to the same orbital. In this case, the
Hamiltonian is written as
H = ‚àít
‚àë
i,j,ùúé
AijÃÇc‚Ä†
iùúéÃÇcjùúé+ U
‚àë
i
ÃÇc‚Ä†
i‚ÜëÃÇci‚ÜëÃÇc‚Ä†
i‚ÜìÃÇci‚Üì,
(4.35)
where t is the hopping parameter and U >
0 indicates that the electrons repel each
other.

4.3 Graphs in Condensed Matter Physics
119
(a)
(b)
Figure 4.1
Representation of two graphene nanoÔ¨Çakes
with closed-shell (a) and open-shell (b) electronic
conÔ¨Ågurations.
Notice
that
if
there
is
no
elec-
tron‚Äìelectron
repulsion
(U = 0),
we
recover
the
tight-binding
Hamiltonian
studied in the previous section. Thus,
in that case, all the results given in the
previous section are valid for the Hub-
bard model without interactions. In the
case of nonhopping systems, t = 0 and
the Hamiltonian is reduced to the elec-
tron interaction part only. In this case,
the remaining Hamiltonian is already in
a diagonal form and the eigenstates can
be easily obtained. The main diÔ¨Éculty
arises when both terms are present in the
Hamiltonian. However, in half-Ô¨Ålled sys-
tems, the model has nice properties from
a mathematical point of view and a few
important results have been proved. These
systems have attracted a lot of attention
after the discovery of graphene. A system is
a half-Ô¨Ålled one if the number of electrons
is the same as the number of sites, that is,
because the total number of electrons can
be 2n, these systems have only a half of the
maximum number of electrons allowed.
This is particularly the case of graphene
and other conjugated aromatic systems.
Owing to the ≈ì ‚àí√ü separation that we
have seen in the previous section, these
systems can be considered as half-Ô¨Ålled,
in which each carbon atom provides one
√ü-electron.
A fundamental result in the theory of
half-Ô¨Ålled systems is the theorem proved by
Lieb [17]. Lieb‚Äôs theorem for repulsive Hub-
bard model states the following. Let G =
(V, E) be a bipartite connected graph repre-
senting a Hubbard model, such that |V| = n
is even and the nodes of the graph are parti-
tioned into two disjoint subsets V1 and V2.
We assume that the hopping parameters
are nonvanishing and that U > 0. Then the
ground states of the model are nondegen-
erate apart from the trivial spin degeneracy,
and have total spin Stot = ||V1| ‚àí|V2||‚àï2.
In order to illustrate the consequences
of Lieb‚Äôs theorem, let us consider two
benzenoid systems that can represent
graphene nanoÔ¨Çakes. The Ô¨Årst of them is
realized in the polycyclic aromatic hydro-
carbon known as pyrene and it is illustrated
in Figure 4.1a. The second is a hypothetical
graphene nanoÔ¨Çake known as triangulene
and is illustrated in Figure 4.1b. In both
cases, we have divided the bipartite graphs
into two subsets, the one marked by empty
circles corresponds to the set V1 and the
unmarked nodes form the set V2. In the
structure of pyrene, we can easily check
that ||V1|| = ||V2|| = 8 so that the total spin
according to Lieb‚Äôs theorem is Stot = 0.
In addition, according to (4.31) given in
the previous section pyrene has no zero-
energy levels as its nullity is zero, that

120
4 Graph and Network Theory
is, ùúÇ(Bz) = 0. In this case, the mean-Ô¨Åeld
Hubbard model solution for this structure
reveals no magnetism.
In the case of triangulene, it can be seen
that ||V1|| = 12 and ||V2|| = 10, which gives
a total spin Stot = 1. In addition, the nul-
lity of this graph is equal to 2, indicat-
ing that it has two zero-energy states. The
result given by Lieb‚Äôs theorem indicates that
triangulene has a spin-triplet ground state,
which means that it has a magnetic moment
of 2ùúáB per molecule. Thus triangulene and
more √ü-extended analogs have intramolec-
ular ferromagnetic interactions owing to √ü-
spin topological structures. Analogs of this
molecule have been already obtained in the
laboratory [18].
4.4
Graphs in Statistical Physics
The connections between statistical physics
and graph theory are extensive and have
a long history. A survey on these connec-
tions was published in 1971 by Essam [19];
it mainly deals with the Ising model. In the
Ising model, we consider a set of particles
or ‚Äúspins,‚Äù which can be in one of two states.
The state of the ith particle is described by
the variable ùúéi which takes one of the two
values ¬±1. The connection with graph the-
ory comes from the calculation of the parti-
tion function of the model. In this chapter,
we consider that the best way of introduc-
ing this connection is through a general-
ization of the Ising model, the Potts model
[20, 21].
The Potts model is one of the most
important models in statistical physics. In
this model, we consider a graph G = (V, E)
with each node of which we associate a
spin. The spin can have one of q values. The
basic physical principle of the model is that
the energy between two interacting spins is
set to zero for identical spins and it is equal
to a constant if they are not. A remark-
able property of the Potts model is that for
q = 3, 4 it exhibits a continuous phase tran-
sition between high- and low-temperature
phases. In this case, the critical singu-
larities in thermodynamic functions are
diÔ¨Äerent from those obtained by using the
Ising model. The Potts model has found
innumerable
applications
in
statistical
physics, for example, in the theory of phase
transitions and critical phenomena, but
also outside this context in areas such
as magnetism, tumor migration, foam
behavior, and social sciences.
In the simplest formulation of the Potts
model with q states {1, 2, ‚Ä¶ , q}, the Hamil-
tonian of the system can have either of the
two following forms:
H1(ùúî) = ‚àíJ
‚àë
(i,j)‚ààE
ùõø(ùúéi, ùúéj
),
(4.36)
H2(ùúî) = J
‚àë
(i,j)‚ààE
[1 ‚àíùõø(ùúéi, ùúéj
)],
(4.37)
where ùúîis a conÔ¨Åguration of the graph, that
is, an assignment of a spin to each node
of G = (V, E); ùúéi is the spin at node i and
ùõøis the Kronecker symbol. The model is
called ferromagnetic if J > 0 and antiferro-
magnetic if J < 0. We notice here that the
Ising model with zero external Ô¨Åeld is a spe-
cial case with q = 2, so that the spins are +1
and ‚àí1.
The probability p (ùúî, ùõΩ) of Ô¨Ånding the
graph in a particular conÔ¨Åguration (state) ùúî
at a given temperature is obtained by con-
sidering a Boltzmann distribution and it is
given by
p (ùúî, ùõΩ) =
exp
(
‚àíùõΩHi (ùúî)
)
Zi(G)
,
(4.38)
where Zi(G) is the partition function for
a given Hamiltonian in the Potts model,
that is,

4.4 Graphs in Statistical Physics
121
1
1
1
ùúî1
ùúî2
ùúî3
1
0
0
0
0
0
0
1
1
0
1
1
ùúî4
ùúî5
ùúî6
0
0
1
1
1
1
1
0
1
Figure 4.2
Representation of spin conÔ¨Ågu-
rations in a cycle with four nodes.
Zi(G) =
‚àë
ùúî
exp
(
‚àíùõΩHi (ùúî)
),
(4.39)
where the sum is over all conÔ¨Ågurations
(states) and Hi may be either H1 or H2.
Here ùõΩ= (kBT)‚àí1, where T is the absolute
temperature of the system, and kB is the
Boltzmann constant.
For instance, let us consider all the dif-
ferent spin conÔ¨Ågurations for a cyclic graph
with n = 4 as given in Figure 4.2. It may be
noted that there are four equivalent con-
Ô¨Ågurations for ùúî2, ùúî4, and ùúî5 as well as
two equivalent conÔ¨Ågurations for ùúî3. The
Hamiltonians H1 (ùúî) for these conÔ¨Ågura-
tions are
H1(ùúî1) = ‚àí4J; H1(ùúî2) = ‚àí2J;
H1(ùúî3) = 0; H1(ùúî4) = ‚àí2J;
H1(ùúî5) = ‚àí2J; H1(ùúî6) = ‚àí4J.
Then, the partition function of the Potts
model for this graph is
Z1(G) = 12 exp (2ùõΩJ) + 2 exp (4ùõΩJ) + 2.
(4.40)
It is usual to set K = ùõΩJ. The probability of
Ô¨Ånding the graph in the conÔ¨Åguration ùúî2 is
p (ùúî2, ùõΩ) =
exp (2K)
12 exp (2K) + 2 exp (4K) + 2.
(4.41)
The important connection between the
Potts model and graph theory comes
through the equivalence of this physical
model and the graph-theoretic concept
of the Tutte polynomial, that is, the parti-
tion functions of the Potts model can be
obtained in the following form:
Z1 (G, q, ùõΩ) = qk(G)vn‚àík(G)T (G; x, y) ,
(4.42)
Z2 (G, q, ùõΩ) = exp (‚àímK) Z1 (G, q, ùõΩ) , (4.43)
where q is the number of spins in the sys-
tem, k(G) is the number of connected com-
ponents of the graph, v = exp (K) ‚àí1, n and
m are the number of nodes and edges in
the graph, respectively, and T (G; x, y) is
the Tutte polynomial, where x = (q + v) ‚àïv
and y = exp (K). Proofs of the relationship
between the Potts partition function and
the Tutte polynomial will not be considered
here and the interested reader is directed to
the literature to Ô¨Ånd the details [22].
Let us deÔ¨Åne the Tutte polynomial [23,
24]. First, we deÔ¨Åne the following graph
operations. The deletion of an edge e in
the graph G, represented by G ‚àíe, consists
of removing the corresponding edge with-
out changing the rest of the graph, that is,
the end nodes of the edge remain in the
graph. The other operation is the edge con-
traction denoted by G‚àïe, which consists in
gluing together the two end nodes of the

122
4 Graph and Network Theory
edge e and then removing e. Both oper-
ations, edge deletion and contraction, are
commutative, and the operations G ‚àíS and
G‚àïS, where S is a subset of edges, are well
deÔ¨Åned. We notice here that the graphs cre-
ated by these transformations are no longer
simple graphs, they are pseudographs that
may contain self-loops and multiple edges.
Let us also deÔ¨Åne the following types of
edges: a bridge is an edge whose removal
disconnects the graph. A (self) loop is an
edge having the two end points incident at
the same node. Let us denote by B and L the
sets of edges that are bridges or loops in the
graph.
Then the Tutte polynomial T (G; x, y)
is
deÔ¨Åned
by
the
following
recursive
formulae:
1. T(G; x, y) = T(G ‚àíe; x, y) + T(G‚àïe;
x, y) if e ‚àâB, L;
2. T(G; x, y) = xiyj if e ‚ààB, L,
where the exponents i and j represent the
number of bridges and self-loops in the
subgraph, respectively.
Using this deÔ¨Ånition, we can obtain
the Tutte polynomial for the cyclic graph
with four nodes C4, as illustrated in the
Figure 4.3, that is, the Tutte polynomial
for C4 is T (C4; x, y) = x3 + x2 + x + y. We
can substitute this expression into (4.42) to
obtain the partition function for the Potts
model of this graph,
Z1 (G; 2, ùõΩ) = 2k(G)vn‚àík(G)
[(q + v
v
)3
+
(q + v
v
)2
+
(q + v
v
)
+1+v
]
,
(4.44)
and so we obtain Z1(G; 2, ùõΩ) = 12 exp(2K)
+2 exp(4K) + 2.
The following is an important mathemat-
ical result related to the universality of the
Tutte polynomial [23, 24]. Let f (G) be a
function on graphs having the following
properties:
1. f (G) = 1 if |V| = 1 and |E| = 0
2. f (G) = af (G ‚àíe) + bf (G‚àïe) if e ‚àâB, L,
3. f (G ‚à™H) = f (G)f (H);
f (G ‚àóH) = f (G)f (H), where G ‚àóH
X3
X2
X
Y
Figure 4.3
Edge deletion and contrac-
tion in a cyclic graph with four nodes.

4.4 Graphs in Statistical Physics
123
means that G and H shares at most one
node.
Then f (G) is an evaluation of the Tutte
polynomial, meaning that it is equivalent
to the Tutte polynomial with some speciÔ¨Åc
values for the parameters, and takes the
form
f (G)=am‚àín+k(G)bn‚àík(G)T
(
G; f (K2)
b
, f (L)
a
)
,
(4.45)
where L is the graph consisting of a single
node with one loop attached, K2 is the com-
plete graph with two nodes.
More formally, the Tutte polynomial is
a generalized Tutte‚ÄìGr√∂thendieck (T-G for
short) invariant. To deÔ¨Åne the T-G invari-
ant, we need the following concepts. Let S
and S‚Ä≤ be two disjoint subsets of edges. A
minor of G is a graph H that is isomorphic
to (G ‚àíS) ‚àïS‚Ä≤. Let Œì be a class of graphs
such that if G is in Œì, then any minor of G
is also in the class. This class is known as
minor closed. A graph invariant is a func-
tion f on the class of all graphs such that if
G and H are isomorphic, then f (G) = f (H).
Then, a T-G invariant is a graph invariant f
from Œì to a commutative ring ‚Ñúwith unity,
such as the conditions (i)‚Äì(iii) above are
fulÔ¨Ålled. A graph invariant is a function f
on the class of all graphs such that f (G1) =
f (G2) whenever the graphs G1 and G2 are
isomorphic. For more details the reader is
referred to the specialized literature on this
topic.
Some interesting evaluations of the Tutte
polynomial are the following:
Let us now consider a proper coloring of
a graph G, which is an assignment of a color
to each node of G such that any two adja-
cent nodes have diÔ¨Äerent colors. The chro-
matic polynomial ùúí(G; q) of the graph G is
the number of ways in which q colors can be
assigned to the nodes of G such that no two
T(G; 1, 1)
Number of spanning trees of
the graph G
T(G; 2, 1)
Number of spanning forests
T(G; 1, 2)
Number of spanning con-
nected subgraphs
T(G; 2, 2)
2|E|
adjacent nodes have the same color. The fol-
lowing are two interesting characteristics of
the chromatic polynomial:
1. ùúí(G; q) = ùúí(G ‚àíe; q) ‚àíùúí(G‚àïe; q),
2. ùúí(G; q) = qn for the trivial graph on n
nodes.
Thus, the chromatic polynomial fulÔ¨Ålls
the same contraction/deletion rules as the
Tutte polynomial. Indeed, the chromatic
polynomial is an evaluation of the Tutte
polynomial,
ùúí(G; q) = qk(G) (‚àí1)n‚àík(G) T (G; 1 ‚àíq, 0) .
(4.46)
To see the connection between the Potts
model and the chromatic polynomial, we
have to consider the Hamiltonian H1 (G; ùúî)
in the zero temperature limit, that is, T ‚Üí
0
(ùõΩ‚Üí‚àû). When ùõΩ‚Üí‚àû, the only spin
conÔ¨Ågurations that contribute to the parti-
tion function are the ones in which adjacent
spins have diÔ¨Äerent values. Then we have
that Z1 (G; q, ùõΩ) ‚Üí1 in the antiferromag-
netic model (J < 0). Thus, Z1 (G; q, ùõΩ‚Üí‚àû)
counts the number of proper colorings of
the graph using q colors. The partition func-
tion in the T = 0 limit of the Potts model is
given by the chromatic polynomial
Z1(G; q, ‚àí1) = ùúí(G)
= (‚àí1)k(G)(‚àí1)nT(G; 1 ‚àíq, 0).
(4.47)

124
4 Graph and Network Theory
4.5
Feynman Graphs
When
studying
elementary-particle
physics, the calculation of higher-order
corrections in perturbative quantum Ô¨Åeld
theory naturally leads to the evaluation
of Feynman integrals. Feynman integrals
are associated to Feynman graphs, which
are graphs G = (V, E) with n nodes and
m edges and some special characteristics
[25‚Äì27]. For instance, the edges play a
fundamental role in the Feynman graphs as
they represent the diÔ¨Äerent particles, such
as fermions (edges with arrows), photons
(wavy lines), gluons (curly lines). Scalar
particles are represented by simple lines.
Let us assign a D-dimensional momentum
vector qj and a number representing the
mass mj to the jth edge representing the
jth particle, where D is the dimension of
the space-time. In the theory of Feynman
graphs, the nodes with degree one are not
represented, leaving the edge without the
end node. This edge is named an external
edge (they are sometimes called legs). The
rest of edges are called internal. Also,
nodes of degree two are omitted as they
represent mass insertions. Thus, Feynman
graphs contain only nodes of degree k ‚â•3,
which represent the interaction of k parti-
cles. At each of these nodes, the sum of all
momenta Ô¨Çowing into the node equals that
of the momenta Ô¨Çowing out of it. As usual
the number of basic cycles, here termed
loops, is given by the cyclomatic number
l = m ‚àín + C, where C is the number of
connected components of the graph.
Here we will only consider Feynman
graphs with scalar propagators and we
refer to them as scalar theories. In scalar
theories,
the
D-dimensional
Feynman
integral has the form
IG =(ùúá2)ùúà‚àílD‚àï2
‚à´
l‚àè
r=1
dDkr
iùúãD‚àï2
n
‚àè
j=1
1
(
‚àíq2
j +m2
j
)ùúàj ,
(4.48)
where l is the number of loops (basic cycles)
in the Feynman diagram, ùúáis an arbitrary
scale parameter used to make the expres-
sions dimensionless, ùúàj is a positive integer
number that gives the power to which the
propagator occurs, ùúà= ùúà1 + ¬∑ ¬∑ ¬∑ + ùúàm, kr is
the independent loop momentum, mj is the
mass of the jth particle, and
qj =
l‚àë
j=1
ùúåijkj +
m
‚àë
j=1
ùúéijpj,
ùúåij, ùúéij ‚àà{‚àí1, 0, 1} ,
(4.49)
represents the momenta Ô¨Çowing through
the internal lines.
The correspondence between the Feyn-
man integral and the Feynman graph is as
follows. An internal edge represents a prop-
agator of the form
i
q2
j ‚àím2
j
,
(4.50)
whereby abusing of the notation q2
j rep-
resents the inner product of the momen-
tum vector with itself, that is, q2
j = qj ‚ãÖqT
j .
Notice that this is a relativistic propagator
that represents a Greens function for inte-
grations over space and time.
Nodes and external edges have weights
equal to one. For each internal momentum
not constrained by momentum conserva-
tion there is also an integration associated.
Now, in order to compute the integral
equation (4.48), we need to assign a (real
or complex) variable xj to each internal
edge, which are known as the Feynman
parameters. Then, we need to use the
Feynman parameter trick for each prop-
agator and evaluate the integrals over the
loop momenta k1, ‚Ä¶ , kl. As a consequence,
we obtain

4.5 Feynman Graphs
125
IG = Œì (ùúà‚àílD‚àï2)
‚àèm
j=1 Œì (ùúàj
) ‚à´xj‚â•0
( m
‚àè
j=1
dxjx
ùúàj‚àí1
j
)
√ó ùõø
(
1 ‚àí
m
‚àë
i=1
xi
)
Uùúà‚àí(l+1)D‚àï2
Fùúà‚àílD‚àï2
.
(4.51)
The real connection with the theory of
graphs comes from the two terms U and
F, which are graph polynomials, known as
the Ô¨Årst and second Symanzik polynomials
(sometimes
called
KirchhoÔ¨Ä‚ÄìSymanzik
polynomials). We will now specify some
methods for obtaining these polynomials
in Feynman graphs.
4.5.1
Symanzik Polynomials and Spanning Trees
The Ô¨Årst Symanzik polynomial can be
obtained by considering all spanning trees
in the Feynman graph. Let ùúè1 be the set of
spanning trees in the Feynman graph G.
Then,
U =
‚àë
T‚ààùúè1
‚àè
ej‚àâT
xj,
(4.52)
where T is a spanning tree and xj is
the Feynman parameter associated with
edge ej.
In order to obtain the second Symanzik
polynomial F, we have to consider the set of
spanning 2-forest ùúè2 in the Feynman graph.
A spanning 2-forest is a spanning forest
formed by only two trees. Then, the ele-
ments of ùúè2 are denoted by (Ti, Tj
). The sec-
ond Symanzik polynomial is given by
F = F0 + U
m
‚àë
i=1
xi
m2
i
ùúá2 .
(4.53)
The term F0 is a polynomial obtained
from the sets of spanning 2-forests of
G in the following way: let PTi be the
set of external momenta attached to the
spanning tree Ti, which is part of the
spanning 2-forest (Ti, Tj). Let pk ‚ãÖpr be
the Minkowski scalar product of the two
momenta vectors associated with the edges
ek and er, respectively. Then
F0 =
‚àë
(Ti,Tj)‚ààùúè2
‚éõ
‚éú
‚éú‚éù
‚àè
ek‚àâ(Ti,Tj)
xk
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
‚àë
pk‚ààPTi
‚àë
pr‚ààPTj
pk ‚ãÖpr
ùúá2
‚éû
‚éü
‚éü‚é†
.
(4.54)
Let us now show how to obtain the
Symanzik polynomials for the simple Feyn-
man graph illustrated in the Figure 4.4. For
the sake of simplicity, we take all internal
masses to be zero.
We Ô¨Årst obtain all spanning trees of this
graph, which are given in Figure 4.5.
Hence, the Ô¨Årst Symanzik polynomial is
obtained as follows:
U = x1x2 + x1x3 + x1x5 + x2x4 + x2x5
+ x3x4 + x3x5 + x4x5
= (x1 + x4)(x2 + x3)
+ (x1 + x2 + x3 + x4)x5.
(4.55)
Now, for the second Symanzik polynomial,
we obtain all the spanning 2-forests of the
graph, which are given in Figure 4.6.
We should notice that the terms x1x2x5
and x3x4x5 do not contribute to F0 because
the momentum sum Ô¨Çowing through all cut
edges is zero. Thus, we can obtain F0 as
follows:
x4
v3
x3
v2
x5
x2
v4
x1
v1
Figure 4.4
Illustration of a Feynman graph with four nodes, Ô¨Åve inter-
nal, and two external edges. The Feynman parameters are represented
by xi on each internal edge.

126
4 Graph and Network Theory
x1x2
x1x3
x1x5
x2x4
x2x5
x3x4
x3x5
x4x5
Figure 4.5
Spanning trees of the
Feynman graphs represented in
Figure 4.4.
F = F0 = (x1x2x3 + x1x2x4 + x1x3x4 + x1x3x5
+ x1x4x5 + x2x3x4 + x2x3x5
+ x2x4x5)
(‚àíp2
ùúá2
)
= [(x1+x2)(x3 + x4)x5 + x1x4(x2 + x3)
+x2x3(x1 + x4)]
(‚àíp2
ùúá2
)
.
(4.56)
L =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
x1 + x2 + x5
‚àíx2
‚àíx5
‚àíx1
‚àíx2
x2 + x3
‚àíx3
0
‚àíx5
‚àíx3
x3 + x4 + x5
‚àíx4
‚àíx1
0
‚àíx4
x1 + x4
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
(4.57)
4.5.2
Symanzik Polynomials and the Laplacian
Matrix
Another graph-theoretic way of obtaining
the Symanzik polynomials is through the
use of the Laplacian matrix. The Laplacian
matrix for the Feynman graphs is deÔ¨Åned as
usual for any weighted graph. For instance,
for the Feynman graph given in Figure 4.4,
the Laplacian matrix is

4.5 Feynman Graphs
127
x1x2x3
x1x2x5
x1x3x5
x2x3x4
x2x4x5
x1x2x4
x1x3x4
x1x4x5
x2x3x5
x3x4x5
Figure 4.6
Spanning 2-forest of
the Feynman graph represented in
Figure 4.4.
Then, we can deÔ¨Åne the auxiliary polyno-
mial K = det L [i], where L [i] denotes the
minor of the Laplacian matrix obtained by
removing the ith row and column of L. This
polynomial is known as the KirchhoÔ¨Äpoly-
nomial of the graph and it is easy to see that
it can be deÔ¨Åned by
K =
‚àë
T‚ààùúè1
‚àè
ej‚ààT
xj.
(4.58)
For instance,
K = det L[1]
=
|||||||
x2 + x3
‚àíx3
0
‚àíx3
x3 + x4 + x5
‚àíx4
0
‚àíx4
x1 + x4
|||||||
= x1x2x3 + x1x2x4 + x1x2x5 + x1x3x4
+ x1x3x5 + x2x3x4 + x2x4x5
+ x3x4x5.
(4.59)
We transform the KirchhoÔ¨Äpolynomial
into the Ô¨Årst Symanzik polynomial by set-
ting U = x1 ¬∑ ¬∑ ¬∑ xmK (x‚àí1
1 , ‚Ä¶ , x‚àí1
m
) , that is,

128
4 Graph and Network Theory
U = x1x2x3x4x5
x1x2x3
+ x1x2x3x4x5
x1x2x4
+ ¬∑ ¬∑ ¬∑
+ x1x2x3x4x5
x3x4x5
= x1x2 + x1x3 + x1x5 + x2x4 + x2x5
+ x3x4 + x3x5 + x4x5
= (x1+x4)(x2+x3) + (x1+x2+x3+x4)x5.
(4.60)
To calculate the second Symanzik polyno-
mial using the Laplacian matrix, we have to
introduce some modiÔ¨Åcations. First assign
a new parameter zj to each of the external
edges of the Feynman graph. Now, build a
diagonal matrix whose diagonal are Dii =
‚àë
j‚Üíi zj, that is the ith diagonal entry of D
represents the sum of the parameters zj for
all the external edges incident with the node
i. Modify the Laplacian matrix as follows:
ÃÉL = L + D. The modiÔ¨Åed Laplacian matrix
ÃÉL is the minor of a Laplacian matrix con-
structed for a modiÔ¨Åcation of the Feynman
graph in which all rows and columns corre-
sponding to the external edges are removed
[26, 27]. The determinant of the modiÔ¨Åed
Laplacian matrix is
W = det ÃÉL,
(4.61)
and let us expand it in a series of polyno-
mials homogeneous in the variables zj, such
that
W = W (0) + W (1) + W (2) + ¬∑ ¬∑ ¬∑ + W (t),
(4.62)
where t is the number of external edges.
Then the Symanzik polynomials are
U = x1 ¬∑ ¬∑ ¬∑ xmW (1)
j
(x‚àí1
1 , ‚Ä¶ , x‚àí1
m ) for any j,
and
F0 =x1¬∑ ¬∑ ¬∑xm
‚àë
(j,k)
(pj ‚ãÖpk
ùúá2
)
W (2)
(j,k)
(x‚àí1
1 , ‚Ä¶ , x‚àí1
m
).
(4.63)
For the Feynman graph given in the previ-
ously analyzed example, we have
ÃÉL =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
x1 + x2 + x5
‚àíx2
‚àíx5
‚àíx1
‚àíx2
x2 + x3 + z1
‚àíx3
0
‚àíx5
‚àíx3
x3 + x4 + x5
‚àíx4
‚àíx1
0
‚àíx4
x1 + x4 + z2
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
(4.64)
and W = det ÃÉL = W (1) + W (2), where
W (1) = (z1 + z2)(x1x2x3 + x1x2x4 + x1x3x4
+ x2x3x4 + x1x2x5 + x1x3x5
+ x2x4x5 + x3x4x5),
(4.65)
W (2) = z1z2(x1x3 + x2x3 + x1x4 + x2x4
+ x1x5 + x2x5 + x3x5 + x4x5).
(4.66)
With this information, the Ô¨Årst and sec-
ond Symanzik polynomials can be easily
obtained.
4.5.3
Symanzik Polynomials and Edge
Deletion/Contraction
The Symanzik polynomials can also be
obtained through the graph transforma-
tions used to deÔ¨Åne the Tutte polynomial,
that is, the Symanzik polynomials obey
the rules for edge deletion and contraction
operations that we encountered in the
previous section. Recall that the deletion of
an edge e in the graph G is represented by
G ‚àíe, and the edge contraction denoted by
G‚àïe, and that B and L are the sets of edges

4.6 Graphs and Electrical Networks
129
that are bridges or loops in the graph (see
Section 4.4). Then
U(G) = U
(
G
ej
)
+ xjU (G ‚àíej
) ,
(4.67)
F0(G) = F0
(
G
ej
)
+ xjF0
(G ‚àíej
) ,
(4.68)
for any ej ‚àâB, L.
Finally, let us mention that there exist
factorization theorems for the Symanzik
polynomials that are based on a beautiful
theorem due to Dodgson [28]. The reader is
reminded that Charles L. Dodgson is better
known as Lewis Carroll who has delighted
many generations with his Alice in Wonder-
land. These factorization theorems are not
given here and the reader is directed to the
excellent reviews of Bogner and Weinzierl
for details [26, 27].
4.6
Graphs and Electrical Networks
The relation between electrical networks
and graphs is very natural and is docu-
mented in many introductory texts on
graph theory. The idea is that a simple
electrical network can be represented as
a graph G = (V, E) in which we place a
Ô¨Åxed electrical resistor at each edge of the
graph. Therefore, these networks can also
be called resistor networks. Let us suppose
that we connect a battery across the nodes
u and v. There are several parameters of
an electrical network that can be consid-
ered in terms of graph-theoretic concepts
but we concentrate here on one that has
important connections with other param-
eters of relevance in physics, namely, the
eÔ¨Äective resistance [29]. Let us calculate
the eÔ¨Äective resistance Œ© (u, v) between
two nodes by using the KirchhoÔ¨Äand
Ohm laws. For the sake of simplicity, we
always consider here resistors of 1 ohm.
In the simple case of a tree, the eÔ¨Äec-
tive resistance is simply the sum of the
resistances along the path connecting u
and v, that is, for a tree, Œ© (u, v) = d (u, v),
where d (u, v) is the shortest-path distance
between the corresponding nodes (number
of links in the shortest path connecting
both nodes). However, in the case of two
nodes connected by multiple routes, the
eÔ¨Äective resistance Œ© (u, v) can be obtained
by using KirchhoÔ¨Ä‚Äôs laws. A characteristic
of the eÔ¨Äective resistance Œ© (u, v) is that it
decreases with the increase of the number
of routes connecting u and v. Thus, in
general, Œ© (u, v) ‚â§d (u, v).
An important result about the eÔ¨Äec-
tive resistance was obtained by Klein and
Randi¬¥c [30]: the eÔ¨Äective resistance is a
proper distance between the pairs of nodes
of a graph, that is,
1. Œ© (u, v) ‚â•0 for all u ‚ààV(G), v ‚ààV(G).
2. Œ© (u, v) = 0 if and only if u = v.
3. Œ© (u, v) = Œ© (v, u) for all
u ‚ààV(G), v ‚ààV(G).
4. Œ© (u, w) ‚â§Œ© (u, v) + Œ© (v, w) for all
u ‚ààV(G), v ‚ààV(G), w ‚ààV(G).
The resistance distance Œ© (u, v) between
a pair of nodes u and v in a connected
component of a network can be calculated
by using the Moore‚ÄìPenrose generalized
inverse L+ of the graph Laplacian L:
Œ© (u, v) = L+ (u, u) + L+ (v, v) ‚àí2L+ (u, v) ,
(4.69)
for u ‚â†v.
Another way of computing the resis-
tance distance for a pair of nodes in a
network is as follows. Let L (G ‚àíu) be the
matrix resulting from removing the uth
row and column of the Laplacian and let
L (G ‚àíu ‚àív) be the matrix resulting from
removing both the uth and vth rows and

130
4 Graph and Network Theory
columns of L. Then it has been proved [31]
that
Œ© (u, v) = det L (G ‚àíu ‚àív)
det L (G ‚àíu)
,
(4.70)
Notice that det L (G ‚àíu) is the KirchhoÔ¨Ä
(Symanzik) polynomial we discussed in the
previous section. Yet another way for com-
puting the resistance distance between a
pair of nodes in the network is given on the
basis of the Laplacian spectra [32]
Œ© (u, v) =
n
‚àë
k=2
1
ùúák
[
Uk (u) ‚àíUk (v)
]2 , (4.71)
where Uk (u) is the uth entry of the kth
orthonormal eigenvector associated to the
Laplacian eigenvalue ùúák, written in the
ordering 0 = ùúá1 < ùúá2 ‚â§¬∑ ¬∑ ¬∑ ‚â§ùúán.
The resistance distance between all pairs
of nodes in the network can be represented
in the resistance matrix ùõÄof the network.
This matrix can be written as
ùõÄ= |1‚ü©diag
{[
L +
( 1
n
)
J
]‚àí1}T
+ diag
[
L +
( 1
n
)
J
]‚àí1
‚ü®1|
‚àí2
(
L +
( 1
n
)
J
)‚àí1
,
(4.72)
where J = |1‚ü©‚ü®1| is a matrix having all
entries equal to 1.
For the case of connected networks, the
resistance distance matrix can be related to
the Moore‚ÄìPenrose inverse of the Lapla-
cian as shown by Gutman and Xiao [33]:
L+ = ‚àí1
2
[
ùõÄ‚àí1
n (ùõÄJ + JùõÄ) + 1
n2 JùõÄJ
]
,
(4.73)
where J is as above.
The resistance distance matrix is a
matrix of squared Euclidean distances. A
matrix M ‚àà‚Ñùn√ón is said to be Euclidean
if there is a set of vectors x1, ‚Ä¶ , xn such
that Mij = ‚Äñxi ‚àíxj‚Äñ2. Because it is easy to
construct vectors such that Œ©ij = ‚Äñxi ‚àíxj‚Äñ2
the resistance distance matrix is squared
Euclidean
and
the
resistance
distance
satisÔ¨Åes the weak triangle inequality
Œ©1‚àï2
ik
‚â§Œ©1‚àï2
ij
+ Œ©1‚àï2
jk ,
(4.74)
for every pair of nodes in the network.
As we noted in the introduction to
this section, eÔ¨Äective resistance has con-
nections with other concepts that are of
relevance in the applications of mathemat-
ics in physics. One of these connections
is between the resistance distance and
Markov chains. In particular, the resistance
distance is proportional to the expected
commute time between two nodes for a
Markov chain deÔ¨Åned by a weighted graph
[29, 34]. If wuv be the weight of the edge
{u, v}, the probability of transition between
u and v in the Markov chain deÔ¨Åned on the
graph is
Puv =
wuv
‚àë
u,v‚ààE
wuv
.
(4.75)
The commuting time is the time taken by
‚Äúinformation‚Äù starting at node u to return
to it after passing through node v. The
expected commuting time ÃÇCuv is related to
the resistance distance [29, 34] by
ÃÇCuv = 2 (1Tw) Œ© (u, v) ,
(4.76)
where 1 is vector of 1‚Äôs and w is the vector
of link weights. Note that if the network is
unweighted ÃÇCuv = 2mŒ© (u, v).
4.7
Graphs and Vibrations
In this section, we develop some connec-
tions between vibrational analysis, which is
important in many areas of physics ranging
from classical to quantum mechanics, and

4.7 Graphs and Vibrations
131
the spectral theory of graphs. Here we
consider the one-dimensional case with
a graph G = (V, E) in which every node
represents a ball of mass m and every edge
represents a spring with the spring constant
mùúî2 connecting two balls. The ball‚Äìspring
network is assumed to be submerged in
a thermal bath at temperature T. The
balls in the graph oscillate under thermal
excitation. For the sake of simplicity, we
assume that there is no damping and no
external forces are applied to the system.
Let xi, i = 1, 2, ‚Ä¶ , n be the coordinates of
each node; this measures the displacement
of the ball i from its equilibrium state
xi = 0. For a complete guide to the results
to be presented here, the reader is directed
to Estrada et al. [35].
4.7.1
Graph Vibrational Hamiltonians
Let us start with a Hamiltonian of the oscil-
lator network in the form
HA =
‚àë
i
[
p2
i
2m + (K ‚àíki
) mùúî2x2
i
2
]
+ mùúî2
2
‚àë
i, j
(i < j)
Aij
(xi ‚àíxj
)2,
(4.77)
where ki is the degree of the node i and
K is a constant satisfying K ‚â•maxi ki. The
second term in the right-hand side is the
potential energy of the springs connecting
the balls, because xi ‚àíxj is the extension
or the contraction of the spring connect-
ing the nodes i and j. The Ô¨Årst term in
the Ô¨Årst set of square parentheses is the
kinetic energy of the ball i, whereas the sec-
ond term in the Ô¨Årst set of square paren-
theses is a term that avoids the movement
of the network as a whole by tying the
network to the ground. We add this term
because we are only interested in small
oscillations around the equilibrium; this
will be explained below again.
The Hamiltonian equation (4.77) can be
rewritten as
HA =
‚àë
i
(
p2
i
2m + Kmùúî2
2
x2
i
)
‚àímùúî2
2
‚àë
i,j
xiAijxj.
(4.78)
Let us next consider the Hamiltonian of the
oscillator network in the form
HL =
‚àë
i
p2
i
2m+mùúî2
2
Aij
(xi ‚àíxj
)2
(4.79)
instead of the Hamiltonian HA in (4.78).
Because the Hamiltonian HL lacks the
springs that tie the whole network to the
ground (the second term in the Ô¨Årst set
of parentheses in the right-hand side of
(4.78), this network can undesirably move
as a whole. We will deal with this motion
shortly.
The
expansion
of
the
Hamiltonian
equation (4.79) as in (4.77) and (4.78) now
gives
HL =
‚àë
i
p2
i
2m + mùúî2
2
‚àë
i,j
xiLijxj,
(4.80)
where Lij denotes an element of the net-
work Laplacian L.
4.7.2
Network of Classical Oscillators
We start by considering a network of clas-
sical harmonic oscillators with the Hamil-
tonian HA. Here the momenta pi and the
coordinates xi are independent variables, so
that the integration of the factor
‚àè
exp
[
‚àíùõΩ
(
p2
i
2m
)]
(4.81)

132
4 Graph and Network Theory
over the momenta {pi
} reduces to a con-
stant term, which does not aÔ¨Äect the inte-
gration over {xi
}. As a consequence, we do
not have to consider the kinetic energy and
we can write the Hamiltonian in the form
HA = mùúî2
2
xT (KI ‚àíA) x,
(4.82)
where x = (x1, x2, ‚Ä¶ , xn)T and I is the n √ó n
identity matrix.
The partition function is given by
Z = ‚à´e‚àíùõΩHA ‚àè
i
dxi
= ‚à´dx exp
(
‚àíùõΩmùúî2
2
xT (KI ‚àíA) x
)
,
(4.83)
where the integral is an n-fold one and can
be evaluated by diagonalizing the matrix A.
The adjacency matrix can be diagonalized
by means of an orthogonal matrix O as in
ùö≤= O (KI ‚àíA) OT,
(4.84)
where ùö≤is the diagonal matrix with eigen-
values ùúÜùúáof (KI ‚àíA) on the diagonal. Let us
consider that K is suÔ¨Éciently large, so that
we can make all eigenvalues ùúÜùúápositive. By
deÔ¨Åning a new set of variables yùúáby y = Ox
and x = OTy, we can transform the Hamil-
tonian equation (4.82) to the form
HA = mùúî2
2
yTùö≤y
=
mùúî2
0
2
‚àë
ùúá
y2
ùúá+ mùúî2
2
‚àë
ùúá
ùúÜùúáy2
ùúá.
(4.85)
Then the integration measure of the n-
fold integration in (4.83) is transformed as
‚àè
i
dxi = ‚àè
ùúá
dyùúá, because the Jacobian of the
orthogonal matrix O is unity. Therefore, the
multifold integration in the partition func-
tion (4.83) is decoupled to give
Z =
‚àè
ùúá
‚àö
2ùúã
ùõΩmùúî2ùúÜùúá
,
(4.86)
which can be rewritten in terms of the adja-
cency matrix as
Z =
(
2ùúã
ùõΩmùúî2
)n‚àï2
1
‚àö
det (KI ‚àíA)
.
(4.87)
As we have made all the eigenvalues
of (KI ‚àíA) positive, its determinant is
positive.
Now we deÔ¨Åne an important quantity,
the mean displacement of a node from its
equilibrium position. It is given by
‚ü®
x2
p
‚ü©
= 1
Z ‚à´x2
pe‚àíùõΩHA ‚àè
i
dxi,
(4.88)
which, by using the spectral decomposition
of A, yields
‚ü®
x2
p
‚ü©
= 1
Z ‚à´
[
‚àë
ùúé
(
OT)
pùúéyùúé
]2
e‚àíùõΩHA ‚àè
ùúá
dyùúá
(4.89)
In the integrand, the odd functions with
respect to yùúávanish. Therefore, only the
terms of y2
ùúésurvive after integration in the
expansion of the square parentheses in the
integrand. This gives
‚ü®
x2
p
‚ü©
= 1
Z
‚àë
ùúé
O2
ùúép‚à´y2
ùúéexp
(
‚àíùõΩmùúî2
2
ùúÜùúéy2
ùúé
)
dyùúé
√ó
‚àè
ùúá(‚â†ùúé)
[
‚à´exp
(
‚àíùõΩmùúî2
2
ùúÜùúáy2
ùúá
)
dyùúá
]
.
(4.90)
Comparing this expression with (4.86), we
have
‚ü®
x2
p
‚ü©
=
1
ùõΩmKùúî2
[(
I ‚àíA
K
)‚àí1]
pp
.
(4.91)

4.7 Graphs and Vibrations
133
The mean node displacement may be given
by the thermal Green‚Äôs function in the
framework of classical mechanics by
‚ü®
x2
p
‚ü©
=
1
ùõΩKmùúî2
[(
I ‚àíA
K
)‚àí1]
pq
.
(4.92)
This represents a correlation between the
node displacements in a network due to
small thermal Ô¨Çuctuations.
The same calculation using the Hamilto-
nian equation (4.80) gives
‚ü®
x2
p
‚ü©‚Ä≤
=
1
ùõΩmùúî2
(L+)
pq ,
(4.93)
where L+ is the Moore‚ÄìPenrose general-
ized inverse of the Laplacian.
4.7.3
Network of Quantum Oscillators
Here we consider the quantum-mechanical
version of the Hamiltonian HA in (4.78) by
considering that the momenta pj and the
coordinates xi are not independent vari-
ables. In this case, they are operators that
satisfy the commutation relation,
[xi, pj
] = i‚Ñèùõøij.
(4.94)
We use the boson creation and annihilation
operators a‚Ä†
i and ai, which allow us to write
the coordinates and momenta as
xi =
‚àö
‚Ñè
2mŒ©
(a‚Ä†
i + ai
) ,
(4.95)
pi =
‚àö
‚Ñè
2mŒ©
(a‚Ä†
i ‚àíai
) ,
(4.96)
where Œ© =
‚àö
K‚àïmùúî. The commutation
relation (4.94) yields
[
ai, a‚Ä†
j
]
= ùõøij.
(4.97)
With the use of these operators, we can
recast the Hamiltonian equation (4.78) into
the form
HA =
‚àë
i
‚ÑèŒ©
(
a‚Ä†
i ai + 1
2
)
‚àí‚Ñèùúî2
4Œ©
‚àë
i,j
(a‚Ä†
i +ai
)Aij
(
a‚Ä†
j +aj
)
. (4.98)
Using the spectral decomposition of the
adjacency matrix, we generate a new set of
boson creation and annihilation operators
given by
bùúá=
‚àë
i
Oùúáiai =
‚àë
i
ai
(OT)
iùúá,
(4.99)
b‚Ä†
ùúá=
‚àë
i
Oùúáia‚Ä†
i =
‚àë
i
a‚Ä†
i
(OT)
iùúá,
(4.100)
Applying the transformations ((4.99) and
(4.100)) to the Hamiltonian equation (4.98),
we can decouple it as
HA =
‚àë
ùúá
Hùúá,
(4.101)
with
Hùúá= ‚ÑèŒ©
[
1 + ùúî2
2Œ©2
(ùúÜùúá‚àíK)] (
b‚Ä†
ùúábùúá+ 1
2
)
+ ‚Ñèùúî2
4Œ©
(ùúÜùúá‚àíK) [(
b‚Ä†
ùúá
)2
+ (bùúá
)2]
.
(4.102)
In order to go further, we now introduce
an approximation in which each mode of
oscillation does not get excited beyond
the Ô¨Årst excited state. In other words, we
restrict ourselves to the space spanned by
the ground state (the vacuum) |vac‚ü©and the
Ô¨Årst excited states b‚Ä†
ùúá|vac‚ü©. Then the second
term of the Hamiltonian equation (4.102)
does not contribute and we therefore have
Hùúá= ‚ÑèŒ©
[
1 + ùúî2
2Œ©2
(ùúÜùúá‚àíK)] (
b‚Ä†
ùúábùúá+ 1
2
)
(4.103)

134
4 Graph and Network Theory
within this approximation. This approxima-
tion is justiÔ¨Åed when the energy level spac-
ing ‚ÑèŒ© is much greater than the energy
scale of external disturbances, (speciÔ¨Åcally
the temperature Ô¨Çuctuation kBT = 1‚àïùõΩ, in
assuming the physical metaphor that the
system is submerged in a thermal bath at
the temperature T), as well as than the
energy of the network springs ‚Ñèùúî, that is,
ùõΩ‚ÑèŒ© ‚â´1 and Œ© ‚â´ùúî. This happens when
the mass of each oscillator is small, when
the springs connecting to the ground mŒ©2
are strong, and when the network springs
mùúî2 are weak. Then an oscillation of tiny
amplitude propagates over the network. We
are going to work in this limit hereafter.
We are now in a position to com-
pute the partition function as well as
the thermal Green‚Äôs function quantum-
mechanically. As stated above, we consider
only the ground state and one excitation
from it. Therefore we have the quantum-
mechanical
partition
function
in
the
form
ZA = ‚ü®vac| e‚àíùõΩHA |vac‚ü©
=
‚àè
ùúá
exp
{
‚àíùõΩ‚ÑèŒ©
2
[
1+ ùúî2
2Œ©2
(ùúÜùúá‚àíK)]}
.
(4.104)
The diagonal thermal Green‚Äôs function giv-
ing the mean node displacement in the
quantum-mechanical framework is given
by
‚ü®
x2
p
‚ü©
= 1
Z ‚ü®vac| ape‚àíùõΩHAa‚Ä†
p |vac‚ü©,
(4.105)
which indicates how much an excitation
at the node p propagates throughout the
graph before coming back to the same node
and being annihilated. Let us compute the
quantity equation (4.105) by
‚ü®
x2
p
‚ü©
= e‚àíùõΩ‚ÑèŒ©
(
exp
[ùõΩ‚Ñèùúî2
2Œ© A
])
pp
, (4.106)
where we have used (4.84). Similarly, we can
compute the oÔ¨Ä-diagonal thermal Green‚Äôs
function as
‚ü®
xp, xq
‚ü©
= e‚àíùõΩ‚ÑèŒ©
(
exp
[ùõΩ‚Ñèùúî2
2Œ© A
])
pq
.
(4.107)
The same quantum-mechanical calculation
by using the Hamiltonian HL in (4.79) gives
‚ü®xp, xq
‚ü©= 1 + lim
Œ©‚Üí0 O2pO2q exp
[
‚àíùõΩ‚Ñèùúî2
2Œ© ùúá2
]
,
(4.108)
where ùúá2 is the second eigenvalue of the
Laplacian matrix.
4.8
Random Graphs
The study of random graphs is one of the
most important areas of theoretical graph
theory. Random graphs have found mul-
tiple applications in physics and they are
used today as a standard null model in sim-
ulating many physical processes on graphs
and networks. There are several ways of
deÔ¨Åning a random graph, that is, a graph
in which, given a set of nodes, the edges
connecting them are selected in a random
way. The simplest model of random graph
was introduced by Erd√∂s and R√©nyi [36].
The construction of a random graph in
this model starts by considering n isolated
nodes. Then, with probability p > 0, a pair
of nodes is connected by an edge. Conse-
quently, the graph is determined only by
the number of nodes and edges such that
it can be written as G (n, m) or G (n, p). In
Figure 4.7, we illustrate some examples of
Erd√∂s‚ÄìR√©nyi (ER) random graphs with the
same number of nodes and diÔ¨Äerent linking
probabilities.
A few properties of ER random graphs
are summarized as follows.

4.8 Random Graphs
135
Figure 4.7
Illustration of the changes of an Erd√∂s‚ÄìR√©nyi
random network with 20 nodes and probabilities that
increases from zero (left) to one (right).
1. The expected number of edges per
node:
m = n (n ‚àí1) p
2
.
(4.109)
2. The expected node degree:
k = (n ‚àí1) p.
3. The average path length for large n:
l (H) = ln n ‚àíùõæ
ln (pn) + 1
2,
(4.110)
where ùõæ‚âà0.577 is the Euler‚Äì
Mascheroni constant.
4. The average clustering coeÔ¨Écient (see
(4.12)):
C = p = ùõø(G).
(4.111)
5. When increasing p, most nodes tends
to be clustered in one giant component,
while the rest of nodes are isolated in
very small components (see Figure 4.8).
6. The structure of GER (n, p) changes as a
function of p = k‚àï(n ‚àí1) giving rise to
the following three stages (see
Figure 4.9):
a
Subcritical k < 1, where all
components are simple and very
small. The size of the largest
component is S = O (ln n).
b
Critical k = 1, where the size of
the largest component is
S = Œò (n2‚àï3).
c
Supercritical k > 1, where the
probability that
(f ‚àíùúÄ) n < S < (f + ùúÄ) n is 1
when n ‚Üí‚àû
ùúÄ> 0, where
f = f (k) is the positive solution of
the equation: e‚àíkf = 1 ‚àíf . The rest
of the components are very small,
with the second largest having size
about ln n.
7. The largest eigenvalue of the adjacency
matrix in an ER network grows
proportionally to n [37]:
limn‚Üí‚àû
(ùúÜ1 (A) ‚àïn) = p.
8. The second largest eigenvalue grows
more slowly than ùúÜ1:
limn‚Üí‚àû
(ùúÜ2 (A) ‚àïnùúÄ) = 0 for every
ùúÄ> 0.5.
9. The smallest eigenvalue also grows with
a similar relation to ùúÜ2 (A):
limn‚Üí‚àû
(ùúÜn (A) ‚àïnùúÄ) = 0 for every
ùúÄ> 0.5.
10. The spectral density of an ER random
network follows Wigner‚Äôs semicircle law
[38], which is simply written as (see
Figure 4.10):

136
4 Graph and Network Theory
0
Size giant component
p
1
Figure 4.8
Change of the size of the giant connected
component in an ER random graph as probability is
increased.
p = 0.0075
p = 0.01
p = 0.025
Figure 4.9
Examples of the diÔ¨Äerent stages of the change
of an ER random graph with the increase in probability:
subcritical (a), critical (b), and supercritical (c).
0.6
0.5
0.4
0.3
œÅ(Œª).r
Œª/r
0.2
0.1
0.0
0
1
2
3
‚àí1
‚àí2
‚àí3
Figure 4.10
Illustration of the Wigner semicircle law for
the spectral density of an ER random graph.

4.9 Introducing Complex Networks
137
ùúå(ùúÜ) =
‚éß
‚é™
‚é®
‚é™‚é©
‚àö
4‚àíùúÜ2
2ùúã
‚àí2 ‚â§ùúÜ
r ‚â§2, r=
‚àö
np(1‚àíp)
0
otherwise.
(4.112)
4.9
Introducing Complex Networks
In the rest of this chapter,we are going
to study the so-called complex networks.
Complex networks can be considered
as the skeleton of complex systems in a
variety of scenarios ranging from social
and ecological to biological and techno-
logical systems. Their study has become
a major Ô¨Åeld of interdisciplinary research
in the twenty-Ô¨Årst century with an impor-
tant participation of physicists who have
contributed signiÔ¨Åcantly by creating new
models and adapting others known in
physics to the study of the topologi-
cal and dynamical properties of these
networks. A number of universal topolog-
ical properties that explain some of the
dynamical and functional properties of
networks have been introduced, such as
‚Äúsmall-world‚Äù and ‚Äúscale-free‚Äù phenomena;
these will be analyzed brieÔ¨Çy in the next
sections.
There is much confusion about what a
complex network is. To start with we should
attempt a clariÔ¨Åcation about what a com-
plex system is. There is no clear-cut deÔ¨Å-
nition of a complex system. First, it must
be clear that the concept of complexity is
a twofold one: it may refer to a quality of
the system or to a quantitative concept. In
the Ô¨Årst case, complexity is what makes the
system complex. In the second, it is a con-
tinuum embracing both the simple and the
complex according to a given measure of
complexity. Standish [39] has stressed that
as a quality ‚Äúcomplexity of a system refers
to the presence of emergence in the system,
or the exhibition of behaviour not speci-
Ô¨Åed in the system speciÔ¨Åcation.‚Äù In other
words, complex systems ‚Äúdisplay organiza-
tion without any external organizing prin-
ciple being applied‚Äù [40]. When we speak
of complexity as a quantity, it ‚Äúrefers to the
amount of information needed to specify the
system.‚Äù
Then, what is a complex network? Before
attempting to answer this question let us
try to make a classiÔ¨Åcation of some of the
systems represented by networks (see [41])
by considering the nature of the links they
represent. The following are some examples
of these classes.
‚Ä¢ Physical linking. Pairs of nodes are
physically connected by a tangible link,
such as a cable, a road, a vein, and so on.
Examples are the Internet, urban street
networks, road networks, vascular
networks, and so on.
‚Ä¢ Physical interactions. The links
between pairs of nodes represent
interactions that are determined by a
physical force. Examples are protein
residue networks, protein‚Äìprotein
interaction networks, and so on.
‚Ä¢ ‚ÄúEthereal‚Äù connections. The links
between pairs of nodes are intangible,
such that information sent from one
node is received at another irrespective
of the ‚Äúphysical‚Äù trajectory. Examples are
WWW, airports network.
‚Ä¢ Geographic closeness. Nodes represent
regions of a surface and their
connections are determined by their
geographic proximity. Examples
arecountries in a map, landscape
networks, and so on.
‚Ä¢ Mass/energy exchange. The links
connecting pairs of nodes indicate that
some energy or mass has been
transferred from one node to another.

138
4 Graph and Network Theory
Examples are reaction networks,
metabolic networks, food webs, trade
networks, and so on.
‚Ä¢ Social connections. The links represent
any kind of social relationship between
nodes. Examples are friendship,
collaboration, and so on.
‚Ä¢ Conceptual linking. The links indicate
conceptual relationships between pairs
of nodes. Examples are dictionaries,
citation networks, and so on.
Now, let us try to characterize the com-
plexity of these networks by giving the min-
imum amount of information needed to
describe them. For the sake of comparison,
let us also consider a regular and a ran-
dom graph of the same size as that of the
real-world networks we want to describe.
For the case of a regular graph, we only
need to specify the number of nodes and
the degree of the nodes (recall that every
node has the same degree). With this infor-
mation, many non-isomorphic graphs can
be constructed, but many of their topologi-
cal and combinatorial properties are deter-
mined by the information provided. In the
case of the random network, we need to
specify the number of nodes and the prob-
ability for joining pairs of nodes. As we
have seen in the previous section, most
of the structural properties of these net-
works are determined by this information.
In contrast, to describe the structure of
one of the networks representing a real-
world system, we need an awful amount of
information, such as number of nodes and
links, degree distribution, degree‚Äìdegree
correlation, diameter, clustering, presence
of communities, patterns of communicabil-
ity, and other properties that we will study
in this section. However, even in this case,
a complete description of the system is still
far away. Thus, the network representa-
tion of these systems deserves the title of
complex networks because their topological
structures cannot be trivially described as
in the cases of random or regular graphs.
In closing, when referring to complex net-
works, we are making an implicit allusion
to the topological or structural complexity
of the graphs representing a complex sys-
tem. We will consider some general topo-
logical and dynamical properties of these
networks in the following sections and the
reader is recommended to consult the Fur-
ther Reading section at the end of this
chapter for more details and examples of
applications.
4.10
Small-World Networks
One of the most popular concepts in net-
work theory is that of the ‚Äúsmall-world.‚Äù In
practically every language and culture, we
have a phrase saying that the world is small
enough so that a randomly chose person
has a connection with some of our friends.
The empirical grounds for this ‚Äúconcept‚Äù
come from an experiment carried out by
Stanley Milgram in 1967 [42]. Milgram
asked some randomly selected people in
the US cities of Omaha (Nebraska) and
Wichita (Kansas) to send a letter to a
target person who lives in Boston (Mas-
sachusetts) on the East Coast. The rules
stipulate that the letter should be sent to
somebody the sender knows personally.
Despite the senders and the target being
separated by about 2000 km, the results
obtained by Milgram were surprising for
the following reasons:
1. The average number of steps needed
for the letters to arrive to its target was
around six.
2. There was a large group inbreeding,
which resulted in acquaintances of one
individual feedback into his/her own

4.11 Degree Distributions
139
circle, thus usually eliminating new
contacts.
The assumption that the underlying
social network is a random one with char-
acteristics such as the ER network fails to
explain these Ô¨Åndings. We already know
that an ER random network displays a very
small average path length, but it fails in
reproducing the large group inbreeding
observed because the number of triangles
and the clustering coeÔ¨Écient in the ER
network are very small. In 1998, Watts
and Strogatz [8] proposed a model that
reproduces the two properties mentioned
in a simple way. Let n be the number
of nodes and k be an even number, the
Watt‚ÄìStrogatz model starts by using the
following construction. Place all nodes
in a circle and connect every node to its
Ô¨Årst k‚àï2 clockwise nearest neighbors as
well as to its k‚àï2 counterclockwise nearest
neighbors (see Figure 4.11). This will create
a ring, which, for k > 2, is full of triangles
and consequently has a large clustering
coeÔ¨Écient. The average clustering coeÔ¨É-
cient for these networks is given by Barrat
and Weigt [43]:
C = 3 (k ‚àí2)
4 (k ‚àí1),
(4.113)
which means that C = 0.75 for very large
values of k.
As can be seen in Figure 4.11 (top left),
the shortest path distance between any pair
of nodes that are opposite to each other in
the network is relatively large. This distance
is, in fact, equal to
‚åà
n
k
‚åâ
. Then
l ‚âà(n ‚àí1) (n + k ‚àí1)
2kn
.
(4.114)
This relatively large average path length is
far from that of the Milgram experiment. In
order to produce a model with small aver-
age path length and still having relatively
large clustering, Watts and Strogatz con-
sider a probability for rewiring the links in
that ring. This rewiring makes the average
path length decrease very fast, while the
clustering coeÔ¨Écient still remains high. In
Figure 4.12, we illustrate what happens to
the clustering and average path length as
the rewiring probability changes from 0 to
1 in a network.
4.11
Degree Distributions
One of the network characteristics that has
received much attention in the literature
is the statistical distribution of the node
degrees. Let p (k) = n (k) ‚àïn, where n (k)
is the number of nodes having degree k
in a network of size n. That is, p (k) repre-
sents the probability that a node selected
0
1
Rewiring probability p
Figure 4.11
Schematic representation of the evolution of
the rewiring process in the Watts‚ÄìStrogatz model.

140
4 Graph and Network Theory
0.0
10‚àí4
10‚àí3
10‚àí2
10‚àí1
100
Rewiring probability, p
Lp/L0
Cp/C0
0.2
0.4
0.6
0.8
1.0
Clustering, path length
Figure 4.12
Schematic representation of the variation in
the average path length and clustering coeÔ¨Écient with the
change of the rewiring probability in the Watts‚ÄìStrogatz
model.
0.06
0.0
0.2
0.4
0.6
0.8
0.05
0.04
0.03
p(k)
p(k)
p(k)‚àº k‚àíŒ≥
0.02
0.01
1.0
0.00
0
(a)
(b)
20
40
60
80
100
0
20
40
60
80
100
p(k) = e‚àík K‚àík
‚Äì ‚Äì
K!
k
k
Figure 4.13
(a,b) Illustration of the Poisson and power-law
degree distributions found in complex networks.
uniformly at random has degree k. The
histogram of p (k) versus k represents the
degree distribution for the network. There
are hundreds of statistical distributions in
which the node degrees of a network can
Ô¨Åt. A typical distribution that is expected
for a random network of the type of ER
is the Poisson distribution. However, a
remarkable characteristic of complex net-
works is that many of them display some
kind of ‚Äúfat-tailed‚Äù degree distributions.
In these distributions, a few nodes appear
with very large degree, while most of the
nodes have relatively small degrees. The
prototypical example of these distributions
is the power-law one, which is illustrated
in the Figure 4.13, but others such as log
normal, Burr, logGamma, Pareto, and so
on [44] fall in the same category.
In
the
case
of
power-law
distribu-
tions (see Figure 4.13a), the probability of
Ô¨Ånding a node with degree k decays as a

4.11 Degree Distributions
141
negative power of the degree: p (k) ‚àºk‚àíùõæ.
This means that the probability of Ô¨Ånding a
high-degree node is relatively small in com-
parison with the high probability of Ô¨Ånding
low-degree nodes. These networks are usu-
ally referred to as scale-free networks. The
term scaling describes the existence of a
power-law relationship between the prob-
ability and the node degree: p (k) = Ak‚àíùõæ.
Scaling the degree by a constant factor c
only produces a proportionate scaling of
the probability:
p (k, c) = A (ck)‚àíùõæ= Ac‚àíùõæ‚ãÖp (k) .
(4.115)
Power-law relations are usually represented
in a logarithmic scale, leading to a straight
line, ln p (k) = ‚àíùõæln k + ln A, where ‚àíùõæis
the slope and ln A the intercept of the func-
tion. Scaling by a constant factor c means
that only the intercept of the straight line
changes but the slope is exactly the same as
before: ln p (k, c) = ‚àíùõæln k ‚àíùõæAc.
Determining the degree distribution of
a network is a complicated task. Among
the diÔ¨Éculties, we can mention the fact
that sometimes the number of data points
used to Ô¨Åt the distribution is too small
and sometimes the data are very noisy. For
instance, in Ô¨Åtting power-law distributions,
the tail of the distribution, the part which
corresponds to high degrees, is usually
very noisy. There are two main approaches
in use for reducing this noise eÔ¨Äect in the
tail of probability distributions. One is
the binning procedure, which consists in
building a histogram using bin sizes that
increase exponentially with degree. The
other approach is to consider the cumu-
lative distribution function (CDF) [45].
The cumulative distributions of power-law
and Poisson distributions are given as
follows:
P (k) =
‚àû
‚àë
k‚Ä≤=k
p (k‚Ä≤),
(4.116)
P (k) = e‚àík
‚åäk‚åã
‚àë
i=1
(
k
)i
i!
,
(4.117)
which represent the probability of choos-
ing at random a node with degree greater
than or equal to k. In the case of power-
law degree distributions, P (k) also shows a
power-law decay with degree
P (k) ‚àº
‚àû
‚àë
k‚Ä≤=k
k‚Ä≤‚àíùõæ‚àºk‚àí(ùõæ‚àí1),
(4.118)
which means that we will also obtain a
straight line for the logarithmic plot of P (k)
versus k in scale-free networks.
4.11.1
‚ÄúScale-Free‚Äù Networks
Among the many possible degree dis-
tributions existing for a given network
the ‚Äúscale-free‚Äù one is one of the most
ubiquitously found distributions. Conse-
quently, it is important to study a model
that is able to produce random networks
with such kind of degree distribution, that
is, a model in which the probability of
Ô¨Ånding a node with degree k decreases
as a power-law of its degree. The most
popular of these models is the one intro-
duced by Barab√°si and Albert [46], which
is described below.
In the Barab√°si‚ÄìAlbert (BA) model, a
network is created by using the following
procedure. Start from a small number m0
of nodes. At each step, add a new node u
to the network and connect it to m ‚â§m0 of
the existing nodes v ‚ààV with probability
pu =
kv
‚àë
w
kw
.
(4.119)

142
4 Graph and Network Theory
We can assume that we start from a con-
nected random network of the ER type with
m0 nodes, GER = (V, E). In this case, the
BA process can be understood as a pro-
cess in which small inhomogeneities in the
degree distribution of the ER network grow
in time. Another option is the one devel-
oped by Bollob√°s and Riordan [47] in which
it is Ô¨Årst assumed that d = 1 and that the
ith node is attached to the jth one with
probability
pi =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™‚é©
kj
1+
i‚àí1
‚àë
j=0
kj
ifj < i
1
1+
i‚àí1
‚àë
j=0
kj
ifj = i
.
(4.120)
Then, for d > 1, the network grows as if
d = 1 until nd nodes have been created
and the size is reduced to n by contract-
ing groups of d consecutive nodes into
one. The network is now speciÔ¨Åed by
two parameters and we denote this by
BA (n, d). Multiple links and self-loops are
created during this process and they can
be simply eliminated if we need a simple
network.
A characteristic of BA networks is that
the probability that a node has degree k ‚â•d
is given by
p (k) =
2d (d ‚àí1)
k (k + 1) (k + 2) ‚àºk‚àí3,
(4.121)
which immediately implies that the cumu-
lative degree distribution is given by
P (k) ‚àºk‚àí2.
(4.122)
For Ô¨Åxed values d ‚â•1, Bollob√°s et al. [48]
have proved that the expected value for the
clustering coeÔ¨Écient C is given by
C ‚àºd ‚àí1
8
log2n
n
,
(4.123)
for n ‚Üí‚àû, which is very diÔ¨Äerent from the
value C ‚àºn‚àí0.75 reported by Barab√°si and
Albert [46] for d = 2.
On the other hand, the average path
length has been estimated for the BA
networks to be as follows [47]:
l = ln n ‚àíln (d‚àï2) ‚àí1 ‚àíùõæ
ln ln n + ln (d‚àï2)
+ 3
2,
(4.124)
where ùõæis the Euler‚ÄìMascheroni constant.
This means that for the same number of
nodes and average degree, BA networks
have smaller average path length than
their ER analogs. Other alternative models
for obtaining power-law degree distribu-
tions with diÔ¨Äerent exponents ùõæcan be
found in the literature [49]. In closing,
using this preferential attachment algo-
rithm, we can generate random networks
that are diÔ¨Äerent from those obtained by
using the ER method in many important
aspects including their degree distribu-
tions, average clustering, and average path
length.
4.12
Network Motifs
The concept of network motifs was intro-
duced by Milo et al. [50] in order to charac-
terize recurring, signiÔ¨Åcant patterns in real-
world networks [50, 51]. A network motif
is a subgraph that appears more frequently
in a real network than could be expected if
the network were built by a random pro-
cess. In order to measure the statistical sig-
niÔ¨Åcance of a given subgraph, the Z-score is
used, which is deÔ¨Åned as follows for a given
subgraph i:
Zi =
Nreal
i
‚àí‚ü®Nrandom
i
‚ü©
ùúérandom
i
,
(4.125)

4.13 Centrality Measures
143
where Nreal
i
is the number of times the
subgraph i appears in the real network,
‚ü®Nrandom
i
‚ü©and ùúérandom
i
are the average and
standard deviation of the number of times
that i appears in the ensemble of random
networks, respectively. Some of the motifs
found by Milo et al. [50] in diÔ¨Äerent real-
world directed networks are illustrated in
the Figure 4.14.
4.13
Centrality Measures
Node centrality in a network is one of the
many concepts that have been created in
the analysis of social networks and then
imported to the study of any kind of net-
worked system. Measures of centrality try
to capture the notion of ‚Äúimportance‚Äù of
nodes in networks by quantifying the ability
of a node to communicate directly with
other nodes, or its closeness to many other
nodes, or the number of pairs of nodes
that need a speciÔ¨Åc node as intermediary
in their communications. Here we describe
some of the most relevant centrality mea-
sures currently in use for studying complex
networks.
The degree of a node was deÔ¨Åned in the
Ô¨Årst section. It was Ô¨Årst considered as a cen-
trality measure for nodes in a network by
Freeman [52] as a way to account for imme-
diate eÔ¨Äects taking place in a network. The
degree centrality can be written as
ki =
n
‚àë
j=1
Aij.
(4.126)
In directed networks, we have two diÔ¨Äerent
kinds of degree centrality, namely, the in-
and out-degree of a node:
kin
i =
n
‚àë
j=1
Aij,
(4.127)
kout
j
=
n
‚àë
i=1
Aij.
(4.128)
Another type of centrality is the closeness
centrality, which measures how close a
node is from the rest of the nodes in the
network. The closeness centrality [52] is
expressed mathematically as follows:
CC (u) = n ‚àí1
s (u) ,
(4.129)
where the distance sum s (u) is
s (u) =
‚àë
v‚ààV(G)
d (u, v).
(4.130)
The betweenness centrality quantifying the
importance of a node is in the communi-
cation between other pairs of nodes in the
network [52]. It measures the proportion
of information that passes through a given
node in the communications between other
pairs of nodes in the network and it is
deÔ¨Åned as follows:
BC (k) =
‚àë
i
‚àë
j
ùúå
(
i, k, j
)
ùúå(i, j) ,
i ‚â†j ‚â†k,
(4.131)
where ùúå(i, j) is the number of shortest
paths from node i to node j, and ùúå
(
i, k, j
)
is the number of these shortest paths that
pass through node k in the network.
The Katz centrality index for a node in a
network is deÔ¨Åned as [53]
Ki =
{[(I ‚àíùúÇ‚àí1A)‚àí1 ‚àíI
]
1
}
i ,
(4.132)
where I is the identity matrix, ùúÇ‚â†ùúÜ1 is
an attenuation factor (ùúÜ1 is the principal
eigenvector of the adjacency matrix), and 1
is column vector of 1‚Äôs. This centrality index
can be considered as an extension of the
degree in order to consider the inÔ¨Çuence

144
4 Graph and Network Theory
Motif
Network 
Three chains
Food webs
Feedforward loop
Gene regulation (transcription)
 
Neurons
Electronic circuits
Three-node feedback loop
Electronic circuits
Uplinked mutual dyad
World Wide Web
Feedback with two mutual dyads
World Wide Web
Fully connected triad
World Wide Web
Bi parallel
Food webs
Electronic circuits
Neurons 
Four-node feedback loop
Electronic circuits 
Bi fan
Gene regulations (transcription)
 
 
Neurons 
Electronic circuits
Figure 4.14
Illustration of some of the motifs found in real-world networks.
not only of the nearest neighbors but also
of the most distant ones.
The Katz centrality index can be deÔ¨Åned
for directed networks:
Kout
i
=
{[(
I ‚àíùúÇ‚àí1A)‚àí1 ‚àíI
]
1
}
i ,
(4.133)
Kin
i =
{
1T [(
I ‚àíùúÇ‚àí1A)‚àí1 ‚àíI
]}
i .
(4.134)
The Kin
i is a measure of the ‚Äúprestige‚Äù of a
node as it accounts for the importance that
a node has due to other nodes that point
to it.

4.13 Centrality Measures
145
Another type of centrality that captures
the inÔ¨Çuence not only of nearest neighbors
but also of more distant nodes in a network
is the eigenvector centrality. This index was
introduced by Bonacich [54, 55] and is the
ith entry of the principal eigenvector of the
adjacency matrix
ùõó1 (i) =
(
1
ùúÜ1
Aùõó1
)
i
.
(4.135)
In directed networks, there are two types of
eigenvector centralities that can be deÔ¨Åned
by using the principal right and left eigen-
vectors of the adjacency matrix:
ùõóR
1(i) =
(
1
ùúÜ1
AùõóR
1
)
i
,
(4.136)
ùõóL
1(i) =
(
1
ùúÜ1
ATùõóL
1
)
i
.
(4.137)
Right eigenvector centrality accounts for
the ‚Äúimportance‚Äù of a node by taking into
account the ‚Äúimportance‚Äù of nodes to which
it points on, that is, it is an extension of the
out-degree concept by taking into account
not only nearest neighbors. On the other
hand, the left-eigenvector centrality mea-
sures the importance of a node by consid-
ering those nodes pointing toward the cor-
responding node and it is an extension of
the in-degree centrality. This is frequently
referred to as prestige in social sciences
contexts.
There is an important diÔ¨Éculty when
we try to apply right- and left-eigenvector
centralities to networks where there are
nodes having out-degree or in-degree equal
to zero, respectively. In the Ô¨Årst case, the
nodes pointing to a given node do not
receive any score for pointing to it. When
the in-degree is zero, the left-eigenvector
centrality or prestige of this node is equal to
zero as no node points to it, even though it
can be pointing to some important nodes.
A solution for this problem is obtained by
the following centrality measure.
The PageRank centrality measure is
the tool used by Google in order to rank
citations of web pages in the WWW [56].
Its main idea is that the importance of a
web page should be proportional to the
importance of other web pages pointing
to it. In other words, the PageRank of a
page is the sum of the PageRanks of all
pages pointing into it. Mathematically,
this intuition is captured by the following
deÔ¨Ånition. The PageRank is obtained by the
vector
(ùõëk+1)T =
(
ùõëk)T G.
(4.138)
The matrix G is deÔ¨Åned by
G = ùõºS +
(1 ‚àíùõº
n
)
11T,
(4.139)
where 0 ‚â§ùõº‚â§1 is a ‚Äúteleportation‚Äù param-
eter, which captures the eÔ¨Äect in which a
web surfer abandons his random approach
of bouncing from one page to another and
initiates a new search simply by typing a
new destination in the browser‚Äôs URL com-
mand line. The matrix S solves the problem
of dead-end nodes in ranking web pages,
and it is deÔ¨Åned as
S = H + a
[( 1
n
)
1T]
,
(4.140)
where the entries of the dangling vector a
are given by
ai =
{ 1
ifkout
i
= 0
0
otherwise.
(4.141)
Finally, the matrix H is deÔ¨Åned as a modi-
Ô¨Åed adjacency matrix for the network
Hij =
{
1
kout
i
if thre is a link from i to j
0
otherwise.

146
4 Graph and Network Theory
The matrix G is row stochastic, which
implies that its largest eigenvalue is equal to
one and the principal left-hand eigenvector
of G is given by
ùõëT = ùõëTG,
(4.142)
where ùõëT1 = 1 [56].
Another type of node centrality that is
based on the spectral properties of the
adjacency matrix of a graph is the subgraph
centrality. The subgraph centrality counts
the number of closed walks starting and
ending at a given node, which are mathe-
matically given by the diagonal entries of
Ak. In general terms, the subgraph central-
ity is a family of centrality measures deÔ¨Åned
on the basis of the following mathematical
expression:
fi (A) =
( ‚àû
‚àë
l=0
clAl
)
ii
,
(4.143)
where coeÔ¨Écients cl are selected such that
the inÔ¨Ånite series converges. One partic-
ularly useful weighting scheme is the fol-
lowing, which eventually converges to the
exponential of the adjacency matrix [57]:
EE (i) =
( ‚àû
‚àë
l=0
Al
l!
)
ii
= (eA)
ii .
(4.144)
We can also deÔ¨Åne subgraph centralities
that take into account only contributions
from odd or even closed walks in the
network:
EEodd (i) = (sinh A)ii ,
(4.145)
EEeven (i) = (cosh A)ii .
(4.146)
A characteristic of the subgraph centrality
in directed networks is that it accounts
for the participation of a node in directed
walks. This means that the subgraph cen-
trality of a node in a directed network
is EE (i) > 1 only if there is at least one
closed walk that starts and returns to this
node. In other cases, EE (i) = 1, that is, the
subgraph centrality in a directed network
measures the returnability of ‚Äúinformation‚Äù
to a given node.
4.14
Statistical Mechanics of Networks
Let us consider that every link of a network
is weighted by a parameter ùõΩ. Evidently,
the case ùõΩ= 1 corresponds to the simple
network. Let W be the adjacency matrix
of this homogeneously weighted network.
It is obvious that W = ùõΩA and the spec-
tral moments of the adjacency matrix are
Mr(W) = TrWr = ùõΩrTrAr = ùõΩrMr. Let us
now count the total number of closed walks
in this weighted network. It is straightfor-
ward to realize [58] that this is given by
Z (G; ùõΩ) = Tr
‚àû
‚àë
r=0
ùõΩrAr
r!
= TreùõΩA =
n
‚àë
j=1
eùõΩùúÜj.
(4.147)
Let us now consider that the parameter
ùõΩ= (kBT)‚àí1 is the inverse temperature of
a thermal bath in which the whole network
is submerged. Here the temperature is a
physical analogy for the external ‚Äústresses‚Äù
that a network is continuously exposed to.
For instance, let us consider the network
in which nodes represent corporations and
the links represent their business relation-
ships. In this case, the external stress can
represent the economical situation of the
world at the moment in which the net-
work is analyzed. In ‚Äúnormal‚Äù economical
situations we are in the presence of a low
level of external stress. In situations of eco-
nomical crisis, the level of external stress is
elevated.
We can consider the probability that the
network is in a conÔ¨Åguration (state) with

4.14 Statistical Mechanics of Networks
147
an energy given by the eigenvalue ùúÜj. The
conÔ¨Åguration or state of the network can
be considered here as provided by the cor-
responding eigenvector of the adjacency
matrix associated with ùúÜj. This probability
is then given by
pj =
eùõΩùúÜj
‚àë
j eùõΩùúÜj =
eùõΩùúÜj
Z (G; ùõΩ),
(4.148)
which identiÔ¨Åes the normalization factor
as the partition function of the network.
This index, introduced by Estrada [59], is
known in the graph theory literature as the
Estrada index of the graph/network and
usually denoted by EE(G).
We can deÔ¨Åne the entropy for the net-
work,
S (G; ùõΩ) = ‚àíkB
‚àë[pj
(ùõΩùúÜj ‚àíln Z)],
(4.149)
where we wrote Z (G; ùõΩ) = Z for the sake of
economy.
The total energy H(G) and Helmholtz
free energy F(G) of the network, respec-
tively [58], are given by
H(G, ùõΩ) = ‚àí1
EE
n
‚àë
j=1
(ùúÜjeùõΩùúÜj)
= ‚àí1
EETr (AeùõΩA)
= ‚àí
n
‚àë
j=1
ùúÜjpj,
(4.150)
F(G, ùõΩ) = ‚àíùõΩ‚àí1 ln EE.
(4.151)
Known bounds for the physical parameters
deÔ¨Åned above, are the following:
0 ‚â§S (G, ùõΩ) ‚â§ùõΩln n,
(4.152)
‚àíùõΩ(n ‚àí1) ‚â§H (G, ùõΩ) ‚â§0,
(4.153)
‚àíùõΩ(n ‚àí1) ‚â§F(G, ) ‚â§‚àíùõΩln n,
(4.154)
where the lower bounds are obtained for
the complete graph as n ‚Üí‚àûand the upper
bounds are reached for the null graph with
n nodes [58].
Next, let us analyze the thermodynamic
functions of networks for extreme values of
the temperature. At very low temperatures,
the total energy and Helmholtz free energy
are reduced to the interaction energy of the
network
[58]:
H (G, ùõΩ‚Üí‚àû) = F(G, ùõΩ‚Üí
‚àû) = ‚àíùúÜ1. At very high temperatures,
ùõΩ‚Üí0, the entropy of the system is com-
pletely determined by the partition func-
tion of the network, S (G; ùõΩ‚Üí0) = kB ln Z
and F(G, ùõΩ‚Üí0) ‚Üí‚àí‚àû.
The introduction of these statistical
mechanics parameters allows the study of
interesting topological and combinatorial
properties of networks by using a well-
understood physical paradigm. For Ô¨Ånding
examples of these applications, the reader
is referred to the specialized literature [41].
4.14.1
Communicability in Networks
The concept of network communicability is
a very recent one. However, it has found
applications in many diÔ¨Äerent areas of net-
work theory [35]. This concept captures
the idea of correlation in a physical system
and translates it into the context of net-
work theory. We deÔ¨Åne here that the com-
municability between a pair of nodes in a
network depends on all routes that con-
nect these two nodes [35]. Among all these
routes, the shortest path is the one mak-
ing the most important contribution as it
is the most ‚Äúeconomic‚Äù way of connecting
two nodes in a network. Thus we can use
the weighted sum of all walks of diÔ¨Äerent

148
4 Graph and Network Theory
lengths between a pair of nodes as a mea-
sure of their communicability, that is,
Gpq =
‚àû
‚àë
k=0
(Ak)
pq
k!
= (eA)
pq,
(4.155)
where eA is the matrix exponential func-
tion. We can express the communicability
function for a pair of nodes in a network
by using the eigenvalues and eigenvectors
of the adjacency matrix:
Grs =
n
‚àë
j=1
ùúëj (r) ùúëj (s) eùúÜj.
(4.156)
By using the concept of inverse temperature
introduced above, we can also express the
communicability function in terms of this
parameter [60]
Grs (ùõΩ) =
‚àû
‚àë
k=0
(ùõΩAk)
rs
k!
= (eùõΩA)
rs.
(4.157)
Intuitively, the communicability between
the two nodes connected by a path should
tend to zero as the length of the path tends
to inÔ¨Ånity. In order to show that this is
exactly the case, we can write the expres-
sion for Grs (ùõΩ) for the path Pn:
Grs =
1
n + 1
(
‚àë
j
cosjùúã(r ‚àís)
n + 1
‚àícosjùúã(r + s)
n + 1
)
e
2cos
(
jùúã
(n+1)
)
,
(4.158)
where we have used ùõΩ‚â°1 without any loss
of generality. Then, it is straightforward to
realize by simple substitution in (4.158) that
Grs ‚Üí0 for the nodes at the end of a linear
path as n ‚Üí‚àû. At the other extreme, we
Ô¨Ånd the complete network Kn, for which
Grs = en+1
n
+ e‚àí1
n
‚àë
j=2
ùúôj(r)ùúôj(s)
= en+1
n
‚àí1
ne = 1
ne (en ‚àí1) ,
(4.159)
which means that Grs ‚Üí‚àûas n ‚Üí‚àû.
In closing, the communicability measure
quantiÔ¨Åes very well our intuition that com-
munication decays to zero when only one
route exists for connecting two nodes at an
inÔ¨Ånite distance (the end nodes of a path)
and it tends to inÔ¨Ånity when there are many
possible routes of very short distance (any
pair of nodes in a complete graph).
4.15
Communities in Networks
The study of communities in complex net-
works is a large area of research with many
existing methods and algorithms. The aim
of all of them is to identify subsets of nodes
in a network the density of whose connec-
tions is signiÔ¨Åcantly larger than the density
of connections between them and the rest
of the nodes. It is impossible to give a com-
plete survey of all the methods for detect-
ing communities in networks in this short
section. Thus we are going to describe some
of the main characteristics of a group of
methods currently used for detecting com-
munities in networks. An excellent review
with details on the many methods available
can be found in [61].
The Ô¨Årst group of methods used for
detecting communities in networks is that
of partitioning methods. Their aim is to
obtain a partition of the network into p
disjoint sets of nodes such that
(i) ‚ãÉp
i=1 Vi = V and Vi
‚ãÇVj = ùúôfor i ‚â†j,
(ii) the number of edges crossing between
subsets (cut size or boundary) is
minimized,

4.15 Communities in Networks
149
(iii) ||Vi|| ‚âàn‚àïp for all i = 1, 2, ‚Ä¶ , p, where
the vertical bars indicates the
cardinality of the set.
When condition (iii) is fulÔ¨Ålled, the
corresponding partition is called balanced.
There are several algorithms that have
been tested in the literature for the pur-
pose of network partitioning that include
local improvement methods and spectral
partitioning. The last family of partition
methods is based on the adjacency, Lapla-
cian, or normalized Laplacian matrices
of graphs. In general, their goal is to Ô¨Ånd
a separation between the nodes of the
network based on the eigenvectors of
these matrices. This separation is carried
out grosso modo by considering that two
nodes v1, v2 are in the same partition if
sgn ùúëM
2
(v1
) = sgn ùúëM
2
(v2
) for M = A, L, ÃÉL.
Otherwise, they are considered to be in
two diÔ¨Äerent partitions {V1, V2
}. Sophisti-
cated versions of these methods exist and
the reader is referred to the specialized
literature for details [61].
The second group of methods is based
on edge centralities. We have deÔ¨Åned a
number of centrality measures for nodes
in a previous section of this chapter; these
can be extended to edges of a network in
a straightforward way. In these methods,
the aim is to identify edges that connect
diÔ¨Äerent communities. The best known
technique is based on edge betweenness
centrality, deÔ¨Åned for edges in a similar
way as for nodes. This method, known as
the Girvan‚ÄìNewman algorithm [62], can
be summarized in the following steps:
1. Calculate the edge betweenness
centrality for all links in the network.
2. Remove the link with the largest edge
betweenness or any of them if more
than one exists.
3. Recalculate the edge betweenness for
the remaining links.
4. Repeat until all links have been
removed.
5. Use a dendrogram for analyzing the
community structure of the network.
Using this dendrogram, a hierarchy of
diÔ¨Äerent communities is identiÔ¨Åed, which
can be discriminated by using diÔ¨Äerent
quality criteria. The most popular among
these quality criteria is the so-called mod-
ularity index. In a network consisting of nV
partitions, V1, V2, ‚Ä¶ , VnC, the modularity
is the sum over all partitions of the diÔ¨Äer-
ence between the fraction of links inside
each partition and the expected fraction
by considering a random network with the
same degree for each node [63]:
Q =
nC
‚àë
k=1
‚é°
‚é¢
‚é¢
‚é¢‚é£
||Ek||
m ‚àí
‚éõ
‚éú
‚éú
‚éú‚éù
‚àë
j‚ààVk
kj
2m
‚éû
‚éü
‚éü
‚éü‚é†
2‚é§
‚é•
‚é•
‚é•‚é¶
,
(4.160)
where ||Ek|| is the number of links between
nodes in the kth partition of the network.
Modularity is interpreted in the following
way. If Q = 0, the number of intracluster
links is not bigger than the expected value
for a random network. Otherwise, Q = 1
means that there is a strong community
structure in the network given by the par-
tition analyzed.
The third group of community detection
methods is based on similarity measures
for the nodes in a network. Such similar-
ity measures for the nodes of a network
can be based on either rows or columns
of the adjacency matrix of the network.
For instance, we can consider as a mea-
sure of similarity between two nodes the
angle between the corresponding rows or
columns of these two nodes in the adja-
cency matrix of the graph. This angle is
deÔ¨Åned as

150
4 Graph and Network Theory
ùúéij = cos ùúóij =
xTy
‚Äñx‚Äñ ‚ãÖ‚Äñy‚Äñ,
(4.161)
which can be seen to equal
ùúéij =
ùúÇij
‚àö
kikj
,
(4.162)
where ùúÇij is the number of common neigh-
bors of nodes i and j.
Other similarity measures between rows
or columns of the adjacency matrices are
the Pearson correlation coeÔ¨Écient, diÔ¨Äer-
ent types of norms and distances (Manhat-
tan, Euclidean, inÔ¨Ånite), and so on. Once a
similarity measure has been chosen, any of
the variety of similarity-based methods for
detecting communities in a network can be
used.
4.16
Dynamical Processes on Networks
There are many dynamical processes that
can be deÔ¨Åned on graphs and networks.
The reader should be aware that this is a
vast area of multidisciplinary research with
a huge number of publications in diÔ¨Äerent
Ô¨Åelds.
4.16.1
Consensus
We will start here with a simple model for
analyzing consensus among the nodes in a
network. We consider a graph G = (V, E)
whose nodes represent agents in a complex
system and the edges represent interactions
between such agents. In such multiagent
system, consensus means an agreement
regarding a certain quantity of interest.
An example is a collection of autonomous
vehicles engaged in cooperative teamwork
in civilian and military applications. Such
coordinated activity allows them to per-
form missions with greater eÔ¨Écacy than if
they perform solo missions [64].
Let n = |V| be the number of agents
forming a network. The collective dynam-
ics of the group of agents is represented by
the following equations for the continuous-
time case:
Ãáùõó= ‚àíLùõó, ùõó(0) = ùõó0,
(4.163)
where ùõó0 is the original distribution, which
may represent opinions, positions in space,
or other quantities with respect to which
the agents should reach a consensus. The
reader surely already has recognized that
(4.163) is identical to the heat equation,
ùúïu
ùúït = hŒîu,
(4.164)
where h is a positive constant and ‚àá2 =
‚àíL is the Laplace operator. In general, this
equation is used to model the diÔ¨Äusion of
‚Äúinformation‚Äù in a physical system, where,
by information we can understand heat, a
chemical substance, or opinions in a social
network.
A consensus is reached if, for all ùúëi (0)
and all i, j = 1, ‚Ä¶ , n, |||ùúëi (t) ‚àíùúëj (t)||| ‚Üí0
as t ‚Üí0. The discrete-time version of the
model has the form
ùõói(t + 1) = ùõói(t) + ùúÄ
‚àë
j‚àºi
Aij
[ùõój (t) ‚àíùõói (t)
],
ùõó(0) = ùõó0,
(4.165)
where ùõói (t) is the value of a quantitative
measure on node i, ùúÄ> 0 is the step-size,
and j ‚àºi indicates that node j is connected
to node i. It has been proved that the con-
sensus is asymptotically reached in a con-
nected graph for all initial states if 0 < ùúÄ<
1‚àïùõømax, where ùõømax is the maximum degree
of the graph. The discrete-time collective

4.16 Dynamical Processes on Networks
151
0
0
5
10
15
35
30
25
20
500
1000
1500
Time
State
Figure 4.15
Time evolution
of consensus dynamics in a
real-world social network with
random initial states for the
nodes.
dynamics of the network can be written in
matrix form as [64] as
ùõó(t + 1) = Pùõó(t) , ùõó(0) = ùõó0,
(4.166)
where P = I ‚àíùúÄL, and I is the n √ó n iden-
tity matrix. The matrix P is the Perron
matrix of the network with parameter 0 <
ùúÄ< 1‚àïùõømax. For any connected undirected
graph, the matrix P is an irreducible, dou-
bly stochastic matrix with all eigenvalues ùúáj
in the interval [‚àí1, 1] and a trivial eigen-
value of 1. The reader can Ô¨Ånd the pre-
viously mentioned concepts in any book
on elementary linear algebra. The relation
between the Laplacian and Perron eigenval-
ues is given by ùúáj = 1 ‚àíùúÄùúÜj.
In Figure 4.15, we illustrate the consen-
sus process in a real-world social network
having 34 nodes and 78 edges.
4.16.2
Synchronization in Networks
A problem closely related to that of consen-
sus in networks is one of synchronization
[65, 66]. The phenomenon of synchro-
nization appears in many natural systems
consisting of a collection of oscillators cou-
pled to each other. These systems include
animal
and
social
behavior,
neurons,
cardiac pacemaker cells, among others.
We can start by considering a network
G = (V, E) with |V| = n nodes represent-
ing coupled identical oscillators. Each node
is an N-dimensional dynamical system that
is described by the following equation:
Ãáxi = f
(
xi
) + c
n
‚àë
j=1
LijH (t) xj,
i = 1, ‚Ä¶ , n,
(4.167)
where xi = (xi1, xi2, ‚Ä¶ , xiN
) ‚àà‚ÑùN is the
state vector of the node i, f (‚ãÖ) ‚à∂‚ÑùN ‚Üí‚ÑùN
is
a
smooth
vector-valued
function
that deÔ¨Ånes the dynamics, c is a con-
stant representing the coupling strength,
H (‚ãÖ) ‚à∂‚ÑùN ‚Üí‚ÑùN is a Ô¨Åxed output func-
tion also known as the outer coupling
matrix, t is the time, and Lij are the ele-
ments of the Laplacian matrix of the
network (sometimes the negative of the
H (xi
) ‚âàH (s) + ùúâiH‚Ä≤ (s) Laplacian matrix

152
4 Graph and Network Theory
is taken here). The network is said to
achieve synchronization if
x1(t)=x2(t)=¬∑ ¬∑ ¬∑=xn(t) ‚Üís(t), ast ‚Üí‚àû.
(4.168)
Let us now consider a small perturbation ùúâi
such that xi = s + ùúâi (ùúâi ‚â™s) and let us ana-
lyze the stability of the synchronized man-
ifold x1 = x2 = ¬∑ ¬∑ ¬∑ = xn. First, we expand
the terms in (4.167) as
f (xi
) ‚âàf (s) + ùúâif ‚Ä≤ (s) ,
(4.169)
H (xi
)
‚âàH (s) + ùúâiH‚Ä≤ (s) ,
(4.170)
where the primes refers to the derivatives
respect to s. Thus, the evolution of the per-
turbations is determined by the following
equation:
Ãáùúâi = f ‚Ä≤ (s) ùúâi + c
‚àë
j
[LijH‚Ä≤ (s)
]ùúâj.
(4.171)
It is known that the system of equations
for the perturbations can be decoupled by
using the set of eigenvectors of the Lapla-
cian matrix, which are an appropriate set
of linear combinations of the perturbations.
Let ùúôj be an eigenvector of the Laplacian
matrix of the network associated with the
eigenvalue ùúáj. Recall that the Laplacian is
positive semideÔ¨Ånite, that is, 0 = ùúá1 ‚â§ùúá2 ‚â§
¬∑ ¬∑ ¬∑ ‚â§ùúán ‚â°ùúámax. Then
Ãáùúôi = [f ‚Ä≤ (s) + cùúáiH‚Ä≤ (s)
] ùúôi.
(4.172)
Let us now assume that at short times the
variations of s are small enough to allow
us to solve these decoupled equations, with
the solutions being
Ãáùúôi (t) = ùúô0
i exp
{[
f ‚Ä≤ (s) + cùúáiH‚Ä≤ (s)
] t} ,
(4.173)
where ùúô0
i is the initially imposed perturba-
tion.
We now consider the term in the expo-
nential of (4.173), Œõi = f ‚Ä≤ (s) + cùúáiH‚Ä≤ (s).
If
f ‚Ä≤ (s) > cùúáiH‚Ä≤ (s) ,
the
perturbations
will
increase
exponentially,
while
if
f ‚Ä≤ (s) < cùúáiH‚Ä≤ (s) ,
they
will
decrease
exponentially. So, the behavior of the
perturbations in time is controlled by the
magnitude of ùúái. Then, the stability of
the synchronized state is determined by
the master stability function:
Œõ (ùõº) ‚â°max
s
[f ‚Ä≤ (s) + ùõºH‚Ä≤ (s)
],
(4.174)
which corresponds to a large number of
functions f , and H is represented in the
Figure 4.16.
Œõ(Œ±)
0
Œ±1
Œ±2
Œ±
Figure 4.16
Schematic representation of
the typical behavior of the master stability
function.

4.16 Dynamical Processes on Networks
153
S
I
R
Figure 4.17
Diagrammatic representation of an SIR model.
As can be seen, the necessary condition
for stability of the synchronous state is that
cùúái is between ùõº1 and ùõº2, which is the region
where Œõ (ùõº) < 0. Then, the condition for
synchronization is [67]:
Q ‚à∂= ùúáN
ùúá2
< ùõº2
ùõº1
,
(4.175)
that is, synchronizability of a network is
favored by a small eigenratio Q, which
indeed depends only on the topology of
the network. There are many studies on
the synchronizability of networks using
diÔ¨Äerent types of oscillators and the reader
is referred to the specialized literature for
the details (See Further Reading, [68]).
4.16.3
Epidemics on Networks
Another area in which the dynamical pro-
cesses on networks play a fundamental role
is the study of the spread of epidemics.
These models are extensions of the clas-
sical models used in epidemiology that
consider the inÔ¨Çuence of the topology of a
network on the propagation of an epidemic
[69]. The simplest model assumes that an
individual who is susceptible (S) to an infec-
tion could become infected (I). In a second
model, the infected individual can also
recover (R) from infection. The Ô¨Årst model
is known as an SI model, while the second is
known as a susceptible‚Äìinfected‚Äìrecovered
(SIR) model. In a third model, known as
the susceptible‚Äìinfected‚Äìsusceptible (SIS),
an individual can be reinfected, so that
infections do not confer immunity on an
infected individual. Finally, a model known
as SIRS allows for recovery and reinfec-
tion as an attempt to model the temporal
immunity conferred by certain infections.
Here we brieÔ¨Çy consider only the SIR and
SIS models on networks.
In the SIR model there are three compart-
ments as sketched in the Figure 4.17, that is,
in a network G = (V, E), a group of nodes
S ‚äÜV are considered susceptible and they
can be infected by directed contact with
infected individuals. Let si, xi, and ri be the
probabilities that the node i is susceptible,
infected, or has recovered. The evolution of
these probabilities in time is governed by
the following equations that deÔ¨Åne the SIR
model:
Ãási = ‚àíùõΩsi
‚àë
j
Aijxj,
(4.176)
Ãáxi = ùõΩsi
‚àë
j
Aijxj ‚àíùõæxi,
(4.177)
Ãári = ùõæxi,
(4.178)
where ùõΩis the spreading rate of the
pathogen, Aij is an entry of the adjacency
matrix of the network, and ùõæis the proba-
bility that a node recovers or dies, that is,
the recovery rate.
In the SIS model, the general Ô¨Çow chart
of the infection can be represented as in
Figure 4.18:
The equations governing the evolution of
the probabilities of susceptible and infected
individuals are given as follows:
Ãási = ‚àíùõΩsi
‚àë
j
Aijxj + ùõæxi,
(4.179)
Ãáxi = ùõΩsi
‚àë
j
Aijxj ‚àíùõæxi.
(4.180)
The analysis of epidemics in networks is
of tremendous importance in modern life.
Today, there is a large mobility of people
across cities, countries, and the entire
world and an epidemic can propagate

154
4 Graph and Network Theory
S
I
Figure 4.18
Diagrammatic representation of an SI model.
through the social networks at very high
rates. The reader can Ô¨Ånd a few examples
in the specialized literature [69].
Glossary
Adjacency matrix of a simple graph: a
binary symmetric matrix whose row and
columns represent the vertices of the graph,
where the i, j entry is one if the correspond-
ing vertices i and j are connected.
Betweenness centrality a centrality mea-
sure for a node that characterizes how cen-
tral a node is in passing information from
other nodes.
Bipartite graph a graph with two sets of
vertices, the nodes of each set being con-
nected only to nodes of the other set.
Bridge an edge whose deletion increases
the number of connected components of
the graph.
Centrality measure an index for a node or
edge of a graph/network that characterizes
its topological or structural importance.
Closeness centrality a centrality measure
for a node that characterizes how close the
node is with respect to the rest in terms of
the shortest-path distance.
Clustering coeÔ¨Écient the ratio of the
number of triangles incident to a node in a
graph to the maximum possible number of
such triangles.
Communicability a measure of how well-
communicated a pair of nodes is by consid-
ering all possible routes of communication
in a graph/network.
Complete graph a graph in which every
pair of vertices are connected to each other.
Connected graph a graph in which there is
a path connecting every pair of nodes.
Cycle a path in which the initial and end
vertices coincide.
Cycle graph a graph in which every node
has degree two.
Degree a centrality measure for a node that
counts the number of edges incident to a
node.
Degree distribution the statistical distri-
bution of the degrees of the nodes of a
graph.
Edge contraction a graph operation in
which an edge of the graph is removed and
the two end nodes are merged together.
Edge deletion a graph operation in which
an edge of the graph is removed leaving the
end nodes in the graph.
Erd√∂s‚ÄìR√©nyi graph
a
random
graph
formed from a given set of nodes and a
probability of create edges among them.
Forest a graph formed by several compo-
nents all of which are trees.
Girth the size of the minimum cycle in a
graph.
Graph a pair formed by a set of vertices or
nodes and a set of edges.
Graph diameter the length of the largest
shortest-path distance in a graph.
Graph diameter the maximum shortest-
path distance in a graph.
Graph invariant a characterization of a
graph that does not depend on the labeling
of vertices or edges.
Graph nullity the multiplicity of the zero
eigenvalue of the adjacency matrix, that is,
the number of times eigenvalue zero occurs
in the spectrum of the adjacency matrix.
Hydrocarbon a molecule formed only by
carbon and hydrogen.
Incidence matrix of a graph a matrix
whose rows correspond to vertices and
whose columns correspond to edges of the

References
155
graph and the i, j entry is one or zero if the
ith vertex is incident with the jth edge or
not, respectively.
Laplacian matrix a square symmetric
matrix with diagonal entries equal to the
degree of the corresponding vertex and
oÔ¨Ä-diagonal entries equal to ‚àí1 or zero
depending on whether the corresponding
vertices are connected or not, respectively.
Loop an edge that is doubly incident to the
same node.
Matching of a graph the number of mutu-
ally nonadjacent edges in the graph.
Mean displacement of an atom (vertex):
refers to the oscillations of an atoms from
its equilibrium position due to thermal Ô¨Çuc-
tuations.
Molecular Hamiltonian the operator rep-
resenting the energy of the electrons and
atomic nuclei in a molecule.
Network community a subset of nodes in
a graph/network that are better connected
among themselves than with the rest of the
nodes.
Network motif a subgraph in a graph that
is overrepresented in relation to a random
graph of the same size.
Path a sequence of diÔ¨Äerent consecutive
vertices and edges in a graph.
Path graph a tree in which all nodes have
degree two except two nodes, which has
degree one.
Regular graph a graph in which every node
has the same degree.
Resistance distance the distance between
any pair of vertices of the graph, deter-
mined by the KirchhoÔ¨Ärules for electrical
sets.
Scale-free network a network/graph with
a power-law degree distribution.
Shortest path between two nodes a path
having the least number of edges among all
paths connecting two vertices.
Simple graph a graph without multiple
edges, self-loops, and weights.
Spanning forest a subgraph of a graph that
contains all the nodes of the graph and is a
forest.
Spanning tree a subgraph of a graph that
contains all the nodes of the graph and is
also a tree.
Star graph a tree consisting of a node with
degree n ‚àí1 and n ‚àí1 nodes of degree one.
Tree a graph that does not have any cycle.
Vertex degree the number of vertices adja-
cent to a given vertex.
Walk a sequence of (not necessarily) dif-
ferent consecutive vertices and edges in a
graph.
References
1. Euler L. (1736) Comm. Acad. Sci. Imp.
Petrop., 8, 128‚Äì140.
2. Biggs, N.L., Lloyd, E. K. and Wilson, L.
(1976) Graph Theory 1736‚Äì1936. Clarendon
Press, Oxford.
3. Sylvester, J. J. (1877‚Äì1878) Nature 17,
284‚Äì285.
4. Harary, F. Ed. (1968) Graph Theory and
Theoretical Physics. Academic Press.
5. Trinajsti¬¥c, N. (1992) Chemical Graph Theory.
CRC Press, Boca Raton, FL.
6. Berkolaiko, G., Kuchment, P. (2013)
Introduction to Quantum Graphs, vol. 186,
American Mathematical Society, Providence,
RI.
7. Harary, F. (1969) Graph Theory.
Addison-Wesley, Reading, MA.
8. Watts, D. J., Strogatz, S. H. (1998) Nature
393, 440‚Äì442.
9. Newman, M. E. J., Strogatz, S. H., Watts, D. J.
(2001) Phys. Rev. E 64, 026118.
10. Canadell, E., Doublet, M.-L., Iung, C. (2012)
Orbital Approach to the Electronic Structure
of Solids. Oxford University Press, Oxford.
11. Kutzelnigg, W. (2006) J. Comput. Chem. 28,
25‚Äì34.
12. Powell, B.J. (2009) An Introduction to
EÔ¨Äective Low-Energy Hamiltonians in
Condensed Matter Physics and Chemistry.
arXiv preprint arXiv:0906.1640.
13. Gutman, I. (2005). J. Serbian Chem. Soc. 70,
441‚Äì456.

156
4 Graph and Network Theory
14. Borovi¬¥canin, B., Gutman, I. (2009) In
Applications of Graph Spectra, D. Cvetkovi¬¥c
and I. Gutman Eds. Mathematical Institute
SANU, pp. 107‚Äì122.
15. Cheng, B., Liu, B. (2007) Electron. J. Linear
Algebra 16 (2007), 60‚Äì67.
16. Tasaki, H. (1999) J. Phys.: Cond. Mat., 10,
4353.
17. Lieb, E. H. (1989) Phys. Rev. Lett. 62, 1201
(Erratum 62, 1927, (1989)).
18. Morita, Y., Suzuki, S., Sato, K., Takui, T.
(2011) Nat. Chem. 3, 197‚Äì204.
19. Essam, J. W. (1971) Discrerte Math. 1,
83‚Äì112.
20. Beaudin, L., Ellis-Monaghan, J., Pangborn,
G., Shrock, R. (2010) Discrete Math. 310,
2037‚Äì2053.
21. Welsh, D. J. A., Merino, C. (2000) J. Math.
Phys. 41, 1127‚Äì1152.
22. Bollob√°s, B., (1998) Modern Graph Theory.
Springer-Verlag, New York.
23. Ellis-Monaghan, J.A. and Merino, C. (2011)
In Structural Analysis of Complex Networks,
ed. M. Dehmer, Birkhauser, Boston, MA, pp.
219‚Äì255.
24. Welsh, D. (1999) Random Struct. Alg., 15,
210‚Äì228.
25. Bogner, C. (2010) Nucl. Phys. B Proc. Suppl.
205, 116‚Äì121.
26. Bogner, C. and Weinzierl S. (2010) Int. J.
Mod. Phys. A 25, 2585.
27. Weinzierl, S. (2010) Introduction to
Feynman Integrals. arXiv preprint
arXiv:1005.1855.
28. Dodgson, C. L. (1866) Proc. R. Soc. London
15, 150‚Äì155.
29. Doyle, P. and Snell, J. (1984) Random Walks
and Electric Networks. Carus Mathematical
Monographs, vol. 22, The Mathematical
Association of America, Washington, DC.
30. Klein, D. J., Randi¬¥c, M. (1993) J. Math. Chem.
12, 81‚Äì95.
31. Bapat, R. B., Gutman, I., Xiao, W. (2003) Z.
Naturforsch. 58a, 494 ‚Äì 498.
32. Xiao, W., Gutman, I., (2003) Theor. Chem.
Acc. 110, 284‚Äì289.
33. Gutman, I., Xiao, O. (2004) Bull. Acad. Serb.
Sci. Arts. 29, 15‚Äì23.
34. Ghosh, A., Boyd, S., Saberi, A. (2008) SIAM
Rev. 50, 37‚Äì66.
35. Estrada, E., Hatano, N., Benzi, M., (2012)
Phys. Rep. 514, 89‚Äì119.
36. Erd√∂s, P., R√©nyi, A. (1959) Publ. Math.
Debrecen 5, 290‚Äì297.
37. Janson, S. (2005) J. Combin. Prob. Comput.
14, 815‚Äì828.
38. Wigner, E. P. (1955) Ann. Math. 62,
548‚Äì564.
39. Standish, R. K. (2008) In Intelligent Complex
Adaptive Systems, Yang, A. and Shan, Y. eds,
IGI Global: Hershey, PA, pp. 105‚Äì124,
arXiv:0805.0685.
40. Ottino, J. M. (2003) AIChE J., 49, 292‚Äì299.
41. Estrada, E. (2011) The Structure of Complex
Networks. Theory and Applications. Oxford
University Press, Oxford.
42. Milgram, S. (1967) Psychol. Today 2, 60‚Äì67.
43. Barrat, A., Weigt, M. (2000) Eur. Phys. J. B
13, 547‚Äì560.
44. Foss, S., Korshunov, D., Zachary, S. (2011)
An Introduction to Heavy-Tailed and
Subexponential Distributions. Springer,
Berlin.
45. Clauset, A., Rohilla Shalizi, C., Newman, M.
E. J. (2010) SIAM Rev. 51, 661‚Äì703.
46. Barab√°si, A.-L. Albert, R. (1999) Science 286,
509‚Äì512.
47. Bollob√°s, B., Riordan, O. (2004)
Combinatorica 24, 5‚Äì34.
48. Bollob√°s, B. (2003) In Handbook of Graph
and Networks: From the Genome to the
Internet, Bornholdt, S., Schuster, H. G. Eds.
Wiley-VCH Verlag GmbH, Weinheim,
pp. 1‚Äì32.
49. Dorogovtsev, S. N. and Mendes, J. F. F.
(2003) Evolution of Networks: From
Biological Nets to the Internet and WWW.
Oxford University Press, Oxford.
50. Milo, R., Shen-Orr, S., Itzkovitz, S. Kashtan,
N. Chklovskii, D. Alon, U. (2002) Science
298, 824‚Äì827.
51. Milo, R., Itzkovitz, S., Kashtan, N., Levitt, R.,
Shen-Orr, S., Ayzenshtat, I., SheÔ¨Äer, M.,
Alon, U. (2004) Science 303, 1538‚Äì1542.
52. Freeman, L. C. (1979) Social Networks 1,
215‚Äì239.
53. Katz, L. (1953) Psychometrica 18, 39‚Äì43.
54. Bonacich, P. (1972) J. Math. Sociol. 2,
113‚Äì120.
55. Bonacich, P., (1987) Am. J. Soc. 92,
1170‚Äì1182.
56. Langville, A. N., Meyer, C. D. (2006). Google‚Äôs
PageRank and Beyond. The Science of Search
Engine Rankings. Princeton University Press,
Princeton, NJ.
57. Estrada, E., Rodr¬¥ƒ±guez-Vel√°zquez, J. A.
(2005) Phys. Rev. E 71, 056103.

Further Reading
157
58. Estrada, E., Hatano, N. (2007) Chem. Phys.
Lett. 439, 247‚Äì251.
59. Estrada, E. (2000) Chem. Phys. Lett. 319,
713‚Äì718.
60. Estrada, E., Hatano, N. (2008) Phys. Rev. E
77, 036111.
61. Fortunato, S. (2010) Phys. Rep. 486, 75‚Äì174.
62. Girvan, M., Newman, E. J. (2002) Proc. Natl.
Acad. Sci. U.S.A. 99, 7821‚Äì7826.
63. Newman, M. E. J. (2006) Proc. Natl. Acad.
Sci. U.S.A. 103, 8577‚Äì8582.
64. Olfati-Saber, R., Fax, J. A., Murray, R. M.
(2007) Proc. IEEE 95, 215‚Äì233.
65. Arenas, A., Diaz-Guilera, A., P√©rez-Vicente,
C. J. (2006). Physica D 224, 27‚Äì34.
66. Chen, G., Wang, X., Li, X., L√º , J. (2009) In
Recent Advances in Nonlinear Dynamics and
Synchronization, K. Kyamakya Ed.,
Springer-Verlag, Berlin, pp. 3‚Äì16.
67. Barahona, M., Pecora, L. M. (2002) Phys.
Rev. Lett. 89, 054101.
68. Barrat, A., Barth√©lemy, M., Vespignani, A.
(2008) Dynamical Processes on Complex
Networks. Cambridge University Press,
Cambridge.
69. Keeling, M. J., Eames, K. T. (2005) J. R. Soc.
Interface 2, 295‚Äì307.
Further Reading
Bollob√°s, B. (1998) Modern Graph Theory.
Springer, Berlin.
Caldarelli, G. (2007) Scale-Free Networks.
Complex Webs in Nature and Technology.
Oxford University Press, Oxford.
Cvetkovi¬¥c, D., Rowlinson, P., Simi¬¥c, S. (2010) An
Introduction to the Theory of Graph Spectra.
Cambridge University Press, Cambridge.
Nakanishi, N. (1971) Graph Theory and Feynman
Integrals. Gordon and Breach.


159
5
Group Theory
Robert Gilmore
5.1
Introduction
Symmetry has sung its siren song to physi-
cists since the beginning of time, or since
even before there were physicists. Today,
the ideas of symmetry are incorporated into
a subject with the less imaginative and sug-
gestive name of group theory. This chapter
introduces many of the ideas of group the-
ory that are important in the natural sci-
ences.
Natural philosophers in the past have
come up with many imaginative arguments
for estimating physical quantities. They
have often used out of the box methods
that were proprietary, to pull rabbits out
of hats. When these ideas were made
available to a wider audience, they were
often improved upon in unexpected and
previously unimaginable ways. A number
of these methods are precursors of group
theory. These are dimensional analysis,
scaling theory, and dynamical similar-
ity. We review these three methods in
Section 5.2.
In Section 5.3, we get down to the busi-
ness at hand, introducing the deÔ¨Ånition of
a group and giving a small set of important
deÔ¨Ånitions (e.g., isomorphism and homo-
morphism). Others will be introduced later
in a context in which they make immediate
sense. In Sections 5.4‚Äì5.6, we present
some useful examples of groups ranging
from Ô¨Ånite and inÔ¨Ånite discrete groups
through matrix groups to Lie groups.
These examples include transformation
groups, which played an important if
under-recognized r√¥le in the development
of classical physics, in particular, the the-
ories of special and general relativity. The
relation between these theories and group
theory is indicated in Section 5.9.
The study of Lie groups is greatly
simpliÔ¨Åed
when
carried
out
on
their
‚ÄúinÔ¨Ånitesimal‚Äù versions. These are Lie alge-
bras, which are introduced in Section 5.7.
In this section, we introduce many of the
important concepts and provide examples
to illustrate all of them. One simple con-
sequence of these beautiful developments
is the possibility of studying and classify-
ing Riemannian symmetric spaces. These
are Riemannian spaces with a particular
symmetry that is, eÔ¨Äectively, time reversal
invariance at each point. These spaces
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

160
5 Group Theory
are cosets (quotients) of one Lie group by
another. They are introduced in Section 5.8.
Despite this important r√¥le in the devel-
opment of physics, groups existed at the
fringe of the physics of the early twentieth
century. It was not until the theory of the
linear matrix representations of groups was
invented that the theory of groups migrated
from the outer fringes to play a more cen-
tral r√¥le in physics. Important points in the
theory of representations are introduced
in Section 5.10. Representations were used
in an increasingly imaginative number of
ways in physics throughout the twentieth
century. Early on, they were used to label
states in quantum systems with a symme-
try group: for example, the rotation group
SO(3). Once states were named, degen-
eracies could be predicted and computa-
tions simpliÔ¨Åed. Such applications are indi-
cated in Section 5.11. Later, they were used
when symmetry was not present, or just
the remnant of a broken symmetry was
present. When used in this sense, they are
often called ‚Äúdynamical groups.‚Äù This type
of use greatly extended the importance of
group theory in physics. Some such appli-
cations of group theory are presented in
Section 5.12.
As a latest tour de force in the develop-
ment of physics, groups play a central r√¥le
in the formulation of gauge theories. These
theories describe the interactions between
fermions and the bosons and lie at the heart
of the standard model. We provide the sim-
plest example of a gauge theory, based on
the simplest compact one-parameter Lie
group U(1), in Section 5.13.
For
an
encore,
in
Section 5.14,
we
show
how the
theory
of
the special
functions of mathematical physics (Leg-
endre and associated Legendre functions,
Laguerre and associated Laguerre func-
tions, Gegenbauer, Chebyshev, Hermite,
Bessel functions, and others) are subsumed
under the theory of representations of
some low-dimensional Lie groups. The
classical theory of special functions came
to fruition in the mid nineteenth cen-
tury, long before Lie groups and their
representations were even invented.
5.2
Precursors to Group Theory
The axioms used to deÔ¨Åne a group were
formulated in the second half of the nine-
teenth century. Long before then, the
important ideas underlying these axioms
were used to derive classical results (for
example, Pythagoras‚Äô theorem: see the fol-
lowing text) in alternative, simpler, and/or
more elegant ways, to obtain new results,
or to consolidate diÔ¨Äerent results under a
single elegant argument. In this section, we
survey some of these imaginative lines of
thought. We begin with a simple argument
due to Barenblatt that has been used to
derive Pythagoras‚Äô theorem. We continue
with a discussion of the central features
of dimensional analysis and illustrate how
this tool can be used to estimate the size of
a hydrogen atom. We continue in the same
vein, using scaling arguments to estimate
the sizes of other ‚Äúatom-like‚Äù structures
based on the known size of the hydrogen
atom. We conclude this section with a brief
description of dynamical similarity and
how the arguments intrinsic to this line
of thinking can be used to estimate one
of Kepler‚Äôs laws and to place four classical
mechanics laws (Kepler, Newton, Galileo,
Hooke) in a common framework.
We emphasize that group theory is not
used explicitly in any of these arguments
but its Ô¨Ångerprints are everywhere. These
digressions should serve as appetizers to
indicate the power of the tool called group
theory in modern physical theories.

5.2 Precursors to Group Theory
161
5.2.1
Classical Geometry
Barenblatt [1] has given a beautiful deriva-
tion of Pythagoras‚Äô theorem that is out of
the box and suggests some of the ideas
behind dimensional analysis. The area
of the right triangle Œî(a, b, c) is 1‚àï2ab
(Figure 5.1). Dimensionally, the area is
proportional to square of any of the sides,
multiplied by some factor. We make a
unique choice of side by choosing the
hypotenuse, so that Œî(a, b, c) = c2 √ó f (ùúÉ), ùúÉ
is one of the two acute angles, and f (ùúÉ) ‚â†0
unless ùúÉ= 0 or ùúã‚àï2. Equating the two
expressions
f (ùúÉ) = 1
2
(a
c
) (
b
c
)
= 1
2
(
b
c
) (a
c
) symmetry
=
f
(ùúã
2 ‚àíùúÉ
)
.(5.1)
This shows (a) that the same function
f (ùúÉ) applies for all similar triangles and (b)
f (ùúÉ) = f (ùúã‚àï2 ‚àíùúÉ). The latter result is due to
reÔ¨Çection ‚Äúsymmetry‚Äù of the triangle about
the bisector of the right angle: the triangle
changes but its area does not. We need (a)
alone to prove Pythagoras‚Äô theorem. The
proof is in the Ô¨Ågure caption.
5.2.2
Dimensional Analysis
How big is a hydrogen atom?
The size of the electron ‚Äúorbit‚Äù around
the proton in the hydrogen atom ought
to depend on the electron mass me,
or more precisely the electron‚Äìproton
reduced
mass
ùúá= meMP‚àï(me + MP).
It should also depend on the value of
Planck‚Äôs constant h or reduced Planck‚Äôs
constant
‚Ñè= h‚àï2ùúã.
Since
the
interac-
tion between the proton with charge e
and the electron with charge ‚àíe is elec-
tromagnetic, of the form V(r) = ‚àíe2‚àïr
(Gaussian
units),
it
should
depend
on e2.
Mass is measured in grams. The dimen-
sions
of
the
charge
coupling
e2
are
determined by recognizing that e2‚àïr is
a
(potential)
energy,
with
dimensions
M1L2T‚àí2. We will use capital letters M
(mass), L (length), and T (time) to charac-
terize the three independent dimensional
‚Äúdirections.‚Äù As a result, the charge coupling
strength e2 has dimensions ML3T‚àí2 and
is measured in g(cm)3 s‚àí2. The quantum
of action ‚Ñèhas dimensions [‚Ñè] = ML2T‚àí1.
Here and below we use the standard
convention that [‚àó] is to be read ‚Äúthe
dimensions of ‚àóare.‚Äù
Cons‚àí
Dimensions
Value
Units
tant
ùúá
M
9.10442 √ó 10‚àí28
g
‚Ñè
ML2T‚àí1
1.05443 √ó 10‚àí27 g cm2 s‚àí1
e2
ML3T‚àí2
2.30655 √ó 10‚àí19 g cm3 s‚àí2
a0
L
?
cm
Can we construct something (e.g., Bohr
orbit aB) with the dimensions of length
from m, e2, and ‚Ñè? To do this, we introduce
three unknown exponents a, b, and c and
a
ùúÉ
d
c
f
e
ùúÉ
b
Figure 5.1
The area of the large right triangle is the sum of
the areas of the two similar smaller right triangles: Œî(a, b, c) =
Œî(d, f, a) + Œî(f, e, b), so that c2f(ùúÉ) = a2f(ùúÉ) + b2f(ùúÉ). Since
f(ùúÉ) ‚â†0 for a nondegenerate right triangle, a2 + b2 = c2.

162
5 Group Theory
write
aB ‚âÉma (e2)b ‚Ñèc
= (M)a (ML3T‚àí2)b (ML2T‚àí1)c
= (M)a+b+c L0a+3b+2c T0a‚àí2b‚àíc
(5.2)
and set this result equal to the dimensions
of whatever we would like to compute; in
this case, the Bohr orbit aB (characteristic
atomic length), with [aB] = L. This results
in a matrix equation
‚é°
‚é¢
‚é¢‚é£
1
1
1
0
3
2
0
‚àí2
‚àí1
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
a
b
c
‚é§
‚é•
‚é•‚é¶
=
‚é°
‚é¢
‚é¢‚é£
0
1
0
‚é§
‚é•
‚é•‚é¶
.
(5.3)
We can invert this matrix to Ô¨Ånd
‚é°
‚é¢
‚é¢‚é£
1
1
1
0
3
2
0
‚àí2
‚àí1
‚é§
‚é•
‚é•‚é¶
‚àí1
=
‚é°
‚é¢
‚é¢‚é£
1
‚àí1
‚àí1
0
‚àí1
‚àí2
0
2
3
‚é§
‚é•
‚é•‚é¶
.
(5.4)
This allows us to determine the values of
the exponents that provide the appropriate
combinations of important physical param-
eters to construct the characteristic atomic
length:
‚é°
‚é¢
‚é¢‚é£
a
b
c
‚é§
‚é•
‚é•‚é¶
=
‚é°
‚é¢
‚é¢‚é£
1
‚àí1
‚àí1
0
‚àí1
‚àí2
0
2
3
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
0
1
0
‚é§
‚é•
‚é•‚é¶
=
‚é°
‚é¢
‚é¢‚é£
‚àí1
‚àí1
2
‚é§
‚é•
‚é•‚é¶
.
(5.5)
This result tells us that
a0 ‚àºm‚àí1(e2)‚àí1(‚Ñè)2 = ‚Ñè2
me2 ‚àº10‚àí8 cm.
(5.6)
To construct a characteristic atomic
time, we can replace the vector col[0, 1, 0]
in (5.5) by the vector col[0, 0, 1], giving us
the result ùúè0 ‚àº‚Ñè3‚àïm(e2)2. Finally, to get
a characteristic energy, we can form the
combination
Óà±‚àºML2T‚àí2 = m(‚Ñè2‚àïme2)2
(‚Ñè3‚àïme4)‚àí2 = me4‚àï‚Ñè2. Another, and more
systematic, way to get this result is to sub-
stitute the vector col[1, 2, ‚àí2]t for [0, 1, 0]t
in (5.5).
Note that our estimate would be some-
what diÔ¨Äerent if we had used h instead of
‚Ñè= h‚àï2ùúãin these arguments. We point out
that this method is very useful for esti-
mating the order of magnitude of physical
parameters and in practised hands usually
gets the prefactor within a factor of 10. The
most critical feature of dimensional anal-
ysis is to identify the parameters that are
most important in governing the science of
the problem, and then to construct a result
depending on only those parameters.
5.2.3
Scaling
Positronium is a bound state of an elec-
tron e with a positron e, its antiparticle with
mass me and charge +e. How big is positro-
nium?
To address this question, we could
work very hard and solve the Schr√∂dinger
equation for the positronium. This is
identical to the Schr√∂dinger equation for
the hydrogen atom, except for replac-
ing the hydrogen atom reduced mass
meMp‚àï(me + Mp) ‚âÉme by the positron-
ium reduced mass meme‚àï(me + me) = 1
2me.
Or we could be lazy and observe that the
hydrogen atom radius is inversely pro-
portional to the reduced electron‚Äìproton
mass, so the positronium radius should
be inversely proportional to the reduced
electron‚Äìpositron mass me‚àï2. Since the
reduced electron‚Äìproton mass is eÔ¨Äec-
tively the electron mass, the positronium
atom is approximately twice as large as the
hydrogen atom.
In a semiconductor, it is possible to excite
an electron (charge ‚àíe) from an almost
Ô¨Ålled (valence) band into an almost empty
(conduction) band. This leaves a ‚Äúhole‚Äù
of charge +e behind in the valence band.
The positively charged hole in the valence
band interacts with the excited electron in

5.2 Precursors to Group Theory
163
the conduction band through a reduced
Coulomb interaction: V(r) = ‚àíe2‚àïùúñr. The
strength of the interaction is reduced by
screening eÔ¨Äects that are swept into a
phenomenological dielectric constant ùúñ.
In addition, the eÔ¨Äective masses m‚àó
e of
the excited electron and the left-behind
hole m‚àó
h are modiÔ¨Åed from the free-space
electron mass values by many-particle
eÔ¨Äects.
How big is an exciton in gallium arsenide
(GaAs)? For this semiconductor, the phe-
nomenological parameters are ùúñ= 12.5,
m‚àó
e = 0.07me, m‚àó
h = 0.4me.
We
extend
the
scaling
argument
above by computing the reduced mass
of
the
electron‚Äìhole
pair:
ùúáe‚àíh =
(0.07me)(0.4me)‚àï(0.07 + 0.4)me = 0.06me
and replacing e2 in the expression (5.4)
for the Bohr radius a0 by e2‚àïùúñ. The eÔ¨Äect
is to multiply a0 by 12.5‚àï0.06 = 208. The
ground-state radius of the exciton formed
in GaAs is about 10‚àí6 cm. The ground-state
binding energy is lower than the hydrogen
atom binding energy of 13.6 eV by a factor
of 0.06‚àï12.52 = 3.8 √ó 10‚àí4 so it is 5.2 meV.
Scaling arguments such as these are
closely related to renormalization group
arguments as presented in Chapter 12.
5.2.4
Dynamical Similarity
Jupiter is about Ô¨Åve times further (5.2 AU)
from our Sun than the Earth. How many
earth years does it take for Jupiter to orbit
the Sun?
Landau and Lifshitz [2] provide an
elegant solution to this simple question
using similarity (scaling) arguments. The
equation of motion for the Earth around
the Sun is
mE
d2xE
dt2
E
= ‚àíGmEMS
ÃÇxE
|xE|2 ,
(5.7)
where xE is a vector from the sun to
the earth and ÃÇxE the unit vector in this
direction. If Jupiter is in a geometrically
similar orbit, then xJ = ùõºxE, with ùõº= 5.2.
Similarly, time will evolve along the Jupiter
trajectory in a scaled version of its evolu-
tion along the Earth‚Äôs trajectory: tJ = ùõΩtE.
Substituting these scaled expressions into
the equation of motion for Jupiter, and
canceling out mJ from both sides, we
Ô¨Ånd
ùõº
ùõΩ2
d2xE
dt2
E
= ‚àí1
ùõº2 GMS
ÃÇxE
|xE|2 .
(5.8)
This scaled equation for Jupiter‚Äôs orbit
can only be equated to the equation
for the Earth‚Äôs trajectory (the orbits are
similar)
provided
ùõº3‚àïùõΩ2 = 1.
That
is,
ùõΩ= ùõº3‚àï2, so that the time-scaling factor is
5.23‚àï2 = 12.5.
We have derived Kepler‚Äôs third law with-
out even solving the equations of motion!
Landau and Lifshitz point out that you can
do even better than that. You don‚Äôt even
need to know the equations of motion to
construct scaling relations when motion
is described by a potential V(x) that is
homogeneous of degree k. This means that
V(ùõºx) = ùõºkV(x). When the equations of
motion are derivable from a variational
principle ùõøI = 0, where
I = ‚à´
(
m
(
dx
dt
)2
‚àíV(x)
)
dt,
(5.9)
then the scaling relations x ‚Üíx‚Ä≤ = ùõºx,
t ‚Üít‚Ä≤ = ùõΩt lead to a modiÔ¨Åed action
I‚Ä≤ = ùõº2
ùõΩ‚à´
(
m
(
dx
dt
)2
‚àíùõºk‚àí2ùõΩ2V(x)
)
dt.
(5.10)
The Action I‚Ä≤ is proportional to the original
Action I, and therefore leads to the same
equations of motion, only when ùõºk‚àí2ùõΩ2 = 1;

164
5 Group Theory
that is, the time elapsed, T, is proportional
to the distance traveled, D, according to
T ‚âÉD(1‚àík‚àï2). Four cases are of interest.
k = ‚àí1
(Coulomb/gravitational
poten-
tial) The period of a planetary
orbit scales as the 3‚àï2 power
of the distance from the Sun
(Kepler‚Äôs third law).
k = 0
(No forces) The distance trav-
eled is proportional to the time
elapsed
(essentially
Newton‚Äôs
Ô¨Årst law). To recover Newton‚Äôs
Ô¨Årst law completely, it is only
necessary to carry out the vari-
ation in (5.10), which leads to
d‚àïdt(dx‚àïdt) = 0.
k = +1
(Free fall in a homogeneous
gravitational Ô¨Åeld) The potential
V(z) = mgz describes free fall
in a homogeneous gravitational
Ô¨Åeld. Galileo is reputed to have
dropped rocks oÔ¨Äthe Leaning
Tower of Pisa to determine that
the distance fallen was propor-
tional to the square of the time
elapsed. The story is apocryphal:
in fact, he rolled stones down an
inclined plane to arrive at the
result Œîz ‚âÉŒît2.
k = +2
(Harmonic oscillator potential)
The period is independent of
displacement:
ùõΩ= 1
indepen-
dent of ùõº. Hooke‚Äôs law, F = ‚àíkx,
V(x) = 1‚àï2kx2 leads to oscilla-
tory motion whose frequency is
independent of the amplitude
of motion. This was particularly
useful for constructing robust
clocks.
These four historical results in the devel-
opment of early science are summarized in
Table 5.1 and Figure 5.2.
Table 5.1
Four important results in the
historical development of science are
consequences of scaling arguments.
k
Scaling
Law
‚àí1
T2 ‚âÉD3
Kepler #3
0
D ‚âÉT
Newton #1
+1
Œîz ‚âÉŒît2
Galileo: rolling stones
+2
T ‚âÉD0
Hooke
5.3
Groups: DeÔ¨Ånitions
In this section, we Ô¨Ånally get to the point
of deÔ¨Åning what a group is by stating
the group axioms (see [3‚Äì8]). These are
illustrated in the following sections with
a number of examples: Ô¨Ånite groups,
including
the
two-element
group,
the
group of transformations that leaves the
equilateral triangle invariant, the permu-
tation group, point groups, and discrete
groups with a countable inÔ¨Ånite number
of group elements, such as space groups.
Then we introduce groups of transfor-
mations in space as matrix groups. Lie
groups are introduced and examples of
matrix Lie groups are presented.
Lie
groups are linearized to form their Lie
algebras, and groups are recovered from
their algebras by reversing the linearization
procedure using the exponential map-
ping. Many of the important properties
of Lie algebras are introduced, including
isomorphisms
among
diÔ¨Äerent
repre-
sentations of a Lie algebra. A powerful
disentangling theorem is presented and
illustrated in a very simple case that plays
a prominent r√¥le in the Ô¨Åeld of quan-
tum optics. We will use this result in
Section 5.14.

5.3 Groups: DeÔ¨Ånitions
165
‚àí1
0
1/2
1
3/2
p: Tp
Kepler #3
Newton #1
Galileo
Hooke
k: V(lùõºxl) = ùõºkV(lr1)
‚àí0
+1
+2
p = 1 ‚àí (k/2)
Figure 5.2
Four substantial advances in the development
of early physics are summarized. Each is a consequence of
using a homogeneous potential with a diÔ¨Äerent degree k in
a variational description of the dynamics. Scaling relates the
size scale of the trajectory ùõºto the time scale ùõΩ= ùõºp,
p =
1 ‚àí1
2 k of the motion.
5.3.1
Group Axioms
A group G consists of
‚Ä¢ a set of group elements:
g0, g1, g2, g3, ‚Ä¶ ‚ààG
‚Ä¢ a group operation, ‚àò, called group
multiplication
that satisfy the following four axioms:
Closure: gi ‚ààG, gj ‚ààG ‚áígi ‚àògj ‚ààG
Associativity: (gi ‚àògj)‚àògk = gi ‚àò(gj‚àògk)
Identity: g0 ‚àògi = gi = gi ‚àòg0
Unique Inverse: gk ‚àògl = g0 = gl ‚àògk.
Group multiplication ‚àòhas two inputs
and one output. The two inputs must be
members of the set. The Ô¨Årst axiom (Clo-
sure) requires that the output must also be
a member of the set.
The composition rule ‚àòdoes not allow us
to multiply three input arguments. Rather,
two can be combined to one, and that
output can be combined with the third. This
can be done in two diÔ¨Äerent ways that pre-
serves the order (i, j, k). The second axiom
(Associativity) requires that these two dif-
ferent ways give the same Ô¨Ånal output.
The third axiom (Identity) requires that
a special group element exists. This, com-
bined with any other group element, gives
back exactly that group element.
The
fourth
axiom
(Unique
Inverse)
guarantees that for each group element
gl,
there
is
another
uniquely
deÔ¨Åned
group
element
gk,
with
the
property
that
the
product
of
the
two
is
the
unique
identity
element
gk ‚àògl = g0.
It
is
a
simple
matter
to
prove
that
gl ‚àògk = g0.
Remark 5.1 ‚Äì Indexes: The notation (sub-
scripts i, j, k, ‚Ä¶) may suggest that the
indices are integers. This is not gen-
erally true: for continuous groups the
indices are points in some subspace of

166
5 Group Theory
a Euclidean space or a more compli-
cated manifold.
Remark 5.2 ‚Äì Commutativity: In general,
the output of the group multiplication
depends on the order of the inputs:
gi ‚àògj ‚â†gj ‚àògi. If the result is indepen-
dent of the order the group is said to
be commutative.
It is not entirely obvious that the Unique
Inverse axiom is needed. It is included
among
the
axioms
because
many
of
our uses involve relating measurements
made by two observers. For example, if
Allyson on the Earth can predict some-
thing about the length of a year on Jupiter,
then Bob on Jupiter should just as well
be able to predict the length of Allyson‚Äôs
year on Earth. Basically, this axiom is an
implementation of Galileo‚Äôs principle of
relativity.
5.3.2
Isomorphisms and Homomorphisms
It is often possible to compare two diÔ¨Äerent
groups. When it is possible, it is very useful.
Suppose we have two groups G with group
elements g0, g1, g2, ‚Ä¶ and group compo-
sition law gi ‚àògj = gk and H with group ele-
ments h0, h1, h2, ‚Ä¶ and group composition
law hi ‚ãÑhj = kk. A mapping f from G to H
is a homomorphism if it preserves the group
operation:
f (gi ‚àògj) = f (gi) ‚ãÑf (gj).
(5.11)
In this expression f (g‚àó) ‚ààH, so the two
group elements f (gi) and f (gj) can only be
combined using the combinatorial opera-
tion ‚ãÑ. If (5.11) is true for all pairs of group
elements in G, the mapping f is a homo-
morphism.
If G has four elements I, C4, C2
4, C3
4
and H has two Id, C2, the mapping
f (I) = f (C2
4) = Id,
f (C4) = f (C3
4) = C2,
the
mapping
f
is
a
homomorphism.
If the mapping f
is a homomorphism
and is also 1 : 1, it is called an iso-
morphism.
Under
these
conditions,
the inverse mapping also is an isomor-
phism:
f ‚àí1(hp ‚ãÑhq) = f ‚àí1(hp) ‚àòf ‚àí1(hq).
(5.12)
As
an
example,
an
isomor-
phism
exists
between
the
four
group
elements
I, C4, C2
4, C3
4
and
the
2 √ó 2
matrices
with
f (C4) =
[
0
‚àí1
1
0
]
.
5.4
Examples of Discrete Groups
5.4.1
Finite Groups
5.4.1.1
The Two-Element Group Z2
The simplest nontrivial group has one addi-
tional element beyond the identity e: G =
{e, g} with g ‚àòg = e. This group can act in
our three-dimensional space R3 in several
diÔ¨Äerent ways:
ReÔ¨Çection: (x, y, z)
g=ùúéZ
‚Üí(+x, +y, ‚àíz)
Rotation: (x, y, z)
g=RZ(ùúã)
‚Üí
(‚àíx, ‚àíy, +z)
Inversion: (x, y, z)
g=Óàº
‚Üí(‚àíx, ‚àíy, ‚àíz)
These three diÔ¨Äerent actions of the order-
two group on R3 describe: reÔ¨Çections in the
x-y plane, ùúéZ; rotations around the Z-axis
through ùúãradians, RZ(ùúã); and inversion in
the origin, the parity element, Óàº. They can
be distinguished by their matrix represen-
tations, which are

5.4 Examples of Discrete Groups
167
ùúéZ
RZ(ùúã)
‚é°
‚é¢
‚é¢‚é£
+1
0
0
0
+1
0
0
0
‚àí1
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
‚àí1
0
0
0
‚àí1
0
0
0
+1
‚é§
‚é•
‚é•‚é¶
Óàº
‚é°
‚é¢
‚é¢‚é£
‚àí1
0
0
0
‚àí1
0
0
0
‚àí1
‚é§
‚é•
‚é•‚é¶
.
(5.13)
5.4.1.2
Group of Equilateral Triangle C3v
The six elements that map the equilateral
triangle to itself constitute the group C3v (cf.
Figure 5.3). There are three distinct types of
elements:
Identity. The element e does nothing: it
maps each vertex into itself.
Rotations. Two rotations C¬±
3 about the
center of the triangle through ¬±2ùúã‚àï3
radians.
ReÔ¨Çections. There are three reÔ¨Çections ùúéi,
each in a straight line through the cen-
ter of the triangle and the vertex i (i =
1, 2, 3, cf. Figure 5.3).
These elements can be deÔ¨Åned by their
action on the vertices of the triangle. For
example
C+
3
( 1
2
3
2
3
1
) ‚é°
‚é¢
‚é¢‚é£
0
1
0
0
0
1
1
0
0
‚é§
‚é•
‚é•‚é¶
.
(5.14)
The Ô¨Årst description (( )) says that the rota-
tion C+
3 maps vertex 1 to vertex 2, 2 ‚Üí3 and
3 ‚Üí1. The second description ([ ]) can be
understood as follows:
‚é°
‚é¢
‚é¢‚é£
2
3
1
‚é§
‚é•
‚é•‚é¶
=
‚é°
‚é¢
‚é¢‚é£
0
1
0
0
0
1
1
0
0
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
1
2
3
‚é§
‚é•
‚é•‚é¶
.
(5.15)
The group multiplication law can be repre-
sented through a 6 √ó 6 matrix (Cayley mul-
tiplication table) that describes the output
of gi ‚àògj, with gi listed by column and gj by
row:
gj
e
C+
3
C‚àí
3
ùúé1
ùúé2
ùúé3
gi
e
e
C+
3
C‚àí
3
ùúé1
ùúé2
ùúé3
C+
3
C+
3
C‚àí
3
e
ùúé2
ùúé3
ùúé1
C‚àí
3
C‚àí
3
e
C+
3
ùúé3
ùúé1
ùúé2
ùúé1
ùúé1
ùúé3
ùúé2
e
C‚àí
3
C+
3
ùúé2
ùúé2
ùúé1
ùúé3
C+
3
e
C‚àí
3
ùúé3
ùúé3
ùúé2
ùúé1
C‚àí
3
C+
3
e.
(5.16)
22
11
ùúé3
ùúé3
ùúé1
ùúé1
ùúé2
ùúé2
33
C+
3
C+
3
C‚àí
3
C‚àí
3
Figure 5.3
The group of the equilateral triangles
consists of (a) the identity group element e; (b) two
rotations C¬±
3 by ¬±2ùúã‚àï3 about the centroid of the
triangle; and (c) three reÔ¨Çections ùúéi in straight lines
between the centroid and each of the vertices i.

168
5 Group Theory
This table makes clear that the group is not
commutative: C‚àí
3 = ùúé1 ‚àòùúé2 ‚â†ùúé2 ‚àòùúé1 = C+
3 .
The partition of the six elements in this
group into three subsets of geometrically
equivalent transformations is typical of
any group. These subsets are called classes.
Classes are deÔ¨Åned by the condition
Class‚à∂{h1, h2, ‚Ä¶} g ‚àòhi ‚àòg‚àí1 =hj all g ‚ààG.
(5.17)
All elements in the same class have essen-
tially the same properties. They are equiv-
alent under a group transformation. The
three classes for the Ô¨Ånite group C3v are
{e} , {C+
3 , C‚àí
3
} , {ùúé1, ùúé2, ùúé3
}.
It is clear from the group multiplica-
tion table (5.16) that C3v has a number
of (proper) subgroups: three subgroups of
order two
{
e, ùúé1
} , {e, ùúé2
} , {e, ùúé3
} and one
of order three {e, C+
3 , C‚àí
3
}. For technical
reasons, the single element {e} and the
entire group C3v are also considered to be
subgroups of C3v (they are not proper sub-
groups). Whenever a group G has a sub-
group H, it is always possible to write each
group element in G as the product of an
element in H with ‚Äúsomething else‚Äù: gi =
hjCk. For example, if H is the subgroup
of order three we can choose the two ele-
ments C1, C2 (2 = 6‚àï3) as
{
e, ùúé1
}. Then
from (5.16)
{e, C+
3 , C‚àí
3
} ‚àòe = {e, C+
3 , C‚àí
3
}
{e, C+
3 , C‚àí
3
} ‚àòùúé1 = {ùúé1 ùúé2, ùúé3
} .
(5.18)
Since in some sense G is a product of group
elements in the subgroup H with group ele-
ments in C (G = H ‚àòC), we can formally
write C as the ‚Äúquotient‚Äù of G by the sub-
group H: C = H‚àñG. If we composed in
the reversed order: G = CH, then we could
write C = G‚àïH.
The set C is called a coset. It is not unique,
but for Ô¨Ånite groups, its order (number of
elements in the set) is unique: the quotient
of the order G by the order of H. A coset
may or may not be a group, depending
whether the subgroup H is invariant in G
(gHg‚àí1 ‚äÇH for all g ‚ààG) or not.
Remarks.
When G and H are Lie groups of dimen-
sions dG and dH, G‚àïH is a manifold of
dimension dG‚àïdH, and under a broad set of
conditions this manifold has a geometric
structure imparted by a Riemannian metric
derived from the geometric properties of
the two groups G and H [3, 4].
5.4.1.3
Cyclic Groups Cn
The cyclic group consists of all rotations
of the circle into itself through the angle
2ùúã‚àïn radians, and integer multiples of this
angle. There are n such elements. The rota-
tion through 2ùúãk‚àïn radians is obtained by
applying the ‚Äúsmallest‚Äù rotation (also called
Cn or C1
n) k times. This smallest rotation is
called a generator of the group. The group is
commutative. There are therefore as many
classes as group elements. The group ele-
ments can be put in 1 ‚à∂1 correspondence
with the complex numbers and also with
real 2 √ó 2 matrices:
[ei2ùúãk‚àïn] 1√ó1
‚ÜêCk
n
2√ó2
‚Üí
[
cos 2ùúãk
n
sin 2ùúãk
n
‚àísin 2ùúãk
n
cos 2ùúãk
n .
]
(5.19)
with k = 0, 1, 2, ‚Ä¶ , n ‚àí1 or k = 1, 2, ‚Ä¶ , n.
Every element in the group can be obtained
by multiplying C1
n by itself. In the same way,
the 1 √ó 1 complex matrix with k = 1 is the
generator for the 1 √ó 1 matrix representa-
tion of this group and the 2 √ó 2 real matrix
with k = 1 is the generator for the 2 √ó 2
matrix representation of the group.
5.4.1.4
Permutation Groups Sn
Permutation groups act to interchange
things. For example, if we have n numbers
1, 2, 3, ‚Ä¶ , n,
each
permutation
group

5.4 Examples of Discrete Groups
169
element will act to scramble the order of
the integers diÔ¨Äerently. Two useful ways
to describe elements in the permutation
group are shown in (5.14) for the per-
mutation group on three vertices of an
equilateral triangle. In the Ô¨Årst case, the
extension of this notation for individual
group elements consists of a matrix with
two rows, the top showing the ordering
before the element is applied, the bottom
showing the ordering after the group ele-
ment has been applied. In the second case
shown in (5.14) the extension consists of
n √ó n matrices with exactly one +1 in each
row and each column. The order of Sn is n!.
Permutation groups are noncommutative
for n > 2. S3 = C3v.
The permutation group plays a fun-
damental role in both mathematics and
physics. In mathematics, it is used to label
the irreducible tensor representations of
all Lie groups of interest. In physics, it is
required to distinguish among diÔ¨Äerent
states that many identical particles (either
bosons or fermions) can assume.
5.4.1.5
Generators and Relations
If G is a discrete group, with either a Ô¨Ånite
or a countable number of group elements, it
is useful to introduce a small set of genera-
tors
{
ùúé1, ùúé2, ‚Ä¶ , ùúék
} to describe the group.
Every element in the group can be rep-
resented as a product of these generators
and/or their inverses in some order.
For example, if there is only one gen-
erator
{ùúé}
and
every
group
element
can be written in the form gn = ùúén,
n = ‚Ä¶ , ‚àí2, ‚àí1, 0, 1, 2, ‚Ä¶ then G has a
countable number of group elements. It
is called a free group with one generator.
If there are two generators {ùúé1, ùúé2
}, the
two generators commute ùúé1ùúé2 = ùúé2ùúé1, and
every group element can be expressed
in the form gm,n = ùúém
1 ùúén
2 (m, n integers),
the group is the free group with two
commuting generators. Free groups with
k > 2 generators are deÔ¨Åned similarly.
Free groups with 1, 2, 3, ‚Ä¶ generators are
isomorphic to groups that act on periodic
lattices in 1, 2, 3, ‚Ä¶ dimensions.
Often the generators satisfy relations.
For example, a single generator ùúémay
satisfy the relation ùúép = I. Then there
are exactly p distinct group elements
I = ùúé0, ùúé1, ùúé2, ‚Ä¶ , ùúép‚àí1. The group with one
generator and one relation is the cyclic
group
Cp.
Generators
{
ùúé1, ùúé2, ‚Ä¶ , ùúék
}
and
relations
fl({ùúé1, ùúé2, ‚Ä¶ , ùúék
}) = I,
l = 1, 2, ‚Ä¶
have
been
used
to
deÔ¨Åne
many diÔ¨Äerent groups. In fact, every
discrete group is either deÔ¨Åned by a
set of generators and relations, or else
a subgroup of such a group. The sym-
metric group Sn
is deÔ¨Åned by n ‚àí1
generators ùúéi, i = 1, 2, ‚Ä¶ , n ‚àí1 and the
relations
ùúé2
i = I, ùúéiùúéj = ùúéjùúéi if j ‚â†i ¬± 1,
and
ùúéiùúéi+1ùúéi = ùúéi+1ùúéiùúéi+1.
The
tetra-
hedral (T), octahedral (O), and icosa-
hedral
(I)
point
groups
are
deÔ¨Åned
by two generators and three relations:
ùúé2
1 = I, ùúé3
2 = I, (ùúé1ùúé2)p = I with p = 3, 4, 5,
respectively. The quaternion group Q8 can
be deÔ¨Åned with two generators and two
relations ùúé1ùúé2ùúé1 = ùúé2, ùúé2ùúé1ùúé2 = ùúé1 or in
terms of three generators and four relations
ùúé4
1 = ùúé4
2 = ùúé4
3 = I, ùúé1ùúé2ùúé3 = I. In the latter
case, the three generators can be chosen as
2 √ó 2 matrices that are the three Pauli spin
matrices, multiplied by i =
‚àö
‚àí1.
The study of discrete groups deÔ¨Åned by
generators and relations has a long and very
rich history [9].
5.4.2
InÔ¨Ånite Discrete Groups
5.4.2.1
Translation Groups: One
Dimension
Imagine a series of points at locations na
along the straight line, where a is a physical

170
5 Group Theory
parameter with dimensions of length ([a] =
L) and n is an integer. The group that leaves
this set invariant consists of rigid displace-
ments through integer multiples of the fun-
damental length. The element Tka displaces
the point at na to position (n + k)a. This
group has a single generator Ta, and Tka =
Ta ‚àòTa‚àò¬∑ ¬∑ ¬∑ ‚àòTa = Tk
a. It is convenient to
represent these group elements by 2 √ó 2
matrices
Tka ‚Üí
[ 1
ka
0
1
]
.
(5.20)
In this representation, group composition
is equivalent to matrix multiplication. The
group is commutative. The generator for
the group and this matrix representation is
obtained by setting k = 1. There is also an
entire set of 1 √ó 1 complex matrix represen-
tations indexed by a real parameter p with
generator Ta ‚Üí
[
eipa]. The representations
with p‚Ä≤ = p + 2ùúã‚àïa are equivalent, so all the
inequivalent complex representations can
be parameterized by real values of p in the
range 0 ‚â§p < 2ùúã‚àïa or, more symmetrically
‚àíùúã‚àïa ‚â§p ‚â§ùúã‚àïa, with the end points iden-
tiÔ¨Åed. The real parameter p is in the dual
space to the lattice, called the Ô¨Årst Brillouin
zone.
5.4.2.2
Translation Groups: Two
Dimensions
Now imagine a series of lattice points in a
plane at positions x = i1f1 + i2f2. Here i1, i2
are integers and the vectors f1, f2 are not
colinear but otherwise arbitrary. Then the
set of rigid displacements (j1, j2) move lat-
tice points x to new locations as per
Tj1f1+j2f2
(i1f1 + i2f2
)
= (i1 + j1)f1 + (i2 + j2)f2.
(5.21)
Generalizing (5.20), there is a simple 1 ‚à∂1
(or faithful) matrix representation for this
group of rigid translations:
Tj1f1+j2f2 ‚Üí
‚é°
‚é¢
‚é¢‚é£
1
0
j1|f1|
0
1
j2|f2|
0
0
1
‚é§
‚é•
‚é•‚é¶
.
(5.22)
Extension to groups of rigid displacements
of lattices in higher dimensions is straight-
forward.
5.4.2.3
Space Groups
When |f1| = |f2| and the two vectors are
orthogonal, rotations through kùúã‚àï2 (k =
1, 2, 3) radians about any lattice point map
the lattice into itself. So also do reÔ¨Çections
in lines perpendicular to |f1| and |f2| as well
as lines perpendicular to ¬±|f1| ¬± |f2|. This
set of group elements contains displace-
ments, rotations, and reÔ¨Çections. It is an
example of a two-dimensional space group.
There are many other space groups in two
dimensions and very many more in three
dimensions. These groups were Ô¨Årst used
to enumerate the types of regular lattices
that nature allows in two and three dimen-
sions [7]. After the development of quan-
tum mechanics, they were used in another
way (depending on the theory of represen-
tations): to give names to wavefunctions
that describe electrons (and also phonons)
in these crystal lattices.
5.5
Examples of Matrix Groups
5.5.1
Translation Groups
The group of rigid translations of points in
R3 through distances a1 in the x-direction,
a2 in the y-direction, and a3 in the z-
direction can be described by simple block
4 √ó 4 (4 = 3 + 1) matrices:

5.5 Examples of Matrix Groups
171
Ta1,a2,a3 ‚Üí
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
1
0
0
a1
0
1
0
a2
0
0
1
a3
0
0
0
1
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
.
(5.23)
If the a belong to a lattice, the group is
discrete. If they are continuous, the group
is continuous and has dimension three.
5.5.2
Heisenberg Group H3
The Heisenberg group H3 plays a fun-
damental role in quantum mechanics.
As it appears in the quantum theory it
is
described
by
‚ÄúinÔ¨Ånite-dimensional‚Äù
matrices. However, the group itself is three
dimensional. In fact, it has a simple faithful
description in terms of 3 √ó 3 matrices
depending on three parameters:
h(a, b, c) =
‚é°
‚é¢
‚é¢‚é£
1
a
c
0
1
b
0
0
1
‚é§
‚é•
‚é•‚é¶
.
(5.24)
The
matrix
representation
is
faithful
because any matrix of the form (5.24)
uniquely deÔ¨Ånes the abstract group element
h(a, b, c). The group is not commutative.
The group multiplication law can be easily
seen via matrix multiplication:
h1h2 = h3 = h(a3, b3, c3)
=
‚é°
‚é¢
‚é¢‚é£
1
a1 + a2
c1 + c2 + a1b2
0
1
b1 + b2
0
0
1
‚é§
‚é•
‚é•‚é¶
.
(5.25)
The group composition law given in (5.25)
deÔ¨Ånes the Heisenberg group. The result
c3 = c1 + c2 + a1b2
leads to remarkable
noncommutativity
properties
among
canonically
conjugate
variables
in
the
quantum theory: [p, x] = ‚Ñè‚àïi.
5.5.3
Rotation Group SO(3)
The set of rigid rotations of R3 forms a
group. It is conveniently represented by
a faithful 3 √ó 3 matrix. The 3 √ó 3 matrix
describing rotations about an axis of unit
length ÃÇn through an angle ùúÉ, 0 ‚â§ùúÉ‚â§ùúã
is
(ÃÇn, ùúÉ) ‚ÜíI3 cos ùúÉ+ ÃÇn ‚ãÖL sin ùúÉ+
‚é°
‚é¢
‚é¢‚é£
ÃÇn1
ÃÇn2
ÃÇn3
‚é§
‚é•
‚é•‚é¶
√ó [
ÃÇn1
ÃÇn2
ÃÇn3
] (1 ‚àícos ùúÉ).
(5.26)
Here L is a set of three 3 √ó 3 angular
momentum matrices
Lx =
‚é°
‚é¢
‚é¢‚é£
0
0
0
0
0
1
0
‚àí1
0
‚é§
‚é•
‚é•‚é¶
Ly =
‚é°
‚é¢
‚é¢‚é£
0
0
‚àí1
0
0
0
1
0
0
‚é§
‚é•
‚é•‚é¶
Lz =
‚é°
‚é¢
‚é¢‚é£
0
1
0
‚àí1
0
0
0
0
0
‚é§
‚é•
‚é•‚é¶
.
(5.27)
The matrix multiplying (1 ‚àícos ùúÉ) in (5.26)
is a 3 √ó 3 matrix: it is the product of a
3 √ó 1 column matrix with a 1 √ó 3 row
matrix. We will show later how this mar-
velous expression has been derived (cf.
(5.51)).
There is a 1:1 correspondence between
points in the interior of a ball of radius
ùúãand rotations through an angle in the
range 0 ‚â§ùúÉ< ùúã. Two points on the sur-
face (ÃÇn, ùúã) and (‚àíÃÇn, ùúã) describe the same
rotation. The parameter space describ-
ing this group is not a simply connected
submanifold of R3: it is a doubly con-
nected manifold. The relation between
continuous groups and their underlying
parameter space involves some fascinating
topology.

172
5 Group Theory
5.5.4
Lorentz Group SO(3, 1)
The Lorentz group is the group of linear
transformations that leave invariant the
square of the distance between two nearby
points in space‚Äìtime: (cdt, dx, dy, dz) and
(cdt‚Ä≤, dx‚Ä≤, dy‚Ä≤, dz‚Ä≤). The distance can be
written in matrix form:
(cdùúè)2 = (cdt)2 ‚àí(dx2 + dy2 + dz2)
= [
cdt
dx
dy
dz
]
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
+1
0
0
0
0
‚àí1
0
0
0
0
‚àí1
0
0
0
0
‚àí1
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
cdt
dx
dy
dz
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
.
(5.28)
If the inÔ¨Ånitesimals in the primed coor-
dinate system are related to those in the
unprimed coordinate system by a linear
transformation ‚Äì dx
‚Ä≤ùúá= Mùúá
ùúàdxùúà‚Äì then the
matrices M must satisfy the constraint (t
means matrix transpose and the summa-
tion convention has been used: doubled
indices are summed over.)
MtI1,3M = I1,3,
(5.29)
where
I1,3
is
the
diagonal
matrix
diag(+1, ‚àí1, ‚àí1, ‚àí1).
The
matrices
M
belong to the orthogonal group O(1, 3).
This is a six-parameter group. Clearly
the rotations (three dimensions worth)
form a subgroup, represented by matrices
of the form
[ ¬±1
0
0
¬±R(ÃÇn, ùúÉ)
]
, where
R(ÃÇn, ùúÉ) is given in (5.26). This group has
four
disconnected
components,
each
connected to a 4 √ó 4 matrix of the form
[ 1
0
0
I3
]
,
[ 1
0
0
‚àíI3
]
,
[ ‚àí1
0
0
I3
]
, or
[ ‚àí1
0
0
‚àíI3
]
. We choose the component
connected to the identity I4. This is the
special Lorentz group SO(1, 3). A general
matrix in this group can be written in the
form
SO(1, 3) = B(ùõÉ)R(ùõâ),
(5.30)
where the matrices B(ùõÉ) describe boost
transformations
and
R(ùõâ) = R(ÃÇn, ùúÉ).
A
boost transformation maps a coordinate
system at rest to a coordinate moving with
velocity v = cùõÉand with axes parallel to
those in the stationary coordinate system.
Since every group element in SO(1, 3)
can be expressed as the product of a rota-
tion element with a boost, we can formally
write boost transformations as elements in
a coset: B(ùõÉ) = SO(1, 3)‚àïSO(3).
A general boost transformation can be
written in the form
B(ùõÉ) =
[
ùõæ
ùõæùõÉ
ùõæùõÉ
I3 + (ùõæ‚àí1)
ùõΩiùõΩj
ùõÉ‚ãÖùõÉ
]
.
(5.31)
For example, a boost in the x-direction with
v‚àïc = (ùõΩ, 0, 0) has the following eÔ¨Äect on
coordinates (ùõΩ= |ùõÉ|):
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
ct
x
y
z
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
‚Ä≤
=
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
ùõæ
ùõæùõΩ
0
0
ùõæùõΩ
ùõæ
0
0
0
0
1
0
0
0
0
1
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
ct
x
y
z
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
=
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
ùõæ(ct + ùõΩx)
ùõæ(x + ùõΩct)
y
z
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
.
(5.32)

5.6 Lie Groups
173
Here ùõæ2 ‚àí(ùõΩùõæ)2 = 1 so ùõæ= 1‚àï
‚àö
1 ‚àíùõΩ2. In
the nonrelativistic limit, x‚Ä≤ = ùõæ(x + ùõΩct) ‚Üí
x + vt, so ùõΩhas an interpretation of ùõÉ= v‚àïc.
The product of two boosts in the same
direction is obtained by matrix multiplica-
tion. This can be carried out on a 2 √ó 2 sub-
matrix of that given in (5.32):
B(ùõΩ1)B(ùõΩ2) =
[ ùõæ1
ùõΩ1ùõæ1
ùõΩ1ùõæ1
ùõæ1
] [ ùõæ2
ùõΩ2ùõæ2
ùõΩ2ùõæ2
ùõæ2
]
=
[
ùõætot
ùõΩtotùõætot
ùõΩtotùõætot
ùõætot
]
.
(5.33)
Simple matrix multiplication shows ùõΩtot =
(ùõΩ1 + ùõΩ2)‚àï(1 + ùõΩ1ùõΩ2), which is the relativis-
tic velocity addition formula for parallel
velocity transformations.
When the boosts are not parallel, their
product is a transformation in SO(1, 3) that
can be written as the product of a boost
with a rotation:
B(ùõÉ1)B(ùõÉ2) = B(ùõÉtot)R(ùõâ).
(5.34)
Multiplying two boost matrices of the
form given in (5.31) leads to a simple
expression for ùõætot and a more complicated
expression for ùõÉtot
ùõætot = ùõæ1ùõæ2(1 + ùõÉ1 ‚ãÖùõÉ2)
ùõætotùõÉtot =
[
ùõæ1ùõæ2 + (ùõæ1 ‚àí1)ùõæ2
ùõÉ1 ‚ãÖùõÉ2
ùõÉ1 ‚ãÖùõÉ1
]
ùõÉ1 + ùõæ2ùõÉ2.
(5.35)
This shows what is intuitively obvious:
the boost direction is in the plane of the
two boosts. Less obvious is the rotation
required by noncollinear boosts. It is
around an axis parallel to the cross product
of the two boosts. When the two boosts are
perpendicular, the result is
ÃÇn sin(ùúÉ) = ‚àíùõÉ1 √ó ùõÉ2 ‚ãÖ
ùõæ1ùõæ2
1 + ùõæ1ùõæ2
.
(5.36)
When one of the boosts is inÔ¨Ånitesimal,
we Ô¨Ånd
B(ùõÉ)B(ùõøùõÉ) = B(ùõÉ+ dùõÉ)R(ÃÇndùúÉ).
(5.37)
Multiplying out these matrices and com-
paring the two sides gives
dùõÉ
=
ùõæ‚àí1ùõøùõÉ+
(ùõæ‚àí1 ‚àí1
ùõæùõΩ2
)
(ùõÉ‚ãÖùõøùõÉ)ùõÉ
ÃÇndùúÉ
=
(1 ‚àíùõæ‚àí1
ùõΩ2
)
ùúπùú∑√ó ùú∑.
(5.38)
In the nonrelativistic limit, when ùõÉis also
small, 1 ‚àíùõæ‚àí1‚àïùõΩ2 ‚Üí1‚àï2. This (in)famous
factor of 1/2 is known as the ‚ÄúThomas fac-
tor‚Äù in atomic physics.
5.6
Lie Groups
The group elements g in a Lie group are
parameterized by points x in a manifold
Óàπn of dimension n: g = g(x), x ‚ààÓàπn.
The product of two group elements g(x)
and g(y) is parameterized by a point z
in the manifold: g(x)‚àòg(y) = g(z), where
z = z(x, y). This composition law can be
very complicated. It is necessarily nonlin-
ear (cf. (5.25) for H3) unless the group is
commutative. For example, the parameter
space for the group SO(3) consists of points
in R3: ùõâ= (ÃÇn, ùúÉ). Only a compact subspace
consisting of a sphere of radius ùúãis needed
to parameterize this Lie group.
Almost all of the Lie groups of use to
physicists exist as matrix groups. For this
reason, it is possible for us to skip over the
fundamental details of whether the compo-
sition law must be analytic and the elegant
details of their deÔ¨Ånition and derivations. A
composition law can be determined as fol-
lows:

174
5 Group Theory
1. Construct a useful way to parameterize
each group element as a matrix
depending on a suitable number of
parameters: x ‚Üíg(x).
2. Perform matrix multiplication
g(x)‚àòg(y) of the matrices representing
the two group elements.
3. Find the parameters z = z(x, y) of the
group element that corresponds to the
product of the two matrices given in
Step 2.
We list several types of matrix groups
below.
‚Ä¢ GL(n;R), GL(n;C), GL(n;Q). These
general linear groups (general means
det ‚â†0) consist of n √ó n invertible
matrices, each of whose n2 matrix
elements are real numbers, complex
numbers, or quaternions. The group
composition law is matrix
multiplication. The numbers of real
parameters required to specify an
element in these groups are
n2, 2 √ó n2, 4 √ó n2, respectively.
‚Ä¢ SL(n;R), SL(n;C). The special linear
groups (‚ÄúS‚Äù for special means det = +1)
are subgroups of GL(n; R) and GL(n; C)
containing the subgroup of matrices
with determinant +1. The real
dimensions of these groups are
(n2 ‚àí1) √ó dim(F) where dim(F) = (1, 2)
for F = (R, C).
‚Ä¢ O(n), U(n), Sp(n). Three important
classes of groups are deÔ¨Åned by placing
quadratic constraints on matrices. The
orthogonal group O(n) is the subgroup
of GL(n; R) containing only matrices M
that satisfy MtInM = In. Here we use
previously introduced notation: In is the
unit n √ó n matrix and t signiÔ¨Åes the
transpose of the matrix. This constraint
arises in a natural way when requiring
that linear transformations in a real
n-dimensional linear vector space
preserve a positive-deÔ¨Ånite inner
product. The unitary group U(n) is the
subgroup of GL(n; C) for which the
matrices M satisfy M‚Ä†InM = In, where ‚Ä†
signiÔ¨Åes the adjoint, or complex
conjugate transpose matrix. The
symplectic group Sp(n) is deÔ¨Åned
similarly for the quaternions. In this
case, ‚Ä† signiÔ¨Åes quaternion conjugate
transpose. The real dimensions of these
groups are n(n ‚àí1)‚àï2, n2, and n(2n + 1),
respectively.
‚Ä¢ SO(n), SU(n). For the group O(n), the
determinant of any group element is a
real number whose modulus is +1: that
is , ¬±1. Placing the special constraint on
the group of orthogonal transformations
reduces the ‚Äúnumber‚Äù of elements in the
group by one half (in a measure
theoretic sense) but does not reduce the
dimension of the space required to
parameterize the elements in this group.
For the group U(n), the determinant of
any group element is a complex number
whose modulus is +1: that is, eiùúô. Placing
the special constraint on U(n) reduces
the dimension by one: dim SU(n) = dim
U(n) ‚àí1 = n2 ‚àí1. The symplectic group
Sp(n) has determinant +1.
‚Ä¢ O(p,q), U(p,q), Sp(p,q). These groups are
deÔ¨Åned by replacing In in the deÔ¨Ånitions
for O(n), U(n), Sp(n) by the matrix
Ip,q =
[ +Ip
0
0
‚àíIq
]
. These groups
preserve an indeÔ¨Ånite nonsingular
metric in linear vector spaces of
dimension (p + q). The groups
O(n), U(n), Sp(n) are compact (a useful
topological concept) and so are relatively
easy to deal with. This means eÔ¨Äectively
that only a Ô¨Ånite volume of parameter
space is required to parameterize every
element in the group. The groups
O(p, q), U(p, q), Sp(p, q) are not compact

5.7 Lie Algebras
175
if both p and q are nonzero. Further,
O(p, q) ‚âÉO(q, p) by a simple similarity
transformation, and similarly for the
others.
5.7
Lie Algebras
A Lie algebra is a linear vector space on
which an additional composition law [, ] is
deÔ¨Åned. If X, Y, Z are elements in a Lie alge-
bra Óà∏, then linear combinations are in the
Lie algebra: ùõºX + ùõΩY ‚ààÓà∏(this is the linear
vector space property), the commutator of
two elements [X, Y] is in Óà∏, and, in addi-
tion, the new composition law satisÔ¨Åes the
following axioms:
[ùõºX + ùõΩY, Z] = ùõº[X, Z] + ùõΩ[Y, Z] ,
[X, Y] + [Y, X] = 0,
[X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y]] = 0.
(5.39)
The Ô¨Årst of these conditions preserves
the linear vector space property of Óà∏. The
second condition deÔ¨Ånes the commutator
bracket [, ] as an antisymmetric composi-
tion law: [X, Y] = ‚àí[Y, X], and the third
imposes an integrability constraint called
the Jacobi identity.
In principle, commutators are deÔ¨Åned by
the properties presented in (5.39), whether
or not composition of the operators X
and Y is deÔ¨Åned. If this composition is
deÔ¨Åned, then [X, Y] = XY ‚àíYX and the
commutator can be computed by apply-
ing the operators X and Y in diÔ¨Äerent
orders and subtracting the diÔ¨Äerence. For
example, if X = yùúïz ‚àízùúïy (here ùúïz = ùúï‚àïùúïz)
and if Y = zùúïx ‚àíxùúïz, then the operator
XY can be applied to a general func-
tion
whose
second
partial
derivatives
are
continuous
to
give
XY f (x, y, z) =
yfx + yzfxz ‚àíz2fxy ‚àíxyfzz + zxfyz. The value
of YXf (x, y, z) is computed similarly, and
the diÔ¨Äerence is (XY ‚àíYX)f = yfx ‚àíxfy =
(yùúïx ‚àíxùúïy)f . Since this holds for an arbi-
trary
function
f (x, y, z)
for
which
all
second partial derivatives exist and are
independent of the order taken, we Ô¨Ånd
[X, Y] = (XY ‚àíYX) = (yùúïx ‚àíxùúïy).
5.7.1
Structure Constants
When the underlying linear vector space
for Óà∏has dimension n, it is possible to
choose a set of n basis vectors (matrices,
operators) Xi. The commutation relations
are encapsulated by a set of structure con-
stants Ck
ij that are deÔ¨Åned by
[Xi, Xj
] = Ck
ijXk.
(5.40)
A Lie algebra is deÔ¨Åned by its structure
constants.
5.7.2
Constructing Lie Algebras by Linearization
The Lie algebra for a Lie group is con-
structed by linearizing the constraints that
deÔ¨Åne the Lie group in the neighborhood
of the identity I. Matrix Lie algebras are
obtained for n √ó n matrix Lie groups by lin-
earizing the matrix group in the neighbor-
hood of the unit matrix In. A Lie group and
its Lie algebra have the same dimension.
In the neighborhood of the identity the
groups, GL(n; R), GL(n; C), and GL(n, Q)
have the form
GL(n; F) ‚ÜíIn + ùõøM.
(5.41)
where ùõøM is an n √ó n matrix, all of whose
matrix elements are small. Over the real,
complex, and quaternion Ô¨Åelds the matrix
elements are small real or complex num-
bers or small quaternions. Quaternions q

176
5 Group Theory
can be expressed as 2 √ó 2 complex matrices
using the Pauli spin matrices ùúéùúá:
q ‚Üí(c0, c1) = (r0, r1, r2, r3) =
3
‚àë
ùúá=0
rùúáùúéùúá
=
[ r0 + ir3
r1 ‚àíir2
r1 + ir2
r0 ‚àíir3
]
. (5.42)
The
Lie
algebras
ùî§ùî©(n; F)
of
GL(n; F)
have
dimensions
dim(F) √ó n2,
with
dim(F) = 1, 2, 4 = 22 for F = R, C, Q.
For the special linear groups, the deter-
minant of a group element near the identity
is
det(In + ùõøM) = 1 + trùõøM + h.o.t.
(5.43)
In order to ensure the unimodular condi-
tion, the Lie algebras of the special linear
groups consist of traceless matrices. The Lie
algebra ùî∞ùî©(n; R) of SL(n; R) consists of real
traceless n √ó n matrices. It has dimension
n2 ‚àí1. The Lie algebra ùî∞ùî©(n; C) of SL(n; C)
consists of traceless complex n √ó n matri-
ces. It has real dimension 2n2 ‚àí2.
Many Lie groups are deÔ¨Åned by a
metric-preserving condition: M‚Ä†GM = G,
where G is some suitable metric matrix
(see the discussion of the Lorentz group
SO(3, 1) ‚âÉSO(1, 3)
that
preserves
the
metric (+1, +1, +1, ‚àí1) in Section 5.5.4
and the groups O(p,q), U(p,q), Sp(p,q)
in Section 5.6). The linearization of this
condition is
M‚Ä†GM = (In + ùõøM)‚Ä†G(In + ùõøM) = G
so that from M‚Ä†GM = G, it follows that,
neglecting small terms of order two
ùõøM‚Ä†G + GùõøM = 0.
(5.44)
Thus the Lie algebras ùî∞ùî¨(n; R), ùî∞ùî≤(n; C),
ùî∞ùî≠(n; Q), which correspond to the case G =
In, consist of real antisymmetric matrices
Mt = ‚àíM, complex traceless antihermitian
matrices M‚Ä† = ‚àíM, and quaternion anti-
hermitian matrices M‚Ä† = ‚àíM, respectively.
The Lie algebra ùî∞ùî¨(3) of the rotation
group SO(3) = SO(3; R) consists of real
3 √ó 3 antisymmetric matrices. This group
and its algebra are three dimensional. The
Lie algebra (it is a linear vector space) is
spanned by three ‚Äúbasis vectors.‚Äù These are
3 √ó 3 antisymmetric matrices. A standard
choice for these basis vectors is given in
(5.27). Their commutation relations are
given by
[Li, Lj
] = ‚àíùúñijkLk.
(5.45)
The structure constants for ùî∞ùî¨(3) are
Ck
ij = ‚àíùúñijk, 1 ‚â§i, j, k ‚â§3. Here ùúñijk is the
‚Äúsign symbol‚Äù (antisymmetric Levi‚ÄìCivita
3-tensor), which is zero if any two symbols
are the same, +1 for a cyclic permuta-
tion of integers (123), and ‚àí1 for a cyclic
permutation of (321).
Two Lie algebras with the same set of
structure constants are isomorphic [3‚Äì6].
The Lie algebra of 2 √ó 2 matrices obtained
from ùî∞ùî≤(2) is spanned by three operators
that can be chosen as (i‚àï2) times the Pauli
spin matrices (cf. (5.42)):
S1 = i
2
[ 0
1
1
0
]
S2 = i
2
[ 0
‚àíi
+i
0
]
S3 = i
2
[ 1
0
0
‚àí1
]
.
(5.46)
These three operators satisfy the commuta-
tion relations
[Si, Sj
] = ‚àíùúñijkSk.
(5.47)
As a result, the Lie algebra for the group
ùî∞ùî¨(3) of rotations in R3 is isomorphic to
the Lie algebra ùî∞ùî≤(2) for the group of
unimodular metric-preserving rotations in
a complex two-dimensional space, SU(2).

5.7 Lie Algebras
177
Spin and orbital rotations are intimately
connected.
5.7.3
Constructing Lie Groups by
Exponentiation
The mapping of a Lie group, with a com-
plicated nonlinear composition, down to a
Lie algebra with a simple linear combinato-
rial structure plus a commutator, would not
be so useful if it were not possible to undo
this mapping. In eÔ¨Äect, the linearization is
‚Äúundone‚Äù by the exponential map. For an
operator X the exponential is deÔ¨Åned in the
usual way:
exp(X) = eX = I + X + X2
2! + X3
3! + ¬∑ ¬∑ ¬∑
=
‚àû
‚àë
k=0
Xk
k! .
(5.48)
The radius of convergence of the exponen-
tial function is inÔ¨Ånite. This means that we
can map a Lie algebra back to its parent Lie
group in an algorithmic way.
We
illustrate
with
two
important
examples. For the Ô¨Årst, we construct a
simple
parameterization
of
the
group
SU(2) by exponentiating its Lie algebra.
The Lie algebra is given in (5.46). DeÔ¨Åne
M = i‚àï2ÃÇn ‚ãÖùõîùúÉ. Then M2 = ‚àí(ùúÉ‚àï2)2I2 is a
diagonal matrix. The exponential expan-
sion can be rearranged to contain even
powers in one sum and odd powers in
another:
eM = I2
(
1 ‚àí(ùúÉ‚àï2)2
2!
+ (ùúÉ‚àï2)4
4!
‚àí¬∑ ¬∑ ¬∑
)
+
M
(
1 ‚àí(ùúÉ‚àï2)2
3!
+ (ùúÉ‚àï2)4
5!
‚àí¬∑ ¬∑ ¬∑
)
.
(5.49)
The even terms sum to cos(ùúÉ‚àï2) and the
odd terms sum to sin(ùúÉ‚àï2)‚àï(ùúÉ‚àï2). The
result is
exp
( i
2 ÃÇn ‚ãÖùõîùúÉ
)
= cos ùúÉ
2I2 + iÃÇn ‚ãÖùõîsin ùúÉ
2.
(5.50)
A similar power series expansion involv-
ing the angular momentum matrices in
(5.27) leads to the parameterization of the
rotation group elements given in (5.26).
SpeciÔ¨Åcally, exp (ÃÇn ‚ãÖLùúÉ) =
I3 cos ùúÉ+ ÃÇn ‚ãÖL sin ùúÉ
+ [
ÃÇn1
ÃÇn2
ÃÇn3
] ‚é°
‚é¢
‚é¢‚é£
ÃÇn1
ÃÇn2
ÃÇn3
‚é§
‚é•
‚é•‚é¶
(1 ‚àícos ùúÉ).
(5.51)
The Lie groups SO(3) and SU(2) possess
isomorphic Lie algebras. The Lie algebra
is three dimensional. The basis vectors in
ùî∞ùî¨(3) can be chosen as the angular momen-
tum matrices given in (5.27) and the basis
vectors for ùî∞ùî≤(2) as i‚àï2 times the Pauli
spin matrices, as in (5.46). A point in the
Lie algebra (e.g., R3) can be identiÔ¨Åed by a
unit vector ÃÇn and a radial distance from the
origin ùúÉ. Under exponentiation, the point
(ÃÇn, ùúÉ) maps to the group element given in
(5.51) for SO(3) and in (5.50) for SU(2).
The simplest way to explore how the Lie
algebra parameterizes the two groups is to
look at how points along a straight line
through the origin of the Lie algebra map to
elements in the two groups. For simplicity,
we choose the z-axis. Then (ÃÇz, ùúÉ) maps to
[ eiùúÉ‚àï2
0
0
e‚àíiùúÉ‚àï2
]
‚ààSU(2),
‚é°
‚é¢
‚é¢‚é£
cos ùúÉ
sin ùúÉ
0
‚àísin ùúÉ
cos ùúÉ
0
0
0
1
‚é§
‚é•
‚é•‚é¶
‚ààSO(3).
(5.52)
As ùúÉincreases from 0 to 2ùúã, the SU(2)
group element varies from +I2 to ‚àíI2, while
the SO(3) group element starts at I3 and
returns to +I3. The SU(2) group element

178
5 Group Theory
returns to the identity +I2 only after ùúÉ
increases from 2ùúãto 4ùúã. The rotations by
ùúÉand ùúÉ+ 2ùúãgive the same group element
in SO(3) but they describe group elements
in SU(2) that diÔ¨Äer by sign: (ÃÇz, 2ùúã+ ùúÉ) =
‚àíI2 √ó (ÃÇz, ùúÉ). Two 2 √ó 2 matrices M and ‚àíM
in SU(2) map to the same group element in
SO(3). In words, SU(2) is a double cover of
SO(3).
For SU(2), all points inside a sphere of
radius 2ùúãin the Lie algebra map to diÔ¨Äer-
ent group elements, and all points on the
sphere surface map to one group element
‚àíI2. The group SU(2) is simply connected.
Any path starting and ending at the same
point (for example, the identity) can be con-
tinuously contracted to the identity.
By contrast, for SO(3), all points inside
a sphere of radius ùúãin the Lie algebra
map to diÔ¨Äerent group elements, and two
points (ÃÇn, ùúã) and ‚àí(ÃÇn, ùúã) at opposite ends
of a straight line through the origin map to
the same group element. The group SO(3)
is not simply connected. Any closed path
from the origin that cuts the surface ùúÉ= ùúã
once (or an odd number of times) cannot be
continuously deformed to the identity. The
group SO(3) is doubly connected.
This is the simplest example of a strong
theorem by Cartan. There is a 1 ‚à∂1 relation
between Lie algebras and simply connected
Lie groups. Every Lie group with the same
(isomorphic) Lie algebra is either simply
connected or else the quotient (coset) of the
simply connected Lie group by a discrete
invariant subgroup.
For matrix Lie groups, discrete invari-
ant subgroups consist of scalar multiples
of the unit matrix. For the isomorphic
Lie algebras ùî∞ùî≤(2) = ùî∞ùî¨(3), the Lie group
SU(2) is simply connected. Its discrete
invariant subgroup consists of multiples
of the identity matrix: {I2, ‚àíI2
}. Cartan‚Äôs
theorem states SO(3) = SU(2)‚àï{I2, ‚àíI2
}.
This
makes
explicit
the
2 ‚Üì1
nature
of
the
relation
between
SU(2)
and
SO(3).
The group SU(3) is simply connected.
It discrete invariant subgroup consists of
{I3, ùúîI3, ùúî2I3
}, with ùúî3 = 1. The only other
Lie group with the Lie algebra ùî∞ùî≤(3) is
the 3 ‚Üì1 image SU(3)‚àï{I3, ùúîI3, ùúî2I3
}. This
group has a description in terms of real
eight-dimensional matrices (‚Äúthe eightfold
way‚Äù).
5.7.4
Cartan Metric
The notation for the structure constants Ck
ij
for a Lie algebra gives the them appearance
of being components of a tensor. In fact,
they are: the tensor is Ô¨Årst-order contravari-
ant (in k) and second-order covariant, and
antisymmetric, in i, j. It is possible to form
a second-order symmetric covariant tensor
(Cartan‚ÄìKilling metric) from the compo-
nents of the structure constant by double
cross contraction:
gij =
‚àë
rs
Cs
irCr
js = gji.
(5.53)
This real symmetric tensor ‚Äúlooks like‚Äù
a metric tensor. In fact, it has very pow-
erful properties. If g‚àó‚àóis nonsingular,
the Lie algebra, and its Lie group, is
‚Äúsemisimple‚Äù or ‚Äúsimple‚Äù (these are tech-
nical terms meaning that the matrices
describing the Lie algebras are either
fully reducible or irreducible). If g‚àó‚àóis
negative deÔ¨Ånite, the group is compact.
It is quite remarkable that an algebraic
structure gives such powerful topological
information.
As an example, for SO(3) and SU(2) the
Cartan‚ÄìKilling metric equation (5.53) is
gij =
‚àë
r,s
(‚àíùúñirs)(‚àíùúñjsr) = ‚àí2ùõøij.
(5.54)

5.7 Lie Algebras
179
For the real forms SO(2, 1) of SO(3)
and SU(1, 1) of SU(2), the Cartan‚ÄìKilling
metric tensor is
g (ùî∞ùî¨(2, 1)) = g (ùî∞ùî≤(1, 1))
= 2
‚é°
‚é¢
‚é¢‚é£
+1
0
0
0
+1
0
0
0
‚àí1
‚é§
‚é•
‚é•‚é¶
.
(5.55)
The structure of this metric tensor (two
positive diagonal elements or eigenval-
ues, and one negative) tells us about the
topology of the groups: they have two
noncompact directions and one compact
direction. The compact direction describes
the compact subgroups SO(2) and U(1),
respectively.
5.7.5
Operator Realizations of Lie Algebras
Each Lie algebra has three useful opera-
tor realizations. They are given in terms
of boson operators, fermion operators, and
diÔ¨Äerential operators.
Boson annihilation operators bi and cre-
ation operators b‚Ä†
j for independent modes
i, j = 1, 2, ‚Ä¶, their fermion counterparts
fi, f ‚Ä†
j , and the operators ùúïi, xj satisfy the fol-
lowing commutation or anticommutation
relations
[
bi, b‚Ä†
j
]
=
bib‚Ä†
j ‚àíb‚Ä†
j bi
=
ùõøij,
{
fi, f ‚Ä†
j
}
=
fif ‚Ä†
j + f ‚Ä†
j fi
=
ùõøij,
[ùúïi, xj
]
=
ùúïixj ‚àíxjùúïi
=
ùõøij.
(5.56)
In spite of the fact that bosons and diÔ¨Äeren-
tial operators satisfy commutation relations
and fermion operators satisfy anticommu-
tation (see the + sign in (5.56)) relations,
bilinear combinations Zij = b‚Ä†
i bj, f ‚Ä†
i fj, xiùúïj of
these operators satisfy commutation rela-
tions:
[Zij, Zrs
] = Zisùõøjr ‚àíZrjùõøsi.
(5.57)
These commutation relations can be
used to associate operator algebras to
matrix Lie algebras. The procedure is sim-
ple. We illustrate this for boson operators.
Assume A, B, C = [A, B] are n √ó n matrices
in a matrix Lie algebra. Associate operator
Óà≠to matrix A by means of
A ‚ÜíÓà≠= b‚Ä†
i Aijbj
(5.58)
and similarly for other matrices. Then
[Óà≠, ÓàÆ] = [b‚Ä†
i Aijbj, b‚Ä†
rBrsbs
]
= b‚Ä†
i [A, B]is bs = b‚Ä†
i Cisbs = ÓàØ.
(5.59)
This result holds if the bilinear combina-
tions of boson creation and annihilation
operators are replaced by bilinear combi-
nations of fermion creation and annihila-
tion operators or products of multiplication
(by xi) and diÔ¨Äerentiation (by ùúïj) operators.
One consequence of this matrix Lie alge-
bra to operator algebra isomorphism is that
any Hamiltonian that can be expressed in
terms of bilinear products of creation and
annihilation operators for either bosons or
fermions can be studied in a simpler matrix
form.
The operator algebra constructed from
the spin operators in (5.46) has been used
by Schwinger for an elegant construction
of all the irreducible representations of
the Lie group SU(2) (cf. Section 5.10.5 and
Figure 5.4).
We use the matrix-to-operator mapping
now to construct a diÔ¨Äerential operator
realization of the Heisenberg group, given
in (5.24). Linearizing about the identify
gives a three-dimensional Lie algebra of the
form
‚é°
‚é¢
‚é¢‚é£
0
l
d
0
0
r
0
0
0
‚é§
‚é•
‚é•‚é¶
= lL + rR + dD.
(5.60)

180
5 Group Theory
0
0
n2
n1
J‚àí = a‚Ä†
2a1
J+ = a‚Ä†
1a2
J‚àí
J+
|n1n2>
1
1
2
2
3
3
4
4
5
5
6
Figure 5.4
Angular momentum
operators J have isomorphic
commutation relations with
speciÔ¨Åc bilinear combinations b‚Ä†
i bj
of boson creation and annihilation
operators for two modes. The
occupation number for the Ô¨Årst
mode is plotted along the x-axis
and that for the second mode
is plotted along the y-axis. The
number-conserving operators
act along diagonals similarly
to the operators J+, J‚àí, Jz to
easily provide states and matrix
elements for the ùî∞ùî≤(2) operators.
Here L, R, D are 3 √ó 3 matrices. The only
nonzero commutator is [L, R] = D. The
corresponding diÔ¨Äerential operator algebra
is
[
x
y
z
] ‚é°
‚é¢
‚é¢‚é£
0
l
d
0
0
r
0
0
0
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
ùúïx
ùúïy
ùúïz
‚é§
‚é•
‚é•‚é¶
= lÓà∏+ rÓàæ+ dÓà∞.
(5.61)
The three diÔ¨Äerential operators are
Óà∏= xùúïy Óàæ= yùúïz Óà∞= xùúïz.
(5.62)
Among these operators, none depends on
z (so ùúïz has nothing to operate on) and
none contains ùúïx, so that in essence x is
an irrelevant variable. A more economical
representation of this algebra is obtained
by zeroing out the cyclic variables z, ùúïx and
replacing their duals by ùúïz, x by +1 (duality
is under the commutator [ùúïi, xj
] = ùõøij)
[
1
y
0
] ‚é°
‚é¢
‚é¢‚é£
0
l
d
0
0
r
0
0
0
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
0
ùúïy
1
‚é§
‚é•
‚é•‚é¶
= lÓà∏‚Ä≤ + rÓàæ‚Ä≤ + dÓà∞‚Ä≤,
(5.63)
Óà∏‚Ä≤ = ùúïy,
Óàæ‚Ä≤ = y,
Óà∞‚Ä≤ = 1.
(5.64)
In essence, we have zeroed out the oper-
ators coupled to the vanishing rows and
columns of the matrix Lie algebra and
replaced their dual variables by 1. The
representation
given
is
essentially
the
Heisenberg representation of the position
(y) and conjugate momentum (py ‚âÉùúïy)
operators in quantum mechanics.
5.7.6
Disentangling Results
It happens surprisingly often in distantly
related Ô¨Åelds of physics that expressions of
the form ex+ùúïx are encountered. Needless
to say, these are not necessarily endearing
to work with. One approach to simplifying
computations involving such operators is
to rewrite the operator in such a way that
all diÔ¨Äerential operators ùúïx act Ô¨Årst, and all
multiplications by x act last. One way to
eÔ¨Äect this decomposition is to cross one‚Äôs
Ô¨Ångers and write this operator as eax+bùúïx ‚âÉ
eaxebùúïx and hope for the best. Of course, this
does not work, since the operators x and ùúïx
do not commute.
Exponential
operator
rearrangements
are called disentangling theorems. Since

5.8 Riemannian Symmetric Spaces
181
the
exponential
mapping
is
involved,
powerful methods are available when the
operators in the exponential belong to a
Ô¨Ånite-dimensional Lie algebra. Here is the
algorithm:
1. Determine the Lie algebra.
2. Find a faithful Ô¨Ånite-dimensional matrix
representation of this Lie algebra.
3. Identify how you want the operators
ordered in the Ô¨Ånal product of
exponentials.
4. Compute this result in the faithful
matrix representation.
5. Lift this result back to the operator
form.
Here is how this algorithm works. The
operators x and ùúïx have one nonzero com-
mutator [ùúïx, x] = 1. These three operators
close under commutation. They therefore
form a Lie algebra. This is the algebra ùî•3 of
the Heisenberg group, (5.64). We also have
a faithful matrix representation of this Lie
algebra, given in (5.63). We make the iden-
tiÔ¨Åcation
eax+bùúïx ‚Üíexp
‚é°
‚é¢
‚é¢‚é£
0
b
0
0
0
a
0
0
0
‚é§
‚é•
‚é•‚é¶
=
‚é°
‚é¢
‚é¢‚é£
1
b
ab
2
0
1
a
0
0
1
‚é§
‚é•
‚é•‚é¶
.
(5.65)
Now we identify this matrix with
erxedIelùúïx ‚Üí
‚é°
‚é¢
‚é¢‚é£
1
0
0
0
1
r
0
0
1
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
1
0
d
0
1
0
0
0
1
‚é§
‚é•
‚é•‚é¶
‚é°
‚é¢
‚é¢‚é£
1
l
0
0
1
0
0
0
1
‚é§
‚é•
‚é•‚é¶
.
(5.66)
By multiplying out the three matrices in
(5.66) and comparing with the matrix
elements of the 3 √ó 3 matrix in (5.65), we
learn that l = b, r = a, d = ab‚àï2. Porting
the results of this matrix calculation back
to the land of operator algebras, we Ô¨Ånd
eax+bùúïx = eaxeab‚àï2ebùúïx.
(5.67)
We
will
use
this
expression
in
Section 5.14.7 to construct a generating
function for the Hermite polynomials.
5.8
Riemannian Symmetric Spaces
A Riemannian symmetric space is a mani-
fold on which a Positive-deÔ¨Ånite metric can
be deÔ¨Åned everywhere. In addition, at each
point p there is an isometry (transforma-
tion that leaves distances between points
unchanged) that (i) leaves p Ô¨Åxed; (ii) is
not the identity; and (iii) whose square is
the identity. It was discovered by Cartan
that Riemannian symmetric spaces are very
closely related to Lie groups. SpeciÔ¨Åcally,
they are quotients of Lie groups by certain
Lie subgroups [3, 4, 10]. We illustrate with
some examples.
The Lie algebra for the Lorentz group
consists of 4 √ó 4 matrices:
ùî∞ùî¨(1, 3) =
‚é°
‚é¢
‚é¢
‚é¢
‚é¢‚é£
0
ùõº1
ùõº2
ùõº3
ùõº1
0
ùúÉ3
‚àíùúÉ2
ùõº2
‚àíùúÉ3
0
ùúÉ1
ùõº3
ùúÉ2
‚àíùúÉ1
0
‚é§
‚é•
‚é•
‚é•
‚é•‚é¶
.
(5.68)
The Cartan‚ÄìKilling metric for SO(1, 3) is
given by the trace of the product of this
matrix with itself:
g(ùî∞ùî¨(1, 3)) = 2(ùõº2
1 + ùõº2
2 + ùõº2
3 ‚àíùúÉ2
1 ‚àíùúÉ2
2 ‚àíùúÉ2
3).
(5.69)
The
subalgebra
of
rotations
ùî∞ùî¨(3)
describes the compact subgroup SO(3).
The remaining three inÔ¨Ånitesimal gen-
erators parameterized by ùõºi
span the

182
5 Group Theory
noncompact part of this group, the coset
SO(1, 3)‚àïSO(3), and exponentiate to boost
elements.
Cartan has pointed out that it is often
possible to Ô¨Ånd a linear transformation, T,
of a Lie algebra ùî§to itself whose square
is the identity: T ‚â†I, T2 = I. Such a T has
two eigenspaces, ùî®and ùî≠, with Tùî§= T(ùî®‚äï
ùî≠) = ùî®‚äñùî≠. The two subspaces are orthogo-
nal under the Cartan metric and satisfy the
commutation relations:
[
ùî®, ùî®
]
‚äÜùî®
[
ùî®, ùî≠
]
‚äÜùî≠
[
ùî≠, ùî≠
]
‚äÜùî®.
(5.70)
When this is possible, exp(ùî®) = K is a
subgroup of G and exp(ùî≠) = P = G‚àïK.
Further, if the Cartan‚ÄìKilling metric is
negative deÔ¨Ånite on ùî®, and positive deÔ¨Å-
nite on ùî≠, then K is a maximal compact
subgroup of G and the coset P = G‚àïK
is
a
Riemannian
symmetric
space.
A
Riemannian symmetric space is homo-
geoeous: every point looks like every
other point. It is not necessarily isotropic:
every direction looks like every other
direction. Spheres are homogeneous and
isotropic.
For the Lorentz group SO(1, 3), by
Cartan‚Äôs criterion, SO(3) is the maximal
compact subgroup and the coset of boost
transformations
B(ùõÉ) = SO(1, 3)‚àïSO(3)
is
a
three-dimensional
Riemannian
space with positive-deÔ¨Ånite metric. In
this case, the space is a 3-hyperboloid
(ct)2 ‚àíx2 ‚àíy2 ‚àíz2 = cst.
embedded
in
R4.
The
metric
on
this
space
is
obtained by moving the metric (1, 1, 1)
at
the
origin
(x, y, z) = (0, 0, 0)
over
the embedded space using the set of
Lorenz
group
transformations
in
the
quotient
space
SO(1, 3)‚àïSO(3).
Car-
tan also
showed that all
Riemannian
symmetric spaces arise as quotients of
(simple) Lie groups by maximal compact
subgroups.
5.9
Applications in Classical Physics
Group theory‚Äôs Ô¨Årst important role in
physics
came
even
before
quantum
mechanics was discovered. The two pil-
lars of classical deterministic physics are
mechanics and electrodynamics. Group
theory played a fundamental role in rec-
tifying the diÔ¨Éculties in describing the
interactions between these two Ô¨Åelds. The
principle tool used, besides group theory,
was Galileo‚Äôs principle of relativity and an
assumption about the underlying elegance
of physical theories.
5.9.1
Principle of Relativity
The principle of relativity posits as follows:
Two observers, S and S‚Ä≤, observe the same
physical system. Each knows how his coor-
dinate system is related to the other‚Äôs ‚Äì that
is, the transformation of coordinates that
maps one coordinate system into the other.
Assume both observers collect data on the
same physical system. Given the data that
S takes, and the coordinate transformation
from S to S‚Ä≤, S can predict the data that S‚Ä≤
has recorded. And he will be correct.
Essentially, without this ability to com-
municate data among observers, there
would be little point in pursuing the
scientiÔ¨Åc method.
A second assumption that is used is usu-
ally not stated explicitly. This assumption is
as follows: the quantitative formulation of
physical laws is simple and elegant (what-
ever that means).
5.9.2
Making Mechanics and Electrodynamics
Compatible
The quantitative formulation of mechanics
for a single particle in an inertial frame is

5.9 Applications in Classical Physics
183
dp
dt = F(x),
(5.71)
where p is deÔ¨Åned by p = mdx‚àïdt for a par-
ticle with Ô¨Åxed mass m. The transforma-
tions from one inertial coordinate system to
another consist of displacements in space
d, displacements in time d, rigid rotations
R, and boosts with constant velocity v that
keep the axis parallel. The space and time
coordinates in S‚Ä≤ are related to those in S
by
x‚Ä≤
=
Rx + vt + d
t‚Ä≤
=
t + d.
(5.72)
In the inertial coordinate system S‚Ä≤, New-
ton‚Äôs equations are
dp‚Ä≤
dt = F‚Ä≤(x‚Ä≤)
p‚Ä≤
=
Rp
F‚Ä≤
=
RF.
(5.73)
The equations of motion have the same vec-
torial form in both inertial coordinate sys-
tems (the simple and elegant assumption).
Newton‚Äôs laws were incredibly successful
in describing planetary motion in our solar
system, so when Maxwell developed his
laws for electrodynamics, it was natural
to assume that they also retained their
form under the set of inertial coordinate
transformations given in (5.72). Apply-
ing these transformations to Maxwell‚Äôs
equations creates a big mess. But if one
looks only at signals propagating along or
opposite the direction of the velocity v,
these assumptions predict
(c dt)‚Ä≤ ‚Üíc‚Ä≤ dt,
c‚Ä≤ = c ¬± |v|.
(5.74)
The round trip time for a light sig-
nal traveling in a cavity of length L as
seen by S is 2L‚àïc, while in S‚Ä≤ the time
lapse was predicted to be L‚àï(c + v) + L‚àï
(c ‚àív) = (2L‚àïc)‚àï(1 ‚àíùõΩ2), with ùõΩ= |v|‚àïc.
This predicted diÔ¨Äerence in elapsed
round trip times in ‚Äúrest‚Äù and ‚Äúmoving‚Äù
frames was thought to enable us to deter-
mine how fast the earth was moving in
the Universe. As ever more precise mea-
surements in the late nineteenth and early
twentieth century led to greater disappoint-
ment, more and more bizarre explanations
were created to ‚Äúexplain‚Äù this null result.
Finally, Einstein and Poincar√© returned to
the culprit (5.74) and asserted what the
experiments showed: c‚Ä≤ = c, so that
(c dt)‚Ä≤ ‚Üíc dt‚Ä≤,
dt‚Ä≤ = linear comb. dx, dy, dz, dt.
(5.75)
The condition that the distance function
(c dùúè)2 = (c dt)2 ‚àí(dx2 + dy2 + dz2)
= (c dt‚Ä≤)2‚àí(dx‚Ä≤2 + dy‚Ä≤2+dz‚Ä≤2)
(5.76)
is
invariant
leads
directly
to
the
transformation
law
for
inÔ¨Ånitesimals
dx‚Ä≤ùúá= Œõùúá
ùúàdxùúà, where the 4 √ó 4 matrices
belong to the Lorentz group Œõ ‚ààSO(3, 1),
Œõùúá
ùúà= ùúïx
‚Ä≤ùúá‚àïùúïxùúà. The transformation laws
taking inertial frames S to inertial frames
S‚Ä≤
involves
inhomogeneous
coordinate
transformations
[ x‚Ä≤
1
]
=
[ SO(3, 1)
d
0
1
] [ x
1
]
. (5.77)
While
Maxwell‚Äôs
equations
remain
unchanged in form under this set of
coordinate
transformations
(inhomoge-
neous Lorentz group), Newton‚Äôs force law
no longer preserves its form.
In order to Ô¨Ånd the proper form for the
laws of classical mechanics under this new
set of transformations the following two-
step process was adopted:
1. Find an equation that has the proper
transformation properties under the
inhomogeneous Lorentz group.

184
5 Group Theory
2. If the equation reduces to Newton‚Äôs
equation of motion in the nonrelati-
vistic limit ùõΩ‚Üí0, it is the proper
generalization of Newton‚Äôs equation of
motion.
Application of the procedure leads to the
relativistic equation for particle motion
dpùúá
dùúè= f ùúá,
(5.78)
where pùúáis deÔ¨Åned by pùúá= m(dxùúá‚àïdùúè).
The components of the relativistic four-
vector f ùúáare related to the three-vector
force F by
f
=
F + (ùõæ) ùú∑‚ãÖF
ùú∑‚ãÖùú∑ùú∑
f 0
=
ùõæùú∑‚ãÖF
(5.79)
(cf. (5.31)).
5.9.3
Gravitation
Einstein wondered how it could be possi-
ble to determine if you were in an iner-
tial frame. He decided that the algorithm
for responding to this question, Newton‚Äôs
Ô¨Årst law (In an inertial frame, an object at
rest remains at rest and an object in motion
remains in motion with the same velocity
unless acted upon by external forces.) was
circularly deÔ¨Åned (How do you know there
are no forces? When you are suÔ¨Éciently far
away from the Ô¨Åxed stars. How do you know
you are suÔ¨Éciently far away? When there
are no forces.)
He therefore set out to formulate the
laws of mechanics in such a way that they
were invariant in form under an arbitrary
coordinate
transformation.
While
the
Lorentz group is six dimensional, gen-
eral coordinate transformations form an
‚ÄúinÔ¨Ånite-dimensional‚Äù group. The transfor-
mation properties at any point are deÔ¨Åned
by
a
Jacobian
matrix
[(ùúïx‚Ä≤ùúá‚àïùúïxùúà)(x)].
While for the Lorentz group, this matrix
is constant throughout space, for gen-
eral coordinate transformations this 4 √ó 4
matrix is position dependent.
Nevertheless, he was able to modify the
algorithm described above to formulate
laws that are invariant under all coordinate
transformations. This two-step process
is a powerful formulation of the equiva-
lence principle. It is called the principle
of general covariance [11] It states that a
law of physics holds in the presence of a
gravitational Ô¨Åeld provided
1. The equation is invariant in form under
an arbitrary coordinate transformation
x ‚Üíx‚Ä≤(x).
2. In a locally free-falling coordinate
system, or the absence of a gravitational
Ô¨Åeld, the equation assumes the form of
a law within the special theory of
relativity.
Using these arguments, he was able to
show that the equation describing the tra-
jectory of a particle is
d2xùúá
dùúè2 = ‚àíŒìùúá
ùúà,ùúÖ
dxùúà
dùúè
dxùúÖ
dùúè.
(5.80)
The ChristoÔ¨Äel symbols are deÔ¨Åned in
terms of the metric tensor gùúá,ùúà(x) and its
inverse gùúà,ùúå(x) by
Œìùúá
ùúà,ùúÖ= 1
2gùúá,ùúå
(ùúïgùúà,ùúå
ùúïxùúÖ+
ùúïgùúå,ùúÖ
ùúïxùúà‚àí
ùúïgùúà,ùúÖ
ùúïxùúå
)
.
(5.81)
They are not components of a tensor, as
coordinate systems can be found (freely
falling, as in an elevator) in which they
vanish and the metric tensor reduces
to
its
form
in
special
relativity:
g =
diag(1, ‚àí1, ‚àí1, ‚àí1).

5.10 Linear Representations
185
Neither the left-hand side nor the right-
hand side of (5.80) is invariant under arbi-
trary coordinate changes (extra terms creep
in), but the following transformation law is
valid [11]
d2x
‚Ä≤ùúá
dùúè2 + Œì
‚Ä≤ùúá
ùúà,ùúÖ
dx
‚Ä≤ùúà
dùúè
dx
‚Ä≤ùúÖ
dùúè
=
(
ùúïx
‚Ä≤ùúá
ùúïxùúÜ
) (
d2xùúÜ
dùúè2 +ŒìùúÜ
ùúà,ùúÖ
dxùúà
dùúè
dxùúÖ
dùúè
)
. (5.82)
This means that the set of terms on the left,
or those within the brackets on the right,
have the simple transformation properties
of a four-vector. In a freely falling coordi-
nate system, the ChristoÔ¨Äel symbols vanish
and what remains is d2xùúÜ‚àïdùúè2. This special
relativity expression is zero in the absence
of forces, so the equation that describes
the trajectory of a particle in a gravitational
Ô¨Åeld is
d2xùúÜ
dùúè2 + ŒìùúÜ
ùúà,ùúÖ
dxùúà
dùúè
dxùúÖ
dùúè= 0.
(5.83)
5.9.4
ReÔ¨Çections
Two lines of reasoning have entered the
reconciliation of the two pillars of classi-
cal deterministic physics and the creation
of a theory of gravitation. One is group the-
ory and is motivated by Galileo‚Äôs principle
of relativity. The other is more vague. It is
a principle of elegance: there is the myste-
rious assumption that the structure of the
‚Äúreal‚Äù equations of physics are simple, ele-
gant, and invariant under a certain class of
coordinate transformations. The groups are
the 10-parameter inhomogeneous Lorentz
group in the case of the special theory of
relativity and the much larger group of gen-
eral coordinate transformations in the case
of the general theory of relativity. There
is every likelihood that intergalactic trav-
elers will recognize the principle of rela-
tivity but no guarantee that their sense of
simplicity and elegance will be anything like
our own.
5.10
Linear Representations
The theory of representations of groups
‚Äì more precisely the linear representa-
tions of groups by matrices ‚Äì was actively
studied by mathematicians, while physi-
cists actively ignored these results. This
picture changed dramatically with the
development of the quantum theory, the
understanding that the appropriate ‚Äúphase
space‚Äù was the Hilbert space describing a
quantum system, and that group elements
acted in these spaces through their linear
matrix representations [8].
A linear matrix representation is a map-
ping of group elements g to matrices g ‚Üí
Œì(g) that preserves the group operation:
gi ‚àògj = gk ‚áíŒì(gi) √ó Œì(gj) = Œì(gk).
(5.84)
Here, ‚àòis the composition in the group and
√ó indicates matrix multiplication. Often
the mapping is one way: many diÔ¨Äerent
group elements can map to the same matrix
(homomorphism). If the mapping is 1 ‚à∂1
the mapping is an isomorphism and the
representation is called faithful.
5.10.1
Maps to Matrices
We have already seen many matrix repre-
sentations. We have seen representations
of the two-element group Z2 as reÔ¨Çection,
rotation, and inversion matrices acting in
R3 (cf. (5.13)).
So, how many representations does a
group have? It is clear from the example
of Z2 that we can create an inÔ¨Ånite num-
ber of representations. However, if we

186
5 Group Theory
look carefully at the three representations
presented in (5.13), we see that all these
representations are diagonal: direct sums
of essentially two distinct one-dimensional
matrix representations:
Z2
e
f
Œì1
[1]
[1]
Œì2
[1]
[‚àí1]
.
(5.85)
Each of the three matrix representations of
Z2 in (5.13) is a direct sum of these two
irreducible representations:
ùúéZ
=
Œì1
‚äï
Œì1
‚äï
Œì2,
RZ(ùúã)
=
Œì1
‚äï
Œì2
‚äï
Œì2,
Óàº
=
Œì2
‚äï
Œì2
‚äï
Œì2.
(5.86)
A basic result of representation theory
is that for large classes of groups (Ô¨Ånite,
discrete, compact Lie groups), every rep-
resentation can be written as a direct
sum of irreducible representations. The
construction of this direct sum proceeds
by
matrix
diagonalization.
Irreducible
representations
are
those
that
cannot
be
further
diagonalized.
In
particular,
one-dimensional
representations
can-
not be further diagonalized. Rather than
enumerating all possible representations
of a group, it is suÔ¨Écient to enumerate
only the much smaller set of irreducible
representations.
5.10.2
Group Element‚ÄìMatrix Element Duality
The members of a group can be treated as
a set of points. It then becomes possible
to deÔ¨Åne a set of functions on this set of
points. How many independent functions
are needed to span this function space?
A not too particularly convenient choice
of basis functions are the delta functions
fi(g) = ùõø(g, gi). For example, for C3v there
are six group elements and therefore six
basis functions for the linear vector space
of functions deÔ¨Åned on this group.
Each matrix element in any representa-
tion is a function deÔ¨Åned on the members
of a group. It would seem reasonable that
the number of matrix elements in all the
irreducible representations of a group pro-
vide a set of basis functions for the function
space deÔ¨Åned on the set of group elements.
This is true: it is a powerful theorem. There
is a far-reaching duality between the ele-
ments in a group and the set of matrix ele-
ments in its set of irreducible representa-
tions. Therefore, if Œìùõº(g), ùõº= 1, 2, ‚Ä¶ are
the irreducible representations of a group G
and the dimension of Œìùõº(g) is dùõº(i.e., Œìùõº(g)
consists of dùõº√ó dùõºmatrices), then the total
number of matrix elements is the order of
the group G:
all irreps
‚àë
ùõº
d2
ùõº= |G|.
(5.87)
Further, the set of functions
‚àö
dùõº‚àï|G|Œìùõº
rs(g)
form a complete orthonormal set of func-
tions on the group space. The orthogonality
relation is
‚àë
g‚ààG
‚àö
dùõº‚Ä≤
|G|Œìùõº‚Ä≤ ‚àó
r‚Ä≤s‚Ä≤ (g)
‚àö
dùõº
|G|Œìùõº
rs(g)
= ùõø(ùõº‚Ä≤, ùõº)ùõø(r‚Ä≤, r)ùõø(s‚Ä≤, s),
(5.88)
and the completeness relation is
‚àë
ùõº
‚àë
r
‚àë
s
‚àö
dùõº
|G|Œìùõº‚àó
rs (g‚Ä≤)
‚àö
dùõº
|G|Œìùõº
rs(g)=ùõø(g‚Ä≤, g).
(5.89)
These complicated expressions can be
considerably simpliÔ¨Åed when written in the
Dirac notation. DeÔ¨Åne
‚ü®
g| ùõº
rs
‚ü©
=
‚àö
dùõº
|G|Œìùõº
rs(g),

5.10 Linear Representations
187
‚ü®ùõº
rs |g
‚ü©
=
‚àö
dùõº
|G|Œìùõº‚àó
rs (g).
(5.90)
For convenience, we have assumed that
the irreducible representations are unitary:
Œì‚Ä†(g) = Œì(g‚àí1) and ‚Ä† =t ‚àó.
In Dirac notation, the orthogonality and
completeness relations are
Orthogonality ‚à∂
‚ü®ùõº‚Ä≤
r‚Ä≤s‚Ä≤ |g
‚ü©‚ü®
g| ùõº
rs
‚ü©
=
‚ü®ùõº‚Ä≤
r‚Ä≤s‚Ä≤ | ùõº
rs
‚ü©
,
Completeness ‚à∂
‚ü®
g‚Ä≤| ùõº
rs
‚ü©‚ü®ùõº
rs |g
‚ü©
=
‚ü®
g‚Ä≤|g
‚ü©
.
(5.91)
As usual, doubled dummy indices are
summed over.
5.10.3
Classes and Characters
The group element‚Äìmatrix element dual-
ity is elegant and powerful. It leads to yet
another duality, somewhat less elegant but,
in compensation, even more powerful. This
is the character‚Äìclass duality.
We have already encountered classes
in (5.17). Two elements c1, c2 are in the
same class if there is a group element, g,
for which gc1g‚àí1 = c2. The character of
a matrix is its trace. All elements in the
same class have the same character in any
representation, for
Tr Œì(c2) = Tr Œì(gc1g‚àí1) = Tr Œì(g)Œì(c1)Œì(g‚àí1)
= Tr Œì(c1).
(5.92)
The last result comes from invariance of
the trace under cyclic permutation of the
argument matrices.
With relatively little work, the powerful
orthogonality and completeness relations
for the group elements‚Äìmatrix elements
can
be
transformed
to
corresponding
orthogonality and completeness relations
for classes and characters. If ùúíùõº(i) is the
character for elements in class i in irre-
ducible representation ùõºand ni is the
number of group elements in that class,
the character‚Äìclass duality is described by
the following relations:
Orthogonality ‚à∂
‚àë
i
niùúíùõº‚Ä≤‚àó(i)ùúíùõº(i) = |G|ùõø(ùõº‚Ä≤, ùõº),
(5.93)
Completeness ‚à∂
‚àë
ùõº
niùúíùõº‚àó(i)ùúíùõº(i‚Ä≤) = |G|ùõø(i‚Ä≤, i).
(5.94)
5.10.4
Fourier Analysis on Groups
The group C3v has six elements. Its set of
irreducible representations has a total of
six matrix elements. Therefore d2
1 + d2
2 +
¬∑ ¬∑ ¬∑ = 6. This group has three classes. By the
character‚Äìclass duality, it has three irre-
ducible representations. As a result, d1 =
d2 = 1 and d3 = 2. The matrices of the six
group elements in the three irreducible rep-
resentations are:
Œì1
Œì2
Œì3
e
[1]
[1]
[ 1
0
0
1
]
C+
3
[1]
[1]
[ ‚àía
b
‚àíb
‚àía
]
C‚àí
3
[1]
[1]
[ ‚àía
‚àíb
b
‚àía
]
ùúé1
[1]
[‚àí1]
[ ‚àí1
0
0
1
]
ùúé2
[1]
[‚àí1]
[ a
b
b
‚àía
]
ùúé3
[1]
[‚àí1]
[
a
‚àíb
‚àíb
‚àía
]
a = 1
2 b =
‚àö
3
2 .
(5.95)

188
5 Group Theory
The character table for this group is
{e}
{C+
3 , C‚àí
3
}
{ùúé1, ùúé2, ùúé3
}
1
2
3
ùúí1
1
1
1
ùúí2
1
1
‚àí1
ùúí3
2
‚àí1
0.
(5.96)
The Ô¨Årst line shows how the group ele-
ments are apportioned to the three classes.
The second shows the number of group ele-
ments in each class. The remaining lines
show the trace of the matrix representatives
of the elements in each class in each repre-
sentation. For example, the ‚àí1 in the mid-
dle of the last line is ‚àí1 = ‚àí1‚àï2 ‚àí1‚àï2. The
character of the identity group element e is
the dimension of the matrix representation,
dùõº. Observe that the rows of the table in
(5.96) satisfy the orthogonality relations in
(5.93) and the columns of the table in (5.96)
satisfy the completeness relations in (5.94).
We use this character table to perform
a Fourier analysis on representations of
this group. For example, the representa-
tion of C3v = S3 in terms of 3 √ó 3 permuta-
tion matrices is not irreducible (cf. (5.15)).
For various reasons, we might like to know
which irreducible representations of C3v are
contained in this reducible representation.
The characters of the matrices describing
each class are
{e}
{C+
3 , C‚àí
3
}
{ùúé1, ùúé2, ùúé3
}
ùúí3√ó3
3
0
1.
(5.97)
To determine the irreducible content of this
representation we take the inner product of
(5.97) with the rows of (5.96) using (5.93)
with the results
‚ü®ùúí3√ó3|ùúí1‚ü©= 1 √ó 3 √ó 1 + 2 √ó 0 √ó 1
+ 3 √ó 1 √ó 1 = 6,
‚ü®ùúí3√ó3|ùúí2‚ü©= 1 √ó 3 √ó 1 + 2 √ó 0 √ó 1
+ 3 √ó 1 √ó ‚àí1 = 0,
‚ü®ùúí3√ó3|ùúí3‚ü©= 1 √ó 3 √ó 2 + 2 √ó 0 √ó ‚àí1
+ 3 √ó 1 √ó 0 = 6.
(5.98)
As a result, the permutation representation
is reducible and ùúí3√ó3 ‚âÉùúí1 ‚äïùúí3.
5.10.4.1
Remark on Terminology
The cyclic group Cn has n group ele-
ments gk, k = 0, 1, 2, ‚Ä¶ , n ‚àí1 that can
be identiÔ¨Åed with rotations through an
angle ùúÉk = 2ùúãk‚àïn. This group is abelian. It
therefore
has
n
one-dimensional
irre-
ducible
matrix
representations
Œìm,
m = 0, 1, 2, ‚Ä¶ , n ‚àí1
whose
matrix
elements
are
Œìm(ùúÉk) = [e2ùúãikm‚àïn].
Any
function deÔ¨Åned at the n equally spaced
points at angles ùúÉk around the circle can be
expressed in terms of the matrix elements
of the unitary irreducible representations
of Cn. The study of such functions, and
their transforms, is the study of Fourier
series. This analysis method can be applied
to functions deÔ¨Åned along the real line
R1 using the unitary irreducible repre-
sentations
(UIR)
Œìk(x) = [eikx]
of
the
commutative group of translations Tx
along the real line through the distance
x. This is Fourier analysis on the real
line. This idea generalizes to groups and
their complete set of unitary irreducible
representations.
5.10.5
Irreps of SU(2)
The UIR (or ‚Äúirreps‚Äù) of Lie groups can
be constructed following two routes. One
route begins with the group. The second
begins with its Lie algebra. The second
method is simpler to implement, so we use
it here to construct the hermitian irreps of
ùî∞ùî≤(2) and then exponentiate them to the
unitary irreps of SU(2).
The Ô¨Årst step is to construct shift opera-
tors from the basis vectors in ùî∞ùî≤(2):

5.10 Linear Representations
189
S+
=
Sx + iSy
=
[ 0
1
0
0
]
S‚àí
=
Sx ‚àíiSy
=
[ 0
0
1
0
]
Sz
=
1
2
[ 1
0
0
‚àí1
]
[Sz, S¬±
]
=
¬±S¬±
[S+, S‚àí
]
=
2Sz
.
(5.99)
Next, we use the matrix algebra to operator
algebra mapping (cf. Section 5.7.5) to con-
struct a useful boson operator realization of
this Lie algebra:
S+ ‚ÜíÓàø+ = b‚Ä†
1b2
S‚àí‚ÜíÓàø‚àí= b‚Ä†
2b1
Sz ‚ÜíÓàøz = 1
2
(b‚Ä†
1b1 ‚àíb‚Ä†
2b2
) .
(5.100)
The next step introduces representations
(of a Lie algebra). Introduce a state space on
which the boson operators b1, b‚Ä†
1 act, with
basis vectors |n1‚ü©, n1 = 0, 1, 2, ‚Ä¶ with the
action given as usual by
b‚Ä†
1|n1‚ü©= |n1 + 1‚ü©
‚àö
n1 + 1,
b1|n1‚ü©= |n1 ‚àí1‚ü©‚àön1.
(5.101)
Introduce a second state space for the oper-
ators b2, b‚Ä†
2 and basis vectors |n2‚ü©, n2 =
0, 1, 2, ‚Ä¶. In order to construct the irre-
ducible representations of ùî∞ùî≤(2), we intro-
duce a grid, or lattice, of states |n1, n2‚ü©=
|n1‚ü©‚äó|n2‚ü©. The operators Óàø¬±, Óàøz are num-
ber conserving and move along the diago-
nal n1 + n2 = cnst. (cf. Figure 5.4). It is very
useful to relabel the basis vectors in this lat-
tice by two integers. One (j) identiÔ¨Åes the
diagonal, the other (m) speciÔ¨Åes position
along a diagonal:
2j = n1 + n2
n1 = j + m
2m = n1 ‚àín2
n2 = j ‚àím |n1, n2‚ü©‚Üî
|||||
j
m
‚ü©
.
(5.102)
The spectrum of allowed values of the
quantum number j is 2j = 0, 1, 2, ‚Ä¶ and
m = ‚àíj, ‚àíj + 1, ‚Ä¶ , +j.
The matrix elements of the operators Óàø
with respect to the basis |
j
m ‚ü©are con-
structed from the matrix elements of the
operators b‚Ä†
i bj on the basis vectors |n1, n2‚ü©.
For Óàøz, we Ô¨Ånd
Óàøz
|||||
j
m
‚ü©
= 1
2(b‚Ä†
1b1 ‚àíb‚Ä†
2b2) ||n1, n2‚ü©
= ||n1, n2‚ü©1
2(n1 ‚àín2) =
|||||
j
m
‚ü©
m.
(5.103)
For the shift-up operator
Óàø+
|||||
j
m
‚ü©
= b‚Ä†
1b2 ||n1, n2‚ü©
= ||n1 + 1, n2 ‚àí1‚ü©
‚àö
n1 + 1‚àön2
=
|||||
j
m + 1
‚ü©‚àö
(j + m + 1)(j ‚àím).
(5.104)
and similarly for the shift-down operator
Óàø‚àí
|||||
j
m
‚ü©
=
|||||
j
m ‚àí1
‚ü©‚àö
(j ‚àím + 1)(j + m).
(5.105)
In this representation of the (spin) angu-
lar momentum algebra ùî∞ùî≤(2), Óàøz = Jz is
diagonal and Óàø¬± = J¬± have one nonzero
diagonal
row
just
above
(below)
the
main
diagonal.
The
hermitian
irre-
ducible
representations
of
ùî∞ùî≤(2)
with

190
5 Group Theory
j = 0, 1
2, 1, 3
2, 2, 5
2, ‚Ä¶ form a complete set of
irreducible hermitian representations for
this Lie algebra.
The UIR of SU(2) are obtained by expo-
nentiating i times the hermitian represen-
tations of ùî∞ùî≤(2):
Óà∞J [SU(2)] = exp (iÃÇn ‚ãÖJùúÉ) ,
(5.106)
with Jx = (J+ + J‚àí)‚àï2 and Jy = (J+ ‚àíJ‚àí)‚àï2i,
and J‚àóare the (2j + 1) √ó (2j + 1) matri-
ces whose matrix elements are given
in (5.103‚Äì5.105). The (2j + 1) √ó (2j + 1)
matrices Óà∞J are traditionally called Wigner
matrices [8]. For many purposes, only
the character of an irreducible represen-
tation is needed. The character depends
only on the class and the class is uniquely
determined by the rotation angle ùúÉsince
rotations by angle ùúÉabout any axis ÃÇn are
geometrically equivalent. It is suÔ¨Écient
to compute the trace of any rotation, for
example, the rotation about the z-axis. This
matrix is diagonal: (eiJzùúÉ)
m‚Ä≤,m = eimùúÉùõøm‚Ä≤,m
and its trace is
ùúíj(ùúÉ) =
+j
‚àë
m=‚àíj
eimùúÉ=
sin(j + 1
2)ùúÉ
sin 1
2ùúÉ
.
(5.107)
These characters are orthonormal with
respect to the weight w(ùúÉ) = 1‚àïùúãsin2(ùúÉ‚àï2).
5.10.6
Crystal Field Theory
The type of Fourier analysis outlined above
has found a useful role in crystal (or ligand)
Ô¨Åeld theory [5, 7]. This theory was created
to describe the behavior of charged parti-
cles (electrons, ions, atoms) in the presence
of an electric Ô¨Åeld that has some symmetry,
usually the symmetry of a host crystal. We
illustrate this with a simple example.
A many-electron atom with total angular
momentum L is placed in a crystal Ô¨Åeld with
cubic symmetry. How do the 2L + 1-fold
degenerate levels split?
Before immersion in the crystal Ô¨Åeld, the
atom has spherical symmetry. Its symme-
try group is the rotation group, the irre-
ducible representations Óà∞L have dimension
2L + 1, the classes are rotations through
angle ùúÉ, and the character for the class ùúÉin
representation Óà∞L is given in (5.107) with
j ‚ÜíL (integer). When the atom is placed
in an electric Ô¨Åeld with cubic symmetry
Oh, the irreducible representations of SO(3)
become reducible. The irreps of Oh are
A1, A2, E, T1, T2. The irreducible content is
obtained through a character analysis. of
Óà∞L [SO(3)].
The group Oh has 24 elements parti-
tioned into Ô¨Åve classes. These include the
identity E, eight rotations C3 by 2ùúã‚àï3 radi-
ans about the diagonals through the oppo-
site vertices of the cube, six rotations C4 by
2ùúã‚àï4 radians about the midpoints of oppo-
site faces, three rotations C2
4 by 2ùúã‚àï2 radi-
ans about the same midpoints of opposite
faces, and six rotations C2 about the mid-
points of opposite edges. The characters for
these Ô¨Åve classes in the Ô¨Åve irreducible rep-
resentations are collected in the character
table for Oh. This is shown at the top in
Table 5.2. At the bottom left of the table are
the characters of the irreducible represen-
tations of the rotation group SO(3) in the
irreducible representations of dimension
2L + 1. These are obtained from (5.107). A
character analysis (cf. (5.98)) leads to the
Oh irreducible content of each of the low-
est six irreducible representations of SO(3),
presented at the bottom right of the table.
5.11
Symmetry Groups
Groups Ô¨Årst appeared in the quantum the-
ory as a tool for labeling eigenstates of

5.11 Symmetry Groups
191
Table 5.2
(top) Character table for the cubic group Oh.
The functions in the right-hand column are some of the
basis vectors that ‚Äúcarry‚Äù the corresponding representation.
(bottom) Characters for rotations through the indicated
angle in the irreducible representations of the rotation
group.
Oh
E
8C3
3C2
4
6C2
6C4
Basis
A1
1
1
1
1
1
r2 = x2 + y2 + z2
A2
1
1
1
‚àí1
‚àí1
E
2
‚àí1
2
0
0
(x2 ‚àíy2, 3z2 ‚àír2)
T1
3
0
‚àí1
‚àí1
1
(x, y, z), (Lx, Ly, Lz)
T2
3
0
‚àí1
1
‚àí1
(yz, zx, xy)
L ‚à∂ùúÉ
0
2ùúã
3
2ùúã
2
2ùúã
2
2ùúã
4
Reduction
0 S
1
1
1
1
1
A1
1 P
3
0
‚àí1
‚àí1
1
T1
2 D
5
‚àí1
1
1
‚àí1
E ‚äïT2
3 F
7
1
‚àí1
‚àí1
‚àí1
A2 ‚äïT1 ‚äïT2
4 G
9
0
1
1
1
A1 ‚äïE ‚äïT1 ‚äïT2
5 H
11
‚àí1
‚àí1
‚àí1
1
E ‚äï2T1 ‚äïT2
a Hamiltonian with useful quantum num-
bers. If a Hamiltonian Óà¥is invariant under
the action of a group G, then gÓà¥g‚àí1 = Óà¥,
g ‚ààG. If |ùúìùõº
ùúá‚ü©satisÔ¨Åes Schr√∂dinger‚Äôs time-
independent equation Óà¥|ùúìùõº
ùúá‚ü©‚àíE|ùúìùõº
ùúá‚ü©= 0,
so that
g(Óà¥‚àíE)|ùúìùõº
ùúá‚ü©= {g(Óà¥‚àíE)g‚àí1} g|ùúìùõº
ùúá‚ü©
= (Óà¥‚àíE)|ùúìùõº
ùúà‚ü©‚ü®ùúìùõº
ùúà|g|ùúìùõº
ùúá‚ü©
= (Óà¥‚àíE)|ùúìùõº
ùúà‚ü©Óà∞ùõº
ùúà,ùúá(g)
= 0.
(5.108)
All states |ùúìùõº
ùúà‚ü©related to each other by
a group transformation g ‚ààG (more pre-
cisely, a group representation Óà∞ùõº(g)) have
the same energy eigenvalue. The existence
of a symmetry group G for a Hamilto-
nian Óà¥provides representation labels for
the quantum states and also describes the
degeneracy patterns that can be observed.
If the symmetry group G is a Lie group, so
that g = eX, then eXÓà¥e‚àíX = Óà¥‚áí[X, Óà¥] =
0. The existence of operators X that com-
mute with the Hamiltonian Óà¥is a clear sig-
nal that the physics described by the Hamil-
tonian is invariant under a Lie group.
For example, for a particle in a spherically
symmetric potential, V(r), Schr√∂dinger‚Äôs
time-independent equation is
(p ‚ãÖp
2m + V(r)
)
ùúì= Eùúì
(5.109)
with p = (‚Ñè‚àïi)‚àá. The Hamiltonian operator
is invariant under rotations. Equivalently,
it commutes with the angular momen-
tum operators L = r √ó p: [L, Óà¥] = 0. The
wavefunctions can be partly labeled by
rotation group quantum numbers, l and
m: ùúì‚Üíùúìl
m(r, ùúÉ, ùúô). In fact, by standard
separation of variables, arguments in this
description can be made more precise:
ùúì(r, ùúÉ, ùúô) = 1‚àïrRnl(r)Y l
m(ùúÉ, ùúô). Here Rnl(r)
are radial wavefunctions that depend on

192
5 Group Theory
the potential V(r) but the angular func-
tion Y l
m(ùúÉ, ùúô) is ‚Äúa piece of geometry‚Äù: it
depends only on the existence of rotation
symmetry. It is the same no matter what
the potential is. In fact, these functions can
be constructed from the matrix represen-
tations of the group SO(3). The action of
a rotation group element g on the angular
functions is
gY l
m(ùúÉ, ùúô) = Y l
m‚Ä≤(ùúÉ, ùúô)Óà∞l
m‚Ä≤m(g),
(5.110)
where the construction of the Wigner
Óà∞
matrices
has
been
described
in
Section 5.10.5.
If the symmetry group is reduced, as
in the case of SO(3) ‚ÜìOh described in
Section 5.10.6, the eigenstates are iden-
tiÔ¨Åed by the labels of the irreducible
representations of Oh: A1, A2, E, T1, T2.
Once the states have been labeled, com-
putations must be done. At this point, the
power of group theory becomes apparent.
Matrices must be computed ‚Äì for example,
matrix elements of a Hamiltonian. Typ-
ically, most matrix elements vanish (by
group-theoretic selection rules). Of the
small number that do not vanish, many are
simply related to a small number of the oth-
ers. In short, using group theory as a guide,
only a small number of computations must
actually be done.
This feature of group theory is illustrated
by computing the eigenstates and their
energy eigenvalues for an electron in the
N = 4 multiplet of the hydrogen atom
under the inÔ¨Çuence of a constant external
Ô¨Åeld Óà±. The Hamiltonian to be diagonalized
is
‚ü®
N‚Ä≤
L‚Ä≤
M‚Ä≤
|||||
p ‚ãÖp
2m ‚àíe2
r + eÓà±‚ãÖr
|||||
N
L
M.
‚ü©
.
(5.111)
The Ô¨Årst two terms in the Hamiltonian
describe the electron in a Coulomb poten-
tial, the last is the Stark perturbation,
which describes the interaction of a dipole
d = ‚àíer with a constant external elec-
tric Ô¨Åeld: Óà¥St. = ‚àíd ‚ãÖÓà±. In the N = 4
multiplet, we set N‚Ä≤ = N = 4, so that
L‚Ä≤, L = 0, 1, 2, 3 and M ranges from ‚àíL
to +L and ‚àíL‚Ä≤ ‚â§M‚Ä≤ ‚â§+L‚Ä≤. The matrix
elements of the Coulomb Hamiltonian are
ENùõøN‚Ä≤NùõøL‚Ä≤LùõøM‚Ä≤M, with E4 = ‚àí13.6‚àï42 eV.
There are ‚àë3=4‚àí1
L=0
(2L + 1) = 16 states
in the N = 4 multiplet, so 162 matrix
elements of the 16 √ó 16 matrix must be
computed. We simplify the computation
by choosing the z-axis in the direction
of the applied uniform electric Ô¨Åeld, so
that eÓà±‚ãÖr ‚ÜíeÓà±z (Óà±= |Óà±|). In addition,
we write z =
‚àö
4ùúã‚àï3rY 1
0 (ùúÉ, ùúô). The matrix
elements factor (separation of variables)
into a radial part and an angular part, as
follows:
‚ü®4L‚Ä≤M‚Ä≤|eÓà±z|4LM‚ü©‚ÜíeÓà±√ó Radial √ó Angular
Radial = ‚à´
‚àû
0
R4L‚Ä≤(r)r1R4L(r)dr
Angular=
‚àö
4ùúã
3 ‚à´Y L‚Ä≤‚àó
M‚Ä≤ (Œ©)Y 1
0 (Œ©)Y L
M(Œ©)dŒ©,
(5.112)
where Œ© = (ùúÉ, ùúô) and dŒ© = sin ùúÉdùúÉdùúô.
Selection rules derived from SO(3) sim-
plify the angular integral. First, the inte-
gral vanishes unless ŒîM = M‚Ä≤ ‚àíM = 0. It
also vanishes unless ŒîL = ¬±1, 0. By parity,
it vanishes if ŒîL = 0, and by time reversal,
its value for M and ‚àíM are the same. The
nonzero angular integrals are
Óà≠(L, M) =
‚àö
4ùúã
3 ‚à´Œ©
Y L‚àó
M‚Ä≤(Œ©)Y 1
0 (Œ©)Y L‚àí1
M (Œ©)dŒ©
= ùõøM‚Ä≤M
‚àö
(L + M)(L ‚àíM)
(2L + 1)(2L ‚àí1), (5.113)
The useful radial integrals, those with
ŒîL = ¬±1, are all related:

5.11 Symmetry Groups
193
Óàæ(N, L) = ‚à´
‚àû
0
RN,L(r)rRN,L‚àí1dr
= N
‚àö
N2 ‚àíL2
2
‚àö
3
√ó Óàæ(2, 1)
(5.114)
with 1 ‚â§L ‚â§N ‚àí1. All integrals are pro-
portional to the single integral Óàæ(2, 1).
This comes from yet another symmetry
that the Coulomb potential exhibits (cf.
Section 5.12), not shared by other spher-
ically symmetric potentials. The single
integral to be evaluated is
Óàæ(2, 1) = ‚àí3
‚àö
3a0.
(5.115)
This integral is proportional to the Bohr
radius a0 of the hydrogen atom, whose
value was estimated in (5.6).
The arguments above show drastic sim-
pliÔ¨Åcations in the computational load for
computing the energy eigenfunctions and
eigenvalues of a many-electron atom in
a uniform external electric Ô¨Åeld (Stark
problem).
Of the 256 = 162 matrix elements to
compute, only 18 are nonzero. All are
real. Since the Hamiltonian is hermitian
(symmetric if real), there are in fact only
9 nonzero matrix elements to construct.
Each is a product of two factors, so only
6 (angular) plus 1 (radial) quantities need
be computed. These numbers must be
stuÔ¨Äed into a 16 √ó 16 matrix to be diago-
nalized. But there are no nonzero matrix
elements between states with M‚Ä≤ ‚â†M.
This means that by organizing the row
and columns appropriately the matrix can
be written in block diagonal form. The
block diagonal form consists of a 1 √ó 1
matrix for M = 3, a 2 √ó 2 matrix for M = 2,
a 3 √ó 3 matrix for M = 1, a 4 √ó 4 matrix
for M = 0, a 3 √ó 3 matrix for M = ‚àí1,
and so on. The 1 √ó 1 matrices are already
diagonal. The 2 √ó 2 matrices are identical,
so only one needs to be diagonalized.
Similarly for the two 3 √ó 3 matrices. There
is only one 4 √ó 4 matrix. The computa-
tional load for diagonalizing this matrix
has been reduced from T ‚âÉ162 log 16 to
T ‚âÉ22 log 2 + 32 log 3 + 42 log 4, a factor
of 20 (assuming the eÔ¨Äort required for
diagonalizing an n √ó n matrix goes like
n2 log n)!
It gets even better. For the N = 5 mul-
tiplet the 1 √ó 1, 2 √ó 2, 3 √ó 3, 4 √ó 4 matrices
are all proportional to the matrices of the
corresponding size for N = 4. The pro-
portionality factor is 5‚àï4. Only one new
matrix needs to be constructed ‚Äì the 5 √ó 5
matrix. This symmetry extends to all values
of N.
This is a rather simple example that can
be carried out by hand. This was done
when quantum mechanics was Ô¨Årst devel-
oped, when the fastest computer was a
greased abacus. Today, time savings of a
factor of 20 on such a simple problem
would hardly be noticed. But calculations
have also inÔ¨Çated in size. Reducing a 106 √ó
106 matrix to about 1000 103 √ó 103 matri-
ces reduces the computational eÔ¨Äort by a
factor of 2000. For example, a computa-
tion that would take 6 years without such
methods could be done in a day with these
methods.
Symmetry groups play several roles in
quantum mechanics.
‚Ä¢ They provide group representation
labels to identify the energy eigenstates
of a Hamiltonian with symmetry.
‚Ä¢ They provide selection rules that save us
the eÔ¨Äort of computing matrix elements
whose values are zero (by symmetry!).
‚Ä¢ And they allow transformation of a
Hamiltonian matrix to block diagonal
form, so that the computational load can
be drastically reduced.

194
5 Group Theory
5.12
Dynamical Groups
A widely accepted bit of wisdom among
physicists is that symmetry implies degen-
eracy, and the larger the symmetry, the
larger the degeneracy. What works forward
ought to work backward (‚ÄúNewton‚Äôs third
law‚Äù): if the degeneracy is greater than
expected, the symmetry is greater than
apparent.
5.12.1
Conformal Symmetry
The hydrogen atom has rotational symme-
try SO(3), and this requires (2L + 1)-fold
degeneracy. But the states with the same
principal quantum number N
are all
degenerate in the absence of spin and
other
relativistic
eÔ¨Äects,
and
nearly
degenerate
in
the
presence
of
these
eÔ¨Äects. It would make sense to look
for a larger than apparent symmetry. It
exists in the form of the Runge‚ÄìLenz
vector
M = 1‚àï2m(p √ó L ‚àíL √ó p) ‚àíe2r‚àïr,
where
r, p, L = r √ó p
are
the
position,
momentum, and orbital angular momen-
tum
operators
for
the
electron.
The
three orbital angular momentum oper-
ators Li and three components of the
Runge‚ÄìLenz vector close under com-
mutation to form a Lie algebra. The six
operators commute with the Hamilto-
nian, so the ‚Äúhidden‚Äù symmetry group
is
larger
than
the
obvious
symmetry
group SO(3). On the bound states, this
Lie
algebra
describes
the
Lie
group
SO(4).
The
irreducible
representation
labels for the quantum states are N,
N = 1, 2, 3, ‚Ä¶ , ‚àû. The three nested groups
SO(2) ‚äÇSO(3) ‚äÇSO(4)
and
their
rep-
resentation labels and branching rules
are
Group Rep.label Degeneracy
Branching
rules
SO(4)
N
N2
SO(3)
L
2L + 1
0, 1, 2, ‚Ä¶ ,
N ‚àí1
SO(2)
M
1
‚àíL ‚â§M‚â§+L.
(5.116)
Branching rules identify the irreducible
representations of a subgroup that any
representation of a larger group splits into
under
group‚Äìsubgroup
reduction.
We
have seen branching rules in Table 5.2.
One advantage of using the larger group
is that there are more shift operators in the
Lie algebra. The shift operators, acting on
one state, moves it to another (cf. |LM‚ü©
L+
‚àí‚Üí
|L, M + 1‚ü©). This means that there are well-
deÔ¨Åned algebraic relations among states
that belong to the same N multiplet. This
means that more of any computation can
be pushed from the physical domain to
the geometric domain, and simpliÔ¨Åcations
accrete.
Why stop there? In the hydrogen atom,
the energy diÔ¨Äerence between the most
tightly bound state, the ground state, and
the most weakly bound state (N ‚Üí‚àû)
is 13.6 eV. When this diÔ¨Äerence is com-
pared with the electron rest energy of
511 KeV, the symmetry-breaking is about
13.6‚àï511 000 ‚âÉ0.000 027 or 2.7 √ó 10‚àí3%.
This suggests that there is a yet larger group
that accounts for this near degeneracy.
Searches eventually lead to the noncom-
pact conformal group SO(4, 2) ‚äÉSO(4) ¬∑ ¬∑ ¬∑
as the all-inclusive ‚Äúsymmetry group‚Äù of
the hydrogen atom. The virtue of using
this larger group is that states in diÔ¨Äerent
multiplets N, N ¬± 1 can be connected by
shift operators within the algebra ùî∞ùî¨(4, 2),
and ultimately there is only one number to
compute [12]. Including this larger group
in (5.116) would include inserting it in the
row above SO(4), showing there is only
one representation label for bound states,

5.12 Dynamical Groups
195
indicating its degeneracy is ‚Äú‚àû‚Äù, and adding
branching rules N = 1, 2, ‚Ä¶ , ‚àûto the
SO(4) row.
5.12.2
Atomic Shell Structure
Broken symmetry beautifully accounts for
the systematics of the chemical elements.
It accounts for the Ô¨Ålling scheme as elec-
trons enter a screened Coulomb poten-
tial around a nuclear charge +Ze as the
nuclear charge increases from Z = 1 to Z >
92. The screening is caused by ‚Äúinner elec-
trons.‚Äù The Ô¨Ålling scheme accounts for the
‚Äúmagic numbers‚Äù among the chemical ele-
ments: these are the nuclear charges of
exceptionally stable chemical elements He,
Ne, Ar, Kr, Xe, Rn with atomic numbers
2, 10, 18, 36, 54, 86.
When more than one electron is present
around a nuclear charge +Ze, then the outer
electrons ‚Äúsee‚Äù a screened central charge
and the SO(4) symmetry arising from the
Coulomb nature of the potential is lost.
There is a reduction in symmetry, a ‚Äúbroken
symmetry‚Äù: SO(4) ‚ÜìSO(3). The quantum
numbers (N, L) can be used to label states
and energies, EN,L, and these energy lev-
els are (2L + 1)-fold degenerate. The SO(4)
multiplet with quantum number N splits
into orbital angular momentum multiplets
with L values ranging from L = 0 to a max-
imum of L = N ‚àí1.
Each additional electron must enter an
orbital that is not already occupied by the
Pauli exclusion principle. This principle is
enforced by the requirement that the total
electron wavefunction transforms under
the unique antisymmetric representation
Œìanti.(Sk) on the permutation group Sk for k
electrons.
Generally,
the
larger
the
L
value
the further the outer electron is from
the central charge, on average. And the
further it is, the larger is the negative
charge density contributed by inner elec-
trons that reduces the strength of the
central nuclear attraction. As a result,
EN,0 < EN,1 < ¬∑ ¬∑ ¬∑ < EN,L=N‚àí1.
There
is
mixing among levels with diÔ¨Äerent values
of N and L. The following energy ordering
scheme, ultimately justiÔ¨Åed by detailed
calculations, accounts for the systematics
of the chemical elements, including the
magic numbers:
1S | 2S 2P | 3S 3P | 4S 3D 4P | 5S 4D 5P |
6S 4F 5D 6P | 7S.
(5.117)
Each level can hold 2(2L + 1) electrons.
The Ô¨Årst factor of 2 = (2s + 1) with s = 1‚àï2
is due to electron spin. The vertical bar |
indicates a large energy gap. The cumu-
lative occupancy reproduces the magic
numbers
of
the
chemical
elements:
2, 10, 18, 36, 54, 86. The Ô¨Ålling order is
shown in Figure 5.5. Broken symmetry is
consistent with Mendeleev‚Äôs periodic table
of the chemical elements.
5.12.3
Nuclear Shell Structure
Magic numbers among nuclei suggested
that, here also, one could possibly describe
many diÔ¨Äerent nuclei with a single sim-
ple organizational structure. The magic
numbers
are:
2, 8, 20, 28, 40, 50, 82, 126,
both for protons and for neutrons. The
following model was used to organize this
information.
Assume that the eÔ¨Äective nuclear poten-
tial for protons (or neutrons) is that of
a three-dimensional isotropic harmonic
oscillator.
The
algebraic
properties
of
the
three-dimensional
isotropic
har-
monic oscillator are described by the
unitary group U(3) and its Lie algebra
ùî≤(3). The basis states for excitations can

196
5 Group Theory
7
S . . .
S P D F G
S P D F G
S P D F
S P D
S P
S
6
5
4
3
2
1
118
5 F Actmides
7 S Fr - Ra
6 P TL - Rn
5 D Lu - Hg
4 F Lanthanides
5 P In - Xe
4 D Y - Cd
5 S Rb - Sr
4 P Ga - Kr
3 D Sc - Zn
3 P AL - Ar
3 S Na - Mg
2 P B - Ne
1 S H - He
2 S Li - Be
4 S K - Ca
6 S Cs - Ba
86
54
36
18
10
2
Figure 5.5
Broken SO(4) dynamical
symmetry due to screening of the
central Coulomb potential by inner
electrons successfully accounts
for the known properties of the
chemical elements, as reÔ¨Çected in
Mendeleev‚Äôs periodic table of the
chemical elements.
be described by |n1, n2, n3‚ü©. One exci-
tation
would
be
threefold
degenerate:
|1, 0, 0‚ü©, |0, 1, 0‚ü©, |0, 0, 1‚ü©, two excitations
would be sixfold degenerate, and states
with N excitations would have a degen-
eracy
(N + 2)(N + 1)‚àï2.
Each
integer
N ‚â•0 describes an irrep of U(3). Under a
spherically symmetric perturbation, these
highly degenerate N multiplets would split
into multiplets identiÔ¨Åed by an angular
momentum index L. A character analysis
gives this branching result
U(3)
SO(3)
N
L values
Spectroscopic
0
0
S
1
1
P
2
2, 0
D, S
3
3, 1
F, P
4
4, 2, 0
G, D, S.
(5.118)
For example, the N = 4 harmonic oscilla-
tor multiplet splits into an L = 4 multiplet,
an L = 2 multiplet, and an L = 0 multi-
plet. The larger the angular momentum,

5.12 Dynamical Groups
197
the lower the energy. After this splitting,
the spin of the proton (or neutron) is cou-
pled to the orbital angular momentum to
give values of the total angular momen-
tum J = L ¬± 1‚àï2, except that for S states
only the J = 1‚àï2 state occurs. Again, the
larger angular momentum occurs at a lower
energy than the smaller angular momen-
tum. The resulting Ô¨Ålling order, analogous
to (5.117), is
0S1‚àï2|1P3‚àï2 1P1‚àï2|2D5‚àï2 2S1‚àï2 2D3‚àï2|3F7‚àï2|
3P3‚àï2 3F5‚àï2 3P1‚àï2 4G9‚àï2|
4D5‚àï2 4G7‚àï2 4S1‚àï2 4D3‚àï2 5H11‚àï2|
5H9‚àï2 5F7‚àï2 5F5‚àï2 5P3‚àï2 5P1‚àï2 6I13‚àï2|.
(5.119)
Each shell with angular momentum j can
hold up to 2j + 1 nucleons. Broken sym-
metry is also consistent with the ‚Äúperiodic
table‚Äù associated with nuclear shell mod-
els. The Ô¨Ålling order is shown in Figure 5.6
[13, 14].
6
I G D S
5P
126
82
50
28
20
8
2
7J15/2
6(Œô11/2, G9/2, 7/2 D5/2, 3/2, S1/2)
5(H9/2, F7/2, F5/2, P3/2, P1/2)
4(G7/2, D5/2, D3/2, S1/2)
6Œô13/2
5H11/2
4G9/2
3P1/2
3F5/2
3P3/2
3F7/2
2D3/2
2S1/2
2D5/2
1P1/2
1P3/2
0S1/2
4S
3P
3F
2S
2D
1P
0S
4D
4G
5F
5H
H F P
G D S
F P
D S
P
S
5
4
3
2
1
0
Figure 5.6
The Ô¨Ålling order describing very
many properties of nuclear ground states is
described by the levels of an isotropic har-
monic oscillator potential with multiplets hav-
ing N excitations and degeneracy (N + 1)(N +
2)‚àï2. The degeneracy is broken by a spheri-
cally symmetric perturbation and broken fur-
ther by spin-orbit coupling. For both perturba-
tions, energy increases as angular momentum
decreases. The Ô¨Ålling order shown success-
fully accounts for the known properties of
the ground states of most even‚Äìeven nuclei,
including the magic numbers. In the higher lev-
els, the largest spin angular momentum state
(e.g., 5H11‚àï2) is pushed down into the next
lower multiplet, containing all the remaining
N = 4 states, with the exception of the 4G9‚àï2.

198
5 Group Theory
At a group theoretical level, our start-
ing point has been the Lie algebra ùî≤(3)
with basis vectors b‚Ä†
i bj (1 ‚â§i, j ‚â§3) whose
representations are labeled by an integer
index N, the number of excitations present.
This algebra can be embedded in a larger
Lie algebra containing in addition shift-up
operators b‚Ä†
i , their counterpart annihilation
operators bj, and the identity operator I.
The Lie algebra is 9 + 2 ‚ãÖ3 + 1 = 16 = 42
dimensional, and is closely related to
the noncompact Lie algebra ùî≤(3, 1). The
embedding
ùî≤(3) ‚äÇùî≤(3, 1)
is
analogous
to the inclusion SO(4) ‚äÇSO(4, 2) for the
hydrogen atom.
5.12.4
Dynamical Models
In this section, so far we have described
the hydrogen atom using a very large group
SO(4, 2) and breaking down the symmetry
to SO(4) and further to SO(3) when there
are Coulomb-breaking perturbations that
maintain their spherical symmetry. We
have also introduced a sequence of groups
and
subgroups
U(3, 1) ‚ÜìU(3) ‚ÜìSO(3)
to provide a basis for the nuclear shell
model.
Nuclear computations are very diÔ¨É-
cult because there is ‚Äúno nuclear force.‚Äù
The force acting between nucleons is
a residual force from the quark‚Äìquark
interaction.
This
is
analogous
to
the
absence of a ‚Äúmolecular force.‚Äù There
is none ‚Äì the force that binds together
atoms in molecules is the residual elec-
tromagnetic
force
after
exchange
and
other interactions have been taken into
account.
For this reason, it would be very use-
ful to develop a systematic way for making
nuclear models and carrying out calcula-
tions within the context of these models.
Group theory comes to the rescue!
The Ô¨Årst step in creating a simple envi-
ronment for quantitative nuclear models
is to assume that pairs of nucleons bind
tightly into boson-like excitations. The
leading assumption is that of all the
nuclear-pair degrees of freedom, the most
important are those with scalar (S, L = 0)
and quadrupole (D, L = 2) transformation
properties under the rotation group SO(3).
States in a Hilbert space describing 2
protons (neutrons, nucleons) can be pro-
duced by creation operators s‚Ä†, d‚Ä†
m acting
on the vacuum |0; 0, 0, 0, 0, 0‚ü©. For n pairs
of nucleons, n creation operators act to
produce
states
|ns; n‚àí2, n‚àí1, n0, n1, n2‚ü©
with
ns + ‚àë
m nm = n.
There
are
(n + 6 ‚àí1)!‚àïn!(6 ‚àí1)!
states
in
this
Hilbert space. For computational con-
venience, they can be arranged by their
transformation properties under rotations
SO(3). For example, the two-boson Hilbert
space has 21 states consisting of an L = 0
state from s‚Ä†s‚Ä†, an L = 2 multiplet from
s‚Ä†d‚Ä†
m, and multiplets with L = 0, 2, 4 from
d‚Ä†
m‚Ä≤d‚Ä†
m.
The Hamiltonian acts within the space
with a Ô¨Åxed number of bosons. It must
therefore be constructed from number-
conserving
operators:
b‚Ä†
i bj,
where
the
boson operators include the s and d excita-
tions. These operators must be rotationally
invariant. At the linear level, only two
such operators exist: s‚Ä†s and d‚Ä†
mdm. At the
quadratic level, there are a small num-
ber of additional rotationally invariant
operators. The n-boson Hamiltonian can
therefore be systematically parameterized
by a relatively small number of terms. The
parameters can be varied in attempts to Ô¨Åt
models to nuclear spectra and transition
rates. In the two-boson example with
21 states, it is suÔ¨Écient to diagonalize
this Hamiltonian in the two-dimensional
subspace of L = 0 multiplets, in another
two-dimensional subspace with the two

5.13 Gauge Theory
199
S0(5)
S0(3)
SU(3)
S0(6)
U(6)
U(5)
Figure 5.7
States with 2N nucleons outside a closed shell
are described by N bosons in the interacting boson model.
The basis states carry a symmetric representation of the
Lie group U(6). Various limiting Hamiltonians that exhibit a
group‚Äìsubgroup symmetry can be diagonalized by hand. The
three group‚Äìsubgroup chains for which this is possible are
shown here.
states with L = 2 and ML = 2 (all other
ML values will give the same result),
and the one-dimensional subspace with
L = 4, ML = 4.
The interacting boson model (IBM)
outlined above has deeply extended our
understanding of nuclear physics [15]. In
fact, some Hamiltonians can be solved ‚Äúby
hand.‚Äù These involve a group‚Äìsubgroup
chain. The chain of groups is shown in
Figure 5.7. This model incorporates in a
magniÔ¨Åcent way the use of groups in their
capacity as symmetry groups, implying
degeneracy, and dynamical groups, imply-
ing relations among multiplets of diÔ¨Äerent
energies.
5.13
Gauge Theory
Gauge transformations were introduced
by Weyl following Einstein‚Äôs development
(1916) of the theory of general relativity.
In crude terms, Weyl‚Äôs original idea was
to introduce a ruler (the ‚Äúgauge‚Äù of gauge
theory) whose length was an arbitrary
function of position. His original objective
was to unify the two then-known forces of
nature: gravitation and electromagnetism.
His theory is quite beautiful but Einstein
raised serious objections, and Weyl eventu-
ally relinquished it. Einstein‚Äôs objection was
that if Weyl‚Äôs theory were correct then the
results of laboratory experiments would
depend on the history of the material being
investigated.
Weyl came back to this general idea fol-
lowing Schr√∂dinger‚Äôs development (1926)
of wave mechanics. In this case, a modiÔ¨Åed
objective was achieved: he succeeded in
describing how light interacts with charged
matter.
The original theory (GR) involved a
real
scaling
transformation
that
was
space‚Äìtime dependent. As a result, it is
in the same spirit as the discussion about
scaling in Section 5.2.3, but more general.
His modiÔ¨Åed theory (QM) involved a com-
plex phase transformation. In some sense
this would be an analytic continuation
of the scaling arguments, but the spirit
of the discussion given in Section 5.2.3
does not in any sense suggest phase
changes.
The starting point of this work is
the observation that if ùúì(x, t) satisÔ¨Åes
Schr√∂dinger‚Äôs time-dependent equation, so
also does eiùúôùúì(x, t), for
(
Óà¥‚àíi‚Ñèùúï
ùúït
)
eiùúôùúì(x, t)
= eiùúô(
Óà¥‚àíi‚Ñèùúï
ùúït
)
ùúì(x, t) = 0. (5.120)
This fails to be true if the phase ùúôdepends
on space‚Äìtime coordinates, for then the
derivative terms act on this phase when we
try to pull it through the Hamiltonian and

200
5 Group Theory
time-derivative operators:
((p ‚ãÖp
2m
)
+ qŒ¶(x, t) ‚àíi‚Ñèùúï
ùúït
)
eiùúô(x,t)ùúì(x, t)
= eiùúô(x,t)
((p + ‚Ñè‚àáùúô)2
2m
+ qŒ¶(x, t)
+ ‚Ñèùúïùúô
ùúït ‚àíi‚Ñèùúï
ùúït
)
ùúì(x, t).
(5.121)
Symmetry is not preserved. What can be
done?
It had long been known that the elec-
tric and magnetic Ô¨Åelds E, B could be repre-
sented by ‚ÄúÔ¨Åctitious‚Äù potentials that served
to simplify Maxwell‚Äôs equations but were
otherwise ‚Äúnot real.‚Äù The vector potential
A and scalar potential Œ¶ are related to the
‚Äúreal‚Äù Ô¨Åelds by
B
=
ùõÅ√ó A
E
=
‚àíùõÅŒ¶ ‚àí1
c
ùúïA
ùúït .
(5.122)
This simpliÔ¨Åcation is not unique. The vector
potential can be changed by the addition of
the gradient of a scalar Ô¨Åeld ùúí(x, t), and the
scalar potential correspondingly changed:
A ‚ÜíA‚Ä≤
=
A + ‚àáùúí
‚áí
B‚Ä≤ = B
Œ¶‚ÜíŒ¶‚Ä≤
=
Œ¶ ‚àí1
c
ùúïùúí
ùúït
‚áí
E‚Ä≤ = E.
(5.123)
The resolution of the diÔ¨Éculty is to
assume that the electrostatic part of the
interaction is described by the term qŒ¶(x, t)
in the Hamiltonian and the magnetic part is
represented by replacing p by p ‚àíq
c A(x, t)
wherever it appears in the Hamiltonian.
Under these conditions,
(
p ‚àíq
c A(x, t)
)
eiùúô(x,t)
= eiùúô(x,t) (
p ‚àíq
c A(x, t) + ‚Ñè‚àáùúô(x, t)
)
(5.124)
and
(
qŒ¶ ‚àíi‚Ñèùúï
ùúït
)
eiùúô(x,t)
= eiùúô(x,t)
(
qŒ¶ + ‚Ñèùúïùúô
ùúït ‚àíi‚Ñèùúï
ùúït
)
.
(5.125)
If we choose ùúô(x, t) = ‚àíq‚àï‚Ñècùúí(x, t), then
the added terms on the right in (5.124) are
p ‚àíq
c A(x, t) ‚àíq
c ‚àáùúí(x, t) = p ‚àíq
c A‚Ä≤(x, t)
(5.126)
and those on the right in (5.125) are
qŒ¶(x, t) ‚àíq
c
ùúïùúí(x, t)
ùúït
‚àíi‚Ñèùúï
ùúït
= qŒ¶‚Ä≤(x, t) ‚àíi‚Ñèùúï
ùúït .
(5.127)
The result is that the structure of the inter-
action between the electromagnetic Ô¨Åelds
and charged particles is invariant provided
the interaction is given in terms of the ‚ÄúÔ¨Åc-
titious‚Äù Ô¨Åelds A, Œ¶ by
p ‚Üíp ‚àíq
c A(x, t)
‚àíi‚Ñèùúï
ùúït ‚Üí‚àíi‚Ñèùúï
ùúït + qŒ¶(x, t).
(5.128)
There are several other ways to couple the
electromagnetic Ô¨Åeld with charged parti-
cles that are allowed by symmetry [16]. But
the structure of the interaction described
by (5.128) is suÔ¨Écient to account for all
known measurements. It turns out that
Maxwell‚Äôs equations are also a consequence
of the structure of this interaction.
This principle is called the principle of
minimal electromagnetic coupling.
The phase transformation introduced
in (5.120) belongs to the Lie group U(1).
Its generalization to position-dependent
phase eiùúô(x,t) does not belong to a Lie
group.
Questions soon surfaced whether the
same process could be used to describe

5.13 Gauge Theory
201
the interaction between more complicated
‚Äúcharged‚Äù particles and the Ô¨Åelds that
cause interactions among them. It seemed
that the proton‚Äìneutron pair was a good
candidate for such a treatment. These two
particles seemed to be essentially the same,
except that one was charged and the other
not. Neglecting charge, these two particles
could be treated as an isospin doublet. The
nucleon wavefunction ùúôcould be treated
as a two-state system, |ùúô‚ü©= | ùúìp
ùúìn
‚ü©, and
the Hamiltonian describing nuclear inter-
actions should be invariant under a global
SU(2)
transformation,
analogous
to
a
global U(1) transformation eiùúôin (5.120).
If the SU(2) rotation were allowed to vary
with position, perhaps it would be possible
to determine the nature of the interaction
between the nucleons (fermions) and the
bosons (ùúã¬±, ùúã0, analogous to photons that
carry
the
electromagnetic
interaction)
responsible for the interaction among the
fermions.
This program was carried out by Yang
and Mills. They succeeded in determin-
ing the nature of the interaction. But we
now understand that nuclear interactions
are residual forces left over from the strong
interactions among the quarks.
Nevertheless, the program persisted. The
gauge program can be phrased as follows.
1. Suppose there is a set of n fermion
Ô¨Åelds that are invariant under a
g-parameter Lie group.
2. Assume that the Hamiltonian
(Lagrangian, action integral) for these
Ô¨Åelds, without any interaction, is
known.
3. Now assume that the Lie group
parameters are allowed to be functions
on space‚Äìtime. What additional terms
occur in the Hamiltonian (cf. (5.121)
above)?
4. How many boson Ô¨Åelds must be
introduced in order to leave the
structure of the Hamiltonian invariant?
5. How must they be introduced into the
Hamiltonian? That is, what is the
structure of the ‚Äúminimal coupling‚Äù in
terms of the Lie algebra parameters (its
structure constants)?
6. How do these new Ô¨Åelds transform
under the Lie group and its space‚Äìtime
extension?
7. What Ô¨Åeld equations do the new Ô¨Åelds
satisfy?
These questions have all been answered
[17]. The number of new Ô¨Åelds required
is exactly the number of generators of the
Lie group (i.e., its dimension). Each Ô¨Åeld
is a four-component Ô¨Åeld. Their dynamical
equations are a consequence of this theory.
All new Ô¨Åelds are massless.
This theory has been applied to describe
the electroweak interaction U(2) ‚âÉU(1) √ó
SU(2) to predict the massless electromag-
netic Ô¨Åeld and three boson Ô¨Åelds called
W ¬±, Z0 that transmit the weak interaction.
This theory was also applied to describe
three quarks. The Lie group used was SU(3)
and the theory predicted the existence of
eight (that is the dimension of the Lie group
SU(3)) gluon Ô¨Åelds, all massless. The gluon
Ô¨Åelds transmit the strong interaction. In the
case of the gluons, the mass seems to be
small enough to be consistent with ‚Äúzero‚Äù
but that is deÔ¨Ånitely not the case of the very
massive weak gauge bosons W ¬±, Z0. A new
mechanism was called for, and proposed,
to describe how these ‚Äúmassless‚Äù particles
acquire such a heavy mass. This mechanism
was proposed by Higgs, among others, and
is called the Higgs mechanism. The discov-
ery of the Higgs boson was announced in
2012.

202
5 Group Theory
5.14
Group Theory and Special Functions
5.14.1
Summary of Some Properties
The classical special functions of math-
ematical physics were developed in the
nineteenth century in response to a variety
of speciÔ¨Åc physical problems. They include
the Legendre and associated Legendre
functions, the Laguerre and associated
Laguerre
functions,
the
Gegenbauer,
Chebyshev, Hermite, and Bessel functions.
They are for the most part orthogonal
polynomials. They are constructed by
choosing a basis set f0, f1, f2, ‚Ä¶ that are
monomials in the position representation
(Dirac notation): ‚ü®x|f0‚ü©= x0, ‚ü®x|f1‚ü©= x1,
‚ü®x|f2‚ü©= x2, ‚Ä¶ and then creating an orthog-
onal set by successive Gram‚ÄìSchmidt
orthogonalization by means of an inner
product ‚ü®f |g‚ü©= ‚à´b
a f ‚àó(x)g(x)w(x)dx with
various weights w(x) for the diÔ¨Äerent
functions:
|ùúô0‚ü©= |f0‚ü©,
|ùúô1‚ü©= |f1‚ü©‚àí|ùúô0‚ü©‚ü®ùúô0|
‚ü®ùúô0|ùúô0‚ü©|f0‚ü©,
|ùúôj‚ü©= |fj‚ü©‚àí
j‚àí1
‚àë
k=0
|ùúôk‚ü©‚ü®ùúôk|
‚ü®ùúôk|ùúôk‚ü©|fj‚ü©.
(5.129)
The Bessel functions are the exception to
this rule, as they are not polynomials.
These functions obey a common variety
of properties
DiÔ¨Äerential equation
g2(x)y‚Ä≤‚Ä≤ + g1(x)y‚Ä≤ + g0(x)y = 0.
(5.130a)
Recurrence relations
a1nfn+1(x)=(a2n+a3nx)fn(x)‚àía4nfn‚àí1(x).
(5.130b)
DiÔ¨Äerential relations
g2(x)dfn(x)
dx
= g1(x)fn(x) + g0(x)fn‚àí1(x).
(5.130c)
Generating functions
g(x, z) =
‚àû
‚àë
n=0
anfn(x)zn.
(5.130d)
Rodrigues‚Äô formula
fn(x) =
1
anùúå(x)
dn
dxn
{ùúå(x)(g(x))n} .
(5.130e)
The
coeÔ¨Écients
and
functions
can
be found in standard tabulations (e.g.,
Abramowitz and Stegun [18]). The Bessel
functions have similar properties.
5.14.2
Relation with Lie Groups
A Lie group lives on a manifold Óàπn
of dimension n. Each group element is
a function of position in the manifold:
g = g(x), x ‚ààÓàπ.
The
product
of
two
group elements is deÔ¨Åned by an analytic
composition law on the manifold:
g(x) ‚àòg(y) = g(z),
z = z(x, y).
(5.131)
It is not until we construct representa-
tions for the group, or on top of the man-
ifold, that really interesting things begin to
happen. Representations
g(x) ‚ÜíŒìùõº
ij(g(x))
(5.132)
are functions deÔ¨Åned on the manifold.
Suitably normalized, the set of matrix
elements for the complete set of UIR form
a complete orthonormal set of functions
on the manifold Óàπn. By duality (the mir-
acles of Hilbert space theory), the triplet
of indices ùõº, i, j is described by as many
integers as the dimension of Óàπn. For
example, for three-dimensional Lie groups,
such as SO(3), SU(2), SO(2, 1), ISO(2), H3
the matrix elements are indexed by three
integers and can be represented in the

5.14 Group Theory and Special Functions
203
form Œìùõº
ij(g(x)) = ‚ü®ùõº
i |g(x)| ùõº
j ‚ü©. Including the
appropriate normalization factor, they can
be expressed as
‚àö
dim(ùõº)
Vol(G) Œìùõº
ij(g(x)) = ‚ü®g(x)| ùõº
i, j ‚ü©. (5.133)
For noncompact groups Vol(G) is not Ô¨Ånite,
but dim(ùõº) is also not Ô¨Ånite, so the ratio
under the radical needs to be taken with
care.
Representations are powerful because
they lie in two worlds: geometric and
algebraic. They have one foot in the man-
ifold (‚ü®g(x)| ‚âÉ‚ü®x| above) and the other in
algebra ( | ùõº
ij ‚ü©‚âÉ|n‚ü©above).
All classical special functions are speciÔ¨Åc
matrix elements, evaluated on speciÔ¨Åc sub-
manifolds, of speciÔ¨Åc irreducible represen-
tations of some Lie group.
We illustrate these ideas with a few
examples without pretending we have
even scratched the surface of this vast
and fascinating Ô¨Åeld [19‚Äì21]. See also
Chapter 7.
5.14.3
Spherical Harmonics and SO(3)
For the group SU(2), the underlying man-
ifold is a solid three-dimensional sphere.
There are many ways to parameterize an
element in this group. We use an Euler-
angle-like parameterization introduced by
Wigner:
Óà∞j
mk(ùúô, ùúÉ, ùúì) =
‚ü®
j
m
|||||
e‚àíiùúôJze‚àíiùúÉJye‚àíiùúìJz
|||||
j
k
‚ü©
= e‚àíimùúôdj
mk(ùúÉ)e‚àíikùúì.
(5.134)
The orthogonality properties of the matrix
elements are
‚à´
2ùúã
0
dùúô‚à´
ùúã
0
sin ùúÉdùúÉ
‚à´
2ùúã
0
dùúìÓà∞j‚Ä≤‚àó
m‚Ä≤k‚Ä≤(ùúô, ùúÉ, ùúì)Óà∞j
mk(ùúô, ùúÉ, ùúì)
=
8ùúã2
2j + 1ùõøj‚Ä≤jùõøm‚Ä≤mùõøk‚Ä≤k.
(5.135)
The volume of the group in this param-
eterization
is
8ùúã2 = (2ùúã)(2)(2ùúã) =
(‚à´2ùúã
0
dùúô)(‚à´ùúã
0 sin ùúÉdùúÉ)(‚à´2ùúã
0
dùúì).
The
nor-
malization factor, converting the matrix
elements to a complete orthonormal set, is
‚àö
(2j + 1)‚àï8ùúã2.
In order to Ô¨Ånd a complete set of func-
tions on the sphere (ùúÉ, ùúô), we search for
those matrix elements above that are inde-
pendent of the angle ùúì. These only occur for
k = 0, which occurs only among the sub-
set of irreducible representations with j =
l (integer). Integrating out the dùúìdepen-
dence in (5.134) leads to a deÔ¨Ånition of
the spherical harmonics in terms of some
Wigner Óà∞matrix elements (cf. (5.135):
Y l
m(ùúÉ, ùúô) =
‚àö
2l + 1
4ùúã
Óà∞l‚àó
m0(ùúô, ùúÉ, ‚àí). (5.136)
These functions on the two-dimensional
unit sphere surface (ùúÉ, ùúô) inherit their
orthogonality and completeness proper-
ties from the corresponding properties
of the UIR matrix elements Óà∞j
mk
on
the
three-dimensional
solid
ball
of
radius 2ùúã.
Other special functions are similarly
related to these matrix elements. The
associated Legendre polynomials are
Pm
l (cos ùúÉ) =
‚àö
(l + m)!
(l ‚àím)! dl
0,0(ùúÉ)
(5.137)
and the Legendre polynomials are
Pl(cos ùúÉ) = Óà∞l
0,0(‚àí, ùúÉ, ‚àí) = dl
0,0(ùúÉ). (5.138)

204
5 Group Theory
These functions inherit the their measure
w(ùúÉ) from the measure on SU(2) and their
orthogonality and completeness properties
from those of the Wigner rotation matrix
elements Óà∞j
mk [SU(2)].
We emphasize again that these functions
are speciÔ¨Åc matrix elements Óà∞j
mk, evaluated
on speciÔ¨Åc submanifolds (sphere, line), of
speciÔ¨Åc irreducible representations (j = l)
of SU(2).
5.14.4
DiÔ¨Äerential and Recursion Relations
We can understand the wide variety of rela-
tions that exist among the special functions
(e.g., recursion relations, etc.) in terms of
group theory/representation theory as fol-
lows. It is possible to compute the matrix
elements of an operator Óàªin either the
continuous basis ‚ü®x‚Ä≤|Óàª|x‚ü©or the discrete
basis ‚ü®n‚Ä≤|Óàª|n‚ü©. In the Ô¨Årst basis, the coor-
dinates x describe points in a submanifold
in the group manifold Óàπn, and the oper-
ator is a diÔ¨Äerential operator. In the sec-
ond basis, the indices n are an appropriate
subset of the group representation ùõºand
row/column (i, j) index set and the operator
is a matrix with entries in the real or com-
plex Ô¨Åeld.
It is also possible to compute the matrix
elements in a mixed basis ‚ü®x|Óàª|n‚ü©. It is in
this basis that really exciting things happen,
for
‚ü®x|Óàª|n‚ü©
‚Üô
‚Üò
‚ü®x|Óàª|x‚Ä≤‚ü©‚ü®x‚Ä≤|n‚ü©= ‚ü®x|n‚Ä≤‚ü©‚ü®n‚Ä≤|Óàª|n‚ü©.
(5.139)
On the left-hand side a diÔ¨Äerential oper-
ator ‚ü®x|Óàª|x‚Ä≤‚ü©acts on the special function
‚ü®x‚Ä≤|n‚ü©, while on the right-hand side, a
matrix ‚ü®n‚Ä≤|Óàª|n‚ü©multiplies the special
functions ‚ü®x|n‚Ä≤‚ü©.
For the rotation group acting on the
sphere surface (ùúÉ, ùúô) and the choice Óàª= L¬±,
we Ô¨Ånd for ‚ü®ùúÉùúô|L¬±|
l
m ‚ü©computed as on
the left in (5.139),
e¬±iùúô
(
¬± ùúï
ùúïùúÉ+ icos ùúÉ
sin ùúÉ
ùúï
ùúïùúô
)
ùõø(cos ùúÉ‚Ä≤ ‚àícos ùúÉ)ùõø(ùúô‚Ä≤ ‚àíùúô)Y l
m(ùúÉ‚Ä≤, ùúô‚Ä≤)
= e¬±iùúô
(
¬± ùúï
ùúïùúÉ+ icos ùúÉ
sin ùúÉ
ùúï
ùúïùúô
)
Y l
m(ùúÉ, ùúô)
(5.140)
and as computed on the right
‚ü®
ùúÉùúô
|||||
l‚Ä≤
m‚Ä≤
‚ü©‚ü®
l‚Ä≤
m‚Ä≤
|||||
L¬±
|||||
l
m
‚ü©
= Y l
m¬±1(ùúÉ, ùúô)
‚àö
(l ¬± m+1)(l‚àìm). (5.141)
There are a number of Lie groups that
can be deÔ¨Åned to act on a one-dimensional
space. In such cases, the inÔ¨Ånitesimal
generators take the form of functions of
the coordinate x and the derivative d‚àïdx.
We illustrate the ideas behind diÔ¨Äerential
and recursion relations in the context of
the Heisenberg group H3. Its algebra ùî•3
is spanned by three operators, universally
identiÔ¨Åed as a, a‚Ä†, I with commutation
relations
[a, a‚Ä†] = I, [a, I] = [a‚Ä†, I] = 0.
These operators have matrix elements as
follows in the continuous basis (geometric)
representation:
‚ü®x‚Ä≤|a|x‚ü©= ùõø(x‚Ä≤ ‚àíx)
1
‚àö
2
(x + D)
‚ü®x‚Ä≤|a‚Ä†|x‚ü©= ùõø(x‚Ä≤ ‚àíx)
1
‚àö
2
(x ‚àíD)
(5.142)
‚ü®x‚Ä≤|I|x‚ü©= ùõø(x‚Ä≤ ‚àíx)
and discrete basis (algebraic) representa-
tion:
‚ü®n‚Ä≤|a|n‚ü©= ùõøn‚Ä≤,n‚àí1
‚àö
n
‚ü®n‚Ä≤|a‚Ä†|n‚ü©= ùõøn‚Ä≤,n+1
‚àö
n‚Ä≤
(5.143)
‚ü®n‚Ä≤|I|n‚ü©= ùõøn‚Ä≤,n.

5.14 Group Theory and Special Functions
205
Here D = d‚àïdx.
The special functions are the mixed basis
matrix elements ‚ü®x|n‚ü©. We can compute
these starting with the ground, or lowest,
state |0‚ü©.
‚ü®x|a|0‚ü©
‚Üô
‚Üò
‚ü®x|a|x‚Ä≤‚ü©‚ü®x‚Ä≤|0‚ü©
=
‚ü®x|n‚ü©‚ü®n|a|0‚ü©
1
‚àö
2(x + D)‚ü®x|0‚ü©
=
0.
(5.144)
This
equation
has
a
unique
solution
N‚ü®x|0‚ü©= e‚àíx2‚àï2
up
to
scale
factor,
N = 1‚àï4‚àö
ùúã.
The remaining normalized basis states
are constructed by applying the raising
operator:
‚ü®x|n‚ü©= ‚ü®x|(a‚Ä†)n
n! |x‚Ä≤‚ü©‚ü®x‚Ä≤|0‚ü©
=
(x ‚àíD)n
‚àö
2nn!
‚àö
ùúã
e‚àíx2‚àï2
= Hn(x)e‚àíx2‚àï2
‚àö
2nn!
‚àö
ùúã
.
(5.145)
The Hermite polynomials in (5.145) are
deÔ¨Åned by
Hn(x) = e+x2‚àï2(x ‚àíD)ne‚àíx2‚àï2.
(5.146)
The states ‚ü®x|n‚ü©are normalized to +1.
In order to construct the recursion rela-
tions for the Hermite polynomials, choose
Óàª= x = (a + a‚Ä†)‚àï
‚àö
2 in (5.139). Then
‚ü®x|Óàª|n‚ü©= xHn(x)e‚àíx2‚àï2
‚àö
2nn!
‚àö
ùúã
=
1
‚àö
2
‚ü®x|n‚Ä≤‚ü©‚ü®n‚Ä≤|(a + a‚Ä†)|n‚ü©. (5.147)
The two nonzero matrix elements on the
right are given in (5.143). They couple
xHn(x) on the left with Hn¬±1(x) on the
right. When the expression is cleaned up,
the standard recursion relation is obtained:
2x Hn(x) = Hn+1(x) + 2n Hn‚àí1(x).
(5.148)
The diÔ¨Äerential relation is obtained in
the same way, replacing x = (a + a‚Ä†)‚àï
‚àö
2
by D = (a ‚àía‚Ä†)‚àï
‚àö
2 in (5.147). On the left-
hand side, we Ô¨Ånd the derivative of Hn(x)
as well as the derivative of e‚àíx2‚àï2, and on
the right-hand side a linear combination of
Hn¬±1(x). When the expression is cleaned
up, there results the standard diÔ¨Äerential
relation
H
‚Ä≤
n(x) = 2n Hn‚àí1(x).
(5.149)
5.14.5
DiÔ¨Äerential Equation
It happens often that an operator can be
formed that is quadratic in the basis vec-
tors of the Lie algebra and it also com-
mutes with every element in the Lie algebra.
Such operators can always be constructed
for semisimple Lie algebras where the Car-
tan metric gij (cf. (5.53)) is nonsingular. The
operator gijXiXj has this property. The con-
struction of nontrivial quadratic operators
with this property is even possible for many
Lie algebras that are not semisimple. When
it is possible, the left-hand side of (5.139) is
a second-order diÔ¨Äerential operator and the
right-hand side is a constant. This constant
is the eigenvalue in the diÔ¨Äerential equation
(Ô¨Årst property listed above).
For the three-dimensional nonsemisim-
ple group ISO(2) of length-preserving
translations and rotations of the plane to
itself, the three inÔ¨Ånitesimal generators
are L3, which generates rotations around
the z-axis, and T1, T2, which generate dis-
placements in the x and y directions. The
operators T1 and T2 commute. The opera-
tors L3, T¬± = T1 ¬± iT2 satisfy commutation

206
5 Group Theory
relations
[L3, T¬±] = ¬±T¬±
[T+, T‚àí] = 0.
(5.150)
When acting on the plane, the three can
be expressed in terms of a radial (r) and
angular (ùúô) variable.
L3 = 1
i
ùúï
ùúïùúô
T¬± = e¬±iùúô
(
¬± ùúï
ùúïr + i
r
ùúï
ùúïùúô
)
.
(5.151)
Basis vectors |m‚ü©are introduced that satisfy
the condition
L3|m‚ü©= m|m‚ü©‚áí‚ü®rùúô|m‚ü©= gm(r)eimùúô.
(5.152)
Single-valuedness requires m is an integer.
Adjacent basis vectors are deÔ¨Åned by
T¬±|m‚ü©= ‚àí|m ¬± 1‚ü©‚áí
(
¬± d
dr ‚àím
r
)
gm(r)
= ‚àígm¬±1(r).
(5.153)
Finally, the identity T+T‚àí|m‚ü©= |m‚ü©gives
Bessel‚Äôs equation
(
1
r
d
dr r d
dr + 1 ‚àím2
r2
)
gm(r) = 0.
(5.154)
5.14.6
Addition Theorems
Addition theorems reÔ¨Çect the group com-
position property through the matrix mul-
tiplication property of representations
‚ü®n|g(x)g(y)|n‚Ä≤‚ü©
‚Üô
‚Üò
‚àë
k‚ü®n|g(x)|k‚ü©‚ü®k|g(y)|n‚Ä≤‚ü©=‚ü®n|g [z(x, y)] |n‚Ä≤‚ü©.
(5.155)
The special function at argument z is
expressed as a pairwise product of special
functions evaluated at the group elements
g(x) and g(y) for which x ‚àòy = z. The best
known of these addition results is
Óà∞l
00(Œò) = Óà∞l
0m(g‚àí1
1 )Óà∞l
m0(g2)
Óà∞l
00(Œò) = Óà∞l‚àó
m0(g1)Óà∞l
m0(g2)
2l + 1
4ùúã
Pl(cos Œò) =
‚àë
m
Y l
m(ùúÉ1, ùúô1)Y l‚àó
m (ùúÉ2, ùúô2).
(5.156)
Here we have taken g1 = (ùúÉ1, ùúô1, ‚àí) and
g2 = (ùúÉ2, ùúô2, ‚àí), and Œò is the angle between
these two points on the sphere surface,
deÔ¨Åned by
cos Œò = cos ùúÉ1 cos ùúÉ2
+ sin ùúÉ1 sin ùúÉ2 cos(ùúô2 ‚àíùúô1). (5.157)
5.14.7
Generating Functions
Generating functions are constructed by
computing the exponential of an operator
Óàªin the Lie algebra in two diÔ¨Äerent ways
and then equating the results. We illus-
trate this for H3 by computing ‚ü®x|e
‚àö
2ta‚Ä†|0‚ü©.
We Ô¨Årst compute the brute-strength Taylor
series expansion of the exponential:
‚ü®x|et(x‚àíD)|x‚Ä≤‚ü©‚ü®x‚Ä≤|0‚ü©= e‚àíx2‚àï2 1
4‚àö
ùúã
‚àû
‚àë
n=0
tn Hn(x)
n!
.
(5.158)
Here ‚ü®x|0‚ü©= e‚àíx2‚àï2‚àï4‚àö
ùúã. Equation (5.146)
was used to obtain this result.
Next, we observe that exponentials of
diÔ¨Äerential operators are closely related
to Taylor series expansions, for instance
e‚àítd‚àïdxf (x) = f (x ‚àít). To exploit this, we
use the result of the disentangling theorem
(5.67) to write
et(x‚àíD) = etxe‚àít2‚àï2e‚àítD.
(5.159)
Then
‚ü®x|et(x‚àíD)|x‚Ä≤‚ü©‚ü®x‚Ä≤|0‚ü©= etxe‚àít2‚àï2e‚àítD‚ü®x|0‚ü©
=
1
4‚àö
ùúã
etxe‚àít2‚àï2e‚àí(x‚àít)2‚àï2.
(5.160)

5.15 Summary
207
By comparing the two calculations, (5.158)
with (5.160), we Ô¨Ånd the standard generat-
ing function for the Hermite polynomials.
e2xt‚àít2 =
‚àû
‚àë
n=0
tnHn(x)
n!
.
(5.161)
5.15
Summary
The study of symmetry has had a profound
inÔ¨Çuence on the development of the natu-
ral sciences. Group theory has been used
in constructive ways before groups even
existed. We have given a Ô¨Çavor of what can
be done with symmetry and related argu-
ments in Section 5.2, which describes three
types of arguments that live in the same
ballpark as group theory. Groups were for-
mally introduced in Section 5.3 and a num-
ber of examples given in the following three
sections, ranging from Ô¨Ånite groups to Lie
groups. Lie algebras were introduced in
Sect. 5.7 and a number of their properties
discussed in that section. In Sect. 5.8, we
introduced the idea of a Riemannian sym-
metric space and showed the close connec-
tion between these spaces and Lie groups,
speciÔ¨Åcally that they are quotients (cosets)
of one Lie group by another, subject to
stringent conditions.
Transformation groups played a big role
in the development of classical physics ‚Äì
mechanics and electrodynamics. In fact,
it was the need to formulate these two
theories so that their structure remained
unchanged under transformations from
the same group that led to the theory
of special relativity. The group at hand
was the inhomogeneous Lorentz group,
the 10-parameter Lie group of Lorentz
transformations and translations acting on
Ô¨Åelds deÔ¨Åned over space‚Äìtime. Section 5.9
describes how group theory played a role
in the development of special relativity.
The next step beyond requiring invariance
under the same Lorentz transformation at
every space‚Äìtime point involved, allowing
the Lorentz transformation to vary from
point to point in a continuous way and still
requiring some kind of invariance (cf. gauge
theories as discussed in Section 5.13). This
extended aesthetic led to the theory of
general relativity.
Up to this point, physicists could have
done without all the particular intrica-
cies of group theory. The next step in
the growth of this subject was the inten-
sive study of the linear representations of
groups. The growth was indispensible when
quantum theory was developed, because
groups acted in Hilbert spaces through
their linear matrix representations. We
provided an overview of representation
theory in Section 5.10. At Ô¨Årst, groups
were applied in the quantum theory as
symmetry groups (cf. Section 5.11. In this
Capacity, they were used to label energy
eigenstates and describe the degeneracies
in energy levels that were required by
symmetry. Shortly afterward, they were
used in a more daring way to describe
nondegenerate levels related to each other
either by a broken symmetry or simply
by operators that had little to do with
symmetry but had the good sense to close
under commutation with each other. In
a very accurate sense, Mendeleev‚Äôs table
of the chemical elements and the table
of nuclear isotopes are manifestations
of broken symmetry applied to the con-
formal group SO(4, 2) that describes all
bound states of the hydrogen atom in a
single UIR, and the group U(3, 1) that
describes all bound states of the three-
dimensional harmonic oscillator in one
representation. These and other applica-
tions of dynamical groups are described in
Section 5.12.

208
5 Group Theory
Gauge theories were brieÔ¨Çy treated in
Section 5.13. In such theories, one begins
with a symmetry group and requires that a
Hamiltonian, Lagrangian, or action remain
‚Äúinvariant‚Äù under the transformation when
the
parameters
of
the
transformation
group are allowed to be functions over
space‚Äìtime. It is remarkable that this
requirement leads to the prediction of
new Ô¨Åelds, the nature of the interaction of
the new Ô¨Åelds with the original Ô¨Åelds, the
structure of the equations of the new Ô¨Åelds,
and the mass spectrum of these new Ô¨Åelds:
all new masses are zero. This problem was
overcome by proposing that a new particle,
now called the Higgs boson, exists. Its
discovery was announced in 2012.
As a closing tribute to the theory of
groups and their linear matrix represen-
tations, we hint how the entire theory
of the special functions of mathematical
physics, which was created long before
the Lie groups were invented, is a study
of the properties of speciÔ¨Åc matrix ele-
ments of speciÔ¨Åc matrix representations of
particular Lie groups acting over special
submanifolds of the diÔ¨Äerentiable manifold
that parameterizes the Lie group. These
ideas are sketched by simple examples in
Section 5.14.
Group theory has progressed from the
outer fringes of theoretical physics in 1928,
when it was referred to as the Gruppenpest
(1928 Weyl to Dirac at Princeton), through
the mainstream of modern physics, to wind
up playing the central role in the develop-
ment of physical theory. Theoretical physi-
cists now believe that if a theory of fun-
damental interactions is not a gauge the-
ory it does not have the right to be consid-
ered a theory of interactions at all. Gauge
theory is the new version of ‚Äúsimple‚Äù and
‚Äúelegant.‚Äù
We learn that Nature was not tamed
until Adam was able to give names to all
the animals. Similarly, we cannot even give
names to particles and their states with-
out knowing at least a little bit about group
theory. Group theory has migrated from
the outer fringes of physics (Gruppenpest,
1928) to the central player, even the lingua
franca, of modern physics.
Glossary
Group: A group is a set {g0, g1, g2, ‚Ä¶},
called group elements, together with a
combinatorial operation, ‚àò, called group
multiplication, with the property that four
axioms are obeyed: Closure, Associativity,
Existence of Identity, and Unique Inverse.
Lie Group: A Lie group is a group whose
elements g(x) are parameterized by points
x in an n-dimensional manifold. Group
multiplication corresponds to mappings of
pairs of points in the manifold to points in
the manifold: g(x) ‚àòg(y) = g(z), where z =
z(x, y).
Lie Algebra: A Lie algebra is a linear vec-
tor space on which one additional compo-
sition law, the commutator [, ], is deÔ¨Åned.
The commutator satisÔ¨Åes three conditions:
it preserves linear vector space properties,
it is antisymmetric, and the Jacobi identity
is satisÔ¨Åed.
Homomorphism: A homomorphism is a
mapping of one set with an algebraic struc-
ture (e.g., group, linear vector space, alge-
bra) onto another algebraic structure of the
same type that preserves all combinatorial
operations.
Isomorphism: An isomorphism is a homo-
morphism that is 1 : 1.
Structure
Constants
C k
ij :
These
are
expansion coeÔ¨Écients for the commutator
of basis vectors in a Lie algebra. If Xi,
i = 1, 2, ‚Ä¶ , n are basis vectors for a Lie
algebra, then the commutator [Xi, Xj
] can

5.15 Summary
209
be expanded in terms of these basis vectors:
[Xi, Xj
] = ‚àë
k C k
ij Xk.
Cartan Metric: A metric tensor that can
be deÔ¨Åned on a Lie algebra by double cross
contraction on the indices of the struc-
ture constants: gij = ‚àë
kl C l
ik C k
jl . This met-
ric tensor has remarkable properties.
Symmetry Group: In quantum mechanics,
this is a group G that leaves the Hamil-
tonian Óà¥of a physical system invariant:
GÓà¥G‚àí1 = Óà¥. Through its matrix repre-
sentations, it maps states of energy Ei into
other, often linearly independent states,
with the same energy Ei.
Dynamical Group: In quantum mechan-
ics, this is a group H that leaves invariant
the time-dependent Schr√∂dinger equation:
(Óà¥‚àíi‚Ñèùúï‚àïùúït) ùúì(x, t) = 0.
Through
its
matrix representations, it maps states of
energy Ei into other, linearly independent
states with diÔ¨Äerent energies Ej.
Subgroup: A subgroup H of a group G
consists of a subset of group elements of
G that obey the group axioms under the
same group multiplication operation that is
deÔ¨Åned on G. For example C3 ‚äÇC3v. Both
the full group G and only the identity ele-
ment e are (improper) subgroups of G.
Coset C: If H is a subgroup of G, then every
group element in G can be written as a
product of an element in H and a group ele-
ment in G: gi = hjck. If nG and nH are the
orders of G and H, then gi ‚ààG, 1 ‚â§i ‚â§nG,
hj ‚ààH, 1 ‚â§j ‚â§nH and ck ‚ààC ‚äÇG, 1 ‚â§
k ‚â§nC, where nC = nG‚àïnH. The group ele-
ments Ck are called coset representatives;
the set {c1, c2, ‚Ä¶ , cnC
} is called a coset. A
coset is not generally a group. If H is an
invariant subgroup of G the coset represen-
tatives can be chosen so that C is a group.
Representation: A representation is a
homomorphism of a set with an algebraic
structure (e.g., group, linear vector space,
algebra) into matrices. If the mapping is an
isomorphism the representation is called
faithful or 1 : 1.
Class: This is a set of elements in a group
that are related to each other by a similar-
ity transformation: if group elements hi and
hj are related by hi = ghjg‚àí1 for some g ‚àà
G, then they are in the same class. Group
elements in the same class are geometri-
cally similar. For example, in the group of
transformations that map the cube to itself,
there are eight rotations by 2ùúã‚àï3 radians
about axes through opposite corners. These
are geometrically similar and belong to the
same class.
Characters: In the theory of group repre-
sentations, characters are traces of the rep-
resentation matrices for the classes of the
group.
Gauge Theory: Schr√∂dinger‚Äôs equation
is
unchanged
if
the
wavefunction
is
multiplied
by
a
phase
factor
that
is
constant
throughout
space
and
time.
If
the
wavefunction
has
two
com-
ponents,
Schr√∂dinger‚Äôs
equation
is
unchanged if the wavefunction is mul-
tiplied (rotated) by a group element from
a 2 √ó 2 matrix from the group U(2) or
SU(2)
(2 ‚Üí3, SU(2) ‚ÜíSU(3),
etc.).
If
the
rotation
depends
on
space‚Äìtime
coordinates,
Schr√∂dinger‚Äôs
equation
is
not
invariant.
Gauge
theory
attempts
to
show
how
Schr√∂dinger‚Äôs
equation
can remain unchanged in form under
space‚Äìtime-dependent
rotations.
This
requires the inclusion of N extra Ô¨Åelds
to the physical problem, where N is the
dimension of the rotation group, U(2),
SU(2), SU(3), ‚Ä¶ These extra Ô¨Åelds describe
particles (bosons) that govern the interac-
tions among the original Ô¨Åelds described by
Schr√∂dinger‚Äôs equation. For example, for an
original charged Ô¨Åeld with one component,
invariance under space‚Äìtime- dependent
phase factor in the group U(1) leads to
the prediction that zero-mass photons are

210
5 Group Theory
responsible for charge-charge interactions.
Gauge theories also predict the form of the
interaction between the original (fermion)
Ô¨Åelds and the newly introduced (gauge
boson) Ô¨Åelds.
References
1. Barenblatt, G. (2003) Scaling, Cambridge
University Press, Cambridge.
2. Landau, L.D. and Lifshitz, E.M. (1960)
Mechanics, Addison-Wesley, Reading, MA.
3. Gilmore, R. (1974) Lie Groups, Lie Alegras,
and Some of their Applications, John Wiley &
Sons, Inc. (reprinted in 2005 by Dover in
New York), New York.
4. Gilmore, R. (2008) Lie Groups, Physics, and
Geometry, An Introduction for Physicists,
Engineers, and Chemists, Cambridge
University Press, Cambridge.
5. Hamermesh, M. (1962) Group Theory and its
Application to Physical Problems,
Addison-Wesley, Reading, MA; reprint
(1989) Dover, New York.
6. Sternberg, S. (1994) Group Theory and
Physics, University Press, Cambridge.
7. Tinkham, M. (1964) Group Theory and
Quantum Mechanics, McGraw Hill,
New York.
8. Wigner, E.P. (1959) Group Theory, and its
Application to the Quantum Mechanics of
Atomic Spectra (ed. J.J. GriÔ¨Én, translator),
Academic Press, New York.
9. Coxeter, H.S.M. and Moser, W.O.J. (1980)
Generators and Relations for Discrete
Groups, 4th edn, Springer-Verlag,
Berlin.
10. Helgason, S. (1962) DiÔ¨Äerential Geometry
and Symmetric spaces, Academic Press,
New York.
11. Weinberg, S. (1972) Gravitation and
Cosmology, Principles and Applications of
the General Theory of Relativity, John Wiley
& Sons, Inc., New York.
12. Barut, A.O. and Raczka, R. (1986) Theory of
Group Representations and Applications,
World ScientiÔ¨Åc, Singapore.
13. Haxel, O., Jensen, J.H.D., and Suess, H.E.
(1949), On the ‚ÄúMagic Numbers‚Äù in nuclear
structure. Phys. Rev., 75, 1766‚Äì1766.
14. Mayer, M.G. (1949) On closed shells in
Nuclei. II. Phys. Rev, 75, 1969‚Äì1970.
15. Arima, A. and Iachello, F. (1976) Interacting
Boson Model of collective states, Part I (the
vibrational limit). Ann. Phys. (New York), 99,
253‚Äì317.
16. Bethe, H.A. and Salpeter, E.E. (1957)
Quantum Mechanics of One- and
Two-Electron Atoms, Academic Press, New
York.
17. Utiyama, R. (1956) Invariant theoretical
interpretation of interaction. Phys. Rev., 101,
1597‚Äì1607.
18. Abramowitz, M. and Stegun, I.A. (1964)
Handbook of Mathematical Functions,
National Bureau of Standards (reprinted in
1972 by Dover in New York), Washington,
DC.
19. Miller, W. Jr. (1968) Lie Theory and Special
Functions, Academic Press, New York.
20. Talman, J.D. (1968) Special Functions: A
Group Theoretic Approach (Based on
Lectures by Eugene P. Wigner), Benjamin,
New York.
21. Vilenkin, N.Ja. (1968) Special Functions and
the Theory of Group Representations,
American Mathematical Society, Providence,
RI.

211
6
Algebraic Topology
Vanessa Robins
6.1
Introduction
Topology is the study of those aspects of
shape and structure that do not depend
on precise knowledge of an object‚Äôs geom-
etry. Accurate measurements are central
to physics, so physicists like to joke that
a topologist is someone who cannot tell
the diÔ¨Äerence between a coÔ¨Äee cup and a
doughnut. However, the qualitative nature
of topology and its ties to global analy-
sis mean that many results are relevant
to physical applications. One of the most
notable areas of overlap comes from the
study of dynamical systems. Some of the
earliest work in algebraic topology was by
Henri Poincar√© in the 1890s, who pioneered
a qualitative approach to the study of celes-
tial mechanics by using topological results
to prove the existence of periodic orbits [1].
Topology continued to play an important
role in dynamical systems with signiÔ¨Åcant
results pertinent to both areas from Smale
in the 1960s [2]. More recently, in the 1990s,
computer analysis of chaotic dynamics was
one of the drivers for innovation in compu-
tational topology [3‚Äì6].
As with any established subject, there
are several branches to topology: General
topology deÔ¨Ånes the notion of ‚Äúcloseness‚Äù
(the neighborhood of a point), limits, conti-
nuity of functions and so on, in the absence
of a metric. These concepts are absolutely
fundamental to modern functional analy-
sis; a standard introductory reference is [7].
Algebraic topology derives algebraic objects
(typically groups) from topological spaces
to help determine when two spaces are
alike. It also allows us to compute quanti-
ties such as the number of pieces the space
has, and the number and type of ‚Äúholes.‚Äù
DiÔ¨Äerential topology builds on the above
and on the diÔ¨Äerential geometry of mani-
folds to study the restrictions on functions
that arise as the result of the structure of
their domain. This chapter is primarily con-
cerned with algebraic topology; it covers
the elementary tools and concepts from
this Ô¨Åeld. It draws on deÔ¨Ånitions and mate-
rial from Chapter 5 on group theory and
Chapters 9 and 10 on diÔ¨Äerential geometry.
A central question in topology is to
decide when two objects are the same
in some sense. In general topology, two
spaces, A and B, are considered to be
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

212
6 Algebraic Topology
the same if there is a homeomorphism, f ,
between them: f ‚à∂A ‚ÜíB is a continu-
ous function with a continuous inverse.
This captures an intrinsic type of equiv-
alence that allows arbitrary stretching
and squeezing of a shape and permits
changes in the way an object sits in a
larger space (its embedding), but excludes
any cutting or gluing. So for example, a
circle (x2 + y2 = 1) is homeomorphic to
the perimeter of a square and to the tre-
foil knot, but not to a line segment, and
a sphere with a single point removed is
homeomorphic to the plane. One of the
ultimate goals in topology is to Ô¨Ånd a set of
quantities (called invariants) that charac-
terize spaces up to homeomorphism. For
arbitrary topological spaces, this is known
to be impossible [1] but for closed, com-
pact 2-manifolds this problem is solved by
Ô¨Ånding the Euler characteristic (see p. 222)
and orientability of the surface [8, 9].
What is the essential diÔ¨Äerence between
a line segment and a circle? Intuitively, it is
the ability to trace your Ô¨Ånger round and
round the circle as many times as you like
without stopping or turning back. Alge-
braic topology is the mathematical machin-
ery that lets us quantify and detect this. The
idea behind algebraic topology is to map
topological spaces into groups (or other
algebraic structures) in such a way that
continuous functions between topological
spaces map to homomorphisms between
their associated groups.1)
In Sections 6.2‚Äì6.4, this chapter covers
the three basic constructions of alge-
braic topology: homotopy, homology, and
cohomology theories. Each has a diÔ¨Äer-
ent method for deÔ¨Åning a group from a
1) A homomorphism between two groups, ùúô‚à∂G ‚Üí
H is a function that respects the group opera-
tion. That is, ùúô(a ‚ãÖb) = ùúô(a) ‚àóùúô(b), for a, b ‚ààG,
where ‚ãÖis the group operation in G and ‚àóis the
group operation in H.
topological space, and although there are
close links between the three, they capture
diÔ¨Äerent qualities of a space. Many of the
more advanced topics in algebraic topology
involve studying functions on a space,
so we introduce the fundamental link
between critical points of a function and
the topology of its domain in Section 6.5
on Morse theory. The computability of
invariants, both analytically and numer-
ically, is vital to physical applications, so
the recent literature on computational
topology is reviewed in Section 6.6. Finally,
we give a brief guide to further reading on
applications of topology to physics.
6.2
Homotopy Theory
A homotopy equivalence is a weaker form
of equivalence between topological spaces
than a homeomorphism that allows us to
collapse a space onto a lower-dimensional
subset of itself (as we will explain in
Section 6.2.3), but it captures many essen-
tial aspects of shape and structure. When
applied to paths in a space, homotopy
equivalence allows us to deÔ¨Åne an algebraic
operation on loops, and provides our Ô¨Årst
bridge between topology and groups.
6.2.1
Homotopy of Paths
We begin by deÔ¨Åning a homotopy between
two continuous functions f , g ‚à∂X ‚ÜíY.
These maps will be homotopic if their
images f (X), g(X), can be continuously
morphed from one to the other within
Y, that is, there is a parameterized set of
images that starts with one and ends with
the other. Formally, this deformation is
achieved by deÔ¨Åning a continuous function

6.2 Homotopy Theory
213
F ‚à∂X √ó [0, 1] ‚ÜíY with F(x, 0) = f (x) and
F(x, 1) = g(x).
For example, consider two maps from
the
unit
circle
into
the
unit
sphere,
f , g ‚à∂S1 ‚ÜíS2. We use the angle ùúÉ‚àà[0, 2ùúã)
to parameterize S1 and the embedding
x2 + y2 + z2 = 1 in ‚Ñù3 to deÔ¨Åne points in
S2. DeÔ¨Åne f (ùúÉ) = (cos ùúÉ, sin ùúÉ, 0) to be a
map from the circle to the equator and
g(ùúÉ) = (0, 0, 1), a constant map from the
circle to the north pole. A homotopy
between f
and g is given by F(ùúÉ, t) =
(cos(ùúãt‚àï2) cos ùúÉ, cos(ùúãt‚àï2) sin ùúÉ, sin(ùúãt‚àï2))
and illustrated in Figure 6.1. Any function
that is homotopic to a constant function,
as in this example, is called null homotopic.
When the domain is the unit interval,
X = [0, 1], and Y is an arbitrary topological
space, the continuous functions f and g are
referred to as paths in Y. A space in which
every pair of points may be joined by a path
is called path connected. It is often useful
to consider homotopies between paths that
Ô¨Åx their end points, y0 and y1, say, so we
have the additional conditions on F that for
all t ‚àà[0, 1], F(0, t) = f (0) = g(0) = y0, and
F(1, t) = f (1) = g(1) = y1. If a path starts
and ends at the same point, y0 = y1, it is
called a loop. A loop that is homotopic to a
single point, that is, a null-homotopic loop
is also said to be contractible or trivial. A
path-connected space in which every loop
is contractible is called simply connected. So
the real line and the surface of the sphere
are simply connected, but the circle and the
surface of a doughnut (the torus) are not.
6.2.2
The Fundamental Group
We are now in a position to deÔ¨Åne our
Ô¨Årst algebraic object, the fundamental
group. The Ô¨Årst step is to choose a base
point y0 in the space Y and consider all
possible loops in Y that start and end at y0.
Two loops belong to the same equivalence
class if they are homotopic: given a loop
f ‚à∂[0, 1] ‚ÜíY
with f (0) = f (1) = y0, we
write [f ] to represent the set of all loops
that are homotopic to f . The appropriate
group operation [f ] ‚àó[g] on these equiv-
alence classes is a concatenation of loops
deÔ¨Åned by tracing each at twice the speed.
SpeciÔ¨Åcally, choose f and g to be repre-
sentatives of their respective equivalence
classes and deÔ¨Åne f ‚àóg(x) = f (2x) when
x ‚àà[0, 1
2] and f ‚àóg(x) = g(2x ‚àí1) when
x ‚àà[ 1
2, 1]. As all loops have the same base
point, this product is another loop based at
y0. We then simply set [f ] ‚àó[g] = [f ‚àóg].
The equivalence class of the product [f ‚àóg]
is independent of the choice of f and
g because we can re-parameterize the
Figure 6.1
The function f ‚à∂S1 ‚ÜíS2 that maps the circle onto the
equator of the sphere is homotopic to the function g ‚à∂S1 ‚ÜíS2 that
maps the circle to the north pole. Three sections of the homotopy
F ‚à∂S1 √ó [0, 1] ‚ÜíS2 are shown in gray.

214
6 Algebraic Topology
(c)
(b)
(a)
Figure 6.2
(a) Two nonhomotopic loops on the torus
with the same base point. (b) A loop homotopic to the
concatenation the loops depicted in (a). (c) Another loop
in the same homotopy class.
homotopies in the same way as we con-
catenated the loops. Note, though, that the
equivalence class [f ‚àóg] consists of more
than just concatenated loops; Figure 6.2
depicts an example on the torus.
The set of all homotopy-equivalence
classes of loops based at y0 with the oper-
ation ‚àóforms a group with the identity
element being the class of null-homotopic
loops [e] where e(x) = y0, and the inverse
of a loop deÔ¨Åned to be the same loop
traced
backwards:
[f ]‚àí1 = [h]
where
h(x) = f (1 ‚àíx). This group is the funda-
mental group of Y with base point y0:
ùúã1(Y, y0).
The operation taking a topological space
to its fundamental group is an example of
a functor. This word expresses the property
alluded to in the introduction that con-
tinuous maps between topological spaces
transform to homomorphisms between
their associated groups. The functorial
nature of the fundamental group is man-
ifest in the following fashion. Suppose we
have a continuous function f ‚à∂X ‚ÜíY with
f (x0) = y0. Then given a loop in X with base
point x0, we can use simple composition of
the loop with f to obtain a loop in Y with
base point y0. Composition also respects
the concatenation of loops and homotopy
equivalences, so it induces a homomor-
phism between the fundamental groups:
ùúã1(f ) ‚à∂ùúã1(X, x0) ‚Üíùúã1(Y, y0).
When
the
function f ‚à∂X ‚ÜíY is a homeomorphism,
it follows that the induced map ùúã1(f ) is
an
isomorphism
of
their
fundamental
groups.
The following are some further elemen-
tary properties of the fundamental group:
‚Ä¢ A simply connected space has a trivial
fundamental group, that is, only the
identity element.
‚Ä¢ If Y is path connected, the fundamental
group is independent of the base point,
and we write ùúã1(Y).
‚Ä¢ The fundamental group respects
products2) of path-connected spaces:
ùúã1(X √ó Y) = ùúã1(X) √ó ùúã1(Y).
‚Ä¢ The wedge product of two
path-connected spaces (obtained by
gluing the spaces together at a single
point) gives a free product3) on their
fundamental groups:
ùúã1(X ‚à®Y) = ùúã1(X) ‚àóùúã1(Y).
‚Ä¢ The van Kampen theorem shows how to
compute the fundamental group of a
space X = U ‚à™V when U, V, and U ‚à©V
are open, path-connected subspaces of X
via a free product with amalgamation:
ùúã1(X) = ùúã1(U) ‚àóùúã1(V)‚àïN, where N is a
2) The (Cartesian or direct) product of two spaces
(or two groups) X √ó Y is deÔ¨Åned by ordered
pairs (x, y) where x ‚ààX and y ‚ààY.
3) The free product of two groups G ‚àóH is an inÔ¨Å-
nite group that contains both G and H as sub-
groups and whose elements are words of the
form g1h1g2h2. ¬∑ ¬∑ ¬∑

6.2 Homotopy Theory
215
normal subgroup generated by elements
of the form iU(ùõæ)iV(ùõæ)‚àí1 and ùõæis a loop
in ùúã1(U ‚à©V), iU and iV are inclusion-
induced maps from ùúã1(U ‚à©V) to ùúã1(U)
and ùúã1(V) respectively. See [10] for
details.
6.2.3
Homotopy of Spaces
Now we look at what it means for two
spaces X and Y to be homotopy equiva-
lent or to have the same homotopy type:
there must be continuous functions f ‚à∂
X ‚ÜíY and g ‚à∂Y ‚ÜíX such that fg ‚à∂Y ‚Üí
Y is homotopic to the identity on Y and gf ‚à∂
X ‚ÜíX is homotopic to the identity on X.
We can show that the unit circle S1 and the
annulus A have the same homotopy type as
follows. Let
S1 = {(r, ùúÉ) | r = 1, ùúÉ‚àà[0, 2ùúã)}
and
A = {(r, ùúÉ) | 1 ‚â§r ‚â§2, ùúÉ‚àà[0, 2ùúã)}
be subsets of the plane parameterized by
polar coordinates. DeÔ¨Åne f ‚à∂S1 ‚ÜíA to be
the inclusion map f (1, ùúÉ) = (1, ùúÉ) and let
g ‚à∂A ‚ÜíS1 map all points with the same
angle to the corresponding point on the
unit circle: g(r, ùúÉ) = (1, ùúÉ). Then gf ‚à∂S1 ‚Üí
S1 is given by gf (1, ùúÉ) = (1, ùúÉ), which is
exactly the identity map. The other compo-
sition is fg ‚à∂A ‚ÜíA is fg(r, ùúÉ) = (1, ùúÉ). This
is homotopic to the identity iA = (r, ùúÉ) via
the homotopy F(r, ùúÉ, t) = (1 + t(r ‚àí1), ùúÉ).
This example is an illustration of a defor-
mation retraction: a homotopy equivalence
between a space (e.g., the annulus) to a sub-
set (the circle) that leaves the subset Ô¨Åxed
throughout.
Spaces that are homotopy equivalent
have isomorphic fundamental groups. A
space that has the homotopy type of a
point is said to be contractible and has
trivial fundamental group. This is much
stronger than being simply connected: for
example, the sphere S2 is simply connected
because every loop can be shrunk to a
point, but it is not a contractible space.
6.2.4
Examples
Real space ‚Ñùm, m ‚â•1, all spheres Sn with
n ‚â•2, any Hilbert space, and any connected
tree (cf. Chapter 4) have trivial fundamental
groups.
The fundamental group of the circle is
isomorphic to the integers under addition:
ùúã1(S1) = ‚Ñ§. This can be seen by noting that
the homotopy class of a loop is determined
by how many times it wraps around the
circle. A formal proof of this result is quite
involved ‚Äì see [10] for details. Any space
that is homotopy equivalent to the circle
will have the same fundamental group ‚Äì
this holds for the annulus, the M√∂bius band,
a cylinder, and the ‚Äúpunctured plane‚Äù ‚Ñù2 ‚ßµ
(0, 0).
The projective plane ‚ÑùP2 is a nonori-
entable
surface
deÔ¨Åned
by
identifying
antipodal points on the boundary of the
unit disk (or equivalently, antipodal points
on the sphere). Its fundamental group is
isomorphic to ‚Ñ§2 (the group with two
elements, the identity and r which is its
own inverse r2 = id). To see this, consider
a loop that starts at the center of the unit
disk (0, 0), goes straight up to (0, 1) which
is identiÔ¨Åed with (0, ‚àí1) then continues
straight back up to the origin. This loop
is in a distinct homotopy class to the
null-homotopic loop but it is in the same
homotopy class as its inverse (to see this,
imagine Ô¨Åxing the loop at (0, 0) and rotate
it by 180‚àòas illustrated in Figure 6.3).
The
fundamental
group
of
a
con-
nected graph with v vertices and e edges
(cf. Chapter 4) is a free group with n

216
6 Algebraic Topology
Figure 6.3
The projective plane, ‚ÑùP2 is modeled by the unit disk
with opposite points on the boundary identiÔ¨Åed. The black loop
starting at (0, 0) is homotopic to its inverse with the equivalence
suggested by the gray loops.
generators4) where n = e ‚àí(v ‚àí1) is the
number of edges in excess of a spanning
tree (the cyclomatic number). This demon-
strates that the fundamental group need
not be Abelian (products do not necessarily
commute).
The torus ùïã= S1 √ó S1, so ùúã1(ùïã) = ‚Ñ§√ó
‚Ñ§. More generally, the fundamental group
of an orientable genus-g surface5) (g ‚â•2)
is isomorphic to a hyperbolic translation
group with 2g generators.
If a space has a Ô¨Ånite cell structure, then
the fundamental group can be computed as
a free group with relations in an algorithmic
manner; this is discussed in Section 6.6.
6.2.5
Covering Spaces
The result about the fundamental group
of a genus-g surface comes from analyzing
the relationship between loops on a sur-
face and paths in its universal covering space
(the hyperbolic plane when g ‚â•2). Cover-
ing spaces are useful in many other con-
texts (from harmonic analysis to diÔ¨Äerential
topology to computer simulation), so we
4) A free group with one generator, a say, is the
inÔ¨Ånite cyclic group with elements ‚Ä¶ , a‚àí1,
1, a, a2, ‚Ä¶ A free group with two generators a, b,
contains all elements of the form ai1bj1ai2bj2 ¬∑ ¬∑ ¬∑
for integers ik, jl. A free group with n generators
is the natural generalization of this.
5) Starting with a sphere, you can obtain all closed
oriented 2-manifolds by attaching some number
of handles (cylinders) to the sphere. The number
of handles is the genus.
describe them brieÔ¨Çy here. They are simply
a more general formulation of the standard
procedure of identifying a real-valued peri-
odic function with a function on the circle.
Given a topological space X, a covering
space for X is a pair (C, p), where C is
another topological space and p ‚à∂C ‚ÜíX is
a continuous function onto X. The covering
map p must satisfy the following condition:
for every point x ‚ààX, there is a neighbor-
hood U of x such that p‚àí1(U) is a disjoint
union of open sets each of which is mapped
homeomorphically onto U by p. The dis-
crete set of points p‚àí1(x) is called the Ô¨Åber
of x. A universal covering space is one in
which C is simply connected. The reason
for the name comes from the fact that a uni-
versal covering of X will cover any other
connected covering of X. For example, the
circle is a covering space of itself with C =
S1 = {z ‚àà‚ÑÇ‚à∂|z| = 1} and pk(z) = zk for
all nonzero integers k, while the univer-
sal cover of the circle is the real line with
pÓâÅ‚à∂‚Ñù‚ÜíS1 given by pÓâÅ(t) = exp(i2ùúãt).
The point z = (1, 0) ‚ààS1 is then covered
by the Ô¨Åber p‚àí1(z) = ‚Ñ§‚äÇ‚Ñù. We illustrate a
covering of the torus in Figure 6.4.
When X and C are suitably nice spaces
(connected and locally path connected),
loops in X based at x0 lift to paths in C
between elements of the Ô¨Åber of x0. So in
the example of S1, a path in ‚Ñùbetween two
integers i < j maps under pÓâÅto a loop that
wraps j ‚àíi times around the circle.
Now consider homeomorphisms of the
covering space h ‚à∂C ‚ÜíC that respect the

6.2 Homotopy Theory
217
Figure 6.4
The universal covering space of the
torus is the Euclidean plane projected onto the
closed surface by identifying opposite edges
of each rectangle with parallel orientations.
The Ô¨Åber of the base point on the torus is a
lattice of points in the cover. The two loops
on the torus lift to the vertical and horizon-
tal paths shown in the cover. The lift of the
concatenation of these two loops is homo-
topic to the diagonal path in the cover (see
Figure 6.2c). The deck transformation group for
this cover is simply the group of translations
that preserve the rectangles; it is isomorphic to
‚Ñ§√ó ‚Ñ§= ùúã1(ùïã).
covering map, p(h(c)) = p(c). The set of
all such homeomorphisms forms a group
under composition called the deck trans-
formation group. When (C, p) is a universal
covering space for X, it is possible to show
that the deck transformation group must be
isomorphic to the fundamental group of X.
This gives a technique for determining the
fundamental group of a space in some situ-
ations; see [10] for details and examples.
6.2.6
Extensions and Applications
As we saw in the examples of Section 6.2.4,
the fundamental group of an n-dimensional
sphere is trivial for n ‚â•2, so the question
naturally arises how we might capture
the diÔ¨Äerent topological structures of Sn.
To generalize the fundamental group, we
examine maps from an n-dimensional
unit cube In into the space X where the
entire boundary of the cube is mapped
to a Ô¨Åxed base point in X, that is,
f ‚à∂In ‚ÜíX,
with
f (ùúïIn) = x0.
Elements
of the higher homotopy groups ùúãn(X, x0)
are then homotopy-equivalence classes
of these maps. The group operation is
concatenation in the Ô¨Årst coordinate just
as we deÔ¨Åned for one-dimensional closed
paths above. The main diÔ¨Äerence in higher
dimensions is that this operation now
commutes.
It is perhaps not too diÔ¨Écult to see that
ùúã2(S2) = ‚Ñ§, although the multiple wrapping
of the sphere by a piece of paper cannot be
physically realized in ‚Ñù3 in the same way a
piece of string wraps many times around a
circle. The surprise comes with the result
that ùúãk(Sn) is nontrivial for most (but cer-
tainly not all) k ‚â•n ‚â•2, and in fact math-
ematicians have not yet determined all the
homotopy groups of spheres for arbitrary k
and n [10]. Higher-order homotopy groups
are a rich and fascinating set of topologi-
cal invariants that are the subject of active
research in mathematics.
One application of homotopy theory
arises in the study of topological defects
in condensed matter physics. A classic
example is that of nematic liquid crystals,
which are Ô¨Çuids comprised of molecules
with an elongated ellipsoidal shape. The
order parameter for this system is the
(time-averaged) direction of the major axis
of the ellipsoidal molecule: n. For identical
and symmetrical molecules, the sign and
the magnitude of the vector is irrelevant,
and so the parameter space for n is the
surface of the sphere with antipodal points

218
6 Algebraic Topology
Figure 6.5
A cross section through a nematic Ô¨Çuid with
a line defect that runs perpendicular to the page. Each
line segment represents the averaged direction of a single
molecule.
identiÔ¨Åed ‚Äì topologically ‚ÑùP2. The exis-
tence of noncontractible loops in ‚ÑùP2 is
associated with the existence of topological
line defects in conÔ¨Ågurations of molecules
in the nematic liquid crystal; see Figure 6.5.
The fact that ùúã1(‚ÑùP2) = ‚Ñ§2 is manifest in
the fact that two defects of the same type
can smoothly cancel one another. The sec-
ond homotopy group is ùúã2(‚ÑùP2) = ‚Ñ§, and
this is manifest in the existence of point
defects (‚Äúhedgehogs‚Äù) where the director
Ô¨Åeld points radially away from a central
point. See Mermin‚Äôs original article [11] or
[12] for further details.
6.3
Homology
The fundamental group is a useful invariant
but it captures only the one-dimensional
structure of equivalent loops in a space
and cannot distinguish between spheres of
dimensions greater than two, for example.
The higher homotopy groups do capture
this structure but are diÔ¨Écult to com-
pute. The homology groups provide a
way to describe structure in all relevant
dimensions, but require a bit more machin-
ery to deÔ¨Åne. This can seem abstract at Ô¨Årst,
but in fact the methods are quite combi-
natorial and there has been much recent
activity devising eÔ¨Écient algorithms to
compute homological quantities from large
data sets (see Section 6.6).
There are a number of diÔ¨Äerent for-
mulations of homology theory that give
essentially the same results for ‚Äúnice‚Äù
spaces (such as diÔ¨Äerentiable manifolds).
The two key ingredients are a discrete cell
complex that captures the way a space is
put together, and a boundary map that
describes incidences between cells of adja-
cent dimensions. The algebraic structure
comes from deÔ¨Åning the addition and
subtraction of cells.
The
earliest
formulation
of
homol-
ogy theory is simplicial homology, based
on triangulations of topological spaces
called simplicial complexes. This theory
has
some
shortcomings
when
dealing
with very general topological spaces and
successive improvements over the past
century have culminated in the current
form based on singular homology and gen-
eral cell complexes. Hatcher [10] provides
an excellent introduction to homology
from this modern perspective. We focus
on simplicial homology here because it
is the most concrete and easy to adapt

6.3 Homology
219
for implementation on a computer. The
notation used in this section is based on
that of Munkres [13].
6.3.1
Simplicial Complexes
The
basic
building
block
is
an
ori-
ented k-simplex, ùúék, the convex hull of
k + 1 geometrically independent points,
{x0, x1, ‚Ä¶ , xk} ‚äÇ‚Ñùm,
with
k ‚â§m.
For
example, a 0-simplex is just a point, a
1-simplex is a line segment, a 2-simplex a
triangle, and a 3-simplex is a tetrahedron.
We write ùúék = ‚ü®x0, x1, ‚Ä¶ , xk‚ü©to denote a
k-simplex and its vertices. The ordering
of the vertices deÔ¨Ånes an orientation of
the simplex. This orientation is chosen
arbitrarily but is Ô¨Åxed and coincides with
the usual notion of orientation of line seg-
ments, triangles, and tetrahedra. Any even
permutation of the vertices in a simplex
gives another simplex with the same ori-
entation, while an odd permutation gives a
simplex with negative orientation.
Given a set V, an abstract simpli-
cial complex, ÓàØ, is a collection of Ô¨Ånite
subsets of V with the property that if
ùúék = {v0, ‚Ä¶ , vk} ‚ààÓàØthen all subsets of
ùúék (its faces) are also in ÓàØ. If the simplicial
complex is Ô¨Ånite, then it can always be
embedded in ‚Ñùm for some m (certain com-
plexes with inÔ¨Ånitely many simplices can
also be embedded in Ô¨Ånite-dimensional
space). An embedded complex is called a
geometric realization of ÓàØ. The subset of
‚Ñùm occupied by the geometric complex
is denoted by |ÓàØ| and is called a polytope
or polyhedron. When a topological space
X is homeomorphic to a polytope, |ÓàØ|,
it is called a triangulated space and the
simplicial complex ÓàØis a triangulation
of X. For example, a circle is homeomor-
phic to the boundary of a triangle, so the
three vertices a, b, c and three 1-simplices,
‚ü®ab‚ü©, ‚ü®bc‚ü©, ‚ü®ca‚ü©are a triangulation of the
circle (see Figure 6.6). All diÔ¨Äerentiable
manifolds
have
triangulations,
but
a
complete characterization of the class of
topological spaces that have a triangulation
is not known. Every topological 2- or 3-
manifold has a triangulation, but there is a
(nonsmooth) 4-manifold that cannot have
a triangulation (it is related to the Lie group
E8 [14]). The situation for nondiÔ¨Äerentiable
manifolds in higher dimensions remains
uncertain.
6.3.2
Simplicial Homology Groups
We now deÔ¨Åne the group structures asso-
ciated with a space X that is triangulated
by a Ô¨Ånite simplicial complex ÓàØ. Although
the triangulation of a space is not unique,
the homology groups for any triangula-
tion of the same space are isomorphic (see
[13]); this makes simplicial homology well
deÔ¨Åned.
The set of all k-simplices from ÓàØform the
basis of a free group called the kth chain
group, Ck(X). The group operation is an
additive one; recall that ‚àíùúék is just ùúék with
the opposite orientation, so this deÔ¨Ånes the
a
b
c
Figure 6.6
The simplicial complex of a triangle consists of one
2-simplex, three 1-simplices, and three vertices (0-simplices).

220
6 Algebraic Topology
inverse elements. In general, a k-chain is the
formal sum of a Ô¨Ånite number of oriented
k-simplices: ck = ‚àë
i aiùúék
i . The coeÔ¨Écients,
ai, are elements of another group, called the
coeÔ¨Écient group that is typically the inte-
gers ‚Ñ§, but can be any Abelian group such
as the integers mod 2 ‚Ñ§2, the rationals ‚Ñö, or
real numbers ‚Ñù. If the coeÔ¨Écient group G
needs to be emphasized, we write Ck(X; G).
When k ‚â•1, the boundary operator ùúïk ‚à∂
Ck ‚ÜíCk‚àí1 maps a k-simplex onto the sum
of the (k ‚àí1)-simplices in its boundary. If
ùúék = ‚ü®x0, x1, ‚Ä¶ , xk‚ü©is a k-simplex, we have
ùúïk(ùúék) =
k‚àë
i=0
(‚àí1)i‚ü®x0, ‚Ä¶ , ÃÇxi, ‚Ä¶ , xk‚ü©,
where ‚ü®x0, ‚Ä¶ , ÃÇxi, ‚Ä¶ , xk‚ü©represents the
(k ‚àí1)-simplex obtained by deleting the
vertex xi. The action of the boundary
operator on general k-chains is obtained
by linear extension from its action on the
k-simplices: ùúïk(‚àë
i aiùúék
i ) = ‚àë
i aiùúïk(ùúék
i ). For
k = 0, the boundary operator is deÔ¨Åned to
be null: ùúï0(c0) = 0. We drop the subscript
from the boundary operator when the
dimension is understood.
As an example, consider the simplicial
complex consisting of a triangle and all its
edges and vertices, as shown in Figure 6.6.
The boundary of the 2-simplex ‚ü®a, b, c‚ü©is
ùúï(‚ü®a, b, c‚ü©) = ‚ü®b, c‚ü©‚àí‚ü®a, c‚ü©+ ‚ü®a, b‚ü©,
and the boundary of this 1-chain is
ùúï(‚ü®b, c‚ü©‚àí‚ü®a, c‚ü©+ ‚ü®a, b‚ü©)
= (c ‚àíb) ‚àí(c ‚àía) + (b ‚àía) = 0.
This illustrates the fundamental property of
the boundary operator, namely,
ùúïk ùúïk+1 = 0.
We
now
consider
two
subgroups
of Ck
that have important geometric
interpretations. The Ô¨Årst subgroup consists
of k-chains that map to zero under the
boundary operator. This group is the group
of cycles denoted Zk, it is the kernel (or
null space) of ùúïk and its elements are called
k-cycles. From the deÔ¨Ånition of ùúï0 we see
that all 0-chains are cycles so Z0 = C0.
The second subgroup of Ck is the group of
k-chains that bound a (k + 1)-chain. This is
the group of boundaries Bk, it is the image
of ùúïk+1. It follows from the above equation
that every boundary is a cycle, that is, the
image of ùúïk+1 is mapped to zero by ùúïk, so Bk
is a subgroup of Zk. In our example of the
triangle simplicial complex, we Ô¨Ånd no 2-
cycles and a 1-cycle, ‚ü®b, c‚ü©‚àí‚ü®a, c‚ü©+ ‚ü®a, b‚ü©,
such that all other 1-cycles are integer
multiples of this. This 1-cycle is also the
only 1-boundary, so Z1 = B1, and the
0-boundaries B0 are generated by the
two 0-chains: c ‚àíb and c ‚àía (the third
boundary a ‚àíb = (c ‚àíb) ‚àí(c ‚àía)).
As Bk ‚äÇZk, we can form a quotient
group, Hk = Zk‚àïBk; this is precisely the
kth homology group. The elements of Hk
are equivalence classes of k-cycles that do
not bound any k + 1 chain, so this is how
homology
characterizes
k-dimensional
holes. Formally, two k-cycles w, z ‚ààZk are
in the same equivalence class if w ‚àíz ‚ààBk;
such cycles are said to be homologous.
We write [z] ‚ààHk for the equivalence
class of cycles homologous to z. For the
simple example of the triangle simplicial
complex, we have already seen that Z1 = B1
so that H1 = {0}. The 0-cycles are gener-
ated by {a, b, c} and the boundaries by
{(c ‚àíb), (c ‚àía)}, so H0 has a single equiv-
alence class, [c], and is isomorphic to ‚Ñ§.
The following are homology groups for
some familiar spaces
‚Ä¢ Real space ‚Ñùn has H0 = ‚Ñ§and Hk = {0}
for k ‚â•1.

6.3 Homology
221
‚Ä¢ The spheres have H0(Sn) = ‚Ñ§,
Hn(Sn) = ‚Ñ§, and Hk(Sn) = {0} for all
other values of k.
‚Ä¢ The torus has H0 = ‚Ñ§, H1 = ‚Ñ§‚äï‚Ñ§,
H2 = ‚Ñ§, Hk = {0} for all other k. The
2-cycle that generates H2 is the entire
surface. This is in contrast to the second
homotopy group for the torus, which is
trivial.
‚Ä¢ The real projective plane, ‚ÑùP2 has
H0 = ‚Ñ§, H1 = ‚Ñ§2, and Hk = {0} for
k ‚â•2. The fact that H2 is trivial is a
result of the surface being nonorientable;
even though ‚ÑùP2 is closed as a manifold,
the 2-chain covering the surface is not a
2-cycle.
The combinatorial nature of simplicial
homology makes it readily computable.
We give the classical algorithm and review
recent work on fast and eÔ¨Écient algorithms
for data in Section 6.6.
6.3.3
Basic Properties of Homology Groups
In general, the homology groups of a Ô¨Ånite
simplicial complex are Ô¨Ånitely generated
Abelian groups, so the following theorem
tells us about their general structure (see
Theorem 4.3 of [13]).
Theorem 6.1 If G is a Ô¨Ånitely generated
Abelian group then it is isomorphic to the
following direct sum:
G ‚âÉ(‚Ñ§‚äï¬∑ ¬∑ ¬∑ ‚äï‚Ñ§) ‚äï‚Ñ§t1 ‚äï¬∑ ¬∑ ¬∑ ‚äï‚Ñ§tm.
The number of copies of the integer group
‚Ñ§is called the Betti number ùõΩ. The cyclic
groups ‚Ñ§ti are called the torsion subgroups
and the ti are the torsion coeÔ¨Écients; they
are deÔ¨Åned so that ti > 1 and t1 divides t2,
which divides t3, and so on. The torsion
coeÔ¨Écients of the homology group Hk(ÓàØ)
measure the twistedness of the space in
some sense. For example, the real projec-
tive plane has H1 = ‚Ñ§2, because the 2-chain
that represents the whole of the surface
has a boundary that is twice the generat-
ing 1-cycle. The Betti number ùõΩk of the
kth homology group counts the number of
nonequivalent nonbounding k-cycles and
this can be loosely interpreted as the num-
ber of k-dimensional holes. The 0th Betti
number, ùõΩ0, counts the number of path-
connected components of the space.
Some other fundamental properties of
the homology groups are as follows:
‚Ä¢ If the simplicial complex has N
connected components,
X = X1 ‚à™¬∑ ¬∑ ¬∑ ‚à™XN, then H0 is
isomorphic to the direct sum of N
copies of the coeÔ¨Écient group, and
Hk(X) = Hk(X1) ‚äï¬∑ ¬∑ ¬∑ ‚äïHk(XN).
‚Ä¢ Homology is a functor. If f ‚à∂X ‚ÜíY is a
continuous function from one simplicial
complex into another, it induces natural
maps on the chain groups
f‚ôØ‚à∂Ck(X) ‚ÜíCk(Y) for each k, which
commute with the boundary operator:
ùúïf‚ôØ= f‚ôØùúï. This commutativity implies
that cycles map to cycles and boundaries
to boundaries, so that the f‚ôØinduce
homomorphisms on the homology
groups f‚àó‚à∂Hk(X) ‚ÜíHk(Y).
‚Ä¢ If two spaces are homotopy equivalent,
they have isomorphic homology groups
(this is shown using the above functorial
property).
‚Ä¢ The Ô¨Årst homology group is the
Abelianization of the fundamental
group. When X is a path-connected
space, the connection between H1(X)
and ùúã1(X) is made by noticing that two
1-cycles are equivalent in homology if
their diÔ¨Äerence is the boundary of a
2-chain; if we parameterize the 1-cycles
as loops then this 2-chain forms a region

222
6 Algebraic Topology
through which one can deÔ¨Åne a
homotopy. See [10] for a formal proof.
‚Ä¢ The higher-dimensional homology
groups have the comforting property
that if all simplices in a complex have
dimensions ‚â§m then Hk = {0} for
k > m. This is in stark contrast to the
higher-dimensional homotopy groups.
A particularly pleasing result in homol-
ogy relates the Betti numbers to another
topological invariant called the Euler char-
acteristic. For a Ô¨Ånite simplicial complex, ÓàØ,
deÔ¨Åne nk to be the number of simplices of
dimension k, then the Euler characteristic
is deÔ¨Åned to be ùúí(ÓàØ) = n0 ‚àín1 + n2 + ¬∑ ¬∑ ¬∑.
The Euler‚ÄìPoincar√© theorem states that
the alternating sum of Betti numbers
is the same as the Euler characteristic
[13]: ùúí= ùõΩ0 ‚àíùõΩ1 + ùõΩ2 + ¬∑ ¬∑ ¬∑. This is one
of many results that connect the Euler
characteristic with other properties of
manifolds. For example, if M is a compact
2-manifold with a Riemannian metric, then
the Gauss‚ÄìBonnet theorem states that 2ùúãùúí
is equal to the integral of Gaussian curva-
ture over the surface plus the integral of
geodesic curvature over the boundary of M
[15]. Further, if M is orientable and has no
boundary, then it must be homeomorphic
to a sphere with g handles and ùúí= 2 ‚àí2g
where g is the genus of the surface. When
M is nonorientable without boundary, then
it is homeomorphic to a sphere with r
cross-caps and ùúí= 2 ‚àír.
The Euler characteristic is a topological
invariant with the property of inclusion‚Äì
exclusion: If a triangulated space X = A ‚à™B
where A and B are both subcomplexes,
then
ùúí(X) = ùúí(A) + ùúí(B) ‚àíùúí(A ‚à©B).
This means the value of ùúíis a localizable
one and can be computed by cutting up
a larger space into smaller chunks. This
property makes it a popular topological
invariant to use in applications [16]. A
recent application that exploits the local
additivity of the Euler characteristic to
great eÔ¨Äect is target enumeration in local-
ized sensor networks [17, 18]. The Euler
characteristic has also been shown to be
an important parameter in the physics of
porous materials [19, 20].
The simple inclusion‚Äìexclusion property
above does not hold for the Betti numbers
because they capture global aspects of the
topology of a space. Relating the homology
of two spaces to their union requires more
sophisticated algebraic machinery that we
review below.
6.3.4
Homological Algebra
Many results and tools in homology are
independent of the details about the
way the chains and boundary operators are
deÔ¨Åned for a topological space; they depend
only on the chain groups and the fact that
ùúïùúï= 0. The study of such abstract chain
complexes and transformations between
them is called homological algebra and is
one of the original examples in category
theory [13].
An abstract chain complex is a sequence
of Abelian groups and homomorphisms
¬∑ ¬∑ ¬∑
dk+2
‚àí‚ÜíCk+1
dk+1
‚àí‚ÜíCk
dk
‚àí‚Üí¬∑ ¬∑ ¬∑
d1
‚àí‚ÜíC0 ‚àí‚Üí{0},
with dkdk+1 = 0.
The homology of this chain complex is
Hk(C) = ker dk‚àïim dk+1. In certain cases
(such as the simplicial chain complex
of a contractible space), we Ô¨Ånd that
im dk+1 = ker dk for k ‚â•1, so the homol-
ogy groups are trivial. Such a sequence is
said to be exact.

6.3 Homology
223
This property of exactness has many nice
consequences. For example, with a short
exact sequence of groups,
0 ‚ÜíA
f‚àí‚ÜíB
g
‚àí‚ÜíC ‚Üí0.
The exactness means that f is a monomor-
phism (one-to-one) and g
is an epi-
morphism (onto). In fact, g induces an
isomorphism of groups C ‚âàB‚àïf (A), and
if these groups are Ô¨Ånitely generated
Abelian, then the Betti numbers sat-
isfy ùõΩ(B) = ùõΩ(C) + ùõΩ(A) (where we have
replaced f (A) by A because f is one-to-one).
Now imagine there is a short exact
sequence of chain complexes, that is,
0 ‚ÜíAk
fk
‚àí‚ÜíBk
gk
‚àí‚ÜíCk ‚Üí0.
is exact for all k and the maps fk, gk com-
mute with the boundary operators in each
complex (i.e., dBfk = fk‚àí1dA, etc.). Typically,
the fk will be induced by an inclusion map
(and so be monomorphisms), and gk by
a quotient map (making them epimor-
phisms) on some underlying topological
spaces. The zigzag lemma shows that
these short exact sequences can be joined
together into a long exact sequence on the
homology groups of A, B, and C:
¬∑ ¬∑ ¬∑ ‚ÜíHk(A)
f‚àó
‚àí‚ÜíHk(B)
g‚àó
‚àí‚ÜíHk(C)
Œî
‚àí‚Üí
Hk‚àí1(A)
f‚àó
‚àí‚ÜíHk‚àí1(B) ‚Üí¬∑ ¬∑ ¬∑
The maps f‚àóand g‚àóare those induced by
the chain maps f and g and the boundary
maps Œî are deÔ¨Åned directly on the homol-
ogy classes in Hk(C) and Hk‚àí1(A) by show-
ing that cycles in Ck map to cycles in Ak‚àí1
via gk, the boundary ùúïB, and fk‚àí1. Details are
given in Hatcher [10].
One of the most useful applications of
this long exact sequence in homology is
the Mayer‚ÄìVietoris exact sequence, a result
that describes the relationship between the
homology groups of two simplicial com-
plexes, X, Y, their union, and their intersec-
tion. This gives us a way to deduce homol-
ogy groups of a larger space from smaller
spaces.
¬∑ ¬∑ ¬∑
jk
‚àí‚ÜíHk(X) ‚äïHk(Y)
sk
‚àí‚ÜíHk(X ‚à™Y)
vk
‚àí‚ÜíHk‚àí1(X ‚à©Y)
jk‚àí1
‚àí‚ÜíHk‚àí1(X) ‚äïHk‚àí1(Y)
sk‚àí1
‚àí‚Üí¬∑ ¬∑ ¬∑
(6.1)
The homomorphisms are deÔ¨Åned as fol-
lows:
jk([u]) = ([u], ‚àí[u])
sk([w], [w‚Ä≤]) = [w + w‚Ä≤]
vk([z]) = [ùúïz‚Ä≤],
Where, in the last equation, z is a cycle in
X ‚à™Y and we can write z = z‚Ä≤ + z‚Ä≤‚Ä≤ where
z‚Ä≤ and z‚Ä≤‚Ä≤ are chains (not necessarily cycles)
in X and Y respectively. These homomor-
phisms are well deÔ¨Åned (see, for example,
Theorem 33.1 of [13]). Exactness implies
that the image of each homomorphism is
equal to the kernel of the following one:
im jk = ker sk, im sk = ker vk, and im vk =
ker jk‚àí1.
The Mayer‚ÄìVietoris sequence has an
interesting interpretation in terms of Betti
numbers. First we deÔ¨Åne Nk = ker jk to be
the subgroup of Hk(X ‚à©Y) deÔ¨Åned by the
k-cycles that bound in both X and in Y.
Then by focusing on the exact sequence
around Hk(X ‚à™Y), it follows that [21, 22]
ùõΩk(X ‚à™Y) = ùõΩk(X) + ùõΩk(Y) ‚àíùõΩk(X ‚à©Y)
+ rank Nk + rank Nk‚àí1.
This is where we see the nonlocalizable
property of homology and the Betti num-
bers most clearly.

224
6 Algebraic Topology
The Mayer‚ÄìVietoris sequence holds in a
more general setting than simplicial homol-
ogy. It is an example of a result that can
be derived from the Eilenberg‚ÄìSteenrod
axioms. Any theory for which these Ô¨Åve
axioms hold is a type of homology theory,
see [10] for further details.
6.3.5
Other Homology Theories
Chain complexes that capture topological
information may be deÔ¨Åned in a number
of ways. We have deÔ¨Åned simplicial chain
complexes above, and will brieÔ¨Çy describe
some other techniques here.
Cubical homology is directly analogous to
simplicial homology using k-dimensional
cubes as building elements rather than
k-dimensional simplices. This theory is
developed in full in [6] and arose from
applications in digital image analysis and
numerical analysis of dynamical systems.
Singular homology is built from singu-
lar k-simplices that are continuous func-
tions from the standard k-simplex into a
topological space X, ùúé‚à∂‚ü®v0, ‚Ä¶ , vk‚ü©‚ÜíX.
Singular chains and the boundary opera-
tor are deÔ¨Åned as they are in simplicial
homology. A greater degree of Ô¨Çexibility
is found in singular homology because the
maps ùúéare allowed to collapse the sim-
plices, for example, the standard k-simplex,
k > 0, may have boundary points mapping
to the same point in X, or the entire simplex
may be mapped to a single point [10].
An even more general formulation of cel-
lular homology is made by considering gen-
eral cell complexes. A cell complex is built
incrementally by starting with a collection
of points X(0) ‚äÇX, then attaching 1-cells
via maps of the unit interval into X so
that end points map into X(0) to form the
1-skeleton X(1). This process continues by
attaching k-cells to the (k ‚àí1)-skeleton by
continuous maps of the closed unit k-ball,
ùúô‚à∂Bk(0, 1) ‚ÜíX that are homeomorphic
on the interior and satisfy ùúô‚à∂ùúïBk(0, 1) ‚Üí
X(k‚àí1). The deÔ¨Ånition of the boundary oper-
ator for a cell complex requires the con-
cept of degree of a map of the k-sphere (i.e.,
the boundary of a (k + 1)-dimensional ball).
For details see Hatcher [10].
We will see in the section on Morse
Theory that it is also possible to deÔ¨Åne a
chain complex from the critical points of a
smooth function on a manifold.
6.4
Cohomology
The cohomology groups are derived by a
simple dualization procedure on the chain
groups (similar to the construction of dual
function spaces in analysis). We will again
give deÔ¨Ånitions in the simplicial setting
but the concepts carry over to other con-
texts. A cochain ùúôk is a function from the
simplicial chain group into the coeÔ¨Écient
group, ùúôk ‚à∂Ck(X; G) ‚ÜíG (recall that G is
usually the integers, ‚Ñ§, but can be any
Abelian group). The space of all k-cochains
forms a group called the kth cochain group
Ck(X; G). The simplicial boundary oper-
ators ùúïk ‚à∂Ck ‚ÜíCk‚àí1 induce coboundary
operators ùõøk‚àí1 ‚à∂Ck‚àí1 ‚ÜíCk on the cochain
groups via ùõø(ùúô) = ùúôùúï. In other words, the
cochain ùõø(ùúô) is deÔ¨Åned via the action of
ùúôon the boundary of each k-simplex ùúé=
‚ü®x0, x1, ‚Ä¶ , xk‚ü©:
ùõø(ùúô)(ùúé) =
‚àë
i
(‚àí1)iùúô(‚ü®x0, ‚Ä¶ , ÃÇxi, ‚Ä¶ , xk‚ü©).
The key property from homology that
ùúïkùúïk+1 = 0
also
holds
true
for
the
coboundary: ùõøkùõøk‚àí1 = 0 (coboundaries are
mapped to zero), so we deÔ¨Åne the kth coho-
mology group as Hk(X) = ker ùõøk‚àïim ùõøk‚àí1.
Note that cochains ùúô‚ààker ùõøare functions

6.4 Cohomology
225
that vanish on the k-boundaries (not the
larger group of k-cycles), and a coboundary
ùúÇk ‚ààim ùõøis one that can be deÔ¨Åned via
the action of some cochain ùúôk‚àí1 on the
(k ‚àí1)-boundaries.
The coboundary operator acts in the
direction of increasing dimension and
this can be a more natural action in some
situations (such as de Rham cohomology
of diÔ¨Äerential forms discussed below) and
also has some interesting algebraic conse-
quences (it leads to the deÔ¨Ånition of the
cup product).
¬∑ ¬∑ ¬∑ ‚Üê‚àíCk+1
ùõøk
‚Üê‚àíCk ‚Üê‚àí¬∑ ¬∑ ¬∑ ‚Üê‚àíC0 ‚Üê‚àí{0}.
This action of the coboundary makes coho-
mology contravariant (induced maps act in
the opposite direction) where homology is
covariant (induced maps act in the same
direction). If f ‚à∂X ‚ÜíY is a continuous
function between two topological spaces
then the group homomorphism induced
on the cohomology groups acts as f ‚àó‚à∂
Hk(Y) ‚ÜíHk(X).
In simplicial homology, the simplices
form bases for the chain groups, and we
can similarly use them as bases for the
cochain groups by deÔ¨Åning an elemen-
tary cochain Ãáùúéas the function that takes
the value one on ùúéand zero on all other
simplices. For a Ô¨Ånite simplicial complex,
it is possible to represent the boundary
operator ùúïas a matrix with respect to the
bases of oriented k- and (k ‚àí1)-simplices.
If we then use the elementary cochains as
bases for the cochain groups, the matrix
representation for the coboundary oper-
ator is just the transpose of that for the
boundary operator. This shows that for
Ô¨Ånite simplicial complexes, the functional
and geometric meanings of duality are the
same.
Another type of duality is that between
homology and cohomology groups on
compact
closed
oriented
manifolds
(i.e., without boundary). Poincar√© dual-
ity
states
that
Hk(M) = Hm‚àík(M)
for
k ‚àà{0, ‚Ä¶ , m} where m is the dimension
of the manifold, M; see [10] for further
details.
Despite this close relationship between
homology
and
cohomology
on
mani-
folds, the cohomology groups have a
naturally deÔ¨Åned product combining two
cochains and this additional structure can
help distinguish between some spaces
that homology does not. We start with
ùúô‚ààCk(X; G) and ùúì‚ààCl(X; G) where the
coeÔ¨Écient group should now be a ring
R (i.e., R should have both addition and
multiplication operations; ‚Ñ§, ‚Ñ§p, and ‚Ñö
are rings.) The cup product is the cochain
ùúô‚å£ùúì‚ààCk+l(X; R) deÔ¨Åned by its action
on a (k + l)-simplex ùúé= ‚ü®v0, ‚Ä¶ , vk+l‚ü©as
follows:
(ùúô‚å£ùúì)(ùúé) = ùúô(‚ü®v0, ‚Ä¶ , vk‚ü©)ùúì(‚ü®vk, ‚Ä¶ , vk+l‚ü©).
The relation between this product and the
coboundary is
ùõø(ùúô‚å£ùúì) = ùõøùúô‚å£ùúì+ (‚àí1)kùúô‚å£ùõøùúì.
From this, we see that the product of
two cocycles is another cocycle, and if
the product is between a cocycle and a
coboundary, then the result is a cobound-
ary. Thus, the cup product is a well-deÔ¨Åned
product on the cohomology groups that is
anticommutative: [ùúô] ‚å£[ùúì] = (‚àí1)kl[ùúì] ‚å£
[ùúô] (provided the coeÔ¨Écient ring, G, is
commutative). These rules for products of
cocycles should look suspiciously familiar
to those who have read Chapter 9. They
are similar to those for exterior products
of diÔ¨Äerential forms and this relationship
is formalized in the next section when we
deÔ¨Åne de Rham cohomology.

226
6 Algebraic Topology
6.4.1
De Rham Cohomology
One interpretation of cohomology that is
of particular interest in physics comes from
the study of diÔ¨Äerential forms on smooth
manifolds; cf. Chapter 9. Recall that a dif-
ferential form of degree k, ùúî, deÔ¨Ånes for
each point p ‚ààM, an alternating multilin-
ear map on k copies of the tangent space to
M at p:
ùúîp ‚à∂TpM √ó ¬∑ ¬∑ ¬∑ √ó TpM ‚Üí‚Ñù.
The set of all diÔ¨Äerential k-forms on a
manifold M is a vector space, Œ©k(M), and
the exterior derivative is a linear opera-
tor that takes a k-form to a (k + 1)-form,
dk ‚à∂Œ©k(M) ‚ÜíŒ©k+1(M)
as
deÔ¨Åned
in
Chapter 9.
The crucial property dd = 0 holds for
the exterior derivative. In this context, k-
forms in the image of d are called exact,
that is, ùúî= dùúéfor some (k ‚àí1)-form ùúé; and
those for which dùúî= 0 are called closed.
We therefore have a cochain complex of
diÔ¨Äerential forms and can form quotient
groups of closed forms modulo the exact
forms to obtain the de Rham cohomology
groups:
Hk
dR(M, ‚Ñù) = ker dk
im dk‚àí1
.
The cup product in de Rham cohomology is
exactly the exterior (or wedge) product on
diÔ¨Äerential forms.
De Rham‚Äôs theorem states that the above
groups are isomorphic to those derived
via simplicial or singular cohomology [23].
And so we see that the topology of a man-
ifold has a direct inÔ¨Çuence on the proper-
ties of diÔ¨Äerential forms that have it as their
domain. For example, the Poincar√© lemma
states that if M is a contractible open sub-
set of ‚Ñùn, then all smooth closed k-forms
on M are exact (the cohomology groups
are trivial). In the language of multivariable
calculus, this becomes Helmholtz‚Äô theorem
that a vector Ô¨Åeld, V, with curlV = 0 in a
simply connected open subset of ‚Ñù3 can
be expressed as the gradient of a poten-
tial function: V = gradf in the appropriate
domain [24]. These considerations play a
key role in the study of electrodynamics via
Maxwell‚Äôs equations [25].
6.5
Morse Theory
We now turn to a primary topic in diÔ¨Äer-
ential topology: to examine the relation-
ship between the topology of a manifold
M and real-valued functions deÔ¨Åned on
M. The basic approach of Morse theory
is to use the level cuts of a function f ‚à∂
M ‚Üí‚Ñùand study how these subsets Ma =
f ‚àí1(‚àí‚àû, a] change as a is varied. For ‚Äúnice‚Äù
functions the level cuts change their topol-
ogy in a well-deÔ¨Åned way only at the critical
points. This leads to a number of power-
ful theorems that relate the homology of a
manifold to the critical points of a function
deÔ¨Åned on it.
6.5.1
Basic Results
A
Morse
function
f ‚à∂M ‚Üí‚Ñù
is
a
smooth
real-valued
function
deÔ¨Åned
on a diÔ¨Äerentiable manifold M such that
each critical point of f is isolated and the
matrix of second derivatives (the Hessian)
is nondegenerate at each critical point. An
example is illustrated in Figure 6.7. The
details on how to deÔ¨Åne these derivatives
with respect to a coordinate chart on M
are given in Chapter 9. This may seem like
a restrictive class of functions but in fact
Morse functions are dense in the space

6.5 Morse Theory
227
(a)
(b)
Figure 6.7
Imagine a torus sitting with one
point in contact with a plane and tilted slightly
into the page as depicted. DeÔ¨Åne a Morse
function by mapping each point on the torus
to its height above the plane. This function has
four critical points: a minimum, two saddles,
and a maximum. (a) Five level cuts of the
height function showing how the topology of
a level cut changes when passing through a
critical point. (b) Gradient Ô¨Çow lines between
the maximum and the two saddle points, and
from each saddle point to the minimum.
of all smooth functions, so any smooth
function can be smoothly perturbed to
obtain a Morse function [26]. Now sup-
pose x ‚ààM is a critical point of f , that is,
df (x) = 0. The index of this critical point is
the number of negative eigenvalues of the
Hessian matrix. Intuitively, this is the num-
ber of directions in which f is decreasing: a
minimum has index 0, and a maximum has
index m where m is the dimension of the
manifold M. Critical points of intermediate
index are called saddles because they have
some increasing and some decreasing
directions.
The two main results about level cuts Ma
of a Morse function f are the following:
‚Ä¢ When [a, b] is an interval for which there
are no critical values of f (i.e., there is no
x ‚ààf ‚àí1([a, b]) for which df (x) = 0), then
Ma and Mb are homotopy equivalent.
‚Ä¢ Let x be a nondegenerate critical point of
f with index i, let f (x) = c, and let ùúñ> 0
be such that f ‚àí1[c ‚àíùúñ, c + ùúñ] is compact
and contains no other critical points.
Then Mc+ùúñis homotopy equivalent to
Mc‚àíùúñwith an i-cell attached.
(Recall that an i-cell is an i-dimensional
unit ball and the attaching map glues the
whole boundary of the i-cell continuously
into the prior space). The proofs of these
theorems rely on homotopies deÔ¨Åned via
the negative gradient Ô¨Çow of f [26].
Gradient Ô¨Çow lines are another key ingre-
dient of Morse theory and allow us to deÔ¨Åne
a chain complex and to compute the homol-
ogy of M. Each point x ‚ààM has a unique
Ô¨Çow line or integral path ùõæx ‚à∂‚Ñù‚ÜíM such
that
ùõæx(0) = x and ùúïùõæx(t)
ùúït
= ‚àí‚àáf (ùõæx(t)).
Taking the limit as t ‚Üí¬±‚àû, each Ô¨Çow line
converges to a destination and an origin
critical point. The unstable manifold of a
critical point p with index i is the set of all
x ‚ààM that have p as their origin; this set
is homeomorphic to an open ball of dimen-
sion i. Correspondingly, the stable manifold
is the set of all x that have p as their des-
tination. For suitably ‚Äúnice‚Äù functions, the
collection of unstable manifolds form a cell
complex for the manifold M [27].

228
6 Algebraic Topology
We can also deÔ¨Åne a more abstract chain
complex, sometimes referred to as the
Morse‚ÄìSmale‚ÄìWitten complex, to reÔ¨Çect
the history of its development. Let Ci be
the chain group derived from formal sums
of critical points of index i. A boundary
operator ùúï‚à∂Ci ‚ÜíCi‚àí1 is then deÔ¨Åned
by mapping p ‚ààCi to a sum of critical
points ‚àëùõºjqj ‚ààCi‚àí1 for which there is a
Ô¨Çow line with p as its origin and q as its
destination. The coeÔ¨Écients ùõºj of the qj
in this boundary chain are the number of
geometrically distinct Ô¨Çow lines that join p
and qj (one can either count mod 2 or keep
track of orientations in a suitable manner).
It requires some eÔ¨Äort to show that ùúïùúï= 0
in this setting; see [27] for details.6) Morse
homology is the homology computed via
this chain complex.
For Ô¨Ånite-dimensional compact mani-
folds, Morse homology is isomorphic to
singular homology, and we obtain the
Morse inequalities relating numbers of
critical points of f ‚à∂M ‚Üí‚Ñùto the Betti
numbers of M:
c0 ‚â•ùõΩ0
c1 ‚àíc0 ‚â•ùõΩ1 ‚àíùõΩ0
c2 ‚àíc1 + c0 ‚â•ùõΩ2 ‚àíùõΩ1 + ùõΩ0
‚ãÆ
‚àë
0‚â§i‚â§m
(‚àí1)m‚àíici =
‚àë
0‚â§i‚â§m
(‚àí1)m‚àíiùõΩi = ùúí(M),
where ci is the number of critical points of
f of index i and ùõΩi is the ith Betti number
of M. Notice that the Ô¨Ånal relationship is
6) In 2D, think of the Ô¨Çow lines that join a single
maximum, minimum pair. In general, such a
region is bounded by Ô¨Çow lines from the maxi-
mum to two saddles and from these saddles to
the minimum. The boundary of the maximum
contains these two saddles and their boundaries
contain the minimum in oppositely induced
orientations.
an equality; the alternating sum of numbers
of critical points is the same as the Euler
characteristic ùúí(M). It also follows from the
above that ci ‚â•ùõΩi for each i.
6.5.2
Extensions and Applications
Morse theory is primarily used as a pow-
erful tool to prove results in other settings.
For example, Morse obtained his results
in order to prove the existence of closed
geodesics on a Riemannian manifold [28];
most famously, Morse theory forms the
foundation of a proof due to Smale of the
higher-dimensional
Poincar√©
conjecture
[29]. Morse theory has been extended
in many ways that relax conditions on
the manifold or the function being stud-
ied [30]. We mention a few of the main
generalizations here.
A Morse‚ÄìBott function is one for which
the critical points may now not be iso-
lated and instead form a critical set that
is a closed submanifold. At the very sim-
plest level for example, this lets us study the
height function of a torus sitting Ô¨Çat on a
table because the circle of points touching
the table is critical [31].
The Conley index from dynamical sys-
tems is a generalization of Morse theory
to Ô¨Çows in a more general class than those
generated by the gradient of a Morse func-
tion. For general Ô¨Çows, invariant sets are
no longer single Ô¨Åxed points but may be
periodic cycles or even fractal ‚Äústrange
attractors.‚Äù In the Morse setting, the index
is simply the dimension of the unstable
manifold of the Ô¨Åxed point, but for gen-
eral Ô¨Çows a more subtle construction is
required. Conley‚Äôs insight was that an
isolated invariant set can be character-
ized by the Ô¨Çow near the boundary of a
neighborhood of the set. The Conley index

6.5 Morse Theory
229
is then (roughly speaking) the homotopy
type of such a neighborhood relative to its
boundary. For details see [32‚Äì34].
Building on Conley‚Äôs work and the
Morse complex of critical points and
connecting orbits, Floer created an inÔ¨Ånite-
dimensional version of Morse homology
now called Floer homology [27]. This has
various formulations that have been used
to study problems in symplectic geometry
(the geometry of Hamiltonian dynam-
ical systems) and also the topology of
three-
and
four-dimensional
manifolds
[35].
There are a number of approaches adapt-
ing Morse theory to a discrete setting, of
increasing importance in geometric model-
ing, image and data analysis, and quantum
Ô¨Åeld theory. The approach due to Forman is
summarized in the following section.
6.5.3
Forman‚Äôs Discrete Morse Theory
Discrete Morse theory is a combinatorial
analogue of Morse theory for functions
deÔ¨Åned on cell complexes. Discrete Morse
functions are not intended to be approxi-
mations to smooth Morse functions, but
the theory developed in [36, 37] keeps
much of the style and Ô¨Çavor of the standard
results from smooth Morse theory.
In keeping with earlier parts of this
chapter, we will give deÔ¨Ånitions for a sim-
plicial complex ÓàØ, but the theory holds for
general cell complexes with little modiÔ¨Åca-
tion. First recall that a simplex ùõºis a face of
another simplex ùõΩif ùõº‚äÇùõΩ, in which case, ùõΩ
is called a coface of ùõº. A function f ‚à∂ÓàØ‚Üí‚Ñù
that assigns a real number to each simplex
in ÓàØis a discrete Morse function if for every
ùõº(p) ‚ààÓàØ, f takes a value less than or equal
to f (ùõº) on at most one coface of ùõºand takes
a value greater than or equal to f (ùõº) on at
most one face of ùõº. In other words,
#{ùõΩ(p+1) > ùõº| f (ùõΩ) ‚â§f (ùõº)} ‚â§1,
and
#{ùõæ(p‚àí1) < ùõº| f (ùõæ) ‚â•f (ùõº)} ‚â§1,
where # denotes the number of elements in
the set. A simplex ùõº(p) is critical if all cofaces
take strictly greater values and all faces are
strictly lower.
A cell ùõºcan fail to be critical in two pos-
sible ways. There can exist ùõæ< ùõºsuch that
f (ùõæ) ‚â•f (ùõº), or there can exist ùõΩ> ùõºsuch
that f (ùõΩ) ‚â§f (ùõº). Lemma 2.5 of [36] shows
that these two possibilities are exclusive:
they cannot be true simultaneously for a
given cell ùõº. Thus each noncritical cell ùõº
may be paired either with a noncritical cell
that is a coface of ùõº, or with a noncritical
cell that is a face of ùõº.
As noted by Forman (Section 3 of [37]), it
is usually simpler to work with pairings of
cells with faces than to construct a discrete
Morse function on a given complex. So we
deÔ¨Åne a discrete vector Ô¨Åeld V as a collec-
tion of pairs (ùõº(p), ùõΩ(p+1)) of cells ùõº< ùõΩ‚ààÓàØ
such that each cell of ÓàØis in at most one pair
of V. A discrete Morse function deÔ¨Ånes a
discrete vector Ô¨Åeld by pairing ùõº(p) < ùõΩ(p+1)
whenever f (ùõΩ) ‚â§f (ùõº). The critical cells are
precisely those that do not appear in any
pair. Discrete vector Ô¨Åelds that arise from
Morse functions are called gradient vector
Ô¨Åelds. See Figure 6.8 for an example.
It is natural to consider the Ô¨Çow associ-
ated with a vector Ô¨Åeld and in the discrete
setting the analogy of a Ô¨Çow line is a V-path.
A V-path is a sequence of cells:
ùõº(p)
0 , ùõΩ(p+1)
0
, ùõº(p)
1 , ùõΩ(p+1)
1
, ùõº(p)
2 , ‚Ä¶ , ùõΩ(p+1)
r‚àí1 , ùõº(p)
r ,
where (ùõºi, ùõΩi) ‚ààV, ùõΩi > ùõºi+1, and ùõºi ‚â†ùõºi+1
for all i = 0, ‚Ä¶ , r ‚àí1. A V-path is a non-
trival closed V-path if ùõºr = ùõº0 for r > 1.

230
6 Algebraic Topology
a
b
c
d
e
f
g
h
i
h
i
e
e
e
f
g
Figure 6.8
A simplicial complex with
the topology of the torus (opposite
edges of the rectangle are identiÔ¨Åed
according to the vertex labels). The
arrows show how to pair simplices in
a gradient vector Ô¨Åeld. A compatible
discrete Morse function has a critical
0-cell (a minimum) at a, two critical
1-cells (saddles) at edges ‚ü®b, h‚ü©
and ‚ü®d, f‚ü©, and a critical 2-cell (a
maximum) at ‚ü®e, i, g‚ü©.
Forman shows that a discrete vector Ô¨Åeld is
the gradient vector Ô¨Åeld of a discrete Morse
function if, and only if, there are no nontriv-
ial closed V-paths (Theorem 9.3 of [36]).
The four results about Morse functions
that we gave earlier all carry over into the
discrete setting: the homotopy equivalence
of level sets away from a critical point,
adding a critical i-cell is homotopy equiv-
alent to attaching an i-cell, the existence
of and homology of the Morse chain com-
plex, and the Morse inequalities. One of the
notable diÔ¨Äerences between the discrete
and continuous theories is that Ô¨Çow lines
for a smooth Morse function on a mani-
fold are uniquely determined at each point,
whereas V-paths can merge and split.
6.6
Computational Topology
An
algorithmic
and
combinatorial
approach to topology has led to signiÔ¨Åcant
results in low-dimensional topology over
the past twenty years. There are two main
aspects to computational topology: Ô¨Årst,
research into methods for making topolog-
ical concepts algorithmic, culminating for
example, in the beginnings of an algorith-
mic classiÔ¨Åcation of (Haken) 3-manifolds
[38] (a result analogous to the classiÔ¨Åca-
tion of closed compact 2-manifolds by
Euler characteristic and orientability). And
second, the challenge to Ô¨Ånd eÔ¨Écient and
useful techniques for extracting topological
invariants from data; see [39] for example.
We start this section by describing simple
algorithms that demonstrate the com-
putability of the fundamental group and
homology groups of a simplicial complex,
and then survey some recent advances in
building cell complexes and computing
homology from data.
6.6.1
The Fundamental Group of a Simplicial
Complex
In Section 6.2, we saw that the fundamen-
tal group of a topological space could be
determined from unions and products of
smaller spaces or by using a covering space.
When the space has a triangulation (i.e., it is
homeomorphic to a polyhedron) there is a
more systematic and algorithmic approach
to Ô¨Ånding the fundamental group as the
quotient of a free group by a set of relations
that we summarize below. See [40] for a
complete treatment of this edge-path group.
Let ÓàØbe a connected Ô¨Ånite simplicial
complex. Any path in |ÓàØ| is homotopic

6.6 Computational Topology
231
to one that follows only edges in ÓàØ, and
any homotopy between edge-paths can
be restricted to the 2-simplices of ÓàØ. This
means the fundamental group depends
only on the topology of the 2-skeleton of ÓàØ.
The algorithm for Ô¨Ånding a presentation of
ùúã1(ÓàØ) proceeds as follows.
First Ô¨Ånd a spanning tree T ‚äÇÓàØ(1) that is,
a connected, contractible subgraph of the
1-skeleton that contains every vertex of ÓàØ;
see Figure 6.9 for an example. One algo-
rithm for doing this simply grows from an
initial vertex v (the root) by adding adjacent
(edge, vertex) pairs only if the other ver-
tex is not already in T. A nontrivial closed
edge-path in ÓàØ(a loop) must include edges
that are not in T and in fact every edge
in ÓàØ‚àíT generates a distinct closed path
in ÓàØ(1). SpeciÔ¨Åcally, for each edge ‚ü®xi, xj‚ü©‚àà
ÓàØ‚àíT there is a closed path starting and
ending at the root v and lying wholly in
T except for the generating edge; we label
this closed path gij. Moreover, any closed
path based at v can be written as a con-
catenation of such generating paths where
inverses are simply followed in the oppo-
site direction: gji = g‚àí1
ij . The gij are therefore
generators for a free group with coeÔ¨Écients
in ‚Ñ§.
Next we use the 2-skeleton ÓàØ(2) to obtain
the homotopy equivalences of closed edge-
paths. Each triangle ‚ü®xi, xj, xk‚ü©‚ààÓàØdeÔ¨Ånes
a relation in the group via gijgjkgki = id
(the identity) where we also set gij = id
if ‚ü®xi, xj‚ü©‚ààT. Let G(ÓàØ, T) be the Ô¨Ånitely
presented group deÔ¨Åned by the above
generators and relations. Then it is possible
to show that we get isomorphic groups
for diÔ¨Äerent choices of T and that G(ÓàØ, T)
is isomorphic to the fundamental group
ùúã1(|ÓàØ|) [40].
If ÓàØhas many vertices, then the presen-
tation of its fundamental group as G(ÓàØ, T)
may not be a very eÔ¨Écient description.
It is possible to reduce the number of
generating edges and relations by using
any
connected
and
contractible
sub-
complex that contains all the vertices
ÓàØ(0) ‚äÇK ‚äÇÓàØ. Generators for the edge-
path group are labeled by edges in ÓàØ‚àíK,
and the homotopy relations are again
deÔ¨Åned by triangles in ÓàØ(2), but we can now
ignore all triangles in K. For the example
of the torus in Figure 6.9, we could take
K to be the eight triangles in the 2 √ó 2
lower left corner of the rectangular grid.
6.6.2
Smith Normal form for Homology
There is also a well-deÔ¨Åned algorithm for
computing the homology groups from a
simplicial complex ÓàØ. This algorithm is
based on Ô¨Ånding the Smith normal form
(SNF) of a matrix representation of the
boundary operator as outlined below.
Recall that the oriented k-simplices form
a basis for the kth chain group, Ck. This
means it is possible to represent the bound-
ary operator, ùúïk ‚à∂Ck ‚ÜíCk‚àí1, by a (non-
square) matrix Ak with entries in {‚àí1, 0, 1}.
The matrix Ak has mk columns and mk‚àí1
rows where mk is the number of k-simplices
in ÓàØ. The entry aij is 1 if ùúéi ‚ààCk‚àí1 is a
face of ùúéj ‚ààCk with consistent orientation,
‚àí1 if ùúéi appears in ùúïùúéj with opposite ori-
entation, and 0 if ùúéi is not a face of ùúéj.
Thus each column of Ak is a boundary
chain in Ck‚àí1 with respect to a basis of
simplices.
The algorithm to reduce an integer
matrix to SNF uses row and column oper-
ations as in standard Gaussian elimination,
but at all stages the entries must remain
integers. The row and column operations
correspond to changing bases for Ck‚àí1 and
Ck respectively and the resulting matrix

232
6 Algebraic Topology
a
b
c
d
e
f
g
h
i
h
i
e
e
e
f
g
Figure 6.9
A simplicial complex with
the topology of a torus (opposite
edges of the rectangle are identiÔ¨Åed
according to the vertex labels). A
spanning tree T with root vertex a
is shown in bold. Any closed path
that starts and ends at a can be
decomposed into a sum of loops that
lie in T except for a single edge.
has the form
Dk =
[Bk
0
0
0
]
, where Bk =
‚é°
‚é¢
‚é¢‚é£
b1
0
‚ã±
0
blk
‚é§
‚é•
‚é•‚é¶
.
Bk is a square matrix with lk nonzero diago-
nal entries that satisfy bi ‚â•1 and b1 divides
b2, divides b3, and so on. For a full descrip-
tion of the basic algorithm see Munkres
[13].
The SNF matrices for ùúïk+1 and ùúïk give
a complete characterization of the kth
homology group Hk. The rank of the
boundary group Bk (im Ak+1) is the num-
ber of nonzero rows of Dk+1, that is, lk+1.
The rank of the cycle group Zk (ker Ak) is
the number of zero columns of Dk, that
is, mk ‚àílk. The torsion coeÔ¨Écients of Hk
are the diagonal entries bi of Dk+1 that are
greater than one. The kth Betti number is
therefore
ùõΩk = rank(Zk) ‚àírank(Bk) = mk ‚àílk ‚àílk+1.
Bases for Zk and Bk (and hence Hk) are
determined by the row and column oper-
ations used in the SNF reduction but the
cycles found in this way typically have poor
geometric properties.
There are two practical problems with
the algorithm for reducing a matrix to SNF
as it is described in Munkres [13]. First,
the time-cost of the algorithm is of a high
polynomial degree in the number of sim-
plices; second, the entries of the interme-
diate matrices can become extremely large
and create numerical problems, even when
the initial matrix and Ô¨Ånal normal form
have small integer entries. When only the
Betti numbers are required, it is possible to
do better. In fact, if we construct the homol-
ogy groups over the rationals, rather than
the integers, then we need only apply Gaus-
sian elimination to diagonalize the bound-
ary operator matrices; doing this, however,
means we lose all information about the
torsion. Devising algorithms that overcome
these problems and are fast enough to be
eÔ¨Äective on large complexes is an area of
active research.
6.6.3
Persistent Homology
The
concept
of
persistent
homology
arose in the late 1990s from attempts to
extract meaningful topological informa-
tion from data [41‚Äì43]. To give a Ô¨Ånite
set of points some interesting topolog-
ical structure requires the introduction
of a parameter to deÔ¨Åne which points
are connected. The key lesson learnt

6.6 Computational Topology
233
from examining data was that rather than
attempting to choose a single best param-
eter value, it is much more valuable to
investigate a range of parameter values
and describe how the topology changes
with this parameter. So persistent homol-
ogy tracks the topological properties of a
sequence of nested spaces called a Ô¨Åltration
¬∑ ¬∑ ¬∑ ‚äÇÓàØa ‚äÇÓàØb ‚äÇ¬∑ ¬∑ ¬∑ where a < b ‚ààÓàµis an
index parameter. In a continuous setting,
the nested spaces might be the level cuts
of a Morse function on a manifold, so that
Óàµis a real interval. In a discrete setting,
this becomes a sequence of subcomplexes
indexed by a Ô¨Ånite set of integers. In either
case, as the Ô¨Åltration grows, topological
features appear and may later disappear.
The persistent homology group, Hk(a, b)
measures the topological features from
ÓàØa that are still present in ÓàØb. Formally,
Hk(a, b) is the image of the map induced
on homology by the simple inclusion of
ÓàØa into ÓàØb. Algebraically, it is deÔ¨Åned by
considering cycles in ÓàØa to be equivalent
with respect to the boundaries in ÓàØb:
Hk(a, b) = Zk(a)‚àï(Bk(b) ‚à©Zk(a)).
Computationally,
persistent
homol-
ogy tracks the birth and death of every
equivalence class of cycle and provides a
complete picture of the topological struc-
ture present at all stages of the Ô¨Åltration.
The initial algorithm for doing this, due
to Edelsbrunner et al. [43], is surprisingly
simple and rests on the observation that
if we build a cell complex by adding a
single cell at each step, then (because all
its faces must already be present) this cell
either creates a new cycle and is Ô¨Çagged
as positive, or ‚ÄúÔ¨Ålls in‚Äù a cycle that already
existed and is labeled negative. If ùúéis a
negative (k + 1)-cell, its boundary ùúïùúéis a
k-cycle and its cells are already Ô¨Çagged as
either positive or negative. The new cell
ùúéis then paired with the most recently
added (i.e., youngest) unpaired positive
cell in ùúïùúé. If there are no unpaired positive
cells available, we must grow ùúïùúéto succes-
sively larger homologous cycles until an
unpaired positive cell is found. By doing
this carefully, we can guarantee that ùúéis
paired with the positive k-cell that created
the homology class of ùúïùúé. Determining
whether a cell is positive or negative a
priori is computationally nontrivial in
general, but there is a more recent version
of the persistence pairing algorithm due
to Zomorodian and Carlsson [44, 45] that
avoids doing this as a separate step, and
also Ô¨Ånds a representative k-cycle for each
homology class.
The
result
of
computing
persistent
homology from a Ô¨Ånite Ô¨Åltration is a list of
pairs of simplices (ùúé(k), ùúè(k+1)) that repre-
sent the birth and death of each homology
class in the Ô¨Åltration. The persistence
interval for each homology class is then
given by the indices at which the creator
ùúéand destroyer ùúèentered the Ô¨Åltration.
Some nontrivial homology classes may be
present at the Ô¨Ånal step of the Ô¨Åltration,
these essential classes have an empty part-
ner and are assigned ‚ÄúinÔ¨Ånite‚Äù persistence.
There are a number of ways to represent
this persistence information graphically:
the two most popular techniques are the
barcode [46] and the persistence diagram
[43, 47]. The barcode has a horizontal axis
representing the Ô¨Åltration index; for each
homology class a solid line spanning the
persistence interval is drawn in a stack
above the axis. The persistence diagram
plots the (birth, death) index pair for each
cycle. These points lie above the diagonal,
and points close to the diagonal are homol-
ogy classes that have low persistence. It
is possible to show that persistence dia-
grams are stable with respect to small
perturbations in the data. SpeciÔ¨Åcally, if

234
6 Algebraic Topology
(a)
(b)
Figure 6.10
(a) Balls of radius a centered on 16 data
points. (b) The nerve of the cover by balls of radius a gives
the ÀáCech complex. In this example, the complex consists of
points, edges, triangles, and a single tetrahedron (shaded
dark gray). As the radius of the balls increases, there are
more intersections between them and higher-dimensional
simplices are created.
the Ô¨Åltration is deÔ¨Åned by the level cuts of a
Morse function on a manifold, then a small
perturbation to this function will produce
a persistence diagram that is close to that
of the original one [47].
6.6.4
Cell Complexes from Data
We now address how to build a cell com-
plex and a Ô¨Åltration for use in persistent
homology computations. Naturally, the
techniques diÔ¨Äer depending on the type of
data being investigated; we discuss some
common scenarios below.
The Ô¨Årst construction is based on a
general technique from topology called
the nerve of a cover. Suppose we have
a collection of ‚Äúgood‚Äù sets (the sets and
their intersections should be contractible)
ÓâÅ= {U1, ‚Ä¶ , UN} whose union ‚ãÉUi is
the space we are interested in. An abstract
simplicial complex Óà∫(ÓâÅ) is deÔ¨Åned by
making each Ui a vertex and adding
a k-simplex whenever the intersection
Ui0 ‚à©¬∑ ¬∑ ¬∑ ‚à©Uik ‚â†‚àÖ.
The
nerve
lemma
states that Óà∫(ÓâÅ) has the same homotopy
type as ‚ãÉUi [10].
If the data set, X, is not too large, and
the points are fairly evenly distributed
over the object they approximate, it makes
sense to choose the Ui to be balls of
radius a centered on each data point:
ÓâÅa = {B(xi, a), xi ‚ààX}. This is often called
the ÀáCech complex; see Figure 6.10. If a < b,
we see that Óà∫(ÓâÅa) ‚äÇÓà∫(ÓâÅb), and we have
a Ô¨Åltration of simplicial complexes that
captures the topology of the data as they
are inÔ¨Çated from isolated points (a = 0) to
Ô¨Ålling all of space (a ‚Üí‚àû).
A similar construction to the ÀáCech com-
plex that is much simpler to compute is
the Vietoris‚ÄìRips or clique complex. Rather
than checking for higher-order intersec-
tions of balls, we build a 1-skeleton from
all pairwise intersections and then add a
k-simplex when all its edges are present.
This construction is not necessarily homo-
topy equivalent to the union of balls, but
is useful when the data set comes from a
high-dimensional space, perhaps with only
an approximate metric.

6.6 Computational Topology
235
(a)
(b)
(c)
Figure 6.11
(a) The Voronoi diagram of a data set with
16 points. (b) The corresponding Delaunay triangulation.
(c) The union of balls of radius a centered on the data
points and partitioned by the Voronoi cells. The corre-
sponding triangulation is almost the same as that shown
in Figure 6.10: instead of the tetrahedron there are just two
acute triangles.
A
drawback
of
the
ÀáCech
and
Vietoris‚ÄìRips complexes is that many
unnecessary
high-dimensional
sim-
plices may be constructed. One way to
avoid this is to build the Delaunay tri-
angulation. There are many equivalent
deÔ¨Ånitions of this widely used geometric
data structure [48]. We start by deÔ¨Åning
the Voronoi partition of space for a data set
{x1, ‚Ä¶ , xN} ‚äÇ‚Ñùm, via the closed cells
V(xi) = {p such that d(p, xi) ‚â§d(p, xj)
for j ‚â†i}.
That is, the Voronoi cell of a data point
is the region of space closer to it than
to any other data point. The boundary
faces of Voronoi cells are pieces of the
(m ‚àí1)-dimensional bisecting hyperplanes
between pairs of data points. The Delau-
nay complex is the geometric dual to the
Voronoi complex: when k + 1 Voronoi cells
share an (m ‚àík)-dimensional face there
is a k-simplex in the Delaunay complex
that spans the corresponding k + 1 data
points.7) See Figure 6.11 for an example
in the plane (m = 2). The geometry of the
Voronoi partition guarantees that there are
no simplices of dimension greater than m
in the Delaunay complex.
Now consider what happens when we
take the intersection of each Voronoi cell
with a ball centered on the data point,
B(xi, a). The Voronoi cells partition the
union of balls ‚ãÉB(xi, a) and the geometric
dual is a subset of the Delaunay complex
that is commonly referred to as an alpha
complex or alpha shape (where alpha
refers to the radius of the ball [49, 50]).
By increasing the ball radius from zero to
some large enough value, we obtain a Ô¨Ål-
tration of the Delaunay complex that starts
with the Ô¨Ånite set of data points and ends
with the entire convex hull. The topology
and geometry of alpha complexes has been
used, for example, in characterizing the
7) This is true for points in general position.
Degenerate conÔ¨Ågurations of points occur in the
plane for example, when four Voronoi cells meet
at a point. In this case the Delaunay complex
may be assigned to contain either a 3-simplex,
a quadrilateral cell, or one of two choices of
triangle pairs.

236
6 Algebraic Topology
shape of and interactions between proteins
[51]. The Betti numbers of alpha shapes
are also a useful tool for characterizing
structural patterns of spatial data [52] such
as the distribution of galaxies in the cosmic
web [53].
When the data set is very large, a
dramatic reduction in the number of sim-
plices used to build a complex is achieved
by deÔ¨Åning landmarks and the witness
complex. This construction generalizes the
Voronoi and Delaunay method, so that only
a subset of data points (the landmarks) are
used as vertices for the complex, while
still maintaining topological accuracy. A
further advantage is that only the distances
between data points are required to deter-
mine whether to include a simplex in the
witness complex. See [54] for details and
[55] for an extensive review of applications
in data analysis.
Another important class of data is digi-
tal images which can be binary (voxels are
black or white), grayscale (voxels take a
range of discrete values), or colored (voxels
are assigned a multidimensional value). In
this setting, the structures of interest arise
from level cuts of functions deÔ¨Åned on a
regular grid. Morse theory is the natural
tool to apply here, although in this applica-
tion, the structures of interest are the level
cuts of the function, while the domain (a
rectangular box) is simple. There are a num-
ber of diÔ¨Äerent approaches to computing
homology from such data and this is an
area of active research. The works [6, 56, 57]
present solutions motivated by applications
in the physical sciences.
Further Reading
We give a brief precis of a few standard texts
on algebraic topology from mathematical
and physical perspectives.
Allen Hatcher‚Äôs Algebraic Topology [10] is
one of the most widely used texts in mathe-
matics courses today and has a strong geo-
metric emphasis. Munkres [13] is an older
text that remains popular. Spanier [40] is a
dense mathematical reference and has one
of the most complete treatments of the fun-
damentals of algebraic topology. A readable
introduction to Morse theory is given by
Matsumoto [26] and Forman‚Äôs [37] review
article is an excellent introduction to his
discrete Morse theory.
Textbooks written for physicists that
cover algebraic topology include Naka-
hara‚Äôs [12] comprehensive book Geometry,
Topology and Physics, Schwarz [58] Topol-
ogy for Physicists, and Naber [59] Topology,
Geometry and Gauge Theory. Each book
goes well beyond algebraic topology to
study
its
interactions
with
diÔ¨Äerential
geometry and functional analysis. A cele-
brated example of this is the Atiyah‚ÄìSinger
index theorem, which relates the analytic
index of an elliptic diÔ¨Äerential operator
on a compact manifold to a topological
index of that manifold, a result that has
been useful in the theoretical physics of
fundamental particles.
References
1. Stillwell, J. (2010) Mathematics and Its
History, 3rd edn, Springer-Verlag, New York.
2. Smale, S. (1967) DiÔ¨Äerentiable dynamical
systems. Bull. Am. Math. Soc., 73, 747‚Äì817.
3. Mindlin, G., Solari, H., Natiello, M., Gilmore,
R., and Hou, X.J. (1991) Topological analysis
of chaotic time series data from the
Belousov-Zhabotinskii reaction. J. Nonlinear
Sci., 1, 147‚Äì173.
4. Muldoon, M., MacKay, R., Huke, J., and
Broomhead, D. (1993) Topology from a time
series. Physica D, 65, 1‚Äì16.
5. Robins, V., Meiss, J., and Bradley, E. (1998)
Computing connectedness: An exercise in
computational topology. Nonlinearity, 11,
913‚Äì922.

References
237
6. Kaczynski, T., Mischaikow, K., and Mrozek,
M. (2004) Computational Homology,
Springer-Verlag, New York.
7. Armstrong, M. (1983) Basic Topology,
Springer-Verlag, New York.
8. Seifert, H. and Threlfall, W. (1980) A
Textbook of Topology, Academic Press.
Published in German 1934. Translated by
M.A. Goldman.
9. Francis, G. and Weeks, J. (1999) Conway‚Äôs
ZIP Proof , The American Mathematical
Monthly, 393‚Äì399.
10. Hatcher, A. (2002) Algebraic Topology,
Cambridge University Press, Cambridge.
11. Mermin, N. (1979) The topological theory of
defects in ordered media. Rev. Mod. Phys.,
51, 591‚Äì648.
12. Nakahara, M. (2003) Geometry, Topology
and Physics, 2nd edn, Taylor and Francis.
13. Munkres, J.R. (1984) Elements of Algebraic
Topology, Addison-Wesley, Reading, MA.
14. Scorpan, A. (2005) The Wild World of
4-Manifolds, American Mathematical
Society, Providence, RI.
15. Hyde, S., Blum, Z., Landh, T., Lidin, S.,
Ninham, B., and Andersson, S. (1996) in The
Language of Shape: The Role of Curvature in
Condensed Matter (ed. K. Larsson), Elsevier,
New York.
16. Mecke, K. (1998) Integral geometry in
statistical physics. Int. J. Mod. Phys. B, 12(9),
861.
17. Baryshnikov, Y. and Ghrist, R. (2009) Target
enumeration via Euler characteristic
integrals. SIAM J. Appl. Math., 70(9),
825‚Äì844.
18. Curry, J., Ghrist, R., and Robinson, M. (2012)
Euler calculus and its applications to signals
and sensing. Proc. Symp. Appl. Math., 70,
75‚Äì146.
19. Arns, C., Knackstedt, M., and Mecke, K.
(2003) Reconstructing complex materials via
eÔ¨Äective grain shapes. Phys. Rev. Lett., 91,
215 506.
20. Scholz, C., Wirner, F., G√∂tz, J., R√ºe, U.,
Schr√∂der-Turk, G., Mecke, K., and
Bechinger, C. (2012) Permeability of porous
materials determined from the Euler
characteristic. Phys. Rev. Lett., 109, 264 504.
21. DelÔ¨Ånado, C.J.A. and Edelsbrunner, H.
(1993) An incremental algorithm for Betti
numbers of simplicial complexes, in SCG ‚Äô93:
Proceedings of the 9th Annual Symposium on
Computational Geometry, ACM Press, New
York, pp. 232‚Äì239.
22. AlexandroÔ¨Ä, P. (1935) in Topologie (ed. H.
Hopf), Springer-Verlag, Berlin.
23. Bott, R. (1982) in DiÔ¨Äerential Forms in
Algebraic Topology (ed. L. Tu),
Springer-Verlag, New York.
24. Nash, C. (1983) in Topology and Geometry
for Physicists (ed. S. Sen), Academic Press,
London.
25. Gross, P. (2004) in Electromagnetic Theory
and Computation: A Topological Approach
(ed. P. Kotiuga), Cambridge University Press,
Cambridge.
26. Matsumoto, Y. (2002) An Introduction to
Morse Theory, AMS Bookstore.
27. Banyaga, A. and Hurtubise, D. (2004)
Lectures on Morse Homology, Kluwer
Academic Publishers, The Netherlands.
28. Morse, M. (1934) The Calculus of Variations
in the Large, Colloquium Publications, vol.
18, American Mathematical Society,
Providence, RI.
29. Smale, S. (1961) Generalized Poincar√©‚Äôs
conjecture in dimensions greater than four.
Ann. Math., 74(2), 391‚Äì406.
30. Bott, R. (1988) Morse theory indomitable.
Publ. Math. de I.H.E.S., 68, 99‚Äì114.
31. Bott, R. (1954) Nondegenerate critical
manifolds. Ann. Math., 60(2), 248‚Äì261.
32. Conley, C.C. and Easton, R. (1971) Isolated
invariant sets and isolating blocks. Trans.
Am. Math. Soc., 158, 35‚Äì61.
33. Conley, C.C. (1978) Isolated Invariant Sets
and the Morse Index, AMS, Providence, RI.
34. Mischaikow, K. (2002) Conley index, in
Handbook of Dynamical Systems, vol. 2,
Chapter 9 (eds M. Mrozek and B. Fiedler),
Elsevier, New York, pp. 393‚Äì460.
35. McDuÔ¨Ä, D. (2005) Floer theory and low
dimensional topology. Bull. Am. Math. Soc.,
43, 25‚Äì42.
36. Forman, R. (1998) Morse theory for cell
complexes. Adv. Math., 134, 90‚Äì145.
37. Forman, R. (2002) A user‚Äôs guide to discrete
Morse theory. S√©minaire Lotharingien de
Combinatoire, 48, B48c.
38. Matveev, S. (2003) Algorithmic Topology
and ClassiÔ¨Åcation of 3-manifolds,
Springer-Verlag, Berlin.
39. Edelsbrunner, H. and Harer, J. (2010)
Computational Topology: An Introduction,
American Mathematical Society, Providence,
RI.

238
6 Algebraic Topology
40. Spanier, E. (1994) Algebraic Topology,
Springer, New York. First published 1966 by
McGraw-Hill.
41. Robins, V. (1999) Towards computing
homology from Ô¨Ånite approximations. Topol.
Proc., 24, 503‚Äì532.
42. Frosini, P. and Landi, C. (1999) Size theory as
a topological tool for computer vision.
Pattern Recogn. Image Anal., 9, 596‚Äì603.
43. Edelsbrunner, H., Letscher, D., and
Zomorodian, A. (2002) Topological
persistence and simpliÔ¨Åcation. Discrete
Comput. Geom., 28, 511‚Äì533.
44. Carlsson, G. and Zomorodian, A. (2005)
Computing persistent homology. Discrete
Comput. Geom., 33, 249‚Äì274.
45. Zomorodian, A. (2009) Computational
topology, in Algorithms and Theory of
Computation Handbook, Special Topics and
Techniques, vol. 2, 2nd edn, Chapter 3, (eds
M. Atallah and M. Blanton), Chapman &
Hall/CRC Press, Boca Raton, FL.
46. Carlsson, G., Zomorodian, A., Collins, A.,
and Guibas, L. (2005) Persistence barcodes
for shapes. Int. J. Shape Modell., 11,
149‚Äì187.
47. Cohen-Steiner, D., Edelsbrunner, H., and
Harer, J. (2007) Stability of persistence
diagrams. Discrete Comput. Geom., 37,
103‚Äì120.
48. Okabe, A., Boots, B., Sugihara, K., and Chiu,
S. (2000) Spatial Tessellations: Concepts and
Applications of Voronoi Diagrams, 2nd edn,
John Wiley & Sons, Ltd, Chichester.
49. Edelsbrunner, H., Kirkpatrick, D., and Seidel,
R. (1983) On the shape of a set of points in
the plane. IEEE Trans. Inform. Theory, 29(4),
551‚Äì559.
50. Edelsbrunner, H. and M√ºcke, E. (1994)
Three-dimensional alpha shapes. ACM
Trans. Graphics, 13, 43‚Äì72.
51. Edelsbrunner, H. (2004) Biological
applications of computational topology, in
Handbook of Discrete and Computational
Geometry, Chapter 63 (eds J. Goodman and
J.O. Rourke), CRC Press, Boca Raton, FL, pp.
1395‚Äì1412.
52. Robins, V. (2006) Betti number signatures of
homogeneous Poisson point processes. Phys.
Rev. E, 74, 061107.
53. van de Weygaert, R., Vegter, G.,
Edelsbrunner, H., Jones, B.J.T., Pranav, P.,
Park, C., Hellwing, W.A., Eldering, B.,
Kruithof, N., Bos, E.G.P., Hidding, J.,
Feldbrugge, J., ten Have, E., van Engelen, M.,
Caroli, M., and Teillaud, M. (2011) Alpha,
Betti and the megaparsec universe: on the
topology of the cosmic web. Trans. Comput.
Sci., XIV, 60‚Äì101.
54. Carlsson, G. (2004) Topological estimation
using witness complexes, in Eurographics
Symposium on Point-Based Graphics (eds V.
de Silva, M. Alexa, and S. Rusinkiewicz),
ETH, Z√ºrich, Switzerland.
55. Carlsson, G. (2009) Topology and data. Bull.
Am. Math. Soc., 46(2), 255‚Äì308.
56. Robins, V., Wood, P.J., and Sheppard, A.P.
(2011) Theory and algorithms for
constructing discrete Morse complexes from
grayscale digital images. IEEE Trans. Pattern
Anal. Mach. Intell., 33(8), 1646‚Äì1658.
57. Bendich, P., Edelsbrunner, H., and Kerber,
M. (2010) Computing robustness and
persistence for images. IEEE Trans. Visual.
Comput. Graphics, 16, 1251‚Äì1260.
58. Schwarz, A. (2002) Topology for Physicists,
Springer-Verlag, Berlin.
59. Naber, G. (2011) Topology, Geometry and
Gauge Ô¨Åelds: Foundations, 2nd edn,
Springer-Verlag, Berlin.

239
7
Special Functions
Chris Athorne
7.1
Introduction
What is so special about a special function?
Within the universe of functions, a zoo
is set aside for classes of functions that are
ubiquitous in applications, particularly well
studied, subject to a general algebraic the-
ory, or judged to be important in some
other manner. There is no deÔ¨Ånition of the
collection comprising such species beyond
writing down a long and never complete list
of the specimens in it.
There have been perhaps three attempts
at constructing large theories of special
functions each of which has been very sat-
isfactory but none entirely comprehensive.
The most fundamental is that of Liou-
ville, later developed by Ritt, in which
functions are built from the ground up
by extending given function Ô¨Åelds via
diÔ¨Äerential equations deÔ¨Åned over those
Ô¨Åelds. Thus, if we allow ourselves to start
with rational functions in x over a Ô¨Åeld of
constants, say ‚ÑÇ, we can extend this Ô¨Åeld by
solutions to ordinary diÔ¨Äerential equations
(ODEs) such as
y‚Ä≤(x) + a(x) = 0
and
y‚Ä≤(x) + a(x)y(x) = 0,
a(x) being a rational function. Most simply,
we may introduce the ‚Äúelementary" func-
tions log and exp by choosing a(x) = 1‚àïx
and a(x) = 1, respectively. The process is
then repeated utilizing the newly created
functions in place of the rationals. Even
then we may not regard all such functions
as special but only those arising in the most
natural manner.
A second way is via the theory of
transformation groups due to Lie. Here
a function inherits particular properties
owing to its being a solution of a diÔ¨Äeren-
tial equation (with boundary conditions)
invariant under some underlying geometri-
cal symmetry. The special functions retain
a memory of this symmetry by arranging
themselves in a highly connected (and
beautiful)
manner
into
representations
of the symmetry group. For instance, the
rotational symmetries of the plane and
of Euclidean three-space are responsible
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

240
7 Special Functions
for the properties of Bessel functions and
spherical harmonics.
A third approach is to start from the
point to which the symmetry theory deliv-
ers us and to call those functions special
that satisfy simple, linear recurrence rela-
tions of the form
ùúïF(x; n)
ùúïx
= ùõºF(x; n + 1).
Here x is a continuous variable, n a discrete
(integer) variable, and ùõºa constant. Many
special functions can be characterized this
way.
Interestingly, this last approach dovetails
best with modern developments in special
function theory. There has been a great
Ô¨Çowering of the theory of discrete systems
in the last decades, connected with dis-
crete geometries, discrete integrable sys-
tems, and ‚Äúquantum" (q‚àí) deformations of
classical objects giving rise to q-analogues
of all special functions.
In the end, it may be that special func-
tions are simply the Ô¨Çotsam and jetsam
thrown up by the sea of mathematical his-
tory. There will always be a need for new
piles into which as yet unclassiÔ¨Åed debris
may be sorted.
The philosophy behind this article is that
since long lists of formulae are to be found
in larger treatises on special functions (to
which the reader will be directed), we con-
centrate on a survey of the landscape and
the mathematical motivation for describ-
ing the properties of such functions. We
will nod in the direction of applications
now and then but we feel that a tome
entitled Tools for Physicists really ought to
leave the question of application as open as
possible.
For the most part, we treat special
functions from the point of view of sym-
metries, summarizing uncomprehensively
but
hopefully
comprehensibly,
classes
of function associated with discrete and
continuous symmetry, referring to the
underlying group theory. We use the
Laplace operator as a source of many of the
paradigms of the theory. We also present
the factorization approach that ties in with
representation theory, as well as an aside
on factorization theory for linear, partial
diÔ¨Äerential operators. Somewhat brieÔ¨Çy,
we cover cases where symmetries are less
apparent: the cases of irregular singular
points; and the general theories of Liouville
and Picard‚ÄìVessiot. Much research eÔ¨Äort
has been expended in developing appli-
cations of functions satisfying nonlinear
ODEs and accordingly we introduce the
elliptic and theta functions as well as the
Painlev√© transcendents. Finally, we take a
step into the more recently explored world
of discrete systems, diÔ¨Äerence equations,
and q-functions, which are related to rep-
resentations of quantum groups in the way
that classical special functions are related
to representations of continuous symmetry
groups. A consistent theme throughout
the treatment is the r√¥le of Ô¨Ånite reÔ¨Çection
groups.
For none of these ideas is there room
for detailed development and many issues
are left untouched. Our treatment through-
out is biased toward the algebraic rather
than the analytic theory. Serious omissions
include the following: asymptotic methods;
completeness of function spaces; integral
representations, and so on. We direct the
reader to cited works as sources of fur-
ther study. References are representative
rather than inclusive and we have tried to
cite what is formative but space, and the
author‚Äôs ignorance, have militated against
an attempt at completeness.

7.2 Discrete Symmetry
241
7.2
Discrete Symmetry
7.2.1
Symmetries
Symmetric objects are distinguished by the
property of invariance under a group of
transformations or redescriptions [1] . The
simplifying consequences of the existence
of symmetric objects for all the sciences
cannot be overstated and the theory of spe-
cial functions is a notable example. We
must start therefore with a brief heuristic
discussion of symmetries.
Let X be a set, assumed Ô¨Ånite for the
moment. We ignore any mathematical
structure this set may possess, so it is really
just a set of labels: say, X = {1, 2, ‚Ä¶ , n}.
Functions from X to itself are then maps
from the set of labels to itself. For example,
we might map every element to the label 1:
f (x) = 1, ‚àÄx ‚ààX. Such a map is not bijec-
tive. The requirement that f be bijective is
natural. In that case, there exists an inverse
function, f ‚àí1 and given any x ‚ààX there
is a unique preimage, f ‚àí1(x), mapping to
x under f . The composition (‚àò) of maps is
associative,
f1 ‚àò(f2 ‚àòf3) = (f1 ‚àòf2) ‚àòf3,
and there is, in particular, a special map
id ‚à∂x ÓÇ∂‚Üíx that maps each label to itself. So
the set of all such maps (which is clearly
Ô¨Ånite) has a group structure (Chapter 5).
This is the symmetric group on n labels, Sn.
Its elements are permutations of the label
set. Sn is the largest group acting on X. Any
subgroup, T, of Sn will also act on X and
the characteristic properties of any such
(sub)group of maps will be that:
id ‚ààT;
f1, f2 ‚ààT ‚áíf1 ‚àòf2 ‚ààT;
f ‚ààT ‚áíf ‚àí1 ‚ààT;
and
id(x) = x,
f1(f2(x)) = (f1 ‚àòf2)(x)
f ‚àòf ‚àí1(x) = f ‚àí1 ‚àòf (x) = x,
‚àÄx.
These properties are taken to deÔ¨Åne the
idea of the group action of any group on any
set (Ô¨Ånite or not).
Now consider the set, denoted by Y X, of
maps from X to some other (usually sim-
pler) set Y. Again, we ignore structure on
Y. If
ùúô‚à∂X ÓÇ∂‚ÜíY
is such a map, and g an element of the group
G acting on X, we deÔ¨Åne a corresponding
action, gÃá, on ùúôby
ùúôÓÇ∂‚Üíg ‚ãÖùúô
where
(g ‚ãÖùúô)(x) = ùúô(g‚àí1(x)) = (ùúô‚àòg‚àí1)(x),
the inverse on the right-hand side being
necessary to ensure the correct ordering in
(g1g2) ‚ãÖùúô= g1 ‚ãÖ(g2 ‚ãÖùúô),
that is, to ensure that the action of G on the
set of maps satisÔ¨Åes our prior requirements
for a group action.
Already we can see the primitive ele-
ments of symmetry creeping in. Let G act
on X (Ô¨Ånite) and let Y X be the set of maps
from X to Y. A map ùúô‚ààY X is said to be
G-invariant if
g ‚ãÖùúô= ùúô, ‚àÄg ‚ààG.

242
7 Special Functions
This means that ùúô(g(x)) = ùúô(x) for each x
and all g. So ùúôis constant on the orbits of
G:
OG(x) = {g(x)|g ‚ààG} ‚äÜX.
For example, let X be the set of n elements
as before and let G be the subgroup Sn‚àí1 ‚äÇ
Sn that Ô¨Åxes only one element, say 1. Then
X is a union of two orbits
X = {1} ‚à™{2, 3, ‚Ä¶ , n}
and any invariant map, ùúô, takes at most two
distinct values: ùúô(1) and ùúô(2) = ùúô(3) ¬∑ ¬∑ ¬∑ =
ùúô(n).
The group Sn has a geometrical interpre-
tation. Consider, for instance, an equilat-
eral triangle with vertices labeled {1, 2, 3}.
The six distinct permutations of these labels
correspond to rotations and reÔ¨Çections of
the triangle to itself. Likewise, the regular
tetrahedron with vertices {1, 2, 3, 4} has a
set of geometrical symmetries correspond-
ing to the elements of S4. These are Ô¨Ånite
subgroups of the group O(3) of isometries
(distance preserving maps‚Äìrotations and
reÔ¨Çections), Ô¨Åxing the origin of ‚Ñù3.
Apart from generalizing the triangle to
regular polygonal prisms, the only regular
polyhedra are the cube, octahedron, dodec-
ahedron and icosahedron. Their symme-
tries complete the list of all Ô¨Ånite subgroups
of O(3). These are not groups of the type Sn.
The symmetry group of the cube and octa-
hedron is generated by S4 and an inversion,
x ÓÇ∂‚Üí‚àíx.
and that of the dodecahedron and icosahe-
dron, is generated by S5 and the inversion.
It is when we pay attention to extra struc-
ture the underlying sets may possess that
we start to make a connection with the the-
ory of special functions. Here are two illus-
trations that we will revisit.
Let K be a Ô¨Åeld, say ‚Ñöor ‚ÑÇ, and let
R = K[x1, ‚Ä¶ , xn]
be the ring of polynomials in indetermi-
nates xi, i = 1, ‚Ä¶ , n. Sn acts on this ring in
the natural way:
(ùúé‚ãÖp)(x1, ‚Ä¶ , xn) = p(xùúé‚àí1(1), ‚Ä¶ , xùúé‚àí1(n)),
p being an arbitrary polynomial (element of
R). Consider
G(p) =
‚àë
ùúé‚ààSn
ùúé‚ãÖp.
This is an invariant element of the ring. For
example,
G(x1) = (n ‚àí1)!(x1 + x2 + ¬∑ ¬∑ ¬∑ + xn).
Such invariant elements are called sym-
metric polynomials (Section 7.2.3) and
although there is clearly an inÔ¨Ånite collec-
tion of such, all can, in fact, be generated
as a polynomial ring over a Ô¨Ånite number
of basic symmetric polynomials for which
a number of canonical choices exist.
As a second example, consider rotations
of the plane about a Ô¨Åxed point (the origin
of coordinates). Let R(ùúÉ) denote the clock-
wise rotation through angle ùúÉ, a linear map
on coordinates, x, y ‚àà‚Ñù2:
x ÓÇ∂‚Üíx cos ùúÉ+ y sin ùúÉ,
y ÓÇ∂‚Üí‚àíx sin ùúÉ+ y cos ùúÉ.
It is easily veriÔ¨Åed that
R(ùúÉ)R(ùúô) = R(ùúÉ+ ùúô),
R(0) = id
and that the set of all R(ùúÉ) comprises an
inÔ¨Ånite group. Real-valued functions on ‚Ñù2

7.2 Discrete Symmetry
243
inherit the group action
R(ùúÉ) ‚ãÖf (x, y) = f (R(‚àíùúÉ)(x, y))).
It is natural to exchange rectangular coor-
dinates for polar coordinates,
x = ùúåcos ùúì,
y = ùúåsin ùúì,
which are adapted to the group: rotations
act on the coordinate function ùúìby trans-
lation,
ùúìÓÇ∂‚Üíùúì‚àíùúÉ,
and the function ùúåis invariant. All invariant
functions are functions of ùúåalone. Such
functions are constant valued on the orbits
of the group, which are the geometrical
circles centered on the origin and the origin
itself.
Because the group is smooth (Chapter 9),
we can also consider a local description of
invariance. In the case of small ùúÉ‚àºùúñ:
x ÓÇ∂‚Üíx + yùúñ+ O(ùúñ2),
y ÓÇ∂‚Üíy ‚àíxùúñ+ O(ùúñ2).
The condition of invariance of a function f
is expressed as
f (x + ùúñy, y ‚àíùúñx) ‚àíf (x, y) = O(ùúñ2)
and by using a Taylor expansion in ùúñ,
(xùúïy ‚àíyùúïx)f = 0.
In polar coordinates, this is simply
ùúïùúÉf = 0.
7.2.2
Coxeter Groups
Although Ô¨Ånite, we shall see in later
sections that groups such as Sn play a
central role in modern aspects of spe-
cial function Theory, for example, Dunkl
operators and Painlev√© equations. One
ubiquitous
class
is
that
of
reÔ¨Çection
(Coxeter) groups [2‚Äì4].
In ‚Ñùn, imagine a set of hyperplanes (‚Äúmir-
rors‚Äù) arranged in such a way that the reÔ¨Çec-
tion of any one hyperplane in another is
again a hyperplane of the set. An example
would be three inÔ¨Ånite systems of orthogo-
nal mirrors, regularly spaced along the x, y,
and z directions in ‚Ñù3 to form an inÔ¨Ånitely
extended ‚Äúmilk crate.‚Äù Another would be
mirrors arranged along the edges of a regu-
lar triangular lattice in ‚Ñù2.
If the set of hyperplanes is Ô¨Ånite (unlike
these two examples), it will associate with
each hyperplane a generating element of a
Ô¨Ånite reÔ¨Çection group and the hyperplanes
will all have a common point of intersec-
tion.
Suppose we have a Ô¨Ånite reÔ¨Çection group
with the hyperplanes in ‚Ñùn all passing
through the origin. To each hyperplane, H,
is associated a nonzero normal vector h,
deÔ¨Åned up to length. The group element
representing a reÔ¨Çection in this hyperplane
is
sh(x) = x ‚àí2 (x, h)
(h, h)h
where the brackets (‚ãÖ, ‚ãÖ) denote the standard
inner product on ‚Ñùn. Clearly sh = sch for
any c ‚àà‚Ñù‚ßµ{0}.
In this way, the set of hyperplanes is asso-
ciated with a classical root system of the
kind exempliÔ¨Åed in the theory of semi-
simple Lie algebras (Section 7.3.1). More
generally, they are root systems of Ô¨Ånite

244
7 Special Functions
Coxeter groups generated by reÔ¨Çections,
W = ‚ü®r1, ‚Ä¶ , rn|(rirj)mij = id‚ü©,
the mij being the positive integer orders
of pair products and, ‚àÄi, mii = 1, so that
r2
i = e.
It is the case that W is a Ô¨Ånite Coxeter
group if and only if it is isomorphic to a
Ô¨Ånite reÔ¨Çection group. Such groups have
been fully classiÔ¨Åed.
Generalizations of the reÔ¨Çection groups
discussed so far are the aÔ¨Éne reÔ¨Çection
groups (aÔ¨Éne Weyl groups). In this case, the
reÔ¨Çection hyperplanes are not restricted to
passing through the origin: one has reÔ¨Çec-
tions in translated planes. Such groups are
semi-direct products of a reÔ¨Çection group
and a translation group associated with a
lattice (as in the examples given at the
beginning of this section). They also have
an important place throughout the mod-
ern theory of special functions and we will
touch on this in later sections.
7.2.3
Symmetric Functions
Symmetric functions are a class of special
functions arising from the most universal
Ô¨Ånite
symmetry
groups
(permutations
of labels) with ubiquitous applications
in mathematics and physics [5, 6]. These
include
combinatorial
problems,
parti-
tioning questions, labeling of irreducible
representations and the parameterization
of geometrical objects such as Grassmann
varieties.
Let Sn act on polynomials in x1, ‚Ä¶ , xn
by permutation of indices. Symmetric
polynomials are those invariant under this
action and the most general examples are
the Schur polynomials.
Let Yùúábe the Young diagram of a parti-
tion of some positive integer m, that is a list
ùúá= (m1, m2, ‚Ä¶)
with the properties
mi‚àí1 ‚â•mi ‚â•0
and
‚àë
i
mi = m.
Thus the partitions of 5 are, writing only
the (Ô¨Ånitely many) nonzero entries: (5),
(4, 1), (3, 2), (3, 1, 1), (2, 2, 1), (2, 1, 1, 1),
(1, 1, 1, 1, 1). The corresponding Young dia-
grams consist of rows of boxes of lengths
mi, left justiÔ¨Åed and ordered downward.
Thus:
Y(5)
=
Y(4,1)
=
Y(3,2)
=
Y(3,1,1)
=
Y(2,2,1)
=
Y(2,1,1,1)
=
Y(1,1,1,1,1)
=
The partition of n 1‚Äôs is abbreviated to (1)n.
From Young diagrams with m boxes,
we create Young tableaux on n labels by
Ô¨Ålling the boxes with choices from the
set {1, ‚Ä¶ , n} subject to the rules that the
numerical values of entries increase weakly
along rows but strictly down columns.
Thus two possible tableaux arising from
Y(3,2) on four labels are

7.2 Discrete Symmetry
245
1 2 2
2 4
and
3 3 3
4 4
.
With any Young tableau we associate a
monomial by including a factor of xi for
each of the occurrences of i in a box of the
tableau. Thus, from the above we get
x1x3
2x4
and
x3
3x2
4.
The Schur function, Sùúá, of a partition ùúá
is the sum (with unit coeÔ¨Écients) over
the monomials associated with all possible
tableaux coming from the Young diagram
of the partition.
Two simple cases correspond to the ele-
mentary symmetric functions
em(x1, ‚Ä¶ , xn) =
‚àë
1‚â§i1<i2<‚Ä¶<im‚â§n
xi1xi2 ‚Ä¶ xim
and the complete symmetric functions
hm(x1, ‚Ä¶ , xn) =
‚àë
1‚â§i1‚â§i2‚â§‚Ä¶‚â§im‚â§n
xi1xi2 ‚Ä¶ xim.
Note that ei = 0 for i > n.
These
polynomials
have
generating
functions:
E(n)(t) =
n
‚àë
0
ektt =
n
‚àè
i=1
(1 + xit)
H(n)(t) =
‚àû
‚àë
0
hktt =
n
‚àè
i=1
(1 ‚àíxit)‚àí1
The elementary and complete symmetric
polynomials are the special cases of Schur
polynomials corresponding to Young dia-
grams of a single column or a single row of
length k ‚à∂ek = S(1)k; hk = S(k).
More generally, a determinantal formula
for Sùúáexists:
Sùúá=
det(x
ùúái+n‚àíi
j
)
i,j
‚àè
1‚â§i<j‚â§n(xi ‚àíxj)
and the Schur polynomials can also be
expressed by the Jacobi‚ÄìTrudi formulae in
terms of either the ek‚Äôs or the hk‚Äôs:
Sùúá= det(hùúái+j‚àíi)i,j
= det(eùúá‚Ä≤
i +j‚àíi)i,j
where ùúá‚Ä≤ denotes the partition of the Young
diagram conjugate to ùúá(transposed about
the diagonal). For example, if
ùúá= (3, 2)
then
Y(3,2) =
so
ùúá‚Ä≤ = (2, 2, 1)
because
Y(2,2,1) =
is the transpose of Y(3,2).
Schur polynomials are rich in identi-
ties and applications. Perhaps the most
important elementary identity is due to
Giambelli. This is simply expressed using
the Frobenius notation for a partition: we

246
7 Special Functions
count in each row the number (ùõºi) of boxes
to the right of the box in the ith place on the
diagonal and in each column the number
(ùõΩi) of boxes below that diagonal place.
Then we write
(ùõº1, ùõº2, ‚Ä¶ |ùõΩ1, ùõΩ2, ‚Ä¶).
For example, the partition (4, 3, 2, 1, 1)
associated with the Young diagram
has ùõº1 = 3, ùõº2 = 1, ùõΩ1 = 4 and ùõΩ2 = 1, its
Frobenius notation being (3, 1|4, 1).
The Giambelli formula expresses the
Schur polynomial of a partition in terms
of simpler Schur polynomials of hook
diagrams:
S(ùõº1,ùõº2,‚Ä¶|ùõΩ1,ùõΩ2,‚Ä¶) = det(S(ùõºi|ùõΩj))i,j.
Another crucial property is a multiplication
formula for Schur polynomials, the Little-
wood‚ÄìRichardson rule: Given Young dia-
grams ùúáand ùúÜwith ùúái ‚â§ùúÜi, ‚àÄi, a skew dia-
gram, ùúÜ‚àïùúáis the shape obtained by remov-
ing the leftmost ùúái boxes from the ith row
of ùúÜ. For example, if ùúÜ= (3, 2, 1) and ùúá=
(1, 1), then ùúÜ‚àïùúáis the skew diagram,
.
A Ô¨Ålling of the boxes in the skew diagram
with integers according to the same rules as
for Young tableaux yields a skew tableau, for
example,
1 1
2
1
2 3
3
4
, . . .
,
and summing over the associated monomi-
als in the same way yields the skew Schur
polynomial associated with that skew dia-
gram.
According
to
the
RSK
(Robinson‚Äì
Schensted‚ÄìKnuth) correspondence, a skew
tableau can be rectiÔ¨Åed to a Young tableau
by a simple algorithmic (Shensted) process.
The
Littlewood‚ÄìRichardson
numbers,
cùúÜ
ùúá,ùúàcount the number of skew tableaux,
ùúÜ‚àïùúáthat rectify to a Young tableau with
diagram ùúà. Then
SùúÜ‚àïùúá=
‚àë
ùúà
cùúÜ
ùúáùúàSùúà,
and
Sùúá‚Ä¢ Sùúà=
‚àë
ùúÜ
cùúÜ
ùúáùúàSùúÜ
where the ‚Ä¢ product is again the Schen-
sted algorithm applied to the tableaux and
extended linearly to the terms in the Schur
polynomials. It is an associative but not a
commutative multiplication.
Quite generally, any partition ùúÜcorre-
sponds to an irreducible representation
(Specht module) of Sn and, in turn, to
an
irreducible
representation
(Schur
or Weyl module) of the general linear
group. The Schur polynomials and Little-
wood‚ÄìRichardson formulae encode the
properties of tensor products and quotients
of these modules.
7.2.4
Invariants of Coxeter Groups
Root systems are associated with Ô¨Ånite
reÔ¨Çection groups according to a stan-
dard classiÔ¨Åcation and we give here some
examples with the corresponding invariant
polynomials [2, 4].
The simplest example is the group of
a single reÔ¨Çection in the hyperplane x1 +

7.2 Discrete Symmetry
247
x2 = 0 acting on ‚Ñù2. Polynomial generators
of the ring of invariants are
y1 = x2
1
y2 = x1x2
y3 = x2
2
subject to a single relation: y1y3 ‚àíy2
2.
For the dihedral group D2n generated by
r1 ‚à∂x1 ÓÇ∂‚Üíx2,
x2 ÓÇ∂‚Üíx1
r2 ‚à∂x1 ÓÇ∂‚ÜíùúÅx1,
x2 ÓÇ∂‚ÜíùúÅ‚àí1x2
ùúÅbeing a primitive nth root of unity, the
ring of invariants is generated by
y1 = x1x2
y2 = xn
1 + xn
2
without relations.
The Weyl group, W = Sn+1, associated
with the series of semi-simple Lie algebras
An (or ùî∞ln) has a root system consisting of
n(n + 1) vectors:
ei ‚àíej,
1 ‚â§i ‚â†j ‚â§n + 1.
The ei are the standard, unit coordinate vec-
tors. A basis of simple, positive roots is Œî =
{ei ‚àíei+1|i = 1, ‚Ä¶ n}. W acts by permuting
coordinate functions xi, i = 1, ‚Ä¶ n + 1 sub-
ject to the hyperplane condition
x1 + x2 + ¬∑ ¬∑ ¬∑ + xn+1 = 0,
and the basis of invariant polynomials is
given by the Ô¨Årst n power sums
fi = xi
1 + xi
2 + ¬∑ ¬∑ ¬∑ + xi
n+1
for i = 1, ‚Ä¶ , n, which can be written in
terms of elementary symmetric functions
using the Newton identities.
7.2.5
Fuchsian Equations
Many partial diÔ¨Äerential equations (PDEs)
arising in physics can, in the presence of
geometrical symmetry, be reduced to a type
of linear ODE having only singular points
in ‚ÑÇof a well-controlled type known as
regular singular points (rsp) described in
the following text [7‚Äì11].
As the simplest type of singular point,
the accompanying theory has been thor-
oughly developed. We summarize some
fundamental results in this section. Further
generalization of these ideas will prove
important later Section 7.6.
Solutions near an rsp, located at z = 0,
are of the form zùõæf (z) or zùõæ(ln z)g(z) where f
or g are holomorphic in the local, complex
parameter z and nonvanishing at the sin-
gular point. The equations of Bessel, Leg-
endre, Hermite, and so on that we shall
meet shortly are of this kind. If, in addition,
the point at inÔ¨Ånity is regular singular, then
the ODE is called Fuchsian. To determine
whether the point at ‚àûhas this property for
a given equation, we transform ‚àûin the z-
plane using w = 1‚àïz to 0 in the w-plane. We
then analyze the singularity at w = 0. The
general family of Fuchsian equations hav-
ing three rsps only (at 0, 1, and ‚àû) is the
hypergeometric family.
A second-order ODE in the complex
plane,
w‚Ä≤‚Ä≤ + p(z)w‚Ä≤ + q(z)w = 0
is said to have a regular singular point at
z = z0 if p and q are meromorphic functions
on ‚ÑÇwith local form,
p(z) = (z ‚àíz0)‚àí1P(z),
P(z0) ‚â†0,
q(z) = (z ‚àíz0)‚àí2Q(z),
Q(z0) ‚â†0,

248
7 Special Functions
P and Q both being analytic near z = z0. If
we shift the independent variable to make
z0 = 0, then a local basis of solutions near 0
is given by
w1 = zùúà1W1(z),
w2 = zùúà2W2(z)
where ùúà1 and ùúà2 satisfy the quadratic indi-
cial equation
ùúà(ùúà‚àí1) + P(0)ùúà+ Q(0) = 0
provided the roots are neither equal nor
diÔ¨Äer by an integer. When the roots are
equal or diÔ¨Äer by an integer, ùúà1 ‚àíùúà2 ‚àà‚Ñ§,
the solution basis may include logarithms
w1 = zùúà1W1(z),
w2 = ln z
w1 + zùúà1W2(z).
Convergent power series solutions for the
W‚Äôs are obtained by substitution into the
ODE to derive second-order recurrence
relations on the coeÔ¨Écients in their analytic
expansion. This is the Frobenius method.
In general, solutions of an ODE near
its rsp‚Äôs are multivalued in the sense that
analytic continuation of a solution along
a closed path about such a point returns
a diÔ¨Äerent solution at the initial point.
Given that one has a local basis of analytic
solutions at each nonsingular point, any
closed path can be associated with a matrix
expressing the continued solutions as lin-
ear combinations of this local basis. Thus
a circuit around the singular point aÔ¨Äects
the Ô¨Årst kind of basis above diagonally,
w1 ÓÇ∂‚Üíe2ùúãiùúà1w1,
w2 ÓÇ∂‚Üíe2ùúãiùúà2w2,
and the second kind of basis triangularly,
w1 ÓÇ∂‚Üíe2ùúãiùúà1w1,
w2 ÓÇ∂‚Üíe2ùúãiùúà1w2 + 2ùúãie2ùúãiùúà1w1.
The matrices formed this way clearly con-
stitute a group: the monodromy group of the
ODE. This group will be Ô¨Ånitely generated if
the number of rsp‚Äôs is Ô¨Ånite but the order of
any generator need not be Ô¨Ånite because the
multivaluedness is determined by the expo-
nents ùúài, which may not be rational, and
by the possible presence of the logarithmic
function. In general, a solution is inÔ¨Ånitely
ramiÔ¨Åed (sheeted) over ‚ÑÇ.
But it is possible to ask, as did Klein,
under
what
circumstances
there
exist
Ô¨Ånitely sheeted solutions. It follows from
the general theory of Riemann surfaces that
such solutions are algebraic: they satisfy a
polynomial equation in themselves with
coeÔ¨Écients polynomial in z ‚àà‚ÑÇ[12].
It is useful to reduce the ODE to a canon-
ical form by eliminating the coeÔ¨Écient p(z)
of w‚Ä≤. A revealing way of doing this is to
attempt to ‚Äúcomplete the square‚Äù in the dif-
ferential operator part by writing it as
(
d
dz + p(z)
2
)2
w + I(z)w = 0
where the function
I(z) = q ‚àíp‚Ä≤
2 ‚àíp2
4
is a diÔ¨Äerential invariant in the sense that it
is unaltered by any z-dependent scaling of
w‚à∂
w(z) ÓÇ∂‚ÜíùúÜ(z)w(z).
We choose linearly independent solutions
w1 and w2 to the ODE, introduce a new
independent variable
s = w1
w2

7.2 Discrete Symmetry
249
and consider z as dependent variable. Then
using the notation
{Z, s} = ‚àí1
Z‚Ä≤2
(
Z‚Ä≤‚Ä≤‚Ä≤
Z‚Ä≤ ‚àí3
2
Z‚Ä≤‚Ä≤2
Z‚Ä≤2
)
,
for the Schwarzian derivative of Z with
respect to s, we obtain
{z, s} = 2I(z).
The Schwarzian derivative is a projective
invariant in the sense that, for ùõº, ùõΩ, ùõæ, and
ùõøconstant with ùõºùõø‚àíùõΩùõæ‚â†0,
{ùõºZ + ùõΩ
ùõæZ + ùõø, s
}
= {Z, s}
and it also has the property
{s, z} = {s, Z}
(
dZ
dz
)2
+ {Z, z}.
Restricting
attention
to
second-order
Fuchsian ODEs and to the situation where
there are only three rsp‚Äôs we may, by a lin-
ear fractional transformation in z, assume
these rsp‚Äôs lie at the points z = 0, z = 1, and
z = ‚àû. Consider a pair of linearly indepen-
dent solutions ùúì1 and ùúì2 to the ODE and
analytically continue along a closed path
around an rsp. The two solutions transform
linearly on return to the initial place and
so their ratio, ùúô= ùúì1‚àïùúì2 transforms by the
M√∂bius map (parameters as above):
ùúôÓÇ∂‚Üíùõºùúô+ ùõΩ
ùõæùúô+ ùõø.
The group of all M√∂bius transformations
is PSL2(‚ÑÇ). We are looking then for ODEs
whose monodromy group is a Ô¨Ånite sub-
group of PSL2(‚ÑÇ). This group is isomor-
phic to SO3(‚Ñù), the group of orientation-
preserving isometries of ‚Ñù3. So we have the
answer to the question in the form of the
Ô¨Ånite subgroups of the rotation group: the
symmetry groups of the regular solids and
prisms discussed earlier (Section 7.2.1).
Klein used this classiÔ¨Åcation to show that
in order for the ODE to have algebraic solu-
tions the invariant I(z) must be one of Ô¨Åve
possible forms up to an arbitrary rational
function, Z(z), namely,
Ii(z) = 1
4Ji(Z)
(
dZ
dz
)2
+ 1
2{Z, z},
where Ji(Z) is given by
J1(Z) = 1 ‚àíN‚àí2
Z2
Ji(Z) =
1 ‚àíùúà‚àí2
2
Z2
+
1 ‚àíùúà‚àí2
1
(Z ‚àí1)2
+
ùúà‚àí2
1 + ùúà‚àí2
2 ‚àíùúà‚àí2
3 ‚àí1
Z(Z ‚àí1)
,
i = 2, 3, 4, 5.
The values of ùúà2, ‚Ä¶ , ùúà5 are tabulated thus:
i
ùúà1
ùúà2
ùúà3
Symmetry
1
N
N
cyclic
2
2
2
n
dihedral
3
2
3
3
tetrahedral
4
2
3
4
cubic
5
2
3
5
icoshedral
The solutions are
w1 = s
(
ds
dz
)‚àí1‚àï2
,
w2 =
(
ds
dz
)‚àí1‚àï2
,
where s is related to Z(z) in each case:
Case 1.
Z =
(s ‚àís1
s ‚àís2
)N
Case 2.
Z = ‚àí1
2
sn ‚àí1
sn

250
7 Special Functions
Case 3.
Z = (s4 + 2
‚àö
3s2 ‚àí1)3
(s4 ‚àí2
‚àö
3s2 ‚àí1)3
Case 4.
Z = (s8 + 14s4 + 1)3
108s4(s4 ‚àí1)2
Case 5.
Z = ‚àí(s20 ‚àí228s15 + 494s10 + 228s5 + 1)3
1728s5(s10 + 11s5 ‚àí1)5
.
Because the ODEs of the special functions
that we shall study below are not always
Fuchsian (‚àûis not an rsp), we can think
of the algebraic solutions above as simple
precursors of the (transcendental) special
functions.
7.3
Continuous Symmetry
Between 1888 and 1893, Sophus Lie [13]
published the three volumes of his Theorie
der Transformationsgruppen, where he laid
the foundations and initiated the systematic
applications of continuous groups of sym-
metries acting on geometric spaces.
The construction of the classical special
functions of applied mathematics can be
approached using continuous symmetries
from two end points.
From one end, we may start with a partial
diÔ¨Äerential operator whose eigenfunctions
we wish to understand and use a geomet-
rical symmetry to reduce the PDE to an
ODE by ‚Äúseparation of variables.‚Äù We may
solve this equation either by the Frobenius
expansion or by a factorization method and
study the properties of the resulting fami-
lies of functions.
Starting from the other end, we may look
for representations of symmetry groups
in function spaces. When these are con-
structed, the corresponding representation
of the Lie algebra gives rise to diÔ¨Äerential
expressions for raising and lowering oper-
ators which present to us the diÔ¨Äerential
equations with which the Ô¨Årst approach
commenced.
As examples, we will discuss spherical
harmonics from the Ô¨Årst point of view and
Bessel functions from the second. We will
also list some results appertaining to other
(but by no means all) classical special func-
tions.
7.3.1
Lie Groups and Lie Algebras
A
group
G
is
said
to
be
a
(Ô¨Ånite-
dimensional) Lie group if it is a Ô¨Ånite-
dimensional
diÔ¨Äerentiable
manifold
endowed with a multiplication map,
ùúá‚à∂G √ó G ‚ÜíG
and an inversion map,
ùúÑ‚à∂G ‚ÜíG,
which are smooth with respect to the dif-
ferentiable structure on G and satisfy the
group axioms for some identity point, e ‚à∂
ùúá(e, x) = ùúá(x, e) = x
ùúá(x, ùúÑ(x)) = ùúá(ùúÑ(x), x) = e
ùúá(ùúá(x, y), z) = ùúá(x, ùúá(y, z)).
They often arise as symmetries of geomet-
rical spaces and their own, intrinsic geom-
etry allows the methodology of diÔ¨Äeren-
tial geometry (Chapter 9) to be eÔ¨Äectively
applied [14‚Äì17].
In particular, G has, at each point g ‚ààG,
a tangent space: TgG. If Lg ‚à∂G ‚ÜíG is the

7.3 Continuous Symmetry
251
map Lg(h) = gh, h being an arbitrary point
in G, then the derivative of this map, dLg
takes ThG to TghG and is an isomorphism
of Ô¨Ånite-dimensional vector spaces.
The set of all vector Ô¨Åelds on G is a mod-
ule over the ring of functions on G and an
inÔ¨Ånite-dimensional vector space over the
Ô¨Åeld of constants.
On the other hand, a vector Ô¨Åeld, v, on G
is left-invariant if
v(Lgh) = dLgv(h), ‚àÄg, h.
The set of such left-invariant Ô¨Åelds is a
Ô¨Ånite-dimensional vector space and can
be identiÔ¨Åed with the tangent space at the
identity, TeG. This is the Lie algebra, ùî§, of
G. In many practical situations in applied
mathematics and theoretical physics, it
is through ùî§that the action of a smooth
symmetry group is recognized. We have
already seen an example: the rotation group
SO(2) has a Lie algebra so(2), which acts
on ‚Ñù2 via a one-dimensional vector Ô¨Åeld:
xùúïy ‚àíyùúïx.
A Lie algebra, ùî§, is endowed with a
(nonassociative) bilinear product, ‚ãÜ, hav-
ing the following properties: every g ‚ààùî§is
nilpotent,
g ‚ãÜg = 0;
and every triple g, h, k ‚ààùî§satisÔ¨Åes the
Jacobi identity,
g ‚ãÜ(h ‚ãÜk) + h ‚ãÜ(k ‚ãÜg) + k ‚ãÜ(g ‚ãÜh) = 0.
Applying the nilpotence condition to g + h,
(g + h) ‚ãÜ(g + h) = 0
and using bilinearity, gives the skewness of
‚ãÜ‚à∂
g ‚ãÜh = ‚àíh ‚ãÜg.
The Universal Enveloping Algebra con-
struction replaces ùî§by the full tensor
algebra (Chapter 9) T(ùî§) (noncommutative
polynomial algebra or sums of ordered
products of elements of ùî§) quotiented by
the ideal generated by elements of the form
g ‚äóh ‚àíh ‚äóg ‚àíg ‚ãÜh.
It allows us to represent the ‚ãÜoperation by
the commutator [‚ãÖ, ‚ãÖ].
The structure of a semi-simple (see next
section), Ô¨Ånite-dimensional Lie algebra is
determined by a commutative subalgebra,
the Cartan subalgebra, ùî•, by which ùî§can
be decomposed as ùî•-eigenspaces, labeled
by roots. These roots are elements ùõº‚ààùî•‚àó,
the vector space dual of ùî•, the elements
of eigenspaces, ùî§ùõº, satisfying [h, e] = ùõº(h)e
for every h ‚ààùî•, e ‚ààùî§ùõº. The roots can be
divided into two types: positive roots ùõº‚àà
Œî+; and negative ùõº‚ààŒî‚àí; the entire algebra
decomposing thus:
g = h
gùõº
gùõº.
ùõº‚ààŒî
ùõº‚ààŒî‚àí
7.3.2
Representations
Very many special functions arise as ele-
ments of Ô¨Ånite- or inÔ¨Ånite-dimensional vec-
tor spaces carrying representations of a Lie
group [14, 18]. In fact, the language of
Lie group and algebra representations is
endemic in modern physics.
If G is a Lie group and V a vector space,
which may be inÔ¨Ånite dimensional, over
a Ô¨Åeld, say ‚ÑÇ, then a representation of G
is a ring homomorphism ùúå‚à∂G ‚ÜíEnd(V),
(Chapter 5) from the group into ‚ÑÇ-linear
maps, endomorphisms, from V to itself, that

252
7 Special Functions
is, ùúåsatisÔ¨Åes the conditions
ùúå(ùõº)ùúå(ùõΩ) = ùúå(ùõºùõΩ)
ùúå(e) = IdV
where e is the group identity and IdV the
identity endomorphism on V. If {ei|i =
1, ‚Ä¶ , n} is a basis of V (and in a formal
sense, we may take n to be inÔ¨Ånity) we
deÔ¨Åne a matrix representation for G via the
identity
ùúå(ùõº)ei =
n
‚àë
j=1
R(ùõº)ijej
and
the
homomorphism
condition
becomes
Rij(ùõºùõΩ) =
n
‚àë
k=1
Rik(ùõº)Rkj(ùõΩ)
Rij(e) = ùõøij.
Representations ùúåon V and ùúå‚Ä≤ on V ‚Ä≤ are
equivalent if there exists an invertible map
S ‚à∂V ‚ÜíV ‚Ä≤ satisfying
ùúå‚Ä≤(ùõº)S = Sùúå(ùõº), ‚àÄùõº‚ààG.
If V has an inner product, <, >‚à∂V √ó V ‚Üí
‚ÑÇ, then ùúåis a unitary representation of G if
for all ùõº‚ààG and all u, v ‚ààV
< ùúå(ùõº)u, ùúå(ùõº)v >=< u, v > .
Any representation of a Ô¨Ånite group or of a
compact Lie Group is equivalent to such a
unitary representation.
A vector subspace U ‚äÇV is invariant if
for all ùõº‚ààG and all u ‚ààU,
ùúå(ùõº)u ‚ààU
and the representation ùúåon V is said to be
reducible if it contains a nontrivial, proper
invariant subspace. If V = U ‚äïW where
U and W are both invariant, then the rep-
resentation ùúåon V is completely reducible
and its matrix representation will decom-
pose into a block diagonal form.
For unitary representations, reducibility
implies complete reducibility.
These ideas carry over to the local situa-
tion via the Lie algebra. ùúÜis a representation
on V of the Lie algebra, ùî§, of the Lie group
G, if for g, h ‚ààùî§and a, b ‚àà‚ÑÇ,
ùúÜ(ag + bh) = aùúÜ(g) + bùúÜ(h)
ùúÜ(g ‚ãÜh) = [ùúÜ(g), ùúÜ(h)],
[‚àó, ‚àó] denoting the commutator of matrices
Equivalence, invariance, reducibility, and
so on, are deÔ¨Åned analogously to the group
case.
A Lie algebra ùî§is solvable if its derived
series
ùî§(0) ‚äÉùî§(1) ‚äÉùî§(2) ¬∑ ¬∑ ¬∑
deÔ¨Åned by
ùî§(i) = ùî§(i‚àí1) ‚ãÜùî§(i‚àí1),
ùî§(0) = ùî§
is Ô¨Ånite. A simple example is the set of
upper triangular matrices. Solvability of an
ideal of ùî§is deÔ¨Åned likewise. ùî§is simple if
it is non-abelian and contains no proper,
nontrivial ideal. It is semi-simple if it has no
nontrivial solvable ideal.
If ùúåis a representation of ùî§on V then V
decomposes into weight spaces
VùúÜ= {v ‚ààV|h(v) = ùúÜ(h)v, ‚àÄh ‚ààùî•}
for each ùúÜ‚ààùî•‚àó. The action of ùî§ùõºon the
weight spaces is
ùî§ùõº‚à∂VùúÜ‚ÜíVùúÜ+ùõº.
The intersections of distinct weight spaces
are trivial, their sum is direct, and in the

7.3 Continuous Symmetry
253
case of Ô¨Ånite-dimensional V,
V =
VùúÜ.
ùúÜ‚ààh‚àó
An element v ‚ààV is said to be maximal
of highest weight ùúÜif v ‚ààVùúÜand ùî§ùõº(v) = 0
for all ùõº‚ààŒî+. The action of the whole of
ùî§on v then generates a cyclic representa-
tion of highest weight ùúÜ. Two cyclic repre-
sentations of the same highest weight are
isomorphic, hence standard. The weights
form a lattice in ùî•‚àóand such a standard
representation exists for any element in the
weight lattice.
Finite-dimensional
cyclic,
irreducible
representations of ùî§necessarily have high-
est weights ùúÜsuch that, for a standard basis,
{hi|i = 1, ‚Ä¶ , rank(ùî•)}, of ùî•, the numbers
ùúÜ(hi) are positive integers.
In the spaces of special functions arising
from the actions of symmetry groups on
diÔ¨Äerential operators, these ùúÜ(hi) appear as
special parameter values and so where the
representations are Ô¨Ånite dimensional, it is
because of integrality conditions on these
parameters.
7.3.3
The Laplace Operator
A commonly occurring situation for the
classical special functions is the need to
solve the Laplace equation in a speciÔ¨Åc
geometry [19, 20]. Of course, one may write
down integral formulae for the solutions in
terms of a Green‚Äôs function for almost any
(asymmetric) geometry, but where there is
some symmetry, the solution space is more
explicitly manifested.
The
Laplace
operator
on
three-
dimensional space, ‚Ñù3,
Œî = ùúï2
x + ùúï2
y + ùúï2
z
has a number of symmetries. In particu-
lar, translations in the x, y, and z variables,
described by the Lie group ‚Ñù3 and rigid
rotations, SO(3) are symmetries.
The Lie algebra of translations is gener-
ated by the set {ùúïx, ùúïy, ùúïz}, which all com-
mute with Œî ‚à∂[Œî, ùúïx] = 0, and so on.
The Lie algebra so(3) of SO(3) is the real,
three-dimensional algebra with basis
{xùúïy ‚àíyùúïx, yùúïz ‚àízùúïx, zùúïx ‚àíxùúïz},
which acts on functions of x, y and z, and
which commutes with Œî.
The commutation property means that if
ùúôis a solution of the Laplace equation, then
so is g(ùúô), g being any of the generators
described above.
If ùúôsolves the Laplace equation on ‚Ñù3,
Œîùúô(x, y, z) = 0,
then so does the function aùúôx + bùúôy + cùúôz,
a, b, and c being constants. So the solution
space is a vector space of functions closed
under this diÔ¨Äerential algebraic operation.
In particular, it suÔ¨Éces to choose functions
satisfying
ùúôx = ùúÜùúô, ùúôy = ùúáùúô, ùúôz = ùúàùúô
for constants ùúÜ, ùúáand ùúà. This is just the
space of functions such as exp(ùúÜx + ùúáy +
ùúàz). (Note that we include complex-valued
functions here because their linear combi-
nations may be real.)
If the geometry of our problem requires
conditions of the form
ùúô(x + L, y + M, z + N) = ùúô(x, y, z)
for Ô¨Ånite, positive L, M and N, then these
are reÔ¨Çected in the choice of sine and cosine
functions as a basis of the solution space.

254
7 Special Functions
An important consequence of these
considerations is that these functions obey
addition laws: if sin x is periodic of period
2ùúã, then so is sin(x + x‚Ä≤) and, by linearity,
must be a linear combination of sin x and
cos x. Determining the multiplicative coef-
Ô¨Åcients from special values of x results in
the addition law:
sin(x + x‚Ä≤) = cos x‚Ä≤ sin x + sin x‚Ä≤ cos x.
This is a common theme in special function
theory, including nonlinear situations.
Other solution spaces will be appropriate
to other geometries. A semi-inÔ¨Ånite, Ô¨Ånite
width strip, for example, will require a com-
bination of periodic and decaying exponen-
tial solutions.
The Laplacian operator is a geometric
object that can be deÔ¨Åned in a coordinate-
free manner [20]. On any smooth, Rieman-
nian manifold with covariant metric gij, the
Laplacian operator is
Œîùúô= ‚àög
‚àí1ùúïi(‚àöggijùúïjùúô)
g being the determinant of gij, regarded as a
symmetric matrix, and where the Einstein
convention for summation of repeated
(contra/covariant) index pairs is in opera-
tion. Coordinates may be chosen adapted
to the geometric symmetry of a problem
and the Laplacian written accordingly.
Examples are listed in [21].
In the case of cylindrical symmetry
about, say, the z-axis, SO(2), it is convenient
to change variables to polar coordinates
(ùúå, ùúì, z). For full rotational symmetry,
SO(3), the spherical polar coordinates
(r, ùúÉ, ùúì) are appropriate. (To be clear, we
use ùúÉhere as the azimuthal angle, constant
on lines of lattitude; ùúìthe equatorial angle,
constant on lines of longitude.)
In such coordinates, the symmetry trans-
formations are essentially trivial, that is,
they amount to simple translations in the
variables.
SO(2) ‚à∂(ùúå, ùúì, z) ÓÇ∂‚Üí(ùúå, ùúì+ ùúñ, z + ùúÇ)
SO(3) ‚à∂(r, ùúÉ, ùúì) ÓÇ∂‚Üí(r, ùúÉ+ ùúÅ, ùúì+ ùúñ)
In the cases of the Laplacian on ‚Ñù3, we
obtain in each case:
Œîùúô= 1
ùúåùúïùúå(ùúåùúïùúåùúô) + 1
ùúå2 ùúï2
ùúìùúô+ ùúï2
zùúô
and
Œîùúô= 1
r2 ùúïr(r2ùúïrùúô) +
1
r2 sin ùúÉùúïùúÉ(sin ùúÉùúïùúÉùúô) +
1
r2 sin2 ùúÉ
ùúï2
ùúìùúô.
Because the group transformations are
translations in ùúÉand ùúì, rotational period-
icity in ùúÉrequires that ùúôbe expanded in
terms of the complete system
{exp inùúì|n ‚àà‚Ñ§}.
Thus
ùúô(r, ùúÉ, ùúì) =
‚àë
n‚àà‚Ñ§
Œ¶neinùúì
where the coeÔ¨Écients Œ¶n(ùúå, z) or Œ¶n(r, ùúÉ)
satisfy
in
each
case
the
diÔ¨Äerential
equations
1
ùúåùúïùúå(ùúåùúïùúåŒ¶n) ‚àín2
ùúå2 Œ¶n + ùúï2
zŒ¶n = 0
and
1
r2 ùúïr(r2ùúïrŒ¶n) +
1
r2 sin ùúÉùúïùúÉ(sin ùúÉùúïùúÉŒ¶n)
‚àí
n2
r2 sin2 ùúÉ
Œ¶n = 0.
For cylindrical symmetry, we may have
either decaying or periodic boundary con-
ditions on z and we obtain an equation of

7.3 Continuous Symmetry
255
Bessel type by expanding in an exponential,
emz series or integral,
1
ùúåùúïùúå(ùúåùúïùúåŒ¶) +
(
m2 ‚àín2
ùúå2
)
Œ¶ = 0
where Œ¶ depends on parameters n and m.
In the case of spherical symmetry, we
may expand Œ¶n(r, ùúÉ) in powers of r,
‚àû
‚àë
l=0
Œ¶nl(ùúÉ)rl
to obtain the associated Legendre equation:
1
sin ùúÉùúïùúÉ(sin ùúÉùúïùúÉŒ¶)+
(
l(l + 1) ‚àí
n2
sin2 ùúÉ
)
Œ¶ = 0
where Œ¶ depends on parameters n and l.
This is more usually written with the inde-
pendent variable x = cos ùúÉ‚à∂
(1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ +
(
l(l + 1) ‚àí
n2
1 ‚àíx2
)
y = 0.
Linearly independent solutions are denoted
Pn
l (x) and Qn
l (x), associated Legendre func-
tions of the Ô¨Årst and second kind. The sec-
ond kind are singular at x = 1 and x = ‚àí1.
When n = 0 and l ‚àà‚Ñ§, the Ô¨Årst kind
functions are polynomial in x and called
Legendre polynomials, Pl(x), if the normal-
ization is chosen such that Pl(1) = 1. l may
be taken to be a positive integer because the
deÔ¨Åning equation
y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + l(l + 1)y = 0
is unchanged under the replacement of l by
‚àí(l + 1).
As follows from later considerations
(Section 7.4), the Legendre polynomials
are given by the Rodrigues formula:
Pl(x) =
1
2ln!
dl
dxl (x2 ‚àí1)l.
The associated functions can be derived
from the Legendre functions using the Fer-
rers formulae:
Pn
l (x) = (1 ‚àíx2)n‚àï2 dn
dxn Pl(x);
Qn
l (x) = (1 ‚àíx2)n‚àï2 dn
dxn Ql(x).
7.3.4
Spherical Harmonics
In many applications, the associated Leg-
endre functions appear under the guise of
(surface) spherical harmonics,
Y n
l (ùúÉ, ùúì) =
‚àö
(2l + 1)(l ‚àín)!
4ùúã(l + m)!
Pn
l (cos ùúÉ)einùúì,
for integer values of m, ‚àíl ‚â§m ‚â§l, or with
a monomial factor in r,
rlY n
l (ùúÉ, ùúì)
as (solid) spherical harmonics [22].
These in turn arise as moments in an
integral expansion in the following manner.
Consider a complex variable ùúÅand an arbi-
trary meromorphic function f . The contour
integral
‚àÆC
dùúÅf
(
XùúÅ+ 2iz + X
ùúÅ, ùúÅ
)
,
X = x + iy, is easily seen to satisfy the
Laplace equation by virtue of
(ùúï2
x + ùúï2
y + ùúï2
z)f =
((
ùúÅ+ 1
ùúÅ
)2
‚àí
(
ùúÅ‚àí1
ùúÅ
)2
‚àí4
)
f ‚Ä≤‚Ä≤ = 0.
If we write ùúÅ= eiùúôand choose the con-
tour to be a unit circle about 0, we obtain as

256
7 Special Functions
an instance of the above integral an expan-
sion in associated Legendre functions:
‚à´
2ùúã
0
dùúô(z + ix cos ùúô+ iy sin ùúô)n cos(mùúô)
= 2ùúãimn!
(n + m)!rnPm
n (cos ùúÉ) cos(mùúì),
ùúÉand ùúìbeing the usual azimuthal and
longitudinal coordinates of spherical polar
coordinates.
The integral representation above is also
of interest as an instance of the (mini-)
twistor transform that has proved funda-
mental to obtaining monopole solutions to
Yang‚ÄìMills‚ÄìHiggs gauge theories [23].
7.3.5
Separation of Variables
In a general coordinate system, {x1, x2, x3},
it is customary to solve the Laplace
equation by choosing an ansatz,
Œ¶(x1, x2, x3) = X1(x1)X2(x2)X3(x3),
and decoupling the dependence of each
variable to obtain three ODE for X1, X2, and
X3 coupled only through some eigenvalue-
like parameters, the possible values of these
parameters being determined by boundary
or initial conditions. This method of sepa-
ration of variables is illustrated now for the
case of parabolic cylinder coordinates [24].
The pair of equations
y2 + 2ùúÜx ‚àíùúÜ2 = 0,
y2 ‚àí2ùúáx ‚àíùúá2 = 0,
represent two mutually orthogonal families
of parabola. ùúÜand ùúáare constant on each
parabola and, with z, constitute parabolic
cylinder coordinates on ‚Ñù3. The standard
metric in these coordinates is,
ds2 = ùúÜ+ ùúá
4
(
dùúÜ2
ùúÜ
+ dùúá2
ùúá
)
+ dz2,
and using the general deÔ¨Ånition of the
Laplacian given earlier (Section 7.3.3) we
obtain,
ŒîŒ¶ =
4
ùúÜ+ ùúá
(‚àö
ùúÜùúïùúÜ(
‚àö
ùúÜùúïùúÜŒ¶)+
‚àö
ùúáùúïùúá(
‚àö
ùúáùúïùúáŒ¶)) + ùúï2
zŒ¶.
Separation of variables,
Œ¶(ùúÜ, ùúá, z) = Œõ(ùúÜ)M(ùúá)Z(z),
and decoupling yields equations for Œõ and
M of the form
d2w
du2 ¬± (1
4u2 ‚àìa)w = 0,
which are solved by Whittaker functions
[22].
7.3.6
Bessel Functions
We illustrate in this section how one starts
from representations of symmetries on
function spaces to arrive at the Bessel
diÔ¨Äerential equation [20, 21, 25‚Äì27]. The
rigid symmetries (isometries) Óà±2 of the
plane, ‚Ñù2, are generated by translations,
x ÓÇ∂‚Üíx + a and rotations, x ÓÇ∂‚ÜíR(ùõº)x. The
general transformation we will denote
ga,ùõº‚à∂x ÓÇ∂‚ÜíR(ùõº)x + a. It is easy to see that
the group law is
ga,ùõºgb,ùõΩ= ga+R(ùõº)b,ùõº+ùõΩ.
This product of groups is a semi-direct
product, ‚Ñù2 ‚ãäSO(2), of translation and
rotation subgroups, the former a normal
subgroup of the full group.

7.3 Continuous Symmetry
257
The corresponding Lie algebra is spanned
by a1 = ùúïx, a2 = ùúïy, and a3 = yùúïx ‚àíxùúïy, x
and y being coordinates on ‚Ñù2. Commuta-
tion relations are:
[a1, a2] = 0
[a2, a3] = a1
[a3, a1] = a2.
By tensoring with ‚ÑÇ, we may as well con-
sider the complex Lie algebra.
It is easily checked that for any z ‚àà‚ÑÇ,
there is a natural representation of Óà±2
on smooth functions on the unit circle,
x ‚ãÖx = 1, namely,
ga,ùõº‚ãÖf (x) = eza‚ãÖxf (R(‚àíùõº)x).
Since a ‚ÑÇ-vector space basis of such
functions is given by the set
{vn ‚à∂= einùúô|n ‚àà‚Ñ§},
we have a representation of the Lie alge-
bra, obtained by diÔ¨Äerentiating the group
representation with respect to its three
parameters:
a1 ‚ãÖvn = 1
2z(vn+1 + vn‚àí1)
a2 ‚ãÖvn = 1
2iz(vn+1 ‚àívn‚àí1)
a3 ‚ãÖvn = ‚àíin vn
These relations show that for z ‚â†0, there is
no invariant subspace and so the function
space is an irreducible representation of Óà±2.
Special functions are entries in the
matrix representation of this action. On
the function space, we have an inner
product,
(f1, f2) = 1
2ùúã‚à´
2ùúã
0
f 1f2 dùúÉ
and we consider the functions
(ga,ùõº‚ãÖvn, vm)
as functions of the group parameters
a ‚àà‚Ñù2 and ùõº. Noting that these depend
only on the diÔ¨Äerences n ‚àím and incor-
porating
a
conventional
multiplicative
factor, we deÔ¨Åne the Bessel functions in the
following way:
Jn(x) = 1
2ùúã‚à´
2ùúã
0
eix sin ùúÉ‚àíinùúÉdùúÉ.
(We have exchanged the ùúåof Section 7.3.3
for x.)
Note that
J‚àín(x) = (‚àí1)nJn(x).
From this deÔ¨Ånition follow, by diÔ¨Äerentia-
tion with respect to x and by integration by
parts, the raising (D+
n ) and lowering (D‚àí
n )
operators:
Jn+1(x) = D+
nJn(x)
Jn‚àí1(x) = D‚àí
nJn(x)
where
D+
n = n
x ‚àíd
dx
D‚àí
n = n
x + d
dx
These operators commute, up to unit shift
in the index, in the sense that
D‚àí
n+1D+
n = D+
n‚àí1D‚àí
n
and give a representation of the translation
algebra on the Bessel functions.
Further one sees that
D‚àí
n+1D+
nJn = D+
n‚àí1D‚àí
nJn = Jn,

258
7 Special Functions
that is, the Bessel function Jn is an eigen-
function, with eigenvalue n2 of the operator
x2 d2
dx2 + x d
dx + x2.
We return to this ‚Äúfactorization" property in
Section 7.4.
7.3.7
Addition Laws
Addition laws between functions deÔ¨Åned
on groups arise because of the underlying
group operation [26, 27]. Thus if g1 and g2
are group elements of G, the matrix ele-
ments of a representation satisfy the homo-
morphism property
(g1g2 ‚ãÖvn, vm) =
‚àë
l
(g1 ‚ãÖvn, vl)(g2 ‚ãÖvl, vm).
We may take a simple choice for the group
elements. In this case, take g1 to be the
translation
ga,0 ‚à∂
a = (x1, 0)
and g2 the translation
ga,0 ‚à∂
a = (x2 cos ùúÉ2, x2 sin ùúÉ2).
Then the product element is a translation
a = (x cos ùúÉ, x sin ùúÉ) with
x2 = x2
1 + x2
2 + 2x1x2 cos ùúÉ2
and
eiùúÉ= x1 + x2eiùúÉ2
x
.
The deÔ¨Ånition of Bessel functions as matrix
elements then gives
einùúÉJn(x) =
‚àû
‚àë
k=‚àí‚àû
eikùúÉ2Jn‚àík(x1)Jk(x2).
From this general addition formula, simpler
ones follow by choices of parameter. For
example, if ùúÉ2 = 0 then we obtain,
Jn(x1 + x2) =
‚àû
‚àë
k=‚àí‚àû
Jn‚àík(x1)Jk(x2)
and for ùúÉ2 = ùúã‚àï2, we obtain
(x1 + ix2
x1 ‚àíix2
)n‚àï2
Jn(
‚àö
x2
1 + x2
2)
=
‚àû
‚àë
k=‚àí‚àû
ikJn‚àík(x1)Jk(x2).
Finally, we cite the Jansen formula,
‚àû
‚àë
k=‚àí‚àû
ikJn+k(x)Jk(x) = ùõøn,0.
7.3.8
The Hypergeometric Equation
The hypergeometric family of functions,
F(ùõº, ùõΩ, ùõæ; z), is universal for special func-
tions with three regular singular points in
the extended complex plane, that is, for
Fuchsian equations (Section 4.2.5)[21, 22,
25]. Many special functions appear as spe-
cial cases of this family.
Suppose
we
have
a
second-order
Fuchsian ODE, in z and w(z), with dis-
tinct regular singular points at z = a, b,
and c, carrying exponent pairs (roots of the
indicial equation in the Frobenius method),
(ùõº1, ùõº2), (ùõΩ1, ùõΩ2) and (ùõæ1, ùõæ2). The Riemann
P-symbol denotes the set of solutions to
this ODE with these local properties:
P
‚éß
‚é™
‚é®
‚é™‚é©
a
b
c
ùõº1
ùõΩ1
ùõæ1
; z
ùõº2
ùõΩ2
ùõæ2
‚é´
‚é™
‚é¨
‚é™‚é≠
The exponents of the solutions of the Fuch-
sian equation, and hence the P-symbol, are

7.3 Continuous Symmetry
259
unaltered by M√∂bius maps on the indepen-
dent z-variable, except for the locations of
the singularities:
z ÓÇ∂‚ÜíùúÜz + ùúá
ùúàz + ùúå,
a ÓÇ∂‚Üía‚Ä≤, b ÓÇ∂‚Üíb‚Ä≤, c ÓÇ∂‚Üíc‚Ä≤,
P
‚éß
‚é™
‚é®
‚é™‚é©
a
b
c
ùõº1
ùõΩ1
ùõæ1
; z
ùõº2
ùõΩ2
ùõæ2
‚é´
‚é™
‚é¨
‚é™‚é≠
=
P
‚éß
‚é™
‚é®
‚é™‚é©
a‚Ä≤
b‚Ä≤
c‚Ä≤
ùõº1
ùõΩ1
ùõæ1
; z
ùõº2
ùõΩ2
ùõæ2
‚é´
‚é™
‚é¨
‚é™‚é≠
;
but under maps on the dependent variable
ÃÉw(z) = (z ‚àía)k(z ‚àíb)l
(z ‚àíc)k+l
w(z)
the symbol transforms as
(z ‚àía)k(z ‚àíb)l
(z ‚àíc)k+l
P
‚éß
‚é™
‚é®
‚é™‚é©
a
b
c
ùõº1
ùõΩ1
ùõæ1
; z
ùõº2
ùõΩ2
ùõæ2
‚é´
‚é™
‚é¨
‚é™‚é≠
= P
‚éß
‚é™
‚é®
‚é™‚é©
a
b
c
ùõº1 + k
ùõΩ1 + l
ùõæ1 ‚àík ‚àíl
; z
ùõº2 + k
ùõΩ2 + k
ùõæ2 ‚àík ‚àíl
‚é´
‚é™
‚é¨
‚é™‚é≠
The hypergeometric diÔ¨Äerential equation is
z(1 ‚àíz)w‚Ä≤‚Ä≤ + (ùõæ‚àí(ùõº+ ùõΩ+ 1)z)w‚Ä≤ ‚àíùõºùõΩw = 0
which, it should be noted, is symmetric in
ùõºand ùõΩ, and the hypergeometric function
solution obtained by the Frobenius method,
F(ùõº, ùõΩ, ùõæ; z) =
‚àû
‚àë
0
(ùõº)n(ùõΩ)n
n!(ùõæ)n
zn,
is valid for |z| < 1. The Pochhammer sym-
bols (‚ãÖ)n are deÔ¨Åned by
(ùõº)n = Œì(ùõº+ n)
Œì(ùõº)
,
(ùõº)0 = 1,
the gamma function being the analytic
extension of the factorial function
Œì(z) = ‚à´
‚àû
0
dte‚àíttz‚àí1,
Re(z) > 0;
Œì(z + 1) = z!,
z ‚àà‚Ñï.
When ùõæ‚àâ‚Ñ§, the second solution inside the
open disk |z| < 1 is given by
z1‚àíùõæF(ùõº‚àíùõæ+ 1, ùõΩ‚àíùõæ+ 1, 2 ‚àíùõæ; z).
The solution corresponds to the Riemann
P-symbol
P
‚éß
‚é™
‚é®
‚é™‚é©
0
1
‚àû
0
0
ùõº
; z
1 ‚àíùõæ
ùõæ‚àíùõº‚àíùõΩ
ùõΩ
‚é´
‚é™
‚é¨
‚é™‚é≠
.
Many elementary and special functions
are associated with speciÔ¨Åc choices of
the parameters. When ùõæ= ùõΩ, we recover
(rational) binomials. Also, for example,
arcsin z = zF
(
1
2, 1
2, 3
2; z2
)
;
ln 1 + z
1 ‚àíz = 2zF
(
1
2, 1, 3
2, z2
)
;
and the Legendre polynomials,
Pn(z) =
(2n)!
2n(n!)2 znF
(
n
2 , 1 ‚àín
2
, 1
2 ‚àín, 1
z2
)
.
A related class of function (including Bessel
functions) have irregular singular points at
inÔ¨Ånity that can be controlled by allowing
two of the regular singular points in the

260
7 Special Functions
hypergeometric function to coalesce. These
are the conÔ¨Çuent hypergeometric functions.
The hypergeometric functions satisfy
three-term recurrence relations:
(ùõæ‚àí1)F(ùõº, ùõΩ, ùõæ‚àí1, z) ‚àíùõºF(ùõº+ 1, ùõΩ, ùõæ, z)
‚àí(ùõæ‚àíùõº‚àí1)F(ùõº, ùõΩ, ùõæ, z) = 0
and
ùõæF(ùõº, ùõΩ, ùõæ, z) ‚àíùõΩzF(ùõº, ùõΩ+ 1, ùõæ+ 1, z)
‚àíùõæF(ùõº‚àí1, ùõΩ, ùõæ, z) = 0.
A further class of generalization is the
hypergeometric
series
in
two
sets
of
parameters,
rFs
(a1 ‚Ä¶ ar
b1, ‚Ä¶ bs
; z
)
=
‚àû
‚àë
n=0
(a1, ‚Ä¶ , ar)n
(b1, ‚Ä¶ , bs)n
zn
n!.
Here the symbols (a1, ‚Ä¶ , ar), and so on,
are deÔ¨Åned as products of the Pochhammer
symbols:
(a1, ‚Ä¶ , ar)n =
r‚àè
i=1
(ai)n;
and the previously deÔ¨Åned hypergeometric
function corresponds to
F(ùõº, ùõΩ, ùõæ; z) = 2F1
(ùõº, ùõΩ
ùõæ
; z
)
.
7.3.9
Orthogonality
The generality of the classical special
functions
satisfy
the
Sturm‚ÄìLiouville
equations:
L(y) ‚â°d
dx
(
k(x) dy
dx
)
+ (ùúÜg(x) ‚àíl(x))y = 0.
k(x) > 0, l(x) and g(x) > 0 are real contin-
uous functions on an interval [a, b], ùúÜan
eigenvalue, and we assume boundary con-
ditions,
ùõº1y(a) + ùõº2y‚Ä≤(a) = 0,
ùõΩ1y(b) + ùõΩ2y‚Ä≤(b) = 0.
It can be shown that there is an unbounded,
inÔ¨Ånite set of eigenvalues {ùúÜi}‚àû
0 with eigen-
functions {yn(x)}‚àû
0
such that yn(x) has
exactly n zeros in the interval (a, b) [9, 22].
The diÔ¨Äerential operator is self-adjoint:
‚à´
b
a
dx uLv = ‚à´
b
a
dx vLu.
From this follows an integral identity for
eigenfunctions,
(ùúÜi ‚àíùúÜj) ‚à´
b
a
dx g(x)yiyj = 0,
so that eigenfunctions of distinct eigenval-
ues are orthogonal with respect to the inner
product
(u, v) = ‚à´
b
a
dx g(x)u(x)v(x).
In this way, one obtains, for instance, for the
associated Legendre functions, the orthog-
onality relations
‚à´
1
‚àí1
dx Pm
n (x)Pm
r (x) =
2
2n + 1
(n + m)!
(n ‚àím)!ùõør,n.
Provided (yi, yi) > 0, we can normalize the
eigenfunctions by positive constants to
obtain an orthonormal set {yn(x)}‚àû
0 ,
(yi, yj) = ùõøi,j.
7.3.10
Orthogonal Polynomials
Suppose we have a positive, Borel mea-
sure, dùúá(x) = g(x)dx, on ‚Ñù[21, 28‚Äì30]. We

7.4 Factorization
261
can always construct from it a sequence
{ùúôn(x)}‚àû
0 , of monic, orthogonal polynomi-
als with respect to this measure in the sense
that
‚à´‚Ñù
dùúáùúôn(x)ùúôm(x) = ùúÅnùõøm,n,
where the numbers {ùúÅn}‚àû
0 are all positive
and ùúÅ0 = 1.
We achieve this by deÔ¨Åning Hankel deter-
minants,
Hn = det(ùúái+j‚àí2)1‚â§i,j‚â§n
the ùúáj being moments of monomials with
respect to dùúá(x) ‚à∂
ùúáj = ‚à´‚Ñù
dùúá(x) xj.
Then deÔ¨Åne
ùúôn(x) =
1
Œîn‚àí1
||||||||||||
ùúá0
ùúá1
‚Ä¶
ùúán
ùúá1
ùúá2
‚Ä¶
ùúán+1
‚ãÆ
‚ãÆ
ùúán‚àí1
ùúán
‚Ä¶
ùúá2n‚àí1
1
x
‚Ä¶
xn
||||||||||||
where Œîn is the Hankel determinant,
Œîn =
|||||||||
ùúá0
ùúá1
‚Ä¶
ùúán
ùúá1
ùúá2
‚Ä¶
ùúán+1
‚ãÆ
‚ãÆ
ùúán
ùúán+1
‚Ä¶
ùúá2n
|||||||||
.
These
polynomials
constitute
such
a
sequence with
ùúÅn =
Œîn
Œîn‚àí1
.
The sequence also satisÔ¨Åes a three-term
recurrence relation of the form
ùúôn+1 + (ùõºn ‚àíx)ùúôn + ùõΩnùúôn‚àí1 = 0,
the constants ùõºn ‚àà‚Ñùand ùõΩn > 0 for n > 0,
depending on the measure.
Further, the converse holds (the spectral
theorem): given such a sequence of monic,
orthogonal polynomials, satisfying a recur-
rence relation of this form, there exists a
measure such that {ùúôn(x)}‚àû
0 , can be con-
structed as above with
ùúÅn = ùõΩ1ùõΩ2 ¬∑ ¬∑ ¬∑ ùõΩn.
Such orthogonal polynomials also satisfy
second-order, linear ODE and diÔ¨Äerential-
diÔ¨Äerence recurrence relations.
One example, amongst many topical in
the current literature because of their q-
deformed cousins is the sequence of Jacobi
polynomials on [‚àí1, 1]:
g(x) = (1 ‚àíx)ùõº(1 + x)ùõΩŒì(ùõº+ ùõΩ+ 2)
2ùõº+ùõΩ+1Œì(ùõº+ 1)Œì(ùõΩ+ 1)
,
pn(x) =
P(ùõº,ùõΩ)
n
(x)
‚àö
h(ùõº,ùõΩ)
n
,
h(ùõº,ùõΩ)
n
=
(ùõº+ ùõΩ+ 1)(ùõº+ 1)n(ùõΩ+ 1)n
(2n + ùõº+ ùõΩ+ 1)n!(ùõº+ ùõΩ+ 1)n
,
and
P(ùõº,ùõΩ)
n
(x)= (ùõº+ 1)n
n!
2F1
(‚àín, ùõº+ ùõΩ+ n + 1
ùõº+ 1
; x
)
7.4
Factorization
The linear diÔ¨Äerential equations satisÔ¨Åed by
special functions of the type described are
all factorizable and this property provides a
neat way to obtain functional properties of
the special functions in question. Originally
driven by the analogy with factorizing poly-
nomial equations, this method is developed
and its applications in quantum mechanics
explored in [31].

262
7 Special Functions
7.4.1
The Bessel Equation
We have seen already that the Bessel oper-
ator factorizes up to a constant
ùúï2
x + 1
xùúïx + (1 ‚àím2
x2 ) =
(ùúïx ‚àím ‚àí1
x
)(ùúïx + m
x ) ‚àí1.
Denoting the factors, as before (Section
7.3.6) by
D¬±
m = ‚àìùúïx + m
x
and the Bessel function, Jm, we have
D+
m‚àí1D‚àí
mJm = D‚àí
m+1D+
mJm = Jm.
Because this holds for all m, it is the case
that
D+
mJm = Jm+1
D‚àí
mJm = Jm‚àí1,
which are recurrence relations for the
Bessel functions. It is easy to obtain a
generating function for them. DeÔ¨Åne
J(z) =
‚àë
m‚àà‚Ñ§
zmJm(x).
Then, summing the recurrence formulae
above over zm,
ùúïxJ(z) = 1
2(z ‚àí1
z )J(x)
and so
J(z) = exp ((z ‚àí1
z )x
2
).
7.4.2
Hermite
The wave function, ùúì, of the quantum har-
monic oscillator of a speciÔ¨Åed energy level
(ùúÜl) obeys the equation,
(‚àíùúï2
x + x2)ùúìl = ùúÜlùúìl.
We can write this in either of two factorized
forms:
‚àí(ùúïx ‚àíx)(ùúïx + x)ùúìl = (ùúÜl ‚àí1)ùúìl;
and
‚àí(ùúïx + x)(ùúïx ‚àíx)ùúìl = (ùúÜl + 1)ùúìl.
Writing D+ = ùúïx ‚àíx and D‚àí= ùúïx + x, and
noting that
[D‚àí, D+] = 2,
one has the system
D‚àíùúìl = (ùúÜl ‚àí1)ùúìl‚àí1
D+ùúìl‚àí1 = (ùúÜl + 1)ùúìl.
If consequently, for each value of l, ùúìl sat-
isÔ¨Åes the diÔ¨Äerential equation with eigen-
value ùúÜl, these eigenfunctions are related
by raising (creation) and lowering (annihi-
lation) operators, D‚àíand D+ and by the
eigenvalue relation ùúÜl+1 = ùúÜl + 2.
Assume there is a highest weight vec-
tor, ùúì0, in the kernel of D‚àí. Then D‚àíùúì0 =
0 implies ùúì0 = e‚àí1‚àï2x2 and ùúÜ0 = 1. Applica-
tion of D+ l times to ùúì0 will yield a prod-
uct of a polynomial Hl(x) and the Gaussian
exponential:
ùúìl = (D+)lùúì0 = Hl(x)e‚àíx2‚àï2,
an eigenfunction of eigenvalue 2l + 1.
The Hl(x) are the Hermite polynomials,
which satisfy the ODE
H‚Ä≤‚Ä≤
l ‚àí2xH‚Ä≤
l + 2lHl = 0

7.4 Factorization
263
obtained from the oscillator via the above
substitution for ùúìl.
Note that as operators we may write
D+ = ex2‚àï2ùúïxe‚àíx2‚àï2,
so that
(D+)l = (ex2‚àï2ùúïxe‚àíx2‚àï2) ¬∑ ¬∑ ¬∑ (ex2‚àï2ùúïxe‚àíx2‚àï2)
= ex2‚àï2ùúïl
xe‚àíx2‚àï2.
Combining these, we get a formula for the
lth Hermite polynomial:
Hl(x) = ex2ùúïl
x(e‚àíx2).
Using ùúìl‚àí1, ùúìl, and ùúìl+1 to eliminate deriva-
tives in the system for ùúìl, one replaces
the second-order diÔ¨Äerential equation by
a three-term recurrence relation for the
Hl(x) ‚à∂
Hl+1 ‚àí2xHl + 2lHl‚àí1 = 0.
Such a relation is the basis for a continued
fraction expansion. Put hl =
Hl
Hl‚àí1 so that the
recurrence relation becomes
hl+1 = 2x ‚àí2l
hl
.
Then
hl = 2x ‚àí
2(l ‚àí1)
2x ‚àí2(l‚àí2)
2x‚àí¬∑¬∑¬∑
.
Finally, we may obtain a generating func-
tion. Start with the system form for the Hl,
H‚Ä≤
l = 2lHl‚àí1,
H‚Ä≤
l ‚àí2xHl = ‚àíHl+1,
which corresponds to the factorizations, up
to constants, of the H-equation,
ùúïx(ùúïx ‚àí2x)Hl = ‚àí2(l + 1)Hl
and
(ùúïx ‚àí2x)ùúïxHl = ‚àí2lHl.
The generating function
H(x, t) =
‚àû
‚àë
l=0
tl
l!Hl(x)
is seen to satisfy the compatible pair of
PDEs
Hx = 2tH,
Ht = 2(x ‚àít)H,
whose solution is
H(x, t) = e‚àít2+2xt.
7.4.3
Legendre
In this case, introduce the linear diÔ¨Äerential
operators [32]
L = eiùúì(ùúïùúÉ+ i cot ùúÉùúïùúì))
L‚àó= ‚àíe‚àíiùúì(ùúïùúÉ‚àíi cot ùúÉùúïùúì))
M = 1
i ùúïùúì.
Then
LL‚àó= ‚àíŒî ‚àíM(M ‚àí1)
L‚àóL = ‚àíŒî ‚àíM(M + 1),
where
Œî = ùúï2
ùúÉ+ cot ùúÉùúïùúÉ+
1
sin2 ùúÉ
ùúï2
ùúì
is the Laplacian in spherical coordinates on
the sphere (constant r).
The operators {L, L‚àó, M} form a repre-
sentation of the Lie algebra su(2) of the Lie
group SU(2) of unitary, 2 √ó 2 matrices of

264
7 Special Functions
unit determinant, as is seen from the com-
mutation relations:
[M, L] = L,
[M, L‚àó] = ‚àíL‚àó,
[L, L‚àó] = 2M.
The equivalence is given by the correspon-
dence
L ‚àº
( 0
i
0
0
)
L‚àó‚àº
( 0
0
‚àíi
0
)
M ‚àº1
2
( 1
0
0
‚àí1
)
One checks that the generators commute
with the Laplacian:
[L, Œî] = [L‚àó, Œî] = 0,
[M, Œî] = 0.
In fact, in this sense, the Œî operator
is a Casimir element of the universal
enveloping algebra of su(2): it commutes
with every generator and its eigenvalues,
l(l + 1), indicate the dimension d of a
(Ô¨Ånite-dimensional) representation via the
formula,
l(l + 1) = 1
4(d2 ‚àí1).
Because Œî and M commute, one may form
a pair of mutually consistent diÔ¨Äerential
equations
MY m
l
= mY m
l ,
and
ŒîY m
l
= ‚àíùúÜ(l)Y m
l ,
deÔ¨Åning functions Y m
l
where the depen-
dence of ùúÜon l is to be determined.
It is easy to show, using the commutation
relations, that the function LY m
l satisÔ¨Åes
M(LY m
l ) = (m + 1)LY m
l ,
Œî(LY m
l ) = ‚àíùúÜ(l)(LY m
l ),
meaning that L plays the r√¥le of a raising
operator:
LY m
l
= Y m+1
l
.
In a similar manner, L‚àóis a lowering opera-
tor:
L‚àóY m
l
= Y m‚àí1
l
.
If we assume that the set {Y m
l |l‚Ä≤ ‚â§m ‚â§l}
forms the basis of a Ô¨Ånite-dimensional rep-
resentation of su(2), then we will have a
highest weight element Y l
l satisfying
LY l
l = 0.
Using the identity for L‚àóL yields
ùúÜ(l) = l(l + 1).
From the theory of Ô¨Ånite-dimensional
representations,
we
conclude that
the
set {Y m
l | ‚àíl ‚â§m ‚â§l} is the basis for an
irreducible representation of su(2) of (odd)
dimension 2l + 1. These are called integer
spin representations. Representations of
even dimension (half integer spin) require
spinors [33].
Solving the deÔ¨Åning equations for the
Y m
l (ùúÉ, ùúì) gives (up to normalization) the
spherical harmonics
Y m
l (ùúÉ, ùúì) = eimùúìPm
l (ùúÉ).
The Pm
l (ùúÉ) are associated Legendre func-
tions and satisfy the associated Legendre

7.4 Factorization
265
equation:
1
sin ùúÉùúïùúÉ(sin ùúÉùúïùúÉPm
l (ùúÉ)) +
(l(l + 1) ‚àí
m2
sin2 ùúÉ
)Pm
l (ùúÉ) = 0
7.4.4
‚ÄúFactorization‚Äù of PDEs
The idea of factorization Ô¨Ånds a general
context in the theory of the linear PDEs
that eÔ¨Äect the inverse scattering transform
for integrable nonlinear PDEs [34‚Äì36]. The
special character of the soliton solutions to
such integrable equations allows us to think
of them as nonlinear special functions.
Given a general second-order PDE of
hyperbolic type in two independent vari-
ables and in canonical form,
Lùúô‚â°(ùúïxùúïy + aùúïx + bùúïy + c)ùúô= 0,
a and b being functions of x and y, we can
attempt a natural factorization in two ways:
((ùúïx + b)(ùúïy + a) ‚àíh)ùúô= 0,
((ùúïy + a)(ùúïx + b) ‚àík)ùúô= 0.
The functions
h = ab + ax ‚àíc
k = ab + by ‚àíc
are invariants under linear scalings of ùúô‚à∂
ùúôÓÇ∂‚ÜíùúÜ(x, y)ùúô
and they measure the obstruction to factor-
izing the linear diÔ¨Äerential operator. Van-
ishing of either allows the reduction of the
second-order equation to triangular form
and the general integration problem to a
pair of quadratures.
The invariants classify operators up
to scaling maps: two distinct operators
sharing the same invariants are necessarily
related by such a scaling map.
From each of the forms, we can deÔ¨Åne
new independent variables
ùúôùúé= (ùúïy + a)ùúô
and
ùúôŒ£ = (ùúïx + b)ùúô
(the Laplace transforms of ùúô) satisfying
(ùúïx + b)ùúôùúé= hùúô
and
(ùúïy + a)ùúôŒ£ = kùúô.
Eliminating ùúôfor ùúôùúéor ùúôŒ£ yields in each
case a new equation of the original type but
with redeÔ¨Åned parameters:
aùúé= a ‚àí(ln h)y,
bùúé= b
hùúé= 2h ‚àík ‚àí(ln h)xy,
kùúé= h
and
aŒ£ = a,
bŒ£ = b ‚àí(ln k)x
hŒ£ = k,
kŒ£ = 2k ‚àíh ‚àí(ln k)xy.
One checks straightforwardly that, for the
invariants,
(kùúé)Œ£ = (kŒ£)ùúé,
(hùúé)Œ£ = (hŒ£)ùúé.
However the (noninvariant) coeÔ¨Écients are
not well behaved: (aùúé)Œ£ ‚â†a, and so on. This
is to be expected as scaling transforma-
tions cannot alter the factorization prop-
erties of the second-order operator even
though they alter coeÔ¨Écients.

266
7 Special Functions
Repetitions of Laplace maps generate
sequences of pairs of invariants. So we label
them
hn = hùúén,
kn = kùúén
subject to the understanding that
hùúé‚àí1 = hŒ£
and so on. Then we may write three-term
recurrence relations for the hn and kn ‚à∂
hn+1 ‚àí2hn + hn‚àí1 = (ln hn)xy
kn+1 ‚àí2kn + kn‚àí1 = (ln kn)xy
and these are a particular form of the two-
dimensional Toda Ô¨Åeld equations studied in
the theory of integrable systems [37, 38].
The family of invariants is associated
with a rank-one lattice, which may be
inÔ¨Ånite, semi-inÔ¨Ånite or Ô¨Ånite depending
on whether there are vanishing invariants
for some value(s) of n.
Suppose we consider the formal adjoint
of the given second-order diÔ¨Äerential oper-
ator, L:
L‚Ä† = ùúïxùúïy ‚àíaùúïx ‚àíbùúïy + c ‚àíax ‚àíby.
Its invariants are
h‚Ä† = k,
k‚Ä† = h.
and it is easy to check that
h‚Ä†ùúé= hŒ£‚Ä†,
k‚Ä†ùúé= kŒ£‚Ä†
so that the operations ùúé= Œ£‚àí1 and ‚Ä† gener-
ate the inÔ¨Ånite dihedral group:
D‚àû= < ‚Ä†, ùúé|ùúé‚Ä† ùúé= ‚Ä†, ‚Ä†2 = id > .
If we start with a second-order hyper-
bolic operator that is self-adjoint, then the
sequence, or one-dimensional lattice, of
invariants generated by Laplace maps is
symmetric in the sense that,
hn = h‚àín,
kn = k‚àín.
If at some point an invariant vanishes, then
the lattice is Ô¨Ånite with an odd number of
sites, the central site corresponding to the
self-adjoint operator ‚Äìa situation reminis-
cent of the Legendre equation with eigen-
value l(l + 1).
In such situations (and others where
a modiÔ¨Åed symmetry obtains), there is a
natural map between lattices of diÔ¨Äerent
lengths. If L is self-adjoint then h = k and it
can, by a scaling transformation, be written
in the form
L = ùúïxùúïy + h = ùúïxùúïy + k.
Assume ùúô0 is a function in the kernel of this
operator and consider the coupled system
(ùúïx + ùúô‚àí1
0 ùúô0x)ùúôùúá= ‚àí(ùúïx ‚àíùúô‚àí1
0 ùúô0x)ùúô
(ùúïy + ùúô‚àí1
0 ùúô0y)ùúôùúá= (ùúïy ‚àíùúô‚àí1
0 ùúô0y)ùúô
for a pair of functions ùúôand ùúôùúá. Elimination
of ùúôùúágives the original equation
(ùúïxùúïy + h)ùúô= 0,
but elimination of ùúôgives
(ùúïxùúïy + h + 2(ln ùúô0)xy)ùúôùúá= 0,
a new, self-adjoint equation with invariant
hùúá= kùúá= h + 2(ln ùúô0)xy,
ùúô0 being a solution of the original self-
adjoint equation. This so-called Moutard
[35] transformation is diÔ¨Äerent in charac-
ter to the Laplace transformation because

7.4 Factorization
267
it takes us outside the diÔ¨Äerential alge-
bra generated by the invariants alone. In
general, the lattice associated with hùúáwill
have a diÔ¨Äerent character to that associated
with h.
The Moutard transformation can be
reduced to a one-dimensional situation
by assuming a symmetry of the form
h(x, y) = h(x + y)
and
correspondingly
ùúô(x, y) = eùúÜ(x‚àíy)ùúì(x + y). Then ùúìsatisÔ¨Åes
ùúì‚Ä≤‚Ä≤ + hùúì= ùúÜ2ùúì.
Following the previous prescription, set
(ùúï‚àíùõº)ùúì= ùúìùõø,
(ùúï+ ùõº)ùúìùõø= ùúÜ2ùúì
with
‚àíùõº‚Ä≤ ‚àíùõº2 = h ‚àíùúá2 + ùúÜ2.
It follows that ùõº= ùúì‚Ä≤
0‚àïùúì0 where
ùúì‚Ä≤‚Ä≤
0 + hùúì0 = ùúá2ùúì0
and ùúìùõøsatisÔ¨Åes
ùúìùõø‚Ä≤‚Ä≤ + hùõøùúìùõø= ùúÜ2ùúìùõø
with
hùõø= h + 2(ln ùúì0)‚Ä≤‚Ä≤.
This is the Darboux transformation, a map
of enormous interest in the theory of soli-
ton equations [39].
If we regard soliton solutions as spe-
cial functions, parameterized by positions,
momenta, and soliton number, for nonlin-
ear equations then Darboux transforma-
tions are analogous to the raising operators
in the classical theory.
7.4.5
Dunkl Operators
Dunkl operators provide an approach to
multivariable generalizations of the clas-
sical special functions via Ô¨Ånite reÔ¨Çection
groups [40‚Äì42]. Given such a group, W,
with positive roots Œî+ ‚äÇ‚ÑùN, so
W =< ùúéùõº|ùõº‚ààŒî+ >,
the Dunkl operators Ti are diÔ¨Äerential-
diÔ¨Äerence operators acting on functions,
f ‚à∂‚ÑùN ‚Üí‚Ñù‚à∂
Tif (x) = ùúïxif (x) +
‚àë
ùõº‚ààŒî+
kùõº
f (x) ‚àíf (ùúéùõº(x))
< x, ùõº>
ùõºi,
where x ‚àà‚ÑùN, ùõºi denotes the ith compo-
nent of ùõº, kùõºis a multiplicity function (the
cardinality of the conjugacy class of ùúéùõºin
W), and < ‚ãÖ, ‚ãÖ> denotes the inner product
on ‚ÑùN.
For example, in the case of the root sys-
tem AN‚àí1, presented earlier (Section 7.2.4),
Ti = ùúïxi + k
‚àë
i‚â†j
1 ‚àíùúé(ij)
xi ‚àíxj
,
ùúé(ij) acting on f by interchange of the ith and
jth arguments.
A crucial property of the Dunkl operators
is that they commute:
[Ti, Tj] = 0.
The Dunkl Laplacian is deÔ¨Åned to be
Œîk =
N
‚àë
i=1
T2
i
and the generalized special functions in
mind are harmonic with respect to this
Laplacian.

268
7 Special Functions
Thus for the example of AN‚àí1,
Œîk = Œî +
2k
‚àë
i<j
1
xi ‚àíxj
(
ùúïxi ‚àíùúïxj ‚àí
1 ‚àíùúé(ij)
xi ‚àíxj
)
.
For A1 we have the expression,
ùúï2 + 2k
x ùúï,
and so Œîk generalizes to many variables the
classical (spherical) Bessel function.
Important players in the theory are the
intertwining operators, Vk ‚à∂
TiVk = Vkùúïxi
which, like the Ti themselves are W-
equivariant, and the Dunkl kernel, E(‚ãÖ, y),
the unique solution to the set of PDE,
Tif = yif ,
i = 1, ‚Ä¶ , N.
In terms of E, the generalized Bessel func-
tion is deÔ¨Åned, for x, y ‚àà‚ÑùN, as
Jk(x, y) =
1
|W|
‚àë
g‚ààW
Ek(gx, y).
7.5
Special Functions Without Symmetry
In this section, we present theories of a
more general character of which diÔ¨Äerential
Galois theory is arguably the most inclu-
sive. It would be invidious to claim that
there is no debt to symmetry but its r√¥le is
less obviously geometric.
7.5.1
Airy Functions
If a singular point of an ODE is not regular,
then it is called irregular and if a (complex)
solution of the form
y(x) = eQ(x)w(x)
exists near the singular point (assumed to
be x = 0), Q(x) being polynomial in 1‚àïx and
w(x) being regular, then it is called a nor-
mal solution [21, 43‚Äì45]. Such solutions
need not exist. Note that even so simple
an equation as a linear ODE with constant
coeÔ¨Écients falls into this class, the point
at inÔ¨Ånity being an irregular singular point
and Q(x) linear in x. The normal solution
has an essential singularity.
If the equation has a normal solution in
the variable x1‚àïk for some positive integer
k > 1 (Puiseux expansion), then the solu-
tion is called subnormal.
An example of such a situation is the Airy
equation,
y‚Ä≤‚Ä≤(x) ‚àíxy(x) = 0,
which has an irregular singular point at
inÔ¨Ånity. Linearly independent solutions are
denoted
Ai(x),
Bi(x).
They have Puiseux expansions expressed
via fractional Bessel functions:
Ai(‚àíx)=1‚àï3
‚àö
x(J1‚àï3(2‚àï3x3‚àï2)+J‚àí1‚àï3(2‚àï3x3‚àï2));
Bi(‚àíx)=1‚àï3
‚àö
x(J‚àí1‚àï3(2‚àï3x3‚àï2)‚àíJ1‚àï3(2‚àï3x3‚àï2)).
There are also integral representations:
(3a)‚àí1‚àï3ùúãAi(¬±(3a)‚àí1‚àï3x) =
‚à´
‚àû
0
dt cos(at3 ¬± xt);
and
(3a)‚àí1‚àï3ùúãBi(¬±(3a)‚àí1‚àï3x) =
= ‚à´
‚àû
0
dt
(
sin(at3 ¬± xt) + e‚àíat3¬±xt)
.

7.5 Special Functions Without Symmetry
269
7.5.1.1
Stokes Phenomenon
This is an important feature of functions
with essential singularities. The coeÔ¨Écients
in the Airy equation being entire on ‚ÑÇand
the equation being linear, it follows that
the general solution is likewise entire on ‚ÑÇ.
However, asymptotic representations of the
solutions,
y¬±(x) ‚âàx‚àí1‚àï4e¬±2‚àï3x3‚àï2,
y‚Ä≤‚Ä≤
¬± ‚àíxy ‚âàx‚àí9‚àï4e¬±2‚àï3x3‚àï2,
are multivalued,
y¬± ‚Üíiy‚àì,
as x traverses a closed loop about x = 0
(equivalently, x = ‚àû.)
Note that unlike the Fuchsian case where
the multivaluedness is a genuine property
of the solution near a regular singular point,
this behavior is an artifact of using multival-
ued approximations near an essential sin-
gularity. One avoids the confusion created
by this representation dependence by allo-
cating representations to speciÔ¨Åc sectors of
‚ÑÇseparated by Stokes lines.
In the case of the Airy functions, the
asymptotic forms are
Ai(x) ‚â°1
2ùúã‚àí1‚àï2x‚àí1‚àï4e2‚àï3x3‚àï2,
for | arg x| < ùúã, and
Ai(‚àíx) ‚â°ùúã‚àí1‚àï2x‚àí1‚àï4 sin
(
2
3x3‚àï2 + ùúã
4
)
for | arg x| < 2‚àï3ùúã.
7.5.2
Liouville Theory
Just as in number theory there are hierar-
chies of number system,
‚Ñï‚äÇ‚Ñ§‚äÇ‚Ñö‚äÇ¬∑ ¬∑ ¬∑ ‚äÇ‚Ñù,
so in function theory too it is helpful to
distinguish
elementary
functions
from
higher functions so that we can answer
the question as to whether a given diÔ¨Äer-
ential equation has solutions that cannot
be deÔ¨Åned by more elementary means
[46, 47].
In the case of number theory, the positive
integers, ‚Ñï, taken as a starting point, allow
us to develop the full set of integers ‚Ñ§and
the rationals ‚Ñöas extensions that provide
solutions to equations
z + n = 0,
n ‚àà‚Ñï,
and
az + b = 0,
a, b ‚àà‚Ñ§, a ‚â†0.
Algebraic numbers are deÔ¨Åned as real solu-
tions to polynomial equations over ‚Ñöin one
variable,
f (z) = 0,
f ‚àà‚Ñö[z].
Beyond
this
point,
completions
with
respect to some norm construct the real,
‚Ñù, and p-adic, Œ©p, numbers. We further
extend to complex versions of any of
these systems by allowing solutions to the
quadratic equation,
z2 + 1 = 0.
For functions, we may proceed in an
algebraically
similar
manner.
Certain
operations are taken as elementary: the
construction of an algebraic function, that
is, a function y(x) satisfying an equation
polynomial in x and y; the exponentiation
of a function, that is, solution of the Ô¨Årst-
order ODE, y‚Ä≤ + f ‚Ä≤y = 0; the taking of a
logarithm, that is, solution of an equation
ey = f (x).

270
7 Special Functions
In his theory of functions, Liouville deÔ¨Ånes
an elementary function as one constructed
by a Ô¨Ånite sequence of such operations and
the order of such a function as the minimal
number of exponentiations or taking of
logarithms in that process. Algebraic oper-
ations are neutral: they do not aÔ¨Äect the
order.
We now give some applications of Liou-
ville‚Äôs approach to special functions.
A theorem of Abel [48] states that if the
integral, along some path in ‚ÑÇ, of an alge-
braic function,
‚à´dx y(x),
is itself algebraic, then it is necessarily ratio-
nal in x and y. We can use this to show
that the Jacobi elliptic functions (Section
7.6.2) are not elementary. Assume that y is
deÔ¨Åned by
y2 = (1 ‚àíx2)(1 ‚àík2x2).
If the integral of y is rational, then according
to Abel‚Äôs result, it must be of the form,
‚à´
dx
y(x) = A(x) + B(x)y,
A and B being rational in x. DiÔ¨Äerentiation
with respect to x allows us to write yA‚Ä≤ as
a function rational in x ‚à∂a contradiction,
unless A = 0. Further analysis of the pos-
sible poles in the relation leads to a simi-
lar contradiction. Hence the Jacobi elliptic
functions are not elementary.
Consider secondly the Gaussian integral,
erf(x) = ‚à´
x
‚àí‚àû
dt e‚àít2.
On the basis of Liouville‚Äôs theory, it must be
of the form
w(x)e‚àíx2 + const,
for a rational w(x), if it is to be elementary
at all. But then w must satisfy the ODE,
1 = w‚Ä≤ ‚àí2x,
and it is straightforward to show that such
a w has no poles either in the Ô¨Ånite part of
‚ÑÇor at ‚àû. Hence, by the Liouville theorem
of complex analysis, it is constant and we
have a contradiction: the erf function is not
elementary.
Finally, we will show that Bessel func-
tions are not in general elementary in an
extended sense (Liouville). This requires a
result concerning Riccati equations:
y‚Ä≤ + y2 = P(x),
P(x) being algebraic. We generalize the
notion of elementary functions by allow-
ing integrals. Since the logarithm of an
algebraic function is also the integral of
an algebraic function, we loosen the pre-
vious deÔ¨Ånition by allowing more general
integrals of algebraic functions and call
the class of extended elementary functions
Liouville functions. The order is deÔ¨Åned as
before but replacing the word ‚Äúlogarithm‚Äù
by ‚Äúintegral.‚Äù
The result we require is that if the given
Riccati equation has a particular solution
that is Liouville, then it has a particular
solution that is algebraic.
In the case of Bessel‚Äôs equation (Section
7.3.6), the transformation y = u‚àï
‚àö
x, x = iz
yields
d2u
dz2 =
(
1 + l(l + 1)
z2
)
u,
for l = n ‚àí1‚àï2 and this is reduced to a
Riccati equation,
dv
dz + v2 = 1 + l(l + 1)
z2
,

7.5 Special Functions Without Symmetry
271
by the diÔ¨Äerential substitution v = u‚Ä≤‚àïu.
(In fact, any second-order, linear, homoge-
neous ODE may be reduced to a Riccati
equation by a similar substitution.)
To settle the question of whether the
Bessel equation has Liouvillian solutions,
we need to see whether it has algebraic
solutions or not for given values of l. Using
expansions of v, one Ô¨Årstly determines that
any algebraic solution is rational in z. Sec-
ondly, it is shown that a rational solution
exists if and only if l is integral. Conse-
quently, the Bessel equation admits a non-
trivial Liouvillian solution (and hence all
solutions are Liouville) if and only if 2n is
an odd integer.
7.5.3
DiÔ¨Äerential Galois Theory
Strictly speaking, the diÔ¨Äerential Galois
theory is concerned with questions of
symmetry but not in the geometrical sense
described earlier. It is also a development
of the Liouville theory [11, 49‚Äì52].
It was from the question of the solvability
of polynomial equations in a single variable
that group theory originally arose. Solvabil-
ity here means obtaining expressions for
solutions in terms of rational operations
augmented by the extraction of simple alge-
braic roots. One associates with an irre-
ducible polynomial with coeÔ¨Écients in a
Ô¨Åeld ùîâ, p(x) ‚ààùîâ[x], the group of automor-
phisms, G, of the Ô¨Åeld ùîè= ùîâ[x]‚àï(p), where
(p) is the ideal generated by p(x), which Ô¨Åx
ùîâelementwise. If ùîâis exactly the set of
points Ô¨Åxed by G, then ùîèis called a normal
extension of ùîâand G is the Galois group,
Gal(ùîè‚àïùîâ) [53].
A group is said to have a normal series if
it has a Ô¨Ånite sequence of subgroups, Gi,
G = G0 ‚äÉG1 ‚äÉG2 ‚äÉ¬∑ ¬∑ ¬∑ ‚äÉGn = {e}
such that each Gi is normal in Gi‚àí1. G is
solvable if it has a normal series where each
quotient Gi+1‚àïGi is abelian.
The fundamental result of Galois theory
is that the equation p(x) = 0 is solvable by
radicals over ùîâif and only if the Galois
group Gal(ùîè‚àïùîâ) is solvable. The fact that
the symmetric groups Sn for n ‚â•5 are not
solvable accounts for the fact that there is
no generic formula for extracting the roots
of polynomial equations of degree Ô¨Åve or
more.
One thinks of the Galois group as a sym-
metry group ‚Äúpermuting" the elements of a
set while Ô¨Åxing those of a subset. It was the
link with solvability that inspired Lie [13]
to develop a group theory for continuous
symmetries (already discussed) and Picard
and Vessiot to develop a diÔ¨Äerential theory
closer in spirit to the Galois theory.
In diÔ¨Äerential Galois theory, ùîâis taken to
be a diÔ¨Äerential Ô¨Åeld, that is, it is a Ô¨Åeld with
derivation
D ‚à∂ùîâ‚Üíùîâ,
D(a + b) = D(a) + D(b),
D(ab) = aD(b) + D(a)b.
Homomorphisms, in particular automor-
phisms, between diÔ¨Äerential Ô¨Åelds,
ùúô‚à∂ùîâ‚Üíùîé,
are required to satisfy the property,
ùúôDùîâ= Dùîéùúô.
The analogue of the polynomial equation
in Galois theory is a linear diÔ¨Äerential
equation and the analogue of ùîè, is the cor-
responding Picard‚ÄìVessiot extension. This
is the diÔ¨Äerential Ô¨Åeld generated by a full
set of linearly independent (over constants)
solutions of the diÔ¨Äerential equation, say,

272
7 Special Functions
{y1, y2, ‚Ä¶ , yn} for a diÔ¨Äerential equation of
order n. Such a set of solutions must have
nonvanishing Wronskian:
|||||||||
y1
y2
‚Ä¶
yn
D(y1)
D(y2)
‚Ä¶
D(yn)
‚ãÆ
‚ãÆ
Dn‚àí1(y1)
Dn‚àí1(y2)
‚Ä¶
Dn‚àí1(yn)
|||||||||
‚â†0.
We write the Picard‚ÄìVessiot extension as
ùîâ‚ü®y1, y2, ‚Ä¶ , yn‚ü©.
It consists of rational, diÔ¨Äerential functions
of the yi with coeÔ¨Écients in the ground
Ô¨Åeld, ùîâ, subject to the constraint of the
linear diÔ¨Äerential equation. It is important
that this Ô¨Åeld extension of ùîâcontain no
new constants and that the Ô¨Åeld of constants
(ker(D)) is algebraically closed. For such
extensions, the Galois group of diÔ¨Äerential
automorphisms Ô¨Åxing ùîâis deÔ¨Åned in anal-
ogy to the polynomial case. If the diÔ¨Äeren-
tial Galois group has a certain ‚Äúsolvability"
property, then the Picard‚ÄìVessiot exten-
sion can be decomposed into a sequence
of subÔ¨Åelds, ùîèi‚àí1 ‚äÉùîèi, each of which is a
simple extension of the subsequent one in
the sense that, √† la Liouville theory, it can
be constructed by adjoining solutions of
either D(y) + ay = 0 or D(y) + a = 0, where
a ‚ààùîèi.
As an example, consider the (Fuchsian)
Cauchy equation,
x2y‚Ä≤‚Ä≤ + axy‚Ä≤ + by = 0,
a and b belonging to the Ô¨Åeld of constants
C ‚äÇùîâ.
Let m1 and m2 be the roots of
m2 + (a ‚àí1)m + b = 0.
If we deÔ¨Åne functions y1 and y2 to be lin-
early independent solutions of
xy‚Ä≤
i = miyi,
1 = 1, 2,
then each satisÔ¨Åes the Cauchy equation.
In
the
case
that
m1 ‚â†m2,
the
Picard‚ÄìVessiot extension is the diÔ¨Äer-
ential Ô¨Åeld, ùîè= ùîâ(y1, y2), C being the Ô¨Åeld
of constants, the diÔ¨Äerential Galois group
will be C √ó C acting multiplicatively and
diagonally,
y1 ÓÇ∂‚Üíc1y1,
y2 ÓÇ∂‚Üíc2y2,
unless y1 and y2 are algebraically dependent
(satisfy a polynomial equation f (y1, y2) = 0
over ùîâ), in which case the Galois group is a
proper subgroup of C √ó C.
In
the
case
that
m1 = m2 = m,
ùîè= ùîâ(y, u) where y and u are solutions to
xy‚Ä≤ = my,
xu‚Ä≤ = 1,
and the diÔ¨Äerential Galois group is C √ó C‚Ä≤,
the second factor acting additively,
y ÓÇ∂‚Üícy,
u ÓÇ∂‚Üíu + d.
Obviously, these are all solvable cases. If
C = ‚ÑÇ, then the diÔ¨Äerential Galois group
will be isomorphic to the monodromy
group (Section 7.2.5).
7.6
Nonlinear Special Functions
7.6.1
Weierstra√ü Elliptic Functions
One encounters the Weierstra√ü ‚Ñò-function
as soon as one departs from the lin-
ear oscillator model, that is, as soon
as the amplitude of oscillations can no

7.6 Nonlinear Special Functions
273
longer be regarded as ‚Äúsmall." Histori-
cally, the ‚Ñò-function arose from exact
analytic treatments of mechanical systems
[21, 54‚Äì56].
The second-order linear equation
y‚Ä≤‚Ä≤ + y = 0
is the equation of simple harmonic motion,
which governs small oscillations. Integrat-
ing it once, we can think of sin ùúÉand cos ùúÉ
functions as solutions of the ODE y‚Ä≤2 = 1 ‚àí
y2 under appropriate boundary conditions.
The most natural form of solution is
ùúÉ= ‚à´
sin ùúÉ
0
dy
‚àö
1 ‚àíy2 ,
that is, we deÔ¨Åne by the integral the func-
tion inverse to sin. The integral is taken
around a path in the complex plane avoid-
ing a cut from ‚àí1 to 1 . A closed path
around the cut returns to its starting point
but ùúÉis augmented by the amount
2 ‚à´
1
0
dy
‚àö
1 ‚àíy2 = 2ùúã,
the periodicity of the circular function.
An obvious simple nonlinear generaliza-
tion of this situation is
y‚Ä≤‚Ä≤ = 6y2 ‚àí1
2g2.
(The prefactor of 6 reÔ¨Çects a choice of
canonical form and g2 is a constant.) Inte-
grating yields
y‚Ä≤2 = 4y3 ‚àíg2y ‚àíg3,
g3 another constant, which is an equation
deÔ¨Åning, in the complex plane, a doubly
periodic function with a double pole that
can be chosen to lie at the origin. This is
the Weierstra√ü ‚Ñò-function. As with circu-
lar functions, we can present an inverse def-
inition:
u = ‚à´
‚Ñò(u)
‚àû
dy
‚àö
4y3 ‚àíg2y ‚àíg3
,
or, equivalently,
‚Ñò‚Ä≤2 = 4‚Ñò3 ‚àíg2‚Ñò‚àíg3.
This time, there are cuts between a pair
of roots of the cubic in y and between the
third root and the point at inÔ¨Ånity. Integrat-
ing around closed loops in the plane avoid-
ing these cuts augments the variable u by
points in a lattice Œõ = 2ùúî1‚Ñ§+ 2ùúî2‚Ñ§, the
ùúîi being two fundamental complex half-
periods (linearly independent over ‚Ñù) of the
‚Ñò-function.
The ‚Ñò-function can also be constructed
as an inÔ¨Ånite sum,
‚Ñò(u) = 1
z2 +
‚Ä≤‚àë
m,n
(
1
(z ‚àíùúîm,n)2 ‚àí
1
ùúî2
m,n
)
,
ùúîm,n being deÔ¨Åned as
ùúîm,n = 2mùúî1 + 2nùúî2
and the primed sum denoting omission of
the term ùúî0,0.
The
parameters
in
the
diÔ¨Äerential
equation above, satisÔ¨Åed by the ‚Ñò-function,
are related to the lattice parameters via
inÔ¨Ånite sums:
g2 = 60
‚Ä≤‚àë
m,n
1
ùúî4
m,n
;
g3 = 140
‚Ä≤‚àë
m,n
1
ùúî6
m,n
;
As for circular functions, there is a (nonlin-
ear) addition law:

274
7 Special Functions
‚Ñò(u) + ‚Ñò(v) + ‚Ñò(u + v)
= 1
4
(‚Ñò‚Ä≤(u) ‚àí‚Ñò‚Ä≤(v)
‚Ñò(u) ‚àí‚Ñò(v)
)2
In more symmetric form,
|||||||
1
‚Ñò(u)
‚Ñò‚Ä≤(u)
1
‚Ñò(v)
‚Ñò‚Ä≤(v)
1
‚Ñò(u + v)
‚àí‚Ñò‚Ä≤(u + v)
|||||||
= 0
Geometrically, the addition law expresses
the fact that the genus one, plane cubic
curve
z2 = 4y3 ‚àíg2y ‚àíg3
is parameterized by the choice (y, z) =
(‚Ñò, ‚Ñò‚Ä≤) and the point corresponding to
parameter u + v is the reÔ¨Çection in the
x-axis of the third intersection point with
the cubic of a straight line through those
points corresponding to u and v.
Thus let (y1, z1) and (y2, z2) be a pair of
points on the cubic curve. The straight line
passing through these points is
z = ùõº+ ùõΩy,
where
ùõº= y1z2 ‚àíy2z1
y1 ‚àíy2
and
ùõΩ= z1 ‚àíz2
y1 ‚àíy2
.
The third point of intersection (y3, z3) arises
from solving
4y3 ‚àíg2y ‚àíg3 ‚àí(ùõº+ ùõΩy)2 =
4(y ‚àíy1)(y ‚àíy2)(y ‚àíy3) = 0,
so that
y1 + y2 + y3 = 1
4ùõΩ2.
The construction is that the ‚Äúsum‚Äù of (y1, z1
and (y2, z2) is (y3, z3) where z = ‚àíz is the
reÔ¨Çection of z in the y-axis. So
y3 = ‚àíy1 ‚àíy2 + 1
4
( z1 ‚àíz2
y1 ‚àíy2
)2
,
and the expression
z3 = ‚àíùõº‚àíùõΩy3
simpliÔ¨Åes to the determinantal form given
above under the identiÔ¨Åcations of y with ‚Ñò
and z with ‚Ñò‚Ä≤.
Any doubly periodic function on ‚ÑÇ
must have poles. Otherwise, by Liouville‚Äôs
theorem, it would be constant. All such
functions form a diÔ¨Äerential Ô¨Åeld generated
by ‚Ñò(u): any such function can be written
as
A(‚Ñò) + B(‚Ñò)‚Ñò‚Ä≤
where A and B are rational functions over
‚ÑÇ.
The ‚Ñò-function is the primary example
of an abelian function: it is meromorphic,
single-valued, and (doubly) periodic.
Two useful, but nonperiodic, functions
are the ùúÅand the Weierstra√ü ùúéfunctions:
‚Ñò(u) = ‚àíd
duùúÅ(u),
ùúÅ(u) ‚àí1
u ‚Üí0, u ‚Üí0;
ùúÅ(u) = d
du ln ùúé(u),
ùúé(u)
u
‚Üí1, u ‚Üí0.
Thus,
‚Ñò(u) = ‚àíd2
du2 ln ùúé(u).
The ùúÅ-function fails to be periodic:
ùúÅ(u + 2ùúîi) = ùúÅ(u) + 2ùúÇi,
i = 1, 2

7.6 Nonlinear Special Functions
275
where ùúÇi = ùúÅ(ùúîi) and
ùúÇ1ùúî2 ‚àíùúÇ2ùúî1 = 1
2iùúã.
The ùúéfunction is likewise not periodic,
ùúé(u + 2ùúîi) = ‚àíe2ùúÇi(u+ùúîi)ùúé(u), i = 1, 2,
but has the additional, beneÔ¨Åcial property
of being entire (everywhere holomorphic)
on ‚ÑÇ. Knowing it has a zero only at the
origin, one may use it to construct general
abelian functions on ‚ÑÇ. Thus consider
f (u) =
n
‚àè
i=1
ùúé(u ‚àíai)
ùúé(u ‚àíbi).
This function is holomorphic apart from
zeros at the ai‚Äôs and poles at the bi‚Äôs. It is
further periodic with half-periods ùúî1 and
ùúî2, provided
n
‚àë
i=1
ai +
n
‚àë
i=1
bi ‚ààŒõ,
which condition guarantees that the factors
acquired by the products of ùúé-function all
cancel out when their arguments are aug-
mented by points in Œõ.
There are many interesting relations
between
the
‚Ñò-functions
and
the
ùúé-
functions. Two notable examples are the
addition law,
‚Ñò(u) ‚àí‚Ñò(v) = ‚àíùúé(u + v)ùúé(u ‚àív)
ùúé2(u)ùúé2(v)
,
and its generalization, the Frobenius‚Äì
Stickelberger relations [57]: if ‚Ñò(j)
i
denotes
the value of dj
duj ‚Ñò(u)|u=ui, then
|||||||||
1
‚Ñò1
‚Ñò(1)
1
‚Ä¶
‚Ñò(n‚àí2)
1
1
‚Ñò2
‚Ñò(1)
2
‚Ä¶
‚Ñò(n‚àí2)
2
‚ãÆ
‚ãÆ
1
‚Ñòn
‚Ñò(1)
n
‚Ä¶
‚Ñò(n‚àí2)
n
|||||||||
=
(‚àí1)1‚àï2n(n‚àí1)
n
‚àè
i=1
(i!)
√ó
ùúé(‚àën
i=1 ui) ‚àè
i<j ùúé(ui ‚àíuj)
‚àèn
i=1 ùúén(ui)
.
Now imagine we are considering the whole
family of nonsingular elliptic curves,
V = {(x, y, g2, g3)|
y2 = 4x2 ‚àíg2x ‚àíg3, g3
2 ‚àí27g2
3 ‚â†0}.
The Ô¨Åeld, F, of abelian functions is gener-
ated by the parameters g2 and g3 and the
‚Ñòand ‚Ñò‚Ä≤ functions (themselves functions
of the parameters). Weierstra√ü showed that
the ùúéfunction satisÔ¨Åes diÔ¨Äerential identi-
ties in the parameters and in u:
Q0ùúé= 0,
Q2ùúé= 0,
where
Q0 = 4g2ùúïg2 + 6g3ùúïg3 ‚àíuùúïu + 1,
Q2 = 6g3ùúïg2 + 1‚àï3g2
2ùúï3 ‚àí1
2ùúï2
u ‚àí1
21g2u2.
The derivations of F, Der(F), are generated
by the three operators
L0 = ‚àíuùúïu + 4g2ùúïg2 + 6g3ùúïg3,
L1 = ùúïu,
L2 = ‚àíùúÅùúïu + 6g3ùúïg2 + 1‚àï3g2
2ùúïg3.
These operators further satisfy the commu-
tation relations of a Lie algebra over F,
[L0, L1] = L1, [L0, L2] = 2L2,
[L1, L2] = ‚ÑòL1,
exhibiting in this way a fundamental sym-
metry structure underlying these functions
[58].

276
7 Special Functions
The ‚Ñò-function appears in a number of
physical applications. One is as the solution
to the nonlinear pendulum equations:
ÃàùúÉ+ sin ùúÉ= 0
and similar contexts. Another is as a sym-
metry reduction of some classes of PDE,
notably the Korteweg‚Äìde Vries (KdV)
equation:
ut + uxxx + 6uux = 0.
The symmetry is translational. One seeks
solutions depending on the invariant ùúÇ=
x ‚àíct and Ô¨Ånds exactly, after two integra-
tions, solutions
u(x, t) = f (ùúÇ)
where
f ‚Ä≤2 = ‚àí2f 3 + cf 2 + C1f + C2,
C1 and C2 being arbitrary constants of inte-
gration. These can be written easily in terms
of ‚Ñò-functions and represent quasiperiodic
nonlinear wave solutions on a Ô¨Ånite space
interval (typically waves in a wave tank.) In
the limit that both C1 = C2 = 0, to accom-
modate the boundary conditions in an inÔ¨Å-
nite tank
f , f ‚Ä≤ ‚Üí0,
|ùúÇ| ‚Üí0,
we obtain the single soliton solution
u(x, t) = 1
2sech2
(‚àö
c
2 (x ‚àíct ‚àíùúÇ0)
)
for constant ùúÇ0.
Analogues of the Weierstra√ü ‚Ñò-function
can be deÔ¨Åned for plane curves of genus
g greater than one (the Weierstra√ü case).
They satisfy addition formulae and are peri-
odic with 2g complex periods in g variables.
For example, the quintic curve
y2 = 4x5 + a4x4 + a3x3 + a2x2 + a1x + a0
is a genus two plane curve and there are
three ‚Ñò-functions of two variables u1 and
u2, written
‚Ñò11(u1, u2), ‚Ñò12(u1, u2), ‚Ñò22(u1, u2).
Integrability conditions on the ‚Ñò-functions
imply the existence of an entire function
ùúé(u1, u2) such that
‚Ñòij(u1, u2) = ‚àí2ùúïuiùúïuj log ùúé(u1, u2).
Further indices represent higher deriva-
tives: ‚Ñò111 = ùúï‚àïùúïu1‚Ñò11, and so on.
The
analogue
of
the
second-order
equation
satisÔ¨Åed
by
the
Weierstrass
function is a set of Ô¨Åve equations for the
‚Ñòij:
‚Ñò1111 ‚àí6‚Ñò2
11 = ‚àí1
2a0a4 + 1
8a1a3
‚àí3a0‚Ñò22 + a1‚Ñò12 + a2‚Ñò11
‚Ñò1112 ‚àí6‚Ñò11‚Ñò12 = ‚àía0
‚àí1
2a1‚Ñò22 + a2‚Ñò12
‚Ñò1122 ‚àí4‚Ñò2
12 ‚àí2‚Ñò11‚Ñò22 = 1
2a3‚Ñò12
‚Ñò1222 ‚àí6‚Ñò12‚Ñò22 = a4‚Ñò12 ‚àí2‚Ñò11
‚Ñò2222 ‚àí6‚Ñò2
22 = 1
2a3 + a4‚Ñò22 + 4‚Ñò12
Such equations have been studied for a long
time [59] and are still an active area of
research [60].
The Weierstra√ü ‚Ñò-function arises in
an important class of linear, Fuchsian
equations with coeÔ¨Écients doubly periodic
on ‚ÑÇ‚à∂

7.6 Nonlinear Special Functions
277
7.6.1.1
Lam√© Equations
The
second-order
family
of
Lam√©
equations,
w‚Ä≤‚Ä≤ ‚àí(h + n(n + 1)‚Ñò(z))w = 0,
for integer n, has regular singular points at
the lattice points of Œõ and exponents n + 1
and ‚àín. Although the exponents diÔ¨Äer by
an integer (2n + 1), no logarithmic terms in
fact occur and, depending on the value of h,
solutions algebraic in the ‚Ñò-function may
exist [9, 22].
For the case n = 1,
w‚Ä≤‚Ä≤ ‚àí(h + 2‚Ñò(z))w = 0,
one can show that two linearly independent
solutions
w1 = e‚àízùúÅ(a) ùúé(z + a)
ùúé(z)
and
w2 = ezùúÅ(a) ùúé(z ‚àía)
ùúé(z)
exist provided h = ‚Ñò(a) is not a root of the
Weierstra√ü cubic, that is, provided ‚Ñò‚Ä≤(a) ‚â†
0. If h = e1 is a root of the cubic, then the
above solutions are equal and a second lin-
early independent solution must be chosen:
w2 = e‚àízùúÇ1 ùúé(z + ùúî1)
ùúé(z)
(ùúÅ(z + ùúî1) + e1z) .
7.6.2
Jacobian Elliptic Functions
A genus one plane curve may also be writ-
ten in quartic form
y2 = a4x4 + 4a3x3 + 6a2x2 + 4a1x + a0
for which all branch points lie in the Ô¨Ånite
part of the complex plane and an analo-
gous treatment leads to parameterization
by alternative doubly periodic functions
due to Jacobi [20, 21].
While ‚Ñò(u) has a double pole in each
period parallelogram, the Jacobi functions
have a pair of distinct single poles. Owing
to the fact that the degree of a divisor of a
meromorphic function on a compact Rie-
mann surface vanishes, there must be, up
to multiplicity, two corresponding zeros for
each function.
DeÔ¨Åning the Jacobi sn function by
sn(u) = t,
u = ‚à´
t
0
dx
‚àö
(1 ‚àíx2)(1 ‚àík2x2)
,
so that
(x‚Ä≤)2 = (1 ‚àíx2)(1 ‚àík2x2),
we deÔ¨Åne cn and dn by
cn2(u) + sn2(u) = 1,
dn2(u) + k2sn2(u) = 1,
choosing the signs in such a way that
cn(u) ‚Üí0,
dn(u) ‚Üí0
as u ‚Üí0.
From these deÔ¨Ånitions one shows that
sn‚Ä≤(u) = cn(u)dn(u);
cn‚Ä≤(u) = ‚àísn(u)dn(u);
dn‚Ä≤(u) = ‚àík2sn(u)cn(u).
In the limit that k ‚Üí0, sn and cn become
circular functions.
Further, if K is the value of the complete
elliptic integral

278
7 Special Functions
K = ‚à´
ùúã‚àï2
0
dùúô
‚àö
1 ‚àík2 sin2 ùúô
then
sn(K) = 1,
cn(K) = 0,
dn(K) =
‚àö
1 ‚àík2.
K determines the periods of the Jacobi ellip-
tic functions:
sn(u + 4K) = sn(u);
cn(u + 4K) = cn(u);
dn(u + 2K) = dn(u).
As with the ‚Ñò-functions, and for the same
underlying algebro-geometric reasons, the
Jacobi functions obey addition laws:
sn(u + v) =
sn(u)cn(v)dn(v) + sn(v)cn(u)dn(u)
1 ‚àík2sn2(u)sn2(v)
;
cn(u + v) =
cn(u)cn(v) ‚àísn(uv)sn(v)dn(u)dn(v)
1 ‚àík2sn2(u)sn2(v)
;
dn(u + v) =
dn(u)dn(v) ‚àík2sn(u)sn(v)cn(u)cn(v)
1 ‚àík2sn2(u)sn2(v)
.
7.6.3
Theta Functions
In many ways, the most natural approach
to doubly (and multi-) periodic functions
in general is via the ùúó-function [12, 22,
60‚Äì62]. We shall present the simplest,
genus one case and indicate roughly how
higher genus ùúó-functions are deÔ¨Åned.
Let ùúèbe a complex number with a strictly
positive imaginary part and let
q = eiùúãùúè.
The (fourth) ùúó-function with parameter ùúè
and variable u is deÔ¨Åned by an inÔ¨Ånite sum:
ùúó4(u|ùúè) =
‚àû
‚àë
n=‚àí‚àû
(‚àí1)nqn2e2niu.
Similar to the ùúé-function, which is a mod-
iÔ¨Åed ùúó-function, it is entire and satisÔ¨Åes
a modularity condition (periodicity up to
multiplicative factors):
ùúó4(u + ùúã|ùúè) = ùúó4(u|ùúè);
ùúó4(u + ùúèùúã|ùúè) = ‚àí1
qe‚àí2iuùúó4(u|ùúè).
The third and Ô¨Årst ùúó-functions are deÔ¨Åned
as
ùúó3(u|ùúè) = ùúó4(u + 1
2ùúã|ùúè)
ùúó1(u|ùúè) = ‚àíieiu+1‚àï4ùúãiùúèùúó4(u + 1
2ùúèùúã|ùúè).
and Ô¨Ånally the second,
ùúó2(u|ùúè) = ùúó1(u + 1
2ùúã|ùúè).
One should think of these apparently
redundant variants of ùúóas analogues of
circular function relations,
cos(u) = sin(u + 1
2ùúã),
which serve to simplify writing compact,
polynomial identities.
Consider a fundamental parallelogram in
the complex plane with corners at 0, ùúã,
ùúèùúãand ùúã+ ùúèùúã, closed on two nonparallel
sides, open on the other two. Any transla-
tion in ‚ÑÇof this parallelogram will contain
a single, simple zero of any of the ùúói(u|ùúè)
functions and they may be used to con-
struct abelian functions in the same way as
was ùúé(u).
The ùúó‚Äôs satisfy many polynomial (alge-
braic) addition laws, one example of which,

7.6 Nonlinear Special Functions
279
the ùúèdependence being understood, is
ùúó3(u + v)ùúó3(u ‚àív)ùúó3(0)2
= ùúó3(u)2ùúó3(v)2 + ùúó1(u)2ùúó1(v)2.
They also satisfy a simple, linear PDE of the
heat equation type, in which the parameter
ùúèplays the r√¥le of ‚Äúimaginary time":
ùúïùúói
ùúïùúè+ iùúã
4
ùúï2ùúói
ùúïu2 = 0.
The higher genus, g ‚â•1, generalization of
the ùúófunction to g independent, complex
variables, u ‚àà‚ÑÇg, involves a g √ó g symmet-
ric, complex-valued matrix ùúèwhose entries
a have positive deÔ¨Ånite imaginary part.
Then the (Riemann) ùúó-function is,
ùúó(u|ùúè) =
‚àë
N‚àà‚Ñ§g
exp 2ùúãi
(1
2NTùúèN + NTu
)
,
where N and u are column vectors and (‚ãÖ)T
denotes the row vector transpose.
Such ùúófunctions are associated with
algebraic curves (on complex dimensional
manifolds) by taking a canonical homology
basis of closed loops (Chapter 6)
{a1, ‚Ä¶ , ag, b1, ‚Ä¶ , bg}
and a basis of holomorphic one-forms
{ùúî1, ‚Ä¶ , ùúîg},
satisfying
‚à´ai
ùúîj = ùõøij,
and
ùúèij = ‚à´bi
ùúîj.
The modular properties of ùúÉare summa-
rized in the statement that, for ùúáand ùúá‚Ä≤ col-
umn vectors in ‚Ñ§g,
ùúó(u + ùúá‚Ä≤ + ùúèùúá, ùúè)
= exp 2ùúãi
(
‚àíùúá‚Ä≤Tu ‚àí1
2ùúá‚Ä≤Tùúèùúá
)
ùúó(u, ùúè).
The analogues of the ùúói are the so-called ùúó-
functions with characteristics
ùúó
[ ùúñ
ùúñ‚Ä≤
]
(u, ùúè)
=
‚àë
N‚àà‚Ñ§g
exp 2ùúãi
(1
2NT
ùúñùúèNùúñ+ NT
ùúñuùúñ‚Ä≤
)
.
Here the notation (‚ãÖ)ùúñstands for augmen-
tation by the additive constant ùúñ‚àï2, for
example, Nùúñ= N + 1
2ùúñ.
When the entries in ùúñand ùúñ‚Ä≤ are inte-
gers, the function is called the Ô¨Årst-order
ùúó-function with integer characteristics.
The variables u are properly said to
live on a g complex dimensional algebraic
(torus) variety, the Jacobi variety, Jac(X), of
the algebraic curve. This is ‚ÑÇg factored by
the lattice Œõ = {ùúá‚Ä≤ + ùúèùúá|ùúá, ùúá‚Ä≤ ‚àà‚Ñ§g}. There
is a natural map, the Abel map, from any
n-fold symmetric product of the algebraic
curve with itself to Jac(X) ‚à∂
ùúôn ‚à∂Xn = X1 √ó ¬∑ ¬∑ ¬∑ √ó Xn ‚Üí‚ÑÇg‚àïŒõ,
given, for some choice of P0 ‚ààX, by
ùúôn(P1, ‚Ä¶ , Pn) =
n
‚àë
i=1 ‚à´
Pi
P0
ùúî,
mod Œõ.
ùúîis the column vector of holomorphic dif-
ferentials, du.
In the case that n = g, the Abel map is a
local homomorphism and the Jacobi inver-
sion problem, still only implicitly solved in
general, is to recover the divisor P1 + P2 +
¬∑ ¬∑ ¬∑ + Pg ‚ààXg from a given point in Jac(X).

280
7 Special Functions
Of particular importance is Riemann‚Äôs
theorem describing the locus of zeros (the
ùúó-divisor) of the ùúó-function:
ùúó(e) = 0 ‚áîe ‚ààc + ùúôg‚àí1(Xg‚àí1)
where c is a very speciÔ¨Åc vector ‚Äì the Rie-
mann vector of constants [12].
7.6.4
Painlev√© Transcendents
Modifying the Weierstra√ü equation a lit-
tle further, one may consider the simple
nonautonomous case:
y‚Ä≤‚Ä≤ = 6y2 + x,
x being the independent variable. This is
the simplest of the Painlev√© equations.
Similar to the Weierstra√ü equation, such
equations arise as symmetry reductions
of integrable PDEs such as the KdV, mod-
iÔ¨Åed KdV, Boussinesq, and sine-Gordon
equations, under scaling [63‚Äì67].
For example, the KdV equation,
ut + uxxx + 6uux = 0
has a symmetry
x ÓÇ∂‚ÜíùúÜx
t ÓÇ∂‚ÜíùúÜ3t
u ÓÇ∂‚ÜíùúÜ‚àí2u
so that in terms of symmetry invariants we
seek solutions, w satisfying
u(x, t) = (3t)‚àí2‚àï3w
(
x
(3t)1‚àï3
)
which are invariant under the symmetry
(scaling solutions). Put ùúÇ=
x
(3t)1‚àï3 . Then the
KdV reduces to
w‚Ä≤‚Ä≤‚Ä≤ ‚àí2w ‚àíùúÇw‚Ä≤ + 6ww‚Ä≤ = 0,
prime denoting d‚àïdùúÇ. Using a Miura trans-
formation
w = v‚Ä≤ ‚àív2.
This equation can be solved subject to
knowledge of solutions to
v‚Ä≤‚Ä≤ = 2v3 + ùúÇv + ùõº,
for constant values of ùõº. This last equation
is a Painlev√© equation of type II.
The solutions share properties of the ‚Ñò-
function in being analytic except at mov-
able poles in the Ô¨Ånite part of the complex
plane, but they are not periodic.
Equations characterized by the property
that their only movable singularites (that
is, singularities whose locations depend on
initial values or constants of integration)
are poles are said to have the Painlev√© prop-
erty. A necessary but insuÔ¨Écient criterion
for a nonlinear ODE to possess the Painlev√©
property is that the exponents ùúàof a series
expansion about an arbitrary point x0,
(x ‚àíx0)ùúà
‚àû
‚àë
0
an(x ‚àíx0)n,
analogous to the Frobenius expansions
described earlier, be integers.
First-order ODEs of this type, polynomial
in the dependent variable and its derivative,
are either linear, Riccati
y‚Ä≤ = a(x)y2 + b(x)y + c(x)
or elliptic [68]. Second-order equations
were roughly classiÔ¨Åed by Painlev√© [66]
(more thoroughly by Gambier and cowork-
ers [69]) and hence go by his name. A list
of the six second-order Painlev√© equations
is given below:

7.6 Nonlinear Special Functions
281
PI
y‚Ä≤‚Ä≤ = 6y2 + x
PII
y‚Ä≤‚Ä≤ = 2y3 + xy + a
PIII
y‚Ä≤‚Ä≤ = y‚Ä≤2
y ‚àíy‚Ä≤
x
+ay2 + b
x
+ cy3 + d
y
PIV
y‚Ä≤‚Ä≤ = y‚Ä≤2
2y + 3‚àï2y3 + 4xy2
+2 (x2 ‚àía) y + b
y
PV
y‚Ä≤‚Ä≤ =
(
1
2y +
1
y ‚àí1
)
y‚Ä≤2 ‚àíy‚Ä≤
x
+(y ‚àí1)2
x2
(
ay + b
y
)
+ c y
x
+d y(y + 1)
y ‚àí1
PVI
y‚Ä≤‚Ä≤ = 1
2
(
1
y +
1
y ‚àí1 +
1
y ‚àíx
)
y‚Ä≤2
‚àí
(
1
x +
1
x ‚àí1 +
1
y ‚àíx
)
y‚Ä≤
+y(y ‚àí1)(y ‚àíx)
x2(x ‚àí1)2
√ó
(
a + b x
y2 + c x ‚àí1
(y ‚àí1)2 + d x(x ‚àí1)
(y ‚àíx)2
)
The parameters a, b, c, and d are arbitrary,
but speciÔ¨Åc relations between them allow
specialization. PVI contains all the other
cases as reductions. Generically, the solu-
tions are transcendents not to be found
among the (linear) elementary and special
functions, except for certain special values
of the parameters.
Painlev√© equations are associated with
linear ODEs with regular singular points in
the following way.
Consider a family of linear, second-order
ODEs with regular singular points in ‚ÑÇ
depending on a complex parameter, ùúÜ. We
can write this as a Ô¨Årst-order equation in
system form,
Œ®x(ùúÜ, x) = B(ùúÜ, x, y)Œ®(ùúÜ, x),
where Œ® is a two-component column vec-
tor of functions and B a 2 √ó 2 matrix with
at worst poles of order one in x and ùúÜ,
polynomial in y. The monodromy group of
this system will be generated, as before, by
linear maps
Œ® ÓÇ∂‚ÜíMiŒ®,
associated with each pole in B, i = 1, 2, ‚Ä¶
We will require that the monodromy
properties, for example, the group expo-
nents, be invariant under variation in the
parameter ùúÜ. This is the isomonodromy
condition and it means that the above
system is compatible with a system of the
form
Œ®ùúÜ(ùúÜ, x) = A(ùúÜ, x, y)Œ®(ùúÜ, x),
A having, in general, Ô¨Årst-order poles in ùúÜ.
The statement that these two systems are
compatible is that they share a common
general solution, so that
ùúïx
(ùúïùúÜŒ®) = ùúïùúÜ
(ùúïxŒ®) ,
which in turn implies

282
7 Special Functions
ùúïxA ‚àíùúïùúÜB + [A, B] = 0.
This system constitutes a (generally nonlin-
ear) ODE in y with independent variable x.
All Painlev√© equations are realized as
isomonodromy conditions in this manner
[70, 71]. For example, in the case of PI, we
may (but this is not unique) choose,
A = (4ùúÜ2 + 2y2 + x)
( 1
0
0
‚àí1
)
+(4ùúÜ2y + 2y2 + x)
( 0
‚àí1
1
0
)
‚àí(2ùúÜyx + 1
2ùúÜ)
( 0
1
1
0
)
,
B = (ùúÜ+ y
ùúÜ)
( 1
0
0
‚àí1
)
+ y
ùúÜ
( 0
‚àí1
1
0
)
.
Painlev√© equations also come equipped
with
B√§cklund
transformations.
Most
generally, a B√§cklund map is a transforma-
tion mapping solutions of one diÔ¨Äerential
equation to those of another or the same
equation
(autoB√§cklund
map)
perhaps
with diÔ¨Äerent parameter values. We have
seen examples for linear equations already:
the Laplace, Moutard, and Darboux maps.
Since PI has no parameters, we consider
the case of PII. An obvious B√§cklund map
is:
y(x, a) ÓÇ∂‚ÜíÃÉy = ‚àíy(x, ‚àía).
A less obvious one, for a ‚â†¬±1‚àï2, is, writing
y(x, a) = ya:
ya ÓÇ∂‚Üíya¬±1 =
‚àíya ‚àí
2a ¬± 1
2y2
a ¬± 2ya,x + x.
The sequence of y(x, a) so generated satis-
Ô¨Åes the three-term recurrence relation:
a + 1
2
ya + ya+1
+
a ‚àí1
2
ya + ya‚àí1
+ 2y2
a + x = 0.
In fact, the B√§cklund maps detailed above
comprise a representation of an extended
aÔ¨Éne Weyl group, the inÔ¨Ånite dihedral
group generated by a reÔ¨Çection and a
translation.
One further aspect of the richness of the
Painlev√© theory is the existence of families
of simple, nontranscendental, solution for
certain parameter values. Let us consider
PII once more. For a ‚àà‚Ñ§there exist solu-
tions
yn = d
dx ln
(Qn‚àí1(x)
Qn(x)
)
the
Qn
satisfying
a
second-order
diÔ¨Äerential-diÔ¨Äerence recurrence relation,
Q‚Ä≤2
n ‚àí4QnQ‚Ä≤‚Ä≤
n
Qn+1Qn‚àí1 ‚àízQ2
n
= ‚àí1
4.
with
initial
terms
Q0 = 1,
Q1 = x,
Q2 = x3 + 4 and so on. The corresponding
rational solutions to PII are
y1 = ‚àí1
x,
y2 = 1
x ‚àí
3x2
x3 + 4,
etc.
Another class of rational solutions to PII
are obtained from the generating function
eùúÜx‚àí(4‚àï3)ùúÜ3 =
‚àû
‚àë
m=0
pm(x)ùúÜm.
Put
ùúèn = det
(djp2i‚àí1(x)
dxj
)
i,j=1,‚Ä¶,n
,
and the solutions are

7.7 Discrete Special Functions
283
y(x, n) = d
dx ln
(ùúèn‚àí1
ùúèn
)
.
There are also solutions expressible using
classical special functions. Thus for a = n +
(1‚àï2), n ‚àà‚Ñ§, a family of solutions to PII
is obtained using the B√§cklund transforma-
tion, ya ÓÇ∂‚Üíya¬±1 above, starting from one of
y(x, ¬±1
2) = ‚àìùúô‚Ä≤(x)
ùúô(x) ,
ùúôbeing an Airy function combination
ùúô(x) = C1Ai(‚àí2‚àí1‚àï3x) + C2Bi(‚àí2‚àí1‚àï3x).
The situations described above for PI
and PII are replicated for other Painlev√©
equations.
7.7
Discrete Special Functions
7.7.1
ùùè‚ÄìùúπTheory
An attempt at a universal theory of special
functions was made in the middle of the last
century by imposing a relationship between
diÔ¨Äerential and diÔ¨Äerence operations [72].
In this approach, the majority of special
function recurrence relations are shown to
be reducible to a relation of the form,
ùúïzF(z, ùõº) = F(z, ùõº+ 1).
This universal equation has many solutions
for F. For instance,
‚Ä¢ F(z, ùõº) = ez;
‚Ä¢ F(z, ùõº) = sin(z ‚àí1
2ùõºz);
‚Ä¢ F(z, ùõº) = e‚àíz2Hùõº(‚àíz);
‚Ä¢ F(z, ùõº) = eiùõºùúãz‚àíùõº‚àï2Jùõº(2
‚àö
z).
The equation can be shown to admit
unique solutions, to be expandable in the
parameters z and in ùõº, to give rise to other
solely diÔ¨Äerential identities, and to provide
generating functions of almost all the clas-
sical special functions.
It also naturally leads in to the ideas of
diÔ¨Äerence calculus.
7.7.2
Quantum Groups
Properly speaking these are not groups,
the name quantum group is nevertheless
established, particularly in the mathemati-
cal physics literature, for certain Hopf bial-
gebras that play a r√¥le in the theory of a
whole new class of discrete or q-diÔ¨Äerence
special functions analogous to that played
by Lie groups in the classical theory [73, 74].
Most importantly, it is their representations
that are studied in applications.
For q a nonzero complex number with
q2 ‚â†1, the quantum group Uq(ùî∞l2) is an
algebra generated over ‚ÑÇ[q] by elements
E, F, K, and K‚àí1 deÔ¨Åned by the relations
KK‚àí1 = K‚àí1K = 1,
KEK‚àí1 = q2E,
KFK‚àí1 = q‚àí2F,
and
EF ‚àíFE = K ‚àíK‚àí1
q ‚àíq‚àí1 .
It carries a comultiplication, a ‚ÑÇ-linear map
from the algebra to its two-fold tensor self-
product,
Œî(E) = E ‚äóK + 1 ‚äóE,
Œî(F) = F ‚äó1 + K‚àí1 ‚äóF
Œî(K) = K ‚äóK;
a counit, ùúñ,

284
7 Special Functions
ùúñ(E) = ùúñ(F) = 0,
ùúñ(K) = 1;
and an antipode, S,
S(K) = K‚àí1,
S(E) = ‚àíEK‚àí1,
S(F) = ‚àíKF.
The comultiplication is consistent with the
deÔ¨Åning relations in the sense that, for
instance,
Œî(E)Œî(F) ‚àíŒî(F)Œî(E) = Œî(K) ‚àíŒî(K‚àí1)
q ‚àíq‚àí1
.
In the limit that q ‚Üí1, one obtains (almost)
the universal enveloping algebra U(ùî∞l2), so
in that sense the quantum group is a defor-
mation of the Lie algebra, which is morally
its classical limit.
In the representation theory of quantum
groups, the Uq(ùî∞l2) generators are realized
as diÔ¨Äerence operators on functions.
7.7.3
DiÔ¨Äerence Operators
The classical special functions have dis-
crete analogues in which diÔ¨Äerential oper-
ators are replaced by diÔ¨Äerence operators of
which the most studied are the q-diÔ¨Äerence
operator [30, 75, 76]
Dq(f (x)) = f (x) ‚àíf (qx)
x ‚àíqx
,
the h-derivative
dh(f (x)) = f (x + h) ‚àíf (x)
h
,
and the Askey‚ÄìWilson diÔ¨Äerence operator
Óà∞q(f (x)) =
ÃÜf (q1‚àï2eiùúÉ) ‚àíÃÜf (q‚àí1‚àï2eiùúÉ)
1
2(q1‚àï2 ‚àíq‚àí1‚àï2)(z ‚àí1‚àïz)
,
where ÃÜf (z) is deÔ¨Åned by,
ÃÜf (z) = f
(
1
2
(
z + 1
z
))
,
x and z are related as
z2 ‚àí2xz + 1 = 0,
and we take the branch
‚àö
x + 1 > 0, x >‚â•
‚àí1.
The corresponding processes, by analogy
with the classical derivative, are sometimes
called quantum calculus.
Although each diÔ¨Äerence operator tends
to the classical limit (on appropriate func-
tion classes) as q ‚Üí1 and h ‚Üí0, such oper-
ators are not quite derivations. Instead, for
example,
Dq(fg)(x) =
f (x)Dq(g(x)) + g(qx)Dq(f (x)).
That such deÔ¨Ånitions are natural is evi-
denced by simple observations of the fol-
lowing sort. The notation for a q-integer
[n]q = 1 ‚àíqn
1 ‚àíq
allows us to write the q-derivative of xn as
Dq(xn) = [n]qxn‚àí1
and suggests the q-factorial deÔ¨Ånition
[n]q! = [n]q[n ‚àí1]q ¬∑ ¬∑ ¬∑ [1]q
with [0]q! = 1 as usual. We can then deÔ¨Åne
q-binomial coeÔ¨Écients,

7.7 Discrete Special Functions
285
[ n
j
]
q
=
[n]q!
[n ‚àíj]q![j]q!
The q-Pascal rules for the q-binomial coef-
Ô¨Åcients are
[ n
j
]
q
=
[ n ‚àí1
j ‚àí1
]
q
+ qj
[ n ‚àí1
j
]
q
and
[ n
j
]
q
=
[ n ‚àí1
j
]
q
+ qn‚àíj
[ n ‚àí1
j ‚àí1
]
q
Assume x and y are noncommutative
objects satisfying yx = qxy ( x and y are
coordinates on the quantum plane). There
is a q-binomial theorem,
(x + y)n =
n
‚àë
j=0
[ n
j
]
q
xjyn‚àíj,
taking due regard of the ordering of the x‚Äôs
and y‚Äôs.
Such ideas go back to Gauss and Euler.
There are two q-analogues of the classical
exponential function:
ex
q =
‚àû
‚àë
j=0
xj
[j]q!
and
Ex
q =
‚àû
‚àë
j=0
qj(j‚àí1)‚àï2 xj
[j]q!.
Elementary calculations conÔ¨Årm q-diÔ¨Ä-
erence identities:
Dqex
q = ex
q
DqEx
q = Eqx
q ;
the reciprocal relation
ex
qE‚àíx
q
= 1
and, subject again to the commutation rela-
tion yx = qxy, the addition law
ex
qey
q = ex+y
q .
(7.1)
q-Analogues of classical special functions
can be constructed. As a simple example we
consider the q-Hermite polynomials.
7.7.4
q-Hermite Polynomials
The nth (continuous) q-Hermite polyno-
mial, denoted H(x|q), satisÔ¨Åes a three-term
recurrence relation:
2xHn(x|q) = Hn+1(x|q)
+(1 ‚àíqn)Hn‚àí1(x|q),
(7.2)
with initial terms
H0(x|q) = 1,
H1(x|q) = 2x.
Let us deÔ¨Åne the symbols,
(a; q)0 = 1,
(a; q)n =
n
‚àè
k=1
(1 ‚àíaqk‚àí1).
Here n may take any value in ‚Ñïor ‚àû, to sig-
nify an inÔ¨Ånite product. Note that if a = q,
we recover the symbol [n]q! deÔ¨Åned earlier.
We also deÔ¨Åne
(a, b; q)n = (a, q)n(b, q)n.
Using this symbol, the q-Hermite polyno-
mial generating functions are
1
(teiùúÉ, te‚àíiùúÉ|q) =
‚àû
‚àë
0
Hn(cos ùúÉ|q)
tn
(q|q)n
.

286
7 Special Functions
They may also be themselves considered
as generating functions for the q-binomial
coeÔ¨Écients:
z‚àínHn(cos ùúÉ|q) =
n
‚àë
k=0
[ n
k
]
q
z‚àí2k,
z = eiùúÉ.
The Hn(x|q) have the classical polynomials
as limits in the following sense:
lim
q‚Üí1‚àí
(
2
1 ‚àíq
)n‚àï2
Hn(x
‚àö
1 ‚àíq
2
|q))
‚ÜíHn(x).
They also satisfy ladder diÔ¨Äerence relations
similar to the diÔ¨Äerential ladder relations of
their classical counterparts:
Óà∞qHn(x|q) =
2(1 ‚àíqn)
1 ‚àíq
q(1‚àín)‚àï2Hn‚àí1(x|q);
1
w(x|q)Óà∞q
(w(x|q)Hn(x|q)) =
‚àí2qn‚àï2
1 ‚àíqHn+1(x|q).
7.7.5
Discrete Painlev√© Equations
There is a list of seven discrete Painlev√©
equations (there are two variants of PV) of
which the Ô¨Årst three are
ùõø-PI
xn+1 + xn‚àí1 = ‚àíxn + ùõºn + ùõΩ
xn
+ 1;
ùõø-PII
xn+1 + xn‚àí1 = (ùõºn + ùõΩ)xn + a
1 ‚àíx2
n
;
q-PIII
xn+1xn‚àí1 = (xn ‚àíaqùúÜn)(xn ‚àíbqùúÜn)
(1 ‚àícxn)(1 ‚àíxn
c )
.
Here, ùõº, ùõΩ, q, and ùúÜare constant parame-
ters and the notation ùõø- or q- refers to the
additive or multiplicative structures of the
diÔ¨Äerence scheme [77].
These equations are of interest in several
respects.
Firstly, they reduce to the classical
Painlev√© equations in appropriate ‚Äúcontin-
uum limits:‚Äù we allow that xn = f (ùúñn) for
diÔ¨Äerentiable f and let ùúñ‚Üí0 with some
adjustment of the free parameters [78, 79].
Secondly, they exhibit a property called
‚Äúsingularity
conÔ¨Ånement‚Äù
[77],
which
means that although certain initial condi-
tions may give rise to indeterminate values
of xn for some n, this indeterminacy is not
stable under small variations in those initial
conditions.
Thirdly, the initial conditions can be
assigned ‚Äúdegrees" in which the order of
growth of the solutions is merely poly-
nomial rather than exponential [80]. This
algebraic entropy is an unusual property
for diÔ¨Äerence schemes.
Finally, in the sense of the Nevanlinna
theory, the solutions are well controlled
[81].
Discrete Painlev√© equations share with
their classical limits the availability of B√§ck-
lund maps and Lax pairs (isomonodromy).
Finally, they are classiÔ¨Åed by the extended
aÔ¨Éne Weyl groups [82] that, so far as this
article is concerned, nicely returns us to
our starting point in the representations of
discrete symmetries.
References
1. Rose, J.S. (1978) A course on Group Theory,
Cambridge University Press.

References
287
2. Benson, D.J. (1993) Polynomial Invariants of
Finite Groups, LMS Lecture Note Series 190,
CUP.
3. Borovik, A.V. and Borovik, A. (2010) Mirrors
and ReÔ¨Çections: The Geometry of Finite
ReÔ¨Çection Groups, Springer.
4. Humphreys, J.E. (1990) ReÔ¨Çection Groups
and Coxeter Groups, Studies in Advanced
Mathematics 29, CUP.
5. Fulton, W. (1997) Young Tableaux: With
Applications to Representation Theory and
Geometry, Cambridge University Press.
6. MacDonald, I.G. (1998) Symmetric Functions
and Hall Polynomials, 2nd edn, OUP.
7. Forsyth, A.R. (1959) Theory of DiÔ¨Äerential
Equations, Dover.
8. Gray, J. (1996) Linear DiÔ¨Äerential Equations
and Group Theory from Riemann to
Poincar√©, 2nd edn, Birkh√§user.
9. Ince, E.L. (1956) Ordinary DiÔ¨Äerential
Equations, Dover.
10. Klein, F.C. (1914) Vorlesungen √ºber das
Ikosaeder und die AuÔ¨Ç√∂sung der
Gleichungen vom 5ten Grade, Teubner
(1884), Lectures on the Icosahedron, and the
Solution of Equations of the Fifth Degree, 2nd
revised edn (ed. English translation by G.G.
Morrice), Kegan Paul & Co., London.
11. Kuga, M. (1994) Galois‚Äô Dream: Group
Theory and DiÔ¨Äerential Equations, Corr. 2nd
printing, Birkh√§user.
12. Farkas, H.M. and Kra, I. (1992) Riemann
Surfaces, Springer.
13. Lie, S. (1970) Transformationsgruppen,
Chelsea, New York.
14. Humphreys, J.E. (1972) Introduction to Lie
Algebras and Representation Theory,
Springer.
15. Olver, P.J. (1986) Applications of Lie Groups
to DiÔ¨Äerential Equations, Springer.
16. Ovsiannikov, L.V. (1982) Group Analysis of
DiÔ¨Äerential Equations, Academic Press.
17. Warner, F.W. (2010) Foundations of
DiÔ¨Äerentiable Manifolds and Lie Groups,
Springer.
18. Fulton, W. and Harris, J. (1999)
Representation Theory: A First Course,
Springer.
19. JeÔ¨Äreys, H. and JeÔ¨Äreys, B. (2000) Methods of
Mathematical Physics, 3rd edn, Cambridge
University Press.
20. Wang, Z.X. and Guo, D.R. (1989) Special
Functions, World ScientiÔ¨Åc.
21. Abramowitz, M. and Stegun, I.A. (1965)
Handbook of Mathematical Functions, Dover
Publications, Inc.
22. Whitaker, E.T. and Watson, G.N. (1996) A
Course of Modern Analysis, 4th edn, CUP.
23. Hitchen, N. (1982) Monopoles and
Geodesics. Commun. Math. Phys., 83,
579‚Äì602.
24. Miller, W. (1977) Symmetry and Separation
of Variables (Encyclopedia of Mathematics
and its Applications), Addison-Wesley.
25. Gradshteyn, I.S. and Ryzhik, I.M. (2007)
Gradshteyn and Ryzhik‚Äôs Table of Integrals,
Series, and Products, 7th edn (eds A. JeÔ¨Ärey
and D. Zwillinger), Academic Press.
26. Talman, J.D. (1968) Special Functions: A
Group Theoretic Approach (based on lectures
by E.P. Wigner), W.A.Benjamin.
27. Vilenkin, N.J. (1968) Special Functions and
the Theory of Group Representations,
Translations of Mathematical Monographs
22, AMS.
28. Bultheel, A., Gonzalez-Vera, P., Hendriksen,
E., and Njastad, O. (1999) Orthogonal
Rational Functions, Cambridge University
Press.
29. Khrushchev, S. (2008) Orthogonal
Polynomials and Continued Fractions, CUP.
30. Ismail, M.E.H. (2005) Classical and
Quantum Orthogonal Polynomials in one
Variable, Encyclopedia of Mathematics and
its Applications 98, CUP.
31. Infeld, L. and Hull, T.E. (1953) The
factorization method. Rev. Mod. Phys., 23,
21‚Äì68.
32. Weyl, H. (1950) The Theory of Groups and
Quantum Mechanics, Dover.
33. Cartan, H. (2003) The Theory of Spinors,
Dover.
34. Darboux, G. (1890) Le√ßons Sur La Theorie
Generale Des Surfaces, Gauthier Villars Et
Fils.
35. (a) Moutard, Th.F. (1895) C. R. Acad. Sci.
Paris, 80, 129; (b) deLEcole, J. (1878)
Polytech. Cahier, 45, 1.
36. Novikov, S., Manakov, S.V., Pitaevskii, L.P.,
and Zakharov, V.E. (1984) Theory of Solitons:
The Inverse Scattering Method, Consultants
Bureau, New York.
37. Kamran, N. (2002) Selected Topics in the
Geometrical Study of DiÔ¨Äerential Equations,
CBMS 96, AMS.
38. Toda, M. (1989) Theory of Nonlinear
Lattices, Springer.

288
7 Special Functions
39. Matveev, V.B. and Salle, M.A. (1991)
Darboux Transformations and Solitons,
Springer.
40. Dunkl, C.F. (2003) Special functions and
generating functions associated with
reÔ¨Çection groups. J. Comput. Appl. Math.,
153, 181‚Äì190.
41. Opdam, E.M. (2000) Lecture Notes on Dunkl
Operators for Real and Complex ReÔ¨Çection
Groups, Mathematical Society of Japan
Memmoirs, vol. 8, Mathematical Society of
Japan.
42. R√∂sler, M. (2003) Dunkl operators: theory
and applications, in Orthogonal Polynomials
and Special Functions, Springer Lecture
Notes in Mathematics, vol. 1817, Springer.
43. Bender, C.M. and Orsag, S.A. (1999)
Advanced Mathematical Methods for
Scientists and Engineers, Springer.
44. Hille, E. (1976) Ordinary DiÔ¨Äerential
Equations in the Complex Domain, John
Wiley & Sons, Inc.
45. Meyer, R.E. (1989) A simple explanation of
the Stokes phenomenon. SIAM Rev., 31,
435‚Äì445.
46. Ritt, J.F. (1950) DiÔ¨Äerential Algebra, AMS.
47. Ritt, J.F. (1948) Integration in Finite Terms:
Liouville‚Äôs Theory of Elementary Methods,
Columbia University Press.
48. Abel, N.H. (2010) Oeuvres Compl√®tes, Nabu
Press.
49. Kaplansky, I. (1976) An Introduction to
DiÔ¨Äerential Algebra, 2nd edn, Hermann.
50. Kolchin, E.R. (1973) DiÔ¨Äerential Algebra and
Algebraic Groups, Pure and Applied
Mathematics 54, Academic Press, Boston,
MA.
51. Magid, A.R. (1994) Lectures on DiÔ¨Äerential
Galois Theory, University Lecture Series, vol.
7, AMS.
52. van der Put, M. and Singer, M.F. (1997)
Galois Theory of DiÔ¨Äerence Equations,
Lecture Notes in Mathematics 1666,
Springer.
53. Artin, E. (1971) Galois Theory, 6th printing,
University of Notre Dame.
54. Armitage, J.V. and Eberlien, W.F. (2006)
Elliptic Functions, LMS student tects 67,
Cambridge University Press.
55. Moll, V. (1999) Elliptic curves: Function
theory, Geometry, Arithmetic, CUP.
56. Weierstra√ü, K. (1856) Theorie der Abel‚Äôschen
Funktionen, Jour. reine & angewandte Math.,
52, 285‚Äì379.
57. Frobenius, G. and Stickelberger, L. (1877)
Zur Theorie der elliptischen Funktionen.
Journal f√ºr Math., 33, 179.
58. Buchstaber, V.M. and Leykin, D.V. (2008)
Solution of problem of diÔ¨Äerentiation of
abelian functions over parameters for
families of (n, s)-curves. Funct. Anal. Appl,
42, 268‚Äì278.
59. Baker, H.F. (1907) Abelian Functions: Abel‚Äôs
Theorem and the Allied Theory of Theta
Functions, Cambridge University Press,
Cambridge 1897 (reprinted in 1995); An
Introduction to the Theory of Multiply
Periodic Functions, Cambridge University
Press.
60. Buchstaber, V.M., Enolski, V., and Leykin, D.
(1997) Kleinian functions, hyperelliptic
Jacobians and applications. Rev. Math. Math.
Phys., 10, 1‚Äì125.
61. Mumford, D. (1983/4) Tata Lectures on
Theta I & II, Birkh√§user.
62. Weierstra√ü, K. (1894) Zur Theorie der
elliptischen Funktionen, Mathematische
Werke, Bd 2, Teubner, Berlin, pp. 245‚Äì255.
63. Bobenko, A.I. and Eitner, U. (2000) Painlev√©
Equations in the DiÔ¨Äerential Geometry of
Surfaces, LNM 1753, Springer.
64. Conte, R. and Musette, M. (2008) The
Painlev√© Handbook, Springer.
65. Clarkson, P. Painlev√© Transcendents, Chapter
32 in Digital Library of Mathematical
Functions, http://dlmf.nist.gov/32 (accessed
on 24 February 2014).
66. Painlev√©, P. (1897) Lecons, sur la theorie
analytique des equations diÔ¨Äerentielles,
professees a Stockholm, Libraire ScientiÔ¨Åque
√† Hermann, Paris.
67. Noumi, M. (2004) Painlev√© Equations
through Symmetry, American Mathematical
Society.
68. Goursat, E. (1917) in A Course in
Mathematical Analysis (eds translation by
E.R. Hedrick and O. Dunkel), Ginn & Co.
69. Gambier, B. (1910) Sur les √©quations
diÔ¨Ä√©rentielles du second ordre et du premier
degr√© dont l‚Äôint√©grale g√©n√©rale est √† points
critique Ô¨Åx√©s. Acta Math., 33, 1‚Äì55.
70. Flaschka, H. and Newell, A.C. (1980)
Monodromy- and spectrum-preserving
deformations. I. Commun. Math. Phys., 76,
65‚Äì116.
71. Jimbo, M. and Miwa, T. (1981) Monodromy
preserving deformation of linear ordinary

References
289
diÔ¨Äerential equations with rational
coeÔ¨Écients. Phys. D 2, 407‚Äì448.
72. Truesdell, C. (1948) An Essay Towards a
UniÔ¨Åed Theory of Special Functions,
Princeton University Press.
73. Brown, K.A. and Goodearl, K.R. (2002)
Lectures on Algebraic Quantum Groups,
Birkh√§user.
74. Klimik, A. and Sm√ºdgen, K. (1997) Quantum
Groups and their representations, Springer.
75. Kac, V. and Cheung, P. (2002) Quantum
Calculus, Springer.
76. van der Put, M. and Singer, M.F. (2003)
Galois Theory of Linear DiÔ¨Äerential
Equations, Springer.
77. Grammaticos, B. and Ramani, A. (2004)
Discrete Painlev√© Equations: A Review,
Lecture Notes in Physics, vol. 644, Springer,
pp. 245‚Äì321.
78. Conte, R. and Musette, M. (1996) A new
method to test discrete Painlev√© equations.
Phys. Lett. A, 223, 439‚Äì448.
79. Fokas, A.S., Grammaticos, B., and Ramani,
A. (1993) From continuous to discrete
Painlev√© equations. J. Math. Anal. Appl.,
180, 342‚Äì3360.
80. Veselov, A.P. (1992) Growth and integrability
in the dynamics of mappings. Commun.
Math. Phys., 145, 181‚Äì193.
81. Ablowitz, M.J. and Halburd, R. (2000)
Nevanlinna theory and diÔ¨Äerence equations
of Painlev√© type. Proceedings of the Workshop
on Nonlinearity, Integrability and All That:
Twenty Years after NEEDS ‚Äô79 (Gallipoli,
1999), 3‚Äì11, 2000, World Science
Publishing, River Edge, NJ.
82. Sakai, H. (2001) Rational surfaces associated
with aÔ¨Éne root systems and geometry of the
Painlev√© equations. Commun. Math. Phys.,
220, 165.


291
8
Computer Algebra
James H. Davenport
8.1
Introduction
Computer algebra, sometimes also called
symbolic computation to distinguish it
from numeric computation, is exactly what
the name suggests: it is the use of com-
puters to do algebra. Though sometimes
referred to as ‚Äúcomputer mathematics,‚Äù
it is not necessarily that, as mathemati-
cally incorrect results can be algebraically
‚Äúcorrect,‚Äù as in the school howler
1 =
‚àö
1 =
‚àö
(‚àí1)2 = ‚àí1.
(8.1)
This
problem
is
further
discussed
in
Section 8.7.2.
In fact, it is not even algebra for which
we need software packages, computers
by themselves cannot actually do arith-
metic: only a limited subset of it. It is
1) In fact, Excel is more complicated even than this, as the calculations in this table show.
i
1
2
3
4
‚Ä¶
10
11
12
‚Ä¶
15
16
a
10i
10
100
1000
1‚Ä¶0
‚Ä¶
1‚Ä¶0
1011
1012
‚Ä¶
1015
1016
b
a-1
9
99
999
9999
9‚Ä¶9
‚Ä¶
9‚Ä¶9
1012
‚Ä¶
1015
1016
c
a-b
1
1
1
1
1
‚Ä¶
1
1
‚Ä¶
1
0
We can see that the printing changes at 12 decimal digits, but that actual accuracy is not lost until we
subtract 1 from 1016.
the case that eùúã
‚àö
163 ‚âà2.625 374 126 407
687 43.9 999 999 999 992 500 725. If we ask
Excel (or any similar software package) to
compute eùúã
‚àö
163 ‚àí262 537 412 640 768 744,
we will be told that the answer is 256 (or
‚àí10944, depending on the version of Excel).
More mysteriously, if we go back and look
at the formula in the cell, we see that it is
now eùúã
‚àö
163 ‚àí262 537 412 640 768 800, or
even eùúã
‚àö
163 ‚àí262 537 412 640 768 000. In
fact, 262 537 412 640 768 744 is too large a
whole number (or integer, as mathemati-
cians say) for Excel to handle, and Excel
has converted it into Ô¨Çoating-point (what
Excel terms scientiÔ¨Åc) notation. Excel, or
any other software using the IEEE stan-
dard [1] representation for Ô¨Çoating-point
numbers, can only store them to a given
accuracy, about (we say ‚Äúabout‚Äù because
the internal representation is binary, rather
than decimal) 16 decimal places.1 In fact,
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

292
8 Computer Algebra
it requires twice this precision to show
that eùúã
‚àö
163 ‚â†262 537 412 640 768 744. As
eùúã
‚àö
163 = (‚àí1)
‚àö
‚àí163, it follows from deep
results of transcendental number theory
[2], that not only is eùúã
‚àö
163 not an integer, it
is not a fraction (or rational number), nor
even the solution of a polynomial equation
with integer coeÔ¨Écients: essentially it is a
‚Äúnew‚Äù number.
Although computers had been used to
support mathematics earlier (in 1951, the
79-digit number 180 (2127 ‚àí1
)2 ‚àí1 was
shown to be prime and in 1952, the great
mathematician Emil Artin had the equally
great computer scientist John von Neu-
mann performed an extensive calculation
relating to elliptic curves on the MANIAC
computer [3, p. 119]), actual computer
algebra is 60 years old, since in 1953, two
theses [4, 5] described programs to dif-
ferentiate expressions, and Haselgrove [6]
showed that algorithms in group theory
could be implemented on computers.
8.2
Computer Algebra Systems
While these early programs could only do
one thing, they soon evolved into more
general-purpose systems that could do
a variety of algebraic calculations. The
descendants of [4, 5] are the ‚Äúpolynomial‚Äù
systems that manipulate (fractions of)
polynomials in ‚Äúindeterminates‚Äù such as x,
y or ‚Äúpseudo-indeterminates‚Äù such as sin x,
ey, and so on, early examples of which are
the Polynomial Manipulator PM [7], the
Symbolic Mathematical Laboratory [8] and
its successor Macsyma [9]2), the Cambridge
Algebra Manipulation Language CAMAL
[10], and Reduce [11]. The group theory of
2) And various oÔ¨Äshoots MAXIMA, Vaxima, and
so on.
Haselgrove [6] gave rise to CAYLEY [12],
then Magma [13] as well as a range of more
specialist systems.
This split survives to this day, with
major ‚Äúpolynomial‚Äù systems, now generally
described as ‚Äúcalculus‚Äù systems, being
Maple, Mathematica, and SAGE (the latter
incorporates much of MAXIMA) and
major group theory systems being GAP
and MAGMA. Why the split? The diÔ¨Äer-
ence seems one of mathematical attitude,
if one can call it that. The designer of a
calculus system envisages it being used
to compute an integral, factor a polyno-
mial, multiply two matrices, or otherwise
operate on a mathematical datum. The
designer of a group theory system, while
he will permit the user to multiply, say,
permutations or matrices, does not regard
this as the object of the system: rather the
object is to manipulate whole groups (etc.)
of permutations (or matrices, or other
mathematical data), that is, mathematical
structures, and take, say, the quotient of two
groups, rather than of two polynomials.
More recently, we have seen the rise of sys-
tems to manipulate polynomial structures
(generally ideals‚Äìsee DeÔ¨Ånition 8.3) such
as CoCoA [14, 15], SINGULAR [16] and
Macaulay [17, 18], and ‚Äúcalculus‚Äù systems
incorporate more of this, so the distinction
is blurring.
8.3
‚ÄúElementary‚Äù Algorithms
8.3.1
Representation of Polynomials
The ‚Äúobvious‚Äù way to represent a poly-
nomial, say 3x3 + 2x ‚àí5, in a computer
would be to store a vector of coeÔ¨É-
cients, that is, [3,0,2,-5]. Addition
of polynomials is then just addition of

8.3 ‚ÄúElementary‚Äù Algorithms
293
vectors, and multiplication is a convolu-
tion. Polynomials in several variables are
then stored either as multidimensional
arrays (a method known as distributed)
or as polynomials in x whose coeÔ¨Écients
are polynomials in y and so on (a method
known as recursive).
The problem with this approach can
already be seen in the previous para-
graph: we had to write 3x3 + 2x ‚àí5 as
3x3 + 0x2 + 2x ‚àí5 in order to get a com-
plete set of coeÔ¨Écients. This might seem
harmless, but a dense representation of
abcde ‚àí1 would be
a1b1c1d1e1 + 0 ‚ãÖa1b1c1d1e0 + ¬∑ ¬∑ ¬∑
‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü
5 terms
+ 0 ‚ãÖa1b1c1d0e0 + ¬∑ ¬∑ ¬∑
‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èû‚èü
10 terms
+ ¬∑ ¬∑ ¬∑ + a0b0c0d0e0
for a total of 32 terms. a2b2c2d2e2 ‚àí1 has
243 terms, and, in general, a polynomial of
degree d in n variables has (d + 1)n terms.
Adding two such polynomials requires (d +
1)n operations, and multiplying two takes
(d + 1)2n operations.3) Storing, or at least
counting, all potential terms is known as
the dense model.
Hence most systems adopt a sparse
approach (analogous to the storage of
sparse matrices in MatLab, etc.) and
only store the nonzero coeÔ¨Écients (and
the
corresponding
exponents).
Hence,
3x3 + 2x ‚àí5 becomes [(3,3),(2,1),
(-5,0)] and a2b2c2d2e2 ‚àí1 becomes
[(1,2,2,2,2,2),(-1,0,0,0,0,0)]
(sparse
distributed)
or
[a,(z,2),(-
1,0)] where z is the representation
of b2c2d2e2 (sparse recursive). If there
3) ‚ÄúFast‚Äù algorithms, such as Karatsuba multiplica-
tion [19] or fast Fourier transform ones [20] can
improve on these running times, but not on the
storage requirements.
are tf and tg nonzero terms in f
and
g,
addition
takes
‚â§min(tf , tg)
arith-
metic operations and multiplication takes
‚â§2tf tg.
We have emphasized arithmetic oper-
ations because the real challenge now
becomes that of arranging the output
in a useful order (nice), and combin-
ing/canceling
duplicates
(necessary
to
preserve sparseness). This can be viewed
as a sorting problem, and techniques such
as HeapSort [21], BucketSort [22], or
hashing (as in Maple [23]) are used. In the
sparse-distributed representation, order is
not quite so obvious: do we put x2y before
(greater x power) or after (lower total
degree) xy3? See Section 8.5.4 for more
details.
Once we move beyond addition and mul-
tiplication, just counting nonzero terms in
the input no longer bounds the complexity.
The division of two two-term polyno-
mials can be arbitrarily large, because
(xn‚àí1)‚àï(x ‚àí1) = xn‚àí1 + xn‚àí2 + ¬∑ ¬∑ ¬∑ + x + 1.
However, it is possible to compute h = f ‚àïg
with the number of operations depend-
ing only on the number of terms tf ,
tg, and th, and not on the degrees [24].
Counting only the nonzero terms in the
input and output is known as the sparse
model.
It is worth noting that neither the dense
model nor the sparse one quite captures
our
intuitive
notion
of
‚Äúsmall‚Äù‚Äìmost
people would think of 1 + (x ‚àí1)100 as a
‚Äúsmall expression,‚Äù but neither the dense
nor sparse models would think so. This is
the domain of the ‚Äústraight-line program‚Äù
model, but this is beyond the scope of this
article. An example showing the superior
performance, both in time and more espe-
cially in space, of the straight-line program
over conventional data structures is given
in [25].

294
8 Computer Algebra
8.3.2
Greatest Common Divisors
The computation of polynomial common
divisors has many uses, not least in the
simpliÔ¨Åcation of rational functions, as in
(x2 + 3x + 2)‚àï(x + 1) ‚Üíx + 2. The problem
might seem to be solved by Euclid‚Äôs Algo-
rithm (8.1), and, indeed, conceptually it is.
In practice, though, there are two serious
problems with this.
Algorithm 8.1 (Euclid)
Input: a0, a1 ‚ààK[x].
Output:
h ‚ààK[x] a greatest common
divisor of a0 and a1
i ‚à∂= 1;
while ai ‚â†0 do
ai+1 = rem(ai‚àí1, ai);
# Let qi be the
corresponding quotient
i ‚à∂= i + 1;
return ai‚àí1;
8.3.2.1
Intermediate Expression Swell
As we go round the loop in Algorithm 8.1,
the degree (in x) of the polynomials does
keep decreasing. But nothing is said about
the size of the coeÔ¨Écients. If we apply this
algorithm to two fairly innocuous polyno-
mials:
A(x) = x8 + x6‚àí3x4 ‚àí3x3 + 8x2 + 2x ‚àí5;
B(x) = 3x6 + 5x4‚àí4x2 ‚àí9x ‚àí21.
}
(8.2)
we get the sequence
‚àí5
9 x4 + 127
9 x2 ‚àí29
3 ,
50157
25
x2 ‚àí9x ‚àí35847
25
,
93 060 801 700
1 557 792 607 653x + 23 315 940 650
173 088 067 517
and Ô¨Ånally
761 030 000 733 847 895 048 691
86 603 128 130 467 228 900
.
As this is a number, it follows that no poly-
nomial can divide both A and B, that is, that
gcd (A, B) = 1. The reader might think that
‚Äúclearing fractions‚Äù would help: it does not,
and we end up with
7 436 622 422 540 486 538 114 177 255
855 890 572 956 445 312 500.
Had the coeÔ¨Écients not been integers,
but rather polynomials in y, z, ‚Ä¶, the
growth
would
have
been
even
more
spectacular: see the worked example at
http://staÔ¨Ä.bath.ac.uk/masjhd/JHD-CA/
GCD3var.html. The good news is that
advanced algorithms (Section 8.4) have
pretty much solved this problem: we can
compute greatest common divisors (gcds)
eÔ¨Éciently, working all the time with data
whose polynomial degrees in y, z, ‚Ä¶ are
no larger than those in the input, and
with numerical coeÔ¨Écients that are not
much larger than in the input. We say ‚Äúnot
much larger‚Äù because a gcd can have larger
coeÔ¨Écients than the input, for example
A = x5 + 3x4 + 2x3 ‚àí2x2 ‚àí3x ‚àí1
= (x + 1)4(x ‚àí1);
B = x6+3x5+3x4+2x3+3x2+3x+1
= (x + 1)4(x2 ‚àíx + 1);
gcd (A, B) = x4 + 4x3 + 6x2 + 4x + 1
= (x + 1)4.
(8.3)
These algorithms tend to be probabilistic,
and produce an answer g, which, if it is a
common divisor of A and B at all (and this
needs to be veriÔ¨Åed, either by checking that
g divides A and B, or by computing A‚àïg
and B‚àïg as well as g), is guaranteed to be
a greatest common divisor.

8.3 ‚ÄúElementary‚Äù Algorithms
295
8.3.2.2
Sparsity
The ‚Äúgood news‚Äù above is indeed good as
far as it goes, and shows that, in the dense
model, we can do about as well as possible,
but it does not speak to the sparse model.
In particular, it still leaves open the possi-
bilities that
1. the gcd g might be much denser than
the inputs A and B
2. even if the gcd g is not much denser
than the inputs A and B, the cofactors
A‚àïg and B‚àïg computed as part of the
veriÔ¨Åcation might be much denser
3. even if neither of these happens,
various intermediate results might be
much denser than A and B.
Problem 2 is easy to demonstrate: con-
sider the cofactors in gcd (xp ‚àí1, xq ‚àí1),
where p and q are distinct primes. Prob-
lem 1 is demonstrated by the following ele-
gant example of [26]
gcd (xpq ‚àí1, xp+q ‚àíxp ‚àíxq + 1)
= xp+q‚àí1 ‚àíxp+q‚àí2 ¬± ¬∑ ¬∑ ¬∑ ‚àí1,
(8.4)
and indeed just knowing whether two poly-
nomials have a nontrivial gcd is hard, by the
following result.
Theorem 8.1 [27] It is NP-hard to deter-
mine whether two sparse polynomials (in
the standard encoding) have a nontrivial
common divisor.
This theorem, like the examples above,
relies on the factorization of xp ‚àí1, and
it is an open question [24, Challenge 3]
whether this is the only obstacle.
More precisely, we have the follow-
ing equivalent of the problem solved for
division.
Challenge 8.1 [24, Challenge 5]
Find
an algorithm for computing h = gcd ( f , g)
which is polynomial-time in tf , tg, and th.
A weaker problem, but still unsolved, is
Challenge 8.2 Find an algorithm for com-
puting h = gcd ( f , g) which is polynomial-
time in tf , tg, th, and tf ‚àïh, tg‚àïh.
Problem 3 is particularly acute in the
case of multivariate polynomials. Further
developments [28, Section 4.4] of the ideas
in Section 8.4, using methods from [29],
have produced algorithms whose time
seems empirically to vary as d2t rather
than dn for polynomials of degree d and t
terms, in n variables. Hence this problem
is largely solved in practice, but theoretical
analysis is still lacking, as we cannot solve
Challenges 8.1 and 8.2.
8.3.3
Square-free Decomposition
One particular use of gcds is given by the
following result.
Proposition 8.1 If ùõºis a repeated root of
the polynomial f , then it is a root of f ‚Ä≤,
and hence of gcd ( f , f ‚Ä≤). In fact, the roots of
gcd ( f , f ‚Ä≤) are precisely the repeated roots
of f .
DeÔ¨Ånition 8.1 A polynomial f is said to be
square-free if it has no repeated roots. The
square-free decomposition of f is f = ‚àèf i
i ,
where each fi is square-free and gcd ( fi, fj) =
1 if i ‚â†j.
Theorem 8.2 [30] Over Z and in the stan-
dard sparse encoding, the two problems
1. deciding if a polynomial is square-free
and
2. deciding if two polynomials have a
nontrivial gcd.
are
equivalent
under
randomized
polynomial-time reduction.
Hence, in the light of Theorem 8.1,
determining
square-freeness
is
hard,

296
8 Computer Algebra
at
least
when
polynomials
with
fac-
tors of the form xp ‚àí1 are involved.
Again, it is an open question whether
these (and scaled variants) are the only
obstacles.
8.3.4
Extended Euclidean Algorithm
If we look at Algorithm 8.1, we see that a0
and a1 are, trivially, each a linear combina-
tion of a0 and a1. ai+1 = ai‚àí1 ‚àíqiai, so is a
linear combination of a0 and a1 if each of
ai‚àí1 and ai are. Hence by induction every ai
is a linear combination of a0 and a1, and, in
particular, the gcd is.
Proposition 8.2 (Bezout‚Äôs identity)
There exist polynomials ùúÜand ùúásuch that
gcd ( f , g) = ùúÜf + ùúág.
Note that Algorithm 8.1, as we ran it,
generated fractions, so ùúÜand ùúámay well
have fractions as coeÔ¨Écients.
A useful application of Bezout‚Äôs identity
is the theory of partial fractions: if f and g
are relatively prime, then
h
fg = h(ùúÜf + ùúág)
fg
= ùúÜh
g + ùúáh
f .
(8.5)
Even if h‚àïfg is reduced (deg(h) < deg( fg)),
the summands may not be, but we can
adjust them:
h
fg = rem(ùúÜh, g)
g
+ rem(ùúáh, f )
f
,
(8.6)
and the summands are now reduced.
8.4
Advanced Algorithms
There are two main classes of ‚Äúadvanced‚Äù
algorithms in computer algebra for solving,
not only the gcd problem and its variants,
but many others.
8.4.1
Modular Algorithms‚ÄìInteger
We described the polynomials in (8.2) as
‚Äúinnocuous.‚Äù They are innocuous if we con-
sider them, and apply Algorithm 8.1, mod-
ulo 5, writing P5 to signify the polynomial
P considered as a polynomial with coeÔ¨É-
cients modulo 5.
A5(x) = x8 + x6 + 2x4 + 2x3 + 3x2 + 2x;
B5(x) = 3x6 + x2 + x + 1;
C5(x) = remainder(A5(x), B5(x))
= A5(x) + 3(x2 + 1) B5(x) = 4x2 + 3;
D5(x) = remainder(B5(x), C5(x))
= B5(x) + (x4 + 4x2 + 3) C5(x) = x;
E5(x) = remainder(C5(x), D5(x))
= C5(x) + x D5(x) = 3.
But if P divides A, then A = PQ for
some polynomial Q, and A5 = P5Q5. Sim-
ilarly, if P divides B, then B = PR for some
polynomial R, and B5 = P5R5. Hence if
P = gcd (A, B), P5 has to be a common
factor of A5 and B5, but there no a non-
trivial one, so P5 has to be 1, and, because
the leading coeÔ¨Écient of P has to be ¬±1,
P = 1. Of course, there is nothing special
about 5: we could have picked almost any
prime‚Äì2 does not work as gcd (A2, B2) ‚â†1
and 3 is dubious as B3 has lower degree
than B. However, we have to answer sev-
eral questions before we can make this
into a complete gcd algorithm replacing
Algorithm 8.1.
1. How do we calculate a nontrivial gcd?
2. what do we do if the gcd modulo p is
not the modular image of the gcd (as
with A2 and B2)? In this case, we say
that we have bad reduction, or that p is
bad. In particular, what do we do if this
keeps on happening?
3. How much does this method cost?

8.4 Advanced Algorithms
297
The Ô¨Årst question already makes us think:
if the gcd is x + 42, we are never going to
decide this by working modulo 5 (which
might suggest x + 2), or indeed any prime
smaller than 85 = 2 √ó 42 + 1, because we
have to allow for the sign, as working mod-
ulo 43 would suggest x ‚àí1 as the answer,
rather than x + 42. Furthermore, we know
from (8.3) that the largest coeÔ¨Écient in
a gcd is not, as we might hope, at most
the largest coeÔ¨Écient in the inputs. For-
tunately, we can nevertheless bound the
largest coeÔ¨Écient in a gcd.
Proposition 8.3 (Landau‚ÄìMignotte
bound [31, 32]) Every coeÔ¨Écient of the gcd
of A = ‚àëùõº
i=0 aixi and B = ‚àëùõΩ
i=0 bixi (with ai
and bi integers) is bounded by
LM(A, B) ‚à∂= 2min(ùõº,ùõΩ) gcd (aùõº, bùõΩ)
min
‚éõ
‚éú
‚éú‚éù
1
|aùõº|
‚àö
‚àö
‚àö
‚àö
ùõº
‚àë
i=0
a2
i ,
1
|bùõΩ|
‚àö
‚àö
‚àö
‚àö
ùõΩ‚àë
i=0
b2
i
‚éû
‚éü
‚éü‚é†
. (8.7)
Furthermore, the number 2 in 2min(ùõº,ùõΩ) can-
not be replaced by any smaller number [33].
Once we know this, we have two choices.
‚Ä¢ Work modulo a prime p > 2LM(A, B), so
that, unless problem 2 occurs, Pp will,
interpreting its coeÔ¨Écients as integers in
‚àíp‚àï2, ‚Ä¶ , p‚àï2, be P itself. This is
perfectly correct, but often ineÔ¨Écient as,
although (8.7) cannot be improved on as
a worst case bound, it is normally far too
pessimistic.
‚Ä¢ Work modulo several diÔ¨Äerent small
primes pi, use the Chinese Remainder
Theorem [28, Section A.3] to compute
P‚àèpi from the various Ppi. As before, if
‚àèpi >2LM(A, B), P‚àèpi will, interpreting
its coeÔ¨Écients as integers in ‚àí(‚àèpi)‚àï2,
‚Ä¶ , (‚àèpi)‚àï2, be P itself. This method
lends itself to various shortcuts if (8.7) is
pessimistic in a given case, as it nearly
always is.
Problem 2 is nevertheless a genuine prob-
lem. Again, we can say something.
Proposition 8.4 (Good
Reduction
Theorem (Z) [28]) The bad p are those
that divide either gcd (aùõº, bùõΩ) or a certain
nonzero determinant D (in fact, the resul-
tant Resx(A‚àïP, B‚àïP)). Furthermore, if p
divides Resx(A‚àïP, B‚àïP) but not gcd (aùõº, bùõΩ),
then the gcd computed modulo p has a
larger degree than the true result.
In particular, there are only a Ô¨Ånite num-
ber of bad primes.
We can test the Ô¨Årst half of this (p divid-
ing gcd (aùõº, bùõΩ)) and not even try such
primes. After this, if we have two primes
that give us diÔ¨Äerent degrees for the gcd,
we know that the prime with the higher
degree has bad reduction, from the ‚ÄúFur-
thermore ‚Ä¶‚Äù statement, so we can discard
it. The corresponding algorithm is given
as [Algorithm Modular GCD (small prime
version)] [28].
As far as the running time is concerned,
if l bounds the length of the coeÔ¨Écients
in the input and output, and d the degree
of the polynomials, then the running time
is C(d + 1)(l2 + (d + 1)) for a suitable con-
stant C, depending on the computer, soft-
ware, and so on, provided we ignore the
possibility of unlucky primes [34, (95)]. We
note that this depends on the degree, not
the number of terms, as even modulo p we
cannot solve Challenges 8.1 and 8.2.
8.4.2
Modular Algorithms‚ÄìPolynomial
Despite our failure to address the sparsity
issue, that algorithm is still substantially
better than Euclid‚Äôs, because the coeÔ¨Écient
growth is bounded, by Proposition 8.3. Can

298
8 Computer Algebra
we do the same for multivariate polynomi-
als, and avoid the spectacular growth seen
in
http://staÔ¨Ä.bath.ac.uk/masjhd/JHD-
CA/GCD3var.html? We consider the case
of polynomials in x and y, regarding x
as the main variable: the generalization
to more variables is easy. There is one
preliminary remark: the gcd might actually
not depend on x, but be a polynomial in y
alone. In that case, it would have to divide
every coeÔ¨Écient of the input polynomials,
regarded as polynomials in x whose coef-
Ô¨Åcients are polynomials in y. Hence, it can
be determined by gcds of polynomials in
y alone, and we will not concern ourselves
further with this (though the software
implementer clearly has to), and consider
only the part of the gcd that depends
on x.
The answer is that we can Ô¨Ånd this eÔ¨É-
ciently, and the idea is similar to the pre-
vious one: we replace ‚Äúworking modulo p‚Äù
with ‚Äúevaluating y at v.‚Äù Just as we wrote
P5 to signify the polynomial P considered
as a polynomial with coeÔ¨Écients modulo 5,
we write Py=v to signify the polynomial P
with y evaluated at the value v. But if P
divides A, then A = PQ for some polyno-
mial Q, and Ay=v = Py=vQy=v. Similarly, if P
divides B, then B = PR for some polyno-
mial R, and By=v = Py=vRy=v. Hence if P =
gcd (A, B), Py=v has to be a common factor
of Ay=v and By=v. In particular, if Ay=v and
By=v have no common factor, the gcd of A
and B cannot depend on x. Analogous ques-
tions to those posed above appear.
1. How do we calculate a nontrivial gcd?
2. What do we do if the gcd of the
evaluated polynomials is not the
evaluation of the true gcd? In this case,
we say that we have bad reduction, or
that v is a bad value. In particular, what
do we do if this keeps on happening?
3. How much does this method cost?
The answers are similar, or indeed eas-
ier. For question 1, we use the Chinese
Remainder Theorem for polynomials [28,
Section A.4] to compute the true result
from several evaluations. How many? In the
integer coeÔ¨Écient case, we had to rely on
Proposition 8.3, but the equivalent is trivial.
Proposition 8.5 (Polynomial version of
Proposition 8.3)
degy(gcd ( f , g)) ‚â§min
(degy ( f ), degy(g)).
For question 2, we have a precise analogue
of Proposition 8.4.
Proposition 8.6 (Good
Reduction
Theorem
(polynomial)
[28])
If
y ‚àív
does not divide gcd (aùõº, bùõΩ) (which can be
checked for in advance) or Resx(A‚àïC, B‚àïC),
then v is of good reduction. Furthermore,
if y ‚àív divides Resx(A‚àïC, B‚àïC) but not
gcd (aùõº, bùõΩ), then the gcd computed modulo
y ‚àív has a larger degree than the true
result.
This gives us algorithms precisely equiva-
lent to those in the previous section, and
then do indeed generalize to n variables.
Question 3 has the following answer. If l
bounds the length of the coeÔ¨Écients in
the input and output, and d the degree of
the polynomials, then the running time is
C(d + 1)n(l2 + l(d + 1)) for a suitable con-
stant C, depending on the computer, soft-
ware, and so on, provided we ignore the
possibility of unlucky primes and values
[34, (95)].
8.4.3
The Challenge of Factorization
If we want to factor polynomials, we might
as well Ô¨Årst do a square-free factorization
(Section 8.3.3): as well as eÔ¨Écient, this will
turn out to be necessary. This means that
gcd ( f , f ‚Ä≤) = 1. It follows from Proposi-
tion 8.4 that there are only Ô¨Ånitely many

8.4 Advanced Algorithms
299
primes for which gcd ( fp, f ‚Ä≤
p) ‚â†1, and we
check for, and discard, these primes.
Then the obvious solution is to follow this
approach, and start with factoring modulo
p. For small primes p and degrees d, we can
just enumerate the p(d+1)‚àï2 possibilities,4)
but it turns out we can do much better.
There are algorithms due to Berlekamp
[35, 36]
and
Cantor‚ÄìZassenhaus
[37],
which take time proportional to d3.
Modular methods, as described in the
previous two sections, can indeed solve
many other problems besides the gcd one.
Indeed, no computer algebra conference
goes past without a new modular algorithm
appearing somewhere. However, there is
one problem they cannot solve eÔ¨Éciently,
and that is polynomial factorization.
This might seem strange, for we have the
same modular/integer relationship as we
had in the gcd case: if f = f (1) ‚ãÖf (2) ‚ãÖ¬∑ ¬∑ ¬∑ ‚ãÖf (k)
over the integers, then fp = f (1)
p
‚ãÖf (2)
p
‚ãÖ¬∑ ¬∑ ¬∑ ‚ãÖ
f (k)
p . In particular, if f and fp have the same
degree, and fp is irreducible, then f is also
irreducible. We may as well remark that the
converse is not true: it is possible for an
irreducible f to factor modulo every prime.
Example 8.1 The polynomial x4 + 1, which
is irreducible over the integers, factors into
two quadratics (and possibly further) mod-
ulo every prime.
p = 2
Then x4 + 1 = (x + 1)4.
p = 4k + 1
In this case, ‚àí1 is always a
square, say ‚àí1 = q2. This gives us the
factorization x4 + 1 = (x2 ‚àíq)(x2 + q).
p = 8k ¬± 1
In this case, 2 is always a
square, say 2 = q2. This gives us the
factorization
x4 + 1 = (x2 ‚àí(2‚àïq)
x + 1)(x2 + (2‚àïq)x + 1). In the case
p = 8k + 1, we have this factorization
4) The exponent (d + 1)‚àï2 comes from the fact that
we need only look for factors of degree ‚â§d‚àï2.
and the factorization given in the
previous case. As these two factoriza-
tions are not equal, we can calculate
the gcds of the factors, in order to Ô¨Ånd
a factorization as the product of four
linear factors.
p = 8k + 3
In this case, ‚àí2 is always a
square, say ‚àí2 = q2. This is a result
of the fact that ‚àí1 and 2 are not
squares, and so their product must
be a square. This property of ‚àí2
gives us the factorization x4 + 1 =
(x2 ‚àí(2‚àïq)x ‚àí1)(x2 + (2‚àïq)x ‚àí1)
This polynomial is not an isolated oddity:
Swinnerton-Dyer [38] and Kaltofen et al.
[39] proved that there are whole families
of polynomials with this property of being
irreducible, but of factorizing modulo
every prime. Furthermore, these polyno-
mials have the annoying habit of being
generated disproportionately often [40].
However, this is not the main problem.
Suppose fp factors as g1g2g3, and fq fac-
tors as h1h2h3. We may be in luck, and these
factorizations may be so incompatible that
we can deduce that f has to be irreducible.
However, they may be compatible, say all
have degree k. Then it would be natural, fol-
lowing Section 8.4.2, to apply the Chinese
Remainder Theorem, and deduce that
fpq = CRT(g1, h1)CRT(g2, h2)CRT(g3, h3).
(8.8)
This would indeed be correct, but it is not
the only possibility. Our labels 1‚Äì3 are
purely arbitrary, and it would be just as con-
sistent to deduce
fpq = CRT(g1, h2)CRT(g2, h3)CRT(g3, h1),
(8.9)
or any of the other four possibilities. If
we take f = x3 + 41 x2 + 551 x + 2431, f5 =
(x + 2)(x + 1)(x + 3) and f7 = (x + 4) (x +
3)(x + 6). Combining (x + 2)5 and (x + 4)7

300
8 Computer Algebra
gives (x + 32)35, and indeed this divides f35.
If we combine (x + 2)5 and (x + 3)7, we get
(x + 17)35, and again this divides f35. In fact,
we deduce that f35, though of degree 3, has
nine linear factors. An algebraist would tell
us that polynomials modulo a composite,
such as 35, do not possess unique factoriza-
tion.
However, not all the possibilities (8.8),
(8.9), and so on correspond to the true
factorization. Although we have no a pri-
ori way of knowing it, a factor modulo p
corresponds (apart from the bad reduction
problem) to one, and only one, factor over
the integers, and this corresponds to one,
and only one, factor modulo q. Hence, if
there are m rather than just three factors,
there may be m! possible matchings of the
results modulo p and q, only one of which
is right.
Worse, if pq is not bigger than twice the
Landau‚ÄìMignotte bound, we will need to
use a third prime r, and we will now have
(m!)2 possibilities, only one of which is
right, and so on. This is clearly unsustain-
able, and we need a diÔ¨Äerent approach.
8.4.4
p-adic Algorithms‚ÄìInteger
Instead of using several primes, and deduc-
ing the answer modulo p, then pq and
pqr, and so on, we will use one prime,
and deduce the answer modulo p, then
p2, p3, and so on. For simplicity, we will
assume that fp = gphp modulo p‚Äìmore fac-
tors do not pose more challenges, but make
the notation diÔ¨Écult. We will also suppose
that f is monic: this is a genuine simpliÔ¨Å-
cation, but solving the nonmonic case as
well would distract us‚Äìdetails are in [28,
Chapter 5].
Let us suppose that fp2 = gp2hp2 where,
and this is the diÔ¨Äerence, we insist that
gp2 corresponds to gp, that is, that they are
equal modulo p. We can therefore write
gp2 = gp + pÃÇgp2, and similarly for the h‚Äôs.
Then
fp2 = (gp + pÃÇgp2
) (
hp + pÃÇhp2
)
,
so
fp2 ‚àígphp
p
‚â°gp ÃÇhp2 + hpÃÇgp2 (mod p), (8.10)
where the cross-product gp2hp2 disappears
as it is multiplied by an extra p, and the
fraction on the left is not really a fraction
because gphp ‚â°fp ‚â°fp2 (mod p).
As we have made fp square-free (remark
at the start of Section 8.4.3), gcd (gp, hp) =
1, Bezout‚Äôs identity (Proposition 8.2) tells us
that there are ùúÜ, ùúásuch that ùúÜgp + ùúáhp = 1.
Hence there are ÃÇùúÜ, ÃÇùúásuch that
ÃÇùúÜgp + ÃÇùúáhp ‚â°
fp2 ‚àígphp
p
(mod p).
(8.11)
The obvious choice is ÃÇùúÜ= ùúÜfp2 ‚àígphp‚àïp,
but this has too large a degree. However,
if we take the remainder (this is the same
idea as (8.6)) of this with respect to hp,
and similarly with ùúá, we obtain ÃÇùúÜ, ÃÇùúásatisfy-
ing (8.11) and with degÃÇùúÜ< deghp, degÃÇùúá<
deggp. Hence we can take gp2 = gp + pÃÇùúá,
hp2 = hp + pÃÇùúÜ.
We now suppose that fp3 = gp3hp3 where
we insist that gp3 corresponds to gp2, that
is, that they are equal modulo p2. We can
therefore write gp3 = gp2 + p2ÃÇgp3, and simi-
larly for the h‚Äôs. Then
fp3 = (gp2 + p2ÃÇgp3
) (
hp2 + p2 ÃÇhp3
)
,
so
fp3 ‚àígp2hp2
p2
‚â°gp ÃÇhp3 + hpÃÇgp3 (mod p) (8.12)

8.5 Solving Polynomial Systems
301
as before. Using the same Bezout‚Äôs identity
as before, we get that there are ÃÇÃÇùúÜ, ÃÇÃÇùúásuch
that
ÃÇÃÇùúÜgp + ÃÇÃÇùúáhp ‚â°
fp3 ‚àígp2hp2
p2
(mod p)
(8.13)
with
degÃÇÃÇùúÜ< deghp,
degÃÇÃÇùúá< deggp.
Hence
we
can
take
gp3 = gp2 + p2ÃÇÃÇùúá,
hp3 = hp2 + p2ÃÇÃÇùúÜ, and so on until we have
fpn = gpnhpn with pn greater than twice
the
Landau‚ÄìMignotte
bound
on
fac-
tors of f . Then, regarded as polynomials
with integer coeÔ¨Écients in the range
(‚àípn‚àï2, pn‚àï2), gpn and hpn should be the
factors of f .
However, as we saw in Example 8.1,
f might factor more modulo p than it
does over the integers. Nevertheless, all
factorizations of f over the integers must
correspond to factorizations, not neces-
sarily into irreducibles, modulo p. Hence
we need merely take all subsets of the
factors modulo p, and see if their product
corresponds to a factor over the integers.
In principle, this process is exponential
in the degree of f if it factors into low-
degree polynomials modulo p but is in
fact irreducible over the integers. In prac-
tice, it is possible to make the constant in
front of the exponential very small, and
to do better than looking at every subset
[41].
8.4.5
p-adic Algorithms‚ÄìPolynomial
It is also possible to use these methods in a
multivariate setting, going from a solution
modulo (y ‚àív) to one modulo (y ‚àív)2, then
modulo (y ‚àív)3, and so on, but the details
are beyond the scope of this chapter: see
[28, Section 5.8]
8.5
Solving Polynomial Systems
8.5.1
Solving One Polynomial
Consider the quadratic equation
ax2 + bx + c = 0.
(8.14)
The solutions are well known to most
schoolchildren: there are two of them, of
the form
x = ‚àíb ¬±
‚àö
b2 ‚àí4ac
2a
.
(8.15)
However, if b2 ‚àí4ac = 0, that is, c = b2‚àï4a
then there is only one solution: x = ‚àíb‚àï2a.
In
this
case,
the
equation
becomes
ax2 + bx + b2‚àï4a = 0, which can be rewrit-
ten as a (x + b‚àï2a)2 = 0, making it more
obvious that there is a repeated root, and
that the polynomial is not square-free
(DeÔ¨Ånition 8.1).
Mathematicians
dislike
the
sort
of
anomaly in ‚Äúthis equations has two solu-
tions except when c = b2‚àï4a,‚Äù especially as
there are two roots as c tends to the value
b2‚àï4a. We therefore say that, in this special
case, x = ‚àíb‚àï2a is a double root of the
equation. When we say we are counting the
roots of f with multiplicity, we mean that
x = ùõºshould be counted i times if (x ‚àíùõº)i
divides f .
Proposition 8.7 (Fundamental Theorem
of Algebra) The number of roots of a poly-
nomial equation over the complex numbers,
counted with multiplicity, is equal to the
degree of the polynomial.
There is a formula for the solutions of the
cubic equation
x3 + ax2 + bx + c = 0,
(8.16)

302
8 Computer Algebra
S
:=
T
:=
3
return
1
6 T
2b
T
‚àí
;
‚àí108 c + 12 S; 
12 b3 + 81 c2 ;
Figure 8.1
Program for computing solutions to a cubic.
albeit less well known to schoolchildren:
1
6
3‚àö
36 ba ‚àí108 c ‚àí8 a3 + 12
‚àö
12 b3 ‚àí3 b2a2 ‚àí54 bac + 81 c2 + 12 ca3
‚àí
2b ‚àí2
3a2
3‚àö
36 ba ‚àí108 c ‚àí8 a3 + 12
‚àö
12 b3 ‚àí3 b2a2 ‚àí54 bac + 81 c2 + 12 ca3
‚àí1
3a.
(8.17)
We can simplify this by making a transfor-
mation5) to (8.16): replacing x by x ‚àía‚àï3.
This transforms it into an equation
x3 + bx + c = 0
(8.18)
(where b and c have changed). This has
solutions of the form
1
6
3‚àö
‚àí108 c + 12
‚àö
12 b3 + 81 c2
‚àí
2b
3‚àö
‚àí108 c + 12
‚àö
12 b3 + 81 c2
.
(8.19)
Many texts (among those who discuss the
cubic at all) stop here, but in fact the com-
putational analysis of (8.19) is nontrivial. A
cubic has (Proposition 8.7) three roots, but
a na¬®ƒ±ve look at (8.19) shows two cube roots,
each with three values, and two square
roots, each with two values, apparently giv-
ing a total of 3 √ó 3 √ó 2 √ó 2 = 36 values. Even
if we decide that the two occurrences of
the square root should have the same sign,
and similarly the cube root should have the
same value, that is, we eÔ¨Äectively execute
the program in Figure 8.1, we would still
5) This is the simplest case of the Tschirnhaus
transformation [42], which can always eliminate
the xn‚àí1 term in a polynomial of degree n.
seem to have six possibilities. In fact, how-
ever, the choice in the Ô¨Årst line is only
apparent, because
1
6
3‚àö
‚àí108 c ‚àí12
‚àö
12 b3 + 81 c2
=
2b
3‚àö
‚àí108 c + 12
‚àö
12 b3 + 81 c2
. (8.20)
In the case of the quadratic with real coef-
Ô¨Åcients, there were two real solutions if b2 ‚àí
4ac > 0, and complex solutions otherwise.
However, the case of the cubic is more chal-
lenging. If we consider x3 ‚àí1 = 0, we com-
pute (in Figure 8.1)
S ‚à∂= 9;
T ‚à∂= 6;
return 1;
(or either of the complex cube roots of unity
if we choose diÔ¨Äerent values of T). If we
consider x3 + 1 = 0, we get
S ‚à∂= 9;
T ‚à∂= 0;
return ‚Äú 0
0‚Äù;
but we can (and must!) take advantage of
(8.20) and compute
S ‚à∂= ‚àí9;
T ‚à∂= ‚àí6;
return‚àí1;
(or either of the complex variants).

8.5 Solving Polynomial Systems
303
For x3 + x, we compute
S ‚à∂=
‚àö
12;
T ‚à∂=
‚àö
12;
return 0;
and the two complex roots come from
choosing the complex roots in the compu-
tation of T, which is really
3‚àö
12
‚àö
12. x3 ‚àíx
is more challenging: we compute
S ‚à∂=
‚àö
‚àí12;
T ‚à∂=
‚àö
‚àí12;
return {‚àí1, 0, 1};
(8.21)
that is, three real roots which can only
be computed (at least via this formula) by
means of complex numbers. In fact, it is
clear that any other formula must have the
same problem, because the only choices
of ambiguity lie in the square and cube
roots, and with the cube root, the ambiguity
involves complex cube roots of unity.
Hence even solving the humble cubic
makes three points.
1. The formulae (8.17) and (8.19) are
ambiguous, and a formulation such as
Figure 8.1 is to be preferred.
2. Even so we need to take care of 0‚àï0
issues.
3. Expressing real roots in terms of
radicals may need complex numbers.
These points are much clearer with the
quartic [28, Section 3.1.3]. Higher degrees
produce an even more fundamental issue.
Theorem 8.3 (Abel, Galois [43]) The gen-
eral polynomial equation of degree 5 or
more is not solvable in radicals (i.e. in terms
of kth roots).
Hence, in general, the only description is ‚Äúa
root of the polynomial x‚Ä¶ + ¬∑ ¬∑ ¬∑.‚Äù As (8.19),
and even more its quartic equivalent, is
nontrivial, it turns out to be preferable to
use this description for cubics and quartics
internally as well, only converting to radi-
cals on output (and possibly only if explic-
itly requested). See the example at (8.33),
and contrast it with (8.27).
8.5.2
Real Roots
While a polynomial of degree n always
has n roots (counted with multiplicity), it
usually6) has fewer real roots. While the
roots ùõºi of an (irreducible) polynomial f
are algebraically indistinguishable, because
they all share the property that f (ùõºi) = 0,
this ceases to be true once we start ask-
ing questions about real roots. f (x) = x2 ‚àí
r has real roots if, and only if, r is nonneg-
ative, so asking questions about the reality
of roots means that we can ask inequalities
as well. Conversely, inequalities only make
sense if we are talking about real, rather
than complex numbers.
Just as we have seen that we can, in gen-
eral, do no better than say ‚Äúa root of the
polynomial f (x) = xn + ¬∑ ¬∑ ¬∑,‚Äù we cannot do
much better than say ‚Äúa real root of the
polynomial f (x) = xn + ¬∑ ¬∑ ¬∑,‚Äù but we can at
least make it clear which of the roots we are
talking about. There are two ways of doing
this, which in practice we always apply to
square-free f .
1. ‚ÄúThe (unique) real root of f (x) between
a and b.‚Äù (a, b) is then known as an
isolating interval, and can be computed
by a variety of means: see [28,
Section 3.1.9]. Once we have an
isolating interval, standard techniques
6) Quite what meaning should be attached to
‚Äúusually‚Äù is surprisingly diÔ¨Écult. The ‚Äúobvious‚Äù
deÔ¨Ånitions would be ‚Äúnormally distributed‚Äù or
‚Äúuniformly distributed in some range,‚Äù and for
these Kac [44] shows that the average number
is (2‚àïùúã) log n. A deÔ¨Ånition with better geometric
invariance properties gives
‚àö
n(n + 2)‚àï3: very
diÔ¨Äerent [45].

304
8 Computer Algebra
of numerical analysis can reÔ¨Åne it to be
as small as we wish.
2. ‚ÄúThe (unique) x such that f (x) = 0 and
the derivatives of f satisfy the following
sign conditions at x.‚Äù It is a result known
as Thom‚Äôs lemma [46, Proposition 1.2]
that specifying the sign conditions
deÔ¨Ånes a root uniquely, if at all: see [28,
Section 3.1.10].
8.5.3
Linear Systems
The same algorithms we are familiar with
from numeric linear algebra are, in princi-
ple, applicable here. Hence, given a system
of linear equations, written in matrix form
as M‚ãÖx = b, we can (possibly with piv-
oting), apply Gaussian elimination (row
operations‚Äìadding a multiple of one row
to another) to get U‚ãÖx = b‚Ä≤, where u is
upper triangular, and do back substitution.
Similarly, we can compute the inverse of a
matrix.
The real problem with symbolic, as
opposed to numeric, linear algebra is not
that the algorithms produce the output
ineÔ¨Éciently so much as the fact that the
output is huge: the inverse of an n √ó n
matrix of numbers is an n √ó n matrix of
numbers, but the inverse of an n √ó n matrix
of small formulae may well be an n √ó n
matrix of very large formulae. While we are
all happy with
( a
b
c
d
)‚àí1
=
(
d
ad‚àíbc
‚àí
b
ad‚àíbc
‚àí
c
ad‚àíbc
a
ad‚àíbc
)
,
(8.22)
the reader is invited to experiment with
the equivalent for larger matrices. In fact,
the determinant of a generic n √ó n symbolic
matrix is the sum of n! terms, and this is the
denominator of a generic inverse, while the
numerators, of which there are n2, are the
sum of (n ‚àí1)! terms, giving a total of (n +
1)! summands. Hence our aim here should
be to avoid solving the generic problem,
and solve systems with as few variables as
possible. A further snag is that the inverse
of a sparse matrix tends to be denser, often
completely dense. For further details, see
[28, Section 3.2].
8.5.4
Multivariate Systems
If nonlinear polynomial equations in one
variable are more complicated than we
might think, and generic linear systems in
several variables are horrendous, we might
despair of nonlinear systems in several vari-
ables. While these might be diÔ¨Écult, they
are not without algorithmic approaches.
One major one,7) Gr√∂bner bases, is based
fundamentally on the sparse-distributed
representation.
In this representation, a nonzero poly-
nomial is represented as a sum of terms
‚àë
i cimi where the ci are nonzero coeÔ¨É-
cients and the mi are monomials, that is,
products of powers of the variables. As
mentioned earlier, when we asked whether
x2y came before or after xy3, a key ques-
tion is how we order the monomials. While
there are many options [28, Section 3.3.3],
three key ways of comparing A = xa1
1 ¬∑ ¬∑ ¬∑ xan
n
and B = xb1
1 ¬∑ ¬∑ ¬∑ xbn
n are the following.
Purely lexicographic ‚Äìplex . We Ô¨Årst
compare a1 and b1. If they diÔ¨Äer, this tells
us whether A > B (a1 > b1) or A < B
(a1 < b1). If they are the same, we go on
to look at a2 versus b2 and so on. The
order is similar to looking up words in a
dictionary/lexicon ‚Äìwe look at the Ô¨Årst
letter, and after Ô¨Ånding this, look at the
second letter, and so on. In this order, x2
is more important than xy10.
7) The other principal one, regular chains is intro-
duced in Section 8.5.6 and described in [28,
Section 3.4].

8.5 Solving Polynomial Systems
305
Total degree, then lexicographic
‚Äìgrlex. We Ô¨Årst look at the total
degrees
a = ‚àëai
and
b = ‚àëbi:
if
a > b, then A > B, and a < b means
A < B. If a = b, then we look at lexico-
graphic comparison. In this order xy10 is
more important than x2, and x2y more
important than xy2.
Total degree, then reverse lexicographic
‚Äìtdeg. This order is the same as the
previous, except that, if the total degrees
are equal, we look lexicographically,
then take the opposite. Many systems,
in particular Maple and Mathematica,
reverse the order of the variables Ô¨Årst.
The reader may ask ‚Äúif the order of the
variables is reversed, and we then reverse
the sense of the answer, what‚Äôs the diÔ¨Äer-
ence?‚Äù Indeed, for two variables, there is
no diÔ¨Äerence. However, with more vari-
ables it does indeed make a diÔ¨Äerence.
For three variables, the monomials of
degree three are ordered as
x3 > x2y > x2z > xy2 > xyz > xz2
> y3 > y2z > yz2 > z3
under grlex, but as
x3 > x2y > xy2 > y3 > x2z > xyz >
y2z > xz2 > yz2 > z3
under tdeg. One way of seeing the
diÔ¨Äerence is to say that grlex with
x > y > z discriminates in favor of x,
whereas tdeg with z > y > x discrimi-
nates against z. This metaphor reinforces
the fact that there is no diÔ¨Äerence with
two variables.
It seems that tdeg is, in general, the
most eÔ¨Écient order.
Once we have Ô¨Åxed such an order, we have
such concepts as the leading monomial
or leading term of a polynomial, denoted
lm(p) and lt(p). The coeÔ¨Écient of the lead-
ing term is the leading coeÔ¨Écient, denoted
lc(p).
8.5.5
Gr√∂bner Bases
Even though the equations are nonlinear,
Gaussian elimination is still available to us.
So, given the three equations
x2 ‚àíy = 0
x2 ‚àíz = 0
y + z = 0,
we can subtract the Ô¨Årst from the second
to get y ‚àíz = 0, hence (linear algebra on
this and the third) y = 0 and z = 0, and we
are left with x2 = 0, so x = 0, albeit with
multiplicity 2.
However, we can do more than this.
Given the two equations
x2 ‚àí1 = 0
and
xy ‚àí1 = 0,
(8.23)
there might seem to be no row operation
available. But in fact we can subtract x times
the second equation from y times the Ô¨Årst,
to get x ‚àíy = 0. Hence the solutions are x =
¬±1, y = x.
This might seem to imply that, given two
equations f = 0 and g = 0, we need to con-
sider replacing f = 0 by Ff + Gg = 0 for
arbitrary polynomials F and G. This would
be erroneous on two counts.
(a)
We do not need to consider arbitrary
polynomials F and G, just terms
(monomials
with
leading
coeÔ¨É-
cients). These terms might as well
be relatively prime, otherwise we
are introducing a spurious common
factor.
(b)
However, we must not replace f ,
because Ff + Gg might have zeros
that are not zeros of f and g: we need

306
8 Computer Algebra
to add Ff + Gg to the set of equations
under consideration.
DeÔ¨Ånition 8.2 This generalization of row
operations leads us to the following con-
cepts.
1. If lm(g) divides lm( f ), then we say that g
reduces f to h = lc(g)f ‚àí(lt( f )‚àïlm(g))g,
written f ‚Üíg h. Otherwise, we say that f
is reduced with respect to g. In this
construction of h, the leading terms of
both lc(g)f and (lt( f )‚àïlm(g))g are
lc( f )lc(g)lm( f ), and so cancel. Hence
lm(f ) comes before lm(h) in our order.
2. Any chain of reductions is Ô¨Ånite
(Dickson‚Äôs Lemma), so any chain
f1 ‚Üíg f2 ‚Üíg f3 ¬∑ ¬∑ ¬∑ terminates in a
polynomial h reduced with respect to g.
We write f1
‚àó‚Üí
g
h.
3. This extends to reduction by a set G of
polynomials, where f ‚ÜíG h means
‚àÉg ‚ààG ‚à∂f ‚Üíg h. We must note that a
polynomial can have several reductions
with respect to G (one for each element
of G whose leading monomial divides
the leading monomial of f ).
4. Let f , g ‚ààR[x1, ‚Ä¶ , xn]. The
S-polynomial of f and g, written S( f , g)
is deÔ¨Åned as
S(f , g) =
lt(g)
gcd (lm(f ), lm(g))f
‚àí
lt(f )
gcd (lm(f ), lm(g))g.
(8.24)
The computation after (8.23), y(x2 ‚àí1) ‚àí
x(xy ‚àí1) is in fact S(x2 ‚àí1, xy ‚àí1). Hence
we might want to think about comput-
ing S-polynomials, but then what about
S-polynomials of S-polynomials, and so
on? In fact, this is the right idea. We Ô¨Årst
note that the precise polynomials are
not particularly important: the zeros of
G = {f1, f2, ‚Ä¶} are not changed if we add
pf1 to G for any polynomial p, or f1 + f2,
and so on. Hence this deÔ¨Ånition.
DeÔ¨Ånition 8.3 Let S be a set of polynomials
in the variables x1, ‚Ä¶ , xn, with coeÔ¨Écients
from R. The ideal generated by S, denoted
(S), is the set of all Ô¨Ånite sums ‚àëfisi: si ‚àà
S, fi ‚ààR[x1, ‚Ä¶ , xn]. If S generates I, we say
that S is a basis for I.
What really matters is the (inÔ¨Ånite) ideal,
and not the particular Ô¨Ånite basis that gen-
erates it. Nevertheless, some bases are nicer
than others.
Theorem 8.4 [47,
Proposition
5.38,
Theorem 5.48] The following conditions
on a set G ‚ààR[x1, ‚Ä¶ , xn], with a Ô¨Åxed
ordering > on monomials, are equivalent.
1. ‚àÄf , g ‚ààG, S(f , g)
‚àó‚Üí
G
0.
2. If f
‚àó‚Üí
G
g1 and f
‚àó‚Üí
G
g2, then g1 and g2
diÔ¨Äer at most by a multiple in R, that is,
‚àó‚Üí
G
is essentially well deÔ¨Åned.
3. ‚àÄf ‚àà(G), f
‚àó‚Üí
G
0.
4. (lm(G)) = (lm((G))), that is, the leading
monomials of G generate the same ideal
as the leading monomials of the whole of
(G).
If G satisÔ¨Åes these conditions, G is called
a Gr√∂bner base (or standard basis).
These are very diÔ¨Äerent kinds of condi-
tions, and the strength of Gr√∂bner theory
lies in their interplay. Condition 2 under-
pins the others:
‚àó‚Üí
G
is well deÔ¨Åned. Con-
dition 1 looks technical, but has the great
advantage that, for Ô¨Ånite G, it is Ô¨Ånitely
checkable: if G has k elements, we take the
k(k ‚àí1)‚àï2 unordered pairs from G, com-
pute the S-polynomials, and check that they
reduce to zero. This gives us either a proof
or an explicit counterexample (which is the

8.5 Solving Polynomial Systems
307
key to the following algorithm). As f
‚àó‚Üí
G
0
means that f ‚àà(G), Condition 3 means that
ideal membership is testable if we have a
Gr√∂bner base for the ideal.
Algorithm 8.2 (Buchberger)
Input: Ô¨Ånite G0 ‚äÇR[x1, ‚Ä¶ , xn]; monomial
ordering >.
Output: G a Gr√∂bner base for (G0) with
respect to >.
G ‚à∂= G0; n ‚à∂= |G|;
# we consider G as {g1, ‚Ä¶ , gn}
P ‚à∂= {(i, j) ‚à∂1 ‚â§i < j ‚â§n}
while P ‚â†‚àÖdo
Pick (i, j) ‚ààP;
P ‚à∂= P ‚ßµ{(i, j)};
Let S(gi, gj)
‚àó‚Üí
G
h
If h ‚â†0 then
# lm(h) ‚àâ(lm(G))
gn+1 ‚à∂= h; G ‚à∂= G ‚à™{h};
P ‚à∂= P ‚à™{(i, n + 1) ‚à∂1 ‚â§i ‚â§n};
n ‚à∂= n + 1;
Given that, every time G grows, we add
more possible S-polynomials to P, it is
not obvious that this process terminates,
but it does ‚Äìessentially (lm(G)) cannot
grow
forever.
Estimating
the
running
time is a much harder problem. There
are doubly exponential (in the number of
variables) lower bounds on the degree of
the polynomials in a Gr√∂bner base [48].
In practice, it is very hard to estimate the
running time for a Gr√∂bner base calcula-
tion, and the author has frequently been
wrong by an order of magnitude ‚Äìin both
directions!
Proposition 8.8 Given a Gr√∂bner base G
with respect to any ordering, it has a Ô¨Ånite
number of solutions if, and only if, each
variable occurs alone (to some power) as
the leading monomial of one of the elements
of G. In this case, the number of solutions,
counted with multiplicity, is the number of
monomials irreducible under G
While a Gr√∂bner base can be computed
with respect to any order, a Gr√∂bner base
with respect to a purely lexicographical
order is particularly simple: essentially the
nonlinear equivalent of a triangular matrix.
If the system has only Ô¨Ånitely many solu-
tions,8) the Gianni‚ÄìKalkbrener algorithm
[[28], Section 3.3.7] provides an equiva-
lent of back substitution: solve the (unique)
polynomial in xn; for each root ùõºn solve
the lowest-degree polynomial in xn‚àí1, xn
whose leading coeÔ¨Écient does not vanish
at xn = ùõºn; for each root ùõºn‚àí1, solve the
lowest-degree polynomial in xn‚àí2, xn‚àí1, xn
whose leading coeÔ¨Écient does not van-
ish at xn‚àí1 = ùõºn‚àí1, xn = ùõºn, and so on. The
reader may be surprised that there can be a
choice of polynomials in xn‚àí1, xn, but con-
sider
G = {x2
1 ‚àí1, x2
2 ‚àí1, (x1 ‚àí1)(x2 ‚àí1)}.
(8.25)
We have two choices for ùõº2: 1 and ‚àí1.
When x2 = ‚àí1, we use the polynomial (x1 ‚àí
1)(x2 ‚àí1) and deduce x1 = 1. But when
x2 = 1, the leading coeÔ¨Écient (and in fact
all) of (x1 ‚àí1)(x2 ‚àí1) vanishes, and we are
forced to use x2
1 ‚àí1, which tells us that x1 =
¬±1.
Unfortunately, plex Gr√∂bner bases are
among the most expensive to compute.
Hence practitioners normally use the fol-
lowing process.
1. Compute a tdeg Gr√∂bner basis G1.
2. Check (Proposition 8.8) that it has only
Ô¨Ånitely many solutions.
3. Maybe give up if there are too many.
8) This condition is unfortunately necessary: see
[49] for an example.

308
8 Computer Algebra
4. Use the Faug√®re‚ÄìGianni‚ÄìLazard‚Äì
Mora algorithm [28, Section 3.3.8] to
convert G1 to a plex Gr√∂bner basis G2.
5. Apply the Gianni‚ÄìKalkbrener
Algorithm to G2.
8.5.6
Regular Chains
Although not (yet?) quite so well known as
Gr√∂bner bases, the theory of regular chains
[50], also known, slightly incorrectly, as tri-
angular sets, provides an alternative, based
instead on a (sparse) recursive view of poly-
nomials. The fundamental algorithm takes
as input a Ô¨Ånite set S of polynomials, and
rather than returning one Gr√∂bner basis,
returns a Ô¨Ånite set of Ô¨Ånite sets {S1, ‚Ä¶ , Sk}
such that
1. (ùõº1, ‚Ä¶ , ùõºn) is a solution of S if, and only
if, it is a solution of some Si.
2. Each Si is triangular and a regular
chain, which in particular means it can
be solved by straightforward back
substitution.
Applied to (8.25), this would produce two
regular chains: (x2
1 ‚àí1, x2 ‚àí1) and (x1 ‚àí
1, x2 + 1), from which the solutions can
indeed be read oÔ¨Ä.
Among widespread systems, this theory
is currently only implemented in Maple.
It has two advantages over the Gr√∂bner
basis approach: it produces triangular, that
is, easy to understand systems even when
there are inÔ¨Ånitely many solutions, and
it can be adapted to looking just for real
solutions [51].
It does have its drawbacks, the most
signiÔ¨Åcant of which is probably that there
is no guarantee that the roots of Si and Sj
are disjoint.
8.6
Integration
We have previously (Section 8.3.3) made
use of the concept of f ‚Ä≤, the derivative of
f . The reader may object that this is a con-
struct of calculus, not of algebra. However,
it is possible to deÔ¨Åne diÔ¨Äerentiation (which
we will regard as being with respect to x)
purely algebraically.
DeÔ¨Ånition 8.4 A map‚Ä≤ ‚à∂K ‚ÜíK is a diÔ¨Äer-
entiation (with respect to x) if it satisÔ¨Åes
1. (f + g)‚Ä≤ = f ‚Ä≤ + g‚Ä≤;
2. (fg)‚Ä≤ = fg‚Ä≤ + f ‚Ä≤g;
3. x‚Ä≤ = 1.
It follows from these that
(a) 0‚Ä≤ = 0 by expanding (x + 0)‚Ä≤ = x‚Ä≤;
(b) 1‚Ä≤ = 0 by expanding (1 ‚ãÖx)‚Ä≤ = x‚Ä≤
(c)
(
p
q
)‚Ä≤
= p‚Ä≤q‚àípq‚Ä≤
q2
by expanding
(
q ‚ãÖ
(
p
q
))‚Ä≤
= p‚Ä≤ according to 2.
From this point of view, indeÔ¨Ånite9) inte-
gration is simply anti-diÔ¨Äerentiation:
solve ‚à´f dx = F ‚áîÔ¨Ånd F such that F‚Ä≤ = f .
(8.26)
It is clear that F is only determined up to
adding a constant c, that is, c‚Ä≤ = 0.
8.6.1
Rational Functions
When faced with ‚à´(8x7‚àï(x8 + 1))dx, most
of us would spot that this is ‚à´(f ‚Ä≤‚àïf )dx, with
log f as the answer. But we would strug-
gle with ‚à´((8x7 + 1)‚àï(x8 + 1))dx, whose
answer is
9) For deÔ¨Ånite integration, see Section 8.7.1.

8.6 Integration
309
(
1
16
‚àö
2 +
‚àö
2 +
i
16
‚àö
2 ‚àí
‚àö
2 + 1
)
log
(
x + 1
2
‚àö
2 +
‚àö
2 + i
2
‚àö
2 ‚àí
‚àö
2
)
+
(
1
16
‚àö
2 ‚àí
‚àö
2 +
i
16
‚àö
2 +
‚àö
2 + 1
)
log
(
x + 1
2
‚àö
2 ‚àí
‚àö
2 + i
2
‚àö
2 +
‚àö
2
)
+
(
‚àí1
16
‚àö
2 ‚àí
‚àö
2 +
i
16
‚àö
2 +
‚àö
2 + 1
)
log
(
x ‚àí1
2
‚àö
2 ‚àí
‚àö
2 + i
2
‚àö
2 +
‚àö
2
)
+
(
‚àí1
16
‚àö
2 +
‚àö
2 +
i
16
‚àö
2 ‚àí
‚àö
2 + 1
)
log
(
x ‚àí1
2
‚àö
2 +
‚àö
2 + i
2
‚àö
2 ‚àí
‚àö
2
)
+
(
‚àí1
16
‚àö
2 +
‚àö
2 ‚àí
i
16
‚àö
2 ‚àí
‚àö
2 + 1
)
log
(
x ‚àí1
2
‚àö
2 +
‚àö
2 ‚àíi
2
‚àö
2 ‚àí
‚àö
2
)
+
(
‚àí1
16
‚àö
2 ‚àí
‚àö
2 ‚àí
i
16
‚àö
2 +
‚àö
2 + 1
)
log
(
x ‚àí1
2
‚àö
2 ‚àí
‚àö
2 ‚àíi
2
‚àö
2 +
‚àö
2
)
+
(
1
16
‚àö
2 ‚àí
‚àö
2 ‚àí
i
16
‚àö
2 +
‚àö
2 + 1
)
log
(
x + 1
2
‚àö
2 ‚àí
‚àö
2 ‚àíi
2
‚àö
2 +
‚àö
2
)
+
(
1
16
‚àö
2 +
‚àö
2 ‚àí
i
16
‚àö
2 ‚àí
‚àö
2 + 1
)
log
(
x + 1
2
‚àö
2 +
‚àö
2 ‚àíi
2
‚àö
2 ‚àí
‚àö
2
)
.
(8.27)
It seems clear that the sort of pattern-
matching
we
have
seen,
and
used
eÔ¨Äectively in school, will not scale to
this level.10)
If we look at the integration of rational
functions ‚à´(q(x)‚àïr(x))dx with degq < degr
(else we have a polynomial part which is
trivial to integrate), we can conceptually
1. perform a square-free decomposition
(DeÔ¨Ånition 8.1) of r = ‚àèn
i=1 ri
i;
2. factorize each ri completely, as
ri(x) = ‚àèni
j=1(x ‚àíùõºi,j), where in general
the ùõºi,j will be RootOf constructs;
3. perform a partial fraction
decomposition (8.6) of q‚àïr as
q
r =
q
‚àèn
i=1 ri
i
=
n
‚àë
i=1
qi
ri
i
=
n
‚àë
i=1
ni
‚àë
j=i
j‚àë
k=1
ùõΩi,j,k
(x ‚àíùõºi,j)k ;
(8.28)
4. integrate this term by term, obtaining
10) However, intelligent transformations do have a
r√¥le to play in producing good human-readable
answers: see [52].
‚à´
q
r =
n
‚àë
i=1
ni
‚àë
j=i
j‚àë
k=2
‚àíùõΩi,j,k
(k ‚àí1)(x ‚àíùõºi,j)k‚àí1
+
n
‚àë
i=1
ni
‚àë
j=i
ùõΩi,j,1log(x ‚àíùõºi,j).
(8.29)
This is not a good idea computa-
tionally, as it would solve the trivial
‚à´((8x7)‚àï(x8 + 1))dx the same way as (8.27),
only to see all the logarithms combine at
the end, but shows us what the general
form of the answer has to be‚Äìa rational
function plus a sum of logarithms with
constant coeÔ¨Écients We may need to
introduce as algebraic numbers all the
roots of the denominator, but no more
algebraic numbers than that. The denom-
inator of the rational function is going to
have11) the same factors as the original r,
but with multiplicity reduced by one, i.e.
‚àèrni‚àí1
i
. This is, in fact, R ‚à∂= gcd (r, r‚Ä≤) and
11) Strictly speaking, this argument only proves
that the denominator is at most this big. But it
must be at least this big, else its derivative, the
integrand, would not have the denominator it
has.

310
8 Computer Algebra
so could be computed without the explicit
factorization of step 2 above.
It can be shown, either by Galois theory
or algorithmically, that, in fact, the numera-
tor of the rational part of the integral can be
written without any of the algebraic num-
bers we conceptually added in step 2 either.
Hence
‚à´
q
r = Q
R +
n
‚àë
i=1
ni
‚àë
j=i
ùõΩi,j,1log(x ‚àíùõºi,j). (8.30)
DiÔ¨Äerentiating this shows that
( n
‚àë
i=1
ni
‚àë
j=i
ùõΩi,j,1log(x ‚àíùõºi,j)
)‚Ä≤
is also a rational function not needing any
of the algebraic numbers we conceptually
added in step 2: call it S‚àï(r‚àïR), but we
should note that this time we have not
proved that the denominator in lowest
terms is exactly r‚àïR, and indeed if there are
no logarithmic terms (all ùõΩi,j,1 = 0), we may
have denominator just 1.
DiÔ¨Äerentiating (8.30) gives us
q
r =
(Q
R
)‚Ä≤
+
S
r‚àïR =
Q‚Ä≤(r‚àïR) ‚àíQ R‚Ä≤r
R2 + SR
r
,
(8.31)
and if we write Q and S as polynomials
in x with unknown coeÔ¨Écients, (8.31)
gives us a set of linear equations for
these unknowns‚Äìa method known as
the
Horowitz‚ÄìOstrogradski
algorithm
[53‚Äì55].
Having found Q‚àïR comparatively easily,
we are left with ‚à´(S‚àï(r‚àïR))dx, whose inte-
gral may or may not involve individual roots
of r‚àïR‚Äìsee the discussion around (8.27).
Hence the real challenge is to integrate
(S‚àï(r‚àïR)) without adding any unnecessary
algebraic numbers. This was solved, and in
a very satisfactory manner, independently
in [56, 57]. It produces a set of polynomi-
als Qi, all of whose roots are necessary for
the expression of ‚à´(S‚àï(r‚àïR))dx in terms of
logarithms, and such that the integral is
‚àë
i
‚àë
ùúå=RootOf(Qi)
gcd (S ‚àíùúå( r
R)‚Ä≤, r
R).
(8.32)
Applying this algorithm to (8.27), we actu-
ally get
1
8
‚àë
ùúå= RootOf(ùõº8 ‚àí64 ùõº7 + 1792 ùõº6
‚àí28672 ùõº5 + 286720 ùõº4 ‚àí1835008 ùõº3
+ 7340032 ùõº2 ‚àí16777216 ùõº+ 16777217)
ùúålog(x + ùúå+ 8),
(8.33)
and (8.27) is the result of forcing a con-
version of the RootOf into radicals: the
polynomial is in fact (ùõº‚àí8)8 + 1, another
example of a small expression that is not
small in either the dense or the sparse
model.
8.6.2
More Complicated Functions
Equation (8.29) shows that every ratio-
nal function has an integral that can be
expressed as a rational function plus a sum
of logarithms with constant coeÔ¨Écients.
Conversely, we are used in calculus to
statements like
e‚àíx2 has no integral,
(8.34)
which is nonsense as a statement of analy-
sis, let alone numerical computation, and is
really the algebraic statement
there is no formula f (x)
such that f ‚Ä≤(x) = e‚àíx2.
(8.35)
Again, this is not correct, as Maple will tell
us
‚à´e‚àíx2dx = 1
2
‚àö
ùúãerf (x).
(8.36)

8.6 Integration
311
The key point really is that this statement is
an oxymoron, because the deÔ¨Ånition of erf
is
(erf(x))‚Ä≤ =
‚àö
4
ùúãe‚àíx2;
erf(0) = 0.
(8.37)
Equation (8.36) is qualitatively diÔ¨Äerent
from
‚à´x3e‚àíx2dx = ‚àí1
2
(x2 + 1
)
e‚àíx2,
(8.38)
which expresses an integral in terms of
things ‚Äúwe already know about.‚Äù Hence the
trick is formalizing that last phrase. Just as
DeÔ¨Ånition 8.4 provided a purely algebraic
deÔ¨Ånition of diÔ¨Äerentiation, we can do the
same with functions normally thought of as
deÔ¨Åned by calculus.
DeÔ¨Ånition 8.5 We say that the abstract
symbol ùúÉis
‚Ä¢ a logarithm of u if ùúÉ‚Ä≤ = u‚Ä≤‚àïu;
‚Ä¢ an exponential of u if ùúÉ‚Ä≤ = u‚Ä≤ùúÉ.
DeÔ¨Ånition 8.6 We say that C(x, ùúÉ1, ‚Ä¶ , ùúÉn)
is a Ô¨Åeld of elementary functions if each ùúÉi
is
1. algebraic over C(x, ùúÉ1, ‚Ä¶ , ùúÉi‚àí1);
2. a logarithm of u ‚ààC(x, ùúÉ1, ‚Ä¶ , ùúÉi‚àí1);
3. an exponential of u ‚ààC(x, ùúÉ1, ‚Ä¶ , ùúÉi‚àí1).
As sin(x) = (1‚àï2i) (exp(ix) ‚àíexp(‚àíix)) and
arcsin(x) = ‚àíi log(
‚àö
1 ‚àíx2 + ix), and so on,
the trigonometric and inverse trigonomet-
ric functions are also included in this class.
The textbook ‚Äú‚à´(1‚àï(1 + x2))dx = arctan x‚Äù
is really
‚Äú ‚à´
1
1 + x2 dx = i
2 log
(1 ‚àíix
1 + ix
)
.‚Äù
(8.39)
We can now make (8.35) precise.
There is no elementary Ô¨Åeld containing a
u with u‚Ä≤ = exp(‚àíx2).
(8.35‚Ä≤)
How does one, or indeed a computer,
prove such a result? And yet computers
can and do: when Maple, say, responds
to ‚à´exp(log4(x))dx by just echoing back
the formula, it is not merely saying ‚ÄúI
couldn‚Äôt Ô¨Ånd an integral,‚Äù it is also asserting
that it has a proof that the integral is not
elementary.
The key result is a major generalization of
(8.29).
Theorem 8.5 (Liouville‚Äôs Principle) If
u ‚ààC(x, ùúÉ1, ‚Ä¶ , ùúÉn) has an elementary inte-
gral v in some larger Ô¨Åeld C(x, ùúÉ1, ‚Ä¶ , ùúÉm), it
is possible to write
‚à´u = v = w +
k‚àë
i=1
ci log wi,
(8.40)
where w and the wi ‚ààC(x, ùúÉ1, ‚Ä¶ , ùúÉn).
The proof, while somewhat technical, is
based on the facts that diÔ¨Äerentiation can-
not eliminate new exponentials, or new
algebraics, and can only eliminate a new
logarithm if it has a constant coeÔ¨Écient. If
u itself is elementary, it is possible to go fur-
ther and produce an algorithm that will Ô¨Ånd
v, or prove that no such elementary v exists
[58‚Äì60].
8.6.3
Linear Ordinary DiÔ¨Äerential Equations
If we actually apply the algorithm we have
just stated exists, it will state that, if elemen-
tary,
‚à´exp(‚àíx2)dx = w(x) exp(‚àíx2),
and
hence, equating coeÔ¨Écients of exp(‚àíx2)
in
exp(‚àíx2) = (w(x) exp(‚àíx2))‚Ä≤,
that
1 = w‚Ä≤ ‚àí2xw. The algorithm thus also has
to solve this linear diÔ¨Äerential equation,
and prove that no rational function can
solve it.
In fact, the method of integrating factors
shows that there is a complete equivalence

312
8 Computer Algebra
between integration and solving Ô¨Årst-order
linear diÔ¨Äerential equations. We can extend
DeÔ¨Ånition 8.6 to allow arbitrary integrals,
and therefore solutions of linear Ô¨Årst-order
diÔ¨Äerential equations, in the class of allow-
able functions.
DeÔ¨Ånition 8.7 We say that C(x, ùúÉ1, ‚Ä¶ , ùúÉn)
is a Ô¨Åeld of Liouvillian functions if each ùúÉi is
1. algebraic over C(x, ùúÉ1, ‚Ä¶ , ùúÉi‚àí1);
2. an integral of u ‚ààC(x, ùúÉ1, ‚Ä¶ , ùúÉi‚àí1),that
is, ùúÉ‚Ä≤
i = u;
3. an exponential of u ‚ààC(x, ùúÉ1, ‚Ä¶ , ùúÉi‚àí1).
We can then ask whether second- or
higher-order diÔ¨Äerential equations have
solutions in terms of Liouvillian functions:
a question solved in [61, 62].
8.7
Interpreting Formulae as Functions
The previous section treated exp(x) and
log(x) as abstract symbolic expressions, and
attached no meaning to exp and log as
functions R ‚ÜíR or R+ ‚ÜíR. Similarly, we
have regarded
‚àö
2 as a number ùõºwith ùõº2 =
2, have ignored the inconvenient fact that
there are two such numbers, and attached
no meaning to
‚àö
as a function R+ ‚ÜíR.
These interpretation questions are looked
at in the next two subsections.
8.7.1
Fundamental Theorem of Calculus
Revisited
DeÔ¨Ånitions 8.4 and 8.5 deÔ¨Åned diÔ¨Äerentia-
tion of formulae in a purely algebraic way,
and therefore (8.26) seemed to reduce the
problem of integration to that of reversing
diÔ¨Äerentiation. From the algebraic point
of view, this is correct. From the analytic
point of view, though, there is some-
thing to prove, and that proof imposes
side-conditions.
Theorem 8.6 (Fundamental Theorem of
Calculus, e.g., [Section 5.3, 63]) Let f and
F be functions deÔ¨Åned on a closed inter-
val [a, b] such that F‚Ä≤ = f . If f is Riemann-
integrable on [a, b], then
‚à´
b
a
f (x)dx = F(b) ‚àíF(a) written [F]b
a .
Though this is the classical statement, we
emphasize that F‚Ä≤ = f must hold through-
out [a, b], and therefore F is diÔ¨Äerentiable,
hence continuous, throughout this interval.
The condition about f being Riemann-
integrable is necessary to prevent calcula-
tions such as
‚à´
1
‚àí1
1
x3 dx
?=
[ ‚àí1
2x2
]1
‚àí1 = ‚àí1
2 ‚àí‚àí1
2 = 0,
(8.41)
whereas, in fact, both ‚à´0
‚àí1(1‚àï(x3))dx and
‚à´1
0 (1‚àï(x3))dx are undeÔ¨Åned.
The warning about continuity would also
cover this case, as ‚àí1‚àï2x2 is not continu-
ous, and might otherwise seem unneces-
sary: after all, are not exp and log contin-
uous?
For a Ô¨Årst example, consider
‚à´
1
2 x2 ‚àí6 x + 5dx
=
{
F1
‚à∂=
‚àíarctan
(
x‚àí1
x‚àí2
)
F2
‚à∂=
arctan (2 x ‚àí3)
,
(8.42)
where arctan‚Ä≤(x) = 1‚àï(1 + x2). From the
point of view of Section 8.6, both are
equally valid; F1 and F2 both diÔ¨Äerentiate
to 1‚àï(2 x2 ‚àí6 x + 5). However, there is a
big diÔ¨Äerence from the point of view of
Theorem 8.6: F1 is discontinuous at x = 2,
while F2 is not, as seen in Figure 8.2, which

8.7 Interpreting Formulae as Functions
313
0
0.5
1.5
1
1
2
z
3
4
‚àí1.5
‚àí0.5
‚àí1
Figure 8.2
F1 and F2 from
(8.42).
also shows a ‚Äúconstant‚Äù diÔ¨Äerence as well.
In fact [F2
]3
1 = arctan(3) + ùúã‚àï4, whereas
[F1
]3
1 = ‚àíarctan(2), a silly result for the
integral of a positive function. In fact, the
two diÔ¨Äer by ùúã, which is the magnitude of
the discontinuity in arctan ‚Äúat inÔ¨Ånity,‚Äù12)
that is, when x = 2 in F2. While F1 and F2
both diÔ¨Äerentiate correctly at most values
of x, F1 is diÔ¨Äerentiable at x = 2, and there-
fore satisÔ¨Åes the diÔ¨Äerentiability hypothesis
of Theorem 8.6, while F2 is not continuous,
and therefore not diÔ¨Äerentiable, at x = 2,
and does not satisfy the hypothesis.
As arctan is built from log, we must ques-
tion our bland assertion that ‚Äúlog is con-
tinuous.‚Äù In fact, while it is continuous as a
function R+ ‚ÜíR, it is not continuous C ‚Üí
C, having a branch cut on the negative real
axis.13) As x goes from ‚àí‚àûto ‚àû, the argu-
ment of the logarithm in (8.39) goes from
12) limx‚Üí‚àûarctan(x) = ùúã‚àï2, whereas
limx‚Üí‚àí‚àûarctan(x) = ‚àíùúã‚àï2.
13) This is the conventional location of the branch
cut these days: see [64].
‚àí1 clockwise round the unit circle, passing
through 1 when x = 0 and arriving back at
‚àí1, but from the other side of the branch
cut from that from which it departed.
Hence, while the methods of Section 8.6
can produce formulae that diÔ¨Äerentiate
(in the sense of DeÔ¨Ånitions 8.4 and 8.5)
correctly to the formulae being integrated,
interpreting these formulae as functions,
and doing deÔ¨Ånite integration, requires us
to check that these formulae actually deÔ¨Åne
continuous functions over the relevant
range.
8.7.2
SimpliÔ¨Åcation of Functions
As
it
is
possible
to
deÔ¨Åne
‚àö
x =
exp (1‚àï2 log(x)), it follows that this func-
tion has a branch cut in the same place as
log, traditionally along the negative real
axis. As with log, this function is continu-
ous R+ ‚ÜíR+, but is not continuous C ‚ÜíC.
We write R+ ‚ÜíR+ because, as R+ ‚ÜíR,

314
8 Computer Algebra
‚àö
is ambiguous: is
‚àö
4 equal to 2 or ‚àí2?
In fact, it is also ambiguous C ‚ÜíC, and
we normally consider
‚àö
‚à∂C ‚ÜíC+, where
C+ = {x + iy ‚à∂x > 0 ‚à®(x = 0 ‚àßy ‚â•0)},
the ‚Äúpositive half-plane.‚Äù
This resolves ambiguity, but does not
solve
all
our
problems.
The
putative
equation
‚àö
x‚àöy
?= ‚àöxy,
(8.43)
which is true for
‚àö
‚à∂R+ ‚ÜíR+, fails
for
‚àö
‚à∂C ‚ÜíC+:
consider
x = y = ‚àí1.
Manipulating such putative identities is
dangerous, and most computer algebra
systems will, by default, refuse to do this:
for
example
Maple‚Äôs
simplify
does
not, but simplify(...,symbolic)
(sometimes) will. However, just because
(8.43) is not universally true does not mean
that some relatives are not true. Consider
the apparently similar
‚àö
z ‚àí1
‚àö
z + 1
?=
‚àö
z2 ‚àí1
(8.44)
‚àö
1 ‚àíz
‚àö
1 + z
?=
‚àö
1 ‚àíz2.
(8.45)
Equation (8.45) is in fact true through-
out C, whereas (8.44) is only true on
C+ ‚à™[‚àí1, 0]. The diÔ¨Äerence is that the
branch cuts of (8.44) include the imaginary
axis, and divide C into distinct regions,
whereas the branch cuts of (8.45) do not
separate the complex plane. The appar-
ently bizarre fact that (8.44) is also valid
on [‚àí1, 0] is due to the fact that multiple
branch cuts include this, and their eÔ¨Äects
cancel. Equally, though the components of
(8.45) have branch cuts, they lie on top of
each other, and the eÔ¨Äects cancel, meaning
that (8.45) itself has only a spurious cut
[65]. See [66, 67] for the general theory,
though it has to be admitted that imple-
mentations in computer algebra systems
are few.
These problems are not conÔ¨Åned to
‚àö
.
log
is
also
not
uniquely
deÔ¨Åned
C ‚ÜíC, because 1 = exp(0) = exp(2ùúãi) =
exp(4ùúãi) = ‚Ä¶
Many
mathematicians
are content to live with this ambiguity,
which is of course anathema to computer
programmers ‚Äìsee [64].
log z1 + log z2
?= log z1z2.
(8.46)
The equation merely states that the
sum of one of the (inÔ¨Ånitely many) log-
arithms of z1 and one of the (inÔ¨Ånitely
many) logarithms of z2 can be found
among the (inÔ¨Ånitely many) logarithms
of z1z2, and conversely every logarithm
of z1z2 can be represented as a sum
of this kind (with a suitable choice of
log z1 and log z2).
[68, pp. 259‚Äì260] (our notation)
It is more normal to use capital letters
to denote the multivalued functions, so
that log(z) = {log(z) + 2kùúãi ‚à∂k ‚ààZ}, and
instead of (8.46) to write
logz1 + logz2 = logz1z2,
(8.47)
interpreting ‚Äú+‚Äù as elementwise addition of
sets. With this notation, log(1‚àïz) = ‚àílog(z),
but the single-valued equivalent log(1‚àïz)
?=
‚àílog(z) is true except on the negative real
axis, where the branch cuts do not can-
cel. Again, it is fair to say that implementa-
tions in computer algebra systems lag sig-
niÔ¨Åcantly behind the theory.
8.7.3
Real Problems
It would be tempting to dismiss these issues
as only aÔ¨Äecting complex numbers. This is
not true: consider the often-quoted iden-
tity

References
315
arctan(x) + arctan(y) = arctan
( x + y
1 ‚àíyx
)
.
(8.48)
It is certainly true at x = y = 0, and
the partial derivatives of the two sides
are equal, so we might be tempted to
conclude that it is true everywhere ‚Äúby
analytic
continuation.‚Äù
However,
when
x = y = 2, the left-hand side is positive and
the right-hand side negative, so something
is clearly wrong. The problem is the same
‚Äúdiscontinuity at inÔ¨Ånity‚Äù of arctan, that is,
when xy = 1, as we saw in Figure 8.2.
In fact, the correct version of (8.48) is
(8.49):
arctan(x) + arctan(y)
= arctan
( x + y
1 ‚àíyx
)
+
‚éß
‚é™
‚é®
‚é™‚é©
‚àíùúã
xy > 1; x < 0
0
< 1
ùúã
xy > 1; x > 0
.
(8.49)
8.8
Conclusion
We have seen various examples of where
the ad hoc methods we have generally
learnt at school, and which process small-
ish examples well, can be converted into
algorithms for doing algebraic manipula-
tion, be it gcd computations (Section 8.4),
factoring
polynomials
(Section 8.4.4),
solving
nonlinear
polynomial
systems
(Section 8.5.5),
or
indeÔ¨Ånite
integra-
tion
(Section 8.6).
Equally,
we
have
seen that this is not always possible
(Theorem 8.3).
Section 8.7 also reminds us that there
is more to mathematics than just algebra,
and that we need to be careful when inter-
preting algebraic objects as actual functions
C ‚ÜíC, or even R ‚ÜíR, as in (8.49).
References
1. IEEE (1985) IEEE Standard 754 for Binary
Floating-Point Arithmetic, IEEE.
2. Baker, A. (1975) Transcendental Number
Theory, Cambridge University Press.
3. Silverman, J.H. and Tate, J. (1992) Rational
Points on Elliptic Curves, Springer-Verlag.
4. Kahrimanian, H.G. (1953) Analytic
diÔ¨Äerentiation by a digital computer.
Master‚Äôs thesis, Temple University
Philadelphia.
5. Nolan, J. (1953) Analytic diÔ¨Äerentiation on a
digital computer. Master‚Äôs thesis,
Mathematics Department M.I.T.
6. Haselgrove, C.B. (1953) Implementations of
the Todd-Coxeter algorithm on EDSAC-1,
Unpublished.
7. Collins, G.E. (1966) PM, a system for
polynomial multiplication. Commun. ACM,
9, 578‚Äì589.
8. Martin, W.A. (1967) Symbolic mathematical
laboratory. PhD thesis, M.I.T. & Project
MAC TR-36.
9. Moses, J. (2010) Macsyma: a personal
history. J. Symb. Comput., 47, 123‚Äì130.
10. Fitch, J.P. (1974) CAMAL Users Manual,
University of Cambridge Computer
Laboratory.
11. Hearn, A.C. (1973) REDUCE-2 User‚Äôs
Manual. Technical Report UCP-19,
Computational Physics Group University of
Utah.
12. Cannon, J.J. (1974) A general purpose group
theory program. Proceedings 2nd
International Conference on Theory of
Groups, pp. 204‚Äì217.
13. Bosma, W., Cannon, J., and Playoust, C.
(1997) The Magma algebra system. I: the user
language. J. Symb. Comput., 24, 235‚Äì265.
14. Giovini, A. and Niesi, G. (1990) CoCoA: a
user-friendly system for commutative
algebra. Proceedings DISCO ‚Äô90, Springer,
pp. 20‚Äì29.
15. Abbott, J.A. (2004) CoCoA: a laboratory for
computations in commutative algebra. ACM
SIGSAM Bull. 1, 38, 18‚Äì19.
16. Greuel, G.-M., PÔ¨Åster, G., and Sch√∂nemann,
H. (2001) SINGULAR ‚Äìa computer algebra
system for polynomial computations, in
Proceedings Calculemus 2000 (eds M. Kerber
and M. Kohlhase), A.K. Peters, Boston Mass,
pp. 227‚Äì234.

316
8 Computer Algebra
17. Bayer, D. and Stillman, M. (1986) The design
of Macaulay: a system for computing in
algebraic geometry and commutative
algebra. Proceedings SYMSAC 86, pp.
157‚Äì162.
18. Grayson, D. and Stillman, M. (2009)
Macaulay2, a software system for research in
algebraic geometry,
http://www.math.uiuc.edu/Macaulay2/
(accessed on 25 February 2014).
19. Karatsuba, A. and Ofman, J. (1963)
Multiplication of multidigit numbers on
automata. Sov. Phys. Dokl., 7, 595‚Äì596.
20. Sch√∂nhage, A. and Strassen, V. (1971)
Schnelle Multiplikation gro√üer Zahlen.
Computing, 7, 282‚Äì292.
21. Johnson, S.C. (1974) Sparse polynomial
arithmetic. Proceedings EUROSAM 74, pp.
63‚Äì71.
22. Yan, T. (1998) The geobucket data structure
for polynomials. J. Symb. Comput., 25,
285‚Äì294.
23. Char, B.W., Geddes, K.O., Gentleman, M.W.,
and Gonnet, G.H. (1983) The design of
MAPLE: a compact, portable and powerful
computer algebra system. Proceedings
EUROCAL 83, pp. 101‚Äì115.
24. Davenport, J.H. and Carette, J. (2010) The
sparsity challenges, in Proceedings SYNASC
2009 (eds S. Watt et al.), IEEE Press, pp. 3‚Äì7.
25. CastaÀúno, B., Heintz, J., Llovet, J., and
Mart¬¥ƒ±nez, R. (2000) On the data structure
straight-line program and its
implementation in symbolic computation.
Math. Comput. Simulat., 51, 497‚Äì528.
26. Schinzel, A. (2003) On the greatest common
divisor of two univariate polynomials, I. A
Panorama of Number Theory or the View
from Baker‚Äôs Garden, pp. 337‚Äì352.
27. Plaisted, D.A. (1977) Sparse complex
polynomials and irreducibility. J. Comput.
Syst. Sci., 14, 210‚Äì221.
28. Davenport, J.H. (2015) Computer algebra, To
be published by C.U.P. in 2014.
29. Zippel, R.E. (1993) EÔ¨Äective Polynomial
Computation, Kluwer Academic Publishers.
30. Karpinski, M. and Shparlinski, I. (1999) On
the computational hardness of testing
square-freeness of sparse polynomials, in
Proceedings AAECC-13 (eds M. Fossorier, H.
Imai, S. Lin, and A. Poli), Springer,
pp. 492‚Äì497.
31. Landau, E. (1905) Sur Quelques Th√©or√®mes
de M. Petrovic Relatif aux Z√©ros des
Fonctions Analytiques. Bull. Soc. Math.
France, 33, 251‚Äì261.
32. Mignotte, M. (1974) An inequality about
factors of polynomials. Math. Comput., 28,
1153‚Äì1157.
33. Mignotte, M. (1981) Some inequalities about
univariate polynomials. Proceedings
SYMSAC 81, pp. 195‚Äì199.
34. Brown, W.S. (1971) On Euclid‚Äôs algorithm
and the computation of polynomial greatest
common divisors. J. ACM, 18, 478‚Äì504.
35. Berlekamp, E.R. (1967) Factoring
polynomials over Ô¨Ånite Ô¨Åelds. Bell Syst. Tech.
J., 46, 1853‚Äì1859.
36. Berlekamp, E.R. (1970) Factoring
polynomials over large Ô¨Ånite Ô¨Åelds. Math.
Comput., 24, 713‚Äì735.
37. Cantor, D.G. and Zassenhaus, H. (1981) A
new algorithm for factoring polynomials
over Ô¨Ånite Ô¨Åelds. Math. Comput., 36,
587‚Äì592.
38. Swinnerton-Dyer, H.P.F. (1970) Letter to
E.H. Berlekamp. Mentioned in [36].
39. Kaltofen, E., Musser, D.R., and Saunders,
B.D. (1983) A generalized class of
polynomials that are hard to factor. SIAM J.
Comput., 12, 473‚Äì483.
40. Abbott, J.A., Bradford, R.J., and Davenport,
J.H. (1985) A remark on factorisation.
SIGSAM Bull. 2, 19, 31‚Äì33.
41. Abbott, J.A., Shoup, V., and Zimmermann, P.
(2000) Factorization in ‚Ñ§[x]: the searching
phase, in Proceedings ISSAC 2000 (ed. C.
Traverso), ACM, New York, pp. 1‚Äì7.
42. von Tschirnhaus, E.W. (1683) Methodus
auferendi omnes terminos intermedios ex
data aeqvatione. Acta Eruditorium, 2,
204‚Äì207.
43. Galois, √â. (1879) ≈íuvres math√©matiques.
Gauthier-Villars (sous l‚Äôauspices de la SMF).
44. Kac, M. (1943) On the average number of
real roots of a random algebraic equation.
Bull. A.M.S., 49, 314‚Äì320.
45. Lerario, A. and Lundberg, E. (2012) Statistics
on Hilbert‚Äôs Sixteenth Problem,
http://arxiv.org/abs/1212.3823.
46. Coste, M. and Roy, M.F. (1988) Thom‚Äôs
Lemma, the coding of real algebraic
numbers and the computation of the
topology of semi-algebraic sets. J. Symb.
Comput., 5, 121‚Äì129.
47. Becker, T., Weispfenning, V. (with Kredel,
H.) Groebner Bases: A Computational

References
317
Approach to Commutative Algebra,
Springer-Verlag, 1993.
48. Mayr, E.W. and Ritscher, S. (2010) Degree
bounds for Gr√∂bner bases of
low-dimensional polynomial ideals, in
Proceedings ISSAC 2010 (ed. S.M. Watt),
ACM, New York, pp. 21‚Äì28.
49. Fortuna, E., Gianni, P., and Trager, B. (2001)
Degree reduction under specialization. J.
Pure Appl. Algebra, 164, 153‚Äì163.
50. Aubry, P., Lazard, D., and Moreno Maza, M.
(1999) On the theories of triangular sets. J.
Symb. Comput., 28, 105‚Äì124.
51. Chen, C., Davenport, J.H., May, J.P.,
Moreno Maza, M., Xia, B., and Xiao, R.
(2013) Triangular decomposition of
semi-algebraic systems. J. Symb. Comput.,
49, 3‚Äì26.
52. Rich, A.D. and JeÔ¨Ärey, D.J. (2009) A
knowledge repository for indeÔ¨Ånite
integration based on transformation rules, in
Proceedings Intelligent Computer
Mathematics (eds J. Carette et al.), Springer,
pp. 480‚Äì485.
53. Horowitz, E. (1969) Algorithm for symbolic
integration of rational functions. PhD thesis,
University of Wisconsin.
54. Horowitz, E. (1971) Algorithms for partial
fraction decomposition and rational
function integration. Proceedings Second
Symposium on Symbolic and Algebraic
Manipulation, pp. 441‚Äì457.
55. Ostrogradski, M.W. (1845) De l‚Äôint√©gration
des fractions rationelles. Bull. Acad. Imp. Sci.
St. Petersburg (Class Phys.-Math.), 4,
145‚Äì167.
56. Rothstein, M. (1976) Aspects of symbolic
integration and simpliÔ¨Åcation of exponential
and primitive functions. PhD thesis,
University of Wisconsin.
57. Trager, B.M. (1976) Algebraic factoring and
rational function integration, in Proceedings
SYMSAC 76 (ed. R.D. Jenks), ACM, New
York, pp. 219‚Äì226.
58. Risch, R.H. (1969) The problem of
integration in Ô¨Ånite terms. Trans. A.M.S.,
139, 167‚Äì189.
59. Davenport, J.H. (1981) On the Integration of
Algebraic Functions, Springer Lecture Notes
in Computer Science 102, Springer.
60. Bronstein, M. (2005) Symbolic Integration I,
2nd edn, Springer-Verlag.
61. Kovacic, J.J. (1986) An algorithm for solving
second order linear homogeneous
diÔ¨Äerential equations. J. Symb. Comput., 2,
3‚Äì43.
62. Singer, M.F. (1981) Liouvillian solutions of
n-th order homogeneous linear diÔ¨Äerential
equations. Am. J. Math., 103, 661‚Äì682.
63. Apostol, T.M. (1967) Calculus, Vol. I, 2nd
edn, Blaisdell.
64. Davenport, J.H. (2010) The challenges of
multivalued ‚ÄúFunctions‚Äù, in Proceedings
AISC/Calculemus/MKM 2010 (eds S.
Autexier et al.), Springer, pp. 1‚Äì12.
65. England, M., Bradford, R., Davenport, J.H.,
and Wilson, D.J. (2013) Understanding
branch cuts of expressions, in Proceedings
CICM 2013 (eds J. Carette et al.), Springer,
pp. 136‚Äì151.
66. Bradford, R.J. and Davenport, J.H. (2002)
Towards better simpliÔ¨Åcation of elementary
functions, in Proceedings ISSAC 2002 (ed. T.
Mora), ACM, New York, pp. 15‚Äì22.
67. Davenport, J.H. (2003) The geometry of Cn is
important for the algebra of elementary
functions. Algebra Geometry and software
systems, Springer, pp. 207‚Äì224.
68. Carath√©odory, C. (1958) Theory of Functions
of a Complex Variable, Chelsea Publishing.


319
9
DiÔ¨Äerentiable Manifolds
Marcelo Epstein
9.1
Introduction
In his authoritative Physics,1)
Aristotle
(384‚Äì322 BC) establishes that space ‚Äúhas
three dimensions, length, breadth, depth,
the dimensions by which all body also is
bounded.‚Äù2) Time is regarded intuitively
as one dimensional. Moreover, both space
and motion are considered as being con-
tinuous in the sense that they are ‚Äúdivisible
into divisibles that are inÔ¨Ånitely divisi-
ble.‚Äù3) The continuity of motion and space
implies, therefore, the continuity of time.4)
From these modest beginnings, it would
take 23 centuries to arrive at a rigorous
mathematical deÔ¨Ånition of the most gen-
eral entity that combines the intuitive
notions of continuity and constancy of
dimension. Accordingly, we introduce Ô¨Årst
the notion of topological space, which is
1) Aristotle, Physics, Hardie R P and Gaye R K
(translators), The Internet Classics Archive.
2) Ibid., Book IV, Part 1.
3) The Greek term used is ùúéùúêùúàùúñùúíÃÅùúÇùúç, which literally
means ‚Äúholding together."
4) Ibid., Book IV, Part 11.
the most general entity that can sustain
continuous functions, and, subsequently,
the notion of topological manifold, which
is a topological space that locally resembles
‚Ñùn.
9.2
Topological Spaces
9.2.1
DeÔ¨Ånition
A topological space is a set A in which a
topology has been introduced. A topology
ÓâÄ(A) on the set A is a collection of subsets
of A, called the open sets of A, with the
following properties:
1. The empty set ‚àÖand the total space A
are open sets.
2. The union of any arbitrary collection of
open sets is an open set.
3. The intersection of any Ô¨Ånite collection
of open sets is an open set.
Given a point a ‚ààA, a neighborhood of a
is an open set N(a) ‚ààÓâÄ(A) containing a. By
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

320
9 DiÔ¨Äerentiable Manifolds
Property 1 above, every point of A has at
least one neighborhood.
9.2.2
Continuity
A function f ‚à∂A ‚ÜíB between the topolog-
ical spaces A and B is continuous at a ‚ààA if
for any neighborhood V of f (a) ‚ààB there
exists a corresponding neighborhood U of
a ‚ààA such that f (U) ‚äÇV. A function that
is continuous at every point of its domain
is said to be continuous. A function is con-
tinuous if, and only if, the inverse image of
every open set (in B) is open (in A).
A bijective (i.e., one-to-one and onto)
function f ‚à∂A ‚ÜíB between topological
spaces is a homeomorphism if it is contin-
uous and its inverse f ‚àí1 ‚à∂B ‚ÜíA is also
continuous. Topology can be understood
as the study of those properties that are
preserved under homeomorphisms.
9.2.3
Further Topological Notions
A subset B ‚äÇA of a topological space A
with a topology ÓâÄ(A) inherits a topology
ÓâÄ(B) as follows: The open sets of B are
deÔ¨Åned as the intersections of the open sets
of A with B. The topology ÓâÄ(B) obtained
in this way is called the relative or subset
topology.
A topological space A is a HausdorÔ¨Ä
space if, given any two diÔ¨Äerent points
a, b ‚ààA, there exist respective disjoint
neighborhoods N(a) and N(b), that is,
N(a) ‚à©N(b) = ‚àÖ.
A base ÓàÆof a topology ÓâÄ(A) is a collec-
tion of open sets such that every open set of
A is a union of elements of ÓàÆ. The topolog-
ical space is said to be generated by a base.
Thus, the open intervals of the real line ‚Ñù
constitute a base of the ordinary topology
of ‚Ñù.
Recall that a set is countable if it can be
put in a one-to-one correspondence with a
subset of the natural numbers ‚Ñï. A topol-
ogy is said to be second countable (or to
satisfy the second axiom of countability) if
it has a countable basis. Second-countable
topologies enjoy many special properties.
A subset of a topological space A is closed
if its complement is open. Notice that an
arbitrary subset of a topological space need
not be either open or closed, or it may be
both open and closed. A topological space
is said to be connected if the only subsets
that are both open and closed are the empty
set and the total space.
An open cover of a topological space A is a
collection ÓàØof open sets whose union is the
total space. An open subcover of ÓàØis a sub-
collection of ÓàØthat is itself an open cover. A
topological space is compact if every open
cover has a Ô¨Ånite open subcover.
The product topology of two topological
spaces, A and B, with respective topologies
Óàø(A) and ÓâÄ(B), is the topology on the
Cartesian product A √ó B generated by
the base of all the Cartesian products of the
form S √ó T, where S ‚ààÓàø(A) and T ‚ààÓâÄ(B).
9.3
Topological Manifolds
9.3.1
Motivation
As, as we have shown, a topological space
is all one needs to deÔ¨Åne continuous func-
tions, it might appear that this is the proper
arena for the formulation of physical the-
ories involving the notion of a continuum.
Nevertheless, even from the Aristotelian
viewpoint, it seems that we also need to
convey the idea of a Ô¨Åxed number of under-
lying dimensions, a concept that is not
automatically embodied in the deÔ¨Ånition

9.3 Topological Manifolds
321
of a topological space. If we think of the
surface of the Earth and its cartographic
representations, we understand intuitively
that, although the surface of a sphere can-
not be continuously mapped once and for
all onto the plane ‚Ñù2, it can be so repre-
sented in a piecewise manner. In generaliz-
ing this example, we look for the notion of
a topological space that is piecewise home-
omorphic to ‚Ñùn.
9.3.2
DeÔ¨Ånition
An n-dimensional topological manifold Óàπ
is a HausdorÔ¨Äsecond-countable topologi-
cal space such that each of its points has
a neighborhood homeomorphic to an open
set of ‚Ñùn.
The standard topology assumed in ‚Ñùn
is the one generated by all open balls in
‚Ñùn. Recall that the open ball of center
(c1, ‚Ä¶ , cn) ‚àà‚Ñùn and radius r ‚àà‚Ñùis the
subset of ‚Ñùn deÔ¨Åned as {(x1, ‚Ä¶ , xn) ‚àà
‚Ñùn ‚à∂(x1 ‚àíc1)2 + ¬∑ ¬∑ ¬∑ + (xn ‚àícn)2 < r2}.
The empty set is obtained setting r ‚â§0.
Although we have deÔ¨Åned only Ô¨Ånite-
dimensional topological manifolds, which
are modeled locally on ‚Ñùn, it is also possible
to deÔ¨Åne topological manifolds modeled on
inÔ¨Ånite-dimensional Banach spaces.
9.3.3
Coordinate Charts
It follows from the deÔ¨Ånition of topological
manifold that there exists an open cover
each of whose elements is homeomorphic
to an open set of ‚Ñùn. If we denote by ÓâÅùõºthe
generic constituent of the open cover and
by ùúôùõºthe corresponding homeomorphism,
where ùõºdenotes a running index, we can
identify the pair (ÓâÅùõº, ùúôùõº) with a coordinate
chart. The collection of all these pairs is
called an atlas of the topological manifold.
The terminology of coordinate charts
arises from the fact that a chart introduces
a local coordinate system. More speciÔ¨Åcally,
the homeomorphism ùúôùõº‚à∂ÓâÅùõº‚Üíùúôùõº(ÓâÅùõº) ‚äÇ
‚Ñùn assigns to each point p ‚ààÓâÅùõºan ordered
n-tuple (x1(p), ‚Ä¶ , xn(p)), called the local
coordinates of p.
Whenever two charts, (ÓâÅùõº, ùúôùõº) and
(ÓâÅùõΩ, ùúôùõΩ), have a nonempty intersection, we
deÔ¨Åne the transition function ùúôùõºùõΩas
ùúôùõºùõΩ= ùúôùõΩ‚àòùúô‚àí1
ùõº‚à∂ùúôùõº(ÓâÅùõº‚à©ÓâÅùõΩ)
‚ÜíùúôùõΩ(ÓâÅùõº‚à©ÓâÅùõΩ).
(9.1)
Each transition function is a homeo-
morphism between open sets of ‚Ñùn. The
inverse of ùúôùõºùõΩis the transition function
ùúôùõΩùõº= ùúô‚àí1
ùõºùõΩ= ùúôùõº‚àòùúô‚àí1
ùõΩ. Denoting by xi and
yi (i = 1, ‚Ä¶ , n),
respectively,
the
local
coordinates of ÓâÅùõºand ÓâÅùõΩ, a transition
function boils down to the speciÔ¨Åcation of
n continuous and continuously invertible
real functions of the form
yi = yi(x1, ‚Ä¶ , xn),
i = 1, ‚Ä¶ , n.
(9.2)
9.3.4
Maps and Their Representations
If Óàπand Óà∫are topological manifolds of
dimensions m and n, respectively, a map
f ‚à∂Óàπ‚ÜíÓà∫is continuous if it is a continu-
ous map between the underlying topolog-
ical spaces. A nice feature of topological
manifolds, as opposed to general topologi-
cal spaces, is the possibility of representing
continuous maps locally as real functions
of real variables. Let p ‚ààÓàπand denote
q = f (p) ‚ààÓà∫. By continuity, we can always
choose a chart (ÓâÅ, ùúô) containing p such
that its image f (ÓâÅ) is contained in a chart
(ÓâÇ, ùúì) containing q. The map

322
9 DiÔ¨Äerentiable Manifolds
ÃÇf = ùúì‚àòf ‚àòùúô‚àí1 ‚à∂ùúô(ÓâÅ) ‚Üíùúì(ÓâÇ)
(9.3)
maps an open set in ‚Ñùm to an open set in
‚Ñùn. This continuous map ÃÇf is the local coor-
dinate representation of f in the coordinate
charts chosen.
9.3.5
A Physical Application
Lagrange‚Äôs
(1736‚Äì1813)
conception
of
mechanics was purportedly purely analyti-
cal. In the Preface to the Ô¨Årst edition of his
M√©canique Analitique,5) he explicitly states
that ‚ÄúOn ne trouvera point de Figures dans
cet Ouvrage. Les m√©thodes que j‚Äôy expose
ne demandent ni constructions, ni raison-
nements g√©om√©triques ou m√©chaniques,
mais seulemnet des op√©rations alg√©briques,
assujeties √† une marche r√©guli√®re et uni-
forme. Ceux qui aiment l‚ÄôAnalyse verront
avec plaisir la M√©chanique en devenir une
nouvelle branche, et me sauront gr√© d‚Äôen
avoir √©tendu ainsi le domain.‚Äù Neverthe-
less, it is not an exaggeration to say that
in laying down the foundations of Ana-
lytical Mechanics, Lagrange was actually
inaugurating
the
diÔ¨Äerential
geometric
approach to Physics. In Lagrange‚Äôs view, a
mechanical system was characterized by
a Ô¨Ånite number n of degrees of freedom to
each of which a generalized coordinate is
assigned. A conÔ¨Åguration of the system
is thus identiÔ¨Åed with an ordered n-tuple
of real numbers. But, is this assignment
unique? And, anyway, what are these
numbers coordinates of?
Consider the classical example of a (rigid)
double pendulum oscillating in a vertical
plane under gravity. Clearly, this system
can be characterized by two independent
degrees of freedom. If we were to adopt as
5) Lagrange (1788), M√©canique Analitique [sic],
chez la Veuve Desaint, Libraire, Paris.
generalized coordinates the horizontal dis-
placements, x1 and x2, of the two masses
from, say, the vertical line through the point
of suspension, we would Ô¨Ånd that to an arbi-
trary combination of these two numbers,
there may correspond as many as 4 diÔ¨Äer-
ent conÔ¨Ågurations. If, to avoid this problem,
we were to adopt as generalized coordinates
the angular deviations ùúÉ1 and ùúÉ2, we would
Ô¨Ånd that a given conÔ¨Åguration can be char-
acterized by an inÔ¨Ånite number of com-
binations of values of these coordinates,
owing to the additive freedom of 2ùúã. If we
attempt to solve this problem by limiting
the range of these coordinates to the inter-
val [0, 2ùúã], we lose continuity of the repre-
sentation (because two neighboring conÔ¨Åg-
urations would correspond to very distant
values of the coordinates).
Let us, therefore, go against Lagrange‚Äôs
own advice and attempt to draw a mental
picture of the geometry of the situation. As
the Ô¨Årst mass (attached to the main point of
suspension) is constrained to move along a
circle, thus constituting a simple pendulum,
we conclude that its conÔ¨Ågurations can be
homeomorphically mapped onto a circum-
ference (or any other closed curve in the
plane). We say that this circumference is the
conÔ¨Åguration space of a simple pendulum.
Now, the second mass can describe a cir-
cumference around any position of the Ô¨Årst.
It is not diÔ¨Écult to conclude that the con-
Ô¨Åguration space of the double pendulum is
given by the surface of a torus. Now that
this basic geometric (topological) question
has been settled, we realize that an atlas
of this torus must consist of several charts.
But the central conceptual gain of the geo-
metrical approach is that the conÔ¨Åguration
space of a mechanical system, whose con-
Ô¨Ågurations are deÔ¨Åned with continuity in
mind, can be faithfully represented by a
unique topological manifold, up to a home-
omorphism.

9.4 DiÔ¨Äerentiable Manifolds
323
9.3.6
Topological Manifolds with Boundary
Consider the upper half space deÔ¨Åned as
the subset ‚Ñçn of ‚Ñùn consisting of all points
satisfying the inequality xn ‚â•0. Moreover,
endow this subset with the subset topol-
ogy induced by the standard topology of
‚Ñùn. An n-dimensional topological mani-
fold with boundary is a HausdorÔ¨Äsecond-
countable topological space such that each
of its points has a neighborhood homeo-
morphic to an open set of ‚Ñçn.
The manifold boundary ùúïM of a topolog-
ical manifold M with boundary is deÔ¨Åned
as the set of all points of M whose last
coordinate xn vanishes. Notice that this def-
inition is independent of the atlas used.
Thus, a topological manifold is a particular
case of a topological manifold with bound-
ary, namely, when the manifold boundary is
empty.
As a physical example, consider the case
of a plane pendulum suspended by means
of a wrinkable, but inextensible, thread. Its
conÔ¨Åguration space consists of a solid disk,
including the circumference and the inte-
rior points. Equivalently, any other solid
simply connected plane Ô¨Ågure can be used
as conÔ¨Åguration space of this mechanical
system.
9.4
DiÔ¨Äerentiable Manifolds
9.4.1
Motivation
As we have learned, topological manifolds
provide the most general arena for the def-
inition of continuous functions. Continu-
ity alone, however, may not be enough to
formulate physical problems in a quantita-
tive manner. Indeed, experience with actual
dynamical and Ô¨Åeld theories of Mechanics
and Electromagnetism, to name only the
classical theories, has taught us to expect
the various phenomena to be governed by
ordinary or partial diÔ¨Äerential equations.
These theories, therefore, must be formu-
lated on a substratum that has more struc-
ture than a topological manifold, namely, an
entity that allows for the deÔ¨Ånition of diÔ¨Äer-
entiable functions. DiÔ¨Äerentiable manifolds
are the natural generalization of topological
manifolds to handle diÔ¨Äerentiability. A dif-
ferentiable manifold corresponds roughly
to what physicists call a continuum.
9.4.2
DeÔ¨Ånition
As a topological space does not possess
in itself enough structure to sustain the
notion of diÔ¨Äerentiability, the key to the
generalization of a topological manifold is
to be found in restrictions imposed upon
the transition functions, which are clearly
deÔ¨Åned in ‚Ñùn. Two charts, (ÓâÅùõº, ùúôùõº) and
(ÓâÅùõΩ, ùúôùõΩ), of a topological manifold Óàπare
said to be Ck-compatible, if the transition
functions ùúôùõºùõΩand ùúôùõΩùõº, as deÔ¨Åned in (9.1),
are of class Ck. In terms of the represen-
tation (9.2), this means that all the partial
derivatives up to and including the order
k exist and are continuous. By conven-
tion, a continuous function is said to be
of class C0 and a smooth function is of
class C‚àû.
In a topological manifold, all charts
of all possible atlases are automatically
C0-compatible. An atlas of class Ck of a
topological manifold Óàπis an atlas whose
charts are Ck-compatible. Two atlases of
class Ck are compatible if each chart of one
is compatible with each chart of the other.
The union of compatible Ck-atlases is a
Ck atlas. Given a Ck atlas, one can deÔ¨Åne

324
9 DiÔ¨Äerentiable Manifolds
the corresponding maximal compatible
atlas of class Ck as the union of all atlases
that are Ck-compatible with the given
one. A maximal atlas, thus, contains all its
compatible atlases.
An n-dimensional diÔ¨Äerentiable man-
ifold of class Ck
is an n-dimensional
topological manifold Óàπtogether with a
maximal atlas of class Ck. For k = 0 one
recovers the topological manifold. The C‚àû
case delivers a smooth manifold, or simply a
manifold.
A maximal Ck-atlas is also called a
Ck-diÔ¨Äerentiable structure. Thus, a Ck-
manifold is a topological manifold with a
Ck-diÔ¨Äerentiable structure. For the par-
ticular case of ‚Ñùn, we can choose the
canonical atlas consisting of a single chart
(the space itself) and the identity map.
The induced C‚àû-diÔ¨Äerentiable structure
is the standard diÔ¨Äerentiable structure
of ‚Ñùn.
A diÔ¨Äerentiable manifold is oriented if it
admits an atlas, called an oriented atlas,
such that all the transition functions have
a positive Jacobian determinant. Two ori-
ented atlases are either compatible or every
transition function between charts of the
two atlases has a negative determinant. An
oriented manifold is an orientable mani-
fold with and oriented maximal atlas. In
other words, only those coordinate trans-
formations that preserve the orientation are
permitted.
Given two diÔ¨Äerentiable manifolds, Óàπ
and Óà∫, of dimensions m and n, respec-
tively, we deÔ¨Åne the (m + n)-dimensional
product manifold by endowing the Carte-
sian product Óàπ√ó Óà∫with an atlas made
of all the Cartesian products of charts
of an atlas of Óàπand an atlas of Óà∫.
The underlying topological space of a
product manifold inherits the product
topology.
9.4.3
DiÔ¨Äerentiable Maps
Let Óàπand Óà∫be (smooth) manifolds of
dimensions m and n, respectively. A con-
tinuous map f ‚à∂Óàπ‚ÜíÓà∫is diÔ¨Äerentiable
of class Ck at a point p ‚ààÓàπif, using charts
(ÓâÅ, ùúô) and (ÓâÇ, ùúì) belonging to the respec-
tive maximal atlases of Óàπand Óà∫, the local
coordinate representation ÃÇf of f , as deÔ¨Åned
in (9.3), is of class Ck at ùúô(p) ‚àà‚Ñùm. This def-
inition is independent of chart, because the
composition of diÔ¨Äerentiable maps in ‚Ñùm is
diÔ¨Äerentiable. Notice how the notion of dif-
ferentiability within the manifolds has been
cleverly deÔ¨Çected to the charts.
Maps of class C‚àûare said to be smooth
maps, to which we will conÔ¨Åne our analy-
sis from now on. In the special case Óà∫= ‚Ñù,
the map f ‚à∂Óàπ‚Üí‚Ñùis called a (real) func-
tion. When, on the other hand, Óàπis an
open interval H = (a, b) of the real line, the
map ùõæ‚à∂H ‚ÜíÓà∫is called a (parameterized)
curve in Óà∫. The name diÔ¨Äeomorphism is
reserved for the case in which Óàπand Óà∫are
of the same dimension and both f and its
(assumed to exist) inverse f ‚àí1 are smooth.
Two manifolds of the same dimension are
said to be diÔ¨Äeomorphic if there exists a dif-
feomorphism between them.
9.4.4
Tangent Vectors
Let H be an open interval of the real line
and, without loss of generality, assume
that 0 ‚ààH. Consider the collection of all
(smooth) curves ùõæ‚à∂H ‚ÜíÓàπsuch that
ùõæ(0) = p. Our aim is to deÔ¨Åne the notion
of tangency of two such curves at p, an
aim that we achieve by using the technique
of deÔ¨Çecting to charts. Indeed, if (ÓâÅ, ùúô)
is a chart containing p, the composition
ÃÇùõæ= ùúô‚àòùõæ‚à∂H ‚Üí‚Ñùm is a curve in ‚Ñùm, where
m is the dimension of Óàπ. The coordinate

9.4 DiÔ¨Äerentiable Manifolds
325
expression of ÃÇùõæis given by m smooth real
functions xi = ùõæi(t), where t is the natural
coordinate of ‚Ñùand i = 1, ‚Ä¶ , m. We say
that two curves, ùõæ1 and ùõæ2, in our collection
are tangent at p if
dùõæi
1
dt
|||||t=0
=
dùõæi
2
dt
|||||t=0
,
i = 1, ‚Ä¶ , m.
(9.4)
It is a simple matter to verify that this deÔ¨Å-
nition is independent of chart.
Noting that tangency at p is an equiva-
lence relation, we deÔ¨Åne a tangent vector
at p as an equivalence class of (smooth,
parameterized) curves tangent at p. A tan-
gent vector is thus visualized as what the
members of a collection of tangent (param-
eterized) curves have in common. More
intuitively, one may say that what these
curves have in common is a small piece of a
curve.
Let
f ‚à∂Óàπ‚Üí‚Ñùbe a (diÔ¨Äerentiable)
function and let v be a tangent vector at
p ‚ààÓàπ. Choosing any representative ùõæin
the equivalence class v, the composition
f ‚àòùõæis a real-valued function deÔ¨Åned on H.
The derivative of f at p along v is deÔ¨Åned as
v(f ) =
d(f ‚àòùõæ)
dt
||||t=0
.
(9.5)
This notation suggests that a vector can be
regarded as a linear operator on the collec-
tion of diÔ¨Äerentiable functions deÔ¨Åned on
a neighborhood of a point. The linearity is
a direct consequence of the linearity of the
derivative. Not every linear operator, how-
ever, is a tangent vector because, by virtue
of (9.5), tangent vectors must also satisfy
the Leibniz rule, namely, for any two func-
tions f and g
v(fg) = f v(g) + v(f )g,
(9.6)
where, on the right-hand side, f and g are
evaluated at p.
9.4.5
Brief Review of Vector Spaces
9.4.5.1
DeÔ¨Ånition
Recall that a (real) vector space V is charac-
terized by two operations: vector addition
and multiplication by a scalar. Vector
addition, denoted by +, is associative and
commutative. Moreover, there exists a zero
vector, denoted by 0, such that it leaves
all vectors unchanged upon addition. For
each vector v there exists a vector ‚àív
such
that
v + (‚àív) = 0.
Multiplication
by a scalar, indicated by simple apposi-
tion, satisÔ¨Åes the consistency condition
1v = v for all vectors v. It is, moreover,
associative, namely, (ùõºùõΩ)v = ùõº(ùõΩv), and
distributive,
namely,
ùõº(u + v) = ùõºu + ùõºv
and (ùõº+ ùõΩ)v = ùõºv + ùõΩv, for all scalars ùõº
and ùõΩand for all vectors u and v.
9.4.5.2
Linear Independence and
Dimension
A linear combination is an expression
of the form ùõº1v1 + ¬∑ ¬∑ ¬∑ + ùõºkvk, where, for
each i = 1, ‚Ä¶ , k, ùõºi is a scalar and vi is a
vector. The vectors v1, ‚Ä¶ , vk are linearly
independent if the only vanishing linear
combination is the trivial one, namely, if
ùõº1v1 + ¬∑ ¬∑ ¬∑ + ùõºkvk = 0
implies
necessar-
ily ùõº1 = ¬∑ ¬∑ ¬∑ = ùõºk = 0. A vector space is
m-dimensional if there exists a maximal
linearly independent set e1, ‚Ä¶ , em. Such a
set, if it exists, is called a basis of the vector
space. All bases have the same number
of elements, namely, m. If no maximal
linearly independent set exists, the vector
space is inÔ¨Ånite dimensional. Given a basis
e1, ‚Ä¶ , em of a Ô¨Ånite-dimensional vector
space, every vector v can be represented
uniquely in terms of components vi as
v = v1e1 + ¬∑ ¬∑ ¬∑ + vmem =
m
‚àë
i=1
viei.
(9.7)

326
9 DiÔ¨Äerentiable Manifolds
In many contexts, it is convenient to
use
Einstein‚Äôs
summation
convention,
whereby the summation symbol is under-
stood whenever a monomial contains a
once-diagonally repeated index. Thus, we
write
v = viei,
(9.8)
where the summation in the range 1 to m is
understood to take place.
9.4.5.3
The Dual Space
A linear function on a vector space V is a
map ùúî‚à∂V ‚Üí‚Ñùsuch that
ùúî(ùõºu + ùõΩv) = ùõºùúî(u) + ùõΩùúî(v),
(9.9)
for arbitrary scalars (ùõº, ùõΩ) and vectors (u
and v). From the physical viewpoint, a
linear function expresses the principle of
superposition. A trivial example of a linear
function is the zero function, assigning to
all vectors in V the number 0.
If we consider the collection V ‚àóof all lin-
ear functions on a given vector space V, it
is possible to introduce in it operations of
addition and of multiplication by a scalar,
as follows. The addition of two linear func-
tions ùúîand ùúéis deÔ¨Åned as the linear func-
tion ùúî+ ùúéthat assigns to each vector v the
sum ùúî(v) + ùúé(v). Similarly, for a scalar ùõº
and a linear function ùúî, we deÔ¨Åne the lin-
ear function ùõºùúîby (ùõºùúî)(v) = ùõº(ùúî(v)). With
these two operations, it is not diÔ¨Écult to
show that V ‚àóacquires the structure of a
vector space, called the dual space of V. Its
elements are called covectors. The action of
a covector ùúîon a vector v is indicated as
ùúî(v) or, equivalently, as ‚ü®ùúî, v‚ü©.
Given a basis e1, ‚Ä¶ , em of V, we deÔ¨Åne
the m linear functions ei by the formula
ei(v) = vi,
i = 1, ‚Ä¶ , m.
(9.10)
In other words, ei is the linear function (i.e.,
the covector) that assigns to a vector v ‚ààV
its ith component in the given basis. It is
not diÔ¨Écult to show that these covectors
form a basis of the dual space V ‚àó, which
is, therefore, of the same dimension as the
original space V. By construction, a basis
and its dual satisfy the identity
ei(ej) = ùõøi
j,
(9.11)
where ùõøi
j is the Kronecker symbol (equal to
1 if i = j and to zero otherwise). Any cov-
ector ùúîcan be expressed uniquely in terms
of components in a dual basis, namely, ùúî=
ùúîiei, where the summation convention is
used.
9.4.6
Tangent and Cotangent Spaces
The collection TpÓàπof all the tangent vec-
tors at p ‚ààÓàπis called the tangent space
to the manifold at p. It is not diÔ¨Écult to
show that tangent vectors at a point p sat-
isfy all the conditions of a vector space if we
deÔ¨Åne their addition and multiplication by
a scalar in the obvious way (for example, by
using chart components). In other words,
TpÓàπis a vector space. To Ô¨Ånd its dimen-
sion, we choose a local chart (ÓâÅ, ùúô) with
coordinates x1, ‚Ä¶ , xm, such that the point p
is mapped to the origin of ‚Ñùm. The inverse
map ùúô‚àí1, when restricted to the natural
coordinate lines of ‚Ñùm, delivers m curves
at p. Each of these curves, called a coor-
dinate line in ÓâÅ, deÔ¨Ånes a tangent vector,
which we suggestively denote by (ùúï‚àïùúïxi)p.
It can be shown that these vectors consti-
tute a basis of TpÓàπ, called the natural basis
associated with the given coordinate system.
The dimension of the tangent space at each
point of a manifold is, therefore, equal to
the dimension of the manifold itself. The
cotangent space at p, denoted by T‚àó
pÓàπ, is
deÔ¨Åned as the dual space of TpÓàπ.

9.4 DiÔ¨Äerentiable Manifolds
327
9.4.7
The Tangent and Cotangent Bundles
If we attach to each point p of an m-
dimensional manifold Óàπits tangent space
TpÓàπ, we obtain, intuitively speaking, a 2m-
dimensional entity, which we denote by
TÓàπ, called the tangent bundle of Óàπ. A
crude visualization of this entity can be
gathered when Óàπis a 2-sphere, such as a
globe, at each point of which we have stuck
a postal stamp or a paper sticker. The tan-
gent bundle is not the globe itself but rather
the collection of the stickers. This collection
of tangent spaces, however, has the prop-
erty that it projects on the original mani-
fold. In our example, each sticker indicates
the point at which it has been attached. In
other words, the set TÓàπis endowed, by
construction, with a projection map ùúèonto
the base manifold Óàπ. More explicitly, a typ-
ical point of TÓàπconsists of a pair (p, vp),
where p ‚ààÓàπand vp ‚ààTpÓàπ. The projec-
tion map
ùúè‚à∂TÓàπ‚ÜíÓàπ
(9.12)
is given by the assignation
ùúè(p, vp) = p.
(9.13)
To see that the set TÓàπcan be regarded as
a manifold, we construct explicitly an atlas
out of any given atlas of the base manifold.
Let (ÓâÅ, ùúô) be a chart in Óàπwith coordinates
xi, ‚Ä¶ , xm. Adopting, as we may, the natu-
ral basis (ùúï‚àïùúïxi)p of TpÓàπat each point p ‚àà
ÓâÅ, we can identify each vector vp with its
components vi
p. Put diÔ¨Äerently, we assign to
each point (p, vp) ‚ààùúè‚àí1(ÓâÅ) ‚äÇTÓàπthe 2m
numbers (x1, ‚Ä¶ , xm, v1, ‚Ä¶ , vm), namely, a
point in ‚Ñù2m. We have thus obtained a coor-
dinate chart on ùúè‚àí1(ÓâÅ). It is now a formality
to extend this construction to a whole atlas
of TÓàπand to show that TÓàπis a diÔ¨Äer-
entiable manifold of dimension 2m. In the
terminology of general Ô¨Åber bundles, the set
TpÓàπ= ùúè‚àí1(p) is called the Ô¨Åber at p ‚ààÓàπ.
As each Ô¨Åber is an m-dimensional vector
space, we say that the typical Ô¨Åber of TÓàπ
is ‚Ñùm.
Upon a coordinate transformation repre-
sented by (9.2), the components ÃÇvi of a vec-
tor v at p in the new natural basis (ùúï‚àïùúïyi)p
are related to the old components vi in the
basis (ùúï‚àïùúïxi)p by the formula
ÃÇvi =
(ùúïyi
ùúïxj
)
p
vj,
(9.14)
while the base vectors themselves are
related by the formula
(
ùúï
ùúïyi
)
p
=
(
ùúïxj
ùúïyi
)
p
( ùúï
ùúïxj
)
p .
(9.15)
Comparing these two formulas, we con-
clude that the components of vectors
behave
contravariantly.
In
traditional
treatments, it was customary to deÔ¨Åne
tangent
vectors
as
indexed
quantities
that
transform
contravariantly
under
coordinate changes.
A similar construction can be carried out
by attaching to each point of a manifold Óàπ
its cotangent space T‚àó
pÓàπto obtain the set
T‚àóÓàπ, called the cotangent bundle of Óàπ. A
typical point of T‚àóÓàπis a pair (p, ùúîp), where
p ‚ààÓàπand ùúîp ‚ààT‚àó
pÓàπ. The projection map
ùúã‚à∂T‚àóÓàπ‚ÜíÓàπis given by
ùúã(p, ùúîp) = p.
(9.16)
Given a chart, the local dual basis to the nat-
ural basis (ùúï‚àïùúïxi)p is denoted by (dxi)p, with
i = 1, ‚Ä¶ , m. The covector ùúîp ‚ààT‚àó
pÓàπcan
be uniquely expressed as ùúîp = ùúîidxi, where
the subscript p has been eliminated for
clarity. Given a point (p, ùúîp) ‚ààùúã‚àí1(ÓâÅ) ‚äÇ
T‚àóÓàπ, we assign to it the 2m numbers
(x1, ‚Ä¶ , xm, ùúî1, ‚Ä¶ , ùúîm). In this way, it can

328
9 DiÔ¨Äerentiable Manifolds
be rigorously shown that T‚àóÓàπis a mani-
fold of dimension 2m.
Upon a coordinate transformation, the
components ÃÇùúîi of a covector ùúîat p trans-
form according to
ÃÇùúîi =
(
ùúïxj
ùúïyi
)
p
ùúîj,
(9.17)
while the dual base vectors themselves are
related by the formula
dyi =
(ùúïyi
ùúïxj
)
p
dxj.
(9.18)
The
components
of
covectors
behave
covariantly.
9.4.8
A Physical Interpretation
In the context of the application presented
in Section 9.3.5, what do the tangent and
cotangent bundles represent? If the mani-
fold ÓàΩis the conÔ¨Åguration space of a system
with m degrees of freedom, then a curve
ùõæ‚à∂(a, b) ‚ÜíÓàΩrepresents a possible trajec-
tory of the system as it evolves in the time
interval a < t < b. At a point q ‚ààÓàΩ, there-
fore, a ‚Äúsmall‚Äù piece of a curve conveys the
notion of a virtual displacement ùõøq, that
is, a small displacement compatible with
the degrees of freedom of the system. In
the limit, we obtain a tangent vector at q,
representing a velocity. We conclude that
the tangent space TqÓàΩis the repository of
all possible velocities (or virtual displace-
ments) of the system at the conÔ¨Åguration
q. The tangent bundle TÓàΩ, accordingly, is
the collection of all possible velocities of
the system at all possible conÔ¨Ågurations. An
element of TÓàΩconsists of an ordered pair
made up of a conÔ¨Åguration and a velocity
at this conÔ¨Åguration. The projection map ùúè
assigns to this pair the conÔ¨Åguration itself.
In Lagrangian Mechanics, the fundamen-
tal geometric arena is precisely the tangent
bundle TÓàΩ. Indeed, the Lagrangian den-
sity Óà∏of a mechanical system is given by
a function Óà∏‚à∂TÓàΩ‚Üí‚Ñù, assigning to each
conÔ¨Åguration and each velocity (at this con-
Ô¨Åguration) a real number.
A covector Q at q is a linear function
that assigns to each tangent vector (vir-
tual displacement ùõøq) at q a real number
ùõøW = ‚ü®Q, ùõøq‚ü©, whose meaning is the vir-
tual work of the generalized force Q on the
virtual displacement ùõøq (or the power of
the generalized force on the correspond-
ing velocity). The terminology and the nota-
tion are due to Lagrange. The interesting
feature of the geometric approach is that,
once the basic geometric entity has been
physically identiÔ¨Åed as a manifold, its tan-
gent and cotangent bundles are automat-
ically the carriers of physical meaning. In
Hamiltonian Mechanics, covectors at q ‚àà
ÓàΩcan be regarded as generalized momenta
of the system. Thus, the cotangent bundle
T‚àóÓàΩis identiÔ¨Åed with the phase space of the
system, namely, the repository of all conÔ¨Åg-
urations and momenta. The Hamiltonian
function of a mechanical system is a func-
tion Óà¥‚à∂T‚àóÓàΩ‚Üí‚Ñù.
9.4.9
The DiÔ¨Äerential of a Map
Given a diÔ¨Äerentiable map
g ‚à∂Óàπ‚ÜíÓà∫
(9.19)
between two manifolds, Óàπand Óà∫, of
dimensions m and n, respectively, we focus
attention on a particular point p ‚ààÓàπand
its image q = g(p) ‚ààÓà∫. Let vp ‚ààTpÓàπbe
a tangent vector at p and let ùõæ‚à∂H ‚ÜíÓàπ
be one of its representative curves. The
composite map

9.4 DiÔ¨Äerentiable Manifolds
329
g ‚àòùõæ‚à∂H ‚àí‚ÜíÓà∫
(9.20)
is then a smooth curve in Óà∫passing
through q. This curve (the image of ùõæby g)
is, therefore, the representative of a tangent
vector at q, which we will denote (g‚àó)p(vp).
The vector (g‚àó)p(vp) is independent of the
representative curve ùõæchosen for vp. More-
over, (g‚àó)p is a linear map on vectors at p.
The map (g‚àó)p just deÔ¨Åned is called the
diÔ¨Äerential of g at p. It is a linear map
between the tangent spaces TpÓàπand
Tg(p)Óà∫. As this construction can be car-
ried out at each and every point of Óàπ,
we obtain a map g‚àóbetween the tangent
bundles, namely,
g‚àó‚à∂TÓàπ‚ÜíTÓà∫,
(9.21)
called the diÔ¨Äerential of g. Alternative nota-
tions for this map are Dg and Tg, and it
is also known as the tangent map. One
should note that the map g‚àóincludes the
map g between the base manifolds, because
it maps vectors at a point p linearly into vec-
tors at the image point q = g(p), and not
just to any vector in TÓà∫. It is, therefore, a
Ô¨Åber-preserving map. This fact is best illus-
trated in the following commutative dia-
gram:
TM
M
g
g‚àó
œÑN
œÑM
N
TN
(9.22)
where ùúèÓàπand ùúèÓà∫are the projection maps
of TÓàπand TÓà∫, respectively. The diÔ¨Äeren-
tial is said to push forward tangent vectors
at p to tangent vectors at the image point
g(p).
In the particular case of a function f ‚à∂
Óàπ‚Üí‚Ñù, the diÔ¨Äerential f‚àócan be inter-
preted somewhat diÔ¨Äerently. Indeed, the
tangent space Tr‚Ñùcan be trivially identi-
Ô¨Åed with ‚Ñùitself, so that f‚àócan be seen as a
real-valued function on TÓàπ. This function
is denoted by df ‚à∂TÓàπ‚Üí‚Ñù. The diÔ¨Äeren-
tial of a function satisÔ¨Åes the identity
df (v) = v(f ).
(9.23)
In local systems of coordinates xi (i =
1, ‚Ä¶ , m) and yùõº(ùõº= 1, ‚Ä¶ , n) around p
and g(p), respectively, the diÔ¨Äerential of g
at p maps the vector with components vi
into the vector with components
[(g‚àó)p(vp)]ùõº=
(ùúïgùõº
ùúïxi
)
p
vi,
(9.24)
where gùõº= gùõº(x1, ‚Ä¶ , xn) is the coordinate
representation of g in the given charts. The
(m √ó n)-matrix with entries {(ùúïgùõº‚àïùúïxi)
p}
is the Jacobian matrix at p of the map g in
the chosen coordinate systems. The rank of
the Jacobian matrix is independent of the
coordinates used. It is called the rank of g
at p.
Let f ‚à∂Óà∫‚Üí‚Ñùbe a diÔ¨Äerentiable func-
tion and let g ‚à∂Óàπ‚ÜíÓà∫be a diÔ¨Äerentiable
map between manifolds. Then,
((g‚àó)pvp)(f ) = vp(f ‚àòg),
p ‚ààÓàπ.
(9.25)
The diÔ¨Äerential of a composition of maps
is equal to the composition of the diÔ¨Äer-
entials. More precisely, if g ‚à∂Óàπ‚ÜíÓà∫and
h ‚à∂Óà∫‚ÜíÓàºare diÔ¨Äerentiable maps, then
((h ‚àòg)‚àó)p(vp) = (h‚àó)g(p)((g‚àó)p(vp)).
(9.26)
In coordinates, this formula amounts to the
multiplication of the Jacobian matrices.

330
9 DiÔ¨Äerentiable Manifolds
9.5
Vector Fields and the Lie Bracket
9.5.1
Vector Fields
A vector Ô¨Åeld V on a manifold Óàπis an
assignment of a tangent vector Vp = V(p) ‚àà
TpÓàπto each point p ‚ààÓàπ. We restrict
our attention to smooth vector Ô¨Åelds, whose
components are smooth functions in any
given chart. A vector Ô¨Åeld is, therefore, a
smooth map
V ‚à∂Óàπ‚ÜíTÓàπ,
(9.27)
satisfying the condition
ùúè‚àòV = idÓàπ,
(9.28)
where idÓàπis the identity map of Óàπ. The
meaning of this last condition is that the
vector assigned to the point p is a tan-
gent vector at p, rather than at any other
point.
A geometrically convenient way to look
at a vector Ô¨Åeld is to regard it as a cross
section of the tangent bundle. This termi-
nology arises from the pictorial representa-
tion depicted in Figure 9.1, where the base
manifold is represented by a shallow arc
and the Ô¨Åbers (namely, the tangent spaces)
by straight lines hovering above it. Then,
a cross section looks like a curve cutting
through the Ô¨Åbers.
9.5.2
The Lie Bracket
If V is a (smooth) vector Ô¨Åeld on a manifold
Óàπand f ‚à∂Óàπ‚Üí‚Ñùis a smooth function,
then the map
Vf ‚à∂Óàπ‚Üí‚Ñù,
(9.29)
deÔ¨Åned as
p ÓÇ∂‚ÜíVp(f )
(9.30)
is again a smooth map. It assigns to each
point p ‚ààÓàπthe directional derivative of
the function f in the direction of the vec-
tor Ô¨Åeld at p. In other words, a vector Ô¨Åeld
assigns to each smooth function another
smooth function. Given, then, two vector
Ô¨Åelds V and W over Óàπ, the iterated eval-
uation
h = W(Vf ) ‚à∂Óàπ‚Üí‚Ñù,
(9.31)
gives rise to a legitimate smooth function h
on Óàπ.
On the basis of the above considerations,
one may be tempted to deÔ¨Åne a composi-
tion of vector Ô¨Åelds by declaring that the
composition W ‚àòU is the vector Ô¨Åeld that
assigns to each function f the function h
deÔ¨Åned by (9.31). This wishful thinking,
however, does not work. To see why, it is
convenient to work in components in some
chart with coordinates xi. Let
V = V i ùúï
ùúïxi
W = W i ùúï
ùúïxi ,
(9.32)
TpM
V(M)
Vp = V(p)
œÑ
p
M
Figure 9.1
A vector Ô¨Åeld as a cross section of the
tangent bundle.

9.5 Vector Fields and the Lie Bracket
331
where the components V i and W i (i =
1, ‚Ä¶ , m) are smooth real-valued functions
deÔ¨Åned over the m-dimensional domain
of the chart. Given a smooth function f ‚à∂
Óàπ‚Üí‚Ñù, the function g = Vf is evaluated
at a point p ‚ààÓàπwith coordinates xi (i =
1, ‚Ä¶ , m) as
g(p) = V i ùúïf
ùúïxi .
(9.33)
Notice the slight abuse of notation we incur
into by identifying the function f with its
representation in the coordinate system.
We now apply the same prescription to
calculate the function h = Wg and obtain
h(p) = W i ùúïg
ùúïxi = W i ùúï
(
V j
ùúïf
ùúïxj
)
ùúïxi
=
(
W i ùúïV j
ùúïxi
) ùúïf
ùúïxj + W iV j ùúï2f
ùúïxiùúïxj .
(9.34)
The last term of this expression, by involv-
ing second derivatives, will certainly not
transform as the components of a vector
should under a change of coordinates. Nei-
ther will the Ô¨Årst. This negative result, on
the other hand, suggests that the oÔ¨Äend-
ing terms could perhaps be eliminated by
subtracting from the composition WV the
opposite composition VW, namely,
(WV ‚àíVW) (f )
=
(
W i ùúïV j
ùúïxi ‚àíV i ùúïW j
ùúïxi
) ùúïf
ùúïxj .
(9.35)
The vector Ô¨Åeld thus obtained is called the
Lie bracket of W and V (in that order) and
is denoted by [W, V]. More explicitly, its
components in the coordinate system xi are
given by
[W, V]j = W i ùúïV j
ùúïxi ‚àíV i ùúïW j
ùúïxi .
(9.36)
Upon a coordinate transformation, these
components transform according to the
rules of transformation of a vector.
The following properties of the Lie
bracket are worthy of notice:
1. Skew symmetry:
[W, V] = ‚àí[V, W].
(9.37)
2. Jacobi identity:
[[W, V], U] + [[V, U], W]
+[[U, W], V] = 0.
(9.38)
The collection of all vector Ô¨Åelds over a
manifold has the natural structure of an
inÔ¨Ånite-dimensional vector space, where
addition and multiplication by a scalar are
deÔ¨Åned in the obvious way. In this vector
space, the Lie bracket operation is bilinear.
A vector space endowed with a bilinear
operation satisfying conditions (1) and (2)
is called a Lie algebra.
Vector Ô¨Åelds can be multiplied by func-
tions to produce new vector Ô¨Åelds. Indeed,
for a given function f and a given vector
Ô¨Åeld V, we can deÔ¨Åne the vector Ô¨Åeld f V by
(f V)p = f (p)Vp.
(9.39)
It can be shown that
[gW, f V] = gf [W, V]
+ g (Wf ) V ‚àíf (Vg) W,
(9.40)
where g, f are smooth functions and W, V
are vector Ô¨Åelds over a manifold Óàπ.
9.5.3
A Physical Interpretation: Continuous
Dislocations
Let an atomic lattice be given by, say, all
points with integer coordinates in ‚Ñù2. To

332
9 DiÔ¨Äerentiable Manifolds
(a)
(b)
Figure 9.2
Dislocation in a crystal
lattice. (a) Perfect lattice and (b)
dislocated lattice.
each atom we can associate two vectors (in
this instance, unit and orthogonal) deter-
mined by joining it to its immediate neigh-
bors to the right and above, respectively.
If the lattice is deformed regularly, these
vectors will deform accordingly, changing
in length and angle, but always remaining
linearly independent at each atom. In the
(not precisely deÔ¨Åned) continuous limit, we
can imagine that each point of ‚Ñù2 has been
endowed with a basis or frame, the collec-
tion of which is called a moving frame (or
rep√®re mobile).6)
Returning to the discrete picture, if there
is a dislocation (for example, a half-line of
atoms is missing, as shown on the right-
hand side of Figure 9.2), the local bases
will be altered diÔ¨Äerently from the case of
a mere deformation. The engineering way
to recognize this is the so-called Burgers‚Äô
circuit, which consists of a four-sided path
made of the same number of atomic spac-
ings in each direction. The failure of such
a path to close is interpreted as the pres-
ence of a local dislocation in the lattice. We
want to show that in the putative continu-
ous limit, this failure is represented by the
non-vanishing of a Lie bracket. What we
have in the continuous case as the only rem-
nant of the discrete picture is a smoothly
distributed collection of bases, which we
have called a moving frame, and which can
6) This idea was introduced mathematically by Car-
tan and, in a physical context, by the brothers
Cosserat.
be seen as two vector Ô¨Åelds Eùõº(ùõº= 1, 2)
over ‚Ñù2.
From the theory of ordinary diÔ¨Äerential
equations, we know that each vector Ô¨Åeld
gives rise, at least locally, to a well-deÔ¨Åned
family of parameterized integral curves,
where the parameter is determined up to
an additive constant. More speciÔ¨Åcally,
these curves are obtained as the solutions
r = r(sùõº) of the systems of equations
dr(sùõº)
dsùõº
= Eùõº[r(sùõº)],
(ùõº= 1, 2;
no sum on ùõº),
(9.41)
where r represents the natural position vec-
tor in ‚Ñù2. The parameter sùõº(one for each of
the two families of curves) can be pinned
down in the following way. Select a point
p0 as origin and draw the (unique) inte-
gral curve ùõæ1 of the Ô¨Årst family passing
through this origin. Adopting the value s1 =
0 for the parameter at the origin, the value
of s1 becomes uniquely deÔ¨Åned for all the
remaining points of the curve. Each of the
curves of the second family must intersect
this curve of the Ô¨Årst family. We adopt,
therefore, for each of the curves of the sec-
ond family the value s2 = 0 at the corre-
sponding point of intersection with that
reference curve (of the Ô¨Årst family). In this
way, we obtain (at least locally) a new coor-
dinate system s1, s2 in ‚Ñù2. By construc-
tion, the second natural base vector of this
coordinate system is E2. But there is no

9.5 Vector Fields and the Lie Bracket
333
guarantee that the Ô¨Årst natural base vector
will coincide with E1, except at the curve
ùõæ1 through the adopted origin. In fact, if we
repeat the previous construction in reverse,
that is, with the same origin but adopting
the curve ùõæ2 of the second family as a refer-
ence, we obtain, in general, a diÔ¨Äerent sys-
tem of coordinates, which is well adapted
to the basis vectors E1, but not necessarily
to E2 (Figure 9.3).
Assume now that, starting at the adopted
origin, we move an amount of Œîs1 along
ùõæ1 to arrive at a point p‚Ä≤ and thereafter we
climb an amount of Œîs2 along the encoun-
tered curve of the second family through
p‚Ä≤. We arrive at some point p1. Incidentally,
this is the point with coordinates (Œîs1, Œîs2)
in the coordinate system obtained by the
Ô¨Årst construction. If, however, starting at
the same origin we move by Œîs2 along the
curve ùõæ2 to a point ÃÇp and then move by
Œîs1 along the encountered curve of the
Ô¨Årst family, we will arrive at a point p2
(whose coordinates are (Œîs1, Œîs2) in the sec-
ond construction) which is, in general, dif-
ferent from p1. Thus, we have detected the
failure of a four-sided circuit to close! The
discrete picture has, therefore, its contin-
uous counterpart in the noncommutativ-
ity of the Ô¨Çows along the two families of
curves.
Let us calculate a Ô¨Årst-order approxima-
tion to the diÔ¨Äerence between p2 and p1.
For this purpose, let us evaluate, to the Ô¨Årst
order, the base vector E2 at the auxiliary
point p‚Ä≤. The result is
E‚Ä≤
2 = E2(p0) + ùúïE2
ùúïxi
dxi
ds1 Œîs1,
(9.42)
where derivatives are calculated at p0. The
position vector of p1, always to Ô¨Årst-order
approximation, is obtained, therefore, as
r1 = Œîs1E1(p0)
+ Œîs2
(
E2(p0) + ùúïE2
ùúïxi
dxi
ds1 Œîs1,
)
.(9.43)
In a completely analogous manner, we cal-
culate the position vector of p2 as
r2 = Œîs2E2(p0)
+ Œîs1
(
E1(p0) + ùúïE1
ùúïxi
dxi
ds2 Œîs2,
)
.(9.44)
By virtue of (9.41), however, we have
dxi
dsùõº= Ei
ùõº,
(9.45)
where Ei
ùõºis the ith component in the natu-
ral basis of ‚Ñù2 of the base vector Eùõº. From
the previous three equations, we obtain
r2 ‚àír1 =
(ùúïE1
ùúïxi Ei
2 ‚àíùúïE2
ùúïxi Ei
1
)
Œîs1Œîs2
= [E1, E2] Œîs1Œîs2.
(9.46)
ùõæ1(s1)
ùõæ2(s2)
E1
E2
Œîs1
Œîs2
Œîs1
Œîs2
p2
p1
p0
p
p‚Ä≤
E‚Ä≤2
Figure 9.3
The continuous case.

334
9 DiÔ¨Äerentiable Manifolds
We thus conÔ¨Årm that the closure of the
inÔ¨Ånitesimal circuits generated by two vec-
tors Ô¨Åelds is tantamount to the vanishing of
their Lie bracket. This vanishing, in turn,
is equivalent to the commutativity of the
Ô¨Çows generated by these vector Ô¨Åelds. For
this reason, the Lie bracket is also called
the commutator of the two vector Ô¨Åelds. In
physical terms, we may say that the vanish-
ing of the Lie brackets between the vector
Ô¨Åelds representing the limit of a lattice is an
indication of the absence of dislocations.
As in this example we have introduced
the notion of a moving frame, that is, a
smooth Ô¨Åeld of bases Ei (i = 1, ‚Ä¶ , n) over
an n-dimensional manifold, it makes sense
to compute all the possible Lie brackets
between the base vectors and to express
them in terms of components in the local
basis. As a Lie bracket of two vector Ô¨Åelds is
itself a vector Ô¨Åeld, there must exist unique
scalar Ô¨Åelds ck
ij such that
[Ei, Ej] = ck
ijEk
(i, j, k = 1, ‚Ä¶ , n).
(9.47)
These scalars are known as the structure
constants of the moving frame. The struc-
ture constants vanish identically if, and only
if, the frames can be seen locally as the nat-
ural base vectors of a coordinate system.
9.5.4
Pushforwards
We have seen that the diÔ¨Äerential of a map
between manifolds carries tangent vectors
to tangent vectors. This operation is some-
times called a pushforward. Does a map also
pushforward vector Ô¨Åelds to vector Ô¨Åelds?
Let V ‚à∂Óàπ‚ÜíTÓàπbe a vector Ô¨Åeld on Óàπ
and let g ‚à∂Óàπ‚ÜíÓà∫be a smooth map. As
the diÔ¨Äerential of g is a map of the form g‚àó‚à∂
TÓàπ‚ÜíTÓà∫, the composition g‚àó‚àòV makes
perfect sense, but it delivers a (well-deÔ¨Åned)
map g‚àóV from Óàπ(and not from Óà∫) into
TÓà∫. This is not a vector Ô¨Åeld, nor can it
in general be turned into one. If the dimen-
sion of Óàπis larger than that of Óà∫, points
in Óà∫will end up being assigned more than
one vector. If the dimension of the source
manifold is less than that of the target, on
the other hand, even if the function is one-
to-one, there will necessarily exist points in
Óà∫to which no vector is assigned. The only
case in which the pushforward of a vector
Ô¨Åeld can be regarded as a vector Ô¨Åeld on the
target manifold is the case in which both
manifolds are of the same dimension and
the map is a diÔ¨Äeomorphism.
Notwithstanding the above remark, let
g ‚à∂Óàπ‚ÜíÓà∫be a smooth map. We say that
the vector Ô¨Åelds V ‚à∂Óàπ‚ÜíTÓàπand W ‚à∂
Óà∫‚ÜíTÓà∫are g-related if
g‚àóV(p) = W(g(p))
‚àÄp ‚ààÓàπ.
(9.48)
According to this deÔ¨Ånition, if g hap-
pens to be a diÔ¨Äeomorphism, then V
and g‚àóV are automatically g-related. The
pushed-forward vector Ô¨Åeld is then given
by g‚àó‚àòV ‚àòg‚àí1.
Theorem 9.1 Let V1 be g-related to W1
and let V2 be g-related to W2. Then the
Lie bracket [V1, V2] is g-related to the Lie
bracket [W1, W2], that is,
[g‚àóV1, g‚àóV2] = g‚àó[V1, V2].
(9.49)
9.6
Review of Tensor Algebra
9.6.1
Linear Operators and the Tensor Product
A linear operator T between two vector
spaces U and V is a linear map T ‚à∂U ‚Üí
V that respects the vector-space structure.

9.6 Review of Tensor Algebra
335
More precisely,
T(ùõºu1 + ùõΩu2) = ùõºT(u1) + ùõΩT(u2),
‚àÄùõº, ùõΩ‚àà‚Ñù, u1, u2 ‚ààU,
(9.50)
where the operations are understood in
the corresponding vector spaces. When the
source and target vector spaces coincide,
the linear operator is called a tensor. Occa-
sionally, the terminology of two-point ten-
sor (or just tensor) is also used for the
general case, particularly when the dimen-
sion of both spaces is the same. We will
use these terms (linear operator, linear map,
tensor, and so on) liberally.
Consider the collection L(U, V) of all
linear operators between two given vector
spaces, and endow it with the natural struc-
ture of a vector space. To do so, we deÔ¨Åne
the sum of two linear operators S and T as
the linear operator S + T whose action on
an arbitrary vector u ‚ààU is given by
(S + T)(u) = S(u) + T(u).
(9.51)
Similarly, we deÔ¨Åne the product of a scalar ùõº
by a linear operator T as the linear operator
ùõºT given by
(ùõºT)(u) = ùõºT(u).
(9.52)
It is a straightforward matter to verify that
the set L(U, V), with these two operations,
is a vector space. In the case of the dual
space V ‚àó(which can be identiÔ¨Åed with
L(V, ‚Ñù)), we were immediately able to
ascertain that it was never empty, because
the zero map is linear. The same is true
for L(U, V), whose zero element is the
linear map 0 ‚à∂U ‚ÜíV assigning to each
vector of U the zero vector of V. Inspired
by the example of the dual space, we will
attempt now to construct a basis of L(U, V)
starting from given bases at U and V. This
point takes some more work, but the result
is, both conceptually and notationally,
extremely creative.
Let ùúî‚ààU‚àóand v ‚ààV be, respectively a
covector of the source space and a vector
of the target space of a linear operator T ‚à∂
U ‚ÜíV. We deÔ¨Åne the tensor product of
v with ùúîas the linear operator v ‚äóùúî‚àà
L(U, V) obtained as follows:
(v ‚äóùúî) (u) = ‚ü®ùúî, u‚ü©v,
‚àÄu ‚ààU. (9.53)
We emphasize that the tensor product is
fundamentally noncommutative. We note,
on the other hand, that the tensor product
is a bilinear operation, namely, it is linear in
each of the factors, namely,
(ùõºu1 + ùõΩu2) ‚äóùúî= ùõº(u1 ‚äóùúî) + ùõΩ(u2 ‚äóùúî)
(9.54)
and
u ‚äó(ùõºùúî1 + ùõΩùúî2) = ùõº(u ‚äóùúî1) + ùõΩ(u ‚äóùúî2)
(9.55)
for all ùõº, ùõΩ‚àà‚Ñù.
One of the reasons for the conceptual
novelty of the tensor product is that it does
not seem to have an immediately intuitive
interpretation. In fact, it is a very singular
linear operator, because, Ô¨Åxing the Ô¨Årst fac-
tor, it squeezes the whole vector space U‚àó
into an image consisting of a single line of
V (the line of action of v).
Let
the
dimensions
of
U
and
V
be,
respectively,
m
and
n,
and
let
{eùõº} (ùõº= 1, ‚Ä¶ , m) and {fi} (i = 1, ‚Ä¶ , n)
be respective bases. It makes sense to
consider
the
m √ó n
tensor
products
fi ‚äóeùõº‚ààL(U, V). We want to show that
these linear operators (considered as vec-
tors belonging to the vector space L(U, V))
are in fact linearly independent. Assume
that a vanishing linear combination has
been found, namely, ùúåi
ùõºfi ‚äóeùõº= 0, where
ùúåi
ùõº‚àà‚Ñùand where the summation conven-
tion is appropriately used (Greek indices

336
9 DiÔ¨Äerentiable Manifolds
ranging from 1 to m, and Latin indices
ranging from 1 to n). Applying this linear
combination to the base vector eùõΩ‚ààV, we
obtain ùúåi
ùõΩfi = 0, whence ùúåi
ùõΩ= 0, proving
that the only vanishing linear combination
is the trivial one.
Let T ‚ààL(U, V) be an arbitrary linear
operator. By linearity, we may write
T(u) = T(uùõºeùõº) = uùõºT(eùõº).
(9.56)
Each T(eùõº), being an element of V, can be
written as a unique linear combination of
the basis, namely,
T(eùõº) = Ti
ùõºfi,
Ti
ùõº‚àà‚Ñù.
(9.57)
We form now the linear operator Ti
ùõºfi ‚äó
eùõºand apply it to the vector u, which yields
Ti
ùõºfi ‚äóeùõº(u) = uùõºTi
ùõºfi.
(9.58)
Comparing this result with (9.56, 9.57) we
conclude that the original operator T and
the operator Ti
ùõºfi ‚äóeùõºproduce the same
result when operating on an arbitrary vec-
tor u ‚ààU. They are, therefore, identical and
we can write
T = Ti
ùõºfi ‚äóeùõº.
(9.59)
In other words, every linear operator in
L(U, V) can be written as a linear combi-
nation of the m √ó n linearly independent
operators
fi ‚äóeùõº,
showing
that
these
operators form a basis of L(U, V), whose
dimension is, therefore, the product of
the dimensions of U and V. For these
reasons, the vector space L(U, V) is also
called the tensor-product space of V and
U‚àó, and is denoted as V ‚äóU‚àó. The unique
coeÔ¨Écients Ti
ùõºare called the components
of the tensor T in the corresponding basis.
The composition of linear operators is a
particular case of the composition of func-
tions. Let T ‚à∂U ‚ÜíV and S ‚à∂V ‚ÜíW be
linear operators between the vector spaces
U, V, and W. The composition S ‚àòT ‚à∂U ‚Üí
W is usually denoted as ST and is called the
product of the operators. Choosing bases in
the vector spaces U, V, and W and express-
ing the operators T ‚à∂U ‚ÜíV and S ‚à∂V ‚Üí
W in components, the composition S ‚àòT
is represented in components by the prod-
uct [S][T] of the matrices of components
of S and T. This is the best justiÔ¨Åcation of
the, at Ô¨Årst sight odd, rule for multiplying
matrices.
We have spoken about the conceptual
novelty of the tensor product. No less
important is its notational convenience. In
the case of a vector, say v ‚ààV, it is obvi-
ously advantageous to be able to express
the master concept of a vector in terms of
the subsidiary notion of its components in a
particular basis by simply writing: v = vifi.
If we change the basis, for instance, the fact
that what we have just written is an invari-
ant expression, with a meaning beyond the
particular basis chosen, can be exploited,
as we have already done. In the case of
linear operators, we have now obtained,
according to (9.59), a similar way to express
the ‚Äúreal thing‚Äù invariantly in terms of its
components on a basis arising from having
chosen arbitrary bases in both the source
and the target spaces. We can now show a
tensor itself, much in the same way as we
are able to show a vector itself. So powerful
is this idea that, historically, the notation
was invented before the concept of tensor
product had been rigorously deÔ¨Åned. In old
Physics texts it was called the dyadic nota-
tion, and it consisted of simply apposing
the elements of the bases involved (usually
in the same (Cartesian) vector space: ii, ij,
etc.). It is also interesting to recall that
in Quantum Mechanics the prevailing
notation for covectors and tensors is the
ingenious device introduced by Dirac in
terms of ‚Äúbras‚Äù and ‚Äúkets.‚Äù

9.6 Review of Tensor Algebra
337
9.6.2
Symmetry and Skew Symmetry
Let T ‚à∂U ‚ÜíV be a linear map. We deÔ¨Åne
the transpose of T as the map TT ‚à∂V ‚àó‚Üí
U‚àóobtained by the prescription
‚ü®TT(ùúî), u‚ü©= ‚ü®ùúî, T(u)‚ü©.
(9.60)
If {eùõº} (ùõº= 1, ‚Ä¶ , m) and {fi} (i = 1, ‚Ä¶ , n)
are, respectively, bases of U and V, whereby
T is expressed as
T = Ti
ùõºfi ‚äóeùõº,
(9.61)
then the transpose of T is expressed as
TT = Ti
ùõºeùõº‚äófi.
(9.62)
In other words, the transpose of a ten-
sor is obtained by leaving the components
unchanged and switching around the base
vectors. On the other hand, we may want
to express the transpose in its own right by
the standard formula
TT = (TT)
i
ùõºeùõº‚äófi,
(9.63)
applicable to the components of a tensor
in terms of a basis. Comparing the last two
equations, we conclude that
(TT)
i
ùõº= Ti
ùõº.
(9.64)
Notice the precise order of the indices in
each case.
A linear operator T is said to be sym-
metric if T = TT and skew- (or anti-) sym-
metric if T = ‚àíTT . Recall, however, that
in general T and TT operate between dif-
ferent spaces. This means that the notion
of symmetry should be reserved to the very
special case in which the target space is pre-
cisely the dual of the source space, namely,
when the linear operator belongs to some
L(U, U‚àó) or, equivalently to U‚àó‚äóU‚àó. We
conclude that, as expected from the very
tensor-product notation, a linear operator
and its transpose are of the same nature
(and may, therefore, be checked for sym-
metry) if, and only if, they belong to a ten-
sor product of the form V ‚äóV. Having
said this, it is clear that if some artiÔ¨Åcial
(noncanonical) isomorphism is introduced
between a space and its dual (by means of
an inner product, for example, as we shall
eventually do), then the concept of symme-
try can be extended.
9.6.3
The Algebra of Tensors on a Vector Space
Although a more general situation may be
envisioned, we now consider the collection
of all possible tensor products involving any
Ô¨Ånite number of factors, each factor being
equal to a given vector space V or its dual
V ‚àó. The order of the factors, of course, mat-
ters, but it is customary to say that a tensor
product is of type (r,s) if it is obtained by
multiplying r copies of V and s copies of
V ‚àó, regardless of the order in which these
copies appear in the product. An element of
such tensor product is also called a tensor of
type (r, s). Another common terminology is
to refer to r and s, respectively, as the con-
travariant and covariant degrees of the ten-
sor. Thus, a vector is a tensor of type (1, 0),
while a covector is of type (0, 1). By conven-
tion, a tensor of type (0, 0) is identiÔ¨Åed with
a scalar. As the Ô¨Åeld of scalars ‚Ñùhas the
natural structure of a vector space (whose
elements are tensors of type (0, 0)), it makes
sense to take its tensor product with a vec-
tor space. Note that ‚Ñù‚äóV = V.
The tensor product of a tensor of type
(r1, s1) with a tensor of type (r2, s2) is a
tensor of type (r1 + r2, s1 + s2). A map from
a Cartesian product of vector spaces into
a vector space is said to be multilinear

338
9 DiÔ¨Äerentiable Manifolds
if it is linear in each of the arguments.
A tensor T of type (r, s) can be con-
sidered as a multilinear map such that
T(ùúî1, ‚Ä¶ , ùúîr, v1, ‚Ä¶ , vs) ‚àà‚Ñù, where vi and
ùúîj belong, respectively, to V and V ‚àó, for
each i = 1, ‚Ä¶ , r and each j = 1, ‚Ä¶ , s.
The collection of all tensors of all orders
deÔ¨Åned on a vector space V can be given
the formal structure of an algebra (with the
operations of direct sum and tensor prod-
uct) known as the algebra of tensors on
V. Considering only tensors of covariant
degree zero, namely, tensors of type (r, 0),
we obtain the contravariant tensor alge-
bra of V. When written in components, all
indices of tensors in this algebra are super-
scripts.
In a similar way, one can deÔ¨Åne the
covariant tensor algebra by considering
tensors of type (0, s). On the other hand,
considering V ‚àóas a vector space in its
own right, we could form its contravariant
tensor algebra, and these two objects turn
out to be the same. The contravariant and
covariant algebras can be considered dual
to each other in the sense that there exists
a canonical way to evaluate an element of
one over an element of the other to pro-
duce a real number linearly. Considering a
tensor T of type (k, 0) and a tensor S of type
(0, k) and using a basis in V, this evaluation
reads
‚ü®S, T‚ü©= Si1¬∑¬∑¬∑ik Ti1¬∑¬∑¬∑ik.
(9.65)
If the tensors are of diÔ¨Äerent orders (that is,
a tensor of type (r, 0) and a tensor of type
(0, s) with s ‚â†r), we deÔ¨Åne the evaluation
as zero.
A tensor T of type (r, 0) can be seen as a
multilinear map
T ‚à∂V ‚àó, ‚Ä¶ , V ‚àó‚àí‚Üí‚Ñù
(ùúî1, ‚Ä¶ , ùúîr) ÓÇ∂‚ÜíT(ùúî1, ‚Ä¶ , ùúîr),
ùúî1, ‚Ä¶ , ùúîr ‚ààV ‚àó. (9.66)
For tensors in the contravariant or covari-
ant algebras it makes sense to speak about
symmetry and skew symmetry.
A tensor of type (r, 0) is said to be (com-
pletely) symmetric if the result of the opera-
tion (9.66) is independent of the order of the
arguments. Put in other words, exchanging
any two arguments with each other pro-
duces no eÔ¨Äect in the result of the multilin-
ear operator T. A similar criterion applies
for completely symmetric tensors of order
(0, s), except that the arguments are vectors
rather than covectors. Choosing a basis in
V, symmetry boils down to indiÔ¨Äerence to
index swapping.
Analogously, a tensor of type (r, 0)
is (completely) skew symmetric if every
mutual exchange of two arguments alters
the sign of the result, leaving the absolute
value unchanged. By convention, all ten-
sors of type (0, 0) (scalars), (1, 0) (vectors),
and (0, 1) (covectors) are considered to
be both symmetric and skew symmetric.
Notice that a completely skew-symmetric
tensor of type (r, 0) with r larger than the
dimension of the vector space of departure
must necessarily vanish.
The collections of all symmetric or skew-
symmetric tensors (whether contravariant
or covariant) do not constitute a subalge-
bra of the tensor algebra, for the simple rea-
son that the tensor multiplication of two
symmetric (or skew-symmetric) tensors is
not symmetric (skew symmetric) in gen-
eral. Nevertheless, it is possible, and conve-
nient, to deÔ¨Åne algebras of symmetric and
skew-symmetric tensors by modifying the
multiplicative operation so that the results
stay within the algebra. The case of skew-
symmetric tensors is the most fruitful. It
gives rise to the so-called exterior algebra of
a vector space, which we will now explore.
It will permit us to answer many intriguing
questions such as: is there anything anal-
ogous to the cross-product of vectors in

9.6 Review of Tensor Algebra
339
dimensions other than 3? What is an area
and what is the meaning of Ô¨Çux?
9.6.4
Exterior Algebra
The space of skew-symmetric contravari-
ant tensors of type (r, 0) will be denoted
by Œõr(V). The elements of Œõr(V) will be
also called r-vectors and, more generally,
multivectors. The number r is the order of
the multivector. As before, the space Œõ0(V)
coincides with the scalar Ô¨Åeld ‚Ñù, while
Œõ1(V) coincides with the vector space V.
Consider the ordered r-tuple of covec-
tors (ùúî1, ùúî2, ‚Ä¶ , ùúîr) and let ùúãdenote a per-
mutation of this set. Such a permutation is
even (odd) if it is obtained by an even (odd)
number of exchanges between pairs of ele-
ments in the original set. An even (odd)
permutation ùúãhas a signature, denoted by
sign(ùúã), equal to 1 (‚àí1).
Given an arbitrary tensor T of type (r, 0),
we deÔ¨Åne its skew-symmetric part Óà≠r(T) as
the multilinear map deÔ¨Åned by the formula
Óà≠r(T)(ùúî1, ùúî2, ‚Ä¶ , ùúîr) = 1
r!
‚àë
ùúã
sign(ùúã) T(ùúã).
(9.67)
As
an
example,
for
the
case
of
a
contravariant
tensor
of
degree
3,
namely,
T = Tijkei ‚äóej ‚äóek,
where
eh (h = 1, ‚Ä¶ , n ‚â•r) is a basis of V, the
skew-symmetric part is obtained as
Óà≠3(T) = 1
6
(Tijk + Tjki + Tkij ‚àíTikj
‚àíTjik ‚àíTkji)ei ‚äóej ‚äóek.
(9.68)
Given two multivectors a and b, of orders r
and s, respectively, we deÔ¨Åne their exterior
product or wedge product as the multivec-
tor a ‚àßb of order (r + s) obtained as
a ‚àßb = Óà≠r+s(a ‚äób).
(9.69)
What this deÔ¨Ånition in eÔ¨Äect is saying
is that in order to multiply two skew-
symmetric tensors and obtain a skew-
symmetric result, all we have to do is take
their tensor product and then project back
into the algebra (that is, skew-symmetrize
the result).7) As Óà≠is, by deÔ¨Ånition, a linear
operator, the wedge product is linear in
each of the factors.
We have seen that the tensor product is
not commutative. But, in the case of the
exterior product, exchanging the order of
the factors can at most aÔ¨Äect the sign. The
general result is
b ‚àßa = (‚àí1)rsa ‚àßb, a ‚ààŒõrV, b ‚ààŒõs(V).
(9.70)
Thus, for example, the wedge product with
itself of a multivector of odd order must
necessarily vanish. With some work, it is
possible to show that the wedge product
is
associative,
namely,
(a ‚àßb) ‚àßc = a ‚àß
(b ‚àßc).
To calculate the dimension of Œõr(V), we
note that, being a tensor, every element
in Œõk(V) is expressible as a linear combi-
nation of the nr tensor products ei1 ‚äó¬∑ ¬∑ ¬∑
‚äóeir, where ei, i = 1, ‚Ä¶ , n, is a basis of V.
Because of the skew symmetry, however, we
need to consider only products of the form
ei1 ‚àß¬∑ ¬∑ ¬∑ ‚àßeir. Two such products involving
the same factors in any order are either
equal or diÔ¨Äer in sign, and a product with
a repeated factor vanishes. This means that
we need only count all possible combina-
tions of n symbols taken r at a time without
7) In spite of the natural character of this deÔ¨Åni-
tion of the wedge product, many authors adopt
a deÔ¨Ånition that includes a combinatorial factor.
Thus, the two deÔ¨Ånitions lead to proportional
results. Each deÔ¨Ånition has some advantages, but
both are essentially equivalent. Our presentation
of exterior algebra follows closely that of Stern-
berg S (1983), Lectures on DiÔ¨Äerential Geometry,
2nd ed., Chelsea.

340
9 DiÔ¨Äerentiable Manifolds
repetition. The number of such combina-
tions is n!‚àï(n ‚àír)!r!. One way to keep track
of all these combinations is to place the
indices i1, ‚Ä¶ , ik in strictly increasing order.
These combinations are linearly indepen-
dent, thus constituting a basis. Therefore,
the dimension of Œõr(V) is n!‚àï(n ‚àír)!r!.
We note that the spaces of r-vectors and
(n ‚àír)-vectors have the same dimension.
There is a kind of fusiform dimensional
symmetry around the middle, the dimen-
sion starting at 1 for r = 0, increasing to a
maximum toward r = n‚àï2 (say, if n is even)
and then going back down to 1 for r = n.
This observation plays an important role
in the identiÔ¨Åcation (and sometimes confu-
sion) of physical quantities. For example, an
n-vector functions very much like a scalar,
but with a subtle diÔ¨Äerence.
Let a skew-symmetric contravariant ten-
sor a ‚ààŒõr(V) be given by means of its
components on the basis of ei1 ‚äó¬∑ ¬∑ ¬∑ ‚äóeir
inherited from a basis e1, ‚Ä¶ , en of V as
a = ai1,‚Ä¶,ir ei1 ‚äó¬∑ ¬∑ ¬∑ ‚äóeir.
(9.71)
Recalling that the skew-symmetry operator
Óà≠r is linear, we obtain
a = Óà≠r(a) = ai1,‚Ä¶,ir Óà≠r (ei1 ‚äó¬∑ ¬∑ ¬∑ ‚äóeir
)
= ai1,‚Ä¶,ir ei1 ‚àß¬∑ ¬∑ ¬∑ ‚àßeir.
(9.72)
In these expressions, the summation con-
vention is implied. We have obtained the
result that, given a skew-symmetric ten-
sor in components, we can substitute the
wedge products for the tensor products of
the base vectors. On the other hand, if we
would like to express the r-vector a in terms
of its components on the basis of Œõr(V)
given by the wedge products of the base
vectors of V taken in strictly increasing
order of the indices, a coeÔ¨Écient of r! will
have to be included, namely,
ai1,‚Ä¶,ir ei1 ‚àß¬∑ ¬∑ ¬∑ ‚àßeir
= r!
‚àë
i1<¬∑¬∑¬∑<ir
ai1,‚Ä¶,ir ei1 ‚àß¬∑ ¬∑ ¬∑ ‚àßeir. (9.73)
This means that the components on the
basis (with strictly increasing indices) of
the skew-symmetric part of a contravariant
tensor of type (k, 0) are obtained without
dividing by the factorial k! in the projection
algorithm. This, of course, is a small advan-
tage to be gained at the expense of the sum-
mation convention.
Consider
the
n-fold
wedge
product
a = v1 ‚àßv2 ‚àß¬∑ ¬∑ ¬∑ ‚àßvn, where the v‚Äôs are
elements of an n-dimensional vector space
V. Let {e1, e2, ‚Ä¶ , en} be a basis of V. As
each of the v‚Äôs is expressible uniquely in
this basis, we may write
a = (vi1
1 ei1) ‚àß(vi2
2 ei2) ‚àß¬∑ ¬∑ ¬∑ ‚àß(vin
n ein)
= vi1
1 vi2
2 ¬∑ ¬∑ ¬∑ vin
n ei1 ‚àßei2 ‚àß¬∑ ¬∑ ¬∑ ‚àßein, (9.74)
where the summation convention is in full
swing. Out of the possible nn terms in this
sum, there are exactly n! that can survive,
because each of the indices can attain n
values, but repeated indices in a term kill
it. However, because each of the surviving
terms consists of a scalar coeÔ¨Écient times
the exterior product of all the n elements
of the basis, we can collect them all into
a single scalar coeÔ¨Écient A multiplied by
the exterior product of the base vectors
arranged in a strictly increasing ordering
of the indices, namely, we must have that
a = Ae1 ‚àße2 ‚àß¬∑ ¬∑ ¬∑ ‚àßen. This scalar coeÔ¨É-
cient consists of the sum of all the prod-
ucts vi1
1 vi2
2 ¬∑ ¬∑ ¬∑ vin
n with no repeated indices
and with a minus sign if the superscripts
form an odd permutation of 1, 2, ‚Ä¶ , n. This
is precisely the deÔ¨Ånition of the determi-
nant of the matrix whose entries are vj
i.
We conclude that, using in Œõn(V) the basis

9.7 Forms and General Tensor Fields
341
induced by a basis in V, the component of
the exterior product of n vectors in an n-
dimensional space is equal to the determi-
nant of the matrix of the components of the
individual vectors. Apart from providing a
neat justiÔ¨Åcation for the notion of determi-
nant, this formula correctly suggests that
the geometrical meaning of an n-vector
is some measure of the ability of the (n-
dimensional) parallelepiped subtended by
the vectors to contain a volume. As we have
not yet introduced any metric notion, we
cannot associate a number to this volume.
Notice on the other hand that, although we
cannot say how large a volume is, we can
certainly tell that a given n-parallelepiped
is, say, twice as large as another. Notice,
Ô¨Ånally, that changing the order of two fac-
tors, or reversing the sense of one factor,
changes the sign of the multivector. So, n-
vectors represent oriented n-dimensional
parallelepipeds.
The collection of all multivectors of all
orders (up to the dimension of V), with
the exterior product replacing the tensor
product, constitutes the exterior algebra of
V. In a similar way, starting from the dual
space V ‚àó, we can construct the algebra of
multicovectors.
9.7
Forms and General Tensor Fields
9.7.1
1-Forms
Let f ‚à∂ÓâÅ‚Üí‚Ñùbe a smooth function
deÔ¨Åned in a neighborhood ÓâÅ‚äÇÓàπof the
point p, and let dfp ‚à∂TÓâÅ‚Üí‚Ñùdenote its
diÔ¨Äerential at p. We can regard this diÔ¨Äer-
ential as an element of T‚àó
pÓàπby deÔ¨Åning its
value ‚ü®dfp, vp‚ü©on any vector vp ‚ààTpÓàπas
‚ü®dfp, vp‚ü©= dfp(vp) = vp(f ).
(9.75)
In other words, the action of the evalua-
tion of the diÔ¨Äerential of the function on
a tangent vector is equal to the directional
derivative of the function in the direction of
the vector.
A smooth assignment of a covector ùúîp
to each point p ‚ààÓàπis called a diÔ¨Äerential
1-form on the manifold. It can be regarded
as a cross section of the cotangent bundle,
namely, a map
ùõÄ‚à∂Óàπ‚ÜíT‚àóÓàπ,
(9.76)
such that ùúã‚àòùõÄ= idÓàπ.
As we have seen, the diÔ¨Äerential of a func-
tion at a point deÔ¨Ånes a covector. It follows
that a smooth scalar function f ‚à∂Óàπ‚Üí‚Ñù
determines, by pointwise diÔ¨Äerentiation, a
diÔ¨Äerential 1-form ùõÄ= df . It is important
to remark that not all diÔ¨Äerential 1-forms
can be obtained as diÔ¨Äerentials of functions.
The ones that can are called exact.
A diÔ¨Äerential 1-form ùõÄ(that is, a cross
section of T‚àóÓàπ) can be regarded as acting
on vector Ô¨Åelds V (cross sections of TÓàπ)
to deliver functions ‚ü®ùõÄ, V‚ü©‚à∂Óàπ‚Üí‚Ñù, by
pointwise evaluation of a covector on a vec-
tor.
9.7.2
Pullbacks
Let f ‚à∂Óà∫‚Üí‚Ñùbe a smooth function. We
deÔ¨Åne its pullback by g as the map g‚àóf ‚à∂
Óàπ‚Üí‚Ñùgiven by the composition
g‚àóf = f ‚àòg.
(9.77)
For a diÔ¨Äerential 1-form ùõÄon Óà∫, we deÔ¨Åne
the pullback g‚àóùõÄ‚à∂Óàπ‚ÜíT‚àóÓàπby showing
how it acts, point by point, on tangent vec-
tors, namely,
‚ü®[g‚àóùõÄ](p), vp‚ü©= ‚ü®ùõÄ(g(p)), (g‚àó)pvp‚ü©,
(9.78)

342
9 DiÔ¨Äerentiable Manifolds
which can be more neatly written in terms
of vector Ô¨Åelds as
‚ü®g‚àóùõÄ, V‚ü©= ‚ü®ùõÄ‚àòg, g‚àóV‚ü©.
(9.79)
Expressed in words, this means that the
pullback by g of a 1-form in Óà∫is the 1-
form in Óàπthat assigns to each vector the
value that the original 1-form assigns to the
image of that vector by g‚àó.
It is important to notice that the pull-
backs of functions and diÔ¨Äerential 1-forms
are always well deÔ¨Åned, regardless of
the dimensions of the spaces involved.
This should be contrasted with the push-
forwards of vector Ô¨Åelds, which fail in
general to be vector Ô¨Åelds on the target
manifold.
9.7.3
Tensor Bundles
Given a point p of a manifold Óàπ, we
may identify the vector space V with the
tangent space TpÓàπand construct the cor-
responding spaces of tensors of any Ô¨Åxed
type. Following the same procedure as for
the tangent and cotangent bundles, which
will thus become particular cases, one can
deÔ¨Åne tensor bundles of any type by adjoin-
ing to each point of a manifold the tensor
space of the corresponding type. A conve-
nient notational scheme is ÓàØk(Óàπ), ÓàØk(Óàπ),
respectively, for the bundles of contravari-
ant and covariant tensors of order k.
Similarly, the bundles of k-vectors and of
k-forms can be denoted, respectively, by
Œõk(Óàπ), Œõk(Óàπ). Each of these bundles can
be shown (by a procedure identical to that
used in the case of the tangent and cotan-
gent bundles) to have a natural structure of
a diÔ¨Äerentiable manifold of the appropriate
dimension. A (smooth) section of a tensor
bundle is called a tensor Ô¨Åeld over Óàπ, of the
corresponding type. A (smooth) section of
the bundle Œõk(Óàπ) of k-forms is also called
a diÔ¨Äerential k-form. A scalar function on a
manifold is also called a diÔ¨Äerential 0-form.
In a chart of the m-dimensional manifold
Óàπwith coordinates xi, a contravariant ten-
sor Ô¨Åeld T of order r is given as
T = Ti1,‚Ä¶,ir
ùúï
ùúïxi1 ‚äó¬∑ ¬∑ ¬∑ ‚äó
ùúï
ùúïxir ,
(9.80)
where Ti1,‚Ä¶,ir = Ti1,‚Ä¶,ir(x1, ‚Ä¶ , xm) are rm
smooth functions of the coordinates. Simi-
larly, a covariant tensor Ô¨Åeld U of order r is
given by
U = Ui1,‚Ä¶,ir dxi1 ‚äó¬∑ ¬∑ ¬∑ ‚äódxir,
(9.81)
and a diÔ¨Äerential r-form ùúîby
ùúî= ùúîi1,‚Ä¶,ir dxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxir.
(9.82)
Notice that, in principle, the indexed quan-
tity ùúîi1,‚Ä¶,ir need not be speciÔ¨Åed as skew
symmetric with respect to the exchange
of any pair of indices, because the exte-
rior product of the base forms will do the
appropriate skew symmetrization job. As
an alternative, we may suspend the stan-
dard summation convention in (9.82) and
consider only indices in ascending order. As
a result, if ùúîi1,‚Ä¶,ir is skew symmetric ab ini-
tio, the corresponding components are to
be multiplied by r!.
Of particular interest for the theory of
integration on manifolds are diÔ¨Äerential m-
forms, where m is the dimension of the
manifold. From our treatment of the alge-
bra of r-forms, we know that the dimension
of the space of m-covectors is exactly 1. In
a coordinate chart, a basis for diÔ¨Äerential
m-forms is, therefore, given by: dx1 ‚àß¬∑ ¬∑ ¬∑ ‚àß
dxm. In other words, the representation of a
diÔ¨Äerential m-form ùúîin a chart is
ùúî= f (x1, ‚Ä¶ , xm) dx1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxm,
(9.83)

9.7 Forms and General Tensor Fields
343
where f (x1, ‚Ä¶ , xm) is a smooth scalar func-
tion of the coordinates in the patch. Con-
sider now another coordinate patch with
coordinates y1, ‚Ä¶ , ym, whose domain has a
nonempty intersection with the domain of
the previous chart. In this chart, we have
ùúî= ÃÇf (y1, ‚Ä¶ , ym) dy1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdym.
(9.84)
We want to Ô¨Ånd the relation between the
functions f and ÃÇf . As the transition func-
tions yi = yi(x1, ‚Ä¶ , xm) are smooth, we can
write
ùúî= ÃÇf (y1, ‚Ä¶ , ym) dy1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdym
= ÃÇf ùúïy1
ùúïxj1
¬∑ ¬∑ ¬∑ ùúïym
ùúïxjm
dxj1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxjm (9.85)
or, by deÔ¨Ånition of determinant
ùúî= det
{ ùúïy1, ‚Ä¶ , ym
ùúïx1, ‚Ä¶ , xm
}
ÃÇf dx1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxm
= Jy,x ÃÇf dx1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxm,
(9.86)
where the Jacobian determinant Jy,x does
not vanish at any point of the intersection
of the two coordinate patches. Comparing
with (9.83), we conclude that
f = Jy,x ÃÇf .
(9.87)
A nowhere vanishing diÔ¨Äerentiable m-form
on a manifold Óàπof dimension m is called
a volume form on Óàπ. It can be shown that
a manifold is orientable if, and only if, it
admits a volume form.
The notion of pullback can be naturally
generalized for covariant tensors of any
order. For a contravariant tensor Ô¨Åeld U
of order r on Óà∫(and, in particular, for
diÔ¨Äerential r-forms on Óà∫), the pullback
by a smooth function g ‚à∂Óàπ‚ÜíÓà∫is a
corresponding Ô¨Åeld on Óàπobtained by an
extension of the case r = 1, as follows:
g‚àóU (V1, ‚Ä¶ , Vr) = (U ‚àòg) (g‚àóV1, ‚Ä¶ , g‚àóVr),
(9.88)
where U is regarded as a multilinear func-
tion of r vector Ô¨Åelds Vi.
9.7.4
The Exterior Derivative
The exterior derivative of diÔ¨Äerential forms
is an operation that generalizes the gradi-
ent, curl, and divergence operators of clas-
sical vector calculus. The exterior derivative
of a diÔ¨Äerential r-form on a manifold Óàπis
a diÔ¨Äerential (r + 1)-form deÔ¨Åned over the
same manifold. Instead of introducing, as
one certainly could, the deÔ¨Ånition of exte-
rior diÔ¨Äerentiation in an intrinsic axiomatic
manner, we will proceed to deÔ¨Åne it in a
coordinate system and show that the def-
inition is, in fact, coordinate independent.
Let, therefore, xi (i = 1, ‚Ä¶ , m) be a coordi-
nate chart and let ùúîbe an r-form given as
ùúî= ùúîi1,‚Ä¶,ir dxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxir,
(9.89)
where
ùúîi1,‚Ä¶,ir = ùúîi1,‚Ä¶,ir(x1, ‚Ä¶ , xm)
are
smooth functions of the coordinates. We
deÔ¨Åne the exterior derivative of ùúî, denoted
by dùúî, as the diÔ¨Äerential (r + 1)-form
obtained as
dùúî= dùúîi1,‚Ä¶,ir ‚àßdxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxir,
(9.90)
where the d on the right-hand side denotes
the ordinary diÔ¨Äerential of functions. More
explicitly,
dùúî=
ùúïùúîi1,‚Ä¶,ir
ùúïxk
dxk ‚àßdxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxir.
(9.91)
Note that for each speciÔ¨Åc combination
of (distinct) indices i1, ‚Ä¶ , ir, the index k
ranges only on the remaining possibilities,

344
9 DiÔ¨Äerentiable Manifolds
because the exterior product is skew sym-
metric. Thus, in particular, if ùúîis a diÔ¨Äeren-
tial m-form deÔ¨Åned over an m-dimensional
manifold, its exterior derivative vanishes
identically (as it should, being an (m + 1)-
form).
Let yi (i = 1, ‚Ä¶ , m) be another coordi-
nate chart with a nonempty intersection
with the previous chart. We have
ùúî= ÃÇùúîi1,‚Ä¶,ir dyi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdyir,
(9.92)
for some smooth functions ÃÇùúîi1,‚Ä¶,ir of the
yi-coordinates. The two sets of components
are related by
ùúîi1,‚Ä¶,ir = ÃÇùúîj1,‚Ä¶,jr
ùúïyj1
ùúïxi1 ¬∑ ¬∑ ¬∑ ùúïyjr
ùúïxir .
(9.93)
Notice that we have not troubled to collect
terms by, for example, prescribing a strictly
increasing order. The summation conven-
tion is in eÔ¨Äect. We now apply the prescrip-
tion (9.90) and obtain
dùúî= d
(
ÃÇùúîj1,‚Ä¶,jr
ùúïyj1
ùúïxi1 ¬∑ ¬∑ ¬∑ ùúïyjr
ùúïxir
)
‚àßdxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxir.
(9.94)
The crucial point now is that the terms con-
taining the second derivatives of the coor-
dinate transformation will evaporate as a
result of their intrinsic symmetry, because
they are contracted with an intrinsically
skew-symmetric wedge product of two 1-
forms. We have, therefore,
dùúî=
ùúïÃÇùúîj1,‚Ä¶,jr
ùúïym
ùúïym
ùúïxk
ùúïyj1
ùúïxi1 ¬∑ ¬∑ ¬∑ ùúïyjr
ùúïxir
dxk ‚àßdxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxir,
(9.95)
or, Ô¨Ånally,
dùúî=
ùúïÃÇùúîj1,‚Ä¶,jr
ùúïym
dym ‚àßdyj1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdyjr,
(9.96)
which is exactly the same prescription in
the coordinate system yi as (9.90) is in the
coordinate system xi. This completes the
proof of independence from the coordinate
system.
From this deÔ¨Ånition, we can deduce a
number of important properties of the exte-
rior derivative, namely,
1. Linearity
d(a ùõº+ b ùõΩ) = a dùõº+ b dùõΩ
‚àÄa, b ‚àà‚Ñù
ùõº, ùõΩ‚ààŒõr(Óàπ).
(9.97)
2. Quasi-Leibniz rule
d(ùõº‚àßùõΩ) = dùõº‚àßùõΩ+ (‚àí1)rùõº‚àßdùõΩ
‚àÄùõº‚ààŒõr(Óàπ), ùõΩ‚ààŒõs(Óàπ).
(9.98)
3. Nilpotence
d2(.) = d(d(.)) = 0.
(9.99)
Moreover, it can be shown that the exte-
rior derivative commutes with pullbacks.
Finally, the exterior derivative of a 1-form
has the following interesting interaction
with the Lie bracket. If ùõºis a diÔ¨Äerential 1-
form and u and v are smooth vector Ô¨Åelds
on a manifold Óàπ, then
‚ü®dùõº| u ‚àßv‚ü©= u (‚ü®ùõº| v‚ü©) ‚àív (‚ü®ùõº| u‚ü©)
‚àí‚ü®ùõº| [u, v]‚ü©.
(9.100)
A diÔ¨Äerential form ùúîis closed if dùúî= 0.
Thus, all m-forms in an m-dimensional
manifold are automatically closed. An
r-form (with r > 1) is exact if there exists
an (r ‚àí1)-form ùúésuch that ùúî= dùúé. By
Property 3 above, all exact forms are
closed. The converse is true locally. In
other words, for every point in a mani-
fold, there exists an open neighborhood
on which the restriction of a closed

9.8 Symplectic Geometry
345
form is exact. But this property may fail
globally. An example is the 1-form given
by ùúî= (x dy ‚àíy dx)‚àï(x2 + y2) deÔ¨Åned on
an annular region of ‚Ñù2 with center at
the origin x = y = 0. This form is closed
but not exact. The existence of forms of
this type reÔ¨Çects the presence of topo-
logical invariants (such as holes) in the
manifold.
9.8
Symplectic Geometry
9.8.1
Symplectic Vector Spaces
A tensor T of type (0, r) on V is a multilin-
ear function acting on r vector arguments,
(v1, ‚Ä¶ , vr). Fixing one argument, say v1, we
obtain a tensor Tv1 of type (0, r ‚àí1). In par-
ticular, a tensor T of type (0, 2) assigns to
each vector u ‚ààV the covector Tu deÔ¨Åned
by
Tu(v) = T(u, v)
‚àÄv ‚ààV.
(9.101)
The tensor T of type (0, 2) is nondegener-
ate if Tu = 0 implies that u = 0. Since, in a
given basis, the components of the covector
Tu are Tijui, we conclude that a necessary
and suÔ¨Écient condition for T to be nonde-
generate is that the matrix with entries [Tij]
must have a non-vanishing determinant, a
condition that is independent of the basis
chosen.
A symplectic vector space is a vector
space in which a nondegenerate 2-covector
ùúîhas been singled out. The standard
example is provided by a vector space
of even dimension 2m. Choosing a basis
{e1, ‚Ä¶ , em, f1, ‚Ä¶ , fm}, the 2-covector
ùúîef =
m
‚àë
i=1
ei ‚àßfi
(9.102)
is nondegenerate. It can be shown that
every symplectic vector space is necessar-
ily even-dimensional and that there exists a
basis for which ùúîhas the form (9.102).
An important property of a symplectic
vector space is that, owing to the nonde-
generacy of the 2-covector ùúî, there exists
a natural correspondence between vectors
and covectors.
9.8.2
Symplectic Manifolds
Recall that an r-form ùúîon a manifold Óàπis
a smooth r-covector Ô¨Åeld, namely, a smooth
assignment of an r-covector ùúîp at each
point p ‚ààÓàπ. Equivalently, ùúîis a (smooth)
section of the bundle Œõr(Óàπ). A symplectic
form on Óàπis a nondegenerate closed 2-
form ùúî. A symplectic manifold (Óàπ, ùúî) is a
manifold in which a symplectic form ùúîhas
been singled out. According to our discus-
sion above, a symplectic manifold is neces-
sarily even-dimensional.
Given an m-dimensional manifold ÓàΩ
(for example, the conÔ¨Åguration space of
a mechanical system), the tangent and
cotangent bundles are manifolds of even
dimension 2m. It is a remarkable fact that
the cotangent bundle T‚àóQ of any manifold
is automatically endowed with a canonical
symplectic form. By ‚Äúcanonical,‚Äù we mean
that this form is deÔ¨Åned intrinsically (i.e.,
independently of any coordinate chart).
It is not surprising, therefore, that this
canonical structure results in a corre-
sponding physical interpretation. For a
mechanical system, the cotangent bundle
represents the phase space (of positions
and momenta) and the canonical form
plays a fundamental role in Hamiltonian
mechanics.
A generic point s ‚ààT‚àóÓàΩhas the form s =
(q, p), where q = ùúã(s) ‚ààÓàΩand p ‚ààT‚àó
q ÓàΩ.
Put diÔ¨Äerently, a point in the cotangent

346
9 DiÔ¨Äerentiable Manifolds
bundle consists of a point q in the base
manifold and a 1-covector p at q. Let V
be a tangent vector to T‚àóÓàΩat the point
s = (q, p) ‚ààT‚àóÓàΩ, namely, V ‚ààT(T‚àóÓàΩ). As
the projection ùúã‚à∂T‚àóQ ‚ÜíQ is a diÔ¨Äeren-
tiable map, its diÔ¨Äerential ùúã‚àó‚à∂T(T‚àóÓàΩ) ‚Üí
TÓàΩis well deÔ¨Åned. In particular, ùúã‚àó(Vs) ‚àà
TqÓàΩ. But the tangent bundle T(T‚àóÓàΩ), as
a tangent bundle, has its own projection
ÃÇùúè‚à∂T(T‚àóÓàΩ) ‚ÜíT‚àóÓàΩ. In particular, ÃÇùúè(Vs) =
s = (q, p). As this is a covector at q ‚ààÓàΩ, it
makes sense to evaluate it on the tangent
vector ùúã‚àó(Vs) ‚ààTqÓàΩ.
Recall that a 1-form on T‚àóÓàΩis a smooth
assignment of a covector ùúÉs at each point
s = (q, p) ‚ààT‚àóÓàΩ. We deÔ¨Åne the canonical
1-form ùúÉon T‚àóÓàΩby the formula
ùúÉ(Vs) = ‚ü®ÃÇùúè(Vs), ùúã‚àó(Vs)‚ü©.
(9.103)
The canonical symplectic form ùúîon T‚àóÓàΩis
deÔ¨Åned as
ùúî= ‚àídùúÉ.
(9.104)
Thus, ùúîis exact and, therefore, closed.
Moreover, it is nondegenerate. It is, in fact,
not diÔ¨Écult to obtain a coordinate expres-
sion of the canonical symplectic form. We
have seen that a chart (q1, ‚Ä¶ , qm) in ÓàΩ
induces a chart in T‚àóÓàΩ. Indeed, any 1-form
p on ÓàΩhas the coordinate expression
p = pidqi,
where
the
summation
con-
vention is in force. The induced chart in
T‚àóQ uses as coordinates the 2m numbers
(q1, ‚Ä¶ , qm, p1, ‚Ä¶ , pm). The canonical 1-
form ùúÉis given by ùúÉ= pidqi. It follows that
the canonical symplectic form is expressed
as ùúî= ‚àídpi ‚àßdqi = dqi ‚àßdpi.
9.8.3
Hamiltonian Systems
A Hamiltonian system consists of a sym-
plectic manifold (Óàπ, ùúî) and a smooth
real-valued function Óà¥‚à∂Óàπ‚Üí‚Ñùcalled
the
system
Hamiltonian.
In
Classical
Mechanics, the symplectic manifold is
identiÔ¨Åed with the phase space Óàπ= T‚àóÓàΩ
of the underlying conÔ¨Åguration manifold ÓàΩ.
A key concept in Hamiltonian systems
is that of Hamiltonian vector Ô¨Åeld. As the
Hamiltonian Óà¥is diÔ¨Äerentiable, its diÔ¨Äer-
ential dÓà¥is a well-deÔ¨Åned 1-form on Óàπ. In
a symplectic manifold, on the other hand, to
each 1-form, we can assign uniquely a vec-
tor Ô¨Åeld, by exploiting the pointwise nonde-
generacy of the symplectic form. We thus
obtain the associated Hamiltonian vector
Ô¨Åeld VH. More explicitly, at each point s ‚àà
Óàπwe have
‚ü®dÓà¥, U‚ü©= ùúî(VH, U)
‚àÄU ‚ààTsÓàπ.
(9.105)
A curve ùõæin Óàπis a trajectory of the Hamil-
tonian system if it satisÔ¨Åes Hamilton‚Äôs
equations, namely, if it is an integral curve
of the Hamiltonian vector Ô¨Åeld, namely,
dùõæ
dt = VH(ùõæ(t)).
(9.106)
In the natural coordinates of a cotangent
bundle, the curve ùõæconsists of the 2m
functions qi = qi(t) and pi = pi(t), with i =
1, ‚Ä¶ , m. The Hamiltonian vector Ô¨Åeld has
the components ùúïH‚àïùúïpi and ‚àíùúïH‚àïùúïqi. We
thus recover the standard form of Hamil-
ton‚Äôs equations, that is,
dqi
dt = ùúïH
ùúïpi
,
(9.107)
and
dpi
dt = ‚àíùúïH
ùúïqi ,
(9.108)
Notice that the construction (9.105) applies
to any smooth real-valued function deÔ¨Åned
on Óàπ, not just the Hamiltonian. Namely, to
any such function Óà≥we can uniquely assign
a vector Ô¨Åeld VG. We can thus deÔ¨Åne an

9.9 The Lie Derivative
347
operation between any two scalar Ô¨Åelds Óà≥
and Óà∑, called the Poisson bracket {Óà≥, Óà∑}, by
any of the equivalent prescriptions:
{Óà≥, Óà∑} = VK(Óà≥) = ‚ü®dÓà≥, VK‚ü©= ùúî(VG, VK).
(9.109)
The derivative of a scalar function Óà≥along
a trajectory ùõæof the Hamiltonian system
(Óàπ, Óà¥) is obtained as
dÓà≥
dt = dùõæ
dt (Óà≥) = ‚ü®dÓà≥, dùõæ
dt ‚ü©
= ‚ü®dÓà≥, VH‚ü©= {Óà≥, Óà¥}.
(9.110)
Thus, the Poisson bracket of a function
Óà≥(representing some physical property of
the system) with the Hamiltonian func-
tion describes the time evolution of Óà≥. The
vanishing of this Poisson bracket indicates,
therefore, a conserved quantity.
9.9
The Lie Derivative
9.9.1
The Flow of a Vector Field
Let V ‚à∂Óàπ‚ÜíTÓàπbe a (smooth) vector
Ô¨Åeld. A (parameterized) curve ùõæ‚à∂H ‚ÜíÓàπ
is called an integral curve of the vector Ô¨Åeld
if its tangent at each point coincides with
the vector Ô¨Åeld at that point. In other words,
denoting by s the curve parameter, the fol-
lowing condition holds:
dùõæ(s)
ds
= V(ùõæ(s))
‚àÄs ‚ààH ‚äÇ‚Ñù.
(9.111)
As a consequence of the fundamental
theorem of existence and uniqueness of
local solutions of systems of ordinary dif-
ferential equations, it is possible to prove
the following fundamental theorem for
vector Ô¨Åelds on manifolds.
Theorem 9.2 If V is a vector Ô¨Åeld on a
manifold Óàπ, then for every p ‚ààÓàπ, there
exists an integral curve ùõæ(s, p) ‚à∂Ip ‚ÜíÓàπ
such that (i) Ip is an open interval of ‚Ñùcon-
taining the origin s = 0; (ii) ùõæ(0, p) = p; and
(iii) Ip is maximal in the sense that there
exists no integral curve starting at p and
deÔ¨Åned on an open interval of which Ip is a
proper subset. Moreover,
ùõæ(s, ùõæ(s‚Ä≤, x)) = ùõæ(s + s‚Ä≤, x)
‚àÄs, s‚Ä≤, s + s‚Ä≤ ‚ààIp.
(9.112)
The map given by
p, s ÓÇ∂‚Üíùõæ(s, p)
(9.113)
is called the Ô¨Çow of the vector Ô¨Åeld V whose
integral curves are ùõæ(s, p). In this deÔ¨Åni-
tion, the map is expressed in terms of its
action on pairs of points belonging to two
diÔ¨Äerent manifolds, Óàπand ‚Ñù, respectively.
Not all pairs, however, are included in the
domain, because Ip is not necessarily equal
to ‚Ñù. Moreover, because the intervals Ip are
point dependent, the domain of the Ô¨Çow is
not even a product manifold. One would
be tempted to take the intersection of all
such intervals so as to work with a prod-
uct manifold given by Óàπtimes the smallest
interval Ip. Unfortunately, as we know from
elementary calculus, this (inÔ¨Ånite) intersec-
tion may consist of a single point. All that
can be said about the domain of the Ô¨Çow
is that it is an open subset of the Cartesian
product Óàπ√ó ‚Ñù. When the domain is equal
to this product manifold, the vector Ô¨Åeld
is said to be complete and the correspond-
ing Ô¨Çow is called a global Ô¨Çow. It can be
shown that if Óàπis compact, or if the vector
Ô¨Åeld is smooth and vanishes outside a com-
pact subset of Óàπ, the Ô¨Çow is necessarily
global.

348
9 DiÔ¨Äerentiable Manifolds
9.9.2
One-parameter Groups of
Transformations Generated by Flows
Given a point p0 ‚ààÓàπ, it is always possi-
ble to Ô¨Ånd a small enough neighborhood
U(p0) ‚äÇÓàπsuch that the intersection of all
the intervals Ip with p ‚ààU(p0) is an open
interval J containing the origin. For each
value s ‚ààJ, the Ô¨Çow ùõæ(s, p) can be regarded
as a map
ùõæs ‚à∂U(p0) ‚àí‚ÜíÓàπ,
(9.114)
deÔ¨Åned as
ùõæs(p) = ùõæ(s, p),
p ‚ààU(p0).
(9.115)
This map is clearly one-to-one, because
otherwise we would have two integral
curves intersecting each other, against the
statement of the fundamental theorem.
Moreover, again according to the funda-
mental theorem, this is a smooth map
with a smooth inverse over its image. The
inverse is, in fact, given by
ùõæ‚àí1
s
= ùõæ‚àís,
(9.116)
where ùõæ‚àís
is deÔ¨Åned over the image
ùõæs(U(p0)). Notice that ùõæ0 is the identity
map of U(p0). Finally, for the appropriate
range of values of s and r, we have the
composition law
ùõær ‚àòùõæs = ùõær+s.
(9.117)
The set of maps ùõæs is said to constitute the
one-parameter local pseudo-group gener-
ated by the vector Ô¨Åeld (or by its Ô¨Çow). If the
neighborhood U(p0) can be extended to the
whole manifold for some open interval J (no
matter how small), each map ùõæs is called a
transformation of Óàπ. In that case, we speak
of a one-parameter pseudo-group of trans-
formations of Óàπ. Finally, in the best of all
possible worlds, if J = ‚Ñùthe one-parameter
subgroup of transformations becomes ele-
vated to a one-parameter group of trans-
formations. This is an Abelian (i.e., com-
mutative) group, as is clearly shown by the
composition law (9.117). We may say that
every complete vector Ô¨Åeld generates a one-
parameter group of transformations of the
manifold.
The
converse
construction,
namely,
the generation of a vector Ô¨Åeld out of
a given one-parameter pseudo-group of
transformations, is also of interest. It can be
shown that every one-parameter pseudo-
group of transformations ùõæs is generated by
the vector Ô¨Åeld
V(p) = dùõæs(p)
ds
|s=0.
(9.118)
9.9.3
The Lie Derivative
We have learned that a vector Ô¨Åeld deter-
mines at least a one-parameter pseudo-
group in a neighborhood of each point of
the underlying manifold. For each value of
the parameter s within a certain interval
containing the origin, this neighborhood
is mapped diÔ¨Äeomorphically onto another
neighborhood. Having at our disposal a dif-
feomorphism, we can consider the pushed-
forward or pulled-back versions of tensors
of every type, including multivectors and
diÔ¨Äerential forms. Physically, these actions
represent how the various quantities are
convected (or dragged) by the Ô¨Çow. To elicit
a mental picture, we show in Figure 9.4 a
vector wp in a manifold as a small segment
‚àí‚Üí
pq (a small piece of a curve, say), and we
draw the integral curves of a vector Ô¨Åeld
V emerging from each of its end points, p
and q. These curves are everywhere tan-
gent to the underlying vector Ô¨Åeld V, which

9.9 The Lie Derivative
349
wp
w‚Ä≤
p
q
p‚Ä≤
q‚Ä≤
Œîs
Œîs
Figure 9.4
Dragging of a vector by a Ô¨Çow.
we do not show in the Ô¨Ågure. If s denotes
the (natural) parameter along these integral
curves, an increment of Œîs applied from
each of these points along the correspond-
ing integral curve, will result in two new
points p‚Ä≤ and q‚Ä≤, respectively. The (small)
segment ‚àí‚àí‚Üí
p‚Ä≤q‚Ä≤ can be seen as a vector w‚Ä≤,
which we regard as the convected counter-
part of wp as it is dragged by the Ô¨Çow of V
by an amount Œîs. If wp happens to be part
of a vector Ô¨Åeld W deÔ¨Åned in a neighbor-
hood of p‚Ä≤, so that wp = W(p), we have that
at the point p‚Ä≤ there is, in addition to the
dragged vector w‚Ä≤, a vector W(p‚Ä≤). There is
no reason why these two vectors should be
equal. The diÔ¨Äerence W(p‚Ä≤) ‚àíw‚Ä≤ (divided
by Œîs) gives us an idea of the meaning of
the Lie derivative of W with respect to V
at p‚Ä≤.
The idea behind the deÔ¨Ånition of the Lie
derivative of a tensor Ô¨Åeld with respect to
a given vector Ô¨Åeld at a point p is the fol-
lowing. We consider a small value s of the
parameter and convect the tensor Ô¨Åeld back
to s = 0 by using the appropriate pullback
or pushforward. This operation will, in par-
ticular, provide a value of the convected
tensor Ô¨Åeld at the point p. We then subtract
from this value the original value of the
Ô¨Åeld at that point (a legitimate operation,
because both tensors operate on the same
tangent and/or cotangent space), divide by
s and compute the limit as s ‚Üí0. To under-
stand how to calculate a Lie derivative, it
is suÔ¨Écient to make the deÔ¨Ånition explicit
for the case of functions, vector Ô¨Åelds, and
1-forms. The general case is then inferred
from these three basic cases, as we shall
demonstrate. We will also prove that the
term ‚Äúderivative‚Äù is justiÔ¨Åed. Notice that a
Lie derivative is deÔ¨Åned with respect to a
given vector Ô¨Åeld. It is not an intrinsic prop-
erty of the tensor Ô¨Åeld being diÔ¨Äerentiated.
The Lie derivative of a tensor Ô¨Åeld at a point
is a tensor of the same type.
9.9.3.1
The Lie Derivative of a Scalar
Let g ‚à∂Óàπ‚ÜíÓà∫be a mapping between two
manifolds and let f ‚à∂Óà∫‚Üí‚Ñùbe a function.
Recall that, according to (9.77), the pullback
of f by g is the map g‚àóf ‚à∂Óàπ‚Üí‚ÑùdeÔ¨Åned as
the composition
g‚àóf = f ‚àòg.
(9.119)
Let a (time-independent, for now) vec-
tor Ô¨Åeld V be deÔ¨Åned on Óàπand let
ùõæs ‚à∂U ‚ÜíÓàπdenote the action of its Ô¨Çow
on a neighborhood of a point p ‚ààÓàπ. If
a function f ‚à∂Óàπ‚Üí‚Ñùis deÔ¨Åned, we can
calculate
ùõæ‚àó
s f ‚à∂= f ‚àòùõæs.
(9.120)
The Lie derivative at the point p is given
by

350
9 DiÔ¨Äerentiable Manifolds
LV f (p) = lim
s‚Üí0
(ùõæ‚àó
s f )(p) ‚àíf (p)
s
= lim
s‚Üí0
f (ùõæs(p)) ‚àíf (p)
s
(9.121)
Thus, we obtain
LV f (p) = vp(f ).
(9.122)
In simple words, the Lie derivative of a
scalar Ô¨Åeld with respect to a given vec-
tor Ô¨Åeld coincides, at each point, with the
directional derivative of the function in the
direction of the Ô¨Åeld at that point.
9.9.3.2
The Lie Derivative of a Vector
Field
Vectors are pulled forward by mappings.
Thus, given the map g ‚à∂Óàπ‚ÜíÓà∫, to bring a
tangent vector from Óà∫back to Óàπ, we must
use the fact that g is invertible and that the
inverse is diÔ¨Äerentiable, such as when g is
a diÔ¨Äeomorphism. Let W ‚à∂Óà∫‚ÜíTÓà∫be a
vector Ô¨Åeld on Óà∫. The corresponding vec-
tor Ô¨Åeld on Óàπis then given by g‚àí1
‚àó‚àòW ‚àòg ‚à∂
Óàπ‚ÜíTÓàπ. Accordingly, the Lie derivative
of the vector Ô¨Åeld W with respect to the
vector Ô¨Åeld V, with Ô¨Çow ùõæs, at a point p ‚ààÓàπ
is deÔ¨Åned as
LVW(p) = lim
s‚Üí0
ùõæ‚àí1
s‚àó‚àòW ‚àòùõæs(p) ‚àíW(p)
s
.
(9.123)
It can be shown that the Lie derivative of a
vector Ô¨Åeld coincides with the Lie bracket
LVW = [V, W].
(9.124)
9.9.3.3
The Lie Derivative of a 1-form
As 1-forms are pulled back by a map, we
deÔ¨Åne the Lie derivative of the 1-form ùúî‚à∂
Óàπ‚ÜíT‚àóÓàπat the point p as
LVùúî(p) = lim
s‚Üí0
ùõæ‚àó
s ‚àòùúî‚àòùõæs(p) ‚àíùúî(p)
s
. (9.125)
9.9.3.4
The Lie Derivative of Arbitrary
Tensor Fields
It is clear that, by virtue of their deÔ¨Åni-
tion by means of limits, the Lie deriva-
tives deÔ¨Åned so far are linear operators. To
extend the deÔ¨Ånition of the Lie derivative to
tensor Ô¨Åelds of arbitrary order, we need to
make sure that the Leibniz rule with respect
to the tensor product is satisÔ¨Åed. Other-
wise, we would not have the right to use the
term ‚Äúderivative‚Äù to describe it. It is enough
to consider the case of a monomial such as
T = ùúî1 ‚äó¬∑ ¬∑ ¬∑ ‚äóùúîm ‚äóW1 ‚äó¬∑ ¬∑ ¬∑ ‚äóWn,
(9.126)
where ùúîi are m 1-forms and Wj are n vector
Ô¨Åelds. We deÔ¨Åne
LVT(p) = lim
s‚Üí0
ùõæ‚àó
s ‚àòùúî1 ‚àòùõæs(p) ‚äó¬∑ ¬∑ ¬∑ ‚äóùõæ‚àí1
s‚àó
‚àòW1 ‚àòùõæs(p) ‚äó¬∑ ¬∑ ¬∑ ‚àíT(p)
s
.
(9.127)
Let us verify the satisfaction of the Leibniz
rule for the case of the tensor product of a
1-form by a vector.
LV(ùúî‚äóW)(p) = lim
s‚Üí0
ùõæ‚àó
s ‚àòùúî‚àòùõæs(p) ‚äóùõæ‚àí1
s‚àó
‚àòW ‚àòùõæs(p) ‚äó‚àíùúî(p) ‚äóW(p)
s
. (9.128)
Subtracting and adding to the denominator
the expression ùúî(p) ‚äóùõæ‚àí1
s‚àó‚àòW ‚àòùõæs(p) the
Leibniz rule follows suit.
An important property of the Lie deriva-
tive is the following: The Lie derivative of a
diÔ¨Äerential form (of any order) commutes
with the exterior derivative, that is,
LV(dùúî) = d(LVùúî),
(9.129)
for all vector Ô¨Åelds V and for all diÔ¨Äerential
forms ùúî.

Further Reading
351
9.9.3.5
The Lie Derivative in Components
Taking advantage of the Leibniz rule, it is
not diÔ¨Écult to calculate the components
of the Lie derivative of a tensor in a given
coordinate system xi, provided the com-
ponents of the Lie derivative of the base
vectors ùúï‚àïùúïxi and the base 1-forms dxi
are known. A direct application of the for-
mula (9.36) for the components of the Lie
bracket, yields
LV
( ùúï
ùúïxi
)
= ‚àíùúïV k
ùúïxi
ùúï
ùúïxk .
(9.130)
To obtain the Lie derivative of dxi, we recall
that the action of dxi (as a covector) on
ùúï‚àïùúïxj is simply ùõøi
j, whose Lie derivative van-
ishes. This action can be seen as the con-
traction of their tensor product. We obtain,
therefore,
0 = LV
‚ü®
dxi, ùúï
ùúïxj
‚ü©
=
‚ü®
dxi, ‚àíùúïV k
ùúïxj
ùúï
ùúïxk
‚ü©
+
‚ü®
Lvdxi, ùúï
ùúïxj
‚ü©
,
(9.131)
whence
LVdxi = ùúïV i
ùúïxk dxk.
(9.132)
Further Reading
Some general treatises on DiÔ¨Äerential
Geometry:
Chern, S.S., Chern, W.H., and Lam, K.S. (1999)
Lectures on DiÔ¨Äerential Geometry, World
ScientiÔ¨Åc.
Kobayashi, S. and Nomizu, K. (1996) Foundations
of DiÔ¨Äerential Geometry, Wiley Classics
Library Edition.
Lee, J.M. (2003) Introduction to Smooth
Manifolds, Springer.
Sternberg, S. (1983) Lectures on DiÔ¨Äerential
Geometry, 2nd edn, Chelsea Publishing
Company.
Warner, F.W. (1983) Foundations of DiÔ¨Äerentiable
Manifolds and Lie Groups, Springer.
Some
books
that
emphasize
physi-
cal applications or deal with particular
physical theories in a geometric way are
Abraham, R. and Marsden, J.E. (2008)
Foundations of Mechanics, 2nd edn, AMS
Chelsea Publishing.
Arnold, V.I. (1978) Mathematical Methods of
Classical Mechanics, Springer.
Choquet-Bruhat, Y., de Witt-Morette, C., and
Dillard-Beck, M. (1977) Analysis, Manifolds
and Physics, North-Holland.
Frankel, T. (2004) The Geometry of Physics: An
Introduction, 2nd edn, Cambridge University
Press.
Misner, W., Thorne, K.S., and Wheeler, J.A.
(1973) Gravitation, W H Freeman and
Company.
Much of the material in this article is
reproduced, with permission, from
Epstein, M. (2010) The Geometrical Language of
Continuum Mechanics, Cambridge University
Press.


353
10
Topics in DiÔ¨Äerential Geometry
Marcelo Epstein
10.1
Integration
10.1.1
Integration of n-Forms in ‚Ñùn
The simplest n-dimensional manifold is ‚Ñùn
itself with the standard topology and the
standard notion of diÔ¨Äerentiability. Accord-
ingly, we present the standard notion of
integration over a domain of ‚Ñùn in terms of
diÔ¨Äerential forms so as to be able to extend
this notion to arbitrary manifolds.
Let x1, ‚Ä¶ , xn be the standard global chart
of ‚Ñùn, and let ùúîbe a smooth n-form deÔ¨Åned
over some open set Óà∞‚äÇ‚Ñùn. There exists,
then, a smooth function f ‚à∂Óà∞‚Üí‚Ñùsuch
that
ùúî= f dx1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxn.
(10.1)
For any regular domain of integration Óà≠‚äÇ
Óà∞, we deÔ¨Åne
‚à´Óà≠
ùúî= ‚à´‚à´¬∑ ¬∑ ¬∑ ‚à´
‚èü‚èû‚èû‚èû‚èû‚èü‚èû‚èû‚èû‚èû‚èü
Óà≠
f dx1dx2 ¬∑ ¬∑ ¬∑ dxn,
(10.2)
where the right-hand side is the ordinary n-
fold Riemann integral in ‚Ñùn.
It is important to check that this deÔ¨Åni-
tion is independent of the coordinate sys-
tem adopted in Óà∞. For this purpose, let
ùúô‚à∂Óà∞‚àí‚Üí‚Ñùn
(10.3)
be a coordinate transformation expressed
in components as the n smooth functions
x1, ‚Ä¶ , xn ÓÇ∂‚Üíy1(x1, ‚Ä¶ , xn), ‚Ä¶ , yn(x1, ‚Ä¶ , xn).
(10.4)
Recall that for (10.4) to qualify as a coordi-
nate transformation, the Jacobian determi-
nant
J = det
[ ùúï(y1, ‚Ä¶ , yn)
ùúï(x1, ‚Ä¶ , xn)
]
,
(10.5)
must be nonzero throughout Óà∞. For deÔ¨Å-
niteness, we will assume that it is strictly
positive (so that the change of coordinates
is orientation preserving). According to
the formulas of transformation of variables
under a multiple Riemann integral, we
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

354
10 Topics in DiÔ¨Äerential Geometry
must have
‚à´‚à´¬∑ ¬∑ ¬∑ ‚à´
‚èü‚èû‚èû‚èû‚èû‚èü‚èû‚èû‚èû‚èû‚èü
Óà≠
f (xi) dx1 ¬∑ ¬∑ ¬∑ dxn
= ‚à´‚à´¬∑ ¬∑ ¬∑ ‚à´
‚èü‚èû‚èû‚èû‚èû‚èü‚èû‚èû‚èû‚èû‚èü
Óà≠
f (xi(yj)) J‚àí1dy1 ¬∑ ¬∑ ¬∑ dyn.
(10.6)
But, because ùúîis an n-form in an n-
dimensional manifold, its representation in
the new coordinate system is precisely
ùúî= f (xi(yj)) J‚àí1dy1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdyn,
(10.7)
which shows that the deÔ¨Ånition (10.2) is
indeed independent of the coordinate sys-
tem adopted in Óà∞.
A more fruitful way to exploit the coordi-
nate independence property is to regard ùúô‚à∂
Óà∞‚Üí‚Ñùn not as a mere coordinate trans-
formation but as an actual change of the
domain of integration. In this case, the
transformation formula is interpreted read-
ily in terms of the pullback of ùúîas
‚à´ùúô(Óà≠)
ùúî= ‚à´Óà≠
ùúô‚àó(ùúî),
(10.8)
for every n-form ùúîdeÔ¨Åned over an open set
containing ùúô(Óà≠).
10.1.2
Integration of Forms on Oriented
Manifolds
Let Óàπbe an oriented manifold of dimen-
sion m and let (ÓâÅ, ùúì) be a (consistently)
oriented chart. The integral of an m-form
ùúîover ÓâÅis deÔ¨Åned as
‚à´ÓâÅ
ùúî= ‚à´ùúì(ÓâÅ)
(
ùúì‚àí1)‚àóùúî.
(10.9)
Notice that the right-hand side is a stan-
dard Riemann (or Lebesgue) integral of a
function in ‚Ñùm, according to (10.2). In other
words, given an m-form deÔ¨Åned over the
domain of a chart in an m-dimensional
manifold, we simply pull back this form to
the codomain of the chart (an open subset
of ‚Ñùm) and integrate. The result is indepen-
dent of the chart used.
To integrate over a domain covered
by more than one chart, we need to use
the concept of partition of unity, whose
detailed presentation we omit. BrieÔ¨Çy and
imprecisely stated, a partition of unity is
a collection of real-valued nonnegative
smooth functions, one for each of the
members of an open cover and vanishing
outside it. Moreover, we assume that the
cover is locally Ô¨Ånite, in the sense that
each point of the manifold belongs to
only a Ô¨Ånite number of members of the
cover. Finally, the (Ô¨Ånite) sum of all these
functions at each point is equal to 1 (hence
the name). It can be shown that partitions
of unity exist provided the manifold is
paracompact, namely, it admits a locally
Ô¨Ånite cover. Denoting by ùúôi the functions
making up the partition of unity, we deÔ¨Åne
the integral as
‚à´Óàπ
ùúî=
‚àë
i ‚à´Óàπ
ùúôiùúî,
(10.10)
where the integrand on the right-hand side
is just the product of a function times a
diÔ¨Äerential form. Each integral on the right-
hand side is well deÔ¨Åned by (10.9).
For the deÔ¨Ånition implied by (10.10) to
make sense, we must prove that the result
is independent of the choice of charts and
of the choice of partition of unity. This can
be done quite straightforwardly by express-
ing each integral of the right-hand side of
one choice in terms of the quantities of the
second choice, and vice versa.

10.2 Fluxes in Continuum Physics
355
10.1.3
Stokes‚Äô Theorem
The boundary of an m-dimensional Óàπwith
boundary will be denoted by ùúïÓàπ, which we
consider as a manifold of dimension m ‚àí1.
For example, Óàπmay be a closed ball in ‚Ñùm
and ùúïÓàπthe bounding spherical surface.
The boundary of an oriented manifold can
be consistently oriented. Given an (m ‚àí1)-
form ùúîon Óàπ, it makes sense to calculate its
integral over the (oriented) boundary ùúïÓà∞.
Stokes‚Äô theorem asserts that
‚à´ùúïÓà∞
ùúî= ‚à´Óà∞
dùúî.
(10.11)
We omit the proof and limit ourselves to
remark that this elegant formula encom-
passes all the integral theorems of ordinary
vector calculus.
10.2
Fluxes in Continuum Physics
One of the basic notions of Continuum
Physics is that of an extensive property, a
term that describes a property that may be
assigned to subsets of a given universe, such
as the mass of various parts of a material
body, the electrical charge enclosed in a
certain region of space, and so on. Mathe-
matically speaking, therefore, an extensive
property is expressed as a real-valued set
function p, whose argument ranges over
subsets Óàæof a universe ÓâÅ. It is usually
assumed, on physical grounds, that the
function p is additive, namely,
p(Óàæ1 ‚à™Óàæ2) = p(Óàæ1) + p(Óàæ2),
whenever
Óàæ1 ‚à©Óàæ2 = ‚àÖ.
(10.12)
With proper regularity assumptions, addi-
tivity means that, from the mathematical
standpoint, p is a measure in ÓâÅ.
In the appropriate space‚Äìtime con-
text, the balance of an extensive property
expresses a relation between the rate of
change of the property in a given region
and the causes responsible for that change.
Of particular importance is the idea of Ô¨Çux
of the property through the boundary of a
region, which is an expression of the rate of
change of the property as a result of inter-
action with other regions. It is a common
assumption that the Ô¨Çux between regions
takes place through, and only through,
common boundaries. In principle, the
Ô¨Çux is a set function on the boundaries of
regions. In most physical theories, how-
ever, this complicated dependence can be
greatly simpliÔ¨Åed by means of the so-called
Cauchy postulates and Cauchy‚Äôs theorem.
10.2.1
Extensive-Property Densities
We will identify the universe ÓâÅas an m-
dimensional diÔ¨Äerentiable manifold. Under
appropriate continuity assumptions, a set
function such as the extensive property p is
characterized by a density. Physically, this
means that the property at hand cannot
be concentrated on subsets of dimension
lower than m. More speciÔ¨Åcally, we assume
that the density ùúåof the extensive property
p is a smooth m-form on ÓâÅsuch that
p(Óàæ) = ‚à´Óàæ
ùúå,
(10.13)
for any subset Óàæ‚äÇÓâÅfor which the integral
is deÔ¨Åned. Clearly, the additivity condition
(10.12) is satisÔ¨Åed automatically.
We introduce the time variable t as if
space‚Äìtime were just a product mani-
fold ‚Ñù√ó ÓâÅ. In fact, this trivialization is
observer dependent, but it will serve for
our present purposes. The density ùúåof the
extensive property p should, accordingly,
be conceived as a function ùúå= ùúå(t, x),

356
10 Topics in DiÔ¨Äerential Geometry
where x ‚ààÓâÅ. Notice that, because for
Ô¨Åxed x and variable t, ùúåbelongs to the same
vector space Œõm (T‚àó
x ÓâÅ), it makes sense to
take the partial derivative with respect to t
to obtain the new m-form
ùõΩ= ùúïùúå
ùúït ,
(10.14)
deÔ¨Åned on ÓâÅ. For a Ô¨Åxed (i.e., time-
independent) region Óàæ, we may write
dp(Óàæ)
dt
= ‚à´Óàæ
ùõΩ.
(10.15)
In other words, the integral of the m-form
ùõΩover a Ô¨Åxed region measures the rate of
change of the content of the property p
inside that region.
10.2.2
Balance Laws, Flux Densities, and Sources
In the classical setting of continuum
physics, it is assumed that the change of
the content of a smooth extensive property
p within a Ô¨Åxed region Óàæcan be attributed
to just two causes: (i) the rate at which the
property is produced (or destroyed) within
Óàæby the presence of sources and sinks and
(ii) the rate at which the property enters or
leaves Óàæthrough its boundaries, namely,
the Ô¨Çux of p. For the sake of deÔ¨Åniteness,
in this section we adopt the convention
that the production rate is positive for
sources (rather than sinks) and that the
Ô¨Çux is positive when there is a an outÔ¨Çow
(rather than an inÔ¨Çow) of the property. The
balance equation for the extensive property
p states that the rate of change of p in a
Ô¨Åxed region Óàæequals the diÔ¨Äerence between
the production rate and the Ô¨Çux. A good
physical example is the balance of internal
energy in a rigid body due to volumetric
heat sources and heat Ô¨Çux through the
boundaries.
As we have assumed continuity for p as a
set function, we will do the same for both
the production and the Ô¨Çux. As a result,
we postulate the existence of an m-form
s, called the source density such that the
production rate in a region Óàæis given by the
integral
‚à´Óàæ
s.
(10.16)
Just as ùúåitself, the m-form s is deÔ¨Åned
over all of ÓâÅand is independent of Óàæ.
Thus, from the physical point of view, we
are assuming that the phenomena at hand
can be described locally. This assumption
excludes interesting phenomena, such as
internal actions at a distance or surface-
tension eÔ¨Äects.
As far as the Ô¨Çux term is concerned, we
also assume that it is a continuous func-
tion of subsets of the boundary ùúïR. We pos-
tulate the existence, for each region Óàæ, of
a smooth (m ‚àí1)-form ùúèÓàæ, called the Ô¨Çux
density, such that the Ô¨Çux of p is given by
‚à´ùúïÓàæ
ùúèÓàæ.
(10.17)
Thus, the classical balance law of the
property p assumes the form
‚à´Óàæ
ùõΩ= ‚à´Óàæ
s ‚àí‚à´ùúïÓàæ
ùúèÓàæ.
(10.18)
An equation of balance is said to be a
conservation law if both s and ùúèÓàævanish
identically.
10.2.3
Flux Forms and Cauchy‚Äôs Formula
We note that (beyond the obvious fact that
ùõΩand s are m-forms, whereas ùúèÓàæis an (m ‚àí
1)-form), there is an essential complication
peculiar to the Ô¨Çux densities ùúèÓàæ. Indeed,
in order to specify the Ô¨Çux for the various

10.2 Fluxes in Continuum Physics
357
regions of interest, it seems that one has
to specify the form ùúèÓàæfor each and every
region Óàæ. In other words, while the rate of
change of the property and the production
term are speciÔ¨Åed by forms whose domain
(for each time t) is the entire space ÓâÅ, the
Ô¨Çux term must be speciÔ¨Åed by means of a
set function, whose domain is the collection
of all regions. We refer to the set function
ÓàæÓÇ∂‚ÜíùúèÓàæas a system of Ô¨Çux densities. Con-
sider, for example, a point x ‚ààÓâÅbelong-
ing simultaneously to the boundaries of two
diÔ¨Äerent regions. Clearly, we do not expect
that the Ô¨Çux density will be the same for
both. The example of suntanning should
be suÔ¨Éciently convincing in this regard.
Consider, however, the following particular
case. Let the natural inclusion map
ùúÑ‚à∂ùúïÓàæ‚àí‚ÜíÓâÅ
(10.19)
be deÔ¨Åned by
ùúÑ(x) = x
‚àÄx ‚ààùúïÓàæ.
(10.20)
Notice that this formula makes sense,
because ùúïÓàæ‚äÇÓâÅ. Moreover, the map ùúÑ
is smooth. It can, therefore, be used to
pullback forms of any order on ÓâÅto forms
of the same order on ùúïÓàæ. In particular, we
can deÔ¨Åne
‚à´ùúïÓàæ
ùúô= ‚à´ùúïÓàæ
ùúÑ‚àó(ùúô),
(10.21)
for any form ùúôon ÓâÅ. Let us now assume
the existence of a globally deÔ¨Åned (m ‚àí1)-
Ô¨Çux form Œ¶ on ÓâÅand let us deÔ¨Åne the
associated system of Ô¨Çux densities by
means of the formula
ùúèÓàæ= ùúÑ‚àó
ùúïÓàæ(Œ¶),
(10.22)
where we use the subscript ùúïÓàæto empha-
size the fact that each region requires its
own inclusion map. Equation (10.22) is
known as Cauchy‚Äôs formula. Clearly, this is
a very special system of Ô¨Çux densities (just
as a conservative force Ô¨Åeld is a special vec-
tor Ô¨Åeld derivable from a single scalar Ô¨Åeld).
Nevertheless, it is one of the fundamental
results of classical Continuum Mechanics
that, under rather general assumptions
(known as Cauchy‚Äôs postulates), every sys-
tem of Ô¨Çux densities can be shown to derive
from a unique Ô¨Çux form using Cauchy‚Äôs for-
mula (10.22). We will omit the general proof
of this fact, known as Cauchy‚Äôs theorem.
In less technical terms, Cauchy‚Äôs formula
is the direct result of assuming that the Ô¨Çux
is given by a single 2-form deÔ¨Åned over the
three-dimensional domain of the body. The
fact that one and the same form is to be used
for a given location, and integrated over the
given boundary, is trivially seen to imply
(and generalize) the linear dependence of
the Ô¨Çux on the normal to the boundary, as
described in the standard treatments.
10.2.4
DiÔ¨Äerential Expression of the Balance Law
Assuming the existence of a Ô¨Çux form Œ¶,
the general balance law (10.18) can be writ-
ten as
‚à´Óàæ
ùõΩ= ‚à´Óàæ
s ‚àí‚à´ùúïÓàæ
ùúÑ‚àó
ùúïÓàæ(Œ¶).
(10.23)
Using Stokes‚Äô theorem (10.11), we can
rewrite the last term as
‚à´ùúïÓàæ
ùúÑ‚àó
ùúïÓàæ(Œ¶) = ‚à´Óàæ
dŒ¶,
(10.24)
where the dependence on ùúïÓàæhas evapo-
rated. Using this result, we write (10.23) as
‚à´Óàæ
ùõΩ= ‚à´Óàæ
s ‚àí‚à´Óàæ
dŒ¶.
(10.25)
As this balance law should be valid for arbi-
trary Óàæ, and because the forms ùõΩ, s, and Œ¶

358
10 Topics in DiÔ¨Äerential Geometry
are deÔ¨Åned globally and independently of
the region of integration, we obtain
ùõΩ= s ‚àídŒ¶.
(10.26)
This equation is known as the diÔ¨Äerential
expression of the general balance law.
10.3
Lie Groups
10.3.1
DeÔ¨Ånition
Recall that a group is a set Óà≥endowed
with a binary associative internal opera-
tion, called group multiplication or group
product, which is usually indicated by sim-
ple apposition, namely, if g, h ‚ààÓà≥then the
product is gh ‚ààÓà≥. Associativity means that
(gh)k = g(hk), for all g, h, k ‚ààÓà≥. Moreover,
there exists an identity element e ‚ààÓà≥such
that eg = ge = g for all g ‚ààÓà≥. Finally, for
each g ‚ààÓà≥there exists an inverse g‚àí1 ‚ààÓà≥
such that gg‚àí1 = g‚àí1g = e. The identity can
be shown to be unique, and so is also the
inverse of each element of the group. If
the group operation is also commutative,
namely, if gh = hg for all g, h ‚ààÓà≥, the group
is said to be commutative or Abelian. In
this case, it is customary to call the oper-
ation group addition and to indicate it as:
g + h. The identity is then called the zero
element and is often denoted as 0. Finally,
the inverse of g is denoted as ‚àíg. This nota-
tion is easy to manipulate as it is reminis-
cent of the addition of numbers, which is
indeed a particular case.
A subgroup of a group Óà≥is a subset Óà¥‚äÇÓà≥
closed under the group operations of mul-
tiplication and inverse. Thus, a subgroup is
itself a group.
Given two groups, Óà≥1 and Óà≥2, a group
homomorphism is a map ùúô‚à∂Óà≥1 ‚ÜíÓà≥2 that
preserves the group multiplication, namely,
ùúô(gh) = ùúô(g) ùúô(h)
‚àÄg, h ‚ààÓà≥1,
(10.27)
where the multiplications on the left- and
right-hand sides are, respectively, the group
multiplications of Óà≥1 and Óà≥2.
The group structure is a purely algebraic
concept, whereby nothing is assumed as far
as the nature of the underlying set is con-
cerned. The concept of a Lie group arises
from making such an assumption. More
speciÔ¨Åcally, a Lie group is a (smooth) mani-
fold Óà≥with a group structure that is compat-
ible with the diÔ¨Äerential structure, namely,
such that the multiplication Óà≥√ó Óà≥‚ÜíÓà≥and
the inversion Óà≥‚ÜíÓà≥are smooth maps.
A homomorphism ùúôbetween two Lie
groups is called a Lie-group homomorphism
if ùúôis C‚àû. If ùúôhappens to be a diÔ¨Äeo-
morphism, we speak of a Lie-group isomor-
phism. An isomorphism of a Lie group with
itself is called a Lie-group automorphism.
Let V be an n-dimensional vector space.
The collection L(V, V) of all linear oper-
ators from V to V can be considered as
a diÔ¨Äerentiable manifold. Indeed, Ô¨Åxing a
basis in V, we obtain a global chart in ‚Ñùn2.
The collection GL(V) of all invertible linear
operators, with the operation of multipli-
cation as the composition of linear opera-
tors, can be shown to be a Lie group, known
as the general linear group of V. The gen-
eral linear group of ‚Ñùn is usually denoted
by GL(n; ‚Ñù). Its elements are the nonsingu-
lar square matrices of order n, with the unit
matrix I acting as the group unit. Its vari-
ous Lie subgroups are known as the matrix
groups. The group operation is the usual
matrix multiplication.
10.3.2
Group Actions
Let Óà≥be a group (not necessarily a Lie
group) and let X be a set (not necessarily
a diÔ¨Äerentiable manifold). We say that the

10.3 Lie Groups
359
group Óà≥acts on the right on the set X if for
each g ‚ààÓà≥there is a map Rg ‚à∂X ‚ÜíX such
that: (i) Re(x) = x for all x ‚ààX, where e is
the group identity; (ii) Rg‚àòRh = Rhg for all
g, h ‚ààÓà≥. When there is no room for confu-
sion, we also use the notation xg for Rg(x).
Each of the maps Rg is a bijection of X.
Moreover, Rg‚àí1 = (Rg)‚àí1. The orbit through
x ‚ààX is the subset xÓà≥of X consisting of
all the elements of X of the form xg, where
g ‚ààÓà≥.
The action of Óà≥on X is said to be eÔ¨Äective
if the condition Rg(x) = x for every x ‚ààX
implies g = e. The action is free if Rg(x) = x
for some x ‚ààX implies g = e. Finally, the
action is transitive if for every x, y ‚ààX,
there exists g ‚ààÓà≥such that Rg(x) = y.
In a completely analogous manner, we
can say that Óà≥acts on the left on X if for
each g ‚ààÓà≥there is a map Lg ‚à∂X ‚ÜíX such
that (i) Le(x) = x for all x ‚ààX, where e is
the group identity; (ii) Lg‚àòLh = Lgh for all
g, h ‚ààÓà≥. The order of the composition is
the essential diÔ¨Äerence between a right and
a left action. We may also use the notation
gx for Lg(x).
The notion of group action can naturally
be applied when Óà≥is a Lie group. In this
instance, a case of particular interest is that
for which the set on which Óà≥acts is a dif-
ferentiable manifold and the induced bijec-
tions are transformations of this manifold.
A transformation of a manifold Óàπis a dif-
feomorphism ùúô‚à∂Óàπ‚ÜíÓàπ. The deÔ¨Ånition
of the action is then supplemented with a
smoothness condition. More explicitly, A
Lie group Óà≥is said to act on the right on a
manifold Óàπif
1. Every element g ‚ààÓà≥induces a
transformation Rg ‚à∂Óàπ‚ÜíÓàπ.
2. Rg‚àòRh = Rhg, namely, (ph)g = p(hg) for
all g, h ‚ààÓà≥and p ‚ààÓàπ.
3. The right action R ‚à∂Óà≥√ó Óàπ‚ÜíÓàπis a
smooth map. In other words, Rg(p) is
diÔ¨Äerentiable in both variables (g and p).
With these conditions, the Lie group Óà≥is
also called a Lie group of transformations
of Óàπ. Just as in the general case, we have
used the alternative notation pg for Rg(p),
with p ‚ààÓàπ, wherever convenient. A simi-
lar deÔ¨Ånition can be given for the left action
of a Lie group on a manifold.
If e is the group identity, then Re
and Le are the identity transformations
of Óàπ. Indeed, because a transforma-
tion is an invertible map, every point
p ‚ààÓàπcan be expressed as qg for some
q ‚ààÓàπand some g ‚ààÓà≥. Using Prop-
erty (2) of the right action, we have
Re(p) = pe = (qg)e = q(ge) = qg = p,
with
a similar proof for the left action.
It is convenient to introduce the follow-
ing (useful, though potentially confusing)
notation. We denote the right action as a
map from Óà≥√ó Óàπto Óàπby the symbol R.
Thus, R = R(g, p) has two arguments, one
in the group and the other in the mani-
fold. Fixing, therefore, a particular element
g in the group, we obtain a function of the
single variable x, which we have already
denoted by Rg ‚à∂Óàπ‚ÜíÓàπ. But we can also
Ô¨Åx a particular element p in the manifold
and thus obtain another function of the sin-
gle variable g. We will denote this func-
tion by Rp ‚à∂Óà≥‚ÜíÓàπ. A similar scheme of
notation can be adopted for a left action L.
Notice that the image of Rp (respectively Lp)
is nothing but the orbit pÓà≥(respectively Óà≥p).
The potential for confusion arises when the
manifold Óàπhappens to coincide with the
group Óà≥, as described below. Whenever an
ambiguous situation arises, we will resort to
the full action function of two variables.
Recall that a Lie group is both a group
and a manifold. Thus, it is not surprising

360
10 Topics in DiÔ¨Äerential Geometry
that every Lie group Óà≥induces two canon-
ical groups of transformations on itself,
one by right action and one by left action,
called,
respectively,
right
translations
and left translations of the group. They
are deÔ¨Åned, respectively, by Rg(h) = hg
and Lg(h) = gh, with g, h ‚ààÓà≥, where the
right-hand sides are given by the group
multiplication itself. For this reason, it
should be clear that these actions are both
free (and, hence, eÔ¨Äective) and transitive.
10.3.3
One-Parameter Subgroups
A one-parameter subgroup of a Lie group Óà≥
is a diÔ¨Äerentiable curve
ùõæ‚à∂‚Ñù‚àí‚ÜíÓà≥
t ÓÇ∂‚Üíg(t),
(10.28)
satisfying
g(0) = e,
(10.29)
and
g(t1) g(t2) = g(t1 + t2),
‚àÄt1, t2 ‚àà‚Ñù.
(10.30)
If the group Óà≥acts (on the left, say) on a
manifold Óàπ, the composition of this action
with a one-parameter subgroup determines
a one-parameter group of transformations
of Óàπ, namely,
ùõæt(p) = Lg(t)(p)
p ‚ààÓàπ.
(10.31)
We know that associated with this Ô¨Çow,
there exists a unique vector Ô¨Åeld vùõæ. More
precisely, we have
vùõæ(p) = dùõæt(p)
dt
||||t=0
.
(10.32)
Fixing the point p, we obtain the map Lp
from the group to the manifold. The image
of the curve ùõæunder this map is obtained by
composition as
t ÓÇ∂‚ÜíLp(g(t)) = L(g(t), p) = Lg(t)(p) = ùõæt(p),
(10.33)
where we have used (10.31). In other
words, the image of the curve ùõæ(deÔ¨Åning
the one-parameter subgroup) by the map
Lp is nothing but the integral curve of the
Ô¨Çow passing through p. By deÔ¨Ånition of
derivative of a map between manifolds, we
conclude that the tangent g to the one-
parameter subgroup ùõæat the group identity
e is mapped by Lp‚àóto the vector vùõæ(p)
vùõæ(p) = (Lp‚àó
)
e g.
(10.34)
This means that a one-parameter subgroup
g(t) appears to be completely characterized
by its tangent vector g at the group identity.
We will shortly conÔ¨Årm this fact more fully.
The vector Ô¨Åeld induced on Óàπby a one-
parameter subgroup is called the funda-
mental vector Ô¨Åeld associated with the cor-
responding vector g at the group identity.
Let us now identify the manifold Óàπwith
the group Óà≥itself. In this case, we have,
as already discussed, two canonical actions
giving rise to the left and right translations
of the group. We want to reinterpret (10.34)
in this particular case. For this purpose, and
to avoid the notational ambiguity alluded to
above, we restore the fully Ô¨Çedged notation
for the action as a function of two variables.
We thus obtain
vùõæ(h) =
(ùúïL(g, h)
ùúïg
)
g=e
g.
(10.35)
Notice that, somewhat puzzlingly, but con-
sistently, this can also be written as
vùõæ(h) = (Rh‚àó
)
e g.
(10.36)
Thus, when deÔ¨Åning the action of a one-
parameter subgroup from the left, it is the

10.4 Fiber Bundles
361
right action whose derivative delivers the
corresponding vector Ô¨Åeld, and vice versa.
10.3.4
Left- and Right-Invariant Vector Fields on a
Lie Group
A vector Ô¨Åeld v ‚à∂Óà≥‚ÜíTÓà≥is said to be left
invariant if
v(Lgh) = Lg‚àóv(h),
‚àÄg, h ‚ààÓà≥.
(10.37)
In other words, vectors at one point are
dragged to vectors at any other point by
the derivative of the appropriate left trans-
lation. A similar deÔ¨Ånition, but replacing
L with R, applies to right-invariant vector
Ô¨Åelds.
A vector Ô¨Åeld is left invariant if, and only
if,
v(g) =
(
Lg‚àó
)
e v(e),
‚àÄg ‚ààÓà≥.
(10.38)
Another way of expressing this result is
by saying that there exists a one-to-one
correspondence between the set of left- (or
right-) invariant vector Ô¨Åelds on Óà≥and the
tangent space TeÓà≥at the group identity.
This correspondence is linear. Moreover,
one can show that the Lie bracket of two
left- (right-) invariant vector Ô¨Åelds is itself
left (right) invariant. The set ùî§of left-
invariant vector Ô¨Åelds (or, equivalently, the
tangent space TeÓà≥) with the Lie bracket
operation is called the Lie algebra of the
group Óà≥. From an intuitive point of view,
the elements of the Lie algebra of a Lie
group represent inÔ¨Ånitesimal approxima-
tions, which Sophus Lie himself called
inÔ¨Ånitesimal generators of the elements
of the group. Although the inÔ¨Ånitesimal
generators are in principle commutative
(sum of vectors), the degree of noncom-
mutativity of the actual group elements is
captured, to Ô¨Årst order, by the Lie bracket.
10.4
Fiber Bundles
10.4.1
Introduction
Fiber bundles are diÔ¨Äerentiable manifolds
with extra structure. Its points have, as it
were, a double allegiance ‚Äì not only to the
manifold itself but also to a smaller entity
called a Ô¨Åber. We have already encountered
two important instances of Ô¨Åber bundles
that clearly exhibit this feature: the tangent
and cotangent bundles of a manifold. The
property of belonging to a speciÔ¨Åc Ô¨Åber is,
in those two examples, represented by the
existence of a projection map. Given a tan-
gent vector or a covector, these maps tell us
to which point of the base manifold they are
attached. Moreover, we have found in those
two cases that, once a chart (ÓâÅ, ùúô) is chosen
in the original manifold, a chart for the bun-
dle can be constructed. EÔ¨Äectively, there-
fore, the part of the bundle sitting above ÓâÅ
becomes assimilated to the Cartesian prod-
uct of ÓâÅand the typical Ô¨Åber. In the case of
the tangent bundle, for example, the chunk
sitting above ÓâÅis ùúè‚àí1(ÓâÅ), while the typi-
cal Ô¨Åber is ‚Ñùm. These two properties (exis-
tence of a projection and local equivalence
to a Cartesian product) are the two essential
ingredients of the deÔ¨Ånition of a general
Ô¨Åber bundle. A third ingredient consists
of the permitted transition functions along
the Ô¨Åbers, a degree of freedom that is con-
trolled by a given group of transformations
of the typical Ô¨Åber.
10.4.2
DeÔ¨Ånition
A Ô¨Åber bundle with base manifold ÓàÆ, typical
Ô¨Åber manifold Óà≤and structure group Óà≥, is a
manifold ÓàØand a smooth surjective bundle-
projection map ùúã‚à∂ÓàØ‚ÜíÓàÆsuch that there

362
10 Topics in DiÔ¨Äerential Geometry
exists an open covering ÓâÅùõºof ÓàÆand respec-
tive local trivializations
ùúìùõº‚à∂ùúã‚àí1(ÓâÅùõº) ‚àí‚ÜíÓâÅùõº√ó Óà≤,
(10.39)
with the property ùúã= pr1‚àòùúìùõº, as illustrated
in the following commutative diagram:
ùúã‚àí1(ÓâÅùõº)

ùúìùõº
ÓâÅùõº√ó F

pr1
ÓâÅùõº


ùúã
(10.40)
Moreover, as illustrated in Figure 10.1,
whenever b ‚ààÓâÅùõº
‚ãÇÓâÅùõΩ‚â†‚àÖ, the transition
maps
ÃÉùúìùõΩ,ùõº(b) = ÃÉùúìùõº,b‚àòÃÉùúì‚àí1
ùõΩ,b belong to the
structure group Óà≥and depend smoothly on
position throughout the intersection.
Consider now, for the same ÓàØ, ÓàÆ, Óà≤,
ùúã, and Óà≥, a diÔ¨Äerent open covering ÓâÇùõΩ
with local trivializations ùúôùõΩ. We say that it
deÔ¨Ånes the same Ô¨Åber bundle as before if,
on nonvanishing intersections, the transi-
tion maps ÃÉùúìùõº,b‚àòÃÉùúô‚àí1
ùõΩ,b belong to the structure
group Óà≥and depend smoothly on position
b throughout the intersection. The two
trivializations are said to be compatible.
In this sense, we can say that the union of
the two trivialization coverings becomes
itself a new trivialization covering of the
Ô¨Åber bundle. When there is no room for
confusion, a Ô¨Åber bundle is denoted as a
pair (ÓàØ, ùúã) indicating just the total space
and the projection. An alternative notation
is ùúã‚à∂ÓàØ‚ÜíÓàÆ. A more complete notation
would be (ÓàØ, ùúã, ÓàÆ, Óà≤, Óà≥).
The fundamental existence theorem of
Ô¨Åber bundles states that, given the mani-
folds ÓàÆand Óà≤and a Lie group Óà≥acting eÔ¨Äec-
tively to the left on Óà≤, and given, moreover,
an open covering ÓâÅùõºof ÓàÆand a smooth
assignment of an element of Óà≥to each point
in every nonvanishing intersection of the
covering, there exists a Ô¨Åber bundle (ÓàØ, ùúã)
with local trivializations based on that cov-
ering and with the assigned elements of Óà≥
as transition maps. Furthermore, any two
bundles with this property are equivalent.
An important application of the funda-
mental existence theorem is that, given a
bundle (ÓàØ, ùúã, ÓàÆ, Óà≤, Óà≥), we can associate to
it other bundles with the same base man-
ifold and the same structure group, but
with diÔ¨Äerent typical Ô¨Åber Óà≤‚Ä≤, in a precise
way. Indeed, we can choose a trivialization
covering of the given bundle, calculate the
transition maps, and then deÔ¨Åne the asso-
ciated bundle (ÓàØ‚Ä≤, ùúã‚Ä≤, ÓàÆ, Óà≤‚Ä≤, Óà≥), modulo an
equivalence, by means of the assertion of
the fundamental theorem. A case of partic-
ular interest is that in which the new Ô¨Åber
is identiÔ¨Åed with the structure group. This
gives rise to the so-called associated princi-
pal bundle.
B
B
Àú
F
œàŒ±
œÄ
œàŒ≤,Œ± (b)
UŒ±
UŒ≤
UŒ±
UŒ≤
pr1
b
b
œàŒ≤
Figure 10.1
A general Ô¨Åber bundle.

10.4 Fiber Bundles
363
10.4.3
Simultaneity in Classical Mechanics
An important example of the use of
Ô¨Åber bundles arises in the notion of
classical space‚Äìtime. Starting from a four-
dimensional manifold of events, Classical
Physics assumes that all observers agree
on whether or not two events happened
simultaneously, regardless of their loca-
tions. As a consequence, a time-projection
operator arises naturally in this context.
As a result, a Physics that abides by the
principle of absolute simultaneity must of
necessity be formulated upon a space‚Äìtime
manifold that has the structure of a Ô¨Åber
bundle, the base manifold being a one-
dimensional manifold. The typical Ô¨Åber,
representing space, is a three-dimensional
manifold. In the Galilean formulation, this
typical Ô¨Åber is the Euclidean space ùîº3. The
structure group is the group of Galilean
transformations of ùîº3 (those preserving
Euclidean length). An observer is a bundle
trivialization. From this point of view, it can
be claimed that the theory of Relativity is
simpler than its Classical counterpart from
at least the following point of view: the
structure of relativistic space‚Äìtime is less
involved than that of Galilean space‚Äìtime.
The extra structure in the latter is provided
by the notion of absolute simultaneity. In
contrast, in Relativity, space‚Äìtime is just a
four-dimensional manifold endowed with
a special metric structure.
10.4.4
Adapted Coordinate Systems
The total space ÓàØof a smooth Ô¨Åber bundle
(ÓàØ, ùúã, ÓàÆ, Óà≤, Óà≥) is a diÔ¨Äerentiable manifold
in its own right and, as such, can sustain a
whole class of equivalent atlases. Among
these atlases, however, there are some
that enjoy the property of being in some
sense adapted to the Ô¨Åbered structure of ÓàØ
seen as a Ô¨Åber bundle. When working in
coordinates, these adapted atlases or, more
particularly, the corresponding adapted
coordinate charts are used almost exclu-
sively. The construction of these special
charts mimics the construction of charts
in a product manifold, by taking advantage
of the local triviality of Ô¨Åber bundles. Thus,
let p be a point in ÓàØand let {ÓâÅ, ùúì} be a
local trivialization, namely, a diÔ¨Äeomor-
phism ùúì‚à∂ùúã‚àí1(ÓâÅ) ‚ÜíÓâÅ√ó Óà≤, such that
ùúã(p) ‚ààÓâÅ. Without loss of generality, we
may assume that the trivialization is subor-
dinate to a chart in the base manifold ÓàÆin
the sense that ÓâÅis the domain of a chart
in ÓàÆwith coordinates xi (i = 1, ‚Ä¶ , m).
Moreover, because the typical Ô¨Åber Óà≤is
itself a diÔ¨Äerentiable manifold of dimen-
sion n, the point f = pr2(ùúì(p)) belongs to
some chart of Óà≤with domain ÓâÇ‚äÇÓà≤and
coordinates uùõº(ùõº= 1, ‚Ä¶ , n). The induced
adapted coordinate chart in ÓàØis the map
uùúì‚à∂ùúì‚àí1(ÓâÅ√ó ÓâÇ) ‚Üí‚Ñùm+n whose value at
p is (x1, ‚Ä¶ , xm, u1, ‚Ä¶ , un). The proof that
in this way an atlas can be constructed
in ÓàØis straightforward. It is implicitly
assumed that the diÔ¨Äerential structure of ÓàØ
is compatible with the one induced by the
adapted atlases.
10.4.5
The Bundle of Linear Frames
The bundle of linear frames, FÓàÆ, of a base
n-dimensional manifold ÓàÆcan be deÔ¨Åned
constructively in the following way. At
each point b ‚ààÓàÆwe form the set FbÓàÆ
of all ordered n-tuples {e}b = (e1, ‚Ä¶ , en)
of linearly independent vectors ei in TbÓàÆ,
namely, the set of all bases of TbÓàÆ. Our total
space will consist of all ordered pairs of the
form (b, {e}b) with the obvious projection
onto ÓàÆ. The pair (b, {e}b) is called a linear
frame at b. Following a procedure identical

364
10 Topics in DiÔ¨Äerential Geometry
to the one used for the tangent bundle, we
obtain that each basis {e}b is expressible
uniquely as
ej = pi
j
ùúï
ùúïxi
(10.41)
in a coordinate system xi, where {pi
j} is a
nonsingular matrix. We conclude that the
typical Ô¨Åber in this case is GL(n; ‚Ñù). But so
is the structure group. Indeed, in another
coordinate system, yi, we have
ej = qi
j
ùúï
ùúïyi ,
(10.42)
where
qi
j = ùúïyi
ùúïxm pm
j = ai
m pm
j .
(10.43)
This is an instance of a principal Ô¨Åber
bundle, namely, a Ô¨Åber bundle whose typ-
ical Ô¨Åber and structure group coincide. The
action of the group on the typical Ô¨Åber is
the natural left action of the group on itself.
One of the interesting features of a principal
bundle is that the structure group has also
a natural right action on the bundle itself,
and this property can be used to provide an
alternative deÔ¨Ånition of principal bundles,
which we shall pursue later. In the case of
FÓàÆ, for example, the right action is deÔ¨Åned,
in a given coordinate system xi, by
Ra{e} = pk
iai
j
ùúï
ùúïxk ,
j = 1, ‚Ä¶ , n,
(10.44)
which sends the basis (10.41) at b to another
basis at b, that is, the action is Ô¨Åber preserv-
ing. One can verify that this deÔ¨Ånition of
the action is independent of the system of
coordinates adopted. The principal bundle
of linear frames of a manifold is associated
to all the tensor bundles, including the tan-
gent and the cotangent bundles, of the same
manifold. By a direct application of the fun-
damental existence theorem, we know that
the associated principal bundle is deÔ¨Åned
uniquely up to an equivalence. Many prop-
erties of bundles can be better understood
by working Ô¨Årst on the associated principal
bundle.
10.4.6
Bodies with Microstructure
The modeling of complex materials, such as
liquid crystals and granular media, requires
the introduction of extra kinematic degrees
of freedom. The matrix or macromedium
is, in this case, an ordinary manifold that
becomes the base manifold of a Ô¨Åber
bundle whose typical Ô¨Åber represents the
micromedium. In the case of granular
media (such as concrete), each grain is
assumed to undergo a homogeneous defor-
mation. It is natural, therefore, to regard
each of the smoothly distributed grains as
the collection of all possible local bases
of the tangent space to the base manifold.
In other words, the granular medium is
represented by the linear frame bundle of
the macromedium.
10.4.7
Principal Bundles
The existence of a free right action on
a manifold is strong enough to provide
an independent deÔ¨Ånition of a principal
Ô¨Åber bundle which, although equivalent
to the one already given, has the merit
of being independent of the notion of
transition maps. Moreover, once this more
elegant and constructive deÔ¨Ånition has
been secured, a subsidiary deÔ¨Ånition of the
associated (nonprincipal) bundles becomes
available, again without an explicit mention
of the transition maps. Finally, this more
abstract deÔ¨Ånition brings out intrinsically
the nature and meaning of the associated
bundles.

10.4 Fiber Bundles
365
Let Óàºbe a diÔ¨Äerentiable manifold (the
total space) and Óà≥a Lie group (the structure
group), and let Óà≥act freely to the right on Óàº.
This means that there exists a smooth map
Rg ‚à∂Óàº√ó Óà≥‚àí‚ÜíÓàº
(p, g) ÓÇ∂‚ÜíRgp = pg,
(10.45)
such that, for all p ‚ààÓàºand all g, h ‚ààÓà≥, we
have
Rghp = RhRgp = pgh,
Rep = p,
(10.46)
where e is the group identity. The fact that
the action is free means that if, for some p ‚àà
Óàºand some g ‚ààÓà≥, Rgp = p, then necessar-
ily g = e. DeÔ¨Åne now the quotient ÓàÆ= Óàº‚àïÓà≥
and check that ÓàÆis a diÔ¨Äerentiable mani-
fold and that the canonical projection ùúãP ‚à∂
Óàº‚ÜíÓàº‚àïÓà≥is diÔ¨Äerentiable. The set ùúã‚àí1
P (b) is
called the Ô¨Åber over b ‚ààÓàÆ.
Recall that an element of the quotient
ÓàÆ= Óàº‚àïÓà≥is, by deÔ¨Ånition of quotient, an
equivalence class in Óàºby the action of the
group Óà≥. In other words, each element b
of the quotient (namely, of the base man-
ifold ÓàÆ) can be regarded as representing
an orbit. The projection map assigns to
each element of Óàºthe orbit to which it
belongs. The Ô¨Åber over b consists of all the
elements of Óàºthat belong to the speciÔ¨Åc
orbit represented by b.
To complete the deÔ¨Ånition of a principal
bundle, we need only to add the condition
that Óàºbe locally trivial, namely, that for
each b ‚ààÓàÆ, there exists a neighborhood
ÓâÅ‚äÇÓàºsuch that ùúã‚àí1
P (ÓâÅ) is isomorphic to
the product ÓâÅ√ó Óà≥. More precisely, there
exists a Ô¨Åber-preserving diÔ¨Äeomorphism
ùúì‚à∂ùúã‚àí1
P (ÓâÅ) ‚àí‚ÜíÓâÅ√ó Óà≥
p ÓÇ∂‚Üí(b, ÃÉùúìb),
(10.47)
where b = ùúãP(p), with the additional prop-
erty that it must be consistent with the group
action, namely (see Figure 10.2),
ÃÉùúìb(pg) = ÃÉùúìb(p)g,
‚àÄp ‚ààùúã‚àí1
P (ÓâÅ), g ‚ààÓà≥.
(10.48)
This completes the deÔ¨Ånition of the prin-
cipal bundle. The right action is Ô¨Åber pre-
serving and every Ô¨Åber is diÔ¨Äeomorphic to
Óà≥. Moreover, every Ô¨Åber coincides with an
orbit of the right action of Óà≥.
10.4.8
Associated Bundles
The concept of associated bundle has
already been deÔ¨Åned and used to introduce
the notion of the principal bundle associ-
ated with any given Ô¨Åber bundle. On the
other hand, in the preceding section, we
have introduced an independent deÔ¨Ånition
of principal bundles by means of the idea
of a right action of a group on a given total
manifold. We want now to show that this
line of thought can be pursued to obtain
B
B
b
b
p
P
œÄP
pr1
œàb
g
g
G
Àú
Figure 10.2
The group consistency condition.

366
10 Topics in DiÔ¨Äerential Geometry
another view of the collection of all (non-
principal) Ô¨Åber bundles associated with a
given principal bundle.
As a more or less intuitive motivation for
this procedure, it is convenient to think of
the example of the principal bundle of lin-
ear frames FÓàÆof a manifold ÓàÆ. We already
know that this bundle is associated to the
tangent bundle TÓàÆ. Consider now a pair
(f , v), where f ‚ààFÓàÆand v ‚ààTÓàÆ, such that
ùúãP(f ) = ùúã(v) = b. In other words, f and v
represent, respectively, a basis and a vector
of the tangent space at some point b ‚ààÓàÆ.
We can, therefore, identify v with its com-
ponents on the linear frame f , namely, with
an element of the typical Ô¨Åber (‚Ñùn) of TÓàÆ.
If we consider now a pair (ÃÇf , v), where v
is the same as before but ÃÇf is a new lin-
ear frame at b, the corresponding element
of the typical Ô¨Åber representing the same
vector v changes. More explicitly, with an
obvious notation, if ÃÇfj = ai
jfi, then vi = ai
jÃÇvj
or ÃÇvi = (a‚àí1)i
jvj. We conclude that to rep-
resent the same object under a change of
frame, there needs to be some kind of com-
pensatory action in the change of the com-
ponents. The object itself (in this case, the
tangent vector) can be identiÔ¨Åed with the
collection (or equivalence class) of all pairs
made up of a frame and a matrix related
in this compensatory way. In terms of the
group actions on the typical Ô¨Åbers, if ÃÇf =
Raf , then the representative r of the vector
v in ‚Ñùn changes according to ÃÇr = La‚àí1r. We
may, therefore, think of a vector as an equiv-
alence class of elements of the Cartesian
product Óà≥√ó Rn, corresponding to the fol-
lowing equivalence relation: (g, r) ‚àº(ÃÇg, ÃÇr) if,
and only if, there exists a ‚ààÓà≥such that ÃÇg =
ga and ÃÇr = La‚àí1r.
With the above motivation in mind, the
following construction of a Ô¨Åber bundle
associated to a given principal bundle
will seem less artiÔ¨Åcial than it otherwise
would. We start from the principal bundle
(Óàº, ùúãP, ÓàÆ, Óà≥, Óà≥) and a manifold Óà≤, which we
want to construe as the typical Ô¨Åber of a
new Ô¨Åber bundle (ÓàØ, ùúã, ÓàÆ, Óà≤, Óà≥) associated
with Óàº. For this to be possible, we need
to have an eÔ¨Äective left action of Óà≥on Óà≤,
which we assume to have been given. To
start oÔ¨Ä, we form the Cartesian product
Óàº√ó Óà≤and notice that the structure group
Óà≥acts on it with a right action induced by
its right action on Óàºand its left action on
Óà≤. To describe this new right action, we
will keep abusing the notation in the sense
that we will use the same symbols for all the
actions in sight, because the context should
make clear which action is being used in
each particular expression. Let (p, f ) be
an element of the product Óàº√ó Óà≤, and let
a ‚ààÓà≥. We deÔ¨Åne the eÔ¨Äective right action
Ra(p, f ) = (Rap, La‚àí1 f ).
(10.49)
The next step toward the construction of
the associated bundle with typical Ô¨Åber Óà≤
consists of taking the quotient space ÓàØgen-
erated by this action. In other words, we
want to deal with a set whose elements are
equivalence classes in ÓàØ√ó Óà≤by the equiva-
lence relation ‚Äú(p1, f1) ‚àº(p2, f2) if, and only
if, there exists a ‚ààÓà≥such that (p2, f2) =
Ra(p1, f1).‚Äù The motivation for this line of
attack should be clear from the introduc-
tory remarks to this section. Recalling that
the right action of Óà≥on Óàºis Ô¨Åber pre-
serving, it becomes obvious that all the
pairs (p, f ) in a given equivalence class have
Ô¨Årst components p with the same projec-
tion ùúãP(p) on ÓàÆ. This means that we have
a perfectly well-deÔ¨Åned projection ùúãin the
quotient space ÓàØ, namely, ùúã‚à∂ÓàØ‚ÜíÓàÆis a
map that assigns to each equivalence class
the common value of the projection of
the Ô¨Årst component of all its constituent
pairs.
Having a projection, we can now deÔ¨Åne
the Ô¨Åber of ÓàØover b ‚ààÓàÆnaturally as

10.5 Connections
367
ùúã‚àí1(b). We need to show now that each
such Ô¨Åber is diÔ¨Äeomorphic to the puta-
tive typical Ô¨Åber Óà≤. More precisely, we
want to show that for each local trivial-
ization (ÓâÅ, ùúì) of the original principal
bundle Óàº, we can also construct a local
trivialization of ùúã‚àí1(ÓâÅ), namely, a dif-
feomorphism
ùúå‚à∂ùúã‚àí1(ÓâÅ) ‚ÜíÓâÅ√ó Óà≤.
To
understand how this works, let us Ô¨Åx a
point b ‚ààÓâÅand recall that, given the local
trivialization (ÓâÅ, ùúì), the map ÃÉùúìb provides
us with a diÔ¨Äeomorphism of the Ô¨Åber
ùúã‚àí1
P (b) with Óà≥. We now form the product
map of ÃÉùúìb with the identity map of Óà≤,
namely,
( ÃÉùúìb, idÓà≤) ‚à∂ùúã‚àí1
P (b) √ó Óà≤‚ÜíÓà≥√ó Óà≤.
Each equivalence class by the right action
(10.49) is mapped by the product map
( ÃÉùúìb, idÓà≤)
into
an
orbit,
as
shown
in
Figure 10.3.
These orbits do not intersect with each
other. Moreover, they can be seen as
graphs of single-valued Óà≤-valued functions
of Óà≥. Therefore, choosing any particu-
lar value g ‚ààÓà≥, we see that these orbits
can be parameterized by Óà≤. This pro-
vides the desired one-to-one and onto
relation between the Ô¨Åber ùúã‚àí1(b) and
the manifold Óà≤, which can now legiti-
mately be called the typical Ô¨Åber of ÓàØ. To
complete the construction of the desired
Ô¨Åber bundle, we need to guarantee that
the Ô¨Åberwise isomorphism that we have
just constructed depends diÔ¨Äerentiably
on b, a requirement that we assume
fulÔ¨Ålled.
10.5
Connections
10.5.1
Introduction
All the Ô¨Åbers of a Ô¨Åber bundle are, by deÔ¨Å-
nition, diÔ¨Äeomorphic to each other. In the
absence of additional structure, however,
there is no canonical way to single out a par-
ticular diÔ¨Äeomorphism between Ô¨Åbers. In
the case of a product bundle, for example,
such a special choice is indeed available
because of the existence of the second pro-
jection map onto the typical Ô¨Åber. In this
extreme case, we may say that we are in the
presence of a canonical distant parallelism
in the Ô¨Åber bundle. An equivalent way to
describe this situation is by saying that we
have a canonical family of nonintersect-
ing smooth cross sections such that each
point in the Ô¨Åber bundle belongs to one, and
only one, of them. In a general Ô¨Åber bundle
we can only aÔ¨Äord this luxury noncanon-
ically and locally. A connection on a Ô¨Åber
bundle is, roughly speaking, an additional
structure deÔ¨Åned on the bundle that per-
mits to establish intrinsic Ô¨Åber diÔ¨Äeomor-
phisms for Ô¨Åbers lying along curves in the
base manifold. In other words, a connec-
tion can be described as a curve-dependent
parallelism. Given a connection, it may so
happen that the induced Ô¨Åber parallelisms
turn out to be curve independent. A quan-
titative measure of this property or the lack
G
F
Figure 10.3
Images of equivalence classes.

368
10 Topics in DiÔ¨Äerential Geometry
thereof is provided by the vanishing, or oth-
erwise, of the curvature of the connection.
10.5.2
Ehresmann Connection
Consider the tangent bundle TÓàØof the
total space ÓàØof an arbitrary Ô¨Åber bundle
(ÓàØ, ùúã, ÓàÆ, Óà≤, Óà≥), and denote by ùúèC ‚à∂TÓàØ‚ÜíÓàØ
its natural projection. If the dimensions of
the base manifold ÓàÆand the typical Ô¨Åber
Óà≤are, respectively, m and n, the dimen-
sion of ÓàØis m + n, and the typical Ô¨Åber
of (TÓàØ, ùúèC) is ‚Ñùm+n, with structure group
GL(m + n; ‚Ñù). At each point c ‚ààÓàØ, the tan-
gent space TcÓàØhas a canonically deÔ¨Åned
vertical subspace Vc, which can be identi-
Ô¨Åed with the tangent space TcÓàØùúã(c) to the
Ô¨Åber of ÓàØat c. The dimension of Vc is n. A
vector in TcÓàØbelongs to the vertical sub-
space Vc (or is vertical) if, and only if, its
projection by ùúã‚àóis the zero vector of Tùúã(c)ÓàÆ.
If a vector in TcÓàØis not vertical, there is no
canonical way to assign to it a vertical com-
ponent. It is this deÔ¨Åciency, and only this
deÔ¨Åciency, that the Ehresmann connection
remedies. Formally, an Ehresmann connec-
tion consists of a smooth horizontal distri-
bution in ÓàØ. This is a smooth assignment
to each point c ‚ààÓàØof an (m-dimensional)
subspace Hc ‚äÇTcÓàØ(called the horizontal
subspace at c), such that
TcÓàØ= Hc ‚äïVc.
(10.50)
In this equation, ‚äïdenotes the direct sum
of vector spaces. Each tangent vector u ‚àà
TcÓàØis, accordingly, uniquely decompos-
able as the sum of a horizontal part h(u)
and a vertical part v(u). A vector is hor-
izontal, if its vertical part vanishes. The
only vector that is simultaneously horizon-
tal and vertical is the zero vector. As Hc and
Tùúã(c)ÓàÆhave the same dimension (m), the
restriction ùúã‚àó|Hc ‚à∂Hc ‚ÜíTùúã(c)ÓàÆ, is a vector-
space isomorphism. We denote its inverse
by Œìc. Thus, given a vector v tangent to
the base manifold at a point b ‚ààÓàÆ, there
is a unique horizontal vector Œìcv at c ‚àà
ùúã‚àí1({b}) such that ùúã‚àó(Œìcv) = v. This unique
vector is called the horizontal lift of v to c.
In particular, Œìc
(ùúã‚àó(u)
)
= Œìc
(ùúã‚àó(h(u))
)
=
h(u). These ideas are schematically illus-
trated in Figure 10.4.
10.5.3
Parallel Transport along a Curve
Let
ùõæ‚à∂(‚àíùúñ, ùúñ) ‚àí‚ÜíÓàÆ
(10.51)
be a smooth curve in the base manifold ÓàÆof
the Ô¨Åber bundle (ÓàØ, ùúã), and let c ‚ààÓàØùõæ(0) be a
point in the Ô¨Åber at ùõæ(0). A horizontal lift of
u
h(u)
ŒΩ(u)
C
B
b
c
Œìc
œÄ‚àó (u)
Figure 10.4
Ehresmann connection.

10.5 Connections
369
ùõæthrough c is deÔ¨Åned as a curve
ÃÇùõæ‚à∂(‚àíùúñ, ùúñ) ‚àí‚ÜíÓàØ,
(10.52)
such that
ÃÇùõæ(0) = c,
(10.53)
ùúã(ÃÇùõæ(t)) = ùõæ(t),
‚àÄt ‚àà(‚àíùúñ, ùúñ),
(10.54)
and
ÃÇùõæ‚Ä≤(t) ‚ààHÃÇùõæ(t),
‚àÄt ‚àà(‚àíùúñ, ùúñ),
(10.55)
where a prime denotes the derivative with
respect to the curve parameter t. A hori-
zontal lift is thus a curve that projects onto
the original curve and, moreover, has a hor-
izontal tangent throughout.
Consider the ‚Äúcylindrical‚Äù subbundle ùõæ‚àóÓàØ
obtained by pulling back the bundle ÓàØto the
curve ùõæor, less technically, by restricting the
base manifold to the curve ùõæ. The tangent
vector Ô¨Åeld of ùõæhas a unique horizontal lift
at each point of this bundle. In other words,
the curve generates a (horizontal) vector
Ô¨Åeld throughout this restricted bundle. By
the fundamental theorem of the theory of
ordinary diÔ¨Äerential equations (ODEs), it
follows that, at least for small enough ùúñ,
there is a unique horizontal lift of ùõæthrough
any given point in the Ô¨Åber at ùõæ(0), namely,
the corresponding integral curve of the hor-
izontal vector Ô¨Åeld. We conclude, therefore,
that the horizontal lift of a curve through a
point in a Ô¨Åber bundle exists and is locally
unique. As the horizontal curve issuing
from c cuts the various Ô¨Åbers lying on ùõæ,
the point c is said to undergo a parallel
transport relative to the given connection
and the given curve. Thus, given a point
c ‚ààÓàØand a curve ùõæthrough ùúã(c) ‚ààÓàÆ, we
obtain a unique parallel transport of c along
ùõæby solving a system of ODEs (so as to
travel always horizontally). These concepts
are illustrated schematically in Figure 10.5
10.5.4
Connections in Principal Bundles
A
connection
in
a
principal
bundle
(Óàº, ùúã, ÓàÆ, Óà≥, Óà≥) is an Ehresmann connec-
tion that is compatible with the right action
Rg of Óà≥on Óàº, namely,
(Rg)‚àó(Hp) = HRgp,
‚àÄg ‚ààÓà≥,
p ‚ààÓàº.
(10.56)
This condition can be stated verbally as fol-
lows: the horizontal distribution is invari-
ant under the group action.
Recall that the group Óà≥acts freely (to the
right) on Óàº. Consequently, the fundamental
vector Ô¨Åeld vg associated with any nonzero
vector g in the Lie algebra ùî§of Óà≥does not
vanish anywhere on Óàº. Moreover, because
the action of Óà≥maps Ô¨Åbers into themselves,
0
B
c
ùõæ
Figure 10.5
Parallel transport
along a curve.

370
10 Topics in DiÔ¨Äerential Geometry
the fundamental vector Ô¨Åelds are all verti-
cal. The correspondence between vectors in
the Lie algebra and tangent vectors to the
Ô¨Åber at any point is clearly linear and one-
to-one. As the dimension of Óà≥is equal to
the dimension of each Ô¨Åber, we conclude
that the map ùî§‚ÜíVp given by g ÓÇ∂‚Üívg(p) is
a linear isomorphism between the Lie alge-
bra and each of the vertical subspaces of the
principal bundle.
Let v ‚ààTÓàºbe any tangent vector to the
Ô¨Åber bundle. A connection Œì assigns to it
a unique vertical part and, as we have just
seen, the action of the group assigns to this
vertical part an element of the Lie algebra
ùî§. This means that we have a well-deÔ¨Åned
linear map
ùúî‚à∂TÓàº‚àí‚Üíùî§,
(10.57)
associated with a given connection in a
principal bundle. This map can be regarded
as a Lie-algebra-valued 1-form. It is called
the connection form associated with Œì.
10.5.5
Distributions and the Theorem of
Frobenius
We have mentioned in Section 10.5.1 the
notion of curvature of a connection as an
indication of how the parallel transport of
an entity along a curve depends on the
curve itself. Before giving a precise def-
inition of this concept, however, it may
prove useful to introduce the more gen-
eral concept of involutivity of a distribu-
tion. The reason for this digression is that
a connection can always be regarded as a
(horizontal) distribution. A k-dimensional
distribution of an m-dimensional manifold
Óàπ(with m ‚â•k) is deÔ¨Åned as a smooth
assignment of a k-dimensional subspace Óà∞x
of the tangent space TxÓàπto each point x ‚àà
Óàπ. A fundamental question in the theory of
distributions is whether or not there exist
integral embedded submanifolds, namely,
embedded submanifolds of dimension k
whose tangent space at each point x coin-
cides with Óà∞x.
An embedded submanifold of dimension
k is deÔ¨Åned as a subset Óàø‚äÇÓàπsuch that
for each point s ‚ààÓàøone can Ô¨Ånd a chart of
Óàπwith coordinates xi (i = 1, ‚Ä¶ , m) such
that s belongs to this chart and such that the
intersection of the set Óàøwith the chart coin-
cides with the set obtained by keeping the
last m ‚àík coordinates constant. This idea
becomes clear if one thinks of the particular
case of ‚Ñù2 as embedded in ‚Ñù3 with coordi-
nates x, y, z. The equation of the embedded
submanifold ‚Ñù2 can be given as z = 0.
In some sense, the question of existence
of integral submanifolds can be regarded as
a generalization to many dimensions of the
question of integrability of systems of ODE,
which would correspond to the case k = 1,
namely, to the case in which the subspaces
of the distribution are mere lines. While in
the particular case k = 1, we are assured,
by the fundamental theorem of ODEs, of
the (local) existence of integral curves, the
answer in the general case k > 1 is usu-
ally negative. A k-dimensional distribution
is said to be completely integrable if for
each point of the manifold Óàπthere exists
a chart xi (i = 1, ‚Ä¶ , m) such that each set
obtained by keeping the last n ‚àík coordi-
nates thereat constant is an integral sub-
manifold (of dimension k). Assume that a
completely integrable distribution has been
given. Then, according to our deÔ¨Ånition, the
Ô¨Årst k natural vectors of the local coordi-
nate system just described belong to the
distribution and constitute a basis of Óà∞x at
each point x in the chart. Any vector Ô¨Åelds
vùõº(ùõº= 1, ‚Ä¶ , k) with this property (of con-
stituting a basis of the distribution) are said
to span the distribution. Within the chart,
any vector Ô¨Åelds vùõº(ùõº= 1, ‚Ä¶ , k) that span

10.5 Connections
371
the distribution must be expressible, there-
fore, as
vùõº= v ùõΩ
ùõº
ùúï
ùúïxùõΩ,
(10.58)
where the summation convention applies
for Greek indices within the range 1, ‚Ä¶ , k.
We now calculate the Lie bracket of any pair
of the spanning vectors as
[vùõº, vùõΩ] = v ùúå
ùõº
ùúïv ùúé
ùõΩ
ùúïxùúå
ùúï
ùúïxùúé‚àív ùúé
ùõΩ
ùúïv ùúå
ùõº
ùúïxùúé
ùúï
ùúïxùúå.
(10.59)
Notice that, in calculating the Lie brackets,
we have used the fact that the components
of the vectors vùõºvanish on the natural base
vectors ùúï‚àïùúïxi with i > k. Moreover, because
the given vectors are linearly independent,
the matrix with entries v ùõΩ
ùõº
is nonsingu-
lar. Inverting, therefore, (10.58), we can
express the natural base vectors ùúï‚àïùúïxùõº(ùõº=
1, ‚Ä¶ , k) in terms of the vectors vùõΩ, with
the result that the Lie brackets are them-
selves linear combinations of these vectors,
namely, there exist scalars Cùõæ
ùõºùõΩsuch that
[vùõº, vùõΩ] = Cùõæ
ùõºùõΩvùõæ.
(10.60)
A distribution with this property (namely,
that the Lie bracket of any two vector Ô¨Åelds
in the distribution is also in the distri-
bution) is said to be involutive. We have
proven, therefore, that every completely
integrable distribution is involutive. The
converse of this result (that is, that every
involutive distribution is completely inte-
grable) is also true, and is the content of
the theorem of Frobenius, whose proof we
omit.
10.5.6
Curvature
Suppose that we draw through a point b of
the base manifold ÓàÆa small closed curve ùõæ.
If we now choose a point p in the Ô¨Åber on b,
we have learned that there exists a unique
horizontal lift ÃÉùõæ, namely, a horizontal curve
containing p and projecting on ùõæ. Is this
curve closed? To clarify the meaning of this
question and its possible answer, recall that
a connection on a principal Ô¨Åber bundle
is a special case of a distribution, which
we have called horizontal (the dimension
of the horizontal distribution equals the
dimension of the base manifold and is thus
strictly smaller than the dimension of the
Ô¨Åber bundle, assuming that the typical Ô¨Åber
is of dimension greater than zero). Clearly,
if the horizontal distribution is involutive,
any horizontal lift of a small curve in the
base manifold will lie entirely on an inte-
gral surface and, therefore, will be closed.
This observation suggests that a measure of
the lack of closure of the horizontal lift of
closed curves is the fact that the Lie bracket
between horizontal vector Ô¨Åelds has a verti-
cal component. We want to see now how to
extract this information from the connec-
tion itself. More particularly, because a con-
nection is speciÔ¨Åed by its connection form
ùúî, we want to extract this information from
ùúîalone.
Consider two horizontal vector Ô¨Åelds u
and v. Let us evaluate the 2-form1) dùúîon
this pair as
‚ü®dùúî| u ‚àßv‚ü©= u (‚ü®ùúî| v‚ü©) ‚àív (‚ü®ùúî| u‚ü©)
‚àí‚ü®ùúî| [u, v]‚ü©,
(10.61)
which, in view of the fact that u and v are
assumed to be horizontal, yields
‚ü®dùúî| u ‚àßv‚ü©= ‚àí‚ü®ùúî| [u, v]‚ü©.
(10.62)
The right-hand side of this equation will
vanish if, and only if, the Lie bracket is hor-
izontal. This means that we have found a
1) Notice that this is a Lie-algebra-valued diÔ¨Äeren-
tial form.

372
10 Topics in DiÔ¨Äerential Geometry
way to extract the right information from
ùúîby just taking its exterior derivative and
applying it to two horizontal vector Ô¨Åelds.
Notice, however, that dùúîcan be applied to
arbitrary pairs of vector Ô¨Åelds, not neces-
sarily horizontal. To formalize this point,
given a connection, we deÔ¨Åne the exterior
covariant derivative Dùõºof an r-form ùõºas
the (r + 1)-form given by
‚ü®Dùõº| U1 ‚àß¬∑ ¬∑ ¬∑ ‚àßUr+1‚ü©
= ‚ü®dùõº| h(U1) ‚àß¬∑ ¬∑ ¬∑ ‚àßh(Ur+1)‚ü©, (10.63)
where h(.) denotes the horizontal compo-
nent of a vector. Accordingly, we deÔ¨Åne the
curvature 2-form Œ© of a connection ùúîon a
principal Ô¨Åber bundle as
Œ© = Dùúî.
(10.64)
10.5.7
Cartan‚Äôs Structural Equation
Our deÔ¨Ånition of curvature, by using both
the connection 1-form and the horizontal
projection map, is a hybrid that mixes both
(equivalent) deÔ¨Ånitions of a connection. It
is possible, on the other hand, to obtain
an elegant formula that involves just the
connection 1-form. This formula, known as
Cartan‚Äôs structural equation, reads
Œ© = dùúî+ 1
2 [ùúî, ùúî],
(10.65)
or, more precisely, for any two vectors u and
v at a point2) of the frame bundle,
‚ü®Œ© | U ‚àßV‚ü©= ‚ü®dùúî| U ‚àßV‚ü©
+1
2 [ùúî(U), ùúî(V)].
(10.66)
2) Notice that this formula is valid pointwise,
because the Lie bracket on the right-hand side is
evaluated in the Lie algebra, not in the manifold.
The proof of this formula, whose details
we omit, is based on a careful examination
of three cases: (i) u and v are horizontal,
whereby the formula is obvious; (ii) u is hor-
izontal and v is vertical, in which case one
can extend them, respectively, to a horizon-
tal and a fundamental (vertical) vector Ô¨Åeld;
(iii) u and v are both vertical, in which case
they can both be extended to fundamental
Ô¨Åelds.
10.5.8
Bianchi Identities
Unlike the ordinary exterior derivative d,
the operator D (of exterior covariant dif-
ferentiation) is not necessarily nilpotent,
namely, in general, D2 ‚â†0. Therefore, there
is no reason to expect that DŒ©, which is
equal to D(Dùúî), will vanish identically. But,
in fact, it does. To see that this is the case,
notice that, by deÔ¨Ånition of D, we need only
verify the vanishing of DŒ© on an arbitrary
triple of horizontal vectors. It can be shown
that
DŒ© = 0.
(10.67)
In terms of components, we obtain diÔ¨Äer-
ential identities to be satisÔ¨Åed by any cur-
vature form. They are known as the Bianchi
identities.
10.5.9
Linear Connections
A connection on the bundle of linear
frames FÓàÆis called a linear connection on
ÓàÆ. Among principal bundles, the bundle
of linear frames occupies a special posi-
tion for various reasons. In the Ô¨Årst place,
the bundle of linear frames is canonically
deÔ¨Åned for any given base manifold ÓàÆ.
Moreover, the associated bundles include
all the tensor bundles, thus allowing for
a uniÔ¨Åed treatment of all such entities.

10.5 Connections
373
Another way to express this peculiar fea-
ture of the bundle of linear frames is that,
whereas the quantities parallel-transported
along curves in a general principal bundle
are of a nature not necessarily related
to the base manifold, in the case of the
bundle of linear frames, the quantities
transported are precisely the very frames
used to express the components of vectors
and forms deÔ¨Åned on the base manifold.
An elegant manifestation of this property
is the existence of a canonical 1-form that
ties everything together. A direct conse-
quence of the existence of this 1-form is
the emergence of the idea of the torsion
of a connection. We start the treatment of
linear connections by lingering for a while
on the deÔ¨Ånition of the canonical 1-form.
10.5.10
The Canonical 1-Form
Given a tangent vector v ‚ààTxÓàÆat a point x
in the base manifold, and a point p ‚ààFxÓàÆin
the Ô¨Åber over x, and recalling that p consists
of a frame (or basis) {e1, ‚Ä¶ , em} of TxÓàÆ, we
can determine uniquely the m components
of v in this frame, namely,
v = vaea.
(10.68)
In other words, at each point p ‚ààFÓàÆ, we
have a well-deÔ¨Åned nonsingular linear
map3)
u(p) ‚à∂Tùúã(p)ÓàÆ‚àí‚Üí‚Ñùm.
(10.69)
The canonical 1-form ùúÉon FÓàÆis deÔ¨Åned as
ùúÉ(V) = u(p)‚àòùúã‚àó(V),
‚àÄV ‚ààTp(FÓàÆ).
(10.70)
Note that this is an ‚Ñùm-valued form. The
canonical 1-form of the frame bundle is a
3) This map is, in fact, an alternative deÔ¨Ånition of a
linear frame at a point of a manifold ÓàÆ.
particular case of a more general construct
known as a soldering form.
It may prove instructive to exhibit
the canonical form in components. Let
x1, ‚Ä¶ , xm be a local coordinate system on
ÓâÅ‚äÇÓàÆ. Every frame {e1, ‚Ä¶ , em} at x ‚ààÓâÅ
can be expressed uniquely by means of a
nonsingular matrix with entries xi
j as
ea = xi
a
ùúï
ùúïxi .
(10.71)
This means that the m + m2 functions
{xi, xi
a} constitute a coordinate system for
the linear frame bundle ùúã‚àí1(ÓâÅ). We call
it the coordinate system induced by xi.
The projection map ùúã‚à∂FÓàÆ‚ÜíÓàÆhas the
coordinate representation
(xi, xi
a) ÓÇ∂‚Üíùúã(xi, xi
a) = (xi),
(10.72)
with some notational abuse.
Consider now the tangent bundle TF(ÓàÆ)
with projection ùúè‚à∂TF(ÓàÆ) ‚ÜíF(ÓàÆ). The
coordinate system {xi, xi
a} induces nat-
urally a coordinate system in TF(ÓàÆ). A
vector X ‚ààTF(ÓàÆ) is expressed in these
coordinates as follows:
X ÓÇ∂‚Üí
(
xi, xi
a, Xi ùúï
ùúïxi + Xi
a
ùúï
ùúïxi
a
)
= (xi, xi
a, Xi, Xi
a
) .
(10.73)
The derivative of the projection ùúãis a map
ùúã‚àóTF(ÓàÆ) ‚ÜíTÓàÆ. Its coordinate representa-
tion is
(xi, xi
a, Xi, Xi
a
) ÓÇ∂‚Üí(xi, Xi) .
(10.74)
The map u deÔ¨Åned in (10.69) is given in
coordinates by
[u(xi, xi
a)](xj, wj) = x‚àía
i wi
(a = 1, ‚Ä¶ , m),
(10.75)
where we have denoted by x‚àía
i
the entries
of the inverse of the matrix with entries xi
a.

374
10 Topics in DiÔ¨Äerential Geometry
Combining (10.74) and (10.75), we obtain
from (10.70) the following coordinate rep-
resentation of the (‚Ñùm)-valued canonical
form ùúÉ:
ùúÉa = x‚àía
i
dxi
(a = 1, ‚Ä¶ , m).
(10.76)
10.5.11
The ChristoÔ¨Äel Symbols
The canonical form ùúÉexists independently
of any connection. Let us now introduce a
connection on F(ÓàÆ), that is, a linear con-
nection on ÓàÆ. If we regard a connection as
a horizontal distribution, there must exist
nonsingular linear maps Œì(x, p) from each
TxÓàÆto each of the tangent spaces TpF(ÓàÆ)
(with ùúã(p) = x) deÔ¨Åning the distribution.
Noticing that the same distribution may
correspond to an inÔ¨Ånite number of such
maps, we pin down a particular one by
imposing the extra condition that they must
be also horizontal lifts. In other words, we
demand that
ùúã‚àó‚àòŒì(x, p) = idTxÓàÆ.
(10.77)
The implication of this condition is that,
when written in components, we must have
Œì(x, p)
(
vi ùúï
ùúïxi
)
= vi ùúï
ùúïxi ‚àíÃÇŒìj
ia(x, p) vi ùúï
ùúïxj
a
,
(10.78)
where ÃÇŒìj
ia(x, p) are smooth functions of x
and p. The minus sign is introduced for
convenience.
These functions, however, cannot be
arbitrary, because they must also satisfy
the compatibility condition (10.56). It is
not diÔ¨Écult to verify that this is the case if,
and only if, the functions Œìj
ik deÔ¨Åned by
Œìj
ik = ÃÇŒìj
ia(x, p) x‚àía
k (p)
(10.79)
are independent of p along each Ô¨Åber.
We conclude that a linear connection is
completely deÔ¨Åned (on a given coordinate
patch) by means of m3 smooth functions.
These functions are known as the Christof-
fel symbols of the connection.
10.5.12
Parallel Transport and the Covariant
Derivative
Now that we are in possession of explicit
coordinate expressions for the horizontal
distribution, we can write explicitly the sys-
tem of ODEs that eÔ¨Äects the horizontal lift
of a curve in ÓàÆ. A solution of this system
is a one-parameter family of frames being
parallel-transported along the curve. Let
the curve ùõæin the base manifold be given
by
xi = xi(t).
(10.80)
On this curve, the connection symbols are
available as functions of t, by composi-
tion. The nontrivial part of the system of
equations is given by
dxi
a(t)
dt
= ‚àíŒìi
jk(t) xk
a(t) dxj(t)
dt
.
(10.81)
The local solution of this system with given
initial condition (say, xi
a(0) = xi
a) is the
desired curve in F(ÓàÆ), representing the
parallel transport of the initial frame along
the given curve.
Let now v be a vector in Txi(0)ÓàÆ, that is,
a vector at the initial ‚Äútime‚Äù t = 0. We say
that the curve v = v(t) in TÓàÆis the paral-
lel transport of v if it projects on ùõæ, with
v(0) = v, and if the components of v(t) in a
parallel-transported frame along ùõæare con-
stant.4) For this deÔ¨Ånition to make sense, we
must make sure that the constancy of the
4) The same criterion for parallel transport that we
are using for the tangent bundle can be used for
any associated bundle.

10.5 Connections
375
components is independent of the particu-
lar initial frame chosen. This, however, is a
direct consequence of the fact that our lin-
ear connection is, by deÔ¨Ånition, consistent
with the right action of the group.
To obtain the system of ODEs corre-
sponding to the parallel transport of v along
ùõæ, we enforce the constancy conditions
vi(t) x‚àía
i (t) = Ca,
(10.82)
where each Ca (a = 1, ‚Ä¶ , m) is a constant
and vi denotes components in the coor-
dinate basis. DiÔ¨Äerentiating this equation
with respect to t and invoking (10.81), we
obtain
dvi
dt
+ Œìi
jk
dxj
dt vk = 0.
(10.83)
A vector Ô¨Åeld along ùõæ
satisfying this
equation is said to be covariantly constant.
For a given vector Ô¨Åeld w on ÓàÆ, the expres-
sion on the left-hand side makes sense in a
pointwise manner whenever a vector u is
deÔ¨Åned at a point (whereby the curve ùõæcan
be seen as a representative at t = 0). The
expression
‚ñøuw =
(
dwi
dt
+ Œìi
jk uj wk
)
ùúï
ùúïxi ,
(10.84)
is called the covariant derivative of v in the
direction of u. From the treatment above, it
can be seen that the covariant derivative is
precisely the limit
‚ñøuw = lim
t‚Üí0
ùúåt,0w ‚àíw(0)
t
,
(10.85)
where ùúå(a, b) denotes the parallel transport
along ùõæfrom t = b to t = a.
10.5.13
Curvature and Torsion
To obtain an explicit equation for the cur-
vature form Œ©, we should start by eluci-
dating the connection form ùúîon the basis
of the connection symbols Œì. Given a vec-
tor X ‚ààTpFÓàÆ, we know that its horizontal
component h(X) is given by
h(X) = Œì(ùúã(p), p)‚àòùúã‚àó(X).
(10.86)
Its vertical component must, therefore, be
given by
v(X) = X ‚àíh(X) = X ‚àíŒì(ùúã(p), p)‚àòùúã‚àó(X).
(10.87)
Recall that the connection form ùúîassigns
to X the vector in ùî§such that v(X) belongs
to its fundamental vector Ô¨Åeld. Let the coor-
dinates of p be (xi, xi
a). The right action of
GL(m; ‚Ñù) is given by
(Rg(p))i
a = xi
b gb
a,
(10.88)
where we have shown only the action on the
Ô¨Åber component and gb
a is the matrix corre-
sponding to g ‚ààGL(m; ‚Ñù). Consequently,
if g(t) is a one-parameter subgroup repre-
sented by the vector
ÃÇga
b =
dga
b(t)
dt
|||||t=0
,
(10.89)
the value of the corresponding fundamental
vector Ô¨Åeld at p is
ÃÉgi
a = xi
b ÃÇgb
a.
(10.90)
The coordinate expression of (10.87) is
(v(X))i
a = Xi
a ‚àíh(X) = X ‚àíŒì(ùúã(p), p)‚àòùúã‚àó(X).
(10.91)

376
10 Topics in DiÔ¨Äerential Geometry
Let the main part of the vector X be given
by
X = vi ùúï
ùúïxi + Xi
a
ùúï
ùúïxi
a
.
(10.92)
Then, (10.91) delivers
(v(X))i
a = Xi
a + Œìj
ik vi xk
a.
(10.93)
According to (10.90), the corresponding
element of the Lie algebra is
ÃÇgb
a =
(
Xj
a + Œìj
ik vi xk
a
)
x‚àíb
j .
(10.94)
Accordingly, the Lie-algebra-valued con-
nection form ùúîis given by
ùúîb
a = Œìj
ik xk
a x‚àíb
j
dxi + x‚àíb
j
dxj
a.
(10.95)
The exterior derivative is given by
dùúîb
a =
ùúïŒìj
ik
ùúïxm xk
a x‚àíb
j
dxm ‚àßdxi
+Œìj
ikx‚àíb
j
dxk
a ‚àßdxi ‚àíŒìj
ikxk
ax‚àíb
s x‚àíc
j dxs
c ‚àßdxi
‚àíx‚àíc
j x‚àíb
s
dxs
c ‚àßdxj
a.
(10.96)
A vector such as (10.92) has the following
horizontal component:
h(X) = vi ùúï
ùúïxi ‚àíŒìj
ikxk
avi ùúï
ùúïxj
a
.
(10.97)
With a similar notation, the horizontal
component of another vector Y is given by
h(Y) = wi ùúï
ùúïxi ‚àíŒìj
ikxk
awi ùúï
ùúïxj
a
.
(10.98)
Consider now the following evaluations:
‚ü®dxj ‚àßdxi | h(X) ‚àßh(Y)‚ü©= vjwi ‚àíviwj,
(10.99)
‚ü®dxk
a ‚àßdxi | h(X) ‚àßh(Y)‚ü©
= ‚àíŒìk
rsxs
a (vrwi ‚àíviwr),
(10.100)
and
‚ü®dxj
c ‚àßdxs
a | h(X) ‚àßh(Y)‚ü©
= ‚àíŒìj
rnxn
c Œìs
ikxk
a (vrwi ‚àíviwr). (10.101)
Putting all these results together, we obtain
‚ü®Œ© | X ‚àßY‚ü©= ‚ü®ùúî| h(X) ‚àßh(Y)‚ü©
= xk
ax‚àíb
j Rj
krivrwi,
(10.102)
where
Rj
kri =
Œìj
ik
ùúïxr ‚àí
Œìj
rk
ùúïxi + Œìj
rhŒìh
ik ‚àíŒìj
ihŒìh
rk
(10.103)
is called the curvature tensor of the linear
connection.
In analogy with the concept of curvature
form, we deÔ¨Åne the torsion form of a con-
nection as
Œò = DùúÉ.
(10.104)
Notice that the coupling with the connec-
tion is in the fact that the operator D is
the exterior covariant derivative, which
involves the horizontal projection. To
understand the meaning of the torsion,
consider a case in which the curvature
vanishes. This means that there exists a
distant (or curve-independent) parallelism
in the manifold ÓàÆ. Thus, Ô¨Åxing a basis of
the tangent space at any one point x0 ‚ààÓàÆ,
a Ô¨Åeld of bases is uniquely determined
at all other points. The question that the
torsion tensor addresses is the following:
does there exist a coordinate system such
that these bases coincide at each point with
its natural basis? An interesting example
can be constructed in ‚Ñù3 as follows. Start-
ing from the standard coordinate system,
move up the x3 axis and, while so doing,
apply a linearly increasing rotation to the
horizontal planes, imitating the action of
a corkscrew. Thus, we obtain a system
of (orthonormal) bases that are perfectly

10.6 Riemannian Manifolds
377
Cartesian plane by horizontal plane, but
twisted with respect to each other as we
ascend. These frames can be used to deÔ¨Åne
a distant parallelism (two vectors are paral-
lel if they have the same components in the
local frame). It is not diÔ¨Écult to show (or to
see intuitively) that there is no coordinate
system that has these as natural bases
(use, for example, Frobenius‚Äô theorem).
This example explains the terminology of
‚Äútorsion.‚Äù
To obtain the coordinate expression of
the torsion form, we start by calculating the
exterior derivative of (10.76) as
dùúÉa = dx‚àía
i
‚àßdxi = ‚àíx‚àía
j x‚àíb
i dxj
b ‚àßdxi.
(10.105)
Using (10.100), we obtain
‚ü®DùúÉ| X ‚àßY‚ü©= ‚ü®dùúÉ| h(X) ‚àßh(Y)‚ü©
= x‚àía
j
Tj
ri vrwi,
(10.106)
where
Tj
ri = Œìj
ri ‚àíŒìj
ir
(10.107)
are the components of the torsion tensor of
the connection.
Suppose that a linear connection with
vanishing curvature has been speciÔ¨Åed on
the manifold ÓàÆ, and let e1, ‚Ä¶ , en be a Ô¨Åeld
of parallel frames on the manifold. Then the
ChristoÔ¨Äel symbols of the connection are
given by the formula
Œìi
kj = ‚àíe‚àía
k
ùúïei
a
ùúïxj ,
(10.108)
where ei
a are the components of the frame
in the natural basis of a coordinate system
x1, ‚Ä¶ , xm. The components of the torsion
tensor are proportional to the components
of the Lie brackets of corresponding pairs
of vectors of the frames.
10.6
Riemannian Manifolds
10.6.1
Inner-Product Spaces
We have come a long way without the need
to speak about metric concepts, such as
the length of a vector or the angle between
two vectors. That even the concept of
power of a force can be introduced without
any metric background may have seemed
somewhat surprising, particularly to those
accustomed to hear about ‚Äúthe magnitude
of the force multiplied by the magnitude of
the velocity and by the cosine of the angle
they form.‚Äù It is very often the case in appli-
cations to particular Ô¨Åelds (Mechanics,
Theoretical Physics, Chemistry, Engineer-
ing, etc.) that there is much more structure
to go around than really needed to formu-
late the basic concepts. For the particular
application at hand, there is nothing wrong
in taking advantage of this extra structure.
Quite to the contrary ‚Äì the extra structure
may be the carrier of implicit assumptions
that permit, consciously or not, the for-
mulation of the physical laws. The most
dramatic example is perhaps the adherence
to Euclidean Geometry as the backbone of
Newtonian Physics. On the other hand, the
elucidation of the minimal (or nearly so)
structure necessary for the formulation of
a fundamental notion, has proven time and
again to be the beginning of an enlighten-
ment that can lead to further developments
and, no less importantly, to a better insight
into the old results.
We have seen how the concept of the
space dual to a given vector space arises
naturally from the consideration of linear
functions on the original vector space.
On the other hand, we have learned that,
intimately related as they are, there is
no natural isomorphism between these

378
10 Topics in DiÔ¨Äerential Geometry
two spaces. In other words, there is no
natural way to associate a covector to a
given vector, and vice versa. In Newtonian
Mechanics, however, the assumption of a
Euclidean metric, whereby the theorem of
Pythagoras holds globally, provides such
identiÔ¨Åcation. In Lagrangian Mechanics, it
is the kinetic energy of the system that can
be shown to provide such extra structure,
at least locally. In General Relativity, this
extra structure (but of a somewhat dif-
ferent nature) becomes the main physical
quantity to be found by solving Einstein‚Äôs
equations. In all these cases, the identiÔ¨Åca-
tion of vectors with covectors is achieved
by means of the introduction of a new
operation called an inner product (or a
dot product or, less felicitously, a scalar
product).
A vector space V is said to be an inner-
product space if it is endowed with an oper-
ation (called an inner product)
‚ãÖ‚à∂V √ó V ‚àí‚Üí‚Ñù
(u, v) ÓÇ∂‚Üíu ‚ãÖv,
(10.109)
satisfying the following properties:5)
1. Commutativity
u ‚ãÖv = v ‚ãÖu,
‚àÄu, v ‚ààV;
(10.110)
2. Bilinearity6)
(ùõºu1 + ùõΩu2) ‚ãÖv = ùõº(u1 ‚ãÖv) + ùõΩ(u2 ‚ãÖv),
‚àÄùõº, ùõΩ‚àà‚Ñù, u1, u2, v ‚ààV;
(10.111)
5) It is to be noted that in the case of a complex
vector space, such as in Quantum Mechanics
applications, these properties need to be altered
somewhat.
6) The term bilinearity refers to the fact that the
inner product is linear in each of its two argu-
ments. Nevertheless, given that we have already
assumed commutativity, we need only to show
linearity with respect to one of the arguments.
3. Positive deÔ¨Åniteness7)
v ‚â†0
=‚áí
v ‚ãÖv > 0.
(10.112)
One can show that 0 ‚ãÖv = 0, for all v.
The magnitude or length of a vector v is
deÔ¨Åned as the nonnegative number
‚àö
v ‚ãÖv.
Two vectors u, v ‚ààV are called orthogonal
(or perpendicular) to each other if u ‚ãÖv = 0.
We want now to show how the existence
of an inner product induces an isomor-
phism between a space and its dual (always
in the Ô¨Ånite-dimensional case). Let v ‚ààV
be a Ô¨Åxed element of V. By the linearity of
the inner product, the product v ‚ãÖu is linear
in the second argument. Accordingly, we
deÔ¨Åne the covector ùúîv ‚ààV ‚àócorresponding
to the vector v ‚ààV, by
‚ü®ùúîv, u‚ü©= v ‚ãÖu,
‚àÄu ‚ààV.
(10.113)
It is not diÔ¨Écult to prove that this linear
map from V to V ‚àóis one-to-one and that,
therefore, it constitutes an isomorphism
between V and V ‚àó. We conclude that in an
inner-product space there is no need to dis-
tinguish notationwise between vectors and
covectors.
We call reciprocal basis the basis of V
that corresponds to the dual basis in the
isomorphism induced by the inner product.
We already know that the dual basis oper-
ates on vectors in the following way:
‚ü®ei, v‚ü©= vi,
‚àÄv ‚ààV,
(10.114)
where vi is the ith component of v ‚ààV
in the basis {ej} (j = 1, ‚Ä¶ , n). The recip-
rocal basis, therefore, consists of vectors
{ej} (j = 1, ‚Ä¶ , n) such that
ei ‚ãÖv = vi,
‚àÄv ‚ààV.
(10.115)
7) In Relativity, this property is removed.

10.6 Riemannian Manifolds
379
Let the components of the reciprocal base
vectors be expressed as
ei = gijej.
(10.116)
In other words, we denote by gij the jth
component of the ith member of the recip-
rocal basis we are seeking. It follows from
(10.115) that
ei ‚ãÖv = (gijej) ‚ãÖ(vkek)
= gij (ej ‚ãÖek) vk = vi,
‚àÄvk ‚àà‚Ñù. (10.117)
Looking at the very last equality, it follows
that
gij (ej ‚ãÖek) = ùõøi
k.
(10.118)
Indeed, regarded as a matrix equation,
(10.117) establishes that the matrix with
entries
[gij (ej ‚ãÖek)]
(summation
con-
vention
understood),
when
multiplied
by an arbitrary column vector, leaves it
unchanged. It follows that this matrix must
be the identity. This is only possible if the
matrix with entries
gij = ei ‚ãÖej,
(10.119)
is the inverse of the matrix with entries
gij. So, the procedure to Ô¨Ånd the recip-
rocal basis is the following: (i) Construct
the (symmetric) square matrix with entries
gij = ei ‚ãÖej. (ii) Invert this matrix to obtain
the matrix with entries gij. (iii) DeÔ¨Åne ei =
gijej. Note that the metric matrix {gij} is
always invertible, as it follows from the lin-
ear independence of the basis.
A basis of an inner-product space is
called orthonormal if all its members are
of unit length and mutually orthogonal.
The reciprocal of an orthonormal basis
coincides with the original basis.
Having identiÔ¨Åed an inner-product space
with its dual, and having brought back the
dual basis to the original space under the
guise of the reciprocal basis, we have at
our disposal contravariant and covariant
components of vectors. Recall that before
the introduction of an inner product, the
choice of a basis in V condemned vectors to
have contravariant components only, while
the components of covectors were covari-
ant.
Starting from v = viei = viei and using
(10.118) and (10.119), the following formu-
las can be derived:
vi = gijvj,
(10.120)
vi = gijvj,
(10.121)
ei = gijej,
(10.122)
vi = v ‚ãÖei,
(10.123)
vi = v ‚ãÖei,
(10.124)
ei ‚ãÖej = gij,
(10.125)
ei ‚ãÖej = ùõøi
j.
(10.126)
A
linear
map
Q ‚à∂U ‚ÜíV
between
inner-product spaces is called orthogonal
if QQT = idV and QTQ = idU, where id
stands for the identity map in the subscript
space. The components of an orthogonal
linear map in orthonormal bases of both
spaces comprise an orthogonal matrix.
A linear map T between inner-product
spaces preserves the inner product if,
and only if, it is an orthogonal map. By
preservation of inner product, we mean
that T(u) ‚ãÖT(v) = u ‚ãÖv, ‚àÄu, v ‚ààU.
10.6.2
Riemannian Manifolds
If each tangent space TxÓàπof the mani-
fold Óàπis endowed with an inner product,
and if this inner product depends smoothly

380
10 Topics in DiÔ¨Äerential Geometry
on x ‚ààÓàπ, we say that Óàπis a Riemannian
manifold. To clarify the concept of smooth-
ness, let {ÓâÅ, ùúô} be a chart in Óàπwith coor-
dinates x1, ‚Ä¶ , xn. This chart induces the
(smooth) basis Ô¨Åeld ùúï‚àïùúïx1, ‚Ä¶ , ùúï‚àïùúïxn. We
deÔ¨Åne the contravariant components of the
metric tensor g associated with the given
inner product (indicated by ‚ãÖ) as
gij =
( ùúï
ùúïxi
)
‚ãÖ
( ùúï
ùúïxj
)
.
(10.127)
Smoothness means that these components
are smooth functions of the coordinates
within the patch. The metric tensor itself is
given by
g = gij dxi ‚äódxj.
(10.128)
We have learned how an inner product
deÔ¨Ånes an isomorphism between a vector
space and its dual. When translated to Rie-
mannian manifolds, this result means that
the tangent and cotangent bundles are nat-
urally isomorphic (via the pointwise iso-
morphisms of the tangent and cotangent
spaces induced by the inner product).
A nontrivial physical example is found in
Lagrangian Mechanics, where the kinetic
energy (assumed to be a positive-deÔ¨Ånite
quadratic form in the generalized veloci-
ties) is used to view the conÔ¨Åguration space
ÓàΩas a Riemannian manifold.
10.6.3
Riemannian Connections
The theory of Riemannian manifolds is very
rich in results. Classical diÔ¨Äerential geome-
try was almost exclusively devoted to their
study and, more particularly, to the study of
two-dimensional surfaces embedded in ‚Ñù3,
where the Riemannian structure is derived
from the Euclidean structure of the sur-
rounding space.
A Riemannian connection is a linear con-
nection on a Riemannian manifold. The
most important basic result for Riemannian
connections is contained in the following
theorem:
Theorem 10.1 On a Riemannian manifold
there exists a unique linear connection with
vanishing torsion and such that the covari-
ant derivative of the metric vanishes identi-
cally.
We omit the proof. The ChristoÔ¨Äel sym-
bols of this connection are given in terms of
the metric tensor by the formula
Œìk
ij = 1
2 gkh
(ùúïgih
ùúïxj +
ùúïgjh
ùúïxi ‚àí
ùúïgij
ùúïxh
)
.
(10.129)
The curvature tensor associated with this
special connection is called the Riemann‚Äì
ChristoÔ¨Äel curvature tensor. A Riemannian
manifold is said to be locally Ô¨Çat if, for each
point, a coordinate chart can be found such
that the metric tensor components every-
where in the chart reduce to the identity
matrix. It can be shown that local Ô¨Çatness is
equivalent to the identical vanishing of the
Riemann‚ÄìChristoÔ¨Äel curvature tensor.
Further Reading
Some general treatises on DiÔ¨Äerential
Geometry:
Chern, S.S., Chern, W.H., and Lam, K.S. (1999)
Lectures on DiÔ¨Äerential Geometry, World
ScientiÔ¨Åc.
Kobayashi, S. and Nomizu, K. (1996) Foundations
of DiÔ¨Äerential Geometry, Wiley Classics
Library Edition.
Lee, J.M. (2003) Introduction to Smooth
Manifolds, Springer.
Sternberg, S. (1983) Lectures on DiÔ¨Äerential
Geometry, 2nd edn, Chelsea Publishing
Company.

Further Reading
381
Warner, F.W. (1983) Foundations of DiÔ¨Äerentiable
Manifolds and Lie Groups, Springer.
Some
books
that
emphasize
physi-
cal applications or deal with particular
physical theories in a geometric way are
Abraham, R. and Marsden, J.E. (2008)
Foundations of Mechanics, 2nd edn, AMS
Chelsea Publishing.
Arnold, V.I. (1978) Mathematical Methods of
Classical Mechanics, Springer.
Choquet-Bruhat, Y., de Witt-Morette, C., and
Dillard-Beck, M. (1977) Analysis, Manifolds
and Physics, North-Holland.
Frankel, T. (2004) The Geometry of Physics: An
Introduction, 2nd edn, Cambridge University
Press.
Misner, W., Thorne, K.S., and Wheeler, J.A.
(1973) Gravitation, W H Freeman and
Company.
Much of the material in this article is
reproduced, with permission, from
Epstein, M. (2010) The Geometrical Language of
Continuum Mechanics, Cambridge University
Press.


383
Part III
Analysis
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.


385
11
Dynamical Systems
David A.W. Barton
11.1
Introduction
11.1.1
DeÔ¨Ånition of a Dynamical System
Dynamical systems are deÔ¨Åned using three
key ingredients: a state space, a time set, and
an evolution operator.
The state space X of a system is the
set of all possible states of a system. For
many purposes X = ‚Ñùn is a suitable state
space, though for some dynamical systems
(notably partial diÔ¨Äerential equations or
delay diÔ¨Äerential equations) the state space
may be a more general Banach space.
The time set T is the set of times at
which a system is deÔ¨Åned. The two stan-
dard cases are T = ‚Ñùfor continuous-time
dynamical systems or T = ‚Ñ§for discrete-
time systems. Only continuous-time sys-
tems are considered here, though many of
the results carry through directly to dis-
crete time systems.
The evolution operator forms the core of
a dynamical system. The evolution operator
ùúôt ‚à∂X ‚ÜíX maps an initial state forward by
t ‚ààT time units, that is, x(t) = ùúôtx0 where
x0 is the initial state at time t = 0. In cer-
tain contexts (e.g., discrete maps), the evo-
lution operator is given explicitly. In most
contexts, however, it is usual for the evo-
lution operator to be deÔ¨Åned implicitly, for
example, through the solution of a diÔ¨Äeren-
tial equation.
Evolution operators have two deÔ¨Åning
characteristics, namely,
ùúô0x0 = x0
(‚Äúno time, no evolution‚Äù)
(11.1)
and
ùúôt+sx0 = ùúôt(ùúôsx0)
(‚Äúdeterminism‚Äù).
(11.2)
Note that nonautonomous or stochastic
systems are excluded by this deÔ¨Ånition;
however, similar formalisms can be derived
for those types of systems [1].
Finally, a dynamical system is formally
deÔ¨Åned as the triple {X, T, ùúôt}, where X is
the state space, T is the time set, and ùúôt
is the evolution operator of the dynamical
system.
Two further concepts, which are impor-
tant in the study of dynamical systems, are
orbits and ùúî-limit sets. For a given point in
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

386
11 Dynamical Systems
state space x0, the orbit that passes through
this point is given by the set {ùúôt(x0)|‚àÄt ‚àà
T}. Conversely, the ùúî-limit set of a point x0
is the set of points {limt‚Üí‚àûùúôt(x0)}.
For a more detailed introduction to the
theory of dynamical systems than this
chapter permits, see [2‚Äì4].
11.1.2
Invariant Sets
The main objects of interest when study-
ing dynamical systems are the invariant sets
of a system, that is, the subsets of state
space that are invariant under the action of
the evolution operator (e.g., equilibria). The
study of these invariant sets, and the asso-
ciated long-time dynamics, provides exten-
sive information about the behavior of the
system without the need for determining
transient behavior, which can be a diÔ¨Écult
task.
The two basic types of invariant set con-
sidered in this chapter are as follows.
1. Equilibria. The simplest type of
invariant set is an equilibrium x‚àó‚ààX
such that x‚àó= ùúôt(x‚àó) for all t. A trivial
example of an equilibrium can be found
in the (linear) dynamical system deÔ¨Åned
by dx‚àïdt = ùúáx for ùúá‚â†0 (i.e., x‚àó= 0).
2. Periodic orbits. These are deÔ¨Åned by a
function x(t) = f (ùúît), where f is a
continuous and periodic (with period
2ùúã) function that satisÔ¨Åes the
underlying dynamical system, and ùúîis
the frequency, of the orbit. Periodic
orbits that are isolated (i.e., no points
neighboring the periodic orbit are part
of another periodic orbit) are also
known as limit cycles.
This is by no means an exhaustive list.
Several obvious omissions are given below.
‚Ä¢ Homoclinic and heteroclinic orbits.
These are deÔ¨Åned by the points
x such that ùúôt(x) ‚Üíxùúîas t ‚Üí‚àû
and
ùúôt(x) ‚Üíxùõº
as
t ‚Üí‚àí‚àû.
For
homoclinic orbits, xùõº= xùúî, and for
heteroclinic orbits, xùõº‚â†xùúî.
‚Ä¢ Quasi-periodic
orbits.
These
are
deÔ¨Åned by a multivariate function
x(t) = f (ùúî1t, ‚Ä¶ , ùúînt)
that
satisÔ¨Åes
the underlying dynamical system.
‚Ä¢ Strange attractors. These are associ-
ated with chaotic dynamics.
However, these and other invariant sets are
beyond the scope of this chapter.
This chapter focuses on the dynamics
local to these invariant sets and the con-
ditions under which the dynamics change
(so-called bifurcations).
11.2
Equilibria
11.2.1
DeÔ¨Ånition and Calculation
Consider the ordinary diÔ¨Äerential equation
(ODE)
dx
dt = f (x, ùúá),
x ‚àà‚Ñùn, ùúá‚àà‚Ñù,
(11.3)
where f ‚à∂‚Ñùn √ó ‚Ñù‚Üí‚Ñùn, x is the state of
the system, and ùúáis a system parameter.
The equilibria of (11.3) are deÔ¨Åned as the
points where dx‚àïdt = 0. As such, for a
particular choice of the parameter ùúá, there
may exist any number of equilibria. The
calculation of equilibria is a root-Ô¨Ånding
problem f (x‚àó, ùúá) = 0 and, for any nontrivial
f , numerical root-Ô¨Ånding methods are
often needed.

11.2 Equilibria
387
11.2.2
Stability
In the context of dynamical systems, there
are several diÔ¨Äerent notions of stability for
equilibria. The two key notions are asymp-
totic stability and Lyapunov stability.
An equilibrium x‚àóis said to be asymptoti-
cally stable if the ùúî-limit set of all the neigh-
boring points is the single point x‚àó. Thus, if
the system is perturbed slightly away from
the equilibrium position, the system will
always return to the equilibrium.
Lyapunov stability is a weaker concept.
An equilibrium x‚àóis said to be Lyapunov
stable if for every neighborhood U of x‚àó
there exists a smaller neighborhood U1 of
x‚àócontained within U such that all solu-
tions starting in U1 remain within U for
all time. Thus if the system is perturbed
slightly away from the equilibrium position,
the system will always stay in the vicinity of
the equilibrium. Clearly, asymptotic stabil-
ity implies Lyapunov stability.
11.2.3
Linearization
In the vicinity of an equilibrium x‚àó, the
ODE (11.3) can be linearized to give
dÃÉx
dt = J ÃÉx,
(11.4)
where J is the Jacobian matrix deÔ¨Åned by
J = [Ji,j
]|||x=x‚àó=
[ ùúïfi
ùúïxj
]|||||x=x‚àó.
(11.5)
The Jacobian matrix contains a great deal of
information about the equilibrium, in par-
ticular, about its stability. An exponential
solution of (11.4) reveals that the growth
or decay of solutions is determined by the
eigenvalues of J. As such, the asymptotic
stability of the equilibria is determined by
the sign of the real part of the eigenval-
ues; for an equilibrium to be stable all
the eigenvalues must have negative real
parts. An equilibrium with eigenvalues that
have both positive and negative real parts
is called a saddle (see, e.g., Figure 11.1b).
Should any of the eigenvalues be complex,
the resulting solutions will have spiraling
behavior (see, e.g., Figure 11.1d and e).
If there are no eigenvalues on the imagi-
nary axis (i.e., none with zero real part) the
equilibrium is said to be hyperbolic.
In the case of a hyperbolic equilibrium,
it is possible to formalize the link between
the original nonlinear dynamical system
and the linearized system through the idea
of topological equivalence, as described in
Section 11.2.5.
Associated with stable equilibria (and
other stable invariant sets) is the notion
of a basin of attraction, that is, the set of
points in state space that, when evolved
forward in time, approach the equilibrium.
Formally, the basin of attraction for an
equilibrium x‚àóis deÔ¨Åned as
B(x‚àó) =
{
x ‚à∂lim
t‚Üí‚àûùúôt(x) = x‚àó}
.
(11.6)
The basins of attraction for diÔ¨Äerent
equilibria become particularly important
when considering the initial value problem
of a dynamical system. The equilibrium
state reached in the long-time limit (assum-
ing the system equilibrates) is determined
by the basin of attraction that the initial
conditions lie in. Various methods exist for
Ô¨Ånding basins of attraction including man-
ifold computations (see Section 11.2.6),
cell mapping [5], and subdivision [6]; typ-
ically, these methods are only applied to
low-dimensional systems as they do not
scale well to larger systems.

388
11 Dynamical Systems
(a)
x
y
(b)
x
y
(c)
x
y
(d)
x
y
(e)
x
y
Figure 11.1
Possible phase portraits of a 2D
linear dynamical system, which show the evo-
lution in state space of several orbits. The
equilibria are classiÔ¨Åed as (a) a stable node;
(b) a saddle; (c) an unstable node; (d) a stable
spiral; (e) an unstable spiral. Only three of the
phase portraits are topologically distinct as
(a) and (d) are topologically equivalent, as are
(c) and (e).
11.2.4
Lyapunov Functions
While the Jacobian matrix provides infor-
mation about the local stability of an equi-
librium, for nontrivial systems it is often
diÔ¨Écult, if not impossible, to calculate the
eigenvalues analytically and so determine
the stability. Furthermore, the Jacobian only
provides a means to determine the local
dynamics and not the global dynamics as is
sometimes desired.
A
common
alternative
approach
to
studying the stability of an equilibrium is
to construct a so-called Lyapunov function.
A Lyapunov function V is a continuous
scalar function of the state variables that
has the following properties (assuming the
equilibrium has been shifted to the origin).
‚Ä¢ V is positive deÔ¨Ånite, that is,
V(x) > 0
‚àÄx ‚ààX‚àñ{0}.
‚Ä¢ dV‚àïdt is negative deÔ¨Ånite, that is,
(d‚àïdt)V(x) < 0
‚àÄx ‚ààX‚àñ{0}.
If these conditions are satisÔ¨Åed, then
the equilibrium is globally asymptotically
stable. If the conditions are weakened
such that V (respectively dV‚àïdt) is locally
positive deÔ¨Ånite (respectively locally neg-
ative deÔ¨Ånite), then the equilibrium will be
locally asymptotically stable. It is possible
to weaken the second condition further
and require only that dV‚àïdt be negative
semi-deÔ¨Ånite (i.e., (d‚àïdt)V(x) ‚â§0
‚àÄx ‚àà
X‚àñ{0}); in this case the equilibrium is said
to be Lyapunov stable.
The main diÔ¨Éculty of using a Lyapunov
function approach is that there are no uni-
versally applicable rules for constructing
them; trial and error, in general, is the only
way. However, for some systems, it is possi-
ble to appeal to physical conservation laws
to Ô¨Ånd a Lyapunov function. One example
is the DuÔ¨Éng equation
d2y
dt2 + ùúâdy
dt + y + ùõºy3 = 0,
(11.7)

11.2 Equilibria
389
where the state space is [y, Ãáy]. When ùúâ= 0,
(11.7) can be multiplied by Ãáy and integrated
with respect to time to arrive at
E(t) = 1
2
(
Ãáy2 + y2 + 1
2ùõºy4)
= const. (11.8)
When ùúâ> 0, ùõº> 0, this function is a Lya-
punov function. Clearly, with the given con-
straints, E(t) > 0 for all nonzero [y, Ãáy] and is
only zero at the equilibrium position [0, 0].
DiÔ¨Äerentiating this expression with respect
to time gives
dE
dt = dy
dt
(d2y
dt2 + y + ùõºy3
)
(11.9)
= dy
dt
(
‚àí
(
ùúâdy
dt + y + ùõºy3
)
+ y + ùõºy3
)
(11.10)
= ‚àíùúâ
(dy
dt
)2
.
(11.11)
Thus the time derivative is negative for
all values of Ãáy > 0. However, dE‚àïdt = 0
regardless of the value of y when Ãáy = 0. As
such, the conditions for asymptotic stabil-
ity are not satisÔ¨Åed but the equilibrium can
be said to be Lyapunov stable. For ùúâ> 0,
the origin is actually asymptotically stable
(as can be veriÔ¨Åed by the linearization); the
choice of Lyapunov function is deÔ¨Åcient in
this case.
One further beneÔ¨Åt of using Lyapunov
functions to prove stability is that Lya-
punov functions can be used even when the
equilibrium in question is nonhyperbolic
(unlike linearization). Take for example the
following system
dx
dt = y ‚àíx3
(11.12)
dy
dt = ‚àíx ‚àíy3.
(11.13)
The linearization of this system yields
purely imaginary eigenvalues. However,
the Lyapunov function V = (1‚àï2) (x2 + y2)
can be used to show that the equilibrium at
x, y = 0 is asymptotically stable.
11.2.5
Topological Equivalence
Another key concept in dynamical systems
is the idea of topological equivalence; it
is a relationship between two dynamical
systems that ensures that their dynamics
are equivalent in a certain sense. Thus, pro-
vided certain criteria are met, the dynamics
of a seemingly complicated system can be
replaced by an equivalent but simpler sys-
tem. Put formally, two dynamical systems
are topologically equivalent if there exists a
homeomorphism that maps orbits of one
onto the orbits of the other, preserving the
direction of time. Typically, this homeo-
morphism is not known explicitly but its
existence can be inferred using various
theorems. Furthermore, topological equiv-
alence is not necessarily a global property,
applying to the whole of state space, but
instead may only be a local property, apply-
ing to the neighborhood of an equilibrium
(or other invariant set).
Two important theorems relating to
topological equivalence are as follows.
Equivalence of linear Ô¨Çows. Two linear
dynamical systems are topologically
equivalent if, and only if, they have
the same number of eigenvalues with
positive real part, the same number
of eigenvalues with negative real part,
and the same number of eigenvalues
with zero real part.
Hartman‚ÄìGrobman theorem. A (nonlin-
ear) dynamical system is topologically
equivalent to its linearization in the
neighborhood of an equilibrium pro-
vided the equilibrium is hyperbolic.

390
11 Dynamical Systems
Thus, the dynamics near a hyperbolic
equilibrium are essentially linear.
The Hartman‚ÄìGrobman theorem com-
bined with the equivalence of linear Ô¨Çows is
a very powerful tool when analyzing non-
linear systems. For example, the dynam-
ics near a hyperbolic equilibrium of the 2D
dynamical system
Ãáx = f (x, y)
Ãáy = g(x, y)
(11.14)
will be topologically equivalent to one of
the phase portraits shown in Figure 11.1
as determined by the eigenvalues of its
linearization.
Similar
eigenvalue-based
characterizations are possible in higher
dimensions, but graphical representations
become more diÔ¨Écult.
Furthermore, these two theorems ensure
that, in the neighborhood of a hyperbolic
equilibrium, small changes in the system
parameters will not (topologically) change
the dynamics. For the dynamics to change,
the equilibrium must Ô¨Årst become nonhy-
perbolic (i.e., one of the eigenvalues lies
on the imaginary axis); in this case, the
Hartman‚ÄìGrobman theorem fails. When
this occurs, the system is said to be struc-
turally unstable, that is, small changes in
the equation can cause topological changes
in the local dynamics. If these small changes
can be realized by varying system parame-
ters, then the system is said to be at a bifur-
cation point.
11.2.6
Manifolds
A hyperbolic saddle-type equilibrium xs
has eigenvalues that have both negative
and positive real parts and so it has direc-
tions in which it is attracting and direc-
tions in which it is repelling. As such, it
possesses a stable manifold Ws(xs), corre-
sponding to the set of points in state space
that approach the equilibrium in forward
time, and an unstable manifold Wu(xs), cor-
responding to the set of points in state space
that approach the equilibrium in backward
time. The stable and unstable manifolds are
thus deÔ¨Åned as follows
Ws(xs) =
{
x ‚à∂lim
t‚Üí‚àûùúôt(x) = xs
}
(stable manifold)
(11.15)
and
Wu(xs) =
{
x ‚à∂lim
t‚Üí‚àí‚àûùúôt(x) = xs
}
(unstable manifold).
(11.16)
The dimension of the stable and unstable
manifolds is equal to the number of eigen-
values with negative or positive real parts,
respectively.
These manifolds are said to be invariant
manifolds; an orbit of the dynamical system
that starts in the (un)stable manifold will
remain contained within the manifold for
all time.
Should an equilibrium be nonhyper-
bolic (i.e., it has eigenvalues with zero real
part), it will also possess a center manifold;
these play a vital part when looking at
bifurcations and will be discussed later in
Section 11.2.11.
Stable manifolds act as separatrices in
state space; that is, they form boundaries
between the basins of attraction of diÔ¨Äer-
ent attractors in the system. This is most
easily seen and exploited in 2D dynamical
systems, see, for example, Figure 11.2, but
also holds true for higher-dimensional sys-
tems. (Note that while a stable manifold can
act as a separatrix, not all separatrices are
stable manifolds.)
Both stable and unstable manifolds are
global objects and in general can only be

11.2 Equilibria
391
2
2
1
1
0
0
x
‚àí1
‚àí1
‚àí2
‚àí2
x
Ws (xs)
Ws (xs)
Wu (xs)
Wu (xs)
Figure 11.2
A phase portrait
of the bistable DuÔ¨Éng equation
Ãàx + Ãáx ‚àíx + x3 = 0 that shows
how the stable manifold Ws
of the saddle equilibrium acts
as a separatrix, dividing the
basins of attraction of the two
stable equilibria. The stable
and unstable manifolds of
the saddle equilibrium are
marked as solid curves and
representative orbits are shown
as dashed curves.
found numerically. However, a local (lin-
ear) approximation to the stable and unsta-
ble manifolds of a saddle equilibrium xs
is given by the eigenvectors correspond-
ing to the stable and unstable eigenvalues,
respectively.
This local approximation suggests a
straightforward method for globalizing a
one-dimensional unstable manifold; simply
choose a starting point close to the equi-
librium along the unstable eigendirection
and evolve the dynamical system forward
in time, that is
Wu(xs) ‚âà{ùúôt(xs + ùúñeu)
‚àÄt > 0} , (11.17)
where eu is the normalized unstable eigen-
vector. The accuracy of the approximation
is determined by the size of ùúñand the
accuracy of the numerical integrator used.
A one-dimensional stable manifold can be
found in a similar manner by reversing the
direction of time to give
Ws(xs) ‚âà{ùúôt(xs + ùúñes)
‚àÄt < 0} , (11.18)
where
es
is
the
normalized
stable
eigenvector.
This approach of globalizing a one-
dimensional
manifold
can
naively
be
extended to two-dimensional manifolds
(and higher) by evolving forward (or back-
ward) in time a set of points chosen from a
circle around the equilibrium in the plane
spanned by the corresponding eigenvec-
tors. This may work in simple cases, but
when the manifold has nontrivial geometry
or the dynamics on the manifold are not
uniform (e.g., when the corresponding
eigenvalues are signiÔ¨Åcantly diÔ¨Äerent in
size), this method will not produce accurate
results. More sophisticated methods are
required. For a comprehensive overview of
methods for calculating manifolds, see [7].
11.2.7
Local Bifurcations
Often the main interest when studying a
particular dynamical system is to under-
stand what happens to the dynamics as
the system parameters change. As stated
in Section 11.2.3, the dynamics in the
neighborhood of an equilibrium can only
change topologically if the equilibrium
becomes nonhyperbolic as a parameter is
varied, causing the Hartman‚ÄìGrobman
theorem to fail. Thus the presence of a non-
hyperbolic equilibrium is a sign that the
dynamical system is structurally unstable,
that is, arbitrarily small parameter pertur-
bations will lead to topological changes in
the dynamics.

392
11 Dynamical Systems
In the case of an equilibrium becoming
nonhyperbolic, the loss of structural stabil-
ity indicates the presence of a local bifur-
cation. The bifurcation is said to be local
because away from the neighborhood of
the equilibrium, the system dynamics will
remain topologically unchanged. To bring
about a global change in the dynamics, a
global bifurcation is required (often involv-
ing the stable and unstable manifolds of a
saddle equilibrium). Global bifurcations are
beyond the scope of this chapter but are
covered in detail in [4].
There are two generic local bifurcations
of equilibria: the saddle-node bifurcation
(otherwise known as a fold owing to its
relationship to singularity theory) and the
Hopf bifurcation. They are generic in the
sense that they can be expected to occur
in an arbitrary dynamical system as a sin-
gle parameter is varied. They are codimen-
sion one bifurcations; if the system has
two parameters, then there will be a one-
dimensional curve of parameter values at
which the bifurcation occurs within the
two-dimensional parameter plane.
All other bifurcations either require
special
properties,
such
as
symme-
try, or are nongeneric (of codimension
higher than one), that is, they can only
be expected to occur in an arbitrary
dynamical system when two or more
parameters
are
varied
simultaneously.
Again, if the system has two parameters,
then for a codimension-two bifurcation
there will be a zero-dimensional point at
which the bifurcation occurs within the
two-dimensional parameter plane.
As
many
physical
systems
contain
symmetries (at least approximately), the
symmetric pitchfork bifurcation, although
nongeneric, is of particular interest and
so is also covered below, along with
the saddle-node bifurcation and Hopf
bifurcation.
11.2.8
Saddle-Node Bifurcation
At a saddle-node bifurcation (fold) two
equilibria collide at a point in state space
and are destroyed. This is most easily seen
in the simple dynamical system deÔ¨Åned by
dx
dt = ùúá‚àíx2,
ùúá, x ‚àà‚Ñù.
(11.19)
Equation (11.19) has two equilibria for
ùúá> 0, one stable (x = ‚àöùúá) and one unsta-
ble (x = ‚àí‚àöùúá). For ùúá< 0, it does not pos-
sess any equilibria; instead x ‚Üí‚àí‚àûas t ‚Üí
‚àûfor all initial conditions. This is shown
graphically in Figure 11.3a and b. When
ùúá= 0, the equilibria coincide at x = 0 and
the linearization of (11.19) indicates that
they are nonhyperbolic. Thus (ùúá= 0, x = 0)
is the bifurcation point.
In terms of the eigenvalues of the
associated Jacobian of an equilibrium, a
saddle-node bifurcation corresponds to an
eigenvalue passing through zero. It should
be noted that this characterization in terms
of eigenvalues is not unique; in the pres-
ence of other properties (e.g., symmetry),
an eigenvalue passing through zero can
correspond to a transcritical or pitchfork
bifurcation. As such, to be sure that a
bifurcation is a saddle-node bifurcation,
certain genericity conditions must be met.
The saddle-node bifurcation theorem is
as follows.
Theorem 11.1 Consider
the
one-
dimensional dynamical system
dx
dt = f (x, ùúá),
x ‚àà‚Ñù, ùúá‚àà‚Ñù,
(11.20)
where f is smooth and has an equilib-
rium x = 0 at ùúá= 0. Furthermore, let
ùúÜ= fx(0, 0) = 0. If the genericity condi-
tions (1) fxx(0, 0) ‚â†0 and (2) fùúá(0, 0) ‚â†0
are satisÔ¨Åed, then (11.20) is topologically

11.2 Equilibria
393
(a)
(b)
ùúá = ‚àí0.25
ùúá = 0.25
‚àí0.5
0.5
ùúá = 0.75
x
x
‚àí1
1
1
0
0
ùúá
Figure 11.3
(a) A one-dimensional phase portrait for
(11.19) as ùúáis varied; the equilibria are marked as circles
and the arrows show the direction of Ô¨Çow. (b) The cor-
responding one-parameter bifurcation diagram with the
saddle-node bifurcation occurring at ùúá= 0, x = 0. Solid
curves mark the stable equilibria and dashed lines mark the
unstable equilibria.
equivalent near the origin to the normal
form
dy
dt = ùõº¬± y2.
(11.21)
The power of this theorem is that the
dynamics of any system satisfying the
bifurcation conditions will be topologically
equivalent to (11.21).
This
theorem
only
applies
to
one-
dimensional
systems;
however,
multi-
dimensional systems that have a single
eigenvalue passing through zero can be
reduced to a one-dimensional system using
a center manifold reduction as described
in Section 11.2.11.
A common tool to visualize the eÔ¨Äect of
parameter changes on the dynamics is the
use of a bifurcation diagram whereby the
equilibria (and other invariant sets) of the
system are plotted using a suitable norm
against the parameter value. The bifur-
cation diagram associated with (11.19) is
shown in Figure 11.3b where it can clearly
be seen that the two equilibria collide and
disappear at ùúá= 0.
Hysteresis loops in a system are often the
result of a pair of saddle-node bifurcations.
A common example is that of a periodi-
cally forced mass‚Äîspring‚Äìdamper system
with
hardening
spring
characteristics
(d2x‚àïdt2) + 2ùúâdx‚àïdt + x + ùõΩx3 = Œì sin(ùúît).
A simpler, analytically treatable equation
(with exact solutions) containing this type
of behavior is
dx
dt = ùúá+ ùúéx ‚àíx3.
(11.22)
A one-parameter bifurcation diagram for
(11.22) showing x plotted against ùúáfor
ùúé= 1 is shown in Figure 11.4a. The second
parameter, ùúé, controls the relative positions
(in terms of ùúá) of the saddle-node bifur-
cations. As ùúé‚Üí0 from above, the saddle-
node bifurcations meet at a point before
disappearing in a codimension-two cusp
bifurcation as shown in the two-parameter
bifurcation diagram Figure 11.4b, where ùúá
is plotted against ùúé.
11.2.9
Hopf Bifurcation
At a Hopf bifurcation (also known as an
Andronov‚ÄìHopf bifurcation), an equilib-
rium becomes unstable as a pair of complex

394
11 Dynamical Systems
(a)
(b)
0
0
ùúá
ùúá
0
0
2
‚àí2
‚àí1
‚àí1
1
1
‚àí1
1
x
ùúé
Figure 11.4
(a) A one-parameter bifurcation
diagram for (11.22) as ùúávaries. Solid curves
mark the stable equilibria and dashed lines
mark the unstable equilibria. A potential hys-
teresis loop that occurs as ùúáis varied is marked
by arrows. (b) A two-parameter bifurcation dia-
gram for the same equation with the solid
curve being a saddle-node bifurcation curve.
A cusp bifurcation occurs at ùúá= 0, ùúé= 0. Inside
the gray shaded region there exist three equi-
libria (two stable and one unstable); every-
where else there exists only one (stable) equi-
librium.
conjugate eigenvalues pass through the
imaginary axis. At the bifurcation point,
a limit cycle is created, which then grows
in amplitude as the system parameters
are changed further. Thus Hopf bifur-
cations are commonly associated with
vibration problems, particularly the onset
of vibrations. A prototype example of a
Hopf bifurcation is given by
dx
dt = ùúáx ‚àíùúîy + ùõºx(x2 + y2),
dy
dt = ùúîx + ùúáy + ùõºy(x2 + y2).
(11.23)
Alternatively, (11.23) can be written in
complex form
dz
dt = (ùúá+ iùúî)z + ùìÅ1|z|2z.
(11.24)
In both forms, ùúáis the bifurcation param-
eter, ùúîis the frequency of the bifurcating
limit cycle (which is equal to the imaginary
part of the eigenvalues passing through
the imaginary axis) and ùìÅ1 is a param-
eter known as the Ô¨Årst Lyapunov coeÔ¨É-
cient, which determines the criticality of
the bifurcation. The Hopf bifurcation can
be either subcritical (ùìÅ1 > 0) or supercrit-
ical (ùìÅ1 < 0).
Figure 11.5
shows
a
one-parameter
bifurcation diagram for (11.23) in the
supercritical case; to obtain the subcritical
case, simply reverse all the directions of
Ô¨Çow indicated in the Ô¨Ågure. Thus, in the
supercritical case, a stable limit cycle is
created and, in the subcritical case, an
unstable limit cycle is created.
If (11.23) is rewritten in polar form,
x = r cos(ùúÉ) and y = r sin(ùúÉ), it is clear that
a Hopf bifurcation is really a pitchfork
bifurcation of the equation governing the
growth of r. As such, a one-parameter
bifurcation diagram showing the growth in
amplitude of the limit cycles that emerge
from the Hopf bifurcation is shown by
Figure 11.7a for the supercritical case and
Figure 11.7b for the subcritical case.
In the neighborhood of a Hopf bifur-
cation (either sub- or supercritical), the
bifurcating limit cycle is well described by
a single sine function; away from the Hopf
bifurcation, however, its shape can change.
Furthermore, close to the bifurcation point,
the amplitude of the limit cycle grows pro-
portional to ‚àöùúá.

11.2 Equilibria
395
0
0
0
1
1
1
‚àí1
‚àí1
0.5
‚àí0.5
‚àí1
Œº
x
y
Figure 11.5
A one-parameter bifurcation
diagram of (11.23) showing the onset of
limit cycle oscillations as ùúáincreases. The
(supercritical) Hopf bifurcation occurs at
ùúá= x = y = 0 and the amplitude of the
resulting limit cycle is shown as a dashed
line.
The Hopf bifurcation theorem itself is
stated below.
Theorem 11.2 Consider
the
two-
dimensional dynamical system
Ãáx = f (x, ùúá),
x ‚àà‚Ñù2, ùúÇ‚àà‚Ñù,
(11.25)
where f is smooth and has an equilib-
rium x = 0 at ùúá= 0. Furthermore, let
ùúÜ1,2 = ùúé(ùúá) ¬± iùúî(ùúá) be the eigenvalues of
the Jacobian matrix such that ùúé= 0 when
ùúá= 0. If the genericity conditions (i) ùìÅ1 ‚â†0,
where ùìÅ1 is the Ô¨Årst Lyapunov coeÔ¨Écient [4],
and (ii) ùúéùúá(0) ‚â†0 are satisÔ¨Åed, then (11.25)
has the topological normal form z ‚àà‚ÑÇ
dz
dt = (ùúé+ iùúî)z + ùìÅ1|z|2z.
As
with
the
saddle-node
bifurcation,
higher-dimensional
systems
can
be
reduced to this two-dimensional form
using a center manifold reduction.
Take for example the van der Pol
equation
d2y
dt2 + y = (ùúá‚àíy2)dy
dt
(11.26)
where ùúáis the bifurcation parameter.
Equation (11.26) has an equilibrium y = 0,
which undergoes a Hopf bifurcation when
ùúá= 0 as seen by a pair of complex con-
jugate eigenvalues of the corresponding
Jacobian passing through the imaginary
axis. (Note that this is not the typical form
of the van der Pol equation ‚Äìif ùúámultiplies
the y2 term as well, a singular Hopf bifur-
cation occurs where the transition from
an inÔ¨Ånitesimal limit cycle to a large limit
cycle happens over an exponentially small
range of parameter values.) To determine
the behavior of the limit cycle near the
bifurcation point, perturbation methods
can be used.
Close to the bifurcation point, the limit
cycle will be small, so rescale y such that
ùúñx = y, which gives
d2x
dt2 + x = (ùúá‚àíùúñ2x2)dy
dt .
(11.27)
Then consider small perturbations to the
bifurcation parameter ùúáabout the bifur-
cation point. It turns out that these small
perturbations must be of order ùúñ2 for a
Hopf bifurcation (doing the following anal-
ysis with ùúáof O(ùúñ) will only Ô¨Ånd the equilib-
rium solution). Thus write ùúá= ùúñ2 ÃÉùúá; abusing
notation slightly, the tilde is immediately
dropped to give
d2x
dt2 + x = ùúñ2(ùúá‚àíx2)dx
dt .
(11.28)
As with most Hopf bifurcation prob-
lems,
(11.28)
is
directly
amenable
to
perturbation
methods
such
as
the
Poincar√©‚ÄìLindstedt method or the method
of multiple scales [8, 8]. For simplicity, the
Poincar√©‚ÄìLindstedt approach is shown
here. First, rescale time such that ùúè= ùúît
where ùúî= 1 + ùúî1ùúñ+ ùúî2ùúñ2 + O(ùúñ3); ùúî1 and

396
11 Dynamical Systems
ùúî2 are constants to be determined. Next,
assume a perturbation solution of the form
x = x0 + x1ùúñ+ x2ùúñ2 + O(ùúñ3).
Expanding
out (11.28) and collecting terms of the
same order in ùúñgives the equations
O(1) ‚à∂d2x0
dùúè2 + x0 = 0,
(11.29)
O(ùúñ) ‚à∂d2x1
dùúè2 + x1 = ‚àí2ùúî1
d2x0
dùúè2 ,
(11.30)
O(ùúñ2) ‚à∂d2x2
dùúè2 + x2 = ‚àí2ùúî1
d2x1
dùúè2
‚àí
(
ùúî2
1 + 2ùúî2
) d2x0
dùúè2
+
(
ùúá‚àíx2
0
) dx0
dùúè. (11.31)
With
the
initial
conditions
x0(0) = A
and x‚Ä≤
0(0) = 0 (because the system is
autonomous,
orbits
can
be
arbitrarily
shifted to meet these conditions), (11.29)
admits a solution x0 = A cos(t). The sec-
ular terms in (11.30) (the cos(ùúè) terms
that arise from x0 and lead to a resonance
eÔ¨Äect) can then be eliminated by setting
ùúî1A = 0; because A = 0 will produce a
trivial solution, ùúî1 = 0 is used instead.
Finally, the elimination of the secular
terms in (11.31) (both cos(ùúè) and sin(ùúè)
terms) gives two constraints
ùúáA ‚àí1
4A3 = 0
and
ùúî2A = 0.
(11.32)
Again, seeking a nontrivial solution for
A gives A = 2‚àöùúáand ùúî2 = 0. Thus, as
expected, the amplitude of the limit cycle
grows proportionally to ‚àöùúá. A comparison
of the perturbation solution and numerical
simulations give excellent agreement as
shown in Figure 11.6.
11.2.10
Pitchfork Bifurcation
The pitchfork bifurcation is a nongeneric
bifurcation (it is of codimension 3 [10]);
however, for systems with ‚Ñ§2 symmetry,
it becomes generic. Many physical systems
possess this symmetry, at least approxi-
mately, and so it is often of interest. A com-
mon example is the buckling of a simple
Euler strut.
The pitchfork bifurcation is also known
as a symmetry breaking bifurcation because
before the bifurcation the equilibrium solu-
tion is invariant under the action of the
symmetry group, whereas the bifurcating
solutions are not. A simple example is
dx
dt = ùúáx ‚àíùõºx3.
(11.33)
As
with
a
saddle-node
bifurcation,
a
pitchfork bifurcation is characterized by
an eigenvalue passing through zero. For
ùõº= +1 and ùúá< 0, this equation has a
0
0
0.5
Amplitude
1
1.5
0.2
0.4
Œº
Figure 11.6
The growth of a limit cycle
from the Hopf bifurcation of the van der
Pol equation (11.26) as ùúávaries; the solid
curve is the perturbation solution, whereas
the dots are from numerical simulations.

11.2 Equilibria
397
x
(a)
x
(b)
ùúá
ùúá
‚àí0.5
0.5
0.5
‚àí1
‚àí1
1
1
‚àí0.5
0
0
‚àí1
1
0
0
Figure 11.7
One-parameter
bifurcation diagrams for (11.33)
showing the existence of a
pitchfork bifurcation at ùúá= 0
and x = 0. Stable equilibria
are denoted by solid curves
and unstable equilibria are
denoted by dashed curves. (a)
The supercritical case (ùõº= 1)
and panel (b) the subcritical
case (ùõº= ‚àí1).
single (stable) equilibrium at x = 0 and it
undergoes a pitchfork bifurcation at ùúá= 0,
which results in three equilibria, two stable
(x = ¬±‚àöùúá) and one unstable (x = 0); this
is the supercritical case. Alternatively, for
ùõº= ‚àí1 and ùúá< 0 this equation has three
equilibria, two unstable (x = ¬±‚àö‚àíùúá) and
one stable (x = 0), and it undergoes a pitch-
fork bifurcation at ùúá= 0, which results in a
single unstable equilibrium at x = 0; this is
the subcritical case.
The
supercritical
case
is
shown
in
Figure 11.7a and the subcritical case in
Figure 11.7b.
The pitchfork bifurcation theorem is as
follows.
Theorem 11.3 Consider
the
one-
dimensional dynamical system
dx
dt = f (x, ùúá),
x ‚àà‚Ñù, ùúá‚àà‚Ñù,
(11.34)
where f
is smooth, has ‚Ñ§2 symmetry
and has an equilibrium x = 0 at ùúá= 0.
Furthermore, let ùúÜ= fx(0, 0) = 0. If the
genericity conditions (i) fxxx(0, 0) ‚â†0 and
(ii) fxùúá‚â†0 are satisÔ¨Åed, then (11.34) is
topologically equivalent near the origin to
the normal form
dy
dt = ùõºy ¬± y3,
(11.35)
where the sign of the y3 term determines the
criticality of the bifurcation.
Again, higher-dimensional systems can
be reduced to this one-dimensional form
using a center manifold reduction.
As many physical systems only pos-
sess the ‚Ñ§2 symmetry approximately, the
unfoldings of the pitchfork bifurcation
become important. Arbitrary asymmetric
perturbations to (11.33) give rise to
dx
dt = ùúáx ‚àíx3 + ùõº1 + ùõº2x2,
(11.36)
where ùõº1 and ùõº2 are unfolding parameters.
(In
general,
a
codimension-n
bifurca-
tion requires n parameters to unfold the
behavior.) The unfolding of the pitchfork
bifurcation reveals the bifurcation scenar-
ios that can occur when a system is close
to having a pitchfork bifurcation. These
scenarios are shown in Figure 11.8. For
small values of ùõº1 and ùõº2, the pitchfork
bifurcation typically unfolds into a single
saddle-node bifurcation and a discon-
nected branch of solutions as shown in
Figure 11.8a and d. However, when ùõº1
lies between 0 and ùõº3
2‚àï27, the pitchfork
bifurcation unfolds into three saddle-node
bifurcations
as
shown
in
Figure 11.8b
and c.
This type of unfolding procedure is
important for higher codimension bifurca-
tions because they often act as organizing
centers for the dynamics, that is, while
the higher codimension bifurcations may

398
11 Dynamical Systems
x
(a)
(b)
x
(c)
(d)
ùúá
ùúá
Figure 11.8
The four diÔ¨Äerent unfoldings
of the pitchfork bifurcation. In (a) and (d)
the pitchfork bifurcation unfolds into a sin-
gle saddle-node bifurcation. In contrast, in
(b) and (c) it unfolds into three saddle-node
bifurcations. The parameter ranges in (11.36)
corresponding to the diÔ¨Äerent unfoldings are
(a) ùõº1 > 0 and ùõº1 > ùõº3
2‚àï27, (b) ùõº1 > 0 and ùõº1 <
ùõº3
2‚àï27, (c) ùõº1 < 0 and ùõº1 > ùõº3
2‚àï27, (d) ùõº1 < 0
and ùõº1 < ùõº3
2‚àï27.
never be seen in the system of interest,
their proximity may be obvious.
11.2.11
Center Manifolds
Consider the two systems of diÔ¨Äerential
equations
dx
dt = xy,
dy
dt = ‚àíy ‚àíx2,
(11.37)
and
dx
dt = x2y ‚àíx5,
dy
dt = ‚àíy + x2.
(11.38)
Both of these systems have an equilibrium
at x = y = 0 and the corresponding Jaco-
bian matrices are identical:
J =
[0
0
0
‚àí1
]
.
(11.39)
As
J
has
an
eigenvalue
at
0,
the
Hartman‚ÄìGrobman theorem fails. Thus
the stability of the equilibria cannot be
deduced from the Jacobian. In this case,
the equilibrium of (11.37) is asymptotically
stable, whereas the equilibrium of (11.38)
is asymptotically unstable.
Center manifold theory is the means by
which the stability of (11.37) and (11.38)
can be determined.
Consider the system
dx
dt = Ax + f (x, y)
dy
dt = By + g(x, y),
(11.40)
where x ‚àà‚Ñùn, y ‚àà‚Ñùm and A and B are
matrices such that the eigenvalues of A have
zero real parts and the eigenvalues of B
have negative real parts.
To put a given system into the form
of (11.40) is simply a case of applying a
sequence of coordinate transformations
that Ô¨Årst shift the equilibrium of interest to
the origin and then second put the system
into Jordan normal form.

11.3 Limit Cycles
399
If f ‚â°g ‚â°0, then (11.40) contains two
trivial invariant manifolds: x = 0 is the
stable manifold of the equilibrium, and
y = 0 is the center manifold of the equi-
librium. Furthermore, the dynamics of
the system will collapse exponentially
quickly onto the center manifold and thus
the dynamics on the center manifold will
determine the stability of the equilibrium.
In general, with nonzero f and g, there
exists a center manifold of (11.40) that can
be written as y = h(x) where h(0) = 0 and
h‚Ä≤(0) = 0 [11]. While this does not permit
a direct solution for the dynamics on the
center manifold, it does permit a polyno-
mial approximation to be calculated to any
desired order.
Take (11.37) and consider the polynomial
expansion of h to Ô¨Åfth order, that is, y =
h(x) = ax2 + bx3 + cx4 + dx5 + O(x6). Sub-
stituting this into dy‚àïdt gives
dy
dt = h‚Ä≤(x)dx
dt = xh‚Ä≤(x)h(x),
= 2a2x4 + 5abx5 + O(x6).
(11.41)
Similarly, substituting the same expan-
sion into the right-hand side of the diÔ¨Äer-
ential equation governing y in (11.37) gives
dy
dt = ‚àíh(x) ‚àíx2,
= ‚àí(a + 1)x2 ‚àíbx3 ‚àícx4 ‚àídx5 + O(x6).
(11.42)
Equating the coeÔ¨Écients of both expres-
sions leads to a = ‚àí1, b = 0, c = ‚àí2, and
d = 0. Thus the center manifold approxi-
mation is y = h(x) = ‚àíx2 ‚àí2x4 + O(x6) and
the resulting dynamics on the center mani-
fold are
dx
dt = ‚àíx3 ‚àí2x5 + O(x7).
(11.43)
Hence the equilibrium at x = 0, y = 0 is
asymptotically stable.
A similar calculation can be performed
for (11.38) to show that the equilibrium is
asymptotically unstable.
11.3
Limit Cycles
11.3.1
DeÔ¨Ånition and Calculation
Once again consider the ODE
dx
dt = f (x, ùúá),
x ‚àà‚Ñùn, ùúá‚àà‚Ñù, (11.44)
where f ‚à∂‚Ñùn √ó ‚Ñù‚Üí‚Ñùn. Many of the con-
cepts developed for equilibria of (11.44)
apply directly to limit cycles of (11.44) as
well, although the underlying proofs of
the corresponding theorems may be quite
diÔ¨Äerent.
A limit cycle is an isolated closed orbit of
(11.44) that contains no rest points (points
where dx‚àïdt = 0). Any solution that passes
through a point on the limit cycle is a peri-
odic solution and has the property that
x(t + T) = x(t) for all t, where the period
is T.
As limit cycles are a function of time,
they are typically more diÔ¨Écult to calcu-
late than equilibria. However, in the case of
weak nonlinearities, perturbation methods
may give a reasonable approximation to a
limit cycle, for example the method of mul-
tiple scales is often used to good eÔ¨Äect. (In
particular, when analyzing the limit cycles
emerging from a Hopf bifurcation; see for
example, Section 11.2.9.)
11.3.1.1
Harmonic Balance Method
Another
analytical
approach
is
the
harmonic
balance
method,
whereby
a
harmonic solution to the equations of
motion is
postulated,
substituted into
the ODE of interest, and the resulting

400
11 Dynamical Systems
expression expanded in terms of sin(nùúît)
and cos(nùúît). The orthogonality of the
sin and cos terms over one period gives
a number of algebraic equations that can
then be solved. For example, consider the
periodically forced DuÔ¨Éng equation
y‚Ä≤‚Ä≤ + ùúáy‚Ä≤ + ùõºy + ùõΩy3 = Œì cos(ùúît).
(11.45)
Assume a solution in the form of a
truncated
Fourier
series
with
coef-
Ô¨Åcients
that
slowly
vary
with
time
y = p(t) + a(t) cos(ùúît) + b(t) sin(ùúît).
The
Ô¨Årst and second derivatives become
y‚Ä≤ = p‚Ä≤ + (a‚Ä≤ + bùúî) cos(ùúît)
+ (b‚Ä≤ ‚àíaùúî) sin(ùúît),
(11.46a)
y‚Ä≤‚Ä≤ = p‚Ä≤‚Ä≤ + (2b‚Ä≤ ‚àíaùúî) ùúîcos(ùúît)
+ (bùúî‚àí2a‚Ä≤) ùúîsin(ùúît),
(11.46b)
where it is presumed that p‚Ä≤‚Ä≤ = a‚Ä≤‚Ä≤ = b‚Ä≤‚Ä≤ =
0 owing to the slowly varying approxima-
tion. The expressions for y, y‚Ä≤, and y‚Ä≤‚Ä≤ are
then substituted into (11.45) and the con-
stant coeÔ¨Écients, along with the coeÔ¨É-
cients of cos(ùúît) and sin(ùúît), are balanced
on each side. Balancing the constant coeÔ¨É-
cients gives
ùúáp‚Ä≤ = ‚àíp
(
ùõΩp2 + ùõº+ 3
2ùõΩr2)
,
(11.47)
and balancing the coeÔ¨Écients of cos(ùúît)
and sin(ùúît) gives
( ùúá
2ùúî
‚àí2ùúî
ùúá
) (a‚Ä≤
b‚Ä≤
)
=
‚éõ
‚éú
‚éú‚éù
a
(
ùúî2 ‚àíùõº‚àí3ùõΩp2 ‚àí3
4ùõΩr2)
‚àíùúáùúîb + Œì
b
(
ùúî2 ‚àíùõº‚àí3ùõΩp2 ‚àí3
4ùõΩr2)
+ ùúáùúîa
‚éû
‚éü
‚éü‚é†
,
(11.48)
where
r2 = a2 + b2.
The
steady-state
response, ÃÉp, ÃÉr, is obtained from the equilib-
ria of (11.47) and (11.48), which requires
a‚Ä≤ = b‚Ä≤ = p‚Ä≤ = 0; squaring and summing
the result from (11.48) gives the frequency
response equation
[(
ùúî2 + ùõº‚àí3ùõΩÃÉp2 ‚àí3
4ùõΩÃÉr2)2
+ (ùúáùúî)2
]
ÃÉr2 =Œì2,
(11.49)
where ÃÉp may take on multiple values. More
speciÔ¨Åcally, after setting p‚Ä≤ = 0 in (11.47),
the steady-state solution ÃÉp reveals that both
ÃÉp = 0 and ÃÉp2 = ‚àíùõº‚àïùõΩ‚àí3‚àï2ÃÉr2 are equilib-
ria. When the DuÔ¨Éng equation is bistable,
where ùõº= ‚àí1 and ùõΩ= 1, the latter solution
is interesting because it restricts the values
of ÃÉr that provide a physical solution for ÃÉp to
ÃÉr2 ‚â§2‚àï3.
The harmonic balance method is par-
ticularly useful in the case of the bistable
DuÔ¨Éng equation because this is a situa-
tion in which perturbation methods are
not applicable owing to the strength of the
nonlinearity. However, higher-order har-
monic approximations often result in alge-
braic equations that can only be solved
numerically and so the harmonic balance
method is often limited to the Ô¨Årst-order
equations.
In general, when dealing with strong
nonlinearities or limit cycles that are far
from harmonic, numerical methods are
the way forward. There are many diÔ¨Äer-
ent methods for calculating limit cycles
numerically; two common methods dis-
cussed here are numerical shooting and
collocation. Both methods treat the prob-
lem of calculating a limit cycle as a periodic
boundary value problem (BVP).
11.3.1.2
Numerical Shooting
Numerical shooting is a straightforward
method for solving BVPs where a numer-
ical integrator is available to calculate
solutions of (11.44) for given initial con-
ditions. For a limit cycle, the BVP of

11.3 Limit Cycles
401
interest is
dx
dt = f (x, ùúá),
with
x(0) ‚àíx(T) = 0,
(11.50)
where the period of the limit cycle is T.
However, (11.50) is not directly amenable to
numerical methods. To calculate a particu-
lar limit cycle, time in (11.50) must Ô¨Årst be
rescaled so that the period T appears as an
explicit parameter to be determined. This
results in
dx
dt = Tf (x, ùúá),
with
x(0) ‚àíx(1) = 0.
(11.51)
Furthermore, owing to the autonomous
nature of (11.51), there exists a one-
parameter family of solutions that are
invariant under time (phase) shifts. As
such a phase condition is needed to restrict
the problem to Ô¨Ånding an isolated limit
cycle and so ensure the correct behavior of
the numerical method.
There are a wide range of possible phase
conditions. For simple implementations, it
may be desirable to use known proper-
ties of the system of interest. For example,
if all limit cycles of interest pass through
the point x = C, then the phase condition
x(0) = C may be used. A more robust, but
harder to implement, condition is x(0) =
x(1‚àï2) which holds for all limit cycles; how-
ever, this condition is still not completely
robust and may become singular.
The most common and robust phase con-
dition is the integral condition
‚à´
1
0
dÃÇx
dt ‚ãÖ[x(t) ‚àíÃÇx(t)]dt = 0,
(11.52)
which minimizes the Óà∏2 distance between
the limit cycle and a reference limit cycle ÃÇx
(typically taken as the last successfully com-
puted limit cycle). The condition (11.52) is
implemented in many bifurcation analysis
packages.
The Ô¨Ånal nonlinear BVP to be solved is
the system formed by (11.51) and (11.52)
where the time series of x is calculated by
numerical integration from the initial con-
dition x(0). The BVP is then solved using
a nonlinear root-Ô¨Ånding method such as
a Newton iteration starting with an initial
guess {x(0)(0), T0}:
[x(i+1)(0)
T(i+1)
]
=
[x(i)(0)
T(i)
]
‚àíJ‚àí1
[
x(i)(0) ‚àíx(i)(1)
‚à´
1
0
dÃÇx
dt ‚ãÖ[x(i)(t) ‚àíÃÇx(t)]dt
]
,
(11.53)
where J is the corresponding Jacobian
matrix, which can be approximated by
a Ô¨Ånite-diÔ¨Äerence scheme or calculated
directly from the variational equations
(see Section 11.3.2). The integral phase
condition can be discretized using a simple
scheme such as the trapezoid method
without a loss of accuracy.
Numerical shooting is suitable for both
stable and unstable limit cycles, though
increasingly
better
initial
guesses
are
required as the limit cycle becomes more
unstable. To overcome stability problems,
multiple shooting is often used.
11.3.1.3
Collocation
Collocation is another method for solving
BVPs of the form (11.51). Similar to numer-
ical shooting, a phase condition (similar
to (11.52)) is required. Collocation diÔ¨Äers
from numerical shooting in how the time
series is computed/represented.
In collocation, the time series is approx-
imated as a series expansion in terms
of
certain
basis
functions
that
can
be
diÔ¨Äerentiated
exactly,
for
example,
x(t) = ‚àën
i=0 xiPi(t) in the case of Lagrange

402
11 Dynamical Systems
polynomials. Common examples include
harmonic
functions
(Fourier
series),
piecewise-deÔ¨Åned Lagrange polynomials,
and
globally
deÔ¨Åned
Chebyshev
poly-
nomials. This series expansion is then
substituted into the system of interest and
evaluated at discrete-time points (colloca-
tion points). The idea is to determine the
coeÔ¨Écients of the basis functions (using a
nonlinear root Ô¨Ånder) such that, at these
collocation points, the series expansion
of the limit cycle exactly satisÔ¨Åes (11.51)
and (11.52); in general, these are the only
points where they are satisÔ¨Åed exactly
and so globally the series expansion is an
approximate solution.
(In contrast, a Galerkin-based method
would use a similar series expansion of
the solution but, instead of pointwise eval-
uations of (11.51) and (11.52), integrals
over the whole period would be calculated.
When the integrals can be calculated ana-
lytically, this has beneÔ¨Åts over collocation.
However, when the integrals must be calcu-
lated numerically using quadrature, these
beneÔ¨Åts disappear.)
For more details see [12] or [13].
11.3.2
Linearization
Following
the
treatment
of
equilibria
in Section 11.2.3, the dynamical system
deÔ¨Åned by (11.44) can similarly be lin-
earized in the vicinity of a limit cycle x‚àó(t).
The linearization is inherently time varying
and is given by
dÃÉx
dt = J(t)ÃÉx(t),
(11.54)
where J is the (time-varying) Jacobian given
by
J(t) = [Ji,j(t)]|||x(t)=x‚àó(t) =
[ ùúïfi
ùúïxj
]|||||x(t)=x‚àó(t)
.
(11.55)
Equation (11.54) is also known as the Ô¨Årst
variational equation and can be derived
formally by considering the growth/decay
of small perturbations to the limit cycle.
To determine the stability of a limit cycle
with period T, a suitable discrete time
map is constructed, the Ô¨Åxed points of
which correspond to the limit cycles of the
original system. There are two methods
for constructing the discrete time map
(both equivalent); these are illustrated in
Figure 11.9. Either consider the time-T
map that evolves the point x(t) to x(t + T),
or alternatively consider the dynamics
of a map from an arbitrary section that
intersects with the limit cycle, back to itself
(a so-called Poincar√© map and Poincar√©
section). When either of the maps are
linearized, the end result is a map of the
form ÃÉx(i+1) = MÃÉx(i) where M is the Jacobian
matrix of the map; M is also known as the
monodromy matrix. The stability of the
Ô¨Åxed point (and correspondingly the limit
cycle) is determined by the eigenvalues of
M, which are known as Floquet multipliers.
(a)
(c)
(b)
Œ£
Figure 11.9
A schematic of the discrete-time maps associated
with the linearization of a limit cycle. Point (a) is a starting point
near a given limit cycle (marked by the dashed closed curve). It is
then either evolved forward in time by T time units to point (b) as
with the time-T map, or it is evolved forward in time until it hits
the Poincar√© section Œ£ once more at point (c) as with the Poincar√©
map.

11.3 Limit Cycles
403
A limit cycle is linearly stable if all the
Floquet multipliers lie inside the unit circle
in the complex plane. Furthermore, a limit
cycle is hyperbolic if no Floquet multipliers
lie on the unit circle.
(Note that the time-T map is a mapping
from ‚Ñùn to ‚Ñùn, whereas the Poincar√© map is
a mapping from ‚Ñùn‚àí1 to ‚Ñùn‚àí1; the extra Flo-
quet multiplier associated with the time-T
map is a so-called trivial multiplier and will
always be +1 because the limit cycle can be
phase-shifted arbitrarily.)
In the case of the time-T map, the mon-
odromy matrix M can be calculated by inte-
grating the Ô¨Årst variational equation (11.54)
with the initial condition ÃÉx(0) = I where I is
the n √ó n identity matrix. The matrix M is
then the matrix ÃÉx(T).
11.3.3
Topological Equivalence
In the vicinity of a hyperbolic equilibrium,
the
notion
of
topological
equivalence
combined with the Hartman‚ÄìGrobman
theorem is a powerful one. It states that
the dynamics near an equilibrium are
equivalent to those of the corresponding
linearization. The straightforward calcula-
tion of stability is one of the possibilities
that emerges from this result.
For limit cycles, a similar proposition
holds; the dynamics in the neighborhood
of a hyperbolic limit cycle (none of its Flo-
quet multipliers lie on the unit circle) are
topologically equivalent to the linearized
dynamics given by the Ô¨Årst variational
equation. This follows from the discrete-
time version of the Hartman‚ÄìGrobman
theorem for the Ô¨Åxed points of a map,
because for any limit cycle, a time-T
map or a Poincar√© map can be deÔ¨Åned as
described in Section 11.3.2.
11.3.4
Manifolds
By analogy with equilibria (see (11.15) and
(11.16)), stable and unstable manifolds for
a limit cycle can be deÔ¨Åned as the set of
points that approach the limit cycle in for-
ward time and backward time, respectively.
An important diÔ¨Äerence between the
manifolds
of
an
equilibrium
and
the
manifolds of a limit cycle is their respec-
tive dimensions. For an equilibrium, the
dimension of the stable manifold (unstable
manifold) is given by the number of eigen-
values with negative (positive) real parts.
For a limit cycle in an autonomous system,
the dimensions of the stable and unstable
manifolds are increased by one due to the
presence of the trivial multiplier at +1.
Figure 11.10 shows the two possible
conÔ¨Ågurations for the stable and unstable
manifolds of a saddle-type limit cycle in
three-dimensional space. The Ô¨Årst case
(a)
(b)
Figure 11.10
Two diÔ¨Äerent conÔ¨Ågurations of
two-dimensional manifolds in three-dimensional
space of a saddle-type limit cycle (marked by a
thick black curve). (a) The case where the Flo-
quet multipliers corresponding to the manifolds
are positive. (b) The case where the Floquet
multipliers are negative and the resulting man-
ifold is twisted, having the form of a M√∂bius
strip. (Only one manifold is shown in (b) for
clarity.) A representative orbit is marked in (b)
by a dashed curve.

404
11 Dynamical Systems
is when the Floquet multipliers of both
manifolds are positive; the manifolds form
simple bands around the limit cycle. The
second case is when the Floquet multipliers
of both manifolds are negative; in this case,
both the manifolds form twisted bands
around the limit cycle.
It should be noted that it is not possible
(in three dimensions) to have a twisted sta-
ble (unstable) manifold and an untwisted
unstable (stable) manifold. This can proven
directly from Liouville‚Äôs formula, which
ensures that the product of all the Floquet
multipliers is positive.
A local (linear) approximations to the sta-
ble and unstable manifolds at a single point
on the limit cycle are given by the eigenvec-
tors of the monodromy matrix. These local
approximations can then be extended to
the complete limit cycle by integrating for-
ward the Ô¨Årst variational equation (11.54)
for time T, using the (normalized) eigen-
vector as the initial condition. Globaliza-
tion of the manifolds can then be achieved
using the same numerical methods as men-
tioned in Section 11.2.6.
11.3.5
Local Bifurcations
It follows from the discrete-time Hart-
man‚ÄìGrobman theorem that the dynamics
in the neighborhood of a limit cycle can
only
change
qualitatively
(i.e.,
topo-
logically)
if
the
limit
cycle
becomes
nonhyperbolic; that is, a Floquet mul-
tiplier lies on the unit circle. Thus, as with
equilibria, a nonhyperbolic limit cycle
signiÔ¨Åes a local bifurcation.
Limit cycles can undergo the same basic
bifurcations as equilibria can. In particular,
the saddle-node and pitchfork bifurcations
are essentially the same with limit cycles
taking the place of the equilibria. These
occur when a Floquet multiplier passes
through the unit circle in the complex
plane at +1. (Again, the pitchfork bifurca-
tion requires ‚Ñ§2 symmetry for it to be a
generic bifurcation.)
Hopf bifurcations (called secondary Hopf
bifurcations or Neimark-Sacker bifurca-
tions) of limit cycles also occur. They are
associated with the onset of quasi-periodic
motion (the existence of an invariant torus)
and they occur when a complex conjugate
pair of eigenvalues passes through the unit
circle. The details of the bifurcation are
signiÔ¨Åcantly complicated if strong reso-
nances occur between the frequency of the
underlying limit cycle and the frequency of
the bifurcation. The details of this bifurca-
tion are beyond the scope of this chapter
and
the
interested
reader
is
referred
to [4].
Finally, limit cycles can undergo one
Ô¨Ånal (generic) local bifurcation known
as a period-doubling bifurcation, which
occurs when a single Floquet multiplier
passes through the unit circle at ‚àí1.
Period-doubling
bifurcations
are
often
associated with the onset of chaos. In
particular, a period-doubling cascade (a
sequence of period-doubling bifurcations
one after the other) is a well known route to
chaos.
11.3.6
Period-Doubling Bifurcation
At a period-doubling bifurcation, a limit
cycle becomes nonhyperbolic as a Floquet
multiplier passes through the unit circle in
the complex plane at ‚àí1. At the bifurcation
point, a new limit cycle is created that has
a period of twice that of the original limit
cycle and, furthermore, the original limit
cycle changes stability.
As with Hopf and pitchfork bifurcations,
a period-doubling bifurcation can be sub-
or supercritical. In the subcritical case,

11.4 Numerical Continuation
405
an unstable limit cycle becomes stable at
the bifurcation point and coexists with
an unstable period-doubled limit cycle. In
the supercritical case, a stable limit cycle
becomes unstable at the bifurcation point
and coexists with a stable period-doubled
limit cycle.
Period-doubling bifurcations are often
a precursor to chaos. For example, the
R√∂ssler system,
dx
dt = ‚àíy ‚àíz,
dy
dt = x + ay,
dz
dt = b + z(x ‚àíc),
(11.56)
becomes chaotic through a sequence of
supercritical period-doubling bifurcations
as shown in Figure 11.11. It should be noted
that an inÔ¨Ånite number of period-doubling
bifurcations exist in a Ô¨Ånite parameter
interval. The parameter values at which
the period-doubling bifurcations occur
tend toward a geometric progression as
the period increases; the ratio of the dis-
tances between successive period-doubling
bifurcations is known as Feigenbaum‚Äôs
constant.
In the midst of the chaotic attractor
shown in Figure 11.11, a period-3 orbit
becomes stable. It then also undergoes a
period-doubling sequence before the sys-
tem becomes fully chaotic again. Windows
of periodic behavior like this are typical
in chaotic attractors of smooth dynamical
systems.
11.4
Numerical Continuation
11.4.1
Natural Parameter Continuation
The preceding sections have expounded the
diÔ¨Äerent dynamical eÔ¨Äects that can occur
as parameter values are changed in a sys-
tem. If the system of interest is suÔ¨Éciently
simple, it is enough to Ô¨Ånd the equilibria
analytically and compute quantities such as
the Jacobian to determine when bifurca-
tions occur.
Many
nontrivial
systems
are
not
amenable to such analysis and require the
application of numerical methods from the
start. In such cases, numerical simulation
is often the Ô¨Årst recourse, swiftly followed
by brute-force bifurcation diagrams such
as Figure 11.11 in Section 11.3.6. A great
deal of information can be gathered using
these simple techniques.
However, such use of numerical sim-
ulation does not reveal much about the
nature of the bifurcations and transitions
that occur as the parameters are varied
because unstable invariant sets are missed.
It is branches of these unstable invariant
0
10
20
30
5
15
20
x
c
10
Figure 11.11
A one-parameter brute-
force bifurcation diagram of the R√∂ssler sys-
tem (11.56) for a = b = 0.1 showing a period-
doubling route to chaos. At each parameter
value, the system is simulated until it reaches
steady state and then the values of x at which
the orbit intersects the Poincar√© map deÔ¨Åned
by dx‚àïdt = 0 are recorded.

406
11 Dynamical Systems
sets that connect the diÔ¨Äerent branches of
stable invariant sets and so reveal what
bifurcations are occurring.
Numerical continuation is a means to
overcome these diÔ¨Éculties. At heart, it is a
path-following method that tracks the solu-
tions of
f (x, ùúá) = 0,
x ‚àà‚Ñùn, ùúá‚àà‚Ñùp,
(11.57)
as the system parameters ùúávary. This
type of problem arises naturally from
equilibrium problems but many other
types of problem can also be put into
this form, for example, the calculation of
limit cycles using numerical shooting or
collocation.
Numerical continuation relies on the
implicit function theorem, which states
that the solutions of (11.57) can be written
explicitly as x = g(ùúá) provided the matrix
of partial derivatives of f with respect to x
is not singular (i.e., the system is not at a
bifurcation point). Thus, when the implicit
function theorem holds, a continuous
branch of solutions can be tracked with
respect to ùúá.
A simple version of numerical contin-
uation is an iterative, predictor-corrector
algorithm called natural parameter con-
tinuation. The algorithm is as follows. To
start the algorithm an initial point {x0, ùúá0}
is required, which can be generated using
numerical
simulation
(or
some
other
method).
1. Take a step in the system parameter of
interest to get ùúái+1 = ùúái + Œî.
2. Generate a predicted solution ÃÉxi+1 of
(11.57) for ùúá= ùúái+1 from the previous
solution xi.
3. Generate a corrected solution xi+1 by
using a nonlinear root Ô¨Ånder (e.g., a
Newton iteration) on f (x, ùúái+1) = 0
using ÃÉxi+1 as an initial guess for the root
Ô¨Ånder.
Repeat this process until the desired
parameter range is covered; Œî should be
suÔ¨Éciently small to ensure convergence
of the nonlinear root Ô¨Ånder and to reveal
the desired level of detail in the solution
branch.
As the calculation of solutions uses a
nonlinear root Ô¨Ånder rather than numeri-
cal simulation, it does not matter whether
the solutions are stable or unstable. In fact,
to determine the stability of equilibria, the
Jacobian matrix must be computed sepa-
rately. Similarly, for limit cycles the calcu-
lation of the monodromy matrix is also a
separate step. (In practice, it is often possi-
ble to use the Jacobian previously calculated
by the Newton iteration for stability infor-
mation.)
There are a number of diÔ¨Äerent ways to
generate predicted solutions. Two common
ways are tangent prediction and secant pre-
diction. Tangent prediction calculates the
local derivative v = fùúá(xi, ùúái) and applies the
prediction ÃÉxi+1 = xi + Œîv. Secant predic-
tion, on the other hand, estimates the local
derivative using a secant estimation, that is,
ÃÉv = (xi ‚àíxi‚àí1)‚àï(ùúái ‚àíùúái‚àí1), and then calcu-
lates the predicted solution as ÃÉxi+1 = xi +
ŒîÃÉv. Secant prediction is simpler in that it
does not require any derivative informa-
tion; however, it requires two initial points
to start the algorithm.
Figure 11.12 shows an example of natural
parameter continuation using the R√∂ssler
system
(compare
with
Figure 11.11).
The
limit
cycle
is
discretized
using
collocation.
Natural
parameter
continuation
is
inherently Ô¨Çawed. As it relies directly on
the implicit function theorem, it fails at
saddle-node
bifurcations
because
past

11.4 Numerical Continuation
407
0
5
5
10
10
15
15
20
x
(a)
(b) (c)
c
Figure 11.12
Numerical
continuation of limit cycles in
the R√∂ssler system. A series of
period-doubling bifurcations
occur and the bifurcating
branches are continued up to
the period 8 limit cycle. The
Ô¨Årst three period-doubling
bifurcations are marked by
(a), (b) and (c). (Compare with
Figure 11.11.)
the bifurcation point, there is no solu-
tion for x. Instead, a diÔ¨Äerent approach is
required.
11.4.2
Pseudo-Arc-Length Continuation
Pseudo-arc-length continuation overcomes
the diÔ¨Éculties posed by saddle-node bifur-
cations by reparameterizing the zero prob-
lem (11.57) in terms of the arc length along
the solution curve. By doing so, the implicit
function theorem no longer fails at saddle-
node bifurcations. Thus the zero problem
becomes
f (x(s), ùúá(s)) = 0,
x ‚àà‚Ñùn, ùúá‚àà‚Ñùp, (11.58)
and both x and ùúáare solved for simultane-
ously in terms of the arc length. To do this,
an additional equation is required (because
ùúáis now an additional unknown). A true
arc-length parameterization would require
the addition of the nonlinear algebraic
equation (ds)2 = (dx)2 + (dùúá)2; in practice
it is better to use a linear approximation
to this nonlinear equation, namely, the
pseudo-arc-length equation
(ÃÇx‚Ä≤)T(x ‚àíÃÇx) + ( ÃÇùúá‚Ä≤)T(ùúá‚àíÃÇùúá) = Œî,
(11.59)
where ÃÇx and ÃÇùúáare previously computed
solutions and the primes ()‚Ä≤ denote dif-
ferentiation with respect to arc length.
(Secant approximations to the derivatives
with respect to arc-length work as well as
the actual derivatives.)
The
pseudo-arc-length
continuation
algorithm is thus as follows. (Use ÃÇx = xi
and ÃÇùúá= ùúái in (11.59).)
1. Generate a predicted solution
{ÃÉxi+1, ÃÉùúái+1} of (11.58) and (11.59) from
the previous solution {xi, ùúái}.
2. Generate a corrected solution
{xi+1, ùúái+1} by using a nonlinear root
Ô¨Ånder (e.g., a Newton iteration) on the
combined system of (11.58) and (11.59)
using {ÃÉxi+1, ÃÉùúái+1} as an initial guess for
the root Ô¨Ånder.
This is illustrated in contrast to natu-
ral parameter continuation in Figure 11.13.
The prediction step is the same as before
except that derivatives are now with respect
to the arc length and predictions are needed
for ùúái+1 as well as xi+1.
11.4.3
Continuation of Bifurcations
As well as tracking invariant sets of a
dynamical system in general, it is also
possible to track bifurcations in terms of

408
11 Dynamical Systems
(a)
(b)
x
xi
xi
xi + 1
xi ‚àí 1
xi ‚àí 1
x
Figure 11.13
(a), (b) Natural parameter con-
tinuation and pseudo-arc-length continuation,
respectively, close to a saddle-node bifurca-
tion. Predicted solutions are denoted by open
circles and corrected solutions are denoted
by dots. The dashed lines denote the tangent
line x‚Ä≤
i (using a secant approximation) and the
dotted lines indicate the search direction of
the nonlinear root Ô¨Ånder. The addition of the
pseudo-arc-length equation (11.59) in (b) skews
the search direction in order to pass around
the saddle-node bifurcation.
the system parameters. For example, it is
possible to track a saddle-node bifurca-
tion as two parameters vary to trace out
bifurcation diagrams as in Figure 11.4. All
that is required is to augment the zero
problem of (11.58) and (11.59) by a third
equation that encompasses the bifurcation
condition. This third equation is known as
a bifurcation test function.
A saddle-node bifurcation of equilibria
occurs when an eigenvalue of the Jacobian
matrix passes through zero. As such, a suit-
able test function is
det(J) = 0,
(11.60)
where J is the Jacobian matrix associ-
ated with the equilibrium. While this
is conceptually a simple test function,
for numerical purposes, it is less useful
because derivatives of this function are
not straightforward to calculate. (A Ô¨Ånite-
diÔ¨Äerence approximation can be used but
this can be computationally expensive
to calculate for large systems.) Instead, a
better approach is to track the eigenvector
corresponding to the zero eigenvalue. As
eigenvectors can be scaled arbitrarily, a
further condition is required to normal-
ize the eigenvector. Thus the system of
equations needed to continue saddle-node
bifurcations of equilibria is formed from
(11.58), (11.59) and
Jv = 0,
‚Äñv‚Äñ = 1,
(11.61)
where v is the eigenvector corresponding to
the zero eigenvalue. Thus the variables to
solve for are x, ùúá1, ùúá2, and v. (Two param-
eters must be allowed to vary to continue a
curve of saddle-node bifurcations.)
For test functions of other bifurca-
tions, and the bifurcations of limit cycles,
see [3, 4].
References
1. Arnold, L. (1998) Random Dynamical
Systems, Springer-Verlag, Berlin.
2. Strogatz, S.H. (1994) Nonlinear Dynamics
and Chaos, Perseus, New York.
3. Seydel, R. (2010) Practical Bifurcation and
Stability Analysis, Interdisciplinary Applied
Mathematics, vol. 5, 3rd edn, Springer-
Verlag, New York.
4. Kuznetsov, Y.A. (1998) Elements of Applied
Bifurcation Theory, Applied Mathematical

References
409
Sciences, 2nd edn, Springer-Verlag, New
York.
5. Hsu, C.S. (1987) Cell-to-Cell Mapping: A
Method of Global Analysis for Nonlinear
Systems, Springer-Verlag.
6. Dellnitz, M. and Hohmann, A. (1997) A
subdivision algorithm for the computation of
unstable manifolds and global attractors.
Numer. Math., 75 (3), 293‚Äì317.
7. Krauskopf, B., Osinga, H.M., Doedel, E.J.,
Henderson, M.E., Guckenheimer, J.,
Vladimirsky, A., Dellnitz, M., and Junge, O.
(2005) A survey of methods for computing
(un)stable manifolds of vector Ô¨Åelds. Int. J.
Bifurcation Chaos, 15 (3), 763‚Äì791.
8. Bender, C.M. and Orszag, S.A. (1978)
Advanced Mathematical Methods
for Scientists and Engineers,
Springer-Verlag.
9. Murdock, J. (2013) Perturbation Methods,
John Wiley & Sons, Ltd.
10. Golubitsky, M. and SchaeÔ¨Äer, D.G. (1985)
Singularities and Groups in Bifurcation
Theory, Applied Mathematical Sciences (eds
F. John, J.E. Marsden, and L. Sirovich),
Springer-Verlag, Berlin.
11. Carr, J. (1981) Applications of Centre
Manifold Theory, Springer-Verlag.
12. Trefethen, L.N. (2000) Spectral Methods in
Matlab, SIAM.
13. Boyd, J.P. (2001) Chebyshev and Fourier
Spectral Methods, 2nd edn, Dover, New York.


411
12
Perturbation Methods
James Murdock
12.1
Introduction
Perturbation theory arises when a situation
is given that admits of a mathematical
description, and one asks how this descrip-
tion changes when the situation is varied
slightly or ‚Äúperturbed.‚Äù This could result
either in a continuation of the original situ-
ation with only small quantitative changes
or in an abrupt qualitative change in the
nature of the situation. Among the possible
‚Äúabrupt‚Äù changes are the formation of a
transition layer and the creation of various
types of bifurcations; although bifurcation
theory is usually treated as a separate
subject
from
perturbation
theory,
the
two areas are closely related. The speciÔ¨Åc
subject matter of this chapter will be the
following two topics.
1. A system of ordinary or partial
diÔ¨Äerential equations is given, together
with initial or boundary conditions. The
system contains a small parameter, and
is explicitly solvable when the
parameter is zero. One asks how to
construct approximate solutions (in
explicit analytic form) when the
parameter is small but nonzero; one
asks for error estimates for these
approximate solutions, and whether the
approximate solutions exhibit the same
qualitative behavior as the unknown
exact solutions.
2. A matrix or linear transformation is
given, depending on a small parameter.
The eigenvalues and eigenvectors (or
the Jordan normal form) are known
when the parameter is zero, and one
asks for approximate calculations of the
eigenvalues or normal form when the
parameter is small but nonzero.
The origins of perturbation theory lie in
three classical problems, planetary motion,
viscous Ô¨Çuid Ô¨Çow past a wall, and changes
in the spectrum as a matrix or linear
operator is varied. The present chapter is
structured in the same threefold way: after
an initial section presenting basic concepts
common to the three areas, we take up
in turn dynamical systems (Section 12.3),
transition layer problems (Section 12.4),
and spectra (Section 12.1.5); the inter-
vening Section 12.5 deals with a recent
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

412
12 Perturbation Methods
method applicable to the problems of both
Section 12.3 and 12.4. To conclude this
introduction, we brieÔ¨Çy describe the three
classical problems.
Isaac Newton showed that the inverse
square law of gravitational force implies
that a single planet will move around the
sun in an ellipse, satisfying Kepler‚Äôs laws of
planetary motion. The same law of grav-
ity, however, implies that the several plan-
ets will exert attractive forces on each other,
which will ‚Äúperturb‚Äù their orbits. Laplace
computed the perturbations, to a certain
degree of approximation, and found that
they were periodic, so that the solar system
was ‚Äústable‚Äù (in Laplace‚Äôs sense) and would
not destroy itself. His techniques were labo-
rious, and can be much simpliÔ¨Åed today by
Hamiltonian mechanics; furthermore, he
did not entirely prove that the solar sys-
tem is stable, since his method, if carried
to higher orders, does not always converge.
However, a great many of the ideas of mod-
ern perturbation theory originated here:
variation of parameters, averaging, multi-
ple scales, and the problems that in the
twentieth century led to the Kolmogorov‚Äì
Arnol‚Äôd‚ÄìMoser theorem, the Nekhoroshev
theorem, and other topics in dynamical sys-
tems theory.
In theory, a Ô¨Çuid that is viscous (even
to the smallest degree) will adhere to the
walls of any container (such as a pipe) in
which it is Ô¨Çowing. However, at any rea-
sonable distance from the walls, the Ô¨Çuid
Ô¨Çows almost as if the wall were not there.
In order to resolve this apparent paradox, L.
Prandtl introduced the idea of a ‚Äúboundary
layer,‚Äù a thin layer of Ô¨Çuid against the wall
in which the Ô¨Çuid passes from rest to rapid
motion. Here the ‚Äúunperturbed‚Äù problem is
the inviscid Ô¨Çow (which does not adhere to
the wall), the ‚Äúperturbation‚Äù is the small vis-
cosity, and the eÔ¨Äect of the perturbation is
not a small correction of the motion but a
quite drastic correction conÔ¨Åned to a small
region (the boundary layer). This example
leads to the ideas of stretched or scaled
coordinates, inner and outer solutions, and
matching.
According to quantum mechanics, all
observable quantities are the eigenvalues
of operators on a Hilbert space. In simple
problems, the Hilbert space will be Ô¨Ånite
dimensional and the operators are repre-
sentable as matrices; in other cases, they
are partial diÔ¨Äerential operators. In model-
ing an atom, for instance, the eigenvalues
will be related to the frequencies of light
(the spectrum) emitted by the atom when
electrons jump from one shell to another.
These frequencies can be perturbed, for
instance, if the atom is placed in a weak
magnetic Ô¨Åeld. Mathematically, the result-
ing problem is to determine the changes
in the ‚Äúspectrum‚Äù (the set of eigenvalues)
of a matrix or other operator when the
operator is slightly perturbed. One striking
diÔ¨Äerence between this problem and the
Ô¨Årst two is that quantum mechanics is
entirely a linear theory, whereas in both
the dynamical systems problems and the
boundary layer problems nonlinearities
can play a crucial role.
12.2
Basic Concepts
12.2.1
Perturbation Methods versus Numerical
Methods
A system of diÔ¨Äerential equations that is not
explicitly solvable calls for an approximate
solution method of some type. The most
useful approximate methods are numer-
ical methods (implemented on a digital
computer) and perturbation methods. Per-
turbation methods are usable only if the
system is ‚Äúclose‚Äù to an explicitly solvable

12.2 Basic Concepts
413
system, in the sense that the system would
become solvable if certain small changes
were made, such as deleting small terms or
averaging some term over a rapidly rotating
angle. In such a case, perturbation theory
takes the solution of the simpliÔ¨Åed problem
as a ‚Äúzeroth-order approximation‚Äù that can
be successively improved, giving higher-
order approximations having explicit for-
mulas. Numerical methods operate with-
out the restriction that the problem be
nearly solvable, but give a solution in the
form of numerical tables or graphs. The
advantage of a formula is that one can see
by inspection the manner in which each
variable and parameter aÔ¨Äects the solution.
As both numerical and perturbation solu-
tion are approximate, it is often helpful
to verify a perturbation solution by com-
paring it with numerical ones (or directly
with experimental data), especially when a
mathematically valid error estimate for the
perturbation solution is missing.
12.2.2
Perturbation Parameters
Mathematical models of physical phenom-
ena typically contain several variables,
which
are
divided
into
‚Äúcoordinates‚Äù
(of space or time) and ‚Äúparameters.‚Äù A
spring/mass system with a cubic nonlin-
earity, for instance, will contain position
and time coordinates as well as parameters
for the mass and for the coeÔ¨Écients of
the linear and cubic terms in the restoring
force. The Ô¨Årst step in preparing such
a system for perturbation analysis is to
nondimensionalize these variables. The
second step is to look for solvable special
cases that can serve as the starting point
for approximating the solution of nearby
cases. Most often, these solvable cases
will be obtained by setting some of the
parameters equal to zero. For instance, the
forced and damped nonlinear oscillator
given by
Ãày + C Ãáy + k2y + Ay3 = B cos ùúît
(12.1)
becomes easily solvable if A = B = C = 0; it
is still solvable if only A = 0, but not quite as
simply, and the case C = B = 0 is solvable
with elliptic functions. It is therefore plau-
sible to look for approximate solutions by
perturbation theory if A, B, and C are small,
and it may also be possible if only A is small,
or if only B and C are small.
Suppose that we choose to investigate the
case when A, B, and C are small. Ideally,
we could treat these as three small indepen-
dent parameters, and considerable work is
now being devoted to the investigation of
such ‚Äúmultiparameter‚Äù perturbation meth-
ods, especially in the context of bifurcation
theory (see [1]). However, most classical
perturbation methods are developed only
for single-parameter problems. Therefore,
it is necessary to make a choice as to how
to reduce (12.1) to a single-parameter prob-
lem. The simplest way is to write A = aùúÄ,
B = bùúÄ, and C = cùúÄ, obtaining
Ãày + ùúÄc Ãáy + k2y + ùúÄay3 = ùúÄb cos ùúît.
(12.2)
We appear to have added a parameter
instead of reducing the number, but now a,
b, and c are regarded as constants, whereas
ùúÄ, the perturbation parameter, is taken as a
small, but variable, quantity; typically, we
expect the perturbation solution to have
the form of a power series in ùúÄ. We have,
in eÔ¨Äect, chosen to investigate a particular
path leading from the origin in the space
of variables A, B, C. It is at once clear that
other paths are possible, for instance
Ãày + ùúÄ2c Ãáy + k2y + ùúÄay3 = ùúÄb cos ùúît.
(12.3)
One might choose (12.3) over (12.2) if the
goal is to investigate systems in which the

414
12 Perturbation Methods
damping is extremely small, small even
compared to the cubic term and the forc-
ing. But it is not clear, without experience,
what the best formulation will be for a
given problem. As an example of the role
of experience, one might expect (knowing
something about resonance in the linear
case) that the results of studying (12.2) will
be diÔ¨Äerent if ùúîis close to k than if it is
far away. But how do you express math-
ematically the idea that ‚Äúùúîis close to k‚Äù?
Recalling that the only parameter available
to express ‚Äúsmallness‚Äù is ùúÄ, the best answer
turns out to be
ùúî2 = k2 + ùúÄd,
(12.4)
where d is another constant. Substituting
(12.4) into (12.2) leads to the ‚Äúcorrect‚Äù for-
mulation of the near-resonance problem.
One can see that the setting up of pertur-
bation problems is sometimes an art rather
than a science. In this chapter, we will for
the most part assume that a perturbation
problem has been chosen. Mathematical
analysis of the problem may then suggest
the use of stretched or otherwise rescaled
variables, which, in fact, amount to a modi-
Ô¨Åcation of the initial perturbation problem.
In recent years, a further consideration
has come to the fore regarding the choice of
parameters in a mathematical model. Phys-
ical considerations may have led to a model
such as (12.1) above, and yet we know that
this model is not exactly correct; there may,
for instance, be very small nonlinearities
other than the cubic term in the restor-
ing force, or nonlinearities in the damp-
ing, or additional harmonics in the forcing.
How many such eÔ¨Äects should be included
in the model? In the past, it was simply
a matter of trial and error. But in certain
cases, there now exists a mathematical the-
ory that is able to determine just which
additional small terms might make a quali-
tative (rather than just a tiny quantitative)
diÔ¨Äerence in the solution. In these cases,
it is sometimes best to add all such sig-
niÔ¨Åcant small terms to the equation before
attempting the solution, even if there is no
evident physical reason for doing so. The
process of adding these additional terms is
called Ô¨Ånding the universal unfolding of the
system. The advantage is that the univer-
sal unfolding will account for all possible
qualitative behaviors that may be observed
as a result of unknown perturbations. For
instance, in the past, many bifurcations
were not observed to occur exactly as pre-
dicted. They were called imperfect bifurca-
tions, and each situation required that the
speciÔ¨Åc perturbation responsible for the
imperfection be discovered and incorpo-
rated into the model. Now it is often pos-
sible to determine all possible imperfec-
tions in advance by examining the universal
unfolding of the original model. See [1].
In addition to the types of problems
already discussed, there exist problems
that do not contain a perturbation param-
eter but nonetheless allow treatment by
perturbation methods. For instance, a
system of diÔ¨Äerential equations may have
a particular solution (often an equilibrium
solution or a periodic solution) that can
be computed exactly, and one may wish to
study the solutions lying in a neighborhood
of this one. One way to treat such problems
is called coordinate perturbations; the
coordinates themselves (or more precisely,
the diÔ¨Äerences between the coordinates
of the known and unknown solutions)
are treated as small quantities, in place
of
a
perturbation
parameter.
Another
approach is to introduce a parameter ùúÄas
a scale factor multiplying these coordinate
diÔ¨Äerences. Both ideas will be illustrated
in the discussion of normal forms in
Section 12.3.6.

12.2 Basic Concepts
415
12.2.3
Perturbation Series
Let us suppose that a perturbation problem
has been posed, and let the exact solution
(which we wish to approximate) be denoted
by u(x, ùúÄ). Here ùúÄis the (scalar) perturba-
tion parameter, x is a vector consisting of
all other variables in the problem including
coordinates and other parameters, and u is
the quantity being solved for. (In the case of
(12.2) above, u = y and x = (t, a, b, c, ùúî), or
if (12.4) is used, then x = (t, a, b, c, d).) The
simplest form in which to seek an approxi-
mation is that of a (truncated) power series
in ùúÄ:
u(x, ùúÄ) ‚âÖu0(x) + ùúÄu1(x) + ¬∑ ¬∑ ¬∑ + ùúÄkuk(x).
(12.5)
There are times when this is insuÔ¨Écient and
we require a Poincar√© series
u(x, ùúÄ) ‚âÖùõø0(ùúÄ)u0(x) + ùõø1(ùúÄ)u1(x)
+ ¬∑ ¬∑ ¬∑ + ùõøk(ùúÄ)uk(x),
(12.6)
where each ùõøi is a monotone function of ùúÄ
deÔ¨Åned for ùúÄ> 0 satisfying
lim
ùúÄ‚Üí0
ùõøi+1(ùúÄ)
ùõøi(ùúÄ)
= 0;
(12.7)
such functions ùõøi are called gauges. (Of
course, a Poincar√© series reduces to a power
series if ùõøi(ùúÄ) = ùúÄi.) Finally, there are times
when not even a Poincar√© series is suÔ¨Écient
and a generalized series is needed:
u(x, ùúÄ) ‚âÖùõø0(ùúÄ)u0(x, ùúÄ) + ùõø1(ùúÄ)u1(x, ùúÄ)
+ ¬∑ ¬∑ ¬∑ + ùõøk(ùúÄ)uk(x, ùúÄ).
(12.8)
With such a series, it might appear that we
could delete the gauges, or rather assimilate
them into the ui because these are now
allowed to depend upon ùúÄ; but the intention
is that the dependence of ui on ùúÄshould not
aÔ¨Äect its order of magnitude. An example
is the following two-term generalized series
in which the vector x consists only of the
time t:
u(t, ùúÄ) ‚âÖsin(1 + ùúÄ)t + ùúÄcos(1 + ùúÄ)t. (12.9)
Here u0(t, ùúÄ) = sin(1 + ùúÄ)t and u1(t, ùúÄ) =
cos(1 + ùúÄ)t; the dependence of these coeÔ¨É-
cients upon ùúÄmodiÔ¨Åes their period but not
their amplitude, and the second term still
has the order of magnitude of its gauge ùúÄ.
Notice that we have written only trun-
cated series in the previous paragraph.
While most perturbation methods allow,
in principle, for the computation of inÔ¨Ånite
series, these series seldom converge, and
in practice it is impossible to calculate
more than a few terms. The type of accu-
racy that we hope for in a perturbation
solution is not convergence (improve-
ment in accuracy as the number of terms
increases), but rather asymptotic validity,
which means improvement in accuracy as
ùúÄapproaches zero. To explain this concept,
let us consider a generalized series
u(ùúÄ) ‚âÖùõø0(ùúÄ)u0(ùúÄ) + ùõø1(ùúÄ)u1(ùúÄ)
+ ¬∑ ¬∑ ¬∑ + ùõøk(ùúÄ)uk(ùúÄ)
(12.10)
that contains no variables other than ùúÄ. We
will say that this series is an asymptotic
approximation to u(ùúÄ) if
u(ùúÄ) = ùõø0(ùúÄ)u0(ùúÄ) + ùõø1(ùúÄ)u1(ùúÄ)
+ ¬∑ ¬∑ ¬∑ + ùõøk(ùúÄ)uk(ùúÄ) + R(ùúÄ),
(12.11)
where the remainder or error R(ùúÄ) satisÔ¨Åes
a bound of the form
|R(ùúÄ)| ‚â§cùõøk+1(ùúÄ)
(12.12)
for some constant c > 0 and some gauge
ùõøk+1 that approaches zero more rapidly than

416
12 Perturbation Methods
ùõøk as ùúÄ‚Üí0. (If u, and hence R, are vector-
valued, then |R(ùúÄ)| denotes a vector norm.)
Equation (12.12) is abbreviated with the
‚Äúbig-oh‚Äù notation
R(ùúÄ) = Óàª(ùõøk+1(ùúÄ)).
(12.13)
The series (12.10) is called asymptotically
valid, or an asymptotic series, if it is an
asymptotic approximation (in the above
sense) and in addition every truncation
of (12.10) is also an asymptotic approxi-
mation, with the error being ‚Äúbig-oh‚Äù of
the Ô¨Årst omitted gauge. The case in which
u(x, ùúÄ) depends upon variables x in addi-
tion to ùúÄwill be discussed in the following
section on uniformity.
As a technical matter, any bound such as
(12.12) is not intended to hold for all ùúÄ, but
only for ùúÄin some interval 0 ‚â§ùúÄ‚â§ùúÄ0. A
perturbation solution is never expected to
be valid for large values of the perturbation
parameter, and the meaning of ‚Äúlarge‚Äù is rel-
ative. In this chapter, we will not continue
to mention ùúÄ0, but it is always lurking in the
background.
Although Fourier series may arise in
perturbation theory when dealing with
oscillatory problems, a Fourier series is
never a perturbation series. Rather, if a per-
turbation series such as u(t, ùúÄ) ‚âÖu0(t) +
ùúÄu1(t) depends periodically on time t, then
the coeÔ¨Écients uk(t) may be expressed as
Fourier series in t.
12.2.4
Uniformity
In the previous section we have deÔ¨Åned
asymptotic validity of a perturbation series
for a function u(ùúÄ) depending only on ùúÄ.
This is adequate for a problem such as
Ô¨Ånding a root of a polynomial (supposing
that the polynomial contains a perturbation
parameter ùúÄ), because the root is a single
number. But for most perturbation prob-
lems (such as diÔ¨Äerential equations), the
solution is a function of space and/or time
coordinates, and possibly various parame-
ters, in addition to ùúÄ. For such problems, the
previous deÔ¨Ånition of asymptotic validity is
insuÔ¨Écient.
Let us return to the generalized series
(12.8), and denote the error by R(x, ùúÄ). Now
we may require that for each Ô¨Åxed x this
error is of order ùõøk+1(ùúÄ):
|R(x, ùúÄ)| ‚â§c(x)ùõøk+1(ùúÄ).
(12.14)
Notice that the ‚Äúconstant‚Äù c here may
change when we move to a new point x. In
this case, we say that the error is pointwise
of order ùõøk+1.
Alternatively, we can choose a domain D
of x and require that the error be uniformly
of order ùõøk+1 for all x in D:
|R(x, ùúÄ)| ‚â§cùõøk+1(ùúÄ).
(12.15)
Here the constant c is truly constant. In
this case, we say R(x, ùúÄ) = Óàª(ùõøk+1(ùúÄ)) uni-
formly in x for x in D. If every truncation
of a perturbation series is uniformly of
the order of the Ô¨Årst omitted gauge, we
say that the series is uniformly valid. (In
the last sentence, we neglected to say ‚Äúin
D,‚Äù but it is important to remember that
such an expression has no meaning unless
the domain D is understood.) Obviously,
uniform asymptotic validity is stronger
than pointwise validity, and it is safe to say
that every method used in perturbation
theory has been introduced in order to gain
uniform validity for a problem for which
previous methods only gave pointwise
validity.
Now the deÔ¨Ånition of uniform validity
calls for an estimate of the error of an
approximation, and such an estimate is a

12.2 Basic Concepts
417
diÔ¨Écult thing to come by. It would be con-
venient if there were an easier test for uni-
formity. In actuality, there is not. How-
ever, it is possible to obtain a simple nec-
essary (but not suÔ¨Écient) condition for a
series to be uniformly valid; namely, it is
not diÔ¨Écult to show that if a series (12.8)
is uniformly valid in a domain D, then each
coeÔ¨Écient uk(x) with k ‚â•1 is bounded on
D, that is, there exist constants ck such
that
|uk(x, ùúÄ)| ‚â§ck
(12.16)
for x in D. If this is true, we say that
the series (12.8) is uniformly ordered. We
have already encountered the concept of a
uniformly ordered series when discussing
(12.9) above. A uniformly ordered series is
one in which each term after the Ô¨Årst is
of no greater order than is indicated by its
gauge. It is easy to inspect a perturbation
series, once it has been constructed, and
determine whether it is uniformly ordered.
If not, the series is called disordered, and
it cannot be uniformly valid. On the other
hand, if it is uniformly ordered, it does not
follow that it is uniformly valid, because one
has done nothing toward estimating the
error. Almost all textbooks are misleading
on this point, because they almost invari-
ably claim to be showing the uniform valid-
ity of a series when, in fact, they are only
showing that it is uniformly ordered. How-
ever if a perturbation series is constructed
on the basis of good intuitive insight into
a problem, and if it is uniformly ordered,
then it frequently turns out to be uniformly
valid as well. (An elementary example in
which this is not the case will be given in
Section 12.3.2.)
With
regard
to
uniform
ordering,
Poincar√© series occupy a special place.
Recall that a series is a Poincar√© series if its
coeÔ¨Écients do not depend on ùúÄ; see (12.6).
In this case, if the domain D is compact and
the coeÔ¨Écients uk(x) are continuous, then
the coeÔ¨Écients are automatically bounded
and the series is uniformly ordered (but
still not automatically uniformly valid).
However, even a Poincar√© series may fail to
be uniformly ordered if D is not compact
or if D is allowed to depend on ùúÄ. We
have not considered this latter possibility
until now, but in fact, in many problems
one must consider domains D(ùúÄ) that
depend on ùúÄ. An important example is a
boundary layer, a thin domain near the
boundary of some larger region, whose
thickness depends on ùúÄ. For such domains,
the deÔ¨Ånitions (12.15) and (12.16) of uni-
form validity and uniform ordering are
the same, except that they are to hold for
all x in D(ùúÄ); that is, for each value of ùúÄ,
(12.15) or (12.16) hold for a diÔ¨Äerent range
of x.
It is now possible to explain one of the
principal divisions in the subject of per-
turbation theory, the division of perturba-
tion problems into regular and singular.
A perturbation problem is regular if there
exists a Poincar√© series that is uniformly
valid on the intended domain; it is singu-
lar if it is necessary to use a generalized
series in order to obtain a uniformly valid
solution on the intended domain. This is
the only correct deÔ¨Ånition. Many textbooks
give partial deÔ¨Ånitions such as ‚Äúa perturba-
tion problem for a diÔ¨Äerential equation is
singular if ùúÄmultiplies the highest deriva-
tive.‚Äù Such a deÔ¨Ånition, which refers only
to the diÔ¨Äerential equation without stat-
ing an intended domain, cannot be cor-
rect. The presence of an ùúÄmultiplying the
highest derivative does, of course, aÔ¨Äect the
domain on which a problem can be regu-
lar; we will see below that a problem such
as Ãàu + u + ùúÄu3 = 0 is regular on any Ô¨Åxed
interval [0, T] but singular on an expand-
ing interval [0, 1‚àïùúÄ], while a problem such

418
12 Perturbation Methods
as ùúÄÃàu + (t2 + 1) Ãáu + u = 0 is regular only on
a shrinking interval [0, ùúÄ] and singular on a
Ô¨Åxed interval.
12.3
Nonlinear Oscillations and Dynamical
Systems
In this section, we discuss the major pertur-
bation methods used in the study of nonlin-
ear ordinary diÔ¨Äerential equations (dynam-
ical systems). Typical problems include the
location and bifurcation of rest (or equi-
librium) points and periodic or quasiperi-
odic solutions; the approximation of solu-
tions close to these; the solution of initial
value problems for systems that become
solvable (usually either linear or integrable
Hamiltonian) when a small parameter van-
ishes; and the splitting of a homoclinic orbit
under perturbation. In all of these prob-
lems, there is an interplay between quali-
tative and quantitative behavior. Advance
knowledge of the qualitative behavior may
assist the choice of a successful perturba-
tion method. Alternatively, if the qualitative
behavior is unknown, perturbation meth-
ods may be used in an exploratory fashion,
as long as it is kept in mind that very fre-
quently, especially in nonlinear problems,
a perturbation solution may appear correct
but may predict qualitative features (such as
periodicity, stability, or presence of chaos)
erroneously. As a general rule, qualitative
results obtained from perturbation solu-
tions (or any other approximate solutions)
should be taken as conjectural, until sup-
ported by some combination of numeri-
cal or experimental evidence and rigorous
mathematical proof.
12.3.1
Rest Points and Regular Perturbations
Given a system of diÔ¨Äerential equations of
the form
Ãáx = f(x, ùúÄ) = f0(x) + ùúÄf1(x) + Óàª(ùúÄ2),
(12.17)
with x = (x1, ‚Ä¶ , xn) ‚àà‚Ñùn and ùúÄ‚àà‚Ñù, the
rest points are the solutions of the system
of equations f(x, ùúÄ) = 0. If a rest point a0 is
known when ùúÄ= 0, it may continue to exist
as a function a(ùúÄ) with
f(a(ùúÄ), ùúÄ) = 0
(12.18)
for ùúÄnear zero, or it may bifurcate into
two or more rest points, or disappear alto-
gether; the results may diÔ¨Äer for ùúÄ> 0 and
ùúÄ< 0. A crucial role is played by the matrix
A0 = fx(x0, 0) of partial derivatives of f at
the unperturbed rest point, and especially
its eigenvalues. If A0 is invertible (zero is
not an eigenvalue), then, by the implicit
function theorem, a unique continuation
a(ùúÄ) exists and is computable as a series
a(ùúÄ) = a0 + ùúÄa1 + Óàª(ùúÄ2).
(12.19)
This is the simplest example of a perturba-
tion series. Putting (12.19) into (12.18) and
expanding gives f0(a0) + ùúÄ(A0a1 + f1(a0)) +
¬∑ ¬∑ ¬∑ = 0, or (because f0(a0) = 0)
a1 = ‚àíA‚àí1
0 f1(a0).
(12.20)
The solution may be continued to higher
order, and the matrix function
A(ùúÄ) = fx(a(ùúÄ), ùúÄ)
(12.21)
may also be studied, because (in most cases)
it determines the stability of the rest point.
If all of the eigenvalues of A0 are oÔ¨Äthe
imaginary axis (A0 is hyperbolic), the same

12.3 Nonlinear Oscillations and Dynamical Systems
419
will be true for A(ùúÄ) for small ùúÄ, and the
stability type (dimensions of the stable and
unstable manifolds) of the rest point will
not change. When this is not the case,
the methods of Section 12.6 determine the
spectrum of A(ùúÄ), and thus usually suÔ¨Éce to
decide how the stability changes.
When A0 is not invertible, bifurcation
(change in the number of rest points) usu-
ally occurs. Even when A0 has only one
zero eigenvalue, various possibilities (such
as saddle-node and pitchfork bifurcations)
exist, and it is not possible to give details
here. A reference treating the subject from
the standpoint of perturbation theory is [2];
a quite diÔ¨Äerent modern treatment is [1].
Now suppose that a solution
x(t, ùúÄ) = x0(t) + ùúÄx1(t) + Óàª(ùúÄ2)
(12.22)
of the system (12.17) is to be found, with ini-
tial condition x(0, ùúÄ) = a(ùúÄ) (no longer a rest
point) given by (12.19). Substituting (12.22)
into (12.17), expanding, and equating terms
of the same degree in ùúÄyields
Ãáx0 = f0(x0)
Ãáx1 = f0x(x0(t))x1 + f1(x0(t)).
(12.23)
If the Ô¨Årst equation of (12.23), which is
the same as (12.17) with ùúÄ= 0, can be
solved with initial condition x0(0) = a0, its
solution x0(t) can be placed in the second
equation of (12.23), which then becomes an
inhomogeneous linear equation for x1; it is
to be solved with initial condition x1(0) =
a1. Equations of this type are not necessar-
ily easy to solve, but are certainly easier than
the nonlinear equation (12.17). If this is suc-
cessful, the procedure may be continued to
higher order.
This is usually called the regular pertur-
bation method or the method of straight-
forward expansion. According to our earlier
deÔ¨Ånition, a perturbation method is regular
if it provides a Poincar√© series that is uni-
formly valid on the intended domain. Here
(12.22) is a Poincar√© series, and it can be
shown to be uniformly valid on any Ô¨Ånite
interval [0, T]; that is, the error bound is
of the order of the Ô¨Årst omitted term, and
once T is chosen, the coeÔ¨Écient in the error
bound is Ô¨Åxed. So the term ‚Äúregular‚Äù is jus-
tiÔ¨Åed if this is the intended domain. In many
problems (below), one seeks a solution valid
on an ‚Äúexpanding‚Äù interval such as [0, 1‚àïùúÄ];
the straightforward expansion is usually not
valid for this purpose, and so is often called
a naive expansion.
There are situations in which straightfor-
ward expansion is valid for much longer
than Ô¨Ånite intervals. For instance, if a solu-
tion is approaching a sink (a rest point with
all eigenvalues in the left half-plane), the
straightforward expansion is valid for all t ‚â•
0; see [3, Chapter 5]. More generally, if the
Ô¨Årst equation of (12.23) has a solution that
connects two hyperbolic rest points, then
a straightforward expansion (to any order)
beginning with that solution will be shad-
owed by an exact solution of (12.17) con-
necting two hyperbolic rest points of that
system; that is, the approximate and exact
solutions will remain close (to the order of
the Ô¨Årst omitted term) for all time, both past
and future, although the two solutions may
not have any point in common. (In partic-
ular, the approximate and shadowing solu-
tions will not satisfy the same initial condi-
tions.) See [3, Chapter 6].
12.3.2
Simple Nonlinear Oscillators and
Lindstedt‚Äôs Method
The ‚Äúhard‚Äù nonlinear spring or unforced
DuÔ¨Éng equation is given by
Ãàu + u + ùúÄu3 = 0
(12.24)

420
12 Perturbation Methods
for ùúÄ> 0. It can be expressed as a Ô¨Årst-order
system in the (u, Ãáu) phase plane in the form
Ãáu = v, Ãáv = ‚àíu ‚àíùúÄu3. In the phase plane, the
orbits are closed curves surrounding the
rest point at the origin; this may be seen
from the conservation of energy. (The ‚Äúsoft‚Äù
spring with ùúÄ< 0 behaves diÔ¨Äerently.) As
the solutions of (12.24) with any initial con-
ditions u(0) = a, Ãáu(0) = b are smooth func-
tions of ùúÄ, they may be expanded as Tay-
lor series having the form (if we retain two
terms)
u(t, ùúÄ) ‚âÖu0(t) + ùúÄu1(t).
(12.25)
The coeÔ¨Écients may be determined by sub-
stituting (12.25) into (12.24), expanding the
u3 term, dropping powers of ùúÄhigher than
the Ô¨Årst, and setting each order in ùúÄsepa-
rately equal to zero. This gives two linear
equations that can be solved (recursively)
for u0 and u1. The result, for b = 0 (and
it is enough to consider this case because
every solution is at rest momentarily when
its amplitude is at its maximum), is
u(t, ùúÄ) ‚âÖa cos t ‚àíùúÄa3
32(cos t
+12t sin t ‚àícos 3t).
(12.26)
Upon examining (12.26) for uniform order-
ing, we discover that all functions of t
appearing there are bounded for all t except
for 12t sin t, which becomes unbounded as
t approaches inÔ¨Ånity. This is an example of
a so-called ‚Äúsecular‚Äù term, one which grows
over the ‚Äúages‚Äù (saeculae in Latin). We con-
clude from this that (12.26) is uniformly
ordered for t in any Ô¨Ånite interval D =
[0, T] but not for D = [0, ‚àû). According to
the general principles discussed in Section
12.2.4, this shows that (12.26) is not uni-
formly valid on [0, ‚àû), and it leads us to sus-
pect, but does not prove, that (12.16) is uni-
formly valid on [0, T]. In the present case,
this conjecture is correct. If the intended
domain D for the solution of (12.24) is
a Ô¨Ånite interval, then we have obtained a
uniform approximation in the form of a
Poincar√© series, and the problem is a regu-
lar one. If a solution valid for a longer time
is desired, the problem will prove to be sin-
gular.
In an eÔ¨Äort to extend the validity of the
solution, we recall that the actual solutions
of (12.24) are periodic, whereas (12.26) is
not. The problem is that the period of the
exact solution depends upon ùúÄ, and there
is no way that a Poincar√© series can have
such a period because the coeÔ¨Écients are
not allowed to depend on ùúÄ. To remedy
this, we seek to approximate the (unknown)
frequency of the solution in the form
ùúà(ùúÄ) ‚âÖÃÉùúà(ùúÄ) = ùúà0 + ùúÄùúà1 + ùúÄ2ùúà2
(12.27)
with the solution itself being represented as
u(t, ùúÄ) ‚âÖu0(ÃÉùúà(ùúÄ)t) + ùúÄu1(ÃÉùúà(ùúÄ)t),
(12.28)
which is now a generalized series. Notice
that we have carried (12.27) to one more
order than (12.28). Now we substitute
(12.27) and (12.28) into (12.24) and attempt
to determine ùúà0, u0, ùúà1, u1, ùúà2 recursively,
in that order, in such a way that each ui is
periodic. The mechanics of doing this will
be explained in the next paragraph; the
result is
u(t, ùúÄ) ‚âÖa cos t+
+ùúÄ
(
‚àí1
32a3 cos t+ + 1
32a3 cos 3t+)
,
(12.29)
where
t+ = ÃÉùúà(ùúÄ)t =
(
1 + ùúÄ3
8a2 ‚àíùúÄ2 21
256a4)
t.
(12.30)

12.3 Nonlinear Oscillations and Dynamical Systems
421
Examining (12.29), we see that it is uni-
formly ordered for all time, because the
coeÔ¨Écients are bounded (there are no sec-
ular terms). One might therefore conjec-
ture that the solution is uniformly valid
for all time, but this would be incorrect!
(This example is an excellent warning as to
the need for proofs of validity in perturba-
tion theory.) The diÔ¨Éculty is that t+ uses
the approximate frequency ÃÉùúà(ùúÄ) in place
of the exact frequency ùúà(ùúÄ); there is no
escape from this, as the exact frequency
remains unknown. Therefore, (12.29) grad-
ually gets out of phase with the exact solu-
tion. The reason for taking (12.27) to one
higher order than (12.28) is to minimize
this eÔ¨Äect. It can be shown that (12.29) is
uniformly valid on the expanding interval
D(ùúÄ) = [0, 1‚àïùúÄ]. (This is our Ô¨Årst example
of a domain that depends on ùúÄ, as dis-
cussed in Section 12.2.4) If the intended
domain is such an expanding interval, then
(12.29) provides a uniformly valid general-
ized series, and the problem is seen to be
singular. (If the intended domain is all t, the
problem is simply impossible to approxi-
mate asymptotically.)
In order to complete the example in the
last paragraph, we must indicate how to
obtain (12.29) and (12.30). The easiest way
is to substitute ùúè= ùúà(ùúÄ)t into (12.24) to
obtain
ùúà(ùúÄ)2 d2u
dùúè2 + u + ùúÄu3 = 0.
(12.31)
Then substitute (12.27) and (12.28) into
(12.31), expand, and set the coeÔ¨Écient of
each power of ùúÄequal to zero as usual. It
is easy to Ô¨Ånd that ùúà0 = 1 and u0 = a cos ùúè
(which in the end becomes a cos t+ because
we do not know the exact frequency ùúà).
The crucial step arises when examining the
equation for u1, which is (writing ‚Ä≤ = d‚àïdùúè)
u‚Ä≤‚Ä≤
1 + u1 = ‚àía3 cos3 ùúè+ 2ùúà1a cos ùúè
(12.32)
=
(
‚àí3
4a3 + 2ùúà1a
)
cos ùúè
‚àí1
4a3 cos 3ùúè.
From the Fourier series expansion (the
second line of (12.32)), we see that the
term in cos ùúèwill be resonant with the free
frequency, and hence produce unbounded
(secular) terms in u1, unless the coeÔ¨Écient
of cos ùúèvanishes. In this way, we conclude
that
ùúà1 = 3
8a2
(12.33)
and, after deleting the cos ùúèterm from
(12.32), we solve it for u1. This procedure is
repeated at each subsequent stage.
The
previous
example
is
typical
of
unforced conservative oscillators, in which
every solution (at least in a certain region)
is
periodic.
There
are
two
additional
classes of oscillators that must be men-
tioned, although we cannot give them as
much space as they deserve: self-excited
oscillators and forced oscillators.
The standard example of a self-excited
oscillator is the van der Pol equation
Ãàu + ùúÄ(u2 ‚àí1) Ãáu + u = 0.
(12.34)
Instead of a region in the phase plane Ô¨Ålled
with periodic solutions, this equation has a
single periodic solution (limit cycle) for ùúÄ>
0. The Lindstedt method, described above,
can be used to approximate the periodic
solution, but must be modiÔ¨Åed slightly: the
initial condition can no longer be assigned
arbitrarily, because to do so will, in gen-
eral, yield a nonperiodic solution for which
the Lindstedt method fails. (These solu-
tions can be found by averaging or mul-
tiple scales; see below.) Suppose that the

422
12 Perturbation Methods
limit cycle crosses the positive x axis (in the
phase plane) at (a(ùúÄ), 0) and has frequency
ùúà(ùúÄ). Then the solution is sought in the form
of (12.27) and (12.28) together with an addi-
tional expansion a(ùúÄ) = ùõº0 + ùúÄa1 + ¬∑ ¬∑ ¬∑; the
coeÔ¨Écients ui, ùúài, and ai are determined
recursively, choosing ùúài and ai so that no
secular terms arise in ui+1. This example
shows the eÔ¨Äect of the dynamics of a system
on the correct formulation of a perturba-
tion problem.
The general nearly linear, periodically
forced oscillator can be written
Ãàu + u = ùúÄf (u, Ãáu, ùúît),
(12.35)
where f (u, v, ùúÉ) is 2ùúã-periodic in ùúÉ; thus the
period of the forcing is 2ùúã‚àïùúî. The dynam-
ics of (12.35) can be complicated, and we
will limit ourselves to one type of peri-
odic solution, the harmonic oscillations,
which are entrained by the forcing so that
they have the same frequency ùúî; these har-
monic solutions occur for ùúÄsmall and ùúî
close to 1 (the frequency of the solutions
when ùúÄ= 0). As the problem contains two
parameters (ùúÄand ùúî) and we are limited
to one-parameter methods, it is necessary
to express the statement ‚Äúùúîis close to 1‚Äù
in terms of the perturbation parameter ùúÄ.
(A study using two independent parameters
would uncover the phenomenon of ‚Äúreso-
nance horns‚Äù or ‚Äúresonance tongues.‚Äù See
[4, Section 5.5]) It turns out that an eÔ¨Écient
way to do so is to write
ùúî2 = 1 + ùúÄùõΩ,
(12.36)
where ùõΩis a new parameter that is consid-
ered Ô¨Åxed (not small). With (12.36), (12.35)
can have one or more isolated periodic
solutions with unknown initial conditions
(a(ùúÄ), b(ùúÄ)). (We can no longer assume b =
0.) On the other hand, the frequency of
the solution this time is not unknown but
equals ùúî. So the Lindstedt method under-
goes another modiÔ¨Åcation dictated by the
dynamics: u, a, and b are expanded in ùúÄand
solved recursively, choosing the coeÔ¨Écients
of a and b to eliminate secular terms from
u. In contrast with the previous cases, there
is no accumulating phase error because the
frequency is known, and the perturbation
approximations are uniformly valid for all
time.
12.3.3
Averaging Method for Single-Frequency
Systems
All of the systems discussed in the previous
section, and a great many others, can be
expressed in periodic standard form,
Ãáx = ùúÄf(x, t, ùúÄ),
(12.37)
where x = (x1, ‚Ä¶ , xn) ‚àà‚Ñùn and where f is
2ùúã-periodic in t. The solutions of (12.37)
may be approximated by the method of
averaging, which not only locates the
periodic solutions (and proves their exis-
tence), but also determines their stability or
instability and approximates the transient
(nonperiodic) solutions. The method of
averaging has been rediscovered many
times and exists (with slight variations)
under
a
variety
of
names,
including:
method of van der Pol; method of Krylov‚Äì
Bogoliubov‚ÄìMitropolski (KBM method);
method of slowly varying amplitude and
phase;
stroboscopic
method;
Struble‚Äôs
method; von Zeipel‚Äôs method (in the
Hamiltonian case); method of Lie series or
Lie transforms. Some of the diÔ¨Äerences in
these ‚Äúmethods‚Äù pertain to how the orig-
inal system is put into periodic standard
form, and others to details about how the
near-identity transformations (described
below) are handled.

12.3 Nonlinear Oscillations and Dynamical Systems
423
To illustrate how a system may be put
into periodic standard form, consider the
van der Pol equation (12.34), or, written as
a system,
Ãáu = v
(12.38)
Ãáv = ‚àíu + ùúÄ(1 ‚àíu2)v.
Rotating polar coordinates (r, ùúë) may be
introduced by u = r cos(ùúë‚àít), v = r sin
(ùúë‚àít), giving
Ãár = ùúÄ
(
1 ‚àír2 cos2(ùúë‚àít)
)
√ó r sin2(ùúë‚àít)
(12.39)
Ãáùúë= ùúÄ(1 ‚àír2 cos2(ùúë‚àít)) sin(ùúë‚àít)
√ó cos(ùúë‚àít),
which is in periodic standard form with x =
(r, ùúë). The same result may be achieved by
seeking a solution of (12.34) by variation
of parameters, in the form u = r cos(ùúë‚àít)
where r and ùúëare variables, and imposing
the requirement that Ãáu = r sin(ùúë‚àít); the
motivation for these choices is that with r
and ùúëconstant, these solve (12.34) for ùúÄ=
0. The transformation to periodic standard
form is merely a change of coordinates, not
an assumption about the nature of the solu-
tions.
In its crudest form, the method of averag-
ing simply consists in replacing (12.37) by
Ãáy = ùúÄf(y),
(12.40)
where
f(y) = 1
2ùúã‚à´
2ùúã
0
f(y, t, 0) dt.
(12.41)
System (12.40) is easier to solve than
(12.37), because it is autonomous. The
form of (12.40) can be motivated by the
fact that for small ùúÄ, x in (12.37) is slowly
varying and therefore nearly constant over
one period; therefore, to a Ô¨Årst approxi-
mation, we might hold x constant while
integrating over one period in (12.37) to
Ô¨Ånd the ‚Äúaverage‚Äù inÔ¨Çuence due to f. But
this sort of motivation gives no idea how to
estimate the error or to extend the method
to higher-order approximations. A much
better procedure is to return to (12.37) and
perform a near-identity change of variables
of the form
x = y + ùúÄu1(y, t, ùúÄ),
(12.42)
where u1 is a periodic function of t, which
is to be determined so that the transformed
equation has the form
Ãáy = ùúÄg1(y) + ùúÄ2ÃÇg(y, t, ùúÄ),
(12.43)
where g1 is independent of t. It turns out
that such a transformation is possible
only if we take g1 = f; by doing so, (12.40)
can be obtained from (12.43) simply by
deleting the ùúÄ2 term. The solution y(t)
of (12.40) is called the Ô¨Årst approxima-
tion to the solution of (12.37); if this is
substituted into (12.42), the result x(t) is
called the improved Ô¨Årst approximation.
This ‚Äúimprovement‚Äù consists in adding (an
approximation to) small periodic Ô¨Çuctua-
tions around y(t) that are actually present,
but are of the same order as the error of the
Ô¨Årst approximation, so while the numerical
results are better, the asymptotic accuracy
of the improved Ô¨Årst approximation is not
better than that of the Ô¨Årst approximation.
When it is formulated in this way, the
Ô¨Årst-order method of averaging is seen to
consist of nothing but coordinate changes
(Ô¨Årst into periodic standard form, then
into form (12.43)), followed by truncation
(dropping ÃÇg); it is only the truncation that
introduces error, and this error can be
estimated using Gronwall‚Äôs inequality. It is
also clear how to proceed to higher orders:

424
12 Perturbation Methods
simply replace (12.42) by
x = y + ùúÄu1(y, t, ùúÄ) + ¬∑ ¬∑ ¬∑ + ùúÄkuk(y, t, ùúÄ)
(12.44)
and (12.43) by
Ãáy = ùúÄg1(y) + ¬∑ ¬∑ ¬∑ + ùúÄkgk(y) + ùúÄk+1ÃÇg(y, t, ùúÄ);
(12.45)
the averaged equations are obtained by
deleting ÃÇg. It is, of course, necessary to
determine the ui and gi recursively in such
a way that the ui are periodic and the gi are
independent of t; this is where the techni-
cal details of various versions of averaging
come into play. (Warning: gi for i > 1 are
not simply averages of higher-order terms
of f, but of expressions involving these and
uj for j < i.) Once (12.45), without ÃÇg, has
been determined, it must be solved. Setting
ùúè= ùúÄt, it becomes
dy
dùúè= g1(y) + ùúÄg2(y) + ¬∑ ¬∑ ¬∑ + ùúÄk‚àí1gk(y),
(12.46)
which can often be solved by regular per-
turbation theory on the interval 0 ‚â§ùúè‚â§1,
that is, 0 ‚â§t ‚â§1‚àïùúÄ. Then the solution must
be put back into the transformation (12.44);
the resulting approximate solutions of
(12.37) will diÔ¨Äer from the exact solutions
(with the same initial condition) by Óàª(ùúÄk)
during a time interval of length Óàª(1‚àïùúÄ).
The
kth
approximation
and
improved
kth approximation are obtained by using
(12.44) without, or with, its kth term;
again, these are asymptotically equivalent,
although the improved approximation is
numerically more accurate.
For additional information about the
method of averaging, see [3]. As in regular
perturbation theory, it is possible under
special conditions to obtain results on half-
inÔ¨Ånite or inÔ¨Ånite intervals of time; see
Chapter 5 for systems with attractors and
Chapter 6 for shadowing. See Appendix E
for averaging applied to partial diÔ¨Äerential
equations.
12.3.4
Multifrequency Systems and Hamiltonian
Systems
Oscillatory problems that cannot be put
into periodic standard form can often be
put into the following angular standard
form:
Ãár = ùúÄf(r, ùúΩ, ùúÄ)
(12.47)
ÃáùúΩ= ùõÄ(r) + ùúÄg(r, ùúΩ, ùúÄ),
where r = (r1, ‚Ä¶ , rn) is a vector of ampli-
tudes and ùúΩ= (ùúÉ1, ‚Ä¶ , ùúÉm) a vector of angles
(so that f and g are periodic in each ùúÉi with
period 2ùúã). This form includes the periodic
standard form, by taking m = 1 and ÃáùúÉ= 1.
The ‚Äúnaive‚Äù method of averaging for (12.47)
would be to replace f and g by their averages
over ùúΩ, for instance
f(r) =
1
(2ùúã)m ‚à´
2ùúã
0
¬∑ ¬∑ ¬∑
√ó ‚à´
2ùúã
0
f(r, ùúΩ, 0) dùúÉ1 ¬∑ ¬∑ ¬∑ dùúÉm.
(12.48)
To justify this process, and to extend the
method to higher order, one tries to make
a near-identity change of variables (r, ùúΩ) ‚Üí
(ùùÜ, ùùã) that will render the system indepen-
dent of ùùãup through a given order k in
ùúÄ. However, one encounters at once the
famous diÔ¨Éculty of ‚Äúsmall divisors‚Äù which
make the existence of such a transforma-
tion doubtful. If f is expanded in a conver-
gent multiple Fourier series
f(r, ùúΩ, 0) =
‚àë
ùùÇ
aùùÇ(r)ei(ùúà1ùúÉ1+¬∑¬∑¬∑+ùúàmùúÉm),
(12.49)
then the transformation to averaged form
necessarily involves the series

12.3 Nonlinear Oscillations and Dynamical Systems
425
‚àë
ùùÇ‚â†0
aùùÇ(r)
i(ùúà1Œ©1(r) + ¬∑ ¬∑ ¬∑ + ùúàmŒ©m(r))ei(ùúà1ùúÉ1+¬∑¬∑¬∑+ùúàmùúÉm),
(12.50)
which may not converge because the
denominators i(ùúà1ùúÉ1 + ¬∑ ¬∑ ¬∑ + ùúàmùúÉm) may be
small (or even zero), causing the coeÔ¨É-
cients to become large. It is of no use at
this point to say that ‚Äúperturbation theory
is not concerned with the convergence of
series,‚Äù because the series in question is not
being used for approximation, but to prove
the existence of a transformation needed
in order to justify the method.
Some preliminary progress can be made
by considering the case in which the series
(12.49), and hence (12.50), are Ô¨Ånite. In
this case, convergence diÔ¨Éculties cannot
arise, but there is still the diÔ¨Éculty that for
some r, one or more of the denominators of
(12.50) may become zero. As Œ©i(r) are the
frequencies of the free oscillations (ùúÄ= 0)
of (12.47), the vanishing of a denominator
indicates a resonance relationship among
these frequencies. In general, for each ùùÇ
there will be a resonance manifold, or res-
onance hypersurface, consisting of those r
for which the denominator involving ùùÇvan-
ishes. On (or near) any such surface it is
not permissible to average over all angles ùúΩ,
although it may be possible to average over
a subset of these angles or over certain inte-
gral linear combinations of them.
Results
beyond
these
have
been
obtained in the important special case
of Hamiltonian systems; the Kolmogorov‚Äì
Arnol‚Äôd‚ÄîMoser ( or KAM) theorem and the
Nekhoroshev theorem are a high point of
modern perturbation theory and together
give the deÔ¨Ånitive answer to the problem
of the stability (in the sense of Laplace) of
the (idealized Newtonian) solar system,
with which this chapter began. Consider a
system deÔ¨Åned by a Hamiltonian function
of the form
H(r, ùúΩ, ùúÄ) = H0(r) + ùúÄH1(r, ùúΩ)
+ùúÄ2H2(r, ùúΩ) + ¬∑ ¬∑ ¬∑ ,
(12.51)
where r and ùúΩare as before except that m =
n. Written in the form (12.47), this system is
Ãár = ùúÄùúïH1
ùúïùúΩ+ ¬∑ ¬∑ ¬∑
(12.52)
ÃáùúΩ= ‚àíùúïH0
ùúïr ‚àíùúÄùúïH1
ùúïr + ¬∑ ¬∑ ¬∑ .
As H1 is assumed to be periodic in the
components of ùúΩ, it may be expanded in a
multiple Fourier series like (12.49); diÔ¨Äer-
entiating with respect to any component
of ùúΩthen eliminates the constant term
(a0(r), which is the average). It follows that
ùúïH1‚àïùúïùúΩhas zero average, so that the (naive)
Ô¨Årst-order averaged equation for r becomes
Ãár ‚âÖ0.
(12.53)
This suggests that the motion is oscillatory
with nearly constant amplitudes. The KAM
theorem states that (if a certain determi-
nant does not vanish) the great majority
of initial conditions will lead to quasiperi-
odic motion on an invariant torus close
to a torus r = constant. The Nekhoroshev
theorem states that even for those initial
conditions that do not lie on invariant
tori, the drift in r (called Arnol‚Äôd diÔ¨Äusion)
takes place exponentially slowly (as ùúÄ‚Üí0).
(Notice that an n-dimensional torus in
2n-dimensional space does not form the
boundary of an open set if n > 1, so the
presence of many such invariant tori does
not prevent other solutions from slowly
drifting oÔ¨Äto inÔ¨Ånity. For n = 2, the invari-
ant 2-tori do not bound in 4-space, but they
do in the three-dimensional submanifolds
of constant energy; because solutions have
constant energy, Arnol‚Äôd discussion cannot

426
12 Perturbation Methods
take place.) For details see [5]. The proofs
involve very deep mathematics, such as the
Moser‚ÄìNash implicit function theorem,
with implications for partial diÔ¨Äerential
equations,
Riemannian
geometry,
and
elsewhere.
In all applications of averaging and
related methods to Hamiltonian systems,
it is necessary to have a means of handling
near-identity transformations that preserve
the Hamiltonian form of the equations;
that is, one needs to construct near-identity
transformations that are canonical (or sym-
plectic). Classically, such transformations
can be constructed from their generating
functions (in the sense of Hamilton‚ÄìJacobi
theory); averaging procedures carried out
in this way are called von Zeipel‚Äôs method.
At the present time, this approach can be
regarded as obsolete. It has been replaced
by the method of Lie transforms, in which
near-identity
canonical
transformations
are generated as the Ô¨Çows of Hamiltonian
systems in which ùúÄtakes the place of time.
(The Lie method is not limited to Hamil-
tonian systems, but is particularly useful in
this context.) Algorithmic procedures for
handling near-identity transformations in
this way have been developed, and they are
considerably simpler than using generating
functions. See [6, Section 5.7] and [3,
Chapter 3].
12.3.5
Multiple-Scale Method
The earliest perturbation problem, that of
planetary motion, illustrates the appeal of
the idea of multiple scales. A single planet
under the inÔ¨Çuence of Newtonian gravi-
tation would travel around the sun in an
elliptic orbit characterized by certain quan-
tities called the ‚ÄúKeplerian elements‚Äù (the
eccentricity, major axis, and certain angles
giving the position of the ellipse in space).
As the actual (perturbed) motion of the
planets Ô¨Åts this same pattern for long peri-
ods of time, it is natural to describe the
perturbed motion as ‚Äúelliptical motion with
slowly varying Keplerian elements.‚Äù A sim-
pler example would be a decaying oscilla-
tion of the form e‚àíùúÄt sin t, which could be
described as a periodic motion with slowly
decaying amplitude. Solutions of nonlinear
oscillations obtained by the method of aver-
aging frequently have this form, in which
time appears both as t and as ùúÄt, the lat-
ter representing slow variation; sometimes
other combinations such as ùúÄ2t appear.
This leads to the question whether it is
possible to arrive at such solutions more
directly, by postulating the necessary time
scales in advance. The ‚Äúmethod of multiple
scales‚Äù is the result of such an approach,
and is sometimes regarded as the most
Ô¨Çexible general method in perturbation
theory, because it is applicable both to
oscillatory problems (such as those cov-
ered by averaging) and to boundary layer
problems (discussed below). However, this
very Ô¨Çexibility is also its drawback, because
the ‚Äúmethod‚Äù exists in an immense variety
of ad hoc formulations adapted to particu-
lar problems. (See [6] for examples of many
of these variations.) There are two-scale
methods using fast time t and slow time
ùúÄt; two-scale methods using strained fast
time (ùúà0 + ùúÄùúà1 + ùúÄ2ùúà2 + ¬∑ ¬∑ ¬∑)t (similar to the
strained time in the Lindstedt method)
and slow time ùúÄt; multiple-scale methods
using t, ùúÄt, ùúÄ2t, ‚Ä¶ , ùúÄnt; and methods using
scales that are nonlinear functions of t.
The scales to be used must be selected in
advance by intuition or experience, while
in other methods (averaging and match-
ing), the required scales are generated
automatically. Sometimes, the length of
validity of a solution can be increased by
increasing the number of scales, but (con-
trary to a common impression) this is by

12.3 Nonlinear Oscillations and Dynamical Systems
427
no means always the case; see [3, Section
3.5]. Some problems come with more than
one time scale from the beginning, for
instance, problems that contain a ‚Äúslowly
varying parameter‚Äù depending on ùúÄt. It
may seem natural to treat such a system by
the method of multiple scales, but another
possibility is to introduce ùúè= ùúÄt as an
additional independent variable subject
to Ãáùúè= ùúÄ. In summary, the popularity of
multiple scales results from its shorter
calculations, but this aside, other methods
have greater power.
The general outlines of the method are as
follows. Suppose the chosen time scales are
t, ùúè, ùúéwith ùúè= ùúÄt, ùúé= ùúÄ2t. An approximate
solution is sought as a series taken to a
certain number of terms, such as
x0(t, ùúè, ùúé) + ùúÄx1(t, ùúè, ùúé) + ùúÄ2x2(t, ùúè, ùúé).
(12.54)
In substituting (12.54) into the diÔ¨Äerential
equations to be solved, the deÔ¨Ånitions of
the scales (such as ùúè= ùúÄt) are used, so
that ordinary derivatives with respect to
t are replaced by combinations of partial
derivatives with respect to the diÔ¨Äerent
scales; thus
d
dt = ùúï
ùúït + ùúÄùúï
ùúïùúè+ ùúÄ2 ùúï
ùúïùúé.
(12.55)
From this point on (until the very end,
when ùúèand ùúéare again replaced by their
deÔ¨Ånitions), the separate time scales are
treated as independent variables. This has
the eÔ¨Äect of changing ordinary diÔ¨Äerential
equations into partial diÔ¨Äerential equations
that are highly underdetermined, so that
various free choices are possible in express-
ing the solution. The point of the method is
now to make these choices skillfully so that
the Ô¨Ånal series (12.54) is uniformly ordered
(and, it is hoped, uniformly valid) on the
desired domain.
As an illustration, we return to the van
der Pol equation (12.34) with initial con-
ditions u(0) = a, Ãáu(0) = 0. Choosing time
scales t and ùúè= ùúÄt, and writing the solution
as u ‚âÖu0(t, ùúè) + ùúÄu1(t, ùúè), one Ô¨Ånds recur-
sively that (with subscripts denoting partial
derivatives) u0tt + u0 = 0 and u1tt + u1 =
‚àí2u0tùúè‚àíu2
0u0t. The Ô¨Årst equation gives
u0(t, ùúè) = A(ùúè) cos t + B(ùúè) sin t,
a
mod-
ulated
oscillation
with
slowly
varying
coeÔ¨Écients. The solution remains under-
determined, because there is nothing here
to Ô¨Åx A(ùúè) and B(ùúè). The solution for u0 is
now substituted into the right-hand side of
the diÔ¨Äerential equation for u1, and A(ùúè)
and B(ùúè) are chosen to eliminate resonant
terms so that the solution for u1 will remain
bounded. (This is similar to the way the
undetermined quantities are Ô¨Åxed in the
Lindstedt method.) The result is
u(t) ‚âÖu0(t, ùúÄt) =
2a
‚àö
a2 + (4 ‚àía2)e‚àíùúÄt cos t.
(12.56)
This is the same result (to Ô¨Årst order) as
would be found by applying averaging to
(12.39), but it has been found without any
preliminary coordinate transformations.
On the other hand, the possibility of con-
structing the solution depended on the
correct initial guess as to the time scales
to be used; the method of averaging gener-
ates the needed time scales automatically.
The solution (12.56) exhibits oscillations
tending toward a limit cycle that is a simple
harmonic motion of amplitude 2. This is
qualitatively correct, but the motion is not
simple harmonic; carrying the solution to
higher orders will introduce corrections.
12.3.6
Normal Forms
Suppose that the origin is a rest point for
a system Ãáx = f(x), x ‚àà‚Ñùn, and it is desired

428
12 Perturbation Methods
to study solutions of the system near this
point. (Any rest point can be moved to the
origin by a shift of coordinates.) The system
can be expanded in a (not necessarily con-
vergent) series
Ãáx = Ax + f2(x) + f3(x) + ¬∑ ¬∑ ¬∑ ,
(12.57)
where A is a matrix, f2 consists of homo-
geneous quadratic terms, and so forth. The
matrix A can be brought into real canon-
ical form by a change of coordinates (or
into Jordan canonical form, if one is will-
ing to allow complex variables and keep
track of the conditions guaranteeing reality
in the original variables). The object of nor-
mal form theory is to continue this simpliÔ¨Å-
cation process into the higher-order terms.
This is usually done recursively, one degree
at a time, by applying changes of coordi-
nates that diÔ¨Äer from the identity by terms
having the same degree as the term to be
simpliÔ¨Åed. This is an example of a coordi-
nate perturbation (Section 12.2.2), because
it is ‚Äñx‚Äñ that is small, not a perturbation
parameter. However, writing x = ùúÄùùÉturns
(12.57) into
ÃáùùÉ= AùùÉ+ ùúÄf2(ùùÉ) + ùúÄ2f3(ùùÉ) + ¬∑ ¬∑ ¬∑ ,
(12.58)
which is an ordinary perturbation of a lin-
ear problem.
When A is semisimple (diagonalizable
using complex numbers), it is possible to
bring all of the terms f2, f3, ‚Ä¶ (up to any
desired order) into a form that exhibits
symmetries determined by A. For instance,
[ Ãáx
Ãáy
]
=
[ 0
‚àí1
1
0
] [ x
y
]
+ùõº1
(x2 + y2) [ x
y
]
+ùõΩ1
(x2 + y2) [ ‚àíy
x
]
+ùõº2
(x2 + y2)2 [ x
y
]
+ùõΩ2
(x2 + y2)2 [ ‚àíy
x
]
+ ¬∑ ¬∑ ¬∑
(12.59)
is the normal form for any system having
this 2 √ó 2 matrix for its linear part; all terms
of even degree have been removed, and all
remaining terms of odd degree are sym-
metrical under the group of rotations about
the origin, which is just the group gener-
ated by the linear part of (12.59). Because of
this symmetry, the system is quite simple in
polar coordinates:
Ãár = ùõº1r3 + ùõº2r5 + ¬∑ ¬∑ ¬∑ + ùõºkr2k+1,
(12.60)
ÃáùúÉ= 1 + ùõΩ1r2 + ùõΩ2r4 + ¬∑ ¬∑ ¬∑ + ùõΩkr2k.
This is solvable by quadrature and (even
without integration) the Ô¨Årst nonzero ùõºj
determines the stability of the origin. In
general, when A is semisimple, the system
in normal form always gains enough sym-
metry to reveal certain geometrical struc-
tures called preserved foliations, and fre-
quently is solvable by quadrature. These
solutions have error estimates (due to trun-
cation) similar to those of the method of
averaging, to which the method of normal
forms is closely related.
When A is not semisimple (its Jordan
form has oÔ¨Ä-diagonal ones), the results of
normalization are not so easy to explain or
to use, because the nonlinear terms acquire
a symmetry diÔ¨Äerent from that of the lin-
ear term. Nevertheless, the normal form in
such cases has proven essential to the study
of such problems as the Takens‚ÄìBogdanov
and Hamiltonian Hopf bifurcations. A full
exposition of normal form theory is given in

12.4 Initial and Boundary Layers
429
[7]. Popular expositions covering only the
semisimple case are [8] and [9].
12.3.7
Perturbation of Invariant Manifolds;
Melnikov Functions
With the steadily increasing importance of
nonlinear phenomena such as chaos and
strange attractors, Ô¨Ånding solutions of spe-
ciÔ¨Åc initial value problems often becomes
less important than Ô¨Ånding families (man-
ifolds) of solutions characterized by their
qualitative behavior. Many of these prob-
lems are accessible by means of perturba-
tion theory. We will brieÔ¨Çy describe one
example. If a dynamical system has a rest
point of saddle type, there will exist a sta-
ble manifold and an unstable manifold of
the saddle point; the former consists of all
points that approach the saddle point as
t ‚Üí‚àû, the latter of points approaching the
saddle as t ‚Üí‚àí‚àû. In some cases, the stable
and unstable manifold will coincide; that is,
points that approach the saddle in the dis-
tant future also emerged from it in the dis-
tant past. (The simplest case occurs in the
plane when the stable and unstable man-
ifolds form a Ô¨Ågure-eight pattern with the
saddle at the crossing point.) If such a sys-
tem is perturbed, it is important to decide
whether the stable and unstable manifolds
separate, or continue to intersect; and if
they intersect, whether they are transverse.
(The latter case leads to chaotic motion.)
The criterion that in many cases decides
between these alternatives is based on the
Melnikov function; if this function has sim-
ple zeros, the manifolds will intersect trans-
versely and there will be a chaotic region.
The Melnikov function is an integral over
the homoclinic orbit of the normal compo-
nent of the perturbation; the form of the
integral is determined by applying regular
perturbation methods to the solutions in
the stable and unstable manifolds and mea-
suring the distance between the approxi-
mate solutions. For details see [10].
12.4
Initial and Boundary Layers
The
problems
considered
in
Sections
12.3.2‚Äì12.3.5
are
regular
perturbation
problems when considered on a Ô¨Åxed
interval of time, but become singular when
considered on an expanding interval such
as 0 ‚â§t ‚â§1‚àïùúÄ. We now turn to problems
that are singular even on a Ô¨Åxed interval.
It is not easy to solve these problems even
numerically, because for suÔ¨Éciently small
ùúÄthey are what numerical analysts call
‚ÄústiÔ¨Ä.‚Äù Each of these problems has (in some
coordinate
system)
a
small
parameter
multiplying a (highest-order) derivative.
12.4.1
Multiple-Scale Method for Initial Layer
Problems
As a Ô¨Årst example, we consider initial value
problems of the form
ùúÄÃàu + b(t) Ãáu + c(t)u = 0
(12.61)
u(0) = ùõº
Ãáu(0) = ùõΩ
ùúÄ+ ùõæ.
One may think, for instance, of an object of
small mass ùúÄsubjected to a time-dependent
restoring force and friction; at time zero,
the position and velocity have just reached
ùõºand ùõæwhen the object is subjected to an
impulse-imparting momentum ùõΩ, increas-
ing the velocity by an amount ùõΩ‚àïùúÄ, which
is large because ùúÄis small. We will use
this example to explain two methods that
are applicable to many problems in which

430
12 Perturbation Methods
a small parameter multiplies the highest
derivative.
In approaching any perturbation prob-
lem, one Ô¨Årst tries to understand the case
ùúÄ= 0, but here it does not make sense
to set ùúÄ= 0. On one hand, the diÔ¨Äeren-
tial equation drops from second order to
Ô¨Årst, and can no longer accept two initial
conditions; on the other hand, the second
initial condition becomes inÔ¨Ånite. Progress
can be made, however, by introducing the
‚Äústretched‚Äù time variable
ùúè= t
ùúÄ.
(12.62)
Upon substituting (12.62) into (12.61) and
writing ‚Ä≤ = d‚àïdùúè, we obtain
u‚Ä≤‚Ä≤ + b(ùúÄùúè)u‚Ä≤ + ùúÄc(ùúÄùúè)u = 0
(12.63)
u(0) = ùõº
u‚Ä≤(0) = ùõΩ+ ùúÄùõæ.
This problem is regular (for a Ô¨Åxed interval
of ùúè) and can be solved readily. For a Ô¨Årst
approximation, it suÔ¨Éces to set ùúÄ= 0 in
(12.63) to obtain u‚Ä≤‚Ä≤ + b0u‚Ä≤ = 0 with b0 =
b(0); the solution is
ui
0 = ‚àíùõΩ
b0
e‚àíb0ùúè+ ùõº+ ùõΩ
b0
,
(12.64)
called
the
Ô¨Årst-order
inner
solution.
(Higher-order
approximations
can
be
found by substituting a perturbation series
ui
0 + ùúÄui
1 + ¬∑ ¬∑ ¬∑ into (12.63).) The name
‚Äúinner solution‚Äù comes from the fact that
(12.64) is only uniformly valid on an inter-
val such as 0 ‚â§ùúè‚â§1, which translates
into 0 ‚â§t ‚â§ùúÄin the original time variable;
this is a narrow ‚Äúinner region‚Äù close to the
initial conditions. It is necessary somehow
to extend this to a solution valid for a Ô¨Åxed
interval of t. This is, of course, equivalent
to an expanding interval of ùúè, and one
might attempt to solve (12.63) on such an
expanding interval by previously discussed
methods. The equation cannot be put in
a form suitable for averaging. However,
the method of multiple scales is Ô¨Çexible
enough to be adapted to this situation. One
takes as time scales ùúèand t, and seeks a
solution in the form
u ‚âÖ{ui
0(ùúè) + ucor
0 (t)}
+ùúÄ{ui
1(ùúè) + ucor
1 (t)} + ¬∑ ¬∑ ¬∑ .
(12.65)
(We could have taken u0(ùúè, t) + ùúÄu1(ùúè, t) +
¬∑ ¬∑ ¬∑, but the solution turns out to be the sum
of the previously calculated ui and a ‚Äúcor-
rection‚Äù ucor, so it is convenient to postulate
this form initially.) One can diÔ¨Äerentiate
(12.65) with respect to ùúèusing (12.62) and
substitute it into (12.63), or equivalently
diÔ¨Äerentiate with respect to t and substi-
tute into (12.61). Assuming that ucor(0) =
0 (because the inner part ui should suf-
Ô¨Åce initially), one Ô¨Ånds that ui must satisfy
(12.63) as expected, and that ucor satisÔ¨Åes
a Ô¨Årst-order diÔ¨Äerential equation together
with the assumed initial condition ucor(0) =
0; thus ucor is fully determined. At the Ô¨Årst
order, ucor
0
in fact satisÔ¨Åes the diÔ¨Äerential
equation obtained from (12.61) by setting
ùúÄ= 0; this is the very equation that we ini-
tially discarded as unlikely to be meaning-
ful. Upon solving this equation (with zero
initial conditions) and adding the result to
ui
0, we obtain the composite solution
uc
0 = ui
0 + ucor
0
= ‚àíùõΩ
b0
e‚àíb0t‚àïùúÄ
+
(
ùõº+ ùõΩ
b0
)
exp
[
‚àí‚à´
t
0
c(s)
b(s) ds
]
.(12.66)
This solution is uniformly valid on any Ô¨Åxed
interval of t.

12.4 Initial and Boundary Layers
431
12.4.2
Matching for Initial Layer Problems
Although the method of multiple scales is
successful for problems of this type, it is not
used as widely as the method of matched
asymptotic expansions, probably because
multiple scales requires that the choices
of gauges and scales be made in advance,
whereas matching allows for the discovery
of the correct gauges and scales as one pro-
ceeds. (Recall that gauges are the functions
ùõøi(ùúÄ), usually just powers ùúÄi, that multiply
successive terms of a perturbation series;
scales are the stretched time or space vari-
ables used.) To apply the matching method
to (12.61), begin with the Ô¨Årst-order inner
solution (12.64) that is valid near the ori-
gin. Assume that at some distance from the
origin, a good Ô¨Årst approximation should
be given by setting ùúÄ= 0 in (12.61) and
discarding the initial conditions (which we
have already seen do not make sense with
ùúÄ= 0); the result is
uo
0 = A exp
[
‚àí‚à´
t
0
c(s)
b(s) ds
]
,
(12.67)
called the Ô¨Årst-order outer solution. As
we have discarded the initial conditions,
the quantity A in (12.67) remains undeter-
mined at this point. Now one compares the
inner solution (12.64) with the outer solu-
tion (12.67) in an eÔ¨Äort to determine the
correct value of A so that these solutions
‚Äúmatch.‚Äù In the present instance, the inner
solution decays rapidly (assuming b0 > 0)
to ùõº+ ùõΩ‚àïb0, while the outer solution has
A as its initial value (at t = 0). One might
try to determine where the ‚Äúinitial layer‚Äù
ends, and choose A so that ui
0 and uo
0 agree
at that point; but in fact it is suÔ¨Écient to
set A = ùõº+ ùõΩ‚àïb0 on the assumption that
the inner solution reaches this value at a
point close enough to t = 0 to allow taking
it as the initial condition for the outer
solution. Finally, we note that adding the
inner and outer solutions would duplicate
the quantity ùõº+ ùõΩ‚àïb0 with which one ends
and the other begins, so we subtract this
‚Äúcommon part‚Äù uio of the inner and outer
solutions to obtain the composite solution
uc = ui + uo ‚àíuio,
(12.68)
which is equal to the result (12.66) obtained
by multiple scales.
In the last paragraph, we have cobbled
together the inner and outer solution in a
very ad hoc manner. In fact, several sys-
tematic procedures exist for carrying out
the matching of ui and uo to any order
and extracting the common part uio. The
most common procedure consists of what
are sometimes called the van Dyke match-
ing rules, details of which will be given
below. Although this procedure is simple
to use, it does not always lead to correct
results, in particular, when it is necessary
to use logarithmic gauges. The other meth-
ods, matching in an intermediate variable
and matching in an overlap domain, are
too lengthy to explain here (see [11]), but
give better results in diÔ¨Écult cases. None
of these methods has a rigorous justiÔ¨Åca-
tion as a method, although it is often pos-
sible to justify the results for a particu-
lar problem or class of problems. Occa-
sionally, one encounters problems in which
the inner and outer solutions cannot be
matched. These cases sometimes require a
‚Äútriple deck,‚Äù that is, a third (or even fourth)
layer time scale. In other cases, there does
not exist a computable asymptotic approx-
imation to the exact solution.
To explain the van Dyke matching rules,
we will Ô¨Årst assume that the inner and
outer solutions ui(ùúè, ùúÄ) and uo(t, ùúÄ) have
been computed to some order ùúÄk. In the
problem we have been studying, the outer

432
12 Perturbation Methods
solution contains undetermined constants
whose value must be determined, and the
inner solution contains none, but in more
general problems to be considered below
there may be undetermined constants in
both. It is important to understand that
the inner and outer solutions are naturally
computed in such a way that ui is ‚Äúexpanded
in powers of ùúÄwith ùúèÔ¨Åxed‚Äù while uo is
‚Äúexpanded in powers of ùúÄwith t Ô¨Åxed.‚Äù We
are about to re-expand each solution with
the opposite variable Ô¨Åxed. The Ô¨Årst step
is to express ui in the outer variable t by
setting ùúè= ùúÄt. The resulting function of t
and ùúÄis then expanded in powers of ùúÄto
order ùúÄk, holding t Ô¨Åxed. This new expan-
sion is called uio, the outer expansion of
the inner solution. Notice that in comput-
ing uio we retain only the terms of degree
‚â§k, so that in eÔ¨Äect part of ui is discarded
because it moves up to order higher than
k; the meaning of this is that the discarded
terms of ui are insigniÔ¨Åcant, at the desired
order of approximation, in the outer region.
The second step is to express uo in the inner
variable ùúèby setting t = ùúè‚àïùúÄand expand
the resulting function of ùúèand ùúÄin pow-
ers of ùúÄto order k holding ùúèconstant. The
result, called uoi or the inner expansion of
the outer solution, contains those parts of
uo that are signiÔ¨Åcant in the inner region (to
order k), arranged according to their signiÔ¨Å-
cance in the inner region. The third step is to
set uio = uoi and use this equation to deter-
mine the unknown constants. The ratio-
nale for this is that if the domains of valid-
ity of the inner and outer regions overlap,
then, because the inner solution is valid in
the overlap, but the overlap belongs to the
outer region, uio, which is the inner solu-
tion stripped of the part that is insigniÔ¨Åcant
in the outer region, should be valid there;
similarly, because the outer solution is valid
in the overlap, but the overlap belongs to
the inner region, uoi should be valid there.
Now in setting uio = uoi, it is not possi-
ble to carry out the necessary computations
unless both are expressed in the same vari-
able, so it is necessary to choose either t
or ùúèand express both sides in that vari-
able before attempting to determine the
unknown constants. The fourth step is to
compute the composite solution uc = ui +
uo ‚àíuio. At this stage, uio (which is equal
to uoi) is known as the common part of ui
and uo; it is subtracted because otherwise it
would be represented twice in the solution.
12.4.3
Slow‚ÄìFast Systems
The systems considered above, and many
others, can be put into the form
Ãáx = f(x, y, ùúÄ)
(12.69)
ùúÄÃáy = g(x, y, ùúÄ)
with x ‚àà‚Ñùn and y ‚àà‚Ñùm, which is called a
slow‚Äìfast system. When ùúÄ= 0, the second
equation changes drastically, ceasing to be
diÔ¨Äerential; the motion is conÔ¨Åned to the
set g(x, y) = 0, called the slow manifold.
We now assume for simplicity that
n = m = 1, so that the vectors x and y
become scalars x and y, and the slow man-
ifold becomes the slow curve. For ùúÄ‚â†0,
the entire (x, y) plane is available, but
(assuming ùúïg‚àïùúïy < 0, in which case we say
the slow curve is stable) any point moves
rapidly toward the slow curve and then
slowly ‚Äúalong‚Äù it (close to, but not actually
on it). These two stages of the motion
can be approximated separately as inner
and outer solutions and then matched. To
obtain the inner solution (the rapid part)
one rescales time by setting t = ùúÄùúèand
obtains (with ‚Ä≤ = d‚àïdùúè)
x‚Ä≤ = ùúÄf (x, y, ùúÄ)
(12.70)
y‚Ä≤ = g(x, y, ùúÄ),

12.4 Initial and Boundary Layers
433
in which ùúÄno longer multiplies a derivative.
This problem is regular (Section 12.3.1) on
Ô¨Ånite intervals of ùúè, which are short inter-
vals of t. For details of the matching see [4,
Section 7.7] and [12, Chapters 6 and 7].
An interesting case arises when the
slow curve is S-shaped, with the upper
and lower branches stable and the middle
section (the doubled-over part) unstable. A
point can move along a stable branch until
it reaches a vertical tangent point, then ‚Äúfall
oÔ¨Ä‚Äù and move rapidly to the other stable
branch, then move along that branch to the
other vertical tangent point and ‚Äúfall oÔ¨Ä‚Äù
the other way, leading to a cyclic motion
called a relaxation oscillation. In a further,
very unusual scenario, the point may actu-
ally turn the corner at the vertical tangent
point and follow the unstable branch for
some distance before ‚Äúfalling.‚Äù This rather
recently discovered phenomenon is called
a canard. The explanation of canards is
that several time scales become involved;
the solution is actually ‚Äúfalling‚Äù away from
the unstable branch all the time, but doing
so at a rate that is slow even compared to
the already slow motion along the branch.
For relaxation oscillations and canards,
see [13].
Recently, an approach to slow‚Äìfast sys-
tems (in any number of dimensions) called
geometric
singular
perturbation
theory
has come into prominence. Initiated by
Fenichel, the idea is to prove that (12.69) for
ùúÄnear zero has an actual invariant manifold
close to the slow manifold deÔ¨Åned above,
and that solutions near this manifold are
(in the stable case) asymptotic to solutions
in the manifold, with asymptotic phase.
The emphasis is on a clear geometric
description of the motion rather than on
computation, but computational aspects
are included. A good introduction is [14].
12.4.4
Boundary Layer Problems
Problems in which a small parameter multi-
plies the highest derivative are encountered
among boundary value problems at least
as frequently as among initial value prob-
lems. As the basic ideas have been covered
in the previous sections, it is only necessary
to point out the diÔ¨Äerences that arise in the
boundary value case. Either the multiple-
scale or matching methods may be used;
we will use matching. The method will be
illustrated here with an ordinary diÔ¨Äeren-
tial equation; a partial diÔ¨Äerential equation
will be treated in Section 12.4.5.
Consider the problem
ùúÄy‚Ä≤‚Ä≤ + b(x)y‚Ä≤ + c(x)y = 0
(12.71)
y(0) = ùõº
y(1) = ùõΩ
on the interval 0 ‚â§x ‚â§1. The diÔ¨Äerential
equation here is the same as (12.61), only
the independent variable is a space vari-
able rather than time in view of the usual
applications. If b(x) is positive through-
out 0 < x < 1, there will be a boundary
layer at the left end point x = 0; if neg-
ative, the boundary layer will be at the
right end point; and if b(x) changes sign,
there may be internal transition layers as
well. We will consider the Ô¨Årst case. To the
Ô¨Årst order, the outer solution yo will satisfy
the Ô¨Årst-order equation b(x)y‚Ä≤ + c(x)y = 0
obtained by setting ùúÄ= 0 in (12.71); it will
also satisfy the right-hand boundary condi-
tion y(1) = ùõΩ. Therefore, the outer solution
is completely determined. The Ô¨Årst-order
inner solution yi will satisfy the equation
d2y‚àïdùúâ2 + b0y = 0 with b0 = b(0), obtained
by substituting the stretched variable ùúâ=
x‚àïùúÄinto (12.71) and setting ùúÄ= 0; it will
also satisfy the left-hand boundary con-
dition y(0) = ùõº. As this is a second-order

434
12 Perturbation Methods
equation with only one boundary condi-
tion, it will contain an undetermined con-
stant that must be identiÔ¨Åed by matching
the inner and outer solutions. The diÔ¨Äer-
ential equations satisÔ¨Åed by the inner and
outer solutions are the same as in the case
of (12.61), the only diÔ¨Äerence being that
this time the constant that must be Ô¨Åxed
by matching belongs to the inner solution
rather than the outer.
12.4.5
WKB Method
There are a great variety of problems that
are more degenerate than the one we have
just discussed, which can exhibit a wide
range of exotic behaviors. These include
internal layers, in which a stretched vari-
able such as ùúâ= (x ‚àíx0)‚àïùúÄmust be intro-
duced around a point x0 in the interior
of the domain; triple decks, in which two
stretched variables such as x‚àïùúÄand x‚àïùúÄ2
must be introduced at one end; and prob-
lems in which the order of the diÔ¨Äerential
equation drops by more than one. The sim-
plest example of the latter type is
ùúÄ2y‚Ä≤‚Ä≤ + a(x)y = 0.
(12.72)
This problem is usually addressed by a tech-
nique called the WKB or WKBJ method.
This method is rather diÔ¨Äerent in spirit
from the others we have discussed, because
it depends heavily on the linearity of the
perturbed problem. Rather than pose ini-
tial or boundary value problems, one Ô¨Ånds
approximations for two linearly indepen-
dent solutions of the linear equation (12.72)
on the whole real line. The general solution
then consists of the linear combinations of
these two. If a(x) = k2(x) > 0, these approx-
imate solutions are
y(1) ‚âÖ
1
‚àö
k(x)
cos 1
ùúÄ‚à´k(x) dx
(12.73)
and
y(2) ‚âÖ
1
‚àö
k(x)
sin 1
ùúÄ‚à´k(x) dx.
(12.74)
If a(x) = ‚àík2(x) < 0, the two solutions are
y(1),(2) ‚âÖ
1
‚àö
k(x)
exp 1
ùúÄ‚à´¬±k(x) dx. (12.75)
A recent derivation of (12.75) is given in
Section 12.5; for classical approaches, see
[15] and [4].
If a(x) changes sign, one has a diÔ¨Écult
situation called a turning point problem.
This can be addressed in various ways by
matching solutions of these two types or by
using Airy functions. The latter are solu-
tions of the diÔ¨Äerential equation y‚Ä≤‚Ä≤ + xy =
0, which is the simplest problem with a
turning point at the origin. These Airy func-
tions can be considered as known (they
can be expressed using Bessel functions of
order 1/3) and solutions to more general
turning point problems can be expressed in
terms of them. For an introduction to turn-
ing point problems, see [16, Chapter 2], and
for theory see [17, Chapter 8].
12.4.6
Fluid Flow
We will conclude this section with a brief
discussion of the problem of Ô¨Çuid Ô¨Çow past
a Ô¨Çat plate, because of its historical impor-
tance (see Section 12.1) and because it illus-
trates two aspects of perturbation theory
that we have avoided so far: the use of
perturbation theory for partial diÔ¨Äerential
equations and the need to combine unde-
termined scales with undetermined gauges.
The classic reference for this material is
[18]. Consider a plane Ô¨Çuid Ô¨Çow in the
upper half-plane, with a ‚ÄúÔ¨Çat plate‚Äù occu-
pying the interval 0 ‚â§x ‚â§1 on the x-axis;
that is, the Ô¨Çuid will adhere to this interval,

12.5 The ‚ÄúRenormalization Group‚Äù Method
435
but not to the rest of the x-axis. The stream
function ùúì(x, y) of such a Ô¨Çuid will satisfy
ùúÄ(ùúìxxxx + 2ùúìxxyy + ùúìyyyy) ‚àíùúìy(ùúìxxx + ùúìxyy)
+ùúìx(ùúìxxy + ùúìyyy) = 0
(12.76)
with
ùúì(x, 0) = 0
for
‚àí‚àû< x < ‚àû,
ùúìy(x, 0) = 0 for 0 ‚â§x ‚â§1, and ùúì(x, y) ‚Üíy
as
x2 + y2 ‚Üí‚àû.
The
latter
condition
describes the Ô¨Çow away from the plate, and
this in fact gives the leading order outer
solution as
ùúìo(x, y) = y.
(12.77)
To Ô¨Ånd an inner solution, we stretch y by an
undetermined scale factor,
ùúÇ=
y
ùúá(ùúÄ),
(12.78)
and expand the inner solution using unde-
termined gauges, giving (to Ô¨Årst order)
ùúìi = ùõø(ùúÄ)Œ®(x, ùúÇ).
(12.79)
Substituting this into (12.76) and discard-
ing terms that are clearly of higher order
yields
ùúÄ
ùúáŒ®ùúÇùúÇùúÇùúÇ+ ùõø[Œ®xŒ®ùúÇùúÇùúÇ‚àíŒ®ùúÇŒ®xùúÇùúÇ
] = 0.
(12.80)
The relative signiÔ¨Åcance of ùúÄ‚àïùúáand ùõøhas
not yet been determined, but if either of
them were dominant, the other term would
drop out of (12.80) to Ô¨Årst order, and the
resulting solution would be too simple to
capture the behavior of the problem. So we
must set
ùúÄ
ùúá= ùõø
(12.81)
and conclude that the Ô¨Årst-order inner solu-
tion satisÔ¨Åes
Œ®ùúÇùúÇùúÇùúÇ+ Œ®xŒ®ùúÇùúÇùúÇ‚àíŒ®ùúÇŒ®xùúÇùúÇ= 0.
(12.82)
It is not possible to solve (12.82) in closed
form, but it is possible to express the solu-
tion as
Œ®(x, ùúÇ) =
‚àö
2xf
(
ùúÇ
‚àö
2x
)
,
(12.83)
where f is the solution of the ordinary dif-
ferential equation f ‚Ä≤‚Ä≤‚Ä≤ + Ô¨Ä‚Ä≤‚Ä≤ = 0 with f (0) =
0, f ‚Ä≤(0) = 0, and f ‚Ä≤(‚àû) = 1. In attempting to
match the inner and outer solutions, it is
discovered that this is only possible if ùõø= ùúá.
Together with (12.81), this Ô¨Ånally Ô¨Åxes the
undetermined scales and gauges as
ùõø= ùúá=
‚àö
ùúÄ.
(12.84)
Upon attempting to continue the solution
to higher orders, obstacles are encountered
that can only be overcome by introducing
triple decks and other innovations. See [19]
and [20].
12.5
The ‚ÄúRenormalization Group‚Äù Method
For many years, applied mathematicians
have wished for a uniÔ¨Åed treatment of the
types of perturbation problems studied in
Sections 12.3 and 12.4. Over the past 20
years, some progress has been made in
this direction under the name renormal-
ization group or RG method. This mathe-
matical RG method has its roots in an ear-
lier physical RG method (in quantum Ô¨Åeld
theory and statistical mechanics) that we
do not discuss here; see [21] for the rela-
tions between these. The RG method pro-
vides a single heuristic that works for most
types of perturbation problems in ordinary
and partial diÔ¨Äerential equations. While
the common perturbation methods aban-
don the naive (or straightforward) expan-
sion when it only has pointwise validity, the

436
12 Perturbation Methods
RG method is built on the idea that the
naive solution contains all the information
necessary to construct a uniformly ordered
formal solution; this information is merely
arranged incorrectly. The RG method does
not (at least so far) automatically prove that
the uniformly ordered solution is uniformly
valid (on an intended domain). Therefore
proofs of validity have not been uniÔ¨Åed, but
are still distinct for diÔ¨Äerent types of prob-
lems. The name ‚ÄúRG method‚Äù is somewhat
unfortunate, because no group is involved,
but it is Ô¨Årmly established.
The RG method introduces two new
operations into perturbation theory, the
absorption process and the envelope process.
There are three forms of the RG method,
which we classify as follows.
1. The mixed method, which uses both the
absorption and envelope processes, was
introduced by Chen et al. [22] on the
basis of the RG method in physics, and
later simpliÔ¨Åed by Ziane [23] and Lee
DeVille et al. [24].
2. The pure envelope method, which uses
only the envelope process, was
introduced by WoodruÔ¨Ä[25] under the
name invariance method, before the
(mathematical) RG method (and with
no reference to the physical one). It was
rediscovered by Paquette [26] as an
improvement on the mixed method,
because it can handle problems in
which absorption does not work.
Nevertheless, it has not become well
known.
3. The pure absorption method, which
avoids the envelope process, was
introduced by Kirkinis [27] and
popularized in [28].
The envelope process was referred to by
other names (RG equation, invariance con-
dition) until it was identiÔ¨Åed by Kunihiro
[29, 30] as equivalent to the classical notion
of an envelope of a family of curves.
Our presentation is limited to ordinary
diÔ¨Äerential equations, and (mostly) to lead-
ing order approximations. For important
applications to partial diÔ¨Äerential equations
in Ô¨Çuid mechanics, see [31].
12.5.1
Initial and Boundary Layer Problems
We begin with the simplest initial layer
problem
ùúÄÃàu + Ãáu + u = 0
(12.85)
u(0) = ùõº
Ãáu(0) = ùõΩ
ùúÄ+ ùõæ,
of the form (12.61). We immediately con-
vert this to
u‚Ä≤‚Ä≤ + u‚Ä≤ + ùúÄu = 0
(12.86)
u(0) = ùõº
u‚Ä≤(0) = ùõΩ+ ùúÄùõæ,
which is of the form (12.63). Recall that Ãá =
d‚àïdt, ‚Ä≤ = d‚àïdùúè, and ùúè= t‚àïùúÄis the fast vari-
able; it is a general rule in the RG method
to work in the fastest natural independent
variable. Another feature of all RG meth-
ods is to set the initial (or boundary) con-
ditions aside temporarily and instead seek a
general solution with integration constants.
For methods that use absorption (methods
1 and 3), this is done as follows. Putting
u = u0 + ùúÄu1 + ¬∑ ¬∑ ¬∑, the naive perturbation
method gives
u‚Ä≤‚Ä≤
0 + u‚Ä≤
0 = 0
(12.87)
u‚Ä≤‚Ä≤
1 + u1 = ‚àíu0

12.5 The ‚ÄúRenormalization Group‚Äù Method
437
so that
u0 = a + be‚àíùúè
(12.88)
u1 = ‚àíaùúè+ bùúèe‚àíùúè+ c + de‚àíùúè,
where a, b, c, and d are arbitrary. Here
‚àíaùúè+ bùúèe‚àít is a particular solution for
u1, while c + de‚àíùúèis the general solution
of the associated homogeneous problem
u‚Ä≤‚Ä≤
1 + u1 = 0. It is convenient to delete
this ‚Äúhomogeneous part‚Äù from u1 (and
all higher ui) by allowing a and b to be
functions of ùúÄ, with
a = a(ùúÄ) = a0 + ùúÄa1 + ¬∑ ¬∑ ¬∑ ,
b = b(ùúÄ) = b0 + ùúÄb1 + ¬∑ ¬∑ ¬∑ ;
(12.89)
thus a1 and b1 replace c and d. Then a
straightforward (or naive) approximation
to the general solution is
ÃÉu(ùúè, ùúÄ) = a + be‚àíùúè+ ùúÄ(‚àíaùúè+ bùúèe‚àíùúè).
(12.90)
By
regular
perturbation
theory,
this
approximation is uniformly valid with
error Óàª(ùúÄ2) for 0 ‚â§ùúè‚â§1, that is, in the ini-
tial layer 0 ‚â§t ‚â§ùúÄ. But it is not uniformly
ordered on 0 ‚â§ùúè‚â§1‚àïùúÄ(0 ‚â§t ‚â§1), so it
cannot be uniformly valid there. (The term
‚àíùúÄaùúèis secular, becoming unbounded as
ùúè‚Üí‚àû; for ùúè= 1‚àïùúÄ, this term is formally
Óàª(ùúÄ) but actually Óàª(1). The term ùúÄbùúèe‚àíùúè
is bounded, achieving its maximum at
ùúè= 1, so it is not truly secular, but is often
classiÔ¨Åed as secular anyway because it is
‚Äúsecular relative to be‚àíùúè‚Äù).
Continuing
by
the
mixed
method
(method 1), let ùúè0 > 0 be arbitrary, and
split the secular terms of (12.90) as follows:
ÃÉu(ùúè, ùúè0, ùúÄ) = a + be‚àíùúè+ ùúÄ[‚àíaùúè0 + bùúè0e‚àíùúè
‚àía(ùúè‚àíùúè0) + b(ùúè‚àíùúè0)e‚àíùúè].
(12.91)
Now the terms in ùúè‚àíùúè0 are secular, while
those in ùúè0 alone are not (because ùúè0 is
considered constant). Next we perform an
absorption operation by setting
ar(ùúè0, ùúÄ) = a ‚àíùúÄaùúè0 + Óàª(ùúÄ2),
br(ùúè0, ùúÄ) = b + ùúÄbùúè0 + Óàª(ùúÄ2),
(12.92)
remembering that (12.89) is still in eÔ¨Äect,
and rewriting (12.91) as
ÃÉu(ùúè, ùúè0, ùúÄ) = ar + bre‚àíùúè+ ùúÄ[‚àíar(ùúè‚àíùúè0)ùúè0
+br(ùúè‚àíùúè0)e‚àíùúè].
(12.93)
This is called ‚Äúabsorbing the secular terms
into the integration constants a and b to
produce renormalized constants ar and br.‚Äù
(‚ÄúConstant‚Äù means independent of ùúè. Later
these constants will become slowly vary-
ing functions of ùúè.) This produces a fam-
ily of naive solutions of (12.86), one naive
solution for each ùúè0 (ignoring initial condi-
tions). Each of the solutions in this family
is pointwise asymptotically valid with error
Óàª(ùúÄ2) for all t, but is uniformly valid only on
a bounded interval around ùúè0; we say that
ÃÉu(t, ùúè0, ùúÄ) is a family of locally valid approx-
imate solutions.
At this point, we stop to observe that
(12.93) could have been obtained at the
start, in place of (12.90). Instead of omitting
the ‚Äúhomogeneous part‚Äù of u1, we could
have included any ‚Äúhomogeneous part‚Äù that
we like in u1 (satisfying u‚Ä≤‚Ä≤
1 + u‚Ä≤
1 = 0), for
instance, aùúè0 ‚àíbùúè0e‚àíùúè, giving u1 = ‚àía(ùúè‚àí
ùúè0) + b(ùúè‚àíùúè0)e‚àíùúè, which immediately pro-
duces
ÃÉu(ùúè, ùúè0, ùúÄ) = a + be‚àíùúè+ ùúÄ(‚àía(ùúè‚àíùúè0)ùúè0
+b(ùúè‚àíùúè0)e‚àíùúè).
(12.94)
This is the same as (12.93) except that
there are no ‚Äúrenormalized variables‚Äù and
no (12.92). (This equation would never be

438
12 Perturbation Methods
used again anyway.) The idea of this ‚Äúpure
envelope method‚Äù (method 2) is to select
the homogeneous part of each ui so that the
secular terms vanish at ùúè0. Note that a and
b in (12.90) are functions of ùúÄby (12.89) and
of ùúè0 because they can be chosen indepen-
dently for each choice of ùúè0.
From this point on, methods 1 and 2
coincide; we use the notation of method 2.
The goal is to perform a kind of asymp-
totic matching of the (continuumly many)
local solutions (12.94) to produce a single,
smoothed-out solution that is as good as
each local solution where that local solu-
tion is good. The procedure to achieve this
goal is to diÔ¨Äerentiate ÃÉu with respect to
ùúè0, set ùúè0 = ùúè, and set the result equal to
zero. The justiÔ¨Åcation for this procedure
varies from author to author; the best are
due to Kunihiro and Paquette. Kunihiro
shows that this procedure amounts to tak-
ing the envelope of the local solutions in
such a way that the envelope is tangent
to each local solution at the point where
ùúè= ùúè0, that is, the point where that local
solution is best. Paquette shows that the
procedure amounts to choosing the ùúè0-
dependent coeÔ¨Écients in (12.94) in such a
way that two local solutions with nearby
values of ùúè0 have an overlap domain in
which both are equally valid asymptoti-
cally. (WoodruÔ¨Ä‚Äôs approach is equivalent,
but more complicated. Chen et. al. claimed
vaguely that ar and br in (12.93) should be
chosen so that ÃÉu does not depend on ùúè0,
because ùúè0 was introduced artiÔ¨Åcially into
the problem and was not part of the original
data. Thus the derivative with respect to ùúè0
should vanish, and, again because ÃÉu should
not depend on ùúè0, we should be free to set
ùúè0 = ùúè. This argument was quickly recog-
nized as inadequate, because the derivative
does not vanish in general, but only when
ùúè0 = ùúè.)
Now we carry out the envelope proce-
dure on the example at hand. DiÔ¨Äerentiat-
ing (12.94) gives
ùúïÃÉu
ùúïùúè0
= ùúïa
ùúïùúè0
+ ùúïb
ùúïùúè0
e‚àíùúè+ ùúÄ
(
‚àíùúïa
ùúïùúè0
(ùúè‚àíùúè0) + a
+ ùúïb
ùúïùúè0
(ùúè‚àíùúè0)e‚àíùúè‚àíbe‚àíùúè
)
.
(12.95)
Setting ùúè= ùúè0 and equating to zero gives
(ùúïa
ùúïùúè+ ùúÄa
)
+
(
ùúïb
ùúïùúè‚àíùúÄb
)
e‚àíùúè= 0. (12.96)
(Notice how setting ùúè0 = ùúèautomatically
makes a and b into functions of ùúèwhen
previously they were ‚Äúconstants‚Äù depending
only on ùúè0 and ùúÄ). The only plausible way to
solve (12.96) is to solve
ùúïa
ùúïùúè+ ùúÄa = 0,
ùúïb
ùúïùúè‚àíùúÄb = 0
(12.97)
separately, giving
a(ùúè, ùúÄ) = a(ùúÄ)e‚àíùúÄùúè,
b(ùúè, ùúÄ) = b(ùúÄ)eùúÄùúè.
(12.98)
(Separating (12.96) into (12.97) can be
avoided by creating two families of solu-
tions, ÃÉu(1) with b = 0 and ÃÉu(2) with a = 0,
and applying the envelope process to each
family separately, but this does not seem
to have been noticed in the literature.)
Note that a and b turn out to depend on
ùúèonly through the combination ùúÄùúè, and
are therefore slowly varying; this shows
how the RG method is able to generate the
required time scales without postulating
them in advance, as must be done in the
multiple-scale method. Now we insert
(12.98) into (12.94) with ùúè0 = ùúèto obtain
ÃÉu(ùúè, ùúÄ) = a(ùúÄ)e‚àíùúÄùúè+ b(ùúÄ)eùúÄùúèe‚àíùúè.
(12.99)
Writing a(ùúÄ) = a0 + ùúÄa1 + ¬∑ ¬∑ ¬∑, b(ùúÄ) = b0 +
ùúÄb1 + ¬∑ ¬∑ ¬∑, with undetermined coeÔ¨Écients,

12.5 The ‚ÄúRenormalization Group‚Äù Method
439
we can now seek to satisfy the initial con-
ditions in (12.86), Ô¨Ånding for instance that
a0 = ùõº+ ùõΩand b0 = ùõΩ, so that the leading
order solution is
ÃÉu = (ùõº+ ùõΩ)e‚àíùúè‚àíùõΩe(‚àí1+ùúÄ)ùúè.
(12.100)
It
is
easy
to
check
(rigorously)
that
e(‚àí1+ùúÄ)ùúè= e‚àíùúè+ Óàª(ùúÄ)
uniformly
for
0 ‚â§ùúè< 0; if this replacement is made,
(12.100) coincides with the approximation
given by (12.66). As expected, ùõædoes not
appear in the leading order. Higher-order
approximations can be carried out as
well, and in this simple example, they
coincide with truncations of the exact
solution (obtained by elementary means
and expanded in ùúÄ).
To solve the same problem by Kirkinis‚Äôs
method 3 (pure absorption), we begin again
from (12.90) as in method 1. This time,
however, we absorb the entire secular (and
relative secular) terms into a and b without
introducing ùúè0, by setting
ar(ùúè, ùúÄ) = a(ùúÄ) ‚àíùúÄa(ùúÄ)ùúè+ ¬∑ ¬∑ ¬∑ ,
br(ùúè, ùúÄ) = b(ùúÄ) + ùúÄb(ùúÄ)ùúè+ ¬∑ ¬∑ ¬∑
(12.101)
and obtaining
ÃÉu(ùúè, ùúÄ) = ar + bre‚àíùúè.
(12.102)
(In this simple problem, all terms of order
ùúÄare secular, so the renormalized form of
ÃÉu looks like the leading term of the original
form. In the problems treated below, there
are usually nonsecular terms remaining at
order ùúÄ.) Then we invert the equations in
(12.101) to obtain
a(ùúÄ) = ar(ùúÄ, ùúè) + ùúÄar(ùúÄ, ùúè),
b(ùúÄ) = br(ùúÄ, ùúè) ‚àíùúÄbr(ùúÄ, ùúè),
(12.103)
omitting terms that are formally of order ùúÄ2.
(In a more complicated problem, equations
(12.101) might be coupled and need to be
inverted as a system. Kirkinis writes the
absorption in the inverted form from the
beginning, with undetermined coeÔ¨Écients,
which he Ô¨Ånds recursively.) Next we diÔ¨Äer-
entiate (12.103) with respect to ùúèand trun-
cate, which gives
0 = dar
dùúè+ ùúÄar,
0 = dbr
dùúè‚àíùúÄbr,
(12.104)
and immediately brings us to (12.97); this
diÔ¨Äerentiation step replaces the enve-
lope process as a way of Ô¨Ånding the ‚ÄúRG
equations.‚Äù The solution proceeds from
here as before.
More general initial and boundary layer
problems, including nonlinear ones, may be
solved the same methods illustrated above
in the simplest case.
12.5.2
Nonlinear Oscillations
Now we consider a broad class of oscilla-
tory problems that are usually solved by the
method of averaging, namely, the class of
systems of the form
Ãáx = Ax + ùúÄf(x),
x(0) = c,
(12.105)
where
x ‚àà‚ÑÇn, A is a diagonal matrix with
eigenvalues ùúÜi on the imaginary axis, and
f(x) is a vector polynomial, expressible as a
Ô¨Ånite sum
f(x) =
‚àë
ùú∂,i
Cùú∂ixùú∂ei,
(12.106)
with ùú∂= (ùõº1, ‚Ä¶ , ùõºn) a nonnegative integer
vector, xùú∂= xùõº1
1 ¬∑ ¬∑ ¬∑ xùõºn
n , and ei the standard
unit vectors in ‚ÑÇn. This class of problems
includes all single and multiple oscillators
with weak polynomial nonlinearities and
weak coupling, when expressed as Ô¨Årst-
order systems in complex variables with

440
12 Perturbation Methods
reality conditions. We will solve this to
leading order by methods 2 and 3, although
our calculations are based on those of
Ziane, who uses method 1. All methods
begin by writing x = x0 + ùúÄx1 + ¬∑ ¬∑ ¬∑ and
obtaining
Ãáx0 = Ax0,
Ãáx1 = Ax1 + ùúÄf(x0). (12.107)
For method 2, we take x0 and x1 to be
x0(t, a) = eAta,
x1(t, t0, a) = eA(t‚àít0)
‚à´
t
t0
e‚àíAsf(eAsa)ds,
(12.108)
where a is an arbitrary vector of integration
constants and x1 is the particular solution
with zero initial conditions at an arbitrary
point t0. (Recall that by letting a depend on
ùúÄ, we may choose only particular solutions
at higher orders, and because x1 vanishes at
t0, so do any secular terms it may contain.)
The integrand in (12.108) has the form
e‚àíAsf(eAsa) =
‚àë
ùú∂,i
Cùú∂ie(‚ü®ùùÄ,ùú∂‚ü©‚àíùúÜi)saùú∂ei
(12.109)
= R(a) + Q(s, a),
(12.110)
where
‚ü®ùùÄ, ùú∂‚ü©= ùúÜ1ùõº1 + ¬∑ ¬∑ ¬∑ ùúÜnùõºn,
aùú∂=
aùõº1
1 ¬∑ ¬∑ ¬∑ aùõºn
n , R contains the terms with
‚ü®ùùÄ, ùú∂‚ü©‚àíùúÜi = 0,
and
Q
the
remaining
terms. It follows by examining the integral
in (12.108) that
x1(t, t0, a) = eA(t‚àít0)[R(a)(t ‚àít0) + S(t, t0, a)],
(12.111)
where S is bounded (nonsecular). Thus
ÃÉx(t, t0, a, ùúÄ) = x0 + ùúÄx1 = eAta + ùúÄeA(t‚àít0)
√ó[R(a)(t ‚àít0) + S(t, t0, a)]
(12.112)
is a family of local approximate solutions,
and we proceed to apply the envelope pro-
cess:
ùúïÃÉx
ùúït0
||||t0=t
= eAt [ùúïa
ùúït ‚àíùúÄR(a)
]
= 0. (12.113)
It follows that
ùúïa
ùúït = ùúÄR(a).
(12.114)
If this equation can be solved for a = a(t, ùúÄ),
the solution is inserted into
ÃÉx(t, a, ùúÄ) = eAta + ùúÄ[S(t, t, a)],
(12.115)
which is obtained from (12.112) by setting
t0 = t.
To compare this with the method of aver-
aging, note that (12.105) is converted into
periodic standard form by setting x = eAtùùÉ
and obtaining
ÃáùùÉ= e‚àíAtf(eAtùùÉ).
(12.116)
This corresponds to (12.37), with ùùÉfor x.
The averaged equation (12.40) then coin-
cides with (12.114), with y for a. Finally,
substituting a into (12.115) corresponds to
Ô¨Årst substituting y into (12.42) to obtain (in
our present notation) ùùÉ, and then multi-
plying that by eAt to get x. Thus the Ô¨Årst-
order RG solution for this class of systems is
exactly the same as the improved Ô¨Årst-order
solution by averaging (rather than the usual
Ô¨Årst-order averaging solution). This illus-
trates the remark made by several authors
that the solution given by the RG method
is sometimes better than the ones given by
more familiar perturbation methods, even
though these are asymptotically equivalent.
To solve the same problem by method
3, write the naive solution of (12.107) to

12.5 The ‚ÄúRenormalization Group‚Äù Method
441
Óàª(ùúÄ) as
ÃÉx(t, a, ùúÄ) = eAt[a + ùúÄ(R(a)t + S(t, a))],
(12.117)
where a depends on ùúÄ. Absorb the entire
secular term into a by writing ar(t, ùúÄ) =
a(ùúÄ) + ùúÄtR(a(ùúÄ)) and then invert this to get
a(ùúÄ) = ar(t, ùúÄ) ‚àíùúÄtR(ar(t, ùúÄ)),
(12.118)
with
ÃÉx(t, ar, ùúÄ) = eAt[ar + ùúÄS(t, a)].
(12.119)
DiÔ¨Äerentiating (12.118) gives
ùúïar
ùúït = ùúÄ
(
I ‚àíùúÄt ùúïR
ùúïar
)‚àí1
√óR(ar) = ùúÄR(ar) + Óàª(ùúÄ2),
(12.120)
in agreement with (12.114) and with the
Ô¨Årst-order averaged equation. As before,
the solution of (12.120) is inserted into
(12.119).
The class of problems we have considered
does not include all problems that can be
put into periodic standard form, so the RG
method does not yet completely encompass
the averaging method, even for the periodic
case.
12.5.3
WKB Problems
Finally, we look brieÔ¨Çy at the WKB problem
Ãày + k2(ùúÄt)y = 0,
(12.121)
which is equivalent to (12.72) with a(x) =
k2(x) and x = ùúÄt. This problem cannot be
solved directly by methods 1 or 3 that use
absorption, but can be solved by method 2,
illustrating Paquette‚Äôs claim that the pure
envelope method is the strongest of the
three forms of RG. (Chen et. al. have given
a very interesting treatment of WKB prob-
lems, including those with turning points,
by method 1, but they begin by chang-
ing variables to achieve a form for which
absorption is possible.) The central idea
is to construct local solutions by form-
ing locally valid diÔ¨Äerential equations and
solving these by ‚Äúnaive‚Äù expansions that
are nevertheless already generalized asymp-
totic expansions (that is, ùúÄappears in the
coeÔ¨Écients, not just the gauges), unlike the
local solutions used in previous examples.
These local solutions are then ‚Äúmatched‚Äù
by the envelope process. This technique
is used by both Woodward and Paquette.
We follow Paquette‚Äôs (simpler) method, but
where he takes only the leading term of ÃÉy
(and so achieves a less accurate solution
than the standard WKB approximation), we
follow Woodward in taking the Ô¨Årst two
terms.
First we expand k2(ùúÄt) in a Taylor series
in t ‚àít0 for arbitrary t0, keeping the Ô¨Årst
two terms. This results in a local equation
approximating (12.121) near t0:
Ãày + [k2(ùúÄt0) + 2ùúÄk(ùúÄt0)k‚Ä≤(ùúÄt0)(t ‚àít0)]y = 0.
(12.122)
An approximate solution of the local
equation is sought in the form y = y0 + ùúÄy1;
we Ô¨Ånd
Ãày0 + k2(ùúÄt0)y0 = 0
(12.123)
Ãày1 + k2(ùúÄt0)y1 = ‚àí2k(ùúÄt0)k‚Ä≤(ùúÄt0)(t ‚àít0)y0.
(Here t0 is a constant.) Two linearly inde-
pendent solutions for y0 are
y(1),(2)
0
(t, t0, ùúÄ) = e¬±ik(ùúÄt0)(t‚àít0).
(12.124)

442
12 Perturbation Methods
Choosing y(1)
0 , the inhomogeneous linear
equation for y(1)
1
is solvable by undeter-
mined coeÔ¨Écients, with solution
y1(t, t0, ùúÄ) =
[ik‚Ä≤(ùúÄt0)
2
(t ‚àít0)2
‚àík‚Ä≤(ùúÄt0)
2k(ùúÄt0)(t ‚àít0)
]
eik(ùúÄt0)(t‚àít0),
(12.125)
and y(2)
1 is similar (but will not be needed).
We create from these the family of local
solutions
ÃÉy = a(t0, ùúÄ)
[
1 + ùúÄ
(ik‚Ä≤(ùúÄt0)
2
(t ‚àít0)2
‚àík‚Ä≤(ùúÄt0)
2k(ùúÄt0)(t ‚àít0)
)]
eik(ùúÄt0)(t‚àít0), (12.126)
where a(t0, ùúÄ) is to be found by the envelope
process: compute ùúïÃÉy‚àïùúït0, put t0 = t, and set
the result equal to zero, obtaining
ùúïa(t, ùúÄ)
ùúït
= a(t, ùúÄ)
[
ik(ùúÄt) ‚àíùúÄk‚Ä≤(ùúÄt)
2k(ùúÄt)
]
.
(12.127)
A nonzero solution of this equation (by
separation of variables) is
a(t, ùúÄ) =
1
‚àö
k(ùúÄt)
exp i ‚à´
t
0
k(ùúÄs)ds. (12.128)
Inserting this into (12.125) with t0 = t leads
(remarkably) to exactly the same expression
for ÃÉy:
ÃÉy(t, ùúÄ) =
1
‚àö
k(ùúÄt)
exp i ‚à´k(ùúÄt)dt. (12.129)
The real and imaginary parts of this solu-
tion are linearly independent, real, approx-
imate solutions of (12.121) that coincide
with (12.73) and (12.74) when expressed in
terms of x.
12.6
Perturbations of Matrices and Spectra
In this section, we address the question, if
A(ùúÄ) = A0 + ùúÄA1 + ¬∑ ¬∑ ¬∑
(12.130)
is a matrix or linear operator depending
on a small parameter, and the spectrum of
A0 is known, can we determine the spec-
trum of A(ùúÄ) for small ùúÄ? For the case of
a matrix, the spectrum is simply the set of
eigenvalues (values of ùúÜfor which Av = ùúÜv
for some nonzero column vector v called
an eigenvector). More generally, the spec-
trum is deÔ¨Åned as the set of ùúÜfor which A ‚àí
ùúÜI is not invertible; for linear transforma-
tions on inÔ¨Ånite-dimensional spaces (such
as Hilbert or Banach spaces), this need not
imply the existence of an eigenvector. Our
attention here will be focused on the matrix
case, but many of the procedures (exclud-
ing those that involve the determinant or
the Jordan normal form) are applicable as
well to any operators whose spectrum con-
sists of eigenvalues. The classical reference
for the general inÔ¨Ånite dimensional case is
[32]. For matrices that are not diagonaliz-
able, one can (and should) ask not only for
the eigenvalues and eigenvectors, but also
generalized eigenvectors v for which (A ‚àí
ùúÜI)kv = 0 for some integer k > 1.
The most direct approach (which we do
not recommend) to Ô¨Ånding the eigenvalues
of (12.130) in the matrix case would be to
examine the characteristic equation
P(ùúÜ, ùúÄ) = det(A(ùúÄ) ‚àíùúÜI) = 0,
(12.131)
having the eigenvalues as roots. There are
standard perturbation methods for Ô¨Ånding
the roots of polynomials (see [4, Chapter
1]), the simplest of which is to substitute
ùúÜ(ùúÄ) = ùúÜ0 + ùúÄùúÜ1 + ¬∑ ¬∑ ¬∑
(12.132)

12.6 Perturbations of Matrices and Spectra
443
into (12.131) and solve recursively for ùúÜi.
This method works if ùúÜ0 is a simple root
of P(ùúÜ, 0) = 0; that is, it will work if A0 has
distinct eigenvalues. If there are repeated
eigenvalues, then in general it is necessary
to replace (12.132) with a fractional power
series involving gauges ùõøi(ùúÄ) = ùúÄi‚àïq for some
integer q that is most readily determined
by using Newton‚Äôs diagram. Although these
are useful perturbation methods for Ô¨Ånding
roots of general polynomials, they have two
drawbacks in the case of eigenvalues: if the
matrices are large, it is diÔ¨Écult to compute
the characteristic polynomial; more impor-
tantly, these methods do not take account of
the special features of eigenvalue problems.
For instance, if A(ùúÄ) is a symmetric matrix,
then its eigenvalues will be real, and frac-
tional powers will not be required (even if
A0 has repeated eigenvalues). But the fact
that A is symmetric is lost in passing to the
characteristic polynomial, and one cannot
take advantage of these facts.
For these reasons, it is best to seek not
only the eigenvalues but also at the same
time the eigenvectors that go with them.
The general procedure (which must be
reÔ¨Åned in particular situations) is to seek
solutions of
A(ùúÄ)v(ùúÄ) = ùúÜ(ùúÄ)v(ùúÄ)
(12.133)
in the form
ùúÜ(ùúÄ) = ùúÜ0 + ùúÄùúÜ1 + ùúÄ2ùúÜ2 + ¬∑ ¬∑ ¬∑
(12.134)
v(ùúÄ) = v0 + ùúÄv1 + ùúÄ2v2 + ¬∑ ¬∑ ¬∑ .
In the Ô¨Årst two orders, the resulting
equations are
(A0 ‚àíùúÜ0I)v0 = 0
(12.135)
(A0 ‚àíùúÜ0I)v1 = (ùúÜ1I ‚àíA1)v0.
We will now discuss how to solve (12.135)
under various circumstances.
The simplest case occurs if A0 is real and
symmetric (or complex and Hermitian),
and also has distinct eigenvalues. In this
case, the Ô¨Årst equation of (12.135) can be
solved simply by choosing an eigenvector v0
for each eigenvalue ùúÜ0 of A0. It is convenient
to normalize v0 to have length one, that is,
(v0, v0) = 1 where (‚ãÖ, ‚ãÖ) is the inner (or ‚Äúdot‚Äù)
product. Now we Ô¨Åx a choice of ùúÜ0 and v0
and insert these into the second equation
of (12.135). The next step is to choose ùúÜ1
so that the right-hand side lies in the image
of A0 ‚àíùúÜ0I; once this is accomplished, it is
possible to solve for v1. To determine ùúÜ1, we
rely upon special properties of the eigen-
vectors of a symmetric matrix; namely, they
are orthogonal (with respect to the inner
product). Thus there exists an orthogonal
basis of eigenvectors in which A0 is diag-
onal; examining A0 ‚àíùúÜ0I in this basis, we
see that its kernel (or null space) is spanned
by v0 and its image (or range) is spanned by
the rest of the eigenvectors. Therefore, the
image is perpendicular to the kernel. It fol-
lows that (ùúÜ1I ‚àíA1)v0 will lie in the image
of A0 ‚àíùúÜ0I if and only if its orthogonal pro-
jection onto v0 is zero, that is, if and only if
(ùúÜ1v0 ‚àíA1v0, v0) = 0, or, using (v0, v0) = 1,
ùúÜ1 = (A1v0, v0).
(12.136)
It is not necessary to Ô¨Ånd v1 unless it is
desired to go on to the next stage and Ô¨Ånd
ùúÜ2. (There is a close similarity between
these steps and those of the Lindstedt
method in Section 3.1, in which each term
in the frequency expansion is determined
to make the next equation solvable.)
If A0 has distinct eigenvalues but is not
symmetric, most of the last paragraph still
applies, but the eigenvectors of A0 need
not be orthogonal. The vector space still
decomposes as a direct sum of the image
and kernel of A0 ‚àíùúÜ0I, but the inner prod-
uct can no longer be used to eÔ¨Äect the

444
12 Perturbation Methods
decomposition; ùúÜ1 can still be determined
but cannot be written in the form (12.136).
If A0 does not have distinct eigenval-
ues, the situation can become quite com-
plicated. First, suppose A(ùúÄ) is symmetric
for all ùúÄ, so that all Ai are symmetric. In
this case, every eigenvalue has a ‚Äúfull set‚Äù
of eigenvectors (as many linearly indepen-
dent eigenvectors as its algebraic multiplic-
ity). However, suppose that A0 has an eigen-
value ùúÜ0 of multiplicity two, with eigen-
vectors z and w. It is likely that for ùúÄ‚â†0,
the eigenvalue ùúÜ0 splits into two distinct
eigenvalues having separate eigenvectors.
In this case, it is not possible to choose
an arbitrary eigenvector v0 from the plane
of z and w to use in the second equation
of (12.135); only the limiting positions (as
ùúÄ‚Üí0) of the two eigenvectors for ùúÄ‚â†0
are suitable candidates for v0. As these are
unknown in advance, one must put v0 =
az + bw (for unknown real a and b) into
(12.135), then Ô¨Ånd two choices of a, b, and
ùúÜ1 that make the second equation solvable.
It also may happen that the degeneracy can-
not be resolved at this stage but must be
carried forward to higher stages before the
eigenvalues split; or, of course, they may
never split.
If A(ùúÄ) is not symmetric, and hence not
necessarily diagonalizable, the possibilities
become even worse. The example
A(ùúÄ) =
[ 1
ùúÄ
0
1
]
(12.137)
shows that a full set of eigenvectors may
exist when ùúÄ= 0 but not for ùúÄ‚â†0; the con-
trary case (diagonalizable for ùúÄ‚â†0 but not
for ùúÄ= 0) is exhibited by
A(ùúÄ) =
[ 1
1
0
1 + ùúÄ
]
.
(12.138)
These examples show that the Jordan nor-
mal form of A(ùúÄ) is not, in general, a con-
tinuous function of ùúÄ.
There is a normal form method, closely
related to that of Section 3.6, that is suc-
cessful in all cases. It consists in simplifying
the terms of (12.130) by applying succes-
sive coordinate transformations of the
form I + ùúÄkSk for k = 1, 2, ‚Ä¶ or a single
coordinate transformation of the form
T(ùúÄ) = I + ùúÄT1 + ùúÄ2T2 + ¬∑ ¬∑ ¬∑; the matrices
Sk or Tk are determined recursively. It is
usually assumed that A0 is in Jordan canon-
ical form, hence is diagonal if possible. If A0
is diagonal and A(ùúÄ) is diagonalizable, the
normalized Ak will be diagonal for k ‚â•1,
so that (12.130) will give the asymptotic
expansion of the eigenvalues and T(ùúÄ) the
asymptotic expansion of all the eigenvalues
(as its columns). In more complicated
cases, the normalized series (12.130) will
belong to a class of matrices called the
Arnol‚Äôd unfolding of A0, and although it
will not always be in Jordan form, it will
be in the simplest form compatible with
smooth dependence on ùúÄ. Still further
simpliÔ¨Åcations (the metanormal form) can
be obtained using fractional powers of ùúÄ.
This theory is described in [7, Chapter 2].
Glossary
Asymptotic approximation: An approxi-
mate solution to a perturbation problem
that increases in accuracy at a known rate
as the perturbation parameter approaches
zero.
Asymptotic series: A series, the partial
sums of which are asymptotic approxi-
mations of some function to successively
higher order.
Averaging: A method of constructing
asymptotic approximations to oscillatory
problems. In the simplest case, it involves

Glossary
445
replacing
periodic
functions
by
their
averages to simplify the equations to be
solved.
Bifurcation: Any change in the number or
qualitative character (such as stability) of
the solutions to an equation as a parameter
is varied.
Boundary layer: A transition layer located
near the boundary of a region where
boundary values are imposed.
Composite solution: A solution uniformly
valid on a certain domain, created by
matching an inner solution and an outer
solution each valid on part of the domain.
Gauge: A monotonic function of a pertur-
bation parameter used to express the order
of a term in an asymptotic series.
Generalized series: An asymptotic series
of the form ‚àëùõøi(ùúÄ)ui(x, ùúÄ) in which the per-
turbation parameter ùúÄappears both in the
gauges and in the coeÔ¨Écients. See Poincar√©
Series.
Initial layer: A transition layer located near
the point at which an initial value is pre-
scribed.
Inner solution: An approximate solution
uniformly valid within a transition layer.
Lie series: A means of representing a near-
identity transformation by a function called
a generator. There are several forms; in
Deprit‚Äôs form, if W(x, ùúÄ) is the generator,
then the solution of dx‚àïdùúÄ= W(x, ùúÄ) with
x(0) = y for small ùúÄis a near-identity trans-
formation of the form x = y + ùúÄu1(y) + ¬∑ ¬∑ ¬∑.
Lindstedt method: A method of approxi-
mating periodic solutions whose frequency
varies with the perturbation parameter by
using a scaled time variable.
Matching: Any of several methods for
choosing the arbitrary constants in an
inner and an outer solution so that they
both approximate the same exact solution.
Multiple scales: The simultaneous use
of two or more variables having the same
physical signiÔ¨Åcance (for instance, time
or distance) but proceeding at diÔ¨Äerent
‚Äúrates‚Äù (in terms of the small parameter),
for instance ‚Äúnormal time‚Äù t and ‚Äúslow
time‚Äù ùúè= ùúÄt. The variables are treated as if
they were independent during part of the
discussion, but at the end are reduced to a
single variable again.
Naive
expansion:
A
Poincar√©
series
obtained
by
the
regular
perturbation
method (straightforward expansion), pro-
posed as a solution to a singular problem,
where a generalized series is required. See
RG method.
Outer solution: An approximate solution
uniformly valid in a region away from a
transition layer.
Overlap domain: A region in which both
an inner and an outer approximation are
valid, and where they can be compared for
purposes of matching.
Perturbation parameter: A parameter,
usually denoted ùúÄ, occurring in a mathe-
matical problem, such that the problem
has a known solution when ùúÄ= 0 and an
approximate solution is sought when ùúÄis
small but nonzero.
Perturbation series: A Ô¨Ånite or inÔ¨Ånite
series obtained as a formal approximate
solution to a perturbation problem, in the
hope that it will be uniformly asymptoti-
cally valid on some domain.
Poincar√© series: An asymptotic series of
the form ‚àëùõøi(ùúÄ)ui(x) in which the per-
turbation parameter ùúÄappears only in the
gauges. See Generalized Series.
Regular perturbation problem: A per-
turbation problem having an approximate
solution in the form of a Poincar√© series that
is uniformly valid on the entire intended
domain.
Relaxation oscillation: A self-sustained
oscillation characterized by a slow buildup
of tension (in a spring, for instance) fol-
lowed by a rapid release or relaxation. The

446
12 Perturbation Methods
rapid phase is an example of a transition
layer.
Rescaled coordinate: A coordinate that
has been obtained from an original vari-
able by a transformation depending on the
perturbation parameter, usually by multi-
plying by a scaling factor. For instance time
t may be rescaled to give a ‚Äúslow time‚Äù ùúÄt
(see multiple scales) or a ‚Äústrained time‚Äù
(ùúî0 + ùúÄùúî1 + ‚Ä¶ )t (see Lindstedt method).
Resonance: In linear problems, an equality
of two frequencies. In nonlinear prob-
lems, any integer relationship holding
between two or more frequencies, of the
form
ùúà1ùúî1 + ¬∑ ¬∑ ¬∑ + ùúàkùúîk = 0,
especially
one involving small integers or one that
produces zero denominators in a Fourier
series.
RG (renormalization group) method: A
method (not involving group theory) that
converts a naive expansion for a singular
problem into a generalized expansion that
may be uniformly valid on the intended
domain.
Self-excited oscillation: An oscillation
about an unstable equilibrium that occurs
because of instability and has its own natu-
ral frequency, rather than an oscillation in
response to an external periodic forcing.
Singular perturbation problem: A pertur-
bation problem that cannot be uniformly
approximated by a Poincar√© series on the
entire intended domain, although this may
be possible over part of the domain. For sin-
gular problems, one seeks a solution in the
form of a generalized series.
Transition layer: A small region in which
the solution of a diÔ¨Äerential equation
changes rapidly and in which some approx-
imate solution (outer solution) that is valid
elsewhere fails.
Triple deck: A problem that exhibits a
transition layer within a transition layer
and that therefore requires the matching
of three approximate solutions rather than
only two.
Unfolding: A family of perturbations of a
given system obtained by adding several
small parameters. An unfolding is universal
if (roughly) it exhibits all possible qualita-
tive behaviors for perturbations of the given
system using the least possible number of
parameters.
Uniform approximation: An approximate
solution whose error is bounded by a con-
stant times a gauge function everywhere on
an intended domain.
References
1. Golubitsky, M. and SchaeÔ¨Äer, D.G. (1985)
Singularities and Groups in Bifurcation
Theory, vol. 1, Springer-Verlag, New York.
2. Iooss, G. and Joseph, D.D. (1980) Elementary
Stability and Bifurcation Theory,
Springer-Verlag, New York.
3. Sanders, J.A., Verhulst, F., and Murdock, J.
(2007) Averaging Methods in Nonlinear
Dynamical Systems, Springer-Verlag, New
York.
4. Murdock, J.A. (1999) Perturbations: Theory
and Methods, Society for Industrial and
Applied Mathematics, Philadelphia, PA.
5. Lochak, P. and Meunier, C. (1988)
Multiphase Averaging for Classical Systems,
Springer-Verlag, New York.
6. Nayfeh, A. (1973) Perturbation Methods,
John Wiley & Sons, Inc., New York.
7. Murdock, J.A. (2003) Normal Forms and
Unfoldings for Local Dynamical Systems,
Springer-Verlag, New York.
8. Nayfeh, A. (1993) Method of Normal Forms,
John Wiley & Sons, Inc., New York.
9. Kahn, P.B. and Zarmi, Y. (1998) Nonlinear
Dynamics: Exploration through Normal
Forms, John Wiley & Sons, Inc., New York.
10. Wiggins, S. (2003) Introduction to Applied
Nonlinear Dynamical Systems and Chaos,
Springer-Verlag, New York.
11. Lagerstrom, P.A. (1988) Matched Asymptotic
Expansions, Springer-Verlag, New York.
12. Smith, D.R. (1985) Singular-Perturbation
Theory, Cambridge University Press,
Cambridge.

Further Reading
447
13. Grasman, J. (1987) Asymptotic Methods for
Relaxation Oscillations and Applications,
Springer-Verlag, New York.
14. Jones, C.K.R.T. (1994) Geometric singular
perturbation theory, in Dynamical Systems
(Montecatini Terme, 1994), Lecture Notes in
Mathematics 1609, Springer-Verlag, New
York, pp. 44‚Äì118.
15. Bender, C.M. and Orszag, S.A. (1999)
Advanced Mathematical Methods for
Scientists and Engineers, Springer-Verlag,
New York.
16. Lakin, W.D. and Sanchez, D.A. (1970) Topics
in Ordinary DiÔ¨Äerential Equations, Dover,
New York.
17. Wasow, W. and Robert, E. (1976) Asymptotic
Expansions for Ordinary DiÔ¨Äerential
Equations, Krieger Publishing Co.,
Huntington, NY.
18. van Dyke, M. (1975) Perturbation Methods
in Fluid Mechanics, Annotated Edition,
Parabolic Press, Stanford, CA.
19. Sychev, V.V., Ruban, A.I., Sychev, V.V., and
Korolev, G.L. (1998) Asymptotic Theory of
Separated Flows, Cambridge University
Press, Cambridge.
20. Rothmayer, A.P. and Smith, F.T. (1998)
Incompressible triple- deck theory, in The
Handbook of Fluid Dynamics (ed. R.W.
Johnson), CRC Press, Boca Raton, FL.
21. Oono, Y. (2000) Renormalization and
asymptotics. Int. J. Mod. Phys. B, 14,
1327‚Äì1361.
22. Chen, L.-Y., Goldenfeld, N., and Oono, Y.
(1996) Renormalization group and singular
perturbations. Phys. Rev. E, 54, 376‚Äì394.
23. Ziane, M. (2000) On a certain
renormalization group method. J. Math.
Phys., 41, 3290‚Äì3299.
24. Lee DeVille, R.E., Harkin, A., Holzer, M.,
Josi¬¥c, K., and Kaper, T.J. (2008) Analysis of a
renormalization group method for solving
perturbed ordinary diÔ¨Äerential equations,
Physica D, 237, 1029‚Äì1052.
25. WoodruÔ¨Ä, S.L. (1993) The use of an
invariance condition in the solution of
multiple-scale singular perturbation
problems: ordinary diÔ¨Äerential equations.
Stud. Appl. Math., 90, 225‚Äì248.
26. Paquette, G.C. (2000) Renormalization
group analysis of diÔ¨Äerential equations
subject to slowly modulated perturbations.
Physica A, 276, 122‚Äì163.
27. Kirkinis, E. (2008) The renormalization
group and the implicit function theorem for
amplitude equations. J. Math. Phys., 49,
1‚Äì16. article 073518.
28. Kirkinis, E. (2012) The renormalization
group: a perturbation method for the
graduate curriculum. SIAM Rev., 54,
374‚Äì388.
29. Kunihiro, T. (1995) A geometrical
formulation of the renormalization group
method for global analysis. Progr. Theor.
Phys., 94, 503‚Äì514; errata, same volume
(94), 835.
30. Kunihiro, T. (1997) The renormalization-
group method applied to asymptotic analysis
of vector Ô¨Åelds. Progr. Theor. Phys., 97,
179‚Äì200.
31. Veysey, J. and Goldenfeld, N. (2007) Simple
viscous Ô¨Çows: from boundary layers to the
renormalization group. Rev. Mod. Phys., 79,
883‚Äì927.
32. Kato, T. (1966) Perturbation Theory for
Linear Operators, Springer-Verlag, New
York.
Further Reading
Andrianov, I.V. and Manevitch, L.I. (2002)
Asymptotology, Kluwer, Dordrecht.
Bender, C.M. and Orszag, S.A. (1999) Advanced
Mathematical Methods for Scientists and
Engineers, Springer-Verlag, New York.
Bush, A.W. (1992) Perturbation Methods for
Engineers and Scientists, CRC Press, Boca
Raton, FL.
Hinch, E.J. (1991) Perturbation Methods,
Cambridge University Press, Cambridge.
Kevorkian, J. and Cole, J.D. (1981; corrected
second printing 1985) Perturbation Methods
in Applied Mathematics, Springer-Verlag,
New York.
Nayfeh, A. (1981) Introduction to Perturbation
Techniques, John Wiley & Sons, Inc., New
York.
O‚ÄôMalley, R.E. (1991) Singular Perturbation
Methods for Ordinary DiÔ¨Äerential Equations,
Springer-Verlag, New York.


449
13
Functional Analysis
Pavel Exner
The area covered by the chapter title is
huge. We describe the part of functional
analysis dealing with linear operators, in
particular, those on Hilbert spaces and their
spectral decompositions, which are impor-
tant for applications in quantum physics.
The claims are presented without proofs,
often highly nontrivial, for which we refer
to the literature mentioned at the end of the
chapter.
13.1
Banach Space and Operators on Them
The Ô¨Årst section summarizes notions from
the general functional analysis that we will
need in the following.
13.1.1
Vector and Normed Spaces
The starting point is the notion of a vec-
tor space V over a Ô¨Åeld ùîΩas a family of
vectors equipped with the operations of
summation and multiplication by a scalar
ùõº‚ààùîΩ. The former is commutative and
associative, and the two operations are
mutually distributive. We consider here
the case when ùîΩ= ‚ÑÇ, the set of complex
numbers, but other Ô¨Åelds are also used,
typically the sets of reals or quaternions.
Standard examples are ‚ÑÇn, the space of
n-tuples of complex numbers, or ùìÅp, the
space of complex sequences {xj}‚àû
j=1 satisfy-
ing ‚àë‚àû
j=1 |xj|p < ‚àûfor a Ô¨Åxed p ‚â•1. Other
frequently occurring examples are function
spaces such as C(Óàµ) consisting of continu-
ous functions on the interval Óàµ, or Lp(Óàµ) the
elements of which are classes of functions
diÔ¨Äering on a zero-measure set and satis-
fying ‚à´Óàµ|f (x)|p dx < ‚àû. The dimension of a
vector space is given by the maximum num-
ber of linearly independent vectors one can
Ô¨Ånd in it. In the examples, the space ‚ÑÇn is
n-dimensional, the other indicated ones are
(countably) inÔ¨Ånite-dimensional.
A map f ‚à∂V ‚Üí‚ÑÇis called a functional.
We say that it is linear if f (ùõºx) = ùõºf (x)
and antilinear if f (ùõºx) = ùõºf (x) for all
ùõº‚àà‚ÑÇ
and
x ‚ààV.
Similarly,
a
semi-
norm is a real functional p satisfying
p(x + y) ‚â§p(x) + p(y) and p(ùõºx) = |ùõº|p(x).
A map F ‚à∂V √ó V ‚Üí‚ÑÇis called a form.
Its (anti)linearity is deÔ¨Åned as above; it
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

450
13 Functional Analysis
is called sesquilinear if it is linear in one
argument and antilinear in the other.
A norm on a vector space is a semi-
norm, usually denoted as ‚Äñ ‚ãÖ‚Äñ, with the
property
that
‚Äñx‚Äñ = 0
implies
x = 0.
A space equipped with it is called a
normed space. The same vector space can
be equipped with diÔ¨Äerent norms, for
example both ‚Äñx‚Äñ‚àû‚à∂= sup1‚â§j‚â§n |xj| and
‚Äñx‚Äñp ‚à∂= ( ‚àë‚àû
j=1 |xj|p)1‚àïp are norms on ‚ÑÇn.
Analogous norms can be introduced on the
other vector spaces mentioned above, for
instance, ‚Äñf ‚Äñ‚àû‚à∂= supx‚ààÓàµ|f (x)| on C(Óàµ) or
‚Äñf ‚Äñp ‚à∂= ( ‚à´Óàµ|f (x)|p dx)1‚àïp on Lp(Óàµ).
The space can be equipped with an inner
(or scalar) product, which is a positive
sesquilinear form (‚ãÖ, ‚ãÖ) with the property
that (x, x) implies x = 0. In such a case,
one way to deÔ¨Åne a norm is through the
formula ‚Äñx‚Äñ ‚à∂=
‚àö
(x, x). This norm is said
to be induced by the scalar product; it
satisÔ¨Åes the Schwarz inequality
|(x, y)| ‚â§‚Äñx‚Äñ‚Äñy‚Äñ .
(13.1)
On the other hand, a norm ‚Äñ ‚ãÖ‚Äñ can be
induced by a scalar product if and only
if it satisÔ¨Åes the parallelogram identity,
‚Äñx + y‚Äñ2 + ‚Äñx ‚àíy‚Äñ2 = 2‚Äñx‚Äñ2 + 2‚Äñy‚Äñ2.
In this way, a vector space is equipped
with a metric structure with the distance
of
points
given
by
d(x, y) ‚à∂= ‚Äñx ‚àíy‚Äñ.
Then one can ask about completeness of
such a space, that is, whether any Cauchy
sequence, {xn} ‚äÇV satisfying d(xn, xm) ‚Üí
0 as n, m ‚Üí‚àû, has a limit in V. A (met-
rically) complete normed space is called a
Banach space; similarly, a complete inner-
product space is called a Hilbert space. The
spaces ùìÅp and Lp(Óàµ) are examples of Banach
spaces, they are Hilbert if p = 2.; The met-
ric induces, mutatis mutandis, a topology
on a vector space making it an example
of topological vector space in which the
vector and topological properties are com-
bined in such a way that the operations
of summation and scalar multiplication
are continuous. A vector space of inÔ¨Ånite
dimension can be equipped with diÔ¨Äerent,
mutually inequivalent topologies. Most
important are locally convex topologies
generated by a family of seminorms with
the property that for each nonzero x ‚ààV
there is a seminorm p such that p(x) ‚â†0.
The norm topology is a particular case.
The presence of topology allows us to
investigate various properties of subsets
and subspaces of V such as compactness,
openness and closeness [14, Chap. II; 20,
Chap. 1], or convergence of sequences
[1, Chap. IV; 2, Chap. 1]. If the space is
equipped with diÔ¨Äerent topologies, the
same sequence may converge with respect
to some of them and have no limit with
respect to others.
13.1.2
Operators on Banach Spaces
A map between two Banach spaces, B ‚à∂
ÓâÑ‚ÜíÓâÖ, is called an operator. It is linear
if B(ùõºx + y) = ùõºBx + By holds; we will con-
sider here linear operators only and there-
fore drop the adjective. Such an operator is
continuous iÔ¨Äit is bounded, which means
there exists a number c such that ‚ÄñBx‚ÄñÓâÖ‚â§
c‚Äñx‚ÄñÓâÑholds for all x ‚ààÓâÑ. The inÔ¨Åmum of
such numbers is called the norm ‚ÄñB‚Äñ of B.
One has ‚ÄñB‚Äñ = supS1 ‚ÄñBx‚ÄñÓâÖ, where S1 ‚à∂=
{x ‚ààÓâÑ‚à∂‚Äñx‚ÄñÓâÑ= 1} is the unit sphere in ÓâÑ.
The functional ‚Äñ ‚ãÖ‚Äñ is indeed a norm; in
particular, the space ÓàÆ(ÓâÑ, ÓâÖ) of all bounded
operators from ÓâÑto ÓâÖequipped with it
becomes a Banach space. In addition, for
the composition of operators B ‚ààÓàÆ(ÓâÖ, ÓâÜ)
and C ‚ààÓàÆ(ÓâÑ, ÓâÖ), the operator norm satis-
Ô¨Åes the inequality ‚ÄñBC‚Äñ ‚â§‚ÄñB‚Äñ‚ÄñC‚Äñ.
In general, an operator can be deÔ¨Åned on
a subspace of ÓâÑonly, which is then called

13.1 Banach Space and Operators on Them
451
the domain of B, denoted by D(B). If D(B) is
dense in ÓâÑand B is bounded, then there is
a unique extension ÃÉB ‚ààÓàÆ(ÓâÑ, ÓâÑ) =‚à∂ÓàÆ(ÓâÑ)
of the operator B to the space ÓâÑ, and more-
over ‚ÄñÃÉB‚Äñ = ‚ÄñB‚Äñ holds, hence bounded
operators may be without loss of gen-
erality supposed to be deÔ¨Åned on the
whole ÓâÑ.
On the other hand, a domain can be
dense in diÔ¨Äerent Banach spaces having
thus diÔ¨Äerent extensions. As an example,
consider
the
Fourier
transformation
deÔ¨Åned by
ÃÇf (y) = (2ùúã)‚àí1‚àï2
‚à´‚Ñù
e‚àíixy f (x) dx
(13.2)
on the set Óàø(‚Ñù) of inÔ¨Ånitely diÔ¨Äeren-
tiable functions f ‚à∂‚Ñù‚Üí‚ÑÇ, which have,
together with all their derivatives, a faster-
than-powerlike decay. Óàø(‚Ñù) is dense in
L1(‚Ñù), hence the map f ÓÇ∂‚ÜíÃÇf extends to
a unique operator from L1(‚Ñù) to the
space
C‚àû(‚Ñù)
of
continuous
functions
satisfying lim|y|‚Üí‚àûÃÇf (y) = 0 (this property
is usually referred to as the Riemann‚Äì
Lebesgue
lemma),
which
is
bounded,
‚ÄñÃÇf ‚Äñ‚àû‚â§(2ùúã)‚àí1‚àï2‚Äñf ‚Äñ1.
However, Óàø(‚Ñù) is dense also in L2(‚Ñù) and
f ÓÇ∂‚ÜíÃÇf can be thus uniquely extended to
a map F ‚à∂L2(‚Ñù) ‚ÜíL2(‚Ñù) called Fourier‚Äì
Plancherel operator with the norm ‚ÄñF‚Äñ = 1.
The right-hand side of (13.2) may not be
deÔ¨Åned for a general f ‚ààL2(‚Ñù), hence we
have to write the action of F as
(Ff )(y) = l.i.m.
n‚Üí‚àû(2ùúã)‚àí1‚àï2
‚à´|x|‚â§n
e‚àíixy f (x) dx ,
(13.3)
where the symbol l.i.m., limes in medio,
means convergence with respect to the
norm of L2(‚Ñù). In a similar way, one
deÔ¨Ånes the Fourier transformation and
Fourier‚ÄìPlancherel operator on functions
f ‚à∂‚Ñùn ‚Üí‚ÑÇ, n > 1.
Functionals represent a particular case of
operators with ÓâÖ= ‚ÑÇ. The space ÓàÆ(ÓâÑ, ‚ÑÇ)
of bounded functionals is called the dual
to the Banach space ÓâÑand denoted as
ÓâÑ‚àó. It is a Banach space with the norm
‚Äñf ‚Äñ = sup‚Äñx‚Äñ=1 |f (x)|. In some cases, the
dual space can be determined explicitly, for
instance, (ùìÅp)‚àóis isomorphic to ùìÅq, where
q = p‚àï(p ‚àí1) for 1 < p < ‚àûand q = ‚àûfor
p = 1.
The metric completeness on Banach
spaces
implies
the
uniform
bound-
edness
principle
for
an
operator
family
Óà≤‚äÇÓàÆ(ÓâÑ, V)
where
V
is
a
normed space with the norm ‚Äñ ‚ãÖ‚Äñ: if
supB‚ààÓà≤‚ÄñBx‚Äñ < ‚àû, then there is a c > 0
such
that
supB‚ààÓà≤‚ÄñB‚Äñ < c.
This
result
has several important consequences, in
particular,
Theorem 13.1 (Open-map theorem) If
an operator ÓàÆ(ÓâÑ, ÓâÖ) maps to the whole
Banach space ÓâÖand G ‚äÇÓâÑis an open set,
then the image set BG is open in ÓâÖ.
Theorem 13.2 (Inverse-mapping
theorem)
If
ÓàÆ(ÓâÑ, ÓâÖ)
is
a
one-to-one
map, then B‚àí1 is a continuous linear map
from ÓâÖto ÓâÑ.
One more consequence concerns closed
operators, which are those whose graph
Œì(T) = {[x, Tx] ‚à∂x ‚ààD(T)} is a closed
set in ÓâÑ‚äïÓâÖ. Alternatively, they can be
characterized
by
sequences:
an
oper-
ator T is closed iÔ¨Ä
for any sequence
{xn} ‚äÇD(T)
such
that
xn ‚Üíx
and
Txn ‚Üíy as n ‚Üí‚àû, we have x ‚ààD(T)
and y = Tx.
Theorem 13.3 (Closed-graph theorem)
A closed linear operator T ‚à∂ÓâÑ‚ÜíÓâÖdeÔ¨Åned
on the whole of ÓâÑis continuous.

452
13 Functional Analysis
13.1.3
Spectra of Closed Operators
Consider now the set ÓàØ(ÓâÑ) of closed linear
operators mapping the Banach space ÓâÑ
to itself. It follows from the closed-graph
theorem that if T ‚ààÓàØ(ÓâÑ) is unbounded,
its domain D(T) ‚â†ÓâÑ. A number ùúÜ‚àà‚ÑÇ
is called an eigenvalue of T if there is a
nonzero x ‚ààD(T), called an eigenvector,
such that Tx = ùúÜx. The span of such vectors
is a subspace in ÓâÑcalled eigenspace (related
to the eigenvalue ùúÜ) and its dimension
is the (geometric) multiplicity of ùúÜ. Any
eigenspace of a closed operator T ‚ààÓàØ(ÓâÑ)
is closed.
This is a familiar way to describe the
spectrum known from linear algebra. An
alternative way is to notice that the inverse
(T ‚àíùúÜ)‚àí1 is not deÔ¨Åned if ùúÜis an eigen-
value. John von Neumann was the Ô¨Årst to
notice that this oÔ¨Äers a Ô¨Åner tool in case
of unbounded operators when (T ‚àíùúÜ)‚àí1
may exist as an unbounded operator, which
means that T ‚àíùúÜdoes not map ÓâÑonto
itself but to its proper subspace only.
Consequently,
we
deÔ¨Åne
the
spec-
trum ùúé(T) of T as the set consisting of
three parts: the point spectrum ùúép(T)
being the family of all eigenvalues, the
continuous
spectrum
ùúéc(T)
for
which
Ran(T ‚àíùúÜ) ‚â†ÓâÑbut it is dense in ÓâÑ, and
Ô¨Ånally the residual spectrum ùúér(T) for
which Ran(T ‚àíùúÜ) ‚â†ÓâÑ, where the symbol
ÓâÖconventionally denotes the closure of
the set ÓâÖ. The spectrum ùúé(T) is a closed
set, its complement ùúå(T) = ‚ÑÇ‚ßµùúé(T) is
called the resolvent set of the operator T
and the map RT ‚à∂ùúå(T) ‚ÜíÓàÆ(ÓâÑ) deÔ¨Åned by
RT(ùúÜ) ‚à∂= (T ‚àíùúÜ)‚àí1 is the resolvent of T.
In
general,
the
spectrum
may
be
empty. For a bounded operator B on a
Banach space ÓâÑ, however, the spectrum
is always a nonempty compact set and
its
radius
r(B) = sup{|ùúÜ| ‚à∂ùúÜ‚ààùúé(T)} =
limn‚Üí‚àû‚ÄñBn‚Äñ1‚àïn ‚â§‚ÄñB‚Äñ. The resolvent can
be in this case expressed through the
Neumann series,
RB(ùúÜ) = ‚àíùúÜ‚àí1I ‚àí
‚àû
‚àë
k=1
ùúÜ‚àí(k+1)Bk,
(13.4)
which converges outside the circle of spec-
tral radius, that is, for |ùúÜ| > ‚ÄñB‚Äñ.
13.2
Hilbert Spaces
As a particular case of Banach spaces,
Hilbert spaces share the general prop-
erties listed above but the existence of
inner product gives them speciÔ¨Åc features.
Hilbert spaces are important especially
from the viewpoint of quantum theory
where they are used as state spaces, the
pure states of such systems being identiÔ¨Åed
with one-dimensional subspaces of an
appropriate Hilbert space.
13.2.1
Hilbert-Space Geometry
Hilbert spaces are distinguished by the
existence of orthogonal projection: given a
closed subspace Óà≥‚äÇÓà¥and a vector x ‚ààÓà¥,
there is a unique vector yx ‚ààÓà¥such that
the
distance
d(x, Óà≥) ‚à∂= infy‚ààÓà≥‚Äñx ‚àíy‚Äñ =
‚Äñx ‚àíyx‚Äñ. It implies that each vector can be
uniquely decomposed into a sum, x = y + z
with y ‚ààÓà≥and z belonging to Óà≥‚üÇ, the
orthogonal complement of Óà≥in Óà¥. It also
provides a criterion for a set M ‚äÇÓà¥to be
total, that is, such that its linear envelope
is dense in Óà¥: it happens iÔ¨ÄM‚üÇ= {0}.
Another
characteristic
feature
of
a
Hilbert space is the relation to its dual: by
the Riesz lemma to any f ‚ààÓà¥‚àó, there is a
unique yf ‚ààÓà¥such that f (x) = (yf , x); the
map f ÓÇ∂‚Üíyf is an antilinear isometry of Óà¥

13.2 Hilbert Spaces
453
and Óà¥‚àó. Functionals allow us to introduce
a convergence in a Hilbert space diÔ¨Äerent
from the one determined by the norm of
Óà¥: a sequence {xn} ‚äÇÓà¥converges weakly
if (y, xn) ‚Üí(y, x) holds for all y ‚ààÓà¥. If
one has in addition ‚Äñxn‚Äñ ‚Üí‚Äñx‚Äñ, then {xn}
converges also in the norm.
A family {eùõº} ‚äÇÓà¥of pairwise orthog-
onal vectors of unit length is called an
orthonormal basis of Óà¥if it is total. Such
a basis always exists and any two orthonor-
mal bases in a given Óà¥have the same car-
dinality, which is called the dimension of
Óà¥. Hilbert spaces Óà¥, Óà¥‚Ä≤ are isomorphic iÔ¨Ä
dim Óà¥= dim Óà¥‚Ä≤. A Hilbert space is separa-
ble if its dimension is at most countable.
Given an orthonormal basis {eùõº}, we
deÔ¨Åne Fourier coeÔ¨Écients of a vector x ‚ààÓà¥
as (eùõº, x). Using them, we can write the
Fourier expansion
x =
‚àë
ùõº
(eùõº, x)eùõº
(13.5)
of x; it always makes sense as a convergent
series because the set of nonzero Fourier
coeÔ¨Écients is at most countable even if the
basis {eùõº} is uncountable, and because the
Parseval identity is valid,
‚Äñx‚Äñ2 =
‚àë
ùõº
|(eùõº, x)|2.
(13.6)
The orthonormal basis most often used
in the space ùìÅ2 consists of the vectors ùúôn ‚à∂=
{0, ‚Ä¶ , 0, 1, 0, ‚Ä¶} with the nonzero entry
on the nth place. In case of spaces L2(Óàµ),
the choice depends on the interval. For the
Ô¨Ånite interval Óàµ= (0, 2ùúã), the trigonometric
basis, {ek ‚à∂k = 0, ¬±1, ¬±2, ‚Ä¶ } with ek(x) =
(2ùúã)‚àí1‚àï2eikx, is used. On the whole line, Óàµ=
‚Ñù, the functions
hn(x) = (2nn!)‚àí1‚àï2ùúã‚àí1‚àï4e‚àíx2‚àï2Hn(x),
n = 0, 1, 2, ‚Ä¶ ,
(13.7)
where
Hn(x) = (‚àí1)Nex2dn‚àïdxne‚àíx2
are
Hermite polynomials, form an orthonormal
basis. Similarly, one can construct a basis
for the semi-inÔ¨Ånite interval Óàµ= (0, ‚àû)
using Laguerre polynomials.
Let us mention two other examples of
Hilbert spaces. Given a set M equipped
with measure ùúáand a Hilbert space
Óà≥,
we
consider
measurable
vector-
valued functions f ‚à∂M ‚ÜíÓà≥such that
‚à´M ‚Äñf (x)‚Äñ2
Óà≥dùúá(x) < ‚àû. We regard classes
of such functions diÔ¨Äering on a set of zero
measure as elements of a new space. One
can check that it is a Hilbert space with
respect to the inner product
(f , g) ‚à∂= ‚à´M
(f (x), g(x))2
Óà≥dùúá(x);
(13.8)
we denote this space as L2(X, dùúá; Óà≥). Such
spaces appear in various applications, for
instance, in quantum mechanics, Óà≥is often
a Ô¨Ånite-dimensional space associated with
spin states of the system.
Our second example is the Hilbert space
of analytic functions f ‚à∂‚ÑÇ‚Üí‚ÑÇequipped
this time with the ‚Äúweighted‚Äù inner prod-
uct,
(f , g) = 1
ùúã‚à´‚ÑÇ
f (z)g(z) e‚àí|z|2 dz,
(13.9)
where dz is a shorthand for the ‚Äútwo-
dimensional‚Äù
measure
d(Rez) d(Imz)
in the complex plane. An orthonormal
basis in it is formed by the monomials,
un(z) = (z!)‚àí1‚àï2zn, n = 0, 1, ‚Ä¶ .
Denoting
ez(w) = ezw, we have the identity
f (w) = 1
ùúã‚à´‚ÑÇ
(ez, f ) ez(w) e‚àí|z|2 dz,
(13.10)
which can be regarded as a continuous ana-
logue of the Fourier expansion with respect
to an ‚Äúovercomplete basis;‚Äù this relation
plays the central role in description of
quantum-mechanical coherent states.

454
13 Functional Analysis
13.2.2
Direct Sums and Tensor Products
Applications require many diÔ¨Äerent spaces,
hence it is useful to know how to con-
struct new Hilbert spaces from given ones.
One possibility are the L2 spaces of vector-
valued functions mentioned above; here we
add two more methods. The Ô¨Årst is based
on direct sums. Let {Óà¥k} be a family of
Hilbert spaces, for simplicity supposed to
be at most countable, with norm of Óà¥k
being denoted by ‚Äñ ‚ãÖ‚Äñk. We consider the set
of sequences X = {xk ‚à∂xk ‚ààÓà¥k} such that
‚àë
k ‚Äñxk‚Äñ2
k < ‚àû, equip it with vector oper-
ations deÔ¨Åned componentwise and deÔ¨Åne
the inner product on it by
(X, Y) =
‚àë
k
(xk, yk)k,
(13.11)
where (‚ãÖ, ‚ãÖ)k is the inner product in Óà¥k,
obtaining a new Hilbert space, which we
denote as ‚®Å
k Óà¥k or ‚àë‚äï
k Óà¥k. The dimension
of the direct product is clearly the sum of
the dimension of its components. In a sim-
ilar way, one can deÔ¨Åne, under appropriate
measurability hypotheses, a direct integral
‚à´‚äï
M Óà¥(x) dùúá(x) with respect to a measure ùúá
of a family of Hilbert spaces dependent on
the integration variable x.
The deÔ¨Ånition of a tensor product of
Hilbert spaces Óà¥1, Óà¥2 is more involved. We
introduce Ô¨Årst a tensor product realiza-
tion as a bilinear map ‚äó‚à∂Óà¥1 √ó Óà¥2 ‚ÜíÓà¥,
which associates with x ‚ààÓà¥1 and y ‚ààÓà¥2
an element x ‚äóy ‚ààÓà¥in such a way that
(x ‚äóy, x‚Ä≤ ‚äóy‚Ä≤) = (x, x‚Ä≤)1(y, y‚Ä≤)2
holds
for
all x, x‚Ä≤ ‚ààÓà¥1 and y, y‚Ä≤ ‚ààÓà¥2 and the set
Óà¥1 ‚äóÓà¥2 is total in Óà¥. For each pair Óà¥1, Óà¥2,
a realization of their tensor product exists,
and furthermore, all such realizations are
isomorphic to each other.
This allows us to investigate the tensor
product through a Ô¨Åxed realization of it, in
particular, to write Óà¥1 ‚äóÓà¥2 = Óà¥having in
mind a concrete map ‚äó. Frequently appear-
ing examples are the relations
L2(M, dùúá)‚äóL2(N, dùúà)=L2(M √ó N, d(ùúá‚äóùúà)) ,
(13.12)
L2(X, dùúá) ‚äóÓà≥= L2(X, dùúá; Óà≥) ,
where ùúá‚äóùúàdenotes the product mea-
sure
of
ùúá
and
ùúà,
deÔ¨Åned
through
the
maps
(f ‚äóg)(x, y) = f (x)g(y)
and
(f ‚äóùúô)(x)=f (x)ùúô, respectively. If {e(1)
ùõº}
and
{e(2)
ùõΩ}
are
orthonormal
bases
in
the two Hilbert spaces, then the vec-
tors
e(1)
ùõº‚äóe(2)
ùõΩ
constitute
an
orthono-
mal
basis
in
Óà¥1 ‚äóÓà¥2,
in
particular,
dim(Óà¥1 ‚äóÓà¥2) = dim Óà¥1 ‚ãÖdim Óà¥2.
The
construction
of
tensor
product
extends naturally to any Ô¨Ånite family of
Hilbert spaces. Tensor products of inÔ¨Ånite
families can also be constructed, however,
the procedure is more involved. In quan-
tum mechanics, tensor products serve to
describe composite systems the state space
of which is of the form ‚®Ç
j Óà¥j where Óà¥j is
the state spaces of the jth component; the
latter can be either a real physical system
or a formal ‚Äúsubsystem‚Äù coming from a
separation of variables.
13.3
Bounded Operators on Hilbert Spaces
As we have mentioned, without loss of gen-
erality, we may regard bounded operators
as deÔ¨Åned on the whole Hilbert space. We
can specify various classes of them.
13.3.1
Hermitean Operators
Given an operator B ‚ààÓàÆ(Óà¥), we deÔ¨Åne
its
adjoint
as
the
unique
operator
B‚àó
satisfying
(y, Bx) = (B‚àóy, x)
for
all
x, y ‚ààÓà¥. The map B ÓÇ∂‚ÜíB‚àóis an antilinear
isometry, ‚ÄñB‚àó‚Äñ = ‚ÄñB‚Äñ. The adjoint satisÔ¨Åes

13.3 Bounded Operators on Hilbert Spaces
455
B‚àó‚àó= B and (BC)‚àó= C‚àóB‚àó, and moreover,
(B‚àó)‚àí1 = (B‚àí1)‚àóholds provided B‚àí1 exists.
Another useful relation is the expression
for the subspace on which B‚àóvanishes,
Ker B‚àó= (Ran B)‚üÇ.
An
operator
A ‚ààÓàÆ(Óà¥)
is
called
Hermitean
if
it
coincides
with
its
adjoint, A = A‚àó. The spectrum of A is
a subset of real axis situated between
mA = inf(x, Ax) and mA = sup(x, Ax); we
have ‚ÄñA‚Äñ = max(|mA|, |MA|). The oper-
ator is called positive if mA ‚â•0. For any
B ‚ààÓàÆ(Óà¥), the product B‚àóB is a Her-
mitean operator which, in addition satisÔ¨Åes
‚ÄñB‚àóB‚Äñ = ‚ÄñB‚Äñ2. The last property means, in
particular, that ÓàÆ(Óà¥) has also the structure
of a C‚àó-algebra [3].
To any positive A, there is a unique posi-
tive operator
‚àö
A having the meaning of its
square root. In particular, one can associate
with any B ‚ààÓàÆ(Óà¥) the operator |B| ‚à∂=
‚àö
B‚àóB. While this oÔ¨Äers an analogy with the
modulus of a complex number, caution is
needed, for instance, none of the relations
|BC| = |B||C|, |B‚àó| = |B|, |B + C| ‚â§|B| +
|C| are valid in general.
An important class of positive opera-
tors are projections assigning to any vector
x ‚ààÓà¥its orthogonal projection to a given
subspace Óà≥‚äÇÓà¥. An operator E ‚ààÓàÆ(Óà¥) is
a projection iÔ¨ÄE2 = E = E‚àó; its spectrum
contains only the eigenvalues 0 and 1 except
for the trivial cases, E = 0, I, when only one
of them is present.
Projections
E, F
are
orthogonal
if
Ran E ‚üÇRan F, which is equivalent to
the
condition
EF = FE = 0.
The
sum
E + F is a projection iÔ¨ÄE and F are
orthogonal.
The
product
of
the
pro-
jections is a projection iÔ¨ÄEF = FE in
which
case
Ran EF = Ran E ‚à©Ran F.
Furthermore, the diÔ¨Äerence E ‚àíF is a
projection iÔ¨ÄE ‚â•F, that is, E ‚àíF
is
positive,
which
is
further
equivalent
to Ran E ‚äÉRan F,
or to
the
relations
EF = FE = F.
13.3.2
Unitary Operators
An operator U ‚ààÓàÆ(Óà¥) is called an isom-
etry if its domain D(U) and range Ran U
are closed subspaces and the norm is pre-
served, ‚ÄñUx‚Äñ = ‚Äñx‚Äñ. If D(U) = Óà¥the oper-
ator is called unitary, the same name is
used for linear isometries between diÔ¨Äerent
Hilbert spaces. If D(U) ‚â†Óà¥we speak about
a partial isometry. A unitary operator pre-
serves also the inner product, (Ux, Uy) =
(x, y), and satisÔ¨Åes the relation U‚àí1 = U‚àó;
its spectrum is a subset of the unit circle,
ùúé(U) ‚äÇ{z ‚àà‚ÑÇ‚à∂|z| = 1}.
If V is a partial isometry, the products
V ‚àóV and VV ‚àóare the projections to its ini-
tial and Ô¨Ånal subspace, D(V) and the range
Ran V, respectively. Using partial isome-
tries, one can Ô¨Ånd a polar decomposition
of a bounded operator: to any B ‚ààÓàÆ(Óà¥)
there is a unique partial isometry WB
such that B = WB|B| and the relations
Ker WB = Ker B and Ran WB = Ran B hold.
As another word of caution, the ‚Äúopposite‚Äù
decomposition, B = |B|W, may not exist if
B is not Hermitean.
Both Hermitean and unitary operators
are
normal,
which
means
they
com-
mute with their adjoints, BB‚àó= B‚àóB, and
similarly the real and imaginary part
of such an operator, Re B ‚à∂= 1
2(B + B‚àó)
and
Im B ‚à∂= (1‚àï2i)(B ‚àíB‚àó),
commute.
Normal operators have empty residual
spectrum and their eigenspaces corre-
sponding
to
diÔ¨Äerent
eigenvalues
are
orthogonal.
An
operator
B ‚ààÓàÆ(Óà¥)
is
said to have a pure point spectrum if its
eigenvectors form an orthonormal basis
in Óà¥. Every such operator is normal and
ùúé(B) = ùúép(B).

456
13 Functional Analysis
13.3.3
Compact Operators
An operator C ‚ààÓàÆ(Óà¥) is called compact if
it maps any bounded subset of Óà¥into a
precompact one, that is, a set the closure
of which is compact. Equivalently, a com-
pact operator maps any weakly convergent
sequence {xn} ‚äÇÓà¥into a sequence conver-
gent with respect to the norm of Óà¥. The
set Óà∑(Óà¥) of compact operators is a sub-
space in ÓàÆ(Óà¥), which is closed in the opera-
tor norm and, in addition, it has the ‚àó-ideal
property, namely, that for any C ‚ààÓà∑(Óà¥)
and B ‚ààÓàÆ(Óà¥) the operators C‚àó, BC, CB
are also compact. Compactness has several
implications for the operator spectrum:
Theorem 13.4 (Riesz‚ÄìSchauder theorem)
For a C ‚ààÓà∑(Óà¥) any nonzero point of
the spectrum is an eigenvalue of Ô¨Ånite
multiplicity and the only possible accu-
mulation point of the spectrum is zero.
The point spectrum is at most countable
and the eigenvalue moduli can be ordered,
namely,
|ùúÜ1| ‚â•¬∑ ¬∑ ¬∑ ‚â•|ùúÜj| ‚â•|ùúÜj+1| ‚â•¬∑ ¬∑ ¬∑,
with limj‚Üí‚àûùúÜj = 0 if dim Óà¥= ‚àû.
Theorem 13.5 (Fredholm alternative)
Given
an
equation
x ‚àíùúÜCx = y
with
C ‚ààÓà∑(Óà¥), ùúÜ‚àà‚ÑÇ, and y ‚ààÓà¥, one and
only of the following situations can occur:
(i) the equation has a unique solution xy for
any y ‚ààÓà¥, in particular, x0 = 0, or (ii) the
equation without the right-hand side has a
nontrivial solution.
Theorem 13.6 (Hilbert‚ÄìSchmidt
theorem) A normal compact operator
has a pure point spectrum.
A Ô¨Ånite-dimensional operator, that is, an
operator C such that dim Ran C < ‚àû, is
compact, and a norm-convergent sequence
of such operators has a compact limit. In
fact, there is a canonical form, which makes
it possible to express every compact oper-
ator as such a limit. The operator |C| cor-
responding to C ‚ààÓà∑(Óà¥) is compact and
its eigenvalues, |C|ej = ùúájej, form a nonin-
creasing sequence in [0, ‚àû); we call them
singular values of C. Using Fourier expan-
sion of |C|x in combination with the polar
decomposition, C = W|C|, and introduc-
ing the vectors fj ‚à∂= Wej, we arrive at the
formula
C =
JC
‚àë
j=1
ùúáj(ej, ‚ãÖ)fj,
(13.13)
where JC ‚à∂= dim Ran |C| and the series
converges in the operator norm if JC = ‚àû.
13.3.4
Schatten Classes
The set Óà∑(Óà¥) has some distinguished sub-
sets. As mentioned above, we can asso-
ciate with a C ‚ààÓàÆ(Óà¥) its singular values
{ùúáj}. For a Ô¨Åxed p ‚â•1, we denote by Óà∂p, or
Óà∂p(Óà¥), the set of compact operators C such
that ‚ÄñC‚Äñp ‚à∂= ( ‚àë
j ùúáp
j
)1‚àïp < ‚àû. Each Óà∂p is a
Banach space, which is a closure of the set of
Ô¨Ånite-dimensional operators with respect
to the norm ‚Äñ ‚ãÖ‚Äñp. The inclusion Óà∂p ‚äÇÓà∂p‚Ä≤
holds for p < p‚Ä≤ and the set Óà∑(Óà¥) is alter-
natively denoted as Óà∂‚àû(Óà¥).
Two of these so-called Schatten classes
are of particular importance. Óà∂2(Óà¥) is a
Hilbert space with respect to the scalar
product deÔ¨Åned by (B, C)2 = ‚àë
j(Bej, Cej)
for any Ô¨Åxed orthonormal basis {ej} ‚äÇÓà¥,
its elements are called Hilbert‚ÄìSchmidt
operators. They form a ‚àó-ideal in ÓàÆ(Óà¥): for
any C ‚ààÓà∂2(Óà¥) and B ‚ààÓàÆ(Óà¥), the opera-
tors C‚àó, BC, CB are also Hilbert‚ÄìSchmidt.
We have a useful criterion for integral
operators,
B ‚ààÓàÆ(L2(M, dùúá))
acting
as
(Bf )(x) = ‚à´M gB(x, y) f (y) dùúá(y):
such
an
operator belongs to the Hilbert‚ÄìSchmidt

13.4 Unbounded Operators
457
class iÔ¨Äits kernel is square integrable,
‚ÄñB‚Äñ2
2 = ‚à´M√óM
|gB(x, y)|2 d(ùúá‚äóùúá)(x, y) < ‚àû,
(13.14)
with respect to the product measure ùúá‚äóùúá
[2, App. A].
Another important class, again an ‚àó-
ideal in ÓàÆ(Óà¥), is Óà∂1(Óà¥) the elements of
which are called trace-class operators. An
operator B ‚ààÓàÆ(Óà¥) belongs to Óà∂1 iÔ¨Äit is a
product of two Hilbert‚ÄìSchmidt operators.
For any C ‚ààÓà∂1(Óà¥), one can deÔ¨Åne its trace
by Tr C ‚à∂= ‚àë
j(ej, Bej) where {ej} is any
Ô¨Åxed orthonormal basis in Óà¥; the Óà∂1-norm
is given by ‚ÄñC‚Äñ1 = Tr |C|. The trace is a
functional of the unit norm satisfying
Tr C‚àó=Tr C ,
Tr (BC) = Tr (CB)
for
C ‚ààÓà∂1(Óà¥) , B ‚ààÓàÆ(Óà¥) .
(13.15)
Important trace-class operators are den-
sity matrices used in quantum physics to
describe mixed states. They are positive
W ‚ààÓà∂1(Óà¥)
satisfying
the
normaliza-
tion
condition
Tr W = 1.
Note
that
one-dimensional
projections
describ-
ing pure states are a particular case of
density matrices; a state is not pure iÔ¨Ä
Tr W 2 < 1. The subset of pure states can be
also characterized geometrically: the set of
all density matrices on a given Óà¥is convex
and one-dimensional projections are its
extremal points.
13.4
Unbounded Operators
Unbounded operators are considerably
more diÔ¨Écult to deal with because one
has to pay attention to their deÔ¨Ånition
domains; however, they appear in numer-
ous applications.
13.4.1
Operator Adjoint and Closure
An operator T on Óà¥is said to be densely
deÔ¨Åned if D(T) = Óà¥; in contrast to the pre-
vious section, there is no standard way to
extend such an operator to the whole Óà¥.
The set of densely deÔ¨Åned operators on Óà¥
will be denoted as Óà∏(Óà¥). To a given T ‚àà
Óà∏(Óà¥) and y ‚ààÓà¥, there is at most one vec-
tor y‚àó‚ààÓà¥such that the relation (y, Tx) =
(y‚àó, x) holds. We denote by D(T‚àó) the sub-
space of y for which such a y‚àóexists and
introduce the adjoint T‚àóof T acting as
T‚àóy = y‚àó.
As
in
the
bounded
case,
we
have
Ker T‚àó= (Ran T)‚üÇ, so Ker T‚àóis a closed
subspace. If T‚àí1 ‚ààÓà∏(Óà¥), then T‚àóis also
invertible and (T‚àó)‚àí1 = (T‚àí1)‚àó. On the
other hand, if S is an extension of T, S ‚äÉT,
then S‚àó‚äÇT‚àó. Other relations valid for
bounded operators hold generally in a
weaker form only, for instance, T‚àó‚àó‚äÉT
provided T‚àó‚ààÓà∏(Óà¥); similarly, the rela-
tions (S + T)‚àó‚äÉS‚àó+ T‚àóand (TS)‚àó‚äÉS‚àóT‚àó
are valid; in both cases, they turn to
identities if T ‚ààÓàÆ(Óà¥).
This allows to introduce two important
notions. An operator A ‚ààÓà∏(Óà¥) is symmet-
ric if A ‚äÇA‚àó, in other words, if (y, Ax) =
(Ay, x) holds for all x, y ‚ààD(A). If, in addi-
tion, A = A‚àó, the operator is called self-
adjoint. If A is bounded, the two notions
coincide and we speak about a Hermitean
operator; in the unbounded case, it is better
avoid this term because it may cause misun-
derstanding.
As an example, let us mention the mul-
tiplication operator Q on Óà¥= L2(Óàµ) acting
as (Qùúì)(x) = ‚à´Óàµxùúì(x) dx, which one asso-
ciates in quantum mechanics with the par-
ticle position. It is bounded iÔ¨Äthe interval
Óàµis bounded; if it is not the case, we put
D(Q) = {ùúì‚ààÓà¥‚à∂‚à´Óàµx2|ùúì(x)|2 dx < ‚àû}. It
is easy to check that the operator Q deÔ¨Åned

458
13 Functional Analysis
in this way is not only symmetric but also
self-adjoint.
The
fact
that
the
domain
of
an
unbounded symmetric operator is not
the whole Óà¥seen in this example is
valid generally, because the closed-graph
theorem has the following consequence:
Theorem 13.7 (Hellinger‚ÄìToeplitz
theorem) A symmetric operator A with
D(A) = Óà¥is bounded.
The notion of adjoint operator allows us
to formulate new properties of closed oper-
ators, in addition to the general ones men-
tioned above. The adjoint to any T ‚ààÓà∏(Óà¥)
is a closed operator. The closure T of an
operator T is its smallest closed extension,
or equivalently, the operator the graph of
which is the closure of Œì(T) in Óà¥‚äïÓà¥. If
the adjoint T‚àóis densely deÔ¨Åned, the clo-
sure of T exists and the relations T‚àó‚àó= T
and (T)‚àó= T are valid. The set of closed
densely deÔ¨Åned operators on a given Óà¥will
be denoted as Óà∏c(Óà¥).
In particular, any self-adjoint operator is
closed. As the closure represents a unique
way of extending an operator, it is useful
to deÔ¨Åne an essentially self-adjoint (e.s.a.)
operator A as such that its closure is self-
adjoint, A = A‚àó. This concept is useful in
applications because it is often easier to
prove that an operator A is e.s.a., in which
case we know it has a unique self-adjoint
extension even if we may not know the
domain D(A) of the latter explicitly. More
generally, a subspace D in the domain of an
operator T ‚ààÓà∏c(Óà¥) is called a core if the
restriction T ‚ÜæD of T to D satisÔ¨Åes T ‚ÜæD =
T. It means that D is a core of a self-adjoint
A iÔ¨ÄA ‚ÜæD is e.s.a.; one usually says that A
is e.s.a. on D.
For any T ‚ààÓà∏c(Óà¥), the product T‚àóT is
self-adjoint and positive, that is, (x, T‚àóTx) =
‚ÄñTx‚Äñ2 ‚â•0 holds for any x ‚ààD(T‚àóT), and
D(T‚àóT) is a core for T; similarly, TT‚àóis self-
adjoint and positive, and D(TT‚àó) is a core
for T‚àó.
For a closed Hilbert-space operator T,
we introduce its essential spectrum, ùúéess(T),
as the set of all ùúÜ‚àà‚ÑÇto which one can Ô¨Ånd
a sequence {xn} ‚äÇD(T) of unit vectors
having no convergent subsequence and
satisfying (T ‚àíùúÜ)xn ‚Üí0 as n ‚Üí‚àû. The
spectrum of the operator T is then the
union of ùúéess(T) with ùúép(T) ‚à™ùúér(T) and
one has ùúéc(T) = ùúéess(T) ‚ßµ(ùúép(T) ‚à™ùúér(T)).
However,
in
distinction
to
the
spec-
tral
decomposition
mentioned
in
Section 13.1.3, the present one is not
disjoint, for instance, an eigenvalue of inÔ¨Å-
nite multiplicity belongs simultaneously to
ùúép(T) and ùúéess(T).
13.4.2
Normal and Self-Adjoint Operators
As in the bounded case, an operator T ‚àà
Óà∏c(Óà¥) is said to be normal if T‚àóT = TT‚àó.
The set of all such operators on a given Óà¥is
denoted as Óà∏n(Óà¥). An operator T is normal
iÔ¨ÄD(T) = D(T‚àó) and ‚ÄñTx‚Äñ = ‚ÄñT‚àóx‚Äñ holds
for all x ‚ààD(T).
A typical example is that of oper-
ators
of
multiplication
by
a
function
generalizing the operator Q described
above.
Given
a
function
f ‚à∂M ‚Üí‚ÑÇ,
measurable
with
respect
to
a
mea-
sure ùúáon M, we deÔ¨Åne the operator
Tf
acting as (Tf ùúì)(x) = f (x)ùúì(x) with
the
domain
D(Tf ) = {ùúì‚ààL2(M, dùúá) ‚à∂
‚à´M |f (x)ùúì(x)|2 dùúá(x) < ‚àû}. Such an oper-
ator is densely deÔ¨Åned and normal. It
is self-adjoint iÔ¨Äf is real-valued almost
everywhere (a.e.) in M and bounded iÔ¨Äf is
essentially bounded, ‚ÄñTf ‚Äñ = ‚Äñf ‚Äñ‚àû.
As in the bounded case, the residual
spectrum of a normal operator is void
and its resolvent set coincides with its
regularity domain, that is, ùúÜ‚àâùúé(T) holds

13.4 Unbounded Operators
459
iÔ¨Äthere is a constant c = c(ùúÜ) > 0 such
that
‚Äñ(T ‚àíùúÜ)x‚Äñ ‚â•c‚Äñx‚Äñ,
which
is
fur-
ther equivalent to Ran (T ‚àíùúÜ) = Óà¥. This
implies, in particular, that the spectrum
of a self-adjoint operator A is a subset
of the real axis satisfying the relation
inf ùúé(A)=inf{(x, Ax) ‚à∂x ‚ààD(A), ‚Äñx‚Äñ =
1}. In the above example, the spectrum
ùúé(Tf ) coincides with the essential range of
the function f , which is the set of all points
such that the f -preimage of each of their
neighborhoods has a nonzero ùúámeasure.
Self-adjointness
is
of
fundamental
importance for quantum physics because
such operators are used to describe observ-
ables of quantum systems.
Theorem 13.8 (Basic
self-adjointness
criterion)
For
a
symmetric
operator
A,
the
following
claims
are
equiv-
alent:
(i)
A
is
self-adjoint,
(ii)
A
is
closed and Ker (A‚àó¬± i) = {0}, and (iii)
Ran (A ¬± i) = Óà¥. In a similar way, essen-
tial self-adjointness of A is equivalent to
Ker (A‚àó¬± i) = {0}, or to Ran (A ¬± i) = Óà¥.
In view of the above-mentioned role
that self-adjointness plays in quantum
mechanics, it is important to have other
suÔ¨Écient conditions. An often used one is
based on a perturbation argument. Given
linear operators A, S on Óà¥, we say that S
is A-bounded if D(S) ‚äÉD(A) and there are
a, b ‚â•0 such that
‚ÄñSx‚Äñ ‚â§a‚ÄñAx‚Äñ + b‚Äñx‚Äñ
(13.16)
holds for any x ‚ààD(A). The inÔ¨Åmum of
the a‚Äôs for which the inequality (13.16)
is satisÔ¨Åed with some b ‚â•0 is called the
A-bound of S. The A-boundedness can be
used to prove self-adjointness of operators
if a is small enough.
Theorem 13.9 (Kato‚ÄìRellich theorem)
Let A be self-adjoint and S symmetric and
A-bounded with the A-bound less than
one, then the sum A + S is self-adjoint.
Moreover, if D ‚äÇD(A) is a core for A, then
A + S is e.s.a. on D.
Another useful criterion relies on ana-
lytical vectors. A vector x that belongs to
D(Aj) for all j ‚â•1, is called analytic (with
respect to the operator A) if the power
series ‚àë
j ‚ÄñAjx‚Äñzj‚àïj! has a nonzero conver-
gence radius.
Theorem 13.10 (Nelson theorem) A
symmetric operator is e.s.a. if it has a total
set of analytic vectors.
Sometimes, one can analyze an opera-
tor by mapping it to another one the struc-
ture of which is more simple. Operators
T on Óà¥and S on Óà≥are unitarily equiva-
lent if there is a unitary U ‚à∂Óà≥‚ÜíÓà¥such
that T = USU‚àí1. Unitary equivalence pre-
serves numerous operator properties. If S is
densely deÔ¨Åned, the same is true for T and
T‚àó= US‚àóU‚àí1; in particular, if S is symmet-
ric or self-adjoint, then T is again symmet-
ric or self-adjoint, respectively. In a simi-
lar way, U preserves operator invertibility,
closedness, and other properties.
As an example, consider the operator
P on L2(‚Ñù) acting by Pùúì= ‚àíiùúì‚Ä≤ with the
domain consisting of functions ùúì‚à∂‚Ñù‚Üí‚ÑÇ
with the derivatives ùúì‚Ä≤ ‚ààL1 ‚à©L2. In quan-
tum mechanics, this operator describes
momentum of a one-dimensional particle.
It is related to the operator Q on L2(‚Ñù)
introduced above by the relation
P = F‚àí1QF,
(13.17)
where F is the Fourier‚ÄìPlancherel opera-
tor (13.3). Both operators are self-adjoint
and also e.s.a. on various subsets of
their
domain,
for
instance,
on
Óàø(‚Ñù).
Moreover, because the unitary equivalence
preserves the spectrum, we have also
ùúé(P) = ùúé(Q) = ‚Ñù.

460
13 Functional Analysis
13.4.3
Tensor Products of Operators
Next we recall how to construct oper-
ators
on
tensor
products
of
Hilbert
spaces.
We
will
consider
a
product
Óà¥1 ‚äóÓà¥2;
an
extension
to
any
Ô¨Ånite
number of Hilbert spaces is straight-
forward.
Given
operators
Bj ‚ààÓàÆ(Óà¥j)
we deÔ¨Åne the map B1 ‚äóB2 on Óà¥1 √ó Óà¥2
by (B1 ‚äóB2)(x1 ‚äóx2) = B1x1 ‚äóB2x2 and
extend it Ô¨Årst linearly, then continuously. As
both the Bj are bounded, the resulting oper-
ator is deÔ¨Åned on the whole of Óà¥1 ‚äóÓà¥2,
satisfying ‚ÄñB1 ‚äóB2‚Äñ = ‚ÄñB1‚Äñ‚ÄñB2‚Äñ.
Tensor products of bounded opera-
tors satisfy the usual rules known from
matrix
algebra,
B1C1 ‚äóB2C2 = (B1 ‚äó
B2)(C1 ‚äóC2)
and
(B1 ‚äóB2)‚àó= B‚àó
1 ‚äóB‚àó
2,
and furthermore, (B1 ‚äóB2)‚àí1 = B‚àí1
1 ‚äóB‚àí1
2
provided that both the Bj are invertible. If
the component operators Bj are normal
(unitary, Hermitean, projections) the same
is respectively true for their tensor product.
If we have operators Tj on Óà¥j, in gen-
eral unbounded, attention has to be paid to
their domains. It is natural to deÔ¨Åne T1 ‚äó
T2 on its ‚Äúminimal‚Äù domain, that is, the
linear hull of D(T1) √ó D(T2). If the Tj‚Äôs are
densely deÔ¨Åned, the same is true for their
tensor product and (T1 ‚äóT2)‚àó‚äÉT‚àó
1 ‚äóT‚àó
2 ;
if the Tj‚Äôs are closable, so is T1 ‚äóT2 and
T1 ‚äóT2 ‚äÉT1 ‚äóT2. One has (T1 + S1) ‚äó
T2 = T1 ‚äóT2 + S1 ‚äóT2; however, for the
product, in general, the inclusion (T1S1) ‚äó
(T2S2) ‚äÇ(T1 ‚äóT2)(S1 ‚äóS2) holds only.
We have mentioned the use of Hilbert-
space tensor products to describe com-
posite quantum systems. If a self-adjoint
operator A describes an observable of a
subsystem with state space Óà¥1 and the
complement state space Óà¥2, then the same
observable related to the composite sys-
tem is described by the operator A ‚äóI
on Óà¥1 ‚äóÓà¥2, where I is the unit opera-
tor. For example, the Ô¨Årst momentum com-
ponent of a three-dimensional particle is
described by the operator P1 = P ‚äóI ‚äóI
on L2(‚Ñù3). In view of (13.3), we can express
it alternatively as P1 = F‚àí1
3 Q1F3, where Q1
is related in the same way to the operator Q
on L2(‚Ñù) and F3 = F ‚äóF ‚äóF is the three-
dimensional Fourier‚ÄìPlancherel operator.
In addition to tensor products them-
selves, some operators constructed from
them are of importance. Given a pair of
self-adjoint operators Aj on Óà¥j, we consider
operators of the polynomial form,
P[A1, A2] =
n1
‚àë
k=0
n2
‚àë
l=0
akl(Ak
1 ‚äóAl
2)
(13.18)
with real-valued coeÔ¨Écients akl. Without
loss of generality, we may suppose that the
senior coeÔ¨Écient an1n2 is nonzero and take
the linear hull of D(An1
1 ) √ó D(An2
2 ) as the
domain of P[A1, A2]; one can check that
such an operator is e.s.a.
A frequently occurring example is that of
self-adjoint operators of the form A1 + A2,
where A1 ‚à∂= A1 ‚äóI2 and A2 ‚à∂= I1 ‚äóA2,
which describe sums of observables related
to the respective subsystems. In the above
example of a three-dimensional quantum
particle, the operator H0 = P2
1 + P2
2 + P2
3 on
L2(‚Ñù3) describes the kinetic energy (up to
a multiplicative factor); we note that it is
e.s.a. on Óàø(‚Ñù3) and acts on its elements as
the negative Laplacian, H0ùúì= ‚àíŒîùúì.
13.4.4
Self-Adjoint Extensions
If A‚Ä≤ is a symmetric extension of an
operator A ‚ààÓà∏(Óà¥), we have A ‚äÇA‚Ä≤ ‚äÇ
(A‚Ä≤)‚àó‚äÇA‚àó.
One
naturally
asks
under
which conditions one can close the gap
by choosing a self-adjoint A‚Ä≤. To show

13.4 Unbounded Operators
461
that there are operators for which self-
adjoint extensions may not exist, consider
again ùúìÓÇ∂‚Üí‚àíiùúì‚Ä≤, this time on the halÔ¨Çine
‚Ñù+ = (0, ‚àû). We denote by ÃÉP such an oper-
ator deÔ¨Åned on all ùúì‚àà(L2 ‚à©L1)(‚Ñù+) and
by P0 its restriction to functions satisfying
ùúì(0+) = 0. Using integration by parts, it is
easy to see that P‚àó
0 = ÃÉP, and furthermore,
that P0 is symmetric, while ÃÉP is not; there
is obviously no self-adjoint P such that
P0 ‚äÇP ‚äÇÃÉP would hold.
To solve the problem in its general-
ity, we introduce deÔ¨Åciency subspaces of
an operator T to be Ker(T‚àó‚àíùúÜ), where
ùúÜis a Ô¨Åxed number from its regular-
ity
domain
as
Ker(T‚àó‚àíùúÜ).
The
map
ùúÜÓÇ∂‚Üídim Ker(T‚àó‚àíùúÜ) is constant on any
arcwise-connected component of the regu-
larity domain. In particular, for a symmetric
operator A any nonreal number belongs to
its regularity domain which thus the latter
has at most two connected components;
this allows us to deÔ¨Åne its deÔ¨Åciency indices
n¬±(A) = dim Ker(A‚àó‚àìi).
The deÔ¨Åciency indices in turn determine
whether self-adjoint extensions of a sym-
metric A exist. By the basic self-adjointness
criterion, we know that A is e.s.a. iÔ¨Ä
n¬±(A) = 0. If it is not the case, all symmet-
ric extensions of A can be parameterized,
according to the theory constructed by
John von Neumann, by isometric maps
from Ker(A‚àó‚àíi) to Ker(A‚àó+ i). Conse-
quently, nontrivial self-adjoint extensions
exist iÔ¨Äthe deÔ¨Åciency indices are nonzero
and equal to each other, n+(A) = n‚àí(A). If
both of them are Ô¨Ånite, any maximal sym-
metric extension is symmetric, otherwise
there may exist maximal extensions that
are not self-adjoint.
Returning to the example, it is now easy
to see why there is no self-adjoint momen-
tum operator on L2(‚Ñù+); it follows from
the fact that the deÔ¨Åciency indices of P0
are (1, 0). On the other hand, the operator
ùúìÓÇ∂‚Üí‚àíiùúì‚Ä≤ on L2(Óàµ), where Óàµ= (a, b) is a
Ô¨Ånite interval, deÔ¨Åned on functions ùúì‚àà
L2 ‚à©L1 satisfying ùúì(a+) = ùúì(b‚àí) = 0 has
deÔ¨Åciency indices (1, 1) and thus a family
of self-adjoint extensions. We can denote
them by PùúÉas each of them can be charac-
terized by the boundary condition ùúì(b‚àí) =
eiùúÉùúì(a+) for some ùúÉ‚àà[0, 2ùúã).
Characterizing
self-adjoint
extensions
by means of boundary conditions is com-
mon for diÔ¨Äerential operators. As another
example, let us mention again a one-
dimensional particle on a halÔ¨Çine. While
its momentum operator does not exist,
the operator of kinetic energy does, act-
ing as ùúìÓÇ∂‚Üí‚àíùúì‚Ä≤‚Ä≤ modulo a multiplicative
constant. If we choose for its domain all
ùúì‚ààL2(‚Ñù+) such that ùúì(0+) = ùúì‚Ä≤(0+) = 0,
we obtain a symmetric operator with
deÔ¨Åciency indices (1, 1). Its self-adjoint
extensions TùúÜare characterized by the
condition ùúì‚Ä≤(0+) ‚àíùúÜùúì(0+) = 0 for ùúÜ‚àà‚Ñù,
or by the Dirichlet condition ùúì(0+) = 0,
which formally corresponds to ùúÜ= ‚àû.
This example is a particular case of
one-dimensional Schr√∂dinger operator on
L2(Óàµ), where the interval Óàµcan be Ô¨Ånite,
semiÔ¨Ånite, or Óàµ= ‚Ñù; such an operator acts
as ùúìÓÇ∂‚Üí‚àíùúì‚Ä≤‚Ä≤ + Vùúì, where the potential
V is a locally integrable function. Asking
whether this formal diÔ¨Äerential operator
can be made self-adjoint, one has to inspect
solutions to the equation
‚àíùúì‚Ä≤‚Ä≤(x) + V(x)ùúì(x) = ùúÜùúì(x) ,
ùúÜ‚àà‚ÑÇ.
(13.19)
Theorem 13.11 (Weyl alternative) At
each end point of Óàµ, just one of the following
possibilities is valid: (i) limit-circle case: for
any ùúÜ‚àà‚ÑÇall solutions are L2 in the vicinity
of the end point, or (ii) limit-point case:
for any ùúÜ‚àà‚ÑÇthere is at least one solution
which is not L2 in the vicinity of the end
point.

462
13 Functional Analysis
If one or both end points of Óàµare of the
limit-circle type, one has to impose bound-
ary conditions there to make the operator
self-adjoint.
A useful tool to Ô¨Ånd relations between
self-adjoint extensions of a given symmetric
operator is the Krein formula: if the max-
imal common part A of self-adjoint oper-
ators A1, A2 has deÔ¨Åciency indices (n, n),
then the relation
(A1 ‚àíz)‚àí1 ‚àí(A2 ‚àíz)‚àí1
=
n
‚àë
j,k=1
ùúÜjk(z)(yk(z), ‚ãÖ)yj(z)
(13.20)
holds for any z ‚ààùúå(A1) ‚à©ùúå(A2), where
the matrix (ùúÜjk) is nonsingular and yj(z),
j = 1, ‚Ä¶ , n, are linearly independent vec-
tors from Ker(A‚àó‚àíz); the functions ùúÜjk(‚ãÖ)
and yj(‚ãÖ) can be chosen to be analytic
in
ùúå(A1) ‚à©ùúå(A2).
Kreins‚Äôs
formula
has
numerous applications, in particular, when
constructing and analyzing solvable models
of quantum systems [7].
13.5
Spectral Theory of Self-Adjoint Operators
It is well known from linear algebra that
any symmetric matrix possesses a unique
Ô¨Ånite family of eigenvalues and that the cor-
responding eigenvectors can be chosen to
form an orthonormal basis in the appropri-
ate vector space. Now we are going to show
how these properties extend to self-adjoint
operators on an arbitrary Hilbert space.
13.5.1
Functional Calculus
Our goal here is to deÔ¨Åne spectral decom-
position of an operator. To that end, we Ô¨Årst
have to introduce integration with respect
to a particular type of operator measures.
They are deÔ¨Åned on a set X equipped with
a family (a ùúé-Ô¨Åeld, to be exact) of measur-
able subsets; typically, we have in mind ‚Ñù,
the real line, and the family ÓàÆof all its Borel
subsets.
Given a Hilbert space Óà¥, we introduce
a projection-valued (or spectral) measure,
a map that associates with any measurable
set M a projection E(M) ‚ààÓàÆ(Óà¥) such that
E(X) = I and the map is ùúé-additive, that
is, for any at most countable disjoint fam-
ily {Mn} of measurable sets, the relation
E( ‚ãÉ
n Mn
) = ‚àë
n E(Mn) is valid; as a conse-
quence of additivity, we have E(‚àÖ) = 0.
If X = ‚Ñùd, one can construct spec-
tral measures starting from projections
assigned to rectangular sets ‚Äì in particular,
intervals if d = 1 ‚Äì in a way fully similar
to that used in the conventional measure
theory. With a spectral measure E on the
real line, one can associate also a spec-
tral decomposition as a right-continuous
map ùúÜÓÇ∂‚ÜíEùúÜ, which is nondecreasing and
satisÔ¨Åes the relations
s lim
ùúÜ‚Üí‚àí‚àûEùúÜ= 0 ,
s lim
ùúÜ‚Üí+‚àûEùúÜ= I,
(13.21)
where the strong convergence means that
EùúÜx converges to 0 or I, respectively, for
any x ‚ààÓà¥; the relation between the two
notions is given by EùúÜ= E((‚àí‚àû, ùúÜ]).
On
a
Ô¨Ånite-dimensional
Óà¥,
any
spectral
measure
is
supported
by
a
Ô¨Ånite set of points, {ùúÜj} ‚äÇ‚Ñù, so that
E(M) = E(M ‚à©{ùúÜj}) holds for any mea-
surable M; the corresponding spectral
decomposition is the steplike function
EùúÜ= E({ùúÜj ‚à∂ùúÜj ‚â§ùúÜ}).
Another
simple
example is the spectral measure EQ of
the multiplication operator Q on L2(‚Ñù)
introduced in Section 13.4.1. It acts as mul-
tiplication by the characteristic function,
EQ(M)ùúì= ùúíMùúì, for any M ‚ààÓàÆ.
Having deÔ¨Åned the spectral measure,
we are able to introduce integration with

13.5 Spectral Theory of Self-Adjoint Operators
463
respect to it as the map which assigns to a
measurable function f on X the operator
ÓâÄ(f ) = ‚à´X
f (ùúÜ) dE(ùúÜ)
(13.22)
on the corresponding Hilbert space Óà¥. To
deÔ¨Åne the action of ÓâÄ(f ), we use a construc-
tion analogous to the one employed in the
usual calculus. First, we deÔ¨Åne the integral
on simple functions having a Ô¨Ånite num-
ber of values, f = ‚àë
j ùúÜjùúíMj for a Ô¨Ånite fam-
ily {Mj} of measurable sets; we set ÓâÄ(f ) ‚à∂=
‚àë
j ùúÜjE(Mj). In the next step, we use the fact
that any bounded measurable function f
can be approximated by simple functions
{fn} in the sense that ‚Äñf ‚àífn‚Äñ‚àû‚Üí0 as n ‚Üí
‚àû, which allows us in the next step to deÔ¨Åne
the operator ÓâÄ(f ) ‚à∂= limn‚Üí‚àûÓâÄ(fn) for such
functions, where the convergence is under-
stood in the sense of operator norm.
The
map
f ÓÇ∂‚ÜíÓâÄ(f )
introduced
in
this
way
is
linear
and
multiplicative,
ÓâÄ(fg) = ÓâÄ(f )ÓâÄ(g) = ÓâÄ(g)ÓâÄ(f ). Denoting by
L‚àû(X, dE) the family of functions bounded
a.e. with respect to the measure E, we have
‚ÄñÓâÄ(f )‚Äñ = ‚Äñf ‚Äñ‚àûand ÓâÄ(f )‚àó= ÓâÄ(f ) for any
such function. Moreover, if a sequence
{fn} ‚äÇL‚àû(X, dE) converges pointwise to a
function f and the set {‚Äñfn‚Äñ‚àû} is bounded,
one has f ‚ààL‚àû(X, dE) and the limit can be
interchanged with the integral.
Integrating
unbounded
functions
is
more involved. First, we notice that for any
spectral measure E and x ‚ààÓà¥the relation
ùúáx(‚ãÖ) = (x, E(‚ãÖ)x) deÔ¨Ånes a numerical mea-
sure on X. Using it, we can associate with a
measurable function f the set
Df ‚à∂=
{
x ‚ààÓà¥‚à∂‚à´X
|f (ùúÜ)|2 dùúáx(ùúÜ) < ‚àû
}
,
(13.23)
which is dense in Óà¥. Furthermore, to
the
function
f
one
can
construct
a
sequence
{fn} ‚äÇL‚àû(X, dE)
such
that
fn(ùúÜ) ‚Üíf (ùúÜ) and |fn(ùúÜ)| ‚â§|f (ùúÜ)| holds E-
a.e. in X. We use it to deÔ¨Åne the operator
ÓâÄ(f ) = ‚à´X f (ùúÜ) dE(ùúÜ) by
ÓâÄ(f )x = lim
n‚Üí‚àûÓâÄ(fn)x ,
x ‚ààDf ;
(13.24)
the chosen domain is natural because
‚ÄñÓâÄ(f )x‚Äñ2 = ‚à´X |f (ùúÜ)|2 dùúáx(ùúÜ).
The map f ÓÇ∂‚ÜíÓâÄ(f ) is homogeneous and
ÓâÄ(f + g) ‚äÉÓâÄ(f ) + ÓâÄ(g); similarly, for the
multiplication of functions, we have in
general ÓâÄ(fg) ‚äÉÓâÄ(f )ÓâÄ(g). It also holds
that ÓâÄ(f ) = ÓâÄ(g) implies that f (ùúÜ) = g(ùúÜ)
holds almost everywhere with respect
to the measure E. Furthermore, we have
ÓâÄ(f )‚àó= ÓâÄ(f ) as in the bounded case
and the relation ÓâÄ(f )‚àí1 = ÓâÄ(f ‚àí1) holds
provided F is nonzero E-a.e.
The operator ÓâÄ(f ) deÔ¨Åned by (13.24) is
normal for any measurable f , it is self-
adjoint if f (t) is real with a possible excep-
tion of an E-zero measure set. The function
f also determines the spectrum ùúé(ÓâÄ(f ))
that coincides with the essential range of f
with respect to the measure E, as deÔ¨Åned
in Section 13.4.2. In particular, ùúÜ‚àà‚ÑÇis an
eigenvalue of ÓâÄ(f ) iÔ¨ÄE(f (‚àí1)({ùúÜ})) ‚â†0 and,
in this case, Ran E(f (‚àí1)({ùúÜ})) is the corre-
sponding eigenspace.
13.5.2
Spectral Theorem
Armed with the notions introduced above,
we are ready to state the fundamental struc-
tural result about self-adjoint operators.
Theorem 13.12 (The spectral theorem)
To any self-adjoint operator A on a Hilbert
space Óà¥, there is a unique spectral measure
EA on Óà¥such that
A = ‚à´‚Ñù
ùúÜdEA(ùúÜ) .
(13.25)

464
13 Functional Analysis
Furthermore, a bounded operator B com-
mutes with A, BA ‚äÇAB, iÔ¨Äit commutes
with its spectral decomposition, that is, with
E(A)
ùúÜ
= EA((‚àí‚àû, ùúÜ]) for any ùúÜ‚àà‚Ñù.
The second claim allows us to specify the
meaning of commutativity, which is, in gen-
eral, not easy to introduce if both the oper-
ators involved are unbounded. If A1, A2 are
self-adjoint we say that they commute if any
element of the spectral decompositions of
A1 commutes with any element of the spec-
tral decompositions of A2.
The spectral theorem has various corol-
laries and modiÔ¨Åcations. For instance, a
bounded normal operator, which we can
write as B = A1 + iA2 with commuting Her-
mitean A1, A2, can be expressed as B =
‚à´‚ÑÇz dF(z), where F is the projection-valued
measure on ‚ÑÇobtained as the product mea-
sure of EA1 and EA2. Furthermore, with a
unitary operator U, one can associate a
unique spectral measure EU with the sup-
port in the interval [0, 2ùúã) ‚äÇ‚Ñùsuch that
U = ‚à´‚Ñù
eiùúÜdEU(ùúÜ) .
(13.26)
These claims are used in one of the pos-
sible ways to prove the spectral theorem, in
which one checks its validity subsequently
for Hermitean, bounded normal, and uni-
tary operators, passing Ô¨Ånally to the gen-
eral case through the appropriate ‚Äúchange
of variables‚Äù in the integral.
As an example of the decomposition
(13.26),
let
us
mention
the
Fourier‚Äì
Plancherel operator (13.3). It acts on the
elements of the orthonormal basis (13.7) as
Fhn = (‚àíi)nhn, n = 0, 1, ‚Ä¶ , hence its spec-
trum is pure point, ùúé(F) = {1, ‚àíi, ‚àí1, i}.
The spectral measure can be in this case
expressed explicitly,
EF(M) = 1
4
3
‚àë
j,k=0
ùúíM
(ùúãk
2
)
(‚àíi)jkFj .
(13.27)
The spectral theorem provides a tool to
describe and classify spectra of self-adjoint
operators. To begin with, a real ùúÜbelongs to
ùúé(A) iÔ¨ÄEA(ùúÜ‚àíùúñ, ùúÜ+ ùúñ) ‚â†0 for any ùúñ> 0;
it is an eigenvalue iÔ¨Äthe point ùúÜitself has
a nonzero EA measure. In particular, any
isolated point of the spectrum is an eigen-
value. The results of the previous section
imply that the spectrum of a self-adjoint A
is always nonempty and that such an oper-
ator is bounded iÔ¨Äits spectrum is bounded.
The essential spectrum of a closed oper-
ator has been deÔ¨Åned in Section 13.4.1.
If
A
is
self-adjoint,
ùúÜ‚ààùúéess(A)
iÔ¨Ä
dim Ran EA(ùúÜ‚àíùúñ, ùúÜ+ ùúñ) = ‚àû
holds
for
any ùúñ> 0 and the essential spectrum is a
closed set. Points of ùúéess(A) fall into three,
mutually nonexclusive categories: such a
ùúÜcan belong to ùúéc(A), be an eigenvalue of
inÔ¨Ånite multiplicity, or an accumulation
point of eigenvalues. The complement of
ùúéess(A) consists of isolated eigenvalues
of Ô¨Ånite multiplicity; it is a subset of the
point spectrum for which we use the term
discrete spectrum.
We have seen in Section 13.3.3 that the
spectrum of compact operators away from
zero consists of isolated eigenvalues of
Ô¨Ånite multiplicity. This is the reason why
the essential spectrum of a self-adjoint A is
stable with respect to compact perturba-
tions; this fact is usually referred to as the
Weyl theorem. It can be substantially gen-
eralized. Given a self-adjoint A, an operator
T is called A-compact if D(T) ‚äÉD(A) and
T(A ‚àíi)‚àí1 is compact; this property again
guarantees spectral stability.
Theorem 13.13 (Generalized Weyl theo-
rem) ùúéess(A + T) = ùúéess(A) holds if the
operator T is symmetric and A-compact.
Using the spectral theorem, we can
introduce also another classiÔ¨Åcation of the
spectrum. We call Óà¥ac(A) the subspace of

13.5 Spectral Theory of Self-Adjoint Operators
465
all x ‚ààÓà¥such that ùúáx(N) = 0 holds any
Borel set N of zero Lebesgue measure, in
other words, such that the measure ùúáx is
absolutely continuous with respect to the
Lebesgue measure. The orthogonal com-
plement of Óà¥ac(A) is denoted by Óà¥s(A). The
projections to these subspace commute
with the operator A, so we can write it
as A = Aac ‚äïAs, its spectrum being the
union of the component spectra, which
we call ùúéac(A), the absolutely continuous
spectrum of A, and ùúés(A), the singular one,
respectively.
Furthermore, the comple-
ment ùúésc(A) ‚à∂= ùúés(A) ‚ßµùúép(A) is called the
singularly continuous spectrum of A; any
of these components may be nonempty if
dim Óà¥= ‚àû.
Before closing this section, let us recall
the role that spectral analysis plays in quan-
tum physics. As we have said, one asso-
ciates a self-adjoint operator A with any
observable of a quantum system. In con-
trast to classical physics, measurement has
a probabilistic character here: its possi-
ble outcomes coincide with the spectrum
of A and the probability of Ô¨Ånding the
measured value in a set M ‚äÇ‚Ñùis given
by w(M, A; W) = Tr(EA(M)W) if the state
of the system before the measurement is
described by a density matrix W, in partic-
ular, by
w(M, A; ùúì) = ‚à´M
d(ùúì, EA(ùúÜ)) = ‚ÄñEA(M)ùúì‚Äñ2
(13.28)
if the state is pure being described by a unit
vector ùúì‚ààÓà¥. The postulate makes sense;
it is easy to check that Tr (EA(‚ãÖ)W) is a
probability measure on ‚Ñù.
Measurement
of
this
type
can
be
regarded
as
the
simplest,
dichotomic
observables with two possible outcomes,
positive (the observed value is found in
M) and negative (it is found outside M);
one often uses the term yes‚Äìno experiment
for them. Using this notion, we can also
describe what happens with the system
after the measurement. If the outcome is
positive, the resulting state is described
by
the
vector
EA(M)ùúì‚àï‚ÄñEA(M)ùúì‚Äñ,
or
more generally by the density matrix
EA(M)WEA(M)‚àïTr(EA(M)W);
in
the
negative case, we replace M by ‚Ñù‚ßµM.
13.5.3
More about Spectral Properties
Functional calculus allows us to deÔ¨Åne
functions of a self-adjoint operator A nat-
urally using its spectral measure by the
relation
f (A) ‚à∂= ÓâÄ(f ) = ‚à´‚Ñù
f (ùúÜ) dEA(ùúÜ) .
(13.29)
The term ‚Äúfunction‚Äù has to be taken with
a grain of salt here because it is f that is
the ‚Äúvariable‚Äù in the above formula. A sim-
ple example is f (Q) on L2(‚Ñù), which acts as
(f (Q)ùúì) = f (x)ùúì(x) on L2 functions satis-
fying ‚à´‚Ñù|f (x)ùúì(x)|2 dx < ‚àû. The deÔ¨Ånition
(13.29) is consistent in the sense that for ele-
mentary functions such as polynomials, it
gives the same result as one would obtain
without using the spectral theory.
A slightly more involved example con-
cerns functions of the momentum operator
P on L2(‚Ñù). One can use the unitary equiv-
alence (13.17) to express their action. For
instance, for the exponential function, one
obtains the unitary operators
(
eiaPùúì)
(x) = ùúì(x + a) ,
a ‚àà‚Ñù,
(13.30)
acting as the translation group in L2(‚Ñù), and
for f ‚ààL2(‚Ñù) we get the expression
(
f (P)ùúì)
(x) =
1
‚àö
2ùúã‚à´‚Ñù
(Ff )(y ‚àíx)ùúì(y) dy
(13.31)

466
13 Functional Analysis
with the integral kernel given by the
Fourier‚ÄìPlancherel image of f .
The
limiting
property
of
sequences
{ÓâÄ(fn)} mentioned above can be used
to derive an expression of the spectral
measure in terms of the resolvent RA(z) =
(A ‚àíz)‚àí1. SpeciÔ¨Åcally, because the func-
tion
arctan (b ‚àíùúÜ‚àïùúñ) ‚àíarctan (a ‚àíùúÜ‚àïùúñ)
for Ô¨Åxed a < b tends to 1
2[ùúí[a,b] + ùúí(a,b)] as
ùúñ‚Üí0, we obtain the relation
EA([a, b]) + EA((a, b))
= 1
ùúãi s lim
ùúñ‚Üí0+ ‚à´
b
a
[RA(ùúÜ+ iùúñ) ‚àíRA(ùúÜ‚àíiùúñ)] dùúÜ,
(13.32)
which is known as the Stone formula. If
ùúép(A) = ‚àÖ, there is no diÔ¨Äerence between
EA([a, b]) and EA((a, b)). The behavior of
the integrand in the vicinity of the real axis
determines also more subtle spectral prop-
erties, in particular, if
sup
0<ùúñ<1 ‚à´
b
a
||Im (ùúì, RA(ùúÜ+ iùúñ)ùúì)||
p dùúÜ< ‚àû
(13.33)
holds for some p > 1 and ùúì‚ààÓà¥, then
EA((a, b))ùúìbelongs to Óà¥ac(A), the abso-
lutely continuous spectral subspace of the
operator A.
In Section 13.5.2, we have mentioned
how a spectral measure on ‚ÑÇcan be asso-
ciated with a bounded normal operator.
In a similar way, one can treat a Ô¨Ånite
family of commuting self-adjoint operators
A1, ‚Ä¶ , An. One can construct a projection-
valued measure E on ‚Ñùn as the product
measure of EA1, ‚Ä¶ , EAn. This allows us
to deÔ¨Åne functions of such a family of
commuting operators by
f (A1, ‚Ä¶ , An) ‚à∂= ‚à´‚Ñùn f (ùúÜ1, ‚Ä¶ , ùúÜn)
dE(ùúÜ1, ‚Ä¶ , ùúÜn) ;
(13.34)
it is easy to see that such an operator com-
mutes with all the A1, ‚Ä¶ , An.
In
quantum
physics,
observables
described
by
commuting
self-adjoint
operators are called compatible; they can
be measured simultaneously, which means
we can measure them in any order provided
the measurements follow immediately after
each other. The probability of Ô¨Ånding the
measured values in the set M ‚äÇ‚Ñùn is
w(M, {A1, ‚Ä¶ , An}; W) = Tr(EA(M)W)
(13.35)
if the system before the measurement was
a mixed state described by a density matrix
W. One can imagine that compatible
observables are measured by a single appa-
ratus. It can also be used to measure their
functions (13.34) after a proper rescaling;
the appropriate probability equals
w(M,f (A1, ‚Ä¶ , An); W)
= w(f (‚àí1)(M), {A1, ‚Ä¶ , An}; W) ,
(13.36)
assuming again that the initial state of the
system is described by a density matrix W.
A particular case of commuting self-
adjoint operators are those which can
be expressed through tensor products
as described in Section 13.4.3. The oper-
ators
A1 = A1 ‚äóI2
and
A2
commute
with each other and ùúé(Aj) = ùúé(Aj) holds
for
j = 1, 2.
The
self-adjoint
operator
P[A1, A2] corresponding to (13.18) coin-
cides with P(A1, A2) deÔ¨Åned according
to (13.34) and its spectrum is given by
ùúé(P[A1, A2]) = P(ùúé(A1) √ó ùúé(A2)).
For example, the measured values of
the total energy of a system consist-
ing
of
two
noninteracting
subsystems
are sums of measured values of the
corresponding
subsystem
energies,
i.e.
ùúé(H1 + H2) = {ùúÜ1 + ùúÜ2 ‚à∂ùúÜj ‚ààùúé(Hj)}.

13.5 Spectral Theory of Self-Adjoint Operators
467
13.5.4
Groups of Unitary Operators
Let us now consider families {U(s) ‚à∂s ‚àà
‚Ñù} of unitary operators on a given Óà¥such
that the map s ÓÇ∂‚ÜíU(s) is strongly contin-
uous, that is, U(‚ãÖ)x is continuous for any
x ‚ààÓà¥, and the group property is satisÔ¨Åed,
U(t + s) = U(t)U(s) for any t, s ‚àà‚Ñù. With
such a group, one can associate its generator
acting as
Tx = lim
s‚Üí0
U(s) ‚àíI
is
x
(13.37)
on the domain consisting of all x ‚ààÓà¥for
which the limit exists. It is not diÔ¨Écult to
check that {eisA ‚à∂s ‚àà‚Ñù} corresponding to
a self-adjoint A is a strongly continuous
unitary group and A is its generator; a deep
result says that the converse is also true.
Theorem 13.14 (Stone theorem) To
any strongly continuous unitary group
{U(s) ‚à∂s ‚àà‚Ñù} there is a unique self-
adjoint operator A such that U(s) = eisA
holds for any s ‚àà‚Ñù.
The theorem has various consequences.
Using functional calculus, for instance, it
yields an alternative expression of com-
mutativity: self-adjoint operators A1, A2
commute iÔ¨Ä[eisA1, eitA2] = 0 holds for all
s, t ‚àà‚Ñù.
Elementary examples are operators by
eisx on L2(‚Ñù) generated by the operator
Q, or the group of translations of L2(‚Ñù)
which is in view of (13.30) generated by the
momentum operator P. Another example
is provided by the group of dilations
associated with scaling of the real axis,
(Ud(s)ùúì)(x) = es‚àï2ùúì(esx) on L2(‚Ñù), which
is generated by the symmetrized product
Ad = 1
2 PQ + QP.
Unitary groups arise in various contexts
in quantum physics. The most important
are the operators U(t) = e‚àíitH, where H is
the operator of total energy, or Hamilto-
nian, which describe the time evolution of
a conservative system (in the units where
‚Ñè= 1). If such a system is undisturbed by
measurements, the state vector ùúì0 at the
initial time instant t = 0 is mapped to ùúìt =
U(t)ùúì0 at the time t. The diÔ¨Äerential form
of the evolution is known as the Schr√∂dinger
equation,
i d
dt ùúìt = Hùúìt
(13.38)
with the initial condition ùúì0 ‚ààD(H), or
alternatively id‚àïdtWtùúô= [H, Wt]ùúôfor a
mixed state described by density matrix
Wt.
If Hermitean operators A, B commute,
then the products eisAeisB form a strongly
continuous unitary group and A + B is its
generator. This need not be true if the oper-
ators are unbounded but the conclusion
still makes sense in the functional-calculus
sense. If A, B do not commute, the products
may not form a group; however, we have a
limiting relation called the Trotter formula:
if C = A + B is e.s.a., then
eitC = s lim
n‚Üí‚àû
(
eitA‚àïneitB‚àïn)n .
(13.39)
This result is useful because often we
have operators representing observables
that are sums of self-adjoint parts, and
we know explicit expressions of unitary
groups of the latter. A prime example is
that of quantum-mechanical Hamiltonians
of the form ‚àíŒî + V(x) where Trotter‚Äôs
formula provides a way to express the
corresponding evolution operator through
the Feynman path integral [1, Chap. X; 4].
If Uj(‚ãÖ) is a strongly continuous unitary
group on Óà¥j with the generator Aj, j = 1, 2,
then the operators U1(s) ‚äóU2(s) also form
such a group and its generator is A1 + A2.
For instance, the evolution operator of

468
13 Functional Analysis
a system consisting of two noninterac-
tion subsystems with Hamiltonians Hj is
e‚àíitH1 ‚äóe‚àíitH2. This conclusion extends
easily to any Ô¨Ånite number of subsystems;
it naturally ceases to be valid if the subsys-
tems interact so that the total Hamiltonian
is no longer of the form H1 + H2.
In a similar way, one can take ten-
sor products of unitary group elements
referring to diÔ¨Äerent values of the param-
eters involved. For example, the operators
U(s) = eis1P1 ‚äó¬∑ ¬∑ ¬∑ ‚äóeisnPn
on
L2(‚Ñùn),
where
s = (s1, ‚Ä¶ , sn),
form
the
group
of
translations
of
the
n-dimensional
Euclidean
space
generalizing
relation
(13.30),
(U(s)ùúì)(x) = ùúì(x + s)
for
any
x = (x1, ‚Ä¶ , xn) ‚àà‚Ñùn. By means of the n-
dimensional Fourier‚ÄìPlancherel operator,
these
operators
are
unitarily
equiva-
lent
to
V(s)ùúì)(x) = eis‚ãÖxùúì(x),
where
s ‚ãÖx = s1x1 + ¬∑ ¬∑ ¬∑ + snxn is the inner product
in ‚Ñùn.
The unitary groups mentioned in the
last example are associated with impor-
tant quantum-mechanical variables, coor-
dinates of the position and momentum. It
is easy to check that their products in a dif-
ferent order diÔ¨Äer by an exponential factor,
U(t)V(s) = eis‚ãÖtV(s)U(t) ,
s, t ‚àà‚Ñùn.
(13.40)
The relations (13.40) can be regarded a
mathematically rigorous form of canonical
commutation relations, often referred to as
their Weyl form; the operators U(t), V(s)
on L2(‚Ñùn) described in the previous para-
graph deÔ¨Åne the so-called Schr√∂dinger rep-
resentation of (13.40). A fundamental ques-
tion concerns the uniqueness of this rep-
resentation; we ask, of course, about irre-
ducible representations for which no non-
trivial projection commutes with all the
operators.
Theorem 13.15 (Stone‚Äìvon
Neumann
theorem) Any irreducible (unitary, strongly
continuous) representation of the Weyl
relations (13.40) is unitarily equivalent
to the Schr√∂dinger representation of the
corresponding dimension.
The reader should be warned that an
analogue of this theorem in situations with
inÔ¨Ånite number of degrees of freedom,
that is, when ‚Ñùn is replaced with a real
Hilbert space of inÔ¨Ånite dimension, is not
valid. Indeed, in quantum Ô¨Åeld theory one
can Ô¨Ånd examples with inÔ¨Ånite number
of inequivalent representations of such
generalized Weyl relations.
13.6
Some Applications in Quantum Mechanics
We have mentioned already some ways
in which functional-analytic notions and
results are used in quantum physics. In the
last section of this chapter, we will brieÔ¨Çy
describe two speciÔ¨Åc applications. We do it
mostly to whet the reader‚Äôs appetite; there is
a large number of related results for which
we refer to the literature indicated at the
end of the chapter.
13.6.1
Schr√∂dinger Operators
In nonrelativistic quantum mechanics, the
Hamiltonian of a spinless quantum particle,
or a system of such particles, is often of the
form
H = ‚àíŒî + V(x)
(13.41)
on L2(‚Ñùn). In general, the expression
involves nontrivial coeÔ¨Écients, in particu-
lar, the e.s.a. operator describing the kinetic
part is ‚àën
j=1(1‚àï2mj)P2
j ; however, one can
always put 2mj = 1 by using suitable units.
The Ô¨Årst question when analyzing oper-
ators (13.41) concerns their (essential)
self-adjointness. The answer is easy if the

13.6 Some Applications in Quantum Mechanics
469
potential V
is bounded; unfortunately,
most potentials we have to deal with in
actual physical models are unbounded.
One way to address the self-adjointness
problem uses perturbation theory; the
Kato‚ÄìRellich theorem can be used to make
the following conclusion:
Theorem 13.16 (SuÔ¨Écient self-adjoint-
ness condition) Assume that the potential
V ‚ààLp + L‚àû, that is, V = Vp + V‚àûwith
V‚àû‚ààL‚àû(‚Ñùn)
and
Vp ‚ààLp(‚Ñùn),
where
p = 2 if n ‚â§3 and p > 1
2n for n ‚â•4, then
the operator (13.41) is e.s.a. on any core of
H0 = ‚àíŒî.
This result is not immediately applica-
ble to operators (13.41) on L2(‚Ñù3N) describ-
ing systems on N particles, N > 1. In this
case, the interaction is typically a sum of
potentials with the property that there is
a three-dimensional projection E in ‚Ñù3N
such that V(x) = V(Ex). Such potentials
describe either one-particle forces when E
refers to coordinates of a single particle, or
two-particle ones when E refers to relative
coordinates of a pair of particles.
Theorem 13.17 (Kato theorem) Let
n =
3N and V = ‚àëm
k=1 Vk, where each potential
component Vk(Ek ‚ãÖ) ‚àà(L2 + L‚àû)(‚Ñù3); then
the operator (13.41) is e.s.a. on any core of
H0.
The main importance of this result is
that it guarantees self-adjointness of atomic
Hamiltonian with potentials of Coulomb
type for electron charge e,
V(x) = ‚àí
Z
‚àë
j=1
Ze2
|xj ‚àíx0| +
‚àë
1‚â§j<k‚â§Z
e2
|xj ‚àíxk|,
(13.42)
referring to a Ô¨Åxed nucleus of atomic num-
ber Z, and similar operators describing
atoms with a Ô¨Ånite nucleus mass as well as
molecules, atomic and molecular ions, and
so on; it means that the usual quantum-
mechanical description of such objects can
be justiÔ¨Åed from the Ô¨Årst principles.
Analyzing the discrete spectrum of oper-
ators (13.41), one is often interested in rela-
tions between the number and position of
the eigenvalues on the one hand and the
potential on the other. There are many such
results of which we will mention just two
important ones. A frequently used estimate
is expressed in the following way:
Theorem 13.18 (Birman‚ÄìSchwinger
bound) Consider the operator (13.41) on
L2(‚Ñù3) with the potential such that the
right-hand side of the relation (13.43) is
Ô¨Ånite; then the number of eigenvalues of H
is Ô¨Ånite and satisÔ¨Åes the inequality
N(V) ‚â§
1
16ùúã2 ‚à´‚Ñù6
|V(x)V(y)|
|x ‚àíy|2
dx dy .
(13.43)
Note that no similar bound exists for n =
1, 2 because there an arbitrarily weak neg-
ative potential produces a bound state, that
is, an isolated negative eigenvalue.
The bound (13.43) does not exhibit a cor-
rect semiclassical behavior, which means
that it becomes poor when we replace V
by gV and study the asymptotic behavior
as g ‚Üí‚àû. This is not case for the following
more general result.
Theorem 13.19 (Lieb‚ÄìThirring inequa-
lity)
Suppose
that
ùúéd(H) = {ùúÜj}
and
Ô¨Åx ùõæ‚â•0 for n ‚â•3, ùõæ> 0 for n = 2, and
ùõæ‚â•1
2 for n = 1; then the inequality
Tr (Hùõæ
‚àí) =
‚àë
j
(‚àíùúÜj)ùõæ‚â§Lùõæ,n ‚à´‚Ñùn V‚àí(x)ùõæ+ n
2 dx
(13.44)
holds,
where
Lùõæ,n ‚â•Lcl
ùõæ,n = Œì(ùõæ+ 1)
[2n
ùúãn‚àï2Œì(ùõæ+ n
2 + 1)]‚àí1
and
V‚àí(x) =
max{‚àíV (x), 0} is the negative part of the
potential.

470
13 Functional Analysis
In
fact,
the
constants
are
Lùõæ,n =
R(ùõæ, n)Lcl
ùõæ,n where R(ùõæ, n) = 1 for ùõæ‚â•3‚àï2,
R(ùõæ, n) ‚â§2 for 1 ‚â§ùõæ< 3‚àï2 or 1‚àï2 ‚â§ùõæ< 1
and n = 1, and R(ùõæ, n) ‚â§4 for 1‚àï2 ‚â§ùõæ< 1
and
n ‚â•2.
In
dimensions
n ‚â•3,
the
inequality holds for ùõæ= 0, which yields a
bound to the number of bound states,
N(V) ‚â§L0,n ‚à´‚Ñùn V‚àí(x)n‚àï2 dx,
(13.45)
known
as
the
Cwikel‚ÄìLieb‚ÄìRozeblium
theorem.
The number of bound states can be inÔ¨Å-
nite if the potential has a slow enough decay
at inÔ¨Ånity. Suppose that V = V2 + V‚àûwith
V2 ‚ààL2(‚Ñù3) and V‚àû‚ààL‚àû(‚Ñù3) such that
V‚àû(x) ‚Üí0 holds as |x| ‚Üí‚àû. This ensures
that ùúéess(H) = [0, ‚àû). If there are c ‚àà[0, 1
4
)
and r0 ‚â•0 such that V(x) ‚â•‚àíc|x|‚àí2 holds
|x| ‚â•r0, then ùúéd(H) is Ô¨Ånite. On the other
hand, the discrete spectrum is inÔ¨Ånite pro-
vided there are positive d, r0, ùúñsuch that
V(x) ‚â§‚àíd|x|‚àí2+ùúñholds |x| ‚â•r0.
Behavior of the potential at large dis-
tances is important in determining the
essential spectrum also more generally.
In some cases of physical importance,
this task is not easy; in particular, for
systems of N particles interacting through
two-particle forces associated with the
potential of the form
V(x1, ‚Ä¶ , xN) =
‚àë
1‚â§j<k‚â§Z
Vjk(xj ‚àíxk), (13.46)
where xj = (xj1, xj2, xj3) are coordinates of
the jth particle. We divide our N particles
into n(D) clusters Ci; if N = 2 there is only
one such partition, for N ‚â•2 there are
diÔ¨Äerent
partitions
D = {C1, ‚Ä¶ , Cn(D)}.
For each partition we deÔ¨Åne ùúñD
jk to be
one if the indices j, k belong to the same
cluster and zero otherwise. This allows
us to deÔ¨Åne the Hamiltonian with the
intercluster
interaction
switched
oÔ¨Ä,
HD = ‚àíŒî + ‚àëN
j<k=1 ùúñD
jk Vjk(xj ‚àíxk), and the
cluster Hamiltonians
HCi = ‚àíŒîCj +
N
‚àë
{j,k‚ààCj‚à∂j<k}
Vjk(xj ‚àíxk) ;
(13.47)
in fact, we are interested in the Hamilto-
nians Hrel
Ci with the center-of-mass motion
separated acting in L2(‚Ñù3ni‚àí3), where ni is
the number of particles in the cluster Ci.
Using them, we deÔ¨Åne for a given D the
quantity ùúÜD ‚à∂= ‚àën(D)
i=1 inf ùúé(Hrel
Ci
).
Theorem 13.20 (Hunziker‚Äìvan Winter‚Äì
Zhislin theorem) Suppose that the two-
particle potentials are Vjk = Vjk,2 + Vjk,‚àû
with Vjk,2 ‚ààL2(‚Ñù3) and Vjk,‚àû‚ààL‚àû(‚Ñù3)
such that Vjk,‚àû(x) ‚Üí0 holds as |x| ‚Üí‚àû;
then the essential spectrum of H is [Œõ, ‚àû)
where
Œõ ‚à∂= min
n(D)‚â•2 ùúÜD = min
n(D)=2 ùúÜD .
(13.48)
13.6.2
Scattering Theory
The second application we are going to dis-
cuss concerns one of the most frequently
occurring situations in physics when we
compare the dynamics of a quantum sys-
tem with an asymptotic one. Suppose that
we have a pair of Hamiltonians, the ‚Äúfull‚Äù
one H and the ‚Äúfree‚Äù one H0, together
with the corresponding evolution opera-
tors, U(t) = e‚àíitH and U0(t) = e‚àíitH0. The
question is how to Ô¨Ånd to a given ùúì‚àà
Óà¥vectors ùúì¬± such that the ‚Äútrajectories‚Äù
U(t)ùúìand U0(t)ùúì¬± coincide asymptotically
as t ‚Üí¬±‚àû.
Naturally, there may be vectors ùúìfor
which no ùúì¬± exist, for instance, bound
states of the system described by eigenvec-
tors of the operator H. The distinguishing
property is whether a state remains local-
ized or leaves a Ô¨Åxed space region. We

13.6 Some Applications in Quantum Mechanics
471
consider an increasing family {Mr ‚à∂r ‚â•0}
of subspaces of the conÔ¨Åguration space
and denote by Fr the respective projections
assuming that s limr‚Üí‚àûFr = I; for deÔ¨Å-
niteness, think of concentric balls of radius
r. Using these projections, we deÔ¨Åne the
family
Óàπs(H) =
{
ùúì‚ààÓà¥‚à∂lim
|t|‚Üí‚àûFrU(t)ùúì= 0
for any r > 0
}
(13.49)
of scattering states, and the complementary
family of bound states,
Óàπb(H) =
{
ùúì‚ààÓà¥‚à∂lim
r‚Üí‚àûsup
t‚àà‚Ñù
‚Äñ(I ‚àíFr)
U(t)ùúì‚Äñ = 0
}
.
(13.50)
It is not diÔ¨Écult to check that Óàπb(H) ‚äÉ
Óà¥p(H) and Óàπs(H) ‚äÉÓà¥c(H) where we use
the spectral subspaces notation introduced
in Section 13.5.2. We identify conven-
tionally the scattering states with the
absolutely continuous part of H, setting
Óàπs(H) ‚äÉÓà¥ac(H). This means to exclude
states associated with Óà¥sc(H) that typically
lie ‚Äúbetween‚Äù the bound and scattering
states: the probability of Ô¨Ånding them in a
Ô¨Åxed bounded region may not have zero
limit as |t| ‚Üí‚àûbut its mean value can be
made arbitrarily small when averaged over
a suÔ¨Éciently long time interval. One usu-
ally aims to prove that such pathological
states are absent, that is, ùúésc(H) = ‚àÖ.
In this setup, we can describe the asymp-
totic relation between the full and free
dynamics by a pair of wave operators
deÔ¨Åned by
Œ©¬±(H, H0) ‚à∂= s lim
t‚Üí¬±‚àûU(t)‚àóU0(t)Eac(H0),
(13.51)
which map Óà¥ac(H0) into Óà¥ac(H). In gen-
eral, it may happen that Ran Œ©¬±(H, H0) ‚â†
Óàπs(H), for instance, if the scattered parti-
cle is captured by the target and is unable to
leave the interaction region. The wave oper-
ators are said to be complete if
Ran Œ©+(H, H0) = Ran Œ©‚àí(H, H0) = Óà¥ac(H),
(13.52)
and asymptotically complete if, in addi-
tion, ùúésc(H) = ‚àÖ. Under the completeness
assumption, one can deÔ¨Åne, in particu-
lar, the scattering operator (or S-matrix),
S(H, H0) ‚à∂= Œ©+(H, H0)‚àóŒ©‚àí(H, H0), which
maps the ‚Äúincoming‚Äù asymptotic states into
the ‚Äúoutgoing‚Äù ones.
The wave operators are assigned to a pair
of operators. They satisfy the chain rule,
Œ©¬±(H, H0) = Œ©¬±(H, H1)Œ©¬±(H1, H0)
for
any
self-adjoints
H, H1, H0
for
which
the
wave
operators
exist.
Another
important property is the intertwining
relation,
Œ©¬±(H, H0)H0 ‚äÇHŒ©¬±(H, H0).
It implies, in particular, that the scat-
tering
operator
commutes
with
the
free
Hamiltonian,
or
equivalently,
U0(t)S(H, H0) = S(H, H0)U0(t)
holds
for all t ‚àà‚Ñù. As a consequence, it can be
expressed in the form of a direct integral,
S(H, H0) = ‚à´
‚äï
ùúéac(H0)
S(ùúÜ) dùúÜ;
(13.53)
for the Ô¨Åber operators S(ùúÜ), we usu-
ally employ the name on-shell scattering
matrix.
Many suÔ¨Écient conditions have been
derived for existence of wave operators.
Some are abstract and rather simple such
as the following one.
Theorem 13.21 (Kato‚ÄìRosenblum
theorem)
Suppose
that
H = H0 + V,
where H0 is self-adjoint and V is a Her-
mitean
trace-class
operator;
then
the
wave operators Œ©¬±(H, H0)H) exist and are
complete.

472
13 Functional Analysis
The range of applications of this result is
rather limited, because the interaction part
of the Hamiltonian in physical models is
rarely of a trace-class character. More often,
one can use the following related result.
Theorem 13.22 (Birman‚ÄìKuroda
theorem) Let H, H0 be self-adjoint and
the resolvent diÔ¨Äerence (H ‚àíz)‚àí1 ‚àí(H0 ‚àí
z)‚àí1 ‚ààÓà∂1(Óà¥) for some z ‚ààùúå(H) ‚à©ùúå(H0);
then the wave operators Œ©¬±(H, H0) exist
and are complete.
As regards Schr√∂dinger operators, one
is usually interested in scattering caused
by the potential; in other words, the situ-
ation where H is the operator (13.41) and
H0 = ‚àíŒî on L2(‚Ñùn). Various existence con-
ditions for the wave operators in terms of
the potential V can be derived, for instance,
Theorem 13.23 (Hack‚ÄìCook theorem)
Let H be the operator (13.41) on L2(‚Ñù3).
If the potential V ‚àà(L2 + Ls)(‚Ñù3) with
s ‚àà[2, 3),
then
the
wave
operators
Œ©¬±(H, H0) exist.
Completeness of the wave operators
requires
additional
hypotheses.
As
a
sample result we mention the following
suÔ¨Écient conditions.
Theorem 13.24 Let H be the operator
(13.41) on L2(‚Ñù3) with V ‚àà(L1 ‚à©L2)(‚Ñù3),
then the wave operators Œ©¬±(H, H0) exist
and are complete. If, in addition,
1
16ùúã2 ‚à´‚Ñù6
|V(x)V(y)|
|x ‚àíy|2
dx dy < 1
(13.54)
holds, then ùúésc(H) = ‚àÖ, so the wave opera-
tors are asymptotically complete.
The asymptotic completeness can be
established for many other scattering sys-
tems described by Schr√∂dinger operators.
In particular, it has been demonstrated
in the situation when (13.41) describes a
system of N particles interacting through
Coulomb potentials of the type (13.42),
hence scattering processes in atomic and
molecular physics can again be studied
from the Ô¨Årst principles.
Glossary
Banach space: a (metrically) complete
normed space
Commutativity of self-adjoint operators:
it means commutativity of their spectral
decompositions
Compact operator: it maps each bounded
set into a precompact one
DeÔ¨Åciency indices of a symmetric A: the
numbers dim Ker(A‚àó‚àìi)
DeÔ¨Åciency subspace of T: all solutions to
the equation T‚àóx = ùúÜx
Density
matrices:
positive
trace-class
operators normalized by Tr W = 1
Discrete spectrum: isolated eigenvalues of
Ô¨Ånite multiplicity
Essential spectrum: complement of the
discrete spectrum
Essentially self-adjoint operator: its clo-
sure is self-adjoint
Fourier
expansion:
expression
of
a
Hilbert-space vector with respect to an
orthonormal basis
Fourier‚ÄìPlancherel operator: the contin-
uous extension of Fourier transformation to
L2(‚Ñùn).
Function spaces: families of functions,
or equivalence classes of functions, with
pointwise deÔ¨Åned addition and multiplica-
tion
Hamiltonian:
an
operator
describing
energy of the system
Hermitean operator: a bounded self-
adjoint operator
Hilbert space: a (metrically) complete
inner-product space

References
473
Hilbert‚ÄìSchmidt operator: an operator
C such that ‚àë
j ‚ÄñCej‚Äñ2 < ‚àûholds for any
orthonormal basis {ej}
Inner product: a sesquilinear form on a
vector space such that (x, x) = 0 implies
x = 0
Limit point ‚Äì limit circle: alternatives
determining deÔ¨Åciency indices of a one-
dimensional Schr√∂dinger operator
Normal operator: it commutes with its
adjoint
Operator: a linear map between (subspaces
of) Banach (Hilbert) spaces
Operator with pure point spectrum: its
eigenvectors form an orthonormal basis
Parseval identity: it expresses the vector
norm through its Fourier coeÔ¨Écients
Point spectrum: eigenvalues of the opera-
tor
Projection: an operator assigning to any
vector x ‚ààÓà¥its orthogonal projection to a
given subspace Óà≥‚äÇÓà¥
Pure state of a quantum system: a one-
dimensional subspace in the state Hilbert
space of the system
Riesz lemma: an expression of a bounded
linear functional on a Hilbert space
Self-adjoint operator: it coincides with its
adjoint
Spectral measure: a measure the values of
which are projections on a given Hilbert
space
Spectrum of an operator T: those ùúÜfor
which the resolvent RT(ùúÜ) = (T ‚àíùúÜ)‚àí1 does
not exist as a bounded operator
Stone formula: expression of the spectral
measure in terms of the resolvent
Symmetric operator: it is a restriction of
its adjoint
Trace-class operator: an operator C such
that Tr |C| < ‚àû
Unitarily equivalent operators: they can
be mapped one to another with the help of
a unitary operator
Unitary
operator:
an
isometric
map
deÔ¨Åned on the whole Hilbert space
Wave
operators:
asymptotic
maps
between the full and free dynamics in
a scattering system
Weyl relations: expression of canonical
commutation relations in terms of the asso-
ciated unitary groups
Weyl theorem: stability of the essential
spectrum with respect to compact pertur-
bations
References
1. Reed, M. and Simon, B. (1972‚Äì1978)
Methods of Modern Mathematical Physics
I‚ÄìIV, Academic Press, New York.
2. Blank, J., Exner, P., and Havl√≠Àácek, M. (2008)
Hilbert Space Operators in Quantum
Physics, 2nd edn, Springer, Dordrecht.
3. Bratelli, O. and Robinson, D.W. (1979, 1981)
Operator Algebras and Quantum Statistical
Mechanics I, II, Springer-Verlag, New York.
4. Albeverio, S.A., H√∏egh-Krohn, R.J., and
Mazzucchi, S. (2008) Mathematical Theory
of Feynman Path Integrals, 2nd edn,
Springer, Berlin.
5. Adams, R.A. and Fournier, J.J.F. (2003)
Sobolev Spaces, 2nd edn, Elsevier, Oxford.
6. Akhiezer, N.I. and Glazman, I.M. (1993)
Theory of Linear Operators in Hilbert Space,
Dover, Mineola, NY.
7. Albeverio, S.A., Gesztesy, F., H√∏egh-Krohn,
R., and Holden, H. (2005) Solvable Models in
Quantum Mechanics, 2nd edn with appendix
by P. Exner, AMS Chelsea, Providence, RI.
8. Amrein, W.O., Jauch, J.M., and Sinha, K.B.
(1977) Scattering Theory in Quantum
Mechanics, Benjamin, Reading, MA.
9. Birman, M.≈†. and Solomyak, M.Z. (1987)
Spectral Theory of Self-Adjoint Operators in
Hilbert Space, Kluwer, Dordrecht.
10. Cycon, H.L., Froese, R.G., Kirsch, W., and
Simon, B. (2007) Schr√∂dinger Operators, with
Applications to Quantum Mechanics and
Global Geometry, 2nd printing, Springer,
Berlin.
11. Davies, E.B. (2007) Linear Operators and
their Spectra, Cambridge University Press,
Cambridge.

474
13 Functional Analysis
12. Exner, P. (1985) Open Quantum Systems and
Feynman Integrals, Reidel, Dordrecht.
13. Jauch, J.M. (1968) Foundations of Quantum
Mechanics, Addison-Wesley, Reading, MA.
14. Kolmogorov, A.N. and Fomin, S.V. (1999)
Elements of the Theory of Functions and
Functional Analysis, Dover, Rochester, NY.
15. Lieb, E.H. and Loss, M. (2001) Analysis, 2nd
edn, AMS, Providence, RI.
16. Lieb, E.H. and Seiringer, R. (2010) The
Stability of Matter in Quantum Mechanics,
Cambridge University Press, Cambridge.
17. von Neumann, J. (1996) Mathematical
Foundations of Quantum Mechanics, 12th
printing, Princeton University Press,
Princeton, NJ.
18. PrugoveÀácki, E. (1981) Quantum Mechanics
in Hilbert Space, 2nd edn, Academic Press,
New York.
19. Riesz, F. and Sz-Nagy, B. (1972) Le√ßons
d‚Äôanalyse foncionelle, 6me edn, Akademiai
Kiad√≥, Budapest.
20. Rudin, W. (1991) Functional Analysis, 2nd
edn, McGraw-Hill, New York.
21. Simon, B. (2005) Trace Ideals and Their
Applications, 2nd edn, AMS Chelsea,
Providence, RI.
22. Weidmann, J. (2000, 2003) Lineare
Operatoren in Hilbertr√§umen I, II, B.G.
Teubner, Stuttgart.
23. Yafaev, D.R. (1992) Mathematical Scattering
Theory, AMS, Providence, RI.

475
14
Numerical Analysis
Lyonell Boulton
14.1
Introduction
The foundations of the compendium of
mathematical
theories
now
known
as
numerical analysis can be traced back
to the Plimton 322 clay tablet, which is
believed to be over 3800 years old. How-
ever, arguably, it was only with the advent
of machines capable of performing a pre-
scribed set of mathematical tasks without
direct human intervention, that numerical
analysis consolidated as an independent
subject of scientiÔ¨Åc interest. For some,
numerical analysis is one of the success
stories of the second half of the twentieth
century.
In recent years, areas of numerical
analysis such as numerical linear algebra,
optimization,
and
numerical
diÔ¨Äeren-
tial equations, have turned themselves
into highly specialized research subjects.
Language and methods are increasingly
becoming only accessible, and often only
of interest, to specialists. Yet some of
these methods are of prime importance
in theoretical and experimental contem-
porary science. As electronically assisted
mathematics, ranging from the use of
highly specialized scientiÔ¨Åc apparatus to
large computer networks, permeated into
most of our scientiÔ¨Åc experience nowa-
days, there is an increasing need for the
development of mathematically rigorous
frameworks capable of validating the data
that is being produced by those means.
In this chapter, we only scratch the sur-
face of some of the classical themes in
numerical analysis. It is intended as a very
rough introduction to the language and
techniques in the theory for nonspecial-
ists. It is by no means exhaustive; however,
it may serve as a reading guide to more
specialized literature on the subject. We
miss many important aspects of the theory,
including the crucial connection with con-
crete computational implementations. An
excellent introduction to the latter is avail-
able in [1].
Section 14.2 is concerned with classical
techniques for the solution of nonlinear
equations and systems. This includes some
qualitative aspects of the described meth-
ods and a brief introduction to numeri-
cal minimization. The particular case of
linear systems of equations, which is far
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

476
14 Numerical Analysis
more specialized and developed, is con-
sidered separately in Section 14.3. There
we describe the standard classiÔ¨Åcation of
methods for the solution of systems of lin-
ear equations into direct and iterative, both
of which are important in their own right.
We also discuss the classical approach to
Ô¨Ånite eigenvalue problems. Section 14.4 is
devoted to approximation of continuous
data. We only describe the classical the-
ory of polynomial interpolation as a back-
ground to the highly developed theory of
Ô¨Ånite elements. In Sections 14.5 and 14.6,
we focus on two concrete subjects. In the
former, we describe in some detail the ele-
ments of the theory for the numerical solu-
tion of the initial-value (Cauchy) problems
for ordinary diÔ¨Äerential equations (ODEs).
In the latter, we describe the use of the
Galerkin method for the numerical solution
of spectral problems in the context of one-
dimensional Schr√∂dinger operators.
14.2
Algebraic Equations
The Ô¨Årst type of mathematical problem that
commonly requires a numerical approx-
imation is an algebraic equation. By far
the most successful general tools for the
approximated solution of general scalar or
vector algebraic equations are the iteration
methods. The special case of linear systems
of equations is of great importance and it
will be dealt with separately in the next
section.
14.2.1
Nonlinear Scalar Equations
Consider a general equation
f (x) = 0
for a given continuous scalar function f ‚à∂
[a, b] ‚àí‚Üí‚Ñù. An analytic solution of this
problem can only be found in close form
for very particular functions f . Otherwise,
approximation of possible solutions can be
achieved by means of Ô¨Åxed-point iteration
schemes: we pick an initial guess x0 and
iterate
xn+1 = g(xn),
n = 1, ‚Ä¶ ,
to get the Ô¨Åxed point
x‚àó= lim
n‚Üí‚àûxn
such that g(x‚àó) = x‚àó. Here x = g(x) is equiv-
alent to f (x) = 0, so that f (x‚àó) = 0.
Example 14.1 [Newton‚Äôs method] See [2,
Section 2.1]. Kepler‚Äôs model for the plane-
tary motion seeks for solutions x‚àó(depen-
dent on time) of the equation
x ‚àíÓà±sin x = ùúè.
Here Óà±is the (Ô¨Åxed) eccentricity of the ellip-
tical orbit and ùúèis proportional to time.
Newton proposed iterating
xn+1 = xn + ùúè‚àíxn + Óà±sin xn
1 ‚àíÓà±cos xn
n = 1, ‚Ä¶ ,
in order to obtain x‚àó. This corresponds to
choosing
g(x) = x ‚àíf (x)
f ‚Ä≤(x) for f (x) = ùúè‚àíx + Óà±sin x
above, and it is the special case of what is
now called Newton‚Äôs method.
If the function g is Lipschitz continuous,
|g(x) ‚àíg(y)| ‚â§ùúÜ|x ‚àíy|

14.2 Algebraic Equations
477
for all x, y in a segment [a, b] containing
x‚àó, then the absolute error en = x‚àó‚àíxn
satisÔ¨Åes
|en| ‚â§ùúÜ|en‚àí1| ‚â§ùúÜn|e0|
for all
n ‚â•1.
If ùúÜ< 1 and g(x) ‚àà[a, b] for all x ‚àà[a, b],
then there exists a unique Ô¨Åxed point x‚àó‚àà
[a, b] of g(x) and |en| ‚Üí0 for any choice of
x0. These conditions are satisÔ¨Åed in a neigh-
borhood of x‚àóif, for example, |g‚Ä≤(x‚àó)| < 1.
In this case, Taylor‚Äôs theorem yields
lim
n‚Üí‚àû
|en|
|en‚àí1| = |g‚Ä≤(x‚àó)|
(linear convergence).
If further |g‚Ä≤(x‚àó)| = 0, then
lim
n‚Üí‚àû
|en|
|en‚àí1|2
= 1
2|g‚Ä≤‚Ä≤(x‚àó)|
(quadratic convergence).
An application of higher-order Taylor‚Äôs
approximation shows that the condition
|g(j)(x‚àó)| = 0 for j = 1, ‚Ä¶ , k ‚àí1, implies
lim
n‚Üí‚àû
|en|
|en‚àí1|k
= 1
k!|g(k)(x‚àó)|
(kth order convergence).
Particular iteration methods for the
solution of scalar algebraic equations are
described in Table 14.1.
14.2.2
Nonlinear Systems
The idea of Ô¨Åxed-point iteration can be
formally extended to multiple dimensions.
Given G ‚à∂Œ© ‚àí‚Üí‚Ñùd where Œ© ‚äÇ‚Ñùd and an
initial x0 ‚ààŒ©, the iteration
xn+1 = G(xn),
n = 1, ‚Ä¶ ,
would converge to a Ô¨Åxed point x‚àó‚ààŒ© such
that G(x‚àó) = x‚àó, if
‚ÄñG(x) ‚àíG(y)‚Äñ ‚â§ùúÜ‚Äñx ‚àíy‚Äñ,
x, y ‚ààŒ©,
Table 14.1
Most common iteration methods for the
approximation of solutions of nonlinear scalar equations.
See [3, Chapter 6].
Usual name
Formula for xn+1
Order
Comments/pitfalls
Newton
xn ‚àíf (xn)
f ‚Ä≤(xn)
2
f ‚Ä≤(x‚àó) ‚â†0
ModiÔ¨Åed Newton
xn ‚àíkf (xn)
f ‚Ä≤(xn)
2
|f (j)(x‚àó)| = 0 for j < k and |f (k)(x‚àó)| ‚â†0
SteÔ¨Äensen
xn ‚àí
f (xn)2
f (xn+f (xn))‚àíf (xn)
2
No need to compute f ‚Ä≤
Secant
xn ‚àí(xn‚àíxn‚àí1)f (xn)
f (xn)‚àíf (xn‚àí1)
1+
‚àö
5
2
Iteration requires less work than
Newton or SteÔ¨Äensen
(General) Ô¨Åxed point
g(xn)
k
g is a general function such that
|g(j)(x‚àó)| = 0 for j < k and |g(k)(x‚àó)| ‚â†0
(General) higher order
g(g(xn))
2k
g is as in the general Ô¨Åxed-point
method and k is its order of
convergence (see previous row)

478
14 Numerical Analysis
for ùúÜ< 1. Here ‚Äñ ‚ãÖ‚Äñ is a suitable norm of
‚Ñùd. The error vector, en = xn ‚àíx‚àó, satisÔ¨Åes
the identity ‚Äñen+1‚Äñ ‚â§ùúÜ‚Äñen‚Äñ. If the Jacobian
JG(x) =
[ùúïGj
ùúïxk
]d
jk=1
exists for all x ‚ààŒ©, we can represent G in
Taylor series,
G(y) = G(x) + JG(x)(x ‚àíy) + RG(x, y),
RG(x, y) ‚â§c‚Äñx ‚àíy‚Äñ2.
This leads to the following general ‚Äúlocal‚Äù
convergence theorem around Ô¨Åxed points
[4, pp. 299‚Äì301].
Theorem 14.1 If ‚ÄñJG(x‚àó)‚Äñ < 1, starting the
iteration at any x0 suÔ¨Éciently close to x‚àó
ensures
lim
n‚Üí‚àû‚Äñxn ‚àíx‚àó‚Äñ = 0
and
lim
n‚Üí‚àû
‚Äñen+1‚Äñ
‚Äñen‚Äñ
= ‚ÄñJG(x‚àó)‚Äñ.
For a system of d algebraic equations in d
unknowns,
F(x) = 0,
a natural extension of the Newton method,
sometimes called the Newton‚ÄìSimpson
method, can be formulated as follows. Sup-
pose that JF ‚à∂Œ© ‚àí‚Üí‚Ñùd√ód is not singular,
for example, det(JF(x)) ‚â†0 for all x ‚ààŒ©.
Starting from x0 ‚ààŒ©, Ô¨Ånd xn+1 the solution
of
JF(xn)xn+1 = JF(xn)xn ‚àíF(xn),
n = 1, ‚Ä¶
(14.1)
This iteration corresponds to picking
G(x) = x ‚àíJF(x)‚àí1F(x).
In practice, the former is preferable to the
latter, as Ô¨Ånding all entries of the inverse
of the Jacobian matrix is in itself a numeri-
cally challenging problem for large dimen-
sions and it is often unnecessary. See [3,
Chapter 7] and [2, Chapter 7].
In concrete implementations, the itera-
tion scheme (14.1) is usually written in cou-
pled form
{
JF(xn)y
n = ‚àíF(xn)
xn+1 = xn ‚àíy
n
n = 1, ‚Ä¶
Arguments involving Theorem 14.1 show
that, if ‚ÄñJF(x)‚àí1F(x)‚Äñ and ‚ÄñJF(x)‚àí1HF(x)‚Äñ
are uniformly bounded by a small enough
constant on a region Œ© (here HF(x) is the
Hessian of F), then xn converges quadrat-
ically to a solution to the corresponding
homogeneous system for any initial x0 ‚ààŒ©.
Example 14.2 [An iteration method for
computing eigenvalues]
See [2, Section
7.2]. Given a hermitian matrix A ‚àà‚Ñùd√ód,
the eigenvalues of A can be estimated by
solving the nonlinear equation in d + 1
components,
F(x, ùúÜ) = 0
for
F(x, ùúÜ) =
[ Ax ‚àíùúÜx
1
2(1 ‚àíxt ‚ãÖx)
]
.
The
Newton‚ÄìSimpson
method
for
F
reduces in this case to the coupled iteration
‚éß
‚é™
‚é®
‚é™‚é©
(A ‚àíùúÜnI)zn = xn
ùúÜn+1 = ùúÜn +
1+xt
n‚ãÖxn
xt
n‚ãÖzn
xn+1 = (ùúÜn+1 ‚àíùúÜn)zn
,
n = 1, ‚Ä¶ ,
which is equivalent to the inverse power
method. Under fairly mild conditions on A,
ùúÜn would converge to the eigenvalue of A
that is smallest in modulus.

14.2 Algebraic Equations
479
14.2.3
Numerical Minimization
Given a diÔ¨Äerentiable function f ‚à∂‚Ñùd ‚àí‚Üí
‚Ñù, we now consider the unconstrained min-
imization problem of Ô¨Ånding x‚àó‚àà‚Ñùd such
that
f (x‚àó) ‚â§f (x)
‚àÄx ‚ààŒ©.
See [5]. When Œ© = ‚Ñùd, x‚àóis called a global
minimizer. When Œ© ‚ää‚Ñùd is only a neigh-
borhood of x‚àó, it is called a local minimizer.
As any minimizer is a critical point,
grad f (x‚àó) = 0,
the above problem is naturally viewed in the
context of solution to nonlinear systems of
equations.
Closely linked with the Ô¨Åxed-point iter-
ative procedures described previously are
the descent methods [3, Section 7.2.2]: pick
an initial x0 ‚ààŒ© and iterate
xn+1 = xn + ùõºnùõøn,
n = 1, ‚Ä¶
Here ùõøn are suitable descent directions sat-
isfying
ùõøt
n ‚ãÖgrad f (xn) < 0
and the scalar ùõºn > 0 measures the stepsize
along this direction. By Taylor‚Äôs theorem
f (x + ùõºùõø) ‚àíf (x) = ùõºgrad f (ùúâ)t ‚ãÖùõø,
where ùúâlies in the segment joining x with
x + ùõºùõø. Hence we always Ô¨Ånd ùõºn > 0 small
enough such that
f (xn + ùõºnùõøn) < f (xn),
(14.2)
whenever grad f (xn) ‚â†0. If grad f (xn) = 0,
we have arrived at a critical point, so we
may assign ùõºn = 0 to stop the iteration.
Every choice of ùõøn and ùõºn gives rise to
a diÔ¨Äerent method, with diÔ¨Äerent approx-
imation properties. See Table 14.2.
Determining ùõºn such that (14.2) is sat-
isÔ¨Åed without incurring into high compu-
tational expense, depends on the type of
problem considered. One commonly used
choice is to minimize
ùúô(ùõº) = f (xn + ùõºùõøn),
which guarantees that
grad f (xn+1)t ‚ãÖùõøn = 0.
Procedures leading toward Ô¨Ånding minima
of ùúô(ùõº) in this context are called linear
search techniques. See [3, Section 7.2.3].
Example 14.3 [Descent
methods
for
quadratic forms] Let A ‚àà‚Ñùd√ód be a hermi-
tian positive deÔ¨Ånite matrix and let b ‚àà‚Ñùd
Table 14.2
Descent directions for some of the standard
methods of unconstrained minimization. See [5].
Method
ùõøn
Comments / pitfalls
Newton
‚àíHf (xn)‚àí1 grad f (xn)
Computing Hf (xn)‚àí1 is usually impractical
Inexact Newton
‚àíB‚àí1
n grad f (xn)
Bn ‚âàHf (xn)‚àí1
Gradient
‚àígrad f (xn)
Inexact Newton with Bn = I and standard for quadratic
forms
Conjugate gradient
‚àígrad f (xn) + ùõΩnùõøn‚àí1
ùõΩn chosen so that ùõøn are mutually orthogonal

480
14 Numerical Analysis
be a Ô¨Åxed vector. Let
f (x) = 1
2xt ‚ãÖAx ‚àíxt ‚ãÖb.
As
grad f (x) = Ax ‚àíb,
the minimum of f will occur exactly at the
solution of a linear system. Let
rn = b ‚àíAxn.
The gradient method uses the direction of
this residual as search direction and
ùõºn =
rt
n ‚ãÖrn
rt
n ‚ãÖArn
.
On the other hand, for the conjugate gradi-
ent method,
ùõºn =
rt
n ‚ãÖùõøn
ùõøt
n ‚ãÖAùõøn
and
ùõΩn =
rt
n ‚ãÖùõøn‚àí1
ùõøt
n‚àí1 ‚ãÖAùõøn‚àí1
.
The latter uses a more clever choice of
search direction, because it avoids reusing
previously chosen directions.
14.3
Finite-Dimensional Linear Systems
Particular techniques for the approxima-
tion of solutions of the linear inhomoge-
neous equation
Ax = b,
(14.3)
as well as the associated linear eigenvalue
problem
Au = ùúÜu
u ‚â†0,
(14.4)
deserve special attention given their sim-
ple structure. Here A ‚àà‚ÑÇd√ód and b ‚àà‚ÑÇd
are data, and the unknowns are x ‚àà‚ÑÇd in
the former and (ùúÜ, u) ‚àà‚ÑÇ√ó ‚ÑÇd in the latter
problem.
If det A ‚â†0, the solution of (14.3) can be
determined from Cramer‚Äôs rule,
xj =
det Aj
det A ,
j = 1, ‚Ä¶ , d,
where Aj is the matrix obtained by replac-
ing the jth entry of A by b. However, this
approach is of practical interest only for
matrices of small size, as the calculation
of determinants requires a high number of
Ô¨Çops (Ô¨Çoating point arithmetic operations).
For standard matrices this will be roughly
3(d + 1)! operations, which is extremely
ineÔ¨Écient for n > 10.
Two types of methods are available for
large matrices, the direct methods and the
iterative methods.
14.3.1
Direct Methods and Matrix Factorization
Direct methods determine the solution of a
linear system in a Ô¨Ånite number of steps.
Gaussian elimination reduces a general
system of the form (14.3) to a triangular row
echelon form,
A1
11x1 + A1
12x2 + ‚Ä¶ + A1
1dxd = b1
1
A2
22x2 + ‚Ä¶ + A2
2dxd = b2
2
‚ãÆ
‚ãÆ
Ad
ddxd = bd
d.
We can Ô¨Ånd an explicit expression for
the coeÔ¨Écients of this system via the
Gauss elimination algorithm running for
r = 1, ‚Ä¶ , d ‚àí1:

14.3 Finite-Dimensional Linear Systems
481
A1
jk = Ajk
Ar+1
jk
= Ar
jk ‚àímjrAr
rk
mjr =
Ar
jr
Ar
rr
b1
j = bj
br+1
j
= br
j ‚àímjrbr
r
j, k = 1, ‚Ä¶ , d
j, k = r + 1, ‚Ä¶ , d
j = r + 1, ‚Ä¶ , d.
This assumes that the pivot elements Ar
rr
are nonzero. The matrices
U = [Aj
jk]d
jk=1
and
L = [mjk]d
jk=1
mjj = 1,
render a decomposition of the form
A = LU,
which is usually called an LU-factorization
of A. Here L ‚àà‚ÑÇd√ód is lower triangular
(Ljk = 0 for k < j) and U ‚àà‚ÑÇd√ód is upper
triangular (Ujk = 0 for k > j).
In turns, (14.3) is equivalent to the two
triangular systems
Ly = b
Ux = y.
The former can be solved by forward sub-
stitution
y1 = b1
L11
yj = 1
Ljj
(
bj ‚àí
j‚àí1
‚àë
k=1
Ljkyk
)
,
j = 2, ‚Ä¶ , d ‚àí1,
while the latter can be solved by back-
ward substitution (with a similar recursive
formula).
The factorization of matrices, such as the
LU-factorization, are important in the eÔ¨É-
cient solution of linear systems, as there can
be a signiÔ¨Åcant reduction in the number of
Ô¨Çops required to get the solution compared
with Gaussian elimination. This is partic-
ularly relevant, when we are required to
solve many problems for the same matrix
A, but diÔ¨Äerent vectors b. Gauss elimination
roughly requires 2d3‚àï3 operations, whereas
forward and backward substitutions only
require d2 Ô¨Çops each. See [2, Chapter 4] and
references therein.
The pivot elements of a given system
are all diÔ¨Äerent from zero if, and only if,
each one of the r √ó r upper-left minors of
A, [Ajk]r
jk=1, are nonsingular. Otherwise,
methods called pivoting, which involve
reordering either the rows or the columns
(partial pivoting) ‚Äì or both (full), can be
employed to produce an LU-factorization
of the matrix. The Gauss elimination
with partial pivoting (either by row or by
column) always leads to nonzero pivot
elements that produce a nonsingular LU-
factorization of A if and only if det A ‚â†0.
In fact, the pivoting technique can also
be implemented to reduce errors caused
by
numerical
rounding,
which
occur
whenever the multipliers mjk are small.
The LU-factorization of a matrix is not
unique. When A has a given structure (e.g.,
symmetric), it is possible to Ô¨Ånd a factor-
ization consistent with this structure, see
Table 14.3. Banded matrices are of particu-
lar interest in applications. If the bandwidth
is not very large, the factorization and back-
ward/forward substitution steps for such
matrices can be achieved in a comparatively
small number of operations.

482
14 Numerical Analysis
Table 14.3
Most common factorization methods for the
solution of linear systems. [3, Chapter 3].
Name
Factorization
Description
Ô¨Çops
Comments / pitfalls
LU
A = LU
L is lower triangular and
U is upper triangular
2d3‚àï3
With pivoting, it applies to
any A ‚àà‚ÑÇd√ód
Cholesky
A = LLt
L is lower triangular with
positive diagonal entries
d3‚àï3
Only for A ‚àà‚Ñùd√ód
hermitian positive deÔ¨Ånite
LDL
A = LDLt
L is lower triangular and D is
diagonal
d3‚àï3
For A ‚àà‚Ñùd√ód general
hermitian
Banded
As above
Factorization of A having
bandwidth b: Ajk = 0
|j ‚àík| > b
2b2d
Gain only for matrices of
small bandwidth
Example 14.4 [Default Matlab and Octave
linear solvers] See [1, Section 5.8]. The
default command for solving linear sys-
tems in the computer languages Matlab and
Octave is the backslash ‚Äú‚àñ‚Äù command. Each
time it is invoked, speciÔ¨Åc algorithms are
used, depending on the structure of the
matrix A. These algorithms are all based on
direct methods.
14.3.2
Iteration Methods for Linear Problems
Iterative methods for linear system are
closely
related
with
their
nonlinear
counterpart.
An
iteration
method
for
the solution of a linear system is called
stationary, if for given initial x0,
xn+1 = Bxn + c,
n = 1, ‚Ä¶
The iteration matrix B depends on A and
determines the method, and
c = (I ‚àíB)A‚àí1b.
Splitting the matrix A = P ‚àíR, where P
is usually called a preconditioner and R =
P ‚àíA, gives B = I ‚àíP‚àí1A and c = P‚àí1b. As
(14.3) is equivalent to the system
Px = Rx + b,
the static method can be written as
Pùõøn = b ‚àíAxn
xn+1 = xn + ùõºùõøn,
where ùõº‚â†0 is a parameter chosen so that
convergence might be improved.
The iteration matrix B should be con-
structed so that each iteration can be com-
puted eÔ¨Éciently. In particular, it is neces-
sary that the system associated to the pre-
conditioner is solved quickly. Writing A =
D + E + F where D is the diagonal part of A,
E is lower triangular with zeros on the diag-
onal and F is upper triangular with zeros on
the diagonal, give rise to the standard meth-
ods as described in Table 14.4.
The residual vector satisÔ¨Åes
‚Äñen+1‚Äñ ‚â§ùúå(B)‚Äñen‚Äñ,
n = 1, ‚Ä¶ ,
where ùúå(B) is the spectral radius: the largest
of the moduli of the eigenvalues of B. Hence
the iteration method will be convergent for
any choice of x0, only if ùúå(B) < 1. When
0 < w < 1, the SOR method can converge
when Gauss‚ÄìSeidel does not. When w =
1, the two methods are the same. When

14.3 Finite-Dimensional Linear Systems
483
Table 14.4
The most common iteration stationary methods
for the solution of linear systems [3, Chapter 4].
Name
P, ùõº
Comments / pitfalls
Jacobi
D, 1
See below.
Gauss‚ÄìSeidel
D ‚àíE, 1
Usually faster than Jacobi.
Successive over relaxation (SOR)
1
wD ‚àíE, 1
Only converges in general for 1 < w < 2.
Static Richardson
any, any
Choice of ùõºcan improve convergence.
1 < w < 2, SOR can converge faster than
Gauss‚ÄìSeidel. If A is a positive deÔ¨Ånite her-
mitian matrix, then SOR converges for all
0 < w < 2. If, in addition, A is tridiagonal,
(bandwidth b = 2), then the optimal choice
of w in the SOR method is
w =
2
1 ‚àí
‚àö
1 ‚àíÃÉùúå
,
where ÃÉùúåis the spectral radius of the matrix
B
of
the
corresponding
Gauss‚ÄìSeidel
method. See [6, 7] for details.
Example 14.5 [The PageRank algorithm]
See [8]. The notion of ranking web pages
arises in commercial and noncommercial
online search tools. The computer algo-
rithm known as PageRank developed by
Larry Page from the software company
Google was fairly popular among website
administrators until 2009. A simpliÔ¨Åed
model of how this algorithm works is
illustrated as follows. Consider as a small
example the set of eight web pages with the
hyperlinks as shown in Figure 14.1.
Suppose that a random walk moves
through the web network by two rules: 85%
of the time follow one of the hyperlinks
from the page it is on by assigning equal
weight to each and picking one at random
with probability 1‚àïHjj, then 15% of the
time skip randomly to another website
anywhere else on the network. Denoting by
xj the probability that the page j is visited,
we have
x = ùõøCtH‚àí1x
‚èü‚èû‚èü‚èû‚èü
Ô¨Årst rule
+ 1 ‚àíùõø
d
e
‚èü‚èü‚èü
second rule
‚áê‚áí
(I ‚àíùõøCtH‚àí1) x = 1 ‚àíùõø
d
e
here ùõø= 0.85, C is the connection matrix
associated to the network, H is the corre-
sponding links-out diagonal matrix and e is
the vector with all entries equal to 1.
The Jacobi method is convenient for the
solution of this linear system when d is
very large for two main reasons. On the
one hand, it can be proved to converge:
ùúå(B) = ùõøin this case. On the other hand,
any of the iterates xn preserves the property
of being a probability vector (nonnegative
entries adding exactly up to 1), if x0 is a
probability vector.
The iteration formula for the stationary
methods can be generalized to
Pùõøn = b ‚àíAxn
xn+1 = xn + ùõºnùõøn,
where ùõºn ‚â†0 is a parameter chosen so that
convergence might be improved. In this
case, the technique is some times called
nonstationary
or
dynamic
Richardson
method. The residual vector
rn = b ‚àíAxn

484
14 Numerical Analysis
a
b
e
g
c
d
h
f
C =
0
1
0
0
1
0
1
0
0
0
1
0
0
0
0
0
1
0
0
1
0
0
1
1
0
0
0
0
1
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
1
0
0
1
0
0
0
H =
3
1
4
2
1
1
1
2
Order the pages as (a,b,c,d,e,f,g,h)
Page 3 links to page 1, 4, 7, and 8
Page 4 links to page 5 and 6
Page 8 links to pages 2 and 5
Page 7 links to page 8
Page 6 links to page 5
Page 5 links to page 1
Page 2 links to page 3
Page 1 links to pages 2, 5, and 7
Links-out matrix
Connection matrix
Figure 14.1
Small example (d = 8) of a web network.
clearly measures how close xn is to the
actual solution of the system. As Pùõøn = rn,
ùõøn is called in this context the precondi-
tioned residual. Recalling Example 14.3, if
we pick P = I and
ùõºn =
rt
n ‚ãÖrn
rt
n ‚ãÖArn
,
we recover the gradient method. Thus the
stationary and nonstationary methods can
be viewed as generalizations of the descent
method for the minimization of quadratic
forms, where the descent direction is taken
to be the preconditioned residual. See [1,
¬ß4.3] and references therein.
As mentioned in Example 14.3, the con-
jugate gradient (CG) method picks ùõøn in a
slightly more sophisticated fashion. Begin-
ning with initial x0 and r0 = ùõø0 = b ‚àíAx0,
we set
ùõºn =
rt
n ‚ãÖùõøn
ùõøt
n ‚ãÖAùõøn
xn+1 =xn + ùõºnùõøn,
ùõΩn+1 =
rt
n+1 ‚ãÖùõøn
ùõøt
n ‚ãÖAùõøn
ùõøn+1 =rn+1 + ùõΩn+1ùõøn,
n=0, ‚Ä¶
If A is hermitian and positive deÔ¨Ånite, both
the gradient and the CG method converge
for any initial choice of x0. Replace the sec-
ond row of this iteration formula by
Py
n+1 = rn+1
ùõΩn+1 =
yt
n+1 ‚ãÖùõøn
ùõøt
n ‚ãÖAùõøn
ùõøn+1 = y
n+1 + ùõΩn+1ùõøn,

14.3 Finite-Dimensional Linear Systems
485
for P hermitian and positive deÔ¨Ånite, and
we obtain the preconditioned conjugate gra-
dient (PCG) method.
When all the iterations of the CG or the
PCG method are computed exactly, they
will converge to the true solution of the
system in exactly d steps. Unfortunately, in
practice, the numerical rounding will pre-
vent this from occurring.
14.3.3
Computing Eigenvalues of Finite Matrices
As for the case of the Ô¨Ånite system of lin-
ear equations, the numerical estimation of
the eigenvalues ùúÜand eigenvectors x, asso-
ciated to the eigenvalue problem (14.4) can
also be achieved by means of minimization
methods. The role of the Rayleigh quotient,
R(x) =
xt ‚ãÖAx
xt ‚ãÖx
in these methods may be illustrated on a
simple case, as follows.
Suppose that A is a 3 √ó 3 hermitian
matrix with eigenvalues ùúÜ1 ‚â§ùúÜ2 ‚â§ùúÜ3 and
eigenvectors
u1, u2, u3 ‚àà‚ÑÇ3
with
unit
Euclidean norm. These eigenvalues may
be written as extremal problems involving
R(x). Indeed, any vector in ‚ÑÇ3 expands as
x = ‚àëxjuj, so
R(x) =
‚àëùúÜj|xj|2
‚àë|xj|2 .
Then
ùúÜ1 = min{R(u)}, ùúÜ2 = min{R(u) ‚à∂u ‚üÇu1},
ùúÜ3 = min{R(u) ‚à∂u ‚üÇu1, u2}.
Moreover, ùúÜj are stationary points of the
map R ‚à∂‚Ñù3 ‚àí‚Üí‚Ñù.
Note that, if we employ a steepest descent
method, essentially no prior knowledge or
calculation of the eigenvectors is required
to get ùúÜ1. We can, in fact, characterize ùúÜ2
also, without explicit information about the
eigenvectors. If Óàø‚äÇ‚ÑÇ3 is an arbitrary two-
dimensional space, there always exists a
nonzero vector x ‚ààÓàøsuch that x ‚üÇu1. As
R(x) ‚â•ùúÜ2, we gather that maxu‚ààÓàøR(u) ‚â•ùúÜ2
and
ùúÜ2 = min
dim Óàø=2 max
u‚ààÓàøR(u).
A similar argument shows that
ùúÜ3 = min
dim Óàø=3 max
u‚ààÓàøR(u).
In turns, bounds for the eigenvalues can
be found from R(u) without much prior
information about the eigenvectors.
The above procedure can be extended to
matrices of any size, and in fact to inÔ¨Ånite-
dimensional linear operators. The following
result is of fundamental importance and it
is known as the min‚Äìmax principle. It was
Ô¨Årst investigated over one hundred years
ago by Lord Rayleigh and Walter Ritz in
connection with acoustics, but in its cur-
rent form is due to Courant and Fischer. See
[9, Section 4.5] or [10, Section XIII.2].
Theorem 14.2 (Min‚Äìmax principle) Let
A ‚àà‚ÑÇd√ód be a hermitian matrix and let its
eigenvalues be ordered nondecreasingly as
ùúÜ1 ‚â§¬∑ ¬∑ ¬∑ ‚â§ùúÜd. Then
ùúÜj = min
dim Óàø=j max
u‚ààÓàøR(u).
In
particular,
ùúÜ1 = min R(u)
and
ùúÜd = max R(u).
For certain practical problems, knowl-
edge of all the eigenvalues might not be
required and quite often only the largest
or smallest eigenvalue is needed. This is
highly relevant, for example, in the context
of quantum mechanics.

486
14 Numerical Analysis
Example 14.6 [Validation
of
the
Lamb
shift] See [10, Section XIII.2]. The ground
state energy of hydrogen can be found
analytically. The same cannot be said for
helium, even though the ground state
energy of a helium ion can be found exactly
from the hydrogen model. In the early
days of quantum mechanics, Hylleraas
computed an upper approximation of the
helium ground state energy by hand. The
calculation involved Ô¨Ånding the smallest
eigenvalue of a 6 √ó 6 matrix and it was
regarded as important support for the con-
Ô¨Årmation of the correctness of quantum
mechanics. With the advent of comput-
ers, Ô¨Ånding eigenvalues of larger matrices
was made possible. Early approximations
involving linear systems of sizes 39 by
Kinoshita and later 1078 by Perkeris, were
crucial in order to test agreement of the
Lamb shift with experimental data. The
min‚Äìmax principle played a crucial role in
this process.
Assume that A ‚àà‚Ñùd√ód has eigenvalues
such that
|ùúÜd| > |ùúÜd‚àí1| ‚â•¬∑ ¬∑ ¬∑ ‚â•|ùúÜ1|.
Note that the largest eigenvalue in modulus
is forced to be simple. The power method
is an iterative procedure to approximate ùúÜd
and ud. For initial nonzero vectors x0 and
y
0 = x0‚àï‚Äñx0‚Äñ, deÔ¨Åne
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™‚é©
xn+1 = Ay
n
y
n+1 =
xn+1
‚Äñxn+1‚Äñ
ùúÜd,n+1 = R
(
y
n+1
)
,
n = 0, ‚Ä¶
Then the nth iterate vector is such that y
n =
ùõΩnAny
0 for suitable scalar ùõΩn. As it turns,
this recursive formula ensures that y
n aligns
with the eigenvector direction ud and
|ùúÜd,n ‚àíùúÜd| ‚àº||||
ùúÜd‚àí1
ùúÜd
||||
n
(generic matrices),
|ùúÜd,n ‚àíùúÜd| ‚àº
||||
ùúÜd‚àí1
ùúÜd
||||
2n
(hermitian matrices)
as n ‚Üí‚àû. Therefore ùúÜd,n ‚ÜíùúÜd, whenever
the above conditions on the largest modu-
lus eigenvalue hold true. See [3, Section 5.3]
and [11, pp. 406‚Äì407].
A slight modiÔ¨Åcation of the power
method allows calculation of the eigen-
value of A that is smallest in modulus,
under the condition 0 < |ùúÜ1| < |ùúÜ2| and no
constraints on |ùúÜd|. For the inverse power
method, we begin with initial vectors as
before, but then iterate
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™‚é©
Axn+1 = y
n
y
n+1 =
xn+1
‚Äñxn+1‚Äñ
ùúád,n = R(y
n+1)
,
n = 0, ‚Ä¶
This is nothing more than the old power
method applied to the matrix A‚àí1, whose
eigenvalues
are
ùúáj = 1‚àïùúÜd‚àíj+1,
and
so
swapped in modulus order with respect to
the eigenvalues of A. From the convergence
of the power method and the above condi-
tion, ùúád,n ‚Üíùúád = ùúÜ‚àí1
1 , so we have a way of
computing ùúÜ1.
For any ùúá‚àà‚ÑÇnot an eigenvalue of A,
the matrix B = (A ‚àíùúáI) has its eigenvalues
equal to (ùúá‚àíùúÜj)‚àí1. Then, replacing the
matrix A with the matrix (A ‚àíùúáI) in the
inverse shifted power method will allow
computing the eigenvalue of A that is
closest to ùúá.
This idea can also be applied to reÔ¨Åne
the computation of ùúÜd, by changing ùúáat
each step of the iteration by the better guess
ùúád,n‚àí1. It is remarkable that, despite the fact

14.4 Approximation of Continuous Data
487
that (A ‚àíùúád,n) is near singular, this proce-
dure might allow convergence under cer-
tain circumstances. One instance of this,
with a diÔ¨Äerent normalization, was already
encountered in Example 14.2.
14.4
Approximation of Continuous Data
We now show how to represent continuous
mathematical quantities by means of a
Ô¨Ånite number of degrees of freedom. In
turns this is of natural importance for
numerical approximation. A fairly com-
plete account on the ideas discussed in this
section can be found in [3, Chapter 8] and
references therein.
14.4.1
Lagrange Interpolation
Let f ‚à∂[ùõº, ùõΩ] ‚àí‚Üí‚Ñùbe a given continuous
function and let
ùõº‚â§x0 < ¬∑ ¬∑ ¬∑ < xN ‚â§ùõΩ
be N + 1 points called interpolation nodes
of the interval [ùõº, ùõΩ]. There is a unique
Lagrange
interpolating
polynomial
of
degree N, LNf ‚àà‚ÑôN, satisfying
LNf (xk) = f (xk),
k = 0, ‚Ä¶ , N.
Indeed, the family of N characteristic poly-
nomials
ùìÅk(x) =
N
‚àè
j=0
j‚â†k
x ‚àíxj
xk ‚àíxj
,
k = 0, ‚Ä¶ , N,
forms a basis of ‚ÑôN, which ensures that we
can write
LNf (x) =
N
‚àë
k=0
f (xk)ùìÅk(x)
in a unique manner. The latter formula is
often referred to as the Lagrange form of the
corresponding interpolating polynomial.
The most economical way of computing
the interpolating polynomial is not the one
given by the Lagrange form. A more neat
procedure is set by the Newton form of
the interpolating polynomial. Set L0f (x) =
f (x0) and
Lkf (x) = Lk‚àí1f (x) + pk(x),
k = 1, ‚Ä¶ , N,
where Lkf ‚àà‚Ñôk is the Lagrange interpolat-
ing polynomial for f at the points {xj}k
j=0
and pk ‚àà‚Ñôk depends on {xj}k
k=0 and only
one unknown coeÔ¨Écient. Then
pk(x) = akwk(x)
for
wk(x) =
k‚àí1
‚àè
j=0
(x ‚àíxj).
Thus
a0 = f (x0),
ak = f (xk) ‚àíLk‚àí1f (xk)
wk(xk)
=
k‚àë
j=0
f (xj)
w‚Ä≤
k+1(xk).
In turns, we see that we can construct
recursively ak in such a way that
LNf (x) =
N
‚àë
k=0
akwk(x).
This formula is known as the Newton
divided diÔ¨Äerence formula and the coeÔ¨É-
cients ak are the kth divided diÔ¨Äerences.
14.4.2
The Interpolation Error
The interpolation error function
ùúÄN(x) = f (x) ‚àíLNf (x)

488
14 Numerical Analysis
naturally measures how well the Lagrange
interpolating polynomial approximates the
function f . If f ‚ààCr(ùõº, ùõΩ), then it can be
shown that [12, 13]
ùúÄN(x) = f (N+1)(ùúâ)
(N + 1)!wN+1(x)
(14.5)
for some ùúâ‚àà(ùõº, ùõΩ). Note that from the def-
inition, it follows that
ùúÄN(x) = aN+1wN+1(x),
where the (N + 1)th divided diÔ¨Äerence is
obtained by considering as interpolation
nodes the set {xj}N
j=0 ‚à™{x}.
We now highlight one remarkable draw-
back of general polynomial interpolation.
Consider a hierarchy of interpolation nodes
of the segment [ùõº, ùõΩ],
ÓâÑ= {ŒûN}‚àû
N=1,
ŒûN = {xN,k}N
k=0.
There always exists a continuous function
f ‚à∂[ùõº, ùõΩ] ‚àí‚Üí‚Ñù, such that the Lagrange
interpolating polynomial on ŒûN does not
capture f uniformly in the full segment
[ùõº, ùõΩ] as N ‚Üí‚àû. To be precise,
lim
N‚Üí‚àûmax
x‚àà[ùõº,ùõΩ] |ùúÄN(x)| ‚â†0.
In fact, such a function can some times be
constructed explicitly. See [14].
Example 14.7 [Runge‚Äôs phenomenon] Let
f (x) =
1
1 + x2 ,
x ‚àà[‚àí5, 5].
Suppose that the partition ŒûN of the seg-
ment [‚àí5, 5] is made out of equally spaced
nodes. Then
lim
N‚Üí‚àûmax
x‚àà[‚àí5,5] |ùúÄN(x)| = ‚àû.
By contrast, if the partition ŒûN of the
segment
[‚àí5, 5]
is
made
out
of
the
Chebyshev‚ÄîGauss‚ÄìLobatto nodes,
xN,k = ùõº+ ùõΩ
2
+ ùõΩ‚àíùõº
2
ÃÉxk
ÃÉxk = ‚àícos
(
ùúãk
N
)
,
k = 0, ‚Ä¶ , N,
then
lim
N‚Üí‚àûmax
x‚àà[‚àí5,5] |ùúÄN(x)| = 0.
14.4.3
Hermite Interpolation
In the case of Hermite interpolation, rather
than values of the function at the nodes,
higher-order derivatives are matched by the
given polynomial. Suppose that we wish to
match
f (r)(xk)
for
r = 0, ‚Ä¶ , mk
and
k = 0, ‚Ä¶ , N.
Let
d = ‚àëN
k=0(mk + 1).
There
exists
a
unique polynomial Hd‚àí1 ‚àà‚Ñôd‚àí1 of order
d ‚àí1, the Hermite interpolating polyno-
mial, such that
H(r)
d‚àí1(xk) = f (r)(xk)
for
r = 0, ‚Ä¶ , mk
and
k = 0, ‚Ä¶ , N.
A concrete formula for the Hermite inter-
polating polynomial is given by
Hd‚àí1(x) =
N
‚àë
k=0
mk
‚àë
r=0
f (r)
k (xk)Kkr(x),
where
K(p)
kr (xj) =
{ 1
k = j and r = p
0
otherwise.

14.4 Approximation of Continuous Data
489
Moreover, the error formula for Hermite
interpolation reads [3, Section 8.4]
ùúÄN(x) = f (x) ‚àíHd‚àí1(x) = f (d)(ùúâ)
d!
ùî•d(x)
ùî•d(x) =
N
‚àè
k=0
(x ‚àíxk)mk+1
(14.6)
for a given ùúâ‚àà[ùõº, ùõΩ].
Example 14.8 [Hermite quadrature formu-
lae] Polynomial interpolation can be used
in order to derive numerical methods for
the approximation of deÔ¨Ånite integrals of
the form
I(f ) = ‚à´
b
a
f (x) dx.
Hermite polynomial interpolation gives
rise to ‚Äúcorrected‚Äù versions of classical
formulae for integration, such as the trape-
zoidal rule. Assuming that the 2N + 2
values of f (xk) and f ‚Ä≤(xk) are given at nodes
{xk}N
k=0 of the segment [a, b]. The corre-
sponding Hermite interpolant polynomial
is
H2N+1f (x) =
N
‚àë
j=0
(f (xj)ùî©j(x) + f ‚Ä≤(xj)ùî™j(x))
for
ùî©j(x)
(
1 ‚àí
w‚Ä≤‚Ä≤
N+1(xj)
w‚Ä≤
N+1(xj)(x ‚àíxj)
)
ùìÅj(x)2
and
ùî™j(x) = (x ‚àíxj)ùìÅj(x)2
(the deÔ¨Ånition of wj and ùìÅj is given in
Section 14.4.1). If we integrate on [a, b], we
derive the quadrature formula
IN(f ) =
N
‚àë
j=0
(ùõºjf (xj) + ùõΩjf ‚Ä≤(xj)) ,
ùõºj = I(ùî©j)
and
ùõΩj = I(ùî™j),
which is an approximation of I(f ). For N =
1, we obtain the corrected trapezoidal for-
mula
I1(f ) = b ‚àía
2
(f (a) + f (b))
+(b ‚àía)2
12
(f ‚Ä≤(a) + f ‚Ä≤(b)) ,
which has higher accuracy than the approx-
imation of I(f ) by the classical trapezium
rule.
14.4.4
Piecewise Polynomial Interpolation
As described in Example 14.7, a possible
way to avoid the Runge phenomenon is by
considering a nonuniform distribution of
interpolation nodes. One other possibility
is to use to piecewise polynomial interpola-
tion. The latter forms the background of the
Ô¨Ånite element method [15, 16].
Let f ‚à∂[a, b] ‚àí‚Üí‚Ñùbe a continuous func-
tion. The idea behind piecewise polynomial
interpolation is to Ô¨Åx a partition of the seg-
ment [a, b] into subsegments whose interi-
ors are disjoint from one another, and con-
duct polynomial interpolation of a given
Ô¨Åxed order in each one of these subseg-
ments. Let Œûh = {Óà∂j}N
j=1 be a partition of
[a, b] into N subintervals,
[a, b] =
N
‚ãÉ
j=1
Óà∂j,
Óà∂j = [xj‚àí1, xj],
whose maximum length is
h = max
1‚â§k‚â§N(xj ‚àíxj‚àí1).
For q ‚àà‚Ñï, let
ÓâÇ(q, Œûh) = {w ‚ààC0(a, b) ‚à∂w|Óà∂j ‚àà‚Ñôq(Óà∂j),
j = 1, ‚Ä¶ , N}

490
14 Numerical Analysis
be the space of continuous functions on
[a, b] which are polynomials of degree no
larger than q in each Óà∂j. The map
Œ†h,q ‚à∂C0(a, b) ‚àí‚ÜíÓâÇ(q, Œûh)
such that Œ†h,qf |Óà∂j is the interpolating poly-
nomial (e.g., Lagrange or Hermite) on suit-
able nodes of the subsegments Óà∂j, is usually
called the piecewise polynomial interpolant
associated to Œûh.
By applying the error formulas (14.5) or
(14.6) in each one of the subsegments of Œûh,
it can be shown that there exists a constant
c(q) > 0 independent of f such that
max
x‚àà[a,b] |f (x) ‚àíŒ†h,q(x)|
‚â§c(r)hq+1 max
x‚àà[a,b] |f (q+1)(x)|
for any f ‚ààCq+1(a, b). Moreover, Œ†h,qf
turns out to approach f also in the mean
square sense. Indeed, denote by L2(a, b)
the complex Hilbert space of all Lebesgue
square integrable functions with the stan-
dard inner product ‚ü®f , g‚ü©= ‚à´b
a f (x)g(x) dx
and norm ‚Äñf ‚Äñ = ‚ü®f , f ‚ü©1‚àï2. For a proof of the
following result in a more general context,
see, for example, [15, Theorem 3.1.6].
Theorem 14.3 Let q ‚â•1 and assume that
r = 0, ‚Ä¶ , q + 1. There exists a constant
c(q) > 0 independent of h ensuring that, if
q+1
‚àë
r=0
‚Äñf (r)‚Äñ < ‚àû,
then
‚Äñf (r) ‚àí(Œ†h,qf )(r)‚Äñ ‚â§c(q)hq+1‚àír‚Äñf (q+1)‚Äñ.
Example 14.9 [The Hermite elements of
order three] The Hermite element of order
three are piecewise polynomials of order
q = 3 with prescribed values of f and f ‚Ä≤ at
the interpolation nodes. Let the functions
ùúì(x) = (|x| ‚àí1)2(2|x| + 1)
and
ùúî(x) = x(|x| ‚àí1)2
be deÔ¨Åned on ‚àí1 ‚â§x ‚â§1, see Figure 14.2.
Then
ùúì, ùúî‚ààÓâÇ(3, Œû1),
Œû1 = {[‚àí1, 0], [0, 1]}.
Both these functions have continuous
derivatives, if extended by 0 to ‚Ñù, and
ùúì(0) = 1, ùúì(¬±1) = ùúì‚Ä≤(0) = ùúì‚Ä≤(¬±1) = 0
ùúî‚Ä≤(0) = 1, ùúî(¬±1) = ùúî(0) = ùúî‚Ä≤(¬±1) = 0.
Œ® (x)
œâ (x)
0
0
‚Äì1
1
‚Äì0.5
0.5
0
0
‚Äì1
1
‚Äì0.5
‚Äì0.4
‚Äì0.2
0.2
0.4
0.6
0.5
0.2
0.4
0.6
0.8
1
Figure 14.2
ProÔ¨Åle of the functions for the Hermite elements of order three in one dimension.

14.5 Initial Value Problems
491
A basis of the linear space ÓâÇ(3, Œûh) where
Œûh a uniform partition of a generic inter-
val [a, b] is obtained by translation and dila-
tion. That is
Œ†h,3f (x) =
N
‚àë
j=0
(f (xj)ùúôj(x) + f ‚Ä≤(xj)ùúîj(x)) ,
for
ùúôj(x) = ùúô
(x ‚àíxj
h
)
ùúîj(x) = ùúî
(x ‚àíxj
h
)
,
j = 0, ‚Ä¶ , N.
14.5
Initial Value Problems
We now examine in some detail numerical
solutions of the general Cauchy problem
d
dt y(t) = g(t, y(t)),
t0 < t < T,
y(t0) = y
0,
(14.7)
where g ‚à∂[t0, ‚àû) √ó ‚Ñùd ‚àí‚Üí‚Ñùd is a continu-
ous function and T > t0.
Using arguments involving Ô¨Åxed-point
theorems, we know quite general condi-
tions ensuring the existence and unique-
ness of solutions of the above evolution
problem [17]. Indeed, (14.7) will admit a
unique local solution in a suitably small
neighborhood Óà∫‚äÇÓàµ√ó Óà∂of (t0, y
0), if the
function g is Lipschitz continuous in the
space variable,
‚Äñg(t, y
2) ‚àíg(t, y
1)‚Äñ ‚â§L‚Äñy
2 ‚àíy
1‚Äñ,
for all t ‚ààÓàµ
and
y
1, y
2 ‚ààÓà∂. (14.8)
Moreover, it will admit a unique global solu-
tion, if the constant L can be chosen uni-
formly for Óà∂= ‚Ñù.
We will be mostly concerned with the
case d = 1 which in itself possesses all the
features of the higher-dimensional case. See
[3, Chapter 11] and references therein.
14.5.1
One-Step Methods
In order to establish numerical procedures
for the solution of the Cauchy problem, we
begin by partitioning the segment [t0, t0 +
T] into a number N of subsegments of step
size h > 0. That is
[t0, t0 + T] =
N‚àí1
‚ãÉ
n=0
[tn, tn+1],
where the nodes tn are given by t0 + nh. Let
yn = y(tn).
We seek for approximations un of the true
solution value yn at the nodes. We set
gn = g(tn, un).
For the one-step methods, the approxi-
mation un+1 at the (n + 1)th node depends
solely on un. In their general formulation,
these methods can be written as
un+1 = un + hŒ¶(tn, un, gn; h)
u0 = y0,
n = 0, ‚Ä¶ , N ‚àí1,
where Œ¶ is often referred to as the incre-
mental function.
By writing
yn+1 = yn + hŒ¶h(tn, yn, g(tn, yn)) + hùúèn+1(h),
n = 0, ‚Ä¶ , N ‚àí1,
where ùúèn(h) is a correction term, we deÔ¨Åne
the local truncation error,
ÃÉùúÄn(h) = |ùúèn(h)|,

492
14 Numerical Analysis
at the node tn. If the incremental function
satisÔ¨Åes the condition
lim
h‚Üí0 Œ¶h(tn, yn, g(tn, yn)) = g(tn, yn),
by expanding y in Taylor series at that node,
we can show that [3, Section 11.3]
lim
h‚Üí0 ÃÉùúÄn(h) = 0,
n = 0, ‚Ä¶ , N ‚àí1.
In fact, it can also be shown that the global
truncation error
ÃÉùúÄ(h) =
max
n=0,‚Ä¶,N‚àí1 |ÃÉùúÄn(h)|
does satisfy
lim
h‚Üí0 ÃÉùúÄ(h) = 0
under the above condition, ensuring the
consistency of the one-step method. The
order of a consistent method is the largest
power q ‚àà‚Ñï, such that
lim
h‚Üí0
ÃÉùúÄ(h)
hq
< ‚àû.
In Table 14.5, we show common choices
of incremental functions alongside the cor-
responding order of approximation. The
method is explicit, if un+1 can be com-
puted directly from uj for j ‚â§n, otherwise
it is called implicit. An iteration formula in
the case of the implicit methods should be
derived for each individual Cauchy prob-
lem.
14.5.2
Multistep Methods
In the case of the multistep methods the
approximated solution at the (n + 1)th
node is computed by means of a formula
involving un, ‚Ä¶ , un+1‚àíq. The smallest value
of q for which this is possible is the number
of steps of the method.
Example 14.10 [the midpoint method] The
midpoint rule for diÔ¨Äerentiation gives rise
to the midpoint method, in which
un+1 = un‚àí1 + 2hgn,
n = 1, ‚Ä¶ , N ‚àí1.
Here u0 = y0 and u1 is to be determined.
This is an example of an explicit multistep
method comprising two steps.
An implicit two-step method is the fol-
lowing.
Example 14.11 [the
Simpson‚Äôs
method]
The Simpson‚Äôs rule for integration gives
rise to the Simpson‚Äôs method. The iteration
formula reads
un+1 = un‚àí1 + h
3
(gn‚àí1 + 4gn + gn+1
) ,
n = 1, ‚Ä¶ , N ‚àí1.
The linear multistep methods represent
an important class of procedures for the
eÔ¨Écient solution of the Cauchy problem.
Table 14.5
Simple one-step methods for the numerical solution of the Cauchy problem.
Name
Œ¶h(tn, un, gn)
Order
Type
Forward Euler
gn
1
Explicit
Backward Euler
gn+1
1
Implicit
Crank‚ÄìNicholson
gn+gn+1
2
2
Implicit
Heun
gn+g(tn+1,un+hgn)
2
2
Explicit

14.5 Initial Value Problems
493
See [18]. Set real coeÔ¨Écients a0, ‚Ä¶ , aq‚àí1
and b‚àí1, ‚Ä¶ , bq‚àí1, such that aq‚àí1bq‚àí1 ‚â†0.
Set the iteration formula
un+1 =
q‚àí1
‚àë
k=0
akun‚àík + h
q‚àí1
‚àë
k=‚àí1
bkgn‚àík,
n = 1, ‚Ä¶ , N ‚àí1.
(14.9)
The parameter q is the numbers of steps
of the method and the coeÔ¨Écients (ak, bk)
fully characterize it. If b‚àí1 = 0, the method
is explicit, otherwise it is implicit.
The local truncation error for multistep
methods can be deÔ¨Åned from the recursion
hÃÉùúÄn+1(h) = yn+1 ‚àí
q‚àí1
‚àë
k=0
akyn‚àík ‚àíh
q‚àí1
‚àë
k=‚àí1
bky‚Ä≤
n‚àíj,
where y‚Ä≤
j = d
dt y(tj); that is, hÃÉùúÄn+1(h) is the
error generated at the node tn+1, if we sub-
stitute the analytical solution of the initial-
value problem into the scheme (14.9). The
deÔ¨Ånition of consistency and order of the
method mentioned above are also available
for multistep methods.
By choosing a0 = 1 and all other aj = 0
we obtain the Adams methods. Integrating
(14.7), yields
y(t) ‚àíy0 = ‚à´
t
t0
g(s, y(s)) ds.
If we approximate this integral on the
segment [tn, tn+1] by the integral of the
interpolating polynomial of g on q distinct
nodes, particular Adams methods can be
derived which, by construction, turn out
to be consistent. Fixing as interpolation
nodes tn, ‚Ä¶ , tn‚àíq+1, gives the Adams‚Äì
Bashforth methods, which are explicit,
as they always lead to b‚àí1 = 0. Fixing as
interpolation nodes tn+1, ‚Ä¶ , tn‚àíq+2, gives
the Adams‚ÄìMoulton methods, which are
implicit.
For
q
steps,
the
Adams‚ÄìBashforth
method can be shown to be of order q.
Example 14.12 [Adams‚ÄìBashforth meth-
ods]
Examples
of
Adams‚ÄìBashforth
methods include the following.
1. For q = 1, we recover the forward
Euler method.
2. For q = 2, the interpolating polyno-
mial is
P(t) = gn + (t ‚àítn)gn‚àí1 ‚àígn
tn‚àí1 ‚àítn
.
Thus
‚à´
tn+1
tn
P(s) ds = h
2(3gn ‚àígn‚àí1).
Hence the iteration formula of the
Adams‚ÄìBashforth method with two
steps requires
(b0, b1) =
(3
2, ‚àí1
2
)
.
3. For q = 3, we get in a similar fashion
an iteration formula for the Adams‚Äì
Bashforth method with three steps,
requiring
(b0, b1, b2) =
(23
12, ‚àí4
3, 5
12
)
.
4. For q = 4, the iteration formula
requires
(b0, b1, b2, b3) =
(55
24, ‚àí59
24, 37
24, ‚àí3
8
)
.
The Adams‚ÄìMoulton methods can be
derived similarly. Table 14.6 shows the cor-
responding coeÔ¨Écients for small q.
For q + 1 steps, the Adams‚ÄìMoulton
methods can be proved to be of order
q + 1. This statement is a consequence

494
14 Numerical Analysis
Table 14.6
Adams‚ÄìMoulton methods.
q
(b‚àí1, ‚Ä¶ , bq‚àí1)
0
1
1
(
1
2, 1
2
)
2
(
5
12 , 2
3, ‚àí1
12
)
3
(
3
8 , 19
24 , ‚àí5
24 , 1
24
)
4
(
251
720 , 323
360 , ‚àí11
30 , 53
360 , ‚àí19
720
)
of the following remarkable result [3,
Theorem 11.3].
Theorem 14.4 The multistep method with
iteration formula (14.9) is consistent if, and
only if,
q‚àí1
‚àë
k=0
ak = 1
q‚àí1
‚àë
k=0
bk ‚àí
q‚àí1
‚àë
k=‚àí1
kak = 1.
Moreover, if the solution y ‚ààCr+1(t0, T),
then the method is of order r if, and only if,
additionally
q‚àí1
‚àë
k=0
(‚àík)jak + j
q‚àí1
‚àë
k=‚àí1
(‚àík)j‚àí1bk = 1,
j = 2, ‚Ä¶ , r.
14.5.3
Runge‚ÄìKutta Methods
An alternative approach to multistep meth-
ods, is the one adopted by the Runge‚ÄìKutta
methods. These are one-step methods that
involve several evaluations of g(t, y) on the
subsegment [tn, tn+1]. Their iteration for-
mula reads
un+1 = un + h
s‚àë
k=1
bkRk,
n = 1, ‚Ä¶ , N ‚àí1,
where
Rk = g
(
tn + ckh, un + h
s‚àë
k=1
ajkRk
)
,
j = 1, ‚Ä¶ , s.
Here s is called the number of stages of
the method. See [19] and the references
therein. The parameters completely deter-
mine the method with the condition cj =
‚àës
k=1 ajk and are represented neatly by the
Butcher array
c
A
bt
for
A = [ajk]s
jk=1
b = (bk)s
k=1
and
c = (ck)s
k=1.
A Runge‚ÄìKutta method is explicit if, and
only if, the matrix A is lower triangular.
Implicit methods are quite diÔ¨Écult to
derive in general, because the computation
of Rk involves solving a nonlinear system of
size s.
Example 14.13 [Some Runge‚ÄìKutta meth-
ods with a small number of stages] The
Butcher arrays
1
1
1
1
2
1
2
1
0
0
0
1
1
2
1
2
1
2
1
2
correspond,
respectively,
to
the
back-
ward Euler, the implicit midpoint, and the
Crank‚ÄìNicolson methods.
It is possible to construct implicit or
explicit Runge‚ÄìKutta methods of any large
order. One of the most widely used Runge‚Äì
Kutta method is the one with four stages

14.5 Initial Value Problems
495
determined by the Butcher array
0
1
2
1
2
1
2
0
1
2
1
0
0
1
1
6
1
3
1
3
1
6
It can be derived in a similar fashion as
for the Adams methods. It is explicit and
convergent to the fourth order with respect
to h. An implicit method with two stages
that is also convergent to fourth order is
given by the array
3‚àí
‚àö
3
6
1
4
3‚àí2
‚àö
3
12
3+
‚àö
3
6
3+2
‚àö
3
12
1
4
1
2
1
2
We have exclusively dealt above with the
case of the scalar Cauchy problem. When
confronted with the case d > 1, two pos-
sible approaches are available. We either
apply to each individual equation one of the
scalar methods, or we write the iteration
formula for the method in vector form.
Example 14.14 [Airy‚Äôs equation] The Airy
functions have a wide range of applications
in mathematical physics. The solution of
the Airy equation that generates the Airy
functions,
d2
dt2 u(t) = tu(t),
t > 0,
u(0) = 1,
d
dt u(0) = 0,
can be achieved by reducing this equation
to the system
d
dt y(t) = g(t, y(t)),
t > 0
y(0) =
[1
0
]
|||||||||
y(t) =
[
u(t)
d
dt u(t)
]
g(t, y) =
[ y2
ty1
]
.
Even though analytical methods can be
applied to the study of Airy functions,
numerical methods as described above
are better suited for the estimation of
numerical values of the Airy functions.
14.5.4
Stability and Global Stability
A scheme on a given Ô¨Åxed bounded seg-
ment will certainly be regarded as stable,
if small perturbations of the data render
bounded perturbations of the numerical
solution in the regime h ‚Üí0. This idea can
be made more precise as follows. Assume
that vn is an approximated solution pro-
duced by the numerical method in ques-
tion, applied to a perturbation of the orig-
inal problem, and that ùúén is the size of this
perturbation at the step n. The numerical
method will be called zero-stable, if for Ô¨Åxed
T > t0, there exists h0, CT, ùúÄ0 > 0 such that
for all 0 < h ‚â§h0 and 0 < ùúÄ‚â§ùúÄ0,
|ùúén| ‚â§ùúÄ
implies
|un ‚àívn| ‚â§CTùúÄ
for all
n = 0, ‚Ä¶ , max{n ‚à∂tn ‚â§t0 + T}.
Example 14.15 [Consistency implies zero-
stability for one-step methods] If g satis-
Ô¨Åes (14.8), a consistent one-step method for
the solution of the corresponding Cauchy
problem will be zero-stable. Moreover, in
this case, the constant CT is proportional to
e(T‚àít0)L.
Let
ùî≠(r) = rq ‚àí
q‚àí1
‚àë
k=0
akrq‚àí1‚àík
= (r ‚àír0) ¬∑ ¬∑ ¬∑ (r ‚àírq‚àí1)
be the characteristic polynomial associated
to the coeÔ¨Écients of the multistep method

496
14 Numerical Analysis
(14.9). The latter will be zero-stable, if and
only if all |rk| ‚â§1 and those with |rk| =
1 are all simple roots. The latter is often
called the root condition of the method. See
Table 14.7.
Theorem 14.5 (Lax‚ÄìRitchmyer) A num-
erical method for the solution of the Cauchy
problem that is consistent, will also be con-
vergent if and only if it is zero-stable.
A complete proof of this result can be
found in [3, Section 11.6.3].
Another notion of stability is concerned
with the behavior of the scheme, when the
corresponding Cauchy problem is posed on
an inÔ¨Ånite segment. For global stability, the
methods are tested against each other by
means of the reference equation
dy
dt = ùúÜy,
t > 0,
y(0) = 1
for all given ùúÜ‚àà‚ÑÇ. This reference equation
has as exact solution y(t) = eùúÜt. The region
of absolute stability of an iterative scheme
is the subset of the complex plane deÔ¨Åned
as
Óàæ= {hùúÜ‚àà‚ÑÇ‚à∂|un| ‚Üí0
tn ‚Üí‚àû}.
That is, the region of values of the product
hùúÜ, such that the numerical scheme leads to
a solution decaying at inÔ¨Ånity.
Example 14.16 [Absolute stability of the
forward Euler method] The iteration for-
mula of the forward Euler method for the
solution of the reference equation is
un+1 = hùúÜun,
u0 = 1,
n = 0, ‚Ä¶
Induction allows Ô¨Ånding the exact solution
to this recursion,
un = (1 + hùúÜ)n,
n = 0, ‚Ä¶
For inclusion in Óàæ, we therefore require in
this case that |1 + hùúÜ| < 1. Hence,
Óàæ=
{
Re(hùúÜ)<0
and
0<h<‚àí2 Re(ùúÜ)
|ùúÜ|2
}
,
which is a circle of radius 1 centered at
(‚àí1, 0).
For a general multistep method, let
ùîÆ(r) =
q‚àí1
‚àë
k=‚àí1
bkrq‚àí1‚àík.
The boundary of the region of absolute sta-
bility is characterized by the identity [3,
Section 11.6.4]
ùúïÓàæ= {hùúÜ‚àà‚ÑÇ‚à∂ùîÆ(r)hùúÜ
= ùî≠(r)
for some
|r| = 1}.
14.6
Spectral Problems
In order to illustrate the numerical solution
to inÔ¨Ånite-dimensional spectral problems,
we
consider
the
reference
eigenvalue
equation
corresponding
to
the
one-
dimensional
Schr√∂dinger
Hamiltonian.
Given a real-valued continuous potential
V ‚à∂(a, b) ‚àí‚Üí‚Ñù, the aim is to determine
Table 14.7
Regions of absolute convergence
for the most common one-step methods.
Method
Óàæ
Forward Euler
{|1 + z| < 1}
Backward Euler
‚ÑÇ‚ßµ{|1 ‚àíz| ‚â§1}
Crank‚ÄìNicolson
{Re z < 0}
Heun
{|1 + z + z2
2 | < 1}

14.6 Spectral Problems
497
approximately the solution of the equation
‚àíd2
dx2 y(x) + V(x)y(x) = ùúÜy(x),
a < x < b,
y(a) = y(b) = 0.
(14.10)
Here a < b and either one of them or
both can be inÔ¨Ånite. By a solution of
(14.10), we mean an eigenvalue ùúÜ‚àà‚Ñùand
a corresponding nonzero eigenfunction
y ‚ààL2(a, b).
This simple equation possesses all the
features of the more complicated multipar-
ticle settings.
14.6.1
The InÔ¨Ånite-Dimensional min‚Äìmax
Principle
The left hand side of (14.10) gives rise to a
Hamiltonian,
H ‚à∂D(H) ‚àí‚ÜíL2(a, b),
which is a self-adjoint operator deÔ¨Åned on
the dense domain D(H) ‚äÇL2(a, b), depen-
dent on the potential. Formally,
H = ‚àíd2
dx2 + V.
The eigenvalue equation is equivalent to the
weak equation
ùî•(y, w) = ùúÜ‚ü®y, w‚ü©,
for all
w ‚ààD(ùî•),
where
the
energy
functional
ùî•‚à∂
D(ùî•) √ó D(ùî•) ‚àí‚Üí‚ÑÇis given by
ùî•(y, w) = ‚ü®Hy, w‚ü©
=‚à´
b
a
(
y‚Ä≤(x)w‚Ä≤(x) + V(x)y(x)w(x)
)
dx.
Here D(ùî•) ‚äÇL2(a, b) is another suitable
dense domain containing D(H) that renders
the form closed.
By mimicking the Ô¨Ånite-dimensional set-
ting, let
ùúÜj(H) = min
Óàø‚äÇD(ùî•)
dim Óàø=j
max
w‚ààÓàøR(w),
j = 1, ‚Ä¶ ,
where the Rayleigh quotient
R(w) = ùî•(w, w)
‚ü®w, w‚ü©.
Then we have an analogous version of
Theorem 14.2, [9, Section 4.5] or [10,
Section XIII.2].
Theorem 14.6 (Min‚Äìmax principle)
for (14.10) Assume that the potential V is
such that R(w) ‚â•c for all w ‚ààD(ùî•). Then
b ‚â§ùúÜj(H) ‚â§e(H) are the eigenvalues of
(14.10) lying below
e(H) = lim
j‚Üí‚àûùúÜj(H).
Moreover, the corresponding nonzero eigen-
functions yj ‚ààD(ùî•) satisfy
ùúÜj(H) =
min
w‚üÇ{yk}j‚àí1
k=1
R(w) = R(yj).
By construction, ùúÜj(H) are nondecreasing
in j. The limit e(H) > b might or might not
be inÔ¨Ånity and it is the bottom of the essen-
tial spectrum of H.
Numerical methods for computing upper
bounds for the ùúÜj(H) can then be derived
by means of the Galerkin method. Pick a
linear subspace Óà∏‚äÇD(ùî•) of dimension d
with a linearly independent basis {bj}d
j=1.
Assemble the matrices
K =[ùî•(bj, bk)]d
jk=1
and
M=[‚ü®bj, bk‚ü©]d
jk=1.

498
14 Numerical Analysis
Then the eigenvalues ùúáj(H, Óà∏) of the
Ô¨Ånite-dimensional linear eigenvalue prob-
lem
Ku = ùúÜMu
are such that ùúáj(H, Óà∏) ‚â•ùúÜj(H).
14.6.2
Systems ConÔ¨Åned to a Box
DiÔ¨Äerent methods arise by constructing
families of subspaces Óà∏in diÔ¨Äerent ways.
We illustrate the standard way of imple-
menting the Galerkin method on an artiÔ¨Å-
cial example, where all quantities involved
can be found analytically.
Example 14.17 [The free particle in a box]
Let V(x) = 0, a = 0 and b > 0. The solution
of (14.10) is given explicitly by
ùúÜj(H) = j2ùúã2
b2 ,
yj(x) = sin
(jùúã
b x
)
.
In this case, D(ùî•) = H1
0(0, b), the Sobolev
space of all absolutely continuous functions
vanishing at 0 and b.
Assume that Óà∏is the space of Lagrange
Ô¨Ånite elements of order 1. A basis for Óà∏
is given by the piecewise linear continuous
functions
bj(x) =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™‚é©
x ‚àíxj‚àí1
xj ‚àíxj‚àí1
xj‚àí1 ‚â§x ‚â§xj
xj+1 ‚àíx
xj+1 ‚àíxj
xj ‚â§x ‚â§xj+1
0
otherwise
j = 1, ‚Ä¶ , N ‚àí1,
where h = b‚àïN and xj = jh. Note that b0
and bN are not considered here, as the trial
functions in Óà∏should satisfy the Dirichlet
boundary conditions.
We can Ô¨Ånd the eigenvalues ùúáj(H, Óà∏)
explicitly. Let
ÃÉM =
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢‚é£
4
1
1
4
1
1
‚ã±
‚ã±
‚ã±
4
1
1
4
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•‚é¶
and
ÃÉK =
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢‚é£
2
‚àí1
‚àí1
2
‚àí1
‚àí1
‚ã±
‚ã±
‚ã±
2
‚àí1
‚àí1
2
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•‚é¶
.
Then K = (1‚àïh) ÃÉK and M = (h‚àï6) ÃÉM. The
eigenvalues of the Toeplitz matrix
T =
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢‚é£
0
1
1
0
1
1
‚ã±
‚ã±
‚ã±
0
1
1
0
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•‚é¶
are ùúèj = 2 cos(jùúã‚àïN), j = 1, ‚Ä¶ , N ‚àí1. As
the commutation relation ÃÉM ÃÉK = ÃÉK ÃÉM holds
true,
ùúáj(H, Óà∏) =
6N2(2 ‚àíùúèj)
b2(4 + ùúèj) .
In the explicit calculation made in the
previous example, ùúáj(H, Óà∏) ‚ÜìùúÜj(H) and
lim
N‚Üí‚àû
ùúáj(H, Óà∏) ‚àíùúÜj(H)
N2
= j4ùúã4
12b2 ,
j = 1, ‚Ä¶ , N ‚àí1.
In other words, the approximated eigenval-
ues miss the exact eigenvalues by a factor
proportional to h2.
Let Œûh be an equidistant partition of the
segment [a, b] into N subintervals
Óà∂j = [xj‚àí1, xj]

Further Reading
499
of length h = b ‚àía‚àïN. Let
ÓâÇ(q, Œûh) = {w ‚ààC1(a, b) ‚à∂w|Óà∂j ‚àà‚Ñôq(Óà∂j), 1
‚â§j ‚â§N, w(a) = w(b) = 0}
be the space of piecewise diÔ¨Äerentiable
functions
on
Œû
that
are
polynomials
of degree q in each Óà∂j and satisfy the
boundary conditions of (14.10). See [16,
Theorem 6.1].
Theorem 14.7 Let a and b be Ô¨Åxed and
both Ô¨Ånite. Assume that the potential V is
continuous. There exists c(q) > 0, such that
ùúÜj(H) ‚â§ùúáj(H, ÓâÇh(q, Œû)) ‚â§ùúÜj(H)
+ c(q)h2qùúÜj(H)q+1
for all h suÔ¨Éciently small.
14.6.3
The Case of UnconÔ¨Åned Systems
When the segment (a, b) is of inÔ¨Ånite
length, Theorem 14.6 is still holds as long
as Óà∏is contained in D(ùî•). One possible
approach is to assume that Óà∏is made out
of Gaussian-type functions.
Another possible approach is to observe
that,
generally,
the
eigenfunctions
of
(14.10) are exponentially decaying at ¬±‚àû
whenever the potential is suÔ¨Éciently reg-
ular. Let L > 0 and denote by ùúÜj(H, L)
the eigenvalues of (14.10) for a = ‚àíL and
b = L.
Theorem 14.8 Let j ‚àà‚Ñïbe Ô¨Åxed and
assume that the potential V is continuous.
There exist constants cj > 0 and aj > 0 both
independent of L, such that
ùúÜj(H) < ùúÜj(H, L) < ùúÜj(H) + cje‚àíajL.
A proof of this result follows from the
known exponential decay of the eigen-
functions of one-dimensional Schr√∂dinger
operators, see, for example, [20, Chapter
3.3] and references therein.
Further Reading
For a more comprehensive survey on
the subjects of numerical solutions of
nonlinear systems and numerical opti-
mization, see [3, Chapter 7]. In particular,
we have ignored completely the subject
of constrained optimization and Lagrange
multiplier techniques [21‚Äì23].
A substantial body of research has been
devoted to the sensitivity analysis of the
solution of linear systems by direct meth-
ods and the eÔ¨Äect of rounding errors. For
details on this, see [11, 24, 25]. The PCG
method is a particular realization of the
more general Krylov methods that can be
applied to nonhermitian matrices [6, 26,
27]. Of particular importance are the full
orthogonalization method (FOM) and the
generalized minimum residual (GMRES)
method, which converge extremely fast for
the right class of matrices.
Computation of all the eigenvalues of a
matrix via QR factorization or singular-
value decomposition is a well-studied
subject [11, 28]. Much research has also
been conducted on the more general
matrix polynomials, in particular, their
factorization properties and the corre-
sponding generalized eigenvalue problem
[29]. The remarkable theory of computa-
tion with matrix functions has also been
recently examined [30].
Reference on topics such as approxima-
tion theory by orthogonal polynomials can
be found in [3, Chapter 10] and references
therein. Classical references on the Ô¨Ånite
element method are [15, 16, 31]. A survey
on the theory of curves and surface Ô¨Åtting
can be found in [32].

500
14 Numerical Analysis
An important class of methods for the
solution of Cauchy problems that we
have not mentioned here is the class of
predictor‚Äìcorrector methods [3, Section
11.7]. For further reading on the numerical
solution of PDEs both evolutionary and
stationary, see [33, 34] and references
therein.
A fairly complete account on the theory
of eigenvalue computation for diÔ¨Äerential
equations including estimation of a priori
and a posteriori complementary bounds for
eigenvalues can be found in [35]. Compu-
tation of eigenvalues in gaps of the essen-
tial spectrum is a much harder problem; see
[36] and references therein.
References
1. Quarteroni, A., Saleri, F., and Gervasio, P.
(2010) Texts in Computational Science and
Engineering, ScientiÔ¨Åc Computing with
MATLAB and Octave, vol. 2,
Springer-Verlag, Berlin.
2. Ridgway Scott, L. (2011) Numerical Analysis,
Princeton University Press, Princeton, NJ.
3. Quarteroni, A., Sacco, R., and Saleri, F.
(2007) Numerical Mathematics, Texts in
Applied Mathematics, vol. 37, 2nd edn,
Springer-Verlag, Berlin.
4. Ortega, J.M. and Rheinboldt, W.C. (2000)
Iterative Solution of Nonlinear Equations in
Several Variables, Classics in Applied
Mathematics, vol. 30, Society for Industrial
and Applied Mathematics (SIAM),
Philadelphia, PA.
5. Dennis, J.E. Jr., and Schnabel, R.B. (1996)
Numerical Methods for Unconstrained
Optimization and Nonlinear Equations,
Classics in Applied Mathematics, vol. 16,
Society for Industrial and Applied
Mathematics (SIAM), Philadelphia, PA.
6. Hackbusch, W. (1994) Iterative Solution of
Large Sparse Systems of Equations, Applied
Mathematical Sciences, vol. 95,
Springer-Verlag, New York.
7. Young, D.M. (2003) Iterative Solution of
Large Linear Systems, Dover Publications,
Inc., Mineola, NY.
8. Langville, A.N. and Meyer, C.D. (2012)
Google‚Äôs PageRank and Beyond: The Science
of Search Engine Rankings, Princeton
University Press, Princeton, NJ.
9. Davies, E.B. (1995) Spectral Theory and
DiÔ¨Äerential Operators, Cambridge Studies in
Advanced Mathematics, vol. 42, Cambridge
University Press, Cambridge.
10. Reed, M. and Simon, B. (1978) Methods of
Modern Mathematical Physics. IV. Analysis
of Operators, Academic Press, New York.
11. Golub, G.H. and Van Loan, C.F. (2013)
Matrix Computations. Johns Hopkins Studies
in the Mathematical Sciences. Johns Hopkins,
4th edn, University Press, Baltimore, MD.
12. WendroÔ¨Ä, B. (1966) Theoretical Numerical
Analysis, Academic Press, New York.
13. Davis, P.J. (1975) Interpolation and
Approximation, Dover Publications, Inc.,
New York.
14. ErdÀùos, P. (1961) Problems and results on the
theory of interpolation. II. Acta Math. Acad.
Sci. Hungar., 12, 235‚Äì244.
15. Ciarlet, P.G. (2002) The Finite Element
Method for Elliptic Problems, Classics in
Applied Mathematics, vol. 40, Society for
Industrial and Applied Mathematics (SIAM),
Philadelphia, PA.
16. Strang, G. and Fix, G. (2008) An Analysis of
the Finite Element Method, 2nd edn,
Wellesley-Cambridge Press, Wellesley, MA.
17. Coddington, E.A. and Levinson, N. (1955)
Theory of Ordinary DiÔ¨Äerential Equations,
McGraw-Hill Book Company, London.
18. Lambert, J.D. (1991) Numerical Methods for
Ordinary DiÔ¨Äerential Systems, John Wiley &
Sons, Ltd., Chichester, The initial value
problem.
19. Butcher, J.C. and Wanner, G. (1996)
Runge-Kutta methods: some historical notes.
Appl. Numer. Math., 22(1-3), 113‚Äì151.
20. Simon, B. (1982) Schr√∂dinger semigroups.
Bull. Amer. Math. Soc. (N.S.), 7(3), 447‚Äì526.
21. Avriel, M. (2003) Nonlinear Programming,
Dover Publications, Inc., Mineola, NY.
22. Bertsekas, D.P. (1982) Constrained
optimization and Lagrange multiplier
methods, in Computer Science and Applied
Mathematics, Academic Press, Inc., New
York.
23. Canon, M.D., Cullum, C.D. Jr., and Polak, E.
(1970) Theory of Optimal Control and
Mathematical Programming, McGraw-Hill
Book Co., New York.

References
501
24. Datta, B.N. (2010) Numerical Linear Algebra
and Applications, 2nd edn, Society for
Industrial and Applied Mathematics (SIAM),
Philadelphia, PA.
25. Stewart, G.W. (1973) Introduction to Matrix
Computations, Academic Press, New York,
London.
26. Axelsson, O. (1994) Iterative Solution
Methods, Cambridge University Press,
Cambridge.
27. Saad, Y. (2003) Iterative Methods for Sparse
Linear Systems, 2nd edn, Society for
Industrial and Applied Mathematics,
Philadelphia, PA.
28. Wilkinson, J.H. (1988) The Algebraic
Eigenvalue Problem, Monographs on
Numerical Analysis, The Clarendon Press -
Oxford University Press, New York.
29. Gohberg, I., Lancaster, P., and Rodman, L.
(1982) Matrix Polynomials, Academic Press,
Inc., New York.
30. Higham, N.J. (2008) Functions of Matrices,
Society for Industrial and Applied
Mathematics (SIAM), Philadelphia, PA.
31. Brenner, S.C. and Ridgway Scott, L. (2008)
The Mathematical Theory of Finite Element
Methods, Texts in Applied Mathematics, vol.
15, 3rd edn, Springer, New York.
32. Lancaster, P. and ≈†alkauskas, K. (1986) Curve
and Surface Fitting, Academic Press, Inc.,
London.
33. Quarteroni, A. and Valli, A. (1994)
Numerical Approximation of Partial
DiÔ¨Äerential Equations, Springer Series in
Computational Mathematics, vol. 23,
Springer-Verlag, Berlin.
34. Ern, A. and Guermond, J.-L. (2004) Theory
and Practice of Finite Elements, Applied
Mathematical Sciences, vol. 159,
Springer-Verlag, New York.
35. Weinberger, H.F. (1974) Variational
Methods for Eigenvalue Approximation,
Society for Industrial and Applied
Mathematics, Philadelphia, PA.
36. Boulton, L., Boussa√Ød, N., and Lewin, M.
(2012) Generalised Weyl theorems and
spectral pollution in the Galerkin method.
J. Spectr. Theory, 2(4), 329‚Äì354.


503
15
Mathematical Transformations
Des McGhee, Rainer Picard, Sascha TrostorÔ¨Ä, and Marcus Waurick
15.1
What are Transformations and Why are
They Useful?
A transformation1) is a process that turns
objects in one area into objects of a diÔ¨Äer-
ent area in such a way that no information
is lost and the original can be reconstructed
without loss. In mathematical terms, this
could be described as an invertible map-
ping from one topological space onto
another such that both the mapping and its
inverse are continuous. Transformations
1) There is a linguistic subtlety regarding the stan-
dard mathematical terminology. Transformation
is a process and the result of the transformation
applied to a particular object is its transform.
In the literature, these terms are almost always
used as synonyms and we shall not attempt to
rectify this situation. The device performing
the transformation is a transformer or transfor-
mator. We will not use the term ‚Äútransformer‚Äù
as it might lead to confusion with other areas
of life, see, for example, [1]. The term ‚Äútrans-
formator‚Äù is rarely used anyway, so we shall
follow the standard practice and use ‚Äútransfor-
mation‚Äù to describe both process and device
(compare map/mapping, operator/operation or
projector/projection for similar terminological
diÔ¨Éculties).
are advantageous if terms, concepts, and
ideas are considered to be more elementary
or intuitive in one area and less elementary
or intuitive in the other. A simple example
is the use of diÔ¨Äerent maps (i.e., diÔ¨Äerent
projections) of the Earth‚Äôs surface. An
angle-preserving map is useful for ocean
navigation, whereas an area-preserving
map is useful for comparing the size of
diÔ¨Äerent islands.
Another more mathematical example is
the principal axis transformation, which
brings the equation of an ellipse into a stan-
dard (canonical) form where, for example,
the axes sizes, eccentricity, and foci are eas-
ily read oÔ¨Äor deduced from formula. This
amounts to the diagonalization of a real
(2 √ó 2)‚àímatrix A
A = U‚àóDU,
(15.1)
where U is an orthogonal (real) (2 √ó 2) ‚àí
matrix, i.e.
U‚àí1 = U‚àó.
Here U‚àóis simply the transpose matrix
to U.
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

504
15 Mathematical Transformations
Physicists should be familiar with the
ubiquitous Laplace and Fourier transfor-
mations (including the sine and cosine
transformations). Probably the most famil-
iar application of these transformations
is to convert ‚Äúhard‚Äù diÔ¨Äerential equations
into ‚Äúeasier‚Äù algebraic equations. Once
the latter are solved, the solutions of the
original diÔ¨Äerential equations are obtained
by applying the inverse transformation.
The choice of transformation is deter-
mined by the diÔ¨Äerential operator involved
including
consideration
of
its
domain
determined by initial or boundary con-
ditions. The Laplace transformation L
deÔ¨Åned by
(Lf )(s) = ‚à´‚Ñù+
e‚àístf (t) dt
is appropriate for initial-value problems on
the half-line [0, ‚àû[. Thus, for some suit-
able range of values of s, the function t ÓÇ∂‚Üí
e‚àístf (t) must be integrable over [0, ‚àû[, a
property that will contribute to the deter-
mination of an appropriate space of func-
tions in which the problem can be con-
sidered. The salient property is that the
Laplace transformation converts diÔ¨Äeren-
tiation into multiplication by the variable
subject to the function value at t = 0 being
known, the easily derived formula being
(Lf ‚Ä≤)(s) = s(Lf )(s) ‚àíf (0)
(derived using integration by parts and
assuming that e‚àístf (t) ‚Üí0 as t ‚Üí‚àû). This
generalizes to higher derivatives as
(Lf (n))(s) = sn(Lf )(s) ‚àí
n‚àí1
‚àë
r=0
sn‚àí1‚àírf (r)(0).
The Fourier transformation F deÔ¨Åned by2)
2) There are various, essentially equivalent, deÔ¨Å-
nitions of the Fourier Transformation. We shall
discuss some issues later.
(Ff )(x) =
1
‚àö
2ùúã‚à´‚Ñù
exp (‚àíixy) f (y) dy
(15.2)
is often appropriate for boundary-value
problems
on
‚Ñù
where
solutions
are
required to decay to zero at ¬±‚àû. Once
again, the requirement for the above inte-
gral to exist contributes to the deÔ¨Ånition
of appropriate spaces in which to consider
problems. Clearly, the integral exists for
any function that is integrable on ‚Ñù, that
is, for all L1(‚Ñù)-functions, because for all
x, y ‚àà‚Ñùwe have | exp (‚àíixy) | = 1. The
Fourier transformation can also be deÔ¨Åned
on all of L2(‚Ñù) where it is traditionally
referred to as the Fourier‚ÄìPlancherel trans-
formation (see [2] and Section 15.4). For
the Fourier transformation, the result for
diÔ¨Äerentiation of an n‚àítimes continuously
diÔ¨Äerentiable function is
(Ff (n))(x) = (ix)n(Ff )(x),
that is, diÔ¨Äerentiation is transformed into
multiplication by i times the variable.
The Fourier sine and cosine transforma-
tions are deÔ¨Åned respectively by3)
(ÃÉFsin f )(x) =
‚àö
2
ùúã‚à´‚Ñù+
sin(xy) f (y) dy,
(ÃÉFcos f )(x) =
‚àö
2
ùúã‚à´‚Ñù+
cos(xy) f (y) dy
for suitable functions f on [0, ‚àû[ such that
the integrals exist. The diÔ¨Äerentiation for-
mulae for twice continuously diÔ¨Äerentiable
function f on [0, ‚àû[ with f (x) ‚Üí0 as x
tends to inÔ¨Ånity are
(ÃÉFsin f ‚Ä≤‚Ä≤)(x) = ‚àíx2(ÃÉFsin f )(x) +
‚àö
2
ùúãxf (0),
(ÃÉFcos f ‚Ä≤‚Ä≤)(x) = ‚àíx2(ÃÉFcos f )(x) ‚àí
‚àö
2
ùúãf ‚Ä≤(0),
3) The rationale for the ‚àºnotation here is
explained in Section 15.6.

15.1 What are Transformations and Why are They Useful?
505
making these transformations useful typ-
ically for the canonical second-order par-
tial diÔ¨Äerential equations (Laplace, wave,
and diÔ¨Äusion equations) on a space domain
[0, ‚àû[ where, respectively, the function or
its derivative is given as boundary data at
zero (along with decay to zero at ‚àû).
Such applications make clear the need for
a transformation to be a continuous one-to-
one mapping with a continuous inverse. If
not one-to-one, then the inverse transfor-
mation would not exist and uniqueness of
the inverse transform would be lost: more
than one function purporting to solve the
diÔ¨Äerential equation would correspond to
any solution of the algebraic equation. If the
mapping or its inverse were not continu-
ous, then any small error (rounding error in
calculation or measurement error in exper-
iment) would result in large, uncontrollable
errors in transforms or inverse transforms,
respectively.
We shall consider these transformations
and others in some mathematical gener-
ality in the following sections. We shall
not however rehearse standard applica-
tions of mathematical transformations to
typical problems of mathematical physics
because there is no shortage of appropriate
sources of such material. Instead, our aim
is threefold:
1. to shed light on why mathematical
transformations work by clarifying
what governs the choice of
transformation that may be appropriate
to a particular problem;
2. to show that various transformations
widely used in physics and engineering
are closely related; and
3. to introduce suÔ¨Écient mathematical
rigor so that the application of
mathematical transformations to
entities such as the Dirac delta and its
derivatives that are so useful in
modeling point sources and impulses,
are fully understood.
We set the scene by considering an
appropriate setting in which to develop a
deeper understanding of the role and use-
fulness of mathematical transformations.
We focus attention on linear mappings
between Hilbert spaces H0 and H1. A
transformation is a linear mapping
T ‚à∂H0 ‚ÜíH1
such that
T‚àí1 ‚à∂H1 ‚ÜíH0
exists (i.e., T is a bijection, a one-to-one
and onto map) and both T and T‚àí1 are
continuous, that is, T ‚ààÓàÆ(H0, H1
) and
T‚àí1 ‚ààÓàÆ(H1, H0
) (see [2]). In other words,
T is a linear homeomorphism. Of partic-
ular interest are transformations T that
preserve the Hilbert space structure, that is,
which are unitary. Unitary transformations
are isometric, that is, norm preserving,
which means that for all x ‚ààH0
|Tx|H1 = |x|H0.
A useful Ô¨Årst example is given by coor-
dinate transformations. Let ùúô‚à∂S0 ‚ÜíS1
be a smooth bijection between open sets
S0, S1 = Ran (ùúô) in ‚Ñùn, n ‚àà{1, 2, 3, ‚Ä¶}
such that the determinant det (ùúïùúô) of the
Jacobian ùúïùúôis bounded and has a bounded
inverse. Such a coordinate transformation
induces a unitary transformation between
the
spaces
L2 (Sk
) ‚âîL2 (Sk, dùúÜ, ‚ÑÇ)
of
(equivalence classes of) complex-valued,
square-integrable functions on ‚Ñùn vanish-
ing outside Sk, k ‚àà{0, 1}. (Here ùúÜdenotes
the standard Lebesgue measure.) Indeed,
for all f , g ‚ààL2 (S1
) , the substitution
formula for integrals yields

506
15 Mathematical Transformations
‚ü®f |g‚ü©L2(S1)
= ‚à´S1
f g dùúÜ
= ‚à´S1
f (y) g (y) dy
= ‚à´S0
f (ùúô(x)) g (ùúô(x)) |det ((ùúïùúô) (x))| dx.
The latter can be re-written as
‚à´S0
‚àö
|det ((ùúïùúô) (x))|f (ùúô(x))
‚àö
|det ((ùúïùúô) (x))|g (ùúô(x)) dx
or
‚ü®‚àö
|det ((ùúïùúô) (m))|f‚àòùúô|
‚àö
|det ((ùúïùúô) (m))|g‚àòùúô
‚ü©
L2(S0) ,
where
‚àö
|det ((ùúïùúô) (m))| is a suggestive
notation for the multiplication operator
mapping f
to x ÓÇ∂‚Üí
‚àö
|det ((ùúïùúô) (x))|f (x) ,
that is, T‚àö
|det((ùúïùúô))| in the notation of [2].
This shows that
L2 (S1
)
‚Üí
L2 (S0
)
f
ÓÇ∂‚Üí
‚àö
|det ((ùúïùúô) (m))| f ‚àòùúô
(15.3)
is an isometry and, in fact, it is also onto and
therefore is a unitary transformation.
We shall focus mainly on some partic-
ular unitary transformations obtained by
continuous extension from integral expres-
sions, because they are particularly relevant
in applications to physics [3‚Äì7].
15.2
The Fourier Series Transformations
15.2.1
The Abstract Fourier Series
The Ô¨Årst example is the so-called abstract
Fourier expansion giving rise to the abstract
Fourier series transformation. It may come
as a surprise to some that Fourier series
expansions are essentially transformations
as deÔ¨Åned above.
Let o be an orthonormal set in a Hilbert
space H. Then
x =
‚àë
e‚àào
‚ü®e|x‚ü©H e
(15.4)
for all x in the linear envelope (linear span,
linear hull) of o, which is the smallest linear
subspace of H containing o. The orthonor-
mal set o is called total or complete (or
an orthonormal basis) if the linear enve-
lope of o is dense in H, [2]. In case of an
orthonormal basis, we have a series expan-
sion of the form (15.4) for every x ‚ààH. The
series expansion holds independently of
any ordering of o, which is why we chose
the summation notation in (15.4). For a
separable Hilbert space, an orthonormal
basis is Ô¨Ånite or countably inÔ¨Ånite and
so there is a parametric ‚Äúenumeration‚Äù
(eùõº
)
ùõº‚ààS of o = {eùõº|ùõº‚ààS} and we may
write ‚àë
ùõº‚ààS ‚ü®eùõº|x‚ü©H eùõº. For S = {0, ‚Ä¶ , N},
S = ‚Ñïor S = ‚Ñ§, the respective notations
‚àëN
ùõº=0 ‚ü®eùõº|x‚ü©H eùõº,
‚àë‚àû
ùõº=0 ‚ü®eùõº|x‚ü©H eùõº
and
‚àë‚àû
ùõº=‚àí‚àû‚ü®eùõº|x‚ü©H eùõº
are
commonly
used.
The associated unitary transformation (as
stated by Parseval‚Äôs equality, [2]) is the
abstract Fourier series transformation
H ‚ÜíùìÅ2 (o)‚âî
{
f ‚à∂o ‚Üí‚ÑÇ
|||||
‚àë
e‚àào
|f (e)|2 < ‚àû
}
,
x ÓÇ∂‚Üí
(
‚ü®e|x‚ü©H
)
e‚àào ,
which associates with each element x ‚ààH
its sequence of so-called Fourier coeÔ¨É-
cients. The inverse of this transformation
is the reconstruction of the original x ‚ààH
from its (abstract) Fourier coeÔ¨Écients
ùìÅ2 (o) ‚ÜíH,
(ùúôe
)
e‚àào ÓÇ∂‚Üí
‚àë
e‚àào
ùúôe e.

15.2 The Fourier Series Transformations
507
15.2.2
The Classical Fourier Series
Of course, the French baron Jean-Baptiste-
Joseph Fourier (March 21, 1768 ‚Äì May
16, 1830) never envisioned such a gen-
eral concept of orthogonal expansion.
He had a very speciÔ¨Åc orthonormal basis
in mind. In today‚Äôs language, he was
concerned with the orthonormal basis
{
1‚àï
‚àö
2ùúãexp (ik ‚ãÖ) |k ‚àà‚Ñ§
}
in the Hilbert
space
L2 (] ‚àíùúã, ùúã]) = L2 (] ‚àíùúã, ùúã], dùúÜ, ‚ÑÇ).
Equivalently, we could have no scalar fac-
tor in front of the exponential and have
{exp (ik ‚ãÖ) |k ‚àà‚Ñ§} as an orthonormal basis
in
L2 (] ‚àíùúã, ùúã], 1‚àï2ùúãdùúÜ, ‚ÑÇ) .
We
shall,
however, stay with the Ô¨Årst option.
The classical Fourier expansion gives rise
to the transformation, that is, a unitary
mapping,
F# ‚à∂L2 (] ‚àíùúã, ùúã]) ‚ÜíùìÅ2 (‚Ñ§) ,
f ÓÇ∂‚Üí
(
ÃÇf (k)
)
k‚àà‚Ñ§,
where the Fourier coeÔ¨Écients ÃÇf (k) are given
by
ÃÇf (k) =
1
‚àö
2ùúã‚à´]‚àíùúã,ùúã]
exp (‚àíiky) f (y) dy,
for each k ‚àà‚Ñ§and the inverse transforma-
tion, F‚àí1
#
= F‚àó
# is given by
F‚àí1
#
‚à∂ùìÅ2 (‚Ñ§) ‚ÜíL2 (] ‚àíùúã, ùúã]) ,
(
ÃÇf (k)
)
k‚àà‚Ñ§ÓÇ∂‚Üíf ,
where
f (x) =
1
‚àö
2ùúã
‚àû
‚àë
k=‚àí‚àû
ÃÇf (k) exp (ikx)
for almost every x ‚àà] ‚àíùúã, ùúã]. Thus, for all
f ‚ààL2 (] ‚àíùúã, ùúã]),
f (x) = 1
2ùúã
‚àû
‚àë
k=‚àí‚àû‚à´]‚àíùúã,ùúã]
exp (‚àíiky) f (y) dy exp (ikx)
for almost all x ‚àà] ‚àíùúã, ùúã].
A unitary equivalence of an operator
A to a multiplication-by-argument oper-
ator in some suitable function space is
called a spectral representation of A.4)
The main point of interest is that F# is a
spectral representation associated with
diÔ¨Äerentiation.
Indeed, if ùúï# denotes the diÔ¨Äerentiation
operator in L2 (] ‚àíùúã, ùúã]) with domain
deÔ¨Åned by periodic boundary conditions
then
ùúï# = F‚àó
#im F#
showing that applying ùúï# in L2 (] ‚àíùúã, ùúã]) is
unitarily equivalent to i times the multi-
plication by the argument m ‚à∂
(
ak
)
k‚àà‚Ñ§ÓÇ∂‚Üí
(k ak
)
k‚àà‚Ñ§in ùìÅ2 (‚Ñ§). In other words, F# is a
spectral representation for 1
i ùúï#. This prop-
erty is used to turn the problem of solving
a linear diÔ¨Äerential equation with constant
coeÔ¨Écients ak ‚àà‚ÑÇ, k ‚àà{0, ‚Ä¶ , N}, N ‚àà‚Ñï,
p (ùúï#
) u ‚âî
N
‚àë
k=0
akùúïk
#u = f
under
a
periodic
boundary
condition
on [‚àíùúã, ùúã] into the question of calcu-
lating F‚àó
#
(
(1‚àïp (ik)) ÃÇf (k)
)
k‚àà‚Ñ§, which is
always possible if the polynomial function
x ÓÇ∂‚Üíp (ix) has no integer zeros.
The
rescaling
(h > 0)
or
rescaling
with reÔ¨Çection (h < 0) of ] ‚àíùúã, ùúã] to
] ‚àí|h| ùúã, |h| ùúã], h ‚àà‚Ñù‚ßµ{0} , is the coordi-
nate transformation
4) In as much as multiplication by a diagonal
matrix can be interpreted as multiplication-
by-argument operator if the argument ranges
over the eigenvalues of the matrix A, (15.1)
states that the unitary matrix U is a spectral
representation of A.

508
15 Mathematical Transformations
‚Ñù‚Üí‚Ñù
t ÓÇ∂‚Üíht
inducing by (15.3) a unitary rescaling trans-
formation ùúéh given by
ùúéh ‚à∂L2 (‚Ñù) ‚ÜíL2 (‚Ñù) ,
(15.5)
f ÓÇ∂‚Üí
‚àö
|h|f (h ‚ãÖ)
with ùúé1‚àïh = ùúé‚àó
h as inverse. This provides
a neat way of obtaining Fourier series
expansion results for functions on diÔ¨Äerent
length intervals. For L2 (] ‚àí|h| ùúã, |h| ùúã]),
the unitary Fourier series transformation is
then5)
F#ùúéh ‚à∂L2 (] ‚àí|h| ùúã, |h| ùúã]) ‚ÜíùìÅ2 (‚Ñ§) ,
f ÓÇ∂‚Üí
(
ÃÇ
ùúéhf (k)
)
k‚àà‚Ñ§
with
ÃÇ
ùúéhf (k)
=
‚ü®
1
‚àö
2ùúã
exp (ik ‚ãÖ) |ùúéhf
‚ü©
L2(]‚àíùúã,ùúã]),
=
‚ü®
ùúé‚àó
h
1
‚àö
2ùúã
exp (ik ‚ãÖ) |f
‚ü©
L2(]‚àí|h|ùúã,|h|ùúã]),
=
‚ü®
1
‚àö
2ùúã|h|
exp (ik ‚ãÖ‚àïh) |f
‚ü©
L2(]‚àí|h|ùúã,|h|ùúã]).
The set {1‚àï
‚àö
2ùúã|h| exp (ik ‚ãÖ‚àïh) |k ‚àà‚Ñ§}
is
an
orthonormal
basis
in
L2 (] ‚àí|h| ùúã, |h| ùúã]), because unitary trans-
formations map orthonormal bases to
orthonormal bases. Conversely,
f (x) =
1
2ùúã|h|
‚àë
k‚àà‚Ñ§
ùõºk exp
(
ikx
h
)
for almost all, where
ùõºk =
‚ü®
exp
(
ik
h ‚ãÖ
)
| f
‚ü©
L2(]‚àí|h|ùúã,|h|ùúã])
for all k ‚àà‚Ñ§" at the end of the sentence.
5) Compositions of unitary mappings are again
unitary.
It may be interesting to note that by
choosing h = N+1
ùúã, N ‚àà‚Ñï, and evaluating
pointwise at integer values from ‚àíN to N +
1, we have that
‚éß
‚é™
‚é®
‚é™‚é©
(
1
‚àö
2 (N + 1)
exp
(
ùúãisk
N + 1
))
s‚àà{‚àíN,‚Ä¶,N+1}
||||||
k ‚àà{‚àíN, ‚Ä¶ , N + 1}
‚é´
‚é™
‚é¨
‚é™‚é≠
is
a
complete
orthonormal
set
in
‚ÑÇ2(N+1). This follows from the fact that
{exp (ùúãis‚àï(N + 1)) |s ‚àà{‚àíN, ‚Ä¶ , N + 1}}
is the set of roots of unity, that is, the
solution set of the equation
z2(N+1) = 1.
(15.6)
For more details we refer to [8].
Thus we obtain the so-called discrete
Fourier transformation
(
1
‚àö
2 (N+1)
exp
(
ùúãi (jk)
(N + 1)
))
k,j‚àà{‚àíN,‚Ä¶,N+1}
as a matrix deÔ¨Åning a unitary map in
‚ÑÇ2(N+1).
Noting that
1
‚àö
2 (N + 1)
N+1
‚àë
s=‚àíN
exp
(
‚àíùúãiks
(N + 1)
)
fs
(15.7)
may serve as a crude approximation of the
kth Fourier coeÔ¨Écient of ùúé(N+1)‚àïùúãf associ-
ated with the step function
f =
N+1
‚àë
s=‚àíN
fsùúí
](k‚àí1),k] ‚ààL2 (] ‚àí(N+1) , (N+1)]) ,
where fs ‚àà‚ÑÇfor s ‚àà{‚àíN, ‚Ä¶ , N + 1}, we
see that the discrete Fourier transforma-
tion can be utilized for numerical purposes.
Noting that (15.7) is actually a polynomial
evaluated at exp (‚àíùúãik‚àï(N + 1)) ‚àà‚ÑÇ, for
numerical purposes one may apply eÔ¨Écient
polynomial evaluation strategies, such as
Horner‚Äôs scheme (or rule), to obtain what

15.3 The z-Transformation
509
is known as the fast Fourier transformation
(FFT) [9].
15.2.3
The Fourier Series Transformation in
L2 (S‚ÑÇ(0, 1))
By the complex coordinate transformation
] ‚àíùúã, ùúã] ‚ÜíS‚ÑÇ(0, 1)
t ÓÇ∂‚Üíexp (it)
of the interval ] ‚àíùúã, ùúã] onto the unit circle
S‚ÑÇ(0, 1) in ‚ÑÇcentered at the origin, the
Fourier series transformation may also be
considered as acting on L2 (S‚ÑÇ(0, 1)
). This
induces the unitary transformation
Œ†1 ‚à∂L2 (S‚ÑÇ(0, 1)
) ‚ÜíL2 (] ‚àíùúã, ùúã])
ùúìÓÇ∂‚ÜíŒ†1ùúì,
where
(Œ†1ùúì)
(t) ‚âîùúì(exp (it)) .
This is easily seen to be an isometry. Indeed,
noting that for the line element ds(z) on
S‚ÑÇ(0, 1) = {exp (it) | t ‚àà] ‚àíùúã, ùúã]}, we have
ds (exp (it)) = dt,
and it follows that
‚à´S‚ÑÇ(0,1)
|ùúì(z)|2 ds (z) = ‚à´
ùúã
‚àíùúã
|ùúì(exp (it))|2 dt
= ‚à´
ùúã
‚àíùúã
|||
(Œ†1ùúì)
(t)|||
2
dt.
By this transformation, it follows that
(z ÓÇ∂‚Üí1‚àï
‚àö
2ùúãzk)k‚àà‚Ñ§is an orthonormal basis
for L2(S‚ÑÇ(0, 1)) and we have the unitary
mapping
F# Œ†1 ‚à∂L2 (S‚ÑÇ(0, 1)
)
‚ÜíùìÅ2 (‚Ñ§) ,
ùúìÓÇ∂‚Üí(F# (ùúì(exp (i ‚ãÖ))) (k)
)
k‚àà‚Ñ§,
as the corresponding Fourier series trans-
formation. Thus for all ùúì‚ààL2 (S‚ÑÇ(0, 1)
) ,
ùúì(z) =
1
‚àö
2ùúã
‚àû
‚àë
k=‚àí‚àû
ÃÇùúì(k) zk
for almost all z ‚ààS‚ÑÇ(0, 1) where
ÃÇùúì(k) =
1
‚àö
2ùúã‚à´S‚ÑÇ(0,1)
z‚àíkùúì(z) ds (z) .
This result is simply a restatement of the
classical Fourier series transformation for
periodic functions because such functions
may be understood as functions deÔ¨Åned on
the unit circle.
15.3
The z-Transformation
Rather than taking the unit circle, we
may repeat the discussion of the previous
section on a circle S‚ÑÇ(0, r) ‚äÜ‚ÑÇof radius
r ‚àà‚Ñù>0 centered at the origin. Thus, we
consider the transformation
] ‚àíùúã, ùúã] ‚ÜíS‚ÑÇ(0, r)
t ÓÇ∂‚Üír exp (it)
of the interval ] ‚àíùúã, ùúã] onto the circle
S‚ÑÇ(0, r). The induced unitary transforma-
tion is now
Œ†r ‚à∂L2 (S‚ÑÇ(0, r)
) ‚ÜíL2 (] ‚àíùúã, ùúã])
ùúìÓÇ∂‚ÜíŒ†rùúì,
where
(Œ†rùúì)
(t) ‚âî
‚àö
rùúì(r exp (it)) .
This time, the line element ds(z) on the
circle S‚ÑÇ(0, r) = {r exp(it) | t ‚àà] ‚àíùúã, ùúã]} is

510
15 Mathematical Transformations
ds (r exp (it)) = r dt,
and therefore
‚à´S‚ÑÇ(0,r)
|ùúì(z)|2 ds(z) =‚à´
ùúã
‚àíùúã
|ùúì(r exp(it))|2 r dt
= ‚à´
ùúã
‚àíùúã
||Œ†rùúì(t)||
2 dt
showing
that
Œ†r ‚à∂L2(S‚ÑÇ(0, r)) ‚Üí
L2(] ‚àíùúã, ùúã]) is indeed an isometry. It
follows
that
(
z ÓÇ∂‚Üí1‚àï
‚àö
2ùúãr(z‚àïr)k)
k‚àà‚Ñ§
forms an orthonormal basis of L2(S‚ÑÇ(0, r))
and we have the unitary mapping F# Œ†r,
that is,
F# Œ†r ‚à∂L2 (S‚ÑÇ(0, r)
) ‚ÜíùìÅ2 (‚Ñ§) ,
ùúìÓÇ∂‚Üí(ÃÉùúì(k))
k‚àà‚Ñ§,
with
ÃÉùúì(k) ‚âî(F#(
‚àö
rùúì(r exp(i ‚ãÖ))))(k)
for
k ‚àà‚Ñ§as the corresponding Fourier series
transformation. The inverse transforma-
tion, that is, the reconstruction of ùúì, is
given by
ùúì(z) =
1
‚àö
2ùúãr
‚àë
k‚àà‚Ñ§
r‚àík ÃÉùúì(k) zk
for
almost
all
z ‚ààS‚ÑÇ(0, r).
This
lat-
ter formula gives rise to the so-called
z-transformation. Consider an exponen-
tially weighted ùìÅ2-type space: r‚àím[ùìÅ2(‚Ñ§)]
‚âî{(ak)k‚àà‚Ñ§| ‚àë
k‚àà‚Ñ§|ak|2r2k < ‚àû}.
For
r ‚àà‚Ñù>0, we deÔ¨Åne
Zr ‚à∂r‚àím [ùìÅ2 (‚Ñ§)
] ‚ÜíL2 (S‚ÑÇ(0, r)
)
(
ak
)
k‚àà‚Ñ§ÓÇ∂‚Üí
1
‚àö
2ùúãr
‚àë
k‚àà‚Ñ§
ak zk.
Thus
we
have
a
family
of
z-
transformations6)
parameterized
by
6) The z-transformation is usually considered with-
out the normalizing factor 1‚àï
‚àö
2ùúãr. This factor
is needed to produce a unitary z-transformation.
r ‚àà‚Ñù>0. The image of (ak
)
k‚àà‚Ñ§under
such a z‚àítransformation is frequently
referred to as its generating function. In
applications, it is often the case that we
have ak = 0 for k < 0.
Depending on the sequence (ak
)
k‚àà‚Ñ§it
may happen that its generating function
Œ¶ ‚âîZr
((ak
)
k‚àà‚Ñ§
) has an analytic exten-
sion
to
an
annulus
R‚ÑÇ
(0, r1, r2
) ‚âî
B‚ÑÇ
(0, r2
) ‚ßµB‚ÑÇ
(0, r1
),
r2 > r1 > 0.
Then
methods from the theory of analytic func-
tions are available for further consideration
of the properties of the sequence (ak
)
k‚àà‚Ñ§
and
for
comparison
with
other
such
sequences in overlapping annuli. Typical
applications are to diÔ¨Äerence equations,
which recursively deÔ¨Åne such sequences
(ak
)
k‚àà‚Ñ§(usually with ak = 0 for k < 0).
The sequence can then be identiÔ¨Åed as
the coeÔ¨Écients in the Laurent expansion
of an analytic function Œ¶ (its generating
function).
15.4
The Fourier‚ÄìLaplace Transformation
The Fourier‚ÄìLaplace transformation is a
generalization of the Fourier‚ÄìPlancherel
transformation
F ‚à∂L2 (‚Ñù) ‚ÜíL2 (‚Ñù)
de-
Ô¨Åned by (15.2) where the integral over ‚Ñùis
understood as the L2-limit of the integral
over [‚àín, n], the so-called ‚Äúlimit in the
mean‚Äù, see [2]. Given ùúö‚àà‚Ñù, deÔ¨Åne
Hùúö,0 (‚Ñù) ‚âîL2 (‚Ñù, exp (‚àí2ùúöm) dùúÜ, ‚ÑÇ)
where the inner product of Hùúö,0 (‚Ñù) is
(f , g) ÓÇ∂‚Üí‚à´‚Ñù
f (t) g (t) exp (‚àí2ùúöt) dt.
The Fourier‚ÄìLaplace transformation is a
family of unitary maps
Lùúö‚âîùêπexp(‚àíùúöm) ‚à∂Hùúö,0(‚Ñù) ‚ÜíL2(‚Ñù),

15.4 The Fourier‚ÄìLaplace Transformation
511
for ùúå‚àà‚Ñùthat is,
(Lùúöf )
(x)
=
1
‚àö
2ùúã‚à´‚Ñù
exp (‚àíixy) exp (‚àíùúöy) f (y) dy
=
1
‚àö
2ùúã‚à´‚Ñù
exp (‚àí(ix + ùúö) y) f (y) dy
=
1
‚àö
2ùúã‚à´‚Ñù
exp (‚àíi (x ‚àíiùúö) y) f (y) dy
‚âïÃÇf (x ‚àíiùúö) ,
where
ÃÇf = Ff ,
the
Fourier‚ÄìPlancherel
transform of f .
It is clear that exp (‚àíùúöm) deÔ¨Åned by
(exp (‚àíùúöm) ùúì) (x) = exp (‚àíùúöx) ùúì(x)
for
almost
all
x ‚àà‚Ñù,
is
a
unitary
map-
ping of Hùúö,0 (‚Ñù) onto L2 (‚Ñù) and so the
Fourier‚ÄìLaplace transformation is unitary
because it is the composition of two uni-
tary maps. A Fourier‚ÄìLaplace transform is
frequently interpreted as an element of the
form ÃÇf (‚àíi ‚ãÖ) in L2 (i [‚Ñù] + ùúö) given by
i [‚Ñù] + ùúö‚Üí‚ÑÇ
p ÓÇ∂‚Üí
1
‚àö
2ùúã‚à´‚Ñù
exp (‚àípy) f (y) dy
= ÃÇf (‚àíip) .
Here L2 (i [‚Ñù] + ùúö) is a Hilbert space with
inner product
(f , g) ÓÇ∂‚Üí‚à´i[‚Ñù]+ùúö
f (z) g (z) ds (z)
induced by the complex transformation
x ÓÇ∂‚Üíix + ùúö
yielding
ds (ix + ùúö) = dx.
The importance of the Fourier‚ÄìLaplace
transformation lies in the property that it
is a spectral representation associated with
the operator
ùúïùúö‚âîexp (ùúöm) (ùúï+ ùúö) exp (‚àíùúöm)
representing diÔ¨Äerentiation in Hùúö,0 (‚Ñù),
where ùúïrepresents ordinary diÔ¨Äerentiation
in L2(‚Ñù). Indeed, we have the unitary
equivalence7)
ùúïùúö= L‚àó
ùúö(im + ùúö) Lùúö.
(15.8)
Note that H0,0 (‚Ñù) = L2 (‚Ñù) and L0 = F,
the
Fourier‚ÄìPlancherel
transformation.
Observing
that
L2 (] ‚àíùúã, ùúã])
may
be
considered as a subspace of L2 (‚Ñù) of
(equivalence classes of) functions van-
ishing (almost everywhere) outside of
] ‚àíùúã, ùúã], we have for the Fourier series
transformation F# discussed above that
F#f = ((Ff )
(k)
)
k‚àà‚Ñ§
= ((L0f )
(k)
)
k‚àà‚Ñ§
for
all
f ‚ààL2 (] ‚àíùúã, ùúã]) ‚äÜL2 (‚Ñù) .
As
already noted for the Fourier transfor-
mation and in the context of the Fourier
series transformation, there are also var-
ious, essentially equivalent, versions of
the Fourier‚ÄìLaplace transformation in
use and in applying ‚Äústandard results‚Äù
from tables, care needs to be taken to
ensure that formulae are correct for the
version of the transformation in use. The
Fourier‚ÄìLaplace transformation variant8)
ÃÉLùúö‚âîùúé2ùúãLùúö
(15.9)
7) This could be rephrased as saying that Lùúöis
a spectral representation for (1‚àïi) (ùúïùúö‚àíùúö) =
(1‚àïi)ùúïùúö+ iùúö.
8) In general, the Fourier‚ÄìLaplace transformation
interacts with rescaling, (15.5), in the following
way
ùúéLLùúö= Lùúö‚àïLùúé1‚àïL, L ‚àà‚Ñù‚ßµ{0} ;
that is, smaller scale is turned into larger scale
and vice versa.

512
15 Mathematical Transformations
has the advantage that neither ÃÉLùúönor its
inverse ÃÉL‚àí1
ùúö
= ÃÉL‚àó
ùúöhave a numerical factor in
front of the integral expression deÔ¨Åning it.
This results in simpliÔ¨Åcations of various for-
mulae. In contrast, ÃÉLùúöyields a more compli-
cated unitary equivalence to diÔ¨Äerentiation
of the form
ùúïùúö= ÃÉL‚àó
ùúö(2ùúãim + ùúå)ÃÉLùúö,
which may not be desirable. The case ùúö= 0
leads to a variant of the Fourier transforma-
tion, Óà≤‚âîÃÉL0 given by
(Óà≤f )(x) = ‚à´‚Ñù
exp (‚àíi2ùúãxy) f (y) dy,
which is popular in particular areas of
physics and electrical engineering. We
shall, however, maintain the use of the
Fourier‚ÄìLaplace transformation version
LùúödeÔ¨Åned above and the special case of
the
Fourier‚ÄìPlancherel
transformation
F = L0.
15.4.1
Convolutions as Functions of ùùèùùî
15.4.1.1
Functions of ùùèùùî
A spectral representation allows us to
straightforwardly deÔ¨Åne functions of an
operator. Generalizing (15.8), we have for
any polynomial P
P (ùúïùúö
) = L‚àó
ùúöP (im + ùúö) Lùúö.
This suggests the deÔ¨Ånition of more general
functions of ùúïùúöas
S(ùúïùúö) ‚âîL‚àó
ùúöS(i m + ùúö)Lùúö,
(15.10)
where S can be fairly general (and even
matrix-valued, in which case the transfor-
mation is carried out entry by entry). In
the case ùúö= 0, functions of ùúïare known as
Ô¨Ålters in the context of signal processing.
Owing to the unitary equivalence between
ùúïùúöand ùúï+ ùúövia the unitary transformation
exp (‚àíùúöm) ‚à∂Hùúö,0 (‚Ñù) ‚ÜíL2 (‚Ñù), we obtain
the unitary equivalence
S
(
ùúïùúö
) = exp (ùúöm) S (ùúï+ ùúö) exp (‚àíùúöm) ,
where
S (ùúï+ ùúö) = F‚àóS(i m + ùúö) F.
(15.11)
This indicates that it would suÔ¨Éce to con-
sider only the Fourier‚ÄìPlancherel transfor-
mation, that is, the case ùúö= 0, taking the
exponential weight exp (‚àíùúö‚ãÖ) into account
separately.
The concept of a function of ùúïùúöis
extremely powerful because many mod-
els discussed in so-called linear systems
theory; see [10, 11], can be understood as
such a function S(ùúïùúö) acting on an input u
resulting in an output S(ùúïùúö)u. Character-
istic properties of such systems are that
their input‚Äìoutput relations are linear and
translation invariant, that is, sums, multi-
ples, and shifts of input result in the same
sums, multiples, and (up to scaling) shifts
of output. Indeed, translation ùúèh deÔ¨Åned
by
ùúèhf ‚âîf ( ‚ãÖ+ h)
is itself a function of ùúïùúöin the above sense.
Indeed, for suitable f we may calculate
exp(h (ix + ùúö)) (Lùúåf )(x)
=
1
‚àö
2ùúã‚à´‚Ñù
exp (‚àí(ix + ùúö)
(y ‚àíh)) f (y) dy
=
1
‚àö
2ùúã‚à´‚Ñù
exp (‚àí(ix + ùúö) s) f (s + h) ds
= (Lùúöùúèhf )
(x)
showing that
ùúèh = L‚àí1
ùúöexp(h (im + ùúö)) Lùúö= exp(hùúïùúö).

15.4 The Fourier‚ÄìLaplace Transformation
513
It is interesting to note that by substitut-
ing the series expansion for the exponen-
tial function, this formula (if applied to a
suitable function f ) is easily recognized as
nothing but the Taylor expansion of f about
a point x.
As another example of functions of ùúïùúö,
we consider fractional calculus [12]. For ùúö‚àà
‚Ñù>0 we obtain ùúï‚àí1
ùúö
as forward causal inte-
gration. Indeed, for integrable functions f
in Hùúö,0 (‚Ñù), we have
(
ùúï‚àí1
ùúöf
)
(t) = ‚à´
t
‚àí‚àû
f (s) ds, t ‚àà‚Ñù.
(15.12)
Fractional powers of ùúï‚àí1
ùúö
can now be
deÔ¨Åned by
ùúï‚àíùõº
ùúö
‚âîL‚àó
ùúö(im + ùúö)‚àíùõºLùúö, ùõº‚àà[0, 1[ ,
and9)
ùúï‚àíùõΩ
ùúö
‚âîùúï‚àí‚åäùõΩ‚åã
ùúö
ùúï‚åäùõΩ‚åã‚àíùõΩ
ùúö
, ùõΩ‚àà‚Ñù.
(15.13)
Note that ùõΩ‚àí‚åäùõΩ‚åã‚àà[0, 1[ and z ÓÇ∂‚Üízùõºfor
ùõº‚àà] ‚àí1, 0[ is the principle root function,
that is, with z = |z| exp (i arg (z)
) , arg (z) ‚àà
] ‚àíùúã, ùúã], we have
zùõº‚âî|z|ùõºexp (i ùõºarg (z)
) .
For positive ùõΩ, (15.13) is the fractional inte-
gral and, for negative ùõΩ, equation (15.13)
deÔ¨Ånes fractional diÔ¨Äerentiation. Here, ùúö‚àà
‚Ñù>0 is important because for ùúö‚àà‚Ñù<0, we
obtain the backward causal integral, and for
ùúö= 0, the inverse of diÔ¨Äerentiation does not
exist; there are the candidates correspond-
ing to the forward or the backward causal
9) Here ‚åäùõΩ‚åãdenotes the Ô¨Çoor function evaluated
at ùõΩ‚àà‚Ñù, which is the largest integer less than
or equal to ùõΩ, that is, ‚åäùõΩ‚åã‚âîsup {k ‚àà‚Ñ§| k ‚â§ùõΩ} .
Note that ‚àí‚åäùõΩ‚åã= ‚åà‚àíùõΩ‚åâfor ùõΩ‚àà‚Ñù,
‚åàùõæ‚åâ‚âîinf {k ‚àà‚Ñ§| ùõæ‚â§k} for ùõæ‚àà‚Ñù.
situation both of which are unbounded lin-
ear operators.
In other cases, or following the observa-
tion (15.11), the choice ùúö= 0 is of particular
interest. An example of a function of ùúï(i.e.,
in the case ùúö= 0) is the so-called Hilbert
transformation
H ‚âîF‚àó1
i sgn (m) F,
= 1
i sgn
(1
i ùúï
)
,
= i sgn (iùúï) .
We see that H is a unitary mapping in L2 (‚Ñù)
because (1‚àïi) sgn (m) is a multiplication
operator on L2 (‚Ñù) with ||(1‚àïi)sgn (m)|| = 1
and
H2 = ‚àí1.
Consequently,
H‚àó= H‚àí1 = ‚àíH
and so H is skew-selfadjoint and unitary in
L2 (‚Ñù).
15.4.1.2
Convolutions
Assuming
that
S
allows
for
a
Fourier‚ÄìLaplace
transformation
by
Lùúö
and
(
ÃÇS
) (
(1‚àïi)ùúïùúö
) is a function of ùúïùúöin the
above sense, we deÔ¨Åne
S ‚àóg ‚âî
‚àö
2ùúã
(
ÃÇS
) (1
i ùúïùúö
)
g,
(15.14)
which is well deÔ¨Åned for any g in the
domain of
‚àö
2ùúã
(
ÃÇS
) ((1‚àïi)ùúïùúö
). In many
cases, S ‚àóg is ‚Äìat least for ‚Äúwell-behaved‚Äù
functions g ‚ààHùúö,0 (‚Ñù) ‚Äìactually given as
an integral expression, which is then a
so-called convolution integral:

514
15 Mathematical Transformations
(S ‚àóg)(x) = ‚à´
‚àû
‚àí‚àû
S(x ‚àíy) g(y) dy
(15.15)
(hence the name convolution operator for
S ‚àó‚âî
‚àö
2ùúã
(
ÃÇS
) ((1‚àïi)ùúïùúö
)). In accordance
with (15.14), for such convolution integrals,
the so-called convolution theorem holds
Lùúö(S ‚àóg) =
‚àö
2ùúã
(
LùúöS
)
(m)
(
Lùúög
)
=
‚àö
2ùúãLùúöS ‚ãÖLùúög.
(15.16)
By the above deÔ¨Ånition, this always holds,
even if there is no actual integral expression
involved.
To illustrate this, let us Ô¨Årst consider the
fractional integral. For ùúö‚àà‚Ñù>0, we obtain
ùúï‚àíùõº
ùúöf = Œ¶ ‚àóf ,
where
Œ¶ (t) ‚âî
1
Œì (ùõº)ùúí
‚Ñù>0 (t) tùõº‚àí1.
For suÔ¨Éciently well-behaved functions f ‚àà
Hùúö,0 (‚Ñù) (so that the integral exists), this
convolution (in the general sense) can actu-
ally be written as a convolution integral10)
(
ùúï‚àíùõº
ùúöf
)
(t)
=
1
Œì (ùõº) ‚à´‚Ñù
ùúí‚Ñù>0 (t ‚àís) (t ‚àís)ùõº‚àí1 f (s) ds
=
1
Œì (ùõº) ‚à´
t
‚àí‚àû
(t ‚àís)ùõº‚àí1 f (s) ds, t ‚àà‚Ñù.
In particular, the case ùõº= 1‚àï2 is referred to
as the Abel transformation which generates
a unitary transformation. Indeed, the map-
ping
10) Frequently, there is an implicit assumption that
f vanishes for negative arguments in which
case we get the convolution integral expression
1
Œì (ùõº) ùúí
‚Ñù>0 (t) ‚à´
t
0
(t ‚àís)ùõº‚àí1 f (s) ds, t ‚àà‚Ñù.
ùúï‚àí1‚àï2
ùúö
‚à∂Hùúö,0 (‚Ñù) ‚ÜíHùúö,0 (‚Ñù)
is clearly an isometry if the Ô¨Årst space
Hùúö,0 (‚Ñù) is equipped with the inner product
(f , g) ÓÇ∂‚Üí
‚ü®
ùúï‚àí1‚àï2
ùúö
f |ùúï‚àí1‚àï2
ùúö
g
‚ü©
ùúö,0 .
Denoting the smallest Hilbert space with
this inner product containing Hùúö,0 (‚Ñù)
by Hùúö,‚àí1‚àï2 (‚Ñù), the Abel transformation
Aùúö‚à∂Hùúö,‚àí1‚àï2 (‚Ñù) ‚ÜíHùúö,0 (‚Ñù), deÔ¨Åned as the
continuous extension of ùúï‚àí1‚àï2
ùúö
, is unitary.
As Aùúöis essentially a fractional integral, it
follows that A‚àó
ùúö= A‚àí1
ùúöis related to the frac-
tional derivative ùúï1‚àï2
ùúö
= ùúïùúöùúï‚àí1‚àï2
ùúö
. The con-
tinuous extension of ùúï1‚àï2
ùúö
to a mapping from
Hùúö,0 (‚Ñù) to Hùúö,‚àí1‚àï2 (‚Ñù) corresponds to A‚àó
ùúö.
The integral representation of Aùúöf , when
f = 0 on ‚Ñù<0 and is suitably well behaved,
is
(Aùúöf )
(t)
=
‚àö
1
ùúãùúí‚Ñù>0 (t)‚à´
t
0
(t ‚àís)‚àí1‚àï2 f (s) ds, t ‚àà‚Ñù.
Letting r‚àí2 = t and substituting s = w‚àí2
yields
r‚àí1 (Aùúöf
) (
r‚àí2)
=2
‚àö
1
ùúãr‚àí1
‚à´
‚àû
r
(
r‚àí2‚àíw‚àí2)‚àí1‚àï2f (w‚àí2)
w‚àí3 dw
=2
‚àö
1
ùúã‚à´
‚àû
r
(w2‚àír2)‚àí1‚àï2(f (w‚àí2) w‚àí3) w dw.
The function
r ÓÇ∂‚Üí2
‚àö
1
ùúã‚à´
‚àû
r
(w2 ‚àír2)‚àí1‚àï2 g (w) w dw
is frequently referred to as the Abel
transform
of
g.
Introducing
suitably
weighted spaces this can (by the unitar-
ity of Aùúö) also be realized as a unitary
transformation.

15.5 The Fourier‚ÄìLaplace Transformation and Distributions
515
15.4.2
The Fourier‚ÄìPlancherel Transformation
Focusing on the case ùúö= 0, we may
consider the Fourier‚ÄìPlancherel transfor-
mation as a unitary mapping in L2 (‚Ñù) in
its own right. Every unitary operator has
its spectrum on the unit circle S‚ÑÇ(0, 1) .
The spectrum of the Fourier‚ÄìPlancherel
transformation
L0 = F
is
particularly
simple. It is pure point spectrum and
consists of the four points in {1, i, ‚àí1, ‚àíi}.
The corresponding orthonormal basis of
eigensolutions is given by11)
(
2‚àík‚àï2
‚àö
k!
(m ‚àíùúï)k ùõæ
)
k‚àà‚Ñï
,
where ùõæis the Gaussian distribution func-
tion deÔ¨Åned by ùõæ(x) = ùúã‚àí1‚àï4 exp (‚àíx2‚àï2) ,
normalized such that |ùõæ|L2(‚Ñù) = 1. These
have the form
2‚àík‚àï2
‚àö
k!
(m ‚àíùúï)k ùõæ= Pk (m) ùõæ,
where Pk
is, up to a renormalization
constant,
the
Hermite
polynomial
of
degree k, k ‚àà‚Ñï. Thus, the spectral rep-
resentation takes on the simple form
discussed in general in Section 15.2.1 and
the Fourier‚ÄìPlancherel transform ÃÇf = Ff
of f ‚ààL2 (‚Ñù) takes the form
‚àû
‚àë
k=0
eikùúã‚àï2
‚ü®
(m ‚àíùúï)k
2kk!
ùõæ
|||||
f
‚ü©
L2(‚Ñù)
(m ‚àíùúï)k ùõæ.
11) The adjoint of the operator 1‚àï
‚àö
2 (m ‚àíùúï) is
1‚àï
‚àö
2 (m + ùúï) and their product
1
2 (m + ùúï) (m ‚àíùúï) = 1
2
((m2 ‚àíùúï2) + 1) ,
which is the (quantum-mechanical) harmonic
oscillator, has the same eigensolutions as the
Fourier‚ÄìPlancherel transformation with associ-
ated point spectrum ‚Ñï.
Recalling that a spectral representation
of an operator allows the deÔ¨Ånition of func-
tions of the operator, we may deÔ¨Åne func-
tions of the Fourier‚ÄìPlancherel transfor-
mation F. A function S deÔ¨Åned on the
spectrum {1, i, ‚àí1, ‚àíi} then gives rise to an
operator S (F) given by
S (F) f
=
‚àû
‚àë
k=0
S(eikùúã‚àï2)
‚ü®
(m ‚àíùúï)k
2kk!
ùõæ
|||||
f
‚ü©
L2(‚Ñù) (m‚àíùúï)kùõæ.
For example, we may deÔ¨Åne fractional
Fourier‚ÄìPlancherel
transformations
Fùõº,
ùõº‚àà‚Ñù, by
Fùõºf
‚âî
‚àû
‚àë
k=0
eikùõºùúã‚àï2
‚ü®
(m ‚àíùúï)k
2kk!
ùõæ
|||||
f
‚ü©
L2(‚Ñù)
(m ‚àíùúï)k ùõæ,
which have in recent years found many
applications in physics and engineering (see
[13]).
15.5
The Fourier‚ÄìLaplace Transformation and
Distributions
We can extend the meaning of the L2 (‚Ñù)-
inner-product,
which
we
now
denote
simply by ‚ü®‚ãÖ| ‚ãÖ‚ü©, by introducing general-
ized functions f (also called distributions,
see [14]) as linear functionals. Initially,
for
simplicity,
consider
the
so-called
space
of
‚Äútest
functions,‚Äù
C‚àû
0 (‚Ñù),
of
inÔ¨Ånitely diÔ¨Äerentiable functions on ‚Ñù
that have compact support, that is, they
vanish outside a bounded interval, and let
f ‚à∂C‚àû
0 (‚Ñù) ‚Üí‚ÑÇ, ùúôÓÇ∂‚Üíf (ùúô) be a linear func-
tional. We introduce the ‚Äúinner product‚Äù
notation
‚ü®f | ùúô‚ü©‚âîf (ùúô).

516
15 Mathematical Transformations
Using this notation, the linearity of f is
expressed by
‚ü®f | ùúô+ ùõºùúì‚ü©= ‚ü®f | ùúô‚ü©+ ùõº‚ü®f | ùúì‚ü©
for
all
complex
numbers
ùõº
and
all
ùúô, ùúì‚ààC‚àû
0 ‚Ñù. Clearly, for any f ‚ààL2‚Ñù
the inner product ‚ü®f |ùúô‚ü©deÔ¨Ånes a general-
ized function (i.e., a complex-valued linear
functional deÔ¨Åned for ùúô‚ààC‚àû
0 ‚Ñù), but for
example exp (i p ‚ãÖ) , p ‚àà‚Ñù, although not an
element of L2‚Ñù, also deÔ¨Ånes a generalized
function via
‚ü®exp(i p ‚ãÖ)|ùúô‚ü©‚âî‚à´‚Ñù
exp(i px) ùúô(x) dx
=
‚àö
2ùúãÃÇùúô(p)
for all ùúô‚ààC‚àû
0 ‚Ñù. Further, a generalized
function, which is not given by any classical
function, is ùõø{ùúî} deÔ¨Åned by
ùõø{ùúî}(ùúô)=‚ü®ùõø{ùúî} | ùúô‚ü©‚âîùúô(ùúî) for all ùúô‚ààC‚àû
0 ‚Ñù.
This distribution samples, that is, evalu-
ates, the test function ùúôat the point ùúî‚àà‚Ñù.
The special choice ùõø‚âîùõø{0} is the so-
called Dirac-ùõø-distribution. As it can only
be distinguished from 0 by testing with
ùúô‚ààC‚àû
0 ‚Ñùsatisfying ùúô(0) ‚â†0, we often say
ùõø= 0 on ‚Ñù‚ßµ{0}. However, ùõøis (by deÔ¨Åni-
tion) not identically zero as a functional.
It is particularly useful in many contexts
to have ùõø{ùúî} (= ùúè‚àíùúîùõø) as a mathematical
model for physical phenomena that are
vanishingly small in space or time such as
a mass point, an elementary charge, a light
point, a short impulse, or a particle at loca-
tion ùúî(see Section 15.9 for the particularly
interesting higher-dimensional case).
An
important
class
of
distributions
is the space of so-called tempered dis-
tributions Óàø‚Ä≤ (see [14]). With suÔ¨Écient
care,
another
space
of
distributions
exp (ùúöm)
[Óàø‚Ä≤] (distributions which after
multiplication by exp (‚àíùúöm) , are in Óàø‚Ä≤)
can be established for which the concept of
derivative and Fourier‚ÄìLaplace transfor-
mation can be generalized by deÔ¨Åning, for
f ‚ààexp (ùúöm)
[
Óàø‚Ä≤],
‚ü®ùúïf |ùúô‚ü©‚âî‚àí‚ü®f |ùúïùúô‚ü©,
‚ü®Lùúöf |ùúô‚ü©‚âî
‚ü®
f | L‚àó
‚àíùúöùúô
‚ü©
,
(15.17)
‚ü®
L‚àó
ùúöf |ùúô
‚ü©
‚âî‚ü®f | L‚àíùúöùúô‚ü©
for all ùúô‚ààC‚àû
0 ‚Ñù.
As an example of generalized diÔ¨Äerentia-
tion, for the characteristic function ùúí
]0,‚àû[ of
the interval ]0, ‚àû[ , we get
ùúïùúí
]0,‚àû[ = ùõø;
that is, the generalized derivative of the
characteristic function of ]0, ‚àû[ is the
Dirac-ùõø-distribution.
The Fourier‚ÄìLaplace transform of the
distribution ùõø{ùúî} is ‚Äìaccording to (15.17) ‚Äì
Lùúöùõø{ùúî} =
1
‚àö
2ùúã
exp (‚àíi ( ‚ãÖ‚àíiùúö) ùúî) .
(15.18)
Conversely, we have
L‚àó
ùúöexp(‚àíi ùúî‚ãÖ) =
‚àö
2ùúãexp (ùúöùúî) ùõø{ùúî}.
(15.19)
The latter fact can be used to detect
oscillatory behavior of frequency ùúîthat
may be hiding in seemingly random data.
The oscillation would show up in the
approximately
Fourier‚ÄìLaplace
trans-
formed data as a ‚Äúpeak‚Äù or ‚Äúspike‚Äù at the
point ùúî.
The concept of functions of ùúïùúöand, in
particular, the concept of convolution
can also be carried over to exponen-
tially weighted tempered distributions in
exp (ùúöm)
[Óàø‚Ä≤]. This will be illustrated by a
few applications.

15.5 The Fourier‚ÄìLaplace Transformation and Distributions
517
15.5.1
Impulse Response
The importance of the Dirac-ùõø-distribution
is that a translation-invariant, linear system
S(ùúïùúö) is completely described by its impulse
response S(ùúïùúö)ùõø. The general response
S(ùúïùúö) f is given by convolution with the
impulse response
S(ùúïùúö) f = (S(ùúïùúö)ùõø) ‚àóf .
In the case that S(ùúïùúö) is the solution oper-
ator S(ùúïùúö) = P(ùúïùúö)‚àí1
of the diÔ¨Äerential
equation P(ùúïùúö)u = f , where P is a polyno-
mial, the impulse response is also known
as the fundamental solution (or Green‚Äôs
function), cf. [14].
15.5.2
Shannon‚Äôs Sampling Theorem
Another distribution of particular interest
is the (sampling or) comb distribution
‚âî‚àë
k‚àà‚Ñ§ùõø{‚àö
2ùúãk
}, which takes samples
at
equidistant
points
(distance
‚àö
2ùúã).
This distribution is reproduced by the
Fourier‚ÄìPlancherel transformation
F
=
,
(15.20)
which is a way of stating the Poisson sum-
mation formula:
F
‚àë
k‚àà‚Ñ§
ùõø{‚àö
2ùúãk
} =
1
‚àö
2ùúã
+‚àû
‚àë
k=‚àí‚àû
exp
(
‚àíi
‚àö
2ùúãk ‚ãÖ
)
=
+‚àû
‚àë
k=‚àí‚àû
ùõø{
‚àö
2ùúãk}.
Note that for the Ô¨Årst equality we have used
(15.18).
If
f ‚ààL2(‚Ñù)
and
f = 0
outside
of
[‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2], then
‚àóf is simply the
periodic extension of f from the interval
[‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2], to all of ‚Ñù. Therefore,
by ‚Äúcutting-oÔ¨Ä‚Äù with the characteristic
function Œ† ‚âîùúí
[‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2], we recover
f = Œ†(
‚àóf ).
(15.21)
As we shall see, the so-called Shannon sam-
pling theorem follows from (15.20). In fact,
applying (15.21) to ÃÇf ‚ààL2(‚Ñù) with ÃÇf = Ff =
0 outside of [‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2] and applying
the inverse Fourier transformation we get
f = F‚àó(Œ†(
‚àóÃÇf )),
=sinc
(‚àö
ùúã‚àï2‚ãÖ
)
‚àó
( +‚àû
‚àë
n=‚àí‚àû
f (
‚àö
2ùúãn) ùõø{
‚àö
2ùúãn}
)
,
=
+‚àû
‚àë
n=‚àí‚àû
f (
‚àö
2ùúãn) sinc
(‚àö
ùúã‚àï2
(
‚ãÖ‚àí
‚àö
2ùúãn
))
,
(15.22)
which is Shannon‚Äôs theorem. Here we have
used the fact that (F‚àóŒ†)(t) = sinc(
‚àö
ùúã‚àï2t)
and sinc(t) ‚âîsin(t)‚àït.
Any function f with ÃÇf vanishing outside
a bounded interval is called band limited.
As any such function can be easily rescaled
to have ÃÇf = 0 outside of [‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2],
(15.22) shows that any band-limited func-
tion f can be completely recovered from
equidistant sampling.
By interchanging the role of f and ÃÇf in the
Shannon sampling theorem, we have
ÃÇf =sinc
(‚àö
ùúã
2 ‚ãÖ
)
‚àó
( +‚àû
‚àë
n=‚àí‚àû
ÃÇf (
‚àö
2ùúãn) ùõø{
‚àö
2ùúãn}
)
(15.23)
and after applying the inverse Fourier trans-
formation we obtain a scaled variant of the
Fourier series expansion
f =
+‚àû
‚àë
n=‚àí‚àû
ÃÇf (
‚àö
2ùúãn) Œ† exp
(
i
‚àö
2ùúãn ‚ãÖ
)
(15.24)
of a function f ‚ààL2(‚Ñù) with f = 0 out-
side
of
[‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2].
Note
that

518
15 Mathematical Transformations
{(2ùúã)‚àí1‚àï4Œ† exp(i
‚àö
2ùúãn ‚ãÖ) | n ‚àà‚Ñ§}
is
an
orthonormal set in L2(‚Ñù), that is, all
elements are normalized and pairwise
orthogonal, which allows a Fourier series
expansion in the sense of (15.24) for
every f ‚ààL2(‚Ñù) with f = 0 outside of
[‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2].
As
the
Fourier‚ÄìPlancherel
transfor-
mation is unitary, it follows that the
Fourier‚ÄìPlancherel
transform
of
the
orthonormal basis
{
(2ùúã)‚àí1‚àï4 Œ† exp(i
‚àö
2ùúãn ‚ãÖ) | n ‚àà‚Ñ§
}
of
L2 (
] ‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2]
)
is
also
an
orthonormal basis. Thus,
{
(2ùúã)‚àí1‚àï4 sinc
(‚àö
ùúã‚àï2
(
‚ãÖ‚àí
‚àö
2ùúãn
))||||
n ‚àà‚Ñ§
}
is also an orthonormal set and indeed
an
orthonormal
basis
for
all
band-
limited functions in L2 (‚Ñù) with band
in [‚àí
‚àö
ùúã‚àï2,
‚àö
ùúã‚àï2].
15.6
The Fourier-Sine and Fourier-Cosine
Transformations
As
a
unitary
operator,
the
Fourier‚ÄìPlancherel
transformation
F
is also normal with real part Fcos ‚âî‚Ñúùî¢F =
(1‚àï2)(F + F‚àó), the Fourier cosine trans-
formation and negative imaginary part
Fsin ‚âî‚àí‚Ñëùî™F = ‚àí(1‚àï2i)(F ‚àíF‚àó),
the
Fourier sine transformation. Being real and
imaginary parts of a normal operator, the
Fourier cosine and the Fourier sine trans-
formations are selfadjoint and therefore
their spectra ùúé(Fcos) and ùúé(Fsin) are real.
We already know that
‚Ñúùî¢ùúé(F) = {+1, ‚àí1} ‚äÜùúé(Fcos),
‚Ñëùî™ùúé(F) = {+1, ‚àí1} ‚äÜùúé(Fsin).
The relation between Fcos, Fsin, and F yields
further insight. We have (with ùúé‚àí1 as the
reÔ¨Çection at the origin)
Fcos = 1
2(F + F‚àó),
= 1
2(1 + ùúé‚àí1) F,
= F 1
2(1 + ùúé‚àí1),
= 1
2(1 + ùúé‚àí1) F‚àó,
= F‚àó1
2(1 + ùúé‚àí1) ;
(15.25)
Fsin = ‚àí1
2i(F ‚àíF‚àó),
= 1
2(1 ‚àíùúé‚àí1) iF,
= iF 1
2(1 ‚àíùúé‚àí1).
It is not hard to see that 1‚àï2(1 + ùúé‚àí1) and
(1‚àï2)(1 ‚àíùúé‚àí1) = 1 ‚àí1
2(1 + ùúé‚àí1)
are
the
orthogonal projections onto the (almost
everywhere) even and odd functions in
L2(‚Ñù), respectively. Therefore, we also have
0 ‚ààùúé(Fcos) and 0 ‚ààùúé(Fsin). Moreover, we
calculate with (15.25)
FsinFcos = FcosFsin = 0,
(15.26)
Fsin Fsin = 1
2(1 ‚àíùúé‚àí1),
(15.27)
FcosFcos = 1
2(1 + ùúé‚àí1).
(15.28)
Thus, we see that Fcos, Fsin are unitary on
the subspaces
1
2(1 ¬± ùúé‚àí1) [L2(‚Ñù)], respec-
tively. This, along with the selfadjointness of
Fcos and Fsin, allows us to deduce that
ùúé(Fcos) = ùúé(Fsin) = Pùúé(Fcos)
= Pùúé(Fsin) = {0, +1, ‚àí1}.
We can identify
1
2(1 ¬± ùúé‚àí1) [L2(‚Ñù)] with
L2(‚Ñù>0)
via
the
unitary
transforma-
tions
E¬± ‚à∂L2(‚Ñù>0) ‚Üí1
2(1 ¬± ùúé‚àí1) [L2(‚Ñù)]

15.7 The Hartley Transformations H¬±
519
deÔ¨Åned by
(E¬±f )(x) ‚âî
1
‚àö
2
{
f (x)
for x ‚àà‚Ñù>0
¬±f (‚àíx)
for x ‚àà‚Ñù‚â§0,
so that, for f ‚ààL2(‚Ñù>0),
‚àö
2E+f is the even
extension of f and
‚àö
2E‚àíf is the odd exten-
sion of f . The calculation
||E¬±ùúë||
2
L2(‚Ñù) = ‚à´‚Ñù
||(E¬±ùúë)(x)||
2 dx,
= 1
2 ‚à´‚Ñù>0
|ùúë(x)|2 dx
+ 1
2 ‚à´‚Ñù<0
|ùúë(‚àíx)|2 dx,
= ‚à´‚Ñù>0
|ùúë(x)|2 dx = |ùúë|2
L2(‚Ñù>0) ,
for all ùúë‚ààL2(‚Ñù>0) shows that E¬± are uni-
tary and so the mappings
E‚àó
+ Fcos E+ ‚à∂L2(‚Ñù>0) ‚ÜíL2(‚Ñù>0),
E‚àó
‚àíFsinE‚àí‚à∂L2(‚Ñù>0) ‚ÜíL2(‚Ñù>0)
are also unitary. Here, E‚àó
¬± = E‚àí1
¬± are deÔ¨Åned
by
E‚àó
¬± ‚à∂1
2(1 ¬± ùúé‚àí1) [L2(‚Ñù)] ‚ÜíL2(‚Ñù>0),
ùúôÓÇ∂‚Üí
‚àö
2 ùúô||‚Ñù>0.
For ‚Äúnice‚Äù functions, for example, for ùúô‚àà
C‚àû
0 (‚Ñù>0), the space of smooth functions
vanishing outside of a compact set in ‚Ñù>0,
we get the following well-known integral
representations discussed in Section 15.1:
(ÃÉFcosùúô)(x) ‚âî(E‚àó
+ Fcos E+ùúô)(x)
=
‚àö
2
ùúã‚à´‚Ñù>0
cos(xy) ùúô(y) dy,
(ÃÉFsinùúô)(x) ‚âî(E‚àó
‚àíFsinE‚àíùúô)(x)
=
‚àö
2
ùúã‚à´‚Ñù>0
sin(xy) ùúô(y) dy.
The unitary transformations ÃÉFcos and ÃÉFsin
are spectral representations for |ùúï| with
no or Dirichlet type boundary condition,
respectively, at the origin. As noted in
Section 15.1, they can be used to dis-
cuss problems involving the second-order
diÔ¨Äerential operator ‚àíùúï2 = |ùúï|2 with Neu-
mann or Dirichlet boundary conditions at
the origin, respectively.
15.7
The Hartley Transformations H¬±
As the Fourier transformation F is unitary,
it follows immediately that exp(i ùúã‚àï4) F is
also unitary. Considering the real and imag-
inary part of exp(i ùúã‚àï4) F leads to an inter-
esting situation:
‚Ñúùî¢(exp(i ùúã‚àï4) F) =
1
‚àö
2
(Fcos + Fsin),
‚Ñëùî™(exp(i ùúã‚àï4) F) =
1
‚àö
2
(Fcos ‚àíFsin),
and therefore, for H¬± ‚âîFcos ¬± Fsin, we have
H+ = ‚Ñúùî¢
(‚àö
2 exp(i ùúã‚àï4) F
)
,
H‚àí= ‚Ñëùî™
(‚àö
2 exp(i ùúã‚àï4) F
)
and so for all ùúë‚ààL2(‚Ñù),
||H¬±ùúë||
2
L2(‚Ñù) = ||(Fcos ¬± Fsin)ùúë||
2
L2(‚Ñù)
= ||Fcosùúë||
2
L2(‚Ñù)
¬± 2‚Ñúùî¢
‚ü®
Fcosùúë||Fsinùúë
‚ü©
L2(‚Ñù)
+ ||Fsinùúë||
2
L2(‚Ñù)
= ||Fcosùúë||
2
L2(‚Ñù) + ||Fsinùúë||
2
L2(‚Ñù)
= |Fùúë| = |ùúë|2
L2(‚Ñù) .
The transformations H¬±, which are unitary
and selfadjoint, are known as Hartley trans-
formations. In particular,

520
15 Mathematical Transformations
ùúé(H¬±) = Pùúé(H¬±) = {+1, ‚àí1}
and
H+H‚àí= H‚àíH+
= FcosFcos ‚àíFsin Fsin
= 1
2(1 + ùúé‚àí1) ‚àí1
2(1 ‚àíùúé‚àí1)
= ùúé‚àí1,
H+H+ = H‚àíH‚àí
= FcosFcos + Fsin Fsin
= 1
2(1 + ùúé‚àí1) + 1
2(1 ‚àíùúé‚àí1)
= 1.
On C‚àû
0 (‚Ñù), we have
(H¬±ùúë)
(ùúî) =
1
‚àö
2ùúã‚à´‚Ñù
(cos (ùúît)
¬± sin(ùúît)) ùúë(t) dt
and
(
H‚àíùúë)
(ùúî) = (H+ùúë)
(‚àíùúî)
for ùúî‚àà‚Ñùor
H‚àí= ùúé‚àí1H+.
As the Fourier‚ÄìPlancherel transformation
has an extension to tempered distributions,
so have the Hartley transformations.
The Hartley transformations are real, that
is, they commute with complex conjuga-
tion, which may be an advantage in appli-
cations; see [15] for more details. On the
downside, the Hartley transformations H¬±
are spectral representations for the rarely
used operators ¬±ùúé‚àí1iùúï. Hidden periodic
behavior is revealed in a more complicated
fashion:
H¬± cos (ùúî‚ãÖ) =
‚àö
ùúã
2
(ùõø{ùúî} + ùõø{‚àíùúî}
) ,
H¬± sin (ùúî‚ãÖ) = ¬±
‚àö
ùúã
2
(ùõø{ùúî} ‚àíùõø{‚àíùúî}
) ,
indicating that period 2ùúã‚àïùúîbehavior in a
function will show up as spikes at ¬±ùúîin the
Hartley transforms of the function.
15.8
The Mellin Transformation
Let us consider the transformation
ln ‚à∂‚Ñù>0 ‚Üí‚Ñù
x ÓÇ∂‚Üíln (x) .
Analogous to the reasoning leading up to
(15.3), this transformation induces a uni-
tary mapping
Œõ ‚à∂L2 (‚Ñù) ‚Üí
‚àö
m [L2 (‚Ñù>0
)]
‚âîL2 (‚Ñù>0, m‚àí1dùúÜ, ‚ÑÇ)
f ÓÇ∂‚Üíf ‚àòln
with
Œõ‚àó= Œõ‚àí1 ‚à∂
‚àö
m [L2 (‚Ñù>0
)] ‚ÜíL2 (‚Ñù)
f ÓÇ∂‚Üíf ‚àòexp .
Unitarity is easily conÔ¨Årmed by the com-
putation done in Section 15.1. The unitary
mapping
M ‚âîFŒõ‚àó‚à∂
‚àö
m [L2 (‚Ñù>0
)] ‚Üí
L2 (‚Ñù) is known as the Mellin transforma-
tion. We Ô¨Ånd the unitary equivalence
mùúï= ŒõùúïŒõ‚àó
= ŒõF‚àóim FŒõ‚àó
and so that M is a spectral representation
for (1‚àïi)mùúï. For well-behaved (to ensure
that the integrals exist) f ‚àà
‚àö
m [L2 (‚Ñù>0
)]
and g ‚ààL2 (‚Ñù), we get
(Mf )
(x) =
1
‚àö
2ùúã‚à´‚Ñù
exp(‚àíixy)f (exp (y)) dy
=
1
‚àö
2ùúã‚à´‚Ñù>0
s‚àíix‚àí1f (s) ds, x ‚àà‚Ñù,

15.9 Higher-Dimensional Transformations
521
and
(M‚àóg)
(t) =
1
‚àö
2ùúã‚à´‚Ñù
exp (i ln (t) y) g (y) dy
=
1
‚àö
2ùúã‚à´‚Ñù
tiyg (y) dy, t ‚àà‚Ñù>0.
15.9
Higher-Dimensional Transformations
Following the rationale that most of the
classical
integral
transformations
are
spectral
representations
of
diÔ¨Äerential
operators or functions of such (see [16]),
to identify applicable higher-dimensional
transformations, we should be looking
for
spectral
representations
associated
with partial diÔ¨Äerential operators and
investigating the corresponding operator
function calculus associated with such a
spectral representation. For many appli-
cations, however, there is a somewhat
easier
approach
to
higher-dimensional
transformations, which is based on tensor
product structures (see [2]), implied by
separation-of-variables arguments.
The idea is roughly as follows. A tensor
product X1 ‚äó¬∑ ¬∑ ¬∑ ‚äóXn, n ‚àà‚Ñï, of com-
plex function spaces Xk, k ‚àà{1, ‚Ä¶ , n},
can be considered to be the completion
with respect to an appropriate metric of
the space of all linear combinations of
functions in product (separable) form
u1 ‚äó¬∑ ¬∑ ¬∑ ‚äóun given by
(x1, ‚Ä¶ , xn
) ÓÇ∂‚Üí
n
‚àè
k=0
uk
(xk
)
with uk ‚ààXk, k ‚àà{1, ‚Ä¶ , n}. If the spaces
Xk, k ‚àà{1, ‚Ä¶ , n}, are Hilbert spaces then
so is X1 ‚äó¬∑ ¬∑ ¬∑ ‚äóXn with the inner product
‚ü®u1 ‚äó¬∑ ¬∑ ¬∑ ‚äóun|v1 ‚äó¬∑ ¬∑ ¬∑ ‚äóvn‚ü©X1‚äó¬∑¬∑¬∑‚äóXn
of
two such functions u1 ‚äó¬∑ ¬∑ ¬∑ ‚äóun, v1 ‚äó
¬∑ ¬∑ ¬∑ ‚äóvn ‚ààX1 ‚äó¬∑ ¬∑ ¬∑ ‚äóXn of product form
given by
n
‚àè
k=1
‚ü®uk|vk
‚ü©
Xk ,
that is, the product of the inner products.
Densely
deÔ¨Åned
closed
linear
opera-
tors Ak ‚à∂D (Ak
) ‚äÜXk ‚ÜíYk, where Yk,
k ‚àà{1, ‚Ä¶ , n},
are
also
Hilbert
spaces
can now be combined to give an oper-
ator denoted by A1 ‚äó¬∑ ¬∑ ¬∑ ‚äóAn between
tensor product spaces X1 ‚äó¬∑ ¬∑ ¬∑ ‚äóXn and
Y1 ‚äó¬∑ ¬∑ ¬∑ ‚äóYn deÔ¨Åned (see [2]) as the
closed linear extension of the mapping
deÔ¨Åned on separable functions by
u1 ‚äó¬∑ ¬∑ ¬∑ ‚äóun ÓÇ∂‚ÜíA1u1 ‚äó¬∑ ¬∑ ¬∑ ‚äóAnun.
Applying this for example to n copies of
the Fourier‚ÄìLaplace transformation yields
the n-dimensional Fourier‚ÄìLaplace trans-
formation. Indeed, because
exp ((z1, ‚Ä¶ , zn
) (
w1, ‚Ä¶ , wn
))
‚âîexp
( n
‚àë
k=1
zkwk
)
= exp (z1w1
) ¬∑ ¬∑ ¬∑ exp (znwn
) ,
for
ùúà=
(
ùúà1, ‚Ä¶ , ùúàn
) ‚àà‚Ñùn
and
Hùúà,0 (‚Ñùn) ‚âî{f ||exp (‚àíùúàm) f ‚ààL2 (‚Ñùn)
} =
‚®Çn
k=1 Hùúàk,0 (‚Ñù),
we
have
that
the
n-dimensional
Fourier‚ÄìLaplace
trans-
formation, n ‚àà‚Ñï‚â•2, can be understood as
a repeated one-dimensional Fourier trans-
formation, that is, with m = (m1, ‚Ä¶ , mn
)
as the n-tuple of multiplication by the
respective argument
(Lùúàf )(p)
=
1
(2ùúã)n‚àï2 ‚à´
‚àû
‚àí‚àûexp(‚àíi x1 p1) ¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑ ‚à´
‚àû
‚àí‚àûexp(‚àíi xn pn) exp (‚àíùúàm) f (x) dxn
¬∑ ¬∑ ¬∑ dx2 dx1,
for well-behaved f ‚ààHùúà,0 (‚Ñùn); in other
terms
Lùúà= Lùúà1 ‚äó¬∑ ¬∑ ¬∑ ‚äóLùúàn.

522
15 Mathematical Transformations
As tensor products of unitary operators
are unitary, it follows that Lùúà‚à∂Hùúà,0 (‚Ñùn) ‚Üí
L2 (‚Ñùn) is a unitary transformation. As a
consequence, many of the above considera-
tions, such as the extension to distribution
spaces, can be carried over immediately to
the higher-dimensional case.
For example, the n-dimensional Dirac-ùõø-
distribution ùõø= ùõø{0} simply takes a single
sample at the origin in ‚Ñùn and similarly
the shifted version ùõø{x} samples at x ‚àà‚Ñùn.
The latter has now 1‚àï(2ùúã)n‚àï2 exp(‚àíi x ‚ãÖ)
as its Fourier‚ÄìPlancherel transform, that
is,
the
Fourier‚ÄìLaplace
transform
for
ùúà= (0, ‚Ä¶ , 0) ‚àà‚Ñùn (compare [2]). Also,
‚âî‚àë
x‚àà‚Ñ§n ùõø{
‚àö
2ùúãx}, which takes samples
at every point of ‚Ñùn with coordinates in
‚àö
2ùúã[‚Ñ§], still satisÔ¨Åes
F
=
.
(15.29)
Thus, in particular, corresponding variants
of the Shannon sampling theorem and
of the Fourier series expansion hold. The
higher-dimensional
Fourier‚ÄìPlancherel
transformation Ô¨Ånds its applications in
higher-dimensional, translation-invariant,
linear systems, [11]. A particularly promi-
nent application is in optics (see, e.g.,
[10, 17, 18].
In fact, the solution theory of gen-
eral linear partial diÔ¨Äerential equations
and systems with constant coeÔ¨Écients
can
be
based
on
higher-dimensional
Fourier‚ÄìLaplace transformation strategies,
see [14].
The
scaling
behavior
of
the
Fourier‚ÄìPlancherel transformation Ô¨Ånds
its generalization in the following. Let
(ùúéAf )
(x) ‚âî
‚àö
|det (A)|f (Ax) , x ‚àà‚Ñùn, for
a non-singular real (n √ó n)-matrix A. Then
the generalization of the one-dimensional
rescaling property, see footnote 8, is
ùúé(A‚àí1)‚àóF = FùúéA.
If A is orthogonal, that is, A‚àó= A‚àí1, then
ùúéA and F commute. This implies that rota-
tional symmetries are preserved by the
Fourier‚ÄìPlancherel transformation, a fact
that is frequently used in applications.
15.10
Some Other Important Transformations
In this Ô¨Ånal section, we consider a few other
mathematical transformations that have
found important application in physics or
engineering.
15.10.1
The Hadamard Transformation
The Hadamard transformation, also called
the
Hadamard‚ÄìRademacher‚ÄìWalsh,
the
Hadamard‚ÄìWalsh,
or
the
Walsh
transformation,
is
an
example
of
a
higher-dimensional
discrete
trans-
formation
deÔ¨Åned
for
functions
in
‚ÑÇ{0,1}n ‚âî{f |f ‚à∂{0, 1}n ‚Üí‚ÑÇ} as a map-
ping W ‚à∂‚ÑÇ{0,1}n ‚Üí‚ÑÇ{0,1}n given by12)
Wùúë(x) ‚âî
‚àë
k‚àà{0,1}n
ùúë(k) (‚àí1)‚ü®k|x‚ü©{0,1}n .
The Hadamard transformation is used in
the analysis of digital devices.
If we choose a particular enumeration
e ‚à∂{0, ‚Ä¶ 2n ‚àí1} ‚Üí{0, 1}n of {0, 1}n , that
is, e is bijective (one-to-one and onto),
then, using the standard inner products
for complex-valued mappings deÔ¨Åned on
Ô¨Ånite sets, we get that
E ‚à∂‚ÑÇ{0,1}n ‚Üí‚ÑÇ2n
ùúëÓÇ∂‚Üíùúë‚àòe
12) Compare higher-dimensional Fourier series
transformation (and the above discussion of the
discrete Fourier transformation at the end of
Section 15.2.2 for the one-dimensional case).

15.10 Some Other Important Transformations
523
is a unitary map. Thus, EWE‚àóyields
a
matrix
representation
in
terms
of
so-called
Hadamard
matrices13)
(
(‚àí1)‚ü®e(s)|e(t)‚ü©{0,1}n )
s,t‚àà{0,‚Ä¶,2n‚àí1} .
15.10.2
The Hankel Transformation
The (one-dimensional) Hankel transforma-
tion H0 is deÔ¨Åned, for suitable functions f
on ]0, ‚àû[, by
(H0f )(s) = ‚à´
‚àû
0
r f (r) J0(sr) dr,
where J0 denotes the Bessel function of the
Ô¨Årst kind x ÓÇ∂‚Üí‚àë‚àû
r=0(‚àí1)r (
x
2
)2r
‚àï(r!)2.
Let
ùúô‚à∂‚Ñù2 ‚Üí‚ÑÇ
be
circularly
sym-
metric, which means that the function
(r, ùúÉ) ÓÇ∂‚Üíùúô(r cos(ùúÉ), r sin(ùúÉ)), obtained by
a polar coordinate transformation of ùúô, is
independent of ùúÉ, that is,
ùúô(r cos(ùúÉ), r sin(ùúÉ)) = f (r) ‚âîùúô(r, 0)
for every ùúÉ‚àà‚Ñù. The Hankel transform
of f can be found by using polar coordi-
nates to evaluate ÃÇùúô, the two-dimensional
Fourier‚ÄìPlancherel transform of ùúô:
ÃÇùúô(s, 0) = ‚à´
‚àû
0
r f (r) J0(sr) dr.
Now,
due
to
the
circular-
symmetry-preserving
property
of
the
Fourier‚ÄìPlancherel
transformation,
we
have
ÃÇùúô(x) = ÃÇùúô(s, 0) for all x ‚àà‚Ñù2 with
|x| = |s| . Thus, the Hankel transformation
H0
is
essentially
the
two-dimensional
Fourier‚ÄìPlancherel
transformation
restricted
to
the
circularly
symmetric
function.
To Ô¨Ånd the precise spaces between
which H0 is unitary, we need to be more
13) Hadamard matrices are orthogonal matrices,
that is, real unitary matrices, with entries ¬±1.
detailed. As circularly symmetric func-
tions are uniquely determined by their
values on ‚Ñù>0 √ó {0} , we Ô¨Ånd, using polar
coordinates,
‚ü®ùúô|ùúì‚ü©L2(‚Ñù2)
= ‚à´‚Ñù2 ùúô(x) ùúì(x) dx
= 2ùúã‚à´‚Ñù>0
ùúô(r, 0) ùúì(r, 0) r dr
=
‚ü®‚àö
2ùúãmùúô(‚ãÖ, 0) |
‚àö
2ùúãmùúì(‚ãÖ, 0)
‚ü©
L2(‚Ñù>0).
Thus, ùúôÓÇ∂‚Üíùúô( ‚ãÖ, 0) is a unitary mapping s0
from
{ùúô‚ààL2 (‚Ñù2) | ùúéUùúô= ùúôfor all rotations U}
to the weighted L2-type space
1
‚àö
2ùúãm
[L2(‚Ñù>0
)]‚âî
{
f |
‚àö
2ùúãmf ‚ààL2(‚Ñù>0
)}
.
Consequently, the Hankel transformation
H0 ‚âîs0Fs‚àó
0 is unitary in the Hilbert space
1‚àï
‚àö
2ùúãm [L2 (‚Ñù>0
)].
The signiÔ¨Åcance of the Hankel trans-
formation lies in particular in the fact
that it is a spectral representation for
‚àö
‚àím‚àí1ùúïmùúï, where ‚àím‚àí1ùúïmùúïis a self-
adjoint realization of the Bessel operator
in
1‚àï
‚àö
2ùúãm [L2 (‚Ñù>0
)]
with
Neumann
boundary condition at 0.
15.10.3
The Radon Transformation
Finally, we consider the Radon transforma-
tion, which is used, for instance, in the Ô¨Åeld
of tomography, cf. [19]. This transformation
also yields a spectral representation of a dif-
ferential operator. The Radon transform of
f ‚à∂‚Ñù2 ‚Üí‚ÑÇis formally given by
Rf (ùõº, ùúé) =
1
‚àö
2ùúã‚à´{x‚àà‚Ñù2|x‚ãÖy(ùõº)=ùúé}
f ds, (15.30)

524
15 Mathematical Transformations
where ùõº‚àà] ‚àíùúã, ùúã], ùúé‚àà‚Ñùand y(ùõº) ‚ààS1,
the unit sphere in ‚Ñù2, given by y(ùõº) ‚âî
( cos ùõº
sin ùõº
)
. To analyze this, we employ a
parameterization of {x ‚àà‚Ñù2|x ‚ãÖy(ùõº) = ùúé}
as the line given by
( x1
x2
)
=
( cos ùõº
‚àísin ùõº
sin ùõº
cos ùõº
) ( ùúé
t
)
for t ‚àà‚Ñù. Substituting this into the integral
expression of R yields
Rf (ùõº, ùúé)
= ‚à´‚Ñù
f
(( cos ùõº
‚àísin ùõº
sin ùõº
cos ùõº
) ( ùúé
t
))
dt.
Using the equality
(
(1 ‚äóF) Rf )
(ùõº, r) = (Ff ) (
r
( cos ùõº
sin ùõº
))
for each ùõº‚àà] ‚àíùúã, ùúã], r ‚àà‚Ñù, we compute
|f |2
L2(‚Ñù2)
= |Ff |2
L2(‚Ñù2)
= ‚à´‚Ñù2 |(Ff ) (x)|2 dx
= ‚à´‚Ñù>0 ‚à´]‚àíùúã,ùúã]
|||||
(Ff )
(
r
( cos ùõº
sin ùõº
))|||||
2
dùõºr dr
= 1
2 ‚à´‚Ñù‚à´]‚àíùúã,ùúã]
|||||
(Ff )
(
s
( cos ùõº
sin ùõº
))|||||
2
dùõº|s| ds
= 1
2 ‚à´‚Ñù‚à´]‚àíùúã,ùúã]
|||
(
(1 ‚äóF) Rf )
(ùõº, s)|||
2
dùõº|s| ds
= 1
2 ‚à´‚Ñù‚à´]‚àíùúã,ùúã]
|||||
(
(1‚äóF)
‚àö
||ùúï2||Rf
)
(ùõº, s)
|||||
2
dùõºds
= 1
2 ‚à´‚Ñù‚à´]‚àíùúã,ùúã]
|||||
(‚àö
||ùúï2||Rf
)
(ùõº, s)
|||||
2
dùõºds
= 1
2
||||
‚àö
||ùúï2||Rf ||||
2
L2(]‚àíùúã,ùúã]√ó‚Ñù)
.
Thus, R deÔ¨Åned by the integral expression
(15.30) can be extended by continuity to all
of L2 (‚Ñù2) and then
1
‚àö
2
R ‚à∂L2 (‚Ñù2) ‚ÜíH1‚àï2
(||ùúï2|| , ] ‚àíùúã, ùúã] √ó ‚Ñù)
f ÓÇ∂‚Üí
1
‚àö
2
Rf
is unitary. Here H1‚àï2
(||ùúï2|| , ] ‚àíùúã, ùúã] √ó ‚Ñù) is
the completion14) of D (ùúï2
) with respect to
the norm induced by the inner product
(
f , g
)
ÓÇ∂‚Üí
‚ü®
||ùúï2||
1‚àï2 f || ||ùúï2||
1‚àï2 g
‚ü©
L2(]‚àíùúã,ùúã]√ó‚Ñù) .
We
now
deÔ¨Åne
a
mapping
R‚ãÑ‚à∂
H‚àí1‚àï2(|ùúï2|, ] ‚àíùúã, ùúã] √ó ‚Ñù) ‚ÜíL2(‚Ñù2) by the
duality relation
‚ü®R‚ãÑg|f ‚ü©L2(‚Ñù2) = ‚ü®g|Rf ‚ü©L2(]‚àíùúã,ùúã]√ó‚Ñù) ,
where the inner product on the right-hand
side is utilized as a so-called duality pair-
ing, which is the continuous extension
of ‚ü®‚ãÖ| ‚ãÖ‚ü©L2(]‚àíùúã,ùúã]√ó‚Ñù) to H‚àí1‚àï2(|ùúï2|, ] ‚àíùúã, ùúã]
√ó ‚Ñù) √ó H1‚àï2 (|ùúï2|, ] ‚àíùúã, ùúã] √ó ‚Ñù).
Inverting our initial parameterization, we
see that for well-behaved g, we have
‚à´‚Ñù‚à´‚Ñù
g(ùõº, ùúé) f
((cos ùõº‚àísin ùõº
sin ùõº
cos ùõº
) (ùúé
t
))
dùúédt
= ‚à´‚Ñù‚à´‚Ñù
g (ùõº, x1 cos ùõº+ x2 sin ùõº)
f (x1, x2
) dx1 dx2
and so, at least formally, we see that
(R‚ãÑg) (x1, x2
)
= ‚à´]‚àíùúã,ùúã]
g
(
ùõº, x1 cos ùõº+ x2 sin ùõº
)
dùõº.
The dual operator R‚ãÑis again the continu-
ous extension of the mapping deÔ¨Åned for
14) That is the construction ‚Äìalready used sev-
eral times ‚Äìto obtain the smallest Hilbert space
with the prescribed inner product containing
D (ùúï2
) .

References
525
well-behaved functions by the latter inte-
gral expression. Then
1
2R‚ãÑ||ùúï2|| Rf = f
and so R‚àí1 = 1‚àï
‚àö
2
(
1
‚àö
2R
)‚àí1
=
1
‚àö
2( 1
‚àö
2R)‚àó
= 1
2R‚àóis given by
1
2R‚ãÑ|ùúï2| ‚à∂H1‚àï2(|ùúï2|, ] ‚àíùúã, ùúã] √ó ‚Ñù)‚ÜíL2(‚Ñù2).
(15.31)
Using the Hilbert transformation H, we
Ô¨Ånd
||ùúï2|| = ||iùúï2||
= iùúï2 sgn (iùúï2
)
= ùúï2 Óà¥2 ,
where Óà¥2 denotes the continuous extension
of 1 ‚äóH to H1‚àï2
(||ùúï2|| , ] ‚àíùúã, ùúã] √ó ‚Ñù) , the
inverse Radon transform may be rewritten
as
R‚àí1 = 1
2R‚ãÑùúï2 Óà¥2.
Note here that Óà¥2 commutes with ùúï2
because it is a function of ùúï2 deÔ¨Åned via the
operator function calculus.
References
1. Bay, M. and Hasbro (2007) Transformers.
Science Ô¨Åction action Ô¨Ålm, Paramount
Pictures, DreamWorks SKG.
2. Functional Analysis. This handbook,
Chapter 13.
3. Ronald, N.B. (1983) The Fourier Transform
and Its Applications, 2nd edn, 3rd printing,
International Student Edition. Auckland etc.:
McGraw-Hill International Book Company.
4. Weisstein, N. (1980) The joy of Fourier
analysis, in Visual Coding and Adaptibility
(ed. C.S. Harris), Erlbaum, Hillsdale, NJ.
5. Akhiezer, N.I. and Glazman, I.M. (1981) in
Theory of Linear Operators in Hilbert Space,
Monographs and Studies in Mathematics, 9,
10, Vols I, II (eds Transl. from the 3rd
Russian ed. by E.R. Dawson and W.N.
Everitt), Publishers in association with
Scottish Academic Press, Edinburgh, Boston,
MA, Pitman Advanced Publishing Program,
London, Melbourne.
6. Zemanian, A.H. (1987) Generalized Integral
Transformations (Unabridged republ. of the
1968 orig. publ. by Interscience, New York),
vol. XVI, Dover Publications, Inc., New
York, 300 p.
7.
Beckenbach, E.F. and Hestenes, M.R. (eds)
(1962) Modern Mathematics for the
Engineer, Second Series (University of
California, Engineering Extension Series),
McGraw-Hill Book Company Inc., New
York, Toronto, London.
8. Akansu, A.N. and Agirman-Tosun, H. (2010)
Generalized Discrete Fourier Transform
With Nonlinear Phase. IEEE Trans. Signal
Process., 58(9), 4547‚Äì4556.
9. Brigham, E.O. (1988) The Fast Fourier
Transform and Applications, Prentice Hall,
Englewood CliÔ¨Äs, NJ.
10. Gaskill, J.D. (1978) Linear Systems, Fourier
Transformations and Optics, John Wiley &
Sons, Inc., New York, Chichester, Brisbane,
Toronto.
11. Norman, F.M. (1981) Lectures on linear
systems theory. J. Math. Psychol., 23, 1‚Äì89.
12. McBride, A.C. (1979) Fractional Calculus
and Integral Transforms of Generalized
Functions, Pitman Research Notes in
Mathematics, vol. 31, Pitman Advanced
Publishing Program, San Francisco, CA,
London, Melbourne, ISBN: 0-273-08415-1.
13. Ozaktas, H.M., Zalevsky, Z., and Alper, M.
(2001) The Fractional Fourier Transform with
Applications in Optics and Signal Processing,
Series in Pure and Applied Optics, John
Wiley & Sons, Inc, ISBN: 0-471-96346-1.
14. Partial DiÔ¨Äerential Equations. This
handbook, Chapter 16.
15. Bracewell, R.N. (1986) The Hartley
Transform, Oxford Engineering Science
Series, vol. 19, Oxford University Press, New
York, Clarendon Press, Oxford.
16. Picard, R. (1989) Hilbert Space Approach to
Some Classical Transforms, John Wiley &
Sons, Inc., New York.

526
15 Mathematical Transformations
17. Goodman, J.W. (1968) Introduction to
Fourier Optics, McGraw-Hill, New York.
18. Taylor, C.A. (1978) Images: A UniÔ¨Åed View of
DiÔ¨Äraction and Image Formation with All
Kinds of Radiation, Wykeham Publ.,
London, Basingstoke.
19. Ramm, A.G. and Katsevich, A.I. (1996) The
Radon Transform and Local Tomography,
CRC Press Inc.

527
16
Partial DiÔ¨Äerential Equations
Des McGhee, Rainer Picard, Sascha TrostorÔ¨Ä, and Marcus Waurick
16.1
What are Partial DiÔ¨Äerential Equations?
Partial diÔ¨Äerential equations (PDEs) pro-
vide the essential mathematical models of
physical phenomena that vary in time and
space. They are equations involving one
or more unknown function(s) of (more
than one) independent variables and their
partial derivatives. In standard applica-
tions, we would often expect one variable,
t, for time and three independent space
variables, Cartesian coordinates x, y, z or
polar coordinates r, ùúÉ, ùúôor some other
standard
three-dimensional
coordinate
system. Of course, sometimes steady-state
models are considered so there is no
dependence on t and/or symmetries allow
lower-dimensional models to be studied
so that only one or two space variables are
required. On the other hand, physics often
involves mathematical models that intro-
duce higher-dimensional ‚Äúspace‚Äù (inverted
commas because this may not be physical
space)
requiring
independent
variables
x1, x2, x3, ‚Ä¶ , xn, n > 3,
so
that,
along
with time, t, there are n + 1 independent
variables.
A core objective is to determine the
unknown functions that satisfy the PDE in
order to predict the behavior of the phys-
ical system being modeled. The physical
phenomenon being modeled will nor-
mally determine an appropriate domain
in which the PDE is to be solved, that is,
a ‚Äúspace‚Äù-domain D ‚äÜ‚Ñùn and, if there is
time dependence, a time interval I ‚äÜ‚Ñùin
which the PDE is to be satisÔ¨Åed. In addition
to the PDE itself, there will often be some
boundary conditions that the unknown
functions have to satisfy on the boundary
ùúïD of D and, if time is involved, some
initial conditions to be satisÔ¨Åed by the
unknown functions at some initial time
t = t0. Often, an analytic solution of the
PDE may not be available but qualitative
information derived from a careful analysis
of the system, for example, existence and
uniqueness of the solution, may allow
numerical methods to be developed and
applied to approximate the solution to an
acceptable degree of accuracy.
As elementary notions of diÔ¨Äerentiation
require functions to be continuous, one
might initially think that the importance of
PDEs is restricted to phenomena that can
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

528
16 Partial DiÔ¨Äerential Equations
be modeled as continuous (indeed smooth)
processes. However, PDEs also play a fun-
damental role in modeling discrete physi-
cal phenomena, most notably in quantum
mechanics via the Schr√∂dinger equation.
In addition, mathematical generalization
allows the precise deÔ¨Ånition of derivatives
of discontinuous functions, and indeed of
generalized functions such as the Dirac-
delta, so that PDEs are often appropri-
ate mathematical models even when the
unknown functions to be found are not
expected to be smooth or even continuous.
In full generality, that is, where the
PDE involves an arbitrary function of the
unknowns and their derivatives, there is
little that can be said about properties of
solutions or methods of solution. However
special cases, particularly those that are
of central interest in applications, have
been dealt with extensively. All the core
physical phenomena, such as sound, heat,
electromagnetism, elasticity, Ô¨Çuid Ô¨Çow,
and so on are modeled by PDEs and these
models have been well studied. In this
chapter, therefore, we shall not dwell on the
standard theory and methods of solution
of the classical models of mathematical
physics referring the reader to standard
texts at elementary level (e.g., [1‚Äì4]) or
advanced level (e.g., [5‚Äì8]). Rather, we
shall concentrate on structural matters
concerning PDEs that are necessary for
reasonable models of physical phenom-
ena and precise notions of solvability. We
shall however illustrate the theory using
well-known (systems of) equations from
mathematical physics.
Historically, linear models of physical
phenomena have been developed and
extensively studied. Progressively, more
and more nonlinear eÔ¨Äects are being taken
into account but, because the analysis
of nonlinear PDEs is based on a deep
understanding of the linear case, the latter
will be the focus of our discussion.
A linear PDE is of the form
‚àëm0
ùõº0=0 ¬∑ ¬∑ ¬∑ ‚àëmn
ùõºn=0 a(ùõº0,‚Ä¶,ùõºn) (t, x)
√ó (ùúï
ùõº0
0 ¬∑ ¬∑ ¬∑ ùúï
ùõºn
n u)
(t, x) = f (t, x) ,
where
(t, x)
ranges
in
a
subset1)
I √ó D ‚äÜ‚Ñù1+n,
that
is,
t ‚ààI ‚äÜ‚Ñù, x =
(x1, ‚Ä¶ , xn
) ‚ààD ‚äÜ‚Ñùn. In many physical
applications we have, as discussed above,
n = 3 and t plays the role of time. The
symbol ùúïk denotes the partial derivative
with respect to the variable at position
k ‚àà{0, 1, ‚Ä¶ , n} in the list (t, x1, ‚Ä¶ , xn
).
The coeÔ¨Écient functions a(ùõº0,‚Ä¶,ùõºn) and the
‚Äúsource term‚Äù f are commonly assumed
to be given and the main task is to Ô¨Ånd
solutions
u that are usually real- or
complex-valued
functions
deÔ¨Åned
on
I √ó D. To emphasize that we are looking
for functions as solutions and not really
for values at a speciÔ¨Åc point, we usu-
ally drop the reference to the variables
(t, x) =
(
t, x1, ‚Ä¶, xn
).
Also,
introduc-
ing the standard multi-index notation
(ùúï0, ‚Ä¶, ùúïn
)(ùõº0,‚Ä¶,ùõºn) for ùúïùõº0
0 ¬∑ ¬∑ ¬∑ ùúïùõºn
n , abbrevi-
ating ùúï‚à∂= (ùúï0, ‚Ä¶, ùúïn
) , ùõº‚à∂= (ùõº0, ‚Ä¶, ùõºn
) ,
m ‚à∂=
(
m0, ‚Ä¶, mn
),
and
writing
0
for
(0, ‚Ä¶ , 0), we get the more compact form
P(ùúï)u ‚à∂=
m
‚àë
ùõº=0
aùõºùúïùõºu = f .
(16.1)
We could also drop the summation range
altogether, because we could implicitly
think of the coeÔ¨Écients aùõºas being zero if
they are not in the prescribed summation
range.
1) Here, we use the superscript 1 + n to draw
attention to the distinction between the time
variable and the space variable(s). We revert
later to the superscript n + 1 when we have
variables (x0, x1, ‚Ä¶ , xn), that is, there is no time
dependence or a suitable variable to regard
as ‚Äútime-like‚Äù has not yet been identiÔ¨Åed (see
Section 16.2.1).

16.2 Partial DiÔ¨Äerential Equations in ‚Ñùn+1, n ‚àà‚Ñï, with Constant CoeÔ¨Écients
529
We may think of the coeÔ¨Écients aùõºnot
just as scalar valued but possibly matrix val-
ued, all of the same matrix size s √ó r, with
u = (u1, u2, ‚Ä¶ , ur) and f = (f1, f2, ‚Ä¶ , fs). In
this way, (16.1) describes a system of PDEs
as well as a single PDE in one uniÔ¨Åed
form. Whichever the case, we shall sim-
ply refer to (16.1) as a PDE. A single PDE
is a special matrix-valued case with 1 √ó 1-
matrices as coeÔ¨Écients and generally lin-
ear algebra teaches us that we should not
expect a unique solution to a linear sys-
tem of equation if the coeÔ¨Écients are not
square matrices and so we shall assume this
throughout.
When can such a problem be consid-
ered as a reasonable model of a physi-
cal phenomenon? In general, the answer
to this question is provided by the three
Hadamard conditions:
1. Existence. There should be at least one
solution for every right-hand side.
2. Uniqueness. There should be at most
one solution.
3. Robustness. The solution should
depend continuously on the given data.
The Ô¨Årst condition may imply a con-
straint on the admissible data, that is, the
prescription of a suitable function space
that f must belong to. Satisfying the sec-
ond condition may require constraints to
the admissible set of solutions, that is, the
deÔ¨Ånition of a suitable function space in
which to seek solutions u. The third con-
dition guarantees that small errors in data,
which will always be present in any physi-
cal measurement, result in only small errors
in solutions; without this requirement, the
solution of the PDE may provide no mean-
ingful insight to reality.
Satisfying all three conditions commonly
results in generalizations of solution con-
cepts that takes us beyond the classical
understanding of diÔ¨Äerentiation because
the function spaces required to ensure
the Hadamard requirements are spaces
of generalized functions (distributions).
Problems satisfying Hadamard‚Äôs require-
ments are called well-posed, those which
do not are called ill-posed. However, for
mathematical models of physical phenom-
ena that are in the Ô¨Årst instance ill-posed
problems, there are often physically justi-
Ô¨Åable modiÔ¨Åcations that can be made to
obtain a problem that is well-posed.
In general, establishing well-posedness
for a PDE of the form (16.1) is not an easy
task. We therefore start with the restric-
tive assumption that the coeÔ¨Écients aùõºare
constant.
16.2
Partial DiÔ¨Äerential Equations in ‚Ñùn+1,
n ‚àà‚Ñï, with Constant CoeÔ¨Écients
Given a PDE with constant coeÔ¨Écients, the
issue of well-posedness can be reduced to
algebraic characterizations via the applica-
tion of an integral transformation. For ùúà‚àà
‚Ñùn+1, the unitary mapping 2)
Lùúà‚à∂Hùúà,0
(‚Ñùn+1) ‚ÜíL2 (‚Ñùn+1)
generated by the integral expression
(Lùúàùúë)
(x) = ÃÇùúë(x ‚àíiùúà)
=
1
(2ùúã)(n+1)‚àï2 ‚à´‚Ñùn+1 exp (‚àí(ix + ùúà) ‚ãÖy) ùúë(y) dy
is known as the Fourier‚ÄìLaplace transfor-
mation, see [9].
2) Recall from [9] that Hùúà,0
(‚Ñùn+1) is the space of
(equivalence classes of) functions f such that
x ÓÇ∂‚Üíexp (‚àíùúà‚ãÖx) f (x) is square integrable. The
inner product is
(f , g) ÓÇ∂‚Üí‚à´‚Ñùn+1 f (x) g (x) exp (‚àí2ùúà‚ãÖx) dx.

530
16 Partial DiÔ¨Äerential Equations
A Cartesian product X0 √ó ¬∑ ¬∑ ¬∑ √ó XN of
Hilbert
spaces,
equipped
with
the
inner
product
Xk,
k ‚àà{0, ‚Ä¶ , N},
‚ü®v|w‚ü©‚®Å
k‚àà{0,‚Ä¶N} Xk ‚à∂= ‚àë
k‚àà{0,‚Ä¶,N}
‚ü®vk | wk
‚ü©
Xk
for all v = (v0, ‚Ä¶ , vN
), w = (w0, ‚Ä¶ , vN
)
‚ààX0 √ó ¬∑ ¬∑ ¬∑ √ó XN
is
a
Hilbert
space,
called
the
(Ô¨Ånite)
direct
sum
of
(Xk
)
k‚àà{0,‚Ä¶,N}
and
is
denoted
by
‚®Å
k‚àà{0,‚Ä¶,N} Xk
or
X0 ‚äï¬∑ ¬∑ ¬∑ ‚äïXN,
see
[10]. The elements (x0, ‚Ä¶ , xN) of such
a direct sum will also be denoted by
x0 ‚äï¬∑ ¬∑ ¬∑ ‚äïxN or ‚Äì more suggestively ‚Äì by
a column matrix
‚éõ
‚éú
‚éú‚éù
x0
‚ãÆ
xN
‚éû
‚éü
‚éü‚é†
.
Similarly,
a
(K + 1) √ó (L + 1)-matrix
of
elements in Hilbert spaces can be regarded
as a (K + 1) ‚ãÖ(L + 1)-dimensional vector
and hence an element in the direct sum
of the appropriate (K + 1) ‚ãÖ(L + 1) Hilbert
spaces.
Applying this construction, we see that
we can easily extend the Fourier‚ÄìLaplace
transformation to vectors or indeed matri-
ces with components in Hùúà,0
(‚Ñùn+1) by
componentwise
application.
Moreover,
this matrix convention makes the action of
a diÔ¨Äerential operator P (ùúï), where P is a
polynomial with matrix coeÔ¨Écients, rather
intuitive as a form of matrix multiplication.
Indeed, for a polynomial matrix P, that is, a
matrix with scalar polynomials as entries,
LùúàP (ùúï) = P (im + ùúà) Lùúà.
(16.2)
Here m symbolizes ‚Äúmultiplication-by-
the-argument‚Äù
and
P (im + ùúà)
denotes
a
multiplication
operator
deÔ¨Åned
by
P (im + ùúà)
(g)
(x) = P (ix + ùúà) g (x) ,
[10].
The
multiplication
on
the
right-hand
side is, for g in an appropriate direct
sum space, to be understood as an ordi-
nary matrix product. Indeed, if P (ùúï)
has
(J + 1) √ó (K + 1)
‚Äì
matrix
coeÔ¨É-
cients,
then
it
is
clear
that
g
must
be either a (K + 1)
‚Äì
column vector
with
entries
in
Hùúà,0
(‚Ñùn+1),
that
is,
g ‚àà‚®Å
k‚àà{0,‚Ä¶,K} Hùúà,0
(‚Ñùn+1),
or
possibly
a (K + 1) √ó (L + 1) ‚Äì matrix with entries
in
Hùúà,0
(‚Ñùn+1),
that
is,
g ‚àà‚®Å
l‚àà{0,‚Ä¶,L}
‚®Å
k‚àà{0,‚Ä¶,K} Hùúà,0
(‚Ñùn+1). We shall, however,
rarely note matrix sizes and simply write
g ‚ààHùúà,0
(‚Ñùn+1), because the matrix sizes
should be clear from the context.
Then
the
application
of
the
Fourier‚ÄìLaplace transformation to the
PDE (16.1) and using (16.2) results in
the algebraic problem
P (im + ùúà) Lùúàu = Lùúàf
for the unknown Lùúàu. If this can be
solved, then the solution of the PDE
can be obtained by applying the inverse
transformation.
16.2.1
Evolutionarity
Before focusing on the issue of construct-
ing solutions, we brieÔ¨Çy discuss how to
detect a direction of a possible time variable
in a diÔ¨Äerential expression P(ùúï). This can
be a problem because changes of variables
made, for example, to simplify the appear-
ance of a PDE may result in the direction of
evolution being somewhat hidden.
A fundamental (and from our point
of view characterizing) property of a
mathematical model
P(ùúï)u = f
of a physical phenomenon is that it should
be causal, that is the eÔ¨Äect of data f on
the solution u should not precede (in time)

16.2 Partial DiÔ¨Äerential Equations in ‚Ñùn+1, n ‚àà‚Ñï, with Constant CoeÔ¨Écients
531
the application of the data. Therefore, if the
polynomial det (P (z)) has no zeros in a half-
space of ‚ÑÇn+1 of the form
i [‚Ñùn+1] + [‚Ñù‚â•ùúö0
] ùúà0
for some ùúö0 ‚àà‚Ñù>0, ùúà0 ‚àà‚Ñùn+1, ||ùúà0|| = 1,
then we call the direction ùúà0 evolution-
ary or time-like because in the direction
of ùúà0 we have causality in the sense that
for all a ‚àà‚Ñù, if the data f vanishes on
the
half-space
[‚Ñù<a
] ùúà0 + {ùúà0
}‚ä•
with
interior unit normal ùúà0 ‚Äìthat is, the set
{x ‚àà‚Ñùn+1|x = sùúà0 + y, s < a, ùúà0 ‚ãÖy = 0} ‚Äì
then the solution P (ùúï)‚àí1 f also vanishes on
the same half-space. The partial diÔ¨Äerential
operator P (ùúï) is then called evolutionary
in direction ùúà0. If P (ùúï) has a direction in
which it is evolutionary then we brieÔ¨Çy say
P (ùúï) is evolutionary; otherwise we say P (ùúï)
is nonevolutionary. If P (ùúï) is evolutionary
in both direction ùúà0 and ‚àíùúà0 we say that
P (ùúï) is reversibly evolutionary (in direction
ùúà0). If P (ùúï) is only evolutionary in direction
ùúà0 but not in direction ‚àíùúà0, then we say
P (ùúï) is irreversibly evolutionary.
In ‚Ñù2, for a second-order scalar dif-
ferential
operator
p (ùúï) = p(ùúï1,ùúï2)
with
real coeÔ¨Écients, a more common name
for irreversibly evolutionary is parabolic,
reversibly
evolutionary
coincides
with
hyperbolic,
and
nonevolutionary
with
elliptic.
This
classical
terminology
is
based on the fact that the three situations
correspond to {x ‚àà‚Ñù2|p (x) = 0} being
respectively a parabola, a hyperbola, or an
ellipse. These classiÔ¨Åcations have various
generalizations to arbitrary dimensions
(leaving, however, many partial diÔ¨Äeren-
tial operators, such as the Schr√∂dinger
operator, unclassiÔ¨Åed).
Note that (1 ‚àíùúï2
1 ‚àíùúï2
2
) is nonevolution-
ary if considered in ‚Ñù2 (static), but it is
reversibly evolutionary in ‚Ñù1+2 in direction
e0 = (1, 0, 0) (quasi-static).
The direction of evolutionarity ùúà0 will
be used as the weight occurring in the
exponential weighted space Hùúà,0(‚Ñùn+1), the
space where we will seek for solutions, via
ùúà= ùúöùúà0 for ùúö> 0 suÔ¨Éciently large and the
Fourier‚ÄìLaplace transformation Lùúàwill be
used to transform the PDE problem into an
algebraic one using (16.2) as outlined above.
In a nonevolutionary case, we may choose
ùúà= 0 and apply the more familiar Fourier
transformation F = L0.
16.2.2
An Outline of Distribution Theory
As indicated above, we need to consider a
concept of generalized functions, so-called
distributions. A classical development of a
theory of distributions relies on the intro-
duction of the Schwartz space Óàø(‚Ñùn+1)
of rapidly decreasing smooth functions
f ‚à∂‚Ñùn+1 ‚Üí‚ÑÇdeÔ¨Åned by
Óàø(‚Ñùn+1) ‚à∂=
{
f ‚ààC‚àû(‚Ñùn+1) |
‚ãÄ
ùõº,ùõΩ‚àà‚Ñïn+1
sup
x‚àà‚Ñùn+1 |xùõºùúïùõΩf (x)| < ‚àû
}
endowed with the topology deÔ¨Åned by
fk ‚Üí0 in Óàø(‚Ñùn+1) as k ‚Üí‚àû‚áê‚áí
‚ãÄ
ùõº,ùõΩ‚àà‚Ñïn+1
sup
x‚àà‚Ñùn+1 |xùõºùúïùõΩfk(x)| ‚Üí0 as k ‚Üí‚àû.
A (tempered) distribution is then an ele-
ment of Óàø‚Ä≤(‚Ñùn+1), the space of continuous
linear functionals on Óàø(‚Ñùn+1). Examples
of tempered distributions are ùõø{ùúî}, the
Dirac-ùõø-functionals
of
evaluation
of
a
function f ‚ààÓàø(‚Ñùn+1) at a point ùúî‚àà‚Ñùn+1,
that is, ùõø{ùúî}f ‚à∂= f (ùúî). Other examples
are functionals ùõøÓàπof ‚Äúevaluation‚Äù on a
manifold Óàπ‚äÜ‚Ñùn+1. These are deÔ¨Åned by
ùõøÓàπf ‚à∂= ‚à´Óàπf . Frequently, it is convenient to

532
16 Partial DiÔ¨Äerential Equations
apply functionals Óàø‚Ä≤(‚Ñùn+1) to elements in a
dense subset of Óàø(‚Ñùn+1) such as C‚àû
0 (‚Ñùn+1),
the space of inÔ¨Ånitely diÔ¨Äerentiable func-
tions on ‚Ñùn+1 that have compact support,
that is, vanish outside of a bounded set.
It is remarkable to note that there is
a more intrinsic way of constructing the
space of tempered distributions than the
ad hoc deÔ¨Ånition from above, see, for
example, [Example 2.2] [11]. For simplicity,
we only treat the one-dimensional case.
For this, consider the position operator
m
of
multiplication-by-argument,
that
is, (mf )(x) ‚à∂= xf (x), and the momentum
operator ùúï, that is, ùúïf (x) = f ‚Ä≤(x) for suitable
f and x ‚àà‚Ñù, realized as operators in L2(‚Ñù).
Then, deÔ¨Åne the annihilation operator
Óà∞‚à∂= (1‚àï
‚àö
2)(m + ùúï)
and
the
creation
operator Óà∞‚àó=
1
‚àö
2(m ‚àíùúï). Realizing that
ùõæ‚à∂x ÓÇ∂‚Üíexp (‚àíx2‚àï2
)
is an eigenfunction of
the harmonic oscillator Óà∞‚àóÓà∞with eigen-
value 1 and using the density of the linear
hull of the Hermite functions {Œìk | k ‚àà‚Ñï},
where
Œìk ‚à∂= (1‚àï|Óà∞kùõæ|L2(‚Ñù))Óà∞kùõæ
for
all
k ‚àà‚Ñï, we can determine the action of
any power of Óà∞‚àóÓà∞by knowing only how
Óà∞‚àóÓà∞acts on {Œìk | k ‚àà‚Ñï}. In this way,
Óàø(‚Ñù) = ‚ãÇ
k‚àà‚ÑïD (
(Óà∞‚àóÓà∞)k).
A
sequence
(ùúôn)n‚àà‚Ñïin Óàø(‚Ñù) then, by deÔ¨Ånition, con-
verges to some ùúô‚ààÓàø(‚Ñù) if for all k ‚àà‚Ñï
we have (Óà∞‚àóÓà∞)k ùúôn ‚Üí(Óà∞‚àóÓà∞)k ùúôin L2(‚Ñù).
Endow D (
(Óà∞‚àóÓà∞)k) with the scalar product
(ùúô, ùúì) ÓÇ∂‚Üí‚ü®(Óà∞‚àóÓà∞)k ùúô| (Óà∞‚àóÓà∞)k ùúì‚ü©,
where
‚ü®‚ãÖ|‚ãÖ‚ü©denotes the usual L2-scalar product.
Then D (
(Óà∞‚àóÓà∞)k) becomes a Hilbert space.
The space of tempered distributions on ‚Ñù
can then be written as
Óàø‚Ä≤(‚Ñù) =
‚ãÉ
k‚àà‚Ñï
D
(
(Óà∞‚àóÓà∞)k)‚Ä≤
.
Thus, for a tempered distribution f ‚àà
Óàø‚Ä≤(‚Ñù), the following continuity estimate
holds:
‚ãÅ
k‚àà‚Ñï,C>0
‚ãÄ
ùúô‚ààÓàø(‚Ñù)
|‚ü®f |ùúô‚ü©|
= |f (ùúô)| ‚â§C| (Óà∞‚àóÓà∞)k ùúô|L2(‚Ñù).
Indeed, if we choose f = ùõø{0} given by
‚ü®ùõø{0}|ùúô‚ü©‚à∂= ùúô(0) for ùúô‚ààÓàø(‚Ñù) we estimate
|ùúô(0)| =
|||||||
0
‚à´
‚àí‚àû
(1 + t)ùúô‚Ä≤(t)
1
1 + t dt
|||||||
‚â§|(1 + m)ùúïùúô|L2(‚Ñù)
‚â§C|Óà∞‚àóÓà∞ùúô|L2(‚Ñù)
for some constant C > 0.
Observing
that
the
Schwartz
space
Óàø(‚Ñùn+1) is also an algebra, that is, for any
two functions ùúô, ùúì‚ààÓàø(‚Ñùn+1), the product
ùúô‚ãÖùúìalso lies in Óàø(‚Ñùn+1), we can deÔ¨Åne
the product of a tempered distribution
f ‚ààÓàø‚Ä≤(‚Ñùn+1) and a function ùúô‚ààÓàø(‚Ñùn+1)
by setting
‚ü®ùúô‚ãÖf |ùúì‚ü©‚à∂= ‚ü®f |ùúô‚ãÖùúì‚ü©
for every ùúì‚ààÓàø(‚Ñùn+1). The resulting func-
tional ùúô‚ãÖf is also a tempered distribution.
Moreover, because the convolution ùúô‚àóùúì
of two functions ùúô, ùúì‚ààÓàø(‚Ñùn+1), given by
(ùúô‚àóùúì) (x) ‚à∂= ‚à´
‚Ñùn+1
ùúô(x ‚àíy)ùúì(y) dy
is again an element of Óàø(‚Ñùn+1), we deÔ¨Åne
the convolution of a tempered distribution
f with a function ùúô‚ààÓàø(‚Ñùn+1) by
‚ü®f ‚àóùúô|ùúì‚ü©‚à∂= ‚ü®f |ùúé‚àí1ùúô‚àóùúì‚ü©
(16.3)
for all ùúì‚ààÓàø(‚Ñùn+1). Here, ùúé‚àí1ùúôis given by
(ùúé‚àí1ùúô)(x) = ùúô(‚àíx) for x ‚àà‚Ñùn+1. The result-
ing functional f ‚àóùúôis again a tempered
distribution.

16.2 Partial DiÔ¨Äerential Equations in ‚Ñùn+1, n ‚àà‚Ñï, with Constant CoeÔ¨Écients
533
One of the most important properties
of the Schwartz space Óàø(‚Ñùn+1) is that the
Fourier transformation F deÔ¨Ånes a bijection
on it. This fact allows to extend the Fourier
transformation to tempered distributions
by deÔ¨Åning
‚ü®Ff |ùúì‚ü©‚à∂= ‚ü®f |F‚àóùúì‚ü©
for all f ‚ààÓàø‚Ä≤(‚Ñùn+1), ùúì‚ààÓàø(‚Ñùn+1).
16.2.3
Integral Transformation Methods as a
Solution Tool
As illustrated above, the Fourier‚ÄìLaplace
transformation can be utilized to reformu-
late a linear PDE with constant coeÔ¨Écients
as an equation involving (matrix-valued)
polynomials only, which is easier to han-
dle. Moreover, it is sometimes possible to
write a solution for P(ùúï)u = f as a certain
superposition of so-called fundamental
solutions. In order to compute such fun-
damental solutions, one has to extend
the Fourier‚ÄìLaplace transformation to
distributions.
For ùúà‚àà‚Ñùn+1 the Fourier‚ÄìLaplace trans-
formation has a continuous extension to
distributions in the space
exp (ùúàm)
[Óàø‚Ä≤ (‚Ñùn+1)] ‚à∂=
{exp (ùúàm) f | f ‚ààÓàø‚Ä≤ (‚Ñùn+1)} ,
that is the space of distributions exp(ùúàm)f
given by
‚ü®exp(ùúàm)f |ùúì‚ü©‚à∂= ‚ü®f | exp(‚àíùúàm)ùúì‚ü©,
for all functions ùúìsuch that exp(‚àíùúàm)ùúì‚àà
Óàø(‚Ñùn+1). For such a distribution, the
Fourier‚ÄìLaplace transformation is given
by
‚ü®Lùúàexp(ùúàm)f |ùúô‚ü©‚à∂= ‚ü®exp(ùúàm)f |L‚àó
ùúàùúô‚ü©
= ‚ü®exp(ùúàm)f | exp(ùúàm)F‚àóùúô‚ü©
= ‚ü®f |F‚àóùúì‚ü©
= ‚ü®Ff |ùúô‚ü©
for ùúô‚ààÓàø(‚Ñùn+1), that is,
Lùúà‚à∂exp(ùúàm) [Óàø‚Ä≤(‚Ñùn+1)] ‚ÜíÓàø‚Ä≤(‚Ñùn+1)
exp(ùúàm)f ÓÇ∂‚ÜíFf .
Let us consider an example. Obviously,
ùõø{0} ‚ààexp (ùúàm)
[Óàø‚Ä≤ (‚Ñùn+1)]
for
every
ùúà‚àà‚Ñùn+1
with
ùõø{0} = exp(ùúàm)ùõø{0},
the
right-hand side ùõø{0} being interpreted in
Óàø‚Ä≤(‚Ñùn+1). Thus, we obtain Lùúàùõø{0} = Fùõø{0}
and
‚ü®Fùõø{0}|ùúô‚ü©= ‚ü®ùõø{0}|F‚àóùúô‚ü©
=
1
(2ùúã)(n+1)‚àï2 ‚à´
‚Ñùn+1
ùúô(x) dx
for each ùúô‚ààÓàø(‚Ñùn+1), showing
Lùúàùõø{0} = Fùõø{0} =
1
(2ùúã)(n+1)‚àï2 .
(16.4)
With this we can now address the possi-
bility of representing the solution u of the
problem
P(ùúï)u = f ,
for given f and a suitable (matrix-valued)
polynomial P as a convolution.
A Green‚Äôs tensor G is a solution of the
equation
P (ùúï) G = ùõø{0},
(16.5)
where the right-hand side is the diagonal
matrix having ùõø{0} as diagonal entries. It
is a rather remarkable fact that if det (P)
is not the zero polynomial, then such
a Green‚Äôs tensor always exists (even if

534
16 Partial DiÔ¨Äerential Equations
x ÓÇ∂‚Üídet (P (ix + ùúà))
has
zeros
and
so
x ÓÇ∂‚ÜíP (ix + ùúà)‚àí1 is not everywhere deÔ¨Åned)
but it may not be uniquely determined
(division problem).
Applying the Fourier‚ÄìLaplace transfor-
mation Lùúàto (16.5) and using (16.4), we
arrive at
P (im + ùúà) LùúàG =
1
(2ùúã)(n+1)‚àï2 .
Note
that
because
ùõø{0}
and
1
(2ùúã)(n+1)‚àï2 ‚à∂= (ùúôÓÇ∂‚Üí‚à´‚Ñùn+1
1
(2ùúã)(n+1)‚àï2 ùúô)
is
a
tempered distribution, so is ‚Äì entry by
entry ‚Äì G.
As the transformed equation involves
only polynomial multiplication, solving
a (system of) PDE(s) reduces to a linear
algebra issue of inverting matrices.
Now, the Green‚Äôs tensor can be utilized
to compute solutions u for the (inhomoge-
neous) equation
P(ùúï)u = f .
Indeed, if LùúàG can be used to deÔ¨Åne a mul-
tiplication operator (LùúàG
)
(m) on f , which
may require severe constraints on f , then
f = L‚àó
ùúà
1
(2ùúã)(n+1)‚àï2 (2ùúã)(n+1)‚àï2 Lùúàf
= (2ùúã)(n+1)‚àï2 L‚àó
ùúàP (im + ùúà)
(LùúàG) (m)Lùúàf
= P (ùúï) (2ùúã)(n+1)‚àï2 L‚àó
ùúà
(LùúàG) (m)Lùúàf .
One usually writes
G ‚àóf ‚à∂= (2ùúã)(n+1)‚àï2 L‚àó
ùúà
(LùúàG
)
(m)Lùúàf
and speaks of a convolution because, for
certain ‚Äúgood‚Äù right-hand sides f , G ‚àóf
can be written as a convolution-type inte-
gral. This deÔ¨Ånition of convolution coin-
cides with the deÔ¨Ånition given in (16.3).
Thus we get the existence of a solution in
the form
u = G ‚àóf .
Typical
examples
of
such
Green‚Äôs
tensors are the fundamental solutions
of the Poisson equation Œîu = f in ‚Ñù3 and
the wave equation (ùúï2
0 ‚àíŒî)u = f in ‚Ñù1+3.
The Green‚Äôs tensor for the former is
‚Ñù3 ‚ßµ{0} ‚àãx ÓÇ∂‚Üí‚àí
1
4ùúã|x|,
and for the latter
‚Ñù‚ßµ{0} ‚àãt ÓÇ∂‚Üí
1
4ùúã
‚àö
2
1
t ùõøC(t, ‚ãÖ),
where ùõøC
is the distribution of inte-
gration over the ‚Äúforward light cone‚Äù
C = {(t, x) ‚àà‚Ñù1+3|t > 0, |x| = t}. Thus, a
solution for Œîu = f is
u(x) = ‚àí‚à´‚Ñù3
1
4ùúã|x ‚àíy|f (y)dy
(x ‚àà‚Ñù3)
and for (ùúï2
0 ‚àíŒî)u = f in ‚Ñù1+3 is
u(t, x) = ‚à´‚Ñù3
1
4ùúã|y|f (t ‚àí|y|, x ‚àíy)dy
assuming well-behaved data f .
There is a convenient mechanism for
obtaining a Green‚Äôs tensor for systems
if the Green‚Äôs tensor for an associated
scalar problem is known. Consider an
(N + 1) √ó (N + 1) matrix operator P (ùúï) .
By the Caley‚ÄìHamilton theorem, we know
that P (ùúï) satisÔ¨Åes its minimal polynomial.
In other words, (treating ùúïas if it were
an array of numbers) there is a (scalar)
polynomial
qP (ùúÜ) ‚à∂= ‚àëd
k=0 ck (ùúï) ùúÜk
(of
smallest 3) degree) with scalar polynomials
ck such that
qP (P (ùúï)) =
d
‚àë
k=0
ck (ùúï) P (ùúï)k = 0.
3) If minimality of degree is not wanted, qP (ùúÜ) can
be replaced by the characteristic polynomial
qP (ùúÜ) ‚à∂= det (P (ùúï) ‚àíùúÜ) .

16.2 Partial DiÔ¨Äerential Equations in ‚Ñùn+1, n ‚àà‚Ñï, with Constant CoeÔ¨Écients
535
If we have a Green‚Äôs tensor (fundamental
solution) g for c0 (ùúï) then, identifying g with
the (N + 1) √ó (N + 1) diagonal matrix with
g as diagonal entries, we obtain
ùõø{0} = c0 (ùúï) g = ‚àí
d
‚àë
k=1
ck (ùúï) P (ùúï)k g
= P (ùúï)
(
‚àí
d‚àí1
‚àë
s=0
cs+1 (ùúï) P (ùúï)s g
)
.
This
calculation
shows
that
G = ‚àí‚àëd‚àí1
s=0 cs+1 (ùúï) P (ùúï)s g
is
a
Green‚Äôs
tensor for P (ùúï) .
As
an
example,
we
consider
an
extended 4) system based on the equations
of magnetostatics:
P (ùúï) ‚à∂=
(
0
‚àá‚ä§
‚àí‚àá
‚àá√ó
)
‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
ùúï1
ùúï2
ùúï3
‚àíùúï1
0
‚àíùúï3
ùúï2
‚àíùúï2
ùúï3
0
‚àíùúï1
‚àíùúï3
‚àíùúï2
ùúï1
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
We Ô¨Ånd
P (ùúï)2 = ‚àí(ùúï2
1 + ùúï2
2 + ùúï2
3
)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
showing
that
qP (ùúÜ) = ùúÜ2 + Œî
so
that
c0 (ùúï) = Œî. Thus, with g = ‚àí1‚àï4ùúã| ‚ãÖ| as
the associated fundamental solution, we
obtain
G = ‚àíP (ùúï) g
as
a
Green‚Äôs
tensor
for
P (ùúï) =
( 0
‚àá‚ä§
‚àí‚àá
‚àá√ó
)
.
Thus,
we
have
4) This extension is a technicality to obtain a for-
mally invertible diÔ¨Äerential operator matrix.
for y ‚àà‚Ñù3 ‚ßµ{0}
G (y) = ‚àí
1
4ùúã|y|3
(
0
y‚ä§
‚àíy
y√ó
)
.
Hence, a solution of
‚àá‚ä§H = 0
‚àá√ó H = J
can, for suitable divergence-free J, be given
in an obvious block matrix notation as
(
0
H (x)
)
in terms of a componentwise
convolution integral as
‚à´‚Ñù3
1
4ùúã|x ‚àíy|3
(
0
‚àí
(
x ‚àíy
)‚ä§
(x ‚àíy)
‚àí(x ‚àíy) √ó
)
(
0
J (y)
)
dy,
that is,
H (x) = ‚àí‚à´‚Ñù3
1
4ùúã|x ‚àíy|3
(x ‚àíy) √ó J (y) dy,
which
is
the
well-known
Biot‚ÄìSavart
formula.
The generality of the above solution con-
cepts (more details can be found in [11]) is
mathematically pleasing. It turns out, how-
ever, that the particular PDEs and systems
of interest in applications are from a rather
small subclass, see [12]. As a Ô¨Årst simpli-
fying observation, we note that we may
assume that we have a speciÔ¨Åc evolutionary
direction, which, by a simple rotation, we
always may assume to be ùúà0 = (1, 0, ‚Ä¶ , 0) ,
that is, the direction of the ‚Äútime‚Äù variable
t. By introducing higher time derivatives as
new unknowns, we may also assume that
the diÔ¨Äerential operators of interest are Ô¨Årst

536
16 Partial DiÔ¨Äerential Equations
order in time, that is, they are of the form
P (ùúï0, ‚Ä¶ , ùúïn
) = ùúï0M0 + M1 + P0
(ùúï1, ‚Ä¶ , ùúïn
)
(16.6)
with
P0 (0) = 0,
i.e.
P (0) = M1
and
P (1, 0, ‚Ä¶ , 0) = M0. Indeed, typical prob-
lems of mathematical physics are in this
form.
As an example, let us consider the
equations of acoustics
ùúï0p + ùúép + ‚àá‚ä§v = f
(ùúá, ùúé, ùúÖ‚àà‚Ñù‚â•0)
combined with a Newton law of the form
ùúï0(ùúáv) + ùúÖv + ‚àáp = 0,
where ‚àá=
‚éõ
‚éú
‚éú‚éù
ùúï1
ùúï2
ùúï3
‚éû
‚éü
‚éü‚é†
, and ‚àá‚ä§= (ùúï1 ùúï2 ùúï3
).
These can be combined in a block matrix
notation to give an equation of the form
P (ùúï)
( p
v
)
=
( f
0
)
involving the partial diÔ¨Äerential operator
P (ùúï0, ùúï1, ùúï2, ùúï3
)
= ùúï0
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
1
(0 0 0)
‚éõ
‚éú
‚éú‚éù
0
0
0
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
ùúá
0
0
0
ùúá
0
0
0
ùúá
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúé
(0 0 0)
‚éõ
‚éú
‚éú‚éù
0
0
0
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
ùúÖ
0
0
0
ùúÖ
0
0
0
ùúÖ
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
+P0
(ùúï1, ùúï2, ùúï3
) ,
(16.7)
where
P0
(ùúï1, ùúï2, ùúï3
) ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
‚àá‚ä§
‚àá
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
(16.8)
It is only after elimination of the veloc-
ity Ô¨Åeld v (assuming ùúï0ùúá+ ùúÖ‚â†0) that we
obtain a familiar wave equation in pressure
p alone:
ùúáùúï2
0p + (ùúÖ+ ùúáùúé) ùúï0p + ùúÖùúép ‚àíŒîp
= ùúï0ùúáf + ùúÖf =‚à∂g.
(16.9)
Note that if ùúá= 0, this turns into a diÔ¨Äusion
equation
ùúÖùúï0p + ùúÇp ‚àíŒîp = g
(ùúÇ‚à∂= ùúÖùúé) (16.10)
(also
known
in
diÔ¨Äerent
contexts
as
a
Fokker‚ÄìPlanck
equation
or
a
heat
equation). This is a typical situation in
applications: what are essentially the same
mathematical (systems of) equations may
have diÔ¨Äerent names (and units) and
interpretations in diÔ¨Äerent contexts.
The limit case ùúÖ= 0, ùúÇ= 1, now leads
to an invertible elliptic problem. The case
ùúÖ= 0, ùúÇ= 0, gives Poisson‚Äôs equation,
imposing further constraints on g and
leading to nonuniqueness. Indeed, there
are inÔ¨Ånitely many polynomial solutions
in Ker (Œî) , the so-called harmonic poly-
nomials. Modifying this degenerate case
to achieve a well-posed problem is the
subject of potential theory and involves
the introduction of growth constraints on
the distributional solution p and decay
conditions for the right-hand side g.
It turns out that, in this Ô¨Årst-order-
in-time form, problems of interest are
characterized by a single property of
fundamental importance: strict positive
deÔ¨Åniteness in Hùúà,0
(‚Ñùn+1), in the nonevo-
lutionary case for a particular ùúà(say ùúà= 0)
or, for the evolutionary case, for all ùúà= ùúöùúà0
in the time direction ùúà0 = (1, 0, ‚Ä¶ , 0) and
all suÔ¨Éciently large ùúö‚àà‚Ñù>0. Indeed, par-
tial diÔ¨Äerential operators P (ùúï) of the form
(16.6), predominant in applications, when

16.3 Partial DiÔ¨Äerential Equations of Mathematical Physics
537
realized in Hùúà,0
(‚Ñùn+1), all satisfy
‚Ñúùî¢‚ü®u|P (ùúï) u‚ü©ùúà,0 ‚â•c0 ‚ü®u|u‚ü©ùúà,0 = c0 |u|2
ùúà,0
(16.11)
for some c0 ‚àà‚Ñù>0 and all u ‚ààHùúà,0
(‚Ñùn+1)
such that P (ùúï) u ‚ààHùúà,0
(‚Ñùn+1). By the
Cauchy‚ÄìSchwarz inequality, we have
‚Ñúùî¢‚ü®u|P (ùúï) u‚ü©ùúà,0 ‚â§|u|ùúà,0 |P (ùúï) u|ùúà,0
and so we read oÔ¨Ä
|P (ùúï) u|ùúà,0 ‚â•c0 |u|ùúà,0 ,
which implies invertibility of P (ùúï) and con-
tinuity of the inverse. The same result fol-
lows for the adjoint diÔ¨Äerential operator
P (ùúï)‚àó. Since by the projection theorem (see
[10]),
Hùúà,0
(‚Ñùn+1) = Ran (P (ùúï))
‚üÇ‚äïRan (P (ùúï))
with
Ran (P (ùúï))
‚üÇ= Ker (P (ùúï)‚àó) = {0}
(because P (ùúï)‚àóis invertible), we get
|||P (ùúï)‚àí1 f |||ùúà,0 ‚â§c‚àí1
0 |f |ùúà,0
for all f ‚ààRan (P (ùúï)) and, by continu-
ous extension, for all f ‚ààRan (P (ùúï)) =
Hùúà,0
(‚Ñùn+1) . Thus, the Hadamard require-
ments are satisÔ¨Åed: we have existence and
uniqueness of solution for every right-hand
side in Hùúà,0
(‚Ñùn+1) and continuous depen-
dence on the data. As ùúà= (ùúö, 0, ‚Ä¶ , 0), we
have
Hùúà,0
(
‚Ñùn+1)
= Hùúö,0 (‚Ñù) ‚äóL2 (‚Ñùn),
(16.12)
where the tensor product (see [10]) on the
right-hand side may more intuitively be
understood as Hùúö,0‚àífunctions on ‚Ñùwith
values in L2 (‚Ñùn) , that is, we may consider
Hùúà,0
(‚Ñùn+1) as Hùúö,0
(‚Ñù, L2 (‚Ñùn)
) noting that
‚ü®u|v‚ü©ùúà,0 = ‚à´‚Ñù
‚ü®u (t) |v (t)‚ü©L2(‚Ñùn) exp (‚àí2ùúöt) dt
=‚à∂‚ü®u|v‚ü©ùúö,0,0 .
(16.13)
In the above example of the equations of
acoustics, (16.11) does not impose any
additional constraints on P0 because, in
this case, ‚Ñúùî¢‚ü®u|P0
(ùúï1, ùúï2, ùúï3
) u‚ü©
ùúà,0 hap-
pens to vanish as an integration by parts
calculation
conÔ¨Årms.
In
other
words,
A ‚à∂= P0
(ùúï1, ùúï2, ùúï3
)
is
skew-self-adjoint,
that is, A = ‚àíA‚àó(A is skew-self-adjoint if
and only if iA is self-adjoint, compare [10]).
That the spatial operator A is skew-self-
adjoint is not exceptional but rather the
general situation in mathematical physics
due to the typical Hamiltonian structure
of A as a block operator matrix of the
form
(0
‚àíC‚àó
C
0
)
where C is a closed,
densely deÔ¨Åned Hilbert space operator. We
shall explore this more deeply in the next
section.
16.3
Partial DiÔ¨Äerential Equations of
Mathematical Physics
As discussed in the previous section, strict
positive deÔ¨Åniteness is at the heart of the
solution theory for PDEs of mathematical
physics. Generalization to the case of phe-
nomena conÔ¨Åned to an open region Œ© in ‚Ñùn
(boundary value problems) and to media
with properties varying with their location
in space (variable coeÔ¨Écients) is straight-
forward. The typical Hamiltonian structure
is usually preserved by a suitable choice of
boundary conditions. Limit cases such as
static or stationary solutions may need spe-
ciÔ¨Åc strategies (e.g., decay constraints or

538
16 Partial DiÔ¨Äerential Equations
radiation conditions). Generic cases, how-
ever, enjoy the same simple structure mak-
ing them accessible to a uniÔ¨Åed solution
theory.
The abstract solution theory for a general
strictly positive deÔ¨Ånite, closed and densely
deÔ¨Åned linear operator ÓàΩwith a strictly
positive deÔ¨Ånite adjoint ÓàΩ‚àófollows as above
by simply replacing P (ùúï) by ÓàΩ. Somewhat
miraculously, this simple idea suÔ¨Éces to
understand the main PDEs of mathematical
physics. We assume that the time direction
is Ô¨Åxed and ùúï0 is the associated derivative
(i.e., t = x0 and ùúï0 is the time derivative).
To leave the spatial part as general as pos-
sible, we merely assume that we are deal-
ing with suitable functions on ‚Ñù(the time
domain) with values in a Hilbert space X.
In comparison with (16.12), (16.13) above,
this amounts to replacing L2 (‚Ñùn) by an
arbitrary Hilbert space so that we are con-
cerned with a solution theory in a Hilbert
space Hùúö,0 (‚Ñù, X) with inner product
‚ü®u|v‚ü©ùúö,0,0 ‚à∂= ‚à´‚Ñù
‚ü®u (t) |v (t)‚ü©X exp (‚àí2ùúöt) dt.
(16.14)
Thus we have an ‚Äúordinary‚Äù diÔ¨Äerential
equation on ‚Ñùin a Hilbert space X.
Following the structure introduced in
(16.6), we focus on the particular class of
abstract diÔ¨Äerential operators
ÓàΩ= ùúï0M0 + M1 + A,
(16.15)
where A is the canonical extension, to
the time-dependent case, of a closed
and densely deÔ¨Åned linear diÔ¨Äerential
operator A0 ‚à∂D (A0
) ‚äÜX ‚ÜíX, that is,
(Au) (t) = A0u (t) for (almost every) t ‚àà‚Ñù,
and similarly M0, M1 are the canonical
extensions to the time-dependent case of
a bounded linear operator in X. From the
observation at the end of the last section,
we may focus our interest on the case
where A0 and consequently A is skew-self-
adjoint. Indeed, the typical shape of A0 is in
a block matrix operator form
(0
‚àíC‚àó
C
0
)
.
Frequently, this is not obvious in historical
formulations of the equations of mathemat-
ical physics. For example, the Schr√∂dinger
equation, involving an operator of the form
ùúï0 + i L,
where L is nonnegative and self-adjoint,
appears not to Ô¨Åt in the above setting. How-
ever, separating real and imaginary part of
the equation
(
ùúï0 + i L
)
u = f
yields the system
(
ùúï0 +
( 0
‚àíL
L
0
)) ( ‚Ñúùî¢u
‚Ñëùî™u
)
=
( ‚Ñúùî¢f
‚Ñëùî™f
)
,
where
A =
( 0
‚àíL
L
0
)
has the required block matrix form because
L is self-adjoint.
Given that A is skew-self-adjoint, the
strict positive deÔ¨Åniteness of ÓàΩand ÓàΩ‚àó
(see (16.15)) reduces to that of ùúï0M0 + M1,
which is equivalent to requiring
‚ü®u|ùúöM0 + ‚Ñúùî¢M1u‚ü©ùúö,0,0 ‚â•c0 ‚ü®u|u‚ü©ùúö,0,0
(16.16)
for all u ‚ààHùúö,0 (‚Ñù, X) and all suÔ¨Éciently
large ùúö‚àà‚Ñù>0 as a constraint on the self-
adjoint, bounded linear operators M0,
‚Ñúùî¢M1 ‚à∂= 1
2
(M1 + M‚àó
1
) in X. Note that
considering ‚Ñúùî¢M1 has little to do with the
entries of M1 being real or complex. For

16.3 Partial DiÔ¨Äerential Equations of Mathematical Physics
539
example, take M1 to be some simple 2 √ó 2
matrices:
‚Ñúùî¢
( 1
‚àí1
0
1
)
= 1
2
(
2
‚àí1
‚àí1
2
)
,
‚Ñúùî¢
( i
‚àíi
0
1
)
= 1
2
( 0
‚àíi
i
2
)
.
If there is a need for more general mate-
rial laws, we may, by a simple perturbation
argument,
include
arbitrary
additional
bounded linear operators on Hùúö,0(‚Ñù, X).
Such an operator may result from a linear
operator ùïÑmapping X-valued step func-
tions (i.e., linear combinations of functions
of the form ùúí
I ‚äóh ‚à∂= t ÓÇ∂‚Üíùúí
I (t) h, where
ùúíI denotes the characteristic function of a
bounded interval I ‚äÜ‚Ñùand h ‚ààX) into
‚ãÇ
ùúö‚â•ùúö0 Hùúö,0 (‚Ñù, X). Assuming the estimate
|ùïÑùúô|ùúö,0,0 ‚â§c (ùúö) |ùúô|ùúö,0,0
for all X-valued step functions ùúôfor suitable
constants c(ùúö) satisfying
lim sup
ùúö‚Üí‚àû
c (ùúö) < c0,
(16.17)
we can continuously extend ùïÑto an oper-
ator
ÃÉ
M ‚à∂Hùúö,0(‚Ñù, X) ‚ÜíHùúö,0(‚Ñù, X).
The
operator
ÃÉM can model time-dependent
material laws or memory eÔ¨Äects. Let us
treat an example. For h < 0, we consider
the operator ùïÑ‚à∂= ùúèh of time translation,
mapping an X-valued step function u to
the step function t ÓÇ∂‚Üíu(t + h). Then for
ùúö> 0, we compute
|ùúèhu|2
ùúö,0,0 = ‚à´
‚Ñù
|u(t + h)|2e‚àí2ùúöt dt
= e2ùúöh|u|2
ùúö,0,0
showing that
lim sup
ùúö‚Üí‚àû
c(ùúö) = lim
ùúö‚Üí‚àûe2ùúöh = 0 < c0,
because h < 0.
A modiÔ¨Åed ÓàΩwould then be of the
form ùúï0M0 + M1 + ÃÉM + A. Abbreviating
M0 + ùúï‚àí1
0
(M1 + ÃÉM) as Óàπ(ùúï‚àí1
0
), we can
rewrite this generalized version of ÓàΩsimply
as ùúï0Óàπ
(
ùúï‚àí1
0
) + A and so we may reformu-
late the problem to be solved as Ô¨Ånding
U, V ‚ààHùúö,0 (‚Ñù, X) such that
ùúï0V + AU = F,
(16.18)
where the ‚Äúmaterial law‚Äù
V = Óàπ(ùúï‚àí1
0
) U
(16.19)
is satisÔ¨Åed.
This perturbation argument can actually
be made more useful by reÔ¨Åning the strict
positive deÔ¨Åniteness condition. An integra-
tion by parts calculation yields
‚Ñúùî¢‚ü®U| (ùúï0M0 + M1
) U‚ü©
ùúö,0,0
= ùúö‚ü®U|M0U‚ü©ùúö,0,0 + ‚ü®U| (‚Ñúùî¢M1
) U‚ü©
ùúö,0,0
‚â•d1ùúö‚ü®PU|PU‚ü©ùúö,0,0
+ d2 ‚ü®(1 ‚àíP) U| (1 ‚àíP) U‚ü©ùúö,0,0 ,
where P is the orthogonal projector onto
Ran (M0), d1 > 0 depends on the positive
deÔ¨Åniteness constant for M0 restricted
to the subspace Ran (M0), and d2 > 0
depends on the positive deÔ¨Åniteness con-
stant for ‚Ñúùî¢M1 restricted to the subspace
Ker (M0
) = (Ran (M0))‚üÇ. Thus, more gen-
eral perturbations ÃÉM can be considered
if ÃÉMP is a bounded linear operator and
ÃÉM (1 ‚àíP) is a bounded linear operator with
a bound ‚Äñ‚Äñ‚Äñ ÃÉM (1 ‚àíP)‚Äñ‚Äñ‚Äñ less than d2 uniformly
for all suÔ¨Éciently large ùúö.

540
16 Partial DiÔ¨Äerential Equations
16.4
Initial-Boundary Value Problems of
Mathematical Physics
As we have indicated, the equations of
mathematical physics have not in general
been formulated as the above type of Ô¨Årst-
order systems and it can be a formidable
task to rewrite the system to display the
generic
Hamiltonian
structure.
As
an
example, we discussed the system of acous-
tics. What is the impact of our abstract
considerations on this particular example?
The system already has the required form
with very simple operators M0, M1, which
are just multiplication by diagonal matrices.
For the spatial operator
A ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
‚àá‚ä§
‚àá
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
(16.20)
to be skew-self-adjoint a suitable bound-
ary condition on the underlying domain
Œ© ‚äÜ‚Ñù3 must be chosen. Vanishing of the
pressure distribution p on the boundary
(Dirichlet boundary condition) or the nor-
mal component of the velocity Ô¨Åeld (Neu-
mann boundary condition for p) are typical
choices resulting in skew-self-adjointness
of A.
From a mathematical perspective, we can
have general operators M0, M1, which may
be such that there is little chance of recov-
ering anything close to a well-known wave
equation. A simple special case would be
that all entries in the (not necessarily block
diagonal) operator matrices M0, M1 are
actually bounded multiplication operators
in space, that is, we consider coeÔ¨Écients
in the usual sense, which are varying in
space. Non-block-diagonal situations have
been investigated in recent times in studies
of metamaterials, see for example, [13, 14].
To illustrate some issues for the problem
class under consideration, we look at a
very speciÔ¨Åc ‚Äúwave equation‚Äù more closely.
Of course, for computational purposes, it
may be interesting to have more or less
explicit representation formulae for the
solutions. Such formulae, however, cannot
be expected in the general case. Even in the
block diagonal case, an example such as
ùúï0
( ‚ÑôŒ©1
0
0
‚ÑôŒ©2
)
+
( ‚ÑôŒ©‚ßµŒ©1
0
0
‚ÑôŒ©‚ßµŒ©2
)
+ A
for measurable sets Œ©1, Œ©2 ‚äÜŒ©, where
‚ÑôS
denotes
the
orthogonal
projector
generated
by
multiplication
with
the
characteristic function ùúí
S
of a subset
S ‚äÜ‚Ñùn, illustrates the problems involved.
In (Œ© ‚ßµŒ©1
) ‚à©(Œ© ‚ßµŒ©2
) the time derivative
vanishes (elliptic case), in Œ©1 ‚à©Œ©2 we
have reversible wave propagation (hyper-
bolic case) and everywhere else, that is, in
(Œ©1 ‚ßµŒ©2
) ‚à™(Œ©2 ‚ßµŒ©1
), we have a system
associated with a diÔ¨Äusion (parabolic case).
However, the complete system is covered
by our general framework.
Returning to our general form ùúï0M0 +
M1 + ÃÉM + A with A from (16.20), we note
that the crucial concept of causality char-
acterizing evolutionary processes is main-
tained for this abstract class in the sense
that, for all a ‚àà‚Ñù, if the right-hand side
(as an X-valued function) vanishes on the
open interval ] ‚àí‚àû, a[ then so does the
solution. Owing to time translation invari-
ance (which is satisÔ¨Åed for M0, M1, A and
which we assume to hold for ÃÉM), it suÔ¨Éces
to consider a = 0. The above yields that,
in a suitable sense, the term M0U associ-
ated with the solution U for a given right-
hand side F vanishing on ‚Ñù‚â§0 must ‚Äì as a
result its continuity ‚Äì vanish on ‚Ñù‚â§0 and
so, in particular, at time 0. Note that, if M0
has a nontrivial null space, U itself may be
discontinuous and so, although U (0‚àí) = 0,

16.4 Initial-Boundary Value Problems of Mathematical Physics
541
U (0) and consequently U (0+) may not be
deÔ¨Åned. From the perspective of classical
initial value problems, U satisÔ¨Åes the initial
condition
(M0U)
(0+) = 0.
(16.21)
Thus,
the
solution
U
of
(ùúï0M0 + M1 + ÃÉM +A) U = F with F van-
ishing on (‚àí‚àû, 0] satisÔ¨Åes homogeneous
initial conditions. How can we imple-
ment nonzero initial data? We think of
nonvanishing initial data as a jump in the
solution occurring at time 0. Noting that
the derivative of a constant is 0, we have
ùúï0M0
(
U ‚àíùúí
‚Ñù>0 ‚äóU0
)
= ùúï0M0U on ‚Ñù>0.
Thus, we may consider
ùúï0M0
(
U ‚àíùúí‚Ñù>0 ‚äóU0
)
+M1U + ÃÉMU + AU = F
(16.22)
as the proper formulation for the initial
value problem
ùúï0M0U + M1U + ÃÉMU + AU = F on ‚Ñù>0,
M0U (0+) = M0U0.
Recall that ÃÉM is a suÔ¨Éciently small per-
turbation in the sense of property (16.17).
As ùúï0ùúí
‚Ñù>0 is the Dirac-ùõø-distribution ùõø{0},
problem (16.22) is formally equivalent to
(ùúï0M0 + M1 + ÃÉM + A) U
= F + ùõø{0} ‚äóM0U0.
(16.23)
This yields the interpretation that initial
data correspond to Dirac ùõø{0} distribution-
type
sources.
However,
rather
than
discussing
possible
generalizations
of
our above solution theory to distribu-
tions, we prefer to reformulate (16.22) as a
problem for Ô¨Ånding V ‚à∂= U ‚àíùúí
‚Ñù>0 ‚äóU0.
Assuming U0 ‚ààD
(
A0
), we obtain
(ùúï0M0 + M1 + ÃÉM + A) V
= F ‚àíùúí‚Ñù>0 ‚äó(M1 + A) U0
‚àíÃÉM
(
ùúí‚Ñù>0 ‚äóU0
)
=‚à∂G,
which is covered by the above solution
theory.
Alternatively, introducing the perturba-
tion argument in a diÔ¨Äerent way, we have
V =
(
U ‚àíùúí‚Ñù>0 ‚äóU0
)
= (ùúï0M0 + M1 + A)‚àí1 G
‚àí(ùúï0M0 + M1 + A)‚àí1 ÃÉMV
=‚à∂T (V) ,
where
T,
by
our
assumptions,
is
a
contraction, that is,
|T (V)|ùúö,0,0 ‚â§q |V|ùúö,0,0
for
some
q < 1
independent
of
V ‚ààHùúö,0 (‚Ñù, X) for all suÔ¨Éciently large
ùúö. Hence, by the contraction mapping
theorem, the solution V can be found
iteratively:
V = lim
k‚Üí‚àûTk (V0
)
for arbitrary choice of V0 ‚ààHùúö,0 (‚Ñù, X) .
The solution U of the initial value problem
(16.22) is then
U = V + ùúí
‚Ñù>0 ‚äóU0.
Let us return to the simpler problem with
ÃÉM = 0. In this case, the underlying media,
assumed to satisfy (16.16), can be roughly
categorized by properties of M0, M1 as
follows.

542
16 Partial DiÔ¨Äerential Equations
Lossless media: ‚Ñúùî¢M1 = 0;
then, for media that are not lossless, we
have,
Lossy media: ‚Ñúùî¢M1 ‚â•0;
Gainy media: ‚Ñúùî¢M1 ‚â§0;
Chiral media: ‚Ñëùî™M1 ‚â†0.
The case ÃÉM = 0 also exhibits ‚Äúenergy
conservation.‚Äù Indeed, if F vanishes above
a time threshold t0, then we have the
following
pointwise
relation
for
the
solution U:
1
2 ‚ü®U (b) |M0U (b)‚ü©X
+ ‚à´
b
a
‚ü®U (s) |‚Ñúùî¢M1U (s)‚ü©X ds
= 1
2 ‚ü®U (a) |M0U (a)‚ü©X
for b ‚â•a ‚â•t0.
While we have concentrated so far on
the acoustic case (16.20), the underlying
considerations
are
completely
general.
Clearly ‚àácan be considered in higher (or
lower) dimensions. Further, we could for
example also replace ‚àáby some other
diÔ¨Äerential operator C (and ‚àá‚ä§by ‚àíC‚àó)
to obtain other well-known systems of
equations from mathematical physics. To
illustrate, we shall consider two other cases
more closely.
16.4.1
Maxwell‚Äôs Equations
First, we consider Maxwell‚Äôs equations in
the isotropic, homogeneous case. These can
be written in 2 √ó 2‚àíblock matrix operator
form
(ùúï0M0 + M1 + A) ( E
H
)
=
( ‚àíJ0
0
)
,
(16.24)
where
M0 ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
‚éõ
‚éú
‚éú‚éù
ùúÄ
0
0
0
ùúÄ
0
0
0
ùúÄ
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
ùúá
0
0
0
ùúá
0
0
0
ùúá
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
,
M1 ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
‚éõ
‚éú
‚éú‚éù
ùúé
0
0
0
ùúé
0
0
0
ùúé
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
,
(ùúÄ, ùúá‚àà‚Ñù>0, ùúé‚àà‚Ñù‚â•0) and
A ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚àá√ó
‚àí‚àá√ó
‚éõ
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
,
(16.25)
where ‚àá√ó ‚à∂=
‚éõ
‚éú
‚éú‚éù
0
‚àíùúï3
ùúï2
ùúï3
0
‚àíùúï1
‚àíùúï2
ùúï1
0
‚éû
‚éü
‚éü‚é†
. By a
suitable choice of boundary condition,
which commonly is the vanishing of the
tangential component of the electric Ô¨Åeld
E on the boundary of the underlying open
set Œ© ‚äÜ‚Ñù3 (electric boundary condition,
boundary condition of total reÔ¨Çexion), the
operator A can be established as skew-self-
adjoint in a six-component L2 (Œ©)-space.
The material laws Óàπ(ùúï‚àí1
0
) can be arbitrary
as long as (16.16) is satisÔ¨Åed. Thus, in this
general formulation, the most complex
materials (chiral media, metamaterials)
become accessible.

16.4 Initial-Boundary Value Problems of Mathematical Physics
543
16.4.2
Viscoelastic Solids
The system of linearized viscoelasticity is
commonly presented as
Div T + f = ùúö0 ùúï2
0u,
where u denotes the displacement Ô¨Åeld,
T = (Tjk
)
j,k‚àà{1,2,3} is the stress tensor, ùúö0
is
the
mass
density
and
Div T ‚à∂=
(‚àë3
k=1 ùúïkTjk
)
j‚àà{1,2,3} denotes the tensorial
divergence (in Cartesian coordinates).
With v ‚à∂= ùúï0u, we Ô¨Årst derive from the
deÔ¨Ånition
Óà±‚à∂= Grad u,
where
Grad u ‚à∂= 1
2
(ùúï‚äóu + (ùúï‚äóu)‚ä§)
denotes the symmetric part of the Jacobi
matrix ùúï‚äóu, another Ô¨Årst-order dynamic
equation
ùúï0Óà±= Grad v.
We can formally summarize the resulting
system in the form
ùúï0
( w
Óà±
)
+
(
0
‚àíDiv
‚àíGrad
0
) ( v
T
)
=
( f
0
)
with w = ùúö0 v. The system is completed by
linear material relations of various forms
linking Óà±and T. We follow the presentation
in [15].
16.4.2.1
The Kelvin‚ÄìVoigt Model
This class of materials is characterized by a
material relation of the form
T = CÓà±+ Dùúï0Óà±,
(16.26)
where the elasticity tensor C and the vis-
cosity tensor D are assumed to be modeled
as bounded, self-adjoint, strictly positive
deÔ¨Ånite mappings in a Hilbert space Xsym of
L2 (Œ©)-valued, symmetric 3 √ó 3-matrices,
with the inner product induced by the
Frobenius norm
(Œ¶, Œ®) ÓÇ∂‚Üí‚à´Œ©
trace
(
Œ¶ (x)‚àóŒ® (x)
)
dx .
By a suitable choice of boundary condi-
tion, that is, domain for
(
0
‚àíDiv
‚àíGrad
0
)
,
we can, as in previous cases, achieve
skew-self-adjointness
in
the
Hilbert
space
X ‚à∂= L2 (Œ©) ‚äïL2 (Œ©) ‚äïL2 (Œ©) ‚äïXsym.
For sake of deÔ¨Åniteness, let us consider the
vanishing of the displacement velocity at
the boundary of the domain Œ© containing
the medium. With this choice of operator
domain D (A),
A ‚à∂=
(
0
‚àíDiv
‚àíGrad
0
)
is skew-self-adjoint in Hùúö,0 (‚Ñù, X).
For D ‚â•c0 > 0, we obtain from (16.26)
that
Óà±= (C + Dùúï0
)‚àí1 T,
= ùúï‚àí1
0
(ùúï‚àí1
0 C + D)‚àí1 T,
which amounts to a ‚Äúmaterial law‚Äù of the
form
( w
Óà±
)
=
( ùúö0
0
0
0
) ( v
T
)
+ ùúï‚àí1
0
(( 0
0
0
D‚àí1
)
+ ÃÉM
) ( v
T
)

544
16 Partial DiÔ¨Äerential Equations
with
ÃÉM = ùúï‚àí1
0
( 0
0
0
‚àí(ùúï‚àí1
0 C + D
)‚àí1 C D‚àí1
)
.
This is the so-called Kelvin‚ÄìVoigt model
of viscoelasticity. The case C = 0 leads
to a system for a purely viscous behavior
(Newton model). On the other hand, if C
is strictly positive deÔ¨Ånite, then the limit
case D = 0 leads to the standard system for
elastic solids.
16.4.2.2
The Poynting‚ÄìThomson Model
(The Linear Standard Model)
The
linear
standard
model
or
Poynting‚ÄìThomson model is based on
a generalization of the Maxwell model
involving another coeÔ¨Écient operator R
and has the form
ùúï0Óà±+ R Óà±= C‚àí1ùúï0T + D‚àí1T.
Solving for Óà±, this yields
Óà±= (ùúï0 + R)‚àí1 (C‚àí1ùúï0 + D‚àí1) T = C‚àí1T
+ ùúï‚àí1
0
(
1 + R ùúï‚àí1
0
)‚àí1 (
D‚àí1 ‚àíRC‚àí1)
T
leading to a slightly more complex material
law
(
w
Óà±
)
=
(ùúö0 0
0 C‚àí1
)( v
T
)
+ ùúï‚àí1
0
((0
0
0
D‚àí1 ‚àíRC‚àí1
)
+ ÃÉM
)( v
T
)
with
ÃÉM = ùúï‚àí1
0
(
0
0
0
‚àíR (1 + R ùúï‚àí1
0
)‚àí1 (D‚àí1 ‚àíRC‚àí1)
)
.
16.5
Coupled Systems
An understanding of the speciÔ¨Åc mathe-
matical form that material laws must take
allows for a transparent discussion on how
to couple diÔ¨Äerent physical phenomena
to obtain a suitable evolutionary prob-
lem. Here we shall focus on the abstract
structure of coupled systems (compare
[16] for a discussion of coupled systems of
mathematical physics).
Without coupling, systems of interest
can be combined simply by writing them
together in diagonal block operator matrix
form:
ùúï0
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
V0
‚ãÆ
‚ãÆ
Vn
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
+ A
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
U0
‚ãÆ
‚ãÆ
Un
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
f0
‚ãÆ
‚ãÆ
fn
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
where
A =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
A0
0
¬∑ ¬∑ ¬∑
0
0
‚ã±
‚ãÆ
‚ãÆ
‚ã±
0
0
¬∑ ¬∑ ¬∑
0
An
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
inherits
the
skew-self-adjointness
in
X = ‚®Ån
k=0 Xk from its skew-self-adjoint
diagonal entries Ak ‚à∂D (Ak
) ‚äÜXk ‚ÜíXk,
k ‚àà{0, ‚Ä¶ , n}. The combined material law
takes the simple Block-diagonal form
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
V0
‚ãÆ
‚ãÆ
Vn
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
Óàπ00
(ùúï‚àí1
0
) 0 ¬∑ ¬∑ ¬∑
0
0
‚ã±
‚ãÆ
‚ãÆ
‚ã±
0
0
¬∑ ¬∑ ¬∑ 0 Óàπnn
(ùúï‚àí1
0
)
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
U0
‚ãÆ
‚ãÆ
Un
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.

16.5 Coupled Systems
545
Coupling between the phenomena now
can be modeled by introducing suitable oÔ¨Ä-
diagonal entries. The full material law now
is of the familiar form
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
V0
‚ãÆ
‚ãÆ
Vn
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
= Óàπ(ùúï‚àí1
0
)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
U0
‚ãÆ
‚ãÆ
Un
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
with
Óàπ(ùúï‚àí1
0
)‚à∂=
‚éõ
‚éú
‚éú
‚éú‚éù
Óàπ00
(ùúï‚àí1
0
) ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ Óàπ0n
(ùúï‚àí1
0
)
‚ãÆ
‚ã±
‚ãÆ
‚ãÆ
‚ã±
‚ãÆ
Óàπn0
(ùúï‚àí1
0
) ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ Óàπnn
(ùúï‚àí1
0
)
‚éû
‚éü
‚éü
‚éü‚é†
.
We consider some applications.
16.5.1
Thermoelasticity
As a Ô¨Årst illustration, we consider the
general thermoelastic system
ùúï0V +
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
Div 0
0
Grad
0
0
0
0
0
0 ‚àá‚ä§
0
0
‚àá
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
v
T
ùúÉ
Q
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
f
0
g
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
where we assume that the domains of oper-
ators containing the spatial derivatives are
such that we model, for example, Dirich-
let boundary conditions for the displace-
ment velocity and the temperature in order
to maintain skew-self-adjointness of
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
Div
0
0
Grad
0
0
0
0
0
0
‚àá‚ä§
0
0
‚àá
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
The material law is of the form
V = Óàπ(ùúï‚àí1
0
)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
v
T
ùúÉ
Q
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
where Óàπ(ùúï‚àí1
0
) is given by
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúö0
0
0
0
0
C‚àí1
C‚àí1Œì
0
0 Œì‚àóC‚àí1 w + Œì‚àóC‚àí1Œì
0
0
0
0
q0 + q2
(ùõº+ ùõΩùúï0
)‚àí1
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
Via symmetric row and column opera-
tions, this can be reduced to the Block-
diagonal form
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúö0
0
0
0
0
C‚àí1
0
0
0
0
w
0
0
0
0
q0 + q2
(ùõº+ ùõΩùúï0
)‚àí1
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
and so the issue of classifying the material
is simpliÔ¨Åed. For example, the issue of strict
positive deÔ¨Åniteness of
M0 =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúö0
0
0
0
0
C‚àí1
C‚àí1Œì
0
0
Œì‚àóC‚àí1
w + Œì‚àóC‚àí1Œì
0
0
0
0
q0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
hinges on the strict positive deÔ¨Åniteness of
ùúö0, C, w, q0.
For q0 = 0, the above system is known
as a type-3 thermoelastic system. With ùõº=
0, we obtain the special case of thermoe-
lasticity with second sound, that is, with
the Cataneo modiÔ¨Åcation of heat transport.
The so-called type-2 thermoelastic system
results by letting q2 = 0.
We point out that the well-known Biot
system, see, for example, [17‚Äì19], which
describes consolidation of a linearly elastic

546
16 Partial DiÔ¨Äerential Equations
porous medium, can be reformulated so
that, up to physical interpretations, it has
the same form as the thermoelastic sys-
tem. The coupling operator Œì of thermoe-
lasticity is, in the poroelastic case, given as
Œì =
‚éõ
‚éú
‚éú‚éù
ùõº
0
0
0
ùõº
0
0
0
ùõº
‚éû
‚éü
‚éü‚é†
, where ùõºis a coupling
parameter.
16.5.2
Piezoelectromagnetism
As a second class of examples we consider
the coupling of elastic and electromagnetic
wave propagation. Here, we have a system
of the form
ùúï0V + A
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
v
T
E
H
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
f
0
‚àíJ
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
where, by a suitable choice of boundary
conditions, A will be a skew-self-adjoint
block operator matrix of the form
A =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
Div
0
0
Grad
0
0
0
0
0
0
‚àá√ó
0
0
‚àí‚àá√ó
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
This system needs to be completed by suit-
able material relations. A simple piezoelec-
tromagnetic model is described by
V =
‚éõ
‚éú
‚éú
‚éú‚éù
ùúö0
0
0
0
0
C‚àí1
C‚àí1d
0
0 d‚àóC‚àí1 ùúÄ+ d‚àóC‚àí1d + ùúï‚àí1
0 ùúé0
0
0
0
ùúá
‚éû
‚éü
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú
‚éú‚éù
v
T
E
H
‚éû
‚éü
‚éü
‚éü‚é†
with the coupling given by a bounded linear
mapping d from L2 (Œ©) ‚äïL2 (Œ©) ‚äïL2 (Œ©)
to Xsym .
Following [20], we obtain a more com-
plicated coupling mechanism. Adding a
conductivity term, the coupling is initially
described in the form
T = C Óà±‚àídE ‚àíq H,
D = d‚àóÓà±+ ùúÄE + e H + ùúï‚àí1
0 ùúéE,
B = q‚àóÓà±+ e‚àóE + ùúáH.
Domain and range spaces for the additional
bounded, linear coeÔ¨Écient operators q and
e are clear from these equations and, for
sake of brevity, we shall not elaborate on
this. As has been already noted, for a proper
formulation we need to solve for Óà±to obtain
suitable material relations. We Ô¨Ånd
Óà±= C‚àí1T + C‚àí1dE + C‚àí1qH,
D = d‚àóC‚àí1T + (ùúÄ+ d‚àóC‚àí1d) E
+ d‚àóC‚àí1q H + e H + ùúï‚àí1
0 ùúéE,
B = q‚àóC‚àí1T + q‚àóC‚àí1dE
+ q‚àóC‚àí1q H + e‚àóE + ùúáH.
Thus, we obtain the material law
V = Óàπ(ùúï‚àí1
0
)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
v
T
E
H
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
with
Óàπ(ùúï‚àí1
0
)
=
‚éõ
‚éú
‚éú‚éù
ùúö0
0
0
0
0
C‚àí1
C‚àí1d
C‚àí1q
0 d‚àóC‚àí1 (ùúÄ+ d‚àóC‚àí1d) + ùúï‚àí1
0 ùúéd‚àóC‚àí1q + e
0 q‚àóC‚àí1
q‚àóC‚àí1d + e‚àó
ùúá+ q‚àóC‚àí1q
‚éû
‚éü
‚éü‚é†
.
Via symmetric row and column operations,
we obtain the block-diagonal operator

16.5 Coupled Systems
547
matrix
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúö0
0
0
0
0
C‚àí1
0
0
0
0
ùúÄ+ ùúï‚àí1
0 ùúé
0
0
0
0
ùúá‚àíe‚àóùúÄ‚àí1e
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
Thus, the given form of material relations
leads to a material law in the above sense if
in addition to the strict positive deÔ¨Åniteness
of the self-adjoint bounded operators ùúö0, C,
ùúÄ, and ùúáwe require
ùúá‚â•ùúá0 + e‚àóùúÄ‚àí1e
for some constant ùúá0 ‚àà‚Ñù>0. Again, we
emphasize that, without additional eÔ¨Äort,
more complex material relations in the
sense of the above theory could be consid-
ered.
16.5.3
The Extended Maxwell System and its Uses
It has been shown [21] that Maxwell‚Äôs
equations in an open domain Œ© ‚äÜ‚Ñù3 may
be formulated as a formally coupled system
of PDEs. For this, we introduce the formal
operator matrices
AD ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
‚àá‚ä§
0
0
‚àá
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
AN ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
0
0
0
0
0
0
0
0
0
0
‚àá
0
0
‚àá‚ä§
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
AE ‚à∂=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
0
0
0
0
0
‚àí‚àá√ó
0
0
‚àá√ó
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
subject
to
boundary
conditions:
AD
inherits homogeneous Dirichlet boundary
conditions for ‚àá, AN is endowed with
homogeneous Neumann boundary condi-
tions for ‚àá‚ä§, and the lower left operator in
the operator matrix AE carries the electric
boundary condition. Now, assuming for
simplicity that all material parameters
are set to one, it turns out that Maxwell‚Äôs
equation (ùúï0 + AE
) u = f can be written as
(ùúï0 + AE + AD + AN
) U = F,
where
U = (0, u, 0)
and
F = ÃÉf + ùúï‚àí1
0
(AN + AD
)ÃÉf with ÃÉf = (0, f , 0).
This
so-called
extended
Maxwell‚Äôs
equation gives a link to the Dirac equation.
Indeed, we compute
(ùúï0 + AE + AD + AN
)(ùúï0 ‚àí(AE + AD + AN
))
= ùúï2
0 ‚àí(AE + AD + AN
)2
= ùúï2
0 ‚àíŒî.
Thus,
the
extended
Maxwell
system
(ùúï0 + AE + AD + AN
)
corresponds
to
a
0-mass Dirac equation.
By appropriate permutation of rows and
columns the more general mass 1 Dirac
equation admits the form
(
ùúï0 +
(0 ‚àíS‚àó
S
0
)
+ AE + AD + AN
)
V = G,
where
S =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
(
0
0
1
)
‚éõ
‚éú
‚éú‚éù
0
0
1
‚éû
‚éü
‚éü‚é†
‚éõ
‚éú
‚éú‚éù
0
1
0
‚àí1
0
0
0
0
0
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.

548
16 Partial DiÔ¨Äerential Equations
Finally, the Maxwell‚ÄìDirac system also
shares a similar form. Consider the system
(ùúï0 + A)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
E
H
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
= f ,
(ùúï0 + M1 + A) Œ® = g,
(16.27)
(
ùúï0 ‚àíA)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
ùõº
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
E
H
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
where A = (AE + AD + AN
) and f and g are
suitable source terms. As ùúï0 and A com-
mute, the Ô¨Årst and third equations can be
combined to give
(ùúï2
0 ‚àíA2)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
ùõº
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
= f ,
and trivially we have
(Aùúï0 ‚àíùúï0A)
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
ùõº
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
= 0.
These last two equations can be written as
(
ùúï0+
( 0
A
A
0
))
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
ùúï0
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
ùõº
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
‚àíA
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
ùõº
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
=
( f
0
)
.
Inserting the second equation from (16.27)
gives
‚éõ
‚éú
‚éú‚éù
ùúï0 + ÃÉM1 +
‚éõ
‚éú
‚éú‚éù
0
0
A
0
A
0
A
0
0
‚éû
‚éü
‚éü‚é†
‚éû
‚éü
‚éü‚é†
U =
‚éõ
‚éú
‚éú‚éù
f
g
0
‚éû
‚éü
‚éü‚é†
,
where
ÃÉM1 =
‚éõ
‚éú
‚éú‚éù
0
0
0
0
M1
0
0
0
0
‚éû
‚éü
‚éü‚é†
and
U =
‚éõ
‚éú
‚éú‚éù
U0
U1
U2
‚éû
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
ùúï0
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
ùõº
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
Œ®
‚àíA
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
ùõº
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
.
Coupling occurs here in an essential way via
a suitable quadratic nonlinear dependence
of f and g on the solution U. The middle
component U1 = Œ® is the Dirac Ô¨Åeld, the
electromagnetic Ô¨Åeld can be recovered
as U0 + U2 =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
0
E
H
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
, and the so-called
potential can be obtained by integrating
U0 = ùúï0
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
ùúë
A
0
0
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
.
References
1. Constanda, C. (2010) Solution Techniques for
Elementary Partial DiÔ¨Äerential Equations,
Taylor and Francis.
2. Haberman, R. (2004) Applied Partial
DiÔ¨Äerential Equations, Prentice Hall.

References
549
3. Weinberger, H.F. (1995) A First Course in
Partial DiÔ¨Äerential Equations, Dover, New
York.
4. Zauderer, E. (1998) Partial DiÔ¨Äerential
Equations of Applied Mathematics,
Wiley-Interscience, New York.
5. Garabedian, P.R. (1964) Partial DiÔ¨Äerential
Equations, John Wiley & Sons , Inc., New
York.
6. Hormander, L. (1990) The Analysis of Linear
Partial DiÔ¨Äerential Operators I,
Springer-Verlag.
7. Leis, R. (1986) Initial Boundary Value
Problems in Mathematical Physics, John
Wiley and Sons Ltd and B. G. Teubner,
Stuttgart.
8. Showalter, R.E. (1977) Hilbert Space
Methods for Partial DiÔ¨Äerential Equations,
Pitman.
9. Mathematical Transformations and Their
Uses. This handbook, Chapter 15.
10. Exner, P. Functional Analysis. This
handbook, Chapter 13.
11. Picard, R. and McGhee, D. (2011) Partial
DiÔ¨Äerential Equations: A UniÔ¨Åed Hilbert
Space Approach, de Gruyter Expositions in
Mathematics 55. de Gruyter, Berlin, xviii.
12. Picard, R. (2009) A structural observation
for linear material laws in classical
mathematical physics. Math. Methods Appl.
Sci., 32 (14), 1768‚Äì1803.
13. Lindell, I.V., Sihvola, A.H., Tretyakov, S.A.,
and Viitanen, A.J. (1994) Electromagnetic
waves in chiral and bi-isotropic media,
Artech House, Boston, MA and London.
14. Lakhtakia, A. (1994) Beltrami Fields in
Chiral Media, World ScientiÔ¨Åc, Singapore.
15. Bertram, A. (2005) Elasticity and Plasticity
of Large Deformations: An Introduction,
Springer, Berlin.
16. Bednarcyk, B.A. (2002) A Fully Coupled
Micro/Macro Theory for
Thermo-Electro-Magneto-Elasto-Plastic
Composite Laminates. Technical Report
211468, NASA.
17. Biot, M.A. (1941) General theory of
three-dimensional consolidation. J. Appl.
Phys., Lancaster, PA, 12, 155‚Äì164.
18. Showalter, R.E. (2000) DiÔ¨Äusion in
poro-elastic media. J. Math. Anal. Appl., 251
(1), 310‚Äì340.
19. Bear, J. and Bachmat, Y. (1990) Introduction
to Modelling of Transport Phenomena in
Porous Media, Theory and Applications of
Transport in Porous Media 4, Kluwer
Academic Publishers, Dordrecht etc.
20. Pan, E. and Heyliger, P.R. (2003) Exact
solutions for magneto-electro-elastic
laminates in cylindrical bending. Int. J. Solids
Struct., 40 (24), 6859‚Äì6876.
21. Picard, R. (1984) On the low frequency
asymptotics in electromagnetic theory. J.
Reine Angew. Math., 354, 50‚Äì73.


551
17
Calculus of Variations
Tom√°≈° Roub√≠Àácek
17.1
Introduction
The history of the calculus of variations
dates back several thousand years, fulÔ¨Ålling
the ambition of mankind to seek lucid prin-
ciples that govern the Universe. Typically,
one tries to identify scalar-valued function-
als having a clear physical interpretation,
for example, time, length, area, energy, and
entropy, whose extremal (critical) points
(sometimes under some constraints) repre-
sent solutions of the problem in question.
Rapid development was initiated between
the sixteenth and nineteenth centuries
when practically every leading scholar, for
example, J. Bernoulli, B. Bolzano, L. Euler,
P. Fermat, J.L. Lagrange, A.-M. Legendre,
G.W. Leibniz,
I. Newton,
K. Weierstrass
and many others, contributed to varia-
tional calculus; at that time, the focus
was rather on one-dimensional problems
cf. also [1‚Äì3]. There has been progress
through the twentieth century, which is
still continuing, informed by the histor-
ically important project of Hilbert [4],
Problems 19, 20, and 23] and accelerated
by the development of functional analysis,
theory of partial diÔ¨Äerential equations,
and eÔ¨Écient computational algorithms
supported by rigorous numerical analysis
and computers of ever-increasing power.
Modern methods allow simple formula-
tions in abstract spaces where technicalities
are suppressed, cf. Section 17.2, although
concrete problems ultimately require addi-
tional tools, cf. Section 17.3. An important
‚Äúside eÔ¨Äect‚Äù has been the development of a
sound theory of optimization and optimal
control and of its foundations, convex and
nonsmooth analysis.
17.2
Abstract Variational Problems
Variational problems typically deal with a
real-valued functional Œ¶ ‚à∂V ‚Üí‚Ñùon an
abstract space V that is equipped with a
linear structure to handle variations and
a topological structure to handle various
continuity/stability/localization concepts.
In the simplest and usually suÔ¨Éciently gen-
eral scenario, V is a Banach space1) [5] or,
1) A linear space equipped with a norm ‚Äñ ‚ãÖ‚Äñ,
that is, 0 ‚â§‚Äñu+v‚Äñ ‚â§‚Äñu‚Äñ+‚Äñv‚Äñ, ‚Äñu‚Äñ=0 ‚áíu=0,
‚ÄñùúÜu‚Äñ = ùúÜ‚Äñu‚Äñ for any ùúÜ‚â•0 and u, v‚ààV, is
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

552
17 Calculus of Variations
in physics, often even a Hilbert space.2) The
Banach space structure allows us to deÔ¨Åne
basic notions, such as linearity, continuity,
and convexity: Œ¶ is called continuous if
Œ¶(uk) ‚ÜíŒ¶(u) for any uk ‚Üíu, convex if
Œ¶(ùúÜu + (1 ‚àíùúÜ)v) ‚â§ùúÜŒ¶(u) + (1 ‚àíùúÜ)Œ¶(v)
for any u, v ‚ààV and 0 ‚â§ùúÜ‚â§1, concave
if ‚àíŒ¶ is convex, or linear if it is convex,
concave, and Œ¶(0) = 0.
Yet it should be pointed out that the lin-
ear structure imposed on a problem is the
result of our choice; it serves rather as a
mathematical tool used to deÔ¨Åne variations
or laws of evolution, or to devise numeri-
cal algorithms, and so on. Often, this choice
is rather artiÔ¨Åcial, especially if it leads to
nonquadratic or even nonconvex function-
als possibly with nonlinear constraints.
17.2.1
Smooth (DiÔ¨Äerentiable) Case
The Banach space structure allows further
to say that Œ¶ is directionally diÔ¨Äerentiable if
the directional derivative at u in the direc-
tion of (variation) v, deÔ¨Åned as
DŒ¶(u, v) = lim
ùúÄ‚Üò0
Œ¶(u + ùúÄv) ‚àíŒ¶(u)
ùúÄ
,
(17.1)
exists for any u, v ‚ààV, and is smooth if it
is directionally diÔ¨Äerentiable and DŒ¶(u, ‚ãÖ) ‚à∂
V ‚Üí‚Ñùis a linear continuous functional;
then the G√¢teaux diÔ¨Äerential Œ¶‚Ä≤(u) ‚ààV ‚àó,
with V ‚àóbeing the dual space,3) is deÔ¨Åned
called a Banach space if it is complete, that
is, any Cauchy sequence {uk}k‚àà‚Ñïconverges:
limmax(k,l)‚Üí‚àû‚Äñuk‚àíul‚Äñ = 0 implies that there is
u‚ààV such that limk‚Üí‚àû‚Äñuk‚àíu‚Äñ = 0; then we
write uk ‚Üíu.
2) This is a Banach space V whose norm makes the
functional V ‚Üí‚Ñù‚à∂u ÓÇ∂‚Üí‚Äñu+v‚Äñ2 ‚àí‚Äñu‚àív‚Äñ2 linear
for any v‚ààV; in this case, we deÔ¨Åne the scalar
product by (u, v) = 1
4 ‚Äñu+v‚Äñ2 ‚àí1
4 ‚Äñu‚àív‚Äñ2.
3) The dual space V ‚àóis the Banach space of all
linear continuous functionals f on V with the
norm ‚Äñf ‚Äñ‚àó= sup‚Äñu‚Äñ‚â§1‚ü®f , u‚ü©, with the duality
by
‚ü®Œ¶‚Ä≤(u), v‚ü©= DŒ¶(u, v).
(17.2)
If Œ¶‚Ä≤ ‚à∂V ‚ÜíV ‚àóis continuous, then Œ¶ is
called continuously diÔ¨Äerentiable. Further-
more, u ‚ààV is called a critical point if
Œ¶‚Ä≤(u) = 0,
(17.3)
which
is
an
abstract
version
of
the
Euler‚ÄìLagrange equation. In fact, (17.3)
is a special case of the abstract operator
equation
A(u) = f
with A ‚à∂V ‚ÜíV ‚àó, f ‚ààV ‚àó,
(17.4)
provided A = Œ¶‚Ä≤ + f for some potential Œ¶
whose existence requires some symmetry
of A: if A itself is G√¢teaux diÔ¨Äerentiable and
hemicontinuous,4) it has a potential if, and
only if, it is symmetric, that is,
‚ü®[A‚Ä≤(u)](v), w‚ü©= ‚ü®[A‚Ä≤(u)](w), v‚ü©
(17.5)
for any u, v, w‚ààV; up to a constant; this
potential is given by the formula
Œ¶(u) = ‚à´
1
0
‚ü®A(ùúÜu), u‚ü©dùúÜ.
(17.6)
Equation (17.3) is satisÔ¨Åed, for example, if
Œ¶ attains its minimum5) or maximum at u.
The former case is often connected with a
minimum-energy principle that is assumed
pairing ‚ü®‚ãÖ, ‚ãÖ‚ü©‚à∂V √ó V ‚àó‚Üí‚Ñùbeing the bilinear
form deÔ¨Åned by ‚ü®f , u‚ü©= f (u).
4) This is a very weak mode of continuity, requiring
that t ÓÇ∂‚Üí‚ü®A(u+tv), w‚ü©is continuous.
5) The proof is simple: suppose Œ¶(u) = min Œ¶(‚ãÖ)
but Œ¶‚Ä≤(u) ‚â†0, then for some v ‚ààV we
would have ‚ü®Œ¶‚Ä≤(u), v‚ü©= DŒ¶(u, v) < 0
so that, for a suÔ¨Éciently small ùúÄ> 0,
Œ¶(u+ùúÄv) = Œ¶(u) + ùúÄ‚ü®Œ¶‚Ä≤(u), v‚ü©+ o(ùúÄ) < Œ¶(u),
a contradiction.

17.2 Abstract Variational Problems
553
to govern many steady-state physical prob-
lems. The existence of solutions to (17.3)
can thus often be based on the existence
of a minimizer of Œ¶, which can rely on the
Bolzano‚ÄìWeierstrass theorem, which states
that a lower (resp. upper) semicontinuous
functional6) on a compact7) set attains its
minimum (resp. maximum).
In inÔ¨Ånite-dimensional Banach spaces,
it is convenient to use this theorem with
respect
to
weak*
convergence:
assum-
ing V = (V ‚Ä≤)‚àófor some Banach space
V ‚Ä≤ (called the pre-dual), we say that a
sequence {uk}k‚àà‚Ñïconverges weakly* to u if
limk‚Üí‚àû‚ü®uk, z‚ü©= ‚ü®u, z‚ü©for any z ‚ààV ‚Ä≤. If V ‚àó
is taken instead of V ‚Ä≤, this mode of con-
vergence is called weak convergence. Often
V ‚Ä≤ = V ‚àó(such spaces are called reÔ¨Çexive),
and then the weak* and the weak conver-
gences coincide. The Bolzano‚ÄìWeierstrass
theorem underlies the direct method,8)
invented essentially in [6], for proving exis-
tence of a solution to (17.3). We say that Œ¶
is coercive if lim‚Äñu‚Äñ‚Üí‚àûŒ¶(u)‚àï‚Äñu‚Äñ = +‚àû.
Theorem 17.1 (Direct method) 9)
Let V
have a pre-dual and Œ¶ ‚à∂V ‚Üí‚Ñùbe weakly*
lower semicontinuous, smooth, and coer-
cive. Then (17.3) has a solution.
6) Lower semicontinuity of Œ¶ means that
lim infk‚Üí‚àûŒ¶(uk) ‚â•Œ¶(u) for any sequence
{uk}k‚àà‚Ñïconverging (in a sense to be speciÔ¨Åed)
to u; more precisely, this is sequential lower
semicontinuity, but we will conÔ¨Åne ourselves to
the sequential concept throughout the chapter,
which is suÔ¨Éciently general provided the related
topologies are metrizable.
7) A set is compact if any sequence has a converg-
ing (in the same sense as used for the semiconti-
nuity of the functional) subsequence.
8) This means that no approximation and subse-
quent convergence is needed.
9) The proof relies on coercivity of Œ¶, which allows
for a localization on bounded sets and then, due
to weak* compactness of convex closed bounded
sets in V, on the Bolzano‚ÄìWeierstrass theorem.
AS continuous convex functionals are
also weakly* lower semicontinuous, one
gets a useful modiÔ¨Åcation:
Theorem 17.2 (Direct method II) Let V
have a pre-dual and let Œ¶ ‚à∂V ‚Üí‚Ñùbe
continuous, smooth, coercive, and convex.
Then (17.3) has a solution.
If
Œ¶
is
furthermore
strictly
convex
in
the
sense
that
Œ¶(ùúÜu+(1‚àíùúÜ)v) <
ùúÜŒ¶(u) + (1‚àíùúÜ)Œ¶(v)
for
any
u ‚â†v
and
0 < ùúÜ< 1, then (17.3) has at most one
solution.
We
say
that
a
nonlinear
oper-
ator
A ‚à∂V ‚ÜíV ‚àó
is
monotone
if
‚ü®A(u)‚àíA(v), u‚àív‚ü©‚â•0
for
any
u, v ‚ààV.
Monotonicity of a potential nonlinear
operator implies convexity of its poten-
tial, and then Theorem 17.2 implies the
following.
Theorem 17.3 Let V
be reÔ¨Çexive and
A ‚à∂V ‚ÜíV ‚àó
be
monotone,
hemicon-
tinuous,
coercive
in
the
sense
that
lim‚Äñu‚Äñ‚Üí‚àû‚ü®A(u), u‚ü©= ‚àû,
and
possess
a
potential. Then, for any f ‚ààV ‚àó, (17.4) has
a solution.
In fact, Theorem 17.3 holds even for
mappings
not
having
a
potential
but
its proof, due to Br√©zis [7], then relies
on an approximation and on implicit,
nonconstructive Ô¨Åxed-point arguments.
The solutions to (17.3) do not need to
represent the global minimizers that we
have considered so far. Local minimizers,
being consistent with physical principles
of minimization of energy, would also
serve well. The same holds for maximizers.
Critical points may, however, have a more
complicated saddle-like character. One
intuitive example is the following: let the
origin, being at the level 0, be surrounded
by a range of mountains all of height h > 0
at distance ùúåfrom the origin, but assume

554
17 Calculus of Variations
that there is at least one location v beyond
that circle, which has lower altitude. Going
from the origin to v, one is tempted to
minimize climbing and takes a mountain
pass. The Ambrosetti‚ÄìRabinowitz moun-
tain pass theorem [8] says that there is such
a mountain pass and Œ¶‚Ä≤ vanishes there.
More rigorously, we have Theorem 17.4.
Theorem 17.4 (Mountain pass) Let
Œ¶
be continuously differentiable, satisfy the
Palais‚ÄìSmale property10) and satisfy the
following three conditions:
Œ¶(0) = 0,
(17.7a)
‚àÉùúå, h > 0‚à∂‚Äñu‚Äñ = ùúå‚áíŒ¶(u) ‚â•h, (17.7b)
‚àÉv ‚ààV‚à∂
‚Äñv‚Äñ > ùúå,
Œ¶(v) < h.
(17.7c)
Then Œ¶ has a critical point u ‚â†0.
A similar assertion relies on a Carte-
sian structure, leading to a von Neumann‚Äôs
saddle-point theorem.
Theorem 17.5 (Saddle point)11) Let V =
Y √ó Z be reÔ¨Çexive, Œ¶(y, ‚ãÖ) ‚à∂Z ‚Üí‚Ñùbe con-
cave continuous and Œ¶(‚ãÖ, z) ‚à∂Y ‚Üí‚Ñùbe
convex continuous for any (y, z) ‚ààY √ó Z,
Œ¶(‚ãÖ, z) ‚à∂Y ‚Üí‚Ñùand let ‚àíŒ¶(y, ‚ãÖ) ‚à∂Z ‚Üí‚Ñù
be coercive for some (y, z) ‚ààY √ó Z. Then
there is (y, z) ‚ààY √ó Z so that
‚àÄÃÉy‚ààY‚àÄÃÉz‚ààZ ‚à∂Œ¶(ÃÉy, z) ‚â•Œ¶(y, z) ‚â•Œ¶(y, ÃÉz)
and, if Œ¶ is smooth, then Œ¶‚Ä≤(y, z) = 0.
10) More speciÔ¨Åcally, {Œ¶(uk)}k‚àà‚Ñïbounded and
limk‚Üí‚àû||Œ¶‚Ä≤(uk)||V ‚àó= 0 imply that {uk}k‚àà‚Ñïhas
a convergent subsequence.
11) The proof is nonconstructive, based on a
Ô¨Åxed-point argument, see, for example, [9,
Theorems 9D and 49A with Prop. 9.9]. The
original von Neumann‚Äôs version [10] dealt with
the Ô¨Ånite-dimensional case only.
17.2.2
Nonsmooth Case
For Œ¶ ‚à∂V ÓÇ∂‚Üí‚Ñù‚à™{+‚àû} convex, we deÔ¨Åne
the subdiÔ¨Äerential of Œ¶ at u as
ùúïŒ¶(u) =
{
f ‚ààV ‚àó; ‚àÄv‚ààV ‚à∂
Œ¶(v) + ‚ü®f , u‚àív‚ü©‚â•Œ¶(u)
}
. (17.8)
If Œ¶ is G√¢teaux diÔ¨Äerentiable, then ùúïŒ¶(u) =
{Œ¶‚Ä≤(u)}, hence this notion is indeed a gen-
eralization of the conventional diÔ¨Äerential.
Instead of the abstract Euler‚ÄìLagrange
equation (17.3), it is natural to consider
the abstract inclusion 0 ‚ààùúïŒ¶(u). More
generally, assuming Œ¶ = Œ¶0 + Œ¶1 with Œ¶0
smooth and Œ¶1 convex, instead of (17.3),
we consider the inclusion
ùúïŒ¶1(u) + Œ¶‚Ä≤
0(u) ‚àã0.
(17.9)
In view of (17.8), this inclusion can equally
be written as a variational inequality
‚àÄv‚ààV ‚à∂Œ¶1(v) + ‚ü®Œ¶‚Ä≤
0(u), v‚àíu‚ü©‚â•Œ¶1(u).
(17.10)
Theorems 17.1 and 17.2 can be reformu-
lated, for example, as follows.
Theorem 17.6 Let V have a pre-dual and
let Œ¶0 ‚à∂V ‚Üí‚Ñùbe weakly* lower semicon-
tinuous and smooth, Œ¶1 ‚à∂V ÓÇ∂‚Üí‚Ñù‚à™{+‚àû}
convex and lower semicontinuous, and let
Œ¶0 + Œ¶1 be coercive. Then (17.9) has a solu-
tion.12)
Introducing the Fr√©chet subdiÔ¨Äerential
ùúïFŒ¶(u) =
{
f ‚ààV ‚àó;
lim inf
‚Äñv‚Äñ‚Üí0
Œ¶(u+v) ‚àíŒ¶(u) ‚àí‚ü®f , v‚ü©
‚Äñv‚Äñ
‚â•0
}
,
(17.11)
12) The proof relies on existence of a minimizer of
Œ¶0 + Œ¶1 as in Theorem 17.1; then one shows
that any such a minimizer satisÔ¨Åes (17.9).

17.2 Abstract Variational Problems
555
the inclusion (17.9) can be written simply
as ùúïFŒ¶(u)‚àã0; in fact, a calculus for Fr√©chet
subdiÔ¨Äerentials can be developed for a
wider class of (sometimes called amenable)
functionals than that considered in (17.9),
cf. [11, 12].
Example 17.1
Let us consider the indica-
tor function ùõøK of a set K ‚äÇV deÔ¨Åned as
ùõøK(u) =
{
0
if u‚ààK,
+‚àû
if u‚àâK.
(17.12)
Clearly, ùõøK is convex or lower semicon-
tinuous if (and only if) K is convex or
closed, respectively. Assuming K convex
closed, it is not diÔ¨Écult to check that
ùúïùõøK(u) = {f ‚ààV ‚àó; ‚àÄv‚ààK ‚à∂‚ü®f , v ‚àíu‚ü©‚â§0}
if u ‚ààK, otherwise ùúïùõøK(u) = ‚àÖ. The set
ùúïùõøK(u) is called the normal cone to K at u;
denoted also by NK(u). For the very special
case Œ¶1 = ùõøK, the variational inequality
(17.10) (i.e. here also Œ¶‚Ä≤
0(u)‚àà‚àíNK(u))
represents the problem of Ô¨Ånding u‚ààK
satisfying
‚àÄv‚ààK ‚à∂
‚ü®
Œ¶‚Ä≤
0(u), v‚àíu
‚ü©
‚â•0.
(17.13)
17.2.3
Constrained Problems
In fact, we saw in Example 17.1 a variational
problem for Œ¶0 with the constraint formed
by a convex set K. Sometimes, there still
is a need to involve constraints of the type
R(u) = 0 (or possibly more general R(u) ‚â§
0) for a nonlinear mapping R ‚à∂V ‚ÜíŒõ with
Œõ a Banach space that is possibly ordered;
we say that Œõ is ordered by ‚Äú ‚â•‚Äù if {ùúÜ‚â•0}
forms a closed convex cone13) in Œõ. Then the
constrained minimization problems reads
as follows:
13) A cone C is a set such that aùúÜ‚ààC whenever
ùúÜ‚ààC and a ‚â•0.
Minimize Œ¶(u) subject to R(u) ‚â§0, u‚ààK.
(17.14)
Let us deÔ¨Åne the tangent cone TK(u) to
K at u as the closure of ‚à™a‚â•0a(K‚àíu).
For A ‚à∂V ‚ÜíŒõ linear continuous, the
adjoint operator A‚àó‚à∂Œõ‚àó‚ÜíV ‚àóis deÔ¨Åned
by
‚ü®A‚àóùúÜ‚àó, u‚ü©= ‚ü®ùúÜ‚àó, Au‚ü©
for
all
ùúÜ‚àó‚ààŒõ‚àó
and u‚ààV. Assuming R to be smooth,
the
Ô¨Årst-order
necessary
optimality
Karush‚ÄîKuhn‚ÄìTucker14) condition takes
the following form:
Theorem 17.7 (First-order condition)
Let u‚ààV solve (17.14) and let15)
‚àÉÃÉu‚ààTK(u) ‚à∂
[R‚Ä≤(u)](ÃÉu) < 0
(17.15)
hold. Then there exists ùúÜ‚àó‚â•
‚àó016) such that17)
‚ü®ùúÜ‚àó, R(u)‚ü©= 0
and
(17.16a)
Œ¶‚Ä≤(u) + R‚Ä≤(u)‚àóùúÜ‚àó+ NK(u) ‚àã0.
(17.16b)
The
condition
(17.15)
is
called
the
Mangasarian‚ÄìFromovitz constraint quali-
Ô¨Åcation,
while
(17.16a)
is
called
the
complementarity (or sometimes orthogo-
nality or transversality) condition and the
triple
R(u) ‚â§0,
ùúÜ‚àó‚â•
‚àó0,
‚ü®ùúÜ‚àó, R(u)‚ü©= 0 (17.17)
is called a complementarity problem. DeÔ¨Ån-
ing the Lagrangean by
‚Ñí(u, ùúÜ‚àó) = Œ¶(u) + ùúÜ‚àó‚àòR(u),
(17.18)
14) Conditions of this kind were Ô¨Årst formulated in
Karush‚Äôs thesis [13] and later independently in
[14].
15) The inequality ‚Äú<‚Äù in (17.15) means that a
neighborhood of [R‚Ä≤(u)](ÃÉu) still lies in the cone
{ùúÜ‚â§0}.
16) The so-called dual ordering ‚â•
‚àóon Œõ‚àómeans
that ùúÜ‚àó‚â•
‚àó0 if, and only if, ‚ü®ùúÜ‚àó, v‚ü©‚â•0 for all v ‚â•
0.
17) The linear operator R‚Ä≤(u)‚àó‚à∂Œõ‚àó‚ÜíV ‚àóis adjoint
to R‚Ä≤(u) ‚à∂V ‚ÜíŒõ and (17.16b) is meant in V ‚àó.

556
17 Calculus of Variations
we can write the inclusion (17.16b) simply
as ‚Ñí‚Ä≤
u(u, ùúÜ‚àó) + NK(u) ‚àã0. The optimality
condition √† la Example 17.1 for maximiza-
tion of ‚Ñí(u, ‚ãÖ) ‚à∂Œõ‚àó‚Üí‚Ñùover the cone
{ùúÜ‚àó‚â•
‚àó0} is simply R(u) ‚â§0.
If R is a convex mapping18) and K is a
convex set, then (17.15) is equivalent to
the simpler Slater constraint qualiÔ¨Åcation:
‚àÉu0 ‚ààK ‚à∂R(u0) < 0. If Œ¶ is also convex,
then (17.16) represents the Ô¨Årst-order suÔ¨É-
cient optimality condition in the sense that
if (17.16) is satisÔ¨Åed, u solves (17.14). More-
over, the couple (u, ùúÜ‚àó) represents a sad-
dle point for ‚Ñíon the set K √ó {ùúÜ‚àó‚â•
‚àó0},
and its existence can be proved by using
Theorem 17.5.
Minimization problems without the con-
straint R(u) ‚â§0 may be much easier to
solve in speciÔ¨Åc cases. In particular, one
can explicitly calculate the value D(ùúÜ‚àó) =
minu‚ààK ‚Ñí(u, ùúÜ‚àó). The functional D ‚à∂Œõ‚àó‚Üí
‚Ñù‚à™{‚àí‚àû} is concave and
maximize D(ùúÜ‚àó) subject to ùúÜ‚àó‚â•
‚àó0 (17.19)
is called the dual problem. The supremum
of (17.19) is always below the inÔ¨Åmum of
(17.14). Under additional conditions, they
can be equal to each other, and (17.19) has
a solution ùúÜ‚àóthat can serve as the multi-
plier for (17.16). For duality theory, see, for
example, [12, Chapter 12].
In the general nonconvex case, (17.16)
is no longer a suÔ¨Écient condition and
construction of such conditions is more
involved. A prototype is a suÔ¨Écient second-
order condition that uses the approximate
critical cone CùúÄ:
CùúÄ(u) = {h‚ààTK(u); Œ¶‚Ä≤(u)h ‚â§ùúÄ‚Äñh‚Äñ,
dist(R‚Ä≤(u)h, T‚àíD(R(u))) ‚â§ùúÄ‚Äñh‚Äñ}
for some ùúÄ> 0:
18) In this Banach-valued case, convexity means
R(ùúÜu+(1‚àíùúÜ)v) ‚â§ùúÜR(u) + (1‚àíùúÜ)R(v) for any
u, v ‚ààV and 0 ‚â§ùúÜ‚â§1 with ‚â§referring to the
ordering in Œõ.
Theorem 17.8 (Second-order condition)
Let Œ¶ and R be twice diÔ¨Äerentiable and let
the Ô¨Årst-order necessary condition (17.16)
with a multiplier ùúÜ‚àó‚â•
‚àó0 hold at some u and
let
‚àÉùúÄ, ùõø> 0 ‚àÄh‚ààCùúÄ(u) ‚à∂
‚Ñí‚Ä≤‚Ä≤
u (u, ùúÜ‚àó)(h, h) ‚â•ùõø‚Äñh‚Äñ2. (17.20)
Then u is a local minimizer for (17.14).
A very special case is when R ‚â°0
and K = V: in this unconstrained case,
NK = {0}, CùúÄ= V, and (17.16) and (17.20)
become,
respectively,
the
well-known
classical condition Œ¶‚Ä≤(u) = 0 and Œ¶‚Ä≤‚Ä≤(u) is
positive deÔ¨Ånite.
17.2.4
Evolutionary Problems
Imposing a linear structure allows us
not only to deÔ¨Åne diÔ¨Äerentials by using
(17.1) and (17.2) but also to deÔ¨Åning the
derivatives du‚àïdt of trajectories t ÓÇ∂‚Üíu(t) ‚à∂
‚Ñù‚ÜíV.
17.2.4.1
Variational Principles
Minimization of the energy Œ¶ is related to
a gradient Ô¨Çow, that is, a process u evolving
in time, governed by the gradient Œ¶‚Ä≤ in the
sense that the velocity du‚àïdt is always in
the direction of steepest descent ‚àíŒ¶‚Ä≤ of Œ¶.
Starting from a given initial condition u0
and generalizing it for a time-dependent
potential Œ¶ ‚àíf (t) with f (t) ‚ààV ‚àó, one
considers
the
initial-value
problem
(a
Cauchy problem) for the abstract parabolic
equation:
du
dt + Œ¶‚Ä≤(u) = f (t),
u(0) = u0.
(17.21)
It is standard to assume V ‚äÇH, with H
a Hilbert space, this embedding being

17.2 Abstract Variational Problems
557
dense19) and continuous. Identifying H with
its own dual, we obtain a Gelfand-triple
V ‚äÇH ‚äÇV ‚àó.
Then,
with
the
coerciv-
ity/growth assumption
‚àÉùúñ> 0 ‚à∂ùúñ‚Äñu‚Äñp
V ‚â§Œ¶(t, u) ‚â§
1+‚Äñu‚Äñp
V
ùúñ
(17.22)
for some 1 < p < +‚àûand Ô¨Åxing a time
horizon T > 0, the solution to (17.21) is
sought in the aÔ¨Éne manifold
{
v‚ààLp(I; V); v(0) = u0, dv
dt ‚ààLp‚Ä≤(I; V ‚àó)
}
(17.23)
with I = [0, T], where Lp(I; V) stands for a
Lebesgue space of abstract functions with
values in a Banach space (here V), which is
called a Bochner space.
By continuation, we obtain a solution
u to (17.21) on [0, +‚àû). If Œ¶ is con-
vex and f is constant in time, there is
a relation to the variational principle
for Œ¶ ‚àíf in Section 17.2.1: the function
t ÓÇ∂‚Üí[Œ¶‚àíf ](u(t)) is nonincreasing and con-
vex, and u(t) converges weakly as t ‚Üí‚àûto
a minimizer of Œ¶‚àíf on V.
The variational principle for (17.21) on
the bounded time interval I uses the func-
tional ùîâdeÔ¨Åned by
ùîâ(u) =‚à´
T
0
Œ¶(t, u(t)) + Œ¶‚àó(
t, f (t) ‚àídu
dt
)
‚àí‚ü®f (t), u(t)‚ü©dt + 1
2‚Äñu(T)‚Äñ2
H, (17.24)
where Œ¶‚àó(t, ‚ãÖ) ‚à∂V ‚àó‚Üí‚Ñù‚à™{+‚àû} is the
Legendre
conjugate
to
Œ¶(t, ‚ãÖ) ‚à∂V ‚Üí
‚Ñù‚à™{+‚àû} deÔ¨Åned by
Œ¶‚àó(t, f ) = sup
v‚ààV
‚ü®f , v‚ü©‚àíŒ¶(v);
(17.25)
19) A subset is dense if its closure is the whole
space, here H.
the construction Œ¶(t, ‚ãÖ) ÓÇ∂‚ÜíŒ¶‚àó(t, ‚ãÖ) is called
the Legendre transformation. Omitting t
for the moment, Œ¶‚àóis convex and
Œ¶‚àó(f ) + Œ¶(v) ‚â•‚ü®f , v‚ü©,
(17.26)
which is Fenchel‚Äôs inequality. If Œ¶, resp.
Œ¶‚àó, is smooth, the equality in (17.26) holds
if, and only if, f ‚ààŒ¶‚Ä≤(v), resp. v‚àà[Œ¶‚àó]‚Ä≤(f ).
Moreover, if Œ¶(‚ãÖ) is lower semicontinuous,
it holds Œ¶‚àó‚àó= Œ¶.
The inÔ¨Åmum of ùîâon (17.24) is equal
to 1
2‚Äñu0‚Äñ2
H. If u from (17.23) minimizes ùîâ
from (17.24), that is, ùîâ(u) = 1
2‚Äñu0‚Äñ2
H, then
u solves the Cauchy problem (17.21); this
is the Brezis‚ÄìEkeland‚ÄìNayroles principle
[15, 16]. It can also be used in the direct
method, see [17] or [18, Section 8.10]:
Theorem 17.9 (Direct method for (17.21))
Let Œ¶ ‚à∂[0, T] √ó V ‚Üí‚Ñùbe a Carath√©odory
function such that Œ¶(t, ‚ãÖ) is convex, both
Œ¶(t, ‚ãÖ) and Œ¶‚àó(t, ‚ãÖ) are smooth, (17.22)
holds, u0 ‚ààH, and f ‚ààL p‚Ä≤(I; V ‚àó). Then
ùîâfrom (17.24) attains a minimum on
(17.23) and the (unique) minimizer solves
the Cauchy problem (17.21).
One can consider another side-condition
instead of the initial condition, for example,
the periodic condition u(0) = u(T), having
the meaning that we are seeking periodic
solutions with an a priori prescribed period
T assuming f is periodic with the period T.
Instead of (17.21), one thus considers
du
dt + Œ¶‚Ä≤(u) = f (t),
u(0) = u(T).
(17.27)
Then, instead of (17.23), solutions are
sought in the linear (in fact, Banach) space
{
v‚ààLp(I; V); v(0) = v(T),
dv
dt ‚ààLp‚Ä≤(I; V ‚àó)
}
.
(17.28)

558
17 Calculus of Variations
The direct method now uses, instead of
(17.24), the functional
ùîâ(u) =‚à´
T
0
Œ¶(t, u(t)) ‚àí‚ü®f (t), u(t)‚ü©
+ Œ¶‚àó(
t, f (t) ‚àídu
dt
)
dt,
(17.29)
and an analog of Theorem 17.9 but using
(17.28) and (17.29); the minimum is 0
and the minimizer need not be unique, in
general.
Often,
physical
and
mechanical
applications use a convex (in general,
nonquadratic)
potential
of
dissipative
forces Œ® ‚à∂H ‚Üí‚Ñù‚à™{+‚àû} leading to a
doubly nonlinear Cauchy problem:
Œ®‚Ä≤(du
dt
) + Œ¶‚Ä≤(u) = f (t),
u(0) = u0. (17.30)
In fact, the hypothesis that (here abstract)
dissipative forces, say A(du‚àïdt), have a
potential needs a symmetry of A, cf. (17.5),
which has been under certain conditions
justiÔ¨Åed in continuum-mechanical (even
anisothermal)
linearly
responding
sys-
tems (so that the resulting Œ® is quadratic);
this is Onsager‚Äôs (or reciprocal) symmetry
condition [19],20) cf. [20, Section 12.3].
Sometimes, (17.30) is also equivalently
written as
du
dt = [Œ®‚àó]‚Ä≤(f (t)‚àíŒ¶‚Ä≤(u)),
u(0) = u0,
(17.31)
where
Œ®‚àó
again
denotes
the
conju-
gate functional, that is, here Œ®‚àó(v‚àó) =
supv‚ààH‚ü®v‚àó, v‚ü©‚àíŒ®(v). If Œ® is also proper in
the sense that Œ® > ‚àí‚àûand Œ® ‚â¢+‚àû, then
[Œ®‚àó]‚Ä≤ = [Œ®‚Ä≤]‚àí1, which was used in (17.31).
For Œ® = 1
2‚Äñ ‚ãÖ‚Äñ2
H, we get du‚àïdt = f ‚àíŒ¶‚Ä≤(u),
20) A Nobel prize was awarded to Lars Onsager in
1968 ‚Äúfor the discovery of the reciprocal rela-
tions bearing his name, which are fundamental
for the thermodynamics of irreversible pro-
cesses.‚Äù
cf. (17.21). Thus, for f = 0, (17.31) rep-
resents a generalized gradient Ô¨Çow. For a
general f , a Stefanelli‚Äôs variational principle
[21] for (17.30) employs the functional
ùîâ(u, w) =
(
‚à´
T
0
Œ®
(du
dt
)
‚àí
‚ü®
f , du
dt
‚ü©
+ Œ®‚àó(w) dt + Œ¶(u(T)) ‚àíŒ¶(u0)
)+
+ ‚à´
T
0
Œ¶(u) ‚àí‚ü®f ‚àíw, u‚ü©+ Œ¶‚àó( f ‚àíw) dt
(17.32)
to be minimized on the aÔ¨Éne manifold
{
(u, w)‚ààL‚àû(I; V);
u(0) = u0,
du
dt ‚ààLq(I; H), w‚ààLq‚Ä≤(I; H)
}
, (17.33)
where 1 < q < +‚àûrefers to a coerciv-
ity/growth condition for Œ®. On the set
(17.33), ùîâ(u, w) ‚â•0 always holds, and
ùîâ(u, w) = 0 means that w=Œ®‚Ä≤(du‚àïdt) and
f ‚àíw=Œ¶‚Ä≤(u) a.e. (almost everywhere) on I,
that is, u solves (17.30).
Another option is to use the conjugation
and Fenchel inequality only for Œ®, which
leads to21)
ùîä(u) = ‚à´
T
0
Œ®(du
dt
) + Œ®‚àó(f ‚àíŒ¶‚Ä≤(u))
+
‚ü®df
dt , u
‚ü©
dt + Œ¶(u(T))
(17.34)
to be minimized on a submanifold {u = w}
of (17.33). The inÔ¨Åmum is Œ¶(u0) ‚àíf (0) +
f (T) and any minimizer u is a solution to
(17.30). Sometimes, this is known under the
name principle of least dissipation, cf. [22]
for Œ® quadratic. The relation
ùîä(u) = Œ¶(u0) ‚àíf (0) + f (T)
(17.35)
21) Here (17.26) reads as Œ®(du‚àïdt) +
Œ®‚àó(f ‚àíŒ¶‚Ä≤(u)) ‚â•‚ü®f ‚àíŒ¶‚Ä≤(u), du‚àïdt‚ü©=
‚ü®f , du‚àïdt‚ü©‚àí[dŒ¶‚àïdt](u), from which (17.34)
results by integration over [0, T].

17.2 Abstract Variational Problems
559
is sometimes called De Giorgi‚Äôs formulation
of (17.30); rather than for existence proofs
by the direct method, this formulation is
used for various passages to a limit. Note
that for f constant, the only time deriva-
tive involved in (17.34) is Œ®(du‚àïdt), which
allows for an interpretation even if V is
only a metric space and thus du‚àïdt itself
is not deÔ¨Åned, which leads to a theory
of gradient Ô¨Çows in metric spaces, cf. [23,
Theorem 2.3.3].
A combination of (17.27) and (17.30)
leads to
Œ®‚Ä≤
(
du
dt
)
+ Œ¶‚Ä≤(u) = f (t),
u(T) = u(0),
and the related variational principle uses ùîâ
from (17.32) but with Œ¶(u(T))‚àíŒ¶(u0) omit-
ted, to be minimized on the linear manifold
(17.33) with u0 replaced by u(T).
Many physical systems exhibit oscillatory
behavior combined possibly with attenu-
ation by nonconservative forces having a
(pseudo)potential Œ®, which can be covered
by the (Cauchy problem for the) abstract
second-order evolution equation
ùíØ‚Ä≤ d2u
dt2 + Œ®‚Ä≤
(
du
dt
)
+ Œ¶‚Ä≤(u) = f (t),
u(0) = u0,
du
dt (0) = v0, (17.36)
where
ùíØ‚à∂H ‚Üí‚Ñù
is
the
positive
(semi)deÔ¨Ånite quadratic form representing
the kinetic energy. The Hamilton varia-
tional principle extended to dissipative
systems says that the solution u to (17.36)
is a critical point of the integral functional
‚à´
T
0
ùíØ
(
du
dt
)
‚àíŒ¶(u) + ‚ü®f ‚àíùî£, u‚ü©dt (17.37)
with a nonconservative force ùî£= Œ®‚Ä≤(du‚àïdt)
considered Ô¨Åxed on the aÔ¨Éne manifold
{u‚ààL‚àû(I; V); du‚àïdt‚ààL‚àû(I; H), d2u‚àïdt2 ‚àà
L2(I; V ‚àó), u(0) = u0, du‚àïdt = v0}, cf. [24].
17.2.4.2
Evolution Variational
Inequalities
For
nonsmooth
potentials,
the
above
evolution equations turn into inclusions.
Instead of the Legendre transformation,
we speak about the Legendre‚ÄìFenchel
transformation. For example, instead of
[Œ®‚àó]‚Ä≤ = [Œ®‚Ä≤]‚àí1,
we
have
ùúïŒ®‚àó= [ùúïŒ®]‚àí1.
Note that variational principles based on
ùîâfrom (17.24), (17.29), or (17.32) do not
involve any derivatives of Œ¶ and Œ® and are
especially designed for nonsmooth prob-
lems, and also ùîäfrom (17.34) allows for Œ®
to be nonsmooth. For example, in the case
of (17.30), with convex but nonsmooth
Œ® and Œ¶, we have the doubly nonlinear
inclusion
ùúïŒ®
(
du
dt
)
+ùúïŒ¶(u)‚àãf (t), u(0) = u0, (17.38)
and ùîâ(u, w) = 0 in (17.32) and (17.33)
means exactly that w‚ààùúïŒ®(du‚àïdt) and
f ‚àíw‚ààùúïŒ¶(u) hold a.e. on I,22) which (in the
spirit of Section 17.2.2) can be written as a
system of two variational inequalities for u
and w:
‚àÄv ‚à∂
Œ®(v) +
‚ü®
w, v‚àídu
dt
‚ü©
‚â•Œ®
(
du
dt
)
,
(17.39a)
‚àÄv ‚à∂
Œ¶(v) + ‚ü®f ‚àíw, v ‚àíu‚ü©‚â•Œ¶(u).
(17.39b)
For a systematic treatment of such multiply
nonlinear inequalities, see [25].
In applications, the nonsmoothness of
Œ® occurs typically at 0 describing activa-
tion phenomena: the abstract driving force
f ‚àíùúïŒ¶(u) must pass a threshold, that is, the
boundary of the convex set ùúïŒ®(0), in order
to trigger the evolution of u. Often, any
22) The idea behind the principle in (17.32) and
(17.33) is to apply two Fenchel inequalities to
(17.38) written as w‚ààùúïŒ®(du‚àïdt) and f ‚àíw‚àà
ùúïŒ¶(u).

560
17 Calculus of Variations
rate dependence is neglected, and then Œ® is
degree-1 positively homogeneous.23) In this
kind of rate-independent case, Œ®‚àó= ùõøùúïŒ®(0),
while Œ® = ùõø‚àó
ùúïŒ®(0), and the De Giorgi formu-
lation (17.35) leads to the energy equality
E(T, u(T)) + ‚à´
T
0
Œ®
(
du
dt
)
dt
= E(0, u0) ‚àí‚à´
T
0
‚ü®df
dt , u
‚ü©
dt
for E(t, u) = Œ¶(u) ‚àí‚ü®f (t), u‚ü©
(17.40a)
together with f (t)‚àíŒ¶‚Ä≤(u(t))‚ààùúïŒ®(0) for a.a.
(almost all) t‚àà[0, T]; here, in accordance
with (17.35), we assume Œ¶ to be smooth for
the moment. This inclusion means Œ®(v) ‚àí
‚ü®f ‚àíŒ¶‚Ä≤(u), v‚ü©‚â•Œ®(0) = 0 and, as Œ¶ is con-
vex, we obtain the stability condition,24)
‚àÄt‚àà[0, T] ‚àÄv‚ààV ‚à∂
E(t, u(t)) ‚â§E(t, v) + Œ®(v‚àíu(t)).
(17.40b)
Moreover, in this rate-independent case,
ùúïŒ®‚àó= NùúïŒ®(0)
and
(17.31)
reads
du‚àï
dt ‚ààNùúïŒ®(0)(f ‚àíŒ¶‚Ä≤(u)). By (17.13), it means
that
‚ü®du‚àïdt, v ‚àíf + Œ¶‚Ä≤(u)‚ü©‚â§0
for
any
v‚ààùúïŒ®(0), that is,
max
v‚ààùúïŒ®(0)
‚ü®du
dt , v
‚ü©
=
‚ü®du
dt , f ‚àíŒ¶‚Ä≤(u)
‚ü©
,
(17.41)
which says that the dissipation due to the
driving force f ‚àíŒ¶‚Ä≤(u) is maximal com-
pared to all admissible driving forces pro-
vided the rate du‚àïdt is kept Ô¨Åxed; this is the
maximum dissipation principle.
In fact, (17.40) does not contain Œ¶‚Ä≤ and
thus works for Œ¶ convex nonsmooth, too.
Actually, (17.40) was invented in [26],
where it is called the energetic formulation
of (17.38), cf. also [27].
23) This means Œ®(ùúÜv) = ùúÜŒ®(v) for any ùúÜ‚â•0.
24) By convexity of Œ¶, we have Œ¶(v) ‚â•
Œ¶(u) + ‚ü®Œ¶‚Ä≤(u), v‚àíu‚ü©, and adding it with
Œ®(v‚àíu) ‚àí‚ü®f ‚àíŒ¶‚Ä≤(u), v‚àíu‚ü©‚â•0, we get (17.40b).
17.2.4.3
Recursive Variational Problems
Arising by Discretization in Time
The variational structure related to the
potentials
of
Section 17.2.4.1
can
be
exploited not only for formulation of
‚Äúglobal‚Äù
in
time-variational
principles,
but, perhaps even more eÔ¨Éciently, to
obtain recursive (incremental) variational
problems when discretizing the abstract
evolution problems in time by using some
(semi) implicit formulae. This can serve as
an eÔ¨Écient theoretical method for analyz-
ing evolution problems (the Rothe method,
[28]) and for designing eÔ¨Écient conceptual
algorithms for numerical solution of such
problems.
Considering a uniform partition of the
time interval with the time step ùúè> 0 with
T‚àïùúèinteger, we discretize (17.21) as
uk
ùúè‚àíuk‚àí1
ùúè
ùúè
+ Œ¶‚Ä≤(uk
ùúè) = f (kùúè),
k = 1, ‚Ä¶ , T
ùúè,
u0
ùúè= u0.
(17.42)
This is also known as the implicit Euler
formula and uk
ùúèfor k = 1, ‚Ä¶ , T‚àïùúèapproxi-
mate respectively the values u(kùúè). One can
apply the direct method by employing the
recursive variational problem for the func-
tional
Œ¶(u) + 1
2ùúè
‚Äñ‚Äñu‚àíuk‚àí1
ùúè
‚Äñ‚Äñ
2
H ‚àí‚ü®f (kùúè), u‚ü©(17.43)
to be minimized for u. Obviously, any criti-
cal point u (and, in particular, a minimizer)
of this functional solves (17.42) and we put
u = uk
ùúè. Typically, after ensuring existence
of the approximate solutions {uk
ùúè}T‚àïùúè
k=1, a pri-
ori estimates have to be derived25) and then
convergence as ùúè‚Üí0 is to be proved by
25) For this, typically, testing (17.42) (or its dif-
ference from k‚àí1 level) by uk
ùúèor by uk
ùúè‚àíuk‚àí1
ùúè
(or uk
ùúè‚àí2uk‚àí1
ùúè
+ uk‚àí2
ùúè
) is used with Young‚Äôs and
(discrete) Gronwall‚Äôs inequalities, and so on.

17.2 Abstract Variational Problems
561
various methods.26) Actually, Œ¶ does not
need to be smooth and, referring to (17.11),
we can investigate the set-valued varia-
tional inclusion du‚àïdt+ùúïFŒ¶(u) ‚àãf .
In speciÔ¨Åc situations, the fully implicit
scheme (17.42) can be advantageously
modiÔ¨Åed in various ways. For example, in
case Œ¶ = Œ¶1 + Œ¶2 and f = f1 + f2, one can
apply the fractional-step method, alterna-
tively to be understood as a Lie‚ÄìTrotter
(or sequential) splitting combined with the
implicit Euler formula:
uk‚àí1‚àï2
ùúè
‚àíuk‚àí1
ùúè
ùúè
+ Œ¶‚Ä≤
1(uk‚àí1‚àï2
ùúè
) = f1(kùúè), (17.44a)
uk
ùúè‚àíuk‚àí1‚àï2
ùúè
ùúè
+ Œ¶‚Ä≤
2(uk
ùúè) ‚àãf2(kùúè),
(17.44b)
with k = 1, ‚Ä¶ , T‚àïùúè. Clearly, (17.44) leads
to two variational problems that are to be
solved in alternation.
Actually, we have needed rather the
splitting
of
the
underlying
operator
A = Œ¶‚Ä≤
1 + Œ¶‚Ä≤
2 ‚à∂V ‚ÜíV ‚àó
and not of its
potential
Œ¶ = Œ¶1 + Œ¶2 ‚à∂V ‚Üí‚Ñù.
In
case Œ¶ ‚à∂V = Y √ó Z ‚Üí‚Ñù, u = (y, z) and
f = (g, h) where (17.21) represents a system
of two equations
dy
dt + Œ¶‚Ä≤
y(y, z) = g,
y(0) = y0,
(17.45a)
dz
dt + Œ¶‚Ä≤
z(y, z) = h,
z(0) = z0,
(17.45b)
with Œ¶‚Ä≤
y and Œ¶‚Ä≤
z denoting partial diÔ¨Äer-
entials, one can thus think also about
the
splitting
Œ¶‚Ä≤‚àíf = (Œ¶‚Ä≤
y‚àíg, Œ¶‚Ä≤
z‚àíh) =
(Œ¶‚Ä≤
y‚àíg, 0) + (0, Œ¶‚Ä≤
z‚àíh).
Then
the
frac-
tional method such as (17.44) yields a
semi-implicit scheme27)
26) Typically, a combination of the arguments
based on weak lower semicontinuity or com-
pactness is used.
27) Indeed, in (17.44), one has uk‚àí1
ùúè
= (yk‚àí1
ùúè
, zk‚àí1
ùúè
),
uk‚àí1‚àï2
ùúè
= (yk
ùúè, zk‚àí1
ùúè
), and eventually uk
ùúè= (yk
ùúè, zk
ùúè).
yk
ùúè‚àíyk‚àí1
ùúè
ùúè
+ Œ¶‚Ä≤
y(yk
ùúè, zk‚àí1
ùúè
) = g(kùúè),
(17.46a)
zk
ùúè‚àízk‚àí1
ùúè
ùúè
+ Œ¶‚Ä≤
z(yk
ùúè, zk
ùúè) = h(kùúè), (17.46b)
again for k = 1, ‚Ä¶ , T‚àïùúè. Note that the use
of zk‚àí1
ùúè
in (17.46a) decouples the system
(17.46), in contrast to the fully implicit
formula which would use zk
ùúèin (17.46a)
and would not decouple the original
system
(17.45).
The
underlying
varia-
tional problems for the functionals y ÓÇ∂‚Üí
Œ¶(y, zk‚àí1
ùúè
) + 1
2ùúè‚Äñy‚àíyk‚àí1
ùúè
‚Äñ2 ‚àí‚ü®g(kùúè), y‚ü©
and
z ÓÇ∂‚ÜíŒ¶(yk
ùúè, z) + 1
2ùúè‚Äñz‚àízk‚àí1
ùúè
‚Äñ2 ‚àí‚ü®h(kùúè), z‚ü©
represent
recursive
alternating
varia-
tional problems; these particular problems
can be convex even if Œ¶ itself is not;
only separate convexity28) of Œ¶ suÔ¨Éces.
Besides, under certain relatively weak con-
ditions, this semi-implicit discretization is
‚Äúnumerically‚Äù stable; cf. [18, Remark 8.25].
For
a
convex/concave
situation
as
in
Theorem 17.5, (17.46) can be understood
as an iterative algorithm of Uzawa‚Äôs type
(with a damping by the implicit formula)
for Ô¨Ånding a saddle point.29)
Of course, this decoupling method can
be advantageously applied to nonsmooth
situations and for u with more than two
components, that is, for systems of more
than two equations or inclusions. Even
more, the splitting as in (17.45) may yield
a variational structure of the decoupled
incremental problems even if the original
problem of the type du‚àïdt + A(u) ‚àã0 itself
does not have it. An obvious example for
this is A(y, z) = (Œ¶‚Ä≤
1(‚ãÖ, z)](y) , Œ¶‚Ä≤
2(y, ‚ãÖ)](z)),
which does not need to satisfy the sym-
metry (17.5) if Œ¶1 ‚â†Œ¶2
although the
corresponding
semi-implicit
scheme
28) This means that only Œ¶(y, ‚ãÖ) and Œ¶(‚ãÖ, z) are con-
vex but not necessarily Œ¶(‚ãÖ, ‚ãÖ).
29) This saddle point is then a steady state of the
underlying evolution system (17.45).

562
17 Calculus of Variations
(17.46) still possesses a ‚Äúbi-variational‚Äù
structure.
Similarly to (17.42), the doubly nonlinear
problem (17.38) uses the formula
ùúïŒ®
(uk
ùúè‚àíuk‚àí1
ùúè
ùúè
)
+ ùúïŒ¶(uk
ùúè) ‚àãf (kùúè) (17.47)
and, instead of (17.43), the functional
Œ¶(u) + ùúèŒ®
(u‚àíuk‚àí1
ùúè
ùúè
)
‚àí‚ü®f (kùúè), u‚ü©. (17.48)
Analogously, for the second-order dou-
bly
nonlinear
problem
(17.36)
in
the
nonsmooth case, that is, ùíØ‚Ä≤d2u‚àïdt2+
ùúïŒ®
(
du‚àïdt
)
+ ùúïŒ¶(u) ‚àãf (t), we would use
ùíØ‚Ä≤ uk
ùúè‚àí2uk‚àí1
ùúè
+ uk‚àí2
ùúè
ùúè2
+ ùúïŒ®
(uk
ùúè‚àíuk‚àí1
ùúè
ùúè
)
+ ùúïŒ¶(uk
ùúè) ‚àãf (kùúè)
(17.49)
and the recursive variational problem for
the functional
Œ¶(u) + ùúèŒ®
(u‚àíuk‚àí1
ùúè
ùúè
)
‚àí‚ü®f (kùúè), u‚ü©
+ ùúè2ùíØ
(u‚àí2uk‚àí1
ùúè
+uk‚àí2
ùúè
ùúè2
)
.
(17.50)
The fractional-step method and, in par-
ticular, various semi-implicit variants of
(17.47) and (17.49) are widely applicable,
too.
17.3
Variational Problems on SpeciÔ¨Åc Function
Spaces
We
now
use
the
abstract
framework
from Section 17.2 for concrete variational
problems formulated on speciÔ¨Åc function
spaces.
17.3.1
Sobolev Spaces
For this, we consider a bounded domain
Œ© ‚äÇ‚Ñùd equipped with the Lebesgue mea-
sure, having a smooth boundary Œì ‚à∂= ùúïŒ©.
For 1‚â§p<‚àû, we will use the standard nota-
tion
Lp(Œ©; ‚Ñùn) =
{
u ‚à∂Œ© ‚Üí‚Ñùn measurable;
‚à´Œ©
|u(x)|p dx < ‚àû
}
for the Lebesgue space; the addition and
the multiplication understood pointwise
makes it a linear space, and introducing the
norm
‚Äñ‚Äñu‚Äñ‚Äñp =
(
‚à´Œ©
|u(x)|p dx
)1‚àïp
makes it a Banach space. For p = ‚àû,
we
deÔ¨Åne
‚Äñu‚Äñ‚àû= ess supx‚ààŒ©|u(x)| =
infN‚äÇŒ©, measd(N)=0 supx‚ààŒ©‚ßµN |u(x)|. For
1<p<‚àû,
Lp(Œ©; ‚Ñùn)
is
reÔ¨Çexive.
For
1‚â§p<‚àû,
Lp(Œ©; ‚Ñùn)‚àó= Lp‚Ä≤(Œ©; ‚Ñùn)
with
p‚Ä≤ = p‚àï(p ‚àí1) if the duality is deÔ¨Åned
naturally as ‚ü®f , u‚ü©= ‚à´Œ© f (x) ‚ãÖu(x) dx. For
p = 2, Lp(Œ©; ‚Ñùn) becomes a Hilbert space.
For n = 1, we write for short Lp(Œ©) instead
of Lp(Œ©; ‚Ñù).
Denoting the kth order gradient of u
by ‚àáku = (ùúïk‚àïùúïxi1 ¬∑ ¬∑ ¬∑ ùúïxiku)1‚â§i1,‚Ä¶,ik‚â§d, we
deÔ¨Åne the Sobolev space by
W k,p(Œ©; ‚Ñùn) =
{
u‚ààLp(Œ©; ‚Ñùn);
‚àáku‚ààLp(Œ©; ‚Ñùn√ódk)
}
,
normed by ‚Äñ‚Äñu‚Äñ‚Äñk,p =
p‚àö
‚Äñu‚Äñp
p + ‚Äñ‚àáku‚Äñp
p .
If n = 1, we will again use the shorthand
notation W k,p(Œ©). If p = 2, W k,p(Œ©; ‚Ñùn)
is a Hilbert space and we will write
Hk(Œ©; ‚Ñùn) = W k,2(Œ©; ‚Ñùn).
Moreover,
we
occasionally use a subspace of W k,p(Œ©; ‚Ñùn)

17.3 Variational Problems on SpeciÔ¨Åc Function Spaces
563
with vanishing traces on the boundary Œì,
denoted by
W k,p
0 (Œ©; ‚Ñùn) = {u‚ààW k,p(Œ©; ‚Ñùn);
‚àálu = 0 on Œì, l = 0, ‚Ä¶ , k‚àí1}.
(17.51)
To give a meaningful interpretation to
traces ‚àálu on Œì, this boundary has to be suf-
Ô¨Åciently regular; roughly speaking, piece-
wise Cl+1 is enough.
We denote by Ck(Œ©) the space of smooth
functions whose gradients up to the order k
are continuous on the closure Œ© of Œ©. For
example, we have obviously embeddings
Ck(Œ©) ‚äÇW k,p(Œ©) ‚äÇLp(Œ©); in fact, these
embeddings are dense.
An important phenomenon is the com-
pactifying eÔ¨Äect of derivatives. A prototype
for it is the Rellich‚ÄìKondrachov theorem,
saying that H1(Œ©) is compactly30) embedded
into L2(Œ©). More generally, we have
Theorem 17.10 (Compact embedding)
For the Sobolev critical exponent
p‚àó
‚éß
‚é™
‚é®
‚é™‚é©
= dp‚àï(d‚àíp)
for p < d,
‚àà[1, +‚àû) arbitrary for p = d,
= +‚àû
for p > d,
the embedding W 1,p(Œ©) ‚äÇLp‚àó‚àíùúñ(Œ©) is com-
pact for any 0 < ùúñ‚â§p‚àó‚àí1.
Iterating this theorem, we can see, for
example, that, for p < d‚àï2, the embedding
W 2,p(Œ©) ‚äÇL[p‚àó]‚àó‚àíùúñ(Œ©) is compact; note that
[p‚àó]‚àó= dp‚àï(d ‚àí2p).
Another important fact is the compact-
ness of the trace operator u ÓÇ∂‚Üíu|Œì:
Theorem 17.11 (Compact trace operator)
For the boundary critical exponent
30) This means that the embedding is a compact
mapping in the sense that weakly converging
sequences in H1(Œ©) converge strongly in L2(Œ©).
p‚ôØ
‚éß
‚é™
‚é®
‚é™‚é©
= (dp‚àíp)‚àï(d‚àíp)
for p < d,
‚àà[1, +‚àû) arbitrary for p = d,
= +‚àû
for p > d,
the trace operator u ÓÇ∂‚Üíu|Œì ‚à∂W 1,p(Œ©) ‚äÇ
Lp‚ôØ‚àíùúñ(Œì) is compact for any 0 < ùúñ‚â§p‚ôØ‚àí1.
For example, the trace operator from
W 2,p(Œ©) is compact into L[p‚àó]‚ôØ‚àíùúñ(Œì).31)
17.3.2
Steady-State Problems
The above abstract functional-analysis sce-
nario gives a lucid insight into concrete
variational problems leading to boundary-
value problems for quasilinear equations in
divergence form which is what we will now
focus on. We consider a bounded domain
Œ© ‚äÇ‚Ñùd with a suÔ¨Éciently regular boundary
Œì divided into two disjoint relatively open
parts ŒìD and ŒìN whose union is dense in
Œì. An important tool is a generalization of
the superposition operator, the NemytskiÀòi
mapping ùí©a, induced by a Carath√©odory32)
mapping a ‚à∂Œ© √ó ‚Ñùn ‚Üí‚Ñùm by prescribing
[ùí©a(u)](x) = a(x, u(x)).
Theorem 17.12 (NemytskiÀòi mapping)
Let a ‚à∂Œ© √ó ‚Ñùn ‚Üí‚Ñùm be a Carath√©odory
mapping and p, q ‚àà[1, ‚àû). Then ùí©a maps
Lp(Œ©; ‚Ñùn) into Lq(Œ©; ‚Ñùm) and is continu-
ous if, and only if, for some ùõæ‚ààLq(Œ©) and
C < ‚àû, we have that
||a(x, u)|| ‚â§ùõæ(x) + C||u||
p‚àïq.
31) To see this, we use Theorem 17.10 to obtain
W 2,p(Œ©) ‚äÇW 1,p‚àó‚àíùúñ(Œ©), and then Theorem 17.11
with p‚àó‚àíùúñin place of p.
32) The Carath√©odory property means measurabil-
ity in the x-variable and continuity in all other
variables.

564
17 Calculus of Variations
17.3.2.1
Second Order Systems of
Equations
First, we consider the integral functional
Œ¶(u) = ‚à´Œ©
ùúë(x, u, ‚àáu) dx +‚à´ŒìN
ùúô(x, u) dS
(17.52a)
involving Carath√©odory integrands ùúë‚à∂
Œ©√ó‚Ñùn√ó‚Ñùn√ód ‚Üí‚Ñù
and
ùúô‚à∂ŒìN√ó‚Ñùn ‚Üí‚Ñù.
The functional Œ¶ is considered on an aÔ¨Éne
closed manifold
{u‚ààW 1,p(Œ©; ‚Ñùn); u|ŒìD = uD
}
(17.52b)
for a suitable given uD; in fact, existence
of uD ‚ààW 1,p(Œ©; ‚Ñùn) such uD = uD|ŒìD
is
to be required. Equipped with the theory
of W 1,p-Sobolev spaces,33) one considers
a
p-polynomial-type
coercivity
of
the
highest-order term and the corresponding
growth restrictions on the partial deriva-
tives ùúë‚Ä≤
F, ùúë‚Ä≤
u, and ùúô‚Ä≤
u with some 1 < p < ‚àû,
that is,
ùúë(x, u, F)‚à∂F ‚â•ùúñ||F||
p + ||u||
ùúñ‚àí1
ùúñ,
(17.53a)
‚àÉùõæ‚ààLp‚Ä≤(Œ©) ‚à∂||ùúë‚Ä≤
F(x, u, F)|| ‚â§ùõæ(x)
+ C||u||
(p‚àó‚àíùúñ)‚àïp‚Ä≤+ C||F||
p‚àí1,
(17.53b)
‚àÉùõæ‚ààLp‚àó‚Ä≤(Œ©) ‚à∂||ùúë‚Ä≤
u(x, u, F)|| ‚â§ùõæ(x)
+ C||u||
p‚àó‚àí1‚àíùúñ+ C||F||
p‚àïp‚àó‚Ä≤
,
(17.53c)
‚àÉùõæ‚ààLp‚ôØ‚Ä≤
(Œì) ‚à∂||ùúô‚Ä≤
u(x, u)||
‚â§ùõæ(x) + C||u||
p‚ôØ‚àí1‚àíùúñ
(17.53d)
for some ùúñ> 0 and C < ‚àû; we used F
as a placeholder for ‚àáu. A generaliza-
tion
of
Theorem 17.12
for
NemytskiÀòi
mappings
of
several
arguments
says
that
(17.53b)
ensures
just
continuity
of
ùí©ùúë‚Ä≤
F ‚à∂Lp‚àó‚àíùúñ(Œ©; ‚Ñùn) √ó Lp(Œ©; ‚Ñùn√ód) ‚Üí
33) More general nonpolynomial growth and coer-
civity conditions would require the theory of
Orlicz spaces instead of the Lebesgue ones, cf.
[9, Chapter 53].
Lp‚Ä≤(Œ©; ‚Ñùn√ód), and analogously also (17.53c)
works for ùí©ùúë‚Ä≤
u, while (17.53d) gives conti-
nuity of ùí©ùúô‚Ä≤
u ‚à∂Lp‚ôØ‚àíùúñ(Œì; ‚Ñùn) ‚ÜíLp‚ôØ‚Ä≤(Œì; ‚Ñùn).
This, together with Theorems 17.10 and
17.11, reveals the motivation for the growth
conditions (17.53b‚Äìd).
For ùúñ‚â•0, (17.53b‚Äìd) ensures that the
functional Œ¶ from (17.52a) is G√¢teaux dif-
ferentiable on W 1,p(Œ©; ‚Ñùn). The abstract
Euler‚ÄìLagrange equation (17.3) then leads
to the integral identity
‚à´Œ©
ùúë‚Ä≤
‚àáu(x, u, ‚àáu)‚à∂‚àáv + ùúë‚Ä≤
u(x, u, ‚àáu)‚ãÖv dx
+ ‚à´ŒìN
ùúô‚Ä≤
u(x, u)‚ãÖv dS = 0
(17.54)
for any v ‚ààW 1,p(Œ©; ‚Ñùn) such that v|ŒìD = 0;
the notation ‚Äú ‚à∂‚Äù or ‚Äú ‚ãÖ‚Äù means summation
over two indices or one index, respectively.
Completed by the Dirichlet condition on
ŒìD, this represents a weak formulation of
the boundary-value problem for a sys-
tem of second-order elliptic quasilinear
equations:34)
div ùúë‚Ä≤
‚àáu(u, ‚àáu) = ùúë‚Ä≤
u(u, ‚àáu) in Œ©,
(17.55a)
ùúë‚Ä≤
‚àáu(u, ‚àáu)‚ãÖ‚Éón + ùúô‚Ä≤
u(u) = 0 on ŒìN, (17.55b)
u||Œì = uD on ŒìD, (17.55c)
where x-dependence has been omitted
for notational simplicity. The conditions
(17.55b) and (17.55c) are called the Robin
and the Dirichlet boundary conditions,
respectively, and (17.55) is called the clas-
sical formulation of this boundary-value
problem.
Any
u ‚ààC2(Œ©; ‚Ñùn)
satisfying
(17.55) is called a classical solution, while
u ‚ààW 1,p(Œ©; ‚Ñùn)
satisfying
(17.54)
for
any v ‚ààW 1,p(Œ©; ‚Ñùn) such that v|ŒìD = 0 is
34) Assuming suÔ¨Éciently smooth data as well as u,
this can be seen by multiplying (17.55a) by v,
using the Green formula ‚à´Œ©(div a)v + a‚ãÖv dx =
‚à´Œì(a ‚ãÖ‚Éón)v dS, and using v = 0 on ŒìD and the
boundary conditions (17.55b) on ŒìN.

17.3 Variational Problems on SpeciÔ¨Åc Function Spaces
565
called a weak solution; note that much less
smoothness is required for weak solutions.
Conversely, taking general Carath√©odory
integrands a ‚à∂Œ©√ó‚Ñùn√ó‚Ñùn√ód ‚Üí‚Ñùn√ód, b ‚à∂
ŒìN√ó‚Ñùn ‚Üí‚Ñùn, and c ‚à∂Œ©√ó‚Ñùn√ó‚Ñùn√ód ‚Üí‚Ñùn,
one can consider a boundary-value prob-
lem for a system of second-order elliptic
quasilinear equations
div a(u, ‚àáu) = c(u, ‚àáu) in Œ©,
(17.56a)
a(u, ‚àáu)‚ãÖ‚Éón + b(u) = 0 on ŒìN,
(17.56b)
u|ŒìD = uD on ŒìD.
(17.56c)
Such a problem does not need to be
induced by any potential Œ¶; nevertheless, it
possesses a weak formulation as in (17.54),
namely, ‚à´Œ© a(u, ‚àáu)‚à∂‚àáv + c(u, ‚àáu) ‚ãÖv dx+
‚à´ŒìN b(u)‚ãÖv dS = 0
for
any
‚Äúvariation‚Äù
v
as in (17.54), and related methods are
sometimes called variational in spite of
absence of a potential Œ¶. The existence of
such a potential requires a certain sym-
metry corresponding to that in (17.5)
for
the
underlying
nonlinear
opera-
tor
A ‚à∂W 1,p(Œ©; ‚Ñùn) ‚ÜíW 1,p(Œ©; ‚Ñùn)‚àó
given
by
‚ü®A(u), v‚ü©= ‚à´Œ© a(u, ‚àáu)‚à∂‚àáv +
c(u, ‚àáu)‚ãÖv dx + ‚à´ŒìN b(u)‚ãÖv dS, namely,
ùúïail(x, u, F)
ùúïFjk
=
ùúïakj(x, u, F)
ùúïFli
,
(17.57a)
ùúïail(x, u, F)
ùúïuj
=
ùúïcj(x, u, F)
ùúïFli
,
(17.57b)
ùúïcj(x, u, F)
ùúïul
= ùúïcl(x, u, F)
ùúïuj
(17.57c)
for all i, k = 1, ‚Ä¶ , d and j, l = 1, ‚Ä¶ , n and for
a.a. (x, u, F)‚ààŒ©√ó‚Ñùn√ó‚Ñùn√ód, and also
ùúïbj(x, u)
ùúïul
= ùúïbl(x, u)
ùúïuj
.
(17.57d)
for all j, l = 1, ‚Ä¶ , n and for a.a. (x, u) ‚àà
Œì √ó ‚Ñùn. Note that (17.57a‚Äìc) just means a
symmetry for the Jacobian of the map-
ping
(F, u) ÓÇ∂‚Üí(a(x, u, F), c(x, u, F)) ‚à∂
‚Ñùn√ód √ó ‚Ñùd ‚Üí‚Ñùn√ód √ó ‚Ñùd
while
(17.57d)
is the symmetry for the Jacobian of
b(x, ‚ãÖ) ‚à∂‚Ñùn ‚Üí‚Ñùn.
Then (17.6) leads to the formula (17.52a)
with
ùúë(x, u, F) = ‚à´
1
0
a(x, ùúÜu, ùúÜF)‚à∂F
+ c(x, ùúÜu, ùúÜF)‚ãÖu dùúÜ, (17.58a)
ùúô(x, u) = ‚à´
1
0
b(x, ùúÜu)‚ãÖu dùúÜ.
(17.58b)
Relying on the minimization-of-energy
principle described above, which is often
a
governing
principle
in
steady-state
mechanical and physical problems, and on
Theorem 17.1 or 17.2, one can prove exis-
tence of weak solutions to the boundary-
value problem by the direct method; cf. e.g.
[29‚Äì32]. Theorem 17.2 imposes a strong
(although
often
applicable)
structural
restriction that ùúë(x, ‚ãÖ, ‚ãÖ) ‚à∂‚Ñùn√ó‚Ñùn√ód ‚Üí‚Ñù
and ùúô(x, ‚ãÖ) ‚à∂‚Ñùn ‚Üí‚Ñùare convex for a.a. x.
Yet, in general, Theorem 17.1 places
fewer restrictions on ùúëand ùúôby requiring
only weak lower semicontinuity of Œ¶. The
precise condition (i.e., suÔ¨Écient and neces-
sary) that guarantees such semicontinuity
of u ÓÇ∂‚Üí‚à´Œ© ùúë(x, u, ‚àáu) dx on W 1,p(Œ©; ‚Ñùn) is
called W 1,p-quasiconvexity, deÔ¨Åned in a
rather nonexplicit way by requiring
‚àÄx‚ààŒ© ‚àÄu‚àà‚Ñùn ‚àÄF ‚àà‚Ñùn√ód ‚à∂ùúë(x, u, F) =
=
inf
v‚ààW1,p
0
(O;‚Ñùd) ‚à´O
ùúë(x, u, F+‚àáv(ùúâ))
measd(O)
dùúâ,
where O ‚äÇ‚Ñùd is an arbitrary smooth
domain. This condition cannot be ver-
iÔ¨Åed eÔ¨Éciently except for very special
cases, unlike, for example, polyconvexity
which is a (strictly) stronger condition.
Subsequently,
another
type
of
con-
vexity,
called
rank-one
convexity,
was
introduced by Morrey [33] by requiring

566
17 Calculus of Variations
ùúÜÓÇ∂‚Üíùúë(x, u, F+ùúÜa‚äób) ‚à∂‚Ñù‚Üí‚Ñùto be con-
vex for any a‚àà‚Ñùd, b‚àà‚Ñùn, [a‚äób]ij = aibj.
For smooth ùúë(x, u, ‚ãÖ), rank-one convexity
is equivalent to the Legendre‚ÄìHadamard
condition ùúë‚Ä≤‚Ä≤
FF(x, u, F)(a‚äób, a‚äób) ‚â•0 for
all a‚àà‚Ñùn
and b‚àà‚Ñùd. Since Morrey‚Äôs
[33] introduction of quasiconvexity, the
question of its coincidence with rank-one
convexity had been open for many decades
and eventually answered negatively by
≈†ver√°k [34] at least if n ‚â•3 and d ‚â•2.
Weak lower semicontinuity of the bound-
ary integral u ÓÇ∂‚Üí‚à´Œ© ùúô(x, u) dS in (17.52a)
does not entail any special structural con-
dition because one can use compactness
of the trace operator, cf. Theorem 17.11.
Here, Theorem 17.1 leads to the following
theorem:
Theorem 17.13 (Direct method) Let
(17.53) hold with ùúñ> 0, let ùúë(x, u, ‚ãÖ) be qua-
siconvex, and let uD ‚ààW 1‚àí1‚àïp,p(ŒìD; ‚Ñùn).35)
Then (17.54) has a solution, that is, the
boundary-value problem (17.55) has a
weak solution.
For n = d, an example for a quasiconvex
function is ùúë(x, u, F) = ùî£(x, u, F, det F) with
a convex function ùî£(x, u, ‚ãÖ, ‚ãÖ) ‚à∂‚Ñùd√ód √ó ‚Ñù‚Üí
‚Ñù. The weak lower semicontinuity of
Œ¶ from (17.52a) is then based on the
weak continuity of the nonlinear map-
ping induced by det ‚à∂‚Ñùd√ód √ó ‚Ñù‚Üí‚Ñùif
restricted to gradients, that is,
uk ‚Üíu weakly in W 1,p(Œ©; ‚Ñùd)
‚áí
det‚àáuk ‚Üídet‚àáu weakly in Lp‚àïd(Œ©),
(17.59)
which holds for p > d; note that nonaÔ¨Éne
mappings on Lebesgue spaces such as G ÓÇ∂‚Üí
35) Without going into detail concerning the so-
called Sobolev‚ÄìSlobodetskiÀòi spaces with frac-
tional derivatives, this condition means exactly
that uD allows an extension onto Œ© belonging
to W 1,p(Œ©; ‚Ñùn).
det G
with
G ‚ààLp(Œ©; ‚Ñùd√ód) ‚ÜíLp‚àïd(Œ©)
can be continuous36) but not weakly con-
tinuous, so (17.59) is not entirely trivial.
Even less trivial, it holds for p = d locally
(i.e., in L1(K) for any compact K ‚äÇŒ©) if
det‚àáuk ‚â•0.37) Invented by Ball [36], such
functions ùúë(x, u, ‚ãÖ) are called polyconvex,
and in general this property requires
ùúë(x, u, F) = ùî£
(
x, u, (adjiF)min(n,d)
i=1
)
(17.60)
for some ùî£‚à∂Œ© √ó ‚Ñùn √ó ‚àèmin(n,d)
i=1
‚ÑùùúÖ(i,n,d) ‚Üí
‚Ñù‚à™{‚àû} such that ùî£(x, u, ‚ãÖ) is convex,
where ùúÖ(i, n, d) is the number of all
minors
of
the
ith
order
and
where
adjiF
denotes
the
determinants
of
all
(i√ói)-submatrices.
Similarly,
as
in
(17.59), we have that adji‚àáuk ‚Üíadji‚àáu
weakly
in
Lp‚àïi(Œ©; ‚ÑùùúÖ(i,n,d))
provided
p > i ‚â§min(n, d),
and
Theorem 17.13
directly applies if ùî£from (17.60) gives ùúë
satisfying (17.53a‚Äìc).
Yet, this special structure allows for much
weaker restrictions on ùúëif one is concerned
with the minimization of Œ¶ itself rather
than the satisfaction of the Euler‚ÄìLagrange
equation (17.54):
Theorem 17.14 (Direct method, poly-
convex) Let ùúëbe a normal integrand38)
satisfying
(17.53a)
with
ùúë(x, u, ‚ãÖ) ‚à∂
‚Ñùn√ód ‚Üí‚Ñù‚à™{‚àû}
polyconvex,
and
let
uD ‚ààW 1‚àí1‚àïp,p(ŒìD; ‚Ñùn). Then the minimiza-
tion problem (17.52) has a solution.
Obviously,
polyconvexity
(and
thus
also
quasi-
and
rank-one
convexity)
is weaker than usual convexity. Only
for min(n, d) = 1, all mentioned modes
coincide with usual convexity of ùúë(x, u, ‚ãÖ).
36) For p ‚â•d, Theorem 17.12 gives this continuity.
37) Surprisingly, not only {det‚àáuk}k‚àà‚Ñïbut even
{det‚àáuk ln(2+det‚àáuk)}k‚àà‚Ñïstays bounded in
L1(K), as proved by S. M√ºller in [35].
38) This means ùúëis measurable but ùúë(x, ‚ãÖ, ‚ãÖ) is only
lower semicontinuous.

17.3 Variational Problems on SpeciÔ¨Åc Function Spaces
567
Example 17.2 [Oscillation eÔ¨Äects.] A sim-
ple one-dimensional counterexample for
nonexistence of a solution due to oscillation
eÔ¨Äects is based on
Œ¶(u) = ‚à´
L
0
((du
dx
)2‚àí1
)2
+ u2 dx
(17.61)
to
be
minimized
for
u ‚ààW 1,4([0, L]).
A minimizing sequence {uk}k‚àà‚Ñïis, for
example,39)
uk(0) = 1
k , duk
dx =
{
1 if sin(kx)>0,
‚àí1 otherwise. (17.62)
Then Œ¶(uk) = ùí™(1‚àïk2) ‚Üí0 for k‚Üí‚àû,
so that inf Œ¶ = 0. Yet, there is no u such
that Œ¶(u) = 0.40) We can observe that
Theorem 17.1 (resp. Theorem 17.2) cannot
be used due to lack of weak lower semicon-
tinuity (resp. convexity) of Œ¶ which is due to
nonconvexity of the double-well potential
density F ÓÇ∂‚Üíùúë(x, u, F) = (|F|2‚àí1)2 + u2; cf.
also (17.105) below for a ‚ÄúÔ¨Åne limit‚Äù of the
fast oscillations from Figure 17.1.
Example 17.3 [Concentration eÔ¨Äects.] The
condition that V in Theorems 17.1 and
17.2 has a pre-dual, is essential. A sim-
ple one-dimensional counterexample for
nonexistence of a solution in the situation
where V is not reÔ¨Çexive and even does not
have any pre-dual, is based on
Œ¶(u) =‚à´
1
‚àí1
(1+x2)|||
du
dx
||| dx
+ (u(‚àí1)+1)2 + (u(1)‚àí1)2
(17.63)
for u ‚ààW 1,1([‚àí1, 1]). If u were a minimizer,
then u must be nondecreasing (otherwise,
39) Actually, uk(0) ‚â†0 was used in (17.62) only for
a better visualization on Figure 17.1.
40) Indeed, then both ‚à´L
0 ((du‚àïdx)2‚àí1)2dx and
‚à´L
0 u2dx would have to be zero, so that u = 0,
but then also du‚àïdx = 0, which however
contradicts ‚à´L
0 ((du‚àïdx)2‚àí1)2dx = 0.
it obviously would not be optimal), and we
can always take some ‚Äúpart‚Äù of the nonneg-
ative derivative of this function and add
the corresponding area in a neighborhood
of 0. This does not aÔ¨Äect u(¬±1) but makes
‚à´1
‚àí1(1+x2)|du‚àïdx| dx lower, contradicting
the original assumption that u is a min-
imizer. In fact, as 1+x2 in (17.63) attains
its minimum only at a single point x = 0,
any minimizing sequence {uk}k‚àà‚Ñïis forced
to concentrate its derivative around x = 0.
For example considering, for k ‚àà‚Ñïand
ùìÅ‚àà‚Ñù, the sequence given by
uk(x) =
ùìÅkx
1 + k|x|
(17.64)
yields
Œ¶(uk) = 2ùìÅ+ 2(ùìÅ‚àí1)2 + ùí™(1‚àïk2).
Obviously,
the
sequence
{uk}k‚àà‚Ñï
will
minimize
Œ¶
provided
ùìÅ= 1‚àï2;
then
limk‚Üí‚àûŒ¶(uk) = 3‚àï2 = inf Œ¶;
see
Figure 17.2. On the other hand, this value
inf Œ¶ cannot be achieved, otherwise such u
must have simultaneously |du‚àïdx| = 0 a.e.
and u(¬±1) = ¬±1‚àï2, which is not possible.41)
A
similar
eÔ¨Äect
occurs
for
ùúë(F) =
‚àö
1 + |F|2 for which ‚à´Œ© ùúë(‚àáu) dx is the
area of the parameterized hypersurface
{(x, u(x)); x ‚ààŒ©} in ‚Ñùd+1. Minimization of
such a functional is known as the Plateau
variational
problem.
Hyper-surfaces
of
minimal area typically do not exists in
W 1,1(Œ©), especially if Œ© is not convex and
the concentration of the gradient typically
occurs on Œì rather than inside Œ©, cf. e.g.
[37, Chapter V].
Example 17.4 [Lavrentiev
phenomenon.]
Coercivity in Theorems 17.1 and 17.2 is also
essential even if Œ¶ is bounded from below.
An
innocent-looking
one-dimensional
41) This is because of the concentration
eÔ¨Äect. More precisely, the sequence
{duk‚àïdx}k‚àà‚Ñï‚äÇL1(‚àí1, 1) is not uniformly
integrable.

568
17 Calculus of Variations
u
u1
u2
u4
u8
etc.
Œ©
L = 6œÄ
0
Figure 17.1
A minimizing sequence (17.62) for Œ¶ from
(17.61) whose gradient exhibits faster and faster spatial
oscillations.
u
‚Ñì
‚àí1
1
0
‚àí‚Ñì
u15
etc.
u7
u3
u1
Œ©
Figure 17.2
A minimizing sequence (17.64) for Œ¶ from
(17.63) whose gradient concentrates around the point x = 0
inside Œ©.
counterexample for nonexistence of a solu-
tion in the situation where V is reÔ¨Çexive
and Œ¶ ‚â•0 is continuous and weakly lower
semicontinuous is based on
Œ¶(u) =‚à´
1
0
(u3‚àíx)2(du
dx
)6
dx
subject to u(0) = 0, u(1) = 1,
(17.65)
for u ‚ààW 1,6([0, 1]) = V. The minimum
of (17.65) is obviously 0, being realized
on
u(x) = x1‚àï3.
Such
u ‚ààW 1,1([0, 1]),
however, does not belong to W 1,6([0, 1])
because |du‚àïdx|6 = 3‚àí6x‚àí4 is not inte-
grable owing to its singularity at x = 0.
Thus (17.65) attains the minimum on
W 1,p([0, 1]) with 1 ‚â§p < 3‚àï2 although Œ¶
is not (weakly lower semi-) continuous
and even not Ô¨Ånite on this space, and
thus abstract Theorem 17.1 cannot be
used. A surprising and not entirely obvious
phenomenon is that the inÔ¨Åmum (17.65)
on W 1,6([0, 1]) is positive, that is, greater
than the inÔ¨Åmum on W 1,p([0, 1]) with
p < 3‚àï2; this eÔ¨Äect was Ô¨Årst observed in
[38], cf. also, e.g. [1, Section 4.3.]. Note
that W 1,6([0, 1]) is dense in W 1,p([0, 1])
but one cannot rely on Œ¶(uk) ‚ÜíŒ¶(u) if
uk ‚Üíu in W 1,p([0, 1]) for p < 6; it can even
happen that Œ¶(u) = 0 while Œ¶(uk) ‚Üí‚àû
for uk ‚Üíu, a repulsive eÔ¨Äect, cf. [39,
Section 7.3]. Here ùúë(x, u, ‚ãÖ) is not uniformly
convex, yet the Lavrentiev phenomenon
can occur even for uniformly convex ùúë‚Äôs,
cf. [40].
17.3.2.2
Fourth Order Systems
Higher-order problems can be considered
analogously but the complexity of the prob-
lem grows with the order. Let us therefore
use for illustration fourth-order problems
only, governed by an integral functional

17.3 Variational Problems on SpeciÔ¨Åc Function Spaces
569
Œ¶(u) = ‚à´Œ©
ùúë(x, u, ‚àáu, ‚àá2u) dx
+ ‚à´ŒìN
ùúô(x, u, ‚àáu) dS
(17.66a)
involving
Carath√©odory
integrands
ùúë‚à∂Œ©√ó‚Ñùn√ó‚Ñùn√ód√ó‚Ñùn√ód√ód ‚Üí‚Ñù
and
ùúô‚à∂ŒìN√ó‚Ñùn√ó‚Ñùn√ód ‚Üí‚Ñù. The functional Œ¶ is
considered on an aÔ¨Éne closed manifold
{
u‚ààW 2,p(Œ©; ‚Ñùn); u|ŒìD = uD,1,
ùúïu
ùúï‚Éón|ŒìD = uD,2
}
(17.66b)
for a given suitable uD,1 and uD,2. Instead
of (17.54), the abstract Euler‚ÄìLagrange
equation (17.3) now leads to the integral
identity:
‚à´Œ©
(
ùúë‚Ä≤
‚àá2u(x, u, ‚àáu, ‚àá2u)‚ãÆ‚àá2v
+ ùúë‚Ä≤
‚àáu(x, u, ‚àáu, ‚àá2u)‚à∂‚àáv
+ ùúë‚Ä≤
u(x, u, ‚àáu, ‚àá2u)‚ãÖv
)
dx
+ ‚à´ŒìN
(
ùúô‚Ä≤
‚àáu(x, u, ‚àáu)‚ãÖùúïv
ùúï‚Éón
+ ùúô‚Ä≤
u(x, u, ‚àáu)‚ãÖv
)
dS = 0
(17.67)
for any v‚ààW 2,p(Œ©; ‚Ñùn) such that v|ŒìD= 0
and ùúïu‚àïùúï‚Éón|ŒìD= 0; the notation ‚Äú ‚ãÆ‚Äù stands
for summation over three indices. Com-
pleted by the Dirichlet conditions on ŒìD,
this represents a weak formulation of the
boundary-value problem for a system of
fourth-order elliptic quasilinear equations
div2 ùúë‚Ä≤
‚àá2u(u, ‚àáu, ‚àá2u)
‚àídiv ùúë‚Ä≤
‚àáu(u, ‚àáu, ‚àá2u)
+ ùúë‚Ä≤
u(u, ‚àáu, ‚àá2u) = 0 in Œ©,
(17.68a)
with two natural (although quite compli-
cated) boundary conditions prescribed on
each part of the boundary, namely,
(div ùúë‚Ä≤
‚àá2u(u,‚àáu,‚àá2u)‚àíùúë‚Ä≤
‚àáu(u,‚àáu,‚àá2u))‚ãÖ‚Éón
+ divS
(ùúë‚Ä≤
‚àá2u(u, ‚àáu, ‚àá2u)‚Éón)
‚àí
(
divS‚Éón
)(‚Éón‚ä§ùúë‚Ä≤
‚àá2u(u, ‚àáu, ‚àá2u)‚Éón)
+ ùúô‚Ä≤
‚àáu(u, ‚àáu) = 0
on ŒìN, (17.68b)
ùúë‚Ä≤
‚àá2u(u, ‚àáu, ‚àá2u)‚à∂(‚Éón‚äó‚Éón)
+ ùúô‚Ä≤
u(u, ‚àáu) = 0
on ŒìN, (17.68c)
u||Œì = uD,1,
ùúïu
ùúï‚Éón
|||Œì = uD,2
on ŒìD. (17.68d)
Again,
(17.68)
is
called
the
classical
formulation
of
the
boundary-value
problem in question, and its derivation
from (17.67) is more involved than in
Section 17.3.2.1. One must use a general
decomposition ‚àáv = ùúïv‚àïùúï‚Éón + ‚àáSv on Œì with
‚àáSv = ‚àáv ‚àíùúïv‚àïùúï‚Éón being the tangential gra-
dient of v. On a smooth boundary Œì, one
can use another (now (d‚àí1)-dimensional)
Green-type formula on tangent spaces:42)
‚à´Œì
a‚à∂(‚Éón‚äó‚àáv) dS
= ‚à´Œì
(
‚Éón‚ä§a‚Éón
) ùúïv
ùúï‚Éón + a‚à∂(‚Éón‚äó‚àáSv) dS
= ‚à´Œì
(
‚Éón‚ä§a‚Éón
) ùúïv
ùúï‚Éón ‚àídivS
(a‚Éón)v
+ (divS‚Éón
)(‚Éón‚ä§a‚Éón
)
v dS,
(17.69)
where
a = ùúë‚Ä≤
‚àá2u(x, u, ‚àáu, ‚àá2u)
and
divS = tr(‚àáS) with tr(‚ãÖ) being the trace
of a (d‚àí1)√ó(d‚àí1)-matrix, denotes the
(d‚àí1)-dimensional surface divergence so
that divS‚Éón is (up to a factor ‚àí1‚àï2) the mean
curvature of the surface Œì. Comparing the
variational formulation as critical points of
(17.66) with the weak formulation (17.67)
and with the classical formulation (17.68),
one can see that although formally all for-
mulations are equivalent to each other, the
advantage of the variational formulations
such as (17.66) in its simplicity is obvious.
42) This ‚Äúsurface‚Äù Green-type formula reads
‚à´Œì w‚à∂((‚àáSv)‚äó‚Éón) dS = ‚à´Œì(divS‚Éón)(w‚à∂(‚Éón‚äó‚Éón))v ‚àí
divS(w‚ãÖ‚Éón)v dS.

570
17 Calculus of Variations
As in (17.56), taking general Carath√©o-
dory integrands a ‚à∂Œ©√ó‚Ñùn√ó‚Ñùn√ód√ó‚Ñùn√ód√ód
‚Üí
‚Ñùn√ód√ód,
b ‚à∂Œ©√ó‚Ñùn√ó‚Ñùn√ód√ó‚Ñùn√ód√ód ‚Üí
‚Ñùn√ód,
c ‚à∂Œ©√ó‚Ñùn√ó‚Ñùn√ód√ó‚Ñùn√ód√ód ‚Üí‚Ñùn,
d ‚à∂ŒìN√ó‚Ñùn√ó‚Ñùn√ód ‚Üí‚Ñùn√ód,
and
Ô¨Ånally
e ‚à∂ŒìN√ó‚Ñùn√ó‚Ñùn√ód ‚Üí‚Ñùn, one can consider
a boundary-value problem for a system of
fourth-order elliptic quasilinear equations:
div2a(u, ‚àáu, ‚àá2u) ‚àídiv b(u, ‚àáu, ‚àá2u)
+ c(u, ‚àáu, ‚àá2u) = 0
in Œ©, (17.70a)
with the boundary conditions (17.68d) and
a(u, ‚àáu, ‚àá2u)‚à∂(‚Éón‚äó‚Éón)
+ d(u, ‚àáu) = 0
on ŒìN,
(17.70b)
(div a(u,‚àáu,‚àá2u)‚àíb(u,‚àáu,‚àá2u))‚ãÖ‚Éón
+ divS
(a(u, ‚àáu, ‚àá2u)‚Éón)
‚àí
(
divS‚Éón
)(‚Éón‚ä§a(u, ‚àáu, ‚àá2u)‚Éón)
+ e(u, ‚àáu) = 0
on ŒìN.
(17.70c)
Existence of a potential for which the
boundary-value problem (17.70) would
represent the Euler‚ÄìLagrange equation
(17.3)
requires
the
symmetry
for
the
Jacobians
of
the
mapping
(H, F, u) ÓÇ∂‚Üí
(a(x, u, F, H), b(x, u, F, H), c(x, u, F, H)) ‚à∂
‚Ñùn√ód√ód√ó‚Ñùn√ód√ó‚Ñùd‚Üí‚Ñùn√ód√ód√ó‚Ñùn√ód√ó‚Ñùd
and of the mapping (F, u) ÓÇ∂‚Üí(d(x, u, F),
e(x, u, F))‚à∂‚Ñùn√ód√ó‚Ñùd‚Üí‚Ñùn√ód√ó‚Ñùd; we used
H as a placeholder for ‚àá2u. The formula
(17.58) then takes the form:
ùúë(x, u, F, H) = ‚à´
1
0
a(x, ùúÜu, ùúÜF, ùúÜH)‚ãÆH
+ b(x, ùúÜu, ùúÜF, ùúÜH)‚à∂F
+ c(x, ùúÜu, ùúÜF, ùúÜH)‚ãÖu dùúÜ,
(17.71a)
ùúô(x, u, F) = ‚à´
1
0
d(x, ùúÜu, ùúÜF)‚à∂F
+ e(x, ùúÜu, ùúÜF)‚ãÖu dùúÜ.
(17.71b)
Analogously
to
Theorem 17.13,
one
can obtain existence of weak solutions
by the direct method under a suitable
coercivity/growth conditions on ùúëand an
analogue of quasiconvexity of ùúë(x, u, F, ‚ãÖ) ‚à∂
‚Ñùn√ód√ód ‚Üí‚Ñù,
and
uD,1 ‚ààW 2‚àí1‚àïp,p(ŒìD; ‚Ñùn)
and uD,2 ‚ààW 1‚àí1‚àïp,p(ŒìD; ‚Ñùn).
So far, we considered two Dirichlet-type
conditions (17.68d) on ŒìD (dealing with
zeroth- and Ô¨Årst-order derivatives) and
two Robin-type conditions (17.68b,c) on
ŒìN (dealing with second- and third-order
derivatives). These arise either by Ô¨Åxing
both u|Œì or ùúïu‚àïùúï‚Éón|Œì or neither of them,
cf. (17.66b). One can, however, think
about Ô¨Åxing only u|Œì or only ùúïu‚àïùúï‚Éón|Œì,
which gives other two options of natural
boundary conditions, dealing with zeroth-
and second-order derivatives or Ô¨Årst- and
third-order derivatives, respectively.
The other two combinations, namely the
zeroth- and the third-order derivatives or
the Ô¨Årst- and the second-order derivatives,
are not natural from the variational view-
point because they overdetermine some of
the two boundary terms arising in the weak
formulation (17.67).
17.3.2.3
Variational Inequalities
Merging
the
previous
Sections 17.3.2.1
‚Äì17.3.2.2
with
the
abstract
scheme
from
Sections 17.2.2‚Äì17.2.3,
has
important applications. Let us use as
illustration
Œ¶0 = Œ¶
from
(17.52)
and
Œ¶1(v) = ‚à´Œ© ùúÅ(v) dx + ‚à´ŒìN ùúâ(v) dS
as
in
Remark 17.1,
now
with
some
convex
ùúÅ, ùúâ‚à∂‚Ñùn ‚Üí‚Ñù‚à™{+‚àû}. In view of the
abstract inequality (17.10), the weak for-
mulation
(17.54)
gives
the
variational
inequality
‚à´Œ©
ùúÅ(v) + ùúë‚Ä≤
‚àáu(x, u, ‚àáu)‚à∂‚àá(v‚àíu)
+ ùúë‚Ä≤
u(x, u, ‚àáu)‚ãÖ(v‚àíu) dx
+ ‚à´Œì
ùúâ(v) + ùúô‚Ä≤
u(x, u)‚ãÖ(v‚àíu) dS

17.3 Variational Problems on SpeciÔ¨Åc Function Spaces
571
‚â•‚à´Œ©
ùúÅ(u) dx + ‚à´Œì
ùúâ(u) dS
(17.72)
for any v ‚ààW 1,p(Œ©; ‚Ñùn) such that v|ŒìD = 0.
The passage from the weak formulation to
the classical boundary-value problem anal-
ogous to (17.54) ‚àí‚Üí(17.55) leads to the dif-
ferential inclusion on Œ©:
div ùúë‚Ä≤
‚àáu(u, ‚àáu) ‚àíùúë‚Ä≤
u(u, ‚àáu) ‚àíùúïùúÅ(u) ‚àã0
in Œ©,
(17.73a)
with another diÔ¨Äerential inclusion in the
boundary conditions
ùúë‚Ä≤
‚àáu(u, ‚àáu)‚ãÖ‚Éón + ùúô‚Ä≤
u(u) + ùúïùúâ(u) ‚àã0
on ŒìN,
(17.73b)
u||Œì = uD
on ŒìD. (17.73c)
There is an extensive literature on mathe-
matical methods in variational inequalities,
cf. e.g. [18, 41‚Äì44].
17.3.3
Some Examples
Applications
of
the
previous
general
boundary-value problems to more spe-
ciÔ¨Åc situations in continuum physics are
illustrated in the following examples.
17.3.3.1
Nonlinear Heat-Transfer Problem
The
steady-state
temperature
distri-
bution
ùúÉ
in
an
anisotropic
nonlinear
heat-conductive body Œ© ‚äÇ‚Ñù3 is described
by the balance law
div j = f
with
j = ‚àíùúÖ(ùúÉ)ùïÇ‚àáùúÉ
on Œ©,
(17.74a)
‚Éón‚ãÖj + b(ùúÉ) = g
on Œì,
(17.74b)
where b(‚ãÖ)>0 is a boundary heat outÔ¨Çow,
g the external heat Ô¨Çux, f the bulk heat
source, and with the heat Ô¨Çux j governed
by the Fourier law involving a symmet-
ric positive deÔ¨Ånite matrix ùïÇ‚àà‚Ñùd√ód and
a
nonlinearity
ùúÖ‚à∂‚Ñù‚Üí‚Ñù+.
In
terms
of a and c in (17.56a), we have n = 1,
a(x, u, F) = ùúÖ(u)ùïÇF
and
c(x, u, F) = f (x)
and the symmetry (17.57b) fails, so that
(17.74) does not have the variational
structure unless ùúÖis constant. Yet, a sim-
ple rescaling of ùúÉ, called the KirchhoÔ¨Ä
transformation, can help: introducing the
substitution
u = ÃÇùúÖ(ùúÉ) = ‚à´ùúÉ
0 ùúÖ(ùúó) dùúó,
we
have j = ‚àíùïÇ‚àáu and (17.74) transforms to
div(ùïÇ‚àáu) + f = 0
on Œ©, (17.75a)
‚Éón‚ä§ùïÇ‚àáu + b(ÃÇùúÖ‚àí1(u)) = g
on Œì,
(17.75b)
which already Ô¨Åts in the framework of
(17.52)
with
ùúë(x, u, F) = 1
2F‚ä§ùïÇF ‚àíf (x)u
and ùúô(x, u) = ÃÉb(u) ‚àíg(x)u where ÃÉb is a
primitive of b ‚àòÃÇùúÖ‚àí1. Eliminating the non-
linearity from the bulk to the boundary, we
thus gain a variational structure at least if
f ‚ààL6‚àï5(Œ©) and g ‚ààL4‚àï3(Œì) 43) and thus, by
the direct method, we obtain existence of
a solution u ‚ààH1(Œ©) to (17.75) as well as a
possibility of its eÔ¨Écient numerical approx-
imation, cf. Section 17.4.1; then ùúÉ= ÃÇùúÖ‚àí1(u)
yields a solution to (17.74). Our optimism
should however be limited because, in the
heat-transfer context, the natural integra-
bility of the heat sources is only f ‚ààL1(Œ©)
and g ‚ààL1(Œì), but this is not consistent with
the variational structure if d>1:
Example 17.5 [Nonexistence of minimiz-
ers] Consider the heat equation ‚àídiv ‚àáu =
0 for d = 3 and with, for simplicity, zero
Dirichlet boundary conditions, so that the
underlying variational problem is to mini-
mize ‚à´Œ©
1
2|‚àáu|2‚àífudx on H1
0(Œ©). Yet,
43) If d = 3, this integrability of the heat sources
is necessary and suÔ¨Écient to ensure the func-
tional (u ÓÇ∂‚Üí‚à´Œ© fu dx + ‚à´Œì gu dS) to belong to
H1(Œ©)‚àó.

572
17 Calculus of Variations
inf
u‚ààH1
0 (Œ©) ‚à´Œ©
1
2|‚àáu|2 ‚àífu dx = ‚àí‚àû,
(17.76)
whenever f ‚ààL1(Œ©)‚ßµL6‚àï5(Œ©).44)
17.3.3.2
Elasticity at Large Strains
A prominent application of multidimen-
sional (d > 1) vectorial (n > 1) variational
calculus is to elasticity under the hypothesis
that the stress response on the deformation
gradient is a gradient of some potential;
such materials are called hyperelastic. The
problem is very diÔ¨Écult especially at large
strains where the geometry of the stan-
dard Cartesian coordinates may be totally
incompatible with the largely deformed
geometry of the specimen Œ©.
Here, u will stand for the deformation
(although the usual notation is rather y) and
we will consider n = d and ùúë= ùúë(F) tak-
ing possibly also the value +‚àû. The ultimate
requirement is frame indiÔ¨Äerence, that is,
ùúë(RF) = ùúë(F) for all R ‚àà‚Ñùd√ód in the spe-
cial orthogonal group SO(d). One could
try to rely on Theorem 17.13 or 17.14. In
the former case, one can consider non-
polyconvex materials and one also has the
Euler‚ÄìLagrange equation (17.54) at one‚Äôs
disposal, but the growth condition (17.53b)
does not allow an inÔ¨Ånite increase of energy
when the volume of the material is locally
shrunk to 0, that is, we cannot satisfy the
condition
ùúë(F) ‚Üí+‚àû
if det F ‚Üí0+
(17.77a)
ùúë(F) = +‚àû
if det F ‚â§0.
(17.77b)
An example for a polyconvex frame-
indiÔ¨Äerent ùúësatisfying (17.77) is the Ogden
material
44) The proof is very simple: f ‚àâH1
0(Œ©)‚àómeans
‚Äñf ‚ÄñH1
0 (Œ©)‚àó= supu‚ààW 1,‚àû
0
(Œ©), ‚Äñu‚Äñ1,2‚â§1 ‚à´Œ© fu dx = +‚àû,
which further means ‚à´Œ© fuk dx ‚Üí+‚àûfor some
uk ‚ààW 1,‚àû
0
(Œ©) such that ‚à´Œ©
1
2 |‚àáuk|2 dx ‚â§1
2 , so
that ‚à´Œ©
1
2 |‚àáuk|2 ‚àífuk dx ‚Üí‚àí‚àû.
ùúë(F) = a1tr(F‚ä§F)b1
+ a2|tr(cof(F‚ä§F))|b2 + ùõæ(det F) (17.78)
with
a1, a2, b1, b2 > 0
and
ùõæ‚à∂‚Ñù+ ‚Üí‚Ñù
convex such that ùõæ(ùõø) = +‚àûfor ùõø‚â§0
and
limùõø‚Üí0+ ùõæ(ùõø) = +‚àû,
and
where
cof(A) = (detA)A‚ä§, considering d = 3, and
where tr A = ‚àëd
i=1 Aii. Particular Ogden
materials are Mooney‚ÄìRivlin materials
with ùúë(F) = |F|2 + |det F|2 ‚àíln(det F) or
compressible neo-Hookean materials with
ùúë(F) = a|F|2 + ùõæ(det F). The importance
of polyconvexity is that the existence of
energy-minimizing deformation can be
based on Theorem 17.14, which allows us
to handle local nonpenetration
det(‚àáu) > 0
a.e. on Œ©
(17.79a)
involved in ùúëvia (17.77). Handling of non-
penetration needs also the global condition
‚à´Œ©
det(‚àáu) dx ‚â§measd
(u(Œ©));
(17.79b)
(17.79) is called the Ciarlet‚ÄìNeÀácas nonpen-
etration condition [45]. Whether it can deal
with quasiconvex but not polyconvex mate-
rials remains an open question, however
cf. [46].
An
interesting
observation
is
that
polyconvex materials allow for an energy-
controlled stress |ùúë‚Ä≤(F)F‚ä§| ‚â§C(1 + ùúë(F))
even though the so-called KirchhoÔ¨Ästress
ùúë‚Ä≤(F)F‚ä§itself does not need to be bounded.
This can be used, for example, in sensi-
tivity analysis and to obtain modiÔ¨Åed
Euler‚ÄìLagrange equations to overcome
the possible failure of (17.54) for such
materials, cf. [46]. It is worth noting that
even such spatially homogeneous, frame-
indiÔ¨Äerent,
and
polyconvex
materials
can exhibit the Lavrentiev phenomenon,
cf. [47].
Another
frequently
used
ansatz
is
just a quadratic form in terms of the

17.3 Variational Problems on SpeciÔ¨Åc Function Spaces
573
Green‚ÄìLagrange strain tensor
E = 1
2F‚ä§F ‚àí1
2ùïÄ.
(17.80)
An
example
is
an
isotropic
material
described by only two elastic constants;
in terms of the Lam√© constants ùúáand ùúÜ, it
takes the form
ùúë(F) = 1
2ùúÜ|trE|2 + ùúá|E|2,
(17.81)
and is called a St.Venant‚ÄìKirchhoÔ¨Ä‚Äôs mate-
rial, cf. [48, Volume I, Section 4.4]. If ùúá> 0
and ùúÜ> ‚àí2
dùúá, ùúëfrom (17.81) is coercive in
the sense of ùúë(F) ‚â•ùúñ0|F|4 ‚àí1‚àïùúñ0 for some
ùúñ0 > 0 but not quasiconvex (and even not
rank-one convex), however. Therefore, exis-
tence of an energy-minimizing deforma-
tion in W 1,4(Œ©; ‚Ñùd) is not guaranteed.
A way to improve solvability for non-
quasiconvex materials imitating additional
interfatial-like energy is to augment ùúë
with
some
small
convex
higher-order
term,
for
example,
ùúë1(F, H) = ùúë(F)
+
‚àëd
i,k,l,m,n=1‚Ñçklmn(x)HiklHimn with ‚Ñça (usu-
ally
only
small)
fourth-order
positive
deÔ¨Ånite tensor, and to employ the fourth-
order framework of Section 17.3.2.2. This
is the idea behind the mechanics of com-
plex (also called nonsimple or a special
micropolar) continua, cf. e.g., [49].
17.3.3.3
Small-Strain Elasticity, Lam√©
System, Signorini Contact
Considering
the
deformation
y
and
displacement
u(x) = y(x) ‚àíx,
we
write
F = ‚àáy = ‚àáu + ùïÄand, for |‚àáu| small, the
tensor E from (17.80) is
E = F‚ä§F‚àíùïÄ
2
= (‚àáu)‚ä§+‚àáu+(‚àáu)‚ä§‚àáu
2
= 1
2(‚àáu)‚ä§+ 1
2‚àáu + o(|‚àáu|),
(17.82)
which
leads
to
the
deÔ¨Ånition
of
the
linearized-strain tensor, also called small-
strain tensor, as e(u) = 1
2(‚àáu)‚ä§+ 1
2‚àáu. In
fact, a vast amount of engineering or also,
for example, geophysical models and cal-
culations are based on the small-strain
concept. The speciÔ¨Åc energy in homo-
geneous materials is then ùúë‚à∂‚Ñùd√ód
sym ‚Üí‚Ñù
where ‚Ñùd√ód
sym = {A ‚àà‚Ñùd√ód; A‚ä§= A}. In lin-
early responding materials, ùúëis quadratic.
An example is an isotropic material; in
terms of Lam√©‚Äôs constants as in (17.81), it
takes the form
ùúëLame(e) = 1
2ùúÜ|tr e|2 + ùúá|e|2.
(17.83)
Such ùúëLame is positive deÔ¨Ånite on ‚Ñùd√ód
sym if
ùúá> 0 and ùúÜ> ‚àí(2d‚àïùúá). The positive deÔ¨Å-
niteness of the functional ‚à´Œ© ùúëLame(e(u)) dx
is a bit delicate as the rigid-body motions
(translations and rotations) are not taken
into account by it. Yet, Ô¨Åxing positive def-
initeness by suitable boundary conditions,
coercivity can then be based on the Korn
inequality
‚àÄv‚ààW 1,p(Œ©; ‚Ñùd), v|ŒìD = 0 ‚à∂
‚Äñ‚Äñv‚Äñ‚ÄñW1,p(Œ©;‚Ñùd) ‚â§Cp‚Äñ‚Äñe(v)‚Äñ‚ÄñLp(Œ©;‚Ñùd√ód
sym)
(17.84)
to be used for p = 2; actually, (17.84)
holds for p > 1 on connected smooth
domains with ŒìD of nonzero measure,
but notably counterexamples exist for
p = 1. Then, by the direct method based
on Theorem 17.2, one proves existence
and uniqueness of the solution to the
Lam√© system arising from (17.55) by con-
sidering
ùúë(x, u, F) = ùúëLame( 1
2F‚ä§+ 1
2F)
and
ùúô(x, u) = g(x)u:
div ùúé+ f = 0
in Œ©
with ùúé= 2ùúáe(u)+ùúÜ(div u)ùïÄ, (17.85a)
ùúé‚Éón = g
on ŒìN,
(17.85b)
u||Œì = uD
on ŒìD.
(17.85c)
The Lam√© potential (17.83) can be
obtained by an asymptotic expansion of an

574
17 Calculus of Variations
Ogden material (17.78), see [48, Volume I,
Theorem 4.10-2].
An illustration of a very speciÔ¨Åc varia-
tional inequality is a Signorini (frictionless)
contact on a third part ŒìC of the boundary
Œì; so now we consider Œì divided into three
disjoint relatively open parts ŒìD, ŒìN, and ŒìC
whose union is dense in Œì. This is a special
case of the general problem (17.72) with
ùúÅ‚â°0 and ùúâ(x, u) = 0 if u‚ãÖ‚Éón(x) ‚â§0 on ŒìC,
otherwise ùúâ(x, u) = +‚àû. In the classical
formulation, the boundary condition on ŒìC
can be identiÔ¨Åed as
(ùúé‚Éón)‚ãÖ‚Éón ‚â§0,
u‚ãÖ‚Éón ‚â§0,
((ùúé‚Éón)‚ãÖ‚Éón)(u‚ãÖ‚Éón) = 0
‚é´
‚é™
‚é¨
‚é™‚é≠
on ŒìC ,
(17.85d)
(ùúé‚Éón) ‚àí((ùúé‚Éón)‚ãÖ‚Éón)‚Éón = 0
on ŒìC ;
(17.85e)
note that (17.85d) has a structure of a
complementarity
problem
(17.17)
for
the normal stress (ùúé‚Éón)‚ãÖ‚Éón and the normal
displacement u‚ãÖ‚Éón, while (17.85e) is the
equilibrium condition for the tangential
stress.
This very short excursion into a wide area
of contact mechanics shows that it may
have a simple variational structure: In terms
of Example 17.1, the convex set K is a cone
(with the vertex not at origin if uD ‚â†0):
K =
{
u‚ààH1(Œ©; ‚Ñùn); u||Œì = uD on ŒìD,
u‚ãÖ‚Éón ‚â§0 on ŒìC
}
(17.86)
and then (17.85) is just the classical formu-
lation of the Ô¨Årst-order condition (17.13)
for the simple problem
minimize Œ¶(u) =‚à´Œ©
ùúÜ
2 |div u|2+ ùúá|e(u)|2dx
subject to u‚ààK from (17.86).
As in Section 17.3.2.2, it again demon-
strates one of the advantages of the
variational formulation as having a much
simpler form in comparison with the
classical formulation.
17.3.3.4
Sphere-Valued Harmonic Maps
Another example is a minimization prob-
lem with ùúë(x, u, F)=ùúî(u)+h(x)‚ãÖu + ùúñ
2|F|2
and the nonconvex constraint |u| = 1, that
is,
minimize Œ¶=‚à´Œ©
ùúî(u)+ùúñ
2|‚àáu|2‚àíh‚ãÖu dx
subject to R(u) = |u|2 ‚àí1 = 0 on Œ©,
(17.87)
which has again the structure (17.14) now
with V = K = H1(Œ©; ‚Ñù3) and Œõ = L2(Œ©)
with the ordering by the trivial cone {0}
of ‚Äúnonnegative‚Äù vectors. This may serve
as a simpliÔ¨Åed model of static micromag-
netism45) in ferromagnetic materials at low
temperatures so that the Heisenberg con-
straint |u| = 1 is well satisÔ¨Åed pointwise by
the magnetization vector u. Alternatively,
it may also serve as a simpliÔ¨Åed model of
liquid crystals. The weak formulation of
this minimization problem is
‚à´Œ©
ùúñ‚àáu‚à∂‚àá(v‚àí(u‚ãÖv)u)
+ (ùúî‚Ä≤(u)‚àíh)‚ãÖ(v‚àí(u‚ãÖv)u) dx = 0 (17.88)
for any v ‚ààH1(Œ©; ‚Ñù3) with |v|2 = 1 a.e. on
Œ©; for ùúî‚â°0 cf. [50, Section 8.4.3]. The cor-
responding classical formulation then has
the form
‚àídiv(ùúñ‚àáu) + ùúî‚Ä≤(u) ‚àíh
= (|‚àáu|2+ (ùúî‚Ä≤(u)‚àíh)‚ãÖu)u in Œ©, (17.89a)
ùúñ‚àáu‚ãÖ‚Éón||Œì = 0
on Œì.
(17.89b)
45) In this case, ùúî‚à∂‚Ñù3 ‚Üí‚Ñùis an anisotropy
energy with minima at easy-axis magnetization
and ùúñ> 0 is an exchange-energy constant, and
h is an outer magnetic Ô¨Åeld. The demagnetizing
Ô¨Åeld is neglected.

17.3 Variational Problems on SpeciÔ¨Åc Function Spaces
575
Comparing it with (17.16) with NK ‚â°{0}
and R‚Ä≤ = 2√óidentity on L2(Œ©; ‚Ñù3), we can
see that ùúÜ‚àó= 1
2|‚àáu|2+ 1
2(ùúî‚Ä≤(u)‚àíh)‚ãÖu plays
the role of the Lagrange multiplier for the
constraint |u|2 = 1 a.e. on Œ©.
Let us remark that, in the above-
mentioned
micromagnetic
model,
R
is more complicated than (17.87) and
involves also the diÔ¨Äerential constraints
div(h‚àíu) = 0 and rot h = j
(17.90)
with j assumed Ô¨Åxed, which is the (steady
state) Maxwell system where j is the electric
current, and the minimization in (17.87) is
to be done over the couples (u, h); in fact,
(17.90) is considered on the whole ‚Ñù3 with j
Ô¨Åxed and u vanishing outside Œ©.
17.3.3.5
Saddle-Point-Type Problems
In addition to minimization principles,
other principles also have applications. For
example, for the usage of the mountain
pass Theorem 17.4 for potentials such as
ùúë(u, F) = 1
2|F|2 + c(u) see [50, Section 8.5]
or [51, Section II.6].
Seeking saddle points of Lagrangeans
such as (17.18) leads to mixed formula-
tions of various constrained problems. For
example, the Signorini problem (17.85)
uses
Œ¶(u) = ‚à´Œ© ùúëLame(e(u)) dx + ‚à´ŒìN g‚ãÖu dS,
R ‚à∂u ÓÇ∂‚Üíu‚ãÖ‚Éón ‚à∂H1(Œ©; ‚Ñùd) ‚ÜíH1‚àï2(ŒìC), and
D = {v ‚ààH1‚àï2(ŒìC);
v ‚â§0}.
The
saddle
point
(u, ùúÜ‚àó) ‚ààH1(Œ©; ‚Ñùd) √ó H‚àí1‚àï2(ŒìC)
with u|ŒìD = g and ùúÜ‚àó‚â§0 on ŒìC exists
and represents the mixed formulation of
(17.85); then ùúÜ‚àó= (ùúé‚Éón)‚ãÖ‚Éón and cf. also the
Karush‚ÄìKuhn‚ÄìTucker conditions (17.16)
with NK ‚â°{0}, cf. e.g. [37, 43].
Another example is a saddle point
on
H1
0(Œ©; ‚Ñùd) √ó L2(Œ©)
for
‚Ñí(u, ùúÜ‚àó) =
‚à´Œ©
1
2|‚àáu|2 + ùúÜ‚àódiv u dx that leads to the
system
‚àíŒîu + ‚àáùúÜ‚àó= 0 and div u = 0,
(17.91)
which is the Stokes system for a steady
Ô¨Çow of viscous incompressible Ô¨Çuid. The
primal formulation minimizes ‚à´Œ©
1
2|‚àáu|2dx
on H1
0(Œ©; ‚Ñùd) subject to div u = 0. The
Karush‚ÄîKuhn‚ÄìTucker conditions (17.16)
with
R ‚à∂u ÓÇ∂‚Üídiv u ‚à∂H1
0(Œ©; ‚Ñùd) ‚ÜíL2(Œ©)
and the ordering of D = {0} as the cone of
nonnegative vectors gives (17.91).
17.3.4
Evolutionary Problems
Let
us
illustrate
the
Brezis‚ÄìEkeland‚Äì
Nayroles principle on the initial-boundary-
value problem for a quasilinear parabolic
equation
ùúïu
ùúït ‚àídiv(|‚àáu|p‚àí2‚àáu) = g in Q, (17.92a)
u = 0 on Œ£, (17.92b)
u(0, ‚ãÖ) = u0 in Œ©
(17.92c)
with Q=[0, T]√óŒ© and Œ£=[0, T]√óŒì. We
consider V = W 1,p
0 (Œ©) equipped with the
norm ‚Äñu‚Äñ1,p = ‚Äñ‚àáu‚Äñp, Œ¶(u) = (1‚àïp)‚Äñ‚àáu‚Äñp
p
and ‚ü®f (t), u‚ü©= ‚à´Œ© g(t, x)u(x) dx. Let us use
the notation Œîp ‚à∂W 1,p
0 (Œ©) ‚ÜíW ‚àí1,p‚Ä≤(Œ©) =
W ‚àí1,p
0
(Œ©)‚àófor the p-Laplacian; this means
Œîpu = div(|‚àáu|p‚àí2‚àáu). We have that
Œ¶‚àó(ùúâ) = 1
p‚Ä≤ ‚Äñ‚Äñùúâ‚Äñ‚Äñ
p‚Ä≤
W‚àí1,p‚Ä≤ (Œ©)
= 1
p‚Ä≤ ‚Äñ‚ÄñŒî‚àí1
p ùúâ‚Äñ‚Äñ
p
1,p = 1
p‚Ä≤ ‚Äñ‚Äñ‚àáŒî‚àí1
p ùúâ‚Äñ‚Äñ
p
p .
Thus we obtain the following explicit form
of the functional ùîâfrom (17.24):
ùîâ(u) = ‚à´Q
1
p|‚àáu|p +
(ùúïu
ùúït ‚àíg
)
u
+ 1
p‚Ä≤
|||‚àáŒî‚àí1
p
(ùúïu
ùúït ‚àíg
)|||
p
dxdt
+ ‚à´Œ©
1
2
||u(T)||
2 dx.
(17.93)
We can observe that the integrand in
(17.93) is nonlocal in space, which is, in

576
17 Calculus of Variations
fact, an inevitable feature for parabolic
problems, cf. [52].
Wide generalization to self-dual prob-
lems of the type (Lx, Ax) ‚ààùúï‚Ñí(Ax, Lx) are
in [53], covering also nonpotential situa-
tions such as the Navier‚ÄìStokes equations
for incompressible Ô¨Çuids and many others.
EÔ¨Écient usage of the ‚Äúglobal‚Äù varia-
tional principles such as (17.24), (17.32), or
(17.34) for parabolic equations or inequal-
ities is, however, limited to theoretical
investigations. Of much wider applicability
are recursive variational problems aris-
ing by implicit or various semi-implicit
time discretization as in Section 17.2.4.3,
possibly combined also with spatial dis-
cretization
and
numerical
algorithms
leading
to
computer
implementations,
mentioned in Section 17.4.1 below.
Other, nonminimization principles have
applications in hyperbolic problems of
the type ùúåùúï2u‚àïùúït2 ‚àídiv(|‚àáu|p‚àí2‚àáu) = g
where the Hamilton principle (17.37) leads
to seeking a critical point of the functional
‚à´T
0 ‚à´Œ© ùúå|ùúïu‚àïùúït|2 ‚àí(1‚àïp)|‚àáu|p + g‚ãÖu dxdt.
17.4
Miscellaneous
The area of the calculus of variations is
extremely wide and the short excursion
above presented a rather narrow selection.
Let us at least brieÔ¨Çy touch a few more
aspects.
17.4.1
Numerical Approximation
Assuming {Vk}k‚àà‚Ñï
is a nondecreasing
sequence of Ô¨Ånite-dimensional linear sub-
spaces of V whose union is dense, that is,
‚àÄv ‚ààV ‚àÉvk ‚ààVk ‚à∂
vk ‚Üív,
(17.94)
we can restrict the original variational
problem of V to Vk. Being Ô¨Ånite dimen-
sional, Vk possesses a basis and, in terms
of coeÔ¨Écients in this basis, the restricted
problem then becomes implementable on
computers. This is the simplest idea behind
numerical approximation, called the Ritz
method [54] or, rather in a more general
nonvariational context, also the Galerkin
method [55]. This idea can be applied on an
abstract level to problems in Section 17.2.
In the simplest (conformal) version instead
of (17.3), one is to seek uk ‚ààVk such that
‚àÄv‚ààVk ‚à∂
‚ü®Œ¶‚Ä≤(uk), v‚ü©= 0;
(17.95)
such uk is a critical point of the restriction
of Œ¶ on Vk, that is, of Œ¶ ‚à∂Vk ‚Üí‚Ñù.
One option is to solve numerically the
system of nonlinear equations (17.95) iter-
atively, for example, by the (quasi) Newton
method. Yet, the variational structure can
advantageously be exploited in a number
of other options such as the conjugate-
gradient or the variable-metric methods, cf.
e.g. [56, Section 7]. Then approximate sat-
isfaction of optimality conditions typically
serves as a stopping criterion for an itera-
tive strategy; in this unconstrained case, it
is the residual in (17.95) that is to be small.
For constrained problems, the methods of
Section 17.2.3 can be adapted.
Application to concrete problems in
Section 17.3.2 on function spaces opens
further interesting possibilities. Typically,
Vk are chosen as linear hulls of piecewise
polynomial functions {vkl}l=1,‚Ä¶,Lk whose
supports in Œ© are only very small sets so
that, for most pairs (l1, l2), we have that
‚à´Œ© ‚àáivkl1‚ãÆ‚àájvkl2dx = 0;
more
precisely,
this holds for each pair (l1, l2) for which
supp(vkl1) ‚à©supp(vkl2) = ‚àÖ.
For
integral
functionals Œ¶ from (17.52) or (17.66a),
the system of algebraic equations resulting
from (17.95) is sparse, which facilitates its

17.4 Miscellaneous
577
implementation and numerical solution
on computers; this is the essence of the
Ô¨Ånite-element method.
Phenomena
discussed
in
Examples 17.2‚Äì17.5
can
make
the
approximation
issue
quite
nontrivial.
For Lavrentiev-type phenomena, see, for
example, [39]. An example sure to lead to
this phenomenon is the constraint |u|2 = 1
in Section 17.3.3.4, which is not compatible
with any polynomial approximation so
that plain usage of standard Ô¨Ånite-element
approximation cannot converge.
All this can be applied to time-discretized
evolution problems from Section 17.2.4.3,
leading
to
implementable
numerical
strategies for evolution problems from
Sections 17.2.4.1 and 17.2.4.2.
17.4.2
Extension of Variational Problems
Historically,
variational
problems
have
been
considered
together
with
the
Euler‚ÄìLagrange equations in their clas-
sical formulations, that is, in particular,
the solutions are assumed to be con-
tinuously diÔ¨Äerentiable. Here (17.55) or
(17.70) are to be considered holding point-
wise for u ‚ààC2(Œ©; ‚Ñùd) and C4(Œ©; ‚Ñùd),
respectively. Yet, such classical solutions
do not need to exist46) and thus the weak
formulations (17.54) and (17.67) repre-
sent a natural extension (using the density
C2k(Œ©) ‚äÇW k,p(Œ©)) of the original problems
deÔ¨Åned for smooth functions. The adjec-
tive ‚Äúnatural‚Äù here means the extension
by continuity, referring to the continuity
46) Historically the Ô¨Årst surprising example
for a minimizer of a (17.52) with ùúë= ùúë(F)
smooth uniformly convex, which was only in
W 1,‚àû(Œ©; ‚Ñùm) but not smooth is due to NeÀácas
[57], solving negatively the 19th Hilbert‚Äôs prob-
lem [4] if d > 1 and m > 1 are suÔ¨Éciently high.
An example for even u ‚àâW 1,‚àû(Œ©; ‚Ñùm) for
d = 3 and m = 5 is in [58].
of Œ¶ ‚à∂C2k(Œ©; ‚Ñùd) ‚Üí‚Ñùwith respect to
the norm of W k,p(Œ©; ‚Ñùd) provided natural
growth conditions on ùúëand ùúôare imposed;
for k = 1; see (17.53). Weak solutions thus
represent a natural generalization of the
concept of classical solutions.
In general, the method of extension by
(lower semi)continuity is called relaxation.
It may provide a natural concept of gener-
alized solutions with some good physical
meaning. One scheme, related to the min-
imization principle, deals with situations
when Theorem 17.1 cannot be applied
owing to the lack of weak* lower semi-
continuity. The relaxation then replaces
Œ¶ by its lower semicontinuous envelope Œ¶
deÔ¨Åned by
Œ¶(u) = lim inf
v‚Üíu weakly* Œ¶(v).
(17.96)
Theorem 17.1 then applies to Œ¶ instead
of the original Œ¶, yielding thus a gener-
alized solution to the original variational
problem. The deÔ¨Ånition (17.96) is only con-
ceptual and more explicit expressions are
desirable and sometimes actually available.
In particular, if n=1 or d=1, the integral
functional (17.52a) admits the formula
Œ¶(u) =‚à´Œ©
ùúë‚àó‚àó(x, u, ‚àáu) dx + ‚à´ŒìN
ùúô(x, u) dS,
(17.97)
where ùúë‚àó‚àó(x, u, ‚ãÖ) ‚à∂‚Ñùn√ód ‚Üí‚Ñùdenotes the
convex envelope of ùúë(x, u, ‚ãÖ), that is, the
maximal convex minorant of ùúë(x, u, ‚ãÖ). In
Example 17.2, Œ¶ is given by (17.97) with
ùúë‚àó‚àó(u, F) =
{
u2+ (|F|2‚àí1)2
if |F| ‚â•1,
u2
if |F| < 1,
cf. Figure 17.3, and with ùúô= 0, and the
only minimizer of Œ¶ on W 1,4(Œ©) is u = 0,
which is also a natural W 1,4-weak limit of all
minimizing sequences for Œ¶, cf. Figure 17.1.

578
17 Calculus of Variations
ùúë(u,‚ãÖ)
ùúë** (u,‚ãÖ)
F
Figure 17.3
A convex envelope ùúë‚àó‚àó(u, ‚ãÖ) of the
double-well potential ùúë(u, ‚ãÖ).
Fast oscillations of gradients of these
minimizing sequences can be interpreted
as microstructure, while the minimizers
of Œ¶ bear only ‚Äúmacroscopical‚Äù infor-
mation.
This
reÔ¨Çects
the
multiscale
character of such variational problems.
In general, if both n > 1 and d > 1,
(17.97) involves the quasiconvex enve-
lope ùúë‚ôØ(x, u, ‚ãÖ) ‚à∂‚Ñùn√ód ‚Üí‚Ñùrather than ùúë‚àó‚àó;
this is deÔ¨Åned by
‚àÄx‚ààŒ© ‚àÄu‚àà‚Ñùn ‚àÄF ‚àà‚Ñùn√ód ‚à∂ùúë‚ôØ(x, u, F)
=
inf
v‚ààW1,p
0
(O;‚Ñùn) ‚à´O
ùúë(x, u, F+‚àáv(ÃÉx))
measd(O)
dÃÉx ;
this deÔ¨Ånition is independent of O but is
only implicit and usually only some upper
and lower estimates (namely, rank-one con-
vex and polyconvex envelopes) are known
explicitly or can numerically be evaluated.
To cope with both nonconvexity and with
the unwanted phenomenon of nonexis-
tence as in Example 17.2, one can consider
singular perturbations, such as
Œ¶ùúÄ(u) = ‚à´Œ©
ùúë(x, u, ‚àáu) + ùúÄ‚Ñç‚àá2u‚ãÆ‚àá2u dx
+ ‚à´ŒìN
ùúô(x, u) dS
(17.98)
with
a
positive
deÔ¨Ånite
fourth-
order
tensor
‚Ñç
and
small
ùúÄ> 0;
cf.
also ‚Ñç
in
Section 17.3.3.2.
Under
the
growth/coercivity conditions on ùúëand ùúô
induced by (17.53) with 1 < p < 2‚àó, (17.98)
possesses a (possibly nonunique) mini-
mizer uùúÄ‚ààW 2,2(Œ©; ‚Ñùd). The parameter
ùúÄdetermines an internal length scale of
possible oscillations of ‚àáuùúÄoccurring if
ùúë(x, u, ‚ãÖ) is not convex, cf. also Figure 17.4.
As ùúÄis usually very small, it makes sense
to investigate the asymptotics when it
approaches 0. For ùúÄ‚Üí0, the sequence
{uùúÄ}ùúÄ>0 possesses a subsequence converg-
ing weakly in W 1,p(Œ©; ‚Ñùd) to some u and
every such a limit u minimizes the relaxed
functional
Œ¶(u) = ‚à´Œ©
ùúë‚ôØ(x, u, ‚àáu) dx + ‚à´ŒìN
ùúô(x, u) dS.
(17.99)
The possible fast spatial oscillations of the
gradient are smeared out in the limit.
To record some information about such
oscillations in the limit, one should make a
relaxation by continuous extension rather
than only by weak lower semicontinuity.
To ensure the existence of solutions, the
extended space should support a com-
pact topology which makes the extended
functional continuous; such a relaxation is
called a compactiÔ¨Åcation. If the extended
space also supports a convex structure (not
necessarily coinciding with the linear struc-
ture of the original space), one can deÔ¨Åne
variations, diÔ¨Äerentials, and the abstract
Euler‚ÄìLagrange equality (17.13); then we
speak about the convex compactiÔ¨Åcation
method, cf. [59].
A relatively simple example can be the
relaxation of the micromagnetic prob-
lem (17.87)‚Äì(17.90) that, in general, does
not have any solution if ùúÄ= 0 due to

17.4 Miscellaneous
579
nonconvexity
of
the
Heisenberg
constraint
|u| = 1.
One
can
embed
the
set
of
admissible
u‚Äôs,
namely
{u‚ààL‚àû(Œ©; ‚Ñù3); u(x)‚ààS for a.a. x}
with
the sphere S = {|s| = 1} ‚äÇ‚Ñù3 into a larger
set ùí¥(Œ©; S) = {ùúà= (ùúàx)x‚ààŒ©; ùúàx a proba-
bility47) measure on S and x ÓÇ∂‚Üíùúàx weakly*
measurable}; the embedding is realized by
the mapping u ÓÇ∂‚Üíùúà= (ùõøu(x))x‚ààŒ© where ùõøs
denotes here the Dirac measure supported
at s ‚ààS. The elements of ùí¥(Œ©; S) are
called Young measures [60]48) and this set is
(considered as) a convex weakly* compact
subset of L‚àû
w‚àó(Œ©; M(S)) where M(S) ‚âÖC(S)‚àó
denotes the set of Borel measures on S.49)
The problem (17.87)‚Äì(17.90) with ùúÄ= 0
then allows a continuous extension
minimize Œ¶(ùúà, h)
=‚à´
Œ©
‚à´
S
ùúî(s)‚àíh‚ãÖs ùúàx(ds)dx
subject to div(h‚àíu) = 0, rot h = j,
ùúà‚ààùí¥(Œ©; S),
with u(x) =‚à´S
s ùúàx(ds) for x‚ààŒ©.
(17.100)
The functional Œ¶ is a continuous extension
of (u, h) ÓÇ∂‚ÜíŒ¶(u, h) from (17.87), which is
even convex and smooth with respect to
the geometry of L‚àû
w‚àó(Œ©; M(S)) √ó L2(Œ©; ‚Ñù3).
Existence
of
solutions
to
the
relaxed
47) The adjective ‚Äúprobability‚Äù means here a pos-
itive measure with a unit mass but does not
refer to any probabilistic concept.
48) In fact, L.C. Young had already introduced
such measures in 1936 in a slightly diÔ¨Äerent
language even before the theory of measure
had been invented. For modern mathemati-
cal theory see, for example, [30, Chapter 8],
[61, Chapter 6‚Äì8], [62, Chapter 2], or [59,
Chapter 3].
49) ‚ÄúL‚àû
w‚àó‚Äù denotes ‚Äúweakly* measurable‚Äù essentially
bounded‚Äù mappings, and L‚àû
w‚àó(Œ©; M(S)) is a dual
space to L1(Œ©; C(S)), which allows to intro-
duce the weak* convergence that makes this set
compact.
problem (17.100) is then obtained by
Theorem 17.1 modiÔ¨Åed for the constrained
case. Taking into account the convexity
of ùí¥(Œ©; S), the necessary and suÔ¨Écient
optimality conditions of the type (17.13)
for (17.100) lead, after a disintegration, to a
pointwise condition
‚à´S
‚Ñåh(x, s) ùúàx(ds) = max
s‚ààS ‚Ñåh(x, s),
with ‚Ñåh(x, s) = h(x)‚ãÖs‚àíùúî(s) (17.101)
to hold for a.a. x‚ààŒ© with h satisfying
the linear constraints in (17.100), that is,
div(h‚àíu) = 0, rot h = j, and u =‚à´S s ùúà(ds).
The integrand of the type ‚Ñåh is some-
times called a Hamiltonian and conditions
like (17.101), the Weierstrass maximum
principle, formulated here for the relaxed
problem and revealed as being a stan-
dard condition of the type (17.13) but
with respect to a nonstandard geometry
imposed by the space L‚àû
w‚àó(Œ©; M(S)). The
solutions to (17.100) are typically nontrivial
Young measures in the sense that ùúàx is not
a Dirac measure. From the maximum prin-
ciple (17.101), one can often see that they
are composed from a weighted sum of a
Ô¨Ånite number of Dirac measures supported
only at such s‚ààS that maximizes ‚Ñåh(x, ‚ãÖ).
This implies that minimizing sequences for
the original problem (17.87)‚Äì(17.90) with
ùúÄ= 0 ultimately must exhibit Ô¨Åner and
Ô¨Åner spatial oscillations of u‚Äôs; this eÔ¨Äect is
experimentally observed in ferromagnetic
materials, see Figure 17.4.50) In fact, a small
parameter ùúÄ> 0 in the original problem
(17.87)‚Äì(17.90) determines the lengthscale
of magnetic domains and also the typical
width of the walls between the domains.
For the Young measure relaxation in
micromagnetism see, for example, [61, 63].
50) Actually, the minimization-energy principle
governs magnetically soft materials where

580
17 Calculus of Variations
20 Œºm
Figure 17.4
Fast spatial oscillations of the magnetization
vector minimizing Œ¶ from (17.87)‚Äì(17.90) with a double-
well potential ùúîforming a Ô¨Åne microstructure in a ferro-
magnetic tetragonal single-crystal of NiMnGa with only one
axis of easy magnetization normal to the observed surface.
(Courtesy O. Heczko, Institute of Physics, ASCR.)
A relaxation by continuous extension of
the originally discussed problem (17.52) is
much more complicated because the vari-
able exhibiting fast-oscillation tendencies
(i.e., ‚àáu) is in fact subjected to some dif-
ferential constraint (namely, rot (‚àáu) = 0)
and because, in contrast to the previous
example, is valued on the whole ‚Ñùn√ód,
which is not compact. We thus use only a
subset of Young measures, namely,
ùí¢p(Œ©; ‚Ñùn√ód) =
{
ùúà‚ààùí¥(Œ©; ‚Ñùn√ód);
‚àÉu‚ààW 1,p(Œ©; ‚Ñùn) ‚à∂
‚àáu(x) =‚à´‚Ñùn√ód F ùúàx(dF) ‚àÄa.a.x‚ààŒ©,
‚à´Œ© ‚à´‚Ñùn√ód |F|p ùúàx(dF)dx < ‚àû
}
.
The relaxed problem to (17.52) obtained by
continuous extension then has the form
minimize Œ¶(ùúà, h)=‚à´‚à´
Œ© ‚Ñùn√ód
ùúë(u, F) ùúàx(dF)dx
+ ‚à´Œì
ùúô(u) dS
subject to ‚àáu(x) =‚à´‚Ñùn√ódF ùúàx(dF) ‚àÄa.a.x,
(u, ùúà)‚ààW 1,p(Œ©; ‚Ñùn)√óùí¢p(Œ©; ‚Ñùn√ód).
(17.102)
the hysteresis caused by pinning eÔ¨Äects is
not dominant.
Proving existence of solutions to (17.102)
is
possible
although
technically
com-
plicated51)
and,
moreover,
ùí¢p(Œ©; ‚Ñùn√ód)
is unfortunately not a convex subset of
L‚àû
w‚àó(Œ©; M(‚Ñùn√ód)) if min(n, d) > 1. Only if
n = 1 or d = 1, we can rely on its convexity
and
derive
Karush‚ÄîKuhn‚ÄìTucker-type
necessary optimality conditions of the type
(17.13) with ùúÜ‚àóbeing the multiplier to
the constraint ‚àáu(x) =‚à´‚Ñùn√ód F ùúàx(dF); the
adjoint operator [R‚Ä≤]‚àóin (17.16) turns ‚Äú‚àá‚Äù
to ‚Äúdiv.‚Äù The resulted system takes the form
‚à´
‚Ñùn√ód
‚Ñåu,ùúÜ‚àó(x, F) ùúàx(dF) = max
F‚àà‚Ñùn√ód ‚Ñåu,ùúÜ‚àó(x, F),
with ‚Ñåu,ùúÜ‚àó(x, F) = ùúÜ‚àó(x)‚ãÖF‚àíùúë(x, u(x), F)
div ùúÜ‚àó= ‚à´‚Ñùn√ód ùúë‚Ä≤
u(u, F) ùúàx(dF) on Œ©,
ùúÜ‚àó‚ãÖ‚Éón + ùúô‚Ä≤
u(u) = 0
on Œì,
(17.103)
cf. [59, Chapter 5]. If ùúë(x, u, ‚ãÖ) is convex,
then there exists a standard weak solution
u, that is, ùúàx = ùõø‚àáu(x), and (17.103) simpliÔ¨Åes
to
51) Actually, using compactness and the direct
method must be combined with proving and
exploiting that minimizing sequences {uk}k‚àà‚Ñï
for (17.52) have {|‚àáuk|p; k ‚àà‚Ñï} uniformly
integrable if the coercivity (17.53a) with p > 1
holds.

17.4 Miscellaneous
581
‚Ñåu,ùúÜ‚àó(x, ‚àáu(x)) = maxF‚àà‚Ñùn√ód ‚Ñåu,ùúÜ‚àó(x, F),
div ùúÜ‚àó= ùúë‚Ä≤
u(u, ‚àáu)
on Œ©,
ùúÜ‚àó‚ãÖ‚Éón + ùúô‚Ä≤
u(u) = 0
on Œì.
(17.104)
One can see that (17.104) combines the
Weierstrass maximum principle with a half
of the Euler‚ÄìLagrange equation (17.55).
If ùúë(x, u, ‚ãÖ) is not convex, the oscillatory
character of ‚àáu for minimizing sequences
can be seen from (17.103) similarly as in
the previous example, leading to nontrivial
Young measures.
We can illustrate it on Example 17.2,
where
(17.103)
leads
to
the
system
dùúÜ‚àó‚àïdx = 2u
and
du‚àïdx = ‚à´‚ÑùF ùúàx(dF)
on (0, 6ùúã) with the boundary conditions
ùúÜ‚àó(0) = 0 = ùúÜ‚àó(6ùúã) and with the Young
measure
{ùúàx}0‚â§x‚â§6ùúã‚ààùí¥4((0, 6ùúã); ‚Ñù)
such
that
ùúàx
is
supported
on
the
Ô¨Ånite
set
{F ‚àà‚Ñù;
ùúÜ‚àó(x)F ‚àí(|F|2‚àí1)2 =
maxÃÉF‚àà‚ÑùùúÜ‚àó(x)ÃÉF ‚àí(|ÃÉF|2‚àí1)2}.
The
(even
unique) solution of this set of conditions is
u(x) = 0,
ùúÜ‚àó(x) = 0,
ùúàx = 1
2ùõø1 + 1
2ùõø‚àí1
(17.105)
for x ‚àà(0, 6ùúã). This (spatially constant)
Young measure indicates the character of
the oscillations of the gradient in (17.62).
Having in mind the elasticity interpre-
tation from Section 17.3.3.2, this eÔ¨Äect is
experimentally observed in some special
materials, see Figure 17.5;52) for modeling
of such microstructure by nonconvex prob-
lems see, for example, [20, 62, 64‚Äì66].
Models based on relaxation of continu-
ous extensions such as (17.100) or (17.102),
are sometimes called mesoscopical, in
contrast to the original problems such as
(17.87)‚Äì(17.90) or (17.98) with small ùúÄ> 0,
52) Actually, Figure 17.5 refers to a multidimen-
sional vectorial case (i.e., d > 1 and n > 1)
where (17.103) is not available.
which are called microscopical, while the
models using original spaces but lower
semicontinuous extensions such as (17.97),
which forget any information about Ô¨Åne
microstructures, are called macroscopical.
17.4.3
ùö™-Convergence
We saw above various situations where the
functional itself depends on a parameter. It
is then worth studying convergence of such
functionals. In the context of minimization,
a prominent role is played by Œì-convergence
introduced by De Giorgi [67], sometimes
also called variational convergence or epi-
graph convergence, cf. also [68‚Äì70]. We say
that the functional Œ¶ is the Œì-limit of a
sequence {Œ¶k}k‚àà‚Ñïif
‚àÄuk ‚Üíu ‚à∂lim inf
k‚Üí‚àûŒ¶k(uk) ‚â•Œ¶(u),
(17.106a)
‚àÄu‚ààV ‚àÉ{uk}k‚àà‚Ñïwith uk ‚Üíu ‚à∂
lim sup
k‚Üí‚àû
Œ¶k(uk) ‚â§Œ¶(u).
(17.106b)
One interesting property justifying this
mode of convergence is the following:
Theorem 17.15 (Œì-convergence.) If Œ¶k ‚Üí
Œ¶ in the sense (17.106) and if uk mini-
mizes Œ¶k, then any converging subsequence
of {uk}k‚àà‚Ñïyields, as its limit, a minimizer
of Œ¶.53)
Identifying the Œì-limit (if it exists) in con-
crete cases can be very diÔ¨Écult. Few rela-
tively simple examples were, in fact, already
stated above.
53) The proof is simply by a contradiction, assum-
ing that Œ¶(u) > Œ¶(v) for u = liml‚Üí‚àûukl and
some v ‚ààV and using (17.106) to have a
recovery sequence vk ‚Üív so that Œ¶(v) =
lim infk‚Üí‚àûŒ¶k(vk) ‚â•lim infk‚Üí‚àûŒ¶k(uk) ‚â•Œ¶(u).

582
17 Calculus of Variations
0.1 mm
Figure 17.5
Oscillations of the
deformation gradient minimizing
Œ¶ with a 6-well potential ùúë(F):
an orthorhombic martensite
microstructure in a single-crystal
of CuAlNi. (Courtesy H. Seiner,
Inst. of Thermomechanics, ASCR.)
A simple example is the numerical
approximation in Section 17.4.1 where we
had the situation
Vk ‚äÇVk+1 ‚äÇV
for k ‚àà‚Ñï, and
(17.107a)
Œ¶k(v) =
{
Œ¶(v)
if v‚ààVk,
+‚àû
otherwise.
(17.107b)
Let us further suppose that Œ¶ is continuous
with respect to the convergence used in
(17.94). Then Œ¶k Œì-converges to Œ¶.54) Note
that lower semicontinuity of Œ¶ would not
be suÔ¨Écient for it, however.55)
Another
example
of
(17.106)
with
the
weak
topology
we
already
saw
is
given
by
singular
perturbations:
the
functionals
Œ¶ùúÄ‚à∂W 1,p(Œ©; ‚Ñùd) ‚Üí
‚Ñù‚à™{+‚àû}
deÔ¨Åned
by
(17.98)
for
u ‚ààW 1,p(Œ©; ‚Ñùd) ‚à©W 2,2(Œ©; ‚Ñùd)
and
by
+‚àû
for
u ‚ààW 1,p(Œ©; ‚Ñùd) ‚ßµW 2,2(Œ©; ‚Ñùd)
Œì-converge, for ùúÄ‚Üí0, to Œ¶ from (17.52)
if
ùúë(x, u, ‚ãÖ)
is
quasiconvex,
otherwise
one should use Œ¶ from (17.99). Alter-
natively, if Œ¶ùúÄis extended to the Young
54) Indeed, (17.106a) holds because Œ¶k ‚â•Œ¶k+1 ‚â•Œ¶
due to (17.107a). For any ÃÇv ‚ààV, there is ÃÇvk ‚àà
Vk such that ÃÇvk ‚ÜíÃÇv. Then limk‚Üí‚àûŒ¶k(ÃÇvk) =
limk‚Üí‚àûŒ¶(ÃÇvk) = Œ¶(ÃÇv) and also limk‚Üí‚àûÃÇvk = v in
V so that {ÃÇvk}k‚àà‚Ñïis a recovery sequence for
(17.106b).
55) A simple counterexample is Œ¶ = +‚àûevery-
where except some v ‚ààV ‚ßµ‚à™k‚àà‚ÑïVk; then Œ¶k ‚â°
‚àûobviously does not Œì-converge to Œ¶.
measures by +‚àûif the Young measure
is not of the form {ùõø‚àáu(x)}x‚ààŒ© for some
u ‚ààW 1,p(Œ©; ‚Ñùd) ‚à©W 2,2(Œ©; ‚Ñùd),
one
Œì-
converges as ùúÄ‚Üí0 to the functional
from (17.102).56) Similarly, (17.87)‚Äì(17.90)
Œì-converges to (17.100) if ùúñ‚Üí0.
Other prominent applications of Œì-
convergence are dimensional reduction
from three-dimensional problems to one-
dimensional (springs, rods, beams) or
two-dimensional (membranes, thin Ô¨Ålms,
shells, plates), or homogenization of com-
posite materials with periodic structure, cf.
e.g. [48, 71].
Glossary
A lot of notions, deÔ¨Ånitions, and assertions
are presented above. The following list tries
to sort them according subjects or disci-
plines, giving the link to particular pages
where the particular item is highlighted.
Topological notions:
Œì-convergence, p.581
continuous (weakly), p.552 (p.553)
56) Again, (17.106a) is simply due to Œ¶ùúÄ‚â•Œ¶ and
Œ¶ is lower semicontinuous. The construction of
particular recovery sequences for (17.106b) is
more involved, smoothing the construction of a
recovery sequence for ùúë‚ôØor for the minimizing
gradient Young measure as in [61, 2].

17.4 Glossary
583
compact mapping, p.563
compact set, p.553
dense, p.557
hemicontinuous mapping, p.552
lower semicontinuous, p.553
envelope, p.577
variational convergence, p.581
Linear spaces, spaces of functions:
adjoint operator, p.555
Banach space, p.551
ordered, p.555
reÔ¨Çexive, p.553
Bochner space Lp(I; V), p.557
boundary critical exponent p‚ôØ, p.563
dual space, p.552
Gelfand triple V ‚äÇH ‚äÇV ‚àó, p.557
Hilbert space, p.552
Lebesgue space Lp, p.562
pre-dual, p.553
smooth functions Ck(Œ©), p.563
Sobolev critical exponent p‚àó, p.563
Sobolev space W k,p, p.562
Young measures, p.579
Convex analysis:
cone, p.555
convex/concave, p.552
convex mapping, p.556
Fenchel inequality, p.557
indicator function ùõøK, p.555
Legendre conjugate, p.557
Legendre transformation, p.557
Legendre‚ÄìFenchel transformation, p.559
linear, p.552
monotone, p.553
normal cone NK(u), p.555
polyconvexity, p.566
rank-one convexity, p.565
strictly convex, p.553
subdiÔ¨Äerential ùúï, p.554
tangent cone TK(u), p.555
Smooth analysis:
continuously diÔ¨Äerentiable, p.552
directionally diÔ¨Äerentiable, p.552
Fr√©chet subdiÔ¨Äerential ùúïF, p.554
G√¢teaux diÔ¨Äerential, pp.552, 564
smooth, p.552
Optimization theory:
adjoint system, p.585
constraint qualiÔ¨Åcation
Mangasarian-Fromovitz, p.555
Slater, p.556
complementarity condition, p.555
critical point, p.552
dual problem, p.556
Euler‚ÄìLagrange equation, pp.552, 564
Karush‚ÄìKuhn‚ÄìTucker condition, p.555
Lagrangean ‚Ñí(u, ùúÜ‚àó), p.555
optimal control, p.585
relaxed, p.585
suÔ¨Écient 2nd-order condition, p.556
transversality condition, p.555
Variational principles and problems:
Brezis‚ÄìEkeland‚ÄìNayroles, pp.557,575
complementarity problem, p.555
Ekeland principle, p.584
Hamilton principle, pp.559,576
Lavrentiev phenomenon, pp.567,572
least dissipation principle, p.558
minimum-energy principle, p.552
maximum dissipation, p.560
nonexistence, pp.567,571
potential, p.552
coercive, p.553
double-well, p.567
of dissipative forces, p.558
Palais‚ÄìSmale property, p.554
Plateau minimal-surface problem, p.567
Pontryagin maximum principle, p.585
relaxation, p.577
by convex compactiÔ¨Åcation, p.578
singular perturbations, pp.578, 582
Stefanelli principle, p.558
symmetry condition, pp.552, 565, 570
Onsager, p.558
variational inequality, p.554
Weierstrass maximum principle, p.579
DiÔ¨Äerential equations and inequalities:
abstract parabolic equation, p.556
boundary conditions, p.564
boundary-value problem, pp.564, 569
Carath√©odory mapping, p.563
Cauchy problem, p.556
doubly nonlinear, p.558
classical solution, p.564
doubly nonlinear inclusion, p.559
formulation
classical, pp.564,569
De Giorgi, p.559
energetic, p.560
mixed, p.575
weak, p.564,569
generalized gradient Ô¨Çow, p.558
in metric spaces, p.559
Legendre‚ÄìHadamard condition, p.566
NemytskiÀòi mapping, p.563

584
17 Calculus of Variations
nonsymmetric equations, p.584
nonvariational methods, p.585
quasiconvex, p.565
envelope, p.578
rate-independent, p.560
Stokes system, p.575
surface divergence divS, p.569
weak solution, p.565
Numerical techniques:
Ô¨Ånite-element method, p.577
fractional-step method, p.561
Galerkin method, p.576
implicit Euler formula, p.560
Ritz method, p.576
Rothe method, p.560
semi-implicit scheme, p.561
sequential splitting, p.561
Uzawa algorithm, p.561
Mechanics of continua:
Ciarlet‚ÄìNeÀácas condition, p.572
complex continuum, p.573
hyperelastic material, p.572
KirchhoÔ¨Ätransformation, p.571
Korn inequality, p.573
Lam√© system, p.573
microstructure, pp.578,580,582
minimal surface, p.567
nonsimple continuum, p.573
Ogden material, p.572
small-strain tensor e(u), p.573
Signorini contact, p.574
St.Venant‚ÄìKirchhoÔ¨Ämaterial, p.573
Some important theorems:
Bolzano‚ÄìWeierstrass, p.553
compact embedding W 1,p ‚äÇLp‚àó‚àíùúñ, p.563
compact trace operator, p.563
direct method, p.553, 553
for elliptic problems, p.566
for parabolic problems, p.557
Œì-convergence, p.581
mountain pass, p.554
NemytskiÀòi-mapping continuity, p.563
von Neumann‚Äôs saddle-point, p.554
1st-order necessary condition, p.555
2nd-order suÔ¨Écient condition, p.556
Further Reading
The convex/smooth setting with one objec-
tive functional on which we primarily
focused in Section 17.2 can be extensively
generalized to nonconvex and nondiÔ¨Äer-
entiable cases and/or to multi-objective
situations, including dualization schemes,
optimality conditions, sensitivity analysis,
generalized equilibria, and many others,
cf. e.g. [9, 12, 37, 72‚Äì74]. Many proof
techniques are based on the remarkable
Ekeland variational principle saying that,
for a G√¢teaux diÔ¨Äerentiable functional Œ¶
bounded from below on a Banach space V,
holds that
‚àÄu ‚ààV, ùúÄ> 0 ‚à∂
Œ¶(u) ‚â§inf Œ¶ + ùúÄ
‚áí
‚àÉv ‚ààV ‚à∂
Œ¶(v) ‚â§Œ¶(u),
‚Äñv‚àíu‚Äñ ‚â§
‚àö
ùúÄ,
‚Äñ‚ÄñŒ¶‚Ä≤(v)‚Äñ‚Äñ ‚â§
‚àö
ùúÄ,
See, for example, [11, 31, 37], in particu-
lar, also for a general formulation in metric
spaces.
In concrete situations, solutions of vari-
ational problems often enjoy additional
properties (typically, despite the coun-
terexamples as [57, 58], some smoothness);
there is an extensive literature in this direc-
tion of regularity of solutions, for example,
[32, 50, 75].
There has been intensive eÔ¨Äort leading
to eÔ¨Écient and widely applicable methods
to avoid the symmetry conditions (17.5),
cf. also (17.57), based on the concept of
monotonicity.
Nonsymmetric
nonlinear
monotone-type operators (possibly gener-
alized, for example, to pseudo-monotone
operators or of the types (M) or (S), etc.)
have been introduced on an abstract level
in the work of Br√©zis [7], Minty [76], and
others. Many monographs are available
on this topic, also applied to concrete
nonsymmetric
quasilinear
equations
or
inequalities, cf. e.g. [18, 50, 77, 78].
Even for situations conforming with
the symmetry conditions of the type
(17.57), Example 17.5 showed that some-
times variational methods even for linear

Further Reading
585
boundary-value problem such as
‚àídiv‚àáu = f on Œ©,
‚àáu‚ãÖ‚Éón + u = g on Œì,
are not compatible with natural physi-
cal demands that the right-hand sides f
and g have an L1-structure. This is why
also nonvariational methods have been
extensively developed. One method to
handle general right-hand sides is Stam-
pacchia‚Äôs [79] transposition method, which
has been analyzed for linear problems by
Lions and Magenes [80]. Another general
method is based on metric properties and
contraction based on accretivity (instead
of compactness and monotonicity) and,
when applied to evolution problems, is
connected with the theory of nonexpansive
semigroups; from a very wide literature
cf. e.g. the monographs by Showalter [78,
Chapter 4], Vainberg [81, Chapter VII],
or Zeidler [9, Chapter 57], or also [18,
Chapter 3 and 9]. An estimation technique
Ô¨Åtted with L1-structure and applicable to
thermal problems possibly coupled with
mechanical or other physical systems, has
been developed in [82], cf. also e.g. [18].
In
fact,
for
d = 1
and
Œ© = [0, T],
Section 17.3.2.1 dealt in particular with
a very special optimal control problem
of the Bolza type: minimize the objective
‚à´T
0 ùúë(t, u(t), v(t)) dt + ùúô(T, u(T))
for
the
initial-value problem for a simple (sys-
tem of) ordinary diÔ¨Äerential equation(s)
du‚àïdt = v, u(0) = u0, with the control v
being possibly subjected to a constraint
v(t) ‚ààS, with t ‚àà[0, T] playing the role
of time. One can also think about gen-
eralization
to
(systems
of)
nonlinear
ordinary
diÔ¨Äerential
equations
of
the
type du‚àïdt = f (t, u, v). If ùúë(t, u, ‚ãÖ) is con-
vex and f (t, u, ‚ãÖ) is aÔ¨Éne, one obtains
existence of optimal control v and the
corresponding
response
by
the
direct
method as we did in Section 17.3.2.1. If
fact, convexity of the so-called orientor
Ô¨Åeld Q(t, u) ‚à∂= {(q0, q1); ‚àÉs‚ààS(t) ‚à∂q0 ‚â•
ùúë(t, u, s),
q1 = f (t, u, s)} is decisive for
existence of optimal control. In the general
case, the existence is not guaranteed and
one can make a relaxation as we did in
(17.100) obtaining the relaxed optimal
control problem
minimize =‚à´
T
0 ‚à´S
ùúë(t, u(t), s) ùúàt(ds)dt
subject to
du
dt =‚à´S
f (t, u(t), s) ùúàt(ds)
ùúà‚ààùí¥([0, T]; S).
(17.108)
The optimality conditions of the type
(17.16) results in a modiÔ¨Åcation of the
Weierstrass maximum principle (17.103),
namely,
‚à´S
‚Ñåu,ùúÜ‚àó(t, s) ùúàt(ds) = max
ÃÉs‚ààS ‚Ñåu,ùúÜ‚àó(t,ÃÉs) with
‚Ñåu,ùúÜ‚àó(t, s)=ùúÜ‚àó(t)‚ãÖf (t, u(t), s)‚àíùúë(t, u(t), s),
dùúÜ‚àó
dx +‚à´S
f ‚Ä≤
u(t, u(t), s)‚ä§ùúÜ‚àó(t) ùúàt(ds)
=‚à´S
ùúë‚Ä≤
u(t, u(t), s)‚ä§ùúàt(ds) on [0, T],
ùúÜ‚àó(T) = ùúô‚Ä≤
u(T, u(T)).
(17.109)
The
linear
terminal-value
problem
in
(17.109) for ùúÜ‚àóis called the adjoint system,
arising from the adjoint operator in (17.16).
Of course, if (by chance) the optimal con-
trol v of the original problem exists, then
the Ô¨Årst condition in (17.109) reads as
‚Ñåu,ùúÜ‚àó(t, v(t)) = maxÃÉv‚ààS ‚Ñåu,ùúÜ‚àó(t,ÃÉv).
Essen-
tially, this has been formulated in [83, 84]
and later become known as the Pontryagin
maximum principle, here in terms of the
so-called relaxed controls. We can see that
it is a generalization of the Weierstrass
principle and can be derived as a standard
Karush‚ÄìKuhn‚ÄìTucker condition but with
respect to the convex geometry induced

586
17 Calculus of Variations
from the space of relaxed controls.57) One
can also consider optimal control of par-
tial diÔ¨Äerential equations instead of the
ordinary ones, cf. also [59]. There is a huge
literature about optimal control theory in
all usual aspects of the calculus of vari-
ations as brieÔ¨Çy presented above, cf. e.g.
[73, 85, 86].
Acknowledgments.
The
author
is
very
thankful to Jan Mal¬¥y for fruitful discussions
(particularly with regard to Example 17.5),
Alexander Mielke, and JiÀár√≠ V. Outrata,
and to Oleg Heczko and Hanu≈° Seiner
for providing the experimental Ô¨Ågures
17.4 and 17.5 . The institutional support
RVO:61388998 ( ÀáCR) and the support from
the grant 201/10/0357 (GA ÀáCR) are also
warmly acknowledged.
References
1. Buttazzo, G., Giaquinta, M., and
Hildebrandt, S. (eds) (1998)
One-Dimensional Variational Problems,
Clarendon, Oxford.
2. Drake, G.W.F. (2005) Variational methods, in
Mathematical Tools for Physicists (ed. G.L.
Trigg), Wiley-VCH Verlag GmbH,
Weinheim, pp. 619‚Äì656.
3. Jost, J. and Li-Jost, X. (1998) Calculus of
Variations, Cambridge University Press,
Cambridge.
4. Hilbert, D. (1901) Mathematische probleme.
Archiv d. Math. u. Physik, 1, 44‚Äì63,
213‚Äì237; (English Translation: (1902) Bull.
Am. Math. Soc., 8, 437‚Äì479).
5. Banach, S. (1932) Th√©orie des Op√©rations
Lin√©aires, M.Garasi¬¥nski, Warszawa.
6. Tonelli, L. (1915) Sur un m√©thode directe du
calcul des variations. Rend. Circ. Mat.
Palermo, 39, 233‚Äì264.
7. Brezis, H. (1968) √âquations et in√©quations
non-lin√©aires dans les espaces vectoriel en
dualit√©. Ann. Inst. Fourier, 18, 115‚Äì176.
57) The original and rather technical method
was based on the so-called needle variations,
however.
8. Ambrosetti, A. and Rabinowitz, P.H. (1973)
Dual variational methods in critical point
theory and applications. J. Funct. Anal., 14,
349‚Äì380.
9. Zeidler, E. (1985) Nonlinear Functional
Analysis and Its Applications III: Variational
Methods and Optimization, Springer,
New York.
10. von Neumann, J. (1928) Zur Theorie der
Gesellschaftsspiele. Math. Ann., 100,
295‚Äì320.
11. Borwein, J.M. and Zhu, Q.J. (eds) (2005)
Techniques of Variational Analysis, Springer,
Berlin.
12. Rockafellar, R.T. and Wets, R.J.-B. (1998)
Variational Analysis, Springer, Berlin.
13. Karush, W. (1939) Minima of functions of
several variables with inequalities as side
conditions. PhD thesis, Department of
Mathematics - University of Chicago,
Chicago, IL.
14. Kuhn, H. and Tucker, A. (1951) Nonlinear
programming, 2nd Berkeley Symposium on
Mathematical Statistics and Probability,
University of California Press, Berkeley,
pp. 481‚Äì492.
15. Brezis, H. and Ekeland, I. (1976) Un prinicpe
varationnel associe√© √† certaines √©quations
paraboliques. C. R. Acad. Sci. Paris, 282,
971‚Äì974 and 1197‚Äì1198.
16. Nayroles, B. (1976) Deux th√©or√®mes de
minimum pour certains syst√®mes dissipatifs.
C. R. Acad. Sci. Paris S√©r. A-B, 282,
A1035‚ÄìA1038.
17. Roub√≠Àácek, T. (2000) Direct method for
parabolic problems. Adv. Math. Sci. Appl.,
10, 57‚Äì65.
18. Roub√≠Àácek, T. (2013)Nonlinear Partial
DiÔ¨Äerential Equations with Applications,
2nd edn, Birkh√§user, Basel.
19. Onsager, L. (1931) Reciprocal relations in
irreversible processes I. Phys. Rev., 37,
405‚Äì426; Part II, 38, 2265‚Äì2279.
20. ≈†ilhav¬¥y, M. (1997) The Mechanics and
Thermodynamics of Continuous Media,
Springer, Berlin.
21. Stefanelli, U. (2008) The Brezis-Ekeland
principle for doubly nonlinear equations.
SIAM J. Control Optim., 47, 1615‚Äì1642.
22. Onsager, L. and Machlup, S. (1953)
Fluctuations and irreversible processes. Phys.
Rev., 91, 1505‚Äì1512.
23. Ambrosio, L., Gigli, N., and Savar√©, G. (2008)
Gradient Flows, 2nd edn, Birkh√§user, Basel.

References
587
24. Bedford, A. (ed.) (1985) Hamilton‚Äôs Principle
in Continuum Mechanics, Pitman, Boston,
MA.
25. Visintin, A. (1996) Models of Phase
Transitions, Birkh√§user, Boston, MA.
26. Mielke, A. and Theil, F. (2004) On
rate-independent hysteresis models. Nonlin.
DiÔ¨Ä. Equ. Appl., 11, 151‚Äì189.
27. Mielke, A. and Roub√≠Àácek, T. (2014)
Rate-Independent Systems - Theory and
Application, Springer, New York, to appear.
28. Rothe, E. (1930) Zweidimensionale
parabolische Randwertaufgaben als
Grenzfall eindimensionaler
Randwertaufgaben. Math. Ann., 102,
650‚Äì670.
29. Dacorogna, B. (1989) Direct Methods in the
Calculus of Variations, Springer, Berlin.
30. Fonseca, I. and Leoni, G. (2007) Modern
Methods in the Calculus of Variations: Lp
Spaces, Springer, New York.
31. Giusti, E. (2003) Direct Methods in Calculus
of Variations, World ScientiÔ¨Åc, Singapore.
32. NeÀácas, J. (1967) Les M√©thodes Directes en la
Th√©orie des Equations Elliptiques,
Academia & Masson, Praha & Paris;
(English Translation: Springer, Berlin, 2012).
33. Morrey, C.B. Jr. (1966) Multiple Integrals in
the Calculus of Variations, Springer, Berlin.
34. ≈†ver√°k, V. (1992) Rank-one convexity does
not imply quasiconvexity. Proc. R. Soc.
Edinburgh Sect. A, 120, 185‚Äì189.
35. M√ºller, S. (1989) A surprising higher
integrability property of mappings with
positive determinants. Bull. Am. Math. Soc.,
21, 245‚Äì248.
36. Ball, J.M. (1977) Convexity conditions and
existence theorems in nonlinear elasticity.
Arch. Ration. Mech. Anal., 63 (4), 337‚Äì403.
37. Ekeland, I. and Temam, R. (1976) Convex
Analysis and Variational Problems,
North-Holland.
38. Lavrentiev, A. (1926) Sur quelques
probl√©mes du calcul des variations. Ann.
Mat. Pura Appl., 41, 107‚Äì124.
39. Carstensen, C. and Ortner, C. (2010)
Analysis of a class of penalty methods for
computing singular minimizers. Comput.
Meth. Appl. Math., 10, 137‚Äì163.
40. Ball, J.M. and Mizel, V.J. (1985)
One-dimensional variational problems
whose minimizers do not satisfy the
Euler-Lagrange equation. Arch. Ration.
Mech. Anal., 90, 325‚Äì388.
41. Baiocchi, C. and Capelo, A. (1984)
Variational and Quasivariational
Inequalities, John Wiley & Sons, Ltd,
Chichester.
42. Duvaut, G. and Lions, J.-L. (1976)
Inequalities in Mechanics and Physics,
Springer, Berlin.
43. Hlav√°Àácek, I., Haslinger, J., NeÀácas, J., and
Lov¬¥ƒ±≈°ek, J. (1988) Solution of Variational
Inequalities in Mechanics, Springer, New
York.
44. Kinderlehrer, D. and Stampacchia, G. (1980)
An Introduction to Variational Inequalities
and their Applications, Academic Press,
New York.
45. Ciarlet, P.G. and NeÀácas, J. (1987) Injectivity
and self-contact in nonlinear elasticity. Arch.
Ration. Mech. Anal., 97, 171‚Äì188.
46. Ball, J.M. (2002) Some open problems in
elasticity, in Geometry, Mechanics, and
Dynamics (ed. P. Newton, P. Holmes, A.
Weinstein), Springer, New York, pp. 3‚Äì59.
47. Foss, M., Hrusa, W.J., and Mizel, V.J. (2003)
The Lavrentiev gap phenomenon in
nonlinear elasticity. Arch. Ration. Mech.
Anal., 167, 337‚Äì365.
48. Ciarlet, P.G. (1988, 1997, 2000)
Mathematical Elasticity, Vol.I:
Three-Dimensional Elasticity, Vol.II: Theory
of Plates, Vol.III: Theory of Shells,
North-Holland, Amsterdam.
49. Eringen, A.C. (2002) Nonlocal Continuum
Field Theories, Springer, New York.
50. Evans, L.C. (1998) Partial DiÔ¨Äerential
Equations, AMS, Providence, RI.
51. Struwe, M. (1990) Variational Methods:
Applications to Nonlinear Partial
DiÔ¨Äerential Equations and Hamiltonian
Systems, Springer, Berlin.
52. Hlav√°Àácek, I. (1969) Variational principle for
parabolic equations. Apl. Mat., 14, 278‚Äì297.
53. Ghoussoub, N. (2009) Self-Dual Partial
DiÔ¨Äerential Systems and Their Variational
Principles, Springer, New York.
54. Ritz, W. (1908) √úber eine neue Methode zur
L√∂sung gewisser Variationsprobleme der
mathematischen Physik. J. Reine u. Angew.
Math., 135, 1‚Äì61.
55. Galerkin, B.G. (1915) Series development for
some cases of equilibrium of plates and
beams (In Russian). Vestnik Inzhinierov
Teknik, 19, 897‚Äì908.
56. Christara, C.C. and Jackson, K.R. (2005)
Numerical methods, in Mathematical Tools

588
17 Calculus of Variations
for Physicists (ed. G.L. Trigg), John Wiley &
Sons, Inc., Weinheim, pp. 281‚Äì383.
57. NeÀácas, J. (1977) Example of an irregular
solution to a nonlinear elliptic system with
analytic coeÔ¨Écients and conditions of
regularity, in Theory of Nonlinear Operators
Proceedings of Summer School (Berlin 1975)
(eds W. Muller, Berlin, Akademie-Verlag, pp.
197‚Äì206.
58. ≈†ver√°k, V. and Yan, X. (2000) A singular
minimizer of a smooth strongly convex
functional in three dimensions. Calc. Var.,
10, 213‚Äì221.
59. Roub√≠Àácek, T. (1997) Relaxation in
Optimization Theory and Variational
Calculus, W. de Gruyter, Berlin.
60. Young, L.C. (1969) Lectures on the Calculus
of Variations and Optimal Control Theory,
W.B. Saunders, Philadelphia, PA.
61. Pedregal, P. (1997) Parametrized Measures
and Variational Principles, Birkh√§user, Basel.
62. Pedregal, P. (2000) Variational Methods in
Nonlinear Elasticity, SIAM, Philadelphia,
PA.
63. Kru≈æ¬¥ƒ±k, M. and Prohl, A. (2006) Recent
developments in the modeling, analysis, and
numerics of ferromagnetism. SIAM Rev., 48,
439‚Äì483.
64. Ball, J.M. and James, R.D. (1987) Fine phase
mixtures as minimizers of energy. Arch.
Ration. Mech. Anal., 100, 13‚Äì52.
65. Bhattacharya, K. (ed.) (2003) Microstructure
of MartenSite. Why it Forms and How it
Gives Rise to the Shape-Memory EÔ¨Äect,
Oxford University Press, New York.
66. M√ºller, S. (1999) Variational models for
microstructure and phase transitions,
Calculus of Variations and Geometric
Evolution Problems, Springer, Berlin, pp.
85‚Äì210.
67. De Giorgi, E. (1977.) Œì-convergenza e
G-convergenza. Boll. Unione Mat. Ital., V.
Ser., A 14, 213‚Äì220.
68. Attouch, H. (1984) Variational Convergence
of Functions and Operators, Pitman.
69. Braides, A. (ed.) (2002) Œì-Convergence for
Beginners, Oxford University Press, Oxford.
70. Dal Maso, G. (1993) An Introduction to
Œì-Convergence, Birkh√§user, Bostonm, MA.
71. Friesecke, G., James, R.D., and M√ºller, S.
(2006) A hierarchy of plate models derived
from nonlinear elasticity by
Gamma-convergence. Arch. Ration. Mech.
Anal., 180, 183‚Äì236.
72. Bonnans, J.F. and Shapiro, A. (eds) (2000)
Perturbation Analysis of Optimization
Problems, Springer-Verlag, New York.
73. Clarke, F.H. (1983) Optimization and
Nonsmooth Analysis, Wiley.
74. Mordukhovich, B.S. (2006) Variational
Analysis and Generalized DiÔ¨Äerentiation I,
II, Springer, Berlin.
75. Mal¬¥y, J. and Ziemer, W.P. (1997) Fine
Regularity of Solutions of Elliptic Partial
DiÔ¨Äerential Equations, American
Mathematical Society, Providence, RI.
76. Minty, G. (1963) On a monotonicity method
for the solution of non-linear equations in
Banach spaces. Proc. Natl. Acad. Sci. U.S.A.,
50, 1038‚Äì1041.
77. Lions, J.L. (1969) Quelques M√©thodes de
R√©solution des Probl√©mes aux Limites non
lin√©aires, Dunod, Paris.
78. Showalter, R.E. (1997) Monotone Operators
in Banach Space and Nonlinear Partial
DiÔ¨Äerential Equations, AMS.
79. Stampacchia, G. (1965) Le probl√®me de
Dirichlet pour les √©quations elliptiques du
second ordre √† coeÔ¨Écients discontinus. Ann.
Inst. Fourier, 15, 189‚Äì258.
80. Lions, J.L. and Magenes, E. (1968) Probl√®mes
aux Limites non homoeg√®nes et Applications,
Dunod, Paris.
81. Vainberg, M.M. (1973) Variational Methods
and Method of Monotone Operators in the
Theory of Nonlinear Equations, John Wiley
& Sons, Inc., New York.
82. Boccardo, L. and Gallouet, T. (1989)
Non-linear elliptic and parabolic equations
involving measure data. J. Funct. Anal., 87,
149‚Äì169.
83. BoltyanskiÀòƒ±, V.G., Gamkrelidze, R.V., and
Pontryagin, L.S. (1956) On the theory of
optimal processes (In Russian). Dokl. Akad.
Nauk USSR, 110, 7‚Äì10.
84. Hestenes, M.R. (1950) A general problem in
the calculus of variations with applications
to the paths of least time. Technical Report
100, RAND Corp., Santa Monica, CA.
85. IoÔ¨Äe, A.D. and Tikhomirov, V.M. (1979)
Theory of Extremal Problems,
North-Holland, Amsterdam.
86. Tr√∂ltzsch, F. (2010) Optimal Control of
Partial DiÔ¨Äerential Equations, American
Mathematical Society, Providence, RI.

589
Index
a
abel transform
514
abelian functions
274, 275, 278
abelian groups
358
absolute error
477
abstract Fourier series transformation
506
abstract parabolic equation
556, 583
acceptance frequency
48
accumulation point
456, 464
accuracy
‚Äì computer algebra
291
‚Äì dynamical systems
391
‚Äì perturbation solutions
415
Adams methods
493, 495
Adams‚ÄìBashforth methods
493
Adams‚ÄìMoulton methods
493, 494
addition formulas
173, 258, 276
addition laws
254, 258, 273‚Äì275, 278, 285
addition theorem
206
additive noise
74, 85, 92, 94, 96
adjacency matrix of a simple graph
154
adjacency operator
113
adjoint operator
457, 458, 555, 585
adjoint system
585
advanced algorithms
294‚Äì301
aÔ¨Éne reÔ¨Çection groups
244
Airy functions
268, 269, 283, 495
‚Äì turning point problem
434
Airy‚Äôs equation
495
algebra of tensors
337, 338
algebraic entropy
286
algebraic equations
‚Äì iteration methods
476‚Äì480
‚Äì transformations
504, 505
alpha complex
235
Ambrosetti‚ÄìRabinowitz mountain pass theorem
554
analytic function
10, 453, 510
Andronov‚ÄìHopf bifurcation
393
angle-preserving map
503
angular momentum
171, 177, 180, 189‚Äì191,
194‚Äì197
angular standard form
424
annihilation operators
133, 179, 180, 198, 532
anticommutation
179
approximation
‚Äì numerical
80, 93, 95, 476‚Äì480, 576
‚Äì of continuous data
476, 487Ô¨Ä
area-preserving map
503
Arnol‚Äôd diÔ¨Äusion
425
Arnol‚Äôd unfolding
444
Arrhenius‚Äô law
103
Askey‚ÄìWilson diÔ¨Äerence operator
284
associated Legendre functions
160, 202, 255,
256, 260, 264
asymptotic approximation
415, 416, 431, 444
asymptotic matching
438
asymptotic series
416, 444, 445
asymptotic stability
387, 389
asymptotic validity
415, 416
atlas
321‚Äì324, 327, 363
‚Äì adapted
363
atomic shell structure
195
attempt frequency
48
attractor
‚Äì chaotic
405
‚Äì strange
228, 386, 429
attrition problem
45
autocovariance
75
automorphism
‚Äì galois theory
271, 272
‚Äì Lie-group
358
averaging
422‚Äì424
‚Äì self-averaging
45
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
¬© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

590
Index
b
B√§cklund maps
282, 286
backward Fokker‚ÄìPlanck equation
102, 103
backward kolmogorov equation
24, 25
backward/forward substitution
481
Bak‚ÄìSneppen model
17
balance equation
12, 356
balance laws
356‚Äì358
‚Äì diÔ¨Äerential expression of the general balance
law
358
‚Äì nonlinear heat-transfer problem
571
balance principle
50, 68
ballot theorem
20
Banach space
449‚Äì451, 551, 562
‚Äì ordered
555
‚Äì reÔ¨Çexive
553
band-limited function
517
banded matrices
481
barcode
233
basin of attraction
387
Bernoulli distribution
35
Bessel equation
206, 255, 262, 270, 271
Bessel functions
202, 240, 256‚Äì258, 268, 270
‚Äì Hankel transformation
523
Betti number
221‚Äì223, 228, 232, 236
betweenness centrality
143, 149, 154
Bezout‚Äôs identity
296, 300, 301
Bianchi identity
372
biased sampling
45
bifurcation point
390, 392, 394, 395, 404‚Äì407
bifurcations
411, 418, 445
‚Äì hopf
392‚Äì396, 399, 404, 428
‚Äì imperfect
414
‚Äì local
391, 392, 404
‚Äì Pitchfork
392, 394, 396‚Äì398, 404
‚Äì saddle-node
392‚Äì398, 406‚Äì408
binomial distribution
35
binomial theorem
5, 15, 285
Biot system
545
Biot‚ÄìSavart formula
535
Birman‚ÄìKuroda theorem
472
Birman‚ÄìSchwinger bound
469
birth-and-death process
24
Black‚ÄìScholes PDE
102
Bochner space
557
Boltzmann constant
89, 121
Bolzano‚ÄìWeierstra√ü theorem
553
bond percolation
31, 32
boole‚Äôs inequality
21
Borel set
465
Borel‚ÄìCantelli lemma
8, 92
Boson annihilation operator
179
boundary conditions
52‚Äì56, 103, 411
‚Äì weierstra√ü elliptic functions
276
boundary critical exponent
563
boundary layer
412, 417, 426, 429‚Äì434, 436‚Äì
439,
445
boundary operator
220‚Äì225, 228, 231, 232
boundary point
224
boundary value problem (BVP)
400, 401,
564‚Äì566, 569‚Äì571
‚Äì classical formulation
564, 569
bounded operators
450, 452, 454‚Äì457, 460, 464,
547
Bragg position
67
branching process
7, 8, 33
Brezis‚ÄìEkeland‚ÄìNayroles principle
557, 575
bridge
122, 129, 154, 212
Brillouin zone
67
‚Äì Ô¨Årst
170
Brownian motion
‚Äì approximation
80
‚Äì geometric
86, 88, 89
‚Äì Monte-Carlo methods
50
‚Äì stochastic diÔ¨Äerential equations
75‚Äì80, 100
‚Äì stochastic process
19
Brute-force bifurcation diagrams
405
bundle, associated
362, 364‚Äì366, 370, 372, 374
burgers‚Äô circuit
332
Butcher array
494, 495
c
canard
433
canonical ensemble
47, 52, 64
canonical transformations
426
carathe√©odory mapping
563
Cartan‚Äôs structural equation
372
Cartan‚Äìkilling metric equation
178
Cauchy equation
272
Cauchy problem
491, 492, 495, 496, 500,
556‚Äì559
‚Äì doubly nonlinear
558
Cauchy sequence
450, 552
Cauchy‚Äôs formula
4, 356, 357, 492
Cauchy‚Äôs postulates
355, 357
Cayley multiplication table
167
ÀáCech complex
234
cellular homology
224
center manifolds
390, 393, 395, 397‚Äì399
central limit theorem
9, 10, 75
centrality measure
143, 145, 146, 149, 154
chain groups
219, 221, 222, 224, 225, 228, 231
Chapman‚ÄìKolmogorov equation
23, 99
Chapman‚ÄìKolmogorov relation
17, 22, 23
character table
188, 190, 191
character-class duality
187
characteristic function
10

Index
591
chart
321, 322
‚Äì adapted
363
Chebyshev polynomials
402
Chebyshev‚ÄìGauss‚Äìlobatto nodes
488
chernoÔ¨Äbound
21
ChristoÔ¨Äel symbols
184, 185, 374, 377, 380
Ciarlet‚ÄìNeÀácas condition
572, 584
circular functions
273, 277, 278
Ck-diÔ¨Äerentiable structure
324
class, group theory
168
classical formulation of variational problem
564,
569
classical Fourier series transformation
509
classical mechanics
133, 160, 183, 346, 363
classical solution
564, 577
clique
29, 30, 114, 234
clique complex
234
closed walk (CW)
114, 146
closed-graph theorem
451, 452, 458
closeness centrality
143, 154
cluster algorithms
49
clustering coeÔ¨Écient
115, 135, 139, 140, 142, 154
cochain
224‚Äì226
codimension one bifurcations
392
coeÔ¨Écient group
220, 221, 224, 225
coeÔ¨Écients, diÔ¨Äerential
272, 507, 522
cohomology
212, 224‚Äì226
collocation
400‚Äì402, 406
colored noise
77‚Äì80
communicability
138, 147, 148, 154
commutative groups
188
commutativity of self-adjoint operators
472
compact embedding theorem
563
compact mapping
563
compact operators
456, 464, 472
compact set
452, 519
compact trace operator theorem
563
compactiÔ¨Åcation
578
‚Äì convex
578, 579
complementarity problem
555, 574
completeness relations
186‚Äì188
complex coordinate transformation
509
complex networks
111, 115, 137, 138, 140, 143,
148
complex systems
63, 67, 111, 137, 138, 150
complex variables
255, 279, 428, 439
complexity
39, 137, 138, 293, 568
composite solution
430‚Äì432, 445
computational topology
211, 212, 230‚Äì236
computer simulations
26, 40, 216
condensed matter
40, 62‚Äì67, 111, 115‚Äì120, 217
conditional expectation
36, 37
conditional probability
11, 26, 31, 35, 37, 46
conÔ¨Çuent hypergeometric functions
260
conformal symmetry
194
conjugate
‚Äì Hermitean
61
‚Äì legendre
557
conjugate gradient (CG) method
479, 480, 484,
485
Conley index
228
connected topological space
320
connections
367‚Äì380
‚Äì principal bundle
369, 370
consensus dynamics
151
conservation laws
51, 356, 388
conservative forces
357
conservative oscillator
421
conservative systems
467
consistency
49, 325, 365, 492, 495
‚Äì of multistep methods
493
constrained minimization problems
555
constraint qualiÔ¨Åcation
‚Äì mangasarian‚Äìfromovitz
555
‚Äì slater
556
continuity
320
‚Äì of probability measure
8, 16, 32
continuous groups
165, 171, 250
continuous maps
214, 224, 321, 322, 324, 462
continuous symmetry
240, 250‚Äì261
continuously diÔ¨Äerentiable
89, 504, 552, 554, 577
contour integrals
255
contravariant tensor algebra
338
control theory
586
Œì-convergence
581, 582, 584
convergence
‚Äì around Ô¨Åxed points
478
‚Äì higher order
477
‚Äì linear
406, 477
‚Äì numerical methods
92, 98
‚Äì quadratic
477
‚Äì strong
92, 94, 96, 98, 462
‚Äì weak
92, 95, 96, 453, 553
‚Äì weak‚àó
553,
579
convergence analysis
477, 478
convex mapping
556, 583
convolution
6, 7, 512‚Äì514
‚Äì integral
513, 514
‚Äì theorem
6, 27, 514
coordinate perturbations
414, 428
corrected trapezoidal formula
489
correlation functions
51, 59, 79
coset
168, 209
cotangent bundle
327, 328, 341, 342, 345, 346,
361, 364, 380
cotangent space
326, 327, 349, 380
coulomb potential
192, 193, 195, 196, 472
countable topology
320

592
Index
counting function
18
coupled systems
266, 544, 545, 547
covariance
75‚Äì80, 88, 105, 184
covariant derivative
372, 374‚Äì376, 380
covariant tensor algebra
338
covector
326
covering space
216, 217, 230
Coxeter group
243, 244, 246
Cramer‚Äôs rule
480
creation operators
179, 198, 532
critical point
552
critical slowing down
49, 55, 68
criticality
43, 394, 397
cross product
173
crystal Ô¨Åeld theory
190
cubical homology
224
cumulants
53, 54
curl
343
curvature form
372, 375, 376
curvature tensor
376, 380
cyclic groups
168, 188, 216, 221
cyclomatic number
124, 216
cylinders
215, 216, 256
cylindrical coordinates
254
d
d-tuples
19, 41
damping
131, 413, 414, 561
Darboux transformation
267
De Moivre‚ÄìLaplace central limit theorem
9
De Rham cohomology
225, 226
De Rham‚Äôs theorem
226
deck transformation group
217
deÔ¨Åciency indices
461, 462, 472, 473
deÔ¨Åciency subspace of t
472
deÔ¨Ånite integrals, numerical approximation
489
degree distribution
138‚Äì142, 154, 155
Delaunay complex
235
delta function
78, 186
dense model
293, 295
densely deÔ¨Åned operators
457‚Äì460, 521, 537,
538
density matrices
457, 472
density of an extensive property
355
deposition, diÔ¨Äusion, and adsorption model
27‚Äì29
derivative
‚Äì covariant
374, 375, 380
‚Äì exterior covariant
372, 376
descent direction method
479, 484
detailed balance condition
48
detailed balance principle
50, 68
determinant, Jacobian
324, 343, 353, 505
diÔ¨Äeomorphism
324, 359
‚Äì between Ô¨Åbers
367
‚Äì Lie-group isomorphism
358
diÔ¨Äerence operators
284
diÔ¨Äerentiable manifolds
323‚Äì328
diÔ¨Äerentiable maps
324, 328, 329, 346
diÔ¨Äerential
‚Äì 0-form
342
‚Äì k-form
226, 342
‚Äì forms
341‚Äì344
diÔ¨Äerential Galois theory
268, 271
diÔ¨Äerential topology
211, 216, 226
diÔ¨Äusion-limited aggregation (DLA)
40, 46, 47
digital images
224, 236
dilation
467, 491
dimension of a vector space
325
ùõø-distribution. See dirac-ùõø-distribution
dirac-ùõø-distribution
78, 79, 516, 517, 522, 531,
541
direct product
244, 256, 454
direct solutions
399
direct sum
186, 221, 454, 530
directionally diÔ¨Äerentiable
552
Dirichlet condition
461, 564, 569
Dirichlet type boundary condition
519
discrete Fourier transformation
508, 522
discrete groups
159, 164, 166‚Äì170
discrete Painlev√© equations
286
discrete special functions
283, 285
discrete spectrum
464, 469, 470, 472
discrete symmetry
241, 243, 245, 247, 249, 286
discrete transformations
522
discrete-time maps
402
discretization
93, 97, 98, 560, 561, 576
disentangling theorem
164, 180, 206
dislocations
331, 332, 334
disordered series
417
dissipation
73, 89, 558, 560, 583
distribution
515‚Äì517, 531‚Äì537
‚Äì horizontal
368‚Äì371, 374
divergence
343, 543
divided diÔ¨Äerence
487, 488
doubly nonlinear cauchy problem
558
doubly nonlinear inclusion
559
drift correction
90
dual problem
556, 576
dual space
170, 326, 335, 341, 451, 552, 583
duality pairing
524
DuÔ¨Éng equation
388, 391, 400, 419
Dunkl Laplacian
267
Dunkl operators
243, 267
dynamic models
68
dynamic richardson method
483
dynamical groups
160, 194‚Äì199, 207, 209
dynamical models
198

Index
593
dynamical similarity
159, 160, 163
dynamical systems
11, 385Ô¨Ä., 418Ô¨Ä.
e
eccentricity
114, 426, 476, 503
edge contraction
121, 128, 154
edge deletion
122, 128, 154
edge-path group
230, 231
Ehrenfest model
11, 15
Ehresmann connection
368, 369
eigenspace
452
eigenvalues
‚Äì for Ô¨Ånite-dimensional linear systems
480‚Äì487, 498
‚Äì iteration method for computing
478
‚Äì of Ô¨Ånite matrices, computing
485
‚Äì perturbations of matrices and spectra
442‚Äì444
Einstein‚Äôs summation convention
326
Ekeland variational principle
584
elasticity
528, 543, 572, 573, 581
electrical networks
33, 111, 129
electrodynamics
111, 182, 183, 207, 226
electromagnetism
199, 323, 528
embedded submanifold
370
endomorphisms
251, 252
energetic formulation
560
energy equality
560
energy levels
134, 195, 207, 262
enrichment technique
45
epidemics on networks
153
epigraph convergence
581
epimorphism
223
equation
‚Äì algebraic
476‚Äì479
‚Äì cauchy
272
‚Äì kepler‚Äôs
476
‚Äì scalar
476, 477
‚Äì weak
497
equilibria
386‚Äì400, 402‚Äì406, 408, 584
equilibrium states
131, 387
equivalence of linear Ô¨Çows
389, 390
equivalence relation
325, 366
ergodicity
51, 68
error
‚Äì absolute
477
‚Äì dynamic correlation
52‚Äì56
‚Äì global truncation
492
‚Äì Hermite interpolation
489
‚Äì interpolation
487, 489
‚Äì local truncation
491
‚Äì ‚Äìfor multistep methods
493
‚Äì multilevel Monte Carlo
97
‚Äì residual vector
483, 484
‚Äì sensitivity analysis
499
‚Äì vector
478
essential spectrum
458, 464, 470, 472, 473, 500
essentially self-adjoint operator
472
Euclid‚Äôs algorithm
294
Euler characteristic
212, 222, 228, 230
Euler‚ÄìLagrange equation
552, 554, 564, 566,
569, 570, 572, 577, 581
Euler‚ÄìMaruyama method
93‚Äì97
event
34
evolution operator
385, 386, 467, 470
excited states
133
existence and uniqueness for SDEs
84
expanding interval
417, 421, 429, 430
expectation
35‚Äì37
explicit formula
413
exponential distribution
9, 25, 46
extended Euclidean algorithm
296
extended Maxwell‚Äôs equations
547f.
extensive property
355, 356
exterior algebra
338, 339, 341
exterior covariant derivative
372, 376
exterior derivative
226, 343f., 350, 372, 376f.
‚Äì covariant
372, 376
extrema
457, 485, 551
f
factorial function
259
factorization
240, 261‚Äì268, 296‚Äì300
‚Äì LU
481
‚Äì table of most common methods
482
faithful representation
185, 209
Fast-Fourier transform (FFT)
509
Feigenbaum‚Äôs constant
405
Fenchel inequality
557‚Äì559
Fermions
59‚Äì61, 179
Ferrers formulae
255
feynman diagrams
124
feynman graphs
124‚Äì128
feynman path integrals
467
Ô¨Åber
216, 327, 361, 365
Ô¨Åber bundle
327, 361‚Äì372
Ô¨Åber-preserving map
329
Fibonacci series
41
Ô¨Åltered probability space
82
Ô¨Åltering
103, 104
Ô¨Ålters, signal processing
512
Ô¨Åltration
82, 233‚Äì235
Ô¨Ånite groups
164, 166‚Äì169, 207, 252
Ô¨Ånite-dimensional linear systems
480‚Äì485
Ô¨Ånite-element method
489, 577, 584
Ô¨Ånite-size problems
52, 53, 55
Ô¨Ånite-size Scaling
53, 68

594
Index
Ô¨Årst Lyapunov coeÔ¨Écient
394, 395
Ô¨Årst variational equation
402‚Äì404
Ô¨Åxed point
8, 85, 228, 242, 402, 403, 476‚Äì478
‚Äì iteration
476, 477
‚Äì iteration schemes
476
‚Äì local convergence
478
Ô¨Çavors
62
Ô¨Çoating points
62, 480
Floer homology
229
Flops
480‚Äì482
Floquet multipliers
402‚Äì404
Flory‚ÄìHuggins theory
64, 65
Ô¨Çuctuation-dissipation relation
89
Ô¨Çuctuations
51, 52
Ô¨Çuid mechanics
434‚Äì436
‚Äì 4He momentum distribution
59
Ô¨Çux density
356, 357
Fokker‚ÄìPlanck equation
24, 99‚Äì103, 536
foliations
428
forest
115, 123, 125, 127, 154, 155
1-form
341
‚Äì canonical
346, 373, 374
formulation
‚Äì classical
564, 569, 571, 574, 577
‚Äì de giorgi
559, 560
‚Äì energetic
560
‚Äì mixed
575
‚Äì weak
564, 565, 569‚Äì571, 574, 577
forward Chapman‚ÄìKolmogorov equation
99
forward Euler method
493, 496
forward/backward substitution
481
Fourier expansion
453, 456, 472, 506, 507
Fourier series transformation
506‚Äì511, 522
Fourier sine/cosine transformation
518
Fourier transform
59, 79, 451
Fourier‚Äìlaplace transformation
510‚Äì518, 521,
522, 529‚Äì531, 533, 534
Fourier‚ÄìPlancherel operator
451, 459, 460, 464,
468, 472
Fourier‚ÄìPlancherel transformation
504,
510‚Äì512, 515, 517, 518, 520, 522, 523
fourth order systems
568
fractals
47, 228
fractional power series
443
fractional-step method
561, 562
fr√©chet subdiÔ¨Äerential
554, 555
Fredholm alternative
456
Fredholm integral equation
40
Frobenius
‚Äì expansion
250, 280
‚Äì method
248, 258, 259
‚Äì norm
76, 176
‚Äì notation
245, 246
‚Äì theorem
370‚Äì372
Frobenius‚ÄìStickleberger relations
275
Fuchsian equations
247‚Äì250
function
‚Äì analytic
10, 453, 510
‚Äì characteristic
10
fundamental group
213‚Äì218, 221, 230, 231
fundamental solution
517, 535
fundamental theorem
‚Äì for vector Ô¨Åelds on manifolds
347
‚Äì of algebra
301
‚Äì of calculus
312
fundamental vector Ô¨Åeld
360, 369, 370, 372, 375
g
Galerkin method
497, 498, 576
galois groups
271, 272
galton‚Äìwatson process
7
gambler‚Äôs ruin
14
gamma function
259
g√¢teaux diÔ¨Äerential
552, 583
gauge theory
61, 62, 160, 199‚Äì201, 207‚Äì210,
236
gauges
415‚Äì417, 445
Gauss‚ÄìBonnet theorem
222
Gauss‚Äìseidel method
483
Gaussian distribution
9, 53, 515
Gaussian elimination
480, 481
general topology
211
generalized series
415‚Äì417, 420, 421, 445, 446
generalized weyl theorem
464
generating function
426
‚Äì convolution
7, 17
‚Äì Hermite polynomials
206, 207
‚Äì moment generating function
9, 10, 21
‚Äì probability generating function
5, 25
‚Äì special functions
202, 245, 262, 263, 282, 283,
285, 286
‚Äì stochastic process
3‚Äì10, 14‚Äì17, 21, 24, 25, 33
‚Äì uniqueness
5, 7, 9
generator of a group
168
genus
216
geometric Brownian motion
86‚Äì88, 90, 91, 93,
95, 102
geometric realization
219
geometric singular perturbation
433
Giambelli formula
246
Gibbs canonical distribution
89
Gibbs ensemble
52
Gibbs random Ô¨Åeld
29‚Äì31
Gillespie‚Äôs algorithm
25, 26
girth
114, 118, 154
global bifurcation
392

Index
595
global Ô¨Çow
347
global minimizer
479, 553
global solution of a cauchy problem
491
global stability
495, 496
global truncation error
492
good reduction theorem
297, 298
google pagerank
145, 483
gradient Ô¨Çow lines
227
gradient method
479, 480, 484
gram‚Äìschmidt orthogonalization
202
grand canonical ensemble
52, 64
graph
‚Äì bipartite
115, 117‚Äì119, 154
‚Äì complete
114, 115, 123, 147, 148, 154
‚Äì connected
114, 118, 119, 150, 154, 215
‚Äì cycle
154
‚Äì directed
112, 114
‚Äì disconnected
115
‚Äì Erd√∂s‚Äìr√©nyi
154
‚Äì formal deÔ¨Ånition
112
‚Äì invariant
123, 154
‚Äì isomorphic
114, 138
‚Äì nullity
154
‚Äì planar
115
‚Äì random
134‚Äì136, 138, 154, 155
‚Äì regular
115, 138, 155
‚Äì simple
112, 113
‚Äì star
155
‚Äì trivial
115, 123
‚Äì undirected
112‚Äì114, 151
‚Äì weighted
112, 126, 130
graph diameter
114, 154
graph theory
111‚Äì137, 147
greatest common divisor (GCD)
294
Green‚Äôs functions
133, 134, 253, 517
Green‚Äôs tensor
533‚Äì535
Green‚ÄìLagrange strain tensor
573
Gr√∂bner bases
304‚Äì308
group action
241, 243, 358, 359, 365, 366
group axioms
165
group consistency condition
365
group element-matrix element duality
186, 187
group of boundaries
220
group of cycles
220
group theory
159‚Äì210
‚Äì symmetries
271
h
Hadamard conditions
529, 566, 583
Hadamard transformation
522
Hamilton principle
576
Hamilton variational principle
559
Hamilton‚ÄìJacobi theory
426
Hamiltonian equation
116, 131‚Äì133
Hamiltonian function
328, 347, 425
Hamiltonian operator
42, 47, 58, 191, 193,
467‚Äì472
Hamiltonian systems
346, 347, 424‚Äì426
Hamiltonian vector Ô¨Åeld
346
Hammersley‚ÄìCliÔ¨Äord theorem
31
Hankel determinants
261
Hankel transformation
523
harmonic balance method
399, 400
harmonic functions
402
harmonic oscillators
131, 164, 195‚Äì197, 207,
262, 515, 532
harmonic polynomials
536
Harris‚ÄìKesten theorem
33
Hartley transformation
519, 520
Hartman‚ÄìGrobman theorem
389‚Äì391, 398,
403, 404
HausdorÔ¨Äspace
320
‚Äì topology
320
heat transport
545
heat-bath method
49, 68
Heisenberg group
171, 179, 181, 204
Heisenberg representation
180
helium
486
Hellinger‚ÄìToeplitz theorem
458
Hermite elements
490
Hermite interpolation
488, 489
Hermite quadrature formulae
489
Hermitean conjugate
61
Hermitean operator
454, 455, 457, 467, 472
hermitian matrices
486
heteroclinic orbits
386
Heun method
91, 96, 97
higgs mechanism
201
higher-dimensional transformations
521, 522
Hilbert space
450, 452‚Äì456, 472, 505‚Äì507, 530,
552, 562
Hilbert space geometry
452
Hilbert transformation
513, 525
Hilbert‚ÄìSchmidt operator
456, 457, 473
Hilbert‚ÄìSchmidt theorem
456
histogram method
52
homoclinic orbits
386, 418, 429
homogeneous systems
478
homological algebra
222‚Äì224
homology
218‚Äì224
homomorphism
166, 212, 358
homotopy 212Ô¨Ä.
Hook diagrams
246
Hopf bifurcation
392‚Äì396, 399, 404, 428
horizontal distribution
368‚Äì371, 374
Horner‚Äôs scheme
508
Hubbard model
118‚Äì120
H√ºckel molecular orbital method
116

596
Index
Hunziker-van Winter‚ÄìZhislin theorem
470
hydrocarbon
117‚Äì119, 154
hyperbolic equilibrium
387, 390, 403
hyperelastic material
572
hypergeometric equation
258
hypergeometric functions
259, 260
hyperplane
243‚Äì247
hypersurface
425, 567
i
identity element
165, 209, 214, 358
identity transformations
359, 422, 426, 445
IEEE standard, Ô¨Çoating-point numbers
291
imperfect bifurcations
414
implicit Euler formula
560, 561, 584
importance sampling
42, 45, 47‚Äì52, 56, 60, 62,
68, 69
incidence matrix
113, 118, 154
inclusion‚Äìexclusion property
222
incremental function
491, 492
independent events
35
independent variables 5Ô¨Ä.
independent vectors
363, 449, 462
indeterminates
242, 292
indicator function ùõøk
555
indicator variable
18, 36, 37
indicial equation
248, 258
induced rounding
52, 68
inÔ¨Ånite discrete groups
159, 169
inÔ¨Ånite groups
214, 242
inÔ¨Ånite-dimensional min-max principle
497
inÔ¨Ånitesimal rate
23
initial-boundary value problems
540‚Äì544
initial layer
429, 431, 436, 437, 445
initial value problem
429, 491‚Äì496, 541, 556
inner solution
430‚Äì435, 445
inner-product spaces
377‚Äì379, 450, 472
innocuous polynomials
294
integral transform
4‚Äì10
integration
‚Äì by parts
87
‚Äì in computer algebra
308‚Äì312
‚Äì Monte Carlo methods
43, 44
‚Äì of n-forms in ‚Ñùn 353‚Äì355
interacting boson model (IBM)
199
interfaces
52, 54, 55, 57
intermediate variable
431
interpolation
93, 476, 487‚Äì490, 493
‚Äì Hermite
488‚Äì490
‚Äì Lagrange
487
‚Äì nodes
487‚Äì490, 493
‚Äì piecewise polynomial
489‚Äì491
interpolation error function
487
invariant density
101
invariant manifolds
390, 429, 433
invariant sets
228, 386, 387, 389, 393, 405‚Äì407
inverse fourier transformation
517
inverse power method
478, 486
inverse radon transform
525
inverse shifted power method
486
inverse transformation
504, 505, 507, 510, 530
inverse-mapping theorem
451
inversely restricted sampling
45
irreducibility
15
irreducible representation
179, 186‚Äì192, 244,
246
irreps
186, 188‚Äì190, 196
Ising model
30, 55, 63, 120
isolated point
234, 464
isomorphic groups
231
isomorphism
166, 358
isospins
201
isothermal-isobaric ensemble
52
iteration matrix
482
iteration stationary methods
483
It√¥ formula
86‚Äì90, 92, 99, 102
It√¥ integral
81‚Äì85, 88, 90, 92‚Äì94, 99, 102
‚Äì martingale property
83
It√¥ isometry
83, 88
It√¥ SDEs
84, 86, 88‚Äì93, 95, 98, 101, 102
j
Jacobi identity
175, 208, 251, 331
Jacobi polynomials
261
Jacobi‚ÄìTrudi formulae
245
Jacobian determinant
324, 343, 353
Jacobian elliptic functions
277
Jacobian matrix
184, 329, 387, 388, 395, 401,
402, 406, 408
jansen formula
258
joins
115, 228
jordan normal form
398, 411, 442, 444
jump rate, Markov dynamics
27
k
Kalman‚ÄìBucy Ô¨Ålter
103, 104
Karhunen‚ÄìLo√®ve expansion
80
Karush‚ÄìKuhn‚ÄìTucker condition
575, 583, 585
Kato theorem
469
Kato‚ÄìRellich theorem
459, 469
Kelvin‚ÄìVoigt model
543, 544
Kepler‚Äôs law
160, 412
KirchhoÔ¨Ätransformation
571, 584
Kolmogorov‚ÄìArnol‚Äôd‚ÄìMoser (KAM) theorem
412, 425
Kolmogorov backward equation
24, 25
Kolmogorov diÔ¨Äerential equations
24
Korn inequality
573, 584

Index
597
Korteweg‚Äìde vries (KDV) equation
276
Krylov‚ÄìBogoliubov‚ÄìMitropolski (KBM) method
422
l
lagrange Ô¨Ånite elements of order
1 498
lagrange form
487
lagrange interpolating polynomial
487, 488
lagrange interpolation
487
lagrangean
555, 575
lagrangian mechanics
328, 378, 380
laguerre polynomials
453
lamb shift
486
Lam√© equations
277
Lam√© system
573
Landau‚ÄìMignotte bound
297, 300, 301
Langevin equation
85, 101‚Äì103
Laplace equation
253, 255
‚Äì separation of variables
256
Laplace transform
10, 504
Laplacian matrix
126‚Äì128, 151, 152, 155
Laplacian operator
113, 114, 253, 255
large deviation theory
103
large deviations, random walks
10, 21, 22
lattice gauge theory
61, 62, 67
lattice sites, random selection
44
Lavrentiev phenomenon
567, 568, 572, 583
law of rare events
7
law of total probability
8, 13, 35
Lax‚ÄìRitchmyer theorem
496
Lebesgue space LP 557, 562, 583
left-invariant vector Ô¨Åelds
361
legendre conjugate
557
legendre polynomials
203, 255, 259
legendre transformation
557, 559, 583
Legendre‚ÄìFenchel transformation
559, 583
legendre‚ÄìHadamard condition
566, 583
Lie algebra
175‚Äì182, 188‚Äì190, 205‚Äì209,
250‚Äì253, 331, 361
Lie bracket
330‚Äì334
Lie derivative
347‚Äì351
Lie group
173‚Äì175, 202, 250, 251, 358‚Äì361
‚Äì automorphism
358
‚Äì constructing
177
lie series
422, 445
lie transforms
422, 426
Lieb‚Äôs theorem
119, 120
Lieb‚ÄìThirring Inequality
469
limit circle
473
limit cycles
386, 394‚Äì396, 399‚Äì408
limit point
473
ùúî-limit sets
385‚Äì387
Lindstedt method
395, 419, 421, 422, 426, 427,
443, 445
linear combination
325
linear diÔ¨Äerential equations
261, 271, 272, 311,
312, 507
linear groups
174, 176, 246, 358
linear homeomorphism
505
linear matrix representation
160, 185, 207, 208
linear multiplicative algorithm
41
linear multistep methods
492
linear operator
334‚Äì337, 449Ô¨Ä.
linear ordinary diÔ¨Äerential equations
311
linear PDE
528
linear search technique
479
linear solvers, Matlab and Octave
482
linear space
491, 551, 562, 583
linear standard model. See Poynting‚ÄìThomson
model
linear systems
480‚Äì487
linear transformations
172, 174, 182, 411, 442
linear vector space
174‚Äì176, 186, 208, 209
linearization
‚Äì constructing lie algebras
175‚Äì177
‚Äì of an ode
387, 402, 403
Liouville theorem
270
Liouville‚Äôs formula
404
Liouville‚Äôs principle
311
Lipschitz condition
84
Lipschitz continuous function
476, 491
Littlewood‚ÄìRichardson rule
246
local bifurcation
391, 392, 404
local coordinate system
321, 370, 373
local truncation error
491, 493
long-term behavior
8, 15
Lorentz group
172, 176, 181‚Äì185, 207
Lu -factorization
481
Lyapunov function
388, 389
Lyapunov stability
387
m
macromedium, diÔ¨Äerential geometry
364
magnetization, spontaneous
53
Mangasarian‚ÄìFromovitz constraint qualiÔ¨Åcation
555
manifolds
‚Äì center
390, 393, 395, 397‚Äì399
‚Äì diÔ¨Äerentiable
323‚Äì328
‚Äì invariant
390, 429, 433
‚Äì of a limit cycle
403, 404
‚Äì of equilibria
390, 391
‚Äì riemannian
377‚Äì380
‚Äì symplectic
345, 346
‚Äì topological
319‚Äì324
Markov chain
‚Äì aperiodic
31
‚Äì continuous time
22‚Äì27, 29

598
Index
Markov chain (contd.)
‚Äì discrete time
10‚Äì18
‚Äì generator
26, 28
‚Äì irreducible
15, 16, 31
‚Äì positive-recurrent
16
‚Äì random Ô¨Åeld
30, 31
‚Äì recurrent
16, 18
‚Äì transient
18
‚Äì transition probabilities
12
Markov chain Monte Carlo
31
Markov process
10, 17, 23, 26, 27, 33, 50, 56, 68,
99
Markov property
‚Äì continuous time
22
‚Äì strong
12, 13, 17, 27
martingale property of the It√¥ integral
83
master equation
23, 48, 50, 51, 55, 59, 69
matching
118, 155, 431, 433, 434, 445, 446
material law
539, 542‚Äì547
Matlab and octave linear solvers
482
matrix groups
170‚Äì173, 358
maximization
556
maximum dissipation principle
560
Maxwell‚Äôs equations
183, 200, 542
‚Äì extended 547f.
Maxwell‚ÄìDirac System
548
Mayer‚ÄìVietoris exact sequence
223
mean displacement of an atom (VERTEX)
155
Mellin transformation
520
Melnikov function
429
metallurgy
63
methods
‚Äì Adams
493‚Äì495
‚Äì Adams‚ÄìBashforth
493
‚Äì Adams‚ÄìMoulton
493, 494
‚Äì conjugate-gradient (CG)
479, 480, 484, 576
‚Äì descent
479, 484, 485
‚Äì descent, for quadratic forms
479, 484
‚Äì direct, for linear systems
480, 499
‚Äì dynamic Richardson
483
‚Äì explicit
492‚Äì494
‚Äì factorization, for linear systems
480, 482
‚Äì Ô¨Ånite element
489, 577, 584
‚Äì Ô¨Åxed point
477
‚Äì Galerkin
497, 498, 576
‚Äì gauss‚Äìseidel
482, 483
‚Äì gaussian elimination
480, 481
‚Äì gradient
479, 480, 484
‚Äì implicit
94, 492, 494, 495
‚Äì inverse power
478, 486
‚Äì inverse shifted power
486
‚Äì iteration, for eigenvalues
478
‚Äì iteration, for linear systems
482
‚Äì iteration, for nonlinear scalar equations
477
‚Äì linear multistep
492
‚Äì midpoint
492
‚Äì multistep
492‚Äì496
‚Äì Newton‚Äôs
401, 476‚Äì479, 576
‚Äì Newton‚ÄìSimpson
478
‚Äì numerical minimization
475, 479, 480
‚Äì one-step
13, 14, 491, 492, 494‚Äì496
‚Äì power
486
‚Äì preconditioned conjugate gradient (PCG)
485
‚Äì Runge‚ÄìKutta
494
‚Äì Simpson‚Äôs
492
‚Äì static Richardson
483
‚Äì stationary, for linear systems
482‚Äì484
‚Äì successive over relaxation (SOR)
482, 483
metric, cartan
178, 182, 205, 209
Metropolis importance sampling
47
micromedium, diÔ¨Äerential geometry
364
microstructure
364, 578, 580‚Äì582
midpoint method
492
Milstein method
94‚Äì98
Min-max principle
485, 486, 497
minimal polynomials
534
minimum-energy principle
552, 583
minus-sign problem
60
mixed method, perturbation methods
436
m√∂bius strip
403
M√∂bius bius transformations
249
modular algorithms
296, 297, 299
moduli
456, 482
molecular dynamics
51, 66, 69
molecular dynamics method
69
molecular Hamiltonian
155
moment generating function
9, 10, 21
moments
100, 101
monodromy group
248, 249, 272, 281
monodromy matrix
402‚Äì404, 406
monomorphism
223
monotone operator
553
Monte-Carlo methods
31, 39‚Äì71, 92, 93, 98
Monte-Carlo step
49, 50, 65, 69
Morse function
226‚Äì230, 233, 234
‚Äì discrete
229, 230
Morse theory
212, 224, 226‚Äì229, 236
‚Äì discrete
229, 236
Morse‚ÄìBott function
228
Morse‚ÄìSmale‚ÄìWitten Complex
228
motion equation for simple harmonic motion
273
mountain pass
554, 575
Moutard transformation
266, 267
multicanonical ensemble
52
multilevel Monte Carlo method
97, 98
multiple shooting
401
multiple-scale method
426, 429, 438

Index
599
multiplicative noise
85, 86
multistep methods, numerical analysis
492
multivariate gaussian
105
multivariate systems
304
n
n-dimensional
‚Äì Fourier‚ÄìLaplace transformation
521
‚Äì topological manifold
321, 323, 324
naive expansion
419, 445, 446
natural boundary conditions
570
Neimark‚ÄìSacker bifurcations
404
Nekhoroshev theorem
412, 425
Nelson theorem
459
NemytskiÀòi mapping theorem
563, 564
nerve of a cover
234
network community
155
network motif
142, 155
network theory
111‚Äì121
network transitivity
115
networks
111Ô¨Ä. 137‚Äì154
Nevanlinna theory
286
Newton divided diÔ¨Äerence formula
487
Newton form
487
Newton‚Äôs method
476, 478, 576
Newton‚ÄìSimpson method
478
nilpotence
251, 344
noise
‚Äì colored
77‚Äì80
‚Äì thermal
74
‚Äì white
73, 77‚Äì79, 86, 101
nonanticipating stochastic process
82
nonlinear heat-transfer problem
571
nonlinear oscillations
413, 418‚Äì427, 439‚Äì441
nonlinear scalar equations
476, 477
nonlinear special functions
265, 272‚Äì282
nonlinear systems
304, 390, 477‚Äì479, 494, 499
nontrival closed V-path
229
nonvariational methods
585
normal cone
555
normal distribution
9, 77, 104
normal form
427, 428
‚Äì Jordan
398, 411, 442, 444
‚Äì Smith (SNF)
231
normal operator
455, 458, 459, 464, 473
normed space
449‚Äì451
nuclear shell structure
195‚Äì198
null homotopic
213
nullity
117‚Äì120, 154
numerical approximation
80, 93, 95, 476, 487,
571, 576, 582
numerical methods
92‚Äì98
‚Äì collocation
401, 402
‚Äì numerical continuation
405‚Äì408
‚Äì numerical shooting
400, 401
numerical minimization
475, 479, 480
numerical solution of diÔ¨Äerential equations
476
NVT ensemble
52
o
Octave linear solvers
482
Ogden material
572, 574, 584
one-dimensional simple random walk
11
one-dimensional stable manifold
391
one-dimensional unstable manifold
391
one-parameter
‚Äì bifurcation diagram
393‚Äì395, 397
‚Äì group of transformations
348, 360
‚Äì pseudo-group of transformations
348
‚Äì subgroups
348, 360, 375
one-step methods
491, 492, 494‚Äì496
Onsager‚Äôs symmetry condition
558
open-map theorem
451
operator with pure point spectrum
473
optimal control, relaxed
585
order of one-step/multistep methods
402
ordered series
417
ordinary diÔ¨Äerential equations (ODEs)
‚Äì equilibria
386Ô¨Ä.
‚Äì Fuchsian equations
247‚Äì250
‚Äì limit cycles
399Ô¨Ä.
‚Äì numerical solution of the cauchy problem
491Ô¨Ä.
‚Äì stochastic diÔ¨Äerential equations
73, 74, 86, 94
oriented incidence matrix
113
ornstein‚ÄìUhlenbeck (OU) process
73‚Äì75, 88,
89, 100
ornstein‚Äìzernike relation
6
orthogonal groups
172, 174, 572
orthogonal polynomials
260, 261
orthogonality of special functions
260
orthogonality relations
186‚Äì188, 260
orthogonalization, Gram‚ÄìSchmidt
202
oscillator
‚Äì harmonic
131, 164, 195‚Äì197, 207, 262, 515,
532
‚Äì quantum
133, 262, 515
outer solution
431‚Äì435, 445, 446
overlap domain
431, 438, 445
p
p-adic algorithms
300, 301
PageRank algorithm
145, 483
Painlev√© equations
243, 280‚Äì283
‚Äì discrete
286
parabolic cylinder coordinates
256
parallel transport
368‚Äì370, 374, 375
parallelism
367, 376, 377

600
Index
parallelogram identity
450
parametrization, perturbation methods
413,
414, 445
parseval identity
453, 473
partial diÔ¨Äerential equations (PDEs)
527‚Äì548
‚Äì factorization
265‚Äì267
‚Äì Fuchsian equations
247
‚Äì perturbation theory
433‚Äì435
‚Äì separation of variables
256
‚Äì stochastic diÔ¨Äerential equations
98‚Äì104
partial diÔ¨Äerential operator
531, 536
particle path
46
partition theorem
6, 13, 37
partition, Monte Carlo methods
61
passage time
13, 103
path integral
58, 59
Path Integral Monte Carlo (PIMC)
57‚Äì60
percolation model
31‚Äì33
period-doubling bifurcation
404, 405, 407
periodic orbits
211, 386
periodic standard form
422‚Äì424, 440, 441
periodicity
15
permutation group
164, 168, 169, 195
persistence diagram
233, 234
persistent homology
232‚Äì234
perturbation parameter
413‚Äì416, 422, 428,
444‚Äì446
perturbation series
415‚Äì418, 430, 431, 445
perturbation theory
411‚Äì444
phase factor
209
phase space
50, 51, 185, 328, 345
phase transitions
52‚Äì54
Picard‚ÄìVessiot extension
271, 272
piecewise polynomial interpolation
489‚Äì491
piecewise polynomials
489, 490, 576
piezoelectromagnetic model
546
piezoelectromagnetism
546‚Äì548
Pitchfork bifurcation
392, 394, 396‚Äì398, 404
pivot elements
481
pivoting technique
481
Planck‚Äôs constant
161
Plateau variational problem
567
Pochhammer symbol
259, 260
Poincar√© groups
415, 419, 445
Poincar√© lemma
226
Poincar√© map
402, 403, 405
Poincar√© section
402
Poincar√©-Lindstedt method
395
point groups
164, 169
point sources
505
point spectrum
452, 455, 456, 464, 473
Poisson bracket
347
Poisson distribution
5, 6, 25, 35, 36, 140, 141
Poisson equation
534, 536
Poisson process
25
Poisson summation formula
517
polar coordinates
243, 423
P√≥lya‚Äôs recurrence theorem
18, 19
polyconvexity
565, 566, 572
polyhedron
219, 230
polymer science
45, 63‚Äì67
polynomials
‚Äì interpolation
476, 488‚Äì490
‚Äì numerical methods
489
polytope
219
pontryagin maximum principle
583, 585
position vectors
332, 333
potential
‚Äì double-well
567, 578
‚Äì of dissipative forces
558
potential energy
58, 85, 131
potential theory
24, 536
Potts model
30, 48, 120‚Äì123
power series
‚Äì Frobenius method
248
‚Äì lie algebra
177
‚Äì perturbation methods
413, 415, 443
Poynting‚ÄìThomson model
544
preconditioned conjugate gradient (PCG) method
485
preconditioned residual
484
preconditioner
482
predictable stochastic process
82, 83
pre-dual
553, 554, 567
preserved foliations
428
principal axis transformation
503
principal bundle
365‚Äì367
‚Äì associated
362, 364
‚Äì connections
369, 370
principle of least dissipation
558
principle of relativity
166, 182, 185
probability density function (PDF)
89, 99, 102,
105
probability generating function
5, 25
probability mass function
5, 7, 29‚Äì31, 35
probability space
34‚Äì36, 82, 105
probability theory
3, 33‚Äì37
problem
‚Äì cauchy
491, 492, 495, 496, 500, 556‚Äì559
‚Äì eigenvalue
40, 443, 476, 485, 498‚Äì500
‚Äì evolution
491, 560, 577, 585
‚Äì initial value
429, 493, 541, 556
‚Äì linear eigenvalue
480, 498
‚Äì spectral
476, 496, 497
product topology
320, 324
production rate
356, 357
projection operators
363
propagation
7, 153, 540, 546

Index
601
propagators
124
pseudo-indeterminates
292
pseudorandom numbers. See random numbers
Puiseux expansions
268
pullbacks
341‚Äì344, 349, 354, 357
pure absorption method
436
pure envelope method
436, 438, 441
pure state of a quantum system
473
pushforwards
334, 342, 349
Pythagoras‚Äô theorem
160, 161
q
q-Hermite polynomials
285
Q-matrix, Markov chain
23, 25
quadratic variation
77
quadrature formulae, Hermite
489
quantization
116
quantum calculus
284
quantum chromodynamics (QCD)
61
quantum Ô¨Åeld theory
61, 124, 229, 435, 468
quantum groups
240, 283, 284
quantum mechanics
468‚Äì472
quantum Monte Carlo methods
57‚Äì61
quantum numbers
189, 191, 194, 195
quantum oscillator
133
quark model
62
quasiconvex envelope
578
quasi-leibniz rule
344
quasi-periodic orbits
386
quaternions
169, 174‚Äì176, 449
quenched approximation
62
r
radiation
46
radioactive decay
39, 40
Radon transformation
523‚Äì525
random behavior
75
random Ô¨Åeld
29‚Äì31, 80
‚Äì Gibbs
29‚Äì31
‚Äì Markov
29‚Äì31
random-number generator (RNG)
40, 41, 44, 69
random numbers
39‚Äì46, 49, 69
random variable
3, 35‚Äì37
random walk
18‚Äì22
‚Äì examples
11, 12, 14, 16
‚Äì self-avoiding (SAW)
44, 45
rank-one convexity
565, 566, 583
rate-independent
560
rational functions
239, 249, 274, 294, 308‚Äì311
rational numbers
292
rayleigh quotient
485, 497
real roots
303
rectangular coordinates
243
rectangular grids
231
recurrence relations
202
‚Äì special functions
240, 248, 260‚Äì263, 266, 282,
283, 285
recurrent markov chain
16, 18
recursion
41, 204, 205, 493, 496
reducibility
252
reÔ¨Çection groups
240, 243, 244, 246, 267
reÔ¨Çection principle
19, 20
reÔ¨Çections
185
region of absolute stability
496
regular chains
304, 308
regular perturbations
417‚Äì419, 424, 429, 437,
445
relative topology
320
relaxation
577
‚Äì by convex compactiÔ¨Åcation
578
relaxation oscillation
433, 445
Rellich‚ÄìKondrachov theorem
563
renewal relation
17, 21
renormalization group (RG) method
446
representations
‚Äì faithful
185, 209
‚Äì lie group
160, 169, 179, 199, 203, 208, 251, 252
rescaled coordinate
446
rescaled variables
414
resistance distance
129, 130, 155
resolvent computation
26
resonance
425, 446
response functions
52
rest points
418, 419
reversibility
12
ricatti equations
104, 270
Riemann P-symbol
258, 259
Riemann integrals
80, 81, 87, 353
Riemannian connections
379, 380
Riemannian manifolds
377‚Äì380
Riemannian symmetric space
159, 181, 182, 207
Riesz lemma
452, 473
Riesz‚Äìschauder theorem
456
Ritz method
576, 584
Robinson‚ÄìSchensted‚ÄìKnuth (RSK) correspon-
dence
246
Rodrigues‚Äô formula
202
R√∂ssler system
405‚Äì407
rotation groups
171
rotation symmetries
239, 522
Rothe method
560, 584
rounding errors
499, 505
Rouse diÔ¨Äusion
66
row echelon form
480
ruin probability
14
Runge‚Äôs phenomenon
488
Runge‚ÄìKutta method
494
Runge‚ÄìLenz vector
194

602
Index
s
saddle equilibrium
391, 392
saddle point
554, 575
saddle-node bifurcation
392‚Äì398, 406‚Äì408
sample path
105
sampling
42‚Äì51, 55‚Äì57, 60, 62, 68, 69
scale-free network
141, 155
Scaling
162, 163
scattering theory
470‚Äì472
Schatten classes
456
Schramm‚ÄìLoewner evolution
33
Schr√∂dinger equation
‚Äì gauge theory
199, 209
‚Äì historical formulation
538
‚Äì Monte Carlo simulation
59, 60
‚Äì quantum mechanics
528
‚Äì Scaling
162
‚Äì symmetry groups
190
‚Äì unitary groups
467
Schr√∂dinger operator
461, 468‚Äì470, 472, 473,
476, 531
Schur polynomials
244‚Äì246
Schwartz space
531‚Äì533
Schwarzian derivative
249
Second order systems
564‚Äì568
Second-order diÔ¨Äerential equations
263
Secondary hopf bifurcations
404
self-adjoint operator
457‚Äì473
‚Äì adjacency operator
113
self-adjoint extensions
458, 460‚Äì462
self-averaging
45
self-avoiding walk (SAW)
44, 45
self-excited oscillation
446
self-similarity
3, 76
semiconductors
162, 163
semi-grand canonical ensemble
52
semi-implicit scheme
561
separation of variables
191, 192, 250, 256
separatrices
390
set theory notation
33, 34
Shannon‚Äôs sampling theorem
517
Shielding problem
46
Shift register generators
41
Shifting
52‚Äì54
shortest path
114, 129, 139, 143, 147, 155
signal processing
80, 512
Signorini contact
573
similarity transformation
175, 209
simple sampling
39, 42‚Äì45, 47, 69
simple symmetric random walk
18, 19
simplicial complex
218‚Äì223, 225, 229‚Äì232, 234
simplicial homology
218, 219, 221, 224, 225
simpliÔ¨Åcation
193, 294, 300, 313, 428
Simpson‚Äôs method
492
singular homology
218, 224, 228
singular hopf bifurcation
395
singular perturbations
417, 446, 578, 582
singularities
120, 259, 269, 280
Skew diagram
246
Skew symmetry
331, 337‚Äì339
Skew-self-adjoint
537, 538, 540, 543, 544
slow‚Äìfast perturbation methods
432, 433
small-strain tensor
573
smith normal form (SNF)
231, 232
smooth functions
323, 563
smooth manifold
324
smooth map
324
smooth real-valued functional
552
smooth vector Ô¨Åelds
330, 344
smoothness
359, 380, 565, 584
Sobolev critical exponent
563
sobolev space
498, 562‚Äì564, 583
soldering form
373
solitons
265, 267, 276
solvability
252, 271, 272, 528, 573
sound waves
50
source terms
528, 548
space group
164, 170
spanning forest
115, 123, 125, 155
spanning tree
115, 123, 125, 126, 155, 216, 231,
232
Sparse matrix
304
Sparse model
293, 295, 310
Sparse polynomials
295
special functions
239‚Äì286
‚Äì group theory
202‚Äì207
special linear groups
174, 176
speciÔ¨Åc function spaces
562‚Äì576
spectra
442‚Äì444
spectral analysis
465
spectral density
79, 135, 136
spectral measure
462‚Äì466, 473
spectral problems
496‚Äì499
spectral radius
452, 482, 483
spectral representations
507, 511, 512, 520, 523
spectral theorem
261, 463, 464
spectral theory
131, 462, 463, 465, 467
spectrum of an operator
473
sphere-valued harmonic maps
574
spherical bessel functions
268
spherical harmonics
203, 204, 240, 250, 255, 256,
264
spins, Potts model
120
spontaneous magnetization
53
square-free decomposition
295, 309
St. Venant‚ÄìkirchhoÔ¨Ämaterial
573
stable manifolds
227, 390, 391, 399, 403, 429
standard topology
321, 323, 353

Index
603
state space
385Ô¨Ä.
stationary covariance
76, 79
stationary distribution
12, 14‚Äì16, 18
statistical ensembles
52, 68
statistical errors
55, 56
statistical ineÔ¨Éciency
55
statistical mechanics
61‚Äì68, 146‚Äì148
statistical thermodynamics
47‚Äì52
steady-state problems
563
steepest descent method
485
stefanelli‚Äôs variational principle
558
stochastic diÔ¨Äerential equations
73‚Äì104
stochastic integrals
76, 80‚Äì84, 90, 95
stochastic matrix
11, 14, 151
stochastic process
3‚Äì37, 75Ô¨Ä.
‚Äì predictable
82, 83
stochastic trajectories
50, 51
Stokes phenomenon
269
Stokes system
575
Stokes‚Äô theorem
355, 357
Stone formula
466, 473
Stone theorem
467
stone‚Äìvon neumann theorem
468
stopping criterion
576
straight-line program
293
straightforward expansion
419, 445
strange attractors
228, 386, 429
Stratonovich integral
81, 82, 90‚Äì92
strictly convex
553
strong convergence
92, 94, 96, 98, 462
strong law of large numbers
21
strong markov property
12, 13, 17, 27
structurally unstable system
390, 391
structure constants
175, 176, 208
‚Äì of the gauge group
61
‚Äì of the moving frame
334
Sturm‚Äìliouville equations
260
subdiÔ¨Äerential
554, 555
subgroups, diÔ¨Äerential geometry
358
subset topology
320, 323
subspace
‚Äì horizontal
368
‚Äì vertical
368, 370
successive over relaxation (SOR)
482, 483
supercomputers
62
superposition
326, 533, 563
surface eÔ¨Äects
54
surface physics
57, 67
susceptibility
53, 54
Suzuki‚ÄìTrotter formula
59
sweep step
50
symanzik polynomial
125‚Äì129
symbolic computation. See computer algebra
symmetric functions
244, 245, 247, 523
symmetric operator
458, 459, 461, 462, 473
symmetric polynomials
242, 244, 245
symmetries
‚Äì continuous
250‚Äì261
‚Äì discrete
241‚Äì250
‚Äì tensor product
337, 338
symmetry condition, Onsager
558
symmetry groups
160, 190‚Äì194, 207‚Äì209
symplectic geometry
229, 345
symplectic manifolds
345, 346
symplectic vector spaces
345
synchronization
151‚Äì153
system
‚Äì conÔ¨Åned to a box
498
‚Äì Ô¨Ånite-dimensional linear
480, 481, 483, 485
‚Äì linear
480‚Äì487
‚Äì nonlinear
304, 477‚Äì479, 494, 499
‚Äì unconÔ¨Åned
499
t
tangent bundle
327‚Äì330
tangent cone Tk (u)
555
tangent map
329
tangent space
326
tangent vector
324, 325
Taylor series expansion
206, 243, 420
Taylor‚Äôs theorem
96, 477, 479
tempered distributions
516, 520, 531‚Äì534
tensor bundles
342, 364, 372
tensor Ô¨Åeld over
342
tensor products
‚Äì direct sums
454
‚Äì linear operators
334‚Äì336
‚Äì of operators
460
test functions
408, 515, 516
‚Äì space of
515
theorem of Frobenius
370‚Äì372, 377
thermal equilibrium
50, 67, 68, 89
thermal noise
74
ùúï‚àíùõø-theory
283
thermodynamics
47‚Äì52
thermoelastic system
545, 546
thermoelasticity
545, 546
theta function
240, 278
Thom‚Äôs lemma
304
Thomas factor
173
tight-binding model
115‚Äì117
time average
51, 68
time dependence
75, 527, 528
time set
385
time-T map
402, 403
tomography
523
topological equivalence
387, 389, 403
topological manifold
319‚Äì324

604
Index
topological space
319, 320
torsion tensor
375‚Äì377
total variation
77
trace-class operator
457, 471, 472
trajectories
20, 57, 470, 556
transformations
‚Äì canonical
360, 426, 505
‚Äì mathematical
503‚Äì525
‚Äì symmetries
241, 522
transient markov chain
18
transition density
99, 101
transition function
22, 323, 324, 343, 361
‚Äì between coordinate charts
321
‚Äì of a markov chain
22
transition layer
411, 433, 445, 446
transition matrix
12‚Äì14, 56
transition probability
199
transition rate matrix
23
translation groups
169, 170, 216, 244, 465
translations
76, 360
transport process
42, 46
transpose matrix
174, 503
transposition
585
tree
115
trial move
49
triangular matrices
252
triangulated space
219, 222
trigonometric functions
311
triple deck
431, 434, 435, 446
trivial multiplier
403
trivializations, diÔ¨Äerential geometry
362
trotter formula
467
truncation error
491‚Äì493
Tschirnhaus transformation
302
turning point problem
434
tutte polynomial
121‚Äì123
two-dimensional Fourier‚ÄìPlancherel
523
two-element groups
164, 166, 185
two-parameter bifurcation diagram
393, 394
u
umbrella sampling
56
unbounded operators
452, 457‚Äì462
uncertainty
3
unconÔ¨Åned systems
499
uncorrelated random variables
80, 105
unfolding
‚Äì Arnol‚Äôd
444
‚Äì Pitchfork bifurcation
397, 398
‚Äì universal
414, 446
unforced conservative oscillator
421
uniform approximation
420, 446
uniformity
416, 417
uniqueness property
4, 7, 9
unitarily equivalent operators
459, 473
unitary Fourier series transformation
506, 508
unitary groups
174, 195, 467, 468, 473
unitary irreducible representations (UIR)
188‚Äì190, 202, 203, 207
unitary operator
455, 467, 468, 473
universal enveloping algebra
251, 264, 284
universal unfolding
414
unstable manifolds
390‚Äì392, 403, 404, 429
v
V-path
229, 230
van der pol equation
395, 396, 421, 423, 427
van dyke matching rule
431
van kampen theorem
214
variable-metric methods
576
variation of constants
88, 89, 100
variational convergence
581, 583
variational inequality
554, 555, 559, 570, 571,
574
variational methods
584
variational principle
556‚Äì560, 576, 583, 584
variational problems
551‚Äì584
varieties
244
vector Ô¨Åeld
330
‚Äì discrete
229, 230
‚Äì fundamental
360, 369, 370, 372, 375
‚Äì gradient
229, 230
‚Äì hamiltonian
346
vector space
325, 326, 449, 451
‚Äì representations
251, 253, 257
vertex degree
155
vertical subspace
368, 370
vibrations
50, 130‚Äì134, 394
vietoris‚Äìrips complex
235
viscoelasticity
543, 544
von neumann‚Äôs saddle-point theorem 554
von Zeipel‚Äôs method
422, 426
Voronoi diagram
235
w
water droplet freezing
52
wave equation
534, 536, 540
wave operator
471‚Äì473
weak solution
85, 565, 566, 570, 577
wedge product
214, 339, 340, 344
Weierstra√ü elliptic functions
272
Weierstra√ü maximum principle
579, 581, 583,
585
Weierstra√ü theorem
553
weight functions
60
wentzell‚Äìfreidlin theory
103
Weyl alternative
461
Weyl group
244, 247, 282, 286

Index
605
Weyl relations
468
Weyl theorem
464, 473
white noise
73, 77‚Äì79, 86, 101
Whittaker functions
256
Wiener process. See Brownian motion
Wiener‚Äìkhintchine theorem
80
Wigner matrices
190
witness complex
236
WKB method
434, 441
x
Young diagram
244‚Äì246
Young measures
579‚Äì583
y
z-transformation
509, 510
Zero Dirichlet boundary condition
103, 571
Zero-energy state
117, 120
Zero-stability
495

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley‚Äôs ebook EULA.


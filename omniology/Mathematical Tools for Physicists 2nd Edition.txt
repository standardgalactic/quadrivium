

Mathematical Tools for Physicists
Edited by
Michael Grinfeld

Related Titles
Goodson, D.Z.
Mathematical Methods for
Physical and Analytical
Chemistry
2011
ISBN: 978-0-470-47354-2
Also available in digital formats
Trigg, G.L. (ed.)
Encyclopedia of Applied Physics
The Classic Softcover Edition
2004
ISBN: 978-3-527-40478-0
Gläser, M., Kochsiek, M. (eds.)
Handbook of Metrology
2010
ISBN: 978-3-527-40666-1
Stock, R. (ed.)
Encyclopedia of Applied High
Energy and Particle Physics
2009
ISBN: 978-3-527-40691-3
Bohr, H.G. (ed.)
Handbook of Molecular
Biophysics
Methods and Applications
2009
ISBN: 978-3-527-40702-6
Andrews, D.L. (ed.)
Encyclopedia of Applied
Spectroscopy
2009
ISBN: 978-3-527-40773-6
Masujima, M.
Applied Mathematical
Methods in Theoretical
Physics
2nd Edition
2009
ISBN: 978-3-527-40936-5
Also available in digital formats
Willatzen, M., Lew Yan Voon, L.C.
Separable Boundary-Value
Problems in Physics
2011
ISBN: 978-3-527-41020-0
Also available in digital formats

Mathematical Tools for Physicists
Edited by
Michael Grinfeld
Second Edition

The Editor
Dr. Michael Grinfeld
University of Strathclyde
Mathematics and Statistics
Glasgow
United Kingdom
m.grinfeld@strath.ac.uk
Cover
“Skeleton and Pore Partition”, x-ray computed
tomography image of a limestone core from
Mt Gambier in South Australia. Courtesy of
Olaf Delgado-Friedrichs
All books published by Wiley-VCH are
carefully produced. Nevertheless, authors,
editors, and publisher do not warrant the
information contained in these books,
including this book, to be free of errors.
Readers are advised to keep in mind that
statements, data, illustrations, procedural
details or other items may inadvertently be
inaccurate.
Library of Congress Card No.: applied for
British Library Cataloguing-in-Publication
Data
A catalogue record for this book is available
from the British Library.
Bibliographic information published by the
Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek
lists this publication in the Deutsche
Nationalbibliograﬁe; detailed bibliographic
data are available on the Internet at
<http://dnb.d-nb.de>.
© 2015 Wiley-VCH Verlag GmbH & Co.
KGaA, Boschstr. 12, 69469 Weinheim,
Germany
All rights reserved (including those of
translation into other languages). No part of
this book may be reproduced in any
form – by photoprinting, microﬁlm, or any
other means – nor transmitted or translated
into a machine language without written
permission from the publishers. Registered
names, trademarks, etc. used in this book,
even when not speciﬁcally marked as such,
are not to be considered unprotected by law.
Print ISBN: 978-3-527-41188-7
ePDF ISBN: 978-3-527-68426-7
ePub ISBN: 978-3-527-68427-4
Mobi ISBN: 978-3-527-68425-0
Cover-Design Graﬁk-Design Schulz,
Fußgönheim, Germany
Typesetting Laserwords Private Limited,
Chennai, India
Printing and Binding Markono Print Media
Pte Ltd, Singapore
Printed on acid-free paper

V
Contents
List of Contributors
XXI
Preface
XXV
Part I: Probability
1
1
Stochastic Processes
3
James R. Cruise, Ostap O. Hryniv, and Andrew R. Wade
1.1
Introduction
3
1.2
Generating Functions and Integral Transforms
4
1.2.1
Generating Functions
4
1.2.2
Example: Branching Processes
7
1.2.3
Other Transforms
9
1.2.3.1
Moment Generating Functions
9
1.2.3.2
Laplace Transforms
10
1.2.3.3
Characteristic Functions
10
1.3
Markov Chains in Discrete Time
10
1.3.1
What is a Markov Chain?
10
1.3.2
Some Examples
11
1.3.3
Stationary Distribution
12
1.3.4
The Strong Markov Property
12
1.3.5
The One-Step Method
13
1.3.6
Further Computational Methods
14
1.3.7
Long-term Behavior; Irreducibility; Periodicity
15
1.3.8
Recurrence and Transience
16
1.3.9
Remarks on General State Spaces
17
1.3.10
Example: Bak–Sneppen and Related Models
17
1.4
Random Walks
18
1.4.1
Simple Symmetric Random Walk
18
1.4.2
Pólya’s Recurrence Theorem
19
1.4.3
One-dimensional Case; Reﬂection Principle
19
1.4.4
Large Deviations and Maxima of Random Walks
21
1.5
Markov Chains in Continuous Time
22

VI
Contents
1.5.1
Markov Property, Transition Function, and Chapman–Kolmogorov
Relation
22
1.5.2
Inﬁnitesimal Rates and Q-matrices
23
1.5.3
Kolmogorov Diﬀerential Equations
24
1.5.4
Exponential Holding-Time Construction; “Gillespie’s Algorithm”
25
1.5.5
Resolvent Computations
26
1.5.6
Example: A Model of Deposition, Diﬀusion, and Adsorption
27
1.5.6.1
N = 1
28
1.5.6.2
N = 3
28
1.6
Gibbs and Markov Random Fields
29
1.6.1
Gibbs Random Field
29
1.6.2
Markov Random Field
30
1.6.3
Connection Between Gibbs and Markov Random Fields
31
1.6.4
Simulation Using Markov Chain Monte Carlo
31
1.7
Percolation
31
1.8
Further Reading
33
1.A
Appendix: Some Results from Probability Theory
33
1.A.1
Set Theory Notation
33
1.A.2
Probability Spaces
34
1.A.3
Conditional Probability and Independence of Events
35
1.A.4
Random Variables and Expectation
35
1.A.5
Conditional Expectation
36
References
37
2
Monte-Carlo Methods
39
Kurt Binder
2.1
Introduction and Overview
39
2.2
Random-Number Generation
40
2.2.1
General Introduction
40
2.2.2
Properties That a Random-Number Generator (RNG) Should Have
40
2.2.3
Comments about a Few Frequently Used Generators
41
2.3
Simple Sampling of Probability Distributions Using Random Numbers
42
2.3.1
Numerical Estimation of Known Probability Distributions
42
2.3.2
“Importance Sampling” versus “Simple Sampling”
42
2.3.3
Monte-Carlo as a Method of Integration
43
2.3.4
Inﬁnite Integration Space
43
2.3.5
Random Selection of Lattice Sites
44
2.3.6
The Self-Avoiding Walk Problem
44
2.3.7
Simple Sampling versus Biased Sampling: the Example of SAWs
Continued
45
2.4
Survey of Applications to Simulation of Transport Processes
46
2.4.1
The “Shielding Problem”
46
2.4.2
Diﬀusion-Limited Aggregation (DLA)
46
2.5
Monte-Carlo Methods in Statistical Thermodynamics: Importance
Sampling
47

Contents
VII
2.5.1
The General Idea of the Metropolis Importance-Sampling Method
47
2.5.2
Comments on the Formulation of a Monte-Carlo Algorithm
48
2.5.3
The Dynamic Interpretation of the Monte-Carlo Method
50
2.5.4
Monte-Carlo Study of the Dynamics of Fluctuations Near Equilibrium and of
the Approach toward Equilibrium
51
2.5.5
The Choice of Statistical Ensembles
52
2.6
Accuracy Problems: Finite-Size Problems, Dynamic Correlation of Errors,
Boundary Conditions
52
2.6.1
Finite-Size–Induced Rounding and Shifting of Phase Transitions
52
2.6.2
Diﬀerent Boundary Conditions: Simulation of Surfaces and Interfaces
54
2.6.3
Estimation of Statistical Errors
55
2.7
Sampling of Free Energies and Free Energy Barriers
56
2.7.1
Bulk Free Energies
56
2.7.2
Interfacial Free Energies
57
2.7.3
Transition Path Sampling
57
2.8
Quantum Monte-Carlo Techniques
57
2.8.1
General Remarks
57
2.8.2
Path-Integral Monte-Carlo Methods
58
2.8.3
A Classical Application: the Momentum Distribution of Fluid 4He
59
2.8.4
A Few Qualitative Comments on Fermion Problems
59
2.9
Lattice Gauge Theory
61
2.9.1
Some Basic Ideas of Lattice Gauge Theory
61
2.9.2
A Famous Application
62
2.10
Selected Applications in Classical Statistical Mechanics of Condensed
Matter
62
2.10.1
Metallurgy and Materials Science
63
2.10.2
Polymer Science
63
2.10.3
Surface Physics
67
2.11
Concluding Remarks
67
Glossary
68
References
69
Further Reading
71
3
Stochastic Diﬀerential Equations
73
Gabriel J. Lord
3.1
Introduction
73
3.2
Brownian Motion / Wiener Process
75
3.2.1
White and Colored Noise
77
3.2.2
Approximation of a Brownian Motion
80
3.3
Stochastic Integrals
80
3.3.1
It̂o Integral
82
3.4
It̂o SDEs
84
3.4.1
It̂o Formula and Exact Solutions
86
3.5
Stratonovich Integral and SDEs
90
3.6
SDEs and Numerical Methods
92

VIII
Contents
3.6.1
Numerical Approximation of It̂o SDEs
93
3.6.2
Numerical Approximation of Stratonovich SDEs
95
3.6.3
Multilevel Monte Carlo
97
3.7
SDEs and PDEs
98
3.7.1
Fokker–Planck Equation
99
3.7.2
Backward Fokker–Planck Equation
102
3.7.2.1
Sketch of Derivation.
102
3.7.2.2
Boundary Conditions
103
3.7.3
Filtering
103
Further Reading
104
Glossary
104
References
105
Part II: Discrete Mathematics, Geometry, Topology
109
4
Graph and Network Theory
111
Ernesto Estrada
4.1
Introduction
111
4.2
The Language of Graphs and Networks
112
4.2.1
Graph Operators
113
4.2.2
General Graph Concepts
114
4.2.3
Types of Graphs
115
4.3
Graphs in Condensed Matter Physics
115
4.3.1
Tight-Binding Models
115
4.3.1.1
Nullity and Zero-Energy States
117
4.3.2
Hubbard Model
118
4.4
Graphs in Statistical Physics
120
4.5
Feynman Graphs
124
4.5.1
Symanzik Polynomials and Spanning Trees
125
4.5.2
Symanzik Polynomials and the Laplacian Matrix
126
4.5.3
Symanzik Polynomials and Edge Deletion/Contraction
128
4.6
Graphs and Electrical Networks
129
4.7
Graphs and Vibrations
130
4.7.1
Graph Vibrational Hamiltonians
131
4.7.2
Network of Classical Oscillators
131
4.7.3
Network of Quantum Oscillators
133
4.8
Random Graphs
134
4.9
Introducing Complex Networks
137
4.10
Small-World Networks
138
4.11
Degree Distributions
139
4.11.1
“Scale-Free” Networks
141
4.12
Network Motifs
142
4.13
Centrality Measures
143
4.14
Statistical Mechanics of Networks
146
4.14.1
Communicability in Networks
147

Contents
IX
4.15
Communities in Networks
148
4.16
Dynamical Processes on Networks
150
4.16.1
Consensus
150
4.16.2
Synchronization in Networks
151
4.16.3
Epidemics on Networks
153
Glossary
154
References
155
Further Reading
157
5
Group Theory
159
Robert Gilmore
5.1
Introduction
159
5.2
Precursors to Group Theory
160
5.2.1
Classical Geometry
161
5.2.2
Dimensional Analysis
161
5.2.3
Scaling
162
5.2.4
Dynamical Similarity
163
5.3
Groups: Deﬁnitions
164
5.3.1
Group Axioms
165
5.3.2
Isomorphisms and Homomorphisms
166
5.4
Examples of Discrete Groups
166
5.4.1
Finite Groups
166
5.4.1.1
The Two-Element Group Z2
166
5.4.1.2
Group of Equilateral Triangle C3v
167
5.4.1.3
Cyclic Groups Cn
168
5.4.1.4
Permutation Groups Sn
168
5.4.1.5
Generators and Relations
169
5.4.2
Inﬁnite Discrete Groups
169
5.4.2.1
Translation Groups: One Dimension
169
5.4.2.2
Translation Groups: Two Dimensions
170
5.4.2.3
Space Groups
170
5.5
Examples of Matrix Groups
170
5.5.1
Translation Groups
170
5.5.2
Heisenberg Group H3
171
5.5.3
Rotation Group SO(3)
171
5.5.4
Lorentz Group SO(3, 1)
172
5.6
Lie Groups
173
5.7
Lie Algebras
175
5.7.1
Structure Constants
175
5.7.2
Constructing Lie Algebras by Linearization
175
5.7.3
Constructing Lie Groups by Exponentiation
177
5.7.4
Cartan Metric
178
5.7.5
Operator Realizations of Lie Algebras
179
5.7.6
Disentangling Results
180
5.8
Riemannian Symmetric Spaces
181

X
Contents
5.9
Applications in Classical Physics
182
5.9.1
Principle of Relativity
182
5.9.2
Making Mechanics and Electrodynamics Compatible
182
5.9.3
Gravitation
184
5.9.4
Reﬂections
185
5.10
Linear Representations
185
5.10.1
Maps to Matrices
185
5.10.2
Group Element–Matrix Element Duality
186
5.10.3
Classes and Characters
187
5.10.4
Fourier Analysis on Groups
187
5.10.4.1
Remark on Terminology
188
5.10.5
Irreps of SU(2)
188
5.10.6
Crystal Field Theory
190
5.11
Symmetry Groups
190
5.12
Dynamical Groups
194
5.12.1
Conformal Symmetry
194
5.12.2
Atomic Shell Structure
195
5.12.3
Nuclear Shell Structure
195
5.12.4
Dynamical Models
198
5.13
Gauge Theory
199
5.14
Group Theory and Special Functions
202
5.14.1
Summary of Some Properties
202
5.14.2
Relation with Lie Groups
202
5.14.3
Spherical Harmonics and SO(3)
203
5.14.4
Diﬀerential and Recursion Relations
204
5.14.5
Diﬀerential Equation
205
5.14.6
Addition Theorems
206
5.14.7
Generating Functions
206
5.15
Summary
207
Glossary
208
References
210
6
Algebraic Topology
211
Vanessa Robins
6.1
Introduction
211
6.2
Homotopy Theory
212
6.2.1
Homotopy of Paths
212
6.2.2
The Fundamental Group
213
6.2.3
Homotopy of Spaces
215
6.2.4
Examples
215
6.2.5
Covering Spaces
216
6.2.6
Extensions and Applications
217
6.3
Homology
218
6.3.1
Simplicial Complexes
219
6.3.2
Simplicial Homology Groups
219

Contents
XI
6.3.3
Basic Properties of Homology Groups
221
6.3.4
Homological Algebra
222
6.3.5
Other Homology Theories
224
6.4
Cohomology
224
6.4.1
De Rham Cohomology
226
6.5
Morse Theory
226
6.5.1
Basic Results
226
6.5.2
Extensions and Applications
228
6.5.3
Forman’s Discrete Morse Theory
229
6.6
Computational Topology
230
6.6.1
The Fundamental Group of a Simplicial Complex
230
6.6.2
Smith Normal form for Homology
231
6.6.3
Persistent Homology
232
6.6.4
Cell Complexes from Data
234
Further Reading
236
References
236
7
Special Functions
239
Chris Athorne
7.1
Introduction
239
7.2
Discrete Symmetry
241
7.2.1
Symmetries
241
7.2.2
Coxeter Groups
243
7.2.3
Symmetric Functions
244
7.2.4
Invariants of Coxeter Groups
246
7.2.5
Fuchsian Equations
247
7.3
Continuous Symmetry
250
7.3.1
Lie Groups and Lie Algebras
250
7.3.2
Representations
251
7.3.3
The Laplace Operator
253
7.3.4
Spherical Harmonics
255
7.3.5
Separation of Variables
256
7.3.6
Bessel Functions
256
7.3.7
Addition Laws
258
7.3.8
The Hypergeometric Equation
258
7.3.9
Orthogonality
260
7.3.10
Orthogonal Polynomials
260
7.4
Factorization
261
7.4.1
The Bessel Equation
262
7.4.2
Hermite
262
7.4.3
Legendre
263
7.4.4
“Factorization” of PDEs
265
7.4.5
Dunkl Operators
267
7.5
Special Functions Without Symmetry
268
7.5.1
Airy Functions
268

XII
Contents
7.5.1.1
Stokes Phenomenon
269
7.5.2
Liouville Theory
269
7.5.3
Diﬀerential Galois Theory
271
7.6
Nonlinear Special Functions
272
7.6.1
Weierstraß Elliptic Functions
272
7.6.1.1
Lamé Equations
277
7.6.2
Jacobian Elliptic Functions
277
7.6.3
Theta Functions
278
7.6.4
Painlevé Transcendents
280
7.7
Discrete Special Functions
283
7.7.1
𝜕–𝛿Theory
283
7.7.2
Quantum Groups
283
7.7.3
Diﬀerence Operators
284
7.7.4
q-Hermite Polynomials
285
7.7.5
Discrete Painlevé Equations
286
References
286
8
Computer Algebra
291
James H. Davenport
8.1
Introduction
291
8.2
Computer Algebra Systems
292
8.3
“Elementary” Algorithms
292
8.3.1
Representation of Polynomials
292
8.3.2
Greatest Common Divisors
294
8.3.2.1
Intermediate Expression Swell
294
8.3.2.2
Sparsity
295
8.3.3
Square-free Decomposition
295
8.3.4
Extended Euclidean Algorithm
296
8.4
Advanced Algorithms
296
8.4.1
Modular Algorithms–Integer
296
8.4.2
Modular Algorithms–Polynomial
297
8.4.3
The Challenge of Factorization
298
8.4.4
p-adic Algorithms–Integer
300
8.4.5
p-adic Algorithms–Polynomial
301
8.5
Solving Polynomial Systems
301
8.5.1
Solving One Polynomial
301
8.5.2
Real Roots
303
8.5.3
Linear Systems
304
8.5.4
Multivariate Systems
304
8.5.5
Gröbner Bases
305
8.5.6
Regular Chains
308
8.6
Integration
308
8.6.1
Rational Functions
308
8.6.2
More Complicated Functions
310
8.6.3
Linear Ordinary Diﬀerential Equations
311

Contents
XIII
8.7
Interpreting Formulae as Functions
312
8.7.1
Fundamental Theorem of Calculus Revisited
312
8.7.2
Simpliﬁcation of Functions
313
8.7.3
Real Problems
314
8.8
Conclusion
315
References
315
9
Diﬀerentiable Manifolds
319
Marcelo Epstein
9.1
Introduction
319
9.2
Topological Spaces
319
9.2.1
Deﬁnition
319
9.2.2
Continuity
320
9.2.3
Further Topological Notions
320
9.3
Topological Manifolds
320
9.3.1
Motivation
320
9.3.2
Deﬁnition
321
9.3.3
Coordinate Charts
321
9.3.4
Maps and Their Representations
321
9.3.5
A Physical Application
322
9.3.6
Topological Manifolds with Boundary
323
9.4
Diﬀerentiable Manifolds
323
9.4.1
Motivation
323
9.4.2
Deﬁnition
323
9.4.3
Diﬀerentiable Maps
324
9.4.4
Tangent Vectors
324
9.4.5
Brief Review of Vector Spaces
325
9.4.5.1
Deﬁnition
325
9.4.5.2
Linear Independence and Dimension
325
9.4.5.3
The Dual Space
326
9.4.6
Tangent and Cotangent Spaces
326
9.4.7
The Tangent and Cotangent Bundles
327
9.4.8
A Physical Interpretation
328
9.4.9
The Diﬀerential of a Map
328
9.5
Vector Fields and the Lie Bracket
330
9.5.1
Vector Fields
330
9.5.2
The Lie Bracket
330
9.5.3
A Physical Interpretation: Continuous Dislocations
331
9.5.4
Pushforwards
334
9.6
Review of Tensor Algebra
334
9.6.1
Linear Operators and the Tensor Product
334
9.6.2
Symmetry and Skew Symmetry
337
9.6.3
The Algebra of Tensors on a Vector Space
337
9.6.4
Exterior Algebra
339
9.7
Forms and General Tensor Fields
341

XIV
Contents
9.7.1
1-Forms
341
9.7.2
Pullbacks
341
9.7.3
Tensor Bundles
342
9.7.4
The Exterior Derivative
343
9.8
Symplectic Geometry
345
9.8.1
Symplectic Vector Spaces
345
9.8.2
Symplectic Manifolds
345
9.8.3
Hamiltonian Systems
346
9.9
The Lie Derivative
347
9.9.1
The Flow of a Vector Field
347
9.9.2
One-parameter Groups of Transformations Generated by Flows
348
9.9.3
The Lie Derivative
348
9.9.3.1
The Lie Derivative of a Scalar
349
9.9.3.2
The Lie Derivative of a Vector Field
350
9.9.3.3
The Lie Derivative of a 1-form
350
9.9.3.4
The Lie Derivative of Arbitrary Tensor Fields
350
9.9.3.5
The Lie Derivative in Components
351
Further Reading
351
10
Topics in Diﬀerential Geometry
353
Marcelo Epstein
10.1
Integration
353
10.1.1
Integration of n-Forms in ℝn
353
10.1.2
Integration of Forms on Oriented Manifolds
354
10.1.3
Stokes’ Theorem
355
10.2
Fluxes in Continuum Physics
355
10.2.1
Extensive-Property Densities
355
10.2.2
Balance Laws, Flux Densities, and Sources
356
10.2.3
Flux Forms and Cauchy’s Formula
356
10.2.4
Diﬀerential Expression of the Balance Law
357
10.3
Lie Groups
358
10.3.1
Deﬁnition
358
10.3.2
Group Actions
358
10.3.3
One-Parameter Subgroups
360
10.3.4
Left- and Right-Invariant Vector Fields on a Lie Group
361
10.4
Fiber Bundles
361
10.4.1
Introduction
361
10.4.2
Deﬁnition
361
10.4.3
Simultaneity in Classical Mechanics
363
10.4.4
Adapted Coordinate Systems
363
10.4.5
The Bundle of Linear Frames
363
10.4.6
Bodies with Microstructure
364
10.4.7
Principal Bundles
364
10.4.8
Associated Bundles
365
10.5
Connections
367

Contents
XV
10.5.1
Introduction
367
10.5.2
Ehresmann Connection
368
10.5.3
Parallel Transport along a Curve
368
10.5.4
Connections in Principal Bundles
369
10.5.5
Distributions and the Theorem of Frobenius
370
10.5.6
Curvature
371
10.5.7
Cartan’s Structural Equation
372
10.5.8
Bianchi Identities
372
10.5.9
Linear Connections
372
10.5.10
The Canonical 1-Form
373
10.5.11
The Christoﬀel Symbols
374
10.5.12
Parallel Transport and the Covariant Derivative
374
10.5.13
Curvature and Torsion
375
10.6
Riemannian Manifolds
377
10.6.1
Inner-Product Spaces
377
10.6.2
Riemannian Manifolds
379
10.6.3
Riemannian Connections
380
Further Reading
380
Part III: Analysis
383
11
Dynamical Systems
385
David A.W. Barton
11.1
Introduction
385
11.1.1
Deﬁnition of a Dynamical System
385
11.1.2
Invariant Sets
386
11.2
Equilibria
386
11.2.1
Deﬁnition and Calculation
386
11.2.2
Stability
387
11.2.3
Linearization
387
11.2.4
Lyapunov Functions
388
11.2.5
Topological Equivalence
389
11.2.6
Manifolds
390
11.2.7
Local Bifurcations
391
11.2.8
Saddle-Node Bifurcation
392
11.2.9
Hopf Bifurcation
393
11.2.10
Pitchfork Bifurcation
396
11.2.11
Center Manifolds
398
11.3
Limit Cycles
399
11.3.1
Deﬁnition and Calculation
399
11.3.1.1
Harmonic Balance Method
399
11.3.1.2
Numerical Shooting
400
11.3.1.3
Collocation
401
11.3.2
Linearization
402
11.3.3
Topological Equivalence
403

XVI
Contents
11.3.4
Manifolds
403
11.3.5
Local Bifurcations
404
11.3.6
Period-Doubling Bifurcation
404
11.4
Numerical Continuation
405
11.4.1
Natural Parameter Continuation
405
11.4.2
Pseudo-Arc-Length Continuation
407
11.4.3
Continuation of Bifurcations
407
References
408
12
Perturbation Methods
411
James Murdock
12.1
Introduction
411
12.2
Basic Concepts
412
12.2.1
Perturbation Methods versus Numerical Methods
412
12.2.2
Perturbation Parameters
413
12.2.3
Perturbation Series
415
12.2.4
Uniformity
416
12.3
Nonlinear Oscillations and Dynamical Systems
418
12.3.1
Rest Points and Regular Perturbations
418
12.3.2
Simple Nonlinear Oscillators and Lindstedt’s Method
419
12.3.3
Averaging Method for Single-Frequency Systems
422
12.3.4
Multifrequency Systems and Hamiltonian Systems
424
12.3.5
Multiple-Scale Method
426
12.3.6
Normal Forms
427
12.3.7
Perturbation of Invariant Manifolds; Melnikov Functions
429
12.4
Initial and Boundary Layers
429
12.4.1
Multiple-Scale Method for Initial Layer Problems
429
12.4.2
Matching for Initial Layer Problems
431
12.4.3
Slow–Fast Systems
432
12.4.4
Boundary Layer Problems
433
12.4.5
WKB Method
434
12.4.6
Fluid Flow
434
12.5
The “Renormalization Group” Method
435
12.5.1
Initial and Boundary Layer Problems
436
12.5.2
Nonlinear Oscillations
439
12.5.3
WKB Problems
441
12.6
Perturbations of Matrices and Spectra
442
Glossary
444
References
446
Further Reading
447
13
Functional Analysis
449
Pavel Exner
13.1
Banach Space and Operators on Them
449
13.1.1
Vector and Normed Spaces
449

Contents
XVII
13.1.2
Operators on Banach Spaces
450
13.1.3
Spectra of Closed Operators
452
13.2
Hilbert Spaces
452
13.2.1
Hilbert-Space Geometry
452
13.2.2
Direct Sums and Tensor Products
454
13.3
Bounded Operators on Hilbert Spaces
454
13.3.1
Hermitean Operators
454
13.3.2
Unitary Operators
455
13.3.3
Compact Operators
456
13.3.4
Schatten Classes
456
13.4
Unbounded Operators
457
13.4.1
Operator Adjoint and Closure
457
13.4.2
Normal and Self-Adjoint Operators
458
13.4.3
Tensor Products of Operators
460
13.4.4
Self-Adjoint Extensions
460
13.5
Spectral Theory of Self-Adjoint Operators
462
13.5.1
Functional Calculus
462
13.5.2
Spectral Theorem
463
13.5.3
More about Spectral Properties
465
13.5.4
Groups of Unitary Operators
467
13.6
Some Applications in Quantum Mechanics
468
13.6.1
Schrödinger Operators
468
13.6.2
Scattering Theory
470
Glossary
472
References
473
14
Numerical Analysis
475
Lyonell Boulton
14.1
Introduction
475
14.2
Algebraic Equations
476
14.2.1
Nonlinear Scalar Equations
476
14.2.2
Nonlinear Systems
477
14.2.3
Numerical Minimization
479
14.3
Finite-Dimensional Linear Systems
480
14.3.1
Direct Methods and Matrix Factorization
480
14.3.2
Iteration Methods for Linear Problems
482
14.3.3
Computing Eigenvalues of Finite Matrices
485
14.4
Approximation of Continuous Data
487
14.4.1
Lagrange Interpolation
487
14.4.2
The Interpolation Error
487
14.4.3
Hermite Interpolation
488
14.4.4
Piecewise Polynomial Interpolation
489
14.5
Initial Value Problems
491
14.5.1
One-Step Methods
491
14.5.2
Multistep Methods
492

XVIII
Contents
14.5.3
Runge–Kutta Methods
494
14.5.4
Stability and Global Stability
495
14.6
Spectral Problems
496
14.6.1
The Inﬁnite-Dimensional min–max Principle
497
14.6.2
Systems Conﬁned to a Box
498
14.6.3
The Case of Unconﬁned Systems
499
Further Reading
499
References
500
15
Mathematical Transformations
503
Des McGhee, Rainer Picard, Sascha Trostorﬀ, and Marcus Waurick
15.1
What are Transformations and Why are They Useful?
503
15.2
The Fourier Series Transformations
506
15.2.1
The Abstract Fourier Series
506
15.2.2
The Classical Fourier Series
507
15.2.3
The Fourier Series Transformation in L2 (Sℂ(0, 1)
)
509
15.3
The z-Transformation
509
15.4
The Fourier–Laplace Transformation
510
15.4.1
Convolutions as Functions of 𝜕𝜚
512
15.4.1.1
Functions of 𝜕𝜚
512
15.4.1.2
Convolutions
513
15.4.2
The Fourier–Plancherel Transformation
515
15.5
The Fourier–Laplace Transformation and Distributions
515
15.5.1
Impulse Response
517
15.5.2
Shannon’s Sampling Theorem
517
15.6
The Fourier-Sine and Fourier-Cosine Transformations
518
15.7
The Hartley Transformations H±
519
15.8
The Mellin Transformation
520
15.9
Higher-Dimensional Transformations
521
15.10
Some Other Important Transformations
522
15.10.1
The Hadamard Transformation
522
15.10.2
The Hankel Transformation
523
15.10.3
The Radon Transformation
523
References
525
16
Partial Diﬀerential Equations
527
Des McGhee, Rainer Picard, Sascha Trostorﬀ, and Marcus Waurick
16.1
What are Partial Diﬀerential Equations?
527
16.2
Partial Diﬀerential Equations in ℝn+1, n ∈ℕ, with Constant
Coeﬃcients
529
16.2.1
Evolutionarity
530
16.2.2
An Outline of Distribution Theory
531
16.2.3
Integral Transformation Methods as a Solution Tool
533
16.3
Partial Diﬀerential Equations of Mathematical Physics
537
16.4
Initial-Boundary Value Problems of Mathematical Physics
540

Contents
XIX
16.4.1
Maxwell’s Equations
542
16.4.2
Viscoelastic Solids
543
16.4.2.1
The Kelvin–Voigt Model
543
16.4.2.2
The Poynting–Thomson Model (The Linear Standard Model)
544
16.5
Coupled Systems
544
16.5.1
Thermoelasticity
545
16.5.2
Piezoelectromagnetism
546
16.5.3
The Extended Maxwell System and its Uses
547
References
548
17
Calculus of Variations
551
Tomáš Roubíˇcek
17.1
Introduction
551
17.2
Abstract Variational Problems
551
17.2.1
Smooth (Diﬀerentiable) Case
552
17.2.2
Nonsmooth Case
554
17.2.3
Constrained Problems
555
17.2.4
Evolutionary Problems
556
17.2.4.1
Variational Principles
556
17.2.4.2
Evolution Variational Inequalities
559
17.2.4.3
Recursive Variational Problems Arising by Discretization in Time
560
17.3
Variational Problems on Speciﬁc Function Spaces
562
17.3.1
Sobolev Spaces
562
17.3.2
Steady-State Problems
563
17.3.2.1
Second Order Systems of Equations
564
17.3.2.2
Fourth Order Systems
568
17.3.2.3
Variational Inequalities
570
17.3.3
Some Examples
571
17.3.3.1
Nonlinear Heat-Transfer Problem
571
17.3.3.2
Elasticity at Large Strains
572
17.3.3.3
Small-Strain Elasticity, Lamé System, Signorini Contact
573
17.3.3.4
Sphere-Valued Harmonic Maps
574
17.3.3.5
Saddle-Point-Type Problems
575
17.3.4
Evolutionary Problems
575
17.4
Miscellaneous
576
17.4.1
Numerical Approximation
576
17.4.2
Extension of Variational Problems
577
17.4.3
Γ-Convergence
581
Glossary
582
Further Reading
584
References
586
Index
589


XXI
List of Contributors
Chris Athorne
University of Glasgow
School of Mathematics and Statistics
15 University Gardens
Glasgow G12 8QW
UK
David A.W. Barton
University of Bristol
Department of Engineering Mathematics
Merchant Venturers Building
Woodland Road
Bristol BS8 1UB
UK
Kurt Binder
Johannes Gutenberg University of Mainz
Department of Physics, Mathematics and
Informatics
Institute of Physics
Staudingerweg 7
55128 Mainz
Germany
Lyonell Boulton
Heriot-Watt University
Department of Mathematics
School of Mathematical and Computer
Sciences
Colin MacLaurin Building
Riccarton
Edinburgh EH14 4AS
UK
James R. Cruise
Heriot-Watt University
Department of Actuarial Mathematics
and Statistics
and the Maxwell Institute for
Mathematical Sciences
Edinburgh EH14 4AS
UK
James H. Davenport
University of Bath
Department of Computer Science
Claverton Down
Bath
Bath BA2 7AY
UK
Marcelo Epstein
University of Calgary
Department of Mechanical and
Manufacturing
Engineering
Schulich
School of Engineering
2500 University Drive NW
Calgary
Alberta, T2N 1N4
Canada
Ernesto Estrada
University of Strathclyde
Department of Mathematics and
Statistics
26 Richmond Street
Glasgow G1 1XQ
UK

XXII
List of Contributors
Pavel Exner
Czech Technical University
Doppler Institute
Bˇrehová 7
11519 Prague
Czech Republic
and
Department of Theoretical Physics
Nuclear Physics Institute
Academy of Sciences of the
Czech Republic
Hlavn´ı 130
25068 ˇRež
Czech Republic
Robert Gilmore
Drexel University
Department of Physics
3141 Chestnut Street
Philadelphia
PA 19104
USA
Ostap Hryniv
Durham University
Department of Mathematical Sciences
South Road
Durham DH1 3LE
UK
Gabriel J. Lord
Heriot-Watt University
Department of Mathematics
Maxwell Institute for Mathematical
Sciences
Edinburgh EH14 4AS
UK
James Murdock
Iowa State University
Department of Mathematics
478 Carver Hall
Ames
IA 50011
USA
Des McGhee
University of Strathclyde
Department of Mathematics and
Statistics
26 Richmond Street
Glasgow G1 1XH
Scotland
UK
Rainer Picard
Technische Universität Dresden
FR Mathematik
Institut für Analysis
Willersbau
Zellescher Weg 12-14
01069 Dresden
Germany
Vanessa Robins
The Australian National University
Department of Applied Mathematics
Research School of Physics and
Engineering
Canberra ACT 0200
Australia
Tomáš Roubíˇcek
Charles University
Mathematical Institute
Sokolovská 83
Prague 186 75
Czech Republic

List of Contributors
XXIII
and
Institute of Thermomechanics
of the ASCR
Prague
Czech Republic
Sascha Trostorff
Technische Universität Dresden
FR Mathematik
Institut für Analysis
Willersbau
Zellescher Weg 12-14
01069 Dresden
Germany
Andrew R. Wade
Durham University
Department of Mathematical Sciences
South Road
Durham DH1 3LE
UK
Marcus Waurick
Technische Universität Dresden
FR Mathematik
Institut für Analysis
Willersbau
Zellescher Weg 12-14
01069 Dresden
Germany


XXV
Preface
The intensely fruitful symbiosis between physics and mathematics is nothing short of
miraculous. It is not a symmetric interaction; however, physical laws, which involve
relations between variables, are always in need of rules and techniques for manipulating
these relations: hence, the role of mathematics in providing tools for physics, and hence,
the need for books such as this one. Physics, in its turn, supplies motivation for the
development of mathematically sound techniques and concepts; it is enough to mention
the importance of celestial mechanics in the evolution of perturbation methods and the
impetus Dirac’s delta “function” gave to the work on generalized functions. So perhaps a
sister volume to the present compendium would be Physical Insights for Mathematicians.
(From the above, one might get the impression that physics and mathematics are two
separate, even opposing, domains. This is of course not true; there is a well-established
ﬁeld of mathematical physics and many mathematicians working in ﬂuid and continuum
mechanics would be hard-pressed to separate the mathematics and the physics elements
of their seamless activity.)
The present book is intended for the use of advanced undergraduates, graduate stu-
dents, and researchers in physics, who are aware of the usefulness of a particular math-
ematical approach and need a quick point of entry into its vocabulary, main results,
and the literature. However, one cannot cleanly single out parts of mathematics that are
useful for physics and ones that are not; for example, while the theory of operators in
Hilbert spaces is undoubtedly indispensable in quantum mechanics, an area as abstruse
as category theory is becoming increasingly popular in cosmology and has found appli-
cations in developmental biology.1) Hence, the concept of “mathematics useful in physics”
arguably covers the same area as mathematics tout court, and anyone embarking on the
publication of a book such as the present one does so in the certainty that no single
book can do justice to the intricate interpenetration of mathematics and physics. It is
quite possible to write a second, and perhaps a third volume of an encyclopedia such as
ours.
Let us quickly mention signiﬁcant areas that are not being covered here. These
include combinatorics, deterministic chaos, fractals, nonlinear partial diﬀerential
1) The exact relation between physics and biology is still shrouded in mystery, but certainly objects of
interest in biology are physical objects. The time for a truly useful Mathematical Tools for Biologists has
not yet arrived, but is awaited eagerly.

XXVI
Preface
equations, and symplectic geometry. It was also felt that a separate chapter on context-
free modeling was not necessary as there is an ample literature on modeling case
studies.
What this book oﬀers is an attractive mix of classical areas of applications of mathe-
matics to physics and of areas that have only come to prominence recently. Thus, we have
substantive chapters on asymptotic methods, calculus of variations, diﬀerential geometry
and topology of manifolds, dynamical systems theory, functional analysis, group theory,
numerical methods, partial diﬀerential equations of mathematical physics, special func-
tions, and transform methods. All these are up-to-date surveys; for example, the chapter
on asymptotic methods discusses recent renormalized group-based approaches, while the
chapter on variational methods considers examples where no smooth minimizers exist.
These chapters appear side by side with a decidedly modern computational take on alge-
braic topology and in-depth reviews of such increasingly important areas as graph and net-
work theory, Monte Carlo simulations, stochastic diﬀerential equations, and algorithms
in symbolic computation, an important complement to analytic and numerical problem-
solving approaches.
It is hoped that the layout of the text allows for easy cross-referencing between chapters,
and that by the end of a chapter, the reader will have a clear view of the area under
discussion and will be know where to go to learn more. Bon voyage!

1
Part I
Probability
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.


3
1
Stochastic Processes
James R. Cruise, Ostap O. Hryniv, and Andrew R. Wade
1.1
Introduction
Basic probability theory deals, among
other things, with random variables and
their properties. A random variable is the
mathematical abstraction of the following
concept: we make a measurement on some
physical system, subject to randomness
or uncertainty, and observe the value. We
can, for example, construct a mathematical
model for the system and try to predict the
behavior of our random observable, per-
haps through its distribution, or at least its
average value (mean). Even in the simplest
applications, however, we are confronted
by systems that change over time. Now we
do not have a single random variable, but a
family of random variables. The nature of
the physical system that we are modeling
determines the structure of dependencies
of the variables.
A stochastic (or random) process is the
mathematical abstraction of these systems
that change randomly over time. Formally,
a stochastic process is a family of random
variables (Xt)t∈T, where T is some index set
representing time. The two main examples
are T = {0, 1, 2, …} (discrete time) and
T = [0, ∞)
(continuous
time);
diﬀerent
applications will favor one or other of
these. Interesting classes of processes are
obtained by imposing additional structure
on the family Xt, as we shall see.
The aim of this chapter is to give a tour of
some of the highlights of stochastic process
theory and its applications in the physical
sciences. In line with the intentions of this
volume, our emphasis is on tools. However,
the combination of a powerful tool and an
unsteady grip is a hazardous one, so we
have attempted to maintain mathematical
accuracy. For reasons of space, the pre-
sentation is necessarily concise. While we
cover several important topics, we omit
many more. We include references for fur-
ther reading on the topics that we do cover
throughout the text and in Section 1.8.
The tools that we exhibit include gener-
ating functions and other transforms, and
renewal structure, including the Markov
property, which can be viewed loosely as a
notion of statistical self-similarity.
In the next section, we discuss some
of the tools that we will use, with some
examples. The basic notions of probability
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

4
1 Stochastic Processes
theory that we use are summarized in
Section 1.A.
1.2
Generating Functions and Integral
Transforms
1.2.1
Generating Functions
Given a sequence (ak)k≥0 of real numbers,
the function
G(s) = Ga(s) =
∑
k≥0
aksk
(1.1)
is called the generating function of (ak)k≥0.
When Ga(s) is ﬁnite for some s ≠0, the
series (1.1) converges in the disc {z ∈ℂ∶
|z| < |s|} and thus its coeﬃcients ak can be
recovered via diﬀerentiation
ak = 1
k!
( d
ds
)k
Ga(s)
||||s=0
,
(1.2)
or using the Cauchy integral formula
ak =
1
2𝜋i ∮|z|=r
Ga(z)
zk+1 dz,
(i2 = −1), (1.3)
with properly chosen r > 0. This obser-
vation is often referred to as the unique-
ness property: if two generating functions,
say Ga(s) and Gb(s), are ﬁnite and coincide
in some open neighborhood of the origin,
then ak = bk for all k ≥0. In particular, one
can identify the sequence from its generat-
ing function.
The generating function is one of many
transforms very useful in applications.
We will discuss some further examples in
Section 1.2.3.
Example 1.1 Imagine one needs to pay the
sum of n pence using only one pence and
two pence coins. In how many ways can this
be done?1)
(a)
If the order matters, that is, when
1 + 2 and 2 + 1 are two distinct ways
of paying 3 pence, the question is
about counting the number an of
monomer/dimer
conﬁgurations
on
the interval of length n. One easily
sees that a0 = a1 = 1, a2 = 2, and
in general an = an−1 + an−2, n ≥2,
because the leftmost monomer can
be extended to a full conﬁguration
in an−1 ways, whereas the leftmost
dimer is compatible with exactly an−2
conﬁgurations on the remainder of
the interval. A straightforward com-
putation using the recurrence relation
above now gives
Ga(s) = 1 + s +
∑
n≥2
(an−1 + an−2
)sn
= 1 + sGa(s) + s2Ga(s),
so
that
Ga(s) = (1 −s −s2)−1.
The
coeﬃcient an can now be recovered
using (1.2), (1.3), or partial fractions
and then power series expansion.
(b)
If the order does not matter, that is,
when 1 + 2 and 2 + 1 are regarded
as identical ways of paying 3 pence,
the question above boils down to cal-
culating the number bn of nonnega-
tive integer solutions to the equation
n1 + 2n2 = n. One easily sees that b0 =
b1 = 1, b2 = b3 = 2, and a straightfor-
ward induction shows that bn is the
coeﬃcient of sk in the product
1) Early use of generating functions was often in
this enumerative vein, going back to de Moivre
and exempliﬁed by Laplace in his Théorie ana-
lytique des probabilités of 1812. The full power
of generating functions in the theory of stochas-
tic processes emerged later with work of Pólya,
Feller, and others.

1.2 Generating Functions and Integral Transforms
5
(1 + s + s2 + s3 + s4 + · · ·)
× (1 + s2 + s4 + s6 + · · ·).
In other words, Gb(s) = ∑
n≥0 bnsn =
(1 −s)−1(1 −s2)−1.
In this chapter, we will make use of
generating functions for various sequences
with probabilistic meaning. In particular,
given a ℤ+-valued2) random variable X,
we can consider the corresponding prob-
ability generating function, which is the
generating function of the sequence (pk),
where pk = ℙ[X = k], k ∈ℤ+, describes
the probability mass function of X. Thus,
the probability generating function of X
is given by (writing 𝔼for expectation: see
Section 1.A)
G(s) = GX(s) =
∑
k≥0
skℙ[X = k] = 𝔼[sX].
(1.4)
Example 1.2
(a)
If Y ∼Be(p) and X ∼Bin(n, p) (see
Example 1.22), then GY(s) = (1 −p) +
ps and, by the binomial theorem,
GX(s) = ∑
k≥0
(n
k
)(sp)k(1 −p)n−k =
((1 −p) + ps)n.
(b)
If X ∼Po(𝜆) (see Example 1.23), then
Taylor’s formula implies
GX(s) =
∑
k≥0
sk 𝜆k
k! e−𝜆= e𝜆(s−1).
Notice that GX(1) = ℙ[X < ∞], and thus
for |s| ≤1, |GX(s)| ≤GX(1) ≤1, implying
that the power series GX(s) can be diﬀer-
entiated in the disk |s| < 1 any number of
times. As a result,
G(k)
X (s)=
( d
ds
)k
GX(s) = 𝔼[(X)ksX−k],
|s| < 1,
2) Throughout we use the notation ℤ+ =
{0, 1, 2, …} and ℕ= {1, 2, …}.
where
(x)k =
x!
(x−k)! = x(x −1) · · · (x −k +
1). Taking the limit s ↗1 we obtain the kth
factorial moment of X, 𝔼[(X)k] = G(k)
X (1−),
where the last expression denotes the value
of the kth left derivative of GX(⋅) at 1.
Example 1.3 If X ∼Po(𝜆), from Example
1.2b, we deduce 𝔼[(X)k] = 𝜆k.
Theorem 1.1 If X and Y are independent
ℤ+-valued random variables, then the sum
Z = X + Y has generating function GZ(s) =
GX(s)GY(s).
Proof. As X and Y are independent, so
are sX and sY; consequently, the expec-
tation factorizes, 𝔼[sXsY] = 𝔼[sX]𝔼[sY], by
Theorem 1.18.
Example 1.4
(a)
If X ∼Po(𝜆) and Y ∼Po(𝜇) are inde-
pendent,
then
X + Y ∼Po(𝜆+ 𝜇).
Indeed, by Theorem 1.1, GX+Y(s) =
e(𝜆+𝜇)(s−1), which is the generating
function of the Po(𝜆+ 𝜇) distribution;
the result now follows by uniqueness.
This fact is known as the additive
property of Poisson distributions.
(b)
If
ℤ+-valued
random
variables
X1, … , Xn
are
independent
and
identically distributed (i.i.d.), then
Sn = X1 + · · · + Xn
has
generat-
ing
function
GSn(s) = (GX(s))n.
If
Sn ∼Bin(n, p),
then
Sn
is
a
sum
of
n
independent
Bernoulli
variables
with
parameter
p,
and
so
GSn(s) = ((1 −p) + ps)n
(see
Example 1.22).
The following example takes this idea
further.
Lemma 1.1 Let
X1, X2, …
be
i.i.d.
ℤ+-valued
random
variables,
and
let

6
1 Stochastic Processes
N
be
a
ℤ+-valued
random
variable
independent of the Xi. Then the random
sum SN = X1 + · · · + XN
has generating
function GSN (s) = GN
(GX(s)).
Proof. The claim follows from the partition
theorem for expectations (see Section 1.A):
𝔼[sSN ] =
∑
n≥0
𝔼[sSN ∣N = n]ℙ[N = n]
=
∑
n≥0
(GX(s))nℙ[N = n].
Example 1.5 If (Xk)k≥1 are independent
Be(p) random variables and if N ∼Po(𝜆) is
independent of (Xk)k≥1, then
GSN (s) = GN(GX(s)) = e𝜆(GX(s)−1) = e𝜆p(s−1),
that is, SN ∼Po(𝜆p). This result has the
following important interpretation: if each
of N ∼Po(𝜆) objects is independently
selected with probability p, then the sample
contains SN ∼Po(𝜆p) objects. This fact is
known as the thinning property of Poisson
distributions.
A
further
important
application
of
Lemma 1.1 is discussed in Section 1.2.2.
Example 1.6 [Renewals] A diligent janitor
replaces a light bulb on the same day as
it burns out. Suppose the ﬁrst bulb is put
in on day 0, and let Xi be the lifetime of
the ith bulb. Suppose that the Xi are i.i.d.
random variables with values in ℕand
common generating function Gf (s). Deﬁne
rn = ℙ[a bulb was replaced on day n]
and
fk = ℙ[the ﬁrst bulb was replaced on day k].
Then r0 = 1, f0 = 0, and for n ≥1,
rn = f1rn−1 + f2rn−2 + · · · + fnr0 =
n
∑
k=1
fkrn−k.
(1.5)
Proceeding as in Example 1.1, we deduce
Gr(s) = 1 + Gf (s) Gr(s) for all |s| ≤1, so
that
Gr(s) =
1
1 −Gf (s).
(1.6)
Remark 1.1 The ideas behind Example 1.6
have a vast area of applicability. For
example,
the
hitting
probabilities
of
discrete-time Markov chains have prop-
erty
similar
to
the
Ornstein–Zernike
relation (1.5), see, for example, Lemma 1.3.
Notice also that by repeatedly expanding
each ri on the right-hand side of (1.5), one
can rewrite the latter as
rn =
∑
𝓁≥1
∑
{k1,k2,…,k𝓁}
𝓁
∏
j=1
fkj,
(1.7)
where
the
middle
sum
runs
over
all
decompositions of the integer n into 𝓁
positive integer parts k1, k2, … , k𝓁
(cf.
Example 1.1a).
The
decomposition (1.7)
is an example of the so-called polymer
representation; it arises in many models of
statistical mechanics.
The convolution of sequences (an)n≥0 and
(bn)n≥0 is (cn)n≥0 given by
cn =
n
∑
k=0
akbn−k,
(n ≥0);
(1.8)
we write c = a ⋆b. A key property of con-
volutions is as follows.
Theorem 1.2 (Convolution theorem) If
c = a ⋆b, then the associated generating
functions Gc(s), Ga(s), and Gb(s) satisfy
Gc(s) = Ga(s)Gb(s).
Proof. Compare the coeﬃcients of sn on
both sides of the equality.
The convolution appears very often in
probability theory because the probability

1.2 Generating Functions and Integral Transforms
7
mass function of the sum X + Y of indepen-
dent variables X and Y is the convolution of
their respective probability mass functions.
In other words, Theorem 1.1 is a particular
case of Theorem 1.2.
Remark 1.2 The
sequence
(bn)n≥0
in
Example 1.1b
is
a
convolution
of
the
sequence 1, 1, 1, 1, …
and the sequence
1, 0, 1, 0, 1, 0, … .
The uniqueness property of generating
functions aﬀords them an important role
in studying convergence of probability
distributions.
Theorem 1.3 For every ﬁxed n ≥1, let
the
sequence
an,0, an,1, …
be
a
prob-
ability
distribution
on
ℤ+,
that
is,
an,k ≥0
and
∑
k≥0 an,k = 1.
Moreover,
let Gn(s) = ∑
k≥0 an,ksk be the generating
function of the sequence (an,k)k≥0. Then
lim
n→∞an,k = ak, for all k ≥0
⇐⇒
lim
n→∞Gn(s) = G(s), for all s ∈[0, 1),
where G(s) is the generating function of the
limiting sequence (ak)k≥0.
Example 1.7 [Law
of
rare
events]
Let
(Xn)n≥1 be random variables such that
Xn ∼Bin(n, pn). If n ⋅pn →𝜆as n →∞,
then the distribution of Xn converges to
that of X ∼Po(𝜆). Indeed, for every ﬁxed
s ∈[0, 1), we have, as n →∞,
GXn(s) = (1 + pn(s −1))n →exp{𝜆(s −1)},
which is the generating function of a Po(𝜆)
random variable.
1.2.2
Example: Branching Processes
Let (Zn,k), n ∈ℕ, k ∈ℕ, be a family of i.i.d.
ℤ+-valued random variables with common
probability mass function (pk)k≥0 and ﬁnite
mean. The corresponding branching pro-
cess (Zn)n≥0 is deﬁned via Z0 = 1, and, for
n ≥1,
Zn = Zn,1 + Zn,2 + · · · + Zn,Zn−1,
(1.9)
where an empty sum is interpreted as zero.
The interpretation is that Zn is the num-
ber of individuals in the nth generation of
a population that evolves via a sequence of
such generations, in which each individual
produces oﬀspring according to (pk)k≥0,
independently of all other individuals in the
same generation; generation n + 1 consists
of all the oﬀspring of individuals in gener-
ation n. The process starts (generation 0)
with a single individual and the population
persists as long as the generations are
successful in producing oﬀspring, or until
extinction
occurs.
Branching
processes
appear naturally when modeling chain
reactions, growth of bacteria, epidemics,
and other similar phenomena3): a cru-
cial characteristic of the process is the
probability of extinction.
Let 𝜑n(s) ∶= 𝔼[sZn] be the generating
function of Zn; for simplicity, we write 𝜑(s)
instead of 𝜑1(s) = 𝔼[sZ1]. Then 𝜑0(s) = s,
and a straightforward induction based on
Lemma 1.1 implies
𝜑n(s) = 𝜑n−1(𝜑(s)),
(n > 1).
(1.10)
Equation (1.10) can be used to determine
the distribution of Zn for any n ≥0. In par-
ticular, one easily deduces that 𝔼[Zn] = mn,
where m = 𝔼[Z1] is the expected num-
ber of oﬀspring of a single individual.
3) The simplest branching processes, as discussed
here, are known as Galton–Watson processes
after F. Galton and H. Watson’s work on the
propagation of human surnames; work on
branching processes in the context of nuclear
ﬁssion, by S. Ulam and others, emerged out of
the Manhattan project.

8
1 Stochastic Processes
The long-term behavior of a branching
process is determined by the expected
value m: the process can be subcritical
(m < 1), critical (m = 1), or supercritical
(m > 1).
Remark 1.3 By Markov’s inequality (see
Section 1.A),
ℙ[Zn > 0] = ℙ[Zn ≥1] ≤
𝔼[Zn] = mn. Hence, in the subcritical case,
ℙ[Zn > 0] →0 as n →∞(i.e., Zn →0 in
probability). Moreover, the average total
population in this case is ﬁnite, because
𝔼[∑
n≥0 Zn] = ∑
n≥0 mn = (1 −m)−1 < ∞.
It
follows
that,
with
probability
1,
∑
n≥0 Zn < ∞,
which
entails
Zn →0
almost
surely.
This
last
statement
can
also
be
deduced
from
the
fact
that
∑
n≥0 ℙ[Zn > 0] < ∞
using
the
Borel–Cantelli lemma (see, e.g., [1, 2]).
Extinction is the event = ∪∞
n=1{Zn =
0}.
As
{Zn = 0} ⊆{Zn+1 = 0}
for
all
n ≥0,
the
extinction
probability
𝜌= ℙ[] is well deﬁned (by continuity
of probability measure: see Section 1.A)
via
𝜌= limn→∞ℙ[Zn = 0],
where
ℙ[Zn = 0] = 𝜑n(0) is the probability of
extinction before the (n + 1)th generation.
Theorem 1.4 If
0 < p0 < 1,
then
the
extinction probability 𝜌is given by the
smallest positive solution to the equation
s = 𝜑(s).
(1.11)
In particular, if m = 𝔼[Z] ≤1, then 𝜌= 1;
otherwise, we have 0 < 𝜌< 1.
Remark 1.4 The relation (1.11) has a clear
probabilistic sense. Indeed, by indepen-
dence, ℙ[∣Z1 = k] = 𝜌k
(as extinction
only occurs if each of the independent
branching processes associated with the
k individuals dies out). Then, by the
law of total probability (see Section 1.A),
we get4)
𝜌= ℙ[] =
∑
k≥0
ℙ[∣Z1 = k]ℙ[Z1 = k]
=
∑
k≥0
𝜌kℙ[Z1 = k] = 𝜑(𝜌).
Notice that Theorem 1.4 characterizes
the extinction probability without the
necessity to compute 𝜑n( ⋅). The excluded
values p0 ∈{0, 1} are trivial: if p0 = 0, then
Zn ≥1 for all n ≥0 so that 𝜌= 0; if p0 = 1,
then ℙ[Z1 = 0] = 𝜌= 1.
Proof of Theorem 1.4 Denote 𝜌n = ℙ[Zn =
0] = 𝜑n(0). By continuity and strict mono-
tonicity of the generating function 𝜑( ⋅), we
have (recall (1.10))
0 < 𝜌1 =𝜑(0) < 𝜌2 = 𝜑(𝜌1) < · · · < 1=𝜑(1) ,
so that 𝜌n ↗𝜌∈(0, 1] with 𝜌= 𝜑(𝜌).
Now if 𝜌is another ﬁxed point of 𝜑( ⋅) in
[0, 1], that is, 𝜌= 𝜑(𝜌), then, by induction,
0 < 𝜌1 = 𝜑(0) < 𝜌2 < · · · < 𝜑(𝜌) = 𝜌,
so that 𝜌= limn→∞𝜌n ≤𝜌, that is, 𝜌is the
smallest positive solution to (1.11).
Finally, by continuity and convexity of
𝜑( ⋅) together with the fact 𝜑(1) = 1, the
condition m = 𝜑′(1) ≤1 implies 𝜌= 1 and
the condition m = 𝜑′(1) > 1 implies that 𝜌
is the unique solution in (0, 1) to the ﬁxed
point equation (1.11).
We thus see that the branching process
exhibits a phase transition: in the subcriti-
cal or critical regimes (m ≤1), the process
dies out with probability 1, whereas in the
supercritical case (m > 1) it survives for-
ever with positive probability 1 −𝜌.
4) In Section 1.3, we will see this calculation as
exploiting the Markov property of the branching
process.

1.2 Generating Functions and Integral Transforms
9
1.2.3
Other Transforms
1.2.3.1
Moment Generating Functions
The moment generating function of a real-
valued random variable X is deﬁned by
MX(t) = 𝔼[etX]. When ﬁnite for t in some
neighborhood of 0, MX(t) behaves simi-
larly to the generating function GX(s) in
that it possesses the uniqueness prop-
erty
(identifying
the
corresponding
distribution),
maps
convolutions
(i.e.,
distributions
of
sums
of
independent
variables) into products, and can be used
to
establish
convergence
in
distribu-
tion.
If X is ℤ+-valued and its generating
function GX(s) is ﬁnite for some s > 1, then
MX(t) is also ﬁnite for some t ≠0, and
the two are related by MX(t) = GX(et). For
example, if X ∼Bin(n, p), then MX(t) =
(1 + p(et −1))n (see Example 1.2a). The
terminology arises from the fact that if
MX(t) is diﬀerentiable at 0, then the kth
derivative of MX(t) evaluated at 0 gives
𝔼[Xk], the kth moment of X.
Example 1.8
In the case of a continuous
distribution
X
with
probability
den-
sity f , the moment generating function
MX(t) becomes the integral transform
∫etxf (x)dx.
(a)
Let 𝜆> 0. If X has density f (x) = 𝜆e−𝜆x
for x > 0, and 0 elsewhere, then X
has the exponential distribution with
parameter 𝜆and
MX(t)=∫
∞
0
𝜆e−(𝜆−t)xdx=
𝜆
𝜆−t , (t < 𝜆).
(b)
If X has density f (x) = e−x2∕2∕
√
2𝜋,
x ∈ℝ, then X has the standard nor-
mal (0, 1) or Gaussian distribution
and
MX(t) =
1
√
2𝜋∫
∞
−∞
et2∕2e−(x−t)2∕2dx
= et2∕2,
(t ∈ℝ).
The normal distribution was in part so
named5) for its ubiquity in real data. It
is also very common in probability and
mathematical statistics, owing to a large
extent to results of the following kind.
Theorem 1.5 (de Moivre–Laplace
central limit theorem) Let Xn ∼Bin(n, p)
with
ﬁxed
p ∈(0, 1).
Denote
X∗
n =
(Xn −𝔼[Xn])∕
√
𝕍ar[Xn].
Then
for
any
t ∈ℝ,
MX∗n(t) →et2∕2, as n →∞;
in other words, the distribution of X∗
n con-
verges to that of (0, 1).
Proof. Recall that Xn can be written as
a sum Y1 + Y2 + · · · + Yn of independent
Be(p) random variables.
In particular,
𝔼[Xn] = np and 𝕍ar[Xn] = np(1 −p) (see
Example 1.25). Using the simple relation
MaZ+b(t) = ebtMZ(at), we deduce that the
random variable ̂Y = (Y −p)∕
√
n has the
moment generating function (with ﬁxed t)
e−tp∕
√
n(
1 + p(et∕
√
n −1))
= 1 + t2
2np(1 −p) + O
(
t3
√
n
)
.
5) The term normal (in the sense of “usual”) was
apparently attached to the distribution by Fran-
cis Galton and others and popularized by Karl
Pearson. The distribution arose in the work of
Gauss and Laplace on least squares and errors
of measurement, and also in Maxwell’s work
on statistical physics. Perhaps its ﬁrst tentative
appearance, however, is in the work of Abraham
de Moivre for whom Theorem 1.5 is named. See
[3] for a discussion.

10
1 Stochastic Processes
Noticing that MX∗
n(t)=(M̂Y(t∕
√
p(1 −p)))n
→et2∕2 as n →∞, we deduce the result.
One other application of moment gener-
ating functions that we will see is to large
deviations: see Section 1.4.4.
1.2.3.2
Laplace Transforms
In general, MX(t) might be inﬁnite for
all t ≠0. However, for nonnegative vari-
ables X ≥0, we have MX(t) = 𝔼[etX] ≤1,
for all t ≤0; in particular, MX(t) is an
analytic function at every point of the
complex plane with negative real part. In
this case, MX(t) behaves very similarly to
generating functions and inherits the main
properties described above. In such a sit-
uation, the function MX(t) (or, sometimes
MX(−t)) is called the Laplace transform
of the variable X. See Chapter 15 in the
present volume for background on Laplace
transforms.
1.2.3.3
Characteristic Functions
Unlike the moment generating function
MX(t), which might be inﬁnite for real
t ≠0, the characteristic function 𝜓X(t) =
𝔼[eitX] (where i2 = −1) always exists and
uniquely identiﬁes the distribution, hence
the name.6) The characteristic functions
inherit all nice properties of (moment)
generating
functions,
though
inverting
them is not always straightforward.
Characteristic functions are the standard
tool of choice for proving results such as the
following generalization of Theorem 1.5.
The proof is similar to that of the previous
theorem, based on a Taylor-type formula: if
𝔼[X2n] < ∞, then
6) The term characteristic function is traditional
in the probabilistic context for what elsewhere
might be called the Fourier transform: see
Chapter 15 of the present volume.
𝜓X(t) =
2n
∑
𝓁=0
(it)𝓁
𝓁! 𝔼[X𝓁] + o(t2n).
Theorem 1.6 (Central limit theorem)
Let Xn = Y1 + Y2 + · · · + Yn, where Yi are
i.i.d. random variables with 𝔼[Y 2
i ] < ∞.
Then,
as
n →∞,
the
distribution
of
X∗
n = (Xn −𝔼[Xn])∕
√
𝕍ar[Xn] converges to
the standard normal, (0, 1).
1.3
Markov Chains in Discrete Time
1.3.1
What is a Markov Chain?
Our tour begins with stochastic processes
in discrete time. Here, we will write our
process as X0, X1, X2, …. A fundamental
class is constituted by the Markov pro-
cesses in which, roughly speaking, given
the present, the future is independent of
the past. In this section, we treat the case
where the Xn take values in a discrete (i.e.,
ﬁnite or countably inﬁnite) state space
S. In this case, the general term Markov
process is often specialized to a Markov
chain, although the usage is not universally
consistent.
The process X = (Xn) taking values in the
discrete set S satisﬁes the Markov property
if, for any n and any i, j, ∈S,7)
ℙ[Xn+1 = j ∣Xn = i, Xn−1 = in−1, … ,
X0 = i0] = pij,
(1.12)
for all previous histories i0, … , in−1 ∈S.
The pij = ℙ[Xn+1 = j ∣Xn = i] are the
one-step transition probabilities for X, and
they satisfy the obvious conditions pij ≥0
for all i, j, and ∑
j∈S pij = 1 for all i. It is
convenient to arrange the pij as a matrix
P = (pij)i,j∈S with nonnegative entries and
7) In (1.12) and elsewhere, we indicate intersec-
tions of events by commas for readability.

1.3 Markov Chains in Discrete Time
11
whose rows all sum to 1: these properties
deﬁne a stochastic matrix. The Markov
property speciﬁes the step-by-step evolu-
tion of the Markov chain. One can imagine
a particle moving at random on S, from
state i selecting its next location according
to distribution (pij)j∈S. It is an exercise in
conditional probability to deduce from
(1.12) that
ℙ[X1 = i1, … , Xn = in ∣X0 = i0]
= pi0i1 · · · pin−1in.
It may be that X0 is itself random, in which
case, in order to assign a probability to
any particular (ﬁnite) sequence of moves
for the particle, in addition to the Markov
property, we also need to know where the
particle starts: this is speciﬁed by the initial
distribution ℙ[X0 = i] = wi, i ∈S.
Now, to compute the probability of get-
ting from i to k in two steps, we sum over
the j-partition to get
ℙ[X2 = k ∣X0 = i]
=
∑
j∈S
ℙ[X1 = j, X2 = k ∣X0 = i]
=
∑
j∈S
pijpjk = (P2)ik,
the (i, k) entry in P2 = P ⋅P; this matrix
multiplication yields the two-step transi-
tion probability. More generally, the n-step
transition probabilities are
p(n)
ij ∶= ℙ[Xn = j ∣X0 = i] = (Pn)ij.
A similar argument shows the following:
p(n+m)
ij
=
∑
k∈S
p(n)
ik p(m)
kj .
(1.13)
Remark 1.5 We restrict ourselves to the
case of time-homogeneous Markov chains,
by stipulating that (1.12) should hold simul-
taneously for all n. One may relax this, and
allow pij to depend also on n.
Remark 1.6 An equivalent deﬁnition of a
Markov chain is as a randomized dynam-
ical system: Xn+1 = f (Xn; Un+1) where f ∶
S × [0, 1] →S is a ﬁxed update function,
and U1, U2, … are independent U[0, 1] ran-
dom variables.8) Monte Carlo simulation of
a Markov chain uses such a scheme.
We make a ﬁnal remark on notation: for
pij, and similar expressions, we sometimes
write pi,j if otherwise the subscripts may be
ambiguous.
1.3.2
Some Examples
Example 1.9 [The Ehrenfest model] In
1907, Ehrenfest and Ehrenfest [4] intro-
duced this simple model of diﬀusion. There
are N particles in a container that has two
chambers separated by a permeable par-
tition. At each step, a particle is chosen
uniformly at random and moved across the
partition. The state of the Markov chain at
each time will be the number of particles
in the ﬁrst chamber, say, so S = {0, … , N}.
The one-step transition probabilities are,
for i ∈{1, … , N −1},
pi,i+1 = N −i
N
,
pi,i−1 = i
N ,
and p0,1 = pN,N−1 = 1.
After a long time, what is the distribution
of the particles? See Section 1.3.6.
Example 1.10 [One-dimensional
simple
random walk] A particle moves at random
on the state space S = ℤ+. From position
i ≠0, the particle jumps one step to the
left with probability pi and one step to the
right with probability 1 −pi. With partial
reﬂection at 0, we can describe this random
8) That is, uniform on [0, 1], having density f (x) =
1 for x ∈[0, 1] and f (x) = 0 elsewhere.

12
1 Stochastic Processes
walk by a Markov chain with one-step
transition probabilities p0,0 = p0 ∈(0, 1),
p0,1 = q0 ∶= 1 −p0, and for i ≥1,
pi,i−1 = pi ∈(0, 1),
pi,i+1 = qi ∶= 1 −pi.
1.3.3
Stationary Distribution
We use the compact notation ℙi for the
(conditional) probability associated with
the Markov chain started from state i ∈S,
that is, ℙi[ ⋅] = ℙ[ ⋅∣X0 = i]. More gener-
ally, if w = (wi)i∈S is a distribution on S (i.e.,
wi ≥0, ∑
i wi = 1), then we write ℙw for
the Markov chain started from the initial
distribution w, that is, ℙw[ ⋅] = ∑
i wiℙi[ ⋅].
A distribution 𝜋= (𝜋i)i∈S is a stationary
distribution for a Markov chain X if
ℙ𝜋[X1 = i] = 𝜋i, for all i ∈S.
(1.14)
Viewing a stationary distribution 𝜋as
a row vector, (1.14) is equivalent to the
matrix-vector equation 𝜋P = 𝜋, that is, 𝜋is
a left eigenvector of P corresponding to the
eigenvalue 1. The nomenclature arises from
the fact that (1.14) implies that ℙ𝜋[Xn =
i] = 𝜋i for all times n, so the distribution of
the Markov chain started according to 𝜋is
stationary in time.
Example 1.11 [A three-state chain] Con-
sider a Markov chain (Xn) with the state
space {1, 2, 3} and transition matrix
P =
⎛
⎜
⎜
⎜⎝
1
2
1
2
0
1
3
1
3
1
3
0
1
2
1
2
⎞
⎟
⎟
⎟⎠
.
(1.15)
We look for a stationary distribution
𝜋= (𝜋1, 𝜋2, 𝜋3). Now 𝜋P = 𝜋
with the
fact that 𝜋1 + 𝜋2 + 𝜋3 = 1 gives a sys-
tem of equations with unique solution
𝜋= ( 2
7, 3
7, 2
7).
A Markov chain with transition probabil-
ities pij is reversible with respect to a dis-
tribution 𝜋= (𝜋i)i∈S if the detailed balance
equations hold:
𝜋ipij = 𝜋jpji, for all i, j ∈S.
(1.16)
Not every Markov chain is reversible. Any
distribution 𝜋satisfying (1.16) is necessar-
ily a stationary distribution, because then,
for all j, ∑
i 𝜋ipij = ∑
i 𝜋jpji = 𝜋j. If the chain
is reversible, then the system of equations
(1.16) is often simpler to solve than the
equations 𝜋P = 𝜋: see Example 1.12. The
“physical” interpretation of reversibility is
that, in equilibrium, the Markov chain is
statistically indistinguishable from a copy
of the chain running backward in time.
Example 1.12 [Random
walk]
Con-
sider Example 1.10. We seek a solution
𝜋= (𝜋i)
to
(1.16),
which
now
reads
𝜋iqi = 𝜋i+1pi+1 for all i ≥0. The solution
is 𝜋i = 𝜋0
∏i−1
j=0(qj∕pj+1). This describes a
proper distribution if ∑
i 𝜋i = 1, that is, if
∞
∑
i=0
i−1
∏
j=0
qj
pj+1
< ∞.
(1.17)
If (1.17) holds, then
𝜋i =
∏i−1
j=0
qj
pj+1
∑∞
i=0
∏i−1
j=0
qj
pj+1
.
For example, if pi = p ∈(0, 1) for all i, then
(1.17) holds if and only if p > 1∕2, in which
case 𝜋i = (q∕(1 −2p))(q∕p)i, where q = 1 −
p, an exponentially decaying stationary dis-
tribution.
1.3.4
The Strong Markov Property
One way of stating the Markov property
is to say that (Xn)n≥m, conditional on

1.3 Markov Chains in Discrete Time
13
{Xm = i}, is distributed as the Markov
chain (Xn)n≥0 with initial state X0 = i. It is
often desirable to extend such a statement
from deterministic times m to random
times T. An important class of random
times are the ﬁrst passage times,9)
Ti ∶= min{n ≥1 ∶Xn = i}, i ∈S.
(1.18)
The Markov property cannot hold at
every
random
time.
For
instance,
if
T′ = Ti −1, then the ﬁrst transition of
the process XT′, XT′+1, … is always from
XT′ to XT′+1 = i, regardless of the original
transition matrix.
The following strong Markov property
clariﬁes these issues. A random time T ∈
ℤ+ ∪{∞} is a stopping time with respect to
(Xn) if, for any n, the event {T ≤n} depends
only on X0, … , Xn (and not on the future
evolution of the chain). The passage times
Ti are stopping times, but T′ described
above is not a stopping time.
Lemma 1.2 Suppose that T is a stopping
time for (Xn). Then, given T < ∞and XT =
i, (XT+n)n≥0 has the same distribution as
(Xn)n≥0 started from X0 = i.
Sketch of proof
Partition over the possi-
ble values of T. Suppose that T = m and
XT = Xm = i; this is a condition only on
X0, … , Xm, because T is a stopping time.
Now apply the usual Markov property at
the deterministic time m.
1.3.5
The One-Step Method
In problems involving Markov chains,
often quantities of interest are hitting prob-
abilities and expected hitting times. One
approach to computing these is via the
9) Here and elsewhere, the convention min ∅= ∞
is in force.
powerful one-step method, which makes
essential use of the Markov property.
Recall the deﬁnition of the passage times
Ti from (1.18). The expected hitting time of
state j starting from state i is 𝔼i[Tj] for i ≠j;
if i = j this is the expected return time to i.
Also of interest is ℙi[Tj < Tk], the probabil-
ity of reaching state j before state k, starting
from i. We illustrate the one-step method
by some examples.
Example 1.13 [Three-state
chain]
We
return to Example 1.11. We partition over
the ﬁrst step of the process to obtain, via the
law of total probability (see Section 1.A),
ℙ2[T1 < T3] =
3
∑
k=1
ℙ2[{T1 < T3} ∩{X1 = k}]
= p2,1 ⋅1 + p2,2 ⋅ℙ2[T1 < T3]
+ p2,3 ⋅0,
by the Markov property. This gives ℙ2[T1 <
T3] = 1∕2.
What about 𝔼2[T1]? Set zi = 𝔼i[T1].
Again we condition on the ﬁrst step,
and now use the partition theorem for
expectations (see Section 1.A):
𝔼2[T1] = 1 + 𝔼2[T1 −1]
= 1 +
3
∑
k=1
p2,k𝔼2[T1 −1 ∣X1 = k].
Now applying the Markov property at time
1, we see that T1 −1, given X0 ≠1 and
X1 = k ≠1, has the same distribution as
T1 given X0 = k ≠1 in the original chain,
and, in particular, has expected value zk.
On the other hand, if X1 = k = 1, then
T1 −1 = 0. So we get z2 = 1 + 1
3z2 + 1
3z3. A
similar argument starting from state 3 gives
z3 = 1 + 1
2z2 + 1
2z3. This system of linear
equations is easily solved to give z2 = 5 and
z3 = 7.

14
1 Stochastic Processes
Example 1.14 [Random
walk;
gambler’s
ruin] Recall Example 1.10. Fix n ∈ℕand
for i ∈{1, … , n −1}, let ui = ℙi[Tn < T0].
The one-step method gives
ui = piui−1 + qiui+1,
(1 ≤i ≤n −1),
with boundary conditions u0 = 0, un = 1.
The standard method to solve this sys-
tem of equations is to rewrite it in
terms of the diﬀerences Δi = ui+1 −ui
to get Δi = Δi−1(pi∕qi) for 1 ≤i ≤n −1,
which yields Δj = Δ0
∏j
k=1(pk∕qk). Then
ui = ∑i−1
j=0 Δj, using the boundary condition
at 0. Using the boundary condition at n to
ﬁx Δ0, the solution obtained is
ui =
∑i−1
j=0
∏j
k=1
pk
qk
∑n−1
j=0
∏j
k=1
pk
qk
.
(1.19)
In the special case where pi = qi = 1∕2 for
all i, we have the elegant formula ui = i∕n.
If we imagine that the state of the Markov
chain is the wealth of a gambler with initial
wealth i who plays a sequence of fair games,
each time either gaining or losing a unit of
wealth, 1 −ui is the ruin probability (and
ui is the probability that the gambler makes
his fortune).
1.3.6
Further Computational Methods
We present by example some additional
techniques.
Example 1.15 [Matrix diagonalization] In
many situations, we want to compute the
n-step transition probability p(n)
ij , that is, an
entry in the matrix power Pn. To calcu-
late Pn, we try to diagonalize P to obtain
P = TΛT−1 for an invertible matrix T and
a diagonal matrix Λ. The usefulness of this
representation is that Pn = TΛnT−1 and Λn
is easy to write down, because Λ is diagonal.
A suﬃcient condition for P to be diagonal-
izable is that all its eigenvalues be distinct.
Consider again the three-state chain with
transition matrix given by (1.15); we have
three eigenvalues, 𝜆1, 𝜆2, 𝜆3, say. As P is a
stochastic matrix, 1 is always an eigenvalue:
𝜆1 = 1, say. Then because tr P = 𝜆1 + 𝜆2 +
𝜆3 and det P = 𝜆1𝜆2𝜆3, we ﬁnd 𝜆2 = 1∕2
and 𝜆3 = −1∕6, say.
It follows from the diagonalized repre-
sentation that
Pn = 𝜆n
1U1 + 𝜆n
2U2 + 𝜆n
3U3
= U1 +
(1
2
)n
U2 +
(
−1
6
)n
U3,
(1.20)
where U1, U2, U3 are 3 × 3 matrices to be
determined. One can solve the simultane-
ous matrix equations arising from the cases
n ∈{0, 1, 2} of (1.20) to obtain U1, U2, and
U3, and hence,
Pn =
⎛
⎜
⎜
⎜⎝
2
7
3
7
2
7
2
7
3
7
2
7
2
7
3
7
2
7
⎞
⎟
⎟
⎟⎠
+
(1
2
)n ⎛
⎜
⎜
⎜⎝
1
2
0
−1
2
0
0
0
−1
2
0
1
2
⎞
⎟
⎟
⎟⎠
+
(
−1
6
)n ⎛
⎜
⎜
⎜⎝
3
14
−3
7
3
14
−2
7
4
7
−2
7
3
14
−3
7
3
14
⎞
⎟
⎟
⎟⎠
.
It follows that limn→∞p(n)
ij
exists, does
not depend on i, and is equal to 𝜋j, the
component of the stationary distribu-
tion that we calculated in Example 1.11.
After a long time, the chain “forgets” its
starting state and approaches a stochastic
equilibrium described by the stationary
distribution. This is an example of a gen-
eral phenomenon to which we return in
Section 1.3.7.
Example 1.16 [Generating functions] We
sketch the use of generating functions to

1.3 Markov Chains in Discrete Time
15
evaluate stationary distributions. Consider
the
Ehrenfest
model
of
Example 1.9.
Suppose that 𝜋= (𝜋0, … , 𝜋N) is a station-
ary distribution for the Markov chain, with
generating function
̂𝜋(s) = ∑N
i=0 𝜋isi. In
this case, the equation 𝜋P = 𝜋reads
𝜋j−1
N −(j −1)
N
+ 𝜋j+1
j + 1
N
= 𝜋j,
which is valid for all j ∈{0, … , N}, pro-
vided we set 𝜋−1 = 𝜋N+1 = 0. Now multiply
through by sj and sum from j = 0 to N. After
some algebra, we obtain
̂𝜋(s) = 1 −s2
N
̂𝜋′(s) + s ̂𝜋(s),
so that d∕ds log ̂𝜋(s) = ̂𝜋′(s)∕̂𝜋(s) = N∕(1 +
s). Integrating with respect to s and using
the fact that ̂𝜋(1) = 1, we obtain
̂𝜋(s) =
(1 + s
2
)N
.
The binomial theorem now enables us to
identify 𝜋i = 2−N(N
i
).
1.3.7
Long-term Behavior; Irreducibility;
Periodicity
We saw in Example 1.15 a Markov chain for
which
lim
n→∞ℙi[Xn = j] = lim
n→∞p(n)
ij = 𝜋j,
(1.21)
for all i, j ∈S, where 𝜋j is from a stationary
distribution. For which Markov chains does
such a result hold? There are (at least) three
obstacles:
(a)
There might be no solutions to 𝜋P =
𝜋, and hence, no right-hand side in
(1.21).
(b)
There might be multiple solutions
to 𝜋P = 𝜋, and so no uniqueness
in (1.21). For example, consider the
Markov chain on the state space
{0, 1, 2} with p00 = 1, p22 = 1 (0
and 2 are absorbing states) and
p10 = p12 = 1∕2. Then p(n)
i2 = i∕2 for
all n ≥1, that is, the limit on the
left-hand side of (1.21) depends on
the starting state i. Note that here
𝜋= (𝛼, 0, 1 −𝛼) is stationary for any
𝛼∈[0, 1].
(c)
In
the
Ehrenfest
model
of
Example 1.9, there is a parity eﬀect,
because
p(n)
00 = 0
for
odd
n,
for
instance. This phenomenon is an
example
of
periodicity,
which
is
another obstacle to (1.21).
Cases (b) and (c) here can be dealt with
after some additional concepts are intro-
duced. A state i ∈S has period d if d is the
greatest common divisor of {n ≥1 ∶p(n)
ii >
0}. For example, all states in the Ehrenfest
model have period 2.
A Markov chain is irreducible if, for all
i, j ∈S, there exist ﬁnite m and n for which
p(n)
ij > 0 and p(m)
ji
> 0, that is, it is possible
to get between any two states in a ﬁnite
number of steps. For the rest of this section,
we will assume that we have an irreducible
Markov chain. We do not discuss the case
of nonirreducible (reducible) chains in a
systematic way, but Section 1.3.10 provides
an illustrative example.
For an irreducible chain, it can be shown
that all states have the same period, in
which case one can speak about the period
of the chain itself. If all states have period 1,
the chain is called aperiodic.
Recall the deﬁnition of Ti from (1.18):
𝔼i[Ti] is the expected return time to i. The
following result answers our question on
the limiting behavior of p(n)
ij .
Theorem 1.7 For an irreducible Markov
chain, the following are equivalent.

16
1 Stochastic Processes
• There exists a unique stationary
distribution 𝜋.
• For some i ∈S, 𝔼i[Ti] < ∞.
• For all i ∈S, 𝔼i[Ti] < ∞.
If these conditions hold, the Markov chain
is called positive recurrent. For a positive-
recurrent chain, the following hold.
• For all i ∈S, 𝜋i = 1∕𝔼i[Ti].
• If the chain is aperiodic, then
ℙi[Xn = j] →𝜋j for all i, j ∈S.
In particular, we have the following
result.
Theorem 1.8 An irreducible Markov chain
on a ﬁnite state space is positive recurrent.
Proofs of these results can be found in
[5, 6], for instance.
1.3.8
Recurrence and Transience
Recall the deﬁnition of Ti from (1.18). A
state i ∈S is called recurrent if ℙi[Ti <
∞] = 1 or transient if ℙi[Ti = ∞] > 0. A
Markov chain will return inﬁnitely often to
a recurrent state, but will visit a transient
state only ﬁnitely often. If a Markov chain is
irreducible (see Section 1.3.7), then either
all states are recurrent, or none is, and so
we can speak of recurrence or transience of
the chain itself.
If an irreducible chain is positive recur-
rent (see Theorem 1.7), then it is neces-
sarily recurrent. A chain that is recurrent
but not positive recurrent is null recurrent,
in which case, for all i, ℙi[Ti < ∞] = 1 but
𝔼i[Ti] = ∞(equivalently, it is recurrent but
no stationary distribution exists). Because
of Theorem 1.8, we know that to observe
null recurrence or transience we must look
at inﬁnite state spaces.
Example 1.17 [One-dimensional random
walk] We return to Example 1.10. Consider
ℙ0[T0 = ∞]. In order for the walk to never
return to 0, the ﬁrst step must be to 1, and
then, starting from 1, the walk must reach
n before 0 for every n ≥2. Thus
ℙ0[T0 = ∞] = q0ℙ1[T0 = ∞]
= q0ℙ1[∩n≥2{Tn < T0}].
Note
{Tn+1 < T0} ⊆{Tn < T0},
so
the
intersection here is over a decreasing
sequence of events. Thus by continuity
of probability measures (see Section 1.A),
ℙ0[T0 = ∞] = q0 limn→∞ℙ1[Tn < T0].
Here ℙ1[Tn < T0] = 1 −u1 where u1 is
given by (1.19). So we obtain
ℙ0[T0 = ∞] > 0 if and only if
∞
∑
j=0
j∏
k=1
(pk
qk
)
< ∞.
(For further discussion, see [7, pp. 65–71])
In particular, if pk = p ∈(0, 1) for all k,
the walk is transient if p < 1∕2 and recur-
rent if p ≥1∕2. The phase transition can
be probed more precisely by taking pk =
1∕2 + c∕k; in this case, the walk is transient
if and only if c < −1∕4, a result due to Har-
ris and greatly generalized by Lamperti [8].
We give one criterion for recurrence that
we will use in Section 1.4.
Lemma 1.3 For any i ∈S, ℙi[Ti < ∞] = 1
if and only if ∑∞
n=0 p(n)
ii = ∞.
We give a proof via generating functions.
Write f (n)
i
= ℙi[Ti = n], the probability that
the ﬁrst return to i occurs at time n; here
f (0)
i
= 0; note that ∑
n f (n)
i
may be less than
1. Denote the corresponding generating
function by 𝜙i(s) = ∑∞
n=0 f (n)
i
sn. Also deﬁne
𝜓i(s) = ∑∞
n=0 p(n)
ii sn, where p(n)
ii = ℙi[Xn = i]
(so
p(0)
ii = 1).
By
conditioning
on
the

1.3 Markov Chains in Discrete Time
17
value of Ti, the strong Markov property
gives
p(n)
ii =
n
∑
m=0
f (m)
i
p(n−m)
ii
, (n ≥1).
Treating the case n = 0 carefully, it follows
that
𝜓i(s) = 1 +
∞
∑
n=0
n
∑
m=0
f (m)
i
p(n−m)
ii
sn.
The ﬁnal term here is a discrete con-
volution
of
the
generating
function
(cf. Theorem 1.2),
so
we
deduce
the
important renewal relation
𝜓i(s) = 1 + 𝜙i(s)𝜓i(s).
(1.22)
Sketch of proof of Lemma 1.3
We have
ℙi[Ti < ∞] = lims↑1 𝜙i(s),
and
(1.22)
implies that the latter limit is 1 if and
only if lims↑1 𝜓i(s) = ∞.
1.3.9
Remarks on General State Spaces
In the case of discrete state spaces, (1.13)
corresponds to the trivial matrix equation
Pn+m = Pn ⋅Pm, which one could describe,
rather grandly, as the semigroup property
of matrix multiplication. More generally,
(1.13) is an instance of the fundamental
Chapman–Kolmogorov relation, and the
connection
to
semigroup
theory
runs
deep.
In a general state space, the analogue of
the transition probability pij is a transition
kernel p(x; A) given by p(x; A) = ℙ[Xn+1 ∈
A ∣Xn = x]. This immediately introduces
technical issues that can only be addressed
in the context of measure theory. We refer
to [2, 9], for example.
1.3.10
Example: Bak–Sneppen and Related
Models
Bak and Sneppen [10] introduced a sim-
ple stochastic model of evolution that
initiated a considerable body of research
by
physicists
and
mathematicians.
In
the original model, N sites are arranged
in a ring. Each site, corresponding to a
species in the evolution model, is initially
assigned an independent U[0, 1] random
variable representing a “ﬁtness” value for
the species. The Bak–Sneppen model is
a discrete-time Markov process, where at
each step the minimal ﬁtness value and
the values at the two neighboring sites
are replaced by three independent U[0, 1]
random variables.
This process is a Markov process on
the continuous state space [0, 1]N, and
its behavior is still not fully understood,
despite a large physics literature devoted
to these models: see the thesis [11] for an
overview of the mathematical results.
Here we treat a much simpler model, fol-
lowing [12]. The state space of our pro-
cess (Xn) will be the “simplex” of ranked
sequence of N ﬁtness values
ΔN ∶= {(x(1), … , x(N)) ∈[0, 1]N ∶
x(1) ≤· · · ≤x(n)}.
Fix a parameter k ∈{1, … , N}. We start
with N independent U[0, 1] values: rank
these to get X0. Given Xn, discard the
kth-ranked value X(k)
n
and replace it by a
new independent U[0, 1] random variable;
rerank to get Xn+1.
For example, if k = 1, we replace the min-
imal value at each step. It is natural to antic-
ipate that Xn should approach (as n →∞)
a limiting (stationary) distribution; observe
that the value of the second-ranked ﬁtness
cannot decrease. A candidate limit is not

18
1 Stochastic Processes
hard to come by: the distribution of the ran-
dom vector (U, 1, 1, 1, … , 1) (a U[0, 1] vari-
able followed by N −1 units) is invariant
under the evolution of the Markov chain.
We show the following result.
Proposition 1.1 Let
N ∈ℕ
and
k ∈{1, 2, … , N}. If at each step we replace
the kth-ranked value by an independent
U[0, 1] value, then, as n →∞,
(X(1)
n , X(2)
n , … , X(N)
n ) →(0, … , 0, U, 1, … , 1),
in distribution,10) where the kth coordinate
of the limit vector U ∼U[0, 1].
The process Xn lives on a continuous
state space, and it might seem that some
fairly sophisticated argument would be
needed to show that it has a unique sta-
tionary distribution. In fact, we can reduce
the problem to a simpler problem on a
ﬁnite state space as follows.
Sketch of proof of Proposition 1.1
We
sketch the argument from [12]. For each
s ∈[0, 1], deﬁne the counting function11)
Cn(s) ∶= ∑N
i=1 1{X(i)
n ≤s},
the
number
of ﬁtnesses of value at most s at time
n. Then Cn(s) is a Markov chain on
{0, 1, 2, … , N}. The transition probabil-
ities px,y = ℙ[Cn+1(s) = y ∣Cn(s) = x] are
given for x ∈{0, … , k −1} by px,x = 1 −s
and px,x+1 = s, and for x ∈{k, … , N} by
px,x = s and px,x−1 = 1 −s. For s ∈(0, 1),
the Markov chain is reducible and all
states are transient apart from those in
the recurrent class Sk = {k −1, k}. The
chain will eventually enter Sk and then
never exit. So the problem reduces to
10) That is, for any x1, … , xN ∈[0, 1],
ℙ[X(1)
n
≤x1, … , X(k)
n
≤xk, … , X(N)
n
≤xN] →xk if
xk+1 · · · xN = 1 and 0 otherwise.
11) “1{ ⋅}” is the indicator random variable of the
appended event: see Section 1.A.
that of the two-state restricted chain on
Sk. It is easy to compute the stationary
distribution and for s ∈(0, 1), analogously
to Theorem 1.7,
lim
n→∞ℙ[Cn(s) = x]=
⎧
⎪
⎨
⎪⎩
1 −s if x = k −1
s
if x = k
0
if n ∉{k −1, k}
.
In particular, for s ∈(0, 1),
lim
n→∞ℙ[X(k)
n
≤s] = lim
n→∞ℙ[Cn(s) ≥k] = s.
That is, X(k)
n
converges in distribution to
a U[0, 1] variable. Moreover, if k > 1, for
any
s ∈(0, 1),
ℙ[X(k−1)
n
≤s] = ℙ[Cn(s) ≥
k −1] →1,
which
implies
that
X(k−1)
n
converges in probability to 0. Similarly,
if k < N, for any s ∈(0, 1), ℙ[X(k+1)
n
≤
s] = ℙ[Cn(s) ≥k + 1] →0, which implies
that X(k+1)
n
converges in probability to 1.
Combining these marginal results, an addi-
tional technical step gives the claimed joint
convergence: we refer to [12] for details.
1.4
Random Walks
A drunk man will eventually ﬁnd his
way home, but a drunk bird may get
lost for ever.
– S. Kakutani’s rendering of Pólya’s
theorem [1, p. 191].
1.4.1
Simple Symmetric Random Walk
The term random walk can refer to many
diﬀerent models or classes of models.
Although random walks in one dimen-
sion had been studied in the context of
games of chance, serious study of random

1.4 Random Walks
19
walks as stochastic processes emerged in
pioneering works in several branches of
science around 1900: Lord Rayleigh’s [13]
theory of sound developed from about
1880, Bachelier’s [14] 1900 model of stock
prices, Pearson and Blakeman’s [15] 1906
theory of random migration of species, and
Einstein’s [16] theory of Brownian motion
developed during 1905–1908.
In this section, we restrict attention to
simple symmetric random walk on the inte-
ger lattice ℤd. This model had been consid-
ered by Lord Rayleigh, but the preeminent
early contribution came from George Pólya
[17]: we describe his recurrence theorem
in the following text. The phrase “random
walk” was ﬁrst applied by statistical pio-
neer Pearson [18] to a diﬀerent model in a
1905 letter to Nature. We refer to [19] for
an overview of a variety of random walk
models.
Let e1, … , ed be the standard orthonor-
mal lattice basis vectors for ℤd. Let
X1, X2, … be i.i.d. random vectors with
ℙ[X1 = ei] = ℙ[X1 = −ei]
= 1
2d , for i ∈{1, … , d}.
Let S0 = 0 and Sn = ∑n
i=1 Xi. Then (Sn)n∈ℤ+
is a simple symmetric random walk on ℤd,
started from 0; “simple” refers to the fact
that the jumps are of size 1.
1.4.2
Pólya’s Recurrence Theorem
Clearly (Sn) is a Markov chain; a fundamen-
tal question is whether it is recurrent or
transient (see Section 1.3.8). Pólya [17] pro-
vided the answer in 1921.
Theorem 1.9 (Pólya’s theorem) A simple
symmetric random walk on ℤd is recurrent
if d = 1 or 2 but transient if d ≥3.
A basic component in the proof is a com-
binatorial statement.
Lemma 1.4 For d ∈ℕand any n ∈ℤ+, we
have
ℙ[S2n = 0] = (2d)−2n
(
2n
n
)
∑
n1+···+nd=n
(
n!
n1! · · · nd!
)2
,
where the sum is over d-tuples of nonnega-
tive integers n1, … , nd that sum to n.
Proof. Each path of length 2n (i.e., the pos-
sible trajectory for S0, S1, … , S2n) has prob-
ability (2d)−2n. Any such path that ﬁnishes
at its starting point must, in each coordi-
nate i, take the same number ni steps in the
positive and negative directions. Enumerat-
ing all such paths, we obtain
ℙ[S2n = 0] = (2d)−2n
∑
n1+···+nd=n
(2n)!
(n1! · · · nd!)2 ,
from which the given formula follows.
Lemma 1.4 and a careful asymptotic
analysis using Stirling’s formula for n!
yields the following result.
Lemma 1.5 For d ∈ℕ, as n →∞,
nd∕2ℙ[S2n = 0] →
(
d
4𝜋
)d∕2
.
Proof of Theorem 1.9 Apply the criterion in
Lemma 1.3 with Lemma 1.5.
1.4.3
One-dimensional Case; Reﬂection
Principle
We consider in more detail the case
d = 1.
Let
Ta ∶= min{n ≥1 ∶Sn = a}.
Theorem 1.9 says that ℙ[T0 < ∞] = 1. The
next result gives the distribution of T0.

20
1 Stochastic Processes
Theorem 1.10
(i)
For
any
n ∈ℤ+,
ℙ[T0 = 2n] = (1∕(2n −1))(2n
n
)2−2n.
(ii) 𝔼[T𝛼
0 ] < ∞if and only if 𝛼< 1∕2.
We proceed by counting sample paths,
following the classic treatment by Feller
[20, chap. 3]. By an n-path we mean
a sequence of integers s0, … , sn where
|si+1 −si| = 1; for an n-path from a to b we
add the requirement that s0 = a and sn = b.
We view paths as space–time trajectories
(0, s0), (1, s1), … , (n, sn).
Let Nn(a, b) denote the number of n-
paths from a to b. Let N0
n(a, b) be the num-
ber of such paths that visit 0. An n-path
from a to b must take (n + b −a)∕2 posi-
tive steps and (n + a −b)∕2 negative steps,
so
Nn(a, b) =
(
n
1
2(n + b −a)
)
,
(1.23)
where we interpret (n
y
) as 0 if y is not an
integer in the range 0 to n.
Lemma 1.6 (Reﬂection principle) If
a, b > 0, then N0
n(a, b) = Nn(−a, b).
Proof. Each n-path from −a to b must visit
0 for the ﬁrst time at some c ∈{1, … , n −
1}. Reﬂect in the horizontal (time) axis the
segment of this path over [0, c] to obtain
an n-path from a to b which visits 0. This
reﬂection is one-to-one.
Theorem 1.11 (Ballot theorem) If b > 0,
then the number of n-paths from 0 to b
which do not revisit 0 is b
nNn(0, b).
Proof. The ﬁrst step of such a path must
be 1, so their number is Nn−1(1, b) −
N0
n−1(1, b) = Nn−1(1, b) −Nn−1(−1, b),
by
Lemma 1.6. Now use (1.23).
Theorem 1.12 If b ≠0 and n ≥1, ℙ[T0 >
n, Sn = b] = |b|
n ℙ[Sn = b].
Proof. Suppose b > 0. The event in ques-
tion occurs if and only if the walk does not
visit 0 during [1, n], and Sn = b. By the bal-
lot theorem, the number of such paths is
b
nNn(0, b). Similarly for b < 0.
At this point, we are ready to prove
Theorem 1.10, but ﬁrst we take a slight
detour to illustrate one further variation on
“reﬂection.”
Theorem 1.13 For
a ≠0
and
n ≥1,
ℙ[Ta = n] = |a|
n ℙ[Sn = a].
Proof via time reversal. Fix n. If the trajec-
tory of the original walk up to time n is
(S0, S1, S2, … , Sn)
=
(
0, X1, X1 + X2, … ,
n
∑
i=1
Xi
)
,
then the trajectory of the reversed walk is
(R0, R1, R2, … , Rn)
=
(
0, Xn, Xn + Xn−1, … ,
n
∑
i=1
Xi
)
,
that is, the increments are taken in reverse
order. The reversed walk has the same
distribution as the original walk, because
the Xi are i.i.d.
Suppose a > 0. The original walk has
Sn = a and T0 > n if and only if the reversed
walk has Rn = a and Rn −Rn−i = X1 + · · · +
Xi > 0 for all i ≥1, that is, the ﬁrst visit of
the reversed walk to a happens at time n. So
ℙ[Ta = n] = ℙ[T0 > n, Sn = a]. Now apply
Theorem 1.12.
Proof of Theorem 1.10
If T0 = 2n, then
S2n−1 = ±1. Thus
ℙ[T0 = 2n] = ℙ[T = 2n, S2n−1 = 1]
+ ℙ[T = 2n, S2n−1 = −1]

1.4 Random Walks
21
= 1
2ℙ[T0 > 2n −1, S2n−1 = 1]
+1
2ℙ[T0 >2n −1, S2n−1 =−1].
Now by Theorem 1.12,
ℙ[T0 = 2n] = 1
2 ⋅
1
2n −1
(ℙ[S2n−1 = 1] + ℙ[S2n−1 = −1]) ,
and part (i) of the theorem follows from
(1.23), after simpliﬁcation. For part (ii), we
have that 𝔼[T𝛼
0 ] = ∑∞
n=1(2n)𝛼ℙ[T0 = 2n],
and Stirling’s formula shows that the sum-
mand here is asymptotically a constant
times n𝛼−(3∕2).
Remark 1.7
(i)
An alternative approach
to Theorem 1.10 is via the remarkable
identity
ℙ[T0 > 2n] = ℙ[S2n = 0],
which can be veriﬁed by a direct but
more
sophisticated
combinatorial
argument: see, for example, [21].
(ii) Yet another approach uses generating
functions. For Sn,
𝜓(s) ∶= 𝔼[sSn] =
∞
∑
n=0
s2n
(
2n
n
)
2−2n
=
1
√
1 −s2
,
by
(1.23)
and
then
Maclaurin’s
theorem.
Then
if
𝜙
is
the
gen-
erating
function
for
T0,
we
can
exploit the renewal relation 𝜓(t) =
1 + 𝜙(t)𝜓(t) (see (1.22)) to obtain
𝜙(s) = 1 −
√
1 −s2, from which we
can deduce Theorem 1.10 once more.
1.4.4
Large Deviations and Maxima of Random
Walks
In this section, we consider more general
one-dimensional random walks in order
to illustrate some further concepts. Again
we take Sn = ∑n
i=1 Xi where the Xi are
i.i.d., but now the distribution of Xi will
be arbitrary subject to the existence of
the mean 𝔼[Xi] = 𝜇. Suppose that 𝜇< 0.
The strong law of large numbers shows
that n−1Sn →𝜇, almost surely, as n →∞.
So if 𝜇< 0, then Sn will tend to −∞, and
in particular the maximum of the walk
M = maxn>0 Sn is well deﬁned.
There are many applications for the
study of M, for example, the modeling
of queues (see [22]). What properties
does the random variable M possess?
We might want to ﬁnd ℙ[M > x], for
any x, but it is often diﬃcult to obtain
exact
results;
instead
we
attempt
to
understand the asymptotic behavior as
x →∞.
Let 𝜑(t) = 𝔼[etX1] be the moment gen-
erating function of the increments. It can
be shown that the behavior of ℙ[M > x]
depends on the form of 𝜑. Here we con-
sider only the classical (light-tailed) case in
which there exists 𝛾> 0 such that 𝜑(𝛾) =
1 and 𝜑′(𝛾) < ∞. For details of the other
cases, see [23].
First, Boole’s inequality (see Section 1.A)
gives
ℙ[M > x]=ℙ[∪∞
n=1{Sn >x}] ≤
∞
∑
n=1
ℙ[Sn > x].
(1.24)
Now the Chernoﬀbound (see Section 1.A)
implies that, for any 𝜃∈[0, 𝛾],
ℙ[Sn > x] ≤e−𝜃x𝔼[e𝜃Sn] = e−𝜃x(𝜑(𝜃))n;
cf. Example 1.4b. We substitute this into
(1.24) to obtain
ℙ[M > x] ≤e−𝜃x
∞
∑
n=1
(𝜑(𝜃))n = e−𝜃x
𝜑(𝜃)
1 −𝜑(𝜃),
provided 𝜑(𝜃) < 1, which is the case if 𝜃∈
(0, 𝛾). For any such 𝜃, we get

22
1 Stochastic Processes
lim sup
x→∞
1
x log ℙ[M > x]
≤−𝜃−lim
x→∞
1
x log(1 −𝜑(𝜃)) = −𝜃.
As 𝜃< 𝛾was arbitrary, we obtain the
sharpest bound on letting 𝜃↗𝛾. The
matching lower bound can also be proved
(see [22]), to conclude that
lim
x→∞
1
x log ℙ[M > x] = −𝛾.
This is an example of a general class of
results referred to as large deviations: fur-
ther details of the general theory can be
found in [24], for example. These tech-
niques have found use in many application
areas including statistical physics: see, for
example, [25].
1.5
Markov Chains in Continuous Time
1.5.1
Markov Property, Transition Function, and
Chapman–Kolmogorov Relation
In many applications, it is natural to work
in continuous time rather than the discrete
time of Section 1.3. As before, we assume
that we have a discrete state space S, but
now our Markov chains X = (X(t)) have
a continuous- time parameter t ∈[0, ∞).
Continuous time introduces analytical dif-
ﬁculties, which we will not dwell on in this
presentation.
As in the discrete-time case, we concen-
trate on time-homogeneous chains, and we
will specify the law of (X(t)) in line with the
Markovian idea that “given the present, the
future is independent of the past.”
The process (X(t)) satisﬁes the Markov
property in continuous time if, for all t, h ≥
0, all i, j ∈S, all 0 ≤t0 < t1 < · · · < tn < t,
and all i1, … , in ∈S,
ℙ[X(t + h) = j ∣X(t) = i, X(tn) = in, … ,
X(t1) = i1] = pij(h).
Here pij( ⋅) = ℙ[X(t + ⋅) = j ∣X(t) = i] is
the transition function of the Markov chain.
As in the discrete-time case, it is convenient
to use matrix notation:
P(t) = (pij(t))i,j∈S given by
pij(t) = ℙi[X(t) = j],
where again a subscript on ℙindicates an
initial state, that is, ℙi[ ⋅] = ℙ[ ⋅∣X(0) =
i]. We can obtain full information on the
law of the Markov chain, analogously to
the discrete-time case. For example, for 0 <
t1 < · · · < tn and j1, … , jn ∈S,
ℙi[X(t1) = j1, … , X(tn) = jn]
= pij1(t1)pj1j2(t2 −t1) · · · pjn−1jn(tn −tn−1).
To this we add information about the ini-
tial distribution: for instance, ℙ[X(t) = j] =
∑
i∈S ℙ[X(0) = i]pij(t). Here we must have
pij(0) = 𝛿ij ∶=
{
1
if i = j
0
if i ≠j
.
We also assume that the transition func-
tions satisfy, for each ﬁxed t, pij(t) ≥0 for
all i, j and ∑
j∈S pij(t) = 1 for all i.
To describe our Markov chain now
seems a formidable task: we must specify
the family of functions P(t). However, we
will see in the next section that the Markov
property enables a local (inﬁnitesimal)
description. First we state a global conse-
quence of the Markov property, namely,
Chapman–Kolmogorov relation
For any s, t ≥0, P(s + t) = P(s)P(t). (1.25)
The fundamental Markovian relation
(1.25) is a special case of the relation

1.5 Markov Chains in Continuous Time
23
known
to
probabilists
as
the
Chap-
man–Kolmogorov equation, and which,
in its most general form, is often taken as
the starting point of the general theory
of Markov processes. Physicists refer to a
relation such as (1.25) as a master equation.
The derivation of (1.25) in our setting is
direct from the Markov property:
ℙi[X(s + t) = j]
=
∑
k∈S
ℙi[X(s) = k, X(s + t) = j]
=
∑
k∈S
ℙi[X(s)=k]ℙi[X(s + t)=j ∣X(s)=k]
=
∑
k∈S
ℙi[X(s) = k]ℙk[X(t) = j],
which is the equality for the (i, j) entry in the
matrix equation (1.25).
1.5.2
Inﬁnitesimal Rates and Q-matrices
In continuous time, there is no smallest
time step and so no concept of a one-step
transition. Often, however, one can encap-
sulate the information in the functions pij(t)
in a single fundamental matrix associated
with the Markov chain, which will serve as
an analogue to the P-matrix in the discrete
theory. This is the Q-matrix.
To proceed, we need to assume some
regularity. We call the chain standard if the
transition probabilities are continuous at 0,
that is, if pij(t) →pij(0) = 𝛿ij as t ↓0.
Lemma 1.7 Suppose that X is a standard
Markov chain with transition functions
pij(t). Then for each i, j, pij(t) is a contin-
uous and diﬀerentiable function of t. The
derivatives p′
ij(t) evaluated at t = 0 we
denote by qij ∶= p′
ij(0); then 0 ≤qij < ∞for
i ≠j and 0 ≤−qii ≤∞.
The proof of this result relies on the
Chapman–Kolmogorov relation, but is
somewhat
involved:
see,
for
example,
[26,
Section 14.1].
A
Taylor’s
formula
expansion now reads
pij(h) = ℙ[X(t + h) = j ∣X(t) = i]
= pij(0) + qijh + o(h)
(1.26)
= 𝛿ij + qijh + o(h), as h ↓0.
(1.27)
So, for i ≠j, qij is the (instantaneous) tran-
sition rate of the process from state i to state
j. It is convenient to deﬁne qi ∶= −qii for all
i (so qi ≥0). Then qi is the rate of departure
from state i.
We further assume that the chain is con-
servative, meaning
∑
j≠i
qij = qi < ∞, for all i.
(1.28)
Note that ∑
j≠i pij(t) = 1 −pii(t), so, for
example, if S is ﬁnite we can diﬀerentiate
to immediately get the equality in (1.28),
and then ∑
j≠i qij < ∞by Lemma 1.7, so a
ﬁnite Markov chain is always conservative.
Note that (1.28) implies that ∑
j qij = 0
and qi = ∑
j≠i qij, so the rows of Q sum to
zero.
The matrix Q = (qij)i,j∈S is called the
transition
rate
matrix,
the
generator
matrix, or simply the Q-matrix of the
Markov chain; it eﬀectively describes the
chain’s dynamics. In particular, under rea-
sonable conditions (see the following text)
the functions pij(⋅) are uniquely determined
by Q. Thus, in applications, a Markov pro-
cess is often deﬁned via a Q-matrix and an
initial distribution.12)
Conversely, given a matrix Q = (qij)i,j∈S
with nonpositive diagonal entries and non-
negative entries elsewhere for which (1.28)
holds, there always exists a Markov process
with Q as transition rate matrix. This fact
12) This is also essentially the approach taken in
[6].

24
1 Stochastic Processes
can be proved by actually constructing the
paths of such a process: see Section 1.5.4.
Example 1.18 [Birth-and-death process]
Here S = ℤ+ and X(t) represents a pop-
ulation size at time t. The size of the
population increases on a birth or decreases
on a death. The nonzero entries in the
Q-matrix are
qi,i+1 = 𝜆i, i ≥0 (birth rate in state i),
qi,i−1 = 𝜇i, i ≥1 (mortality rate in state i),
q0,0 = −𝜆0 and qi,i = −(𝜆i + 𝜇i), i ≥1.
In a linear process, 𝜆i = 𝜆i and 𝜇i = 𝜇i,
so 𝜆and 𝜇can be interpreted as per
individual rates of birth and mortality,
respectively.
1.5.3
Kolmogorov Diﬀerential Equations
We
now
consider
some
diﬀerential
equations which, given Q, can be used
to determine the functions pij(⋅). The start-
ing point is the Chapman–Kolmogorov
relation
pij(s + t) = ∑
k∈S pik(s)pkj(t).
If
S is ﬁnite, say, then it is legitimate to
diﬀerentiate with respect to s to get
p′
ij(s + t) =
∑
k∈S
p′
ik(s)pkj(t).
Now setting s = 0, we obtain
p′
ij(t) =
∑
k∈S
qikpkj(t),
which
is
the
Kolmogorov
backward
equation. If instead, we diﬀerentiate with
respect to t and then put t = 0, we obtain
(after a change of variable)
p′
ij(t) =
∑
k∈S
pik(t)qkj,
which is the Kolmogorov forward equation.
These diﬀerential equations are particu-
larly compact in matrix form.
Theorem 1.14 Given Q satisfying (1.28),
we have
P′(t) = P(t)Q,
(Kolmogorov forward equation);
P′(t) = QP(t),
(Kolmogorov backward equation).
We sketched the derivation in the case
where S is ﬁnite. In the general case, a
proof can be found in, for example, [26,
Section 14.2].
Remark 1.8 Suitable versions of the Kol-
mogorov diﬀerential equations also apply
to processes on continuous state spaces,
such as diﬀusions, where they take the
form of partial diﬀerential equations. In
this context, the forward equation can
be framed as a Fokker–Planck equation
for the evolution of a probability den-
sity. The connections among diﬀusions,
boundary-value problems, and potential
theory are explored, for example, in [2]; an
approach to Fokker–Planck equations from
a more physical perspective can be found
in [27].
We give one example of how to use
the Kolmogorov equations, together with
generating functions, to compute P(t)
from Q.
Example 1.19 [Homogeneous birth process]
We consider a special case of Example 1.18
with only births, where, for all i, 𝜆i = 𝜆> 0
and 𝜇i = 0. So qi,i = −𝜆and qi,i+1 = 𝜆. The
Kolmogorov forward equation in this case
gives
p′
i,j(t) = −𝜆pi,j(t) + 𝜆pi,j−1(t),

1.5 Markov Chains in Continuous Time
25
where we interpret pi,−1(t) as 0. In particu-
lar, if i = 0, we have
p′
0,j(t) = −𝜆p0,j(t) + 𝜆p0,j−1(t).
(1.29)
The initial conditions are assumed to be
p0,0(0) = 1 and p0,i(0) = 0 for i ≥1, so that
the process starts in state 0. Consider the
generating function
𝜙t(u) = 𝔼[uX(t)] =
∞
∑
j=0
p0,j(t)uj, |u| < 1.
Multiplying both sides of (1.29) by sj and
summing over j we get
𝜕
𝜕t 𝜙t(u) = −𝜆𝜙t(u) + 𝜆u𝜙t(u)
= −𝜆(1 −u)𝜙t(u).
It follows that 𝜙t(u) = A(u)e−(1−u)𝜆t. The
initial conditions imply that 𝜙0(u) = 1, so
in fact A(u) = 1 here, and 𝜙t(u) = e−(1−u)𝜆t,
which is the probability generating func-
tion of a Poisson distribution with mean
𝜆t (see Example 1.2). Hence, X(t) ∼Po(𝜆t).
In fact X(t) is an example of a Pois-
son process: see, for example, [2, 5, 6,
26].
By analogy with the scalar case, under
suitable conditions one can deﬁne the
matrix exponential
exp{Qt} =
∞
∑
k=0
Qktk
k! ,
with
Q0 = I
(identity).
Then
P(t) =
exp{Qt} is a formal solution to both
the
Kolmogorov
forward
and
back-
ward
equations.
In
analytic
terminol-
ogy, Q is the generator of the semi-
group P.
1.5.4
Exponential Holding-Time Construction;
“Gillespie’s Algorithm”
Given the Q-matrix one can construct sam-
ple paths of a continuous- time Markov
chain. The following scheme also tells you
how to simulate a continuous-time Markov
chain.
Suppose the chain starts in a ﬁxed state
X(0) = i for i ∈S. Let 𝜏0 = 0 and deﬁne
recursively for n ≥0,
𝜏n+1 = inf{t ≥𝜏n ∶X(t) ≠X(𝜏n)}.
Thus 𝜏n is the nth jump time of X, that is,
the nth time at which the process changes
its state.
How long does the chain stay in a partic-
ular state? We have
ℙi[𝜏1 > t + h ∣𝜏1 > t]
= ℙi[𝜏1 > t + h ∣𝜏1 > t, X(t) = i]
= ℙi[𝜏1 > h],
by the Markov property. This mermory-
less property is indicative of the exponential
distribution (see Example 1.8a). Recall that
Y ∼exp(𝜆) if ℙ[Y > t] = e−𝜆t, t ≥0. A cal-
culation shows that
ℙ[Y > t + h ∣Y > t] = ℙ[Y > h]
= e−𝜆h = 1 −𝜆h + o(h),
(1.30)
as h →0.
In fact, the exponential distribution
is essentially the only distribution with
this property. So it turns out that 𝜏1 is
exponential. A heuristic calculation, which
can be justiﬁed, suggests that ℙi[𝜏1 >
h] ∼ℙi[X(h) = i] = pii(h) = 1 −qih + o(h).
A comparison with (1.30) suggests that
𝜏1 ∼exp(qi).
When the chain does jump, where
does it go? Now, for j ≠i, ℙ[X(t + h) = j

26
1 Stochastic Processes
∣X(t) = i] = pij(h) = qijh + o(h),
while
ℙ[X(t + h) ≠i ∣X(t) = i] = qih + o(h), so a
conditional probability calculations gives
ℙ[X(t + h) = j ∣X(t) = i, X(t + h) ≠i]
=
qij
qi
+ o(1).
Careful argument along these lines (see,
e.g., [26, Section 14.3]) gives the next
result.13)
Theorem 1.15 Under the law ℙi of the
Markov chain started in X(0) = i, the
random
variables
𝜏1
and
X(𝜏1)
are
independent.
The
distribution
of
𝜏1
is exponential with rate qi. Moreover,
ℙi[X(𝜏1) = j] = qij∕qi.
Perhaps the most striking aspect of this
result is that the holding time and the jump
destination are independent. Theorem 1.15
tells us how to construct the Markov chain,
by iterating the following procedure.
• Given 𝜏n and X(𝜏n) = i, generate an
exp(qi) random variable Yn (this is easily
done via Yn = −q−1
i
log Un, where
Un ∼U[0, 1]). Set 𝜏n+1 = 𝜏n + Yn.
• Select the next state X(𝜏n+1) according to
the distribution qij∕qi.
Although this standard construction of
Markov chain sample paths goes back to
classical work of Doeblin, Doob, Feller,
and others in the 1940s, and was even
implemented by Kendall and Bartlett in
pioneering computer simulations in the
early 1950s, the scheme is known in certain
applied circles as “Gillespie’s algorithm”
after Gillespie’s 1977 paper that rederived
the construction in the context of chemical
reaction modeling.
13) Actually Theorem 1.15 assumes that we are
working with the minimal version of the pro-
cess: see, for example, [6, 26] for details of this
technical point.
Remark 1.9 Let
X∗
n = X(𝜏n).
Then
(X∗
n)n∈ℤ+ deﬁnes a discrete-time Markov
chain, called the jump chain associated
with X(t), with one-step transitions
p∗
ij =
{ qij
qi
if i ≠j,
0
if i = j.
,
as long as qi > 0. If qi = 0, then p∗
ii = 1, that
is, i is an absorbing state.
1.5.5
Resolvent Computations
Consider a Markov chain with transition
functions pij(t) determined by its generator
matrix Q. The Laplace transform of pij is rij
given by
rij(𝜆) = ∫
∞
0
e−𝜆tpij(t)dt.
(1.31)
Then R(𝜆) = (rij(𝜆))i,j∈S is the resolvent
matrix of the chain. A formal calculation,
which can be justiﬁed under the conditions
in force in this section, shows that R(𝜆)
can be expressed as the matrix inverse
R(𝜆) = (𝜆I −Q)−1, where I is the identity.
See Chapter 15 of the present volume for
background on Laplace transforms.
Let 𝜏be an exp(𝜆) random variable, inde-
pendent of the Markov chain. Then
𝜆rij(𝜆) = ∫
∞
0
𝜆e−𝜆tpij(t)dt
= ∫
∞
0
ℙ[𝜏∈dt]ℙi[X(t) = j ∣𝜏= t],
which is just ℙi[X(𝜏) = j], the probability
that, starting from state i, the chain is in
state j at the random time 𝜏.
Resolvents play an important role in the
theoretical development of Markov pro-
cesses, and in particular in the abstract
semigroup approach to the theory. Here,
however, we view the resolvent as a com-
putational tool, which enables, in principle,

1.5 Markov Chains in Continuous Time
27
calculation of probabilities and hitting-time
distributions.
Speciﬁcally, (1.31) implies that pij(t)
can be recovered by inverting its Laplace
transform
̂pij(𝜆) = rij(𝜆).
Moreover,
let
Ti ∶= inf{t ≥0 ∶X(t) = i}, the ﬁrst hitting
time of state i. Write Fij(t) ∶= ℙi[Tj ≤t],
and set fij(t) = F′
ij(t) for the density of
the hitting-time distribution. We proceed
analogously
to
the
discrete
argument
for the proof of Lemma 1.3. An applica-
tion of the strong Markov property for
continuous-time chains (cf Section 1.3.4)
gives,
pij(t) = ∫
t
0
fij(s)pjj(t −s)ds.
The
convolution
theorem
for
Laplace
transforms (see Chapter 15) implies that
the Laplace transform of fij is given by
̂fij(𝜆) =
rij(𝜆)
rjj(𝜆).
(1.32)
In the next section, we give some examples
of using resolvent ideas in computations.
1.5.6
Example: A Model of Deposition,
Diﬀusion, and Adsorption
We describe a continuous-time Markov
model of deposition of particles that sub-
sequently
perform
random
walks
and
interact to form barriers according to an
occupation criterion, inspired by models
of submonolayer ﬁlm growth [28, 29].
Particles arrive randomly one by one on a
one-dimensional substrate SN ∶= {0, 1, … ,
N + 1} and diﬀuse until M ≥2 particles
end up at the same site, when they clump
together (“nucleate”) to form an “island.”
Islands
form
absorbing
barriers
with
respect to the diﬀusion of other particles.
We assume that initially, sites 0 and N + 1
are occupied by M particles (so are already
islands) but all other sites are empty.
The Markov dynamics are as follows.
• At each site x ∈SN, new particles arrive
independently at rate 𝜌> 0.
• If at any time a site is occupied by M or
more particles, all those particles are
held in place and are inactive. Particles
that are not inactive are active.
• Each active particle independently
performs a symmetric simple random
walk at rate 1, that is, from x it jumps to
x + 1 or x −1 each at rate 1∕2.
A state 𝜔of the Markov process is a
vector of the occupancies of the sites
1, … , N (it is not necessary to keep track
of the occupancies of 0 or N + 1): 𝜔(x)
is the number of particles at site x. We
can simulate the process via the expo-
nential
holding-time
construction
of
Section 1.5.4. To do so, we need to keep
track of T(𝜔) = ∑
1≤x≤N 𝜔(x)1{𝜔(x) < M},
the total number of active particles in state
𝜔. The waiting time in a state 𝜔is then
exponential with parameter T(𝜔) + N𝜌;
at the end of this time, with probabil-
ity T(𝜔)∕(T(𝜔) + N𝜌), one of the active
particles jumps (chosen uniformly from
all active particles, and equally likely to
be a jump left or right), else a new par-
ticle arrives at a uniform random site in
{1, … , N}.
An analysis of the general model just
described would be interesting but is
beyond the scope of this presentation. We
use small examples (in terms of M and N) to
illustrate the resolvent methods described
in Section 1.5.5. For simplicity, we take
M = 2 and stop the process the ﬁrst time
that two particles occupy any internal site.
Conﬁgurations can be viewed as elements
of {0, 1, 2}{1,2,…,N}, but symmetry can be
used to further reduce the state space for
our questions of interest.

28
1 Stochastic Processes
1.5.6.1
N = 1
Take N = 1. The state space for our Markov
chain X(t) is {0, 1, 2}, the number of parti-
cles in position 1, with X(0) = 0, and 2 as
the absorbing state. Clearly X(t) = 2 even-
tually; the only question is how long we
have to wait for absorption. The answer is
not trivial, even in this minimal example.
The generator matrix for the Markov
chain is
Q =
⎛
⎜
⎜⎝
−𝜌
𝜌
0
1
−1 −𝜌
𝜌
0
0
0
⎞
⎟
⎟⎠
, and so
𝜆I −Q =
⎛
⎜
⎜⎝
𝜆+ 𝜌
−𝜌
0
−1
1 + 𝜆+ 𝜌
−𝜌
0
0
𝜆
⎞
⎟
⎟⎠
.
To work out p02(t), we compute r02(𝜆):
r02(𝜆) = (𝜆I −Q)−1
02 =
det
(
−𝜌
0
1 + 𝜆+ 𝜌
−𝜌
)
det(𝜆I −Q)
=
𝜌2
𝜆((𝜆+ 𝜌)2 + 𝜆).
Inverting the Laplace transform, we obtain
1−p02(t)=e−((1+2𝜌)∕(2))t
(
cosh
(( t
2
) √
1+4𝜌
)
+ 1+2𝜌
√
1+4𝜌
sinh
(( t
2
)√
1+4𝜌
))
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
−3𝜌
2𝜌
𝜌
0
0
0
0
0
1
2
−1 −3𝜌
1
2
𝜌
𝜌
0
𝜌
0
0
1
−1 −3𝜌
2𝜌
0
0
0
𝜌
0
0
1
2
−2 −3𝜌
1
2
𝜌
1
2 + 𝜌
1
2 + 𝜌
0
1
0
1
−2 −3𝜌
𝜌
2𝜌
0
0
0
0
1
0
−3 −3𝜌
1 + 2𝜌
1 + 𝜌
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
.
∼1
2
(
1 +
1 + 2𝜌
√
1 + 4𝜌
)
exp
{
−t
2
(
1 + 2𝜌−
√
1 + 4𝜌
)}
,
as t →∞.
For example, if 𝜌= 3∕4, the exact expres-
sion simpliﬁes to
p02(t) = 1 −9
8e−t∕4 + 1
8e−9t∕4.
As in this case, 2 is absorbing, ℙ[T2 ≤t] =
ℙ[X(t) = 2] = p02(t), so
f02(t) = d
dt p02(t) = 9
32(e−t∕4 −e−9t∕4),
in the 𝜌= 3∕4 example. The same answer
can be obtained using (1.32).
1.5.6.2
N = 3
For the problem of the distribution of the
point of the ﬁrst collision, the ﬁrst nontriv-
ial case is N = 3. Making use of the symme-
try, we can now describe the Markov chain
with the 8 states
(0, 0, 0), (1, 0, 0), (0, 1, 0), (1, 1, 0),
(1, 0, 1), (1, 1, 1), (2, ∗, ∗), (∗, 2, ∗),
in that order, where each ∗indicates either
a 0 or a 1; so for example (1, 0, 0) stands for
(1, 0, 0) or (0, 0, 1). The generator matrix is
now

1.6 Gibbs and Markov Random Fields
29
The probability of the ﬁrst collision
occurring at the midpoint is
z(𝜌) = ℙ[ lim
t→∞X(t) = (∗, 2, ∗)].
According to MAPLE, inverting the appro-
priate Laplace transform gives
z(𝜌)= 1
9 ⋅486𝜌4+1354𝜌3+1375𝜌2+598𝜌+87
162𝜌4+414𝜌3+392𝜌2+160𝜌+ 23 .
In particular, if deposition dominates diﬀu-
sion,
lim
𝜌→∞z(𝜌) = 1
9 ⋅486
162 = 1
3,
which is as it should be!
1.6
Gibbs and Markov Random Fields
We have so far focused on stochastic
processes that vary through time. In this
section and in Section 1.7, we take a detour
into spatial processes. The role of space
in our models is played by an underlying
graph structure, describing vertices V and
the edges E that connect them. This section
is devoted to random ﬁelds, that is, ensem-
bles of random variables associated with
the vertices subject to certain constraints
imposed by the edges.
Let G = (V, E) be an undirected ﬁnite
graph with vertex set V and edge set
E consisting of pairs (i, j) where i, j ∈V;
(i, j) ∈E indicates an edge between vertices
i and j.14) With this graph, we associate
random variables {Xi} for i ∈V; to keep
things simple, we take Xi ∈{−1, 1}.
We consider two ways of specifying these
random variables – as Gibbs or Markov
14) Our graphs are undirected, which means that
(i, j) = (j, i) is an unordered pair.
random ﬁelds
–
and the relationship
between the two. Finally, we will consider
how to use ideas from Markov chains
(Section 1.3) to simulate a Markov random
ﬁeld.
1.6.1
Gibbs Random Field
To deﬁne a Gibbs random ﬁeld, we need the
concept of a clique. A clique in G = (V, E) is
a subset K of V such that E contains all the
possible edges between members of K; we
include the set of no vertices ∅as a clique.
Let be the set of all cliques of graph G.
Let x = (x1, … , x|V|) ∈{−1, 1}V denote
an assignment of a value ±1 to each ver-
tex of G. For K ⊆V we write x|K for the
restriction of x to K. The random variables
{Xi}i∈V on G constitute a Gibbs random
ﬁeld if, for all x ∈{−1, 1}V,
ℙ[Xi = xi for all i ∈V]
= 1
Z exp
{ ∑
K∈
fK(x|K)
}
,
(1.33)
where fK ∶{−1, 1}|K| →ℝfor each clique
K, and Z is a normalization:
Z =
∑
x∈{−1,1}V
exp
{ ∑
K∈
fK(x|K)
}
.
To see why this is a natural deﬁnition
for the law of {Xi} in many situations,
consider
associating
an
energy
func-
tion
∶{−1, 1}V →ℝ
to
the
states.
We seek the maximum entropy distri-
bution for {Xi}i∈V
for a given mean
energy. So we want to ﬁnd the proba-
bility mass function, f , on {−1, 1}V that
maximizes −∑
x∈{−1,1}V f (x) log f (x) sub-
ject to ∑
x∈{−1,1}V f (x)(x) = const. One
can show this is achieved by
f (x) ∝exp{−𝛽(x)},

30
1 Stochastic Processes
where 𝛽∈(0, ∞) is chosen to obtain the
required energy. (The analogy here is
between 𝛽and the inverse temperature
1∕(kT) in thermodynamics.) Now if can
be decomposed as a sum over the cliques
of the graph, we recover (1.33).
Example 1.20 [Ising model] Consider the
N × N grid in two dimensions. We label
the vertices by members of the set LN =
{1, … , N} × {1, … , N}. We put an edge
between i = (i1, i2) and j = (j1, j2) if |i1 −
j1| + |i2 −j2| = 1; this adjacency condition
we write as i ∼j. Here the clique set is
made up of the empty set, singleton nodes,
and pairs of nodes that are distance one
apart in the lattice.
Given
constants
𝛽> 0,
J > 0,
and
h ∈ℝwe consider the Gibbs random
ﬁeld
with
probability
mass
function
f (x) = Z−1 exp{−𝛽(x)}, where
(x) = −J
∑
i,j∈LN ∶i∼j
XiXj −h
∑
i∈LN
Xi.
The sum over pairs of nodes (the interac-
tion term) means that neighboring nodes
have a propensity to be in the same state.
The second (external ﬁeld) term leads
to nodes more likely to be in either of
the states 1 or −1, depending on the sign
of h.
This model has been studied widely as a
model for ferromagnetism and was initially
proposed by Ising [30] under the guidance
of Lenz. The Potts model is a generaliza-
tion with q-valued states and more general
interactions: see [31].
1.6.2
Markov Random Field
We now consider a second speciﬁcation
of random ﬁeld that adapts the Markov
property to spatial processes. For a given
subset W ⊆V, we deﬁne its boundary as
𝜕W = {v ∈V ⧵W ∶(v, w) ∈E
for some w ∈W}.
The concept of Markov random ﬁeld
extends the temporal Markov property
(1.12), which said that, conditional on the
previous states of the process, the future
depends on the past only through the
present, to a spatial (or topological) one.
This “Markov property” will say that the
state of nodes in some set of vertices W
conditioned on the state of all the other
vertices only depends on the state of the
vertices in 𝜕W.
The random variables {Xi}i∈V on G con-
stitute a Markov random ﬁeld if
• they have a positive probability mass
function,
ℙ[{Xi}i∈V = x] > 0,
for all x ∈{−1, 1}V,
• and obey the global Markov property: for
all W ⊆V,
ℙ[{Xi}i∈W =x|W ∣{Xi}i∈V⧵W =x|V⧵W]
= ℙ[{Xi}i∈W =x|W ∣{Xi}i∈𝜕W =x|𝜕W].
Example 1.21
As in Example 1.20, con-
sider a random ﬁeld taking values ±1 on the
vertices of the N × N lattice. We specify the
(conditional) probability that a vertex, i, is
in state 1, given the states of its neighbors
to be
e𝛽(h+Jyi)
e−𝛽(h+Jyi) + e𝛽(h+Jyi) , where yi =
∑
j∼i
xj.
(1.34)
Here 𝛽> 0, J > 0, and h ∈ℝare param-
eters. The larger yi, which is the number
of neighbours of i with spin +1 minus the

1.7 Percolation
31
number with spin −1, the greater the prob-
ability that vertex i will itself have state 1.
1.6.3
Connection Between Gibbs and Markov
Random Fields
In Examples 1.20 and 1.21, we have used
the same notation for the parameters. In
fact, both speciﬁcations (one Gibbs, the
other Markov) deﬁne the same probability
measure on {−1, 1}V. This is an example of
the following result.
Theorem 1.16 (Hammersley–Cliﬀord
theorem) The ensemble of random vari-
ables {Xi}i∈V on G is a Markov random ﬁeld
if and only if it is a Gibbs random ﬁeld with
a positive probability mass function.
A proof can be found in [31]. From this
point forward, we will use the terms Gibbs
random ﬁeld and Markov random ﬁeld
interchangeably.
1.6.4
Simulation Using Markov Chain Monte
Carlo
Direct simulation of a Gibbs random ﬁeld
on a graph is computationally diﬃcult
because the calculation of the normalizing
constant, Z, requires a sum over all the pos-
sible conﬁgurations. In many situations,
this is impractical. Here we consider an
alternative way to simulate a Gibbs random
ﬁeld making use of Markov chains.
We saw in Section 1.3.7 that an irre-
ducible, aperiodic Markov chain converges
to its stationary distribution. The idea now
is to design a Markov chain on the state
space {−1, 1}V whose stationary distribu-
tion coincides with the desired Gibbs ran-
dom ﬁeld. We simulate the Markov chain
for a long time to obtain what should be a
distribution close to stationarity, and hence,
a good approximation to a realization of the
Gibbs random ﬁeld.
We initialize the Markov chain with any
initial state 𝜎∈{−1, 1}V. To update the
state of the chain, we randomly select a ver-
tex uniformly from all vertices in the graph.
We will update the state associated with
this vertex by randomly selecting a new
state using the conditional probabilities
given, subject to the neighboring vertices’
states, taking advantage of the Markov
random ﬁeld description. For example, in
Example 1.21, we set the node state to 1
with probability given by (1.34) and to −1,
otherwise.
It is easy to check that this Markov
chain is irreducible, aperiodic, and has the
required stationary distribution. This is an
example of a more general methodology of
using a Markov chain with simple update
steps to simulate from a distribution that
is computationally diﬃcult to evaluate
directly, called Markov chain Monte Carlo.
Speciﬁcally, we have used a Gibbs sampler
here, but there are many other schemes for
creating a Markov chain with the correct
stationary distribution. Many of these
techniques have been developed within the
setting of Bayesian statistics but have appli-
cations in many other ﬁelds, including spin
glass models and theoretical chemistry.
1.7
Percolation
Consider the inﬁnite square lattice ℤ2.
Independently, for each edge in the lattice,
the edge is declared open with probability
p; else (with probability 1 −p) it is closed.
This model is called bond percolation on
the square lattice.
For two vertices x, y ∈ℤ2 write x ←→y
if x and y are joined by a path consisting of
open edges in the percolation model on ℤ2.

32
1 Stochastic Processes
The open cluster containing vertex x, C(x),
is the (random) set of all vertices joined to
x by an open path
C(x) ∶= {y ∈ℤ2 ∶y ←→x}.
A fundamental question in percolation
regards the nature of C(0), the open cluster
at 0, and how its (statistical) properties
depend on the parameter p.
We write ℙp for the probability associ-
ated with percolation with parameter p.
Write |C(0)| for the number of vertices in
the open cluster at 0. The percolation prob-
ability is
𝜃(p) = ℙp[|C(0)| = ∞].
By translation invariance, 𝜃(p) ∈[0, 1] is
not speciﬁc to the origin: ℙp[|C(x)| = ∞] =
𝜃(p) for any x.
Let H∞be the event that |C(x)| = ∞for
some x. It is not hard to show that
𝜃(p) = 0 =⇒ℙp[H∞] = 0
𝜃(p) > 0 =⇒ℙp[H∞] = 1.
We state a fundamental result that may
seem obvious; the proof we give, due to
Hammersley, demonstrates the eﬀective-
ness of another probabilistic tool: coupling.
Lemma 1.8 𝜃(p) is nondecreasing as a
function of p.
Proof. List the edges of the lattice ℤ2 in
some order as e1, e2, …. Let U1, U2, … be
independent uniform random variables on
[0, 1]. Assign Ui to edge ei.
Let Ep = {ei ∶Ui ≤p}. Then Ep is the
set of open edges in bond percolation
with parameter p. This construction cou-
ples bond percolation models for every
p ∈[0, 1] in a monotone way: if ei ∈Ep
then ei ∈Eq for all q ≥p.
Let
Cp(0)
denote
the
cluster
con-
taining 0 using edges in Ep. If p ≤q,
then by construction Cp(0) ⊆Cq(0). So
{|Cp(0)| = ∞} ⊆{|Cq(0)| = ∞},
and
hence, 𝜃(p) ≤𝜃(q).
As 𝜃(p) is nondecreasing, and clearly
𝜃(0) = 0 and 𝜃(1) = 1, there must be some
threshold value
pc ∶= inf{p ∈[0, 1] ∶𝜃(p) > 0}.
So for p < pc, ℙp[H∞] = 0, while for p > pc,
ℙp[H∞] = 1.
The ﬁrst question is this: is there a non-
trivial phase transition, that is, is 0 < pc <
1? This question was answered by Broad-
bent and Hammersley in the late 1950s.
Proposition 1.2 1∕3 ≤pc ≤2∕3.
Proof of pc ≥1∕3 Let An be the event that
there exists a self-avoiding open path start-
ing at 0 of length n. Then
A1 ⊇A2 ⊇A3 · · ·
and
∞
⋂
n=1
An = {|C(0)| = ∞}.
So
𝜃(p) = ℙp[|C(0)| = ∞] = limn→∞ℙp
[An], by continuity of probability measure
(see Section 1.A). Let Γn be the set of all
possible self-avoiding paths of length n
starting at 0. Then
ℙp[An] = ℙp
⋃
𝛾∈Γn
{𝛾is open}
≤
∑
𝛾∈Γn
ℙp[𝛾is open] = |Γn|pn
≤4 ⋅3n−1 ⋅pn,
which tends to 0 if p < 1∕3. So pc ≥1∕3.
On the basis of pioneering Monte Carlo
simulations, Hammersley conjectured that

1.A Appendix: Some Results from Probability Theory
33
pc was 1∕2. Harris proved in 1960 that
𝜃(1∕2) = 0, which implies that pc ≥1∕2. It
was not until 1980 that a seminal paper of
Kesten settled things.
Theorem 1.17 (Harris 1960, Kesten
1980) pc = 1∕2.
Harris’s result 𝜃(1∕2) = 0 thus means
that 𝜃(pc) = 0; this is conjectured to be the
case in many percolation models (e.g., on
ℤd, it is proved for d = 2 and d ≥19, but is
conjectured to hold for all d ≥2). In recent
years, there has been much interest in the
detailed structure of percolation when
p = pc. The Schramm–Loewner evolution
has provided an important new mathemat-
ical tool to investigate physical predictions,
which often originated in conformal ﬁeld
theory; see [32].
1.8
Further Reading
A wealth of information on stochastic
processes and the tools that we have
introduced here can be found in [2, 5, 9,
20, 26], for example. All of those books
cover Markov chains. The general theory
of Markov processes can be found in [2,
9], which also cover the connection to
semigroup theory. Feller gives a masterly
presentation of random walks and generat-
ing functions [20] and Laplace transforms,
characteristic functions, and their appli-
cations [9]. Branching processes can be
found in [5, 20]. A thorough treatment of
percolation is presented in [33]. We have
said almost nothing here about Brownian
motion or diﬀusions, for which we refer the
reader to Chapter 3 of this volume as well as
[2, 5, 9, 26]. Physicists and mathematicians
alike ﬁnd it hard not to be struck by the
beauty of the connection between random
walks and electrical networks, as exposited
in [34]. Applications of stochastic processes
in physics and related ﬁelds are speciﬁcally
treated in [27, 35]; the array of applications
of random walks alone is indicated in
[19, 36, 37].
1.A
Appendix: Some Results from Probability
Theory
There is no other simple mathematical
theory that is so badly taught to physi-
cists as probability.
– R. F. Streater [38, p. 19].
Essentially, the theory of probability
is nothing but good common sense
reduced to mathematics.
– P.-S. de Laplace, Essai philosophique
sur les probabilités, 1813.
Kolmogorov’s 1933 axiomatization of
probability on the mathematical founda-
tion of measure theory was fundamental
to the development of the subject and is
essential for understanding the modern
theory. Many excellent textbook treat-
ments are available. Here we emphasize a
few points directly relevant for the rest of
this chapter.
1.A.1
Set Theory Notation
A set is a collection of elements. The
set of no elements is the empty set ∅.
Finite nonempty sets can be listed as
S = {a1, … , an}. If a set S contains an ele-
ment a, we write a ∈S. A set R is a subset
of a set S, written R ⊆S, if every a ∈R also
satisﬁes a ∈S. For two sets S and T, their
intersection is S ∩T, the set of elements
that are in both A and B, and their union is

34
1 Stochastic Processes
S ∪T, the set of elements in at least one of
S or T. For two sets S and T, “S minus T” is
the set S ⧵T = {a ∈S ∶a ∉T}, the set of
elements that are in S but not in T.
Note that S ∩∅= ∅, S ∪∅= S, and S ⧵
∅= S.
1.A.2
Probability Spaces
Suppose we perform an experiment that
gives a random outcome. Let Ω denote the
set of all possible outcomes: the sample
space. To start with, we take Ω to be dis-
crete, which means it is ﬁnite or countably
inﬁnite. This means that we can write Ω as
a (possibly inﬁnite) list:
Ω = {𝜔1, 𝜔2, 𝜔3, …},
where 𝜔1, 𝜔2, 𝜔3, … are the possible out-
comes to our experiment.
A set A ⊆Ω is called an event. Given
events A, B ⊆Ω, we can build new events
using the operations of set theory:
• A ∪B (“A or B”), the event that A
happens, or B happens, or both.
• A ∩B (“A and B”), the event that A and B
both happen.
Two events A and B are called disjoint or
mutually exclusive if A ∩B = ∅.
We want to assign probabilities to events.
Let Ω be a nonempty discrete sample space.
A function ℙthat gives a value ℙ[A] ∈[0, 1]
for every subset A ⊆Ω is called a discrete
probability measure on Ω if
(P1) ℙ[∅] = 0 and ℙ[Ω] = 1;
(P2) For any A1, A2, … , pairwise disjoint
subsets of Ω (so Ai ∩Aj = ∅for i ≠j),
ℙ
[ ∞
⋃
i=1
Ai
]
=
∞
∑
i=1
ℙ[Ai] (𝜎−additivity).
Given Ω and a probability measure ℙ, we
call (Ω, ℙ) a discrete probability space.
Remark 1.10 For
nondiscrete
sample
spaces, we may not be able to assign prob-
abilities to all subsets of Ω in a sensible
way, and so smaller collections of events
are required. In this appendix, we treat the
discrete case only, as is suﬃcient for most
(but not all) of the discussion in this chapter.
For the more general case, which requires
a deeper understanding of measure theory,
there are many excellent treatments, such
as [1, 2, 39, 40].
For an event A ⊆Ω, we deﬁne its com-
plement, denoted Ac and read “not A”, to
be Ac ∶= Ω ⧵A = {𝜔∈Ω ∶𝜔∉A}. Note
that (Ac)c = A, A ∩Ac = ∅, and A ∪Ac = Ω.
If (Ω, ℙ) is a discrete probability space,
then
• For A ⊆Ω, ℙ[Ac] = 1 −ℙ[A];
• If A, B ⊆Ω and A ⊆B, then ℙ[A] ≤ℙ[B]
(monotonicity);
• If A, B ⊆Ω, then
ℙ[A ∪B] = ℙ[A] + ℙ[B] −ℙ[A ∩B]. In
particular, if A ∩B = ∅,
ℙ[A ∪B] = ℙ[A] + ℙ[B].
It follows from this last statement that
ℙ[A ∪B] ≤ℙ[A] + ℙ[B]; more generally,
we have the elementary but useful Boole’s
inequality: ℙ[∪nAn] ≤∑
n ℙ[An].
At several points we use the continuity
property of probability measures:
• If A1 ⊆A2 ⊆A3 ⊆· · · are events, then
ℙ[∪∞
i=1Ai] = limn→∞ℙ[An].
• If A1 ⊇A2 ⊇A3 ⊇· · · are events, then
ℙ[∩∞
i=1Ai] = limn→∞ℙ[An].
To see the ﬁrst statement, we can write
∪∞
i=1Ai = ∪∞
i=1(Ai ⧵Ai−1), where we set A0 =

1.A Appendix: Some Results from Probability Theory
35
∅, and the latter union is over pairwise dis-
joint events. So, by 𝜎-additivity,
ℙ[∪∞
i=1Ai] =
∞
∑
i=1
ℙ[Ai ⧵Ai−1]
= lim
n→∞
n
∑
i=1
ℙ[Ai ⧵Ai−1].
But
∑n
i=1 ℙ[Ai ⧵Ai−1] = ℙ[∪n
i=1(Ai ⧵
Ai−1)] = ℙ[An],
giving
the
result.
The
second statement is analogous.
1.A.3
Conditional Probability and Independence
of Events
If A and B are events with ℙ[B] > 0 then the
conditional probability ℙ[A ∣B] of A given
B is deﬁned by
ℙ[A ∣B] ∶= ℙ[A ∩B]
ℙ[B]
.
A
countable
collection
of
events
E1, E2, … is called a partition of Ω if
(a)
for all i, Ei ⊆Ω and Ei ≠∅;
(b)
for i ≠j, Ei ∩Ej = ∅(the events are
disjoint);
(c)
⋃
i Ei = Ω (the events ﬁll the sample
space).
Let E1, E2, … be a partition of Ω. Follow-
ing from the deﬁnitions is the basic law of
total probability, which states that for all
A ⊆Ω,
ℙ[A] =
∑
i
ℙ[Ei]ℙ[A ∣Ei].
A
countable
collection
(Ai, i ∈I)
of
events is called independent if, for every
ﬁnite subset J ⊆I,
ℙ
[ ⋂
j∈J
Aj
]
=
∏
j∈J
ℙ[Aj].
In particular, two events A and B are inde-
pendent if ℙ[A ∩B] = ℙ[A]ℙ[B] (i.e., if
ℙ[B] > 0, ℙ[A ∣B] = ℙ[A]).
1.A.4
Random Variables and Expectation
Let (Ω, ℙ) be a discrete probability space. A
function X ∶Ω →ℝis a random variable.
So each 𝜔∈Ω is mapped to a real number
X(𝜔). The set of possible values for X
is
X(Ω) = {X(𝜔) ∶𝜔∈Ω} ⊂ℝ.
Notice
that because Ω is discrete, X(Ω) must be
also.
If X and Y are two random variables
on (Ω, ℙ), then X + Y, XY, and so on, are
also random variables. For example, (X +
Y)(𝜔) = X(𝜔) + Y(𝜔). In some cases (such
as the expected hitting times deﬁned at
(1.18)), we extend the domain of a random
variable from ℝto ℝ∪{∞}.
The probability mass function of a dis-
crete random variable X is the collection
ℙ[X = x] for all x ∈X(Ω). The distribution
function of X is F ∶ℝ→[0, 1] given by
F(x) = ℙ[X ≤x].
Example 1.22 [Bernoulli and binomial dis-
tributions] Let n be a positive integer and
p ∈[0, 1]. We say X has a binomial distri-
bution with parameters (n, p), written X ∼
Bin(n, p), if ℙ[X = k] = (n
k
)pk(1 −p)n−k for
k ∈{0, 1, … , n}. The case Y ∼Bin(1, p) is
the Bernoulli distribution; here ℙ[Y = 1] =
p = 1 −ℙ[Y = 0] and we write Y ∼Be(p).
The binomial distribution has the following
interpretation: Perform n independent “tri-
als” (e.g., coin tosses) each with probability
p of “success” (e.g., “heads”), and count the
total number of successes.
Example 1.23 [Poisson distribution] Let
𝜆> 0 and pk ∶= e−𝜆(𝜆k∕k!) for k ∈ℤ+.
If ℙ[X = k] = pk, X is a Poisson random
variable with parameter 𝜆.

36
1 Stochastic Processes
Let X be a discrete random variable. The
expectation, expected value, or mean of X is
given by
𝔼[X] =
∑
x∈X(Ω)
xℙ[X = x],
provided the sum is ﬁnite. The variance
of X is 𝕍ar[X] = 𝔼[(X −𝔼[X])2] = 𝔼[X2] −
(𝔼[X])2.
Example 1.24 [Indicator variables.] Let A
be an event. Let 1A denote the indicator
random variable of A, that is, 1A ∶Ω →
{0, 1} given by
1A(𝜔) ∶=
{ 1 if 𝜔∈A
0 if 𝜔∉A
So 1A is 1 if A happens and 0 if not. Then
𝔼[1A] = 1 ⋅ℙ[1A = 1] + 0 ⋅ℙ[1A = 0]
= ℙ[1A = 1] = ℙ[A].
Expectation has the following basic prop-
erties. For X and Y random variables with
well-deﬁned expectations and a, b ∈ℝ,
(a)
𝔼[aX + bY] = a𝔼[X] + b𝔼[Y]
(lin-
earity).
(b)
If ℙ[X ≤Y] = 1, then 𝔼[X] ≤𝔼[Y]
(monotonicity).
(c)
|𝔼[X]| ≤𝔼[|X|] (triangle inequality).
(d)
If
h ∶X(Ω) →ℝ,
then
𝔼[h(X)] =
∑
x∈X(Ω) h(x)ℙ[X = x] (“law of the
unconscious statistician”).
Let X be a nonnegative random vari-
able. Then, for any x > 0, x1{X ≥x} ≤X
holds with probability 1. Taking expec-
tations and using monotonicity yields
ℙ[X ≥x] ≤x−1𝔼[X]. This is usually known
as Markov’s inequality, although it is also
sometimes
referred
to
as
Chebyshev’s
inequality.15) Applying Markov’s inequality
to e𝜃X, 𝜃> 0, gives
ℙ[X ≥x] = ℙ[e𝜃X ≥e𝜃x] ≤e−𝜃x𝔼[e𝜃X],
which is sometimes known as Chernoﬀ’s
inequality.
Let (Ω, ℙ) be a discrete probability space.
A family (Xi, i ∈I) of random variables is
called independent if for any ﬁnite subset
J ⊆I and all xj ∈Xj(Ω),
ℙ
( ⋂
j∈J
{Xj = xj}
)
=
∏
j∈J
ℙ(Xj = xj).
In particular, random variables X and
Y are independent if ℙ[X = x, Y = y] =
ℙ[X = x]ℙ[Y = y] for all x and y.
Theorem 1.18 If X and Y are independent,
then 𝔼[XY] = 𝔼[X]𝔼[Y].
A consequence of Theorem 1.18 is that
if X and Y are independent, then 𝕍ar[X +
Y] = 𝕍ar[X] + 𝕍ar[Y].
Example 1.25
(a)
If Y ∼Be(p) then 𝔼[Y] = 𝔼[Y 2] =
p ⋅1 + (1 −p) ⋅0 = p,
so
𝕍ar[Y] =
p −p2 = p(1 −p).
(b)
If X ∼Bin(n, p) then we can write
X = ∑n
i=1 Yi
where
Yi ∼Be(p)
are
independent. By linearity of expec-
tation,
𝔼[X] = ∑n
i=1 𝔼[Yi] = np.
Also,
by
independence,
𝕍ar[X] =
∑n
i=1 𝕍ar[Yi] = np(1 −p).
1.A.5
Conditional Expectation
On a discrete probability space (Ω, ℙ), let
B be an event with ℙ[B] > 0 and let X be
15) Chebyshev’s inequality is the name more com-
monly associated with Markov’s inequality
applied to the random variable (X −𝔼[X])2, to
give ℙ[|X −𝔼[X]| ≥x] ≤x−2𝕍ar[X].

References
37
a random variable. The conditional expec-
tation of X given B is
𝔼[X ∣B] =
∑
x∈X(Ω)
xℙ[X = x ∣B].
So 𝔼[X ∣B] can be thought of as expecta-
tion with respect to the conditional proba-
bility measure ℙ[ ⋅∣B]. An alternative is
𝔼[X ∣B] = 𝔼[X1B]
ℙ[B] ,
(1.35)
where 1B is the indicator random variable
of B. The proof of (1.35) is an exercise in
interchanging summations. First,
𝔼[X ∣B] =
∑
x∈X(Ω)
xℙ[X = x ∣B]
=
∑
x∈X(Ω)
xℙ[{X = x} ∩B]
ℙ[B]
.
(1.36)
On the other hand, the random variable
1BX takes values x ≠0 with
ℙ[1BX = x] =
∑
𝜔∈Ω∶𝜔∈B∩{X=x}
ℙ[{𝜔}]
= ℙ[{X = x} ∩B],
so by comparison we see that the ﬁnal
expression in (1.36) is indeed 𝔼[1BX]∕ℙ[B].
Let (Ei, i ∈I) be a partition of Ω, so
∑
i∈I 1Ei = 1. Hence,
𝔼[X] = 𝔼
[
X
∑
i∈I
1Ei
]
= 𝔼
[ ∑
i∈I
X1Ei
]
=
∑
i∈I
𝔼[X1Ei],
by linearity. By (1.35), 𝔼[X1Ei] = 𝔼[X ∣
Ei]ℙ[Ei]. Thus we verify the partition
theorem for expectations:
𝔼[X] =
∑
i∈I
𝔼[X ∣Ei]ℙ[Ei].
Given two discrete random variables X
and Y, the conditional expectation of X
given Y, denoted 𝔼[X ∣Y], is the random
variable
𝔼[X ∣Y](𝜔) = 𝔼[X ∣Y = Y(𝜔)],
which
takes
values
𝔼[X ∣Y = y]
with
probabilities ℙ[Y = y].
References
1. Durrett, R. (2010) Probability: Theory and
Examples, 4th edn, Cambridge University
Press, Cambridge.
2. Kallenberg, O. (2002) Foundations of
Modern Probability, 2nd edn,
Springer-Verlag, New York.
3. Fischer, H. (2011) A History of the Central
Limit Theorem, Springer-Verlag, New York.
4. Ehrenfest, P. and Ehrenfest, T. (1907) Über
zwei bekannte Einwände gegen das
Boltzmannsche H-Theorem. Phys. Z., 8,
311–314.
5. Karlin, S. and Taylor, H.M. (1975) A First
Course in Stochastic Processes, 2nd edn,
Academic Press.
6. Norris, J.R. (1997) Markov Chains,
Cambridge University Press, Cambridge.
7. Chung, K.L. (1967) Markov Chains with
Stationary Transition Probabilities, 2nd edn,
Springer-Verlag, Berlin.
8. Lamperti, J. (1960) Criteria for the
recurrence or transience of stochastic
process. I. J. Math. Anal. Appl., 1, 314–330.
9. Feller, W. (1971) An Introduction to
Probability Theory and its Applications, Vol.
II, 2nd edn, John Wiley & Sons, Inc., New
York.
10. Bak, P. and Sneppen, K. (1993) Punctuated
equilibrium and criticality in a simple model
of evolution. Phys. Rev. Lett., 71, 4083–4086.
11.
Gillett, A.J. (2007) Phase Transitions in
Bak–Sneppen Avalanches and in a
Continuum Percolation Model. PhD
dissertation, Vrije Universiteit, Amsterdam.
12. Grinfeld, M., Knight, P.A., and Wade, A.R.
(2012) Rank-driven Markov processes. J.
Stat. Phys., 146, 378–407.
13. Rayleigh, L. (1880) On the resultant of a large
number of vibrations of the same pitch and
of arbitrary phase. Philos. Mag., 10, 73–78.
14. Bachelier, L. (1900) Théorie de la
spéculation. Ann. Sci. École Norm. Sup., 17,
21–86.

38
1 Stochastic Processes
15. Pearson, K. and Blakeman, J. (1906) A
Mathematical Theory of Random Migration,
Drapers’ Company Research Memoirs
Biometric Series, Dulau and co., London.
16. Einstein, A. (1956) Investigations on the
Theory of the Brownian Movement, Dover
Publications Inc., New York.
17. Pólya, G. (1921) Über eine Aufgabe der
Wahrscheinlichkeitsrechnung betreﬀend die
Irrfahrt im Straßennetz. Math. Ann., 84,
149–160.
18. Pearson, K. (1905) The problem of the
random walk. Nature, 72, 342.
19. Hughes, B.D. (1995) Random Walks and
Random Environments, vol. 1, Oxford
University Press, New York.
20. Feller, W. (1968) An Introduction to
Probability Theory and its Applications, Vol.
I, 3rd edn, John Wiley & Sons, Inc., New
York.
21. Doherty, M. (1975) An Amusing Proof in
Fluctuation Theory, Combinatorial
Mathematics III, vol. 452, Springer,
101–104.
22. Ganesh, A., O’Connell, N., and Wischik, D.
(2004) Big Queues, Springer, Berlin.
23. Foss, S., Korshunov, D., and Zachary, S.
(2011) An Introduction to Heavy-Tailed and
Subexponential Distributions,
Springer-Verlag.
24. Dembo, A. and Zeitouni, O. (1998) Large
Deviations Techniques and Applications,
Springer, New York.
25. Touchette, H. (2009) The large deviation
approach to statistical mechanics. Phys. Rep.,
478, 1–69.
26. Karlin, S. and Taylor, H.M. (1981) A Second
Course in Stochastic Processes, Academic
Press.
27. Lax, M., Cai, W., and Xu, M. (2006) Random
Processes in Physics and Finance, Oxford
University Press.
28. Blackman, J.A. and Mulheran, P.A. (1996)
Scaling behaviour in submonolayer ﬁlm
growth: a one-dimensional model. Phys. Rev.
B, 54, 11 681.
29. O’Neill, K.P., Grinfeld, M., Lamb, W., and
Mulheran, P.A. (2012) Gap-size and
capture-zone distributions in
one-dimensional point-island nucleation and
growth simulations: Asymptotics and
models. Phys. Rev. E, 85, 21 601.
30. Ising, E. (1925) Beitrag zur Theorie des
Ferromagnetismus. Z. Phys., 31,
253–258.
31. Grimmett, G. (2010) Probability on Graphs,
Cambridge University Press.
32. Lawler, G.F. (2005) Conformally Invariant
Processes in the Plane, American
Mathematical Society.
33. Grimmett, G. (1999) Percolation, 2nd edn,
Springer-Verlag, Berlin.
34. Doyle, P.G. and Snell, J.L. (1984) Random
Walks and Electric Networks, Mathematical
Association of America, Washington, DC.
35. Kac, M. (1959) Probability and Related
Topics in Physical Sciences, vol. 1957,
Interscience Publishers, London and New
York.
36. Shlesinger, M.F. and West, B.J. (1984)
Random Walks and Their Applications on
the Physical and Biological Sciences,
American Institute of Physics, New York.
37.
Weiss, G.H. and Rubin, R.J. (1983) Random
walks: theory and selected applications. Adv.
Chem. Phys., 52, 363–505.
38. Streater, R.F. (2007) Lost Causes in and
Beyond Physics, Springer-Verlag.
39. Billingsley, P. (1995) Probability and
Measure, 3rd edn, John Wiley & Sons, Inc.
New York.
40. Chung, K.L. (2001) A Course in Probability
Theory, 3rd edn, Academic Press Inc., San
Diego, CA.

39
2
Monte-Carlo Methods
Kurt Binder
2.1
Introduction and Overview
Many
problems
in
science
are
very
complex: for example, statistical ther-
modynamics considers thermal properties
of matter resulting from the interplay of a
large number of particles. A deterministic
description in terms of the equations of
motion of all these particles would make
no sense and a probabilistic description
is required. A probabilistic description
may even be intrinsically implied by the
quantum-mechanical nature of the basic
processes (e.g., emission of neutrons in
radioactive decay) or because the prob-
lem is incompletely characterized, only
some degrees of freedom being considered
explicitly, while the others act as a kind of
background causing random noise. While
thus the concept of probability distribu-
tions is ubiquitous in physics, often it is
not possible to compute these probabil-
ity distribution functions analytically in
explicit form, because of the complexity
of the problem. For example, interac-
tions between atoms in a ﬂuid produce
strong and nontrivial correlations between
atomic positions, and, hence, it is not
possible to calculate these correlations
analytically.
Monte-Carlo methods now aim at a
numerical estimation of probability dis-
tributions (as well as of averages that can
be calculated from them), making use of
(pseudo) random numbers. By “pseudo-
random numbers” one means a sequence
of numbers produced on a computer with
a deterministic procedure from a suitable
“seed.” Hence, this sequence is not truly
random: See Section 2.2 for a discussion of
this problem.
The outline of the present article is
as follows. As all Monte-Carlo methods
heavily rely on the use of random num-
bers, we brieﬂy review random-number
generation in Section 2.2. In Section 2.3,
we then elaborate on the discussion of
“simple sampling,” that is, problems where
a straightforward generation of probabil-
ity distributions using random numbers
is possible. Section 2.4 brieﬂy mentions
some applications to transport problems,
such as radiation shielding, and growth
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

40
2 Monte-Carlo Methods
phenomena,
such
as
“diﬀusion-limited
aggregation” (DLA).
Section 2.5
then
considers
the
importance-sampling
methods
of
sta-
tistical
thermodynamics,
including
the
use of diﬀerent thermodynamic ensem-
bles. This chapter emphasizes applications
in
statistical
mechanics
of
condensed
matter, because this is the ﬁeld where
most activity with Monte-Carlo methods
occurs.
Some more practical aspects important
for the implementation of algorithms and
the judgment of the tractability of sim-
ulation approaches to physical problems
are then considered in Section 2.6: eﬀects
resulting from the ﬁnite size of simulation
boxes, eﬀects of choosing various bound-
ary conditions, dynamic correlation of
errors, and the application to studies of
the dynamics of thermal ﬂuctuations. In
addition, methods for the sampling of free
energies and of free energy barriers in con-
ﬁguration space will be brieﬂy discussed
(Section 2.7).
The extension to quantum-mechanical
problems is mentioned in Section 2.8 and
the application to elementary particle
theory (lattice gauge theory) in Section 2.9.
Section 2.10
then
illustrates
some
of
the general concepts with a variety of
applications taken from condensed mat-
ter physics, while Section 2.11 contains
concluding remarks.
We do not discuss problems of applied
mathematics such as applications to the
solution
of
linear
operator
equations
(Fredholm integral equations, the Dirich-
let boundary-value problem, eigenvalue
problems, etc.); for a concise discussion
of such problems, see, for example, Ham-
mersley and Handscomb [1]. Nor do we
discuss simulations of chemical kinetics,
such as polymerization processes (see, e.g.,
[2]).
2.2
Random-Number Generation
2.2.1
General Introduction
The precise deﬁnition of “randomness” is a
problem in itself (see, e.g., [3]) and is out-
side the scope of this chapter. Truly random
numbers are unpredictable in advance and
must be produced by an appropriate phys-
ical process such as radioactive decay, but
are not useful for computer simulation in
practice.
Pseudorandom numbers are produced in
the computer by one of various algorithms,
some of which will be discussed below, and
thus are predictable, as their sequence is
exactly reproducible. (This reproducibility,
of course, is desirable, as it allows detailed
checks of Monte-Carlo simulation pro-
grams.) They are thus not truly random,
but they have statistical properties (nearly
uniform
distribution,
nearly
vanishing
correlation coeﬃcients, etc.) that are very
similar to the statistical properties of truly
random numbers. Thus, a given sequence
of
(pseudo)random
numbers
appears
“random” for many practical purposes. In
the following, the preﬁx “pseudo” will be
omitted throughout.
2.2.2
Properties That a Random-Number
Generator (RNG) Should Have
What one needs are numbers that are uni-
formly distributed in the interval [0,1] and
that are uncorrelated. By “uncorrelated” we
not only mean vanishing pair correlations
for arbitrary distances along the random-
number
sequence,
but
also
vanishing
triplet and higher correlations. No algo-
rithm exists that satisﬁes these desirable
requirements fully, of course; the extent to

2.2 Random-Number Generation
41
which the remaining correlations between
the generated random numbers lead to
erroneous results of Monte-Carlo simula-
tions has been a matter of long-standing
concern
[4–6];
even
random-number
generators (RNGs) that have passed all
common statistical tests and have been
used successfully for years may fail for a
new application, in particular, if it involves
a new type of Monte-Carlo algorithm.
Therefore, the testing of RNGs is a ﬁeld of
research in itself (see, e.g., [7–9]).
A limitation due to the ﬁnite word
length of computers is the ﬁnite period:
Every generator begins after a long but
ﬁnite period to produce exactly the same
sequence
again.
For
example,
simple
generators for 32-bit computers have a
maximum period of 230 (≈109) numbers
only. This is not enough for recent high-
quality applications! Of course, one can
get around this problem [4, 5], but, at
the same time, one likes the code rep-
resenting the RNG to be “portable” (i.e.,
in a high-level programming language
such as FORTRAN usable for computers
from diﬀerent manufacturers) and “eﬃ-
cient” (i.e., extremely fast so it does not
unduly slow down the simulation pro-
gram as a whole). Thus, inventing new
RNGs that are in certain respects a better
compromise between these partially con-
ﬂicting requirements is still of interest (e.g.,
[10]).
2.2.3
Comments about a Few Frequently Used
Generators
The best known among the frequently used
RNG is the linear multiplicative algorithm
[11], which produces random integers Xi
recursively from the formula
xi = aXi−1 + c
(modulo
m),
(2.1)
which means that m is added when the
result is negative, but multiples of m are
subtracted when the result is larger than
m. For 32-bit computers, m = 231 −1 (the
largest integer that can be used for that
computer). The integer constants a, c, and
X0 (the starting value of the recursion, the
so-called “seed”) need to be appropriately
chosen (e.g., a = 16 807, c = 0, X0 odd).
Obviously, the “randomness” of the Xi
results because, after a few multiplications
with a, the result would exceed m and
hence is truncated, and so the leading
digits of Xi are more or less random. But
there are severe correlations: if d-tuples of
such numbers are used to represent points
in d-dimensional space having a lattice
structure, they lie on a certain number of
hyperplanes [12].
Equation (2.1) produces random num-
bers between 0 and m. Converting them
into real numbers and dividing by m yields
random numbers in the interval [0,1], as
desired.
More popular now are shift-register gen-
erators [13, 14] based on the formula
Xi = Xi−p ⋅XOR ⋅Xi−q,
(2.2)
where ⋅XOR⋅is the bitwise “exclusive or”
operation, and the “lags” p, q have to be
properly chosen (the popular “R250” [14]
uses p = 109, q = 250 and thus needs 250
initializing integers). “Good” generators
based on (2.2) have fewer correlations
between random numbers than those
resulting from (2.1), and much larger
period.
A third type of generators is based on the
Fibonacci series and is also recommended
in the literature [4, 5, 15]. But a general
recommendation is that users of random
numbers should not rely on their quality
blindly and should perform their own tests
in the context of the application.

42
2 Monte-Carlo Methods
2.3
Simple Sampling of Probability
Distributions Using Random Numbers
In this section, we give a few simple
examples
of
the
use
of
Monte-Carlo
methods that will be useful for the under-
standing of later sections. More material
on this subject can be found in standard
textbooks such as Koonin [16] and Gould
and Tobochnik [17].
2.3.1
Numerical Estimation of Known
Probability Distributions
A known probability distribution in which
a (discrete) state i, 1 ≤i ≤n, occurs with
probability pi with ∑n
i=1 pi = 1, is numeri-
cally realized using random numbers uni-
formly distributed in the interval from zero
to unity as follows: deﬁning Pi = ∑i
j=1 pi, we
choose state i if the random number 𝜁satis-
ﬁes Pi−1 < 𝜁< Pi, with P0 = 0. In the limit
of a large number (M) of trials, the gen-
erated distribution approximates pi, with
errors of order 1∕
√
M.
Monte-Carlo
methods
in
statistical
mechanics can be viewed as an extension
of this simple concept to the situation
in which the probability that a point X
in phase space occurs is given by the
Boltzmann probability
Peq(X) = 1
Z exp
[−ℋ(X)
kBT
]
,
kB being Boltzmann’s constant, T the abso-
lute temperature, and Z = ∑exp
[
−ℋ(X)
kBT
]
the partition function, although in general
neither Z nor Peq(X) can be written explic-
itly (as function of the variables of interest,
such as T, particle number N, volume V,
etc.). The term ℋ(X) denotes the Hamilto-
nian of the (classical) system.
2.3.2
“Importance Sampling” versus “Simple
Sampling”
The sampling of the Boltzmann probability
Peq(X) by Monte-Carlo methods is not
completely
straightforward:
One
must
not choose the points X in phase space
completely at random, because Peq(X)
is extremely sharply peaked. Thus, one
needs
“importance-sampling”
methods
that generate points X preferably from the
“important” region of space where this
narrow peak occurs.
Before we treat this problem of statisti-
cal mechanics in more detail, we empha-
size the more straightforward applications
of “simple sampling” techniques. In the fol-
lowing, we list a few problems where sim-
ple sampling is useful. Suppose one wishes
to generate a conﬁguration of a randomly
mixed crystal of a given lattice structure,
for example, a binary mixture of compo-
sition AxB1−x. Again, one uses pseudoran-
dom numbers 𝜁uniformly distributed in
[0,1] to choose the occupancy of lattice sites
{j}: If 𝜁j < x, the site j is taken by an A atom,
else by a B atom. Such conﬁgurations now
can be used as starting point for a numerical
study of the dynamical matrix if one is inter-
ested in the phonon spectrum of mixed
crystals. One can study the distribution of
sizes of “clusters” formed by neighboring A
atoms if one is interested in the “site perco-
lation problem” [18], and so on.
If one is interested in simulating trans-
port processes such as diﬀusion, a basic
approach is the generation of simple ran-
dom walks (RWs). Such RWs, resulting
from addition of vectors whose orientation
is random, can be generated both on lattices
and in the continuum. Such simulations are
desirable if one wishes to consider compli-
cated geometries or boundary conditions
of the medium where the diﬀusion takes

2.3 Simple Sampling of Probability Distributions Using Random Numbers
43
place. In addition, it is straightforward
to include competing processes (e.g., in
a reactor, diﬀusion of neutrons in the
moderator competes with loss of neutrons
due to nuclear reactions, radiation going
to the outside, etc., or gain due to ﬁssion
events). Actually, this problem of reactor
criticality (and related problems for nuclear
weapons!) was the starting point for the
ﬁrst large-scale applications of Monte-
Carlo methods by Fermi, von Neumann,
Ulam, and their coworkers [1].
2.3.3
Monte-Carlo as a Method of Integration
Many Monte-Carlo computations may be
viewed as attempts to estimate the value
of (multiple) integrals. To give the ﬂavor of
this idea, we discuss the one-dimensional
integral
I =
1
∫
0
f (x) dx
≡
1
∫
0
1
∫
0
g (x, y) dxdy; with
(2.3)
g(x, y) =
{0 if
f (x) < y,
1 if
f (x) ≥y,
}
as an example (suppose, for simplicity, that
also 0 ≤f (x) ≤1 for 0 ≤x ≤1). Then I sim-
ply is interpreted as the fraction of the
unit square 0 ≤x, y ≤1 lying underneath
the curve y = f (x). Now a straightforward
(though often not very eﬃcient) Monte-
Carlo estimation of (2.3) is the “hit-or-miss”
method: We take n points (𝜁x, 𝜁y) uniformly
distributed in the unit square 0 ≤𝜁x ≤1,
0 ≤𝜁y ≤1. Then I is estimated by the aver-
age g of g(x,y),
g = 1
n
n
∑
i=1
g (𝜁xi, 𝜁yi
) = n∗
n ,
(2.4)
n* being the number of points for which
f (𝜁xi) ≥𝜁yi. Thus, we count the fraction of
points that lie underneath the curve y =
f (x). Of course, such Monte-Carlo meth-
ods are inferior to many other techniques
of numerical integration, if the integration
space is low dimensional, but the situa-
tion for standard integration methods is
worse for high-dimensional spaces: For any
method using a regular grid of points for
which the integrand needs to be evaluated,
the number of points sampled along each
coordinate is M1∕d in d dimensions, which
is small for any reasonable sample size M if
d is very large.
2.3.4
Inﬁnite Integration Space
Not always is the integration space limited
to a bounded interval in space. For example,
the 𝜙4 model of ﬁeld theory considers a ﬁeld
variable 𝜙(x), where x is drawn from a d-
dimensional space and 𝜙(x) is a real variable
with distribution
P(𝜙) ∝exp
[
−𝛼
(
−1
2𝜙2 + 1
4𝜙4)]
;
𝛼> 0.
(2.5a)
While −∞< 𝜙< +∞, the distribution
P′(y)
P′(y) =
∫
y
−∞P(𝜙)d𝜙
∫
+∞
−∞P(𝜙)d𝜙
(2.5b)
varies in the unit interval, 0 ≤P′ ≤1.
Hence, deﬁning Y = Y(P′) as the inverse
function of P′(y), we can choose a ran-
dom
number
𝜁
uniformly
distributed
between zero and one, to obtain 𝜙= Y(𝜁)
distributed according to the chosen dis-
tribution P(𝜙). Of course, this method
works not only for the example chosen in
(2.5a) but for any distribution of interest.
Often it will not be possible to obtain Y(P′)
analytically, but then one can compute

44
2 Monte-Carlo Methods
numerically a table before the start of the
sampling [19].
2.3.5
Random Selection of Lattice Sites
A problem that occurs very frequently (e.g.,
in solid-state physics) is one that considers
a large lattice (e.g., a model of a simple
cubic crystal with N = Lx × Ly × Lz sites),
and one wishes to select a lattice site (nx,
ny, nz) at random. This is trivially done
using the integer arithmetics of standard
computers, converting a uniformly dis-
tributed random number 𝜁x(0 ≤𝜁x < 1)
to an integer nx with 1 ≤nx ≤Lx via
the statement nx = int(𝜁xLx + 1). This is
already an example where one must be
careful, however, when three successive
pseudorandom numbers drawn from an
RNG are used for this purpose: If one
uses an RNG with bad statistical qualities,
the frequency with which individual sites
are visited may deviate distinctly from a
truly random choice. In unfavorable cases,
successive pseudorandom numbers are so
strongly correlated that certain lattice sites
would never be visited.
2.3.6
The Self-Avoiding Walk Problem
As an example of the straightforward use
of simple sampling techniques, we now
discuss the study of self-avoiding walks
(SAWs) on lattices (which may be con-
sidered as a simple model for polymer
chains in good solvents; see [20]). Suppose
one considers a square or simple cubic
lattice with coordination number (number
of nearest neighbors) z. Then, for a RW
with N steps, we would have ZRW = zN
conﬁgurations, but many of these RWs
intersect themselves and thus would not be
self-avoiding. For SAWs, one only expects
of the order of ZSAW = const. × N𝛾−1zN
eﬀ
conﬁgurations, where 𝛾> 1 is a character-
istic exponent (which is not known exactly
for d = 3 dimensions), and zeﬀ≤z −1 is
an eﬀective coordination number, which
also is not known exactly. But it is already
obvious that an exact enumeration of
all conﬁgurations would be possible for
rather small N only, while most questions
of interest refer to the behavior for large
N; for example, one wishes to study the
end-to-end distance of the SAW,
⟨R2⟩
SAW =
1
ZSAW
∑
X
[R(X)]2,
(2.6)
the sum being extended over all conﬁgura-
tions of SAWs which we denote formally as
points X in phase space. One expects that
⟨R2⟩
SAW ∝N2𝜈, where v is another charac-
teristic exponent. A Monte-Carlo estima-
tion of
⟨
R2⟩
SAW now is based on generating
a sample of only M
≪
ZSAW conﬁgura-
tions Xl, that is,
R2 = 1
M
M
∑
l=1
[R (Xl
)]2 ≈⟨R2⟩
SAW .
(2.7)
If the M conﬁgurations are statistically
independent,
standard
error
analysis
applies, and we expect that the relative
error behaves as
(𝛿R2)2
(
R2
)2 ≈
1
M −1
[⟨
R4⟩
SAW
⟨R2⟩2
SAW
−1
]
.
(2.8)
While the law of large numbers then
implies that R2 is Gaussian distributed
around ⟨R2⟩
SAW with a variance deter-
mined by (2.8), one should note that the
variance does not decrease with increas-
ing N. Statistical mechanics tells us that
ﬂuctuations decrease with increasing the
number of degrees of freedom N; that is,
one equilibrium conﬁguration diﬀers in its

2.3 Simple Sampling of Probability Distributions Using Random Numbers
45
energy E(x) from the average ⟨E⟩only by
an amount of order 1∕
√
N. This property
is called self-averaging. Obviously, such
a property is not true for ⟨R2⟩
SAW. This
“lack of self-averaging” is easy to show for
ordinary RWs [21].
2.3.7
Simple Sampling versus Biased Sampling:
the Example of SAWs Continued
Apart from this problem, that the accu-
racy of the estimation of ⟨R2⟩does
not increase with the number of steps
of the walk, it is also not easy to gen-
erate a large sample of conﬁgurations
of SAWs for large N. Suppose we do
this at each step by choosing one of
z −1
neighbors
at
random
(eliminat-
ing, from the start, immediate reversals,
which would violate the SAW condi-
tion). Whenever the chosen lattice site
is
already
taken,
the
attempted
walk
must be terminated. Now the fraction
of walks that will continue successfully
for N steps will only be of the order of
ZSAW∕(z −1)N
∝[zeﬀ∕(z −1)]NN𝛾−1,
which decreases to zero exponentially
(∝exp(−N𝜅) with 𝜅= ln[(z −1)∕zeﬀ] for
large N); this failure of success in gen-
erating long SAWs is called the attrition
problem.
The obvious recipe, to select at each step
only from among the lattice sites that do
not violate the SAW restriction, does not
give equal statistical weight for each con-
ﬁguration generated, of course, and so the
average would not be the averaging that one
needs in (2.6). One ﬁnds that this method
would create a “bias” toward more com-
pact conﬁgurations of the walk. But one
can calculate the weights of conﬁgurations
w(X) that result in this so-called “inversely
restricted sampling” [22], and, in this way,
correct for the bias and estimate the SAW
averages as
R2 =
{ M
∑
l=1
[w (Xl
)]−1
}−1
×
M
∑
l=1
[w (Xl
)]−1 [R (Xl
)]2 .
(2.9)
However, error analysis of this biased sam-
pling is delicate [20].
A popular alternative to overcome the
above attrition problem is the “enrich-
ment technique,” founded on the principle
“Hold fast to that which is good.” Namely,
whenever a walk attains a length that is
a multiple of s steps without intersecting
itself, n independent attempts to con-
tinue it (rather than a single attempt) are
made. The numbers n, s are ﬁxed, and, if
we choose n ≈exp(𝜅s), the numbers of
walks of various lengths generated will
be approximately equal. Enrichment has
the advantage over inversely restricted
sampling that all walks of a given length
have equal weights, while the weights in
(2.9) vary over many orders of magnitude
for large N. But the linear dimensions of
the walks are highly correlated, because
some of them have many steps in common.
For these reasons, simple sampling and
its extensions are useful only for a few
problems in polymer science; “importance
sampling” (Section 2.5) is much more
used. But we emphasize that related prob-
lems are encountered for the sampling of
“random surfaces” (this problem arises in
the ﬁeld theory of quantum gravity), in
path-integral Monte-Carlo treatments of
quantum problems, and in many other
contexts.
A variant of such techniques that still
is widely used to simulate single polymer
chains is the PERM algorithm (“pruned-
enriched Rosenbluth method,” [23, 24]).
In this method, a population of chains

46
2 Monte-Carlo Methods
growing in their chain length step by step is
generated and their statistical weights are
considered. From time to time, members
of the population with low weight are
removed (“pruning”) and members with
high weight are enriched.
2.4
Survey of Applications to Simulation of
Transport Processes
There are many possibilities to simulate the
random motion of particles. Therefore, it
is diﬃcult to comment about such prob-
lems in general. Thus, we only discuss a few
examples that illustrate the spirit of such
approaches.
2.4.1
The “Shielding Problem”
A thick shield of absorbing material is
exposed to 𝛾radiation (energetic pho-
tons), of speciﬁed distribution of energy
and angle of incidence. We want to know
the
intensity
and
energy
distribution
of
the
radiation
that
penetrates
that
shield.
The description is here that one gen-
erates a lot of “histories” of those parti-
cles traveling through the medium. The
paths of these 𝛾particles between scat-
tering events are straight lines and diﬀer-
ent 𝛾particles do not interact with each
other. A particle with energy E, instan-
taneously at the point r and traveling in
the direction of the unit vector w, contin-
ues to travel in the same direction with
the same energy, until a scattering event
with an atom of the medium occurs. The
standard assumption is that these atoms
are distributed randomly in space. Then
the total probability that the particle col-
lides with an atom while traveling a length
𝛿s of its path is 𝜎c(E)𝛿s, 𝜎c(E) being the
cross section. In a region of space where
𝜎c(E) is constant, the probability that a
particle travels without collision a distance
s is Fc(s) = 1 −exp[−𝜎c(E)s]. If a collision
occurs, it leads to absorption or scattering,
and the cross sections for these types of
events are assumed to be known.
A Monte-Carlo solution now simply
involves the tracking of simulated parti-
cles from collision to collision, generating
the distances s that the particles travel
without collision from the exponential
distribution quoted above. Particles leaving
a collision point are sampled from the
appropriate conditional probabilities as
determined from the respective diﬀerential
cross sections. For increasing sampling
eﬃciency, many obvious tricks are known.
For example, one may avoid losing particles
by absorption events: If the absorption
probability (i.e., the conditional probability
that absorption occurs given that a collision
has occurred) is 𝛼, one may replace 𝜎c(E)
by 𝜎c(E)(1 −𝛼), and allow only scattering
to take place with the appropriate rela-
tive probability. Special methods for the
shielding problem have been extensively
developed already and have been reviewed
by Hammersley and Handscomb [1].
2.4.2
Diﬀusion-Limited Aggregation (DLA)
DLA is a model for the irreversible for-
mation of random aggregates by diﬀusion
of particles that get stuck at random posi-
tions on the already formed object if they
hit its surface in the course of their dif-
fusion (see Vicsek [25], Meakin [26], and
Herrmann [19] for detailed reviews of this
problem and related phenomena). Many
structures (shapes of snowﬂakes, size dis-
tribution of asteroids, roughness of crack
surfaces, etc.) can be understood as the

2.5 Monte-Carlo Methods in Statistical Thermodynamics: Importance Sampling
47
end product of similar random irreversible
growth processes. DLA is just one example
of them. It is simulated by iterating the
following steps: From a randomly selected
position on a spherical surface of radius Rm
that encloses the aggregate (grown in the
previous steps, its center of gravity being
in the center of the sphere), a particle of
unit mass is launched to start a simple
RW trajectory. If it touches the aggregate,
it sticks irreversibly on its surface. After
the particle has either stuck or moved a
distance Rf from the center of the aggre-
gate such that it is unlikely that it will hit
in the future, a new particle is launched.
Ideally, one would like to have Rf →∞
but, in practice, Rf = 2Rm suﬃces. By this
irreversible aggregation of particles, one
forms fractal clusters. This means that the
dimension df characterizing the relation
between the mass of the grown object and
its (gyration) radius R, M ∝Rdf, is less than
the dimension d of space in which the
growth takes place. Again there are some
tricks to make such simulations more eﬃ-
cient: For example, one may allow the par-
ticles to jump over larger steps when they
travel in empty regions. From such studies,
researchers have found that df = 1.715 ±
0.004 for DLA in d = 2, while df = 2.485 ±
0.005 in d = 3 [27].
2.5
Monte-Carlo Methods in Statistical
Thermodynamics: Importance Sampling
2.5.1
The General Idea of the Metropolis
Importance-Sampling Method
In the canonical ensemble, the average of an
observable A(X) takes the form
⟨A⟩= 1
Z ∫Ω
dkXA(X)exp
[
−ℋ(X)
kBT
]
, (2.10)
where Z is the partition function,
Z = ∫Ω
dkXexp
[
−ℋ(X)
kBT
]
,
(2.11)
Ω denoting the (k-dimensional) volume of
phase space {X} over which the integra-
tion is extended, ℋ(X) being the (classical)
Hamiltonian. For this problem, a simple
sampling analogue to Section 2.3 would not
work: The probability distribution p(X) =
(1∕Z) exp[−ℋ(X)∕kBT] has a very sharp
peak in phase space where all extensive
variables A(X) are close to their average val-
ues ⟨A⟩. This peak may be approximated
by a Gaussian centered at ⟨A⟩, with a rela-
tive half-width of order 1∕
√
N only, if we
consider a system of N particles. Hence,
for a practically useful method, one can-
not sample the phase space uniformly, but
the points X𝜈must be chosen preferentially
from the important region of phase space,
that is, from the vicinity of the peak of p(X).
This goal is achieved by the importance-
sampling method [28]: Starting from some
initial conﬁguration X1, one constructs a
sequence of conﬁgurations X𝜈deﬁned in
terms of a transition probability W(X𝜈→
X′
𝜈) that rules stochastic “moves” from an
old state X𝜈to a new state X′
𝜈, and, hence,
one creates a “RW through phase space.”
The idea is to choose W(X →X′) such that
the probability with which a point X is cho-
sen converges toward the canonical proba-
bility
Peq(X) =
( 1
Z
)
exp
[
−ℋ(X)
kBT
]
in the limit where the number M of states
X generated goes to inﬁnity. A condition
suﬃcient to ensure this convergence is the
so-called principle of detailed balance,
Peq(X)W(X →X′) = Peq(X′)W(X′ −X).
(2.12)

48
2 Monte-Carlo Methods
For a justiﬁcation that (2.12) actually yields
this desired convergence, we refer to Ham-
mersley and Handscomb [1], Binder [29],
Binder and Heermann [30], and Kalos
and Whitlock [31]. In this importance-
sampling technique, the average (2.10)
then is estimated in terms of a simple
arithmetic average,
A =
1
M −M0
M
∑
v = M0+1
A(Xv).
(2.13)
Here it is anticipated that it is advantageous
to eliminate the residual inﬂuence of the
initial conﬁguration X1 by eliminating a
large enough number M0 of states from
the average. (The judgment of what is
“large enough” is often diﬃcult; see Binder
[29] and Section 2.5.3 below.) Note that
this Metropolis method can be used for
sampling any distribution P(X): One sim-
ply must choose a transition probability
W(X→X′) that satisﬁes a detailed bal-
ance condition with P(X) rather than with
Peq(X).
2.5.2
Comments on the Formulation of a
Monte-Carlo Algorithm
What is now meant in practice by the tran-
sition X→X′? Again, there is no general
answer to this question; the choice of the
process may depend both on the model
studied and the purpose of the study. As
(2.12) implies that W(X −X′)∕W(X′ →
X) = exp(−𝛿ℋ∕kBT),
𝛿ℋ
being
the
energy change caused by the move from
X→X′, typically it is necessary to con-
sider small changes of the state X only.
Otherwise, |𝛿ℋ| would be rather large,
and then either W(X→X′) or W(X′ →X)
would be very small, and the procedure
would be poorly convergent. For example,
in the lattice gas model at constant particle
number, a transition X→X′ may consist
of moving one particle to a randomly
chosen neighboring site. In the lattice
gas at constant chemical potential, one
removes (or adds) just one particle at
a time, which is isomorphic to single
ﬂips in the Ising model of anisotropic
magnets.
Another arbitrariness concerns the order
in which the particles are selected for con-
sidering a move. Often, one selects them
in the order of their labels (in the simula-
tion of a ﬂuid at constant particle number)
or to go through the lattice in a regu-
lar typewriter-type manner (in the case of
spin models, for instance). For lattice sys-
tems, it may be convenient to use sub-
lattices (e.g., the “checkerboard algorithm,”
where the white and black sublattices are
updated in alternation, for the sake of an
eﬃcient “vectorization” of the program; see
[32]). An alternative is to choose the lat-
tice sites (or particle numbers) randomly.
The latter procedure is somewhat more
time consuming, but it is a more faithful
representation of a dynamic time evolu-
tion of the model described by a master
equation (see below).
It is also helpful to realize that often
the transition probability W(X→X′) can
be written as a product of an “attempt
frequency”
times
an
“acceptance
fre-
quency.” By clever choice of the attempt
frequency, one sometimes can attempt
large moves and still have a high accep-
tance, making the computations more
eﬃcient.
For spin models on lattices, such as
Ising or Potts models, XY, and Heisen-
berg ferromagnets, and so on, algorithms
have been devised where one does not
update single spins in the move X→X′,
but, rather, one updates specially con-
structed clusters of spins (see [33], for a

2.5 Monte-Carlo Methods in Statistical Thermodynamics: Importance Sampling
49
review). These algorithms have the merit
that they reduce critical slowing down,
which hampers the eﬃciency of Monte-
Carlo
simulations
near
second-order
phase transitions. “Critical slowing down”
means a dramatic increase of relaxation
times at the critical point of such tran-
sitions and these relaxation times also
control statistical errors in simulations;
see Section 2.6. As these “cluster algo-
rithms” work for rather special models
only, they will not be discussed further
here.
There is also some arbitrariness in
the choice of the transition probability
W(X→X′) itself. The original choice of
Metropolis et al. [28] is
W(X →X′) =
{
exp
(
−𝛿ℋ
kBT
)
if 𝛿ℋ> 0,
1
otherwise.
(2.14)
An alternative choice is the so-called “heat-
bath method.” There one assigns the new
value 𝛼′
i of the ith local degree of freedom in
the move from X to X′ irrespective of what
the old value 𝛼i was. Considering the local
energy ℋi(𝛼′
i), one chooses the state 𝛼′
i with
probability
exp
[
−ℋi
(
𝛼′
i
)
kBT
]
∑
{𝛼′′
i }
exp
[
−ℋi
(
𝛼′′
i
)
kBT
].
We now outline the realization of the
sequence of states X with chosen transition
probability W. At each step of the proce-
dure, one performs a trial move 𝛼i →𝛼′
i,
computes W(X→X′) for this trial move,
and compares it with a random number
𝜅, uniformly distributed in the interval
[0,1]. If W < 𝜅, the trial move is rejected,
and the old state (with 𝛼i) is counted once
more in the average, (2.13). Then another
trial is made. If W > 𝜅, on the other hand,
the trial move is accepted, and the new
conﬁguration thus generated is taken into
account in the average, (2.13). It serves
then also as a starting point of the next
step.
As subsequent states X𝜈in this Markov
chain diﬀer by the coordinate 𝛼i of one
particle only (if they diﬀer at all), they
are highly correlated. Therefore, it is not
straightforward to estimate the error of
the average, (2.13). Let us assume for
the moment that, after n steps, these
correlations
have
died
out.
Then
we
may estimate
the statistical
error
𝛿A
of the estimate A from the standard
formula,
(𝛿A)2 =
1
m(m −1)
m + 𝜇0−1
∑
𝜇=𝜇0
[A(X𝜇) −A]2,
m ≫1,
(2.15)
where the integers 𝜇0, 𝜇, m are deﬁned
by m = (M −M0)/n, 𝜇0 labels the state
𝜈= M0+ 1, 𝜇= 𝜇0 + 1 labels the state
𝜈= M0+ 1 +n, and so on. Then, for consis-
tency, A should be taken as
A = 1
m
m + 𝜇0−1
∑
𝜇=𝜇0
A(X𝜇).
(2.16)
If the computational eﬀort of carrying
out the “measurement” of A(X𝜇) in the
simulation is rather small, it is advanta-
geous to keep taking measurements every
Monte-Carlo step (MCS) per degree of
freedom but to construct block averages
over n successive measurements, varying
n until uncorrelated block averages are
obtained. Details on error estimation for
Monte-Carlo simulations can be found in
Berg [34].

50
2 Monte-Carlo Methods
2.5.3
The Dynamic Interpretation of the
Monte-Carlo Method
It is not always easy to estimate the number
of states M0 after which the correlations to
the initial state X1, which typically is a state
far from equilibrium, have died out, nor is
it easy to estimate the number n between
steps after which correlations in equilib-
rium have died out. A formal answer to this
problem, in terms of relaxation times of
the associated master equation describing
the Monte-Carlo process, is discussed
in Section 2.5.4. This interpretation of
Monte-Carlo sampling in terms of master
equations is also the basis for Monte-Carlo
studies of the dynamics of ﬂuctuations near
thermal equilibrium, and is discussed now.
One introduces the probability P(X,t) that
a state X occurs at time t. This probability
then decreases by all moves X →X′, where
the system reaches a neighboring state X′;
inverse processes X′ →X lead to a gain of
probability. Thus, one can write down a
rate equation, similar to chemical kinetics,
considering the balance of all gain and loss
processes:
d
dt P(X, t) = −
∑
X′
W(X →X′)P(X, t)
+
∑
X′
W(X′ →X)P(X′, t). (2.17)
The
Monte-Carlo
sampling
(i.e.,
the
sequence of generated states X1 →X2 →
· · · →X𝜈→· · ·) can hence be interpreted
as a numerical realization of the master
equation, (2.17), and then a “time” t is
associated with the index v of subsequent
conﬁgurations. In a system with N parti-
cles, we may normalize the “time unit” such
that N single-particle moves are attempted
per unit time. This is often called a sweep
step or 1 Monte-Carlo step (MCS).
For the thermal equilibrium distribution
Peq(X), because of the detailed balance
principle, (2.12), there is no change of
probability
with
time,
dP(X, t)∕dt = 0;
thus, thermal equilibrium arises as the
stationary solution of the master equation,
(2.17). Thus, it is also plausible that Markov
processes described by (2.17) describe a
relaxation that always leads toward thermal
equilibrium, as desired.
Now, for a physical system (whose trajec-
tory in phase space, according to classical
statistical mechanics, follows from New-
ton’s laws of motion), it is clear that the
stochastic trajectory through phase space
that is described by (2.17) in general has
nothing to do with the actual dynamics. For
example, (2.17) never describes any propa-
gating waves (such as spin waves in a mag-
net, or sound waves in a ﬂuid, etc.).
In spite of this observation, the dynamics
of the Monte-Carlo “trajectory” described
by (2.17) sometimes does have physical sig-
niﬁcance. In many situations, one does not
consider the full set of dynamical variables
of the system, but rather a subset only:
For instance, in modeling the diﬀusion pro-
cesses in an interstitial alloy, the diﬀusion
of the interstitials may be described by a
stochastic hopping between the available
lattice sites. As the mean time between
two successive jumps is orders of magni-
tude larger than the time scale of atomic
vibrations in the solid, the phonons can be
approximated as a heat bath, as far as the
diﬀusion is concerned.
There are many examples where such
a separation of time scales for diﬀerent
degrees of freedom occurs: For example,
describing the Brownian motion of poly-
mer chains in polymer melts, the fast
bond-angle and bond-length vibrations
may be treated as heat bath, and so on. As
a rule of thumb, any very slow relaxation
phenomena (kinetics of nucleation, decay

2.5 Monte-Carlo Methods in Statistical Thermodynamics: Importance Sampling
51
of remnant magnetization in spin glasses,
growth of ordered domains in adsorbed
monolayers at surfaces, etc.) can be mod-
eled by Monte-Carlo methods. Of course,
one must build in relevant conservation
laws into the model properly (e.g., in an
interstitial alloy, the overall concentra-
tion of interstitials is conserved; in a spin
glass, the magnetization is not conserved)
and choose microscopically reasonable
elementary steps for the move X →X′.
The great ﬂexibility of the Monte-Carlo
method, where one can choose the level
of the modeling appropriately for the sys-
tem at hand and identify the degrees of
freedom that one wishes to consider, as
well as the type and nature of transitions
between them, is a great advantage and
thus allows complementary applications
to more atomistically realistic simula-
tion approaches such as the molecular
dynamics (MD) method where one numer-
ically integrates Newton’s equations of
motion [35].
2.5.4
Monte-Carlo Study of the Dynamics of
Fluctuations Near Equilibrium and of the
Approach toward Equilibrium
Accepting (2.17), the average in (2.13) then
is interpreted as a time average along the
stochastic trajectory in phase space,
A =
1
tM −tM0 ∫
tM
tM0
A(t)dt,
tM = M
N ,
tM0 = M0
N .
(2.18)
It is thus no surprise that, for importance-
sampling
Monte-Carlo,
one
needs
to
consider carefully the problem of ergod-
icity: Time averages need not agree with
ensemble averages. For example, near
ﬁrst-order phase transitions, there may be
long-lived metastable states. Sometimes,
the considered moves do not allow one to
reach all conﬁgurations (e.g., in dynamic
Monte-Carlo
methods
for
SAWs;
see
[20]).
One can also deﬁne time-displaced cor-
relation functions: ⟨A (t) B (0)⟩, where A, B
stand symbolically for any physical observ-
ables, is estimated by
A(t)B(0) =
1
tM −t −tM0 ∫
tM−t
tM0
A(t + t′)
B(t′)dt′,
tM −t > tM0.
(2.19)
Equation (2.19) refers to a case where tM0 is
chosen large enough such that the system
has relaxed toward equilibrium during the
time tM0; then the pair correlation depends
only on t and not the two individual times
t′, t′ + t separately.
However, it is also interesting to study the
nonequilibrium process by which equilib-
rium is approached. In this region, A(t) −A
depends systematically on the observation
time t, and an ensemble average ⟨A (t)⟩T −
⟨A (∞)⟩T (limt→∞A = ⟨A⟩T ≡⟨A (∞)⟩T if
the system is ergodic, that is, the time aver-
age agrees with the ensemble average at
the chosen temperature T) is nonzero. One
may deﬁne
⟨A(t)⟩T =
∑
{X}
P(X, t)A(X)
=
∑
{X}
P(X, 0)A(X(t)).
(2.20)
In the second step of (2.20), the fact was
used that the ensemble average involved is
actually an average weighted by P(X, 0) over
an ensemble of initial states X(t = 0), which
then evolve as described by the master
equation, (2.17). In practice, (2.20) means
an average over a large number nrun ≫1
statistically independent runs,
[
A(t)
]
av =
1
nrun
nrun
∑
l=1
A(t, l),
(2.21)

52
2 Monte-Carlo Methods
where A(t, l) is the observable A observed at
time t in the lth run of this nonequilibrium
Monte-Carlo averaging.
Many concepts of nonequilibrium statis-
tical mechanics can immediately be used
in such simulations. For instance, one can
introduce “ﬁelds” that can be switched oﬀ
to study the dynamic response functions,
for both linear and nonlinear responses
[36].
2.5.5
The Choice of Statistical Ensembles
While so far the discussion has been
(implicitly) restricted to the case of the
canonical ensemble (NVT ensemble, for
the case of a ﬂuid, particle number N,
volume V, and temperature T being held
ﬁxed), it is sometimes useful to use other
statistical ensembles. Particularly useful
is the grand canonical ensemble (𝜇VT),
where the chemical potential 𝜇rather than
the particle number N is ﬁxed. In addition
to moves where the conﬁguration of par-
ticles in the box relaxes, one has moves
where one attempts to add or remove a
particle from the box.
In the case of binary (AB) mixtures, a use-
ful variation is the “semi–grand canonical”
ensemble, where Δ𝜇= 𝜇A−𝜇B is held ﬁxed
and moves where an A particle is converted
into a B particle (or vice versa) are consid-
ered, in an otherwise identical system con-
ﬁguration.
The isothermal-isobaric (NpT) ensemble,
on the other hand, ﬁxes the pressure and
then volume changes V →V ′ = V + ΔV
need to be considered (rescaling properly
the positions of the particles).
It also is possible to deﬁne artiﬁcial
ensembles that are not in the textbooks
on statistical mechanics. An example is
the
Gaussian
ensemble
(interpolating
between the canonical and microcanonical
ensemble, useful for the study of ﬁrst-order
phase transitions, such as the so-called
“multicanonical ensemble”). Particularly
useful is the “Gibbs ensemble,” where one
considers the equilibrium between two
simulation boxes (one containing liquid,
the other gas), which can exchange both
volume ΔV and particles (ΔN), while the
total volume and total particle number
contained in the two boxes are held ﬁxed.
The Gibbs ensemble is useful for the sim-
ulation of gas-ﬂuid coexistence, avoiding
interfaces [37, 38].
A simulation at a given state point (NVT)
contains information not only on averages
at that point but also on neighboring states
(NVT′), via suitable reweighting of the
energy distribution PN(E) with a factor
exp(E/kBT)exp(−E/kBT′). Such “histogram
methods”
are
particularly
useful
near
critical points [33].
2.6
Accuracy Problems: Finite-Size Problems,
Dynamic Correlation of Errors, Boundary
Conditions
2.6.1
Finite-Size–Induced Rounding and
Shifting of Phase Transitions
A prominent application of Monte-Carlo
simulation in statistical thermodynamics
and lattice theory is the study of phase
transitions. it is well known now in statis-
tical physics that sharp phase transitions
can occur in the thermodynamic limit only,
N →∞. This is no practical problem in
everyday life – even a small water droplet
freezing into a snowﬂake contains about
N = 1018H2O molecules, and, thus, the
rounding and shifting of the freezing are on
a relative scale of
1
√
N = 10−9 and thus com-
pletely negligible. But the situation diﬀers

2.6 Accuracy Problems: Finite-Size Problems, Dynamic Correlation of Errors, Boundary Conditions
53
for simulations, which often consider very
small systems (e.g., a d-dimensional box
with linear dimensions L, V = Ld, and
periodic boundary conditions), where only
N ∼103 −106 particles are involved.
In such small systems, phase transi-
tions are strongly rounded and shifted
[39–42]. Thus, care needs to be applied
when simulated systems indicate phase
changes. It turns out, however, that these
ﬁnite-size eﬀects can be used as a valu-
able tool to infer properties of the inﬁnite
system from the ﬁnite-size behavior. As a
typical example, we discuss the phase tran-
sition of an Ising ferromagnet (Figure 2.1),
which has a second-order phase tran-
sition at a critical temperature Tc. For
L →𝛼, the spontaneous magnetization
Mspont vanishes according to a power law,
Mspont = B(1 −T/Tc)𝛽, B being a critical
amplitude and 𝛽a critical exponent [43],
and the susceptibility 𝜒and correlation
length 𝜉diverge,
𝜒∝
||||
1 −T
Tc
||||
−𝛾
, 𝜉∝
||||
1 −T
Tc
||||
−𝜈
(2.22)
where 𝛾and 𝜈are critical exponents. In
a ﬁnite system, 𝜉cannot exceed L, hence,
these singularities are smeared out.
Now ﬁnite-size scaling theory [39, 42]
implies that these ﬁnite-size eﬀects are
understood from the principle that “L
scales with 𝜉”; that is, the order-parameter
probability
distribution
PL(M)
can
be
written [40, 41]
PL = L𝛽∕v ̃P
(
L
𝜉, ML
𝛽
v
)
,
(2.23)
where ̃P is a “scaling function.” From (2.23),
one immediately obtains the ﬁnite-size
scaling relations for order parameter ⟨|M|⟩
and the susceptibility (deﬁned from a ﬂuc-
tuation relation) by taking the moments of
the distribution PL:
⟨|M|⟩= L−𝛽∕v ̃M
(
L
𝜉
)
,
(2.24)
kBT𝜒′ = Ld (⟨M2⟩−⟨|M|⟩2)=L𝛾∕v ̃X
(
L
𝜉
)
,
(2.25)
where ̃M, ̃𝜒are scaling functions that fol-
low from ̃P in (2.23) At Tc where 𝜉→∞, we
thus have 𝜒′ ∝L𝛾∕𝜈; from this variation of
𝜒′ with L, hence, the exponent 𝛾∕𝜈can be
extracted.
The fourth-order cumulant UL is a func-
tion of L/𝜉only,
UL ≡1 −
⟨M4⟩
(3 ⟨M2⟩2) = ̃U
(
L
𝜉
)
.
(2.26)
Here UL →0 for a Gaussian centered at
M = 0, that is, for T > Tc; UL →2/3 for
the double-Gaussian distribution, that is,
for T < Tc; while UL = ̃U(0) is a universal
nontrivial constant for T = Tc. Cumulants
for diﬀerent system sizes hence intersect
at Tc, and this can be used to locate Tc
precisely [40, 41].
A simple discussion of ﬁnite-size eﬀects
at ﬁrst-order transitions is similarly possi-
ble [41]: one describes the various phases
that coexist at the ﬁrst-order transition in
terms of Gaussians if L ≫𝜉(note that 𝜉
stays ﬁnite at the ﬁrst-order transition).
In a ﬁnite system, these phases coexist
not only right at the transition but over
a ﬁnite parameter region. The weights of
the respective peaks are given in terms of
the free-energy diﬀerence of the various
phases. From this description, energy and
order-parameter distributions and their
moments can be worked out. Of course,
this description applies only for long
enough runs where the system jumps from
one phase to the other many times, while
for short runs where the systems stay in a
single phase, one observes hysteresis.

54
2 Monte-Carlo Methods
PL (M)
PL (M)
PL (M)
M
M
L
L finite
Mspont = B(1−T/Tc)β
0
0
+ Mspont
− Mspont
0
0
Tc
T  > Tc
T = Tc
T < Tc
T
Tc
UL
Tc
T
T
∞
<IMI>
Ld ( <M2> − <IMI>2 )
L
L
L′ > L
∞
L
∞
kBTX / L°
∝L−β/ν
∝|I-T/Tc|−Y
2/3
Figure 2.1
(a–f) Schematic evolution of
the order-parameter probability distribution
PL(M) from T > Tc to T < Tc (from above to
below, left part), for an Ising ferromagnet
(where M is the magnetization) in a box of
volume V = Ld. The right part shows the cor-
responding temperature dependence of the
mean order parameter ⟨|M|⟩, the susceptibility
kBT𝜒′ = Ld (⟨M2⟩−⟨|M|⟩2), and the reduced
fourth-order cumulant UL = 1 −⟨M4⟩∕[3⟨M2⟩2].
Dash-dotted curves indicate the singular vari-
ation that results in the thermodynamic limit,
L →∞.
2.6.2
Diﬀerent Boundary Conditions: Simulation
of Surfaces and Interfaces
We now brieﬂy discuss the eﬀect of various
boundary conditions. Typically, one uses
periodic boundary conditions to study
bulk properties of systems not obscured
by surface eﬀects. However, it also is
possible to choose boundary conditions
to study surface eﬀects deliberately; for
example, one may simulate thin ﬁlms in
an L × L × D geometry with two free L × L
surfaces and periodic boundary condi-
tions otherwise. If the ﬁlm thickness D
is large enough, the two surfaces do not
inﬂuence each other, and one can infer the
properties of a semi-inﬁnite system. One

2.6 Accuracy Problems: Finite-Size Problems, Dynamic Correlation of Errors, Boundary Conditions
55
may choose special interactions near the
free surfaces, apply surface “ﬁelds” (even if
they cannot be applied in the laboratory,
it may nevertheless be useful to study the
response to them in the simulation), and
so on.
Sometimes, the boundary conditions
may stabilize interfaces in the system (e.g.,
in an Ising model for T < Tc a domain wall
between phases with opposite magneti-
zation will be present, if we apply strong
enough surface ﬁelds of opposite sign).
Such interfaces also are often the object
of study. It may be desirable to simulate
interfaces without having the systems
disturbed by free surfaces. In an Ising
system, this may simply be done by choos-
ing
antiperiodic
boundary
conditions.
Combining
antiperiodic
and
staggered
periodic boundary conditions, even tilted
interfaces may be stabilized in the system.
In all such simulations of systems con-
taining interfaces, one must keep in mind,
however, that because of capillary-wave
excitations, interfaces usually are very
slowly relaxing objects, and often a major
computing eﬀort is needed to equilibrate
them. A further diﬃculty (when one is
interested in interfacial proﬁles) is the fact
that the center of the interface is typically
delocalized.
2.6.3
Estimation of Statistical Errors
We now return to the problem of judg-
ing the time needed for having reasonably
small errors in Monte-Carlo sampling. If
the subsequent conﬁgurations used were
uncorrelated, we could use (2.15), but in the
case of correlations we have rather (here
the index 𝜇should not be confused with
the chemical potential of Section 2.5.5, of
course)
⟨(𝛿A)2⟩=
⟨[
1
n
n
∑
𝜇=1
A𝜇−⟨A⟩
]2⟩
= 1
n
[
⟨A2⟩−⟨A⟩2 + 2
n
∑
𝜇=1
(
1 −𝜇
n
)
×
(⟨A0A𝜇⟩−⟨A⟩2)
]
.
(2.27)
Now we remember that a time t𝜇= 𝜇𝛿t is
associated with the Monte-Carlo process,
𝛿t being the time interval between two suc-
cessive observations A𝜇, A𝜇+1. Transform-
ing the summation to a time integration
yields
⟨(𝛿A)2⟩= 1
n
(⟨A2⟩−⟨A⟩2)
×
[
1+ 2
𝛿t ∫
tn
0
(
1−t
tn
)
𝜙A(t)dt
]
,
(2.28)
where
𝜙A(t) ≡⟨A(0)A(t)⟩−⟨A⟩2
⟨A2⟩−⟨A⟩2
.
Deﬁning a relaxation time 𝜏A = ∫∞
0 dt𝜙A(t),
one obtains for 𝜏A ≪n𝛿t = 𝜏obs (the obser-
vation time)
⟨(𝛿A)2⟩= 1
n[⟨A2⟩−⟨A⟩2]
(
1 + 2𝜏A
𝛿t
)
≈2
( 𝜏A
𝜏obs
)
[⟨A2⟩−⟨A⟩2].
(2.29)
In comparison with (2.15), the dynamic
correlations inherent in a Monte-Carlo
sampling as described by the master
equation, (2.17), lead to an enhancement
of the expected statistical error ⟨(𝛿A)2⟩by
a “dynamic factor” 1 + 2𝜏A/𝛿t (sometimes
also called the statistical ineﬃciency).
This dynamic factor is particularly cum-
bersome near second-order phase transi-
tions (𝜏A diverges: critical slowing down)
and near ﬁrst-order phase transitions (𝜏A
diverges at phase coexistence, because of

56
2 Monte-Carlo Methods
the large lifetime of metastable states).
Thus, even if one is interested only in
static quantities in a Monte-Carlo simula-
tion, understanding the dynamics may be
useful for estimating errors. In addition,
the question of how many conﬁgurations
(M0) must be omitted at the start of the
averaging for the sake of equilibrium (2.18)
can be formally answered in terms of a
nonlinear relaxation function
𝜙(nl)(t) = ⟨A(t)⟩T −⟨A(∞)⟩T
⟨A(0)⟩T −⟨A(∞)⟩T
and its associated time 𝜏(nl)
A
= ∫∞
0 𝜙(nl)
A (t)dt
by the condition tM0 ≫𝜏(nl)
A . Here tM0 is the
time corresponding to the number of states
M0 omitted.
2.7
Sampling of Free Energies and Free Energy
Barriers
2.7.1
Bulk Free Energies
Carrying out the step from (2.10) to (2.13),
the explicit knowledge on the partition
function Z of the system, and hence its
free energy F = −kBT ln Z is lost. In many
cases, however, knowledge of F would be
very useful: for example, at a ﬁrst-order
phase transition the free energies F1, F2 of
two distinct phases 1, 2 of a system become
equal.
There are many ways how this drawback
of importance-sampling Monte-Carlo can
be overcome (see [30, 36]). We describe
here very brieﬂy the most popular method,
Wang-Landau sampling [44]. It starts from
the concept that Z can be expressed in
terms of the energy density of states g(E) as
Z = ∫dEg (E) exp
(
−E
kBT
)
.
Suppose g(E) is known and one deﬁnes a
Markov process via a transition probability
p(E →E′) as
p
(
E →E
′)
= min
{
g (E)
g (E
′), 1
}
This process would generate a ﬂat his-
togram H(E) = const. As g(E) is not known
beforehand, we can use the process deﬁned
by the above equation to construct an
iteration that yields an estimate for g(E):
when we know g(E), Z and hence F are also
known.
In the absence of any a priori knowledge
on g(E), the iteration starts with g(E) ≡1
for all E (for simplicity, we consider here
an energy spectrum that is discrete and
bounded, Emin ≤E ≤Emax). Now Monte-
Carlo moves (e.g., spin ﬂips in an Ising
model) are carried out to accumulate a his-
togram H(E), starting out with H(E) = 0 for
all E and replacing H(E) by H(E) + 1 when-
ever E is visited. We also replace g(E) by
g(E)f where the modiﬁcation factor f ini-
tially is large, for example, f = e1. The sam-
pling of the histogram is continued, until
H(E) is reasonably “ﬂat” (e.g., nowhere less
than 80% of its maximum value).
Then H(E) is reset to H(E) = 0 for all E,
f is replaced by
√
f , and the process con-
tinues: in the ith step of the iteration fi =
√
fi−1, so f tends to unity; that is, we have
reached the above p(E →E′), as desired,
and can use the corresponding g(E) to com-
pute the desired averages.
This algorithm is robust and widely
used (see Landau and Binder [36] for
examples). We stress, however, that many
other schemes for obtaining g(E) exist,
such as the so-called “umbrella sampling,”
“multicanonical Monte-Carlo,” “transition
matrix Monte-Carlo,” and so on. Often the
density of states is required as a function
of several variable (e.g., g(E,M) where M is

2.8 Quantum Monte-Carlo Techniques
57
the magnetization, for an Ising model). We
refer to the more specialized literature [36]
for more details on these techniques.
2.7.2
Interfacial Free Energies
When in the volume taken by a system
several phases coexist, the interfaces cause
excess contribution to the free energy. The
simplest case again is provided by the Ising
model, considering again the distribution
PL(M) of the magnetization M in a system
of ﬁnite linear dimension L, cf. Figure 2.1,
but now for T ≪Tc (Figure 2.1): A state
with M ≈0 actually is characterized by
two-phase coexistence in the simulation
box: a domain with M ≈+Mspont is sepa-
rated from a (roughly equally large) domain
with M ≈−Mspont by two planar interfaces
(of area Ld−1 if the volume is Ld; note the
periodic boundary conditions). As a result,
one can argue that PL(M ≈0)∕PL(M =
Mspont) ≈exp(−2Ld−1
fint∕kBT), with fint
the interface free-energy density between
the coexisting phases.
Accurate sampling of PL(M) near its
minimum (which can be done by umbrella
sampling, for instance) in fact is a use-
ful method for estimating fint in various
systems. Of course, there exist many alter-
native techniques, and also methods to
estimate the excess free energies due to
external walls, and related quantities such
as contact angles of droplets, line tensions,
and so on. This subject is still a very active
area of research [45, 46].
2.7.3
Transition Path Sampling
Suppose we consider a system with a com-
plex free-energy landscape, where the sys-
tem can stay in some minimum (denoted
as A) for a long time, and in a rare event
may pass through some saddle point region
to another minimum (denoted as B). Being
interested in the rates of such process, one
must consider the possibility that many tra-
jectories from A to B in conﬁguration space
may compete with each other. Transition
path sampling [47] is a technique to explore
the dominating trajectories and thus the
rate of such rare events. Starting out from
one trial trajectory that leads from A to B, a
sampling of stochastic variations of this tra-
jectory is performed. Again, the technique
and its theoretical foundations are rather
involved, and still under development.
2.8
Quantum Monte-Carlo Techniques
2.8.1
General Remarks
Development
of
Monte-Carlo
tech-
niques
to
study
ground-state
and
ﬁnite-temperature
properties
of
inter-
acting quantum many-body systems is an
active area of research (for reviews, see
[48–56]). These methods are of interest for
problems such as the structure of nuclei
[57] and elementary particles [58], super-
ﬂuidity of 3He and 4He [53, 54], high-Tc
superconductivity (e.g., [59]), magnetism
(e.g., [60, 61]), surface physics [62], and so
on. Despite this widespread interest, much
of this research has the character of “work
in progress” and hence cannot feature
more prominently in this chapter. Besides,
there is not just one quantum Monte-Carlo
method, but many variants that exist:
variational Monte-Carlo (VMC), Green’s-
function Monte-Carlo (GFMC), projector
Monte-Carlo (PMC), path-integral Monte-
Carlo (PIMC), grand canonical quantum
Monte-Carlo (GCMC), world-line quan-
tum Monte-Carlo (WLQMC), and so on.

58
2 Monte-Carlo Methods
Some of these (such as VMC, GFMC)
address
ground-state
properties,
oth-
ers (such as PIMC) ﬁnite temperatures.
Here only the PIMC technique will be
brieﬂy
sketched,
following
Gillan
and
Christodoulos [63].
2.8.2
Path-Integral Monte-Carlo Methods
We wish to calculate thermal averages for
a quantum system and thus rewrite (2.10)
and (2.11) appropriately,
⟨A⟩= 1
Z Tr exp
(
−
̂
ℋ
kBT
)
̂A,
Z = Tr exp
(
−
̂
ℋ
kBT
)
,
(2.30)
using a notation that emphasizes the oper-
ator character of the Hamiltonian ̂
ℋand of
the quantity Â associated with the variable
A that we consider. For simplicity, we con-
sider ﬁrst a system of a single particle in one
dimension acted on by a potential V(x). Its
Hamiltonian is
̂
ℋ= −ℏ2
2m
d2
dx2 + V(x).
(2.31)
Expressing the trace in the position repre-
sentation, the partition function becomes
Z = ∫dx
⟨
x
|||||
exp
(
−̂
ℋ
kBT
)|||||
x
⟩
,
(2.32)
where |x⟩is an eigenvector of the position
operator. Writing exp(−̂
ℋ∕kBT) formally
as [exp(−̂
ℋ∕kBTP)]P, where P is a posi-
tive integer, we can insert a complete set of
states between the factors:
Z = ∫dx1 · · · ∫dxP⟨x1|exp
(
−̂
ℋ
kBTP
)
|x2⟩
⟨x2| · · · |xp⟩⟨xP|exp
(
−̂
ℋ
kBTP
)
|x1⟩.
(2.33)
For large P, it is a good approximation to
ignore the fact that kinetic and potential
energy do not commute. Hence, one gets
⟨x|exp
(
−̂
ℋ
kBTP
)
|x′⟩≈
(kBTmP
2𝜋ℏ2
)1∕2
× exp
[−kBTmP
2𝜋ℏ2
(x −x′)2
]
× exp
{
−1
2kBTP
[
V (x) + V
(
x′)]}
(2.34)
and
Z ≈
(kBTmP
2𝜋ℏ2
)P∕2
∫dx1 · · · ∫dxP
× exp
{
−1
kBT
[
1
2
P
∑
s=1
𝜅(xs −xs+1
)2
]
+ P−1
P
∑
s=1
V(xs)
}
,
(2.35)
where
𝜅≡
(kBT
ℏ
)2
mP.
(2.36)
In the limit P →∞, (2.35) becomes exact.
Apart from the prefactor, (2.35) is precisely
the conﬁgurational partition function of a
classical system, a ring polymer consisting
of P beads coupled by harmonic springs
with spring constant 𝜅. Each bead is in a
potential V(x)/P.
This approach is straightforwardly gener-
alized to a system of N interacting quantum
particles: one gets a system of N classical
cyclic “polymer” chains. As a result of this
isomorphism, the Monte-Carlo method
for simulating classical systems can be
carried over to such quantum-mechanical
problems too. It is also easy to see that
the system always behaves classically at
high temperatures: 𝜅gets very large, and
then the cyclic chains contract essentially
to a point, while at low temperatures,

2.8 Quantum Monte-Carlo Techniques
59
they are spread out, representing zero-
point motion. However, PIMC becomes
increasingly diﬃcult at low temperatures,
because P has to be the larger the lower
that T is: If 𝜎is a characteristic distance
over which the potential V(x) changes,
one must have ℏ2∕m𝜎2 ≪kBTP in order
that two neighbors along the “polymer
chain” are at a distance much smaller than
𝜎. In PIMC simulations, one empirically
determines and uses that P beyond which
the thermodynamic properties do not
eﬀectively change.
This
approach
can
be
general-
ized to the density matrix 𝜌(x −x′) =
⟨x| exp(−̂
ℋ∕kBT)|x′⟩,
while
there
are
problems
with
time-displaced
corre-
lation functions ⟨A(t)B(0)⟩, where t is
now the true time (associated with the
time evolution of states following from
the Schrödinger equation, rather than the
“time” of Section 2.5.3 related to the master
equation).
The step leading to (2.34) can be viewed
as a special case of the Suzuki–Trotter for-
mula [51]
exp ( ̂A + ̂B)= lim
P→∞
[
exp
( ̂A
P
)
exp
( ̂B
P
)]P
,
(2.37)
which
is
also
used
for
mapping
d-
dimensional quantum problems on lattices
to equivalent classical problems (in d+ 1
dimensions, because of the “Trotter direc-
tion” corresponding to the imaginary time
direction of the path integral).
2.8.3
A Classical Application: the Momentum
Distribution of Fluid 4He
We now consider the dynamic structure
factor S(k, 𝜔), which is the Fourier trans-
form of a time-displaced pair correlation
function of the density at a point r1 at
time t1 and the density at point r2 at time
t2 (ℏk being the momentum transfer and
ℏ𝜔the energy transfer of an inelastic
scattering experiment by which one can
measure S(k, 𝜔)). In the “impulse approx-
imation,” S(k, 𝜔) can be related to the
Fourier transform of the single-particle
density matrix 𝜌1(r), which for 4He can
be written in terms of the wave function
𝜓(r) as 𝜌1 (r) = ⟨𝜓+ (r′ + r) 𝜓(r)
⟩. This
relation is
S(k, 𝜔) ∝J(Y) = 1
𝜋∫
∞
0
𝜌1(r)cos(Yr)dr,
where Y ≡m(𝜔−k2/2m)/k [64]. As S(k, 𝜔)
has been measured via neutron scatter-
ing [65], a comparison between experiment
and simulation can be performed without
adjustable parameters (Figure 2.2). Thus,
the PIMC method yields accurate data in
good agreement with experiment.
The studies of 4He have also yielded qual-
itative evidence for superﬂuidity [53]. For
a quantitative analysis of the 𝜆transition,
a careful assessment of ﬁnite-size eﬀects
(Figure 2.1) is needed because one works
with very small particle numbers (of the
order of 102 4He atoms only). This has not
been possible so far.
2.8.4
A Few Qualitative Comments on Fermion
Problems
Particles obeying Fermi–Dirac statistics
(such as electrons or 3He, for instance, see
Ceperley [54]) pose particular challenges
to Monte-Carlo simulation. If one tries to
solve the Schrödinger equation of a many-
body system by Monte-Carlo methods,
one exploits its analogy with a diﬀusion
equation [48, 49]. Diﬀusion processes cor-
respond to RWs and are hence accessible to
Monte-Carlo simulation. However, while
the diﬀusion equation (for one particle)

60
2 Monte-Carlo Methods
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
−0.05
−1
−2
−3
−4
1
2
3
4
J(Y)
0
0
Y (A−1)
Figure 2.2
The measured momentum distribution J(Y)
of 4He at T = 3.3 K (circles, from [65]) compared with the
PIMC result of Ceperley and Pollock [66] (solid line). (Source:
From Schmidt and Ceperley [67].)
considers the probability P(r, t) that a
particle starting at time t = 0 at the ori-
gin has reached the position r at time t,
the wave function 𝜓in the Schrödinger
equation is not positive deﬁnite. This fact
creates severe problems for wave functions
of many-fermion systems, because these
wave functions must be antisymmetric,
and the “nodal surface” in conﬁguration
space (where 𝜓changes sign) is unknown.
Formally,
the
diﬃculty
of
applying
importance-sampling techniques to dis-
tributions 𝜌(r) that are not always positive
can be overcome by splitting 𝜌(r) into its
sign, s = sign(𝜌), and its absolute value,
𝜌= s|𝜌|, and one can use ̃𝜌(r) =
|𝜌(r)|
∫|𝜌(r)|d3r
as a probability density for importance
sampling, and absorb the sign of 𝜌(r) in the
quantity to be measured. Symbolically, the
average of an observable A is obtained as
⟨A⟩=
∫d3rA(r)s(r) ̃𝜌(r)
∫s(r) ̃𝜌(r)d3r
=
⟨As⟩̃𝜌
⟨s⟩̃p
,
where ⟨…⟩̃𝜌means averaging with ̃𝜌as
weight function. However, as is not unex-
pected, using ̃𝜌one predominantly samples
unimportant regions in phase space; there-
fore, in sampling the sign ⟨s⟩̃𝜌, one has large
cancellations from regions where the sign
is negative, and, for N degrees of freedom,
one gets ⟨s⟩̃𝜌∝exp(−const. × N). This dif-
ﬁculty is known as the minus-sign problem
and still hampers applications to fermion
problems signiﬁcantly.
Sometimes it is possible to start with
a trial wave function where nodes are a
reasonable ﬁrst approximation to the actual
nodes, and, starting with the population

2.9 Lattice Gauge Theory
61
of RWs from this ﬁxed-node approxima-
tion given by the trial function, one now
admits walks that cross this nodal surface
and sample the sign as indicated above. In
this way, it has been possible to estimate the
exchange-correlation energy of the homo-
geneous electron gas [68] over a wide range
of densities very well.
2.9
Lattice Gauge Theory
Monte-Carlo simulation has become the
primary tool for nonperturbative quantum
chromodynamics, the ﬁeld theory of quarks
and hadrons and other elementary particles
(e.g., [58, 69]). In this section, we ﬁrst stress
the basic problem, to make the analogy with
the calculations of statistical mechanics
clear. Then we very brieﬂy highlight some
of the results that have been obtained so far.
2.9.1
Some Basic Ideas of Lattice Gauge Theory
The theory of elementary particles is a ﬁeld
theory of gauge ﬁelds and matter ﬁelds.
Choice of a lattice is useful to provide a cut-
oﬀthat removes the ultraviolet divergences
that would otherwise occur in these quan-
tum ﬁeld theories. The ﬁrst step, hence, is
the appropriate translation from the four-
dimensional continuum (3 space + 1 time
dimensions) to the lattice.
The generating functional (analogous to
the partition function in statistical mechan-
ics) is
Z=∫DAD𝜓D𝜓exp [−Sg(A, 𝜓, 𝜓)] , (2.38)
where A represents the gauge ﬁelds, 𝜓and
𝜓represent the (fermionic) matter ﬁeld, Sg
is the action of the theory (containing a
coupling constant g, which corresponds to
inverse temperature in statistical mechan-
ics as const./g2 →1/kBT), and the sym-
bols ∫D stand for functional integration.
The action of the gauge ﬁeld itself is, using
the summation convention that indices that
appear twice are summed over,
SG = 1
4 ∫d4xF𝛼
𝜇v(x)F𝜇v
𝛼(x),
(2.39)
F𝛼
𝜇v being the ﬁelds that derive from the
vector potential A𝛼
𝜇(x). These are
F𝛼
𝜇v(x)=𝜕𝜇A𝛼
v(x) −𝜕vA𝛼
𝜇(x)+gf 𝛼
𝛽𝛾A𝛽
𝜇(x)A𝛾
v(x),
(2.40)
f 𝛼
𝛽𝛾being the structure constants of the
gauge group, and g a coupling constant.
The variables that one then introduces
are elements U𝜇(x) of the gauge group G,
which are associated with the links of the
four-dimensional lattice, connecting x and
a nearest-neighbor point x+ 𝜇:
U𝜇(x) = exp
[
igaT𝛼A𝛼
𝜇(x)
]
,
[Um(x + m)]† = Um(x),
(2.41)
where a is the lattice spacing and T𝛼a group
generator. Here U+ denotes the Hermitean
conjugate of U. Wilson [70] invented a lat-
tice action that reduces in the continuum
limit to (2.39), namely,
SU
kBT = 1
g2
∑
n
∑
𝜇>v
Re TrU𝜇(n) × Uv(n + 𝜇)
U†
𝜇(n + v)U†
v (n),
(2.42)
where the links in (2.42) form a closed con-
tour along an elementary plaquette of the
lattice.
Using (2.42) in (2.38), which amounts
to the study of a “pure” gauge theory (no
matter ﬁelds), we recognize that the prob-
lem is equivalent to a statistical mechan-
ics problem (such as spins on a lattice),

62
2 Monte-Carlo Methods
the diﬀerence being that now the dynami-
cal variables are the gauge group elements
U𝜇(n). Thus importance-sampling Monte-
Carlo algorithms can be put to work, just
as in statistical mechanics.
To include matter ﬁelds, one starts from
a partition function of the form
Z = ∫DUD𝜓D𝜓exp
{
−SU
kBT +
nf
∑
i=1
𝜓M𝜓
}
= ∫DU(det M)nf exp
(
−SU
kBT
)
,
(2.43)
where we have assumed fermions with nf
degenerate “ﬂavors.” It has also been indi-
cated that the fermion ﬁelds can be inte-
grated out analytically, but the price is that
one has to deal with the “fermion determi-
nant” of the matrix M. In principle, for any
change of the U’s this determinant needs to
be recomputed; together with the fact that
one needs to work on rather large lattices in
four dimensions, in order to reproduce the
continuum limit, this problem is responsi-
ble for the huge requirement of computing
resources in this ﬁeld.
It is clear that lattice gauge theory cannot
be explained in depth on two pages – we
only intend to give a vague idea of what
these calculations are about to a reader who
is not familiar with this subject.
2.9.2
A Famous Application
Among the many Monte-Carlo studies of
various problems (which include problems
in cosmology, such as the phase transition
from the quark-gluon plasma to hadronic
matter in the early universe), we focus here
on the problem of predicting the masses of
elementary particles. Butler et al. [71] used
a new parallel supercomputer with 480 pro-
cessors (“GF11”) exclusively for one year
to run lattice sizes ranging from 83 × 32 to
243 × 32, 243 × 36, and 30 × 322 × 40. Their
program executed at a speed of more than
5 Gﬂops (Giga ﬂoating point operations
per second), and the rather good statistics
reached allowed a meaningful elimination
of ﬁnite-size eﬀects by an extrapolation to
the inﬁnite-volume limit. This problem is
important, because the wave function of
a hadron is spread out over many lattice
sites.
Even with this impressive eﬀort, several
approximations were necessary.
The fermion determinant mentioned
above is neglected (this is called quenched
approximation). One cannot work at the
(physically
relevant)
very
small
quark
mass mq, but rather data on the hadron
masses were taken for a range of quark
masses and extrapolated (Figure 2.3). After
a double extrapolation (mq →0, lattice
spacing at ﬁxed volume →0), one obtained
mass ratios that are in satisfactory agree-
ment with experiment. For example, for
the nucleon the mass ratio for the ﬁnite
volume is mN/mæ = 1.285 ± 0.070, extrap-
olated to inﬁnite volume 1.219 ± 0.105,
the experimental value being 1.222 (all
masses in units of the mass mæ of a rho
meson).
Fifteen years later [72], it became possi-
ble to avoid the “quenched approximation”
and other limitations of the early work and
calculate the light hadron masses, obtain-
ing very good agreement with experiment,
using supercomputers that were a million
times faster.
2.10
Selected Applications in Classical
Statistical Mechanics of Condensed Matter
In this section, we mention a few applica-
tions, to give the ﬂavor of the type of work
that is done and the kind of questions that

2.10 Selected Applications in Classical Statistical Mechanics of Condensed Matter
63
0.0
0.5
1.0
1.5
2.0
2.5
3.0
2.0
4.0
6.0
0
0
1
2
3
π
ρ
Δ
N
[mρ,mN,mΔ] / mρ(mn)
mπ
2 / mρ(mn)2
mq / ms
Figure 2.3
For a 30 × 322× 40 lattice at (kBT)−1 =
6.17, m2
ß, mæ, mN, and m€ in units of the physical rho
meson mass mæ(mn), as functions of the quark mass mq
in units of the strange quark mass ms. Particles studied are
pion, rho meson, nucleon, and delta baryon, respectively.
(Source: From Butler et al. [71].)
are answered by Monte-Carlo simulations.
More extensive reviews can be found in the
literature [29, 36, 73, 74].
2.10.1
Metallurgy and Materials Science
A widespread application of Monte-Carlo
simulation in this area is the study of
order-disorder phenomena in alloys: One
tests analytical approximations to calculate
phase diagrams, such as the cluster varia-
tion (CV) method, and one tests to what
extent a simple model can describe the
properties of complicated materials.
An example (Figure 2.4) shows the order
parameter for long-range order (LRO)
and short-range order (SRO, for nearest
neighbors) as function of temperature for
a model of Cu3Au alloys on the fcc lattice.
An Ising model with antiferromagnetic
interactions between nearest neighbors is
studied, and the Monte-Carlo data (ﬁlled
symbols and symbols with crosses) are
compared to CV calculations (broken
curve) and other analytical calculations
(dash-dotted curve) and to experiment
(open circles). The simulation shows that
the analytical approximations describe the
ordering of the model only qualitatively.
Of course, there is no perfect agreement
with the experiment either; this is to be
expected, of course, because in real alloys
the interaction range is considerably larger
than just extending to nearest neighbors
only.
2.10.2
Polymer Science
One can study phase transitions not only
for models of magnets or alloys, but also
for complex systems such as mixtures
of ﬂexible polymers. A question heavily
debated in the literature is the dependence
of the critical temperature of unmixing
of a symmetric polymer mixture (both
constituents have the same degree of

64
2 Monte-Carlo Methods
1.0
0.5
0.6
0.8
1.0
1.2
0.0
T / Tc
ψ
SRO
LRO
CV
Kittler and Falicov
Cu3Au
A3B
Cu3Au
MC
Figure 2.4
Long-range order parameter (LRO)
and absolute value of nearest-neighbor short-
range order parameter (SRO) plotted versus
temperature T (in units of the temperature Tc
where the ﬁrst-order transition occurs) for a
nearest-neighbor model of binary alloys on
the face-centered cubic lattice with A3B struc-
ture. Open circles: experimental data for Cu3Au;
broken and dash-dotted curves: results of ana-
lytical theories. Full dots: Monte-Carlo results
obtained in the semi-grand canonical ensemble
(chemical potential diﬀerence between A and B
atoms treated as independent variable); circles
with crosses: values obtained from a canonical
ensemble simulation (concentration of B atoms
ﬁxed at 25%). (Source: From Binder et al. [75].)
polymerization NA = NB = N) on chain
length N. The classical Flory–Huggins
theory predicted Tc ∝N, while an inte-
gral equation theory predicted Tc ∝
√
N
[76]. This law would lead in the plot of
Figure 2.5 to a straight line through the
origin. Obviously, the data seem to rule
out this behavior, and are rather quali-
tatively consistent with Flory–Huggins
theory
(though
the
latter
signiﬁcantly
overestimates the prefactor in the relation
Tc ∝N).
Polymer physics provides examples for
many questions where simulations could
contribute signiﬁcantly to provide a better
understanding. Figure 2.6 provides one
more example [78]. The problem is to
provide truly microscopic evidence for
the reptation concept [79]. This concept
implies that, as a result of “entanglements”
between chains in a dense melt, each chain
moves snakelike along its own contour.
This behavior leads to a special behavior of
mean square displacements: After a charac-
teristic time 𝜏e, one should see a crossover
from a law g1(t) ≡⟨[ri(t) −ri(0)]2⟩∝t1∕2
(Rouse model) to a law g1(t) ∝t1∕4, and, at a
still later time (𝜏R), one should see another
crossover to g1(t) ∝t1∕2 again. At the same
time, the center of gravity displacement
should also show an intermediate regime
of anomalously slow diﬀusion, g3(t) ∝t1∕2.
Figure 2.6 provides qualitative evidence for
these predictions, although the eﬀective
exponents indicated do not quite have the
expected values.
While this is an example where dynamic
Monte-Carlo simulations are used to check
theories and pose further theoretical ques-
tions, one can also compare to experiment
if one uses data in suitably normalized
form. In Figure 2.7, the diﬀusion constant
D of the chains is normalized by its value
in the Rouse regime (limit for small N)
and plotted versus N/Ne where the char-
acteristic “entanglement chain length” Ne
is extracted from 𝜏e shown in Figure 2.6

2.10 Selected Applications in Classical Statistical Mechanics of Condensed Matter
65
1.0
1.5
2.0
2.5
3.0
0
0
0.1
0.15
0.2
N−1/2
0.3
0.25
0.05
0.5
kBTc / Nɛ
256 128
32
16
64
Figure 2.5
Normalized critical temperature
kBTc∕N𝜀of a symmetric polymer mixture (N is
the chain length, 𝜀is the energy parameter
describing the repulsive interaction between
A–B pairs of monomers) plotted versus N−1∕2.
Data are results of simulations for the bond-
ﬂuctuation model, 16 ≤N ≤256. The data
are consistent with an asymptotic extrapo-
lation kBTc∕𝜀≈2.15 N, while Flory–Huggins
theory (in the present units) would yield
kBTc∕𝜀≈7N, and the integral equation the-
ory kBTc
𝜖
∝
√
N. (Source: From Deutsch and
Binder [77].)
100 000
10 000 000
10 00 000
t (MCS)
g3 (t)
g1 (t)
t0.8
τR
τe
t0.3
t0.9
t0.62
t0.5
t0.5
1000
500
200
100
50
20
10
5
Figure 2.6
Log-log plot of the mean square
displacements of inner monomers [g1(t)] and
of the center of gravity of the chain [g3(t)] ver-
sus time t (measured in units of Monte-Carlo
steps, while lengths are measured in units of
the lattice spacing). Straight lines show various
power laws as indicated; various characteristic
times are indicated by arrows (see text). Data
refer to the bond-ﬂuctuation model on the sim-
ple cubic lattice, for an athermal model of a
polymer melt with chain length N = 200 and
a volume fraction 𝜙= 0.5 of occupied lattice
sites. (Source: From Paul et al. [78].)

66
2 Monte-Carlo Methods
1
0.1
0.1
0.2
0.5
10
1.0
N / Ne
Ne
D / DRouse
Symbol
Method
35
96
30
40
MD - simulation
PE NMR data
This work Ø = 0.5
This work Ø = 0.4
Figure 2.7
Log-log plot of the self-diﬀusion
constant D of polymer chains, normalized by
the Rouse diﬀusivity, versus N/Ne (Ne is the
entanglement chain length, estimated inde-
pendently, and indicated in the inset). Circles,
from Monte-Carlo simulations of Paul et al. [80];
squares, from molecular dynamics simulations
[81]; triangles, experimental data [82]. (Source:
From [80].)
1500
1000
500
0
0.90
0.95
1.00
1.05
1.10
q / π
20
40
63
100
140
200
300
400
S (q, t)
Figure 2.8
Structure factor S(q, t) versus wavevector q
at various times t (in units MCS per lattice site), after
the system was started in a completely random conﬁg-
uration. Temperature (measured in units of the nearest-
neighbor repulsion Wnn) is 1.33 (Tc ≈2.07 in these
units), and coverage 𝜃= 1∕2. (Source: From Sadiq and
Binder [83].)

2.11 Concluding Remarks
67
(see [78, 80], for details). The Monte-Carlo
data presented in this scaled form agree
with results from both a MD simulation
[81]
and
experiment
on
polyethylene
(PE) [82].
This example also shows that, for slow
diﬀusive motions, Monte-Carlo simula-
tion is competitive to MD, although it
does not describe the fast atomic motions
realistically.
2.10.3
Surface Physics
Our last example considers phenomena
far from thermal equilibrium. Studying
the ordering behavior of superstructures,
we treat the problem where initially the
adatoms are adsorbed at random, and one
gradually follows the formation of ordered
domains out of initially disordered conﬁg-
urations. In a scattering experiment, one
sees this by the gradual growth of a peak
at the Bragg position qB. Figure 2.8 shows
a simulation for the case of the (2 × 1)
structure on the square lattice, where the
Bragg position is at the Brillouin-zone
boundary (qB = 𝜋if lengths are measured
in units of the lattice spacing). Here a lattice
gas with repulsive interactions between
nearest and next-nearest neighbors (of
equal strength) was used [83], using a
single-spin-ﬂip kinetics (if the lattice gas is
translated to an Ising spin model). This is
appropriate for a description of a mono-
layer in equilibrium with surrounding gas,
the random “spin ﬂips” then correspond to
random evaporation-condensation events.
Figure 2.9 presents evidence that these data
on the kinetics of ordering satisfy a scaling
hypothesis, namely,
S(q, t) = [L(t)]2̃S(|q −qB|L(t)),
(2.44)
where ̃S is a scaling function. This hypoth-
esis, (2.44), was ﬁrst proposed using sim-
ulations, and later it was established to
describe experimental data as well.
2.11
Concluding Remarks
In this chapter, the basic features of the
most widely used numerical techniques
that fall into the category of Monte-Carlo
calculations were described. There is a
vast literature on the subject; the author
estimates the number of papers using
Monte-Carlo methods in condensed mat-
ter physics of the order of 105, in lattice
gauge theory of the order of 104. Thus many
important variants of algorithms could not
be treated here and interesting applications
(e.g., the study of neural-network models)
were completely omitted.
There
also
exist
other
techniques
for the numerical simulation of com-
plex systems, which sometimes are an
alternative
approach
to
Monte-Carlo
simulation. The MD method (numerical
integration of Newton’s equations) has
already been mentioned in the text, and
there exist combinations of both meth-
ods
(“hybrid
Monte-Carlo,”
“Brownian
dynamics,” etc.). A combination of MD
with the local-density approximation of
quantum mechanics is the basis of the
Car–Parrinello method.
Problems
such
as
that
shown
in
Figures 2.8 and 2.9 can also be formu-
lated in terms of numerically solving
appropriate diﬀerential equations, which
may in turn even be discretized to cellular
automata. When planning a Monte-Carlo
simulation, hence, some thought to the
question “When which method?” should
be given.

68
2 Monte-Carlo Methods
−0.4
−0.2
0.0
+0.2
+0.4
(q/π − 1) / σ(t)
S (q, t) / S (π, t)
1.0
0.75
0.50
0.25
0.0
Symbol
t
400
300
200
140
100
Figure 2.9
Structure factor of Figure 2.8 plotted in scaled
form, normalizing S(q, t) by its peak value S(𝜋, t) and
normalizing q∕𝜋−1 by the half-width 𝜎(t) = L−1(t), where
L(t) thus deﬁned is the characteristic domain size. (Source:
From Sadiq and Binder [83].)
Glossary
Critical slowing down: Divergence of the
relaxation time of dynamic models of sta-
tistical mechanics at a second-order phase
transition (critical point).
Detailed balance principle: Relation link-
ing the transition probability for a move and
the transition probability for the inverse
move to the ratio of the probability for
the occurrence of these states in thermal
equilibrium. This condition is suﬃcient for
a Markov process to tend toward thermal
equilibrium.
Ergodicity: Property that ensures equal-
ity of statistical ensemble averages (such as
the “canonic ensemble”) and time averages
along the trajectory of the system through
phase space.
Finite-size scaling: Theory that describes
the ﬁnite-size-induced rounding of singu-
larities that would occur at phase transi-
tions in the thermodynamic limit.
Heat-bath method: Choice of transition
probability where the probability to “draw”
a trial value for a degree of freedom does
not depend on its previous value.
Importance-sampling:
Monte-Carlo
method that chooses the states that are gen-
erated according to the desired probability
distribution. For example, for statistical
mechanics, states are chosen with weights
proportional to the Boltzmann factor.
Lattice gauge theory: Field theory of
quarks and gluons in which space and time

References
69
are discretized into a four-dimensional lat-
tice, gauge ﬁeld variables being associated
to the links of the lattice.
Master equation: Rate equation describ-
ing the “time” evolution of the probabil-
ity that a state occurs as a function of
a “time” coordinate labeling the sequence
of states (in the context of importance-
sampling Monte-Carlo methods).
Molecular dynamics method: Simulation
method for interacting many-body systems
based on numerical integration of the New-
tonian equations of motion.
Monte-Carlo step: Unit of (pseudo) time
in (dynamically interpreted) importance
sampling where, on the average, each
degree of freedom in the system gets one
chance to be changed (or “updated”).
Random-number
generator
(RNG):
Computer subroutine to produce pseudo-
random numbers that are approximately
not correlated with each other and approx-
imately
uniformly
distributed
in
the
interval from zero to one. RNGs typically
are strictly periodic, but the period is large
enough that, for practical applications, this
periodicity does not matter.
Simple sampling: Monte-Carlo method
that
chooses
states
uniformly
and
at
random from the available phase space.
Transition probability: Probability that
controls the move from one state to the
next one in a Monte-Carlo process.
References
1. Hammersley, J. M., and Handscomb, D. C.
(1964) Monte-Carlo Methods, Chapman &
Hall, London.
2. Bruns, W., Motoc, I., and O’Driscoll, K. F.
(1981) Monte-Carlo Applications in Polymer
Science, Springer, Berlin.
3. Compagner, A. (1991) Am. J. Phys. 59,
700–705.
4. Knuth, D. (1969) The Art of Computer
Programming, Vol. 2, Addison-Wesley,
Reading, MA.
5. James, F. (1990) Comput. Phys. Commun. 60,
329–344.
6. Mascagni, M. and Srinivasan, A. (2000)
ACM Trans. Math. Software 26, 436–461.
7. Marsaglia, G. A. (1985) in: L. Billard Ed.
Computer Science and Statistics: The
Interface, Chapter 1, Elsevier, Amsterdam.
8. Compagner, A. and Hoogland, A. (1987) J.
Comput. Phys. 71, 391–428.
9. Coddington, P. D. (1994) Int. J. Mod. Phys.
C5, 547.
10. Marsaglia, G. A., Narasumhan, B., and
Zaman, A. (1990) Comput. Phys. Commun.
60, 345–349.
11. Lehmer, D. H. (1951) Proceedings of the 2nd
Symposium on Large-Scale Digital
Computing Machinery, Harvard University,
Cambridge, pp. 142–145.
12. Marsaglia, G. A. (1986), Proc. Natl. Acad.
Sci. U.S.A. 61, 25–28.
13. Tausworthe, R. C. (1965) Math. Comput. 19,
201–208.
14. Kirkpatrick, S. and Stoll, E. (1981) J. Comput.
Phys. 40, 517–526.
15. Ahrens, J. H. and Dieter, U. (1979) Pseudo
Random Numbers, John Wiley & Sons, Inc.,
New York.
16. Koonin, S. E. (1981) Computational Physics,
Benjamin, Reading, MA.
17. Gould, H. and Tobochnik, J. (1988) An
Introduction to Computer Simulation
Methods/Applications to Physical Systems,
Parts 1 and 2, Addison-Wesley, Reading,
MA.
18. Stauﬀer, D. and Aharony, A. (1994) An
Introduction to Percolation Theory, Taylor &
Francis, London.
19. Herrmann, H. J. (1986) Phys. Rep. 136,
153–227.
20. Kremer, K. and Binder, K. (1988) Comput.
Phys. Rep. 7, 259–310.
21. Milchev, A., Binder, K., and Heermann,
D. W. (1986) Z. Phys. B 63, 521–535.
22. Rosenbluth, M. N. and Rosenbluth, A. W.
(1955) J. Chem. Phys. 23, 356–362.
23. Grassberger, P. (1997) Phys. Rev. E56,
3682–3693.
24. Hsu, H.-P., and Grassberger, P. (2011) J. Stat.
Phys. 144, 597–637.
25. Vicsek, T. (1989) Fractal Growth
Phenomena, World Scientiﬁc, Singapore.

70
2 Monte-Carlo Methods
26. Meakin, P. (1998) Fractals, Scaling and
Growth Far From Equilibrium. Cambridge
University Press, Cambridge.
27. Tolman, S. and Meakin, P. (1989) Phys. Rev.
A40, 428–437.
28. Metropolis, N., Rosenbluth, A. W.,
Rosenbluth, M. N., Teller, A.M., and Teller,
E. (1953) J. Chem. Phys. 21, 1087–1092.
29. Binder, K. (1976) in: C. Domb, M.S. Green
Eds. Phase Transitions and Critical
Phenomena, Vol. 5b, p. 1, Academic Press,
New York.
30. Binder, K. and Heermann, D. W. (2010)
Monte-Carlo Simulation in Statistical
Physics: An Introduction, 5th edn, Springer,
Berlin.
31. Kalos, M. H. and Whitlock, P. A. (1986)
Monte-Carlo Methods, Vol. 1, John Wiley &
Sons, Inc., New York.
32. Landau, D. P. (1992) in: K. Binder Ed. The
Monte-Carlo Method in Condensed Matter
Physics, Chapter 2, Springer, Berlin.
33. Swendsen, R. H., Wang, J. S., and Ferrenberg,
A. M. (1992), in: K. Binder Ed., The
Monte-Carlo Method in Condensed Matter
Physics, Chapter 4, Springer, Berlin.
34. Berg, B. A. (2004) Markov Chain
Monte-Carlo Simulations and Their
Statistical Analysis. World Scientiﬁc,
Singapore.
35. Rapaport, D. C. (2004) The Art of Molecular
Dynamics Simulation, 2nd edn, Cambridge
University Press, Cambridge.
36. Landau, D. P. and Binder, K. (2009) A Guide
to Monte-Carlo Simulation in Statistical
Physics, 3rd edn., Cambridge University
Press Cambridge.
37. Panagiotopoulos, A. Z. (1992) Mol. Simul. 9,
1–23.
38. Frenkel, D. and Smit, B. (2002)
Understanding Molecular Simulation: From
Algorithms to Applications, 2nd edn.,
Academic Press, San Diego, CA.
39. Barber, M. N. (1983), in: C. Domb, and J. L.
Lebowitz Eds, Phase Transitions and Critical
Phenomena, Chapter 2, Vol. 8, Academic
Press, New York.
40. Binder, K. (1987) Ferroelectrics 73, 43–67.
41. Binder K. (1997) Rep. Progr. Phys. 60,
487–559.
42. Privman, V. Ed. (1990) Finite Size Scaling
and Numerical Simulation of Statistical
Systems, World Scientiﬁc, Singapore.
43. Stanley, H. E. (1971) An Introduction to
Phase Transitions and Critical Phenomena,
Oxford University Press, Oxford.
44. Wang, F. and Landau, D. P. (2001) Phys. Rev.
E 64, 056101.
45. Binder, K., Block, B., Das, S. K., Virnau, P.,
and Winter, D. (2011) J. Stat. Phys. 144,
690–729.
46. Grzelak, E. M. and Errington, J. R. (2008) J.
Chem. Phys. 128, 014710.
47. Bolhuis, P. G., Chandler, D., Dellago, C., and
Geissler, P. L. (2002) Ann. Rev. Phys. Chem.
53, 291.
48. Ceperley, D. M. and Kalos, M. H. (1979) in:
K. Binder Ed. Monte-Carlo Methods in
Statistical Physics, Springer, Berlin, pp.
145–194.
49. Kalos, M. H. Ed. (1984) Monte-Carlo
Methods in Quantum Problems, Reidel,
Dordrecht.
50. Berne, B. J. and Thirumalai, D. (1986) Annu.
Rev. Phys. Chem. 37, 401.
51. Suzuki, M. Ed. (1986) Quantum
Monte-Carlo Methods, Springer, Berlin.
52. Schmidt, K. E. and Ceperley, D. M. (1992) in
K. Binder Ed., The Monte-Carlo Methods in
Condensed Matter Physics, Springer, Berlin,
pp. 105–148.
53. Ceperley, D. M. (1995) Rev. Mod. Phys. 67,
279–355.
54. Ceperley, D. M. (1996) in K. Binder and G.
Ciccotti Eds., Monte-Carlo and Molecular
Dynamics of Condensed Matter Systems,
Societa Italiana di Fisica, Bologna, pp.
443–482.
55. Rothman, S. ed. (2002) Recent Advances in
Quantum Monte-Carlo Methods II, World
Scientiﬁc, Singapore.
56. Trebst, S. and Troyer M. (2006) In: M.
Ferrario, G. Ciccotti, and K. Binder eds.
Computer Simulations in Condensed Matter:
From Materials to Chemical Biology,
Vol. 1,Springer, Berlin, pp. 591–640.
57. Carlson, J. (1988) Phys. Rev. C 38,
1879–1885.
58. De Grand, T. (1992) in: H. Gausterer and C.
B. Lang Eds., Computational Methods in
Field Theory, Springer, Berlin, pp. 159–203.
59. Frick, M., Pattnaik, P. C., Morgenstern, I.,
Newns, D. M., von der Linden, W. (1990)
Phys. Rev. B 42, 2665–2668.
60. Reger, J. D. and Young, A. P. (1988) Phys. Rev.
B 37, 5978–5981.

Further Reading
71
61. Sachdev, S. (2000) Quantum Phase
Transitions, Cambridge University Press,
Cambridge.
62. Marx, D., Opitz, O., Nielaba, P., and Binder,
K. (1993) Phys. Rev. Lett. 70, 2908–2911.
63. Gillan, M. J. and Christodoulos, F. (1993) Int.
J. Mod. Phys. C4, 287–297.
64. West, G. B. (1975), Phys. Rep. 18C, 263–323.
65. Sokol, P. E., Sosnick, T. R., and Snow, W. M.
(1989), in R. E. Silver and P. E. Sokol Eds.
Momentum Distributions, Plenum Press,
New York.
66. Ceperley and Pollock (1987) Can. J. Phy. 65,
1416–1423.
67. Schmidt, K. E. and Ceperley, D. M. (1992),
in: K. Binder Ed. The Monte-Carlo Method in
Condensed Matter Physics, pp. 203–248,
Springer, Berlin.
68. Ceperley, D. M., Alder, B. J. (1980) Phys. Rev.
Lett. 45, 566–569.
69. Rebbi, C. (1984), in: K. Binder Ed.
Application of the Monte-Carlo Method in
Statistical Physics, Springer, Berlin, p. 277.
70. Wilson, K. (1974) Phys. Rev. D10,
2445–2453.
71. Butler, F., Chen, H., Sexton, J., Vaccarino, A.,
and Weingarten, D. (1993) Phys. Rev. Lett.
70, 2849–2852.
72. Dürr, S. et al. (2008) Science 322,
1224–1227.
73. Binder, K. Ed. (1979) Monte-Carlo Methods
in Statistical Physics, Springer, Berlin.
74. Binder, K. Ed. (1995) The Monte-Carlo
Method in Condensed Matter Physics,
Springer, Berlin.
75. Binder, K., Lebowitz, J. L., Phani, M. K., and
Kalos, M. H. (1981) Acta Metall. 29,
1655–1665.
76. Schweizer, K. S. and Curro, J. G. (1990)
Chem. Phys. 149, 105–127.
77. Deutsch, H. P. and Binder, K. (1992)
Macromolecules 25, 6214–6230.
78. Paul, W., Binder, K., Heermann, D. W., and
Kremer, K. (1991) J. Chem. Phys. 95,
7726–7740.
79. Doi, M. and Edwards, S. F. (1986) Theory of
Polymer Dynamics, Clarendon Press,
Oxford.
80. Paul, W., Binder, K., Heermann, D. W., and
Kremer, K. (1991) J. Phys. (Paris) 111,
37–60.
81. Kremer, K. and Grest, G. S. (1990) J. Chem.
Phys. 92, 5057–5086.
82. Pearson, D. S., Verstrate, G., von Meerwall,
E., and Schilling, F. C. (1987)
Macromolecules 20, 113–1139.
83. Sadiq, A. and Binder, K. (1984) J. Stat. Phys.
35, 517–585.
Further Reading
A textbook describing for the beginner how
to learn to write Monte-Carlo programs
and to analyze the output generated by
them has been written by Binder, K., Heer-
mann, D.W. (2010), Monte-Carlo Simula-
tion in Statistical Physics: An Introduction,
5th edn, Berlin: Springer. This book empha-
sizes applications of statistical mechanics
such as random walks, percolation, and the
Ising model.
More extensive and systematic textbooks
have been written by D. Frenkel and B. Smit
(2002), Understanding Molecular Simula-
tion: From Algorithms to Applications, San
Diego, CA: Academic Press, and by D. P.
Landau and K. Binder A Guide to Monte-
Carlo Simulations in Statistical Physics,
Cambridge: Cambridge University Press
Practical Considerations are emphasized
in B. A. Berg (2004) Markov Chain Monte-
Carlo Simulations and Their Statistical
Analysis, Singapore: World Scientiﬁc.
A useful book that gives much weight to
applications outside of statistical mechan-
ics is Kalos, M. H., Whitlock, P. A. (1986),
Monte-Carlo Methods, vol. 1, New York:
John Wiley & Sons, Inc.
A more general but pedagogic introduc-
tion to computer simulation is presented
in Gould, H., Tobochnik, J. (1988), An
Introduction
to
Computer
Simulation
Methods/Applications to Physical Systems,
Parts 1 and 2, Reading, MA: Addison-
Wesley.


73
3
Stochastic Diﬀerential Equations
Gabriel J. Lord
3.1
Introduction
The consequence of inclusion of random
eﬀects in ordinary diﬀerential equations
(ODEs) falls in to two broad classes – one
in which the solution has a smooth path
and one where the path is not smooth
(i.e., it is not diﬀerentiable). If the random
eﬀect is included as a random choice of
a parameter or as a random initial condi-
tion, u0, then we have random diﬀerentiable
equations. Each realization (i.e., each pick
of the parameter from a random set) can be
solved using standard deterministic meth-
ods (see Chapter 14). If we need to solve
a large number of times to get averaged
properties using Monte Carlo techniques
(see Chapter 2), then eﬃcient schemes are
required.
If the forcing is rough, then the solu-
tions are not diﬀerentiable and new tech-
niques are required (in fact, a new calculus
is required). Here we concentrate on diﬀer-
ential equations that are coupled to some
random forcing 𝜁(t) that is Gaussian white
noise, so the random forces have mean zero
and are uncorrelated at diﬀerent times t.
When 𝜁(t) is white, it can be characterized
as the derivative of Brownian motion W(t),
so that “𝜁(t) = dW∕dt.” Ordinary diﬀer-
ential equations with this sort of forcing
are called stochastic diﬀerential equations
(SDEs).
Example 3.1 [Ornstein–Uhlenbeck (OU)
process] Consider a particle of unit mass
moving with momentum p(t) at time t in
a liquid. The dynamics of the particle can
be modeled by a dissipative force −𝜆p(t),
where 𝜆> 0 is known as the dissipation
constant. In the absence of any external
forcing, we have
dp
dt = −𝜆p,
given p(0) = p0 ∈ℝ
(3.1)
and the particle will converge to the rest
state p = 0 as t →∞. This is not physi-
cal as the particle is subject to irregular
bombardment by molecules in the liquid
which can be modeled as a ﬂuctuating force
𝜎𝜁(t), where 𝜁(t) is white noise and 𝜎> 0
is called the diﬀusion constant. Newton’s
second law of motion gives the acceleration
dp∕dt as the sum of the two forces. We have
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

74
3 Stochastic Diﬀerential Equations
dp
dt = −𝜆p + 𝜎𝜁(t),
(3.2)
which is a linear SDE with additive noise
𝜁(t) = dW∕dt. It is also written as
dp = −𝜆p dt + 𝜎dW(t).
(3.3)
The solution p(t) is known as the Orn-
stein–Uhlenbeck (OU) process. We will see
that dW∕dt does not strictly speaking make
sense as a function and we need to inter-
pret the SDE as an integral equation. In
Figure 3.1, we show two sample realizations
of (3.3) computed numerically and we also
plot, for reference, the solution of the cor-
responding deterministic equation (3.1), all
with the same initial data.
Note that (3.2) can also be interpreted as
a model for thermal noise in an electrical
resistor.
Our aim is to make sense of equations
such as (3.3) and to introduce some of
the techniques for looking at them. Con-
sider the general ODE du∕dt = f (u) with
initial data u(0) = u0 and recall that it can
be integrated to get an equivalent integral
equation
u(t) = u(0) + ∫
t
0
f (s, u(s))ds.
When we include the eﬀects of a stochastic
forcing term, 𝜁(t), we have the SDE
du
dt = f (u) + g(u)𝜁(t).
(3.4)
When “𝜁(t) = dW∕dt,” where W is Brown-
ian motion, we will understand this as the
integral equation
u(t) = u0+∫
t
0
f (u(s))ds + ∫
t
0
g(u(s))dW(s).
(3.5)
Equation (3.4) and the integral equation
above are often written in shorthand as the
SDE
du = f (u)dt + g(u)dW.
(3.6)
In (3.6), f (u) is called the drift term and g(u)
is called the diﬀusion term . When g is inde-
pendent of u, such as in (3.3) where g = 𝜎,
0
(a)
(b)
10
20
30
40
50
60
70
80
90 100
−1.5
−1
−0.5
0
0.5
1
1.5
t
u(t)
−2 −1.5 −1 −0.5
0
0.5
1
1.5
2
0
50
100
150
200
250
300
p
Frequency
Figure 3.1
(a) Two numerical solutions of the
SDE (3.3) on the interval [0, T] with 𝜆= 0.5
and 𝜎= 0.5. The initial data is the same in
each case, u(0) = 1. With each realization of
the noise we obtain a diﬀerent solution path.
Also shown is the solution of the determin-
istic ODE (3.1). In (b), we examine the distri-
bution at t = 100 showing a histogram from
2000 diﬀerent realizations; note that as t →∞
p(t) →N(0, 𝜎2∕2𝜆).

3.2 Brownian Motion / Wiener Process
75
the SDE is said to have additive (or extrin-
sic) noise. When g(u) varies with u, then the
SDE has multiplicative (or intrinsic) noise.
Remark 3.1 Often in the probabilistic and
stochastic literature, time dependence of a
variable u is denoted by ut, so that u(t) ≡ut,
W(t) ≡Wt. We have not used that notation
here as it can lead to confusion with partial
derivatives.
3.2
Brownian Motion / Wiener Process
Although there are many diﬀerent types of
stochastic processes that could be taken,
we look at one type of noise (probably the
most common) that arises from Brownian
motion. A Scottish botanist, Robert Brown,
observed that a small particle such as a
seed in a liquid or gas moves about in
a seemingly random way. This is because
it is being hit continuously by the gas or
liquid molecules. Later, Norbert Wiener,
an American mathematician, gave a math-
ematical description of the process and
proved its existence. In the literature, both
the term Brownian motion and Wiener
process are widely used; here we will use
Brownian motion and denote it by W.
Deﬁnition 3.1 A scalar standard Brown-
ian motion or standard Wiener process is
a collection of random variables W(t) that
satisfy
1. W(0) = 0 with probability 1.
2. For 0 ≤s ≤t, the random variable given
by the increment W(t) −W(s) is
normally distributed with mean zero
and variance t −s. Equivalently,
W(t) −W(s) ∼
√
t −s N(0, 1).
3. For 0 ≤s < t ≤u < v the increments
W(t) −W(s) and W(v) −W(u) are
independent.
4. W(t) is continuous in t.
Brownian motion is often used to model
random eﬀects that are, on a microscale,
small, random, independent, and additive,
where a random behavior is seen on a
larger scale. The central limit theorem (see
Chapter 1) then gives that the sum of these
small contributions converges to incre-
ments that are normally distributed on the
large scale. We recall that the covariance
is a measure of how a random variable X
changes against a diﬀerent random variable
Y and
Cov (X, Y) ∶= E[(X −E[X])(Y −E[Y])]
= E[XY] −E[X]E[Y].
Thus the variance of a random variable
is given by Var (X) = Cov (X, X). To look
at the correlation in time of a random
variable X and Y, we consider the covari-
ance function c(s, t) ∶= Cov (X(s), Y(t)), for
s, t ∈[0, T]. If X = Y, then this is called the
autocovariance. When c(s, t) = c(s −t), the
covariance is said to be stationary.
The following properties of W
are
straightforward to see.
Lemma 3.1 Let W(t), t ≥0, be a standard
Brownian motion. Then,
1. E[W(t)] = 0 and E[(W(t))2] = t.
2. Cov (W(s), W(t)) = min(s, t).
3. the process Y(t) = −W(t) is also a
standard Brownian motion.
Proof. As
W(0) = 0
and
W(t) =
W(t) −W(0) ∼N(0, t), we ﬁnd that
E[W(t)] = 0
E[(W(t))2] = t.
The
covariance
Cov (W(s), W(t)) =
E[W(t)W(s)] because E[W(t)] = 0. Let us
take 0 ≤s ≤t. Then
E[W(t)W(s)] = E[(W(s) + W(t)
−W(s))W(s)]

76
3 Stochastic Diﬀerential Equations
and so
E[W(t)W(s)] = E[(W(s))2]
+ E[(W(t) −W(s))W(s)].
From
Deﬁnition 3.1,
the
increments
W(t) −W(s) and W(s) −W(0) are inde-
pendent, so
E[W(t)W(s)] = s + E[(W(t) −W(s))]
E[W(s)] = s.
Thus
E[W(t)W(s)] = min(s, t),
for all s, t ≥0.
To show that Y(t) = −W(t) is also a
Brownian motion, it is straightforward to
verify the conditions of Deﬁnition 3.1.
The increment W(t) −W(s) of a Brow-
nian motion over the interval [s, t] has
distribution N(0, t −s) and hence the dis-
tribution of W(t) −W(s) is the same as
that of W(t + h) −W(s + h) for any h > 0;
that is, the distribution of the increments
is independent of translations and we say
that Brownian motion has stationary incre-
ments (recall the stationary covariance).
Another important property is that the
Brownian motion is self-similar, that is,
by a suitable scaling, we obtain another
Brownian motion.
Lemma 3.2 (self-similarity) If W(t) is a
standard Brownian motion then for 𝛼>
0, Y(t) ∶= 𝛼1∕2W(t∕𝛼) is also a Brownian
motion.
Proof. To show that Y is Brownian motion,
we need to check the conditions of Deﬁni-
tion 3.1.
1. Y(0) = W(0) = 0 with probability 1
2. Consider s < t and increment
Y(t) −Y(s) = 𝛼1∕2(W(t∕𝛼) −W(s∕𝛼)).
This has distribution
𝛼1∕2N(0, 𝛼−1(t −s)) = N(0, (t −s)).
3. Increments are independent. Consider
Y(t) −Y(s) = 𝛼1∕2(W(t∕𝛼) −W(s∕𝛼))
and
Y(v) −Y(u) = 𝛼1∕2(W(v∕𝛼) −W(u∕𝛼))
for 0 ≤s ≤t ≤u ≤v ≤T, because
(W(t∕𝛼) −W(s∕𝛼)) and
(W(v∕𝛼) −W(u∕𝛼)) are independent.
4. Continuity of Y follows from the
continuity of W.
Hence we have shown Y(t) is also a Brow-
nian motion.
We now examine how rough Brownian
motion is. To do this, we will need to con-
sider convergence of random variables. We
say that the random variables Xj converge
to X in mean square if
E[‖Xj −X‖2] →0,
as
j →∞
and they converge in root mean square if
{E[‖Xj −X‖2]}1∕2 →0,
as
j →∞.
We need to specify a norm. For vectors,
we will take the standard Euclidean norm
‖ ‖2 on ℝd with inner product ⟨xxx,yyy⟩for xxx,
yyy ∈ℝd, so ‖x‖2
2 = ⟨xxx,xxx⟩. For a matrix X ∈
ℝd×m, we will use the Frobenius norm
‖X‖F =
( d
∑
i=1
m
∑
j=1
|xij|2
)1∕2
=
√
trace(X∗X),
where X∗is the conjugate transpose.
We start by examining the quadratic
variation,
∑N
j=0(W(tj+1) −W(tj))2
as
N →∞. The following lemma will also be
useful for developing stochastic integrals
later.

3.2 Brownian Motion / Wiener Process
77
Lemma 3.3 (Quadratic variation) Let
0 = t0 < t1 < · · · < tN = T
be
a
parti-
tion of [0, T] where it is understood that
Δt ∶= max1≤j≤N |tj+1 −tj| →0 as N →∞.
Let
ΔWj ∶= (W(tj+1) −W(tj)).
Then
∑N
j=0(ΔWj)2 →T
in
mean
square
as
N →∞.
Proof. Let
Δtj ∶= tj+1 −tj,
VN ∶=
∑N
j=0(ΔWj)2 and note that
VN −T =
N−1
∑
j=0
[(ΔWj)2 −Δtj
] .
The terms (ΔWj)2 −Δtj are independent
and have mean 0. Therefore, because the
cross terms are 0,
E[(VN −T)2] =
N−1
∑
j=0
E
[((ΔWj)2 −Δtj
)2]
,
and
(
(ΔWj)2 −Δtj
)2
= (ΔWj)4 −2Δtj(ΔWj)2 + Δt2
j
=
(
(ΔWj)4
Δt2
j
−2
(ΔWj)2
Δtj
+ 1
)
Δt2
j
= (X2
j −1)2Δt2
j ,
where Xj ∶= ΔWj∕√Δtj ∼N(0, 1). Thus,
for some C > 0, we have
[(ΔWj)2 −Δtj
]2 ≤C
N−1
∑
j=0
Δt2
j ≤CTΔt.
Hence as N →∞, E
[
(VN −T)2] →0.
We can now show that, although the
quadratic variation is ﬁnite, the total vari-
ation of Brownian paths is unbounded.
Lemma 3.4 (Total variation) Let
0 = t0
< t1 < · · · < tN = T
be
a
partition
of
[0, T]
as
in
Lemma 3.3.
Then
∑N
j=0 |W(tj+1) −W(tj)| →∞as N →∞.
Proof. The proof is by contradiction and we
outline the idea here. Note that
N
∑
j=0
(W(tj+1) −W(tj))2 ≤max
1≤j≤N |W(tj+1)
−W(tj)|
N
∑
j=0
|W(tj+1) −W(tj)|.
Now Brownian motion is continuous, so
as N →∞, we have max1≤j≤N |W(tj+1) −
W(tj)| →0. If ∑|W(tj+1) −W(tj)| is ﬁnite
then ∑N
j=0(W(tj+1) −W(tj))2 →0, but this
contradicts Lemma 3.3.
The fact that ∑N
j=0 |W(tj+1) −W(tj)| →
∞hints that Brownian motion, even if it
is continuous, may in fact be very rough.
In Section 3.2.1 we argue that Brownian
motion is not diﬀerentiable. First, however,
we extend the deﬁnition of a Brown-
ian motion on ℝto ℝd. We can easily
extend Deﬁnition 3.1 to processes W
W
W(t) =
(W1(t), W2(2), … , Wd(t))T ∈ℝd
where
increments W
W
W(t) −W
W
W(s) ∼
√
t −s N(0, I).
Here N(0, I) is the d-dimensional normal
distribution with covariance matrix I, the
ℝd×d identity matrix. It is straightforward
to show the following lemma.
Lemma 3.5 The
process
W
W
W(t) =
(W1(t), W2(2), … , Wd(t))T ∈ℝd
is
a
Brownian motion (or a Wiener process) if
and only if each of the components Wi(t) is
a Brownian motion.
3.2.1
White and Colored Noise
Despite
the nice properties of Brown-
ian motion, a well-known theorem of
Dvoretzky, Erdös, and Kakutani states that
sample paths of Brownian motion are not
diﬀerentiable anywhere. To see why this
might be the case, note that from Deﬁni-
tion 3.1, the increment W(t + h) −W(t),

78
3 Stochastic Diﬀerential Equations
for h > 0, is a normal random variable
with mean 0 and variance h. Now con-
sider Y(t) = (W(t + h) −W(t))∕h. This is
a normal random variable with mean 0
and variance h−1. For Y to approximate
a derivative of W, we need to look at
the limit as h →0. We see that the vari-
ance of the random variable Y goes to
inﬁnity as h →0 and, in the limit, Y is
not well behaved. The numerical deriva-
tive of two diﬀerent Brownian paths is
plotted in Figure 3.2. As Δt →0 these
numerical derivatives do not converge to a
well-deﬁned function.
Although we have just argued that
Brownian motion W(t) is not diﬀeren-
tiable, often in applications, one will see
that dW(t)∕dt used. To understand this
properly, we will need to develop some
stochastic integration theory. Before we do
this, let us ﬁrst relate the term dW(t)∕dt
to “white noise” with covariance (or the
autocorrelation function), which is the
Dirac delta function, so that
E
[dW(t)
dt
dW(s)
ds
]
= 𝛿(s −t).
(3.7)
Recall that the Dirac delta satisﬁes the fol-
lowing properties:
𝛿(s) = 0, for s ≠0,
∫
∞
−∞
𝛿(s)𝜙(s)ds = 𝜙(0)
for any continuous function 𝜙(s). In partic-
ular, if 𝜙(s) ≡1, then ∫∞
−∞𝛿(s)ds = 1.
To see why (3.7) might be true, let us ﬁx s
and t and consider
d(s)∶=E
[(W(t+h)−W(t))
h
(W(s+h)−W(s))
h
]
.
For small h, this should approximate the
Dirac delta. We can simplify d(s) to get
d(s) = 1
h2
(E[W(t + h)W(s + h)]
−E
[
W(t + h)W(s)
]
−E[W(t)W(s + h)] + E[W(t)W(s)])
and use Lemma 3.1 on the Brownian
motion to obtain
d(s) = 1
h2 (min(t + h, s + h) −min(t + h, s)
−min(t, s + h) + min(t, s)).
We see that d is a piecewise linear function
with nodes at s = t −h, t, t + h and
d(s) =
⎧
⎪
⎪
⎨
⎪
⎪⎩
0
for s ≤t −h
(s−t+h)
h2
for t −h < s < t
(t+h−s)
h2
for t < s < t + h
0
for s ≥t + h
.
Furthermore, ∫∞
−∞d(s)ds = 1. Hence, we
see d approximates 𝛿(s −t) for small h,
which suggests (3.7) holds.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−100
−50
0
50
100
dW1/dt
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−100
−50
0
50
100
dW2/dt
t
Figure 3.2
Numerical derivatives of W1(t)
and W2(t) shown in Figure 3.3(a).

3.2 Brownian Motion / Wiener Process
79
We brieﬂy discuss two ways to see that
dW(t)∕dt can be interpreted as white noise
and what this means. The ﬁrst of these
is based on the Fourier transform of the
covariance function. The spectral density
of a stochastic process X with stationary
covariance function c (i.e., c(s, t) = c(s −t))
is deﬁned for 𝜆∈ℝby
f (𝜆) ∶= 1
2𝜋∫
∞
−∞
e−i𝜆tc(t)dt.
When c(t) = 𝛿(t) we ﬁnd for all frequencies
𝜆,
f (𝜆) = 1
2𝜋∫
∞
−∞
e−i𝜆t𝛿(t)dt = 1
2𝜋.
Therefore, by analogy with white light, all
frequencies contribute equally.
Remark 3.2 Note
that
the
covariance
function can also be found from the inverse
Fourier transform of the spectrum. One
practical application of this is to estimate
the correlation function from numerical
data using the Fourier transform.
The precise statement is given in the fol-
lowing theorem.
Theorem 3.1 (Wiener–Khintchine) The
following statements are equivalent.
1. There exists a mean square continuous
stationary process {X(t)∶t ∈ℝ} with
stationary covariance c(t).
2. The function c ∶ℝ→ℝis such that
c(t) = ∫ℝ
ei𝜈tf (𝜈) d𝜈.
A second way to illustrate why dW(t)∕dt
and the covariance (3.7) are called white
noise is to consider t ∈[0, 1] and let
{𝜙j(t)}∞
j=1 be an orthonormal basis, for
example, 𝜙j(t) =
√
2 sin(j𝜋t). Now con-
struct 𝜁(t) by the random series with
t ∈[0, 1]
𝜁(t) ∶=
∞
∑
j=1
𝜁j𝜙j(t),
where each 𝜁j ∼N(0, 1) and are indepen-
dent of each other. Then we have formed a
random 𝜁(t) that has a homogeneous mix
of the diﬀerent basis functions 𝜙j – just like
white light consists of a homogeneous mix
of wavelengths. Let us look at the covari-
ance function for 𝜁(t),
Cov (𝜁(s), 𝜁(t)) =
∞
∑
j,k=1
Cov (𝜁j, 𝜁k
)𝜙j(s)𝜙k(t)
=
∞
∑
j
𝜙j(s)𝜙k(t).
Although the Dirac delta 𝛿is not strictly
speaking a function, formally we can write
𝛿(s) =
∞
∑
j=1
⟨𝛿, 𝜙j(s)⟩𝜙j(s) =
∞
∑
j=1
𝜙j(0)𝜙j(s)
and so the covariance function for 𝜁(t)
is Cov (𝜁(s), 𝜁(t)) = c(s, t) = 𝛿(s −t); that is,
𝜁(t) is a stochastic process with a homoge-
neous mix of the basis functions that has
the Dirac delta as covariance function and
the noise is uncorrelated.
Colored noise, as the name suggests, has
a heterogeneous mix of the basis func-
tions. For example, consider the stochastic
process
𝜈(t) =
∞
∑
j=1
√𝜈j𝜁j𝜙j(t),
(3.8)
where each 𝜁j ∼N(0, 1) and are indepen-
dent and 𝜈j ≥0. As we vary the 𝜈j with
j, the process 𝜈(t) is said to be colored
noise and the random variables 𝜈(t) and

80
3 Stochastic Diﬀerential Equations
𝜈(s) are correlated. Expansions of the form
(3.8) are a useful way to examine many
stochastic processes. When the basis func-
tions are chosen as the eigenfunctions of
the covariance function, this is termed the
Karhunen–Loève expansion.
Theorem 3.2 (Karhunen–Loève)
Consider a stochastic process {X(t)∶t ∈
[0, T]} and suppose that E[∫T
0 (X(s))2ds] <
∞. Then,
X(t) = 𝜇(t) +
∞
∑
j=1
√𝜈j𝜙j(t)𝜉j,
𝜉j ∶=
1
√𝜈j ∫
T
0
(X(s) −𝜇(s))𝜙j(s)ds
(3.9)
and {𝜈j, 𝜙j} denote the eigenvalues and
eigenfunctions of the covariance operator
so that ∫T
0 c(s, t)𝜙j(s)ds = 𝜈j𝜙j(t). The sum
in (3.9) converges in a mean square sense.
The random variables 𝜉j have mean 0,
unit variance, and are pairwise uncor-
related. Furthermore, if the process is
Gaussian, then 𝜉j ∼N(0, 1) independent
and identically distributed.
Another way to generate colored noise is
to solve a stochastic SDE, see Section 3.4,
whose
solution
has
a
particular
fre-
quency
distribution.
The
OU
process
of Example 3.1 is often used to generate
colored noise. For more information on
colored noise, see, for example, [1]. The
Wiener–Khintchine theorem is covered
in a wide range of books, including [2–4].
The Karhunen–Loève expansion is widely
used to construct random ﬁelds as well as
in data analysis and signal processing; see
also [4, 5].
3.2.2
Approximation of a Brownian Motion
We can use Deﬁnition 3.1 directly to con-
struct a numerical approximation Wn to a
Brownian motion W(t) at times tn, where
0 = t0 < t1 < · · · < tN. From (1) of Deﬁni-
tion 3.1 we have that W0 = W(0) = 0. We
construct the approximation by noting that
W(tn+1) = W(tn) + (W(tn+1) −W(tn)) and
from (2) of Deﬁnition 3.1 we know how
increments are distributed.
Letting Wn ≈W(tn), we get that
Wn+1 = Wn + dWn,
n = 1, 2, … , N,
where
dWn ∼
√
ΔtN(0, 1).
This
is
described in Algorithm 3.1 and two typ-
ical results of this process are shown
in Figure 3.3a. In the ﬁgure, we use a
piecewise linear approximation to W(t)
for t ∈[tn, tn+1]. To approximate a d-
dimensional W
W
W by Lemma 3.5, we can
simply use Algorithm 3.1 for each com-
ponent; the result of this is shown in
Figure 3.3b.
Algorithm 3.1 Brownian motion in one
dimension. We assume t0 = 0.
INPUT : vector t = [t0, t1, … , tN]
OUTPUT: vector W such that component
Wn = W(tn).
1: W0 = 0
2: for n = 1 ∶N do
3:
Δt = tn −tn−1
4:
ﬁnd z ∼N(0, 1)
5:
dWn =
√
Δtz
6:
Wn = Wn−1 + dWn
7: end for
3.3
Stochastic Integrals
Before we consider a stochastic integral, let
us brieﬂy recall how the standard deter-
ministic Riemann integral can be deﬁned.
Given a bounded function g ∶[0, T] →ℝ,

3.3 Stochastic Integrals
81
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
W1(t)
0
(a)
(b)
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
W2(t)
t
−1
−0.5
0
0.5
1
1.5
−2
−1
0
1
2
3
4
5
W1(t)
W2(t)
Figure 3.3
(a) Two discretized Brownian motions W1(t),
W2(t) constructed over [0, 5] with N = 5000, so Δt = 0.001.
(b) Brownian motion W1(t) plotted against W2(t). The
paths start at (0, 0) and the ﬁnal point at t = 5 is marked
with a ⋆.
we can deﬁne the Riemann integral using
the Riemann sum
∫
T
0
g(t)dt = lim
Δt→0
N−1
∑
j=0
g(zj)(tj+1 −tj)
(3.10)
where 0 = t0 < t1 < … < tN is a partition of
the interval [0, T], Δt = max1≤j≤N |tj+1 −tj|
and we take any zj ∈[tj, tj+1]. The key
elements are to say for which functions
the integral holds, deﬁne the partition,
and examine the limit as more points
are added to the partition (and so that
Δt →0).
In the stochastic case, we will need to say
what type of functions we can integrate and
how we take the limit. It will also turn out
that the choice of zj will also become impor-
tant when we try and integrate a stochastic
process.
It is standard to illustrate the ideas of a
stochastic integral by considering
I = ∫
T
0
W(s)dW(s) ∶= lim
N→∞SN,
SN ∶=
N−1
∑
j=0
W(zj)(W(tj+1) −W(tj)),
(3.11)
where zj = (1 −𝛼)tj + 𝛼tj+1 and the limit as
N →∞reﬁnes the partition on [0, T]. The
idea of deﬁning I in (3.11) follows by anal-
ogy with the deterministic integral (3.10).
For the stochastic integrals, we examine
these limits in a mean square sense. Note
that for 𝛼= 0, zj = tj (this will correspond
to the Itô integral) and for 𝛼= 1∕2, zj =
(tj + tj+1)∕2 (which will correspond to a
Stratonovich integral).
Lemma 3.6 Let
0 = t0 < t1 < · · · < tN =
T be a partition of [0, T] where it is under-
stood that Δt ∶= max1≤j≤N |tj+1 −tj| →0
as N →∞. Then in mean square we have
I = lim
N→∞SN = (W(T))2
2
+
(
𝛼−1
2
)
T.
Proof. Let ΔWj ∶= W(tj+1) −W(tj). Then
SN = 1
2
N−1
∑
j=0
(W(tj+1)2 −W(tj)2)
−1
2
N−1
∑
j=0
(ΔWj)2 +
N−1
∑
j=0
(W(zj) −W(tj))2

82
3 Stochastic Diﬀerential Equations
+
N−1
∑
j=0
(W(tj+1)−W(zj))(W(zj)−W(tj))
=∶A + B + C + D.
Now the ﬁrst sum A is a telescoping sum
so that A = 1∕2(W(T))2. By Lemma 3.3, the
second sum B →T∕2 as N →∞and a sim-
ilar lemma gives that C →𝛼T. For D, using
the fact that the increments in the sum are
independent, it can be shown that D →0 as
N →∞.
Lemma 3.6 shows that for ∫t
0 W(s)dW(s),
the choice of 𝛼for zj is important.
• When 𝛼= 1∕2, an average is taken of W
over the time interval Δtj = tj+1 −tj and
gives rise to the Stratonovich integral,
which is denoted by ∘, and we have
∫
T
0
W(s) ∘dW(s) = 1
2(W(T))2.
(3.12)
This follows directly from Lemma 3.6
with 𝛼= 1∕2. The result is similar to the
deterministic integral where
∫T
0 sds = s2∕2.
• When 𝛼= 0 on the interval, we have the
important case of the Itô integral. From
Lemma 3.6, with 𝛼= 0, we ﬁnd the Itô
integral
∫
T
0
W(s)dW(s) = 1
2(W(T))2 −T
2 .
(3.13)
Compared to the deterministic integral,
we have an additional term. On the
interval Δtj, we only use information at
tj and no information in the future, and
this gives the Itô integral some special
and useful properties. Because the Itô
integral does not anticipate the future, it
is widely used in ﬁnancial modeling.
We now discuss in detail the Itô version
of the stochastic integral.
3.3.1
Itô Integral
Now let us extend the deﬁnition of the
Itô integral (3.11) (where 𝛼= 0) to a wider
class of functions. We deﬁne the Itô inte-
gral through the mean square limit as fol-
lows
∫
T
0
X(t)dW(t) ∶= lim
N→∞
N−1
∑
j=0
X(tj)
(W(tj+1) −W(tj)) ,
where 0 = t0 < t1 < · · · < tN = T is a parti-
tion as in Lemma 3.6. We need to say some-
thing about the sort of X that we can inte-
grate, as we have seen X may be a stochas-
tic process. Most importantly, for the Itô
integral, we do not want X to see into
the future – the ideas required to make
this precise are described in the following
paragraph.
We have the idea that the stochastic pro-
cess, X(t), is a model of something that
evolves in time and we can therefore ask at
time t questions about the past s < t (X(t)
is known from observations of the past)
and the future s > t. A probability space
(Ω, , P) consists of a sample space Ω, a
set of events , and a probability measure
P. A ﬁltered probability space consists of
(Ω, , t, P) where t is a ﬁltration of .
The ﬁltration t is a way of denoting the
events that are observable by time t and so,
as t increases, t contains more and more
events. If X(t), t ∈[0, T] is nonanticipating
(or t adapted), then X(t) is t measurable
for all t ∈[0, T] (roughly X(t) does not see
into the future). Finally, X(t) is predictable
if it is nonanticipating and can be approxi-
mated by a sequence X(sj) →X(s) if sj →s
for all s ∈[0, T], sj < s.
Theorem 3.3 (Itô integral) Let
X(t),
t ∈[0, T]
be
a
predictable
stochastic

3.3 Stochastic Integrals
83
process such that
(
E
[
∫
t
0
|X(s)|2ds
])1∕2
< ∞.
Then
1. ∫t
0 X(s) dW(s) for t ∈[0, T] has
continuous sample paths.
2. The Itô integral has the martingale
property . That is, best predictions of
∫t
0 X(s) dW(s) based on information up
to time r is ∫r
0 X(s) dW(s). In particular,
the integral has mean 0 :
E
[
∫
t
0
X(s) dW(s)
]
= 0.
3. We have the Itô isometry
E
[
||| ∫
t
0
X(s) dW(s)|||
2]
= ∫
t
0
E[|X(s)|2]
ds, t ∈[0, T]. (3.14)
Example 3.2
Reconsider the stochastic
integral
I(t) = ∫
t
0
W(s) dW(s),
t ≥0.
The martingale property gives E[I(t)] = 0
and the Itô isometry (3.14) gives
E[I(t)2] =∫
t
0
E[(W(s))2] ds =∫
t
0
s ds = 1
2t2.
(3.15)
Note that using (3.13), we can write I(t)
as I(t) = 1
2(W(t))2 −t
2, or rearranged as
1
2(W(t))2 = 1
2t + I(t). This is often written
in shorthand as
d
(1
2(W(t)
)2
= 1
2 dt + W(t) dW(t). (3.16)
The Itô integral can be extended to be
vector-valued.
Deﬁnition 3.2 Let W
W
W(t) = (W1(t), W2(2),
… , Wm(t))T ∈ℝm and let X ∈ℝd×m be
such that each Xij is a predictable stochastic
process such that
(
E
[
∫
t
0
|Xij(s)|2ds
])1∕2
< ∞.
(3.17)
Then ∫t
0 X(s)dW(s) is the random variable
in ℝd with ith component
m
∑
j=1 ∫
t
0
Xij(s)dWj(s).
It can be shown that the following prop-
erties hold for the vector-valued Itô inte-
gral.
1. The integral ∫t
0 X(s) dW
W
W(s) for t ∈[0, T]
is a predictable process.
2. The martingale property holds and the
Itô integral has mean 0:
E
[
∫
t
0
X(s) dW
W
W(s)
]
= 0.
3. Given two stochastically integrable
processes X and Y, that is, X and Y are
predictable and (3.17) holds, then if
̂t ∶= min(t1, t2),
E
[⟨
∫
t1
0
X(s) dW
W
W(s), ∫
t2
0
Y(s) dW
W
W(s)
⟩]
= ∫
̂t
0
m
∑
i=1
E
[⟨
XXXi(s),YYY i(s)
⟩]
ds,
where XXXi,YYY i denote the ith column of X
and Y and ⟨⋅, ⋅⟩is the ℝd inner product.
From this, the Itô isometry follows
E
[
‖ ∫
t
0
X(s) dW
W
W(s)‖2
2
]
= ∫
t
0
E[‖X(s)‖2
F
] ds, t ∈[0, T].
(3.18)

84
3 Stochastic Diﬀerential Equations
In fact, the class of processes for which
the stochastic integral can be developed
is wider, see, for example, [5, 6]. With this
deﬁnition, we can examine systems of Itô
SDEs.
3.4
It̂o SDEs
We described in the introduction that (3.6)
is shorthand for the integral equation (3.5)
and in Section 3.2.1 we saw that Brownian
motion is not diﬀerentiable.
We
now
consider
the
Itô
SDEs
where
uuu ∈ℝd
and
we
are
given
a
process
with
drift
fff (uuu) ∶ℝd →ℝd,
diﬀusion
G(uuu) ∶ℝd →ℝd×m,
and
W
W
W(t) = (W1(t), W2(2), … , Wm(t))T ∈ℝm.
Instead of writing
duuu
dt = fff (uuu) + G(uuu)dW
W
W(t)
dt
,
we realize that W is not diﬀerentiable and
hence write
duuu = fff (uuu)dt + G(uuu)dW
W
W(t),
(3.19)
which we interpret as an Itô stochastic inte-
gral equation
uuu(t) = uuu(0) + ∫
t
0
fff (uuu(s))ds
+ ∫
t
0
G(uuu(s))dW
W
W(s).
(3.20)
The last integral in (3.20) is the stochastic
integral from Deﬁnition 3.2 and G(uuu) is a
d × m matrix.
Example 3.3 Consider the system of SDEs
with two independent Brownian motions
W1(t) and W2(t)
du1 = f1(u1, u2) dt + g11(u1, u2)dW1
+ g12(u1, u2)dW2
du2 = f2(u1, u2) dt + g21(u1, u2)dW1
+ g22(u1, u2)dW2.
(3.21)
We can write (3.21) in the form of (3.19)
with d = m = 2 by taking uuu = (u1, u2)T and
f ∶ℝ2 →ℝ2, G ∶ℝ2 →ℝ2×2 given by
fff (uuu) =
(f1(u1, u2)
f2(u1, u2)
)
,
G(uuu) =
(g11(u1, u2)
g12(u1, u2)
g21(u1, u2)
g22(u1, u2)
)
.
If the following conditions on the drift
fff and diﬀusion G are satisﬁed, it can be
shown that a solution exists to the SDE
(3.19).
Assumption 3.1 (linear growth/
Lipschitz condition) There exists a con-
stant L > 0 such that the linear growth con-
dition holds for uuu ∈ℝd,
‖‖fff (uuu)‖‖
2 ≤L(1 + ‖uuu‖2),
‖G(uuu)‖2 ≤L(1 + ‖uuu‖2),
uuu ∈ℝd,
and the global Lipschitz condition holds for
uuu1,uuu2 ∈ℝd,
‖‖fff (uuu1) −fff (uuu2)‖‖ ≤L ‖‖uuu1 −uuu2‖‖ ,
‖‖G(uuu1) −G(uuu2)‖‖ ≤L ‖‖uuu1 −uuu2‖‖ .
Theorem 3.4 (existence and uniqueness
for
SDEs)
Suppose
that
Assump-
tion 3.1
holds
and
that
W(t)
is
a
Brownian
motion.
For
each
T > 0
and uuu0 ∈ℝd, there is a unique uuu with
supt∈[0,T]
(E[‖uuu(t)‖2])1∕2 < ∞
such
that
for t ∈[0, T]
uuu(t)=uuu0 +∫
t
0
fff (uuu(s)) ds +∫
t
0
G(uuu(s)) dW
W
W(s).
(3.22)

3.4 It̂o SDEs
85
The notion of solution that we have taken
is a stochastic process that is determined
for any Brownian path – this is called a
strong solution. We could also ask for solu-
tions that are valid only for particular Brow-
nian paths, in which case the Brownian path
also becomes part of the solution. This is
the idea of a weak solution. We do not con-
sider this further. In Example 3.1, we gave a
simple example of an SDE for which f (u) =
−𝜆u and G(u) = 𝜎. As G is independent of
u, the noise is additive and (3.3) is equiva-
lent to the Itô integral equation
p(t) = p(0) −𝜆∫
t
0
p(s)ds + 𝜎∫
s
0
dW(s).
We now present two further SDEs. The ﬁrst
describes the dynamics of a particle subject
to noise. The second models the ﬂuctuation
of an asset price on a stock market.
Example 3.4 [Langevin equation] Denote
by p the momentum, q the position, and
by H(q, p) the system energy of a particle.
If the particle in Example 3.1 has potential
energy V(q) at position q ∈ℝ, its dynam-
ics are described by the following SDE for
(q(t), p(t))T
dq = p dt
dp = −𝜆p dt −V ′(q) dt + 𝜎dW(t)
(3.23)
for parameters 𝜆, 𝜎> 0. Here d = 2, m = 1,
and for uuu = (q, p)T
fff (uuu) =
(
p
−𝜆p −V ′(q)
)
,
G(uuu) =
(0
𝜎
)
.
The noise is again additive and acts directly
only on the momentum p.
In Figure 3.4, we plot a solution of (3.23)
for one sample path of the noise. The
potential V(q) = (q2 −1)2∕4 is a double-
well potential and we take 𝜆= 0.1, 𝜎= 0.2.
In the absence of noise, there are three
ﬁxed points (0, 0)T (which is unstable)
and (±1, 0)T (which are stable). With the
stochastic forcing we see that the particle
no longer comes to rest and can cross
the energy barrier between (−1, 0)T and
(1, 0)T.
The SDEs of Examples 3.1 and 3.4 both
have additive noise. The following example
has multiplicative noise.
−1.5
−1
−0.5
0
0.5
1
1.5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
p(t)
q(t)
0
20
40
60
80 100 120 140 160 180 200
−1
0
1
q(t)
0
(a)
(b)
20
40
60
80 100 120 140 160 180 200
−1
−0.5
0
0.5
1
t
p(t)
Figure 3.4
Solution of Langevin equation (3.23) with
V(q) = (q2 −1)2∕4, 𝜆= 0.1, 𝜎= 0.2. In (a) we show p(t)
and q(t) for a sample path of the noise. In (b) we plot the
phase portrait.

86
3 Stochastic Diﬀerential Equations
Example 3.5 [geometric
Brownian
mo-
tion] In ﬁnancial modeling, the price u(t) at
time t of a risk-free asset with interest rate
r obeys a diﬀerential equation du∕dt = ru.
On the stock market, stock prices ﬂuctuate
rapidly and the ﬂuctuations are modeled by
replacing the risk-free interest rate r by a
stochastic process r + 𝜎dW∕dt, where 𝜎is
the volatility and dW∕dt is white noise. We
obtain the SDE with multiplicative noise
du = r u dt + 𝜎u dW(t),
u(0) = u0,
(3.24)
where u0 ≥0 is the price at time t = 0.
Here d = m = 1, the drift f (u) = ru, and
the diﬀusion G(u) = 𝜎u. The solution u(t)
depends on the interpretation of the inte-
gral ∫t
0 u(s) dW(s). The Itô SDE is chosen
to model stock prices, because the cur-
rent price u(t) of the stock is supposed to
be determined by past events and is inde-
pendent of the current ﬂuctuations dW(t).
The solution process u(t) of (3.24) is called
geometric Brownian motion. In Figure 3.5a
we plot ﬁve sample realizations of (3.24)
computed numerically with r = 0.5 and 𝜎=
0.5 and all with initial data u(0) = 1. In
Figure 3.5a we also plot E[u(t)] found from
the exact solution, see Example 3.7.
Note that although we have described
(3.24) from a mathematical ﬁnance point of
view, it could also be considered as a basic
population growth model where exponen-
tial population growth is modulated with
random ﬂuctuations.
3.4.1
Itô Formula and Exact Solutions
The chain rule of ordinary calculus changes
in the Itô calculus to the so-called Itô for-
mula. We now discuss the Itô formula (also
called Itô’s Lemma) for (3.19) with d = m =
1 (so, in one dimension). We apply the
Itô formula to geometric Brownian motion
(see Example 3.5) and the mean reverting
OU process of which Example 3.1 is one
case. Finally, we quote a general form of the
Itô formula.
Lemma 3.7 (one-dimensional Itô
formula)
Let Φ ∶[0, T] × ℝ→ℝhave
continuous
partial
derivatives
𝜕Φ∕𝜕t,
𝜕Φ∕𝜕u and 𝜕2Φ∕𝜕u2. Let u satisfy the Itô
SDE (3.19) with d = m = 1, so
du = f (u)dt + G(u)dW(t),
u(0) = u0
and suppose Assumption 3.1 holds. Then,
almost surely,
dΦ = 𝜕Φ
𝜕t dt + 𝜕Φ
𝜕u du + 1
2
𝜕2Φ
𝜕u2 G2 dt
(3.25)
or written in full
Φ(t, u(t)) = Φ(0, u0) + ∫
t
0
𝜕Φ
𝜕t (s, u(s))
+𝜕Φ
𝜕u (s, u(s))f (u(s))
+1
2
𝜕2Φ
𝜕u2 (s, u(s))G(u(s))2 ds
+ ∫
t
0
𝜕Φ
𝜕u (s, u(s))G(u(s)) dW(s).
To interpret Lemma 3.7, consider the
solution u(t) of the deterministic ODE
du∕dt = 𝜆
and
let
𝜙(u) = 1∕2u2.
The
standard chain rule implies that
d𝜙(u)
dt
= u du
dt = 𝜆u(t).
However,
if
u(t)
satisﬁes
du = 𝜆dt +
𝜎dW(t), the Itô formula (3.25) says that
d𝜙(u) = u du + 𝜎2
2 dt
(3.26)
and we pick up an unexpected extra term
𝜎2∕2dt. This shows that the Itô calculus is
diﬀerent from the standard deterministic

3.4 It̂o SDEs
87
0
(a)
(b)
1
2
3
4
5
6
7
8
9
10
20
40
60
80
100
120
t
u(t)
0
1
2
3
4
5
6
7
8
9
10
0
100
200
300
400
0
1
2
3
4
5
6
7
8
9
10
−5
0
5
10
15
t
Error
Figure 3.5
(a) Five numerical solutions of the
SDE (3.24) for geometric Brownian motion on
the interval [0, 10] with r = 0.5 and 𝜎= 0.5.
The initial data is the same in each case, u(0) =
1. With each realization of the noise we obtain
a diﬀerent solution path. The solid smooth line
shows E[u(t)] from (3.28). In (b) (top) we plot
the exact solution and approximate numerical
solution + together on the interval [0, 10]. In
(b) (bottom) we plot the error.
calculus. We can expand (3.26) using the
SDE to get
d
(1
2u2)
= u (𝜆udt + 𝜎dW) + 𝜎2
2 dt
=
(
𝜆u2 + 𝜎2
2
)
dt + 𝜎udW. (3.27)
When 𝜆= 0 and 𝜎= 1, the SDE is simply
du = dW and u(t) = W(t). In which case,
(3.27) becomes
d
(1
2W 2)
= 1
2dt + WdW,
and we have recovered (3.16) using the Itô
formula.
Example 3.6 [Integration by parts] Recon-
sider the simple SDE du = dW and let
𝜙(t, u) = tu. Then by the Itô formula,
d𝜙= udt + tdu
and
hence
d(tW) =
Wdt + tdW. Written in integral form,
we have tW(t) = ∫t
0 W(s)ds + ∫t
0 sdW(s) or
∫
t
0
sdW(s) = tW(t) −∫
t
0
W(s)ds,
which is a formula for integration by parts.
Remark 3.3 The
integral
Y(T) =
∫T
0 W(s)ds is the area under the path
W and because W is stochastic so is the
value of the integral. In fact, for ﬁxed T,
Y ∼N(0, T3∕3).
This
follows
from
the
deﬁnition of the Riemann integral. For a
partition of [0, T] with T = NΔt we have
Y = lim
N→∞
N
∑
n=1
ΔtW(tn)
= lim
Δt→0
[NW(t1) + (N −1)(W(t2) −W(t1))
+ · · · + (W(tN) −W(tN−1))] .
On the right, we have a sum of indepen-
dent normal random variables and clearly
E[Y] = 0. For the variance, we have
Var (Y) = lim
Δt→0
[Δt2 Var (NW(t1)) + · · ·
+ Δt2 Var (W(tN) −W(tN−1))]
= lim
Δt→0
[Δt3(N2 + · · · + 1)]
= lim
Δt→0
[
Δt3 1
6N(N + 1)(2N + 1)
]
= T3
3 .

88
3 Stochastic Diﬀerential Equations
We now look at two applications of the Itô
formula to solve for geometric Brownian
motion (Example 3.5) and the OU process
(Example 3.1).
Example 3.7 [geometric Brownian mot-
ion] We show the solution of the geometric
Brownian motion SDE (3.24) is given by
u(t) = exp
((
r −𝜎2
2
)
t + 𝜎W(t)
)
u0. (3.28)
For u(0) = 0, u(t) = 0 is clearly the solution
to (3.24). For u > 0, let 𝜙(u) = log u, so that
𝜙′(u) = u−1 and 𝜙′′(u) = −u−2. By the Itô
formula with Φ(t, u) = 𝜙(u),
d𝜙(u) = r dt + 𝜎dW(t) −1
2𝜎2 dt.
Hence,
𝜙(u(t)) = 𝜙(u0) + ∫
t
0
(
r −𝜎2
2
)
ds
+ ∫
t
0
𝜎dW(s)
and
log u(t) = log(u0) + (r −(𝜎2∕2))t +
𝜎W(t). Taking the exponential, we ﬁnd
(3.28). It is clear that, when u0 ≥0, the
solution u(t) ≥0 for all t ≥0. This is desir-
able in ﬁnancial modeling, where stock
prices are nonnegative and also for mod-
els of populations where populations are
positive.
From
(3.28),
we
see
that
because
E[W(t)] = 0, we have E[u(t)] = exp((r −
𝜎2∕2)t)u0. This is plotted in Figure 3.5a
along with ﬁve sample paths of the SDE
(3.24).
Example 3.8 [mean reverting OU process]
We consider the following generalization of
(3.3)
du = 𝜆(𝜇−u)dt + 𝜎dW(t),
u(0) = u0,
for 𝜆, 𝜇, 𝜎∈ℝ. We solve this SDE by using
the Itô formula (3.25) with Φ(t, u) = e𝜆t u.
Then,
dΦ(t, u) = 𝜆e𝜆tu dt
+ e𝜆t(𝜆(𝜇−u)dt + 𝜎dW(t)) + 0
and
Φ(t, u(t)) −Φ(0, u0) = e𝜆tu(t) −u0
= 𝜆𝜇∫
t
0
e𝜆s ds + 𝜎∫
t
0
e𝜆s dW(s).
After evaluating the deterministic integral,
we ﬁnd
u(t)=e−𝜆tu0+𝜇(
1 −e−𝜆t)
+𝜎∫
t
0
e𝜆(s−t)dW(s)
(3.29)
and this is known as the variation of
constants solution. Notice that u(t) is a
Gaussian process and can be speciﬁed
by its mean 𝜇(t) = E[u(t)] and covari-
ance
c(s, t) = Cov (u(s), u(t)).
Using
the
mean zero property of the Itô integral
(Theorem 3.3), the mean is
𝜇(t) = E[u(t)] = e−𝜆tu(0) + 𝜇(1 −e−𝜆t)
(3.30)
so that 𝜇(t) →𝜇as t →∞and the process
is “mean reverting.” For the covariance, ﬁrst
note that
c(s, t) = Cov (u(t), u(s))
= E[(u(s) −E[u(s)]
) (u(t) −E[u(t)]
)]
= E
[
∫
s
0
𝜎e𝜆(r−s)dW(r) ∫
t
0
𝜎e𝜆(r−t)dW(r)
]
= 𝜎2e−𝜆(s+t)E
[
∫
s
0
e𝜆rdW(r) ∫
t
0
e𝜆rdW(r)
]
.
Then, using the Itô isometry, property (3)
from Deﬁnition 3.2,
c(s, t) = 𝜎2
2𝜆e−𝜆(s+t)(e2𝜆min(s,t) −1).

3.4 It̂o SDEs
89
In particular, the variance Var (u(t)) =
𝜎2(1 −e−2𝜆t)∕2𝜆so that Var (u(t)) →𝜎2∕2𝜆
and u(t) →N(𝜇, 𝜎2∕2𝜆) in the distribution
as t →∞.
Reconsider Example 3.1, which is the
mean reverting OU process with 𝜇= 0.
Example 3.9 For
dp = −𝜆pdt + 𝜎dW(t)
the variation of constants (3.29) gives
p(t) = e−𝜆tp0 + 𝜎∫
t
0
e−𝜆(t−s) dW(s).
We can use this to derive an important
relationship in statistical physics. By def-
inition, the expected kinetic energy per
degree of freedom of a system at temper-
ature T is given by kBT∕2, where kB is the
Boltzmann constant. The temperature of
a system of particles in thermal equilib-
rium can be determined from 𝜆and 𝜎. For
the OU model, the expected kinetic energy
E[p(t)2∕2] is easily calculated
E[p(t)2]
= e−2𝜆tp2
0 + 𝜎2
∫
t
0
e−2𝜆(t−s) ds
= e−2𝜆tp2
0 + 𝜎2
2𝜆(1 −e−2𝜆t)
→𝜎2
2𝜆
as t →∞.
We see that the expected kinetic energy
E[1∕2p(t)2] converges to 𝜎2∕4𝜆as t →∞.
Thus, 𝜎2∕4𝜆is the equilibrium kinetic
energy and the equilibrium temperature is
given by kBT = 𝜎2∕2𝜆, which is known as
the ﬂuctuation-dissipation relation.
We saw in Example 3.8 that p(t) →
N(0, 𝜎2∕2𝜆)
in
distribution.
This
is
illustrated
by
numerical
simulations
in
Figure 3.1.
This
limiting
distribu-
tion is known as the Gibbs canonical
distribution,
often
written
N(0, kBT)
with
probability
density
function
p(q, p) = e−𝛽H(q,p)∕Z,
where
𝛽= 1∕kBT
is the inverse temperature, Z is a normal-
ization constant, and H(q, p) is the system
energy for a particle with position q and
momentum p. In this case, H = p2∕2.
We now state a more general form of Itô’s
formula.
Lemma 3.8 (Itô formula) If Φ(t,uuu0) is a
continuously diﬀerentiable function of t ∈
[0, T] and twice continuously diﬀerentiable
functions of uuu0 ∈ℝd and uuu(t) denotes the
solution of
duuu = fff (uuu)dt + G(uuu)dW
W
W(t)
under Assumption 3.1, then
Φ(t,uuu(t))
= Φ(0,uuu0) + ∫
t
0
(
𝜕
𝜕t + 
)
Φ(s,uuu(s)) ds
+
m
∑
k=1 ∫
t
0
kΦ(s,uuu(s)) dWk(s),
(3.31)
where
Φ(t,uuu) ∶= fff (uuu)T∇Φ(t,uuu)
+1
2
m
∑
k=1
gggk(uuu)T∇2Φ (t,uuu)gggk(uuu),
kΦ(t,uuu) ∶= ∇Φ(t,uuu)Tgggk(uuu),
(3.32)
for uuu ∈ℝd and t > 0. Here gggk denotes the
kth column of the diﬀusion matrix G. ∇Φ
is the gradient and ∇2Φ the Hessian matrix
of second partial derivatives of Φ(t,xxx) with
respect to xxx.
Itô’s formula can be generalized fur-
ther, for example, see [6]. With the Itô
formula (3.31) it is possible to generalize
the variation of constants formula (3.29).
Consider the semilinear Itô SDE
duuu =
[
−Auuu + fff (uuu)
]
dt + G(uuu) dW
W
W(t),
uuu(0) = uuu0 ∈ℝd

90
3 Stochastic Diﬀerential Equations
Then,
by
the
same
techniques
as
in
Example 3.8 we have
uuu(t) = e−tAuuu0 + ∫
t
0
e−(t−s)Afff (uuu(s)) ds
+ ∫
t
0
e−(t−s)AG(uuu(s)) dW
W
W(s).
In Section 3.7 the Itô formula is used to
obtain deterministic equations from the
SDEs.
3.5
Stratonovich Integral and SDEs
Let
W(t)
be
the
Brownian
motion
and
let
g ∶[0, T] →ℝ
be
such
that
(E[∫t
0 |g(s)|2ds])1∕2 < ∞. Then the one-
dimensional
Stratonovich
integral
is
deﬁned through the mean square limit as
follows
∫
T
0
g(t)dW(t) ∶= lim
N→∞
N−1
∑
j=0
g
((tj + tj+1)
2
)
(
W(tj+1) −W(tj)) , (3.33)
where
0 = t0 < t1 < · · · < tN = T
is
a
partition as in Lemma 3.6. The multidi-
mensional Stratonovich integral is deﬁned
analogously to Deﬁnition 3.2. We saw that
the Itô integral has mean 0; however, this is
not the case for the Stratonovich one. We
can illustrate this for the stochastic integral
I = ∫T
0 W(s) ∘dW(s).
Lemma 3.6
shows
that I = 1
2(W(T))2 and
E[I] = 1
2E[(W(T))2] = T
2 .
We
can
transform
between
the
Stratonovich and Itô integrals by ensur-
ing the mean zero property. This is often
called a drift correction. Provided g is
diﬀerentiable, we have
∫
t
0
g(W(s)) ∘dW(s)
= ∫
t
0
g(W(s)) dW(s)
+ 1
2 ∫
t
0
g′(W(s)) ds.
(3.34)
Example 3.10 Consider the case where
g(W) = W. Then from (3.34), we have
using (3.13)
∫
t
0
W(s) ∘dW(s) = ∫
t
0
W(s) dW(s)+ 1
2 ∫
t
0
ds
= (W(T))2
2
,
and we have recovered (3.12) exactly.
We write the Stratonovich SDE with drift
f and diﬀusion g as
dv = f (v) dt + g(v) ∘dW(t),
v(0) = v0,
(3.35)
which we interpret as the integral equation
v(t) = v0 + ∫
t
0
f (v(s)) ds + ∫
t
0
g(v(s)) ∘W(s)
(3.36)
and the stochastic integral is a Stratonovich
integral.
We can replace the Stratonovich integral
using the transformation to ﬁnd that v(t)
satisﬁes the Itô SDE
v(t) = v0 + ∫
t
0
[
f (v(s)) + 1
2
dg(v(s))
dv
g(v(s))
]
ds
+ ∫
t
0
g(v(s))dW(s).
Example 3.11 Consider the SDE
du = rudt + 𝜎u ∘dW,
given
u(0) = u0.
(3.37)
This looks like the geometric Brownian
motion SDE, only here the stochastic forc-
ing is interpreted in the Stratonovich sense.

3.5 Stratonovich Integral and SDEs
91
0
(a)
(b)
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
1
2
3
4
5
6
t
u(t)
Stratonovich
Ito
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
1
2
3
4
5
6
t
u(t)
Figure 3.6
In both (a) and (b) the sample path for the
Brownian motion is ﬁxed. In (a), we compare the Itô SDE
(3.24) and Stratonovich SDEs (3.37), computed using the
Heun method. In (b), we plot the solution of the equivalent
Itô SDE, (3.38) to (3.37) and see that these agree.
In Figure 3.6a, we ﬁx a sample path for the
Brownian motion and plot a sample path
of (3.37) and compare it to a sample path
of (3.24). It is clear the solutions of the two
SDEs do not agree. Equation (3.37) is equiv-
alent to the following Itô SDE
du =
(
ru + 1
2𝜎2u
)
dt + 𝜎udW,
given
u(0) = u0.
(3.38)
In Figure 3.6b, we plot the solution of this
SDE using the same Brownian motion
as in Figure 3.6a. We observe that this
now agrees with the Stratonovich path in
Figure 3.6b.
Example 3.12
Now let us start with an
Itô SDE and convert to an equivalent
Stratonovich form. Let us consider the geo-
metric Brownian motion of (3.24), this can
be rewritten as the equivalent Stratonovich
SDE
du =
(
ru −1
2𝜎2u
)
dt + 𝜎u ∘dW,
given
u(0) = u0.
For a system of Stratonovich SDEs
in dimension d > 1 with forcing by m
Brownian motions, we have
dvvv = fff (vvv) dt + G(vvv) ∘dW
W
W(t),
vvv(0) = vvv0,
(3.39)
for f ∶ℝd →ℝd, G∶ℝd →ℝd×m, and
W
W
W(t) = (W1(t), … , Wm(t))T
for
inde-
pendent Brownian motions Wi(t). Then,
vvv(t) = (v1(t), … , vd(t))T
is
a
solution
to
(3.39) if the components vi(t) for
i = 1, … , d satisfy
vi(t) = v0,i + ∫
t
0
fi(vvv(s)) ds
+
m
∑
j=1 ∫
t
0
gij(vvv(s)) ∘dWj(s),
t > 0,
where the last term is the Stratonovich inte-
gral deﬁned as in (3.33).
Provided G is diﬀerentiable, we are
able to convert between the Itô and
Stratonovich interpretations of the SDEs.
We write (3.39) as an Itô SDE. Deﬁne
̃f (vvv) = (̃f1(vvv), … , ̃fd(vvv))T, where

92
3 Stochastic Diﬀerential Equations
̃fi(vvv) = fi(vvv) + 1
2
d
∑
k=1
m
∑
j=1
gkj(vvv)
𝜕gij(vvv)
𝜕vk
,
i = 1, … , d,
vvv ∈ℝd.
(3.40)
Then the solution vvv(t) of (3.39) satisﬁes the
equivalent Itô SDE
dvvv = ̃fff (vvv) dt + G(vvv) dW
W
W(t),
vvv(0) = vvv0.
Note that it is clear that for additive
noise the Itô and Stratonovich interpre-
tations coincide as 𝜕gij∕𝜕vk = 0 for all
i, j. Finally, we quote the corresponding
Stratonovich version of the Itô formula
of Lemma 3.8. This shows that the chain
rule for Stratonovich calculus resembles
the deterministic version of the chain
rule.
Lemma 3.9 (Stratonovich formula)
Suppose that Φ is as in Lemma 3.8 and vvv(t)
is the solution of (3.39). Then
Φ(t,vvv(t))
= Φ(0,vvv0) +∫
t
0
(
𝜕
𝜕t + strat
)
Φ(s,vvv(s)) ds
+
m
∑
k=1 ∫
t
0
kΦ(s,vvv(s)) ∘dWk(s), (3.41)
where stratΦ = fff T∇Φ and kΦ ∶= ∇ΦTgggk
is deﬁned by (3.32).
Remark 3.4 One
advantage
of
the
Stratonovich integral is that it follows
the standard rules of calculus. The Itô
integral is better suited to modeling ﬁnan-
cial applications and for analysis. Which
form of integral is appropriate is essen-
tially a modeling question. However, the
integrals are the same if the integrand is
not stochastic and we can convert between
them provided the diﬀusion term G is
diﬀerentiable.
3.6
SDEs and Numerical Methods
We construct approximations uuun to the
solution uuu(tn) of a system of SDEs where
tn = nΔt and we are given initial data
uuu(0) = uuu0. As we have seen the solution
depends on the sample path of the noise
that is taken and so convergence of numeri-
cal methods for SDEs has to be approached
with care.
If we are worried about approximating
the sample path of the solution uuu(t) for a
given sample path of the noise, then we
need a strong approximation and we are
interested in strong convergence, that is we
look at the root mean square error
sup
0≤tn≤T
(E[‖uuu(tn) −uuun‖])1∕2 .
Using the Borel–Cantelli lemma, it is pos-
sible to obtain from the strong conver-
gence, a pathwise error sup0≤tn≤T ‖uuu(tn) −
uuun‖, example, see [7–9].
In contrast, in many situations it is aver-
age quantities that are of interest and not
the individual sample paths. For example,
we may be actually interested in E[𝜙(uuu(T))]
for
some
given
function
𝜙∶ℝd →ℝ,
where uuu(T) is the solution of an SDE at
time T. This leads to the notion of weak
convergence where we examine
sup
0≤tn≤T
|E[𝜙(uuu(tn))] −E[𝜙(uuun))]|.
We note that if strong convergence can
be established, then weak convergence fol-
lows, for example, see [10].
In practice, to approximate E[𝜙(uuu(T))],
we need a combination of Monte Carlo and
numerical methods for SDEs. We need to
generate M independent samplesuuuj
N for j =
1, … , M of an approximationuuuN touuu(T) for
tN = T and approximate E[𝜙(uuu(T))] by the

3.6 SDEs and Numerical Methods
93
sample average
𝜇M ∶= 1
M
M
∑
j=1
𝜙(uuuj
N).
(3.42)
It can be shown that the Monte Carlo error
from only taking M samples satisﬁes
P
⎛
⎜
⎜
⎜⎝
|||E[𝜙(uuuN)] −𝜇M||| <
2
√
Var (𝜙(uuuN))
√
M
⎞
⎟
⎟
⎟⎠
> 0.95 + O(M−1∕2),
(see Chapter 2). The total error made in
approximating E[𝜙(uuu(tn))] can then be
divided as the sum of the weak discretiza-
tion error because of approximating uuu(T)
by uuuN and the Monte Carlo error because
of taking M samples,
E[𝜙(uuu(T))] −𝜇M
=
[
E[𝜙(uuu(T))] −E[𝜙(uuuN)]]
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
weak discretization error
+
[
E[𝜙(uuuN)] −𝜇M
]
.
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
Monte Carlo error
(3.43)
3.6.1
Numerical Approximation of Itô SDEs
Consider the Itô SDE (3.19) that we can
write as the integral equation (3.20). We
now want to approximate the solution of
the SDE over a ﬁxed time interval [0, T]. For
convenience, we take a ﬁxed step Δt, so that
NΔt = T. The simplest of such methods
is the Euler–Maruyama method and we
now outline its derivation. We have from
(3.20),
uuu(tn+1) = uuu0 + ∫
tn+1
0
fff (uuu(s)) ds
+ ∫
tn+1
0
G(uuu(s)) dW
W
W(s),
and
uuu(tn) = uuu0 + ∫
tn
0
fff (uuu(s)) ds
+ ∫
tn
0
G(uuu(s)) dW
W
W(s).
Subtracting, we obtain
uuu(tn+1) = uuu(tn) + ∫
tn+1
tn
fff (uuu(s)) ds
+ ∫
tn+1
tn
G(uuu(s)) dW
W
W(s).
(3.44)
We approximate both the integrands fff
and G by a constant over [tn, tn+1) on
the basis of the value at tn to obtain the
Euler–Maruyama method
uuun+1 = uuun + fff (uuun) Δt + G(uuun)ΔW
W
W n,
ΔW
W
W n ∶= W
W
W(tn+1) −W
W
W(tn).
(3.45)
As on each subinterval, G is evaluated at
the left-hand end point, the approxima-
tion is consistent with our deﬁnition of the
Itô integral. The noise increments ΔW
W
W n ∶=
W
W
W(tn+1) −W
W
W(tn) have mean 0 and variance
Δt. In implementation, we can usually get
standard normal random variables, that is,
𝜁n ∼N(0, 1); thus, to form the increments
ΔW
W
W n, we need to use these. To do this, the
Euler–Maruyama scheme is rewritten as
uuun+1 = uuun + fff (uuun) Δt +
√
ΔtG(uuun)𝜁𝜁𝜁n,
where 𝜁𝜁𝜁n ∼N(0, I) (see Algorithm 3.2).
This scheme only gives values at the dis-
crete points tn = nΔt. If values are required
at intermediate points, linear interpolation
can be used [5].
Example 3.13
Apply
the
Euler–
Maruyama
scheme
to
the
geometric
Brownian motion SDE (3.24), to ﬁnd

94
3 Stochastic Diﬀerential Equations
Algorithm
3.2
Euler–Maruyama
to
approximate
solution
of
SDE
duuu = fff (uuu)dt + G(uuu)dW
W
W.
INPUT : Initial data u0, Final time T, N num-
ber of steps to take.
OUTPUT: Vector t of time and vector u such
that un ≈u(tn).
1: Δt = T∕N, t0 = 0
2: for n = 1 ∶N do
3:
tn = nΔt
4:
ﬁnd z ∼N(0, 1)
5:
dWn =
√
Δtz
6:
un = un−1 + Δtf (un−1) +
G(un−1)dWn
7: end for
un+1 ≈u(tn+1) given u0 = u(0), from
un+1 = un + runΔt + 𝜎unΔWn,
ΔWn ∶= W(tn+1) −W(tn).
Figure 3.5b top shows a numerical solu-
tion and an explicit solution given from
(3.28), and in Figure 3.5b (bottom), we plot
the error u(tn) −un. We took r = 0.5, 𝜎=
0.5, and Δt = 0.01 and we see a reason-
able agreement between the numerical and
exact solution.
For
deterministic
ODEs
numerical
methods, we often need to impose a
timestep restriction to get the correct
long-time
dynamics,
for
example,
if
u(t) →0 and t →∞, we ask that un →0
as n →∞. In the context of SDEs, such
stability questions are treated in a mean
square sense, that is if E[‖uuu(t)‖2] →0 as
t →∞, we ask E[‖uuun‖2] →0 as n →∞.
To counter stability constraints on the
step size Δt, the drift term can be approx-
imated at fff (uuun+1). This gives an implicit
method
uuun+1 = uuun + fff (uuun+1) Δt +
√
ΔtG(uuun)𝜁𝜁𝜁n.
Note
that
the
noise
term
is
still
approximated at the left-hand end point
and is consistent with the deﬁnition of
the Itô integral. It can be shown that for
stability of the Euler–Maruyama approx-
imation
in
Example 3.13,
we
require
0 < Δt < −2(r + 𝜎2∕2)∕r2.
For
stabil-
ity of numerical methods for SDEs, see
[11–14].
Theorem 3.5 (strong convergence) Let
uuu(t) be the solution of (3.19) and uuun be
the Euler–Maruyama approximation to
uuu(tn). Then the Euler–Maruyama method
converges strongly with order Δt1∕2, that is,
there is a C > 0, independent of Δt such
that
sup
0≤tn≤T
(E[‖uuu(tn) −uuun‖2])1∕2 ≤CΔt1∕2.
If G(uuu) is independent of uuu, then the rate of
convergence is improved and
sup
0≤tn≤T
(E[‖uuu(tn) −uuun‖2])1∕2 ≤CΔt.
The Δt1∕2 rate of strong convergence
for the general case is very slow, and
much slower than that for additive noise.
Higher-order numerical schemes can be
constructed, and the most popular of these
is the Milstein scheme, which is convergent
with order Δt (see Figure 3.7a). In the case
of additive noise, the Euler–Maruyama
and Milstein are equivalent (see (3.46))
and hence we have the improved rate
of convergence in Theorem 3.5 in this
case.
We examine the Milstein method for the
SDE (3.19) with d = m = 1 only. In (3.44),
instead of approximating G at the left-hand
point we now use a Taylor expansion
G(u(t)) = G(u(tn) + u(t) −u(tn)) ≈G(u(tn))
+ G′(u(tn))(u(t) −u(tn)).

3.6 SDEs and Numerical Methods
95
From
the
Euler–Maruyama
approx-
imation, neglecting
the drift,
we can
approximate u(t) −u(tn) for t ∈[tn, tn+1]
by G(u(tn))(W(t) −W(tn)). Thus,
∫
tn+1
tn
G(u(s)) dW(s)
≈∫
tn+1
tn
G(u(tn))dW(s)
+ G′(u(tn))G(u(tn))(W(s)
−W(tn)) dW(s).
Now using the integral (3.16), we ﬁnd
∫
tn+1
tn
G(u(s)) dW(s)
≈G(u(tn)) (W(tn+1) −W(tn))
+1
2G′(u(tn))G(u(tn))
[(W(tn+1) −W(tn))2 −Δt
]
.
(3.46)
Combined with (3.44) and approximating f
as before, (3.46) gives the Milstein method
for SDEs in ℝ
un = un−1 + Δtf (un−1) + G(un−1)dWn
+ 1
2G′(un−1)G(un−1)(ΔW 2
n −Δt),
(see Algorithm 3.3). The scheme can be
extended to general SDEs in ℝd with
noise in ℝm. In general, however, such
higher-order methods require the non-
trivial approximation of iterated stochastic
integrals (see [5]). Numerically, we observe
in Figure 3.7 a strong rate of convergence
of order Δt for the Milstein method.
The
following
theorem
states
the
rate
of
weak
convergence
for
the
Euler–Maruyama and Milstein methods.
Theorem 3.6 (weak convergence)
Suppose that fff and G are C∞functions
and all derivatives of fff and G are bounded.
Suppose
𝜙∶ℝd →ℝ
is
Lipschitz.
Let
Algorithm
3.3
Milstein
method
to
approximate
solution
of
SDE
du = f (u)dt + G(u)dW.
INPUT : Initial data u0, Final time T, N num-
ber of steps to take.
OUTPUT: Vector t of time and vector u such
that un ≈u(tn).
1: Δt = T∕N, t0 = 0
2: for n = 1 ∶N do
3:
tn = nΔt
4:
ﬁnd z ∼N(0, 1)
5:
dWn =
√
Δtz
6:
un = un−1 + Δtf (un−1) +
G(un−1)dWn +
1
2G′(un−1)G(un−1)(dW 2
n −Δt)
7: end for
uuu(t) be the solution of (3.19) and uuun be
either the Euler–Maruyama or Milstein
approximation to uuu(tn). Then there exists
C > 0,independent of Δt, such that
|||E[𝜙(uuu(T))] −E
[
𝜙(uuuN)]||| ≤CΔt.
In Figure 3.7, we observe from numerical
simulations, the weak convergence rate of
O(Δt) for the scheme applied to geometric
Brownian motion (3.24) with 𝜙(u) = u and
T = 1.
3.6.2
Numerical Approximation of Stratonovich
SDEs
The numerical solution of the Stratonovich
SDE (3.35) can be achieved by trans-
forming the equation to an Itô SDE
with a modiﬁed drift ̃fff and applying, for
example, the Euler–Maruyama method.
It is also possible to approximate solu-
tions of Stratonovich SDEs directly. Heun’s
method is the natural equivalent of the
Euler–Maruyama method to Stratonovich
SDEs. Let us consider the Stratonovich SDE
(3.39) and approximate this over a ﬁxed

96
3 Stochastic Diﬀerential Equations
10−4
10−3
10−5
(a)
(b)
10−4
10−3
10−2
Δt
Strong error
10−4
10−3
10−2
10−4
10−3
10−2
Δt
Weak error
Figure 3.7
(a) a loglog plot illustrating strong
convergence of the Euler–Maruyama method
for (3.24) marked by o and for the Milstein
scheme, marked by x. The Euler–Maruyama
method converges with order Δt1∕2, whereas
the Milstein scheme is of order Δt. We also
plot the solution of (3.3) with additive noise
marked by + solved with the Euler–Maruyama
method. Other lines on the plot are reference
lines with slopes 0.5 and 1. We took 1000
realizations to estimate the expectations. In
(b), we illustrate weak convergence of the
Euler–Maruyama method for (3.24) and esti-
mate E[u(1)]. We see an improvement in the
rate of convergence over the strong conver-
gence in (a) for (3.24). We took 10 000 realiza-
tions to estimate the expectations.
time interval [0, T] with ﬁxed step Δt, so
that NΔt = T. We outline the derivation,
uuu(tn+1) = uuu0 + ∫
tn+1
0
fff (uuu(s)) ds
+ ∫
tn+1
0
G(uuu(s)) ∘dW
W
W(s),
and
uuu(tn) = uuu0 + ∫
tn
0
fff (uuu(s)) ds
+ ∫
tn
0
G(uuu(s)) ∘dW
W
W(s).
Subtracting, we see that
uuu(tn+1) = uuu(tn) + ∫
tn+1
tn
fff (uuu(s)) ds
+ ∫
tn+1
tn
G(uuu(s)) ∘dW
W
W(s).
Taking both
fff
and G constant over
[tn, tn+1), we obtain the Heun method. This
time, we evaluate G at the midpoint to be
consistent with the Stratonovich calculus,
that is, we examine
uuun+1 = uuun + fff (uuun) Δt + G(uuun+1∕2)ΔW
W
W n,
ΔW
W
W n ∶= W
W
W(tn+1) −W
W
W(tn).
To implement this, we need, however, an
estimate of G(uuun+1∕2). Let us consider this
with d = m = 1. By Taylor’s theorem on G,
we have
G(un+1∕2) ≈G(u(tn)) + Δt
2 G′(u(tn))
and by a ﬁnite diﬀerence approximation G′,
we get
G(un+1∕2) ≈1
2
(
G(u(tn)) + G(u(tn+1))
)
.
Using the Euler–Maruyama to approxi-
mate u(tn+1), we can get a prediction v for

3.6 SDEs and Numerical Methods
97
u(tn+1):
v = un + f (un)Δt + G(un)ΔWn
and then
un+1 =un+f (un)Δt+ 1
2
(
G(un) + G(v)
)
ΔWn.
Note that the Brownian increment ΔWn is
the same for the predicting step to get v
and the update step to get un+1. The incre-
ments ΔWn ∼N(0, Δt). We can rewrite the
scheme in terms of 𝜁n ∼N(0, 1) as follows:
v = un + f (un)Δt +
√
ΔtG(un)𝜁n
and
un+1 =un+f (un)Δt+ 1
2(G(un) + G(v))
√
Δt𝜁n.
This gives the Heun method to approximate
the Stratonovich SDE, which, in general,
becomes
vvv = uuun + fff (uuun)Δt +
√
ΔtG(uuun)𝜁𝜁𝜁n
and
uuun+1 =uuun+fff (uuun)Δt+ 1
2(G(uuun)+G(vvv))
√
Δt𝜁𝜁𝜁n,
where 𝜁𝜁𝜁n ∼N(0, I) and is given in Algo-
rithm 3.4.
Theorem 3.7 (convergence) Let uuu(t) be
the solution of (3.39) and uuun be the Heun
approximation touuu(tn). Then, in general, the
Heun method converges strongly with order
Δt1∕2, that is, there is a C > 0, independent
of Δt, such that
sup
0≤tn≤T
(E[‖uuu(tn) −uuun‖2])1∕2 ≤CΔt1∕2.
Suppose that fff and G are C∞functions and
all derivatives of fff and G are bounded. Sup-
pose 𝜙∶ℝd →ℝis Lipschitz. Then, there
Algorithm
3.4
Heun
method
to
approximate
solution
of
SDE
du = f (u)dt + G(u) ∘dW.
Input : Initial data u0, Final time T, N num-
ber of steps to take.
Output: Vector t of time and vector u such
that un ≈u(tn).
1: Δt = T∕N, t0 = 0
2: for n = 1 ∶N do
3:
tn = nΔt
4:
ﬁnd z ∼N(0, 1)
5:
dWn =
√
Δtz
6:
v = un−1 + Δtf (un−1) + G(un−1)dWn
7:
un = un−1 + Δtf (un−1) +
1
2(G(un−1) + G(v))dWn
8: end for
exists C > 0, independent of Δt, such that
|||E[𝜙(uuu(T))] −E[𝜙(uuuN)]||| ≤CΔt.
The Heun method is the Stratonovich
equivalent
of
the
Euler–Maruyama
method. As with the Itô case, we can
also derive a Stratonovich Milstein and
other higher-order methods [5].
3.6.3
Multilevel Monte Carlo
Suppose
we
need
to
approximate
E[𝜙(uuu(T))] to a given accuracy 𝜖. To
achieve this, both the weak discretization
error and the Monte Carlo sampling error
should be O(𝜖) in (3.43). We have seen
that the weak discretization error for the
Heun and Euler–Maruyama methods is
O(Δt), so we require Δt = O(𝜖). The Monte
Carlo error is O(1∕
√
M), so we require
M = O(𝜖−2).
We can measure the computational cost
by counting the total number of steps taken
by the numerical method. Finding one sam-
ple ofuuuN requires T∕Δt steps and ﬁnding M

98
3 Stochastic Diﬀerential Equations
samples requires MT∕Δt steps. Thus, with
M = O(𝜖−2) and Δt = O(𝜖), the total cost to
obtain a result with accuracy 𝜖is
cost(𝜇M) = MT
Δt = O(𝜖−3).
(3.47)
A
more
eﬃcient
method,
in
many
cases,
is
the
multilevel
Monte
Carlo
method, which is essentially a form of
variance
reduction.
This
method
uses
a hierarchy of time discretizations Δt𝓁
to compute uuu𝓁≈uuu(tn) on diﬀerent lev-
els 𝓁= L, L −1, … , 𝓁0, so, for example,
Δt𝓁= Δt𝓁−1∕2.
The starting point of the multilevel
Monte Carlo method is the telescoping
sum
E[𝜙(uuuL)] = E[𝜙(uuu𝓁0)]
+
L
∑
𝓁=𝓁0+1
E[𝜙(uuu𝓁) −𝜙(uuu𝓁−1)].
(3.48)
Thus, E[𝜙(uuuL)], the expected value with the
smallest time step ΔtL, is rewritten into
E
[
𝜙(uuu𝓁0)
]
, computed using the largest step
size Δt0 and a sum of correction terms. The
sample average is used to independently
estimate the expectations with a diﬀerent
number of samples on each level.
To estimate E
[
𝜙(uuu𝓁0)] at level 𝓁0, we use
the sample average 𝜇𝓁0 with M𝓁0 indepen-
dent samples, given by
𝜇𝓁0 ∶=
1
M𝓁0
M𝓁0
∑
j0=1
𝜙(uuu
j0
𝓁0).
(3.49)
Similarly, to estimate the correction at level
𝓁= 𝓁0 + 1, … , L, we use the sample aver-
age 𝜇𝓁with M𝓁samples, deﬁned by
𝜇𝓁∶=
1
M𝓁
M𝓁
∑
j𝓁=1
(
𝜙(uuu
j𝓁
𝓁) −𝜙(uuu
j𝓁
𝓁−1
))
.
(3.50)
Note that 𝜙(uuuk
𝓁) −𝜙(uuuk
𝓁−1) is computed
using two diﬀerent time steps Δt𝓁and
Δt𝓁−1 but using the same Brownian path
for uuuk
𝓁and uuuk
𝓁−1. Independent sample paths
are used for each k and for each level 𝓁.
Finally, E[𝜙(uuu(T))] is estimated by
𝜇ML ∶=
L
∑
𝓁=𝓁0
𝜇𝓁.
(3.51)
Because of strong convergence of the
numerical method, the correction terms
should become small and the variance of
these terms decay.
As reappearing in the literature in [15],
the multilevel Monte Carlo method has
received a great deal of interest. It can read-
ily be extended to Milstein or other meth-
ods and also to other noise processes [16,
17]. It has been examined for nonglobal
Lipschitz functions 𝜙[18] and combined
with other variance reduction techniques
[19].
3.7
SDEs and PDEs
The solutions of SDEs are closely related to
some partial diﬀerential equations (PDEs)
and we investigate these now. Suppose we
have that u satisﬁes the Itô SDE (3.19). We
could estimate statistics on the solution
u(t) by direct simulations and using Monte
Carlo techniques (see Chapter 2). Here,
we examine some alternative approaches.
There are two basic types of question we
can ask.
1. Given knowledge of the initial data,
what is the probability of being in a
particular state at time t. Here we are
looking forward.
2. What is the expected value of some
function of the solution at time t as a

3.7 SDEs and PDEs
99
function of the initial data. Here we are
looking backward in time.
We can obtain deterministic PDEs that
help answer these two questions. The main
tool is to use the Itô formula and the mean
zero property of the Itô integrals.
The solution of the SDE (3.19) is a Markov
process (see Chapter 1). This roughly says
the future evolution of u given what has
happened up to time t is the same as when
starting at u(t).
P(u(t + s) ∈A|u(r) ∶0 ≤r ≤t)
= P(u(t + s) ∈A|u(t)).
The
probability
P(u(t) ∈A|u(s) = v)
is
called
the
transition
probability
and
the time-dependent probability density
function or transition density p(u, t; v, s)
satisﬁes
P(u(t) ∈A|u(s) = v) = ∫A
p(u, t; v, s)du.
To be a Markov process, the transition
density
needs
to
satisfy
the
Chap-
man–Kolmogorov equation for s < r < t,
p(u, t; v, s) = ∫ℝ
p(z, r; v, s)p(z, r; u, t)dz.
This states that we can get from v at time s to
u at time t by passing through z at interme-
diate times r. The Chapman–Kolmogorov
equation gives p(u, t; v, s) by integrating
over all possible intermediate z. For further
details, see [2, 6, 20, 21].
3.7.1
Fokker–Planck Equation
We start by looking at the forward problem
where we know the system at time 0 and we
want to have information about the system
at time t > 0. We develop ideas for an Itô
SDE (3.19) with d = m = 1 and show that
p(u, t; u0, 0) satisﬁes the following PDE
𝜕
𝜕t p(u, t; u0, 0)
= −𝜕
𝜕u
(f (u(t))p(u, t; u0, 0)
)
+1
2
𝜕2
𝜕u2
((G(u(t)))2p(u, t; u0, 0)) .
This
is
called
the
(forward)
Fokker–Planck
equation
or
forward
Chapman–Kolmogorov equation.
Sketch of derivation. Given any smooth
function 𝜙we examine Eu0[𝜙(u(t))] ∶=
E[𝜙(u(t))|u(0) = u0
], that is, the expecta-
tion conditional on being at u0 at time 0,
so
Eu0[𝜙(u(t))] = ∫ℝ
𝜙(u(t))p(u, t; u0, 0)du.
From the Itô formula (3.25) and using
that Itô integrals have mean zero (see Deﬁ-
nition 3.2), we ﬁnd that
Eu0[𝜙(u(t))] = Eu0[𝜙(u(s))]
+ Eu0
[
∫
t
0
𝜕𝜙
𝜕u f (u(s))
+ 1
2
𝜕2𝜙
𝜕u2 (G(u(s)))2ds
]
.
We diﬀerentiate with respect to t to get
d
dt Eu0[𝜙(u(t))] = Eu0
[𝜕𝜙
𝜕u f (u(t))
+ 1
2
𝜕𝜙
𝜕u2 (G(u(t)))2
]
.
Now use the deﬁnition of the expectation to
get
∫ℝ
𝜙(u) 𝜕
𝜕t p(u, t; u0, 0)du
= ∫ℝ
(𝜕𝜙
𝜕u f (u(t))
+ 1
2
𝜕2𝜙
𝜕u2 (G(u(t)))2
)
p(u, t; u0, 0)du.

100
3 Stochastic Diﬀerential Equations
If we make assumptions on p →0 as u →
∞, we can use integration by parts once on
𝜕𝜙∕𝜕u fp and twice on 𝜕2𝜙∕𝜕u2Gp to get
∫ℝ
𝜙(u) 𝜕
𝜕t p(u, t; u0, 0)du
= ∫ℝ
𝜙(u)
(
−𝜕
𝜕u(f (u(t))p(u, t; u0, 0))
+ 1
2
𝜕2
𝜕u2
((G(u(t)))2p(u, t; u0, 0)))
du.
The above relation is true for any smooth 𝜙.
Hence, we must have that
𝜕
𝜕t p(u, t; u0, 0)
= −𝜕
𝜕u(f (u(t))p(u, t; u0, 0))
+ 1
2
𝜕2
𝜕u2
((G(u(t)))2p(u, t; u0, 0)) . (3.52)
As initial data, we need p(u, 0; u0, 0) =
𝛿(u0). The solution of the SDE u takes any
value in ℝand so the PDE (3.52) is posed
on ℝ. Sometimes, (3.52) is rewritten with
p = p(u, 0; u0, 0) as
𝜕p
𝜕t = 𝜕J
𝜕x,
where J ∶= fp + 1
2
𝜕
𝜕u(G2p) is a “probability
ﬂux.”
Example 3.14 [Brownian motion] For the
SDE du = dW, f = 0 and G = 1. Hence the
Fokker–Planck equation for the probability
density p satisﬁes the heat equation
𝜕
𝜕t p = 1
2
𝜕2
𝜕u2 p,
with u ∈ℝ. If the initial condition is
p(u, 0) = 𝛿(u0), the solution is given by
p(u, t) = 1∕
√
2𝜋t e−u2∕(2t).
We can use the Fokker–Planck equation
to examine moments deﬁned by
Mn ∶= ∫ℝ
unp(u, t)dx
for
n = 0, 1, 2, … .
Example 3.15 [OU process] For the SDE
du = −𝜆u + 𝜎dW, f = −𝜆u and G = 𝜎.
Here the Fokker–Planck equation for the
probability density p satisﬁes the advection
diﬀusion equation
𝜕
𝜕t p = 𝜆𝜕
𝜕u(up) + 𝜎2
2
𝜕2
𝜕u2 p
u ∈ℝ. Let us use this to look at the ﬁrst
few moments. For n = 0, we integrate the
Fokker–Planck equation over ℝ
∫ℝ
𝜕
𝜕t pdu = 𝜆∫ℝ
𝜕
𝜕u(up)du + ∫ℝ
𝜎2
2
𝜕2
𝜕u2 pdu.
Integration by parts and using the decay
of p as u →±∞gives d∕dt M0 = 0 and so
M0(t) = M0(0) = 1. This is a statement that
probability is conserved.
For n = 1,
∫ℝ
u 𝜕
𝜕t pdu = 𝜆∫ℝ
u 𝜕
𝜕u(up)du
+ ∫ℝ
u𝜎2
2
𝜕2
𝜕u2 pdu.
Once again, integration by parts and decay
of p as u →±∞gives d∕dt M1 = −𝜆M1 and
so the ﬁrst moment decays exponentially
fast, M1(t) = e−𝜆tM1(0). We have already
seen this in (3.30).
For n ≥2, we can proceed in the same
way and get a diﬀerential equation for Mn,
d
dt Mn = −𝜆nMn + 𝜎2
2 n(n −1)Mn−2,
which
has
solution
by
variation
of
constants

3.7 SDEs and PDEs
101
Mn(t) = e−𝜆ntMn(0) + 𝜎2
2 n(n −1)
× ∫
t
0
e−𝜆n(t−s)Mn−2(s)ds.
This method of obtaining equations for
moments can be applied to SDEs in general.
These equations for the moments can then
be solved or simulated to obtain statistics
on the solutions. To obtain a ﬁnite-sized
system, the inﬁnite system of equations for
the moments needs to be approximated by
a ﬁnite set. This is often called moment
closure.
We now state the general form for the
Fokker–Planck equation.
Theorem 3.8 (Fokker–Planck) Consider
the Itô SDE (3.19)
duuu = fff (uuu)dt + G(uuu)dW
W
W(t).
Then the transition probability density p =
p(u, t; u0, 0) satisﬁes
𝜕p
𝜕t = L∗p,
where
L∗p(u) = −
d
∑
i=1
𝜕
𝜕u
(fi(u)p(u))
+ 1
2
d
∑
i,j=1
𝜕2
𝜕ui𝜕uj
((G(u)G(u)T)ijp(u)) .
If the stochastic process u(t) has an invari-
ant density p0, then it satisﬁes L∗p0 = 0.
Another
application
of
the
Fokker–Planck equation is to examine the
long-term time behavior of the transition
density. The SDE is said to have a invari-
ant density if p∞(u) = limt→∞p(u, t; u0, 0)
converges for all initial data u0. For Brow-
nian motion of Example 3.14, we see that
p∞= 0. For the OU process, we see from
Example 3.15
that
p∞(u) = Ce−𝜆u2∕𝜎2,
where C is such that ∫ℝp∞(u)du = 1, that
is, the invariant density of the OU process
is a Gaussian.
Example 3.16 Consider the SDE
du = 𝜕
𝜕u log(q(u))dt +
√
2dW.
This SDE has an invariant density p∞= q.
Let us look at the transition density for this
SDE. From (3.52), we ﬁnd
𝜕
𝜕t p = −𝜕
𝜕u
(
𝜕
𝜕uq1
qp
)
+ 𝜕2
𝜕u2 p
and thus if p = q, we have 𝜕p∕𝜕t = 0. This
idea can be used for Monte Carlo Markov
Chain methods and drawing samples from
given distributions q for Metropolis Hast-
ings algorithms (see, for example, [22]).
Example 3.17 [Langevin equations] Con-
sider a particle in a potential V(u) subject
to some white noise
du = −𝜆𝜕V
𝜕u dt +
√
2𝜎dW
for 𝜆, 𝜎> 0. Assume that V is twice dif-
ferentiable. The Fokker–Planck equation is
given by
𝜕p
𝜕t = 𝜆𝜕
𝜕u
(𝜕V
𝜕u p
)
+ 𝜎𝜕2p
𝜕u2 .
For the steady-state solution, we need the
ﬂux 𝜕V∕𝜕u p + 𝜎𝜕p∕𝜕u to be constant. As
p is assumed to decay to zero at inﬁnity, we
can argue that
𝜕V
𝜕u p + 𝜎𝜕p
𝜕u = 0,
which is solved by p∞(u) = Ce−𝜆V(u)∕𝜎2.

102
3 Stochastic Diﬀerential Equations
Now reconsider the Example 3.4 and the
Langevin equation (3.23). It can be shown
using the Fokker–Planck equation that
the equilibrium distribution has probabil-
ity density function p(q, p) = e−𝛽H(q,p)∕Z
for
H(q, p) = p2∕2 + V(q),
normaliza-
tion constant Z, and inverse temperature
𝛽= 1∕kBT = 2𝜆∕𝜎2.
3.7.2
Backward Fokker–Planck Equation
Now
we
are
interested
in
E[Ψ(u(T))|u(t) = v],
that
is,
we
want
the expected value of some function Ψ of
u(T) at T > t, given that u(t) = v. Let us
consider a simple case. Suppose u satisﬁes
the Itô SDE (3.19) with d = m = 1, so
du = f (u)dt + G(u)dW.
We show that p(u, t)∶=E[Ψ(u(T)|u(t)=v]
satisﬁes the PDE
𝜕
𝜕t p(u, t) = −f (u) 𝜕
𝜕up(u, t) −1
2
𝜕2
𝜕u2 p(u, t),
for
t < T,
p(u, T) = Ψ(u(T)).
(3.53)
3.7.2.1
Sketch of Derivation.
The techniques are essentially the same as
for the forward Fokker–Planck equation.
Let 𝜙(u(t), t) be the solution of (3.53). We
show that 𝜙(u(t), t) = E[Ψ(u(T)|u(t) = v].
We apply the Itô formula (3.25) to
𝜙(u(t), t) to get
d(𝜙(u(s), s))
= 𝜕
𝜕u𝜙du + 1
2
𝜕2
𝜕u2 𝜙du + 𝜕
𝜕s𝜙ds
=
(
𝜕
𝜕s𝜙+ f (u) 𝜕
𝜕u𝜙+ 1
2(G(u))2 𝜕2
𝜕u2 𝜙
)
dt
+ G(u) 𝜕
𝜕u𝜙dW.
Thus, because 𝜙(u(t), t) is solution of (3.53),
we have
𝜙(u(T), T)=𝜙(u(t), t)+∫
T
t
G(u) 𝜕
𝜕u𝜙dW(s).
Taking the expectation and using the prop-
erty that the Itô integral has mean value 0,
we get
E[𝜙(u(T), T)|u(t) = v] = 𝜙(u(t), t).
As the solution of (3.53) is unique, we have
the result.
Example 3.18 [Black–Scholes PDE] Con-
sider the geometric Brownian motion SDE
(3.24). Then p(u, t) = E[𝜙(u(T), T)] satis-
ﬁes the advection diﬀusion equation
𝜕
𝜕t p = −ru 𝜕
𝜕u(p) + 𝜎2
2 u2 𝜕2
𝜕u2 (p),
u ∈ℝ, t ∈[0, T], and p(u, T) = Φ(u(T)).
This is of interest in ﬁnance as it used for
pricing of options at t = 0 with the so-called
payoﬀΨ at time T.
In fact, the backward Fokker–Planck can
also be written for transition probabilities.
Theorem 3.9 (backward Fokker–Planck)
Consider the Itô SDE
duuu = fff (uuu)dt + G(uuu)dW
W
W(t).
The
transition
probability
density
p(u, t; v, s) satisﬁes
𝜕p
𝜕t = Lp,
where
Lp(u) = −
d
∑
i=1
fi(u) 𝜕
𝜕u (p(u))
+ 1
2
d
∑
i,j=1
(G(u)G(u)T)ij
𝜕2
𝜕ui𝜕uj
(p(u)) .

3.7 SDEs and PDEs
103
Remark 3.5 Suppose the SDE
duuu = fff (uuu)dt + G(uuu)dW
W
W(t)
has initial data u(0) = v. Let 𝜙∶ℝd →ℝ
be a bounded (measurable) function. We
can associate with the SDE a family of linear
operators S(t), t ≥0 such that
S(t)𝜙(u) = E[𝜙(u(t)].
The generator of the semigroup S is the oper-
ator L such that
L𝜙(u) = lim
t→0
S(t)𝜙(u) −𝜙(u)
t
.
The operator L∗in Theorem 3.8 is the
adjoint of L.
3.7.2.2
Boundary Conditions
In our discussion of the Fokker–Planck
equations, we assumed that u ∈ℝand we
imposed decay conditions on p(u, t; u0, 0)
as u →±∞. We could also consider impos-
ing some boundaries on u and hence pose
the Fokker–Planck equations on a ﬁnite
(or semi-inﬁnite) domain. Consider, for
example, the Langevin equation for the
motion of a particle in Example 3.17. We
may be interested in the particle arriving
at a certain part of phase space and in
particular the ﬁrst time that it arrives
there. This gives the idea of ﬁrst passage
time.
If the particle is absorbed as it reaches
the boundary, then we get an absorbing
boundary condition so that p(u, t) = 0
on the boundary. For example, if we
have u ∈[a, b] ⊂ℝand the particle gets
absorbed when it reaches either a or b,
then p(a, t) = p(b, t) = 0 (also called a zero
Dirichlet boundary condition). If the parti-
cle is reﬂected, we have reﬂecting boundary
conditions for the PDE and the outward
normal derivative of p is zero at the bound-
ary (n ⋅∇P = 0). In one dimension, with
reﬂecting boundaries at a and b, we have
𝜕p∕𝜕u|u=a = 𝜕p∕𝜕u|u=b = 0.
The backward Fokker–Planck equation
can be used to obtain expressions for the
mean ﬁrst passage times. This is of par-
ticular interest for Langevin-type dynam-
ics of a particle moving in a potential V.
We can seek the ﬁrst exit time from some
domain . For the one-dimensional SDE
in Example 3.17, it can be shown that the
expected ﬁrst passage is of the order e2R∕𝜎2
where R is the minimal potential diﬀer-
ence required to leave . This is known as
Arrhenius’ law. There are a large number of
results in this area (see, for example, [2, 20,
21]). An alternative approach is based on
large deviation theory. Although the sam-
ple paths of the solution of an SDE are
not diﬀerentiable, the idea is that for small
noise they should be close to some smooth
curves that are often taken from the deter-
ministic system. Suppose we have a smooth
curve 𝜓∶[0 ∶T] →ℝd. A rate function J
is introduced that measures the cost of the
sample path u(t) being close to the curve
𝜙over the interval [0, T]. The theory of
Wentzell–Freidlin then gives estimates on
staying close to the mean path. For a review
of large deviation theory, mean ﬁrst passage
times, and Kramer’s method see [2, 3, 6, 21,
23, 24].
3.7.3
Filtering
Filtering is a classic problem of trying to
estimate a function of the state of a noisy
system X(t) at time t where you are only
given noisy observations Y(s), s ≤t up to
time t.
The Kalman–Bucy model assumes that
X and Y are determined by linear SDEs so
that, in one dimension, we have the signal

104
3 Stochastic Diﬀerential Equations
X is the solution of
dX = 𝜆(t)X(t)dt + 𝜎(t)dW1(t),
X(0) = X0,
and the observations satisfy
dY = 𝜇(t)X(t)dt + dW2(t),
Y(0) = Y0.
It is assumed that 𝜆(t), 𝜇(t), and 𝜎(t) sat-
isfy ∫T
0 |a(t)|dt < ∞, a ∈{𝜆, 𝜇, 𝜎}. W1 and
W2 are assumed to be independent Brow-
nian motions. The idea is to estimate the
expected value of X given the observations
Y, m(t) ∶= E[X(t)|Y], and the mean square
error z(t) ∶= E[(X(t) −m(t))2]
Theorem 3.10 (Kalman–Bucy) The
estimate m(t) satisﬁes the SDE
dm = (𝜆(t) −z(t)(𝜇(t))2)m(t)dt + z(t)𝜇(t)dY,
m(0) = E[X0|Y0
],
where z(t) satisﬁes the deterministic Ricatti
equation
dz
dt = 2𝜆(t)z(t) −(z(t))2(𝜇(t))2 + (𝜎(t))2,
z(0) = E[X0 −m(0)].
Furthermore, the solution of the equations is
unique.
For further examples on SDEs and ﬁl-
tering and related problems, see [6] and
reviews [25–28].
Further Reading
SDEs and their mathematical theory are
described in many texts, including [6,
29–31] and from a physical perspective
in [2, 21, 32, 33]. Numerical methods
and their analysis are covered in [5, 34].
The book [4] covers much of the mate-
rial and also considers both random and
stochastic
PDEs
and
their
numerical
simulation.
In place of a Brownian motion W
W
W(t),
other types of stochastic process can be
used to force the SDE and this is currently
an active research topic. For example, we
may consider forcing by a fractional Brow-
nian motion BH(t) in place of W(t). See,
for example, [35] and numerical methods
for such equations in [36]. More gener-
ally, Lyon’s theory of rough paths [37, 38]
allows very general driving terms to be
considered.
This article has only covered some of the
basic ideas related to SDEs. It has not dis-
cussed, for example, the dynamics related
to SDEs. There has been a lot of interest in
phenomena such as stochastic resonance
where the presence of a small amount of
noise in a system can lead to larger scale
eﬀects. This is often through the eﬀects
of noise when the deterministic system is
close to a bifurcation. For more general
random dynamical systems, see [39]. A
good book on noise-induced phenomena
for slow–fast systems, including stochastic
resonance, is [3].
There is a growing interest in stochas-
tically forced partial diﬀerential equations
(SPDEs). The classic theoretical reference is
[40]. More recent expositions on the theory
include [41, 42].
Glossary
expectation Let p(x) be the pdf of X. Then
the expected value of X is given by E[X] =
∫ℝxp(x). This can be approximated by the
sample average.
Gaussian
A
random
variable
X
follows
the
Gaussian
or
normal
dis-
tribution
(X ∼N(𝜇, 𝜎2))
if
its
pdf
is
p(x) = 1∕
√
2𝜋𝜎2 exp(−(x −𝜇)2∕(2𝜎2)).

References
105
independent
If X and Y are real values
with densities pX and pY they are inde-
pendent if and only if the joint probability
density function pX,Y(x, y) = pX(x), pY(y),
x, y ∈ℝ. Independent random variables
are uncorrelated.
multivariate Gaussian
X ∈ℝd is a mul-
tivariate Gaussian if it follows the distri-
bution N(𝜇𝜇𝜇, C) where C is a covariance
matrix. Gaussian processes are uniquely
determined by their mean and their covari-
ance functions.
probability density function (pdf)
is
the function such that P(B) = ∫B p(x)dx, so
P(X ∈(a, b)) = ∫b
a p(x)dx.
probability space
(Ω, , P) consists of a
sample space Ω, a set of events and a
probability measure P.
sample path/realisation
For a ﬁxed 𝜔∈
Ω a X(t, 𝜔) is sample path.
stochastic process is a collection of ran-
dom variables X(t) that represents the evo-
lution of a system over time that depends on
the probability space (Ω, , P). To empha-
size dependence on the probability space
X(t) is sometimes written as X(t, 𝜔), 𝜔∈Ω.
uncorrelated
If Cov (X, Y) = 0 then the
random variables X and Y are uncorrelated.
References
1. Hänggi, P. and Jung, P. (1995) Colored noise
in dynamical systems. Adv. Chem. Phys.,
239–326.
2. Gardiner, C. (2009) Stochastic Methods: A
Handbook for the Natural and Social
Sciences, Springer Series in Synergetics, 4th
edn, Springer, Berlin.
3. Berglund, N. and Gentz, B. (2006)
Noise-Induced Phenomena in Slow-Fast
Dynamical Systems. Probability and its
Applications (New York). Springer-Verlag
London Ltd, London, A sample-paths
approach.
4. Lord, G.J., Powell, C.E., and Shardlow, T.
(2014) Introduction to Computational
Stochastic Partial Diﬀerential Equations,
CUP.
5. Peter, E. and Eckhard Platen, K. (1992)
Numerical Solution of Stochastic Diﬀerential
Equations, Applications of Mathematics, vol.
23, Springer.
6. Øksendal, B. (2003) Stochastic Diﬀerential
Equations, 6th edn, Universitext Springer,
Berlin.
7. Kloeden, P.E. and Neuenkirch, A. (2007) The
pathwise convergence of approximation
schemes for stochastic diﬀerential equations.
LMS J. Comput. Math., 10, 235–253.
8. Jentzen, A., Kloeden, P.E., and Neuenkirch,
A. (2008) Pathwise convergence of
numerical schemes for random and
stochastic diﬀerential equations, in
Foundations of Computational Mathematics,
London Mathematical Society Lecture Note
Series, vol. 363 (ed. H. Kong), Cambridge
University Press, pp. 140–161.
9. Jentzen, A., Kloeden, P.E., and Neuenkirch,
A. (2009) Pathwise approximation of
stochastic diﬀerential equations on domains:
higher order convergence rates without
global Lipschitz coeﬃcients. Numer. Math.,
112(1), 41–64, doi: 10.1007/s00211-008-
0200-8. URL http://dx.doi.org/10.1007/
s00211-008-0200-8.
10. Williams, D. (1991) Probability with
Martingales, Cambridge University Press.
11. Buckwar, E. and Sickenberger, T. (2011) A
comparative linear mean-square stability
analysis of Maruyama- and Milstein-type
methods. Math. Comput. Simulat., 81(6),
1110–1127.
12. Higham, D.J., Mao, X., and Stuart, A.M.
(2003) Exponential mean-square stability of
numerical solutions to stochastic diﬀerential
equations. London Math. Soc. J. Comput.
Math., 6, 297–313.
13. Burrage, K., Burrage, P., and Mitsui, T. (2000)
Numerical solutions of stochastic diﬀerential
equations — implementation and stability
issues. J. Comput. Appl. Math., 125(1–2),
171–182, doi: 10.1016/S0377-0427(00)
00467-2. URL http://dx.doi.org/10.1016/
S0377-0427(00)00467-2.
14. Higham, D.J. (2000) Mean-square and
asymptotic stability of the stochastic theta
method. SIAM J. Numer. Anal., 38(3),
753–769, doi: 10.1137/S003614299834736X.

106
3 Stochastic Diﬀerential Equations
URL http://dx.doi.org/10.1137/S0036142
99834736X.
15. Giles, M.B. (2008) Multilevel Monte Carlo
path simulation. Oper. Res., 56(3), 607–617,
doi: 10.1287/opre.1070.0496. URL
http://dx.doi.org/10.1287/opre.1070.0496.
16. Giles, M.B. (2008) Improved multilevel
Monte Carlo convergence using the Milstein
scheme, in Monte Carlo and Quasi-Monte
Carlo Methods 2006, Springer, Berlin, pp.
343–358, doi: 10.1007/978-3-540-74496-
2_20. URL http://dx.doi.org/10.1007/978-3-
540-74496-2_20.
17. Dereich, S. (2011) Multilevel Monte Carlo
algorithms for Lévy-driven SDEs with
Gaussian correction. Ann. Appl. Probab.,
21(1), 283–311, doi: 10.1214/10-AAP695.
URL http://dx.doi.org/10.1214/10-AAP695.
18. Giles, M.B., Higham, D.J., and Mao, X.
(2009) Analysing multi-level Monte Carlo
for options with non-globally Lipschitz
payoﬀ. Finance Stoch., 13(3), 403–413, doi:
10.1007/s00780-009-0092-1. URL
http://dx.doi.org/10.1007/
s00780-009-0092-1.
19. Giles, M.B. and Waterhouse, B.J. (2009)
Multilevel quasi-Monte Carlo path
simulation, in Advanced Financial
Modelling, Radon Series on Computational
and Applied Mathematics, vol. 8 (eds H.
Abrecher, W.J. Runggaldier, and W.
Schachermayer), Walter de Gruyter, Berlin,
pp. 165–181, doi: 10.1515/9783110213
140.165. URL http://dx.doi.org/10.1515/
9783110213140.165.
20. Risken, H. (1996) The Fokker-Planck
Equation: Methods of Solution and
Applications, Lecture Notes in Mathematics,
Springer-Verlag.
21. van Kampen, N.G. (1997) Stochastic
Processes in Physics and Chemistry, 2nd edn,
North–Holland.
22. Lelièvre, T., Stoltz, G., and Rousset, M.
(2010) Free Energy Computations: A
Mathematical Perspective, Imperial College
Press.
23. Freidlin, I. and Wentzell, A.D. (1998)
Random Perturbations of Dynamical
Systems, Die Grundlehren der
mathematischen Wissenschaften,
Springer-Verlag.
24. Varadhan, S.R.S. (1984) Large Deviations
and Applications, CBMS-NSF Regional
Conference Series in Applied Mathematics,
Society for Industrial and Applied
Mathematics.
25. Rozovski˘ı, B.L. (1990) Stochastic Evolution
Systems, Mathematics and its Applications
(Soviet Series), vol. 35, Kluwer Academic
Publishers Group, Dordrecht.
26. Bain, A. and Crisan, D. (2009) Fundamentals
of Stochastic Filtering, Stochastic Modelling
and Applied Probability, vol. 60, Springer,
New York.
27. Stuart, A.M. (2010) Inverse problems: a
Bayesian perspective. Acta Numer., 19,
451–559, doi: 10.1017/S0962492910000061.
URL http://dx.doi.org/10.1017/S09624
92910000061.
28. Crisan, D. and Rozovski˘ı, B. (2011)
Introduction, in The Oxford Handbook of
Nonlinear Filtering, Oxford University Press,
Oxford, pp. 1–15.
29. Karatzas, I. and Shreve, S.E. (1991) Brownian
Motion and Stochastic Calculus, Graduate
Texts in Mathematics, vol. 113, 2nd edn,
Springer, New York.
30. Mao, X. (2008) Stochastic Diﬀerential
Equations and Applications, 2nd edn,
Horwood.
31. Protter, P.E. (2005) Stochastic Integration
and Diﬀerential Equations, Stochastic
Modelling and Applied Probability, vol. 21,
2nd edn, Springer.
32. Öttinger, H.C. (1996) Stochastic Processes in
Polymeric Fluids, Springer, Berlin.
33. Nelson, E. (1967) Dynamical Theories of
Brownian Motion, Princeton University
Press.
34. Milstein, G.N. and Tretyakov, M.V. (2004)
Stochastic Numerics for Mathematical
Physics, Springer, Berlin.
35. Mishura, Y.S. (2008) Stochastic Calculus for
Fractional Brownian Motion and Related
Processes, Lecture Notes in Mathematics, vol.
1929, Springer, Berlin, doi: 10.1007/978-3-
540-75873-0. URL http://dx.doi.org/10.1007/
978-3-540-75873-0.
36. Neuenkirch, A. (2008) Optimal pointwise
approximation of stochastic diﬀerential
equations driven by fractional Brownian
motion. Stochastic Process. Appl., 118(12),
2294–2333, doi: 10.1016/j.spa.2008.01.002.
URL http://dx.doi.org/10.1016/j.spa.
2008.01.002.
37. Friz, P.K. and Victoir, N.B. (2010)
Multidimensional Stochastic Processes as
Rough Paths, Cambridge Studies in

References
107
Advanced Mathematics, vol. 120, Cambridge
University Press.
38. Davie, A.M. (2007) Diﬀerential equations
driven by rough paths: an approach via
discrete approximation. Appl. Math. Res.
Express., 2, 40.
39. Arnold, L. (1998) Random Dynamical
Systems, Monographs in Mathematics,
Springer.
40. Da Prato, G. and Zabczyk, J. (1992)
Stochastic Equations in Inﬁnite Dimensions,
Encyclopedia of Mathematics and its
Applications, vol. 44, Cambridge University
Press.
41. Prévôt, C. and Röckner, M. (2007) A Concise
Course on Stochastic Partial Diﬀerential
Equations, Lecture Notes in Mathematics,
vol. 1905, Springer, Berlin.
42. Chow, P.-L. (2007) Stochastic Partial
Diﬀerential Equations, Chapman &
Hall/CRC, Boca Raton, FL.


109
Part II
Discrete Mathematics, Geometry, Topology
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.


111
4
Graph and Network Theory
Ernesto Estrada
4.1
Introduction
Graph Theory was born in 1736 when
Leonhard Euler [1] published “Solutio
problematic as geometriam situs pertinen-
tis” (The solution of a problem relating to
the theory of position). This history is well
documented [2] and widely available in
any textbook of graph or network theory.
However, the word graph appeared for the
ﬁrst time in the context of natural sciences
in 1878, when the English mathemati-
cian James J. Sylvester [3] wrote a paper
entitled “Chemistry and Algebra” which
was published in Nature, where he wrote
that “Every invariant and covariant thus
becomes expressible by a graph precisely
identical with a Kekulean diagram or
chemicograph.” The use of graph theory in
condensed matter physics, pioneered by
many chemical and physical graph theo-
rists [4, 5], is today well established; it has
become even more popular after the recent
discovery of graphene.
There are few, if any, areas of physics in
the twenty-ﬁrst century in which graphs
and networks are not involved directly or
indirectly. Hence it is impossible to cover
all of them in this chapter. Thus I owe the
reader an apology for the incompleteness
of this chapter and a promise to write
a more complete treatise. For instance,
quantum graphs are not considered in
this chapter and the reader is referred to
a recent introductory monograph on this
topic for details [6]. In this chapter, we
will cover some of the most important
areas of applications of graph theory in
physics. These include condensed mat-
ter physics, statistical physics, quantum
electrodynamics,
electrical
networks,
and vibrational problems. In the second
part, we summarize some of the most
important aspects of the study of com-
plex networks. This is an interdisciplinary
area that has emerged with tremendous
impetus in the twenty-ﬁrst century and
that studies networks appearing in com-
plex systems. These systems range from
molecular and biological to ecological,
social, and technological systems. Thus
graph theory and network theory have
helped to broaden the horizons of physics
to embrace the study of new complex
systems.
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

112
4 Graph and Network Theory
We hope this chapter motivates the
reader to ﬁnd more about the connec-
tions between graph/network theory and
physics, consolidating this discipline as an
important part of the curriculum for the
physicists of the twenty-ﬁrst century.
4.2
The Language of Graphs and Networks
The ﬁrst thing that needs to be clariﬁed
is that the terms graphs and networks are
used indistinctly in the literature. In this
chapter, we will reserve the term graph for
the abstract mathematical concept, in gen-
eral referred to small, artiﬁcial formations
of nodes and edges. The term network is
then reserved for the graphs representing
real-world objects in which the nodes rep-
resent entities of the system and the edges
represent the relationships among them.
Therefore, it is clear that we will refer to the
system of individuals and their interactions
as a “social network” and not as a “social
graph.” However, they should mean exactly
the same.
For the basic concepts of graph theory,
the reader is recommended to consult
the introductory book by Harary [7]. We
start by deﬁning a graph formally. Let us
consider a ﬁnite set V = {v1, v2, … , vn
}
of unspeciﬁed elements and let V ⊗V be
the set of all ordered pairs [vi, vj
] of the
elements of V. A relation on the set V is
any subset E ⊆V ⊗V. The relation E is
symmetric if [vi, vj
] ∈E implies [vj, vi
] ∈E
and it is reﬂexive if ∀v ∈V, [v, v] ∈E. The
relation E is antireﬂexive if [vi, vj
] ∈E
implies vi ≠vj. Now we can deﬁne a simple
graph as the pair G = (V, E), where V is a
ﬁnite set of nodes, vertices, or points and
E is a symmetric and antireﬂexive relation
on V, whose elements are known as the
edges or links of the graph. In a directed
graph, the relation E is nonsymmetric. In
many physical applications, the edges of
the graphs are required to support weights,
that is, real numbers indicating a speciﬁc
property of the edge. In this case, the
following more general deﬁnition is conve-
nient. A weighted graph is the quadruple
G = (V, E, W, f ) where V is a ﬁnite set
of
nodes,
E ⊆V ⊗V = {e1, e2, … , em
}
is a set of edges, W = {w1, w2, … , wr
}
is a set of weights such that wi ∈ℝ, and
f ∶E →W is a surjective mapping that
assigns a weight to each edge. If the weights
are natural numbers, then the resulting
graph is a multigraph in which there could
be multiple edges between pairs of vertices;
that is, if the weight between nodes p and
q is k ∈N, it means that there are k links
between the two nodes.
In an undirected graph, we say that two
nodes p and q are adjacent if they are joined
by an edge e = {p, q}. In this case, we say
that the nodes p and q are incident to the
link e, and the link e is incident to the
nodes p and q. The two nodes are called
the end nodes of the edge. Two edges e1 =
{p, q} and e2 = {r, s} are adjacent if they are
both incident to at least one node. A sim-
ple but important characteristic of a node
is its degree, which is deﬁned as the num-
ber of edges that are incident to it or equiv-
alently as the number of nodes adjacent
to it. Slightly diﬀerent deﬁnitions apply to
directed graphs. The node p is adjacent to
node q if there is a directed link from p to
q, e = (p, q). We also say that a link from p
to q is incident from p and incident to q;
p is incident to e and q is incident from e.
Consequently, we have two diﬀerent kinds
of degrees in directed graphs. The in-degree
of a node is the number of links incident to
it and its out-degree is the number of links
incident from it.

4.2 The Language of Graphs and Networks
113
4.2.1
Graph Operators
The incidence and adjacency relations in
graphs allow us to deﬁne the following
graph operators. We consider an undi-
rected graph for which we construct its
incidence matrix with an arbitrary ori-
entation of its entries. This is necessary
to consider that the incidence matrix is
a discrete analogue of the gradient; that
is, for every edge {p, q}, p is the positive
(head) and q the negative (tail) end of the
oriented link. Let the links of the graph be
labeled as e1, e2, … , em. Hence the oriented
incidence matrix ∇(G) is
∇ij(G) =
⎧
⎪
⎨
⎪⎩
+1
node vi is the head of link ej
−1
node vi is the tail of link ej
0
otherwise
.
We remark that the results obtained below
are independent of the orientation of the
links but assume that once the links are ori-
ented, this orientation is not changed. Let
the vertex LV and edge LE spaces be the
vector spaces of all real-valued functions
deﬁned on V and E, respectively. The inci-
dence operator of the graph is then deﬁned
as
∇(G) ∶LV →LE,
(4.1)
such that for an arbitrary function f ∶V →
ℝ, ∇(G)f ∶E →ℜis given by
(
∇(G)f
)
(e) = f (p) −f (q),
(4.2)
where p are the starting (head) and q the
ending (tail) points of the oriented link e.
Here we consider that f is a real or vector-
valued function on the graph with |f | being
𝜇-measurable for certain measure 𝜇on the
graph.
On the other hand, let H be a Hilbert
space with scalar product ⟨⋅, ⋅⟩and norm
‖⋅‖. Let G = (V, E) be a simple graph. The
adjacency operator is an operator acting on
the Hilbert space H ∶= l2 (V) deﬁned as
(Af )
(p) ∶=
∑
u,v∈E
f (q), f ∈H, i ∈V.
(4.3)
The adjacency operator of an undirected
network is a self-adjoint operator, which
is bounded on l2 (V). We recall that l2
is the Hilbert space of square summable
sequences with inner product, and that
an operator is self-adjoint if its matrix is
equal to its own conjugate transpose, that
is, it is Hermitian. It is worth pointing
out here that the adjacency operator of a
directed network might not be self-adjoint.
The matrix representation of this operator
is the adjacency matrix A, which, for a sim-
ple graph, is deﬁned as
Aij =
{ 1
if i, j ∈E
0
otherwise.
(4.4)
A third operator related to the previous
two, which plays a fundamental role in the
applications of graph theory in physics is
the Laplacian operator. This operator is
deﬁned by
L(G)f = −∇⋅(∇f ) ,
(4.5)
and it is the graph version of the Laplacian
operator
Δf = 𝜕2f
𝜕x12 + 𝜕2f
𝜕x22 + · · · + 𝜕2f
𝜕xn2 .
(4.6)
The negative sign in (4.5) is used by conven-
tion. Then the Laplacian operator acting on
the function f previously deﬁned is given by
(
L(G)f
)
(u) =
∑
{u,v}∈E
[
f (u) −f (v)
]
,
(4.7)

114
4 Graph and Network Theory
which in matrix form is given by
Luv(G) =
∑
e∈E
∇eu∇ev =
⎧
⎪
⎨
⎪⎩
−1
ku
0
if uv ∈E,
if u = v,
otherwise.
(4.8)
Using the degree matrix K, which is a diago-
nal matrix of the degrees of the nodes in the
graph, the Laplacian and adjacency matri-
ces of a graph are related by
L = K −A.
(4.9)
4.2.2
General Graph Concepts
Other important general concepts of graph
theory that are fundamental for the study
of graphs and networks in physics are the
following. Two graphs G1 and G2 are iso-
morphic if there is a one-to-one correspon-
dence between the nodes of G1 and those
of G2, such as, the number of edges joining
each pair of nodes in G1 is equal to that join-
ing the corresponding pair of nodes in G2.
If the graphs are directed, the edges must
coincide not only in number but also in
direction. The graph S = (V ′, E′) is a sub-
graph of a graph G = (V, E) if, and only if,
V ′ ⊆V and E′ ⊆E. The clique is a partic-
ular kind of subgraph, which is a maximal
complete subgraph of a graph. A complete
graph is one in which every pair of nodes
are connected. A (directed) walk of length L
from v1 to vL+1 is any sequence of (not nec-
essarily diﬀerent) nodes v1, v2, … , vL, vL+1
such that for each i = 1, 2, … , L there is link
from vi to vi+1. A walk is closed (CW) if
vL+1 = v1. A particular kind of walk is the
path of length L, which is a walk of length
L in which all the nodes (and all the edges)
are distinct. A trial has all the links diﬀerent
but not necessarily all the nodes. A cycle is a
closed walk in which all the edges and all the
nodes (except the ﬁrst and last) are distinct.
The girth of the graph is the size (number of
nodes) of the minimum cycle in the graph.
A graph is connected if there is a path
between any pair of nodes in the graph.
Otherwise, it is disconnected. Every con-
nected subgraph is a connected component
of the graph. The analogous concept in a
directed graph is that of a strongly con-
nected graph. A directed graph is strongly
connected if there is a directed path
between each pair of nodes. The strongly
connected
components
of
a
directed
graph are its maximal strongly connected
subgraphs.
In an undirected graph, the shortest-path
distance d (p, q) = dpq is the number of
edges in the shortest path between the
nodes p and q in the graph. If p and q
are in diﬀerent connected components of
the graph the distance between them is
set to inﬁnite, d (p, q) ∶= ∞. In a directed
graph, it is typical to consider the directed
distance ⃗d (p, q) between a pair of nodes p
and q as the length of the directed shortest
path from p to q. However, in general
⃗d (p, q) ≠⃗d (q, p), which violates the sym-
metry property of a metric, so that ⃗d (p, q)
is not a distance but a pseudo-distance or
a pseudo-metric. The distance between all
pairs of nodes in a graph can be arranged in
a distance matrix D, which, for undirected
graphs, is a square symmetric matrix. The
maximum entry for a given row/column
of the distance matrix of an undirected
(strongly connected directed) graph is
known as the eccentricity e (p) of the node
p, e (p) = max
x∈V(G) {d (p, x)}. The maximum
eccentricity among the nodes of a graph
is the diameter of the graph, which is
diam(G) = max
x,y∈V(G)
{d (x, y)}. The average
path length l of a graph with n nodes is
l =
1
n (n −1)
∑
x,y
d (x, y).
(4.10)

4.3 Graphs in Condensed Matter Physics
115
An important measure for the study of
networks was introduced by Watts and
Strogatz [8] as a way of quantifying how
clustered a node is. For a given node i,
the clustering coeﬃcient is the number of
triangles connected to this node ||C3(i)||
divided by the number of triples centered
on it
Ci =
2 ||C3(i)||
ki
(
ki −1
),
(4.11)
where ki is the degree of the node. The
average value of the clustering for all nodes
in a network C
C = 1
n
n
∑
i=1
Ci
(4.12)
has been extensively used in the analysis of
complex networks (see Section 4.9 of this
chapter).
A second clustering coeﬃcient has been
introduced as a global characterization of
network cliquishness [9]. This index, which
is also known as network transitivity, is
deﬁned as the ratio of three times the num-
ber of triangles divided by the number of
connected triples (two paths):
C = 3 ||C3||
||P2||
.
(4.13)
4.2.3
Types of Graphs
The simplest type of graph is the tree. A tree
of n nodes is a graph that is connected and
has no cycles. The simplest tree is the path
Pn. The path (also known as linear path
or chain) is the tree of n nodes, n −2 of
which have degree 2 and two nodes have
degree 1. For any kind of graph, we can
ﬁnd a spanning tree, which is a subgraph
of this graph that includes every node and
is a tree. A forest is a disconnected graph
in which every connected component is a
tree. A spanning forest is a subgraph of the
graph that includes every node and is a
forest.
An r-regular graph is a graph with rn∕2
edges in which all nodes have degree r.
A particular case of a regular graph is
the complete graph previously deﬁned.
Another type of regular graph is the cycle,
which is a regular graph of degree 2, that
is, a 2-regular graph, denoted by Cn. The
complement of a graph G is the graph G
with the same set of nodes as G but two
nodes in G are connected if, and only if,
they are not connected in G. An empty or
trivial graph is a graph with no links. It is
denoted as Kn as it is the complement of
the complete graph.
A graph is bipartite if its nodes can be
split into two disjoint (nonempty) subsets
V1 ⊂V (V1 ≠𝜙) and V2 ⊂V (V2 ≠𝜙) and
V1 ∪V2 = V, such that each edge joins a
node in V1 and a node in V2. Bipartite
graphs do not contain cycles of odd length.
If all nodes in V1 are connected to all nodes
in V2 the graph is known as a complete
bipartite graph, denoted by Kn1,n2, where
n1 = ||V1|| and n2 = ||V2|| are the number of
nodes in V1 and V2, respectively. Finally, a
graph is planar if it can be drawn in a plane
in such a way that no two edges intersect
except at a node with which they are both
incident.
4.3
Graphs in Condensed Matter Physics
4.3.1
Tight-Binding Models
In condensed matter physics, it is usual to
describe solid-state and molecular systems
by considering the interaction between N
electrons whose behavior is determined by

116
4 Graph and Network Theory
a Hamiltonian of the following form:
H=
N
∑
n=1
[
−
ℏ2∇2
n
2m +U(rn
)+ 1
2
∑
m≠n
V (rn−rm
)
]
,
(4.14)
where U (rn
) is an external potential and
V (rn −rm
) is the potential describing the
interactions between electrons. Using the
second quantization formalism of quantum
mechanics, this Hamiltonian can be written
as
̂H = −
∑
ij
tiĵc†
i ̂cj + 1
2
∑
ijkl
Vijkl̂c†
i ̂c†
k̂cl̂cj, (4.15)
where ̂c†
i and ̂ci are “ladder operators,” tij
and Vijkl are integrals that control the hop-
ping of an electron from one site to another
and the interaction between electrons,
respectively. They are usually calculated
directly from ﬁnite basis sets [10].
In the tight-binding approach for study-
ing solids and certain classes of molecules,
the
interaction
between
electrons
is
neglected
and
Vijkl = 0, ∀i, j, k, l.
This
method, which is known as the Hückel
molecular orbital method in chemistry,
can be seen as very drastic in its approx-
imation, but let us think of the physical
picture behind it [11, 12]. We concentrate
our discussion on alternant conjugated
molecules in which single and double
bonds alternate. Consider a molecule like
benzene in which every carbon atom has an
sp2 hybridization. The frontal overlapping
sp2 −sp2 of adjacent carbon atoms creates
very stable œ-bonds, while the lateral
overlapping p–p between adjacent carbon
atoms creates very labile ß-bonds. Thus it
is clear from the reactivity of this molecule
that a œ −ß separation is plausible and we
can consider that our basis set consists of
orbitals centered on the particular carbon
atoms in such a way that there is only one
orbital per spin state at each site. Then we
can write the Hamiltonian of the system
as
̂Htb = −
∑
ij
tiĵc†
i𝜌̂ci𝜌,
(4.16)
where ̂c(†)
i𝜌creates (annihilates) an electron
with spin 𝜌in a ß (or other) orbital centered
at the atom i. We can now separate the in-
site energy 𝛼i from the transfer energy 𝛽ij
and write the Hamiltonian as
̂Htb =
∑
ij
𝛼îc†
i𝜌̂ci𝜌+
∑
⟨ij⟩𝜌
𝛽iĵc†
i𝜌̂ci𝜌,
(4.17)
where the second sum is carried out over all
pairs of nearest neighbors. Consequently,
in a molecule or solid with N atoms the
Hamiltonian equation (4.16) is reduced to
an N × N matrix,
Hij =
⎧
⎪
⎨
⎪⎩
𝛼i
if
i = j
𝛽ij
if
i isconnectedto j
0
otherwise.
(4.18)
Owing to the homogeneous geometrical
and electronic conﬁguration of many sys-
tems analyzed by this method, we may
take 𝛼i = 𝛼, ∀i (Fermi energy) and 𝛽ij = 𝛽≈
−2.70eV for all pairs of connected atoms.
Thus,
H = 𝛼I + 𝛽A,
(4.19)
where I is the identity matrix and A is the
adjacency matrix of the graph representing
the carbon skeleton of the molecule. The
Hamiltonian and the adjacency matrix of
the graph have the same eigenfunctions 𝜑j
and their eigenvalues are simply related by
H𝜑j =EjA, A𝜑j = 𝜆jHEj = 𝛼+ 𝛽𝜆j.
(4.20)
Hence, everything we have to do in the
analysis of the electronic structure of
molecules or solids that can be represented
by a tight-binding Hamiltonian is to study

4.3 Graphs in Condensed Matter Physics
117
the spectra of the graphs associated with
them. The study of spectral properties of
graphs represents an entire area of research
in algebraic graph theory. The spectrum
of a matrix is the set of eigenvalues of the
matrix together with their multiplicities.
For the case of the adjacency matrix, let
𝜆1(A) ≥𝜆2(A) ≥· · · ≥𝜆n(A) be the dis-
tinct eigenvalues of A and let m(𝜆1(A)),
m(𝜆2(A)), … , m(𝜆n(A)) be their algebraic
multiplicities, that is, the number of times
each of them appears as an eigenvalue
of A. Then the spectrum of A can be
written as
SpA =
(
𝜆1 (A)
𝜆2 (A)
· · ·
𝜆n (A)
m (𝜆1 (A)
)
m (𝜆2 (A)
)
· · ·
m (𝜆n (A)
)
)
.
(4.21)
The total ß (molecular) energy is given by
E = 𝛼ne + 𝛽
n
∑
j=1
gj𝜆j,
(4.22)
where ne is the number of ß-electrons in
the molecule and gj is the occupation num-
ber of the jth molecular orbital. For neutral
conjugated systems in their ground state,
we have [13]
E=
⎧
⎪
⎪
⎨
⎪
⎪⎩
2
n∕2
∑
j=1
𝜆j
n even,
2
(n+1)∕2
∑
j=1
𝜆j + 𝜆(j+1)∕2
n odd.
(4.23)
Because an alternant conjugated hydro-
carbon has a bipartite molecular graph,
𝜆j = −𝜆n−j+1 for all j = 1, 2, … , n. In a few
molecular systems, the spectrum of the
adjacency matrix is known. For instance
[11], we have
1. Polyenes CnHn+2
𝜆j (A) = 2 cos
( 𝜋j
n + 1
)
,
j = 1, … , n,
(4.24)
2. Cyclic polyenes CnHn
𝜆j (A) = 2 cos
(2𝜋j
n
)
,
j = 1, … , n, 𝜆j = 𝜆n−j
(4.25)
3. Polyacenes
N = 1
N = 2
N = 3
𝜆r (A) = 1; 𝜆s (A) = −1;
𝜆k (A) = ±1
2
{
1 ± 9 + 8 cos
k𝜋
N + 1
}
,
k = 1, … , N
(4.26)
A few bounds exist for the total energy
of systems represented by graphs with n
vertices and m edges. For instance,
√
2m + n (n −1) (det A)n∕2 ≤E ≤
√
mn
(4.27)
and if G is a bipartite graph with n vertices
and m edges, then
E ≤4m
n +
√
(n −2)
(
2m −8m2
n2
)
. (4.28)
4.3.1.1
Nullity and Zero-Energy States
Another characteristic of a graph that is
related to an important molecular property
is the nullity. The nullity of a graph, denoted
by 𝜂= 𝜂(G), is the algebraic multiplicity
of the zero eigenvalue in the spectrum of

118
4 Graph and Network Theory
the adjacency matrix of the graph [14].
This property is very relevant for the sta-
bility of alternant unsaturated conjugated
hydrocarbons. An alternant unsaturated
conjugated hydrocarbon with 𝜂= 0 is
predicted to have a closed-shell electron
conﬁguration. Otherwise, the respective
molecule is predicted to have an open-shell
electron conﬁguration, that is, when 𝜂> 0,
the molecule has unpaired electrons in
the form of radicals that are relevant for
several electronic and magnetic properties
of materials. In a molecule with an even
number of atoms, 𝜂is either zero or it is an
even positive integer.
The following are a few important facts
about the nullity of graphs. Let M = M(G)
be the size of the maximum matching of
a graph, that is, the maximum number of
mutually nonadjacent edges of G. Let T be
a tree with n ≥1 vertices. Then,
𝜂(T) = n −2M.
(4.29)
If G is a bipartite graph with n ≥1 vertices
and no cycle of length 4s (s = 1, 2, …), then
𝜂(G) = n −2M.
(4.30)
Also for a bipartite graph G with incidence
matrix ∇, 𝜂(G) = n −2r (∇), where r (∇) is
the rank of ∇= ∇(G). In the particular case
of benzenoid graphs Bz, which may contain
cycles of length 4s, the nullity is given by
𝜂(Bz) = n −2M.
(4.31)
The following are some known bounds for
the nullity of graphs [15]. Let G be a graph
with n vertices and at least one cycle,
𝜂(G)≤
{n−2g(G) + 2g(G) ≡0
(mod 4),
n−2g(G)
otherwise,
(4.32)
where g(G) is the girth of the graph.
If there is a path of length d (p, q) between
the vertices p and q of G
𝜂(G) ≤
{n −d(p, q)
if d (p, q) is even,
n −d (p, q) −1
otherwise.
(4.33)
Let G be a simple connected graph of diam-
eter D. Then
𝜂(G) ≤
{ n −D
if D is even,
n −D −1
otherwise.
(4.34)
4.3.2
Hubbard Model
Let us now consider one of the most
important models in theoretical physics:
the Hubbard model. This model accounts
for the quantum-mechanical motion of
electrons in a solid or conjugated hydro-
carbon and includes nonlinear repulsive
interactions between electrons. In brief,
the interest in this model is due to the fact
that it exhibits various interesting phenom-
ena including metal–insulator transition,
antiferromagnetism, ferrimagnetism, fer-
romagnetism, Tomonaga–Luttinger liquid,
and superconductivity [16].
The Hubbard model can be seen as an
extension of the tight-binding Hamiltonian
we have studied in the previous section in
which we introduce the electron–electron
interactions. To keep things simple, we
allow onsite interactions only, that is, we
consider one orbital per site and Vijkl ≠0
in (4.15) if, and only if, i, j, k, and l all
refer to the same orbital. In this case, the
Hamiltonian is written as
H = −t
∑
i,j,𝜎
Aiĵc†
i𝜎̂cj𝜎+ U
∑
i
̂c†
i↑̂ci↑̂c†
i↓̂ci↓,
(4.35)
where t is the hopping parameter and U >
0 indicates that the electrons repel each
other.

4.3 Graphs in Condensed Matter Physics
119
(a)
(b)
Figure 4.1
Representation of two graphene nanoﬂakes
with closed-shell (a) and open-shell (b) electronic
conﬁgurations.
Notice
that
if
there
is
no
elec-
tron–electron
repulsion
(U = 0),
we
recover
the
tight-binding
Hamiltonian
studied in the previous section. Thus,
in that case, all the results given in the
previous section are valid for the Hub-
bard model without interactions. In the
case of nonhopping systems, t = 0 and
the Hamiltonian is reduced to the elec-
tron interaction part only. In this case,
the remaining Hamiltonian is already in
a diagonal form and the eigenstates can
be easily obtained. The main diﬃculty
arises when both terms are present in the
Hamiltonian. However, in half-ﬁlled sys-
tems, the model has nice properties from
a mathematical point of view and a few
important results have been proved. These
systems have attracted a lot of attention
after the discovery of graphene. A system is
a half-ﬁlled one if the number of electrons
is the same as the number of sites, that is,
because the total number of electrons can
be 2n, these systems have only a half of the
maximum number of electrons allowed.
This is particularly the case of graphene
and other conjugated aromatic systems.
Owing to the œ −ß separation that we
have seen in the previous section, these
systems can be considered as half-ﬁlled,
in which each carbon atom provides one
ß-electron.
A fundamental result in the theory of
half-ﬁlled systems is the theorem proved by
Lieb [17]. Lieb’s theorem for repulsive Hub-
bard model states the following. Let G =
(V, E) be a bipartite connected graph repre-
senting a Hubbard model, such that |V| = n
is even and the nodes of the graph are parti-
tioned into two disjoint subsets V1 and V2.
We assume that the hopping parameters
are nonvanishing and that U > 0. Then the
ground states of the model are nondegen-
erate apart from the trivial spin degeneracy,
and have total spin Stot = ||V1| −|V2||∕2.
In order to illustrate the consequences
of Lieb’s theorem, let us consider two
benzenoid systems that can represent
graphene nanoﬂakes. The ﬁrst of them is
realized in the polycyclic aromatic hydro-
carbon known as pyrene and it is illustrated
in Figure 4.1a. The second is a hypothetical
graphene nanoﬂake known as triangulene
and is illustrated in Figure 4.1b. In both
cases, we have divided the bipartite graphs
into two subsets, the one marked by empty
circles corresponds to the set V1 and the
unmarked nodes form the set V2. In the
structure of pyrene, we can easily check
that ||V1|| = ||V2|| = 8 so that the total spin
according to Lieb’s theorem is Stot = 0.
In addition, according to (4.31) given in
the previous section pyrene has no zero-
energy levels as its nullity is zero, that

120
4 Graph and Network Theory
is, 𝜂(Bz) = 0. In this case, the mean-ﬁeld
Hubbard model solution for this structure
reveals no magnetism.
In the case of triangulene, it can be seen
that ||V1|| = 12 and ||V2|| = 10, which gives
a total spin Stot = 1. In addition, the nul-
lity of this graph is equal to 2, indicat-
ing that it has two zero-energy states. The
result given by Lieb’s theorem indicates that
triangulene has a spin-triplet ground state,
which means that it has a magnetic moment
of 2𝜇B per molecule. Thus triangulene and
more ß-extended analogs have intramolec-
ular ferromagnetic interactions owing to ß-
spin topological structures. Analogs of this
molecule have been already obtained in the
laboratory [18].
4.4
Graphs in Statistical Physics
The connections between statistical physics
and graph theory are extensive and have
a long history. A survey on these connec-
tions was published in 1971 by Essam [19];
it mainly deals with the Ising model. In the
Ising model, we consider a set of particles
or “spins,” which can be in one of two states.
The state of the ith particle is described by
the variable 𝜎i which takes one of the two
values ±1. The connection with graph the-
ory comes from the calculation of the parti-
tion function of the model. In this chapter,
we consider that the best way of introduc-
ing this connection is through a general-
ization of the Ising model, the Potts model
[20, 21].
The Potts model is one of the most
important models in statistical physics. In
this model, we consider a graph G = (V, E)
with each node of which we associate a
spin. The spin can have one of q values. The
basic physical principle of the model is that
the energy between two interacting spins is
set to zero for identical spins and it is equal
to a constant if they are not. A remark-
able property of the Potts model is that for
q = 3, 4 it exhibits a continuous phase tran-
sition between high- and low-temperature
phases. In this case, the critical singu-
larities in thermodynamic functions are
diﬀerent from those obtained by using the
Ising model. The Potts model has found
innumerable
applications
in
statistical
physics, for example, in the theory of phase
transitions and critical phenomena, but
also outside this context in areas such
as magnetism, tumor migration, foam
behavior, and social sciences.
In the simplest formulation of the Potts
model with q states {1, 2, … , q}, the Hamil-
tonian of the system can have either of the
two following forms:
H1(𝜔) = −J
∑
(i,j)∈E
𝛿(𝜎i, 𝜎j
),
(4.36)
H2(𝜔) = J
∑
(i,j)∈E
[1 −𝛿(𝜎i, 𝜎j
)],
(4.37)
where 𝜔is a conﬁguration of the graph, that
is, an assignment of a spin to each node
of G = (V, E); 𝜎i is the spin at node i and
𝛿is the Kronecker symbol. The model is
called ferromagnetic if J > 0 and antiferro-
magnetic if J < 0. We notice here that the
Ising model with zero external ﬁeld is a spe-
cial case with q = 2, so that the spins are +1
and −1.
The probability p (𝜔, 𝛽) of ﬁnding the
graph in a particular conﬁguration (state) 𝜔
at a given temperature is obtained by con-
sidering a Boltzmann distribution and it is
given by
p (𝜔, 𝛽) =
exp
(
−𝛽Hi (𝜔)
)
Zi(G)
,
(4.38)
where Zi(G) is the partition function for
a given Hamiltonian in the Potts model,
that is,

4.4 Graphs in Statistical Physics
121
1
1
1
𝜔1
𝜔2
𝜔3
1
0
0
0
0
0
0
1
1
0
1
1
𝜔4
𝜔5
𝜔6
0
0
1
1
1
1
1
0
1
Figure 4.2
Representation of spin conﬁgu-
rations in a cycle with four nodes.
Zi(G) =
∑
𝜔
exp
(
−𝛽Hi (𝜔)
),
(4.39)
where the sum is over all conﬁgurations
(states) and Hi may be either H1 or H2.
Here 𝛽= (kBT)−1, where T is the absolute
temperature of the system, and kB is the
Boltzmann constant.
For instance, let us consider all the dif-
ferent spin conﬁgurations for a cyclic graph
with n = 4 as given in Figure 4.2. It may be
noted that there are four equivalent con-
ﬁgurations for 𝜔2, 𝜔4, and 𝜔5 as well as
two equivalent conﬁgurations for 𝜔3. The
Hamiltonians H1 (𝜔) for these conﬁgura-
tions are
H1(𝜔1) = −4J; H1(𝜔2) = −2J;
H1(𝜔3) = 0; H1(𝜔4) = −2J;
H1(𝜔5) = −2J; H1(𝜔6) = −4J.
Then, the partition function of the Potts
model for this graph is
Z1(G) = 12 exp (2𝛽J) + 2 exp (4𝛽J) + 2.
(4.40)
It is usual to set K = 𝛽J. The probability of
ﬁnding the graph in the conﬁguration 𝜔2 is
p (𝜔2, 𝛽) =
exp (2K)
12 exp (2K) + 2 exp (4K) + 2.
(4.41)
The important connection between the
Potts model and graph theory comes
through the equivalence of this physical
model and the graph-theoretic concept
of the Tutte polynomial, that is, the parti-
tion functions of the Potts model can be
obtained in the following form:
Z1 (G, q, 𝛽) = qk(G)vn−k(G)T (G; x, y) ,
(4.42)
Z2 (G, q, 𝛽) = exp (−mK) Z1 (G, q, 𝛽) , (4.43)
where q is the number of spins in the sys-
tem, k(G) is the number of connected com-
ponents of the graph, v = exp (K) −1, n and
m are the number of nodes and edges in
the graph, respectively, and T (G; x, y) is
the Tutte polynomial, where x = (q + v) ∕v
and y = exp (K). Proofs of the relationship
between the Potts partition function and
the Tutte polynomial will not be considered
here and the interested reader is directed to
the literature to ﬁnd the details [22].
Let us deﬁne the Tutte polynomial [23,
24]. First, we deﬁne the following graph
operations. The deletion of an edge e in
the graph G, represented by G −e, consists
of removing the corresponding edge with-
out changing the rest of the graph, that is,
the end nodes of the edge remain in the
graph. The other operation is the edge con-
traction denoted by G∕e, which consists in
gluing together the two end nodes of the

122
4 Graph and Network Theory
edge e and then removing e. Both oper-
ations, edge deletion and contraction, are
commutative, and the operations G −S and
G∕S, where S is a subset of edges, are well
deﬁned. We notice here that the graphs cre-
ated by these transformations are no longer
simple graphs, they are pseudographs that
may contain self-loops and multiple edges.
Let us also deﬁne the following types of
edges: a bridge is an edge whose removal
disconnects the graph. A (self) loop is an
edge having the two end points incident at
the same node. Let us denote by B and L the
sets of edges that are bridges or loops in the
graph.
Then the Tutte polynomial T (G; x, y)
is
deﬁned
by
the
following
recursive
formulae:
1. T(G; x, y) = T(G −e; x, y) + T(G∕e;
x, y) if e ∉B, L;
2. T(G; x, y) = xiyj if e ∈B, L,
where the exponents i and j represent the
number of bridges and self-loops in the
subgraph, respectively.
Using this deﬁnition, we can obtain
the Tutte polynomial for the cyclic graph
with four nodes C4, as illustrated in the
Figure 4.3, that is, the Tutte polynomial
for C4 is T (C4; x, y) = x3 + x2 + x + y. We
can substitute this expression into (4.42) to
obtain the partition function for the Potts
model of this graph,
Z1 (G; 2, 𝛽) = 2k(G)vn−k(G)
[(q + v
v
)3
+
(q + v
v
)2
+
(q + v
v
)
+1+v
]
,
(4.44)
and so we obtain Z1(G; 2, 𝛽) = 12 exp(2K)
+2 exp(4K) + 2.
The following is an important mathemat-
ical result related to the universality of the
Tutte polynomial [23, 24]. Let f (G) be a
function on graphs having the following
properties:
1. f (G) = 1 if |V| = 1 and |E| = 0
2. f (G) = af (G −e) + bf (G∕e) if e ∉B, L,
3. f (G ∪H) = f (G)f (H);
f (G ∗H) = f (G)f (H), where G ∗H
X3
X2
X
Y
Figure 4.3
Edge deletion and contrac-
tion in a cyclic graph with four nodes.

4.4 Graphs in Statistical Physics
123
means that G and H shares at most one
node.
Then f (G) is an evaluation of the Tutte
polynomial, meaning that it is equivalent
to the Tutte polynomial with some speciﬁc
values for the parameters, and takes the
form
f (G)=am−n+k(G)bn−k(G)T
(
G; f (K2)
b
, f (L)
a
)
,
(4.45)
where L is the graph consisting of a single
node with one loop attached, K2 is the com-
plete graph with two nodes.
More formally, the Tutte polynomial is
a generalized Tutte–Gröthendieck (T-G for
short) invariant. To deﬁne the T-G invari-
ant, we need the following concepts. Let S
and S′ be two disjoint subsets of edges. A
minor of G is a graph H that is isomorphic
to (G −S) ∕S′. Let Γ be a class of graphs
such that if G is in Γ, then any minor of G
is also in the class. This class is known as
minor closed. A graph invariant is a func-
tion f on the class of all graphs such that if
G and H are isomorphic, then f (G) = f (H).
Then, a T-G invariant is a graph invariant f
from Γ to a commutative ring ℜwith unity,
such as the conditions (i)–(iii) above are
fulﬁlled. A graph invariant is a function f
on the class of all graphs such that f (G1) =
f (G2) whenever the graphs G1 and G2 are
isomorphic. For more details the reader is
referred to the specialized literature on this
topic.
Some interesting evaluations of the Tutte
polynomial are the following:
Let us now consider a proper coloring of
a graph G, which is an assignment of a color
to each node of G such that any two adja-
cent nodes have diﬀerent colors. The chro-
matic polynomial 𝜒(G; q) of the graph G is
the number of ways in which q colors can be
assigned to the nodes of G such that no two
T(G; 1, 1)
Number of spanning trees of
the graph G
T(G; 2, 1)
Number of spanning forests
T(G; 1, 2)
Number of spanning con-
nected subgraphs
T(G; 2, 2)
2|E|
adjacent nodes have the same color. The fol-
lowing are two interesting characteristics of
the chromatic polynomial:
1. 𝜒(G; q) = 𝜒(G −e; q) −𝜒(G∕e; q),
2. 𝜒(G; q) = qn for the trivial graph on n
nodes.
Thus, the chromatic polynomial fulﬁlls
the same contraction/deletion rules as the
Tutte polynomial. Indeed, the chromatic
polynomial is an evaluation of the Tutte
polynomial,
𝜒(G; q) = qk(G) (−1)n−k(G) T (G; 1 −q, 0) .
(4.46)
To see the connection between the Potts
model and the chromatic polynomial, we
have to consider the Hamiltonian H1 (G; 𝜔)
in the zero temperature limit, that is, T →
0
(𝛽→∞). When 𝛽→∞, the only spin
conﬁgurations that contribute to the parti-
tion function are the ones in which adjacent
spins have diﬀerent values. Then we have
that Z1 (G; q, 𝛽) →1 in the antiferromag-
netic model (J < 0). Thus, Z1 (G; q, 𝛽→∞)
counts the number of proper colorings of
the graph using q colors. The partition func-
tion in the T = 0 limit of the Potts model is
given by the chromatic polynomial
Z1(G; q, −1) = 𝜒(G)
= (−1)k(G)(−1)nT(G; 1 −q, 0).
(4.47)

124
4 Graph and Network Theory
4.5
Feynman Graphs
When
studying
elementary-particle
physics, the calculation of higher-order
corrections in perturbative quantum ﬁeld
theory naturally leads to the evaluation
of Feynman integrals. Feynman integrals
are associated to Feynman graphs, which
are graphs G = (V, E) with n nodes and
m edges and some special characteristics
[25–27]. For instance, the edges play a
fundamental role in the Feynman graphs as
they represent the diﬀerent particles, such
as fermions (edges with arrows), photons
(wavy lines), gluons (curly lines). Scalar
particles are represented by simple lines.
Let us assign a D-dimensional momentum
vector qj and a number representing the
mass mj to the jth edge representing the
jth particle, where D is the dimension of
the space-time. In the theory of Feynman
graphs, the nodes with degree one are not
represented, leaving the edge without the
end node. This edge is named an external
edge (they are sometimes called legs). The
rest of edges are called internal. Also,
nodes of degree two are omitted as they
represent mass insertions. Thus, Feynman
graphs contain only nodes of degree k ≥3,
which represent the interaction of k parti-
cles. At each of these nodes, the sum of all
momenta ﬂowing into the node equals that
of the momenta ﬂowing out of it. As usual
the number of basic cycles, here termed
loops, is given by the cyclomatic number
l = m −n + C, where C is the number of
connected components of the graph.
Here we will only consider Feynman
graphs with scalar propagators and we
refer to them as scalar theories. In scalar
theories,
the
D-dimensional
Feynman
integral has the form
IG =(𝜇2)𝜈−lD∕2
∫
l∏
r=1
dDkr
i𝜋D∕2
n
∏
j=1
1
(
−q2
j +m2
j
)𝜈j ,
(4.48)
where l is the number of loops (basic cycles)
in the Feynman diagram, 𝜇is an arbitrary
scale parameter used to make the expres-
sions dimensionless, 𝜈j is a positive integer
number that gives the power to which the
propagator occurs, 𝜈= 𝜈1 + · · · + 𝜈m, kr is
the independent loop momentum, mj is the
mass of the jth particle, and
qj =
l∑
j=1
𝜌ijkj +
m
∑
j=1
𝜎ijpj,
𝜌ij, 𝜎ij ∈{−1, 0, 1} ,
(4.49)
represents the momenta ﬂowing through
the internal lines.
The correspondence between the Feyn-
man integral and the Feynman graph is as
follows. An internal edge represents a prop-
agator of the form
i
q2
j −m2
j
,
(4.50)
whereby abusing of the notation q2
j rep-
resents the inner product of the momen-
tum vector with itself, that is, q2
j = qj ⋅qT
j .
Notice that this is a relativistic propagator
that represents a Greens function for inte-
grations over space and time.
Nodes and external edges have weights
equal to one. For each internal momentum
not constrained by momentum conserva-
tion there is also an integration associated.
Now, in order to compute the integral
equation (4.48), we need to assign a (real
or complex) variable xj to each internal
edge, which are known as the Feynman
parameters. Then, we need to use the
Feynman parameter trick for each prop-
agator and evaluate the integrals over the
loop momenta k1, … , kl. As a consequence,
we obtain

4.5 Feynman Graphs
125
IG = Γ (𝜈−lD∕2)
∏m
j=1 Γ (𝜈j
) ∫xj≥0
( m
∏
j=1
dxjx
𝜈j−1
j
)
× 𝛿
(
1 −
m
∑
i=1
xi
)
U𝜈−(l+1)D∕2
F𝜈−lD∕2
.
(4.51)
The real connection with the theory of
graphs comes from the two terms U and
F, which are graph polynomials, known as
the ﬁrst and second Symanzik polynomials
(sometimes
called
Kirchhoﬀ–Symanzik
polynomials). We will now specify some
methods for obtaining these polynomials
in Feynman graphs.
4.5.1
Symanzik Polynomials and Spanning Trees
The ﬁrst Symanzik polynomial can be
obtained by considering all spanning trees
in the Feynman graph. Let 𝜏1 be the set of
spanning trees in the Feynman graph G.
Then,
U =
∑
T∈𝜏1
∏
ej∉T
xj,
(4.52)
where T is a spanning tree and xj is
the Feynman parameter associated with
edge ej.
In order to obtain the second Symanzik
polynomial F, we have to consider the set of
spanning 2-forest 𝜏2 in the Feynman graph.
A spanning 2-forest is a spanning forest
formed by only two trees. Then, the ele-
ments of 𝜏2 are denoted by (Ti, Tj
). The sec-
ond Symanzik polynomial is given by
F = F0 + U
m
∑
i=1
xi
m2
i
𝜇2 .
(4.53)
The term F0 is a polynomial obtained
from the sets of spanning 2-forests of
G in the following way: let PTi be the
set of external momenta attached to the
spanning tree Ti, which is part of the
spanning 2-forest (Ti, Tj). Let pk ⋅pr be
the Minkowski scalar product of the two
momenta vectors associated with the edges
ek and er, respectively. Then
F0 =
∑
(Ti,Tj)∈𝜏2
⎛
⎜
⎜⎝
∏
ek∉(Ti,Tj)
xk
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
∑
pk∈PTi
∑
pr∈PTj
pk ⋅pr
𝜇2
⎞
⎟
⎟⎠
.
(4.54)
Let us now show how to obtain the
Symanzik polynomials for the simple Feyn-
man graph illustrated in the Figure 4.4. For
the sake of simplicity, we take all internal
masses to be zero.
We ﬁrst obtain all spanning trees of this
graph, which are given in Figure 4.5.
Hence, the ﬁrst Symanzik polynomial is
obtained as follows:
U = x1x2 + x1x3 + x1x5 + x2x4 + x2x5
+ x3x4 + x3x5 + x4x5
= (x1 + x4)(x2 + x3)
+ (x1 + x2 + x3 + x4)x5.
(4.55)
Now, for the second Symanzik polynomial,
we obtain all the spanning 2-forests of the
graph, which are given in Figure 4.6.
We should notice that the terms x1x2x5
and x3x4x5 do not contribute to F0 because
the momentum sum ﬂowing through all cut
edges is zero. Thus, we can obtain F0 as
follows:
x4
v3
x3
v2
x5
x2
v4
x1
v1
Figure 4.4
Illustration of a Feynman graph with four nodes, ﬁve inter-
nal, and two external edges. The Feynman parameters are represented
by xi on each internal edge.

126
4 Graph and Network Theory
x1x2
x1x3
x1x5
x2x4
x2x5
x3x4
x3x5
x4x5
Figure 4.5
Spanning trees of the
Feynman graphs represented in
Figure 4.4.
F = F0 = (x1x2x3 + x1x2x4 + x1x3x4 + x1x3x5
+ x1x4x5 + x2x3x4 + x2x3x5
+ x2x4x5)
(−p2
𝜇2
)
= [(x1+x2)(x3 + x4)x5 + x1x4(x2 + x3)
+x2x3(x1 + x4)]
(−p2
𝜇2
)
.
(4.56)
L =
⎛
⎜
⎜
⎜
⎜⎝
x1 + x2 + x5
−x2
−x5
−x1
−x2
x2 + x3
−x3
0
−x5
−x3
x3 + x4 + x5
−x4
−x1
0
−x4
x1 + x4
⎞
⎟
⎟
⎟
⎟⎠
.
(4.57)
4.5.2
Symanzik Polynomials and the Laplacian
Matrix
Another graph-theoretic way of obtaining
the Symanzik polynomials is through the
use of the Laplacian matrix. The Laplacian
matrix for the Feynman graphs is deﬁned as
usual for any weighted graph. For instance,
for the Feynman graph given in Figure 4.4,
the Laplacian matrix is

4.5 Feynman Graphs
127
x1x2x3
x1x2x5
x1x3x5
x2x3x4
x2x4x5
x1x2x4
x1x3x4
x1x4x5
x2x3x5
x3x4x5
Figure 4.6
Spanning 2-forest of
the Feynman graph represented in
Figure 4.4.
Then, we can deﬁne the auxiliary polyno-
mial K = det L [i], where L [i] denotes the
minor of the Laplacian matrix obtained by
removing the ith row and column of L. This
polynomial is known as the Kirchhoﬀpoly-
nomial of the graph and it is easy to see that
it can be deﬁned by
K =
∑
T∈𝜏1
∏
ej∈T
xj.
(4.58)
For instance,
K = det L[1]
=
|||||||
x2 + x3
−x3
0
−x3
x3 + x4 + x5
−x4
0
−x4
x1 + x4
|||||||
= x1x2x3 + x1x2x4 + x1x2x5 + x1x3x4
+ x1x3x5 + x2x3x4 + x2x4x5
+ x3x4x5.
(4.59)
We transform the Kirchhoﬀpolynomial
into the ﬁrst Symanzik polynomial by set-
ting U = x1 · · · xmK (x−1
1 , … , x−1
m
) , that is,

128
4 Graph and Network Theory
U = x1x2x3x4x5
x1x2x3
+ x1x2x3x4x5
x1x2x4
+ · · ·
+ x1x2x3x4x5
x3x4x5
= x1x2 + x1x3 + x1x5 + x2x4 + x2x5
+ x3x4 + x3x5 + x4x5
= (x1+x4)(x2+x3) + (x1+x2+x3+x4)x5.
(4.60)
To calculate the second Symanzik polyno-
mial using the Laplacian matrix, we have to
introduce some modiﬁcations. First assign
a new parameter zj to each of the external
edges of the Feynman graph. Now, build a
diagonal matrix whose diagonal are Dii =
∑
j→i zj, that is the ith diagonal entry of D
represents the sum of the parameters zj for
all the external edges incident with the node
i. Modify the Laplacian matrix as follows:
̃L = L + D. The modiﬁed Laplacian matrix
̃L is the minor of a Laplacian matrix con-
structed for a modiﬁcation of the Feynman
graph in which all rows and columns corre-
sponding to the external edges are removed
[26, 27]. The determinant of the modiﬁed
Laplacian matrix is
W = det ̃L,
(4.61)
and let us expand it in a series of polyno-
mials homogeneous in the variables zj, such
that
W = W (0) + W (1) + W (2) + · · · + W (t),
(4.62)
where t is the number of external edges.
Then the Symanzik polynomials are
U = x1 · · · xmW (1)
j
(x−1
1 , … , x−1
m ) for any j,
and
F0 =x1· · ·xm
∑
(j,k)
(pj ⋅pk
𝜇2
)
W (2)
(j,k)
(x−1
1 , … , x−1
m
).
(4.63)
For the Feynman graph given in the previ-
ously analyzed example, we have
̃L =
⎛
⎜
⎜
⎜
⎜⎝
x1 + x2 + x5
−x2
−x5
−x1
−x2
x2 + x3 + z1
−x3
0
−x5
−x3
x3 + x4 + x5
−x4
−x1
0
−x4
x1 + x4 + z2
⎞
⎟
⎟
⎟
⎟⎠
,
(4.64)
and W = det ̃L = W (1) + W (2), where
W (1) = (z1 + z2)(x1x2x3 + x1x2x4 + x1x3x4
+ x2x3x4 + x1x2x5 + x1x3x5
+ x2x4x5 + x3x4x5),
(4.65)
W (2) = z1z2(x1x3 + x2x3 + x1x4 + x2x4
+ x1x5 + x2x5 + x3x5 + x4x5).
(4.66)
With this information, the ﬁrst and sec-
ond Symanzik polynomials can be easily
obtained.
4.5.3
Symanzik Polynomials and Edge
Deletion/Contraction
The Symanzik polynomials can also be
obtained through the graph transforma-
tions used to deﬁne the Tutte polynomial,
that is, the Symanzik polynomials obey
the rules for edge deletion and contraction
operations that we encountered in the
previous section. Recall that the deletion of
an edge e in the graph G is represented by
G −e, and the edge contraction denoted by
G∕e, and that B and L are the sets of edges

4.6 Graphs and Electrical Networks
129
that are bridges or loops in the graph (see
Section 4.4). Then
U(G) = U
(
G
ej
)
+ xjU (G −ej
) ,
(4.67)
F0(G) = F0
(
G
ej
)
+ xjF0
(G −ej
) ,
(4.68)
for any ej ∉B, L.
Finally, let us mention that there exist
factorization theorems for the Symanzik
polynomials that are based on a beautiful
theorem due to Dodgson [28]. The reader is
reminded that Charles L. Dodgson is better
known as Lewis Carroll who has delighted
many generations with his Alice in Wonder-
land. These factorization theorems are not
given here and the reader is directed to the
excellent reviews of Bogner and Weinzierl
for details [26, 27].
4.6
Graphs and Electrical Networks
The relation between electrical networks
and graphs is very natural and is docu-
mented in many introductory texts on
graph theory. The idea is that a simple
electrical network can be represented as
a graph G = (V, E) in which we place a
ﬁxed electrical resistor at each edge of the
graph. Therefore, these networks can also
be called resistor networks. Let us suppose
that we connect a battery across the nodes
u and v. There are several parameters of
an electrical network that can be consid-
ered in terms of graph-theoretic concepts
but we concentrate here on one that has
important connections with other param-
eters of relevance in physics, namely, the
eﬀective resistance [29]. Let us calculate
the eﬀective resistance Ω (u, v) between
two nodes by using the Kirchhoﬀand
Ohm laws. For the sake of simplicity, we
always consider here resistors of 1 ohm.
In the simple case of a tree, the eﬀec-
tive resistance is simply the sum of the
resistances along the path connecting u
and v, that is, for a tree, Ω (u, v) = d (u, v),
where d (u, v) is the shortest-path distance
between the corresponding nodes (number
of links in the shortest path connecting
both nodes). However, in the case of two
nodes connected by multiple routes, the
eﬀective resistance Ω (u, v) can be obtained
by using Kirchhoﬀ’s laws. A characteristic
of the eﬀective resistance Ω (u, v) is that it
decreases with the increase of the number
of routes connecting u and v. Thus, in
general, Ω (u, v) ≤d (u, v).
An important result about the eﬀec-
tive resistance was obtained by Klein and
Randi´c [30]: the eﬀective resistance is a
proper distance between the pairs of nodes
of a graph, that is,
1. Ω (u, v) ≥0 for all u ∈V(G), v ∈V(G).
2. Ω (u, v) = 0 if and only if u = v.
3. Ω (u, v) = Ω (v, u) for all
u ∈V(G), v ∈V(G).
4. Ω (u, w) ≤Ω (u, v) + Ω (v, w) for all
u ∈V(G), v ∈V(G), w ∈V(G).
The resistance distance Ω (u, v) between
a pair of nodes u and v in a connected
component of a network can be calculated
by using the Moore–Penrose generalized
inverse L+ of the graph Laplacian L:
Ω (u, v) = L+ (u, u) + L+ (v, v) −2L+ (u, v) ,
(4.69)
for u ≠v.
Another way of computing the resis-
tance distance for a pair of nodes in a
network is as follows. Let L (G −u) be the
matrix resulting from removing the uth
row and column of the Laplacian and let
L (G −u −v) be the matrix resulting from
removing both the uth and vth rows and

130
4 Graph and Network Theory
columns of L. Then it has been proved [31]
that
Ω (u, v) = det L (G −u −v)
det L (G −u)
,
(4.70)
Notice that det L (G −u) is the Kirchhoﬀ
(Symanzik) polynomial we discussed in the
previous section. Yet another way for com-
puting the resistance distance between a
pair of nodes in the network is given on the
basis of the Laplacian spectra [32]
Ω (u, v) =
n
∑
k=2
1
𝜇k
[
Uk (u) −Uk (v)
]2 , (4.71)
where Uk (u) is the uth entry of the kth
orthonormal eigenvector associated to the
Laplacian eigenvalue 𝜇k, written in the
ordering 0 = 𝜇1 < 𝜇2 ≤· · · ≤𝜇n.
The resistance distance between all pairs
of nodes in the network can be represented
in the resistance matrix 𝛀of the network.
This matrix can be written as
𝛀= |1⟩diag
{[
L +
( 1
n
)
J
]−1}T
+ diag
[
L +
( 1
n
)
J
]−1
⟨1|
−2
(
L +
( 1
n
)
J
)−1
,
(4.72)
where J = |1⟩⟨1| is a matrix having all
entries equal to 1.
For the case of connected networks, the
resistance distance matrix can be related to
the Moore–Penrose inverse of the Lapla-
cian as shown by Gutman and Xiao [33]:
L+ = −1
2
[
𝛀−1
n (𝛀J + J𝛀) + 1
n2 J𝛀J
]
,
(4.73)
where J is as above.
The resistance distance matrix is a
matrix of squared Euclidean distances. A
matrix M ∈ℝn×n is said to be Euclidean
if there is a set of vectors x1, … , xn such
that Mij = ‖xi −xj‖2. Because it is easy to
construct vectors such that Ωij = ‖xi −xj‖2
the resistance distance matrix is squared
Euclidean
and
the
resistance
distance
satisﬁes the weak triangle inequality
Ω1∕2
ik
≤Ω1∕2
ij
+ Ω1∕2
jk ,
(4.74)
for every pair of nodes in the network.
As we noted in the introduction to
this section, eﬀective resistance has con-
nections with other concepts that are of
relevance in the applications of mathemat-
ics in physics. One of these connections
is between the resistance distance and
Markov chains. In particular, the resistance
distance is proportional to the expected
commute time between two nodes for a
Markov chain deﬁned by a weighted graph
[29, 34]. If wuv be the weight of the edge
{u, v}, the probability of transition between
u and v in the Markov chain deﬁned on the
graph is
Puv =
wuv
∑
u,v∈E
wuv
.
(4.75)
The commuting time is the time taken by
“information” starting at node u to return
to it after passing through node v. The
expected commuting time ̂Cuv is related to
the resistance distance [29, 34] by
̂Cuv = 2 (1Tw) Ω (u, v) ,
(4.76)
where 1 is vector of 1’s and w is the vector
of link weights. Note that if the network is
unweighted ̂Cuv = 2mΩ (u, v).
4.7
Graphs and Vibrations
In this section, we develop some connec-
tions between vibrational analysis, which is
important in many areas of physics ranging
from classical to quantum mechanics, and

4.7 Graphs and Vibrations
131
the spectral theory of graphs. Here we
consider the one-dimensional case with
a graph G = (V, E) in which every node
represents a ball of mass m and every edge
represents a spring with the spring constant
m𝜔2 connecting two balls. The ball–spring
network is assumed to be submerged in
a thermal bath at temperature T. The
balls in the graph oscillate under thermal
excitation. For the sake of simplicity, we
assume that there is no damping and no
external forces are applied to the system.
Let xi, i = 1, 2, … , n be the coordinates of
each node; this measures the displacement
of the ball i from its equilibrium state
xi = 0. For a complete guide to the results
to be presented here, the reader is directed
to Estrada et al. [35].
4.7.1
Graph Vibrational Hamiltonians
Let us start with a Hamiltonian of the oscil-
lator network in the form
HA =
∑
i
[
p2
i
2m + (K −ki
) m𝜔2x2
i
2
]
+ m𝜔2
2
∑
i, j
(i < j)
Aij
(xi −xj
)2,
(4.77)
where ki is the degree of the node i and
K is a constant satisfying K ≥maxi ki. The
second term in the right-hand side is the
potential energy of the springs connecting
the balls, because xi −xj is the extension
or the contraction of the spring connect-
ing the nodes i and j. The ﬁrst term in
the ﬁrst set of square parentheses is the
kinetic energy of the ball i, whereas the sec-
ond term in the ﬁrst set of square paren-
theses is a term that avoids the movement
of the network as a whole by tying the
network to the ground. We add this term
because we are only interested in small
oscillations around the equilibrium; this
will be explained below again.
The Hamiltonian equation (4.77) can be
rewritten as
HA =
∑
i
(
p2
i
2m + Km𝜔2
2
x2
i
)
−m𝜔2
2
∑
i,j
xiAijxj.
(4.78)
Let us next consider the Hamiltonian of the
oscillator network in the form
HL =
∑
i
p2
i
2m+m𝜔2
2
Aij
(xi −xj
)2
(4.79)
instead of the Hamiltonian HA in (4.78).
Because the Hamiltonian HL lacks the
springs that tie the whole network to the
ground (the second term in the ﬁrst set
of parentheses in the right-hand side of
(4.78), this network can undesirably move
as a whole. We will deal with this motion
shortly.
The
expansion
of
the
Hamiltonian
equation (4.79) as in (4.77) and (4.78) now
gives
HL =
∑
i
p2
i
2m + m𝜔2
2
∑
i,j
xiLijxj,
(4.80)
where Lij denotes an element of the net-
work Laplacian L.
4.7.2
Network of Classical Oscillators
We start by considering a network of clas-
sical harmonic oscillators with the Hamil-
tonian HA. Here the momenta pi and the
coordinates xi are independent variables, so
that the integration of the factor
∏
exp
[
−𝛽
(
p2
i
2m
)]
(4.81)

132
4 Graph and Network Theory
over the momenta {pi
} reduces to a con-
stant term, which does not aﬀect the inte-
gration over {xi
}. As a consequence, we do
not have to consider the kinetic energy and
we can write the Hamiltonian in the form
HA = m𝜔2
2
xT (KI −A) x,
(4.82)
where x = (x1, x2, … , xn)T and I is the n × n
identity matrix.
The partition function is given by
Z = ∫e−𝛽HA ∏
i
dxi
= ∫dx exp
(
−𝛽m𝜔2
2
xT (KI −A) x
)
,
(4.83)
where the integral is an n-fold one and can
be evaluated by diagonalizing the matrix A.
The adjacency matrix can be diagonalized
by means of an orthogonal matrix O as in
𝚲= O (KI −A) OT,
(4.84)
where 𝚲is the diagonal matrix with eigen-
values 𝜆𝜇of (KI −A) on the diagonal. Let us
consider that K is suﬃciently large, so that
we can make all eigenvalues 𝜆𝜇positive. By
deﬁning a new set of variables y𝜇by y = Ox
and x = OTy, we can transform the Hamil-
tonian equation (4.82) to the form
HA = m𝜔2
2
yT𝚲y
=
m𝜔2
0
2
∑
𝜇
y2
𝜇+ m𝜔2
2
∑
𝜇
𝜆𝜇y2
𝜇.
(4.85)
Then the integration measure of the n-
fold integration in (4.83) is transformed as
∏
i
dxi = ∏
𝜇
dy𝜇, because the Jacobian of the
orthogonal matrix O is unity. Therefore, the
multifold integration in the partition func-
tion (4.83) is decoupled to give
Z =
∏
𝜇
√
2𝜋
𝛽m𝜔2𝜆𝜇
,
(4.86)
which can be rewritten in terms of the adja-
cency matrix as
Z =
(
2𝜋
𝛽m𝜔2
)n∕2
1
√
det (KI −A)
.
(4.87)
As we have made all the eigenvalues
of (KI −A) positive, its determinant is
positive.
Now we deﬁne an important quantity,
the mean displacement of a node from its
equilibrium position. It is given by
⟨
x2
p
⟩
= 1
Z ∫x2
pe−𝛽HA ∏
i
dxi,
(4.88)
which, by using the spectral decomposition
of A, yields
⟨
x2
p
⟩
= 1
Z ∫
[
∑
𝜎
(
OT)
p𝜎y𝜎
]2
e−𝛽HA ∏
𝜇
dy𝜇
(4.89)
In the integrand, the odd functions with
respect to y𝜇vanish. Therefore, only the
terms of y2
𝜎survive after integration in the
expansion of the square parentheses in the
integrand. This gives
⟨
x2
p
⟩
= 1
Z
∑
𝜎
O2
𝜎p∫y2
𝜎exp
(
−𝛽m𝜔2
2
𝜆𝜎y2
𝜎
)
dy𝜎
×
∏
𝜇(≠𝜎)
[
∫exp
(
−𝛽m𝜔2
2
𝜆𝜇y2
𝜇
)
dy𝜇
]
.
(4.90)
Comparing this expression with (4.86), we
have
⟨
x2
p
⟩
=
1
𝛽mK𝜔2
[(
I −A
K
)−1]
pp
.
(4.91)

4.7 Graphs and Vibrations
133
The mean node displacement may be given
by the thermal Green’s function in the
framework of classical mechanics by
⟨
x2
p
⟩
=
1
𝛽Km𝜔2
[(
I −A
K
)−1]
pq
.
(4.92)
This represents a correlation between the
node displacements in a network due to
small thermal ﬂuctuations.
The same calculation using the Hamilto-
nian equation (4.80) gives
⟨
x2
p
⟩′
=
1
𝛽m𝜔2
(L+)
pq ,
(4.93)
where L+ is the Moore–Penrose general-
ized inverse of the Laplacian.
4.7.3
Network of Quantum Oscillators
Here we consider the quantum-mechanical
version of the Hamiltonian HA in (4.78) by
considering that the momenta pj and the
coordinates xi are not independent vari-
ables. In this case, they are operators that
satisfy the commutation relation,
[xi, pj
] = iℏ𝛿ij.
(4.94)
We use the boson creation and annihilation
operators a†
i and ai, which allow us to write
the coordinates and momenta as
xi =
√
ℏ
2mΩ
(a†
i + ai
) ,
(4.95)
pi =
√
ℏ
2mΩ
(a†
i −ai
) ,
(4.96)
where Ω =
√
K∕m𝜔. The commutation
relation (4.94) yields
[
ai, a†
j
]
= 𝛿ij.
(4.97)
With the use of these operators, we can
recast the Hamiltonian equation (4.78) into
the form
HA =
∑
i
ℏΩ
(
a†
i ai + 1
2
)
−ℏ𝜔2
4Ω
∑
i,j
(a†
i +ai
)Aij
(
a†
j +aj
)
. (4.98)
Using the spectral decomposition of the
adjacency matrix, we generate a new set of
boson creation and annihilation operators
given by
b𝜇=
∑
i
O𝜇iai =
∑
i
ai
(OT)
i𝜇,
(4.99)
b†
𝜇=
∑
i
O𝜇ia†
i =
∑
i
a†
i
(OT)
i𝜇,
(4.100)
Applying the transformations ((4.99) and
(4.100)) to the Hamiltonian equation (4.98),
we can decouple it as
HA =
∑
𝜇
H𝜇,
(4.101)
with
H𝜇= ℏΩ
[
1 + 𝜔2
2Ω2
(𝜆𝜇−K)] (
b†
𝜇b𝜇+ 1
2
)
+ ℏ𝜔2
4Ω
(𝜆𝜇−K) [(
b†
𝜇
)2
+ (b𝜇
)2]
.
(4.102)
In order to go further, we now introduce
an approximation in which each mode of
oscillation does not get excited beyond
the ﬁrst excited state. In other words, we
restrict ourselves to the space spanned by
the ground state (the vacuum) |vac⟩and the
ﬁrst excited states b†
𝜇|vac⟩. Then the second
term of the Hamiltonian equation (4.102)
does not contribute and we therefore have
H𝜇= ℏΩ
[
1 + 𝜔2
2Ω2
(𝜆𝜇−K)] (
b†
𝜇b𝜇+ 1
2
)
(4.103)

134
4 Graph and Network Theory
within this approximation. This approxima-
tion is justiﬁed when the energy level spac-
ing ℏΩ is much greater than the energy
scale of external disturbances, (speciﬁcally
the temperature ﬂuctuation kBT = 1∕𝛽, in
assuming the physical metaphor that the
system is submerged in a thermal bath at
the temperature T), as well as than the
energy of the network springs ℏ𝜔, that is,
𝛽ℏΩ ≫1 and Ω ≫𝜔. This happens when
the mass of each oscillator is small, when
the springs connecting to the ground mΩ2
are strong, and when the network springs
m𝜔2 are weak. Then an oscillation of tiny
amplitude propagates over the network. We
are going to work in this limit hereafter.
We are now in a position to com-
pute the partition function as well as
the thermal Green’s function quantum-
mechanically. As stated above, we consider
only the ground state and one excitation
from it. Therefore we have the quantum-
mechanical
partition
function
in
the
form
ZA = ⟨vac| e−𝛽HA |vac⟩
=
∏
𝜇
exp
{
−𝛽ℏΩ
2
[
1+ 𝜔2
2Ω2
(𝜆𝜇−K)]}
.
(4.104)
The diagonal thermal Green’s function giv-
ing the mean node displacement in the
quantum-mechanical framework is given
by
⟨
x2
p
⟩
= 1
Z ⟨vac| ape−𝛽HAa†
p |vac⟩,
(4.105)
which indicates how much an excitation
at the node p propagates throughout the
graph before coming back to the same node
and being annihilated. Let us compute the
quantity equation (4.105) by
⟨
x2
p
⟩
= e−𝛽ℏΩ
(
exp
[𝛽ℏ𝜔2
2Ω A
])
pp
, (4.106)
where we have used (4.84). Similarly, we can
compute the oﬀ-diagonal thermal Green’s
function as
⟨
xp, xq
⟩
= e−𝛽ℏΩ
(
exp
[𝛽ℏ𝜔2
2Ω A
])
pq
.
(4.107)
The same quantum-mechanical calculation
by using the Hamiltonian HL in (4.79) gives
⟨xp, xq
⟩= 1 + lim
Ω→0 O2pO2q exp
[
−𝛽ℏ𝜔2
2Ω 𝜇2
]
,
(4.108)
where 𝜇2 is the second eigenvalue of the
Laplacian matrix.
4.8
Random Graphs
The study of random graphs is one of the
most important areas of theoretical graph
theory. Random graphs have found mul-
tiple applications in physics and they are
used today as a standard null model in sim-
ulating many physical processes on graphs
and networks. There are several ways of
deﬁning a random graph, that is, a graph
in which, given a set of nodes, the edges
connecting them are selected in a random
way. The simplest model of random graph
was introduced by Erdös and Rényi [36].
The construction of a random graph in
this model starts by considering n isolated
nodes. Then, with probability p > 0, a pair
of nodes is connected by an edge. Conse-
quently, the graph is determined only by
the number of nodes and edges such that
it can be written as G (n, m) or G (n, p). In
Figure 4.7, we illustrate some examples of
Erdös–Rényi (ER) random graphs with the
same number of nodes and diﬀerent linking
probabilities.
A few properties of ER random graphs
are summarized as follows.

4.8 Random Graphs
135
Figure 4.7
Illustration of the changes of an Erdös–Rényi
random network with 20 nodes and probabilities that
increases from zero (left) to one (right).
1. The expected number of edges per
node:
m = n (n −1) p
2
.
(4.109)
2. The expected node degree:
k = (n −1) p.
3. The average path length for large n:
l (H) = ln n −𝛾
ln (pn) + 1
2,
(4.110)
where 𝛾≈0.577 is the Euler–
Mascheroni constant.
4. The average clustering coeﬃcient (see
(4.12)):
C = p = 𝛿(G).
(4.111)
5. When increasing p, most nodes tends
to be clustered in one giant component,
while the rest of nodes are isolated in
very small components (see Figure 4.8).
6. The structure of GER (n, p) changes as a
function of p = k∕(n −1) giving rise to
the following three stages (see
Figure 4.9):
a
Subcritical k < 1, where all
components are simple and very
small. The size of the largest
component is S = O (ln n).
b
Critical k = 1, where the size of
the largest component is
S = Θ (n2∕3).
c
Supercritical k > 1, where the
probability that
(f −𝜀) n < S < (f + 𝜀) n is 1
when n →∞
𝜀> 0, where
f = f (k) is the positive solution of
the equation: e−kf = 1 −f . The rest
of the components are very small,
with the second largest having size
about ln n.
7. The largest eigenvalue of the adjacency
matrix in an ER network grows
proportionally to n [37]:
limn→∞
(𝜆1 (A) ∕n) = p.
8. The second largest eigenvalue grows
more slowly than 𝜆1:
limn→∞
(𝜆2 (A) ∕n𝜀) = 0 for every
𝜀> 0.5.
9. The smallest eigenvalue also grows with
a similar relation to 𝜆2 (A):
limn→∞
(𝜆n (A) ∕n𝜀) = 0 for every
𝜀> 0.5.
10. The spectral density of an ER random
network follows Wigner’s semicircle law
[38], which is simply written as (see
Figure 4.10):

136
4 Graph and Network Theory
0
Size giant component
p
1
Figure 4.8
Change of the size of the giant connected
component in an ER random graph as probability is
increased.
p = 0.0075
p = 0.01
p = 0.025
Figure 4.9
Examples of the diﬀerent stages of the change
of an ER random graph with the increase in probability:
subcritical (a), critical (b), and supercritical (c).
0.6
0.5
0.4
0.3
ρ(λ).r
λ/r
0.2
0.1
0.0
0
1
2
3
−1
−2
−3
Figure 4.10
Illustration of the Wigner semicircle law for
the spectral density of an ER random graph.

4.9 Introducing Complex Networks
137
𝜌(𝜆) =
⎧
⎪
⎨
⎪⎩
√
4−𝜆2
2𝜋
−2 ≤𝜆
r ≤2, r=
√
np(1−p)
0
otherwise.
(4.112)
4.9
Introducing Complex Networks
In the rest of this chapter,we are going
to study the so-called complex networks.
Complex networks can be considered
as the skeleton of complex systems in a
variety of scenarios ranging from social
and ecological to biological and techno-
logical systems. Their study has become
a major ﬁeld of interdisciplinary research
in the twenty-ﬁrst century with an impor-
tant participation of physicists who have
contributed signiﬁcantly by creating new
models and adapting others known in
physics to the study of the topologi-
cal and dynamical properties of these
networks. A number of universal topolog-
ical properties that explain some of the
dynamical and functional properties of
networks have been introduced, such as
“small-world” and “scale-free” phenomena;
these will be analyzed brieﬂy in the next
sections.
There is much confusion about what a
complex network is. To start with we should
attempt a clariﬁcation about what a com-
plex system is. There is no clear-cut deﬁ-
nition of a complex system. First, it must
be clear that the concept of complexity is
a twofold one: it may refer to a quality of
the system or to a quantitative concept. In
the ﬁrst case, complexity is what makes the
system complex. In the second, it is a con-
tinuum embracing both the simple and the
complex according to a given measure of
complexity. Standish [39] has stressed that
as a quality “complexity of a system refers
to the presence of emergence in the system,
or the exhibition of behaviour not speci-
ﬁed in the system speciﬁcation.” In other
words, complex systems “display organiza-
tion without any external organizing prin-
ciple being applied” [40]. When we speak
of complexity as a quantity, it “refers to the
amount of information needed to specify the
system.”
Then, what is a complex network? Before
attempting to answer this question let us
try to make a classiﬁcation of some of the
systems represented by networks (see [41])
by considering the nature of the links they
represent. The following are some examples
of these classes.
• Physical linking. Pairs of nodes are
physically connected by a tangible link,
such as a cable, a road, a vein, and so on.
Examples are the Internet, urban street
networks, road networks, vascular
networks, and so on.
• Physical interactions. The links
between pairs of nodes represent
interactions that are determined by a
physical force. Examples are protein
residue networks, protein–protein
interaction networks, and so on.
• “Ethereal” connections. The links
between pairs of nodes are intangible,
such that information sent from one
node is received at another irrespective
of the “physical” trajectory. Examples are
WWW, airports network.
• Geographic closeness. Nodes represent
regions of a surface and their
connections are determined by their
geographic proximity. Examples
arecountries in a map, landscape
networks, and so on.
• Mass/energy exchange. The links
connecting pairs of nodes indicate that
some energy or mass has been
transferred from one node to another.

138
4 Graph and Network Theory
Examples are reaction networks,
metabolic networks, food webs, trade
networks, and so on.
• Social connections. The links represent
any kind of social relationship between
nodes. Examples are friendship,
collaboration, and so on.
• Conceptual linking. The links indicate
conceptual relationships between pairs
of nodes. Examples are dictionaries,
citation networks, and so on.
Now, let us try to characterize the com-
plexity of these networks by giving the min-
imum amount of information needed to
describe them. For the sake of comparison,
let us also consider a regular and a ran-
dom graph of the same size as that of the
real-world networks we want to describe.
For the case of a regular graph, we only
need to specify the number of nodes and
the degree of the nodes (recall that every
node has the same degree). With this infor-
mation, many non-isomorphic graphs can
be constructed, but many of their topologi-
cal and combinatorial properties are deter-
mined by the information provided. In the
case of the random network, we need to
specify the number of nodes and the prob-
ability for joining pairs of nodes. As we
have seen in the previous section, most
of the structural properties of these net-
works are determined by this information.
In contrast, to describe the structure of
one of the networks representing a real-
world system, we need an awful amount of
information, such as number of nodes and
links, degree distribution, degree–degree
correlation, diameter, clustering, presence
of communities, patterns of communicabil-
ity, and other properties that we will study
in this section. However, even in this case,
a complete description of the system is still
far away. Thus, the network representa-
tion of these systems deserves the title of
complex networks because their topological
structures cannot be trivially described as
in the cases of random or regular graphs.
In closing, when referring to complex net-
works, we are making an implicit allusion
to the topological or structural complexity
of the graphs representing a complex sys-
tem. We will consider some general topo-
logical and dynamical properties of these
networks in the following sections and the
reader is recommended to consult the Fur-
ther Reading section at the end of this
chapter for more details and examples of
applications.
4.10
Small-World Networks
One of the most popular concepts in net-
work theory is that of the “small-world.” In
practically every language and culture, we
have a phrase saying that the world is small
enough so that a randomly chose person
has a connection with some of our friends.
The empirical grounds for this “concept”
come from an experiment carried out by
Stanley Milgram in 1967 [42]. Milgram
asked some randomly selected people in
the US cities of Omaha (Nebraska) and
Wichita (Kansas) to send a letter to a
target person who lives in Boston (Mas-
sachusetts) on the East Coast. The rules
stipulate that the letter should be sent to
somebody the sender knows personally.
Despite the senders and the target being
separated by about 2000 km, the results
obtained by Milgram were surprising for
the following reasons:
1. The average number of steps needed
for the letters to arrive to its target was
around six.
2. There was a large group inbreeding,
which resulted in acquaintances of one
individual feedback into his/her own

4.11 Degree Distributions
139
circle, thus usually eliminating new
contacts.
The assumption that the underlying
social network is a random one with char-
acteristics such as the ER network fails to
explain these ﬁndings. We already know
that an ER random network displays a very
small average path length, but it fails in
reproducing the large group inbreeding
observed because the number of triangles
and the clustering coeﬃcient in the ER
network are very small. In 1998, Watts
and Strogatz [8] proposed a model that
reproduces the two properties mentioned
in a simple way. Let n be the number
of nodes and k be an even number, the
Watt–Strogatz model starts by using the
following construction. Place all nodes
in a circle and connect every node to its
ﬁrst k∕2 clockwise nearest neighbors as
well as to its k∕2 counterclockwise nearest
neighbors (see Figure 4.11). This will create
a ring, which, for k > 2, is full of triangles
and consequently has a large clustering
coeﬃcient. The average clustering coeﬃ-
cient for these networks is given by Barrat
and Weigt [43]:
C = 3 (k −2)
4 (k −1),
(4.113)
which means that C = 0.75 for very large
values of k.
As can be seen in Figure 4.11 (top left),
the shortest path distance between any pair
of nodes that are opposite to each other in
the network is relatively large. This distance
is, in fact, equal to
⌈
n
k
⌉
. Then
l ≈(n −1) (n + k −1)
2kn
.
(4.114)
This relatively large average path length is
far from that of the Milgram experiment. In
order to produce a model with small aver-
age path length and still having relatively
large clustering, Watts and Strogatz con-
sider a probability for rewiring the links in
that ring. This rewiring makes the average
path length decrease very fast, while the
clustering coeﬃcient still remains high. In
Figure 4.12, we illustrate what happens to
the clustering and average path length as
the rewiring probability changes from 0 to
1 in a network.
4.11
Degree Distributions
One of the network characteristics that has
received much attention in the literature
is the statistical distribution of the node
degrees. Let p (k) = n (k) ∕n, where n (k)
is the number of nodes having degree k
in a network of size n. That is, p (k) repre-
sents the probability that a node selected
0
1
Rewiring probability p
Figure 4.11
Schematic representation of the evolution of
the rewiring process in the Watts–Strogatz model.

140
4 Graph and Network Theory
0.0
10−4
10−3
10−2
10−1
100
Rewiring probability, p
Lp/L0
Cp/C0
0.2
0.4
0.6
0.8
1.0
Clustering, path length
Figure 4.12
Schematic representation of the variation in
the average path length and clustering coeﬃcient with the
change of the rewiring probability in the Watts–Strogatz
model.
0.06
0.0
0.2
0.4
0.6
0.8
0.05
0.04
0.03
p(k)
p(k)
p(k)∼ k−γ
0.02
0.01
1.0
0.00
0
(a)
(b)
20
40
60
80
100
0
20
40
60
80
100
p(k) = e−k K−k
– –
K!
k
k
Figure 4.13
(a,b) Illustration of the Poisson and power-law
degree distributions found in complex networks.
uniformly at random has degree k. The
histogram of p (k) versus k represents the
degree distribution for the network. There
are hundreds of statistical distributions in
which the node degrees of a network can
ﬁt. A typical distribution that is expected
for a random network of the type of ER
is the Poisson distribution. However, a
remarkable characteristic of complex net-
works is that many of them display some
kind of “fat-tailed” degree distributions.
In these distributions, a few nodes appear
with very large degree, while most of the
nodes have relatively small degrees. The
prototypical example of these distributions
is the power-law one, which is illustrated
in the Figure 4.13, but others such as log
normal, Burr, logGamma, Pareto, and so
on [44] fall in the same category.
In
the
case
of
power-law
distribu-
tions (see Figure 4.13a), the probability of
ﬁnding a node with degree k decays as a

4.11 Degree Distributions
141
negative power of the degree: p (k) ∼k−𝛾.
This means that the probability of ﬁnding a
high-degree node is relatively small in com-
parison with the high probability of ﬁnding
low-degree nodes. These networks are usu-
ally referred to as scale-free networks. The
term scaling describes the existence of a
power-law relationship between the prob-
ability and the node degree: p (k) = Ak−𝛾.
Scaling the degree by a constant factor c
only produces a proportionate scaling of
the probability:
p (k, c) = A (ck)−𝛾= Ac−𝛾⋅p (k) .
(4.115)
Power-law relations are usually represented
in a logarithmic scale, leading to a straight
line, ln p (k) = −𝛾ln k + ln A, where −𝛾is
the slope and ln A the intercept of the func-
tion. Scaling by a constant factor c means
that only the intercept of the straight line
changes but the slope is exactly the same as
before: ln p (k, c) = −𝛾ln k −𝛾Ac.
Determining the degree distribution of
a network is a complicated task. Among
the diﬃculties, we can mention the fact
that sometimes the number of data points
used to ﬁt the distribution is too small
and sometimes the data are very noisy. For
instance, in ﬁtting power-law distributions,
the tail of the distribution, the part which
corresponds to high degrees, is usually
very noisy. There are two main approaches
in use for reducing this noise eﬀect in the
tail of probability distributions. One is
the binning procedure, which consists in
building a histogram using bin sizes that
increase exponentially with degree. The
other approach is to consider the cumu-
lative distribution function (CDF) [45].
The cumulative distributions of power-law
and Poisson distributions are given as
follows:
P (k) =
∞
∑
k′=k
p (k′),
(4.116)
P (k) = e−k
⌊k⌋
∑
i=1
(
k
)i
i!
,
(4.117)
which represent the probability of choos-
ing at random a node with degree greater
than or equal to k. In the case of power-
law degree distributions, P (k) also shows a
power-law decay with degree
P (k) ∼
∞
∑
k′=k
k′−𝛾∼k−(𝛾−1),
(4.118)
which means that we will also obtain a
straight line for the logarithmic plot of P (k)
versus k in scale-free networks.
4.11.1
“Scale-Free” Networks
Among the many possible degree dis-
tributions existing for a given network
the “scale-free” one is one of the most
ubiquitously found distributions. Conse-
quently, it is important to study a model
that is able to produce random networks
with such kind of degree distribution, that
is, a model in which the probability of
ﬁnding a node with degree k decreases
as a power-law of its degree. The most
popular of these models is the one intro-
duced by Barabási and Albert [46], which
is described below.
In the Barabási–Albert (BA) model, a
network is created by using the following
procedure. Start from a small number m0
of nodes. At each step, add a new node u
to the network and connect it to m ≤m0 of
the existing nodes v ∈V with probability
pu =
kv
∑
w
kw
.
(4.119)

142
4 Graph and Network Theory
We can assume that we start from a con-
nected random network of the ER type with
m0 nodes, GER = (V, E). In this case, the
BA process can be understood as a pro-
cess in which small inhomogeneities in the
degree distribution of the ER network grow
in time. Another option is the one devel-
oped by Bollobás and Riordan [47] in which
it is ﬁrst assumed that d = 1 and that the
ith node is attached to the jth one with
probability
pi =
⎧
⎪
⎪
⎨
⎪
⎪⎩
kj
1+
i−1
∑
j=0
kj
ifj < i
1
1+
i−1
∑
j=0
kj
ifj = i
.
(4.120)
Then, for d > 1, the network grows as if
d = 1 until nd nodes have been created
and the size is reduced to n by contract-
ing groups of d consecutive nodes into
one. The network is now speciﬁed by
two parameters and we denote this by
BA (n, d). Multiple links and self-loops are
created during this process and they can
be simply eliminated if we need a simple
network.
A characteristic of BA networks is that
the probability that a node has degree k ≥d
is given by
p (k) =
2d (d −1)
k (k + 1) (k + 2) ∼k−3,
(4.121)
which immediately implies that the cumu-
lative degree distribution is given by
P (k) ∼k−2.
(4.122)
For ﬁxed values d ≥1, Bollobás et al. [48]
have proved that the expected value for the
clustering coeﬃcient C is given by
C ∼d −1
8
log2n
n
,
(4.123)
for n →∞, which is very diﬀerent from the
value C ∼n−0.75 reported by Barabási and
Albert [46] for d = 2.
On the other hand, the average path
length has been estimated for the BA
networks to be as follows [47]:
l = ln n −ln (d∕2) −1 −𝛾
ln ln n + ln (d∕2)
+ 3
2,
(4.124)
where 𝛾is the Euler–Mascheroni constant.
This means that for the same number of
nodes and average degree, BA networks
have smaller average path length than
their ER analogs. Other alternative models
for obtaining power-law degree distribu-
tions with diﬀerent exponents 𝛾can be
found in the literature [49]. In closing,
using this preferential attachment algo-
rithm, we can generate random networks
that are diﬀerent from those obtained by
using the ER method in many important
aspects including their degree distribu-
tions, average clustering, and average path
length.
4.12
Network Motifs
The concept of network motifs was intro-
duced by Milo et al. [50] in order to charac-
terize recurring, signiﬁcant patterns in real-
world networks [50, 51]. A network motif
is a subgraph that appears more frequently
in a real network than could be expected if
the network were built by a random pro-
cess. In order to measure the statistical sig-
niﬁcance of a given subgraph, the Z-score is
used, which is deﬁned as follows for a given
subgraph i:
Zi =
Nreal
i
−⟨Nrandom
i
⟩
𝜎random
i
,
(4.125)

4.13 Centrality Measures
143
where Nreal
i
is the number of times the
subgraph i appears in the real network,
⟨Nrandom
i
⟩and 𝜎random
i
are the average and
standard deviation of the number of times
that i appears in the ensemble of random
networks, respectively. Some of the motifs
found by Milo et al. [50] in diﬀerent real-
world directed networks are illustrated in
the Figure 4.14.
4.13
Centrality Measures
Node centrality in a network is one of the
many concepts that have been created in
the analysis of social networks and then
imported to the study of any kind of net-
worked system. Measures of centrality try
to capture the notion of “importance” of
nodes in networks by quantifying the ability
of a node to communicate directly with
other nodes, or its closeness to many other
nodes, or the number of pairs of nodes
that need a speciﬁc node as intermediary
in their communications. Here we describe
some of the most relevant centrality mea-
sures currently in use for studying complex
networks.
The degree of a node was deﬁned in the
ﬁrst section. It was ﬁrst considered as a cen-
trality measure for nodes in a network by
Freeman [52] as a way to account for imme-
diate eﬀects taking place in a network. The
degree centrality can be written as
ki =
n
∑
j=1
Aij.
(4.126)
In directed networks, we have two diﬀerent
kinds of degree centrality, namely, the in-
and out-degree of a node:
kin
i =
n
∑
j=1
Aij,
(4.127)
kout
j
=
n
∑
i=1
Aij.
(4.128)
Another type of centrality is the closeness
centrality, which measures how close a
node is from the rest of the nodes in the
network. The closeness centrality [52] is
expressed mathematically as follows:
CC (u) = n −1
s (u) ,
(4.129)
where the distance sum s (u) is
s (u) =
∑
v∈V(G)
d (u, v).
(4.130)
The betweenness centrality quantifying the
importance of a node is in the communi-
cation between other pairs of nodes in the
network [52]. It measures the proportion
of information that passes through a given
node in the communications between other
pairs of nodes in the network and it is
deﬁned as follows:
BC (k) =
∑
i
∑
j
𝜌
(
i, k, j
)
𝜌(i, j) ,
i ≠j ≠k,
(4.131)
where 𝜌(i, j) is the number of shortest
paths from node i to node j, and 𝜌
(
i, k, j
)
is the number of these shortest paths that
pass through node k in the network.
The Katz centrality index for a node in a
network is deﬁned as [53]
Ki =
{[(I −𝜂−1A)−1 −I
]
1
}
i ,
(4.132)
where I is the identity matrix, 𝜂≠𝜆1 is
an attenuation factor (𝜆1 is the principal
eigenvector of the adjacency matrix), and 1
is column vector of 1’s. This centrality index
can be considered as an extension of the
degree in order to consider the inﬂuence

144
4 Graph and Network Theory
Motif
Network 
Three chains
Food webs
Feedforward loop
Gene regulation (transcription)
 
Neurons
Electronic circuits
Three-node feedback loop
Electronic circuits
Uplinked mutual dyad
World Wide Web
Feedback with two mutual dyads
World Wide Web
Fully connected triad
World Wide Web
Bi parallel
Food webs
Electronic circuits
Neurons 
Four-node feedback loop
Electronic circuits 
Bi fan
Gene regulations (transcription)
 
 
Neurons 
Electronic circuits
Figure 4.14
Illustration of some of the motifs found in real-world networks.
not only of the nearest neighbors but also
of the most distant ones.
The Katz centrality index can be deﬁned
for directed networks:
Kout
i
=
{[(
I −𝜂−1A)−1 −I
]
1
}
i ,
(4.133)
Kin
i =
{
1T [(
I −𝜂−1A)−1 −I
]}
i .
(4.134)
The Kin
i is a measure of the “prestige” of a
node as it accounts for the importance that
a node has due to other nodes that point
to it.

4.13 Centrality Measures
145
Another type of centrality that captures
the inﬂuence not only of nearest neighbors
but also of more distant nodes in a network
is the eigenvector centrality. This index was
introduced by Bonacich [54, 55] and is the
ith entry of the principal eigenvector of the
adjacency matrix
𝛗1 (i) =
(
1
𝜆1
A𝛗1
)
i
.
(4.135)
In directed networks, there are two types of
eigenvector centralities that can be deﬁned
by using the principal right and left eigen-
vectors of the adjacency matrix:
𝛗R
1(i) =
(
1
𝜆1
A𝛗R
1
)
i
,
(4.136)
𝛗L
1(i) =
(
1
𝜆1
AT𝛗L
1
)
i
.
(4.137)
Right eigenvector centrality accounts for
the “importance” of a node by taking into
account the “importance” of nodes to which
it points on, that is, it is an extension of the
out-degree concept by taking into account
not only nearest neighbors. On the other
hand, the left-eigenvector centrality mea-
sures the importance of a node by consid-
ering those nodes pointing toward the cor-
responding node and it is an extension of
the in-degree centrality. This is frequently
referred to as prestige in social sciences
contexts.
There is an important diﬃculty when
we try to apply right- and left-eigenvector
centralities to networks where there are
nodes having out-degree or in-degree equal
to zero, respectively. In the ﬁrst case, the
nodes pointing to a given node do not
receive any score for pointing to it. When
the in-degree is zero, the left-eigenvector
centrality or prestige of this node is equal to
zero as no node points to it, even though it
can be pointing to some important nodes.
A solution for this problem is obtained by
the following centrality measure.
The PageRank centrality measure is
the tool used by Google in order to rank
citations of web pages in the WWW [56].
Its main idea is that the importance of a
web page should be proportional to the
importance of other web pages pointing
to it. In other words, the PageRank of a
page is the sum of the PageRanks of all
pages pointing into it. Mathematically,
this intuition is captured by the following
deﬁnition. The PageRank is obtained by the
vector
(𝛑k+1)T =
(
𝛑k)T G.
(4.138)
The matrix G is deﬁned by
G = 𝛼S +
(1 −𝛼
n
)
11T,
(4.139)
where 0 ≤𝛼≤1 is a “teleportation” param-
eter, which captures the eﬀect in which a
web surfer abandons his random approach
of bouncing from one page to another and
initiates a new search simply by typing a
new destination in the browser’s URL com-
mand line. The matrix S solves the problem
of dead-end nodes in ranking web pages,
and it is deﬁned as
S = H + a
[( 1
n
)
1T]
,
(4.140)
where the entries of the dangling vector a
are given by
ai =
{ 1
ifkout
i
= 0
0
otherwise.
(4.141)
Finally, the matrix H is deﬁned as a modi-
ﬁed adjacency matrix for the network
Hij =
{
1
kout
i
if thre is a link from i to j
0
otherwise.

146
4 Graph and Network Theory
The matrix G is row stochastic, which
implies that its largest eigenvalue is equal to
one and the principal left-hand eigenvector
of G is given by
𝛑T = 𝛑TG,
(4.142)
where 𝛑T1 = 1 [56].
Another type of node centrality that is
based on the spectral properties of the
adjacency matrix of a graph is the subgraph
centrality. The subgraph centrality counts
the number of closed walks starting and
ending at a given node, which are mathe-
matically given by the diagonal entries of
Ak. In general terms, the subgraph central-
ity is a family of centrality measures deﬁned
on the basis of the following mathematical
expression:
fi (A) =
( ∞
∑
l=0
clAl
)
ii
,
(4.143)
where coeﬃcients cl are selected such that
the inﬁnite series converges. One partic-
ularly useful weighting scheme is the fol-
lowing, which eventually converges to the
exponential of the adjacency matrix [57]:
EE (i) =
( ∞
∑
l=0
Al
l!
)
ii
= (eA)
ii .
(4.144)
We can also deﬁne subgraph centralities
that take into account only contributions
from odd or even closed walks in the
network:
EEodd (i) = (sinh A)ii ,
(4.145)
EEeven (i) = (cosh A)ii .
(4.146)
A characteristic of the subgraph centrality
in directed networks is that it accounts
for the participation of a node in directed
walks. This means that the subgraph cen-
trality of a node in a directed network
is EE (i) > 1 only if there is at least one
closed walk that starts and returns to this
node. In other cases, EE (i) = 1, that is, the
subgraph centrality in a directed network
measures the returnability of “information”
to a given node.
4.14
Statistical Mechanics of Networks
Let us consider that every link of a network
is weighted by a parameter 𝛽. Evidently,
the case 𝛽= 1 corresponds to the simple
network. Let W be the adjacency matrix
of this homogeneously weighted network.
It is obvious that W = 𝛽A and the spec-
tral moments of the adjacency matrix are
Mr(W) = TrWr = 𝛽rTrAr = 𝛽rMr. Let us
now count the total number of closed walks
in this weighted network. It is straightfor-
ward to realize [58] that this is given by
Z (G; 𝛽) = Tr
∞
∑
r=0
𝛽rAr
r!
= Tre𝛽A =
n
∑
j=1
e𝛽𝜆j.
(4.147)
Let us now consider that the parameter
𝛽= (kBT)−1 is the inverse temperature of
a thermal bath in which the whole network
is submerged. Here the temperature is a
physical analogy for the external “stresses”
that a network is continuously exposed to.
For instance, let us consider the network
in which nodes represent corporations and
the links represent their business relation-
ships. In this case, the external stress can
represent the economical situation of the
world at the moment in which the net-
work is analyzed. In “normal” economical
situations we are in the presence of a low
level of external stress. In situations of eco-
nomical crisis, the level of external stress is
elevated.
We can consider the probability that the
network is in a conﬁguration (state) with

4.14 Statistical Mechanics of Networks
147
an energy given by the eigenvalue 𝜆j. The
conﬁguration or state of the network can
be considered here as provided by the cor-
responding eigenvector of the adjacency
matrix associated with 𝜆j. This probability
is then given by
pj =
e𝛽𝜆j
∑
j e𝛽𝜆j =
e𝛽𝜆j
Z (G; 𝛽),
(4.148)
which identiﬁes the normalization factor
as the partition function of the network.
This index, introduced by Estrada [59], is
known in the graph theory literature as the
Estrada index of the graph/network and
usually denoted by EE(G).
We can deﬁne the entropy for the net-
work,
S (G; 𝛽) = −kB
∑[pj
(𝛽𝜆j −ln Z)],
(4.149)
where we wrote Z (G; 𝛽) = Z for the sake of
economy.
The total energy H(G) and Helmholtz
free energy F(G) of the network, respec-
tively [58], are given by
H(G, 𝛽) = −1
EE
n
∑
j=1
(𝜆je𝛽𝜆j)
= −1
EETr (Ae𝛽A)
= −
n
∑
j=1
𝜆jpj,
(4.150)
F(G, 𝛽) = −𝛽−1 ln EE.
(4.151)
Known bounds for the physical parameters
deﬁned above, are the following:
0 ≤S (G, 𝛽) ≤𝛽ln n,
(4.152)
−𝛽(n −1) ≤H (G, 𝛽) ≤0,
(4.153)
−𝛽(n −1) ≤F(G, ) ≤−𝛽ln n,
(4.154)
where the lower bounds are obtained for
the complete graph as n →∞and the upper
bounds are reached for the null graph with
n nodes [58].
Next, let us analyze the thermodynamic
functions of networks for extreme values of
the temperature. At very low temperatures,
the total energy and Helmholtz free energy
are reduced to the interaction energy of the
network
[58]:
H (G, 𝛽→∞) = F(G, 𝛽→
∞) = −𝜆1. At very high temperatures,
𝛽→0, the entropy of the system is com-
pletely determined by the partition func-
tion of the network, S (G; 𝛽→0) = kB ln Z
and F(G, 𝛽→0) →−∞.
The introduction of these statistical
mechanics parameters allows the study of
interesting topological and combinatorial
properties of networks by using a well-
understood physical paradigm. For ﬁnding
examples of these applications, the reader
is referred to the specialized literature [41].
4.14.1
Communicability in Networks
The concept of network communicability is
a very recent one. However, it has found
applications in many diﬀerent areas of net-
work theory [35]. This concept captures
the idea of correlation in a physical system
and translates it into the context of net-
work theory. We deﬁne here that the com-
municability between a pair of nodes in a
network depends on all routes that con-
nect these two nodes [35]. Among all these
routes, the shortest path is the one mak-
ing the most important contribution as it
is the most “economic” way of connecting
two nodes in a network. Thus we can use
the weighted sum of all walks of diﬀerent

148
4 Graph and Network Theory
lengths between a pair of nodes as a mea-
sure of their communicability, that is,
Gpq =
∞
∑
k=0
(Ak)
pq
k!
= (eA)
pq,
(4.155)
where eA is the matrix exponential func-
tion. We can express the communicability
function for a pair of nodes in a network
by using the eigenvalues and eigenvectors
of the adjacency matrix:
Grs =
n
∑
j=1
𝜑j (r) 𝜑j (s) e𝜆j.
(4.156)
By using the concept of inverse temperature
introduced above, we can also express the
communicability function in terms of this
parameter [60]
Grs (𝛽) =
∞
∑
k=0
(𝛽Ak)
rs
k!
= (e𝛽A)
rs.
(4.157)
Intuitively, the communicability between
the two nodes connected by a path should
tend to zero as the length of the path tends
to inﬁnity. In order to show that this is
exactly the case, we can write the expres-
sion for Grs (𝛽) for the path Pn:
Grs =
1
n + 1
(
∑
j
cosj𝜋(r −s)
n + 1
−cosj𝜋(r + s)
n + 1
)
e
2cos
(
j𝜋
(n+1)
)
,
(4.158)
where we have used 𝛽≡1 without any loss
of generality. Then, it is straightforward to
realize by simple substitution in (4.158) that
Grs →0 for the nodes at the end of a linear
path as n →∞. At the other extreme, we
ﬁnd the complete network Kn, for which
Grs = en+1
n
+ e−1
n
∑
j=2
𝜙j(r)𝜙j(s)
= en+1
n
−1
ne = 1
ne (en −1) ,
(4.159)
which means that Grs →∞as n →∞.
In closing, the communicability measure
quantiﬁes very well our intuition that com-
munication decays to zero when only one
route exists for connecting two nodes at an
inﬁnite distance (the end nodes of a path)
and it tends to inﬁnity when there are many
possible routes of very short distance (any
pair of nodes in a complete graph).
4.15
Communities in Networks
The study of communities in complex net-
works is a large area of research with many
existing methods and algorithms. The aim
of all of them is to identify subsets of nodes
in a network the density of whose connec-
tions is signiﬁcantly larger than the density
of connections between them and the rest
of the nodes. It is impossible to give a com-
plete survey of all the methods for detect-
ing communities in networks in this short
section. Thus we are going to describe some
of the main characteristics of a group of
methods currently used for detecting com-
munities in networks. An excellent review
with details on the many methods available
can be found in [61].
The ﬁrst group of methods used for
detecting communities in networks is that
of partitioning methods. Their aim is to
obtain a partition of the network into p
disjoint sets of nodes such that
(i) ⋃p
i=1 Vi = V and Vi
⋂Vj = 𝜙for i ≠j,
(ii) the number of edges crossing between
subsets (cut size or boundary) is
minimized,

4.15 Communities in Networks
149
(iii) ||Vi|| ≈n∕p for all i = 1, 2, … , p, where
the vertical bars indicates the
cardinality of the set.
When condition (iii) is fulﬁlled, the
corresponding partition is called balanced.
There are several algorithms that have
been tested in the literature for the pur-
pose of network partitioning that include
local improvement methods and spectral
partitioning. The last family of partition
methods is based on the adjacency, Lapla-
cian, or normalized Laplacian matrices
of graphs. In general, their goal is to ﬁnd
a separation between the nodes of the
network based on the eigenvectors of
these matrices. This separation is carried
out grosso modo by considering that two
nodes v1, v2 are in the same partition if
sgn 𝜑M
2
(v1
) = sgn 𝜑M
2
(v2
) for M = A, L, ̃L.
Otherwise, they are considered to be in
two diﬀerent partitions {V1, V2
}. Sophisti-
cated versions of these methods exist and
the reader is referred to the specialized
literature for details [61].
The second group of methods is based
on edge centralities. We have deﬁned a
number of centrality measures for nodes
in a previous section of this chapter; these
can be extended to edges of a network in
a straightforward way. In these methods,
the aim is to identify edges that connect
diﬀerent communities. The best known
technique is based on edge betweenness
centrality, deﬁned for edges in a similar
way as for nodes. This method, known as
the Girvan–Newman algorithm [62], can
be summarized in the following steps:
1. Calculate the edge betweenness
centrality for all links in the network.
2. Remove the link with the largest edge
betweenness or any of them if more
than one exists.
3. Recalculate the edge betweenness for
the remaining links.
4. Repeat until all links have been
removed.
5. Use a dendrogram for analyzing the
community structure of the network.
Using this dendrogram, a hierarchy of
diﬀerent communities is identiﬁed, which
can be discriminated by using diﬀerent
quality criteria. The most popular among
these quality criteria is the so-called mod-
ularity index. In a network consisting of nV
partitions, V1, V2, … , VnC, the modularity
is the sum over all partitions of the diﬀer-
ence between the fraction of links inside
each partition and the expected fraction
by considering a random network with the
same degree for each node [63]:
Q =
nC
∑
k=1
⎡
⎢
⎢
⎢⎣
||Ek||
m −
⎛
⎜
⎜
⎜⎝
∑
j∈Vk
kj
2m
⎞
⎟
⎟
⎟⎠
2⎤
⎥
⎥
⎥⎦
,
(4.160)
where ||Ek|| is the number of links between
nodes in the kth partition of the network.
Modularity is interpreted in the following
way. If Q = 0, the number of intracluster
links is not bigger than the expected value
for a random network. Otherwise, Q = 1
means that there is a strong community
structure in the network given by the par-
tition analyzed.
The third group of community detection
methods is based on similarity measures
for the nodes in a network. Such similar-
ity measures for the nodes of a network
can be based on either rows or columns
of the adjacency matrix of the network.
For instance, we can consider as a mea-
sure of similarity between two nodes the
angle between the corresponding rows or
columns of these two nodes in the adja-
cency matrix of the graph. This angle is
deﬁned as

150
4 Graph and Network Theory
𝜎ij = cos 𝜗ij =
xTy
‖x‖ ⋅‖y‖,
(4.161)
which can be seen to equal
𝜎ij =
𝜂ij
√
kikj
,
(4.162)
where 𝜂ij is the number of common neigh-
bors of nodes i and j.
Other similarity measures between rows
or columns of the adjacency matrices are
the Pearson correlation coeﬃcient, diﬀer-
ent types of norms and distances (Manhat-
tan, Euclidean, inﬁnite), and so on. Once a
similarity measure has been chosen, any of
the variety of similarity-based methods for
detecting communities in a network can be
used.
4.16
Dynamical Processes on Networks
There are many dynamical processes that
can be deﬁned on graphs and networks.
The reader should be aware that this is a
vast area of multidisciplinary research with
a huge number of publications in diﬀerent
ﬁelds.
4.16.1
Consensus
We will start here with a simple model for
analyzing consensus among the nodes in a
network. We consider a graph G = (V, E)
whose nodes represent agents in a complex
system and the edges represent interactions
between such agents. In such multiagent
system, consensus means an agreement
regarding a certain quantity of interest.
An example is a collection of autonomous
vehicles engaged in cooperative teamwork
in civilian and military applications. Such
coordinated activity allows them to per-
form missions with greater eﬃcacy than if
they perform solo missions [64].
Let n = |V| be the number of agents
forming a network. The collective dynam-
ics of the group of agents is represented by
the following equations for the continuous-
time case:
̇𝛗= −L𝛗, 𝛗(0) = 𝛗0,
(4.163)
where 𝛗0 is the original distribution, which
may represent opinions, positions in space,
or other quantities with respect to which
the agents should reach a consensus. The
reader surely already has recognized that
(4.163) is identical to the heat equation,
𝜕u
𝜕t = hΔu,
(4.164)
where h is a positive constant and ∇2 =
−L is the Laplace operator. In general, this
equation is used to model the diﬀusion of
“information” in a physical system, where,
by information we can understand heat, a
chemical substance, or opinions in a social
network.
A consensus is reached if, for all 𝜑i (0)
and all i, j = 1, … , n, |||𝜑i (t) −𝜑j (t)||| →0
as t →0. The discrete-time version of the
model has the form
𝛗i(t + 1) = 𝛗i(t) + 𝜀
∑
j∼i
Aij
[𝛗j (t) −𝛗i (t)
],
𝛗(0) = 𝛗0,
(4.165)
where 𝛗i (t) is the value of a quantitative
measure on node i, 𝜀> 0 is the step-size,
and j ∼i indicates that node j is connected
to node i. It has been proved that the con-
sensus is asymptotically reached in a con-
nected graph for all initial states if 0 < 𝜀<
1∕𝛿max, where 𝛿max is the maximum degree
of the graph. The discrete-time collective

4.16 Dynamical Processes on Networks
151
0
0
5
10
15
35
30
25
20
500
1000
1500
Time
State
Figure 4.15
Time evolution
of consensus dynamics in a
real-world social network with
random initial states for the
nodes.
dynamics of the network can be written in
matrix form as [64] as
𝛗(t + 1) = P𝛗(t) , 𝛗(0) = 𝛗0,
(4.166)
where P = I −𝜀L, and I is the n × n iden-
tity matrix. The matrix P is the Perron
matrix of the network with parameter 0 <
𝜀< 1∕𝛿max. For any connected undirected
graph, the matrix P is an irreducible, dou-
bly stochastic matrix with all eigenvalues 𝜇j
in the interval [−1, 1] and a trivial eigen-
value of 1. The reader can ﬁnd the pre-
viously mentioned concepts in any book
on elementary linear algebra. The relation
between the Laplacian and Perron eigenval-
ues is given by 𝜇j = 1 −𝜀𝜆j.
In Figure 4.15, we illustrate the consen-
sus process in a real-world social network
having 34 nodes and 78 edges.
4.16.2
Synchronization in Networks
A problem closely related to that of consen-
sus in networks is one of synchronization
[65, 66]. The phenomenon of synchro-
nization appears in many natural systems
consisting of a collection of oscillators cou-
pled to each other. These systems include
animal
and
social
behavior,
neurons,
cardiac pacemaker cells, among others.
We can start by considering a network
G = (V, E) with |V| = n nodes represent-
ing coupled identical oscillators. Each node
is an N-dimensional dynamical system that
is described by the following equation:
̇xi = f
(
xi
) + c
n
∑
j=1
LijH (t) xj,
i = 1, … , n,
(4.167)
where xi = (xi1, xi2, … , xiN
) ∈ℝN is the
state vector of the node i, f (⋅) ∶ℝN →ℝN
is
a
smooth
vector-valued
function
that deﬁnes the dynamics, c is a con-
stant representing the coupling strength,
H (⋅) ∶ℝN →ℝN is a ﬁxed output func-
tion also known as the outer coupling
matrix, t is the time, and Lij are the ele-
ments of the Laplacian matrix of the
network (sometimes the negative of the
H (xi
) ≈H (s) + 𝜉iH′ (s) Laplacian matrix

152
4 Graph and Network Theory
is taken here). The network is said to
achieve synchronization if
x1(t)=x2(t)=· · ·=xn(t) →s(t), ast →∞.
(4.168)
Let us now consider a small perturbation 𝜉i
such that xi = s + 𝜉i (𝜉i ≪s) and let us ana-
lyze the stability of the synchronized man-
ifold x1 = x2 = · · · = xn. First, we expand
the terms in (4.167) as
f (xi
) ≈f (s) + 𝜉if ′ (s) ,
(4.169)
H (xi
)
≈H (s) + 𝜉iH′ (s) ,
(4.170)
where the primes refers to the derivatives
respect to s. Thus, the evolution of the per-
turbations is determined by the following
equation:
̇𝜉i = f ′ (s) 𝜉i + c
∑
j
[LijH′ (s)
]𝜉j.
(4.171)
It is known that the system of equations
for the perturbations can be decoupled by
using the set of eigenvectors of the Lapla-
cian matrix, which are an appropriate set
of linear combinations of the perturbations.
Let 𝜙j be an eigenvector of the Laplacian
matrix of the network associated with the
eigenvalue 𝜇j. Recall that the Laplacian is
positive semideﬁnite, that is, 0 = 𝜇1 ≤𝜇2 ≤
· · · ≤𝜇n ≡𝜇max. Then
̇𝜙i = [f ′ (s) + c𝜇iH′ (s)
] 𝜙i.
(4.172)
Let us now assume that at short times the
variations of s are small enough to allow
us to solve these decoupled equations, with
the solutions being
̇𝜙i (t) = 𝜙0
i exp
{[
f ′ (s) + c𝜇iH′ (s)
] t} ,
(4.173)
where 𝜙0
i is the initially imposed perturba-
tion.
We now consider the term in the expo-
nential of (4.173), Λi = f ′ (s) + c𝜇iH′ (s).
If
f ′ (s) > c𝜇iH′ (s) ,
the
perturbations
will
increase
exponentially,
while
if
f ′ (s) < c𝜇iH′ (s) ,
they
will
decrease
exponentially. So, the behavior of the
perturbations in time is controlled by the
magnitude of 𝜇i. Then, the stability of
the synchronized state is determined by
the master stability function:
Λ (𝛼) ≡max
s
[f ′ (s) + 𝛼H′ (s)
],
(4.174)
which corresponds to a large number of
functions f , and H is represented in the
Figure 4.16.
Λ(α)
0
α1
α2
α
Figure 4.16
Schematic representation of
the typical behavior of the master stability
function.

4.16 Dynamical Processes on Networks
153
S
I
R
Figure 4.17
Diagrammatic representation of an SIR model.
As can be seen, the necessary condition
for stability of the synchronous state is that
c𝜇i is between 𝛼1 and 𝛼2, which is the region
where Λ (𝛼) < 0. Then, the condition for
synchronization is [67]:
Q ∶= 𝜇N
𝜇2
< 𝛼2
𝛼1
,
(4.175)
that is, synchronizability of a network is
favored by a small eigenratio Q, which
indeed depends only on the topology of
the network. There are many studies on
the synchronizability of networks using
diﬀerent types of oscillators and the reader
is referred to the specialized literature for
the details (See Further Reading, [68]).
4.16.3
Epidemics on Networks
Another area in which the dynamical pro-
cesses on networks play a fundamental role
is the study of the spread of epidemics.
These models are extensions of the clas-
sical models used in epidemiology that
consider the inﬂuence of the topology of a
network on the propagation of an epidemic
[69]. The simplest model assumes that an
individual who is susceptible (S) to an infec-
tion could become infected (I). In a second
model, the infected individual can also
recover (R) from infection. The ﬁrst model
is known as an SI model, while the second is
known as a susceptible–infected–recovered
(SIR) model. In a third model, known as
the susceptible–infected–susceptible (SIS),
an individual can be reinfected, so that
infections do not confer immunity on an
infected individual. Finally, a model known
as SIRS allows for recovery and reinfec-
tion as an attempt to model the temporal
immunity conferred by certain infections.
Here we brieﬂy consider only the SIR and
SIS models on networks.
In the SIR model there are three compart-
ments as sketched in the Figure 4.17, that is,
in a network G = (V, E), a group of nodes
S ⊆V are considered susceptible and they
can be infected by directed contact with
infected individuals. Let si, xi, and ri be the
probabilities that the node i is susceptible,
infected, or has recovered. The evolution of
these probabilities in time is governed by
the following equations that deﬁne the SIR
model:
̇si = −𝛽si
∑
j
Aijxj,
(4.176)
̇xi = 𝛽si
∑
j
Aijxj −𝛾xi,
(4.177)
̇ri = 𝛾xi,
(4.178)
where 𝛽is the spreading rate of the
pathogen, Aij is an entry of the adjacency
matrix of the network, and 𝛾is the proba-
bility that a node recovers or dies, that is,
the recovery rate.
In the SIS model, the general ﬂow chart
of the infection can be represented as in
Figure 4.18:
The equations governing the evolution of
the probabilities of susceptible and infected
individuals are given as follows:
̇si = −𝛽si
∑
j
Aijxj + 𝛾xi,
(4.179)
̇xi = 𝛽si
∑
j
Aijxj −𝛾xi.
(4.180)
The analysis of epidemics in networks is
of tremendous importance in modern life.
Today, there is a large mobility of people
across cities, countries, and the entire
world and an epidemic can propagate

154
4 Graph and Network Theory
S
I
Figure 4.18
Diagrammatic representation of an SI model.
through the social networks at very high
rates. The reader can ﬁnd a few examples
in the specialized literature [69].
Glossary
Adjacency matrix of a simple graph: a
binary symmetric matrix whose row and
columns represent the vertices of the graph,
where the i, j entry is one if the correspond-
ing vertices i and j are connected.
Betweenness centrality a centrality mea-
sure for a node that characterizes how cen-
tral a node is in passing information from
other nodes.
Bipartite graph a graph with two sets of
vertices, the nodes of each set being con-
nected only to nodes of the other set.
Bridge an edge whose deletion increases
the number of connected components of
the graph.
Centrality measure an index for a node or
edge of a graph/network that characterizes
its topological or structural importance.
Closeness centrality a centrality measure
for a node that characterizes how close the
node is with respect to the rest in terms of
the shortest-path distance.
Clustering coeﬃcient the ratio of the
number of triangles incident to a node in a
graph to the maximum possible number of
such triangles.
Communicability a measure of how well-
communicated a pair of nodes is by consid-
ering all possible routes of communication
in a graph/network.
Complete graph a graph in which every
pair of vertices are connected to each other.
Connected graph a graph in which there is
a path connecting every pair of nodes.
Cycle a path in which the initial and end
vertices coincide.
Cycle graph a graph in which every node
has degree two.
Degree a centrality measure for a node that
counts the number of edges incident to a
node.
Degree distribution the statistical distri-
bution of the degrees of the nodes of a
graph.
Edge contraction a graph operation in
which an edge of the graph is removed and
the two end nodes are merged together.
Edge deletion a graph operation in which
an edge of the graph is removed leaving the
end nodes in the graph.
Erdös–Rényi graph
a
random
graph
formed from a given set of nodes and a
probability of create edges among them.
Forest a graph formed by several compo-
nents all of which are trees.
Girth the size of the minimum cycle in a
graph.
Graph a pair formed by a set of vertices or
nodes and a set of edges.
Graph diameter the length of the largest
shortest-path distance in a graph.
Graph diameter the maximum shortest-
path distance in a graph.
Graph invariant a characterization of a
graph that does not depend on the labeling
of vertices or edges.
Graph nullity the multiplicity of the zero
eigenvalue of the adjacency matrix, that is,
the number of times eigenvalue zero occurs
in the spectrum of the adjacency matrix.
Hydrocarbon a molecule formed only by
carbon and hydrogen.
Incidence matrix of a graph a matrix
whose rows correspond to vertices and
whose columns correspond to edges of the

References
155
graph and the i, j entry is one or zero if the
ith vertex is incident with the jth edge or
not, respectively.
Laplacian matrix a square symmetric
matrix with diagonal entries equal to the
degree of the corresponding vertex and
oﬀ-diagonal entries equal to −1 or zero
depending on whether the corresponding
vertices are connected or not, respectively.
Loop an edge that is doubly incident to the
same node.
Matching of a graph the number of mutu-
ally nonadjacent edges in the graph.
Mean displacement of an atom (vertex):
refers to the oscillations of an atoms from
its equilibrium position due to thermal ﬂuc-
tuations.
Molecular Hamiltonian the operator rep-
resenting the energy of the electrons and
atomic nuclei in a molecule.
Network community a subset of nodes in
a graph/network that are better connected
among themselves than with the rest of the
nodes.
Network motif a subgraph in a graph that
is overrepresented in relation to a random
graph of the same size.
Path a sequence of diﬀerent consecutive
vertices and edges in a graph.
Path graph a tree in which all nodes have
degree two except two nodes, which has
degree one.
Regular graph a graph in which every node
has the same degree.
Resistance distance the distance between
any pair of vertices of the graph, deter-
mined by the Kirchhoﬀrules for electrical
sets.
Scale-free network a network/graph with
a power-law degree distribution.
Shortest path between two nodes a path
having the least number of edges among all
paths connecting two vertices.
Simple graph a graph without multiple
edges, self-loops, and weights.
Spanning forest a subgraph of a graph that
contains all the nodes of the graph and is a
forest.
Spanning tree a subgraph of a graph that
contains all the nodes of the graph and is
also a tree.
Star graph a tree consisting of a node with
degree n −1 and n −1 nodes of degree one.
Tree a graph that does not have any cycle.
Vertex degree the number of vertices adja-
cent to a given vertex.
Walk a sequence of (not necessarily) dif-
ferent consecutive vertices and edges in a
graph.
References
1. Euler L. (1736) Comm. Acad. Sci. Imp.
Petrop., 8, 128–140.
2. Biggs, N.L., Lloyd, E. K. and Wilson, L.
(1976) Graph Theory 1736–1936. Clarendon
Press, Oxford.
3. Sylvester, J. J. (1877–1878) Nature 17,
284–285.
4. Harary, F. Ed. (1968) Graph Theory and
Theoretical Physics. Academic Press.
5. Trinajsti´c, N. (1992) Chemical Graph Theory.
CRC Press, Boca Raton, FL.
6. Berkolaiko, G., Kuchment, P. (2013)
Introduction to Quantum Graphs, vol. 186,
American Mathematical Society, Providence,
RI.
7. Harary, F. (1969) Graph Theory.
Addison-Wesley, Reading, MA.
8. Watts, D. J., Strogatz, S. H. (1998) Nature
393, 440–442.
9. Newman, M. E. J., Strogatz, S. H., Watts, D. J.
(2001) Phys. Rev. E 64, 026118.
10. Canadell, E., Doublet, M.-L., Iung, C. (2012)
Orbital Approach to the Electronic Structure
of Solids. Oxford University Press, Oxford.
11. Kutzelnigg, W. (2006) J. Comput. Chem. 28,
25–34.
12. Powell, B.J. (2009) An Introduction to
Eﬀective Low-Energy Hamiltonians in
Condensed Matter Physics and Chemistry.
arXiv preprint arXiv:0906.1640.
13. Gutman, I. (2005). J. Serbian Chem. Soc. 70,
441–456.

156
4 Graph and Network Theory
14. Borovi´canin, B., Gutman, I. (2009) In
Applications of Graph Spectra, D. Cvetkovi´c
and I. Gutman Eds. Mathematical Institute
SANU, pp. 107–122.
15. Cheng, B., Liu, B. (2007) Electron. J. Linear
Algebra 16 (2007), 60–67.
16. Tasaki, H. (1999) J. Phys.: Cond. Mat., 10,
4353.
17. Lieb, E. H. (1989) Phys. Rev. Lett. 62, 1201
(Erratum 62, 1927, (1989)).
18. Morita, Y., Suzuki, S., Sato, K., Takui, T.
(2011) Nat. Chem. 3, 197–204.
19. Essam, J. W. (1971) Discrerte Math. 1,
83–112.
20. Beaudin, L., Ellis-Monaghan, J., Pangborn,
G., Shrock, R. (2010) Discrete Math. 310,
2037–2053.
21. Welsh, D. J. A., Merino, C. (2000) J. Math.
Phys. 41, 1127–1152.
22. Bollobás, B., (1998) Modern Graph Theory.
Springer-Verlag, New York.
23. Ellis-Monaghan, J.A. and Merino, C. (2011)
In Structural Analysis of Complex Networks,
ed. M. Dehmer, Birkhauser, Boston, MA, pp.
219–255.
24. Welsh, D. (1999) Random Struct. Alg., 15,
210–228.
25. Bogner, C. (2010) Nucl. Phys. B Proc. Suppl.
205, 116–121.
26. Bogner, C. and Weinzierl S. (2010) Int. J.
Mod. Phys. A 25, 2585.
27. Weinzierl, S. (2010) Introduction to
Feynman Integrals. arXiv preprint
arXiv:1005.1855.
28. Dodgson, C. L. (1866) Proc. R. Soc. London
15, 150–155.
29. Doyle, P. and Snell, J. (1984) Random Walks
and Electric Networks. Carus Mathematical
Monographs, vol. 22, The Mathematical
Association of America, Washington, DC.
30. Klein, D. J., Randi´c, M. (1993) J. Math. Chem.
12, 81–95.
31. Bapat, R. B., Gutman, I., Xiao, W. (2003) Z.
Naturforsch. 58a, 494 – 498.
32. Xiao, W., Gutman, I., (2003) Theor. Chem.
Acc. 110, 284–289.
33. Gutman, I., Xiao, O. (2004) Bull. Acad. Serb.
Sci. Arts. 29, 15–23.
34. Ghosh, A., Boyd, S., Saberi, A. (2008) SIAM
Rev. 50, 37–66.
35. Estrada, E., Hatano, N., Benzi, M., (2012)
Phys. Rep. 514, 89–119.
36. Erdös, P., Rényi, A. (1959) Publ. Math.
Debrecen 5, 290–297.
37. Janson, S. (2005) J. Combin. Prob. Comput.
14, 815–828.
38. Wigner, E. P. (1955) Ann. Math. 62,
548–564.
39. Standish, R. K. (2008) In Intelligent Complex
Adaptive Systems, Yang, A. and Shan, Y. eds,
IGI Global: Hershey, PA, pp. 105–124,
arXiv:0805.0685.
40. Ottino, J. M. (2003) AIChE J., 49, 292–299.
41. Estrada, E. (2011) The Structure of Complex
Networks. Theory and Applications. Oxford
University Press, Oxford.
42. Milgram, S. (1967) Psychol. Today 2, 60–67.
43. Barrat, A., Weigt, M. (2000) Eur. Phys. J. B
13, 547–560.
44. Foss, S., Korshunov, D., Zachary, S. (2011)
An Introduction to Heavy-Tailed and
Subexponential Distributions. Springer,
Berlin.
45. Clauset, A., Rohilla Shalizi, C., Newman, M.
E. J. (2010) SIAM Rev. 51, 661–703.
46. Barabási, A.-L. Albert, R. (1999) Science 286,
509–512.
47. Bollobás, B., Riordan, O. (2004)
Combinatorica 24, 5–34.
48. Bollobás, B. (2003) In Handbook of Graph
and Networks: From the Genome to the
Internet, Bornholdt, S., Schuster, H. G. Eds.
Wiley-VCH Verlag GmbH, Weinheim,
pp. 1–32.
49. Dorogovtsev, S. N. and Mendes, J. F. F.
(2003) Evolution of Networks: From
Biological Nets to the Internet and WWW.
Oxford University Press, Oxford.
50. Milo, R., Shen-Orr, S., Itzkovitz, S. Kashtan,
N. Chklovskii, D. Alon, U. (2002) Science
298, 824–827.
51. Milo, R., Itzkovitz, S., Kashtan, N., Levitt, R.,
Shen-Orr, S., Ayzenshtat, I., Sheﬀer, M.,
Alon, U. (2004) Science 303, 1538–1542.
52. Freeman, L. C. (1979) Social Networks 1,
215–239.
53. Katz, L. (1953) Psychometrica 18, 39–43.
54. Bonacich, P. (1972) J. Math. Sociol. 2,
113–120.
55. Bonacich, P., (1987) Am. J. Soc. 92,
1170–1182.
56. Langville, A. N., Meyer, C. D. (2006). Google’s
PageRank and Beyond. The Science of Search
Engine Rankings. Princeton University Press,
Princeton, NJ.
57. Estrada, E., Rodr´ıguez-Velázquez, J. A.
(2005) Phys. Rev. E 71, 056103.

Further Reading
157
58. Estrada, E., Hatano, N. (2007) Chem. Phys.
Lett. 439, 247–251.
59. Estrada, E. (2000) Chem. Phys. Lett. 319,
713–718.
60. Estrada, E., Hatano, N. (2008) Phys. Rev. E
77, 036111.
61. Fortunato, S. (2010) Phys. Rep. 486, 75–174.
62. Girvan, M., Newman, E. J. (2002) Proc. Natl.
Acad. Sci. U.S.A. 99, 7821–7826.
63. Newman, M. E. J. (2006) Proc. Natl. Acad.
Sci. U.S.A. 103, 8577–8582.
64. Olfati-Saber, R., Fax, J. A., Murray, R. M.
(2007) Proc. IEEE 95, 215–233.
65. Arenas, A., Diaz-Guilera, A., Pérez-Vicente,
C. J. (2006). Physica D 224, 27–34.
66. Chen, G., Wang, X., Li, X., Lü , J. (2009) In
Recent Advances in Nonlinear Dynamics and
Synchronization, K. Kyamakya Ed.,
Springer-Verlag, Berlin, pp. 3–16.
67. Barahona, M., Pecora, L. M. (2002) Phys.
Rev. Lett. 89, 054101.
68. Barrat, A., Barthélemy, M., Vespignani, A.
(2008) Dynamical Processes on Complex
Networks. Cambridge University Press,
Cambridge.
69. Keeling, M. J., Eames, K. T. (2005) J. R. Soc.
Interface 2, 295–307.
Further Reading
Bollobás, B. (1998) Modern Graph Theory.
Springer, Berlin.
Caldarelli, G. (2007) Scale-Free Networks.
Complex Webs in Nature and Technology.
Oxford University Press, Oxford.
Cvetkovi´c, D., Rowlinson, P., Simi´c, S. (2010) An
Introduction to the Theory of Graph Spectra.
Cambridge University Press, Cambridge.
Nakanishi, N. (1971) Graph Theory and Feynman
Integrals. Gordon and Breach.


159
5
Group Theory
Robert Gilmore
5.1
Introduction
Symmetry has sung its siren song to physi-
cists since the beginning of time, or since
even before there were physicists. Today,
the ideas of symmetry are incorporated into
a subject with the less imaginative and sug-
gestive name of group theory. This chapter
introduces many of the ideas of group the-
ory that are important in the natural sci-
ences.
Natural philosophers in the past have
come up with many imaginative arguments
for estimating physical quantities. They
have often used out of the box methods
that were proprietary, to pull rabbits out
of hats. When these ideas were made
available to a wider audience, they were
often improved upon in unexpected and
previously unimaginable ways. A number
of these methods are precursors of group
theory. These are dimensional analysis,
scaling theory, and dynamical similar-
ity. We review these three methods in
Section 5.2.
In Section 5.3, we get down to the busi-
ness at hand, introducing the deﬁnition of
a group and giving a small set of important
deﬁnitions (e.g., isomorphism and homo-
morphism). Others will be introduced later
in a context in which they make immediate
sense. In Sections 5.4–5.6, we present
some useful examples of groups ranging
from ﬁnite and inﬁnite discrete groups
through matrix groups to Lie groups.
These examples include transformation
groups, which played an important if
under-recognized rôle in the development
of classical physics, in particular, the the-
ories of special and general relativity. The
relation between these theories and group
theory is indicated in Section 5.9.
The study of Lie groups is greatly
simpliﬁed
when
carried
out
on
their
“inﬁnitesimal” versions. These are Lie alge-
bras, which are introduced in Section 5.7.
In this section, we introduce many of the
important concepts and provide examples
to illustrate all of them. One simple con-
sequence of these beautiful developments
is the possibility of studying and classify-
ing Riemannian symmetric spaces. These
are Riemannian spaces with a particular
symmetry that is, eﬀectively, time reversal
invariance at each point. These spaces
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

160
5 Group Theory
are cosets (quotients) of one Lie group by
another. They are introduced in Section 5.8.
Despite this important rôle in the devel-
opment of physics, groups existed at the
fringe of the physics of the early twentieth
century. It was not until the theory of the
linear matrix representations of groups was
invented that the theory of groups migrated
from the outer fringes to play a more cen-
tral rôle in physics. Important points in the
theory of representations are introduced
in Section 5.10. Representations were used
in an increasingly imaginative number of
ways in physics throughout the twentieth
century. Early on, they were used to label
states in quantum systems with a symme-
try group: for example, the rotation group
SO(3). Once states were named, degen-
eracies could be predicted and computa-
tions simpliﬁed. Such applications are indi-
cated in Section 5.11. Later, they were used
when symmetry was not present, or just
the remnant of a broken symmetry was
present. When used in this sense, they are
often called “dynamical groups.” This type
of use greatly extended the importance of
group theory in physics. Some such appli-
cations of group theory are presented in
Section 5.12.
As a latest tour de force in the develop-
ment of physics, groups play a central rôle
in the formulation of gauge theories. These
theories describe the interactions between
fermions and the bosons and lie at the heart
of the standard model. We provide the sim-
plest example of a gauge theory, based on
the simplest compact one-parameter Lie
group U(1), in Section 5.13.
For
an
encore,
in
Section 5.14,
we
show
how the
theory
of
the special
functions of mathematical physics (Leg-
endre and associated Legendre functions,
Laguerre and associated Laguerre func-
tions, Gegenbauer, Chebyshev, Hermite,
Bessel functions, and others) are subsumed
under the theory of representations of
some low-dimensional Lie groups. The
classical theory of special functions came
to fruition in the mid nineteenth cen-
tury, long before Lie groups and their
representations were even invented.
5.2
Precursors to Group Theory
The axioms used to deﬁne a group were
formulated in the second half of the nine-
teenth century. Long before then, the
important ideas underlying these axioms
were used to derive classical results (for
example, Pythagoras’ theorem: see the fol-
lowing text) in alternative, simpler, and/or
more elegant ways, to obtain new results,
or to consolidate diﬀerent results under a
single elegant argument. In this section, we
survey some of these imaginative lines of
thought. We begin with a simple argument
due to Barenblatt that has been used to
derive Pythagoras’ theorem. We continue
with a discussion of the central features
of dimensional analysis and illustrate how
this tool can be used to estimate the size of
a hydrogen atom. We continue in the same
vein, using scaling arguments to estimate
the sizes of other “atom-like” structures
based on the known size of the hydrogen
atom. We conclude this section with a brief
description of dynamical similarity and
how the arguments intrinsic to this line
of thinking can be used to estimate one
of Kepler’s laws and to place four classical
mechanics laws (Kepler, Newton, Galileo,
Hooke) in a common framework.
We emphasize that group theory is not
used explicitly in any of these arguments
but its ﬁngerprints are everywhere. These
digressions should serve as appetizers to
indicate the power of the tool called group
theory in modern physical theories.

5.2 Precursors to Group Theory
161
5.2.1
Classical Geometry
Barenblatt [1] has given a beautiful deriva-
tion of Pythagoras’ theorem that is out of
the box and suggests some of the ideas
behind dimensional analysis. The area
of the right triangle Δ(a, b, c) is 1∕2ab
(Figure 5.1). Dimensionally, the area is
proportional to square of any of the sides,
multiplied by some factor. We make a
unique choice of side by choosing the
hypotenuse, so that Δ(a, b, c) = c2 × f (𝜃), 𝜃
is one of the two acute angles, and f (𝜃) ≠0
unless 𝜃= 0 or 𝜋∕2. Equating the two
expressions
f (𝜃) = 1
2
(a
c
) (
b
c
)
= 1
2
(
b
c
) (a
c
) symmetry
=
f
(𝜋
2 −𝜃
)
.(5.1)
This shows (a) that the same function
f (𝜃) applies for all similar triangles and (b)
f (𝜃) = f (𝜋∕2 −𝜃). The latter result is due to
reﬂection “symmetry” of the triangle about
the bisector of the right angle: the triangle
changes but its area does not. We need (a)
alone to prove Pythagoras’ theorem. The
proof is in the ﬁgure caption.
5.2.2
Dimensional Analysis
How big is a hydrogen atom?
The size of the electron “orbit” around
the proton in the hydrogen atom ought
to depend on the electron mass me,
or more precisely the electron–proton
reduced
mass
𝜇= meMP∕(me + MP).
It should also depend on the value of
Planck’s constant h or reduced Planck’s
constant
ℏ= h∕2𝜋.
Since
the
interac-
tion between the proton with charge e
and the electron with charge −e is elec-
tromagnetic, of the form V(r) = −e2∕r
(Gaussian
units),
it
should
depend
on e2.
Mass is measured in grams. The dimen-
sions
of
the
charge
coupling
e2
are
determined by recognizing that e2∕r is
a
(potential)
energy,
with
dimensions
M1L2T−2. We will use capital letters M
(mass), L (length), and T (time) to charac-
terize the three independent dimensional
“directions.” As a result, the charge coupling
strength e2 has dimensions ML3T−2 and
is measured in g(cm)3 s−2. The quantum
of action ℏhas dimensions [ℏ] = ML2T−1.
Here and below we use the standard
convention that [∗] is to be read “the
dimensions of ∗are.”
Cons−
Dimensions
Value
Units
tant
𝜇
M
9.10442 × 10−28
g
ℏ
ML2T−1
1.05443 × 10−27 g cm2 s−1
e2
ML3T−2
2.30655 × 10−19 g cm3 s−2
a0
L
?
cm
Can we construct something (e.g., Bohr
orbit aB) with the dimensions of length
from m, e2, and ℏ? To do this, we introduce
three unknown exponents a, b, and c and
a
𝜃
d
c
f
e
𝜃
b
Figure 5.1
The area of the large right triangle is the sum of
the areas of the two similar smaller right triangles: Δ(a, b, c) =
Δ(d, f, a) + Δ(f, e, b), so that c2f(𝜃) = a2f(𝜃) + b2f(𝜃). Since
f(𝜃) ≠0 for a nondegenerate right triangle, a2 + b2 = c2.

162
5 Group Theory
write
aB ≃ma (e2)b ℏc
= (M)a (ML3T−2)b (ML2T−1)c
= (M)a+b+c L0a+3b+2c T0a−2b−c
(5.2)
and set this result equal to the dimensions
of whatever we would like to compute; in
this case, the Bohr orbit aB (characteristic
atomic length), with [aB] = L. This results
in a matrix equation
⎡
⎢
⎢⎣
1
1
1
0
3
2
0
−2
−1
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
a
b
c
⎤
⎥
⎥⎦
=
⎡
⎢
⎢⎣
0
1
0
⎤
⎥
⎥⎦
.
(5.3)
We can invert this matrix to ﬁnd
⎡
⎢
⎢⎣
1
1
1
0
3
2
0
−2
−1
⎤
⎥
⎥⎦
−1
=
⎡
⎢
⎢⎣
1
−1
−1
0
−1
−2
0
2
3
⎤
⎥
⎥⎦
.
(5.4)
This allows us to determine the values of
the exponents that provide the appropriate
combinations of important physical param-
eters to construct the characteristic atomic
length:
⎡
⎢
⎢⎣
a
b
c
⎤
⎥
⎥⎦
=
⎡
⎢
⎢⎣
1
−1
−1
0
−1
−2
0
2
3
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
0
1
0
⎤
⎥
⎥⎦
=
⎡
⎢
⎢⎣
−1
−1
2
⎤
⎥
⎥⎦
.
(5.5)
This result tells us that
a0 ∼m−1(e2)−1(ℏ)2 = ℏ2
me2 ∼10−8 cm.
(5.6)
To construct a characteristic atomic
time, we can replace the vector col[0, 1, 0]
in (5.5) by the vector col[0, 0, 1], giving us
the result 𝜏0 ∼ℏ3∕m(e2)2. Finally, to get
a characteristic energy, we can form the
combination
∼ML2T−2 = m(ℏ2∕me2)2
(ℏ3∕me4)−2 = me4∕ℏ2. Another, and more
systematic, way to get this result is to sub-
stitute the vector col[1, 2, −2]t for [0, 1, 0]t
in (5.5).
Note that our estimate would be some-
what diﬀerent if we had used h instead of
ℏ= h∕2𝜋in these arguments. We point out
that this method is very useful for esti-
mating the order of magnitude of physical
parameters and in practised hands usually
gets the prefactor within a factor of 10. The
most critical feature of dimensional anal-
ysis is to identify the parameters that are
most important in governing the science of
the problem, and then to construct a result
depending on only those parameters.
5.2.3
Scaling
Positronium is a bound state of an elec-
tron e with a positron e, its antiparticle with
mass me and charge +e. How big is positro-
nium?
To address this question, we could
work very hard and solve the Schrödinger
equation for the positronium. This is
identical to the Schrödinger equation for
the hydrogen atom, except for replac-
ing the hydrogen atom reduced mass
meMp∕(me + Mp) ≃me by the positron-
ium reduced mass meme∕(me + me) = 1
2me.
Or we could be lazy and observe that the
hydrogen atom radius is inversely pro-
portional to the reduced electron–proton
mass, so the positronium radius should
be inversely proportional to the reduced
electron–positron mass me∕2. Since the
reduced electron–proton mass is eﬀec-
tively the electron mass, the positronium
atom is approximately twice as large as the
hydrogen atom.
In a semiconductor, it is possible to excite
an electron (charge −e) from an almost
ﬁlled (valence) band into an almost empty
(conduction) band. This leaves a “hole”
of charge +e behind in the valence band.
The positively charged hole in the valence
band interacts with the excited electron in

5.2 Precursors to Group Theory
163
the conduction band through a reduced
Coulomb interaction: V(r) = −e2∕𝜖r. The
strength of the interaction is reduced by
screening eﬀects that are swept into a
phenomenological dielectric constant 𝜖.
In addition, the eﬀective masses m∗
e of
the excited electron and the left-behind
hole m∗
h are modiﬁed from the free-space
electron mass values by many-particle
eﬀects.
How big is an exciton in gallium arsenide
(GaAs)? For this semiconductor, the phe-
nomenological parameters are 𝜖= 12.5,
m∗
e = 0.07me, m∗
h = 0.4me.
We
extend
the
scaling
argument
above by computing the reduced mass
of
the
electron–hole
pair:
𝜇e−h =
(0.07me)(0.4me)∕(0.07 + 0.4)me = 0.06me
and replacing e2 in the expression (5.4)
for the Bohr radius a0 by e2∕𝜖. The eﬀect
is to multiply a0 by 12.5∕0.06 = 208. The
ground-state radius of the exciton formed
in GaAs is about 10−6 cm. The ground-state
binding energy is lower than the hydrogen
atom binding energy of 13.6 eV by a factor
of 0.06∕12.52 = 3.8 × 10−4 so it is 5.2 meV.
Scaling arguments such as these are
closely related to renormalization group
arguments as presented in Chapter 12.
5.2.4
Dynamical Similarity
Jupiter is about ﬁve times further (5.2 AU)
from our Sun than the Earth. How many
earth years does it take for Jupiter to orbit
the Sun?
Landau and Lifshitz [2] provide an
elegant solution to this simple question
using similarity (scaling) arguments. The
equation of motion for the Earth around
the Sun is
mE
d2xE
dt2
E
= −GmEMS
̂xE
|xE|2 ,
(5.7)
where xE is a vector from the sun to
the earth and ̂xE the unit vector in this
direction. If Jupiter is in a geometrically
similar orbit, then xJ = 𝛼xE, with 𝛼= 5.2.
Similarly, time will evolve along the Jupiter
trajectory in a scaled version of its evolu-
tion along the Earth’s trajectory: tJ = 𝛽tE.
Substituting these scaled expressions into
the equation of motion for Jupiter, and
canceling out mJ from both sides, we
ﬁnd
𝛼
𝛽2
d2xE
dt2
E
= −1
𝛼2 GMS
̂xE
|xE|2 .
(5.8)
This scaled equation for Jupiter’s orbit
can only be equated to the equation
for the Earth’s trajectory (the orbits are
similar)
provided
𝛼3∕𝛽2 = 1.
That
is,
𝛽= 𝛼3∕2, so that the time-scaling factor is
5.23∕2 = 12.5.
We have derived Kepler’s third law with-
out even solving the equations of motion!
Landau and Lifshitz point out that you can
do even better than that. You don’t even
need to know the equations of motion to
construct scaling relations when motion
is described by a potential V(x) that is
homogeneous of degree k. This means that
V(𝛼x) = 𝛼kV(x). When the equations of
motion are derivable from a variational
principle 𝛿I = 0, where
I = ∫
(
m
(
dx
dt
)2
−V(x)
)
dt,
(5.9)
then the scaling relations x →x′ = 𝛼x,
t →t′ = 𝛽t lead to a modiﬁed action
I′ = 𝛼2
𝛽∫
(
m
(
dx
dt
)2
−𝛼k−2𝛽2V(x)
)
dt.
(5.10)
The Action I′ is proportional to the original
Action I, and therefore leads to the same
equations of motion, only when 𝛼k−2𝛽2 = 1;

164
5 Group Theory
that is, the time elapsed, T, is proportional
to the distance traveled, D, according to
T ≃D(1−k∕2). Four cases are of interest.
k = −1
(Coulomb/gravitational
poten-
tial) The period of a planetary
orbit scales as the 3∕2 power
of the distance from the Sun
(Kepler’s third law).
k = 0
(No forces) The distance trav-
eled is proportional to the time
elapsed
(essentially
Newton’s
ﬁrst law). To recover Newton’s
ﬁrst law completely, it is only
necessary to carry out the vari-
ation in (5.10), which leads to
d∕dt(dx∕dt) = 0.
k = +1
(Free fall in a homogeneous
gravitational ﬁeld) The potential
V(z) = mgz describes free fall
in a homogeneous gravitational
ﬁeld. Galileo is reputed to have
dropped rocks oﬀthe Leaning
Tower of Pisa to determine that
the distance fallen was propor-
tional to the square of the time
elapsed. The story is apocryphal:
in fact, he rolled stones down an
inclined plane to arrive at the
result Δz ≃Δt2.
k = +2
(Harmonic oscillator potential)
The period is independent of
displacement:
𝛽= 1
indepen-
dent of 𝛼. Hooke’s law, F = −kx,
V(x) = 1∕2kx2 leads to oscilla-
tory motion whose frequency is
independent of the amplitude
of motion. This was particularly
useful for constructing robust
clocks.
These four historical results in the devel-
opment of early science are summarized in
Table 5.1 and Figure 5.2.
Table 5.1
Four important results in the
historical development of science are
consequences of scaling arguments.
k
Scaling
Law
−1
T2 ≃D3
Kepler #3
0
D ≃T
Newton #1
+1
Δz ≃Δt2
Galileo: rolling stones
+2
T ≃D0
Hooke
5.3
Groups: Deﬁnitions
In this section, we ﬁnally get to the point
of deﬁning what a group is by stating
the group axioms (see [3–8]). These are
illustrated in the following sections with
a number of examples: ﬁnite groups,
including
the
two-element
group,
the
group of transformations that leaves the
equilateral triangle invariant, the permu-
tation group, point groups, and discrete
groups with a countable inﬁnite number
of group elements, such as space groups.
Then we introduce groups of transfor-
mations in space as matrix groups. Lie
groups are introduced and examples of
matrix Lie groups are presented.
Lie
groups are linearized to form their Lie
algebras, and groups are recovered from
their algebras by reversing the linearization
procedure using the exponential map-
ping. Many of the important properties
of Lie algebras are introduced, including
isomorphisms
among
diﬀerent
repre-
sentations of a Lie algebra. A powerful
disentangling theorem is presented and
illustrated in a very simple case that plays
a prominent rôle in the ﬁeld of quan-
tum optics. We will use this result in
Section 5.14.

5.3 Groups: Deﬁnitions
165
−1
0
1/2
1
3/2
p: Tp
Kepler #3
Newton #1
Galileo
Hooke
k: V(l𝛼xl) = 𝛼kV(lr1)
−0
+1
+2
p = 1 − (k/2)
Figure 5.2
Four substantial advances in the development
of early physics are summarized. Each is a consequence of
using a homogeneous potential with a diﬀerent degree k in
a variational description of the dynamics. Scaling relates the
size scale of the trajectory 𝛼to the time scale 𝛽= 𝛼p,
p =
1 −1
2 k of the motion.
5.3.1
Group Axioms
A group G consists of
• a set of group elements:
g0, g1, g2, g3, … ∈G
• a group operation, ∘, called group
multiplication
that satisfy the following four axioms:
Closure: gi ∈G, gj ∈G ⇒gi ∘gj ∈G
Associativity: (gi ∘gj)∘gk = gi ∘(gj∘gk)
Identity: g0 ∘gi = gi = gi ∘g0
Unique Inverse: gk ∘gl = g0 = gl ∘gk.
Group multiplication ∘has two inputs
and one output. The two inputs must be
members of the set. The ﬁrst axiom (Clo-
sure) requires that the output must also be
a member of the set.
The composition rule ∘does not allow us
to multiply three input arguments. Rather,
two can be combined to one, and that
output can be combined with the third. This
can be done in two diﬀerent ways that pre-
serves the order (i, j, k). The second axiom
(Associativity) requires that these two dif-
ferent ways give the same ﬁnal output.
The third axiom (Identity) requires that
a special group element exists. This, com-
bined with any other group element, gives
back exactly that group element.
The
fourth
axiom
(Unique
Inverse)
guarantees that for each group element
gl,
there
is
another
uniquely
deﬁned
group
element
gk,
with
the
property
that
the
product
of
the
two
is
the
unique
identity
element
gk ∘gl = g0.
It
is
a
simple
matter
to
prove
that
gl ∘gk = g0.
Remark 5.1 – Indexes: The notation (sub-
scripts i, j, k, …) may suggest that the
indices are integers. This is not gen-
erally true: for continuous groups the
indices are points in some subspace of

166
5 Group Theory
a Euclidean space or a more compli-
cated manifold.
Remark 5.2 – Commutativity: In general,
the output of the group multiplication
depends on the order of the inputs:
gi ∘gj ≠gj ∘gi. If the result is indepen-
dent of the order the group is said to
be commutative.
It is not entirely obvious that the Unique
Inverse axiom is needed. It is included
among
the
axioms
because
many
of
our uses involve relating measurements
made by two observers. For example, if
Allyson on the Earth can predict some-
thing about the length of a year on Jupiter,
then Bob on Jupiter should just as well
be able to predict the length of Allyson’s
year on Earth. Basically, this axiom is an
implementation of Galileo’s principle of
relativity.
5.3.2
Isomorphisms and Homomorphisms
It is often possible to compare two diﬀerent
groups. When it is possible, it is very useful.
Suppose we have two groups G with group
elements g0, g1, g2, … and group compo-
sition law gi ∘gj = gk and H with group ele-
ments h0, h1, h2, … and group composition
law hi ⋄hj = kk. A mapping f from G to H
is a homomorphism if it preserves the group
operation:
f (gi ∘gj) = f (gi) ⋄f (gj).
(5.11)
In this expression f (g∗) ∈H, so the two
group elements f (gi) and f (gj) can only be
combined using the combinatorial opera-
tion ⋄. If (5.11) is true for all pairs of group
elements in G, the mapping f is a homo-
morphism.
If G has four elements I, C4, C2
4, C3
4
and H has two Id, C2, the mapping
f (I) = f (C2
4) = Id,
f (C4) = f (C3
4) = C2,
the
mapping
f
is
a
homomorphism.
If the mapping f
is a homomorphism
and is also 1 : 1, it is called an iso-
morphism.
Under
these
conditions,
the inverse mapping also is an isomor-
phism:
f −1(hp ⋄hq) = f −1(hp) ∘f −1(hq).
(5.12)
As
an
example,
an
isomor-
phism
exists
between
the
four
group
elements
I, C4, C2
4, C3
4
and
the
2 × 2
matrices
with
f (C4) =
[
0
−1
1
0
]
.
5.4
Examples of Discrete Groups
5.4.1
Finite Groups
5.4.1.1
The Two-Element Group Z2
The simplest nontrivial group has one addi-
tional element beyond the identity e: G =
{e, g} with g ∘g = e. This group can act in
our three-dimensional space R3 in several
diﬀerent ways:
Reﬂection: (x, y, z)
g=𝜎Z
→(+x, +y, −z)
Rotation: (x, y, z)
g=RZ(𝜋)
→
(−x, −y, +z)
Inversion: (x, y, z)
g=
→(−x, −y, −z)
These three diﬀerent actions of the order-
two group on R3 describe: reﬂections in the
x-y plane, 𝜎Z; rotations around the Z-axis
through 𝜋radians, RZ(𝜋); and inversion in
the origin, the parity element, . They can
be distinguished by their matrix represen-
tations, which are

5.4 Examples of Discrete Groups
167
𝜎Z
RZ(𝜋)
⎡
⎢
⎢⎣
+1
0
0
0
+1
0
0
0
−1
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
−1
0
0
0
−1
0
0
0
+1
⎤
⎥
⎥⎦

⎡
⎢
⎢⎣
−1
0
0
0
−1
0
0
0
−1
⎤
⎥
⎥⎦
.
(5.13)
5.4.1.2
Group of Equilateral Triangle C3v
The six elements that map the equilateral
triangle to itself constitute the group C3v (cf.
Figure 5.3). There are three distinct types of
elements:
Identity. The element e does nothing: it
maps each vertex into itself.
Rotations. Two rotations C±
3 about the
center of the triangle through ±2𝜋∕3
radians.
Reﬂections. There are three reﬂections 𝜎i,
each in a straight line through the cen-
ter of the triangle and the vertex i (i =
1, 2, 3, cf. Figure 5.3).
These elements can be deﬁned by their
action on the vertices of the triangle. For
example
C+
3
( 1
2
3
2
3
1
) ⎡
⎢
⎢⎣
0
1
0
0
0
1
1
0
0
⎤
⎥
⎥⎦
.
(5.14)
The ﬁrst description (( )) says that the rota-
tion C+
3 maps vertex 1 to vertex 2, 2 →3 and
3 →1. The second description ([ ]) can be
understood as follows:
⎡
⎢
⎢⎣
2
3
1
⎤
⎥
⎥⎦
=
⎡
⎢
⎢⎣
0
1
0
0
0
1
1
0
0
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
1
2
3
⎤
⎥
⎥⎦
.
(5.15)
The group multiplication law can be repre-
sented through a 6 × 6 matrix (Cayley mul-
tiplication table) that describes the output
of gi ∘gj, with gi listed by column and gj by
row:
gj
e
C+
3
C−
3
𝜎1
𝜎2
𝜎3
gi
e
e
C+
3
C−
3
𝜎1
𝜎2
𝜎3
C+
3
C+
3
C−
3
e
𝜎2
𝜎3
𝜎1
C−
3
C−
3
e
C+
3
𝜎3
𝜎1
𝜎2
𝜎1
𝜎1
𝜎3
𝜎2
e
C−
3
C+
3
𝜎2
𝜎2
𝜎1
𝜎3
C+
3
e
C−
3
𝜎3
𝜎3
𝜎2
𝜎1
C−
3
C+
3
e.
(5.16)
22
11
𝜎3
𝜎3
𝜎1
𝜎1
𝜎2
𝜎2
33
C+
3
C+
3
C−
3
C−
3
Figure 5.3
The group of the equilateral triangles
consists of (a) the identity group element e; (b) two
rotations C±
3 by ±2𝜋∕3 about the centroid of the
triangle; and (c) three reﬂections 𝜎i in straight lines
between the centroid and each of the vertices i.

168
5 Group Theory
This table makes clear that the group is not
commutative: C−
3 = 𝜎1 ∘𝜎2 ≠𝜎2 ∘𝜎1 = C+
3 .
The partition of the six elements in this
group into three subsets of geometrically
equivalent transformations is typical of
any group. These subsets are called classes.
Classes are deﬁned by the condition
Class∶{h1, h2, …} g ∘hi ∘g−1 =hj all g ∈G.
(5.17)
All elements in the same class have essen-
tially the same properties. They are equiv-
alent under a group transformation. The
three classes for the ﬁnite group C3v are
{e} , {C+
3 , C−
3
} , {𝜎1, 𝜎2, 𝜎3
}.
It is clear from the group multiplica-
tion table (5.16) that C3v has a number
of (proper) subgroups: three subgroups of
order two
{
e, 𝜎1
} , {e, 𝜎2
} , {e, 𝜎3
} and one
of order three {e, C+
3 , C−
3
}. For technical
reasons, the single element {e} and the
entire group C3v are also considered to be
subgroups of C3v (they are not proper sub-
groups). Whenever a group G has a sub-
group H, it is always possible to write each
group element in G as the product of an
element in H with “something else”: gi =
hjCk. For example, if H is the subgroup
of order three we can choose the two ele-
ments C1, C2 (2 = 6∕3) as
{
e, 𝜎1
}. Then
from (5.16)
{e, C+
3 , C−
3
} ∘e = {e, C+
3 , C−
3
}
{e, C+
3 , C−
3
} ∘𝜎1 = {𝜎1 𝜎2, 𝜎3
} .
(5.18)
Since in some sense G is a product of group
elements in the subgroup H with group ele-
ments in C (G = H ∘C), we can formally
write C as the “quotient” of G by the sub-
group H: C = H∖G. If we composed in
the reversed order: G = CH, then we could
write C = G∕H.
The set C is called a coset. It is not unique,
but for ﬁnite groups, its order (number of
elements in the set) is unique: the quotient
of the order G by the order of H. A coset
may or may not be a group, depending
whether the subgroup H is invariant in G
(gHg−1 ⊂H for all g ∈G) or not.
Remarks.
When G and H are Lie groups of dimen-
sions dG and dH, G∕H is a manifold of
dimension dG∕dH, and under a broad set of
conditions this manifold has a geometric
structure imparted by a Riemannian metric
derived from the geometric properties of
the two groups G and H [3, 4].
5.4.1.3
Cyclic Groups Cn
The cyclic group consists of all rotations
of the circle into itself through the angle
2𝜋∕n radians, and integer multiples of this
angle. There are n such elements. The rota-
tion through 2𝜋k∕n radians is obtained by
applying the “smallest” rotation (also called
Cn or C1
n) k times. This smallest rotation is
called a generator of the group. The group is
commutative. There are therefore as many
classes as group elements. The group ele-
ments can be put in 1 ∶1 correspondence
with the complex numbers and also with
real 2 × 2 matrices:
[ei2𝜋k∕n] 1×1
←Ck
n
2×2
→
[
cos 2𝜋k
n
sin 2𝜋k
n
−sin 2𝜋k
n
cos 2𝜋k
n .
]
(5.19)
with k = 0, 1, 2, … , n −1 or k = 1, 2, … , n.
Every element in the group can be obtained
by multiplying C1
n by itself. In the same way,
the 1 × 1 complex matrix with k = 1 is the
generator for the 1 × 1 matrix representa-
tion of this group and the 2 × 2 real matrix
with k = 1 is the generator for the 2 × 2
matrix representation of the group.
5.4.1.4
Permutation Groups Sn
Permutation groups act to interchange
things. For example, if we have n numbers
1, 2, 3, … , n,
each
permutation
group

5.4 Examples of Discrete Groups
169
element will act to scramble the order of
the integers diﬀerently. Two useful ways
to describe elements in the permutation
group are shown in (5.14) for the per-
mutation group on three vertices of an
equilateral triangle. In the ﬁrst case, the
extension of this notation for individual
group elements consists of a matrix with
two rows, the top showing the ordering
before the element is applied, the bottom
showing the ordering after the group ele-
ment has been applied. In the second case
shown in (5.14) the extension consists of
n × n matrices with exactly one +1 in each
row and each column. The order of Sn is n!.
Permutation groups are noncommutative
for n > 2. S3 = C3v.
The permutation group plays a fun-
damental role in both mathematics and
physics. In mathematics, it is used to label
the irreducible tensor representations of
all Lie groups of interest. In physics, it is
required to distinguish among diﬀerent
states that many identical particles (either
bosons or fermions) can assume.
5.4.1.5
Generators and Relations
If G is a discrete group, with either a ﬁnite
or a countable number of group elements, it
is useful to introduce a small set of genera-
tors
{
𝜎1, 𝜎2, … , 𝜎k
} to describe the group.
Every element in the group can be rep-
resented as a product of these generators
and/or their inverses in some order.
For example, if there is only one gen-
erator
{𝜎}
and
every
group
element
can be written in the form gn = 𝜎n,
n = … , −2, −1, 0, 1, 2, … then G has a
countable number of group elements. It
is called a free group with one generator.
If there are two generators {𝜎1, 𝜎2
}, the
two generators commute 𝜎1𝜎2 = 𝜎2𝜎1, and
every group element can be expressed
in the form gm,n = 𝜎m
1 𝜎n
2 (m, n integers),
the group is the free group with two
commuting generators. Free groups with
k > 2 generators are deﬁned similarly.
Free groups with 1, 2, 3, … generators are
isomorphic to groups that act on periodic
lattices in 1, 2, 3, … dimensions.
Often the generators satisfy relations.
For example, a single generator 𝜎may
satisfy the relation 𝜎p = I. Then there
are exactly p distinct group elements
I = 𝜎0, 𝜎1, 𝜎2, … , 𝜎p−1. The group with one
generator and one relation is the cyclic
group
Cp.
Generators
{
𝜎1, 𝜎2, … , 𝜎k
}
and
relations
fl({𝜎1, 𝜎2, … , 𝜎k
}) = I,
l = 1, 2, …
have
been
used
to
deﬁne
many diﬀerent groups. In fact, every
discrete group is either deﬁned by a
set of generators and relations, or else
a subgroup of such a group. The sym-
metric group Sn
is deﬁned by n −1
generators 𝜎i, i = 1, 2, … , n −1 and the
relations
𝜎2
i = I, 𝜎i𝜎j = 𝜎j𝜎i if j ≠i ± 1,
and
𝜎i𝜎i+1𝜎i = 𝜎i+1𝜎i𝜎i+1.
The
tetra-
hedral (T), octahedral (O), and icosa-
hedral
(I)
point
groups
are
deﬁned
by two generators and three relations:
𝜎2
1 = I, 𝜎3
2 = I, (𝜎1𝜎2)p = I with p = 3, 4, 5,
respectively. The quaternion group Q8 can
be deﬁned with two generators and two
relations 𝜎1𝜎2𝜎1 = 𝜎2, 𝜎2𝜎1𝜎2 = 𝜎1 or in
terms of three generators and four relations
𝜎4
1 = 𝜎4
2 = 𝜎4
3 = I, 𝜎1𝜎2𝜎3 = I. In the latter
case, the three generators can be chosen as
2 × 2 matrices that are the three Pauli spin
matrices, multiplied by i =
√
−1.
The study of discrete groups deﬁned by
generators and relations has a long and very
rich history [9].
5.4.2
Inﬁnite Discrete Groups
5.4.2.1
Translation Groups: One
Dimension
Imagine a series of points at locations na
along the straight line, where a is a physical

170
5 Group Theory
parameter with dimensions of length ([a] =
L) and n is an integer. The group that leaves
this set invariant consists of rigid displace-
ments through integer multiples of the fun-
damental length. The element Tka displaces
the point at na to position (n + k)a. This
group has a single generator Ta, and Tka =
Ta ∘Ta∘· · · ∘Ta = Tk
a. It is convenient to
represent these group elements by 2 × 2
matrices
Tka →
[ 1
ka
0
1
]
.
(5.20)
In this representation, group composition
is equivalent to matrix multiplication. The
group is commutative. The generator for
the group and this matrix representation is
obtained by setting k = 1. There is also an
entire set of 1 × 1 complex matrix represen-
tations indexed by a real parameter p with
generator Ta →
[
eipa]. The representations
with p′ = p + 2𝜋∕a are equivalent, so all the
inequivalent complex representations can
be parameterized by real values of p in the
range 0 ≤p < 2𝜋∕a or, more symmetrically
−𝜋∕a ≤p ≤𝜋∕a, with the end points iden-
tiﬁed. The real parameter p is in the dual
space to the lattice, called the ﬁrst Brillouin
zone.
5.4.2.2
Translation Groups: Two
Dimensions
Now imagine a series of lattice points in a
plane at positions x = i1f1 + i2f2. Here i1, i2
are integers and the vectors f1, f2 are not
colinear but otherwise arbitrary. Then the
set of rigid displacements (j1, j2) move lat-
tice points x to new locations as per
Tj1f1+j2f2
(i1f1 + i2f2
)
= (i1 + j1)f1 + (i2 + j2)f2.
(5.21)
Generalizing (5.20), there is a simple 1 ∶1
(or faithful) matrix representation for this
group of rigid translations:
Tj1f1+j2f2 →
⎡
⎢
⎢⎣
1
0
j1|f1|
0
1
j2|f2|
0
0
1
⎤
⎥
⎥⎦
.
(5.22)
Extension to groups of rigid displacements
of lattices in higher dimensions is straight-
forward.
5.4.2.3
Space Groups
When |f1| = |f2| and the two vectors are
orthogonal, rotations through k𝜋∕2 (k =
1, 2, 3) radians about any lattice point map
the lattice into itself. So also do reﬂections
in lines perpendicular to |f1| and |f2| as well
as lines perpendicular to ±|f1| ± |f2|. This
set of group elements contains displace-
ments, rotations, and reﬂections. It is an
example of a two-dimensional space group.
There are many other space groups in two
dimensions and very many more in three
dimensions. These groups were ﬁrst used
to enumerate the types of regular lattices
that nature allows in two and three dimen-
sions [7]. After the development of quan-
tum mechanics, they were used in another
way (depending on the theory of represen-
tations): to give names to wavefunctions
that describe electrons (and also phonons)
in these crystal lattices.
5.5
Examples of Matrix Groups
5.5.1
Translation Groups
The group of rigid translations of points in
R3 through distances a1 in the x-direction,
a2 in the y-direction, and a3 in the z-
direction can be described by simple block
4 × 4 (4 = 3 + 1) matrices:

5.5 Examples of Matrix Groups
171
Ta1,a2,a3 →
⎡
⎢
⎢
⎢
⎢⎣
1
0
0
a1
0
1
0
a2
0
0
1
a3
0
0
0
1
⎤
⎥
⎥
⎥
⎥⎦
.
(5.23)
If the a belong to a lattice, the group is
discrete. If they are continuous, the group
is continuous and has dimension three.
5.5.2
Heisenberg Group H3
The Heisenberg group H3 plays a fun-
damental role in quantum mechanics.
As it appears in the quantum theory it
is
described
by
“inﬁnite-dimensional”
matrices. However, the group itself is three
dimensional. In fact, it has a simple faithful
description in terms of 3 × 3 matrices
depending on three parameters:
h(a, b, c) =
⎡
⎢
⎢⎣
1
a
c
0
1
b
0
0
1
⎤
⎥
⎥⎦
.
(5.24)
The
matrix
representation
is
faithful
because any matrix of the form (5.24)
uniquely deﬁnes the abstract group element
h(a, b, c). The group is not commutative.
The group multiplication law can be easily
seen via matrix multiplication:
h1h2 = h3 = h(a3, b3, c3)
=
⎡
⎢
⎢⎣
1
a1 + a2
c1 + c2 + a1b2
0
1
b1 + b2
0
0
1
⎤
⎥
⎥⎦
.
(5.25)
The group composition law given in (5.25)
deﬁnes the Heisenberg group. The result
c3 = c1 + c2 + a1b2
leads to remarkable
noncommutativity
properties
among
canonically
conjugate
variables
in
the
quantum theory: [p, x] = ℏ∕i.
5.5.3
Rotation Group SO(3)
The set of rigid rotations of R3 forms a
group. It is conveniently represented by
a faithful 3 × 3 matrix. The 3 × 3 matrix
describing rotations about an axis of unit
length ̂n through an angle 𝜃, 0 ≤𝜃≤𝜋
is
(̂n, 𝜃) →I3 cos 𝜃+ ̂n ⋅L sin 𝜃+
⎡
⎢
⎢⎣
̂n1
̂n2
̂n3
⎤
⎥
⎥⎦
× [
̂n1
̂n2
̂n3
] (1 −cos 𝜃).
(5.26)
Here L is a set of three 3 × 3 angular
momentum matrices
Lx =
⎡
⎢
⎢⎣
0
0
0
0
0
1
0
−1
0
⎤
⎥
⎥⎦
Ly =
⎡
⎢
⎢⎣
0
0
−1
0
0
0
1
0
0
⎤
⎥
⎥⎦
Lz =
⎡
⎢
⎢⎣
0
1
0
−1
0
0
0
0
0
⎤
⎥
⎥⎦
.
(5.27)
The matrix multiplying (1 −cos 𝜃) in (5.26)
is a 3 × 3 matrix: it is the product of a
3 × 1 column matrix with a 1 × 3 row
matrix. We will show later how this mar-
velous expression has been derived (cf.
(5.51)).
There is a 1:1 correspondence between
points in the interior of a ball of radius
𝜋and rotations through an angle in the
range 0 ≤𝜃< 𝜋. Two points on the sur-
face (̂n, 𝜋) and (−̂n, 𝜋) describe the same
rotation. The parameter space describ-
ing this group is not a simply connected
submanifold of R3: it is a doubly con-
nected manifold. The relation between
continuous groups and their underlying
parameter space involves some fascinating
topology.

172
5 Group Theory
5.5.4
Lorentz Group SO(3, 1)
The Lorentz group is the group of linear
transformations that leave invariant the
square of the distance between two nearby
points in space–time: (cdt, dx, dy, dz) and
(cdt′, dx′, dy′, dz′). The distance can be
written in matrix form:
(cd𝜏)2 = (cdt)2 −(dx2 + dy2 + dz2)
= [
cdt
dx
dy
dz
]
⎡
⎢
⎢
⎢
⎢⎣
+1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1
⎤
⎥
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢
⎢⎣
cdt
dx
dy
dz
⎤
⎥
⎥
⎥
⎥⎦
.
(5.28)
If the inﬁnitesimals in the primed coor-
dinate system are related to those in the
unprimed coordinate system by a linear
transformation – dx
′𝜇= M𝜇
𝜈dx𝜈– then the
matrices M must satisfy the constraint (t
means matrix transpose and the summa-
tion convention has been used: doubled
indices are summed over.)
MtI1,3M = I1,3,
(5.29)
where
I1,3
is
the
diagonal
matrix
diag(+1, −1, −1, −1).
The
matrices
M
belong to the orthogonal group O(1, 3).
This is a six-parameter group. Clearly
the rotations (three dimensions worth)
form a subgroup, represented by matrices
of the form
[ ±1
0
0
±R(̂n, 𝜃)
]
, where
R(̂n, 𝜃) is given in (5.26). This group has
four
disconnected
components,
each
connected to a 4 × 4 matrix of the form
[ 1
0
0
I3
]
,
[ 1
0
0
−I3
]
,
[ −1
0
0
I3
]
, or
[ −1
0
0
−I3
]
. We choose the component
connected to the identity I4. This is the
special Lorentz group SO(1, 3). A general
matrix in this group can be written in the
form
SO(1, 3) = B(𝛃)R(𝛉),
(5.30)
where the matrices B(𝛃) describe boost
transformations
and
R(𝛉) = R(̂n, 𝜃).
A
boost transformation maps a coordinate
system at rest to a coordinate moving with
velocity v = c𝛃and with axes parallel to
those in the stationary coordinate system.
Since every group element in SO(1, 3)
can be expressed as the product of a rota-
tion element with a boost, we can formally
write boost transformations as elements in
a coset: B(𝛃) = SO(1, 3)∕SO(3).
A general boost transformation can be
written in the form
B(𝛃) =
[
𝛾
𝛾𝛃
𝛾𝛃
I3 + (𝛾−1)
𝛽i𝛽j
𝛃⋅𝛃
]
.
(5.31)
For example, a boost in the x-direction with
v∕c = (𝛽, 0, 0) has the following eﬀect on
coordinates (𝛽= |𝛃|):
⎡
⎢
⎢
⎢
⎢⎣
ct
x
y
z
⎤
⎥
⎥
⎥
⎥⎦
′
=
⎡
⎢
⎢
⎢
⎢⎣
𝛾
𝛾𝛽
0
0
𝛾𝛽
𝛾
0
0
0
0
1
0
0
0
0
1
⎤
⎥
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢
⎢⎣
ct
x
y
z
⎤
⎥
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢
⎢⎣
𝛾(ct + 𝛽x)
𝛾(x + 𝛽ct)
y
z
⎤
⎥
⎥
⎥
⎥⎦
.
(5.32)

5.6 Lie Groups
173
Here 𝛾2 −(𝛽𝛾)2 = 1 so 𝛾= 1∕
√
1 −𝛽2. In
the nonrelativistic limit, x′ = 𝛾(x + 𝛽ct) →
x + vt, so 𝛽has an interpretation of 𝛃= v∕c.
The product of two boosts in the same
direction is obtained by matrix multiplica-
tion. This can be carried out on a 2 × 2 sub-
matrix of that given in (5.32):
B(𝛽1)B(𝛽2) =
[ 𝛾1
𝛽1𝛾1
𝛽1𝛾1
𝛾1
] [ 𝛾2
𝛽2𝛾2
𝛽2𝛾2
𝛾2
]
=
[
𝛾tot
𝛽tot𝛾tot
𝛽tot𝛾tot
𝛾tot
]
.
(5.33)
Simple matrix multiplication shows 𝛽tot =
(𝛽1 + 𝛽2)∕(1 + 𝛽1𝛽2), which is the relativis-
tic velocity addition formula for parallel
velocity transformations.
When the boosts are not parallel, their
product is a transformation in SO(1, 3) that
can be written as the product of a boost
with a rotation:
B(𝛃1)B(𝛃2) = B(𝛃tot)R(𝛉).
(5.34)
Multiplying two boost matrices of the
form given in (5.31) leads to a simple
expression for 𝛾tot and a more complicated
expression for 𝛃tot
𝛾tot = 𝛾1𝛾2(1 + 𝛃1 ⋅𝛃2)
𝛾tot𝛃tot =
[
𝛾1𝛾2 + (𝛾1 −1)𝛾2
𝛃1 ⋅𝛃2
𝛃1 ⋅𝛃1
]
𝛃1 + 𝛾2𝛃2.
(5.35)
This shows what is intuitively obvious:
the boost direction is in the plane of the
two boosts. Less obvious is the rotation
required by noncollinear boosts. It is
around an axis parallel to the cross product
of the two boosts. When the two boosts are
perpendicular, the result is
̂n sin(𝜃) = −𝛃1 × 𝛃2 ⋅
𝛾1𝛾2
1 + 𝛾1𝛾2
.
(5.36)
When one of the boosts is inﬁnitesimal,
we ﬁnd
B(𝛃)B(𝛿𝛃) = B(𝛃+ d𝛃)R(̂nd𝜃).
(5.37)
Multiplying out these matrices and com-
paring the two sides gives
d𝛃
=
𝛾−1𝛿𝛃+
(𝛾−1 −1
𝛾𝛽2
)
(𝛃⋅𝛿𝛃)𝛃
̂nd𝜃
=
(1 −𝛾−1
𝛽2
)
𝜹𝜷× 𝜷.
(5.38)
In the nonrelativistic limit, when 𝛃is also
small, 1 −𝛾−1∕𝛽2 →1∕2. This (in)famous
factor of 1/2 is known as the “Thomas fac-
tor” in atomic physics.
5.6
Lie Groups
The group elements g in a Lie group are
parameterized by points x in a manifold
n of dimension n: g = g(x), x ∈n.
The product of two group elements g(x)
and g(y) is parameterized by a point z
in the manifold: g(x)∘g(y) = g(z), where
z = z(x, y). This composition law can be
very complicated. It is necessarily nonlin-
ear (cf. (5.25) for H3) unless the group is
commutative. For example, the parameter
space for the group SO(3) consists of points
in R3: 𝛉= (̂n, 𝜃). Only a compact subspace
consisting of a sphere of radius 𝜋is needed
to parameterize this Lie group.
Almost all of the Lie groups of use to
physicists exist as matrix groups. For this
reason, it is possible for us to skip over the
fundamental details of whether the compo-
sition law must be analytic and the elegant
details of their deﬁnition and derivations. A
composition law can be determined as fol-
lows:

174
5 Group Theory
1. Construct a useful way to parameterize
each group element as a matrix
depending on a suitable number of
parameters: x →g(x).
2. Perform matrix multiplication
g(x)∘g(y) of the matrices representing
the two group elements.
3. Find the parameters z = z(x, y) of the
group element that corresponds to the
product of the two matrices given in
Step 2.
We list several types of matrix groups
below.
• GL(n;R), GL(n;C), GL(n;Q). These
general linear groups (general means
det ≠0) consist of n × n invertible
matrices, each of whose n2 matrix
elements are real numbers, complex
numbers, or quaternions. The group
composition law is matrix
multiplication. The numbers of real
parameters required to specify an
element in these groups are
n2, 2 × n2, 4 × n2, respectively.
• SL(n;R), SL(n;C). The special linear
groups (“S” for special means det = +1)
are subgroups of GL(n; R) and GL(n; C)
containing the subgroup of matrices
with determinant +1. The real
dimensions of these groups are
(n2 −1) × dim(F) where dim(F) = (1, 2)
for F = (R, C).
• O(n), U(n), Sp(n). Three important
classes of groups are deﬁned by placing
quadratic constraints on matrices. The
orthogonal group O(n) is the subgroup
of GL(n; R) containing only matrices M
that satisfy MtInM = In. Here we use
previously introduced notation: In is the
unit n × n matrix and t signiﬁes the
transpose of the matrix. This constraint
arises in a natural way when requiring
that linear transformations in a real
n-dimensional linear vector space
preserve a positive-deﬁnite inner
product. The unitary group U(n) is the
subgroup of GL(n; C) for which the
matrices M satisfy M†InM = In, where †
signiﬁes the adjoint, or complex
conjugate transpose matrix. The
symplectic group Sp(n) is deﬁned
similarly for the quaternions. In this
case, † signiﬁes quaternion conjugate
transpose. The real dimensions of these
groups are n(n −1)∕2, n2, and n(2n + 1),
respectively.
• SO(n), SU(n). For the group O(n), the
determinant of any group element is a
real number whose modulus is +1: that
is , ±1. Placing the special constraint on
the group of orthogonal transformations
reduces the “number” of elements in the
group by one half (in a measure
theoretic sense) but does not reduce the
dimension of the space required to
parameterize the elements in this group.
For the group U(n), the determinant of
any group element is a complex number
whose modulus is +1: that is, ei𝜙. Placing
the special constraint on U(n) reduces
the dimension by one: dim SU(n) = dim
U(n) −1 = n2 −1. The symplectic group
Sp(n) has determinant +1.
• O(p,q), U(p,q), Sp(p,q). These groups are
deﬁned by replacing In in the deﬁnitions
for O(n), U(n), Sp(n) by the matrix
Ip,q =
[ +Ip
0
0
−Iq
]
. These groups
preserve an indeﬁnite nonsingular
metric in linear vector spaces of
dimension (p + q). The groups
O(n), U(n), Sp(n) are compact (a useful
topological concept) and so are relatively
easy to deal with. This means eﬀectively
that only a ﬁnite volume of parameter
space is required to parameterize every
element in the group. The groups
O(p, q), U(p, q), Sp(p, q) are not compact

5.7 Lie Algebras
175
if both p and q are nonzero. Further,
O(p, q) ≃O(q, p) by a simple similarity
transformation, and similarly for the
others.
5.7
Lie Algebras
A Lie algebra is a linear vector space on
which an additional composition law [, ] is
deﬁned. If X, Y, Z are elements in a Lie alge-
bra , then linear combinations are in the
Lie algebra: 𝛼X + 𝛽Y ∈(this is the linear
vector space property), the commutator of
two elements [X, Y] is in , and, in addi-
tion, the new composition law satisﬁes the
following axioms:
[𝛼X + 𝛽Y, Z] = 𝛼[X, Z] + 𝛽[Y, Z] ,
[X, Y] + [Y, X] = 0,
[X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y]] = 0.
(5.39)
The ﬁrst of these conditions preserves
the linear vector space property of . The
second condition deﬁnes the commutator
bracket [, ] as an antisymmetric composi-
tion law: [X, Y] = −[Y, X], and the third
imposes an integrability constraint called
the Jacobi identity.
In principle, commutators are deﬁned by
the properties presented in (5.39), whether
or not composition of the operators X
and Y is deﬁned. If this composition is
deﬁned, then [X, Y] = XY −YX and the
commutator can be computed by apply-
ing the operators X and Y in diﬀerent
orders and subtracting the diﬀerence. For
example, if X = y𝜕z −z𝜕y (here 𝜕z = 𝜕∕𝜕z)
and if Y = z𝜕x −x𝜕z, then the operator
XY can be applied to a general func-
tion
whose
second
partial
derivatives
are
continuous
to
give
XY f (x, y, z) =
yfx + yzfxz −z2fxy −xyfzz + zxfyz. The value
of YXf (x, y, z) is computed similarly, and
the diﬀerence is (XY −YX)f = yfx −xfy =
(y𝜕x −x𝜕y)f . Since this holds for an arbi-
trary
function
f (x, y, z)
for
which
all
second partial derivatives exist and are
independent of the order taken, we ﬁnd
[X, Y] = (XY −YX) = (y𝜕x −x𝜕y).
5.7.1
Structure Constants
When the underlying linear vector space
for has dimension n, it is possible to
choose a set of n basis vectors (matrices,
operators) Xi. The commutation relations
are encapsulated by a set of structure con-
stants Ck
ij that are deﬁned by
[Xi, Xj
] = Ck
ijXk.
(5.40)
A Lie algebra is deﬁned by its structure
constants.
5.7.2
Constructing Lie Algebras by Linearization
The Lie algebra for a Lie group is con-
structed by linearizing the constraints that
deﬁne the Lie group in the neighborhood
of the identity I. Matrix Lie algebras are
obtained for n × n matrix Lie groups by lin-
earizing the matrix group in the neighbor-
hood of the unit matrix In. A Lie group and
its Lie algebra have the same dimension.
In the neighborhood of the identity the
groups, GL(n; R), GL(n; C), and GL(n, Q)
have the form
GL(n; F) →In + 𝛿M.
(5.41)
where 𝛿M is an n × n matrix, all of whose
matrix elements are small. Over the real,
complex, and quaternion ﬁelds the matrix
elements are small real or complex num-
bers or small quaternions. Quaternions q

176
5 Group Theory
can be expressed as 2 × 2 complex matrices
using the Pauli spin matrices 𝜎𝜇:
q →(c0, c1) = (r0, r1, r2, r3) =
3
∑
𝜇=0
r𝜇𝜎𝜇
=
[ r0 + ir3
r1 −ir2
r1 + ir2
r0 −ir3
]
. (5.42)
The
Lie
algebras
𝔤𝔩(n; F)
of
GL(n; F)
have
dimensions
dim(F) × n2,
with
dim(F) = 1, 2, 4 = 22 for F = R, C, Q.
For the special linear groups, the deter-
minant of a group element near the identity
is
det(In + 𝛿M) = 1 + tr𝛿M + h.o.t.
(5.43)
In order to ensure the unimodular condi-
tion, the Lie algebras of the special linear
groups consist of traceless matrices. The Lie
algebra 𝔰𝔩(n; R) of SL(n; R) consists of real
traceless n × n matrices. It has dimension
n2 −1. The Lie algebra 𝔰𝔩(n; C) of SL(n; C)
consists of traceless complex n × n matri-
ces. It has real dimension 2n2 −2.
Many Lie groups are deﬁned by a
metric-preserving condition: M†GM = G,
where G is some suitable metric matrix
(see the discussion of the Lorentz group
SO(3, 1) ≃SO(1, 3)
that
preserves
the
metric (+1, +1, +1, −1) in Section 5.5.4
and the groups O(p,q), U(p,q), Sp(p,q)
in Section 5.6). The linearization of this
condition is
M†GM = (In + 𝛿M)†G(In + 𝛿M) = G
so that from M†GM = G, it follows that,
neglecting small terms of order two
𝛿M†G + G𝛿M = 0.
(5.44)
Thus the Lie algebras 𝔰𝔬(n; R), 𝔰𝔲(n; C),
𝔰𝔭(n; Q), which correspond to the case G =
In, consist of real antisymmetric matrices
Mt = −M, complex traceless antihermitian
matrices M† = −M, and quaternion anti-
hermitian matrices M† = −M, respectively.
The Lie algebra 𝔰𝔬(3) of the rotation
group SO(3) = SO(3; R) consists of real
3 × 3 antisymmetric matrices. This group
and its algebra are three dimensional. The
Lie algebra (it is a linear vector space) is
spanned by three “basis vectors.” These are
3 × 3 antisymmetric matrices. A standard
choice for these basis vectors is given in
(5.27). Their commutation relations are
given by
[Li, Lj
] = −𝜖ijkLk.
(5.45)
The structure constants for 𝔰𝔬(3) are
Ck
ij = −𝜖ijk, 1 ≤i, j, k ≤3. Here 𝜖ijk is the
“sign symbol” (antisymmetric Levi–Civita
3-tensor), which is zero if any two symbols
are the same, +1 for a cyclic permuta-
tion of integers (123), and −1 for a cyclic
permutation of (321).
Two Lie algebras with the same set of
structure constants are isomorphic [3–6].
The Lie algebra of 2 × 2 matrices obtained
from 𝔰𝔲(2) is spanned by three operators
that can be chosen as (i∕2) times the Pauli
spin matrices (cf. (5.42)):
S1 = i
2
[ 0
1
1
0
]
S2 = i
2
[ 0
−i
+i
0
]
S3 = i
2
[ 1
0
0
−1
]
.
(5.46)
These three operators satisfy the commuta-
tion relations
[Si, Sj
] = −𝜖ijkSk.
(5.47)
As a result, the Lie algebra for the group
𝔰𝔬(3) of rotations in R3 is isomorphic to
the Lie algebra 𝔰𝔲(2) for the group of
unimodular metric-preserving rotations in
a complex two-dimensional space, SU(2).

5.7 Lie Algebras
177
Spin and orbital rotations are intimately
connected.
5.7.3
Constructing Lie Groups by
Exponentiation
The mapping of a Lie group, with a com-
plicated nonlinear composition, down to a
Lie algebra with a simple linear combinato-
rial structure plus a commutator, would not
be so useful if it were not possible to undo
this mapping. In eﬀect, the linearization is
“undone” by the exponential map. For an
operator X the exponential is deﬁned in the
usual way:
exp(X) = eX = I + X + X2
2! + X3
3! + · · ·
=
∞
∑
k=0
Xk
k! .
(5.48)
The radius of convergence of the exponen-
tial function is inﬁnite. This means that we
can map a Lie algebra back to its parent Lie
group in an algorithmic way.
We
illustrate
with
two
important
examples. For the ﬁrst, we construct a
simple
parameterization
of
the
group
SU(2) by exponentiating its Lie algebra.
The Lie algebra is given in (5.46). Deﬁne
M = i∕2̂n ⋅𝛔𝜃. Then M2 = −(𝜃∕2)2I2 is a
diagonal matrix. The exponential expan-
sion can be rearranged to contain even
powers in one sum and odd powers in
another:
eM = I2
(
1 −(𝜃∕2)2
2!
+ (𝜃∕2)4
4!
−· · ·
)
+
M
(
1 −(𝜃∕2)2
3!
+ (𝜃∕2)4
5!
−· · ·
)
.
(5.49)
The even terms sum to cos(𝜃∕2) and the
odd terms sum to sin(𝜃∕2)∕(𝜃∕2). The
result is
exp
( i
2 ̂n ⋅𝛔𝜃
)
= cos 𝜃
2I2 + în ⋅𝛔sin 𝜃
2.
(5.50)
A similar power series expansion involv-
ing the angular momentum matrices in
(5.27) leads to the parameterization of the
rotation group elements given in (5.26).
Speciﬁcally, exp (̂n ⋅L𝜃) =
I3 cos 𝜃+ ̂n ⋅L sin 𝜃
+ [
̂n1
̂n2
̂n3
] ⎡
⎢
⎢⎣
̂n1
̂n2
̂n3
⎤
⎥
⎥⎦
(1 −cos 𝜃).
(5.51)
The Lie groups SO(3) and SU(2) possess
isomorphic Lie algebras. The Lie algebra
is three dimensional. The basis vectors in
𝔰𝔬(3) can be chosen as the angular momen-
tum matrices given in (5.27) and the basis
vectors for 𝔰𝔲(2) as i∕2 times the Pauli
spin matrices, as in (5.46). A point in the
Lie algebra (e.g., R3) can be identiﬁed by a
unit vector ̂n and a radial distance from the
origin 𝜃. Under exponentiation, the point
(̂n, 𝜃) maps to the group element given in
(5.51) for SO(3) and in (5.50) for SU(2).
The simplest way to explore how the Lie
algebra parameterizes the two groups is to
look at how points along a straight line
through the origin of the Lie algebra map to
elements in the two groups. For simplicity,
we choose the z-axis. Then (̂z, 𝜃) maps to
[ ei𝜃∕2
0
0
e−i𝜃∕2
]
∈SU(2),
⎡
⎢
⎢⎣
cos 𝜃
sin 𝜃
0
−sin 𝜃
cos 𝜃
0
0
0
1
⎤
⎥
⎥⎦
∈SO(3).
(5.52)
As 𝜃increases from 0 to 2𝜋, the SU(2)
group element varies from +I2 to −I2, while
the SO(3) group element starts at I3 and
returns to +I3. The SU(2) group element

178
5 Group Theory
returns to the identity +I2 only after 𝜃
increases from 2𝜋to 4𝜋. The rotations by
𝜃and 𝜃+ 2𝜋give the same group element
in SO(3) but they describe group elements
in SU(2) that diﬀer by sign: (̂z, 2𝜋+ 𝜃) =
−I2 × (̂z, 𝜃). Two 2 × 2 matrices M and −M
in SU(2) map to the same group element in
SO(3). In words, SU(2) is a double cover of
SO(3).
For SU(2), all points inside a sphere of
radius 2𝜋in the Lie algebra map to diﬀer-
ent group elements, and all points on the
sphere surface map to one group element
−I2. The group SU(2) is simply connected.
Any path starting and ending at the same
point (for example, the identity) can be con-
tinuously contracted to the identity.
By contrast, for SO(3), all points inside
a sphere of radius 𝜋in the Lie algebra
map to diﬀerent group elements, and two
points (̂n, 𝜋) and −(̂n, 𝜋) at opposite ends
of a straight line through the origin map to
the same group element. The group SO(3)
is not simply connected. Any closed path
from the origin that cuts the surface 𝜃= 𝜋
once (or an odd number of times) cannot be
continuously deformed to the identity. The
group SO(3) is doubly connected.
This is the simplest example of a strong
theorem by Cartan. There is a 1 ∶1 relation
between Lie algebras and simply connected
Lie groups. Every Lie group with the same
(isomorphic) Lie algebra is either simply
connected or else the quotient (coset) of the
simply connected Lie group by a discrete
invariant subgroup.
For matrix Lie groups, discrete invari-
ant subgroups consist of scalar multiples
of the unit matrix. For the isomorphic
Lie algebras 𝔰𝔲(2) = 𝔰𝔬(3), the Lie group
SU(2) is simply connected. Its discrete
invariant subgroup consists of multiples
of the identity matrix: {I2, −I2
}. Cartan’s
theorem states SO(3) = SU(2)∕{I2, −I2
}.
This
makes
explicit
the
2 ↓1
nature
of
the
relation
between
SU(2)
and
SO(3).
The group SU(3) is simply connected.
It discrete invariant subgroup consists of
{I3, 𝜔I3, 𝜔2I3
}, with 𝜔3 = 1. The only other
Lie group with the Lie algebra 𝔰𝔲(3) is
the 3 ↓1 image SU(3)∕{I3, 𝜔I3, 𝜔2I3
}. This
group has a description in terms of real
eight-dimensional matrices (“the eightfold
way”).
5.7.4
Cartan Metric
The notation for the structure constants Ck
ij
for a Lie algebra gives the them appearance
of being components of a tensor. In fact,
they are: the tensor is ﬁrst-order contravari-
ant (in k) and second-order covariant, and
antisymmetric, in i, j. It is possible to form
a second-order symmetric covariant tensor
(Cartan–Killing metric) from the compo-
nents of the structure constant by double
cross contraction:
gij =
∑
rs
Cs
irCr
js = gji.
(5.53)
This real symmetric tensor “looks like”
a metric tensor. In fact, it has very pow-
erful properties. If g∗∗is nonsingular,
the Lie algebra, and its Lie group, is
“semisimple” or “simple” (these are tech-
nical terms meaning that the matrices
describing the Lie algebras are either
fully reducible or irreducible). If g∗∗is
negative deﬁnite, the group is compact.
It is quite remarkable that an algebraic
structure gives such powerful topological
information.
As an example, for SO(3) and SU(2) the
Cartan–Killing metric equation (5.53) is
gij =
∑
r,s
(−𝜖irs)(−𝜖jsr) = −2𝛿ij.
(5.54)

5.7 Lie Algebras
179
For the real forms SO(2, 1) of SO(3)
and SU(1, 1) of SU(2), the Cartan–Killing
metric tensor is
g (𝔰𝔬(2, 1)) = g (𝔰𝔲(1, 1))
= 2
⎡
⎢
⎢⎣
+1
0
0
0
+1
0
0
0
−1
⎤
⎥
⎥⎦
.
(5.55)
The structure of this metric tensor (two
positive diagonal elements or eigenval-
ues, and one negative) tells us about the
topology of the groups: they have two
noncompact directions and one compact
direction. The compact direction describes
the compact subgroups SO(2) and U(1),
respectively.
5.7.5
Operator Realizations of Lie Algebras
Each Lie algebra has three useful opera-
tor realizations. They are given in terms
of boson operators, fermion operators, and
diﬀerential operators.
Boson annihilation operators bi and cre-
ation operators b†
j for independent modes
i, j = 1, 2, …, their fermion counterparts
fi, f †
j , and the operators 𝜕i, xj satisfy the fol-
lowing commutation or anticommutation
relations
[
bi, b†
j
]
=
bib†
j −b†
j bi
=
𝛿ij,
{
fi, f †
j
}
=
fif †
j + f †
j fi
=
𝛿ij,
[𝜕i, xj
]
=
𝜕ixj −xj𝜕i
=
𝛿ij.
(5.56)
In spite of the fact that bosons and diﬀeren-
tial operators satisfy commutation relations
and fermion operators satisfy anticommu-
tation (see the + sign in (5.56)) relations,
bilinear combinations Zij = b†
i bj, f †
i fj, xi𝜕j of
these operators satisfy commutation rela-
tions:
[Zij, Zrs
] = Zis𝛿jr −Zrj𝛿si.
(5.57)
These commutation relations can be
used to associate operator algebras to
matrix Lie algebras. The procedure is sim-
ple. We illustrate this for boson operators.
Assume A, B, C = [A, B] are n × n matrices
in a matrix Lie algebra. Associate operator
to matrix A by means of
A →= b†
i Aijbj
(5.58)
and similarly for other matrices. Then
[, ] = [b†
i Aijbj, b†
rBrsbs
]
= b†
i [A, B]is bs = b†
i Cisbs = .
(5.59)
This result holds if the bilinear combina-
tions of boson creation and annihilation
operators are replaced by bilinear combi-
nations of fermion creation and annihila-
tion operators or products of multiplication
(by xi) and diﬀerentiation (by 𝜕j) operators.
One consequence of this matrix Lie alge-
bra to operator algebra isomorphism is that
any Hamiltonian that can be expressed in
terms of bilinear products of creation and
annihilation operators for either bosons or
fermions can be studied in a simpler matrix
form.
The operator algebra constructed from
the spin operators in (5.46) has been used
by Schwinger for an elegant construction
of all the irreducible representations of
the Lie group SU(2) (cf. Section 5.10.5 and
Figure 5.4).
We use the matrix-to-operator mapping
now to construct a diﬀerential operator
realization of the Heisenberg group, given
in (5.24). Linearizing about the identify
gives a three-dimensional Lie algebra of the
form
⎡
⎢
⎢⎣
0
l
d
0
0
r
0
0
0
⎤
⎥
⎥⎦
= lL + rR + dD.
(5.60)

180
5 Group Theory
0
0
n2
n1
J− = a†
2a1
J+ = a†
1a2
J−
J+
|n1n2>
1
1
2
2
3
3
4
4
5
5
6
Figure 5.4
Angular momentum
operators J have isomorphic
commutation relations with
speciﬁc bilinear combinations b†
i bj
of boson creation and annihilation
operators for two modes. The
occupation number for the ﬁrst
mode is plotted along the x-axis
and that for the second mode
is plotted along the y-axis. The
number-conserving operators
act along diagonals similarly
to the operators J+, J−, Jz to
easily provide states and matrix
elements for the 𝔰𝔲(2) operators.
Here L, R, D are 3 × 3 matrices. The only
nonzero commutator is [L, R] = D. The
corresponding diﬀerential operator algebra
is
[
x
y
z
] ⎡
⎢
⎢⎣
0
l
d
0
0
r
0
0
0
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
𝜕x
𝜕y
𝜕z
⎤
⎥
⎥⎦
= l+ r+ d.
(5.61)
The three diﬀerential operators are
= x𝜕y = y𝜕z = x𝜕z.
(5.62)
Among these operators, none depends on
z (so 𝜕z has nothing to operate on) and
none contains 𝜕x, so that in essence x is
an irrelevant variable. A more economical
representation of this algebra is obtained
by zeroing out the cyclic variables z, 𝜕x and
replacing their duals by 𝜕z, x by +1 (duality
is under the commutator [𝜕i, xj
] = 𝛿ij)
[
1
y
0
] ⎡
⎢
⎢⎣
0
l
d
0
0
r
0
0
0
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
0
𝜕y
1
⎤
⎥
⎥⎦
= l′ + r′ + d′,
(5.63)
′ = 𝜕y,
′ = y,
′ = 1.
(5.64)
In essence, we have zeroed out the oper-
ators coupled to the vanishing rows and
columns of the matrix Lie algebra and
replaced their dual variables by 1. The
representation
given
is
essentially
the
Heisenberg representation of the position
(y) and conjugate momentum (py ≃𝜕y)
operators in quantum mechanics.
5.7.6
Disentangling Results
It happens surprisingly often in distantly
related ﬁelds of physics that expressions of
the form ex+𝜕x are encountered. Needless
to say, these are not necessarily endearing
to work with. One approach to simplifying
computations involving such operators is
to rewrite the operator in such a way that
all diﬀerential operators 𝜕x act ﬁrst, and all
multiplications by x act last. One way to
eﬀect this decomposition is to cross one’s
ﬁngers and write this operator as eax+b𝜕x ≃
eaxeb𝜕x and hope for the best. Of course, this
does not work, since the operators x and 𝜕x
do not commute.
Exponential
operator
rearrangements
are called disentangling theorems. Since

5.8 Riemannian Symmetric Spaces
181
the
exponential
mapping
is
involved,
powerful methods are available when the
operators in the exponential belong to a
ﬁnite-dimensional Lie algebra. Here is the
algorithm:
1. Determine the Lie algebra.
2. Find a faithful ﬁnite-dimensional matrix
representation of this Lie algebra.
3. Identify how you want the operators
ordered in the ﬁnal product of
exponentials.
4. Compute this result in the faithful
matrix representation.
5. Lift this result back to the operator
form.
Here is how this algorithm works. The
operators x and 𝜕x have one nonzero com-
mutator [𝜕x, x] = 1. These three operators
close under commutation. They therefore
form a Lie algebra. This is the algebra 𝔥3 of
the Heisenberg group, (5.64). We also have
a faithful matrix representation of this Lie
algebra, given in (5.63). We make the iden-
tiﬁcation
eax+b𝜕x →exp
⎡
⎢
⎢⎣
0
b
0
0
0
a
0
0
0
⎤
⎥
⎥⎦
=
⎡
⎢
⎢⎣
1
b
ab
2
0
1
a
0
0
1
⎤
⎥
⎥⎦
.
(5.65)
Now we identify this matrix with
erxedIel𝜕x →
⎡
⎢
⎢⎣
1
0
0
0
1
r
0
0
1
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
1
0
d
0
1
0
0
0
1
⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
1
l
0
0
1
0
0
0
1
⎤
⎥
⎥⎦
.
(5.66)
By multiplying out the three matrices in
(5.66) and comparing with the matrix
elements of the 3 × 3 matrix in (5.65), we
learn that l = b, r = a, d = ab∕2. Porting
the results of this matrix calculation back
to the land of operator algebras, we ﬁnd
eax+b𝜕x = eaxeab∕2eb𝜕x.
(5.67)
We
will
use
this
expression
in
Section 5.14.7 to construct a generating
function for the Hermite polynomials.
5.8
Riemannian Symmetric Spaces
A Riemannian symmetric space is a mani-
fold on which a Positive-deﬁnite metric can
be deﬁned everywhere. In addition, at each
point p there is an isometry (transforma-
tion that leaves distances between points
unchanged) that (i) leaves p ﬁxed; (ii) is
not the identity; and (iii) whose square is
the identity. It was discovered by Cartan
that Riemannian symmetric spaces are very
closely related to Lie groups. Speciﬁcally,
they are quotients of Lie groups by certain
Lie subgroups [3, 4, 10]. We illustrate with
some examples.
The Lie algebra for the Lorentz group
consists of 4 × 4 matrices:
𝔰𝔬(1, 3) =
⎡
⎢
⎢
⎢
⎢⎣
0
𝛼1
𝛼2
𝛼3
𝛼1
0
𝜃3
−𝜃2
𝛼2
−𝜃3
0
𝜃1
𝛼3
𝜃2
−𝜃1
0
⎤
⎥
⎥
⎥
⎥⎦
.
(5.68)
The Cartan–Killing metric for SO(1, 3) is
given by the trace of the product of this
matrix with itself:
g(𝔰𝔬(1, 3)) = 2(𝛼2
1 + 𝛼2
2 + 𝛼2
3 −𝜃2
1 −𝜃2
2 −𝜃2
3).
(5.69)
The
subalgebra
of
rotations
𝔰𝔬(3)
describes the compact subgroup SO(3).
The remaining three inﬁnitesimal gen-
erators parameterized by 𝛼i
span the

182
5 Group Theory
noncompact part of this group, the coset
SO(1, 3)∕SO(3), and exponentiate to boost
elements.
Cartan has pointed out that it is often
possible to ﬁnd a linear transformation, T,
of a Lie algebra 𝔤to itself whose square
is the identity: T ≠I, T2 = I. Such a T has
two eigenspaces, 𝔨and 𝔭, with T𝔤= T(𝔨⊕
𝔭) = 𝔨⊖𝔭. The two subspaces are orthogo-
nal under the Cartan metric and satisfy the
commutation relations:
[
𝔨, 𝔨
]
⊆𝔨
[
𝔨, 𝔭
]
⊆𝔭
[
𝔭, 𝔭
]
⊆𝔨.
(5.70)
When this is possible, exp(𝔨) = K is a
subgroup of G and exp(𝔭) = P = G∕K.
Further, if the Cartan–Killing metric is
negative deﬁnite on 𝔨, and positive deﬁ-
nite on 𝔭, then K is a maximal compact
subgroup of G and the coset P = G∕K
is
a
Riemannian
symmetric
space.
A
Riemannian symmetric space is homo-
geoeous: every point looks like every
other point. It is not necessarily isotropic:
every direction looks like every other
direction. Spheres are homogeneous and
isotropic.
For the Lorentz group SO(1, 3), by
Cartan’s criterion, SO(3) is the maximal
compact subgroup and the coset of boost
transformations
B(𝛃) = SO(1, 3)∕SO(3)
is
a
three-dimensional
Riemannian
space with positive-deﬁnite metric. In
this case, the space is a 3-hyperboloid
(ct)2 −x2 −y2 −z2 = cst.
embedded
in
R4.
The
metric
on
this
space
is
obtained by moving the metric (1, 1, 1)
at
the
origin
(x, y, z) = (0, 0, 0)
over
the embedded space using the set of
Lorenz
group
transformations
in
the
quotient
space
SO(1, 3)∕SO(3).
Car-
tan also
showed that all
Riemannian
symmetric spaces arise as quotients of
(simple) Lie groups by maximal compact
subgroups.
5.9
Applications in Classical Physics
Group theory’s ﬁrst important role in
physics
came
even
before
quantum
mechanics was discovered. The two pil-
lars of classical deterministic physics are
mechanics and electrodynamics. Group
theory played a fundamental role in rec-
tifying the diﬃculties in describing the
interactions between these two ﬁelds. The
principle tool used, besides group theory,
was Galileo’s principle of relativity and an
assumption about the underlying elegance
of physical theories.
5.9.1
Principle of Relativity
The principle of relativity posits as follows:
Two observers, S and S′, observe the same
physical system. Each knows how his coor-
dinate system is related to the other’s – that
is, the transformation of coordinates that
maps one coordinate system into the other.
Assume both observers collect data on the
same physical system. Given the data that
S takes, and the coordinate transformation
from S to S′, S can predict the data that S′
has recorded. And he will be correct.
Essentially, without this ability to com-
municate data among observers, there
would be little point in pursuing the
scientiﬁc method.
A second assumption that is used is usu-
ally not stated explicitly. This assumption is
as follows: the quantitative formulation of
physical laws is simple and elegant (what-
ever that means).
5.9.2
Making Mechanics and Electrodynamics
Compatible
The quantitative formulation of mechanics
for a single particle in an inertial frame is

5.9 Applications in Classical Physics
183
dp
dt = F(x),
(5.71)
where p is deﬁned by p = mdx∕dt for a par-
ticle with ﬁxed mass m. The transforma-
tions from one inertial coordinate system to
another consist of displacements in space
d, displacements in time d, rigid rotations
R, and boosts with constant velocity v that
keep the axis parallel. The space and time
coordinates in S′ are related to those in S
by
x′
=
Rx + vt + d
t′
=
t + d.
(5.72)
In the inertial coordinate system S′, New-
ton’s equations are
dp′
dt = F′(x′)
p′
=
Rp
F′
=
RF.
(5.73)
The equations of motion have the same vec-
torial form in both inertial coordinate sys-
tems (the simple and elegant assumption).
Newton’s laws were incredibly successful
in describing planetary motion in our solar
system, so when Maxwell developed his
laws for electrodynamics, it was natural
to assume that they also retained their
form under the set of inertial coordinate
transformations given in (5.72). Apply-
ing these transformations to Maxwell’s
equations creates a big mess. But if one
looks only at signals propagating along or
opposite the direction of the velocity v,
these assumptions predict
(c dt)′ →c′ dt,
c′ = c ± |v|.
(5.74)
The round trip time for a light sig-
nal traveling in a cavity of length L as
seen by S is 2L∕c, while in S′ the time
lapse was predicted to be L∕(c + v) + L∕
(c −v) = (2L∕c)∕(1 −𝛽2), with 𝛽= |v|∕c.
This predicted diﬀerence in elapsed
round trip times in “rest” and “moving”
frames was thought to enable us to deter-
mine how fast the earth was moving in
the Universe. As ever more precise mea-
surements in the late nineteenth and early
twentieth century led to greater disappoint-
ment, more and more bizarre explanations
were created to “explain” this null result.
Finally, Einstein and Poincaré returned to
the culprit (5.74) and asserted what the
experiments showed: c′ = c, so that
(c dt)′ →c dt′,
dt′ = linear comb. dx, dy, dz, dt.
(5.75)
The condition that the distance function
(c d𝜏)2 = (c dt)2 −(dx2 + dy2 + dz2)
= (c dt′)2−(dx′2 + dy′2+dz′2)
(5.76)
is
invariant
leads
directly
to
the
transformation
law
for
inﬁnitesimals
dx′𝜇= Λ𝜇
𝜈dx𝜈, where the 4 × 4 matrices
belong to the Lorentz group Λ ∈SO(3, 1),
Λ𝜇
𝜈= 𝜕x
′𝜇∕𝜕x𝜈. The transformation laws
taking inertial frames S to inertial frames
S′
involves
inhomogeneous
coordinate
transformations
[ x′
1
]
=
[ SO(3, 1)
d
0
1
] [ x
1
]
. (5.77)
While
Maxwell’s
equations
remain
unchanged in form under this set of
coordinate
transformations
(inhomoge-
neous Lorentz group), Newton’s force law
no longer preserves its form.
In order to ﬁnd the proper form for the
laws of classical mechanics under this new
set of transformations the following two-
step process was adopted:
1. Find an equation that has the proper
transformation properties under the
inhomogeneous Lorentz group.

184
5 Group Theory
2. If the equation reduces to Newton’s
equation of motion in the nonrelati-
vistic limit 𝛽→0, it is the proper
generalization of Newton’s equation of
motion.
Application of the procedure leads to the
relativistic equation for particle motion
dp𝜇
d𝜏= f 𝜇,
(5.78)
where p𝜇is deﬁned by p𝜇= m(dx𝜇∕d𝜏).
The components of the relativistic four-
vector f 𝜇are related to the three-vector
force F by
f
=
F + (𝛾) 𝜷⋅F
𝜷⋅𝜷𝜷
f 0
=
𝛾𝜷⋅F
(5.79)
(cf. (5.31)).
5.9.3
Gravitation
Einstein wondered how it could be possi-
ble to determine if you were in an iner-
tial frame. He decided that the algorithm
for responding to this question, Newton’s
ﬁrst law (In an inertial frame, an object at
rest remains at rest and an object in motion
remains in motion with the same velocity
unless acted upon by external forces.) was
circularly deﬁned (How do you know there
are no forces? When you are suﬃciently far
away from the ﬁxed stars. How do you know
you are suﬃciently far away? When there
are no forces.)
He therefore set out to formulate the
laws of mechanics in such a way that they
were invariant in form under an arbitrary
coordinate
transformation.
While
the
Lorentz group is six dimensional, gen-
eral coordinate transformations form an
“inﬁnite-dimensional” group. The transfor-
mation properties at any point are deﬁned
by
a
Jacobian
matrix
[(𝜕x′𝜇∕𝜕x𝜈)(x)].
While for the Lorentz group, this matrix
is constant throughout space, for gen-
eral coordinate transformations this 4 × 4
matrix is position dependent.
Nevertheless, he was able to modify the
algorithm described above to formulate
laws that are invariant under all coordinate
transformations. This two-step process
is a powerful formulation of the equiva-
lence principle. It is called the principle
of general covariance [11] It states that a
law of physics holds in the presence of a
gravitational ﬁeld provided
1. The equation is invariant in form under
an arbitrary coordinate transformation
x →x′(x).
2. In a locally free-falling coordinate
system, or the absence of a gravitational
ﬁeld, the equation assumes the form of
a law within the special theory of
relativity.
Using these arguments, he was able to
show that the equation describing the tra-
jectory of a particle is
d2x𝜇
d𝜏2 = −Γ𝜇
𝜈,𝜅
dx𝜈
d𝜏
dx𝜅
d𝜏.
(5.80)
The Christoﬀel symbols are deﬁned in
terms of the metric tensor g𝜇,𝜈(x) and its
inverse g𝜈,𝜌(x) by
Γ𝜇
𝜈,𝜅= 1
2g𝜇,𝜌
(𝜕g𝜈,𝜌
𝜕x𝜅+
𝜕g𝜌,𝜅
𝜕x𝜈−
𝜕g𝜈,𝜅
𝜕x𝜌
)
.
(5.81)
They are not components of a tensor, as
coordinate systems can be found (freely
falling, as in an elevator) in which they
vanish and the metric tensor reduces
to
its
form
in
special
relativity:
g =
diag(1, −1, −1, −1).

5.10 Linear Representations
185
Neither the left-hand side nor the right-
hand side of (5.80) is invariant under arbi-
trary coordinate changes (extra terms creep
in), but the following transformation law is
valid [11]
d2x
′𝜇
d𝜏2 + Γ
′𝜇
𝜈,𝜅
dx
′𝜈
d𝜏
dx
′𝜅
d𝜏
=
(
𝜕x
′𝜇
𝜕x𝜆
) (
d2x𝜆
d𝜏2 +Γ𝜆
𝜈,𝜅
dx𝜈
d𝜏
dx𝜅
d𝜏
)
. (5.82)
This means that the set of terms on the left,
or those within the brackets on the right,
have the simple transformation properties
of a four-vector. In a freely falling coordi-
nate system, the Christoﬀel symbols vanish
and what remains is d2x𝜆∕d𝜏2. This special
relativity expression is zero in the absence
of forces, so the equation that describes
the trajectory of a particle in a gravitational
ﬁeld is
d2x𝜆
d𝜏2 + Γ𝜆
𝜈,𝜅
dx𝜈
d𝜏
dx𝜅
d𝜏= 0.
(5.83)
5.9.4
Reﬂections
Two lines of reasoning have entered the
reconciliation of the two pillars of classi-
cal deterministic physics and the creation
of a theory of gravitation. One is group the-
ory and is motivated by Galileo’s principle
of relativity. The other is more vague. It is
a principle of elegance: there is the myste-
rious assumption that the structure of the
“real” equations of physics are simple, ele-
gant, and invariant under a certain class of
coordinate transformations. The groups are
the 10-parameter inhomogeneous Lorentz
group in the case of the special theory of
relativity and the much larger group of gen-
eral coordinate transformations in the case
of the general theory of relativity. There
is every likelihood that intergalactic trav-
elers will recognize the principle of rela-
tivity but no guarantee that their sense of
simplicity and elegance will be anything like
our own.
5.10
Linear Representations
The theory of representations of groups
– more precisely the linear representa-
tions of groups by matrices – was actively
studied by mathematicians, while physi-
cists actively ignored these results. This
picture changed dramatically with the
development of the quantum theory, the
understanding that the appropriate “phase
space” was the Hilbert space describing a
quantum system, and that group elements
acted in these spaces through their linear
matrix representations [8].
A linear matrix representation is a map-
ping of group elements g to matrices g →
Γ(g) that preserves the group operation:
gi ∘gj = gk ⇒Γ(gi) × Γ(gj) = Γ(gk).
(5.84)
Here, ∘is the composition in the group and
× indicates matrix multiplication. Often
the mapping is one way: many diﬀerent
group elements can map to the same matrix
(homomorphism). If the mapping is 1 ∶1
the mapping is an isomorphism and the
representation is called faithful.
5.10.1
Maps to Matrices
We have already seen many matrix repre-
sentations. We have seen representations
of the two-element group Z2 as reﬂection,
rotation, and inversion matrices acting in
R3 (cf. (5.13)).
So, how many representations does a
group have? It is clear from the example
of Z2 that we can create an inﬁnite num-
ber of representations. However, if we

186
5 Group Theory
look carefully at the three representations
presented in (5.13), we see that all these
representations are diagonal: direct sums
of essentially two distinct one-dimensional
matrix representations:
Z2
e
f
Γ1
[1]
[1]
Γ2
[1]
[−1]
.
(5.85)
Each of the three matrix representations of
Z2 in (5.13) is a direct sum of these two
irreducible representations:
𝜎Z
=
Γ1
⊕
Γ1
⊕
Γ2,
RZ(𝜋)
=
Γ1
⊕
Γ2
⊕
Γ2,

=
Γ2
⊕
Γ2
⊕
Γ2.
(5.86)
A basic result of representation theory
is that for large classes of groups (ﬁnite,
discrete, compact Lie groups), every rep-
resentation can be written as a direct
sum of irreducible representations. The
construction of this direct sum proceeds
by
matrix
diagonalization.
Irreducible
representations
are
those
that
cannot
be
further
diagonalized.
In
particular,
one-dimensional
representations
can-
not be further diagonalized. Rather than
enumerating all possible representations
of a group, it is suﬃcient to enumerate
only the much smaller set of irreducible
representations.
5.10.2
Group Element–Matrix Element Duality
The members of a group can be treated as
a set of points. It then becomes possible
to deﬁne a set of functions on this set of
points. How many independent functions
are needed to span this function space?
A not too particularly convenient choice
of basis functions are the delta functions
fi(g) = 𝛿(g, gi). For example, for C3v there
are six group elements and therefore six
basis functions for the linear vector space
of functions deﬁned on this group.
Each matrix element in any representa-
tion is a function deﬁned on the members
of a group. It would seem reasonable that
the number of matrix elements in all the
irreducible representations of a group pro-
vide a set of basis functions for the function
space deﬁned on the set of group elements.
This is true: it is a powerful theorem. There
is a far-reaching duality between the ele-
ments in a group and the set of matrix ele-
ments in its set of irreducible representa-
tions. Therefore, if Γ𝛼(g), 𝛼= 1, 2, … are
the irreducible representations of a group G
and the dimension of Γ𝛼(g) is d𝛼(i.e., Γ𝛼(g)
consists of d𝛼× d𝛼matrices), then the total
number of matrix elements is the order of
the group G:
all irreps
∑
𝛼
d2
𝛼= |G|.
(5.87)
Further, the set of functions
√
d𝛼∕|G|Γ𝛼
rs(g)
form a complete orthonormal set of func-
tions on the group space. The orthogonality
relation is
∑
g∈G
√
d𝛼′
|G|Γ𝛼′ ∗
r′s′ (g)
√
d𝛼
|G|Γ𝛼
rs(g)
= 𝛿(𝛼′, 𝛼)𝛿(r′, r)𝛿(s′, s),
(5.88)
and the completeness relation is
∑
𝛼
∑
r
∑
s
√
d𝛼
|G|Γ𝛼∗
rs (g′)
√
d𝛼
|G|Γ𝛼
rs(g)=𝛿(g′, g).
(5.89)
These complicated expressions can be
considerably simpliﬁed when written in the
Dirac notation. Deﬁne
⟨
g| 𝛼
rs
⟩
=
√
d𝛼
|G|Γ𝛼
rs(g),

5.10 Linear Representations
187
⟨𝛼
rs |g
⟩
=
√
d𝛼
|G|Γ𝛼∗
rs (g).
(5.90)
For convenience, we have assumed that
the irreducible representations are unitary:
Γ†(g) = Γ(g−1) and † =t ∗.
In Dirac notation, the orthogonality and
completeness relations are
Orthogonality ∶
⟨𝛼′
r′s′ |g
⟩⟨
g| 𝛼
rs
⟩
=
⟨𝛼′
r′s′ | 𝛼
rs
⟩
,
Completeness ∶
⟨
g′| 𝛼
rs
⟩⟨𝛼
rs |g
⟩
=
⟨
g′|g
⟩
.
(5.91)
As usual, doubled dummy indices are
summed over.
5.10.3
Classes and Characters
The group element–matrix element dual-
ity is elegant and powerful. It leads to yet
another duality, somewhat less elegant but,
in compensation, even more powerful. This
is the character–class duality.
We have already encountered classes
in (5.17). Two elements c1, c2 are in the
same class if there is a group element, g,
for which gc1g−1 = c2. The character of
a matrix is its trace. All elements in the
same class have the same character in any
representation, for
Tr Γ(c2) = Tr Γ(gc1g−1) = Tr Γ(g)Γ(c1)Γ(g−1)
= Tr Γ(c1).
(5.92)
The last result comes from invariance of
the trace under cyclic permutation of the
argument matrices.
With relatively little work, the powerful
orthogonality and completeness relations
for the group elements–matrix elements
can
be
transformed
to
corresponding
orthogonality and completeness relations
for classes and characters. If 𝜒𝛼(i) is the
character for elements in class i in irre-
ducible representation 𝛼and ni is the
number of group elements in that class,
the character–class duality is described by
the following relations:
Orthogonality ∶
∑
i
ni𝜒𝛼′∗(i)𝜒𝛼(i) = |G|𝛿(𝛼′, 𝛼),
(5.93)
Completeness ∶
∑
𝛼
ni𝜒𝛼∗(i)𝜒𝛼(i′) = |G|𝛿(i′, i).
(5.94)
5.10.4
Fourier Analysis on Groups
The group C3v has six elements. Its set of
irreducible representations has a total of
six matrix elements. Therefore d2
1 + d2
2 +
· · · = 6. This group has three classes. By the
character–class duality, it has three irre-
ducible representations. As a result, d1 =
d2 = 1 and d3 = 2. The matrices of the six
group elements in the three irreducible rep-
resentations are:
Γ1
Γ2
Γ3
e
[1]
[1]
[ 1
0
0
1
]
C+
3
[1]
[1]
[ −a
b
−b
−a
]
C−
3
[1]
[1]
[ −a
−b
b
−a
]
𝜎1
[1]
[−1]
[ −1
0
0
1
]
𝜎2
[1]
[−1]
[ a
b
b
−a
]
𝜎3
[1]
[−1]
[
a
−b
−b
−a
]
a = 1
2 b =
√
3
2 .
(5.95)

188
5 Group Theory
The character table for this group is
{e}
{C+
3 , C−
3
}
{𝜎1, 𝜎2, 𝜎3
}
1
2
3
𝜒1
1
1
1
𝜒2
1
1
−1
𝜒3
2
−1
0.
(5.96)
The ﬁrst line shows how the group ele-
ments are apportioned to the three classes.
The second shows the number of group ele-
ments in each class. The remaining lines
show the trace of the matrix representatives
of the elements in each class in each repre-
sentation. For example, the −1 in the mid-
dle of the last line is −1 = −1∕2 −1∕2. The
character of the identity group element e is
the dimension of the matrix representation,
d𝛼. Observe that the rows of the table in
(5.96) satisfy the orthogonality relations in
(5.93) and the columns of the table in (5.96)
satisfy the completeness relations in (5.94).
We use this character table to perform
a Fourier analysis on representations of
this group. For example, the representa-
tion of C3v = S3 in terms of 3 × 3 permuta-
tion matrices is not irreducible (cf. (5.15)).
For various reasons, we might like to know
which irreducible representations of C3v are
contained in this reducible representation.
The characters of the matrices describing
each class are
{e}
{C+
3 , C−
3
}
{𝜎1, 𝜎2, 𝜎3
}
𝜒3×3
3
0
1.
(5.97)
To determine the irreducible content of this
representation we take the inner product of
(5.97) with the rows of (5.96) using (5.93)
with the results
⟨𝜒3×3|𝜒1⟩= 1 × 3 × 1 + 2 × 0 × 1
+ 3 × 1 × 1 = 6,
⟨𝜒3×3|𝜒2⟩= 1 × 3 × 1 + 2 × 0 × 1
+ 3 × 1 × −1 = 0,
⟨𝜒3×3|𝜒3⟩= 1 × 3 × 2 + 2 × 0 × −1
+ 3 × 1 × 0 = 6.
(5.98)
As a result, the permutation representation
is reducible and 𝜒3×3 ≃𝜒1 ⊕𝜒3.
5.10.4.1
Remark on Terminology
The cyclic group Cn has n group ele-
ments gk, k = 0, 1, 2, … , n −1 that can
be identiﬁed with rotations through an
angle 𝜃k = 2𝜋k∕n. This group is abelian. It
therefore
has
n
one-dimensional
irre-
ducible
matrix
representations
Γm,
m = 0, 1, 2, … , n −1
whose
matrix
elements
are
Γm(𝜃k) = [e2𝜋ikm∕n].
Any
function deﬁned at the n equally spaced
points at angles 𝜃k around the circle can be
expressed in terms of the matrix elements
of the unitary irreducible representations
of Cn. The study of such functions, and
their transforms, is the study of Fourier
series. This analysis method can be applied
to functions deﬁned along the real line
R1 using the unitary irreducible repre-
sentations
(UIR)
Γk(x) = [eikx]
of
the
commutative group of translations Tx
along the real line through the distance
x. This is Fourier analysis on the real
line. This idea generalizes to groups and
their complete set of unitary irreducible
representations.
5.10.5
Irreps of SU(2)
The UIR (or “irreps”) of Lie groups can
be constructed following two routes. One
route begins with the group. The second
begins with its Lie algebra. The second
method is simpler to implement, so we use
it here to construct the hermitian irreps of
𝔰𝔲(2) and then exponentiate them to the
unitary irreps of SU(2).
The ﬁrst step is to construct shift opera-
tors from the basis vectors in 𝔰𝔲(2):

5.10 Linear Representations
189
S+
=
Sx + iSy
=
[ 0
1
0
0
]
S−
=
Sx −iSy
=
[ 0
0
1
0
]
Sz
=
1
2
[ 1
0
0
−1
]
[Sz, S±
]
=
±S±
[S+, S−
]
=
2Sz
.
(5.99)
Next, we use the matrix algebra to operator
algebra mapping (cf. Section 5.7.5) to con-
struct a useful boson operator realization of
this Lie algebra:
S+ →+ = b†
1b2
S−→−= b†
2b1
Sz →z = 1
2
(b†
1b1 −b†
2b2
) .
(5.100)
The next step introduces representations
(of a Lie algebra). Introduce a state space on
which the boson operators b1, b†
1 act, with
basis vectors |n1⟩, n1 = 0, 1, 2, … with the
action given as usual by
b†
1|n1⟩= |n1 + 1⟩
√
n1 + 1,
b1|n1⟩= |n1 −1⟩√n1.
(5.101)
Introduce a second state space for the oper-
ators b2, b†
2 and basis vectors |n2⟩, n2 =
0, 1, 2, …. In order to construct the irre-
ducible representations of 𝔰𝔲(2), we intro-
duce a grid, or lattice, of states |n1, n2⟩=
|n1⟩⊗|n2⟩. The operators ±, z are num-
ber conserving and move along the diago-
nal n1 + n2 = cnst. (cf. Figure 5.4). It is very
useful to relabel the basis vectors in this lat-
tice by two integers. One (j) identiﬁes the
diagonal, the other (m) speciﬁes position
along a diagonal:
2j = n1 + n2
n1 = j + m
2m = n1 −n2
n2 = j −m |n1, n2⟩↔
|||||
j
m
⟩
.
(5.102)
The spectrum of allowed values of the
quantum number j is 2j = 0, 1, 2, … and
m = −j, −j + 1, … , +j.
The matrix elements of the operators 
with respect to the basis |
j
m ⟩are con-
structed from the matrix elements of the
operators b†
i bj on the basis vectors |n1, n2⟩.
For z, we ﬁnd
z
|||||
j
m
⟩
= 1
2(b†
1b1 −b†
2b2) ||n1, n2⟩
= ||n1, n2⟩1
2(n1 −n2) =
|||||
j
m
⟩
m.
(5.103)
For the shift-up operator
+
|||||
j
m
⟩
= b†
1b2 ||n1, n2⟩
= ||n1 + 1, n2 −1⟩
√
n1 + 1√n2
=
|||||
j
m + 1
⟩√
(j + m + 1)(j −m).
(5.104)
and similarly for the shift-down operator
−
|||||
j
m
⟩
=
|||||
j
m −1
⟩√
(j −m + 1)(j + m).
(5.105)
In this representation of the (spin) angu-
lar momentum algebra 𝔰𝔲(2), z = Jz is
diagonal and ± = J± have one nonzero
diagonal
row
just
above
(below)
the
main
diagonal.
The
hermitian
irre-
ducible
representations
of
𝔰𝔲(2)
with

190
5 Group Theory
j = 0, 1
2, 1, 3
2, 2, 5
2, … form a complete set of
irreducible hermitian representations for
this Lie algebra.
The UIR of SU(2) are obtained by expo-
nentiating i times the hermitian represen-
tations of 𝔰𝔲(2):
J [SU(2)] = exp (în ⋅J𝜃) ,
(5.106)
with Jx = (J+ + J−)∕2 and Jy = (J+ −J−)∕2i,
and J∗are the (2j + 1) × (2j + 1) matri-
ces whose matrix elements are given
in (5.103–5.105). The (2j + 1) × (2j + 1)
matrices J are traditionally called Wigner
matrices [8]. For many purposes, only
the character of an irreducible represen-
tation is needed. The character depends
only on the class and the class is uniquely
determined by the rotation angle 𝜃since
rotations by angle 𝜃about any axis ̂n are
geometrically equivalent. It is suﬃcient
to compute the trace of any rotation, for
example, the rotation about the z-axis. This
matrix is diagonal: (eiJz𝜃)
m′,m = eim𝜃𝛿m′,m
and its trace is
𝜒j(𝜃) =
+j
∑
m=−j
eim𝜃=
sin(j + 1
2)𝜃
sin 1
2𝜃
.
(5.107)
These characters are orthonormal with
respect to the weight w(𝜃) = 1∕𝜋sin2(𝜃∕2).
5.10.6
Crystal Field Theory
The type of Fourier analysis outlined above
has found a useful role in crystal (or ligand)
ﬁeld theory [5, 7]. This theory was created
to describe the behavior of charged parti-
cles (electrons, ions, atoms) in the presence
of an electric ﬁeld that has some symmetry,
usually the symmetry of a host crystal. We
illustrate this with a simple example.
A many-electron atom with total angular
momentum L is placed in a crystal ﬁeld with
cubic symmetry. How do the 2L + 1-fold
degenerate levels split?
Before immersion in the crystal ﬁeld, the
atom has spherical symmetry. Its symme-
try group is the rotation group, the irre-
ducible representations L have dimension
2L + 1, the classes are rotations through
angle 𝜃, and the character for the class 𝜃in
representation L is given in (5.107) with
j →L (integer). When the atom is placed
in an electric ﬁeld with cubic symmetry
Oh, the irreducible representations of SO(3)
become reducible. The irreps of Oh are
A1, A2, E, T1, T2. The irreducible content is
obtained through a character analysis. of
L [SO(3)].
The group Oh has 24 elements parti-
tioned into ﬁve classes. These include the
identity E, eight rotations C3 by 2𝜋∕3 radi-
ans about the diagonals through the oppo-
site vertices of the cube, six rotations C4 by
2𝜋∕4 radians about the midpoints of oppo-
site faces, three rotations C2
4 by 2𝜋∕2 radi-
ans about the same midpoints of opposite
faces, and six rotations C2 about the mid-
points of opposite edges. The characters for
these ﬁve classes in the ﬁve irreducible rep-
resentations are collected in the character
table for Oh. This is shown at the top in
Table 5.2. At the bottom left of the table are
the characters of the irreducible represen-
tations of the rotation group SO(3) in the
irreducible representations of dimension
2L + 1. These are obtained from (5.107). A
character analysis (cf. (5.98)) leads to the
Oh irreducible content of each of the low-
est six irreducible representations of SO(3),
presented at the bottom right of the table.
5.11
Symmetry Groups
Groups ﬁrst appeared in the quantum the-
ory as a tool for labeling eigenstates of

5.11 Symmetry Groups
191
Table 5.2
(top) Character table for the cubic group Oh.
The functions in the right-hand column are some of the
basis vectors that “carry” the corresponding representation.
(bottom) Characters for rotations through the indicated
angle in the irreducible representations of the rotation
group.
Oh
E
8C3
3C2
4
6C2
6C4
Basis
A1
1
1
1
1
1
r2 = x2 + y2 + z2
A2
1
1
1
−1
−1
E
2
−1
2
0
0
(x2 −y2, 3z2 −r2)
T1
3
0
−1
−1
1
(x, y, z), (Lx, Ly, Lz)
T2
3
0
−1
1
−1
(yz, zx, xy)
L ∶𝜃
0
2𝜋
3
2𝜋
2
2𝜋
2
2𝜋
4
Reduction
0 S
1
1
1
1
1
A1
1 P
3
0
−1
−1
1
T1
2 D
5
−1
1
1
−1
E ⊕T2
3 F
7
1
−1
−1
−1
A2 ⊕T1 ⊕T2
4 G
9
0
1
1
1
A1 ⊕E ⊕T1 ⊕T2
5 H
11
−1
−1
−1
1
E ⊕2T1 ⊕T2
a Hamiltonian with useful quantum num-
bers. If a Hamiltonian is invariant under
the action of a group G, then gg−1 = ,
g ∈G. If |𝜓𝛼
𝜇⟩satisﬁes Schrödinger’s time-
independent equation |𝜓𝛼
𝜇⟩−E|𝜓𝛼
𝜇⟩= 0,
so that
g(−E)|𝜓𝛼
𝜇⟩= {g(−E)g−1} g|𝜓𝛼
𝜇⟩
= (−E)|𝜓𝛼
𝜈⟩⟨𝜓𝛼
𝜈|g|𝜓𝛼
𝜇⟩
= (−E)|𝜓𝛼
𝜈⟩𝛼
𝜈,𝜇(g)
= 0.
(5.108)
All states |𝜓𝛼
𝜈⟩related to each other by
a group transformation g ∈G (more pre-
cisely, a group representation 𝛼(g)) have
the same energy eigenvalue. The existence
of a symmetry group G for a Hamilto-
nian provides representation labels for
the quantum states and also describes the
degeneracy patterns that can be observed.
If the symmetry group G is a Lie group, so
that g = eX, then eXe−X = ⇒[X, ] =
0. The existence of operators X that com-
mute with the Hamiltonian is a clear sig-
nal that the physics described by the Hamil-
tonian is invariant under a Lie group.
For example, for a particle in a spherically
symmetric potential, V(r), Schrödinger’s
time-independent equation is
(p ⋅p
2m + V(r)
)
𝜓= E𝜓
(5.109)
with p = (ℏ∕i)∇. The Hamiltonian operator
is invariant under rotations. Equivalently,
it commutes with the angular momen-
tum operators L = r × p: [L, ] = 0. The
wavefunctions can be partly labeled by
rotation group quantum numbers, l and
m: 𝜓→𝜓l
m(r, 𝜃, 𝜙). In fact, by standard
separation of variables, arguments in this
description can be made more precise:
𝜓(r, 𝜃, 𝜙) = 1∕rRnl(r)Y l
m(𝜃, 𝜙). Here Rnl(r)
are radial wavefunctions that depend on

192
5 Group Theory
the potential V(r) but the angular func-
tion Y l
m(𝜃, 𝜙) is “a piece of geometry”: it
depends only on the existence of rotation
symmetry. It is the same no matter what
the potential is. In fact, these functions can
be constructed from the matrix represen-
tations of the group SO(3). The action of
a rotation group element g on the angular
functions is
gY l
m(𝜃, 𝜙) = Y l
m′(𝜃, 𝜙)l
m′m(g),
(5.110)
where the construction of the Wigner

matrices
has
been
described
in
Section 5.10.5.
If the symmetry group is reduced, as
in the case of SO(3) ↓Oh described in
Section 5.10.6, the eigenstates are iden-
tiﬁed by the labels of the irreducible
representations of Oh: A1, A2, E, T1, T2.
Once the states have been labeled, com-
putations must be done. At this point, the
power of group theory becomes apparent.
Matrices must be computed – for example,
matrix elements of a Hamiltonian. Typ-
ically, most matrix elements vanish (by
group-theoretic selection rules). Of the
small number that do not vanish, many are
simply related to a small number of the oth-
ers. In short, using group theory as a guide,
only a small number of computations must
actually be done.
This feature of group theory is illustrated
by computing the eigenstates and their
energy eigenvalues for an electron in the
N = 4 multiplet of the hydrogen atom
under the inﬂuence of a constant external
ﬁeld . The Hamiltonian to be diagonalized
is
⟨
N′
L′
M′
|||||
p ⋅p
2m −e2
r + e⋅r
|||||
N
L
M.
⟩
.
(5.111)
The ﬁrst two terms in the Hamiltonian
describe the electron in a Coulomb poten-
tial, the last is the Stark perturbation,
which describes the interaction of a dipole
d = −er with a constant external elec-
tric ﬁeld: St. = −d ⋅. In the N = 4
multiplet, we set N′ = N = 4, so that
L′, L = 0, 1, 2, 3 and M ranges from −L
to +L and −L′ ≤M′ ≤+L′. The matrix
elements of the Coulomb Hamiltonian are
EN𝛿N′N𝛿L′L𝛿M′M, with E4 = −13.6∕42 eV.
There are ∑3=4−1
L=0
(2L + 1) = 16 states
in the N = 4 multiplet, so 162 matrix
elements of the 16 × 16 matrix must be
computed. We simplify the computation
by choosing the z-axis in the direction
of the applied uniform electric ﬁeld, so
that e⋅r →ez (= ||). In addition,
we write z =
√
4𝜋∕3rY 1
0 (𝜃, 𝜙). The matrix
elements factor (separation of variables)
into a radial part and an angular part, as
follows:
⟨4L′M′|ez|4LM⟩→e× Radial × Angular
Radial = ∫
∞
0
R4L′(r)r1R4L(r)dr
Angular=
√
4𝜋
3 ∫Y L′∗
M′ (Ω)Y 1
0 (Ω)Y L
M(Ω)dΩ,
(5.112)
where Ω = (𝜃, 𝜙) and dΩ = sin 𝜃d𝜃d𝜙.
Selection rules derived from SO(3) sim-
plify the angular integral. First, the inte-
gral vanishes unless ΔM = M′ −M = 0. It
also vanishes unless ΔL = ±1, 0. By parity,
it vanishes if ΔL = 0, and by time reversal,
its value for M and −M are the same. The
nonzero angular integrals are
(L, M) =
√
4𝜋
3 ∫Ω
Y L∗
M′(Ω)Y 1
0 (Ω)Y L−1
M (Ω)dΩ
= 𝛿M′M
√
(L + M)(L −M)
(2L + 1)(2L −1), (5.113)
The useful radial integrals, those with
ΔL = ±1, are all related:

5.11 Symmetry Groups
193
(N, L) = ∫
∞
0
RN,L(r)rRN,L−1dr
= N
√
N2 −L2
2
√
3
× (2, 1)
(5.114)
with 1 ≤L ≤N −1. All integrals are pro-
portional to the single integral (2, 1).
This comes from yet another symmetry
that the Coulomb potential exhibits (cf.
Section 5.12), not shared by other spher-
ically symmetric potentials. The single
integral to be evaluated is
(2, 1) = −3
√
3a0.
(5.115)
This integral is proportional to the Bohr
radius a0 of the hydrogen atom, whose
value was estimated in (5.6).
The arguments above show drastic sim-
pliﬁcations in the computational load for
computing the energy eigenfunctions and
eigenvalues of a many-electron atom in
a uniform external electric ﬁeld (Stark
problem).
Of the 256 = 162 matrix elements to
compute, only 18 are nonzero. All are
real. Since the Hamiltonian is hermitian
(symmetric if real), there are in fact only
9 nonzero matrix elements to construct.
Each is a product of two factors, so only
6 (angular) plus 1 (radial) quantities need
be computed. These numbers must be
stuﬀed into a 16 × 16 matrix to be diago-
nalized. But there are no nonzero matrix
elements between states with M′ ≠M.
This means that by organizing the row
and columns appropriately the matrix can
be written in block diagonal form. The
block diagonal form consists of a 1 × 1
matrix for M = 3, a 2 × 2 matrix for M = 2,
a 3 × 3 matrix for M = 1, a 4 × 4 matrix
for M = 0, a 3 × 3 matrix for M = −1,
and so on. The 1 × 1 matrices are already
diagonal. The 2 × 2 matrices are identical,
so only one needs to be diagonalized.
Similarly for the two 3 × 3 matrices. There
is only one 4 × 4 matrix. The computa-
tional load for diagonalizing this matrix
has been reduced from T ≃162 log 16 to
T ≃22 log 2 + 32 log 3 + 42 log 4, a factor
of 20 (assuming the eﬀort required for
diagonalizing an n × n matrix goes like
n2 log n)!
It gets even better. For the N = 5 mul-
tiplet the 1 × 1, 2 × 2, 3 × 3, 4 × 4 matrices
are all proportional to the matrices of the
corresponding size for N = 4. The pro-
portionality factor is 5∕4. Only one new
matrix needs to be constructed – the 5 × 5
matrix. This symmetry extends to all values
of N.
This is a rather simple example that can
be carried out by hand. This was done
when quantum mechanics was ﬁrst devel-
oped, when the fastest computer was a
greased abacus. Today, time savings of a
factor of 20 on such a simple problem
would hardly be noticed. But calculations
have also inﬂated in size. Reducing a 106 ×
106 matrix to about 1000 103 × 103 matri-
ces reduces the computational eﬀort by a
factor of 2000. For example, a computa-
tion that would take 6 years without such
methods could be done in a day with these
methods.
Symmetry groups play several roles in
quantum mechanics.
• They provide group representation
labels to identify the energy eigenstates
of a Hamiltonian with symmetry.
• They provide selection rules that save us
the eﬀort of computing matrix elements
whose values are zero (by symmetry!).
• And they allow transformation of a
Hamiltonian matrix to block diagonal
form, so that the computational load can
be drastically reduced.

194
5 Group Theory
5.12
Dynamical Groups
A widely accepted bit of wisdom among
physicists is that symmetry implies degen-
eracy, and the larger the symmetry, the
larger the degeneracy. What works forward
ought to work backward (“Newton’s third
law”): if the degeneracy is greater than
expected, the symmetry is greater than
apparent.
5.12.1
Conformal Symmetry
The hydrogen atom has rotational symme-
try SO(3), and this requires (2L + 1)-fold
degeneracy. But the states with the same
principal quantum number N
are all
degenerate in the absence of spin and
other
relativistic
eﬀects,
and
nearly
degenerate
in
the
presence
of
these
eﬀects. It would make sense to look
for a larger than apparent symmetry. It
exists in the form of the Runge–Lenz
vector
M = 1∕2m(p × L −L × p) −e2r∕r,
where
r, p, L = r × p
are
the
position,
momentum, and orbital angular momen-
tum
operators
for
the
electron.
The
three orbital angular momentum oper-
ators Li and three components of the
Runge–Lenz vector close under com-
mutation to form a Lie algebra. The six
operators commute with the Hamilto-
nian, so the “hidden” symmetry group
is
larger
than
the
obvious
symmetry
group SO(3). On the bound states, this
Lie
algebra
describes
the
Lie
group
SO(4).
The
irreducible
representation
labels for the quantum states are N,
N = 1, 2, 3, … , ∞. The three nested groups
SO(2) ⊂SO(3) ⊂SO(4)
and
their
rep-
resentation labels and branching rules
are
Group Rep.label Degeneracy
Branching
rules
SO(4)
N
N2
SO(3)
L
2L + 1
0, 1, 2, … ,
N −1
SO(2)
M
1
−L ≤M≤+L.
(5.116)
Branching rules identify the irreducible
representations of a subgroup that any
representation of a larger group splits into
under
group–subgroup
reduction.
We
have seen branching rules in Table 5.2.
One advantage of using the larger group
is that there are more shift operators in the
Lie algebra. The shift operators, acting on
one state, moves it to another (cf. |LM⟩
L+
−→
|L, M + 1⟩). This means that there are well-
deﬁned algebraic relations among states
that belong to the same N multiplet. This
means that more of any computation can
be pushed from the physical domain to
the geometric domain, and simpliﬁcations
accrete.
Why stop there? In the hydrogen atom,
the energy diﬀerence between the most
tightly bound state, the ground state, and
the most weakly bound state (N →∞)
is 13.6 eV. When this diﬀerence is com-
pared with the electron rest energy of
511 KeV, the symmetry-breaking is about
13.6∕511 000 ≃0.000 027 or 2.7 × 10−3%.
This suggests that there is a yet larger group
that accounts for this near degeneracy.
Searches eventually lead to the noncom-
pact conformal group SO(4, 2) ⊃SO(4) · · ·
as the all-inclusive “symmetry group” of
the hydrogen atom. The virtue of using
this larger group is that states in diﬀerent
multiplets N, N ± 1 can be connected by
shift operators within the algebra 𝔰𝔬(4, 2),
and ultimately there is only one number to
compute [12]. Including this larger group
in (5.116) would include inserting it in the
row above SO(4), showing there is only
one representation label for bound states,

5.12 Dynamical Groups
195
indicating its degeneracy is “∞”, and adding
branching rules N = 1, 2, … , ∞to the
SO(4) row.
5.12.2
Atomic Shell Structure
Broken symmetry beautifully accounts for
the systematics of the chemical elements.
It accounts for the ﬁlling scheme as elec-
trons enter a screened Coulomb poten-
tial around a nuclear charge +Ze as the
nuclear charge increases from Z = 1 to Z >
92. The screening is caused by “inner elec-
trons.” The ﬁlling scheme accounts for the
“magic numbers” among the chemical ele-
ments: these are the nuclear charges of
exceptionally stable chemical elements He,
Ne, Ar, Kr, Xe, Rn with atomic numbers
2, 10, 18, 36, 54, 86.
When more than one electron is present
around a nuclear charge +Ze, then the outer
electrons “see” a screened central charge
and the SO(4) symmetry arising from the
Coulomb nature of the potential is lost.
There is a reduction in symmetry, a “broken
symmetry”: SO(4) ↓SO(3). The quantum
numbers (N, L) can be used to label states
and energies, EN,L, and these energy lev-
els are (2L + 1)-fold degenerate. The SO(4)
multiplet with quantum number N splits
into orbital angular momentum multiplets
with L values ranging from L = 0 to a max-
imum of L = N −1.
Each additional electron must enter an
orbital that is not already occupied by the
Pauli exclusion principle. This principle is
enforced by the requirement that the total
electron wavefunction transforms under
the unique antisymmetric representation
Γanti.(Sk) on the permutation group Sk for k
electrons.
Generally,
the
larger
the
L
value
the further the outer electron is from
the central charge, on average. And the
further it is, the larger is the negative
charge density contributed by inner elec-
trons that reduces the strength of the
central nuclear attraction. As a result,
EN,0 < EN,1 < · · · < EN,L=N−1.
There
is
mixing among levels with diﬀerent values
of N and L. The following energy ordering
scheme, ultimately justiﬁed by detailed
calculations, accounts for the systematics
of the chemical elements, including the
magic numbers:
1S | 2S 2P | 3S 3P | 4S 3D 4P | 5S 4D 5P |
6S 4F 5D 6P | 7S.
(5.117)
Each level can hold 2(2L + 1) electrons.
The ﬁrst factor of 2 = (2s + 1) with s = 1∕2
is due to electron spin. The vertical bar |
indicates a large energy gap. The cumu-
lative occupancy reproduces the magic
numbers
of
the
chemical
elements:
2, 10, 18, 36, 54, 86. The ﬁlling order is
shown in Figure 5.5. Broken symmetry is
consistent with Mendeleev’s periodic table
of the chemical elements.
5.12.3
Nuclear Shell Structure
Magic numbers among nuclei suggested
that, here also, one could possibly describe
many diﬀerent nuclei with a single sim-
ple organizational structure. The magic
numbers
are:
2, 8, 20, 28, 40, 50, 82, 126,
both for protons and for neutrons. The
following model was used to organize this
information.
Assume that the eﬀective nuclear poten-
tial for protons (or neutrons) is that of
a three-dimensional isotropic harmonic
oscillator.
The
algebraic
properties
of
the
three-dimensional
isotropic
har-
monic oscillator are described by the
unitary group U(3) and its Lie algebra
𝔲(3). The basis states for excitations can

196
5 Group Theory
7
S . . .
S P D F G
S P D F G
S P D F
S P D
S P
S
6
5
4
3
2
1
118
5 F Actmides
7 S Fr - Ra
6 P TL - Rn
5 D Lu - Hg
4 F Lanthanides
5 P In - Xe
4 D Y - Cd
5 S Rb - Sr
4 P Ga - Kr
3 D Sc - Zn
3 P AL - Ar
3 S Na - Mg
2 P B - Ne
1 S H - He
2 S Li - Be
4 S K - Ca
6 S Cs - Ba
86
54
36
18
10
2
Figure 5.5
Broken SO(4) dynamical
symmetry due to screening of the
central Coulomb potential by inner
electrons successfully accounts
for the known properties of the
chemical elements, as reﬂected in
Mendeleev’s periodic table of the
chemical elements.
be described by |n1, n2, n3⟩. One exci-
tation
would
be
threefold
degenerate:
|1, 0, 0⟩, |0, 1, 0⟩, |0, 0, 1⟩, two excitations
would be sixfold degenerate, and states
with N excitations would have a degen-
eracy
(N + 2)(N + 1)∕2.
Each
integer
N ≥0 describes an irrep of U(3). Under a
spherically symmetric perturbation, these
highly degenerate N multiplets would split
into multiplets identiﬁed by an angular
momentum index L. A character analysis
gives this branching result
U(3)
SO(3)
N
L values
Spectroscopic
0
0
S
1
1
P
2
2, 0
D, S
3
3, 1
F, P
4
4, 2, 0
G, D, S.
(5.118)
For example, the N = 4 harmonic oscilla-
tor multiplet splits into an L = 4 multiplet,
an L = 2 multiplet, and an L = 0 multi-
plet. The larger the angular momentum,

5.12 Dynamical Groups
197
the lower the energy. After this splitting,
the spin of the proton (or neutron) is cou-
pled to the orbital angular momentum to
give values of the total angular momen-
tum J = L ± 1∕2, except that for S states
only the J = 1∕2 state occurs. Again, the
larger angular momentum occurs at a lower
energy than the smaller angular momen-
tum. The resulting ﬁlling order, analogous
to (5.117), is
0S1∕2|1P3∕2 1P1∕2|2D5∕2 2S1∕2 2D3∕2|3F7∕2|
3P3∕2 3F5∕2 3P1∕2 4G9∕2|
4D5∕2 4G7∕2 4S1∕2 4D3∕2 5H11∕2|
5H9∕2 5F7∕2 5F5∕2 5P3∕2 5P1∕2 6I13∕2|.
(5.119)
Each shell with angular momentum j can
hold up to 2j + 1 nucleons. Broken sym-
metry is also consistent with the “periodic
table” associated with nuclear shell mod-
els. The ﬁlling order is shown in Figure 5.6
[13, 14].
6
I G D S
5P
126
82
50
28
20
8
2
7J15/2
6(Ι11/2, G9/2, 7/2 D5/2, 3/2, S1/2)
5(H9/2, F7/2, F5/2, P3/2, P1/2)
4(G7/2, D5/2, D3/2, S1/2)
6Ι13/2
5H11/2
4G9/2
3P1/2
3F5/2
3P3/2
3F7/2
2D3/2
2S1/2
2D5/2
1P1/2
1P3/2
0S1/2
4S
3P
3F
2S
2D
1P
0S
4D
4G
5F
5H
H F P
G D S
F P
D S
P
S
5
4
3
2
1
0
Figure 5.6
The ﬁlling order describing very
many properties of nuclear ground states is
described by the levels of an isotropic har-
monic oscillator potential with multiplets hav-
ing N excitations and degeneracy (N + 1)(N +
2)∕2. The degeneracy is broken by a spheri-
cally symmetric perturbation and broken fur-
ther by spin-orbit coupling. For both perturba-
tions, energy increases as angular momentum
decreases. The ﬁlling order shown success-
fully accounts for the known properties of
the ground states of most even–even nuclei,
including the magic numbers. In the higher lev-
els, the largest spin angular momentum state
(e.g., 5H11∕2) is pushed down into the next
lower multiplet, containing all the remaining
N = 4 states, with the exception of the 4G9∕2.

198
5 Group Theory
At a group theoretical level, our start-
ing point has been the Lie algebra 𝔲(3)
with basis vectors b†
i bj (1 ≤i, j ≤3) whose
representations are labeled by an integer
index N, the number of excitations present.
This algebra can be embedded in a larger
Lie algebra containing in addition shift-up
operators b†
i , their counterpart annihilation
operators bj, and the identity operator I.
The Lie algebra is 9 + 2 ⋅3 + 1 = 16 = 42
dimensional, and is closely related to
the noncompact Lie algebra 𝔲(3, 1). The
embedding
𝔲(3) ⊂𝔲(3, 1)
is
analogous
to the inclusion SO(4) ⊂SO(4, 2) for the
hydrogen atom.
5.12.4
Dynamical Models
In this section, so far we have described
the hydrogen atom using a very large group
SO(4, 2) and breaking down the symmetry
to SO(4) and further to SO(3) when there
are Coulomb-breaking perturbations that
maintain their spherical symmetry. We
have also introduced a sequence of groups
and
subgroups
U(3, 1) ↓U(3) ↓SO(3)
to provide a basis for the nuclear shell
model.
Nuclear computations are very diﬃ-
cult because there is “no nuclear force.”
The force acting between nucleons is
a residual force from the quark–quark
interaction.
This
is
analogous
to
the
absence of a “molecular force.” There
is none – the force that binds together
atoms in molecules is the residual elec-
tromagnetic
force
after
exchange
and
other interactions have been taken into
account.
For this reason, it would be very use-
ful to develop a systematic way for making
nuclear models and carrying out calcula-
tions within the context of these models.
Group theory comes to the rescue!
The ﬁrst step in creating a simple envi-
ronment for quantitative nuclear models
is to assume that pairs of nucleons bind
tightly into boson-like excitations. The
leading assumption is that of all the
nuclear-pair degrees of freedom, the most
important are those with scalar (S, L = 0)
and quadrupole (D, L = 2) transformation
properties under the rotation group SO(3).
States in a Hilbert space describing 2
protons (neutrons, nucleons) can be pro-
duced by creation operators s†, d†
m acting
on the vacuum |0; 0, 0, 0, 0, 0⟩. For n pairs
of nucleons, n creation operators act to
produce
states
|ns; n−2, n−1, n0, n1, n2⟩
with
ns + ∑
m nm = n.
There
are
(n + 6 −1)!∕n!(6 −1)!
states
in
this
Hilbert space. For computational con-
venience, they can be arranged by their
transformation properties under rotations
SO(3). For example, the two-boson Hilbert
space has 21 states consisting of an L = 0
state from s†s†, an L = 2 multiplet from
s†d†
m, and multiplets with L = 0, 2, 4 from
d†
m′d†
m.
The Hamiltonian acts within the space
with a ﬁxed number of bosons. It must
therefore be constructed from number-
conserving
operators:
b†
i bj,
where
the
boson operators include the s and d excita-
tions. These operators must be rotationally
invariant. At the linear level, only two
such operators exist: s†s and d†
mdm. At the
quadratic level, there are a small num-
ber of additional rotationally invariant
operators. The n-boson Hamiltonian can
therefore be systematically parameterized
by a relatively small number of terms. The
parameters can be varied in attempts to ﬁt
models to nuclear spectra and transition
rates. In the two-boson example with
21 states, it is suﬃcient to diagonalize
this Hamiltonian in the two-dimensional
subspace of L = 0 multiplets, in another
two-dimensional subspace with the two

5.13 Gauge Theory
199
S0(5)
S0(3)
SU(3)
S0(6)
U(6)
U(5)
Figure 5.7
States with 2N nucleons outside a closed shell
are described by N bosons in the interacting boson model.
The basis states carry a symmetric representation of the
Lie group U(6). Various limiting Hamiltonians that exhibit a
group–subgroup symmetry can be diagonalized by hand. The
three group–subgroup chains for which this is possible are
shown here.
states with L = 2 and ML = 2 (all other
ML values will give the same result),
and the one-dimensional subspace with
L = 4, ML = 4.
The interacting boson model (IBM)
outlined above has deeply extended our
understanding of nuclear physics [15]. In
fact, some Hamiltonians can be solved “by
hand.” These involve a group–subgroup
chain. The chain of groups is shown in
Figure 5.7. This model incorporates in a
magniﬁcent way the use of groups in their
capacity as symmetry groups, implying
degeneracy, and dynamical groups, imply-
ing relations among multiplets of diﬀerent
energies.
5.13
Gauge Theory
Gauge transformations were introduced
by Weyl following Einstein’s development
(1916) of the theory of general relativity.
In crude terms, Weyl’s original idea was
to introduce a ruler (the “gauge” of gauge
theory) whose length was an arbitrary
function of position. His original objective
was to unify the two then-known forces of
nature: gravitation and electromagnetism.
His theory is quite beautiful but Einstein
raised serious objections, and Weyl eventu-
ally relinquished it. Einstein’s objection was
that if Weyl’s theory were correct then the
results of laboratory experiments would
depend on the history of the material being
investigated.
Weyl came back to this general idea fol-
lowing Schrödinger’s development (1926)
of wave mechanics. In this case, a modiﬁed
objective was achieved: he succeeded in
describing how light interacts with charged
matter.
The original theory (GR) involved a
real
scaling
transformation
that
was
space–time dependent. As a result, it is
in the same spirit as the discussion about
scaling in Section 5.2.3, but more general.
His modiﬁed theory (QM) involved a com-
plex phase transformation. In some sense
this would be an analytic continuation
of the scaling arguments, but the spirit
of the discussion given in Section 5.2.3
does not in any sense suggest phase
changes.
The starting point of this work is
the observation that if 𝜓(x, t) satisﬁes
Schrödinger’s time-dependent equation, so
also does ei𝜙𝜓(x, t), for
(
−iℏ𝜕
𝜕t
)
ei𝜙𝜓(x, t)
= ei𝜙(
−iℏ𝜕
𝜕t
)
𝜓(x, t) = 0. (5.120)
This fails to be true if the phase 𝜙depends
on space–time coordinates, for then the
derivative terms act on this phase when we
try to pull it through the Hamiltonian and

200
5 Group Theory
time-derivative operators:
((p ⋅p
2m
)
+ qΦ(x, t) −iℏ𝜕
𝜕t
)
ei𝜙(x,t)𝜓(x, t)
= ei𝜙(x,t)
((p + ℏ∇𝜙)2
2m
+ qΦ(x, t)
+ ℏ𝜕𝜙
𝜕t −iℏ𝜕
𝜕t
)
𝜓(x, t).
(5.121)
Symmetry is not preserved. What can be
done?
It had long been known that the elec-
tric and magnetic ﬁelds E, B could be repre-
sented by “ﬁctitious” potentials that served
to simplify Maxwell’s equations but were
otherwise “not real.” The vector potential
A and scalar potential Φ are related to the
“real” ﬁelds by
B
=
𝛁× A
E
=
−𝛁Φ −1
c
𝜕A
𝜕t .
(5.122)
This simpliﬁcation is not unique. The vector
potential can be changed by the addition of
the gradient of a scalar ﬁeld 𝜒(x, t), and the
scalar potential correspondingly changed:
A →A′
=
A + ∇𝜒
⇒
B′ = B
Φ→Φ′
=
Φ −1
c
𝜕𝜒
𝜕t
⇒
E′ = E.
(5.123)
The resolution of the diﬃculty is to
assume that the electrostatic part of the
interaction is described by the term qΦ(x, t)
in the Hamiltonian and the magnetic part is
represented by replacing p by p −q
c A(x, t)
wherever it appears in the Hamiltonian.
Under these conditions,
(
p −q
c A(x, t)
)
ei𝜙(x,t)
= ei𝜙(x,t) (
p −q
c A(x, t) + ℏ∇𝜙(x, t)
)
(5.124)
and
(
qΦ −iℏ𝜕
𝜕t
)
ei𝜙(x,t)
= ei𝜙(x,t)
(
qΦ + ℏ𝜕𝜙
𝜕t −iℏ𝜕
𝜕t
)
.
(5.125)
If we choose 𝜙(x, t) = −q∕ℏc𝜒(x, t), then
the added terms on the right in (5.124) are
p −q
c A(x, t) −q
c ∇𝜒(x, t) = p −q
c A′(x, t)
(5.126)
and those on the right in (5.125) are
qΦ(x, t) −q
c
𝜕𝜒(x, t)
𝜕t
−iℏ𝜕
𝜕t
= qΦ′(x, t) −iℏ𝜕
𝜕t .
(5.127)
The result is that the structure of the inter-
action between the electromagnetic ﬁelds
and charged particles is invariant provided
the interaction is given in terms of the “ﬁc-
titious” ﬁelds A, Φ by
p →p −q
c A(x, t)
−iℏ𝜕
𝜕t →−iℏ𝜕
𝜕t + qΦ(x, t).
(5.128)
There are several other ways to couple the
electromagnetic ﬁeld with charged parti-
cles that are allowed by symmetry [16]. But
the structure of the interaction described
by (5.128) is suﬃcient to account for all
known measurements. It turns out that
Maxwell’s equations are also a consequence
of the structure of this interaction.
This principle is called the principle of
minimal electromagnetic coupling.
The phase transformation introduced
in (5.120) belongs to the Lie group U(1).
Its generalization to position-dependent
phase ei𝜙(x,t) does not belong to a Lie
group.
Questions soon surfaced whether the
same process could be used to describe

5.13 Gauge Theory
201
the interaction between more complicated
“charged” particles and the ﬁelds that
cause interactions among them. It seemed
that the proton–neutron pair was a good
candidate for such a treatment. These two
particles seemed to be essentially the same,
except that one was charged and the other
not. Neglecting charge, these two particles
could be treated as an isospin doublet. The
nucleon wavefunction 𝜙could be treated
as a two-state system, |𝜙⟩= | 𝜓p
𝜓n
⟩, and
the Hamiltonian describing nuclear inter-
actions should be invariant under a global
SU(2)
transformation,
analogous
to
a
global U(1) transformation ei𝜙in (5.120).
If the SU(2) rotation were allowed to vary
with position, perhaps it would be possible
to determine the nature of the interaction
between the nucleons (fermions) and the
bosons (𝜋±, 𝜋0, analogous to photons that
carry
the
electromagnetic
interaction)
responsible for the interaction among the
fermions.
This program was carried out by Yang
and Mills. They succeeded in determin-
ing the nature of the interaction. But we
now understand that nuclear interactions
are residual forces left over from the strong
interactions among the quarks.
Nevertheless, the program persisted. The
gauge program can be phrased as follows.
1. Suppose there is a set of n fermion
ﬁelds that are invariant under a
g-parameter Lie group.
2. Assume that the Hamiltonian
(Lagrangian, action integral) for these
ﬁelds, without any interaction, is
known.
3. Now assume that the Lie group
parameters are allowed to be functions
on space–time. What additional terms
occur in the Hamiltonian (cf. (5.121)
above)?
4. How many boson ﬁelds must be
introduced in order to leave the
structure of the Hamiltonian invariant?
5. How must they be introduced into the
Hamiltonian? That is, what is the
structure of the “minimal coupling” in
terms of the Lie algebra parameters (its
structure constants)?
6. How do these new ﬁelds transform
under the Lie group and its space–time
extension?
7. What ﬁeld equations do the new ﬁelds
satisfy?
These questions have all been answered
[17]. The number of new ﬁelds required
is exactly the number of generators of the
Lie group (i.e., its dimension). Each ﬁeld
is a four-component ﬁeld. Their dynamical
equations are a consequence of this theory.
All new ﬁelds are massless.
This theory has been applied to describe
the electroweak interaction U(2) ≃U(1) ×
SU(2) to predict the massless electromag-
netic ﬁeld and three boson ﬁelds called
W ±, Z0 that transmit the weak interaction.
This theory was also applied to describe
three quarks. The Lie group used was SU(3)
and the theory predicted the existence of
eight (that is the dimension of the Lie group
SU(3)) gluon ﬁelds, all massless. The gluon
ﬁelds transmit the strong interaction. In the
case of the gluons, the mass seems to be
small enough to be consistent with “zero”
but that is deﬁnitely not the case of the very
massive weak gauge bosons W ±, Z0. A new
mechanism was called for, and proposed,
to describe how these “massless” particles
acquire such a heavy mass. This mechanism
was proposed by Higgs, among others, and
is called the Higgs mechanism. The discov-
ery of the Higgs boson was announced in
2012.

202
5 Group Theory
5.14
Group Theory and Special Functions
5.14.1
Summary of Some Properties
The classical special functions of math-
ematical physics were developed in the
nineteenth century in response to a variety
of speciﬁc physical problems. They include
the Legendre and associated Legendre
functions, the Laguerre and associated
Laguerre
functions,
the
Gegenbauer,
Chebyshev, Hermite, and Bessel functions.
They are for the most part orthogonal
polynomials. They are constructed by
choosing a basis set f0, f1, f2, … that are
monomials in the position representation
(Dirac notation): ⟨x|f0⟩= x0, ⟨x|f1⟩= x1,
⟨x|f2⟩= x2, … and then creating an orthog-
onal set by successive Gram–Schmidt
orthogonalization by means of an inner
product ⟨f |g⟩= ∫b
a f ∗(x)g(x)w(x)dx with
various weights w(x) for the diﬀerent
functions:
|𝜙0⟩= |f0⟩,
|𝜙1⟩= |f1⟩−|𝜙0⟩⟨𝜙0|
⟨𝜙0|𝜙0⟩|f0⟩,
|𝜙j⟩= |fj⟩−
j−1
∑
k=0
|𝜙k⟩⟨𝜙k|
⟨𝜙k|𝜙k⟩|fj⟩.
(5.129)
The Bessel functions are the exception to
this rule, as they are not polynomials.
These functions obey a common variety
of properties
Diﬀerential equation
g2(x)y′′ + g1(x)y′ + g0(x)y = 0.
(5.130a)
Recurrence relations
a1nfn+1(x)=(a2n+a3nx)fn(x)−a4nfn−1(x).
(5.130b)
Diﬀerential relations
g2(x)dfn(x)
dx
= g1(x)fn(x) + g0(x)fn−1(x).
(5.130c)
Generating functions
g(x, z) =
∞
∑
n=0
anfn(x)zn.
(5.130d)
Rodrigues’ formula
fn(x) =
1
an𝜌(x)
dn
dxn
{𝜌(x)(g(x))n} .
(5.130e)
The
coeﬃcients
and
functions
can
be found in standard tabulations (e.g.,
Abramowitz and Stegun [18]). The Bessel
functions have similar properties.
5.14.2
Relation with Lie Groups
A Lie group lives on a manifold n
of dimension n. Each group element is
a function of position in the manifold:
g = g(x), x ∈.
The
product
of
two
group elements is deﬁned by an analytic
composition law on the manifold:
g(x) ∘g(y) = g(z),
z = z(x, y).
(5.131)
It is not until we construct representa-
tions for the group, or on top of the man-
ifold, that really interesting things begin to
happen. Representations
g(x) →Γ𝛼
ij(g(x))
(5.132)
are functions deﬁned on the manifold.
Suitably normalized, the set of matrix
elements for the complete set of UIR form
a complete orthonormal set of functions
on the manifold n. By duality (the mir-
acles of Hilbert space theory), the triplet
of indices 𝛼, i, j is described by as many
integers as the dimension of n. For
example, for three-dimensional Lie groups,
such as SO(3), SU(2), SO(2, 1), ISO(2), H3
the matrix elements are indexed by three
integers and can be represented in the

5.14 Group Theory and Special Functions
203
form Γ𝛼
ij(g(x)) = ⟨𝛼
i |g(x)| 𝛼
j ⟩. Including the
appropriate normalization factor, they can
be expressed as
√
dim(𝛼)
Vol(G) Γ𝛼
ij(g(x)) = ⟨g(x)| 𝛼
i, j ⟩. (5.133)
For noncompact groups Vol(G) is not ﬁnite,
but dim(𝛼) is also not ﬁnite, so the ratio
under the radical needs to be taken with
care.
Representations are powerful because
they lie in two worlds: geometric and
algebraic. They have one foot in the man-
ifold (⟨g(x)| ≃⟨x| above) and the other in
algebra ( | 𝛼
ij ⟩≃|n⟩above).
All classical special functions are speciﬁc
matrix elements, evaluated on speciﬁc sub-
manifolds, of speciﬁc irreducible represen-
tations of some Lie group.
We illustrate these ideas with a few
examples without pretending we have
even scratched the surface of this vast
and fascinating ﬁeld [19–21]. See also
Chapter 7.
5.14.3
Spherical Harmonics and SO(3)
For the group SU(2), the underlying man-
ifold is a solid three-dimensional sphere.
There are many ways to parameterize an
element in this group. We use an Euler-
angle-like parameterization introduced by
Wigner:
j
mk(𝜙, 𝜃, 𝜓) =
⟨
j
m
|||||
e−i𝜙Jze−i𝜃Jye−i𝜓Jz
|||||
j
k
⟩
= e−im𝜙dj
mk(𝜃)e−ik𝜓.
(5.134)
The orthogonality properties of the matrix
elements are
∫
2𝜋
0
d𝜙∫
𝜋
0
sin 𝜃d𝜃
∫
2𝜋
0
d𝜓j′∗
m′k′(𝜙, 𝜃, 𝜓)j
mk(𝜙, 𝜃, 𝜓)
=
8𝜋2
2j + 1𝛿j′j𝛿m′m𝛿k′k.
(5.135)
The volume of the group in this param-
eterization
is
8𝜋2 = (2𝜋)(2)(2𝜋) =
(∫2𝜋
0
d𝜙)(∫𝜋
0 sin 𝜃d𝜃)(∫2𝜋
0
d𝜓).
The
nor-
malization factor, converting the matrix
elements to a complete orthonormal set, is
√
(2j + 1)∕8𝜋2.
In order to ﬁnd a complete set of func-
tions on the sphere (𝜃, 𝜙), we search for
those matrix elements above that are inde-
pendent of the angle 𝜓. These only occur for
k = 0, which occurs only among the sub-
set of irreducible representations with j =
l (integer). Integrating out the d𝜓depen-
dence in (5.134) leads to a deﬁnition of
the spherical harmonics in terms of some
Wigner matrix elements (cf. (5.135):
Y l
m(𝜃, 𝜙) =
√
2l + 1
4𝜋
l∗
m0(𝜙, 𝜃, −). (5.136)
These functions on the two-dimensional
unit sphere surface (𝜃, 𝜙) inherit their
orthogonality and completeness proper-
ties from the corresponding properties
of the UIR matrix elements j
mk
on
the
three-dimensional
solid
ball
of
radius 2𝜋.
Other special functions are similarly
related to these matrix elements. The
associated Legendre polynomials are
Pm
l (cos 𝜃) =
√
(l + m)!
(l −m)! dl
0,0(𝜃)
(5.137)
and the Legendre polynomials are
Pl(cos 𝜃) = l
0,0(−, 𝜃, −) = dl
0,0(𝜃). (5.138)

204
5 Group Theory
These functions inherit the their measure
w(𝜃) from the measure on SU(2) and their
orthogonality and completeness properties
from those of the Wigner rotation matrix
elements j
mk [SU(2)].
We emphasize again that these functions
are speciﬁc matrix elements j
mk, evaluated
on speciﬁc submanifolds (sphere, line), of
speciﬁc irreducible representations (j = l)
of SU(2).
5.14.4
Diﬀerential and Recursion Relations
We can understand the wide variety of rela-
tions that exist among the special functions
(e.g., recursion relations, etc.) in terms of
group theory/representation theory as fol-
lows. It is possible to compute the matrix
elements of an operator in either the
continuous basis ⟨x′||x⟩or the discrete
basis ⟨n′||n⟩. In the ﬁrst basis, the coor-
dinates x describe points in a submanifold
in the group manifold n, and the oper-
ator is a diﬀerential operator. In the sec-
ond basis, the indices n are an appropriate
subset of the group representation 𝛼and
row/column (i, j) index set and the operator
is a matrix with entries in the real or com-
plex ﬁeld.
It is also possible to compute the matrix
elements in a mixed basis ⟨x||n⟩. It is in
this basis that really exciting things happen,
for
⟨x||n⟩
↙
↘
⟨x||x′⟩⟨x′|n⟩= ⟨x|n′⟩⟨n′||n⟩.
(5.139)
On the left-hand side a diﬀerential oper-
ator ⟨x||x′⟩acts on the special function
⟨x′|n⟩, while on the right-hand side, a
matrix ⟨n′||n⟩multiplies the special
functions ⟨x|n′⟩.
For the rotation group acting on the
sphere surface (𝜃, 𝜙) and the choice = L±,
we ﬁnd for ⟨𝜃𝜙|L±|
l
m ⟩computed as on
the left in (5.139),
e±i𝜙
(
± 𝜕
𝜕𝜃+ icos 𝜃
sin 𝜃
𝜕
𝜕𝜙
)
𝛿(cos 𝜃′ −cos 𝜃)𝛿(𝜙′ −𝜙)Y l
m(𝜃′, 𝜙′)
= e±i𝜙
(
± 𝜕
𝜕𝜃+ icos 𝜃
sin 𝜃
𝜕
𝜕𝜙
)
Y l
m(𝜃, 𝜙)
(5.140)
and as computed on the right
⟨
𝜃𝜙
|||||
l′
m′
⟩⟨
l′
m′
|||||
L±
|||||
l
m
⟩
= Y l
m±1(𝜃, 𝜙)
√
(l ± m+1)(l∓m). (5.141)
There are a number of Lie groups that
can be deﬁned to act on a one-dimensional
space. In such cases, the inﬁnitesimal
generators take the form of functions of
the coordinate x and the derivative d∕dx.
We illustrate the ideas behind diﬀerential
and recursion relations in the context of
the Heisenberg group H3. Its algebra 𝔥3
is spanned by three operators, universally
identiﬁed as a, a†, I with commutation
relations
[a, a†] = I, [a, I] = [a†, I] = 0.
These operators have matrix elements as
follows in the continuous basis (geometric)
representation:
⟨x′|a|x⟩= 𝛿(x′ −x)
1
√
2
(x + D)
⟨x′|a†|x⟩= 𝛿(x′ −x)
1
√
2
(x −D)
(5.142)
⟨x′|I|x⟩= 𝛿(x′ −x)
and discrete basis (algebraic) representa-
tion:
⟨n′|a|n⟩= 𝛿n′,n−1
√
n
⟨n′|a†|n⟩= 𝛿n′,n+1
√
n′
(5.143)
⟨n′|I|n⟩= 𝛿n′,n.

5.14 Group Theory and Special Functions
205
Here D = d∕dx.
The special functions are the mixed basis
matrix elements ⟨x|n⟩. We can compute
these starting with the ground, or lowest,
state |0⟩.
⟨x|a|0⟩
↙
↘
⟨x|a|x′⟩⟨x′|0⟩
=
⟨x|n⟩⟨n|a|0⟩
1
√
2(x + D)⟨x|0⟩
=
0.
(5.144)
This
equation
has
a
unique
solution
N⟨x|0⟩= e−x2∕2
up
to
scale
factor,
N = 1∕4√
𝜋.
The remaining normalized basis states
are constructed by applying the raising
operator:
⟨x|n⟩= ⟨x|(a†)n
n! |x′⟩⟨x′|0⟩
=
(x −D)n
√
2nn!
√
𝜋
e−x2∕2
= Hn(x)e−x2∕2
√
2nn!
√
𝜋
.
(5.145)
The Hermite polynomials in (5.145) are
deﬁned by
Hn(x) = e+x2∕2(x −D)ne−x2∕2.
(5.146)
The states ⟨x|n⟩are normalized to +1.
In order to construct the recursion rela-
tions for the Hermite polynomials, choose
= x = (a + a†)∕
√
2 in (5.139). Then
⟨x||n⟩= xHn(x)e−x2∕2
√
2nn!
√
𝜋
=
1
√
2
⟨x|n′⟩⟨n′|(a + a†)|n⟩. (5.147)
The two nonzero matrix elements on the
right are given in (5.143). They couple
xHn(x) on the left with Hn±1(x) on the
right. When the expression is cleaned up,
the standard recursion relation is obtained:
2x Hn(x) = Hn+1(x) + 2n Hn−1(x).
(5.148)
The diﬀerential relation is obtained in
the same way, replacing x = (a + a†)∕
√
2
by D = (a −a†)∕
√
2 in (5.147). On the left-
hand side, we ﬁnd the derivative of Hn(x)
as well as the derivative of e−x2∕2, and on
the right-hand side a linear combination of
Hn±1(x). When the expression is cleaned
up, there results the standard diﬀerential
relation
H
′
n(x) = 2n Hn−1(x).
(5.149)
5.14.5
Diﬀerential Equation
It happens often that an operator can be
formed that is quadratic in the basis vec-
tors of the Lie algebra and it also com-
mutes with every element in the Lie algebra.
Such operators can always be constructed
for semisimple Lie algebras where the Car-
tan metric gij (cf. (5.53)) is nonsingular. The
operator gijXiXj has this property. The con-
struction of nontrivial quadratic operators
with this property is even possible for many
Lie algebras that are not semisimple. When
it is possible, the left-hand side of (5.139) is
a second-order diﬀerential operator and the
right-hand side is a constant. This constant
is the eigenvalue in the diﬀerential equation
(ﬁrst property listed above).
For the three-dimensional nonsemisim-
ple group ISO(2) of length-preserving
translations and rotations of the plane to
itself, the three inﬁnitesimal generators
are L3, which generates rotations around
the z-axis, and T1, T2, which generate dis-
placements in the x and y directions. The
operators T1 and T2 commute. The opera-
tors L3, T± = T1 ± iT2 satisfy commutation

206
5 Group Theory
relations
[L3, T±] = ±T±
[T+, T−] = 0.
(5.150)
When acting on the plane, the three can
be expressed in terms of a radial (r) and
angular (𝜙) variable.
L3 = 1
i
𝜕
𝜕𝜙
T± = e±i𝜙
(
± 𝜕
𝜕r + i
r
𝜕
𝜕𝜙
)
.
(5.151)
Basis vectors |m⟩are introduced that satisfy
the condition
L3|m⟩= m|m⟩⇒⟨r𝜙|m⟩= gm(r)eim𝜙.
(5.152)
Single-valuedness requires m is an integer.
Adjacent basis vectors are deﬁned by
T±|m⟩= −|m ± 1⟩⇒
(
± d
dr −m
r
)
gm(r)
= −gm±1(r).
(5.153)
Finally, the identity T+T−|m⟩= |m⟩gives
Bessel’s equation
(
1
r
d
dr r d
dr + 1 −m2
r2
)
gm(r) = 0.
(5.154)
5.14.6
Addition Theorems
Addition theorems reﬂect the group com-
position property through the matrix mul-
tiplication property of representations
⟨n|g(x)g(y)|n′⟩
↙
↘
∑
k⟨n|g(x)|k⟩⟨k|g(y)|n′⟩=⟨n|g [z(x, y)] |n′⟩.
(5.155)
The special function at argument z is
expressed as a pairwise product of special
functions evaluated at the group elements
g(x) and g(y) for which x ∘y = z. The best
known of these addition results is
l
00(Θ) = l
0m(g−1
1 )l
m0(g2)
l
00(Θ) = l∗
m0(g1)l
m0(g2)
2l + 1
4𝜋
Pl(cos Θ) =
∑
m
Y l
m(𝜃1, 𝜙1)Y l∗
m (𝜃2, 𝜙2).
(5.156)
Here we have taken g1 = (𝜃1, 𝜙1, −) and
g2 = (𝜃2, 𝜙2, −), and Θ is the angle between
these two points on the sphere surface,
deﬁned by
cos Θ = cos 𝜃1 cos 𝜃2
+ sin 𝜃1 sin 𝜃2 cos(𝜙2 −𝜙1). (5.157)
5.14.7
Generating Functions
Generating functions are constructed by
computing the exponential of an operator
in the Lie algebra in two diﬀerent ways
and then equating the results. We illus-
trate this for H3 by computing ⟨x|e
√
2ta†|0⟩.
We ﬁrst compute the brute-strength Taylor
series expansion of the exponential:
⟨x|et(x−D)|x′⟩⟨x′|0⟩= e−x2∕2 1
4√
𝜋
∞
∑
n=0
tn Hn(x)
n!
.
(5.158)
Here ⟨x|0⟩= e−x2∕2∕4√
𝜋. Equation (5.146)
was used to obtain this result.
Next, we observe that exponentials of
diﬀerential operators are closely related
to Taylor series expansions, for instance
e−td∕dxf (x) = f (x −t). To exploit this, we
use the result of the disentangling theorem
(5.67) to write
et(x−D) = etxe−t2∕2e−tD.
(5.159)
Then
⟨x|et(x−D)|x′⟩⟨x′|0⟩= etxe−t2∕2e−tD⟨x|0⟩
=
1
4√
𝜋
etxe−t2∕2e−(x−t)2∕2.
(5.160)

5.15 Summary
207
By comparing the two calculations, (5.158)
with (5.160), we ﬁnd the standard generat-
ing function for the Hermite polynomials.
e2xt−t2 =
∞
∑
n=0
tnHn(x)
n!
.
(5.161)
5.15
Summary
The study of symmetry has had a profound
inﬂuence on the development of the natu-
ral sciences. Group theory has been used
in constructive ways before groups even
existed. We have given a ﬂavor of what can
be done with symmetry and related argu-
ments in Section 5.2, which describes three
types of arguments that live in the same
ballpark as group theory. Groups were for-
mally introduced in Section 5.3 and a num-
ber of examples given in the following three
sections, ranging from ﬁnite groups to Lie
groups. Lie algebras were introduced in
Sect. 5.7 and a number of their properties
discussed in that section. In Sect. 5.8, we
introduced the idea of a Riemannian sym-
metric space and showed the close connec-
tion between these spaces and Lie groups,
speciﬁcally that they are quotients (cosets)
of one Lie group by another, subject to
stringent conditions.
Transformation groups played a big role
in the development of classical physics –
mechanics and electrodynamics. In fact,
it was the need to formulate these two
theories so that their structure remained
unchanged under transformations from
the same group that led to the theory
of special relativity. The group at hand
was the inhomogeneous Lorentz group,
the 10-parameter Lie group of Lorentz
transformations and translations acting on
ﬁelds deﬁned over space–time. Section 5.9
describes how group theory played a role
in the development of special relativity.
The next step beyond requiring invariance
under the same Lorentz transformation at
every space–time point involved, allowing
the Lorentz transformation to vary from
point to point in a continuous way and still
requiring some kind of invariance (cf. gauge
theories as discussed in Section 5.13). This
extended aesthetic led to the theory of
general relativity.
Up to this point, physicists could have
done without all the particular intrica-
cies of group theory. The next step in
the growth of this subject was the inten-
sive study of the linear representations of
groups. The growth was indispensible when
quantum theory was developed, because
groups acted in Hilbert spaces through
their linear matrix representations. We
provided an overview of representation
theory in Section 5.10. At ﬁrst, groups
were applied in the quantum theory as
symmetry groups (cf. Section 5.11. In this
Capacity, they were used to label energy
eigenstates and describe the degeneracies
in energy levels that were required by
symmetry. Shortly afterward, they were
used in a more daring way to describe
nondegenerate levels related to each other
either by a broken symmetry or simply
by operators that had little to do with
symmetry but had the good sense to close
under commutation with each other. In
a very accurate sense, Mendeleev’s table
of the chemical elements and the table
of nuclear isotopes are manifestations
of broken symmetry applied to the con-
formal group SO(4, 2) that describes all
bound states of the hydrogen atom in a
single UIR, and the group U(3, 1) that
describes all bound states of the three-
dimensional harmonic oscillator in one
representation. These and other applica-
tions of dynamical groups are described in
Section 5.12.

208
5 Group Theory
Gauge theories were brieﬂy treated in
Section 5.13. In such theories, one begins
with a symmetry group and requires that a
Hamiltonian, Lagrangian, or action remain
“invariant” under the transformation when
the
parameters
of
the
transformation
group are allowed to be functions over
space–time. It is remarkable that this
requirement leads to the prediction of
new ﬁelds, the nature of the interaction of
the new ﬁelds with the original ﬁelds, the
structure of the equations of the new ﬁelds,
and the mass spectrum of these new ﬁelds:
all new masses are zero. This problem was
overcome by proposing that a new particle,
now called the Higgs boson, exists. Its
discovery was announced in 2012.
As a closing tribute to the theory of
groups and their linear matrix represen-
tations, we hint how the entire theory
of the special functions of mathematical
physics, which was created long before
the Lie groups were invented, is a study
of the properties of speciﬁc matrix ele-
ments of speciﬁc matrix representations of
particular Lie groups acting over special
submanifolds of the diﬀerentiable manifold
that parameterizes the Lie group. These
ideas are sketched by simple examples in
Section 5.14.
Group theory has progressed from the
outer fringes of theoretical physics in 1928,
when it was referred to as the Gruppenpest
(1928 Weyl to Dirac at Princeton), through
the mainstream of modern physics, to wind
up playing the central role in the develop-
ment of physical theory. Theoretical physi-
cists now believe that if a theory of fun-
damental interactions is not a gauge the-
ory it does not have the right to be consid-
ered a theory of interactions at all. Gauge
theory is the new version of “simple” and
“elegant.”
We learn that Nature was not tamed
until Adam was able to give names to all
the animals. Similarly, we cannot even give
names to particles and their states with-
out knowing at least a little bit about group
theory. Group theory has migrated from
the outer fringes of physics (Gruppenpest,
1928) to the central player, even the lingua
franca, of modern physics.
Glossary
Group: A group is a set {g0, g1, g2, …},
called group elements, together with a
combinatorial operation, ∘, called group
multiplication, with the property that four
axioms are obeyed: Closure, Associativity,
Existence of Identity, and Unique Inverse.
Lie Group: A Lie group is a group whose
elements g(x) are parameterized by points
x in an n-dimensional manifold. Group
multiplication corresponds to mappings of
pairs of points in the manifold to points in
the manifold: g(x) ∘g(y) = g(z), where z =
z(x, y).
Lie Algebra: A Lie algebra is a linear vec-
tor space on which one additional compo-
sition law, the commutator [, ], is deﬁned.
The commutator satisﬁes three conditions:
it preserves linear vector space properties,
it is antisymmetric, and the Jacobi identity
is satisﬁed.
Homomorphism: A homomorphism is a
mapping of one set with an algebraic struc-
ture (e.g., group, linear vector space, alge-
bra) onto another algebraic structure of the
same type that preserves all combinatorial
operations.
Isomorphism: An isomorphism is a homo-
morphism that is 1 : 1.
Structure
Constants
C k
ij :
These
are
expansion coeﬃcients for the commutator
of basis vectors in a Lie algebra. If Xi,
i = 1, 2, … , n are basis vectors for a Lie
algebra, then the commutator [Xi, Xj
] can

5.15 Summary
209
be expanded in terms of these basis vectors:
[Xi, Xj
] = ∑
k C k
ij Xk.
Cartan Metric: A metric tensor that can
be deﬁned on a Lie algebra by double cross
contraction on the indices of the struc-
ture constants: gij = ∑
kl C l
ik C k
jl . This met-
ric tensor has remarkable properties.
Symmetry Group: In quantum mechanics,
this is a group G that leaves the Hamil-
tonian of a physical system invariant:
GG−1 = . Through its matrix repre-
sentations, it maps states of energy Ei into
other, often linearly independent states,
with the same energy Ei.
Dynamical Group: In quantum mechan-
ics, this is a group H that leaves invariant
the time-dependent Schrödinger equation:
(−iℏ𝜕∕𝜕t) 𝜓(x, t) = 0.
Through
its
matrix representations, it maps states of
energy Ei into other, linearly independent
states with diﬀerent energies Ej.
Subgroup: A subgroup H of a group G
consists of a subset of group elements of
G that obey the group axioms under the
same group multiplication operation that is
deﬁned on G. For example C3 ⊂C3v. Both
the full group G and only the identity ele-
ment e are (improper) subgroups of G.
Coset C: If H is a subgroup of G, then every
group element in G can be written as a
product of an element in H and a group ele-
ment in G: gi = hjck. If nG and nH are the
orders of G and H, then gi ∈G, 1 ≤i ≤nG,
hj ∈H, 1 ≤j ≤nH and ck ∈C ⊂G, 1 ≤
k ≤nC, where nC = nG∕nH. The group ele-
ments Ck are called coset representatives;
the set {c1, c2, … , cnC
} is called a coset. A
coset is not generally a group. If H is an
invariant subgroup of G the coset represen-
tatives can be chosen so that C is a group.
Representation: A representation is a
homomorphism of a set with an algebraic
structure (e.g., group, linear vector space,
algebra) into matrices. If the mapping is an
isomorphism the representation is called
faithful or 1 : 1.
Class: This is a set of elements in a group
that are related to each other by a similar-
ity transformation: if group elements hi and
hj are related by hi = ghjg−1 for some g ∈
G, then they are in the same class. Group
elements in the same class are geometri-
cally similar. For example, in the group of
transformations that map the cube to itself,
there are eight rotations by 2𝜋∕3 radians
about axes through opposite corners. These
are geometrically similar and belong to the
same class.
Characters: In the theory of group repre-
sentations, characters are traces of the rep-
resentation matrices for the classes of the
group.
Gauge Theory: Schrödinger’s equation
is
unchanged
if
the
wavefunction
is
multiplied
by
a
phase
factor
that
is
constant
throughout
space
and
time.
If
the
wavefunction
has
two
com-
ponents,
Schrödinger’s
equation
is
unchanged if the wavefunction is mul-
tiplied (rotated) by a group element from
a 2 × 2 matrix from the group U(2) or
SU(2)
(2 →3, SU(2) →SU(3),
etc.).
If
the
rotation
depends
on
space–time
coordinates,
Schrödinger’s
equation
is
not
invariant.
Gauge
theory
attempts
to
show
how
Schrödinger’s
equation
can remain unchanged in form under
space–time-dependent
rotations.
This
requires the inclusion of N extra ﬁelds
to the physical problem, where N is the
dimension of the rotation group, U(2),
SU(2), SU(3), … These extra ﬁelds describe
particles (bosons) that govern the interac-
tions among the original ﬁelds described by
Schrödinger’s equation. For example, for an
original charged ﬁeld with one component,
invariance under space–time- dependent
phase factor in the group U(1) leads to
the prediction that zero-mass photons are

210
5 Group Theory
responsible for charge-charge interactions.
Gauge theories also predict the form of the
interaction between the original (fermion)
ﬁelds and the newly introduced (gauge
boson) ﬁelds.
References
1. Barenblatt, G. (2003) Scaling, Cambridge
University Press, Cambridge.
2. Landau, L.D. and Lifshitz, E.M. (1960)
Mechanics, Addison-Wesley, Reading, MA.
3. Gilmore, R. (1974) Lie Groups, Lie Alegras,
and Some of their Applications, John Wiley &
Sons, Inc. (reprinted in 2005 by Dover in
New York), New York.
4. Gilmore, R. (2008) Lie Groups, Physics, and
Geometry, An Introduction for Physicists,
Engineers, and Chemists, Cambridge
University Press, Cambridge.
5. Hamermesh, M. (1962) Group Theory and its
Application to Physical Problems,
Addison-Wesley, Reading, MA; reprint
(1989) Dover, New York.
6. Sternberg, S. (1994) Group Theory and
Physics, University Press, Cambridge.
7. Tinkham, M. (1964) Group Theory and
Quantum Mechanics, McGraw Hill,
New York.
8. Wigner, E.P. (1959) Group Theory, and its
Application to the Quantum Mechanics of
Atomic Spectra (ed. J.J. Griﬃn, translator),
Academic Press, New York.
9. Coxeter, H.S.M. and Moser, W.O.J. (1980)
Generators and Relations for Discrete
Groups, 4th edn, Springer-Verlag,
Berlin.
10. Helgason, S. (1962) Diﬀerential Geometry
and Symmetric spaces, Academic Press,
New York.
11. Weinberg, S. (1972) Gravitation and
Cosmology, Principles and Applications of
the General Theory of Relativity, John Wiley
& Sons, Inc., New York.
12. Barut, A.O. and Raczka, R. (1986) Theory of
Group Representations and Applications,
World Scientiﬁc, Singapore.
13. Haxel, O., Jensen, J.H.D., and Suess, H.E.
(1949), On the “Magic Numbers” in nuclear
structure. Phys. Rev., 75, 1766–1766.
14. Mayer, M.G. (1949) On closed shells in
Nuclei. II. Phys. Rev, 75, 1969–1970.
15. Arima, A. and Iachello, F. (1976) Interacting
Boson Model of collective states, Part I (the
vibrational limit). Ann. Phys. (New York), 99,
253–317.
16. Bethe, H.A. and Salpeter, E.E. (1957)
Quantum Mechanics of One- and
Two-Electron Atoms, Academic Press, New
York.
17. Utiyama, R. (1956) Invariant theoretical
interpretation of interaction. Phys. Rev., 101,
1597–1607.
18. Abramowitz, M. and Stegun, I.A. (1964)
Handbook of Mathematical Functions,
National Bureau of Standards (reprinted in
1972 by Dover in New York), Washington,
DC.
19. Miller, W. Jr. (1968) Lie Theory and Special
Functions, Academic Press, New York.
20. Talman, J.D. (1968) Special Functions: A
Group Theoretic Approach (Based on
Lectures by Eugene P. Wigner), Benjamin,
New York.
21. Vilenkin, N.Ja. (1968) Special Functions and
the Theory of Group Representations,
American Mathematical Society, Providence,
RI.

211
6
Algebraic Topology
Vanessa Robins
6.1
Introduction
Topology is the study of those aspects of
shape and structure that do not depend
on precise knowledge of an object’s geom-
etry. Accurate measurements are central
to physics, so physicists like to joke that
a topologist is someone who cannot tell
the diﬀerence between a coﬀee cup and a
doughnut. However, the qualitative nature
of topology and its ties to global analy-
sis mean that many results are relevant
to physical applications. One of the most
notable areas of overlap comes from the
study of dynamical systems. Some of the
earliest work in algebraic topology was by
Henri Poincaré in the 1890s, who pioneered
a qualitative approach to the study of celes-
tial mechanics by using topological results
to prove the existence of periodic orbits [1].
Topology continued to play an important
role in dynamical systems with signiﬁcant
results pertinent to both areas from Smale
in the 1960s [2]. More recently, in the 1990s,
computer analysis of chaotic dynamics was
one of the drivers for innovation in compu-
tational topology [3–6].
As with any established subject, there
are several branches to topology: General
topology deﬁnes the notion of “closeness”
(the neighborhood of a point), limits, conti-
nuity of functions and so on, in the absence
of a metric. These concepts are absolutely
fundamental to modern functional analy-
sis; a standard introductory reference is [7].
Algebraic topology derives algebraic objects
(typically groups) from topological spaces
to help determine when two spaces are
alike. It also allows us to compute quanti-
ties such as the number of pieces the space
has, and the number and type of “holes.”
Diﬀerential topology builds on the above
and on the diﬀerential geometry of mani-
folds to study the restrictions on functions
that arise as the result of the structure of
their domain. This chapter is primarily con-
cerned with algebraic topology; it covers
the elementary tools and concepts from
this ﬁeld. It draws on deﬁnitions and mate-
rial from Chapter 5 on group theory and
Chapters 9 and 10 on diﬀerential geometry.
A central question in topology is to
decide when two objects are the same
in some sense. In general topology, two
spaces, A and B, are considered to be
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

212
6 Algebraic Topology
the same if there is a homeomorphism, f ,
between them: f ∶A →B is a continu-
ous function with a continuous inverse.
This captures an intrinsic type of equiv-
alence that allows arbitrary stretching
and squeezing of a shape and permits
changes in the way an object sits in a
larger space (its embedding), but excludes
any cutting or gluing. So for example, a
circle (x2 + y2 = 1) is homeomorphic to
the perimeter of a square and to the tre-
foil knot, but not to a line segment, and
a sphere with a single point removed is
homeomorphic to the plane. One of the
ultimate goals in topology is to ﬁnd a set of
quantities (called invariants) that charac-
terize spaces up to homeomorphism. For
arbitrary topological spaces, this is known
to be impossible [1] but for closed, com-
pact 2-manifolds this problem is solved by
ﬁnding the Euler characteristic (see p. 222)
and orientability of the surface [8, 9].
What is the essential diﬀerence between
a line segment and a circle? Intuitively, it is
the ability to trace your ﬁnger round and
round the circle as many times as you like
without stopping or turning back. Alge-
braic topology is the mathematical machin-
ery that lets us quantify and detect this. The
idea behind algebraic topology is to map
topological spaces into groups (or other
algebraic structures) in such a way that
continuous functions between topological
spaces map to homomorphisms between
their associated groups.1)
In Sections 6.2–6.4, this chapter covers
the three basic constructions of alge-
braic topology: homotopy, homology, and
cohomology theories. Each has a diﬀer-
ent method for deﬁning a group from a
1) A homomorphism between two groups, 𝜙∶G →
H is a function that respects the group opera-
tion. That is, 𝜙(a ⋅b) = 𝜙(a) ∗𝜙(b), for a, b ∈G,
where ⋅is the group operation in G and ∗is the
group operation in H.
topological space, and although there are
close links between the three, they capture
diﬀerent qualities of a space. Many of the
more advanced topics in algebraic topology
involve studying functions on a space,
so we introduce the fundamental link
between critical points of a function and
the topology of its domain in Section 6.5
on Morse theory. The computability of
invariants, both analytically and numer-
ically, is vital to physical applications, so
the recent literature on computational
topology is reviewed in Section 6.6. Finally,
we give a brief guide to further reading on
applications of topology to physics.
6.2
Homotopy Theory
A homotopy equivalence is a weaker form
of equivalence between topological spaces
than a homeomorphism that allows us to
collapse a space onto a lower-dimensional
subset of itself (as we will explain in
Section 6.2.3), but it captures many essen-
tial aspects of shape and structure. When
applied to paths in a space, homotopy
equivalence allows us to deﬁne an algebraic
operation on loops, and provides our ﬁrst
bridge between topology and groups.
6.2.1
Homotopy of Paths
We begin by deﬁning a homotopy between
two continuous functions f , g ∶X →Y.
These maps will be homotopic if their
images f (X), g(X), can be continuously
morphed from one to the other within
Y, that is, there is a parameterized set of
images that starts with one and ends with
the other. Formally, this deformation is
achieved by deﬁning a continuous function

6.2 Homotopy Theory
213
F ∶X × [0, 1] →Y with F(x, 0) = f (x) and
F(x, 1) = g(x).
For example, consider two maps from
the
unit
circle
into
the
unit
sphere,
f , g ∶S1 →S2. We use the angle 𝜃∈[0, 2𝜋)
to parameterize S1 and the embedding
x2 + y2 + z2 = 1 in ℝ3 to deﬁne points in
S2. Deﬁne f (𝜃) = (cos 𝜃, sin 𝜃, 0) to be a
map from the circle to the equator and
g(𝜃) = (0, 0, 1), a constant map from the
circle to the north pole. A homotopy
between f
and g is given by F(𝜃, t) =
(cos(𝜋t∕2) cos 𝜃, cos(𝜋t∕2) sin 𝜃, sin(𝜋t∕2))
and illustrated in Figure 6.1. Any function
that is homotopic to a constant function,
as in this example, is called null homotopic.
When the domain is the unit interval,
X = [0, 1], and Y is an arbitrary topological
space, the continuous functions f and g are
referred to as paths in Y. A space in which
every pair of points may be joined by a path
is called path connected. It is often useful
to consider homotopies between paths that
ﬁx their end points, y0 and y1, say, so we
have the additional conditions on F that for
all t ∈[0, 1], F(0, t) = f (0) = g(0) = y0, and
F(1, t) = f (1) = g(1) = y1. If a path starts
and ends at the same point, y0 = y1, it is
called a loop. A loop that is homotopic to a
single point, that is, a null-homotopic loop
is also said to be contractible or trivial. A
path-connected space in which every loop
is contractible is called simply connected. So
the real line and the surface of the sphere
are simply connected, but the circle and the
surface of a doughnut (the torus) are not.
6.2.2
The Fundamental Group
We are now in a position to deﬁne our
ﬁrst algebraic object, the fundamental
group. The ﬁrst step is to choose a base
point y0 in the space Y and consider all
possible loops in Y that start and end at y0.
Two loops belong to the same equivalence
class if they are homotopic: given a loop
f ∶[0, 1] →Y
with f (0) = f (1) = y0, we
write [f ] to represent the set of all loops
that are homotopic to f . The appropriate
group operation [f ] ∗[g] on these equiv-
alence classes is a concatenation of loops
deﬁned by tracing each at twice the speed.
Speciﬁcally, choose f and g to be repre-
sentatives of their respective equivalence
classes and deﬁne f ∗g(x) = f (2x) when
x ∈[0, 1
2] and f ∗g(x) = g(2x −1) when
x ∈[ 1
2, 1]. As all loops have the same base
point, this product is another loop based at
y0. We then simply set [f ] ∗[g] = [f ∗g].
The equivalence class of the product [f ∗g]
is independent of the choice of f and
g because we can re-parameterize the
Figure 6.1
The function f ∶S1 →S2 that maps the circle onto the
equator of the sphere is homotopic to the function g ∶S1 →S2 that
maps the circle to the north pole. Three sections of the homotopy
F ∶S1 × [0, 1] →S2 are shown in gray.

214
6 Algebraic Topology
(c)
(b)
(a)
Figure 6.2
(a) Two nonhomotopic loops on the torus
with the same base point. (b) A loop homotopic to the
concatenation the loops depicted in (a). (c) Another loop
in the same homotopy class.
homotopies in the same way as we con-
catenated the loops. Note, though, that the
equivalence class [f ∗g] consists of more
than just concatenated loops; Figure 6.2
depicts an example on the torus.
The set of all homotopy-equivalence
classes of loops based at y0 with the oper-
ation ∗forms a group with the identity
element being the class of null-homotopic
loops [e] where e(x) = y0, and the inverse
of a loop deﬁned to be the same loop
traced
backwards:
[f ]−1 = [h]
where
h(x) = f (1 −x). This group is the funda-
mental group of Y with base point y0:
𝜋1(Y, y0).
The operation taking a topological space
to its fundamental group is an example of
a functor. This word expresses the property
alluded to in the introduction that con-
tinuous maps between topological spaces
transform to homomorphisms between
their associated groups. The functorial
nature of the fundamental group is man-
ifest in the following fashion. Suppose we
have a continuous function f ∶X →Y with
f (x0) = y0. Then given a loop in X with base
point x0, we can use simple composition of
the loop with f to obtain a loop in Y with
base point y0. Composition also respects
the concatenation of loops and homotopy
equivalences, so it induces a homomor-
phism between the fundamental groups:
𝜋1(f ) ∶𝜋1(X, x0) →𝜋1(Y, y0).
When
the
function f ∶X →Y is a homeomorphism,
it follows that the induced map 𝜋1(f ) is
an
isomorphism
of
their
fundamental
groups.
The following are some further elemen-
tary properties of the fundamental group:
• A simply connected space has a trivial
fundamental group, that is, only the
identity element.
• If Y is path connected, the fundamental
group is independent of the base point,
and we write 𝜋1(Y).
• The fundamental group respects
products2) of path-connected spaces:
𝜋1(X × Y) = 𝜋1(X) × 𝜋1(Y).
• The wedge product of two
path-connected spaces (obtained by
gluing the spaces together at a single
point) gives a free product3) on their
fundamental groups:
𝜋1(X ∨Y) = 𝜋1(X) ∗𝜋1(Y).
• The van Kampen theorem shows how to
compute the fundamental group of a
space X = U ∪V when U, V, and U ∩V
are open, path-connected subspaces of X
via a free product with amalgamation:
𝜋1(X) = 𝜋1(U) ∗𝜋1(V)∕N, where N is a
2) The (Cartesian or direct) product of two spaces
(or two groups) X × Y is deﬁned by ordered
pairs (x, y) where x ∈X and y ∈Y.
3) The free product of two groups G ∗H is an inﬁ-
nite group that contains both G and H as sub-
groups and whose elements are words of the
form g1h1g2h2. · · ·

6.2 Homotopy Theory
215
normal subgroup generated by elements
of the form iU(𝛾)iV(𝛾)−1 and 𝛾is a loop
in 𝜋1(U ∩V), iU and iV are inclusion-
induced maps from 𝜋1(U ∩V) to 𝜋1(U)
and 𝜋1(V) respectively. See [10] for
details.
6.2.3
Homotopy of Spaces
Now we look at what it means for two
spaces X and Y to be homotopy equiva-
lent or to have the same homotopy type:
there must be continuous functions f ∶
X →Y and g ∶Y →X such that fg ∶Y →
Y is homotopic to the identity on Y and gf ∶
X →X is homotopic to the identity on X.
We can show that the unit circle S1 and the
annulus A have the same homotopy type as
follows. Let
S1 = {(r, 𝜃) | r = 1, 𝜃∈[0, 2𝜋)}
and
A = {(r, 𝜃) | 1 ≤r ≤2, 𝜃∈[0, 2𝜋)}
be subsets of the plane parameterized by
polar coordinates. Deﬁne f ∶S1 →A to be
the inclusion map f (1, 𝜃) = (1, 𝜃) and let
g ∶A →S1 map all points with the same
angle to the corresponding point on the
unit circle: g(r, 𝜃) = (1, 𝜃). Then gf ∶S1 →
S1 is given by gf (1, 𝜃) = (1, 𝜃), which is
exactly the identity map. The other compo-
sition is fg ∶A →A is fg(r, 𝜃) = (1, 𝜃). This
is homotopic to the identity iA = (r, 𝜃) via
the homotopy F(r, 𝜃, t) = (1 + t(r −1), 𝜃).
This example is an illustration of a defor-
mation retraction: a homotopy equivalence
between a space (e.g., the annulus) to a sub-
set (the circle) that leaves the subset ﬁxed
throughout.
Spaces that are homotopy equivalent
have isomorphic fundamental groups. A
space that has the homotopy type of a
point is said to be contractible and has
trivial fundamental group. This is much
stronger than being simply connected: for
example, the sphere S2 is simply connected
because every loop can be shrunk to a
point, but it is not a contractible space.
6.2.4
Examples
Real space ℝm, m ≥1, all spheres Sn with
n ≥2, any Hilbert space, and any connected
tree (cf. Chapter 4) have trivial fundamental
groups.
The fundamental group of the circle is
isomorphic to the integers under addition:
𝜋1(S1) = ℤ. This can be seen by noting that
the homotopy class of a loop is determined
by how many times it wraps around the
circle. A formal proof of this result is quite
involved – see [10] for details. Any space
that is homotopy equivalent to the circle
will have the same fundamental group –
this holds for the annulus, the Möbius band,
a cylinder, and the “punctured plane” ℝ2 ⧵
(0, 0).
The projective plane ℝP2 is a nonori-
entable
surface
deﬁned
by
identifying
antipodal points on the boundary of the
unit disk (or equivalently, antipodal points
on the sphere). Its fundamental group is
isomorphic to ℤ2 (the group with two
elements, the identity and r which is its
own inverse r2 = id). To see this, consider
a loop that starts at the center of the unit
disk (0, 0), goes straight up to (0, 1) which
is identiﬁed with (0, −1) then continues
straight back up to the origin. This loop
is in a distinct homotopy class to the
null-homotopic loop but it is in the same
homotopy class as its inverse (to see this,
imagine ﬁxing the loop at (0, 0) and rotate
it by 180∘as illustrated in Figure 6.3).
The
fundamental
group
of
a
con-
nected graph with v vertices and e edges
(cf. Chapter 4) is a free group with n

216
6 Algebraic Topology
Figure 6.3
The projective plane, ℝP2 is modeled by the unit disk
with opposite points on the boundary identiﬁed. The black loop
starting at (0, 0) is homotopic to its inverse with the equivalence
suggested by the gray loops.
generators4) where n = e −(v −1) is the
number of edges in excess of a spanning
tree (the cyclomatic number). This demon-
strates that the fundamental group need
not be Abelian (products do not necessarily
commute).
The torus 𝕋= S1 × S1, so 𝜋1(𝕋) = ℤ×
ℤ. More generally, the fundamental group
of an orientable genus-g surface5) (g ≥2)
is isomorphic to a hyperbolic translation
group with 2g generators.
If a space has a ﬁnite cell structure, then
the fundamental group can be computed as
a free group with relations in an algorithmic
manner; this is discussed in Section 6.6.
6.2.5
Covering Spaces
The result about the fundamental group
of a genus-g surface comes from analyzing
the relationship between loops on a sur-
face and paths in its universal covering space
(the hyperbolic plane when g ≥2). Cover-
ing spaces are useful in many other con-
texts (from harmonic analysis to diﬀerential
topology to computer simulation), so we
4) A free group with one generator, a say, is the
inﬁnite cyclic group with elements … , a−1,
1, a, a2, … A free group with two generators a, b,
contains all elements of the form ai1bj1ai2bj2 · · ·
for integers ik, jl. A free group with n generators
is the natural generalization of this.
5) Starting with a sphere, you can obtain all closed
oriented 2-manifolds by attaching some number
of handles (cylinders) to the sphere. The number
of handles is the genus.
describe them brieﬂy here. They are simply
a more general formulation of the standard
procedure of identifying a real-valued peri-
odic function with a function on the circle.
Given a topological space X, a covering
space for X is a pair (C, p), where C is
another topological space and p ∶C →X is
a continuous function onto X. The covering
map p must satisfy the following condition:
for every point x ∈X, there is a neighbor-
hood U of x such that p−1(U) is a disjoint
union of open sets each of which is mapped
homeomorphically onto U by p. The dis-
crete set of points p−1(x) is called the ﬁber
of x. A universal covering space is one in
which C is simply connected. The reason
for the name comes from the fact that a uni-
versal covering of X will cover any other
connected covering of X. For example, the
circle is a covering space of itself with C =
S1 = {z ∈ℂ∶|z| = 1} and pk(z) = zk for
all nonzero integers k, while the univer-
sal cover of the circle is the real line with
p∶ℝ→S1 given by p(t) = exp(i2𝜋t).
The point z = (1, 0) ∈S1 is then covered
by the ﬁber p−1(z) = ℤ⊂ℝ. We illustrate a
covering of the torus in Figure 6.4.
When X and C are suitably nice spaces
(connected and locally path connected),
loops in X based at x0 lift to paths in C
between elements of the ﬁber of x0. So in
the example of S1, a path in ℝbetween two
integers i < j maps under pto a loop that
wraps j −i times around the circle.
Now consider homeomorphisms of the
covering space h ∶C →C that respect the

6.2 Homotopy Theory
217
Figure 6.4
The universal covering space of the
torus is the Euclidean plane projected onto the
closed surface by identifying opposite edges
of each rectangle with parallel orientations.
The ﬁber of the base point on the torus is a
lattice of points in the cover. The two loops
on the torus lift to the vertical and horizon-
tal paths shown in the cover. The lift of the
concatenation of these two loops is homo-
topic to the diagonal path in the cover (see
Figure 6.2c). The deck transformation group for
this cover is simply the group of translations
that preserve the rectangles; it is isomorphic to
ℤ× ℤ= 𝜋1(𝕋).
covering map, p(h(c)) = p(c). The set of
all such homeomorphisms forms a group
under composition called the deck trans-
formation group. When (C, p) is a universal
covering space for X, it is possible to show
that the deck transformation group must be
isomorphic to the fundamental group of X.
This gives a technique for determining the
fundamental group of a space in some situ-
ations; see [10] for details and examples.
6.2.6
Extensions and Applications
As we saw in the examples of Section 6.2.4,
the fundamental group of an n-dimensional
sphere is trivial for n ≥2, so the question
naturally arises how we might capture
the diﬀerent topological structures of Sn.
To generalize the fundamental group, we
examine maps from an n-dimensional
unit cube In into the space X where the
entire boundary of the cube is mapped
to a ﬁxed base point in X, that is,
f ∶In →X,
with
f (𝜕In) = x0.
Elements
of the higher homotopy groups 𝜋n(X, x0)
are then homotopy-equivalence classes
of these maps. The group operation is
concatenation in the ﬁrst coordinate just
as we deﬁned for one-dimensional closed
paths above. The main diﬀerence in higher
dimensions is that this operation now
commutes.
It is perhaps not too diﬃcult to see that
𝜋2(S2) = ℤ, although the multiple wrapping
of the sphere by a piece of paper cannot be
physically realized in ℝ3 in the same way a
piece of string wraps many times around a
circle. The surprise comes with the result
that 𝜋k(Sn) is nontrivial for most (but cer-
tainly not all) k ≥n ≥2, and in fact math-
ematicians have not yet determined all the
homotopy groups of spheres for arbitrary k
and n [10]. Higher-order homotopy groups
are a rich and fascinating set of topologi-
cal invariants that are the subject of active
research in mathematics.
One application of homotopy theory
arises in the study of topological defects
in condensed matter physics. A classic
example is that of nematic liquid crystals,
which are ﬂuids comprised of molecules
with an elongated ellipsoidal shape. The
order parameter for this system is the
(time-averaged) direction of the major axis
of the ellipsoidal molecule: n. For identical
and symmetrical molecules, the sign and
the magnitude of the vector is irrelevant,
and so the parameter space for n is the
surface of the sphere with antipodal points

218
6 Algebraic Topology
Figure 6.5
A cross section through a nematic ﬂuid with
a line defect that runs perpendicular to the page. Each
line segment represents the averaged direction of a single
molecule.
identiﬁed – topologically ℝP2. The exis-
tence of noncontractible loops in ℝP2 is
associated with the existence of topological
line defects in conﬁgurations of molecules
in the nematic liquid crystal; see Figure 6.5.
The fact that 𝜋1(ℝP2) = ℤ2 is manifest in
the fact that two defects of the same type
can smoothly cancel one another. The sec-
ond homotopy group is 𝜋2(ℝP2) = ℤ, and
this is manifest in the existence of point
defects (“hedgehogs”) where the director
ﬁeld points radially away from a central
point. See Mermin’s original article [11] or
[12] for further details.
6.3
Homology
The fundamental group is a useful invariant
but it captures only the one-dimensional
structure of equivalent loops in a space
and cannot distinguish between spheres of
dimensions greater than two, for example.
The higher homotopy groups do capture
this structure but are diﬃcult to com-
pute. The homology groups provide a
way to describe structure in all relevant
dimensions, but require a bit more machin-
ery to deﬁne. This can seem abstract at ﬁrst,
but in fact the methods are quite combi-
natorial and there has been much recent
activity devising eﬃcient algorithms to
compute homological quantities from large
data sets (see Section 6.6).
There are a number of diﬀerent for-
mulations of homology theory that give
essentially the same results for “nice”
spaces (such as diﬀerentiable manifolds).
The two key ingredients are a discrete cell
complex that captures the way a space is
put together, and a boundary map that
describes incidences between cells of adja-
cent dimensions. The algebraic structure
comes from deﬁning the addition and
subtraction of cells.
The
earliest
formulation
of
homol-
ogy theory is simplicial homology, based
on triangulations of topological spaces
called simplicial complexes. This theory
has
some
shortcomings
when
dealing
with very general topological spaces and
successive improvements over the past
century have culminated in the current
form based on singular homology and gen-
eral cell complexes. Hatcher [10] provides
an excellent introduction to homology
from this modern perspective. We focus
on simplicial homology here because it
is the most concrete and easy to adapt

6.3 Homology
219
for implementation on a computer. The
notation used in this section is based on
that of Munkres [13].
6.3.1
Simplicial Complexes
The
basic
building
block
is
an
ori-
ented k-simplex, 𝜎k, the convex hull of
k + 1 geometrically independent points,
{x0, x1, … , xk} ⊂ℝm,
with
k ≤m.
For
example, a 0-simplex is just a point, a
1-simplex is a line segment, a 2-simplex a
triangle, and a 3-simplex is a tetrahedron.
We write 𝜎k = ⟨x0, x1, … , xk⟩to denote a
k-simplex and its vertices. The ordering
of the vertices deﬁnes an orientation of
the simplex. This orientation is chosen
arbitrarily but is ﬁxed and coincides with
the usual notion of orientation of line seg-
ments, triangles, and tetrahedra. Any even
permutation of the vertices in a simplex
gives another simplex with the same ori-
entation, while an odd permutation gives a
simplex with negative orientation.
Given a set V, an abstract simpli-
cial complex, , is a collection of ﬁnite
subsets of V with the property that if
𝜎k = {v0, … , vk} ∈then all subsets of
𝜎k (its faces) are also in . If the simplicial
complex is ﬁnite, then it can always be
embedded in ℝm for some m (certain com-
plexes with inﬁnitely many simplices can
also be embedded in ﬁnite-dimensional
space). An embedded complex is called a
geometric realization of . The subset of
ℝm occupied by the geometric complex
is denoted by || and is called a polytope
or polyhedron. When a topological space
X is homeomorphic to a polytope, ||,
it is called a triangulated space and the
simplicial complex is a triangulation
of X. For example, a circle is homeomor-
phic to the boundary of a triangle, so the
three vertices a, b, c and three 1-simplices,
⟨ab⟩, ⟨bc⟩, ⟨ca⟩are a triangulation of the
circle (see Figure 6.6). All diﬀerentiable
manifolds
have
triangulations,
but
a
complete characterization of the class of
topological spaces that have a triangulation
is not known. Every topological 2- or 3-
manifold has a triangulation, but there is a
(nonsmooth) 4-manifold that cannot have
a triangulation (it is related to the Lie group
E8 [14]). The situation for nondiﬀerentiable
manifolds in higher dimensions remains
uncertain.
6.3.2
Simplicial Homology Groups
We now deﬁne the group structures asso-
ciated with a space X that is triangulated
by a ﬁnite simplicial complex . Although
the triangulation of a space is not unique,
the homology groups for any triangula-
tion of the same space are isomorphic (see
[13]); this makes simplicial homology well
deﬁned.
The set of all k-simplices from form the
basis of a free group called the kth chain
group, Ck(X). The group operation is an
additive one; recall that −𝜎k is just 𝜎k with
the opposite orientation, so this deﬁnes the
a
b
c
Figure 6.6
The simplicial complex of a triangle consists of one
2-simplex, three 1-simplices, and three vertices (0-simplices).

220
6 Algebraic Topology
inverse elements. In general, a k-chain is the
formal sum of a ﬁnite number of oriented
k-simplices: ck = ∑
i ai𝜎k
i . The coeﬃcients,
ai, are elements of another group, called the
coeﬃcient group that is typically the inte-
gers ℤ, but can be any Abelian group such
as the integers mod 2 ℤ2, the rationals ℚ, or
real numbers ℝ. If the coeﬃcient group G
needs to be emphasized, we write Ck(X; G).
When k ≥1, the boundary operator 𝜕k ∶
Ck →Ck−1 maps a k-simplex onto the sum
of the (k −1)-simplices in its boundary. If
𝜎k = ⟨x0, x1, … , xk⟩is a k-simplex, we have
𝜕k(𝜎k) =
k∑
i=0
(−1)i⟨x0, … , ̂xi, … , xk⟩,
where ⟨x0, … , ̂xi, … , xk⟩represents the
(k −1)-simplex obtained by deleting the
vertex xi. The action of the boundary
operator on general k-chains is obtained
by linear extension from its action on the
k-simplices: 𝜕k(∑
i ai𝜎k
i ) = ∑
i ai𝜕k(𝜎k
i ). For
k = 0, the boundary operator is deﬁned to
be null: 𝜕0(c0) = 0. We drop the subscript
from the boundary operator when the
dimension is understood.
As an example, consider the simplicial
complex consisting of a triangle and all its
edges and vertices, as shown in Figure 6.6.
The boundary of the 2-simplex ⟨a, b, c⟩is
𝜕(⟨a, b, c⟩) = ⟨b, c⟩−⟨a, c⟩+ ⟨a, b⟩,
and the boundary of this 1-chain is
𝜕(⟨b, c⟩−⟨a, c⟩+ ⟨a, b⟩)
= (c −b) −(c −a) + (b −a) = 0.
This illustrates the fundamental property of
the boundary operator, namely,
𝜕k 𝜕k+1 = 0.
We
now
consider
two
subgroups
of Ck
that have important geometric
interpretations. The ﬁrst subgroup consists
of k-chains that map to zero under the
boundary operator. This group is the group
of cycles denoted Zk, it is the kernel (or
null space) of 𝜕k and its elements are called
k-cycles. From the deﬁnition of 𝜕0 we see
that all 0-chains are cycles so Z0 = C0.
The second subgroup of Ck is the group of
k-chains that bound a (k + 1)-chain. This is
the group of boundaries Bk, it is the image
of 𝜕k+1. It follows from the above equation
that every boundary is a cycle, that is, the
image of 𝜕k+1 is mapped to zero by 𝜕k, so Bk
is a subgroup of Zk. In our example of the
triangle simplicial complex, we ﬁnd no 2-
cycles and a 1-cycle, ⟨b, c⟩−⟨a, c⟩+ ⟨a, b⟩,
such that all other 1-cycles are integer
multiples of this. This 1-cycle is also the
only 1-boundary, so Z1 = B1, and the
0-boundaries B0 are generated by the
two 0-chains: c −b and c −a (the third
boundary a −b = (c −b) −(c −a)).
As Bk ⊂Zk, we can form a quotient
group, Hk = Zk∕Bk; this is precisely the
kth homology group. The elements of Hk
are equivalence classes of k-cycles that do
not bound any k + 1 chain, so this is how
homology
characterizes
k-dimensional
holes. Formally, two k-cycles w, z ∈Zk are
in the same equivalence class if w −z ∈Bk;
such cycles are said to be homologous.
We write [z] ∈Hk for the equivalence
class of cycles homologous to z. For the
simple example of the triangle simplicial
complex, we have already seen that Z1 = B1
so that H1 = {0}. The 0-cycles are gener-
ated by {a, b, c} and the boundaries by
{(c −b), (c −a)}, so H0 has a single equiv-
alence class, [c], and is isomorphic to ℤ.
The following are homology groups for
some familiar spaces
• Real space ℝn has H0 = ℤand Hk = {0}
for k ≥1.

6.3 Homology
221
• The spheres have H0(Sn) = ℤ,
Hn(Sn) = ℤ, and Hk(Sn) = {0} for all
other values of k.
• The torus has H0 = ℤ, H1 = ℤ⊕ℤ,
H2 = ℤ, Hk = {0} for all other k. The
2-cycle that generates H2 is the entire
surface. This is in contrast to the second
homotopy group for the torus, which is
trivial.
• The real projective plane, ℝP2 has
H0 = ℤ, H1 = ℤ2, and Hk = {0} for
k ≥2. The fact that H2 is trivial is a
result of the surface being nonorientable;
even though ℝP2 is closed as a manifold,
the 2-chain covering the surface is not a
2-cycle.
The combinatorial nature of simplicial
homology makes it readily computable.
We give the classical algorithm and review
recent work on fast and eﬃcient algorithms
for data in Section 6.6.
6.3.3
Basic Properties of Homology Groups
In general, the homology groups of a ﬁnite
simplicial complex are ﬁnitely generated
Abelian groups, so the following theorem
tells us about their general structure (see
Theorem 4.3 of [13]).
Theorem 6.1 If G is a ﬁnitely generated
Abelian group then it is isomorphic to the
following direct sum:
G ≃(ℤ⊕· · · ⊕ℤ) ⊕ℤt1 ⊕· · · ⊕ℤtm.
The number of copies of the integer group
ℤis called the Betti number 𝛽. The cyclic
groups ℤti are called the torsion subgroups
and the ti are the torsion coeﬃcients; they
are deﬁned so that ti > 1 and t1 divides t2,
which divides t3, and so on. The torsion
coeﬃcients of the homology group Hk()
measure the twistedness of the space in
some sense. For example, the real projec-
tive plane has H1 = ℤ2, because the 2-chain
that represents the whole of the surface
has a boundary that is twice the generat-
ing 1-cycle. The Betti number 𝛽k of the
kth homology group counts the number of
nonequivalent nonbounding k-cycles and
this can be loosely interpreted as the num-
ber of k-dimensional holes. The 0th Betti
number, 𝛽0, counts the number of path-
connected components of the space.
Some other fundamental properties of
the homology groups are as follows:
• If the simplicial complex has N
connected components,
X = X1 ∪· · · ∪XN, then H0 is
isomorphic to the direct sum of N
copies of the coeﬃcient group, and
Hk(X) = Hk(X1) ⊕· · · ⊕Hk(XN).
• Homology is a functor. If f ∶X →Y is a
continuous function from one simplicial
complex into another, it induces natural
maps on the chain groups
f♯∶Ck(X) →Ck(Y) for each k, which
commute with the boundary operator:
𝜕f♯= f♯𝜕. This commutativity implies
that cycles map to cycles and boundaries
to boundaries, so that the f♯induce
homomorphisms on the homology
groups f∗∶Hk(X) →Hk(Y).
• If two spaces are homotopy equivalent,
they have isomorphic homology groups
(this is shown using the above functorial
property).
• The ﬁrst homology group is the
Abelianization of the fundamental
group. When X is a path-connected
space, the connection between H1(X)
and 𝜋1(X) is made by noticing that two
1-cycles are equivalent in homology if
their diﬀerence is the boundary of a
2-chain; if we parameterize the 1-cycles
as loops then this 2-chain forms a region

222
6 Algebraic Topology
through which one can deﬁne a
homotopy. See [10] for a formal proof.
• The higher-dimensional homology
groups have the comforting property
that if all simplices in a complex have
dimensions ≤m then Hk = {0} for
k > m. This is in stark contrast to the
higher-dimensional homotopy groups.
A particularly pleasing result in homol-
ogy relates the Betti numbers to another
topological invariant called the Euler char-
acteristic. For a ﬁnite simplicial complex, ,
deﬁne nk to be the number of simplices of
dimension k, then the Euler characteristic
is deﬁned to be 𝜒() = n0 −n1 + n2 + · · ·.
The Euler–Poincaré theorem states that
the alternating sum of Betti numbers
is the same as the Euler characteristic
[13]: 𝜒= 𝛽0 −𝛽1 + 𝛽2 + · · ·. This is one
of many results that connect the Euler
characteristic with other properties of
manifolds. For example, if M is a compact
2-manifold with a Riemannian metric, then
the Gauss–Bonnet theorem states that 2𝜋𝜒
is equal to the integral of Gaussian curva-
ture over the surface plus the integral of
geodesic curvature over the boundary of M
[15]. Further, if M is orientable and has no
boundary, then it must be homeomorphic
to a sphere with g handles and 𝜒= 2 −2g
where g is the genus of the surface. When
M is nonorientable without boundary, then
it is homeomorphic to a sphere with r
cross-caps and 𝜒= 2 −r.
The Euler characteristic is a topological
invariant with the property of inclusion–
exclusion: If a triangulated space X = A ∪B
where A and B are both subcomplexes,
then
𝜒(X) = 𝜒(A) + 𝜒(B) −𝜒(A ∩B).
This means the value of 𝜒is a localizable
one and can be computed by cutting up
a larger space into smaller chunks. This
property makes it a popular topological
invariant to use in applications [16]. A
recent application that exploits the local
additivity of the Euler characteristic to
great eﬀect is target enumeration in local-
ized sensor networks [17, 18]. The Euler
characteristic has also been shown to be
an important parameter in the physics of
porous materials [19, 20].
The simple inclusion–exclusion property
above does not hold for the Betti numbers
because they capture global aspects of the
topology of a space. Relating the homology
of two spaces to their union requires more
sophisticated algebraic machinery that we
review below.
6.3.4
Homological Algebra
Many results and tools in homology are
independent of the details about the
way the chains and boundary operators are
deﬁned for a topological space; they depend
only on the chain groups and the fact that
𝜕𝜕= 0. The study of such abstract chain
complexes and transformations between
them is called homological algebra and is
one of the original examples in category
theory [13].
An abstract chain complex is a sequence
of Abelian groups and homomorphisms
· · ·
dk+2
−→Ck+1
dk+1
−→Ck
dk
−→· · ·
d1
−→C0 −→{0},
with dkdk+1 = 0.
The homology of this chain complex is
Hk(C) = ker dk∕im dk+1. In certain cases
(such as the simplicial chain complex
of a contractible space), we ﬁnd that
im dk+1 = ker dk for k ≥1, so the homol-
ogy groups are trivial. Such a sequence is
said to be exact.

6.3 Homology
223
This property of exactness has many nice
consequences. For example, with a short
exact sequence of groups,
0 →A
f−→B
g
−→C →0.
The exactness means that f is a monomor-
phism (one-to-one) and g
is an epi-
morphism (onto). In fact, g induces an
isomorphism of groups C ≈B∕f (A), and
if these groups are ﬁnitely generated
Abelian, then the Betti numbers sat-
isfy 𝛽(B) = 𝛽(C) + 𝛽(A) (where we have
replaced f (A) by A because f is one-to-one).
Now imagine there is a short exact
sequence of chain complexes, that is,
0 →Ak
fk
−→Bk
gk
−→Ck →0.
is exact for all k and the maps fk, gk com-
mute with the boundary operators in each
complex (i.e., dBfk = fk−1dA, etc.). Typically,
the fk will be induced by an inclusion map
(and so be monomorphisms), and gk by
a quotient map (making them epimor-
phisms) on some underlying topological
spaces. The zigzag lemma shows that
these short exact sequences can be joined
together into a long exact sequence on the
homology groups of A, B, and C:
· · · →Hk(A)
f∗
−→Hk(B)
g∗
−→Hk(C)
Δ
−→
Hk−1(A)
f∗
−→Hk−1(B) →· · ·
The maps f∗and g∗are those induced by
the chain maps f and g and the boundary
maps Δ are deﬁned directly on the homol-
ogy classes in Hk(C) and Hk−1(A) by show-
ing that cycles in Ck map to cycles in Ak−1
via gk, the boundary 𝜕B, and fk−1. Details are
given in Hatcher [10].
One of the most useful applications of
this long exact sequence in homology is
the Mayer–Vietoris exact sequence, a result
that describes the relationship between the
homology groups of two simplicial com-
plexes, X, Y, their union, and their intersec-
tion. This gives us a way to deduce homol-
ogy groups of a larger space from smaller
spaces.
· · ·
jk
−→Hk(X) ⊕Hk(Y)
sk
−→Hk(X ∪Y)
vk
−→Hk−1(X ∩Y)
jk−1
−→Hk−1(X) ⊕Hk−1(Y)
sk−1
−→· · ·
(6.1)
The homomorphisms are deﬁned as fol-
lows:
jk([u]) = ([u], −[u])
sk([w], [w′]) = [w + w′]
vk([z]) = [𝜕z′],
Where, in the last equation, z is a cycle in
X ∪Y and we can write z = z′ + z′′ where
z′ and z′′ are chains (not necessarily cycles)
in X and Y respectively. These homomor-
phisms are well deﬁned (see, for example,
Theorem 33.1 of [13]). Exactness implies
that the image of each homomorphism is
equal to the kernel of the following one:
im jk = ker sk, im sk = ker vk, and im vk =
ker jk−1.
The Mayer–Vietoris sequence has an
interesting interpretation in terms of Betti
numbers. First we deﬁne Nk = ker jk to be
the subgroup of Hk(X ∩Y) deﬁned by the
k-cycles that bound in both X and in Y.
Then by focusing on the exact sequence
around Hk(X ∪Y), it follows that [21, 22]
𝛽k(X ∪Y) = 𝛽k(X) + 𝛽k(Y) −𝛽k(X ∩Y)
+ rank Nk + rank Nk−1.
This is where we see the nonlocalizable
property of homology and the Betti num-
bers most clearly.

224
6 Algebraic Topology
The Mayer–Vietoris sequence holds in a
more general setting than simplicial homol-
ogy. It is an example of a result that can
be derived from the Eilenberg–Steenrod
axioms. Any theory for which these ﬁve
axioms hold is a type of homology theory,
see [10] for further details.
6.3.5
Other Homology Theories
Chain complexes that capture topological
information may be deﬁned in a number
of ways. We have deﬁned simplicial chain
complexes above, and will brieﬂy describe
some other techniques here.
Cubical homology is directly analogous to
simplicial homology using k-dimensional
cubes as building elements rather than
k-dimensional simplices. This theory is
developed in full in [6] and arose from
applications in digital image analysis and
numerical analysis of dynamical systems.
Singular homology is built from singu-
lar k-simplices that are continuous func-
tions from the standard k-simplex into a
topological space X, 𝜎∶⟨v0, … , vk⟩→X.
Singular chains and the boundary opera-
tor are deﬁned as they are in simplicial
homology. A greater degree of ﬂexibility
is found in singular homology because the
maps 𝜎are allowed to collapse the sim-
plices, for example, the standard k-simplex,
k > 0, may have boundary points mapping
to the same point in X, or the entire simplex
may be mapped to a single point [10].
An even more general formulation of cel-
lular homology is made by considering gen-
eral cell complexes. A cell complex is built
incrementally by starting with a collection
of points X(0) ⊂X, then attaching 1-cells
via maps of the unit interval into X so
that end points map into X(0) to form the
1-skeleton X(1). This process continues by
attaching k-cells to the (k −1)-skeleton by
continuous maps of the closed unit k-ball,
𝜙∶Bk(0, 1) →X that are homeomorphic
on the interior and satisfy 𝜙∶𝜕Bk(0, 1) →
X(k−1). The deﬁnition of the boundary oper-
ator for a cell complex requires the con-
cept of degree of a map of the k-sphere (i.e.,
the boundary of a (k + 1)-dimensional ball).
For details see Hatcher [10].
We will see in the section on Morse
Theory that it is also possible to deﬁne a
chain complex from the critical points of a
smooth function on a manifold.
6.4
Cohomology
The cohomology groups are derived by a
simple dualization procedure on the chain
groups (similar to the construction of dual
function spaces in analysis). We will again
give deﬁnitions in the simplicial setting
but the concepts carry over to other con-
texts. A cochain 𝜙k is a function from the
simplicial chain group into the coeﬃcient
group, 𝜙k ∶Ck(X; G) →G (recall that G is
usually the integers, ℤ, but can be any
Abelian group). The space of all k-cochains
forms a group called the kth cochain group
Ck(X; G). The simplicial boundary oper-
ators 𝜕k ∶Ck →Ck−1 induce coboundary
operators 𝛿k−1 ∶Ck−1 →Ck on the cochain
groups via 𝛿(𝜙) = 𝜙𝜕. In other words, the
cochain 𝛿(𝜙) is deﬁned via the action of
𝜙on the boundary of each k-simplex 𝜎=
⟨x0, x1, … , xk⟩:
𝛿(𝜙)(𝜎) =
∑
i
(−1)i𝜙(⟨x0, … , ̂xi, … , xk⟩).
The key property from homology that
𝜕k𝜕k+1 = 0
also
holds
true
for
the
coboundary: 𝛿k𝛿k−1 = 0 (coboundaries are
mapped to zero), so we deﬁne the kth coho-
mology group as Hk(X) = ker 𝛿k∕im 𝛿k−1.
Note that cochains 𝜙∈ker 𝛿are functions

6.4 Cohomology
225
that vanish on the k-boundaries (not the
larger group of k-cycles), and a coboundary
𝜂k ∈im 𝛿is one that can be deﬁned via
the action of some cochain 𝜙k−1 on the
(k −1)-boundaries.
The coboundary operator acts in the
direction of increasing dimension and
this can be a more natural action in some
situations (such as de Rham cohomology
of diﬀerential forms discussed below) and
also has some interesting algebraic conse-
quences (it leads to the deﬁnition of the
cup product).
· · · ←−Ck+1
𝛿k
←−Ck ←−· · · ←−C0 ←−{0}.
This action of the coboundary makes coho-
mology contravariant (induced maps act in
the opposite direction) where homology is
covariant (induced maps act in the same
direction). If f ∶X →Y is a continuous
function between two topological spaces
then the group homomorphism induced
on the cohomology groups acts as f ∗∶
Hk(Y) →Hk(X).
In simplicial homology, the simplices
form bases for the chain groups, and we
can similarly use them as bases for the
cochain groups by deﬁning an elemen-
tary cochain ̇𝜎as the function that takes
the value one on 𝜎and zero on all other
simplices. For a ﬁnite simplicial complex,
it is possible to represent the boundary
operator 𝜕as a matrix with respect to the
bases of oriented k- and (k −1)-simplices.
If we then use the elementary cochains as
bases for the cochain groups, the matrix
representation for the coboundary oper-
ator is just the transpose of that for the
boundary operator. This shows that for
ﬁnite simplicial complexes, the functional
and geometric meanings of duality are the
same.
Another type of duality is that between
homology and cohomology groups on
compact
closed
oriented
manifolds
(i.e., without boundary). Poincaré dual-
ity
states
that
Hk(M) = Hm−k(M)
for
k ∈{0, … , m} where m is the dimension
of the manifold, M; see [10] for further
details.
Despite this close relationship between
homology
and
cohomology
on
mani-
folds, the cohomology groups have a
naturally deﬁned product combining two
cochains and this additional structure can
help distinguish between some spaces
that homology does not. We start with
𝜙∈Ck(X; G) and 𝜓∈Cl(X; G) where the
coeﬃcient group should now be a ring
R (i.e., R should have both addition and
multiplication operations; ℤ, ℤp, and ℚ
are rings.) The cup product is the cochain
𝜙⌣𝜓∈Ck+l(X; R) deﬁned by its action
on a (k + l)-simplex 𝜎= ⟨v0, … , vk+l⟩as
follows:
(𝜙⌣𝜓)(𝜎) = 𝜙(⟨v0, … , vk⟩)𝜓(⟨vk, … , vk+l⟩).
The relation between this product and the
coboundary is
𝛿(𝜙⌣𝜓) = 𝛿𝜙⌣𝜓+ (−1)k𝜙⌣𝛿𝜓.
From this, we see that the product of
two cocycles is another cocycle, and if
the product is between a cocycle and a
coboundary, then the result is a cobound-
ary. Thus, the cup product is a well-deﬁned
product on the cohomology groups that is
anticommutative: [𝜙] ⌣[𝜓] = (−1)kl[𝜓] ⌣
[𝜙] (provided the coeﬃcient ring, G, is
commutative). These rules for products of
cocycles should look suspiciously familiar
to those who have read Chapter 9. They
are similar to those for exterior products
of diﬀerential forms and this relationship
is formalized in the next section when we
deﬁne de Rham cohomology.

226
6 Algebraic Topology
6.4.1
De Rham Cohomology
One interpretation of cohomology that is
of particular interest in physics comes from
the study of diﬀerential forms on smooth
manifolds; cf. Chapter 9. Recall that a dif-
ferential form of degree k, 𝜔, deﬁnes for
each point p ∈M, an alternating multilin-
ear map on k copies of the tangent space to
M at p:
𝜔p ∶TpM × · · · × TpM →ℝ.
The set of all diﬀerential k-forms on a
manifold M is a vector space, Ωk(M), and
the exterior derivative is a linear opera-
tor that takes a k-form to a (k + 1)-form,
dk ∶Ωk(M) →Ωk+1(M)
as
deﬁned
in
Chapter 9.
The crucial property dd = 0 holds for
the exterior derivative. In this context, k-
forms in the image of d are called exact,
that is, 𝜔= d𝜎for some (k −1)-form 𝜎; and
those for which d𝜔= 0 are called closed.
We therefore have a cochain complex of
diﬀerential forms and can form quotient
groups of closed forms modulo the exact
forms to obtain the de Rham cohomology
groups:
Hk
dR(M, ℝ) = ker dk
im dk−1
.
The cup product in de Rham cohomology is
exactly the exterior (or wedge) product on
diﬀerential forms.
De Rham’s theorem states that the above
groups are isomorphic to those derived
via simplicial or singular cohomology [23].
And so we see that the topology of a man-
ifold has a direct inﬂuence on the proper-
ties of diﬀerential forms that have it as their
domain. For example, the Poincaré lemma
states that if M is a contractible open sub-
set of ℝn, then all smooth closed k-forms
on M are exact (the cohomology groups
are trivial). In the language of multivariable
calculus, this becomes Helmholtz’ theorem
that a vector ﬁeld, V, with curlV = 0 in a
simply connected open subset of ℝ3 can
be expressed as the gradient of a poten-
tial function: V = gradf in the appropriate
domain [24]. These considerations play a
key role in the study of electrodynamics via
Maxwell’s equations [25].
6.5
Morse Theory
We now turn to a primary topic in diﬀer-
ential topology: to examine the relation-
ship between the topology of a manifold
M and real-valued functions deﬁned on
M. The basic approach of Morse theory
is to use the level cuts of a function f ∶
M →ℝand study how these subsets Ma =
f −1(−∞, a] change as a is varied. For “nice”
functions the level cuts change their topol-
ogy in a well-deﬁned way only at the critical
points. This leads to a number of power-
ful theorems that relate the homology of a
manifold to the critical points of a function
deﬁned on it.
6.5.1
Basic Results
A
Morse
function
f ∶M →ℝ
is
a
smooth
real-valued
function
deﬁned
on a diﬀerentiable manifold M such that
each critical point of f is isolated and the
matrix of second derivatives (the Hessian)
is nondegenerate at each critical point. An
example is illustrated in Figure 6.7. The
details on how to deﬁne these derivatives
with respect to a coordinate chart on M
are given in Chapter 9. This may seem like
a restrictive class of functions but in fact
Morse functions are dense in the space

6.5 Morse Theory
227
(a)
(b)
Figure 6.7
Imagine a torus sitting with one
point in contact with a plane and tilted slightly
into the page as depicted. Deﬁne a Morse
function by mapping each point on the torus
to its height above the plane. This function has
four critical points: a minimum, two saddles,
and a maximum. (a) Five level cuts of the
height function showing how the topology of
a level cut changes when passing through a
critical point. (b) Gradient ﬂow lines between
the maximum and the two saddle points, and
from each saddle point to the minimum.
of all smooth functions, so any smooth
function can be smoothly perturbed to
obtain a Morse function [26]. Now sup-
pose x ∈M is a critical point of f , that is,
df (x) = 0. The index of this critical point is
the number of negative eigenvalues of the
Hessian matrix. Intuitively, this is the num-
ber of directions in which f is decreasing: a
minimum has index 0, and a maximum has
index m where m is the dimension of the
manifold M. Critical points of intermediate
index are called saddles because they have
some increasing and some decreasing
directions.
The two main results about level cuts Ma
of a Morse function f are the following:
• When [a, b] is an interval for which there
are no critical values of f (i.e., there is no
x ∈f −1([a, b]) for which df (x) = 0), then
Ma and Mb are homotopy equivalent.
• Let x be a nondegenerate critical point of
f with index i, let f (x) = c, and let 𝜖> 0
be such that f −1[c −𝜖, c + 𝜖] is compact
and contains no other critical points.
Then Mc+𝜖is homotopy equivalent to
Mc−𝜖with an i-cell attached.
(Recall that an i-cell is an i-dimensional
unit ball and the attaching map glues the
whole boundary of the i-cell continuously
into the prior space). The proofs of these
theorems rely on homotopies deﬁned via
the negative gradient ﬂow of f [26].
Gradient ﬂow lines are another key ingre-
dient of Morse theory and allow us to deﬁne
a chain complex and to compute the homol-
ogy of M. Each point x ∈M has a unique
ﬂow line or integral path 𝛾x ∶ℝ→M such
that
𝛾x(0) = x and 𝜕𝛾x(t)
𝜕t
= −∇f (𝛾x(t)).
Taking the limit as t →±∞, each ﬂow line
converges to a destination and an origin
critical point. The unstable manifold of a
critical point p with index i is the set of all
x ∈M that have p as their origin; this set
is homeomorphic to an open ball of dimen-
sion i. Correspondingly, the stable manifold
is the set of all x that have p as their des-
tination. For suitably “nice” functions, the
collection of unstable manifolds form a cell
complex for the manifold M [27].

228
6 Algebraic Topology
We can also deﬁne a more abstract chain
complex, sometimes referred to as the
Morse–Smale–Witten complex, to reﬂect
the history of its development. Let Ci be
the chain group derived from formal sums
of critical points of index i. A boundary
operator 𝜕∶Ci →Ci−1 is then deﬁned
by mapping p ∈Ci to a sum of critical
points ∑𝛼jqj ∈Ci−1 for which there is a
ﬂow line with p as its origin and q as its
destination. The coeﬃcients 𝛼j of the qj
in this boundary chain are the number of
geometrically distinct ﬂow lines that join p
and qj (one can either count mod 2 or keep
track of orientations in a suitable manner).
It requires some eﬀort to show that 𝜕𝜕= 0
in this setting; see [27] for details.6) Morse
homology is the homology computed via
this chain complex.
For ﬁnite-dimensional compact mani-
folds, Morse homology is isomorphic to
singular homology, and we obtain the
Morse inequalities relating numbers of
critical points of f ∶M →ℝto the Betti
numbers of M:
c0 ≥𝛽0
c1 −c0 ≥𝛽1 −𝛽0
c2 −c1 + c0 ≥𝛽2 −𝛽1 + 𝛽0
⋮
∑
0≤i≤m
(−1)m−ici =
∑
0≤i≤m
(−1)m−i𝛽i = 𝜒(M),
where ci is the number of critical points of
f of index i and 𝛽i is the ith Betti number
of M. Notice that the ﬁnal relationship is
6) In 2D, think of the ﬂow lines that join a single
maximum, minimum pair. In general, such a
region is bounded by ﬂow lines from the maxi-
mum to two saddles and from these saddles to
the minimum. The boundary of the maximum
contains these two saddles and their boundaries
contain the minimum in oppositely induced
orientations.
an equality; the alternating sum of numbers
of critical points is the same as the Euler
characteristic 𝜒(M). It also follows from the
above that ci ≥𝛽i for each i.
6.5.2
Extensions and Applications
Morse theory is primarily used as a pow-
erful tool to prove results in other settings.
For example, Morse obtained his results
in order to prove the existence of closed
geodesics on a Riemannian manifold [28];
most famously, Morse theory forms the
foundation of a proof due to Smale of the
higher-dimensional
Poincaré
conjecture
[29]. Morse theory has been extended
in many ways that relax conditions on
the manifold or the function being stud-
ied [30]. We mention a few of the main
generalizations here.
A Morse–Bott function is one for which
the critical points may now not be iso-
lated and instead form a critical set that
is a closed submanifold. At the very sim-
plest level for example, this lets us study the
height function of a torus sitting ﬂat on a
table because the circle of points touching
the table is critical [31].
The Conley index from dynamical sys-
tems is a generalization of Morse theory
to ﬂows in a more general class than those
generated by the gradient of a Morse func-
tion. For general ﬂows, invariant sets are
no longer single ﬁxed points but may be
periodic cycles or even fractal “strange
attractors.” In the Morse setting, the index
is simply the dimension of the unstable
manifold of the ﬁxed point, but for gen-
eral ﬂows a more subtle construction is
required. Conley’s insight was that an
isolated invariant set can be character-
ized by the ﬂow near the boundary of a
neighborhood of the set. The Conley index

6.5 Morse Theory
229
is then (roughly speaking) the homotopy
type of such a neighborhood relative to its
boundary. For details see [32–34].
Building on Conley’s work and the
Morse complex of critical points and
connecting orbits, Floer created an inﬁnite-
dimensional version of Morse homology
now called Floer homology [27]. This has
various formulations that have been used
to study problems in symplectic geometry
(the geometry of Hamiltonian dynam-
ical systems) and also the topology of
three-
and
four-dimensional
manifolds
[35].
There are a number of approaches adapt-
ing Morse theory to a discrete setting, of
increasing importance in geometric model-
ing, image and data analysis, and quantum
ﬁeld theory. The approach due to Forman is
summarized in the following section.
6.5.3
Forman’s Discrete Morse Theory
Discrete Morse theory is a combinatorial
analogue of Morse theory for functions
deﬁned on cell complexes. Discrete Morse
functions are not intended to be approxi-
mations to smooth Morse functions, but
the theory developed in [36, 37] keeps
much of the style and ﬂavor of the standard
results from smooth Morse theory.
In keeping with earlier parts of this
chapter, we will give deﬁnitions for a sim-
plicial complex , but the theory holds for
general cell complexes with little modiﬁca-
tion. First recall that a simplex 𝛼is a face of
another simplex 𝛽if 𝛼⊂𝛽, in which case, 𝛽
is called a coface of 𝛼. A function f ∶→ℝ
that assigns a real number to each simplex
in is a discrete Morse function if for every
𝛼(p) ∈, f takes a value less than or equal
to f (𝛼) on at most one coface of 𝛼and takes
a value greater than or equal to f (𝛼) on at
most one face of 𝛼. In other words,
#{𝛽(p+1) > 𝛼| f (𝛽) ≤f (𝛼)} ≤1,
and
#{𝛾(p−1) < 𝛼| f (𝛾) ≥f (𝛼)} ≤1,
where # denotes the number of elements in
the set. A simplex 𝛼(p) is critical if all cofaces
take strictly greater values and all faces are
strictly lower.
A cell 𝛼can fail to be critical in two pos-
sible ways. There can exist 𝛾< 𝛼such that
f (𝛾) ≥f (𝛼), or there can exist 𝛽> 𝛼such
that f (𝛽) ≤f (𝛼). Lemma 2.5 of [36] shows
that these two possibilities are exclusive:
they cannot be true simultaneously for a
given cell 𝛼. Thus each noncritical cell 𝛼
may be paired either with a noncritical cell
that is a coface of 𝛼, or with a noncritical
cell that is a face of 𝛼.
As noted by Forman (Section 3 of [37]), it
is usually simpler to work with pairings of
cells with faces than to construct a discrete
Morse function on a given complex. So we
deﬁne a discrete vector ﬁeld V as a collec-
tion of pairs (𝛼(p), 𝛽(p+1)) of cells 𝛼< 𝛽∈
such that each cell of is in at most one pair
of V. A discrete Morse function deﬁnes a
discrete vector ﬁeld by pairing 𝛼(p) < 𝛽(p+1)
whenever f (𝛽) ≤f (𝛼). The critical cells are
precisely those that do not appear in any
pair. Discrete vector ﬁelds that arise from
Morse functions are called gradient vector
ﬁelds. See Figure 6.8 for an example.
It is natural to consider the ﬂow associ-
ated with a vector ﬁeld and in the discrete
setting the analogy of a ﬂow line is a V-path.
A V-path is a sequence of cells:
𝛼(p)
0 , 𝛽(p+1)
0
, 𝛼(p)
1 , 𝛽(p+1)
1
, 𝛼(p)
2 , … , 𝛽(p+1)
r−1 , 𝛼(p)
r ,
where (𝛼i, 𝛽i) ∈V, 𝛽i > 𝛼i+1, and 𝛼i ≠𝛼i+1
for all i = 0, … , r −1. A V-path is a non-
trival closed V-path if 𝛼r = 𝛼0 for r > 1.

230
6 Algebraic Topology
a
b
c
d
e
f
g
h
i
h
i
e
e
e
f
g
Figure 6.8
A simplicial complex with
the topology of the torus (opposite
edges of the rectangle are identiﬁed
according to the vertex labels). The
arrows show how to pair simplices in
a gradient vector ﬁeld. A compatible
discrete Morse function has a critical
0-cell (a minimum) at a, two critical
1-cells (saddles) at edges ⟨b, h⟩
and ⟨d, f⟩, and a critical 2-cell (a
maximum) at ⟨e, i, g⟩.
Forman shows that a discrete vector ﬁeld is
the gradient vector ﬁeld of a discrete Morse
function if, and only if, there are no nontriv-
ial closed V-paths (Theorem 9.3 of [36]).
The four results about Morse functions
that we gave earlier all carry over into the
discrete setting: the homotopy equivalence
of level sets away from a critical point,
adding a critical i-cell is homotopy equiv-
alent to attaching an i-cell, the existence
of and homology of the Morse chain com-
plex, and the Morse inequalities. One of the
notable diﬀerences between the discrete
and continuous theories is that ﬂow lines
for a smooth Morse function on a mani-
fold are uniquely determined at each point,
whereas V-paths can merge and split.
6.6
Computational Topology
An
algorithmic
and
combinatorial
approach to topology has led to signiﬁcant
results in low-dimensional topology over
the past twenty years. There are two main
aspects to computational topology: ﬁrst,
research into methods for making topolog-
ical concepts algorithmic, culminating for
example, in the beginnings of an algorith-
mic classiﬁcation of (Haken) 3-manifolds
[38] (a result analogous to the classiﬁca-
tion of closed compact 2-manifolds by
Euler characteristic and orientability). And
second, the challenge to ﬁnd eﬃcient and
useful techniques for extracting topological
invariants from data; see [39] for example.
We start this section by describing simple
algorithms that demonstrate the com-
putability of the fundamental group and
homology groups of a simplicial complex,
and then survey some recent advances in
building cell complexes and computing
homology from data.
6.6.1
The Fundamental Group of a Simplicial
Complex
In Section 6.2, we saw that the fundamen-
tal group of a topological space could be
determined from unions and products of
smaller spaces or by using a covering space.
When the space has a triangulation (i.e., it is
homeomorphic to a polyhedron) there is a
more systematic and algorithmic approach
to ﬁnding the fundamental group as the
quotient of a free group by a set of relations
that we summarize below. See [40] for a
complete treatment of this edge-path group.
Let be a connected ﬁnite simplicial
complex. Any path in || is homotopic

6.6 Computational Topology
231
to one that follows only edges in , and
any homotopy between edge-paths can
be restricted to the 2-simplices of . This
means the fundamental group depends
only on the topology of the 2-skeleton of .
The algorithm for ﬁnding a presentation of
𝜋1() proceeds as follows.
First ﬁnd a spanning tree T ⊂(1) that is,
a connected, contractible subgraph of the
1-skeleton that contains every vertex of ;
see Figure 6.9 for an example. One algo-
rithm for doing this simply grows from an
initial vertex v (the root) by adding adjacent
(edge, vertex) pairs only if the other ver-
tex is not already in T. A nontrivial closed
edge-path in (a loop) must include edges
that are not in T and in fact every edge
in −T generates a distinct closed path
in (1). Speciﬁcally, for each edge ⟨xi, xj⟩∈
−T there is a closed path starting and
ending at the root v and lying wholly in
T except for the generating edge; we label
this closed path gij. Moreover, any closed
path based at v can be written as a con-
catenation of such generating paths where
inverses are simply followed in the oppo-
site direction: gji = g−1
ij . The gij are therefore
generators for a free group with coeﬃcients
in ℤ.
Next we use the 2-skeleton (2) to obtain
the homotopy equivalences of closed edge-
paths. Each triangle ⟨xi, xj, xk⟩∈deﬁnes
a relation in the group via gijgjkgki = id
(the identity) where we also set gij = id
if ⟨xi, xj⟩∈T. Let G(, T) be the ﬁnitely
presented group deﬁned by the above
generators and relations. Then it is possible
to show that we get isomorphic groups
for diﬀerent choices of T and that G(, T)
is isomorphic to the fundamental group
𝜋1(||) [40].
If has many vertices, then the presen-
tation of its fundamental group as G(, T)
may not be a very eﬃcient description.
It is possible to reduce the number of
generating edges and relations by using
any
connected
and
contractible
sub-
complex that contains all the vertices
(0) ⊂K ⊂. Generators for the edge-
path group are labeled by edges in −K,
and the homotopy relations are again
deﬁned by triangles in (2), but we can now
ignore all triangles in K. For the example
of the torus in Figure 6.9, we could take
K to be the eight triangles in the 2 × 2
lower left corner of the rectangular grid.
6.6.2
Smith Normal form for Homology
There is also a well-deﬁned algorithm for
computing the homology groups from a
simplicial complex . This algorithm is
based on ﬁnding the Smith normal form
(SNF) of a matrix representation of the
boundary operator as outlined below.
Recall that the oriented k-simplices form
a basis for the kth chain group, Ck. This
means it is possible to represent the bound-
ary operator, 𝜕k ∶Ck →Ck−1, by a (non-
square) matrix Ak with entries in {−1, 0, 1}.
The matrix Ak has mk columns and mk−1
rows where mk is the number of k-simplices
in . The entry aij is 1 if 𝜎i ∈Ck−1 is a
face of 𝜎j ∈Ck with consistent orientation,
−1 if 𝜎i appears in 𝜕𝜎j with opposite ori-
entation, and 0 if 𝜎i is not a face of 𝜎j.
Thus each column of Ak is a boundary
chain in Ck−1 with respect to a basis of
simplices.
The algorithm to reduce an integer
matrix to SNF uses row and column oper-
ations as in standard Gaussian elimination,
but at all stages the entries must remain
integers. The row and column operations
correspond to changing bases for Ck−1 and
Ck respectively and the resulting matrix

232
6 Algebraic Topology
a
b
c
d
e
f
g
h
i
h
i
e
e
e
f
g
Figure 6.9
A simplicial complex with
the topology of a torus (opposite
edges of the rectangle are identiﬁed
according to the vertex labels). A
spanning tree T with root vertex a
is shown in bold. Any closed path
that starts and ends at a can be
decomposed into a sum of loops that
lie in T except for a single edge.
has the form
Dk =
[Bk
0
0
0
]
, where Bk =
⎡
⎢
⎢⎣
b1
0
⋱
0
blk
⎤
⎥
⎥⎦
.
Bk is a square matrix with lk nonzero diago-
nal entries that satisfy bi ≥1 and b1 divides
b2, divides b3, and so on. For a full descrip-
tion of the basic algorithm see Munkres
[13].
The SNF matrices for 𝜕k+1 and 𝜕k give
a complete characterization of the kth
homology group Hk. The rank of the
boundary group Bk (im Ak+1) is the num-
ber of nonzero rows of Dk+1, that is, lk+1.
The rank of the cycle group Zk (ker Ak) is
the number of zero columns of Dk, that
is, mk −lk. The torsion coeﬃcients of Hk
are the diagonal entries bi of Dk+1 that are
greater than one. The kth Betti number is
therefore
𝛽k = rank(Zk) −rank(Bk) = mk −lk −lk+1.
Bases for Zk and Bk (and hence Hk) are
determined by the row and column oper-
ations used in the SNF reduction but the
cycles found in this way typically have poor
geometric properties.
There are two practical problems with
the algorithm for reducing a matrix to SNF
as it is described in Munkres [13]. First,
the time-cost of the algorithm is of a high
polynomial degree in the number of sim-
plices; second, the entries of the interme-
diate matrices can become extremely large
and create numerical problems, even when
the initial matrix and ﬁnal normal form
have small integer entries. When only the
Betti numbers are required, it is possible to
do better. In fact, if we construct the homol-
ogy groups over the rationals, rather than
the integers, then we need only apply Gaus-
sian elimination to diagonalize the bound-
ary operator matrices; doing this, however,
means we lose all information about the
torsion. Devising algorithms that overcome
these problems and are fast enough to be
eﬀective on large complexes is an area of
active research.
6.6.3
Persistent Homology
The
concept
of
persistent
homology
arose in the late 1990s from attempts to
extract meaningful topological informa-
tion from data [41–43]. To give a ﬁnite
set of points some interesting topolog-
ical structure requires the introduction
of a parameter to deﬁne which points
are connected. The key lesson learnt

6.6 Computational Topology
233
from examining data was that rather than
attempting to choose a single best param-
eter value, it is much more valuable to
investigate a range of parameter values
and describe how the topology changes
with this parameter. So persistent homol-
ogy tracks the topological properties of a
sequence of nested spaces called a ﬁltration
· · · ⊂a ⊂b ⊂· · · where a < b ∈is an
index parameter. In a continuous setting,
the nested spaces might be the level cuts
of a Morse function on a manifold, so that
is a real interval. In a discrete setting,
this becomes a sequence of subcomplexes
indexed by a ﬁnite set of integers. In either
case, as the ﬁltration grows, topological
features appear and may later disappear.
The persistent homology group, Hk(a, b)
measures the topological features from
a that are still present in b. Formally,
Hk(a, b) is the image of the map induced
on homology by the simple inclusion of
a into b. Algebraically, it is deﬁned by
considering cycles in a to be equivalent
with respect to the boundaries in b:
Hk(a, b) = Zk(a)∕(Bk(b) ∩Zk(a)).
Computationally,
persistent
homol-
ogy tracks the birth and death of every
equivalence class of cycle and provides a
complete picture of the topological struc-
ture present at all stages of the ﬁltration.
The initial algorithm for doing this, due
to Edelsbrunner et al. [43], is surprisingly
simple and rests on the observation that
if we build a cell complex by adding a
single cell at each step, then (because all
its faces must already be present) this cell
either creates a new cycle and is ﬂagged
as positive, or “ﬁlls in” a cycle that already
existed and is labeled negative. If 𝜎is a
negative (k + 1)-cell, its boundary 𝜕𝜎is a
k-cycle and its cells are already ﬂagged as
either positive or negative. The new cell
𝜎is then paired with the most recently
added (i.e., youngest) unpaired positive
cell in 𝜕𝜎. If there are no unpaired positive
cells available, we must grow 𝜕𝜎to succes-
sively larger homologous cycles until an
unpaired positive cell is found. By doing
this carefully, we can guarantee that 𝜎is
paired with the positive k-cell that created
the homology class of 𝜕𝜎. Determining
whether a cell is positive or negative a
priori is computationally nontrivial in
general, but there is a more recent version
of the persistence pairing algorithm due
to Zomorodian and Carlsson [44, 45] that
avoids doing this as a separate step, and
also ﬁnds a representative k-cycle for each
homology class.
The
result
of
computing
persistent
homology from a ﬁnite ﬁltration is a list of
pairs of simplices (𝜎(k), 𝜏(k+1)) that repre-
sent the birth and death of each homology
class in the ﬁltration. The persistence
interval for each homology class is then
given by the indices at which the creator
𝜎and destroyer 𝜏entered the ﬁltration.
Some nontrivial homology classes may be
present at the ﬁnal step of the ﬁltration,
these essential classes have an empty part-
ner and are assigned “inﬁnite” persistence.
There are a number of ways to represent
this persistence information graphically:
the two most popular techniques are the
barcode [46] and the persistence diagram
[43, 47]. The barcode has a horizontal axis
representing the ﬁltration index; for each
homology class a solid line spanning the
persistence interval is drawn in a stack
above the axis. The persistence diagram
plots the (birth, death) index pair for each
cycle. These points lie above the diagonal,
and points close to the diagonal are homol-
ogy classes that have low persistence. It
is possible to show that persistence dia-
grams are stable with respect to small
perturbations in the data. Speciﬁcally, if

234
6 Algebraic Topology
(a)
(b)
Figure 6.10
(a) Balls of radius a centered on 16 data
points. (b) The nerve of the cover by balls of radius a gives
the ˇCech complex. In this example, the complex consists of
points, edges, triangles, and a single tetrahedron (shaded
dark gray). As the radius of the balls increases, there are
more intersections between them and higher-dimensional
simplices are created.
the ﬁltration is deﬁned by the level cuts of a
Morse function on a manifold, then a small
perturbation to this function will produce
a persistence diagram that is close to that
of the original one [47].
6.6.4
Cell Complexes from Data
We now address how to build a cell com-
plex and a ﬁltration for use in persistent
homology computations. Naturally, the
techniques diﬀer depending on the type of
data being investigated; we discuss some
common scenarios below.
The ﬁrst construction is based on a
general technique from topology called
the nerve of a cover. Suppose we have
a collection of “good” sets (the sets and
their intersections should be contractible)
= {U1, … , UN} whose union ⋃Ui is
the space we are interested in. An abstract
simplicial complex () is deﬁned by
making each Ui a vertex and adding
a k-simplex whenever the intersection
Ui0 ∩· · · ∩Uik ≠∅.
The
nerve
lemma
states that () has the same homotopy
type as ⋃Ui [10].
If the data set, X, is not too large, and
the points are fairly evenly distributed
over the object they approximate, it makes
sense to choose the Ui to be balls of
radius a centered on each data point:
a = {B(xi, a), xi ∈X}. This is often called
the ˇCech complex; see Figure 6.10. If a < b,
we see that (a) ⊂(b), and we have
a ﬁltration of simplicial complexes that
captures the topology of the data as they
are inﬂated from isolated points (a = 0) to
ﬁlling all of space (a →∞).
A similar construction to the ˇCech com-
plex that is much simpler to compute is
the Vietoris–Rips or clique complex. Rather
than checking for higher-order intersec-
tions of balls, we build a 1-skeleton from
all pairwise intersections and then add a
k-simplex when all its edges are present.
This construction is not necessarily homo-
topy equivalent to the union of balls, but
is useful when the data set comes from a
high-dimensional space, perhaps with only
an approximate metric.

6.6 Computational Topology
235
(a)
(b)
(c)
Figure 6.11
(a) The Voronoi diagram of a data set with
16 points. (b) The corresponding Delaunay triangulation.
(c) The union of balls of radius a centered on the data
points and partitioned by the Voronoi cells. The corre-
sponding triangulation is almost the same as that shown
in Figure 6.10: instead of the tetrahedron there are just two
acute triangles.
A
drawback
of
the
ˇCech
and
Vietoris–Rips complexes is that many
unnecessary
high-dimensional
sim-
plices may be constructed. One way to
avoid this is to build the Delaunay tri-
angulation. There are many equivalent
deﬁnitions of this widely used geometric
data structure [48]. We start by deﬁning
the Voronoi partition of space for a data set
{x1, … , xN} ⊂ℝm, via the closed cells
V(xi) = {p such that d(p, xi) ≤d(p, xj)
for j ≠i}.
That is, the Voronoi cell of a data point
is the region of space closer to it than
to any other data point. The boundary
faces of Voronoi cells are pieces of the
(m −1)-dimensional bisecting hyperplanes
between pairs of data points. The Delau-
nay complex is the geometric dual to the
Voronoi complex: when k + 1 Voronoi cells
share an (m −k)-dimensional face there
is a k-simplex in the Delaunay complex
that spans the corresponding k + 1 data
points.7) See Figure 6.11 for an example
in the plane (m = 2). The geometry of the
Voronoi partition guarantees that there are
no simplices of dimension greater than m
in the Delaunay complex.
Now consider what happens when we
take the intersection of each Voronoi cell
with a ball centered on the data point,
B(xi, a). The Voronoi cells partition the
union of balls ⋃B(xi, a) and the geometric
dual is a subset of the Delaunay complex
that is commonly referred to as an alpha
complex or alpha shape (where alpha
refers to the radius of the ball [49, 50]).
By increasing the ball radius from zero to
some large enough value, we obtain a ﬁl-
tration of the Delaunay complex that starts
with the ﬁnite set of data points and ends
with the entire convex hull. The topology
and geometry of alpha complexes has been
used, for example, in characterizing the
7) This is true for points in general position.
Degenerate conﬁgurations of points occur in the
plane for example, when four Voronoi cells meet
at a point. In this case the Delaunay complex
may be assigned to contain either a 3-simplex,
a quadrilateral cell, or one of two choices of
triangle pairs.

236
6 Algebraic Topology
shape of and interactions between proteins
[51]. The Betti numbers of alpha shapes
are also a useful tool for characterizing
structural patterns of spatial data [52] such
as the distribution of galaxies in the cosmic
web [53].
When the data set is very large, a
dramatic reduction in the number of sim-
plices used to build a complex is achieved
by deﬁning landmarks and the witness
complex. This construction generalizes the
Voronoi and Delaunay method, so that only
a subset of data points (the landmarks) are
used as vertices for the complex, while
still maintaining topological accuracy. A
further advantage is that only the distances
between data points are required to deter-
mine whether to include a simplex in the
witness complex. See [54] for details and
[55] for an extensive review of applications
in data analysis.
Another important class of data is digi-
tal images which can be binary (voxels are
black or white), grayscale (voxels take a
range of discrete values), or colored (voxels
are assigned a multidimensional value). In
this setting, the structures of interest arise
from level cuts of functions deﬁned on a
regular grid. Morse theory is the natural
tool to apply here, although in this applica-
tion, the structures of interest are the level
cuts of the function, while the domain (a
rectangular box) is simple. There are a num-
ber of diﬀerent approaches to computing
homology from such data and this is an
area of active research. The works [6, 56, 57]
present solutions motivated by applications
in the physical sciences.
Further Reading
We give a brief precis of a few standard texts
on algebraic topology from mathematical
and physical perspectives.
Allen Hatcher’s Algebraic Topology [10] is
one of the most widely used texts in mathe-
matics courses today and has a strong geo-
metric emphasis. Munkres [13] is an older
text that remains popular. Spanier [40] is a
dense mathematical reference and has one
of the most complete treatments of the fun-
damentals of algebraic topology. A readable
introduction to Morse theory is given by
Matsumoto [26] and Forman’s [37] review
article is an excellent introduction to his
discrete Morse theory.
Textbooks written for physicists that
cover algebraic topology include Naka-
hara’s [12] comprehensive book Geometry,
Topology and Physics, Schwarz [58] Topol-
ogy for Physicists, and Naber [59] Topology,
Geometry and Gauge Theory. Each book
goes well beyond algebraic topology to
study
its
interactions
with
diﬀerential
geometry and functional analysis. A cele-
brated example of this is the Atiyah–Singer
index theorem, which relates the analytic
index of an elliptic diﬀerential operator
on a compact manifold to a topological
index of that manifold, a result that has
been useful in the theoretical physics of
fundamental particles.
References
1. Stillwell, J. (2010) Mathematics and Its
History, 3rd edn, Springer-Verlag, New York.
2. Smale, S. (1967) Diﬀerentiable dynamical
systems. Bull. Am. Math. Soc., 73, 747–817.
3. Mindlin, G., Solari, H., Natiello, M., Gilmore,
R., and Hou, X.J. (1991) Topological analysis
of chaotic time series data from the
Belousov-Zhabotinskii reaction. J. Nonlinear
Sci., 1, 147–173.
4. Muldoon, M., MacKay, R., Huke, J., and
Broomhead, D. (1993) Topology from a time
series. Physica D, 65, 1–16.
5. Robins, V., Meiss, J., and Bradley, E. (1998)
Computing connectedness: An exercise in
computational topology. Nonlinearity, 11,
913–922.

References
237
6. Kaczynski, T., Mischaikow, K., and Mrozek,
M. (2004) Computational Homology,
Springer-Verlag, New York.
7. Armstrong, M. (1983) Basic Topology,
Springer-Verlag, New York.
8. Seifert, H. and Threlfall, W. (1980) A
Textbook of Topology, Academic Press.
Published in German 1934. Translated by
M.A. Goldman.
9. Francis, G. and Weeks, J. (1999) Conway’s
ZIP Proof , The American Mathematical
Monthly, 393–399.
10. Hatcher, A. (2002) Algebraic Topology,
Cambridge University Press, Cambridge.
11. Mermin, N. (1979) The topological theory of
defects in ordered media. Rev. Mod. Phys.,
51, 591–648.
12. Nakahara, M. (2003) Geometry, Topology
and Physics, 2nd edn, Taylor and Francis.
13. Munkres, J.R. (1984) Elements of Algebraic
Topology, Addison-Wesley, Reading, MA.
14. Scorpan, A. (2005) The Wild World of
4-Manifolds, American Mathematical
Society, Providence, RI.
15. Hyde, S., Blum, Z., Landh, T., Lidin, S.,
Ninham, B., and Andersson, S. (1996) in The
Language of Shape: The Role of Curvature in
Condensed Matter (ed. K. Larsson), Elsevier,
New York.
16. Mecke, K. (1998) Integral geometry in
statistical physics. Int. J. Mod. Phys. B, 12(9),
861.
17. Baryshnikov, Y. and Ghrist, R. (2009) Target
enumeration via Euler characteristic
integrals. SIAM J. Appl. Math., 70(9),
825–844.
18. Curry, J., Ghrist, R., and Robinson, M. (2012)
Euler calculus and its applications to signals
and sensing. Proc. Symp. Appl. Math., 70,
75–146.
19. Arns, C., Knackstedt, M., and Mecke, K.
(2003) Reconstructing complex materials via
eﬀective grain shapes. Phys. Rev. Lett., 91,
215 506.
20. Scholz, C., Wirner, F., Götz, J., Rüe, U.,
Schröder-Turk, G., Mecke, K., and
Bechinger, C. (2012) Permeability of porous
materials determined from the Euler
characteristic. Phys. Rev. Lett., 109, 264 504.
21. Delﬁnado, C.J.A. and Edelsbrunner, H.
(1993) An incremental algorithm for Betti
numbers of simplicial complexes, in SCG ’93:
Proceedings of the 9th Annual Symposium on
Computational Geometry, ACM Press, New
York, pp. 232–239.
22. Alexandroﬀ, P. (1935) in Topologie (ed. H.
Hopf), Springer-Verlag, Berlin.
23. Bott, R. (1982) in Diﬀerential Forms in
Algebraic Topology (ed. L. Tu),
Springer-Verlag, New York.
24. Nash, C. (1983) in Topology and Geometry
for Physicists (ed. S. Sen), Academic Press,
London.
25. Gross, P. (2004) in Electromagnetic Theory
and Computation: A Topological Approach
(ed. P. Kotiuga), Cambridge University Press,
Cambridge.
26. Matsumoto, Y. (2002) An Introduction to
Morse Theory, AMS Bookstore.
27. Banyaga, A. and Hurtubise, D. (2004)
Lectures on Morse Homology, Kluwer
Academic Publishers, The Netherlands.
28. Morse, M. (1934) The Calculus of Variations
in the Large, Colloquium Publications, vol.
18, American Mathematical Society,
Providence, RI.
29. Smale, S. (1961) Generalized Poincaré’s
conjecture in dimensions greater than four.
Ann. Math., 74(2), 391–406.
30. Bott, R. (1988) Morse theory indomitable.
Publ. Math. de I.H.E.S., 68, 99–114.
31. Bott, R. (1954) Nondegenerate critical
manifolds. Ann. Math., 60(2), 248–261.
32. Conley, C.C. and Easton, R. (1971) Isolated
invariant sets and isolating blocks. Trans.
Am. Math. Soc., 158, 35–61.
33. Conley, C.C. (1978) Isolated Invariant Sets
and the Morse Index, AMS, Providence, RI.
34. Mischaikow, K. (2002) Conley index, in
Handbook of Dynamical Systems, vol. 2,
Chapter 9 (eds M. Mrozek and B. Fiedler),
Elsevier, New York, pp. 393–460.
35. McDuﬀ, D. (2005) Floer theory and low
dimensional topology. Bull. Am. Math. Soc.,
43, 25–42.
36. Forman, R. (1998) Morse theory for cell
complexes. Adv. Math., 134, 90–145.
37. Forman, R. (2002) A user’s guide to discrete
Morse theory. Séminaire Lotharingien de
Combinatoire, 48, B48c.
38. Matveev, S. (2003) Algorithmic Topology
and Classiﬁcation of 3-manifolds,
Springer-Verlag, Berlin.
39. Edelsbrunner, H. and Harer, J. (2010)
Computational Topology: An Introduction,
American Mathematical Society, Providence,
RI.

238
6 Algebraic Topology
40. Spanier, E. (1994) Algebraic Topology,
Springer, New York. First published 1966 by
McGraw-Hill.
41. Robins, V. (1999) Towards computing
homology from ﬁnite approximations. Topol.
Proc., 24, 503–532.
42. Frosini, P. and Landi, C. (1999) Size theory as
a topological tool for computer vision.
Pattern Recogn. Image Anal., 9, 596–603.
43. Edelsbrunner, H., Letscher, D., and
Zomorodian, A. (2002) Topological
persistence and simpliﬁcation. Discrete
Comput. Geom., 28, 511–533.
44. Carlsson, G. and Zomorodian, A. (2005)
Computing persistent homology. Discrete
Comput. Geom., 33, 249–274.
45. Zomorodian, A. (2009) Computational
topology, in Algorithms and Theory of
Computation Handbook, Special Topics and
Techniques, vol. 2, 2nd edn, Chapter 3, (eds
M. Atallah and M. Blanton), Chapman &
Hall/CRC Press, Boca Raton, FL.
46. Carlsson, G., Zomorodian, A., Collins, A.,
and Guibas, L. (2005) Persistence barcodes
for shapes. Int. J. Shape Modell., 11,
149–187.
47. Cohen-Steiner, D., Edelsbrunner, H., and
Harer, J. (2007) Stability of persistence
diagrams. Discrete Comput. Geom., 37,
103–120.
48. Okabe, A., Boots, B., Sugihara, K., and Chiu,
S. (2000) Spatial Tessellations: Concepts and
Applications of Voronoi Diagrams, 2nd edn,
John Wiley & Sons, Ltd, Chichester.
49. Edelsbrunner, H., Kirkpatrick, D., and Seidel,
R. (1983) On the shape of a set of points in
the plane. IEEE Trans. Inform. Theory, 29(4),
551–559.
50. Edelsbrunner, H. and Mücke, E. (1994)
Three-dimensional alpha shapes. ACM
Trans. Graphics, 13, 43–72.
51. Edelsbrunner, H. (2004) Biological
applications of computational topology, in
Handbook of Discrete and Computational
Geometry, Chapter 63 (eds J. Goodman and
J.O. Rourke), CRC Press, Boca Raton, FL, pp.
1395–1412.
52. Robins, V. (2006) Betti number signatures of
homogeneous Poisson point processes. Phys.
Rev. E, 74, 061107.
53. van de Weygaert, R., Vegter, G.,
Edelsbrunner, H., Jones, B.J.T., Pranav, P.,
Park, C., Hellwing, W.A., Eldering, B.,
Kruithof, N., Bos, E.G.P., Hidding, J.,
Feldbrugge, J., ten Have, E., van Engelen, M.,
Caroli, M., and Teillaud, M. (2011) Alpha,
Betti and the megaparsec universe: on the
topology of the cosmic web. Trans. Comput.
Sci., XIV, 60–101.
54. Carlsson, G. (2004) Topological estimation
using witness complexes, in Eurographics
Symposium on Point-Based Graphics (eds V.
de Silva, M. Alexa, and S. Rusinkiewicz),
ETH, Zürich, Switzerland.
55. Carlsson, G. (2009) Topology and data. Bull.
Am. Math. Soc., 46(2), 255–308.
56. Robins, V., Wood, P.J., and Sheppard, A.P.
(2011) Theory and algorithms for
constructing discrete Morse complexes from
grayscale digital images. IEEE Trans. Pattern
Anal. Mach. Intell., 33(8), 1646–1658.
57. Bendich, P., Edelsbrunner, H., and Kerber,
M. (2010) Computing robustness and
persistence for images. IEEE Trans. Visual.
Comput. Graphics, 16, 1251–1260.
58. Schwarz, A. (2002) Topology for Physicists,
Springer-Verlag, Berlin.
59. Naber, G. (2011) Topology, Geometry and
Gauge ﬁelds: Foundations, 2nd edn,
Springer-Verlag, Berlin.

239
7
Special Functions
Chris Athorne
7.1
Introduction
What is so special about a special function?
Within the universe of functions, a zoo
is set aside for classes of functions that are
ubiquitous in applications, particularly well
studied, subject to a general algebraic the-
ory, or judged to be important in some
other manner. There is no deﬁnition of the
collection comprising such species beyond
writing down a long and never complete list
of the specimens in it.
There have been perhaps three attempts
at constructing large theories of special
functions each of which has been very sat-
isfactory but none entirely comprehensive.
The most fundamental is that of Liou-
ville, later developed by Ritt, in which
functions are built from the ground up
by extending given function ﬁelds via
diﬀerential equations deﬁned over those
ﬁelds. Thus, if we allow ourselves to start
with rational functions in x over a ﬁeld of
constants, say ℂ, we can extend this ﬁeld by
solutions to ordinary diﬀerential equations
(ODEs) such as
y′(x) + a(x) = 0
and
y′(x) + a(x)y(x) = 0,
a(x) being a rational function. Most simply,
we may introduce the “elementary" func-
tions log and exp by choosing a(x) = 1∕x
and a(x) = 1, respectively. The process is
then repeated utilizing the newly created
functions in place of the rationals. Even
then we may not regard all such functions
as special but only those arising in the most
natural manner.
A second way is via the theory of
transformation groups due to Lie. Here
a function inherits particular properties
owing to its being a solution of a diﬀeren-
tial equation (with boundary conditions)
invariant under some underlying geometri-
cal symmetry. The special functions retain
a memory of this symmetry by arranging
themselves in a highly connected (and
beautiful)
manner
into
representations
of the symmetry group. For instance, the
rotational symmetries of the plane and
of Euclidean three-space are responsible
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

240
7 Special Functions
for the properties of Bessel functions and
spherical harmonics.
A third approach is to start from the
point to which the symmetry theory deliv-
ers us and to call those functions special
that satisfy simple, linear recurrence rela-
tions of the form
𝜕F(x; n)
𝜕x
= 𝛼F(x; n + 1).
Here x is a continuous variable, n a discrete
(integer) variable, and 𝛼a constant. Many
special functions can be characterized this
way.
Interestingly, this last approach dovetails
best with modern developments in special
function theory. There has been a great
ﬂowering of the theory of discrete systems
in the last decades, connected with dis-
crete geometries, discrete integrable sys-
tems, and “quantum" (q−) deformations of
classical objects giving rise to q-analogues
of all special functions.
In the end, it may be that special func-
tions are simply the ﬂotsam and jetsam
thrown up by the sea of mathematical his-
tory. There will always be a need for new
piles into which as yet unclassiﬁed debris
may be sorted.
The philosophy behind this article is that
since long lists of formulae are to be found
in larger treatises on special functions (to
which the reader will be directed), we con-
centrate on a survey of the landscape and
the mathematical motivation for describ-
ing the properties of such functions. We
will nod in the direction of applications
now and then but we feel that a tome
entitled Tools for Physicists really ought to
leave the question of application as open as
possible.
For the most part, we treat special
functions from the point of view of sym-
metries, summarizing uncomprehensively
but
hopefully
comprehensibly,
classes
of function associated with discrete and
continuous symmetry, referring to the
underlying group theory. We use the
Laplace operator as a source of many of the
paradigms of the theory. We also present
the factorization approach that ties in with
representation theory, as well as an aside
on factorization theory for linear, partial
diﬀerential operators. Somewhat brieﬂy,
we cover cases where symmetries are less
apparent: the cases of irregular singular
points; and the general theories of Liouville
and Picard–Vessiot. Much research eﬀort
has been expended in developing appli-
cations of functions satisfying nonlinear
ODEs and accordingly we introduce the
elliptic and theta functions as well as the
Painlevé transcendents. Finally, we take a
step into the more recently explored world
of discrete systems, diﬀerence equations,
and q-functions, which are related to rep-
resentations of quantum groups in the way
that classical special functions are related
to representations of continuous symmetry
groups. A consistent theme throughout
the treatment is the rôle of ﬁnite reﬂection
groups.
For none of these ideas is there room
for detailed development and many issues
are left untouched. Our treatment through-
out is biased toward the algebraic rather
than the analytic theory. Serious omissions
include the following: asymptotic methods;
completeness of function spaces; integral
representations, and so on. We direct the
reader to cited works as sources of fur-
ther study. References are representative
rather than inclusive and we have tried to
cite what is formative but space, and the
author’s ignorance, have militated against
an attempt at completeness.

7.2 Discrete Symmetry
241
7.2
Discrete Symmetry
7.2.1
Symmetries
Symmetric objects are distinguished by the
property of invariance under a group of
transformations or redescriptions [1] . The
simplifying consequences of the existence
of symmetric objects for all the sciences
cannot be overstated and the theory of spe-
cial functions is a notable example. We
must start therefore with a brief heuristic
discussion of symmetries.
Let X be a set, assumed ﬁnite for the
moment. We ignore any mathematical
structure this set may possess, so it is really
just a set of labels: say, X = {1, 2, … , n}.
Functions from X to itself are then maps
from the set of labels to itself. For example,
we might map every element to the label 1:
f (x) = 1, ∀x ∈X. Such a map is not bijec-
tive. The requirement that f be bijective is
natural. In that case, there exists an inverse
function, f −1 and given any x ∈X there
is a unique preimage, f −1(x), mapping to
x under f . The composition (∘) of maps is
associative,
f1 ∘(f2 ∘f3) = (f1 ∘f2) ∘f3,
and there is, in particular, a special map
id ∶x →x that maps each label to itself. So
the set of all such maps (which is clearly
ﬁnite) has a group structure (Chapter 5).
This is the symmetric group on n labels, Sn.
Its elements are permutations of the label
set. Sn is the largest group acting on X. Any
subgroup, T, of Sn will also act on X and
the characteristic properties of any such
(sub)group of maps will be that:
id ∈T;
f1, f2 ∈T ⇒f1 ∘f2 ∈T;
f ∈T ⇒f −1 ∈T;
and
id(x) = x,
f1(f2(x)) = (f1 ∘f2)(x)
f ∘f −1(x) = f −1 ∘f (x) = x,
∀x.
These properties are taken to deﬁne the
idea of the group action of any group on any
set (ﬁnite or not).
Now consider the set, denoted by Y X, of
maps from X to some other (usually sim-
pler) set Y. Again, we ignore structure on
Y. If
𝜙∶X →Y
is such a map, and g an element of the group
G acting on X, we deﬁne a corresponding
action, ġ, on 𝜙by
𝜙→g ⋅𝜙
where
(g ⋅𝜙)(x) = 𝜙(g−1(x)) = (𝜙∘g−1)(x),
the inverse on the right-hand side being
necessary to ensure the correct ordering in
(g1g2) ⋅𝜙= g1 ⋅(g2 ⋅𝜙),
that is, to ensure that the action of G on the
set of maps satisﬁes our prior requirements
for a group action.
Already we can see the primitive ele-
ments of symmetry creeping in. Let G act
on X (ﬁnite) and let Y X be the set of maps
from X to Y. A map 𝜙∈Y X is said to be
G-invariant if
g ⋅𝜙= 𝜙, ∀g ∈G.

242
7 Special Functions
This means that 𝜙(g(x)) = 𝜙(x) for each x
and all g. So 𝜙is constant on the orbits of
G:
OG(x) = {g(x)|g ∈G} ⊆X.
For example, let X be the set of n elements
as before and let G be the subgroup Sn−1 ⊂
Sn that ﬁxes only one element, say 1. Then
X is a union of two orbits
X = {1} ∪{2, 3, … , n}
and any invariant map, 𝜙, takes at most two
distinct values: 𝜙(1) and 𝜙(2) = 𝜙(3) · · · =
𝜙(n).
The group Sn has a geometrical interpre-
tation. Consider, for instance, an equilat-
eral triangle with vertices labeled {1, 2, 3}.
The six distinct permutations of these labels
correspond to rotations and reﬂections of
the triangle to itself. Likewise, the regular
tetrahedron with vertices {1, 2, 3, 4} has a
set of geometrical symmetries correspond-
ing to the elements of S4. These are ﬁnite
subgroups of the group O(3) of isometries
(distance preserving maps–rotations and
reﬂections), ﬁxing the origin of ℝ3.
Apart from generalizing the triangle to
regular polygonal prisms, the only regular
polyhedra are the cube, octahedron, dodec-
ahedron and icosahedron. Their symme-
tries complete the list of all ﬁnite subgroups
of O(3). These are not groups of the type Sn.
The symmetry group of the cube and octa-
hedron is generated by S4 and an inversion,
x →−x.
and that of the dodecahedron and icosahe-
dron, is generated by S5 and the inversion.
It is when we pay attention to extra struc-
ture the underlying sets may possess that
we start to make a connection with the the-
ory of special functions. Here are two illus-
trations that we will revisit.
Let K be a ﬁeld, say ℚor ℂ, and let
R = K[x1, … , xn]
be the ring of polynomials in indetermi-
nates xi, i = 1, … , n. Sn acts on this ring in
the natural way:
(𝜎⋅p)(x1, … , xn) = p(x𝜎−1(1), … , x𝜎−1(n)),
p being an arbitrary polynomial (element of
R). Consider
G(p) =
∑
𝜎∈Sn
𝜎⋅p.
This is an invariant element of the ring. For
example,
G(x1) = (n −1)!(x1 + x2 + · · · + xn).
Such invariant elements are called sym-
metric polynomials (Section 7.2.3) and
although there is clearly an inﬁnite collec-
tion of such, all can, in fact, be generated
as a polynomial ring over a ﬁnite number
of basic symmetric polynomials for which
a number of canonical choices exist.
As a second example, consider rotations
of the plane about a ﬁxed point (the origin
of coordinates). Let R(𝜃) denote the clock-
wise rotation through angle 𝜃, a linear map
on coordinates, x, y ∈ℝ2:
x →x cos 𝜃+ y sin 𝜃,
y →−x sin 𝜃+ y cos 𝜃.
It is easily veriﬁed that
R(𝜃)R(𝜙) = R(𝜃+ 𝜙),
R(0) = id
and that the set of all R(𝜃) comprises an
inﬁnite group. Real-valued functions on ℝ2

7.2 Discrete Symmetry
243
inherit the group action
R(𝜃) ⋅f (x, y) = f (R(−𝜃)(x, y))).
It is natural to exchange rectangular coor-
dinates for polar coordinates,
x = 𝜌cos 𝜓,
y = 𝜌sin 𝜓,
which are adapted to the group: rotations
act on the coordinate function 𝜓by trans-
lation,
𝜓→𝜓−𝜃,
and the function 𝜌is invariant. All invariant
functions are functions of 𝜌alone. Such
functions are constant valued on the orbits
of the group, which are the geometrical
circles centered on the origin and the origin
itself.
Because the group is smooth (Chapter 9),
we can also consider a local description of
invariance. In the case of small 𝜃∼𝜖:
x →x + y𝜖+ O(𝜖2),
y →y −x𝜖+ O(𝜖2).
The condition of invariance of a function f
is expressed as
f (x + 𝜖y, y −𝜖x) −f (x, y) = O(𝜖2)
and by using a Taylor expansion in 𝜖,
(x𝜕y −y𝜕x)f = 0.
In polar coordinates, this is simply
𝜕𝜃f = 0.
7.2.2
Coxeter Groups
Although ﬁnite, we shall see in later
sections that groups such as Sn play a
central role in modern aspects of spe-
cial function Theory, for example, Dunkl
operators and Painlevé equations. One
ubiquitous
class
is
that
of
reﬂection
(Coxeter) groups [2–4].
In ℝn, imagine a set of hyperplanes (“mir-
rors”) arranged in such a way that the reﬂec-
tion of any one hyperplane in another is
again a hyperplane of the set. An example
would be three inﬁnite systems of orthogo-
nal mirrors, regularly spaced along the x, y,
and z directions in ℝ3 to form an inﬁnitely
extended “milk crate.” Another would be
mirrors arranged along the edges of a regu-
lar triangular lattice in ℝ2.
If the set of hyperplanes is ﬁnite (unlike
these two examples), it will associate with
each hyperplane a generating element of a
ﬁnite reﬂection group and the hyperplanes
will all have a common point of intersec-
tion.
Suppose we have a ﬁnite reﬂection group
with the hyperplanes in ℝn all passing
through the origin. To each hyperplane, H,
is associated a nonzero normal vector h,
deﬁned up to length. The group element
representing a reﬂection in this hyperplane
is
sh(x) = x −2 (x, h)
(h, h)h
where the brackets (⋅, ⋅) denote the standard
inner product on ℝn. Clearly sh = sch for
any c ∈ℝ⧵{0}.
In this way, the set of hyperplanes is asso-
ciated with a classical root system of the
kind exempliﬁed in the theory of semi-
simple Lie algebras (Section 7.3.1). More
generally, they are root systems of ﬁnite

244
7 Special Functions
Coxeter groups generated by reﬂections,
W = ⟨r1, … , rn|(rirj)mij = id⟩,
the mij being the positive integer orders
of pair products and, ∀i, mii = 1, so that
r2
i = e.
It is the case that W is a ﬁnite Coxeter
group if and only if it is isomorphic to a
ﬁnite reﬂection group. Such groups have
been fully classiﬁed.
Generalizations of the reﬂection groups
discussed so far are the aﬃne reﬂection
groups (aﬃne Weyl groups). In this case, the
reﬂection hyperplanes are not restricted to
passing through the origin: one has reﬂec-
tions in translated planes. Such groups are
semi-direct products of a reﬂection group
and a translation group associated with a
lattice (as in the examples given at the
beginning of this section). They also have
an important place throughout the mod-
ern theory of special functions and we will
touch on this in later sections.
7.2.3
Symmetric Functions
Symmetric functions are a class of special
functions arising from the most universal
ﬁnite
symmetry
groups
(permutations
of labels) with ubiquitous applications
in mathematics and physics [5, 6]. These
include
combinatorial
problems,
parti-
tioning questions, labeling of irreducible
representations and the parameterization
of geometrical objects such as Grassmann
varieties.
Let Sn act on polynomials in x1, … , xn
by permutation of indices. Symmetric
polynomials are those invariant under this
action and the most general examples are
the Schur polynomials.
Let Y𝜇be the Young diagram of a parti-
tion of some positive integer m, that is a list
𝜇= (m1, m2, …)
with the properties
mi−1 ≥mi ≥0
and
∑
i
mi = m.
Thus the partitions of 5 are, writing only
the (ﬁnitely many) nonzero entries: (5),
(4, 1), (3, 2), (3, 1, 1), (2, 2, 1), (2, 1, 1, 1),
(1, 1, 1, 1, 1). The corresponding Young dia-
grams consist of rows of boxes of lengths
mi, left justiﬁed and ordered downward.
Thus:
Y(5)
=
Y(4,1)
=
Y(3,2)
=
Y(3,1,1)
=
Y(2,2,1)
=
Y(2,1,1,1)
=
Y(1,1,1,1,1)
=
The partition of n 1’s is abbreviated to (1)n.
From Young diagrams with m boxes,
we create Young tableaux on n labels by
ﬁlling the boxes with choices from the
set {1, … , n} subject to the rules that the
numerical values of entries increase weakly
along rows but strictly down columns.
Thus two possible tableaux arising from
Y(3,2) on four labels are

7.2 Discrete Symmetry
245
1 2 2
2 4
and
3 3 3
4 4
.
With any Young tableau we associate a
monomial by including a factor of xi for
each of the occurrences of i in a box of the
tableau. Thus, from the above we get
x1x3
2x4
and
x3
3x2
4.
The Schur function, S𝜇, of a partition 𝜇
is the sum (with unit coeﬃcients) over
the monomials associated with all possible
tableaux coming from the Young diagram
of the partition.
Two simple cases correspond to the ele-
mentary symmetric functions
em(x1, … , xn) =
∑
1≤i1<i2<…<im≤n
xi1xi2 … xim
and the complete symmetric functions
hm(x1, … , xn) =
∑
1≤i1≤i2≤…≤im≤n
xi1xi2 … xim.
Note that ei = 0 for i > n.
These
polynomials
have
generating
functions:
E(n)(t) =
n
∑
0
ektt =
n
∏
i=1
(1 + xit)
H(n)(t) =
∞
∑
0
hktt =
n
∏
i=1
(1 −xit)−1
The elementary and complete symmetric
polynomials are the special cases of Schur
polynomials corresponding to Young dia-
grams of a single column or a single row of
length k ∶ek = S(1)k; hk = S(k).
More generally, a determinantal formula
for S𝜇exists:
S𝜇=
det(x
𝜇i+n−i
j
)
i,j
∏
1≤i<j≤n(xi −xj)
and the Schur polynomials can also be
expressed by the Jacobi–Trudi formulae in
terms of either the ek’s or the hk’s:
S𝜇= det(h𝜇i+j−i)i,j
= det(e𝜇′
i +j−i)i,j
where 𝜇′ denotes the partition of the Young
diagram conjugate to 𝜇(transposed about
the diagonal). For example, if
𝜇= (3, 2)
then
Y(3,2) =
so
𝜇′ = (2, 2, 1)
because
Y(2,2,1) =
is the transpose of Y(3,2).
Schur polynomials are rich in identi-
ties and applications. Perhaps the most
important elementary identity is due to
Giambelli. This is simply expressed using
the Frobenius notation for a partition: we

246
7 Special Functions
count in each row the number (𝛼i) of boxes
to the right of the box in the ith place on the
diagonal and in each column the number
(𝛽i) of boxes below that diagonal place.
Then we write
(𝛼1, 𝛼2, … |𝛽1, 𝛽2, …).
For example, the partition (4, 3, 2, 1, 1)
associated with the Young diagram
has 𝛼1 = 3, 𝛼2 = 1, 𝛽1 = 4 and 𝛽2 = 1, its
Frobenius notation being (3, 1|4, 1).
The Giambelli formula expresses the
Schur polynomial of a partition in terms
of simpler Schur polynomials of hook
diagrams:
S(𝛼1,𝛼2,…|𝛽1,𝛽2,…) = det(S(𝛼i|𝛽j))i,j.
Another crucial property is a multiplication
formula for Schur polynomials, the Little-
wood–Richardson rule: Given Young dia-
grams 𝜇and 𝜆with 𝜇i ≤𝜆i, ∀i, a skew dia-
gram, 𝜆∕𝜇is the shape obtained by remov-
ing the leftmost 𝜇i boxes from the ith row
of 𝜆. For example, if 𝜆= (3, 2, 1) and 𝜇=
(1, 1), then 𝜆∕𝜇is the skew diagram,
.
A ﬁlling of the boxes in the skew diagram
with integers according to the same rules as
for Young tableaux yields a skew tableau, for
example,
1 1
2
1
2 3
3
4
, . . .
,
and summing over the associated monomi-
als in the same way yields the skew Schur
polynomial associated with that skew dia-
gram.
According
to
the
RSK
(Robinson–
Schensted–Knuth) correspondence, a skew
tableau can be rectiﬁed to a Young tableau
by a simple algorithmic (Shensted) process.
The
Littlewood–Richardson
numbers,
c𝜆
𝜇,𝜈count the number of skew tableaux,
𝜆∕𝜇that rectify to a Young tableau with
diagram 𝜈. Then
S𝜆∕𝜇=
∑
𝜈
c𝜆
𝜇𝜈S𝜈,
and
S𝜇• S𝜈=
∑
𝜆
c𝜆
𝜇𝜈S𝜆
where the • product is again the Schen-
sted algorithm applied to the tableaux and
extended linearly to the terms in the Schur
polynomials. It is an associative but not a
commutative multiplication.
Quite generally, any partition 𝜆corre-
sponds to an irreducible representation
(Specht module) of Sn and, in turn, to
an
irreducible
representation
(Schur
or Weyl module) of the general linear
group. The Schur polynomials and Little-
wood–Richardson formulae encode the
properties of tensor products and quotients
of these modules.
7.2.4
Invariants of Coxeter Groups
Root systems are associated with ﬁnite
reﬂection groups according to a stan-
dard classiﬁcation and we give here some
examples with the corresponding invariant
polynomials [2, 4].
The simplest example is the group of
a single reﬂection in the hyperplane x1 +

7.2 Discrete Symmetry
247
x2 = 0 acting on ℝ2. Polynomial generators
of the ring of invariants are
y1 = x2
1
y2 = x1x2
y3 = x2
2
subject to a single relation: y1y3 −y2
2.
For the dihedral group D2n generated by
r1 ∶x1 →x2,
x2 →x1
r2 ∶x1 →𝜁x1,
x2 →𝜁−1x2
𝜁being a primitive nth root of unity, the
ring of invariants is generated by
y1 = x1x2
y2 = xn
1 + xn
2
without relations.
The Weyl group, W = Sn+1, associated
with the series of semi-simple Lie algebras
An (or 𝔰ln) has a root system consisting of
n(n + 1) vectors:
ei −ej,
1 ≤i ≠j ≤n + 1.
The ei are the standard, unit coordinate vec-
tors. A basis of simple, positive roots is Δ =
{ei −ei+1|i = 1, … n}. W acts by permuting
coordinate functions xi, i = 1, … n + 1 sub-
ject to the hyperplane condition
x1 + x2 + · · · + xn+1 = 0,
and the basis of invariant polynomials is
given by the ﬁrst n power sums
fi = xi
1 + xi
2 + · · · + xi
n+1
for i = 1, … , n, which can be written in
terms of elementary symmetric functions
using the Newton identities.
7.2.5
Fuchsian Equations
Many partial diﬀerential equations (PDEs)
arising in physics can, in the presence of
geometrical symmetry, be reduced to a type
of linear ODE having only singular points
in ℂof a well-controlled type known as
regular singular points (rsp) described in
the following text [7–11].
As the simplest type of singular point,
the accompanying theory has been thor-
oughly developed. We summarize some
fundamental results in this section. Further
generalization of these ideas will prove
important later Section 7.6.
Solutions near an rsp, located at z = 0,
are of the form z𝛾f (z) or z𝛾(ln z)g(z) where f
or g are holomorphic in the local, complex
parameter z and nonvanishing at the sin-
gular point. The equations of Bessel, Leg-
endre, Hermite, and so on that we shall
meet shortly are of this kind. If, in addition,
the point at inﬁnity is regular singular, then
the ODE is called Fuchsian. To determine
whether the point at ∞has this property for
a given equation, we transform ∞in the z-
plane using w = 1∕z to 0 in the w-plane. We
then analyze the singularity at w = 0. The
general family of Fuchsian equations hav-
ing three rsps only (at 0, 1, and ∞) is the
hypergeometric family.
A second-order ODE in the complex
plane,
w′′ + p(z)w′ + q(z)w = 0
is said to have a regular singular point at
z = z0 if p and q are meromorphic functions
on ℂwith local form,
p(z) = (z −z0)−1P(z),
P(z0) ≠0,
q(z) = (z −z0)−2Q(z),
Q(z0) ≠0,

248
7 Special Functions
P and Q both being analytic near z = z0. If
we shift the independent variable to make
z0 = 0, then a local basis of solutions near 0
is given by
w1 = z𝜈1W1(z),
w2 = z𝜈2W2(z)
where 𝜈1 and 𝜈2 satisfy the quadratic indi-
cial equation
𝜈(𝜈−1) + P(0)𝜈+ Q(0) = 0
provided the roots are neither equal nor
diﬀer by an integer. When the roots are
equal or diﬀer by an integer, 𝜈1 −𝜈2 ∈ℤ,
the solution basis may include logarithms
w1 = z𝜈1W1(z),
w2 = ln z
w1 + z𝜈1W2(z).
Convergent power series solutions for the
W’s are obtained by substitution into the
ODE to derive second-order recurrence
relations on the coeﬃcients in their analytic
expansion. This is the Frobenius method.
In general, solutions of an ODE near
its rsp’s are multivalued in the sense that
analytic continuation of a solution along
a closed path about such a point returns
a diﬀerent solution at the initial point.
Given that one has a local basis of analytic
solutions at each nonsingular point, any
closed path can be associated with a matrix
expressing the continued solutions as lin-
ear combinations of this local basis. Thus
a circuit around the singular point aﬀects
the ﬁrst kind of basis above diagonally,
w1 →e2𝜋i𝜈1w1,
w2 →e2𝜋i𝜈2w2,
and the second kind of basis triangularly,
w1 →e2𝜋i𝜈1w1,
w2 →e2𝜋i𝜈1w2 + 2𝜋ie2𝜋i𝜈1w1.
The matrices formed this way clearly con-
stitute a group: the monodromy group of the
ODE. This group will be ﬁnitely generated if
the number of rsp’s is ﬁnite but the order of
any generator need not be ﬁnite because the
multivaluedness is determined by the expo-
nents 𝜈i, which may not be rational, and
by the possible presence of the logarithmic
function. In general, a solution is inﬁnitely
ramiﬁed (sheeted) over ℂ.
But it is possible to ask, as did Klein,
under
what
circumstances
there
exist
ﬁnitely sheeted solutions. It follows from
the general theory of Riemann surfaces that
such solutions are algebraic: they satisfy a
polynomial equation in themselves with
coeﬃcients polynomial in z ∈ℂ[12].
It is useful to reduce the ODE to a canon-
ical form by eliminating the coeﬃcient p(z)
of w′. A revealing way of doing this is to
attempt to “complete the square” in the dif-
ferential operator part by writing it as
(
d
dz + p(z)
2
)2
w + I(z)w = 0
where the function
I(z) = q −p′
2 −p2
4
is a diﬀerential invariant in the sense that it
is unaltered by any z-dependent scaling of
w∶
w(z) →𝜆(z)w(z).
We choose linearly independent solutions
w1 and w2 to the ODE, introduce a new
independent variable
s = w1
w2

7.2 Discrete Symmetry
249
and consider z as dependent variable. Then
using the notation
{Z, s} = −1
Z′2
(
Z′′′
Z′ −3
2
Z′′2
Z′2
)
,
for the Schwarzian derivative of Z with
respect to s, we obtain
{z, s} = 2I(z).
The Schwarzian derivative is a projective
invariant in the sense that, for 𝛼, 𝛽, 𝛾, and
𝛿constant with 𝛼𝛿−𝛽𝛾≠0,
{𝛼Z + 𝛽
𝛾Z + 𝛿, s
}
= {Z, s}
and it also has the property
{s, z} = {s, Z}
(
dZ
dz
)2
+ {Z, z}.
Restricting
attention
to
second-order
Fuchsian ODEs and to the situation where
there are only three rsp’s we may, by a lin-
ear fractional transformation in z, assume
these rsp’s lie at the points z = 0, z = 1, and
z = ∞. Consider a pair of linearly indepen-
dent solutions 𝜓1 and 𝜓2 to the ODE and
analytically continue along a closed path
around an rsp. The two solutions transform
linearly on return to the initial place and
so their ratio, 𝜙= 𝜓1∕𝜓2 transforms by the
Möbius map (parameters as above):
𝜙→𝛼𝜙+ 𝛽
𝛾𝜙+ 𝛿.
The group of all Möbius transformations
is PSL2(ℂ). We are looking then for ODEs
whose monodromy group is a ﬁnite sub-
group of PSL2(ℂ). This group is isomor-
phic to SO3(ℝ), the group of orientation-
preserving isometries of ℝ3. So we have the
answer to the question in the form of the
ﬁnite subgroups of the rotation group: the
symmetry groups of the regular solids and
prisms discussed earlier (Section 7.2.1).
Klein used this classiﬁcation to show that
in order for the ODE to have algebraic solu-
tions the invariant I(z) must be one of ﬁve
possible forms up to an arbitrary rational
function, Z(z), namely,
Ii(z) = 1
4Ji(Z)
(
dZ
dz
)2
+ 1
2{Z, z},
where Ji(Z) is given by
J1(Z) = 1 −N−2
Z2
Ji(Z) =
1 −𝜈−2
2
Z2
+
1 −𝜈−2
1
(Z −1)2
+
𝜈−2
1 + 𝜈−2
2 −𝜈−2
3 −1
Z(Z −1)
,
i = 2, 3, 4, 5.
The values of 𝜈2, … , 𝜈5 are tabulated thus:
i
𝜈1
𝜈2
𝜈3
Symmetry
1
N
N
cyclic
2
2
2
n
dihedral
3
2
3
3
tetrahedral
4
2
3
4
cubic
5
2
3
5
icoshedral
The solutions are
w1 = s
(
ds
dz
)−1∕2
,
w2 =
(
ds
dz
)−1∕2
,
where s is related to Z(z) in each case:
Case 1.
Z =
(s −s1
s −s2
)N
Case 2.
Z = −1
2
sn −1
sn

250
7 Special Functions
Case 3.
Z = (s4 + 2
√
3s2 −1)3
(s4 −2
√
3s2 −1)3
Case 4.
Z = (s8 + 14s4 + 1)3
108s4(s4 −1)2
Case 5.
Z = −(s20 −228s15 + 494s10 + 228s5 + 1)3
1728s5(s10 + 11s5 −1)5
.
Because the ODEs of the special functions
that we shall study below are not always
Fuchsian (∞is not an rsp), we can think
of the algebraic solutions above as simple
precursors of the (transcendental) special
functions.
7.3
Continuous Symmetry
Between 1888 and 1893, Sophus Lie [13]
published the three volumes of his Theorie
der Transformationsgruppen, where he laid
the foundations and initiated the systematic
applications of continuous groups of sym-
metries acting on geometric spaces.
The construction of the classical special
functions of applied mathematics can be
approached using continuous symmetries
from two end points.
From one end, we may start with a partial
diﬀerential operator whose eigenfunctions
we wish to understand and use a geomet-
rical symmetry to reduce the PDE to an
ODE by “separation of variables.” We may
solve this equation either by the Frobenius
expansion or by a factorization method and
study the properties of the resulting fami-
lies of functions.
Starting from the other end, we may look
for representations of symmetry groups
in function spaces. When these are con-
structed, the corresponding representation
of the Lie algebra gives rise to diﬀerential
expressions for raising and lowering oper-
ators which present to us the diﬀerential
equations with which the ﬁrst approach
commenced.
As examples, we will discuss spherical
harmonics from the ﬁrst point of view and
Bessel functions from the second. We will
also list some results appertaining to other
(but by no means all) classical special func-
tions.
7.3.1
Lie Groups and Lie Algebras
A
group
G
is
said
to
be
a
(ﬁnite-
dimensional) Lie group if it is a ﬁnite-
dimensional
diﬀerentiable
manifold
endowed with a multiplication map,
𝜇∶G × G →G
and an inversion map,
𝜄∶G →G,
which are smooth with respect to the dif-
ferentiable structure on G and satisfy the
group axioms for some identity point, e ∶
𝜇(e, x) = 𝜇(x, e) = x
𝜇(x, 𝜄(x)) = 𝜇(𝜄(x), x) = e
𝜇(𝜇(x, y), z) = 𝜇(x, 𝜇(y, z)).
They often arise as symmetries of geomet-
rical spaces and their own, intrinsic geom-
etry allows the methodology of diﬀeren-
tial geometry (Chapter 9) to be eﬀectively
applied [14–17].
In particular, G has, at each point g ∈G,
a tangent space: TgG. If Lg ∶G →G is the

7.3 Continuous Symmetry
251
map Lg(h) = gh, h being an arbitrary point
in G, then the derivative of this map, dLg
takes ThG to TghG and is an isomorphism
of ﬁnite-dimensional vector spaces.
The set of all vector ﬁelds on G is a mod-
ule over the ring of functions on G and an
inﬁnite-dimensional vector space over the
ﬁeld of constants.
On the other hand, a vector ﬁeld, v, on G
is left-invariant if
v(Lgh) = dLgv(h), ∀g, h.
The set of such left-invariant ﬁelds is a
ﬁnite-dimensional vector space and can
be identiﬁed with the tangent space at the
identity, TeG. This is the Lie algebra, 𝔤, of
G. In many practical situations in applied
mathematics and theoretical physics, it
is through 𝔤that the action of a smooth
symmetry group is recognized. We have
already seen an example: the rotation group
SO(2) has a Lie algebra so(2), which acts
on ℝ2 via a one-dimensional vector ﬁeld:
x𝜕y −y𝜕x.
A Lie algebra, 𝔤, is endowed with a
(nonassociative) bilinear product, ⋆, hav-
ing the following properties: every g ∈𝔤is
nilpotent,
g ⋆g = 0;
and every triple g, h, k ∈𝔤satisﬁes the
Jacobi identity,
g ⋆(h ⋆k) + h ⋆(k ⋆g) + k ⋆(g ⋆h) = 0.
Applying the nilpotence condition to g + h,
(g + h) ⋆(g + h) = 0
and using bilinearity, gives the skewness of
⋆∶
g ⋆h = −h ⋆g.
The Universal Enveloping Algebra con-
struction replaces 𝔤by the full tensor
algebra (Chapter 9) T(𝔤) (noncommutative
polynomial algebra or sums of ordered
products of elements of 𝔤) quotiented by
the ideal generated by elements of the form
g ⊗h −h ⊗g −g ⋆h.
It allows us to represent the ⋆operation by
the commutator [⋅, ⋅].
The structure of a semi-simple (see next
section), ﬁnite-dimensional Lie algebra is
determined by a commutative subalgebra,
the Cartan subalgebra, 𝔥, by which 𝔤can
be decomposed as 𝔥-eigenspaces, labeled
by roots. These roots are elements 𝛼∈𝔥∗,
the vector space dual of 𝔥, the elements
of eigenspaces, 𝔤𝛼, satisfying [h, e] = 𝛼(h)e
for every h ∈𝔥, e ∈𝔤𝛼. The roots can be
divided into two types: positive roots 𝛼∈
Δ+; and negative 𝛼∈Δ−; the entire algebra
decomposing thus:
g = h
g𝛼
g𝛼.
𝛼∈Δ
𝛼∈Δ−
7.3.2
Representations
Very many special functions arise as ele-
ments of ﬁnite- or inﬁnite-dimensional vec-
tor spaces carrying representations of a Lie
group [14, 18]. In fact, the language of
Lie group and algebra representations is
endemic in modern physics.
If G is a Lie group and V a vector space,
which may be inﬁnite dimensional, over
a ﬁeld, say ℂ, then a representation of G
is a ring homomorphism 𝜌∶G →End(V),
(Chapter 5) from the group into ℂ-linear
maps, endomorphisms, from V to itself, that

252
7 Special Functions
is, 𝜌satisﬁes the conditions
𝜌(𝛼)𝜌(𝛽) = 𝜌(𝛼𝛽)
𝜌(e) = IdV
where e is the group identity and IdV the
identity endomorphism on V. If {ei|i =
1, … , n} is a basis of V (and in a formal
sense, we may take n to be inﬁnity) we
deﬁne a matrix representation for G via the
identity
𝜌(𝛼)ei =
n
∑
j=1
R(𝛼)ijej
and
the
homomorphism
condition
becomes
Rij(𝛼𝛽) =
n
∑
k=1
Rik(𝛼)Rkj(𝛽)
Rij(e) = 𝛿ij.
Representations 𝜌on V and 𝜌′ on V ′ are
equivalent if there exists an invertible map
S ∶V →V ′ satisfying
𝜌′(𝛼)S = S𝜌(𝛼), ∀𝛼∈G.
If V has an inner product, <, >∶V × V →
ℂ, then 𝜌is a unitary representation of G if
for all 𝛼∈G and all u, v ∈V
< 𝜌(𝛼)u, 𝜌(𝛼)v >=< u, v > .
Any representation of a ﬁnite group or of a
compact Lie Group is equivalent to such a
unitary representation.
A vector subspace U ⊂V is invariant if
for all 𝛼∈G and all u ∈U,
𝜌(𝛼)u ∈U
and the representation 𝜌on V is said to be
reducible if it contains a nontrivial, proper
invariant subspace. If V = U ⊕W where
U and W are both invariant, then the rep-
resentation 𝜌on V is completely reducible
and its matrix representation will decom-
pose into a block diagonal form.
For unitary representations, reducibility
implies complete reducibility.
These ideas carry over to the local situa-
tion via the Lie algebra. 𝜆is a representation
on V of the Lie algebra, 𝔤, of the Lie group
G, if for g, h ∈𝔤and a, b ∈ℂ,
𝜆(ag + bh) = a𝜆(g) + b𝜆(h)
𝜆(g ⋆h) = [𝜆(g), 𝜆(h)],
[∗, ∗] denoting the commutator of matrices
Equivalence, invariance, reducibility, and
so on, are deﬁned analogously to the group
case.
A Lie algebra 𝔤is solvable if its derived
series
𝔤(0) ⊃𝔤(1) ⊃𝔤(2) · · ·
deﬁned by
𝔤(i) = 𝔤(i−1) ⋆𝔤(i−1),
𝔤(0) = 𝔤
is ﬁnite. A simple example is the set of
upper triangular matrices. Solvability of an
ideal of 𝔤is deﬁned likewise. 𝔤is simple if
it is non-abelian and contains no proper,
nontrivial ideal. It is semi-simple if it has no
nontrivial solvable ideal.
If 𝜌is a representation of 𝔤on V then V
decomposes into weight spaces
V𝜆= {v ∈V|h(v) = 𝜆(h)v, ∀h ∈𝔥}
for each 𝜆∈𝔥∗. The action of 𝔤𝛼on the
weight spaces is
𝔤𝛼∶V𝜆→V𝜆+𝛼.
The intersections of distinct weight spaces
are trivial, their sum is direct, and in the

7.3 Continuous Symmetry
253
case of ﬁnite-dimensional V,
V =
V𝜆.
𝜆∈h∗
An element v ∈V is said to be maximal
of highest weight 𝜆if v ∈V𝜆and 𝔤𝛼(v) = 0
for all 𝛼∈Δ+. The action of the whole of
𝔤on v then generates a cyclic representa-
tion of highest weight 𝜆. Two cyclic repre-
sentations of the same highest weight are
isomorphic, hence standard. The weights
form a lattice in 𝔥∗and such a standard
representation exists for any element in the
weight lattice.
Finite-dimensional
cyclic,
irreducible
representations of 𝔤necessarily have high-
est weights 𝜆such that, for a standard basis,
{hi|i = 1, … , rank(𝔥)}, of 𝔥, the numbers
𝜆(hi) are positive integers.
In the spaces of special functions arising
from the actions of symmetry groups on
diﬀerential operators, these 𝜆(hi) appear as
special parameter values and so where the
representations are ﬁnite dimensional, it is
because of integrality conditions on these
parameters.
7.3.3
The Laplace Operator
A commonly occurring situation for the
classical special functions is the need to
solve the Laplace equation in a speciﬁc
geometry [19, 20]. Of course, one may write
down integral formulae for the solutions in
terms of a Green’s function for almost any
(asymmetric) geometry, but where there is
some symmetry, the solution space is more
explicitly manifested.
The
Laplace
operator
on
three-
dimensional space, ℝ3,
Δ = 𝜕2
x + 𝜕2
y + 𝜕2
z
has a number of symmetries. In particu-
lar, translations in the x, y, and z variables,
described by the Lie group ℝ3 and rigid
rotations, SO(3) are symmetries.
The Lie algebra of translations is gener-
ated by the set {𝜕x, 𝜕y, 𝜕z}, which all com-
mute with Δ ∶[Δ, 𝜕x] = 0, and so on.
The Lie algebra so(3) of SO(3) is the real,
three-dimensional algebra with basis
{x𝜕y −y𝜕x, y𝜕z −z𝜕x, z𝜕x −x𝜕z},
which acts on functions of x, y and z, and
which commutes with Δ.
The commutation property means that if
𝜙is a solution of the Laplace equation, then
so is g(𝜙), g being any of the generators
described above.
If 𝜙solves the Laplace equation on ℝ3,
Δ𝜙(x, y, z) = 0,
then so does the function a𝜙x + b𝜙y + c𝜙z,
a, b, and c being constants. So the solution
space is a vector space of functions closed
under this diﬀerential algebraic operation.
In particular, it suﬃces to choose functions
satisfying
𝜙x = 𝜆𝜙, 𝜙y = 𝜇𝜙, 𝜙z = 𝜈𝜙
for constants 𝜆, 𝜇and 𝜈. This is just the
space of functions such as exp(𝜆x + 𝜇y +
𝜈z). (Note that we include complex-valued
functions here because their linear combi-
nations may be real.)
If the geometry of our problem requires
conditions of the form
𝜙(x + L, y + M, z + N) = 𝜙(x, y, z)
for ﬁnite, positive L, M and N, then these
are reﬂected in the choice of sine and cosine
functions as a basis of the solution space.

254
7 Special Functions
An important consequence of these
considerations is that these functions obey
addition laws: if sin x is periodic of period
2𝜋, then so is sin(x + x′) and, by linearity,
must be a linear combination of sin x and
cos x. Determining the multiplicative coef-
ﬁcients from special values of x results in
the addition law:
sin(x + x′) = cos x′ sin x + sin x′ cos x.
This is a common theme in special function
theory, including nonlinear situations.
Other solution spaces will be appropriate
to other geometries. A semi-inﬁnite, ﬁnite
width strip, for example, will require a com-
bination of periodic and decaying exponen-
tial solutions.
The Laplacian operator is a geometric
object that can be deﬁned in a coordinate-
free manner [20]. On any smooth, Rieman-
nian manifold with covariant metric gij, the
Laplacian operator is
Δ𝜙= √g
−1𝜕i(√ggij𝜕j𝜙)
g being the determinant of gij, regarded as a
symmetric matrix, and where the Einstein
convention for summation of repeated
(contra/covariant) index pairs is in opera-
tion. Coordinates may be chosen adapted
to the geometric symmetry of a problem
and the Laplacian written accordingly.
Examples are listed in [21].
In the case of cylindrical symmetry
about, say, the z-axis, SO(2), it is convenient
to change variables to polar coordinates
(𝜌, 𝜓, z). For full rotational symmetry,
SO(3), the spherical polar coordinates
(r, 𝜃, 𝜓) are appropriate. (To be clear, we
use 𝜃here as the azimuthal angle, constant
on lines of lattitude; 𝜓the equatorial angle,
constant on lines of longitude.)
In such coordinates, the symmetry trans-
formations are essentially trivial, that is,
they amount to simple translations in the
variables.
SO(2) ∶(𝜌, 𝜓, z) →(𝜌, 𝜓+ 𝜖, z + 𝜂)
SO(3) ∶(r, 𝜃, 𝜓) →(r, 𝜃+ 𝜁, 𝜓+ 𝜖)
In the cases of the Laplacian on ℝ3, we
obtain in each case:
Δ𝜙= 1
𝜌𝜕𝜌(𝜌𝜕𝜌𝜙) + 1
𝜌2 𝜕2
𝜓𝜙+ 𝜕2
z𝜙
and
Δ𝜙= 1
r2 𝜕r(r2𝜕r𝜙) +
1
r2 sin 𝜃𝜕𝜃(sin 𝜃𝜕𝜃𝜙) +
1
r2 sin2 𝜃
𝜕2
𝜓𝜙.
Because the group transformations are
translations in 𝜃and 𝜓, rotational period-
icity in 𝜃requires that 𝜙be expanded in
terms of the complete system
{exp in𝜓|n ∈ℤ}.
Thus
𝜙(r, 𝜃, 𝜓) =
∑
n∈ℤ
Φnein𝜓
where the coeﬃcients Φn(𝜌, z) or Φn(r, 𝜃)
satisfy
in
each
case
the
diﬀerential
equations
1
𝜌𝜕𝜌(𝜌𝜕𝜌Φn) −n2
𝜌2 Φn + 𝜕2
zΦn = 0
and
1
r2 𝜕r(r2𝜕rΦn) +
1
r2 sin 𝜃𝜕𝜃(sin 𝜃𝜕𝜃Φn)
−
n2
r2 sin2 𝜃
Φn = 0.
For cylindrical symmetry, we may have
either decaying or periodic boundary con-
ditions on z and we obtain an equation of

7.3 Continuous Symmetry
255
Bessel type by expanding in an exponential,
emz series or integral,
1
𝜌𝜕𝜌(𝜌𝜕𝜌Φ) +
(
m2 −n2
𝜌2
)
Φ = 0
where Φ depends on parameters n and m.
In the case of spherical symmetry, we
may expand Φn(r, 𝜃) in powers of r,
∞
∑
l=0
Φnl(𝜃)rl
to obtain the associated Legendre equation:
1
sin 𝜃𝜕𝜃(sin 𝜃𝜕𝜃Φ)+
(
l(l + 1) −
n2
sin2 𝜃
)
Φ = 0
where Φ depends on parameters n and l.
This is more usually written with the inde-
pendent variable x = cos 𝜃∶
(1 −x2)y′′ −2xy′ +
(
l(l + 1) −
n2
1 −x2
)
y = 0.
Linearly independent solutions are denoted
Pn
l (x) and Qn
l (x), associated Legendre func-
tions of the ﬁrst and second kind. The sec-
ond kind are singular at x = 1 and x = −1.
When n = 0 and l ∈ℤ, the ﬁrst kind
functions are polynomial in x and called
Legendre polynomials, Pl(x), if the normal-
ization is chosen such that Pl(1) = 1. l may
be taken to be a positive integer because the
deﬁning equation
y′′ −2xy′ + l(l + 1)y = 0
is unchanged under the replacement of l by
−(l + 1).
As follows from later considerations
(Section 7.4), the Legendre polynomials
are given by the Rodrigues formula:
Pl(x) =
1
2ln!
dl
dxl (x2 −1)l.
The associated functions can be derived
from the Legendre functions using the Fer-
rers formulae:
Pn
l (x) = (1 −x2)n∕2 dn
dxn Pl(x);
Qn
l (x) = (1 −x2)n∕2 dn
dxn Ql(x).
7.3.4
Spherical Harmonics
In many applications, the associated Leg-
endre functions appear under the guise of
(surface) spherical harmonics,
Y n
l (𝜃, 𝜓) =
√
(2l + 1)(l −n)!
4𝜋(l + m)!
Pn
l (cos 𝜃)ein𝜓,
for integer values of m, −l ≤m ≤l, or with
a monomial factor in r,
rlY n
l (𝜃, 𝜓)
as (solid) spherical harmonics [22].
These in turn arise as moments in an
integral expansion in the following manner.
Consider a complex variable 𝜁and an arbi-
trary meromorphic function f . The contour
integral
∮C
d𝜁f
(
X𝜁+ 2iz + X
𝜁, 𝜁
)
,
X = x + iy, is easily seen to satisfy the
Laplace equation by virtue of
(𝜕2
x + 𝜕2
y + 𝜕2
z)f =
((
𝜁+ 1
𝜁
)2
−
(
𝜁−1
𝜁
)2
−4
)
f ′′ = 0.
If we write 𝜁= ei𝜙and choose the con-
tour to be a unit circle about 0, we obtain as

256
7 Special Functions
an instance of the above integral an expan-
sion in associated Legendre functions:
∫
2𝜋
0
d𝜙(z + ix cos 𝜙+ iy sin 𝜙)n cos(m𝜙)
= 2𝜋imn!
(n + m)!rnPm
n (cos 𝜃) cos(m𝜓),
𝜃and 𝜓being the usual azimuthal and
longitudinal coordinates of spherical polar
coordinates.
The integral representation above is also
of interest as an instance of the (mini-)
twistor transform that has proved funda-
mental to obtaining monopole solutions to
Yang–Mills–Higgs gauge theories [23].
7.3.5
Separation of Variables
In a general coordinate system, {x1, x2, x3},
it is customary to solve the Laplace
equation by choosing an ansatz,
Φ(x1, x2, x3) = X1(x1)X2(x2)X3(x3),
and decoupling the dependence of each
variable to obtain three ODE for X1, X2, and
X3 coupled only through some eigenvalue-
like parameters, the possible values of these
parameters being determined by boundary
or initial conditions. This method of sepa-
ration of variables is illustrated now for the
case of parabolic cylinder coordinates [24].
The pair of equations
y2 + 2𝜆x −𝜆2 = 0,
y2 −2𝜇x −𝜇2 = 0,
represent two mutually orthogonal families
of parabola. 𝜆and 𝜇are constant on each
parabola and, with z, constitute parabolic
cylinder coordinates on ℝ3. The standard
metric in these coordinates is,
ds2 = 𝜆+ 𝜇
4
(
d𝜆2
𝜆
+ d𝜇2
𝜇
)
+ dz2,
and using the general deﬁnition of the
Laplacian given earlier (Section 7.3.3) we
obtain,
ΔΦ =
4
𝜆+ 𝜇
(√
𝜆𝜕𝜆(
√
𝜆𝜕𝜆Φ)+
√
𝜇𝜕𝜇(
√
𝜇𝜕𝜇Φ)) + 𝜕2
zΦ.
Separation of variables,
Φ(𝜆, 𝜇, z) = Λ(𝜆)M(𝜇)Z(z),
and decoupling yields equations for Λ and
M of the form
d2w
du2 ± (1
4u2 ∓a)w = 0,
which are solved by Whittaker functions
[22].
7.3.6
Bessel Functions
We illustrate in this section how one starts
from representations of symmetries on
function spaces to arrive at the Bessel
diﬀerential equation [20, 21, 25–27]. The
rigid symmetries (isometries) 2 of the
plane, ℝ2, are generated by translations,
x →x + a and rotations, x →R(𝛼)x. The
general transformation we will denote
ga,𝛼∶x →R(𝛼)x + a. It is easy to see that
the group law is
ga,𝛼gb,𝛽= ga+R(𝛼)b,𝛼+𝛽.
This product of groups is a semi-direct
product, ℝ2 ⋊SO(2), of translation and
rotation subgroups, the former a normal
subgroup of the full group.

7.3 Continuous Symmetry
257
The corresponding Lie algebra is spanned
by a1 = 𝜕x, a2 = 𝜕y, and a3 = y𝜕x −x𝜕y, x
and y being coordinates on ℝ2. Commuta-
tion relations are:
[a1, a2] = 0
[a2, a3] = a1
[a3, a1] = a2.
By tensoring with ℂ, we may as well con-
sider the complex Lie algebra.
It is easily checked that for any z ∈ℂ,
there is a natural representation of 2
on smooth functions on the unit circle,
x ⋅x = 1, namely,
ga,𝛼⋅f (x) = eza⋅xf (R(−𝛼)x).
Since a ℂ-vector space basis of such
functions is given by the set
{vn ∶= ein𝜙|n ∈ℤ},
we have a representation of the Lie alge-
bra, obtained by diﬀerentiating the group
representation with respect to its three
parameters:
a1 ⋅vn = 1
2z(vn+1 + vn−1)
a2 ⋅vn = 1
2iz(vn+1 −vn−1)
a3 ⋅vn = −in vn
These relations show that for z ≠0, there is
no invariant subspace and so the function
space is an irreducible representation of 2.
Special functions are entries in the
matrix representation of this action. On
the function space, we have an inner
product,
(f1, f2) = 1
2𝜋∫
2𝜋
0
f 1f2 d𝜃
and we consider the functions
(ga,𝛼⋅vn, vm)
as functions of the group parameters
a ∈ℝ2 and 𝛼. Noting that these depend
only on the diﬀerences n −m and incor-
porating
a
conventional
multiplicative
factor, we deﬁne the Bessel functions in the
following way:
Jn(x) = 1
2𝜋∫
2𝜋
0
eix sin 𝜃−in𝜃d𝜃.
(We have exchanged the 𝜌of Section 7.3.3
for x.)
Note that
J−n(x) = (−1)nJn(x).
From this deﬁnition follow, by diﬀerentia-
tion with respect to x and by integration by
parts, the raising (D+
n ) and lowering (D−
n )
operators:
Jn+1(x) = D+
nJn(x)
Jn−1(x) = D−
nJn(x)
where
D+
n = n
x −d
dx
D−
n = n
x + d
dx
These operators commute, up to unit shift
in the index, in the sense that
D−
n+1D+
n = D+
n−1D−
n
and give a representation of the translation
algebra on the Bessel functions.
Further one sees that
D−
n+1D+
nJn = D+
n−1D−
nJn = Jn,

258
7 Special Functions
that is, the Bessel function Jn is an eigen-
function, with eigenvalue n2 of the operator
x2 d2
dx2 + x d
dx + x2.
We return to this “factorization" property in
Section 7.4.
7.3.7
Addition Laws
Addition laws between functions deﬁned
on groups arise because of the underlying
group operation [26, 27]. Thus if g1 and g2
are group elements of G, the matrix ele-
ments of a representation satisfy the homo-
morphism property
(g1g2 ⋅vn, vm) =
∑
l
(g1 ⋅vn, vl)(g2 ⋅vl, vm).
We may take a simple choice for the group
elements. In this case, take g1 to be the
translation
ga,0 ∶
a = (x1, 0)
and g2 the translation
ga,0 ∶
a = (x2 cos 𝜃2, x2 sin 𝜃2).
Then the product element is a translation
a = (x cos 𝜃, x sin 𝜃) with
x2 = x2
1 + x2
2 + 2x1x2 cos 𝜃2
and
ei𝜃= x1 + x2ei𝜃2
x
.
The deﬁnition of Bessel functions as matrix
elements then gives
ein𝜃Jn(x) =
∞
∑
k=−∞
eik𝜃2Jn−k(x1)Jk(x2).
From this general addition formula, simpler
ones follow by choices of parameter. For
example, if 𝜃2 = 0 then we obtain,
Jn(x1 + x2) =
∞
∑
k=−∞
Jn−k(x1)Jk(x2)
and for 𝜃2 = 𝜋∕2, we obtain
(x1 + ix2
x1 −ix2
)n∕2
Jn(
√
x2
1 + x2
2)
=
∞
∑
k=−∞
ikJn−k(x1)Jk(x2).
Finally, we cite the Jansen formula,
∞
∑
k=−∞
ikJn+k(x)Jk(x) = 𝛿n,0.
7.3.8
The Hypergeometric Equation
The hypergeometric family of functions,
F(𝛼, 𝛽, 𝛾; z), is universal for special func-
tions with three regular singular points in
the extended complex plane, that is, for
Fuchsian equations (Section 4.2.5)[21, 22,
25]. Many special functions appear as spe-
cial cases of this family.
Suppose
we
have
a
second-order
Fuchsian ODE, in z and w(z), with dis-
tinct regular singular points at z = a, b,
and c, carrying exponent pairs (roots of the
indicial equation in the Frobenius method),
(𝛼1, 𝛼2), (𝛽1, 𝛽2) and (𝛾1, 𝛾2). The Riemann
P-symbol denotes the set of solutions to
this ODE with these local properties:
P
⎧
⎪
⎨
⎪⎩
a
b
c
𝛼1
𝛽1
𝛾1
; z
𝛼2
𝛽2
𝛾2
⎫
⎪
⎬
⎪⎭
The exponents of the solutions of the Fuch-
sian equation, and hence the P-symbol, are

7.3 Continuous Symmetry
259
unaltered by Möbius maps on the indepen-
dent z-variable, except for the locations of
the singularities:
z →𝜆z + 𝜇
𝜈z + 𝜌,
a →a′, b →b′, c →c′,
P
⎧
⎪
⎨
⎪⎩
a
b
c
𝛼1
𝛽1
𝛾1
; z
𝛼2
𝛽2
𝛾2
⎫
⎪
⎬
⎪⎭
=
P
⎧
⎪
⎨
⎪⎩
a′
b′
c′
𝛼1
𝛽1
𝛾1
; z
𝛼2
𝛽2
𝛾2
⎫
⎪
⎬
⎪⎭
;
but under maps on the dependent variable
̃w(z) = (z −a)k(z −b)l
(z −c)k+l
w(z)
the symbol transforms as
(z −a)k(z −b)l
(z −c)k+l
P
⎧
⎪
⎨
⎪⎩
a
b
c
𝛼1
𝛽1
𝛾1
; z
𝛼2
𝛽2
𝛾2
⎫
⎪
⎬
⎪⎭
= P
⎧
⎪
⎨
⎪⎩
a
b
c
𝛼1 + k
𝛽1 + l
𝛾1 −k −l
; z
𝛼2 + k
𝛽2 + k
𝛾2 −k −l
⎫
⎪
⎬
⎪⎭
The hypergeometric diﬀerential equation is
z(1 −z)w′′ + (𝛾−(𝛼+ 𝛽+ 1)z)w′ −𝛼𝛽w = 0
which, it should be noted, is symmetric in
𝛼and 𝛽, and the hypergeometric function
solution obtained by the Frobenius method,
F(𝛼, 𝛽, 𝛾; z) =
∞
∑
0
(𝛼)n(𝛽)n
n!(𝛾)n
zn,
is valid for |z| < 1. The Pochhammer sym-
bols (⋅)n are deﬁned by
(𝛼)n = Γ(𝛼+ n)
Γ(𝛼)
,
(𝛼)0 = 1,
the gamma function being the analytic
extension of the factorial function
Γ(z) = ∫
∞
0
dte−ttz−1,
Re(z) > 0;
Γ(z + 1) = z!,
z ∈ℕ.
When 𝛾∉ℤ, the second solution inside the
open disk |z| < 1 is given by
z1−𝛾F(𝛼−𝛾+ 1, 𝛽−𝛾+ 1, 2 −𝛾; z).
The solution corresponds to the Riemann
P-symbol
P
⎧
⎪
⎨
⎪⎩
0
1
∞
0
0
𝛼
; z
1 −𝛾
𝛾−𝛼−𝛽
𝛽
⎫
⎪
⎬
⎪⎭
.
Many elementary and special functions
are associated with speciﬁc choices of
the parameters. When 𝛾= 𝛽, we recover
(rational) binomials. Also, for example,
arcsin z = zF
(
1
2, 1
2, 3
2; z2
)
;
ln 1 + z
1 −z = 2zF
(
1
2, 1, 3
2, z2
)
;
and the Legendre polynomials,
Pn(z) =
(2n)!
2n(n!)2 znF
(
n
2 , 1 −n
2
, 1
2 −n, 1
z2
)
.
A related class of function (including Bessel
functions) have irregular singular points at
inﬁnity that can be controlled by allowing
two of the regular singular points in the

260
7 Special Functions
hypergeometric function to coalesce. These
are the conﬂuent hypergeometric functions.
The hypergeometric functions satisfy
three-term recurrence relations:
(𝛾−1)F(𝛼, 𝛽, 𝛾−1, z) −𝛼F(𝛼+ 1, 𝛽, 𝛾, z)
−(𝛾−𝛼−1)F(𝛼, 𝛽, 𝛾, z) = 0
and
𝛾F(𝛼, 𝛽, 𝛾, z) −𝛽zF(𝛼, 𝛽+ 1, 𝛾+ 1, z)
−𝛾F(𝛼−1, 𝛽, 𝛾, z) = 0.
A further class of generalization is the
hypergeometric
series
in
two
sets
of
parameters,
rFs
(a1 … ar
b1, … bs
; z
)
=
∞
∑
n=0
(a1, … , ar)n
(b1, … , bs)n
zn
n!.
Here the symbols (a1, … , ar), and so on,
are deﬁned as products of the Pochhammer
symbols:
(a1, … , ar)n =
r∏
i=1
(ai)n;
and the previously deﬁned hypergeometric
function corresponds to
F(𝛼, 𝛽, 𝛾; z) = 2F1
(𝛼, 𝛽
𝛾
; z
)
.
7.3.9
Orthogonality
The generality of the classical special
functions
satisfy
the
Sturm–Liouville
equations:
L(y) ≡d
dx
(
k(x) dy
dx
)
+ (𝜆g(x) −l(x))y = 0.
k(x) > 0, l(x) and g(x) > 0 are real contin-
uous functions on an interval [a, b], 𝜆an
eigenvalue, and we assume boundary con-
ditions,
𝛼1y(a) + 𝛼2y′(a) = 0,
𝛽1y(b) + 𝛽2y′(b) = 0.
It can be shown that there is an unbounded,
inﬁnite set of eigenvalues {𝜆i}∞
0 with eigen-
functions {yn(x)}∞
0
such that yn(x) has
exactly n zeros in the interval (a, b) [9, 22].
The diﬀerential operator is self-adjoint:
∫
b
a
dx uLv = ∫
b
a
dx vLu.
From this follows an integral identity for
eigenfunctions,
(𝜆i −𝜆j) ∫
b
a
dx g(x)yiyj = 0,
so that eigenfunctions of distinct eigenval-
ues are orthogonal with respect to the inner
product
(u, v) = ∫
b
a
dx g(x)u(x)v(x).
In this way, one obtains, for instance, for the
associated Legendre functions, the orthog-
onality relations
∫
1
−1
dx Pm
n (x)Pm
r (x) =
2
2n + 1
(n + m)!
(n −m)!𝛿r,n.
Provided (yi, yi) > 0, we can normalize the
eigenfunctions by positive constants to
obtain an orthonormal set {yn(x)}∞
0 ,
(yi, yj) = 𝛿i,j.
7.3.10
Orthogonal Polynomials
Suppose we have a positive, Borel mea-
sure, d𝜇(x) = g(x)dx, on ℝ[21, 28–30]. We

7.4 Factorization
261
can always construct from it a sequence
{𝜙n(x)}∞
0 , of monic, orthogonal polynomi-
als with respect to this measure in the sense
that
∫ℝ
d𝜇𝜙n(x)𝜙m(x) = 𝜁n𝛿m,n,
where the numbers {𝜁n}∞
0 are all positive
and 𝜁0 = 1.
We achieve this by deﬁning Hankel deter-
minants,
Hn = det(𝜇i+j−2)1≤i,j≤n
the 𝜇j being moments of monomials with
respect to d𝜇(x) ∶
𝜇j = ∫ℝ
d𝜇(x) xj.
Then deﬁne
𝜙n(x) =
1
Δn−1
||||||||||||
𝜇0
𝜇1
…
𝜇n
𝜇1
𝜇2
…
𝜇n+1
⋮
⋮
𝜇n−1
𝜇n
…
𝜇2n−1
1
x
…
xn
||||||||||||
where Δn is the Hankel determinant,
Δn =
|||||||||
𝜇0
𝜇1
…
𝜇n
𝜇1
𝜇2
…
𝜇n+1
⋮
⋮
𝜇n
𝜇n+1
…
𝜇2n
|||||||||
.
These
polynomials
constitute
such
a
sequence with
𝜁n =
Δn
Δn−1
.
The sequence also satisﬁes a three-term
recurrence relation of the form
𝜙n+1 + (𝛼n −x)𝜙n + 𝛽n𝜙n−1 = 0,
the constants 𝛼n ∈ℝand 𝛽n > 0 for n > 0,
depending on the measure.
Further, the converse holds (the spectral
theorem): given such a sequence of monic,
orthogonal polynomials, satisfying a recur-
rence relation of this form, there exists a
measure such that {𝜙n(x)}∞
0 , can be con-
structed as above with
𝜁n = 𝛽1𝛽2 · · · 𝛽n.
Such orthogonal polynomials also satisfy
second-order, linear ODE and diﬀerential-
diﬀerence recurrence relations.
One example, amongst many topical in
the current literature because of their q-
deformed cousins is the sequence of Jacobi
polynomials on [−1, 1]:
g(x) = (1 −x)𝛼(1 + x)𝛽Γ(𝛼+ 𝛽+ 2)
2𝛼+𝛽+1Γ(𝛼+ 1)Γ(𝛽+ 1)
,
pn(x) =
P(𝛼,𝛽)
n
(x)
√
h(𝛼,𝛽)
n
,
h(𝛼,𝛽)
n
=
(𝛼+ 𝛽+ 1)(𝛼+ 1)n(𝛽+ 1)n
(2n + 𝛼+ 𝛽+ 1)n!(𝛼+ 𝛽+ 1)n
,
and
P(𝛼,𝛽)
n
(x)= (𝛼+ 1)n
n!
2F1
(−n, 𝛼+ 𝛽+ n + 1
𝛼+ 1
; x
)
7.4
Factorization
The linear diﬀerential equations satisﬁed by
special functions of the type described are
all factorizable and this property provides a
neat way to obtain functional properties of
the special functions in question. Originally
driven by the analogy with factorizing poly-
nomial equations, this method is developed
and its applications in quantum mechanics
explored in [31].

262
7 Special Functions
7.4.1
The Bessel Equation
We have seen already that the Bessel oper-
ator factorizes up to a constant
𝜕2
x + 1
x𝜕x + (1 −m2
x2 ) =
(𝜕x −m −1
x
)(𝜕x + m
x ) −1.
Denoting the factors, as before (Section
7.3.6) by
D±
m = ∓𝜕x + m
x
and the Bessel function, Jm, we have
D+
m−1D−
mJm = D−
m+1D+
mJm = Jm.
Because this holds for all m, it is the case
that
D+
mJm = Jm+1
D−
mJm = Jm−1,
which are recurrence relations for the
Bessel functions. It is easy to obtain a
generating function for them. Deﬁne
J(z) =
∑
m∈ℤ
zmJm(x).
Then, summing the recurrence formulae
above over zm,
𝜕xJ(z) = 1
2(z −1
z )J(x)
and so
J(z) = exp ((z −1
z )x
2
).
7.4.2
Hermite
The wave function, 𝜓, of the quantum har-
monic oscillator of a speciﬁed energy level
(𝜆l) obeys the equation,
(−𝜕2
x + x2)𝜓l = 𝜆l𝜓l.
We can write this in either of two factorized
forms:
−(𝜕x −x)(𝜕x + x)𝜓l = (𝜆l −1)𝜓l;
and
−(𝜕x + x)(𝜕x −x)𝜓l = (𝜆l + 1)𝜓l.
Writing D+ = 𝜕x −x and D−= 𝜕x + x, and
noting that
[D−, D+] = 2,
one has the system
D−𝜓l = (𝜆l −1)𝜓l−1
D+𝜓l−1 = (𝜆l + 1)𝜓l.
If consequently, for each value of l, 𝜓l sat-
isﬁes the diﬀerential equation with eigen-
value 𝜆l, these eigenfunctions are related
by raising (creation) and lowering (annihi-
lation) operators, D−and D+ and by the
eigenvalue relation 𝜆l+1 = 𝜆l + 2.
Assume there is a highest weight vec-
tor, 𝜓0, in the kernel of D−. Then D−𝜓0 =
0 implies 𝜓0 = e−1∕2x2 and 𝜆0 = 1. Applica-
tion of D+ l times to 𝜓0 will yield a prod-
uct of a polynomial Hl(x) and the Gaussian
exponential:
𝜓l = (D+)l𝜓0 = Hl(x)e−x2∕2,
an eigenfunction of eigenvalue 2l + 1.
The Hl(x) are the Hermite polynomials,
which satisfy the ODE
H′′
l −2xH′
l + 2lHl = 0

7.4 Factorization
263
obtained from the oscillator via the above
substitution for 𝜓l.
Note that as operators we may write
D+ = ex2∕2𝜕xe−x2∕2,
so that
(D+)l = (ex2∕2𝜕xe−x2∕2) · · · (ex2∕2𝜕xe−x2∕2)
= ex2∕2𝜕l
xe−x2∕2.
Combining these, we get a formula for the
lth Hermite polynomial:
Hl(x) = ex2𝜕l
x(e−x2).
Using 𝜓l−1, 𝜓l, and 𝜓l+1 to eliminate deriva-
tives in the system for 𝜓l, one replaces
the second-order diﬀerential equation by
a three-term recurrence relation for the
Hl(x) ∶
Hl+1 −2xHl + 2lHl−1 = 0.
Such a relation is the basis for a continued
fraction expansion. Put hl =
Hl
Hl−1 so that the
recurrence relation becomes
hl+1 = 2x −2l
hl
.
Then
hl = 2x −
2(l −1)
2x −2(l−2)
2x−···
.
Finally, we may obtain a generating func-
tion. Start with the system form for the Hl,
H′
l = 2lHl−1,
H′
l −2xHl = −Hl+1,
which corresponds to the factorizations, up
to constants, of the H-equation,
𝜕x(𝜕x −2x)Hl = −2(l + 1)Hl
and
(𝜕x −2x)𝜕xHl = −2lHl.
The generating function
H(x, t) =
∞
∑
l=0
tl
l!Hl(x)
is seen to satisfy the compatible pair of
PDEs
Hx = 2tH,
Ht = 2(x −t)H,
whose solution is
H(x, t) = e−t2+2xt.
7.4.3
Legendre
In this case, introduce the linear diﬀerential
operators [32]
L = ei𝜓(𝜕𝜃+ i cot 𝜃𝜕𝜓))
L∗= −e−i𝜓(𝜕𝜃−i cot 𝜃𝜕𝜓))
M = 1
i 𝜕𝜓.
Then
LL∗= −Δ −M(M −1)
L∗L = −Δ −M(M + 1),
where
Δ = 𝜕2
𝜃+ cot 𝜃𝜕𝜃+
1
sin2 𝜃
𝜕2
𝜓
is the Laplacian in spherical coordinates on
the sphere (constant r).
The operators {L, L∗, M} form a repre-
sentation of the Lie algebra su(2) of the Lie
group SU(2) of unitary, 2 × 2 matrices of

264
7 Special Functions
unit determinant, as is seen from the com-
mutation relations:
[M, L] = L,
[M, L∗] = −L∗,
[L, L∗] = 2M.
The equivalence is given by the correspon-
dence
L ∼
( 0
i
0
0
)
L∗∼
( 0
0
−i
0
)
M ∼1
2
( 1
0
0
−1
)
One checks that the generators commute
with the Laplacian:
[L, Δ] = [L∗, Δ] = 0,
[M, Δ] = 0.
In fact, in this sense, the Δ operator
is a Casimir element of the universal
enveloping algebra of su(2): it commutes
with every generator and its eigenvalues,
l(l + 1), indicate the dimension d of a
(ﬁnite-dimensional) representation via the
formula,
l(l + 1) = 1
4(d2 −1).
Because Δ and M commute, one may form
a pair of mutually consistent diﬀerential
equations
MY m
l
= mY m
l ,
and
ΔY m
l
= −𝜆(l)Y m
l ,
deﬁning functions Y m
l
where the depen-
dence of 𝜆on l is to be determined.
It is easy to show, using the commutation
relations, that the function LY m
l satisﬁes
M(LY m
l ) = (m + 1)LY m
l ,
Δ(LY m
l ) = −𝜆(l)(LY m
l ),
meaning that L plays the rôle of a raising
operator:
LY m
l
= Y m+1
l
.
In a similar manner, L∗is a lowering opera-
tor:
L∗Y m
l
= Y m−1
l
.
If we assume that the set {Y m
l |l′ ≤m ≤l}
forms the basis of a ﬁnite-dimensional rep-
resentation of su(2), then we will have a
highest weight element Y l
l satisfying
LY l
l = 0.
Using the identity for L∗L yields
𝜆(l) = l(l + 1).
From the theory of ﬁnite-dimensional
representations,
we
conclude that
the
set {Y m
l | −l ≤m ≤l} is the basis for an
irreducible representation of su(2) of (odd)
dimension 2l + 1. These are called integer
spin representations. Representations of
even dimension (half integer spin) require
spinors [33].
Solving the deﬁning equations for the
Y m
l (𝜃, 𝜓) gives (up to normalization) the
spherical harmonics
Y m
l (𝜃, 𝜓) = eim𝜓Pm
l (𝜃).
The Pm
l (𝜃) are associated Legendre func-
tions and satisfy the associated Legendre

7.4 Factorization
265
equation:
1
sin 𝜃𝜕𝜃(sin 𝜃𝜕𝜃Pm
l (𝜃)) +
(l(l + 1) −
m2
sin2 𝜃
)Pm
l (𝜃) = 0
7.4.4
“Factorization” of PDEs
The idea of factorization ﬁnds a general
context in the theory of the linear PDEs
that eﬀect the inverse scattering transform
for integrable nonlinear PDEs [34–36]. The
special character of the soliton solutions to
such integrable equations allows us to think
of them as nonlinear special functions.
Given a general second-order PDE of
hyperbolic type in two independent vari-
ables and in canonical form,
L𝜙≡(𝜕x𝜕y + a𝜕x + b𝜕y + c)𝜙= 0,
a and b being functions of x and y, we can
attempt a natural factorization in two ways:
((𝜕x + b)(𝜕y + a) −h)𝜙= 0,
((𝜕y + a)(𝜕x + b) −k)𝜙= 0.
The functions
h = ab + ax −c
k = ab + by −c
are invariants under linear scalings of 𝜙∶
𝜙→𝜆(x, y)𝜙
and they measure the obstruction to factor-
izing the linear diﬀerential operator. Van-
ishing of either allows the reduction of the
second-order equation to triangular form
and the general integration problem to a
pair of quadratures.
The invariants classify operators up
to scaling maps: two distinct operators
sharing the same invariants are necessarily
related by such a scaling map.
From each of the forms, we can deﬁne
new independent variables
𝜙𝜎= (𝜕y + a)𝜙
and
𝜙Σ = (𝜕x + b)𝜙
(the Laplace transforms of 𝜙) satisfying
(𝜕x + b)𝜙𝜎= h𝜙
and
(𝜕y + a)𝜙Σ = k𝜙.
Eliminating 𝜙for 𝜙𝜎or 𝜙Σ yields in each
case a new equation of the original type but
with redeﬁned parameters:
a𝜎= a −(ln h)y,
b𝜎= b
h𝜎= 2h −k −(ln h)xy,
k𝜎= h
and
aΣ = a,
bΣ = b −(ln k)x
hΣ = k,
kΣ = 2k −h −(ln k)xy.
One checks straightforwardly that, for the
invariants,
(k𝜎)Σ = (kΣ)𝜎,
(h𝜎)Σ = (hΣ)𝜎.
However the (noninvariant) coeﬃcients are
not well behaved: (a𝜎)Σ ≠a, and so on. This
is to be expected as scaling transforma-
tions cannot alter the factorization prop-
erties of the second-order operator even
though they alter coeﬃcients.

266
7 Special Functions
Repetitions of Laplace maps generate
sequences of pairs of invariants. So we label
them
hn = h𝜎n,
kn = k𝜎n
subject to the understanding that
h𝜎−1 = hΣ
and so on. Then we may write three-term
recurrence relations for the hn and kn ∶
hn+1 −2hn + hn−1 = (ln hn)xy
kn+1 −2kn + kn−1 = (ln kn)xy
and these are a particular form of the two-
dimensional Toda ﬁeld equations studied in
the theory of integrable systems [37, 38].
The family of invariants is associated
with a rank-one lattice, which may be
inﬁnite, semi-inﬁnite or ﬁnite depending
on whether there are vanishing invariants
for some value(s) of n.
Suppose we consider the formal adjoint
of the given second-order diﬀerential oper-
ator, L:
L† = 𝜕x𝜕y −a𝜕x −b𝜕y + c −ax −by.
Its invariants are
h† = k,
k† = h.
and it is easy to check that
h†𝜎= hΣ†,
k†𝜎= kΣ†
so that the operations 𝜎= Σ−1 and † gener-
ate the inﬁnite dihedral group:
D∞= < †, 𝜎|𝜎† 𝜎= †, †2 = id > .
If we start with a second-order hyper-
bolic operator that is self-adjoint, then the
sequence, or one-dimensional lattice, of
invariants generated by Laplace maps is
symmetric in the sense that,
hn = h−n,
kn = k−n.
If at some point an invariant vanishes, then
the lattice is ﬁnite with an odd number of
sites, the central site corresponding to the
self-adjoint operator –a situation reminis-
cent of the Legendre equation with eigen-
value l(l + 1).
In such situations (and others where
a modiﬁed symmetry obtains), there is a
natural map between lattices of diﬀerent
lengths. If L is self-adjoint then h = k and it
can, by a scaling transformation, be written
in the form
L = 𝜕x𝜕y + h = 𝜕x𝜕y + k.
Assume 𝜙0 is a function in the kernel of this
operator and consider the coupled system
(𝜕x + 𝜙−1
0 𝜙0x)𝜙𝜇= −(𝜕x −𝜙−1
0 𝜙0x)𝜙
(𝜕y + 𝜙−1
0 𝜙0y)𝜙𝜇= (𝜕y −𝜙−1
0 𝜙0y)𝜙
for a pair of functions 𝜙and 𝜙𝜇. Elimination
of 𝜙𝜇gives the original equation
(𝜕x𝜕y + h)𝜙= 0,
but elimination of 𝜙gives
(𝜕x𝜕y + h + 2(ln 𝜙0)xy)𝜙𝜇= 0,
a new, self-adjoint equation with invariant
h𝜇= k𝜇= h + 2(ln 𝜙0)xy,
𝜙0 being a solution of the original self-
adjoint equation. This so-called Moutard
[35] transformation is diﬀerent in charac-
ter to the Laplace transformation because

7.4 Factorization
267
it takes us outside the diﬀerential alge-
bra generated by the invariants alone. In
general, the lattice associated with h𝜇will
have a diﬀerent character to that associated
with h.
The Moutard transformation can be
reduced to a one-dimensional situation
by assuming a symmetry of the form
h(x, y) = h(x + y)
and
correspondingly
𝜙(x, y) = e𝜆(x−y)𝜓(x + y). Then 𝜓satisﬁes
𝜓′′ + h𝜓= 𝜆2𝜓.
Following the previous prescription, set
(𝜕−𝛼)𝜓= 𝜓𝛿,
(𝜕+ 𝛼)𝜓𝛿= 𝜆2𝜓
with
−𝛼′ −𝛼2 = h −𝜇2 + 𝜆2.
It follows that 𝛼= 𝜓′
0∕𝜓0 where
𝜓′′
0 + h𝜓0 = 𝜇2𝜓0
and 𝜓𝛿satisﬁes
𝜓𝛿′′ + h𝛿𝜓𝛿= 𝜆2𝜓𝛿
with
h𝛿= h + 2(ln 𝜓0)′′.
This is the Darboux transformation, a map
of enormous interest in the theory of soli-
ton equations [39].
If we regard soliton solutions as spe-
cial functions, parameterized by positions,
momenta, and soliton number, for nonlin-
ear equations then Darboux transforma-
tions are analogous to the raising operators
in the classical theory.
7.4.5
Dunkl Operators
Dunkl operators provide an approach to
multivariable generalizations of the clas-
sical special functions via ﬁnite reﬂection
groups [40–42]. Given such a group, W,
with positive roots Δ+ ⊂ℝN, so
W =< 𝜎𝛼|𝛼∈Δ+ >,
the Dunkl operators Ti are diﬀerential-
diﬀerence operators acting on functions,
f ∶ℝN →ℝ∶
Tif (x) = 𝜕xif (x) +
∑
𝛼∈Δ+
k𝛼
f (x) −f (𝜎𝛼(x))
< x, 𝛼>
𝛼i,
where x ∈ℝN, 𝛼i denotes the ith compo-
nent of 𝛼, k𝛼is a multiplicity function (the
cardinality of the conjugacy class of 𝜎𝛼in
W), and < ⋅, ⋅> denotes the inner product
on ℝN.
For example, in the case of the root sys-
tem AN−1, presented earlier (Section 7.2.4),
Ti = 𝜕xi + k
∑
i≠j
1 −𝜎(ij)
xi −xj
,
𝜎(ij) acting on f by interchange of the ith and
jth arguments.
A crucial property of the Dunkl operators
is that they commute:
[Ti, Tj] = 0.
The Dunkl Laplacian is deﬁned to be
Δk =
N
∑
i=1
T2
i
and the generalized special functions in
mind are harmonic with respect to this
Laplacian.

268
7 Special Functions
Thus for the example of AN−1,
Δk = Δ +
2k
∑
i<j
1
xi −xj
(
𝜕xi −𝜕xj −
1 −𝜎(ij)
xi −xj
)
.
For A1 we have the expression,
𝜕2 + 2k
x 𝜕,
and so Δk generalizes to many variables the
classical (spherical) Bessel function.
Important players in the theory are the
intertwining operators, Vk ∶
TiVk = Vk𝜕xi
which, like the Ti themselves are W-
equivariant, and the Dunkl kernel, E(⋅, y),
the unique solution to the set of PDE,
Tif = yif ,
i = 1, … , N.
In terms of E, the generalized Bessel func-
tion is deﬁned, for x, y ∈ℝN, as
Jk(x, y) =
1
|W|
∑
g∈W
Ek(gx, y).
7.5
Special Functions Without Symmetry
In this section, we present theories of a
more general character of which diﬀerential
Galois theory is arguably the most inclu-
sive. It would be invidious to claim that
there is no debt to symmetry but its rôle is
less obviously geometric.
7.5.1
Airy Functions
If a singular point of an ODE is not regular,
then it is called irregular and if a (complex)
solution of the form
y(x) = eQ(x)w(x)
exists near the singular point (assumed to
be x = 0), Q(x) being polynomial in 1∕x and
w(x) being regular, then it is called a nor-
mal solution [21, 43–45]. Such solutions
need not exist. Note that even so simple
an equation as a linear ODE with constant
coeﬃcients falls into this class, the point
at inﬁnity being an irregular singular point
and Q(x) linear in x. The normal solution
has an essential singularity.
If the equation has a normal solution in
the variable x1∕k for some positive integer
k > 1 (Puiseux expansion), then the solu-
tion is called subnormal.
An example of such a situation is the Airy
equation,
y′′(x) −xy(x) = 0,
which has an irregular singular point at
inﬁnity. Linearly independent solutions are
denoted
Ai(x),
Bi(x).
They have Puiseux expansions expressed
via fractional Bessel functions:
Ai(−x)=1∕3
√
x(J1∕3(2∕3x3∕2)+J−1∕3(2∕3x3∕2));
Bi(−x)=1∕3
√
x(J−1∕3(2∕3x3∕2)−J1∕3(2∕3x3∕2)).
There are also integral representations:
(3a)−1∕3𝜋Ai(±(3a)−1∕3x) =
∫
∞
0
dt cos(at3 ± xt);
and
(3a)−1∕3𝜋Bi(±(3a)−1∕3x) =
= ∫
∞
0
dt
(
sin(at3 ± xt) + e−at3±xt)
.

7.5 Special Functions Without Symmetry
269
7.5.1.1
Stokes Phenomenon
This is an important feature of functions
with essential singularities. The coeﬃcients
in the Airy equation being entire on ℂand
the equation being linear, it follows that
the general solution is likewise entire on ℂ.
However, asymptotic representations of the
solutions,
y±(x) ≈x−1∕4e±2∕3x3∕2,
y′′
± −xy ≈x−9∕4e±2∕3x3∕2,
are multivalued,
y± →iy∓,
as x traverses a closed loop about x = 0
(equivalently, x = ∞.)
Note that unlike the Fuchsian case where
the multivaluedness is a genuine property
of the solution near a regular singular point,
this behavior is an artifact of using multival-
ued approximations near an essential sin-
gularity. One avoids the confusion created
by this representation dependence by allo-
cating representations to speciﬁc sectors of
ℂseparated by Stokes lines.
In the case of the Airy functions, the
asymptotic forms are
Ai(x) ≡1
2𝜋−1∕2x−1∕4e2∕3x3∕2,
for | arg x| < 𝜋, and
Ai(−x) ≡𝜋−1∕2x−1∕4 sin
(
2
3x3∕2 + 𝜋
4
)
for | arg x| < 2∕3𝜋.
7.5.2
Liouville Theory
Just as in number theory there are hierar-
chies of number system,
ℕ⊂ℤ⊂ℚ⊂· · · ⊂ℝ,
so in function theory too it is helpful to
distinguish
elementary
functions
from
higher functions so that we can answer
the question as to whether a given diﬀer-
ential equation has solutions that cannot
be deﬁned by more elementary means
[46, 47].
In the case of number theory, the positive
integers, ℕ, taken as a starting point, allow
us to develop the full set of integers ℤand
the rationals ℚas extensions that provide
solutions to equations
z + n = 0,
n ∈ℕ,
and
az + b = 0,
a, b ∈ℤ, a ≠0.
Algebraic numbers are deﬁned as real solu-
tions to polynomial equations over ℚin one
variable,
f (z) = 0,
f ∈ℚ[z].
Beyond
this
point,
completions
with
respect to some norm construct the real,
ℝ, and p-adic, Ωp, numbers. We further
extend to complex versions of any of
these systems by allowing solutions to the
quadratic equation,
z2 + 1 = 0.
For functions, we may proceed in an
algebraically
similar
manner.
Certain
operations are taken as elementary: the
construction of an algebraic function, that
is, a function y(x) satisfying an equation
polynomial in x and y; the exponentiation
of a function, that is, solution of the ﬁrst-
order ODE, y′ + f ′y = 0; the taking of a
logarithm, that is, solution of an equation
ey = f (x).

270
7 Special Functions
In his theory of functions, Liouville deﬁnes
an elementary function as one constructed
by a ﬁnite sequence of such operations and
the order of such a function as the minimal
number of exponentiations or taking of
logarithms in that process. Algebraic oper-
ations are neutral: they do not aﬀect the
order.
We now give some applications of Liou-
ville’s approach to special functions.
A theorem of Abel [48] states that if the
integral, along some path in ℂ, of an alge-
braic function,
∫dx y(x),
is itself algebraic, then it is necessarily ratio-
nal in x and y. We can use this to show
that the Jacobi elliptic functions (Section
7.6.2) are not elementary. Assume that y is
deﬁned by
y2 = (1 −x2)(1 −k2x2).
If the integral of y is rational, then according
to Abel’s result, it must be of the form,
∫
dx
y(x) = A(x) + B(x)y,
A and B being rational in x. Diﬀerentiation
with respect to x allows us to write yA′ as
a function rational in x ∶a contradiction,
unless A = 0. Further analysis of the pos-
sible poles in the relation leads to a simi-
lar contradiction. Hence the Jacobi elliptic
functions are not elementary.
Consider secondly the Gaussian integral,
erf(x) = ∫
x
−∞
dt e−t2.
On the basis of Liouville’s theory, it must be
of the form
w(x)e−x2 + const,
for a rational w(x), if it is to be elementary
at all. But then w must satisfy the ODE,
1 = w′ −2x,
and it is straightforward to show that such
a w has no poles either in the ﬁnite part of
ℂor at ∞. Hence, by the Liouville theorem
of complex analysis, it is constant and we
have a contradiction: the erf function is not
elementary.
Finally, we will show that Bessel func-
tions are not in general elementary in an
extended sense (Liouville). This requires a
result concerning Riccati equations:
y′ + y2 = P(x),
P(x) being algebraic. We generalize the
notion of elementary functions by allow-
ing integrals. Since the logarithm of an
algebraic function is also the integral of
an algebraic function, we loosen the pre-
vious deﬁnition by allowing more general
integrals of algebraic functions and call
the class of extended elementary functions
Liouville functions. The order is deﬁned as
before but replacing the word “logarithm”
by “integral.”
The result we require is that if the given
Riccati equation has a particular solution
that is Liouville, then it has a particular
solution that is algebraic.
In the case of Bessel’s equation (Section
7.3.6), the transformation y = u∕
√
x, x = iz
yields
d2u
dz2 =
(
1 + l(l + 1)
z2
)
u,
for l = n −1∕2 and this is reduced to a
Riccati equation,
dv
dz + v2 = 1 + l(l + 1)
z2
,

7.5 Special Functions Without Symmetry
271
by the diﬀerential substitution v = u′∕u.
(In fact, any second-order, linear, homoge-
neous ODE may be reduced to a Riccati
equation by a similar substitution.)
To settle the question of whether the
Bessel equation has Liouvillian solutions,
we need to see whether it has algebraic
solutions or not for given values of l. Using
expansions of v, one ﬁrstly determines that
any algebraic solution is rational in z. Sec-
ondly, it is shown that a rational solution
exists if and only if l is integral. Conse-
quently, the Bessel equation admits a non-
trivial Liouvillian solution (and hence all
solutions are Liouville) if and only if 2n is
an odd integer.
7.5.3
Diﬀerential Galois Theory
Strictly speaking, the diﬀerential Galois
theory is concerned with questions of
symmetry but not in the geometrical sense
described earlier. It is also a development
of the Liouville theory [11, 49–52].
It was from the question of the solvability
of polynomial equations in a single variable
that group theory originally arose. Solvabil-
ity here means obtaining expressions for
solutions in terms of rational operations
augmented by the extraction of simple alge-
braic roots. One associates with an irre-
ducible polynomial with coeﬃcients in a
ﬁeld 𝔉, p(x) ∈𝔉[x], the group of automor-
phisms, G, of the ﬁeld 𝔏= 𝔉[x]∕(p), where
(p) is the ideal generated by p(x), which ﬁx
𝔉elementwise. If 𝔉is exactly the set of
points ﬁxed by G, then 𝔏is called a normal
extension of 𝔉and G is the Galois group,
Gal(𝔏∕𝔉) [53].
A group is said to have a normal series if
it has a ﬁnite sequence of subgroups, Gi,
G = G0 ⊃G1 ⊃G2 ⊃· · · ⊃Gn = {e}
such that each Gi is normal in Gi−1. G is
solvable if it has a normal series where each
quotient Gi+1∕Gi is abelian.
The fundamental result of Galois theory
is that the equation p(x) = 0 is solvable by
radicals over 𝔉if and only if the Galois
group Gal(𝔏∕𝔉) is solvable. The fact that
the symmetric groups Sn for n ≥5 are not
solvable accounts for the fact that there is
no generic formula for extracting the roots
of polynomial equations of degree ﬁve or
more.
One thinks of the Galois group as a sym-
metry group “permuting" the elements of a
set while ﬁxing those of a subset. It was the
link with solvability that inspired Lie [13]
to develop a group theory for continuous
symmetries (already discussed) and Picard
and Vessiot to develop a diﬀerential theory
closer in spirit to the Galois theory.
In diﬀerential Galois theory, 𝔉is taken to
be a diﬀerential ﬁeld, that is, it is a ﬁeld with
derivation
D ∶𝔉→𝔉,
D(a + b) = D(a) + D(b),
D(ab) = aD(b) + D(a)b.
Homomorphisms, in particular automor-
phisms, between diﬀerential ﬁelds,
𝜙∶𝔉→𝔎,
are required to satisfy the property,
𝜙D𝔉= D𝔎𝜙.
The analogue of the polynomial equation
in Galois theory is a linear diﬀerential
equation and the analogue of 𝔏, is the cor-
responding Picard–Vessiot extension. This
is the diﬀerential ﬁeld generated by a full
set of linearly independent (over constants)
solutions of the diﬀerential equation, say,

272
7 Special Functions
{y1, y2, … , yn} for a diﬀerential equation of
order n. Such a set of solutions must have
nonvanishing Wronskian:
|||||||||
y1
y2
…
yn
D(y1)
D(y2)
…
D(yn)
⋮
⋮
Dn−1(y1)
Dn−1(y2)
…
Dn−1(yn)
|||||||||
≠0.
We write the Picard–Vessiot extension as
𝔉⟨y1, y2, … , yn⟩.
It consists of rational, diﬀerential functions
of the yi with coeﬃcients in the ground
ﬁeld, 𝔉, subject to the constraint of the
linear diﬀerential equation. It is important
that this ﬁeld extension of 𝔉contain no
new constants and that the ﬁeld of constants
(ker(D)) is algebraically closed. For such
extensions, the Galois group of diﬀerential
automorphisms ﬁxing 𝔉is deﬁned in anal-
ogy to the polynomial case. If the diﬀeren-
tial Galois group has a certain “solvability"
property, then the Picard–Vessiot exten-
sion can be decomposed into a sequence
of subﬁelds, 𝔏i−1 ⊃𝔏i, each of which is a
simple extension of the subsequent one in
the sense that, à la Liouville theory, it can
be constructed by adjoining solutions of
either D(y) + ay = 0 or D(y) + a = 0, where
a ∈𝔏i.
As an example, consider the (Fuchsian)
Cauchy equation,
x2y′′ + axy′ + by = 0,
a and b belonging to the ﬁeld of constants
C ⊂𝔉.
Let m1 and m2 be the roots of
m2 + (a −1)m + b = 0.
If we deﬁne functions y1 and y2 to be lin-
early independent solutions of
xy′
i = miyi,
1 = 1, 2,
then each satisﬁes the Cauchy equation.
In
the
case
that
m1 ≠m2,
the
Picard–Vessiot extension is the diﬀer-
ential ﬁeld, 𝔏= 𝔉(y1, y2), C being the ﬁeld
of constants, the diﬀerential Galois group
will be C × C acting multiplicatively and
diagonally,
y1 →c1y1,
y2 →c2y2,
unless y1 and y2 are algebraically dependent
(satisfy a polynomial equation f (y1, y2) = 0
over 𝔉), in which case the Galois group is a
proper subgroup of C × C.
In
the
case
that
m1 = m2 = m,
𝔏= 𝔉(y, u) where y and u are solutions to
xy′ = my,
xu′ = 1,
and the diﬀerential Galois group is C × C′,
the second factor acting additively,
y →cy,
u →u + d.
Obviously, these are all solvable cases. If
C = ℂ, then the diﬀerential Galois group
will be isomorphic to the monodromy
group (Section 7.2.5).
7.6
Nonlinear Special Functions
7.6.1
Weierstraß Elliptic Functions
One encounters the Weierstraß ℘-function
as soon as one departs from the lin-
ear oscillator model, that is, as soon
as the amplitude of oscillations can no

7.6 Nonlinear Special Functions
273
longer be regarded as “small." Histori-
cally, the ℘-function arose from exact
analytic treatments of mechanical systems
[21, 54–56].
The second-order linear equation
y′′ + y = 0
is the equation of simple harmonic motion,
which governs small oscillations. Integrat-
ing it once, we can think of sin 𝜃and cos 𝜃
functions as solutions of the ODE y′2 = 1 −
y2 under appropriate boundary conditions.
The most natural form of solution is
𝜃= ∫
sin 𝜃
0
dy
√
1 −y2 ,
that is, we deﬁne by the integral the func-
tion inverse to sin. The integral is taken
around a path in the complex plane avoid-
ing a cut from −1 to 1 . A closed path
around the cut returns to its starting point
but 𝜃is augmented by the amount
2 ∫
1
0
dy
√
1 −y2 = 2𝜋,
the periodicity of the circular function.
An obvious simple nonlinear generaliza-
tion of this situation is
y′′ = 6y2 −1
2g2.
(The prefactor of 6 reﬂects a choice of
canonical form and g2 is a constant.) Inte-
grating yields
y′2 = 4y3 −g2y −g3,
g3 another constant, which is an equation
deﬁning, in the complex plane, a doubly
periodic function with a double pole that
can be chosen to lie at the origin. This is
the Weierstraß ℘-function. As with circu-
lar functions, we can present an inverse def-
inition:
u = ∫
℘(u)
∞
dy
√
4y3 −g2y −g3
,
or, equivalently,
℘′2 = 4℘3 −g2℘−g3.
This time, there are cuts between a pair
of roots of the cubic in y and between the
third root and the point at inﬁnity. Integrat-
ing around closed loops in the plane avoid-
ing these cuts augments the variable u by
points in a lattice Λ = 2𝜔1ℤ+ 2𝜔2ℤ, the
𝜔i being two fundamental complex half-
periods (linearly independent over ℝ) of the
℘-function.
The ℘-function can also be constructed
as an inﬁnite sum,
℘(u) = 1
z2 +
′∑
m,n
(
1
(z −𝜔m,n)2 −
1
𝜔2
m,n
)
,
𝜔m,n being deﬁned as
𝜔m,n = 2m𝜔1 + 2n𝜔2
and the primed sum denoting omission of
the term 𝜔0,0.
The
parameters
in
the
diﬀerential
equation above, satisﬁed by the ℘-function,
are related to the lattice parameters via
inﬁnite sums:
g2 = 60
′∑
m,n
1
𝜔4
m,n
;
g3 = 140
′∑
m,n
1
𝜔6
m,n
;
As for circular functions, there is a (nonlin-
ear) addition law:

274
7 Special Functions
℘(u) + ℘(v) + ℘(u + v)
= 1
4
(℘′(u) −℘′(v)
℘(u) −℘(v)
)2
In more symmetric form,
|||||||
1
℘(u)
℘′(u)
1
℘(v)
℘′(v)
1
℘(u + v)
−℘′(u + v)
|||||||
= 0
Geometrically, the addition law expresses
the fact that the genus one, plane cubic
curve
z2 = 4y3 −g2y −g3
is parameterized by the choice (y, z) =
(℘, ℘′) and the point corresponding to
parameter u + v is the reﬂection in the
x-axis of the third intersection point with
the cubic of a straight line through those
points corresponding to u and v.
Thus let (y1, z1) and (y2, z2) be a pair of
points on the cubic curve. The straight line
passing through these points is
z = 𝛼+ 𝛽y,
where
𝛼= y1z2 −y2z1
y1 −y2
and
𝛽= z1 −z2
y1 −y2
.
The third point of intersection (y3, z3) arises
from solving
4y3 −g2y −g3 −(𝛼+ 𝛽y)2 =
4(y −y1)(y −y2)(y −y3) = 0,
so that
y1 + y2 + y3 = 1
4𝛽2.
The construction is that the “sum” of (y1, z1
and (y2, z2) is (y3, z3) where z = −z is the
reﬂection of z in the y-axis. So
y3 = −y1 −y2 + 1
4
( z1 −z2
y1 −y2
)2
,
and the expression
z3 = −𝛼−𝛽y3
simpliﬁes to the determinantal form given
above under the identiﬁcations of y with ℘
and z with ℘′.
Any doubly periodic function on ℂ
must have poles. Otherwise, by Liouville’s
theorem, it would be constant. All such
functions form a diﬀerential ﬁeld generated
by ℘(u): any such function can be written
as
A(℘) + B(℘)℘′
where A and B are rational functions over
ℂ.
The ℘-function is the primary example
of an abelian function: it is meromorphic,
single-valued, and (doubly) periodic.
Two useful, but nonperiodic, functions
are the 𝜁and the Weierstraß 𝜎functions:
℘(u) = −d
du𝜁(u),
𝜁(u) −1
u →0, u →0;
𝜁(u) = d
du ln 𝜎(u),
𝜎(u)
u
→1, u →0.
Thus,
℘(u) = −d2
du2 ln 𝜎(u).
The 𝜁-function fails to be periodic:
𝜁(u + 2𝜔i) = 𝜁(u) + 2𝜂i,
i = 1, 2

7.6 Nonlinear Special Functions
275
where 𝜂i = 𝜁(𝜔i) and
𝜂1𝜔2 −𝜂2𝜔1 = 1
2i𝜋.
The 𝜎function is likewise not periodic,
𝜎(u + 2𝜔i) = −e2𝜂i(u+𝜔i)𝜎(u), i = 1, 2,
but has the additional, beneﬁcial property
of being entire (everywhere holomorphic)
on ℂ. Knowing it has a zero only at the
origin, one may use it to construct general
abelian functions on ℂ. Thus consider
f (u) =
n
∏
i=1
𝜎(u −ai)
𝜎(u −bi).
This function is holomorphic apart from
zeros at the ai’s and poles at the bi’s. It is
further periodic with half-periods 𝜔1 and
𝜔2, provided
n
∑
i=1
ai +
n
∑
i=1
bi ∈Λ,
which condition guarantees that the factors
acquired by the products of 𝜎-function all
cancel out when their arguments are aug-
mented by points in Λ.
There are many interesting relations
between
the
℘-functions
and
the
𝜎-
functions. Two notable examples are the
addition law,
℘(u) −℘(v) = −𝜎(u + v)𝜎(u −v)
𝜎2(u)𝜎2(v)
,
and its generalization, the Frobenius–
Stickelberger relations [57]: if ℘(j)
i
denotes
the value of dj
duj ℘(u)|u=ui, then
|||||||||
1
℘1
℘(1)
1
…
℘(n−2)
1
1
℘2
℘(1)
2
…
℘(n−2)
2
⋮
⋮
1
℘n
℘(1)
n
…
℘(n−2)
n
|||||||||
=
(−1)1∕2n(n−1)
n
∏
i=1
(i!)
×
𝜎(∑n
i=1 ui) ∏
i<j 𝜎(ui −uj)
∏n
i=1 𝜎n(ui)
.
Now imagine we are considering the whole
family of nonsingular elliptic curves,
V = {(x, y, g2, g3)|
y2 = 4x2 −g2x −g3, g3
2 −27g2
3 ≠0}.
The ﬁeld, F, of abelian functions is gener-
ated by the parameters g2 and g3 and the
℘and ℘′ functions (themselves functions
of the parameters). Weierstraß showed that
the 𝜎function satisﬁes diﬀerential identi-
ties in the parameters and in u:
Q0𝜎= 0,
Q2𝜎= 0,
where
Q0 = 4g2𝜕g2 + 6g3𝜕g3 −u𝜕u + 1,
Q2 = 6g3𝜕g2 + 1∕3g2
2𝜕3 −1
2𝜕2
u −1
21g2u2.
The derivations of F, Der(F), are generated
by the three operators
L0 = −u𝜕u + 4g2𝜕g2 + 6g3𝜕g3,
L1 = 𝜕u,
L2 = −𝜁𝜕u + 6g3𝜕g2 + 1∕3g2
2𝜕g3.
These operators further satisfy the commu-
tation relations of a Lie algebra over F,
[L0, L1] = L1, [L0, L2] = 2L2,
[L1, L2] = ℘L1,
exhibiting in this way a fundamental sym-
metry structure underlying these functions
[58].

276
7 Special Functions
The ℘-function appears in a number of
physical applications. One is as the solution
to the nonlinear pendulum equations:
̈𝜃+ sin 𝜃= 0
and similar contexts. Another is as a sym-
metry reduction of some classes of PDE,
notably the Korteweg–de Vries (KdV)
equation:
ut + uxxx + 6uux = 0.
The symmetry is translational. One seeks
solutions depending on the invariant 𝜂=
x −ct and ﬁnds exactly, after two integra-
tions, solutions
u(x, t) = f (𝜂)
where
f ′2 = −2f 3 + cf 2 + C1f + C2,
C1 and C2 being arbitrary constants of inte-
gration. These can be written easily in terms
of ℘-functions and represent quasiperiodic
nonlinear wave solutions on a ﬁnite space
interval (typically waves in a wave tank.) In
the limit that both C1 = C2 = 0, to accom-
modate the boundary conditions in an inﬁ-
nite tank
f , f ′ →0,
|𝜂| →0,
we obtain the single soliton solution
u(x, t) = 1
2sech2
(√
c
2 (x −ct −𝜂0)
)
for constant 𝜂0.
Analogues of the Weierstraß ℘-function
can be deﬁned for plane curves of genus
g greater than one (the Weierstraß case).
They satisfy addition formulae and are peri-
odic with 2g complex periods in g variables.
For example, the quintic curve
y2 = 4x5 + a4x4 + a3x3 + a2x2 + a1x + a0
is a genus two plane curve and there are
three ℘-functions of two variables u1 and
u2, written
℘11(u1, u2), ℘12(u1, u2), ℘22(u1, u2).
Integrability conditions on the ℘-functions
imply the existence of an entire function
𝜎(u1, u2) such that
℘ij(u1, u2) = −2𝜕ui𝜕uj log 𝜎(u1, u2).
Further indices represent higher deriva-
tives: ℘111 = 𝜕∕𝜕u1℘11, and so on.
The
analogue
of
the
second-order
equation
satisﬁed
by
the
Weierstrass
function is a set of ﬁve equations for the
℘ij:
℘1111 −6℘2
11 = −1
2a0a4 + 1
8a1a3
−3a0℘22 + a1℘12 + a2℘11
℘1112 −6℘11℘12 = −a0
−1
2a1℘22 + a2℘12
℘1122 −4℘2
12 −2℘11℘22 = 1
2a3℘12
℘1222 −6℘12℘22 = a4℘12 −2℘11
℘2222 −6℘2
22 = 1
2a3 + a4℘22 + 4℘12
Such equations have been studied for a long
time [59] and are still an active area of
research [60].
The Weierstraß ℘-function arises in
an important class of linear, Fuchsian
equations with coeﬃcients doubly periodic
on ℂ∶

7.6 Nonlinear Special Functions
277
7.6.1.1
Lamé Equations
The
second-order
family
of
Lamé
equations,
w′′ −(h + n(n + 1)℘(z))w = 0,
for integer n, has regular singular points at
the lattice points of Λ and exponents n + 1
and −n. Although the exponents diﬀer by
an integer (2n + 1), no logarithmic terms in
fact occur and, depending on the value of h,
solutions algebraic in the ℘-function may
exist [9, 22].
For the case n = 1,
w′′ −(h + 2℘(z))w = 0,
one can show that two linearly independent
solutions
w1 = e−z𝜁(a) 𝜎(z + a)
𝜎(z)
and
w2 = ez𝜁(a) 𝜎(z −a)
𝜎(z)
exist provided h = ℘(a) is not a root of the
Weierstraß cubic, that is, provided ℘′(a) ≠
0. If h = e1 is a root of the cubic, then the
above solutions are equal and a second lin-
early independent solution must be chosen:
w2 = e−z𝜂1 𝜎(z + 𝜔1)
𝜎(z)
(𝜁(z + 𝜔1) + e1z) .
7.6.2
Jacobian Elliptic Functions
A genus one plane curve may also be writ-
ten in quartic form
y2 = a4x4 + 4a3x3 + 6a2x2 + 4a1x + a0
for which all branch points lie in the ﬁnite
part of the complex plane and an analo-
gous treatment leads to parameterization
by alternative doubly periodic functions
due to Jacobi [20, 21].
While ℘(u) has a double pole in each
period parallelogram, the Jacobi functions
have a pair of distinct single poles. Owing
to the fact that the degree of a divisor of a
meromorphic function on a compact Rie-
mann surface vanishes, there must be, up
to multiplicity, two corresponding zeros for
each function.
Deﬁning the Jacobi sn function by
sn(u) = t,
u = ∫
t
0
dx
√
(1 −x2)(1 −k2x2)
,
so that
(x′)2 = (1 −x2)(1 −k2x2),
we deﬁne cn and dn by
cn2(u) + sn2(u) = 1,
dn2(u) + k2sn2(u) = 1,
choosing the signs in such a way that
cn(u) →0,
dn(u) →0
as u →0.
From these deﬁnitions one shows that
sn′(u) = cn(u)dn(u);
cn′(u) = −sn(u)dn(u);
dn′(u) = −k2sn(u)cn(u).
In the limit that k →0, sn and cn become
circular functions.
Further, if K is the value of the complete
elliptic integral

278
7 Special Functions
K = ∫
𝜋∕2
0
d𝜙
√
1 −k2 sin2 𝜙
then
sn(K) = 1,
cn(K) = 0,
dn(K) =
√
1 −k2.
K determines the periods of the Jacobi ellip-
tic functions:
sn(u + 4K) = sn(u);
cn(u + 4K) = cn(u);
dn(u + 2K) = dn(u).
As with the ℘-functions, and for the same
underlying algebro-geometric reasons, the
Jacobi functions obey addition laws:
sn(u + v) =
sn(u)cn(v)dn(v) + sn(v)cn(u)dn(u)
1 −k2sn2(u)sn2(v)
;
cn(u + v) =
cn(u)cn(v) −sn(uv)sn(v)dn(u)dn(v)
1 −k2sn2(u)sn2(v)
;
dn(u + v) =
dn(u)dn(v) −k2sn(u)sn(v)cn(u)cn(v)
1 −k2sn2(u)sn2(v)
.
7.6.3
Theta Functions
In many ways, the most natural approach
to doubly (and multi-) periodic functions
in general is via the 𝜗-function [12, 22,
60–62]. We shall present the simplest,
genus one case and indicate roughly how
higher genus 𝜗-functions are deﬁned.
Let 𝜏be a complex number with a strictly
positive imaginary part and let
q = ei𝜋𝜏.
The (fourth) 𝜗-function with parameter 𝜏
and variable u is deﬁned by an inﬁnite sum:
𝜗4(u|𝜏) =
∞
∑
n=−∞
(−1)nqn2e2niu.
Similar to the 𝜎-function, which is a mod-
iﬁed 𝜗-function, it is entire and satisﬁes
a modularity condition (periodicity up to
multiplicative factors):
𝜗4(u + 𝜋|𝜏) = 𝜗4(u|𝜏);
𝜗4(u + 𝜏𝜋|𝜏) = −1
qe−2iu𝜗4(u|𝜏).
The third and ﬁrst 𝜗-functions are deﬁned
as
𝜗3(u|𝜏) = 𝜗4(u + 1
2𝜋|𝜏)
𝜗1(u|𝜏) = −ieiu+1∕4𝜋i𝜏𝜗4(u + 1
2𝜏𝜋|𝜏).
and ﬁnally the second,
𝜗2(u|𝜏) = 𝜗1(u + 1
2𝜋|𝜏).
One should think of these apparently
redundant variants of 𝜗as analogues of
circular function relations,
cos(u) = sin(u + 1
2𝜋),
which serve to simplify writing compact,
polynomial identities.
Consider a fundamental parallelogram in
the complex plane with corners at 0, 𝜋,
𝜏𝜋and 𝜋+ 𝜏𝜋, closed on two nonparallel
sides, open on the other two. Any transla-
tion in ℂof this parallelogram will contain
a single, simple zero of any of the 𝜗i(u|𝜏)
functions and they may be used to con-
struct abelian functions in the same way as
was 𝜎(u).
The 𝜗’s satisfy many polynomial (alge-
braic) addition laws, one example of which,

7.6 Nonlinear Special Functions
279
the 𝜏dependence being understood, is
𝜗3(u + v)𝜗3(u −v)𝜗3(0)2
= 𝜗3(u)2𝜗3(v)2 + 𝜗1(u)2𝜗1(v)2.
They also satisfy a simple, linear PDE of the
heat equation type, in which the parameter
𝜏plays the rôle of “imaginary time":
𝜕𝜗i
𝜕𝜏+ i𝜋
4
𝜕2𝜗i
𝜕u2 = 0.
The higher genus, g ≥1, generalization of
the 𝜗function to g independent, complex
variables, u ∈ℂg, involves a g × g symmet-
ric, complex-valued matrix 𝜏whose entries
a have positive deﬁnite imaginary part.
Then the (Riemann) 𝜗-function is,
𝜗(u|𝜏) =
∑
N∈ℤg
exp 2𝜋i
(1
2NT𝜏N + NTu
)
,
where N and u are column vectors and (⋅)T
denotes the row vector transpose.
Such 𝜗functions are associated with
algebraic curves (on complex dimensional
manifolds) by taking a canonical homology
basis of closed loops (Chapter 6)
{a1, … , ag, b1, … , bg}
and a basis of holomorphic one-forms
{𝜔1, … , 𝜔g},
satisfying
∫ai
𝜔j = 𝛿ij,
and
𝜏ij = ∫bi
𝜔j.
The modular properties of 𝜃are summa-
rized in the statement that, for 𝜇and 𝜇′ col-
umn vectors in ℤg,
𝜗(u + 𝜇′ + 𝜏𝜇, 𝜏)
= exp 2𝜋i
(
−𝜇′Tu −1
2𝜇′T𝜏𝜇
)
𝜗(u, 𝜏).
The analogues of the 𝜗i are the so-called 𝜗-
functions with characteristics
𝜗
[ 𝜖
𝜖′
]
(u, 𝜏)
=
∑
N∈ℤg
exp 2𝜋i
(1
2NT
𝜖𝜏N𝜖+ NT
𝜖u𝜖′
)
.
Here the notation (⋅)𝜖stands for augmen-
tation by the additive constant 𝜖∕2, for
example, N𝜖= N + 1
2𝜖.
When the entries in 𝜖and 𝜖′ are inte-
gers, the function is called the ﬁrst-order
𝜗-function with integer characteristics.
The variables u are properly said to
live on a g complex dimensional algebraic
(torus) variety, the Jacobi variety, Jac(X), of
the algebraic curve. This is ℂg factored by
the lattice Λ = {𝜇′ + 𝜏𝜇|𝜇, 𝜇′ ∈ℤg}. There
is a natural map, the Abel map, from any
n-fold symmetric product of the algebraic
curve with itself to Jac(X) ∶
𝜙n ∶Xn = X1 × · · · × Xn →ℂg∕Λ,
given, for some choice of P0 ∈X, by
𝜙n(P1, … , Pn) =
n
∑
i=1 ∫
Pi
P0
𝜔,
mod Λ.
𝜔is the column vector of holomorphic dif-
ferentials, du.
In the case that n = g, the Abel map is a
local homomorphism and the Jacobi inver-
sion problem, still only implicitly solved in
general, is to recover the divisor P1 + P2 +
· · · + Pg ∈Xg from a given point in Jac(X).

280
7 Special Functions
Of particular importance is Riemann’s
theorem describing the locus of zeros (the
𝜗-divisor) of the 𝜗-function:
𝜗(e) = 0 ⇔e ∈c + 𝜙g−1(Xg−1)
where c is a very speciﬁc vector – the Rie-
mann vector of constants [12].
7.6.4
Painlevé Transcendents
Modifying the Weierstraß equation a lit-
tle further, one may consider the simple
nonautonomous case:
y′′ = 6y2 + x,
x being the independent variable. This is
the simplest of the Painlevé equations.
Similar to the Weierstraß equation, such
equations arise as symmetry reductions
of integrable PDEs such as the KdV, mod-
iﬁed KdV, Boussinesq, and sine-Gordon
equations, under scaling [63–67].
For example, the KdV equation,
ut + uxxx + 6uux = 0
has a symmetry
x →𝜆x
t →𝜆3t
u →𝜆−2u
so that in terms of symmetry invariants we
seek solutions, w satisfying
u(x, t) = (3t)−2∕3w
(
x
(3t)1∕3
)
which are invariant under the symmetry
(scaling solutions). Put 𝜂=
x
(3t)1∕3 . Then the
KdV reduces to
w′′′ −2w −𝜂w′ + 6ww′ = 0,
prime denoting d∕d𝜂. Using a Miura trans-
formation
w = v′ −v2.
This equation can be solved subject to
knowledge of solutions to
v′′ = 2v3 + 𝜂v + 𝛼,
for constant values of 𝛼. This last equation
is a Painlevé equation of type II.
The solutions share properties of the ℘-
function in being analytic except at mov-
able poles in the ﬁnite part of the complex
plane, but they are not periodic.
Equations characterized by the property
that their only movable singularites (that
is, singularities whose locations depend on
initial values or constants of integration)
are poles are said to have the Painlevé prop-
erty. A necessary but insuﬃcient criterion
for a nonlinear ODE to possess the Painlevé
property is that the exponents 𝜈of a series
expansion about an arbitrary point x0,
(x −x0)𝜈
∞
∑
0
an(x −x0)n,
analogous to the Frobenius expansions
described earlier, be integers.
First-order ODEs of this type, polynomial
in the dependent variable and its derivative,
are either linear, Riccati
y′ = a(x)y2 + b(x)y + c(x)
or elliptic [68]. Second-order equations
were roughly classiﬁed by Painlevé [66]
(more thoroughly by Gambier and cowork-
ers [69]) and hence go by his name. A list
of the six second-order Painlevé equations
is given below:

7.6 Nonlinear Special Functions
281
PI
y′′ = 6y2 + x
PII
y′′ = 2y3 + xy + a
PIII
y′′ = y′2
y −y′
x
+ay2 + b
x
+ cy3 + d
y
PIV
y′′ = y′2
2y + 3∕2y3 + 4xy2
+2 (x2 −a) y + b
y
PV
y′′ =
(
1
2y +
1
y −1
)
y′2 −y′
x
+(y −1)2
x2
(
ay + b
y
)
+ c y
x
+d y(y + 1)
y −1
PVI
y′′ = 1
2
(
1
y +
1
y −1 +
1
y −x
)
y′2
−
(
1
x +
1
x −1 +
1
y −x
)
y′
+y(y −1)(y −x)
x2(x −1)2
×
(
a + b x
y2 + c x −1
(y −1)2 + d x(x −1)
(y −x)2
)
The parameters a, b, c, and d are arbitrary,
but speciﬁc relations between them allow
specialization. PVI contains all the other
cases as reductions. Generically, the solu-
tions are transcendents not to be found
among the (linear) elementary and special
functions, except for certain special values
of the parameters.
Painlevé equations are associated with
linear ODEs with regular singular points in
the following way.
Consider a family of linear, second-order
ODEs with regular singular points in ℂ
depending on a complex parameter, 𝜆. We
can write this as a ﬁrst-order equation in
system form,
Ψx(𝜆, x) = B(𝜆, x, y)Ψ(𝜆, x),
where Ψ is a two-component column vec-
tor of functions and B a 2 × 2 matrix with
at worst poles of order one in x and 𝜆,
polynomial in y. The monodromy group of
this system will be generated, as before, by
linear maps
Ψ →MiΨ,
associated with each pole in B, i = 1, 2, …
We will require that the monodromy
properties, for example, the group expo-
nents, be invariant under variation in the
parameter 𝜆. This is the isomonodromy
condition and it means that the above
system is compatible with a system of the
form
Ψ𝜆(𝜆, x) = A(𝜆, x, y)Ψ(𝜆, x),
A having, in general, ﬁrst-order poles in 𝜆.
The statement that these two systems are
compatible is that they share a common
general solution, so that
𝜕x
(𝜕𝜆Ψ) = 𝜕𝜆
(𝜕xΨ) ,
which in turn implies

282
7 Special Functions
𝜕xA −𝜕𝜆B + [A, B] = 0.
This system constitutes a (generally nonlin-
ear) ODE in y with independent variable x.
All Painlevé equations are realized as
isomonodromy conditions in this manner
[70, 71]. For example, in the case of PI, we
may (but this is not unique) choose,
A = (4𝜆2 + 2y2 + x)
( 1
0
0
−1
)
+(4𝜆2y + 2y2 + x)
( 0
−1
1
0
)
−(2𝜆yx + 1
2𝜆)
( 0
1
1
0
)
,
B = (𝜆+ y
𝜆)
( 1
0
0
−1
)
+ y
𝜆
( 0
−1
1
0
)
.
Painlevé equations also come equipped
with
Bäcklund
transformations.
Most
generally, a Bäcklund map is a transforma-
tion mapping solutions of one diﬀerential
equation to those of another or the same
equation
(autoBäcklund
map)
perhaps
with diﬀerent parameter values. We have
seen examples for linear equations already:
the Laplace, Moutard, and Darboux maps.
Since PI has no parameters, we consider
the case of PII. An obvious Bäcklund map
is:
y(x, a) →̃y = −y(x, −a).
A less obvious one, for a ≠±1∕2, is, writing
y(x, a) = ya:
ya →ya±1 =
−ya −
2a ± 1
2y2
a ± 2ya,x + x.
The sequence of y(x, a) so generated satis-
ﬁes the three-term recurrence relation:
a + 1
2
ya + ya+1
+
a −1
2
ya + ya−1
+ 2y2
a + x = 0.
In fact, the Bäcklund maps detailed above
comprise a representation of an extended
aﬃne Weyl group, the inﬁnite dihedral
group generated by a reﬂection and a
translation.
One further aspect of the richness of the
Painlevé theory is the existence of families
of simple, nontranscendental, solution for
certain parameter values. Let us consider
PII once more. For a ∈ℤthere exist solu-
tions
yn = d
dx ln
(Qn−1(x)
Qn(x)
)
the
Qn
satisfying
a
second-order
diﬀerential-diﬀerence recurrence relation,
Q′2
n −4QnQ′′
n
Qn+1Qn−1 −zQ2
n
= −1
4.
with
initial
terms
Q0 = 1,
Q1 = x,
Q2 = x3 + 4 and so on. The corresponding
rational solutions to PII are
y1 = −1
x,
y2 = 1
x −
3x2
x3 + 4,
etc.
Another class of rational solutions to PII
are obtained from the generating function
e𝜆x−(4∕3)𝜆3 =
∞
∑
m=0
pm(x)𝜆m.
Put
𝜏n = det
(djp2i−1(x)
dxj
)
i,j=1,…,n
,
and the solutions are

7.7 Discrete Special Functions
283
y(x, n) = d
dx ln
(𝜏n−1
𝜏n
)
.
There are also solutions expressible using
classical special functions. Thus for a = n +
(1∕2), n ∈ℤ, a family of solutions to PII
is obtained using the Bäcklund transforma-
tion, ya →ya±1 above, starting from one of
y(x, ±1
2) = ∓𝜙′(x)
𝜙(x) ,
𝜙being an Airy function combination
𝜙(x) = C1Ai(−2−1∕3x) + C2Bi(−2−1∕3x).
The situations described above for PI
and PII are replicated for other Painlevé
equations.
7.7
Discrete Special Functions
7.7.1
𝝏–𝜹Theory
An attempt at a universal theory of special
functions was made in the middle of the last
century by imposing a relationship between
diﬀerential and diﬀerence operations [72].
In this approach, the majority of special
function recurrence relations are shown to
be reducible to a relation of the form,
𝜕zF(z, 𝛼) = F(z, 𝛼+ 1).
This universal equation has many solutions
for F. For instance,
• F(z, 𝛼) = ez;
• F(z, 𝛼) = sin(z −1
2𝛼z);
• F(z, 𝛼) = e−z2H𝛼(−z);
• F(z, 𝛼) = ei𝛼𝜋z−𝛼∕2J𝛼(2
√
z).
The equation can be shown to admit
unique solutions, to be expandable in the
parameters z and in 𝛼, to give rise to other
solely diﬀerential identities, and to provide
generating functions of almost all the clas-
sical special functions.
It also naturally leads in to the ideas of
diﬀerence calculus.
7.7.2
Quantum Groups
Properly speaking these are not groups,
the name quantum group is nevertheless
established, particularly in the mathemati-
cal physics literature, for certain Hopf bial-
gebras that play a rôle in the theory of a
whole new class of discrete or q-diﬀerence
special functions analogous to that played
by Lie groups in the classical theory [73, 74].
Most importantly, it is their representations
that are studied in applications.
For q a nonzero complex number with
q2 ≠1, the quantum group Uq(𝔰l2) is an
algebra generated over ℂ[q] by elements
E, F, K, and K−1 deﬁned by the relations
KK−1 = K−1K = 1,
KEK−1 = q2E,
KFK−1 = q−2F,
and
EF −FE = K −K−1
q −q−1 .
It carries a comultiplication, a ℂ-linear map
from the algebra to its two-fold tensor self-
product,
Δ(E) = E ⊗K + 1 ⊗E,
Δ(F) = F ⊗1 + K−1 ⊗F
Δ(K) = K ⊗K;
a counit, 𝜖,

284
7 Special Functions
𝜖(E) = 𝜖(F) = 0,
𝜖(K) = 1;
and an antipode, S,
S(K) = K−1,
S(E) = −EK−1,
S(F) = −KF.
The comultiplication is consistent with the
deﬁning relations in the sense that, for
instance,
Δ(E)Δ(F) −Δ(F)Δ(E) = Δ(K) −Δ(K−1)
q −q−1
.
In the limit that q →1, one obtains (almost)
the universal enveloping algebra U(𝔰l2), so
in that sense the quantum group is a defor-
mation of the Lie algebra, which is morally
its classical limit.
In the representation theory of quantum
groups, the Uq(𝔰l2) generators are realized
as diﬀerence operators on functions.
7.7.3
Diﬀerence Operators
The classical special functions have dis-
crete analogues in which diﬀerential oper-
ators are replaced by diﬀerence operators of
which the most studied are the q-diﬀerence
operator [30, 75, 76]
Dq(f (x)) = f (x) −f (qx)
x −qx
,
the h-derivative
dh(f (x)) = f (x + h) −f (x)
h
,
and the Askey–Wilson diﬀerence operator
q(f (x)) =
̆f (q1∕2ei𝜃) −̆f (q−1∕2ei𝜃)
1
2(q1∕2 −q−1∕2)(z −1∕z)
,
where ̆f (z) is deﬁned by,
̆f (z) = f
(
1
2
(
z + 1
z
))
,
x and z are related as
z2 −2xz + 1 = 0,
and we take the branch
√
x + 1 > 0, x >≥
−1.
The corresponding processes, by analogy
with the classical derivative, are sometimes
called quantum calculus.
Although each diﬀerence operator tends
to the classical limit (on appropriate func-
tion classes) as q →1 and h →0, such oper-
ators are not quite derivations. Instead, for
example,
Dq(fg)(x) =
f (x)Dq(g(x)) + g(qx)Dq(f (x)).
That such deﬁnitions are natural is evi-
denced by simple observations of the fol-
lowing sort. The notation for a q-integer
[n]q = 1 −qn
1 −q
allows us to write the q-derivative of xn as
Dq(xn) = [n]qxn−1
and suggests the q-factorial deﬁnition
[n]q! = [n]q[n −1]q · · · [1]q
with [0]q! = 1 as usual. We can then deﬁne
q-binomial coeﬃcients,

7.7 Discrete Special Functions
285
[ n
j
]
q
=
[n]q!
[n −j]q![j]q!
The q-Pascal rules for the q-binomial coef-
ﬁcients are
[ n
j
]
q
=
[ n −1
j −1
]
q
+ qj
[ n −1
j
]
q
and
[ n
j
]
q
=
[ n −1
j
]
q
+ qn−j
[ n −1
j −1
]
q
Assume x and y are noncommutative
objects satisfying yx = qxy ( x and y are
coordinates on the quantum plane). There
is a q-binomial theorem,
(x + y)n =
n
∑
j=0
[ n
j
]
q
xjyn−j,
taking due regard of the ordering of the x’s
and y’s.
Such ideas go back to Gauss and Euler.
There are two q-analogues of the classical
exponential function:
ex
q =
∞
∑
j=0
xj
[j]q!
and
Ex
q =
∞
∑
j=0
qj(j−1)∕2 xj
[j]q!.
Elementary calculations conﬁrm q-diﬀ-
erence identities:
Dqex
q = ex
q
DqEx
q = Eqx
q ;
the reciprocal relation
ex
qE−x
q
= 1
and, subject again to the commutation rela-
tion yx = qxy, the addition law
ex
qey
q = ex+y
q .
(7.1)
q-Analogues of classical special functions
can be constructed. As a simple example we
consider the q-Hermite polynomials.
7.7.4
q-Hermite Polynomials
The nth (continuous) q-Hermite polyno-
mial, denoted H(x|q), satisﬁes a three-term
recurrence relation:
2xHn(x|q) = Hn+1(x|q)
+(1 −qn)Hn−1(x|q),
(7.2)
with initial terms
H0(x|q) = 1,
H1(x|q) = 2x.
Let us deﬁne the symbols,
(a; q)0 = 1,
(a; q)n =
n
∏
k=1
(1 −aqk−1).
Here n may take any value in ℕor ∞, to sig-
nify an inﬁnite product. Note that if a = q,
we recover the symbol [n]q! deﬁned earlier.
We also deﬁne
(a, b; q)n = (a, q)n(b, q)n.
Using this symbol, the q-Hermite polyno-
mial generating functions are
1
(tei𝜃, te−i𝜃|q) =
∞
∑
0
Hn(cos 𝜃|q)
tn
(q|q)n
.

286
7 Special Functions
They may also be themselves considered
as generating functions for the q-binomial
coeﬃcients:
z−nHn(cos 𝜃|q) =
n
∑
k=0
[ n
k
]
q
z−2k,
z = ei𝜃.
The Hn(x|q) have the classical polynomials
as limits in the following sense:
lim
q→1−
(
2
1 −q
)n∕2
Hn(x
√
1 −q
2
|q))
→Hn(x).
They also satisfy ladder diﬀerence relations
similar to the diﬀerential ladder relations of
their classical counterparts:
qHn(x|q) =
2(1 −qn)
1 −q
q(1−n)∕2Hn−1(x|q);
1
w(x|q)q
(w(x|q)Hn(x|q)) =
−2qn∕2
1 −qHn+1(x|q).
7.7.5
Discrete Painlevé Equations
There is a list of seven discrete Painlevé
equations (there are two variants of PV) of
which the ﬁrst three are
𝛿-PI
xn+1 + xn−1 = −xn + 𝛼n + 𝛽
xn
+ 1;
𝛿-PII
xn+1 + xn−1 = (𝛼n + 𝛽)xn + a
1 −x2
n
;
q-PIII
xn+1xn−1 = (xn −aq𝜆n)(xn −bq𝜆n)
(1 −cxn)(1 −xn
c )
.
Here, 𝛼, 𝛽, q, and 𝜆are constant parame-
ters and the notation 𝛿- or q- refers to the
additive or multiplicative structures of the
diﬀerence scheme [77].
These equations are of interest in several
respects.
Firstly, they reduce to the classical
Painlevé equations in appropriate “contin-
uum limits:” we allow that xn = f (𝜖n) for
diﬀerentiable f and let 𝜖→0 with some
adjustment of the free parameters [78, 79].
Secondly, they exhibit a property called
“singularity
conﬁnement”
[77],
which
means that although certain initial condi-
tions may give rise to indeterminate values
of xn for some n, this indeterminacy is not
stable under small variations in those initial
conditions.
Thirdly, the initial conditions can be
assigned “degrees" in which the order of
growth of the solutions is merely poly-
nomial rather than exponential [80]. This
algebraic entropy is an unusual property
for diﬀerence schemes.
Finally, in the sense of the Nevanlinna
theory, the solutions are well controlled
[81].
Discrete Painlevé equations share with
their classical limits the availability of Bäck-
lund maps and Lax pairs (isomonodromy).
Finally, they are classiﬁed by the extended
aﬃne Weyl groups [82] that, so far as this
article is concerned, nicely returns us to
our starting point in the representations of
discrete symmetries.
References
1. Rose, J.S. (1978) A course on Group Theory,
Cambridge University Press.

References
287
2. Benson, D.J. (1993) Polynomial Invariants of
Finite Groups, LMS Lecture Note Series 190,
CUP.
3. Borovik, A.V. and Borovik, A. (2010) Mirrors
and Reﬂections: The Geometry of Finite
Reﬂection Groups, Springer.
4. Humphreys, J.E. (1990) Reﬂection Groups
and Coxeter Groups, Studies in Advanced
Mathematics 29, CUP.
5. Fulton, W. (1997) Young Tableaux: With
Applications to Representation Theory and
Geometry, Cambridge University Press.
6. MacDonald, I.G. (1998) Symmetric Functions
and Hall Polynomials, 2nd edn, OUP.
7. Forsyth, A.R. (1959) Theory of Diﬀerential
Equations, Dover.
8. Gray, J. (1996) Linear Diﬀerential Equations
and Group Theory from Riemann to
Poincaré, 2nd edn, Birkhäuser.
9. Ince, E.L. (1956) Ordinary Diﬀerential
Equations, Dover.
10. Klein, F.C. (1914) Vorlesungen über das
Ikosaeder und die Auﬂösung der
Gleichungen vom 5ten Grade, Teubner
(1884), Lectures on the Icosahedron, and the
Solution of Equations of the Fifth Degree, 2nd
revised edn (ed. English translation by G.G.
Morrice), Kegan Paul & Co., London.
11. Kuga, M. (1994) Galois’ Dream: Group
Theory and Diﬀerential Equations, Corr. 2nd
printing, Birkhäuser.
12. Farkas, H.M. and Kra, I. (1992) Riemann
Surfaces, Springer.
13. Lie, S. (1970) Transformationsgruppen,
Chelsea, New York.
14. Humphreys, J.E. (1972) Introduction to Lie
Algebras and Representation Theory,
Springer.
15. Olver, P.J. (1986) Applications of Lie Groups
to Diﬀerential Equations, Springer.
16. Ovsiannikov, L.V. (1982) Group Analysis of
Diﬀerential Equations, Academic Press.
17. Warner, F.W. (2010) Foundations of
Diﬀerentiable Manifolds and Lie Groups,
Springer.
18. Fulton, W. and Harris, J. (1999)
Representation Theory: A First Course,
Springer.
19. Jeﬀreys, H. and Jeﬀreys, B. (2000) Methods of
Mathematical Physics, 3rd edn, Cambridge
University Press.
20. Wang, Z.X. and Guo, D.R. (1989) Special
Functions, World Scientiﬁc.
21. Abramowitz, M. and Stegun, I.A. (1965)
Handbook of Mathematical Functions, Dover
Publications, Inc.
22. Whitaker, E.T. and Watson, G.N. (1996) A
Course of Modern Analysis, 4th edn, CUP.
23. Hitchen, N. (1982) Monopoles and
Geodesics. Commun. Math. Phys., 83,
579–602.
24. Miller, W. (1977) Symmetry and Separation
of Variables (Encyclopedia of Mathematics
and its Applications), Addison-Wesley.
25. Gradshteyn, I.S. and Ryzhik, I.M. (2007)
Gradshteyn and Ryzhik’s Table of Integrals,
Series, and Products, 7th edn (eds A. Jeﬀrey
and D. Zwillinger), Academic Press.
26. Talman, J.D. (1968) Special Functions: A
Group Theoretic Approach (based on lectures
by E.P. Wigner), W.A.Benjamin.
27. Vilenkin, N.J. (1968) Special Functions and
the Theory of Group Representations,
Translations of Mathematical Monographs
22, AMS.
28. Bultheel, A., Gonzalez-Vera, P., Hendriksen,
E., and Njastad, O. (1999) Orthogonal
Rational Functions, Cambridge University
Press.
29. Khrushchev, S. (2008) Orthogonal
Polynomials and Continued Fractions, CUP.
30. Ismail, M.E.H. (2005) Classical and
Quantum Orthogonal Polynomials in one
Variable, Encyclopedia of Mathematics and
its Applications 98, CUP.
31. Infeld, L. and Hull, T.E. (1953) The
factorization method. Rev. Mod. Phys., 23,
21–68.
32. Weyl, H. (1950) The Theory of Groups and
Quantum Mechanics, Dover.
33. Cartan, H. (2003) The Theory of Spinors,
Dover.
34. Darboux, G. (1890) Leçons Sur La Theorie
Generale Des Surfaces, Gauthier Villars Et
Fils.
35. (a) Moutard, Th.F. (1895) C. R. Acad. Sci.
Paris, 80, 129; (b) deLEcole, J. (1878)
Polytech. Cahier, 45, 1.
36. Novikov, S., Manakov, S.V., Pitaevskii, L.P.,
and Zakharov, V.E. (1984) Theory of Solitons:
The Inverse Scattering Method, Consultants
Bureau, New York.
37. Kamran, N. (2002) Selected Topics in the
Geometrical Study of Diﬀerential Equations,
CBMS 96, AMS.
38. Toda, M. (1989) Theory of Nonlinear
Lattices, Springer.

288
7 Special Functions
39. Matveev, V.B. and Salle, M.A. (1991)
Darboux Transformations and Solitons,
Springer.
40. Dunkl, C.F. (2003) Special functions and
generating functions associated with
reﬂection groups. J. Comput. Appl. Math.,
153, 181–190.
41. Opdam, E.M. (2000) Lecture Notes on Dunkl
Operators for Real and Complex Reﬂection
Groups, Mathematical Society of Japan
Memmoirs, vol. 8, Mathematical Society of
Japan.
42. Rösler, M. (2003) Dunkl operators: theory
and applications, in Orthogonal Polynomials
and Special Functions, Springer Lecture
Notes in Mathematics, vol. 1817, Springer.
43. Bender, C.M. and Orsag, S.A. (1999)
Advanced Mathematical Methods for
Scientists and Engineers, Springer.
44. Hille, E. (1976) Ordinary Diﬀerential
Equations in the Complex Domain, John
Wiley & Sons, Inc.
45. Meyer, R.E. (1989) A simple explanation of
the Stokes phenomenon. SIAM Rev., 31,
435–445.
46. Ritt, J.F. (1950) Diﬀerential Algebra, AMS.
47. Ritt, J.F. (1948) Integration in Finite Terms:
Liouville’s Theory of Elementary Methods,
Columbia University Press.
48. Abel, N.H. (2010) Oeuvres Complètes, Nabu
Press.
49. Kaplansky, I. (1976) An Introduction to
Diﬀerential Algebra, 2nd edn, Hermann.
50. Kolchin, E.R. (1973) Diﬀerential Algebra and
Algebraic Groups, Pure and Applied
Mathematics 54, Academic Press, Boston,
MA.
51. Magid, A.R. (1994) Lectures on Diﬀerential
Galois Theory, University Lecture Series, vol.
7, AMS.
52. van der Put, M. and Singer, M.F. (1997)
Galois Theory of Diﬀerence Equations,
Lecture Notes in Mathematics 1666,
Springer.
53. Artin, E. (1971) Galois Theory, 6th printing,
University of Notre Dame.
54. Armitage, J.V. and Eberlien, W.F. (2006)
Elliptic Functions, LMS student tects 67,
Cambridge University Press.
55. Moll, V. (1999) Elliptic curves: Function
theory, Geometry, Arithmetic, CUP.
56. Weierstraß, K. (1856) Theorie der Abel’schen
Funktionen, Jour. reine & angewandte Math.,
52, 285–379.
57. Frobenius, G. and Stickelberger, L. (1877)
Zur Theorie der elliptischen Funktionen.
Journal für Math., 33, 179.
58. Buchstaber, V.M. and Leykin, D.V. (2008)
Solution of problem of diﬀerentiation of
abelian functions over parameters for
families of (n, s)-curves. Funct. Anal. Appl,
42, 268–278.
59. Baker, H.F. (1907) Abelian Functions: Abel’s
Theorem and the Allied Theory of Theta
Functions, Cambridge University Press,
Cambridge 1897 (reprinted in 1995); An
Introduction to the Theory of Multiply
Periodic Functions, Cambridge University
Press.
60. Buchstaber, V.M., Enolski, V., and Leykin, D.
(1997) Kleinian functions, hyperelliptic
Jacobians and applications. Rev. Math. Math.
Phys., 10, 1–125.
61. Mumford, D. (1983/4) Tata Lectures on
Theta I & II, Birkhäuser.
62. Weierstraß, K. (1894) Zur Theorie der
elliptischen Funktionen, Mathematische
Werke, Bd 2, Teubner, Berlin, pp. 245–255.
63. Bobenko, A.I. and Eitner, U. (2000) Painlevé
Equations in the Diﬀerential Geometry of
Surfaces, LNM 1753, Springer.
64. Conte, R. and Musette, M. (2008) The
Painlevé Handbook, Springer.
65. Clarkson, P. Painlevé Transcendents, Chapter
32 in Digital Library of Mathematical
Functions, http://dlmf.nist.gov/32 (accessed
on 24 February 2014).
66. Painlevé, P. (1897) Lecons, sur la theorie
analytique des equations diﬀerentielles,
professees a Stockholm, Libraire Scientiﬁque
à Hermann, Paris.
67. Noumi, M. (2004) Painlevé Equations
through Symmetry, American Mathematical
Society.
68. Goursat, E. (1917) in A Course in
Mathematical Analysis (eds translation by
E.R. Hedrick and O. Dunkel), Ginn & Co.
69. Gambier, B. (1910) Sur les équations
diﬀérentielles du second ordre et du premier
degré dont l’intégrale générale est à points
critique ﬁxés. Acta Math., 33, 1–55.
70. Flaschka, H. and Newell, A.C. (1980)
Monodromy- and spectrum-preserving
deformations. I. Commun. Math. Phys., 76,
65–116.
71. Jimbo, M. and Miwa, T. (1981) Monodromy
preserving deformation of linear ordinary

References
289
diﬀerential equations with rational
coeﬃcients. Phys. D 2, 407–448.
72. Truesdell, C. (1948) An Essay Towards a
Uniﬁed Theory of Special Functions,
Princeton University Press.
73. Brown, K.A. and Goodearl, K.R. (2002)
Lectures on Algebraic Quantum Groups,
Birkhäuser.
74. Klimik, A. and Smüdgen, K. (1997) Quantum
Groups and their representations, Springer.
75. Kac, V. and Cheung, P. (2002) Quantum
Calculus, Springer.
76. van der Put, M. and Singer, M.F. (2003)
Galois Theory of Linear Diﬀerential
Equations, Springer.
77. Grammaticos, B. and Ramani, A. (2004)
Discrete Painlevé Equations: A Review,
Lecture Notes in Physics, vol. 644, Springer,
pp. 245–321.
78. Conte, R. and Musette, M. (1996) A new
method to test discrete Painlevé equations.
Phys. Lett. A, 223, 439–448.
79. Fokas, A.S., Grammaticos, B., and Ramani,
A. (1993) From continuous to discrete
Painlevé equations. J. Math. Anal. Appl.,
180, 342–3360.
80. Veselov, A.P. (1992) Growth and integrability
in the dynamics of mappings. Commun.
Math. Phys., 145, 181–193.
81. Ablowitz, M.J. and Halburd, R. (2000)
Nevanlinna theory and diﬀerence equations
of Painlevé type. Proceedings of the Workshop
on Nonlinearity, Integrability and All That:
Twenty Years after NEEDS ’79 (Gallipoli,
1999), 3–11, 2000, World Science
Publishing, River Edge, NJ.
82. Sakai, H. (2001) Rational surfaces associated
with aﬃne root systems and geometry of the
Painlevé equations. Commun. Math. Phys.,
220, 165.


291
8
Computer Algebra
James H. Davenport
8.1
Introduction
Computer algebra, sometimes also called
symbolic computation to distinguish it
from numeric computation, is exactly what
the name suggests: it is the use of com-
puters to do algebra. Though sometimes
referred to as “computer mathematics,”
it is not necessarily that, as mathemati-
cally incorrect results can be algebraically
“correct,” as in the school howler
1 =
√
1 =
√
(−1)2 = −1.
(8.1)
This
problem
is
further
discussed
in
Section 8.7.2.
In fact, it is not even algebra for which
we need software packages, computers
by themselves cannot actually do arith-
metic: only a limited subset of it. It is
1) In fact, Excel is more complicated even than this, as the calculations in this table show.
i
1
2
3
4
…
10
11
12
…
15
16
a
10i
10
100
1000
1…0
…
1…0
1011
1012
…
1015
1016
b
a-1
9
99
999
9999
9…9
…
9…9
1012
…
1015
1016
c
a-b
1
1
1
1
1
…
1
1
…
1
0
We can see that the printing changes at 12 decimal digits, but that actual accuracy is not lost until we
subtract 1 from 1016.
the case that e𝜋
√
163 ≈2.625 374 126 407
687 43.9 999 999 999 992 500 725. If we ask
Excel (or any similar software package) to
compute e𝜋
√
163 −262 537 412 640 768 744,
we will be told that the answer is 256 (or
−10944, depending on the version of Excel).
More mysteriously, if we go back and look
at the formula in the cell, we see that it is
now e𝜋
√
163 −262 537 412 640 768 800, or
even e𝜋
√
163 −262 537 412 640 768 000. In
fact, 262 537 412 640 768 744 is too large a
whole number (or integer, as mathemati-
cians say) for Excel to handle, and Excel
has converted it into ﬂoating-point (what
Excel terms scientiﬁc) notation. Excel, or
any other software using the IEEE stan-
dard [1] representation for ﬂoating-point
numbers, can only store them to a given
accuracy, about (we say “about” because
the internal representation is binary, rather
than decimal) 16 decimal places.1 In fact,
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

292
8 Computer Algebra
it requires twice this precision to show
that e𝜋
√
163 ≠262 537 412 640 768 744. As
e𝜋
√
163 = (−1)
√
−163, it follows from deep
results of transcendental number theory
[2], that not only is e𝜋
√
163 not an integer, it
is not a fraction (or rational number), nor
even the solution of a polynomial equation
with integer coeﬃcients: essentially it is a
“new” number.
Although computers had been used to
support mathematics earlier (in 1951, the
79-digit number 180 (2127 −1
)2 −1 was
shown to be prime and in 1952, the great
mathematician Emil Artin had the equally
great computer scientist John von Neu-
mann performed an extensive calculation
relating to elliptic curves on the MANIAC
computer [3, p. 119]), actual computer
algebra is 60 years old, since in 1953, two
theses [4, 5] described programs to dif-
ferentiate expressions, and Haselgrove [6]
showed that algorithms in group theory
could be implemented on computers.
8.2
Computer Algebra Systems
While these early programs could only do
one thing, they soon evolved into more
general-purpose systems that could do
a variety of algebraic calculations. The
descendants of [4, 5] are the “polynomial”
systems that manipulate (fractions of)
polynomials in “indeterminates” such as x,
y or “pseudo-indeterminates” such as sin x,
ey, and so on, early examples of which are
the Polynomial Manipulator PM [7], the
Symbolic Mathematical Laboratory [8] and
its successor Macsyma [9]2), the Cambridge
Algebra Manipulation Language CAMAL
[10], and Reduce [11]. The group theory of
2) And various oﬀshoots MAXIMA, Vaxima, and
so on.
Haselgrove [6] gave rise to CAYLEY [12],
then Magma [13] as well as a range of more
specialist systems.
This split survives to this day, with
major “polynomial” systems, now generally
described as “calculus” systems, being
Maple, Mathematica, and SAGE (the latter
incorporates much of MAXIMA) and
major group theory systems being GAP
and MAGMA. Why the split? The diﬀer-
ence seems one of mathematical attitude,
if one can call it that. The designer of a
calculus system envisages it being used
to compute an integral, factor a polyno-
mial, multiply two matrices, or otherwise
operate on a mathematical datum. The
designer of a group theory system, while
he will permit the user to multiply, say,
permutations or matrices, does not regard
this as the object of the system: rather the
object is to manipulate whole groups (etc.)
of permutations (or matrices, or other
mathematical data), that is, mathematical
structures, and take, say, the quotient of two
groups, rather than of two polynomials.
More recently, we have seen the rise of sys-
tems to manipulate polynomial structures
(generally ideals–see Deﬁnition 8.3) such
as CoCoA [14, 15], SINGULAR [16] and
Macaulay [17, 18], and “calculus” systems
incorporate more of this, so the distinction
is blurring.
8.3
“Elementary” Algorithms
8.3.1
Representation of Polynomials
The “obvious” way to represent a poly-
nomial, say 3x3 + 2x −5, in a computer
would be to store a vector of coeﬃ-
cients, that is, [3,0,2,-5]. Addition
of polynomials is then just addition of

8.3 “Elementary” Algorithms
293
vectors, and multiplication is a convolu-
tion. Polynomials in several variables are
then stored either as multidimensional
arrays (a method known as distributed)
or as polynomials in x whose coeﬃcients
are polynomials in y and so on (a method
known as recursive).
The problem with this approach can
already be seen in the previous para-
graph: we had to write 3x3 + 2x −5 as
3x3 + 0x2 + 2x −5 in order to get a com-
plete set of coeﬃcients. This might seem
harmless, but a dense representation of
abcde −1 would be
a1b1c1d1e1 + 0 ⋅a1b1c1d1e0 + · · ·
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
5 terms
+ 0 ⋅a1b1c1d0e0 + · · ·
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
10 terms
+ · · · + a0b0c0d0e0
for a total of 32 terms. a2b2c2d2e2 −1 has
243 terms, and, in general, a polynomial of
degree d in n variables has (d + 1)n terms.
Adding two such polynomials requires (d +
1)n operations, and multiplying two takes
(d + 1)2n operations.3) Storing, or at least
counting, all potential terms is known as
the dense model.
Hence most systems adopt a sparse
approach (analogous to the storage of
sparse matrices in MatLab, etc.) and
only store the nonzero coeﬃcients (and
the
corresponding
exponents).
Hence,
3x3 + 2x −5 becomes [(3,3),(2,1),
(-5,0)] and a2b2c2d2e2 −1 becomes
[(1,2,2,2,2,2),(-1,0,0,0,0,0)]
(sparse
distributed)
or
[a,(z,2),(-
1,0)] where z is the representation
of b2c2d2e2 (sparse recursive). If there
3) “Fast” algorithms, such as Karatsuba multiplica-
tion [19] or fast Fourier transform ones [20] can
improve on these running times, but not on the
storage requirements.
are tf and tg nonzero terms in f
and
g,
addition
takes
≤min(tf , tg)
arith-
metic operations and multiplication takes
≤2tf tg.
We have emphasized arithmetic oper-
ations because the real challenge now
becomes that of arranging the output
in a useful order (nice), and combin-
ing/canceling
duplicates
(necessary
to
preserve sparseness). This can be viewed
as a sorting problem, and techniques such
as HeapSort [21], BucketSort [22], or
hashing (as in Maple [23]) are used. In the
sparse-distributed representation, order is
not quite so obvious: do we put x2y before
(greater x power) or after (lower total
degree) xy3? See Section 8.5.4 for more
details.
Once we move beyond addition and mul-
tiplication, just counting nonzero terms in
the input no longer bounds the complexity.
The division of two two-term polyno-
mials can be arbitrarily large, because
(xn−1)∕(x −1) = xn−1 + xn−2 + · · · + x + 1.
However, it is possible to compute h = f ∕g
with the number of operations depend-
ing only on the number of terms tf ,
tg, and th, and not on the degrees [24].
Counting only the nonzero terms in the
input and output is known as the sparse
model.
It is worth noting that neither the dense
model nor the sparse one quite captures
our
intuitive
notion
of
“small”–most
people would think of 1 + (x −1)100 as a
“small expression,” but neither the dense
nor sparse models would think so. This is
the domain of the “straight-line program”
model, but this is beyond the scope of this
article. An example showing the superior
performance, both in time and more espe-
cially in space, of the straight-line program
over conventional data structures is given
in [25].

294
8 Computer Algebra
8.3.2
Greatest Common Divisors
The computation of polynomial common
divisors has many uses, not least in the
simpliﬁcation of rational functions, as in
(x2 + 3x + 2)∕(x + 1) →x + 2. The problem
might seem to be solved by Euclid’s Algo-
rithm (8.1), and, indeed, conceptually it is.
In practice, though, there are two serious
problems with this.
Algorithm 8.1 (Euclid)
Input: a0, a1 ∈K[x].
Output:
h ∈K[x] a greatest common
divisor of a0 and a1
i ∶= 1;
while ai ≠0 do
ai+1 = rem(ai−1, ai);
# Let qi be the
corresponding quotient
i ∶= i + 1;
return ai−1;
8.3.2.1
Intermediate Expression Swell
As we go round the loop in Algorithm 8.1,
the degree (in x) of the polynomials does
keep decreasing. But nothing is said about
the size of the coeﬃcients. If we apply this
algorithm to two fairly innocuous polyno-
mials:
A(x) = x8 + x6−3x4 −3x3 + 8x2 + 2x −5;
B(x) = 3x6 + 5x4−4x2 −9x −21.
}
(8.2)
we get the sequence
−5
9 x4 + 127
9 x2 −29
3 ,
50157
25
x2 −9x −35847
25
,
93 060 801 700
1 557 792 607 653x + 23 315 940 650
173 088 067 517
and ﬁnally
761 030 000 733 847 895 048 691
86 603 128 130 467 228 900
.
As this is a number, it follows that no poly-
nomial can divide both A and B, that is, that
gcd (A, B) = 1. The reader might think that
“clearing fractions” would help: it does not,
and we end up with
7 436 622 422 540 486 538 114 177 255
855 890 572 956 445 312 500.
Had the coeﬃcients not been integers,
but rather polynomials in y, z, …, the
growth
would
have
been
even
more
spectacular: see the worked example at
http://staﬀ.bath.ac.uk/masjhd/JHD-CA/
GCD3var.html. The good news is that
advanced algorithms (Section 8.4) have
pretty much solved this problem: we can
compute greatest common divisors (gcds)
eﬃciently, working all the time with data
whose polynomial degrees in y, z, … are
no larger than those in the input, and
with numerical coeﬃcients that are not
much larger than in the input. We say “not
much larger” because a gcd can have larger
coeﬃcients than the input, for example
A = x5 + 3x4 + 2x3 −2x2 −3x −1
= (x + 1)4(x −1);
B = x6+3x5+3x4+2x3+3x2+3x+1
= (x + 1)4(x2 −x + 1);
gcd (A, B) = x4 + 4x3 + 6x2 + 4x + 1
= (x + 1)4.
(8.3)
These algorithms tend to be probabilistic,
and produce an answer g, which, if it is a
common divisor of A and B at all (and this
needs to be veriﬁed, either by checking that
g divides A and B, or by computing A∕g
and B∕g as well as g), is guaranteed to be
a greatest common divisor.

8.3 “Elementary” Algorithms
295
8.3.2.2
Sparsity
The “good news” above is indeed good as
far as it goes, and shows that, in the dense
model, we can do about as well as possible,
but it does not speak to the sparse model.
In particular, it still leaves open the possi-
bilities that
1. the gcd g might be much denser than
the inputs A and B
2. even if the gcd g is not much denser
than the inputs A and B, the cofactors
A∕g and B∕g computed as part of the
veriﬁcation might be much denser
3. even if neither of these happens,
various intermediate results might be
much denser than A and B.
Problem 2 is easy to demonstrate: con-
sider the cofactors in gcd (xp −1, xq −1),
where p and q are distinct primes. Prob-
lem 1 is demonstrated by the following ele-
gant example of [26]
gcd (xpq −1, xp+q −xp −xq + 1)
= xp+q−1 −xp+q−2 ± · · · −1,
(8.4)
and indeed just knowing whether two poly-
nomials have a nontrivial gcd is hard, by the
following result.
Theorem 8.1 [27] It is NP-hard to deter-
mine whether two sparse polynomials (in
the standard encoding) have a nontrivial
common divisor.
This theorem, like the examples above,
relies on the factorization of xp −1, and
it is an open question [24, Challenge 3]
whether this is the only obstacle.
More precisely, we have the follow-
ing equivalent of the problem solved for
division.
Challenge 8.1 [24, Challenge 5]
Find
an algorithm for computing h = gcd ( f , g)
which is polynomial-time in tf , tg, and th.
A weaker problem, but still unsolved, is
Challenge 8.2 Find an algorithm for com-
puting h = gcd ( f , g) which is polynomial-
time in tf , tg, th, and tf ∕h, tg∕h.
Problem 3 is particularly acute in the
case of multivariate polynomials. Further
developments [28, Section 4.4] of the ideas
in Section 8.4, using methods from [29],
have produced algorithms whose time
seems empirically to vary as d2t rather
than dn for polynomials of degree d and t
terms, in n variables. Hence this problem
is largely solved in practice, but theoretical
analysis is still lacking, as we cannot solve
Challenges 8.1 and 8.2.
8.3.3
Square-free Decomposition
One particular use of gcds is given by the
following result.
Proposition 8.1 If 𝛼is a repeated root of
the polynomial f , then it is a root of f ′,
and hence of gcd ( f , f ′). In fact, the roots of
gcd ( f , f ′) are precisely the repeated roots
of f .
Deﬁnition 8.1 A polynomial f is said to be
square-free if it has no repeated roots. The
square-free decomposition of f is f = ∏f i
i ,
where each fi is square-free and gcd ( fi, fj) =
1 if i ≠j.
Theorem 8.2 [30] Over Z and in the stan-
dard sparse encoding, the two problems
1. deciding if a polynomial is square-free
and
2. deciding if two polynomials have a
nontrivial gcd.
are
equivalent
under
randomized
polynomial-time reduction.
Hence, in the light of Theorem 8.1,
determining
square-freeness
is
hard,

296
8 Computer Algebra
at
least
when
polynomials
with
fac-
tors of the form xp −1 are involved.
Again, it is an open question whether
these (and scaled variants) are the only
obstacles.
8.3.4
Extended Euclidean Algorithm
If we look at Algorithm 8.1, we see that a0
and a1 are, trivially, each a linear combina-
tion of a0 and a1. ai+1 = ai−1 −qiai, so is a
linear combination of a0 and a1 if each of
ai−1 and ai are. Hence by induction every ai
is a linear combination of a0 and a1, and, in
particular, the gcd is.
Proposition 8.2 (Bezout’s identity)
There exist polynomials 𝜆and 𝜇such that
gcd ( f , g) = 𝜆f + 𝜇g.
Note that Algorithm 8.1, as we ran it,
generated fractions, so 𝜆and 𝜇may well
have fractions as coeﬃcients.
A useful application of Bezout’s identity
is the theory of partial fractions: if f and g
are relatively prime, then
h
fg = h(𝜆f + 𝜇g)
fg
= 𝜆h
g + 𝜇h
f .
(8.5)
Even if h∕fg is reduced (deg(h) < deg( fg)),
the summands may not be, but we can
adjust them:
h
fg = rem(𝜆h, g)
g
+ rem(𝜇h, f )
f
,
(8.6)
and the summands are now reduced.
8.4
Advanced Algorithms
There are two main classes of “advanced”
algorithms in computer algebra for solving,
not only the gcd problem and its variants,
but many others.
8.4.1
Modular Algorithms–Integer
We described the polynomials in (8.2) as
“innocuous.” They are innocuous if we con-
sider them, and apply Algorithm 8.1, mod-
ulo 5, writing P5 to signify the polynomial
P considered as a polynomial with coeﬃ-
cients modulo 5.
A5(x) = x8 + x6 + 2x4 + 2x3 + 3x2 + 2x;
B5(x) = 3x6 + x2 + x + 1;
C5(x) = remainder(A5(x), B5(x))
= A5(x) + 3(x2 + 1) B5(x) = 4x2 + 3;
D5(x) = remainder(B5(x), C5(x))
= B5(x) + (x4 + 4x2 + 3) C5(x) = x;
E5(x) = remainder(C5(x), D5(x))
= C5(x) + x D5(x) = 3.
But if P divides A, then A = PQ for
some polynomial Q, and A5 = P5Q5. Sim-
ilarly, if P divides B, then B = PR for some
polynomial R, and B5 = P5R5. Hence if
P = gcd (A, B), P5 has to be a common
factor of A5 and B5, but there no a non-
trivial one, so P5 has to be 1, and, because
the leading coeﬃcient of P has to be ±1,
P = 1. Of course, there is nothing special
about 5: we could have picked almost any
prime–2 does not work as gcd (A2, B2) ≠1
and 3 is dubious as B3 has lower degree
than B. However, we have to answer sev-
eral questions before we can make this
into a complete gcd algorithm replacing
Algorithm 8.1.
1. How do we calculate a nontrivial gcd?
2. what do we do if the gcd modulo p is
not the modular image of the gcd (as
with A2 and B2)? In this case, we say
that we have bad reduction, or that p is
bad. In particular, what do we do if this
keeps on happening?
3. How much does this method cost?

8.4 Advanced Algorithms
297
The ﬁrst question already makes us think:
if the gcd is x + 42, we are never going to
decide this by working modulo 5 (which
might suggest x + 2), or indeed any prime
smaller than 85 = 2 × 42 + 1, because we
have to allow for the sign, as working mod-
ulo 43 would suggest x −1 as the answer,
rather than x + 42. Furthermore, we know
from (8.3) that the largest coeﬃcient in
a gcd is not, as we might hope, at most
the largest coeﬃcient in the inputs. For-
tunately, we can nevertheless bound the
largest coeﬃcient in a gcd.
Proposition 8.3 (Landau–Mignotte
bound [31, 32]) Every coeﬃcient of the gcd
of A = ∑𝛼
i=0 aixi and B = ∑𝛽
i=0 bixi (with ai
and bi integers) is bounded by
LM(A, B) ∶= 2min(𝛼,𝛽) gcd (a𝛼, b𝛽)
min
⎛
⎜
⎜⎝
1
|a𝛼|
√
√
√
√
𝛼
∑
i=0
a2
i ,
1
|b𝛽|
√
√
√
√
𝛽∑
i=0
b2
i
⎞
⎟
⎟⎠
. (8.7)
Furthermore, the number 2 in 2min(𝛼,𝛽) can-
not be replaced by any smaller number [33].
Once we know this, we have two choices.
• Work modulo a prime p > 2LM(A, B), so
that, unless problem 2 occurs, Pp will,
interpreting its coeﬃcients as integers in
−p∕2, … , p∕2, be P itself. This is
perfectly correct, but often ineﬃcient as,
although (8.7) cannot be improved on as
a worst case bound, it is normally far too
pessimistic.
• Work modulo several diﬀerent small
primes pi, use the Chinese Remainder
Theorem [28, Section A.3] to compute
P∏pi from the various Ppi. As before, if
∏pi >2LM(A, B), P∏pi will, interpreting
its coeﬃcients as integers in −(∏pi)∕2,
… , (∏pi)∕2, be P itself. This method
lends itself to various shortcuts if (8.7) is
pessimistic in a given case, as it nearly
always is.
Problem 2 is nevertheless a genuine prob-
lem. Again, we can say something.
Proposition 8.4 (Good
Reduction
Theorem (Z) [28]) The bad p are those
that divide either gcd (a𝛼, b𝛽) or a certain
nonzero determinant D (in fact, the resul-
tant Resx(A∕P, B∕P)). Furthermore, if p
divides Resx(A∕P, B∕P) but not gcd (a𝛼, b𝛽),
then the gcd computed modulo p has a
larger degree than the true result.
In particular, there are only a ﬁnite num-
ber of bad primes.
We can test the ﬁrst half of this (p divid-
ing gcd (a𝛼, b𝛽)) and not even try such
primes. After this, if we have two primes
that give us diﬀerent degrees for the gcd,
we know that the prime with the higher
degree has bad reduction, from the “Fur-
thermore …” statement, so we can discard
it. The corresponding algorithm is given
as [Algorithm Modular GCD (small prime
version)] [28].
As far as the running time is concerned,
if l bounds the length of the coeﬃcients
in the input and output, and d the degree
of the polynomials, then the running time
is C(d + 1)(l2 + (d + 1)) for a suitable con-
stant C, depending on the computer, soft-
ware, and so on, provided we ignore the
possibility of unlucky primes [34, (95)]. We
note that this depends on the degree, not
the number of terms, as even modulo p we
cannot solve Challenges 8.1 and 8.2.
8.4.2
Modular Algorithms–Polynomial
Despite our failure to address the sparsity
issue, that algorithm is still substantially
better than Euclid’s, because the coeﬃcient
growth is bounded, by Proposition 8.3. Can

298
8 Computer Algebra
we do the same for multivariate polynomi-
als, and avoid the spectacular growth seen
in
http://staﬀ.bath.ac.uk/masjhd/JHD-
CA/GCD3var.html? We consider the case
of polynomials in x and y, regarding x
as the main variable: the generalization
to more variables is easy. There is one
preliminary remark: the gcd might actually
not depend on x, but be a polynomial in y
alone. In that case, it would have to divide
every coeﬃcient of the input polynomials,
regarded as polynomials in x whose coef-
ﬁcients are polynomials in y. Hence, it can
be determined by gcds of polynomials in
y alone, and we will not concern ourselves
further with this (though the software
implementer clearly has to), and consider
only the part of the gcd that depends
on x.
The answer is that we can ﬁnd this eﬃ-
ciently, and the idea is similar to the pre-
vious one: we replace “working modulo p”
with “evaluating y at v.” Just as we wrote
P5 to signify the polynomial P considered
as a polynomial with coeﬃcients modulo 5,
we write Py=v to signify the polynomial P
with y evaluated at the value v. But if P
divides A, then A = PQ for some polyno-
mial Q, and Ay=v = Py=vQy=v. Similarly, if P
divides B, then B = PR for some polyno-
mial R, and By=v = Py=vRy=v. Hence if P =
gcd (A, B), Py=v has to be a common factor
of Ay=v and By=v. In particular, if Ay=v and
By=v have no common factor, the gcd of A
and B cannot depend on x. Analogous ques-
tions to those posed above appear.
1. How do we calculate a nontrivial gcd?
2. What do we do if the gcd of the
evaluated polynomials is not the
evaluation of the true gcd? In this case,
we say that we have bad reduction, or
that v is a bad value. In particular, what
do we do if this keeps on happening?
3. How much does this method cost?
The answers are similar, or indeed eas-
ier. For question 1, we use the Chinese
Remainder Theorem for polynomials [28,
Section A.4] to compute the true result
from several evaluations. How many? In the
integer coeﬃcient case, we had to rely on
Proposition 8.3, but the equivalent is trivial.
Proposition 8.5 (Polynomial version of
Proposition 8.3)
degy(gcd ( f , g)) ≤min
(degy ( f ), degy(g)).
For question 2, we have a precise analogue
of Proposition 8.4.
Proposition 8.6 (Good
Reduction
Theorem
(polynomial)
[28])
If
y −v
does not divide gcd (a𝛼, b𝛽) (which can be
checked for in advance) or Resx(A∕C, B∕C),
then v is of good reduction. Furthermore,
if y −v divides Resx(A∕C, B∕C) but not
gcd (a𝛼, b𝛽), then the gcd computed modulo
y −v has a larger degree than the true
result.
This gives us algorithms precisely equiva-
lent to those in the previous section, and
then do indeed generalize to n variables.
Question 3 has the following answer. If l
bounds the length of the coeﬃcients in
the input and output, and d the degree of
the polynomials, then the running time is
C(d + 1)n(l2 + l(d + 1)) for a suitable con-
stant C, depending on the computer, soft-
ware, and so on, provided we ignore the
possibility of unlucky primes and values
[34, (95)].
8.4.3
The Challenge of Factorization
If we want to factor polynomials, we might
as well ﬁrst do a square-free factorization
(Section 8.3.3): as well as eﬃcient, this will
turn out to be necessary. This means that
gcd ( f , f ′) = 1. It follows from Proposi-
tion 8.4 that there are only ﬁnitely many

8.4 Advanced Algorithms
299
primes for which gcd ( fp, f ′
p) ≠1, and we
check for, and discard, these primes.
Then the obvious solution is to follow this
approach, and start with factoring modulo
p. For small primes p and degrees d, we can
just enumerate the p(d+1)∕2 possibilities,4)
but it turns out we can do much better.
There are algorithms due to Berlekamp
[35, 36]
and
Cantor–Zassenhaus
[37],
which take time proportional to d3.
Modular methods, as described in the
previous two sections, can indeed solve
many other problems besides the gcd one.
Indeed, no computer algebra conference
goes past without a new modular algorithm
appearing somewhere. However, there is
one problem they cannot solve eﬃciently,
and that is polynomial factorization.
This might seem strange, for we have the
same modular/integer relationship as we
had in the gcd case: if f = f (1) ⋅f (2) ⋅· · · ⋅f (k)
over the integers, then fp = f (1)
p
⋅f (2)
p
⋅· · · ⋅
f (k)
p . In particular, if f and fp have the same
degree, and fp is irreducible, then f is also
irreducible. We may as well remark that the
converse is not true: it is possible for an
irreducible f to factor modulo every prime.
Example 8.1 The polynomial x4 + 1, which
is irreducible over the integers, factors into
two quadratics (and possibly further) mod-
ulo every prime.
p = 2
Then x4 + 1 = (x + 1)4.
p = 4k + 1
In this case, −1 is always a
square, say −1 = q2. This gives us the
factorization x4 + 1 = (x2 −q)(x2 + q).
p = 8k ± 1
In this case, 2 is always a
square, say 2 = q2. This gives us the
factorization
x4 + 1 = (x2 −(2∕q)
x + 1)(x2 + (2∕q)x + 1). In the case
p = 8k + 1, we have this factorization
4) The exponent (d + 1)∕2 comes from the fact that
we need only look for factors of degree ≤d∕2.
and the factorization given in the
previous case. As these two factoriza-
tions are not equal, we can calculate
the gcds of the factors, in order to ﬁnd
a factorization as the product of four
linear factors.
p = 8k + 3
In this case, −2 is always a
square, say −2 = q2. This is a result
of the fact that −1 and 2 are not
squares, and so their product must
be a square. This property of −2
gives us the factorization x4 + 1 =
(x2 −(2∕q)x −1)(x2 + (2∕q)x −1)
This polynomial is not an isolated oddity:
Swinnerton-Dyer [38] and Kaltofen et al.
[39] proved that there are whole families
of polynomials with this property of being
irreducible, but of factorizing modulo
every prime. Furthermore, these polyno-
mials have the annoying habit of being
generated disproportionately often [40].
However, this is not the main problem.
Suppose fp factors as g1g2g3, and fq fac-
tors as h1h2h3. We may be in luck, and these
factorizations may be so incompatible that
we can deduce that f has to be irreducible.
However, they may be compatible, say all
have degree k. Then it would be natural, fol-
lowing Section 8.4.2, to apply the Chinese
Remainder Theorem, and deduce that
fpq = CRT(g1, h1)CRT(g2, h2)CRT(g3, h3).
(8.8)
This would indeed be correct, but it is not
the only possibility. Our labels 1–3 are
purely arbitrary, and it would be just as con-
sistent to deduce
fpq = CRT(g1, h2)CRT(g2, h3)CRT(g3, h1),
(8.9)
or any of the other four possibilities. If
we take f = x3 + 41 x2 + 551 x + 2431, f5 =
(x + 2)(x + 1)(x + 3) and f7 = (x + 4) (x +
3)(x + 6). Combining (x + 2)5 and (x + 4)7

300
8 Computer Algebra
gives (x + 32)35, and indeed this divides f35.
If we combine (x + 2)5 and (x + 3)7, we get
(x + 17)35, and again this divides f35. In fact,
we deduce that f35, though of degree 3, has
nine linear factors. An algebraist would tell
us that polynomials modulo a composite,
such as 35, do not possess unique factoriza-
tion.
However, not all the possibilities (8.8),
(8.9), and so on correspond to the true
factorization. Although we have no a pri-
ori way of knowing it, a factor modulo p
corresponds (apart from the bad reduction
problem) to one, and only one, factor over
the integers, and this corresponds to one,
and only one, factor modulo q. Hence, if
there are m rather than just three factors,
there may be m! possible matchings of the
results modulo p and q, only one of which
is right.
Worse, if pq is not bigger than twice the
Landau–Mignotte bound, we will need to
use a third prime r, and we will now have
(m!)2 possibilities, only one of which is
right, and so on. This is clearly unsustain-
able, and we need a diﬀerent approach.
8.4.4
p-adic Algorithms–Integer
Instead of using several primes, and deduc-
ing the answer modulo p, then pq and
pqr, and so on, we will use one prime,
and deduce the answer modulo p, then
p2, p3, and so on. For simplicity, we will
assume that fp = gphp modulo p–more fac-
tors do not pose more challenges, but make
the notation diﬃcult. We will also suppose
that f is monic: this is a genuine simpliﬁ-
cation, but solving the nonmonic case as
well would distract us–details are in [28,
Chapter 5].
Let us suppose that fp2 = gp2hp2 where,
and this is the diﬀerence, we insist that
gp2 corresponds to gp, that is, that they are
equal modulo p. We can therefore write
gp2 = gp + p̂gp2, and similarly for the h’s.
Then
fp2 = (gp + p̂gp2
) (
hp + p̂hp2
)
,
so
fp2 −gphp
p
≡gp ̂hp2 + hp̂gp2 (mod p), (8.10)
where the cross-product gp2hp2 disappears
as it is multiplied by an extra p, and the
fraction on the left is not really a fraction
because gphp ≡fp ≡fp2 (mod p).
As we have made fp square-free (remark
at the start of Section 8.4.3), gcd (gp, hp) =
1, Bezout’s identity (Proposition 8.2) tells us
that there are 𝜆, 𝜇such that 𝜆gp + 𝜇hp = 1.
Hence there are ̂𝜆, ̂𝜇such that
̂𝜆gp + ̂𝜇hp ≡
fp2 −gphp
p
(mod p).
(8.11)
The obvious choice is ̂𝜆= 𝜆fp2 −gphp∕p,
but this has too large a degree. However,
if we take the remainder (this is the same
idea as (8.6)) of this with respect to hp,
and similarly with 𝜇, we obtain ̂𝜆, ̂𝜇satisfy-
ing (8.11) and with deĝ𝜆< deghp, deĝ𝜇<
deggp. Hence we can take gp2 = gp + p̂𝜇,
hp2 = hp + p̂𝜆.
We now suppose that fp3 = gp3hp3 where
we insist that gp3 corresponds to gp2, that
is, that they are equal modulo p2. We can
therefore write gp3 = gp2 + p2̂gp3, and simi-
larly for the h’s. Then
fp3 = (gp2 + p2̂gp3
) (
hp2 + p2 ̂hp3
)
,
so
fp3 −gp2hp2
p2
≡gp ̂hp3 + hp̂gp3 (mod p) (8.12)

8.5 Solving Polynomial Systems
301
as before. Using the same Bezout’s identity
as before, we get that there are ̂̂𝜆, ̂̂𝜇such
that
̂̂𝜆gp + ̂̂𝜇hp ≡
fp3 −gp2hp2
p2
(mod p)
(8.13)
with
deĝ̂𝜆< deghp,
deĝ̂𝜇< deggp.
Hence
we
can
take
gp3 = gp2 + p2̂̂𝜇,
hp3 = hp2 + p2̂̂𝜆, and so on until we have
fpn = gpnhpn with pn greater than twice
the
Landau–Mignotte
bound
on
fac-
tors of f . Then, regarded as polynomials
with integer coeﬃcients in the range
(−pn∕2, pn∕2), gpn and hpn should be the
factors of f .
However, as we saw in Example 8.1,
f might factor more modulo p than it
does over the integers. Nevertheless, all
factorizations of f over the integers must
correspond to factorizations, not neces-
sarily into irreducibles, modulo p. Hence
we need merely take all subsets of the
factors modulo p, and see if their product
corresponds to a factor over the integers.
In principle, this process is exponential
in the degree of f if it factors into low-
degree polynomials modulo p but is in
fact irreducible over the integers. In prac-
tice, it is possible to make the constant in
front of the exponential very small, and
to do better than looking at every subset
[41].
8.4.5
p-adic Algorithms–Polynomial
It is also possible to use these methods in a
multivariate setting, going from a solution
modulo (y −v) to one modulo (y −v)2, then
modulo (y −v)3, and so on, but the details
are beyond the scope of this chapter: see
[28, Section 5.8]
8.5
Solving Polynomial Systems
8.5.1
Solving One Polynomial
Consider the quadratic equation
ax2 + bx + c = 0.
(8.14)
The solutions are well known to most
schoolchildren: there are two of them, of
the form
x = −b ±
√
b2 −4ac
2a
.
(8.15)
However, if b2 −4ac = 0, that is, c = b2∕4a
then there is only one solution: x = −b∕2a.
In
this
case,
the
equation
becomes
ax2 + bx + b2∕4a = 0, which can be rewrit-
ten as a (x + b∕2a)2 = 0, making it more
obvious that there is a repeated root, and
that the polynomial is not square-free
(Deﬁnition 8.1).
Mathematicians
dislike
the
sort
of
anomaly in “this equations has two solu-
tions except when c = b2∕4a,” especially as
there are two roots as c tends to the value
b2∕4a. We therefore say that, in this special
case, x = −b∕2a is a double root of the
equation. When we say we are counting the
roots of f with multiplicity, we mean that
x = 𝛼should be counted i times if (x −𝛼)i
divides f .
Proposition 8.7 (Fundamental Theorem
of Algebra) The number of roots of a poly-
nomial equation over the complex numbers,
counted with multiplicity, is equal to the
degree of the polynomial.
There is a formula for the solutions of the
cubic equation
x3 + ax2 + bx + c = 0,
(8.16)

302
8 Computer Algebra
S
:=
T
:=
3
return
1
6 T
2b
T
−
;
−108 c + 12 S; 
12 b3 + 81 c2 ;
Figure 8.1
Program for computing solutions to a cubic.
albeit less well known to schoolchildren:
1
6
3√
36 ba −108 c −8 a3 + 12
√
12 b3 −3 b2a2 −54 bac + 81 c2 + 12 ca3
−
2b −2
3a2
3√
36 ba −108 c −8 a3 + 12
√
12 b3 −3 b2a2 −54 bac + 81 c2 + 12 ca3
−1
3a.
(8.17)
We can simplify this by making a transfor-
mation5) to (8.16): replacing x by x −a∕3.
This transforms it into an equation
x3 + bx + c = 0
(8.18)
(where b and c have changed). This has
solutions of the form
1
6
3√
−108 c + 12
√
12 b3 + 81 c2
−
2b
3√
−108 c + 12
√
12 b3 + 81 c2
.
(8.19)
Many texts (among those who discuss the
cubic at all) stop here, but in fact the com-
putational analysis of (8.19) is nontrivial. A
cubic has (Proposition 8.7) three roots, but
a na¨ıve look at (8.19) shows two cube roots,
each with three values, and two square
roots, each with two values, apparently giv-
ing a total of 3 × 3 × 2 × 2 = 36 values. Even
if we decide that the two occurrences of
the square root should have the same sign,
and similarly the cube root should have the
same value, that is, we eﬀectively execute
the program in Figure 8.1, we would still
5) This is the simplest case of the Tschirnhaus
transformation [42], which can always eliminate
the xn−1 term in a polynomial of degree n.
seem to have six possibilities. In fact, how-
ever, the choice in the ﬁrst line is only
apparent, because
1
6
3√
−108 c −12
√
12 b3 + 81 c2
=
2b
3√
−108 c + 12
√
12 b3 + 81 c2
. (8.20)
In the case of the quadratic with real coef-
ﬁcients, there were two real solutions if b2 −
4ac > 0, and complex solutions otherwise.
However, the case of the cubic is more chal-
lenging. If we consider x3 −1 = 0, we com-
pute (in Figure 8.1)
S ∶= 9;
T ∶= 6;
return 1;
(or either of the complex cube roots of unity
if we choose diﬀerent values of T). If we
consider x3 + 1 = 0, we get
S ∶= 9;
T ∶= 0;
return “ 0
0”;
but we can (and must!) take advantage of
(8.20) and compute
S ∶= −9;
T ∶= −6;
return−1;
(or either of the complex variants).

8.5 Solving Polynomial Systems
303
For x3 + x, we compute
S ∶=
√
12;
T ∶=
√
12;
return 0;
and the two complex roots come from
choosing the complex roots in the compu-
tation of T, which is really
3√
12
√
12. x3 −x
is more challenging: we compute
S ∶=
√
−12;
T ∶=
√
−12;
return {−1, 0, 1};
(8.21)
that is, three real roots which can only
be computed (at least via this formula) by
means of complex numbers. In fact, it is
clear that any other formula must have the
same problem, because the only choices
of ambiguity lie in the square and cube
roots, and with the cube root, the ambiguity
involves complex cube roots of unity.
Hence even solving the humble cubic
makes three points.
1. The formulae (8.17) and (8.19) are
ambiguous, and a formulation such as
Figure 8.1 is to be preferred.
2. Even so we need to take care of 0∕0
issues.
3. Expressing real roots in terms of
radicals may need complex numbers.
These points are much clearer with the
quartic [28, Section 3.1.3]. Higher degrees
produce an even more fundamental issue.
Theorem 8.3 (Abel, Galois [43]) The gen-
eral polynomial equation of degree 5 or
more is not solvable in radicals (i.e. in terms
of kth roots).
Hence, in general, the only description is “a
root of the polynomial x… + · · ·.” As (8.19),
and even more its quartic equivalent, is
nontrivial, it turns out to be preferable to
use this description for cubics and quartics
internally as well, only converting to radi-
cals on output (and possibly only if explic-
itly requested). See the example at (8.33),
and contrast it with (8.27).
8.5.2
Real Roots
While a polynomial of degree n always
has n roots (counted with multiplicity), it
usually6) has fewer real roots. While the
roots 𝛼i of an (irreducible) polynomial f
are algebraically indistinguishable, because
they all share the property that f (𝛼i) = 0,
this ceases to be true once we start ask-
ing questions about real roots. f (x) = x2 −
r has real roots if, and only if, r is nonneg-
ative, so asking questions about the reality
of roots means that we can ask inequalities
as well. Conversely, inequalities only make
sense if we are talking about real, rather
than complex numbers.
Just as we have seen that we can, in gen-
eral, do no better than say “a root of the
polynomial f (x) = xn + · · ·,” we cannot do
much better than say “a real root of the
polynomial f (x) = xn + · · ·,” but we can at
least make it clear which of the roots we are
talking about. There are two ways of doing
this, which in practice we always apply to
square-free f .
1. “The (unique) real root of f (x) between
a and b.” (a, b) is then known as an
isolating interval, and can be computed
by a variety of means: see [28,
Section 3.1.9]. Once we have an
isolating interval, standard techniques
6) Quite what meaning should be attached to
“usually” is surprisingly diﬃcult. The “obvious”
deﬁnitions would be “normally distributed” or
“uniformly distributed in some range,” and for
these Kac [44] shows that the average number
is (2∕𝜋) log n. A deﬁnition with better geometric
invariance properties gives
√
n(n + 2)∕3: very
diﬀerent [45].

304
8 Computer Algebra
of numerical analysis can reﬁne it to be
as small as we wish.
2. “The (unique) x such that f (x) = 0 and
the derivatives of f satisfy the following
sign conditions at x.” It is a result known
as Thom’s lemma [46, Proposition 1.2]
that specifying the sign conditions
deﬁnes a root uniquely, if at all: see [28,
Section 3.1.10].
8.5.3
Linear Systems
The same algorithms we are familiar with
from numeric linear algebra are, in princi-
ple, applicable here. Hence, given a system
of linear equations, written in matrix form
as M⋅x = b, we can (possibly with piv-
oting), apply Gaussian elimination (row
operations–adding a multiple of one row
to another) to get U⋅x = b′, where u is
upper triangular, and do back substitution.
Similarly, we can compute the inverse of a
matrix.
The real problem with symbolic, as
opposed to numeric, linear algebra is not
that the algorithms produce the output
ineﬃciently so much as the fact that the
output is huge: the inverse of an n × n
matrix of numbers is an n × n matrix of
numbers, but the inverse of an n × n matrix
of small formulae may well be an n × n
matrix of very large formulae. While we are
all happy with
( a
b
c
d
)−1
=
(
d
ad−bc
−
b
ad−bc
−
c
ad−bc
a
ad−bc
)
,
(8.22)
the reader is invited to experiment with
the equivalent for larger matrices. In fact,
the determinant of a generic n × n symbolic
matrix is the sum of n! terms, and this is the
denominator of a generic inverse, while the
numerators, of which there are n2, are the
sum of (n −1)! terms, giving a total of (n +
1)! summands. Hence our aim here should
be to avoid solving the generic problem,
and solve systems with as few variables as
possible. A further snag is that the inverse
of a sparse matrix tends to be denser, often
completely dense. For further details, see
[28, Section 3.2].
8.5.4
Multivariate Systems
If nonlinear polynomial equations in one
variable are more complicated than we
might think, and generic linear systems in
several variables are horrendous, we might
despair of nonlinear systems in several vari-
ables. While these might be diﬃcult, they
are not without algorithmic approaches.
One major one,7) Gröbner bases, is based
fundamentally on the sparse-distributed
representation.
In this representation, a nonzero poly-
nomial is represented as a sum of terms
∑
i cimi where the ci are nonzero coeﬃ-
cients and the mi are monomials, that is,
products of powers of the variables. As
mentioned earlier, when we asked whether
x2y came before or after xy3, a key ques-
tion is how we order the monomials. While
there are many options [28, Section 3.3.3],
three key ways of comparing A = xa1
1 · · · xan
n
and B = xb1
1 · · · xbn
n are the following.
Purely lexicographic –plex . We ﬁrst
compare a1 and b1. If they diﬀer, this tells
us whether A > B (a1 > b1) or A < B
(a1 < b1). If they are the same, we go on
to look at a2 versus b2 and so on. The
order is similar to looking up words in a
dictionary/lexicon –we look at the ﬁrst
letter, and after ﬁnding this, look at the
second letter, and so on. In this order, x2
is more important than xy10.
7) The other principal one, regular chains is intro-
duced in Section 8.5.6 and described in [28,
Section 3.4].

8.5 Solving Polynomial Systems
305
Total degree, then lexicographic
–grlex. We ﬁrst look at the total
degrees
a = ∑ai
and
b = ∑bi:
if
a > b, then A > B, and a < b means
A < B. If a = b, then we look at lexico-
graphic comparison. In this order xy10 is
more important than x2, and x2y more
important than xy2.
Total degree, then reverse lexicographic
–tdeg. This order is the same as the
previous, except that, if the total degrees
are equal, we look lexicographically,
then take the opposite. Many systems,
in particular Maple and Mathematica,
reverse the order of the variables ﬁrst.
The reader may ask “if the order of the
variables is reversed, and we then reverse
the sense of the answer, what’s the diﬀer-
ence?” Indeed, for two variables, there is
no diﬀerence. However, with more vari-
ables it does indeed make a diﬀerence.
For three variables, the monomials of
degree three are ordered as
x3 > x2y > x2z > xy2 > xyz > xz2
> y3 > y2z > yz2 > z3
under grlex, but as
x3 > x2y > xy2 > y3 > x2z > xyz >
y2z > xz2 > yz2 > z3
under tdeg. One way of seeing the
diﬀerence is to say that grlex with
x > y > z discriminates in favor of x,
whereas tdeg with z > y > x discrimi-
nates against z. This metaphor reinforces
the fact that there is no diﬀerence with
two variables.
It seems that tdeg is, in general, the
most eﬃcient order.
Once we have ﬁxed such an order, we have
such concepts as the leading monomial
or leading term of a polynomial, denoted
lm(p) and lt(p). The coeﬃcient of the lead-
ing term is the leading coeﬃcient, denoted
lc(p).
8.5.5
Gröbner Bases
Even though the equations are nonlinear,
Gaussian elimination is still available to us.
So, given the three equations
x2 −y = 0
x2 −z = 0
y + z = 0,
we can subtract the ﬁrst from the second
to get y −z = 0, hence (linear algebra on
this and the third) y = 0 and z = 0, and we
are left with x2 = 0, so x = 0, albeit with
multiplicity 2.
However, we can do more than this.
Given the two equations
x2 −1 = 0
and
xy −1 = 0,
(8.23)
there might seem to be no row operation
available. But in fact we can subtract x times
the second equation from y times the ﬁrst,
to get x −y = 0. Hence the solutions are x =
±1, y = x.
This might seem to imply that, given two
equations f = 0 and g = 0, we need to con-
sider replacing f = 0 by Ff + Gg = 0 for
arbitrary polynomials F and G. This would
be erroneous on two counts.
(a)
We do not need to consider arbitrary
polynomials F and G, just terms
(monomials
with
leading
coeﬃ-
cients). These terms might as well
be relatively prime, otherwise we
are introducing a spurious common
factor.
(b)
However, we must not replace f ,
because Ff + Gg might have zeros
that are not zeros of f and g: we need

306
8 Computer Algebra
to add Ff + Gg to the set of equations
under consideration.
Deﬁnition 8.2 This generalization of row
operations leads us to the following con-
cepts.
1. If lm(g) divides lm( f ), then we say that g
reduces f to h = lc(g)f −(lt( f )∕lm(g))g,
written f →g h. Otherwise, we say that f
is reduced with respect to g. In this
construction of h, the leading terms of
both lc(g)f and (lt( f )∕lm(g))g are
lc( f )lc(g)lm( f ), and so cancel. Hence
lm(f ) comes before lm(h) in our order.
2. Any chain of reductions is ﬁnite
(Dickson’s Lemma), so any chain
f1 →g f2 →g f3 · · · terminates in a
polynomial h reduced with respect to g.
We write f1
∗→
g
h.
3. This extends to reduction by a set G of
polynomials, where f →G h means
∃g ∈G ∶f →g h. We must note that a
polynomial can have several reductions
with respect to G (one for each element
of G whose leading monomial divides
the leading monomial of f ).
4. Let f , g ∈R[x1, … , xn]. The
S-polynomial of f and g, written S( f , g)
is deﬁned as
S(f , g) =
lt(g)
gcd (lm(f ), lm(g))f
−
lt(f )
gcd (lm(f ), lm(g))g.
(8.24)
The computation after (8.23), y(x2 −1) −
x(xy −1) is in fact S(x2 −1, xy −1). Hence
we might want to think about comput-
ing S-polynomials, but then what about
S-polynomials of S-polynomials, and so
on? In fact, this is the right idea. We ﬁrst
note that the precise polynomials are
not particularly important: the zeros of
G = {f1, f2, …} are not changed if we add
pf1 to G for any polynomial p, or f1 + f2,
and so on. Hence this deﬁnition.
Deﬁnition 8.3 Let S be a set of polynomials
in the variables x1, … , xn, with coeﬃcients
from R. The ideal generated by S, denoted
(S), is the set of all ﬁnite sums ∑fisi: si ∈
S, fi ∈R[x1, … , xn]. If S generates I, we say
that S is a basis for I.
What really matters is the (inﬁnite) ideal,
and not the particular ﬁnite basis that gen-
erates it. Nevertheless, some bases are nicer
than others.
Theorem 8.4 [47,
Proposition
5.38,
Theorem 5.48] The following conditions
on a set G ∈R[x1, … , xn], with a ﬁxed
ordering > on monomials, are equivalent.
1. ∀f , g ∈G, S(f , g)
∗→
G
0.
2. If f
∗→
G
g1 and f
∗→
G
g2, then g1 and g2
diﬀer at most by a multiple in R, that is,
∗→
G
is essentially well deﬁned.
3. ∀f ∈(G), f
∗→
G
0.
4. (lm(G)) = (lm((G))), that is, the leading
monomials of G generate the same ideal
as the leading monomials of the whole of
(G).
If G satisﬁes these conditions, G is called
a Gröbner base (or standard basis).
These are very diﬀerent kinds of condi-
tions, and the strength of Gröbner theory
lies in their interplay. Condition 2 under-
pins the others:
∗→
G
is well deﬁned. Con-
dition 1 looks technical, but has the great
advantage that, for ﬁnite G, it is ﬁnitely
checkable: if G has k elements, we take the
k(k −1)∕2 unordered pairs from G, com-
pute the S-polynomials, and check that they
reduce to zero. This gives us either a proof
or an explicit counterexample (which is the

8.5 Solving Polynomial Systems
307
key to the following algorithm). As f
∗→
G
0
means that f ∈(G), Condition 3 means that
ideal membership is testable if we have a
Gröbner base for the ideal.
Algorithm 8.2 (Buchberger)
Input: ﬁnite G0 ⊂R[x1, … , xn]; monomial
ordering >.
Output: G a Gröbner base for (G0) with
respect to >.
G ∶= G0; n ∶= |G|;
# we consider G as {g1, … , gn}
P ∶= {(i, j) ∶1 ≤i < j ≤n}
while P ≠∅do
Pick (i, j) ∈P;
P ∶= P ⧵{(i, j)};
Let S(gi, gj)
∗→
G
h
If h ≠0 then
# lm(h) ∉(lm(G))
gn+1 ∶= h; G ∶= G ∪{h};
P ∶= P ∪{(i, n + 1) ∶1 ≤i ≤n};
n ∶= n + 1;
Given that, every time G grows, we add
more possible S-polynomials to P, it is
not obvious that this process terminates,
but it does –essentially (lm(G)) cannot
grow
forever.
Estimating
the
running
time is a much harder problem. There
are doubly exponential (in the number of
variables) lower bounds on the degree of
the polynomials in a Gröbner base [48].
In practice, it is very hard to estimate the
running time for a Gröbner base calcula-
tion, and the author has frequently been
wrong by an order of magnitude –in both
directions!
Proposition 8.8 Given a Gröbner base G
with respect to any ordering, it has a ﬁnite
number of solutions if, and only if, each
variable occurs alone (to some power) as
the leading monomial of one of the elements
of G. In this case, the number of solutions,
counted with multiplicity, is the number of
monomials irreducible under G
While a Gröbner base can be computed
with respect to any order, a Gröbner base
with respect to a purely lexicographical
order is particularly simple: essentially the
nonlinear equivalent of a triangular matrix.
If the system has only ﬁnitely many solu-
tions,8) the Gianni–Kalkbrener algorithm
[[28], Section 3.3.7] provides an equiva-
lent of back substitution: solve the (unique)
polynomial in xn; for each root 𝛼n solve
the lowest-degree polynomial in xn−1, xn
whose leading coeﬃcient does not vanish
at xn = 𝛼n; for each root 𝛼n−1, solve the
lowest-degree polynomial in xn−2, xn−1, xn
whose leading coeﬃcient does not van-
ish at xn−1 = 𝛼n−1, xn = 𝛼n, and so on. The
reader may be surprised that there can be a
choice of polynomials in xn−1, xn, but con-
sider
G = {x2
1 −1, x2
2 −1, (x1 −1)(x2 −1)}.
(8.25)
We have two choices for 𝛼2: 1 and −1.
When x2 = −1, we use the polynomial (x1 −
1)(x2 −1) and deduce x1 = 1. But when
x2 = 1, the leading coeﬃcient (and in fact
all) of (x1 −1)(x2 −1) vanishes, and we are
forced to use x2
1 −1, which tells us that x1 =
±1.
Unfortunately, plex Gröbner bases are
among the most expensive to compute.
Hence practitioners normally use the fol-
lowing process.
1. Compute a tdeg Gröbner basis G1.
2. Check (Proposition 8.8) that it has only
ﬁnitely many solutions.
3. Maybe give up if there are too many.
8) This condition is unfortunately necessary: see
[49] for an example.

308
8 Computer Algebra
4. Use the Faugère–Gianni–Lazard–
Mora algorithm [28, Section 3.3.8] to
convert G1 to a plex Gröbner basis G2.
5. Apply the Gianni–Kalkbrener
Algorithm to G2.
8.5.6
Regular Chains
Although not (yet?) quite so well known as
Gröbner bases, the theory of regular chains
[50], also known, slightly incorrectly, as tri-
angular sets, provides an alternative, based
instead on a (sparse) recursive view of poly-
nomials. The fundamental algorithm takes
as input a ﬁnite set S of polynomials, and
rather than returning one Gröbner basis,
returns a ﬁnite set of ﬁnite sets {S1, … , Sk}
such that
1. (𝛼1, … , 𝛼n) is a solution of S if, and only
if, it is a solution of some Si.
2. Each Si is triangular and a regular
chain, which in particular means it can
be solved by straightforward back
substitution.
Applied to (8.25), this would produce two
regular chains: (x2
1 −1, x2 −1) and (x1 −
1, x2 + 1), from which the solutions can
indeed be read oﬀ.
Among widespread systems, this theory
is currently only implemented in Maple.
It has two advantages over the Gröbner
basis approach: it produces triangular, that
is, easy to understand systems even when
there are inﬁnitely many solutions, and
it can be adapted to looking just for real
solutions [51].
It does have its drawbacks, the most
signiﬁcant of which is probably that there
is no guarantee that the roots of Si and Sj
are disjoint.
8.6
Integration
We have previously (Section 8.3.3) made
use of the concept of f ′, the derivative of
f . The reader may object that this is a con-
struct of calculus, not of algebra. However,
it is possible to deﬁne diﬀerentiation (which
we will regard as being with respect to x)
purely algebraically.
Deﬁnition 8.4 A map′ ∶K →K is a diﬀer-
entiation (with respect to x) if it satisﬁes
1. (f + g)′ = f ′ + g′;
2. (fg)′ = fg′ + f ′g;
3. x′ = 1.
It follows from these that
(a) 0′ = 0 by expanding (x + 0)′ = x′;
(b) 1′ = 0 by expanding (1 ⋅x)′ = x′
(c)
(
p
q
)′
= p′q−pq′
q2
by expanding
(
q ⋅
(
p
q
))′
= p′ according to 2.
From this point of view, indeﬁnite9) inte-
gration is simply anti-diﬀerentiation:
solve ∫f dx = F ⇔ﬁnd F such that F′ = f .
(8.26)
It is clear that F is only determined up to
adding a constant c, that is, c′ = 0.
8.6.1
Rational Functions
When faced with ∫(8x7∕(x8 + 1))dx, most
of us would spot that this is ∫(f ′∕f )dx, with
log f as the answer. But we would strug-
gle with ∫((8x7 + 1)∕(x8 + 1))dx, whose
answer is
9) For deﬁnite integration, see Section 8.7.1.

8.6 Integration
309
(
1
16
√
2 +
√
2 +
i
16
√
2 −
√
2 + 1
)
log
(
x + 1
2
√
2 +
√
2 + i
2
√
2 −
√
2
)
+
(
1
16
√
2 −
√
2 +
i
16
√
2 +
√
2 + 1
)
log
(
x + 1
2
√
2 −
√
2 + i
2
√
2 +
√
2
)
+
(
−1
16
√
2 −
√
2 +
i
16
√
2 +
√
2 + 1
)
log
(
x −1
2
√
2 −
√
2 + i
2
√
2 +
√
2
)
+
(
−1
16
√
2 +
√
2 +
i
16
√
2 −
√
2 + 1
)
log
(
x −1
2
√
2 +
√
2 + i
2
√
2 −
√
2
)
+
(
−1
16
√
2 +
√
2 −
i
16
√
2 −
√
2 + 1
)
log
(
x −1
2
√
2 +
√
2 −i
2
√
2 −
√
2
)
+
(
−1
16
√
2 −
√
2 −
i
16
√
2 +
√
2 + 1
)
log
(
x −1
2
√
2 −
√
2 −i
2
√
2 +
√
2
)
+
(
1
16
√
2 −
√
2 −
i
16
√
2 +
√
2 + 1
)
log
(
x + 1
2
√
2 −
√
2 −i
2
√
2 +
√
2
)
+
(
1
16
√
2 +
√
2 −
i
16
√
2 −
√
2 + 1
)
log
(
x + 1
2
√
2 +
√
2 −i
2
√
2 −
√
2
)
.
(8.27)
It seems clear that the sort of pattern-
matching
we
have
seen,
and
used
eﬀectively in school, will not scale to
this level.10)
If we look at the integration of rational
functions ∫(q(x)∕r(x))dx with degq < degr
(else we have a polynomial part which is
trivial to integrate), we can conceptually
1. perform a square-free decomposition
(Deﬁnition 8.1) of r = ∏n
i=1 ri
i;
2. factorize each ri completely, as
ri(x) = ∏ni
j=1(x −𝛼i,j), where in general
the 𝛼i,j will be RootOf constructs;
3. perform a partial fraction
decomposition (8.6) of q∕r as
q
r =
q
∏n
i=1 ri
i
=
n
∑
i=1
qi
ri
i
=
n
∑
i=1
ni
∑
j=i
j∑
k=1
𝛽i,j,k
(x −𝛼i,j)k ;
(8.28)
4. integrate this term by term, obtaining
10) However, intelligent transformations do have a
rôle to play in producing good human-readable
answers: see [52].
∫
q
r =
n
∑
i=1
ni
∑
j=i
j∑
k=2
−𝛽i,j,k
(k −1)(x −𝛼i,j)k−1
+
n
∑
i=1
ni
∑
j=i
𝛽i,j,1log(x −𝛼i,j).
(8.29)
This is not a good idea computa-
tionally, as it would solve the trivial
∫((8x7)∕(x8 + 1))dx the same way as (8.27),
only to see all the logarithms combine at
the end, but shows us what the general
form of the answer has to be–a rational
function plus a sum of logarithms with
constant coeﬃcients We may need to
introduce as algebraic numbers all the
roots of the denominator, but no more
algebraic numbers than that. The denom-
inator of the rational function is going to
have11) the same factors as the original r,
but with multiplicity reduced by one, i.e.
∏rni−1
i
. This is, in fact, R ∶= gcd (r, r′) and
11) Strictly speaking, this argument only proves
that the denominator is at most this big. But it
must be at least this big, else its derivative, the
integrand, would not have the denominator it
has.

310
8 Computer Algebra
so could be computed without the explicit
factorization of step 2 above.
It can be shown, either by Galois theory
or algorithmically, that, in fact, the numera-
tor of the rational part of the integral can be
written without any of the algebraic num-
bers we conceptually added in step 2 either.
Hence
∫
q
r = Q
R +
n
∑
i=1
ni
∑
j=i
𝛽i,j,1log(x −𝛼i,j). (8.30)
Diﬀerentiating this shows that
( n
∑
i=1
ni
∑
j=i
𝛽i,j,1log(x −𝛼i,j)
)′
is also a rational function not needing any
of the algebraic numbers we conceptually
added in step 2: call it S∕(r∕R), but we
should note that this time we have not
proved that the denominator in lowest
terms is exactly r∕R, and indeed if there are
no logarithmic terms (all 𝛽i,j,1 = 0), we may
have denominator just 1.
Diﬀerentiating (8.30) gives us
q
r =
(Q
R
)′
+
S
r∕R =
Q′(r∕R) −Q R′r
R2 + SR
r
,
(8.31)
and if we write Q and S as polynomials
in x with unknown coeﬃcients, (8.31)
gives us a set of linear equations for
these unknowns–a method known as
the
Horowitz–Ostrogradski
algorithm
[53–55].
Having found Q∕R comparatively easily,
we are left with ∫(S∕(r∕R))dx, whose inte-
gral may or may not involve individual roots
of r∕R–see the discussion around (8.27).
Hence the real challenge is to integrate
(S∕(r∕R)) without adding any unnecessary
algebraic numbers. This was solved, and in
a very satisfactory manner, independently
in [56, 57]. It produces a set of polynomi-
als Qi, all of whose roots are necessary for
the expression of ∫(S∕(r∕R))dx in terms of
logarithms, and such that the integral is
∑
i
∑
𝜌=RootOf(Qi)
gcd (S −𝜌( r
R)′, r
R).
(8.32)
Applying this algorithm to (8.27), we actu-
ally get
1
8
∑
𝜌= RootOf(𝛼8 −64 𝛼7 + 1792 𝛼6
−28672 𝛼5 + 286720 𝛼4 −1835008 𝛼3
+ 7340032 𝛼2 −16777216 𝛼+ 16777217)
𝜌log(x + 𝜌+ 8),
(8.33)
and (8.27) is the result of forcing a con-
version of the RootOf into radicals: the
polynomial is in fact (𝛼−8)8 + 1, another
example of a small expression that is not
small in either the dense or the sparse
model.
8.6.2
More Complicated Functions
Equation (8.29) shows that every ratio-
nal function has an integral that can be
expressed as a rational function plus a sum
of logarithms with constant coeﬃcients.
Conversely, we are used in calculus to
statements like
e−x2 has no integral,
(8.34)
which is nonsense as a statement of analy-
sis, let alone numerical computation, and is
really the algebraic statement
there is no formula f (x)
such that f ′(x) = e−x2.
(8.35)
Again, this is not correct, as Maple will tell
us
∫e−x2dx = 1
2
√
𝜋erf (x).
(8.36)

8.6 Integration
311
The key point really is that this statement is
an oxymoron, because the deﬁnition of erf
is
(erf(x))′ =
√
4
𝜋e−x2;
erf(0) = 0.
(8.37)
Equation (8.36) is qualitatively diﬀerent
from
∫x3e−x2dx = −1
2
(x2 + 1
)
e−x2,
(8.38)
which expresses an integral in terms of
things “we already know about.” Hence the
trick is formalizing that last phrase. Just as
Deﬁnition 8.4 provided a purely algebraic
deﬁnition of diﬀerentiation, we can do the
same with functions normally thought of as
deﬁned by calculus.
Deﬁnition 8.5 We say that the abstract
symbol 𝜃is
• a logarithm of u if 𝜃′ = u′∕u;
• an exponential of u if 𝜃′ = u′𝜃.
Deﬁnition 8.6 We say that C(x, 𝜃1, … , 𝜃n)
is a ﬁeld of elementary functions if each 𝜃i
is
1. algebraic over C(x, 𝜃1, … , 𝜃i−1);
2. a logarithm of u ∈C(x, 𝜃1, … , 𝜃i−1);
3. an exponential of u ∈C(x, 𝜃1, … , 𝜃i−1).
As sin(x) = (1∕2i) (exp(ix) −exp(−ix)) and
arcsin(x) = −i log(
√
1 −x2 + ix), and so on,
the trigonometric and inverse trigonomet-
ric functions are also included in this class.
The textbook “∫(1∕(1 + x2))dx = arctan x”
is really
“ ∫
1
1 + x2 dx = i
2 log
(1 −ix
1 + ix
)
.”
(8.39)
We can now make (8.35) precise.
There is no elementary ﬁeld containing a
u with u′ = exp(−x2).
(8.35′)
How does one, or indeed a computer,
prove such a result? And yet computers
can and do: when Maple, say, responds
to ∫exp(log4(x))dx by just echoing back
the formula, it is not merely saying “I
couldn’t ﬁnd an integral,” it is also asserting
that it has a proof that the integral is not
elementary.
The key result is a major generalization of
(8.29).
Theorem 8.5 (Liouville’s Principle) If
u ∈C(x, 𝜃1, … , 𝜃n) has an elementary inte-
gral v in some larger ﬁeld C(x, 𝜃1, … , 𝜃m), it
is possible to write
∫u = v = w +
k∑
i=1
ci log wi,
(8.40)
where w and the wi ∈C(x, 𝜃1, … , 𝜃n).
The proof, while somewhat technical, is
based on the facts that diﬀerentiation can-
not eliminate new exponentials, or new
algebraics, and can only eliminate a new
logarithm if it has a constant coeﬃcient. If
u itself is elementary, it is possible to go fur-
ther and produce an algorithm that will ﬁnd
v, or prove that no such elementary v exists
[58–60].
8.6.3
Linear Ordinary Diﬀerential Equations
If we actually apply the algorithm we have
just stated exists, it will state that, if elemen-
tary,
∫exp(−x2)dx = w(x) exp(−x2),
and
hence, equating coeﬃcients of exp(−x2)
in
exp(−x2) = (w(x) exp(−x2))′,
that
1 = w′ −2xw. The algorithm thus also has
to solve this linear diﬀerential equation,
and prove that no rational function can
solve it.
In fact, the method of integrating factors
shows that there is a complete equivalence

312
8 Computer Algebra
between integration and solving ﬁrst-order
linear diﬀerential equations. We can extend
Deﬁnition 8.6 to allow arbitrary integrals,
and therefore solutions of linear ﬁrst-order
diﬀerential equations, in the class of allow-
able functions.
Deﬁnition 8.7 We say that C(x, 𝜃1, … , 𝜃n)
is a ﬁeld of Liouvillian functions if each 𝜃i is
1. algebraic over C(x, 𝜃1, … , 𝜃i−1);
2. an integral of u ∈C(x, 𝜃1, … , 𝜃i−1),that
is, 𝜃′
i = u;
3. an exponential of u ∈C(x, 𝜃1, … , 𝜃i−1).
We can then ask whether second- or
higher-order diﬀerential equations have
solutions in terms of Liouvillian functions:
a question solved in [61, 62].
8.7
Interpreting Formulae as Functions
The previous section treated exp(x) and
log(x) as abstract symbolic expressions, and
attached no meaning to exp and log as
functions R →R or R+ →R. Similarly, we
have regarded
√
2 as a number 𝛼with 𝛼2 =
2, have ignored the inconvenient fact that
there are two such numbers, and attached
no meaning to
√
as a function R+ →R.
These interpretation questions are looked
at in the next two subsections.
8.7.1
Fundamental Theorem of Calculus
Revisited
Deﬁnitions 8.4 and 8.5 deﬁned diﬀerentia-
tion of formulae in a purely algebraic way,
and therefore (8.26) seemed to reduce the
problem of integration to that of reversing
diﬀerentiation. From the algebraic point
of view, this is correct. From the analytic
point of view, though, there is some-
thing to prove, and that proof imposes
side-conditions.
Theorem 8.6 (Fundamental Theorem of
Calculus, e.g., [Section 5.3, 63]) Let f and
F be functions deﬁned on a closed inter-
val [a, b] such that F′ = f . If f is Riemann-
integrable on [a, b], then
∫
b
a
f (x)dx = F(b) −F(a) written [F]b
a .
Though this is the classical statement, we
emphasize that F′ = f must hold through-
out [a, b], and therefore F is diﬀerentiable,
hence continuous, throughout this interval.
The condition about f being Riemann-
integrable is necessary to prevent calcula-
tions such as
∫
1
−1
1
x3 dx
?=
[ −1
2x2
]1
−1 = −1
2 −−1
2 = 0,
(8.41)
whereas, in fact, both ∫0
−1(1∕(x3))dx and
∫1
0 (1∕(x3))dx are undeﬁned.
The warning about continuity would also
cover this case, as −1∕2x2 is not continu-
ous, and might otherwise seem unneces-
sary: after all, are not exp and log contin-
uous?
For a ﬁrst example, consider
∫
1
2 x2 −6 x + 5dx
=
{
F1
∶=
−arctan
(
x−1
x−2
)
F2
∶=
arctan (2 x −3)
,
(8.42)
where arctan′(x) = 1∕(1 + x2). From the
point of view of Section 8.6, both are
equally valid; F1 and F2 both diﬀerentiate
to 1∕(2 x2 −6 x + 5). However, there is a
big diﬀerence from the point of view of
Theorem 8.6: F1 is discontinuous at x = 2,
while F2 is not, as seen in Figure 8.2, which

8.7 Interpreting Formulae as Functions
313
0
0.5
1.5
1
1
2
z
3
4
−1.5
−0.5
−1
Figure 8.2
F1 and F2 from
(8.42).
also shows a “constant” diﬀerence as well.
In fact [F2
]3
1 = arctan(3) + 𝜋∕4, whereas
[F1
]3
1 = −arctan(2), a silly result for the
integral of a positive function. In fact, the
two diﬀer by 𝜋, which is the magnitude of
the discontinuity in arctan “at inﬁnity,”12)
that is, when x = 2 in F2. While F1 and F2
both diﬀerentiate correctly at most values
of x, F1 is diﬀerentiable at x = 2, and there-
fore satisﬁes the diﬀerentiability hypothesis
of Theorem 8.6, while F2 is not continuous,
and therefore not diﬀerentiable, at x = 2,
and does not satisfy the hypothesis.
As arctan is built from log, we must ques-
tion our bland assertion that “log is con-
tinuous.” In fact, while it is continuous as a
function R+ →R, it is not continuous C →
C, having a branch cut on the negative real
axis.13) As x goes from −∞to ∞, the argu-
ment of the logarithm in (8.39) goes from
12) limx→∞arctan(x) = 𝜋∕2, whereas
limx→−∞arctan(x) = −𝜋∕2.
13) This is the conventional location of the branch
cut these days: see [64].
−1 clockwise round the unit circle, passing
through 1 when x = 0 and arriving back at
−1, but from the other side of the branch
cut from that from which it departed.
Hence, while the methods of Section 8.6
can produce formulae that diﬀerentiate
(in the sense of Deﬁnitions 8.4 and 8.5)
correctly to the formulae being integrated,
interpreting these formulae as functions,
and doing deﬁnite integration, requires us
to check that these formulae actually deﬁne
continuous functions over the relevant
range.
8.7.2
Simpliﬁcation of Functions
As
it
is
possible
to
deﬁne
√
x =
exp (1∕2 log(x)), it follows that this func-
tion has a branch cut in the same place as
log, traditionally along the negative real
axis. As with log, this function is continu-
ous R+ →R+, but is not continuous C →C.
We write R+ →R+ because, as R+ →R,

314
8 Computer Algebra
√
is ambiguous: is
√
4 equal to 2 or −2?
In fact, it is also ambiguous C →C, and
we normally consider
√
∶C →C+, where
C+ = {x + iy ∶x > 0 ∨(x = 0 ∧y ≥0)},
the “positive half-plane.”
This resolves ambiguity, but does not
solve
all
our
problems.
The
putative
equation
√
x√y
?= √xy,
(8.43)
which is true for
√
∶R+ →R+, fails
for
√
∶C →C+:
consider
x = y = −1.
Manipulating such putative identities is
dangerous, and most computer algebra
systems will, by default, refuse to do this:
for
example
Maple’s
simplify
does
not, but simplify(...,symbolic)
(sometimes) will. However, just because
(8.43) is not universally true does not mean
that some relatives are not true. Consider
the apparently similar
√
z −1
√
z + 1
?=
√
z2 −1
(8.44)
√
1 −z
√
1 + z
?=
√
1 −z2.
(8.45)
Equation (8.45) is in fact true through-
out C, whereas (8.44) is only true on
C+ ∪[−1, 0]. The diﬀerence is that the
branch cuts of (8.44) include the imaginary
axis, and divide C into distinct regions,
whereas the branch cuts of (8.45) do not
separate the complex plane. The appar-
ently bizarre fact that (8.44) is also valid
on [−1, 0] is due to the fact that multiple
branch cuts include this, and their eﬀects
cancel. Equally, though the components of
(8.45) have branch cuts, they lie on top of
each other, and the eﬀects cancel, meaning
that (8.45) itself has only a spurious cut
[65]. See [66, 67] for the general theory,
though it has to be admitted that imple-
mentations in computer algebra systems
are few.
These problems are not conﬁned to
√
.
log
is
also
not
uniquely
deﬁned
C →C, because 1 = exp(0) = exp(2𝜋i) =
exp(4𝜋i) = …
Many
mathematicians
are content to live with this ambiguity,
which is of course anathema to computer
programmers –see [64].
log z1 + log z2
?= log z1z2.
(8.46)
The equation merely states that the
sum of one of the (inﬁnitely many) log-
arithms of z1 and one of the (inﬁnitely
many) logarithms of z2 can be found
among the (inﬁnitely many) logarithms
of z1z2, and conversely every logarithm
of z1z2 can be represented as a sum
of this kind (with a suitable choice of
log z1 and log z2).
[68, pp. 259–260] (our notation)
It is more normal to use capital letters
to denote the multivalued functions, so
that log(z) = {log(z) + 2k𝜋i ∶k ∈Z}, and
instead of (8.46) to write
logz1 + logz2 = logz1z2,
(8.47)
interpreting “+” as elementwise addition of
sets. With this notation, log(1∕z) = −log(z),
but the single-valued equivalent log(1∕z)
?=
−log(z) is true except on the negative real
axis, where the branch cuts do not can-
cel. Again, it is fair to say that implementa-
tions in computer algebra systems lag sig-
niﬁcantly behind the theory.
8.7.3
Real Problems
It would be tempting to dismiss these issues
as only aﬀecting complex numbers. This is
not true: consider the often-quoted iden-
tity

References
315
arctan(x) + arctan(y) = arctan
( x + y
1 −yx
)
.
(8.48)
It is certainly true at x = y = 0, and
the partial derivatives of the two sides
are equal, so we might be tempted to
conclude that it is true everywhere “by
analytic
continuation.”
However,
when
x = y = 2, the left-hand side is positive and
the right-hand side negative, so something
is clearly wrong. The problem is the same
“discontinuity at inﬁnity” of arctan, that is,
when xy = 1, as we saw in Figure 8.2.
In fact, the correct version of (8.48) is
(8.49):
arctan(x) + arctan(y)
= arctan
( x + y
1 −yx
)
+
⎧
⎪
⎨
⎪⎩
−𝜋
xy > 1; x < 0
0
< 1
𝜋
xy > 1; x > 0
.
(8.49)
8.8
Conclusion
We have seen various examples of where
the ad hoc methods we have generally
learnt at school, and which process small-
ish examples well, can be converted into
algorithms for doing algebraic manipula-
tion, be it gcd computations (Section 8.4),
factoring
polynomials
(Section 8.4.4),
solving
nonlinear
polynomial
systems
(Section 8.5.5),
or
indeﬁnite
integra-
tion
(Section 8.6).
Equally,
we
have
seen that this is not always possible
(Theorem 8.3).
Section 8.7 also reminds us that there
is more to mathematics than just algebra,
and that we need to be careful when inter-
preting algebraic objects as actual functions
C →C, or even R →R, as in (8.49).
References
1. IEEE (1985) IEEE Standard 754 for Binary
Floating-Point Arithmetic, IEEE.
2. Baker, A. (1975) Transcendental Number
Theory, Cambridge University Press.
3. Silverman, J.H. and Tate, J. (1992) Rational
Points on Elliptic Curves, Springer-Verlag.
4. Kahrimanian, H.G. (1953) Analytic
diﬀerentiation by a digital computer.
Master’s thesis, Temple University
Philadelphia.
5. Nolan, J. (1953) Analytic diﬀerentiation on a
digital computer. Master’s thesis,
Mathematics Department M.I.T.
6. Haselgrove, C.B. (1953) Implementations of
the Todd-Coxeter algorithm on EDSAC-1,
Unpublished.
7. Collins, G.E. (1966) PM, a system for
polynomial multiplication. Commun. ACM,
9, 578–589.
8. Martin, W.A. (1967) Symbolic mathematical
laboratory. PhD thesis, M.I.T. & Project
MAC TR-36.
9. Moses, J. (2010) Macsyma: a personal
history. J. Symb. Comput., 47, 123–130.
10. Fitch, J.P. (1974) CAMAL Users Manual,
University of Cambridge Computer
Laboratory.
11. Hearn, A.C. (1973) REDUCE-2 User’s
Manual. Technical Report UCP-19,
Computational Physics Group University of
Utah.
12. Cannon, J.J. (1974) A general purpose group
theory program. Proceedings 2nd
International Conference on Theory of
Groups, pp. 204–217.
13. Bosma, W., Cannon, J., and Playoust, C.
(1997) The Magma algebra system. I: the user
language. J. Symb. Comput., 24, 235–265.
14. Giovini, A. and Niesi, G. (1990) CoCoA: a
user-friendly system for commutative
algebra. Proceedings DISCO ’90, Springer,
pp. 20–29.
15. Abbott, J.A. (2004) CoCoA: a laboratory for
computations in commutative algebra. ACM
SIGSAM Bull. 1, 38, 18–19.
16. Greuel, G.-M., Pﬁster, G., and Schönemann,
H. (2001) SINGULAR –a computer algebra
system for polynomial computations, in
Proceedings Calculemus 2000 (eds M. Kerber
and M. Kohlhase), A.K. Peters, Boston Mass,
pp. 227–234.

316
8 Computer Algebra
17. Bayer, D. and Stillman, M. (1986) The design
of Macaulay: a system for computing in
algebraic geometry and commutative
algebra. Proceedings SYMSAC 86, pp.
157–162.
18. Grayson, D. and Stillman, M. (2009)
Macaulay2, a software system for research in
algebraic geometry,
http://www.math.uiuc.edu/Macaulay2/
(accessed on 25 February 2014).
19. Karatsuba, A. and Ofman, J. (1963)
Multiplication of multidigit numbers on
automata. Sov. Phys. Dokl., 7, 595–596.
20. Schönhage, A. and Strassen, V. (1971)
Schnelle Multiplikation großer Zahlen.
Computing, 7, 282–292.
21. Johnson, S.C. (1974) Sparse polynomial
arithmetic. Proceedings EUROSAM 74, pp.
63–71.
22. Yan, T. (1998) The geobucket data structure
for polynomials. J. Symb. Comput., 25,
285–294.
23. Char, B.W., Geddes, K.O., Gentleman, M.W.,
and Gonnet, G.H. (1983) The design of
MAPLE: a compact, portable and powerful
computer algebra system. Proceedings
EUROCAL 83, pp. 101–115.
24. Davenport, J.H. and Carette, J. (2010) The
sparsity challenges, in Proceedings SYNASC
2009 (eds S. Watt et al.), IEEE Press, pp. 3–7.
25. Casta˜no, B., Heintz, J., Llovet, J., and
Mart´ınez, R. (2000) On the data structure
straight-line program and its
implementation in symbolic computation.
Math. Comput. Simulat., 51, 497–528.
26. Schinzel, A. (2003) On the greatest common
divisor of two univariate polynomials, I. A
Panorama of Number Theory or the View
from Baker’s Garden, pp. 337–352.
27. Plaisted, D.A. (1977) Sparse complex
polynomials and irreducibility. J. Comput.
Syst. Sci., 14, 210–221.
28. Davenport, J.H. (2015) Computer algebra, To
be published by C.U.P. in 2014.
29. Zippel, R.E. (1993) Eﬀective Polynomial
Computation, Kluwer Academic Publishers.
30. Karpinski, M. and Shparlinski, I. (1999) On
the computational hardness of testing
square-freeness of sparse polynomials, in
Proceedings AAECC-13 (eds M. Fossorier, H.
Imai, S. Lin, and A. Poli), Springer,
pp. 492–497.
31. Landau, E. (1905) Sur Quelques Théorèmes
de M. Petrovic Relatif aux Zéros des
Fonctions Analytiques. Bull. Soc. Math.
France, 33, 251–261.
32. Mignotte, M. (1974) An inequality about
factors of polynomials. Math. Comput., 28,
1153–1157.
33. Mignotte, M. (1981) Some inequalities about
univariate polynomials. Proceedings
SYMSAC 81, pp. 195–199.
34. Brown, W.S. (1971) On Euclid’s algorithm
and the computation of polynomial greatest
common divisors. J. ACM, 18, 478–504.
35. Berlekamp, E.R. (1967) Factoring
polynomials over ﬁnite ﬁelds. Bell Syst. Tech.
J., 46, 1853–1859.
36. Berlekamp, E.R. (1970) Factoring
polynomials over large ﬁnite ﬁelds. Math.
Comput., 24, 713–735.
37. Cantor, D.G. and Zassenhaus, H. (1981) A
new algorithm for factoring polynomials
over ﬁnite ﬁelds. Math. Comput., 36,
587–592.
38. Swinnerton-Dyer, H.P.F. (1970) Letter to
E.H. Berlekamp. Mentioned in [36].
39. Kaltofen, E., Musser, D.R., and Saunders,
B.D. (1983) A generalized class of
polynomials that are hard to factor. SIAM J.
Comput., 12, 473–483.
40. Abbott, J.A., Bradford, R.J., and Davenport,
J.H. (1985) A remark on factorisation.
SIGSAM Bull. 2, 19, 31–33.
41. Abbott, J.A., Shoup, V., and Zimmermann, P.
(2000) Factorization in ℤ[x]: the searching
phase, in Proceedings ISSAC 2000 (ed. C.
Traverso), ACM, New York, pp. 1–7.
42. von Tschirnhaus, E.W. (1683) Methodus
auferendi omnes terminos intermedios ex
data aeqvatione. Acta Eruditorium, 2,
204–207.
43. Galois, É. (1879) Œuvres mathématiques.
Gauthier-Villars (sous l’auspices de la SMF).
44. Kac, M. (1943) On the average number of
real roots of a random algebraic equation.
Bull. A.M.S., 49, 314–320.
45. Lerario, A. and Lundberg, E. (2012) Statistics
on Hilbert’s Sixteenth Problem,
http://arxiv.org/abs/1212.3823.
46. Coste, M. and Roy, M.F. (1988) Thom’s
Lemma, the coding of real algebraic
numbers and the computation of the
topology of semi-algebraic sets. J. Symb.
Comput., 5, 121–129.
47. Becker, T., Weispfenning, V. (with Kredel,
H.) Groebner Bases: A Computational

References
317
Approach to Commutative Algebra,
Springer-Verlag, 1993.
48. Mayr, E.W. and Ritscher, S. (2010) Degree
bounds for Gröbner bases of
low-dimensional polynomial ideals, in
Proceedings ISSAC 2010 (ed. S.M. Watt),
ACM, New York, pp. 21–28.
49. Fortuna, E., Gianni, P., and Trager, B. (2001)
Degree reduction under specialization. J.
Pure Appl. Algebra, 164, 153–163.
50. Aubry, P., Lazard, D., and Moreno Maza, M.
(1999) On the theories of triangular sets. J.
Symb. Comput., 28, 105–124.
51. Chen, C., Davenport, J.H., May, J.P.,
Moreno Maza, M., Xia, B., and Xiao, R.
(2013) Triangular decomposition of
semi-algebraic systems. J. Symb. Comput.,
49, 3–26.
52. Rich, A.D. and Jeﬀrey, D.J. (2009) A
knowledge repository for indeﬁnite
integration based on transformation rules, in
Proceedings Intelligent Computer
Mathematics (eds J. Carette et al.), Springer,
pp. 480–485.
53. Horowitz, E. (1969) Algorithm for symbolic
integration of rational functions. PhD thesis,
University of Wisconsin.
54. Horowitz, E. (1971) Algorithms for partial
fraction decomposition and rational
function integration. Proceedings Second
Symposium on Symbolic and Algebraic
Manipulation, pp. 441–457.
55. Ostrogradski, M.W. (1845) De l’intégration
des fractions rationelles. Bull. Acad. Imp. Sci.
St. Petersburg (Class Phys.-Math.), 4,
145–167.
56. Rothstein, M. (1976) Aspects of symbolic
integration and simpliﬁcation of exponential
and primitive functions. PhD thesis,
University of Wisconsin.
57. Trager, B.M. (1976) Algebraic factoring and
rational function integration, in Proceedings
SYMSAC 76 (ed. R.D. Jenks), ACM, New
York, pp. 219–226.
58. Risch, R.H. (1969) The problem of
integration in ﬁnite terms. Trans. A.M.S.,
139, 167–189.
59. Davenport, J.H. (1981) On the Integration of
Algebraic Functions, Springer Lecture Notes
in Computer Science 102, Springer.
60. Bronstein, M. (2005) Symbolic Integration I,
2nd edn, Springer-Verlag.
61. Kovacic, J.J. (1986) An algorithm for solving
second order linear homogeneous
diﬀerential equations. J. Symb. Comput., 2,
3–43.
62. Singer, M.F. (1981) Liouvillian solutions of
n-th order homogeneous linear diﬀerential
equations. Am. J. Math., 103, 661–682.
63. Apostol, T.M. (1967) Calculus, Vol. I, 2nd
edn, Blaisdell.
64. Davenport, J.H. (2010) The challenges of
multivalued “Functions”, in Proceedings
AISC/Calculemus/MKM 2010 (eds S.
Autexier et al.), Springer, pp. 1–12.
65. England, M., Bradford, R., Davenport, J.H.,
and Wilson, D.J. (2013) Understanding
branch cuts of expressions, in Proceedings
CICM 2013 (eds J. Carette et al.), Springer,
pp. 136–151.
66. Bradford, R.J. and Davenport, J.H. (2002)
Towards better simpliﬁcation of elementary
functions, in Proceedings ISSAC 2002 (ed. T.
Mora), ACM, New York, pp. 15–22.
67. Davenport, J.H. (2003) The geometry of Cn is
important for the algebra of elementary
functions. Algebra Geometry and software
systems, Springer, pp. 207–224.
68. Carathéodory, C. (1958) Theory of Functions
of a Complex Variable, Chelsea Publishing.


319
9
Diﬀerentiable Manifolds
Marcelo Epstein
9.1
Introduction
In his authoritative Physics,1)
Aristotle
(384–322 BC) establishes that space “has
three dimensions, length, breadth, depth,
the dimensions by which all body also is
bounded.”2) Time is regarded intuitively
as one dimensional. Moreover, both space
and motion are considered as being con-
tinuous in the sense that they are “divisible
into divisibles that are inﬁnitely divisi-
ble.”3) The continuity of motion and space
implies, therefore, the continuity of time.4)
From these modest beginnings, it would
take 23 centuries to arrive at a rigorous
mathematical deﬁnition of the most gen-
eral entity that combines the intuitive
notions of continuity and constancy of
dimension. Accordingly, we introduce ﬁrst
the notion of topological space, which is
1) Aristotle, Physics, Hardie R P and Gaye R K
(translators), The Internet Classics Archive.
2) Ibid., Book IV, Part 1.
3) The Greek term used is 𝜎𝜐𝜈𝜖𝜒́𝜂𝜍, which literally
means “holding together."
4) Ibid., Book IV, Part 11.
the most general entity that can sustain
continuous functions, and, subsequently,
the notion of topological manifold, which
is a topological space that locally resembles
ℝn.
9.2
Topological Spaces
9.2.1
Deﬁnition
A topological space is a set A in which a
topology has been introduced. A topology
(A) on the set A is a collection of subsets
of A, called the open sets of A, with the
following properties:
1. The empty set ∅and the total space A
are open sets.
2. The union of any arbitrary collection of
open sets is an open set.
3. The intersection of any ﬁnite collection
of open sets is an open set.
Given a point a ∈A, a neighborhood of a
is an open set N(a) ∈(A) containing a. By
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

320
9 Diﬀerentiable Manifolds
Property 1 above, every point of A has at
least one neighborhood.
9.2.2
Continuity
A function f ∶A →B between the topolog-
ical spaces A and B is continuous at a ∈A if
for any neighborhood V of f (a) ∈B there
exists a corresponding neighborhood U of
a ∈A such that f (U) ⊂V. A function that
is continuous at every point of its domain
is said to be continuous. A function is con-
tinuous if, and only if, the inverse image of
every open set (in B) is open (in A).
A bijective (i.e., one-to-one and onto)
function f ∶A →B between topological
spaces is a homeomorphism if it is contin-
uous and its inverse f −1 ∶B →A is also
continuous. Topology can be understood
as the study of those properties that are
preserved under homeomorphisms.
9.2.3
Further Topological Notions
A subset B ⊂A of a topological space A
with a topology (A) inherits a topology
(B) as follows: The open sets of B are
deﬁned as the intersections of the open sets
of A with B. The topology (B) obtained
in this way is called the relative or subset
topology.
A topological space A is a Hausdorﬀ
space if, given any two diﬀerent points
a, b ∈A, there exist respective disjoint
neighborhoods N(a) and N(b), that is,
N(a) ∩N(b) = ∅.
A base of a topology (A) is a collec-
tion of open sets such that every open set of
A is a union of elements of . The topolog-
ical space is said to be generated by a base.
Thus, the open intervals of the real line ℝ
constitute a base of the ordinary topology
of ℝ.
Recall that a set is countable if it can be
put in a one-to-one correspondence with a
subset of the natural numbers ℕ. A topol-
ogy is said to be second countable (or to
satisfy the second axiom of countability) if
it has a countable basis. Second-countable
topologies enjoy many special properties.
A subset of a topological space A is closed
if its complement is open. Notice that an
arbitrary subset of a topological space need
not be either open or closed, or it may be
both open and closed. A topological space
is said to be connected if the only subsets
that are both open and closed are the empty
set and the total space.
An open cover of a topological space A is a
collection of open sets whose union is the
total space. An open subcover of is a sub-
collection of that is itself an open cover. A
topological space is compact if every open
cover has a ﬁnite open subcover.
The product topology of two topological
spaces, A and B, with respective topologies
(A) and (B), is the topology on the
Cartesian product A × B generated by
the base of all the Cartesian products of the
form S × T, where S ∈(A) and T ∈(B).
9.3
Topological Manifolds
9.3.1
Motivation
As, as we have shown, a topological space
is all one needs to deﬁne continuous func-
tions, it might appear that this is the proper
arena for the formulation of physical the-
ories involving the notion of a continuum.
Nevertheless, even from the Aristotelian
viewpoint, it seems that we also need to
convey the idea of a ﬁxed number of under-
lying dimensions, a concept that is not
automatically embodied in the deﬁnition

9.3 Topological Manifolds
321
of a topological space. If we think of the
surface of the Earth and its cartographic
representations, we understand intuitively
that, although the surface of a sphere can-
not be continuously mapped once and for
all onto the plane ℝ2, it can be so repre-
sented in a piecewise manner. In generaliz-
ing this example, we look for the notion of
a topological space that is piecewise home-
omorphic to ℝn.
9.3.2
Deﬁnition
An n-dimensional topological manifold 
is a Hausdorﬀsecond-countable topologi-
cal space such that each of its points has
a neighborhood homeomorphic to an open
set of ℝn.
The standard topology assumed in ℝn
is the one generated by all open balls in
ℝn. Recall that the open ball of center
(c1, … , cn) ∈ℝn and radius r ∈ℝis the
subset of ℝn deﬁned as {(x1, … , xn) ∈
ℝn ∶(x1 −c1)2 + · · · + (xn −cn)2 < r2}.
The empty set is obtained setting r ≤0.
Although we have deﬁned only ﬁnite-
dimensional topological manifolds, which
are modeled locally on ℝn, it is also possible
to deﬁne topological manifolds modeled on
inﬁnite-dimensional Banach spaces.
9.3.3
Coordinate Charts
It follows from the deﬁnition of topological
manifold that there exists an open cover
each of whose elements is homeomorphic
to an open set of ℝn. If we denote by 𝛼the
generic constituent of the open cover and
by 𝜙𝛼the corresponding homeomorphism,
where 𝛼denotes a running index, we can
identify the pair (𝛼, 𝜙𝛼) with a coordinate
chart. The collection of all these pairs is
called an atlas of the topological manifold.
The terminology of coordinate charts
arises from the fact that a chart introduces
a local coordinate system. More speciﬁcally,
the homeomorphism 𝜙𝛼∶𝛼→𝜙𝛼(𝛼) ⊂
ℝn assigns to each point p ∈𝛼an ordered
n-tuple (x1(p), … , xn(p)), called the local
coordinates of p.
Whenever two charts, (𝛼, 𝜙𝛼) and
(𝛽, 𝜙𝛽), have a nonempty intersection, we
deﬁne the transition function 𝜙𝛼𝛽as
𝜙𝛼𝛽= 𝜙𝛽∘𝜙−1
𝛼∶𝜙𝛼(𝛼∩𝛽)
→𝜙𝛽(𝛼∩𝛽).
(9.1)
Each transition function is a homeo-
morphism between open sets of ℝn. The
inverse of 𝜙𝛼𝛽is the transition function
𝜙𝛽𝛼= 𝜙−1
𝛼𝛽= 𝜙𝛼∘𝜙−1
𝛽. Denoting by xi and
yi (i = 1, … , n),
respectively,
the
local
coordinates of 𝛼and 𝛽, a transition
function boils down to the speciﬁcation of
n continuous and continuously invertible
real functions of the form
yi = yi(x1, … , xn),
i = 1, … , n.
(9.2)
9.3.4
Maps and Their Representations
If and are topological manifolds of
dimensions m and n, respectively, a map
f ∶→is continuous if it is a continu-
ous map between the underlying topolog-
ical spaces. A nice feature of topological
manifolds, as opposed to general topologi-
cal spaces, is the possibility of representing
continuous maps locally as real functions
of real variables. Let p ∈and denote
q = f (p) ∈. By continuity, we can always
choose a chart (, 𝜙) containing p such
that its image f () is contained in a chart
(, 𝜓) containing q. The map

322
9 Diﬀerentiable Manifolds
̂f = 𝜓∘f ∘𝜙−1 ∶𝜙() →𝜓()
(9.3)
maps an open set in ℝm to an open set in
ℝn. This continuous map ̂f is the local coor-
dinate representation of f in the coordinate
charts chosen.
9.3.5
A Physical Application
Lagrange’s
(1736–1813)
conception
of
mechanics was purportedly purely analyti-
cal. In the Preface to the ﬁrst edition of his
Mécanique Analitique,5) he explicitly states
that “On ne trouvera point de Figures dans
cet Ouvrage. Les méthodes que j’y expose
ne demandent ni constructions, ni raison-
nements géométriques ou méchaniques,
mais seulemnet des opérations algébriques,
assujeties à une marche régulière et uni-
forme. Ceux qui aiment l’Analyse verront
avec plaisir la Méchanique en devenir une
nouvelle branche, et me sauront gré d’en
avoir étendu ainsi le domain.” Neverthe-
less, it is not an exaggeration to say that
in laying down the foundations of Ana-
lytical Mechanics, Lagrange was actually
inaugurating
the
diﬀerential
geometric
approach to Physics. In Lagrange’s view, a
mechanical system was characterized by
a ﬁnite number n of degrees of freedom to
each of which a generalized coordinate is
assigned. A conﬁguration of the system
is thus identiﬁed with an ordered n-tuple
of real numbers. But, is this assignment
unique? And, anyway, what are these
numbers coordinates of?
Consider the classical example of a (rigid)
double pendulum oscillating in a vertical
plane under gravity. Clearly, this system
can be characterized by two independent
degrees of freedom. If we were to adopt as
5) Lagrange (1788), Mécanique Analitique [sic],
chez la Veuve Desaint, Libraire, Paris.
generalized coordinates the horizontal dis-
placements, x1 and x2, of the two masses
from, say, the vertical line through the point
of suspension, we would ﬁnd that to an arbi-
trary combination of these two numbers,
there may correspond as many as 4 diﬀer-
ent conﬁgurations. If, to avoid this problem,
we were to adopt as generalized coordinates
the angular deviations 𝜃1 and 𝜃2, we would
ﬁnd that a given conﬁguration can be char-
acterized by an inﬁnite number of com-
binations of values of these coordinates,
owing to the additive freedom of 2𝜋. If we
attempt to solve this problem by limiting
the range of these coordinates to the inter-
val [0, 2𝜋], we lose continuity of the repre-
sentation (because two neighboring conﬁg-
urations would correspond to very distant
values of the coordinates).
Let us, therefore, go against Lagrange’s
own advice and attempt to draw a mental
picture of the geometry of the situation. As
the ﬁrst mass (attached to the main point of
suspension) is constrained to move along a
circle, thus constituting a simple pendulum,
we conclude that its conﬁgurations can be
homeomorphically mapped onto a circum-
ference (or any other closed curve in the
plane). We say that this circumference is the
conﬁguration space of a simple pendulum.
Now, the second mass can describe a cir-
cumference around any position of the ﬁrst.
It is not diﬃcult to conclude that the con-
ﬁguration space of the double pendulum is
given by the surface of a torus. Now that
this basic geometric (topological) question
has been settled, we realize that an atlas
of this torus must consist of several charts.
But the central conceptual gain of the geo-
metrical approach is that the conﬁguration
space of a mechanical system, whose con-
ﬁgurations are deﬁned with continuity in
mind, can be faithfully represented by a
unique topological manifold, up to a home-
omorphism.

9.4 Diﬀerentiable Manifolds
323
9.3.6
Topological Manifolds with Boundary
Consider the upper half space deﬁned as
the subset ℍn of ℝn consisting of all points
satisfying the inequality xn ≥0. Moreover,
endow this subset with the subset topol-
ogy induced by the standard topology of
ℝn. An n-dimensional topological mani-
fold with boundary is a Hausdorﬀsecond-
countable topological space such that each
of its points has a neighborhood homeo-
morphic to an open set of ℍn.
The manifold boundary 𝜕M of a topolog-
ical manifold M with boundary is deﬁned
as the set of all points of M whose last
coordinate xn vanishes. Notice that this def-
inition is independent of the atlas used.
Thus, a topological manifold is a particular
case of a topological manifold with bound-
ary, namely, when the manifold boundary is
empty.
As a physical example, consider the case
of a plane pendulum suspended by means
of a wrinkable, but inextensible, thread. Its
conﬁguration space consists of a solid disk,
including the circumference and the inte-
rior points. Equivalently, any other solid
simply connected plane ﬁgure can be used
as conﬁguration space of this mechanical
system.
9.4
Diﬀerentiable Manifolds
9.4.1
Motivation
As we have learned, topological manifolds
provide the most general arena for the def-
inition of continuous functions. Continu-
ity alone, however, may not be enough to
formulate physical problems in a quantita-
tive manner. Indeed, experience with actual
dynamical and ﬁeld theories of Mechanics
and Electromagnetism, to name only the
classical theories, has taught us to expect
the various phenomena to be governed by
ordinary or partial diﬀerential equations.
These theories, therefore, must be formu-
lated on a substratum that has more struc-
ture than a topological manifold, namely, an
entity that allows for the deﬁnition of diﬀer-
entiable functions. Diﬀerentiable manifolds
are the natural generalization of topological
manifolds to handle diﬀerentiability. A dif-
ferentiable manifold corresponds roughly
to what physicists call a continuum.
9.4.2
Deﬁnition
As a topological space does not possess
in itself enough structure to sustain the
notion of diﬀerentiability, the key to the
generalization of a topological manifold is
to be found in restrictions imposed upon
the transition functions, which are clearly
deﬁned in ℝn. Two charts, (𝛼, 𝜙𝛼) and
(𝛽, 𝜙𝛽), of a topological manifold are
said to be Ck-compatible, if the transition
functions 𝜙𝛼𝛽and 𝜙𝛽𝛼, as deﬁned in (9.1),
are of class Ck. In terms of the represen-
tation (9.2), this means that all the partial
derivatives up to and including the order
k exist and are continuous. By conven-
tion, a continuous function is said to be
of class C0 and a smooth function is of
class C∞.
In a topological manifold, all charts
of all possible atlases are automatically
C0-compatible. An atlas of class Ck of a
topological manifold is an atlas whose
charts are Ck-compatible. Two atlases of
class Ck are compatible if each chart of one
is compatible with each chart of the other.
The union of compatible Ck-atlases is a
Ck atlas. Given a Ck atlas, one can deﬁne

324
9 Diﬀerentiable Manifolds
the corresponding maximal compatible
atlas of class Ck as the union of all atlases
that are Ck-compatible with the given
one. A maximal atlas, thus, contains all its
compatible atlases.
An n-dimensional diﬀerentiable man-
ifold of class Ck
is an n-dimensional
topological manifold together with a
maximal atlas of class Ck. For k = 0 one
recovers the topological manifold. The C∞
case delivers a smooth manifold, or simply a
manifold.
A maximal Ck-atlas is also called a
Ck-diﬀerentiable structure. Thus, a Ck-
manifold is a topological manifold with a
Ck-diﬀerentiable structure. For the par-
ticular case of ℝn, we can choose the
canonical atlas consisting of a single chart
(the space itself) and the identity map.
The induced C∞-diﬀerentiable structure
is the standard diﬀerentiable structure
of ℝn.
A diﬀerentiable manifold is oriented if it
admits an atlas, called an oriented atlas,
such that all the transition functions have
a positive Jacobian determinant. Two ori-
ented atlases are either compatible or every
transition function between charts of the
two atlases has a negative determinant. An
oriented manifold is an orientable mani-
fold with and oriented maximal atlas. In
other words, only those coordinate trans-
formations that preserve the orientation are
permitted.
Given two diﬀerentiable manifolds, 
and , of dimensions m and n, respec-
tively, we deﬁne the (m + n)-dimensional
product manifold by endowing the Carte-
sian product × with an atlas made
of all the Cartesian products of charts
of an atlas of and an atlas of .
The underlying topological space of a
product manifold inherits the product
topology.
9.4.3
Diﬀerentiable Maps
Let and be (smooth) manifolds of
dimensions m and n, respectively. A con-
tinuous map f ∶→is diﬀerentiable
of class Ck at a point p ∈if, using charts
(, 𝜙) and (, 𝜓) belonging to the respec-
tive maximal atlases of and , the local
coordinate representation ̂f of f , as deﬁned
in (9.3), is of class Ck at 𝜙(p) ∈ℝm. This def-
inition is independent of chart, because the
composition of diﬀerentiable maps in ℝm is
diﬀerentiable. Notice how the notion of dif-
ferentiability within the manifolds has been
cleverly deﬂected to the charts.
Maps of class C∞are said to be smooth
maps, to which we will conﬁne our analy-
sis from now on. In the special case = ℝ,
the map f ∶→ℝis called a (real) func-
tion. When, on the other hand, is an
open interval H = (a, b) of the real line, the
map 𝛾∶H →is called a (parameterized)
curve in . The name diﬀeomorphism is
reserved for the case in which and are
of the same dimension and both f and its
(assumed to exist) inverse f −1 are smooth.
Two manifolds of the same dimension are
said to be diﬀeomorphic if there exists a dif-
feomorphism between them.
9.4.4
Tangent Vectors
Let H be an open interval of the real line
and, without loss of generality, assume
that 0 ∈H. Consider the collection of all
(smooth) curves 𝛾∶H →such that
𝛾(0) = p. Our aim is to deﬁne the notion
of tangency of two such curves at p, an
aim that we achieve by using the technique
of deﬂecting to charts. Indeed, if (, 𝜙)
is a chart containing p, the composition
̂𝛾= 𝜙∘𝛾∶H →ℝm is a curve in ℝm, where
m is the dimension of . The coordinate

9.4 Diﬀerentiable Manifolds
325
expression of ̂𝛾is given by m smooth real
functions xi = 𝛾i(t), where t is the natural
coordinate of ℝand i = 1, … , m. We say
that two curves, 𝛾1 and 𝛾2, in our collection
are tangent at p if
d𝛾i
1
dt
|||||t=0
=
d𝛾i
2
dt
|||||t=0
,
i = 1, … , m.
(9.4)
It is a simple matter to verify that this deﬁ-
nition is independent of chart.
Noting that tangency at p is an equiva-
lence relation, we deﬁne a tangent vector
at p as an equivalence class of (smooth,
parameterized) curves tangent at p. A tan-
gent vector is thus visualized as what the
members of a collection of tangent (param-
eterized) curves have in common. More
intuitively, one may say that what these
curves have in common is a small piece of a
curve.
Let
f ∶→ℝbe a (diﬀerentiable)
function and let v be a tangent vector at
p ∈. Choosing any representative 𝛾in
the equivalence class v, the composition
f ∘𝛾is a real-valued function deﬁned on H.
The derivative of f at p along v is deﬁned as
v(f ) =
d(f ∘𝛾)
dt
||||t=0
.
(9.5)
This notation suggests that a vector can be
regarded as a linear operator on the collec-
tion of diﬀerentiable functions deﬁned on
a neighborhood of a point. The linearity is
a direct consequence of the linearity of the
derivative. Not every linear operator, how-
ever, is a tangent vector because, by virtue
of (9.5), tangent vectors must also satisfy
the Leibniz rule, namely, for any two func-
tions f and g
v(fg) = f v(g) + v(f )g,
(9.6)
where, on the right-hand side, f and g are
evaluated at p.
9.4.5
Brief Review of Vector Spaces
9.4.5.1
Deﬁnition
Recall that a (real) vector space V is charac-
terized by two operations: vector addition
and multiplication by a scalar. Vector
addition, denoted by +, is associative and
commutative. Moreover, there exists a zero
vector, denoted by 0, such that it leaves
all vectors unchanged upon addition. For
each vector v there exists a vector −v
such
that
v + (−v) = 0.
Multiplication
by a scalar, indicated by simple apposi-
tion, satisﬁes the consistency condition
1v = v for all vectors v. It is, moreover,
associative, namely, (𝛼𝛽)v = 𝛼(𝛽v), and
distributive,
namely,
𝛼(u + v) = 𝛼u + 𝛼v
and (𝛼+ 𝛽)v = 𝛼v + 𝛽v, for all scalars 𝛼
and 𝛽and for all vectors u and v.
9.4.5.2
Linear Independence and
Dimension
A linear combination is an expression
of the form 𝛼1v1 + · · · + 𝛼kvk, where, for
each i = 1, … , k, 𝛼i is a scalar and vi is a
vector. The vectors v1, … , vk are linearly
independent if the only vanishing linear
combination is the trivial one, namely, if
𝛼1v1 + · · · + 𝛼kvk = 0
implies
necessar-
ily 𝛼1 = · · · = 𝛼k = 0. A vector space is
m-dimensional if there exists a maximal
linearly independent set e1, … , em. Such a
set, if it exists, is called a basis of the vector
space. All bases have the same number
of elements, namely, m. If no maximal
linearly independent set exists, the vector
space is inﬁnite dimensional. Given a basis
e1, … , em of a ﬁnite-dimensional vector
space, every vector v can be represented
uniquely in terms of components vi as
v = v1e1 + · · · + vmem =
m
∑
i=1
viei.
(9.7)

326
9 Diﬀerentiable Manifolds
In many contexts, it is convenient to
use
Einstein’s
summation
convention,
whereby the summation symbol is under-
stood whenever a monomial contains a
once-diagonally repeated index. Thus, we
write
v = viei,
(9.8)
where the summation in the range 1 to m is
understood to take place.
9.4.5.3
The Dual Space
A linear function on a vector space V is a
map 𝜔∶V →ℝsuch that
𝜔(𝛼u + 𝛽v) = 𝛼𝜔(u) + 𝛽𝜔(v),
(9.9)
for arbitrary scalars (𝛼, 𝛽) and vectors (u
and v). From the physical viewpoint, a
linear function expresses the principle of
superposition. A trivial example of a linear
function is the zero function, assigning to
all vectors in V the number 0.
If we consider the collection V ∗of all lin-
ear functions on a given vector space V, it
is possible to introduce in it operations of
addition and of multiplication by a scalar,
as follows. The addition of two linear func-
tions 𝜔and 𝜎is deﬁned as the linear func-
tion 𝜔+ 𝜎that assigns to each vector v the
sum 𝜔(v) + 𝜎(v). Similarly, for a scalar 𝛼
and a linear function 𝜔, we deﬁne the lin-
ear function 𝛼𝜔by (𝛼𝜔)(v) = 𝛼(𝜔(v)). With
these two operations, it is not diﬃcult to
show that V ∗acquires the structure of a
vector space, called the dual space of V. Its
elements are called covectors. The action of
a covector 𝜔on a vector v is indicated as
𝜔(v) or, equivalently, as ⟨𝜔, v⟩.
Given a basis e1, … , em of V, we deﬁne
the m linear functions ei by the formula
ei(v) = vi,
i = 1, … , m.
(9.10)
In other words, ei is the linear function (i.e.,
the covector) that assigns to a vector v ∈V
its ith component in the given basis. It is
not diﬃcult to show that these covectors
form a basis of the dual space V ∗, which
is, therefore, of the same dimension as the
original space V. By construction, a basis
and its dual satisfy the identity
ei(ej) = 𝛿i
j,
(9.11)
where 𝛿i
j is the Kronecker symbol (equal to
1 if i = j and to zero otherwise). Any cov-
ector 𝜔can be expressed uniquely in terms
of components in a dual basis, namely, 𝜔=
𝜔iei, where the summation convention is
used.
9.4.6
Tangent and Cotangent Spaces
The collection Tpof all the tangent vec-
tors at p ∈is called the tangent space
to the manifold at p. It is not diﬃcult to
show that tangent vectors at a point p sat-
isfy all the conditions of a vector space if we
deﬁne their addition and multiplication by
a scalar in the obvious way (for example, by
using chart components). In other words,
Tpis a vector space. To ﬁnd its dimen-
sion, we choose a local chart (, 𝜙) with
coordinates x1, … , xm, such that the point p
is mapped to the origin of ℝm. The inverse
map 𝜙−1, when restricted to the natural
coordinate lines of ℝm, delivers m curves
at p. Each of these curves, called a coor-
dinate line in , deﬁnes a tangent vector,
which we suggestively denote by (𝜕∕𝜕xi)p.
It can be shown that these vectors consti-
tute a basis of Tp, called the natural basis
associated with the given coordinate system.
The dimension of the tangent space at each
point of a manifold is, therefore, equal to
the dimension of the manifold itself. The
cotangent space at p, denoted by T∗
p, is
deﬁned as the dual space of Tp.

9.4 Diﬀerentiable Manifolds
327
9.4.7
The Tangent and Cotangent Bundles
If we attach to each point p of an m-
dimensional manifold its tangent space
Tp, we obtain, intuitively speaking, a 2m-
dimensional entity, which we denote by
T, called the tangent bundle of . A
crude visualization of this entity can be
gathered when is a 2-sphere, such as a
globe, at each point of which we have stuck
a postal stamp or a paper sticker. The tan-
gent bundle is not the globe itself but rather
the collection of the stickers. This collection
of tangent spaces, however, has the prop-
erty that it projects on the original mani-
fold. In our example, each sticker indicates
the point at which it has been attached. In
other words, the set Tis endowed, by
construction, with a projection map 𝜏onto
the base manifold . More explicitly, a typ-
ical point of Tconsists of a pair (p, vp),
where p ∈and vp ∈Tp. The projec-
tion map
𝜏∶T→
(9.12)
is given by the assignation
𝜏(p, vp) = p.
(9.13)
To see that the set Tcan be regarded as
a manifold, we construct explicitly an atlas
out of any given atlas of the base manifold.
Let (, 𝜙) be a chart in with coordinates
xi, … , xm. Adopting, as we may, the natu-
ral basis (𝜕∕𝜕xi)p of Tpat each point p ∈
, we can identify each vector vp with its
components vi
p. Put diﬀerently, we assign to
each point (p, vp) ∈𝜏−1() ⊂Tthe 2m
numbers (x1, … , xm, v1, … , vm), namely, a
point in ℝ2m. We have thus obtained a coor-
dinate chart on 𝜏−1(). It is now a formality
to extend this construction to a whole atlas
of Tand to show that Tis a diﬀer-
entiable manifold of dimension 2m. In the
terminology of general ﬁber bundles, the set
Tp= 𝜏−1(p) is called the ﬁber at p ∈.
As each ﬁber is an m-dimensional vector
space, we say that the typical ﬁber of T
is ℝm.
Upon a coordinate transformation repre-
sented by (9.2), the components ̂vi of a vec-
tor v at p in the new natural basis (𝜕∕𝜕yi)p
are related to the old components vi in the
basis (𝜕∕𝜕xi)p by the formula
̂vi =
(𝜕yi
𝜕xj
)
p
vj,
(9.14)
while the base vectors themselves are
related by the formula
(
𝜕
𝜕yi
)
p
=
(
𝜕xj
𝜕yi
)
p
( 𝜕
𝜕xj
)
p .
(9.15)
Comparing these two formulas, we con-
clude that the components of vectors
behave
contravariantly.
In
traditional
treatments, it was customary to deﬁne
tangent
vectors
as
indexed
quantities
that
transform
contravariantly
under
coordinate changes.
A similar construction can be carried out
by attaching to each point of a manifold 
its cotangent space T∗
pto obtain the set
T∗, called the cotangent bundle of . A
typical point of T∗is a pair (p, 𝜔p), where
p ∈and 𝜔p ∈T∗
p. The projection map
𝜋∶T∗→is given by
𝜋(p, 𝜔p) = p.
(9.16)
Given a chart, the local dual basis to the nat-
ural basis (𝜕∕𝜕xi)p is denoted by (dxi)p, with
i = 1, … , m. The covector 𝜔p ∈T∗
pcan
be uniquely expressed as 𝜔p = 𝜔idxi, where
the subscript p has been eliminated for
clarity. Given a point (p, 𝜔p) ∈𝜋−1() ⊂
T∗, we assign to it the 2m numbers
(x1, … , xm, 𝜔1, … , 𝜔m). In this way, it can

328
9 Diﬀerentiable Manifolds
be rigorously shown that T∗is a mani-
fold of dimension 2m.
Upon a coordinate transformation, the
components ̂𝜔i of a covector 𝜔at p trans-
form according to
̂𝜔i =
(
𝜕xj
𝜕yi
)
p
𝜔j,
(9.17)
while the dual base vectors themselves are
related by the formula
dyi =
(𝜕yi
𝜕xj
)
p
dxj.
(9.18)
The
components
of
covectors
behave
covariantly.
9.4.8
A Physical Interpretation
In the context of the application presented
in Section 9.3.5, what do the tangent and
cotangent bundles represent? If the mani-
fold is the conﬁguration space of a system
with m degrees of freedom, then a curve
𝛾∶(a, b) →represents a possible trajec-
tory of the system as it evolves in the time
interval a < t < b. At a point q ∈, there-
fore, a “small” piece of a curve conveys the
notion of a virtual displacement 𝛿q, that
is, a small displacement compatible with
the degrees of freedom of the system. In
the limit, we obtain a tangent vector at q,
representing a velocity. We conclude that
the tangent space Tqis the repository of
all possible velocities (or virtual displace-
ments) of the system at the conﬁguration
q. The tangent bundle T, accordingly, is
the collection of all possible velocities of
the system at all possible conﬁgurations. An
element of Tconsists of an ordered pair
made up of a conﬁguration and a velocity
at this conﬁguration. The projection map 𝜏
assigns to this pair the conﬁguration itself.
In Lagrangian Mechanics, the fundamen-
tal geometric arena is precisely the tangent
bundle T. Indeed, the Lagrangian den-
sity of a mechanical system is given by
a function ∶T→ℝ, assigning to each
conﬁguration and each velocity (at this con-
ﬁguration) a real number.
A covector Q at q is a linear function
that assigns to each tangent vector (vir-
tual displacement 𝛿q) at q a real number
𝛿W = ⟨Q, 𝛿q⟩, whose meaning is the vir-
tual work of the generalized force Q on the
virtual displacement 𝛿q (or the power of
the generalized force on the correspond-
ing velocity). The terminology and the nota-
tion are due to Lagrange. The interesting
feature of the geometric approach is that,
once the basic geometric entity has been
physically identiﬁed as a manifold, its tan-
gent and cotangent bundles are automat-
ically the carriers of physical meaning. In
Hamiltonian Mechanics, covectors at q ∈
can be regarded as generalized momenta
of the system. Thus, the cotangent bundle
T∗is identiﬁed with the phase space of the
system, namely, the repository of all conﬁg-
urations and momenta. The Hamiltonian
function of a mechanical system is a func-
tion ∶T∗→ℝ.
9.4.9
The Diﬀerential of a Map
Given a diﬀerentiable map
g ∶→
(9.19)
between two manifolds, and , of
dimensions m and n, respectively, we focus
attention on a particular point p ∈and
its image q = g(p) ∈. Let vp ∈Tpbe
a tangent vector at p and let 𝛾∶H →
be one of its representative curves. The
composite map

9.4 Diﬀerentiable Manifolds
329
g ∘𝛾∶H −→
(9.20)
is then a smooth curve in passing
through q. This curve (the image of 𝛾by g)
is, therefore, the representative of a tangent
vector at q, which we will denote (g∗)p(vp).
The vector (g∗)p(vp) is independent of the
representative curve 𝛾chosen for vp. More-
over, (g∗)p is a linear map on vectors at p.
The map (g∗)p just deﬁned is called the
diﬀerential of g at p. It is a linear map
between the tangent spaces Tpand
Tg(p). As this construction can be car-
ried out at each and every point of ,
we obtain a map g∗between the tangent
bundles, namely,
g∗∶T→T,
(9.21)
called the diﬀerential of g. Alternative nota-
tions for this map are Dg and Tg, and it
is also known as the tangent map. One
should note that the map g∗includes the
map g between the base manifolds, because
it maps vectors at a point p linearly into vec-
tors at the image point q = g(p), and not
just to any vector in T. It is, therefore, a
ﬁber-preserving map. This fact is best illus-
trated in the following commutative dia-
gram:
TM
M
g
g∗
τN
τM
N
TN
(9.22)
where 𝜏and 𝜏are the projection maps
of Tand T, respectively. The diﬀeren-
tial is said to push forward tangent vectors
at p to tangent vectors at the image point
g(p).
In the particular case of a function f ∶
→ℝ, the diﬀerential f∗can be inter-
preted somewhat diﬀerently. Indeed, the
tangent space Trℝcan be trivially identi-
ﬁed with ℝitself, so that f∗can be seen as a
real-valued function on T. This function
is denoted by df ∶T→ℝ. The diﬀeren-
tial of a function satisﬁes the identity
df (v) = v(f ).
(9.23)
In local systems of coordinates xi (i =
1, … , m) and y𝛼(𝛼= 1, … , n) around p
and g(p), respectively, the diﬀerential of g
at p maps the vector with components vi
into the vector with components
[(g∗)p(vp)]𝛼=
(𝜕g𝛼
𝜕xi
)
p
vi,
(9.24)
where g𝛼= g𝛼(x1, … , xn) is the coordinate
representation of g in the given charts. The
(m × n)-matrix with entries {(𝜕g𝛼∕𝜕xi)
p}
is the Jacobian matrix at p of the map g in
the chosen coordinate systems. The rank of
the Jacobian matrix is independent of the
coordinates used. It is called the rank of g
at p.
Let f ∶→ℝbe a diﬀerentiable func-
tion and let g ∶→be a diﬀerentiable
map between manifolds. Then,
((g∗)pvp)(f ) = vp(f ∘g),
p ∈.
(9.25)
The diﬀerential of a composition of maps
is equal to the composition of the diﬀer-
entials. More precisely, if g ∶→and
h ∶→are diﬀerentiable maps, then
((h ∘g)∗)p(vp) = (h∗)g(p)((g∗)p(vp)).
(9.26)
In coordinates, this formula amounts to the
multiplication of the Jacobian matrices.

330
9 Diﬀerentiable Manifolds
9.5
Vector Fields and the Lie Bracket
9.5.1
Vector Fields
A vector ﬁeld V on a manifold is an
assignment of a tangent vector Vp = V(p) ∈
Tpto each point p ∈. We restrict
our attention to smooth vector ﬁelds, whose
components are smooth functions in any
given chart. A vector ﬁeld is, therefore, a
smooth map
V ∶→T,
(9.27)
satisfying the condition
𝜏∘V = id,
(9.28)
where idis the identity map of . The
meaning of this last condition is that the
vector assigned to the point p is a tan-
gent vector at p, rather than at any other
point.
A geometrically convenient way to look
at a vector ﬁeld is to regard it as a cross
section of the tangent bundle. This termi-
nology arises from the pictorial representa-
tion depicted in Figure 9.1, where the base
manifold is represented by a shallow arc
and the ﬁbers (namely, the tangent spaces)
by straight lines hovering above it. Then,
a cross section looks like a curve cutting
through the ﬁbers.
9.5.2
The Lie Bracket
If V is a (smooth) vector ﬁeld on a manifold
and f ∶→ℝis a smooth function,
then the map
Vf ∶→ℝ,
(9.29)
deﬁned as
p →Vp(f )
(9.30)
is again a smooth map. It assigns to each
point p ∈the directional derivative of
the function f in the direction of the vec-
tor ﬁeld at p. In other words, a vector ﬁeld
assigns to each smooth function another
smooth function. Given, then, two vector
ﬁelds V and W over , the iterated eval-
uation
h = W(Vf ) ∶→ℝ,
(9.31)
gives rise to a legitimate smooth function h
on .
On the basis of the above considerations,
one may be tempted to deﬁne a composi-
tion of vector ﬁelds by declaring that the
composition W ∘U is the vector ﬁeld that
assigns to each function f the function h
deﬁned by (9.31). This wishful thinking,
however, does not work. To see why, it is
convenient to work in components in some
chart with coordinates xi. Let
V = V i 𝜕
𝜕xi
W = W i 𝜕
𝜕xi ,
(9.32)
TpM
V(M)
Vp = V(p)
τ
p
M
Figure 9.1
A vector ﬁeld as a cross section of the
tangent bundle.

9.5 Vector Fields and the Lie Bracket
331
where the components V i and W i (i =
1, … , m) are smooth real-valued functions
deﬁned over the m-dimensional domain
of the chart. Given a smooth function f ∶
→ℝ, the function g = Vf is evaluated
at a point p ∈with coordinates xi (i =
1, … , m) as
g(p) = V i 𝜕f
𝜕xi .
(9.33)
Notice the slight abuse of notation we incur
into by identifying the function f with its
representation in the coordinate system.
We now apply the same prescription to
calculate the function h = Wg and obtain
h(p) = W i 𝜕g
𝜕xi = W i 𝜕
(
V j
𝜕f
𝜕xj
)
𝜕xi
=
(
W i 𝜕V j
𝜕xi
) 𝜕f
𝜕xj + W iV j 𝜕2f
𝜕xi𝜕xj .
(9.34)
The last term of this expression, by involv-
ing second derivatives, will certainly not
transform as the components of a vector
should under a change of coordinates. Nei-
ther will the ﬁrst. This negative result, on
the other hand, suggests that the oﬀend-
ing terms could perhaps be eliminated by
subtracting from the composition WV the
opposite composition VW, namely,
(WV −VW) (f )
=
(
W i 𝜕V j
𝜕xi −V i 𝜕W j
𝜕xi
) 𝜕f
𝜕xj .
(9.35)
The vector ﬁeld thus obtained is called the
Lie bracket of W and V (in that order) and
is denoted by [W, V]. More explicitly, its
components in the coordinate system xi are
given by
[W, V]j = W i 𝜕V j
𝜕xi −V i 𝜕W j
𝜕xi .
(9.36)
Upon a coordinate transformation, these
components transform according to the
rules of transformation of a vector.
The following properties of the Lie
bracket are worthy of notice:
1. Skew symmetry:
[W, V] = −[V, W].
(9.37)
2. Jacobi identity:
[[W, V], U] + [[V, U], W]
+[[U, W], V] = 0.
(9.38)
The collection of all vector ﬁelds over a
manifold has the natural structure of an
inﬁnite-dimensional vector space, where
addition and multiplication by a scalar are
deﬁned in the obvious way. In this vector
space, the Lie bracket operation is bilinear.
A vector space endowed with a bilinear
operation satisfying conditions (1) and (2)
is called a Lie algebra.
Vector ﬁelds can be multiplied by func-
tions to produce new vector ﬁelds. Indeed,
for a given function f and a given vector
ﬁeld V, we can deﬁne the vector ﬁeld f V by
(f V)p = f (p)Vp.
(9.39)
It can be shown that
[gW, f V] = gf [W, V]
+ g (Wf ) V −f (Vg) W,
(9.40)
where g, f are smooth functions and W, V
are vector ﬁelds over a manifold .
9.5.3
A Physical Interpretation: Continuous
Dislocations
Let an atomic lattice be given by, say, all
points with integer coordinates in ℝ2. To

332
9 Diﬀerentiable Manifolds
(a)
(b)
Figure 9.2
Dislocation in a crystal
lattice. (a) Perfect lattice and (b)
dislocated lattice.
each atom we can associate two vectors (in
this instance, unit and orthogonal) deter-
mined by joining it to its immediate neigh-
bors to the right and above, respectively.
If the lattice is deformed regularly, these
vectors will deform accordingly, changing
in length and angle, but always remaining
linearly independent at each atom. In the
(not precisely deﬁned) continuous limit, we
can imagine that each point of ℝ2 has been
endowed with a basis or frame, the collec-
tion of which is called a moving frame (or
repère mobile).6)
Returning to the discrete picture, if there
is a dislocation (for example, a half-line of
atoms is missing, as shown on the right-
hand side of Figure 9.2), the local bases
will be altered diﬀerently from the case of
a mere deformation. The engineering way
to recognize this is the so-called Burgers’
circuit, which consists of a four-sided path
made of the same number of atomic spac-
ings in each direction. The failure of such
a path to close is interpreted as the pres-
ence of a local dislocation in the lattice. We
want to show that in the putative continu-
ous limit, this failure is represented by the
non-vanishing of a Lie bracket. What we
have in the continuous case as the only rem-
nant of the discrete picture is a smoothly
distributed collection of bases, which we
have called a moving frame, and which can
6) This idea was introduced mathematically by Car-
tan and, in a physical context, by the brothers
Cosserat.
be seen as two vector ﬁelds E𝛼(𝛼= 1, 2)
over ℝ2.
From the theory of ordinary diﬀerential
equations, we know that each vector ﬁeld
gives rise, at least locally, to a well-deﬁned
family of parameterized integral curves,
where the parameter is determined up to
an additive constant. More speciﬁcally,
these curves are obtained as the solutions
r = r(s𝛼) of the systems of equations
dr(s𝛼)
ds𝛼
= E𝛼[r(s𝛼)],
(𝛼= 1, 2;
no sum on 𝛼),
(9.41)
where r represents the natural position vec-
tor in ℝ2. The parameter s𝛼(one for each of
the two families of curves) can be pinned
down in the following way. Select a point
p0 as origin and draw the (unique) inte-
gral curve 𝛾1 of the ﬁrst family passing
through this origin. Adopting the value s1 =
0 for the parameter at the origin, the value
of s1 becomes uniquely deﬁned for all the
remaining points of the curve. Each of the
curves of the second family must intersect
this curve of the ﬁrst family. We adopt,
therefore, for each of the curves of the sec-
ond family the value s2 = 0 at the corre-
sponding point of intersection with that
reference curve (of the ﬁrst family). In this
way, we obtain (at least locally) a new coor-
dinate system s1, s2 in ℝ2. By construc-
tion, the second natural base vector of this
coordinate system is E2. But there is no

9.5 Vector Fields and the Lie Bracket
333
guarantee that the ﬁrst natural base vector
will coincide with E1, except at the curve
𝛾1 through the adopted origin. In fact, if we
repeat the previous construction in reverse,
that is, with the same origin but adopting
the curve 𝛾2 of the second family as a refer-
ence, we obtain, in general, a diﬀerent sys-
tem of coordinates, which is well adapted
to the basis vectors E1, but not necessarily
to E2 (Figure 9.3).
Assume now that, starting at the adopted
origin, we move an amount of Δs1 along
𝛾1 to arrive at a point p′ and thereafter we
climb an amount of Δs2 along the encoun-
tered curve of the second family through
p′. We arrive at some point p1. Incidentally,
this is the point with coordinates (Δs1, Δs2)
in the coordinate system obtained by the
ﬁrst construction. If, however, starting at
the same origin we move by Δs2 along the
curve 𝛾2 to a point ̂p and then move by
Δs1 along the encountered curve of the
ﬁrst family, we will arrive at a point p2
(whose coordinates are (Δs1, Δs2) in the sec-
ond construction) which is, in general, dif-
ferent from p1. Thus, we have detected the
failure of a four-sided circuit to close! The
discrete picture has, therefore, its contin-
uous counterpart in the noncommutativ-
ity of the ﬂows along the two families of
curves.
Let us calculate a ﬁrst-order approxima-
tion to the diﬀerence between p2 and p1.
For this purpose, let us evaluate, to the ﬁrst
order, the base vector E2 at the auxiliary
point p′. The result is
E′
2 = E2(p0) + 𝜕E2
𝜕xi
dxi
ds1 Δs1,
(9.42)
where derivatives are calculated at p0. The
position vector of p1, always to ﬁrst-order
approximation, is obtained, therefore, as
r1 = Δs1E1(p0)
+ Δs2
(
E2(p0) + 𝜕E2
𝜕xi
dxi
ds1 Δs1,
)
.(9.43)
In a completely analogous manner, we cal-
culate the position vector of p2 as
r2 = Δs2E2(p0)
+ Δs1
(
E1(p0) + 𝜕E1
𝜕xi
dxi
ds2 Δs2,
)
.(9.44)
By virtue of (9.41), however, we have
dxi
ds𝛼= Ei
𝛼,
(9.45)
where Ei
𝛼is the ith component in the natu-
ral basis of ℝ2 of the base vector E𝛼. From
the previous three equations, we obtain
r2 −r1 =
(𝜕E1
𝜕xi Ei
2 −𝜕E2
𝜕xi Ei
1
)
Δs1Δs2
= [E1, E2] Δs1Δs2.
(9.46)
𝛾1(s1)
𝛾2(s2)
E1
E2
Δs1
Δs2
Δs1
Δs2
p2
p1
p0
p
p′
E′2
Figure 9.3
The continuous case.

334
9 Diﬀerentiable Manifolds
We thus conﬁrm that the closure of the
inﬁnitesimal circuits generated by two vec-
tors ﬁelds is tantamount to the vanishing of
their Lie bracket. This vanishing, in turn,
is equivalent to the commutativity of the
ﬂows generated by these vector ﬁelds. For
this reason, the Lie bracket is also called
the commutator of the two vector ﬁelds. In
physical terms, we may say that the vanish-
ing of the Lie brackets between the vector
ﬁelds representing the limit of a lattice is an
indication of the absence of dislocations.
As in this example we have introduced
the notion of a moving frame, that is, a
smooth ﬁeld of bases Ei (i = 1, … , n) over
an n-dimensional manifold, it makes sense
to compute all the possible Lie brackets
between the base vectors and to express
them in terms of components in the local
basis. As a Lie bracket of two vector ﬁelds is
itself a vector ﬁeld, there must exist unique
scalar ﬁelds ck
ij such that
[Ei, Ej] = ck
ijEk
(i, j, k = 1, … , n).
(9.47)
These scalars are known as the structure
constants of the moving frame. The struc-
ture constants vanish identically if, and only
if, the frames can be seen locally as the nat-
ural base vectors of a coordinate system.
9.5.4
Pushforwards
We have seen that the diﬀerential of a map
between manifolds carries tangent vectors
to tangent vectors. This operation is some-
times called a pushforward. Does a map also
pushforward vector ﬁelds to vector ﬁelds?
Let V ∶→Tbe a vector ﬁeld on 
and let g ∶→be a smooth map. As
the diﬀerential of g is a map of the form g∗∶
T→T, the composition g∗∘V makes
perfect sense, but it delivers a (well-deﬁned)
map g∗V from (and not from ) into
T. This is not a vector ﬁeld, nor can it
in general be turned into one. If the dimen-
sion of is larger than that of , points
in will end up being assigned more than
one vector. If the dimension of the source
manifold is less than that of the target, on
the other hand, even if the function is one-
to-one, there will necessarily exist points in
to which no vector is assigned. The only
case in which the pushforward of a vector
ﬁeld can be regarded as a vector ﬁeld on the
target manifold is the case in which both
manifolds are of the same dimension and
the map is a diﬀeomorphism.
Notwithstanding the above remark, let
g ∶→be a smooth map. We say that
the vector ﬁelds V ∶→Tand W ∶
→Tare g-related if
g∗V(p) = W(g(p))
∀p ∈.
(9.48)
According to this deﬁnition, if g hap-
pens to be a diﬀeomorphism, then V
and g∗V are automatically g-related. The
pushed-forward vector ﬁeld is then given
by g∗∘V ∘g−1.
Theorem 9.1 Let V1 be g-related to W1
and let V2 be g-related to W2. Then the
Lie bracket [V1, V2] is g-related to the Lie
bracket [W1, W2], that is,
[g∗V1, g∗V2] = g∗[V1, V2].
(9.49)
9.6
Review of Tensor Algebra
9.6.1
Linear Operators and the Tensor Product
A linear operator T between two vector
spaces U and V is a linear map T ∶U →
V that respects the vector-space structure.

9.6 Review of Tensor Algebra
335
More precisely,
T(𝛼u1 + 𝛽u2) = 𝛼T(u1) + 𝛽T(u2),
∀𝛼, 𝛽∈ℝ, u1, u2 ∈U,
(9.50)
where the operations are understood in
the corresponding vector spaces. When the
source and target vector spaces coincide,
the linear operator is called a tensor. Occa-
sionally, the terminology of two-point ten-
sor (or just tensor) is also used for the
general case, particularly when the dimen-
sion of both spaces is the same. We will
use these terms (linear operator, linear map,
tensor, and so on) liberally.
Consider the collection L(U, V) of all
linear operators between two given vector
spaces, and endow it with the natural struc-
ture of a vector space. To do so, we deﬁne
the sum of two linear operators S and T as
the linear operator S + T whose action on
an arbitrary vector u ∈U is given by
(S + T)(u) = S(u) + T(u).
(9.51)
Similarly, we deﬁne the product of a scalar 𝛼
by a linear operator T as the linear operator
𝛼T given by
(𝛼T)(u) = 𝛼T(u).
(9.52)
It is a straightforward matter to verify that
the set L(U, V), with these two operations,
is a vector space. In the case of the dual
space V ∗(which can be identiﬁed with
L(V, ℝ)), we were immediately able to
ascertain that it was never empty, because
the zero map is linear. The same is true
for L(U, V), whose zero element is the
linear map 0 ∶U →V assigning to each
vector of U the zero vector of V. Inspired
by the example of the dual space, we will
attempt now to construct a basis of L(U, V)
starting from given bases at U and V. This
point takes some more work, but the result
is, both conceptually and notationally,
extremely creative.
Let 𝜔∈U∗and v ∈V be, respectively a
covector of the source space and a vector
of the target space of a linear operator T ∶
U →V. We deﬁne the tensor product of
v with 𝜔as the linear operator v ⊗𝜔∈
L(U, V) obtained as follows:
(v ⊗𝜔) (u) = ⟨𝜔, u⟩v,
∀u ∈U. (9.53)
We emphasize that the tensor product is
fundamentally noncommutative. We note,
on the other hand, that the tensor product
is a bilinear operation, namely, it is linear in
each of the factors, namely,
(𝛼u1 + 𝛽u2) ⊗𝜔= 𝛼(u1 ⊗𝜔) + 𝛽(u2 ⊗𝜔)
(9.54)
and
u ⊗(𝛼𝜔1 + 𝛽𝜔2) = 𝛼(u ⊗𝜔1) + 𝛽(u ⊗𝜔2)
(9.55)
for all 𝛼, 𝛽∈ℝ.
One of the reasons for the conceptual
novelty of the tensor product is that it does
not seem to have an immediately intuitive
interpretation. In fact, it is a very singular
linear operator, because, ﬁxing the ﬁrst fac-
tor, it squeezes the whole vector space U∗
into an image consisting of a single line of
V (the line of action of v).
Let
the
dimensions
of
U
and
V
be,
respectively,
m
and
n,
and
let
{e𝛼} (𝛼= 1, … , m) and {fi} (i = 1, … , n)
be respective bases. It makes sense to
consider
the
m × n
tensor
products
fi ⊗e𝛼∈L(U, V). We want to show that
these linear operators (considered as vec-
tors belonging to the vector space L(U, V))
are in fact linearly independent. Assume
that a vanishing linear combination has
been found, namely, 𝜌i
𝛼fi ⊗e𝛼= 0, where
𝜌i
𝛼∈ℝand where the summation conven-
tion is appropriately used (Greek indices

336
9 Diﬀerentiable Manifolds
ranging from 1 to m, and Latin indices
ranging from 1 to n). Applying this linear
combination to the base vector e𝛽∈V, we
obtain 𝜌i
𝛽fi = 0, whence 𝜌i
𝛽= 0, proving
that the only vanishing linear combination
is the trivial one.
Let T ∈L(U, V) be an arbitrary linear
operator. By linearity, we may write
T(u) = T(u𝛼e𝛼) = u𝛼T(e𝛼).
(9.56)
Each T(e𝛼), being an element of V, can be
written as a unique linear combination of
the basis, namely,
T(e𝛼) = Ti
𝛼fi,
Ti
𝛼∈ℝ.
(9.57)
We form now the linear operator Ti
𝛼fi ⊗
e𝛼and apply it to the vector u, which yields
Ti
𝛼fi ⊗e𝛼(u) = u𝛼Ti
𝛼fi.
(9.58)
Comparing this result with (9.56, 9.57) we
conclude that the original operator T and
the operator Ti
𝛼fi ⊗e𝛼produce the same
result when operating on an arbitrary vec-
tor u ∈U. They are, therefore, identical and
we can write
T = Ti
𝛼fi ⊗e𝛼.
(9.59)
In other words, every linear operator in
L(U, V) can be written as a linear combi-
nation of the m × n linearly independent
operators
fi ⊗e𝛼,
showing
that
these
operators form a basis of L(U, V), whose
dimension is, therefore, the product of
the dimensions of U and V. For these
reasons, the vector space L(U, V) is also
called the tensor-product space of V and
U∗, and is denoted as V ⊗U∗. The unique
coeﬃcients Ti
𝛼are called the components
of the tensor T in the corresponding basis.
The composition of linear operators is a
particular case of the composition of func-
tions. Let T ∶U →V and S ∶V →W be
linear operators between the vector spaces
U, V, and W. The composition S ∘T ∶U →
W is usually denoted as ST and is called the
product of the operators. Choosing bases in
the vector spaces U, V, and W and express-
ing the operators T ∶U →V and S ∶V →
W in components, the composition S ∘T
is represented in components by the prod-
uct [S][T] of the matrices of components
of S and T. This is the best justiﬁcation of
the, at ﬁrst sight odd, rule for multiplying
matrices.
We have spoken about the conceptual
novelty of the tensor product. No less
important is its notational convenience. In
the case of a vector, say v ∈V, it is obvi-
ously advantageous to be able to express
the master concept of a vector in terms of
the subsidiary notion of its components in a
particular basis by simply writing: v = vifi.
If we change the basis, for instance, the fact
that what we have just written is an invari-
ant expression, with a meaning beyond the
particular basis chosen, can be exploited,
as we have already done. In the case of
linear operators, we have now obtained,
according to (9.59), a similar way to express
the “real thing” invariantly in terms of its
components on a basis arising from having
chosen arbitrary bases in both the source
and the target spaces. We can now show a
tensor itself, much in the same way as we
are able to show a vector itself. So powerful
is this idea that, historically, the notation
was invented before the concept of tensor
product had been rigorously deﬁned. In old
Physics texts it was called the dyadic nota-
tion, and it consisted of simply apposing
the elements of the bases involved (usually
in the same (Cartesian) vector space: ii, ij,
etc.). It is also interesting to recall that
in Quantum Mechanics the prevailing
notation for covectors and tensors is the
ingenious device introduced by Dirac in
terms of “bras” and “kets.”

9.6 Review of Tensor Algebra
337
9.6.2
Symmetry and Skew Symmetry
Let T ∶U →V be a linear map. We deﬁne
the transpose of T as the map TT ∶V ∗→
U∗obtained by the prescription
⟨TT(𝜔), u⟩= ⟨𝜔, T(u)⟩.
(9.60)
If {e𝛼} (𝛼= 1, … , m) and {fi} (i = 1, … , n)
are, respectively, bases of U and V, whereby
T is expressed as
T = Ti
𝛼fi ⊗e𝛼,
(9.61)
then the transpose of T is expressed as
TT = Ti
𝛼e𝛼⊗fi.
(9.62)
In other words, the transpose of a ten-
sor is obtained by leaving the components
unchanged and switching around the base
vectors. On the other hand, we may want
to express the transpose in its own right by
the standard formula
TT = (TT)
i
𝛼e𝛼⊗fi,
(9.63)
applicable to the components of a tensor
in terms of a basis. Comparing the last two
equations, we conclude that
(TT)
i
𝛼= Ti
𝛼.
(9.64)
Notice the precise order of the indices in
each case.
A linear operator T is said to be sym-
metric if T = TT and skew- (or anti-) sym-
metric if T = −TT . Recall, however, that
in general T and TT operate between dif-
ferent spaces. This means that the notion
of symmetry should be reserved to the very
special case in which the target space is pre-
cisely the dual of the source space, namely,
when the linear operator belongs to some
L(U, U∗) or, equivalently to U∗⊗U∗. We
conclude that, as expected from the very
tensor-product notation, a linear operator
and its transpose are of the same nature
(and may, therefore, be checked for sym-
metry) if, and only if, they belong to a ten-
sor product of the form V ⊗V. Having
said this, it is clear that if some artiﬁcial
(noncanonical) isomorphism is introduced
between a space and its dual (by means of
an inner product, for example, as we shall
eventually do), then the concept of symme-
try can be extended.
9.6.3
The Algebra of Tensors on a Vector Space
Although a more general situation may be
envisioned, we now consider the collection
of all possible tensor products involving any
ﬁnite number of factors, each factor being
equal to a given vector space V or its dual
V ∗. The order of the factors, of course, mat-
ters, but it is customary to say that a tensor
product is of type (r,s) if it is obtained by
multiplying r copies of V and s copies of
V ∗, regardless of the order in which these
copies appear in the product. An element of
such tensor product is also called a tensor of
type (r, s). Another common terminology is
to refer to r and s, respectively, as the con-
travariant and covariant degrees of the ten-
sor. Thus, a vector is a tensor of type (1, 0),
while a covector is of type (0, 1). By conven-
tion, a tensor of type (0, 0) is identiﬁed with
a scalar. As the ﬁeld of scalars ℝhas the
natural structure of a vector space (whose
elements are tensors of type (0, 0)), it makes
sense to take its tensor product with a vec-
tor space. Note that ℝ⊗V = V.
The tensor product of a tensor of type
(r1, s1) with a tensor of type (r2, s2) is a
tensor of type (r1 + r2, s1 + s2). A map from
a Cartesian product of vector spaces into
a vector space is said to be multilinear

338
9 Diﬀerentiable Manifolds
if it is linear in each of the arguments.
A tensor T of type (r, s) can be con-
sidered as a multilinear map such that
T(𝜔1, … , 𝜔r, v1, … , vs) ∈ℝ, where vi and
𝜔j belong, respectively, to V and V ∗, for
each i = 1, … , r and each j = 1, … , s.
The collection of all tensors of all orders
deﬁned on a vector space V can be given
the formal structure of an algebra (with the
operations of direct sum and tensor prod-
uct) known as the algebra of tensors on
V. Considering only tensors of covariant
degree zero, namely, tensors of type (r, 0),
we obtain the contravariant tensor alge-
bra of V. When written in components, all
indices of tensors in this algebra are super-
scripts.
In a similar way, one can deﬁne the
covariant tensor algebra by considering
tensors of type (0, s). On the other hand,
considering V ∗as a vector space in its
own right, we could form its contravariant
tensor algebra, and these two objects turn
out to be the same. The contravariant and
covariant algebras can be considered dual
to each other in the sense that there exists
a canonical way to evaluate an element of
one over an element of the other to pro-
duce a real number linearly. Considering a
tensor T of type (k, 0) and a tensor S of type
(0, k) and using a basis in V, this evaluation
reads
⟨S, T⟩= Si1···ik Ti1···ik.
(9.65)
If the tensors are of diﬀerent orders (that is,
a tensor of type (r, 0) and a tensor of type
(0, s) with s ≠r), we deﬁne the evaluation
as zero.
A tensor T of type (r, 0) can be seen as a
multilinear map
T ∶V ∗, … , V ∗−→ℝ
(𝜔1, … , 𝜔r) →T(𝜔1, … , 𝜔r),
𝜔1, … , 𝜔r ∈V ∗. (9.66)
For tensors in the contravariant or covari-
ant algebras it makes sense to speak about
symmetry and skew symmetry.
A tensor of type (r, 0) is said to be (com-
pletely) symmetric if the result of the opera-
tion (9.66) is independent of the order of the
arguments. Put in other words, exchanging
any two arguments with each other pro-
duces no eﬀect in the result of the multilin-
ear operator T. A similar criterion applies
for completely symmetric tensors of order
(0, s), except that the arguments are vectors
rather than covectors. Choosing a basis in
V, symmetry boils down to indiﬀerence to
index swapping.
Analogously, a tensor of type (r, 0)
is (completely) skew symmetric if every
mutual exchange of two arguments alters
the sign of the result, leaving the absolute
value unchanged. By convention, all ten-
sors of type (0, 0) (scalars), (1, 0) (vectors),
and (0, 1) (covectors) are considered to
be both symmetric and skew symmetric.
Notice that a completely skew-symmetric
tensor of type (r, 0) with r larger than the
dimension of the vector space of departure
must necessarily vanish.
The collections of all symmetric or skew-
symmetric tensors (whether contravariant
or covariant) do not constitute a subalge-
bra of the tensor algebra, for the simple rea-
son that the tensor multiplication of two
symmetric (or skew-symmetric) tensors is
not symmetric (skew symmetric) in gen-
eral. Nevertheless, it is possible, and conve-
nient, to deﬁne algebras of symmetric and
skew-symmetric tensors by modifying the
multiplicative operation so that the results
stay within the algebra. The case of skew-
symmetric tensors is the most fruitful. It
gives rise to the so-called exterior algebra of
a vector space, which we will now explore.
It will permit us to answer many intriguing
questions such as: is there anything anal-
ogous to the cross-product of vectors in

9.6 Review of Tensor Algebra
339
dimensions other than 3? What is an area
and what is the meaning of ﬂux?
9.6.4
Exterior Algebra
The space of skew-symmetric contravari-
ant tensors of type (r, 0) will be denoted
by Λr(V). The elements of Λr(V) will be
also called r-vectors and, more generally,
multivectors. The number r is the order of
the multivector. As before, the space Λ0(V)
coincides with the scalar ﬁeld ℝ, while
Λ1(V) coincides with the vector space V.
Consider the ordered r-tuple of covec-
tors (𝜔1, 𝜔2, … , 𝜔r) and let 𝜋denote a per-
mutation of this set. Such a permutation is
even (odd) if it is obtained by an even (odd)
number of exchanges between pairs of ele-
ments in the original set. An even (odd)
permutation 𝜋has a signature, denoted by
sign(𝜋), equal to 1 (−1).
Given an arbitrary tensor T of type (r, 0),
we deﬁne its skew-symmetric part r(T) as
the multilinear map deﬁned by the formula
r(T)(𝜔1, 𝜔2, … , 𝜔r) = 1
r!
∑
𝜋
sign(𝜋) T(𝜋).
(9.67)
As
an
example,
for
the
case
of
a
contravariant
tensor
of
degree
3,
namely,
T = Tijkei ⊗ej ⊗ek,
where
eh (h = 1, … , n ≥r) is a basis of V, the
skew-symmetric part is obtained as
3(T) = 1
6
(Tijk + Tjki + Tkij −Tikj
−Tjik −Tkji)ei ⊗ej ⊗ek.
(9.68)
Given two multivectors a and b, of orders r
and s, respectively, we deﬁne their exterior
product or wedge product as the multivec-
tor a ∧b of order (r + s) obtained as
a ∧b = r+s(a ⊗b).
(9.69)
What this deﬁnition in eﬀect is saying
is that in order to multiply two skew-
symmetric tensors and obtain a skew-
symmetric result, all we have to do is take
their tensor product and then project back
into the algebra (that is, skew-symmetrize
the result).7) As is, by deﬁnition, a linear
operator, the wedge product is linear in
each of the factors.
We have seen that the tensor product is
not commutative. But, in the case of the
exterior product, exchanging the order of
the factors can at most aﬀect the sign. The
general result is
b ∧a = (−1)rsa ∧b, a ∈ΛrV, b ∈Λs(V).
(9.70)
Thus, for example, the wedge product with
itself of a multivector of odd order must
necessarily vanish. With some work, it is
possible to show that the wedge product
is
associative,
namely,
(a ∧b) ∧c = a ∧
(b ∧c).
To calculate the dimension of Λr(V), we
note that, being a tensor, every element
in Λk(V) is expressible as a linear combi-
nation of the nr tensor products ei1 ⊗· · ·
⊗eir, where ei, i = 1, … , n, is a basis of V.
Because of the skew symmetry, however, we
need to consider only products of the form
ei1 ∧· · · ∧eir. Two such products involving
the same factors in any order are either
equal or diﬀer in sign, and a product with
a repeated factor vanishes. This means that
we need only count all possible combina-
tions of n symbols taken r at a time without
7) In spite of the natural character of this deﬁni-
tion of the wedge product, many authors adopt
a deﬁnition that includes a combinatorial factor.
Thus, the two deﬁnitions lead to proportional
results. Each deﬁnition has some advantages, but
both are essentially equivalent. Our presentation
of exterior algebra follows closely that of Stern-
berg S (1983), Lectures on Diﬀerential Geometry,
2nd ed., Chelsea.

340
9 Diﬀerentiable Manifolds
repetition. The number of such combina-
tions is n!∕(n −r)!r!. One way to keep track
of all these combinations is to place the
indices i1, … , ik in strictly increasing order.
These combinations are linearly indepen-
dent, thus constituting a basis. Therefore,
the dimension of Λr(V) is n!∕(n −r)!r!.
We note that the spaces of r-vectors and
(n −r)-vectors have the same dimension.
There is a kind of fusiform dimensional
symmetry around the middle, the dimen-
sion starting at 1 for r = 0, increasing to a
maximum toward r = n∕2 (say, if n is even)
and then going back down to 1 for r = n.
This observation plays an important role
in the identiﬁcation (and sometimes confu-
sion) of physical quantities. For example, an
n-vector functions very much like a scalar,
but with a subtle diﬀerence.
Let a skew-symmetric contravariant ten-
sor a ∈Λr(V) be given by means of its
components on the basis of ei1 ⊗· · · ⊗eir
inherited from a basis e1, … , en of V as
a = ai1,…,ir ei1 ⊗· · · ⊗eir.
(9.71)
Recalling that the skew-symmetry operator
r is linear, we obtain
a = r(a) = ai1,…,ir r (ei1 ⊗· · · ⊗eir
)
= ai1,…,ir ei1 ∧· · · ∧eir.
(9.72)
In these expressions, the summation con-
vention is implied. We have obtained the
result that, given a skew-symmetric ten-
sor in components, we can substitute the
wedge products for the tensor products of
the base vectors. On the other hand, if we
would like to express the r-vector a in terms
of its components on the basis of Λr(V)
given by the wedge products of the base
vectors of V taken in strictly increasing
order of the indices, a coeﬃcient of r! will
have to be included, namely,
ai1,…,ir ei1 ∧· · · ∧eir
= r!
∑
i1<···<ir
ai1,…,ir ei1 ∧· · · ∧eir. (9.73)
This means that the components on the
basis (with strictly increasing indices) of
the skew-symmetric part of a contravariant
tensor of type (k, 0) are obtained without
dividing by the factorial k! in the projection
algorithm. This, of course, is a small advan-
tage to be gained at the expense of the sum-
mation convention.
Consider
the
n-fold
wedge
product
a = v1 ∧v2 ∧· · · ∧vn, where the v’s are
elements of an n-dimensional vector space
V. Let {e1, e2, … , en} be a basis of V. As
each of the v’s is expressible uniquely in
this basis, we may write
a = (vi1
1 ei1) ∧(vi2
2 ei2) ∧· · · ∧(vin
n ein)
= vi1
1 vi2
2 · · · vin
n ei1 ∧ei2 ∧· · · ∧ein, (9.74)
where the summation convention is in full
swing. Out of the possible nn terms in this
sum, there are exactly n! that can survive,
because each of the indices can attain n
values, but repeated indices in a term kill
it. However, because each of the surviving
terms consists of a scalar coeﬃcient times
the exterior product of all the n elements
of the basis, we can collect them all into
a single scalar coeﬃcient A multiplied by
the exterior product of the base vectors
arranged in a strictly increasing ordering
of the indices, namely, we must have that
a = Ae1 ∧e2 ∧· · · ∧en. This scalar coeﬃ-
cient consists of the sum of all the prod-
ucts vi1
1 vi2
2 · · · vin
n with no repeated indices
and with a minus sign if the superscripts
form an odd permutation of 1, 2, … , n. This
is precisely the deﬁnition of the determi-
nant of the matrix whose entries are vj
i.
We conclude that, using in Λn(V) the basis

9.7 Forms and General Tensor Fields
341
induced by a basis in V, the component of
the exterior product of n vectors in an n-
dimensional space is equal to the determi-
nant of the matrix of the components of the
individual vectors. Apart from providing a
neat justiﬁcation for the notion of determi-
nant, this formula correctly suggests that
the geometrical meaning of an n-vector
is some measure of the ability of the (n-
dimensional) parallelepiped subtended by
the vectors to contain a volume. As we have
not yet introduced any metric notion, we
cannot associate a number to this volume.
Notice on the other hand that, although we
cannot say how large a volume is, we can
certainly tell that a given n-parallelepiped
is, say, twice as large as another. Notice,
ﬁnally, that changing the order of two fac-
tors, or reversing the sense of one factor,
changes the sign of the multivector. So, n-
vectors represent oriented n-dimensional
parallelepipeds.
The collection of all multivectors of all
orders (up to the dimension of V), with
the exterior product replacing the tensor
product, constitutes the exterior algebra of
V. In a similar way, starting from the dual
space V ∗, we can construct the algebra of
multicovectors.
9.7
Forms and General Tensor Fields
9.7.1
1-Forms
Let f ∶→ℝbe a smooth function
deﬁned in a neighborhood ⊂of the
point p, and let dfp ∶T→ℝdenote its
diﬀerential at p. We can regard this diﬀer-
ential as an element of T∗
pby deﬁning its
value ⟨dfp, vp⟩on any vector vp ∈Tpas
⟨dfp, vp⟩= dfp(vp) = vp(f ).
(9.75)
In other words, the action of the evalua-
tion of the diﬀerential of the function on
a tangent vector is equal to the directional
derivative of the function in the direction of
the vector.
A smooth assignment of a covector 𝜔p
to each point p ∈is called a diﬀerential
1-form on the manifold. It can be regarded
as a cross section of the cotangent bundle,
namely, a map
𝛀∶→T∗,
(9.76)
such that 𝜋∘𝛀= id.
As we have seen, the diﬀerential of a func-
tion at a point deﬁnes a covector. It follows
that a smooth scalar function f ∶→ℝ
determines, by pointwise diﬀerentiation, a
diﬀerential 1-form 𝛀= df . It is important
to remark that not all diﬀerential 1-forms
can be obtained as diﬀerentials of functions.
The ones that can are called exact.
A diﬀerential 1-form 𝛀(that is, a cross
section of T∗) can be regarded as acting
on vector ﬁelds V (cross sections of T)
to deliver functions ⟨𝛀, V⟩∶→ℝ, by
pointwise evaluation of a covector on a vec-
tor.
9.7.2
Pullbacks
Let f ∶→ℝbe a smooth function. We
deﬁne its pullback by g as the map g∗f ∶
→ℝgiven by the composition
g∗f = f ∘g.
(9.77)
For a diﬀerential 1-form 𝛀on , we deﬁne
the pullback g∗𝛀∶→T∗by showing
how it acts, point by point, on tangent vec-
tors, namely,
⟨[g∗𝛀](p), vp⟩= ⟨𝛀(g(p)), (g∗)pvp⟩,
(9.78)

342
9 Diﬀerentiable Manifolds
which can be more neatly written in terms
of vector ﬁelds as
⟨g∗𝛀, V⟩= ⟨𝛀∘g, g∗V⟩.
(9.79)
Expressed in words, this means that the
pullback by g of a 1-form in is the 1-
form in that assigns to each vector the
value that the original 1-form assigns to the
image of that vector by g∗.
It is important to notice that the pull-
backs of functions and diﬀerential 1-forms
are always well deﬁned, regardless of
the dimensions of the spaces involved.
This should be contrasted with the push-
forwards of vector ﬁelds, which fail in
general to be vector ﬁelds on the target
manifold.
9.7.3
Tensor Bundles
Given a point p of a manifold , we
may identify the vector space V with the
tangent space Tpand construct the cor-
responding spaces of tensors of any ﬁxed
type. Following the same procedure as for
the tangent and cotangent bundles, which
will thus become particular cases, one can
deﬁne tensor bundles of any type by adjoin-
ing to each point of a manifold the tensor
space of the corresponding type. A conve-
nient notational scheme is k(), k(),
respectively, for the bundles of contravari-
ant and covariant tensors of order k.
Similarly, the bundles of k-vectors and of
k-forms can be denoted, respectively, by
Λk(), Λk(). Each of these bundles can
be shown (by a procedure identical to that
used in the case of the tangent and cotan-
gent bundles) to have a natural structure of
a diﬀerentiable manifold of the appropriate
dimension. A (smooth) section of a tensor
bundle is called a tensor ﬁeld over , of the
corresponding type. A (smooth) section of
the bundle Λk() of k-forms is also called
a diﬀerential k-form. A scalar function on a
manifold is also called a diﬀerential 0-form.
In a chart of the m-dimensional manifold
with coordinates xi, a contravariant ten-
sor ﬁeld T of order r is given as
T = Ti1,…,ir
𝜕
𝜕xi1 ⊗· · · ⊗
𝜕
𝜕xir ,
(9.80)
where Ti1,…,ir = Ti1,…,ir(x1, … , xm) are rm
smooth functions of the coordinates. Simi-
larly, a covariant tensor ﬁeld U of order r is
given by
U = Ui1,…,ir dxi1 ⊗· · · ⊗dxir,
(9.81)
and a diﬀerential r-form 𝜔by
𝜔= 𝜔i1,…,ir dxi1 ∧· · · ∧dxir.
(9.82)
Notice that, in principle, the indexed quan-
tity 𝜔i1,…,ir need not be speciﬁed as skew
symmetric with respect to the exchange
of any pair of indices, because the exte-
rior product of the base forms will do the
appropriate skew symmetrization job. As
an alternative, we may suspend the stan-
dard summation convention in (9.82) and
consider only indices in ascending order. As
a result, if 𝜔i1,…,ir is skew symmetric ab ini-
tio, the corresponding components are to
be multiplied by r!.
Of particular interest for the theory of
integration on manifolds are diﬀerential m-
forms, where m is the dimension of the
manifold. From our treatment of the alge-
bra of r-forms, we know that the dimension
of the space of m-covectors is exactly 1. In
a coordinate chart, a basis for diﬀerential
m-forms is, therefore, given by: dx1 ∧· · · ∧
dxm. In other words, the representation of a
diﬀerential m-form 𝜔in a chart is
𝜔= f (x1, … , xm) dx1 ∧· · · ∧dxm,
(9.83)

9.7 Forms and General Tensor Fields
343
where f (x1, … , xm) is a smooth scalar func-
tion of the coordinates in the patch. Con-
sider now another coordinate patch with
coordinates y1, … , ym, whose domain has a
nonempty intersection with the domain of
the previous chart. In this chart, we have
𝜔= ̂f (y1, … , ym) dy1 ∧· · · ∧dym.
(9.84)
We want to ﬁnd the relation between the
functions f and ̂f . As the transition func-
tions yi = yi(x1, … , xm) are smooth, we can
write
𝜔= ̂f (y1, … , ym) dy1 ∧· · · ∧dym
= ̂f 𝜕y1
𝜕xj1
· · · 𝜕ym
𝜕xjm
dxj1 ∧· · · ∧dxjm (9.85)
or, by deﬁnition of determinant
𝜔= det
{ 𝜕y1, … , ym
𝜕x1, … , xm
}
̂f dx1 ∧· · · ∧dxm
= Jy,x ̂f dx1 ∧· · · ∧dxm,
(9.86)
where the Jacobian determinant Jy,x does
not vanish at any point of the intersection
of the two coordinate patches. Comparing
with (9.83), we conclude that
f = Jy,x ̂f .
(9.87)
A nowhere vanishing diﬀerentiable m-form
on a manifold of dimension m is called
a volume form on . It can be shown that
a manifold is orientable if, and only if, it
admits a volume form.
The notion of pullback can be naturally
generalized for covariant tensors of any
order. For a contravariant tensor ﬁeld U
of order r on (and, in particular, for
diﬀerential r-forms on ), the pullback
by a smooth function g ∶→is a
corresponding ﬁeld on obtained by an
extension of the case r = 1, as follows:
g∗U (V1, … , Vr) = (U ∘g) (g∗V1, … , g∗Vr),
(9.88)
where U is regarded as a multilinear func-
tion of r vector ﬁelds Vi.
9.7.4
The Exterior Derivative
The exterior derivative of diﬀerential forms
is an operation that generalizes the gradi-
ent, curl, and divergence operators of clas-
sical vector calculus. The exterior derivative
of a diﬀerential r-form on a manifold is
a diﬀerential (r + 1)-form deﬁned over the
same manifold. Instead of introducing, as
one certainly could, the deﬁnition of exte-
rior diﬀerentiation in an intrinsic axiomatic
manner, we will proceed to deﬁne it in a
coordinate system and show that the def-
inition is, in fact, coordinate independent.
Let, therefore, xi (i = 1, … , m) be a coordi-
nate chart and let 𝜔be an r-form given as
𝜔= 𝜔i1,…,ir dxi1 ∧· · · ∧dxir,
(9.89)
where
𝜔i1,…,ir = 𝜔i1,…,ir(x1, … , xm)
are
smooth functions of the coordinates. We
deﬁne the exterior derivative of 𝜔, denoted
by d𝜔, as the diﬀerential (r + 1)-form
obtained as
d𝜔= d𝜔i1,…,ir ∧dxi1 ∧· · · ∧dxir,
(9.90)
where the d on the right-hand side denotes
the ordinary diﬀerential of functions. More
explicitly,
d𝜔=
𝜕𝜔i1,…,ir
𝜕xk
dxk ∧dxi1 ∧· · · ∧dxir.
(9.91)
Note that for each speciﬁc combination
of (distinct) indices i1, … , ir, the index k
ranges only on the remaining possibilities,

344
9 Diﬀerentiable Manifolds
because the exterior product is skew sym-
metric. Thus, in particular, if 𝜔is a diﬀeren-
tial m-form deﬁned over an m-dimensional
manifold, its exterior derivative vanishes
identically (as it should, being an (m + 1)-
form).
Let yi (i = 1, … , m) be another coordi-
nate chart with a nonempty intersection
with the previous chart. We have
𝜔= ̂𝜔i1,…,ir dyi1 ∧· · · ∧dyir,
(9.92)
for some smooth functions ̂𝜔i1,…,ir of the
yi-coordinates. The two sets of components
are related by
𝜔i1,…,ir = ̂𝜔j1,…,jr
𝜕yj1
𝜕xi1 · · · 𝜕yjr
𝜕xir .
(9.93)
Notice that we have not troubled to collect
terms by, for example, prescribing a strictly
increasing order. The summation conven-
tion is in eﬀect. We now apply the prescrip-
tion (9.90) and obtain
d𝜔= d
(
̂𝜔j1,…,jr
𝜕yj1
𝜕xi1 · · · 𝜕yjr
𝜕xir
)
∧dxi1 ∧· · · ∧dxir.
(9.94)
The crucial point now is that the terms con-
taining the second derivatives of the coor-
dinate transformation will evaporate as a
result of their intrinsic symmetry, because
they are contracted with an intrinsically
skew-symmetric wedge product of two 1-
forms. We have, therefore,
d𝜔=
𝜕̂𝜔j1,…,jr
𝜕ym
𝜕ym
𝜕xk
𝜕yj1
𝜕xi1 · · · 𝜕yjr
𝜕xir
dxk ∧dxi1 ∧· · · ∧dxir,
(9.95)
or, ﬁnally,
d𝜔=
𝜕̂𝜔j1,…,jr
𝜕ym
dym ∧dyj1 ∧· · · ∧dyjr,
(9.96)
which is exactly the same prescription in
the coordinate system yi as (9.90) is in the
coordinate system xi. This completes the
proof of independence from the coordinate
system.
From this deﬁnition, we can deduce a
number of important properties of the exte-
rior derivative, namely,
1. Linearity
d(a 𝛼+ b 𝛽) = a d𝛼+ b d𝛽
∀a, b ∈ℝ
𝛼, 𝛽∈Λr().
(9.97)
2. Quasi-Leibniz rule
d(𝛼∧𝛽) = d𝛼∧𝛽+ (−1)r𝛼∧d𝛽
∀𝛼∈Λr(), 𝛽∈Λs().
(9.98)
3. Nilpotence
d2(.) = d(d(.)) = 0.
(9.99)
Moreover, it can be shown that the exte-
rior derivative commutes with pullbacks.
Finally, the exterior derivative of a 1-form
has the following interesting interaction
with the Lie bracket. If 𝛼is a diﬀerential 1-
form and u and v are smooth vector ﬁelds
on a manifold , then
⟨d𝛼| u ∧v⟩= u (⟨𝛼| v⟩) −v (⟨𝛼| u⟩)
−⟨𝛼| [u, v]⟩.
(9.100)
A diﬀerential form 𝜔is closed if d𝜔= 0.
Thus, all m-forms in an m-dimensional
manifold are automatically closed. An
r-form (with r > 1) is exact if there exists
an (r −1)-form 𝜎such that 𝜔= d𝜎. By
Property 3 above, all exact forms are
closed. The converse is true locally. In
other words, for every point in a mani-
fold, there exists an open neighborhood
on which the restriction of a closed

9.8 Symplectic Geometry
345
form is exact. But this property may fail
globally. An example is the 1-form given
by 𝜔= (x dy −y dx)∕(x2 + y2) deﬁned on
an annular region of ℝ2 with center at
the origin x = y = 0. This form is closed
but not exact. The existence of forms of
this type reﬂects the presence of topo-
logical invariants (such as holes) in the
manifold.
9.8
Symplectic Geometry
9.8.1
Symplectic Vector Spaces
A tensor T of type (0, r) on V is a multilin-
ear function acting on r vector arguments,
(v1, … , vr). Fixing one argument, say v1, we
obtain a tensor Tv1 of type (0, r −1). In par-
ticular, a tensor T of type (0, 2) assigns to
each vector u ∈V the covector Tu deﬁned
by
Tu(v) = T(u, v)
∀v ∈V.
(9.101)
The tensor T of type (0, 2) is nondegener-
ate if Tu = 0 implies that u = 0. Since, in a
given basis, the components of the covector
Tu are Tijui, we conclude that a necessary
and suﬃcient condition for T to be nonde-
generate is that the matrix with entries [Tij]
must have a non-vanishing determinant, a
condition that is independent of the basis
chosen.
A symplectic vector space is a vector
space in which a nondegenerate 2-covector
𝜔has been singled out. The standard
example is provided by a vector space
of even dimension 2m. Choosing a basis
{e1, … , em, f1, … , fm}, the 2-covector
𝜔ef =
m
∑
i=1
ei ∧fi
(9.102)
is nondegenerate. It can be shown that
every symplectic vector space is necessar-
ily even-dimensional and that there exists a
basis for which 𝜔has the form (9.102).
An important property of a symplectic
vector space is that, owing to the nonde-
generacy of the 2-covector 𝜔, there exists
a natural correspondence between vectors
and covectors.
9.8.2
Symplectic Manifolds
Recall that an r-form 𝜔on a manifold is
a smooth r-covector ﬁeld, namely, a smooth
assignment of an r-covector 𝜔p at each
point p ∈. Equivalently, 𝜔is a (smooth)
section of the bundle Λr(). A symplectic
form on is a nondegenerate closed 2-
form 𝜔. A symplectic manifold (, 𝜔) is a
manifold in which a symplectic form 𝜔has
been singled out. According to our discus-
sion above, a symplectic manifold is neces-
sarily even-dimensional.
Given an m-dimensional manifold 
(for example, the conﬁguration space of
a mechanical system), the tangent and
cotangent bundles are manifolds of even
dimension 2m. It is a remarkable fact that
the cotangent bundle T∗Q of any manifold
is automatically endowed with a canonical
symplectic form. By “canonical,” we mean
that this form is deﬁned intrinsically (i.e.,
independently of any coordinate chart).
It is not surprising, therefore, that this
canonical structure results in a corre-
sponding physical interpretation. For a
mechanical system, the cotangent bundle
represents the phase space (of positions
and momenta) and the canonical form
plays a fundamental role in Hamiltonian
mechanics.
A generic point s ∈T∗has the form s =
(q, p), where q = 𝜋(s) ∈and p ∈T∗
q .
Put diﬀerently, a point in the cotangent

346
9 Diﬀerentiable Manifolds
bundle consists of a point q in the base
manifold and a 1-covector p at q. Let V
be a tangent vector to T∗at the point
s = (q, p) ∈T∗, namely, V ∈T(T∗). As
the projection 𝜋∶T∗Q →Q is a diﬀeren-
tiable map, its diﬀerential 𝜋∗∶T(T∗) →
Tis well deﬁned. In particular, 𝜋∗(Vs) ∈
Tq. But the tangent bundle T(T∗), as
a tangent bundle, has its own projection
̂𝜏∶T(T∗) →T∗. In particular, ̂𝜏(Vs) =
s = (q, p). As this is a covector at q ∈, it
makes sense to evaluate it on the tangent
vector 𝜋∗(Vs) ∈Tq.
Recall that a 1-form on T∗is a smooth
assignment of a covector 𝜃s at each point
s = (q, p) ∈T∗. We deﬁne the canonical
1-form 𝜃on T∗by the formula
𝜃(Vs) = ⟨̂𝜏(Vs), 𝜋∗(Vs)⟩.
(9.103)
The canonical symplectic form 𝜔on T∗is
deﬁned as
𝜔= −d𝜃.
(9.104)
Thus, 𝜔is exact and, therefore, closed.
Moreover, it is nondegenerate. It is, in fact,
not diﬃcult to obtain a coordinate expres-
sion of the canonical symplectic form. We
have seen that a chart (q1, … , qm) in 
induces a chart in T∗. Indeed, any 1-form
p on has the coordinate expression
p = pidqi,
where
the
summation
con-
vention is in force. The induced chart in
T∗Q uses as coordinates the 2m numbers
(q1, … , qm, p1, … , pm). The canonical 1-
form 𝜃is given by 𝜃= pidqi. It follows that
the canonical symplectic form is expressed
as 𝜔= −dpi ∧dqi = dqi ∧dpi.
9.8.3
Hamiltonian Systems
A Hamiltonian system consists of a sym-
plectic manifold (, 𝜔) and a smooth
real-valued function ∶→ℝcalled
the
system
Hamiltonian.
In
Classical
Mechanics, the symplectic manifold is
identiﬁed with the phase space = T∗
of the underlying conﬁguration manifold .
A key concept in Hamiltonian systems
is that of Hamiltonian vector ﬁeld. As the
Hamiltonian is diﬀerentiable, its diﬀer-
ential dis a well-deﬁned 1-form on . In
a symplectic manifold, on the other hand, to
each 1-form, we can assign uniquely a vec-
tor ﬁeld, by exploiting the pointwise nonde-
generacy of the symplectic form. We thus
obtain the associated Hamiltonian vector
ﬁeld VH. More explicitly, at each point s ∈
we have
⟨d, U⟩= 𝜔(VH, U)
∀U ∈Ts.
(9.105)
A curve 𝛾in is a trajectory of the Hamil-
tonian system if it satisﬁes Hamilton’s
equations, namely, if it is an integral curve
of the Hamiltonian vector ﬁeld, namely,
d𝛾
dt = VH(𝛾(t)).
(9.106)
In the natural coordinates of a cotangent
bundle, the curve 𝛾consists of the 2m
functions qi = qi(t) and pi = pi(t), with i =
1, … , m. The Hamiltonian vector ﬁeld has
the components 𝜕H∕𝜕pi and −𝜕H∕𝜕qi. We
thus recover the standard form of Hamil-
ton’s equations, that is,
dqi
dt = 𝜕H
𝜕pi
,
(9.107)
and
dpi
dt = −𝜕H
𝜕qi ,
(9.108)
Notice that the construction (9.105) applies
to any smooth real-valued function deﬁned
on , not just the Hamiltonian. Namely, to
any such function we can uniquely assign
a vector ﬁeld VG. We can thus deﬁne an

9.9 The Lie Derivative
347
operation between any two scalar ﬁelds 
and , called the Poisson bracket {, }, by
any of the equivalent prescriptions:
{, } = VK() = ⟨d, VK⟩= 𝜔(VG, VK).
(9.109)
The derivative of a scalar function along
a trajectory 𝛾of the Hamiltonian system
(, ) is obtained as
d
dt = d𝛾
dt () = ⟨d, d𝛾
dt ⟩
= ⟨d, VH⟩= {, }.
(9.110)
Thus, the Poisson bracket of a function
(representing some physical property of
the system) with the Hamiltonian func-
tion describes the time evolution of . The
vanishing of this Poisson bracket indicates,
therefore, a conserved quantity.
9.9
The Lie Derivative
9.9.1
The Flow of a Vector Field
Let V ∶→Tbe a (smooth) vector
ﬁeld. A (parameterized) curve 𝛾∶H →
is called an integral curve of the vector ﬁeld
if its tangent at each point coincides with
the vector ﬁeld at that point. In other words,
denoting by s the curve parameter, the fol-
lowing condition holds:
d𝛾(s)
ds
= V(𝛾(s))
∀s ∈H ⊂ℝ.
(9.111)
As a consequence of the fundamental
theorem of existence and uniqueness of
local solutions of systems of ordinary dif-
ferential equations, it is possible to prove
the following fundamental theorem for
vector ﬁelds on manifolds.
Theorem 9.2 If V is a vector ﬁeld on a
manifold , then for every p ∈, there
exists an integral curve 𝛾(s, p) ∶Ip →
such that (i) Ip is an open interval of ℝcon-
taining the origin s = 0; (ii) 𝛾(0, p) = p; and
(iii) Ip is maximal in the sense that there
exists no integral curve starting at p and
deﬁned on an open interval of which Ip is a
proper subset. Moreover,
𝛾(s, 𝛾(s′, x)) = 𝛾(s + s′, x)
∀s, s′, s + s′ ∈Ip.
(9.112)
The map given by
p, s →𝛾(s, p)
(9.113)
is called the ﬂow of the vector ﬁeld V whose
integral curves are 𝛾(s, p). In this deﬁni-
tion, the map is expressed in terms of its
action on pairs of points belonging to two
diﬀerent manifolds, and ℝ, respectively.
Not all pairs, however, are included in the
domain, because Ip is not necessarily equal
to ℝ. Moreover, because the intervals Ip are
point dependent, the domain of the ﬂow is
not even a product manifold. One would
be tempted to take the intersection of all
such intervals so as to work with a prod-
uct manifold given by times the smallest
interval Ip. Unfortunately, as we know from
elementary calculus, this (inﬁnite) intersec-
tion may consist of a single point. All that
can be said about the domain of the ﬂow
is that it is an open subset of the Cartesian
product × ℝ. When the domain is equal
to this product manifold, the vector ﬁeld
is said to be complete and the correspond-
ing ﬂow is called a global ﬂow. It can be
shown that if is compact, or if the vector
ﬁeld is smooth and vanishes outside a com-
pact subset of , the ﬂow is necessarily
global.

348
9 Diﬀerentiable Manifolds
9.9.2
One-parameter Groups of
Transformations Generated by Flows
Given a point p0 ∈, it is always possi-
ble to ﬁnd a small enough neighborhood
U(p0) ⊂such that the intersection of all
the intervals Ip with p ∈U(p0) is an open
interval J containing the origin. For each
value s ∈J, the ﬂow 𝛾(s, p) can be regarded
as a map
𝛾s ∶U(p0) −→,
(9.114)
deﬁned as
𝛾s(p) = 𝛾(s, p),
p ∈U(p0).
(9.115)
This map is clearly one-to-one, because
otherwise we would have two integral
curves intersecting each other, against the
statement of the fundamental theorem.
Moreover, again according to the funda-
mental theorem, this is a smooth map
with a smooth inverse over its image. The
inverse is, in fact, given by
𝛾−1
s
= 𝛾−s,
(9.116)
where 𝛾−s
is deﬁned over the image
𝛾s(U(p0)). Notice that 𝛾0 is the identity
map of U(p0). Finally, for the appropriate
range of values of s and r, we have the
composition law
𝛾r ∘𝛾s = 𝛾r+s.
(9.117)
The set of maps 𝛾s is said to constitute the
one-parameter local pseudo-group gener-
ated by the vector ﬁeld (or by its ﬂow). If the
neighborhood U(p0) can be extended to the
whole manifold for some open interval J (no
matter how small), each map 𝛾s is called a
transformation of . In that case, we speak
of a one-parameter pseudo-group of trans-
formations of . Finally, in the best of all
possible worlds, if J = ℝthe one-parameter
subgroup of transformations becomes ele-
vated to a one-parameter group of trans-
formations. This is an Abelian (i.e., com-
mutative) group, as is clearly shown by the
composition law (9.117). We may say that
every complete vector ﬁeld generates a one-
parameter group of transformations of the
manifold.
The
converse
construction,
namely,
the generation of a vector ﬁeld out of
a given one-parameter pseudo-group of
transformations, is also of interest. It can be
shown that every one-parameter pseudo-
group of transformations 𝛾s is generated by
the vector ﬁeld
V(p) = d𝛾s(p)
ds
|s=0.
(9.118)
9.9.3
The Lie Derivative
We have learned that a vector ﬁeld deter-
mines at least a one-parameter pseudo-
group in a neighborhood of each point of
the underlying manifold. For each value of
the parameter s within a certain interval
containing the origin, this neighborhood
is mapped diﬀeomorphically onto another
neighborhood. Having at our disposal a dif-
feomorphism, we can consider the pushed-
forward or pulled-back versions of tensors
of every type, including multivectors and
diﬀerential forms. Physically, these actions
represent how the various quantities are
convected (or dragged) by the ﬂow. To elicit
a mental picture, we show in Figure 9.4 a
vector wp in a manifold as a small segment
−→
pq (a small piece of a curve, say), and we
draw the integral curves of a vector ﬁeld
V emerging from each of its end points, p
and q. These curves are everywhere tan-
gent to the underlying vector ﬁeld V, which

9.9 The Lie Derivative
349
wp
w′
p
q
p′
q′
Δs
Δs
Figure 9.4
Dragging of a vector by a ﬂow.
we do not show in the ﬁgure. If s denotes
the (natural) parameter along these integral
curves, an increment of Δs applied from
each of these points along the correspond-
ing integral curve, will result in two new
points p′ and q′, respectively. The (small)
segment −−→
p′q′ can be seen as a vector w′,
which we regard as the convected counter-
part of wp as it is dragged by the ﬂow of V
by an amount Δs. If wp happens to be part
of a vector ﬁeld W deﬁned in a neighbor-
hood of p′, so that wp = W(p), we have that
at the point p′ there is, in addition to the
dragged vector w′, a vector W(p′). There is
no reason why these two vectors should be
equal. The diﬀerence W(p′) −w′ (divided
by Δs) gives us an idea of the meaning of
the Lie derivative of W with respect to V
at p′.
The idea behind the deﬁnition of the Lie
derivative of a tensor ﬁeld with respect to
a given vector ﬁeld at a point p is the fol-
lowing. We consider a small value s of the
parameter and convect the tensor ﬁeld back
to s = 0 by using the appropriate pullback
or pushforward. This operation will, in par-
ticular, provide a value of the convected
tensor ﬁeld at the point p. We then subtract
from this value the original value of the
ﬁeld at that point (a legitimate operation,
because both tensors operate on the same
tangent and/or cotangent space), divide by
s and compute the limit as s →0. To under-
stand how to calculate a Lie derivative, it
is suﬃcient to make the deﬁnition explicit
for the case of functions, vector ﬁelds, and
1-forms. The general case is then inferred
from these three basic cases, as we shall
demonstrate. We will also prove that the
term “derivative” is justiﬁed. Notice that a
Lie derivative is deﬁned with respect to a
given vector ﬁeld. It is not an intrinsic prop-
erty of the tensor ﬁeld being diﬀerentiated.
The Lie derivative of a tensor ﬁeld at a point
is a tensor of the same type.
9.9.3.1
The Lie Derivative of a Scalar
Let g ∶→be a mapping between two
manifolds and let f ∶→ℝbe a function.
Recall that, according to (9.77), the pullback
of f by g is the map g∗f ∶→ℝdeﬁned as
the composition
g∗f = f ∘g.
(9.119)
Let a (time-independent, for now) vec-
tor ﬁeld V be deﬁned on and let
𝛾s ∶U →denote the action of its ﬂow
on a neighborhood of a point p ∈. If
a function f ∶→ℝis deﬁned, we can
calculate
𝛾∗
s f ∶= f ∘𝛾s.
(9.120)
The Lie derivative at the point p is given
by

350
9 Diﬀerentiable Manifolds
LV f (p) = lim
s→0
(𝛾∗
s f )(p) −f (p)
s
= lim
s→0
f (𝛾s(p)) −f (p)
s
(9.121)
Thus, we obtain
LV f (p) = vp(f ).
(9.122)
In simple words, the Lie derivative of a
scalar ﬁeld with respect to a given vec-
tor ﬁeld coincides, at each point, with the
directional derivative of the function in the
direction of the ﬁeld at that point.
9.9.3.2
The Lie Derivative of a Vector
Field
Vectors are pulled forward by mappings.
Thus, given the map g ∶→, to bring a
tangent vector from back to , we must
use the fact that g is invertible and that the
inverse is diﬀerentiable, such as when g is
a diﬀeomorphism. Let W ∶→Tbe a
vector ﬁeld on . The corresponding vec-
tor ﬁeld on is then given by g−1
∗∘W ∘g ∶
→T. Accordingly, the Lie derivative
of the vector ﬁeld W with respect to the
vector ﬁeld V, with ﬂow 𝛾s, at a point p ∈
is deﬁned as
LVW(p) = lim
s→0
𝛾−1
s∗∘W ∘𝛾s(p) −W(p)
s
.
(9.123)
It can be shown that the Lie derivative of a
vector ﬁeld coincides with the Lie bracket
LVW = [V, W].
(9.124)
9.9.3.3
The Lie Derivative of a 1-form
As 1-forms are pulled back by a map, we
deﬁne the Lie derivative of the 1-form 𝜔∶
→T∗at the point p as
LV𝜔(p) = lim
s→0
𝛾∗
s ∘𝜔∘𝛾s(p) −𝜔(p)
s
. (9.125)
9.9.3.4
The Lie Derivative of Arbitrary
Tensor Fields
It is clear that, by virtue of their deﬁni-
tion by means of limits, the Lie deriva-
tives deﬁned so far are linear operators. To
extend the deﬁnition of the Lie derivative to
tensor ﬁelds of arbitrary order, we need to
make sure that the Leibniz rule with respect
to the tensor product is satisﬁed. Other-
wise, we would not have the right to use the
term “derivative” to describe it. It is enough
to consider the case of a monomial such as
T = 𝜔1 ⊗· · · ⊗𝜔m ⊗W1 ⊗· · · ⊗Wn,
(9.126)
where 𝜔i are m 1-forms and Wj are n vector
ﬁelds. We deﬁne
LVT(p) = lim
s→0
𝛾∗
s ∘𝜔1 ∘𝛾s(p) ⊗· · · ⊗𝛾−1
s∗
∘W1 ∘𝛾s(p) ⊗· · · −T(p)
s
.
(9.127)
Let us verify the satisfaction of the Leibniz
rule for the case of the tensor product of a
1-form by a vector.
LV(𝜔⊗W)(p) = lim
s→0
𝛾∗
s ∘𝜔∘𝛾s(p) ⊗𝛾−1
s∗
∘W ∘𝛾s(p) ⊗−𝜔(p) ⊗W(p)
s
. (9.128)
Subtracting and adding to the denominator
the expression 𝜔(p) ⊗𝛾−1
s∗∘W ∘𝛾s(p) the
Leibniz rule follows suit.
An important property of the Lie deriva-
tive is the following: The Lie derivative of a
diﬀerential form (of any order) commutes
with the exterior derivative, that is,
LV(d𝜔) = d(LV𝜔),
(9.129)
for all vector ﬁelds V and for all diﬀerential
forms 𝜔.

Further Reading
351
9.9.3.5
The Lie Derivative in Components
Taking advantage of the Leibniz rule, it is
not diﬃcult to calculate the components
of the Lie derivative of a tensor in a given
coordinate system xi, provided the com-
ponents of the Lie derivative of the base
vectors 𝜕∕𝜕xi and the base 1-forms dxi
are known. A direct application of the for-
mula (9.36) for the components of the Lie
bracket, yields
LV
( 𝜕
𝜕xi
)
= −𝜕V k
𝜕xi
𝜕
𝜕xk .
(9.130)
To obtain the Lie derivative of dxi, we recall
that the action of dxi (as a covector) on
𝜕∕𝜕xj is simply 𝛿i
j, whose Lie derivative van-
ishes. This action can be seen as the con-
traction of their tensor product. We obtain,
therefore,
0 = LV
⟨
dxi, 𝜕
𝜕xj
⟩
=
⟨
dxi, −𝜕V k
𝜕xj
𝜕
𝜕xk
⟩
+
⟨
Lvdxi, 𝜕
𝜕xj
⟩
,
(9.131)
whence
LVdxi = 𝜕V i
𝜕xk dxk.
(9.132)
Further Reading
Some general treatises on Diﬀerential
Geometry:
Chern, S.S., Chern, W.H., and Lam, K.S. (1999)
Lectures on Diﬀerential Geometry, World
Scientiﬁc.
Kobayashi, S. and Nomizu, K. (1996) Foundations
of Diﬀerential Geometry, Wiley Classics
Library Edition.
Lee, J.M. (2003) Introduction to Smooth
Manifolds, Springer.
Sternberg, S. (1983) Lectures on Diﬀerential
Geometry, 2nd edn, Chelsea Publishing
Company.
Warner, F.W. (1983) Foundations of Diﬀerentiable
Manifolds and Lie Groups, Springer.
Some
books
that
emphasize
physi-
cal applications or deal with particular
physical theories in a geometric way are
Abraham, R. and Marsden, J.E. (2008)
Foundations of Mechanics, 2nd edn, AMS
Chelsea Publishing.
Arnold, V.I. (1978) Mathematical Methods of
Classical Mechanics, Springer.
Choquet-Bruhat, Y., de Witt-Morette, C., and
Dillard-Beck, M. (1977) Analysis, Manifolds
and Physics, North-Holland.
Frankel, T. (2004) The Geometry of Physics: An
Introduction, 2nd edn, Cambridge University
Press.
Misner, W., Thorne, K.S., and Wheeler, J.A.
(1973) Gravitation, W H Freeman and
Company.
Much of the material in this article is
reproduced, with permission, from
Epstein, M. (2010) The Geometrical Language of
Continuum Mechanics, Cambridge University
Press.


353
10
Topics in Diﬀerential Geometry
Marcelo Epstein
10.1
Integration
10.1.1
Integration of n-Forms in ℝn
The simplest n-dimensional manifold is ℝn
itself with the standard topology and the
standard notion of diﬀerentiability. Accord-
ingly, we present the standard notion of
integration over a domain of ℝn in terms of
diﬀerential forms so as to be able to extend
this notion to arbitrary manifolds.
Let x1, … , xn be the standard global chart
of ℝn, and let 𝜔be a smooth n-form deﬁned
over some open set ⊂ℝn. There exists,
then, a smooth function f ∶→ℝsuch
that
𝜔= f dx1 ∧· · · ∧dxn.
(10.1)
For any regular domain of integration ⊂
, we deﬁne
∫
𝜔= ∫∫· · · ∫
⏟⏞⏞⏞⏞⏟⏞⏞⏞⏞⏟

f dx1dx2 · · · dxn,
(10.2)
where the right-hand side is the ordinary n-
fold Riemann integral in ℝn.
It is important to check that this deﬁni-
tion is independent of the coordinate sys-
tem adopted in . For this purpose, let
𝜙∶−→ℝn
(10.3)
be a coordinate transformation expressed
in components as the n smooth functions
x1, … , xn →y1(x1, … , xn), … , yn(x1, … , xn).
(10.4)
Recall that for (10.4) to qualify as a coordi-
nate transformation, the Jacobian determi-
nant
J = det
[ 𝜕(y1, … , yn)
𝜕(x1, … , xn)
]
,
(10.5)
must be nonzero throughout . For deﬁ-
niteness, we will assume that it is strictly
positive (so that the change of coordinates
is orientation preserving). According to
the formulas of transformation of variables
under a multiple Riemann integral, we
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

354
10 Topics in Diﬀerential Geometry
must have
∫∫· · · ∫
⏟⏞⏞⏞⏞⏟⏞⏞⏞⏞⏟

f (xi) dx1 · · · dxn
= ∫∫· · · ∫
⏟⏞⏞⏞⏞⏟⏞⏞⏞⏞⏟

f (xi(yj)) J−1dy1 · · · dyn.
(10.6)
But, because 𝜔is an n-form in an n-
dimensional manifold, its representation in
the new coordinate system is precisely
𝜔= f (xi(yj)) J−1dy1 ∧· · · ∧dyn,
(10.7)
which shows that the deﬁnition (10.2) is
indeed independent of the coordinate sys-
tem adopted in .
A more fruitful way to exploit the coordi-
nate independence property is to regard 𝜙∶
→ℝn not as a mere coordinate trans-
formation but as an actual change of the
domain of integration. In this case, the
transformation formula is interpreted read-
ily in terms of the pullback of 𝜔as
∫𝜙()
𝜔= ∫
𝜙∗(𝜔),
(10.8)
for every n-form 𝜔deﬁned over an open set
containing 𝜙().
10.1.2
Integration of Forms on Oriented
Manifolds
Let be an oriented manifold of dimen-
sion m and let (, 𝜓) be a (consistently)
oriented chart. The integral of an m-form
𝜔over is deﬁned as
∫
𝜔= ∫𝜓()
(
𝜓−1)∗𝜔.
(10.9)
Notice that the right-hand side is a stan-
dard Riemann (or Lebesgue) integral of a
function in ℝm, according to (10.2). In other
words, given an m-form deﬁned over the
domain of a chart in an m-dimensional
manifold, we simply pull back this form to
the codomain of the chart (an open subset
of ℝm) and integrate. The result is indepen-
dent of the chart used.
To integrate over a domain covered
by more than one chart, we need to use
the concept of partition of unity, whose
detailed presentation we omit. Brieﬂy and
imprecisely stated, a partition of unity is
a collection of real-valued nonnegative
smooth functions, one for each of the
members of an open cover and vanishing
outside it. Moreover, we assume that the
cover is locally ﬁnite, in the sense that
each point of the manifold belongs to
only a ﬁnite number of members of the
cover. Finally, the (ﬁnite) sum of all these
functions at each point is equal to 1 (hence
the name). It can be shown that partitions
of unity exist provided the manifold is
paracompact, namely, it admits a locally
ﬁnite cover. Denoting by 𝜙i the functions
making up the partition of unity, we deﬁne
the integral as
∫
𝜔=
∑
i ∫
𝜙i𝜔,
(10.10)
where the integrand on the right-hand side
is just the product of a function times a
diﬀerential form. Each integral on the right-
hand side is well deﬁned by (10.9).
For the deﬁnition implied by (10.10) to
make sense, we must prove that the result
is independent of the choice of charts and
of the choice of partition of unity. This can
be done quite straightforwardly by express-
ing each integral of the right-hand side of
one choice in terms of the quantities of the
second choice, and vice versa.

10.2 Fluxes in Continuum Physics
355
10.1.3
Stokes’ Theorem
The boundary of an m-dimensional with
boundary will be denoted by 𝜕, which we
consider as a manifold of dimension m −1.
For example, may be a closed ball in ℝm
and 𝜕the bounding spherical surface.
The boundary of an oriented manifold can
be consistently oriented. Given an (m −1)-
form 𝜔on , it makes sense to calculate its
integral over the (oriented) boundary 𝜕.
Stokes’ theorem asserts that
∫𝜕
𝜔= ∫
d𝜔.
(10.11)
We omit the proof and limit ourselves to
remark that this elegant formula encom-
passes all the integral theorems of ordinary
vector calculus.
10.2
Fluxes in Continuum Physics
One of the basic notions of Continuum
Physics is that of an extensive property, a
term that describes a property that may be
assigned to subsets of a given universe, such
as the mass of various parts of a material
body, the electrical charge enclosed in a
certain region of space, and so on. Mathe-
matically speaking, therefore, an extensive
property is expressed as a real-valued set
function p, whose argument ranges over
subsets of a universe . It is usually
assumed, on physical grounds, that the
function p is additive, namely,
p(1 ∪2) = p(1) + p(2),
whenever
1 ∩2 = ∅.
(10.12)
With proper regularity assumptions, addi-
tivity means that, from the mathematical
standpoint, p is a measure in .
In the appropriate space–time con-
text, the balance of an extensive property
expresses a relation between the rate of
change of the property in a given region
and the causes responsible for that change.
Of particular importance is the idea of ﬂux
of the property through the boundary of a
region, which is an expression of the rate of
change of the property as a result of inter-
action with other regions. It is a common
assumption that the ﬂux between regions
takes place through, and only through,
common boundaries. In principle, the
ﬂux is a set function on the boundaries of
regions. In most physical theories, how-
ever, this complicated dependence can be
greatly simpliﬁed by means of the so-called
Cauchy postulates and Cauchy’s theorem.
10.2.1
Extensive-Property Densities
We will identify the universe as an m-
dimensional diﬀerentiable manifold. Under
appropriate continuity assumptions, a set
function such as the extensive property p is
characterized by a density. Physically, this
means that the property at hand cannot
be concentrated on subsets of dimension
lower than m. More speciﬁcally, we assume
that the density 𝜌of the extensive property
p is a smooth m-form on such that
p() = ∫
𝜌,
(10.13)
for any subset ⊂for which the integral
is deﬁned. Clearly, the additivity condition
(10.12) is satisﬁed automatically.
We introduce the time variable t as if
space–time were just a product mani-
fold ℝ× . In fact, this trivialization is
observer dependent, but it will serve for
our present purposes. The density 𝜌of the
extensive property p should, accordingly,
be conceived as a function 𝜌= 𝜌(t, x),

356
10 Topics in Diﬀerential Geometry
where x ∈. Notice that, because for
ﬁxed x and variable t, 𝜌belongs to the same
vector space Λm (T∗
x ), it makes sense to
take the partial derivative with respect to t
to obtain the new m-form
𝛽= 𝜕𝜌
𝜕t ,
(10.14)
deﬁned on . For a ﬁxed (i.e., time-
independent) region , we may write
dp()
dt
= ∫
𝛽.
(10.15)
In other words, the integral of the m-form
𝛽over a ﬁxed region measures the rate of
change of the content of the property p
inside that region.
10.2.2
Balance Laws, Flux Densities, and Sources
In the classical setting of continuum
physics, it is assumed that the change of
the content of a smooth extensive property
p within a ﬁxed region can be attributed
to just two causes: (i) the rate at which the
property is produced (or destroyed) within
by the presence of sources and sinks and
(ii) the rate at which the property enters or
leaves through its boundaries, namely,
the ﬂux of p. For the sake of deﬁniteness,
in this section we adopt the convention
that the production rate is positive for
sources (rather than sinks) and that the
ﬂux is positive when there is a an outﬂow
(rather than an inﬂow) of the property. The
balance equation for the extensive property
p states that the rate of change of p in a
ﬁxed region equals the diﬀerence between
the production rate and the ﬂux. A good
physical example is the balance of internal
energy in a rigid body due to volumetric
heat sources and heat ﬂux through the
boundaries.
As we have assumed continuity for p as a
set function, we will do the same for both
the production and the ﬂux. As a result,
we postulate the existence of an m-form
s, called the source density such that the
production rate in a region is given by the
integral
∫
s.
(10.16)
Just as 𝜌itself, the m-form s is deﬁned
over all of and is independent of .
Thus, from the physical point of view, we
are assuming that the phenomena at hand
can be described locally. This assumption
excludes interesting phenomena, such as
internal actions at a distance or surface-
tension eﬀects.
As far as the ﬂux term is concerned, we
also assume that it is a continuous func-
tion of subsets of the boundary 𝜕R. We pos-
tulate the existence, for each region , of
a smooth (m −1)-form 𝜏, called the ﬂux
density, such that the ﬂux of p is given by
∫𝜕
𝜏.
(10.17)
Thus, the classical balance law of the
property p assumes the form
∫
𝛽= ∫
s −∫𝜕
𝜏.
(10.18)
An equation of balance is said to be a
conservation law if both s and 𝜏vanish
identically.
10.2.3
Flux Forms and Cauchy’s Formula
We note that (beyond the obvious fact that
𝛽and s are m-forms, whereas 𝜏is an (m −
1)-form), there is an essential complication
peculiar to the ﬂux densities 𝜏. Indeed,
in order to specify the ﬂux for the various

10.2 Fluxes in Continuum Physics
357
regions of interest, it seems that one has
to specify the form 𝜏for each and every
region . In other words, while the rate of
change of the property and the production
term are speciﬁed by forms whose domain
(for each time t) is the entire space , the
ﬂux term must be speciﬁed by means of a
set function, whose domain is the collection
of all regions. We refer to the set function
→𝜏as a system of ﬂux densities. Con-
sider, for example, a point x ∈belong-
ing simultaneously to the boundaries of two
diﬀerent regions. Clearly, we do not expect
that the ﬂux density will be the same for
both. The example of suntanning should
be suﬃciently convincing in this regard.
Consider, however, the following particular
case. Let the natural inclusion map
𝜄∶𝜕−→
(10.19)
be deﬁned by
𝜄(x) = x
∀x ∈𝜕.
(10.20)
Notice that this formula makes sense,
because 𝜕⊂. Moreover, the map 𝜄
is smooth. It can, therefore, be used to
pullback forms of any order on to forms
of the same order on 𝜕. In particular, we
can deﬁne
∫𝜕
𝜙= ∫𝜕
𝜄∗(𝜙),
(10.21)
for any form 𝜙on . Let us now assume
the existence of a globally deﬁned (m −1)-
ﬂux form Φ on and let us deﬁne the
associated system of ﬂux densities by
means of the formula
𝜏= 𝜄∗
𝜕(Φ),
(10.22)
where we use the subscript 𝜕to empha-
size the fact that each region requires its
own inclusion map. Equation (10.22) is
known as Cauchy’s formula. Clearly, this is
a very special system of ﬂux densities (just
as a conservative force ﬁeld is a special vec-
tor ﬁeld derivable from a single scalar ﬁeld).
Nevertheless, it is one of the fundamental
results of classical Continuum Mechanics
that, under rather general assumptions
(known as Cauchy’s postulates), every sys-
tem of ﬂux densities can be shown to derive
from a unique ﬂux form using Cauchy’s for-
mula (10.22). We will omit the general proof
of this fact, known as Cauchy’s theorem.
In less technical terms, Cauchy’s formula
is the direct result of assuming that the ﬂux
is given by a single 2-form deﬁned over the
three-dimensional domain of the body. The
fact that one and the same form is to be used
for a given location, and integrated over the
given boundary, is trivially seen to imply
(and generalize) the linear dependence of
the ﬂux on the normal to the boundary, as
described in the standard treatments.
10.2.4
Diﬀerential Expression of the Balance Law
Assuming the existence of a ﬂux form Φ,
the general balance law (10.18) can be writ-
ten as
∫
𝛽= ∫
s −∫𝜕
𝜄∗
𝜕(Φ).
(10.23)
Using Stokes’ theorem (10.11), we can
rewrite the last term as
∫𝜕
𝜄∗
𝜕(Φ) = ∫
dΦ,
(10.24)
where the dependence on 𝜕has evapo-
rated. Using this result, we write (10.23) as
∫
𝛽= ∫
s −∫
dΦ.
(10.25)
As this balance law should be valid for arbi-
trary , and because the forms 𝛽, s, and Φ

358
10 Topics in Diﬀerential Geometry
are deﬁned globally and independently of
the region of integration, we obtain
𝛽= s −dΦ.
(10.26)
This equation is known as the diﬀerential
expression of the general balance law.
10.3
Lie Groups
10.3.1
Deﬁnition
Recall that a group is a set endowed
with a binary associative internal opera-
tion, called group multiplication or group
product, which is usually indicated by sim-
ple apposition, namely, if g, h ∈then the
product is gh ∈. Associativity means that
(gh)k = g(hk), for all g, h, k ∈. Moreover,
there exists an identity element e ∈such
that eg = ge = g for all g ∈. Finally, for
each g ∈there exists an inverse g−1 ∈
such that gg−1 = g−1g = e. The identity can
be shown to be unique, and so is also the
inverse of each element of the group. If
the group operation is also commutative,
namely, if gh = hg for all g, h ∈, the group
is said to be commutative or Abelian. In
this case, it is customary to call the oper-
ation group addition and to indicate it as:
g + h. The identity is then called the zero
element and is often denoted as 0. Finally,
the inverse of g is denoted as −g. This nota-
tion is easy to manipulate as it is reminis-
cent of the addition of numbers, which is
indeed a particular case.
A subgroup of a group is a subset ⊂
closed under the group operations of mul-
tiplication and inverse. Thus, a subgroup is
itself a group.
Given two groups, 1 and 2, a group
homomorphism is a map 𝜙∶1 →2 that
preserves the group multiplication, namely,
𝜙(gh) = 𝜙(g) 𝜙(h)
∀g, h ∈1,
(10.27)
where the multiplications on the left- and
right-hand sides are, respectively, the group
multiplications of 1 and 2.
The group structure is a purely algebraic
concept, whereby nothing is assumed as far
as the nature of the underlying set is con-
cerned. The concept of a Lie group arises
from making such an assumption. More
speciﬁcally, a Lie group is a (smooth) mani-
fold with a group structure that is compat-
ible with the diﬀerential structure, namely,
such that the multiplication × →and
the inversion →are smooth maps.
A homomorphism 𝜙between two Lie
groups is called a Lie-group homomorphism
if 𝜙is C∞. If 𝜙happens to be a diﬀeo-
morphism, we speak of a Lie-group isomor-
phism. An isomorphism of a Lie group with
itself is called a Lie-group automorphism.
Let V be an n-dimensional vector space.
The collection L(V, V) of all linear oper-
ators from V to V can be considered as
a diﬀerentiable manifold. Indeed, ﬁxing a
basis in V, we obtain a global chart in ℝn2.
The collection GL(V) of all invertible linear
operators, with the operation of multipli-
cation as the composition of linear opera-
tors, can be shown to be a Lie group, known
as the general linear group of V. The gen-
eral linear group of ℝn is usually denoted
by GL(n; ℝ). Its elements are the nonsingu-
lar square matrices of order n, with the unit
matrix I acting as the group unit. Its vari-
ous Lie subgroups are known as the matrix
groups. The group operation is the usual
matrix multiplication.
10.3.2
Group Actions
Let be a group (not necessarily a Lie
group) and let X be a set (not necessarily
a diﬀerentiable manifold). We say that the

10.3 Lie Groups
359
group acts on the right on the set X if for
each g ∈there is a map Rg ∶X →X such
that: (i) Re(x) = x for all x ∈X, where e is
the group identity; (ii) Rg∘Rh = Rhg for all
g, h ∈. When there is no room for confu-
sion, we also use the notation xg for Rg(x).
Each of the maps Rg is a bijection of X.
Moreover, Rg−1 = (Rg)−1. The orbit through
x ∈X is the subset xof X consisting of
all the elements of X of the form xg, where
g ∈.
The action of on X is said to be eﬀective
if the condition Rg(x) = x for every x ∈X
implies g = e. The action is free if Rg(x) = x
for some x ∈X implies g = e. Finally, the
action is transitive if for every x, y ∈X,
there exists g ∈such that Rg(x) = y.
In a completely analogous manner, we
can say that acts on the left on X if for
each g ∈there is a map Lg ∶X →X such
that (i) Le(x) = x for all x ∈X, where e is
the group identity; (ii) Lg∘Lh = Lgh for all
g, h ∈. The order of the composition is
the essential diﬀerence between a right and
a left action. We may also use the notation
gx for Lg(x).
The notion of group action can naturally
be applied when is a Lie group. In this
instance, a case of particular interest is that
for which the set on which acts is a dif-
ferentiable manifold and the induced bijec-
tions are transformations of this manifold.
A transformation of a manifold is a dif-
feomorphism 𝜙∶→. The deﬁnition
of the action is then supplemented with a
smoothness condition. More explicitly, A
Lie group is said to act on the right on a
manifold if
1. Every element g ∈induces a
transformation Rg ∶→.
2. Rg∘Rh = Rhg, namely, (ph)g = p(hg) for
all g, h ∈and p ∈.
3. The right action R ∶× →is a
smooth map. In other words, Rg(p) is
diﬀerentiable in both variables (g and p).
With these conditions, the Lie group is
also called a Lie group of transformations
of . Just as in the general case, we have
used the alternative notation pg for Rg(p),
with p ∈, wherever convenient. A simi-
lar deﬁnition can be given for the left action
of a Lie group on a manifold.
If e is the group identity, then Re
and Le are the identity transformations
of . Indeed, because a transforma-
tion is an invertible map, every point
p ∈can be expressed as qg for some
q ∈and some g ∈. Using Prop-
erty (2) of the right action, we have
Re(p) = pe = (qg)e = q(ge) = qg = p,
with
a similar proof for the left action.
It is convenient to introduce the follow-
ing (useful, though potentially confusing)
notation. We denote the right action as a
map from × to by the symbol R.
Thus, R = R(g, p) has two arguments, one
in the group and the other in the mani-
fold. Fixing, therefore, a particular element
g in the group, we obtain a function of the
single variable x, which we have already
denoted by Rg ∶→. But we can also
ﬁx a particular element p in the manifold
and thus obtain another function of the sin-
gle variable g. We will denote this func-
tion by Rp ∶→. A similar scheme of
notation can be adopted for a left action L.
Notice that the image of Rp (respectively Lp)
is nothing but the orbit p(respectively p).
The potential for confusion arises when the
manifold happens to coincide with the
group , as described below. Whenever an
ambiguous situation arises, we will resort to
the full action function of two variables.
Recall that a Lie group is both a group
and a manifold. Thus, it is not surprising

360
10 Topics in Diﬀerential Geometry
that every Lie group induces two canon-
ical groups of transformations on itself,
one by right action and one by left action,
called,
respectively,
right
translations
and left translations of the group. They
are deﬁned, respectively, by Rg(h) = hg
and Lg(h) = gh, with g, h ∈, where the
right-hand sides are given by the group
multiplication itself. For this reason, it
should be clear that these actions are both
free (and, hence, eﬀective) and transitive.
10.3.3
One-Parameter Subgroups
A one-parameter subgroup of a Lie group 
is a diﬀerentiable curve
𝛾∶ℝ−→
t →g(t),
(10.28)
satisfying
g(0) = e,
(10.29)
and
g(t1) g(t2) = g(t1 + t2),
∀t1, t2 ∈ℝ.
(10.30)
If the group acts (on the left, say) on a
manifold , the composition of this action
with a one-parameter subgroup determines
a one-parameter group of transformations
of , namely,
𝛾t(p) = Lg(t)(p)
p ∈.
(10.31)
We know that associated with this ﬂow,
there exists a unique vector ﬁeld v𝛾. More
precisely, we have
v𝛾(p) = d𝛾t(p)
dt
||||t=0
.
(10.32)
Fixing the point p, we obtain the map Lp
from the group to the manifold. The image
of the curve 𝛾under this map is obtained by
composition as
t →Lp(g(t)) = L(g(t), p) = Lg(t)(p) = 𝛾t(p),
(10.33)
where we have used (10.31). In other
words, the image of the curve 𝛾(deﬁning
the one-parameter subgroup) by the map
Lp is nothing but the integral curve of the
ﬂow passing through p. By deﬁnition of
derivative of a map between manifolds, we
conclude that the tangent g to the one-
parameter subgroup 𝛾at the group identity
e is mapped by Lp∗to the vector v𝛾(p)
v𝛾(p) = (Lp∗
)
e g.
(10.34)
This means that a one-parameter subgroup
g(t) appears to be completely characterized
by its tangent vector g at the group identity.
We will shortly conﬁrm this fact more fully.
The vector ﬁeld induced on by a one-
parameter subgroup is called the funda-
mental vector ﬁeld associated with the cor-
responding vector g at the group identity.
Let us now identify the manifold with
the group itself. In this case, we have,
as already discussed, two canonical actions
giving rise to the left and right translations
of the group. We want to reinterpret (10.34)
in this particular case. For this purpose, and
to avoid the notational ambiguity alluded to
above, we restore the fully ﬂedged notation
for the action as a function of two variables.
We thus obtain
v𝛾(h) =
(𝜕L(g, h)
𝜕g
)
g=e
g.
(10.35)
Notice that, somewhat puzzlingly, but con-
sistently, this can also be written as
v𝛾(h) = (Rh∗
)
e g.
(10.36)
Thus, when deﬁning the action of a one-
parameter subgroup from the left, it is the

10.4 Fiber Bundles
361
right action whose derivative delivers the
corresponding vector ﬁeld, and vice versa.
10.3.4
Left- and Right-Invariant Vector Fields on a
Lie Group
A vector ﬁeld v ∶→Tis said to be left
invariant if
v(Lgh) = Lg∗v(h),
∀g, h ∈.
(10.37)
In other words, vectors at one point are
dragged to vectors at any other point by
the derivative of the appropriate left trans-
lation. A similar deﬁnition, but replacing
L with R, applies to right-invariant vector
ﬁelds.
A vector ﬁeld is left invariant if, and only
if,
v(g) =
(
Lg∗
)
e v(e),
∀g ∈.
(10.38)
Another way of expressing this result is
by saying that there exists a one-to-one
correspondence between the set of left- (or
right-) invariant vector ﬁelds on and the
tangent space Teat the group identity.
This correspondence is linear. Moreover,
one can show that the Lie bracket of two
left- (right-) invariant vector ﬁelds is itself
left (right) invariant. The set 𝔤of left-
invariant vector ﬁelds (or, equivalently, the
tangent space Te) with the Lie bracket
operation is called the Lie algebra of the
group . From an intuitive point of view,
the elements of the Lie algebra of a Lie
group represent inﬁnitesimal approxima-
tions, which Sophus Lie himself called
inﬁnitesimal generators of the elements
of the group. Although the inﬁnitesimal
generators are in principle commutative
(sum of vectors), the degree of noncom-
mutativity of the actual group elements is
captured, to ﬁrst order, by the Lie bracket.
10.4
Fiber Bundles
10.4.1
Introduction
Fiber bundles are diﬀerentiable manifolds
with extra structure. Its points have, as it
were, a double allegiance – not only to the
manifold itself but also to a smaller entity
called a ﬁber. We have already encountered
two important instances of ﬁber bundles
that clearly exhibit this feature: the tangent
and cotangent bundles of a manifold. The
property of belonging to a speciﬁc ﬁber is,
in those two examples, represented by the
existence of a projection map. Given a tan-
gent vector or a covector, these maps tell us
to which point of the base manifold they are
attached. Moreover, we have found in those
two cases that, once a chart (, 𝜙) is chosen
in the original manifold, a chart for the bun-
dle can be constructed. Eﬀectively, there-
fore, the part of the bundle sitting above 
becomes assimilated to the Cartesian prod-
uct of and the typical ﬁber. In the case of
the tangent bundle, for example, the chunk
sitting above is 𝜏−1(), while the typi-
cal ﬁber is ℝm. These two properties (exis-
tence of a projection and local equivalence
to a Cartesian product) are the two essential
ingredients of the deﬁnition of a general
ﬁber bundle. A third ingredient consists
of the permitted transition functions along
the ﬁbers, a degree of freedom that is con-
trolled by a given group of transformations
of the typical ﬁber.
10.4.2
Deﬁnition
A ﬁber bundle with base manifold , typical
ﬁber manifold and structure group , is a
manifold and a smooth surjective bundle-
projection map 𝜋∶→such that there

362
10 Topics in Diﬀerential Geometry
exists an open covering 𝛼of and respec-
tive local trivializations
𝜓𝛼∶𝜋−1(𝛼) −→𝛼× ,
(10.39)
with the property 𝜋= pr1∘𝜓𝛼, as illustrated
in the following commutative diagram:
𝜋−1(𝛼)

𝜓𝛼
𝛼× F

pr1
𝛼


𝜋
(10.40)
Moreover, as illustrated in Figure 10.1,
whenever b ∈𝛼
⋂𝛽≠∅, the transition
maps
̃𝜓𝛽,𝛼(b) = ̃𝜓𝛼,b∘̃𝜓−1
𝛽,b belong to the
structure group and depend smoothly on
position throughout the intersection.
Consider now, for the same , , ,
𝜋, and , a diﬀerent open covering 𝛽
with local trivializations 𝜙𝛽. We say that it
deﬁnes the same ﬁber bundle as before if,
on nonvanishing intersections, the transi-
tion maps ̃𝜓𝛼,b∘̃𝜙−1
𝛽,b belong to the structure
group and depend smoothly on position
b throughout the intersection. The two
trivializations are said to be compatible.
In this sense, we can say that the union of
the two trivialization coverings becomes
itself a new trivialization covering of the
ﬁber bundle. When there is no room for
confusion, a ﬁber bundle is denoted as a
pair (, 𝜋) indicating just the total space
and the projection. An alternative notation
is 𝜋∶→. A more complete notation
would be (, 𝜋, , , ).
The fundamental existence theorem of
ﬁber bundles states that, given the mani-
folds and and a Lie group acting eﬀec-
tively to the left on , and given, moreover,
an open covering 𝛼of and a smooth
assignment of an element of to each point
in every nonvanishing intersection of the
covering, there exists a ﬁber bundle (, 𝜋)
with local trivializations based on that cov-
ering and with the assigned elements of 
as transition maps. Furthermore, any two
bundles with this property are equivalent.
An important application of the funda-
mental existence theorem is that, given a
bundle (, 𝜋, , , ), we can associate to
it other bundles with the same base man-
ifold and the same structure group, but
with diﬀerent typical ﬁber ′, in a precise
way. Indeed, we can choose a trivialization
covering of the given bundle, calculate the
transition maps, and then deﬁne the asso-
ciated bundle (′, 𝜋′, , ′, ), modulo an
equivalence, by means of the assertion of
the fundamental theorem. A case of partic-
ular interest is that in which the new ﬁber
is identiﬁed with the structure group. This
gives rise to the so-called associated princi-
pal bundle.
B
B
˜
F
ψα
π
ψβ,α (b)
Uα
Uβ
Uα
Uβ
pr1
b
b
ψβ
Figure 10.1
A general ﬁber bundle.

10.4 Fiber Bundles
363
10.4.3
Simultaneity in Classical Mechanics
An important example of the use of
ﬁber bundles arises in the notion of
classical space–time. Starting from a four-
dimensional manifold of events, Classical
Physics assumes that all observers agree
on whether or not two events happened
simultaneously, regardless of their loca-
tions. As a consequence, a time-projection
operator arises naturally in this context.
As a result, a Physics that abides by the
principle of absolute simultaneity must of
necessity be formulated upon a space–time
manifold that has the structure of a ﬁber
bundle, the base manifold being a one-
dimensional manifold. The typical ﬁber,
representing space, is a three-dimensional
manifold. In the Galilean formulation, this
typical ﬁber is the Euclidean space 𝔼3. The
structure group is the group of Galilean
transformations of 𝔼3 (those preserving
Euclidean length). An observer is a bundle
trivialization. From this point of view, it can
be claimed that the theory of Relativity is
simpler than its Classical counterpart from
at least the following point of view: the
structure of relativistic space–time is less
involved than that of Galilean space–time.
The extra structure in the latter is provided
by the notion of absolute simultaneity. In
contrast, in Relativity, space–time is just a
four-dimensional manifold endowed with
a special metric structure.
10.4.4
Adapted Coordinate Systems
The total space of a smooth ﬁber bundle
(, 𝜋, , , ) is a diﬀerentiable manifold
in its own right and, as such, can sustain a
whole class of equivalent atlases. Among
these atlases, however, there are some
that enjoy the property of being in some
sense adapted to the ﬁbered structure of 
seen as a ﬁber bundle. When working in
coordinates, these adapted atlases or, more
particularly, the corresponding adapted
coordinate charts are used almost exclu-
sively. The construction of these special
charts mimics the construction of charts
in a product manifold, by taking advantage
of the local triviality of ﬁber bundles. Thus,
let p be a point in and let {, 𝜓} be a
local trivialization, namely, a diﬀeomor-
phism 𝜓∶𝜋−1() →× , such that
𝜋(p) ∈. Without loss of generality, we
may assume that the trivialization is subor-
dinate to a chart in the base manifold in
the sense that is the domain of a chart
in with coordinates xi (i = 1, … , m).
Moreover, because the typical ﬁber is
itself a diﬀerentiable manifold of dimen-
sion n, the point f = pr2(𝜓(p)) belongs to
some chart of with domain ⊂and
coordinates u𝛼(𝛼= 1, … , n). The induced
adapted coordinate chart in is the map
u𝜓∶𝜓−1(× ) →ℝm+n whose value at
p is (x1, … , xm, u1, … , un). The proof that
in this way an atlas can be constructed
in is straightforward. It is implicitly
assumed that the diﬀerential structure of 
is compatible with the one induced by the
adapted atlases.
10.4.5
The Bundle of Linear Frames
The bundle of linear frames, F, of a base
n-dimensional manifold can be deﬁned
constructively in the following way. At
each point b ∈we form the set Fb
of all ordered n-tuples {e}b = (e1, … , en)
of linearly independent vectors ei in Tb,
namely, the set of all bases of Tb. Our total
space will consist of all ordered pairs of the
form (b, {e}b) with the obvious projection
onto . The pair (b, {e}b) is called a linear
frame at b. Following a procedure identical

364
10 Topics in Diﬀerential Geometry
to the one used for the tangent bundle, we
obtain that each basis {e}b is expressible
uniquely as
ej = pi
j
𝜕
𝜕xi
(10.41)
in a coordinate system xi, where {pi
j} is a
nonsingular matrix. We conclude that the
typical ﬁber in this case is GL(n; ℝ). But so
is the structure group. Indeed, in another
coordinate system, yi, we have
ej = qi
j
𝜕
𝜕yi ,
(10.42)
where
qi
j = 𝜕yi
𝜕xm pm
j = ai
m pm
j .
(10.43)
This is an instance of a principal ﬁber
bundle, namely, a ﬁber bundle whose typ-
ical ﬁber and structure group coincide. The
action of the group on the typical ﬁber is
the natural left action of the group on itself.
One of the interesting features of a principal
bundle is that the structure group has also
a natural right action on the bundle itself,
and this property can be used to provide an
alternative deﬁnition of principal bundles,
which we shall pursue later. In the case of
F, for example, the right action is deﬁned,
in a given coordinate system xi, by
Ra{e} = pk
iai
j
𝜕
𝜕xk ,
j = 1, … , n,
(10.44)
which sends the basis (10.41) at b to another
basis at b, that is, the action is ﬁber preserv-
ing. One can verify that this deﬁnition of
the action is independent of the system of
coordinates adopted. The principal bundle
of linear frames of a manifold is associated
to all the tensor bundles, including the tan-
gent and the cotangent bundles, of the same
manifold. By a direct application of the fun-
damental existence theorem, we know that
the associated principal bundle is deﬁned
uniquely up to an equivalence. Many prop-
erties of bundles can be better understood
by working ﬁrst on the associated principal
bundle.
10.4.6
Bodies with Microstructure
The modeling of complex materials, such as
liquid crystals and granular media, requires
the introduction of extra kinematic degrees
of freedom. The matrix or macromedium
is, in this case, an ordinary manifold that
becomes the base manifold of a ﬁber
bundle whose typical ﬁber represents the
micromedium. In the case of granular
media (such as concrete), each grain is
assumed to undergo a homogeneous defor-
mation. It is natural, therefore, to regard
each of the smoothly distributed grains as
the collection of all possible local bases
of the tangent space to the base manifold.
In other words, the granular medium is
represented by the linear frame bundle of
the macromedium.
10.4.7
Principal Bundles
The existence of a free right action on
a manifold is strong enough to provide
an independent deﬁnition of a principal
ﬁber bundle which, although equivalent
to the one already given, has the merit
of being independent of the notion of
transition maps. Moreover, once this more
elegant and constructive deﬁnition has
been secured, a subsidiary deﬁnition of the
associated (nonprincipal) bundles becomes
available, again without an explicit mention
of the transition maps. Finally, this more
abstract deﬁnition brings out intrinsically
the nature and meaning of the associated
bundles.

10.4 Fiber Bundles
365
Let be a diﬀerentiable manifold (the
total space) and a Lie group (the structure
group), and let act freely to the right on .
This means that there exists a smooth map
Rg ∶× −→
(p, g) →Rgp = pg,
(10.45)
such that, for all p ∈and all g, h ∈, we
have
Rghp = RhRgp = pgh,
Rep = p,
(10.46)
where e is the group identity. The fact that
the action is free means that if, for some p ∈
and some g ∈, Rgp = p, then necessar-
ily g = e. Deﬁne now the quotient = ∕
and check that is a diﬀerentiable mani-
fold and that the canonical projection 𝜋P ∶
→∕is diﬀerentiable. The set 𝜋−1
P (b) is
called the ﬁber over b ∈.
Recall that an element of the quotient
= ∕is, by deﬁnition of quotient, an
equivalence class in by the action of the
group . In other words, each element b
of the quotient (namely, of the base man-
ifold ) can be regarded as representing
an orbit. The projection map assigns to
each element of the orbit to which it
belongs. The ﬁber over b consists of all the
elements of that belong to the speciﬁc
orbit represented by b.
To complete the deﬁnition of a principal
bundle, we need only to add the condition
that be locally trivial, namely, that for
each b ∈, there exists a neighborhood
⊂such that 𝜋−1
P () is isomorphic to
the product × . More precisely, there
exists a ﬁber-preserving diﬀeomorphism
𝜓∶𝜋−1
P () −→× 
p →(b, ̃𝜓b),
(10.47)
where b = 𝜋P(p), with the additional prop-
erty that it must be consistent with the group
action, namely (see Figure 10.2),
̃𝜓b(pg) = ̃𝜓b(p)g,
∀p ∈𝜋−1
P (), g ∈.
(10.48)
This completes the deﬁnition of the prin-
cipal bundle. The right action is ﬁber pre-
serving and every ﬁber is diﬀeomorphic to
. Moreover, every ﬁber coincides with an
orbit of the right action of .
10.4.8
Associated Bundles
The concept of associated bundle has
already been deﬁned and used to introduce
the notion of the principal bundle associ-
ated with any given ﬁber bundle. On the
other hand, in the preceding section, we
have introduced an independent deﬁnition
of principal bundles by means of the idea
of a right action of a group on a given total
manifold. We want now to show that this
line of thought can be pursued to obtain
B
B
b
b
p
P
πP
pr1
ψb
g
g
G
˜
Figure 10.2
The group consistency condition.

366
10 Topics in Diﬀerential Geometry
another view of the collection of all (non-
principal) ﬁber bundles associated with a
given principal bundle.
As a more or less intuitive motivation for
this procedure, it is convenient to think of
the example of the principal bundle of lin-
ear frames Fof a manifold . We already
know that this bundle is associated to the
tangent bundle T. Consider now a pair
(f , v), where f ∈Fand v ∈T, such that
𝜋P(f ) = 𝜋(v) = b. In other words, f and v
represent, respectively, a basis and a vector
of the tangent space at some point b ∈.
We can, therefore, identify v with its com-
ponents on the linear frame f , namely, with
an element of the typical ﬁber (ℝn) of T.
If we consider now a pair (̂f , v), where v
is the same as before but ̂f is a new lin-
ear frame at b, the corresponding element
of the typical ﬁber representing the same
vector v changes. More explicitly, with an
obvious notation, if ̂fj = ai
jfi, then vi = ai
ĵvj
or ̂vi = (a−1)i
jvj. We conclude that to rep-
resent the same object under a change of
frame, there needs to be some kind of com-
pensatory action in the change of the com-
ponents. The object itself (in this case, the
tangent vector) can be identiﬁed with the
collection (or equivalence class) of all pairs
made up of a frame and a matrix related
in this compensatory way. In terms of the
group actions on the typical ﬁbers, if ̂f =
Raf , then the representative r of the vector
v in ℝn changes according to ̂r = La−1r. We
may, therefore, think of a vector as an equiv-
alence class of elements of the Cartesian
product × Rn, corresponding to the fol-
lowing equivalence relation: (g, r) ∼(̂g, ̂r) if,
and only if, there exists a ∈such that ̂g =
ga and ̂r = La−1r.
With the above motivation in mind, the
following construction of a ﬁber bundle
associated to a given principal bundle
will seem less artiﬁcial than it otherwise
would. We start from the principal bundle
(, 𝜋P, , , ) and a manifold , which we
want to construe as the typical ﬁber of a
new ﬁber bundle (, 𝜋, , , ) associated
with . For this to be possible, we need
to have an eﬀective left action of on ,
which we assume to have been given. To
start oﬀ, we form the Cartesian product
× and notice that the structure group
acts on it with a right action induced by
its right action on and its left action on
. To describe this new right action, we
will keep abusing the notation in the sense
that we will use the same symbols for all the
actions in sight, because the context should
make clear which action is being used in
each particular expression. Let (p, f ) be
an element of the product × , and let
a ∈. We deﬁne the eﬀective right action
Ra(p, f ) = (Rap, La−1 f ).
(10.49)
The next step toward the construction of
the associated bundle with typical ﬁber 
consists of taking the quotient space gen-
erated by this action. In other words, we
want to deal with a set whose elements are
equivalence classes in × by the equiva-
lence relation “(p1, f1) ∼(p2, f2) if, and only
if, there exists a ∈such that (p2, f2) =
Ra(p1, f1).” The motivation for this line of
attack should be clear from the introduc-
tory remarks to this section. Recalling that
the right action of on is ﬁber pre-
serving, it becomes obvious that all the
pairs (p, f ) in a given equivalence class have
ﬁrst components p with the same projec-
tion 𝜋P(p) on . This means that we have
a perfectly well-deﬁned projection 𝜋in the
quotient space , namely, 𝜋∶→is a
map that assigns to each equivalence class
the common value of the projection of
the ﬁrst component of all its constituent
pairs.
Having a projection, we can now deﬁne
the ﬁber of over b ∈naturally as

10.5 Connections
367
𝜋−1(b). We need to show now that each
such ﬁber is diﬀeomorphic to the puta-
tive typical ﬁber . More precisely, we
want to show that for each local trivial-
ization (, 𝜓) of the original principal
bundle , we can also construct a local
trivialization of 𝜋−1(), namely, a dif-
feomorphism
𝜌∶𝜋−1() →× .
To
understand how this works, let us ﬁx a
point b ∈and recall that, given the local
trivialization (, 𝜓), the map ̃𝜓b provides
us with a diﬀeomorphism of the ﬁber
𝜋−1
P (b) with . We now form the product
map of ̃𝜓b with the identity map of ,
namely,
( ̃𝜓b, id) ∶𝜋−1
P (b) × →× .
Each equivalence class by the right action
(10.49) is mapped by the product map
( ̃𝜓b, id)
into
an
orbit,
as
shown
in
Figure 10.3.
These orbits do not intersect with each
other. Moreover, they can be seen as
graphs of single-valued -valued functions
of . Therefore, choosing any particu-
lar value g ∈, we see that these orbits
can be parameterized by . This pro-
vides the desired one-to-one and onto
relation between the ﬁber 𝜋−1(b) and
the manifold , which can now legiti-
mately be called the typical ﬁber of . To
complete the construction of the desired
ﬁber bundle, we need to guarantee that
the ﬁberwise isomorphism that we have
just constructed depends diﬀerentiably
on b, a requirement that we assume
fulﬁlled.
10.5
Connections
10.5.1
Introduction
All the ﬁbers of a ﬁber bundle are, by deﬁ-
nition, diﬀeomorphic to each other. In the
absence of additional structure, however,
there is no canonical way to single out a par-
ticular diﬀeomorphism between ﬁbers. In
the case of a product bundle, for example,
such a special choice is indeed available
because of the existence of the second pro-
jection map onto the typical ﬁber. In this
extreme case, we may say that we are in the
presence of a canonical distant parallelism
in the ﬁber bundle. An equivalent way to
describe this situation is by saying that we
have a canonical family of nonintersect-
ing smooth cross sections such that each
point in the ﬁber bundle belongs to one, and
only one, of them. In a general ﬁber bundle
we can only aﬀord this luxury noncanon-
ically and locally. A connection on a ﬁber
bundle is, roughly speaking, an additional
structure deﬁned on the bundle that per-
mits to establish intrinsic ﬁber diﬀeomor-
phisms for ﬁbers lying along curves in the
base manifold. In other words, a connec-
tion can be described as a curve-dependent
parallelism. Given a connection, it may so
happen that the induced ﬁber parallelisms
turn out to be curve independent. A quan-
titative measure of this property or the lack
G
F
Figure 10.3
Images of equivalence classes.

368
10 Topics in Diﬀerential Geometry
thereof is provided by the vanishing, or oth-
erwise, of the curvature of the connection.
10.5.2
Ehresmann Connection
Consider the tangent bundle Tof the
total space of an arbitrary ﬁber bundle
(, 𝜋, , , ), and denote by 𝜏C ∶T→
its natural projection. If the dimensions of
the base manifold and the typical ﬁber
are, respectively, m and n, the dimen-
sion of is m + n, and the typical ﬁber
of (T, 𝜏C) is ℝm+n, with structure group
GL(m + n; ℝ). At each point c ∈, the tan-
gent space Tchas a canonically deﬁned
vertical subspace Vc, which can be identi-
ﬁed with the tangent space Tc𝜋(c) to the
ﬁber of at c. The dimension of Vc is n. A
vector in Tcbelongs to the vertical sub-
space Vc (or is vertical) if, and only if, its
projection by 𝜋∗is the zero vector of T𝜋(c).
If a vector in Tcis not vertical, there is no
canonical way to assign to it a vertical com-
ponent. It is this deﬁciency, and only this
deﬁciency, that the Ehresmann connection
remedies. Formally, an Ehresmann connec-
tion consists of a smooth horizontal distri-
bution in . This is a smooth assignment
to each point c ∈of an (m-dimensional)
subspace Hc ⊂Tc(called the horizontal
subspace at c), such that
Tc= Hc ⊕Vc.
(10.50)
In this equation, ⊕denotes the direct sum
of vector spaces. Each tangent vector u ∈
Tcis, accordingly, uniquely decompos-
able as the sum of a horizontal part h(u)
and a vertical part v(u). A vector is hor-
izontal, if its vertical part vanishes. The
only vector that is simultaneously horizon-
tal and vertical is the zero vector. As Hc and
T𝜋(c)have the same dimension (m), the
restriction 𝜋∗|Hc ∶Hc →T𝜋(c), is a vector-
space isomorphism. We denote its inverse
by Γc. Thus, given a vector v tangent to
the base manifold at a point b ∈, there
is a unique horizontal vector Γcv at c ∈
𝜋−1({b}) such that 𝜋∗(Γcv) = v. This unique
vector is called the horizontal lift of v to c.
In particular, Γc
(𝜋∗(u)
)
= Γc
(𝜋∗(h(u))
)
=
h(u). These ideas are schematically illus-
trated in Figure 10.4.
10.5.3
Parallel Transport along a Curve
Let
𝛾∶(−𝜖, 𝜖) −→
(10.51)
be a smooth curve in the base manifold of
the ﬁber bundle (, 𝜋), and let c ∈𝛾(0) be a
point in the ﬁber at 𝛾(0). A horizontal lift of
u
h(u)
ν(u)
C
B
b
c
Γc
π∗ (u)
Figure 10.4
Ehresmann connection.

10.5 Connections
369
𝛾through c is deﬁned as a curve
̂𝛾∶(−𝜖, 𝜖) −→,
(10.52)
such that
̂𝛾(0) = c,
(10.53)
𝜋(̂𝛾(t)) = 𝛾(t),
∀t ∈(−𝜖, 𝜖),
(10.54)
and
̂𝛾′(t) ∈Ĥ𝛾(t),
∀t ∈(−𝜖, 𝜖),
(10.55)
where a prime denotes the derivative with
respect to the curve parameter t. A hori-
zontal lift is thus a curve that projects onto
the original curve and, moreover, has a hor-
izontal tangent throughout.
Consider the “cylindrical” subbundle 𝛾∗
obtained by pulling back the bundle to the
curve 𝛾or, less technically, by restricting the
base manifold to the curve 𝛾. The tangent
vector ﬁeld of 𝛾has a unique horizontal lift
at each point of this bundle. In other words,
the curve generates a (horizontal) vector
ﬁeld throughout this restricted bundle. By
the fundamental theorem of the theory of
ordinary diﬀerential equations (ODEs), it
follows that, at least for small enough 𝜖,
there is a unique horizontal lift of 𝛾through
any given point in the ﬁber at 𝛾(0), namely,
the corresponding integral curve of the hor-
izontal vector ﬁeld. We conclude, therefore,
that the horizontal lift of a curve through a
point in a ﬁber bundle exists and is locally
unique. As the horizontal curve issuing
from c cuts the various ﬁbers lying on 𝛾,
the point c is said to undergo a parallel
transport relative to the given connection
and the given curve. Thus, given a point
c ∈and a curve 𝛾through 𝜋(c) ∈, we
obtain a unique parallel transport of c along
𝛾by solving a system of ODEs (so as to
travel always horizontally). These concepts
are illustrated schematically in Figure 10.5
10.5.4
Connections in Principal Bundles
A
connection
in
a
principal
bundle
(, 𝜋, , , ) is an Ehresmann connec-
tion that is compatible with the right action
Rg of on , namely,
(Rg)∗(Hp) = HRgp,
∀g ∈,
p ∈.
(10.56)
This condition can be stated verbally as fol-
lows: the horizontal distribution is invari-
ant under the group action.
Recall that the group acts freely (to the
right) on . Consequently, the fundamental
vector ﬁeld vg associated with any nonzero
vector g in the Lie algebra 𝔤of does not
vanish anywhere on . Moreover, because
the action of maps ﬁbers into themselves,
0
B
c
𝛾
Figure 10.5
Parallel transport
along a curve.

370
10 Topics in Diﬀerential Geometry
the fundamental vector ﬁelds are all verti-
cal. The correspondence between vectors in
the Lie algebra and tangent vectors to the
ﬁber at any point is clearly linear and one-
to-one. As the dimension of is equal to
the dimension of each ﬁber, we conclude
that the map 𝔤→Vp given by g →vg(p) is
a linear isomorphism between the Lie alge-
bra and each of the vertical subspaces of the
principal bundle.
Let v ∈Tbe any tangent vector to the
ﬁber bundle. A connection Γ assigns to it
a unique vertical part and, as we have just
seen, the action of the group assigns to this
vertical part an element of the Lie algebra
𝔤. This means that we have a well-deﬁned
linear map
𝜔∶T−→𝔤,
(10.57)
associated with a given connection in a
principal bundle. This map can be regarded
as a Lie-algebra-valued 1-form. It is called
the connection form associated with Γ.
10.5.5
Distributions and the Theorem of
Frobenius
We have mentioned in Section 10.5.1 the
notion of curvature of a connection as an
indication of how the parallel transport of
an entity along a curve depends on the
curve itself. Before giving a precise def-
inition of this concept, however, it may
prove useful to introduce the more gen-
eral concept of involutivity of a distribu-
tion. The reason for this digression is that
a connection can always be regarded as a
(horizontal) distribution. A k-dimensional
distribution of an m-dimensional manifold
(with m ≥k) is deﬁned as a smooth
assignment of a k-dimensional subspace x
of the tangent space Txto each point x ∈
. A fundamental question in the theory of
distributions is whether or not there exist
integral embedded submanifolds, namely,
embedded submanifolds of dimension k
whose tangent space at each point x coin-
cides with x.
An embedded submanifold of dimension
k is deﬁned as a subset ⊂such that
for each point s ∈one can ﬁnd a chart of
with coordinates xi (i = 1, … , m) such
that s belongs to this chart and such that the
intersection of the set with the chart coin-
cides with the set obtained by keeping the
last m −k coordinates constant. This idea
becomes clear if one thinks of the particular
case of ℝ2 as embedded in ℝ3 with coordi-
nates x, y, z. The equation of the embedded
submanifold ℝ2 can be given as z = 0.
In some sense, the question of existence
of integral submanifolds can be regarded as
a generalization to many dimensions of the
question of integrability of systems of ODE,
which would correspond to the case k = 1,
namely, to the case in which the subspaces
of the distribution are mere lines. While in
the particular case k = 1, we are assured,
by the fundamental theorem of ODEs, of
the (local) existence of integral curves, the
answer in the general case k > 1 is usu-
ally negative. A k-dimensional distribution
is said to be completely integrable if for
each point of the manifold there exists
a chart xi (i = 1, … , m) such that each set
obtained by keeping the last n −k coordi-
nates thereat constant is an integral sub-
manifold (of dimension k). Assume that a
completely integrable distribution has been
given. Then, according to our deﬁnition, the
ﬁrst k natural vectors of the local coordi-
nate system just described belong to the
distribution and constitute a basis of x at
each point x in the chart. Any vector ﬁelds
v𝛼(𝛼= 1, … , k) with this property (of con-
stituting a basis of the distribution) are said
to span the distribution. Within the chart,
any vector ﬁelds v𝛼(𝛼= 1, … , k) that span

10.5 Connections
371
the distribution must be expressible, there-
fore, as
v𝛼= v 𝛽
𝛼
𝜕
𝜕x𝛽,
(10.58)
where the summation convention applies
for Greek indices within the range 1, … , k.
We now calculate the Lie bracket of any pair
of the spanning vectors as
[v𝛼, v𝛽] = v 𝜌
𝛼
𝜕v 𝜎
𝛽
𝜕x𝜌
𝜕
𝜕x𝜎−v 𝜎
𝛽
𝜕v 𝜌
𝛼
𝜕x𝜎
𝜕
𝜕x𝜌.
(10.59)
Notice that, in calculating the Lie brackets,
we have used the fact that the components
of the vectors v𝛼vanish on the natural base
vectors 𝜕∕𝜕xi with i > k. Moreover, because
the given vectors are linearly independent,
the matrix with entries v 𝛽
𝛼
is nonsingu-
lar. Inverting, therefore, (10.58), we can
express the natural base vectors 𝜕∕𝜕x𝛼(𝛼=
1, … , k) in terms of the vectors v𝛽, with
the result that the Lie brackets are them-
selves linear combinations of these vectors,
namely, there exist scalars C𝛾
𝛼𝛽such that
[v𝛼, v𝛽] = C𝛾
𝛼𝛽v𝛾.
(10.60)
A distribution with this property (namely,
that the Lie bracket of any two vector ﬁelds
in the distribution is also in the distri-
bution) is said to be involutive. We have
proven, therefore, that every completely
integrable distribution is involutive. The
converse of this result (that is, that every
involutive distribution is completely inte-
grable) is also true, and is the content of
the theorem of Frobenius, whose proof we
omit.
10.5.6
Curvature
Suppose that we draw through a point b of
the base manifold a small closed curve 𝛾.
If we now choose a point p in the ﬁber on b,
we have learned that there exists a unique
horizontal lift ̃𝛾, namely, a horizontal curve
containing p and projecting on 𝛾. Is this
curve closed? To clarify the meaning of this
question and its possible answer, recall that
a connection on a principal ﬁber bundle
is a special case of a distribution, which
we have called horizontal (the dimension
of the horizontal distribution equals the
dimension of the base manifold and is thus
strictly smaller than the dimension of the
ﬁber bundle, assuming that the typical ﬁber
is of dimension greater than zero). Clearly,
if the horizontal distribution is involutive,
any horizontal lift of a small curve in the
base manifold will lie entirely on an inte-
gral surface and, therefore, will be closed.
This observation suggests that a measure of
the lack of closure of the horizontal lift of
closed curves is the fact that the Lie bracket
between horizontal vector ﬁelds has a verti-
cal component. We want to see now how to
extract this information from the connec-
tion itself. More particularly, because a con-
nection is speciﬁed by its connection form
𝜔, we want to extract this information from
𝜔alone.
Consider two horizontal vector ﬁelds u
and v. Let us evaluate the 2-form1) d𝜔on
this pair as
⟨d𝜔| u ∧v⟩= u (⟨𝜔| v⟩) −v (⟨𝜔| u⟩)
−⟨𝜔| [u, v]⟩,
(10.61)
which, in view of the fact that u and v are
assumed to be horizontal, yields
⟨d𝜔| u ∧v⟩= −⟨𝜔| [u, v]⟩.
(10.62)
The right-hand side of this equation will
vanish if, and only if, the Lie bracket is hor-
izontal. This means that we have found a
1) Notice that this is a Lie-algebra-valued diﬀeren-
tial form.

372
10 Topics in Diﬀerential Geometry
way to extract the right information from
𝜔by just taking its exterior derivative and
applying it to two horizontal vector ﬁelds.
Notice, however, that d𝜔can be applied to
arbitrary pairs of vector ﬁelds, not neces-
sarily horizontal. To formalize this point,
given a connection, we deﬁne the exterior
covariant derivative D𝛼of an r-form 𝛼as
the (r + 1)-form given by
⟨D𝛼| U1 ∧· · · ∧Ur+1⟩
= ⟨d𝛼| h(U1) ∧· · · ∧h(Ur+1)⟩, (10.63)
where h(.) denotes the horizontal compo-
nent of a vector. Accordingly, we deﬁne the
curvature 2-form Ω of a connection 𝜔on a
principal ﬁber bundle as
Ω = D𝜔.
(10.64)
10.5.7
Cartan’s Structural Equation
Our deﬁnition of curvature, by using both
the connection 1-form and the horizontal
projection map, is a hybrid that mixes both
(equivalent) deﬁnitions of a connection. It
is possible, on the other hand, to obtain
an elegant formula that involves just the
connection 1-form. This formula, known as
Cartan’s structural equation, reads
Ω = d𝜔+ 1
2 [𝜔, 𝜔],
(10.65)
or, more precisely, for any two vectors u and
v at a point2) of the frame bundle,
⟨Ω | U ∧V⟩= ⟨d𝜔| U ∧V⟩
+1
2 [𝜔(U), 𝜔(V)].
(10.66)
2) Notice that this formula is valid pointwise,
because the Lie bracket on the right-hand side is
evaluated in the Lie algebra, not in the manifold.
The proof of this formula, whose details
we omit, is based on a careful examination
of three cases: (i) u and v are horizontal,
whereby the formula is obvious; (ii) u is hor-
izontal and v is vertical, in which case one
can extend them, respectively, to a horizon-
tal and a fundamental (vertical) vector ﬁeld;
(iii) u and v are both vertical, in which case
they can both be extended to fundamental
ﬁelds.
10.5.8
Bianchi Identities
Unlike the ordinary exterior derivative d,
the operator D (of exterior covariant dif-
ferentiation) is not necessarily nilpotent,
namely, in general, D2 ≠0. Therefore, there
is no reason to expect that DΩ, which is
equal to D(D𝜔), will vanish identically. But,
in fact, it does. To see that this is the case,
notice that, by deﬁnition of D, we need only
verify the vanishing of DΩ on an arbitrary
triple of horizontal vectors. It can be shown
that
DΩ = 0.
(10.67)
In terms of components, we obtain diﬀer-
ential identities to be satisﬁed by any cur-
vature form. They are known as the Bianchi
identities.
10.5.9
Linear Connections
A connection on the bundle of linear
frames Fis called a linear connection on
. Among principal bundles, the bundle
of linear frames occupies a special posi-
tion for various reasons. In the ﬁrst place,
the bundle of linear frames is canonically
deﬁned for any given base manifold .
Moreover, the associated bundles include
all the tensor bundles, thus allowing for
a uniﬁed treatment of all such entities.

10.5 Connections
373
Another way to express this peculiar fea-
ture of the bundle of linear frames is that,
whereas the quantities parallel-transported
along curves in a general principal bundle
are of a nature not necessarily related
to the base manifold, in the case of the
bundle of linear frames, the quantities
transported are precisely the very frames
used to express the components of vectors
and forms deﬁned on the base manifold.
An elegant manifestation of this property
is the existence of a canonical 1-form that
ties everything together. A direct conse-
quence of the existence of this 1-form is
the emergence of the idea of the torsion
of a connection. We start the treatment of
linear connections by lingering for a while
on the deﬁnition of the canonical 1-form.
10.5.10
The Canonical 1-Form
Given a tangent vector v ∈Txat a point x
in the base manifold, and a point p ∈Fxin
the ﬁber over x, and recalling that p consists
of a frame (or basis) {e1, … , em} of Tx, we
can determine uniquely the m components
of v in this frame, namely,
v = vaea.
(10.68)
In other words, at each point p ∈F, we
have a well-deﬁned nonsingular linear
map3)
u(p) ∶T𝜋(p)−→ℝm.
(10.69)
The canonical 1-form 𝜃on Fis deﬁned as
𝜃(V) = u(p)∘𝜋∗(V),
∀V ∈Tp(F).
(10.70)
Note that this is an ℝm-valued form. The
canonical 1-form of the frame bundle is a
3) This map is, in fact, an alternative deﬁnition of a
linear frame at a point of a manifold .
particular case of a more general construct
known as a soldering form.
It may prove instructive to exhibit
the canonical form in components. Let
x1, … , xm be a local coordinate system on
⊂. Every frame {e1, … , em} at x ∈
can be expressed uniquely by means of a
nonsingular matrix with entries xi
j as
ea = xi
a
𝜕
𝜕xi .
(10.71)
This means that the m + m2 functions
{xi, xi
a} constitute a coordinate system for
the linear frame bundle 𝜋−1(). We call
it the coordinate system induced by xi.
The projection map 𝜋∶F→has the
coordinate representation
(xi, xi
a) →𝜋(xi, xi
a) = (xi),
(10.72)
with some notational abuse.
Consider now the tangent bundle TF()
with projection 𝜏∶TF() →F(). The
coordinate system {xi, xi
a} induces nat-
urally a coordinate system in TF(). A
vector X ∈TF() is expressed in these
coordinates as follows:
X →
(
xi, xi
a, Xi 𝜕
𝜕xi + Xi
a
𝜕
𝜕xi
a
)
= (xi, xi
a, Xi, Xi
a
) .
(10.73)
The derivative of the projection 𝜋is a map
𝜋∗TF() →T. Its coordinate representa-
tion is
(xi, xi
a, Xi, Xi
a
) →(xi, Xi) .
(10.74)
The map u deﬁned in (10.69) is given in
coordinates by
[u(xi, xi
a)](xj, wj) = x−a
i wi
(a = 1, … , m),
(10.75)
where we have denoted by x−a
i
the entries
of the inverse of the matrix with entries xi
a.

374
10 Topics in Diﬀerential Geometry
Combining (10.74) and (10.75), we obtain
from (10.70) the following coordinate rep-
resentation of the (ℝm)-valued canonical
form 𝜃:
𝜃a = x−a
i
dxi
(a = 1, … , m).
(10.76)
10.5.11
The Christoﬀel Symbols
The canonical form 𝜃exists independently
of any connection. Let us now introduce a
connection on F(), that is, a linear con-
nection on . If we regard a connection as
a horizontal distribution, there must exist
nonsingular linear maps Γ(x, p) from each
Txto each of the tangent spaces TpF()
(with 𝜋(p) = x) deﬁning the distribution.
Noticing that the same distribution may
correspond to an inﬁnite number of such
maps, we pin down a particular one by
imposing the extra condition that they must
be also horizontal lifts. In other words, we
demand that
𝜋∗∘Γ(x, p) = idTx.
(10.77)
The implication of this condition is that,
when written in components, we must have
Γ(x, p)
(
vi 𝜕
𝜕xi
)
= vi 𝜕
𝜕xi −̂Γj
ia(x, p) vi 𝜕
𝜕xj
a
,
(10.78)
where ̂Γj
ia(x, p) are smooth functions of x
and p. The minus sign is introduced for
convenience.
These functions, however, cannot be
arbitrary, because they must also satisfy
the compatibility condition (10.56). It is
not diﬃcult to verify that this is the case if,
and only if, the functions Γj
ik deﬁned by
Γj
ik = ̂Γj
ia(x, p) x−a
k (p)
(10.79)
are independent of p along each ﬁber.
We conclude that a linear connection is
completely deﬁned (on a given coordinate
patch) by means of m3 smooth functions.
These functions are known as the Christof-
fel symbols of the connection.
10.5.12
Parallel Transport and the Covariant
Derivative
Now that we are in possession of explicit
coordinate expressions for the horizontal
distribution, we can write explicitly the sys-
tem of ODEs that eﬀects the horizontal lift
of a curve in . A solution of this system
is a one-parameter family of frames being
parallel-transported along the curve. Let
the curve 𝛾in the base manifold be given
by
xi = xi(t).
(10.80)
On this curve, the connection symbols are
available as functions of t, by composi-
tion. The nontrivial part of the system of
equations is given by
dxi
a(t)
dt
= −Γi
jk(t) xk
a(t) dxj(t)
dt
.
(10.81)
The local solution of this system with given
initial condition (say, xi
a(0) = xi
a) is the
desired curve in F(), representing the
parallel transport of the initial frame along
the given curve.
Let now v be a vector in Txi(0), that is,
a vector at the initial “time” t = 0. We say
that the curve v = v(t) in Tis the paral-
lel transport of v if it projects on 𝛾, with
v(0) = v, and if the components of v(t) in a
parallel-transported frame along 𝛾are con-
stant.4) For this deﬁnition to make sense, we
must make sure that the constancy of the
4) The same criterion for parallel transport that we
are using for the tangent bundle can be used for
any associated bundle.

10.5 Connections
375
components is independent of the particu-
lar initial frame chosen. This, however, is a
direct consequence of the fact that our lin-
ear connection is, by deﬁnition, consistent
with the right action of the group.
To obtain the system of ODEs corre-
sponding to the parallel transport of v along
𝛾, we enforce the constancy conditions
vi(t) x−a
i (t) = Ca,
(10.82)
where each Ca (a = 1, … , m) is a constant
and vi denotes components in the coor-
dinate basis. Diﬀerentiating this equation
with respect to t and invoking (10.81), we
obtain
dvi
dt
+ Γi
jk
dxj
dt vk = 0.
(10.83)
A vector ﬁeld along 𝛾
satisfying this
equation is said to be covariantly constant.
For a given vector ﬁeld w on , the expres-
sion on the left-hand side makes sense in a
pointwise manner whenever a vector u is
deﬁned at a point (whereby the curve 𝛾can
be seen as a representative at t = 0). The
expression
▿uw =
(
dwi
dt
+ Γi
jk uj wk
)
𝜕
𝜕xi ,
(10.84)
is called the covariant derivative of v in the
direction of u. From the treatment above, it
can be seen that the covariant derivative is
precisely the limit
▿uw = lim
t→0
𝜌t,0w −w(0)
t
,
(10.85)
where 𝜌(a, b) denotes the parallel transport
along 𝛾from t = b to t = a.
10.5.13
Curvature and Torsion
To obtain an explicit equation for the cur-
vature form Ω, we should start by eluci-
dating the connection form 𝜔on the basis
of the connection symbols Γ. Given a vec-
tor X ∈TpF, we know that its horizontal
component h(X) is given by
h(X) = Γ(𝜋(p), p)∘𝜋∗(X).
(10.86)
Its vertical component must, therefore, be
given by
v(X) = X −h(X) = X −Γ(𝜋(p), p)∘𝜋∗(X).
(10.87)
Recall that the connection form 𝜔assigns
to X the vector in 𝔤such that v(X) belongs
to its fundamental vector ﬁeld. Let the coor-
dinates of p be (xi, xi
a). The right action of
GL(m; ℝ) is given by
(Rg(p))i
a = xi
b gb
a,
(10.88)
where we have shown only the action on the
ﬁber component and gb
a is the matrix corre-
sponding to g ∈GL(m; ℝ). Consequently,
if g(t) is a one-parameter subgroup repre-
sented by the vector
̂ga
b =
dga
b(t)
dt
|||||t=0
,
(10.89)
the value of the corresponding fundamental
vector ﬁeld at p is
̃gi
a = xi
b ̂gb
a.
(10.90)
The coordinate expression of (10.87) is
(v(X))i
a = Xi
a −h(X) = X −Γ(𝜋(p), p)∘𝜋∗(X).
(10.91)

376
10 Topics in Diﬀerential Geometry
Let the main part of the vector X be given
by
X = vi 𝜕
𝜕xi + Xi
a
𝜕
𝜕xi
a
.
(10.92)
Then, (10.91) delivers
(v(X))i
a = Xi
a + Γj
ik vi xk
a.
(10.93)
According to (10.90), the corresponding
element of the Lie algebra is
̂gb
a =
(
Xj
a + Γj
ik vi xk
a
)
x−b
j .
(10.94)
Accordingly, the Lie-algebra-valued con-
nection form 𝜔is given by
𝜔b
a = Γj
ik xk
a x−b
j
dxi + x−b
j
dxj
a.
(10.95)
The exterior derivative is given by
d𝜔b
a =
𝜕Γj
ik
𝜕xm xk
a x−b
j
dxm ∧dxi
+Γj
ikx−b
j
dxk
a ∧dxi −Γj
ikxk
ax−b
s x−c
j dxs
c ∧dxi
−x−c
j x−b
s
dxs
c ∧dxj
a.
(10.96)
A vector such as (10.92) has the following
horizontal component:
h(X) = vi 𝜕
𝜕xi −Γj
ikxk
avi 𝜕
𝜕xj
a
.
(10.97)
With a similar notation, the horizontal
component of another vector Y is given by
h(Y) = wi 𝜕
𝜕xi −Γj
ikxk
awi 𝜕
𝜕xj
a
.
(10.98)
Consider now the following evaluations:
⟨dxj ∧dxi | h(X) ∧h(Y)⟩= vjwi −viwj,
(10.99)
⟨dxk
a ∧dxi | h(X) ∧h(Y)⟩
= −Γk
rsxs
a (vrwi −viwr),
(10.100)
and
⟨dxj
c ∧dxs
a | h(X) ∧h(Y)⟩
= −Γj
rnxn
c Γs
ikxk
a (vrwi −viwr). (10.101)
Putting all these results together, we obtain
⟨Ω | X ∧Y⟩= ⟨𝜔| h(X) ∧h(Y)⟩
= xk
ax−b
j Rj
krivrwi,
(10.102)
where
Rj
kri =
Γj
ik
𝜕xr −
Γj
rk
𝜕xi + Γj
rhΓh
ik −Γj
ihΓh
rk
(10.103)
is called the curvature tensor of the linear
connection.
In analogy with the concept of curvature
form, we deﬁne the torsion form of a con-
nection as
Θ = D𝜃.
(10.104)
Notice that the coupling with the connec-
tion is in the fact that the operator D is
the exterior covariant derivative, which
involves the horizontal projection. To
understand the meaning of the torsion,
consider a case in which the curvature
vanishes. This means that there exists a
distant (or curve-independent) parallelism
in the manifold . Thus, ﬁxing a basis of
the tangent space at any one point x0 ∈,
a ﬁeld of bases is uniquely determined
at all other points. The question that the
torsion tensor addresses is the following:
does there exist a coordinate system such
that these bases coincide at each point with
its natural basis? An interesting example
can be constructed in ℝ3 as follows. Start-
ing from the standard coordinate system,
move up the x3 axis and, while so doing,
apply a linearly increasing rotation to the
horizontal planes, imitating the action of
a corkscrew. Thus, we obtain a system
of (orthonormal) bases that are perfectly

10.6 Riemannian Manifolds
377
Cartesian plane by horizontal plane, but
twisted with respect to each other as we
ascend. These frames can be used to deﬁne
a distant parallelism (two vectors are paral-
lel if they have the same components in the
local frame). It is not diﬃcult to show (or to
see intuitively) that there is no coordinate
system that has these as natural bases
(use, for example, Frobenius’ theorem).
This example explains the terminology of
“torsion.”
To obtain the coordinate expression of
the torsion form, we start by calculating the
exterior derivative of (10.76) as
d𝜃a = dx−a
i
∧dxi = −x−a
j x−b
i dxj
b ∧dxi.
(10.105)
Using (10.100), we obtain
⟨D𝜃| X ∧Y⟩= ⟨d𝜃| h(X) ∧h(Y)⟩
= x−a
j
Tj
ri vrwi,
(10.106)
where
Tj
ri = Γj
ri −Γj
ir
(10.107)
are the components of the torsion tensor of
the connection.
Suppose that a linear connection with
vanishing curvature has been speciﬁed on
the manifold , and let e1, … , en be a ﬁeld
of parallel frames on the manifold. Then the
Christoﬀel symbols of the connection are
given by the formula
Γi
kj = −e−a
k
𝜕ei
a
𝜕xj ,
(10.108)
where ei
a are the components of the frame
in the natural basis of a coordinate system
x1, … , xm. The components of the torsion
tensor are proportional to the components
of the Lie brackets of corresponding pairs
of vectors of the frames.
10.6
Riemannian Manifolds
10.6.1
Inner-Product Spaces
We have come a long way without the need
to speak about metric concepts, such as
the length of a vector or the angle between
two vectors. That even the concept of
power of a force can be introduced without
any metric background may have seemed
somewhat surprising, particularly to those
accustomed to hear about “the magnitude
of the force multiplied by the magnitude of
the velocity and by the cosine of the angle
they form.” It is very often the case in appli-
cations to particular ﬁelds (Mechanics,
Theoretical Physics, Chemistry, Engineer-
ing, etc.) that there is much more structure
to go around than really needed to formu-
late the basic concepts. For the particular
application at hand, there is nothing wrong
in taking advantage of this extra structure.
Quite to the contrary – the extra structure
may be the carrier of implicit assumptions
that permit, consciously or not, the for-
mulation of the physical laws. The most
dramatic example is perhaps the adherence
to Euclidean Geometry as the backbone of
Newtonian Physics. On the other hand, the
elucidation of the minimal (or nearly so)
structure necessary for the formulation of
a fundamental notion, has proven time and
again to be the beginning of an enlighten-
ment that can lead to further developments
and, no less importantly, to a better insight
into the old results.
We have seen how the concept of the
space dual to a given vector space arises
naturally from the consideration of linear
functions on the original vector space.
On the other hand, we have learned that,
intimately related as they are, there is
no natural isomorphism between these

378
10 Topics in Diﬀerential Geometry
two spaces. In other words, there is no
natural way to associate a covector to a
given vector, and vice versa. In Newtonian
Mechanics, however, the assumption of a
Euclidean metric, whereby the theorem of
Pythagoras holds globally, provides such
identiﬁcation. In Lagrangian Mechanics, it
is the kinetic energy of the system that can
be shown to provide such extra structure,
at least locally. In General Relativity, this
extra structure (but of a somewhat dif-
ferent nature) becomes the main physical
quantity to be found by solving Einstein’s
equations. In all these cases, the identiﬁca-
tion of vectors with covectors is achieved
by means of the introduction of a new
operation called an inner product (or a
dot product or, less felicitously, a scalar
product).
A vector space V is said to be an inner-
product space if it is endowed with an oper-
ation (called an inner product)
⋅∶V × V −→ℝ
(u, v) →u ⋅v,
(10.109)
satisfying the following properties:5)
1. Commutativity
u ⋅v = v ⋅u,
∀u, v ∈V;
(10.110)
2. Bilinearity6)
(𝛼u1 + 𝛽u2) ⋅v = 𝛼(u1 ⋅v) + 𝛽(u2 ⋅v),
∀𝛼, 𝛽∈ℝ, u1, u2, v ∈V;
(10.111)
5) It is to be noted that in the case of a complex
vector space, such as in Quantum Mechanics
applications, these properties need to be altered
somewhat.
6) The term bilinearity refers to the fact that the
inner product is linear in each of its two argu-
ments. Nevertheless, given that we have already
assumed commutativity, we need only to show
linearity with respect to one of the arguments.
3. Positive deﬁniteness7)
v ≠0
=⇒
v ⋅v > 0.
(10.112)
One can show that 0 ⋅v = 0, for all v.
The magnitude or length of a vector v is
deﬁned as the nonnegative number
√
v ⋅v.
Two vectors u, v ∈V are called orthogonal
(or perpendicular) to each other if u ⋅v = 0.
We want now to show how the existence
of an inner product induces an isomor-
phism between a space and its dual (always
in the ﬁnite-dimensional case). Let v ∈V
be a ﬁxed element of V. By the linearity of
the inner product, the product v ⋅u is linear
in the second argument. Accordingly, we
deﬁne the covector 𝜔v ∈V ∗corresponding
to the vector v ∈V, by
⟨𝜔v, u⟩= v ⋅u,
∀u ∈V.
(10.113)
It is not diﬃcult to prove that this linear
map from V to V ∗is one-to-one and that,
therefore, it constitutes an isomorphism
between V and V ∗. We conclude that in an
inner-product space there is no need to dis-
tinguish notationwise between vectors and
covectors.
We call reciprocal basis the basis of V
that corresponds to the dual basis in the
isomorphism induced by the inner product.
We already know that the dual basis oper-
ates on vectors in the following way:
⟨ei, v⟩= vi,
∀v ∈V,
(10.114)
where vi is the ith component of v ∈V
in the basis {ej} (j = 1, … , n). The recip-
rocal basis, therefore, consists of vectors
{ej} (j = 1, … , n) such that
ei ⋅v = vi,
∀v ∈V.
(10.115)
7) In Relativity, this property is removed.

10.6 Riemannian Manifolds
379
Let the components of the reciprocal base
vectors be expressed as
ei = gijej.
(10.116)
In other words, we denote by gij the jth
component of the ith member of the recip-
rocal basis we are seeking. It follows from
(10.115) that
ei ⋅v = (gijej) ⋅(vkek)
= gij (ej ⋅ek) vk = vi,
∀vk ∈ℝ. (10.117)
Looking at the very last equality, it follows
that
gij (ej ⋅ek) = 𝛿i
k.
(10.118)
Indeed, regarded as a matrix equation,
(10.117) establishes that the matrix with
entries
[gij (ej ⋅ek)]
(summation
con-
vention
understood),
when
multiplied
by an arbitrary column vector, leaves it
unchanged. It follows that this matrix must
be the identity. This is only possible if the
matrix with entries
gij = ei ⋅ej,
(10.119)
is the inverse of the matrix with entries
gij. So, the procedure to ﬁnd the recip-
rocal basis is the following: (i) Construct
the (symmetric) square matrix with entries
gij = ei ⋅ej. (ii) Invert this matrix to obtain
the matrix with entries gij. (iii) Deﬁne ei =
gijej. Note that the metric matrix {gij} is
always invertible, as it follows from the lin-
ear independence of the basis.
A basis of an inner-product space is
called orthonormal if all its members are
of unit length and mutually orthogonal.
The reciprocal of an orthonormal basis
coincides with the original basis.
Having identiﬁed an inner-product space
with its dual, and having brought back the
dual basis to the original space under the
guise of the reciprocal basis, we have at
our disposal contravariant and covariant
components of vectors. Recall that before
the introduction of an inner product, the
choice of a basis in V condemned vectors to
have contravariant components only, while
the components of covectors were covari-
ant.
Starting from v = viei = viei and using
(10.118) and (10.119), the following formu-
las can be derived:
vi = gijvj,
(10.120)
vi = gijvj,
(10.121)
ei = gijej,
(10.122)
vi = v ⋅ei,
(10.123)
vi = v ⋅ei,
(10.124)
ei ⋅ej = gij,
(10.125)
ei ⋅ej = 𝛿i
j.
(10.126)
A
linear
map
Q ∶U →V
between
inner-product spaces is called orthogonal
if QQT = idV and QTQ = idU, where id
stands for the identity map in the subscript
space. The components of an orthogonal
linear map in orthonormal bases of both
spaces comprise an orthogonal matrix.
A linear map T between inner-product
spaces preserves the inner product if,
and only if, it is an orthogonal map. By
preservation of inner product, we mean
that T(u) ⋅T(v) = u ⋅v, ∀u, v ∈U.
10.6.2
Riemannian Manifolds
If each tangent space Txof the mani-
fold is endowed with an inner product,
and if this inner product depends smoothly

380
10 Topics in Diﬀerential Geometry
on x ∈, we say that is a Riemannian
manifold. To clarify the concept of smooth-
ness, let {, 𝜙} be a chart in with coor-
dinates x1, … , xn. This chart induces the
(smooth) basis ﬁeld 𝜕∕𝜕x1, … , 𝜕∕𝜕xn. We
deﬁne the contravariant components of the
metric tensor g associated with the given
inner product (indicated by ⋅) as
gij =
( 𝜕
𝜕xi
)
⋅
( 𝜕
𝜕xj
)
.
(10.127)
Smoothness means that these components
are smooth functions of the coordinates
within the patch. The metric tensor itself is
given by
g = gij dxi ⊗dxj.
(10.128)
We have learned how an inner product
deﬁnes an isomorphism between a vector
space and its dual. When translated to Rie-
mannian manifolds, this result means that
the tangent and cotangent bundles are nat-
urally isomorphic (via the pointwise iso-
morphisms of the tangent and cotangent
spaces induced by the inner product).
A nontrivial physical example is found in
Lagrangian Mechanics, where the kinetic
energy (assumed to be a positive-deﬁnite
quadratic form in the generalized veloci-
ties) is used to view the conﬁguration space
as a Riemannian manifold.
10.6.3
Riemannian Connections
The theory of Riemannian manifolds is very
rich in results. Classical diﬀerential geome-
try was almost exclusively devoted to their
study and, more particularly, to the study of
two-dimensional surfaces embedded in ℝ3,
where the Riemannian structure is derived
from the Euclidean structure of the sur-
rounding space.
A Riemannian connection is a linear con-
nection on a Riemannian manifold. The
most important basic result for Riemannian
connections is contained in the following
theorem:
Theorem 10.1 On a Riemannian manifold
there exists a unique linear connection with
vanishing torsion and such that the covari-
ant derivative of the metric vanishes identi-
cally.
We omit the proof. The Christoﬀel sym-
bols of this connection are given in terms of
the metric tensor by the formula
Γk
ij = 1
2 gkh
(𝜕gih
𝜕xj +
𝜕gjh
𝜕xi −
𝜕gij
𝜕xh
)
.
(10.129)
The curvature tensor associated with this
special connection is called the Riemann–
Christoﬀel curvature tensor. A Riemannian
manifold is said to be locally ﬂat if, for each
point, a coordinate chart can be found such
that the metric tensor components every-
where in the chart reduce to the identity
matrix. It can be shown that local ﬂatness is
equivalent to the identical vanishing of the
Riemann–Christoﬀel curvature tensor.
Further Reading
Some general treatises on Diﬀerential
Geometry:
Chern, S.S., Chern, W.H., and Lam, K.S. (1999)
Lectures on Diﬀerential Geometry, World
Scientiﬁc.
Kobayashi, S. and Nomizu, K. (1996) Foundations
of Diﬀerential Geometry, Wiley Classics
Library Edition.
Lee, J.M. (2003) Introduction to Smooth
Manifolds, Springer.
Sternberg, S. (1983) Lectures on Diﬀerential
Geometry, 2nd edn, Chelsea Publishing
Company.

Further Reading
381
Warner, F.W. (1983) Foundations of Diﬀerentiable
Manifolds and Lie Groups, Springer.
Some
books
that
emphasize
physi-
cal applications or deal with particular
physical theories in a geometric way are
Abraham, R. and Marsden, J.E. (2008)
Foundations of Mechanics, 2nd edn, AMS
Chelsea Publishing.
Arnold, V.I. (1978) Mathematical Methods of
Classical Mechanics, Springer.
Choquet-Bruhat, Y., de Witt-Morette, C., and
Dillard-Beck, M. (1977) Analysis, Manifolds
and Physics, North-Holland.
Frankel, T. (2004) The Geometry of Physics: An
Introduction, 2nd edn, Cambridge University
Press.
Misner, W., Thorne, K.S., and Wheeler, J.A.
(1973) Gravitation, W H Freeman and
Company.
Much of the material in this article is
reproduced, with permission, from
Epstein, M. (2010) The Geometrical Language of
Continuum Mechanics, Cambridge University
Press.


383
Part III
Analysis
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.


385
11
Dynamical Systems
David A.W. Barton
11.1
Introduction
11.1.1
Deﬁnition of a Dynamical System
Dynamical systems are deﬁned using three
key ingredients: a state space, a time set, and
an evolution operator.
The state space X of a system is the
set of all possible states of a system. For
many purposes X = ℝn is a suitable state
space, though for some dynamical systems
(notably partial diﬀerential equations or
delay diﬀerential equations) the state space
may be a more general Banach space.
The time set T is the set of times at
which a system is deﬁned. The two stan-
dard cases are T = ℝfor continuous-time
dynamical systems or T = ℤfor discrete-
time systems. Only continuous-time sys-
tems are considered here, though many of
the results carry through directly to dis-
crete time systems.
The evolution operator forms the core of
a dynamical system. The evolution operator
𝜙t ∶X →X maps an initial state forward by
t ∈T time units, that is, x(t) = 𝜙tx0 where
x0 is the initial state at time t = 0. In cer-
tain contexts (e.g., discrete maps), the evo-
lution operator is given explicitly. In most
contexts, however, it is usual for the evo-
lution operator to be deﬁned implicitly, for
example, through the solution of a diﬀeren-
tial equation.
Evolution operators have two deﬁning
characteristics, namely,
𝜙0x0 = x0
(“no time, no evolution”)
(11.1)
and
𝜙t+sx0 = 𝜙t(𝜙sx0)
(“determinism”).
(11.2)
Note that nonautonomous or stochastic
systems are excluded by this deﬁnition;
however, similar formalisms can be derived
for those types of systems [1].
Finally, a dynamical system is formally
deﬁned as the triple {X, T, 𝜙t}, where X is
the state space, T is the time set, and 𝜙t
is the evolution operator of the dynamical
system.
Two further concepts, which are impor-
tant in the study of dynamical systems, are
orbits and 𝜔-limit sets. For a given point in
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

386
11 Dynamical Systems
state space x0, the orbit that passes through
this point is given by the set {𝜙t(x0)|∀t ∈
T}. Conversely, the 𝜔-limit set of a point x0
is the set of points {limt→∞𝜙t(x0)}.
For a more detailed introduction to the
theory of dynamical systems than this
chapter permits, see [2–4].
11.1.2
Invariant Sets
The main objects of interest when study-
ing dynamical systems are the invariant sets
of a system, that is, the subsets of state
space that are invariant under the action of
the evolution operator (e.g., equilibria). The
study of these invariant sets, and the asso-
ciated long-time dynamics, provides exten-
sive information about the behavior of the
system without the need for determining
transient behavior, which can be a diﬃcult
task.
The two basic types of invariant set con-
sidered in this chapter are as follows.
1. Equilibria. The simplest type of
invariant set is an equilibrium x∗∈X
such that x∗= 𝜙t(x∗) for all t. A trivial
example of an equilibrium can be found
in the (linear) dynamical system deﬁned
by dx∕dt = 𝜇x for 𝜇≠0 (i.e., x∗= 0).
2. Periodic orbits. These are deﬁned by a
function x(t) = f (𝜔t), where f is a
continuous and periodic (with period
2𝜋) function that satisﬁes the
underlying dynamical system, and 𝜔is
the frequency, of the orbit. Periodic
orbits that are isolated (i.e., no points
neighboring the periodic orbit are part
of another periodic orbit) are also
known as limit cycles.
This is by no means an exhaustive list.
Several obvious omissions are given below.
• Homoclinic and heteroclinic orbits.
These are deﬁned by the points
x such that 𝜙t(x) →x𝜔as t →∞
and
𝜙t(x) →x𝛼
as
t →−∞.
For
homoclinic orbits, x𝛼= x𝜔, and for
heteroclinic orbits, x𝛼≠x𝜔.
• Quasi-periodic
orbits.
These
are
deﬁned by a multivariate function
x(t) = f (𝜔1t, … , 𝜔nt)
that
satisﬁes
the underlying dynamical system.
• Strange attractors. These are associ-
ated with chaotic dynamics.
However, these and other invariant sets are
beyond the scope of this chapter.
This chapter focuses on the dynamics
local to these invariant sets and the con-
ditions under which the dynamics change
(so-called bifurcations).
11.2
Equilibria
11.2.1
Deﬁnition and Calculation
Consider the ordinary diﬀerential equation
(ODE)
dx
dt = f (x, 𝜇),
x ∈ℝn, 𝜇∈ℝ,
(11.3)
where f ∶ℝn × ℝ→ℝn, x is the state of
the system, and 𝜇is a system parameter.
The equilibria of (11.3) are deﬁned as the
points where dx∕dt = 0. As such, for a
particular choice of the parameter 𝜇, there
may exist any number of equilibria. The
calculation of equilibria is a root-ﬁnding
problem f (x∗, 𝜇) = 0 and, for any nontrivial
f , numerical root-ﬁnding methods are
often needed.

11.2 Equilibria
387
11.2.2
Stability
In the context of dynamical systems, there
are several diﬀerent notions of stability for
equilibria. The two key notions are asymp-
totic stability and Lyapunov stability.
An equilibrium x∗is said to be asymptoti-
cally stable if the 𝜔-limit set of all the neigh-
boring points is the single point x∗. Thus, if
the system is perturbed slightly away from
the equilibrium position, the system will
always return to the equilibrium.
Lyapunov stability is a weaker concept.
An equilibrium x∗is said to be Lyapunov
stable if for every neighborhood U of x∗
there exists a smaller neighborhood U1 of
x∗contained within U such that all solu-
tions starting in U1 remain within U for
all time. Thus if the system is perturbed
slightly away from the equilibrium position,
the system will always stay in the vicinity of
the equilibrium. Clearly, asymptotic stabil-
ity implies Lyapunov stability.
11.2.3
Linearization
In the vicinity of an equilibrium x∗, the
ODE (11.3) can be linearized to give
d̃x
dt = J ̃x,
(11.4)
where J is the Jacobian matrix deﬁned by
J = [Ji,j
]|||x=x∗=
[ 𝜕fi
𝜕xj
]|||||x=x∗.
(11.5)
The Jacobian matrix contains a great deal of
information about the equilibrium, in par-
ticular, about its stability. An exponential
solution of (11.4) reveals that the growth
or decay of solutions is determined by the
eigenvalues of J. As such, the asymptotic
stability of the equilibria is determined by
the sign of the real part of the eigenval-
ues; for an equilibrium to be stable all
the eigenvalues must have negative real
parts. An equilibrium with eigenvalues that
have both positive and negative real parts
is called a saddle (see, e.g., Figure 11.1b).
Should any of the eigenvalues be complex,
the resulting solutions will have spiraling
behavior (see, e.g., Figure 11.1d and e).
If there are no eigenvalues on the imagi-
nary axis (i.e., none with zero real part) the
equilibrium is said to be hyperbolic.
In the case of a hyperbolic equilibrium,
it is possible to formalize the link between
the original nonlinear dynamical system
and the linearized system through the idea
of topological equivalence, as described in
Section 11.2.5.
Associated with stable equilibria (and
other stable invariant sets) is the notion
of a basin of attraction, that is, the set of
points in state space that, when evolved
forward in time, approach the equilibrium.
Formally, the basin of attraction for an
equilibrium x∗is deﬁned as
B(x∗) =
{
x ∶lim
t→∞𝜙t(x) = x∗}
.
(11.6)
The basins of attraction for diﬀerent
equilibria become particularly important
when considering the initial value problem
of a dynamical system. The equilibrium
state reached in the long-time limit (assum-
ing the system equilibrates) is determined
by the basin of attraction that the initial
conditions lie in. Various methods exist for
ﬁnding basins of attraction including man-
ifold computations (see Section 11.2.6),
cell mapping [5], and subdivision [6]; typ-
ically, these methods are only applied to
low-dimensional systems as they do not
scale well to larger systems.

388
11 Dynamical Systems
(a)
x
y
(b)
x
y
(c)
x
y
(d)
x
y
(e)
x
y
Figure 11.1
Possible phase portraits of a 2D
linear dynamical system, which show the evo-
lution in state space of several orbits. The
equilibria are classiﬁed as (a) a stable node;
(b) a saddle; (c) an unstable node; (d) a stable
spiral; (e) an unstable spiral. Only three of the
phase portraits are topologically distinct as
(a) and (d) are topologically equivalent, as are
(c) and (e).
11.2.4
Lyapunov Functions
While the Jacobian matrix provides infor-
mation about the local stability of an equi-
librium, for nontrivial systems it is often
diﬃcult, if not impossible, to calculate the
eigenvalues analytically and so determine
the stability. Furthermore, the Jacobian only
provides a means to determine the local
dynamics and not the global dynamics as is
sometimes desired.
A
common
alternative
approach
to
studying the stability of an equilibrium is
to construct a so-called Lyapunov function.
A Lyapunov function V is a continuous
scalar function of the state variables that
has the following properties (assuming the
equilibrium has been shifted to the origin).
• V is positive deﬁnite, that is,
V(x) > 0
∀x ∈X∖{0}.
• dV∕dt is negative deﬁnite, that is,
(d∕dt)V(x) < 0
∀x ∈X∖{0}.
If these conditions are satisﬁed, then
the equilibrium is globally asymptotically
stable. If the conditions are weakened
such that V (respectively dV∕dt) is locally
positive deﬁnite (respectively locally neg-
ative deﬁnite), then the equilibrium will be
locally asymptotically stable. It is possible
to weaken the second condition further
and require only that dV∕dt be negative
semi-deﬁnite (i.e., (d∕dt)V(x) ≤0
∀x ∈
X∖{0}); in this case the equilibrium is said
to be Lyapunov stable.
The main diﬃculty of using a Lyapunov
function approach is that there are no uni-
versally applicable rules for constructing
them; trial and error, in general, is the only
way. However, for some systems, it is possi-
ble to appeal to physical conservation laws
to ﬁnd a Lyapunov function. One example
is the Duﬃng equation
d2y
dt2 + 𝜉dy
dt + y + 𝛼y3 = 0,
(11.7)

11.2 Equilibria
389
where the state space is [y, ̇y]. When 𝜉= 0,
(11.7) can be multiplied by ̇y and integrated
with respect to time to arrive at
E(t) = 1
2
(
̇y2 + y2 + 1
2𝛼y4)
= const. (11.8)
When 𝜉> 0, 𝛼> 0, this function is a Lya-
punov function. Clearly, with the given con-
straints, E(t) > 0 for all nonzero [y, ̇y] and is
only zero at the equilibrium position [0, 0].
Diﬀerentiating this expression with respect
to time gives
dE
dt = dy
dt
(d2y
dt2 + y + 𝛼y3
)
(11.9)
= dy
dt
(
−
(
𝜉dy
dt + y + 𝛼y3
)
+ y + 𝛼y3
)
(11.10)
= −𝜉
(dy
dt
)2
.
(11.11)
Thus the time derivative is negative for
all values of ̇y > 0. However, dE∕dt = 0
regardless of the value of y when ̇y = 0. As
such, the conditions for asymptotic stabil-
ity are not satisﬁed but the equilibrium can
be said to be Lyapunov stable. For 𝜉> 0,
the origin is actually asymptotically stable
(as can be veriﬁed by the linearization); the
choice of Lyapunov function is deﬁcient in
this case.
One further beneﬁt of using Lyapunov
functions to prove stability is that Lya-
punov functions can be used even when the
equilibrium in question is nonhyperbolic
(unlike linearization). Take for example the
following system
dx
dt = y −x3
(11.12)
dy
dt = −x −y3.
(11.13)
The linearization of this system yields
purely imaginary eigenvalues. However,
the Lyapunov function V = (1∕2) (x2 + y2)
can be used to show that the equilibrium at
x, y = 0 is asymptotically stable.
11.2.5
Topological Equivalence
Another key concept in dynamical systems
is the idea of topological equivalence; it
is a relationship between two dynamical
systems that ensures that their dynamics
are equivalent in a certain sense. Thus, pro-
vided certain criteria are met, the dynamics
of a seemingly complicated system can be
replaced by an equivalent but simpler sys-
tem. Put formally, two dynamical systems
are topologically equivalent if there exists a
homeomorphism that maps orbits of one
onto the orbits of the other, preserving the
direction of time. Typically, this homeo-
morphism is not known explicitly but its
existence can be inferred using various
theorems. Furthermore, topological equiv-
alence is not necessarily a global property,
applying to the whole of state space, but
instead may only be a local property, apply-
ing to the neighborhood of an equilibrium
(or other invariant set).
Two important theorems relating to
topological equivalence are as follows.
Equivalence of linear ﬂows. Two linear
dynamical systems are topologically
equivalent if, and only if, they have
the same number of eigenvalues with
positive real part, the same number
of eigenvalues with negative real part,
and the same number of eigenvalues
with zero real part.
Hartman–Grobman theorem. A (nonlin-
ear) dynamical system is topologically
equivalent to its linearization in the
neighborhood of an equilibrium pro-
vided the equilibrium is hyperbolic.

390
11 Dynamical Systems
Thus, the dynamics near a hyperbolic
equilibrium are essentially linear.
The Hartman–Grobman theorem com-
bined with the equivalence of linear ﬂows is
a very powerful tool when analyzing non-
linear systems. For example, the dynam-
ics near a hyperbolic equilibrium of the 2D
dynamical system
̇x = f (x, y)
̇y = g(x, y)
(11.14)
will be topologically equivalent to one of
the phase portraits shown in Figure 11.1
as determined by the eigenvalues of its
linearization.
Similar
eigenvalue-based
characterizations are possible in higher
dimensions, but graphical representations
become more diﬃcult.
Furthermore, these two theorems ensure
that, in the neighborhood of a hyperbolic
equilibrium, small changes in the system
parameters will not (topologically) change
the dynamics. For the dynamics to change,
the equilibrium must ﬁrst become nonhy-
perbolic (i.e., one of the eigenvalues lies
on the imaginary axis); in this case, the
Hartman–Grobman theorem fails. When
this occurs, the system is said to be struc-
turally unstable, that is, small changes in
the equation can cause topological changes
in the local dynamics. If these small changes
can be realized by varying system parame-
ters, then the system is said to be at a bifur-
cation point.
11.2.6
Manifolds
A hyperbolic saddle-type equilibrium xs
has eigenvalues that have both negative
and positive real parts and so it has direc-
tions in which it is attracting and direc-
tions in which it is repelling. As such, it
possesses a stable manifold Ws(xs), corre-
sponding to the set of points in state space
that approach the equilibrium in forward
time, and an unstable manifold Wu(xs), cor-
responding to the set of points in state space
that approach the equilibrium in backward
time. The stable and unstable manifolds are
thus deﬁned as follows
Ws(xs) =
{
x ∶lim
t→∞𝜙t(x) = xs
}
(stable manifold)
(11.15)
and
Wu(xs) =
{
x ∶lim
t→−∞𝜙t(x) = xs
}
(unstable manifold).
(11.16)
The dimension of the stable and unstable
manifolds is equal to the number of eigen-
values with negative or positive real parts,
respectively.
These manifolds are said to be invariant
manifolds; an orbit of the dynamical system
that starts in the (un)stable manifold will
remain contained within the manifold for
all time.
Should an equilibrium be nonhyper-
bolic (i.e., it has eigenvalues with zero real
part), it will also possess a center manifold;
these play a vital part when looking at
bifurcations and will be discussed later in
Section 11.2.11.
Stable manifolds act as separatrices in
state space; that is, they form boundaries
between the basins of attraction of diﬀer-
ent attractors in the system. This is most
easily seen and exploited in 2D dynamical
systems, see, for example, Figure 11.2, but
also holds true for higher-dimensional sys-
tems. (Note that while a stable manifold can
act as a separatrix, not all separatrices are
stable manifolds.)
Both stable and unstable manifolds are
global objects and in general can only be

11.2 Equilibria
391
2
2
1
1
0
0
x
−1
−1
−2
−2
x
Ws (xs)
Ws (xs)
Wu (xs)
Wu (xs)
Figure 11.2
A phase portrait
of the bistable Duﬃng equation
̈x + ̇x −x + x3 = 0 that shows
how the stable manifold Ws
of the saddle equilibrium acts
as a separatrix, dividing the
basins of attraction of the two
stable equilibria. The stable
and unstable manifolds of
the saddle equilibrium are
marked as solid curves and
representative orbits are shown
as dashed curves.
found numerically. However, a local (lin-
ear) approximation to the stable and unsta-
ble manifolds of a saddle equilibrium xs
is given by the eigenvectors correspond-
ing to the stable and unstable eigenvalues,
respectively.
This local approximation suggests a
straightforward method for globalizing a
one-dimensional unstable manifold; simply
choose a starting point close to the equi-
librium along the unstable eigendirection
and evolve the dynamical system forward
in time, that is
Wu(xs) ≈{𝜙t(xs + 𝜖eu)
∀t > 0} , (11.17)
where eu is the normalized unstable eigen-
vector. The accuracy of the approximation
is determined by the size of 𝜖and the
accuracy of the numerical integrator used.
A one-dimensional stable manifold can be
found in a similar manner by reversing the
direction of time to give
Ws(xs) ≈{𝜙t(xs + 𝜖es)
∀t < 0} , (11.18)
where
es
is
the
normalized
stable
eigenvector.
This approach of globalizing a one-
dimensional
manifold
can
naively
be
extended to two-dimensional manifolds
(and higher) by evolving forward (or back-
ward) in time a set of points chosen from a
circle around the equilibrium in the plane
spanned by the corresponding eigenvec-
tors. This may work in simple cases, but
when the manifold has nontrivial geometry
or the dynamics on the manifold are not
uniform (e.g., when the corresponding
eigenvalues are signiﬁcantly diﬀerent in
size), this method will not produce accurate
results. More sophisticated methods are
required. For a comprehensive overview of
methods for calculating manifolds, see [7].
11.2.7
Local Bifurcations
Often the main interest when studying a
particular dynamical system is to under-
stand what happens to the dynamics as
the system parameters change. As stated
in Section 11.2.3, the dynamics in the
neighborhood of an equilibrium can only
change topologically if the equilibrium
becomes nonhyperbolic as a parameter is
varied, causing the Hartman–Grobman
theorem to fail. Thus the presence of a non-
hyperbolic equilibrium is a sign that the
dynamical system is structurally unstable,
that is, arbitrarily small parameter pertur-
bations will lead to topological changes in
the dynamics.

392
11 Dynamical Systems
In the case of an equilibrium becoming
nonhyperbolic, the loss of structural stabil-
ity indicates the presence of a local bifur-
cation. The bifurcation is said to be local
because away from the neighborhood of
the equilibrium, the system dynamics will
remain topologically unchanged. To bring
about a global change in the dynamics, a
global bifurcation is required (often involv-
ing the stable and unstable manifolds of a
saddle equilibrium). Global bifurcations are
beyond the scope of this chapter but are
covered in detail in [4].
There are two generic local bifurcations
of equilibria: the saddle-node bifurcation
(otherwise known as a fold owing to its
relationship to singularity theory) and the
Hopf bifurcation. They are generic in the
sense that they can be expected to occur
in an arbitrary dynamical system as a sin-
gle parameter is varied. They are codimen-
sion one bifurcations; if the system has
two parameters, then there will be a one-
dimensional curve of parameter values at
which the bifurcation occurs within the
two-dimensional parameter plane.
All other bifurcations either require
special
properties,
such
as
symme-
try, or are nongeneric (of codimension
higher than one), that is, they can only
be expected to occur in an arbitrary
dynamical system when two or more
parameters
are
varied
simultaneously.
Again, if the system has two parameters,
then for a codimension-two bifurcation
there will be a zero-dimensional point at
which the bifurcation occurs within the
two-dimensional parameter plane.
As
many
physical
systems
contain
symmetries (at least approximately), the
symmetric pitchfork bifurcation, although
nongeneric, is of particular interest and
so is also covered below, along with
the saddle-node bifurcation and Hopf
bifurcation.
11.2.8
Saddle-Node Bifurcation
At a saddle-node bifurcation (fold) two
equilibria collide at a point in state space
and are destroyed. This is most easily seen
in the simple dynamical system deﬁned by
dx
dt = 𝜇−x2,
𝜇, x ∈ℝ.
(11.19)
Equation (11.19) has two equilibria for
𝜇> 0, one stable (x = √𝜇) and one unsta-
ble (x = −√𝜇). For 𝜇< 0, it does not pos-
sess any equilibria; instead x →−∞as t →
∞for all initial conditions. This is shown
graphically in Figure 11.3a and b. When
𝜇= 0, the equilibria coincide at x = 0 and
the linearization of (11.19) indicates that
they are nonhyperbolic. Thus (𝜇= 0, x = 0)
is the bifurcation point.
In terms of the eigenvalues of the
associated Jacobian of an equilibrium, a
saddle-node bifurcation corresponds to an
eigenvalue passing through zero. It should
be noted that this characterization in terms
of eigenvalues is not unique; in the pres-
ence of other properties (e.g., symmetry),
an eigenvalue passing through zero can
correspond to a transcritical or pitchfork
bifurcation. As such, to be sure that a
bifurcation is a saddle-node bifurcation,
certain genericity conditions must be met.
The saddle-node bifurcation theorem is
as follows.
Theorem 11.1 Consider
the
one-
dimensional dynamical system
dx
dt = f (x, 𝜇),
x ∈ℝ, 𝜇∈ℝ,
(11.20)
where f is smooth and has an equilib-
rium x = 0 at 𝜇= 0. Furthermore, let
𝜆= fx(0, 0) = 0. If the genericity condi-
tions (1) fxx(0, 0) ≠0 and (2) f𝜇(0, 0) ≠0
are satisﬁed, then (11.20) is topologically

11.2 Equilibria
393
(a)
(b)
𝜇 = −0.25
𝜇 = 0.25
−0.5
0.5
𝜇 = 0.75
x
x
−1
1
1
0
0
𝜇
Figure 11.3
(a) A one-dimensional phase portrait for
(11.19) as 𝜇is varied; the equilibria are marked as circles
and the arrows show the direction of ﬂow. (b) The cor-
responding one-parameter bifurcation diagram with the
saddle-node bifurcation occurring at 𝜇= 0, x = 0. Solid
curves mark the stable equilibria and dashed lines mark the
unstable equilibria.
equivalent near the origin to the normal
form
dy
dt = 𝛼± y2.
(11.21)
The power of this theorem is that the
dynamics of any system satisfying the
bifurcation conditions will be topologically
equivalent to (11.21).
This
theorem
only
applies
to
one-
dimensional
systems;
however,
multi-
dimensional systems that have a single
eigenvalue passing through zero can be
reduced to a one-dimensional system using
a center manifold reduction as described
in Section 11.2.11.
A common tool to visualize the eﬀect of
parameter changes on the dynamics is the
use of a bifurcation diagram whereby the
equilibria (and other invariant sets) of the
system are plotted using a suitable norm
against the parameter value. The bifur-
cation diagram associated with (11.19) is
shown in Figure 11.3b where it can clearly
be seen that the two equilibria collide and
disappear at 𝜇= 0.
Hysteresis loops in a system are often the
result of a pair of saddle-node bifurcations.
A common example is that of a periodi-
cally forced mass—spring–damper system
with
hardening
spring
characteristics
(d2x∕dt2) + 2𝜉dx∕dt + x + 𝛽x3 = Γ sin(𝜔t).
A simpler, analytically treatable equation
(with exact solutions) containing this type
of behavior is
dx
dt = 𝜇+ 𝜎x −x3.
(11.22)
A one-parameter bifurcation diagram for
(11.22) showing x plotted against 𝜇for
𝜎= 1 is shown in Figure 11.4a. The second
parameter, 𝜎, controls the relative positions
(in terms of 𝜇) of the saddle-node bifur-
cations. As 𝜎→0 from above, the saddle-
node bifurcations meet at a point before
disappearing in a codimension-two cusp
bifurcation as shown in the two-parameter
bifurcation diagram Figure 11.4b, where 𝜇
is plotted against 𝜎.
11.2.9
Hopf Bifurcation
At a Hopf bifurcation (also known as an
Andronov–Hopf bifurcation), an equilib-
rium becomes unstable as a pair of complex

394
11 Dynamical Systems
(a)
(b)
0
0
𝜇
𝜇
0
0
2
−2
−1
−1
1
1
−1
1
x
𝜎
Figure 11.4
(a) A one-parameter bifurcation
diagram for (11.22) as 𝜇varies. Solid curves
mark the stable equilibria and dashed lines
mark the unstable equilibria. A potential hys-
teresis loop that occurs as 𝜇is varied is marked
by arrows. (b) A two-parameter bifurcation dia-
gram for the same equation with the solid
curve being a saddle-node bifurcation curve.
A cusp bifurcation occurs at 𝜇= 0, 𝜎= 0. Inside
the gray shaded region there exist three equi-
libria (two stable and one unstable); every-
where else there exists only one (stable) equi-
librium.
conjugate eigenvalues pass through the
imaginary axis. At the bifurcation point,
a limit cycle is created, which then grows
in amplitude as the system parameters
are changed further. Thus Hopf bifur-
cations are commonly associated with
vibration problems, particularly the onset
of vibrations. A prototype example of a
Hopf bifurcation is given by
dx
dt = 𝜇x −𝜔y + 𝛼x(x2 + y2),
dy
dt = 𝜔x + 𝜇y + 𝛼y(x2 + y2).
(11.23)
Alternatively, (11.23) can be written in
complex form
dz
dt = (𝜇+ i𝜔)z + 𝓁1|z|2z.
(11.24)
In both forms, 𝜇is the bifurcation param-
eter, 𝜔is the frequency of the bifurcating
limit cycle (which is equal to the imaginary
part of the eigenvalues passing through
the imaginary axis) and 𝓁1 is a param-
eter known as the ﬁrst Lyapunov coeﬃ-
cient, which determines the criticality of
the bifurcation. The Hopf bifurcation can
be either subcritical (𝓁1 > 0) or supercrit-
ical (𝓁1 < 0).
Figure 11.5
shows
a
one-parameter
bifurcation diagram for (11.23) in the
supercritical case; to obtain the subcritical
case, simply reverse all the directions of
ﬂow indicated in the ﬁgure. Thus, in the
supercritical case, a stable limit cycle is
created and, in the subcritical case, an
unstable limit cycle is created.
If (11.23) is rewritten in polar form,
x = r cos(𝜃) and y = r sin(𝜃), it is clear that
a Hopf bifurcation is really a pitchfork
bifurcation of the equation governing the
growth of r. As such, a one-parameter
bifurcation diagram showing the growth in
amplitude of the limit cycles that emerge
from the Hopf bifurcation is shown by
Figure 11.7a for the supercritical case and
Figure 11.7b for the subcritical case.
In the neighborhood of a Hopf bifur-
cation (either sub- or supercritical), the
bifurcating limit cycle is well described by
a single sine function; away from the Hopf
bifurcation, however, its shape can change.
Furthermore, close to the bifurcation point,
the amplitude of the limit cycle grows pro-
portional to √𝜇.

11.2 Equilibria
395
0
0
0
1
1
1
−1
−1
0.5
−0.5
−1
μ
x
y
Figure 11.5
A one-parameter bifurcation
diagram of (11.23) showing the onset of
limit cycle oscillations as 𝜇increases. The
(supercritical) Hopf bifurcation occurs at
𝜇= x = y = 0 and the amplitude of the
resulting limit cycle is shown as a dashed
line.
The Hopf bifurcation theorem itself is
stated below.
Theorem 11.2 Consider
the
two-
dimensional dynamical system
̇x = f (x, 𝜇),
x ∈ℝ2, 𝜂∈ℝ,
(11.25)
where f is smooth and has an equilib-
rium x = 0 at 𝜇= 0. Furthermore, let
𝜆1,2 = 𝜎(𝜇) ± i𝜔(𝜇) be the eigenvalues of
the Jacobian matrix such that 𝜎= 0 when
𝜇= 0. If the genericity conditions (i) 𝓁1 ≠0,
where 𝓁1 is the ﬁrst Lyapunov coeﬃcient [4],
and (ii) 𝜎𝜇(0) ≠0 are satisﬁed, then (11.25)
has the topological normal form z ∈ℂ
dz
dt = (𝜎+ i𝜔)z + 𝓁1|z|2z.
As
with
the
saddle-node
bifurcation,
higher-dimensional
systems
can
be
reduced to this two-dimensional form
using a center manifold reduction.
Take for example the van der Pol
equation
d2y
dt2 + y = (𝜇−y2)dy
dt
(11.26)
where 𝜇is the bifurcation parameter.
Equation (11.26) has an equilibrium y = 0,
which undergoes a Hopf bifurcation when
𝜇= 0 as seen by a pair of complex con-
jugate eigenvalues of the corresponding
Jacobian passing through the imaginary
axis. (Note that this is not the typical form
of the van der Pol equation –if 𝜇multiplies
the y2 term as well, a singular Hopf bifur-
cation occurs where the transition from
an inﬁnitesimal limit cycle to a large limit
cycle happens over an exponentially small
range of parameter values.) To determine
the behavior of the limit cycle near the
bifurcation point, perturbation methods
can be used.
Close to the bifurcation point, the limit
cycle will be small, so rescale y such that
𝜖x = y, which gives
d2x
dt2 + x = (𝜇−𝜖2x2)dy
dt .
(11.27)
Then consider small perturbations to the
bifurcation parameter 𝜇about the bifur-
cation point. It turns out that these small
perturbations must be of order 𝜖2 for a
Hopf bifurcation (doing the following anal-
ysis with 𝜇of O(𝜖) will only ﬁnd the equilib-
rium solution). Thus write 𝜇= 𝜖2 ̃𝜇; abusing
notation slightly, the tilde is immediately
dropped to give
d2x
dt2 + x = 𝜖2(𝜇−x2)dx
dt .
(11.28)
As with most Hopf bifurcation prob-
lems,
(11.28)
is
directly
amenable
to
perturbation
methods
such
as
the
Poincaré–Lindstedt method or the method
of multiple scales [8, 8]. For simplicity, the
Poincaré–Lindstedt approach is shown
here. First, rescale time such that 𝜏= 𝜔t
where 𝜔= 1 + 𝜔1𝜖+ 𝜔2𝜖2 + O(𝜖3); 𝜔1 and

396
11 Dynamical Systems
𝜔2 are constants to be determined. Next,
assume a perturbation solution of the form
x = x0 + x1𝜖+ x2𝜖2 + O(𝜖3).
Expanding
out (11.28) and collecting terms of the
same order in 𝜖gives the equations
O(1) ∶d2x0
d𝜏2 + x0 = 0,
(11.29)
O(𝜖) ∶d2x1
d𝜏2 + x1 = −2𝜔1
d2x0
d𝜏2 ,
(11.30)
O(𝜖2) ∶d2x2
d𝜏2 + x2 = −2𝜔1
d2x1
d𝜏2
−
(
𝜔2
1 + 2𝜔2
) d2x0
d𝜏2
+
(
𝜇−x2
0
) dx0
d𝜏. (11.31)
With
the
initial
conditions
x0(0) = A
and x′
0(0) = 0 (because the system is
autonomous,
orbits
can
be
arbitrarily
shifted to meet these conditions), (11.29)
admits a solution x0 = A cos(t). The sec-
ular terms in (11.30) (the cos(𝜏) terms
that arise from x0 and lead to a resonance
eﬀect) can then be eliminated by setting
𝜔1A = 0; because A = 0 will produce a
trivial solution, 𝜔1 = 0 is used instead.
Finally, the elimination of the secular
terms in (11.31) (both cos(𝜏) and sin(𝜏)
terms) gives two constraints
𝜇A −1
4A3 = 0
and
𝜔2A = 0.
(11.32)
Again, seeking a nontrivial solution for
A gives A = 2√𝜇and 𝜔2 = 0. Thus, as
expected, the amplitude of the limit cycle
grows proportionally to √𝜇. A comparison
of the perturbation solution and numerical
simulations give excellent agreement as
shown in Figure 11.6.
11.2.10
Pitchfork Bifurcation
The pitchfork bifurcation is a nongeneric
bifurcation (it is of codimension 3 [10]);
however, for systems with ℤ2 symmetry,
it becomes generic. Many physical systems
possess this symmetry, at least approxi-
mately, and so it is often of interest. A com-
mon example is the buckling of a simple
Euler strut.
The pitchfork bifurcation is also known
as a symmetry breaking bifurcation because
before the bifurcation the equilibrium solu-
tion is invariant under the action of the
symmetry group, whereas the bifurcating
solutions are not. A simple example is
dx
dt = 𝜇x −𝛼x3.
(11.33)
As
with
a
saddle-node
bifurcation,
a
pitchfork bifurcation is characterized by
an eigenvalue passing through zero. For
𝛼= +1 and 𝜇< 0, this equation has a
0
0
0.5
Amplitude
1
1.5
0.2
0.4
μ
Figure 11.6
The growth of a limit cycle
from the Hopf bifurcation of the van der
Pol equation (11.26) as 𝜇varies; the solid
curve is the perturbation solution, whereas
the dots are from numerical simulations.

11.2 Equilibria
397
x
(a)
x
(b)
𝜇
𝜇
−0.5
0.5
0.5
−1
−1
1
1
−0.5
0
0
−1
1
0
0
Figure 11.7
One-parameter
bifurcation diagrams for (11.33)
showing the existence of a
pitchfork bifurcation at 𝜇= 0
and x = 0. Stable equilibria
are denoted by solid curves
and unstable equilibria are
denoted by dashed curves. (a)
The supercritical case (𝛼= 1)
and panel (b) the subcritical
case (𝛼= −1).
single (stable) equilibrium at x = 0 and it
undergoes a pitchfork bifurcation at 𝜇= 0,
which results in three equilibria, two stable
(x = ±√𝜇) and one unstable (x = 0); this
is the supercritical case. Alternatively, for
𝛼= −1 and 𝜇< 0 this equation has three
equilibria, two unstable (x = ±√−𝜇) and
one stable (x = 0), and it undergoes a pitch-
fork bifurcation at 𝜇= 0, which results in a
single unstable equilibrium at x = 0; this is
the subcritical case.
The
supercritical
case
is
shown
in
Figure 11.7a and the subcritical case in
Figure 11.7b.
The pitchfork bifurcation theorem is as
follows.
Theorem 11.3 Consider
the
one-
dimensional dynamical system
dx
dt = f (x, 𝜇),
x ∈ℝ, 𝜇∈ℝ,
(11.34)
where f
is smooth, has ℤ2 symmetry
and has an equilibrium x = 0 at 𝜇= 0.
Furthermore, let 𝜆= fx(0, 0) = 0. If the
genericity conditions (i) fxxx(0, 0) ≠0 and
(ii) fx𝜇≠0 are satisﬁed, then (11.34) is
topologically equivalent near the origin to
the normal form
dy
dt = 𝛼y ± y3,
(11.35)
where the sign of the y3 term determines the
criticality of the bifurcation.
Again, higher-dimensional systems can
be reduced to this one-dimensional form
using a center manifold reduction.
As many physical systems only pos-
sess the ℤ2 symmetry approximately, the
unfoldings of the pitchfork bifurcation
become important. Arbitrary asymmetric
perturbations to (11.33) give rise to
dx
dt = 𝜇x −x3 + 𝛼1 + 𝛼2x2,
(11.36)
where 𝛼1 and 𝛼2 are unfolding parameters.
(In
general,
a
codimension-n
bifurca-
tion requires n parameters to unfold the
behavior.) The unfolding of the pitchfork
bifurcation reveals the bifurcation scenar-
ios that can occur when a system is close
to having a pitchfork bifurcation. These
scenarios are shown in Figure 11.8. For
small values of 𝛼1 and 𝛼2, the pitchfork
bifurcation typically unfolds into a single
saddle-node bifurcation and a discon-
nected branch of solutions as shown in
Figure 11.8a and d. However, when 𝛼1
lies between 0 and 𝛼3
2∕27, the pitchfork
bifurcation unfolds into three saddle-node
bifurcations
as
shown
in
Figure 11.8b
and c.
This type of unfolding procedure is
important for higher codimension bifurca-
tions because they often act as organizing
centers for the dynamics, that is, while
the higher codimension bifurcations may

398
11 Dynamical Systems
x
(a)
(b)
x
(c)
(d)
𝜇
𝜇
Figure 11.8
The four diﬀerent unfoldings
of the pitchfork bifurcation. In (a) and (d)
the pitchfork bifurcation unfolds into a sin-
gle saddle-node bifurcation. In contrast, in
(b) and (c) it unfolds into three saddle-node
bifurcations. The parameter ranges in (11.36)
corresponding to the diﬀerent unfoldings are
(a) 𝛼1 > 0 and 𝛼1 > 𝛼3
2∕27, (b) 𝛼1 > 0 and 𝛼1 <
𝛼3
2∕27, (c) 𝛼1 < 0 and 𝛼1 > 𝛼3
2∕27, (d) 𝛼1 < 0
and 𝛼1 < 𝛼3
2∕27.
never be seen in the system of interest,
their proximity may be obvious.
11.2.11
Center Manifolds
Consider the two systems of diﬀerential
equations
dx
dt = xy,
dy
dt = −y −x2,
(11.37)
and
dx
dt = x2y −x5,
dy
dt = −y + x2.
(11.38)
Both of these systems have an equilibrium
at x = y = 0 and the corresponding Jaco-
bian matrices are identical:
J =
[0
0
0
−1
]
.
(11.39)
As
J
has
an
eigenvalue
at
0,
the
Hartman–Grobman theorem fails. Thus
the stability of the equilibria cannot be
deduced from the Jacobian. In this case,
the equilibrium of (11.37) is asymptotically
stable, whereas the equilibrium of (11.38)
is asymptotically unstable.
Center manifold theory is the means by
which the stability of (11.37) and (11.38)
can be determined.
Consider the system
dx
dt = Ax + f (x, y)
dy
dt = By + g(x, y),
(11.40)
where x ∈ℝn, y ∈ℝm and A and B are
matrices such that the eigenvalues of A have
zero real parts and the eigenvalues of B
have negative real parts.
To put a given system into the form
of (11.40) is simply a case of applying a
sequence of coordinate transformations
that ﬁrst shift the equilibrium of interest to
the origin and then second put the system
into Jordan normal form.

11.3 Limit Cycles
399
If f ≡g ≡0, then (11.40) contains two
trivial invariant manifolds: x = 0 is the
stable manifold of the equilibrium, and
y = 0 is the center manifold of the equi-
librium. Furthermore, the dynamics of
the system will collapse exponentially
quickly onto the center manifold and thus
the dynamics on the center manifold will
determine the stability of the equilibrium.
In general, with nonzero f and g, there
exists a center manifold of (11.40) that can
be written as y = h(x) where h(0) = 0 and
h′(0) = 0 [11]. While this does not permit
a direct solution for the dynamics on the
center manifold, it does permit a polyno-
mial approximation to be calculated to any
desired order.
Take (11.37) and consider the polynomial
expansion of h to ﬁfth order, that is, y =
h(x) = ax2 + bx3 + cx4 + dx5 + O(x6). Sub-
stituting this into dy∕dt gives
dy
dt = h′(x)dx
dt = xh′(x)h(x),
= 2a2x4 + 5abx5 + O(x6).
(11.41)
Similarly, substituting the same expan-
sion into the right-hand side of the diﬀer-
ential equation governing y in (11.37) gives
dy
dt = −h(x) −x2,
= −(a + 1)x2 −bx3 −cx4 −dx5 + O(x6).
(11.42)
Equating the coeﬃcients of both expres-
sions leads to a = −1, b = 0, c = −2, and
d = 0. Thus the center manifold approxi-
mation is y = h(x) = −x2 −2x4 + O(x6) and
the resulting dynamics on the center mani-
fold are
dx
dt = −x3 −2x5 + O(x7).
(11.43)
Hence the equilibrium at x = 0, y = 0 is
asymptotically stable.
A similar calculation can be performed
for (11.38) to show that the equilibrium is
asymptotically unstable.
11.3
Limit Cycles
11.3.1
Deﬁnition and Calculation
Once again consider the ODE
dx
dt = f (x, 𝜇),
x ∈ℝn, 𝜇∈ℝ, (11.44)
where f ∶ℝn × ℝ→ℝn. Many of the con-
cepts developed for equilibria of (11.44)
apply directly to limit cycles of (11.44) as
well, although the underlying proofs of
the corresponding theorems may be quite
diﬀerent.
A limit cycle is an isolated closed orbit of
(11.44) that contains no rest points (points
where dx∕dt = 0). Any solution that passes
through a point on the limit cycle is a peri-
odic solution and has the property that
x(t + T) = x(t) for all t, where the period
is T.
As limit cycles are a function of time,
they are typically more diﬃcult to calcu-
late than equilibria. However, in the case of
weak nonlinearities, perturbation methods
may give a reasonable approximation to a
limit cycle, for example the method of mul-
tiple scales is often used to good eﬀect. (In
particular, when analyzing the limit cycles
emerging from a Hopf bifurcation; see for
example, Section 11.2.9.)
11.3.1.1
Harmonic Balance Method
Another
analytical
approach
is
the
harmonic
balance
method,
whereby
a
harmonic solution to the equations of
motion is
postulated,
substituted into
the ODE of interest, and the resulting

400
11 Dynamical Systems
expression expanded in terms of sin(n𝜔t)
and cos(n𝜔t). The orthogonality of the
sin and cos terms over one period gives
a number of algebraic equations that can
then be solved. For example, consider the
periodically forced Duﬃng equation
y′′ + 𝜇y′ + 𝛼y + 𝛽y3 = Γ cos(𝜔t).
(11.45)
Assume a solution in the form of a
truncated
Fourier
series
with
coef-
ﬁcients
that
slowly
vary
with
time
y = p(t) + a(t) cos(𝜔t) + b(t) sin(𝜔t).
The
ﬁrst and second derivatives become
y′ = p′ + (a′ + b𝜔) cos(𝜔t)
+ (b′ −a𝜔) sin(𝜔t),
(11.46a)
y′′ = p′′ + (2b′ −a𝜔) 𝜔cos(𝜔t)
+ (b𝜔−2a′) 𝜔sin(𝜔t),
(11.46b)
where it is presumed that p′′ = a′′ = b′′ =
0 owing to the slowly varying approxima-
tion. The expressions for y, y′, and y′′ are
then substituted into (11.45) and the con-
stant coeﬃcients, along with the coeﬃ-
cients of cos(𝜔t) and sin(𝜔t), are balanced
on each side. Balancing the constant coeﬃ-
cients gives
𝜇p′ = −p
(
𝛽p2 + 𝛼+ 3
2𝛽r2)
,
(11.47)
and balancing the coeﬃcients of cos(𝜔t)
and sin(𝜔t) gives
( 𝜇
2𝜔
−2𝜔
𝜇
) (a′
b′
)
=
⎛
⎜
⎜⎝
a
(
𝜔2 −𝛼−3𝛽p2 −3
4𝛽r2)
−𝜇𝜔b + Γ
b
(
𝜔2 −𝛼−3𝛽p2 −3
4𝛽r2)
+ 𝜇𝜔a
⎞
⎟
⎟⎠
,
(11.48)
where
r2 = a2 + b2.
The
steady-state
response, ̃p, ̃r, is obtained from the equilib-
ria of (11.47) and (11.48), which requires
a′ = b′ = p′ = 0; squaring and summing
the result from (11.48) gives the frequency
response equation
[(
𝜔2 + 𝛼−3𝛽̃p2 −3
4𝛽̃r2)2
+ (𝜇𝜔)2
]
̃r2 =Γ2,
(11.49)
where ̃p may take on multiple values. More
speciﬁcally, after setting p′ = 0 in (11.47),
the steady-state solution ̃p reveals that both
̃p = 0 and ̃p2 = −𝛼∕𝛽−3∕2̃r2 are equilib-
ria. When the Duﬃng equation is bistable,
where 𝛼= −1 and 𝛽= 1, the latter solution
is interesting because it restricts the values
of ̃r that provide a physical solution for ̃p to
̃r2 ≤2∕3.
The harmonic balance method is par-
ticularly useful in the case of the bistable
Duﬃng equation because this is a situa-
tion in which perturbation methods are
not applicable owing to the strength of the
nonlinearity. However, higher-order har-
monic approximations often result in alge-
braic equations that can only be solved
numerically and so the harmonic balance
method is often limited to the ﬁrst-order
equations.
In general, when dealing with strong
nonlinearities or limit cycles that are far
from harmonic, numerical methods are
the way forward. There are many diﬀer-
ent methods for calculating limit cycles
numerically; two common methods dis-
cussed here are numerical shooting and
collocation. Both methods treat the prob-
lem of calculating a limit cycle as a periodic
boundary value problem (BVP).
11.3.1.2
Numerical Shooting
Numerical shooting is a straightforward
method for solving BVPs where a numer-
ical integrator is available to calculate
solutions of (11.44) for given initial con-
ditions. For a limit cycle, the BVP of

11.3 Limit Cycles
401
interest is
dx
dt = f (x, 𝜇),
with
x(0) −x(T) = 0,
(11.50)
where the period of the limit cycle is T.
However, (11.50) is not directly amenable to
numerical methods. To calculate a particu-
lar limit cycle, time in (11.50) must ﬁrst be
rescaled so that the period T appears as an
explicit parameter to be determined. This
results in
dx
dt = Tf (x, 𝜇),
with
x(0) −x(1) = 0.
(11.51)
Furthermore, owing to the autonomous
nature of (11.51), there exists a one-
parameter family of solutions that are
invariant under time (phase) shifts. As
such a phase condition is needed to restrict
the problem to ﬁnding an isolated limit
cycle and so ensure the correct behavior of
the numerical method.
There are a wide range of possible phase
conditions. For simple implementations, it
may be desirable to use known proper-
ties of the system of interest. For example,
if all limit cycles of interest pass through
the point x = C, then the phase condition
x(0) = C may be used. A more robust, but
harder to implement, condition is x(0) =
x(1∕2) which holds for all limit cycles; how-
ever, this condition is still not completely
robust and may become singular.
The most common and robust phase con-
dition is the integral condition
∫
1
0
d̂x
dt ⋅[x(t) −̂x(t)]dt = 0,
(11.52)
which minimizes the 2 distance between
the limit cycle and a reference limit cycle ̂x
(typically taken as the last successfully com-
puted limit cycle). The condition (11.52) is
implemented in many bifurcation analysis
packages.
The ﬁnal nonlinear BVP to be solved is
the system formed by (11.51) and (11.52)
where the time series of x is calculated by
numerical integration from the initial con-
dition x(0). The BVP is then solved using
a nonlinear root-ﬁnding method such as
a Newton iteration starting with an initial
guess {x(0)(0), T0}:
[x(i+1)(0)
T(i+1)
]
=
[x(i)(0)
T(i)
]
−J−1
[
x(i)(0) −x(i)(1)
∫
1
0
d̂x
dt ⋅[x(i)(t) −̂x(t)]dt
]
,
(11.53)
where J is the corresponding Jacobian
matrix, which can be approximated by
a ﬁnite-diﬀerence scheme or calculated
directly from the variational equations
(see Section 11.3.2). The integral phase
condition can be discretized using a simple
scheme such as the trapezoid method
without a loss of accuracy.
Numerical shooting is suitable for both
stable and unstable limit cycles, though
increasingly
better
initial
guesses
are
required as the limit cycle becomes more
unstable. To overcome stability problems,
multiple shooting is often used.
11.3.1.3
Collocation
Collocation is another method for solving
BVPs of the form (11.51). Similar to numer-
ical shooting, a phase condition (similar
to (11.52)) is required. Collocation diﬀers
from numerical shooting in how the time
series is computed/represented.
In collocation, the time series is approx-
imated as a series expansion in terms
of
certain
basis
functions
that
can
be
diﬀerentiated
exactly,
for
example,
x(t) = ∑n
i=0 xiPi(t) in the case of Lagrange

402
11 Dynamical Systems
polynomials. Common examples include
harmonic
functions
(Fourier
series),
piecewise-deﬁned Lagrange polynomials,
and
globally
deﬁned
Chebyshev
poly-
nomials. This series expansion is then
substituted into the system of interest and
evaluated at discrete-time points (colloca-
tion points). The idea is to determine the
coeﬃcients of the basis functions (using a
nonlinear root ﬁnder) such that, at these
collocation points, the series expansion
of the limit cycle exactly satisﬁes (11.51)
and (11.52); in general, these are the only
points where they are satisﬁed exactly
and so globally the series expansion is an
approximate solution.
(In contrast, a Galerkin-based method
would use a similar series expansion of
the solution but, instead of pointwise eval-
uations of (11.51) and (11.52), integrals
over the whole period would be calculated.
When the integrals can be calculated ana-
lytically, this has beneﬁts over collocation.
However, when the integrals must be calcu-
lated numerically using quadrature, these
beneﬁts disappear.)
For more details see [12] or [13].
11.3.2
Linearization
Following
the
treatment
of
equilibria
in Section 11.2.3, the dynamical system
deﬁned by (11.44) can similarly be lin-
earized in the vicinity of a limit cycle x∗(t).
The linearization is inherently time varying
and is given by
d̃x
dt = J(t)̃x(t),
(11.54)
where J is the (time-varying) Jacobian given
by
J(t) = [Ji,j(t)]|||x(t)=x∗(t) =
[ 𝜕fi
𝜕xj
]|||||x(t)=x∗(t)
.
(11.55)
Equation (11.54) is also known as the ﬁrst
variational equation and can be derived
formally by considering the growth/decay
of small perturbations to the limit cycle.
To determine the stability of a limit cycle
with period T, a suitable discrete time
map is constructed, the ﬁxed points of
which correspond to the limit cycles of the
original system. There are two methods
for constructing the discrete time map
(both equivalent); these are illustrated in
Figure 11.9. Either consider the time-T
map that evolves the point x(t) to x(t + T),
or alternatively consider the dynamics
of a map from an arbitrary section that
intersects with the limit cycle, back to itself
(a so-called Poincaré map and Poincaré
section). When either of the maps are
linearized, the end result is a map of the
form ̃x(i+1) = M̃x(i) where M is the Jacobian
matrix of the map; M is also known as the
monodromy matrix. The stability of the
ﬁxed point (and correspondingly the limit
cycle) is determined by the eigenvalues of
M, which are known as Floquet multipliers.
(a)
(c)
(b)
Σ
Figure 11.9
A schematic of the discrete-time maps associated
with the linearization of a limit cycle. Point (a) is a starting point
near a given limit cycle (marked by the dashed closed curve). It is
then either evolved forward in time by T time units to point (b) as
with the time-T map, or it is evolved forward in time until it hits
the Poincaré section Σ once more at point (c) as with the Poincaré
map.

11.3 Limit Cycles
403
A limit cycle is linearly stable if all the
Floquet multipliers lie inside the unit circle
in the complex plane. Furthermore, a limit
cycle is hyperbolic if no Floquet multipliers
lie on the unit circle.
(Note that the time-T map is a mapping
from ℝn to ℝn, whereas the Poincaré map is
a mapping from ℝn−1 to ℝn−1; the extra Flo-
quet multiplier associated with the time-T
map is a so-called trivial multiplier and will
always be +1 because the limit cycle can be
phase-shifted arbitrarily.)
In the case of the time-T map, the mon-
odromy matrix M can be calculated by inte-
grating the ﬁrst variational equation (11.54)
with the initial condition ̃x(0) = I where I is
the n × n identity matrix. The matrix M is
then the matrix ̃x(T).
11.3.3
Topological Equivalence
In the vicinity of a hyperbolic equilibrium,
the
notion
of
topological
equivalence
combined with the Hartman–Grobman
theorem is a powerful one. It states that
the dynamics near an equilibrium are
equivalent to those of the corresponding
linearization. The straightforward calcula-
tion of stability is one of the possibilities
that emerges from this result.
For limit cycles, a similar proposition
holds; the dynamics in the neighborhood
of a hyperbolic limit cycle (none of its Flo-
quet multipliers lie on the unit circle) are
topologically equivalent to the linearized
dynamics given by the ﬁrst variational
equation. This follows from the discrete-
time version of the Hartman–Grobman
theorem for the ﬁxed points of a map,
because for any limit cycle, a time-T
map or a Poincaré map can be deﬁned as
described in Section 11.3.2.
11.3.4
Manifolds
By analogy with equilibria (see (11.15) and
(11.16)), stable and unstable manifolds for
a limit cycle can be deﬁned as the set of
points that approach the limit cycle in for-
ward time and backward time, respectively.
An important diﬀerence between the
manifolds
of
an
equilibrium
and
the
manifolds of a limit cycle is their respec-
tive dimensions. For an equilibrium, the
dimension of the stable manifold (unstable
manifold) is given by the number of eigen-
values with negative (positive) real parts.
For a limit cycle in an autonomous system,
the dimensions of the stable and unstable
manifolds are increased by one due to the
presence of the trivial multiplier at +1.
Figure 11.10 shows the two possible
conﬁgurations for the stable and unstable
manifolds of a saddle-type limit cycle in
three-dimensional space. The ﬁrst case
(a)
(b)
Figure 11.10
Two diﬀerent conﬁgurations of
two-dimensional manifolds in three-dimensional
space of a saddle-type limit cycle (marked by a
thick black curve). (a) The case where the Flo-
quet multipliers corresponding to the manifolds
are positive. (b) The case where the Floquet
multipliers are negative and the resulting man-
ifold is twisted, having the form of a Möbius
strip. (Only one manifold is shown in (b) for
clarity.) A representative orbit is marked in (b)
by a dashed curve.

404
11 Dynamical Systems
is when the Floquet multipliers of both
manifolds are positive; the manifolds form
simple bands around the limit cycle. The
second case is when the Floquet multipliers
of both manifolds are negative; in this case,
both the manifolds form twisted bands
around the limit cycle.
It should be noted that it is not possible
(in three dimensions) to have a twisted sta-
ble (unstable) manifold and an untwisted
unstable (stable) manifold. This can proven
directly from Liouville’s formula, which
ensures that the product of all the Floquet
multipliers is positive.
A local (linear) approximations to the sta-
ble and unstable manifolds at a single point
on the limit cycle are given by the eigenvec-
tors of the monodromy matrix. These local
approximations can then be extended to
the complete limit cycle by integrating for-
ward the ﬁrst variational equation (11.54)
for time T, using the (normalized) eigen-
vector as the initial condition. Globaliza-
tion of the manifolds can then be achieved
using the same numerical methods as men-
tioned in Section 11.2.6.
11.3.5
Local Bifurcations
It follows from the discrete-time Hart-
man–Grobman theorem that the dynamics
in the neighborhood of a limit cycle can
only
change
qualitatively
(i.e.,
topo-
logically)
if
the
limit
cycle
becomes
nonhyperbolic; that is, a Floquet mul-
tiplier lies on the unit circle. Thus, as with
equilibria, a nonhyperbolic limit cycle
signiﬁes a local bifurcation.
Limit cycles can undergo the same basic
bifurcations as equilibria can. In particular,
the saddle-node and pitchfork bifurcations
are essentially the same with limit cycles
taking the place of the equilibria. These
occur when a Floquet multiplier passes
through the unit circle in the complex
plane at +1. (Again, the pitchfork bifurca-
tion requires ℤ2 symmetry for it to be a
generic bifurcation.)
Hopf bifurcations (called secondary Hopf
bifurcations or Neimark-Sacker bifurca-
tions) of limit cycles also occur. They are
associated with the onset of quasi-periodic
motion (the existence of an invariant torus)
and they occur when a complex conjugate
pair of eigenvalues passes through the unit
circle. The details of the bifurcation are
signiﬁcantly complicated if strong reso-
nances occur between the frequency of the
underlying limit cycle and the frequency of
the bifurcation. The details of this bifurca-
tion are beyond the scope of this chapter
and
the
interested
reader
is
referred
to [4].
Finally, limit cycles can undergo one
ﬁnal (generic) local bifurcation known
as a period-doubling bifurcation, which
occurs when a single Floquet multiplier
passes through the unit circle at −1.
Period-doubling
bifurcations
are
often
associated with the onset of chaos. In
particular, a period-doubling cascade (a
sequence of period-doubling bifurcations
one after the other) is a well known route to
chaos.
11.3.6
Period-Doubling Bifurcation
At a period-doubling bifurcation, a limit
cycle becomes nonhyperbolic as a Floquet
multiplier passes through the unit circle in
the complex plane at −1. At the bifurcation
point, a new limit cycle is created that has
a period of twice that of the original limit
cycle and, furthermore, the original limit
cycle changes stability.
As with Hopf and pitchfork bifurcations,
a period-doubling bifurcation can be sub-
or supercritical. In the subcritical case,

11.4 Numerical Continuation
405
an unstable limit cycle becomes stable at
the bifurcation point and coexists with
an unstable period-doubled limit cycle. In
the supercritical case, a stable limit cycle
becomes unstable at the bifurcation point
and coexists with a stable period-doubled
limit cycle.
Period-doubling bifurcations are often
a precursor to chaos. For example, the
Rössler system,
dx
dt = −y −z,
dy
dt = x + ay,
dz
dt = b + z(x −c),
(11.56)
becomes chaotic through a sequence of
supercritical period-doubling bifurcations
as shown in Figure 11.11. It should be noted
that an inﬁnite number of period-doubling
bifurcations exist in a ﬁnite parameter
interval. The parameter values at which
the period-doubling bifurcations occur
tend toward a geometric progression as
the period increases; the ratio of the dis-
tances between successive period-doubling
bifurcations is known as Feigenbaum’s
constant.
In the midst of the chaotic attractor
shown in Figure 11.11, a period-3 orbit
becomes stable. It then also undergoes a
period-doubling sequence before the sys-
tem becomes fully chaotic again. Windows
of periodic behavior like this are typical
in chaotic attractors of smooth dynamical
systems.
11.4
Numerical Continuation
11.4.1
Natural Parameter Continuation
The preceding sections have expounded the
diﬀerent dynamical eﬀects that can occur
as parameter values are changed in a sys-
tem. If the system of interest is suﬃciently
simple, it is enough to ﬁnd the equilibria
analytically and compute quantities such as
the Jacobian to determine when bifurca-
tions occur.
Many
nontrivial
systems
are
not
amenable to such analysis and require the
application of numerical methods from the
start. In such cases, numerical simulation
is often the ﬁrst recourse, swiftly followed
by brute-force bifurcation diagrams such
as Figure 11.11 in Section 11.3.6. A great
deal of information can be gathered using
these simple techniques.
However, such use of numerical sim-
ulation does not reveal much about the
nature of the bifurcations and transitions
that occur as the parameters are varied
because unstable invariant sets are missed.
It is branches of these unstable invariant
0
10
20
30
5
15
20
x
c
10
Figure 11.11
A one-parameter brute-
force bifurcation diagram of the Rössler sys-
tem (11.56) for a = b = 0.1 showing a period-
doubling route to chaos. At each parameter
value, the system is simulated until it reaches
steady state and then the values of x at which
the orbit intersects the Poincaré map deﬁned
by dx∕dt = 0 are recorded.

406
11 Dynamical Systems
sets that connect the diﬀerent branches of
stable invariant sets and so reveal what
bifurcations are occurring.
Numerical continuation is a means to
overcome these diﬃculties. At heart, it is a
path-following method that tracks the solu-
tions of
f (x, 𝜇) = 0,
x ∈ℝn, 𝜇∈ℝp,
(11.57)
as the system parameters 𝜇vary. This
type of problem arises naturally from
equilibrium problems but many other
types of problem can also be put into
this form, for example, the calculation of
limit cycles using numerical shooting or
collocation.
Numerical continuation relies on the
implicit function theorem, which states
that the solutions of (11.57) can be written
explicitly as x = g(𝜇) provided the matrix
of partial derivatives of f with respect to x
is not singular (i.e., the system is not at a
bifurcation point). Thus, when the implicit
function theorem holds, a continuous
branch of solutions can be tracked with
respect to 𝜇.
A simple version of numerical contin-
uation is an iterative, predictor-corrector
algorithm called natural parameter con-
tinuation. The algorithm is as follows. To
start the algorithm an initial point {x0, 𝜇0}
is required, which can be generated using
numerical
simulation
(or
some
other
method).
1. Take a step in the system parameter of
interest to get 𝜇i+1 = 𝜇i + Δ.
2. Generate a predicted solution ̃xi+1 of
(11.57) for 𝜇= 𝜇i+1 from the previous
solution xi.
3. Generate a corrected solution xi+1 by
using a nonlinear root ﬁnder (e.g., a
Newton iteration) on f (x, 𝜇i+1) = 0
using ̃xi+1 as an initial guess for the root
ﬁnder.
Repeat this process until the desired
parameter range is covered; Δ should be
suﬃciently small to ensure convergence
of the nonlinear root ﬁnder and to reveal
the desired level of detail in the solution
branch.
As the calculation of solutions uses a
nonlinear root ﬁnder rather than numeri-
cal simulation, it does not matter whether
the solutions are stable or unstable. In fact,
to determine the stability of equilibria, the
Jacobian matrix must be computed sepa-
rately. Similarly, for limit cycles the calcu-
lation of the monodromy matrix is also a
separate step. (In practice, it is often possi-
ble to use the Jacobian previously calculated
by the Newton iteration for stability infor-
mation.)
There are a number of diﬀerent ways to
generate predicted solutions. Two common
ways are tangent prediction and secant pre-
diction. Tangent prediction calculates the
local derivative v = f𝜇(xi, 𝜇i) and applies the
prediction ̃xi+1 = xi + Δv. Secant predic-
tion, on the other hand, estimates the local
derivative using a secant estimation, that is,
̃v = (xi −xi−1)∕(𝜇i −𝜇i−1), and then calcu-
lates the predicted solution as ̃xi+1 = xi +
Δ̃v. Secant prediction is simpler in that it
does not require any derivative informa-
tion; however, it requires two initial points
to start the algorithm.
Figure 11.12 shows an example of natural
parameter continuation using the Rössler
system
(compare
with
Figure 11.11).
The
limit
cycle
is
discretized
using
collocation.
Natural
parameter
continuation
is
inherently ﬂawed. As it relies directly on
the implicit function theorem, it fails at
saddle-node
bifurcations
because
past

11.4 Numerical Continuation
407
0
5
5
10
10
15
15
20
x
(a)
(b) (c)
c
Figure 11.12
Numerical
continuation of limit cycles in
the Rössler system. A series of
period-doubling bifurcations
occur and the bifurcating
branches are continued up to
the period 8 limit cycle. The
ﬁrst three period-doubling
bifurcations are marked by
(a), (b) and (c). (Compare with
Figure 11.11.)
the bifurcation point, there is no solu-
tion for x. Instead, a diﬀerent approach is
required.
11.4.2
Pseudo-Arc-Length Continuation
Pseudo-arc-length continuation overcomes
the diﬃculties posed by saddle-node bifur-
cations by reparameterizing the zero prob-
lem (11.57) in terms of the arc length along
the solution curve. By doing so, the implicit
function theorem no longer fails at saddle-
node bifurcations. Thus the zero problem
becomes
f (x(s), 𝜇(s)) = 0,
x ∈ℝn, 𝜇∈ℝp, (11.58)
and both x and 𝜇are solved for simultane-
ously in terms of the arc length. To do this,
an additional equation is required (because
𝜇is now an additional unknown). A true
arc-length parameterization would require
the addition of the nonlinear algebraic
equation (ds)2 = (dx)2 + (d𝜇)2; in practice
it is better to use a linear approximation
to this nonlinear equation, namely, the
pseudo-arc-length equation
(̂x′)T(x −̂x) + ( ̂𝜇′)T(𝜇−̂𝜇) = Δ,
(11.59)
where ̂x and ̂𝜇are previously computed
solutions and the primes ()′ denote dif-
ferentiation with respect to arc length.
(Secant approximations to the derivatives
with respect to arc-length work as well as
the actual derivatives.)
The
pseudo-arc-length
continuation
algorithm is thus as follows. (Use ̂x = xi
and ̂𝜇= 𝜇i in (11.59).)
1. Generate a predicted solution
{̃xi+1, ̃𝜇i+1} of (11.58) and (11.59) from
the previous solution {xi, 𝜇i}.
2. Generate a corrected solution
{xi+1, 𝜇i+1} by using a nonlinear root
ﬁnder (e.g., a Newton iteration) on the
combined system of (11.58) and (11.59)
using {̃xi+1, ̃𝜇i+1} as an initial guess for
the root ﬁnder.
This is illustrated in contrast to natu-
ral parameter continuation in Figure 11.13.
The prediction step is the same as before
except that derivatives are now with respect
to the arc length and predictions are needed
for 𝜇i+1 as well as xi+1.
11.4.3
Continuation of Bifurcations
As well as tracking invariant sets of a
dynamical system in general, it is also
possible to track bifurcations in terms of

408
11 Dynamical Systems
(a)
(b)
x
xi
xi
xi + 1
xi − 1
xi − 1
x
Figure 11.13
(a), (b) Natural parameter con-
tinuation and pseudo-arc-length continuation,
respectively, close to a saddle-node bifurca-
tion. Predicted solutions are denoted by open
circles and corrected solutions are denoted
by dots. The dashed lines denote the tangent
line x′
i (using a secant approximation) and the
dotted lines indicate the search direction of
the nonlinear root ﬁnder. The addition of the
pseudo-arc-length equation (11.59) in (b) skews
the search direction in order to pass around
the saddle-node bifurcation.
the system parameters. For example, it is
possible to track a saddle-node bifurca-
tion as two parameters vary to trace out
bifurcation diagrams as in Figure 11.4. All
that is required is to augment the zero
problem of (11.58) and (11.59) by a third
equation that encompasses the bifurcation
condition. This third equation is known as
a bifurcation test function.
A saddle-node bifurcation of equilibria
occurs when an eigenvalue of the Jacobian
matrix passes through zero. As such, a suit-
able test function is
det(J) = 0,
(11.60)
where J is the Jacobian matrix associ-
ated with the equilibrium. While this
is conceptually a simple test function,
for numerical purposes, it is less useful
because derivatives of this function are
not straightforward to calculate. (A ﬁnite-
diﬀerence approximation can be used but
this can be computationally expensive
to calculate for large systems.) Instead, a
better approach is to track the eigenvector
corresponding to the zero eigenvalue. As
eigenvectors can be scaled arbitrarily, a
further condition is required to normal-
ize the eigenvector. Thus the system of
equations needed to continue saddle-node
bifurcations of equilibria is formed from
(11.58), (11.59) and
Jv = 0,
‖v‖ = 1,
(11.61)
where v is the eigenvector corresponding to
the zero eigenvalue. Thus the variables to
solve for are x, 𝜇1, 𝜇2, and v. (Two param-
eters must be allowed to vary to continue a
curve of saddle-node bifurcations.)
For test functions of other bifurca-
tions, and the bifurcations of limit cycles,
see [3, 4].
References
1. Arnold, L. (1998) Random Dynamical
Systems, Springer-Verlag, Berlin.
2. Strogatz, S.H. (1994) Nonlinear Dynamics
and Chaos, Perseus, New York.
3. Seydel, R. (2010) Practical Bifurcation and
Stability Analysis, Interdisciplinary Applied
Mathematics, vol. 5, 3rd edn, Springer-
Verlag, New York.
4. Kuznetsov, Y.A. (1998) Elements of Applied
Bifurcation Theory, Applied Mathematical

References
409
Sciences, 2nd edn, Springer-Verlag, New
York.
5. Hsu, C.S. (1987) Cell-to-Cell Mapping: A
Method of Global Analysis for Nonlinear
Systems, Springer-Verlag.
6. Dellnitz, M. and Hohmann, A. (1997) A
subdivision algorithm for the computation of
unstable manifolds and global attractors.
Numer. Math., 75 (3), 293–317.
7. Krauskopf, B., Osinga, H.M., Doedel, E.J.,
Henderson, M.E., Guckenheimer, J.,
Vladimirsky, A., Dellnitz, M., and Junge, O.
(2005) A survey of methods for computing
(un)stable manifolds of vector ﬁelds. Int. J.
Bifurcation Chaos, 15 (3), 763–791.
8. Bender, C.M. and Orszag, S.A. (1978)
Advanced Mathematical Methods
for Scientists and Engineers,
Springer-Verlag.
9. Murdock, J. (2013) Perturbation Methods,
John Wiley & Sons, Ltd.
10. Golubitsky, M. and Schaeﬀer, D.G. (1985)
Singularities and Groups in Bifurcation
Theory, Applied Mathematical Sciences (eds
F. John, J.E. Marsden, and L. Sirovich),
Springer-Verlag, Berlin.
11. Carr, J. (1981) Applications of Centre
Manifold Theory, Springer-Verlag.
12. Trefethen, L.N. (2000) Spectral Methods in
Matlab, SIAM.
13. Boyd, J.P. (2001) Chebyshev and Fourier
Spectral Methods, 2nd edn, Dover, New York.


411
12
Perturbation Methods
James Murdock
12.1
Introduction
Perturbation theory arises when a situation
is given that admits of a mathematical
description, and one asks how this descrip-
tion changes when the situation is varied
slightly or “perturbed.” This could result
either in a continuation of the original situ-
ation with only small quantitative changes
or in an abrupt qualitative change in the
nature of the situation. Among the possible
“abrupt” changes are the formation of a
transition layer and the creation of various
types of bifurcations; although bifurcation
theory is usually treated as a separate
subject
from
perturbation
theory,
the
two areas are closely related. The speciﬁc
subject matter of this chapter will be the
following two topics.
1. A system of ordinary or partial
diﬀerential equations is given, together
with initial or boundary conditions. The
system contains a small parameter, and
is explicitly solvable when the
parameter is zero. One asks how to
construct approximate solutions (in
explicit analytic form) when the
parameter is small but nonzero; one
asks for error estimates for these
approximate solutions, and whether the
approximate solutions exhibit the same
qualitative behavior as the unknown
exact solutions.
2. A matrix or linear transformation is
given, depending on a small parameter.
The eigenvalues and eigenvectors (or
the Jordan normal form) are known
when the parameter is zero, and one
asks for approximate calculations of the
eigenvalues or normal form when the
parameter is small but nonzero.
The origins of perturbation theory lie in
three classical problems, planetary motion,
viscous ﬂuid ﬂow past a wall, and changes
in the spectrum as a matrix or linear
operator is varied. The present chapter is
structured in the same threefold way: after
an initial section presenting basic concepts
common to the three areas, we take up
in turn dynamical systems (Section 12.3),
transition layer problems (Section 12.4),
and spectra (Section 12.1.5); the inter-
vening Section 12.5 deals with a recent
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

412
12 Perturbation Methods
method applicable to the problems of both
Section 12.3 and 12.4. To conclude this
introduction, we brieﬂy describe the three
classical problems.
Isaac Newton showed that the inverse
square law of gravitational force implies
that a single planet will move around the
sun in an ellipse, satisfying Kepler’s laws of
planetary motion. The same law of grav-
ity, however, implies that the several plan-
ets will exert attractive forces on each other,
which will “perturb” their orbits. Laplace
computed the perturbations, to a certain
degree of approximation, and found that
they were periodic, so that the solar system
was “stable” (in Laplace’s sense) and would
not destroy itself. His techniques were labo-
rious, and can be much simpliﬁed today by
Hamiltonian mechanics; furthermore, he
did not entirely prove that the solar sys-
tem is stable, since his method, if carried
to higher orders, does not always converge.
However, a great many of the ideas of mod-
ern perturbation theory originated here:
variation of parameters, averaging, multi-
ple scales, and the problems that in the
twentieth century led to the Kolmogorov–
Arnol’d–Moser theorem, the Nekhoroshev
theorem, and other topics in dynamical sys-
tems theory.
In theory, a ﬂuid that is viscous (even
to the smallest degree) will adhere to the
walls of any container (such as a pipe) in
which it is ﬂowing. However, at any rea-
sonable distance from the walls, the ﬂuid
ﬂows almost as if the wall were not there.
In order to resolve this apparent paradox, L.
Prandtl introduced the idea of a “boundary
layer,” a thin layer of ﬂuid against the wall
in which the ﬂuid passes from rest to rapid
motion. Here the “unperturbed” problem is
the inviscid ﬂow (which does not adhere to
the wall), the “perturbation” is the small vis-
cosity, and the eﬀect of the perturbation is
not a small correction of the motion but a
quite drastic correction conﬁned to a small
region (the boundary layer). This example
leads to the ideas of stretched or scaled
coordinates, inner and outer solutions, and
matching.
According to quantum mechanics, all
observable quantities are the eigenvalues
of operators on a Hilbert space. In simple
problems, the Hilbert space will be ﬁnite
dimensional and the operators are repre-
sentable as matrices; in other cases, they
are partial diﬀerential operators. In model-
ing an atom, for instance, the eigenvalues
will be related to the frequencies of light
(the spectrum) emitted by the atom when
electrons jump from one shell to another.
These frequencies can be perturbed, for
instance, if the atom is placed in a weak
magnetic ﬁeld. Mathematically, the result-
ing problem is to determine the changes
in the “spectrum” (the set of eigenvalues)
of a matrix or other operator when the
operator is slightly perturbed. One striking
diﬀerence between this problem and the
ﬁrst two is that quantum mechanics is
entirely a linear theory, whereas in both
the dynamical systems problems and the
boundary layer problems nonlinearities
can play a crucial role.
12.2
Basic Concepts
12.2.1
Perturbation Methods versus Numerical
Methods
A system of diﬀerential equations that is not
explicitly solvable calls for an approximate
solution method of some type. The most
useful approximate methods are numer-
ical methods (implemented on a digital
computer) and perturbation methods. Per-
turbation methods are usable only if the
system is “close” to an explicitly solvable

12.2 Basic Concepts
413
system, in the sense that the system would
become solvable if certain small changes
were made, such as deleting small terms or
averaging some term over a rapidly rotating
angle. In such a case, perturbation theory
takes the solution of the simpliﬁed problem
as a “zeroth-order approximation” that can
be successively improved, giving higher-
order approximations having explicit for-
mulas. Numerical methods operate with-
out the restriction that the problem be
nearly solvable, but give a solution in the
form of numerical tables or graphs. The
advantage of a formula is that one can see
by inspection the manner in which each
variable and parameter aﬀects the solution.
As both numerical and perturbation solu-
tion are approximate, it is often helpful
to verify a perturbation solution by com-
paring it with numerical ones (or directly
with experimental data), especially when a
mathematically valid error estimate for the
perturbation solution is missing.
12.2.2
Perturbation Parameters
Mathematical models of physical phenom-
ena typically contain several variables,
which
are
divided
into
“coordinates”
(of space or time) and “parameters.” A
spring/mass system with a cubic nonlin-
earity, for instance, will contain position
and time coordinates as well as parameters
for the mass and for the coeﬃcients of
the linear and cubic terms in the restoring
force. The ﬁrst step in preparing such
a system for perturbation analysis is to
nondimensionalize these variables. The
second step is to look for solvable special
cases that can serve as the starting point
for approximating the solution of nearby
cases. Most often, these solvable cases
will be obtained by setting some of the
parameters equal to zero. For instance, the
forced and damped nonlinear oscillator
given by
̈y + C ̇y + k2y + Ay3 = B cos 𝜔t
(12.1)
becomes easily solvable if A = B = C = 0; it
is still solvable if only A = 0, but not quite as
simply, and the case C = B = 0 is solvable
with elliptic functions. It is therefore plau-
sible to look for approximate solutions by
perturbation theory if A, B, and C are small,
and it may also be possible if only A is small,
or if only B and C are small.
Suppose that we choose to investigate the
case when A, B, and C are small. Ideally,
we could treat these as three small indepen-
dent parameters, and considerable work is
now being devoted to the investigation of
such “multiparameter” perturbation meth-
ods, especially in the context of bifurcation
theory (see [1]). However, most classical
perturbation methods are developed only
for single-parameter problems. Therefore,
it is necessary to make a choice as to how
to reduce (12.1) to a single-parameter prob-
lem. The simplest way is to write A = a𝜀,
B = b𝜀, and C = c𝜀, obtaining
̈y + 𝜀c ̇y + k2y + 𝜀ay3 = 𝜀b cos 𝜔t.
(12.2)
We appear to have added a parameter
instead of reducing the number, but now a,
b, and c are regarded as constants, whereas
𝜀, the perturbation parameter, is taken as a
small, but variable, quantity; typically, we
expect the perturbation solution to have
the form of a power series in 𝜀. We have,
in eﬀect, chosen to investigate a particular
path leading from the origin in the space
of variables A, B, C. It is at once clear that
other paths are possible, for instance
̈y + 𝜀2c ̇y + k2y + 𝜀ay3 = 𝜀b cos 𝜔t.
(12.3)
One might choose (12.3) over (12.2) if the
goal is to investigate systems in which the

414
12 Perturbation Methods
damping is extremely small, small even
compared to the cubic term and the forc-
ing. But it is not clear, without experience,
what the best formulation will be for a
given problem. As an example of the role
of experience, one might expect (knowing
something about resonance in the linear
case) that the results of studying (12.2) will
be diﬀerent if 𝜔is close to k than if it is
far away. But how do you express math-
ematically the idea that “𝜔is close to k”?
Recalling that the only parameter available
to express “smallness” is 𝜀, the best answer
turns out to be
𝜔2 = k2 + 𝜀d,
(12.4)
where d is another constant. Substituting
(12.4) into (12.2) leads to the “correct” for-
mulation of the near-resonance problem.
One can see that the setting up of pertur-
bation problems is sometimes an art rather
than a science. In this chapter, we will for
the most part assume that a perturbation
problem has been chosen. Mathematical
analysis of the problem may then suggest
the use of stretched or otherwise rescaled
variables, which, in fact, amount to a modi-
ﬁcation of the initial perturbation problem.
In recent years, a further consideration
has come to the fore regarding the choice of
parameters in a mathematical model. Phys-
ical considerations may have led to a model
such as (12.1) above, and yet we know that
this model is not exactly correct; there may,
for instance, be very small nonlinearities
other than the cubic term in the restor-
ing force, or nonlinearities in the damp-
ing, or additional harmonics in the forcing.
How many such eﬀects should be included
in the model? In the past, it was simply
a matter of trial and error. But in certain
cases, there now exists a mathematical the-
ory that is able to determine just which
additional small terms might make a quali-
tative (rather than just a tiny quantitative)
diﬀerence in the solution. In these cases,
it is sometimes best to add all such sig-
niﬁcant small terms to the equation before
attempting the solution, even if there is no
evident physical reason for doing so. The
process of adding these additional terms is
called ﬁnding the universal unfolding of the
system. The advantage is that the univer-
sal unfolding will account for all possible
qualitative behaviors that may be observed
as a result of unknown perturbations. For
instance, in the past, many bifurcations
were not observed to occur exactly as pre-
dicted. They were called imperfect bifurca-
tions, and each situation required that the
speciﬁc perturbation responsible for the
imperfection be discovered and incorpo-
rated into the model. Now it is often pos-
sible to determine all possible imperfec-
tions in advance by examining the universal
unfolding of the original model. See [1].
In addition to the types of problems
already discussed, there exist problems
that do not contain a perturbation param-
eter but nonetheless allow treatment by
perturbation methods. For instance, a
system of diﬀerential equations may have
a particular solution (often an equilibrium
solution or a periodic solution) that can
be computed exactly, and one may wish to
study the solutions lying in a neighborhood
of this one. One way to treat such problems
is called coordinate perturbations; the
coordinates themselves (or more precisely,
the diﬀerences between the coordinates
of the known and unknown solutions)
are treated as small quantities, in place
of
a
perturbation
parameter.
Another
approach is to introduce a parameter 𝜀as
a scale factor multiplying these coordinate
diﬀerences. Both ideas will be illustrated
in the discussion of normal forms in
Section 12.3.6.

12.2 Basic Concepts
415
12.2.3
Perturbation Series
Let us suppose that a perturbation problem
has been posed, and let the exact solution
(which we wish to approximate) be denoted
by u(x, 𝜀). Here 𝜀is the (scalar) perturba-
tion parameter, x is a vector consisting of
all other variables in the problem including
coordinates and other parameters, and u is
the quantity being solved for. (In the case of
(12.2) above, u = y and x = (t, a, b, c, 𝜔), or
if (12.4) is used, then x = (t, a, b, c, d).) The
simplest form in which to seek an approxi-
mation is that of a (truncated) power series
in 𝜀:
u(x, 𝜀) ≅u0(x) + 𝜀u1(x) + · · · + 𝜀kuk(x).
(12.5)
There are times when this is insuﬃcient and
we require a Poincaré series
u(x, 𝜀) ≅𝛿0(𝜀)u0(x) + 𝛿1(𝜀)u1(x)
+ · · · + 𝛿k(𝜀)uk(x),
(12.6)
where each 𝛿i is a monotone function of 𝜀
deﬁned for 𝜀> 0 satisfying
lim
𝜀→0
𝛿i+1(𝜀)
𝛿i(𝜀)
= 0;
(12.7)
such functions 𝛿i are called gauges. (Of
course, a Poincaré series reduces to a power
series if 𝛿i(𝜀) = 𝜀i.) Finally, there are times
when not even a Poincaré series is suﬃcient
and a generalized series is needed:
u(x, 𝜀) ≅𝛿0(𝜀)u0(x, 𝜀) + 𝛿1(𝜀)u1(x, 𝜀)
+ · · · + 𝛿k(𝜀)uk(x, 𝜀).
(12.8)
With such a series, it might appear that we
could delete the gauges, or rather assimilate
them into the ui because these are now
allowed to depend upon 𝜀; but the intention
is that the dependence of ui on 𝜀should not
aﬀect its order of magnitude. An example
is the following two-term generalized series
in which the vector x consists only of the
time t:
u(t, 𝜀) ≅sin(1 + 𝜀)t + 𝜀cos(1 + 𝜀)t. (12.9)
Here u0(t, 𝜀) = sin(1 + 𝜀)t and u1(t, 𝜀) =
cos(1 + 𝜀)t; the dependence of these coeﬃ-
cients upon 𝜀modiﬁes their period but not
their amplitude, and the second term still
has the order of magnitude of its gauge 𝜀.
Notice that we have written only trun-
cated series in the previous paragraph.
While most perturbation methods allow,
in principle, for the computation of inﬁnite
series, these series seldom converge, and
in practice it is impossible to calculate
more than a few terms. The type of accu-
racy that we hope for in a perturbation
solution is not convergence (improve-
ment in accuracy as the number of terms
increases), but rather asymptotic validity,
which means improvement in accuracy as
𝜀approaches zero. To explain this concept,
let us consider a generalized series
u(𝜀) ≅𝛿0(𝜀)u0(𝜀) + 𝛿1(𝜀)u1(𝜀)
+ · · · + 𝛿k(𝜀)uk(𝜀)
(12.10)
that contains no variables other than 𝜀. We
will say that this series is an asymptotic
approximation to u(𝜀) if
u(𝜀) = 𝛿0(𝜀)u0(𝜀) + 𝛿1(𝜀)u1(𝜀)
+ · · · + 𝛿k(𝜀)uk(𝜀) + R(𝜀),
(12.11)
where the remainder or error R(𝜀) satisﬁes
a bound of the form
|R(𝜀)| ≤c𝛿k+1(𝜀)
(12.12)
for some constant c > 0 and some gauge
𝛿k+1 that approaches zero more rapidly than

416
12 Perturbation Methods
𝛿k as 𝜀→0. (If u, and hence R, are vector-
valued, then |R(𝜀)| denotes a vector norm.)
Equation (12.12) is abbreviated with the
“big-oh” notation
R(𝜀) = (𝛿k+1(𝜀)).
(12.13)
The series (12.10) is called asymptotically
valid, or an asymptotic series, if it is an
asymptotic approximation (in the above
sense) and in addition every truncation
of (12.10) is also an asymptotic approxi-
mation, with the error being “big-oh” of
the ﬁrst omitted gauge. The case in which
u(x, 𝜀) depends upon variables x in addi-
tion to 𝜀will be discussed in the following
section on uniformity.
As a technical matter, any bound such as
(12.12) is not intended to hold for all 𝜀, but
only for 𝜀in some interval 0 ≤𝜀≤𝜀0. A
perturbation solution is never expected to
be valid for large values of the perturbation
parameter, and the meaning of “large” is rel-
ative. In this chapter, we will not continue
to mention 𝜀0, but it is always lurking in the
background.
Although Fourier series may arise in
perturbation theory when dealing with
oscillatory problems, a Fourier series is
never a perturbation series. Rather, if a per-
turbation series such as u(t, 𝜀) ≅u0(t) +
𝜀u1(t) depends periodically on time t, then
the coeﬃcients uk(t) may be expressed as
Fourier series in t.
12.2.4
Uniformity
In the previous section we have deﬁned
asymptotic validity of a perturbation series
for a function u(𝜀) depending only on 𝜀.
This is adequate for a problem such as
ﬁnding a root of a polynomial (supposing
that the polynomial contains a perturbation
parameter 𝜀), because the root is a single
number. But for most perturbation prob-
lems (such as diﬀerential equations), the
solution is a function of space and/or time
coordinates, and possibly various parame-
ters, in addition to 𝜀. For such problems, the
previous deﬁnition of asymptotic validity is
insuﬃcient.
Let us return to the generalized series
(12.8), and denote the error by R(x, 𝜀). Now
we may require that for each ﬁxed x this
error is of order 𝛿k+1(𝜀):
|R(x, 𝜀)| ≤c(x)𝛿k+1(𝜀).
(12.14)
Notice that the “constant” c here may
change when we move to a new point x. In
this case, we say that the error is pointwise
of order 𝛿k+1.
Alternatively, we can choose a domain D
of x and require that the error be uniformly
of order 𝛿k+1 for all x in D:
|R(x, 𝜀)| ≤c𝛿k+1(𝜀).
(12.15)
Here the constant c is truly constant. In
this case, we say R(x, 𝜀) = (𝛿k+1(𝜀)) uni-
formly in x for x in D. If every truncation
of a perturbation series is uniformly of
the order of the ﬁrst omitted gauge, we
say that the series is uniformly valid. (In
the last sentence, we neglected to say “in
D,” but it is important to remember that
such an expression has no meaning unless
the domain D is understood.) Obviously,
uniform asymptotic validity is stronger
than pointwise validity, and it is safe to say
that every method used in perturbation
theory has been introduced in order to gain
uniform validity for a problem for which
previous methods only gave pointwise
validity.
Now the deﬁnition of uniform validity
calls for an estimate of the error of an
approximation, and such an estimate is a

12.2 Basic Concepts
417
diﬃcult thing to come by. It would be con-
venient if there were an easier test for uni-
formity. In actuality, there is not. How-
ever, it is possible to obtain a simple nec-
essary (but not suﬃcient) condition for a
series to be uniformly valid; namely, it is
not diﬃcult to show that if a series (12.8)
is uniformly valid in a domain D, then each
coeﬃcient uk(x) with k ≥1 is bounded on
D, that is, there exist constants ck such
that
|uk(x, 𝜀)| ≤ck
(12.16)
for x in D. If this is true, we say that
the series (12.8) is uniformly ordered. We
have already encountered the concept of a
uniformly ordered series when discussing
(12.9) above. A uniformly ordered series is
one in which each term after the ﬁrst is
of no greater order than is indicated by its
gauge. It is easy to inspect a perturbation
series, once it has been constructed, and
determine whether it is uniformly ordered.
If not, the series is called disordered, and
it cannot be uniformly valid. On the other
hand, if it is uniformly ordered, it does not
follow that it is uniformly valid, because one
has done nothing toward estimating the
error. Almost all textbooks are misleading
on this point, because they almost invari-
ably claim to be showing the uniform valid-
ity of a series when, in fact, they are only
showing that it is uniformly ordered. How-
ever if a perturbation series is constructed
on the basis of good intuitive insight into
a problem, and if it is uniformly ordered,
then it frequently turns out to be uniformly
valid as well. (An elementary example in
which this is not the case will be given in
Section 12.3.2.)
With
regard
to
uniform
ordering,
Poincaré series occupy a special place.
Recall that a series is a Poincaré series if its
coeﬃcients do not depend on 𝜀; see (12.6).
In this case, if the domain D is compact and
the coeﬃcients uk(x) are continuous, then
the coeﬃcients are automatically bounded
and the series is uniformly ordered (but
still not automatically uniformly valid).
However, even a Poincaré series may fail to
be uniformly ordered if D is not compact
or if D is allowed to depend on 𝜀. We
have not considered this latter possibility
until now, but in fact, in many problems
one must consider domains D(𝜀) that
depend on 𝜀. An important example is a
boundary layer, a thin domain near the
boundary of some larger region, whose
thickness depends on 𝜀. For such domains,
the deﬁnitions (12.15) and (12.16) of uni-
form validity and uniform ordering are
the same, except that they are to hold for
all x in D(𝜀); that is, for each value of 𝜀,
(12.15) or (12.16) hold for a diﬀerent range
of x.
It is now possible to explain one of the
principal divisions in the subject of per-
turbation theory, the division of perturba-
tion problems into regular and singular.
A perturbation problem is regular if there
exists a Poincaré series that is uniformly
valid on the intended domain; it is singu-
lar if it is necessary to use a generalized
series in order to obtain a uniformly valid
solution on the intended domain. This is
the only correct deﬁnition. Many textbooks
give partial deﬁnitions such as “a perturba-
tion problem for a diﬀerential equation is
singular if 𝜀multiplies the highest deriva-
tive.” Such a deﬁnition, which refers only
to the diﬀerential equation without stat-
ing an intended domain, cannot be cor-
rect. The presence of an 𝜀multiplying the
highest derivative does, of course, aﬀect the
domain on which a problem can be regu-
lar; we will see below that a problem such
as ̈u + u + 𝜀u3 = 0 is regular on any ﬁxed
interval [0, T] but singular on an expand-
ing interval [0, 1∕𝜀], while a problem such

418
12 Perturbation Methods
as 𝜀̈u + (t2 + 1) ̇u + u = 0 is regular only on
a shrinking interval [0, 𝜀] and singular on a
ﬁxed interval.
12.3
Nonlinear Oscillations and Dynamical
Systems
In this section, we discuss the major pertur-
bation methods used in the study of nonlin-
ear ordinary diﬀerential equations (dynam-
ical systems). Typical problems include the
location and bifurcation of rest (or equi-
librium) points and periodic or quasiperi-
odic solutions; the approximation of solu-
tions close to these; the solution of initial
value problems for systems that become
solvable (usually either linear or integrable
Hamiltonian) when a small parameter van-
ishes; and the splitting of a homoclinic orbit
under perturbation. In all of these prob-
lems, there is an interplay between quali-
tative and quantitative behavior. Advance
knowledge of the qualitative behavior may
assist the choice of a successful perturba-
tion method. Alternatively, if the qualitative
behavior is unknown, perturbation meth-
ods may be used in an exploratory fashion,
as long as it is kept in mind that very fre-
quently, especially in nonlinear problems,
a perturbation solution may appear correct
but may predict qualitative features (such as
periodicity, stability, or presence of chaos)
erroneously. As a general rule, qualitative
results obtained from perturbation solu-
tions (or any other approximate solutions)
should be taken as conjectural, until sup-
ported by some combination of numeri-
cal or experimental evidence and rigorous
mathematical proof.
12.3.1
Rest Points and Regular Perturbations
Given a system of diﬀerential equations of
the form
̇x = f(x, 𝜀) = f0(x) + 𝜀f1(x) + (𝜀2),
(12.17)
with x = (x1, … , xn) ∈ℝn and 𝜀∈ℝ, the
rest points are the solutions of the system
of equations f(x, 𝜀) = 0. If a rest point a0 is
known when 𝜀= 0, it may continue to exist
as a function a(𝜀) with
f(a(𝜀), 𝜀) = 0
(12.18)
for 𝜀near zero, or it may bifurcate into
two or more rest points, or disappear alto-
gether; the results may diﬀer for 𝜀> 0 and
𝜀< 0. A crucial role is played by the matrix
A0 = fx(x0, 0) of partial derivatives of f at
the unperturbed rest point, and especially
its eigenvalues. If A0 is invertible (zero is
not an eigenvalue), then, by the implicit
function theorem, a unique continuation
a(𝜀) exists and is computable as a series
a(𝜀) = a0 + 𝜀a1 + (𝜀2).
(12.19)
This is the simplest example of a perturba-
tion series. Putting (12.19) into (12.18) and
expanding gives f0(a0) + 𝜀(A0a1 + f1(a0)) +
· · · = 0, or (because f0(a0) = 0)
a1 = −A−1
0 f1(a0).
(12.20)
The solution may be continued to higher
order, and the matrix function
A(𝜀) = fx(a(𝜀), 𝜀)
(12.21)
may also be studied, because (in most cases)
it determines the stability of the rest point.
If all of the eigenvalues of A0 are oﬀthe
imaginary axis (A0 is hyperbolic), the same

12.3 Nonlinear Oscillations and Dynamical Systems
419
will be true for A(𝜀) for small 𝜀, and the
stability type (dimensions of the stable and
unstable manifolds) of the rest point will
not change. When this is not the case,
the methods of Section 12.6 determine the
spectrum of A(𝜀), and thus usually suﬃce to
decide how the stability changes.
When A0 is not invertible, bifurcation
(change in the number of rest points) usu-
ally occurs. Even when A0 has only one
zero eigenvalue, various possibilities (such
as saddle-node and pitchfork bifurcations)
exist, and it is not possible to give details
here. A reference treating the subject from
the standpoint of perturbation theory is [2];
a quite diﬀerent modern treatment is [1].
Now suppose that a solution
x(t, 𝜀) = x0(t) + 𝜀x1(t) + (𝜀2)
(12.22)
of the system (12.17) is to be found, with ini-
tial condition x(0, 𝜀) = a(𝜀) (no longer a rest
point) given by (12.19). Substituting (12.22)
into (12.17), expanding, and equating terms
of the same degree in 𝜀yields
̇x0 = f0(x0)
̇x1 = f0x(x0(t))x1 + f1(x0(t)).
(12.23)
If the ﬁrst equation of (12.23), which is
the same as (12.17) with 𝜀= 0, can be
solved with initial condition x0(0) = a0, its
solution x0(t) can be placed in the second
equation of (12.23), which then becomes an
inhomogeneous linear equation for x1; it is
to be solved with initial condition x1(0) =
a1. Equations of this type are not necessar-
ily easy to solve, but are certainly easier than
the nonlinear equation (12.17). If this is suc-
cessful, the procedure may be continued to
higher order.
This is usually called the regular pertur-
bation method or the method of straight-
forward expansion. According to our earlier
deﬁnition, a perturbation method is regular
if it provides a Poincaré series that is uni-
formly valid on the intended domain. Here
(12.22) is a Poincaré series, and it can be
shown to be uniformly valid on any ﬁnite
interval [0, T]; that is, the error bound is
of the order of the ﬁrst omitted term, and
once T is chosen, the coeﬃcient in the error
bound is ﬁxed. So the term “regular” is jus-
tiﬁed if this is the intended domain. In many
problems (below), one seeks a solution valid
on an “expanding” interval such as [0, 1∕𝜀];
the straightforward expansion is usually not
valid for this purpose, and so is often called
a naive expansion.
There are situations in which straightfor-
ward expansion is valid for much longer
than ﬁnite intervals. For instance, if a solu-
tion is approaching a sink (a rest point with
all eigenvalues in the left half-plane), the
straightforward expansion is valid for all t ≥
0; see [3, Chapter 5]. More generally, if the
ﬁrst equation of (12.23) has a solution that
connects two hyperbolic rest points, then
a straightforward expansion (to any order)
beginning with that solution will be shad-
owed by an exact solution of (12.17) con-
necting two hyperbolic rest points of that
system; that is, the approximate and exact
solutions will remain close (to the order of
the ﬁrst omitted term) for all time, both past
and future, although the two solutions may
not have any point in common. (In partic-
ular, the approximate and shadowing solu-
tions will not satisfy the same initial condi-
tions.) See [3, Chapter 6].
12.3.2
Simple Nonlinear Oscillators and
Lindstedt’s Method
The “hard” nonlinear spring or unforced
Duﬃng equation is given by
̈u + u + 𝜀u3 = 0
(12.24)

420
12 Perturbation Methods
for 𝜀> 0. It can be expressed as a ﬁrst-order
system in the (u, ̇u) phase plane in the form
̇u = v, ̇v = −u −𝜀u3. In the phase plane, the
orbits are closed curves surrounding the
rest point at the origin; this may be seen
from the conservation of energy. (The “soft”
spring with 𝜀< 0 behaves diﬀerently.) As
the solutions of (12.24) with any initial con-
ditions u(0) = a, ̇u(0) = b are smooth func-
tions of 𝜀, they may be expanded as Tay-
lor series having the form (if we retain two
terms)
u(t, 𝜀) ≅u0(t) + 𝜀u1(t).
(12.25)
The coeﬃcients may be determined by sub-
stituting (12.25) into (12.24), expanding the
u3 term, dropping powers of 𝜀higher than
the ﬁrst, and setting each order in 𝜀sepa-
rately equal to zero. This gives two linear
equations that can be solved (recursively)
for u0 and u1. The result, for b = 0 (and
it is enough to consider this case because
every solution is at rest momentarily when
its amplitude is at its maximum), is
u(t, 𝜀) ≅a cos t −𝜀a3
32(cos t
+12t sin t −cos 3t).
(12.26)
Upon examining (12.26) for uniform order-
ing, we discover that all functions of t
appearing there are bounded for all t except
for 12t sin t, which becomes unbounded as
t approaches inﬁnity. This is an example of
a so-called “secular” term, one which grows
over the “ages” (saeculae in Latin). We con-
clude from this that (12.26) is uniformly
ordered for t in any ﬁnite interval D =
[0, T] but not for D = [0, ∞). According to
the general principles discussed in Section
12.2.4, this shows that (12.26) is not uni-
formly valid on [0, ∞), and it leads us to sus-
pect, but does not prove, that (12.16) is uni-
formly valid on [0, T]. In the present case,
this conjecture is correct. If the intended
domain D for the solution of (12.24) is
a ﬁnite interval, then we have obtained a
uniform approximation in the form of a
Poincaré series, and the problem is a regu-
lar one. If a solution valid for a longer time
is desired, the problem will prove to be sin-
gular.
In an eﬀort to extend the validity of the
solution, we recall that the actual solutions
of (12.24) are periodic, whereas (12.26) is
not. The problem is that the period of the
exact solution depends upon 𝜀, and there
is no way that a Poincaré series can have
such a period because the coeﬃcients are
not allowed to depend on 𝜀. To remedy
this, we seek to approximate the (unknown)
frequency of the solution in the form
𝜈(𝜀) ≅̃𝜈(𝜀) = 𝜈0 + 𝜀𝜈1 + 𝜀2𝜈2
(12.27)
with the solution itself being represented as
u(t, 𝜀) ≅u0(̃𝜈(𝜀)t) + 𝜀u1(̃𝜈(𝜀)t),
(12.28)
which is now a generalized series. Notice
that we have carried (12.27) to one more
order than (12.28). Now we substitute
(12.27) and (12.28) into (12.24) and attempt
to determine 𝜈0, u0, 𝜈1, u1, 𝜈2 recursively,
in that order, in such a way that each ui is
periodic. The mechanics of doing this will
be explained in the next paragraph; the
result is
u(t, 𝜀) ≅a cos t+
+𝜀
(
−1
32a3 cos t+ + 1
32a3 cos 3t+)
,
(12.29)
where
t+ = ̃𝜈(𝜀)t =
(
1 + 𝜀3
8a2 −𝜀2 21
256a4)
t.
(12.30)

12.3 Nonlinear Oscillations and Dynamical Systems
421
Examining (12.29), we see that it is uni-
formly ordered for all time, because the
coeﬃcients are bounded (there are no sec-
ular terms). One might therefore conjec-
ture that the solution is uniformly valid
for all time, but this would be incorrect!
(This example is an excellent warning as to
the need for proofs of validity in perturba-
tion theory.) The diﬃculty is that t+ uses
the approximate frequency ̃𝜈(𝜀) in place
of the exact frequency 𝜈(𝜀); there is no
escape from this, as the exact frequency
remains unknown. Therefore, (12.29) grad-
ually gets out of phase with the exact solu-
tion. The reason for taking (12.27) to one
higher order than (12.28) is to minimize
this eﬀect. It can be shown that (12.29) is
uniformly valid on the expanding interval
D(𝜀) = [0, 1∕𝜀]. (This is our ﬁrst example
of a domain that depends on 𝜀, as dis-
cussed in Section 12.2.4) If the intended
domain is such an expanding interval, then
(12.29) provides a uniformly valid general-
ized series, and the problem is seen to be
singular. (If the intended domain is all t, the
problem is simply impossible to approxi-
mate asymptotically.)
In order to complete the example in the
last paragraph, we must indicate how to
obtain (12.29) and (12.30). The easiest way
is to substitute 𝜏= 𝜈(𝜀)t into (12.24) to
obtain
𝜈(𝜀)2 d2u
d𝜏2 + u + 𝜀u3 = 0.
(12.31)
Then substitute (12.27) and (12.28) into
(12.31), expand, and set the coeﬃcient of
each power of 𝜀equal to zero as usual. It
is easy to ﬁnd that 𝜈0 = 1 and u0 = a cos 𝜏
(which in the end becomes a cos t+ because
we do not know the exact frequency 𝜈).
The crucial step arises when examining the
equation for u1, which is (writing ′ = d∕d𝜏)
u′′
1 + u1 = −a3 cos3 𝜏+ 2𝜈1a cos 𝜏
(12.32)
=
(
−3
4a3 + 2𝜈1a
)
cos 𝜏
−1
4a3 cos 3𝜏.
From the Fourier series expansion (the
second line of (12.32)), we see that the
term in cos 𝜏will be resonant with the free
frequency, and hence produce unbounded
(secular) terms in u1, unless the coeﬃcient
of cos 𝜏vanishes. In this way, we conclude
that
𝜈1 = 3
8a2
(12.33)
and, after deleting the cos 𝜏term from
(12.32), we solve it for u1. This procedure is
repeated at each subsequent stage.
The
previous
example
is
typical
of
unforced conservative oscillators, in which
every solution (at least in a certain region)
is
periodic.
There
are
two
additional
classes of oscillators that must be men-
tioned, although we cannot give them as
much space as they deserve: self-excited
oscillators and forced oscillators.
The standard example of a self-excited
oscillator is the van der Pol equation
̈u + 𝜀(u2 −1) ̇u + u = 0.
(12.34)
Instead of a region in the phase plane ﬁlled
with periodic solutions, this equation has a
single periodic solution (limit cycle) for 𝜀>
0. The Lindstedt method, described above,
can be used to approximate the periodic
solution, but must be modiﬁed slightly: the
initial condition can no longer be assigned
arbitrarily, because to do so will, in gen-
eral, yield a nonperiodic solution for which
the Lindstedt method fails. (These solu-
tions can be found by averaging or mul-
tiple scales; see below.) Suppose that the

422
12 Perturbation Methods
limit cycle crosses the positive x axis (in the
phase plane) at (a(𝜀), 0) and has frequency
𝜈(𝜀). Then the solution is sought in the form
of (12.27) and (12.28) together with an addi-
tional expansion a(𝜀) = 𝛼0 + 𝜀a1 + · · ·; the
coeﬃcients ui, 𝜈i, and ai are determined
recursively, choosing 𝜈i and ai so that no
secular terms arise in ui+1. This example
shows the eﬀect of the dynamics of a system
on the correct formulation of a perturba-
tion problem.
The general nearly linear, periodically
forced oscillator can be written
̈u + u = 𝜀f (u, ̇u, 𝜔t),
(12.35)
where f (u, v, 𝜃) is 2𝜋-periodic in 𝜃; thus the
period of the forcing is 2𝜋∕𝜔. The dynam-
ics of (12.35) can be complicated, and we
will limit ourselves to one type of peri-
odic solution, the harmonic oscillations,
which are entrained by the forcing so that
they have the same frequency 𝜔; these har-
monic solutions occur for 𝜀small and 𝜔
close to 1 (the frequency of the solutions
when 𝜀= 0). As the problem contains two
parameters (𝜀and 𝜔) and we are limited
to one-parameter methods, it is necessary
to express the statement “𝜔is close to 1”
in terms of the perturbation parameter 𝜀.
(A study using two independent parameters
would uncover the phenomenon of “reso-
nance horns” or “resonance tongues.” See
[4, Section 5.5]) It turns out that an eﬃcient
way to do so is to write
𝜔2 = 1 + 𝜀𝛽,
(12.36)
where 𝛽is a new parameter that is consid-
ered ﬁxed (not small). With (12.36), (12.35)
can have one or more isolated periodic
solutions with unknown initial conditions
(a(𝜀), b(𝜀)). (We can no longer assume b =
0.) On the other hand, the frequency of
the solution this time is not unknown but
equals 𝜔. So the Lindstedt method under-
goes another modiﬁcation dictated by the
dynamics: u, a, and b are expanded in 𝜀and
solved recursively, choosing the coeﬃcients
of a and b to eliminate secular terms from
u. In contrast with the previous cases, there
is no accumulating phase error because the
frequency is known, and the perturbation
approximations are uniformly valid for all
time.
12.3.3
Averaging Method for Single-Frequency
Systems
All of the systems discussed in the previous
section, and a great many others, can be
expressed in periodic standard form,
̇x = 𝜀f(x, t, 𝜀),
(12.37)
where x = (x1, … , xn) ∈ℝn and where f is
2𝜋-periodic in t. The solutions of (12.37)
may be approximated by the method of
averaging, which not only locates the
periodic solutions (and proves their exis-
tence), but also determines their stability or
instability and approximates the transient
(nonperiodic) solutions. The method of
averaging has been rediscovered many
times and exists (with slight variations)
under
a
variety
of
names,
including:
method of van der Pol; method of Krylov–
Bogoliubov–Mitropolski (KBM method);
method of slowly varying amplitude and
phase;
stroboscopic
method;
Struble’s
method; von Zeipel’s method (in the
Hamiltonian case); method of Lie series or
Lie transforms. Some of the diﬀerences in
these “methods” pertain to how the orig-
inal system is put into periodic standard
form, and others to details about how the
near-identity transformations (described
below) are handled.

12.3 Nonlinear Oscillations and Dynamical Systems
423
To illustrate how a system may be put
into periodic standard form, consider the
van der Pol equation (12.34), or, written as
a system,
̇u = v
(12.38)
̇v = −u + 𝜀(1 −u2)v.
Rotating polar coordinates (r, 𝜑) may be
introduced by u = r cos(𝜑−t), v = r sin
(𝜑−t), giving
̇r = 𝜀
(
1 −r2 cos2(𝜑−t)
)
× r sin2(𝜑−t)
(12.39)
̇𝜑= 𝜀(1 −r2 cos2(𝜑−t)) sin(𝜑−t)
× cos(𝜑−t),
which is in periodic standard form with x =
(r, 𝜑). The same result may be achieved by
seeking a solution of (12.34) by variation
of parameters, in the form u = r cos(𝜑−t)
where r and 𝜑are variables, and imposing
the requirement that ̇u = r sin(𝜑−t); the
motivation for these choices is that with r
and 𝜑constant, these solve (12.34) for 𝜀=
0. The transformation to periodic standard
form is merely a change of coordinates, not
an assumption about the nature of the solu-
tions.
In its crudest form, the method of averag-
ing simply consists in replacing (12.37) by
̇y = 𝜀f(y),
(12.40)
where
f(y) = 1
2𝜋∫
2𝜋
0
f(y, t, 0) dt.
(12.41)
System (12.40) is easier to solve than
(12.37), because it is autonomous. The
form of (12.40) can be motivated by the
fact that for small 𝜀, x in (12.37) is slowly
varying and therefore nearly constant over
one period; therefore, to a ﬁrst approxi-
mation, we might hold x constant while
integrating over one period in (12.37) to
ﬁnd the “average” inﬂuence due to f. But
this sort of motivation gives no idea how to
estimate the error or to extend the method
to higher-order approximations. A much
better procedure is to return to (12.37) and
perform a near-identity change of variables
of the form
x = y + 𝜀u1(y, t, 𝜀),
(12.42)
where u1 is a periodic function of t, which
is to be determined so that the transformed
equation has the form
̇y = 𝜀g1(y) + 𝜀2̂g(y, t, 𝜀),
(12.43)
where g1 is independent of t. It turns out
that such a transformation is possible
only if we take g1 = f; by doing so, (12.40)
can be obtained from (12.43) simply by
deleting the 𝜀2 term. The solution y(t)
of (12.40) is called the ﬁrst approxima-
tion to the solution of (12.37); if this is
substituted into (12.42), the result x(t) is
called the improved ﬁrst approximation.
This “improvement” consists in adding (an
approximation to) small periodic ﬂuctua-
tions around y(t) that are actually present,
but are of the same order as the error of the
ﬁrst approximation, so while the numerical
results are better, the asymptotic accuracy
of the improved ﬁrst approximation is not
better than that of the ﬁrst approximation.
When it is formulated in this way, the
ﬁrst-order method of averaging is seen to
consist of nothing but coordinate changes
(ﬁrst into periodic standard form, then
into form (12.43)), followed by truncation
(dropping ̂g); it is only the truncation that
introduces error, and this error can be
estimated using Gronwall’s inequality. It is
also clear how to proceed to higher orders:

424
12 Perturbation Methods
simply replace (12.42) by
x = y + 𝜀u1(y, t, 𝜀) + · · · + 𝜀kuk(y, t, 𝜀)
(12.44)
and (12.43) by
̇y = 𝜀g1(y) + · · · + 𝜀kgk(y) + 𝜀k+1̂g(y, t, 𝜀);
(12.45)
the averaged equations are obtained by
deleting ̂g. It is, of course, necessary to
determine the ui and gi recursively in such
a way that the ui are periodic and the gi are
independent of t; this is where the techni-
cal details of various versions of averaging
come into play. (Warning: gi for i > 1 are
not simply averages of higher-order terms
of f, but of expressions involving these and
uj for j < i.) Once (12.45), without ̂g, has
been determined, it must be solved. Setting
𝜏= 𝜀t, it becomes
dy
d𝜏= g1(y) + 𝜀g2(y) + · · · + 𝜀k−1gk(y),
(12.46)
which can often be solved by regular per-
turbation theory on the interval 0 ≤𝜏≤1,
that is, 0 ≤t ≤1∕𝜀. Then the solution must
be put back into the transformation (12.44);
the resulting approximate solutions of
(12.37) will diﬀer from the exact solutions
(with the same initial condition) by (𝜀k)
during a time interval of length (1∕𝜀).
The
kth
approximation
and
improved
kth approximation are obtained by using
(12.44) without, or with, its kth term;
again, these are asymptotically equivalent,
although the improved approximation is
numerically more accurate.
For additional information about the
method of averaging, see [3]. As in regular
perturbation theory, it is possible under
special conditions to obtain results on half-
inﬁnite or inﬁnite intervals of time; see
Chapter 5 for systems with attractors and
Chapter 6 for shadowing. See Appendix E
for averaging applied to partial diﬀerential
equations.
12.3.4
Multifrequency Systems and Hamiltonian
Systems
Oscillatory problems that cannot be put
into periodic standard form can often be
put into the following angular standard
form:
̇r = 𝜀f(r, 𝜽, 𝜀)
(12.47)
̇𝜽= 𝛀(r) + 𝜀g(r, 𝜽, 𝜀),
where r = (r1, … , rn) is a vector of ampli-
tudes and 𝜽= (𝜃1, … , 𝜃m) a vector of angles
(so that f and g are periodic in each 𝜃i with
period 2𝜋). This form includes the periodic
standard form, by taking m = 1 and ̇𝜃= 1.
The “naive” method of averaging for (12.47)
would be to replace f and g by their averages
over 𝜽, for instance
f(r) =
1
(2𝜋)m ∫
2𝜋
0
· · ·
× ∫
2𝜋
0
f(r, 𝜽, 0) d𝜃1 · · · d𝜃m.
(12.48)
To justify this process, and to extend the
method to higher order, one tries to make
a near-identity change of variables (r, 𝜽) →
(𝝆, 𝝋) that will render the system indepen-
dent of 𝝋up through a given order k in
𝜀. However, one encounters at once the
famous diﬃculty of “small divisors” which
make the existence of such a transforma-
tion doubtful. If f is expanded in a conver-
gent multiple Fourier series
f(r, 𝜽, 0) =
∑
𝝂
a𝝂(r)ei(𝜈1𝜃1+···+𝜈m𝜃m),
(12.49)
then the transformation to averaged form
necessarily involves the series

12.3 Nonlinear Oscillations and Dynamical Systems
425
∑
𝝂≠0
a𝝂(r)
i(𝜈1Ω1(r) + · · · + 𝜈mΩm(r))ei(𝜈1𝜃1+···+𝜈m𝜃m),
(12.50)
which may not converge because the
denominators i(𝜈1𝜃1 + · · · + 𝜈m𝜃m) may be
small (or even zero), causing the coeﬃ-
cients to become large. It is of no use at
this point to say that “perturbation theory
is not concerned with the convergence of
series,” because the series in question is not
being used for approximation, but to prove
the existence of a transformation needed
in order to justify the method.
Some preliminary progress can be made
by considering the case in which the series
(12.49), and hence (12.50), are ﬁnite. In
this case, convergence diﬃculties cannot
arise, but there is still the diﬃculty that for
some r, one or more of the denominators of
(12.50) may become zero. As Ωi(r) are the
frequencies of the free oscillations (𝜀= 0)
of (12.47), the vanishing of a denominator
indicates a resonance relationship among
these frequencies. In general, for each 𝝂
there will be a resonance manifold, or res-
onance hypersurface, consisting of those r
for which the denominator involving 𝝂van-
ishes. On (or near) any such surface it is
not permissible to average over all angles 𝜽,
although it may be possible to average over
a subset of these angles or over certain inte-
gral linear combinations of them.
Results
beyond
these
have
been
obtained in the important special case
of Hamiltonian systems; the Kolmogorov–
Arnol’d—Moser ( or KAM) theorem and the
Nekhoroshev theorem are a high point of
modern perturbation theory and together
give the deﬁnitive answer to the problem
of the stability (in the sense of Laplace) of
the (idealized Newtonian) solar system,
with which this chapter began. Consider a
system deﬁned by a Hamiltonian function
of the form
H(r, 𝜽, 𝜀) = H0(r) + 𝜀H1(r, 𝜽)
+𝜀2H2(r, 𝜽) + · · · ,
(12.51)
where r and 𝜽are as before except that m =
n. Written in the form (12.47), this system is
̇r = 𝜀𝜕H1
𝜕𝜽+ · · ·
(12.52)
̇𝜽= −𝜕H0
𝜕r −𝜀𝜕H1
𝜕r + · · · .
As H1 is assumed to be periodic in the
components of 𝜽, it may be expanded in a
multiple Fourier series like (12.49); diﬀer-
entiating with respect to any component
of 𝜽then eliminates the constant term
(a0(r), which is the average). It follows that
𝜕H1∕𝜕𝜽has zero average, so that the (naive)
ﬁrst-order averaged equation for r becomes
̇r ≅0.
(12.53)
This suggests that the motion is oscillatory
with nearly constant amplitudes. The KAM
theorem states that (if a certain determi-
nant does not vanish) the great majority
of initial conditions will lead to quasiperi-
odic motion on an invariant torus close
to a torus r = constant. The Nekhoroshev
theorem states that even for those initial
conditions that do not lie on invariant
tori, the drift in r (called Arnol’d diﬀusion)
takes place exponentially slowly (as 𝜀→0).
(Notice that an n-dimensional torus in
2n-dimensional space does not form the
boundary of an open set if n > 1, so the
presence of many such invariant tori does
not prevent other solutions from slowly
drifting oﬀto inﬁnity. For n = 2, the invari-
ant 2-tori do not bound in 4-space, but they
do in the three-dimensional submanifolds
of constant energy; because solutions have
constant energy, Arnol’d discussion cannot

426
12 Perturbation Methods
take place.) For details see [5]. The proofs
involve very deep mathematics, such as the
Moser–Nash implicit function theorem,
with implications for partial diﬀerential
equations,
Riemannian
geometry,
and
elsewhere.
In all applications of averaging and
related methods to Hamiltonian systems,
it is necessary to have a means of handling
near-identity transformations that preserve
the Hamiltonian form of the equations;
that is, one needs to construct near-identity
transformations that are canonical (or sym-
plectic). Classically, such transformations
can be constructed from their generating
functions (in the sense of Hamilton–Jacobi
theory); averaging procedures carried out
in this way are called von Zeipel’s method.
At the present time, this approach can be
regarded as obsolete. It has been replaced
by the method of Lie transforms, in which
near-identity
canonical
transformations
are generated as the ﬂows of Hamiltonian
systems in which 𝜀takes the place of time.
(The Lie method is not limited to Hamil-
tonian systems, but is particularly useful in
this context.) Algorithmic procedures for
handling near-identity transformations in
this way have been developed, and they are
considerably simpler than using generating
functions. See [6, Section 5.7] and [3,
Chapter 3].
12.3.5
Multiple-Scale Method
The earliest perturbation problem, that of
planetary motion, illustrates the appeal of
the idea of multiple scales. A single planet
under the inﬂuence of Newtonian gravi-
tation would travel around the sun in an
elliptic orbit characterized by certain quan-
tities called the “Keplerian elements” (the
eccentricity, major axis, and certain angles
giving the position of the ellipse in space).
As the actual (perturbed) motion of the
planets ﬁts this same pattern for long peri-
ods of time, it is natural to describe the
perturbed motion as “elliptical motion with
slowly varying Keplerian elements.” A sim-
pler example would be a decaying oscilla-
tion of the form e−𝜀t sin t, which could be
described as a periodic motion with slowly
decaying amplitude. Solutions of nonlinear
oscillations obtained by the method of aver-
aging frequently have this form, in which
time appears both as t and as 𝜀t, the lat-
ter representing slow variation; sometimes
other combinations such as 𝜀2t appear.
This leads to the question whether it is
possible to arrive at such solutions more
directly, by postulating the necessary time
scales in advance. The “method of multiple
scales” is the result of such an approach,
and is sometimes regarded as the most
ﬂexible general method in perturbation
theory, because it is applicable both to
oscillatory problems (such as those cov-
ered by averaging) and to boundary layer
problems (discussed below). However, this
very ﬂexibility is also its drawback, because
the “method” exists in an immense variety
of ad hoc formulations adapted to particu-
lar problems. (See [6] for examples of many
of these variations.) There are two-scale
methods using fast time t and slow time
𝜀t; two-scale methods using strained fast
time (𝜈0 + 𝜀𝜈1 + 𝜀2𝜈2 + · · ·)t (similar to the
strained time in the Lindstedt method)
and slow time 𝜀t; multiple-scale methods
using t, 𝜀t, 𝜀2t, … , 𝜀nt; and methods using
scales that are nonlinear functions of t.
The scales to be used must be selected in
advance by intuition or experience, while
in other methods (averaging and match-
ing), the required scales are generated
automatically. Sometimes, the length of
validity of a solution can be increased by
increasing the number of scales, but (con-
trary to a common impression) this is by

12.3 Nonlinear Oscillations and Dynamical Systems
427
no means always the case; see [3, Section
3.5]. Some problems come with more than
one time scale from the beginning, for
instance, problems that contain a “slowly
varying parameter” depending on 𝜀t. It
may seem natural to treat such a system by
the method of multiple scales, but another
possibility is to introduce 𝜏= 𝜀t as an
additional independent variable subject
to ̇𝜏= 𝜀. In summary, the popularity of
multiple scales results from its shorter
calculations, but this aside, other methods
have greater power.
The general outlines of the method are as
follows. Suppose the chosen time scales are
t, 𝜏, 𝜎with 𝜏= 𝜀t, 𝜎= 𝜀2t. An approximate
solution is sought as a series taken to a
certain number of terms, such as
x0(t, 𝜏, 𝜎) + 𝜀x1(t, 𝜏, 𝜎) + 𝜀2x2(t, 𝜏, 𝜎).
(12.54)
In substituting (12.54) into the diﬀerential
equations to be solved, the deﬁnitions of
the scales (such as 𝜏= 𝜀t) are used, so
that ordinary derivatives with respect to
t are replaced by combinations of partial
derivatives with respect to the diﬀerent
scales; thus
d
dt = 𝜕
𝜕t + 𝜀𝜕
𝜕𝜏+ 𝜀2 𝜕
𝜕𝜎.
(12.55)
From this point on (until the very end,
when 𝜏and 𝜎are again replaced by their
deﬁnitions), the separate time scales are
treated as independent variables. This has
the eﬀect of changing ordinary diﬀerential
equations into partial diﬀerential equations
that are highly underdetermined, so that
various free choices are possible in express-
ing the solution. The point of the method is
now to make these choices skillfully so that
the ﬁnal series (12.54) is uniformly ordered
(and, it is hoped, uniformly valid) on the
desired domain.
As an illustration, we return to the van
der Pol equation (12.34) with initial con-
ditions u(0) = a, ̇u(0) = 0. Choosing time
scales t and 𝜏= 𝜀t, and writing the solution
as u ≅u0(t, 𝜏) + 𝜀u1(t, 𝜏), one ﬁnds recur-
sively that (with subscripts denoting partial
derivatives) u0tt + u0 = 0 and u1tt + u1 =
−2u0t𝜏−u2
0u0t. The ﬁrst equation gives
u0(t, 𝜏) = A(𝜏) cos t + B(𝜏) sin t,
a
mod-
ulated
oscillation
with
slowly
varying
coeﬃcients. The solution remains under-
determined, because there is nothing here
to ﬁx A(𝜏) and B(𝜏). The solution for u0 is
now substituted into the right-hand side of
the diﬀerential equation for u1, and A(𝜏)
and B(𝜏) are chosen to eliminate resonant
terms so that the solution for u1 will remain
bounded. (This is similar to the way the
undetermined quantities are ﬁxed in the
Lindstedt method.) The result is
u(t) ≅u0(t, 𝜀t) =
2a
√
a2 + (4 −a2)e−𝜀t cos t.
(12.56)
This is the same result (to ﬁrst order) as
would be found by applying averaging to
(12.39), but it has been found without any
preliminary coordinate transformations.
On the other hand, the possibility of con-
structing the solution depended on the
correct initial guess as to the time scales
to be used; the method of averaging gener-
ates the needed time scales automatically.
The solution (12.56) exhibits oscillations
tending toward a limit cycle that is a simple
harmonic motion of amplitude 2. This is
qualitatively correct, but the motion is not
simple harmonic; carrying the solution to
higher orders will introduce corrections.
12.3.6
Normal Forms
Suppose that the origin is a rest point for
a system ̇x = f(x), x ∈ℝn, and it is desired

428
12 Perturbation Methods
to study solutions of the system near this
point. (Any rest point can be moved to the
origin by a shift of coordinates.) The system
can be expanded in a (not necessarily con-
vergent) series
̇x = Ax + f2(x) + f3(x) + · · · ,
(12.57)
where A is a matrix, f2 consists of homo-
geneous quadratic terms, and so forth. The
matrix A can be brought into real canon-
ical form by a change of coordinates (or
into Jordan canonical form, if one is will-
ing to allow complex variables and keep
track of the conditions guaranteeing reality
in the original variables). The object of nor-
mal form theory is to continue this simpliﬁ-
cation process into the higher-order terms.
This is usually done recursively, one degree
at a time, by applying changes of coordi-
nates that diﬀer from the identity by terms
having the same degree as the term to be
simpliﬁed. This is an example of a coordi-
nate perturbation (Section 12.2.2), because
it is ‖x‖ that is small, not a perturbation
parameter. However, writing x = 𝜀𝝃turns
(12.57) into
̇𝝃= A𝝃+ 𝜀f2(𝝃) + 𝜀2f3(𝝃) + · · · ,
(12.58)
which is an ordinary perturbation of a lin-
ear problem.
When A is semisimple (diagonalizable
using complex numbers), it is possible to
bring all of the terms f2, f3, … (up to any
desired order) into a form that exhibits
symmetries determined by A. For instance,
[ ̇x
̇y
]
=
[ 0
−1
1
0
] [ x
y
]
+𝛼1
(x2 + y2) [ x
y
]
+𝛽1
(x2 + y2) [ −y
x
]
+𝛼2
(x2 + y2)2 [ x
y
]
+𝛽2
(x2 + y2)2 [ −y
x
]
+ · · ·
(12.59)
is the normal form for any system having
this 2 × 2 matrix for its linear part; all terms
of even degree have been removed, and all
remaining terms of odd degree are sym-
metrical under the group of rotations about
the origin, which is just the group gener-
ated by the linear part of (12.59). Because of
this symmetry, the system is quite simple in
polar coordinates:
̇r = 𝛼1r3 + 𝛼2r5 + · · · + 𝛼kr2k+1,
(12.60)
̇𝜃= 1 + 𝛽1r2 + 𝛽2r4 + · · · + 𝛽kr2k.
This is solvable by quadrature and (even
without integration) the ﬁrst nonzero 𝛼j
determines the stability of the origin. In
general, when A is semisimple, the system
in normal form always gains enough sym-
metry to reveal certain geometrical struc-
tures called preserved foliations, and fre-
quently is solvable by quadrature. These
solutions have error estimates (due to trun-
cation) similar to those of the method of
averaging, to which the method of normal
forms is closely related.
When A is not semisimple (its Jordan
form has oﬀ-diagonal ones), the results of
normalization are not so easy to explain or
to use, because the nonlinear terms acquire
a symmetry diﬀerent from that of the lin-
ear term. Nevertheless, the normal form in
such cases has proven essential to the study
of such problems as the Takens–Bogdanov
and Hamiltonian Hopf bifurcations. A full
exposition of normal form theory is given in

12.4 Initial and Boundary Layers
429
[7]. Popular expositions covering only the
semisimple case are [8] and [9].
12.3.7
Perturbation of Invariant Manifolds;
Melnikov Functions
With the steadily increasing importance of
nonlinear phenomena such as chaos and
strange attractors, ﬁnding solutions of spe-
ciﬁc initial value problems often becomes
less important than ﬁnding families (man-
ifolds) of solutions characterized by their
qualitative behavior. Many of these prob-
lems are accessible by means of perturba-
tion theory. We will brieﬂy describe one
example. If a dynamical system has a rest
point of saddle type, there will exist a sta-
ble manifold and an unstable manifold of
the saddle point; the former consists of all
points that approach the saddle point as
t →∞, the latter of points approaching the
saddle as t →−∞. In some cases, the stable
and unstable manifold will coincide; that is,
points that approach the saddle in the dis-
tant future also emerged from it in the dis-
tant past. (The simplest case occurs in the
plane when the stable and unstable man-
ifolds form a ﬁgure-eight pattern with the
saddle at the crossing point.) If such a sys-
tem is perturbed, it is important to decide
whether the stable and unstable manifolds
separate, or continue to intersect; and if
they intersect, whether they are transverse.
(The latter case leads to chaotic motion.)
The criterion that in many cases decides
between these alternatives is based on the
Melnikov function; if this function has sim-
ple zeros, the manifolds will intersect trans-
versely and there will be a chaotic region.
The Melnikov function is an integral over
the homoclinic orbit of the normal compo-
nent of the perturbation; the form of the
integral is determined by applying regular
perturbation methods to the solutions in
the stable and unstable manifolds and mea-
suring the distance between the approxi-
mate solutions. For details see [10].
12.4
Initial and Boundary Layers
The
problems
considered
in
Sections
12.3.2–12.3.5
are
regular
perturbation
problems when considered on a ﬁxed
interval of time, but become singular when
considered on an expanding interval such
as 0 ≤t ≤1∕𝜀. We now turn to problems
that are singular even on a ﬁxed interval.
It is not easy to solve these problems even
numerically, because for suﬃciently small
𝜀they are what numerical analysts call
“stiﬀ.” Each of these problems has (in some
coordinate
system)
a
small
parameter
multiplying a (highest-order) derivative.
12.4.1
Multiple-Scale Method for Initial Layer
Problems
As a ﬁrst example, we consider initial value
problems of the form
𝜀̈u + b(t) ̇u + c(t)u = 0
(12.61)
u(0) = 𝛼
̇u(0) = 𝛽
𝜀+ 𝛾.
One may think, for instance, of an object of
small mass 𝜀subjected to a time-dependent
restoring force and friction; at time zero,
the position and velocity have just reached
𝛼and 𝛾when the object is subjected to an
impulse-imparting momentum 𝛽, increas-
ing the velocity by an amount 𝛽∕𝜀, which
is large because 𝜀is small. We will use
this example to explain two methods that
are applicable to many problems in which

430
12 Perturbation Methods
a small parameter multiplies the highest
derivative.
In approaching any perturbation prob-
lem, one ﬁrst tries to understand the case
𝜀= 0, but here it does not make sense
to set 𝜀= 0. On one hand, the diﬀeren-
tial equation drops from second order to
ﬁrst, and can no longer accept two initial
conditions; on the other hand, the second
initial condition becomes inﬁnite. Progress
can be made, however, by introducing the
“stretched” time variable
𝜏= t
𝜀.
(12.62)
Upon substituting (12.62) into (12.61) and
writing ′ = d∕d𝜏, we obtain
u′′ + b(𝜀𝜏)u′ + 𝜀c(𝜀𝜏)u = 0
(12.63)
u(0) = 𝛼
u′(0) = 𝛽+ 𝜀𝛾.
This problem is regular (for a ﬁxed interval
of 𝜏) and can be solved readily. For a ﬁrst
approximation, it suﬃces to set 𝜀= 0 in
(12.63) to obtain u′′ + b0u′ = 0 with b0 =
b(0); the solution is
ui
0 = −𝛽
b0
e−b0𝜏+ 𝛼+ 𝛽
b0
,
(12.64)
called
the
ﬁrst-order
inner
solution.
(Higher-order
approximations
can
be
found by substituting a perturbation series
ui
0 + 𝜀ui
1 + · · · into (12.63).) The name
“inner solution” comes from the fact that
(12.64) is only uniformly valid on an inter-
val such as 0 ≤𝜏≤1, which translates
into 0 ≤t ≤𝜀in the original time variable;
this is a narrow “inner region” close to the
initial conditions. It is necessary somehow
to extend this to a solution valid for a ﬁxed
interval of t. This is, of course, equivalent
to an expanding interval of 𝜏, and one
might attempt to solve (12.63) on such an
expanding interval by previously discussed
methods. The equation cannot be put in
a form suitable for averaging. However,
the method of multiple scales is ﬂexible
enough to be adapted to this situation. One
takes as time scales 𝜏and t, and seeks a
solution in the form
u ≅{ui
0(𝜏) + ucor
0 (t)}
+𝜀{ui
1(𝜏) + ucor
1 (t)} + · · · .
(12.65)
(We could have taken u0(𝜏, t) + 𝜀u1(𝜏, t) +
· · ·, but the solution turns out to be the sum
of the previously calculated ui and a “cor-
rection” ucor, so it is convenient to postulate
this form initially.) One can diﬀerentiate
(12.65) with respect to 𝜏using (12.62) and
substitute it into (12.63), or equivalently
diﬀerentiate with respect to t and substi-
tute into (12.61). Assuming that ucor(0) =
0 (because the inner part ui should suf-
ﬁce initially), one ﬁnds that ui must satisfy
(12.63) as expected, and that ucor satisﬁes
a ﬁrst-order diﬀerential equation together
with the assumed initial condition ucor(0) =
0; thus ucor is fully determined. At the ﬁrst
order, ucor
0
in fact satisﬁes the diﬀerential
equation obtained from (12.61) by setting
𝜀= 0; this is the very equation that we ini-
tially discarded as unlikely to be meaning-
ful. Upon solving this equation (with zero
initial conditions) and adding the result to
ui
0, we obtain the composite solution
uc
0 = ui
0 + ucor
0
= −𝛽
b0
e−b0t∕𝜀
+
(
𝛼+ 𝛽
b0
)
exp
[
−∫
t
0
c(s)
b(s) ds
]
.(12.66)
This solution is uniformly valid on any ﬁxed
interval of t.

12.4 Initial and Boundary Layers
431
12.4.2
Matching for Initial Layer Problems
Although the method of multiple scales is
successful for problems of this type, it is not
used as widely as the method of matched
asymptotic expansions, probably because
multiple scales requires that the choices
of gauges and scales be made in advance,
whereas matching allows for the discovery
of the correct gauges and scales as one pro-
ceeds. (Recall that gauges are the functions
𝛿i(𝜀), usually just powers 𝜀i, that multiply
successive terms of a perturbation series;
scales are the stretched time or space vari-
ables used.) To apply the matching method
to (12.61), begin with the ﬁrst-order inner
solution (12.64) that is valid near the ori-
gin. Assume that at some distance from the
origin, a good ﬁrst approximation should
be given by setting 𝜀= 0 in (12.61) and
discarding the initial conditions (which we
have already seen do not make sense with
𝜀= 0); the result is
uo
0 = A exp
[
−∫
t
0
c(s)
b(s) ds
]
,
(12.67)
called the ﬁrst-order outer solution. As
we have discarded the initial conditions,
the quantity A in (12.67) remains undeter-
mined at this point. Now one compares the
inner solution (12.64) with the outer solu-
tion (12.67) in an eﬀort to determine the
correct value of A so that these solutions
“match.” In the present instance, the inner
solution decays rapidly (assuming b0 > 0)
to 𝛼+ 𝛽∕b0, while the outer solution has
A as its initial value (at t = 0). One might
try to determine where the “initial layer”
ends, and choose A so that ui
0 and uo
0 agree
at that point; but in fact it is suﬃcient to
set A = 𝛼+ 𝛽∕b0 on the assumption that
the inner solution reaches this value at a
point close enough to t = 0 to allow taking
it as the initial condition for the outer
solution. Finally, we note that adding the
inner and outer solutions would duplicate
the quantity 𝛼+ 𝛽∕b0 with which one ends
and the other begins, so we subtract this
“common part” uio of the inner and outer
solutions to obtain the composite solution
uc = ui + uo −uio,
(12.68)
which is equal to the result (12.66) obtained
by multiple scales.
In the last paragraph, we have cobbled
together the inner and outer solution in a
very ad hoc manner. In fact, several sys-
tematic procedures exist for carrying out
the matching of ui and uo to any order
and extracting the common part uio. The
most common procedure consists of what
are sometimes called the van Dyke match-
ing rules, details of which will be given
below. Although this procedure is simple
to use, it does not always lead to correct
results, in particular, when it is necessary
to use logarithmic gauges. The other meth-
ods, matching in an intermediate variable
and matching in an overlap domain, are
too lengthy to explain here (see [11]), but
give better results in diﬃcult cases. None
of these methods has a rigorous justiﬁca-
tion as a method, although it is often pos-
sible to justify the results for a particu-
lar problem or class of problems. Occa-
sionally, one encounters problems in which
the inner and outer solutions cannot be
matched. These cases sometimes require a
“triple deck,” that is, a third (or even fourth)
layer time scale. In other cases, there does
not exist a computable asymptotic approx-
imation to the exact solution.
To explain the van Dyke matching rules,
we will ﬁrst assume that the inner and
outer solutions ui(𝜏, 𝜀) and uo(t, 𝜀) have
been computed to some order 𝜀k. In the
problem we have been studying, the outer

432
12 Perturbation Methods
solution contains undetermined constants
whose value must be determined, and the
inner solution contains none, but in more
general problems to be considered below
there may be undetermined constants in
both. It is important to understand that
the inner and outer solutions are naturally
computed in such a way that ui is “expanded
in powers of 𝜀with 𝜏ﬁxed” while uo is
“expanded in powers of 𝜀with t ﬁxed.” We
are about to re-expand each solution with
the opposite variable ﬁxed. The ﬁrst step
is to express ui in the outer variable t by
setting 𝜏= 𝜀t. The resulting function of t
and 𝜀is then expanded in powers of 𝜀to
order 𝜀k, holding t ﬁxed. This new expan-
sion is called uio, the outer expansion of
the inner solution. Notice that in comput-
ing uio we retain only the terms of degree
≤k, so that in eﬀect part of ui is discarded
because it moves up to order higher than
k; the meaning of this is that the discarded
terms of ui are insigniﬁcant, at the desired
order of approximation, in the outer region.
The second step is to express uo in the inner
variable 𝜏by setting t = 𝜏∕𝜀and expand
the resulting function of 𝜏and 𝜀in pow-
ers of 𝜀to order k holding 𝜏constant. The
result, called uoi or the inner expansion of
the outer solution, contains those parts of
uo that are signiﬁcant in the inner region (to
order k), arranged according to their signiﬁ-
cance in the inner region. The third step is to
set uio = uoi and use this equation to deter-
mine the unknown constants. The ratio-
nale for this is that if the domains of valid-
ity of the inner and outer regions overlap,
then, because the inner solution is valid in
the overlap, but the overlap belongs to the
outer region, uio, which is the inner solu-
tion stripped of the part that is insigniﬁcant
in the outer region, should be valid there;
similarly, because the outer solution is valid
in the overlap, but the overlap belongs to
the inner region, uoi should be valid there.
Now in setting uio = uoi, it is not possi-
ble to carry out the necessary computations
unless both are expressed in the same vari-
able, so it is necessary to choose either t
or 𝜏and express both sides in that vari-
able before attempting to determine the
unknown constants. The fourth step is to
compute the composite solution uc = ui +
uo −uio. At this stage, uio (which is equal
to uoi) is known as the common part of ui
and uo; it is subtracted because otherwise it
would be represented twice in the solution.
12.4.3
Slow–Fast Systems
The systems considered above, and many
others, can be put into the form
̇x = f(x, y, 𝜀)
(12.69)
𝜀̇y = g(x, y, 𝜀)
with x ∈ℝn and y ∈ℝm, which is called a
slow–fast system. When 𝜀= 0, the second
equation changes drastically, ceasing to be
diﬀerential; the motion is conﬁned to the
set g(x, y) = 0, called the slow manifold.
We now assume for simplicity that
n = m = 1, so that the vectors x and y
become scalars x and y, and the slow man-
ifold becomes the slow curve. For 𝜀≠0,
the entire (x, y) plane is available, but
(assuming 𝜕g∕𝜕y < 0, in which case we say
the slow curve is stable) any point moves
rapidly toward the slow curve and then
slowly “along” it (close to, but not actually
on it). These two stages of the motion
can be approximated separately as inner
and outer solutions and then matched. To
obtain the inner solution (the rapid part)
one rescales time by setting t = 𝜀𝜏and
obtains (with ′ = d∕d𝜏)
x′ = 𝜀f (x, y, 𝜀)
(12.70)
y′ = g(x, y, 𝜀),

12.4 Initial and Boundary Layers
433
in which 𝜀no longer multiplies a derivative.
This problem is regular (Section 12.3.1) on
ﬁnite intervals of 𝜏, which are short inter-
vals of t. For details of the matching see [4,
Section 7.7] and [12, Chapters 6 and 7].
An interesting case arises when the
slow curve is S-shaped, with the upper
and lower branches stable and the middle
section (the doubled-over part) unstable. A
point can move along a stable branch until
it reaches a vertical tangent point, then “fall
oﬀ” and move rapidly to the other stable
branch, then move along that branch to the
other vertical tangent point and “fall oﬀ”
the other way, leading to a cyclic motion
called a relaxation oscillation. In a further,
very unusual scenario, the point may actu-
ally turn the corner at the vertical tangent
point and follow the unstable branch for
some distance before “falling.” This rather
recently discovered phenomenon is called
a canard. The explanation of canards is
that several time scales become involved;
the solution is actually “falling” away from
the unstable branch all the time, but doing
so at a rate that is slow even compared to
the already slow motion along the branch.
For relaxation oscillations and canards,
see [13].
Recently, an approach to slow–fast sys-
tems (in any number of dimensions) called
geometric
singular
perturbation
theory
has come into prominence. Initiated by
Fenichel, the idea is to prove that (12.69) for
𝜀near zero has an actual invariant manifold
close to the slow manifold deﬁned above,
and that solutions near this manifold are
(in the stable case) asymptotic to solutions
in the manifold, with asymptotic phase.
The emphasis is on a clear geometric
description of the motion rather than on
computation, but computational aspects
are included. A good introduction is [14].
12.4.4
Boundary Layer Problems
Problems in which a small parameter multi-
plies the highest derivative are encountered
among boundary value problems at least
as frequently as among initial value prob-
lems. As the basic ideas have been covered
in the previous sections, it is only necessary
to point out the diﬀerences that arise in the
boundary value case. Either the multiple-
scale or matching methods may be used;
we will use matching. The method will be
illustrated here with an ordinary diﬀeren-
tial equation; a partial diﬀerential equation
will be treated in Section 12.4.5.
Consider the problem
𝜀y′′ + b(x)y′ + c(x)y = 0
(12.71)
y(0) = 𝛼
y(1) = 𝛽
on the interval 0 ≤x ≤1. The diﬀerential
equation here is the same as (12.61), only
the independent variable is a space vari-
able rather than time in view of the usual
applications. If b(x) is positive through-
out 0 < x < 1, there will be a boundary
layer at the left end point x = 0; if neg-
ative, the boundary layer will be at the
right end point; and if b(x) changes sign,
there may be internal transition layers as
well. We will consider the ﬁrst case. To the
ﬁrst order, the outer solution yo will satisfy
the ﬁrst-order equation b(x)y′ + c(x)y = 0
obtained by setting 𝜀= 0 in (12.71); it will
also satisfy the right-hand boundary condi-
tion y(1) = 𝛽. Therefore, the outer solution
is completely determined. The ﬁrst-order
inner solution yi will satisfy the equation
d2y∕d𝜉2 + b0y = 0 with b0 = b(0), obtained
by substituting the stretched variable 𝜉=
x∕𝜀into (12.71) and setting 𝜀= 0; it will
also satisfy the left-hand boundary con-
dition y(0) = 𝛼. As this is a second-order

434
12 Perturbation Methods
equation with only one boundary condi-
tion, it will contain an undetermined con-
stant that must be identiﬁed by matching
the inner and outer solutions. The diﬀer-
ential equations satisﬁed by the inner and
outer solutions are the same as in the case
of (12.61), the only diﬀerence being that
this time the constant that must be ﬁxed
by matching belongs to the inner solution
rather than the outer.
12.4.5
WKB Method
There are a great variety of problems that
are more degenerate than the one we have
just discussed, which can exhibit a wide
range of exotic behaviors. These include
internal layers, in which a stretched vari-
able such as 𝜉= (x −x0)∕𝜀must be intro-
duced around a point x0 in the interior
of the domain; triple decks, in which two
stretched variables such as x∕𝜀and x∕𝜀2
must be introduced at one end; and prob-
lems in which the order of the diﬀerential
equation drops by more than one. The sim-
plest example of the latter type is
𝜀2y′′ + a(x)y = 0.
(12.72)
This problem is usually addressed by a tech-
nique called the WKB or WKBJ method.
This method is rather diﬀerent in spirit
from the others we have discussed, because
it depends heavily on the linearity of the
perturbed problem. Rather than pose ini-
tial or boundary value problems, one ﬁnds
approximations for two linearly indepen-
dent solutions of the linear equation (12.72)
on the whole real line. The general solution
then consists of the linear combinations of
these two. If a(x) = k2(x) > 0, these approx-
imate solutions are
y(1) ≅
1
√
k(x)
cos 1
𝜀∫k(x) dx
(12.73)
and
y(2) ≅
1
√
k(x)
sin 1
𝜀∫k(x) dx.
(12.74)
If a(x) = −k2(x) < 0, the two solutions are
y(1),(2) ≅
1
√
k(x)
exp 1
𝜀∫±k(x) dx. (12.75)
A recent derivation of (12.75) is given in
Section 12.5; for classical approaches, see
[15] and [4].
If a(x) changes sign, one has a diﬃcult
situation called a turning point problem.
This can be addressed in various ways by
matching solutions of these two types or by
using Airy functions. The latter are solu-
tions of the diﬀerential equation y′′ + xy =
0, which is the simplest problem with a
turning point at the origin. These Airy func-
tions can be considered as known (they
can be expressed using Bessel functions of
order 1/3) and solutions to more general
turning point problems can be expressed in
terms of them. For an introduction to turn-
ing point problems, see [16, Chapter 2], and
for theory see [17, Chapter 8].
12.4.6
Fluid Flow
We will conclude this section with a brief
discussion of the problem of ﬂuid ﬂow past
a ﬂat plate, because of its historical impor-
tance (see Section 12.1) and because it illus-
trates two aspects of perturbation theory
that we have avoided so far: the use of
perturbation theory for partial diﬀerential
equations and the need to combine unde-
termined scales with undetermined gauges.
The classic reference for this material is
[18]. Consider a plane ﬂuid ﬂow in the
upper half-plane, with a “ﬂat plate” occu-
pying the interval 0 ≤x ≤1 on the x-axis;
that is, the ﬂuid will adhere to this interval,

12.5 The “Renormalization Group” Method
435
but not to the rest of the x-axis. The stream
function 𝜓(x, y) of such a ﬂuid will satisfy
𝜀(𝜓xxxx + 2𝜓xxyy + 𝜓yyyy) −𝜓y(𝜓xxx + 𝜓xyy)
+𝜓x(𝜓xxy + 𝜓yyy) = 0
(12.76)
with
𝜓(x, 0) = 0
for
−∞< x < ∞,
𝜓y(x, 0) = 0 for 0 ≤x ≤1, and 𝜓(x, y) →y
as
x2 + y2 →∞.
The
latter
condition
describes the ﬂow away from the plate, and
this in fact gives the leading order outer
solution as
𝜓o(x, y) = y.
(12.77)
To ﬁnd an inner solution, we stretch y by an
undetermined scale factor,
𝜂=
y
𝜇(𝜀),
(12.78)
and expand the inner solution using unde-
termined gauges, giving (to ﬁrst order)
𝜓i = 𝛿(𝜀)Ψ(x, 𝜂).
(12.79)
Substituting this into (12.76) and discard-
ing terms that are clearly of higher order
yields
𝜀
𝜇Ψ𝜂𝜂𝜂𝜂+ 𝛿[ΨxΨ𝜂𝜂𝜂−Ψ𝜂Ψx𝜂𝜂
] = 0.
(12.80)
The relative signiﬁcance of 𝜀∕𝜇and 𝛿has
not yet been determined, but if either of
them were dominant, the other term would
drop out of (12.80) to ﬁrst order, and the
resulting solution would be too simple to
capture the behavior of the problem. So we
must set
𝜀
𝜇= 𝛿
(12.81)
and conclude that the ﬁrst-order inner solu-
tion satisﬁes
Ψ𝜂𝜂𝜂𝜂+ ΨxΨ𝜂𝜂𝜂−Ψ𝜂Ψx𝜂𝜂= 0.
(12.82)
It is not possible to solve (12.82) in closed
form, but it is possible to express the solu-
tion as
Ψ(x, 𝜂) =
√
2xf
(
𝜂
√
2x
)
,
(12.83)
where f is the solution of the ordinary dif-
ferential equation f ′′′ + ﬀ′′ = 0 with f (0) =
0, f ′(0) = 0, and f ′(∞) = 1. In attempting to
match the inner and outer solutions, it is
discovered that this is only possible if 𝛿= 𝜇.
Together with (12.81), this ﬁnally ﬁxes the
undetermined scales and gauges as
𝛿= 𝜇=
√
𝜀.
(12.84)
Upon attempting to continue the solution
to higher orders, obstacles are encountered
that can only be overcome by introducing
triple decks and other innovations. See [19]
and [20].
12.5
The “Renormalization Group” Method
For many years, applied mathematicians
have wished for a uniﬁed treatment of the
types of perturbation problems studied in
Sections 12.3 and 12.4. Over the past 20
years, some progress has been made in
this direction under the name renormal-
ization group or RG method. This mathe-
matical RG method has its roots in an ear-
lier physical RG method (in quantum ﬁeld
theory and statistical mechanics) that we
do not discuss here; see [21] for the rela-
tions between these. The RG method pro-
vides a single heuristic that works for most
types of perturbation problems in ordinary
and partial diﬀerential equations. While
the common perturbation methods aban-
don the naive (or straightforward) expan-
sion when it only has pointwise validity, the

436
12 Perturbation Methods
RG method is built on the idea that the
naive solution contains all the information
necessary to construct a uniformly ordered
formal solution; this information is merely
arranged incorrectly. The RG method does
not (at least so far) automatically prove that
the uniformly ordered solution is uniformly
valid (on an intended domain). Therefore
proofs of validity have not been uniﬁed, but
are still distinct for diﬀerent types of prob-
lems. The name “RG method” is somewhat
unfortunate, because no group is involved,
but it is ﬁrmly established.
The RG method introduces two new
operations into perturbation theory, the
absorption process and the envelope process.
There are three forms of the RG method,
which we classify as follows.
1. The mixed method, which uses both the
absorption and envelope processes, was
introduced by Chen et al. [22] on the
basis of the RG method in physics, and
later simpliﬁed by Ziane [23] and Lee
DeVille et al. [24].
2. The pure envelope method, which uses
only the envelope process, was
introduced by Woodruﬀ[25] under the
name invariance method, before the
(mathematical) RG method (and with
no reference to the physical one). It was
rediscovered by Paquette [26] as an
improvement on the mixed method,
because it can handle problems in
which absorption does not work.
Nevertheless, it has not become well
known.
3. The pure absorption method, which
avoids the envelope process, was
introduced by Kirkinis [27] and
popularized in [28].
The envelope process was referred to by
other names (RG equation, invariance con-
dition) until it was identiﬁed by Kunihiro
[29, 30] as equivalent to the classical notion
of an envelope of a family of curves.
Our presentation is limited to ordinary
diﬀerential equations, and (mostly) to lead-
ing order approximations. For important
applications to partial diﬀerential equations
in ﬂuid mechanics, see [31].
12.5.1
Initial and Boundary Layer Problems
We begin with the simplest initial layer
problem
𝜀̈u + ̇u + u = 0
(12.85)
u(0) = 𝛼
̇u(0) = 𝛽
𝜀+ 𝛾,
of the form (12.61). We immediately con-
vert this to
u′′ + u′ + 𝜀u = 0
(12.86)
u(0) = 𝛼
u′(0) = 𝛽+ 𝜀𝛾,
which is of the form (12.63). Recall that ̇ =
d∕dt, ′ = d∕d𝜏, and 𝜏= t∕𝜀is the fast vari-
able; it is a general rule in the RG method
to work in the fastest natural independent
variable. Another feature of all RG meth-
ods is to set the initial (or boundary) con-
ditions aside temporarily and instead seek a
general solution with integration constants.
For methods that use absorption (methods
1 and 3), this is done as follows. Putting
u = u0 + 𝜀u1 + · · ·, the naive perturbation
method gives
u′′
0 + u′
0 = 0
(12.87)
u′′
1 + u1 = −u0

12.5 The “Renormalization Group” Method
437
so that
u0 = a + be−𝜏
(12.88)
u1 = −a𝜏+ b𝜏e−𝜏+ c + de−𝜏,
where a, b, c, and d are arbitrary. Here
−a𝜏+ b𝜏e−t is a particular solution for
u1, while c + de−𝜏is the general solution
of the associated homogeneous problem
u′′
1 + u1 = 0. It is convenient to delete
this “homogeneous part” from u1 (and
all higher ui) by allowing a and b to be
functions of 𝜀, with
a = a(𝜀) = a0 + 𝜀a1 + · · · ,
b = b(𝜀) = b0 + 𝜀b1 + · · · ;
(12.89)
thus a1 and b1 replace c and d. Then a
straightforward (or naive) approximation
to the general solution is
̃u(𝜏, 𝜀) = a + be−𝜏+ 𝜀(−a𝜏+ b𝜏e−𝜏).
(12.90)
By
regular
perturbation
theory,
this
approximation is uniformly valid with
error (𝜀2) for 0 ≤𝜏≤1, that is, in the ini-
tial layer 0 ≤t ≤𝜀. But it is not uniformly
ordered on 0 ≤𝜏≤1∕𝜀(0 ≤t ≤1), so it
cannot be uniformly valid there. (The term
−𝜀a𝜏is secular, becoming unbounded as
𝜏→∞; for 𝜏= 1∕𝜀, this term is formally
(𝜀) but actually (1). The term 𝜀b𝜏e−𝜏
is bounded, achieving its maximum at
𝜏= 1, so it is not truly secular, but is often
classiﬁed as secular anyway because it is
“secular relative to be−𝜏”).
Continuing
by
the
mixed
method
(method 1), let 𝜏0 > 0 be arbitrary, and
split the secular terms of (12.90) as follows:
̃u(𝜏, 𝜏0, 𝜀) = a + be−𝜏+ 𝜀[−a𝜏0 + b𝜏0e−𝜏
−a(𝜏−𝜏0) + b(𝜏−𝜏0)e−𝜏].
(12.91)
Now the terms in 𝜏−𝜏0 are secular, while
those in 𝜏0 alone are not (because 𝜏0 is
considered constant). Next we perform an
absorption operation by setting
ar(𝜏0, 𝜀) = a −𝜀a𝜏0 + (𝜀2),
br(𝜏0, 𝜀) = b + 𝜀b𝜏0 + (𝜀2),
(12.92)
remembering that (12.89) is still in eﬀect,
and rewriting (12.91) as
̃u(𝜏, 𝜏0, 𝜀) = ar + bre−𝜏+ 𝜀[−ar(𝜏−𝜏0)𝜏0
+br(𝜏−𝜏0)e−𝜏].
(12.93)
This is called “absorbing the secular terms
into the integration constants a and b to
produce renormalized constants ar and br.”
(“Constant” means independent of 𝜏. Later
these constants will become slowly vary-
ing functions of 𝜏.) This produces a fam-
ily of naive solutions of (12.86), one naive
solution for each 𝜏0 (ignoring initial condi-
tions). Each of the solutions in this family
is pointwise asymptotically valid with error
(𝜀2) for all t, but is uniformly valid only on
a bounded interval around 𝜏0; we say that
̃u(t, 𝜏0, 𝜀) is a family of locally valid approx-
imate solutions.
At this point, we stop to observe that
(12.93) could have been obtained at the
start, in place of (12.90). Instead of omitting
the “homogeneous part” of u1, we could
have included any “homogeneous part” that
we like in u1 (satisfying u′′
1 + u′
1 = 0), for
instance, a𝜏0 −b𝜏0e−𝜏, giving u1 = −a(𝜏−
𝜏0) + b(𝜏−𝜏0)e−𝜏, which immediately pro-
duces
̃u(𝜏, 𝜏0, 𝜀) = a + be−𝜏+ 𝜀(−a(𝜏−𝜏0)𝜏0
+b(𝜏−𝜏0)e−𝜏).
(12.94)
This is the same as (12.93) except that
there are no “renormalized variables” and
no (12.92). (This equation would never be

438
12 Perturbation Methods
used again anyway.) The idea of this “pure
envelope method” (method 2) is to select
the homogeneous part of each ui so that the
secular terms vanish at 𝜏0. Note that a and
b in (12.90) are functions of 𝜀by (12.89) and
of 𝜏0 because they can be chosen indepen-
dently for each choice of 𝜏0.
From this point on, methods 1 and 2
coincide; we use the notation of method 2.
The goal is to perform a kind of asymp-
totic matching of the (continuumly many)
local solutions (12.94) to produce a single,
smoothed-out solution that is as good as
each local solution where that local solu-
tion is good. The procedure to achieve this
goal is to diﬀerentiate ̃u with respect to
𝜏0, set 𝜏0 = 𝜏, and set the result equal to
zero. The justiﬁcation for this procedure
varies from author to author; the best are
due to Kunihiro and Paquette. Kunihiro
shows that this procedure amounts to tak-
ing the envelope of the local solutions in
such a way that the envelope is tangent
to each local solution at the point where
𝜏= 𝜏0, that is, the point where that local
solution is best. Paquette shows that the
procedure amounts to choosing the 𝜏0-
dependent coeﬃcients in (12.94) in such a
way that two local solutions with nearby
values of 𝜏0 have an overlap domain in
which both are equally valid asymptoti-
cally. (Woodruﬀ’s approach is equivalent,
but more complicated. Chen et. al. claimed
vaguely that ar and br in (12.93) should be
chosen so that ̃u does not depend on 𝜏0,
because 𝜏0 was introduced artiﬁcially into
the problem and was not part of the original
data. Thus the derivative with respect to 𝜏0
should vanish, and, again because ̃u should
not depend on 𝜏0, we should be free to set
𝜏0 = 𝜏. This argument was quickly recog-
nized as inadequate, because the derivative
does not vanish in general, but only when
𝜏0 = 𝜏.)
Now we carry out the envelope proce-
dure on the example at hand. Diﬀerentiat-
ing (12.94) gives
𝜕̃u
𝜕𝜏0
= 𝜕a
𝜕𝜏0
+ 𝜕b
𝜕𝜏0
e−𝜏+ 𝜀
(
−𝜕a
𝜕𝜏0
(𝜏−𝜏0) + a
+ 𝜕b
𝜕𝜏0
(𝜏−𝜏0)e−𝜏−be−𝜏
)
.
(12.95)
Setting 𝜏= 𝜏0 and equating to zero gives
(𝜕a
𝜕𝜏+ 𝜀a
)
+
(
𝜕b
𝜕𝜏−𝜀b
)
e−𝜏= 0. (12.96)
(Notice how setting 𝜏0 = 𝜏automatically
makes a and b into functions of 𝜏when
previously they were “constants” depending
only on 𝜏0 and 𝜀). The only plausible way to
solve (12.96) is to solve
𝜕a
𝜕𝜏+ 𝜀a = 0,
𝜕b
𝜕𝜏−𝜀b = 0
(12.97)
separately, giving
a(𝜏, 𝜀) = a(𝜀)e−𝜀𝜏,
b(𝜏, 𝜀) = b(𝜀)e𝜀𝜏.
(12.98)
(Separating (12.96) into (12.97) can be
avoided by creating two families of solu-
tions, ̃u(1) with b = 0 and ̃u(2) with a = 0,
and applying the envelope process to each
family separately, but this does not seem
to have been noticed in the literature.)
Note that a and b turn out to depend on
𝜏only through the combination 𝜀𝜏, and
are therefore slowly varying; this shows
how the RG method is able to generate the
required time scales without postulating
them in advance, as must be done in the
multiple-scale method. Now we insert
(12.98) into (12.94) with 𝜏0 = 𝜏to obtain
̃u(𝜏, 𝜀) = a(𝜀)e−𝜀𝜏+ b(𝜀)e𝜀𝜏e−𝜏.
(12.99)
Writing a(𝜀) = a0 + 𝜀a1 + · · ·, b(𝜀) = b0 +
𝜀b1 + · · ·, with undetermined coeﬃcients,

12.5 The “Renormalization Group” Method
439
we can now seek to satisfy the initial con-
ditions in (12.86), ﬁnding for instance that
a0 = 𝛼+ 𝛽and b0 = 𝛽, so that the leading
order solution is
̃u = (𝛼+ 𝛽)e−𝜏−𝛽e(−1+𝜀)𝜏.
(12.100)
It
is
easy
to
check
(rigorously)
that
e(−1+𝜀)𝜏= e−𝜏+ (𝜀)
uniformly
for
0 ≤𝜏< 0; if this replacement is made,
(12.100) coincides with the approximation
given by (12.66). As expected, 𝛾does not
appear in the leading order. Higher-order
approximations can be carried out as
well, and in this simple example, they
coincide with truncations of the exact
solution (obtained by elementary means
and expanded in 𝜀).
To solve the same problem by Kirkinis’s
method 3 (pure absorption), we begin again
from (12.90) as in method 1. This time,
however, we absorb the entire secular (and
relative secular) terms into a and b without
introducing 𝜏0, by setting
ar(𝜏, 𝜀) = a(𝜀) −𝜀a(𝜀)𝜏+ · · · ,
br(𝜏, 𝜀) = b(𝜀) + 𝜀b(𝜀)𝜏+ · · ·
(12.101)
and obtaining
̃u(𝜏, 𝜀) = ar + bre−𝜏.
(12.102)
(In this simple problem, all terms of order
𝜀are secular, so the renormalized form of
̃u looks like the leading term of the original
form. In the problems treated below, there
are usually nonsecular terms remaining at
order 𝜀.) Then we invert the equations in
(12.101) to obtain
a(𝜀) = ar(𝜀, 𝜏) + 𝜀ar(𝜀, 𝜏),
b(𝜀) = br(𝜀, 𝜏) −𝜀br(𝜀, 𝜏),
(12.103)
omitting terms that are formally of order 𝜀2.
(In a more complicated problem, equations
(12.101) might be coupled and need to be
inverted as a system. Kirkinis writes the
absorption in the inverted form from the
beginning, with undetermined coeﬃcients,
which he ﬁnds recursively.) Next we diﬀer-
entiate (12.103) with respect to 𝜏and trun-
cate, which gives
0 = dar
d𝜏+ 𝜀ar,
0 = dbr
d𝜏−𝜀br,
(12.104)
and immediately brings us to (12.97); this
diﬀerentiation step replaces the enve-
lope process as a way of ﬁnding the “RG
equations.” The solution proceeds from
here as before.
More general initial and boundary layer
problems, including nonlinear ones, may be
solved the same methods illustrated above
in the simplest case.
12.5.2
Nonlinear Oscillations
Now we consider a broad class of oscilla-
tory problems that are usually solved by the
method of averaging, namely, the class of
systems of the form
̇x = Ax + 𝜀f(x),
x(0) = c,
(12.105)
where
x ∈ℂn, A is a diagonal matrix with
eigenvalues 𝜆i on the imaginary axis, and
f(x) is a vector polynomial, expressible as a
ﬁnite sum
f(x) =
∑
𝜶,i
C𝜶ix𝜶ei,
(12.106)
with 𝜶= (𝛼1, … , 𝛼n) a nonnegative integer
vector, x𝜶= x𝛼1
1 · · · x𝛼n
n , and ei the standard
unit vectors in ℂn. This class of problems
includes all single and multiple oscillators
with weak polynomial nonlinearities and
weak coupling, when expressed as ﬁrst-
order systems in complex variables with

440
12 Perturbation Methods
reality conditions. We will solve this to
leading order by methods 2 and 3, although
our calculations are based on those of
Ziane, who uses method 1. All methods
begin by writing x = x0 + 𝜀x1 + · · · and
obtaining
̇x0 = Ax0,
̇x1 = Ax1 + 𝜀f(x0). (12.107)
For method 2, we take x0 and x1 to be
x0(t, a) = eAta,
x1(t, t0, a) = eA(t−t0)
∫
t
t0
e−Asf(eAsa)ds,
(12.108)
where a is an arbitrary vector of integration
constants and x1 is the particular solution
with zero initial conditions at an arbitrary
point t0. (Recall that by letting a depend on
𝜀, we may choose only particular solutions
at higher orders, and because x1 vanishes at
t0, so do any secular terms it may contain.)
The integrand in (12.108) has the form
e−Asf(eAsa) =
∑
𝜶,i
C𝜶ie(⟨𝝀,𝜶⟩−𝜆i)sa𝜶ei
(12.109)
= R(a) + Q(s, a),
(12.110)
where
⟨𝝀, 𝜶⟩= 𝜆1𝛼1 + · · · 𝜆n𝛼n,
a𝜶=
a𝛼1
1 · · · a𝛼n
n , R contains the terms with
⟨𝝀, 𝜶⟩−𝜆i = 0,
and
Q
the
remaining
terms. It follows by examining the integral
in (12.108) that
x1(t, t0, a) = eA(t−t0)[R(a)(t −t0) + S(t, t0, a)],
(12.111)
where S is bounded (nonsecular). Thus
̃x(t, t0, a, 𝜀) = x0 + 𝜀x1 = eAta + 𝜀eA(t−t0)
×[R(a)(t −t0) + S(t, t0, a)]
(12.112)
is a family of local approximate solutions,
and we proceed to apply the envelope pro-
cess:
𝜕̃x
𝜕t0
||||t0=t
= eAt [𝜕a
𝜕t −𝜀R(a)
]
= 0. (12.113)
It follows that
𝜕a
𝜕t = 𝜀R(a).
(12.114)
If this equation can be solved for a = a(t, 𝜀),
the solution is inserted into
̃x(t, a, 𝜀) = eAta + 𝜀[S(t, t, a)],
(12.115)
which is obtained from (12.112) by setting
t0 = t.
To compare this with the method of aver-
aging, note that (12.105) is converted into
periodic standard form by setting x = eAt𝝃
and obtaining
̇𝝃= e−Atf(eAt𝝃).
(12.116)
This corresponds to (12.37), with 𝝃for x.
The averaged equation (12.40) then coin-
cides with (12.114), with y for a. Finally,
substituting a into (12.115) corresponds to
ﬁrst substituting y into (12.42) to obtain (in
our present notation) 𝝃, and then multi-
plying that by eAt to get x. Thus the ﬁrst-
order RG solution for this class of systems is
exactly the same as the improved ﬁrst-order
solution by averaging (rather than the usual
ﬁrst-order averaging solution). This illus-
trates the remark made by several authors
that the solution given by the RG method
is sometimes better than the ones given by
more familiar perturbation methods, even
though these are asymptotically equivalent.
To solve the same problem by method
3, write the naive solution of (12.107) to

12.5 The “Renormalization Group” Method
441
(𝜀) as
̃x(t, a, 𝜀) = eAt[a + 𝜀(R(a)t + S(t, a))],
(12.117)
where a depends on 𝜀. Absorb the entire
secular term into a by writing ar(t, 𝜀) =
a(𝜀) + 𝜀tR(a(𝜀)) and then invert this to get
a(𝜀) = ar(t, 𝜀) −𝜀tR(ar(t, 𝜀)),
(12.118)
with
̃x(t, ar, 𝜀) = eAt[ar + 𝜀S(t, a)].
(12.119)
Diﬀerentiating (12.118) gives
𝜕ar
𝜕t = 𝜀
(
I −𝜀t 𝜕R
𝜕ar
)−1
×R(ar) = 𝜀R(ar) + (𝜀2),
(12.120)
in agreement with (12.114) and with the
ﬁrst-order averaged equation. As before,
the solution of (12.120) is inserted into
(12.119).
The class of problems we have considered
does not include all problems that can be
put into periodic standard form, so the RG
method does not yet completely encompass
the averaging method, even for the periodic
case.
12.5.3
WKB Problems
Finally, we look brieﬂy at the WKB problem
̈y + k2(𝜀t)y = 0,
(12.121)
which is equivalent to (12.72) with a(x) =
k2(x) and x = 𝜀t. This problem cannot be
solved directly by methods 1 or 3 that use
absorption, but can be solved by method 2,
illustrating Paquette’s claim that the pure
envelope method is the strongest of the
three forms of RG. (Chen et. al. have given
a very interesting treatment of WKB prob-
lems, including those with turning points,
by method 1, but they begin by chang-
ing variables to achieve a form for which
absorption is possible.) The central idea
is to construct local solutions by form-
ing locally valid diﬀerential equations and
solving these by “naive” expansions that
are nevertheless already generalized asymp-
totic expansions (that is, 𝜀appears in the
coeﬃcients, not just the gauges), unlike the
local solutions used in previous examples.
These local solutions are then “matched”
by the envelope process. This technique
is used by both Woodward and Paquette.
We follow Paquette’s (simpler) method, but
where he takes only the leading term of ̃y
(and so achieves a less accurate solution
than the standard WKB approximation), we
follow Woodward in taking the ﬁrst two
terms.
First we expand k2(𝜀t) in a Taylor series
in t −t0 for arbitrary t0, keeping the ﬁrst
two terms. This results in a local equation
approximating (12.121) near t0:
̈y + [k2(𝜀t0) + 2𝜀k(𝜀t0)k′(𝜀t0)(t −t0)]y = 0.
(12.122)
An approximate solution of the local
equation is sought in the form y = y0 + 𝜀y1;
we ﬁnd
̈y0 + k2(𝜀t0)y0 = 0
(12.123)
̈y1 + k2(𝜀t0)y1 = −2k(𝜀t0)k′(𝜀t0)(t −t0)y0.
(Here t0 is a constant.) Two linearly inde-
pendent solutions for y0 are
y(1),(2)
0
(t, t0, 𝜀) = e±ik(𝜀t0)(t−t0).
(12.124)

442
12 Perturbation Methods
Choosing y(1)
0 , the inhomogeneous linear
equation for y(1)
1
is solvable by undeter-
mined coeﬃcients, with solution
y1(t, t0, 𝜀) =
[ik′(𝜀t0)
2
(t −t0)2
−k′(𝜀t0)
2k(𝜀t0)(t −t0)
]
eik(𝜀t0)(t−t0),
(12.125)
and y(2)
1 is similar (but will not be needed).
We create from these the family of local
solutions
̃y = a(t0, 𝜀)
[
1 + 𝜀
(ik′(𝜀t0)
2
(t −t0)2
−k′(𝜀t0)
2k(𝜀t0)(t −t0)
)]
eik(𝜀t0)(t−t0), (12.126)
where a(t0, 𝜀) is to be found by the envelope
process: compute 𝜕̃y∕𝜕t0, put t0 = t, and set
the result equal to zero, obtaining
𝜕a(t, 𝜀)
𝜕t
= a(t, 𝜀)
[
ik(𝜀t) −𝜀k′(𝜀t)
2k(𝜀t)
]
.
(12.127)
A nonzero solution of this equation (by
separation of variables) is
a(t, 𝜀) =
1
√
k(𝜀t)
exp i ∫
t
0
k(𝜀s)ds. (12.128)
Inserting this into (12.125) with t0 = t leads
(remarkably) to exactly the same expression
for ̃y:
̃y(t, 𝜀) =
1
√
k(𝜀t)
exp i ∫k(𝜀t)dt. (12.129)
The real and imaginary parts of this solu-
tion are linearly independent, real, approx-
imate solutions of (12.121) that coincide
with (12.73) and (12.74) when expressed in
terms of x.
12.6
Perturbations of Matrices and Spectra
In this section, we address the question, if
A(𝜀) = A0 + 𝜀A1 + · · ·
(12.130)
is a matrix or linear operator depending
on a small parameter, and the spectrum of
A0 is known, can we determine the spec-
trum of A(𝜀) for small 𝜀? For the case of
a matrix, the spectrum is simply the set of
eigenvalues (values of 𝜆for which Av = 𝜆v
for some nonzero column vector v called
an eigenvector). More generally, the spec-
trum is deﬁned as the set of 𝜆for which A −
𝜆I is not invertible; for linear transforma-
tions on inﬁnite-dimensional spaces (such
as Hilbert or Banach spaces), this need not
imply the existence of an eigenvector. Our
attention here will be focused on the matrix
case, but many of the procedures (exclud-
ing those that involve the determinant or
the Jordan normal form) are applicable as
well to any operators whose spectrum con-
sists of eigenvalues. The classical reference
for the general inﬁnite dimensional case is
[32]. For matrices that are not diagonaliz-
able, one can (and should) ask not only for
the eigenvalues and eigenvectors, but also
generalized eigenvectors v for which (A −
𝜆I)kv = 0 for some integer k > 1.
The most direct approach (which we do
not recommend) to ﬁnding the eigenvalues
of (12.130) in the matrix case would be to
examine the characteristic equation
P(𝜆, 𝜀) = det(A(𝜀) −𝜆I) = 0,
(12.131)
having the eigenvalues as roots. There are
standard perturbation methods for ﬁnding
the roots of polynomials (see [4, Chapter
1]), the simplest of which is to substitute
𝜆(𝜀) = 𝜆0 + 𝜀𝜆1 + · · ·
(12.132)

12.6 Perturbations of Matrices and Spectra
443
into (12.131) and solve recursively for 𝜆i.
This method works if 𝜆0 is a simple root
of P(𝜆, 0) = 0; that is, it will work if A0 has
distinct eigenvalues. If there are repeated
eigenvalues, then in general it is necessary
to replace (12.132) with a fractional power
series involving gauges 𝛿i(𝜀) = 𝜀i∕q for some
integer q that is most readily determined
by using Newton’s diagram. Although these
are useful perturbation methods for ﬁnding
roots of general polynomials, they have two
drawbacks in the case of eigenvalues: if the
matrices are large, it is diﬃcult to compute
the characteristic polynomial; more impor-
tantly, these methods do not take account of
the special features of eigenvalue problems.
For instance, if A(𝜀) is a symmetric matrix,
then its eigenvalues will be real, and frac-
tional powers will not be required (even if
A0 has repeated eigenvalues). But the fact
that A is symmetric is lost in passing to the
characteristic polynomial, and one cannot
take advantage of these facts.
For these reasons, it is best to seek not
only the eigenvalues but also at the same
time the eigenvectors that go with them.
The general procedure (which must be
reﬁned in particular situations) is to seek
solutions of
A(𝜀)v(𝜀) = 𝜆(𝜀)v(𝜀)
(12.133)
in the form
𝜆(𝜀) = 𝜆0 + 𝜀𝜆1 + 𝜀2𝜆2 + · · ·
(12.134)
v(𝜀) = v0 + 𝜀v1 + 𝜀2v2 + · · · .
In the ﬁrst two orders, the resulting
equations are
(A0 −𝜆0I)v0 = 0
(12.135)
(A0 −𝜆0I)v1 = (𝜆1I −A1)v0.
We will now discuss how to solve (12.135)
under various circumstances.
The simplest case occurs if A0 is real and
symmetric (or complex and Hermitian),
and also has distinct eigenvalues. In this
case, the ﬁrst equation of (12.135) can be
solved simply by choosing an eigenvector v0
for each eigenvalue 𝜆0 of A0. It is convenient
to normalize v0 to have length one, that is,
(v0, v0) = 1 where (⋅, ⋅) is the inner (or “dot”)
product. Now we ﬁx a choice of 𝜆0 and v0
and insert these into the second equation
of (12.135). The next step is to choose 𝜆1
so that the right-hand side lies in the image
of A0 −𝜆0I; once this is accomplished, it is
possible to solve for v1. To determine 𝜆1, we
rely upon special properties of the eigen-
vectors of a symmetric matrix; namely, they
are orthogonal (with respect to the inner
product). Thus there exists an orthogonal
basis of eigenvectors in which A0 is diag-
onal; examining A0 −𝜆0I in this basis, we
see that its kernel (or null space) is spanned
by v0 and its image (or range) is spanned by
the rest of the eigenvectors. Therefore, the
image is perpendicular to the kernel. It fol-
lows that (𝜆1I −A1)v0 will lie in the image
of A0 −𝜆0I if and only if its orthogonal pro-
jection onto v0 is zero, that is, if and only if
(𝜆1v0 −A1v0, v0) = 0, or, using (v0, v0) = 1,
𝜆1 = (A1v0, v0).
(12.136)
It is not necessary to ﬁnd v1 unless it is
desired to go on to the next stage and ﬁnd
𝜆2. (There is a close similarity between
these steps and those of the Lindstedt
method in Section 3.1, in which each term
in the frequency expansion is determined
to make the next equation solvable.)
If A0 has distinct eigenvalues but is not
symmetric, most of the last paragraph still
applies, but the eigenvectors of A0 need
not be orthogonal. The vector space still
decomposes as a direct sum of the image
and kernel of A0 −𝜆0I, but the inner prod-
uct can no longer be used to eﬀect the

444
12 Perturbation Methods
decomposition; 𝜆1 can still be determined
but cannot be written in the form (12.136).
If A0 does not have distinct eigenval-
ues, the situation can become quite com-
plicated. First, suppose A(𝜀) is symmetric
for all 𝜀, so that all Ai are symmetric. In
this case, every eigenvalue has a “full set”
of eigenvectors (as many linearly indepen-
dent eigenvectors as its algebraic multiplic-
ity). However, suppose that A0 has an eigen-
value 𝜆0 of multiplicity two, with eigen-
vectors z and w. It is likely that for 𝜀≠0,
the eigenvalue 𝜆0 splits into two distinct
eigenvalues having separate eigenvectors.
In this case, it is not possible to choose
an arbitrary eigenvector v0 from the plane
of z and w to use in the second equation
of (12.135); only the limiting positions (as
𝜀→0) of the two eigenvectors for 𝜀≠0
are suitable candidates for v0. As these are
unknown in advance, one must put v0 =
az + bw (for unknown real a and b) into
(12.135), then ﬁnd two choices of a, b, and
𝜆1 that make the second equation solvable.
It also may happen that the degeneracy can-
not be resolved at this stage but must be
carried forward to higher stages before the
eigenvalues split; or, of course, they may
never split.
If A(𝜀) is not symmetric, and hence not
necessarily diagonalizable, the possibilities
become even worse. The example
A(𝜀) =
[ 1
𝜀
0
1
]
(12.137)
shows that a full set of eigenvectors may
exist when 𝜀= 0 but not for 𝜀≠0; the con-
trary case (diagonalizable for 𝜀≠0 but not
for 𝜀= 0) is exhibited by
A(𝜀) =
[ 1
1
0
1 + 𝜀
]
.
(12.138)
These examples show that the Jordan nor-
mal form of A(𝜀) is not, in general, a con-
tinuous function of 𝜀.
There is a normal form method, closely
related to that of Section 3.6, that is suc-
cessful in all cases. It consists in simplifying
the terms of (12.130) by applying succes-
sive coordinate transformations of the
form I + 𝜀kSk for k = 1, 2, … or a single
coordinate transformation of the form
T(𝜀) = I + 𝜀T1 + 𝜀2T2 + · · ·; the matrices
Sk or Tk are determined recursively. It is
usually assumed that A0 is in Jordan canon-
ical form, hence is diagonal if possible. If A0
is diagonal and A(𝜀) is diagonalizable, the
normalized Ak will be diagonal for k ≥1,
so that (12.130) will give the asymptotic
expansion of the eigenvalues and T(𝜀) the
asymptotic expansion of all the eigenvalues
(as its columns). In more complicated
cases, the normalized series (12.130) will
belong to a class of matrices called the
Arnol’d unfolding of A0, and although it
will not always be in Jordan form, it will
be in the simplest form compatible with
smooth dependence on 𝜀. Still further
simpliﬁcations (the metanormal form) can
be obtained using fractional powers of 𝜀.
This theory is described in [7, Chapter 2].
Glossary
Asymptotic approximation: An approxi-
mate solution to a perturbation problem
that increases in accuracy at a known rate
as the perturbation parameter approaches
zero.
Asymptotic series: A series, the partial
sums of which are asymptotic approxi-
mations of some function to successively
higher order.
Averaging: A method of constructing
asymptotic approximations to oscillatory
problems. In the simplest case, it involves

Glossary
445
replacing
periodic
functions
by
their
averages to simplify the equations to be
solved.
Bifurcation: Any change in the number or
qualitative character (such as stability) of
the solutions to an equation as a parameter
is varied.
Boundary layer: A transition layer located
near the boundary of a region where
boundary values are imposed.
Composite solution: A solution uniformly
valid on a certain domain, created by
matching an inner solution and an outer
solution each valid on part of the domain.
Gauge: A monotonic function of a pertur-
bation parameter used to express the order
of a term in an asymptotic series.
Generalized series: An asymptotic series
of the form ∑𝛿i(𝜀)ui(x, 𝜀) in which the per-
turbation parameter 𝜀appears both in the
gauges and in the coeﬃcients. See Poincaré
Series.
Initial layer: A transition layer located near
the point at which an initial value is pre-
scribed.
Inner solution: An approximate solution
uniformly valid within a transition layer.
Lie series: A means of representing a near-
identity transformation by a function called
a generator. There are several forms; in
Deprit’s form, if W(x, 𝜀) is the generator,
then the solution of dx∕d𝜀= W(x, 𝜀) with
x(0) = y for small 𝜀is a near-identity trans-
formation of the form x = y + 𝜀u1(y) + · · ·.
Lindstedt method: A method of approxi-
mating periodic solutions whose frequency
varies with the perturbation parameter by
using a scaled time variable.
Matching: Any of several methods for
choosing the arbitrary constants in an
inner and an outer solution so that they
both approximate the same exact solution.
Multiple scales: The simultaneous use
of two or more variables having the same
physical signiﬁcance (for instance, time
or distance) but proceeding at diﬀerent
“rates” (in terms of the small parameter),
for instance “normal time” t and “slow
time” 𝜏= 𝜀t. The variables are treated as if
they were independent during part of the
discussion, but at the end are reduced to a
single variable again.
Naive
expansion:
A
Poincaré
series
obtained
by
the
regular
perturbation
method (straightforward expansion), pro-
posed as a solution to a singular problem,
where a generalized series is required. See
RG method.
Outer solution: An approximate solution
uniformly valid in a region away from a
transition layer.
Overlap domain: A region in which both
an inner and an outer approximation are
valid, and where they can be compared for
purposes of matching.
Perturbation parameter: A parameter,
usually denoted 𝜀, occurring in a mathe-
matical problem, such that the problem
has a known solution when 𝜀= 0 and an
approximate solution is sought when 𝜀is
small but nonzero.
Perturbation series: A ﬁnite or inﬁnite
series obtained as a formal approximate
solution to a perturbation problem, in the
hope that it will be uniformly asymptoti-
cally valid on some domain.
Poincaré series: An asymptotic series of
the form ∑𝛿i(𝜀)ui(x) in which the per-
turbation parameter 𝜀appears only in the
gauges. See Generalized Series.
Regular perturbation problem: A per-
turbation problem having an approximate
solution in the form of a Poincaré series that
is uniformly valid on the entire intended
domain.
Relaxation oscillation: A self-sustained
oscillation characterized by a slow buildup
of tension (in a spring, for instance) fol-
lowed by a rapid release or relaxation. The

446
12 Perturbation Methods
rapid phase is an example of a transition
layer.
Rescaled coordinate: A coordinate that
has been obtained from an original vari-
able by a transformation depending on the
perturbation parameter, usually by multi-
plying by a scaling factor. For instance time
t may be rescaled to give a “slow time” 𝜀t
(see multiple scales) or a “strained time”
(𝜔0 + 𝜀𝜔1 + … )t (see Lindstedt method).
Resonance: In linear problems, an equality
of two frequencies. In nonlinear prob-
lems, any integer relationship holding
between two or more frequencies, of the
form
𝜈1𝜔1 + · · · + 𝜈k𝜔k = 0,
especially
one involving small integers or one that
produces zero denominators in a Fourier
series.
RG (renormalization group) method: A
method (not involving group theory) that
converts a naive expansion for a singular
problem into a generalized expansion that
may be uniformly valid on the intended
domain.
Self-excited oscillation: An oscillation
about an unstable equilibrium that occurs
because of instability and has its own natu-
ral frequency, rather than an oscillation in
response to an external periodic forcing.
Singular perturbation problem: A pertur-
bation problem that cannot be uniformly
approximated by a Poincaré series on the
entire intended domain, although this may
be possible over part of the domain. For sin-
gular problems, one seeks a solution in the
form of a generalized series.
Transition layer: A small region in which
the solution of a diﬀerential equation
changes rapidly and in which some approx-
imate solution (outer solution) that is valid
elsewhere fails.
Triple deck: A problem that exhibits a
transition layer within a transition layer
and that therefore requires the matching
of three approximate solutions rather than
only two.
Unfolding: A family of perturbations of a
given system obtained by adding several
small parameters. An unfolding is universal
if (roughly) it exhibits all possible qualita-
tive behaviors for perturbations of the given
system using the least possible number of
parameters.
Uniform approximation: An approximate
solution whose error is bounded by a con-
stant times a gauge function everywhere on
an intended domain.
References
1. Golubitsky, M. and Schaeﬀer, D.G. (1985)
Singularities and Groups in Bifurcation
Theory, vol. 1, Springer-Verlag, New York.
2. Iooss, G. and Joseph, D.D. (1980) Elementary
Stability and Bifurcation Theory,
Springer-Verlag, New York.
3. Sanders, J.A., Verhulst, F., and Murdock, J.
(2007) Averaging Methods in Nonlinear
Dynamical Systems, Springer-Verlag, New
York.
4. Murdock, J.A. (1999) Perturbations: Theory
and Methods, Society for Industrial and
Applied Mathematics, Philadelphia, PA.
5. Lochak, P. and Meunier, C. (1988)
Multiphase Averaging for Classical Systems,
Springer-Verlag, New York.
6. Nayfeh, A. (1973) Perturbation Methods,
John Wiley & Sons, Inc., New York.
7. Murdock, J.A. (2003) Normal Forms and
Unfoldings for Local Dynamical Systems,
Springer-Verlag, New York.
8. Nayfeh, A. (1993) Method of Normal Forms,
John Wiley & Sons, Inc., New York.
9. Kahn, P.B. and Zarmi, Y. (1998) Nonlinear
Dynamics: Exploration through Normal
Forms, John Wiley & Sons, Inc., New York.
10. Wiggins, S. (2003) Introduction to Applied
Nonlinear Dynamical Systems and Chaos,
Springer-Verlag, New York.
11. Lagerstrom, P.A. (1988) Matched Asymptotic
Expansions, Springer-Verlag, New York.
12. Smith, D.R. (1985) Singular-Perturbation
Theory, Cambridge University Press,
Cambridge.

Further Reading
447
13. Grasman, J. (1987) Asymptotic Methods for
Relaxation Oscillations and Applications,
Springer-Verlag, New York.
14. Jones, C.K.R.T. (1994) Geometric singular
perturbation theory, in Dynamical Systems
(Montecatini Terme, 1994), Lecture Notes in
Mathematics 1609, Springer-Verlag, New
York, pp. 44–118.
15. Bender, C.M. and Orszag, S.A. (1999)
Advanced Mathematical Methods for
Scientists and Engineers, Springer-Verlag,
New York.
16. Lakin, W.D. and Sanchez, D.A. (1970) Topics
in Ordinary Diﬀerential Equations, Dover,
New York.
17. Wasow, W. and Robert, E. (1976) Asymptotic
Expansions for Ordinary Diﬀerential
Equations, Krieger Publishing Co.,
Huntington, NY.
18. van Dyke, M. (1975) Perturbation Methods
in Fluid Mechanics, Annotated Edition,
Parabolic Press, Stanford, CA.
19. Sychev, V.V., Ruban, A.I., Sychev, V.V., and
Korolev, G.L. (1998) Asymptotic Theory of
Separated Flows, Cambridge University
Press, Cambridge.
20. Rothmayer, A.P. and Smith, F.T. (1998)
Incompressible triple- deck theory, in The
Handbook of Fluid Dynamics (ed. R.W.
Johnson), CRC Press, Boca Raton, FL.
21. Oono, Y. (2000) Renormalization and
asymptotics. Int. J. Mod. Phys. B, 14,
1327–1361.
22. Chen, L.-Y., Goldenfeld, N., and Oono, Y.
(1996) Renormalization group and singular
perturbations. Phys. Rev. E, 54, 376–394.
23. Ziane, M. (2000) On a certain
renormalization group method. J. Math.
Phys., 41, 3290–3299.
24. Lee DeVille, R.E., Harkin, A., Holzer, M.,
Josi´c, K., and Kaper, T.J. (2008) Analysis of a
renormalization group method for solving
perturbed ordinary diﬀerential equations,
Physica D, 237, 1029–1052.
25. Woodruﬀ, S.L. (1993) The use of an
invariance condition in the solution of
multiple-scale singular perturbation
problems: ordinary diﬀerential equations.
Stud. Appl. Math., 90, 225–248.
26. Paquette, G.C. (2000) Renormalization
group analysis of diﬀerential equations
subject to slowly modulated perturbations.
Physica A, 276, 122–163.
27. Kirkinis, E. (2008) The renormalization
group and the implicit function theorem for
amplitude equations. J. Math. Phys., 49,
1–16. article 073518.
28. Kirkinis, E. (2012) The renormalization
group: a perturbation method for the
graduate curriculum. SIAM Rev., 54,
374–388.
29. Kunihiro, T. (1995) A geometrical
formulation of the renormalization group
method for global analysis. Progr. Theor.
Phys., 94, 503–514; errata, same volume
(94), 835.
30. Kunihiro, T. (1997) The renormalization-
group method applied to asymptotic analysis
of vector ﬁelds. Progr. Theor. Phys., 97,
179–200.
31. Veysey, J. and Goldenfeld, N. (2007) Simple
viscous ﬂows: from boundary layers to the
renormalization group. Rev. Mod. Phys., 79,
883–927.
32. Kato, T. (1966) Perturbation Theory for
Linear Operators, Springer-Verlag, New
York.
Further Reading
Andrianov, I.V. and Manevitch, L.I. (2002)
Asymptotology, Kluwer, Dordrecht.
Bender, C.M. and Orszag, S.A. (1999) Advanced
Mathematical Methods for Scientists and
Engineers, Springer-Verlag, New York.
Bush, A.W. (1992) Perturbation Methods for
Engineers and Scientists, CRC Press, Boca
Raton, FL.
Hinch, E.J. (1991) Perturbation Methods,
Cambridge University Press, Cambridge.
Kevorkian, J. and Cole, J.D. (1981; corrected
second printing 1985) Perturbation Methods
in Applied Mathematics, Springer-Verlag,
New York.
Nayfeh, A. (1981) Introduction to Perturbation
Techniques, John Wiley & Sons, Inc., New
York.
O’Malley, R.E. (1991) Singular Perturbation
Methods for Ordinary Diﬀerential Equations,
Springer-Verlag, New York.


449
13
Functional Analysis
Pavel Exner
The area covered by the chapter title is
huge. We describe the part of functional
analysis dealing with linear operators, in
particular, those on Hilbert spaces and their
spectral decompositions, which are impor-
tant for applications in quantum physics.
The claims are presented without proofs,
often highly nontrivial, for which we refer
to the literature mentioned at the end of the
chapter.
13.1
Banach Space and Operators on Them
The ﬁrst section summarizes notions from
the general functional analysis that we will
need in the following.
13.1.1
Vector and Normed Spaces
The starting point is the notion of a vec-
tor space V over a ﬁeld 𝔽as a family of
vectors equipped with the operations of
summation and multiplication by a scalar
𝛼∈𝔽. The former is commutative and
associative, and the two operations are
mutually distributive. We consider here
the case when 𝔽= ℂ, the set of complex
numbers, but other ﬁelds are also used,
typically the sets of reals or quaternions.
Standard examples are ℂn, the space of
n-tuples of complex numbers, or 𝓁p, the
space of complex sequences {xj}∞
j=1 satisfy-
ing ∑∞
j=1 |xj|p < ∞for a ﬁxed p ≥1. Other
frequently occurring examples are function
spaces such as C() consisting of continu-
ous functions on the interval , or Lp() the
elements of which are classes of functions
diﬀering on a zero-measure set and satis-
fying ∫|f (x)|p dx < ∞. The dimension of a
vector space is given by the maximum num-
ber of linearly independent vectors one can
ﬁnd in it. In the examples, the space ℂn is
n-dimensional, the other indicated ones are
(countably) inﬁnite-dimensional.
A map f ∶V →ℂis called a functional.
We say that it is linear if f (𝛼x) = 𝛼f (x)
and antilinear if f (𝛼x) = 𝛼f (x) for all
𝛼∈ℂ
and
x ∈V.
Similarly,
a
semi-
norm is a real functional p satisfying
p(x + y) ≤p(x) + p(y) and p(𝛼x) = |𝛼|p(x).
A map F ∶V × V →ℂis called a form.
Its (anti)linearity is deﬁned as above; it
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

450
13 Functional Analysis
is called sesquilinear if it is linear in one
argument and antilinear in the other.
A norm on a vector space is a semi-
norm, usually denoted as ‖ ⋅‖, with the
property
that
‖x‖ = 0
implies
x = 0.
A space equipped with it is called a
normed space. The same vector space can
be equipped with diﬀerent norms, for
example both ‖x‖∞∶= sup1≤j≤n |xj| and
‖x‖p ∶= ( ∑∞
j=1 |xj|p)1∕p are norms on ℂn.
Analogous norms can be introduced on the
other vector spaces mentioned above, for
instance, ‖f ‖∞∶= supx∈|f (x)| on C() or
‖f ‖p ∶= ( ∫|f (x)|p dx)1∕p on Lp().
The space can be equipped with an inner
(or scalar) product, which is a positive
sesquilinear form (⋅, ⋅) with the property
that (x, x) implies x = 0. In such a case,
one way to deﬁne a norm is through the
formula ‖x‖ ∶=
√
(x, x). This norm is said
to be induced by the scalar product; it
satisﬁes the Schwarz inequality
|(x, y)| ≤‖x‖‖y‖ .
(13.1)
On the other hand, a norm ‖ ⋅‖ can be
induced by a scalar product if and only
if it satisﬁes the parallelogram identity,
‖x + y‖2 + ‖x −y‖2 = 2‖x‖2 + 2‖y‖2.
In this way, a vector space is equipped
with a metric structure with the distance
of
points
given
by
d(x, y) ∶= ‖x −y‖.
Then one can ask about completeness of
such a space, that is, whether any Cauchy
sequence, {xn} ⊂V satisfying d(xn, xm) →
0 as n, m →∞, has a limit in V. A (met-
rically) complete normed space is called a
Banach space; similarly, a complete inner-
product space is called a Hilbert space. The
spaces 𝓁p and Lp() are examples of Banach
spaces, they are Hilbert if p = 2.; The met-
ric induces, mutatis mutandis, a topology
on a vector space making it an example
of topological vector space in which the
vector and topological properties are com-
bined in such a way that the operations
of summation and scalar multiplication
are continuous. A vector space of inﬁnite
dimension can be equipped with diﬀerent,
mutually inequivalent topologies. Most
important are locally convex topologies
generated by a family of seminorms with
the property that for each nonzero x ∈V
there is a seminorm p such that p(x) ≠0.
The norm topology is a particular case.
The presence of topology allows us to
investigate various properties of subsets
and subspaces of V such as compactness,
openness and closeness [14, Chap. II; 20,
Chap. 1], or convergence of sequences
[1, Chap. IV; 2, Chap. 1]. If the space is
equipped with diﬀerent topologies, the
same sequence may converge with respect
to some of them and have no limit with
respect to others.
13.1.2
Operators on Banach Spaces
A map between two Banach spaces, B ∶
→, is called an operator. It is linear
if B(𝛼x + y) = 𝛼Bx + By holds; we will con-
sider here linear operators only and there-
fore drop the adjective. Such an operator is
continuous iﬀit is bounded, which means
there exists a number c such that ‖Bx‖≤
c‖x‖holds for all x ∈. The inﬁmum of
such numbers is called the norm ‖B‖ of B.
One has ‖B‖ = supS1 ‖Bx‖, where S1 ∶=
{x ∈∶‖x‖= 1} is the unit sphere in .
The functional ‖ ⋅‖ is indeed a norm; in
particular, the space (, ) of all bounded
operators from to equipped with it
becomes a Banach space. In addition, for
the composition of operators B ∈(, )
and C ∈(, ), the operator norm satis-
ﬁes the inequality ‖BC‖ ≤‖B‖‖C‖.
In general, an operator can be deﬁned on
a subspace of only, which is then called

13.1 Banach Space and Operators on Them
451
the domain of B, denoted by D(B). If D(B) is
dense in and B is bounded, then there is
a unique extension ̃B ∈(, ) =∶()
of the operator B to the space , and more-
over ‖̃B‖ = ‖B‖ holds, hence bounded
operators may be without loss of gen-
erality supposed to be deﬁned on the
whole .
On the other hand, a domain can be
dense in diﬀerent Banach spaces having
thus diﬀerent extensions. As an example,
consider
the
Fourier
transformation
deﬁned by
̂f (y) = (2𝜋)−1∕2
∫ℝ
e−ixy f (x) dx
(13.2)
on the set (ℝ) of inﬁnitely diﬀeren-
tiable functions f ∶ℝ→ℂ, which have,
together with all their derivatives, a faster-
than-powerlike decay. (ℝ) is dense in
L1(ℝ), hence the map f →̂f extends to
a unique operator from L1(ℝ) to the
space
C∞(ℝ)
of
continuous
functions
satisfying lim|y|→∞̂f (y) = 0 (this property
is usually referred to as the Riemann–
Lebesgue
lemma),
which
is
bounded,
‖̂f ‖∞≤(2𝜋)−1∕2‖f ‖1.
However, (ℝ) is dense also in L2(ℝ) and
f →̂f can be thus uniquely extended to
a map F ∶L2(ℝ) →L2(ℝ) called Fourier–
Plancherel operator with the norm ‖F‖ = 1.
The right-hand side of (13.2) may not be
deﬁned for a general f ∈L2(ℝ), hence we
have to write the action of F as
(Ff )(y) = l.i.m.
n→∞(2𝜋)−1∕2
∫|x|≤n
e−ixy f (x) dx ,
(13.3)
where the symbol l.i.m., limes in medio,
means convergence with respect to the
norm of L2(ℝ). In a similar way, one
deﬁnes the Fourier transformation and
Fourier–Plancherel operator on functions
f ∶ℝn →ℂ, n > 1.
Functionals represent a particular case of
operators with = ℂ. The space (, ℂ)
of bounded functionals is called the dual
to the Banach space and denoted as
∗. It is a Banach space with the norm
‖f ‖ = sup‖x‖=1 |f (x)|. In some cases, the
dual space can be determined explicitly, for
instance, (𝓁p)∗is isomorphic to 𝓁q, where
q = p∕(p −1) for 1 < p < ∞and q = ∞for
p = 1.
The metric completeness on Banach
spaces
implies
the
uniform
bound-
edness
principle
for
an
operator
family
⊂(, V)
where
V
is
a
normed space with the norm ‖ ⋅‖: if
supB∈‖Bx‖ < ∞, then there is a c > 0
such
that
supB∈‖B‖ < c.
This
result
has several important consequences, in
particular,
Theorem 13.1 (Open-map theorem) If
an operator (, ) maps to the whole
Banach space and G ⊂is an open set,
then the image set BG is open in .
Theorem 13.2 (Inverse-mapping
theorem)
If
(, )
is
a
one-to-one
map, then B−1 is a continuous linear map
from to .
One more consequence concerns closed
operators, which are those whose graph
Γ(T) = {[x, Tx] ∶x ∈D(T)} is a closed
set in ⊕. Alternatively, they can be
characterized
by
sequences:
an
oper-
ator T is closed iﬀ
for any sequence
{xn} ⊂D(T)
such
that
xn →x
and
Txn →y as n →∞, we have x ∈D(T)
and y = Tx.
Theorem 13.3 (Closed-graph theorem)
A closed linear operator T ∶→deﬁned
on the whole of is continuous.

452
13 Functional Analysis
13.1.3
Spectra of Closed Operators
Consider now the set () of closed linear
operators mapping the Banach space 
to itself. It follows from the closed-graph
theorem that if T ∈() is unbounded,
its domain D(T) ≠. A number 𝜆∈ℂ
is called an eigenvalue of T if there is a
nonzero x ∈D(T), called an eigenvector,
such that Tx = 𝜆x. The span of such vectors
is a subspace in called eigenspace (related
to the eigenvalue 𝜆) and its dimension
is the (geometric) multiplicity of 𝜆. Any
eigenspace of a closed operator T ∈()
is closed.
This is a familiar way to describe the
spectrum known from linear algebra. An
alternative way is to notice that the inverse
(T −𝜆)−1 is not deﬁned if 𝜆is an eigen-
value. John von Neumann was the ﬁrst to
notice that this oﬀers a ﬁner tool in case
of unbounded operators when (T −𝜆)−1
may exist as an unbounded operator, which
means that T −𝜆does not map onto
itself but to its proper subspace only.
Consequently,
we
deﬁne
the
spec-
trum 𝜎(T) of T as the set consisting of
three parts: the point spectrum 𝜎p(T)
being the family of all eigenvalues, the
continuous
spectrum
𝜎c(T)
for
which
Ran(T −𝜆) ≠but it is dense in , and
ﬁnally the residual spectrum 𝜎r(T) for
which Ran(T −𝜆) ≠, where the symbol
conventionally denotes the closure of
the set . The spectrum 𝜎(T) is a closed
set, its complement 𝜌(T) = ℂ⧵𝜎(T) is
called the resolvent set of the operator T
and the map RT ∶𝜌(T) →() deﬁned by
RT(𝜆) ∶= (T −𝜆)−1 is the resolvent of T.
In
general,
the
spectrum
may
be
empty. For a bounded operator B on a
Banach space , however, the spectrum
is always a nonempty compact set and
its
radius
r(B) = sup{|𝜆| ∶𝜆∈𝜎(T)} =
limn→∞‖Bn‖1∕n ≤‖B‖. The resolvent can
be in this case expressed through the
Neumann series,
RB(𝜆) = −𝜆−1I −
∞
∑
k=1
𝜆−(k+1)Bk,
(13.4)
which converges outside the circle of spec-
tral radius, that is, for |𝜆| > ‖B‖.
13.2
Hilbert Spaces
As a particular case of Banach spaces,
Hilbert spaces share the general prop-
erties listed above but the existence of
inner product gives them speciﬁc features.
Hilbert spaces are important especially
from the viewpoint of quantum theory
where they are used as state spaces, the
pure states of such systems being identiﬁed
with one-dimensional subspaces of an
appropriate Hilbert space.
13.2.1
Hilbert-Space Geometry
Hilbert spaces are distinguished by the
existence of orthogonal projection: given a
closed subspace ⊂and a vector x ∈,
there is a unique vector yx ∈such that
the
distance
d(x, ) ∶= infy∈‖x −y‖ =
‖x −yx‖. It implies that each vector can be
uniquely decomposed into a sum, x = y + z
with y ∈and z belonging to ⟂, the
orthogonal complement of in . It also
provides a criterion for a set M ⊂to be
total, that is, such that its linear envelope
is dense in : it happens iﬀM⟂= {0}.
Another
characteristic
feature
of
a
Hilbert space is the relation to its dual: by
the Riesz lemma to any f ∈∗, there is a
unique yf ∈such that f (x) = (yf , x); the
map f →yf is an antilinear isometry of 

13.2 Hilbert Spaces
453
and ∗. Functionals allow us to introduce
a convergence in a Hilbert space diﬀerent
from the one determined by the norm of
: a sequence {xn} ⊂converges weakly
if (y, xn) →(y, x) holds for all y ∈. If
one has in addition ‖xn‖ →‖x‖, then {xn}
converges also in the norm.
A family {e𝛼} ⊂of pairwise orthog-
onal vectors of unit length is called an
orthonormal basis of if it is total. Such
a basis always exists and any two orthonor-
mal bases in a given have the same car-
dinality, which is called the dimension of
. Hilbert spaces , ′ are isomorphic iﬀ
dim = dim ′. A Hilbert space is separa-
ble if its dimension is at most countable.
Given an orthonormal basis {e𝛼}, we
deﬁne Fourier coeﬃcients of a vector x ∈
as (e𝛼, x). Using them, we can write the
Fourier expansion
x =
∑
𝛼
(e𝛼, x)e𝛼
(13.5)
of x; it always makes sense as a convergent
series because the set of nonzero Fourier
coeﬃcients is at most countable even if the
basis {e𝛼} is uncountable, and because the
Parseval identity is valid,
‖x‖2 =
∑
𝛼
|(e𝛼, x)|2.
(13.6)
The orthonormal basis most often used
in the space 𝓁2 consists of the vectors 𝜙n ∶=
{0, … , 0, 1, 0, …} with the nonzero entry
on the nth place. In case of spaces L2(),
the choice depends on the interval. For the
ﬁnite interval = (0, 2𝜋), the trigonometric
basis, {ek ∶k = 0, ±1, ±2, … } with ek(x) =
(2𝜋)−1∕2eikx, is used. On the whole line, =
ℝ, the functions
hn(x) = (2nn!)−1∕2𝜋−1∕4e−x2∕2Hn(x),
n = 0, 1, 2, … ,
(13.7)
where
Hn(x) = (−1)Nex2dn∕dxne−x2
are
Hermite polynomials, form an orthonormal
basis. Similarly, one can construct a basis
for the semi-inﬁnite interval = (0, ∞)
using Laguerre polynomials.
Let us mention two other examples of
Hilbert spaces. Given a set M equipped
with measure 𝜇and a Hilbert space
,
we
consider
measurable
vector-
valued functions f ∶M →such that
∫M ‖f (x)‖2
d𝜇(x) < ∞. We regard classes
of such functions diﬀering on a set of zero
measure as elements of a new space. One
can check that it is a Hilbert space with
respect to the inner product
(f , g) ∶= ∫M
(f (x), g(x))2
d𝜇(x);
(13.8)
we denote this space as L2(X, d𝜇; ). Such
spaces appear in various applications, for
instance, in quantum mechanics, is often
a ﬁnite-dimensional space associated with
spin states of the system.
Our second example is the Hilbert space
of analytic functions f ∶ℂ→ℂequipped
this time with the “weighted” inner prod-
uct,
(f , g) = 1
𝜋∫ℂ
f (z)g(z) e−|z|2 dz,
(13.9)
where dz is a shorthand for the “two-
dimensional”
measure
d(Rez) d(Imz)
in the complex plane. An orthonormal
basis in it is formed by the monomials,
un(z) = (z!)−1∕2zn, n = 0, 1, … .
Denoting
ez(w) = ezw, we have the identity
f (w) = 1
𝜋∫ℂ
(ez, f ) ez(w) e−|z|2 dz,
(13.10)
which can be regarded as a continuous ana-
logue of the Fourier expansion with respect
to an “overcomplete basis;” this relation
plays the central role in description of
quantum-mechanical coherent states.

454
13 Functional Analysis
13.2.2
Direct Sums and Tensor Products
Applications require many diﬀerent spaces,
hence it is useful to know how to con-
struct new Hilbert spaces from given ones.
One possibility are the L2 spaces of vector-
valued functions mentioned above; here we
add two more methods. The ﬁrst is based
on direct sums. Let {k} be a family of
Hilbert spaces, for simplicity supposed to
be at most countable, with norm of k
being denoted by ‖ ⋅‖k. We consider the set
of sequences X = {xk ∶xk ∈k} such that
∑
k ‖xk‖2
k < ∞, equip it with vector oper-
ations deﬁned componentwise and deﬁne
the inner product on it by
(X, Y) =
∑
k
(xk, yk)k,
(13.11)
where (⋅, ⋅)k is the inner product in k,
obtaining a new Hilbert space, which we
denote as ⨁
k k or ∑⊕
k k. The dimension
of the direct product is clearly the sum of
the dimension of its components. In a sim-
ilar way, one can deﬁne, under appropriate
measurability hypotheses, a direct integral
∫⊕
M (x) d𝜇(x) with respect to a measure 𝜇
of a family of Hilbert spaces dependent on
the integration variable x.
The deﬁnition of a tensor product of
Hilbert spaces 1, 2 is more involved. We
introduce ﬁrst a tensor product realiza-
tion as a bilinear map ⊗∶1 × 2 →,
which associates with x ∈1 and y ∈2
an element x ⊗y ∈in such a way that
(x ⊗y, x′ ⊗y′) = (x, x′)1(y, y′)2
holds
for
all x, x′ ∈1 and y, y′ ∈2 and the set
1 ⊗2 is total in . For each pair 1, 2,
a realization of their tensor product exists,
and furthermore, all such realizations are
isomorphic to each other.
This allows us to investigate the tensor
product through a ﬁxed realization of it, in
particular, to write 1 ⊗2 = having in
mind a concrete map ⊗. Frequently appear-
ing examples are the relations
L2(M, d𝜇)⊗L2(N, d𝜈)=L2(M × N, d(𝜇⊗𝜈)) ,
(13.12)
L2(X, d𝜇) ⊗= L2(X, d𝜇; ) ,
where 𝜇⊗𝜈denotes the product mea-
sure
of
𝜇
and
𝜈,
deﬁned
through
the
maps
(f ⊗g)(x, y) = f (x)g(y)
and
(f ⊗𝜙)(x)=f (x)𝜙, respectively. If {e(1)
𝛼}
and
{e(2)
𝛽}
are
orthonormal
bases
in
the two Hilbert spaces, then the vec-
tors
e(1)
𝛼⊗e(2)
𝛽
constitute
an
orthono-
mal
basis
in
1 ⊗2,
in
particular,
dim(1 ⊗2) = dim 1 ⋅dim 2.
The
construction
of
tensor
product
extends naturally to any ﬁnite family of
Hilbert spaces. Tensor products of inﬁnite
families can also be constructed, however,
the procedure is more involved. In quan-
tum mechanics, tensor products serve to
describe composite systems the state space
of which is of the form ⨂
j j where j is
the state spaces of the jth component; the
latter can be either a real physical system
or a formal “subsystem” coming from a
separation of variables.
13.3
Bounded Operators on Hilbert Spaces
As we have mentioned, without loss of gen-
erality, we may regard bounded operators
as deﬁned on the whole Hilbert space. We
can specify various classes of them.
13.3.1
Hermitean Operators
Given an operator B ∈(), we deﬁne
its
adjoint
as
the
unique
operator
B∗
satisfying
(y, Bx) = (B∗y, x)
for
all
x, y ∈. The map B →B∗is an antilinear
isometry, ‖B∗‖ = ‖B‖. The adjoint satisﬁes

13.3 Bounded Operators on Hilbert Spaces
455
B∗∗= B and (BC)∗= C∗B∗, and moreover,
(B∗)−1 = (B−1)∗holds provided B−1 exists.
Another useful relation is the expression
for the subspace on which B∗vanishes,
Ker B∗= (Ran B)⟂.
An
operator
A ∈()
is
called
Hermitean
if
it
coincides
with
its
adjoint, A = A∗. The spectrum of A is
a subset of real axis situated between
mA = inf(x, Ax) and mA = sup(x, Ax); we
have ‖A‖ = max(|mA|, |MA|). The oper-
ator is called positive if mA ≥0. For any
B ∈(), the product B∗B is a Her-
mitean operator which, in addition satisﬁes
‖B∗B‖ = ‖B‖2. The last property means, in
particular, that () has also the structure
of a C∗-algebra [3].
To any positive A, there is a unique posi-
tive operator
√
A having the meaning of its
square root. In particular, one can associate
with any B ∈() the operator |B| ∶=
√
B∗B. While this oﬀers an analogy with the
modulus of a complex number, caution is
needed, for instance, none of the relations
|BC| = |B||C|, |B∗| = |B|, |B + C| ≤|B| +
|C| are valid in general.
An important class of positive opera-
tors are projections assigning to any vector
x ∈its orthogonal projection to a given
subspace ⊂. An operator E ∈() is
a projection iﬀE2 = E = E∗; its spectrum
contains only the eigenvalues 0 and 1 except
for the trivial cases, E = 0, I, when only one
of them is present.
Projections
E, F
are
orthogonal
if
Ran E ⟂Ran F, which is equivalent to
the
condition
EF = FE = 0.
The
sum
E + F is a projection iﬀE and F are
orthogonal.
The
product
of
the
pro-
jections is a projection iﬀEF = FE in
which
case
Ran EF = Ran E ∩Ran F.
Furthermore, the diﬀerence E −F is a
projection iﬀE ≥F, that is, E −F
is
positive,
which
is
further
equivalent
to Ran E ⊃Ran F,
or to
the
relations
EF = FE = F.
13.3.2
Unitary Operators
An operator U ∈() is called an isom-
etry if its domain D(U) and range Ran U
are closed subspaces and the norm is pre-
served, ‖Ux‖ = ‖x‖. If D(U) = the oper-
ator is called unitary, the same name is
used for linear isometries between diﬀerent
Hilbert spaces. If D(U) ≠we speak about
a partial isometry. A unitary operator pre-
serves also the inner product, (Ux, Uy) =
(x, y), and satisﬁes the relation U−1 = U∗;
its spectrum is a subset of the unit circle,
𝜎(U) ⊂{z ∈ℂ∶|z| = 1}.
If V is a partial isometry, the products
V ∗V and VV ∗are the projections to its ini-
tial and ﬁnal subspace, D(V) and the range
Ran V, respectively. Using partial isome-
tries, one can ﬁnd a polar decomposition
of a bounded operator: to any B ∈()
there is a unique partial isometry WB
such that B = WB|B| and the relations
Ker WB = Ker B and Ran WB = Ran B hold.
As another word of caution, the “opposite”
decomposition, B = |B|W, may not exist if
B is not Hermitean.
Both Hermitean and unitary operators
are
normal,
which
means
they
com-
mute with their adjoints, BB∗= B∗B, and
similarly the real and imaginary part
of such an operator, Re B ∶= 1
2(B + B∗)
and
Im B ∶= (1∕2i)(B −B∗),
commute.
Normal operators have empty residual
spectrum and their eigenspaces corre-
sponding
to
diﬀerent
eigenvalues
are
orthogonal.
An
operator
B ∈()
is
said to have a pure point spectrum if its
eigenvectors form an orthonormal basis
in . Every such operator is normal and
𝜎(B) = 𝜎p(B).

456
13 Functional Analysis
13.3.3
Compact Operators
An operator C ∈() is called compact if
it maps any bounded subset of into a
precompact one, that is, a set the closure
of which is compact. Equivalently, a com-
pact operator maps any weakly convergent
sequence {xn} ⊂into a sequence conver-
gent with respect to the norm of . The
set () of compact operators is a sub-
space in (), which is closed in the opera-
tor norm and, in addition, it has the ∗-ideal
property, namely, that for any C ∈()
and B ∈() the operators C∗, BC, CB
are also compact. Compactness has several
implications for the operator spectrum:
Theorem 13.4 (Riesz–Schauder theorem)
For a C ∈() any nonzero point of
the spectrum is an eigenvalue of ﬁnite
multiplicity and the only possible accu-
mulation point of the spectrum is zero.
The point spectrum is at most countable
and the eigenvalue moduli can be ordered,
namely,
|𝜆1| ≥· · · ≥|𝜆j| ≥|𝜆j+1| ≥· · ·,
with limj→∞𝜆j = 0 if dim = ∞.
Theorem 13.5 (Fredholm alternative)
Given
an
equation
x −𝜆Cx = y
with
C ∈(), 𝜆∈ℂ, and y ∈, one and
only of the following situations can occur:
(i) the equation has a unique solution xy for
any y ∈, in particular, x0 = 0, or (ii) the
equation without the right-hand side has a
nontrivial solution.
Theorem 13.6 (Hilbert–Schmidt
theorem) A normal compact operator
has a pure point spectrum.
A ﬁnite-dimensional operator, that is, an
operator C such that dim Ran C < ∞, is
compact, and a norm-convergent sequence
of such operators has a compact limit. In
fact, there is a canonical form, which makes
it possible to express every compact oper-
ator as such a limit. The operator |C| cor-
responding to C ∈() is compact and
its eigenvalues, |C|ej = 𝜇jej, form a nonin-
creasing sequence in [0, ∞); we call them
singular values of C. Using Fourier expan-
sion of |C|x in combination with the polar
decomposition, C = W|C|, and introduc-
ing the vectors fj ∶= Wej, we arrive at the
formula
C =
JC
∑
j=1
𝜇j(ej, ⋅)fj,
(13.13)
where JC ∶= dim Ran |C| and the series
converges in the operator norm if JC = ∞.
13.3.4
Schatten Classes
The set () has some distinguished sub-
sets. As mentioned above, we can asso-
ciate with a C ∈() its singular values
{𝜇j}. For a ﬁxed p ≥1, we denote by p, or
p(), the set of compact operators C such
that ‖C‖p ∶= ( ∑
j 𝜇p
j
)1∕p < ∞. Each p is a
Banach space, which is a closure of the set of
ﬁnite-dimensional operators with respect
to the norm ‖ ⋅‖p. The inclusion p ⊂p′
holds for p < p′ and the set () is alter-
natively denoted as ∞().
Two of these so-called Schatten classes
are of particular importance. 2() is a
Hilbert space with respect to the scalar
product deﬁned by (B, C)2 = ∑
j(Bej, Cej)
for any ﬁxed orthonormal basis {ej} ⊂,
its elements are called Hilbert–Schmidt
operators. They form a ∗-ideal in (): for
any C ∈2() and B ∈(), the opera-
tors C∗, BC, CB are also Hilbert–Schmidt.
We have a useful criterion for integral
operators,
B ∈(L2(M, d𝜇))
acting
as
(Bf )(x) = ∫M gB(x, y) f (y) d𝜇(y):
such
an
operator belongs to the Hilbert–Schmidt

13.4 Unbounded Operators
457
class iﬀits kernel is square integrable,
‖B‖2
2 = ∫M×M
|gB(x, y)|2 d(𝜇⊗𝜇)(x, y) < ∞,
(13.14)
with respect to the product measure 𝜇⊗𝜇
[2, App. A].
Another important class, again an ∗-
ideal in (), is 1() the elements of
which are called trace-class operators. An
operator B ∈() belongs to 1 iﬀit is a
product of two Hilbert–Schmidt operators.
For any C ∈1(), one can deﬁne its trace
by Tr C ∶= ∑
j(ej, Bej) where {ej} is any
ﬁxed orthonormal basis in ; the 1-norm
is given by ‖C‖1 = Tr |C|. The trace is a
functional of the unit norm satisfying
Tr C∗=Tr C ,
Tr (BC) = Tr (CB)
for
C ∈1() , B ∈() .
(13.15)
Important trace-class operators are den-
sity matrices used in quantum physics to
describe mixed states. They are positive
W ∈1()
satisfying
the
normaliza-
tion
condition
Tr W = 1.
Note
that
one-dimensional
projections
describ-
ing pure states are a particular case of
density matrices; a state is not pure iﬀ
Tr W 2 < 1. The subset of pure states can be
also characterized geometrically: the set of
all density matrices on a given is convex
and one-dimensional projections are its
extremal points.
13.4
Unbounded Operators
Unbounded operators are considerably
more diﬃcult to deal with because one
has to pay attention to their deﬁnition
domains; however, they appear in numer-
ous applications.
13.4.1
Operator Adjoint and Closure
An operator T on is said to be densely
deﬁned if D(T) = ; in contrast to the pre-
vious section, there is no standard way to
extend such an operator to the whole .
The set of densely deﬁned operators on 
will be denoted as (). To a given T ∈
() and y ∈, there is at most one vec-
tor y∗∈such that the relation (y, Tx) =
(y∗, x) holds. We denote by D(T∗) the sub-
space of y for which such a y∗exists and
introduce the adjoint T∗of T acting as
T∗y = y∗.
As
in
the
bounded
case,
we
have
Ker T∗= (Ran T)⟂, so Ker T∗is a closed
subspace. If T−1 ∈(), then T∗is also
invertible and (T∗)−1 = (T−1)∗. On the
other hand, if S is an extension of T, S ⊃T,
then S∗⊂T∗. Other relations valid for
bounded operators hold generally in a
weaker form only, for instance, T∗∗⊃T
provided T∗∈(); similarly, the rela-
tions (S + T)∗⊃S∗+ T∗and (TS)∗⊃S∗T∗
are valid; in both cases, they turn to
identities if T ∈().
This allows to introduce two important
notions. An operator A ∈() is symmet-
ric if A ⊂A∗, in other words, if (y, Ax) =
(Ay, x) holds for all x, y ∈D(A). If, in addi-
tion, A = A∗, the operator is called self-
adjoint. If A is bounded, the two notions
coincide and we speak about a Hermitean
operator; in the unbounded case, it is better
avoid this term because it may cause misun-
derstanding.
As an example, let us mention the mul-
tiplication operator Q on = L2() acting
as (Q𝜓)(x) = ∫x𝜓(x) dx, which one asso-
ciates in quantum mechanics with the par-
ticle position. It is bounded iﬀthe interval
is bounded; if it is not the case, we put
D(Q) = {𝜓∈∶∫x2|𝜓(x)|2 dx < ∞}. It
is easy to check that the operator Q deﬁned

458
13 Functional Analysis
in this way is not only symmetric but also
self-adjoint.
The
fact
that
the
domain
of
an
unbounded symmetric operator is not
the whole seen in this example is
valid generally, because the closed-graph
theorem has the following consequence:
Theorem 13.7 (Hellinger–Toeplitz
theorem) A symmetric operator A with
D(A) = is bounded.
The notion of adjoint operator allows us
to formulate new properties of closed oper-
ators, in addition to the general ones men-
tioned above. The adjoint to any T ∈()
is a closed operator. The closure T of an
operator T is its smallest closed extension,
or equivalently, the operator the graph of
which is the closure of Γ(T) in ⊕. If
the adjoint T∗is densely deﬁned, the clo-
sure of T exists and the relations T∗∗= T
and (T)∗= T are valid. The set of closed
densely deﬁned operators on a given will
be denoted as c().
In particular, any self-adjoint operator is
closed. As the closure represents a unique
way of extending an operator, it is useful
to deﬁne an essentially self-adjoint (e.s.a.)
operator A as such that its closure is self-
adjoint, A = A∗. This concept is useful in
applications because it is often easier to
prove that an operator A is e.s.a., in which
case we know it has a unique self-adjoint
extension even if we may not know the
domain D(A) of the latter explicitly. More
generally, a subspace D in the domain of an
operator T ∈c() is called a core if the
restriction T ↾D of T to D satisﬁes T ↾D =
T. It means that D is a core of a self-adjoint
A iﬀA ↾D is e.s.a.; one usually says that A
is e.s.a. on D.
For any T ∈c(), the product T∗T is
self-adjoint and positive, that is, (x, T∗Tx) =
‖Tx‖2 ≥0 holds for any x ∈D(T∗T), and
D(T∗T) is a core for T; similarly, TT∗is self-
adjoint and positive, and D(TT∗) is a core
for T∗.
For a closed Hilbert-space operator T,
we introduce its essential spectrum, 𝜎ess(T),
as the set of all 𝜆∈ℂto which one can ﬁnd
a sequence {xn} ⊂D(T) of unit vectors
having no convergent subsequence and
satisfying (T −𝜆)xn →0 as n →∞. The
spectrum of the operator T is then the
union of 𝜎ess(T) with 𝜎p(T) ∪𝜎r(T) and
one has 𝜎c(T) = 𝜎ess(T) ⧵(𝜎p(T) ∪𝜎r(T)).
However,
in
distinction
to
the
spec-
tral
decomposition
mentioned
in
Section 13.1.3, the present one is not
disjoint, for instance, an eigenvalue of inﬁ-
nite multiplicity belongs simultaneously to
𝜎p(T) and 𝜎ess(T).
13.4.2
Normal and Self-Adjoint Operators
As in the bounded case, an operator T ∈
c() is said to be normal if T∗T = TT∗.
The set of all such operators on a given is
denoted as n(). An operator T is normal
iﬀD(T) = D(T∗) and ‖Tx‖ = ‖T∗x‖ holds
for all x ∈D(T).
A typical example is that of oper-
ators
of
multiplication
by
a
function
generalizing the operator Q described
above.
Given
a
function
f ∶M →ℂ,
measurable
with
respect
to
a
mea-
sure 𝜇on M, we deﬁne the operator
Tf
acting as (Tf 𝜓)(x) = f (x)𝜓(x) with
the
domain
D(Tf ) = {𝜓∈L2(M, d𝜇) ∶
∫M |f (x)𝜓(x)|2 d𝜇(x) < ∞}. Such an oper-
ator is densely deﬁned and normal. It
is self-adjoint iﬀf is real-valued almost
everywhere (a.e.) in M and bounded iﬀf is
essentially bounded, ‖Tf ‖ = ‖f ‖∞.
As in the bounded case, the residual
spectrum of a normal operator is void
and its resolvent set coincides with its
regularity domain, that is, 𝜆∉𝜎(T) holds

13.4 Unbounded Operators
459
iﬀthere is a constant c = c(𝜆) > 0 such
that
‖(T −𝜆)x‖ ≥c‖x‖,
which
is
fur-
ther equivalent to Ran (T −𝜆) = . This
implies, in particular, that the spectrum
of a self-adjoint operator A is a subset
of the real axis satisfying the relation
inf 𝜎(A)=inf{(x, Ax) ∶x ∈D(A), ‖x‖ =
1}. In the above example, the spectrum
𝜎(Tf ) coincides with the essential range of
the function f , which is the set of all points
such that the f -preimage of each of their
neighborhoods has a nonzero 𝜇measure.
Self-adjointness
is
of
fundamental
importance for quantum physics because
such operators are used to describe observ-
ables of quantum systems.
Theorem 13.8 (Basic
self-adjointness
criterion)
For
a
symmetric
operator
A,
the
following
claims
are
equiv-
alent:
(i)
A
is
self-adjoint,
(ii)
A
is
closed and Ker (A∗± i) = {0}, and (iii)
Ran (A ± i) = . In a similar way, essen-
tial self-adjointness of A is equivalent to
Ker (A∗± i) = {0}, or to Ran (A ± i) = .
In view of the above-mentioned role
that self-adjointness plays in quantum
mechanics, it is important to have other
suﬃcient conditions. An often used one is
based on a perturbation argument. Given
linear operators A, S on , we say that S
is A-bounded if D(S) ⊃D(A) and there are
a, b ≥0 such that
‖Sx‖ ≤a‖Ax‖ + b‖x‖
(13.16)
holds for any x ∈D(A). The inﬁmum of
the a’s for which the inequality (13.16)
is satisﬁed with some b ≥0 is called the
A-bound of S. The A-boundedness can be
used to prove self-adjointness of operators
if a is small enough.
Theorem 13.9 (Kato–Rellich theorem)
Let A be self-adjoint and S symmetric and
A-bounded with the A-bound less than
one, then the sum A + S is self-adjoint.
Moreover, if D ⊂D(A) is a core for A, then
A + S is e.s.a. on D.
Another useful criterion relies on ana-
lytical vectors. A vector x that belongs to
D(Aj) for all j ≥1, is called analytic (with
respect to the operator A) if the power
series ∑
j ‖Ajx‖zj∕j! has a nonzero conver-
gence radius.
Theorem 13.10 (Nelson theorem) A
symmetric operator is e.s.a. if it has a total
set of analytic vectors.
Sometimes, one can analyze an opera-
tor by mapping it to another one the struc-
ture of which is more simple. Operators
T on and S on are unitarily equiva-
lent if there is a unitary U ∶→such
that T = USU−1. Unitary equivalence pre-
serves numerous operator properties. If S is
densely deﬁned, the same is true for T and
T∗= US∗U−1; in particular, if S is symmet-
ric or self-adjoint, then T is again symmet-
ric or self-adjoint, respectively. In a simi-
lar way, U preserves operator invertibility,
closedness, and other properties.
As an example, consider the operator
P on L2(ℝ) acting by P𝜓= −i𝜓′ with the
domain consisting of functions 𝜓∶ℝ→ℂ
with the derivatives 𝜓′ ∈L1 ∩L2. In quan-
tum mechanics, this operator describes
momentum of a one-dimensional particle.
It is related to the operator Q on L2(ℝ)
introduced above by the relation
P = F−1QF,
(13.17)
where F is the Fourier–Plancherel opera-
tor (13.3). Both operators are self-adjoint
and also e.s.a. on various subsets of
their
domain,
for
instance,
on
(ℝ).
Moreover, because the unitary equivalence
preserves the spectrum, we have also
𝜎(P) = 𝜎(Q) = ℝ.

460
13 Functional Analysis
13.4.3
Tensor Products of Operators
Next we recall how to construct oper-
ators
on
tensor
products
of
Hilbert
spaces.
We
will
consider
a
product
1 ⊗2;
an
extension
to
any
ﬁnite
number of Hilbert spaces is straight-
forward.
Given
operators
Bj ∈(j)
we deﬁne the map B1 ⊗B2 on 1 × 2
by (B1 ⊗B2)(x1 ⊗x2) = B1x1 ⊗B2x2 and
extend it ﬁrst linearly, then continuously. As
both the Bj are bounded, the resulting oper-
ator is deﬁned on the whole of 1 ⊗2,
satisfying ‖B1 ⊗B2‖ = ‖B1‖‖B2‖.
Tensor products of bounded opera-
tors satisfy the usual rules known from
matrix
algebra,
B1C1 ⊗B2C2 = (B1 ⊗
B2)(C1 ⊗C2)
and
(B1 ⊗B2)∗= B∗
1 ⊗B∗
2,
and furthermore, (B1 ⊗B2)−1 = B−1
1 ⊗B−1
2
provided that both the Bj are invertible. If
the component operators Bj are normal
(unitary, Hermitean, projections) the same
is respectively true for their tensor product.
If we have operators Tj on j, in gen-
eral unbounded, attention has to be paid to
their domains. It is natural to deﬁne T1 ⊗
T2 on its “minimal” domain, that is, the
linear hull of D(T1) × D(T2). If the Tj’s are
densely deﬁned, the same is true for their
tensor product and (T1 ⊗T2)∗⊃T∗
1 ⊗T∗
2 ;
if the Tj’s are closable, so is T1 ⊗T2 and
T1 ⊗T2 ⊃T1 ⊗T2. One has (T1 + S1) ⊗
T2 = T1 ⊗T2 + S1 ⊗T2; however, for the
product, in general, the inclusion (T1S1) ⊗
(T2S2) ⊂(T1 ⊗T2)(S1 ⊗S2) holds only.
We have mentioned the use of Hilbert-
space tensor products to describe com-
posite quantum systems. If a self-adjoint
operator A describes an observable of a
subsystem with state space 1 and the
complement state space 2, then the same
observable related to the composite sys-
tem is described by the operator A ⊗I
on 1 ⊗2, where I is the unit opera-
tor. For example, the ﬁrst momentum com-
ponent of a three-dimensional particle is
described by the operator P1 = P ⊗I ⊗I
on L2(ℝ3). In view of (13.3), we can express
it alternatively as P1 = F−1
3 Q1F3, where Q1
is related in the same way to the operator Q
on L2(ℝ) and F3 = F ⊗F ⊗F is the three-
dimensional Fourier–Plancherel operator.
In addition to tensor products them-
selves, some operators constructed from
them are of importance. Given a pair of
self-adjoint operators Aj on j, we consider
operators of the polynomial form,
P[A1, A2] =
n1
∑
k=0
n2
∑
l=0
akl(Ak
1 ⊗Al
2)
(13.18)
with real-valued coeﬃcients akl. Without
loss of generality, we may suppose that the
senior coeﬃcient an1n2 is nonzero and take
the linear hull of D(An1
1 ) × D(An2
2 ) as the
domain of P[A1, A2]; one can check that
such an operator is e.s.a.
A frequently occurring example is that of
self-adjoint operators of the form A1 + A2,
where A1 ∶= A1 ⊗I2 and A2 ∶= I1 ⊗A2,
which describe sums of observables related
to the respective subsystems. In the above
example of a three-dimensional quantum
particle, the operator H0 = P2
1 + P2
2 + P2
3 on
L2(ℝ3) describes the kinetic energy (up to
a multiplicative factor); we note that it is
e.s.a. on (ℝ3) and acts on its elements as
the negative Laplacian, H0𝜓= −Δ𝜓.
13.4.4
Self-Adjoint Extensions
If A′ is a symmetric extension of an
operator A ∈(), we have A ⊂A′ ⊂
(A′)∗⊂A∗.
One
naturally
asks
under
which conditions one can close the gap
by choosing a self-adjoint A′. To show

13.4 Unbounded Operators
461
that there are operators for which self-
adjoint extensions may not exist, consider
again 𝜓→−i𝜓′, this time on the halﬂine
ℝ+ = (0, ∞). We denote by ̃P such an oper-
ator deﬁned on all 𝜓∈(L2 ∩L1)(ℝ+) and
by P0 its restriction to functions satisfying
𝜓(0+) = 0. Using integration by parts, it is
easy to see that P∗
0 = ̃P, and furthermore,
that P0 is symmetric, while ̃P is not; there
is obviously no self-adjoint P such that
P0 ⊂P ⊂̃P would hold.
To solve the problem in its general-
ity, we introduce deﬁciency subspaces of
an operator T to be Ker(T∗−𝜆), where
𝜆is a ﬁxed number from its regular-
ity
domain
as
Ker(T∗−𝜆).
The
map
𝜆→dim Ker(T∗−𝜆) is constant on any
arcwise-connected component of the regu-
larity domain. In particular, for a symmetric
operator A any nonreal number belongs to
its regularity domain which thus the latter
has at most two connected components;
this allows us to deﬁne its deﬁciency indices
n±(A) = dim Ker(A∗∓i).
The deﬁciency indices in turn determine
whether self-adjoint extensions of a sym-
metric A exist. By the basic self-adjointness
criterion, we know that A is e.s.a. iﬀ
n±(A) = 0. If it is not the case, all symmet-
ric extensions of A can be parameterized,
according to the theory constructed by
John von Neumann, by isometric maps
from Ker(A∗−i) to Ker(A∗+ i). Conse-
quently, nontrivial self-adjoint extensions
exist iﬀthe deﬁciency indices are nonzero
and equal to each other, n+(A) = n−(A). If
both of them are ﬁnite, any maximal sym-
metric extension is symmetric, otherwise
there may exist maximal extensions that
are not self-adjoint.
Returning to the example, it is now easy
to see why there is no self-adjoint momen-
tum operator on L2(ℝ+); it follows from
the fact that the deﬁciency indices of P0
are (1, 0). On the other hand, the operator
𝜓→−i𝜓′ on L2(), where = (a, b) is a
ﬁnite interval, deﬁned on functions 𝜓∈
L2 ∩L1 satisfying 𝜓(a+) = 𝜓(b−) = 0 has
deﬁciency indices (1, 1) and thus a family
of self-adjoint extensions. We can denote
them by P𝜃as each of them can be charac-
terized by the boundary condition 𝜓(b−) =
ei𝜃𝜓(a+) for some 𝜃∈[0, 2𝜋).
Characterizing
self-adjoint
extensions
by means of boundary conditions is com-
mon for diﬀerential operators. As another
example, let us mention again a one-
dimensional particle on a halﬂine. While
its momentum operator does not exist,
the operator of kinetic energy does, act-
ing as 𝜓→−𝜓′′ modulo a multiplicative
constant. If we choose for its domain all
𝜓∈L2(ℝ+) such that 𝜓(0+) = 𝜓′(0+) = 0,
we obtain a symmetric operator with
deﬁciency indices (1, 1). Its self-adjoint
extensions T𝜆are characterized by the
condition 𝜓′(0+) −𝜆𝜓(0+) = 0 for 𝜆∈ℝ,
or by the Dirichlet condition 𝜓(0+) = 0,
which formally corresponds to 𝜆= ∞.
This example is a particular case of
one-dimensional Schrödinger operator on
L2(), where the interval can be ﬁnite,
semiﬁnite, or = ℝ; such an operator acts
as 𝜓→−𝜓′′ + V𝜓, where the potential
V is a locally integrable function. Asking
whether this formal diﬀerential operator
can be made self-adjoint, one has to inspect
solutions to the equation
−𝜓′′(x) + V(x)𝜓(x) = 𝜆𝜓(x) ,
𝜆∈ℂ.
(13.19)
Theorem 13.11 (Weyl alternative) At
each end point of , just one of the following
possibilities is valid: (i) limit-circle case: for
any 𝜆∈ℂall solutions are L2 in the vicinity
of the end point, or (ii) limit-point case:
for any 𝜆∈ℂthere is at least one solution
which is not L2 in the vicinity of the end
point.

462
13 Functional Analysis
If one or both end points of are of the
limit-circle type, one has to impose bound-
ary conditions there to make the operator
self-adjoint.
A useful tool to ﬁnd relations between
self-adjoint extensions of a given symmetric
operator is the Krein formula: if the max-
imal common part A of self-adjoint oper-
ators A1, A2 has deﬁciency indices (n, n),
then the relation
(A1 −z)−1 −(A2 −z)−1
=
n
∑
j,k=1
𝜆jk(z)(yk(z), ⋅)yj(z)
(13.20)
holds for any z ∈𝜌(A1) ∩𝜌(A2), where
the matrix (𝜆jk) is nonsingular and yj(z),
j = 1, … , n, are linearly independent vec-
tors from Ker(A∗−z); the functions 𝜆jk(⋅)
and yj(⋅) can be chosen to be analytic
in
𝜌(A1) ∩𝜌(A2).
Kreins’s
formula
has
numerous applications, in particular, when
constructing and analyzing solvable models
of quantum systems [7].
13.5
Spectral Theory of Self-Adjoint Operators
It is well known from linear algebra that
any symmetric matrix possesses a unique
ﬁnite family of eigenvalues and that the cor-
responding eigenvectors can be chosen to
form an orthonormal basis in the appropri-
ate vector space. Now we are going to show
how these properties extend to self-adjoint
operators on an arbitrary Hilbert space.
13.5.1
Functional Calculus
Our goal here is to deﬁne spectral decom-
position of an operator. To that end, we ﬁrst
have to introduce integration with respect
to a particular type of operator measures.
They are deﬁned on a set X equipped with
a family (a 𝜎-ﬁeld, to be exact) of measur-
able subsets; typically, we have in mind ℝ,
the real line, and the family of all its Borel
subsets.
Given a Hilbert space , we introduce
a projection-valued (or spectral) measure,
a map that associates with any measurable
set M a projection E(M) ∈() such that
E(X) = I and the map is 𝜎-additive, that
is, for any at most countable disjoint fam-
ily {Mn} of measurable sets, the relation
E( ⋃
n Mn
) = ∑
n E(Mn) is valid; as a conse-
quence of additivity, we have E(∅) = 0.
If X = ℝd, one can construct spec-
tral measures starting from projections
assigned to rectangular sets – in particular,
intervals if d = 1 – in a way fully similar
to that used in the conventional measure
theory. With a spectral measure E on the
real line, one can associate also a spec-
tral decomposition as a right-continuous
map 𝜆→E𝜆, which is nondecreasing and
satisﬁes the relations
s lim
𝜆→−∞E𝜆= 0 ,
s lim
𝜆→+∞E𝜆= I,
(13.21)
where the strong convergence means that
E𝜆x converges to 0 or I, respectively, for
any x ∈; the relation between the two
notions is given by E𝜆= E((−∞, 𝜆]).
On
a
ﬁnite-dimensional
,
any
spectral
measure
is
supported
by
a
ﬁnite set of points, {𝜆j} ⊂ℝ, so that
E(M) = E(M ∩{𝜆j}) holds for any mea-
surable M; the corresponding spectral
decomposition is the steplike function
E𝜆= E({𝜆j ∶𝜆j ≤𝜆}).
Another
simple
example is the spectral measure EQ of
the multiplication operator Q on L2(ℝ)
introduced in Section 13.4.1. It acts as mul-
tiplication by the characteristic function,
EQ(M)𝜓= 𝜒M𝜓, for any M ∈.
Having deﬁned the spectral measure,
we are able to introduce integration with

13.5 Spectral Theory of Self-Adjoint Operators
463
respect to it as the map which assigns to a
measurable function f on X the operator
(f ) = ∫X
f (𝜆) dE(𝜆)
(13.22)
on the corresponding Hilbert space . To
deﬁne the action of (f ), we use a construc-
tion analogous to the one employed in the
usual calculus. First, we deﬁne the integral
on simple functions having a ﬁnite num-
ber of values, f = ∑
j 𝜆j𝜒Mj for a ﬁnite fam-
ily {Mj} of measurable sets; we set (f ) ∶=
∑
j 𝜆jE(Mj). In the next step, we use the fact
that any bounded measurable function f
can be approximated by simple functions
{fn} in the sense that ‖f −fn‖∞→0 as n →
∞, which allows us in the next step to deﬁne
the operator (f ) ∶= limn→∞(fn) for such
functions, where the convergence is under-
stood in the sense of operator norm.
The
map
f →(f )
introduced
in
this
way
is
linear
and
multiplicative,
(fg) = (f )(g) = (g)(f ). Denoting by
L∞(X, dE) the family of functions bounded
a.e. with respect to the measure E, we have
‖(f )‖ = ‖f ‖∞and (f )∗= (f ) for any
such function. Moreover, if a sequence
{fn} ⊂L∞(X, dE) converges pointwise to a
function f and the set {‖fn‖∞} is bounded,
one has f ∈L∞(X, dE) and the limit can be
interchanged with the integral.
Integrating
unbounded
functions
is
more involved. First, we notice that for any
spectral measure E and x ∈the relation
𝜇x(⋅) = (x, E(⋅)x) deﬁnes a numerical mea-
sure on X. Using it, we can associate with a
measurable function f the set
Df ∶=
{
x ∈∶∫X
|f (𝜆)|2 d𝜇x(𝜆) < ∞
}
,
(13.23)
which is dense in . Furthermore, to
the
function
f
one
can
construct
a
sequence
{fn} ⊂L∞(X, dE)
such
that
fn(𝜆) →f (𝜆) and |fn(𝜆)| ≤|f (𝜆)| holds E-
a.e. in X. We use it to deﬁne the operator
(f ) = ∫X f (𝜆) dE(𝜆) by
(f )x = lim
n→∞(fn)x ,
x ∈Df ;
(13.24)
the chosen domain is natural because
‖(f )x‖2 = ∫X |f (𝜆)|2 d𝜇x(𝜆).
The map f →(f ) is homogeneous and
(f + g) ⊃(f ) + (g); similarly, for the
multiplication of functions, we have in
general (fg) ⊃(f )(g). It also holds
that (f ) = (g) implies that f (𝜆) = g(𝜆)
holds almost everywhere with respect
to the measure E. Furthermore, we have
(f )∗= (f ) as in the bounded case
and the relation (f )−1 = (f −1) holds
provided F is nonzero E-a.e.
The operator (f ) deﬁned by (13.24) is
normal for any measurable f , it is self-
adjoint if f (t) is real with a possible excep-
tion of an E-zero measure set. The function
f also determines the spectrum 𝜎((f ))
that coincides with the essential range of f
with respect to the measure E, as deﬁned
in Section 13.4.2. In particular, 𝜆∈ℂis an
eigenvalue of (f ) iﬀE(f (−1)({𝜆})) ≠0 and,
in this case, Ran E(f (−1)({𝜆})) is the corre-
sponding eigenspace.
13.5.2
Spectral Theorem
Armed with the notions introduced above,
we are ready to state the fundamental struc-
tural result about self-adjoint operators.
Theorem 13.12 (The spectral theorem)
To any self-adjoint operator A on a Hilbert
space , there is a unique spectral measure
EA on such that
A = ∫ℝ
𝜆dEA(𝜆) .
(13.25)

464
13 Functional Analysis
Furthermore, a bounded operator B com-
mutes with A, BA ⊂AB, iﬀit commutes
with its spectral decomposition, that is, with
E(A)
𝜆
= EA((−∞, 𝜆]) for any 𝜆∈ℝ.
The second claim allows us to specify the
meaning of commutativity, which is, in gen-
eral, not easy to introduce if both the oper-
ators involved are unbounded. If A1, A2 are
self-adjoint we say that they commute if any
element of the spectral decompositions of
A1 commutes with any element of the spec-
tral decompositions of A2.
The spectral theorem has various corol-
laries and modiﬁcations. For instance, a
bounded normal operator, which we can
write as B = A1 + iA2 with commuting Her-
mitean A1, A2, can be expressed as B =
∫ℂz dF(z), where F is the projection-valued
measure on ℂobtained as the product mea-
sure of EA1 and EA2. Furthermore, with a
unitary operator U, one can associate a
unique spectral measure EU with the sup-
port in the interval [0, 2𝜋) ⊂ℝsuch that
U = ∫ℝ
ei𝜆dEU(𝜆) .
(13.26)
These claims are used in one of the pos-
sible ways to prove the spectral theorem, in
which one checks its validity subsequently
for Hermitean, bounded normal, and uni-
tary operators, passing ﬁnally to the gen-
eral case through the appropriate “change
of variables” in the integral.
As an example of the decomposition
(13.26),
let
us
mention
the
Fourier–
Plancherel operator (13.3). It acts on the
elements of the orthonormal basis (13.7) as
Fhn = (−i)nhn, n = 0, 1, … , hence its spec-
trum is pure point, 𝜎(F) = {1, −i, −1, i}.
The spectral measure can be in this case
expressed explicitly,
EF(M) = 1
4
3
∑
j,k=0
𝜒M
(𝜋k
2
)
(−i)jkFj .
(13.27)
The spectral theorem provides a tool to
describe and classify spectra of self-adjoint
operators. To begin with, a real 𝜆belongs to
𝜎(A) iﬀEA(𝜆−𝜖, 𝜆+ 𝜖) ≠0 for any 𝜖> 0;
it is an eigenvalue iﬀthe point 𝜆itself has
a nonzero EA measure. In particular, any
isolated point of the spectrum is an eigen-
value. The results of the previous section
imply that the spectrum of a self-adjoint A
is always nonempty and that such an oper-
ator is bounded iﬀits spectrum is bounded.
The essential spectrum of a closed oper-
ator has been deﬁned in Section 13.4.1.
If
A
is
self-adjoint,
𝜆∈𝜎ess(A)
iﬀ
dim Ran EA(𝜆−𝜖, 𝜆+ 𝜖) = ∞
holds
for
any 𝜖> 0 and the essential spectrum is a
closed set. Points of 𝜎ess(A) fall into three,
mutually nonexclusive categories: such a
𝜆can belong to 𝜎c(A), be an eigenvalue of
inﬁnite multiplicity, or an accumulation
point of eigenvalues. The complement of
𝜎ess(A) consists of isolated eigenvalues
of ﬁnite multiplicity; it is a subset of the
point spectrum for which we use the term
discrete spectrum.
We have seen in Section 13.3.3 that the
spectrum of compact operators away from
zero consists of isolated eigenvalues of
ﬁnite multiplicity. This is the reason why
the essential spectrum of a self-adjoint A is
stable with respect to compact perturba-
tions; this fact is usually referred to as the
Weyl theorem. It can be substantially gen-
eralized. Given a self-adjoint A, an operator
T is called A-compact if D(T) ⊃D(A) and
T(A −i)−1 is compact; this property again
guarantees spectral stability.
Theorem 13.13 (Generalized Weyl theo-
rem) 𝜎ess(A + T) = 𝜎ess(A) holds if the
operator T is symmetric and A-compact.
Using the spectral theorem, we can
introduce also another classiﬁcation of the
spectrum. We call ac(A) the subspace of

13.5 Spectral Theory of Self-Adjoint Operators
465
all x ∈such that 𝜇x(N) = 0 holds any
Borel set N of zero Lebesgue measure, in
other words, such that the measure 𝜇x is
absolutely continuous with respect to the
Lebesgue measure. The orthogonal com-
plement of ac(A) is denoted by s(A). The
projections to these subspace commute
with the operator A, so we can write it
as A = Aac ⊕As, its spectrum being the
union of the component spectra, which
we call 𝜎ac(A), the absolutely continuous
spectrum of A, and 𝜎s(A), the singular one,
respectively.
Furthermore, the comple-
ment 𝜎sc(A) ∶= 𝜎s(A) ⧵𝜎p(A) is called the
singularly continuous spectrum of A; any
of these components may be nonempty if
dim = ∞.
Before closing this section, let us recall
the role that spectral analysis plays in quan-
tum physics. As we have said, one asso-
ciates a self-adjoint operator A with any
observable of a quantum system. In con-
trast to classical physics, measurement has
a probabilistic character here: its possi-
ble outcomes coincide with the spectrum
of A and the probability of ﬁnding the
measured value in a set M ⊂ℝis given
by w(M, A; W) = Tr(EA(M)W) if the state
of the system before the measurement is
described by a density matrix W, in partic-
ular, by
w(M, A; 𝜓) = ∫M
d(𝜓, EA(𝜆)) = ‖EA(M)𝜓‖2
(13.28)
if the state is pure being described by a unit
vector 𝜓∈. The postulate makes sense;
it is easy to check that Tr (EA(⋅)W) is a
probability measure on ℝ.
Measurement
of
this
type
can
be
regarded
as
the
simplest,
dichotomic
observables with two possible outcomes,
positive (the observed value is found in
M) and negative (it is found outside M);
one often uses the term yes–no experiment
for them. Using this notion, we can also
describe what happens with the system
after the measurement. If the outcome is
positive, the resulting state is described
by
the
vector
EA(M)𝜓∕‖EA(M)𝜓‖,
or
more generally by the density matrix
EA(M)WEA(M)∕Tr(EA(M)W);
in
the
negative case, we replace M by ℝ⧵M.
13.5.3
More about Spectral Properties
Functional calculus allows us to deﬁne
functions of a self-adjoint operator A nat-
urally using its spectral measure by the
relation
f (A) ∶= (f ) = ∫ℝ
f (𝜆) dEA(𝜆) .
(13.29)
The term “function” has to be taken with
a grain of salt here because it is f that is
the “variable” in the above formula. A sim-
ple example is f (Q) on L2(ℝ), which acts as
(f (Q)𝜓) = f (x)𝜓(x) on L2 functions satis-
fying ∫ℝ|f (x)𝜓(x)|2 dx < ∞. The deﬁnition
(13.29) is consistent in the sense that for ele-
mentary functions such as polynomials, it
gives the same result as one would obtain
without using the spectral theory.
A slightly more involved example con-
cerns functions of the momentum operator
P on L2(ℝ). One can use the unitary equiv-
alence (13.17) to express their action. For
instance, for the exponential function, one
obtains the unitary operators
(
eiaP𝜓)
(x) = 𝜓(x + a) ,
a ∈ℝ,
(13.30)
acting as the translation group in L2(ℝ), and
for f ∈L2(ℝ) we get the expression
(
f (P)𝜓)
(x) =
1
√
2𝜋∫ℝ
(Ff )(y −x)𝜓(y) dy
(13.31)

466
13 Functional Analysis
with the integral kernel given by the
Fourier–Plancherel image of f .
The
limiting
property
of
sequences
{(fn)} mentioned above can be used
to derive an expression of the spectral
measure in terms of the resolvent RA(z) =
(A −z)−1. Speciﬁcally, because the func-
tion
arctan (b −𝜆∕𝜖) −arctan (a −𝜆∕𝜖)
for ﬁxed a < b tends to 1
2[𝜒[a,b] + 𝜒(a,b)] as
𝜖→0, we obtain the relation
EA([a, b]) + EA((a, b))
= 1
𝜋i s lim
𝜖→0+ ∫
b
a
[RA(𝜆+ i𝜖) −RA(𝜆−i𝜖)] d𝜆,
(13.32)
which is known as the Stone formula. If
𝜎p(A) = ∅, there is no diﬀerence between
EA([a, b]) and EA((a, b)). The behavior of
the integrand in the vicinity of the real axis
determines also more subtle spectral prop-
erties, in particular, if
sup
0<𝜖<1 ∫
b
a
||Im (𝜓, RA(𝜆+ i𝜖)𝜓)||
p d𝜆< ∞
(13.33)
holds for some p > 1 and 𝜓∈, then
EA((a, b))𝜓belongs to ac(A), the abso-
lutely continuous spectral subspace of the
operator A.
In Section 13.5.2, we have mentioned
how a spectral measure on ℂcan be asso-
ciated with a bounded normal operator.
In a similar way, one can treat a ﬁnite
family of commuting self-adjoint operators
A1, … , An. One can construct a projection-
valued measure E on ℝn as the product
measure of EA1, … , EAn. This allows us
to deﬁne functions of such a family of
commuting operators by
f (A1, … , An) ∶= ∫ℝn f (𝜆1, … , 𝜆n)
dE(𝜆1, … , 𝜆n) ;
(13.34)
it is easy to see that such an operator com-
mutes with all the A1, … , An.
In
quantum
physics,
observables
described
by
commuting
self-adjoint
operators are called compatible; they can
be measured simultaneously, which means
we can measure them in any order provided
the measurements follow immediately after
each other. The probability of ﬁnding the
measured values in the set M ⊂ℝn is
w(M, {A1, … , An}; W) = Tr(EA(M)W)
(13.35)
if the system before the measurement was
a mixed state described by a density matrix
W. One can imagine that compatible
observables are measured by a single appa-
ratus. It can also be used to measure their
functions (13.34) after a proper rescaling;
the appropriate probability equals
w(M,f (A1, … , An); W)
= w(f (−1)(M), {A1, … , An}; W) ,
(13.36)
assuming again that the initial state of the
system is described by a density matrix W.
A particular case of commuting self-
adjoint operators are those which can
be expressed through tensor products
as described in Section 13.4.3. The oper-
ators
A1 = A1 ⊗I2
and
A2
commute
with each other and 𝜎(Aj) = 𝜎(Aj) holds
for
j = 1, 2.
The
self-adjoint
operator
P[A1, A2] corresponding to (13.18) coin-
cides with P(A1, A2) deﬁned according
to (13.34) and its spectrum is given by
𝜎(P[A1, A2]) = P(𝜎(A1) × 𝜎(A2)).
For example, the measured values of
the total energy of a system consist-
ing
of
two
noninteracting
subsystems
are sums of measured values of the
corresponding
subsystem
energies,
i.e.
𝜎(H1 + H2) = {𝜆1 + 𝜆2 ∶𝜆j ∈𝜎(Hj)}.

13.5 Spectral Theory of Self-Adjoint Operators
467
13.5.4
Groups of Unitary Operators
Let us now consider families {U(s) ∶s ∈
ℝ} of unitary operators on a given such
that the map s →U(s) is strongly contin-
uous, that is, U(⋅)x is continuous for any
x ∈, and the group property is satisﬁed,
U(t + s) = U(t)U(s) for any t, s ∈ℝ. With
such a group, one can associate its generator
acting as
Tx = lim
s→0
U(s) −I
is
x
(13.37)
on the domain consisting of all x ∈for
which the limit exists. It is not diﬃcult to
check that {eisA ∶s ∈ℝ} corresponding to
a self-adjoint A is a strongly continuous
unitary group and A is its generator; a deep
result says that the converse is also true.
Theorem 13.14 (Stone theorem) To
any strongly continuous unitary group
{U(s) ∶s ∈ℝ} there is a unique self-
adjoint operator A such that U(s) = eisA
holds for any s ∈ℝ.
The theorem has various consequences.
Using functional calculus, for instance, it
yields an alternative expression of com-
mutativity: self-adjoint operators A1, A2
commute iﬀ[eisA1, eitA2] = 0 holds for all
s, t ∈ℝ.
Elementary examples are operators by
eisx on L2(ℝ) generated by the operator
Q, or the group of translations of L2(ℝ)
which is in view of (13.30) generated by the
momentum operator P. Another example
is provided by the group of dilations
associated with scaling of the real axis,
(Ud(s)𝜓)(x) = es∕2𝜓(esx) on L2(ℝ), which
is generated by the symmetrized product
Ad = 1
2 PQ + QP.
Unitary groups arise in various contexts
in quantum physics. The most important
are the operators U(t) = e−itH, where H is
the operator of total energy, or Hamilto-
nian, which describe the time evolution of
a conservative system (in the units where
ℏ= 1). If such a system is undisturbed by
measurements, the state vector 𝜓0 at the
initial time instant t = 0 is mapped to 𝜓t =
U(t)𝜓0 at the time t. The diﬀerential form
of the evolution is known as the Schrödinger
equation,
i d
dt 𝜓t = H𝜓t
(13.38)
with the initial condition 𝜓0 ∈D(H), or
alternatively id∕dtWt𝜙= [H, Wt]𝜙for a
mixed state described by density matrix
Wt.
If Hermitean operators A, B commute,
then the products eisAeisB form a strongly
continuous unitary group and A + B is its
generator. This need not be true if the oper-
ators are unbounded but the conclusion
still makes sense in the functional-calculus
sense. If A, B do not commute, the products
may not form a group; however, we have a
limiting relation called the Trotter formula:
if C = A + B is e.s.a., then
eitC = s lim
n→∞
(
eitA∕neitB∕n)n .
(13.39)
This result is useful because often we
have operators representing observables
that are sums of self-adjoint parts, and
we know explicit expressions of unitary
groups of the latter. A prime example is
that of quantum-mechanical Hamiltonians
of the form −Δ + V(x) where Trotter’s
formula provides a way to express the
corresponding evolution operator through
the Feynman path integral [1, Chap. X; 4].
If Uj(⋅) is a strongly continuous unitary
group on j with the generator Aj, j = 1, 2,
then the operators U1(s) ⊗U2(s) also form
such a group and its generator is A1 + A2.
For instance, the evolution operator of

468
13 Functional Analysis
a system consisting of two noninterac-
tion subsystems with Hamiltonians Hj is
e−itH1 ⊗e−itH2. This conclusion extends
easily to any ﬁnite number of subsystems;
it naturally ceases to be valid if the subsys-
tems interact so that the total Hamiltonian
is no longer of the form H1 + H2.
In a similar way, one can take ten-
sor products of unitary group elements
referring to diﬀerent values of the param-
eters involved. For example, the operators
U(s) = eis1P1 ⊗· · · ⊗eisnPn
on
L2(ℝn),
where
s = (s1, … , sn),
form
the
group
of
translations
of
the
n-dimensional
Euclidean
space
generalizing
relation
(13.30),
(U(s)𝜓)(x) = 𝜓(x + s)
for
any
x = (x1, … , xn) ∈ℝn. By means of the n-
dimensional Fourier–Plancherel operator,
these
operators
are
unitarily
equiva-
lent
to
V(s)𝜓)(x) = eis⋅x𝜓(x),
where
s ⋅x = s1x1 + · · · + snxn is the inner product
in ℝn.
The unitary groups mentioned in the
last example are associated with impor-
tant quantum-mechanical variables, coor-
dinates of the position and momentum. It
is easy to check that their products in a dif-
ferent order diﬀer by an exponential factor,
U(t)V(s) = eis⋅tV(s)U(t) ,
s, t ∈ℝn.
(13.40)
The relations (13.40) can be regarded a
mathematically rigorous form of canonical
commutation relations, often referred to as
their Weyl form; the operators U(t), V(s)
on L2(ℝn) described in the previous para-
graph deﬁne the so-called Schrödinger rep-
resentation of (13.40). A fundamental ques-
tion concerns the uniqueness of this rep-
resentation; we ask, of course, about irre-
ducible representations for which no non-
trivial projection commutes with all the
operators.
Theorem 13.15 (Stone–von
Neumann
theorem) Any irreducible (unitary, strongly
continuous) representation of the Weyl
relations (13.40) is unitarily equivalent
to the Schrödinger representation of the
corresponding dimension.
The reader should be warned that an
analogue of this theorem in situations with
inﬁnite number of degrees of freedom,
that is, when ℝn is replaced with a real
Hilbert space of inﬁnite dimension, is not
valid. Indeed, in quantum ﬁeld theory one
can ﬁnd examples with inﬁnite number
of inequivalent representations of such
generalized Weyl relations.
13.6
Some Applications in Quantum Mechanics
We have mentioned already some ways
in which functional-analytic notions and
results are used in quantum physics. In the
last section of this chapter, we will brieﬂy
describe two speciﬁc applications. We do it
mostly to whet the reader’s appetite; there is
a large number of related results for which
we refer to the literature indicated at the
end of the chapter.
13.6.1
Schrödinger Operators
In nonrelativistic quantum mechanics, the
Hamiltonian of a spinless quantum particle,
or a system of such particles, is often of the
form
H = −Δ + V(x)
(13.41)
on L2(ℝn). In general, the expression
involves nontrivial coeﬃcients, in particu-
lar, the e.s.a. operator describing the kinetic
part is ∑n
j=1(1∕2mj)P2
j ; however, one can
always put 2mj = 1 by using suitable units.
The ﬁrst question when analyzing oper-
ators (13.41) concerns their (essential)
self-adjointness. The answer is easy if the

13.6 Some Applications in Quantum Mechanics
469
potential V
is bounded; unfortunately,
most potentials we have to deal with in
actual physical models are unbounded.
One way to address the self-adjointness
problem uses perturbation theory; the
Kato–Rellich theorem can be used to make
the following conclusion:
Theorem 13.16 (Suﬃcient self-adjoint-
ness condition) Assume that the potential
V ∈Lp + L∞, that is, V = Vp + V∞with
V∞∈L∞(ℝn)
and
Vp ∈Lp(ℝn),
where
p = 2 if n ≤3 and p > 1
2n for n ≥4, then
the operator (13.41) is e.s.a. on any core of
H0 = −Δ.
This result is not immediately applica-
ble to operators (13.41) on L2(ℝ3N) describ-
ing systems on N particles, N > 1. In this
case, the interaction is typically a sum of
potentials with the property that there is
a three-dimensional projection E in ℝ3N
such that V(x) = V(Ex). Such potentials
describe either one-particle forces when E
refers to coordinates of a single particle, or
two-particle ones when E refers to relative
coordinates of a pair of particles.
Theorem 13.17 (Kato theorem) Let
n =
3N and V = ∑m
k=1 Vk, where each potential
component Vk(Ek ⋅) ∈(L2 + L∞)(ℝ3); then
the operator (13.41) is e.s.a. on any core of
H0.
The main importance of this result is
that it guarantees self-adjointness of atomic
Hamiltonian with potentials of Coulomb
type for electron charge e,
V(x) = −
Z
∑
j=1
Ze2
|xj −x0| +
∑
1≤j<k≤Z
e2
|xj −xk|,
(13.42)
referring to a ﬁxed nucleus of atomic num-
ber Z, and similar operators describing
atoms with a ﬁnite nucleus mass as well as
molecules, atomic and molecular ions, and
so on; it means that the usual quantum-
mechanical description of such objects can
be justiﬁed from the ﬁrst principles.
Analyzing the discrete spectrum of oper-
ators (13.41), one is often interested in rela-
tions between the number and position of
the eigenvalues on the one hand and the
potential on the other. There are many such
results of which we will mention just two
important ones. A frequently used estimate
is expressed in the following way:
Theorem 13.18 (Birman–Schwinger
bound) Consider the operator (13.41) on
L2(ℝ3) with the potential such that the
right-hand side of the relation (13.43) is
ﬁnite; then the number of eigenvalues of H
is ﬁnite and satisﬁes the inequality
N(V) ≤
1
16𝜋2 ∫ℝ6
|V(x)V(y)|
|x −y|2
dx dy .
(13.43)
Note that no similar bound exists for n =
1, 2 because there an arbitrarily weak neg-
ative potential produces a bound state, that
is, an isolated negative eigenvalue.
The bound (13.43) does not exhibit a cor-
rect semiclassical behavior, which means
that it becomes poor when we replace V
by gV and study the asymptotic behavior
as g →∞. This is not case for the following
more general result.
Theorem 13.19 (Lieb–Thirring inequa-
lity)
Suppose
that
𝜎d(H) = {𝜆j}
and
ﬁx 𝛾≥0 for n ≥3, 𝛾> 0 for n = 2, and
𝛾≥1
2 for n = 1; then the inequality
Tr (H𝛾
−) =
∑
j
(−𝜆j)𝛾≤L𝛾,n ∫ℝn V−(x)𝛾+ n
2 dx
(13.44)
holds,
where
L𝛾,n ≥Lcl
𝛾,n = Γ(𝛾+ 1)
[2n
𝜋n∕2Γ(𝛾+ n
2 + 1)]−1
and
V−(x) =
max{−V (x), 0} is the negative part of the
potential.

470
13 Functional Analysis
In
fact,
the
constants
are
L𝛾,n =
R(𝛾, n)Lcl
𝛾,n where R(𝛾, n) = 1 for 𝛾≥3∕2,
R(𝛾, n) ≤2 for 1 ≤𝛾< 3∕2 or 1∕2 ≤𝛾< 1
and n = 1, and R(𝛾, n) ≤4 for 1∕2 ≤𝛾< 1
and
n ≥2.
In
dimensions
n ≥3,
the
inequality holds for 𝛾= 0, which yields a
bound to the number of bound states,
N(V) ≤L0,n ∫ℝn V−(x)n∕2 dx,
(13.45)
known
as
the
Cwikel–Lieb–Rozeblium
theorem.
The number of bound states can be inﬁ-
nite if the potential has a slow enough decay
at inﬁnity. Suppose that V = V2 + V∞with
V2 ∈L2(ℝ3) and V∞∈L∞(ℝ3) such that
V∞(x) →0 holds as |x| →∞. This ensures
that 𝜎ess(H) = [0, ∞). If there are c ∈[0, 1
4
)
and r0 ≥0 such that V(x) ≥−c|x|−2 holds
|x| ≥r0, then 𝜎d(H) is ﬁnite. On the other
hand, the discrete spectrum is inﬁnite pro-
vided there are positive d, r0, 𝜖such that
V(x) ≤−d|x|−2+𝜖holds |x| ≥r0.
Behavior of the potential at large dis-
tances is important in determining the
essential spectrum also more generally.
In some cases of physical importance,
this task is not easy; in particular, for
systems of N particles interacting through
two-particle forces associated with the
potential of the form
V(x1, … , xN) =
∑
1≤j<k≤Z
Vjk(xj −xk), (13.46)
where xj = (xj1, xj2, xj3) are coordinates of
the jth particle. We divide our N particles
into n(D) clusters Ci; if N = 2 there is only
one such partition, for N ≥2 there are
diﬀerent
partitions
D = {C1, … , Cn(D)}.
For each partition we deﬁne 𝜖D
jk to be
one if the indices j, k belong to the same
cluster and zero otherwise. This allows
us to deﬁne the Hamiltonian with the
intercluster
interaction
switched
oﬀ,
HD = −Δ + ∑N
j<k=1 𝜖D
jk Vjk(xj −xk), and the
cluster Hamiltonians
HCi = −ΔCj +
N
∑
{j,k∈Cj∶j<k}
Vjk(xj −xk) ;
(13.47)
in fact, we are interested in the Hamilto-
nians Hrel
Ci with the center-of-mass motion
separated acting in L2(ℝ3ni−3), where ni is
the number of particles in the cluster Ci.
Using them, we deﬁne for a given D the
quantity 𝜆D ∶= ∑n(D)
i=1 inf 𝜎(Hrel
Ci
).
Theorem 13.20 (Hunziker–van Winter–
Zhislin theorem) Suppose that the two-
particle potentials are Vjk = Vjk,2 + Vjk,∞
with Vjk,2 ∈L2(ℝ3) and Vjk,∞∈L∞(ℝ3)
such that Vjk,∞(x) →0 holds as |x| →∞;
then the essential spectrum of H is [Λ, ∞)
where
Λ ∶= min
n(D)≥2 𝜆D = min
n(D)=2 𝜆D .
(13.48)
13.6.2
Scattering Theory
The second application we are going to dis-
cuss concerns one of the most frequently
occurring situations in physics when we
compare the dynamics of a quantum sys-
tem with an asymptotic one. Suppose that
we have a pair of Hamiltonians, the “full”
one H and the “free” one H0, together
with the corresponding evolution opera-
tors, U(t) = e−itH and U0(t) = e−itH0. The
question is how to ﬁnd to a given 𝜓∈
vectors 𝜓± such that the “trajectories”
U(t)𝜓and U0(t)𝜓± coincide asymptotically
as t →±∞.
Naturally, there may be vectors 𝜓for
which no 𝜓± exist, for instance, bound
states of the system described by eigenvec-
tors of the operator H. The distinguishing
property is whether a state remains local-
ized or leaves a ﬁxed space region. We

13.6 Some Applications in Quantum Mechanics
471
consider an increasing family {Mr ∶r ≥0}
of subspaces of the conﬁguration space
and denote by Fr the respective projections
assuming that s limr→∞Fr = I; for deﬁ-
niteness, think of concentric balls of radius
r. Using these projections, we deﬁne the
family
s(H) =
{
𝜓∈∶lim
|t|→∞FrU(t)𝜓= 0
for any r > 0
}
(13.49)
of scattering states, and the complementary
family of bound states,
b(H) =
{
𝜓∈∶lim
r→∞sup
t∈ℝ
‖(I −Fr)
U(t)𝜓‖ = 0
}
.
(13.50)
It is not diﬃcult to check that b(H) ⊃
p(H) and s(H) ⊃c(H) where we use
the spectral subspaces notation introduced
in Section 13.5.2. We identify conven-
tionally the scattering states with the
absolutely continuous part of H, setting
s(H) ⊃ac(H). This means to exclude
states associated with sc(H) that typically
lie “between” the bound and scattering
states: the probability of ﬁnding them in a
ﬁxed bounded region may not have zero
limit as |t| →∞but its mean value can be
made arbitrarily small when averaged over
a suﬃciently long time interval. One usu-
ally aims to prove that such pathological
states are absent, that is, 𝜎sc(H) = ∅.
In this setup, we can describe the asymp-
totic relation between the full and free
dynamics by a pair of wave operators
deﬁned by
Ω±(H, H0) ∶= s lim
t→±∞U(t)∗U0(t)Eac(H0),
(13.51)
which map ac(H0) into ac(H). In gen-
eral, it may happen that Ran Ω±(H, H0) ≠
s(H), for instance, if the scattered parti-
cle is captured by the target and is unable to
leave the interaction region. The wave oper-
ators are said to be complete if
Ran Ω+(H, H0) = Ran Ω−(H, H0) = ac(H),
(13.52)
and asymptotically complete if, in addi-
tion, 𝜎sc(H) = ∅. Under the completeness
assumption, one can deﬁne, in particu-
lar, the scattering operator (or S-matrix),
S(H, H0) ∶= Ω+(H, H0)∗Ω−(H, H0), which
maps the “incoming” asymptotic states into
the “outgoing” ones.
The wave operators are assigned to a pair
of operators. They satisfy the chain rule,
Ω±(H, H0) = Ω±(H, H1)Ω±(H1, H0)
for
any
self-adjoints
H, H1, H0
for
which
the
wave
operators
exist.
Another
important property is the intertwining
relation,
Ω±(H, H0)H0 ⊂HΩ±(H, H0).
It implies, in particular, that the scat-
tering
operator
commutes
with
the
free
Hamiltonian,
or
equivalently,
U0(t)S(H, H0) = S(H, H0)U0(t)
holds
for all t ∈ℝ. As a consequence, it can be
expressed in the form of a direct integral,
S(H, H0) = ∫
⊕
𝜎ac(H0)
S(𝜆) d𝜆;
(13.53)
for the ﬁber operators S(𝜆), we usu-
ally employ the name on-shell scattering
matrix.
Many suﬃcient conditions have been
derived for existence of wave operators.
Some are abstract and rather simple such
as the following one.
Theorem 13.21 (Kato–Rosenblum
theorem)
Suppose
that
H = H0 + V,
where H0 is self-adjoint and V is a Her-
mitean
trace-class
operator;
then
the
wave operators Ω±(H, H0)H) exist and are
complete.

472
13 Functional Analysis
The range of applications of this result is
rather limited, because the interaction part
of the Hamiltonian in physical models is
rarely of a trace-class character. More often,
one can use the following related result.
Theorem 13.22 (Birman–Kuroda
theorem) Let H, H0 be self-adjoint and
the resolvent diﬀerence (H −z)−1 −(H0 −
z)−1 ∈1() for some z ∈𝜌(H) ∩𝜌(H0);
then the wave operators Ω±(H, H0) exist
and are complete.
As regards Schrödinger operators, one
is usually interested in scattering caused
by the potential; in other words, the situ-
ation where H is the operator (13.41) and
H0 = −Δ on L2(ℝn). Various existence con-
ditions for the wave operators in terms of
the potential V can be derived, for instance,
Theorem 13.23 (Hack–Cook theorem)
Let H be the operator (13.41) on L2(ℝ3).
If the potential V ∈(L2 + Ls)(ℝ3) with
s ∈[2, 3),
then
the
wave
operators
Ω±(H, H0) exist.
Completeness of the wave operators
requires
additional
hypotheses.
As
a
sample result we mention the following
suﬃcient conditions.
Theorem 13.24 Let H be the operator
(13.41) on L2(ℝ3) with V ∈(L1 ∩L2)(ℝ3),
then the wave operators Ω±(H, H0) exist
and are complete. If, in addition,
1
16𝜋2 ∫ℝ6
|V(x)V(y)|
|x −y|2
dx dy < 1
(13.54)
holds, then 𝜎sc(H) = ∅, so the wave opera-
tors are asymptotically complete.
The asymptotic completeness can be
established for many other scattering sys-
tems described by Schrödinger operators.
In particular, it has been demonstrated
in the situation when (13.41) describes a
system of N particles interacting through
Coulomb potentials of the type (13.42),
hence scattering processes in atomic and
molecular physics can again be studied
from the ﬁrst principles.
Glossary
Banach space: a (metrically) complete
normed space
Commutativity of self-adjoint operators:
it means commutativity of their spectral
decompositions
Compact operator: it maps each bounded
set into a precompact one
Deﬁciency indices of a symmetric A: the
numbers dim Ker(A∗∓i)
Deﬁciency subspace of T: all solutions to
the equation T∗x = 𝜆x
Density
matrices:
positive
trace-class
operators normalized by Tr W = 1
Discrete spectrum: isolated eigenvalues of
ﬁnite multiplicity
Essential spectrum: complement of the
discrete spectrum
Essentially self-adjoint operator: its clo-
sure is self-adjoint
Fourier
expansion:
expression
of
a
Hilbert-space vector with respect to an
orthonormal basis
Fourier–Plancherel operator: the contin-
uous extension of Fourier transformation to
L2(ℝn).
Function spaces: families of functions,
or equivalence classes of functions, with
pointwise deﬁned addition and multiplica-
tion
Hamiltonian:
an
operator
describing
energy of the system
Hermitean operator: a bounded self-
adjoint operator
Hilbert space: a (metrically) complete
inner-product space

References
473
Hilbert–Schmidt operator: an operator
C such that ∑
j ‖Cej‖2 < ∞holds for any
orthonormal basis {ej}
Inner product: a sesquilinear form on a
vector space such that (x, x) = 0 implies
x = 0
Limit point – limit circle: alternatives
determining deﬁciency indices of a one-
dimensional Schrödinger operator
Normal operator: it commutes with its
adjoint
Operator: a linear map between (subspaces
of) Banach (Hilbert) spaces
Operator with pure point spectrum: its
eigenvectors form an orthonormal basis
Parseval identity: it expresses the vector
norm through its Fourier coeﬃcients
Point spectrum: eigenvalues of the opera-
tor
Projection: an operator assigning to any
vector x ∈its orthogonal projection to a
given subspace ⊂
Pure state of a quantum system: a one-
dimensional subspace in the state Hilbert
space of the system
Riesz lemma: an expression of a bounded
linear functional on a Hilbert space
Self-adjoint operator: it coincides with its
adjoint
Spectral measure: a measure the values of
which are projections on a given Hilbert
space
Spectrum of an operator T: those 𝜆for
which the resolvent RT(𝜆) = (T −𝜆)−1 does
not exist as a bounded operator
Stone formula: expression of the spectral
measure in terms of the resolvent
Symmetric operator: it is a restriction of
its adjoint
Trace-class operator: an operator C such
that Tr |C| < ∞
Unitarily equivalent operators: they can
be mapped one to another with the help of
a unitary operator
Unitary
operator:
an
isometric
map
deﬁned on the whole Hilbert space
Wave
operators:
asymptotic
maps
between the full and free dynamics in
a scattering system
Weyl relations: expression of canonical
commutation relations in terms of the asso-
ciated unitary groups
Weyl theorem: stability of the essential
spectrum with respect to compact pertur-
bations
References
1. Reed, M. and Simon, B. (1972–1978)
Methods of Modern Mathematical Physics
I–IV, Academic Press, New York.
2. Blank, J., Exner, P., and Havlíˇcek, M. (2008)
Hilbert Space Operators in Quantum
Physics, 2nd edn, Springer, Dordrecht.
3. Bratelli, O. and Robinson, D.W. (1979, 1981)
Operator Algebras and Quantum Statistical
Mechanics I, II, Springer-Verlag, New York.
4. Albeverio, S.A., Høegh-Krohn, R.J., and
Mazzucchi, S. (2008) Mathematical Theory
of Feynman Path Integrals, 2nd edn,
Springer, Berlin.
5. Adams, R.A. and Fournier, J.J.F. (2003)
Sobolev Spaces, 2nd edn, Elsevier, Oxford.
6. Akhiezer, N.I. and Glazman, I.M. (1993)
Theory of Linear Operators in Hilbert Space,
Dover, Mineola, NY.
7. Albeverio, S.A., Gesztesy, F., Høegh-Krohn,
R., and Holden, H. (2005) Solvable Models in
Quantum Mechanics, 2nd edn with appendix
by P. Exner, AMS Chelsea, Providence, RI.
8. Amrein, W.O., Jauch, J.M., and Sinha, K.B.
(1977) Scattering Theory in Quantum
Mechanics, Benjamin, Reading, MA.
9. Birman, M.Š. and Solomyak, M.Z. (1987)
Spectral Theory of Self-Adjoint Operators in
Hilbert Space, Kluwer, Dordrecht.
10. Cycon, H.L., Froese, R.G., Kirsch, W., and
Simon, B. (2007) Schrödinger Operators, with
Applications to Quantum Mechanics and
Global Geometry, 2nd printing, Springer,
Berlin.
11. Davies, E.B. (2007) Linear Operators and
their Spectra, Cambridge University Press,
Cambridge.

474
13 Functional Analysis
12. Exner, P. (1985) Open Quantum Systems and
Feynman Integrals, Reidel, Dordrecht.
13. Jauch, J.M. (1968) Foundations of Quantum
Mechanics, Addison-Wesley, Reading, MA.
14. Kolmogorov, A.N. and Fomin, S.V. (1999)
Elements of the Theory of Functions and
Functional Analysis, Dover, Rochester, NY.
15. Lieb, E.H. and Loss, M. (2001) Analysis, 2nd
edn, AMS, Providence, RI.
16. Lieb, E.H. and Seiringer, R. (2010) The
Stability of Matter in Quantum Mechanics,
Cambridge University Press, Cambridge.
17. von Neumann, J. (1996) Mathematical
Foundations of Quantum Mechanics, 12th
printing, Princeton University Press,
Princeton, NJ.
18. Prugoveˇcki, E. (1981) Quantum Mechanics
in Hilbert Space, 2nd edn, Academic Press,
New York.
19. Riesz, F. and Sz-Nagy, B. (1972) Leçons
d’analyse foncionelle, 6me edn, Akademiai
Kiadó, Budapest.
20. Rudin, W. (1991) Functional Analysis, 2nd
edn, McGraw-Hill, New York.
21. Simon, B. (2005) Trace Ideals and Their
Applications, 2nd edn, AMS Chelsea,
Providence, RI.
22. Weidmann, J. (2000, 2003) Lineare
Operatoren in Hilberträumen I, II, B.G.
Teubner, Stuttgart.
23. Yafaev, D.R. (1992) Mathematical Scattering
Theory, AMS, Providence, RI.

475
14
Numerical Analysis
Lyonell Boulton
14.1
Introduction
The foundations of the compendium of
mathematical
theories
now
known
as
numerical analysis can be traced back
to the Plimton 322 clay tablet, which is
believed to be over 3800 years old. How-
ever, arguably, it was only with the advent
of machines capable of performing a pre-
scribed set of mathematical tasks without
direct human intervention, that numerical
analysis consolidated as an independent
subject of scientiﬁc interest. For some,
numerical analysis is one of the success
stories of the second half of the twentieth
century.
In recent years, areas of numerical
analysis such as numerical linear algebra,
optimization,
and
numerical
diﬀeren-
tial equations, have turned themselves
into highly specialized research subjects.
Language and methods are increasingly
becoming only accessible, and often only
of interest, to specialists. Yet some of
these methods are of prime importance
in theoretical and experimental contem-
porary science. As electronically assisted
mathematics, ranging from the use of
highly specialized scientiﬁc apparatus to
large computer networks, permeated into
most of our scientiﬁc experience nowa-
days, there is an increasing need for the
development of mathematically rigorous
frameworks capable of validating the data
that is being produced by those means.
In this chapter, we only scratch the sur-
face of some of the classical themes in
numerical analysis. It is intended as a very
rough introduction to the language and
techniques in the theory for nonspecial-
ists. It is by no means exhaustive; however,
it may serve as a reading guide to more
specialized literature on the subject. We
miss many important aspects of the theory,
including the crucial connection with con-
crete computational implementations. An
excellent introduction to the latter is avail-
able in [1].
Section 14.2 is concerned with classical
techniques for the solution of nonlinear
equations and systems. This includes some
qualitative aspects of the described meth-
ods and a brief introduction to numeri-
cal minimization. The particular case of
linear systems of equations, which is far
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

476
14 Numerical Analysis
more specialized and developed, is con-
sidered separately in Section 14.3. There
we describe the standard classiﬁcation of
methods for the solution of systems of lin-
ear equations into direct and iterative, both
of which are important in their own right.
We also discuss the classical approach to
ﬁnite eigenvalue problems. Section 14.4 is
devoted to approximation of continuous
data. We only describe the classical the-
ory of polynomial interpolation as a back-
ground to the highly developed theory of
ﬁnite elements. In Sections 14.5 and 14.6,
we focus on two concrete subjects. In the
former, we describe in some detail the ele-
ments of the theory for the numerical solu-
tion of the initial-value (Cauchy) problems
for ordinary diﬀerential equations (ODEs).
In the latter, we describe the use of the
Galerkin method for the numerical solution
of spectral problems in the context of one-
dimensional Schrödinger operators.
14.2
Algebraic Equations
The ﬁrst type of mathematical problem that
commonly requires a numerical approx-
imation is an algebraic equation. By far
the most successful general tools for the
approximated solution of general scalar or
vector algebraic equations are the iteration
methods. The special case of linear systems
of equations is of great importance and it
will be dealt with separately in the next
section.
14.2.1
Nonlinear Scalar Equations
Consider a general equation
f (x) = 0
for a given continuous scalar function f ∶
[a, b] −→ℝ. An analytic solution of this
problem can only be found in close form
for very particular functions f . Otherwise,
approximation of possible solutions can be
achieved by means of ﬁxed-point iteration
schemes: we pick an initial guess x0 and
iterate
xn+1 = g(xn),
n = 1, … ,
to get the ﬁxed point
x∗= lim
n→∞xn
such that g(x∗) = x∗. Here x = g(x) is equiv-
alent to f (x) = 0, so that f (x∗) = 0.
Example 14.1 [Newton’s method] See [2,
Section 2.1]. Kepler’s model for the plane-
tary motion seeks for solutions x∗(depen-
dent on time) of the equation
x −sin x = 𝜏.
Here is the (ﬁxed) eccentricity of the ellip-
tical orbit and 𝜏is proportional to time.
Newton proposed iterating
xn+1 = xn + 𝜏−xn + sin xn
1 −cos xn
n = 1, … ,
in order to obtain x∗. This corresponds to
choosing
g(x) = x −f (x)
f ′(x) for f (x) = 𝜏−x + sin x
above, and it is the special case of what is
now called Newton’s method.
If the function g is Lipschitz continuous,
|g(x) −g(y)| ≤𝜆|x −y|

14.2 Algebraic Equations
477
for all x, y in a segment [a, b] containing
x∗, then the absolute error en = x∗−xn
satisﬁes
|en| ≤𝜆|en−1| ≤𝜆n|e0|
for all
n ≥1.
If 𝜆< 1 and g(x) ∈[a, b] for all x ∈[a, b],
then there exists a unique ﬁxed point x∗∈
[a, b] of g(x) and |en| →0 for any choice of
x0. These conditions are satisﬁed in a neigh-
borhood of x∗if, for example, |g′(x∗)| < 1.
In this case, Taylor’s theorem yields
lim
n→∞
|en|
|en−1| = |g′(x∗)|
(linear convergence).
If further |g′(x∗)| = 0, then
lim
n→∞
|en|
|en−1|2
= 1
2|g′′(x∗)|
(quadratic convergence).
An application of higher-order Taylor’s
approximation shows that the condition
|g(j)(x∗)| = 0 for j = 1, … , k −1, implies
lim
n→∞
|en|
|en−1|k
= 1
k!|g(k)(x∗)|
(kth order convergence).
Particular iteration methods for the
solution of scalar algebraic equations are
described in Table 14.1.
14.2.2
Nonlinear Systems
The idea of ﬁxed-point iteration can be
formally extended to multiple dimensions.
Given G ∶Ω −→ℝd where Ω ⊂ℝd and an
initial x0 ∈Ω, the iteration
xn+1 = G(xn),
n = 1, … ,
would converge to a ﬁxed point x∗∈Ω such
that G(x∗) = x∗, if
‖G(x) −G(y)‖ ≤𝜆‖x −y‖,
x, y ∈Ω,
Table 14.1
Most common iteration methods for the
approximation of solutions of nonlinear scalar equations.
See [3, Chapter 6].
Usual name
Formula for xn+1
Order
Comments/pitfalls
Newton
xn −f (xn)
f ′(xn)
2
f ′(x∗) ≠0
Modiﬁed Newton
xn −kf (xn)
f ′(xn)
2
|f (j)(x∗)| = 0 for j < k and |f (k)(x∗)| ≠0
Steﬀensen
xn −
f (xn)2
f (xn+f (xn))−f (xn)
2
No need to compute f ′
Secant
xn −(xn−xn−1)f (xn)
f (xn)−f (xn−1)
1+
√
5
2
Iteration requires less work than
Newton or Steﬀensen
(General) ﬁxed point
g(xn)
k
g is a general function such that
|g(j)(x∗)| = 0 for j < k and |g(k)(x∗)| ≠0
(General) higher order
g(g(xn))
2k
g is as in the general ﬁxed-point
method and k is its order of
convergence (see previous row)

478
14 Numerical Analysis
for 𝜆< 1. Here ‖ ⋅‖ is a suitable norm of
ℝd. The error vector, en = xn −x∗, satisﬁes
the identity ‖en+1‖ ≤𝜆‖en‖. If the Jacobian
JG(x) =
[𝜕Gj
𝜕xk
]d
jk=1
exists for all x ∈Ω, we can represent G in
Taylor series,
G(y) = G(x) + JG(x)(x −y) + RG(x, y),
RG(x, y) ≤c‖x −y‖2.
This leads to the following general “local”
convergence theorem around ﬁxed points
[4, pp. 299–301].
Theorem 14.1 If ‖JG(x∗)‖ < 1, starting the
iteration at any x0 suﬃciently close to x∗
ensures
lim
n→∞‖xn −x∗‖ = 0
and
lim
n→∞
‖en+1‖
‖en‖
= ‖JG(x∗)‖.
For a system of d algebraic equations in d
unknowns,
F(x) = 0,
a natural extension of the Newton method,
sometimes called the Newton–Simpson
method, can be formulated as follows. Sup-
pose that JF ∶Ω −→ℝd×d is not singular,
for example, det(JF(x)) ≠0 for all x ∈Ω.
Starting from x0 ∈Ω, ﬁnd xn+1 the solution
of
JF(xn)xn+1 = JF(xn)xn −F(xn),
n = 1, …
(14.1)
This iteration corresponds to picking
G(x) = x −JF(x)−1F(x).
In practice, the former is preferable to the
latter, as ﬁnding all entries of the inverse
of the Jacobian matrix is in itself a numeri-
cally challenging problem for large dimen-
sions and it is often unnecessary. See [3,
Chapter 7] and [2, Chapter 7].
In concrete implementations, the itera-
tion scheme (14.1) is usually written in cou-
pled form
{
JF(xn)y
n = −F(xn)
xn+1 = xn −y
n
n = 1, …
Arguments involving Theorem 14.1 show
that, if ‖JF(x)−1F(x)‖ and ‖JF(x)−1HF(x)‖
are uniformly bounded by a small enough
constant on a region Ω (here HF(x) is the
Hessian of F), then xn converges quadrat-
ically to a solution to the corresponding
homogeneous system for any initial x0 ∈Ω.
Example 14.2 [An iteration method for
computing eigenvalues]
See [2, Section
7.2]. Given a hermitian matrix A ∈ℝd×d,
the eigenvalues of A can be estimated by
solving the nonlinear equation in d + 1
components,
F(x, 𝜆) = 0
for
F(x, 𝜆) =
[ Ax −𝜆x
1
2(1 −xt ⋅x)
]
.
The
Newton–Simpson
method
for
F
reduces in this case to the coupled iteration
⎧
⎪
⎨
⎪⎩
(A −𝜆nI)zn = xn
𝜆n+1 = 𝜆n +
1+xt
n⋅xn
xt
n⋅zn
xn+1 = (𝜆n+1 −𝜆n)zn
,
n = 1, … ,
which is equivalent to the inverse power
method. Under fairly mild conditions on A,
𝜆n would converge to the eigenvalue of A
that is smallest in modulus.

14.2 Algebraic Equations
479
14.2.3
Numerical Minimization
Given a diﬀerentiable function f ∶ℝd −→
ℝ, we now consider the unconstrained min-
imization problem of ﬁnding x∗∈ℝd such
that
f (x∗) ≤f (x)
∀x ∈Ω.
See [5]. When Ω = ℝd, x∗is called a global
minimizer. When Ω ⊊ℝd is only a neigh-
borhood of x∗, it is called a local minimizer.
As any minimizer is a critical point,
grad f (x∗) = 0,
the above problem is naturally viewed in the
context of solution to nonlinear systems of
equations.
Closely linked with the ﬁxed-point iter-
ative procedures described previously are
the descent methods [3, Section 7.2.2]: pick
an initial x0 ∈Ω and iterate
xn+1 = xn + 𝛼n𝛿n,
n = 1, …
Here 𝛿n are suitable descent directions sat-
isfying
𝛿t
n ⋅grad f (xn) < 0
and the scalar 𝛼n > 0 measures the stepsize
along this direction. By Taylor’s theorem
f (x + 𝛼𝛿) −f (x) = 𝛼grad f (𝜉)t ⋅𝛿,
where 𝜉lies in the segment joining x with
x + 𝛼𝛿. Hence we always ﬁnd 𝛼n > 0 small
enough such that
f (xn + 𝛼n𝛿n) < f (xn),
(14.2)
whenever grad f (xn) ≠0. If grad f (xn) = 0,
we have arrived at a critical point, so we
may assign 𝛼n = 0 to stop the iteration.
Every choice of 𝛿n and 𝛼n gives rise to
a diﬀerent method, with diﬀerent approx-
imation properties. See Table 14.2.
Determining 𝛼n such that (14.2) is sat-
isﬁed without incurring into high compu-
tational expense, depends on the type of
problem considered. One commonly used
choice is to minimize
𝜙(𝛼) = f (xn + 𝛼𝛿n),
which guarantees that
grad f (xn+1)t ⋅𝛿n = 0.
Procedures leading toward ﬁnding minima
of 𝜙(𝛼) in this context are called linear
search techniques. See [3, Section 7.2.3].
Example 14.3 [Descent
methods
for
quadratic forms] Let A ∈ℝd×d be a hermi-
tian positive deﬁnite matrix and let b ∈ℝd
Table 14.2
Descent directions for some of the standard
methods of unconstrained minimization. See [5].
Method
𝛿n
Comments / pitfalls
Newton
−Hf (xn)−1 grad f (xn)
Computing Hf (xn)−1 is usually impractical
Inexact Newton
−B−1
n grad f (xn)
Bn ≈Hf (xn)−1
Gradient
−grad f (xn)
Inexact Newton with Bn = I and standard for quadratic
forms
Conjugate gradient
−grad f (xn) + 𝛽n𝛿n−1
𝛽n chosen so that 𝛿n are mutually orthogonal

480
14 Numerical Analysis
be a ﬁxed vector. Let
f (x) = 1
2xt ⋅Ax −xt ⋅b.
As
grad f (x) = Ax −b,
the minimum of f will occur exactly at the
solution of a linear system. Let
rn = b −Axn.
The gradient method uses the direction of
this residual as search direction and
𝛼n =
rt
n ⋅rn
rt
n ⋅Arn
.
On the other hand, for the conjugate gradi-
ent method,
𝛼n =
rt
n ⋅𝛿n
𝛿t
n ⋅A𝛿n
and
𝛽n =
rt
n ⋅𝛿n−1
𝛿t
n−1 ⋅A𝛿n−1
.
The latter uses a more clever choice of
search direction, because it avoids reusing
previously chosen directions.
14.3
Finite-Dimensional Linear Systems
Particular techniques for the approxima-
tion of solutions of the linear inhomoge-
neous equation
Ax = b,
(14.3)
as well as the associated linear eigenvalue
problem
Au = 𝜆u
u ≠0,
(14.4)
deserve special attention given their sim-
ple structure. Here A ∈ℂd×d and b ∈ℂd
are data, and the unknowns are x ∈ℂd in
the former and (𝜆, u) ∈ℂ× ℂd in the latter
problem.
If det A ≠0, the solution of (14.3) can be
determined from Cramer’s rule,
xj =
det Aj
det A ,
j = 1, … , d,
where Aj is the matrix obtained by replac-
ing the jth entry of A by b. However, this
approach is of practical interest only for
matrices of small size, as the calculation
of determinants requires a high number of
ﬂops (ﬂoating point arithmetic operations).
For standard matrices this will be roughly
3(d + 1)! operations, which is extremely
ineﬃcient for n > 10.
Two types of methods are available for
large matrices, the direct methods and the
iterative methods.
14.3.1
Direct Methods and Matrix Factorization
Direct methods determine the solution of a
linear system in a ﬁnite number of steps.
Gaussian elimination reduces a general
system of the form (14.3) to a triangular row
echelon form,
A1
11x1 + A1
12x2 + … + A1
1dxd = b1
1
A2
22x2 + … + A2
2dxd = b2
2
⋮
⋮
Ad
ddxd = bd
d.
We can ﬁnd an explicit expression for
the coeﬃcients of this system via the
Gauss elimination algorithm running for
r = 1, … , d −1:

14.3 Finite-Dimensional Linear Systems
481
A1
jk = Ajk
Ar+1
jk
= Ar
jk −mjrAr
rk
mjr =
Ar
jr
Ar
rr
b1
j = bj
br+1
j
= br
j −mjrbr
r
j, k = 1, … , d
j, k = r + 1, … , d
j = r + 1, … , d.
This assumes that the pivot elements Ar
rr
are nonzero. The matrices
U = [Aj
jk]d
jk=1
and
L = [mjk]d
jk=1
mjj = 1,
render a decomposition of the form
A = LU,
which is usually called an LU-factorization
of A. Here L ∈ℂd×d is lower triangular
(Ljk = 0 for k < j) and U ∈ℂd×d is upper
triangular (Ujk = 0 for k > j).
In turns, (14.3) is equivalent to the two
triangular systems
Ly = b
Ux = y.
The former can be solved by forward sub-
stitution
y1 = b1
L11
yj = 1
Ljj
(
bj −
j−1
∑
k=1
Ljkyk
)
,
j = 2, … , d −1,
while the latter can be solved by back-
ward substitution (with a similar recursive
formula).
The factorization of matrices, such as the
LU-factorization, are important in the eﬃ-
cient solution of linear systems, as there can
be a signiﬁcant reduction in the number of
ﬂops required to get the solution compared
with Gaussian elimination. This is partic-
ularly relevant, when we are required to
solve many problems for the same matrix
A, but diﬀerent vectors b. Gauss elimination
roughly requires 2d3∕3 operations, whereas
forward and backward substitutions only
require d2 ﬂops each. See [2, Chapter 4] and
references therein.
The pivot elements of a given system
are all diﬀerent from zero if, and only if,
each one of the r × r upper-left minors of
A, [Ajk]r
jk=1, are nonsingular. Otherwise,
methods called pivoting, which involve
reordering either the rows or the columns
(partial pivoting) – or both (full), can be
employed to produce an LU-factorization
of the matrix. The Gauss elimination
with partial pivoting (either by row or by
column) always leads to nonzero pivot
elements that produce a nonsingular LU-
factorization of A if and only if det A ≠0.
In fact, the pivoting technique can also
be implemented to reduce errors caused
by
numerical
rounding,
which
occur
whenever the multipliers mjk are small.
The LU-factorization of a matrix is not
unique. When A has a given structure (e.g.,
symmetric), it is possible to ﬁnd a factor-
ization consistent with this structure, see
Table 14.3. Banded matrices are of particu-
lar interest in applications. If the bandwidth
is not very large, the factorization and back-
ward/forward substitution steps for such
matrices can be achieved in a comparatively
small number of operations.

482
14 Numerical Analysis
Table 14.3
Most common factorization methods for the
solution of linear systems. [3, Chapter 3].
Name
Factorization
Description
ﬂops
Comments / pitfalls
LU
A = LU
L is lower triangular and
U is upper triangular
2d3∕3
With pivoting, it applies to
any A ∈ℂd×d
Cholesky
A = LLt
L is lower triangular with
positive diagonal entries
d3∕3
Only for A ∈ℝd×d
hermitian positive deﬁnite
LDL
A = LDLt
L is lower triangular and D is
diagonal
d3∕3
For A ∈ℝd×d general
hermitian
Banded
As above
Factorization of A having
bandwidth b: Ajk = 0
|j −k| > b
2b2d
Gain only for matrices of
small bandwidth
Example 14.4 [Default Matlab and Octave
linear solvers] See [1, Section 5.8]. The
default command for solving linear sys-
tems in the computer languages Matlab and
Octave is the backslash “∖” command. Each
time it is invoked, speciﬁc algorithms are
used, depending on the structure of the
matrix A. These algorithms are all based on
direct methods.
14.3.2
Iteration Methods for Linear Problems
Iterative methods for linear system are
closely
related
with
their
nonlinear
counterpart.
An
iteration
method
for
the solution of a linear system is called
stationary, if for given initial x0,
xn+1 = Bxn + c,
n = 1, …
The iteration matrix B depends on A and
determines the method, and
c = (I −B)A−1b.
Splitting the matrix A = P −R, where P
is usually called a preconditioner and R =
P −A, gives B = I −P−1A and c = P−1b. As
(14.3) is equivalent to the system
Px = Rx + b,
the static method can be written as
P𝛿n = b −Axn
xn+1 = xn + 𝛼𝛿n,
where 𝛼≠0 is a parameter chosen so that
convergence might be improved.
The iteration matrix B should be con-
structed so that each iteration can be com-
puted eﬃciently. In particular, it is neces-
sary that the system associated to the pre-
conditioner is solved quickly. Writing A =
D + E + F where D is the diagonal part of A,
E is lower triangular with zeros on the diag-
onal and F is upper triangular with zeros on
the diagonal, give rise to the standard meth-
ods as described in Table 14.4.
The residual vector satisﬁes
‖en+1‖ ≤𝜌(B)‖en‖,
n = 1, … ,
where 𝜌(B) is the spectral radius: the largest
of the moduli of the eigenvalues of B. Hence
the iteration method will be convergent for
any choice of x0, only if 𝜌(B) < 1. When
0 < w < 1, the SOR method can converge
when Gauss–Seidel does not. When w =
1, the two methods are the same. When

14.3 Finite-Dimensional Linear Systems
483
Table 14.4
The most common iteration stationary methods
for the solution of linear systems [3, Chapter 4].
Name
P, 𝛼
Comments / pitfalls
Jacobi
D, 1
See below.
Gauss–Seidel
D −E, 1
Usually faster than Jacobi.
Successive over relaxation (SOR)
1
wD −E, 1
Only converges in general for 1 < w < 2.
Static Richardson
any, any
Choice of 𝛼can improve convergence.
1 < w < 2, SOR can converge faster than
Gauss–Seidel. If A is a positive deﬁnite her-
mitian matrix, then SOR converges for all
0 < w < 2. If, in addition, A is tridiagonal,
(bandwidth b = 2), then the optimal choice
of w in the SOR method is
w =
2
1 −
√
1 −̃𝜌
,
where ̃𝜌is the spectral radius of the matrix
B
of
the
corresponding
Gauss–Seidel
method. See [6, 7] for details.
Example 14.5 [The PageRank algorithm]
See [8]. The notion of ranking web pages
arises in commercial and noncommercial
online search tools. The computer algo-
rithm known as PageRank developed by
Larry Page from the software company
Google was fairly popular among website
administrators until 2009. A simpliﬁed
model of how this algorithm works is
illustrated as follows. Consider as a small
example the set of eight web pages with the
hyperlinks as shown in Figure 14.1.
Suppose that a random walk moves
through the web network by two rules: 85%
of the time follow one of the hyperlinks
from the page it is on by assigning equal
weight to each and picking one at random
with probability 1∕Hjj, then 15% of the
time skip randomly to another website
anywhere else on the network. Denoting by
xj the probability that the page j is visited,
we have
x = 𝛿CtH−1x
⏟⏞⏟⏞⏟
ﬁrst rule
+ 1 −𝛿
d
e
⏟⏟⏟
second rule
⇐⇒
(I −𝛿CtH−1) x = 1 −𝛿
d
e
here 𝛿= 0.85, C is the connection matrix
associated to the network, H is the corre-
sponding links-out diagonal matrix and e is
the vector with all entries equal to 1.
The Jacobi method is convenient for the
solution of this linear system when d is
very large for two main reasons. On the
one hand, it can be proved to converge:
𝜌(B) = 𝛿in this case. On the other hand,
any of the iterates xn preserves the property
of being a probability vector (nonnegative
entries adding exactly up to 1), if x0 is a
probability vector.
The iteration formula for the stationary
methods can be generalized to
P𝛿n = b −Axn
xn+1 = xn + 𝛼n𝛿n,
where 𝛼n ≠0 is a parameter chosen so that
convergence might be improved. In this
case, the technique is some times called
nonstationary
or
dynamic
Richardson
method. The residual vector
rn = b −Axn

484
14 Numerical Analysis
a
b
e
g
c
d
h
f
C =
0
1
0
0
1
0
1
0
0
0
1
0
0
0
0
0
1
0
0
1
0
0
1
1
0
0
0
0
1
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
1
0
0
1
0
0
0
H =
3
1
4
2
1
1
1
2
Order the pages as (a,b,c,d,e,f,g,h)
Page 3 links to page 1, 4, 7, and 8
Page 4 links to page 5 and 6
Page 8 links to pages 2 and 5
Page 7 links to page 8
Page 6 links to page 5
Page 5 links to page 1
Page 2 links to page 3
Page 1 links to pages 2, 5, and 7
Links-out matrix
Connection matrix
Figure 14.1
Small example (d = 8) of a web network.
clearly measures how close xn is to the
actual solution of the system. As P𝛿n = rn,
𝛿n is called in this context the precondi-
tioned residual. Recalling Example 14.3, if
we pick P = I and
𝛼n =
rt
n ⋅rn
rt
n ⋅Arn
,
we recover the gradient method. Thus the
stationary and nonstationary methods can
be viewed as generalizations of the descent
method for the minimization of quadratic
forms, where the descent direction is taken
to be the preconditioned residual. See [1,
§4.3] and references therein.
As mentioned in Example 14.3, the con-
jugate gradient (CG) method picks 𝛿n in a
slightly more sophisticated fashion. Begin-
ning with initial x0 and r0 = 𝛿0 = b −Ax0,
we set
𝛼n =
rt
n ⋅𝛿n
𝛿t
n ⋅A𝛿n
xn+1 =xn + 𝛼n𝛿n,
𝛽n+1 =
rt
n+1 ⋅𝛿n
𝛿t
n ⋅A𝛿n
𝛿n+1 =rn+1 + 𝛽n+1𝛿n,
n=0, …
If A is hermitian and positive deﬁnite, both
the gradient and the CG method converge
for any initial choice of x0. Replace the sec-
ond row of this iteration formula by
Py
n+1 = rn+1
𝛽n+1 =
yt
n+1 ⋅𝛿n
𝛿t
n ⋅A𝛿n
𝛿n+1 = y
n+1 + 𝛽n+1𝛿n,

14.3 Finite-Dimensional Linear Systems
485
for P hermitian and positive deﬁnite, and
we obtain the preconditioned conjugate gra-
dient (PCG) method.
When all the iterations of the CG or the
PCG method are computed exactly, they
will converge to the true solution of the
system in exactly d steps. Unfortunately, in
practice, the numerical rounding will pre-
vent this from occurring.
14.3.3
Computing Eigenvalues of Finite Matrices
As for the case of the ﬁnite system of lin-
ear equations, the numerical estimation of
the eigenvalues 𝜆and eigenvectors x, asso-
ciated to the eigenvalue problem (14.4) can
also be achieved by means of minimization
methods. The role of the Rayleigh quotient,
R(x) =
xt ⋅Ax
xt ⋅x
in these methods may be illustrated on a
simple case, as follows.
Suppose that A is a 3 × 3 hermitian
matrix with eigenvalues 𝜆1 ≤𝜆2 ≤𝜆3 and
eigenvectors
u1, u2, u3 ∈ℂ3
with
unit
Euclidean norm. These eigenvalues may
be written as extremal problems involving
R(x). Indeed, any vector in ℂ3 expands as
x = ∑xjuj, so
R(x) =
∑𝜆j|xj|2
∑|xj|2 .
Then
𝜆1 = min{R(u)}, 𝜆2 = min{R(u) ∶u ⟂u1},
𝜆3 = min{R(u) ∶u ⟂u1, u2}.
Moreover, 𝜆j are stationary points of the
map R ∶ℝ3 −→ℝ.
Note that, if we employ a steepest descent
method, essentially no prior knowledge or
calculation of the eigenvectors is required
to get 𝜆1. We can, in fact, characterize 𝜆2
also, without explicit information about the
eigenvectors. If ⊂ℂ3 is an arbitrary two-
dimensional space, there always exists a
nonzero vector x ∈such that x ⟂u1. As
R(x) ≥𝜆2, we gather that maxu∈R(u) ≥𝜆2
and
𝜆2 = min
dim =2 max
u∈R(u).
A similar argument shows that
𝜆3 = min
dim =3 max
u∈R(u).
In turns, bounds for the eigenvalues can
be found from R(u) without much prior
information about the eigenvectors.
The above procedure can be extended to
matrices of any size, and in fact to inﬁnite-
dimensional linear operators. The following
result is of fundamental importance and it
is known as the min–max principle. It was
ﬁrst investigated over one hundred years
ago by Lord Rayleigh and Walter Ritz in
connection with acoustics, but in its cur-
rent form is due to Courant and Fischer. See
[9, Section 4.5] or [10, Section XIII.2].
Theorem 14.2 (Min–max principle) Let
A ∈ℂd×d be a hermitian matrix and let its
eigenvalues be ordered nondecreasingly as
𝜆1 ≤· · · ≤𝜆d. Then
𝜆j = min
dim =j max
u∈R(u).
In
particular,
𝜆1 = min R(u)
and
𝜆d = max R(u).
For certain practical problems, knowl-
edge of all the eigenvalues might not be
required and quite often only the largest
or smallest eigenvalue is needed. This is
highly relevant, for example, in the context
of quantum mechanics.

486
14 Numerical Analysis
Example 14.6 [Validation
of
the
Lamb
shift] See [10, Section XIII.2]. The ground
state energy of hydrogen can be found
analytically. The same cannot be said for
helium, even though the ground state
energy of a helium ion can be found exactly
from the hydrogen model. In the early
days of quantum mechanics, Hylleraas
computed an upper approximation of the
helium ground state energy by hand. The
calculation involved ﬁnding the smallest
eigenvalue of a 6 × 6 matrix and it was
regarded as important support for the con-
ﬁrmation of the correctness of quantum
mechanics. With the advent of comput-
ers, ﬁnding eigenvalues of larger matrices
was made possible. Early approximations
involving linear systems of sizes 39 by
Kinoshita and later 1078 by Perkeris, were
crucial in order to test agreement of the
Lamb shift with experimental data. The
min–max principle played a crucial role in
this process.
Assume that A ∈ℝd×d has eigenvalues
such that
|𝜆d| > |𝜆d−1| ≥· · · ≥|𝜆1|.
Note that the largest eigenvalue in modulus
is forced to be simple. The power method
is an iterative procedure to approximate 𝜆d
and ud. For initial nonzero vectors x0 and
y
0 = x0∕‖x0‖, deﬁne
⎧
⎪
⎪
⎨
⎪
⎪⎩
xn+1 = Ay
n
y
n+1 =
xn+1
‖xn+1‖
𝜆d,n+1 = R
(
y
n+1
)
,
n = 0, …
Then the nth iterate vector is such that y
n =
𝛽nAny
0 for suitable scalar 𝛽n. As it turns,
this recursive formula ensures that y
n aligns
with the eigenvector direction ud and
|𝜆d,n −𝜆d| ∼||||
𝜆d−1
𝜆d
||||
n
(generic matrices),
|𝜆d,n −𝜆d| ∼
||||
𝜆d−1
𝜆d
||||
2n
(hermitian matrices)
as n →∞. Therefore 𝜆d,n →𝜆d, whenever
the above conditions on the largest modu-
lus eigenvalue hold true. See [3, Section 5.3]
and [11, pp. 406–407].
A slight modiﬁcation of the power
method allows calculation of the eigen-
value of A that is smallest in modulus,
under the condition 0 < |𝜆1| < |𝜆2| and no
constraints on |𝜆d|. For the inverse power
method, we begin with initial vectors as
before, but then iterate
⎧
⎪
⎪
⎨
⎪
⎪⎩
Axn+1 = y
n
y
n+1 =
xn+1
‖xn+1‖
𝜇d,n = R(y
n+1)
,
n = 0, …
This is nothing more than the old power
method applied to the matrix A−1, whose
eigenvalues
are
𝜇j = 1∕𝜆d−j+1,
and
so
swapped in modulus order with respect to
the eigenvalues of A. From the convergence
of the power method and the above condi-
tion, 𝜇d,n →𝜇d = 𝜆−1
1 , so we have a way of
computing 𝜆1.
For any 𝜇∈ℂnot an eigenvalue of A,
the matrix B = (A −𝜇I) has its eigenvalues
equal to (𝜇−𝜆j)−1. Then, replacing the
matrix A with the matrix (A −𝜇I) in the
inverse shifted power method will allow
computing the eigenvalue of A that is
closest to 𝜇.
This idea can also be applied to reﬁne
the computation of 𝜆d, by changing 𝜇at
each step of the iteration by the better guess
𝜇d,n−1. It is remarkable that, despite the fact

14.4 Approximation of Continuous Data
487
that (A −𝜇d,n) is near singular, this proce-
dure might allow convergence under cer-
tain circumstances. One instance of this,
with a diﬀerent normalization, was already
encountered in Example 14.2.
14.4
Approximation of Continuous Data
We now show how to represent continuous
mathematical quantities by means of a
ﬁnite number of degrees of freedom. In
turns this is of natural importance for
numerical approximation. A fairly com-
plete account on the ideas discussed in this
section can be found in [3, Chapter 8] and
references therein.
14.4.1
Lagrange Interpolation
Let f ∶[𝛼, 𝛽] −→ℝbe a given continuous
function and let
𝛼≤x0 < · · · < xN ≤𝛽
be N + 1 points called interpolation nodes
of the interval [𝛼, 𝛽]. There is a unique
Lagrange
interpolating
polynomial
of
degree N, LNf ∈ℙN, satisfying
LNf (xk) = f (xk),
k = 0, … , N.
Indeed, the family of N characteristic poly-
nomials
𝓁k(x) =
N
∏
j=0
j≠k
x −xj
xk −xj
,
k = 0, … , N,
forms a basis of ℙN, which ensures that we
can write
LNf (x) =
N
∑
k=0
f (xk)𝓁k(x)
in a unique manner. The latter formula is
often referred to as the Lagrange form of the
corresponding interpolating polynomial.
The most economical way of computing
the interpolating polynomial is not the one
given by the Lagrange form. A more neat
procedure is set by the Newton form of
the interpolating polynomial. Set L0f (x) =
f (x0) and
Lkf (x) = Lk−1f (x) + pk(x),
k = 1, … , N,
where Lkf ∈ℙk is the Lagrange interpolat-
ing polynomial for f at the points {xj}k
j=0
and pk ∈ℙk depends on {xj}k
k=0 and only
one unknown coeﬃcient. Then
pk(x) = akwk(x)
for
wk(x) =
k−1
∏
j=0
(x −xj).
Thus
a0 = f (x0),
ak = f (xk) −Lk−1f (xk)
wk(xk)
=
k∑
j=0
f (xj)
w′
k+1(xk).
In turns, we see that we can construct
recursively ak in such a way that
LNf (x) =
N
∑
k=0
akwk(x).
This formula is known as the Newton
divided diﬀerence formula and the coeﬃ-
cients ak are the kth divided diﬀerences.
14.4.2
The Interpolation Error
The interpolation error function
𝜀N(x) = f (x) −LNf (x)

488
14 Numerical Analysis
naturally measures how well the Lagrange
interpolating polynomial approximates the
function f . If f ∈Cr(𝛼, 𝛽), then it can be
shown that [12, 13]
𝜀N(x) = f (N+1)(𝜉)
(N + 1)!wN+1(x)
(14.5)
for some 𝜉∈(𝛼, 𝛽). Note that from the def-
inition, it follows that
𝜀N(x) = aN+1wN+1(x),
where the (N + 1)th divided diﬀerence is
obtained by considering as interpolation
nodes the set {xj}N
j=0 ∪{x}.
We now highlight one remarkable draw-
back of general polynomial interpolation.
Consider a hierarchy of interpolation nodes
of the segment [𝛼, 𝛽],
= {ΞN}∞
N=1,
ΞN = {xN,k}N
k=0.
There always exists a continuous function
f ∶[𝛼, 𝛽] −→ℝ, such that the Lagrange
interpolating polynomial on ΞN does not
capture f uniformly in the full segment
[𝛼, 𝛽] as N →∞. To be precise,
lim
N→∞max
x∈[𝛼,𝛽] |𝜀N(x)| ≠0.
In fact, such a function can some times be
constructed explicitly. See [14].
Example 14.7 [Runge’s phenomenon] Let
f (x) =
1
1 + x2 ,
x ∈[−5, 5].
Suppose that the partition ΞN of the seg-
ment [−5, 5] is made out of equally spaced
nodes. Then
lim
N→∞max
x∈[−5,5] |𝜀N(x)| = ∞.
By contrast, if the partition ΞN of the
segment
[−5, 5]
is
made
out
of
the
Chebyshev—Gauss–Lobatto nodes,
xN,k = 𝛼+ 𝛽
2
+ 𝛽−𝛼
2
̃xk
̃xk = −cos
(
𝜋k
N
)
,
k = 0, … , N,
then
lim
N→∞max
x∈[−5,5] |𝜀N(x)| = 0.
14.4.3
Hermite Interpolation
In the case of Hermite interpolation, rather
than values of the function at the nodes,
higher-order derivatives are matched by the
given polynomial. Suppose that we wish to
match
f (r)(xk)
for
r = 0, … , mk
and
k = 0, … , N.
Let
d = ∑N
k=0(mk + 1).
There
exists
a
unique polynomial Hd−1 ∈ℙd−1 of order
d −1, the Hermite interpolating polyno-
mial, such that
H(r)
d−1(xk) = f (r)(xk)
for
r = 0, … , mk
and
k = 0, … , N.
A concrete formula for the Hermite inter-
polating polynomial is given by
Hd−1(x) =
N
∑
k=0
mk
∑
r=0
f (r)
k (xk)Kkr(x),
where
K(p)
kr (xj) =
{ 1
k = j and r = p
0
otherwise.

14.4 Approximation of Continuous Data
489
Moreover, the error formula for Hermite
interpolation reads [3, Section 8.4]
𝜀N(x) = f (x) −Hd−1(x) = f (d)(𝜉)
d!
𝔥d(x)
𝔥d(x) =
N
∏
k=0
(x −xk)mk+1
(14.6)
for a given 𝜉∈[𝛼, 𝛽].
Example 14.8 [Hermite quadrature formu-
lae] Polynomial interpolation can be used
in order to derive numerical methods for
the approximation of deﬁnite integrals of
the form
I(f ) = ∫
b
a
f (x) dx.
Hermite polynomial interpolation gives
rise to “corrected” versions of classical
formulae for integration, such as the trape-
zoidal rule. Assuming that the 2N + 2
values of f (xk) and f ′(xk) are given at nodes
{xk}N
k=0 of the segment [a, b]. The corre-
sponding Hermite interpolant polynomial
is
H2N+1f (x) =
N
∑
j=0
(f (xj)𝔩j(x) + f ′(xj)𝔪j(x))
for
𝔩j(x)
(
1 −
w′′
N+1(xj)
w′
N+1(xj)(x −xj)
)
𝓁j(x)2
and
𝔪j(x) = (x −xj)𝓁j(x)2
(the deﬁnition of wj and 𝓁j is given in
Section 14.4.1). If we integrate on [a, b], we
derive the quadrature formula
IN(f ) =
N
∑
j=0
(𝛼jf (xj) + 𝛽jf ′(xj)) ,
𝛼j = I(𝔩j)
and
𝛽j = I(𝔪j),
which is an approximation of I(f ). For N =
1, we obtain the corrected trapezoidal for-
mula
I1(f ) = b −a
2
(f (a) + f (b))
+(b −a)2
12
(f ′(a) + f ′(b)) ,
which has higher accuracy than the approx-
imation of I(f ) by the classical trapezium
rule.
14.4.4
Piecewise Polynomial Interpolation
As described in Example 14.7, a possible
way to avoid the Runge phenomenon is by
considering a nonuniform distribution of
interpolation nodes. One other possibility
is to use to piecewise polynomial interpola-
tion. The latter forms the background of the
ﬁnite element method [15, 16].
Let f ∶[a, b] −→ℝbe a continuous func-
tion. The idea behind piecewise polynomial
interpolation is to ﬁx a partition of the seg-
ment [a, b] into subsegments whose interi-
ors are disjoint from one another, and con-
duct polynomial interpolation of a given
ﬁxed order in each one of these subseg-
ments. Let Ξh = {j}N
j=1 be a partition of
[a, b] into N subintervals,
[a, b] =
N
⋃
j=1
j,
j = [xj−1, xj],
whose maximum length is
h = max
1≤k≤N(xj −xj−1).
For q ∈ℕ, let
(q, Ξh) = {w ∈C0(a, b) ∶w|j ∈ℙq(j),
j = 1, … , N}

490
14 Numerical Analysis
be the space of continuous functions on
[a, b] which are polynomials of degree no
larger than q in each j. The map
Πh,q ∶C0(a, b) −→(q, Ξh)
such that Πh,qf |j is the interpolating poly-
nomial (e.g., Lagrange or Hermite) on suit-
able nodes of the subsegments j, is usually
called the piecewise polynomial interpolant
associated to Ξh.
By applying the error formulas (14.5) or
(14.6) in each one of the subsegments of Ξh,
it can be shown that there exists a constant
c(q) > 0 independent of f such that
max
x∈[a,b] |f (x) −Πh,q(x)|
≤c(r)hq+1 max
x∈[a,b] |f (q+1)(x)|
for any f ∈Cq+1(a, b). Moreover, Πh,qf
turns out to approach f also in the mean
square sense. Indeed, denote by L2(a, b)
the complex Hilbert space of all Lebesgue
square integrable functions with the stan-
dard inner product ⟨f , g⟩= ∫b
a f (x)g(x) dx
and norm ‖f ‖ = ⟨f , f ⟩1∕2. For a proof of the
following result in a more general context,
see, for example, [15, Theorem 3.1.6].
Theorem 14.3 Let q ≥1 and assume that
r = 0, … , q + 1. There exists a constant
c(q) > 0 independent of h ensuring that, if
q+1
∑
r=0
‖f (r)‖ < ∞,
then
‖f (r) −(Πh,qf )(r)‖ ≤c(q)hq+1−r‖f (q+1)‖.
Example 14.9 [The Hermite elements of
order three] The Hermite element of order
three are piecewise polynomials of order
q = 3 with prescribed values of f and f ′ at
the interpolation nodes. Let the functions
𝜓(x) = (|x| −1)2(2|x| + 1)
and
𝜔(x) = x(|x| −1)2
be deﬁned on −1 ≤x ≤1, see Figure 14.2.
Then
𝜓, 𝜔∈(3, Ξ1),
Ξ1 = {[−1, 0], [0, 1]}.
Both these functions have continuous
derivatives, if extended by 0 to ℝ, and
𝜓(0) = 1, 𝜓(±1) = 𝜓′(0) = 𝜓′(±1) = 0
𝜔′(0) = 1, 𝜔(±1) = 𝜔(0) = 𝜔′(±1) = 0.
Ψ (x)
ω (x)
0
0
–1
1
–0.5
0.5
0
0
–1
1
–0.5
–0.4
–0.2
0.2
0.4
0.6
0.5
0.2
0.4
0.6
0.8
1
Figure 14.2
Proﬁle of the functions for the Hermite elements of order three in one dimension.

14.5 Initial Value Problems
491
A basis of the linear space (3, Ξh) where
Ξh a uniform partition of a generic inter-
val [a, b] is obtained by translation and dila-
tion. That is
Πh,3f (x) =
N
∑
j=0
(f (xj)𝜙j(x) + f ′(xj)𝜔j(x)) ,
for
𝜙j(x) = 𝜙
(x −xj
h
)
𝜔j(x) = 𝜔
(x −xj
h
)
,
j = 0, … , N.
14.5
Initial Value Problems
We now examine in some detail numerical
solutions of the general Cauchy problem
d
dt y(t) = g(t, y(t)),
t0 < t < T,
y(t0) = y
0,
(14.7)
where g ∶[t0, ∞) × ℝd −→ℝd is a continu-
ous function and T > t0.
Using arguments involving ﬁxed-point
theorems, we know quite general condi-
tions ensuring the existence and unique-
ness of solutions of the above evolution
problem [17]. Indeed, (14.7) will admit a
unique local solution in a suitably small
neighborhood ⊂× of (t0, y
0), if the
function g is Lipschitz continuous in the
space variable,
‖g(t, y
2) −g(t, y
1)‖ ≤L‖y
2 −y
1‖,
for all t ∈
and
y
1, y
2 ∈. (14.8)
Moreover, it will admit a unique global solu-
tion, if the constant L can be chosen uni-
formly for = ℝ.
We will be mostly concerned with the
case d = 1 which in itself possesses all the
features of the higher-dimensional case. See
[3, Chapter 11] and references therein.
14.5.1
One-Step Methods
In order to establish numerical procedures
for the solution of the Cauchy problem, we
begin by partitioning the segment [t0, t0 +
T] into a number N of subsegments of step
size h > 0. That is
[t0, t0 + T] =
N−1
⋃
n=0
[tn, tn+1],
where the nodes tn are given by t0 + nh. Let
yn = y(tn).
We seek for approximations un of the true
solution value yn at the nodes. We set
gn = g(tn, un).
For the one-step methods, the approxi-
mation un+1 at the (n + 1)th node depends
solely on un. In their general formulation,
these methods can be written as
un+1 = un + hΦ(tn, un, gn; h)
u0 = y0,
n = 0, … , N −1,
where Φ is often referred to as the incre-
mental function.
By writing
yn+1 = yn + hΦh(tn, yn, g(tn, yn)) + h𝜏n+1(h),
n = 0, … , N −1,
where 𝜏n(h) is a correction term, we deﬁne
the local truncation error,
̃𝜀n(h) = |𝜏n(h)|,

492
14 Numerical Analysis
at the node tn. If the incremental function
satisﬁes the condition
lim
h→0 Φh(tn, yn, g(tn, yn)) = g(tn, yn),
by expanding y in Taylor series at that node,
we can show that [3, Section 11.3]
lim
h→0 ̃𝜀n(h) = 0,
n = 0, … , N −1.
In fact, it can also be shown that the global
truncation error
̃𝜀(h) =
max
n=0,…,N−1 |̃𝜀n(h)|
does satisfy
lim
h→0 ̃𝜀(h) = 0
under the above condition, ensuring the
consistency of the one-step method. The
order of a consistent method is the largest
power q ∈ℕ, such that
lim
h→0
̃𝜀(h)
hq
< ∞.
In Table 14.5, we show common choices
of incremental functions alongside the cor-
responding order of approximation. The
method is explicit, if un+1 can be com-
puted directly from uj for j ≤n, otherwise
it is called implicit. An iteration formula in
the case of the implicit methods should be
derived for each individual Cauchy prob-
lem.
14.5.2
Multistep Methods
In the case of the multistep methods the
approximated solution at the (n + 1)th
node is computed by means of a formula
involving un, … , un+1−q. The smallest value
of q for which this is possible is the number
of steps of the method.
Example 14.10 [the midpoint method] The
midpoint rule for diﬀerentiation gives rise
to the midpoint method, in which
un+1 = un−1 + 2hgn,
n = 1, … , N −1.
Here u0 = y0 and u1 is to be determined.
This is an example of an explicit multistep
method comprising two steps.
An implicit two-step method is the fol-
lowing.
Example 14.11 [the
Simpson’s
method]
The Simpson’s rule for integration gives
rise to the Simpson’s method. The iteration
formula reads
un+1 = un−1 + h
3
(gn−1 + 4gn + gn+1
) ,
n = 1, … , N −1.
The linear multistep methods represent
an important class of procedures for the
eﬃcient solution of the Cauchy problem.
Table 14.5
Simple one-step methods for the numerical solution of the Cauchy problem.
Name
Φh(tn, un, gn)
Order
Type
Forward Euler
gn
1
Explicit
Backward Euler
gn+1
1
Implicit
Crank–Nicholson
gn+gn+1
2
2
Implicit
Heun
gn+g(tn+1,un+hgn)
2
2
Explicit

14.5 Initial Value Problems
493
See [18]. Set real coeﬃcients a0, … , aq−1
and b−1, … , bq−1, such that aq−1bq−1 ≠0.
Set the iteration formula
un+1 =
q−1
∑
k=0
akun−k + h
q−1
∑
k=−1
bkgn−k,
n = 1, … , N −1.
(14.9)
The parameter q is the numbers of steps
of the method and the coeﬃcients (ak, bk)
fully characterize it. If b−1 = 0, the method
is explicit, otherwise it is implicit.
The local truncation error for multistep
methods can be deﬁned from the recursion
h̃𝜀n+1(h) = yn+1 −
q−1
∑
k=0
akyn−k −h
q−1
∑
k=−1
bky′
n−j,
where y′
j = d
dt y(tj); that is, h̃𝜀n+1(h) is the
error generated at the node tn+1, if we sub-
stitute the analytical solution of the initial-
value problem into the scheme (14.9). The
deﬁnition of consistency and order of the
method mentioned above are also available
for multistep methods.
By choosing a0 = 1 and all other aj = 0
we obtain the Adams methods. Integrating
(14.7), yields
y(t) −y0 = ∫
t
t0
g(s, y(s)) ds.
If we approximate this integral on the
segment [tn, tn+1] by the integral of the
interpolating polynomial of g on q distinct
nodes, particular Adams methods can be
derived which, by construction, turn out
to be consistent. Fixing as interpolation
nodes tn, … , tn−q+1, gives the Adams–
Bashforth methods, which are explicit,
as they always lead to b−1 = 0. Fixing as
interpolation nodes tn+1, … , tn−q+2, gives
the Adams–Moulton methods, which are
implicit.
For
q
steps,
the
Adams–Bashforth
method can be shown to be of order q.
Example 14.12 [Adams–Bashforth meth-
ods]
Examples
of
Adams–Bashforth
methods include the following.
1. For q = 1, we recover the forward
Euler method.
2. For q = 2, the interpolating polyno-
mial is
P(t) = gn + (t −tn)gn−1 −gn
tn−1 −tn
.
Thus
∫
tn+1
tn
P(s) ds = h
2(3gn −gn−1).
Hence the iteration formula of the
Adams–Bashforth method with two
steps requires
(b0, b1) =
(3
2, −1
2
)
.
3. For q = 3, we get in a similar fashion
an iteration formula for the Adams–
Bashforth method with three steps,
requiring
(b0, b1, b2) =
(23
12, −4
3, 5
12
)
.
4. For q = 4, the iteration formula
requires
(b0, b1, b2, b3) =
(55
24, −59
24, 37
24, −3
8
)
.
The Adams–Moulton methods can be
derived similarly. Table 14.6 shows the cor-
responding coeﬃcients for small q.
For q + 1 steps, the Adams–Moulton
methods can be proved to be of order
q + 1. This statement is a consequence

494
14 Numerical Analysis
Table 14.6
Adams–Moulton methods.
q
(b−1, … , bq−1)
0
1
1
(
1
2, 1
2
)
2
(
5
12 , 2
3, −1
12
)
3
(
3
8 , 19
24 , −5
24 , 1
24
)
4
(
251
720 , 323
360 , −11
30 , 53
360 , −19
720
)
of the following remarkable result [3,
Theorem 11.3].
Theorem 14.4 The multistep method with
iteration formula (14.9) is consistent if, and
only if,
q−1
∑
k=0
ak = 1
q−1
∑
k=0
bk −
q−1
∑
k=−1
kak = 1.
Moreover, if the solution y ∈Cr+1(t0, T),
then the method is of order r if, and only if,
additionally
q−1
∑
k=0
(−k)jak + j
q−1
∑
k=−1
(−k)j−1bk = 1,
j = 2, … , r.
14.5.3
Runge–Kutta Methods
An alternative approach to multistep meth-
ods, is the one adopted by the Runge–Kutta
methods. These are one-step methods that
involve several evaluations of g(t, y) on the
subsegment [tn, tn+1]. Their iteration for-
mula reads
un+1 = un + h
s∑
k=1
bkRk,
n = 1, … , N −1,
where
Rk = g
(
tn + ckh, un + h
s∑
k=1
ajkRk
)
,
j = 1, … , s.
Here s is called the number of stages of
the method. See [19] and the references
therein. The parameters completely deter-
mine the method with the condition cj =
∑s
k=1 ajk and are represented neatly by the
Butcher array
c
A
bt
for
A = [ajk]s
jk=1
b = (bk)s
k=1
and
c = (ck)s
k=1.
A Runge–Kutta method is explicit if, and
only if, the matrix A is lower triangular.
Implicit methods are quite diﬃcult to
derive in general, because the computation
of Rk involves solving a nonlinear system of
size s.
Example 14.13 [Some Runge–Kutta meth-
ods with a small number of stages] The
Butcher arrays
1
1
1
1
2
1
2
1
0
0
0
1
1
2
1
2
1
2
1
2
correspond,
respectively,
to
the
back-
ward Euler, the implicit midpoint, and the
Crank–Nicolson methods.
It is possible to construct implicit or
explicit Runge–Kutta methods of any large
order. One of the most widely used Runge–
Kutta method is the one with four stages

14.5 Initial Value Problems
495
determined by the Butcher array
0
1
2
1
2
1
2
0
1
2
1
0
0
1
1
6
1
3
1
3
1
6
It can be derived in a similar fashion as
for the Adams methods. It is explicit and
convergent to the fourth order with respect
to h. An implicit method with two stages
that is also convergent to fourth order is
given by the array
3−
√
3
6
1
4
3−2
√
3
12
3+
√
3
6
3+2
√
3
12
1
4
1
2
1
2
We have exclusively dealt above with the
case of the scalar Cauchy problem. When
confronted with the case d > 1, two pos-
sible approaches are available. We either
apply to each individual equation one of the
scalar methods, or we write the iteration
formula for the method in vector form.
Example 14.14 [Airy’s equation] The Airy
functions have a wide range of applications
in mathematical physics. The solution of
the Airy equation that generates the Airy
functions,
d2
dt2 u(t) = tu(t),
t > 0,
u(0) = 1,
d
dt u(0) = 0,
can be achieved by reducing this equation
to the system
d
dt y(t) = g(t, y(t)),
t > 0
y(0) =
[1
0
]
|||||||||
y(t) =
[
u(t)
d
dt u(t)
]
g(t, y) =
[ y2
ty1
]
.
Even though analytical methods can be
applied to the study of Airy functions,
numerical methods as described above
are better suited for the estimation of
numerical values of the Airy functions.
14.5.4
Stability and Global Stability
A scheme on a given ﬁxed bounded seg-
ment will certainly be regarded as stable,
if small perturbations of the data render
bounded perturbations of the numerical
solution in the regime h →0. This idea can
be made more precise as follows. Assume
that vn is an approximated solution pro-
duced by the numerical method in ques-
tion, applied to a perturbation of the orig-
inal problem, and that 𝜎n is the size of this
perturbation at the step n. The numerical
method will be called zero-stable, if for ﬁxed
T > t0, there exists h0, CT, 𝜀0 > 0 such that
for all 0 < h ≤h0 and 0 < 𝜀≤𝜀0,
|𝜎n| ≤𝜀
implies
|un −vn| ≤CT𝜀
for all
n = 0, … , max{n ∶tn ≤t0 + T}.
Example 14.15 [Consistency implies zero-
stability for one-step methods] If g satis-
ﬁes (14.8), a consistent one-step method for
the solution of the corresponding Cauchy
problem will be zero-stable. Moreover, in
this case, the constant CT is proportional to
e(T−t0)L.
Let
𝔭(r) = rq −
q−1
∑
k=0
akrq−1−k
= (r −r0) · · · (r −rq−1)
be the characteristic polynomial associated
to the coeﬃcients of the multistep method

496
14 Numerical Analysis
(14.9). The latter will be zero-stable, if and
only if all |rk| ≤1 and those with |rk| =
1 are all simple roots. The latter is often
called the root condition of the method. See
Table 14.7.
Theorem 14.5 (Lax–Ritchmyer) A num-
erical method for the solution of the Cauchy
problem that is consistent, will also be con-
vergent if and only if it is zero-stable.
A complete proof of this result can be
found in [3, Section 11.6.3].
Another notion of stability is concerned
with the behavior of the scheme, when the
corresponding Cauchy problem is posed on
an inﬁnite segment. For global stability, the
methods are tested against each other by
means of the reference equation
dy
dt = 𝜆y,
t > 0,
y(0) = 1
for all given 𝜆∈ℂ. This reference equation
has as exact solution y(t) = e𝜆t. The region
of absolute stability of an iterative scheme
is the subset of the complex plane deﬁned
as
= {h𝜆∈ℂ∶|un| →0
tn →∞}.
That is, the region of values of the product
h𝜆, such that the numerical scheme leads to
a solution decaying at inﬁnity.
Example 14.16 [Absolute stability of the
forward Euler method] The iteration for-
mula of the forward Euler method for the
solution of the reference equation is
un+1 = h𝜆un,
u0 = 1,
n = 0, …
Induction allows ﬁnding the exact solution
to this recursion,
un = (1 + h𝜆)n,
n = 0, …
For inclusion in , we therefore require in
this case that |1 + h𝜆| < 1. Hence,
=
{
Re(h𝜆)<0
and
0<h<−2 Re(𝜆)
|𝜆|2
}
,
which is a circle of radius 1 centered at
(−1, 0).
For a general multistep method, let
𝔮(r) =
q−1
∑
k=−1
bkrq−1−k.
The boundary of the region of absolute sta-
bility is characterized by the identity [3,
Section 11.6.4]
𝜕= {h𝜆∈ℂ∶𝔮(r)h𝜆
= 𝔭(r)
for some
|r| = 1}.
14.6
Spectral Problems
In order to illustrate the numerical solution
to inﬁnite-dimensional spectral problems,
we
consider
the
reference
eigenvalue
equation
corresponding
to
the
one-
dimensional
Schrödinger
Hamiltonian.
Given a real-valued continuous potential
V ∶(a, b) −→ℝ, the aim is to determine
Table 14.7
Regions of absolute convergence
for the most common one-step methods.
Method

Forward Euler
{|1 + z| < 1}
Backward Euler
ℂ⧵{|1 −z| ≤1}
Crank–Nicolson
{Re z < 0}
Heun
{|1 + z + z2
2 | < 1}

14.6 Spectral Problems
497
approximately the solution of the equation
−d2
dx2 y(x) + V(x)y(x) = 𝜆y(x),
a < x < b,
y(a) = y(b) = 0.
(14.10)
Here a < b and either one of them or
both can be inﬁnite. By a solution of
(14.10), we mean an eigenvalue 𝜆∈ℝand
a corresponding nonzero eigenfunction
y ∈L2(a, b).
This simple equation possesses all the
features of the more complicated multipar-
ticle settings.
14.6.1
The Inﬁnite-Dimensional min–max
Principle
The left hand side of (14.10) gives rise to a
Hamiltonian,
H ∶D(H) −→L2(a, b),
which is a self-adjoint operator deﬁned on
the dense domain D(H) ⊂L2(a, b), depen-
dent on the potential. Formally,
H = −d2
dx2 + V.
The eigenvalue equation is equivalent to the
weak equation
𝔥(y, w) = 𝜆⟨y, w⟩,
for all
w ∈D(𝔥),
where
the
energy
functional
𝔥∶
D(𝔥) × D(𝔥) −→ℂis given by
𝔥(y, w) = ⟨Hy, w⟩
=∫
b
a
(
y′(x)w′(x) + V(x)y(x)w(x)
)
dx.
Here D(𝔥) ⊂L2(a, b) is another suitable
dense domain containing D(H) that renders
the form closed.
By mimicking the ﬁnite-dimensional set-
ting, let
𝜆j(H) = min
⊂D(𝔥)
dim =j
max
w∈R(w),
j = 1, … ,
where the Rayleigh quotient
R(w) = 𝔥(w, w)
⟨w, w⟩.
Then we have an analogous version of
Theorem 14.2, [9, Section 4.5] or [10,
Section XIII.2].
Theorem 14.6 (Min–max principle)
for (14.10) Assume that the potential V is
such that R(w) ≥c for all w ∈D(𝔥). Then
b ≤𝜆j(H) ≤e(H) are the eigenvalues of
(14.10) lying below
e(H) = lim
j→∞𝜆j(H).
Moreover, the corresponding nonzero eigen-
functions yj ∈D(𝔥) satisfy
𝜆j(H) =
min
w⟂{yk}j−1
k=1
R(w) = R(yj).
By construction, 𝜆j(H) are nondecreasing
in j. The limit e(H) > b might or might not
be inﬁnity and it is the bottom of the essen-
tial spectrum of H.
Numerical methods for computing upper
bounds for the 𝜆j(H) can then be derived
by means of the Galerkin method. Pick a
linear subspace ⊂D(𝔥) of dimension d
with a linearly independent basis {bj}d
j=1.
Assemble the matrices
K =[𝔥(bj, bk)]d
jk=1
and
M=[⟨bj, bk⟩]d
jk=1.

498
14 Numerical Analysis
Then the eigenvalues 𝜇j(H, ) of the
ﬁnite-dimensional linear eigenvalue prob-
lem
Ku = 𝜆Mu
are such that 𝜇j(H, ) ≥𝜆j(H).
14.6.2
Systems Conﬁned to a Box
Diﬀerent methods arise by constructing
families of subspaces in diﬀerent ways.
We illustrate the standard way of imple-
menting the Galerkin method on an artiﬁ-
cial example, where all quantities involved
can be found analytically.
Example 14.17 [The free particle in a box]
Let V(x) = 0, a = 0 and b > 0. The solution
of (14.10) is given explicitly by
𝜆j(H) = j2𝜋2
b2 ,
yj(x) = sin
(j𝜋
b x
)
.
In this case, D(𝔥) = H1
0(0, b), the Sobolev
space of all absolutely continuous functions
vanishing at 0 and b.
Assume that is the space of Lagrange
ﬁnite elements of order 1. A basis for 
is given by the piecewise linear continuous
functions
bj(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
x −xj−1
xj −xj−1
xj−1 ≤x ≤xj
xj+1 −x
xj+1 −xj
xj ≤x ≤xj+1
0
otherwise
j = 1, … , N −1,
where h = b∕N and xj = jh. Note that b0
and bN are not considered here, as the trial
functions in should satisfy the Dirichlet
boundary conditions.
We can ﬁnd the eigenvalues 𝜇j(H, )
explicitly. Let
̃M =
⎡
⎢
⎢
⎢
⎢
⎢⎣
4
1
1
4
1
1
⋱
⋱
⋱
4
1
1
4
⎤
⎥
⎥
⎥
⎥
⎥⎦
and
̃K =
⎡
⎢
⎢
⎢
⎢
⎢⎣
2
−1
−1
2
−1
−1
⋱
⋱
⋱
2
−1
−1
2
⎤
⎥
⎥
⎥
⎥
⎥⎦
.
Then K = (1∕h) ̃K and M = (h∕6) ̃M. The
eigenvalues of the Toeplitz matrix
T =
⎡
⎢
⎢
⎢
⎢
⎢⎣
0
1
1
0
1
1
⋱
⋱
⋱
0
1
1
0
⎤
⎥
⎥
⎥
⎥
⎥⎦
are 𝜏j = 2 cos(j𝜋∕N), j = 1, … , N −1. As
the commutation relation ̃M ̃K = ̃K ̃M holds
true,
𝜇j(H, ) =
6N2(2 −𝜏j)
b2(4 + 𝜏j) .
In the explicit calculation made in the
previous example, 𝜇j(H, ) ↓𝜆j(H) and
lim
N→∞
𝜇j(H, ) −𝜆j(H)
N2
= j4𝜋4
12b2 ,
j = 1, … , N −1.
In other words, the approximated eigenval-
ues miss the exact eigenvalues by a factor
proportional to h2.
Let Ξh be an equidistant partition of the
segment [a, b] into N subintervals
j = [xj−1, xj]

Further Reading
499
of length h = b −a∕N. Let
(q, Ξh) = {w ∈C1(a, b) ∶w|j ∈ℙq(j), 1
≤j ≤N, w(a) = w(b) = 0}
be the space of piecewise diﬀerentiable
functions
on
Ξ
that
are
polynomials
of degree q in each j and satisfy the
boundary conditions of (14.10). See [16,
Theorem 6.1].
Theorem 14.7 Let a and b be ﬁxed and
both ﬁnite. Assume that the potential V is
continuous. There exists c(q) > 0, such that
𝜆j(H) ≤𝜇j(H, h(q, Ξ)) ≤𝜆j(H)
+ c(q)h2q𝜆j(H)q+1
for all h suﬃciently small.
14.6.3
The Case of Unconﬁned Systems
When the segment (a, b) is of inﬁnite
length, Theorem 14.6 is still holds as long
as is contained in D(𝔥). One possible
approach is to assume that is made out
of Gaussian-type functions.
Another possible approach is to observe
that,
generally,
the
eigenfunctions
of
(14.10) are exponentially decaying at ±∞
whenever the potential is suﬃciently reg-
ular. Let L > 0 and denote by 𝜆j(H, L)
the eigenvalues of (14.10) for a = −L and
b = L.
Theorem 14.8 Let j ∈ℕbe ﬁxed and
assume that the potential V is continuous.
There exist constants cj > 0 and aj > 0 both
independent of L, such that
𝜆j(H) < 𝜆j(H, L) < 𝜆j(H) + cje−ajL.
A proof of this result follows from the
known exponential decay of the eigen-
functions of one-dimensional Schrödinger
operators, see, for example, [20, Chapter
3.3] and references therein.
Further Reading
For a more comprehensive survey on
the subjects of numerical solutions of
nonlinear systems and numerical opti-
mization, see [3, Chapter 7]. In particular,
we have ignored completely the subject
of constrained optimization and Lagrange
multiplier techniques [21–23].
A substantial body of research has been
devoted to the sensitivity analysis of the
solution of linear systems by direct meth-
ods and the eﬀect of rounding errors. For
details on this, see [11, 24, 25]. The PCG
method is a particular realization of the
more general Krylov methods that can be
applied to nonhermitian matrices [6, 26,
27]. Of particular importance are the full
orthogonalization method (FOM) and the
generalized minimum residual (GMRES)
method, which converge extremely fast for
the right class of matrices.
Computation of all the eigenvalues of a
matrix via QR factorization or singular-
value decomposition is a well-studied
subject [11, 28]. Much research has also
been conducted on the more general
matrix polynomials, in particular, their
factorization properties and the corre-
sponding generalized eigenvalue problem
[29]. The remarkable theory of computa-
tion with matrix functions has also been
recently examined [30].
Reference on topics such as approxima-
tion theory by orthogonal polynomials can
be found in [3, Chapter 10] and references
therein. Classical references on the ﬁnite
element method are [15, 16, 31]. A survey
on the theory of curves and surface ﬁtting
can be found in [32].

500
14 Numerical Analysis
An important class of methods for the
solution of Cauchy problems that we
have not mentioned here is the class of
predictor–corrector methods [3, Section
11.7]. For further reading on the numerical
solution of PDEs both evolutionary and
stationary, see [33, 34] and references
therein.
A fairly complete account on the theory
of eigenvalue computation for diﬀerential
equations including estimation of a priori
and a posteriori complementary bounds for
eigenvalues can be found in [35]. Compu-
tation of eigenvalues in gaps of the essen-
tial spectrum is a much harder problem; see
[36] and references therein.
References
1. Quarteroni, A., Saleri, F., and Gervasio, P.
(2010) Texts in Computational Science and
Engineering, Scientiﬁc Computing with
MATLAB and Octave, vol. 2,
Springer-Verlag, Berlin.
2. Ridgway Scott, L. (2011) Numerical Analysis,
Princeton University Press, Princeton, NJ.
3. Quarteroni, A., Sacco, R., and Saleri, F.
(2007) Numerical Mathematics, Texts in
Applied Mathematics, vol. 37, 2nd edn,
Springer-Verlag, Berlin.
4. Ortega, J.M. and Rheinboldt, W.C. (2000)
Iterative Solution of Nonlinear Equations in
Several Variables, Classics in Applied
Mathematics, vol. 30, Society for Industrial
and Applied Mathematics (SIAM),
Philadelphia, PA.
5. Dennis, J.E. Jr., and Schnabel, R.B. (1996)
Numerical Methods for Unconstrained
Optimization and Nonlinear Equations,
Classics in Applied Mathematics, vol. 16,
Society for Industrial and Applied
Mathematics (SIAM), Philadelphia, PA.
6. Hackbusch, W. (1994) Iterative Solution of
Large Sparse Systems of Equations, Applied
Mathematical Sciences, vol. 95,
Springer-Verlag, New York.
7. Young, D.M. (2003) Iterative Solution of
Large Linear Systems, Dover Publications,
Inc., Mineola, NY.
8. Langville, A.N. and Meyer, C.D. (2012)
Google’s PageRank and Beyond: The Science
of Search Engine Rankings, Princeton
University Press, Princeton, NJ.
9. Davies, E.B. (1995) Spectral Theory and
Diﬀerential Operators, Cambridge Studies in
Advanced Mathematics, vol. 42, Cambridge
University Press, Cambridge.
10. Reed, M. and Simon, B. (1978) Methods of
Modern Mathematical Physics. IV. Analysis
of Operators, Academic Press, New York.
11. Golub, G.H. and Van Loan, C.F. (2013)
Matrix Computations. Johns Hopkins Studies
in the Mathematical Sciences. Johns Hopkins,
4th edn, University Press, Baltimore, MD.
12. Wendroﬀ, B. (1966) Theoretical Numerical
Analysis, Academic Press, New York.
13. Davis, P.J. (1975) Interpolation and
Approximation, Dover Publications, Inc.,
New York.
14. Erd˝os, P. (1961) Problems and results on the
theory of interpolation. II. Acta Math. Acad.
Sci. Hungar., 12, 235–244.
15. Ciarlet, P.G. (2002) The Finite Element
Method for Elliptic Problems, Classics in
Applied Mathematics, vol. 40, Society for
Industrial and Applied Mathematics (SIAM),
Philadelphia, PA.
16. Strang, G. and Fix, G. (2008) An Analysis of
the Finite Element Method, 2nd edn,
Wellesley-Cambridge Press, Wellesley, MA.
17. Coddington, E.A. and Levinson, N. (1955)
Theory of Ordinary Diﬀerential Equations,
McGraw-Hill Book Company, London.
18. Lambert, J.D. (1991) Numerical Methods for
Ordinary Diﬀerential Systems, John Wiley &
Sons, Ltd., Chichester, The initial value
problem.
19. Butcher, J.C. and Wanner, G. (1996)
Runge-Kutta methods: some historical notes.
Appl. Numer. Math., 22(1-3), 113–151.
20. Simon, B. (1982) Schrödinger semigroups.
Bull. Amer. Math. Soc. (N.S.), 7(3), 447–526.
21. Avriel, M. (2003) Nonlinear Programming,
Dover Publications, Inc., Mineola, NY.
22. Bertsekas, D.P. (1982) Constrained
optimization and Lagrange multiplier
methods, in Computer Science and Applied
Mathematics, Academic Press, Inc., New
York.
23. Canon, M.D., Cullum, C.D. Jr., and Polak, E.
(1970) Theory of Optimal Control and
Mathematical Programming, McGraw-Hill
Book Co., New York.

References
501
24. Datta, B.N. (2010) Numerical Linear Algebra
and Applications, 2nd edn, Society for
Industrial and Applied Mathematics (SIAM),
Philadelphia, PA.
25. Stewart, G.W. (1973) Introduction to Matrix
Computations, Academic Press, New York,
London.
26. Axelsson, O. (1994) Iterative Solution
Methods, Cambridge University Press,
Cambridge.
27. Saad, Y. (2003) Iterative Methods for Sparse
Linear Systems, 2nd edn, Society for
Industrial and Applied Mathematics,
Philadelphia, PA.
28. Wilkinson, J.H. (1988) The Algebraic
Eigenvalue Problem, Monographs on
Numerical Analysis, The Clarendon Press -
Oxford University Press, New York.
29. Gohberg, I., Lancaster, P., and Rodman, L.
(1982) Matrix Polynomials, Academic Press,
Inc., New York.
30. Higham, N.J. (2008) Functions of Matrices,
Society for Industrial and Applied
Mathematics (SIAM), Philadelphia, PA.
31. Brenner, S.C. and Ridgway Scott, L. (2008)
The Mathematical Theory of Finite Element
Methods, Texts in Applied Mathematics, vol.
15, 3rd edn, Springer, New York.
32. Lancaster, P. and Šalkauskas, K. (1986) Curve
and Surface Fitting, Academic Press, Inc.,
London.
33. Quarteroni, A. and Valli, A. (1994)
Numerical Approximation of Partial
Diﬀerential Equations, Springer Series in
Computational Mathematics, vol. 23,
Springer-Verlag, Berlin.
34. Ern, A. and Guermond, J.-L. (2004) Theory
and Practice of Finite Elements, Applied
Mathematical Sciences, vol. 159,
Springer-Verlag, New York.
35. Weinberger, H.F. (1974) Variational
Methods for Eigenvalue Approximation,
Society for Industrial and Applied
Mathematics, Philadelphia, PA.
36. Boulton, L., Boussaïd, N., and Lewin, M.
(2012) Generalised Weyl theorems and
spectral pollution in the Galerkin method.
J. Spectr. Theory, 2(4), 329–354.


503
15
Mathematical Transformations
Des McGhee, Rainer Picard, Sascha Trostorﬀ, and Marcus Waurick
15.1
What are Transformations and Why are
They Useful?
A transformation1) is a process that turns
objects in one area into objects of a diﬀer-
ent area in such a way that no information
is lost and the original can be reconstructed
without loss. In mathematical terms, this
could be described as an invertible map-
ping from one topological space onto
another such that both the mapping and its
inverse are continuous. Transformations
1) There is a linguistic subtlety regarding the stan-
dard mathematical terminology. Transformation
is a process and the result of the transformation
applied to a particular object is its transform.
In the literature, these terms are almost always
used as synonyms and we shall not attempt to
rectify this situation. The device performing
the transformation is a transformer or transfor-
mator. We will not use the term “transformer”
as it might lead to confusion with other areas
of life, see, for example, [1]. The term “trans-
formator” is rarely used anyway, so we shall
follow the standard practice and use “transfor-
mation” to describe both process and device
(compare map/mapping, operator/operation or
projector/projection for similar terminological
diﬃculties).
are advantageous if terms, concepts, and
ideas are considered to be more elementary
or intuitive in one area and less elementary
or intuitive in the other. A simple example
is the use of diﬀerent maps (i.e., diﬀerent
projections) of the Earth’s surface. An
angle-preserving map is useful for ocean
navigation, whereas an area-preserving
map is useful for comparing the size of
diﬀerent islands.
Another more mathematical example is
the principal axis transformation, which
brings the equation of an ellipse into a stan-
dard (canonical) form where, for example,
the axes sizes, eccentricity, and foci are eas-
ily read oﬀor deduced from formula. This
amounts to the diagonalization of a real
(2 × 2)−matrix A
A = U∗DU,
(15.1)
where U is an orthogonal (real) (2 × 2) −
matrix, i.e.
U−1 = U∗.
Here U∗is simply the transpose matrix
to U.
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

504
15 Mathematical Transformations
Physicists should be familiar with the
ubiquitous Laplace and Fourier transfor-
mations (including the sine and cosine
transformations). Probably the most famil-
iar application of these transformations
is to convert “hard” diﬀerential equations
into “easier” algebraic equations. Once
the latter are solved, the solutions of the
original diﬀerential equations are obtained
by applying the inverse transformation.
The choice of transformation is deter-
mined by the diﬀerential operator involved
including
consideration
of
its
domain
determined by initial or boundary con-
ditions. The Laplace transformation L
deﬁned by
(Lf )(s) = ∫ℝ+
e−stf (t) dt
is appropriate for initial-value problems on
the half-line [0, ∞[. Thus, for some suit-
able range of values of s, the function t →
e−stf (t) must be integrable over [0, ∞[, a
property that will contribute to the deter-
mination of an appropriate space of func-
tions in which the problem can be con-
sidered. The salient property is that the
Laplace transformation converts diﬀeren-
tiation into multiplication by the variable
subject to the function value at t = 0 being
known, the easily derived formula being
(Lf ′)(s) = s(Lf )(s) −f (0)
(derived using integration by parts and
assuming that e−stf (t) →0 as t →∞). This
generalizes to higher derivatives as
(Lf (n))(s) = sn(Lf )(s) −
n−1
∑
r=0
sn−1−rf (r)(0).
The Fourier transformation F deﬁned by2)
2) There are various, essentially equivalent, deﬁ-
nitions of the Fourier Transformation. We shall
discuss some issues later.
(Ff )(x) =
1
√
2𝜋∫ℝ
exp (−ixy) f (y) dy
(15.2)
is often appropriate for boundary-value
problems
on
ℝ
where
solutions
are
required to decay to zero at ±∞. Once
again, the requirement for the above inte-
gral to exist contributes to the deﬁnition
of appropriate spaces in which to consider
problems. Clearly, the integral exists for
any function that is integrable on ℝ, that
is, for all L1(ℝ)-functions, because for all
x, y ∈ℝwe have | exp (−ixy) | = 1. The
Fourier transformation can also be deﬁned
on all of L2(ℝ) where it is traditionally
referred to as the Fourier–Plancherel trans-
formation (see [2] and Section 15.4). For
the Fourier transformation, the result for
diﬀerentiation of an n−times continuously
diﬀerentiable function is
(Ff (n))(x) = (ix)n(Ff )(x),
that is, diﬀerentiation is transformed into
multiplication by i times the variable.
The Fourier sine and cosine transforma-
tions are deﬁned respectively by3)
(̃Fsin f )(x) =
√
2
𝜋∫ℝ+
sin(xy) f (y) dy,
(̃Fcos f )(x) =
√
2
𝜋∫ℝ+
cos(xy) f (y) dy
for suitable functions f on [0, ∞[ such that
the integrals exist. The diﬀerentiation for-
mulae for twice continuously diﬀerentiable
function f on [0, ∞[ with f (x) →0 as x
tends to inﬁnity are
(̃Fsin f ′′)(x) = −x2(̃Fsin f )(x) +
√
2
𝜋xf (0),
(̃Fcos f ′′)(x) = −x2(̃Fcos f )(x) −
√
2
𝜋f ′(0),
3) The rationale for the ∼notation here is
explained in Section 15.6.

15.1 What are Transformations and Why are They Useful?
505
making these transformations useful typ-
ically for the canonical second-order par-
tial diﬀerential equations (Laplace, wave,
and diﬀusion equations) on a space domain
[0, ∞[ where, respectively, the function or
its derivative is given as boundary data at
zero (along with decay to zero at ∞).
Such applications make clear the need for
a transformation to be a continuous one-to-
one mapping with a continuous inverse. If
not one-to-one, then the inverse transfor-
mation would not exist and uniqueness of
the inverse transform would be lost: more
than one function purporting to solve the
diﬀerential equation would correspond to
any solution of the algebraic equation. If the
mapping or its inverse were not continu-
ous, then any small error (rounding error in
calculation or measurement error in exper-
iment) would result in large, uncontrollable
errors in transforms or inverse transforms,
respectively.
We shall consider these transformations
and others in some mathematical gener-
ality in the following sections. We shall
not however rehearse standard applica-
tions of mathematical transformations to
typical problems of mathematical physics
because there is no shortage of appropriate
sources of such material. Instead, our aim
is threefold:
1. to shed light on why mathematical
transformations work by clarifying
what governs the choice of
transformation that may be appropriate
to a particular problem;
2. to show that various transformations
widely used in physics and engineering
are closely related; and
3. to introduce suﬃcient mathematical
rigor so that the application of
mathematical transformations to
entities such as the Dirac delta and its
derivatives that are so useful in
modeling point sources and impulses,
are fully understood.
We set the scene by considering an
appropriate setting in which to develop a
deeper understanding of the role and use-
fulness of mathematical transformations.
We focus attention on linear mappings
between Hilbert spaces H0 and H1. A
transformation is a linear mapping
T ∶H0 →H1
such that
T−1 ∶H1 →H0
exists (i.e., T is a bijection, a one-to-one
and onto map) and both T and T−1 are
continuous, that is, T ∈(H0, H1
) and
T−1 ∈(H1, H0
) (see [2]). In other words,
T is a linear homeomorphism. Of partic-
ular interest are transformations T that
preserve the Hilbert space structure, that is,
which are unitary. Unitary transformations
are isometric, that is, norm preserving,
which means that for all x ∈H0
|Tx|H1 = |x|H0.
A useful ﬁrst example is given by coor-
dinate transformations. Let 𝜙∶S0 →S1
be a smooth bijection between open sets
S0, S1 = Ran (𝜙) in ℝn, n ∈{1, 2, 3, …}
such that the determinant det (𝜕𝜙) of the
Jacobian 𝜕𝜙is bounded and has a bounded
inverse. Such a coordinate transformation
induces a unitary transformation between
the
spaces
L2 (Sk
) ≔L2 (Sk, d𝜆, ℂ)
of
(equivalence classes of) complex-valued,
square-integrable functions on ℝn vanish-
ing outside Sk, k ∈{0, 1}. (Here 𝜆denotes
the standard Lebesgue measure.) Indeed,
for all f , g ∈L2 (S1
) , the substitution
formula for integrals yields

506
15 Mathematical Transformations
⟨f |g⟩L2(S1)
= ∫S1
f g d𝜆
= ∫S1
f (y) g (y) dy
= ∫S0
f (𝜙(x)) g (𝜙(x)) |det ((𝜕𝜙) (x))| dx.
The latter can be re-written as
∫S0
√
|det ((𝜕𝜙) (x))|f (𝜙(x))
√
|det ((𝜕𝜙) (x))|g (𝜙(x)) dx
or
⟨√
|det ((𝜕𝜙) (m))|f∘𝜙|
√
|det ((𝜕𝜙) (m))|g∘𝜙
⟩
L2(S0) ,
where
√
|det ((𝜕𝜙) (m))| is a suggestive
notation for the multiplication operator
mapping f
to x →
√
|det ((𝜕𝜙) (x))|f (x) ,
that is, T√
|det((𝜕𝜙))| in the notation of [2].
This shows that
L2 (S1
)
→
L2 (S0
)
f
→
√
|det ((𝜕𝜙) (m))| f ∘𝜙
(15.3)
is an isometry and, in fact, it is also onto and
therefore is a unitary transformation.
We shall focus mainly on some partic-
ular unitary transformations obtained by
continuous extension from integral expres-
sions, because they are particularly relevant
in applications to physics [3–7].
15.2
The Fourier Series Transformations
15.2.1
The Abstract Fourier Series
The ﬁrst example is the so-called abstract
Fourier expansion giving rise to the abstract
Fourier series transformation. It may come
as a surprise to some that Fourier series
expansions are essentially transformations
as deﬁned above.
Let o be an orthonormal set in a Hilbert
space H. Then
x =
∑
e∈o
⟨e|x⟩H e
(15.4)
for all x in the linear envelope (linear span,
linear hull) of o, which is the smallest linear
subspace of H containing o. The orthonor-
mal set o is called total or complete (or
an orthonormal basis) if the linear enve-
lope of o is dense in H, [2]. In case of an
orthonormal basis, we have a series expan-
sion of the form (15.4) for every x ∈H. The
series expansion holds independently of
any ordering of o, which is why we chose
the summation notation in (15.4). For a
separable Hilbert space, an orthonormal
basis is ﬁnite or countably inﬁnite and
so there is a parametric “enumeration”
(e𝛼
)
𝛼∈S of o = {e𝛼|𝛼∈S} and we may
write ∑
𝛼∈S ⟨e𝛼|x⟩H e𝛼. For S = {0, … , N},
S = ℕor S = ℤ, the respective notations
∑N
𝛼=0 ⟨e𝛼|x⟩H e𝛼,
∑∞
𝛼=0 ⟨e𝛼|x⟩H e𝛼
and
∑∞
𝛼=−∞⟨e𝛼|x⟩H e𝛼
are
commonly
used.
The associated unitary transformation (as
stated by Parseval’s equality, [2]) is the
abstract Fourier series transformation
H →𝓁2 (o)≔
{
f ∶o →ℂ
|||||
∑
e∈o
|f (e)|2 < ∞
}
,
x →
(
⟨e|x⟩H
)
e∈o ,
which associates with each element x ∈H
its sequence of so-called Fourier coeﬃ-
cients. The inverse of this transformation
is the reconstruction of the original x ∈H
from its (abstract) Fourier coeﬃcients
𝓁2 (o) →H,
(𝜙e
)
e∈o →
∑
e∈o
𝜙e e.

15.2 The Fourier Series Transformations
507
15.2.2
The Classical Fourier Series
Of course, the French baron Jean-Baptiste-
Joseph Fourier (March 21, 1768 – May
16, 1830) never envisioned such a gen-
eral concept of orthogonal expansion.
He had a very speciﬁc orthonormal basis
in mind. In today’s language, he was
concerned with the orthonormal basis
{
1∕
√
2𝜋exp (ik ⋅) |k ∈ℤ
}
in the Hilbert
space
L2 (] −𝜋, 𝜋]) = L2 (] −𝜋, 𝜋], d𝜆, ℂ).
Equivalently, we could have no scalar fac-
tor in front of the exponential and have
{exp (ik ⋅) |k ∈ℤ} as an orthonormal basis
in
L2 (] −𝜋, 𝜋], 1∕2𝜋d𝜆, ℂ) .
We
shall,
however, stay with the ﬁrst option.
The classical Fourier expansion gives rise
to the transformation, that is, a unitary
mapping,
F# ∶L2 (] −𝜋, 𝜋]) →𝓁2 (ℤ) ,
f →
(
̂f (k)
)
k∈ℤ,
where the Fourier coeﬃcients ̂f (k) are given
by
̂f (k) =
1
√
2𝜋∫]−𝜋,𝜋]
exp (−iky) f (y) dy,
for each k ∈ℤand the inverse transforma-
tion, F−1
#
= F∗
# is given by
F−1
#
∶𝓁2 (ℤ) →L2 (] −𝜋, 𝜋]) ,
(
̂f (k)
)
k∈ℤ→f ,
where
f (x) =
1
√
2𝜋
∞
∑
k=−∞
̂f (k) exp (ikx)
for almost every x ∈] −𝜋, 𝜋]. Thus, for all
f ∈L2 (] −𝜋, 𝜋]),
f (x) = 1
2𝜋
∞
∑
k=−∞∫]−𝜋,𝜋]
exp (−iky) f (y) dy exp (ikx)
for almost all x ∈] −𝜋, 𝜋].
A unitary equivalence of an operator
A to a multiplication-by-argument oper-
ator in some suitable function space is
called a spectral representation of A.4)
The main point of interest is that F# is a
spectral representation associated with
diﬀerentiation.
Indeed, if 𝜕# denotes the diﬀerentiation
operator in L2 (] −𝜋, 𝜋]) with domain
deﬁned by periodic boundary conditions
then
𝜕# = F∗
#im F#
showing that applying 𝜕# in L2 (] −𝜋, 𝜋]) is
unitarily equivalent to i times the multi-
plication by the argument m ∶
(
ak
)
k∈ℤ→
(k ak
)
k∈ℤin 𝓁2 (ℤ). In other words, F# is a
spectral representation for 1
i 𝜕#. This prop-
erty is used to turn the problem of solving
a linear diﬀerential equation with constant
coeﬃcients ak ∈ℂ, k ∈{0, … , N}, N ∈ℕ,
p (𝜕#
) u ≔
N
∑
k=0
ak𝜕k
#u = f
under
a
periodic
boundary
condition
on [−𝜋, 𝜋] into the question of calcu-
lating F∗
#
(
(1∕p (ik)) ̂f (k)
)
k∈ℤ, which is
always possible if the polynomial function
x →p (ix) has no integer zeros.
The
rescaling
(h > 0)
or
rescaling
with reﬂection (h < 0) of ] −𝜋, 𝜋] to
] −|h| 𝜋, |h| 𝜋], h ∈ℝ⧵{0} , is the coordi-
nate transformation
4) In as much as multiplication by a diagonal
matrix can be interpreted as multiplication-
by-argument operator if the argument ranges
over the eigenvalues of the matrix A, (15.1)
states that the unitary matrix U is a spectral
representation of A.

508
15 Mathematical Transformations
ℝ→ℝ
t →ht
inducing by (15.3) a unitary rescaling trans-
formation 𝜎h given by
𝜎h ∶L2 (ℝ) →L2 (ℝ) ,
(15.5)
f →
√
|h|f (h ⋅)
with 𝜎1∕h = 𝜎∗
h as inverse. This provides
a neat way of obtaining Fourier series
expansion results for functions on diﬀerent
length intervals. For L2 (] −|h| 𝜋, |h| 𝜋]),
the unitary Fourier series transformation is
then5)
F#𝜎h ∶L2 (] −|h| 𝜋, |h| 𝜋]) →𝓁2 (ℤ) ,
f →
(
̂
𝜎hf (k)
)
k∈ℤ
with
̂
𝜎hf (k)
=
⟨
1
√
2𝜋
exp (ik ⋅) |𝜎hf
⟩
L2(]−𝜋,𝜋]),
=
⟨
𝜎∗
h
1
√
2𝜋
exp (ik ⋅) |f
⟩
L2(]−|h|𝜋,|h|𝜋]),
=
⟨
1
√
2𝜋|h|
exp (ik ⋅∕h) |f
⟩
L2(]−|h|𝜋,|h|𝜋]).
The set {1∕
√
2𝜋|h| exp (ik ⋅∕h) |k ∈ℤ}
is
an
orthonormal
basis
in
L2 (] −|h| 𝜋, |h| 𝜋]), because unitary trans-
formations map orthonormal bases to
orthonormal bases. Conversely,
f (x) =
1
2𝜋|h|
∑
k∈ℤ
𝛼k exp
(
ikx
h
)
for almost all, where
𝛼k =
⟨
exp
(
ik
h ⋅
)
| f
⟩
L2(]−|h|𝜋,|h|𝜋])
for all k ∈ℤ" at the end of the sentence.
5) Compositions of unitary mappings are again
unitary.
It may be interesting to note that by
choosing h = N+1
𝜋, N ∈ℕ, and evaluating
pointwise at integer values from −N to N +
1, we have that
⎧
⎪
⎨
⎪⎩
(
1
√
2 (N + 1)
exp
(
𝜋isk
N + 1
))
s∈{−N,…,N+1}
||||||
k ∈{−N, … , N + 1}
⎫
⎪
⎬
⎪⎭
is
a
complete
orthonormal
set
in
ℂ2(N+1). This follows from the fact that
{exp (𝜋is∕(N + 1)) |s ∈{−N, … , N + 1}}
is the set of roots of unity, that is, the
solution set of the equation
z2(N+1) = 1.
(15.6)
For more details we refer to [8].
Thus we obtain the so-called discrete
Fourier transformation
(
1
√
2 (N+1)
exp
(
𝜋i (jk)
(N + 1)
))
k,j∈{−N,…,N+1}
as a matrix deﬁning a unitary map in
ℂ2(N+1).
Noting that
1
√
2 (N + 1)
N+1
∑
s=−N
exp
(
−𝜋iks
(N + 1)
)
fs
(15.7)
may serve as a crude approximation of the
kth Fourier coeﬃcient of 𝜎(N+1)∕𝜋f associ-
ated with the step function
f =
N+1
∑
s=−N
fs𝜒
](k−1),k] ∈L2 (] −(N+1) , (N+1)]) ,
where fs ∈ℂfor s ∈{−N, … , N + 1}, we
see that the discrete Fourier transforma-
tion can be utilized for numerical purposes.
Noting that (15.7) is actually a polynomial
evaluated at exp (−𝜋ik∕(N + 1)) ∈ℂ, for
numerical purposes one may apply eﬃcient
polynomial evaluation strategies, such as
Horner’s scheme (or rule), to obtain what

15.3 The z-Transformation
509
is known as the fast Fourier transformation
(FFT) [9].
15.2.3
The Fourier Series Transformation in
L2 (Sℂ(0, 1))
By the complex coordinate transformation
] −𝜋, 𝜋] →Sℂ(0, 1)
t →exp (it)
of the interval ] −𝜋, 𝜋] onto the unit circle
Sℂ(0, 1) in ℂcentered at the origin, the
Fourier series transformation may also be
considered as acting on L2 (Sℂ(0, 1)
). This
induces the unitary transformation
Π1 ∶L2 (Sℂ(0, 1)
) →L2 (] −𝜋, 𝜋])
𝜓→Π1𝜓,
where
(Π1𝜓)
(t) ≔𝜓(exp (it)) .
This is easily seen to be an isometry. Indeed,
noting that for the line element ds(z) on
Sℂ(0, 1) = {exp (it) | t ∈] −𝜋, 𝜋]}, we have
ds (exp (it)) = dt,
and it follows that
∫Sℂ(0,1)
|𝜓(z)|2 ds (z) = ∫
𝜋
−𝜋
|𝜓(exp (it))|2 dt
= ∫
𝜋
−𝜋
|||
(Π1𝜓)
(t)|||
2
dt.
By this transformation, it follows that
(z →1∕
√
2𝜋zk)k∈ℤis an orthonormal basis
for L2(Sℂ(0, 1)) and we have the unitary
mapping
F# Π1 ∶L2 (Sℂ(0, 1)
)
→𝓁2 (ℤ) ,
𝜓→(F# (𝜓(exp (i ⋅))) (k)
)
k∈ℤ,
as the corresponding Fourier series trans-
formation. Thus for all 𝜓∈L2 (Sℂ(0, 1)
) ,
𝜓(z) =
1
√
2𝜋
∞
∑
k=−∞
̂𝜓(k) zk
for almost all z ∈Sℂ(0, 1) where
̂𝜓(k) =
1
√
2𝜋∫Sℂ(0,1)
z−k𝜓(z) ds (z) .
This result is simply a restatement of the
classical Fourier series transformation for
periodic functions because such functions
may be understood as functions deﬁned on
the unit circle.
15.3
The z-Transformation
Rather than taking the unit circle, we
may repeat the discussion of the previous
section on a circle Sℂ(0, r) ⊆ℂof radius
r ∈ℝ>0 centered at the origin. Thus, we
consider the transformation
] −𝜋, 𝜋] →Sℂ(0, r)
t →r exp (it)
of the interval ] −𝜋, 𝜋] onto the circle
Sℂ(0, r). The induced unitary transforma-
tion is now
Πr ∶L2 (Sℂ(0, r)
) →L2 (] −𝜋, 𝜋])
𝜓→Πr𝜓,
where
(Πr𝜓)
(t) ≔
√
r𝜓(r exp (it)) .
This time, the line element ds(z) on the
circle Sℂ(0, r) = {r exp(it) | t ∈] −𝜋, 𝜋]} is

510
15 Mathematical Transformations
ds (r exp (it)) = r dt,
and therefore
∫Sℂ(0,r)
|𝜓(z)|2 ds(z) =∫
𝜋
−𝜋
|𝜓(r exp(it))|2 r dt
= ∫
𝜋
−𝜋
||Πr𝜓(t)||
2 dt
showing
that
Πr ∶L2(Sℂ(0, r)) →
L2(] −𝜋, 𝜋]) is indeed an isometry. It
follows
that
(
z →1∕
√
2𝜋r(z∕r)k)
k∈ℤ
forms an orthonormal basis of L2(Sℂ(0, r))
and we have the unitary mapping F# Πr,
that is,
F# Πr ∶L2 (Sℂ(0, r)
) →𝓁2 (ℤ) ,
𝜓→(̃𝜓(k))
k∈ℤ,
with
̃𝜓(k) ≔(F#(
√
r𝜓(r exp(i ⋅))))(k)
for
k ∈ℤas the corresponding Fourier series
transformation. The inverse transforma-
tion, that is, the reconstruction of 𝜓, is
given by
𝜓(z) =
1
√
2𝜋r
∑
k∈ℤ
r−k ̃𝜓(k) zk
for
almost
all
z ∈Sℂ(0, r).
This
lat-
ter formula gives rise to the so-called
z-transformation. Consider an exponen-
tially weighted 𝓁2-type space: r−m[𝓁2(ℤ)]
≔{(ak)k∈ℤ| ∑
k∈ℤ|ak|2r2k < ∞}.
For
r ∈ℝ>0, we deﬁne
Zr ∶r−m [𝓁2 (ℤ)
] →L2 (Sℂ(0, r)
)
(
ak
)
k∈ℤ→
1
√
2𝜋r
∑
k∈ℤ
ak zk.
Thus
we
have
a
family
of
z-
transformations6)
parameterized
by
6) The z-transformation is usually considered with-
out the normalizing factor 1∕
√
2𝜋r. This factor
is needed to produce a unitary z-transformation.
r ∈ℝ>0. The image of (ak
)
k∈ℤunder
such a z−transformation is frequently
referred to as its generating function. In
applications, it is often the case that we
have ak = 0 for k < 0.
Depending on the sequence (ak
)
k∈ℤit
may happen that its generating function
Φ ≔Zr
((ak
)
k∈ℤ
) has an analytic exten-
sion
to
an
annulus
Rℂ
(0, r1, r2
) ≔
Bℂ
(0, r2
) ⧵Bℂ
(0, r1
),
r2 > r1 > 0.
Then
methods from the theory of analytic func-
tions are available for further consideration
of the properties of the sequence (ak
)
k∈ℤ
and
for
comparison
with
other
such
sequences in overlapping annuli. Typical
applications are to diﬀerence equations,
which recursively deﬁne such sequences
(ak
)
k∈ℤ(usually with ak = 0 for k < 0).
The sequence can then be identiﬁed as
the coeﬃcients in the Laurent expansion
of an analytic function Φ (its generating
function).
15.4
The Fourier–Laplace Transformation
The Fourier–Laplace transformation is a
generalization of the Fourier–Plancherel
transformation
F ∶L2 (ℝ) →L2 (ℝ)
de-
ﬁned by (15.2) where the integral over ℝis
understood as the L2-limit of the integral
over [−n, n], the so-called “limit in the
mean”, see [2]. Given 𝜚∈ℝ, deﬁne
H𝜚,0 (ℝ) ≔L2 (ℝ, exp (−2𝜚m) d𝜆, ℂ)
where the inner product of H𝜚,0 (ℝ) is
(f , g) →∫ℝ
f (t) g (t) exp (−2𝜚t) dt.
The Fourier–Laplace transformation is a
family of unitary maps
L𝜚≔𝐹exp(−𝜚m) ∶H𝜚,0(ℝ) →L2(ℝ),

15.4 The Fourier–Laplace Transformation
511
for 𝜌∈ℝthat is,
(L𝜚f )
(x)
=
1
√
2𝜋∫ℝ
exp (−ixy) exp (−𝜚y) f (y) dy
=
1
√
2𝜋∫ℝ
exp (−(ix + 𝜚) y) f (y) dy
=
1
√
2𝜋∫ℝ
exp (−i (x −i𝜚) y) f (y) dy
≕̂f (x −i𝜚) ,
where
̂f = Ff ,
the
Fourier–Plancherel
transform of f .
It is clear that exp (−𝜚m) deﬁned by
(exp (−𝜚m) 𝜓) (x) = exp (−𝜚x) 𝜓(x)
for
almost
all
x ∈ℝ,
is
a
unitary
map-
ping of H𝜚,0 (ℝ) onto L2 (ℝ) and so the
Fourier–Laplace transformation is unitary
because it is the composition of two uni-
tary maps. A Fourier–Laplace transform is
frequently interpreted as an element of the
form ̂f (−i ⋅) in L2 (i [ℝ] + 𝜚) given by
i [ℝ] + 𝜚→ℂ
p →
1
√
2𝜋∫ℝ
exp (−py) f (y) dy
= ̂f (−ip) .
Here L2 (i [ℝ] + 𝜚) is a Hilbert space with
inner product
(f , g) →∫i[ℝ]+𝜚
f (z) g (z) ds (z)
induced by the complex transformation
x →ix + 𝜚
yielding
ds (ix + 𝜚) = dx.
The importance of the Fourier–Laplace
transformation lies in the property that it
is a spectral representation associated with
the operator
𝜕𝜚≔exp (𝜚m) (𝜕+ 𝜚) exp (−𝜚m)
representing diﬀerentiation in H𝜚,0 (ℝ),
where 𝜕represents ordinary diﬀerentiation
in L2(ℝ). Indeed, we have the unitary
equivalence7)
𝜕𝜚= L∗
𝜚(im + 𝜚) L𝜚.
(15.8)
Note that H0,0 (ℝ) = L2 (ℝ) and L0 = F,
the
Fourier–Plancherel
transformation.
Observing
that
L2 (] −𝜋, 𝜋])
may
be
considered as a subspace of L2 (ℝ) of
(equivalence classes of) functions van-
ishing (almost everywhere) outside of
] −𝜋, 𝜋], we have for the Fourier series
transformation F# discussed above that
F#f = ((Ff )
(k)
)
k∈ℤ
= ((L0f )
(k)
)
k∈ℤ
for
all
f ∈L2 (] −𝜋, 𝜋]) ⊆L2 (ℝ) .
As
already noted for the Fourier transfor-
mation and in the context of the Fourier
series transformation, there are also var-
ious, essentially equivalent, versions of
the Fourier–Laplace transformation in
use and in applying “standard results”
from tables, care needs to be taken to
ensure that formulae are correct for the
version of the transformation in use. The
Fourier–Laplace transformation variant8)
̃L𝜚≔𝜎2𝜋L𝜚
(15.9)
7) This could be rephrased as saying that L𝜚is
a spectral representation for (1∕i) (𝜕𝜚−𝜚) =
(1∕i)𝜕𝜚+ i𝜚.
8) In general, the Fourier–Laplace transformation
interacts with rescaling, (15.5), in the following
way
𝜎LL𝜚= L𝜚∕L𝜎1∕L, L ∈ℝ⧵{0} ;
that is, smaller scale is turned into larger scale
and vice versa.

512
15 Mathematical Transformations
has the advantage that neither ̃L𝜚nor its
inverse ̃L−1
𝜚
= ̃L∗
𝜚have a numerical factor in
front of the integral expression deﬁning it.
This results in simpliﬁcations of various for-
mulae. In contrast, ̃L𝜚yields a more compli-
cated unitary equivalence to diﬀerentiation
of the form
𝜕𝜚= ̃L∗
𝜚(2𝜋im + 𝜌)̃L𝜚,
which may not be desirable. The case 𝜚= 0
leads to a variant of the Fourier transforma-
tion, ≔̃L0 given by
(f )(x) = ∫ℝ
exp (−i2𝜋xy) f (y) dy,
which is popular in particular areas of
physics and electrical engineering. We
shall, however, maintain the use of the
Fourier–Laplace transformation version
L𝜚deﬁned above and the special case of
the
Fourier–Plancherel
transformation
F = L0.
15.4.1
Convolutions as Functions of 𝝏𝝔
15.4.1.1
Functions of 𝝏𝝔
A spectral representation allows us to
straightforwardly deﬁne functions of an
operator. Generalizing (15.8), we have for
any polynomial P
P (𝜕𝜚
) = L∗
𝜚P (im + 𝜚) L𝜚.
This suggests the deﬁnition of more general
functions of 𝜕𝜚as
S(𝜕𝜚) ≔L∗
𝜚S(i m + 𝜚)L𝜚,
(15.10)
where S can be fairly general (and even
matrix-valued, in which case the transfor-
mation is carried out entry by entry). In
the case 𝜚= 0, functions of 𝜕are known as
ﬁlters in the context of signal processing.
Owing to the unitary equivalence between
𝜕𝜚and 𝜕+ 𝜚via the unitary transformation
exp (−𝜚m) ∶H𝜚,0 (ℝ) →L2 (ℝ), we obtain
the unitary equivalence
S
(
𝜕𝜚
) = exp (𝜚m) S (𝜕+ 𝜚) exp (−𝜚m) ,
where
S (𝜕+ 𝜚) = F∗S(i m + 𝜚) F.
(15.11)
This indicates that it would suﬃce to con-
sider only the Fourier–Plancherel transfor-
mation, that is, the case 𝜚= 0, taking the
exponential weight exp (−𝜚⋅) into account
separately.
The concept of a function of 𝜕𝜚is
extremely powerful because many mod-
els discussed in so-called linear systems
theory; see [10, 11], can be understood as
such a function S(𝜕𝜚) acting on an input u
resulting in an output S(𝜕𝜚)u. Character-
istic properties of such systems are that
their input–output relations are linear and
translation invariant, that is, sums, multi-
ples, and shifts of input result in the same
sums, multiples, and (up to scaling) shifts
of output. Indeed, translation 𝜏h deﬁned
by
𝜏hf ≔f ( ⋅+ h)
is itself a function of 𝜕𝜚in the above sense.
Indeed, for suitable f we may calculate
exp(h (ix + 𝜚)) (L𝜌f )(x)
=
1
√
2𝜋∫ℝ
exp (−(ix + 𝜚)
(y −h)) f (y) dy
=
1
√
2𝜋∫ℝ
exp (−(ix + 𝜚) s) f (s + h) ds
= (L𝜚𝜏hf )
(x)
showing that
𝜏h = L−1
𝜚exp(h (im + 𝜚)) L𝜚= exp(h𝜕𝜚).

15.4 The Fourier–Laplace Transformation
513
It is interesting to note that by substitut-
ing the series expansion for the exponen-
tial function, this formula (if applied to a
suitable function f ) is easily recognized as
nothing but the Taylor expansion of f about
a point x.
As another example of functions of 𝜕𝜚,
we consider fractional calculus [12]. For 𝜚∈
ℝ>0 we obtain 𝜕−1
𝜚
as forward causal inte-
gration. Indeed, for integrable functions f
in H𝜚,0 (ℝ), we have
(
𝜕−1
𝜚f
)
(t) = ∫
t
−∞
f (s) ds, t ∈ℝ.
(15.12)
Fractional powers of 𝜕−1
𝜚
can now be
deﬁned by
𝜕−𝛼
𝜚
≔L∗
𝜚(im + 𝜚)−𝛼L𝜚, 𝛼∈[0, 1[ ,
and9)
𝜕−𝛽
𝜚
≔𝜕−⌊𝛽⌋
𝜚
𝜕⌊𝛽⌋−𝛽
𝜚
, 𝛽∈ℝ.
(15.13)
Note that 𝛽−⌊𝛽⌋∈[0, 1[ and z →z𝛼for
𝛼∈] −1, 0[ is the principle root function,
that is, with z = |z| exp (i arg (z)
) , arg (z) ∈
] −𝜋, 𝜋], we have
z𝛼≔|z|𝛼exp (i 𝛼arg (z)
) .
For positive 𝛽, (15.13) is the fractional inte-
gral and, for negative 𝛽, equation (15.13)
deﬁnes fractional diﬀerentiation. Here, 𝜚∈
ℝ>0 is important because for 𝜚∈ℝ<0, we
obtain the backward causal integral, and for
𝜚= 0, the inverse of diﬀerentiation does not
exist; there are the candidates correspond-
ing to the forward or the backward causal
9) Here ⌊𝛽⌋denotes the ﬂoor function evaluated
at 𝛽∈ℝ, which is the largest integer less than
or equal to 𝛽, that is, ⌊𝛽⌋≔sup {k ∈ℤ| k ≤𝛽} .
Note that −⌊𝛽⌋= ⌈−𝛽⌉for 𝛽∈ℝ,
⌈𝛾⌉≔inf {k ∈ℤ| 𝛾≤k} for 𝛾∈ℝ.
situation both of which are unbounded lin-
ear operators.
In other cases, or following the observa-
tion (15.11), the choice 𝜚= 0 is of particular
interest. An example of a function of 𝜕(i.e.,
in the case 𝜚= 0) is the so-called Hilbert
transformation
H ≔F∗1
i sgn (m) F,
= 1
i sgn
(1
i 𝜕
)
,
= i sgn (i𝜕) .
We see that H is a unitary mapping in L2 (ℝ)
because (1∕i) sgn (m) is a multiplication
operator on L2 (ℝ) with ||(1∕i)sgn (m)|| = 1
and
H2 = −1.
Consequently,
H∗= H−1 = −H
and so H is skew-selfadjoint and unitary in
L2 (ℝ).
15.4.1.2
Convolutions
Assuming
that
S
allows
for
a
Fourier–Laplace
transformation
by
L𝜚
and
(
̂S
) (
(1∕i)𝜕𝜚
) is a function of 𝜕𝜚in the
above sense, we deﬁne
S ∗g ≔
√
2𝜋
(
̂S
) (1
i 𝜕𝜚
)
g,
(15.14)
which is well deﬁned for any g in the
domain of
√
2𝜋
(
̂S
) ((1∕i)𝜕𝜚
). In many
cases, S ∗g is –at least for “well-behaved”
functions g ∈H𝜚,0 (ℝ) –actually given as
an integral expression, which is then a
so-called convolution integral:

514
15 Mathematical Transformations
(S ∗g)(x) = ∫
∞
−∞
S(x −y) g(y) dy
(15.15)
(hence the name convolution operator for
S ∗≔
√
2𝜋
(
̂S
) ((1∕i)𝜕𝜚
)). In accordance
with (15.14), for such convolution integrals,
the so-called convolution theorem holds
L𝜚(S ∗g) =
√
2𝜋
(
L𝜚S
)
(m)
(
L𝜚g
)
=
√
2𝜋L𝜚S ⋅L𝜚g.
(15.16)
By the above deﬁnition, this always holds,
even if there is no actual integral expression
involved.
To illustrate this, let us ﬁrst consider the
fractional integral. For 𝜚∈ℝ>0, we obtain
𝜕−𝛼
𝜚f = Φ ∗f ,
where
Φ (t) ≔
1
Γ (𝛼)𝜒
ℝ>0 (t) t𝛼−1.
For suﬃciently well-behaved functions f ∈
H𝜚,0 (ℝ) (so that the integral exists), this
convolution (in the general sense) can actu-
ally be written as a convolution integral10)
(
𝜕−𝛼
𝜚f
)
(t)
=
1
Γ (𝛼) ∫ℝ
𝜒ℝ>0 (t −s) (t −s)𝛼−1 f (s) ds
=
1
Γ (𝛼) ∫
t
−∞
(t −s)𝛼−1 f (s) ds, t ∈ℝ.
In particular, the case 𝛼= 1∕2 is referred to
as the Abel transformation which generates
a unitary transformation. Indeed, the map-
ping
10) Frequently, there is an implicit assumption that
f vanishes for negative arguments in which
case we get the convolution integral expression
1
Γ (𝛼) 𝜒
ℝ>0 (t) ∫
t
0
(t −s)𝛼−1 f (s) ds, t ∈ℝ.
𝜕−1∕2
𝜚
∶H𝜚,0 (ℝ) →H𝜚,0 (ℝ)
is clearly an isometry if the ﬁrst space
H𝜚,0 (ℝ) is equipped with the inner product
(f , g) →
⟨
𝜕−1∕2
𝜚
f |𝜕−1∕2
𝜚
g
⟩
𝜚,0 .
Denoting the smallest Hilbert space with
this inner product containing H𝜚,0 (ℝ)
by H𝜚,−1∕2 (ℝ), the Abel transformation
A𝜚∶H𝜚,−1∕2 (ℝ) →H𝜚,0 (ℝ), deﬁned as the
continuous extension of 𝜕−1∕2
𝜚
, is unitary.
As A𝜚is essentially a fractional integral, it
follows that A∗
𝜚= A−1
𝜚is related to the frac-
tional derivative 𝜕1∕2
𝜚
= 𝜕𝜚𝜕−1∕2
𝜚
. The con-
tinuous extension of 𝜕1∕2
𝜚
to a mapping from
H𝜚,0 (ℝ) to H𝜚,−1∕2 (ℝ) corresponds to A∗
𝜚.
The integral representation of A𝜚f , when
f = 0 on ℝ<0 and is suitably well behaved,
is
(A𝜚f )
(t)
=
√
1
𝜋𝜒ℝ>0 (t)∫
t
0
(t −s)−1∕2 f (s) ds, t ∈ℝ.
Letting r−2 = t and substituting s = w−2
yields
r−1 (A𝜚f
) (
r−2)
=2
√
1
𝜋r−1
∫
∞
r
(
r−2−w−2)−1∕2f (w−2)
w−3 dw
=2
√
1
𝜋∫
∞
r
(w2−r2)−1∕2(f (w−2) w−3) w dw.
The function
r →2
√
1
𝜋∫
∞
r
(w2 −r2)−1∕2 g (w) w dw
is frequently referred to as the Abel
transform
of
g.
Introducing
suitably
weighted spaces this can (by the unitar-
ity of A𝜚) also be realized as a unitary
transformation.

15.5 The Fourier–Laplace Transformation and Distributions
515
15.4.2
The Fourier–Plancherel Transformation
Focusing on the case 𝜚= 0, we may
consider the Fourier–Plancherel transfor-
mation as a unitary mapping in L2 (ℝ) in
its own right. Every unitary operator has
its spectrum on the unit circle Sℂ(0, 1) .
The spectrum of the Fourier–Plancherel
transformation
L0 = F
is
particularly
simple. It is pure point spectrum and
consists of the four points in {1, i, −1, −i}.
The corresponding orthonormal basis of
eigensolutions is given by11)
(
2−k∕2
√
k!
(m −𝜕)k 𝛾
)
k∈ℕ
,
where 𝛾is the Gaussian distribution func-
tion deﬁned by 𝛾(x) = 𝜋−1∕4 exp (−x2∕2) ,
normalized such that |𝛾|L2(ℝ) = 1. These
have the form
2−k∕2
√
k!
(m −𝜕)k 𝛾= Pk (m) 𝛾,
where Pk
is, up to a renormalization
constant,
the
Hermite
polynomial
of
degree k, k ∈ℕ. Thus, the spectral rep-
resentation takes on the simple form
discussed in general in Section 15.2.1 and
the Fourier–Plancherel transform ̂f = Ff
of f ∈L2 (ℝ) takes the form
∞
∑
k=0
eik𝜋∕2
⟨
(m −𝜕)k
2kk!
𝛾
|||||
f
⟩
L2(ℝ)
(m −𝜕)k 𝛾.
11) The adjoint of the operator 1∕
√
2 (m −𝜕) is
1∕
√
2 (m + 𝜕) and their product
1
2 (m + 𝜕) (m −𝜕) = 1
2
((m2 −𝜕2) + 1) ,
which is the (quantum-mechanical) harmonic
oscillator, has the same eigensolutions as the
Fourier–Plancherel transformation with associ-
ated point spectrum ℕ.
Recalling that a spectral representation
of an operator allows the deﬁnition of func-
tions of the operator, we may deﬁne func-
tions of the Fourier–Plancherel transfor-
mation F. A function S deﬁned on the
spectrum {1, i, −1, −i} then gives rise to an
operator S (F) given by
S (F) f
=
∞
∑
k=0
S(eik𝜋∕2)
⟨
(m −𝜕)k
2kk!
𝛾
|||||
f
⟩
L2(ℝ) (m−𝜕)k𝛾.
For example, we may deﬁne fractional
Fourier–Plancherel
transformations
F𝛼,
𝛼∈ℝ, by
F𝛼f
≔
∞
∑
k=0
eik𝛼𝜋∕2
⟨
(m −𝜕)k
2kk!
𝛾
|||||
f
⟩
L2(ℝ)
(m −𝜕)k 𝛾,
which have in recent years found many
applications in physics and engineering (see
[13]).
15.5
The Fourier–Laplace Transformation and
Distributions
We can extend the meaning of the L2 (ℝ)-
inner-product,
which
we
now
denote
simply by ⟨⋅| ⋅⟩, by introducing general-
ized functions f (also called distributions,
see [14]) as linear functionals. Initially,
for
simplicity,
consider
the
so-called
space
of
“test
functions,”
C∞
0 (ℝ),
of
inﬁnitely diﬀerentiable functions on ℝ
that have compact support, that is, they
vanish outside a bounded interval, and let
f ∶C∞
0 (ℝ) →ℂ, 𝜙→f (𝜙) be a linear func-
tional. We introduce the “inner product”
notation
⟨f | 𝜙⟩≔f (𝜙).

516
15 Mathematical Transformations
Using this notation, the linearity of f is
expressed by
⟨f | 𝜙+ 𝛼𝜓⟩= ⟨f | 𝜙⟩+ 𝛼⟨f | 𝜓⟩
for
all
complex
numbers
𝛼
and
all
𝜙, 𝜓∈C∞
0 ℝ. Clearly, for any f ∈L2ℝ
the inner product ⟨f |𝜙⟩deﬁnes a general-
ized function (i.e., a complex-valued linear
functional deﬁned for 𝜙∈C∞
0 ℝ), but for
example exp (i p ⋅) , p ∈ℝ, although not an
element of L2ℝ, also deﬁnes a generalized
function via
⟨exp(i p ⋅)|𝜙⟩≔∫ℝ
exp(i px) 𝜙(x) dx
=
√
2𝜋̂𝜙(p)
for all 𝜙∈C∞
0 ℝ. Further, a generalized
function, which is not given by any classical
function, is 𝛿{𝜔} deﬁned by
𝛿{𝜔}(𝜙)=⟨𝛿{𝜔} | 𝜙⟩≔𝜙(𝜔) for all 𝜙∈C∞
0 ℝ.
This distribution samples, that is, evalu-
ates, the test function 𝜙at the point 𝜔∈ℝ.
The special choice 𝛿≔𝛿{0} is the so-
called Dirac-𝛿-distribution. As it can only
be distinguished from 0 by testing with
𝜙∈C∞
0 ℝsatisfying 𝜙(0) ≠0, we often say
𝛿= 0 on ℝ⧵{0}. However, 𝛿is (by deﬁni-
tion) not identically zero as a functional.
It is particularly useful in many contexts
to have 𝛿{𝜔} (= 𝜏−𝜔𝛿) as a mathematical
model for physical phenomena that are
vanishingly small in space or time such as
a mass point, an elementary charge, a light
point, a short impulse, or a particle at loca-
tion 𝜔(see Section 15.9 for the particularly
interesting higher-dimensional case).
An
important
class
of
distributions
is the space of so-called tempered dis-
tributions ′ (see [14]). With suﬃcient
care,
another
space
of
distributions
exp (𝜚m)
[′] (distributions which after
multiplication by exp (−𝜚m) , are in ′)
can be established for which the concept of
derivative and Fourier–Laplace transfor-
mation can be generalized by deﬁning, for
f ∈exp (𝜚m)
[
′],
⟨𝜕f |𝜙⟩≔−⟨f |𝜕𝜙⟩,
⟨L𝜚f |𝜙⟩≔
⟨
f | L∗
−𝜚𝜙
⟩
,
(15.17)
⟨
L∗
𝜚f |𝜙
⟩
≔⟨f | L−𝜚𝜙⟩
for all 𝜙∈C∞
0 ℝ.
As an example of generalized diﬀerentia-
tion, for the characteristic function 𝜒
]0,∞[ of
the interval ]0, ∞[ , we get
𝜕𝜒
]0,∞[ = 𝛿;
that is, the generalized derivative of the
characteristic function of ]0, ∞[ is the
Dirac-𝛿-distribution.
The Fourier–Laplace transform of the
distribution 𝛿{𝜔} is –according to (15.17) –
L𝜚𝛿{𝜔} =
1
√
2𝜋
exp (−i ( ⋅−i𝜚) 𝜔) .
(15.18)
Conversely, we have
L∗
𝜚exp(−i 𝜔⋅) =
√
2𝜋exp (𝜚𝜔) 𝛿{𝜔}.
(15.19)
The latter fact can be used to detect
oscillatory behavior of frequency 𝜔that
may be hiding in seemingly random data.
The oscillation would show up in the
approximately
Fourier–Laplace
trans-
formed data as a “peak” or “spike” at the
point 𝜔.
The concept of functions of 𝜕𝜚and, in
particular, the concept of convolution
can also be carried over to exponen-
tially weighted tempered distributions in
exp (𝜚m)
[′]. This will be illustrated by a
few applications.

15.5 The Fourier–Laplace Transformation and Distributions
517
15.5.1
Impulse Response
The importance of the Dirac-𝛿-distribution
is that a translation-invariant, linear system
S(𝜕𝜚) is completely described by its impulse
response S(𝜕𝜚)𝛿. The general response
S(𝜕𝜚) f is given by convolution with the
impulse response
S(𝜕𝜚) f = (S(𝜕𝜚)𝛿) ∗f .
In the case that S(𝜕𝜚) is the solution oper-
ator S(𝜕𝜚) = P(𝜕𝜚)−1
of the diﬀerential
equation P(𝜕𝜚)u = f , where P is a polyno-
mial, the impulse response is also known
as the fundamental solution (or Green’s
function), cf. [14].
15.5.2
Shannon’s Sampling Theorem
Another distribution of particular interest
is the (sampling or) comb distribution
≔∑
k∈ℤ𝛿{√
2𝜋k
}, which takes samples
at
equidistant
points
(distance
√
2𝜋).
This distribution is reproduced by the
Fourier–Plancherel transformation
F
=
,
(15.20)
which is a way of stating the Poisson sum-
mation formula:
F
∑
k∈ℤ
𝛿{√
2𝜋k
} =
1
√
2𝜋
+∞
∑
k=−∞
exp
(
−i
√
2𝜋k ⋅
)
=
+∞
∑
k=−∞
𝛿{
√
2𝜋k}.
Note that for the ﬁrst equality we have used
(15.18).
If
f ∈L2(ℝ)
and
f = 0
outside
of
[−
√
𝜋∕2,
√
𝜋∕2], then
∗f is simply the
periodic extension of f from the interval
[−
√
𝜋∕2,
√
𝜋∕2], to all of ℝ. Therefore,
by “cutting-oﬀ” with the characteristic
function Π ≔𝜒
[−
√
𝜋∕2,
√
𝜋∕2], we recover
f = Π(
∗f ).
(15.21)
As we shall see, the so-called Shannon sam-
pling theorem follows from (15.20). In fact,
applying (15.21) to ̂f ∈L2(ℝ) with ̂f = Ff =
0 outside of [−
√
𝜋∕2,
√
𝜋∕2] and applying
the inverse Fourier transformation we get
f = F∗(Π(
∗̂f )),
=sinc
(√
𝜋∕2⋅
)
∗
( +∞
∑
n=−∞
f (
√
2𝜋n) 𝛿{
√
2𝜋n}
)
,
=
+∞
∑
n=−∞
f (
√
2𝜋n) sinc
(√
𝜋∕2
(
⋅−
√
2𝜋n
))
,
(15.22)
which is Shannon’s theorem. Here we have
used the fact that (F∗Π)(t) = sinc(
√
𝜋∕2t)
and sinc(t) ≔sin(t)∕t.
Any function f with ̂f vanishing outside
a bounded interval is called band limited.
As any such function can be easily rescaled
to have ̂f = 0 outside of [−
√
𝜋∕2,
√
𝜋∕2],
(15.22) shows that any band-limited func-
tion f can be completely recovered from
equidistant sampling.
By interchanging the role of f and ̂f in the
Shannon sampling theorem, we have
̂f =sinc
(√
𝜋
2 ⋅
)
∗
( +∞
∑
n=−∞
̂f (
√
2𝜋n) 𝛿{
√
2𝜋n}
)
(15.23)
and after applying the inverse Fourier trans-
formation we obtain a scaled variant of the
Fourier series expansion
f =
+∞
∑
n=−∞
̂f (
√
2𝜋n) Π exp
(
i
√
2𝜋n ⋅
)
(15.24)
of a function f ∈L2(ℝ) with f = 0 out-
side
of
[−
√
𝜋∕2,
√
𝜋∕2].
Note
that

518
15 Mathematical Transformations
{(2𝜋)−1∕4Π exp(i
√
2𝜋n ⋅) | n ∈ℤ}
is
an
orthonormal set in L2(ℝ), that is, all
elements are normalized and pairwise
orthogonal, which allows a Fourier series
expansion in the sense of (15.24) for
every f ∈L2(ℝ) with f = 0 outside of
[−
√
𝜋∕2,
√
𝜋∕2].
As
the
Fourier–Plancherel
transfor-
mation is unitary, it follows that the
Fourier–Plancherel
transform
of
the
orthonormal basis
{
(2𝜋)−1∕4 Π exp(i
√
2𝜋n ⋅) | n ∈ℤ
}
of
L2 (
] −
√
𝜋∕2,
√
𝜋∕2]
)
is
also
an
orthonormal basis. Thus,
{
(2𝜋)−1∕4 sinc
(√
𝜋∕2
(
⋅−
√
2𝜋n
))||||
n ∈ℤ
}
is also an orthonormal set and indeed
an
orthonormal
basis
for
all
band-
limited functions in L2 (ℝ) with band
in [−
√
𝜋∕2,
√
𝜋∕2].
15.6
The Fourier-Sine and Fourier-Cosine
Transformations
As
a
unitary
operator,
the
Fourier–Plancherel
transformation
F
is also normal with real part Fcos ≔ℜ𝔢F =
(1∕2)(F + F∗), the Fourier cosine trans-
formation and negative imaginary part
Fsin ≔−ℑ𝔪F = −(1∕2i)(F −F∗),
the
Fourier sine transformation. Being real and
imaginary parts of a normal operator, the
Fourier cosine and the Fourier sine trans-
formations are selfadjoint and therefore
their spectra 𝜎(Fcos) and 𝜎(Fsin) are real.
We already know that
ℜ𝔢𝜎(F) = {+1, −1} ⊆𝜎(Fcos),
ℑ𝔪𝜎(F) = {+1, −1} ⊆𝜎(Fsin).
The relation between Fcos, Fsin, and F yields
further insight. We have (with 𝜎−1 as the
reﬂection at the origin)
Fcos = 1
2(F + F∗),
= 1
2(1 + 𝜎−1) F,
= F 1
2(1 + 𝜎−1),
= 1
2(1 + 𝜎−1) F∗,
= F∗1
2(1 + 𝜎−1) ;
(15.25)
Fsin = −1
2i(F −F∗),
= 1
2(1 −𝜎−1) iF,
= iF 1
2(1 −𝜎−1).
It is not hard to see that 1∕2(1 + 𝜎−1) and
(1∕2)(1 −𝜎−1) = 1 −1
2(1 + 𝜎−1)
are
the
orthogonal projections onto the (almost
everywhere) even and odd functions in
L2(ℝ), respectively. Therefore, we also have
0 ∈𝜎(Fcos) and 0 ∈𝜎(Fsin). Moreover, we
calculate with (15.25)
FsinFcos = FcosFsin = 0,
(15.26)
Fsin Fsin = 1
2(1 −𝜎−1),
(15.27)
FcosFcos = 1
2(1 + 𝜎−1).
(15.28)
Thus, we see that Fcos, Fsin are unitary on
the subspaces
1
2(1 ± 𝜎−1) [L2(ℝ)], respec-
tively. This, along with the selfadjointness of
Fcos and Fsin, allows us to deduce that
𝜎(Fcos) = 𝜎(Fsin) = P𝜎(Fcos)
= P𝜎(Fsin) = {0, +1, −1}.
We can identify
1
2(1 ± 𝜎−1) [L2(ℝ)] with
L2(ℝ>0)
via
the
unitary
transforma-
tions
E± ∶L2(ℝ>0) →1
2(1 ± 𝜎−1) [L2(ℝ)]

15.7 The Hartley Transformations H±
519
deﬁned by
(E±f )(x) ≔
1
√
2
{
f (x)
for x ∈ℝ>0
±f (−x)
for x ∈ℝ≤0,
so that, for f ∈L2(ℝ>0),
√
2E+f is the even
extension of f and
√
2E−f is the odd exten-
sion of f . The calculation
||E±𝜑||
2
L2(ℝ) = ∫ℝ
||(E±𝜑)(x)||
2 dx,
= 1
2 ∫ℝ>0
|𝜑(x)|2 dx
+ 1
2 ∫ℝ<0
|𝜑(−x)|2 dx,
= ∫ℝ>0
|𝜑(x)|2 dx = |𝜑|2
L2(ℝ>0) ,
for all 𝜑∈L2(ℝ>0) shows that E± are uni-
tary and so the mappings
E∗
+ Fcos E+ ∶L2(ℝ>0) →L2(ℝ>0),
E∗
−FsinE−∶L2(ℝ>0) →L2(ℝ>0)
are also unitary. Here, E∗
± = E−1
± are deﬁned
by
E∗
± ∶1
2(1 ± 𝜎−1) [L2(ℝ)] →L2(ℝ>0),
𝜙→
√
2 𝜙||ℝ>0.
For “nice” functions, for example, for 𝜙∈
C∞
0 (ℝ>0), the space of smooth functions
vanishing outside of a compact set in ℝ>0,
we get the following well-known integral
representations discussed in Section 15.1:
(̃Fcos𝜙)(x) ≔(E∗
+ Fcos E+𝜙)(x)
=
√
2
𝜋∫ℝ>0
cos(xy) 𝜙(y) dy,
(̃Fsin𝜙)(x) ≔(E∗
−FsinE−𝜙)(x)
=
√
2
𝜋∫ℝ>0
sin(xy) 𝜙(y) dy.
The unitary transformations ̃Fcos and ̃Fsin
are spectral representations for |𝜕| with
no or Dirichlet type boundary condition,
respectively, at the origin. As noted in
Section 15.1, they can be used to dis-
cuss problems involving the second-order
diﬀerential operator −𝜕2 = |𝜕|2 with Neu-
mann or Dirichlet boundary conditions at
the origin, respectively.
15.7
The Hartley Transformations H±
As the Fourier transformation F is unitary,
it follows immediately that exp(i 𝜋∕4) F is
also unitary. Considering the real and imag-
inary part of exp(i 𝜋∕4) F leads to an inter-
esting situation:
ℜ𝔢(exp(i 𝜋∕4) F) =
1
√
2
(Fcos + Fsin),
ℑ𝔪(exp(i 𝜋∕4) F) =
1
√
2
(Fcos −Fsin),
and therefore, for H± ≔Fcos ± Fsin, we have
H+ = ℜ𝔢
(√
2 exp(i 𝜋∕4) F
)
,
H−= ℑ𝔪
(√
2 exp(i 𝜋∕4) F
)
and so for all 𝜑∈L2(ℝ),
||H±𝜑||
2
L2(ℝ) = ||(Fcos ± Fsin)𝜑||
2
L2(ℝ)
= ||Fcos𝜑||
2
L2(ℝ)
± 2ℜ𝔢
⟨
Fcos𝜑||Fsin𝜑
⟩
L2(ℝ)
+ ||Fsin𝜑||
2
L2(ℝ)
= ||Fcos𝜑||
2
L2(ℝ) + ||Fsin𝜑||
2
L2(ℝ)
= |F𝜑| = |𝜑|2
L2(ℝ) .
The transformations H±, which are unitary
and selfadjoint, are known as Hartley trans-
formations. In particular,

520
15 Mathematical Transformations
𝜎(H±) = P𝜎(H±) = {+1, −1}
and
H+H−= H−H+
= FcosFcos −Fsin Fsin
= 1
2(1 + 𝜎−1) −1
2(1 −𝜎−1)
= 𝜎−1,
H+H+ = H−H−
= FcosFcos + Fsin Fsin
= 1
2(1 + 𝜎−1) + 1
2(1 −𝜎−1)
= 1.
On C∞
0 (ℝ), we have
(H±𝜑)
(𝜔) =
1
√
2𝜋∫ℝ
(cos (𝜔t)
± sin(𝜔t)) 𝜑(t) dt
and
(
H−𝜑)
(𝜔) = (H+𝜑)
(−𝜔)
for 𝜔∈ℝor
H−= 𝜎−1H+.
As the Fourier–Plancherel transformation
has an extension to tempered distributions,
so have the Hartley transformations.
The Hartley transformations are real, that
is, they commute with complex conjuga-
tion, which may be an advantage in appli-
cations; see [15] for more details. On the
downside, the Hartley transformations H±
are spectral representations for the rarely
used operators ±𝜎−1i𝜕. Hidden periodic
behavior is revealed in a more complicated
fashion:
H± cos (𝜔⋅) =
√
𝜋
2
(𝛿{𝜔} + 𝛿{−𝜔}
) ,
H± sin (𝜔⋅) = ±
√
𝜋
2
(𝛿{𝜔} −𝛿{−𝜔}
) ,
indicating that period 2𝜋∕𝜔behavior in a
function will show up as spikes at ±𝜔in the
Hartley transforms of the function.
15.8
The Mellin Transformation
Let us consider the transformation
ln ∶ℝ>0 →ℝ
x →ln (x) .
Analogous to the reasoning leading up to
(15.3), this transformation induces a uni-
tary mapping
Λ ∶L2 (ℝ) →
√
m [L2 (ℝ>0
)]
≔L2 (ℝ>0, m−1d𝜆, ℂ)
f →f ∘ln
with
Λ∗= Λ−1 ∶
√
m [L2 (ℝ>0
)] →L2 (ℝ)
f →f ∘exp .
Unitarity is easily conﬁrmed by the com-
putation done in Section 15.1. The unitary
mapping
M ≔FΛ∗∶
√
m [L2 (ℝ>0
)] →
L2 (ℝ) is known as the Mellin transforma-
tion. We ﬁnd the unitary equivalence
m𝜕= Λ𝜕Λ∗
= ΛF∗im FΛ∗
and so that M is a spectral representation
for (1∕i)m𝜕. For well-behaved (to ensure
that the integrals exist) f ∈
√
m [L2 (ℝ>0
)]
and g ∈L2 (ℝ), we get
(Mf )
(x) =
1
√
2𝜋∫ℝ
exp(−ixy)f (exp (y)) dy
=
1
√
2𝜋∫ℝ>0
s−ix−1f (s) ds, x ∈ℝ,

15.9 Higher-Dimensional Transformations
521
and
(M∗g)
(t) =
1
√
2𝜋∫ℝ
exp (i ln (t) y) g (y) dy
=
1
√
2𝜋∫ℝ
tiyg (y) dy, t ∈ℝ>0.
15.9
Higher-Dimensional Transformations
Following the rationale that most of the
classical
integral
transformations
are
spectral
representations
of
diﬀerential
operators or functions of such (see [16]),
to identify applicable higher-dimensional
transformations, we should be looking
for
spectral
representations
associated
with partial diﬀerential operators and
investigating the corresponding operator
function calculus associated with such a
spectral representation. For many appli-
cations, however, there is a somewhat
easier
approach
to
higher-dimensional
transformations, which is based on tensor
product structures (see [2]), implied by
separation-of-variables arguments.
The idea is roughly as follows. A tensor
product X1 ⊗· · · ⊗Xn, n ∈ℕ, of com-
plex function spaces Xk, k ∈{1, … , n},
can be considered to be the completion
with respect to an appropriate metric of
the space of all linear combinations of
functions in product (separable) form
u1 ⊗· · · ⊗un given by
(x1, … , xn
) →
n
∏
k=0
uk
(xk
)
with uk ∈Xk, k ∈{1, … , n}. If the spaces
Xk, k ∈{1, … , n}, are Hilbert spaces then
so is X1 ⊗· · · ⊗Xn with the inner product
⟨u1 ⊗· · · ⊗un|v1 ⊗· · · ⊗vn⟩X1⊗···⊗Xn
of
two such functions u1 ⊗· · · ⊗un, v1 ⊗
· · · ⊗vn ∈X1 ⊗· · · ⊗Xn of product form
given by
n
∏
k=1
⟨uk|vk
⟩
Xk ,
that is, the product of the inner products.
Densely
deﬁned
closed
linear
opera-
tors Ak ∶D (Ak
) ⊆Xk →Yk, where Yk,
k ∈{1, … , n},
are
also
Hilbert
spaces
can now be combined to give an oper-
ator denoted by A1 ⊗· · · ⊗An between
tensor product spaces X1 ⊗· · · ⊗Xn and
Y1 ⊗· · · ⊗Yn deﬁned (see [2]) as the
closed linear extension of the mapping
deﬁned on separable functions by
u1 ⊗· · · ⊗un →A1u1 ⊗· · · ⊗Anun.
Applying this for example to n copies of
the Fourier–Laplace transformation yields
the n-dimensional Fourier–Laplace trans-
formation. Indeed, because
exp ((z1, … , zn
) (
w1, … , wn
))
≔exp
( n
∑
k=1
zkwk
)
= exp (z1w1
) · · · exp (znwn
) ,
for
𝜈=
(
𝜈1, … , 𝜈n
) ∈ℝn
and
H𝜈,0 (ℝn) ≔{f ||exp (−𝜈m) f ∈L2 (ℝn)
} =
⨂n
k=1 H𝜈k,0 (ℝ),
we
have
that
the
n-dimensional
Fourier–Laplace
trans-
formation, n ∈ℕ≥2, can be understood as
a repeated one-dimensional Fourier trans-
formation, that is, with m = (m1, … , mn
)
as the n-tuple of multiplication by the
respective argument
(L𝜈f )(p)
=
1
(2𝜋)n∕2 ∫
∞
−∞exp(−i x1 p1) · · ·
· · · ∫
∞
−∞exp(−i xn pn) exp (−𝜈m) f (x) dxn
· · · dx2 dx1,
for well-behaved f ∈H𝜈,0 (ℝn); in other
terms
L𝜈= L𝜈1 ⊗· · · ⊗L𝜈n.

522
15 Mathematical Transformations
As tensor products of unitary operators
are unitary, it follows that L𝜈∶H𝜈,0 (ℝn) →
L2 (ℝn) is a unitary transformation. As a
consequence, many of the above considera-
tions, such as the extension to distribution
spaces, can be carried over immediately to
the higher-dimensional case.
For example, the n-dimensional Dirac-𝛿-
distribution 𝛿= 𝛿{0} simply takes a single
sample at the origin in ℝn and similarly
the shifted version 𝛿{x} samples at x ∈ℝn.
The latter has now 1∕(2𝜋)n∕2 exp(−i x ⋅)
as its Fourier–Plancherel transform, that
is,
the
Fourier–Laplace
transform
for
𝜈= (0, … , 0) ∈ℝn (compare [2]). Also,
≔∑
x∈ℤn 𝛿{
√
2𝜋x}, which takes samples
at every point of ℝn with coordinates in
√
2𝜋[ℤ], still satisﬁes
F
=
.
(15.29)
Thus, in particular, corresponding variants
of the Shannon sampling theorem and
of the Fourier series expansion hold. The
higher-dimensional
Fourier–Plancherel
transformation ﬁnds its applications in
higher-dimensional, translation-invariant,
linear systems, [11]. A particularly promi-
nent application is in optics (see, e.g.,
[10, 17, 18].
In fact, the solution theory of gen-
eral linear partial diﬀerential equations
and systems with constant coeﬃcients
can
be
based
on
higher-dimensional
Fourier–Laplace transformation strategies,
see [14].
The
scaling
behavior
of
the
Fourier–Plancherel transformation ﬁnds
its generalization in the following. Let
(𝜎Af )
(x) ≔
√
|det (A)|f (Ax) , x ∈ℝn, for
a non-singular real (n × n)-matrix A. Then
the generalization of the one-dimensional
rescaling property, see footnote 8, is
𝜎(A−1)∗F = F𝜎A.
If A is orthogonal, that is, A∗= A−1, then
𝜎A and F commute. This implies that rota-
tional symmetries are preserved by the
Fourier–Plancherel transformation, a fact
that is frequently used in applications.
15.10
Some Other Important Transformations
In this ﬁnal section, we consider a few other
mathematical transformations that have
found important application in physics or
engineering.
15.10.1
The Hadamard Transformation
The Hadamard transformation, also called
the
Hadamard–Rademacher–Walsh,
the
Hadamard–Walsh,
or
the
Walsh
transformation,
is
an
example
of
a
higher-dimensional
discrete
trans-
formation
deﬁned
for
functions
in
ℂ{0,1}n ≔{f |f ∶{0, 1}n →ℂ} as a map-
ping W ∶ℂ{0,1}n →ℂ{0,1}n given by12)
W𝜑(x) ≔
∑
k∈{0,1}n
𝜑(k) (−1)⟨k|x⟩{0,1}n .
The Hadamard transformation is used in
the analysis of digital devices.
If we choose a particular enumeration
e ∶{0, … 2n −1} →{0, 1}n of {0, 1}n , that
is, e is bijective (one-to-one and onto),
then, using the standard inner products
for complex-valued mappings deﬁned on
ﬁnite sets, we get that
E ∶ℂ{0,1}n →ℂ2n
𝜑→𝜑∘e
12) Compare higher-dimensional Fourier series
transformation (and the above discussion of the
discrete Fourier transformation at the end of
Section 15.2.2 for the one-dimensional case).

15.10 Some Other Important Transformations
523
is a unitary map. Thus, EWE∗yields
a
matrix
representation
in
terms
of
so-called
Hadamard
matrices13)
(
(−1)⟨e(s)|e(t)⟩{0,1}n )
s,t∈{0,…,2n−1} .
15.10.2
The Hankel Transformation
The (one-dimensional) Hankel transforma-
tion H0 is deﬁned, for suitable functions f
on ]0, ∞[, by
(H0f )(s) = ∫
∞
0
r f (r) J0(sr) dr,
where J0 denotes the Bessel function of the
ﬁrst kind x →∑∞
r=0(−1)r (
x
2
)2r
∕(r!)2.
Let
𝜙∶ℝ2 →ℂ
be
circularly
sym-
metric, which means that the function
(r, 𝜃) →𝜙(r cos(𝜃), r sin(𝜃)), obtained by
a polar coordinate transformation of 𝜙, is
independent of 𝜃, that is,
𝜙(r cos(𝜃), r sin(𝜃)) = f (r) ≔𝜙(r, 0)
for every 𝜃∈ℝ. The Hankel transform
of f can be found by using polar coordi-
nates to evaluate ̂𝜙, the two-dimensional
Fourier–Plancherel transform of 𝜙:
̂𝜙(s, 0) = ∫
∞
0
r f (r) J0(sr) dr.
Now,
due
to
the
circular-
symmetry-preserving
property
of
the
Fourier–Plancherel
transformation,
we
have
̂𝜙(x) = ̂𝜙(s, 0) for all x ∈ℝ2 with
|x| = |s| . Thus, the Hankel transformation
H0
is
essentially
the
two-dimensional
Fourier–Plancherel
transformation
restricted
to
the
circularly
symmetric
function.
To ﬁnd the precise spaces between
which H0 is unitary, we need to be more
13) Hadamard matrices are orthogonal matrices,
that is, real unitary matrices, with entries ±1.
detailed. As circularly symmetric func-
tions are uniquely determined by their
values on ℝ>0 × {0} , we ﬁnd, using polar
coordinates,
⟨𝜙|𝜓⟩L2(ℝ2)
= ∫ℝ2 𝜙(x) 𝜓(x) dx
= 2𝜋∫ℝ>0
𝜙(r, 0) 𝜓(r, 0) r dr
=
⟨√
2𝜋m𝜙(⋅, 0) |
√
2𝜋m𝜓(⋅, 0)
⟩
L2(ℝ>0).
Thus, 𝜙→𝜙( ⋅, 0) is a unitary mapping s0
from
{𝜙∈L2 (ℝ2) | 𝜎U𝜙= 𝜙for all rotations U}
to the weighted L2-type space
1
√
2𝜋m
[L2(ℝ>0
)]≔
{
f |
√
2𝜋mf ∈L2(ℝ>0
)}
.
Consequently, the Hankel transformation
H0 ≔s0Fs∗
0 is unitary in the Hilbert space
1∕
√
2𝜋m [L2 (ℝ>0
)].
The signiﬁcance of the Hankel trans-
formation lies in particular in the fact
that it is a spectral representation for
√
−m−1𝜕m𝜕, where −m−1𝜕m𝜕is a self-
adjoint realization of the Bessel operator
in
1∕
√
2𝜋m [L2 (ℝ>0
)]
with
Neumann
boundary condition at 0.
15.10.3
The Radon Transformation
Finally, we consider the Radon transforma-
tion, which is used, for instance, in the ﬁeld
of tomography, cf. [19]. This transformation
also yields a spectral representation of a dif-
ferential operator. The Radon transform of
f ∶ℝ2 →ℂis formally given by
Rf (𝛼, 𝜎) =
1
√
2𝜋∫{x∈ℝ2|x⋅y(𝛼)=𝜎}
f ds, (15.30)

524
15 Mathematical Transformations
where 𝛼∈] −𝜋, 𝜋], 𝜎∈ℝand y(𝛼) ∈S1,
the unit sphere in ℝ2, given by y(𝛼) ≔
( cos 𝛼
sin 𝛼
)
. To analyze this, we employ a
parameterization of {x ∈ℝ2|x ⋅y(𝛼) = 𝜎}
as the line given by
( x1
x2
)
=
( cos 𝛼
−sin 𝛼
sin 𝛼
cos 𝛼
) ( 𝜎
t
)
for t ∈ℝ. Substituting this into the integral
expression of R yields
Rf (𝛼, 𝜎)
= ∫ℝ
f
(( cos 𝛼
−sin 𝛼
sin 𝛼
cos 𝛼
) ( 𝜎
t
))
dt.
Using the equality
(
(1 ⊗F) Rf )
(𝛼, r) = (Ff ) (
r
( cos 𝛼
sin 𝛼
))
for each 𝛼∈] −𝜋, 𝜋], r ∈ℝ, we compute
|f |2
L2(ℝ2)
= |Ff |2
L2(ℝ2)
= ∫ℝ2 |(Ff ) (x)|2 dx
= ∫ℝ>0 ∫]−𝜋,𝜋]
|||||
(Ff )
(
r
( cos 𝛼
sin 𝛼
))|||||
2
d𝛼r dr
= 1
2 ∫ℝ∫]−𝜋,𝜋]
|||||
(Ff )
(
s
( cos 𝛼
sin 𝛼
))|||||
2
d𝛼|s| ds
= 1
2 ∫ℝ∫]−𝜋,𝜋]
|||
(
(1 ⊗F) Rf )
(𝛼, s)|||
2
d𝛼|s| ds
= 1
2 ∫ℝ∫]−𝜋,𝜋]
|||||
(
(1⊗F)
√
||𝜕2||Rf
)
(𝛼, s)
|||||
2
d𝛼ds
= 1
2 ∫ℝ∫]−𝜋,𝜋]
|||||
(√
||𝜕2||Rf
)
(𝛼, s)
|||||
2
d𝛼ds
= 1
2
||||
√
||𝜕2||Rf ||||
2
L2(]−𝜋,𝜋]×ℝ)
.
Thus, R deﬁned by the integral expression
(15.30) can be extended by continuity to all
of L2 (ℝ2) and then
1
√
2
R ∶L2 (ℝ2) →H1∕2
(||𝜕2|| , ] −𝜋, 𝜋] × ℝ)
f →
1
√
2
Rf
is unitary. Here H1∕2
(||𝜕2|| , ] −𝜋, 𝜋] × ℝ) is
the completion14) of D (𝜕2
) with respect to
the norm induced by the inner product
(
f , g
)
→
⟨
||𝜕2||
1∕2 f || ||𝜕2||
1∕2 g
⟩
L2(]−𝜋,𝜋]×ℝ) .
We
now
deﬁne
a
mapping
R⋄∶
H−1∕2(|𝜕2|, ] −𝜋, 𝜋] × ℝ) →L2(ℝ2) by the
duality relation
⟨R⋄g|f ⟩L2(ℝ2) = ⟨g|Rf ⟩L2(]−𝜋,𝜋]×ℝ) ,
where the inner product on the right-hand
side is utilized as a so-called duality pair-
ing, which is the continuous extension
of ⟨⋅| ⋅⟩L2(]−𝜋,𝜋]×ℝ) to H−1∕2(|𝜕2|, ] −𝜋, 𝜋]
× ℝ) × H1∕2 (|𝜕2|, ] −𝜋, 𝜋] × ℝ).
Inverting our initial parameterization, we
see that for well-behaved g, we have
∫ℝ∫ℝ
g(𝛼, 𝜎) f
((cos 𝛼−sin 𝛼
sin 𝛼
cos 𝛼
) (𝜎
t
))
d𝜎dt
= ∫ℝ∫ℝ
g (𝛼, x1 cos 𝛼+ x2 sin 𝛼)
f (x1, x2
) dx1 dx2
and so, at least formally, we see that
(R⋄g) (x1, x2
)
= ∫]−𝜋,𝜋]
g
(
𝛼, x1 cos 𝛼+ x2 sin 𝛼
)
d𝛼.
The dual operator R⋄is again the continu-
ous extension of the mapping deﬁned for
14) That is the construction –already used sev-
eral times –to obtain the smallest Hilbert space
with the prescribed inner product containing
D (𝜕2
) .

References
525
well-behaved functions by the latter inte-
gral expression. Then
1
2R⋄||𝜕2|| Rf = f
and so R−1 = 1∕
√
2
(
1
√
2R
)−1
=
1
√
2( 1
√
2R)∗
= 1
2R∗is given by
1
2R⋄|𝜕2| ∶H1∕2(|𝜕2|, ] −𝜋, 𝜋] × ℝ)→L2(ℝ2).
(15.31)
Using the Hilbert transformation H, we
ﬁnd
||𝜕2|| = ||i𝜕2||
= i𝜕2 sgn (i𝜕2
)
= 𝜕2 2 ,
where 2 denotes the continuous extension
of 1 ⊗H to H1∕2
(||𝜕2|| , ] −𝜋, 𝜋] × ℝ) , the
inverse Radon transform may be rewritten
as
R−1 = 1
2R⋄𝜕2 2.
Note here that 2 commutes with 𝜕2
because it is a function of 𝜕2 deﬁned via the
operator function calculus.
References
1. Bay, M. and Hasbro (2007) Transformers.
Science ﬁction action ﬁlm, Paramount
Pictures, DreamWorks SKG.
2. Functional Analysis. This handbook,
Chapter 13.
3. Ronald, N.B. (1983) The Fourier Transform
and Its Applications, 2nd edn, 3rd printing,
International Student Edition. Auckland etc.:
McGraw-Hill International Book Company.
4. Weisstein, N. (1980) The joy of Fourier
analysis, in Visual Coding and Adaptibility
(ed. C.S. Harris), Erlbaum, Hillsdale, NJ.
5. Akhiezer, N.I. and Glazman, I.M. (1981) in
Theory of Linear Operators in Hilbert Space,
Monographs and Studies in Mathematics, 9,
10, Vols I, II (eds Transl. from the 3rd
Russian ed. by E.R. Dawson and W.N.
Everitt), Publishers in association with
Scottish Academic Press, Edinburgh, Boston,
MA, Pitman Advanced Publishing Program,
London, Melbourne.
6. Zemanian, A.H. (1987) Generalized Integral
Transformations (Unabridged republ. of the
1968 orig. publ. by Interscience, New York),
vol. XVI, Dover Publications, Inc., New
York, 300 p.
7.
Beckenbach, E.F. and Hestenes, M.R. (eds)
(1962) Modern Mathematics for the
Engineer, Second Series (University of
California, Engineering Extension Series),
McGraw-Hill Book Company Inc., New
York, Toronto, London.
8. Akansu, A.N. and Agirman-Tosun, H. (2010)
Generalized Discrete Fourier Transform
With Nonlinear Phase. IEEE Trans. Signal
Process., 58(9), 4547–4556.
9. Brigham, E.O. (1988) The Fast Fourier
Transform and Applications, Prentice Hall,
Englewood Cliﬀs, NJ.
10. Gaskill, J.D. (1978) Linear Systems, Fourier
Transformations and Optics, John Wiley &
Sons, Inc., New York, Chichester, Brisbane,
Toronto.
11. Norman, F.M. (1981) Lectures on linear
systems theory. J. Math. Psychol., 23, 1–89.
12. McBride, A.C. (1979) Fractional Calculus
and Integral Transforms of Generalized
Functions, Pitman Research Notes in
Mathematics, vol. 31, Pitman Advanced
Publishing Program, San Francisco, CA,
London, Melbourne, ISBN: 0-273-08415-1.
13. Ozaktas, H.M., Zalevsky, Z., and Alper, M.
(2001) The Fractional Fourier Transform with
Applications in Optics and Signal Processing,
Series in Pure and Applied Optics, John
Wiley & Sons, Inc, ISBN: 0-471-96346-1.
14. Partial Diﬀerential Equations. This
handbook, Chapter 16.
15. Bracewell, R.N. (1986) The Hartley
Transform, Oxford Engineering Science
Series, vol. 19, Oxford University Press, New
York, Clarendon Press, Oxford.
16. Picard, R. (1989) Hilbert Space Approach to
Some Classical Transforms, John Wiley &
Sons, Inc., New York.

526
15 Mathematical Transformations
17. Goodman, J.W. (1968) Introduction to
Fourier Optics, McGraw-Hill, New York.
18. Taylor, C.A. (1978) Images: A Uniﬁed View of
Diﬀraction and Image Formation with All
Kinds of Radiation, Wykeham Publ.,
London, Basingstoke.
19. Ramm, A.G. and Katsevich, A.I. (1996) The
Radon Transform and Local Tomography,
CRC Press Inc.

527
16
Partial Diﬀerential Equations
Des McGhee, Rainer Picard, Sascha Trostorﬀ, and Marcus Waurick
16.1
What are Partial Diﬀerential Equations?
Partial diﬀerential equations (PDEs) pro-
vide the essential mathematical models of
physical phenomena that vary in time and
space. They are equations involving one
or more unknown function(s) of (more
than one) independent variables and their
partial derivatives. In standard applica-
tions, we would often expect one variable,
t, for time and three independent space
variables, Cartesian coordinates x, y, z or
polar coordinates r, 𝜃, 𝜙or some other
standard
three-dimensional
coordinate
system. Of course, sometimes steady-state
models are considered so there is no
dependence on t and/or symmetries allow
lower-dimensional models to be studied
so that only one or two space variables are
required. On the other hand, physics often
involves mathematical models that intro-
duce higher-dimensional “space” (inverted
commas because this may not be physical
space)
requiring
independent
variables
x1, x2, x3, … , xn, n > 3,
so
that,
along
with time, t, there are n + 1 independent
variables.
A core objective is to determine the
unknown functions that satisfy the PDE in
order to predict the behavior of the phys-
ical system being modeled. The physical
phenomenon being modeled will nor-
mally determine an appropriate domain
in which the PDE is to be solved, that is,
a “space”-domain D ⊆ℝn and, if there is
time dependence, a time interval I ⊆ℝin
which the PDE is to be satisﬁed. In addition
to the PDE itself, there will often be some
boundary conditions that the unknown
functions have to satisfy on the boundary
𝜕D of D and, if time is involved, some
initial conditions to be satisﬁed by the
unknown functions at some initial time
t = t0. Often, an analytic solution of the
PDE may not be available but qualitative
information derived from a careful analysis
of the system, for example, existence and
uniqueness of the solution, may allow
numerical methods to be developed and
applied to approximate the solution to an
acceptable degree of accuracy.
As elementary notions of diﬀerentiation
require functions to be continuous, one
might initially think that the importance of
PDEs is restricted to phenomena that can
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

528
16 Partial Diﬀerential Equations
be modeled as continuous (indeed smooth)
processes. However, PDEs also play a fun-
damental role in modeling discrete physi-
cal phenomena, most notably in quantum
mechanics via the Schrödinger equation.
In addition, mathematical generalization
allows the precise deﬁnition of derivatives
of discontinuous functions, and indeed of
generalized functions such as the Dirac-
delta, so that PDEs are often appropri-
ate mathematical models even when the
unknown functions to be found are not
expected to be smooth or even continuous.
In full generality, that is, where the
PDE involves an arbitrary function of the
unknowns and their derivatives, there is
little that can be said about properties of
solutions or methods of solution. However
special cases, particularly those that are
of central interest in applications, have
been dealt with extensively. All the core
physical phenomena, such as sound, heat,
electromagnetism, elasticity, ﬂuid ﬂow,
and so on are modeled by PDEs and these
models have been well studied. In this
chapter, therefore, we shall not dwell on the
standard theory and methods of solution
of the classical models of mathematical
physics referring the reader to standard
texts at elementary level (e.g., [1–4]) or
advanced level (e.g., [5–8]). Rather, we
shall concentrate on structural matters
concerning PDEs that are necessary for
reasonable models of physical phenom-
ena and precise notions of solvability. We
shall however illustrate the theory using
well-known (systems of) equations from
mathematical physics.
Historically, linear models of physical
phenomena have been developed and
extensively studied. Progressively, more
and more nonlinear eﬀects are being taken
into account but, because the analysis
of nonlinear PDEs is based on a deep
understanding of the linear case, the latter
will be the focus of our discussion.
A linear PDE is of the form
∑m0
𝛼0=0 · · · ∑mn
𝛼n=0 a(𝛼0,…,𝛼n) (t, x)
× (𝜕
𝛼0
0 · · · 𝜕
𝛼n
n u)
(t, x) = f (t, x) ,
where
(t, x)
ranges
in
a
subset1)
I × D ⊆ℝ1+n,
that
is,
t ∈I ⊆ℝ, x =
(x1, … , xn
) ∈D ⊆ℝn. In many physical
applications we have, as discussed above,
n = 3 and t plays the role of time. The
symbol 𝜕k denotes the partial derivative
with respect to the variable at position
k ∈{0, 1, … , n} in the list (t, x1, … , xn
).
The coeﬃcient functions a(𝛼0,…,𝛼n) and the
“source term” f are commonly assumed
to be given and the main task is to ﬁnd
solutions
u that are usually real- or
complex-valued
functions
deﬁned
on
I × D. To emphasize that we are looking
for functions as solutions and not really
for values at a speciﬁc point, we usu-
ally drop the reference to the variables
(t, x) =
(
t, x1, …, xn
).
Also,
introduc-
ing the standard multi-index notation
(𝜕0, …, 𝜕n
)(𝛼0,…,𝛼n) for 𝜕𝛼0
0 · · · 𝜕𝛼n
n , abbrevi-
ating 𝜕∶= (𝜕0, …, 𝜕n
) , 𝛼∶= (𝛼0, …, 𝛼n
) ,
m ∶=
(
m0, …, mn
),
and
writing
0
for
(0, … , 0), we get the more compact form
P(𝜕)u ∶=
m
∑
𝛼=0
a𝛼𝜕𝛼u = f .
(16.1)
We could also drop the summation range
altogether, because we could implicitly
think of the coeﬃcients a𝛼as being zero if
they are not in the prescribed summation
range.
1) Here, we use the superscript 1 + n to draw
attention to the distinction between the time
variable and the space variable(s). We revert
later to the superscript n + 1 when we have
variables (x0, x1, … , xn), that is, there is no time
dependence or a suitable variable to regard
as “time-like” has not yet been identiﬁed (see
Section 16.2.1).

16.2 Partial Diﬀerential Equations in ℝn+1, n ∈ℕ, with Constant Coeﬃcients
529
We may think of the coeﬃcients a𝛼not
just as scalar valued but possibly matrix val-
ued, all of the same matrix size s × r, with
u = (u1, u2, … , ur) and f = (f1, f2, … , fs). In
this way, (16.1) describes a system of PDEs
as well as a single PDE in one uniﬁed
form. Whichever the case, we shall sim-
ply refer to (16.1) as a PDE. A single PDE
is a special matrix-valued case with 1 × 1-
matrices as coeﬃcients and generally lin-
ear algebra teaches us that we should not
expect a unique solution to a linear sys-
tem of equation if the coeﬃcients are not
square matrices and so we shall assume this
throughout.
When can such a problem be consid-
ered as a reasonable model of a physi-
cal phenomenon? In general, the answer
to this question is provided by the three
Hadamard conditions:
1. Existence. There should be at least one
solution for every right-hand side.
2. Uniqueness. There should be at most
one solution.
3. Robustness. The solution should
depend continuously on the given data.
The ﬁrst condition may imply a con-
straint on the admissible data, that is, the
prescription of a suitable function space
that f must belong to. Satisfying the sec-
ond condition may require constraints to
the admissible set of solutions, that is, the
deﬁnition of a suitable function space in
which to seek solutions u. The third con-
dition guarantees that small errors in data,
which will always be present in any physi-
cal measurement, result in only small errors
in solutions; without this requirement, the
solution of the PDE may provide no mean-
ingful insight to reality.
Satisfying all three conditions commonly
results in generalizations of solution con-
cepts that takes us beyond the classical
understanding of diﬀerentiation because
the function spaces required to ensure
the Hadamard requirements are spaces
of generalized functions (distributions).
Problems satisfying Hadamard’s require-
ments are called well-posed, those which
do not are called ill-posed. However, for
mathematical models of physical phenom-
ena that are in the ﬁrst instance ill-posed
problems, there are often physically justi-
ﬁable modiﬁcations that can be made to
obtain a problem that is well-posed.
In general, establishing well-posedness
for a PDE of the form (16.1) is not an easy
task. We therefore start with the restric-
tive assumption that the coeﬃcients a𝛼are
constant.
16.2
Partial Diﬀerential Equations in ℝn+1,
n ∈ℕ, with Constant Coeﬃcients
Given a PDE with constant coeﬃcients, the
issue of well-posedness can be reduced to
algebraic characterizations via the applica-
tion of an integral transformation. For 𝜈∈
ℝn+1, the unitary mapping 2)
L𝜈∶H𝜈,0
(ℝn+1) →L2 (ℝn+1)
generated by the integral expression
(L𝜈𝜑)
(x) = ̂𝜑(x −i𝜈)
=
1
(2𝜋)(n+1)∕2 ∫ℝn+1 exp (−(ix + 𝜈) ⋅y) 𝜑(y) dy
is known as the Fourier–Laplace transfor-
mation, see [9].
2) Recall from [9] that H𝜈,0
(ℝn+1) is the space of
(equivalence classes of) functions f such that
x →exp (−𝜈⋅x) f (x) is square integrable. The
inner product is
(f , g) →∫ℝn+1 f (x) g (x) exp (−2𝜈⋅x) dx.

530
16 Partial Diﬀerential Equations
A Cartesian product X0 × · · · × XN of
Hilbert
spaces,
equipped
with
the
inner
product
Xk,
k ∈{0, … , N},
⟨v|w⟩⨁
k∈{0,…N} Xk ∶= ∑
k∈{0,…,N}
⟨vk | wk
⟩
Xk
for all v = (v0, … , vN
), w = (w0, … , vN
)
∈X0 × · · · × XN
is
a
Hilbert
space,
called
the
(ﬁnite)
direct
sum
of
(Xk
)
k∈{0,…,N}
and
is
denoted
by
⨁
k∈{0,…,N} Xk
or
X0 ⊕· · · ⊕XN,
see
[10]. The elements (x0, … , xN) of such
a direct sum will also be denoted by
x0 ⊕· · · ⊕xN or – more suggestively – by
a column matrix
⎛
⎜
⎜⎝
x0
⋮
xN
⎞
⎟
⎟⎠
.
Similarly,
a
(K + 1) × (L + 1)-matrix
of
elements in Hilbert spaces can be regarded
as a (K + 1) ⋅(L + 1)-dimensional vector
and hence an element in the direct sum
of the appropriate (K + 1) ⋅(L + 1) Hilbert
spaces.
Applying this construction, we see that
we can easily extend the Fourier–Laplace
transformation to vectors or indeed matri-
ces with components in H𝜈,0
(ℝn+1) by
componentwise
application.
Moreover,
this matrix convention makes the action of
a diﬀerential operator P (𝜕), where P is a
polynomial with matrix coeﬃcients, rather
intuitive as a form of matrix multiplication.
Indeed, for a polynomial matrix P, that is, a
matrix with scalar polynomials as entries,
L𝜈P (𝜕) = P (im + 𝜈) L𝜈.
(16.2)
Here m symbolizes “multiplication-by-
the-argument”
and
P (im + 𝜈)
denotes
a
multiplication
operator
deﬁned
by
P (im + 𝜈)
(g)
(x) = P (ix + 𝜈) g (x) ,
[10].
The
multiplication
on
the
right-hand
side is, for g in an appropriate direct
sum space, to be understood as an ordi-
nary matrix product. Indeed, if P (𝜕)
has
(J + 1) × (K + 1)
–
matrix
coeﬃ-
cients,
then
it
is
clear
that
g
must
be either a (K + 1)
–
column vector
with
entries
in
H𝜈,0
(ℝn+1),
that
is,
g ∈⨁
k∈{0,…,K} H𝜈,0
(ℝn+1),
or
possibly
a (K + 1) × (L + 1) – matrix with entries
in
H𝜈,0
(ℝn+1),
that
is,
g ∈⨁
l∈{0,…,L}
⨁
k∈{0,…,K} H𝜈,0
(ℝn+1). We shall, however,
rarely note matrix sizes and simply write
g ∈H𝜈,0
(ℝn+1), because the matrix sizes
should be clear from the context.
Then
the
application
of
the
Fourier–Laplace transformation to the
PDE (16.1) and using (16.2) results in
the algebraic problem
P (im + 𝜈) L𝜈u = L𝜈f
for the unknown L𝜈u. If this can be
solved, then the solution of the PDE
can be obtained by applying the inverse
transformation.
16.2.1
Evolutionarity
Before focusing on the issue of construct-
ing solutions, we brieﬂy discuss how to
detect a direction of a possible time variable
in a diﬀerential expression P(𝜕). This can
be a problem because changes of variables
made, for example, to simplify the appear-
ance of a PDE may result in the direction of
evolution being somewhat hidden.
A fundamental (and from our point
of view characterizing) property of a
mathematical model
P(𝜕)u = f
of a physical phenomenon is that it should
be causal, that is the eﬀect of data f on
the solution u should not precede (in time)

16.2 Partial Diﬀerential Equations in ℝn+1, n ∈ℕ, with Constant Coeﬃcients
531
the application of the data. Therefore, if the
polynomial det (P (z)) has no zeros in a half-
space of ℂn+1 of the form
i [ℝn+1] + [ℝ≥𝜚0
] 𝜈0
for some 𝜚0 ∈ℝ>0, 𝜈0 ∈ℝn+1, ||𝜈0|| = 1,
then we call the direction 𝜈0 evolution-
ary or time-like because in the direction
of 𝜈0 we have causality in the sense that
for all a ∈ℝ, if the data f vanishes on
the
half-space
[ℝ<a
] 𝜈0 + {𝜈0
}⊥
with
interior unit normal 𝜈0 –that is, the set
{x ∈ℝn+1|x = s𝜈0 + y, s < a, 𝜈0 ⋅y = 0} –
then the solution P (𝜕)−1 f also vanishes on
the same half-space. The partial diﬀerential
operator P (𝜕) is then called evolutionary
in direction 𝜈0. If P (𝜕) has a direction in
which it is evolutionary then we brieﬂy say
P (𝜕) is evolutionary; otherwise we say P (𝜕)
is nonevolutionary. If P (𝜕) is evolutionary
in both direction 𝜈0 and −𝜈0 we say that
P (𝜕) is reversibly evolutionary (in direction
𝜈0). If P (𝜕) is only evolutionary in direction
𝜈0 but not in direction −𝜈0, then we say
P (𝜕) is irreversibly evolutionary.
In ℝ2, for a second-order scalar dif-
ferential
operator
p (𝜕) = p(𝜕1,𝜕2)
with
real coeﬃcients, a more common name
for irreversibly evolutionary is parabolic,
reversibly
evolutionary
coincides
with
hyperbolic,
and
nonevolutionary
with
elliptic.
This
classical
terminology
is
based on the fact that the three situations
correspond to {x ∈ℝ2|p (x) = 0} being
respectively a parabola, a hyperbola, or an
ellipse. These classiﬁcations have various
generalizations to arbitrary dimensions
(leaving, however, many partial diﬀeren-
tial operators, such as the Schrödinger
operator, unclassiﬁed).
Note that (1 −𝜕2
1 −𝜕2
2
) is nonevolution-
ary if considered in ℝ2 (static), but it is
reversibly evolutionary in ℝ1+2 in direction
e0 = (1, 0, 0) (quasi-static).
The direction of evolutionarity 𝜈0 will
be used as the weight occurring in the
exponential weighted space H𝜈,0(ℝn+1), the
space where we will seek for solutions, via
𝜈= 𝜚𝜈0 for 𝜚> 0 suﬃciently large and the
Fourier–Laplace transformation L𝜈will be
used to transform the PDE problem into an
algebraic one using (16.2) as outlined above.
In a nonevolutionary case, we may choose
𝜈= 0 and apply the more familiar Fourier
transformation F = L0.
16.2.2
An Outline of Distribution Theory
As indicated above, we need to consider a
concept of generalized functions, so-called
distributions. A classical development of a
theory of distributions relies on the intro-
duction of the Schwartz space (ℝn+1)
of rapidly decreasing smooth functions
f ∶ℝn+1 →ℂdeﬁned by
(ℝn+1) ∶=
{
f ∈C∞(ℝn+1) |
⋀
𝛼,𝛽∈ℕn+1
sup
x∈ℝn+1 |x𝛼𝜕𝛽f (x)| < ∞
}
endowed with the topology deﬁned by
fk →0 in (ℝn+1) as k →∞⇐⇒
⋀
𝛼,𝛽∈ℕn+1
sup
x∈ℝn+1 |x𝛼𝜕𝛽fk(x)| →0 as k →∞.
A (tempered) distribution is then an ele-
ment of ′(ℝn+1), the space of continuous
linear functionals on (ℝn+1). Examples
of tempered distributions are 𝛿{𝜔}, the
Dirac-𝛿-functionals
of
evaluation
of
a
function f ∈(ℝn+1) at a point 𝜔∈ℝn+1,
that is, 𝛿{𝜔}f ∶= f (𝜔). Other examples
are functionals 𝛿of “evaluation” on a
manifold ⊆ℝn+1. These are deﬁned by
𝛿f ∶= ∫f . Frequently, it is convenient to

532
16 Partial Diﬀerential Equations
apply functionals ′(ℝn+1) to elements in a
dense subset of (ℝn+1) such as C∞
0 (ℝn+1),
the space of inﬁnitely diﬀerentiable func-
tions on ℝn+1 that have compact support,
that is, vanish outside of a bounded set.
It is remarkable to note that there is
a more intrinsic way of constructing the
space of tempered distributions than the
ad hoc deﬁnition from above, see, for
example, [Example 2.2] [11]. For simplicity,
we only treat the one-dimensional case.
For this, consider the position operator
m
of
multiplication-by-argument,
that
is, (mf )(x) ∶= xf (x), and the momentum
operator 𝜕, that is, 𝜕f (x) = f ′(x) for suitable
f and x ∈ℝ, realized as operators in L2(ℝ).
Then, deﬁne the annihilation operator
∶= (1∕
√
2)(m + 𝜕)
and
the
creation
operator ∗=
1
√
2(m −𝜕). Realizing that
𝛾∶x →exp (−x2∕2
)
is an eigenfunction of
the harmonic oscillator ∗with eigen-
value 1 and using the density of the linear
hull of the Hermite functions {Γk | k ∈ℕ},
where
Γk ∶= (1∕|k𝛾|L2(ℝ))k𝛾
for
all
k ∈ℕ, we can determine the action of
any power of ∗by knowing only how
∗acts on {Γk | k ∈ℕ}. In this way,
(ℝ) = ⋂
k∈ℕD (
(∗)k).
A
sequence
(𝜙n)n∈ℕin (ℝ) then, by deﬁnition, con-
verges to some 𝜙∈(ℝ) if for all k ∈ℕ
we have (∗)k 𝜙n →(∗)k 𝜙in L2(ℝ).
Endow D (
(∗)k) with the scalar product
(𝜙, 𝜓) →⟨(∗)k 𝜙| (∗)k 𝜓⟩,
where
⟨⋅|⋅⟩denotes the usual L2-scalar product.
Then D (
(∗)k) becomes a Hilbert space.
The space of tempered distributions on ℝ
can then be written as
′(ℝ) =
⋃
k∈ℕ
D
(
(∗)k)′
.
Thus, for a tempered distribution f ∈
′(ℝ), the following continuity estimate
holds:
⋁
k∈ℕ,C>0
⋀
𝜙∈(ℝ)
|⟨f |𝜙⟩|
= |f (𝜙)| ≤C| (∗)k 𝜙|L2(ℝ).
Indeed, if we choose f = 𝛿{0} given by
⟨𝛿{0}|𝜙⟩∶= 𝜙(0) for 𝜙∈(ℝ) we estimate
|𝜙(0)| =
|||||||
0
∫
−∞
(1 + t)𝜙′(t)
1
1 + t dt
|||||||
≤|(1 + m)𝜕𝜙|L2(ℝ)
≤C|∗𝜙|L2(ℝ)
for some constant C > 0.
Observing
that
the
Schwartz
space
(ℝn+1) is also an algebra, that is, for any
two functions 𝜙, 𝜓∈(ℝn+1), the product
𝜙⋅𝜓also lies in (ℝn+1), we can deﬁne
the product of a tempered distribution
f ∈′(ℝn+1) and a function 𝜙∈(ℝn+1)
by setting
⟨𝜙⋅f |𝜓⟩∶= ⟨f |𝜙⋅𝜓⟩
for every 𝜓∈(ℝn+1). The resulting func-
tional 𝜙⋅f is also a tempered distribution.
Moreover, because the convolution 𝜙∗𝜓
of two functions 𝜙, 𝜓∈(ℝn+1), given by
(𝜙∗𝜓) (x) ∶= ∫
ℝn+1
𝜙(x −y)𝜓(y) dy
is again an element of (ℝn+1), we deﬁne
the convolution of a tempered distribution
f with a function 𝜙∈(ℝn+1) by
⟨f ∗𝜙|𝜓⟩∶= ⟨f |𝜎−1𝜙∗𝜓⟩
(16.3)
for all 𝜓∈(ℝn+1). Here, 𝜎−1𝜙is given by
(𝜎−1𝜙)(x) = 𝜙(−x) for x ∈ℝn+1. The result-
ing functional f ∗𝜙is again a tempered
distribution.

16.2 Partial Diﬀerential Equations in ℝn+1, n ∈ℕ, with Constant Coeﬃcients
533
One of the most important properties
of the Schwartz space (ℝn+1) is that the
Fourier transformation F deﬁnes a bijection
on it. This fact allows to extend the Fourier
transformation to tempered distributions
by deﬁning
⟨Ff |𝜓⟩∶= ⟨f |F∗𝜓⟩
for all f ∈′(ℝn+1), 𝜓∈(ℝn+1).
16.2.3
Integral Transformation Methods as a
Solution Tool
As illustrated above, the Fourier–Laplace
transformation can be utilized to reformu-
late a linear PDE with constant coeﬃcients
as an equation involving (matrix-valued)
polynomials only, which is easier to han-
dle. Moreover, it is sometimes possible to
write a solution for P(𝜕)u = f as a certain
superposition of so-called fundamental
solutions. In order to compute such fun-
damental solutions, one has to extend
the Fourier–Laplace transformation to
distributions.
For 𝜈∈ℝn+1 the Fourier–Laplace trans-
formation has a continuous extension to
distributions in the space
exp (𝜈m)
[′ (ℝn+1)] ∶=
{exp (𝜈m) f | f ∈′ (ℝn+1)} ,
that is the space of distributions exp(𝜈m)f
given by
⟨exp(𝜈m)f |𝜓⟩∶= ⟨f | exp(−𝜈m)𝜓⟩,
for all functions 𝜓such that exp(−𝜈m)𝜓∈
(ℝn+1). For such a distribution, the
Fourier–Laplace transformation is given
by
⟨L𝜈exp(𝜈m)f |𝜙⟩∶= ⟨exp(𝜈m)f |L∗
𝜈𝜙⟩
= ⟨exp(𝜈m)f | exp(𝜈m)F∗𝜙⟩
= ⟨f |F∗𝜓⟩
= ⟨Ff |𝜙⟩
for 𝜙∈(ℝn+1), that is,
L𝜈∶exp(𝜈m) [′(ℝn+1)] →′(ℝn+1)
exp(𝜈m)f →Ff .
Let us consider an example. Obviously,
𝛿{0} ∈exp (𝜈m)
[′ (ℝn+1)]
for
every
𝜈∈ℝn+1
with
𝛿{0} = exp(𝜈m)𝛿{0},
the
right-hand side 𝛿{0} being interpreted in
′(ℝn+1). Thus, we obtain L𝜈𝛿{0} = F𝛿{0}
and
⟨F𝛿{0}|𝜙⟩= ⟨𝛿{0}|F∗𝜙⟩
=
1
(2𝜋)(n+1)∕2 ∫
ℝn+1
𝜙(x) dx
for each 𝜙∈(ℝn+1), showing
L𝜈𝛿{0} = F𝛿{0} =
1
(2𝜋)(n+1)∕2 .
(16.4)
With this we can now address the possi-
bility of representing the solution u of the
problem
P(𝜕)u = f ,
for given f and a suitable (matrix-valued)
polynomial P as a convolution.
A Green’s tensor G is a solution of the
equation
P (𝜕) G = 𝛿{0},
(16.5)
where the right-hand side is the diagonal
matrix having 𝛿{0} as diagonal entries. It
is a rather remarkable fact that if det (P)
is not the zero polynomial, then such
a Green’s tensor always exists (even if

534
16 Partial Diﬀerential Equations
x →det (P (ix + 𝜈))
has
zeros
and
so
x →P (ix + 𝜈)−1 is not everywhere deﬁned)
but it may not be uniquely determined
(division problem).
Applying the Fourier–Laplace transfor-
mation L𝜈to (16.5) and using (16.4), we
arrive at
P (im + 𝜈) L𝜈G =
1
(2𝜋)(n+1)∕2 .
Note
that
because
𝛿{0}
and
1
(2𝜋)(n+1)∕2 ∶= (𝜙→∫ℝn+1
1
(2𝜋)(n+1)∕2 𝜙)
is
a
tempered distribution, so is – entry by
entry – G.
As the transformed equation involves
only polynomial multiplication, solving
a (system of) PDE(s) reduces to a linear
algebra issue of inverting matrices.
Now, the Green’s tensor can be utilized
to compute solutions u for the (inhomoge-
neous) equation
P(𝜕)u = f .
Indeed, if L𝜈G can be used to deﬁne a mul-
tiplication operator (L𝜈G
)
(m) on f , which
may require severe constraints on f , then
f = L∗
𝜈
1
(2𝜋)(n+1)∕2 (2𝜋)(n+1)∕2 L𝜈f
= (2𝜋)(n+1)∕2 L∗
𝜈P (im + 𝜈)
(L𝜈G) (m)L𝜈f
= P (𝜕) (2𝜋)(n+1)∕2 L∗
𝜈
(L𝜈G) (m)L𝜈f .
One usually writes
G ∗f ∶= (2𝜋)(n+1)∕2 L∗
𝜈
(L𝜈G
)
(m)L𝜈f
and speaks of a convolution because, for
certain “good” right-hand sides f , G ∗f
can be written as a convolution-type inte-
gral. This deﬁnition of convolution coin-
cides with the deﬁnition given in (16.3).
Thus we get the existence of a solution in
the form
u = G ∗f .
Typical
examples
of
such
Green’s
tensors are the fundamental solutions
of the Poisson equation Δu = f in ℝ3 and
the wave equation (𝜕2
0 −Δ)u = f in ℝ1+3.
The Green’s tensor for the former is
ℝ3 ⧵{0} ∋x →−
1
4𝜋|x|,
and for the latter
ℝ⧵{0} ∋t →
1
4𝜋
√
2
1
t 𝛿C(t, ⋅),
where 𝛿C
is the distribution of inte-
gration over the “forward light cone”
C = {(t, x) ∈ℝ1+3|t > 0, |x| = t}. Thus, a
solution for Δu = f is
u(x) = −∫ℝ3
1
4𝜋|x −y|f (y)dy
(x ∈ℝ3)
and for (𝜕2
0 −Δ)u = f in ℝ1+3 is
u(t, x) = ∫ℝ3
1
4𝜋|y|f (t −|y|, x −y)dy
assuming well-behaved data f .
There is a convenient mechanism for
obtaining a Green’s tensor for systems
if the Green’s tensor for an associated
scalar problem is known. Consider an
(N + 1) × (N + 1) matrix operator P (𝜕) .
By the Caley–Hamilton theorem, we know
that P (𝜕) satisﬁes its minimal polynomial.
In other words, (treating 𝜕as if it were
an array of numbers) there is a (scalar)
polynomial
qP (𝜆) ∶= ∑d
k=0 ck (𝜕) 𝜆k
(of
smallest 3) degree) with scalar polynomials
ck such that
qP (P (𝜕)) =
d
∑
k=0
ck (𝜕) P (𝜕)k = 0.
3) If minimality of degree is not wanted, qP (𝜆) can
be replaced by the characteristic polynomial
qP (𝜆) ∶= det (P (𝜕) −𝜆) .

16.2 Partial Diﬀerential Equations in ℝn+1, n ∈ℕ, with Constant Coeﬃcients
535
If we have a Green’s tensor (fundamental
solution) g for c0 (𝜕) then, identifying g with
the (N + 1) × (N + 1) diagonal matrix with
g as diagonal entries, we obtain
𝛿{0} = c0 (𝜕) g = −
d
∑
k=1
ck (𝜕) P (𝜕)k g
= P (𝜕)
(
−
d−1
∑
s=0
cs+1 (𝜕) P (𝜕)s g
)
.
This
calculation
shows
that
G = −∑d−1
s=0 cs+1 (𝜕) P (𝜕)s g
is
a
Green’s
tensor for P (𝜕) .
As
an
example,
we
consider
an
extended 4) system based on the equations
of magnetostatics:
P (𝜕) ∶=
(
0
∇⊤
−∇
∇×
)
∶=
⎛
⎜
⎜
⎜
⎜⎝
0
𝜕1
𝜕2
𝜕3
−𝜕1
0
−𝜕3
𝜕2
−𝜕2
𝜕3
0
−𝜕1
−𝜕3
−𝜕2
𝜕1
0
⎞
⎟
⎟
⎟
⎟⎠
.
We ﬁnd
P (𝜕)2 = −(𝜕2
1 + 𝜕2
2 + 𝜕2
3
)
⎛
⎜
⎜
⎜
⎜⎝
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟⎠
showing
that
qP (𝜆) = 𝜆2 + Δ
so
that
c0 (𝜕) = Δ. Thus, with g = −1∕4𝜋| ⋅| as
the associated fundamental solution, we
obtain
G = −P (𝜕) g
as
a
Green’s
tensor
for
P (𝜕) =
( 0
∇⊤
−∇
∇×
)
.
Thus,
we
have
4) This extension is a technicality to obtain a for-
mally invertible diﬀerential operator matrix.
for y ∈ℝ3 ⧵{0}
G (y) = −
1
4𝜋|y|3
(
0
y⊤
−y
y×
)
.
Hence, a solution of
∇⊤H = 0
∇× H = J
can, for suitable divergence-free J, be given
in an obvious block matrix notation as
(
0
H (x)
)
in terms of a componentwise
convolution integral as
∫ℝ3
1
4𝜋|x −y|3
(
0
−
(
x −y
)⊤
(x −y)
−(x −y) ×
)
(
0
J (y)
)
dy,
that is,
H (x) = −∫ℝ3
1
4𝜋|x −y|3
(x −y) × J (y) dy,
which
is
the
well-known
Biot–Savart
formula.
The generality of the above solution con-
cepts (more details can be found in [11]) is
mathematically pleasing. It turns out, how-
ever, that the particular PDEs and systems
of interest in applications are from a rather
small subclass, see [12]. As a ﬁrst simpli-
fying observation, we note that we may
assume that we have a speciﬁc evolutionary
direction, which, by a simple rotation, we
always may assume to be 𝜈0 = (1, 0, … , 0) ,
that is, the direction of the “time” variable
t. By introducing higher time derivatives as
new unknowns, we may also assume that
the diﬀerential operators of interest are ﬁrst

536
16 Partial Diﬀerential Equations
order in time, that is, they are of the form
P (𝜕0, … , 𝜕n
) = 𝜕0M0 + M1 + P0
(𝜕1, … , 𝜕n
)
(16.6)
with
P0 (0) = 0,
i.e.
P (0) = M1
and
P (1, 0, … , 0) = M0. Indeed, typical prob-
lems of mathematical physics are in this
form.
As an example, let us consider the
equations of acoustics
𝜕0p + 𝜎p + ∇⊤v = f
(𝜇, 𝜎, 𝜅∈ℝ≥0)
combined with a Newton law of the form
𝜕0(𝜇v) + 𝜅v + ∇p = 0,
where ∇=
⎛
⎜
⎜⎝
𝜕1
𝜕2
𝜕3
⎞
⎟
⎟⎠
, and ∇⊤= (𝜕1 𝜕2 𝜕3
).
These can be combined in a block matrix
notation to give an equation of the form
P (𝜕)
( p
v
)
=
( f
0
)
involving the partial diﬀerential operator
P (𝜕0, 𝜕1, 𝜕2, 𝜕3
)
= 𝜕0
⎛
⎜
⎜
⎜
⎜⎝
1
(0 0 0)
⎛
⎜
⎜⎝
0
0
0
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
𝜇
0
0
0
𝜇
0
0
0
𝜇
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟⎠
+
⎛
⎜
⎜
⎜
⎜⎝
𝜎
(0 0 0)
⎛
⎜
⎜⎝
0
0
0
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
𝜅
0
0
0
𝜅
0
0
0
𝜅
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟⎠
+P0
(𝜕1, 𝜕2, 𝜕3
) ,
(16.7)
where
P0
(𝜕1, 𝜕2, 𝜕3
) ∶=
⎛
⎜
⎜
⎜
⎜⎝
0
∇⊤
∇
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟⎠
.
(16.8)
It is only after elimination of the veloc-
ity ﬁeld v (assuming 𝜕0𝜇+ 𝜅≠0) that we
obtain a familiar wave equation in pressure
p alone:
𝜇𝜕2
0p + (𝜅+ 𝜇𝜎) 𝜕0p + 𝜅𝜎p −Δp
= 𝜕0𝜇f + 𝜅f =∶g.
(16.9)
Note that if 𝜇= 0, this turns into a diﬀusion
equation
𝜅𝜕0p + 𝜂p −Δp = g
(𝜂∶= 𝜅𝜎) (16.10)
(also
known
in
diﬀerent
contexts
as
a
Fokker–Planck
equation
or
a
heat
equation). This is a typical situation in
applications: what are essentially the same
mathematical (systems of) equations may
have diﬀerent names (and units) and
interpretations in diﬀerent contexts.
The limit case 𝜅= 0, 𝜂= 1, now leads
to an invertible elliptic problem. The case
𝜅= 0, 𝜂= 0, gives Poisson’s equation,
imposing further constraints on g and
leading to nonuniqueness. Indeed, there
are inﬁnitely many polynomial solutions
in Ker (Δ) , the so-called harmonic poly-
nomials. Modifying this degenerate case
to achieve a well-posed problem is the
subject of potential theory and involves
the introduction of growth constraints on
the distributional solution p and decay
conditions for the right-hand side g.
It turns out that, in this ﬁrst-order-
in-time form, problems of interest are
characterized by a single property of
fundamental importance: strict positive
deﬁniteness in H𝜈,0
(ℝn+1), in the nonevo-
lutionary case for a particular 𝜈(say 𝜈= 0)
or, for the evolutionary case, for all 𝜈= 𝜚𝜈0
in the time direction 𝜈0 = (1, 0, … , 0) and
all suﬃciently large 𝜚∈ℝ>0. Indeed, par-
tial diﬀerential operators P (𝜕) of the form
(16.6), predominant in applications, when

16.3 Partial Diﬀerential Equations of Mathematical Physics
537
realized in H𝜈,0
(ℝn+1), all satisfy
ℜ𝔢⟨u|P (𝜕) u⟩𝜈,0 ≥c0 ⟨u|u⟩𝜈,0 = c0 |u|2
𝜈,0
(16.11)
for some c0 ∈ℝ>0 and all u ∈H𝜈,0
(ℝn+1)
such that P (𝜕) u ∈H𝜈,0
(ℝn+1). By the
Cauchy–Schwarz inequality, we have
ℜ𝔢⟨u|P (𝜕) u⟩𝜈,0 ≤|u|𝜈,0 |P (𝜕) u|𝜈,0
and so we read oﬀ
|P (𝜕) u|𝜈,0 ≥c0 |u|𝜈,0 ,
which implies invertibility of P (𝜕) and con-
tinuity of the inverse. The same result fol-
lows for the adjoint diﬀerential operator
P (𝜕)∗. Since by the projection theorem (see
[10]),
H𝜈,0
(ℝn+1) = Ran (P (𝜕))
⟂⊕Ran (P (𝜕))
with
Ran (P (𝜕))
⟂= Ker (P (𝜕)∗) = {0}
(because P (𝜕)∗is invertible), we get
|||P (𝜕)−1 f |||𝜈,0 ≤c−1
0 |f |𝜈,0
for all f ∈Ran (P (𝜕)) and, by continu-
ous extension, for all f ∈Ran (P (𝜕)) =
H𝜈,0
(ℝn+1) . Thus, the Hadamard require-
ments are satisﬁed: we have existence and
uniqueness of solution for every right-hand
side in H𝜈,0
(ℝn+1) and continuous depen-
dence on the data. As 𝜈= (𝜚, 0, … , 0), we
have
H𝜈,0
(
ℝn+1)
= H𝜚,0 (ℝ) ⊗L2 (ℝn),
(16.12)
where the tensor product (see [10]) on the
right-hand side may more intuitively be
understood as H𝜚,0−functions on ℝwith
values in L2 (ℝn) , that is, we may consider
H𝜈,0
(ℝn+1) as H𝜚,0
(ℝ, L2 (ℝn)
) noting that
⟨u|v⟩𝜈,0 = ∫ℝ
⟨u (t) |v (t)⟩L2(ℝn) exp (−2𝜚t) dt
=∶⟨u|v⟩𝜚,0,0 .
(16.13)
In the above example of the equations of
acoustics, (16.11) does not impose any
additional constraints on P0 because, in
this case, ℜ𝔢⟨u|P0
(𝜕1, 𝜕2, 𝜕3
) u⟩
𝜈,0 hap-
pens to vanish as an integration by parts
calculation
conﬁrms.
In
other
words,
A ∶= P0
(𝜕1, 𝜕2, 𝜕3
)
is
skew-self-adjoint,
that is, A = −A∗(A is skew-self-adjoint if
and only if iA is self-adjoint, compare [10]).
That the spatial operator A is skew-self-
adjoint is not exceptional but rather the
general situation in mathematical physics
due to the typical Hamiltonian structure
of A as a block operator matrix of the
form
(0
−C∗
C
0
)
where C is a closed,
densely deﬁned Hilbert space operator. We
shall explore this more deeply in the next
section.
16.3
Partial Diﬀerential Equations of
Mathematical Physics
As discussed in the previous section, strict
positive deﬁniteness is at the heart of the
solution theory for PDEs of mathematical
physics. Generalization to the case of phe-
nomena conﬁned to an open region Ω in ℝn
(boundary value problems) and to media
with properties varying with their location
in space (variable coeﬃcients) is straight-
forward. The typical Hamiltonian structure
is usually preserved by a suitable choice of
boundary conditions. Limit cases such as
static or stationary solutions may need spe-
ciﬁc strategies (e.g., decay constraints or

538
16 Partial Diﬀerential Equations
radiation conditions). Generic cases, how-
ever, enjoy the same simple structure mak-
ing them accessible to a uniﬁed solution
theory.
The abstract solution theory for a general
strictly positive deﬁnite, closed and densely
deﬁned linear operator with a strictly
positive deﬁnite adjoint ∗follows as above
by simply replacing P (𝜕) by . Somewhat
miraculously, this simple idea suﬃces to
understand the main PDEs of mathematical
physics. We assume that the time direction
is ﬁxed and 𝜕0 is the associated derivative
(i.e., t = x0 and 𝜕0 is the time derivative).
To leave the spatial part as general as pos-
sible, we merely assume that we are deal-
ing with suitable functions on ℝ(the time
domain) with values in a Hilbert space X.
In comparison with (16.12), (16.13) above,
this amounts to replacing L2 (ℝn) by an
arbitrary Hilbert space so that we are con-
cerned with a solution theory in a Hilbert
space H𝜚,0 (ℝ, X) with inner product
⟨u|v⟩𝜚,0,0 ∶= ∫ℝ
⟨u (t) |v (t)⟩X exp (−2𝜚t) dt.
(16.14)
Thus we have an “ordinary” diﬀerential
equation on ℝin a Hilbert space X.
Following the structure introduced in
(16.6), we focus on the particular class of
abstract diﬀerential operators
= 𝜕0M0 + M1 + A,
(16.15)
where A is the canonical extension, to
the time-dependent case, of a closed
and densely deﬁned linear diﬀerential
operator A0 ∶D (A0
) ⊆X →X, that is,
(Au) (t) = A0u (t) for (almost every) t ∈ℝ,
and similarly M0, M1 are the canonical
extensions to the time-dependent case of
a bounded linear operator in X. From the
observation at the end of the last section,
we may focus our interest on the case
where A0 and consequently A is skew-self-
adjoint. Indeed, the typical shape of A0 is in
a block matrix operator form
(0
−C∗
C
0
)
.
Frequently, this is not obvious in historical
formulations of the equations of mathemat-
ical physics. For example, the Schrödinger
equation, involving an operator of the form
𝜕0 + i L,
where L is nonnegative and self-adjoint,
appears not to ﬁt in the above setting. How-
ever, separating real and imaginary part of
the equation
(
𝜕0 + i L
)
u = f
yields the system
(
𝜕0 +
( 0
−L
L
0
)) ( ℜ𝔢u
ℑ𝔪u
)
=
( ℜ𝔢f
ℑ𝔪f
)
,
where
A =
( 0
−L
L
0
)
has the required block matrix form because
L is self-adjoint.
Given that A is skew-self-adjoint, the
strict positive deﬁniteness of and ∗
(see (16.15)) reduces to that of 𝜕0M0 + M1,
which is equivalent to requiring
⟨u|𝜚M0 + ℜ𝔢M1u⟩𝜚,0,0 ≥c0 ⟨u|u⟩𝜚,0,0
(16.16)
for all u ∈H𝜚,0 (ℝ, X) and all suﬃciently
large 𝜚∈ℝ>0 as a constraint on the self-
adjoint, bounded linear operators M0,
ℜ𝔢M1 ∶= 1
2
(M1 + M∗
1
) in X. Note that
considering ℜ𝔢M1 has little to do with the
entries of M1 being real or complex. For

16.3 Partial Diﬀerential Equations of Mathematical Physics
539
example, take M1 to be some simple 2 × 2
matrices:
ℜ𝔢
( 1
−1
0
1
)
= 1
2
(
2
−1
−1
2
)
,
ℜ𝔢
( i
−i
0
1
)
= 1
2
( 0
−i
i
2
)
.
If there is a need for more general mate-
rial laws, we may, by a simple perturbation
argument,
include
arbitrary
additional
bounded linear operators on H𝜚,0(ℝ, X).
Such an operator may result from a linear
operator 𝕄mapping X-valued step func-
tions (i.e., linear combinations of functions
of the form 𝜒
I ⊗h ∶= t →𝜒
I (t) h, where
𝜒I denotes the characteristic function of a
bounded interval I ⊆ℝand h ∈X) into
⋂
𝜚≥𝜚0 H𝜚,0 (ℝ, X). Assuming the estimate
|𝕄𝜙|𝜚,0,0 ≤c (𝜚) |𝜙|𝜚,0,0
for all X-valued step functions 𝜙for suitable
constants c(𝜚) satisfying
lim sup
𝜚→∞
c (𝜚) < c0,
(16.17)
we can continuously extend 𝕄to an oper-
ator
̃
M ∶H𝜚,0(ℝ, X) →H𝜚,0(ℝ, X).
The
operator
̃M can model time-dependent
material laws or memory eﬀects. Let us
treat an example. For h < 0, we consider
the operator 𝕄∶= 𝜏h of time translation,
mapping an X-valued step function u to
the step function t →u(t + h). Then for
𝜚> 0, we compute
|𝜏hu|2
𝜚,0,0 = ∫
ℝ
|u(t + h)|2e−2𝜚t dt
= e2𝜚h|u|2
𝜚,0,0
showing that
lim sup
𝜚→∞
c(𝜚) = lim
𝜚→∞e2𝜚h = 0 < c0,
because h < 0.
A modiﬁed would then be of the
form 𝜕0M0 + M1 + ̃M + A. Abbreviating
M0 + 𝜕−1
0
(M1 + ̃M) as (𝜕−1
0
), we can
rewrite this generalized version of simply
as 𝜕0
(
𝜕−1
0
) + A and so we may reformu-
late the problem to be solved as ﬁnding
U, V ∈H𝜚,0 (ℝ, X) such that
𝜕0V + AU = F,
(16.18)
where the “material law”
V = (𝜕−1
0
) U
(16.19)
is satisﬁed.
This perturbation argument can actually
be made more useful by reﬁning the strict
positive deﬁniteness condition. An integra-
tion by parts calculation yields
ℜ𝔢⟨U| (𝜕0M0 + M1
) U⟩
𝜚,0,0
= 𝜚⟨U|M0U⟩𝜚,0,0 + ⟨U| (ℜ𝔢M1
) U⟩
𝜚,0,0
≥d1𝜚⟨PU|PU⟩𝜚,0,0
+ d2 ⟨(1 −P) U| (1 −P) U⟩𝜚,0,0 ,
where P is the orthogonal projector onto
Ran (M0), d1 > 0 depends on the positive
deﬁniteness constant for M0 restricted
to the subspace Ran (M0), and d2 > 0
depends on the positive deﬁniteness con-
stant for ℜ𝔢M1 restricted to the subspace
Ker (M0
) = (Ran (M0))⟂. Thus, more gen-
eral perturbations ̃M can be considered
if ̃MP is a bounded linear operator and
̃M (1 −P) is a bounded linear operator with
a bound ‖‖‖ ̃M (1 −P)‖‖‖ less than d2 uniformly
for all suﬃciently large 𝜚.

540
16 Partial Diﬀerential Equations
16.4
Initial-Boundary Value Problems of
Mathematical Physics
As we have indicated, the equations of
mathematical physics have not in general
been formulated as the above type of ﬁrst-
order systems and it can be a formidable
task to rewrite the system to display the
generic
Hamiltonian
structure.
As
an
example, we discussed the system of acous-
tics. What is the impact of our abstract
considerations on this particular example?
The system already has the required form
with very simple operators M0, M1, which
are just multiplication by diagonal matrices.
For the spatial operator
A ∶=
⎛
⎜
⎜
⎜
⎜⎝
0
∇⊤
∇
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟⎠
(16.20)
to be skew-self-adjoint a suitable bound-
ary condition on the underlying domain
Ω ⊆ℝ3 must be chosen. Vanishing of the
pressure distribution p on the boundary
(Dirichlet boundary condition) or the nor-
mal component of the velocity ﬁeld (Neu-
mann boundary condition for p) are typical
choices resulting in skew-self-adjointness
of A.
From a mathematical perspective, we can
have general operators M0, M1, which may
be such that there is little chance of recov-
ering anything close to a well-known wave
equation. A simple special case would be
that all entries in the (not necessarily block
diagonal) operator matrices M0, M1 are
actually bounded multiplication operators
in space, that is, we consider coeﬃcients
in the usual sense, which are varying in
space. Non-block-diagonal situations have
been investigated in recent times in studies
of metamaterials, see for example, [13, 14].
To illustrate some issues for the problem
class under consideration, we look at a
very speciﬁc “wave equation” more closely.
Of course, for computational purposes, it
may be interesting to have more or less
explicit representation formulae for the
solutions. Such formulae, however, cannot
be expected in the general case. Even in the
block diagonal case, an example such as
𝜕0
( ℙΩ1
0
0
ℙΩ2
)
+
( ℙΩ⧵Ω1
0
0
ℙΩ⧵Ω2
)
+ A
for measurable sets Ω1, Ω2 ⊆Ω, where
ℙS
denotes
the
orthogonal
projector
generated
by
multiplication
with
the
characteristic function 𝜒
S
of a subset
S ⊆ℝn, illustrates the problems involved.
In (Ω ⧵Ω1
) ∩(Ω ⧵Ω2
) the time derivative
vanishes (elliptic case), in Ω1 ∩Ω2 we
have reversible wave propagation (hyper-
bolic case) and everywhere else, that is, in
(Ω1 ⧵Ω2
) ∪(Ω2 ⧵Ω1
), we have a system
associated with a diﬀusion (parabolic case).
However, the complete system is covered
by our general framework.
Returning to our general form 𝜕0M0 +
M1 + ̃M + A with A from (16.20), we note
that the crucial concept of causality char-
acterizing evolutionary processes is main-
tained for this abstract class in the sense
that, for all a ∈ℝ, if the right-hand side
(as an X-valued function) vanishes on the
open interval ] −∞, a[ then so does the
solution. Owing to time translation invari-
ance (which is satisﬁed for M0, M1, A and
which we assume to hold for ̃M), it suﬃces
to consider a = 0. The above yields that,
in a suitable sense, the term M0U associ-
ated with the solution U for a given right-
hand side F vanishing on ℝ≤0 must – as a
result its continuity – vanish on ℝ≤0 and
so, in particular, at time 0. Note that, if M0
has a nontrivial null space, U itself may be
discontinuous and so, although U (0−) = 0,

16.4 Initial-Boundary Value Problems of Mathematical Physics
541
U (0) and consequently U (0+) may not be
deﬁned. From the perspective of classical
initial value problems, U satisﬁes the initial
condition
(M0U)
(0+) = 0.
(16.21)
Thus,
the
solution
U
of
(𝜕0M0 + M1 + ̃M +A) U = F with F van-
ishing on (−∞, 0] satisﬁes homogeneous
initial conditions. How can we imple-
ment nonzero initial data? We think of
nonvanishing initial data as a jump in the
solution occurring at time 0. Noting that
the derivative of a constant is 0, we have
𝜕0M0
(
U −𝜒
ℝ>0 ⊗U0
)
= 𝜕0M0U on ℝ>0.
Thus, we may consider
𝜕0M0
(
U −𝜒ℝ>0 ⊗U0
)
+M1U + ̃MU + AU = F
(16.22)
as the proper formulation for the initial
value problem
𝜕0M0U + M1U + ̃MU + AU = F on ℝ>0,
M0U (0+) = M0U0.
Recall that ̃M is a suﬃciently small per-
turbation in the sense of property (16.17).
As 𝜕0𝜒
ℝ>0 is the Dirac-𝛿-distribution 𝛿{0},
problem (16.22) is formally equivalent to
(𝜕0M0 + M1 + ̃M + A) U
= F + 𝛿{0} ⊗M0U0.
(16.23)
This yields the interpretation that initial
data correspond to Dirac 𝛿{0} distribution-
type
sources.
However,
rather
than
discussing
possible
generalizations
of
our above solution theory to distribu-
tions, we prefer to reformulate (16.22) as a
problem for ﬁnding V ∶= U −𝜒
ℝ>0 ⊗U0.
Assuming U0 ∈D
(
A0
), we obtain
(𝜕0M0 + M1 + ̃M + A) V
= F −𝜒ℝ>0 ⊗(M1 + A) U0
−̃M
(
𝜒ℝ>0 ⊗U0
)
=∶G,
which is covered by the above solution
theory.
Alternatively, introducing the perturba-
tion argument in a diﬀerent way, we have
V =
(
U −𝜒ℝ>0 ⊗U0
)
= (𝜕0M0 + M1 + A)−1 G
−(𝜕0M0 + M1 + A)−1 ̃MV
=∶T (V) ,
where
T,
by
our
assumptions,
is
a
contraction, that is,
|T (V)|𝜚,0,0 ≤q |V|𝜚,0,0
for
some
q < 1
independent
of
V ∈H𝜚,0 (ℝ, X) for all suﬃciently large
𝜚. Hence, by the contraction mapping
theorem, the solution V can be found
iteratively:
V = lim
k→∞Tk (V0
)
for arbitrary choice of V0 ∈H𝜚,0 (ℝ, X) .
The solution U of the initial value problem
(16.22) is then
U = V + 𝜒
ℝ>0 ⊗U0.
Let us return to the simpler problem with
̃M = 0. In this case, the underlying media,
assumed to satisfy (16.16), can be roughly
categorized by properties of M0, M1 as
follows.

542
16 Partial Diﬀerential Equations
Lossless media: ℜ𝔢M1 = 0;
then, for media that are not lossless, we
have,
Lossy media: ℜ𝔢M1 ≥0;
Gainy media: ℜ𝔢M1 ≤0;
Chiral media: ℑ𝔪M1 ≠0.
The case ̃M = 0 also exhibits “energy
conservation.” Indeed, if F vanishes above
a time threshold t0, then we have the
following
pointwise
relation
for
the
solution U:
1
2 ⟨U (b) |M0U (b)⟩X
+ ∫
b
a
⟨U (s) |ℜ𝔢M1U (s)⟩X ds
= 1
2 ⟨U (a) |M0U (a)⟩X
for b ≥a ≥t0.
While we have concentrated so far on
the acoustic case (16.20), the underlying
considerations
are
completely
general.
Clearly ∇can be considered in higher (or
lower) dimensions. Further, we could for
example also replace ∇by some other
diﬀerential operator C (and ∇⊤by −C∗)
to obtain other well-known systems of
equations from mathematical physics. To
illustrate, we shall consider two other cases
more closely.
16.4.1
Maxwell’s Equations
First, we consider Maxwell’s equations in
the isotropic, homogeneous case. These can
be written in 2 × 2−block matrix operator
form
(𝜕0M0 + M1 + A) ( E
H
)
=
( −J0
0
)
,
(16.24)
where
M0 ∶=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
⎛
⎜
⎜⎝
𝜀
0
0
0
𝜀
0
0
0
𝜀
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
𝜇
0
0
0
𝜇
0
0
0
𝜇
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
,
M1 ∶=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
⎛
⎜
⎜⎝
𝜎
0
0
0
𝜎
0
0
0
𝜎
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
,
(𝜀, 𝜇∈ℝ>0, 𝜎∈ℝ≥0) and
A ∶=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
∇×
−∇×
⎛
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
,
(16.25)
where ∇× ∶=
⎛
⎜
⎜⎝
0
−𝜕3
𝜕2
𝜕3
0
−𝜕1
−𝜕2
𝜕1
0
⎞
⎟
⎟⎠
. By a
suitable choice of boundary condition,
which commonly is the vanishing of the
tangential component of the electric ﬁeld
E on the boundary of the underlying open
set Ω ⊆ℝ3 (electric boundary condition,
boundary condition of total reﬂexion), the
operator A can be established as skew-self-
adjoint in a six-component L2 (Ω)-space.
The material laws (𝜕−1
0
) can be arbitrary
as long as (16.16) is satisﬁed. Thus, in this
general formulation, the most complex
materials (chiral media, metamaterials)
become accessible.

16.4 Initial-Boundary Value Problems of Mathematical Physics
543
16.4.2
Viscoelastic Solids
The system of linearized viscoelasticity is
commonly presented as
Div T + f = 𝜚0 𝜕2
0u,
where u denotes the displacement ﬁeld,
T = (Tjk
)
j,k∈{1,2,3} is the stress tensor, 𝜚0
is
the
mass
density
and
Div T ∶=
(∑3
k=1 𝜕kTjk
)
j∈{1,2,3} denotes the tensorial
divergence (in Cartesian coordinates).
With v ∶= 𝜕0u, we ﬁrst derive from the
deﬁnition
∶= Grad u,
where
Grad u ∶= 1
2
(𝜕⊗u + (𝜕⊗u)⊤)
denotes the symmetric part of the Jacobi
matrix 𝜕⊗u, another ﬁrst-order dynamic
equation
𝜕0= Grad v.
We can formally summarize the resulting
system in the form
𝜕0
( w

)
+
(
0
−Div
−Grad
0
) ( v
T
)
=
( f
0
)
with w = 𝜚0 v. The system is completed by
linear material relations of various forms
linking and T. We follow the presentation
in [15].
16.4.2.1
The Kelvin–Voigt Model
This class of materials is characterized by a
material relation of the form
T = C+ D𝜕0,
(16.26)
where the elasticity tensor C and the vis-
cosity tensor D are assumed to be modeled
as bounded, self-adjoint, strictly positive
deﬁnite mappings in a Hilbert space Xsym of
L2 (Ω)-valued, symmetric 3 × 3-matrices,
with the inner product induced by the
Frobenius norm
(Φ, Ψ) →∫Ω
trace
(
Φ (x)∗Ψ (x)
)
dx .
By a suitable choice of boundary condi-
tion, that is, domain for
(
0
−Div
−Grad
0
)
,
we can, as in previous cases, achieve
skew-self-adjointness
in
the
Hilbert
space
X ∶= L2 (Ω) ⊕L2 (Ω) ⊕L2 (Ω) ⊕Xsym.
For sake of deﬁniteness, let us consider the
vanishing of the displacement velocity at
the boundary of the domain Ω containing
the medium. With this choice of operator
domain D (A),
A ∶=
(
0
−Div
−Grad
0
)
is skew-self-adjoint in H𝜚,0 (ℝ, X).
For D ≥c0 > 0, we obtain from (16.26)
that
= (C + D𝜕0
)−1 T,
= 𝜕−1
0
(𝜕−1
0 C + D)−1 T,
which amounts to a “material law” of the
form
( w

)
=
( 𝜚0
0
0
0
) ( v
T
)
+ 𝜕−1
0
(( 0
0
0
D−1
)
+ ̃M
) ( v
T
)

544
16 Partial Diﬀerential Equations
with
̃M = 𝜕−1
0
( 0
0
0
−(𝜕−1
0 C + D
)−1 C D−1
)
.
This is the so-called Kelvin–Voigt model
of viscoelasticity. The case C = 0 leads
to a system for a purely viscous behavior
(Newton model). On the other hand, if C
is strictly positive deﬁnite, then the limit
case D = 0 leads to the standard system for
elastic solids.
16.4.2.2
The Poynting–Thomson Model
(The Linear Standard Model)
The
linear
standard
model
or
Poynting–Thomson model is based on
a generalization of the Maxwell model
involving another coeﬃcient operator R
and has the form
𝜕0+ R = C−1𝜕0T + D−1T.
Solving for , this yields
= (𝜕0 + R)−1 (C−1𝜕0 + D−1) T = C−1T
+ 𝜕−1
0
(
1 + R 𝜕−1
0
)−1 (
D−1 −RC−1)
T
leading to a slightly more complex material
law
(
w

)
=
(𝜚0 0
0 C−1
)( v
T
)
+ 𝜕−1
0
((0
0
0
D−1 −RC−1
)
+ ̃M
)( v
T
)
with
̃M = 𝜕−1
0
(
0
0
0
−R (1 + R 𝜕−1
0
)−1 (D−1 −RC−1)
)
.
16.5
Coupled Systems
An understanding of the speciﬁc mathe-
matical form that material laws must take
allows for a transparent discussion on how
to couple diﬀerent physical phenomena
to obtain a suitable evolutionary prob-
lem. Here we shall focus on the abstract
structure of coupled systems (compare
[16] for a discussion of coupled systems of
mathematical physics).
Without coupling, systems of interest
can be combined simply by writing them
together in diagonal block operator matrix
form:
𝜕0
⎛
⎜
⎜
⎜
⎜⎝
V0
⋮
⋮
Vn
⎞
⎟
⎟
⎟
⎟⎠
+ A
⎛
⎜
⎜
⎜
⎜⎝
U0
⋮
⋮
Un
⎞
⎟
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜
⎜⎝
f0
⋮
⋮
fn
⎞
⎟
⎟
⎟
⎟⎠
,
where
A =
⎛
⎜
⎜
⎜
⎜⎝
A0
0
· · ·
0
0
⋱
⋮
⋮
⋱
0
0
· · ·
0
An
⎞
⎟
⎟
⎟
⎟⎠
inherits
the
skew-self-adjointness
in
X = ⨁n
k=0 Xk from its skew-self-adjoint
diagonal entries Ak ∶D (Ak
) ⊆Xk →Xk,
k ∈{0, … , n}. The combined material law
takes the simple Block-diagonal form
⎛
⎜
⎜
⎜
⎜⎝
V0
⋮
⋮
Vn
⎞
⎟
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜
⎜⎝
00
(𝜕−1
0
) 0 · · ·
0
0
⋱
⋮
⋮
⋱
0
0
· · · 0 nn
(𝜕−1
0
)
⎞
⎟
⎟
⎟
⎟⎠
⎛
⎜
⎜
⎜
⎜⎝
U0
⋮
⋮
Un
⎞
⎟
⎟
⎟
⎟⎠
.

16.5 Coupled Systems
545
Coupling between the phenomena now
can be modeled by introducing suitable oﬀ-
diagonal entries. The full material law now
is of the familiar form
⎛
⎜
⎜
⎜
⎜⎝
V0
⋮
⋮
Vn
⎞
⎟
⎟
⎟
⎟⎠
= (𝜕−1
0
)
⎛
⎜
⎜
⎜
⎜⎝
U0
⋮
⋮
Un
⎞
⎟
⎟
⎟
⎟⎠
with
(𝜕−1
0
)∶=
⎛
⎜
⎜
⎜⎝
00
(𝜕−1
0
) · · · · · · 0n
(𝜕−1
0
)
⋮
⋱
⋮
⋮
⋱
⋮
n0
(𝜕−1
0
) · · · · · · nn
(𝜕−1
0
)
⎞
⎟
⎟
⎟⎠
.
We consider some applications.
16.5.1
Thermoelasticity
As a ﬁrst illustration, we consider the
general thermoelastic system
𝜕0V +
⎛
⎜
⎜
⎜
⎜⎝
0
Div 0
0
Grad
0
0
0
0
0
0 ∇⊤
0
0
∇
0
⎞
⎟
⎟
⎟
⎟⎠
⎛
⎜
⎜
⎜
⎜⎝
v
T
𝜃
Q
⎞
⎟
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜
⎜⎝
f
0
g
0
⎞
⎟
⎟
⎟
⎟⎠
,
where we assume that the domains of oper-
ators containing the spatial derivatives are
such that we model, for example, Dirich-
let boundary conditions for the displace-
ment velocity and the temperature in order
to maintain skew-self-adjointness of
⎛
⎜
⎜
⎜
⎜⎝
0
Div
0
0
Grad
0
0
0
0
0
0
∇⊤
0
0
∇
0
⎞
⎟
⎟
⎟
⎟⎠
.
The material law is of the form
V = (𝜕−1
0
)
⎛
⎜
⎜
⎜
⎜⎝
v
T
𝜃
Q
⎞
⎟
⎟
⎟
⎟⎠
,
where (𝜕−1
0
) is given by
⎛
⎜
⎜
⎜
⎜⎝
𝜚0
0
0
0
0
C−1
C−1Γ
0
0 Γ∗C−1 w + Γ∗C−1Γ
0
0
0
0
q0 + q2
(𝛼+ 𝛽𝜕0
)−1
⎞
⎟
⎟
⎟
⎟⎠
.
Via symmetric row and column opera-
tions, this can be reduced to the Block-
diagonal form
⎛
⎜
⎜
⎜
⎜⎝
𝜚0
0
0
0
0
C−1
0
0
0
0
w
0
0
0
0
q0 + q2
(𝛼+ 𝛽𝜕0
)−1
⎞
⎟
⎟
⎟
⎟⎠
and so the issue of classifying the material
is simpliﬁed. For example, the issue of strict
positive deﬁniteness of
M0 =
⎛
⎜
⎜
⎜
⎜⎝
𝜚0
0
0
0
0
C−1
C−1Γ
0
0
Γ∗C−1
w + Γ∗C−1Γ
0
0
0
0
q0
⎞
⎟
⎟
⎟
⎟⎠
hinges on the strict positive deﬁniteness of
𝜚0, C, w, q0.
For q0 = 0, the above system is known
as a type-3 thermoelastic system. With 𝛼=
0, we obtain the special case of thermoe-
lasticity with second sound, that is, with
the Cataneo modiﬁcation of heat transport.
The so-called type-2 thermoelastic system
results by letting q2 = 0.
We point out that the well-known Biot
system, see, for example, [17–19], which
describes consolidation of a linearly elastic

546
16 Partial Diﬀerential Equations
porous medium, can be reformulated so
that, up to physical interpretations, it has
the same form as the thermoelastic sys-
tem. The coupling operator Γ of thermoe-
lasticity is, in the poroelastic case, given as
Γ =
⎛
⎜
⎜⎝
𝛼
0
0
0
𝛼
0
0
0
𝛼
⎞
⎟
⎟⎠
, where 𝛼is a coupling
parameter.
16.5.2
Piezoelectromagnetism
As a second class of examples we consider
the coupling of elastic and electromagnetic
wave propagation. Here, we have a system
of the form
𝜕0V + A
⎛
⎜
⎜
⎜
⎜⎝
v
T
E
H
⎞
⎟
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜
⎜⎝
f
0
−J
0
⎞
⎟
⎟
⎟
⎟⎠
,
where, by a suitable choice of boundary
conditions, A will be a skew-self-adjoint
block operator matrix of the form
A =
⎛
⎜
⎜
⎜
⎜⎝
0
Div
0
0
Grad
0
0
0
0
0
0
∇×
0
0
−∇×
0
⎞
⎟
⎟
⎟
⎟⎠
.
This system needs to be completed by suit-
able material relations. A simple piezoelec-
tromagnetic model is described by
V =
⎛
⎜
⎜
⎜⎝
𝜚0
0
0
0
0
C−1
C−1d
0
0 d∗C−1 𝜀+ d∗C−1d + 𝜕−1
0 𝜎0
0
0
0
𝜇
⎞
⎟
⎟
⎟⎠
⎛
⎜
⎜
⎜⎝
v
T
E
H
⎞
⎟
⎟
⎟⎠
with the coupling given by a bounded linear
mapping d from L2 (Ω) ⊕L2 (Ω) ⊕L2 (Ω)
to Xsym .
Following [20], we obtain a more com-
plicated coupling mechanism. Adding a
conductivity term, the coupling is initially
described in the form
T = C −dE −q H,
D = d∗+ 𝜀E + e H + 𝜕−1
0 𝜎E,
B = q∗+ e∗E + 𝜇H.
Domain and range spaces for the additional
bounded, linear coeﬃcient operators q and
e are clear from these equations and, for
sake of brevity, we shall not elaborate on
this. As has been already noted, for a proper
formulation we need to solve for to obtain
suitable material relations. We ﬁnd
= C−1T + C−1dE + C−1qH,
D = d∗C−1T + (𝜀+ d∗C−1d) E
+ d∗C−1q H + e H + 𝜕−1
0 𝜎E,
B = q∗C−1T + q∗C−1dE
+ q∗C−1q H + e∗E + 𝜇H.
Thus, we obtain the material law
V = (𝜕−1
0
)
⎛
⎜
⎜
⎜
⎜⎝
v
T
E
H
⎞
⎟
⎟
⎟
⎟⎠
with
(𝜕−1
0
)
=
⎛
⎜
⎜⎝
𝜚0
0
0
0
0
C−1
C−1d
C−1q
0 d∗C−1 (𝜀+ d∗C−1d) + 𝜕−1
0 𝜎d∗C−1q + e
0 q∗C−1
q∗C−1d + e∗
𝜇+ q∗C−1q
⎞
⎟
⎟⎠
.
Via symmetric row and column operations,
we obtain the block-diagonal operator

16.5 Coupled Systems
547
matrix
⎛
⎜
⎜
⎜
⎜⎝
𝜚0
0
0
0
0
C−1
0
0
0
0
𝜀+ 𝜕−1
0 𝜎
0
0
0
0
𝜇−e∗𝜀−1e
⎞
⎟
⎟
⎟
⎟⎠
.
Thus, the given form of material relations
leads to a material law in the above sense if
in addition to the strict positive deﬁniteness
of the self-adjoint bounded operators 𝜚0, C,
𝜀, and 𝜇we require
𝜇≥𝜇0 + e∗𝜀−1e
for some constant 𝜇0 ∈ℝ>0. Again, we
emphasize that, without additional eﬀort,
more complex material relations in the
sense of the above theory could be consid-
ered.
16.5.3
The Extended Maxwell System and its Uses
It has been shown [21] that Maxwell’s
equations in an open domain Ω ⊆ℝ3 may
be formulated as a formally coupled system
of PDEs. For this, we introduce the formal
operator matrices
AD ∶=
⎛
⎜
⎜
⎜
⎜⎝
0
∇⊤
0
0
∇
0
0
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟⎠
,
AN ∶=
⎛
⎜
⎜
⎜
⎜⎝
0
0
0
0
0
0
0
0
0
0
0
∇
0
0
∇⊤
0
⎞
⎟
⎟
⎟
⎟⎠
,
AE ∶=
⎛
⎜
⎜
⎜
⎜⎝
0
0
0
0
0
0
−∇×
0
0
∇×
0
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟⎠
,
subject
to
boundary
conditions:
AD
inherits homogeneous Dirichlet boundary
conditions for ∇, AN is endowed with
homogeneous Neumann boundary condi-
tions for ∇⊤, and the lower left operator in
the operator matrix AE carries the electric
boundary condition. Now, assuming for
simplicity that all material parameters
are set to one, it turns out that Maxwell’s
equation (𝜕0 + AE
) u = f can be written as
(𝜕0 + AE + AD + AN
) U = F,
where
U = (0, u, 0)
and
F = ̃f + 𝜕−1
0
(AN + AD
)̃f with ̃f = (0, f , 0).
This
so-called
extended
Maxwell’s
equation gives a link to the Dirac equation.
Indeed, we compute
(𝜕0 + AE + AD + AN
)(𝜕0 −(AE + AD + AN
))
= 𝜕2
0 −(AE + AD + AN
)2
= 𝜕2
0 −Δ.
Thus,
the
extended
Maxwell
system
(𝜕0 + AE + AD + AN
)
corresponds
to
a
0-mass Dirac equation.
By appropriate permutation of rows and
columns the more general mass 1 Dirac
equation admits the form
(
𝜕0 +
(0 −S∗
S
0
)
+ AE + AD + AN
)
V = G,
where
S =
⎛
⎜
⎜
⎜
⎜⎝
0
(
0
0
1
)
⎛
⎜
⎜⎝
0
0
1
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
0
1
0
−1
0
0
0
0
0
⎞
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟⎠
.

548
16 Partial Diﬀerential Equations
Finally, the Maxwell–Dirac system also
shares a similar form. Consider the system
(𝜕0 + A)
⎛
⎜
⎜
⎜
⎜⎝
0
E
H
0
⎞
⎟
⎟
⎟
⎟⎠
= f ,
(𝜕0 + M1 + A) Ψ = g,
(16.27)
(
𝜕0 −A)
⎛
⎜
⎜
⎜
⎜⎝
𝜑
𝛼
0
0
⎞
⎟
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜
⎜⎝
0
E
H
0
⎞
⎟
⎟
⎟
⎟⎠
,
where A = (AE + AD + AN
) and f and g are
suitable source terms. As 𝜕0 and A com-
mute, the ﬁrst and third equations can be
combined to give
(𝜕2
0 −A2)
⎛
⎜
⎜
⎜
⎜⎝
𝜑
𝛼
0
0
⎞
⎟
⎟
⎟
⎟⎠
= f ,
and trivially we have
(A𝜕0 −𝜕0A)
⎛
⎜
⎜
⎜
⎜⎝
𝜑
𝛼
0
0
⎞
⎟
⎟
⎟
⎟⎠
= 0.
These last two equations can be written as
(
𝜕0+
( 0
A
A
0
))
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
𝜕0
⎛
⎜
⎜
⎜
⎜⎝
𝜑
𝛼
0
0
⎞
⎟
⎟
⎟
⎟⎠
−A
⎛
⎜
⎜
⎜
⎜⎝
𝜑
𝛼
0
0
⎞
⎟
⎟
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
=
( f
0
)
.
Inserting the second equation from (16.27)
gives
⎛
⎜
⎜⎝
𝜕0 + ̃M1 +
⎛
⎜
⎜⎝
0
0
A
0
A
0
A
0
0
⎞
⎟
⎟⎠
⎞
⎟
⎟⎠
U =
⎛
⎜
⎜⎝
f
g
0
⎞
⎟
⎟⎠
,
where
̃M1 =
⎛
⎜
⎜⎝
0
0
0
0
M1
0
0
0
0
⎞
⎟
⎟⎠
and
U =
⎛
⎜
⎜⎝
U0
U1
U2
⎞
⎟
⎟⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
𝜕0
⎛
⎜
⎜
⎜
⎜⎝
𝜑
𝛼
0
0
⎞
⎟
⎟
⎟
⎟⎠
Ψ
−A
⎛
⎜
⎜
⎜
⎜⎝
𝜑
𝛼
0
0
⎞
⎟
⎟
⎟
⎟⎠
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
.
Coupling occurs here in an essential way via
a suitable quadratic nonlinear dependence
of f and g on the solution U. The middle
component U1 = Ψ is the Dirac ﬁeld, the
electromagnetic ﬁeld can be recovered
as U0 + U2 =
⎛
⎜
⎜
⎜
⎜⎝
0
E
H
0
⎞
⎟
⎟
⎟
⎟⎠
, and the so-called
potential can be obtained by integrating
U0 = 𝜕0
⎛
⎜
⎜
⎜
⎜⎝
𝜑
A
0
0
⎞
⎟
⎟
⎟
⎟⎠
.
References
1. Constanda, C. (2010) Solution Techniques for
Elementary Partial Diﬀerential Equations,
Taylor and Francis.
2. Haberman, R. (2004) Applied Partial
Diﬀerential Equations, Prentice Hall.

References
549
3. Weinberger, H.F. (1995) A First Course in
Partial Diﬀerential Equations, Dover, New
York.
4. Zauderer, E. (1998) Partial Diﬀerential
Equations of Applied Mathematics,
Wiley-Interscience, New York.
5. Garabedian, P.R. (1964) Partial Diﬀerential
Equations, John Wiley & Sons , Inc., New
York.
6. Hormander, L. (1990) The Analysis of Linear
Partial Diﬀerential Operators I,
Springer-Verlag.
7. Leis, R. (1986) Initial Boundary Value
Problems in Mathematical Physics, John
Wiley and Sons Ltd and B. G. Teubner,
Stuttgart.
8. Showalter, R.E. (1977) Hilbert Space
Methods for Partial Diﬀerential Equations,
Pitman.
9. Mathematical Transformations and Their
Uses. This handbook, Chapter 15.
10. Exner, P. Functional Analysis. This
handbook, Chapter 13.
11. Picard, R. and McGhee, D. (2011) Partial
Diﬀerential Equations: A Uniﬁed Hilbert
Space Approach, de Gruyter Expositions in
Mathematics 55. de Gruyter, Berlin, xviii.
12. Picard, R. (2009) A structural observation
for linear material laws in classical
mathematical physics. Math. Methods Appl.
Sci., 32 (14), 1768–1803.
13. Lindell, I.V., Sihvola, A.H., Tretyakov, S.A.,
and Viitanen, A.J. (1994) Electromagnetic
waves in chiral and bi-isotropic media,
Artech House, Boston, MA and London.
14. Lakhtakia, A. (1994) Beltrami Fields in
Chiral Media, World Scientiﬁc, Singapore.
15. Bertram, A. (2005) Elasticity and Plasticity
of Large Deformations: An Introduction,
Springer, Berlin.
16. Bednarcyk, B.A. (2002) A Fully Coupled
Micro/Macro Theory for
Thermo-Electro-Magneto-Elasto-Plastic
Composite Laminates. Technical Report
211468, NASA.
17. Biot, M.A. (1941) General theory of
three-dimensional consolidation. J. Appl.
Phys., Lancaster, PA, 12, 155–164.
18. Showalter, R.E. (2000) Diﬀusion in
poro-elastic media. J. Math. Anal. Appl., 251
(1), 310–340.
19. Bear, J. and Bachmat, Y. (1990) Introduction
to Modelling of Transport Phenomena in
Porous Media, Theory and Applications of
Transport in Porous Media 4, Kluwer
Academic Publishers, Dordrecht etc.
20. Pan, E. and Heyliger, P.R. (2003) Exact
solutions for magneto-electro-elastic
laminates in cylindrical bending. Int. J. Solids
Struct., 40 (24), 6859–6876.
21. Picard, R. (1984) On the low frequency
asymptotics in electromagnetic theory. J.
Reine Angew. Math., 354, 50–73.


551
17
Calculus of Variations
Tomáš Roubíˇcek
17.1
Introduction
The history of the calculus of variations
dates back several thousand years, fulﬁlling
the ambition of mankind to seek lucid prin-
ciples that govern the Universe. Typically,
one tries to identify scalar-valued function-
als having a clear physical interpretation,
for example, time, length, area, energy, and
entropy, whose extremal (critical) points
(sometimes under some constraints) repre-
sent solutions of the problem in question.
Rapid development was initiated between
the sixteenth and nineteenth centuries
when practically every leading scholar, for
example, J. Bernoulli, B. Bolzano, L. Euler,
P. Fermat, J.L. Lagrange, A.-M. Legendre,
G.W. Leibniz,
I. Newton,
K. Weierstrass
and many others, contributed to varia-
tional calculus; at that time, the focus
was rather on one-dimensional problems
cf. also [1–3]. There has been progress
through the twentieth century, which is
still continuing, informed by the histor-
ically important project of Hilbert [4],
Problems 19, 20, and 23] and accelerated
by the development of functional analysis,
theory of partial diﬀerential equations,
and eﬃcient computational algorithms
supported by rigorous numerical analysis
and computers of ever-increasing power.
Modern methods allow simple formula-
tions in abstract spaces where technicalities
are suppressed, cf. Section 17.2, although
concrete problems ultimately require addi-
tional tools, cf. Section 17.3. An important
“side eﬀect” has been the development of a
sound theory of optimization and optimal
control and of its foundations, convex and
nonsmooth analysis.
17.2
Abstract Variational Problems
Variational problems typically deal with a
real-valued functional Φ ∶V →ℝon an
abstract space V that is equipped with a
linear structure to handle variations and
a topological structure to handle various
continuity/stability/localization concepts.
In the simplest and usually suﬃciently gen-
eral scenario, V is a Banach space1) [5] or,
1) A linear space equipped with a norm ‖ ⋅‖,
that is, 0 ≤‖u+v‖ ≤‖u‖+‖v‖, ‖u‖=0 ⇒u=0,
‖𝜆u‖ = 𝜆‖u‖ for any 𝜆≥0 and u, v∈V, is
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

552
17 Calculus of Variations
in physics, often even a Hilbert space.2) The
Banach space structure allows us to deﬁne
basic notions, such as linearity, continuity,
and convexity: Φ is called continuous if
Φ(uk) →Φ(u) for any uk →u, convex if
Φ(𝜆u + (1 −𝜆)v) ≤𝜆Φ(u) + (1 −𝜆)Φ(v)
for any u, v ∈V and 0 ≤𝜆≤1, concave
if −Φ is convex, or linear if it is convex,
concave, and Φ(0) = 0.
Yet it should be pointed out that the lin-
ear structure imposed on a problem is the
result of our choice; it serves rather as a
mathematical tool used to deﬁne variations
or laws of evolution, or to devise numeri-
cal algorithms, and so on. Often, this choice
is rather artiﬁcial, especially if it leads to
nonquadratic or even nonconvex function-
als possibly with nonlinear constraints.
17.2.1
Smooth (Diﬀerentiable) Case
The Banach space structure allows further
to say that Φ is directionally diﬀerentiable if
the directional derivative at u in the direc-
tion of (variation) v, deﬁned as
DΦ(u, v) = lim
𝜀↘0
Φ(u + 𝜀v) −Φ(u)
𝜀
,
(17.1)
exists for any u, v ∈V, and is smooth if it
is directionally diﬀerentiable and DΦ(u, ⋅) ∶
V →ℝis a linear continuous functional;
then the Gâteaux diﬀerential Φ′(u) ∈V ∗,
with V ∗being the dual space,3) is deﬁned
called a Banach space if it is complete, that
is, any Cauchy sequence {uk}k∈ℕconverges:
limmax(k,l)→∞‖uk−ul‖ = 0 implies that there is
u∈V such that limk→∞‖uk−u‖ = 0; then we
write uk →u.
2) This is a Banach space V whose norm makes the
functional V →ℝ∶u →‖u+v‖2 −‖u−v‖2 linear
for any v∈V; in this case, we deﬁne the scalar
product by (u, v) = 1
4 ‖u+v‖2 −1
4 ‖u−v‖2.
3) The dual space V ∗is the Banach space of all
linear continuous functionals f on V with the
norm ‖f ‖∗= sup‖u‖≤1⟨f , u⟩, with the duality
by
⟨Φ′(u), v⟩= DΦ(u, v).
(17.2)
If Φ′ ∶V →V ∗is continuous, then Φ is
called continuously diﬀerentiable. Further-
more, u ∈V is called a critical point if
Φ′(u) = 0,
(17.3)
which
is
an
abstract
version
of
the
Euler–Lagrange equation. In fact, (17.3)
is a special case of the abstract operator
equation
A(u) = f
with A ∶V →V ∗, f ∈V ∗,
(17.4)
provided A = Φ′ + f for some potential Φ
whose existence requires some symmetry
of A: if A itself is Gâteaux diﬀerentiable and
hemicontinuous,4) it has a potential if, and
only if, it is symmetric, that is,
⟨[A′(u)](v), w⟩= ⟨[A′(u)](w), v⟩
(17.5)
for any u, v, w∈V; up to a constant; this
potential is given by the formula
Φ(u) = ∫
1
0
⟨A(𝜆u), u⟩d𝜆.
(17.6)
Equation (17.3) is satisﬁed, for example, if
Φ attains its minimum5) or maximum at u.
The former case is often connected with a
minimum-energy principle that is assumed
pairing ⟨⋅, ⋅⟩∶V × V ∗→ℝbeing the bilinear
form deﬁned by ⟨f , u⟩= f (u).
4) This is a very weak mode of continuity, requiring
that t →⟨A(u+tv), w⟩is continuous.
5) The proof is simple: suppose Φ(u) = min Φ(⋅)
but Φ′(u) ≠0, then for some v ∈V we
would have ⟨Φ′(u), v⟩= DΦ(u, v) < 0
so that, for a suﬃciently small 𝜀> 0,
Φ(u+𝜀v) = Φ(u) + 𝜀⟨Φ′(u), v⟩+ o(𝜀) < Φ(u),
a contradiction.

17.2 Abstract Variational Problems
553
to govern many steady-state physical prob-
lems. The existence of solutions to (17.3)
can thus often be based on the existence
of a minimizer of Φ, which can rely on the
Bolzano–Weierstrass theorem, which states
that a lower (resp. upper) semicontinuous
functional6) on a compact7) set attains its
minimum (resp. maximum).
In inﬁnite-dimensional Banach spaces,
it is convenient to use this theorem with
respect
to
weak*
convergence:
assum-
ing V = (V ′)∗for some Banach space
V ′ (called the pre-dual), we say that a
sequence {uk}k∈ℕconverges weakly* to u if
limk→∞⟨uk, z⟩= ⟨u, z⟩for any z ∈V ′. If V ∗
is taken instead of V ′, this mode of con-
vergence is called weak convergence. Often
V ′ = V ∗(such spaces are called reﬂexive),
and then the weak* and the weak conver-
gences coincide. The Bolzano–Weierstrass
theorem underlies the direct method,8)
invented essentially in [6], for proving exis-
tence of a solution to (17.3). We say that Φ
is coercive if lim‖u‖→∞Φ(u)∕‖u‖ = +∞.
Theorem 17.1 (Direct method) 9)
Let V
have a pre-dual and Φ ∶V →ℝbe weakly*
lower semicontinuous, smooth, and coer-
cive. Then (17.3) has a solution.
6) Lower semicontinuity of Φ means that
lim infk→∞Φ(uk) ≥Φ(u) for any sequence
{uk}k∈ℕconverging (in a sense to be speciﬁed)
to u; more precisely, this is sequential lower
semicontinuity, but we will conﬁne ourselves to
the sequential concept throughout the chapter,
which is suﬃciently general provided the related
topologies are metrizable.
7) A set is compact if any sequence has a converg-
ing (in the same sense as used for the semiconti-
nuity of the functional) subsequence.
8) This means that no approximation and subse-
quent convergence is needed.
9) The proof relies on coercivity of Φ, which allows
for a localization on bounded sets and then, due
to weak* compactness of convex closed bounded
sets in V, on the Bolzano–Weierstrass theorem.
AS continuous convex functionals are
also weakly* lower semicontinuous, one
gets a useful modiﬁcation:
Theorem 17.2 (Direct method II) Let V
have a pre-dual and let Φ ∶V →ℝbe
continuous, smooth, coercive, and convex.
Then (17.3) has a solution.
If
Φ
is
furthermore
strictly
convex
in
the
sense
that
Φ(𝜆u+(1−𝜆)v) <
𝜆Φ(u) + (1−𝜆)Φ(v)
for
any
u ≠v
and
0 < 𝜆< 1, then (17.3) has at most one
solution.
We
say
that
a
nonlinear
oper-
ator
A ∶V →V ∗
is
monotone
if
⟨A(u)−A(v), u−v⟩≥0
for
any
u, v ∈V.
Monotonicity of a potential nonlinear
operator implies convexity of its poten-
tial, and then Theorem 17.2 implies the
following.
Theorem 17.3 Let V
be reﬂexive and
A ∶V →V ∗
be
monotone,
hemicon-
tinuous,
coercive
in
the
sense
that
lim‖u‖→∞⟨A(u), u⟩= ∞,
and
possess
a
potential. Then, for any f ∈V ∗, (17.4) has
a solution.
In fact, Theorem 17.3 holds even for
mappings
not
having
a
potential
but
its proof, due to Brézis [7], then relies
on an approximation and on implicit,
nonconstructive ﬁxed-point arguments.
The solutions to (17.3) do not need to
represent the global minimizers that we
have considered so far. Local minimizers,
being consistent with physical principles
of minimization of energy, would also
serve well. The same holds for maximizers.
Critical points may, however, have a more
complicated saddle-like character. One
intuitive example is the following: let the
origin, being at the level 0, be surrounded
by a range of mountains all of height h > 0
at distance 𝜌from the origin, but assume

554
17 Calculus of Variations
that there is at least one location v beyond
that circle, which has lower altitude. Going
from the origin to v, one is tempted to
minimize climbing and takes a mountain
pass. The Ambrosetti–Rabinowitz moun-
tain pass theorem [8] says that there is such
a mountain pass and Φ′ vanishes there.
More rigorously, we have Theorem 17.4.
Theorem 17.4 (Mountain pass) Let
Φ
be continuously differentiable, satisfy the
Palais–Smale property10) and satisfy the
following three conditions:
Φ(0) = 0,
(17.7a)
∃𝜌, h > 0∶‖u‖ = 𝜌⇒Φ(u) ≥h, (17.7b)
∃v ∈V∶
‖v‖ > 𝜌,
Φ(v) < h.
(17.7c)
Then Φ has a critical point u ≠0.
A similar assertion relies on a Carte-
sian structure, leading to a von Neumann’s
saddle-point theorem.
Theorem 17.5 (Saddle point)11) Let V =
Y × Z be reﬂexive, Φ(y, ⋅) ∶Z →ℝbe con-
cave continuous and Φ(⋅, z) ∶Y →ℝbe
convex continuous for any (y, z) ∈Y × Z,
Φ(⋅, z) ∶Y →ℝand let −Φ(y, ⋅) ∶Z →ℝ
be coercive for some (y, z) ∈Y × Z. Then
there is (y, z) ∈Y × Z so that
∀̃y∈Y∀̃z∈Z ∶Φ(̃y, z) ≥Φ(y, z) ≥Φ(y, ̃z)
and, if Φ is smooth, then Φ′(y, z) = 0.
10) More speciﬁcally, {Φ(uk)}k∈ℕbounded and
limk→∞||Φ′(uk)||V ∗= 0 imply that {uk}k∈ℕhas
a convergent subsequence.
11) The proof is nonconstructive, based on a
ﬁxed-point argument, see, for example, [9,
Theorems 9D and 49A with Prop. 9.9]. The
original von Neumann’s version [10] dealt with
the ﬁnite-dimensional case only.
17.2.2
Nonsmooth Case
For Φ ∶V →ℝ∪{+∞} convex, we deﬁne
the subdiﬀerential of Φ at u as
𝜕Φ(u) =
{
f ∈V ∗; ∀v∈V ∶
Φ(v) + ⟨f , u−v⟩≥Φ(u)
}
. (17.8)
If Φ is Gâteaux diﬀerentiable, then 𝜕Φ(u) =
{Φ′(u)}, hence this notion is indeed a gen-
eralization of the conventional diﬀerential.
Instead of the abstract Euler–Lagrange
equation (17.3), it is natural to consider
the abstract inclusion 0 ∈𝜕Φ(u). More
generally, assuming Φ = Φ0 + Φ1 with Φ0
smooth and Φ1 convex, instead of (17.3),
we consider the inclusion
𝜕Φ1(u) + Φ′
0(u) ∋0.
(17.9)
In view of (17.8), this inclusion can equally
be written as a variational inequality
∀v∈V ∶Φ1(v) + ⟨Φ′
0(u), v−u⟩≥Φ1(u).
(17.10)
Theorems 17.1 and 17.2 can be reformu-
lated, for example, as follows.
Theorem 17.6 Let V have a pre-dual and
let Φ0 ∶V →ℝbe weakly* lower semicon-
tinuous and smooth, Φ1 ∶V →ℝ∪{+∞}
convex and lower semicontinuous, and let
Φ0 + Φ1 be coercive. Then (17.9) has a solu-
tion.12)
Introducing the Fréchet subdiﬀerential
𝜕FΦ(u) =
{
f ∈V ∗;
lim inf
‖v‖→0
Φ(u+v) −Φ(u) −⟨f , v⟩
‖v‖
≥0
}
,
(17.11)
12) The proof relies on existence of a minimizer of
Φ0 + Φ1 as in Theorem 17.1; then one shows
that any such a minimizer satisﬁes (17.9).

17.2 Abstract Variational Problems
555
the inclusion (17.9) can be written simply
as 𝜕FΦ(u)∋0; in fact, a calculus for Fréchet
subdiﬀerentials can be developed for a
wider class of (sometimes called amenable)
functionals than that considered in (17.9),
cf. [11, 12].
Example 17.1
Let us consider the indica-
tor function 𝛿K of a set K ⊂V deﬁned as
𝛿K(u) =
{
0
if u∈K,
+∞
if u∉K.
(17.12)
Clearly, 𝛿K is convex or lower semicon-
tinuous if (and only if) K is convex or
closed, respectively. Assuming K convex
closed, it is not diﬃcult to check that
𝜕𝛿K(u) = {f ∈V ∗; ∀v∈K ∶⟨f , v −u⟩≤0}
if u ∈K, otherwise 𝜕𝛿K(u) = ∅. The set
𝜕𝛿K(u) is called the normal cone to K at u;
denoted also by NK(u). For the very special
case Φ1 = 𝛿K, the variational inequality
(17.10) (i.e. here also Φ′
0(u)∈−NK(u))
represents the problem of ﬁnding u∈K
satisfying
∀v∈K ∶
⟨
Φ′
0(u), v−u
⟩
≥0.
(17.13)
17.2.3
Constrained Problems
In fact, we saw in Example 17.1 a variational
problem for Φ0 with the constraint formed
by a convex set K. Sometimes, there still
is a need to involve constraints of the type
R(u) = 0 (or possibly more general R(u) ≤
0) for a nonlinear mapping R ∶V →Λ with
Λ a Banach space that is possibly ordered;
we say that Λ is ordered by “ ≥” if {𝜆≥0}
forms a closed convex cone13) in Λ. Then the
constrained minimization problems reads
as follows:
13) A cone C is a set such that a𝜆∈C whenever
𝜆∈C and a ≥0.
Minimize Φ(u) subject to R(u) ≤0, u∈K.
(17.14)
Let us deﬁne the tangent cone TK(u) to
K at u as the closure of ∪a≥0a(K−u).
For A ∶V →Λ linear continuous, the
adjoint operator A∗∶Λ∗→V ∗is deﬁned
by
⟨A∗𝜆∗, u⟩= ⟨𝜆∗, Au⟩
for
all
𝜆∗∈Λ∗
and u∈V. Assuming R to be smooth,
the
ﬁrst-order
necessary
optimality
Karush—Kuhn–Tucker14) condition takes
the following form:
Theorem 17.7 (First-order condition)
Let u∈V solve (17.14) and let15)
∃̃u∈TK(u) ∶
[R′(u)](̃u) < 0
(17.15)
hold. Then there exists 𝜆∗≥
∗016) such that17)
⟨𝜆∗, R(u)⟩= 0
and
(17.16a)
Φ′(u) + R′(u)∗𝜆∗+ NK(u) ∋0.
(17.16b)
The
condition
(17.15)
is
called
the
Mangasarian–Fromovitz constraint quali-
ﬁcation,
while
(17.16a)
is
called
the
complementarity (or sometimes orthogo-
nality or transversality) condition and the
triple
R(u) ≤0,
𝜆∗≥
∗0,
⟨𝜆∗, R(u)⟩= 0 (17.17)
is called a complementarity problem. Deﬁn-
ing the Lagrangean by
ℒ(u, 𝜆∗) = Φ(u) + 𝜆∗∘R(u),
(17.18)
14) Conditions of this kind were ﬁrst formulated in
Karush’s thesis [13] and later independently in
[14].
15) The inequality “<” in (17.15) means that a
neighborhood of [R′(u)](̃u) still lies in the cone
{𝜆≤0}.
16) The so-called dual ordering ≥
∗on Λ∗means
that 𝜆∗≥
∗0 if, and only if, ⟨𝜆∗, v⟩≥0 for all v ≥
0.
17) The linear operator R′(u)∗∶Λ∗→V ∗is adjoint
to R′(u) ∶V →Λ and (17.16b) is meant in V ∗.

556
17 Calculus of Variations
we can write the inclusion (17.16b) simply
as ℒ′
u(u, 𝜆∗) + NK(u) ∋0. The optimality
condition à la Example 17.1 for maximiza-
tion of ℒ(u, ⋅) ∶Λ∗→ℝover the cone
{𝜆∗≥
∗0} is simply R(u) ≤0.
If R is a convex mapping18) and K is a
convex set, then (17.15) is equivalent to
the simpler Slater constraint qualiﬁcation:
∃u0 ∈K ∶R(u0) < 0. If Φ is also convex,
then (17.16) represents the ﬁrst-order suﬃ-
cient optimality condition in the sense that
if (17.16) is satisﬁed, u solves (17.14). More-
over, the couple (u, 𝜆∗) represents a sad-
dle point for ℒon the set K × {𝜆∗≥
∗0},
and its existence can be proved by using
Theorem 17.5.
Minimization problems without the con-
straint R(u) ≤0 may be much easier to
solve in speciﬁc cases. In particular, one
can explicitly calculate the value D(𝜆∗) =
minu∈K ℒ(u, 𝜆∗). The functional D ∶Λ∗→
ℝ∪{−∞} is concave and
maximize D(𝜆∗) subject to 𝜆∗≥
∗0 (17.19)
is called the dual problem. The supremum
of (17.19) is always below the inﬁmum of
(17.14). Under additional conditions, they
can be equal to each other, and (17.19) has
a solution 𝜆∗that can serve as the multi-
plier for (17.16). For duality theory, see, for
example, [12, Chapter 12].
In the general nonconvex case, (17.16)
is no longer a suﬃcient condition and
construction of such conditions is more
involved. A prototype is a suﬃcient second-
order condition that uses the approximate
critical cone C𝜀:
C𝜀(u) = {h∈TK(u); Φ′(u)h ≤𝜀‖h‖,
dist(R′(u)h, T−D(R(u))) ≤𝜀‖h‖}
for some 𝜀> 0:
18) In this Banach-valued case, convexity means
R(𝜆u+(1−𝜆)v) ≤𝜆R(u) + (1−𝜆)R(v) for any
u, v ∈V and 0 ≤𝜆≤1 with ≤referring to the
ordering in Λ.
Theorem 17.8 (Second-order condition)
Let Φ and R be twice diﬀerentiable and let
the ﬁrst-order necessary condition (17.16)
with a multiplier 𝜆∗≥
∗0 hold at some u and
let
∃𝜀, 𝛿> 0 ∀h∈C𝜀(u) ∶
ℒ′′
u (u, 𝜆∗)(h, h) ≥𝛿‖h‖2. (17.20)
Then u is a local minimizer for (17.14).
A very special case is when R ≡0
and K = V: in this unconstrained case,
NK = {0}, C𝜀= V, and (17.16) and (17.20)
become,
respectively,
the
well-known
classical condition Φ′(u) = 0 and Φ′′(u) is
positive deﬁnite.
17.2.4
Evolutionary Problems
Imposing a linear structure allows us
not only to deﬁne diﬀerentials by using
(17.1) and (17.2) but also to deﬁning the
derivatives du∕dt of trajectories t →u(t) ∶
ℝ→V.
17.2.4.1
Variational Principles
Minimization of the energy Φ is related to
a gradient ﬂow, that is, a process u evolving
in time, governed by the gradient Φ′ in the
sense that the velocity du∕dt is always in
the direction of steepest descent −Φ′ of Φ.
Starting from a given initial condition u0
and generalizing it for a time-dependent
potential Φ −f (t) with f (t) ∈V ∗, one
considers
the
initial-value
problem
(a
Cauchy problem) for the abstract parabolic
equation:
du
dt + Φ′(u) = f (t),
u(0) = u0.
(17.21)
It is standard to assume V ⊂H, with H
a Hilbert space, this embedding being

17.2 Abstract Variational Problems
557
dense19) and continuous. Identifying H with
its own dual, we obtain a Gelfand-triple
V ⊂H ⊂V ∗.
Then,
with
the
coerciv-
ity/growth assumption
∃𝜖> 0 ∶𝜖‖u‖p
V ≤Φ(t, u) ≤
1+‖u‖p
V
𝜖
(17.22)
for some 1 < p < +∞and ﬁxing a time
horizon T > 0, the solution to (17.21) is
sought in the aﬃne manifold
{
v∈Lp(I; V); v(0) = u0, dv
dt ∈Lp′(I; V ∗)
}
(17.23)
with I = [0, T], where Lp(I; V) stands for a
Lebesgue space of abstract functions with
values in a Banach space (here V), which is
called a Bochner space.
By continuation, we obtain a solution
u to (17.21) on [0, +∞). If Φ is con-
vex and f is constant in time, there is
a relation to the variational principle
for Φ −f in Section 17.2.1: the function
t →[Φ−f ](u(t)) is nonincreasing and con-
vex, and u(t) converges weakly as t →∞to
a minimizer of Φ−f on V.
The variational principle for (17.21) on
the bounded time interval I uses the func-
tional 𝔉deﬁned by
𝔉(u) =∫
T
0
Φ(t, u(t)) + Φ∗(
t, f (t) −du
dt
)
−⟨f (t), u(t)⟩dt + 1
2‖u(T)‖2
H, (17.24)
where Φ∗(t, ⋅) ∶V ∗→ℝ∪{+∞} is the
Legendre
conjugate
to
Φ(t, ⋅) ∶V →
ℝ∪{+∞} deﬁned by
Φ∗(t, f ) = sup
v∈V
⟨f , v⟩−Φ(v);
(17.25)
19) A subset is dense if its closure is the whole
space, here H.
the construction Φ(t, ⋅) →Φ∗(t, ⋅) is called
the Legendre transformation. Omitting t
for the moment, Φ∗is convex and
Φ∗(f ) + Φ(v) ≥⟨f , v⟩,
(17.26)
which is Fenchel’s inequality. If Φ, resp.
Φ∗, is smooth, the equality in (17.26) holds
if, and only if, f ∈Φ′(v), resp. v∈[Φ∗]′(f ).
Moreover, if Φ(⋅) is lower semicontinuous,
it holds Φ∗∗= Φ.
The inﬁmum of 𝔉on (17.24) is equal
to 1
2‖u0‖2
H. If u from (17.23) minimizes 𝔉
from (17.24), that is, 𝔉(u) = 1
2‖u0‖2
H, then
u solves the Cauchy problem (17.21); this
is the Brezis–Ekeland–Nayroles principle
[15, 16]. It can also be used in the direct
method, see [17] or [18, Section 8.10]:
Theorem 17.9 (Direct method for (17.21))
Let Φ ∶[0, T] × V →ℝbe a Carathéodory
function such that Φ(t, ⋅) is convex, both
Φ(t, ⋅) and Φ∗(t, ⋅) are smooth, (17.22)
holds, u0 ∈H, and f ∈L p′(I; V ∗). Then
𝔉from (17.24) attains a minimum on
(17.23) and the (unique) minimizer solves
the Cauchy problem (17.21).
One can consider another side-condition
instead of the initial condition, for example,
the periodic condition u(0) = u(T), having
the meaning that we are seeking periodic
solutions with an a priori prescribed period
T assuming f is periodic with the period T.
Instead of (17.21), one thus considers
du
dt + Φ′(u) = f (t),
u(0) = u(T).
(17.27)
Then, instead of (17.23), solutions are
sought in the linear (in fact, Banach) space
{
v∈Lp(I; V); v(0) = v(T),
dv
dt ∈Lp′(I; V ∗)
}
.
(17.28)

558
17 Calculus of Variations
The direct method now uses, instead of
(17.24), the functional
𝔉(u) =∫
T
0
Φ(t, u(t)) −⟨f (t), u(t)⟩
+ Φ∗(
t, f (t) −du
dt
)
dt,
(17.29)
and an analog of Theorem 17.9 but using
(17.28) and (17.29); the minimum is 0
and the minimizer need not be unique, in
general.
Often,
physical
and
mechanical
applications use a convex (in general,
nonquadratic)
potential
of
dissipative
forces Ψ ∶H →ℝ∪{+∞} leading to a
doubly nonlinear Cauchy problem:
Ψ′(du
dt
) + Φ′(u) = f (t),
u(0) = u0. (17.30)
In fact, the hypothesis that (here abstract)
dissipative forces, say A(du∕dt), have a
potential needs a symmetry of A, cf. (17.5),
which has been under certain conditions
justiﬁed in continuum-mechanical (even
anisothermal)
linearly
responding
sys-
tems (so that the resulting Ψ is quadratic);
this is Onsager’s (or reciprocal) symmetry
condition [19],20) cf. [20, Section 12.3].
Sometimes, (17.30) is also equivalently
written as
du
dt = [Ψ∗]′(f (t)−Φ′(u)),
u(0) = u0,
(17.31)
where
Ψ∗
again
denotes
the
conju-
gate functional, that is, here Ψ∗(v∗) =
supv∈H⟨v∗, v⟩−Ψ(v). If Ψ is also proper in
the sense that Ψ > −∞and Ψ ≢+∞, then
[Ψ∗]′ = [Ψ′]−1, which was used in (17.31).
For Ψ = 1
2‖ ⋅‖2
H, we get du∕dt = f −Φ′(u),
20) A Nobel prize was awarded to Lars Onsager in
1968 “for the discovery of the reciprocal rela-
tions bearing his name, which are fundamental
for the thermodynamics of irreversible pro-
cesses.”
cf. (17.21). Thus, for f = 0, (17.31) rep-
resents a generalized gradient ﬂow. For a
general f , a Stefanelli’s variational principle
[21] for (17.30) employs the functional
𝔉(u, w) =
(
∫
T
0
Ψ
(du
dt
)
−
⟨
f , du
dt
⟩
+ Ψ∗(w) dt + Φ(u(T)) −Φ(u0)
)+
+ ∫
T
0
Φ(u) −⟨f −w, u⟩+ Φ∗( f −w) dt
(17.32)
to be minimized on the aﬃne manifold
{
(u, w)∈L∞(I; V);
u(0) = u0,
du
dt ∈Lq(I; H), w∈Lq′(I; H)
}
, (17.33)
where 1 < q < +∞refers to a coerciv-
ity/growth condition for Ψ. On the set
(17.33), 𝔉(u, w) ≥0 always holds, and
𝔉(u, w) = 0 means that w=Ψ′(du∕dt) and
f −w=Φ′(u) a.e. (almost everywhere) on I,
that is, u solves (17.30).
Another option is to use the conjugation
and Fenchel inequality only for Ψ, which
leads to21)
𝔊(u) = ∫
T
0
Ψ(du
dt
) + Ψ∗(f −Φ′(u))
+
⟨df
dt , u
⟩
dt + Φ(u(T))
(17.34)
to be minimized on a submanifold {u = w}
of (17.33). The inﬁmum is Φ(u0) −f (0) +
f (T) and any minimizer u is a solution to
(17.30). Sometimes, this is known under the
name principle of least dissipation, cf. [22]
for Ψ quadratic. The relation
𝔊(u) = Φ(u0) −f (0) + f (T)
(17.35)
21) Here (17.26) reads as Ψ(du∕dt) +
Ψ∗(f −Φ′(u)) ≥⟨f −Φ′(u), du∕dt⟩=
⟨f , du∕dt⟩−[dΦ∕dt](u), from which (17.34)
results by integration over [0, T].

17.2 Abstract Variational Problems
559
is sometimes called De Giorgi’s formulation
of (17.30); rather than for existence proofs
by the direct method, this formulation is
used for various passages to a limit. Note
that for f constant, the only time deriva-
tive involved in (17.34) is Ψ(du∕dt), which
allows for an interpretation even if V is
only a metric space and thus du∕dt itself
is not deﬁned, which leads to a theory
of gradient ﬂows in metric spaces, cf. [23,
Theorem 2.3.3].
A combination of (17.27) and (17.30)
leads to
Ψ′
(
du
dt
)
+ Φ′(u) = f (t),
u(T) = u(0),
and the related variational principle uses 𝔉
from (17.32) but with Φ(u(T))−Φ(u0) omit-
ted, to be minimized on the linear manifold
(17.33) with u0 replaced by u(T).
Many physical systems exhibit oscillatory
behavior combined possibly with attenu-
ation by nonconservative forces having a
(pseudo)potential Ψ, which can be covered
by the (Cauchy problem for the) abstract
second-order evolution equation
𝒯′ d2u
dt2 + Ψ′
(
du
dt
)
+ Φ′(u) = f (t),
u(0) = u0,
du
dt (0) = v0, (17.36)
where
𝒯∶H →ℝ
is
the
positive
(semi)deﬁnite quadratic form representing
the kinetic energy. The Hamilton varia-
tional principle extended to dissipative
systems says that the solution u to (17.36)
is a critical point of the integral functional
∫
T
0
𝒯
(
du
dt
)
−Φ(u) + ⟨f −𝔣, u⟩dt (17.37)
with a nonconservative force 𝔣= Ψ′(du∕dt)
considered ﬁxed on the aﬃne manifold
{u∈L∞(I; V); du∕dt∈L∞(I; H), d2u∕dt2 ∈
L2(I; V ∗), u(0) = u0, du∕dt = v0}, cf. [24].
17.2.4.2
Evolution Variational
Inequalities
For
nonsmooth
potentials,
the
above
evolution equations turn into inclusions.
Instead of the Legendre transformation,
we speak about the Legendre–Fenchel
transformation. For example, instead of
[Ψ∗]′ = [Ψ′]−1,
we
have
𝜕Ψ∗= [𝜕Ψ]−1.
Note that variational principles based on
𝔉from (17.24), (17.29), or (17.32) do not
involve any derivatives of Φ and Ψ and are
especially designed for nonsmooth prob-
lems, and also 𝔊from (17.34) allows for Ψ
to be nonsmooth. For example, in the case
of (17.30), with convex but nonsmooth
Ψ and Φ, we have the doubly nonlinear
inclusion
𝜕Ψ
(
du
dt
)
+𝜕Φ(u)∋f (t), u(0) = u0, (17.38)
and 𝔉(u, w) = 0 in (17.32) and (17.33)
means exactly that w∈𝜕Ψ(du∕dt) and
f −w∈𝜕Φ(u) hold a.e. on I,22) which (in the
spirit of Section 17.2.2) can be written as a
system of two variational inequalities for u
and w:
∀v ∶
Ψ(v) +
⟨
w, v−du
dt
⟩
≥Ψ
(
du
dt
)
,
(17.39a)
∀v ∶
Φ(v) + ⟨f −w, v −u⟩≥Φ(u).
(17.39b)
For a systematic treatment of such multiply
nonlinear inequalities, see [25].
In applications, the nonsmoothness of
Ψ occurs typically at 0 describing activa-
tion phenomena: the abstract driving force
f −𝜕Φ(u) must pass a threshold, that is, the
boundary of the convex set 𝜕Ψ(0), in order
to trigger the evolution of u. Often, any
22) The idea behind the principle in (17.32) and
(17.33) is to apply two Fenchel inequalities to
(17.38) written as w∈𝜕Ψ(du∕dt) and f −w∈
𝜕Φ(u).

560
17 Calculus of Variations
rate dependence is neglected, and then Ψ is
degree-1 positively homogeneous.23) In this
kind of rate-independent case, Ψ∗= 𝛿𝜕Ψ(0),
while Ψ = 𝛿∗
𝜕Ψ(0), and the De Giorgi formu-
lation (17.35) leads to the energy equality
E(T, u(T)) + ∫
T
0
Ψ
(
du
dt
)
dt
= E(0, u0) −∫
T
0
⟨df
dt , u
⟩
dt
for E(t, u) = Φ(u) −⟨f (t), u⟩
(17.40a)
together with f (t)−Φ′(u(t))∈𝜕Ψ(0) for a.a.
(almost all) t∈[0, T]; here, in accordance
with (17.35), we assume Φ to be smooth for
the moment. This inclusion means Ψ(v) −
⟨f −Φ′(u), v⟩≥Ψ(0) = 0 and, as Φ is con-
vex, we obtain the stability condition,24)
∀t∈[0, T] ∀v∈V ∶
E(t, u(t)) ≤E(t, v) + Ψ(v−u(t)).
(17.40b)
Moreover, in this rate-independent case,
𝜕Ψ∗= N𝜕Ψ(0)
and
(17.31)
reads
du∕
dt ∈N𝜕Ψ(0)(f −Φ′(u)). By (17.13), it means
that
⟨du∕dt, v −f + Φ′(u)⟩≤0
for
any
v∈𝜕Ψ(0), that is,
max
v∈𝜕Ψ(0)
⟨du
dt , v
⟩
=
⟨du
dt , f −Φ′(u)
⟩
,
(17.41)
which says that the dissipation due to the
driving force f −Φ′(u) is maximal com-
pared to all admissible driving forces pro-
vided the rate du∕dt is kept ﬁxed; this is the
maximum dissipation principle.
In fact, (17.40) does not contain Φ′ and
thus works for Φ convex nonsmooth, too.
Actually, (17.40) was invented in [26],
where it is called the energetic formulation
of (17.38), cf. also [27].
23) This means Ψ(𝜆v) = 𝜆Ψ(v) for any 𝜆≥0.
24) By convexity of Φ, we have Φ(v) ≥
Φ(u) + ⟨Φ′(u), v−u⟩, and adding it with
Ψ(v−u) −⟨f −Φ′(u), v−u⟩≥0, we get (17.40b).
17.2.4.3
Recursive Variational Problems
Arising by Discretization in Time
The variational structure related to the
potentials
of
Section 17.2.4.1
can
be
exploited not only for formulation of
“global”
in
time-variational
principles,
but, perhaps even more eﬃciently, to
obtain recursive (incremental) variational
problems when discretizing the abstract
evolution problems in time by using some
(semi) implicit formulae. This can serve as
an eﬃcient theoretical method for analyz-
ing evolution problems (the Rothe method,
[28]) and for designing eﬃcient conceptual
algorithms for numerical solution of such
problems.
Considering a uniform partition of the
time interval with the time step 𝜏> 0 with
T∕𝜏integer, we discretize (17.21) as
uk
𝜏−uk−1
𝜏
𝜏
+ Φ′(uk
𝜏) = f (k𝜏),
k = 1, … , T
𝜏,
u0
𝜏= u0.
(17.42)
This is also known as the implicit Euler
formula and uk
𝜏for k = 1, … , T∕𝜏approxi-
mate respectively the values u(k𝜏). One can
apply the direct method by employing the
recursive variational problem for the func-
tional
Φ(u) + 1
2𝜏
‖‖u−uk−1
𝜏
‖‖
2
H −⟨f (k𝜏), u⟩(17.43)
to be minimized for u. Obviously, any criti-
cal point u (and, in particular, a minimizer)
of this functional solves (17.42) and we put
u = uk
𝜏. Typically, after ensuring existence
of the approximate solutions {uk
𝜏}T∕𝜏
k=1, a pri-
ori estimates have to be derived25) and then
convergence as 𝜏→0 is to be proved by
25) For this, typically, testing (17.42) (or its dif-
ference from k−1 level) by uk
𝜏or by uk
𝜏−uk−1
𝜏
(or uk
𝜏−2uk−1
𝜏
+ uk−2
𝜏
) is used with Young’s and
(discrete) Gronwall’s inequalities, and so on.

17.2 Abstract Variational Problems
561
various methods.26) Actually, Φ does not
need to be smooth and, referring to (17.11),
we can investigate the set-valued varia-
tional inclusion du∕dt+𝜕FΦ(u) ∋f .
In speciﬁc situations, the fully implicit
scheme (17.42) can be advantageously
modiﬁed in various ways. For example, in
case Φ = Φ1 + Φ2 and f = f1 + f2, one can
apply the fractional-step method, alterna-
tively to be understood as a Lie–Trotter
(or sequential) splitting combined with the
implicit Euler formula:
uk−1∕2
𝜏
−uk−1
𝜏
𝜏
+ Φ′
1(uk−1∕2
𝜏
) = f1(k𝜏), (17.44a)
uk
𝜏−uk−1∕2
𝜏
𝜏
+ Φ′
2(uk
𝜏) ∋f2(k𝜏),
(17.44b)
with k = 1, … , T∕𝜏. Clearly, (17.44) leads
to two variational problems that are to be
solved in alternation.
Actually, we have needed rather the
splitting
of
the
underlying
operator
A = Φ′
1 + Φ′
2 ∶V →V ∗
and not of its
potential
Φ = Φ1 + Φ2 ∶V →ℝ.
In
case Φ ∶V = Y × Z →ℝ, u = (y, z) and
f = (g, h) where (17.21) represents a system
of two equations
dy
dt + Φ′
y(y, z) = g,
y(0) = y0,
(17.45a)
dz
dt + Φ′
z(y, z) = h,
z(0) = z0,
(17.45b)
with Φ′
y and Φ′
z denoting partial diﬀer-
entials, one can thus think also about
the
splitting
Φ′−f = (Φ′
y−g, Φ′
z−h) =
(Φ′
y−g, 0) + (0, Φ′
z−h).
Then
the
frac-
tional method such as (17.44) yields a
semi-implicit scheme27)
26) Typically, a combination of the arguments
based on weak lower semicontinuity or com-
pactness is used.
27) Indeed, in (17.44), one has uk−1
𝜏
= (yk−1
𝜏
, zk−1
𝜏
),
uk−1∕2
𝜏
= (yk
𝜏, zk−1
𝜏
), and eventually uk
𝜏= (yk
𝜏, zk
𝜏).
yk
𝜏−yk−1
𝜏
𝜏
+ Φ′
y(yk
𝜏, zk−1
𝜏
) = g(k𝜏),
(17.46a)
zk
𝜏−zk−1
𝜏
𝜏
+ Φ′
z(yk
𝜏, zk
𝜏) = h(k𝜏), (17.46b)
again for k = 1, … , T∕𝜏. Note that the use
of zk−1
𝜏
in (17.46a) decouples the system
(17.46), in contrast to the fully implicit
formula which would use zk
𝜏in (17.46a)
and would not decouple the original
system
(17.45).
The
underlying
varia-
tional problems for the functionals y →
Φ(y, zk−1
𝜏
) + 1
2𝜏‖y−yk−1
𝜏
‖2 −⟨g(k𝜏), y⟩
and
z →Φ(yk
𝜏, z) + 1
2𝜏‖z−zk−1
𝜏
‖2 −⟨h(k𝜏), z⟩
represent
recursive
alternating
varia-
tional problems; these particular problems
can be convex even if Φ itself is not;
only separate convexity28) of Φ suﬃces.
Besides, under certain relatively weak con-
ditions, this semi-implicit discretization is
“numerically” stable; cf. [18, Remark 8.25].
For
a
convex/concave
situation
as
in
Theorem 17.5, (17.46) can be understood
as an iterative algorithm of Uzawa’s type
(with a damping by the implicit formula)
for ﬁnding a saddle point.29)
Of course, this decoupling method can
be advantageously applied to nonsmooth
situations and for u with more than two
components, that is, for systems of more
than two equations or inclusions. Even
more, the splitting as in (17.45) may yield
a variational structure of the decoupled
incremental problems even if the original
problem of the type du∕dt + A(u) ∋0 itself
does not have it. An obvious example for
this is A(y, z) = (Φ′
1(⋅, z)](y) , Φ′
2(y, ⋅)](z)),
which does not need to satisfy the sym-
metry (17.5) if Φ1 ≠Φ2
although the
corresponding
semi-implicit
scheme
28) This means that only Φ(y, ⋅) and Φ(⋅, z) are con-
vex but not necessarily Φ(⋅, ⋅).
29) This saddle point is then a steady state of the
underlying evolution system (17.45).

562
17 Calculus of Variations
(17.46) still possesses a “bi-variational”
structure.
Similarly to (17.42), the doubly nonlinear
problem (17.38) uses the formula
𝜕Ψ
(uk
𝜏−uk−1
𝜏
𝜏
)
+ 𝜕Φ(uk
𝜏) ∋f (k𝜏) (17.47)
and, instead of (17.43), the functional
Φ(u) + 𝜏Ψ
(u−uk−1
𝜏
𝜏
)
−⟨f (k𝜏), u⟩. (17.48)
Analogously, for the second-order dou-
bly
nonlinear
problem
(17.36)
in
the
nonsmooth case, that is, 𝒯′d2u∕dt2+
𝜕Ψ
(
du∕dt
)
+ 𝜕Φ(u) ∋f (t), we would use
𝒯′ uk
𝜏−2uk−1
𝜏
+ uk−2
𝜏
𝜏2
+ 𝜕Ψ
(uk
𝜏−uk−1
𝜏
𝜏
)
+ 𝜕Φ(uk
𝜏) ∋f (k𝜏)
(17.49)
and the recursive variational problem for
the functional
Φ(u) + 𝜏Ψ
(u−uk−1
𝜏
𝜏
)
−⟨f (k𝜏), u⟩
+ 𝜏2𝒯
(u−2uk−1
𝜏
+uk−2
𝜏
𝜏2
)
.
(17.50)
The fractional-step method and, in par-
ticular, various semi-implicit variants of
(17.47) and (17.49) are widely applicable,
too.
17.3
Variational Problems on Speciﬁc Function
Spaces
We
now
use
the
abstract
framework
from Section 17.2 for concrete variational
problems formulated on speciﬁc function
spaces.
17.3.1
Sobolev Spaces
For this, we consider a bounded domain
Ω ⊂ℝd equipped with the Lebesgue mea-
sure, having a smooth boundary Γ ∶= 𝜕Ω.
For 1≤p<∞, we will use the standard nota-
tion
Lp(Ω; ℝn) =
{
u ∶Ω →ℝn measurable;
∫Ω
|u(x)|p dx < ∞
}
for the Lebesgue space; the addition and
the multiplication understood pointwise
makes it a linear space, and introducing the
norm
‖‖u‖‖p =
(
∫Ω
|u(x)|p dx
)1∕p
makes it a Banach space. For p = ∞,
we
deﬁne
‖u‖∞= ess supx∈Ω|u(x)| =
infN⊂Ω, measd(N)=0 supx∈Ω⧵N |u(x)|. For
1<p<∞,
Lp(Ω; ℝn)
is
reﬂexive.
For
1≤p<∞,
Lp(Ω; ℝn)∗= Lp′(Ω; ℝn)
with
p′ = p∕(p −1) if the duality is deﬁned
naturally as ⟨f , u⟩= ∫Ω f (x) ⋅u(x) dx. For
p = 2, Lp(Ω; ℝn) becomes a Hilbert space.
For n = 1, we write for short Lp(Ω) instead
of Lp(Ω; ℝ).
Denoting the kth order gradient of u
by ∇ku = (𝜕k∕𝜕xi1 · · · 𝜕xiku)1≤i1,…,ik≤d, we
deﬁne the Sobolev space by
W k,p(Ω; ℝn) =
{
u∈Lp(Ω; ℝn);
∇ku∈Lp(Ω; ℝn×dk)
}
,
normed by ‖‖u‖‖k,p =
p√
‖u‖p
p + ‖∇ku‖p
p .
If n = 1, we will again use the shorthand
notation W k,p(Ω). If p = 2, W k,p(Ω; ℝn)
is a Hilbert space and we will write
Hk(Ω; ℝn) = W k,2(Ω; ℝn).
Moreover,
we
occasionally use a subspace of W k,p(Ω; ℝn)

17.3 Variational Problems on Speciﬁc Function Spaces
563
with vanishing traces on the boundary Γ,
denoted by
W k,p
0 (Ω; ℝn) = {u∈W k,p(Ω; ℝn);
∇lu = 0 on Γ, l = 0, … , k−1}.
(17.51)
To give a meaningful interpretation to
traces ∇lu on Γ, this boundary has to be suf-
ﬁciently regular; roughly speaking, piece-
wise Cl+1 is enough.
We denote by Ck(Ω) the space of smooth
functions whose gradients up to the order k
are continuous on the closure Ω of Ω. For
example, we have obviously embeddings
Ck(Ω) ⊂W k,p(Ω) ⊂Lp(Ω); in fact, these
embeddings are dense.
An important phenomenon is the com-
pactifying eﬀect of derivatives. A prototype
for it is the Rellich–Kondrachov theorem,
saying that H1(Ω) is compactly30) embedded
into L2(Ω). More generally, we have
Theorem 17.10 (Compact embedding)
For the Sobolev critical exponent
p∗
⎧
⎪
⎨
⎪⎩
= dp∕(d−p)
for p < d,
∈[1, +∞) arbitrary for p = d,
= +∞
for p > d,
the embedding W 1,p(Ω) ⊂Lp∗−𝜖(Ω) is com-
pact for any 0 < 𝜖≤p∗−1.
Iterating this theorem, we can see, for
example, that, for p < d∕2, the embedding
W 2,p(Ω) ⊂L[p∗]∗−𝜖(Ω) is compact; note that
[p∗]∗= dp∕(d −2p).
Another important fact is the compact-
ness of the trace operator u →u|Γ:
Theorem 17.11 (Compact trace operator)
For the boundary critical exponent
30) This means that the embedding is a compact
mapping in the sense that weakly converging
sequences in H1(Ω) converge strongly in L2(Ω).
p♯
⎧
⎪
⎨
⎪⎩
= (dp−p)∕(d−p)
for p < d,
∈[1, +∞) arbitrary for p = d,
= +∞
for p > d,
the trace operator u →u|Γ ∶W 1,p(Ω) ⊂
Lp♯−𝜖(Γ) is compact for any 0 < 𝜖≤p♯−1.
For example, the trace operator from
W 2,p(Ω) is compact into L[p∗]♯−𝜖(Γ).31)
17.3.2
Steady-State Problems
The above abstract functional-analysis sce-
nario gives a lucid insight into concrete
variational problems leading to boundary-
value problems for quasilinear equations in
divergence form which is what we will now
focus on. We consider a bounded domain
Ω ⊂ℝd with a suﬃciently regular boundary
Γ divided into two disjoint relatively open
parts ΓD and ΓN whose union is dense in
Γ. An important tool is a generalization of
the superposition operator, the Nemytski˘i
mapping 𝒩a, induced by a Carathéodory32)
mapping a ∶Ω × ℝn →ℝm by prescribing
[𝒩a(u)](x) = a(x, u(x)).
Theorem 17.12 (Nemytski˘i mapping)
Let a ∶Ω × ℝn →ℝm be a Carathéodory
mapping and p, q ∈[1, ∞). Then 𝒩a maps
Lp(Ω; ℝn) into Lq(Ω; ℝm) and is continu-
ous if, and only if, for some 𝛾∈Lq(Ω) and
C < ∞, we have that
||a(x, u)|| ≤𝛾(x) + C||u||
p∕q.
31) To see this, we use Theorem 17.10 to obtain
W 2,p(Ω) ⊂W 1,p∗−𝜖(Ω), and then Theorem 17.11
with p∗−𝜖in place of p.
32) The Carathéodory property means measurabil-
ity in the x-variable and continuity in all other
variables.

564
17 Calculus of Variations
17.3.2.1
Second Order Systems of
Equations
First, we consider the integral functional
Φ(u) = ∫Ω
𝜑(x, u, ∇u) dx +∫ΓN
𝜙(x, u) dS
(17.52a)
involving Carathéodory integrands 𝜑∶
Ω×ℝn×ℝn×d →ℝ
and
𝜙∶ΓN×ℝn →ℝ.
The functional Φ is considered on an aﬃne
closed manifold
{u∈W 1,p(Ω; ℝn); u|ΓD = uD
}
(17.52b)
for a suitable given uD; in fact, existence
of uD ∈W 1,p(Ω; ℝn) such uD = uD|ΓD
is
to be required. Equipped with the theory
of W 1,p-Sobolev spaces,33) one considers
a
p-polynomial-type
coercivity
of
the
highest-order term and the corresponding
growth restrictions on the partial deriva-
tives 𝜑′
F, 𝜑′
u, and 𝜙′
u with some 1 < p < ∞,
that is,
𝜑(x, u, F)∶F ≥𝜖||F||
p + ||u||
𝜖−1
𝜖,
(17.53a)
∃𝛾∈Lp′(Ω) ∶||𝜑′
F(x, u, F)|| ≤𝛾(x)
+ C||u||
(p∗−𝜖)∕p′+ C||F||
p−1,
(17.53b)
∃𝛾∈Lp∗′(Ω) ∶||𝜑′
u(x, u, F)|| ≤𝛾(x)
+ C||u||
p∗−1−𝜖+ C||F||
p∕p∗′
,
(17.53c)
∃𝛾∈Lp♯′
(Γ) ∶||𝜙′
u(x, u)||
≤𝛾(x) + C||u||
p♯−1−𝜖
(17.53d)
for some 𝜖> 0 and C < ∞; we used F
as a placeholder for ∇u. A generaliza-
tion
of
Theorem 17.12
for
Nemytski˘i
mappings
of
several
arguments
says
that
(17.53b)
ensures
just
continuity
of
𝒩𝜑′
F ∶Lp∗−𝜖(Ω; ℝn) × Lp(Ω; ℝn×d) →
33) More general nonpolynomial growth and coer-
civity conditions would require the theory of
Orlicz spaces instead of the Lebesgue ones, cf.
[9, Chapter 53].
Lp′(Ω; ℝn×d), and analogously also (17.53c)
works for 𝒩𝜑′
u, while (17.53d) gives conti-
nuity of 𝒩𝜙′
u ∶Lp♯−𝜖(Γ; ℝn) →Lp♯′(Γ; ℝn).
This, together with Theorems 17.10 and
17.11, reveals the motivation for the growth
conditions (17.53b–d).
For 𝜖≥0, (17.53b–d) ensures that the
functional Φ from (17.52a) is Gâteaux dif-
ferentiable on W 1,p(Ω; ℝn). The abstract
Euler–Lagrange equation (17.3) then leads
to the integral identity
∫Ω
𝜑′
∇u(x, u, ∇u)∶∇v + 𝜑′
u(x, u, ∇u)⋅v dx
+ ∫ΓN
𝜙′
u(x, u)⋅v dS = 0
(17.54)
for any v ∈W 1,p(Ω; ℝn) such that v|ΓD = 0;
the notation “ ∶” or “ ⋅” means summation
over two indices or one index, respectively.
Completed by the Dirichlet condition on
ΓD, this represents a weak formulation of
the boundary-value problem for a sys-
tem of second-order elliptic quasilinear
equations:34)
div 𝜑′
∇u(u, ∇u) = 𝜑′
u(u, ∇u) in Ω,
(17.55a)
𝜑′
∇u(u, ∇u)⋅⃗n + 𝜙′
u(u) = 0 on ΓN, (17.55b)
u||Γ = uD on ΓD, (17.55c)
where x-dependence has been omitted
for notational simplicity. The conditions
(17.55b) and (17.55c) are called the Robin
and the Dirichlet boundary conditions,
respectively, and (17.55) is called the clas-
sical formulation of this boundary-value
problem.
Any
u ∈C2(Ω; ℝn)
satisfying
(17.55) is called a classical solution, while
u ∈W 1,p(Ω; ℝn)
satisfying
(17.54)
for
any v ∈W 1,p(Ω; ℝn) such that v|ΓD = 0 is
34) Assuming suﬃciently smooth data as well as u,
this can be seen by multiplying (17.55a) by v,
using the Green formula ∫Ω(div a)v + a⋅v dx =
∫Γ(a ⋅⃗n)v dS, and using v = 0 on ΓD and the
boundary conditions (17.55b) on ΓN.

17.3 Variational Problems on Speciﬁc Function Spaces
565
called a weak solution; note that much less
smoothness is required for weak solutions.
Conversely, taking general Carathéodory
integrands a ∶Ω×ℝn×ℝn×d →ℝn×d, b ∶
ΓN×ℝn →ℝn, and c ∶Ω×ℝn×ℝn×d →ℝn,
one can consider a boundary-value prob-
lem for a system of second-order elliptic
quasilinear equations
div a(u, ∇u) = c(u, ∇u) in Ω,
(17.56a)
a(u, ∇u)⋅⃗n + b(u) = 0 on ΓN,
(17.56b)
u|ΓD = uD on ΓD.
(17.56c)
Such a problem does not need to be
induced by any potential Φ; nevertheless, it
possesses a weak formulation as in (17.54),
namely, ∫Ω a(u, ∇u)∶∇v + c(u, ∇u) ⋅v dx+
∫ΓN b(u)⋅v dS = 0
for
any
“variation”
v
as in (17.54), and related methods are
sometimes called variational in spite of
absence of a potential Φ. The existence of
such a potential requires a certain sym-
metry corresponding to that in (17.5)
for
the
underlying
nonlinear
opera-
tor
A ∶W 1,p(Ω; ℝn) →W 1,p(Ω; ℝn)∗
given
by
⟨A(u), v⟩= ∫Ω a(u, ∇u)∶∇v +
c(u, ∇u)⋅v dx + ∫ΓN b(u)⋅v dS, namely,
𝜕ail(x, u, F)
𝜕Fjk
=
𝜕akj(x, u, F)
𝜕Fli
,
(17.57a)
𝜕ail(x, u, F)
𝜕uj
=
𝜕cj(x, u, F)
𝜕Fli
,
(17.57b)
𝜕cj(x, u, F)
𝜕ul
= 𝜕cl(x, u, F)
𝜕uj
(17.57c)
for all i, k = 1, … , d and j, l = 1, … , n and for
a.a. (x, u, F)∈Ω×ℝn×ℝn×d, and also
𝜕bj(x, u)
𝜕ul
= 𝜕bl(x, u)
𝜕uj
.
(17.57d)
for all j, l = 1, … , n and for a.a. (x, u) ∈
Γ × ℝn. Note that (17.57a–c) just means a
symmetry for the Jacobian of the map-
ping
(F, u) →(a(x, u, F), c(x, u, F)) ∶
ℝn×d × ℝd →ℝn×d × ℝd
while
(17.57d)
is the symmetry for the Jacobian of
b(x, ⋅) ∶ℝn →ℝn.
Then (17.6) leads to the formula (17.52a)
with
𝜑(x, u, F) = ∫
1
0
a(x, 𝜆u, 𝜆F)∶F
+ c(x, 𝜆u, 𝜆F)⋅u d𝜆, (17.58a)
𝜙(x, u) = ∫
1
0
b(x, 𝜆u)⋅u d𝜆.
(17.58b)
Relying on the minimization-of-energy
principle described above, which is often
a
governing
principle
in
steady-state
mechanical and physical problems, and on
Theorem 17.1 or 17.2, one can prove exis-
tence of weak solutions to the boundary-
value problem by the direct method; cf. e.g.
[29–32]. Theorem 17.2 imposes a strong
(although
often
applicable)
structural
restriction that 𝜑(x, ⋅, ⋅) ∶ℝn×ℝn×d →ℝ
and 𝜙(x, ⋅) ∶ℝn →ℝare convex for a.a. x.
Yet, in general, Theorem 17.1 places
fewer restrictions on 𝜑and 𝜙by requiring
only weak lower semicontinuity of Φ. The
precise condition (i.e., suﬃcient and neces-
sary) that guarantees such semicontinuity
of u →∫Ω 𝜑(x, u, ∇u) dx on W 1,p(Ω; ℝn) is
called W 1,p-quasiconvexity, deﬁned in a
rather nonexplicit way by requiring
∀x∈Ω ∀u∈ℝn ∀F ∈ℝn×d ∶𝜑(x, u, F) =
=
inf
v∈W1,p
0
(O;ℝd) ∫O
𝜑(x, u, F+∇v(𝜉))
measd(O)
d𝜉,
where O ⊂ℝd is an arbitrary smooth
domain. This condition cannot be ver-
iﬁed eﬃciently except for very special
cases, unlike, for example, polyconvexity
which is a (strictly) stronger condition.
Subsequently,
another
type
of
con-
vexity,
called
rank-one
convexity,
was
introduced by Morrey [33] by requiring

566
17 Calculus of Variations
𝜆→𝜑(x, u, F+𝜆a⊗b) ∶ℝ→ℝto be con-
vex for any a∈ℝd, b∈ℝn, [a⊗b]ij = aibj.
For smooth 𝜑(x, u, ⋅), rank-one convexity
is equivalent to the Legendre–Hadamard
condition 𝜑′′
FF(x, u, F)(a⊗b, a⊗b) ≥0 for
all a∈ℝn
and b∈ℝd. Since Morrey’s
[33] introduction of quasiconvexity, the
question of its coincidence with rank-one
convexity had been open for many decades
and eventually answered negatively by
Šverák [34] at least if n ≥3 and d ≥2.
Weak lower semicontinuity of the bound-
ary integral u →∫Ω 𝜙(x, u) dS in (17.52a)
does not entail any special structural con-
dition because one can use compactness
of the trace operator, cf. Theorem 17.11.
Here, Theorem 17.1 leads to the following
theorem:
Theorem 17.13 (Direct method) Let
(17.53) hold with 𝜖> 0, let 𝜑(x, u, ⋅) be qua-
siconvex, and let uD ∈W 1−1∕p,p(ΓD; ℝn).35)
Then (17.54) has a solution, that is, the
boundary-value problem (17.55) has a
weak solution.
For n = d, an example for a quasiconvex
function is 𝜑(x, u, F) = 𝔣(x, u, F, det F) with
a convex function 𝔣(x, u, ⋅, ⋅) ∶ℝd×d × ℝ→
ℝ. The weak lower semicontinuity of
Φ from (17.52a) is then based on the
weak continuity of the nonlinear map-
ping induced by det ∶ℝd×d × ℝ→ℝif
restricted to gradients, that is,
uk →u weakly in W 1,p(Ω; ℝd)
⇒
det∇uk →det∇u weakly in Lp∕d(Ω),
(17.59)
which holds for p > d; note that nonaﬃne
mappings on Lebesgue spaces such as G →
35) Without going into detail concerning the so-
called Sobolev–Slobodetski˘i spaces with frac-
tional derivatives, this condition means exactly
that uD allows an extension onto Ω belonging
to W 1,p(Ω; ℝn).
det G
with
G ∈Lp(Ω; ℝd×d) →Lp∕d(Ω)
can be continuous36) but not weakly con-
tinuous, so (17.59) is not entirely trivial.
Even less trivial, it holds for p = d locally
(i.e., in L1(K) for any compact K ⊂Ω) if
det∇uk ≥0.37) Invented by Ball [36], such
functions 𝜑(x, u, ⋅) are called polyconvex,
and in general this property requires
𝜑(x, u, F) = 𝔣
(
x, u, (adjiF)min(n,d)
i=1
)
(17.60)
for some 𝔣∶Ω × ℝn × ∏min(n,d)
i=1
ℝ𝜅(i,n,d) →
ℝ∪{∞} such that 𝔣(x, u, ⋅) is convex,
where 𝜅(i, n, d) is the number of all
minors
of
the
ith
order
and
where
adjiF
denotes
the
determinants
of
all
(i×i)-submatrices.
Similarly,
as
in
(17.59), we have that adji∇uk →adji∇u
weakly
in
Lp∕i(Ω; ℝ𝜅(i,n,d))
provided
p > i ≤min(n, d),
and
Theorem 17.13
directly applies if 𝔣from (17.60) gives 𝜑
satisfying (17.53a–c).
Yet, this special structure allows for much
weaker restrictions on 𝜑if one is concerned
with the minimization of Φ itself rather
than the satisfaction of the Euler–Lagrange
equation (17.54):
Theorem 17.14 (Direct method, poly-
convex) Let 𝜑be a normal integrand38)
satisfying
(17.53a)
with
𝜑(x, u, ⋅) ∶
ℝn×d →ℝ∪{∞}
polyconvex,
and
let
uD ∈W 1−1∕p,p(ΓD; ℝn). Then the minimiza-
tion problem (17.52) has a solution.
Obviously,
polyconvexity
(and
thus
also
quasi-
and
rank-one
convexity)
is weaker than usual convexity. Only
for min(n, d) = 1, all mentioned modes
coincide with usual convexity of 𝜑(x, u, ⋅).
36) For p ≥d, Theorem 17.12 gives this continuity.
37) Surprisingly, not only {det∇uk}k∈ℕbut even
{det∇uk ln(2+det∇uk)}k∈ℕstays bounded in
L1(K), as proved by S. Müller in [35].
38) This means 𝜑is measurable but 𝜑(x, ⋅, ⋅) is only
lower semicontinuous.

17.3 Variational Problems on Speciﬁc Function Spaces
567
Example 17.2 [Oscillation eﬀects.] A sim-
ple one-dimensional counterexample for
nonexistence of a solution due to oscillation
eﬀects is based on
Φ(u) = ∫
L
0
((du
dx
)2−1
)2
+ u2 dx
(17.61)
to
be
minimized
for
u ∈W 1,4([0, L]).
A minimizing sequence {uk}k∈ℕis, for
example,39)
uk(0) = 1
k , duk
dx =
{
1 if sin(kx)>0,
−1 otherwise. (17.62)
Then Φ(uk) = 𝒪(1∕k2) →0 for k→∞,
so that inf Φ = 0. Yet, there is no u such
that Φ(u) = 0.40) We can observe that
Theorem 17.1 (resp. Theorem 17.2) cannot
be used due to lack of weak lower semicon-
tinuity (resp. convexity) of Φ which is due to
nonconvexity of the double-well potential
density F →𝜑(x, u, F) = (|F|2−1)2 + u2; cf.
also (17.105) below for a “ﬁne limit” of the
fast oscillations from Figure 17.1.
Example 17.3 [Concentration eﬀects.] The
condition that V in Theorems 17.1 and
17.2 has a pre-dual, is essential. A sim-
ple one-dimensional counterexample for
nonexistence of a solution in the situation
where V is not reﬂexive and even does not
have any pre-dual, is based on
Φ(u) =∫
1
−1
(1+x2)|||
du
dx
||| dx
+ (u(−1)+1)2 + (u(1)−1)2
(17.63)
for u ∈W 1,1([−1, 1]). If u were a minimizer,
then u must be nondecreasing (otherwise,
39) Actually, uk(0) ≠0 was used in (17.62) only for
a better visualization on Figure 17.1.
40) Indeed, then both ∫L
0 ((du∕dx)2−1)2dx and
∫L
0 u2dx would have to be zero, so that u = 0,
but then also du∕dx = 0, which however
contradicts ∫L
0 ((du∕dx)2−1)2dx = 0.
it obviously would not be optimal), and we
can always take some “part” of the nonneg-
ative derivative of this function and add
the corresponding area in a neighborhood
of 0. This does not aﬀect u(±1) but makes
∫1
−1(1+x2)|du∕dx| dx lower, contradicting
the original assumption that u is a min-
imizer. In fact, as 1+x2 in (17.63) attains
its minimum only at a single point x = 0,
any minimizing sequence {uk}k∈ℕis forced
to concentrate its derivative around x = 0.
For example considering, for k ∈ℕand
𝓁∈ℝ, the sequence given by
uk(x) =
𝓁kx
1 + k|x|
(17.64)
yields
Φ(uk) = 2𝓁+ 2(𝓁−1)2 + 𝒪(1∕k2).
Obviously,
the
sequence
{uk}k∈ℕ
will
minimize
Φ
provided
𝓁= 1∕2;
then
limk→∞Φ(uk) = 3∕2 = inf Φ;
see
Figure 17.2. On the other hand, this value
inf Φ cannot be achieved, otherwise such u
must have simultaneously |du∕dx| = 0 a.e.
and u(±1) = ±1∕2, which is not possible.41)
A
similar
eﬀect
occurs
for
𝜑(F) =
√
1 + |F|2 for which ∫Ω 𝜑(∇u) dx is the
area of the parameterized hypersurface
{(x, u(x)); x ∈Ω} in ℝd+1. Minimization of
such a functional is known as the Plateau
variational
problem.
Hyper-surfaces
of
minimal area typically do not exists in
W 1,1(Ω), especially if Ω is not convex and
the concentration of the gradient typically
occurs on Γ rather than inside Ω, cf. e.g.
[37, Chapter V].
Example 17.4 [Lavrentiev
phenomenon.]
Coercivity in Theorems 17.1 and 17.2 is also
essential even if Φ is bounded from below.
An
innocent-looking
one-dimensional
41) This is because of the concentration
eﬀect. More precisely, the sequence
{duk∕dx}k∈ℕ⊂L1(−1, 1) is not uniformly
integrable.

568
17 Calculus of Variations
u
u1
u2
u4
u8
etc.
Ω
L = 6π
0
Figure 17.1
A minimizing sequence (17.62) for Φ from
(17.61) whose gradient exhibits faster and faster spatial
oscillations.
u
ℓ
−1
1
0
−ℓ
u15
etc.
u7
u3
u1
Ω
Figure 17.2
A minimizing sequence (17.64) for Φ from
(17.63) whose gradient concentrates around the point x = 0
inside Ω.
counterexample for nonexistence of a solu-
tion in the situation where V is reﬂexive
and Φ ≥0 is continuous and weakly lower
semicontinuous is based on
Φ(u) =∫
1
0
(u3−x)2(du
dx
)6
dx
subject to u(0) = 0, u(1) = 1,
(17.65)
for u ∈W 1,6([0, 1]) = V. The minimum
of (17.65) is obviously 0, being realized
on
u(x) = x1∕3.
Such
u ∈W 1,1([0, 1]),
however, does not belong to W 1,6([0, 1])
because |du∕dx|6 = 3−6x−4 is not inte-
grable owing to its singularity at x = 0.
Thus (17.65) attains the minimum on
W 1,p([0, 1]) with 1 ≤p < 3∕2 although Φ
is not (weakly lower semi-) continuous
and even not ﬁnite on this space, and
thus abstract Theorem 17.1 cannot be
used. A surprising and not entirely obvious
phenomenon is that the inﬁmum (17.65)
on W 1,6([0, 1]) is positive, that is, greater
than the inﬁmum on W 1,p([0, 1]) with
p < 3∕2; this eﬀect was ﬁrst observed in
[38], cf. also, e.g. [1, Section 4.3.]. Note
that W 1,6([0, 1]) is dense in W 1,p([0, 1])
but one cannot rely on Φ(uk) →Φ(u) if
uk →u in W 1,p([0, 1]) for p < 6; it can even
happen that Φ(u) = 0 while Φ(uk) →∞
for uk →u, a repulsive eﬀect, cf. [39,
Section 7.3]. Here 𝜑(x, u, ⋅) is not uniformly
convex, yet the Lavrentiev phenomenon
can occur even for uniformly convex 𝜑’s,
cf. [40].
17.3.2.2
Fourth Order Systems
Higher-order problems can be considered
analogously but the complexity of the prob-
lem grows with the order. Let us therefore
use for illustration fourth-order problems
only, governed by an integral functional

17.3 Variational Problems on Speciﬁc Function Spaces
569
Φ(u) = ∫Ω
𝜑(x, u, ∇u, ∇2u) dx
+ ∫ΓN
𝜙(x, u, ∇u) dS
(17.66a)
involving
Carathéodory
integrands
𝜑∶Ω×ℝn×ℝn×d×ℝn×d×d →ℝ
and
𝜙∶ΓN×ℝn×ℝn×d →ℝ. The functional Φ is
considered on an aﬃne closed manifold
{
u∈W 2,p(Ω; ℝn); u|ΓD = uD,1,
𝜕u
𝜕⃗n|ΓD = uD,2
}
(17.66b)
for a given suitable uD,1 and uD,2. Instead
of (17.54), the abstract Euler–Lagrange
equation (17.3) now leads to the integral
identity:
∫Ω
(
𝜑′
∇2u(x, u, ∇u, ∇2u)⋮∇2v
+ 𝜑′
∇u(x, u, ∇u, ∇2u)∶∇v
+ 𝜑′
u(x, u, ∇u, ∇2u)⋅v
)
dx
+ ∫ΓN
(
𝜙′
∇u(x, u, ∇u)⋅𝜕v
𝜕⃗n
+ 𝜙′
u(x, u, ∇u)⋅v
)
dS = 0
(17.67)
for any v∈W 2,p(Ω; ℝn) such that v|ΓD= 0
and 𝜕u∕𝜕⃗n|ΓD= 0; the notation “ ⋮” stands
for summation over three indices. Com-
pleted by the Dirichlet conditions on ΓD,
this represents a weak formulation of the
boundary-value problem for a system of
fourth-order elliptic quasilinear equations
div2 𝜑′
∇2u(u, ∇u, ∇2u)
−div 𝜑′
∇u(u, ∇u, ∇2u)
+ 𝜑′
u(u, ∇u, ∇2u) = 0 in Ω,
(17.68a)
with two natural (although quite compli-
cated) boundary conditions prescribed on
each part of the boundary, namely,
(div 𝜑′
∇2u(u,∇u,∇2u)−𝜑′
∇u(u,∇u,∇2u))⋅⃗n
+ divS
(𝜑′
∇2u(u, ∇u, ∇2u)⃗n)
−
(
divS⃗n
)(⃗n⊤𝜑′
∇2u(u, ∇u, ∇2u)⃗n)
+ 𝜙′
∇u(u, ∇u) = 0
on ΓN, (17.68b)
𝜑′
∇2u(u, ∇u, ∇2u)∶(⃗n⊗⃗n)
+ 𝜙′
u(u, ∇u) = 0
on ΓN, (17.68c)
u||Γ = uD,1,
𝜕u
𝜕⃗n
|||Γ = uD,2
on ΓD. (17.68d)
Again,
(17.68)
is
called
the
classical
formulation
of
the
boundary-value
problem in question, and its derivation
from (17.67) is more involved than in
Section 17.3.2.1. One must use a general
decomposition ∇v = 𝜕v∕𝜕⃗n + ∇Sv on Γ with
∇Sv = ∇v −𝜕v∕𝜕⃗n being the tangential gra-
dient of v. On a smooth boundary Γ, one
can use another (now (d−1)-dimensional)
Green-type formula on tangent spaces:42)
∫Γ
a∶(⃗n⊗∇v) dS
= ∫Γ
(
⃗n⊤a⃗n
) 𝜕v
𝜕⃗n + a∶(⃗n⊗∇Sv) dS
= ∫Γ
(
⃗n⊤a⃗n
) 𝜕v
𝜕⃗n −divS
(a⃗n)v
+ (divS⃗n
)(⃗n⊤a⃗n
)
v dS,
(17.69)
where
a = 𝜑′
∇2u(x, u, ∇u, ∇2u)
and
divS = tr(∇S) with tr(⋅) being the trace
of a (d−1)×(d−1)-matrix, denotes the
(d−1)-dimensional surface divergence so
that divS⃗n is (up to a factor −1∕2) the mean
curvature of the surface Γ. Comparing the
variational formulation as critical points of
(17.66) with the weak formulation (17.67)
and with the classical formulation (17.68),
one can see that although formally all for-
mulations are equivalent to each other, the
advantage of the variational formulations
such as (17.66) in its simplicity is obvious.
42) This “surface” Green-type formula reads
∫Γ w∶((∇Sv)⊗⃗n) dS = ∫Γ(divS⃗n)(w∶(⃗n⊗⃗n))v −
divS(w⋅⃗n)v dS.

570
17 Calculus of Variations
As in (17.56), taking general Carathéo-
dory integrands a ∶Ω×ℝn×ℝn×d×ℝn×d×d
→
ℝn×d×d,
b ∶Ω×ℝn×ℝn×d×ℝn×d×d →
ℝn×d,
c ∶Ω×ℝn×ℝn×d×ℝn×d×d →ℝn,
d ∶ΓN×ℝn×ℝn×d →ℝn×d,
and
ﬁnally
e ∶ΓN×ℝn×ℝn×d →ℝn, one can consider
a boundary-value problem for a system of
fourth-order elliptic quasilinear equations:
div2a(u, ∇u, ∇2u) −div b(u, ∇u, ∇2u)
+ c(u, ∇u, ∇2u) = 0
in Ω, (17.70a)
with the boundary conditions (17.68d) and
a(u, ∇u, ∇2u)∶(⃗n⊗⃗n)
+ d(u, ∇u) = 0
on ΓN,
(17.70b)
(div a(u,∇u,∇2u)−b(u,∇u,∇2u))⋅⃗n
+ divS
(a(u, ∇u, ∇2u)⃗n)
−
(
divS⃗n
)(⃗n⊤a(u, ∇u, ∇2u)⃗n)
+ e(u, ∇u) = 0
on ΓN.
(17.70c)
Existence of a potential for which the
boundary-value problem (17.70) would
represent the Euler–Lagrange equation
(17.3)
requires
the
symmetry
for
the
Jacobians
of
the
mapping
(H, F, u) →
(a(x, u, F, H), b(x, u, F, H), c(x, u, F, H)) ∶
ℝn×d×d×ℝn×d×ℝd→ℝn×d×d×ℝn×d×ℝd
and of the mapping (F, u) →(d(x, u, F),
e(x, u, F))∶ℝn×d×ℝd→ℝn×d×ℝd; we used
H as a placeholder for ∇2u. The formula
(17.58) then takes the form:
𝜑(x, u, F, H) = ∫
1
0
a(x, 𝜆u, 𝜆F, 𝜆H)⋮H
+ b(x, 𝜆u, 𝜆F, 𝜆H)∶F
+ c(x, 𝜆u, 𝜆F, 𝜆H)⋅u d𝜆,
(17.71a)
𝜙(x, u, F) = ∫
1
0
d(x, 𝜆u, 𝜆F)∶F
+ e(x, 𝜆u, 𝜆F)⋅u d𝜆.
(17.71b)
Analogously
to
Theorem 17.13,
one
can obtain existence of weak solutions
by the direct method under a suitable
coercivity/growth conditions on 𝜑and an
analogue of quasiconvexity of 𝜑(x, u, F, ⋅) ∶
ℝn×d×d →ℝ,
and
uD,1 ∈W 2−1∕p,p(ΓD; ℝn)
and uD,2 ∈W 1−1∕p,p(ΓD; ℝn).
So far, we considered two Dirichlet-type
conditions (17.68d) on ΓD (dealing with
zeroth- and ﬁrst-order derivatives) and
two Robin-type conditions (17.68b,c) on
ΓN (dealing with second- and third-order
derivatives). These arise either by ﬁxing
both u|Γ or 𝜕u∕𝜕⃗n|Γ or neither of them,
cf. (17.66b). One can, however, think
about ﬁxing only u|Γ or only 𝜕u∕𝜕⃗n|Γ,
which gives other two options of natural
boundary conditions, dealing with zeroth-
and second-order derivatives or ﬁrst- and
third-order derivatives, respectively.
The other two combinations, namely the
zeroth- and the third-order derivatives or
the ﬁrst- and the second-order derivatives,
are not natural from the variational view-
point because they overdetermine some of
the two boundary terms arising in the weak
formulation (17.67).
17.3.2.3
Variational Inequalities
Merging
the
previous
Sections 17.3.2.1
–17.3.2.2
with
the
abstract
scheme
from
Sections 17.2.2–17.2.3,
has
important applications. Let us use as
illustration
Φ0 = Φ
from
(17.52)
and
Φ1(v) = ∫Ω 𝜁(v) dx + ∫ΓN 𝜉(v) dS
as
in
Remark 17.1,
now
with
some
convex
𝜁, 𝜉∶ℝn →ℝ∪{+∞}. In view of the
abstract inequality (17.10), the weak for-
mulation
(17.54)
gives
the
variational
inequality
∫Ω
𝜁(v) + 𝜑′
∇u(x, u, ∇u)∶∇(v−u)
+ 𝜑′
u(x, u, ∇u)⋅(v−u) dx
+ ∫Γ
𝜉(v) + 𝜙′
u(x, u)⋅(v−u) dS

17.3 Variational Problems on Speciﬁc Function Spaces
571
≥∫Ω
𝜁(u) dx + ∫Γ
𝜉(u) dS
(17.72)
for any v ∈W 1,p(Ω; ℝn) such that v|ΓD = 0.
The passage from the weak formulation to
the classical boundary-value problem anal-
ogous to (17.54) −→(17.55) leads to the dif-
ferential inclusion on Ω:
div 𝜑′
∇u(u, ∇u) −𝜑′
u(u, ∇u) −𝜕𝜁(u) ∋0
in Ω,
(17.73a)
with another diﬀerential inclusion in the
boundary conditions
𝜑′
∇u(u, ∇u)⋅⃗n + 𝜙′
u(u) + 𝜕𝜉(u) ∋0
on ΓN,
(17.73b)
u||Γ = uD
on ΓD. (17.73c)
There is an extensive literature on mathe-
matical methods in variational inequalities,
cf. e.g. [18, 41–44].
17.3.3
Some Examples
Applications
of
the
previous
general
boundary-value problems to more spe-
ciﬁc situations in continuum physics are
illustrated in the following examples.
17.3.3.1
Nonlinear Heat-Transfer Problem
The
steady-state
temperature
distri-
bution
𝜃
in
an
anisotropic
nonlinear
heat-conductive body Ω ⊂ℝ3 is described
by the balance law
div j = f
with
j = −𝜅(𝜃)𝕂∇𝜃
on Ω,
(17.74a)
⃗n⋅j + b(𝜃) = g
on Γ,
(17.74b)
where b(⋅)>0 is a boundary heat outﬂow,
g the external heat ﬂux, f the bulk heat
source, and with the heat ﬂux j governed
by the Fourier law involving a symmet-
ric positive deﬁnite matrix 𝕂∈ℝd×d and
a
nonlinearity
𝜅∶ℝ→ℝ+.
In
terms
of a and c in (17.56a), we have n = 1,
a(x, u, F) = 𝜅(u)𝕂F
and
c(x, u, F) = f (x)
and the symmetry (17.57b) fails, so that
(17.74) does not have the variational
structure unless 𝜅is constant. Yet, a sim-
ple rescaling of 𝜃, called the Kirchhoﬀ
transformation, can help: introducing the
substitution
u = ̂𝜅(𝜃) = ∫𝜃
0 𝜅(𝜗) d𝜗,
we
have j = −𝕂∇u and (17.74) transforms to
div(𝕂∇u) + f = 0
on Ω, (17.75a)
⃗n⊤𝕂∇u + b(̂𝜅−1(u)) = g
on Γ,
(17.75b)
which already ﬁts in the framework of
(17.52)
with
𝜑(x, u, F) = 1
2F⊤𝕂F −f (x)u
and 𝜙(x, u) = ̃b(u) −g(x)u where ̃b is a
primitive of b ∘̂𝜅−1. Eliminating the non-
linearity from the bulk to the boundary, we
thus gain a variational structure at least if
f ∈L6∕5(Ω) and g ∈L4∕3(Γ) 43) and thus, by
the direct method, we obtain existence of
a solution u ∈H1(Ω) to (17.75) as well as a
possibility of its eﬃcient numerical approx-
imation, cf. Section 17.4.1; then 𝜃= ̂𝜅−1(u)
yields a solution to (17.74). Our optimism
should however be limited because, in the
heat-transfer context, the natural integra-
bility of the heat sources is only f ∈L1(Ω)
and g ∈L1(Γ), but this is not consistent with
the variational structure if d>1:
Example 17.5 [Nonexistence of minimiz-
ers] Consider the heat equation −div ∇u =
0 for d = 3 and with, for simplicity, zero
Dirichlet boundary conditions, so that the
underlying variational problem is to mini-
mize ∫Ω
1
2|∇u|2−fudx on H1
0(Ω). Yet,
43) If d = 3, this integrability of the heat sources
is necessary and suﬃcient to ensure the func-
tional (u →∫Ω fu dx + ∫Γ gu dS) to belong to
H1(Ω)∗.

572
17 Calculus of Variations
inf
u∈H1
0 (Ω) ∫Ω
1
2|∇u|2 −fu dx = −∞,
(17.76)
whenever f ∈L1(Ω)⧵L6∕5(Ω).44)
17.3.3.2
Elasticity at Large Strains
A prominent application of multidimen-
sional (d > 1) vectorial (n > 1) variational
calculus is to elasticity under the hypothesis
that the stress response on the deformation
gradient is a gradient of some potential;
such materials are called hyperelastic. The
problem is very diﬃcult especially at large
strains where the geometry of the stan-
dard Cartesian coordinates may be totally
incompatible with the largely deformed
geometry of the specimen Ω.
Here, u will stand for the deformation
(although the usual notation is rather y) and
we will consider n = d and 𝜑= 𝜑(F) tak-
ing possibly also the value +∞. The ultimate
requirement is frame indiﬀerence, that is,
𝜑(RF) = 𝜑(F) for all R ∈ℝd×d in the spe-
cial orthogonal group SO(d). One could
try to rely on Theorem 17.13 or 17.14. In
the former case, one can consider non-
polyconvex materials and one also has the
Euler–Lagrange equation (17.54) at one’s
disposal, but the growth condition (17.53b)
does not allow an inﬁnite increase of energy
when the volume of the material is locally
shrunk to 0, that is, we cannot satisfy the
condition
𝜑(F) →+∞
if det F →0+
(17.77a)
𝜑(F) = +∞
if det F ≤0.
(17.77b)
An example for a polyconvex frame-
indiﬀerent 𝜑satisfying (17.77) is the Ogden
material
44) The proof is very simple: f ∉H1
0(Ω)∗means
‖f ‖H1
0 (Ω)∗= supu∈W 1,∞
0
(Ω), ‖u‖1,2≤1 ∫Ω fu dx = +∞,
which further means ∫Ω fuk dx →+∞for some
uk ∈W 1,∞
0
(Ω) such that ∫Ω
1
2 |∇uk|2 dx ≤1
2 , so
that ∫Ω
1
2 |∇uk|2 −fuk dx →−∞.
𝜑(F) = a1tr(F⊤F)b1
+ a2|tr(cof(F⊤F))|b2 + 𝛾(det F) (17.78)
with
a1, a2, b1, b2 > 0
and
𝛾∶ℝ+ →ℝ
convex such that 𝛾(𝛿) = +∞for 𝛿≤0
and
lim𝛿→0+ 𝛾(𝛿) = +∞,
and
where
cof(A) = (detA)A⊤, considering d = 3, and
where tr A = ∑d
i=1 Aii. Particular Ogden
materials are Mooney–Rivlin materials
with 𝜑(F) = |F|2 + |det F|2 −ln(det F) or
compressible neo-Hookean materials with
𝜑(F) = a|F|2 + 𝛾(det F). The importance
of polyconvexity is that the existence of
energy-minimizing deformation can be
based on Theorem 17.14, which allows us
to handle local nonpenetration
det(∇u) > 0
a.e. on Ω
(17.79a)
involved in 𝜑via (17.77). Handling of non-
penetration needs also the global condition
∫Ω
det(∇u) dx ≤measd
(u(Ω));
(17.79b)
(17.79) is called the Ciarlet–Neˇcas nonpen-
etration condition [45]. Whether it can deal
with quasiconvex but not polyconvex mate-
rials remains an open question, however
cf. [46].
An
interesting
observation
is
that
polyconvex materials allow for an energy-
controlled stress |𝜑′(F)F⊤| ≤C(1 + 𝜑(F))
even though the so-called Kirchhoﬀstress
𝜑′(F)F⊤itself does not need to be bounded.
This can be used, for example, in sensi-
tivity analysis and to obtain modiﬁed
Euler–Lagrange equations to overcome
the possible failure of (17.54) for such
materials, cf. [46]. It is worth noting that
even such spatially homogeneous, frame-
indiﬀerent,
and
polyconvex
materials
can exhibit the Lavrentiev phenomenon,
cf. [47].
Another
frequently
used
ansatz
is
just a quadratic form in terms of the

17.3 Variational Problems on Speciﬁc Function Spaces
573
Green–Lagrange strain tensor
E = 1
2F⊤F −1
2𝕀.
(17.80)
An
example
is
an
isotropic
material
described by only two elastic constants;
in terms of the Lamé constants 𝜇and 𝜆, it
takes the form
𝜑(F) = 1
2𝜆|trE|2 + 𝜇|E|2,
(17.81)
and is called a St.Venant–Kirchhoﬀ’s mate-
rial, cf. [48, Volume I, Section 4.4]. If 𝜇> 0
and 𝜆> −2
d𝜇, 𝜑from (17.81) is coercive in
the sense of 𝜑(F) ≥𝜖0|F|4 −1∕𝜖0 for some
𝜖0 > 0 but not quasiconvex (and even not
rank-one convex), however. Therefore, exis-
tence of an energy-minimizing deforma-
tion in W 1,4(Ω; ℝd) is not guaranteed.
A way to improve solvability for non-
quasiconvex materials imitating additional
interfatial-like energy is to augment 𝜑
with
some
small
convex
higher-order
term,
for
example,
𝜑1(F, H) = 𝜑(F)
+
∑d
i,k,l,m,n=1ℍklmn(x)HiklHimn with ℍa (usu-
ally
only
small)
fourth-order
positive
deﬁnite tensor, and to employ the fourth-
order framework of Section 17.3.2.2. This
is the idea behind the mechanics of com-
plex (also called nonsimple or a special
micropolar) continua, cf. e.g., [49].
17.3.3.3
Small-Strain Elasticity, Lamé
System, Signorini Contact
Considering
the
deformation
y
and
displacement
u(x) = y(x) −x,
we
write
F = ∇y = ∇u + 𝕀and, for |∇u| small, the
tensor E from (17.80) is
E = F⊤F−𝕀
2
= (∇u)⊤+∇u+(∇u)⊤∇u
2
= 1
2(∇u)⊤+ 1
2∇u + o(|∇u|),
(17.82)
which
leads
to
the
deﬁnition
of
the
linearized-strain tensor, also called small-
strain tensor, as e(u) = 1
2(∇u)⊤+ 1
2∇u. In
fact, a vast amount of engineering or also,
for example, geophysical models and cal-
culations are based on the small-strain
concept. The speciﬁc energy in homo-
geneous materials is then 𝜑∶ℝd×d
sym →ℝ
where ℝd×d
sym = {A ∈ℝd×d; A⊤= A}. In lin-
early responding materials, 𝜑is quadratic.
An example is an isotropic material; in
terms of Lamé’s constants as in (17.81), it
takes the form
𝜑Lame(e) = 1
2𝜆|tr e|2 + 𝜇|e|2.
(17.83)
Such 𝜑Lame is positive deﬁnite on ℝd×d
sym if
𝜇> 0 and 𝜆> −(2d∕𝜇). The positive deﬁ-
niteness of the functional ∫Ω 𝜑Lame(e(u)) dx
is a bit delicate as the rigid-body motions
(translations and rotations) are not taken
into account by it. Yet, ﬁxing positive def-
initeness by suitable boundary conditions,
coercivity can then be based on the Korn
inequality
∀v∈W 1,p(Ω; ℝd), v|ΓD = 0 ∶
‖‖v‖‖W1,p(Ω;ℝd) ≤Cp‖‖e(v)‖‖Lp(Ω;ℝd×d
sym)
(17.84)
to be used for p = 2; actually, (17.84)
holds for p > 1 on connected smooth
domains with ΓD of nonzero measure,
but notably counterexamples exist for
p = 1. Then, by the direct method based
on Theorem 17.2, one proves existence
and uniqueness of the solution to the
Lamé system arising from (17.55) by con-
sidering
𝜑(x, u, F) = 𝜑Lame( 1
2F⊤+ 1
2F)
and
𝜙(x, u) = g(x)u:
div 𝜎+ f = 0
in Ω
with 𝜎= 2𝜇e(u)+𝜆(div u)𝕀, (17.85a)
𝜎⃗n = g
on ΓN,
(17.85b)
u||Γ = uD
on ΓD.
(17.85c)
The Lamé potential (17.83) can be
obtained by an asymptotic expansion of an

574
17 Calculus of Variations
Ogden material (17.78), see [48, Volume I,
Theorem 4.10-2].
An illustration of a very speciﬁc varia-
tional inequality is a Signorini (frictionless)
contact on a third part ΓC of the boundary
Γ; so now we consider Γ divided into three
disjoint relatively open parts ΓD, ΓN, and ΓC
whose union is dense in Γ. This is a special
case of the general problem (17.72) with
𝜁≡0 and 𝜉(x, u) = 0 if u⋅⃗n(x) ≤0 on ΓC,
otherwise 𝜉(x, u) = +∞. In the classical
formulation, the boundary condition on ΓC
can be identiﬁed as
(𝜎⃗n)⋅⃗n ≤0,
u⋅⃗n ≤0,
((𝜎⃗n)⋅⃗n)(u⋅⃗n) = 0
⎫
⎪
⎬
⎪⎭
on ΓC ,
(17.85d)
(𝜎⃗n) −((𝜎⃗n)⋅⃗n)⃗n = 0
on ΓC ;
(17.85e)
note that (17.85d) has a structure of a
complementarity
problem
(17.17)
for
the normal stress (𝜎⃗n)⋅⃗n and the normal
displacement u⋅⃗n, while (17.85e) is the
equilibrium condition for the tangential
stress.
This very short excursion into a wide area
of contact mechanics shows that it may
have a simple variational structure: In terms
of Example 17.1, the convex set K is a cone
(with the vertex not at origin if uD ≠0):
K =
{
u∈H1(Ω; ℝn); u||Γ = uD on ΓD,
u⋅⃗n ≤0 on ΓC
}
(17.86)
and then (17.85) is just the classical formu-
lation of the ﬁrst-order condition (17.13)
for the simple problem
minimize Φ(u) =∫Ω
𝜆
2 |div u|2+ 𝜇|e(u)|2dx
subject to u∈K from (17.86).
As in Section 17.3.2.2, it again demon-
strates one of the advantages of the
variational formulation as having a much
simpler form in comparison with the
classical formulation.
17.3.3.4
Sphere-Valued Harmonic Maps
Another example is a minimization prob-
lem with 𝜑(x, u, F)=𝜔(u)+h(x)⋅u + 𝜖
2|F|2
and the nonconvex constraint |u| = 1, that
is,
minimize Φ=∫Ω
𝜔(u)+𝜖
2|∇u|2−h⋅u dx
subject to R(u) = |u|2 −1 = 0 on Ω,
(17.87)
which has again the structure (17.14) now
with V = K = H1(Ω; ℝ3) and Λ = L2(Ω)
with the ordering by the trivial cone {0}
of “nonnegative” vectors. This may serve
as a simpliﬁed model of static micromag-
netism45) in ferromagnetic materials at low
temperatures so that the Heisenberg con-
straint |u| = 1 is well satisﬁed pointwise by
the magnetization vector u. Alternatively,
it may also serve as a simpliﬁed model of
liquid crystals. The weak formulation of
this minimization problem is
∫Ω
𝜖∇u∶∇(v−(u⋅v)u)
+ (𝜔′(u)−h)⋅(v−(u⋅v)u) dx = 0 (17.88)
for any v ∈H1(Ω; ℝ3) with |v|2 = 1 a.e. on
Ω; for 𝜔≡0 cf. [50, Section 8.4.3]. The cor-
responding classical formulation then has
the form
−div(𝜖∇u) + 𝜔′(u) −h
= (|∇u|2+ (𝜔′(u)−h)⋅u)u in Ω, (17.89a)
𝜖∇u⋅⃗n||Γ = 0
on Γ.
(17.89b)
45) In this case, 𝜔∶ℝ3 →ℝis an anisotropy
energy with minima at easy-axis magnetization
and 𝜖> 0 is an exchange-energy constant, and
h is an outer magnetic ﬁeld. The demagnetizing
ﬁeld is neglected.

17.3 Variational Problems on Speciﬁc Function Spaces
575
Comparing it with (17.16) with NK ≡{0}
and R′ = 2×identity on L2(Ω; ℝ3), we can
see that 𝜆∗= 1
2|∇u|2+ 1
2(𝜔′(u)−h)⋅u plays
the role of the Lagrange multiplier for the
constraint |u|2 = 1 a.e. on Ω.
Let us remark that, in the above-
mentioned
micromagnetic
model,
R
is more complicated than (17.87) and
involves also the diﬀerential constraints
div(h−u) = 0 and rot h = j
(17.90)
with j assumed ﬁxed, which is the (steady
state) Maxwell system where j is the electric
current, and the minimization in (17.87) is
to be done over the couples (u, h); in fact,
(17.90) is considered on the whole ℝ3 with j
ﬁxed and u vanishing outside Ω.
17.3.3.5
Saddle-Point-Type Problems
In addition to minimization principles,
other principles also have applications. For
example, for the usage of the mountain
pass Theorem 17.4 for potentials such as
𝜑(u, F) = 1
2|F|2 + c(u) see [50, Section 8.5]
or [51, Section II.6].
Seeking saddle points of Lagrangeans
such as (17.18) leads to mixed formula-
tions of various constrained problems. For
example, the Signorini problem (17.85)
uses
Φ(u) = ∫Ω 𝜑Lame(e(u)) dx + ∫ΓN g⋅u dS,
R ∶u →u⋅⃗n ∶H1(Ω; ℝd) →H1∕2(ΓC), and
D = {v ∈H1∕2(ΓC);
v ≤0}.
The
saddle
point
(u, 𝜆∗) ∈H1(Ω; ℝd) × H−1∕2(ΓC)
with u|ΓD = g and 𝜆∗≤0 on ΓC exists
and represents the mixed formulation of
(17.85); then 𝜆∗= (𝜎⃗n)⋅⃗n and cf. also the
Karush–Kuhn–Tucker conditions (17.16)
with NK ≡{0}, cf. e.g. [37, 43].
Another example is a saddle point
on
H1
0(Ω; ℝd) × L2(Ω)
for
ℒ(u, 𝜆∗) =
∫Ω
1
2|∇u|2 + 𝜆∗div u dx that leads to the
system
−Δu + ∇𝜆∗= 0 and div u = 0,
(17.91)
which is the Stokes system for a steady
ﬂow of viscous incompressible ﬂuid. The
primal formulation minimizes ∫Ω
1
2|∇u|2dx
on H1
0(Ω; ℝd) subject to div u = 0. The
Karush—Kuhn–Tucker conditions (17.16)
with
R ∶u →div u ∶H1
0(Ω; ℝd) →L2(Ω)
and the ordering of D = {0} as the cone of
nonnegative vectors gives (17.91).
17.3.4
Evolutionary Problems
Let
us
illustrate
the
Brezis–Ekeland–
Nayroles principle on the initial-boundary-
value problem for a quasilinear parabolic
equation
𝜕u
𝜕t −div(|∇u|p−2∇u) = g in Q, (17.92a)
u = 0 on Σ, (17.92b)
u(0, ⋅) = u0 in Ω
(17.92c)
with Q=[0, T]×Ω and Σ=[0, T]×Γ. We
consider V = W 1,p
0 (Ω) equipped with the
norm ‖u‖1,p = ‖∇u‖p, Φ(u) = (1∕p)‖∇u‖p
p
and ⟨f (t), u⟩= ∫Ω g(t, x)u(x) dx. Let us use
the notation Δp ∶W 1,p
0 (Ω) →W −1,p′(Ω) =
W −1,p
0
(Ω)∗for the p-Laplacian; this means
Δpu = div(|∇u|p−2∇u). We have that
Φ∗(𝜉) = 1
p′ ‖‖𝜉‖‖
p′
W−1,p′ (Ω)
= 1
p′ ‖‖Δ−1
p 𝜉‖‖
p
1,p = 1
p′ ‖‖∇Δ−1
p 𝜉‖‖
p
p .
Thus we obtain the following explicit form
of the functional 𝔉from (17.24):
𝔉(u) = ∫Q
1
p|∇u|p +
(𝜕u
𝜕t −g
)
u
+ 1
p′
|||∇Δ−1
p
(𝜕u
𝜕t −g
)|||
p
dxdt
+ ∫Ω
1
2
||u(T)||
2 dx.
(17.93)
We can observe that the integrand in
(17.93) is nonlocal in space, which is, in

576
17 Calculus of Variations
fact, an inevitable feature for parabolic
problems, cf. [52].
Wide generalization to self-dual prob-
lems of the type (Lx, Ax) ∈𝜕ℒ(Ax, Lx) are
in [53], covering also nonpotential situa-
tions such as the Navier–Stokes equations
for incompressible ﬂuids and many others.
Eﬃcient usage of the “global” varia-
tional principles such as (17.24), (17.32), or
(17.34) for parabolic equations or inequal-
ities is, however, limited to theoretical
investigations. Of much wider applicability
are recursive variational problems aris-
ing by implicit or various semi-implicit
time discretization as in Section 17.2.4.3,
possibly combined also with spatial dis-
cretization
and
numerical
algorithms
leading
to
computer
implementations,
mentioned in Section 17.4.1 below.
Other, nonminimization principles have
applications in hyperbolic problems of
the type 𝜌𝜕2u∕𝜕t2 −div(|∇u|p−2∇u) = g
where the Hamilton principle (17.37) leads
to seeking a critical point of the functional
∫T
0 ∫Ω 𝜌|𝜕u∕𝜕t|2 −(1∕p)|∇u|p + g⋅u dxdt.
17.4
Miscellaneous
The area of the calculus of variations is
extremely wide and the short excursion
above presented a rather narrow selection.
Let us at least brieﬂy touch a few more
aspects.
17.4.1
Numerical Approximation
Assuming {Vk}k∈ℕ
is a nondecreasing
sequence of ﬁnite-dimensional linear sub-
spaces of V whose union is dense, that is,
∀v ∈V ∃vk ∈Vk ∶
vk →v,
(17.94)
we can restrict the original variational
problem of V to Vk. Being ﬁnite dimen-
sional, Vk possesses a basis and, in terms
of coeﬃcients in this basis, the restricted
problem then becomes implementable on
computers. This is the simplest idea behind
numerical approximation, called the Ritz
method [54] or, rather in a more general
nonvariational context, also the Galerkin
method [55]. This idea can be applied on an
abstract level to problems in Section 17.2.
In the simplest (conformal) version instead
of (17.3), one is to seek uk ∈Vk such that
∀v∈Vk ∶
⟨Φ′(uk), v⟩= 0;
(17.95)
such uk is a critical point of the restriction
of Φ on Vk, that is, of Φ ∶Vk →ℝ.
One option is to solve numerically the
system of nonlinear equations (17.95) iter-
atively, for example, by the (quasi) Newton
method. Yet, the variational structure can
advantageously be exploited in a number
of other options such as the conjugate-
gradient or the variable-metric methods, cf.
e.g. [56, Section 7]. Then approximate sat-
isfaction of optimality conditions typically
serves as a stopping criterion for an itera-
tive strategy; in this unconstrained case, it
is the residual in (17.95) that is to be small.
For constrained problems, the methods of
Section 17.2.3 can be adapted.
Application to concrete problems in
Section 17.3.2 on function spaces opens
further interesting possibilities. Typically,
Vk are chosen as linear hulls of piecewise
polynomial functions {vkl}l=1,…,Lk whose
supports in Ω are only very small sets so
that, for most pairs (l1, l2), we have that
∫Ω ∇ivkl1⋮∇jvkl2dx = 0;
more
precisely,
this holds for each pair (l1, l2) for which
supp(vkl1) ∩supp(vkl2) = ∅.
For
integral
functionals Φ from (17.52) or (17.66a),
the system of algebraic equations resulting
from (17.95) is sparse, which facilitates its

17.4 Miscellaneous
577
implementation and numerical solution
on computers; this is the essence of the
ﬁnite-element method.
Phenomena
discussed
in
Examples 17.2–17.5
can
make
the
approximation
issue
quite
nontrivial.
For Lavrentiev-type phenomena, see, for
example, [39]. An example sure to lead to
this phenomenon is the constraint |u|2 = 1
in Section 17.3.3.4, which is not compatible
with any polynomial approximation so
that plain usage of standard ﬁnite-element
approximation cannot converge.
All this can be applied to time-discretized
evolution problems from Section 17.2.4.3,
leading
to
implementable
numerical
strategies for evolution problems from
Sections 17.2.4.1 and 17.2.4.2.
17.4.2
Extension of Variational Problems
Historically,
variational
problems
have
been
considered
together
with
the
Euler–Lagrange equations in their clas-
sical formulations, that is, in particular,
the solutions are assumed to be con-
tinuously diﬀerentiable. Here (17.55) or
(17.70) are to be considered holding point-
wise for u ∈C2(Ω; ℝd) and C4(Ω; ℝd),
respectively. Yet, such classical solutions
do not need to exist46) and thus the weak
formulations (17.54) and (17.67) repre-
sent a natural extension (using the density
C2k(Ω) ⊂W k,p(Ω)) of the original problems
deﬁned for smooth functions. The adjec-
tive “natural” here means the extension
by continuity, referring to the continuity
46) Historically the ﬁrst surprising example
for a minimizer of a (17.52) with 𝜑= 𝜑(F)
smooth uniformly convex, which was only in
W 1,∞(Ω; ℝm) but not smooth is due to Neˇcas
[57], solving negatively the 19th Hilbert’s prob-
lem [4] if d > 1 and m > 1 are suﬃciently high.
An example for even u ∉W 1,∞(Ω; ℝm) for
d = 3 and m = 5 is in [58].
of Φ ∶C2k(Ω; ℝd) →ℝwith respect to
the norm of W k,p(Ω; ℝd) provided natural
growth conditions on 𝜑and 𝜙are imposed;
for k = 1; see (17.53). Weak solutions thus
represent a natural generalization of the
concept of classical solutions.
In general, the method of extension by
(lower semi)continuity is called relaxation.
It may provide a natural concept of gener-
alized solutions with some good physical
meaning. One scheme, related to the min-
imization principle, deals with situations
when Theorem 17.1 cannot be applied
owing to the lack of weak* lower semi-
continuity. The relaxation then replaces
Φ by its lower semicontinuous envelope Φ
deﬁned by
Φ(u) = lim inf
v→u weakly* Φ(v).
(17.96)
Theorem 17.1 then applies to Φ instead
of the original Φ, yielding thus a gener-
alized solution to the original variational
problem. The deﬁnition (17.96) is only con-
ceptual and more explicit expressions are
desirable and sometimes actually available.
In particular, if n=1 or d=1, the integral
functional (17.52a) admits the formula
Φ(u) =∫Ω
𝜑∗∗(x, u, ∇u) dx + ∫ΓN
𝜙(x, u) dS,
(17.97)
where 𝜑∗∗(x, u, ⋅) ∶ℝn×d →ℝdenotes the
convex envelope of 𝜑(x, u, ⋅), that is, the
maximal convex minorant of 𝜑(x, u, ⋅). In
Example 17.2, Φ is given by (17.97) with
𝜑∗∗(u, F) =
{
u2+ (|F|2−1)2
if |F| ≥1,
u2
if |F| < 1,
cf. Figure 17.3, and with 𝜙= 0, and the
only minimizer of Φ on W 1,4(Ω) is u = 0,
which is also a natural W 1,4-weak limit of all
minimizing sequences for Φ, cf. Figure 17.1.

578
17 Calculus of Variations
𝜑(u,⋅)
𝜑** (u,⋅)
F
Figure 17.3
A convex envelope 𝜑∗∗(u, ⋅) of the
double-well potential 𝜑(u, ⋅).
Fast oscillations of gradients of these
minimizing sequences can be interpreted
as microstructure, while the minimizers
of Φ bear only “macroscopical” infor-
mation.
This
reﬂects
the
multiscale
character of such variational problems.
In general, if both n > 1 and d > 1,
(17.97) involves the quasiconvex enve-
lope 𝜑♯(x, u, ⋅) ∶ℝn×d →ℝrather than 𝜑∗∗;
this is deﬁned by
∀x∈Ω ∀u∈ℝn ∀F ∈ℝn×d ∶𝜑♯(x, u, F)
=
inf
v∈W1,p
0
(O;ℝn) ∫O
𝜑(x, u, F+∇v(̃x))
measd(O)
d̃x ;
this deﬁnition is independent of O but is
only implicit and usually only some upper
and lower estimates (namely, rank-one con-
vex and polyconvex envelopes) are known
explicitly or can numerically be evaluated.
To cope with both nonconvexity and with
the unwanted phenomenon of nonexis-
tence as in Example 17.2, one can consider
singular perturbations, such as
Φ𝜀(u) = ∫Ω
𝜑(x, u, ∇u) + 𝜀ℍ∇2u⋮∇2u dx
+ ∫ΓN
𝜙(x, u) dS
(17.98)
with
a
positive
deﬁnite
fourth-
order
tensor
ℍ
and
small
𝜀> 0;
cf.
also ℍ
in
Section 17.3.3.2.
Under
the
growth/coercivity conditions on 𝜑and 𝜙
induced by (17.53) with 1 < p < 2∗, (17.98)
possesses a (possibly nonunique) mini-
mizer u𝜀∈W 2,2(Ω; ℝd). The parameter
𝜀determines an internal length scale of
possible oscillations of ∇u𝜀occurring if
𝜑(x, u, ⋅) is not convex, cf. also Figure 17.4.
As 𝜀is usually very small, it makes sense
to investigate the asymptotics when it
approaches 0. For 𝜀→0, the sequence
{u𝜀}𝜀>0 possesses a subsequence converg-
ing weakly in W 1,p(Ω; ℝd) to some u and
every such a limit u minimizes the relaxed
functional
Φ(u) = ∫Ω
𝜑♯(x, u, ∇u) dx + ∫ΓN
𝜙(x, u) dS.
(17.99)
The possible fast spatial oscillations of the
gradient are smeared out in the limit.
To record some information about such
oscillations in the limit, one should make a
relaxation by continuous extension rather
than only by weak lower semicontinuity.
To ensure the existence of solutions, the
extended space should support a com-
pact topology which makes the extended
functional continuous; such a relaxation is
called a compactiﬁcation. If the extended
space also supports a convex structure (not
necessarily coinciding with the linear struc-
ture of the original space), one can deﬁne
variations, diﬀerentials, and the abstract
Euler–Lagrange equality (17.13); then we
speak about the convex compactiﬁcation
method, cf. [59].
A relatively simple example can be the
relaxation of the micromagnetic prob-
lem (17.87)–(17.90) that, in general, does
not have any solution if 𝜀= 0 due to

17.4 Miscellaneous
579
nonconvexity
of
the
Heisenberg
constraint
|u| = 1.
One
can
embed
the
set
of
admissible
u’s,
namely
{u∈L∞(Ω; ℝ3); u(x)∈S for a.a. x}
with
the sphere S = {|s| = 1} ⊂ℝ3 into a larger
set 𝒴(Ω; S) = {𝜈= (𝜈x)x∈Ω; 𝜈x a proba-
bility47) measure on S and x →𝜈x weakly*
measurable}; the embedding is realized by
the mapping u →𝜈= (𝛿u(x))x∈Ω where 𝛿s
denotes here the Dirac measure supported
at s ∈S. The elements of 𝒴(Ω; S) are
called Young measures [60]48) and this set is
(considered as) a convex weakly* compact
subset of L∞
w∗(Ω; M(S)) where M(S) ≅C(S)∗
denotes the set of Borel measures on S.49)
The problem (17.87)–(17.90) with 𝜀= 0
then allows a continuous extension
minimize Φ(𝜈, h)
=∫
Ω
∫
S
𝜔(s)−h⋅s 𝜈x(ds)dx
subject to div(h−u) = 0, rot h = j,
𝜈∈𝒴(Ω; S),
with u(x) =∫S
s 𝜈x(ds) for x∈Ω.
(17.100)
The functional Φ is a continuous extension
of (u, h) →Φ(u, h) from (17.87), which is
even convex and smooth with respect to
the geometry of L∞
w∗(Ω; M(S)) × L2(Ω; ℝ3).
Existence
of
solutions
to
the
relaxed
47) The adjective “probability” means here a pos-
itive measure with a unit mass but does not
refer to any probabilistic concept.
48) In fact, L.C. Young had already introduced
such measures in 1936 in a slightly diﬀerent
language even before the theory of measure
had been invented. For modern mathemati-
cal theory see, for example, [30, Chapter 8],
[61, Chapter 6–8], [62, Chapter 2], or [59,
Chapter 3].
49) “L∞
w∗” denotes “weakly* measurable” essentially
bounded” mappings, and L∞
w∗(Ω; M(S)) is a dual
space to L1(Ω; C(S)), which allows to intro-
duce the weak* convergence that makes this set
compact.
problem (17.100) is then obtained by
Theorem 17.1 modiﬁed for the constrained
case. Taking into account the convexity
of 𝒴(Ω; S), the necessary and suﬃcient
optimality conditions of the type (17.13)
for (17.100) lead, after a disintegration, to a
pointwise condition
∫S
ℌh(x, s) 𝜈x(ds) = max
s∈S ℌh(x, s),
with ℌh(x, s) = h(x)⋅s−𝜔(s) (17.101)
to hold for a.a. x∈Ω with h satisfying
the linear constraints in (17.100), that is,
div(h−u) = 0, rot h = j, and u =∫S s 𝜈(ds).
The integrand of the type ℌh is some-
times called a Hamiltonian and conditions
like (17.101), the Weierstrass maximum
principle, formulated here for the relaxed
problem and revealed as being a stan-
dard condition of the type (17.13) but
with respect to a nonstandard geometry
imposed by the space L∞
w∗(Ω; M(S)). The
solutions to (17.100) are typically nontrivial
Young measures in the sense that 𝜈x is not
a Dirac measure. From the maximum prin-
ciple (17.101), one can often see that they
are composed from a weighted sum of a
ﬁnite number of Dirac measures supported
only at such s∈S that maximizes ℌh(x, ⋅).
This implies that minimizing sequences for
the original problem (17.87)–(17.90) with
𝜀= 0 ultimately must exhibit ﬁner and
ﬁner spatial oscillations of u’s; this eﬀect is
experimentally observed in ferromagnetic
materials, see Figure 17.4.50) In fact, a small
parameter 𝜀> 0 in the original problem
(17.87)–(17.90) determines the lengthscale
of magnetic domains and also the typical
width of the walls between the domains.
For the Young measure relaxation in
micromagnetism see, for example, [61, 63].
50) Actually, the minimization-energy principle
governs magnetically soft materials where

580
17 Calculus of Variations
20 μm
Figure 17.4
Fast spatial oscillations of the magnetization
vector minimizing Φ from (17.87)–(17.90) with a double-
well potential 𝜔forming a ﬁne microstructure in a ferro-
magnetic tetragonal single-crystal of NiMnGa with only one
axis of easy magnetization normal to the observed surface.
(Courtesy O. Heczko, Institute of Physics, ASCR.)
A relaxation by continuous extension of
the originally discussed problem (17.52) is
much more complicated because the vari-
able exhibiting fast-oscillation tendencies
(i.e., ∇u) is in fact subjected to some dif-
ferential constraint (namely, rot (∇u) = 0)
and because, in contrast to the previous
example, is valued on the whole ℝn×d,
which is not compact. We thus use only a
subset of Young measures, namely,
𝒢p(Ω; ℝn×d) =
{
𝜈∈𝒴(Ω; ℝn×d);
∃u∈W 1,p(Ω; ℝn) ∶
∇u(x) =∫ℝn×d F 𝜈x(dF) ∀a.a.x∈Ω,
∫Ω ∫ℝn×d |F|p 𝜈x(dF)dx < ∞
}
.
The relaxed problem to (17.52) obtained by
continuous extension then has the form
minimize Φ(𝜈, h)=∫∫
Ω ℝn×d
𝜑(u, F) 𝜈x(dF)dx
+ ∫Γ
𝜙(u) dS
subject to ∇u(x) =∫ℝn×dF 𝜈x(dF) ∀a.a.x,
(u, 𝜈)∈W 1,p(Ω; ℝn)×𝒢p(Ω; ℝn×d).
(17.102)
the hysteresis caused by pinning eﬀects is
not dominant.
Proving existence of solutions to (17.102)
is
possible
although
technically
com-
plicated51)
and,
moreover,
𝒢p(Ω; ℝn×d)
is unfortunately not a convex subset of
L∞
w∗(Ω; M(ℝn×d)) if min(n, d) > 1. Only if
n = 1 or d = 1, we can rely on its convexity
and
derive
Karush—Kuhn–Tucker-type
necessary optimality conditions of the type
(17.13) with 𝜆∗being the multiplier to
the constraint ∇u(x) =∫ℝn×d F 𝜈x(dF); the
adjoint operator [R′]∗in (17.16) turns “∇”
to “div.” The resulted system takes the form
∫
ℝn×d
ℌu,𝜆∗(x, F) 𝜈x(dF) = max
F∈ℝn×d ℌu,𝜆∗(x, F),
with ℌu,𝜆∗(x, F) = 𝜆∗(x)⋅F−𝜑(x, u(x), F)
div 𝜆∗= ∫ℝn×d 𝜑′
u(u, F) 𝜈x(dF) on Ω,
𝜆∗⋅⃗n + 𝜙′
u(u) = 0
on Γ,
(17.103)
cf. [59, Chapter 5]. If 𝜑(x, u, ⋅) is convex,
then there exists a standard weak solution
u, that is, 𝜈x = 𝛿∇u(x), and (17.103) simpliﬁes
to
51) Actually, using compactness and the direct
method must be combined with proving and
exploiting that minimizing sequences {uk}k∈ℕ
for (17.52) have {|∇uk|p; k ∈ℕ} uniformly
integrable if the coercivity (17.53a) with p > 1
holds.

17.4 Miscellaneous
581
ℌu,𝜆∗(x, ∇u(x)) = maxF∈ℝn×d ℌu,𝜆∗(x, F),
div 𝜆∗= 𝜑′
u(u, ∇u)
on Ω,
𝜆∗⋅⃗n + 𝜙′
u(u) = 0
on Γ.
(17.104)
One can see that (17.104) combines the
Weierstrass maximum principle with a half
of the Euler–Lagrange equation (17.55).
If 𝜑(x, u, ⋅) is not convex, the oscillatory
character of ∇u for minimizing sequences
can be seen from (17.103) similarly as in
the previous example, leading to nontrivial
Young measures.
We can illustrate it on Example 17.2,
where
(17.103)
leads
to
the
system
d𝜆∗∕dx = 2u
and
du∕dx = ∫ℝF 𝜈x(dF)
on (0, 6𝜋) with the boundary conditions
𝜆∗(0) = 0 = 𝜆∗(6𝜋) and with the Young
measure
{𝜈x}0≤x≤6𝜋∈𝒴4((0, 6𝜋); ℝ)
such
that
𝜈x
is
supported
on
the
ﬁnite
set
{F ∈ℝ;
𝜆∗(x)F −(|F|2−1)2 =
max̃F∈ℝ𝜆∗(x)̃F −(|̃F|2−1)2}.
The
(even
unique) solution of this set of conditions is
u(x) = 0,
𝜆∗(x) = 0,
𝜈x = 1
2𝛿1 + 1
2𝛿−1
(17.105)
for x ∈(0, 6𝜋). This (spatially constant)
Young measure indicates the character of
the oscillations of the gradient in (17.62).
Having in mind the elasticity interpre-
tation from Section 17.3.3.2, this eﬀect is
experimentally observed in some special
materials, see Figure 17.5;52) for modeling
of such microstructure by nonconvex prob-
lems see, for example, [20, 62, 64–66].
Models based on relaxation of continu-
ous extensions such as (17.100) or (17.102),
are sometimes called mesoscopical, in
contrast to the original problems such as
(17.87)–(17.90) or (17.98) with small 𝜀> 0,
52) Actually, Figure 17.5 refers to a multidimen-
sional vectorial case (i.e., d > 1 and n > 1)
where (17.103) is not available.
which are called microscopical, while the
models using original spaces but lower
semicontinuous extensions such as (17.97),
which forget any information about ﬁne
microstructures, are called macroscopical.
17.4.3
𝚪-Convergence
We saw above various situations where the
functional itself depends on a parameter. It
is then worth studying convergence of such
functionals. In the context of minimization,
a prominent role is played by Γ-convergence
introduced by De Giorgi [67], sometimes
also called variational convergence or epi-
graph convergence, cf. also [68–70]. We say
that the functional Φ is the Γ-limit of a
sequence {Φk}k∈ℕif
∀uk →u ∶lim inf
k→∞Φk(uk) ≥Φ(u),
(17.106a)
∀u∈V ∃{uk}k∈ℕwith uk →u ∶
lim sup
k→∞
Φk(uk) ≤Φ(u).
(17.106b)
One interesting property justifying this
mode of convergence is the following:
Theorem 17.15 (Γ-convergence.) If Φk →
Φ in the sense (17.106) and if uk mini-
mizes Φk, then any converging subsequence
of {uk}k∈ℕyields, as its limit, a minimizer
of Φ.53)
Identifying the Γ-limit (if it exists) in con-
crete cases can be very diﬃcult. Few rela-
tively simple examples were, in fact, already
stated above.
53) The proof is simply by a contradiction, assum-
ing that Φ(u) > Φ(v) for u = liml→∞ukl and
some v ∈V and using (17.106) to have a
recovery sequence vk →v so that Φ(v) =
lim infk→∞Φk(vk) ≥lim infk→∞Φk(uk) ≥Φ(u).

582
17 Calculus of Variations
0.1 mm
Figure 17.5
Oscillations of the
deformation gradient minimizing
Φ with a 6-well potential 𝜑(F):
an orthorhombic martensite
microstructure in a single-crystal
of CuAlNi. (Courtesy H. Seiner,
Inst. of Thermomechanics, ASCR.)
A simple example is the numerical
approximation in Section 17.4.1 where we
had the situation
Vk ⊂Vk+1 ⊂V
for k ∈ℕ, and
(17.107a)
Φk(v) =
{
Φ(v)
if v∈Vk,
+∞
otherwise.
(17.107b)
Let us further suppose that Φ is continuous
with respect to the convergence used in
(17.94). Then Φk Γ-converges to Φ.54) Note
that lower semicontinuity of Φ would not
be suﬃcient for it, however.55)
Another
example
of
(17.106)
with
the
weak
topology
we
already
saw
is
given
by
singular
perturbations:
the
functionals
Φ𝜀∶W 1,p(Ω; ℝd) →
ℝ∪{+∞}
deﬁned
by
(17.98)
for
u ∈W 1,p(Ω; ℝd) ∩W 2,2(Ω; ℝd)
and
by
+∞
for
u ∈W 1,p(Ω; ℝd) ⧵W 2,2(Ω; ℝd)
Γ-converge, for 𝜀→0, to Φ from (17.52)
if
𝜑(x, u, ⋅)
is
quasiconvex,
otherwise
one should use Φ from (17.99). Alter-
natively, if Φ𝜀is extended to the Young
54) Indeed, (17.106a) holds because Φk ≥Φk+1 ≥Φ
due to (17.107a). For any ̂v ∈V, there is ̂vk ∈
Vk such that ̂vk →̂v. Then limk→∞Φk(̂vk) =
limk→∞Φ(̂vk) = Φ(̂v) and also limk→∞̂vk = v in
V so that {̂vk}k∈ℕis a recovery sequence for
(17.106b).
55) A simple counterexample is Φ = +∞every-
where except some v ∈V ⧵∪k∈ℕVk; then Φk ≡
∞obviously does not Γ-converge to Φ.
measures by +∞if the Young measure
is not of the form {𝛿∇u(x)}x∈Ω for some
u ∈W 1,p(Ω; ℝd) ∩W 2,2(Ω; ℝd),
one
Γ-
converges as 𝜀→0 to the functional
from (17.102).56) Similarly, (17.87)–(17.90)
Γ-converges to (17.100) if 𝜖→0.
Other prominent applications of Γ-
convergence are dimensional reduction
from three-dimensional problems to one-
dimensional (springs, rods, beams) or
two-dimensional (membranes, thin ﬁlms,
shells, plates), or homogenization of com-
posite materials with periodic structure, cf.
e.g. [48, 71].
Glossary
A lot of notions, deﬁnitions, and assertions
are presented above. The following list tries
to sort them according subjects or disci-
plines, giving the link to particular pages
where the particular item is highlighted.
Topological notions:
Γ-convergence, p.581
continuous (weakly), p.552 (p.553)
56) Again, (17.106a) is simply due to Φ𝜀≥Φ and
Φ is lower semicontinuous. The construction of
particular recovery sequences for (17.106b) is
more involved, smoothing the construction of a
recovery sequence for 𝜑♯or for the minimizing
gradient Young measure as in [61, 2].

17.4 Glossary
583
compact mapping, p.563
compact set, p.553
dense, p.557
hemicontinuous mapping, p.552
lower semicontinuous, p.553
envelope, p.577
variational convergence, p.581
Linear spaces, spaces of functions:
adjoint operator, p.555
Banach space, p.551
ordered, p.555
reﬂexive, p.553
Bochner space Lp(I; V), p.557
boundary critical exponent p♯, p.563
dual space, p.552
Gelfand triple V ⊂H ⊂V ∗, p.557
Hilbert space, p.552
Lebesgue space Lp, p.562
pre-dual, p.553
smooth functions Ck(Ω), p.563
Sobolev critical exponent p∗, p.563
Sobolev space W k,p, p.562
Young measures, p.579
Convex analysis:
cone, p.555
convex/concave, p.552
convex mapping, p.556
Fenchel inequality, p.557
indicator function 𝛿K, p.555
Legendre conjugate, p.557
Legendre transformation, p.557
Legendre–Fenchel transformation, p.559
linear, p.552
monotone, p.553
normal cone NK(u), p.555
polyconvexity, p.566
rank-one convexity, p.565
strictly convex, p.553
subdiﬀerential 𝜕, p.554
tangent cone TK(u), p.555
Smooth analysis:
continuously diﬀerentiable, p.552
directionally diﬀerentiable, p.552
Fréchet subdiﬀerential 𝜕F, p.554
Gâteaux diﬀerential, pp.552, 564
smooth, p.552
Optimization theory:
adjoint system, p.585
constraint qualiﬁcation
Mangasarian-Fromovitz, p.555
Slater, p.556
complementarity condition, p.555
critical point, p.552
dual problem, p.556
Euler–Lagrange equation, pp.552, 564
Karush–Kuhn–Tucker condition, p.555
Lagrangean ℒ(u, 𝜆∗), p.555
optimal control, p.585
relaxed, p.585
suﬃcient 2nd-order condition, p.556
transversality condition, p.555
Variational principles and problems:
Brezis–Ekeland–Nayroles, pp.557,575
complementarity problem, p.555
Ekeland principle, p.584
Hamilton principle, pp.559,576
Lavrentiev phenomenon, pp.567,572
least dissipation principle, p.558
minimum-energy principle, p.552
maximum dissipation, p.560
nonexistence, pp.567,571
potential, p.552
coercive, p.553
double-well, p.567
of dissipative forces, p.558
Palais–Smale property, p.554
Plateau minimal-surface problem, p.567
Pontryagin maximum principle, p.585
relaxation, p.577
by convex compactiﬁcation, p.578
singular perturbations, pp.578, 582
Stefanelli principle, p.558
symmetry condition, pp.552, 565, 570
Onsager, p.558
variational inequality, p.554
Weierstrass maximum principle, p.579
Diﬀerential equations and inequalities:
abstract parabolic equation, p.556
boundary conditions, p.564
boundary-value problem, pp.564, 569
Carathéodory mapping, p.563
Cauchy problem, p.556
doubly nonlinear, p.558
classical solution, p.564
doubly nonlinear inclusion, p.559
formulation
classical, pp.564,569
De Giorgi, p.559
energetic, p.560
mixed, p.575
weak, p.564,569
generalized gradient ﬂow, p.558
in metric spaces, p.559
Legendre–Hadamard condition, p.566
Nemytski˘i mapping, p.563

584
17 Calculus of Variations
nonsymmetric equations, p.584
nonvariational methods, p.585
quasiconvex, p.565
envelope, p.578
rate-independent, p.560
Stokes system, p.575
surface divergence divS, p.569
weak solution, p.565
Numerical techniques:
ﬁnite-element method, p.577
fractional-step method, p.561
Galerkin method, p.576
implicit Euler formula, p.560
Ritz method, p.576
Rothe method, p.560
semi-implicit scheme, p.561
sequential splitting, p.561
Uzawa algorithm, p.561
Mechanics of continua:
Ciarlet–Neˇcas condition, p.572
complex continuum, p.573
hyperelastic material, p.572
Kirchhoﬀtransformation, p.571
Korn inequality, p.573
Lamé system, p.573
microstructure, pp.578,580,582
minimal surface, p.567
nonsimple continuum, p.573
Ogden material, p.572
small-strain tensor e(u), p.573
Signorini contact, p.574
St.Venant–Kirchhoﬀmaterial, p.573
Some important theorems:
Bolzano–Weierstrass, p.553
compact embedding W 1,p ⊂Lp∗−𝜖, p.563
compact trace operator, p.563
direct method, p.553, 553
for elliptic problems, p.566
for parabolic problems, p.557
Γ-convergence, p.581
mountain pass, p.554
Nemytski˘i-mapping continuity, p.563
von Neumann’s saddle-point, p.554
1st-order necessary condition, p.555
2nd-order suﬃcient condition, p.556
Further Reading
The convex/smooth setting with one objec-
tive functional on which we primarily
focused in Section 17.2 can be extensively
generalized to nonconvex and nondiﬀer-
entiable cases and/or to multi-objective
situations, including dualization schemes,
optimality conditions, sensitivity analysis,
generalized equilibria, and many others,
cf. e.g. [9, 12, 37, 72–74]. Many proof
techniques are based on the remarkable
Ekeland variational principle saying that,
for a Gâteaux diﬀerentiable functional Φ
bounded from below on a Banach space V,
holds that
∀u ∈V, 𝜀> 0 ∶
Φ(u) ≤inf Φ + 𝜀
⇒
∃v ∈V ∶
Φ(v) ≤Φ(u),
‖v−u‖ ≤
√
𝜀,
‖‖Φ′(v)‖‖ ≤
√
𝜀,
See, for example, [11, 31, 37], in particu-
lar, also for a general formulation in metric
spaces.
In concrete situations, solutions of vari-
ational problems often enjoy additional
properties (typically, despite the coun-
terexamples as [57, 58], some smoothness);
there is an extensive literature in this direc-
tion of regularity of solutions, for example,
[32, 50, 75].
There has been intensive eﬀort leading
to eﬃcient and widely applicable methods
to avoid the symmetry conditions (17.5),
cf. also (17.57), based on the concept of
monotonicity.
Nonsymmetric
nonlinear
monotone-type operators (possibly gener-
alized, for example, to pseudo-monotone
operators or of the types (M) or (S), etc.)
have been introduced on an abstract level
in the work of Brézis [7], Minty [76], and
others. Many monographs are available
on this topic, also applied to concrete
nonsymmetric
quasilinear
equations
or
inequalities, cf. e.g. [18, 50, 77, 78].
Even for situations conforming with
the symmetry conditions of the type
(17.57), Example 17.5 showed that some-
times variational methods even for linear

Further Reading
585
boundary-value problem such as
−div∇u = f on Ω,
∇u⋅⃗n + u = g on Γ,
are not compatible with natural physi-
cal demands that the right-hand sides f
and g have an L1-structure. This is why
also nonvariational methods have been
extensively developed. One method to
handle general right-hand sides is Stam-
pacchia’s [79] transposition method, which
has been analyzed for linear problems by
Lions and Magenes [80]. Another general
method is based on metric properties and
contraction based on accretivity (instead
of compactness and monotonicity) and,
when applied to evolution problems, is
connected with the theory of nonexpansive
semigroups; from a very wide literature
cf. e.g. the monographs by Showalter [78,
Chapter 4], Vainberg [81, Chapter VII],
or Zeidler [9, Chapter 57], or also [18,
Chapter 3 and 9]. An estimation technique
ﬁtted with L1-structure and applicable to
thermal problems possibly coupled with
mechanical or other physical systems, has
been developed in [82], cf. also e.g. [18].
In
fact,
for
d = 1
and
Ω = [0, T],
Section 17.3.2.1 dealt in particular with
a very special optimal control problem
of the Bolza type: minimize the objective
∫T
0 𝜑(t, u(t), v(t)) dt + 𝜙(T, u(T))
for
the
initial-value problem for a simple (sys-
tem of) ordinary diﬀerential equation(s)
du∕dt = v, u(0) = u0, with the control v
being possibly subjected to a constraint
v(t) ∈S, with t ∈[0, T] playing the role
of time. One can also think about gen-
eralization
to
(systems
of)
nonlinear
ordinary
diﬀerential
equations
of
the
type du∕dt = f (t, u, v). If 𝜑(t, u, ⋅) is con-
vex and f (t, u, ⋅) is aﬃne, one obtains
existence of optimal control v and the
corresponding
response
by
the
direct
method as we did in Section 17.3.2.1. If
fact, convexity of the so-called orientor
ﬁeld Q(t, u) ∶= {(q0, q1); ∃s∈S(t) ∶q0 ≥
𝜑(t, u, s),
q1 = f (t, u, s)} is decisive for
existence of optimal control. In the general
case, the existence is not guaranteed and
one can make a relaxation as we did in
(17.100) obtaining the relaxed optimal
control problem
minimize =∫
T
0 ∫S
𝜑(t, u(t), s) 𝜈t(ds)dt
subject to
du
dt =∫S
f (t, u(t), s) 𝜈t(ds)
𝜈∈𝒴([0, T]; S).
(17.108)
The optimality conditions of the type
(17.16) results in a modiﬁcation of the
Weierstrass maximum principle (17.103),
namely,
∫S
ℌu,𝜆∗(t, s) 𝜈t(ds) = max
̃s∈S ℌu,𝜆∗(t,̃s) with
ℌu,𝜆∗(t, s)=𝜆∗(t)⋅f (t, u(t), s)−𝜑(t, u(t), s),
d𝜆∗
dx +∫S
f ′
u(t, u(t), s)⊤𝜆∗(t) 𝜈t(ds)
=∫S
𝜑′
u(t, u(t), s)⊤𝜈t(ds) on [0, T],
𝜆∗(T) = 𝜙′
u(T, u(T)).
(17.109)
The
linear
terminal-value
problem
in
(17.109) for 𝜆∗is called the adjoint system,
arising from the adjoint operator in (17.16).
Of course, if (by chance) the optimal con-
trol v of the original problem exists, then
the ﬁrst condition in (17.109) reads as
ℌu,𝜆∗(t, v(t)) = max̃v∈S ℌu,𝜆∗(t,̃v).
Essen-
tially, this has been formulated in [83, 84]
and later become known as the Pontryagin
maximum principle, here in terms of the
so-called relaxed controls. We can see that
it is a generalization of the Weierstrass
principle and can be derived as a standard
Karush–Kuhn–Tucker condition but with
respect to the convex geometry induced

586
17 Calculus of Variations
from the space of relaxed controls.57) One
can also consider optimal control of par-
tial diﬀerential equations instead of the
ordinary ones, cf. also [59]. There is a huge
literature about optimal control theory in
all usual aspects of the calculus of vari-
ations as brieﬂy presented above, cf. e.g.
[73, 85, 86].
Acknowledgments.
The
author
is
very
thankful to Jan Mal´y for fruitful discussions
(particularly with regard to Example 17.5),
Alexander Mielke, and Jiˇrí V. Outrata,
and to Oleg Heczko and Hanuš Seiner
for providing the experimental ﬁgures
17.4 and 17.5 . The institutional support
RVO:61388998 ( ˇCR) and the support from
the grant 201/10/0357 (GA ˇCR) are also
warmly acknowledged.
References
1. Buttazzo, G., Giaquinta, M., and
Hildebrandt, S. (eds) (1998)
One-Dimensional Variational Problems,
Clarendon, Oxford.
2. Drake, G.W.F. (2005) Variational methods, in
Mathematical Tools for Physicists (ed. G.L.
Trigg), Wiley-VCH Verlag GmbH,
Weinheim, pp. 619–656.
3. Jost, J. and Li-Jost, X. (1998) Calculus of
Variations, Cambridge University Press,
Cambridge.
4. Hilbert, D. (1901) Mathematische probleme.
Archiv d. Math. u. Physik, 1, 44–63,
213–237; (English Translation: (1902) Bull.
Am. Math. Soc., 8, 437–479).
5. Banach, S. (1932) Théorie des Opérations
Linéaires, M.Garasi´nski, Warszawa.
6. Tonelli, L. (1915) Sur un méthode directe du
calcul des variations. Rend. Circ. Mat.
Palermo, 39, 233–264.
7. Brezis, H. (1968) Équations et inéquations
non-linéaires dans les espaces vectoriel en
dualité. Ann. Inst. Fourier, 18, 115–176.
57) The original and rather technical method
was based on the so-called needle variations,
however.
8. Ambrosetti, A. and Rabinowitz, P.H. (1973)
Dual variational methods in critical point
theory and applications. J. Funct. Anal., 14,
349–380.
9. Zeidler, E. (1985) Nonlinear Functional
Analysis and Its Applications III: Variational
Methods and Optimization, Springer,
New York.
10. von Neumann, J. (1928) Zur Theorie der
Gesellschaftsspiele. Math. Ann., 100,
295–320.
11. Borwein, J.M. and Zhu, Q.J. (eds) (2005)
Techniques of Variational Analysis, Springer,
Berlin.
12. Rockafellar, R.T. and Wets, R.J.-B. (1998)
Variational Analysis, Springer, Berlin.
13. Karush, W. (1939) Minima of functions of
several variables with inequalities as side
conditions. PhD thesis, Department of
Mathematics - University of Chicago,
Chicago, IL.
14. Kuhn, H. and Tucker, A. (1951) Nonlinear
programming, 2nd Berkeley Symposium on
Mathematical Statistics and Probability,
University of California Press, Berkeley,
pp. 481–492.
15. Brezis, H. and Ekeland, I. (1976) Un prinicpe
varationnel associeé à certaines équations
paraboliques. C. R. Acad. Sci. Paris, 282,
971–974 and 1197–1198.
16. Nayroles, B. (1976) Deux théorèmes de
minimum pour certains systèmes dissipatifs.
C. R. Acad. Sci. Paris Sér. A-B, 282,
A1035–A1038.
17. Roubíˇcek, T. (2000) Direct method for
parabolic problems. Adv. Math. Sci. Appl.,
10, 57–65.
18. Roubíˇcek, T. (2013)Nonlinear Partial
Diﬀerential Equations with Applications,
2nd edn, Birkhäuser, Basel.
19. Onsager, L. (1931) Reciprocal relations in
irreversible processes I. Phys. Rev., 37,
405–426; Part II, 38, 2265–2279.
20. Šilhav´y, M. (1997) The Mechanics and
Thermodynamics of Continuous Media,
Springer, Berlin.
21. Stefanelli, U. (2008) The Brezis-Ekeland
principle for doubly nonlinear equations.
SIAM J. Control Optim., 47, 1615–1642.
22. Onsager, L. and Machlup, S. (1953)
Fluctuations and irreversible processes. Phys.
Rev., 91, 1505–1512.
23. Ambrosio, L., Gigli, N., and Savaré, G. (2008)
Gradient Flows, 2nd edn, Birkhäuser, Basel.

References
587
24. Bedford, A. (ed.) (1985) Hamilton’s Principle
in Continuum Mechanics, Pitman, Boston,
MA.
25. Visintin, A. (1996) Models of Phase
Transitions, Birkhäuser, Boston, MA.
26. Mielke, A. and Theil, F. (2004) On
rate-independent hysteresis models. Nonlin.
Diﬀ. Equ. Appl., 11, 151–189.
27. Mielke, A. and Roubíˇcek, T. (2014)
Rate-Independent Systems - Theory and
Application, Springer, New York, to appear.
28. Rothe, E. (1930) Zweidimensionale
parabolische Randwertaufgaben als
Grenzfall eindimensionaler
Randwertaufgaben. Math. Ann., 102,
650–670.
29. Dacorogna, B. (1989) Direct Methods in the
Calculus of Variations, Springer, Berlin.
30. Fonseca, I. and Leoni, G. (2007) Modern
Methods in the Calculus of Variations: Lp
Spaces, Springer, New York.
31. Giusti, E. (2003) Direct Methods in Calculus
of Variations, World Scientiﬁc, Singapore.
32. Neˇcas, J. (1967) Les Méthodes Directes en la
Théorie des Equations Elliptiques,
Academia & Masson, Praha & Paris;
(English Translation: Springer, Berlin, 2012).
33. Morrey, C.B. Jr. (1966) Multiple Integrals in
the Calculus of Variations, Springer, Berlin.
34. Šverák, V. (1992) Rank-one convexity does
not imply quasiconvexity. Proc. R. Soc.
Edinburgh Sect. A, 120, 185–189.
35. Müller, S. (1989) A surprising higher
integrability property of mappings with
positive determinants. Bull. Am. Math. Soc.,
21, 245–248.
36. Ball, J.M. (1977) Convexity conditions and
existence theorems in nonlinear elasticity.
Arch. Ration. Mech. Anal., 63 (4), 337–403.
37. Ekeland, I. and Temam, R. (1976) Convex
Analysis and Variational Problems,
North-Holland.
38. Lavrentiev, A. (1926) Sur quelques
problémes du calcul des variations. Ann.
Mat. Pura Appl., 41, 107–124.
39. Carstensen, C. and Ortner, C. (2010)
Analysis of a class of penalty methods for
computing singular minimizers. Comput.
Meth. Appl. Math., 10, 137–163.
40. Ball, J.M. and Mizel, V.J. (1985)
One-dimensional variational problems
whose minimizers do not satisfy the
Euler-Lagrange equation. Arch. Ration.
Mech. Anal., 90, 325–388.
41. Baiocchi, C. and Capelo, A. (1984)
Variational and Quasivariational
Inequalities, John Wiley & Sons, Ltd,
Chichester.
42. Duvaut, G. and Lions, J.-L. (1976)
Inequalities in Mechanics and Physics,
Springer, Berlin.
43. Hlaváˇcek, I., Haslinger, J., Neˇcas, J., and
Lov´ıšek, J. (1988) Solution of Variational
Inequalities in Mechanics, Springer, New
York.
44. Kinderlehrer, D. and Stampacchia, G. (1980)
An Introduction to Variational Inequalities
and their Applications, Academic Press,
New York.
45. Ciarlet, P.G. and Neˇcas, J. (1987) Injectivity
and self-contact in nonlinear elasticity. Arch.
Ration. Mech. Anal., 97, 171–188.
46. Ball, J.M. (2002) Some open problems in
elasticity, in Geometry, Mechanics, and
Dynamics (ed. P. Newton, P. Holmes, A.
Weinstein), Springer, New York, pp. 3–59.
47. Foss, M., Hrusa, W.J., and Mizel, V.J. (2003)
The Lavrentiev gap phenomenon in
nonlinear elasticity. Arch. Ration. Mech.
Anal., 167, 337–365.
48. Ciarlet, P.G. (1988, 1997, 2000)
Mathematical Elasticity, Vol.I:
Three-Dimensional Elasticity, Vol.II: Theory
of Plates, Vol.III: Theory of Shells,
North-Holland, Amsterdam.
49. Eringen, A.C. (2002) Nonlocal Continuum
Field Theories, Springer, New York.
50. Evans, L.C. (1998) Partial Diﬀerential
Equations, AMS, Providence, RI.
51. Struwe, M. (1990) Variational Methods:
Applications to Nonlinear Partial
Diﬀerential Equations and Hamiltonian
Systems, Springer, Berlin.
52. Hlaváˇcek, I. (1969) Variational principle for
parabolic equations. Apl. Mat., 14, 278–297.
53. Ghoussoub, N. (2009) Self-Dual Partial
Diﬀerential Systems and Their Variational
Principles, Springer, New York.
54. Ritz, W. (1908) Über eine neue Methode zur
Lösung gewisser Variationsprobleme der
mathematischen Physik. J. Reine u. Angew.
Math., 135, 1–61.
55. Galerkin, B.G. (1915) Series development for
some cases of equilibrium of plates and
beams (In Russian). Vestnik Inzhinierov
Teknik, 19, 897–908.
56. Christara, C.C. and Jackson, K.R. (2005)
Numerical methods, in Mathematical Tools

588
17 Calculus of Variations
for Physicists (ed. G.L. Trigg), John Wiley &
Sons, Inc., Weinheim, pp. 281–383.
57. Neˇcas, J. (1977) Example of an irregular
solution to a nonlinear elliptic system with
analytic coeﬃcients and conditions of
regularity, in Theory of Nonlinear Operators
Proceedings of Summer School (Berlin 1975)
(eds W. Muller, Berlin, Akademie-Verlag, pp.
197–206.
58. Šverák, V. and Yan, X. (2000) A singular
minimizer of a smooth strongly convex
functional in three dimensions. Calc. Var.,
10, 213–221.
59. Roubíˇcek, T. (1997) Relaxation in
Optimization Theory and Variational
Calculus, W. de Gruyter, Berlin.
60. Young, L.C. (1969) Lectures on the Calculus
of Variations and Optimal Control Theory,
W.B. Saunders, Philadelphia, PA.
61. Pedregal, P. (1997) Parametrized Measures
and Variational Principles, Birkhäuser, Basel.
62. Pedregal, P. (2000) Variational Methods in
Nonlinear Elasticity, SIAM, Philadelphia,
PA.
63. Kruž´ık, M. and Prohl, A. (2006) Recent
developments in the modeling, analysis, and
numerics of ferromagnetism. SIAM Rev., 48,
439–483.
64. Ball, J.M. and James, R.D. (1987) Fine phase
mixtures as minimizers of energy. Arch.
Ration. Mech. Anal., 100, 13–52.
65. Bhattacharya, K. (ed.) (2003) Microstructure
of MartenSite. Why it Forms and How it
Gives Rise to the Shape-Memory Eﬀect,
Oxford University Press, New York.
66. Müller, S. (1999) Variational models for
microstructure and phase transitions,
Calculus of Variations and Geometric
Evolution Problems, Springer, Berlin, pp.
85–210.
67. De Giorgi, E. (1977.) Γ-convergenza e
G-convergenza. Boll. Unione Mat. Ital., V.
Ser., A 14, 213–220.
68. Attouch, H. (1984) Variational Convergence
of Functions and Operators, Pitman.
69. Braides, A. (ed.) (2002) Γ-Convergence for
Beginners, Oxford University Press, Oxford.
70. Dal Maso, G. (1993) An Introduction to
Γ-Convergence, Birkhäuser, Bostonm, MA.
71. Friesecke, G., James, R.D., and Müller, S.
(2006) A hierarchy of plate models derived
from nonlinear elasticity by
Gamma-convergence. Arch. Ration. Mech.
Anal., 180, 183–236.
72. Bonnans, J.F. and Shapiro, A. (eds) (2000)
Perturbation Analysis of Optimization
Problems, Springer-Verlag, New York.
73. Clarke, F.H. (1983) Optimization and
Nonsmooth Analysis, Wiley.
74. Mordukhovich, B.S. (2006) Variational
Analysis and Generalized Diﬀerentiation I,
II, Springer, Berlin.
75. Mal´y, J. and Ziemer, W.P. (1997) Fine
Regularity of Solutions of Elliptic Partial
Diﬀerential Equations, American
Mathematical Society, Providence, RI.
76. Minty, G. (1963) On a monotonicity method
for the solution of non-linear equations in
Banach spaces. Proc. Natl. Acad. Sci. U.S.A.,
50, 1038–1041.
77. Lions, J.L. (1969) Quelques Méthodes de
Résolution des Problémes aux Limites non
linéaires, Dunod, Paris.
78. Showalter, R.E. (1997) Monotone Operators
in Banach Space and Nonlinear Partial
Diﬀerential Equations, AMS.
79. Stampacchia, G. (1965) Le problème de
Dirichlet pour les équations elliptiques du
second ordre à coeﬃcients discontinus. Ann.
Inst. Fourier, 15, 189–258.
80. Lions, J.L. and Magenes, E. (1968) Problèmes
aux Limites non homoegènes et Applications,
Dunod, Paris.
81. Vainberg, M.M. (1973) Variational Methods
and Method of Monotone Operators in the
Theory of Nonlinear Equations, John Wiley
& Sons, Inc., New York.
82. Boccardo, L. and Gallouet, T. (1989)
Non-linear elliptic and parabolic equations
involving measure data. J. Funct. Anal., 87,
149–169.
83. Boltyanski˘ı, V.G., Gamkrelidze, R.V., and
Pontryagin, L.S. (1956) On the theory of
optimal processes (In Russian). Dokl. Akad.
Nauk USSR, 110, 7–10.
84. Hestenes, M.R. (1950) A general problem in
the calculus of variations with applications
to the paths of least time. Technical Report
100, RAND Corp., Santa Monica, CA.
85. Ioﬀe, A.D. and Tikhomirov, V.M. (1979)
Theory of Extremal Problems,
North-Holland, Amsterdam.
86. Tröltzsch, F. (2010) Optimal Control of
Partial Diﬀerential Equations, American
Mathematical Society, Providence, RI.

589
Index
a
abel transform
514
abelian functions
274, 275, 278
abelian groups
358
absolute error
477
abstract Fourier series transformation
506
abstract parabolic equation
556, 583
acceptance frequency
48
accumulation point
456, 464
accuracy
– computer algebra
291
– dynamical systems
391
– perturbation solutions
415
Adams methods
493, 495
Adams–Bashforth methods
493
Adams–Moulton methods
493, 494
addition formulas
173, 258, 276
addition laws
254, 258, 273–275, 278, 285
addition theorem
206
additive noise
74, 85, 92, 94, 96
adjacency matrix of a simple graph
154
adjacency operator
113
adjoint operator
457, 458, 555, 585
adjoint system
585
advanced algorithms
294–301
aﬃne reﬂection groups
244
Airy functions
268, 269, 283, 495
– turning point problem
434
Airy’s equation
495
algebra of tensors
337, 338
algebraic entropy
286
algebraic equations
– iteration methods
476–480
– transformations
504, 505
alpha complex
235
Ambrosetti–Rabinowitz mountain pass theorem
554
analytic function
10, 453, 510
Andronov–Hopf bifurcation
393
angle-preserving map
503
angular momentum
171, 177, 180, 189–191,
194–197
angular standard form
424
annihilation operators
133, 179, 180, 198, 532
anticommutation
179
approximation
– numerical
80, 93, 95, 476–480, 576
– of continuous data
476, 487ﬀ
area-preserving map
503
Arnol’d diﬀusion
425
Arnol’d unfolding
444
Arrhenius’ law
103
Askey–Wilson diﬀerence operator
284
associated Legendre functions
160, 202, 255,
256, 260, 264
asymptotic approximation
415, 416, 431, 444
asymptotic matching
438
asymptotic series
416, 444, 445
asymptotic stability
387, 389
asymptotic validity
415, 416
atlas
321–324, 327, 363
– adapted
363
atomic shell structure
195
attempt frequency
48
attractor
– chaotic
405
– strange
228, 386, 429
attrition problem
45
autocovariance
75
automorphism
– galois theory
271, 272
– Lie-group
358
averaging
422–424
– self-averaging
45
Mathematical Tools for Physicists, Second Edition. Edited by Michael Grinfeld.
© 2015 Wiley-VCH Verlag GmbH & Co. KGaA. Published 2015 by Wiley-VCH Verlag GmbH & Co. KGaA.

590
Index
b
Bäcklund maps
282, 286
backward Fokker–Planck equation
102, 103
backward kolmogorov equation
24, 25
backward/forward substitution
481
Bak–Sneppen model
17
balance equation
12, 356
balance laws
356–358
– diﬀerential expression of the general balance
law
358
– nonlinear heat-transfer problem
571
balance principle
50, 68
ballot theorem
20
Banach space
449–451, 551, 562
– ordered
555
– reﬂexive
553
band-limited function
517
banded matrices
481
barcode
233
basin of attraction
387
Bernoulli distribution
35
Bessel equation
206, 255, 262, 270, 271
Bessel functions
202, 240, 256–258, 268, 270
– Hankel transformation
523
Betti number
221–223, 228, 232, 236
betweenness centrality
143, 149, 154
Bezout’s identity
296, 300, 301
Bianchi identity
372
biased sampling
45
bifurcation point
390, 392, 394, 395, 404–407
bifurcations
411, 418, 445
– hopf
392–396, 399, 404, 428
– imperfect
414
– local
391, 392, 404
– Pitchfork
392, 394, 396–398, 404
– saddle-node
392–398, 406–408
binomial distribution
35
binomial theorem
5, 15, 285
Biot system
545
Biot–Savart formula
535
Birman–Kuroda theorem
472
Birman–Schwinger bound
469
birth-and-death process
24
Black–Scholes PDE
102
Bochner space
557
Boltzmann constant
89, 121
Bolzano–Weierstraß theorem
553
bond percolation
31, 32
boole’s inequality
21
Borel set
465
Borel–Cantelli lemma
8, 92
Boson annihilation operator
179
boundary conditions
52–56, 103, 411
– weierstraß elliptic functions
276
boundary critical exponent
563
boundary layer
412, 417, 426, 429–434, 436–
439,
445
boundary operator
220–225, 228, 231, 232
boundary point
224
boundary value problem (BVP)
400, 401,
564–566, 569–571
– classical formulation
564, 569
bounded operators
450, 452, 454–457, 460, 464,
547
Bragg position
67
branching process
7, 8, 33
Brezis–Ekeland–Nayroles principle
557, 575
bridge
122, 129, 154, 212
Brillouin zone
67
– ﬁrst
170
Brownian motion
– approximation
80
– geometric
86, 88, 89
– Monte-Carlo methods
50
– stochastic diﬀerential equations
75–80, 100
– stochastic process
19
Brute-force bifurcation diagrams
405
bundle, associated
362, 364–366, 370, 372, 374
burgers’ circuit
332
Butcher array
494, 495
c
canard
433
canonical ensemble
47, 52, 64
canonical transformations
426
caratheéodory mapping
563
Cartan’s structural equation
372
Cartan–killing metric equation
178
Cauchy equation
272
Cauchy problem
491, 492, 495, 496, 500,
556–559
– doubly nonlinear
558
Cauchy sequence
450, 552
Cauchy’s formula
4, 356, 357, 492
Cauchy’s postulates
355, 357
Cayley multiplication table
167
ˇCech complex
234
cellular homology
224
center manifolds
390, 393, 395, 397–399
central limit theorem
9, 10, 75
centrality measure
143, 145, 146, 149, 154
chain groups
219, 221, 222, 224, 225, 228, 231
Chapman–Kolmogorov equation
23, 99
Chapman–Kolmogorov relation
17, 22, 23
character table
188, 190, 191
character-class duality
187
characteristic function
10

Index
591
chart
321, 322
– adapted
363
Chebyshev polynomials
402
Chebyshev–Gauss–lobatto nodes
488
chernoﬀbound
21
Christoﬀel symbols
184, 185, 374, 377, 380
Ciarlet–Neˇcas condition
572, 584
circular functions
273, 277, 278
Ck-diﬀerentiable structure
324
class, group theory
168
classical formulation of variational problem
564,
569
classical Fourier series transformation
509
classical mechanics
133, 160, 183, 346, 363
classical solution
564, 577
clique
29, 30, 114, 234
clique complex
234
closed walk (CW)
114, 146
closed-graph theorem
451, 452, 458
closeness centrality
143, 154
cluster algorithms
49
clustering coeﬃcient
115, 135, 139, 140, 142, 154
cochain
224–226
codimension one bifurcations
392
coeﬃcient group
220, 221, 224, 225
coeﬃcients, diﬀerential
272, 507, 522
cohomology
212, 224–226
collocation
400–402, 406
colored noise
77–80
communicability
138, 147, 148, 154
commutative groups
188
commutativity of self-adjoint operators
472
compact embedding theorem
563
compact mapping
563
compact operators
456, 464, 472
compact set
452, 519
compact trace operator theorem
563
compactiﬁcation
578
– convex
578, 579
complementarity problem
555, 574
completeness relations
186–188
complex coordinate transformation
509
complex networks
111, 115, 137, 138, 140, 143,
148
complex systems
63, 67, 111, 137, 138, 150
complex variables
255, 279, 428, 439
complexity
39, 137, 138, 293, 568
composite solution
430–432, 445
computational topology
211, 212, 230–236
computer simulations
26, 40, 216
condensed matter
40, 62–67, 111, 115–120, 217
conditional expectation
36, 37
conditional probability
11, 26, 31, 35, 37, 46
conﬂuent hypergeometric functions
260
conformal symmetry
194
conjugate
– Hermitean
61
– legendre
557
conjugate gradient (CG) method
479, 480, 484,
485
Conley index
228
connected topological space
320
connections
367–380
– principal bundle
369, 370
consensus dynamics
151
conservation laws
51, 356, 388
conservative forces
357
conservative oscillator
421
conservative systems
467
consistency
49, 325, 365, 492, 495
– of multistep methods
493
constrained minimization problems
555
constraint qualiﬁcation
– mangasarian–fromovitz
555
– slater
556
continuity
320
– of probability measure
8, 16, 32
continuous groups
165, 171, 250
continuous maps
214, 224, 321, 322, 324, 462
continuous symmetry
240, 250–261
continuously diﬀerentiable
89, 504, 552, 554, 577
contour integrals
255
contravariant tensor algebra
338
control theory
586
Γ-convergence
581, 582, 584
convergence
– around ﬁxed points
478
– higher order
477
– linear
406, 477
– numerical methods
92, 98
– quadratic
477
– strong
92, 94, 96, 98, 462
– weak
92, 95, 96, 453, 553
– weak∗
553,
579
convergence analysis
477, 478
convex mapping
556, 583
convolution
6, 7, 512–514
– integral
513, 514
– theorem
6, 27, 514
coordinate perturbations
414, 428
corrected trapezoidal formula
489
correlation functions
51, 59, 79
coset
168, 209
cotangent bundle
327, 328, 341, 342, 345, 346,
361, 364, 380
cotangent space
326, 327, 349, 380
coulomb potential
192, 193, 195, 196, 472
countable topology
320

592
Index
counting function
18
coupled systems
266, 544, 545, 547
covariance
75–80, 88, 105, 184
covariant derivative
372, 374–376, 380
covariant tensor algebra
338
covector
326
covering space
216, 217, 230
Coxeter group
243, 244, 246
Cramer’s rule
480
creation operators
179, 198, 532
critical point
552
critical slowing down
49, 55, 68
criticality
43, 394, 397
cross product
173
crystal ﬁeld theory
190
cubical homology
224
cumulants
53, 54
curl
343
curvature form
372, 375, 376
curvature tensor
376, 380
cyclic groups
168, 188, 216, 221
cyclomatic number
124, 216
cylinders
215, 216, 256
cylindrical coordinates
254
d
d-tuples
19, 41
damping
131, 413, 414, 561
Darboux transformation
267
De Moivre–Laplace central limit theorem
9
De Rham cohomology
225, 226
De Rham’s theorem
226
deck transformation group
217
deﬁciency indices
461, 462, 472, 473
deﬁciency subspace of t
472
deﬁnite integrals, numerical approximation
489
degree distribution
138–142, 154, 155
Delaunay complex
235
delta function
78, 186
dense model
293, 295
densely deﬁned operators
457–460, 521, 537,
538
density matrices
457, 472
density of an extensive property
355
deposition, diﬀusion, and adsorption model
27–29
derivative
– covariant
374, 375, 380
– exterior covariant
372, 376
descent direction method
479, 484
detailed balance condition
48
detailed balance principle
50, 68
determinant, Jacobian
324, 343, 353, 505
diﬀeomorphism
324, 359
– between ﬁbers
367
– Lie-group isomorphism
358
diﬀerence operators
284
diﬀerentiable manifolds
323–328
diﬀerentiable maps
324, 328, 329, 346
diﬀerential
– 0-form
342
– k-form
226, 342
– forms
341–344
diﬀerential Galois theory
268, 271
diﬀerential topology
211, 216, 226
diﬀusion-limited aggregation (DLA)
40, 46, 47
digital images
224, 236
dilation
467, 491
dimension of a vector space
325
𝛿-distribution. See dirac-𝛿-distribution
dirac-𝛿-distribution
78, 79, 516, 517, 522, 531,
541
direct product
244, 256, 454
direct solutions
399
direct sum
186, 221, 454, 530
directionally diﬀerentiable
552
Dirichlet condition
461, 564, 569
Dirichlet type boundary condition
519
discrete Fourier transformation
508, 522
discrete groups
159, 164, 166–170
discrete Painlevé equations
286
discrete special functions
283, 285
discrete spectrum
464, 469, 470, 472
discrete symmetry
241, 243, 245, 247, 249, 286
discrete transformations
522
discrete-time maps
402
discretization
93, 97, 98, 560, 561, 576
disentangling theorem
164, 180, 206
dislocations
331, 332, 334
disordered series
417
dissipation
73, 89, 558, 560, 583
distribution
515–517, 531–537
– horizontal
368–371, 374
divergence
343, 543
divided diﬀerence
487, 488
doubly nonlinear cauchy problem
558
doubly nonlinear inclusion
559
drift correction
90
dual problem
556, 576
dual space
170, 326, 335, 341, 451, 552, 583
duality pairing
524
Duﬃng equation
388, 391, 400, 419
Dunkl Laplacian
267
Dunkl operators
243, 267
dynamic models
68
dynamic richardson method
483
dynamical groups
160, 194–199, 207, 209
dynamical models
198

Index
593
dynamical similarity
159, 160, 163
dynamical systems
11, 385ﬀ., 418ﬀ.
e
eccentricity
114, 426, 476, 503
edge contraction
121, 128, 154
edge deletion
122, 128, 154
edge-path group
230, 231
Ehrenfest model
11, 15
Ehresmann connection
368, 369
eigenspace
452
eigenvalues
– for ﬁnite-dimensional linear systems
480–487, 498
– iteration method for computing
478
– of ﬁnite matrices, computing
485
– perturbations of matrices and spectra
442–444
Einstein’s summation convention
326
Ekeland variational principle
584
elasticity
528, 543, 572, 573, 581
electrical networks
33, 111, 129
electrodynamics
111, 182, 183, 207, 226
electromagnetism
199, 323, 528
embedded submanifold
370
endomorphisms
251, 252
energetic formulation
560
energy equality
560
energy levels
134, 195, 207, 262
enrichment technique
45
epidemics on networks
153
epigraph convergence
581
epimorphism
223
equation
– algebraic
476–479
– cauchy
272
– kepler’s
476
– scalar
476, 477
– weak
497
equilibria
386–400, 402–406, 408, 584
equilibrium states
131, 387
equivalence of linear ﬂows
389, 390
equivalence relation
325, 366
ergodicity
51, 68
error
– absolute
477
– dynamic correlation
52–56
– global truncation
492
– Hermite interpolation
489
– interpolation
487, 489
– local truncation
491
– –for multistep methods
493
– multilevel Monte Carlo
97
– residual vector
483, 484
– sensitivity analysis
499
– vector
478
essential spectrum
458, 464, 470, 472, 473, 500
essentially self-adjoint operator
472
Euclid’s algorithm
294
Euler characteristic
212, 222, 228, 230
Euler–Lagrange equation
552, 554, 564, 566,
569, 570, 572, 577, 581
Euler–Maruyama method
93–97
event
34
evolution operator
385, 386, 467, 470
excited states
133
existence and uniqueness for SDEs
84
expanding interval
417, 421, 429, 430
expectation
35–37
explicit formula
413
exponential distribution
9, 25, 46
extended Euclidean algorithm
296
extended Maxwell’s equations
547f.
extensive property
355, 356
exterior algebra
338, 339, 341
exterior covariant derivative
372, 376
exterior derivative
226, 343f., 350, 372, 376f.
– covariant
372, 376
extrema
457, 485, 551
f
factorial function
259
factorization
240, 261–268, 296–300
– LU
481
– table of most common methods
482
faithful representation
185, 209
Fast-Fourier transform (FFT)
509
Feigenbaum’s constant
405
Fenchel inequality
557–559
Fermions
59–61, 179
Ferrers formulae
255
feynman diagrams
124
feynman graphs
124–128
feynman path integrals
467
ﬁber
216, 327, 361, 365
ﬁber bundle
327, 361–372
ﬁber-preserving map
329
Fibonacci series
41
ﬁltered probability space
82
ﬁltering
103, 104
ﬁlters, signal processing
512
ﬁltration
82, 233–235
ﬁnite groups
164, 166–169, 207, 252
ﬁnite-dimensional linear systems
480–485
ﬁnite-element method
489, 577, 584
ﬁnite-size problems
52, 53, 55
ﬁnite-size Scaling
53, 68

594
Index
ﬁrst Lyapunov coeﬃcient
394, 395
ﬁrst variational equation
402–404
ﬁxed point
8, 85, 228, 242, 402, 403, 476–478
– iteration
476, 477
– iteration schemes
476
– local convergence
478
ﬂavors
62
ﬂoating points
62, 480
Floer homology
229
Flops
480–482
Floquet multipliers
402–404
Flory–Huggins theory
64, 65
ﬂuctuation-dissipation relation
89
ﬂuctuations
51, 52
ﬂuid mechanics
434–436
– 4He momentum distribution
59
ﬂux density
356, 357
Fokker–Planck equation
24, 99–103, 536
foliations
428
forest
115, 123, 125, 127, 154, 155
1-form
341
– canonical
346, 373, 374
formulation
– classical
564, 569, 571, 574, 577
– de giorgi
559, 560
– energetic
560
– mixed
575
– weak
564, 565, 569–571, 574, 577
forward Chapman–Kolmogorov equation
99
forward Euler method
493, 496
forward/backward substitution
481
Fourier expansion
453, 456, 472, 506, 507
Fourier series transformation
506–511, 522
Fourier sine/cosine transformation
518
Fourier transform
59, 79, 451
Fourier–laplace transformation
510–518, 521,
522, 529–531, 533, 534
Fourier–Plancherel operator
451, 459, 460, 464,
468, 472
Fourier–Plancherel transformation
504,
510–512, 515, 517, 518, 520, 522, 523
fourth order systems
568
fractals
47, 228
fractional power series
443
fractional-step method
561, 562
fréchet subdiﬀerential
554, 555
Fredholm alternative
456
Fredholm integral equation
40
Frobenius
– expansion
250, 280
– method
248, 258, 259
– norm
76, 176
– notation
245, 246
– theorem
370–372
Frobenius–Stickleberger relations
275
Fuchsian equations
247–250
function
– analytic
10, 453, 510
– characteristic
10
fundamental group
213–218, 221, 230, 231
fundamental solution
517, 535
fundamental theorem
– for vector ﬁelds on manifolds
347
– of algebra
301
– of calculus
312
fundamental vector ﬁeld
360, 369, 370, 372, 375
g
Galerkin method
497, 498, 576
galois groups
271, 272
galton–watson process
7
gambler’s ruin
14
gamma function
259
gâteaux diﬀerential
552, 583
gauge theory
61, 62, 160, 199–201, 207–210,
236
gauges
415–417, 445
Gauss–Bonnet theorem
222
Gauss–seidel method
483
Gaussian distribution
9, 53, 515
Gaussian elimination
480, 481
general topology
211
generalized series
415–417, 420, 421, 445, 446
generalized weyl theorem
464
generating function
426
– convolution
7, 17
– Hermite polynomials
206, 207
– moment generating function
9, 10, 21
– probability generating function
5, 25
– special functions
202, 245, 262, 263, 282, 283,
285, 286
– stochastic process
3–10, 14–17, 21, 24, 25, 33
– uniqueness
5, 7, 9
generator of a group
168
genus
216
geometric Brownian motion
86–88, 90, 91, 93,
95, 102
geometric realization
219
geometric singular perturbation
433
Giambelli formula
246
Gibbs canonical distribution
89
Gibbs ensemble
52
Gibbs random ﬁeld
29–31
Gillespie’s algorithm
25, 26
girth
114, 118, 154
global bifurcation
392

Index
595
global ﬂow
347
global minimizer
479, 553
global solution of a cauchy problem
491
global stability
495, 496
global truncation error
492
good reduction theorem
297, 298
google pagerank
145, 483
gradient ﬂow lines
227
gradient method
479, 480, 484
gram–schmidt orthogonalization
202
grand canonical ensemble
52, 64
graph
– bipartite
115, 117–119, 154
– complete
114, 115, 123, 147, 148, 154
– connected
114, 118, 119, 150, 154, 215
– cycle
154
– directed
112, 114
– disconnected
115
– Erdös–rényi
154
– formal deﬁnition
112
– invariant
123, 154
– isomorphic
114, 138
– nullity
154
– planar
115
– random
134–136, 138, 154, 155
– regular
115, 138, 155
– simple
112, 113
– star
155
– trivial
115, 123
– undirected
112–114, 151
– weighted
112, 126, 130
graph diameter
114, 154
graph theory
111–137, 147
greatest common divisor (GCD)
294
Green’s functions
133, 134, 253, 517
Green’s tensor
533–535
Green–Lagrange strain tensor
573
Gröbner bases
304–308
group action
241, 243, 358, 359, 365, 366
group axioms
165
group consistency condition
365
group element-matrix element duality
186, 187
group of boundaries
220
group of cycles
220
group theory
159–210
– symmetries
271
h
Hadamard conditions
529, 566, 583
Hadamard transformation
522
Hamilton principle
576
Hamilton variational principle
559
Hamilton–Jacobi theory
426
Hamiltonian equation
116, 131–133
Hamiltonian function
328, 347, 425
Hamiltonian operator
42, 47, 58, 191, 193,
467–472
Hamiltonian systems
346, 347, 424–426
Hamiltonian vector ﬁeld
346
Hammersley–Cliﬀord theorem
31
Hankel determinants
261
Hankel transformation
523
harmonic balance method
399, 400
harmonic functions
402
harmonic oscillators
131, 164, 195–197, 207,
262, 515, 532
harmonic polynomials
536
Harris–Kesten theorem
33
Hartley transformation
519, 520
Hartman–Grobman theorem
389–391, 398,
403, 404
Hausdorﬀspace
320
– topology
320
heat transport
545
heat-bath method
49, 68
Heisenberg group
171, 179, 181, 204
Heisenberg representation
180
helium
486
Hellinger–Toeplitz theorem
458
Hermite elements
490
Hermite interpolation
488, 489
Hermite quadrature formulae
489
Hermitean conjugate
61
Hermitean operator
454, 455, 457, 467, 472
hermitian matrices
486
heteroclinic orbits
386
Heun method
91, 96, 97
higgs mechanism
201
higher-dimensional transformations
521, 522
Hilbert space
450, 452–456, 472, 505–507, 530,
552, 562
Hilbert space geometry
452
Hilbert transformation
513, 525
Hilbert–Schmidt operator
456, 457, 473
Hilbert–Schmidt theorem
456
histogram method
52
homoclinic orbits
386, 418, 429
homogeneous systems
478
homological algebra
222–224
homology
218–224
homomorphism
166, 212, 358
homotopy 212ﬀ.
Hook diagrams
246
Hopf bifurcation
392–396, 399, 404, 428
horizontal distribution
368–371, 374
Horner’s scheme
508
Hubbard model
118–120
Hückel molecular orbital method
116

596
Index
Hunziker-van Winter–Zhislin theorem
470
hydrocarbon
117–119, 154
hyperbolic equilibrium
387, 390, 403
hyperelastic material
572
hypergeometric equation
258
hypergeometric functions
259, 260
hyperplane
243–247
hypersurface
425, 567
i
identity element
165, 209, 214, 358
identity transformations
359, 422, 426, 445
IEEE standard, ﬂoating-point numbers
291
imperfect bifurcations
414
implicit Euler formula
560, 561, 584
importance sampling
42, 45, 47–52, 56, 60, 62,
68, 69
incidence matrix
113, 118, 154
inclusion–exclusion property
222
incremental function
491, 492
independent events
35
independent variables 5ﬀ.
independent vectors
363, 449, 462
indeterminates
242, 292
indicator function 𝛿k
555
indicator variable
18, 36, 37
indicial equation
248, 258
induced rounding
52, 68
inﬁnite discrete groups
159, 169
inﬁnite groups
214, 242
inﬁnite-dimensional min-max principle
497
inﬁnitesimal rate
23
initial-boundary value problems
540–544
initial layer
429, 431, 436, 437, 445
initial value problem
429, 491–496, 541, 556
inner solution
430–435, 445
inner-product spaces
377–379, 450, 472
innocuous polynomials
294
integral transform
4–10
integration
– by parts
87
– in computer algebra
308–312
– Monte Carlo methods
43, 44
– of n-forms in ℝn 353–355
interacting boson model (IBM)
199
interfaces
52, 54, 55, 57
intermediate variable
431
interpolation
93, 476, 487–490, 493
– Hermite
488–490
– Lagrange
487
– nodes
487–490, 493
– piecewise polynomial
489–491
interpolation error function
487
invariant density
101
invariant manifolds
390, 429, 433
invariant sets
228, 386, 387, 389, 393, 405–407
inverse fourier transformation
517
inverse power method
478, 486
inverse radon transform
525
inverse shifted power method
486
inverse transformation
504, 505, 507, 510, 530
inverse-mapping theorem
451
inversely restricted sampling
45
irreducibility
15
irreducible representation
179, 186–192, 244,
246
irreps
186, 188–190, 196
Ising model
30, 55, 63, 120
isolated point
234, 464
isomorphic groups
231
isomorphism
166, 358
isospins
201
isothermal-isobaric ensemble
52
iteration matrix
482
iteration stationary methods
483
Itô formula
86–90, 92, 99, 102
Itô integral
81–85, 88, 90, 92–94, 99, 102
– martingale property
83
Itô isometry
83, 88
Itô SDEs
84, 86, 88–93, 95, 98, 101, 102
j
Jacobi identity
175, 208, 251, 331
Jacobi polynomials
261
Jacobi–Trudi formulae
245
Jacobian determinant
324, 343, 353
Jacobian elliptic functions
277
Jacobian matrix
184, 329, 387, 388, 395, 401,
402, 406, 408
jansen formula
258
joins
115, 228
jordan normal form
398, 411, 442, 444
jump rate, Markov dynamics
27
k
Kalman–Bucy ﬁlter
103, 104
Karhunen–Loève expansion
80
Karush–Kuhn–Tucker condition
575, 583, 585
Kato theorem
469
Kato–Rellich theorem
459, 469
Kelvin–Voigt model
543, 544
Kepler’s law
160, 412
Kirchhoﬀtransformation
571, 584
Kolmogorov–Arnol’d–Moser (KAM) theorem
412, 425
Kolmogorov backward equation
24, 25
Kolmogorov diﬀerential equations
24
Korn inequality
573, 584

Index
597
Korteweg–de vries (KDV) equation
276
Krylov–Bogoliubov–Mitropolski (KBM) method
422
l
lagrange ﬁnite elements of order
1 498
lagrange form
487
lagrange interpolating polynomial
487, 488
lagrange interpolation
487
lagrangean
555, 575
lagrangian mechanics
328, 378, 380
laguerre polynomials
453
lamb shift
486
Lamé equations
277
Lamé system
573
Landau–Mignotte bound
297, 300, 301
Langevin equation
85, 101–103
Laplace equation
253, 255
– separation of variables
256
Laplace transform
10, 504
Laplacian matrix
126–128, 151, 152, 155
Laplacian operator
113, 114, 253, 255
large deviation theory
103
large deviations, random walks
10, 21, 22
lattice gauge theory
61, 62, 67
lattice sites, random selection
44
Lavrentiev phenomenon
567, 568, 572, 583
law of rare events
7
law of total probability
8, 13, 35
Lax–Ritchmyer theorem
496
Lebesgue space LP 557, 562, 583
left-invariant vector ﬁelds
361
legendre conjugate
557
legendre polynomials
203, 255, 259
legendre transformation
557, 559, 583
Legendre–Fenchel transformation
559, 583
legendre–Hadamard condition
566, 583
Lie algebra
175–182, 188–190, 205–209,
250–253, 331, 361
Lie bracket
330–334
Lie derivative
347–351
Lie group
173–175, 202, 250, 251, 358–361
– automorphism
358
– constructing
177
lie series
422, 445
lie transforms
422, 426
Lieb’s theorem
119, 120
Lieb–Thirring Inequality
469
limit circle
473
limit cycles
386, 394–396, 399–408
limit point
473
𝜔-limit sets
385–387
Lindstedt method
395, 419, 421, 422, 426, 427,
443, 445
linear combination
325
linear diﬀerential equations
261, 271, 272, 311,
312, 507
linear groups
174, 176, 246, 358
linear homeomorphism
505
linear matrix representation
160, 185, 207, 208
linear multiplicative algorithm
41
linear multistep methods
492
linear operator
334–337, 449ﬀ.
linear ordinary diﬀerential equations
311
linear PDE
528
linear search technique
479
linear solvers, Matlab and Octave
482
linear space
491, 551, 562, 583
linear standard model. See Poynting–Thomson
model
linear systems
480–487
linear transformations
172, 174, 182, 411, 442
linear vector space
174–176, 186, 208, 209
linearization
– constructing lie algebras
175–177
– of an ode
387, 402, 403
Liouville theorem
270
Liouville’s formula
404
Liouville’s principle
311
Lipschitz condition
84
Lipschitz continuous function
476, 491
Littlewood–Richardson rule
246
local bifurcation
391, 392, 404
local coordinate system
321, 370, 373
local truncation error
491, 493
long-term behavior
8, 15
Lorentz group
172, 176, 181–185, 207
Lu -factorization
481
Lyapunov function
388, 389
Lyapunov stability
387
m
macromedium, diﬀerential geometry
364
magnetization, spontaneous
53
Mangasarian–Fromovitz constraint qualiﬁcation
555
manifolds
– center
390, 393, 395, 397–399
– diﬀerentiable
323–328
– invariant
390, 429, 433
– of a limit cycle
403, 404
– of equilibria
390, 391
– riemannian
377–380
– symplectic
345, 346
– topological
319–324
Markov chain
– aperiodic
31
– continuous time
22–27, 29

598
Index
Markov chain (contd.)
– discrete time
10–18
– generator
26, 28
– irreducible
15, 16, 31
– positive-recurrent
16
– random ﬁeld
30, 31
– recurrent
16, 18
– transient
18
– transition probabilities
12
Markov chain Monte Carlo
31
Markov process
10, 17, 23, 26, 27, 33, 50, 56, 68,
99
Markov property
– continuous time
22
– strong
12, 13, 17, 27
martingale property of the Itô integral
83
master equation
23, 48, 50, 51, 55, 59, 69
matching
118, 155, 431, 433, 434, 445, 446
material law
539, 542–547
Matlab and octave linear solvers
482
matrix groups
170–173, 358
maximization
556
maximum dissipation principle
560
Maxwell’s equations
183, 200, 542
– extended 547f.
Maxwell–Dirac System
548
Mayer–Vietoris exact sequence
223
mean displacement of an atom (VERTEX)
155
Mellin transformation
520
Melnikov function
429
metallurgy
63
methods
– Adams
493–495
– Adams–Bashforth
493
– Adams–Moulton
493, 494
– conjugate-gradient (CG)
479, 480, 484, 576
– descent
479, 484, 485
– descent, for quadratic forms
479, 484
– direct, for linear systems
480, 499
– dynamic Richardson
483
– explicit
492–494
– factorization, for linear systems
480, 482
– ﬁnite element
489, 577, 584
– ﬁxed point
477
– Galerkin
497, 498, 576
– gauss–seidel
482, 483
– gaussian elimination
480, 481
– gradient
479, 480, 484
– implicit
94, 492, 494, 495
– inverse power
478, 486
– inverse shifted power
486
– iteration, for eigenvalues
478
– iteration, for linear systems
482
– iteration, for nonlinear scalar equations
477
– linear multistep
492
– midpoint
492
– multistep
492–496
– Newton’s
401, 476–479, 576
– Newton–Simpson
478
– numerical minimization
475, 479, 480
– one-step
13, 14, 491, 492, 494–496
– power
486
– preconditioned conjugate gradient (PCG)
485
– Runge–Kutta
494
– Simpson’s
492
– static Richardson
483
– stationary, for linear systems
482–484
– successive over relaxation (SOR)
482, 483
metric, cartan
178, 182, 205, 209
Metropolis importance sampling
47
micromedium, diﬀerential geometry
364
microstructure
364, 578, 580–582
midpoint method
492
Milstein method
94–98
Min-max principle
485, 486, 497
minimal polynomials
534
minimum-energy principle
552, 583
minus-sign problem
60
mixed method, perturbation methods
436
möbius strip
403
Möbius bius transformations
249
modular algorithms
296, 297, 299
moduli
456, 482
molecular dynamics
51, 66, 69
molecular dynamics method
69
molecular Hamiltonian
155
moment generating function
9, 10, 21
moments
100, 101
monodromy group
248, 249, 272, 281
monodromy matrix
402–404, 406
monomorphism
223
monotone operator
553
Monte-Carlo methods
31, 39–71, 92, 93, 98
Monte-Carlo step
49, 50, 65, 69
Morse function
226–230, 233, 234
– discrete
229, 230
Morse theory
212, 224, 226–229, 236
– discrete
229, 236
Morse–Bott function
228
Morse–Smale–Witten Complex
228
motion equation for simple harmonic motion
273
mountain pass
554, 575
Moutard transformation
266, 267
multicanonical ensemble
52
multilevel Monte Carlo method
97, 98
multiple shooting
401
multiple-scale method
426, 429, 438

Index
599
multiplicative noise
85, 86
multistep methods, numerical analysis
492
multivariate gaussian
105
multivariate systems
304
n
n-dimensional
– Fourier–Laplace transformation
521
– topological manifold
321, 323, 324
naive expansion
419, 445, 446
natural boundary conditions
570
Neimark–Sacker bifurcations
404
Nekhoroshev theorem
412, 425
Nelson theorem
459
Nemytski˘i mapping theorem
563, 564
nerve of a cover
234
network community
155
network motif
142, 155
network theory
111–121
network transitivity
115
networks
111ﬀ. 137–154
Nevanlinna theory
286
Newton divided diﬀerence formula
487
Newton form
487
Newton’s method
476, 478, 576
Newton–Simpson method
478
nilpotence
251, 344
noise
– colored
77–80
– thermal
74
– white
73, 77–79, 86, 101
nonanticipating stochastic process
82
nonlinear heat-transfer problem
571
nonlinear oscillations
413, 418–427, 439–441
nonlinear scalar equations
476, 477
nonlinear special functions
265, 272–282
nonlinear systems
304, 390, 477–479, 494, 499
nontrival closed V-path
229
nonvariational methods
585
normal cone
555
normal distribution
9, 77, 104
normal form
427, 428
– Jordan
398, 411, 442, 444
– Smith (SNF)
231
normal operator
455, 458, 459, 464, 473
normed space
449–451
nuclear shell structure
195–198
null homotopic
213
nullity
117–120, 154
numerical approximation
80, 93, 95, 476, 487,
571, 576, 582
numerical methods
92–98
– collocation
401, 402
– numerical continuation
405–408
– numerical shooting
400, 401
numerical minimization
475, 479, 480
numerical solution of diﬀerential equations
476
NVT ensemble
52
o
Octave linear solvers
482
Ogden material
572, 574, 584
one-dimensional simple random walk
11
one-dimensional stable manifold
391
one-dimensional unstable manifold
391
one-parameter
– bifurcation diagram
393–395, 397
– group of transformations
348, 360
– pseudo-group of transformations
348
– subgroups
348, 360, 375
one-step methods
491, 492, 494–496
Onsager’s symmetry condition
558
open-map theorem
451
operator with pure point spectrum
473
optimal control, relaxed
585
order of one-step/multistep methods
402
ordered series
417
ordinary diﬀerential equations (ODEs)
– equilibria
386ﬀ.
– Fuchsian equations
247–250
– limit cycles
399ﬀ.
– numerical solution of the cauchy problem
491ﬀ.
– stochastic diﬀerential equations
73, 74, 86, 94
oriented incidence matrix
113
ornstein–Uhlenbeck (OU) process
73–75, 88,
89, 100
ornstein–zernike relation
6
orthogonal groups
172, 174, 572
orthogonal polynomials
260, 261
orthogonality of special functions
260
orthogonality relations
186–188, 260
orthogonalization, Gram–Schmidt
202
oscillator
– harmonic
131, 164, 195–197, 207, 262, 515,
532
– quantum
133, 262, 515
outer solution
431–435, 445, 446
overlap domain
431, 438, 445
p
p-adic algorithms
300, 301
PageRank algorithm
145, 483
Painlevé equations
243, 280–283
– discrete
286
parabolic cylinder coordinates
256
parallel transport
368–370, 374, 375
parallelism
367, 376, 377

600
Index
parallelogram identity
450
parametrization, perturbation methods
413,
414, 445
parseval identity
453, 473
partial diﬀerential equations (PDEs)
527–548
– factorization
265–267
– Fuchsian equations
247
– perturbation theory
433–435
– separation of variables
256
– stochastic diﬀerential equations
98–104
partial diﬀerential operator
531, 536
particle path
46
partition theorem
6, 13, 37
partition, Monte Carlo methods
61
passage time
13, 103
path integral
58, 59
Path Integral Monte Carlo (PIMC)
57–60
percolation model
31–33
period-doubling bifurcation
404, 405, 407
periodic orbits
211, 386
periodic standard form
422–424, 440, 441
periodicity
15
permutation group
164, 168, 169, 195
persistence diagram
233, 234
persistent homology
232–234
perturbation parameter
413–416, 422, 428,
444–446
perturbation series
415–418, 430, 431, 445
perturbation theory
411–444
phase factor
209
phase space
50, 51, 185, 328, 345
phase transitions
52–54
Picard–Vessiot extension
271, 272
piecewise polynomial interpolation
489–491
piecewise polynomials
489, 490, 576
piezoelectromagnetic model
546
piezoelectromagnetism
546–548
Pitchfork bifurcation
392, 394, 396–398, 404
pivot elements
481
pivoting technique
481
Planck’s constant
161
Plateau variational problem
567
Pochhammer symbol
259, 260
Poincaré groups
415, 419, 445
Poincaré lemma
226
Poincaré map
402, 403, 405
Poincaré section
402
Poincaré-Lindstedt method
395
point groups
164, 169
point sources
505
point spectrum
452, 455, 456, 464, 473
Poisson bracket
347
Poisson distribution
5, 6, 25, 35, 36, 140, 141
Poisson equation
534, 536
Poisson process
25
Poisson summation formula
517
polar coordinates
243, 423
Pólya’s recurrence theorem
18, 19
polyconvexity
565, 566, 572
polyhedron
219, 230
polymer science
45, 63–67
polynomials
– interpolation
476, 488–490
– numerical methods
489
polytope
219
pontryagin maximum principle
583, 585
position vectors
332, 333
potential
– double-well
567, 578
– of dissipative forces
558
potential energy
58, 85, 131
potential theory
24, 536
Potts model
30, 48, 120–123
power series
– Frobenius method
248
– lie algebra
177
– perturbation methods
413, 415, 443
Poynting–Thomson model
544
preconditioned conjugate gradient (PCG) method
485
preconditioned residual
484
preconditioner
482
predictable stochastic process
82, 83
pre-dual
553, 554, 567
preserved foliations
428
principal axis transformation
503
principal bundle
365–367
– associated
362, 364
– connections
369, 370
principle of least dissipation
558
principle of relativity
166, 182, 185
probability density function (PDF)
89, 99, 102,
105
probability generating function
5, 25
probability mass function
5, 7, 29–31, 35
probability space
34–36, 82, 105
probability theory
3, 33–37
problem
– cauchy
491, 492, 495, 496, 500, 556–559
– eigenvalue
40, 443, 476, 485, 498–500
– evolution
491, 560, 577, 585
– initial value
429, 493, 541, 556
– linear eigenvalue
480, 498
– spectral
476, 496, 497
product topology
320, 324
production rate
356, 357
projection operators
363
propagation
7, 153, 540, 546

Index
601
propagators
124
pseudo-indeterminates
292
pseudorandom numbers. See random numbers
Puiseux expansions
268
pullbacks
341–344, 349, 354, 357
pure absorption method
436
pure envelope method
436, 438, 441
pure state of a quantum system
473
pushforwards
334, 342, 349
Pythagoras’ theorem
160, 161
q
q-Hermite polynomials
285
Q-matrix, Markov chain
23, 25
quadratic variation
77
quadrature formulae, Hermite
489
quantization
116
quantum calculus
284
quantum chromodynamics (QCD)
61
quantum ﬁeld theory
61, 124, 229, 435, 468
quantum groups
240, 283, 284
quantum mechanics
468–472
quantum Monte Carlo methods
57–61
quantum numbers
189, 191, 194, 195
quantum oscillator
133
quark model
62
quasiconvex envelope
578
quasi-leibniz rule
344
quasi-periodic orbits
386
quaternions
169, 174–176, 449
quenched approximation
62
r
radiation
46
radioactive decay
39, 40
Radon transformation
523–525
random behavior
75
random ﬁeld
29–31, 80
– Gibbs
29–31
– Markov
29–31
random-number generator (RNG)
40, 41, 44, 69
random numbers
39–46, 49, 69
random variable
3, 35–37
random walk
18–22
– examples
11, 12, 14, 16
– self-avoiding (SAW)
44, 45
rank-one convexity
565, 566, 583
rate-independent
560
rational functions
239, 249, 274, 294, 308–311
rational numbers
292
rayleigh quotient
485, 497
real roots
303
rectangular coordinates
243
rectangular grids
231
recurrence relations
202
– special functions
240, 248, 260–263, 266, 282,
283, 285
recurrent markov chain
16, 18
recursion
41, 204, 205, 493, 496
reducibility
252
reﬂection groups
240, 243, 244, 246, 267
reﬂection principle
19, 20
reﬂections
185
region of absolute stability
496
regular chains
304, 308
regular perturbations
417–419, 424, 429, 437,
445
relative topology
320
relaxation
577
– by convex compactiﬁcation
578
relaxation oscillation
433, 445
Rellich–Kondrachov theorem
563
renewal relation
17, 21
renormalization group (RG) method
446
representations
– faithful
185, 209
– lie group
160, 169, 179, 199, 203, 208, 251, 252
rescaled coordinate
446
rescaled variables
414
resistance distance
129, 130, 155
resolvent computation
26
resonance
425, 446
response functions
52
rest points
418, 419
reversibility
12
ricatti equations
104, 270
Riemann P-symbol
258, 259
Riemann integrals
80, 81, 87, 353
Riemannian connections
379, 380
Riemannian manifolds
377–380
Riemannian symmetric space
159, 181, 182, 207
Riesz lemma
452, 473
Riesz–schauder theorem
456
Ritz method
576, 584
Robinson–Schensted–Knuth (RSK) correspon-
dence
246
Rodrigues’ formula
202
Rössler system
405–407
rotation groups
171
rotation symmetries
239, 522
Rothe method
560, 584
rounding errors
499, 505
Rouse diﬀusion
66
row echelon form
480
ruin probability
14
Runge’s phenomenon
488
Runge–Kutta method
494
Runge–Lenz vector
194

602
Index
s
saddle equilibrium
391, 392
saddle point
554, 575
saddle-node bifurcation
392–398, 406–408
sample path
105
sampling
42–51, 55–57, 60, 62, 68, 69
scale-free network
141, 155
Scaling
162, 163
scattering theory
470–472
Schatten classes
456
Schramm–Loewner evolution
33
Schrödinger equation
– gauge theory
199, 209
– historical formulation
538
– Monte Carlo simulation
59, 60
– quantum mechanics
528
– Scaling
162
– symmetry groups
190
– unitary groups
467
Schrödinger operator
461, 468–470, 472, 473,
476, 531
Schur polynomials
244–246
Schwartz space
531–533
Schwarzian derivative
249
Second order systems
564–568
Second-order diﬀerential equations
263
Secondary hopf bifurcations
404
self-adjoint operator
457–473
– adjacency operator
113
self-adjoint extensions
458, 460–462
self-averaging
45
self-avoiding walk (SAW)
44, 45
self-excited oscillation
446
self-similarity
3, 76
semiconductors
162, 163
semi-grand canonical ensemble
52
semi-implicit scheme
561
separation of variables
191, 192, 250, 256
separatrices
390
set theory notation
33, 34
Shannon’s sampling theorem
517
Shielding problem
46
Shift register generators
41
Shifting
52–54
shortest path
114, 129, 139, 143, 147, 155
signal processing
80, 512
Signorini contact
573
similarity transformation
175, 209
simple sampling
39, 42–45, 47, 69
simple symmetric random walk
18, 19
simplicial complex
218–223, 225, 229–232, 234
simplicial homology
218, 219, 221, 224, 225
simpliﬁcation
193, 294, 300, 313, 428
Simpson’s method
492
singular homology
218, 224, 228
singular hopf bifurcation
395
singular perturbations
417, 446, 578, 582
singularities
120, 259, 269, 280
Skew diagram
246
Skew symmetry
331, 337–339
Skew-self-adjoint
537, 538, 540, 543, 544
slow–fast perturbation methods
432, 433
small-strain tensor
573
smith normal form (SNF)
231, 232
smooth functions
323, 563
smooth manifold
324
smooth map
324
smooth real-valued functional
552
smooth vector ﬁelds
330, 344
smoothness
359, 380, 565, 584
Sobolev critical exponent
563
sobolev space
498, 562–564, 583
soldering form
373
solitons
265, 267, 276
solvability
252, 271, 272, 528, 573
sound waves
50
source terms
528, 548
space group
164, 170
spanning forest
115, 123, 125, 155
spanning tree
115, 123, 125, 126, 155, 216, 231,
232
Sparse matrix
304
Sparse model
293, 295, 310
Sparse polynomials
295
special functions
239–286
– group theory
202–207
special linear groups
174, 176
speciﬁc function spaces
562–576
spectra
442–444
spectral analysis
465
spectral density
79, 135, 136
spectral measure
462–466, 473
spectral problems
496–499
spectral radius
452, 482, 483
spectral representations
507, 511, 512, 520, 523
spectral theorem
261, 463, 464
spectral theory
131, 462, 463, 465, 467
spectrum of an operator
473
sphere-valued harmonic maps
574
spherical bessel functions
268
spherical harmonics
203, 204, 240, 250, 255, 256,
264
spins, Potts model
120
spontaneous magnetization
53
square-free decomposition
295, 309
St. Venant–kirchhoﬀmaterial
573
stable manifolds
227, 390, 391, 399, 403, 429
standard topology
321, 323, 353

Index
603
state space
385ﬀ.
stationary covariance
76, 79
stationary distribution
12, 14–16, 18
statistical ensembles
52, 68
statistical errors
55, 56
statistical ineﬃciency
55
statistical mechanics
61–68, 146–148
statistical thermodynamics
47–52
steady-state problems
563
steepest descent method
485
stefanelli’s variational principle
558
stochastic diﬀerential equations
73–104
stochastic integrals
76, 80–84, 90, 95
stochastic matrix
11, 14, 151
stochastic process
3–37, 75ﬀ.
– predictable
82, 83
stochastic trajectories
50, 51
Stokes phenomenon
269
Stokes system
575
Stokes’ theorem
355, 357
Stone formula
466, 473
Stone theorem
467
stone–von neumann theorem
468
stopping criterion
576
straight-line program
293
straightforward expansion
419, 445
strange attractors
228, 386, 429
Stratonovich integral
81, 82, 90–92
strictly convex
553
strong convergence
92, 94, 96, 98, 462
strong law of large numbers
21
strong markov property
12, 13, 17, 27
structurally unstable system
390, 391
structure constants
175, 176, 208
– of the gauge group
61
– of the moving frame
334
Sturm–liouville equations
260
subdiﬀerential
554, 555
subgroups, diﬀerential geometry
358
subset topology
320, 323
subspace
– horizontal
368
– vertical
368, 370
successive over relaxation (SOR)
482, 483
supercomputers
62
superposition
326, 533, 563
surface eﬀects
54
surface physics
57, 67
susceptibility
53, 54
Suzuki–Trotter formula
59
sweep step
50
symanzik polynomial
125–129
symbolic computation. See computer algebra
symmetric functions
244, 245, 247, 523
symmetric operator
458, 459, 461, 462, 473
symmetric polynomials
242, 244, 245
symmetries
– continuous
250–261
– discrete
241–250
– tensor product
337, 338
symmetry condition, Onsager
558
symmetry groups
160, 190–194, 207–209
symplectic geometry
229, 345
symplectic manifolds
345, 346
symplectic vector spaces
345
synchronization
151–153
system
– conﬁned to a box
498
– ﬁnite-dimensional linear
480, 481, 483, 485
– linear
480–487
– nonlinear
304, 477–479, 494, 499
– unconﬁned
499
t
tangent bundle
327–330
tangent cone Tk (u)
555
tangent map
329
tangent space
326
tangent vector
324, 325
Taylor series expansion
206, 243, 420
Taylor’s theorem
96, 477, 479
tempered distributions
516, 520, 531–534
tensor bundles
342, 364, 372
tensor ﬁeld over
342
tensor products
– direct sums
454
– linear operators
334–336
– of operators
460
test functions
408, 515, 516
– space of
515
theorem of Frobenius
370–372, 377
thermal equilibrium
50, 67, 68, 89
thermal noise
74
𝜕−𝛿-theory
283
thermodynamics
47–52
thermoelastic system
545, 546
thermoelasticity
545, 546
theta function
240, 278
Thom’s lemma
304
Thomas factor
173
tight-binding model
115–117
time average
51, 68
time dependence
75, 527, 528
time set
385
time-T map
402, 403
tomography
523
topological equivalence
387, 389, 403
topological manifold
319–324

604
Index
topological space
319, 320
torsion tensor
375–377
total variation
77
trace-class operator
457, 471, 472
trajectories
20, 57, 470, 556
transformations
– canonical
360, 426, 505
– mathematical
503–525
– symmetries
241, 522
transient markov chain
18
transition density
99, 101
transition function
22, 323, 324, 343, 361
– between coordinate charts
321
– of a markov chain
22
transition layer
411, 433, 445, 446
transition matrix
12–14, 56
transition probability
199
transition rate matrix
23
translation groups
169, 170, 216, 244, 465
translations
76, 360
transport process
42, 46
transpose matrix
174, 503
transposition
585
tree
115
trial move
49
triangular matrices
252
triangulated space
219, 222
trigonometric functions
311
triple deck
431, 434, 435, 446
trivial multiplier
403
trivializations, diﬀerential geometry
362
trotter formula
467
truncation error
491–493
Tschirnhaus transformation
302
turning point problem
434
tutte polynomial
121–123
two-dimensional Fourier–Plancherel
523
two-element groups
164, 166, 185
two-parameter bifurcation diagram
393, 394
u
umbrella sampling
56
unbounded operators
452, 457–462
uncertainty
3
unconﬁned systems
499
uncorrelated random variables
80, 105
unfolding
– Arnol’d
444
– Pitchfork bifurcation
397, 398
– universal
414, 446
unforced conservative oscillator
421
uniform approximation
420, 446
uniformity
416, 417
uniqueness property
4, 7, 9
unitarily equivalent operators
459, 473
unitary Fourier series transformation
506, 508
unitary groups
174, 195, 467, 468, 473
unitary irreducible representations (UIR)
188–190, 202, 203, 207
unitary operator
455, 467, 468, 473
universal enveloping algebra
251, 264, 284
universal unfolding
414
unstable manifolds
390–392, 403, 404, 429
v
V-path
229, 230
van der pol equation
395, 396, 421, 423, 427
van dyke matching rule
431
van kampen theorem
214
variable-metric methods
576
variation of constants
88, 89, 100
variational convergence
581, 583
variational inequality
554, 555, 559, 570, 571,
574
variational methods
584
variational principle
556–560, 576, 583, 584
variational problems
551–584
varieties
244
vector ﬁeld
330
– discrete
229, 230
– fundamental
360, 369, 370, 372, 375
– gradient
229, 230
– hamiltonian
346
vector space
325, 326, 449, 451
– representations
251, 253, 257
vertex degree
155
vertical subspace
368, 370
vibrations
50, 130–134, 394
vietoris–rips complex
235
viscoelasticity
543, 544
von neumann’s saddle-point theorem 554
von Zeipel’s method
422, 426
Voronoi diagram
235
w
water droplet freezing
52
wave equation
534, 536, 540
wave operator
471–473
weak solution
85, 565, 566, 570, 577
wedge product
214, 339, 340, 344
Weierstraß elliptic functions
272
Weierstraß maximum principle
579, 581, 583,
585
Weierstraß theorem
553
weight functions
60
wentzell–freidlin theory
103
Weyl alternative
461
Weyl group
244, 247, 282, 286

Index
605
Weyl relations
468
Weyl theorem
464, 473
white noise
73, 77–79, 86, 101
Whittaker functions
256
Wiener process. See Brownian motion
Wiener–khintchine theorem
80
Wigner matrices
190
witness complex
236
WKB method
434, 441
x
Young diagram
244–246
Young measures
579–583
y
z-transformation
509, 510
Zero Dirichlet boundary condition
103, 571
Zero-energy state
117, 120
Zero-stability
495

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook EULA.


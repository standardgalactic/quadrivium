Leif Mejlbro
Random variables II
Probability Examples c-3
Download free books at

2 
 
 
 
  
Leif Mejlbro
 
Probability Examples c-3
Random variables II
Download free eBooks at bookboon.com

3 
 
 
 
Probability Examples c-3 â€“ Random variables II 
Â© 2009 Leif Mejlbro & Ventus Publishing ApS 
ISBN 978-87-7681-518-9
 
Download free eBooks at bookboon.com

Random variables II
 
4 
Contents
	
Introduction 	
5
1 	
Some theoretical results 	
6
2 	
Law of total probability 	
20
3 	
Correlation coefficient and skewness 	
23
4 	
Examples concerning the Poisson distribution 	
60
5 	
Miscellaneous examples 	
70
	
Index 	
116
Contents
Download free eBooks at bookboon.com

Random variables II
 
5 
Introduction
Introduction
This is the third book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole JÃ¸rsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The topic itself, Random Variables, is so big that I have felt it necessary to divide it into three books,
of which this is the second one. We shall here continue the study of frequencies and distribution
functions in 1 and 2 dimensions, and consider the correlation coeï¬ƒcient. We consider in particular
the Poisson distribution.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ï¬rst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
26th October 2009
Download free eBooks at bookboon.com

Random variables II
 
6 
1. Some theoretical results
1
Some theoretical results
The abstract (and precise) deï¬nition of a random variable X is that X is a real function on â„¦, where
the triple (â„¦, F, P) is a probability ï¬eld, such that
{Ï‰ âˆˆâ„¦| X(Ï‰) â‰¤x} âˆˆF
for every x âˆˆR.
This deï¬nition leads to the concept of a distribution function for the random variable X, which is the
function F : R â†’R, which is deï¬ned by
F(x) = P{X â‰¤x}
(= P{Ï‰ âˆˆâ„¦| X(Ï‰) â‰¤x}),
where the latter expression is the mathematically precise deï¬nition which, however, for obvious reasons
everywhere in the following will be replaced by the former expression.
A distribution function for a random variable X has the following properties:
0 â‰¤F(x) â‰¤1
for every x âˆˆR.
The function F is weakly increasing, i.e. F(x) â‰¤F(y) for x â‰¤y.
limxâ†’âˆ’âˆF(x) = 0
and
limxâ†’+âˆF(x) = 1.
The function F is continuous from the right, i.e. limhâ†’0+ F(x + h) = F(x)
for every x âˆˆR.
One may in some cases be interested in giving a crude description of the behaviour of the distribution
function. We deï¬ne a median of a random variable X with the distribution function F(x) as a real
number a = (X) âˆˆR, for which
P{X â‰¤a} â‰¥1
2
and
P{X â‰¥a} â‰¥1
2.
Expressed by means of the distribution function it follows that a âˆˆR is a median, if
F(a) â‰¥1
2
and
F(aâˆ’) = lim
hâ†’0âˆ’F(x + h) â‰¤1
2.
In general we deï¬ne a p-quantile, p âˆˆ]0, 1[, of the random variable as a number ap âˆˆR, for which
P {X â‰¤ap} â‰¥p
and
P {X â‰¥ap} â‰¥1 âˆ’p,
which can also be expressed by
F (ap) â‰¥p
and
F (apâˆ’) â‰¤p.
If the random variable X only has a ï¬nite or a countable number of values, x1, x2, . . . , we call it
discrete, and we say that X has a discrete distribution.
A very special case occurs when X only has one value. In this case we say that X is causally distributed,
or that X is constant.
Download free eBooks at bookboon.com

Random variables II
 
7 
1. Some theoretical results
The random variable X is called continuous, if its distribution function F(x) can be written as an
integral of the form
F(x) =
 x
âˆ’âˆ
f(u) du,
x âˆˆR,
where f is a nonnegative integrable function.
In this case we also say that X has a continuous
distribution, and the integrand f : R â†’R is called a frequency of the random variable X.
Let again (â„¦, F, P) be a given probability ï¬eld. Let us consider two random variables X and Y , which
are both deï¬ned on â„¦. We may consider the pair (X, Y ) as a 2-dimensional random variable, which
implies that we then shall make precise the extensions of the previous concepts for a single random
variable.
We say that the simultaneous distribution, or just the distribution, of (X, Y ) is known, if we know
P{(X, Y ) âˆˆA}
for every Borel set A âŠ†R2.
When the simultaneous distribution of (X, Y ) is known, we deï¬ne the marginal distributions of X
and Y by
PX(B) = P{X âˆˆB} := P{(X, Y ) âˆˆB Ã— R},
where B âŠ†R is a Borel set,
PY (B) = P{Y âˆˆB} := P{(X, Y ) âˆˆR Ã— B},
where B âŠ†R is a Borel set.
Notice that we can always ï¬nd the marginal distributions from the simultaneous distribution, while it
is far from always possible to ï¬nd the simultaneous distribution from the marginal distributions. We
now introduce
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables II
 
8 
1. Some theoretical results
The simultaneous distribution function of the 2-dimensional random variable (X, Y ) is deï¬ned as the
function F : R2 â†’R, given by
F(x, y) := P{X â‰¤x âˆ§Y â‰¤y}.
We have
â€¢ If (x, y) âˆˆR2, then 0 â‰¤F(x, y) â‰¤1.
â€¢ If x âˆˆR is kept ï¬xed, then F(x, y) is a weakly increasing function in y, which is continuous from
the right and which satisï¬es the condition limyâ†’âˆ’âˆF(x, y) = 0.
â€¢ If y âˆˆR is kept ï¬xed, then F(x, y) is a weakly increasing function in x, which is continuous from
the right and which satisï¬es the condition limxâ†’âˆ’âˆF(x, y) = 0.
â€¢ When both x and y tend towards inï¬nity, then
lim
x, yâ†’+âˆF(x, y) = 1.
â€¢ If x1, x2, y1, y2 âˆˆR satisfy x1 â‰¤x2 and y1 â‰¤y2, then
F (x2, y2) âˆ’F (x1, y2) âˆ’F (x2, y1) + F (x1, y2) â‰¥0.
Given the simultaneous distribution function F(x, y) of (X, Y ) we can ï¬nd the distribution functions
of X and Y by the formulÃ¦
FX(x) = F(x, +âˆ) =
lim
yâ†’+âˆF(x, y),
for x âˆˆR,
Fy(x) = F(+âˆ, y) =
lim
xâ†’+âˆF(x, y),
for y âˆˆR.
The 2-dimensional random variable (X, Y ) is called discrete, or that it has a discrete distribution, if
both X and Y are discrete.
The 2-dimensional random variable (X, Y ) is called continuous, or we say that it has a continuous
distribution, if there exists a nonnegative integrable function (a frequency) f : R2 â†’R, such that the
distribution function F(x, y) can be written in the form
F(x, y) =
 x
âˆ’âˆ
 y
âˆ’âˆ
f(t, u) du

dt,
for (x, y) âˆˆR2.
In this case we can ï¬nd the function f(x, y) at the diï¬€erentiability points of F(x, y) by the formula
f(x, y) = âˆ‚2F(x, y)
âˆ‚xâˆ‚y
.
It should now be obvious why one should know something about the theory of integration in more
variables, cf. e.g. the Ventus: Calculus 2 series.
We note that if f(x, y) is a frequency of the continuous 2-dimensional random variable (X, Y ), then X
and Y are both continuous 1-dimensional random variables, and we get their (marginal) frequencies
by
fX(x) =
 +âˆ
âˆ’âˆ
f(x, y) dy,
for x âˆˆR,
Download free eBooks at bookboon.com

Random variables II
 
9 
1. Some theoretical results
and
fY (y) =
 +âˆ
âˆ’âˆ
f(x, y) dx,
for y âˆˆR.
It was mentioned above that one far from always can ï¬nd the simultaneous distribution function from
the marginal distribution function. It is, however, possible in the case when the two random variables
X and Y are independent.
Let the two random variables X and Y be deï¬ned on the same probability ï¬eld (â„¦, F, P). We say
that X and Y are independent, if for all pairs of Borel sets A, B âŠ†R,
P{X âˆˆA âˆ§Y âˆˆB} = P{X âˆˆA} Â· P{Y âˆˆB},
which can also be put in the simpler form
F(x, y) = FX(x) Â· FY (y)
for every (x, y) âˆˆR2.
If X and Y are not independent, then we of course say that they are dependent.
In two special cases we can obtain more information of independent random variables:
If the 2-dimensional random variable (X, Y ) is discrete, then X and Y are independent, if
hij = fi Â· gj
for every i and j.
Here, fi denotes the probabilities of X, and gj the probabilities of Y .
If the 2-dimensional random variable (X, Y ) is continuous, then X and Y are independent, if their
frequencies satisfy
f(x, y) = fX(x) Â· fY (y)
almost everywhere.
The concept â€œalmost everywhereâ€ is rarely given a precise deï¬nition in books on applied mathematics.
Roughly speaking it means that the relation above holds outside a set in R2 of area zero, a so-called
null set. The common examples of null sets are either ï¬nite or countable sets. There exists, however,
also non-countable null sets. Simple examples are graphs of any (piecewise) C 1-curve.
Concerning maps of random variables we have the following very important results,
Theorem 1.1 Let X and Y be independent random variables. Let Ï• : R â†’R and Ïˆ : R â†’R be
given functions. Then Ï•(X) and Ïˆ(Y ) are again independent random variables.
If X is a continuous random variable of the frequency I, then we have the following important theorem,
where it should be pointed out that one always shall check all assumptions in order to be able to
conclude that the result holds:
Download free eBooks at bookboon.com

Random variables II
 
10 
1. Some theoretical results
Theorem 1.2 Given a continuous random variable X of frequency f.
1) Let I be an open interval, such that P{X âˆˆI} = 1.
2) Let Ï„ : I â†’J be a bijective map of I onto an open interval J.
3) Furthermore, assume that Ï„ is diï¬€erentiable with a continuous derivative Ï„ â€², which satisï¬es
Ï„ â€²(x) Ì¸= 0
for alle x âˆˆI.
Under the assumptions above Y := Ï„(X) is also a continuous random variable, and its frequency g(y)
is given by
g(y) =
ï£±
ï£´
ï£²
ï£´
ï£³
f

Ï„ âˆ’1(y)

Â·


Ï„ âˆ’1â€² (y)
 ,
for y âˆˆJ,
0,
otherwise.
We note that if just one of the assumptions above is not fulï¬lled, then we shall instead ï¬nd the
distribution function G(y) of Y := Ï„(X) by the general formula
G(y) = P{Ï„(X) âˆˆ] âˆ’âˆ, y]} = P

X âˆˆÏ„ â—¦âˆ’1(] âˆ’âˆ, y])

,
where Ï„ â—¦âˆ’1 = Ï„ âˆ’1 denotes the inverse set map.
Note also that if the assumptions of the theorem are all satisï¬ed, then Ï„ is necessarily monotone.
At a ï¬rst glance it may be strange that we at this early stage introduce 2-dimensional random variables.
The reason is that by applying the simultaneous distribution for (X, Y ) it is fairly easy to deï¬ne the
elementary operations of calculus between X and Y . Thus we have the following general result for a
continuous 2-dimensional random variable.
Theorem 1.3 Let (X, Y ) be a continuous random variable of the frequency h(x, y).
The frequency of the sum X + Y is
k1(z) =
 +âˆ
âˆ’âˆh(x, z âˆ’x) dx.
The frequency of the diï¬€erence X âˆ’Y is
k2(z) =
 +âˆ
âˆ’âˆh(x, x âˆ’z) dx.
The frequency of the product X Â· Y is
k3(z) =
 +âˆ
âˆ’âˆh

x , z
x

Â· 1
|x| dx.
The frequency of the quotient X/Y is
k4(z) =
 +âˆ
âˆ’âˆh(zx , x) Â· |x| dx.
Notice that one must be very careful by computing the product and the quotient, because the corre-
sponding integrals are improper.
If we furthermore assume that X and Y are independent, and f(x) is a frequency of X, and g(y) is a
frequency of Y , then we get an even better result:
Download free eBooks at bookboon.com

Random variables II
 
11 
1. Some theoretical results
Theorem 1.4 Let X and Y be continuous and independent random variables with the frequencies
f(x) and g(y), resp..
The frequency of the sum X + Y is
k1(z) =
 +âˆ
âˆ’âˆf(x)g(z âˆ’x) dx.
The frequency of the diï¬€erence X âˆ’Y is
k2(z) =
 +âˆ
âˆ’âˆf(x)g(x âˆ’z) dx.
The frequency of the product X Â· Y is
k3(z) =
 +âˆ
âˆ’âˆf(x) g
 z
x

Â· 1
|x| dx.
The frequency of the quotient X/Y is
k4 =
 +âˆ
âˆ’âˆf(zx)g(x) Â· |x| dx.
Let X and Y be independent random variables with the distribution functions FX and FY , resp.. We
introduce two random variables by
U := max{X, Y }
and
V := min{X, Y },
the distribution functions of which are denoted by FU and FV , resp.. Then these are given by
FU(u) = FX(u) Â· FY (u)
for u âˆˆR,
and
FV (v) = 1 âˆ’(1 âˆ’FX(v)) Â· (1 âˆ’FY (v))
for v âˆˆR.
These formulÃ¦ are general, provided only that X and Y are independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables II
 
12 
1. Some theoretical results
If X and Y are continuous and independent, then the frequencies of U and V are given by
fU(u) = FX(u) Â· fY (u) + fX(u) Â· FY (u),
for u âˆˆR,
and
fV (v) = (1 âˆ’FX(v)) Â· fY (v) + fX(v) Â· (1 âˆ’Fy(v)) ,
for v âˆˆR,
where we note that we shall apply both the frequencies and the distribution functions of X and Y .
The results above can also be extended to bijective maps Ï• = (Ï•1 , Ï•2) : R2 â†’R2, or subsets of R2.
We shall need the Jacobian of Ï•, introduced in e.g. the Ventus: Calculus 2 series.
It is important here to deï¬ne the notation and the variables in the most convenient way. We start
by assuming that D is an open domain in the (x1 x2) plane, and that ËœD is an open domain in the
(y1 , y2) plane. Then let Ï• = (Ï•1 , Ï•2) be a bijective map of ËœD onto D with the inverse Ï„ = Ï•âˆ’1, i.e.
the opposite of what one probably would expect:
Ï• = (Ï•1 , Ï•2) : ËœD â†’D,
with (x1 , x2) = Ï• (y1 , y2) .
The corresponding Jacobian is deï¬ned by
JÏ• = âˆ‚(x1 , x2)
âˆ‚(y1 , y2) =

âˆ‚Ï•1
âˆ‚y1
âˆ‚Ï•2
âˆ‚y1
âˆ‚Ï•1
âˆ‚y1
âˆ‚Ï•2
âˆ‚y2

,
where the independent variables (y1 , y2) are in the â€œdenominatorsâ€. Then recall the Theorem of
transform of plane integrals, cf. e.g. the Ventus: Calculus 2 series: If h : D â†’R is an integrable
function, where D âŠ†R2 is given as above, then for every (measurable) subset A âŠ†D,

A
h (x1 , x2) dx1dx2 =

Ï•âˆ’1(A)
h (x1 , x2) Â·

âˆ‚(x1 , x2)
âˆ‚(y1 , y2)
 dy1dy2.
Of course, this formula is not mathematically correct; but it shows intuitively what is going on:
Roughly speaking we â€œdelete the y-sâ€. The correct mathematical formula is of course the well-known

A
h (x1 , x2) dx1dx2 =

Ï•âˆ’1(A)
(Ï•1 (y1 , y2) , Ï•2 (y1 , y2)) Â·
JÏ• (y1 , y2)
 dy1dy2,
although experience shows that it in practice is more confusing then helping the reader.
Download free eBooks at bookboon.com

Random variables II
 
13 
1. Some theoretical results
Theorem 1.5 Let (X1, X2) be a continuous 2-dimensional random variable with the frequency h (x1 , x2).
Let D âŠ†R2 be an open domain, such that
P {(X1 , X2) âˆˆD} = 1.
Let Ï„ : D â†’ËœD be a bijective map of D onto another open domain ËœD, and let Ï• = (Ï•1 , Ï•2) =
Ï„ âˆ’1, where we assume that Ï•1 and Ï•2 have continuous partial derivatives and that the corresponding
Jacobian is diï¬€erent from 0 in all of ËœD.
Then the 2-dimensional random variable
(Y1 , Y2) = Ï„ (X1 , X2) = (Ï„1 (X1 , X2) , Ï„2 (X1 , X2))
has the frequency k (y1 , y2), given by
k (y1 , y2) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
h (Ï•1 (y1 , y2) , Ï•2 (y1 , y2)) Â·

âˆ‚(x1 , x2)
âˆ‚(y1 , y2)
 ,
for (y1 , y2) âˆˆËœD,
0,
otherwise
We have previously introduced the concept conditional probability. We shall now introduce a similar
concept, namely the conditional distribution.
If X and Y are discrete, we deï¬ne the conditional distribution of X for given Y = yj by
P {X = xi | Y = yj} = P {X = xi âˆ§Y = yj}
P {Y = yj}
= hij
gj
.
It follows that for ï¬xed j we have that P {X = xi | Y = yj} indeed is a distribution. We note in
particular that we have the law of the total probability
P {X = xi} =

j
P {X = xi | Y = yj} Â· P {Y = yj} .
Analogously we deï¬ne for two continuous random variables X and Y the conditional distribution
function of X for given Y = y by
P{X â‰¤x | Y = y} =
 x
âˆ’âˆf(u, y) du
fY (y)
,
forudsat, at fY (y) > 0.
Note that the conditional distribution function is not deï¬ned at points in which fY (y) = 0.
The corresponding frequency is
f(x | y) = f(x, y)
fY (y) ,
provided that fY (y) = 0.
We shall use the convention that â€œ0 times undeï¬ned = 0â€. Then we get the Law of total probability,
 +âˆ
âˆ’âˆ
f(x | y) Â· fY (y) dy =
 +âˆ
âˆ’âˆ
f(x, y) dy = fX(x).
We now introduce the mean, or expectation of a random variable, provided that it exists.
Download free eBooks at bookboon.com

Random variables II
 
14 
1. Some theoretical results
1) Let X be a discrete random variable with the possible values {xi} and the corresponding proba-
bilities pi = P {X = xi}. The mean, or expectation, of X is deï¬ned by
E{X} :=

i
xi pi,
provided that the series is absolutely convergent. If this is not the case, the mean does not exists.
2) Let X be a continuous random variable with the frequency f(x). We deï¬ne the mean, or expectation
of X by
E{X} =
 +âˆ
âˆ’âˆ
x f(x) dx,
provided that the integral is absolutely convergent. If this is not the case, the mean does not exist.
If the random variable X only has nonnegative values, i.e. the image of X is contained in [0, +âˆ[,
and the mean exists, then the mean is given by
E{X} =
 +âˆ
0
P{X â‰¥x} dx.
Concerning maps of random variables, means are transformed according to the theorem below, pro-
vided that the given expressions are absolutely convergent.
Theorem 1.6 Let the random variable Y = Ï•(X) be a function of X.
1) If X is a discrete random variable with the possible values {xi} of corresponding probabilities
pi = P{X = xi}, then the mean of Y = Ï•(X) is given by
E{Ï•(X)} =

i
Ï• (xi) pi,
provided that the series is absolutely convergent.
2) If X is a continuous random variable with the frequency f(x), then the mean of Y = Ï•(X) is
given by
E{Ï•(X)} =
 +âˆ
âˆ’âˆ
Ï•(x) g(x) dx,
provided that the integral is absolutely convergent.
Assume that X is a random variable of mean Âµ. We add the following concepts, where k âˆˆN:
The k-th moment,
E

Xk
.
The k-th absolute moment,
E

|X|k
.
The k-th central moment,
E

(X âˆ’Âµ)k
.
The k-th absolute central moment,
E

|X âˆ’Âµ|k
.
The variance, i.e. the second central moment,
V {X} = E

(X âˆ’Âµ)2
,
Download free eBooks at bookboon.com

Random variables II
 
15 
1. Some theoretical results
provided that the deï¬ning series or integrals are absolutely convergent. In particular, the variance is
very important. We mention
Theorem 1.7 Let X be a random variable of mean E{X} = Âµ and variance V {X}. Then
E

(X âˆ’c)2
= V {X} + (Âµ âˆ’c)2
for every c âˆˆR,
V {X} = E

X2
âˆ’(E{X})2
for c = 0,
E{aX + b} = a E{X} + b
for every a, b âˆˆR,
V {aX + b} = a2V {X}
for every a, b âˆˆR.
It is not always an easy task to compute the distribution function of a random variable. We have the
following result which gives an estimate of the probability that a random variable X diï¬€ers more than
some given a > 0 from the mean E{X}.
Theorem 1.8 (Ë‡CebyË‡sevâ€™s inequality). If the random variable X has the mean Âµ and the variance
Ïƒ2, then we have for every a > 0,
P{|X âˆ’Âµ| â‰¥a} â‰¤Ïƒ2
a2 .
If we here put a = kÏƒ, we get the equivalent statement
P{Âµ âˆ’kÏƒ < X < Âµ + kÏƒ} â‰¥1 âˆ’1
k2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables II
 
16 
1. Some theoretical results
These concepts are then generalized to 2-dimensional random variables. Thus,
Theorem 1.9 Let Z = Ï•(X, Y ) be a function of the 2-dimensional random variable (X, Y ).
1) If (X, Y ) is discrete, then the mean of Z = Ï•(X, Y ) is given by
E{Ï•(X, Y )} =

i, j
Ï• (xi , yj) Â· P {X = xi âˆ§Y = yj} ,
provided that the series is absolutely convergent.
2) If (X, Y ) is continuous, then the mean of Z = Ï•(X, Y ) is given by
E{Ï•(X, Y )} =

R2 Ï•(x, y) f(x, y) dxdy,
provided that the integral is absolutely convergent.
It is easily proved that if (X, Y ) is a 2-dimensional random variable, and Ï•(x, y) = Ï•1(x) + Ï•2(y),
then
E {Ï•1(X) + Ï•2(Y )} = E {Ï•1(X)} + E {Ï•2(Y )} ,
provided that E {Ï•1(X)} and E {Ï•2(Y )} exists. In particular,
E{X + Y } = E{X} + E{Y }.
If we furthermore assume that X and Y are independent and choose Ï•(x, y) = Ï•1(x)Â·Ï•2(y), then also
E {Ï•1(X) Â· Ï•2(Y )} = E {Ï•1(X)} Â· E {Ï•2(Y )} ,
provided that E {Ï•1(X)} and E {Ï•2(Y )} exists. In particular we get under the assumptions above
that
E{X Â· Y } = E{X} Â· E{Y },
and
E{(X âˆ’E{X}) Â· (Y âˆ’E{Y })} = 0.
These formulÃ¦ are easily generalized to n random variables. We have e.g.
E
 n

i=1
Xi

=
n

i=1
E {Xi} ,
provided that all means E {Xi} exist.
If two random variables X and Y are not independent, we shall ï¬nd a measure of how much they
â€œdependâ€ on each other. This measure is described by the correlation, which we now introduce.
Consider a 2-dimensional random variable (X, Y ), where
E{X} = ÂµX,
E{Y } = ÂµY ,
V {X} = Ïƒ2
X > 0,
V {Y } = Ïƒ2
Y > 0,
Download free eBooks at bookboon.com

Random variables II
 
17 
1. Some theoretical results
all exist. We deï¬ne the covariance between X and Y , denoted by Cov(X, Y ), as
Cov(X, Y ) := E {(X âˆ’ÂµX) Â· (Y âˆ’ÂµY )} .
We deï¬ne the correlation between X and Y , denoted by Ï±(X, Y ), as
Ï±(X, Y ) := Cov(X, Y )
ÏƒX Â· ÏƒY
.
Theorem 1.10 Let X and Y be two random variables, where
E{X} = ÂµX,
E{Y } = ÂµY ,
V {X} = Ïƒ2
X > 0,
V {Y } = Ïƒ2
Y > 0,
all exist. Then
Cov(X, Y ) = 0,
if X and Y are independent,
Cov(X, Y ) = E{X Â· Y } âˆ’E{X} Â· E{Y },
|Cov(X, Y )| â‰¤ÏƒX Â· Ïƒy,
Cov(X, Y ) = Cov(Y, X),
V {X + Y } = V {X} + V {Y } + 2Cov(X, Y ),
V {X + Y } = V {X} + V {Y },
if X and Y are independent,
Ï±(X, Y ) = 0,
if X and Y are independent,
Ï±(X, X) = 1,
Ï±(X, âˆ’X) = âˆ’1,
|Ï±(X, Y )| â‰¤1.
Let Z be another random variable, for which the mean and the variance both exist- Then
Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z),
for every a, b âˆˆR,
and if U = aX + b and V = cY + d, where a > 0 and c > 0, then
Ï±(U, V ) = Ï±(aX + b, cY + d) = Ï±(X, Y ).
Two independent random variables are always non-correlated, while two non-correlated random vari-
ables are not necessarily independent.
By the obvious generalization,
V
 n

i=1
Xi

=
n

i=1
V {Xi} + 2
n

j=2
jâˆ’1

i=1
Cov (Xi, Xj) .
If all X1, X2, . . . , Xn are independent of each other, this is of course reduced to
V
 n

i=1
Xi

=
n

i=1
V {Xi} .
Finally we mention the various types of convergence which are natural in connection with sequences
of random variables. We consider a sequence Xn of random variables, deï¬ned on the same probability
ï¬eld (â„¦, F, P).
Download free eBooks at bookboon.com

Random variables II
 
18 
1. Some theoretical results
1) We say that Xn converges in probability towards a random variable X on the probability ï¬eld
(â„¦, F, P), if
P {|Xn âˆ’X| â‰¥Îµ} â†’0
for n â†’+âˆ,
for every ï¬xed Îµ > 0.
2) We say that Xn converges in probability towards a constant c, if every ï¬xed Îµ > 0,
P {|Xn âˆ’c| â‰¥Îµ} â†’0
for n â†’+âˆ.
3) If each Xn has the distribution function Fn, and X has the distribution function F, we say that
the sequence Xn of random variables converges in distribution towards X, if at every point of
continuity x of F(x),
lim
nâ†’+âˆFn(x) = F(x).
Finally, we mention the following theorems which are connected with these concepts of convergence.
The ï¬rst one resembles Ë‡CebyË‡sevâ€™s inequality.
Theorem 1.11 (The weak law of large numbers). Let Xn be a sequence of independent random
variables, all deï¬ned on (â„¦, F, P), and assume that they all have the same mean and variance,
E {Xi} = Âµ
and
V {Xi} = Ïƒ2.
Then for every ï¬xed Îµ > 0,
P

1
n
n

i=1
Xi âˆ’Âµ
 â‰¥Îµ

â†’0
for n â†’+âˆ.
A slightly diï¬€erent version of the weak law of large numbers is the following
Theorem 1.12 If Xn is a sequence of independent identical distributed random variables, deï¬ned
on (â„¦, F, P) where E {Xi} = Âµ, (notice that we do not assume the existence of the variance), then
for every ï¬xed Îµ > 0,
P

1
n
n

i=1
Xi âˆ’Âµ
 â‰¥Îµ

â†’0
for n â†’+âˆ.
We have concerning convergence in distribution,
Theorem 1.13 (Helly-Brayâ€™s lemma). Assume that the sequence Xn of random variables con-
verges in distribution towards the random variable X, and assume that there are real constants a and
b, such that
P {a â‰¤Xn â‰¤b} = 1
for every n âˆˆN.
If Ï• is a continuous function on the interval [a, b], then
lim
nâ†’+âˆE {Ï• (Xn)} = E{Ï•(X)}.
In particular,
lim
nâ†’+âˆE {Xn}
and
lim
nâ†’+âˆV {Xn} = V {X}.
Download free eBooks at bookboon.com

Random variables II
 
19 
1. Some theoretical results
Finally, the following theorem gives us the relationship between the two concepts of convergence:
Theorem 1.14 1) If Xn converges in probability towards X, then Xn also converges in distribution
towards X.
2) If Xn converges in distribution towards a constant c, then Xn also converges in probability towards
the constant c.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables II
 
20 
2. Law of the total probability
2
Law of total probability
Example 2.1 Given a countable number of boxes: U1, U2, . . . , Un, . . . . Let box number n contain
n slips of paper with the numbers 1, 2, . . . , n. We choose at random with probability pn the box Un,
and from this box we choose randomly one of the slips of paper. Let X denote the random variable,
which indicates the number of the chosen box, and let Y denote the random variable, which gives the
number on the chosen slip of paper.
1) Find the distribution of the random variable Y .
2) Prove that the mean E{Y } exists if and only if the mean E{X} exists. When both these means
exist one shall express E{Y } by means of E{X}.
3) Assume that pn = pqnâˆ’1, where p > 0, q > 0 and p + q = 1. Find
P{Y = 1}.
1) It is given that
âˆ

n=1
pn = 1,
pn â‰¥0,
and
P{X = b} = pn,
n âˆˆN,
and
P{Y = k | X = n} =
ï£±
ï£´
ï£²
ï£´
ï£³
1
n,
k = 1, . . . , n,
0,
otherwise.
When we apply the law of total probability, it follows for any k âˆˆN that
P{Y = k}
=
âˆ

n=1
P{Y = k | X = n} Â· P{X = n} =
âˆ

n=k
P{Y = k | X = n} Â· P{X = n}
=
âˆ

n=k
1
n pn.
2) Assume that E{Y } exists. Since all terms are â‰¥0, we can interchange the summations,
E{Y }
=
âˆ

k=1
k P{Y = k} =
âˆ

k=1
âˆ

n=k
k
n Â· pn =
âˆ

n=1
n

k=1
k Â· 1
n pn =
âˆ

n=1
1
2 n(n + 1) 1
n pn
=
1
2
âˆ

n=1
(n + 1)pn = 1
2
âˆ

n=1
npn + 1
2
âˆ

n=1
pn = 1
2 + 1
2 E{X}.
If on the other hand E{X} exists, then we can reverse all computations above and conclude that
E{Y } exists. In fact, every term is â‰¥0, so the summations can be interchanged, which gives
E{Y } = 1
2 (1 + E{X}).
Download free eBooks at bookboon.com

Random variables II
 
21 
2. Law of the total probability
3) If pn = pqnâˆ’1, it follows from (1) that
P{Y = 1} =
âˆ

n=1
1
n p qnâˆ’1 = p
q
âˆ

n=1
1
n qn = p
q {âˆ’ln(1 âˆ’q)} =
p
1 âˆ’p ln
1
p

.
Example 2.2 Throw once an (honest) dice and let the random variable N denote the number given
by the dice.
Then ï¬‚ip a coin N times, where N is the random variable above, and let X denote the number of
heads in these throws.
1) Find P{X = 0 âˆ§N = i} for i = 1, 2, 3, 4, 5, 6.
2) Find P{X = 0}.
3) Find the mean E{X}.
1) If N = i, then X = 0 means that we get tails i times, thus
P{X = 0 âˆ§N = i} =
1
2

,
i = 1, 2, 3, 4, 5, 6.
2) By the law of total probability,
P{X = 0} =
6

i=1
P{X = 0 âˆ§N = i} Â· P{N = i} =
6

i=1
1
2

Â· 1
6 = 1
6

1 âˆ’1
26

= 21
128.
3) We get for j âˆˆ{1, . . . , i}, i âˆˆ{1, . . . , 6},
P{X = j âˆ§N = i} =

i
j

Â·
1
2
j
Â·
1
2
iâˆ’j
=

i
j
 1
2
i
,
hence
P{X = j} =
6

i=j
P{X = j âˆ§N = i} Â· P{N = i} = 1
6
6

i=j

i
j
 1
2
i
.
Then by interchanging the order of summation,
E{X}
=
6

j=1
j P{X = j} =
6

j=1
j
6
6

i=j
 i
j
 1
2
i
= 1
6
6

i=1
1
2
i
i

j=1
j
 i
j

=
1
6
6

i=1
1
2
i
Â· i
i

j=1

i âˆ’1
j âˆ’1

= 1
6
6

i=1
i
1
2
i iâˆ’1

k=0

i âˆ’1
k

= 1
6
6

i=1
i
1
2
i
2iâˆ’1
=
1
12
6

i=1
i = 1
12 Â· 1
2 Â· 6 Â· 7 = 7
4.
Download free eBooks at bookboon.com

Random variables II
 
22 
2. Law of the total probability
Example 2.3 A box contains N balls with the numbers 1, 2, . . . , N. Choose at random a ball from
the box and note its number X, without returning it to the box. Then select another ball and note its
number Y .
1) Find the distribution of the 2-dimensional random variable (X, Y ).
2) Find the distribution of the random variable Z = |X âˆ’Y |.
1) It is obvious that
P{(X, Y ) = (k, n)} =
ï£±
ï£´
ï£²
ï£´
ï£³
1
N(N âˆ’1)
for k, n âˆˆ{1, . . . , N} and k Ì¸= n,
0
otherwise.
2) Since X Ì¸= Y , the random variable Z = |X âˆ’Y | can only attain the values 1, 2, . . . , N âˆ’1. If
n âˆˆ{1, 2, . . . , N âˆ’1}, then
P{Z = n}
=
P{|X âˆ’Y | = n} = P{X âˆ’Y = n} + P{Y âˆ’X = n}
=
B

k=1
P{(X, Y ) = (n + k, k)} +
N

k=1
P{(X, Y ) = (k, n + k)}
=
2
N

k=1
P{(X, Y ) = (k, n + k)} = 2
Nâˆ’n

k=1
P{(X, Y ) = (k, n + k)} = 2
N âˆ’n
N(N âˆ’1).
Control. It follows that
Nâˆ’1

n=1
P{Z = n} =
Nâˆ’1

n=1
2 Â·
N âˆ’n
N(N âˆ’1) =
2
N(N âˆ’1)
Nâˆ’1

n=1
n =
2
N(N âˆ’1) Â· 1
2 (N âˆ’1)N = 1.
Download free eBooks at bookboon.com

Random variables II
 
23 
2. Law of the total probability
3
Correlation coeï¬ƒcient and skewness
Example 3.1 A random variable X has its distribution given by
P{X = i} =
1
100,
i = 1, 2, 3, . . . , 98, 99, 100.
Two random variables Y and Z depend on X, such that
Y =
ï£±
ï£²
ï£³
1,
if X can be divided by at least one of the numbers 2 or 3,
0,
otherwise,
and
Z =
ï£±
ï£²
ï£³
1,
if X can be divided by 3,
0,
otherwise.
Compute the correlation coeï¬ƒcient Ï±(Y, Z).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables II
 
24 
2. Law of the total probability
We shall ï¬nd
Ï±(Y, Z) = Cov(Y, Z)
Ïƒ1Ïƒ2
,
where
Cov(Y, Z) = E{Y Z} âˆ’E{Y }E{Z},
and
Ïƒ2
1 = V {Y }
and
Ïƒ2
2 = V {Z}.
The distribution functions of Y and Z are found by simply counting,
P{Y = 1}
=
P{X even} + P{X odd, and X is divisible by 3}
=
50

n=1
P{X = 2n} +
17

n=1
P{X = 6n âˆ’3} = 50
100 + 17
100 = 67
100,
and
P{Z = 1} = P{X is divisible by 3} =
33

n=1
P{X = 3n} = 33
100.
Since Y and Z can only have the values 0 and 1 (where 02 = 0 and 12 = 1), we get
E

Y 2
= E{Y } =
1

i=0
i(2)P{Y = i} = P{Y = 1} = 67
100,
and
E

Z2
= E{Z} =
1

i=0
i(2)P{Z = i} = P{Z = 1} = 33
100,
hence
Ïƒ2
1 = V {Y } = E

Y 2
âˆ’(E{Y })2 = 67
100 âˆ’
 67
100
2
= 67
100 Â· 33
100,
and
Ïƒ2
2 = V {Z} = E

Z2
âˆ’(E{Z})2 = 33
100 âˆ’
 33
100
2
= 33
100 Â· 67
100,
whence
Ïƒ1Ïƒ2 =

67
100 Â· 33
100 Â· 33
100 Â· 67
100 = 33
100 Â· 67
100.
Finally,
E{Y Z}
=
1

i=0
1

j=0
ij P{Y = i âˆ§Z = j} = P{Y = 1 âˆ§Z = 1}
=
P{X is divisible by 3} = P{Z = 1} = 33
100 = E{Z},
Download free eBooks at bookboon.com

Random variables II
 
25 
2. Law of the total probability
so
Cov(Y, Z) = E{Y Z} âˆ’E{Y }E{Z} = 33
100

1 âˆ’67
100

= 332
1002 .
We derive that the correlation coeï¬ƒcient is
Ï±(Y, Z) = Cov(Y, Z)
Ïƒ1Ïƒ2
=
332
1002
67
100 Â· 33
100
= 33
67.
Example 3.2 Let X denote a random variable, for which E{X} = Âµ, V {X} = Ïƒ2 and E

X3
all
exist.
1. Prove the formula
E

(X âˆ’Âµ)3
= E

X3
âˆ’Âµ

3Ïƒ2 + Âµ2
.
When V {X} is bigger than 0, we deï¬ne the skewness (asymmetry) of the distribution by the number
Î³(X), given by
Î³(X) = E

(X âˆ’Âµ)3
Ïƒ3
.
A random variable X has the possible values 0, 1, 2, of the corresponding probabilities p, 1
2, 1
2 âˆ’p,
where 0 â‰¤p â‰¤1
2.
2. Find the number Î³(X) of this distribution.
3. Find the values of p, for which Î³(X) = 0.
4. Find Î³(X) for p = 1
8.
1) The claim is proved in the continuous case.
The proof in the discrete case is analogous.
A
straightforward computation gives
E

(X âˆ’Âµ)3
=
 âˆ
âˆ’âˆ
(x âˆ’Âµ)3f(x) dx =
 âˆ
âˆ’âˆ

x3 âˆ’3Âµx2 + 3Âµ2x âˆ’Âµ3
f(x) dx
=
 âˆ
âˆ’âˆ
x3f(x) dx âˆ’Âµ
 âˆ
âˆ’âˆ

3x2 âˆ’3Âµx + Âµ2
f(x) dx
=
E

X3
âˆ’Âµ
 âˆ
âˆ’âˆ

3x2 âˆ’6Âµx + 3Âµ2 + 3Âµx âˆ’2Âµ2
f(x) dx
=
E

X3
âˆ’3Âµ
 âˆ
âˆ’âˆ
(x âˆ’Âµ)2f(x) dx âˆ’3Âµ2
 âˆ
âˆ’âˆ
x f(x) dx + 2Âµ3
 âˆ
âˆ’âˆ
f(x) dx
=
E

X3
âˆ’3ÂµÏƒ2 âˆ’2Âµ2Âµ + 2Âµ3 = E

X3
âˆ’Âµ

3Ïƒ2 + Âµ2
.
Download free eBooks at bookboon.com

Random variables II
 
26 
2. Law of the total probability
Alternatively, apply the following direct proof (all cases),
E

(X âˆ’Âµ)3
=
E

X3 âˆ’3ÂµX2 + 3Âµ2X âˆ’Âµ3
= E

X3
âˆ’3Âµ E

X2
+ 3Âµ2 E{X} âˆ’Âµ3
=
E

X3
âˆ’3Âµ

E

X2
âˆ’(E{X})2
âˆ’3Âµ (E{X})2 + 3Âµ3 âˆ’Âµ3
=
E

X3
âˆ’3ÂµÏƒ2 âˆ’3Âµ3 + 3Âµ3 âˆ’Âµ3
=
E

X3
âˆ’Âµ

3Ïƒ2 + Âµ2
.
2) If
P{X = 0} = p,
P{X = 1} = 1
2
og
P{X = 2} = 1
2 âˆ’p,
where 0 â‰¤p â‰¤1
2, then
Âµ = E{X} =
2

i=0
i P{X = i} = 0 Â· p + 1 Â· 1
2 + 2
1
2 âˆ’p

= 3
2 âˆ’2p,
and
E

X2
=
2

i=0
i2P{X = i} = 0 Â· p + 1 Â· 1
2 + 4
1
2 âˆ’p

= 5
2 âˆ’4p,
hence
Ïƒ2
=
E

X2
âˆ’(E{X})2 = 5
2 âˆ’4p âˆ’
3
2 âˆ’2p
2
= 5
2 âˆ’4p âˆ’
9
4 âˆ’6p + 4p2

=
1
4 + 2p âˆ’4p2 = 1
4

1 + 8p âˆ’16p2

â‰¥1
4

.
Finally,
E

X3
=
2

i=0
i3P{X = i} = 0 Â· p + 1 Â· 1
2 + 8
1
2 âˆ’p

= 9
2 âˆ’8p,
thus
E

(X âˆ’Âµ)3
=
E

X3
âˆ’Âµ

3Ïƒ2 + Âµ2
= 9
2 âˆ’8p âˆ’
3
2 âˆ’2p
 
3
4 + 6p âˆ’12p2 +
3
2 âˆ’2p
2
=
9
2 âˆ’8p âˆ’
3
2 âˆ’2p
 3
4 + 6p âˆ’12p2 + 9
4 âˆ’6p + 4p2

=
9
2 âˆ’8p âˆ’
3
2 âˆ’2p
 
3 âˆ’8p2
= 9
2 âˆ’8p âˆ’
9
2 âˆ’12p2 âˆ’6p + 16p3

=
9
2 âˆ’8p âˆ’9
2 + 12p2 + 6p âˆ’16p3 = âˆ’2p + 12p2 âˆ’16p3 = âˆ’p

16p2 âˆ’12p + 2

=
âˆ’16p

p âˆ’1
4
 
p âˆ’1
2

.
Download free eBooks at bookboon.com

Random variables II
 
27 
2. Law of the total probability
This implies that
Î³(X) = E

(X âˆ’Âµ)3
Ïƒ3
=
âˆ’16p

p âˆ’1
4
 
p âˆ’1
2

1
8 (1 + 8p âˆ’16p2)3/2
= âˆ’
128p

p âˆ’1
2
 
p âˆ’1
4

{2 âˆ’(4p âˆ’1)2}3/2
.
3) It follows immediately that Î³(X) = 0 for p = 0, 1
4, 1
2.
4) If p = 1
8, then
Î³(X) = âˆ’
128 Â· 1
8
1
8 âˆ’1
2
 1
8 âˆ’1
4


2 âˆ’
1
2 âˆ’1
23/2
= âˆ’
16 Â· 3
8 Â· 1
8

2 âˆ’1
4
3/2 = âˆ’
3
4
7
4

7
4
= âˆ’6
7
âˆš
7 â‰ˆâˆ’0.324.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables II
 
28 
2. Law of the total probability
Example 3.3 Given for any n âˆˆN a random variable Xn of the frequency
fn(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
(n âˆ’1)! anxnâˆ’1eâˆ’ax,
x > 0,
0,
otherwise,
where a is a positive constant.
Compute the skewness Î³ (Xn), and show that Î³ (Xn) â†’0 for n â†’âˆ.
According to Example 3.2 the skewness Î³ (Xn) is deï¬ned by
Î³ (Xn) =
E

(Xn âˆ’Âµn)3
Ïƒ3n
,
where
E

(Xn âˆ’Âµn)3
= E

X3
n

âˆ’Âµn

3Ïƒ2
n + Âµ2
n

.
By some small computations,
Âµn = E {Xn} =
an
(n âˆ’1)!
 âˆ
0
xneâˆ’ax dx =
1
a(n âˆ’1)!
 âˆ
0
tneâˆ’1 dt =
n!
a(n âˆ’1)! = n
a ,
and
E

X2
n

=
an
(n âˆ’1)!
 âˆ
0
xn+1eâˆ’ax dx =
(n + 1)!
a2(n âˆ’1)! = n(n + 1)
a2
,
hence
Ïƒ2
n = E

X2
n

âˆ’(E {Xn})2 = n(n + 1)
an
âˆ’n2
a2 = n
a2 ,
and
E

X3
n

=
an
(n âˆ’1)!
 âˆ
0
xn+2eâˆ’ax dx =
(n + 2)!
a3(n âˆ’1)! = n(n + 1)(n + 2)
a3
,
whence
E

(Xn âˆ’Âµn)3
=
E

X3
n

âˆ’Âµn

3Ïƒ2
n + Âµ2
n

= n(n + 1)(n + 2)
a3
âˆ’n
a Â·
3n
a2 + n2
a2

=
n
a3

n2 + 3n + 2 âˆ’3n âˆ’n2
= 2n
a3 .
The skewness is
Î³ (Xn) =
E

(Xn âˆ’Âµn)3
Ïƒ3n
= 2n
a3 Â· a3
n3/2 =
2
âˆšn â†’0
for n â†’âˆ.
Download free eBooks at bookboon.com

Random variables II
 
29 
2. Law of the total probability
Example 3.4 Assume that the 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
ï£±
ï£´
ï£²
ï£´
ï£³
2
A2 ,
0 < y < x < A,
0,
otherwise,
where A is a positive constant.
1) Find the frequencies of X and Y .
2) Find the means of X and Y .
3) Find the variances of X and Y .
4) Compute the correlation coeï¬ƒcient Ï± between X and Y , and prove that it does not depend on A.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 1: The domain where f(x, y) > 0 for A = 1.
1) If x âˆˆ]0, A[, then
fX(x) =
 x
0
2
A2 dy = 2x
A2 ,
and
fX(x) = 0 otherwise.
If y âˆˆ]0, A[, then
fY (y) =
 A
y
2
A2 dx = 2(A âˆ’y)
A2
= 2
A âˆ’2y
A2 ,
og
fY (y) = 0 otherwise.
2) The means are
E{X} =
 A
0
2x2
A2 dx = 2
3 A,
and
E{Y } =
 A
0
2y
A âˆ’2y2
A2

dy =
y2
A âˆ’2
3
y3
A2
A
0
= 1
3 A.
Download free eBooks at bookboon.com

Random variables II
 
30 
2. Law of the total probability
3) It follows from
E

X2
=
 A
0
2x3
A2 dx =
 x4
2A2
A
0
= A2
2
that
V {X} = E

X2
âˆ’(E{X})2 = A2
2 âˆ’4
9 A2 = A2
18 .
It follows from
E

Y 2
=
 A
0
2y2
A âˆ’2y3
A2

dy =
2y3
3A âˆ’y4
2A2
A
0
=
2
3 âˆ’1
2

A2 = A2
6
that
V {Y } = E

Y 2
âˆ’(E{Y })2 = A2
6 âˆ’A2
9 = A2
18 .
4) First compute
E{XY }
=
 
R2 xy f(x, y) dxdy = 2
A2
 A
0
 x
0
yx dy

dx = 2
A2
 A
0
xy2
2
x
y=0
dx
=
1
A2
 A
0
x3 dx = A2
4 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
31 
2. Law of the total probability
Then by insertion,
Cov(X, Y ) = E{XY } âˆ’E{X} Â· E{Y } = A2
4 âˆ’2
3 A Â· 1
3 A =
1
4 âˆ’2
9

A2 = A2
36 .
Finally, we obtain
Ï±(X, Y ) = Cov(X, Y )
ÏƒXÏƒy
=
1
36 A2
1
18 A2 = 1
2,
which is independent of A.
Example 3.5 Consider a 2-dimensional random variable (X, Y ), which in the parallelogram given by
the inequalities
0 â‰¤x â‰¤1
and
x â‰¤y â‰¤x + 1
has the frequency
f(x, y) = 2
3 (x + y),
while the frequency is equal to 0 anywhere else in the (x, y) plane.
1) Find the frequencies of the de random variables X and Y .
2) Find the means of each of the random variables X and Y .
3) Find the covariance Cov(X, Y ).
0
0.5
1
1.5
2
0.2
0.4
0.6
0.8
1
1) When x âˆˆ]0, 1[, it follows by a vertical integration that
fX(x) = 2
3
 x+1
x
(x + y) dy = 1
3

(x + y)2x+1
y=x = 1
3

(2x + 1)2 âˆ’(2x)2
= 4
3 x + 1
3,
Download free eBooks at bookboon.com

Random variables II
 
32 
2. Law of the total probability
thus
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
4
3 x + 1
3,
x âˆˆ]0, 1[,
0,
otherwise.
If y /âˆˆ]0, 2[, then fY (y) = 0.
If y âˆˆ]0, 1[, then by a horizontal integration,
fY (y) = 2
3
 y
0
(x + y)dx = 1
3

(x + y)2y
x=0 = 1
3

(2y)2 âˆ’y2
= y2.
If y âˆˆ]1, 2[, it follows again by a horizontal integration that
fY (y)
=
2
3
 1
yâˆ’1
(x + y) dy = 1
3

(x + y)21
x=yâˆ’1 = 1
3

(y + 1)2 âˆ’(2y âˆ’1)2
=
1
3

y2 + 2y + 1 âˆ’4y2 + 4y âˆ’1

= 2y âˆ’y2,
hence
fY (y) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
y2,
y âˆˆ]0, 1[,
2y âˆ’y2 = 1 âˆ’(y âˆ’1)2,
y âˆˆ]1, 2[,
0,
otherwise.
2) The means are
E{X} =
 1
0
x
4
3 x + 1
3

dx =
 1
0
4
3 x2 + 1
3 x

dx = 4
9 + 1
6 = 11
18,
and
E{Y }
=
 1
0
y3dy =
 2
1

2y2 âˆ’y3
dy = 1
4 +
2
3 y3 âˆ’1
4 y4
2
1
=
1
4 + 16
3 âˆ’16
4 âˆ’2
3 + 1
4 = 14
3 + 1
2 âˆ’4 = 2
3 + 1
2 = 7
6.
Download free eBooks at bookboon.com

Random variables II
 
33 
2. Law of the total probability
3) We ï¬rst compute
E{XY }
=
2
3
 1
0
 x+1
x
xy(x + y) dy

dx = 2
3
 1
0
 x+1
x

x2y + xy2
dy

dx
=
2
3
 1
0
1
2 x2y2 + 1
3 xy3
x+1
y=x
dx
=
2
3
 1
0
1
2 x2 
(x + 1)2 âˆ’x2
+ 1
3

(x + 1)3 âˆ’x3
dx
=
2
3
 1
0
1
2 x2(2x + 1) + 1
3 x

3x2 + 3x + 1

dx
=
2
3
 1
0

x3 + 1
2 x2 + x3 + x2 + 1
3 x

dx
=
2
3
 1
0

2x3 + 3
2 x2 + 1
3 x

dx = 2
3
1
2 + 1
2 + 1
6

= 2
3 Â· 7
6 = 7
9.
Then by insertion,
Cov(X, Y ) = E{XY } âˆ’E{X}E{Y } = 7
9 âˆ’11
18 Â· 7
6 = 7
9 Â·

1 âˆ’11
12

=
7
108.
Example 3.6 Consider a 2-dimensional random variable (X, Y ), which in the ï¬rst quadrant has the
frequency
h(x, y) =
a
(1 + x + y)5 ,
while the frequency is equal to 0 anywhere else in the (x, y) plane.
1) Find the constant a.
2) Find the distribution function and the frequency of random variable Z = X + Y .
3) Find the mean E{Z} and the variance V {Z}.
1) When we integrate over the ï¬rst quadrant we obtain
1
=
 âˆ
0
 âˆ
0
h(x, y) dx dy = a
 âˆ
0
 âˆ
0
(1 + x + y)5 dx dy
=
a
 âˆ
0

âˆ’1
4 (1 + x + y)âˆ’4
âˆ
x=0
dy = a
4
 âˆ
0
(1 + y)âˆ’4 dy = a
12,
from which we conclude that a = 12. Hence the frequency is
h(x, y) =
ï£±
ï£´
ï£²
ï£´
ï£³
12
(1 + x + y)5
for x > 0 and y > 0,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
34 
2. Law of the total probability
2) The frequency of Z = X + Y for z > 0 is given by
fZ(z) =
 âˆ
âˆ’âˆ
h(x, z âˆ’x) dx =
 z
0
h(x, z âˆ’x) dx =
 z
0
12
(1 + x + z âˆ’x)5 dx =
12z
(1 + z)5 ,
i.e.
fZ(z) =
ï£±
ï£´
ï£²
ï£´
ï£³
12z
(1 + z)5
for z > 0,
0
otherwise.
The distribution function is FZ(z) = 0 for z â‰¤0.
If z > 0, then
FZ(z)
=
 z
0
fZ(t) dt = 12
 z
0
t + 1 âˆ’1
(t + 1)5 dt =
 z
0

12(t + 1)âˆ’4 âˆ’12(t + 1)âˆ’5
dt
=

âˆ’4(t + 1)âˆ’3 + 3(t + 1)âˆ’4z
0 = 1 âˆ’
4
(z + 1)3 +
3
(z + 1)4 = 1 âˆ’4z + 1
(z + 1)4 .
Summing up we get
FZ(z) =
ï£±
ï£´
ï£²
ï£´
ï£³
1 âˆ’4z + 1
(z + 1)4
for z > 0,
0
for z â‰¤0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables II
 
35 
2. Law of the total probability
3) The mean is
E{Z}
=
 âˆ
0
12z2
(z + 1)5 dz = 12
 âˆ
0
z2 + 2z + 1 âˆ’2z âˆ’2 + 1
(z + 1)5
dz
=
 âˆ
0

12(z + 1)âˆ’3 âˆ’24(z + 1)âˆ’4 + 12(z + 1)âˆ’5
dz
=

âˆ’6(z + 1)âˆ’2 + 8(z + 1)âˆ’3 âˆ’3(z + 1)âˆ’4âˆ
0 = 6 âˆ’8 + 3 = 1.
We get in the same way,
E

Z2
=
 âˆ
0
12z3
(z + 1)5 = 12
 âˆ
0
(z3 + 3z2 + 3z + 1) âˆ’(3z2 + 6z + 3) + (3 + 3z) âˆ’1
(z + 1)5
dz
=
 âˆ
0

12(z + 1)âˆ’2 âˆ’36(z + 1)âˆ’3 + 36(z + 1)âˆ’4 âˆ’12(z + 1)âˆ’5
dz
=

âˆ’12(z + 1)âˆ’1 + 18(z + 1)âˆ’2 âˆ’12(z + 1)âˆ’3 + 3(z + 1)âˆ’4âˆ
0
=
12 âˆ’18 + 12 âˆ’3 = 3.
Then ï¬nally,
V {Z} = E

Z2
âˆ’(E{Z})2 = 3 âˆ’1 = 2.
Example 3.7 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2 x3 eâˆ’x(y+1)
for x > 0 and y > 0,
0
otherwise.
1) Find the frequencies of X and Y .
2) Find Ï±(X, Y ).
1) If x > 0, then
fX(x) = 1
2 x3
 âˆ
0
eâˆ’x(y+1) dy = 1
2 x2 eâˆ’x,
and if y > 0, then
fY (y) = 1
2
 âˆ
0
x3 eâˆ’x(y+1) dx = 1
2 Â·
1
(y + 1)4
 âˆ
0
t3 eâˆ’t dt =
3
(y + 1)4 ,
hence, by summing up,
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2 x2 eâˆ’x
for x > 0,
0
for x â‰¤0,
Download free eBooks at bookboon.com

Random variables II
 
36 
2. Law of the total probability
and
fY (y) =
ï£±
ï£´
ï£²
ï£´
ï£³
3
(y + 1)4
for y > 0,
0
otherwise.
2) Then we get
E{X} = 1
2
 âˆ
0
x3 eâˆ’x dx = 3!
2 = 3,
and
E

X2
= 1
2
 âˆ
0
x4 eâˆ’x dx = 4!
2 = 12,
hence
V {X} = E

X2
âˆ’(E{X})2 = 12 âˆ’32 = 3.
Analogously we obtain
E{Y } = 3
 âˆ
0
y + 1 âˆ’1
(y + 1)4 dy = 3
 âˆ
0

1
(y + 1)3 âˆ’
1
(y + 1)4

dy = 3
1
2 âˆ’1
3

= 1
2,
and
E

Y 2
=
3
 âˆ
0
y2 + 2y + 1 âˆ’2y âˆ’2 + 1
(y + 1)4
dy
=
3
 âˆ
0

1
(y + 1)2 âˆ’
2
(y + 1)3 +
1
(y + 1)4

dy = 3

1 âˆ’1 + 1
3

= 1,
so the variance of Y is
V {Y } = E

Y 2
âˆ’(E{Y })2 = 1 âˆ’1
4 = 3
4.
Finally,
E{XY }
=
 âˆ
0
 âˆ
0
1
2 x4 y eâˆ’x(y+1) dy

dx =
 âˆ
0
1
2 x4eâˆ’x
 âˆ
0
y eâˆ’xy dy

dx
=
 âˆ
0
1
2 x2eâˆ’x dx = 1.
hence
Cov(X, Y ) = E{XY } âˆ’E{X} Â· E{Y } = 1 âˆ’3 Â· 1
2 = âˆ’1
2,
and the correlation coeï¬ƒcient is
Ï±(X, Y ) =
Cov(X, Y )

V {X} Â· V {Y }
=
âˆ’1
2

3 Â· 3
4
= âˆ’1
3.
Download free eBooks at bookboon.com

Random variables II
 
37 
2. Law of the total probability
Example 3.8 Let X1 and X2 be independent, identically distributed random variables of the frequency
f(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
âˆš
2Ï€x exp

âˆ’x
2

,
x > 0,
0,
x â‰¤0.
1) Find the frequency of Y = X1
X2
.
2) Check if E{Y } exists, and if so, ï¬nd E{Y }.
1) Let fY (y) be the frequency of Y = X1
X2
. Then
fY (y) =
 âˆ
âˆ’âˆ
f(yx) f(x) |x| dx.
Clearly, fY (y) = 0 for y â‰¤0.
If y > 0, then
fY (y)
=
 âˆ
0
1
âˆš2Ï€yx exp

âˆ’yx
2

Â·
1
âˆš
2Ï€x exp

âˆ’x
2

|x| dx
=
1
2Ï€âˆšy
 âˆ
0
exp

âˆ’y + 1
2
x

dx =
1
2Ï€âˆšy Â·
2
y + 1 = 1
Ï€ Â·
1
y + 1 Â· 1
âˆšy ,
hence
fY (y) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
1
Ï€ Â·
1
y + 1 Â· 1
âˆšy
for y > 0,
0
for y â‰¤0.
2) Since fY (y) Ì¸= 0 is equivalent to y > 0 and fY (y) > 0, the integrand satisï¬es y fY (y) â‰¥0, hence
the check of the existence is reduced to check the convergence for A â†’âˆof
 A
0
y fY /y) dy
=
1
Ï€
 A
0
y
y + 1 Â· 1
âˆšy dy = 1
Ï€
 A
0
y + 1 âˆ’1
y + 1
Â· 1
âˆšy dy
=
1
Ï€
 A
0
1
âˆšy dy âˆ’1
Ï€
 A
0
1
y + 1 Â· 1
âˆšy dy
=
1
Ï€ [2âˆšy]A
0 âˆ’2
Ï€ [Arctan âˆšy]A
0
=
2
Ï€
âˆš
A âˆ’2
Ï€ Arctan
âˆš
A.
Since âˆ’2
Ï€ Arctan
âˆš
A â†’âˆ’2
Ï€ Â· Ï€
2 = âˆ’1 and 2
Ï€
âˆš
A â†’âˆfor A â†’âˆ, we conclude that E{Y } does
not exist.
Alternatively, it follows that the integrand
y
y + 1 Â· 1
âˆšy âˆ¼
1
âˆšy , and since
 âˆ
0
1
âˆšy dy is divergent,
 âˆ
0
y
y + 1
1
âˆšy dy is also divergent, and the mean E{Y } does not exist. â™¦
Download free eBooks at bookboon.com

Random variables II
 
38 
2. Law of the total probability
Example 3.9 A 2-dimensional random variable (X, Y ) has in the ï¬rst quadrant the frequency
h(x, y) = 1
2 (x + y) eâˆ’(x+y),
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find the frequencies of X and Y .
2) Find frequency of Z = X + Y .
3) Find the mean and the variance of the random variable Z.
4) Find the correlation coeï¬ƒcient Ï±(X, Y ).
1) If x > 0, then
fX(x)
=
1
2
 âˆ
0
(x + y) eâˆ’(x+y) dy = 1
2 x eâˆ’x
 âˆ
0
eâˆ’y dy + 1
2
 âˆ
0
y eâˆ’y dy
=
1
2 x eâˆ’x + 1
2 eâˆ’x = 1
2 (x + 1)eâˆ’x.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables II
 
39 
2. Law of the total probability
By the symmetry,
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2 (x + 1) eâˆ’x,
x > 0,
0,
x â‰¤0,
and
fY (x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2 (y + 1) eâˆ’y,
y > 0,
0,
y â‰¤0,
2) If z > 0, then Z = X + Y has the frequency
fZ(z) =
 z
0
h(x, z âˆ’x) dx =
 z
0
1
2 z eâˆ’z dx = 1
2 z2 eâˆ’z,
and if z â‰¤0, the frequency is 0, thus
fZ(z) =

1
2 z2eâˆ’z
for z > 0,
0
for z â‰¤0.
3) We get
E{Z} =
 âˆ
0
1
2 z3 eâˆ’z dz = 3,
E

Z2
=
 âˆ
0
1
2 z4 eâˆ’z dz = 12,
and
V {Z} = 12 âˆ’32 = 3.
4) First notice that
E{X} = E{Y } = 1
2 (E{X} + E{Y }) = 1
2 E{Z} = 3
2.
Then
E

X2
= E

Y 2
= 1
2
 âˆ
0

t3eâˆ’t + t2eâˆ’t
dt = 1
2 (3! + 2!) = 4,
hence
V {X} = V {Y } = E

X2
âˆ’(E{X})2 = 4 âˆ’9
4 = 7
4.
We ï¬nally compute
E{XY }
=
1
2
 âˆ
0
 âˆ
0
xy(x + y) eâˆ’(x+y) dx dy
=
1
2
 âˆ
0

yeâˆ’y
 âˆ
0
x2 eâˆ’x dx + y2eâˆ’y
 âˆ
0
x eâˆ’x dx

dy
=
1
2
 âˆ
0

2! y eâˆ’y + 1! y2eâˆ’y
dy = 1
2 (2 Â· 1! + 1 Â· 2!) = 2,
Download free eBooks at bookboon.com

Random variables II
 
40 
2. Law of the total probability
thus
Cov(X, Y ) = E{XY } âˆ’E{X}E{Y } = 2 âˆ’3
2 Â· 3
2 = 2 âˆ’9
4 = âˆ’1
4,
and
Ï±(X, Y ) =
Cov(X, Y )

V {X}V {Y }
= âˆ’1
4
7
4
= âˆ’1
7.
Alternatively, it follows from
V {Z} = V {X} + V {Y } + 2 Cov(X, Y ),
that
Cov(X, Y ) = âˆ’1
4,
and hence
Ï±(X, Y ) =
Cov(X, Y )

V {X}V {Y }
= âˆ’1/4
7/4 = âˆ’1
7.
Example 3.10 A compound experiment can be described by ï¬rst choosing at random a real number
X in the interval ]0, 1[, and then at random to choose a real number Y in the interval ]X, 1[. The
frequency of the 2-dimensional random variable (X, Y ) is denoted by h(x, y).
1) Prove that h(x, y) is 0 outside the triangle in the (x, y) plane of the vertices (0, 0), (0, 1) and (1, 1),
and that h(x, y) inside the mentioned triangle above is given by
h(x, y) =
1
1 âˆ’x.
2) Find the frequencies f(x) and g(y) of the random variables X and Y .
3) Find the mean and variance of the random variables X and Y .
1) We see that
fX(x) =
ï£±
ï£²
ï£³
1
for x âˆˆ]0, 1[,
0
otherwise.
If we keep x âˆˆ]0, 1[ ï¬xed, then
f(y | x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
1 âˆ’x
for y âˆˆ]x, 1[,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
41 
2. Law of the total probability
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Hence, if x âˆˆ]0, 1[, then
f(y | x) = h(x, y)
fX(x) = h(x, y),
and we have proved that
h(x, y) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
1 âˆ’x
for 0 < x < y < 1,
0
otherwise.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
2) Obviously,
f(x) = fX(x) =
ï£±
ï£²
ï£³
1
for x âˆˆ]0, 1[,
0
otherwise.
If y âˆˆ]0, 1[, then
g(y) =
 y
0
h(x, y) dx =
 y
0
dx
1 âˆ’x = [âˆ’ln |1 âˆ’x|]y
0 = ln
1
1 âˆ’y ,
Download free eBooks at bookboon.com

Random variables II
 
42 
2. Law of the total probability
hence
g(y) =
ï£±
ï£´
ï£²
ï£´
ï£³
ln
1
1 âˆ’y = âˆ’ln(1 âˆ’y)
for y âˆˆ]0, 1[,
0
otherwise.
3) Clearly,
E{X} = 1
2
and
E

X2
=
 1
0
x2 dx = 1
3,
so
V {X} = E

X2
âˆ’(E{X})2 = 1
3 âˆ’1
4 = 1
12.
One may of course instead notice that X is rectangularly distributed, so
E{X} = 1
2
and
V {X} = 1
12.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
43 
2. Law of the total probability
Then turn to Y . We get by the change of variable t = 1 âˆ’y,
E{Y }
=
 1
0
y {âˆ’ln(1 âˆ’y)} dy = âˆ’
 1
0
(1 âˆ’t) ln t dt = âˆ’
 1
0
ln t dt +
 1
0
t ln t dt
=
âˆ’[t ln t âˆ’t]1
0 +
t2
2 ln t
1
0
âˆ’1
2
 1
0
t dt = âˆ’{0 âˆ’1} + 0 âˆ’1
2 Â· 1
2 = 3
4,
and
E

Y 2
=
 1
0
y2{âˆ’ln(1 âˆ’y)} dy = âˆ’
 1
0
(1 âˆ’t)2 ln t dt
=
âˆ’
 1
0
ln t dt + 2
 1
0
t ln t dt âˆ’
 1
0
t2 ln t dt
=
âˆ’[t ln t âˆ’t]1
0 + 2
t2
2 ln t
1
0
âˆ’
 1
0
t dt âˆ’
t3
3 ln t
1
0
+ 1
3
 1
0
t2 dt
=
1 + 2 Â· 0 âˆ’1
2 âˆ’0 + 1
9 = 1
2 + 1
9 = 11
18.
Alternatively, perform the computations
E{Y } =
 1
x=0
 1
y=x
y Â·
1
1 âˆ’x dy

dx = 1
2
 1
0
1 âˆ’x2
1 âˆ’x dx =
 1
0
1
2 (1 + x) dx = 3
4,
and
E

Y 2
=
 1
x=0
 1
y=x
y2 Â·
1
1 âˆ’x dy

dx = 1
3
 1
0
1 âˆ’x3
1 âˆ’x dx
=
1
2
 1
0

1 + x + x2
dx = 11
18.
This gives us the variance,
V {Y }
=
E

Y 2
âˆ’(E{Y })2 = 11
18 âˆ’9
16
=
1
2
11
9 âˆ’9
8

= 1
2
2
9 âˆ’1
8

= 16 âˆ’9
144
=
7
144.
Download free eBooks at bookboon.com

Random variables II
 
44 
2. Law of the total probability
Example 3.11 The point A is in the (x, y) plane given by its polar coordinates r = OA = 1 and
âˆ (x, OA) = Î˜. The projections of A onto the two coordinate axes are called X and Y .
r=1
theta
A
X
Y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
We assume that Î˜ is a rectangularly distributed random variable over the interval

âˆ’Ï€
2 , Ï€
2

.
1) Find the distribution functions and the frequencies of the two random variables X and Y .
2) Find the means E{X} and E{Y }.
3) Find the variances V {X} and V {Y }.
4) Explain that the random variables X and Y are non-correlated, though not independent of each
other.
The frequency of Î˜ is
f(Î¸) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
Ï€
for x âˆˆ

âˆ’Ï€
2 , Ï€
2

,
0
otherwise.
Furthermore, X = cos Î˜ and Y = sin Î˜.
1) Since cos Î¸ > 0 for Î¸ âˆˆ

âˆ’Ï€
2 , Ï€
2

, where cos Î¸ is not monotonous, we get for x âˆˆ]0, 1[,
FX(x)
=
P{X â‰¤x} = P{cos Î¸ â‰¤x} = P{Arccos x â‰¤Î¸ â‰¤Ï€ âˆ’Arccos x}
=
1
Ï€
 Ï€âˆ’Arccos x
Arccos x
dÎ¸ = 1 âˆ’2
Ï€ Arccos x,
hence
FX(x) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
1,
x â‰¥1,
1 âˆ’2
Ï€ Arccos x,
0 < x < 1,
0,
x â‰¤0,
Download free eBooks at bookboon.com

Random variables II
 
45 
2. Law of the total probability
and
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
2
Ï€
1
âˆš
1 âˆ’x2 ,
x âˆˆ]0, 1[,
0,
otherwise.
Analogously, we get for y âˆˆ] âˆ’1, 1[,
FY (y)
=
P{Y â‰¤y} = P{sin Î¸ â‰¤y} = P{Î¸ â‰¤Arcsin y}
=
1
Ï€

Arcsin y
âˆ’Ï€
2
dÎ¸ = 1
2 + 1
Ï€ Arcsin y,
hence
FY (y) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
1,
y â‰¥1,
1
2 + 1
Ï€ Arcsin y,
âˆ’1 < y < 1,
0,
y â‰¤âˆ’1,
and
fY (y) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
1
Ï€
1

1 âˆ’y2 ,
y âˆˆ] âˆ’1, 1[,
0,
otherwise.
2) The means are
E{X} = 2
Ï€
 1
0
x
âˆš
1 âˆ’x2 dx = 2
Ï€

âˆ’

1 âˆ’x2
1
0 = 2
Ï€ ,
and
E{Y } = 1
Ï€ = 1
Ï€
 1
âˆ’1
y

1 âˆ’y2 dy = 0.
3) We get by the substitution x = sin t,
E

X2
=
2
Ï€
 1
0
x2
âˆš
1 âˆ’x2 dx = 2
Ï€

Ï€
2
0
sin2 t

1 âˆ’sin2 t
Â· cos t dt
=
1
Ï€

Ï€
2
0

sin2 t + cos2 t

dt = 1
2.
Furthermore,
E

Y 2
= 1
Ï€
 1
âˆ’1
y2

1 âˆ’y2 dy = 2
Ï€
 1
0
y2

1 âˆ’y2 dy = E

X2
= 1
2.
Download free eBooks at bookboon.com

Random variables II
 
46 
2. Law of the total probability
The variances are
V {X} = E

X2
âˆ’(E{X})2 = 1
2 âˆ’
 2
Ï€
2
= 1
2 âˆ’4
Ï€2
(â‰ˆ0, 095),
and
V {Y } = E

Y 2
âˆ’(E{Y })2 = 1
2.
4) Since X2 + Y 2 = 1, it is obvious that X and Y are not independent.
Let f(x, y) be the frequency of Z = (X, Y ). Then
f(x, y) = f(x | y) Â· fY (y) = f(x | y) Â· 1
Ï€ Â·
1

1 âˆ’y2
for y âˆˆ] âˆ’1, 1[,
where
f(x | y) =
ï£±
ï£²
ï£³
1
for x =

1 âˆ’y2,
0
otherwise.
Then
E{XY } =
 1
âˆ’1

1 âˆ’y2 Â· y Â· 1
Ï€ Â·
1

1 âˆ’y2 dy = 1
Ï€
 1
âˆ’1
y dy = 0,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables II
 
47 
2. Law of the total probability
Thus
Cov(X, Y ) = E{XY } âˆ’E{X} Â· E{Y } = 0 âˆ’2
Ï€ Â· 0 = 0,
so X and Y are non-correlated.
Example 3.12 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =
ï£±
ï£²
ï£³
2a2 eâˆ’a(x+y),
0 < x < y,
0,
otherwise,
where a is a positive constant.
1) Find the frequencies of the random variables X and Y .
2) Find the means E{X} and E{Y }.
3) Find Cov(X, Y ).
4) Find the frequency of Z = X + Y .
5) Find the mean E{Z} and the variance V {Z}.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1) When x > 0, we get by a vertical integration,
fX(x) =
 âˆ
x
2a2 eâˆ’a(x+y) dy = 2a eâˆ’ax 
âˆ’eâˆ’ayâˆ
x = 2a eâˆ’2ax,
hence
fX(x) =
ï£±
ï£²
ï£³
2a eâˆ’2ax
for x > 0,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
48 
2. Law of the total probability
When y > 0, we get by a horizontal integration,
fY (y) =
 y
0
2a2 eâˆ’a(x+y) dx = 2a eâˆ’ay 
âˆ’eâˆ’axy
0 = 2a eâˆ’ay âˆ’2a eâˆ’2ay,
hence
fY (y) =
ï£±
ï£²
ï£³
2a eâˆ’ay âˆ’2a eâˆ’2ay
for y > 0,
0
otherwise.
2) The means are given by
E{X} =
 âˆ
0
2a x eâˆ’2ax dx = 1
2a
 âˆ
0
t eâˆ’t dt = 1
2a,
and
E{Y }
=
 âˆ
0
2a y eâˆ’ay dy âˆ’
 âˆ
0
2a y eâˆ’2ay dy = 2
a
 âˆ
0
t eâˆ’t dt âˆ’1
2a
 âˆ
0
t eâˆ’t dt
=
2
a âˆ’1
2a = 3
2a.
3) Then we compute
E{XY }
=
 âˆ
0
 y
0
xy Â· 2a2eâˆ’a(x+y) dx

dy =
 âˆ
0
2y eâˆ’ay
 y
0
a x eâˆ’ax a dx

dy
=
 âˆ
0
2y eâˆ’ay
 ay
0
t eâˆ’t dt

dt =
 âˆ
0
2y eâˆ’ay 
âˆ’t eâˆ’t âˆ’eâˆ’tay
0 dy
=
 âˆ
0
2y eâˆ’ay 
1 âˆ’ay eâˆ’ay âˆ’eâˆ’ay
dy
=
 âˆ
0
2y eâˆ’ay dy âˆ’
 âˆ
0
2a y2 eâˆ’2ay dy âˆ’
 âˆ
0
2y eâˆ’2ay dy
=
2
a2
 âˆ
0
t eâˆ’t dt âˆ’
1
4a2
 âˆ
0
t2eâˆ’t dt âˆ’
1
2a2
 âˆ
0
t eâˆ’t dt
=
2
a2 âˆ’
1
2a2 âˆ’
1
2a2 = 1
a2 .
It follows that
Cov(X, Y ) = E{XY } âˆ’E{X} Â· E{Y } = 1
a2 âˆ’1
2a Â· 3
2a =
1
4a2 .
4) Clearly, fZ(z) = 0 for z â‰¤0. NËšar z > 0, so
fZ(z) =
 âˆ
âˆ’âˆ
h(x, z âˆ’x) dx =
 âˆ
0
h(x, z âˆ’x) dx.
The integrand is only Ì¸= 0, if x < y = z âˆ’x, i.e. when x < 1
2 z, hence
fZ(z) =

z
2
0
g(x, z âˆ’x) dx = 2a2eâˆ’az

z
2
0
dz = a2z eâˆ’az,
Download free eBooks at bookboon.com

Random variables II
 
49 
2. Law of the total probability
and thus
fZ(z) =
ï£±
ï£²
ï£³
a2z eâˆ’az
for z > 0,
0
otherwise.
5) The mean is
E{Z})E{X} + E{Y } = 1
2a + 3
2a = 2
a,
or alternatively and more elaborated,
E{Z} =
 âˆ
0
a2z2eâˆ’az dz = 1
a
 âˆ
0
t2eâˆ’t dt = 2
a.
Furthermore,
E

Z2
=
 âˆ
0
a2z3eâˆ’az dz = 1
a2
 âˆ
0
t3eâˆ’t dt = 6
a2 ,
hence
V {Z} = E

Z2
âˆ’(E{Z})2 = 6
a2 âˆ’4a2 = 2
a2 .
Example 3.13 A 2-dimensional random variable (X, Y ) has the frequency h(x, y) = 1 inside the
triangle in the (x, y) plane of vertices at the points (0, 0), (0, 2) and (1, 1), while the frequency is 0
anywhere else outside this triangle.
1) Find the frequencies of the random variables X and Y .
2) Prove that X and Y are non-correlated, though not independent.
3) Find the distribution function and the frequency for each of the random variables Z = X + Y and
V = X âˆ’Y .
1) If x âˆˆ]0, 1[, then
fX(x) =
 2âˆ’x
x
dy = 2 âˆ’2x,
hence
fX(x) =
ï£±
ï£²
ï£³
2 âˆ’2x
for x âˆˆ]0, 1[,
0
otherwise.
If y âˆˆ]0, 1], then
fY (y) =
 y
0
dx = y.
Download free eBooks at bookboon.com

Random variables II
 
50 
2. Law of the total probability
0
0.5
1
1.5
2
0.2
0.4
0.6
0.8
1
If y âˆˆ]1, 2[, then
fY (y) =
 2âˆ’y
0
dx = 2 âˆ’y.
Summing up,
fY (y) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
y
for y âˆˆ]0, 1],
2 âˆ’y
for y âˆˆ]1, 2[,
0
otherwise.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables II
 
51 
2. Law of the total probability
2) It follows by considering a ï¬gure that E{Y } = 1. Furthermore,
E{X} =
 1
0

2x âˆ’2x2
dx = 1 âˆ’2
3 = 1
3.
Then by a double integration, where we start in the inner integral to integrate vertically after y),
E{XY }
=
 1
0
 2âˆ’x
x
xy dy

dx =
 1
0
x
y2
2
2âˆ’x
x
dx
=
1
2
 1
0
x(4 âˆ’4x) dx =
 1
0

2x âˆ’2x2
dx = 1
3.
Since
Cov(X, Y ) = E{XY } âˆ’E{X}E{Y } = 1
3 âˆ’1
3 Â· 1 = 0,
it follows that X and Y are non-correlated.
Since fX(x) Â· fY (y) Ì¸= 0 in the square ]0, 1[ Ã— ]0, 2[, we see that fX(x) Â· fY (y) cannot be equal to
h(x, y). [This can of course also be seen directly.] Hence, X and Y are not independent.
3) The frequency of Z = X + Y is
fZ(z) =
 1
0
h(x, z âˆ’x) dx.
The integrand is Ì¸= 0, when y = z âˆ’x âˆˆ]x, 2 âˆ’x[, e.g. 2x < z < 2, hence
fZ(z) =

z
2
0
h(x, z âˆ’x) dx =

z
2
0
dx = z
2,
and we ï¬nd the frequency
fZ(z) =
ï£±
ï£´
ï£²
ï£´
ï£³
z
2
for z âˆˆ]0, 2[,
0
otherwise,
and the distribution function
FZ(z) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
0
for z â‰¤0,
z2
4
for z âˆˆ]0, 2[,
1
for z â‰¥2.
Then we note that X = X âˆ’Y has values in ] âˆ’2, 0[. If v âˆˆ] âˆ’2, 0[, then
FV (v) = P{X âˆ’Y â‰¤v} =

{xâˆ’yâ‰¤v}
h(x, y) dx dy =
 2
0
 v+y
0
h(x, y) dx

dy.
We get by a diï¬€erentiation,
fV (v) = F â€²
V (v) =
 2
0
h(v + y, y) dy.
Download free eBooks at bookboon.com

Random variables II
 
52 
2. Law of the total probability
The integrand is Ì¸= 0 for
0 < v + y < 1
and
v + y < y < 2 âˆ’v âˆ’y,
hence
0 < âˆ’v < y < 1 âˆ’v
2 < 2.
If v âˆˆ] âˆ’2, 0[, then
fV (v) =
 1âˆ’v
2
âˆ’v
dy = 1 âˆ’v
2 + v = 1 + v
2,
thus the frequency of V is
fV (v) =
ï£±
ï£´
ï£²
ï£´
ï£³
1 + v
2
for v âˆˆ] âˆ’2, 0[,
0
otherwise,
and the corresponding distribution function is
FV (v) =
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
0,
for v â‰¤âˆ’2,

1 + v
2
2
,
for v âˆˆ] âˆ’2, 0[,
1,
for v â‰¥0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
53 
2. Law of the total probability
Example 3.14 Given the functions
f(x) =
ï£±
ï£²
ï£³
12x2(1 âˆ’x),
0 < x < 1,
0,
otherwise,
g(y) =
ï£±
ï£²
ï£³
12y(1 âˆ’y)2,
0 < y < 1,
0,
otherwise.
1. Prove that f and g are frequencies.
In the remaining part of the example we let X and Y denote random variables, where X has the
frequency f(x), and Y has the frequency g(y).
2. Find the mean and variance of X.
3. Prove that Y has the same distribution as 1 âˆ’X.
4. Find the mean and the variance of Y .
5. Prove that X + Y and X âˆ’Y are non-correlated.
6. We now assume that X and Y are independent. Explain why the two probabilities
P

X + Y > 1
2

and
P

X âˆ’Y > 1
2

are positive (one shall not compute the probabilities). Check, e.g. by applying this result, if X + Y
and X âˆ’Y are independent.
7. Here we assume that Cov(X, Y ) = âˆ’1
25. Prove that Y is then a function of X, and ï¬nd this
function.
Hint: Compute e.g. the variance of X + Y .
1) It is obvious that f(x) â‰¥0 for every x âˆˆR. Since furthermore
 1
0
12x2(1 âˆ’x) dx = 12
 1
0

x2 âˆ’x3
dx = 12
1
3 âˆ’1
4

= 1,
it follows that f(x) is a frequency.
Since g(y) = f(1 âˆ’y) and

dx
dy
 = | âˆ’1| = 1, it follows that g(y) is also a frequency.
2) The mean of X is
E{X} = 12
 1
0

x3 âˆ’x4
dx = 12
1
4 âˆ’1
5

= 12
20 = 3
5.
Since furthermore,
E

X2
= 12
 1
0

x4 âˆ’x5
dx = 12
1
5 âˆ’1
6

= 12
30 = 2
5,
the variance is
V {X} = 2
5 âˆ’
3
5
2
= 10 âˆ’9
25
= 1
25.
Download free eBooks at bookboon.com

Random variables II
 
54 
2. Law of the total probability
3) The frequency of Ï•(X) = 1 âˆ’X is
f(1 âˆ’x) Â·

d(1 âˆ’x)
dx
 = f(1 âˆ’x) =
ï£±
ï£²
ï£³
12x(1 âˆ’x)2
for 0 < x < 1,
0,
otherwise.
This is precisely the structure of the frequency of Y , with x instead of y, thus Y and 1 âˆ’X have
the same distribution.
4) It follows from (3) that
E{Y } = E{1 âˆ’X} = 1 âˆ’E{X} = 1 âˆ’3
5 = 2
5
and
V {Y } = V {1 âˆ’X} = V {1} + V {X} = 0 + 1
25 = 1
25 = V {X}.
5) It follows from the deï¬nition,
Cov(X + Y, X âˆ’Y ) = V {X} âˆ’V {Y } + Cov(Y, X) âˆ’Cov(X, Y ) = 1
25 âˆ’1
25 = 0,
hence X + Y and X âˆ’Y are non-correlated.
6) It is obvious that X and Y both have their values in ]0, 1[ with a positive probability for every
open, non-empty subinterval of ]0, 1[. Then both

X + Y > 3
2

and

X âˆ’Y > 1
2

have a positive probability. Since 2X = (X + Y ) + (X âˆ’Y ), we get
{X > 1} = {2X > 2} â«†

X + Y > 3
2

âˆ©

X âˆ’Y > 1
2

.
Since
P{X > 1} = 0,
P

X + Y > 3
2

> 0,
P

X âˆ’Y > 1
2

> 0,
we get
0
=
P{X > 1} = P

X + Y > 3
2

âˆ©

X âˆ’Y > 1
2

Ì¸=
P

X + Y > 3
2

Â· P

X âˆ’Y > 1
2

,
proving that X + Y and X âˆ’Y are not independent.
7) Since
V {X + Y } = V {X} + V {Y } âˆ’2 Cov(X, Y ) = 1
25 + 1
25 âˆ’2
25 = 0,
it follows that X + Y is causal, so X + Y = X + (1 âˆ’X) = 1 = a with the only possibility
Y = 1 âˆ’X.
Download free eBooks at bookboon.com

Random variables II
 
55 
2. Law of the total probability
Example 3.15 A rectangular triangle has the two smaller sides X1 and X2, where X1 and X2 are
independent random variables of the frequencies
fX1 (x1) =
ï£±
ï£²
ï£³
1,
0 < x1 < 1,
0,
otherwise,
fX2 (x2) =
ï£±
ï£²
ï£³
1
2,
0 < x2 < 2,
0,
otherwise.
Let Y1 = X1 + X2 denote the sum of the lengths of the two smaller sides and let Y2 = 1
2 X1X2 denote
the area of the triangle.
1) Compute the mean and the variance of Y1.
2) Compute the mean and variance of Y2.
3) Prove that
Cov (X1 + X2, X1X2) = E {X1} V {X2} + E {X2} C {X1} ,
and then compute Cov (Y1, Y2).
4) Find the frequency of Y1.
1) The mean of Y1 = X1 + X2 is
E {Y1} = E {X1} + E {X2} 1
2 + 1 = 3
2.
Since X1 and X2 are independent, the variance is
V {Y1} = V {X1} + V {X2} = 1
12

12 + 22
= 5
12.
2) Since X1 and X2 are independent, we ï¬nd that
E {Y2} = 1
2 E {X1} Â· E {X2} = 1
2 Â· 1
2 Â· 1 = 1
4,
and
V {Y2}
=
1
4 V {X1X2} = 1
4

E

X2
1X2
2

âˆ’(E {X1} E {X2})2
=
1
4

E

X2
1

E

X2
2

âˆ’(E {X1} E {X2})2
= 1
4

1
3 Â· 4
3 âˆ’
1
2
2
= 1
4
4
9 âˆ’1
4

=
7
144.
3) By a direct computation,
Cov (X1 + X2, X1X2) = E {(X1 + X2 âˆ’E {X1} âˆ’E {X2}) (X1X2 âˆ’E {X1} Â· E {X2})}
=
E {(X1 âˆ’E {X1}) (X1 âˆ’E {X1}) X2} + E {(X1 âˆ’E {X1}) X2} Â· E {X1}
+E {(X2 âˆ’E {X2}) (X2 âˆ’E {X2}) X1} + E {X1 (X2 âˆ’E {X2})} Â· E {X2}
âˆ’E {X1} E {X2} Â· E {(X1 âˆ’E {X1} + X2 âˆ’E {X2})}
=
V {X1} E {X2} + 0 + V {X2} E {X1} + 0 + 0
=
E {X1} V {X2} + E {X2} V {X1} .
Download free eBooks at bookboon.com

Random variables II
 
56 
2. Law of the total probability
Then
Cov (Y1, Y2)
=
1
2 Cov (X1 + X2, X1X2) = 1
2 (E {X1} V {X2} + E {X2} V {X1})
=
1
2
1
2 Â· 1
3 + 1 Â· 1
12

= 1
2
1
6 + 1
12

= 1
8.
4) Since X1 takes its values in ]0, 1[, and X2 takes its values in ]0, 2[, the sum Y1 = X1 + X2 will take
its values in ]0, 3[. If y âˆˆ]0, 3[, then the frequency of Y is given by
fY (y) =
 y
0
fX1(x)fX2(y âˆ’x) dx.
Then we must split the investigation according to the diï¬€erent subintervals.
a) If y âˆˆ]0, 1], then
fY (y) =
 y
0
1 Â· 1
2 dx = y
2.
b) If y âˆˆ]1, 2], then
fY (y) =
 1
0
1 Â· 1
2 dx = 1
2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables II
 
57 
2. Law of the total probability
c) If y âˆˆ]2, 3], then
fY (y) =
 1
0
1 Â· fX2(y âˆ’x) dx =
 1
yâˆ’2
1 Â· 1
2 dx = 1
2 (3 âˆ’y).
Summing up,
fY (y) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
y
2,
for y âˆˆ]0, 1],
1
2,
for y âˆˆ]1, 2],
1
2 (3 âˆ’y),
for y âˆˆ]2, 3[,
0,
otherwise.
Example 3.16 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =
ï£±
ï£²
ï£³
x + y,
0 < x < 1,
0 < y < 1,
0,
otherwise.
1. Find the marginal frequencies of X and Y .
2. Find the means of X and Y .
3. Find the variances of X and Y .
4. Compute the covariance between X and Y , and the correlation coeï¬ƒcient between X and Y .
Let the random variables U and V be given by
U = max{X, Y }
and
V = min{X, Y }.
5. Compute the probability P

U â‰¤1
2

and the probability P

V â‰¤1
2

.
1) Due to the symmetry, X and Y have the same marginal frequency. If x âˆˆ[0, 1], then
f(x) =
 1
0
(x + y) dy =
(x + y)2
2
1
y=0
= 1
2

(x + 1)2 âˆ’x2
= x + 1
2,
hence
f(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
x + 1
2
for x âˆˆ[0, 1],
0
otherwise,
and
g(y) =
ï£±
ï£´
ï£²
ï£´
ï£³
y + 1
2
for y âˆˆ[0, 1],
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
58 
2. Law of the total probability
2) It also follows from the symmetry that
E{X} = E{Y } =
 1
0
x

x + 1
2

dx =
 1
0

x2 + x
2

dx =
x3
3 + x2
4
1
0
= 7
12.
3) For the same reason,
E

X2
= E

Y 2
=
 1
0
x2

x + 1
2

dx =
 1
0

x3 + x2
2

dx =
x4
4 + x3
6
1
0
= 1
4 + 1
6 = 5
12.
Hence
V {X} = V {Y } = E

X2
âˆ’(E{X})2 = 5
12 âˆ’
 7
12
2
= 60 âˆ’49
144
= 11
144.
4) According to a formula, the covariance is
Cov(X, Y )
=
E{XY } âˆ’E{X} Â· E{Y } =
 1
0
 1
0
xy(x + y) dy

dx âˆ’7
12 Â· 7
12
=
 1
0
x
 1
0

yx + y2
dy

dx âˆ’49
144 =
 1
0
x
1
2 x + 1
3

dx âˆ’49
144
=
 1
0
x2
2 + x
3

dx âˆ’49
144 = 1
6 + 1
6 âˆ’49
144 = 1
3 âˆ’49
144 = âˆ’1
144.
Then we get the correlation coeï¬ƒcient
Ï±(X, Y ) =
Cov(X, Y )

V {X} Â· V {Y }
=
âˆ’1
144
11
144
= âˆ’1
11.
5) If U = max{X, Y }, then
P

U â‰¤1
2

=
P

X â‰¤1
2 âˆ§Y â‰¤1
2

=

1
2
0

1
2
0
(x + y) dy

dx
=

1
2
0
1
2

(x + y)2 1
2
y=0 dx = 1
2

1
2
0

x + 1
2
2
âˆ’x2

dx
=
1
6

x + 1
2
3
âˆ’x3
 1
2
0
= 1
6

13 âˆ’
1
2
3
âˆ’
1
2
3
+ 03

=
1
6

1 âˆ’1
4

= 1
6 Â· 3
4 = 1
8.
Download free eBooks at bookboon.com

Random variables II
 
59 
2. Law of the total probability
If V = min{X, Y }, we get by using the complementary probability that
P

V â‰¤1
2

=
1 âˆ’P

V > 1
2

= 1 âˆ’
 1
1
2
 1
1
2
(x + y) dy

dx = 1 âˆ’1
2
 1
1
2

(x + y)21
y= 1
2 dx
=
1 âˆ’1
2
 1
1
2

(x + 1)2 âˆ’

x + 1
2
2
dx = 1 âˆ’1
6

(x + 1)3 âˆ’

x + 1
2
31
1
2
=
1 âˆ’1
6

23 âˆ’
3
2
3
âˆ’
3
2
3
+ 13

= 1 âˆ’1
6

8 âˆ’27
4 + 1

=
1 âˆ’1
6
36 âˆ’27
4

= 1 âˆ’1
6 Â· 9
4 = 1 âˆ’3
8 = 5
8.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables II
 
60 
4. Examples concerning the Poisson distribution
4
Examples concerning the Poisson distribution
Example 4.1 Let X and Y be independent random variables, and let X have the frequency f(x), and
let Y have the frequency g(y).
1. Prove that the frequency of the random variable U = X âˆ’Y is given by
k(u) =
 âˆ
âˆ’âˆ
f(x)g(x âˆ’u)dx,
u âˆˆR.
In the remaining of the example we assume that
f(x) =
ï£±
ï£²
ï£³
Î» eâˆ’Î»x,
x > 0,
0,
x â‰¤0,
g(y) =
ï£±
ï£²
ï£³
Âµ eâˆ’Âµy,
y > 0,
0,
y â‰¤0,
where Î» and Âµ are positive constants.
2. Find the frequency of the random variable U.
3. Find the mean E{U} and the variance V {U}.
4. Compute the correlation coeï¬ƒcient Ï±(U, X).
1) Let K(u) be the distribution function of U. Then
K(u) = P{X âˆ’Y â‰¤u} =

{xâˆ’yâ‰¤u}
f(x)g(y) dx dy =
 âˆ
âˆ’âˆ
 u+y
âˆ’âˆ
f(x)g(y) dx

dy.
By diï¬€erentiation, followed by the change of variable x = u + y,
k(u) =
 âˆ
âˆ’âˆ
f(u + y)g(y) dy =
 âˆ
âˆ’âˆ
g(x)g(x âˆ’u) dx,
u âˆˆR.
2) It follows from
k(u) =
 âˆ
âˆ’âˆ
g(x)g(x âˆ’u) dx =
 âˆ
0
f(x)g(x âˆ’u) dx
that if u > 0 then the integrand is only Ì¸= 0 for x > u, thus
k(u)
=
 âˆ
u
Î» eâˆ’Î»x Â· Âµ Â· eâˆ’Âµ(xâˆ’u) dx = Î» Âµ eÂµu
 âˆ
u
eâˆ’(Î»+Âµ)x dx
=
Î» Âµ
Î» + Âµ eÂµu Â· eâˆ’(Î»+Âµ)u =
Î» Âµ
Î» + Âµ eâˆ’Î» u.
If instead u â‰¤0, then
k(u) =
 âˆ
0
Î» eâˆ’Î»x Â· Âµ eâˆ’Âµ(xâˆ’u) dx = Î» Âµ eÂµu
 âˆ
0
eâˆ’(Î»+Âµ)x dx =
Î» Âµ
Î» + Âµ eÂµ u.
Download free eBooks at bookboon.com

Random variables II
 
61 
4. Examples concerning the Poisson distribution
Summing up,
k(u) =
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
Î» Âµ
Î» + Âµ eâˆ’Î» u
for u > 0,
Î» Âµ
Î» + Âµ eÂµ u
for u â‰¤0.
3) The mean is
E{U} = E{X} âˆ’E{Y } = 1
Î» âˆ’1
Âµ = Âµ âˆ’Î»
Î» Âµ .
Furthermore,
E

U 2
=
Âµ
Î» + Âµ
 âˆ
0
Î» u2 eâˆ’Î» u du +
Î»
Î» + Âµ
 0
âˆ’âˆ
Âµ u2 eÂµ u du
=
1
Î» + Âµ Â· Âµ
Î»2
 âˆ
0
t2 eâˆ’t dt +
1
Î» + Âµ Â· Î»
Âµ2
 âˆ
0
t2 eâˆ’t dt
=
2
Î» + Âµ
 Î»
Âµ2 + Âµ
Î»2

=
2
Î» + Âµ Â· Î»3 + Âµ3
Î»2Âµ2
= 2 Â· Î»2 âˆ’Î» Âµ + Âµ2
Î»2Âµ2
.
The variance is
V {U}
=
E

U 2
âˆ’(E{U})2 = 2 Â· Î»2 âˆ’Î» Âµ + Âµ2
Î»2Âµ2
âˆ’Î»2 âˆ’2Î» Âµ + Âµ2
Î»2Âµ2
=
Î»2 + Âµ2
Î»2Âµ2
= 1
Î»2 + 1
Âµ2 .
4) It is well-known that
E{X} = 1
Î»
and
V {X} = 1
Î»2 .
Since X and Y are stochastically independent, we have
E{XY } = E{X}E{Y }.
By the rules of computation,
Cov(U, X) = Cov(X âˆ’Y, X) = Cov(X, X) âˆ’Cov(Y, X) = V {X} = 1
Î»2 ,
hence
Ï±(U, X) =
Cov(U, X)

V {U}V {X}
= 1
Î»2 Â·
1

Î»2 + Âµ2
Î»2Âµ2
Â· 1
Î»2
=
Âµ

Î»2 + Âµ2 .
Download free eBooks at bookboon.com

Random variables II
 
62 
4. Examples concerning the Poisson distribution
Example 4.2 A radioactive material emits both Î± and Î² particles, where these two types of particles
are emitted independently of each other. We shall study this emission from (and included) the time
t = 0.
Let X1, X1 + X2, X1 + X2 + X3, . . . , indicate the times of the emission of the ï¬rst, second, third,
. . . , Î± particle.
We assume that the random variables Xi, i = 1, 2, . . . , are mutually independent of the frequency
f(x) =
ï£±
ï£²
ï£³
Î» eâˆ’Î» x,
x â‰¥0,
0,
x < 0,
Î» > 0.
Analogously, Y1, Y1 + Y2, Y1 + Y2 + Y3, . . . , indicates the times of the emission of the ï¬rst, second,
third, . . . , Î² particle.
We assume that the random variables Yi, i = 1, 2, . . . , also are mutually independent, and then by
the assumption independent of the Xi of the frequency
g(y) =
ï£±
ï£²
ï£³
Âµ eâˆ’Âµ y,
Y â‰¥0,
0,
y < 0,
Âµ > 0.
1) Find the frequency of X1 + X2.
2) Find the probability that there is emitted at least two Î± particles before one Î² particle is emitted.
For which value of Î»
Âµ is this probability equal to 1
2?
1) When x > 0, then the frequency of X1 + X2 is given by the convolution integral
f2(x) =
 x
0
f(x)f(x âˆ’t) dt =
 x
0
Î» eâˆ’t Î» Â· Î» eâˆ’(xâˆ’t)Î» dt = Î»2 x eâˆ’Î» x,
and f2(x) = 0 otherwise.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
2) We shall ï¬nd P {X1 + X2 < Y1}.
Download free eBooks at bookboon.com

Random variables II
 
63 
4. Examples concerning the Poisson distribution
First method. The simultaneous frequency of (X1 + X2, Y1) is f2(x)g(y), hence
P {X1 + X2 < Y1} =

{x<y}
f2(x)g(y) dx dy
=
 âˆ
x=0
f2(x)
 âˆ
y=x
g(y) dy

dx =
 âˆ
0
Î»2 x eâˆ’Î» x
 âˆ
y=x
Âµ eâˆ’Âµ y dy

dx
=
 âˆ
0
Î»2 x eâˆ’(Î»+Âµ)x dx =
Î»2
(Î» + Âµ)2
 âˆ
0
t eâˆ’t dt =

Î»
2
Î»
Âµ + 1

,
where we have applied the substitution t = (Î» + Âµ)x.
Remark 4.1 Here it is diï¬ƒcult to compute the double integral in the order
 âˆ
y=0
 y
x=0 Â· Â· Â· , so
we omit this variant. â™¦
Second method. (More diï¬ƒcult.) The frequency of Z =
Y1
X1 + X2
is computed according to
some formula. If z > 0, then
k(z)
=
 âˆ
0
g(zx)f2(x) x dx =
 âˆ
0
Âµ eâˆ’Âµ z xÎ»2 x eâˆ’Î» xx dx
=
ÂµÎ»2
 âˆ
0
x2eâˆ’(Î»+Âµ z)x dx =
2Î»2Âµ
(Î» + Âµ z)3 ,
hence
P {X1 + X2 < Y1}
=
P{Z > 1} =
 âˆ
1
k(z) dz
=
2Î»2Âµ

1âˆ(Î» + Âµ z)âˆ’3 dz =
Î»2
(Î» + Âµ)2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
64 
4. Examples concerning the Poisson distribution
Third method. (Sketch). Find the frequency of X1 + X2
Y1
, cf. the second method.
Fourth method. (Even more diï¬ƒcult; only a sketch). Find the frequency of
U = (X1 + X2) âˆ’Y1.
Then
P {X1 + X2 < Y1} = P{U < 0} =
 0
âˆ’âˆ
fU(u) du.
The probability is 1
2, when
Î»
Âµ
Î»
Âµ + 1 =
1
âˆš
2,
and we get
Î»
Âµ =
âˆš
2 + 1.
Example 4.3 . (Continuation of Example 4.2).
1) Find the probability that there is emitted at least three Î± particles, before the ï¬rst Î² particle is
emitted.
2) Find the probability that there is emitted precisely two Î± particles, before the ï¬rst Î² particle is
emitted.
3) Find the probability Pn(t) that there in the time interval ]0, 1[ is emitted a total of n particles.
1) It follows from Example 4.2 that X1 + X2 has the frequency
f2(x) =
ï£±
ï£²
ï£³
Î»2 x eâˆ’Î»x
for x â‰¥0,
0
for x < 0.
Then X3 has the frequency
f(x) =
ï£±
ï£²
ï£³
Î» eâˆ’Î»x
for x â‰¥0,
0
for x < 0,
so the frequency f3(s) of X1 + X2 + X3 is zero for s â‰¤0. If s > 0, then
f3(s) =
 s
0
Î»2x eâˆ’Î»x Â· Î» eâˆ’Î»(sâˆ’x) dx = Î»3eâˆ’Î»s
 s
0
x dx = 1
2 Î»3s2eâˆ’Î»s.
Download free eBooks at bookboon.com

Random variables II
 
65 
4. Examples concerning the Poisson distribution
Then (cf. Example 4.2)
P {X1 + X2 + X3 < Y1} =
 0
âˆ’âˆ
 âˆ
0
f3(x)g(x âˆ’s) dx

ds
=
 0
âˆ’âˆ
 âˆ
0
1
2 Î»3x2eâˆ’Î»x Â· Âµ eâˆ’Âµ(xâˆ’s)dx

ds
= 1
2 Î»3Âµ
 0
âˆ’âˆ
eâˆ’Âµsds Â·
 âˆ
0
x2eâˆ’(Î»+Âµ)x dx
= 1
2 Î»3 Â· 1 Â·
1
(Î» + Âµ)3
 âˆ
0
t2eâˆ’tdt =

Î»
Î» + Âµ
3
.
2) The probability that there is emitted precisely two Î± particles before one Î² particle is emitted is
P {X1 + X2 < Y1} âˆ’P {X1 + X2 + X3 < Y1}
=

Î»
Î» + Âµ
2
âˆ’

Î»
Î» + Âµ
3
=

Î»
Î» + Âµ
3 Î» + Âµ
Î»
âˆ’1

=
Î»2Âµ
(Î» + Âµ)3 .
3) Assume that Zn = X1 + Â· Â· Â· + Xn has the frequency fk(s). Then fk(s) = 0 for s â‰¤0, and we have
for s > 0,
fn(s) =
 s
0
fmâˆ’1(x)f(s âˆ’x) dx =
 s
0
fnâˆ’1(x) Î» eâˆ’Î»(sâˆ’x)dx = Î» eâˆ’Î»s
 s
0
eÎ»xfnâˆ’1(x) dx,
i.e.
f2(s) = Î» eâˆ’Î»s  s
0 eÎ»xÎ» eâˆ’Î»x dx = Î»2s eâˆ’Î»s,
s > 0,
f3(s) = Î» eâˆ’Î»s  s
0 eÎ»x Â· Î»2x eâˆ’Î»x dx = Î»3 Â· s2
2! eâˆ’Î»s,
s > 0,
and then by induction
fn(s) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
Î»n
(n âˆ’1)! snâˆ’1eâˆ’Î»s,
s > 0,
0,
s â‰¤0.
It follows that
P {Zk < t} =
 t
0
fk(s) ds =
Î»k
(k âˆ’1)!
 t
0
skâˆ’1eÎ»s ds,
0 â‰¤k â‰¤n,
which is the probability that there is emitted at least k of the Î± particles before time t.
The probability that there is emitted precisely k particles of Î± type before time t, is
P {Zk < t} âˆ’P {Zk+1 < t} =
Î»k
(k âˆ’1)!
 t
0
skâˆ’1eâˆ’Î»sds âˆ’Î»k+1
k!
 t
0
skeâˆ’Î»sds
=
Î»k
(k âˆ’1)!
 t
0
skâˆ’1eâˆ’Î»s dx +
Î»k
k! skeâˆ’Î»s
t
0
âˆ’
Î»k
(k âˆ’1)!
 t
0
skâˆ’1eâˆ’Î»s ds
=
Î»k
k! skeâˆ’Î»s
t
0
= Î»k
k! tkeâˆ’Î»t = (Î»t)k
k!
eâˆ’Î»t.
Download free eBooks at bookboon.com

Random variables II
 
66 
4. Examples concerning the Poisson distribution
Analogously, the probability that there is emitted precisely n âˆ’k particles of type Î² in ]0, 1[ is
given by
(Âµt)nâˆ’k
(n âˆ’k)! eâˆ’Âµt.
Finally, the probability that there is emitted precisely n particle (of either type Î± or type Î²) in
the time interval ]0, 1[ is
n

k=0
(Î»t)k
k!
eâˆ’Î»t Â· (Âµt)nâˆ’k
(n âˆ’k)! eâˆ’Âµt = 1
n! eâˆ’(Î»+Âµ)t
n

k=0
n!
k!(n âˆ’k)! (Î»t)k(Âµt)nâˆ’k
= 1
n! tneâˆ’(Î»+Âµ)t
n

k=0

n
k

Î»kÂµnâˆ’k = 1
n! (Î» + Âµ)ntneâˆ’(Î»+Âµ)t.
Example 4.4 An instrument A contains two components, which can fail independently of each other.
The instrument does not work, if just one of the components does not work.
The lifetime for each of the two components has a distribution given by the frequency
f(x) =
ï£±
ï£²
ï£³
Î» eâˆ’Î»x,
x > 0,
0,
x â‰¤0,
where Î» is a positive constant.
The task is to ï¬nd the distribution of the lifetime of the instrument A.
There is in another instrument B only one component, the lifetime of which has the same frequency
f(x) as above.
We shall ï¬nd the probability that the lifetime of instrument B is at least the double of the lifetime of
A.
Let us imagine that we ï¬rst apply instrument A, and when it is ruined, then we apply instrument B.
Find the distribution of the total lifetime and ï¬nd the mean of this lifetime.
Let Y1 and Y2 denote the lifetimes of the two components of A, and Y the lifetime of A, and X the
lifetime of B.
Clearly, Y = min {Y1, Y2}.
Then Y is exponentially distributed of frequency
g(y) =
ï£±
ï£²
ï£³
2Î» eâˆ’2Î»y,
y > 0,
0,
y â‰¤0,
In the next subtask we shall ï¬nd P{X â‰¥2Y }.
A reasonable assumption is that A and B function independently of each other. This means that
(X, Y ) has the simultaneous frequency f(x)g(y), thus
P{X â‰¥2Y }
=

{xâ‰¥2y}
f(x)g(y) dx dy =
 âˆ
y=0
2Î» eâˆ’2Î»y
 âˆ
x=2y
Î» eâˆ’Î»x dx

dy
=
 âˆ
y=0
2Î» eâˆ’4Î»y dy = 1
2.
Download free eBooks at bookboon.com

Random variables II
 
67 
4. Examples concerning the Poisson distribution
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
The frequency of X + Y for z > 0 is given by the convolution integral,
k(z) =
 z
0
Î» eâˆ’Î»x Â· 2Î» eâˆ’2Î»(zâˆ’x) dx = 2Î»2eâˆ’2Î»z
 z
0
eÎ»x dx = 2Î»

eâˆ’Î»x âˆ’eâˆ’2Î»z
,
and k(z) = 0 for z â‰¤0.
Finally,
E{X + Y } = E{X} + E{Y } = 1
Î» + 1
2Î» = 3
2Î».
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
68 
4. Examples concerning the Poisson distribution
Alternatively,
E{X + Y } =
 âˆ
0
z k(z) dz = 2Î»
 âˆ
0

z eâˆ’Î»z âˆ’z eâˆ’2Î»z
dz = 2
Î» Â· 1! âˆ’1
2Î» Â· 1! = 3
2Î».
Example 4.5 1. Let X be a non-negative random variable of frequency f(x) and mean E{X}. Prove
that
(1) E{X} =
 âˆ
0
P{X â‰¥x} dx.
Hint: Express e.g. P{X â‰¥x} by means of the frequency f(x).
We shall allow in the following without proof to apply the result that the mean of every non-negative
random variable is given by (1).
Two patients A1 and A2 arrive to a doctorâ€™s waiting room at the times X1 and X1 + X2, where X1
and X2 are independent random variables, both of the frequency
f(x) =
ï£±
ï£²
ï£³
Î» eâˆ’Î»x,
x > 0,
0,
x â‰¤0,
where Î» is a positive constant.
The times of treatment of A1 and A2 are assumed to be the random variables Y1 and Y2, which
are mutually independent (and also independent of X1 and X2), and we assume that they have the
frequency
g(y) =
ï£±
ï£²
ï£³
Âµ eâˆ’Âµy,
y > 0,
0,
y â‰¤0,
where Âµ is a positive constant.
The patient A1 is treated immediately after his arrival, while A2 possibly may wait to after the treatment
of A1.
2. Describe, expressed by Y1 and Y2, the event that A2 does not wait for his treatment, and ï¬nd the
probability of this event.
3. Find for every z > 0 the probability that the waiting time Z of A2 is â‰¥z.
4. Find the mean of the random variable Z.
1) Since f(t) â‰¥0, and f(t) = 0 for t < 0, we get
 âˆ
0
P{X â‰¥x} dx =
 âˆ
0
 âˆ
x
f(t) dt

dx =
 âˆ
0
 t
0
f(t) dx

dt =
 âˆ
0
t f(t) dt = E{X}.
2) The condition that A2 does not have to wait is
X1 + Y1 â‰¤X1 + X2,
thus
Y1 â‰¤X2,
Download free eBooks at bookboon.com

Random variables II
 
69 
4. Examples concerning the Poisson distribution
hence
P {X2 â‰¥Y1}
=
 âˆ
y=0
g(y)
 âˆ
x=y
f(x) dx

dy =
 âˆ
y=0
Âµ eâˆ’Âµy
 âˆ
x=y
Î» eâˆ’Î»x dx

dy
=
 âˆ
y=0
Âµ eâˆ’(Î»+Âµ)y dy =
Âµ
Î» + Âµ.
3) When the waiting time is positive, it is described by Z = Y1 âˆ’X2. Then for z > 0,
P{Z â‰¥z}
=
P {Y1 â‰¥X2 + z} =
 âˆ
x=0
f(x)
 âˆ
y=x+z
g(y) dy

dx
=
 âˆ
x=0
Î» eâˆ’(Î»+Âµ)x dx Â· eâˆ’Âµz =
Î»
Î» + Âµ Â· eâˆ’Âµz.
4) It follows from (1) that
E{Z} =
 âˆ
0
P{Z â‰¥z} dz =
Î»
Î» + Âµ
 âˆ
0
eâˆ’Âµzdz = Î»
Âµ Â·
1
Î» + Âµ.
Remark 4.2 The distribution of Z is of mixed type, i.e. neither discrete nor continuous. â™¦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables II
 
70 
5. Miscellaneous examples
5
Miscellaneous examples
Example 5.1 A 2-dimensional random variable (X, Y ) has in the domain given by the inequalities
1 â‰¤x2 + y2 â‰¤4
the frequency
h(x, y) = 1
3Ï€ ,
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find the frequency of the random variable X, and sketch the graph of this function.
2) Find the variance of the random variable X.
3) Explain why the random variable X and Y are non-correlated, though not independent.
4) Find the probability that |X| + |Y | â‰¥2.
â€“2
â€“1
0
1
2
â€“2
â€“1
1
2
Figure 2: The frequency has its support in the annulus.
1) If |x| â‰¥2, then fX(x) = 0.
By the symmetry, fX(âˆ’x) = fX(x). If |x| âˆˆ[1, 2], then it follows by a vertical integration (a
consideration of a graph) that
fX(x) = 1
3Ï€ Â· 2

4 âˆ’x2 = 2
3Ï€

4 âˆ’x2.
If |x| âˆˆ[0, 1], then we get instead
fX(x) = 2
3Ï€

4 âˆ’x2 âˆ’

1 âˆ’x2

.
Summing up,
fX(x)
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
2
3Ï€
âˆš
4 âˆ’x2 âˆ’
âˆš
1 âˆ’x2
,
x âˆˆ[âˆ’1, 1],
2
3Ï€
âˆš
4 âˆ’x2,
1 â‰¤|x| â‰¤2,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
71 
5. Miscellaneous examples
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
â€“2
â€“1
1
2
t
Figure 3: The graph of fX(x).
2) The mean is trivially E{X} = 0, so the variance is
V {X}
=
E

X2
âˆ’(E{X})2 = E

X2
=
4
3Ï€
 1
0
x2 
4 âˆ’x2 âˆ’

1 âˆ’x2

+ 4
3Ï€
 2
1
x2
4 âˆ’x2 dx
=
4
3Ï€
 2
0
x2
4 âˆ’x2 dx âˆ’4
3Ï€
 1
0
x2
1 âˆ’x2 dx
=
4
3Ï€

Ï€
2
0
4 sin2 t Â· 2 cos t Â· 2 cos t dt âˆ’4
3Ï€

Ï€
2
0
sin2 t cos t cos t dt
=
16 âˆ’1
3Ï€

Ï€
2
0
4 sin2 t cos2 t dt = 5
Ï€

Ï€
2
0
sin2 2t dt = 5
Ï€ Â· Ï€
4 = 5
4.
3) The support of h (i.e. the closure of the set, where h(x, y) Ì¸= 0) is not a rectangle. Hence, X and
Y cannot be independent.
The annulus is denoted by â„¦. By using that E{X} = 0, it follows by the symmetry that
Cov(X, Y ) = E{XY } âˆ’E{X}E{Y } =
 
â„¦
xy Â· 1
3Ï€ dx dy = 0,
hence X and Y are non-correlated.
4) It follows by considering the ï¬gure that P{|X| + |Y | â‰¥2} is equal to the integral of h(x, y) over
the four circular segments, thus equal to 1
3Ï€ times the area of these four circular segments, hence
P{|X| + |Y | â‰¥2} = 1
3Ï€

Ï€ Â· 22 âˆ’(2
âˆš
2)2
= 4
3Ï€ (Ï€ âˆ’2) = 4
3 âˆ’8
3Ï€ â‰ˆ0.485.
Download free eBooks at bookboon.com

Random variables II
 
72 
5. Miscellaneous examples
â€“2
â€“1
1
2
â€“2
â€“1
1
2
Figure 4: The domain where |X| + |Y | â‰¥2, is the union of the four circular segments on the ï¬gure.
Example 5.2 1) Find the pairs of numbers (a, b), for which
g(x, y) =
ï£±
ï£²
ï£³
ax + by
for 0 â‰¤x â‰¤2 og 0 â‰¤y â‰¤1,
0
otherwise,
is the frequency of a 2-dimensional random variable (X, Y ).
2) Find, expressed by a, the means E{X} and E{Y }.
3) Find the pairs of numbers (a, b), for which the product E{X}E{Y } is largest, and compute the
maximum.
4) Compute for (a, b) =
1
4, 1
2

the covariance Cov(X, Y ).
1) Since g(x, y) â‰¥0 everywhere, we must have a â‰¥0 and b â‰¥0. Furthermore, we derive the condition
1 =
 2
0
ax
 1
0
dy

dx +
 2
0
b
 1
0
y dy

dx = 2a + 2b Â· 1
2 = 2a + b,
thus b = 1 âˆ’2a, where a âˆˆ

0, 1
2

, hence
g(x, y) =
ï£±
ï£²
ï£³
ax + (1 âˆ’2a)y
for 0 â‰¤x â‰¤2 and 0 â‰¤y â‰¤1,
0
otherwise,
a âˆˆ

0, 1
2

.
Download free eBooks at bookboon.com

Random variables II
 
73 
5. Miscellaneous examples
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 5: The support of g(x, y).
0
0.2
0.4
0.6
0.8
1
0.1
0.2
0.3
0.4
0.5
Figure 6: The possible values of (a, b) lie on the oblique line.
2) If a âˆˆ

0, 1
2

we get the mean
E{X}
=
 2
0
x fX(x) dx =
 2
0
 1
0
x{ax + (1 âˆ’2a)y}dy

dx
=
 2
0
ax2dx +
 2
0
x dx Â· (1 âˆ’2a)
 1
0
y dy
=
8a
3 + (1 âˆ’2a) Â· 1
2 Â· 22
2 = 1 + 2
3 a,
and analogously
E{Y }
=
 1
0
y fY (y) dy =
 2
0
 1
0
y{ax + (1 âˆ’2a)y} dy

dx
=
a
 2
0
x dx Â·
 1
0
y dy?(1 âˆ’2a) Â· 2
 1
0
y2dy
=
2a Â· 1
2 + 2
3 (1 âˆ’2a) = a + 2
3 âˆ’4
3 a = 2
3 âˆ’1
3 a.
Download free eBooks at bookboon.com

Random variables II
 
74 
5. Miscellaneous examples
3) If we put
Ï•(a)
=
E{X}E{Y } =

1 + 2
3 a
 2
3 âˆ’1
3 a

=
1
9 (3 + 2a)(2 âˆ’a) = 1
9

6 + a âˆ’2a2
,
then
Ï•â€²(a) = 1 âˆ’4a = 0
for a = 1
4.
Since Ï•â€²(a) > 0 for a < 1
4, and Ï•â€²(a) < 0 for a > 1
4, it follows that a = 1
4 corresponds to the
maximum
Ï•
1
4

= 1
9

6 + 1
4 âˆ’1
8

= 48 + 2 âˆ’1
72
= 49
72.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables II
 
75 
5. Miscellaneous examples
4) If (a, b) =
1
4, 1
2

, then
E{XY }
=
 2
0
 1
0
xy
1
4 x + 1
2 y

dy

dx = 1
4
 2
0
x2dx Â·
 1
0
y dy + 1
2
 2
0
x dx Â·
 1
0
y2dy
=
1
4 Â· 8
3 Â· 1
2 + 1
2 Â· 4
2 Â· 1
3)1
3 + 1
3 = 2
3,
hence
Cov(X, Y ) = E{XY } âˆ’E{X}E{Y } = 2
3 âˆ’49
72 = 2 Â· 24 âˆ’49
72
= âˆ’1
72.
Example 5.3 A 2-dimensional random variable (X, Y ) has in the square deï¬ned by 0 < x < Ï€
2 and
0 < y < Ï€
2 the frequency
h(x, y) = k(sin x + cos y),
while the frequency is 0 outside this square.
1) Prove that the constant k is equal to 1
Ï€ .
2) Find the frequencies fX(x) and fY (y) of the random variables X and Y .
3) Find the means E{X} and E{Y } of the random variables X and Y .
4) Find the frequency fZ(z) of the random variable Z = X + Y , and sketch the graph of the function.
1) Clearly, h(x, y) â‰¥0, if and only if k â‰¥0. If h(x, y) is a frequency, then necessarily
1
=

Ï€
2
0

Ï€
2
0
h(x, y) dx dy = k

Ï€
2
0

Ï€
2
0
sin x dx

dy +

Ï€
2
0

Ï€
2
0
cos y dy

dx

=
k
Ï€
2 + Ï€
2

= k Â· Ï€,
and we conclude that k = 1
Ï€ as claimed.
2) When x /âˆˆ

0, Ï€
2

, then fX(x) = 0. When x âˆˆ

0, Ï€
2

, it follows by a vertical integration that
fX(x) =

Ï€
2
0
h(x, y) dy = 1
Ï€

Ï€
2
0
{sin x + cos y} dy = 1
2 sin x + 1
Ï€ .
When x /âˆˆ

0, Ï€
2

, then fY (y) = 0. When y âˆˆ

0, Ï€
2

, it follows by a horizontal integration that
fY (y) =

Ï€
2
0
h(x, y) dx = 1
Ï€

Ï€
2
0
{sin x + cos y} dx = 1
2 cos y + 1
Ï€ .
Download free eBooks at bookboon.com

Random variables II
 
76 
5. Miscellaneous examples
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Figure 7: The square ]0, Ï€
2 [ Ã— ]0, Ï€
2 [.
3) The means are
E{X}
=

Ï€
2
0
x fX(x) dx =

Ï€
2
0
1
2 x sin x + x
Ï€

dx =

âˆ’x
2 cos x + x2
2Ï€
 Ï€
2
0
+ 1
2

Ï€
2
0
cos x dx
=
Ï€
8 + 1
2,
and
E{Y }
=

Ï€
2
0
y fY (y) dy =

Ï€
2
0
1
2 y cos y + y
Ï€

dy =
1
2 y sin y + y2
2Ï€
 Ï€
2
0
âˆ’1
2

Ï€
2
0
sin y dy
=
Ï€
4 + Ï€
8 âˆ’1
2 = 3Ï€
8 âˆ’1
2.
4) Clearly, X+Y has values in ]0, Ï€[. Since X and Y are not independent, the frequency of Z = X+Y
is given by
fZ(z) =
 âˆ
âˆ’âˆ
h(x, z âˆ’x) dx =

Ï€
2
0
h(x, z âˆ’x) dx.
Now let 0 < z < Ï€. The the integrand is Ì¸= 0, if 0 < x < Ï€
2 and 0 < zâˆ’x < Ï€
2 , i.e. if zâˆ’Ï€
2 < x < z.
Then we must split into two cases:
a) If 0 < z â‰¤Ï€
2 , then the domain of integration is 0 < x < z, so
fZ
=
 z
0
h(x, z âˆ’x) dx = 1
Ï€
 z
0
{sin x + cos(z âˆ’x)} dx
=
1
Ï€ [âˆ’cos x + sin(x âˆ’z)]z
0 = 1
Ï€ {1 + sin z âˆ’cos z}.
Download free eBooks at bookboon.com

Random variables II
 
77 
5. Miscellaneous examples
0
0.1
0.2
0.3
0.4
0.5
0.6
0.5
1
1.5
2
2.5
3
Figure 8: The graph of fZ(z).
b) If Ï€
2 < z < Ï€, then the domain of integration is z âˆ’Ï€
2 < x < Ï€
2 , hence
fZ(z)
=
1
Ï€ [âˆ’cos x + sin(x âˆ’z)]
Ï€
2
zâˆ’Ï€
2
=
1
Ï€

âˆ’0 + sin
Ï€
2 âˆ’z

+ cos

z âˆ’Ï€
2

âˆ’sin

âˆ’Ï€
2

= 1
Ï€ {1 + sin z + cos z}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables II
 
78 
5. Miscellaneous examples
Summing up,
fZ(z) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
1
Ï€ {1 + sin z âˆ’cos z}
for 0 < z â‰¤Ï€
2 ,
1
Ï€ {1 + sin z + cos z}
for Ï€
2 < z < Ï€,
0
otherwise.
Example 5.4 Let X and Y be independent random variables, which both are rectangularly distributed
over the interval ]1, 2[.
1. Find the frequency of the random variable Z = X
Y .
Compute the mean of Z.
Find the median of Z.
A random variable U is given by U = X
Y + Y
Z .
4. Which values can U have?
5. Find the probability that U < 25
12.
1) Clearly, Z has its values in
1
2, 2

.
The frequency of Z = X
Y is given by
fZ(z) =
 âˆ
âˆ’âˆ
fX(zx) Â· fY (x) Â· |x| dx =
 2
1
fX(zx) x dx.
When z âˆˆ
1
2, 2

, then the conditions become 1 < x < 2 and 1 < zx < 2, hence 1
z < x < 2
z .
a) When z âˆˆ
1
2, 1

, then the interval of integration is
1
z , 2

, hence
fZ(z) =
 2
1
z
x dx =
x2
2
2
1
z
= 2 âˆ’
1
2z2

= 4z2 âˆ’1
2z2

.
b) When z âˆˆ]1, 2[, then the interval of integration is

1, 2
z

, hence
fZ(z) =

2
z
1
x dx =
x2
2
 2
z
1
= 2
z2 âˆ’1
2

= 4 âˆ’z2
2z2

.
Download free eBooks at bookboon.com

Random variables II
 
79 
5. Miscellaneous examples
Summing up,
fZ) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
2 âˆ’
1
2z2
for z âˆˆ
1
2, 1

,
2
z2 âˆ’1
2
for z âˆˆ]1, 2[,
0
otherwise.
2) The mean is
E{Z}
=
 1
1
2

2z âˆ’1
2z

dz +
 2
1
2
z âˆ’z
2

dz =

z2 âˆ’1
2 ln z
1
1
2
+

2 ln z âˆ’z2
4
2
1
=
1 âˆ’1
4 + 1
2 ln 1
2 + 2 ln 2 âˆ’1 + 1
4 = 3
2 ln 2.
3) For 1
2 â‰¤z â‰¤1 the distribution function is given by
FZ(z) =
 z
1
2

2 âˆ’1
2t2

dt =

2t + 1
2t
z
1
2
= 2z + 1
2z âˆ’1 âˆ’1 = 2z + 1
2z âˆ’2 = (2z âˆ’1)2
2z
.
When z = 1, we get FZ(1) = 1
2, so the median is (Z) = 1, and there is in this question no need to
ï¬nd the expression of the distribution function.
4) If we put z = x
y âˆˆ
1
2, 2

, then u = z + 1
z , which has a minimum for z = 1 and is increasing for
z âˆˆ]1, 2[. It follows that U has its values in

2, 5
2

.
The inequality U = Z + 1
Z < 25
12 is equivalent to Z2 âˆ’25
12 Z + 1 < 0, thus Z lies between the roots
of the equation
z2 âˆ’25
12 z + 1 = 0.
These roots are
z = 25
24 Â±
25
24
2
âˆ’1 = 25
24 Â±

49
24 Â· 1
24 = 25
24 Â± 7
4 =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
4
3,
3
4.
Then
P

U < 25
12

= P
3
4 < Z < 4
3

= FZ
4
3

âˆ’FZ
3
4

.
Download free eBooks at bookboon.com

Random variables II
 
80 
5. Miscellaneous examples
We shall now need the explicit expression of the distribution function FZ(z) when z âˆˆ]1, 2[. We
ï¬nd
FZ
4
3

=
1
2 +

4
3
1
 2
t2 âˆ’1
2

dt = 1
2 +

âˆ’2
t âˆ’t
2
 4
3
1
= 1
2 âˆ’2
4
3
âˆ’
4
3
2 + 2 + 1
2
=
3 âˆ’3
2 âˆ’2
3 = 3
2 âˆ’2
3 = 5
6,
hence by insertion
P

U < 25
12

= 5
6 âˆ’
 3
2 âˆ’1
2
3
2
= 5
6 âˆ’1
6 = 4
6 = 2
3.
Example 5.5 A 2-dimensional random variable (X, Y ) has in the domain given by 0 â‰¤x â‰¤a,
x â‰¤y â‰¤x + 1 (where a > 0) the frequency
h(x, y) = 1
a,
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find, possibly without ï¬rst ï¬nding the marginal frequencies, the means E{X} and E{Y }, the
variances V {X} and V {Y }, and the mean E{XY }.
2) Indicate, expressed by a, the correlation coeï¬ƒcient Ï±(X, Y ).
3) Find limaâ†’âˆÏ±(X, Y ) and limaâ†’0 Ï±(X, Y ).
0
0.5
1
1.5
2
2.5
3
0.5
1
1.5
2
Figure 9: The domain for a = 2.
1) It follows immediately that
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
a
for x âˆˆ]0, a[,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
81 
5. Miscellaneous examples
thus X is rectangularly distributed, hence
E{X} = a
2
and
V {X} = a2
12.
Then
E{Y }
=
1
a
 a
0
 x+1
x
y dy

dx = 1
2a
 a
0

(x + 1)2 âˆ’x2
dx
=
1
2a
 a
0
(2x + 1) dx = 1
2a

x2 + x
a
0 = a2 + a
2a
= a + 1
2
,
and
E

Y 2
=
1
a
 a
0
 x+1
x
y2dy

dx = 1
3a
 a
0

(x + 1)3 âˆ’x3
dx
=
1
3a
 a
0

3x2 + 3x + 1

dx = 1
3a

x3 + 3
2 x2 + x
a
0
=
1
3a

a3 + 3
2 a2 + a

= 1
6

2a2 + 3a + 2

,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables II
 
82 
5. Miscellaneous examples
hence
V {Y }
=
1
6

2a2 + 3a + 2

âˆ’1
4 (a + 1)2
=
1
12

4a2 + 6a + 4 âˆ’3a2 âˆ’6a âˆ’3

= a2 + 1
12
.
Finally,
E{XY }
=
1
a
 a
0
x
 x+1
x
y dy

dx = 1
2a
 a
0
x{2x + 1} dx = 1
2a
 a
0

2x2 + x

dx
=
1
2a
2
3 a3 + 1
2 a2

= 1
3 a2 + 1
4 a.
2) It follows by insertion,
Cov(X, Y )
=
E{XY } âˆ’E{X}E{Y } = 1
3 a2 + 1
4 a âˆ’a
2
a + 1
2

=
a2
3 + a
4 âˆ’a2
4 âˆ’a
4 = a2
12.
This implies that
Ï±(X, Y ) =
Cov(X, Y )

V {X}V {Y }
= a2
12 Â·
1

a2
12 Â· a2 + 1
12
=
a
âˆš
a2 + 1
.
3) The limits are trivial,
lim
aâ†’âˆÏ±(X, Y ) = lim
â†’âˆ
a
âˆš
a2 + 1
= 1,
and
lim
aâ†’0 Ï±(X, Y ) = 0.
Download free eBooks at bookboon.com

Random variables II
 
83 
5. Miscellaneous examples
Example 5.6 A 2-dimensional random variable (X, Y ) has in the domain D given by 0 < x < 1,
0 < y < 1, the frequency
f(x, y) = 6
5

x + y2
,
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find the frequencies and the distribution function of the random variables X and Y .
2) Find the means E{X} and E{Y }, the variances V {X} and V {Y }, and the covariance Cov(X, Y ).
3) Find the distribution function F(x, y) of the 2-dimensional random variable (X, Y ) in the domain
D.
4) Find the set M of all points in the (x, y) plane, for which
F(x, y) = 7
20,
and sketch the graph of the point set M.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 10: The domain D.
1) When 0 < x < 1, then
fX(x) = 6
5
 1
0

x + y2
dy = 6
5

x + 1
3

= 6
5 x + 2
5,
and fX(x) = 0 otherwise.
When 0 < y < 1, then
fY (y) = 6
5
 1
0

x + y2
dx = 6
5
1
2 + y2

= 3
5 + 6
5 y2,
and fy(y) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables II
 
84 
5. Miscellaneous examples
Summing up, the frequency of X is given by
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
6
5 x + 2
5
for 0 < x < 1,
0
otherwise,
and the corresponding distribution function is
FX(x) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
0,
x â‰¤0,
3
5 x2 + 2
5 x,
0 < x < 1,
1,
x â‰¥1.
Analogously, the frequency of Y is given by
fY (y) =
ï£±
ï£´
ï£²
ï£´
ï£³
3
5 + 6
5 y2
for 0 < y < 1,
0
otherwise,
and the corresponding distribution function is
FY (y) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
0,
y â‰¤0,
2
5 y3 + 3
5 y,
0 < y < 1,
1,
y â‰¥1.
2) The means are
E{X} =
 1
0
6
5 x2 + 2
5 x

dx = 2
5 + 1
5 = 3
5,
and
E{Y } =
 1
0
3
5 y + 6
5 y3

dy = 3
10 + 3
10 = 3
5.
Furthermore,
E

X2
=
 1
0
6
5 x3 + 2
5 x2

dx = 1
5
3
2 + 2
3

= 13
30,
and
E

Y 2
= 1
5
 1
0

3y2 + 6y4
dy = 1
5

1 + 6
5

= 11
25,
thus the variances are
V {X} = 13
30 âˆ’9
25 = 65 âˆ’54
150
= 11
150,
Download free eBooks at bookboon.com

Random variables II
 
85 
5. Miscellaneous examples
and
V {Y } = 11
25 âˆ’9
25 = 2
25.
Finally,
E{XY }
=
6
5
 1
0
x
 1
0

xy + y3
dy

dx = 6
5
 1
0
x
x
2 + 1
4

dx
=
3
10
 1
0

2x2 + x

dx = 3
10
2
3 + 1
2

= 3 Â· 7
60 = 7
20,
hence the covariance is
Cov(X, Y ) = E{XY } âˆ’E{X} Â· E{Y } = 7
20 âˆ’9
25 = 35 âˆ’36
100
= âˆ’1
100.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables II
 
86 
5. Miscellaneous examples
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 11: The domain of integration for the determination of F(x, y).
3) If (x, y) âˆˆD, i.e. 0 < x < 1 and 0 < y < 1, then the distribution function is given by
F(x, y)
=
6
5
 x
0
 y
0

t + u2
du

dt = 6
5
 x
0

ty + 1
3 y3

dt
=
6
5
1
2 x2y + 1
3 xy3

= 1
5

3x2y + 2xy3
= 1
5 xy

3x + 2y2
.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0.2
0.4
0.6
0.8
1
1.2
1.4
Figure 12: The curve M, where F(x, y) = 7
20.
4) We have in D,
F(x, y) = 3
5 y Â· x2 + 2
5 y3 Â· x = 7
20,
when
(12y) Â· x2 +

8y3
x âˆ’7 = 0.
Since y Ì¸= 0 for every solution, we ï¬nd by solving with respect to x that
x = âˆ’8y3 +

64y6 + 4 Â· 7 Â· 12y
24y
=

4y6 + 21y âˆ’2y3
6y
.
Download free eBooks at bookboon.com

Random variables II
 
87 
5. Miscellaneous examples
If we in particular choose y = 1, then x = 1
6 {âˆš4 + 21 âˆ’2} = 1
2- Then F
1
2, y

= 7
20 for every
y â‰¥1.
Choosing x = 1, the equation is reduced to 8y3 + 12y âˆ’7 = 0, the only solution of which in [0, 1]
is y = 1
2. Then F

x, 1
2

= 7
20 for every x â‰¥1.
Example 5.7 A point set D in the (x, y) plane is the union of the following two sets
D1
=

(x, y)
 0 â‰¤x â‰¤1, 0 â‰¤y â‰¤x
2

,
D2
=

(x, y)
 0 â‰¤x â‰¤1, 1 + x
2
â‰¤y â‰¤1

.
A 2-dimensional random variable (X, Y ) has in D the frequency f(x, y) = 2, while the frequency is 0
everywhere else in the plane.
1) Find the frequencies fX(x) and fY (y) of the random variable X and Y .
2) Find the means E{X} and E{Y } and the variances V {X} and V {Y }.
3) Find the covariance Cov(X, Y ).
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 13: The subdomain D1 is the lower triangle and the subdomain D2 is the upper triangle.
1) By mental arithmetic (i.e. it is strictly speaking a vertical integration) it follows that
fX(x) =
ï£±
ï£²
ï£³
1
for x âˆˆ[0, 1],
0
otherwise.
so X is rectangularly distributed over [0, 1].
Download free eBooks at bookboon.com

Random variables II
 
88 
5. Miscellaneous examples
When y âˆˆ

0, 1
2

, we get by a horizontal integration (also mental arithmetic) that
fY (y) = 2 Â· (1 âˆ’2y) = 2 âˆ’4y.
If on the other hand, y âˆˆ
1
2, 1

, then analogously,
fY (y) = 2 Â· (2y âˆ’1) = 4Y âˆ’2.
Summing up,
fY (y) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
2 âˆ’4y
for y âˆˆ

0, 1
2

,
4y âˆ’2
for y âˆˆ
1
2, 1

,
0
otherwise,
which is reduced to
fY (y) =
ï£±
ï£²
ï£³
2 |2y âˆ’1|
for y âˆˆ[0, 1],
0
otherwise.
2) Since X is rectangularly distributed, we have
E{X} = 1
2
and
V {X} = 1
12.
It follows by the symmetry that E{Y } = 1
2.
Alternatively, this follows by the computation
E{Y }
=

1
2
0

2y âˆ’4y2
dy +
 1
1
2

4y2 âˆ’2y

dy =

y2 âˆ’4
3 y3
 1
2
0
+
4
3 y3 âˆ’y2
1
1
2
=
1
4 âˆ’1
6 + 4
3 âˆ’1 âˆ’1
6 + 1
4 = 1
2.
Furthermore,
E

Y 2
=

1
2
0

2y2 âˆ’4y3
dy +
 1
1
2

4y3 âˆ’2y2
dy =
2
3 y3 âˆ’y4
 1
2
0
+

y4 âˆ’2
3 y3
1
1
2
=
1
6 âˆ’1
16 + 1 âˆ’2
3 âˆ’1
16 + 1
6 = 1
3 âˆ’1
8 + 1
3 = 2
3 âˆ’1
8 = 16 âˆ’3
24
= 13
24,
hence
V {Y } = 13
24 âˆ’1
4 = 7
24.
Download free eBooks at bookboon.com

Random variables II
 
89 
5. Miscellaneous examples
3) First compute
E{XY }
=
2
 
D1
xy dx dy + 2
 
D2
xy dx dy
=
2
 1
0
x

x
2
0
y dy

dx + 2
 1
0
 1
1+x
2
y dy

dx =
 1
0
x
x
2
2
+ 1 âˆ’
1 + x
2
2
dx
=
 1
0
x
1 + 2x
2
Â·

âˆ’1
2

+ 1

dx = 1
4
 1
0
x(4 âˆ’1 âˆ’2x) dx
=
1
4
 1
0

3x âˆ’2x2
dx = 1
4
3
2 âˆ’2
3

= 1
4 Â· 0 âˆ’4
6
= 5
24.
We ï¬nally get
Cov(X, Y ) = E{XY } âˆ’E{X}E{Y } = 5
24 âˆ’1
2 Â· 12 = âˆ’1
24.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables II
 
90 
5. Miscellaneous examples
Example 5.8 A 2-dimensional random variable (X, Y ) has in the domain D = ]0, 1[ Ã— ]0, 1[ the fre-
quency
f(x, y) = 3

xy2 + yx2
,
while the frequency is 0 everywhere else in the (x, y) plane.
1. Find the frequency fX(x) and the distribution function FX(x) of the random variable X.
2. Compute the mean E{X} and the variance V {X}.
3. Find for every real number k the simultaneous distribution function F(x, y) of (X, Y ) at the point
(k, k).
4. Find the probability that both X and Y are smaller than 1
2.
5. Find the probability that both X and Y are bigger than 1
2.
The parabolic arcs y = x2 and y = âˆšx, 0 â‰¤x â‰¤1, divide D into three domains D1, D2, D3.
6. Find the probabilities
P {(X, Y ) âˆˆD1} ,
P {(X, y) âˆˆD2}
and
P {(X, Y ) âˆˆD3}
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 14: The domain D.
1) When x âˆˆ]0, 1[, we get by a vertical integration,
fX(x) = 3
 1
0

xy2 + y2x

dy = x + 3
2 x2,
thus the frequency is
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
x + 3
2 x2,
x âˆˆ]0, 1[,
0,
otherwise,
Download free eBooks at bookboon.com

Random variables II
 
91 
5. Miscellaneous examples
and the distribution function is
F(x) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
0,
x â‰¤0,
1
2

x2 + x3
,
0 < x < 1,
1,
x â‰¥1.
2) The mean is
E{X} =
 1
0

x2 + 3
2, x3

dx = 1
3 + 3
8 = 8 + 9
24
= 17
24.
Since
E

X2
=
 1
0

x2 + 3
2 x4

dx = 1
4 + 3
10 = 5 + 6
20
= 11
20,
the variance becomes
V {X} = 11
20 âˆ’
17
24
2
= 139
2880.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 15: The domain of integration for the determination of F(k, k); here k = 2
3.
3) When k â‰¤0, then F(k, k) = 0, and when k â‰¥1, then F(k, k) = 1.
When 0 < k < 1, then
F(k, k)
=
 k
0
 k
0

3xy2 + 3yx2
dy

dx =
 k
0

xy3 + 3
2 y2x2
k
y=0
dx
=
 k
0

k3x + 3
2 k2x2

dx =
1
2 k3x2 + 1
2 k2x3
k
0
= k5.
Download free eBooks at bookboon.com

Random variables II
 
92 
5. Miscellaneous examples
4) The probability that both X and Y are â‰¤1
2, is
P

X â‰¤1
2, Y â‰¤1
2

= F
1
2, 1
2

=
1
2
5
= 1
32.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 16: The domain of integration of question 5 is the upper square.
5) The probability that both X and Y are â‰¥1
2, is due to the symmetry,
P

X â‰¥1
2, Y â‰¥1
2

=
1 âˆ’P

P â‰¥1
2

âˆ’P

Y â‰¥1
2

+ P

X â‰¤1
2, Y â‰¤1
2

=
1 âˆ’2 P

X â‰¤1
2

+ F
1
2, 1
2

= 1 âˆ’2 FX
1
2

+ F
1
2, 1
2

=
1 âˆ’
1
4 + 1
8

+ 1
32 = 32 âˆ’8 âˆ’4 + 1
32
= 21
32.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 17: The domains D1 (down most), D2 (in the middle) and D3 (uppermost).
Download free eBooks at bookboon.com

Random variables II
 
93 
5. Miscellaneous examples
6) It follows by the symmetry that
P {(X, Y ) âˆˆD1} = P {(X, Y ) âˆˆD3} ,
hence
P {(X, Y ) âˆˆD2} = 1 âˆ’2 P {(X, Y ) âˆˆD1} .
Then by a planar integral,
P {(X, Y ) âˆˆD1}
=
 1
0
 x2
0

3xy2 + 3yx2
dy

dx =
 1
0

xy3 + 3
2 y2x2
x2
0
dx
=
 1
0

x7 + 3
2 x6

dx = 1
8 + 3
14 = 7 + 12
56
= 19
56,
hence
P {(X, y) âˆˆD1} = P {(X, Y ) âˆˆD3} = 19
56,
and
P {(X, Y ) âˆˆD2} = 1 âˆ’19
28 = 9
28.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables II
 
94 
5. Miscellaneous examples
Example 5.9 Given for every k âˆˆ]0, 1[ a function fk by
fk(x) =
ï£±
ï£²
ï£³
k eâˆ’x + 2(1 âˆ’k)eâˆ’2x
for x > 0,
0
for x â‰¤0.
1) Prove that fk(x) is a frequency of a random variable, which is denoted by Xk.
2) Find the distribution function Fk(x) of Xk.
3) Find the mean and variance of Xk.
4) Find the median of X 1
2 .
5) The random variable Yk is given by
Yk = exp
Xk
2

.
Find the distribution function and the frequency of Yk, and compute the mean E {Yk}.
6) Then assume that the random variable X 1
2 is observed twice by independent observations. Find
the probability that the second observation is bigger than the half of the ï¬rst one.
1) When k âˆˆ]0, 1[, then fk(x) â‰¥0. Then by an integration
 âˆ
âˆ’âˆ
fk(x) dx =
 âˆ
0

k eâˆ’x + 2(1 âˆ’k)eâˆ’2k
dx = k + 2 Â· 1
2 (1 âˆ’k) = 1,
thus fk(x) is a frequency of a random variable Xk.
2) When x â‰¤0, then Fk(x) = 0. When x > 0, then
Fk(x)
=
 x
0

k eâˆ’t + 2(1 âˆ’k)eâˆ’2t
dt =

âˆ’k eâˆ’t âˆ’(1 âˆ’k)eâˆ’2tx
0
=
1 âˆ’k eâˆ’x âˆ’(1 âˆ’k)eâˆ’2k,
hence summing up,
Fk(x) =
ï£±
ï£²
ï£³
1 âˆ’k eâˆ’x âˆ’(1 âˆ’k)eâˆ’2x
for x > 0,
0
otherwise.
3) The mean is
E{X} =
 âˆ
0

k Â· x eâˆ’x + (1 âˆ’k) Â· 2x eâˆ’2x
dx = k Â· 1! + 1
2 (1 âˆ’k) Â· 1! = 1 + k
2
.
Furthermore,
E

X2
=
 âˆ
0

k Â· x2eâˆ’x + 2(1 âˆ’k) Â· x2eâˆ’2x
dx
=
k Â· 2! + 1
4 (1 âˆ’k)2! = 2k + 1 âˆ’k
2
= 1 + 3k
2
,
Download free eBooks at bookboon.com

Random variables II
 
95 
5. Miscellaneous examples
so the variance becomes
V {X} = 1 + 3k
2
âˆ’
1 + k
2
2
= 1
4

2 + 6k âˆ’1 âˆ’2k âˆ’k2
= 1 + 4k âˆ’k2
4
.
4) The median

X 1
2

is the solution of F 1
2 (x) = 1
2, i.e. of the equation
1 âˆ’1
2 eâˆ’x âˆ’1
2

eâˆ’x2 = 1
2.
This is rewritten as the equation of second degree in eâˆ’x,

eâˆ’x2 +

eâˆ’x
âˆ’1 = 0,
hence
eâˆ’x = âˆ’1
2 (Â±)
âˆš
5
2
=
âˆš
5 âˆ’1
2
=
2
âˆš
5 + 1,
and whence

X 1
2

= ln
âˆš
5 + 1
2

.
5) The image of Yk is ]1, âˆ[. When y âˆˆ]1, âˆ[, then
FYk(y)
=
P

Yk = exp
Xk
2

â‰¤y

= P {Xk â‰¤2 ln y} = Fk(2 ln y)
=
1 âˆ’k eâˆ’2 ln y âˆ’(1 âˆ’k)eâˆ’2Â·2 ln y = 1 âˆ’k
y2 âˆ’1 âˆ’k
y4
,
hence the distribution function is
FYk(y) =
ï£±
ï£´
ï£²
ï£´
ï£³
1 âˆ’k
y2 âˆ’1 âˆ’k
y4
for y > 1,
0
for y â‰¤1.
The corresponding frequency is obtained by a diï¬€erentiation,
fYk(y) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
2k
y3 + 4(1 âˆ’k)
y5
for y > 1,
0
for y â‰¤1.
The mean is
E {Yk} =
 âˆ
1
y fYk(y) dy =
 âˆ
1
2k
y2 + 4(1 âˆ’k)
y4

dy = 2k + 4
3 (1 âˆ’k) = 2
3 (k + 2).
Download free eBooks at bookboon.com

Random variables II
 
96 
5. Miscellaneous examples
0
0.1
0.2
0.3
0.4
0.5
0.2
0.4
0.6
0.8
1
Figure 18: The domain of integration of question 6 lies in the ï¬rst quadrant above the oblique line.
6) Let Xâ€²
1/2 and Xâ€²â€²
1/2 be two independent random variables, both of the frequency f1/2. Then
P

Xâ€²â€²
1/2 > 1
2 Xâ€²
1/2

=
 âˆ
x=0
 âˆ
y= 1
2 x
f1/2(x) f1/2(y) dy

dx
=
 âˆ
x=0
1
2

eâˆ’x + 2 eâˆ’2x
Â·
 âˆ
y= 1
2 x
1
2

eâˆ’y + 2 eâˆ’2y
dy

dx
=
 âˆ
x=0
1
2

eâˆ’x + 2 eâˆ’2x
Â·
1
2 eâˆ’x/2 + 1
2 eâˆ’x

dx
=
 âˆ
0
1
4 eâˆ’3x/2 + 1
4 eâˆ’2x + 1
2 eâˆ’5x/2 + 1
2 eâˆ’3x

dx
= 1
4 Â· 2
3 + 1
4 Â· 1
2 + 1
2 Â· 2
5 + 1
2 Â· 1
3 = 79
120.
Download free eBooks at bookboon.com

Random variables II
 
97 
5. Miscellaneous examples
Example 5.10 A rectangle has the side lengths X1 and X2, where X1 and X2 are independent random
variables, and where X1 and X1 are both rectangularly distributed over ]1, 2[.
Let Y1 = 2X1 + 2X2 denote the circumference of the rectangle, and let Y2 = X1X2 denote the area of
the rectangle.
1) Compute the mean and the variance of Y1.
2) Compute the mean and the variance of Y2.
3) Compute the covariance Cov(Y1, Y2).
4) Compute the correlation coeï¬ƒcient Ï± (Y1, Y2).
5) Compute the frequency of Y1.
6) Compute the frequency of Y2.
1) Since X1 and X2 are independent, and e.g.
E {Xi} =
 2
1
t dt = 3
2,
which of course also can be seen directly, we get
E {Y1} = 2 E {X1} + 2 E {X2} = 4
 2
1
t dt = 4 Â· 3
2 = 6,
and
V {Y1} = 22V {X1} + 22V {X2} = 8
 2
1

t âˆ’3
2
2
dt = 8
3

t âˆ’3
2
32
1
= 2
3,
just to demonstrate a couple of the possible variants. (There are of course more direct method by
e.g. applying that the mean and variance are known for the rectangular distribution).
2) For the same reason,
E {Y2} = E {X1} Â· E {X2} = 3
2 Â· 3
2 = 9
4.
Furthermore,
E

Y 2
2

= E

X2
1

Â· E

X2
2

=
 2
1
x2dx
2
=
1
3 x3
2
1
2
=
7
3
2
= 49
9 ,
hence
V {Y2} = E

Y 2
2

âˆ’(E {Y2})2 = 49
9 âˆ’
9
4
2
= 49
9 âˆ’81
16 = 55
144.
Download free eBooks at bookboon.com

Random variables II
 
98 
5. Miscellaneous examples
3) Since the covariance is bilinear, we get by insertion of Y1 = 2X1 + 2X2 and Y2 = X1X2 that
Cov (Y1, Y2)
=
Cov (2X1 + 2X2, X1X2)
=
2 Cov (X1, X1X2) + 2 Cov (X2, X1X2) = 4 Cov (X1, X1X2)
=
4 (E {X1 Â· X1X2} âˆ’E {X1} Â· E {X1X2})
=
4

E

X2
1

Â· E {X2} âˆ’(E {X1})2 Â· E {X2}

=
4

E

X2
1

âˆ’(E {X1})2
Â· E {X2} = 4 V {X1} Â· E {X2}
=
4 Â· 1
12 Â· 3
2 = 1
2,
because it follows by question 1 that
V {X1} = 1
8 V {Y1} = 1
8 Â· 2
3 = 1
12,
which we also can obtain directly by using that X1 is rectangularly distributed.
4) We have now
Cov (Y1, Y2) = 1
2,
V {Y1} = 2
3,
V {Y2} = 55
144,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables II
 
99 
5. Miscellaneous examples
so the correlation coeï¬ƒcient is
Ï± (Y1, Y2) =
Cov (Y1, Y2)

X {Y1} Â· V {Y2}
=
1
2

2
3 Â· 55
144
=
âˆš
3 Â· 12
2
âˆš
110 = 6
âˆš
3
âˆš
110 =

54
55 = 3
âˆš
330
55
,
where there are more possibilities of the indication of the result.
5) First compute the frequency of X1 + X2:
g(s) =
 âˆ
âˆ’âˆ
f(x) f(s âˆ’x) dx,
where
f(x) =
ï£±
ï£²
ï£³
1
for x âˆˆ]1, 2[,
0
otherwise.
If g(s) Ì¸= 0, then we must have the restrictions
1 < x < 2
og
1 < s âˆ’x < 2,
i.e. after a rearrangement
1 < x < 2
and
s âˆ’2 < x < s âˆ’1.
Then we have two possibilities,
a) When s âˆˆ]2, 3[, then g(s) =
 sâˆ’1
1
1 dx = s âˆ’2.
b) When s âˆˆ]3, 4[, then g(s) =
 2
sâˆ’2 1 dx = 4 âˆ’s.
0
0.2
0.4
0.6
0.8
1
1
2
3
4
Figure 19: The graph of g(s).
Summing up we get
g(s) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
s âˆ’2,
for 2 â‰¤s â‰¤3,
4 âˆ’s,
for 3 < s â‰¤4,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
100 
5. Miscellaneous examples
The random variable Y1 = 2 (X1 + X2) has the frequency
h(s) = 1
2 g
s
2

,
where s
2 âˆˆ]2, 4[ for s âˆˆ]4, 8[, i.e.
(2) h(s) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
1
2
s
2 âˆ’2

= s
4 âˆ’1,
for 4 â‰¤s â‰¤6,
1
2

4 âˆ’s
2

= 2 âˆ’s
4,
for 6 < s â‰¤8,
0,
otherwise.
Alternatively, it is seen that 2X1 and 2X2 are both rectangularly distributed over ]2, 4[.
Alternatively we consider a ï¬gure in order to determine the the distribution function of 2X1 +
2X2. We have two cases:
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 20: When 4 â‰¤s â‰¤6, then the distribution function H(s) is the area of the triangle of the
ï¬gure.
a) When 4 â‰¤s â‰¤6, then the distribution function is equal to the area of the triangle on ï¬gure 5,
the smaller sides of which both have the length s
2 âˆ’1, thus
H(s) = 1
2
s
2 âˆ’2
2
.
We get the frequency by a diï¬€erentiation,
h(s) =
s
2 âˆ’2

Â· 1
2 = s
4 âˆ’1.
b) When 6 â‰¤s â‰¤8, then the distribution function is equal to the area of the square minus the
area of the triangle on ï¬gure 5a, hence
H(s) = 1 âˆ’1
2

4 âˆ’s
2
2
.
Download free eBooks at bookboon.com

Random variables II
 
101 
5. Miscellaneous examples
0
0.5
1
1.5
2
2.5
0.5
1
1.5
2
Figure 21: When 4 â‰¤s â‰¤6, then the distribution function H(s) is the area of the square minus the
area of the triangle on the ï¬gure.
We get the frequency by a diï¬€erentiation,
h(s) =

4 âˆ’s
2

Â· 1
2 = 2 âˆ’s
2.
Summing up that we again obtain (2).
6) The frequency of Y2 = X1X2 is
k(s) =
 âˆ
âˆ’âˆ
f(x) f
 s
x
 1
|x| dx.
If the integrand is Ì¸= 0, then we must have 1 < x < 2 and 1 < s
x < 2, thus
1 < x < 2
and
s
2 < x < s.
Again we have two cases.
a) If s âˆˆ]1, 2[, then
k(s) =
 s
1
1
x dx = ln s.
b) If s âˆˆ]2, 4[, then
k(s) =
 2
s
2
1
x dx = ln 2 âˆ’ln s
2 = ln 4 âˆ’ln s.
Hence we get
(3) k(s) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
ln s,
for 1 < s â‰¤2,
ln 4 âˆ’ln s = ln 4
s,
for 2 < s < 4,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
102 
5. Miscellaneous examples
Alternatively one may again apply a consideration of a ï¬gure in the determination of the
distribution function of X1X2, where we again must consider two cases:
a) When 1 < s < 2, then the distribution function H(s) is equal to the area of the curvilinear
triangle on the ï¬gure on the next page, hence
K(s) =
 s
1
 s
x âˆ’1

dx = s ln s âˆ’s + 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
103 
5. Miscellaneous examples
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 22: The distribution function H(s) is the area of the curvilinear triangle.
We obtain the frequency by a diï¬€erentiation,
k(s) = ln 2,
for 1 < s < 2.
0
0.5
1
1.5
2
2.5
0.5
1
1.5
2
Figure 23: The distribution function is the area of the square minus the area of the curvilinear triangle.
b) When 2 < s < 4, then H(s) is the area of the square minus the area of the curvilinear triangle,
hence
K(s) = 1 âˆ’
 2
s
2

1 âˆ’
 s
x âˆ’1

dx = 1 âˆ’
 2
s
2

2 âˆ’s
x

dx = âˆ’3 + s ln 4 + s âˆ’s ln s.
We obtain the frequency by a diï¬€erentiation,
k(s) = ln 4 âˆ’ln s,
for 2 < s < 4.
Summing up we again obtain (3).
Download free eBooks at bookboon.com

Random variables II
 
104 
5. Miscellaneous examples
Example 5.11 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
ï£±
ï£²
ï£³
4x(1 âˆ’y),
0 < x < 1, 0 < y < 1,
0,
otherwise.
1) Prove that the random variables X and Y are independent.
2) Find the means E{X} and E{Y }.
3) Find the variances V {X} and V {Y }.
4) Find the frequency of the random variable X âˆ’Y .
5) Let C denote the disc x2 + y2 â‰¤1. Compute P{(X, Y ) âˆˆC}.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 24: The domain D, where f(x, y) Ì¸= 0.
1) It follows immediately that
fX(x) =
ï£±
ï£²
ï£³
2x,
0 < x < 1,
0,
otherwise,
and
fY (y) =
ï£±
ï£²
ï£³
2(1 âˆ’y),
0 < y < 1,
0,
otherwise,
Furthermore,
f(x, y) = fX(x) Â· fY (y),
so X and Y are stochastically independent.
Download free eBooks at bookboon.com

Random variables II
 
105 
5. Miscellaneous examples
2) The means are given by
E{X} =
 1
0
2x2 dx = 2
3
and
E{Y } =
 1
0

2y âˆ’2y2
dy = 1 âˆ’2
3 = 1
3.
3) It follows from
E

X2
=
 1
0
2x3 dx = 1
2,
that
V {X} = 1
2 âˆ’4
9 = 1
18.
Similarly
E

Y 2
=
 1
0

2y2 âˆ’2y3
dy = 2
3 âˆ’1
2 = 1
6,
implies that
V {Y } = 1
6 âˆ’1
9 = 1
18.
4) The random variable Z = X âˆ’Y has its values in ] âˆ’1, 1[. The frequency is for âˆ’1 < z < 1 given
by
fZ(z) =
 âˆ
âˆ’âˆ
fX(x) fY (x âˆ’z) dx.
The integrand is Ì¸= 0, when 0 < x < 1 and 0 < x âˆ’z < 1, i.e. when
0 < x < 1
and
z < x < z + 1.
We shall then split into two cases:
a) If z âˆˆ] âˆ’1, 0], then the domain of integration is ]0, z + 1[, thus
fZ(z)
=
 z+1
0
fX(x) fY (x âˆ’z) dx = 4
 z+1
0
x(1 + z âˆ’x) dx
=

2(1 + z)x2 âˆ’4
3 x3
z+1
0
= 2(1 + z)3 âˆ’4
3 (1 + z)3 = 2
3 (1 + z)3.
b) If z âˆˆ]0, 1[, then the domain of integration is ]z, 1[, thus
fZ(z)
=

2(1 + z)x2 âˆ’4
3 x3
1
z
= 2(1 + z) âˆ’4
3 âˆ’2(1 + z)z2 + 4
3 z3
=
2
3 + 2z âˆ’2
3 z3 âˆ’2z2 = 2
3

1 âˆ’z3 + 3z(1 âˆ’z)

=
2
3

1 + 3z âˆ’3z2 âˆ’z3

= 2
3 (1 âˆ’z)

1 + 4z + z2
.
Download free eBooks at bookboon.com

Random variables II
 
106 
5. Miscellaneous examples
Summing up,
fZ(z) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
2
3

1 + 3z + 3z2 + z3
for z âˆˆ] âˆ’1, 0],
2
3

1 + 3z âˆ’3z2 âˆ’z3
for z âˆˆ]0, 1[,
0
otherwise.
â€“1
â€“0.5
0
0.5
1
â€“1
â€“0.5
0.5
1
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables II
 
107 
5. Miscellaneous examples
c) By considering a ï¬gure it follows by using polar coordinates.
P{(X, Y ) âˆˆC}
=
 
C
f(x, y) dx dy =

Ï€
2
0
 1
0
4r cos Ï• (1 âˆ’r sin Ï•)r dr

dÏ•
=

Ï€
2
0
 1
0

4r2 cos Ï• âˆ’4r3 cos Ï• sin Ï•

dr

dÏ•
=

Ï€
2
0
4
3 cos Ï• âˆ’cos Ï• Â· sin Ï•

dÏ• =
4
3 sin Ï• âˆ’1
2 sin2 Ï•
 Ï€
2
0
=
4
3 âˆ’1
2 = 5
6.
Example 5.12 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2 xy
0 < y < x < 2,
0
otherwise.
1) Find the frequencies fX(x) and fY (y) of the random variables X and Y .
2) Find the means E{X} and E{Y } of the random variables X and Y .
3) Find the medians of the random variable X and Y .
4) Find the frequency fZ(z) of the random variable Z = X + Y .
5) Find the means E{Z} and E
 1
Z

of the random variables Z and 1
Z .
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 25: The domain D, where f(x, y) Ì¸= 0.
1) If 0 < x < 2, then we get by a vertical integration,
fX(x) =
 x
0
1
2 xy dy = 1
4 x3.
If 0 < y < 2, then we get by a horizontal integration,
fY (y) =
 2
y
1
2 xy dx = 1
4 y

4 âˆ’y2
= y âˆ’1
4 y3.
Download free eBooks at bookboon.com

Random variables II
 
108 
5. Miscellaneous examples
Summing up,
fX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
4 x3
0 < x < 2,
0
otherwise,
fY (y) =
ï£±
ï£´
ï£²
ï£´
ï£³
y âˆ’1
4 y3
0 < y < 2,
0
otherwise.
2) The means are given by
E{X} =
 2
0
1
4 x4 dx =
 1
20 x5
2
0
= 32
20 = 8
5,
and
E{Y } =
 2
0

y2 âˆ’1
4 y4

dy =
y3
3 âˆ’y5
20
2
0
= 8
3 âˆ’8
5 = 16
15.
3) The distribution function FX(x), when 0 < x < 2, is given by
FX(x) =
 x
0
1
4 t3 dt = 1
16 x4

= 1
2
for x =
4âˆš
8

,
hence the median is (X) =
4âˆš
8.
The distribution function FY (y), when 0 < y < 2, is given by
FY (y) = 1
2 y2 âˆ’1
16 y4.
The median is given by
1
2 y2 âˆ’1
16 y4 = 1
2,
hence
8y2 âˆ’y4 = 8,
and whence by a rearrangement,

y22 âˆ’8y2 + 8 = 0,
i.e.

y2 âˆ’4
2 = 8.
Therefore, we get y2 = 4 Â±
âˆš
8. However, since also 0 < y < 2, we cannot apply +, so we conclude
that y2 = 4 âˆ’
âˆš
8, which implies that the median is
(Y ) =

4 âˆ’2
âˆš
2.
4) Clearly, Z = X + Y has its values in ]0, 4[. The frequency is
fZ(z) =
 âˆ
âˆ’âˆ
f(x, z âˆ’x) dx,
where the integrand is Ì¸= 0, when 0 < z âˆ’x < x < 2. The conditions are
(4) 0 < x < 2
and
z
2 < x < z,
which both should be fulï¬lled.
When f(x, z âˆ’x) Ì¸= 0, then an integral is given by

f(x, z âˆ’x) dx =
 1
2 x(z âˆ’x) dx = 1
4 zx2 âˆ’1
6 x3 = 1
12 x2(3z âˆ’2x).
Download free eBooks at bookboon.com

Random variables II
 
109 
5. Miscellaneous examples
a) When z âˆˆ]0, 2[, then the domain of integration is
z
2, z

, according to (4). Hence
fZ(z) =
 z
z
2
f(x, z âˆ’x) dx =
 1
12 x2(3z âˆ’2x)
z
z
2
= 1
12 z3 âˆ’1
24 z3 = z3
24.
b) When z âˆˆ]2, 4[, then the domain of integration is
z
2, 2

, according to (4). Hence
fZ(z) =
 2
z
2
f(x, z âˆ’x) dx =
 1
12 x2(3z âˆ’2x)
2
z
2
= 1
3 (3z âˆ’4) âˆ’z3
24 = z âˆ’4
3 âˆ’z3
24.
Summing up,
fZ(z) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
z3
24
for 0 < z â‰¤2,
z âˆ’4
3 âˆ’z3
24
for 2 < z < 4,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
110 
5. Miscellaneous examples
5) The means are
E{Z} = E{X} + E{Y })8
5 + 16
15 = 40
15 = 8
3,
and
E
 1
Z

=
 2
0
z2
24 dz +
 4
2

1 âˆ’4
3 Â· 1
z âˆ’z2
24

dz = 1
9 + 11
9 âˆ’4
3 ln 2 = 4
3 (1 âˆ’ln 2).
Example 5.13 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
ï£±
ï£²
ï£³
eâˆ’|x| Â· eâˆ’y,
y > |x|,
0,
otherwise.
1) Find the frequencies fX(x) and fY (y) of the random variables X and Y .
2) Find the means E{X} and E{Y } of the random variables X and Y .
3) Prove that the random variables X and Y are non-correlated.
4) Check if the random variables X are Y independent.
5) Find the frequency fZ(z) of the random variable Z = X + Y .
0
0.5
1
1.5
2
â€“2
â€“1
1
2
Figure 26: The support of f(x, y) with a couple of paths of integration.
1) Clearly, we must split into the two cases x â‰¥0 and x < 0.
a) If x â‰¥0, then
fX(x) = eâˆ’x
 âˆ
y=x
eâˆ’y dy = eâˆ’2x.
Download free eBooks at bookboon.com

Random variables II
 
111 
5. Miscellaneous examples
b) If x < 0, then
fX(x) = ex
 âˆ
y=âˆ’x
eâˆ’y dy = e+2x = eâˆ’2|x|.
Summing up,
fX(x) = eâˆ’2|x|,
x âˆˆR.
If y â‰¤0, then fY (y) = 0. If y > 0, then
fY (y) = eâˆ’y
 y
âˆ’y
eâˆ’|x| dx = 2eâˆ’y
 y
0
eâˆ’x dx = 2eâˆ’y 
1 âˆ’eâˆ’y
.
Summing up,
fY (y) =
ï£±
ï£²
ï£³
2eâˆ’y (1 âˆ’eâˆ’y)
for y > 0,
0
for y â‰¤0.
2) Due to the exponential factors, the integrals of the means are clearly convergent. We conclude by
the symmetry that
E{X} =
 âˆ
âˆ’âˆ
x eâˆ’2|x| dx = 0.
Furthermore,
E{Y } =
 âˆ
0

2y eâˆ’y âˆ’2y eâˆ’2y
dy = 2 âˆ’1
2 = 3
2.
3) It follows from
E{XY } =
 âˆ
y=0
y eâˆ’y
 y
x=âˆ’y
x eâˆ’|x| dx

dy = 0 = E{X} Â· E{Y },
that X and Y are non-correlated.
4) Since f(x, y) Ì¸= fX(x) fY (y), we conclude that X and Y are not independent.
5) Since f(x, y) is only Ì¸= 0 for y > |x|, it follows that Z = X + Y can only have values > 0. If z > 0,
then
fZ(z) =
 âˆ
âˆ’âˆ
f(x, z âˆ’x) dx.
Since z > 0, the integrand is Ì¸= 0 for x âˆ’z < x < z âˆ’x, hence for x < z
2. Then
fZ(z)
=

z
2
âˆ’âˆ
eâˆ’|x| exâˆ’z dx =
 0
âˆ’âˆ
e2x dx Â· eâˆ’z +

z
2
0
1 dx Â· eâˆ’z
=
1
2 eâˆ’z + z
2 eâˆ’z = 1
2 (1 + z) eâˆ’z.
Download free eBooks at bookboon.com

Random variables II
 
112 
5. Miscellaneous examples
Summing up,
fZ(z) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2 (1 + z) eâˆ’z
for z > 0,
0
for z â‰¤0.
Example 5.14 A rectangular triangle has its shorter sides X1 and X2, where X1 and X2 are inde-
pendent random variables of the frequencies
fX1 (x1) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2 x1,
0 < x1 < 2,
0,
otherwise.
fX2 (x2) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
2
0 < x2 < 2,
0,
otherwise.
Let Y1 = X1 + X2 denote the sum of the lengths of the shorter sides, and let Y2 = 1
2 X1X2 denote the
area of the triangle
1) Compute the mean and the variance of Y1.
2) Compute the mean and the variance of Y2.
3) Compute the frequency of Y1.
4) Compute the frequency of Y2.
5) Check if the random variable Z = X2/X1 has a mean, and if so, ï¬nd it.
We start by the following computations,
E {X1} =
 2
0
1
2 x2
1 dx1 =
1
6 x3
1
2
0
= 4
3,
and
E

X2
1

=
 2
0
1
2 x3
1 dx1 =
1
8 x4
1
2
0
= 2,
thus the variance of X1 is
V {X1} = 2 âˆ’16
9 = 2
9.
Analogously,
E {X2} =
 2
0
1
2 x2 dx2 =
1
4 x2
2
2
0
= 1,
Download free eBooks at bookboon.com

Random variables II
 
113 
5. Miscellaneous examples
and
E

X2
2

=
 2
0
1
2 x2
2 dx2 =
1
6 x3
2
2
0
= 4
3,
hence
V {X2} = 4
3 âˆ’12 = 1
3,
which also follows directly from the fact that X2 is rectangularly distributed over ]0, 2[.
Since X1 and X2 are stochastically independent, the following computations become much easier.
1) The mean and variance of Y1 are
E {Y1} = E {X1 + X2} = E {X1} + E {X2} = 4
3 + 1 = 7
3,
and
V {Y1} = V {X1 + X2} = V {X1} + V {X2} = 2
9 + 1
3 = 5
9.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables II
 
114 
5. Miscellaneous examples
2) The mean and variance of Y2 are
E {Y2} = E
1
2 X1X2

= 1
2 E {X1} Â· E {X2} = 1
2 Â· 4
3 Â· 1 = 2
3,
and
E

Y 2
2

= 1
4 E

X2
1

Â· E

X2
2

= 1
4 Â· 2 Â· 4
3 = 2
3,
hence
V {Y2} = E

Y 2
2

âˆ’(E {Y2})2 = 2
3 âˆ’
2
3
2
= 2
9.
3) Since X1 and X2 only have values between 0 and 2, it follows that Y1 = X1 + X2 has only values
between 0 and 4, and the frequency of Y1 is given by the convolution integral
fY1 (y1) =
 âˆ
âˆ’âˆ
fX1(x) fX2 (y1 âˆ’x) dx.
This expression is only Ì¸= 0, when 0 < x < 2 and 0 < y1 âˆ’x < 2, so the restrictions are
0 < x < 2
and
y1 âˆ’2 < x < y1.
a) If 0 < y1 â‰¤2, the restrictions are reduced to 0 < x < y1, hence
fY1 (y1) =
 y1
0
1
2 x Â· 1
2 dx = 1
8 y2
1.
b) If 2 < y1 < 4, the restrictions are reduced to y1 âˆ’2 < x < 2, hence
fY1 (y1) =
 2
y1âˆ’2
1
4 x dx =
1
8 x2
2
y1âˆ’2
= 1
2 âˆ’1
8 (y1 âˆ’2)2 = 1
2 y1 âˆ’1
8 y2
1.
Summing up,
fY1 (y1) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
1
8 y2
1,
for 0 < y1 â‰¤2,
1
2 y1 âˆ’1
8 y2
1,
for 2 < y1 < 4,
0,
otherwise.
4) Analogously, Y2 = 1
2 X1X2 = X1 Â·
1
2 X2

has only values between 0 and 2. The rewriting is
convenient, because 1
2 X2 is rectangularly distributed over ]0, 1[ of the frequency
g (x2) =
ï£±
ï£²
ï£³
1,
for x âˆˆ]0, 1[,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
115 
5. Miscellaneous examples
If 0 < y2 < 2, then the frequency of Y2 is given by
fY2 (y2) =
 âˆ
âˆ’âˆ
fX1(x) g
y2
x
 1
|x| dx.
the integrand is Ì¸= 0, when 0 < x < 2 and 0 < y2
x < 1, so we get the restrictions
0 < x < 2
and
0 < y2 < x,
thus
y2 < x < 2.
Hence,
fY2 (y2) =
 2
y2
1
2 x Â· 1 dx
x =
 2
y2
1
2 dx = 1 âˆ’1
2 y2,
and summing up,
fY2 (y2) =
ï£±
ï£´
ï£²
ï£´
ï£³
1 âˆ’1
2 y2
for 0 < y2 < 2,
0
otherwise.
5) Since X1 and C2 aer independent, we get
E{Z} = E

X2 Â· 1
X1

= E {X2} Â· E
 1
X1

= 1 Â·
 2
0
1
x1
Â· 1
2 x1 dx1 =
1
2 x1
2
0
= 1.
In particular, the mean exists.
Remark 5.1 It is possible, though far more diï¬ƒcult ï¬rst to solve the questions 3 and 4, from which
questions 1 and 2 can be derived. These computations are far bigger than the computations above. â™¦
Download free eBooks at bookboon.com

Random variables II
 
116 
Index
Index
2-dimensional random variable, 5
almost everywhere, 7
causal distribution, 4
Ë‡CebyË‡sevâ€™s inequality, 13
conditional distribution, 11
conditional distribution function, 11
conditional probability, 11
continuous distribution, 5, 6
continuous random variable, 5, 6
convergence in distribution, 16
convergence in probability, 16
correlation, 15
correlation coeï¬ƒcient, 21
covariance, 15
discrete distribution, 4, 6
discrete random variable, 4, 6
distribution function, 4
expectation, 11
frequency, 5, 6
Helly-Brayâ€™s lemma, 16
independent random variables, 7
Jacobian, 10
law of total probability, 11, 18
marginal distribution, 5
marginal frequency, 6
mean, 11
median, 4
moment, 12
null-set, 7
Poisson distribution, 58
polar coordinates, 42
probability ï¬eld, 4
quantile, 4
random variable, 4
rectangular distribution, 42, 76, 79, 85, 96, 111,
112
simultaneous distribution, 5
simultaneous distribution function, 6
skewness, 21
transformation theorem, 8
weak law of large numbers, 16
Download free eBooks at bookboon.com


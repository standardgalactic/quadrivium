Leif Mejlbro
Random variables II
Probability Examples c-3
Download free books at

2 
 
 
 
  
Leif Mejlbro
 
Probability Examples c-3
Random variables II
Download free eBooks at bookboon.com

3 
 
 
 
Probability Examples c-3 – Random variables II 
© 2009 Leif Mejlbro & Ventus Publishing ApS 
ISBN 978-87-7681-518-9
 
Download free eBooks at bookboon.com

Random variables II
 
4 
Contents
	
Introduction 	
5
1 	
Some theoretical results 	
6
2 	
Law of total probability 	
20
3 	
Correlation coefficient and skewness 	
23
4 	
Examples concerning the Poisson distribution 	
60
5 	
Miscellaneous examples 	
70
	
Index 	
116
Contents
Download free eBooks at bookboon.com

Random variables II
 
5 
Introduction
Introduction
This is the third book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole Jørsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The topic itself, Random Variables, is so big that I have felt it necessary to divide it into three books,
of which this is the second one. We shall here continue the study of frequencies and distribution
functions in 1 and 2 dimensions, and consider the correlation coeﬃcient. We consider in particular
the Poisson distribution.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ﬁrst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
26th October 2009
Download free eBooks at bookboon.com

Random variables II
 
6 
1. Some theoretical results
1
Some theoretical results
The abstract (and precise) deﬁnition of a random variable X is that X is a real function on Ω, where
the triple (Ω, F, P) is a probability ﬁeld, such that
{ω ∈Ω| X(ω) ≤x} ∈F
for every x ∈R.
This deﬁnition leads to the concept of a distribution function for the random variable X, which is the
function F : R →R, which is deﬁned by
F(x) = P{X ≤x}
(= P{ω ∈Ω| X(ω) ≤x}),
where the latter expression is the mathematically precise deﬁnition which, however, for obvious reasons
everywhere in the following will be replaced by the former expression.
A distribution function for a random variable X has the following properties:
0 ≤F(x) ≤1
for every x ∈R.
The function F is weakly increasing, i.e. F(x) ≤F(y) for x ≤y.
limx→−∞F(x) = 0
and
limx→+∞F(x) = 1.
The function F is continuous from the right, i.e. limh→0+ F(x + h) = F(x)
for every x ∈R.
One may in some cases be interested in giving a crude description of the behaviour of the distribution
function. We deﬁne a median of a random variable X with the distribution function F(x) as a real
number a = (X) ∈R, for which
P{X ≤a} ≥1
2
and
P{X ≥a} ≥1
2.
Expressed by means of the distribution function it follows that a ∈R is a median, if
F(a) ≥1
2
and
F(a−) = lim
h→0−F(x + h) ≤1
2.
In general we deﬁne a p-quantile, p ∈]0, 1[, of the random variable as a number ap ∈R, for which
P {X ≤ap} ≥p
and
P {X ≥ap} ≥1 −p,
which can also be expressed by
F (ap) ≥p
and
F (ap−) ≤p.
If the random variable X only has a ﬁnite or a countable number of values, x1, x2, . . . , we call it
discrete, and we say that X has a discrete distribution.
A very special case occurs when X only has one value. In this case we say that X is causally distributed,
or that X is constant.
Download free eBooks at bookboon.com

Random variables II
 
7 
1. Some theoretical results
The random variable X is called continuous, if its distribution function F(x) can be written as an
integral of the form
F(x) =
 x
−∞
f(u) du,
x ∈R,
where f is a nonnegative integrable function.
In this case we also say that X has a continuous
distribution, and the integrand f : R →R is called a frequency of the random variable X.
Let again (Ω, F, P) be a given probability ﬁeld. Let us consider two random variables X and Y , which
are both deﬁned on Ω. We may consider the pair (X, Y ) as a 2-dimensional random variable, which
implies that we then shall make precise the extensions of the previous concepts for a single random
variable.
We say that the simultaneous distribution, or just the distribution, of (X, Y ) is known, if we know
P{(X, Y ) ∈A}
for every Borel set A ⊆R2.
When the simultaneous distribution of (X, Y ) is known, we deﬁne the marginal distributions of X
and Y by
PX(B) = P{X ∈B} := P{(X, Y ) ∈B × R},
where B ⊆R is a Borel set,
PY (B) = P{Y ∈B} := P{(X, Y ) ∈R × B},
where B ⊆R is a Borel set.
Notice that we can always ﬁnd the marginal distributions from the simultaneous distribution, while it
is far from always possible to ﬁnd the simultaneous distribution from the marginal distributions. We
now introduce
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables II
 
8 
1. Some theoretical results
The simultaneous distribution function of the 2-dimensional random variable (X, Y ) is deﬁned as the
function F : R2 →R, given by
F(x, y) := P{X ≤x ∧Y ≤y}.
We have
• If (x, y) ∈R2, then 0 ≤F(x, y) ≤1.
• If x ∈R is kept ﬁxed, then F(x, y) is a weakly increasing function in y, which is continuous from
the right and which satisﬁes the condition limy→−∞F(x, y) = 0.
• If y ∈R is kept ﬁxed, then F(x, y) is a weakly increasing function in x, which is continuous from
the right and which satisﬁes the condition limx→−∞F(x, y) = 0.
• When both x and y tend towards inﬁnity, then
lim
x, y→+∞F(x, y) = 1.
• If x1, x2, y1, y2 ∈R satisfy x1 ≤x2 and y1 ≤y2, then
F (x2, y2) −F (x1, y2) −F (x2, y1) + F (x1, y2) ≥0.
Given the simultaneous distribution function F(x, y) of (X, Y ) we can ﬁnd the distribution functions
of X and Y by the formulæ
FX(x) = F(x, +∞) =
lim
y→+∞F(x, y),
for x ∈R,
Fy(x) = F(+∞, y) =
lim
x→+∞F(x, y),
for y ∈R.
The 2-dimensional random variable (X, Y ) is called discrete, or that it has a discrete distribution, if
both X and Y are discrete.
The 2-dimensional random variable (X, Y ) is called continuous, or we say that it has a continuous
distribution, if there exists a nonnegative integrable function (a frequency) f : R2 →R, such that the
distribution function F(x, y) can be written in the form
F(x, y) =
 x
−∞
 y
−∞
f(t, u) du

dt,
for (x, y) ∈R2.
In this case we can ﬁnd the function f(x, y) at the diﬀerentiability points of F(x, y) by the formula
f(x, y) = ∂2F(x, y)
∂x∂y
.
It should now be obvious why one should know something about the theory of integration in more
variables, cf. e.g. the Ventus: Calculus 2 series.
We note that if f(x, y) is a frequency of the continuous 2-dimensional random variable (X, Y ), then X
and Y are both continuous 1-dimensional random variables, and we get their (marginal) frequencies
by
fX(x) =
 +∞
−∞
f(x, y) dy,
for x ∈R,
Download free eBooks at bookboon.com

Random variables II
 
9 
1. Some theoretical results
and
fY (y) =
 +∞
−∞
f(x, y) dx,
for y ∈R.
It was mentioned above that one far from always can ﬁnd the simultaneous distribution function from
the marginal distribution function. It is, however, possible in the case when the two random variables
X and Y are independent.
Let the two random variables X and Y be deﬁned on the same probability ﬁeld (Ω, F, P). We say
that X and Y are independent, if for all pairs of Borel sets A, B ⊆R,
P{X ∈A ∧Y ∈B} = P{X ∈A} · P{Y ∈B},
which can also be put in the simpler form
F(x, y) = FX(x) · FY (y)
for every (x, y) ∈R2.
If X and Y are not independent, then we of course say that they are dependent.
In two special cases we can obtain more information of independent random variables:
If the 2-dimensional random variable (X, Y ) is discrete, then X and Y are independent, if
hij = fi · gj
for every i and j.
Here, fi denotes the probabilities of X, and gj the probabilities of Y .
If the 2-dimensional random variable (X, Y ) is continuous, then X and Y are independent, if their
frequencies satisfy
f(x, y) = fX(x) · fY (y)
almost everywhere.
The concept “almost everywhere” is rarely given a precise deﬁnition in books on applied mathematics.
Roughly speaking it means that the relation above holds outside a set in R2 of area zero, a so-called
null set. The common examples of null sets are either ﬁnite or countable sets. There exists, however,
also non-countable null sets. Simple examples are graphs of any (piecewise) C 1-curve.
Concerning maps of random variables we have the following very important results,
Theorem 1.1 Let X and Y be independent random variables. Let ϕ : R →R and ψ : R →R be
given functions. Then ϕ(X) and ψ(Y ) are again independent random variables.
If X is a continuous random variable of the frequency I, then we have the following important theorem,
where it should be pointed out that one always shall check all assumptions in order to be able to
conclude that the result holds:
Download free eBooks at bookboon.com

Random variables II
 
10 
1. Some theoretical results
Theorem 1.2 Given a continuous random variable X of frequency f.
1) Let I be an open interval, such that P{X ∈I} = 1.
2) Let τ : I →J be a bijective map of I onto an open interval J.
3) Furthermore, assume that τ is diﬀerentiable with a continuous derivative τ ′, which satisﬁes
τ ′(x) ̸= 0
for alle x ∈I.
Under the assumptions above Y := τ(X) is also a continuous random variable, and its frequency g(y)
is given by
g(y) =





f

τ −1(y)

·


τ −1′ (y)
 ,
for y ∈J,
0,
otherwise.
We note that if just one of the assumptions above is not fulﬁlled, then we shall instead ﬁnd the
distribution function G(y) of Y := τ(X) by the general formula
G(y) = P{τ(X) ∈] −∞, y]} = P

X ∈τ ◦−1(] −∞, y])

,
where τ ◦−1 = τ −1 denotes the inverse set map.
Note also that if the assumptions of the theorem are all satisﬁed, then τ is necessarily monotone.
At a ﬁrst glance it may be strange that we at this early stage introduce 2-dimensional random variables.
The reason is that by applying the simultaneous distribution for (X, Y ) it is fairly easy to deﬁne the
elementary operations of calculus between X and Y . Thus we have the following general result for a
continuous 2-dimensional random variable.
Theorem 1.3 Let (X, Y ) be a continuous random variable of the frequency h(x, y).
The frequency of the sum X + Y is
k1(z) =
 +∞
−∞h(x, z −x) dx.
The frequency of the diﬀerence X −Y is
k2(z) =
 +∞
−∞h(x, x −z) dx.
The frequency of the product X · Y is
k3(z) =
 +∞
−∞h

x , z
x

· 1
|x| dx.
The frequency of the quotient X/Y is
k4(z) =
 +∞
−∞h(zx , x) · |x| dx.
Notice that one must be very careful by computing the product and the quotient, because the corre-
sponding integrals are improper.
If we furthermore assume that X and Y are independent, and f(x) is a frequency of X, and g(y) is a
frequency of Y , then we get an even better result:
Download free eBooks at bookboon.com

Random variables II
 
11 
1. Some theoretical results
Theorem 1.4 Let X and Y be continuous and independent random variables with the frequencies
f(x) and g(y), resp..
The frequency of the sum X + Y is
k1(z) =
 +∞
−∞f(x)g(z −x) dx.
The frequency of the diﬀerence X −Y is
k2(z) =
 +∞
−∞f(x)g(x −z) dx.
The frequency of the product X · Y is
k3(z) =
 +∞
−∞f(x) g
 z
x

· 1
|x| dx.
The frequency of the quotient X/Y is
k4 =
 +∞
−∞f(zx)g(x) · |x| dx.
Let X and Y be independent random variables with the distribution functions FX and FY , resp.. We
introduce two random variables by
U := max{X, Y }
and
V := min{X, Y },
the distribution functions of which are denoted by FU and FV , resp.. Then these are given by
FU(u) = FX(u) · FY (u)
for u ∈R,
and
FV (v) = 1 −(1 −FX(v)) · (1 −FY (v))
for v ∈R.
These formulæ are general, provided only that X and Y are independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables II
 
12 
1. Some theoretical results
If X and Y are continuous and independent, then the frequencies of U and V are given by
fU(u) = FX(u) · fY (u) + fX(u) · FY (u),
for u ∈R,
and
fV (v) = (1 −FX(v)) · fY (v) + fX(v) · (1 −Fy(v)) ,
for v ∈R,
where we note that we shall apply both the frequencies and the distribution functions of X and Y .
The results above can also be extended to bijective maps ϕ = (ϕ1 , ϕ2) : R2 →R2, or subsets of R2.
We shall need the Jacobian of ϕ, introduced in e.g. the Ventus: Calculus 2 series.
It is important here to deﬁne the notation and the variables in the most convenient way. We start
by assuming that D is an open domain in the (x1 x2) plane, and that ˜D is an open domain in the
(y1 , y2) plane. Then let ϕ = (ϕ1 , ϕ2) be a bijective map of ˜D onto D with the inverse τ = ϕ−1, i.e.
the opposite of what one probably would expect:
ϕ = (ϕ1 , ϕ2) : ˜D →D,
with (x1 , x2) = ϕ (y1 , y2) .
The corresponding Jacobian is deﬁned by
Jϕ = ∂(x1 , x2)
∂(y1 , y2) =

∂ϕ1
∂y1
∂ϕ2
∂y1
∂ϕ1
∂y1
∂ϕ2
∂y2

,
where the independent variables (y1 , y2) are in the “denominators”. Then recall the Theorem of
transform of plane integrals, cf. e.g. the Ventus: Calculus 2 series: If h : D →R is an integrable
function, where D ⊆R2 is given as above, then for every (measurable) subset A ⊆D,

A
h (x1 , x2) dx1dx2 =

ϕ−1(A)
h (x1 , x2) ·

∂(x1 , x2)
∂(y1 , y2)
 dy1dy2.
Of course, this formula is not mathematically correct; but it shows intuitively what is going on:
Roughly speaking we “delete the y-s”. The correct mathematical formula is of course the well-known

A
h (x1 , x2) dx1dx2 =

ϕ−1(A)
(ϕ1 (y1 , y2) , ϕ2 (y1 , y2)) ·
Jϕ (y1 , y2)
 dy1dy2,
although experience shows that it in practice is more confusing then helping the reader.
Download free eBooks at bookboon.com

Random variables II
 
13 
1. Some theoretical results
Theorem 1.5 Let (X1, X2) be a continuous 2-dimensional random variable with the frequency h (x1 , x2).
Let D ⊆R2 be an open domain, such that
P {(X1 , X2) ∈D} = 1.
Let τ : D →˜D be a bijective map of D onto another open domain ˜D, and let ϕ = (ϕ1 , ϕ2) =
τ −1, where we assume that ϕ1 and ϕ2 have continuous partial derivatives and that the corresponding
Jacobian is diﬀerent from 0 in all of ˜D.
Then the 2-dimensional random variable
(Y1 , Y2) = τ (X1 , X2) = (τ1 (X1 , X2) , τ2 (X1 , X2))
has the frequency k (y1 , y2), given by
k (y1 , y2) =







h (ϕ1 (y1 , y2) , ϕ2 (y1 , y2)) ·

∂(x1 , x2)
∂(y1 , y2)
 ,
for (y1 , y2) ∈˜D,
0,
otherwise
We have previously introduced the concept conditional probability. We shall now introduce a similar
concept, namely the conditional distribution.
If X and Y are discrete, we deﬁne the conditional distribution of X for given Y = yj by
P {X = xi | Y = yj} = P {X = xi ∧Y = yj}
P {Y = yj}
= hij
gj
.
It follows that for ﬁxed j we have that P {X = xi | Y = yj} indeed is a distribution. We note in
particular that we have the law of the total probability
P {X = xi} =

j
P {X = xi | Y = yj} · P {Y = yj} .
Analogously we deﬁne for two continuous random variables X and Y the conditional distribution
function of X for given Y = y by
P{X ≤x | Y = y} =
 x
−∞f(u, y) du
fY (y)
,
forudsat, at fY (y) > 0.
Note that the conditional distribution function is not deﬁned at points in which fY (y) = 0.
The corresponding frequency is
f(x | y) = f(x, y)
fY (y) ,
provided that fY (y) = 0.
We shall use the convention that “0 times undeﬁned = 0”. Then we get the Law of total probability,
 +∞
−∞
f(x | y) · fY (y) dy =
 +∞
−∞
f(x, y) dy = fX(x).
We now introduce the mean, or expectation of a random variable, provided that it exists.
Download free eBooks at bookboon.com

Random variables II
 
14 
1. Some theoretical results
1) Let X be a discrete random variable with the possible values {xi} and the corresponding proba-
bilities pi = P {X = xi}. The mean, or expectation, of X is deﬁned by
E{X} :=

i
xi pi,
provided that the series is absolutely convergent. If this is not the case, the mean does not exists.
2) Let X be a continuous random variable with the frequency f(x). We deﬁne the mean, or expectation
of X by
E{X} =
 +∞
−∞
x f(x) dx,
provided that the integral is absolutely convergent. If this is not the case, the mean does not exist.
If the random variable X only has nonnegative values, i.e. the image of X is contained in [0, +∞[,
and the mean exists, then the mean is given by
E{X} =
 +∞
0
P{X ≥x} dx.
Concerning maps of random variables, means are transformed according to the theorem below, pro-
vided that the given expressions are absolutely convergent.
Theorem 1.6 Let the random variable Y = ϕ(X) be a function of X.
1) If X is a discrete random variable with the possible values {xi} of corresponding probabilities
pi = P{X = xi}, then the mean of Y = ϕ(X) is given by
E{ϕ(X)} =

i
ϕ (xi) pi,
provided that the series is absolutely convergent.
2) If X is a continuous random variable with the frequency f(x), then the mean of Y = ϕ(X) is
given by
E{ϕ(X)} =
 +∞
−∞
ϕ(x) g(x) dx,
provided that the integral is absolutely convergent.
Assume that X is a random variable of mean µ. We add the following concepts, where k ∈N:
The k-th moment,
E

Xk
.
The k-th absolute moment,
E

|X|k
.
The k-th central moment,
E

(X −µ)k
.
The k-th absolute central moment,
E

|X −µ|k
.
The variance, i.e. the second central moment,
V {X} = E

(X −µ)2
,
Download free eBooks at bookboon.com

Random variables II
 
15 
1. Some theoretical results
provided that the deﬁning series or integrals are absolutely convergent. In particular, the variance is
very important. We mention
Theorem 1.7 Let X be a random variable of mean E{X} = µ and variance V {X}. Then
E

(X −c)2
= V {X} + (µ −c)2
for every c ∈R,
V {X} = E

X2
−(E{X})2
for c = 0,
E{aX + b} = a E{X} + b
for every a, b ∈R,
V {aX + b} = a2V {X}
for every a, b ∈R.
It is not always an easy task to compute the distribution function of a random variable. We have the
following result which gives an estimate of the probability that a random variable X diﬀers more than
some given a > 0 from the mean E{X}.
Theorem 1.8 (ˇCebyˇsev’s inequality). If the random variable X has the mean µ and the variance
σ2, then we have for every a > 0,
P{|X −µ| ≥a} ≤σ2
a2 .
If we here put a = kσ, we get the equivalent statement
P{µ −kσ < X < µ + kσ} ≥1 −1
k2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables II
 
16 
1. Some theoretical results
These concepts are then generalized to 2-dimensional random variables. Thus,
Theorem 1.9 Let Z = ϕ(X, Y ) be a function of the 2-dimensional random variable (X, Y ).
1) If (X, Y ) is discrete, then the mean of Z = ϕ(X, Y ) is given by
E{ϕ(X, Y )} =

i, j
ϕ (xi , yj) · P {X = xi ∧Y = yj} ,
provided that the series is absolutely convergent.
2) If (X, Y ) is continuous, then the mean of Z = ϕ(X, Y ) is given by
E{ϕ(X, Y )} =

R2 ϕ(x, y) f(x, y) dxdy,
provided that the integral is absolutely convergent.
It is easily proved that if (X, Y ) is a 2-dimensional random variable, and ϕ(x, y) = ϕ1(x) + ϕ2(y),
then
E {ϕ1(X) + ϕ2(Y )} = E {ϕ1(X)} + E {ϕ2(Y )} ,
provided that E {ϕ1(X)} and E {ϕ2(Y )} exists. In particular,
E{X + Y } = E{X} + E{Y }.
If we furthermore assume that X and Y are independent and choose ϕ(x, y) = ϕ1(x)·ϕ2(y), then also
E {ϕ1(X) · ϕ2(Y )} = E {ϕ1(X)} · E {ϕ2(Y )} ,
provided that E {ϕ1(X)} and E {ϕ2(Y )} exists. In particular we get under the assumptions above
that
E{X · Y } = E{X} · E{Y },
and
E{(X −E{X}) · (Y −E{Y })} = 0.
These formulæ are easily generalized to n random variables. We have e.g.
E
 n

i=1
Xi

=
n

i=1
E {Xi} ,
provided that all means E {Xi} exist.
If two random variables X and Y are not independent, we shall ﬁnd a measure of how much they
“depend” on each other. This measure is described by the correlation, which we now introduce.
Consider a 2-dimensional random variable (X, Y ), where
E{X} = µX,
E{Y } = µY ,
V {X} = σ2
X > 0,
V {Y } = σ2
Y > 0,
Download free eBooks at bookboon.com

Random variables II
 
17 
1. Some theoretical results
all exist. We deﬁne the covariance between X and Y , denoted by Cov(X, Y ), as
Cov(X, Y ) := E {(X −µX) · (Y −µY )} .
We deﬁne the correlation between X and Y , denoted by ϱ(X, Y ), as
ϱ(X, Y ) := Cov(X, Y )
σX · σY
.
Theorem 1.10 Let X and Y be two random variables, where
E{X} = µX,
E{Y } = µY ,
V {X} = σ2
X > 0,
V {Y } = σ2
Y > 0,
all exist. Then
Cov(X, Y ) = 0,
if X and Y are independent,
Cov(X, Y ) = E{X · Y } −E{X} · E{Y },
|Cov(X, Y )| ≤σX · σy,
Cov(X, Y ) = Cov(Y, X),
V {X + Y } = V {X} + V {Y } + 2Cov(X, Y ),
V {X + Y } = V {X} + V {Y },
if X and Y are independent,
ϱ(X, Y ) = 0,
if X and Y are independent,
ϱ(X, X) = 1,
ϱ(X, −X) = −1,
|ϱ(X, Y )| ≤1.
Let Z be another random variable, for which the mean and the variance both exist- Then
Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z),
for every a, b ∈R,
and if U = aX + b and V = cY + d, where a > 0 and c > 0, then
ϱ(U, V ) = ϱ(aX + b, cY + d) = ϱ(X, Y ).
Two independent random variables are always non-correlated, while two non-correlated random vari-
ables are not necessarily independent.
By the obvious generalization,
V
 n

i=1
Xi

=
n

i=1
V {Xi} + 2
n

j=2
j−1

i=1
Cov (Xi, Xj) .
If all X1, X2, . . . , Xn are independent of each other, this is of course reduced to
V
 n

i=1
Xi

=
n

i=1
V {Xi} .
Finally we mention the various types of convergence which are natural in connection with sequences
of random variables. We consider a sequence Xn of random variables, deﬁned on the same probability
ﬁeld (Ω, F, P).
Download free eBooks at bookboon.com

Random variables II
 
18 
1. Some theoretical results
1) We say that Xn converges in probability towards a random variable X on the probability ﬁeld
(Ω, F, P), if
P {|Xn −X| ≥ε} →0
for n →+∞,
for every ﬁxed ε > 0.
2) We say that Xn converges in probability towards a constant c, if every ﬁxed ε > 0,
P {|Xn −c| ≥ε} →0
for n →+∞.
3) If each Xn has the distribution function Fn, and X has the distribution function F, we say that
the sequence Xn of random variables converges in distribution towards X, if at every point of
continuity x of F(x),
lim
n→+∞Fn(x) = F(x).
Finally, we mention the following theorems which are connected with these concepts of convergence.
The ﬁrst one resembles ˇCebyˇsev’s inequality.
Theorem 1.11 (The weak law of large numbers). Let Xn be a sequence of independent random
variables, all deﬁned on (Ω, F, P), and assume that they all have the same mean and variance,
E {Xi} = µ
and
V {Xi} = σ2.
Then for every ﬁxed ε > 0,
P

1
n
n

i=1
Xi −µ
 ≥ε

→0
for n →+∞.
A slightly diﬀerent version of the weak law of large numbers is the following
Theorem 1.12 If Xn is a sequence of independent identical distributed random variables, deﬁned
on (Ω, F, P) where E {Xi} = µ, (notice that we do not assume the existence of the variance), then
for every ﬁxed ε > 0,
P

1
n
n

i=1
Xi −µ
 ≥ε

→0
for n →+∞.
We have concerning convergence in distribution,
Theorem 1.13 (Helly-Bray’s lemma). Assume that the sequence Xn of random variables con-
verges in distribution towards the random variable X, and assume that there are real constants a and
b, such that
P {a ≤Xn ≤b} = 1
for every n ∈N.
If ϕ is a continuous function on the interval [a, b], then
lim
n→+∞E {ϕ (Xn)} = E{ϕ(X)}.
In particular,
lim
n→+∞E {Xn}
and
lim
n→+∞V {Xn} = V {X}.
Download free eBooks at bookboon.com

Random variables II
 
19 
1. Some theoretical results
Finally, the following theorem gives us the relationship between the two concepts of convergence:
Theorem 1.14 1) If Xn converges in probability towards X, then Xn also converges in distribution
towards X.
2) If Xn converges in distribution towards a constant c, then Xn also converges in probability towards
the constant c.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables II
 
20 
2. Law of the total probability
2
Law of total probability
Example 2.1 Given a countable number of boxes: U1, U2, . . . , Un, . . . . Let box number n contain
n slips of paper with the numbers 1, 2, . . . , n. We choose at random with probability pn the box Un,
and from this box we choose randomly one of the slips of paper. Let X denote the random variable,
which indicates the number of the chosen box, and let Y denote the random variable, which gives the
number on the chosen slip of paper.
1) Find the distribution of the random variable Y .
2) Prove that the mean E{Y } exists if and only if the mean E{X} exists. When both these means
exist one shall express E{Y } by means of E{X}.
3) Assume that pn = pqn−1, where p > 0, q > 0 and p + q = 1. Find
P{Y = 1}.
1) It is given that
∞

n=1
pn = 1,
pn ≥0,
and
P{X = b} = pn,
n ∈N,
and
P{Y = k | X = n} =





1
n,
k = 1, . . . , n,
0,
otherwise.
When we apply the law of total probability, it follows for any k ∈N that
P{Y = k}
=
∞

n=1
P{Y = k | X = n} · P{X = n} =
∞

n=k
P{Y = k | X = n} · P{X = n}
=
∞

n=k
1
n pn.
2) Assume that E{Y } exists. Since all terms are ≥0, we can interchange the summations,
E{Y }
=
∞

k=1
k P{Y = k} =
∞

k=1
∞

n=k
k
n · pn =
∞

n=1
n

k=1
k · 1
n pn =
∞

n=1
1
2 n(n + 1) 1
n pn
=
1
2
∞

n=1
(n + 1)pn = 1
2
∞

n=1
npn + 1
2
∞

n=1
pn = 1
2 + 1
2 E{X}.
If on the other hand E{X} exists, then we can reverse all computations above and conclude that
E{Y } exists. In fact, every term is ≥0, so the summations can be interchanged, which gives
E{Y } = 1
2 (1 + E{X}).
Download free eBooks at bookboon.com

Random variables II
 
21 
2. Law of the total probability
3) If pn = pqn−1, it follows from (1) that
P{Y = 1} =
∞

n=1
1
n p qn−1 = p
q
∞

n=1
1
n qn = p
q {−ln(1 −q)} =
p
1 −p ln
1
p

.
Example 2.2 Throw once an (honest) dice and let the random variable N denote the number given
by the dice.
Then ﬂip a coin N times, where N is the random variable above, and let X denote the number of
heads in these throws.
1) Find P{X = 0 ∧N = i} for i = 1, 2, 3, 4, 5, 6.
2) Find P{X = 0}.
3) Find the mean E{X}.
1) If N = i, then X = 0 means that we get tails i times, thus
P{X = 0 ∧N = i} =
1
2

,
i = 1, 2, 3, 4, 5, 6.
2) By the law of total probability,
P{X = 0} =
6

i=1
P{X = 0 ∧N = i} · P{N = i} =
6

i=1
1
2

· 1
6 = 1
6

1 −1
26

= 21
128.
3) We get for j ∈{1, . . . , i}, i ∈{1, . . . , 6},
P{X = j ∧N = i} =

i
j

·
1
2
j
·
1
2
i−j
=

i
j
 1
2
i
,
hence
P{X = j} =
6

i=j
P{X = j ∧N = i} · P{N = i} = 1
6
6

i=j

i
j
 1
2
i
.
Then by interchanging the order of summation,
E{X}
=
6

j=1
j P{X = j} =
6

j=1
j
6
6

i=j
 i
j
 1
2
i
= 1
6
6

i=1
1
2
i
i

j=1
j
 i
j

=
1
6
6

i=1
1
2
i
· i
i

j=1

i −1
j −1

= 1
6
6

i=1
i
1
2
i i−1

k=0

i −1
k

= 1
6
6

i=1
i
1
2
i
2i−1
=
1
12
6

i=1
i = 1
12 · 1
2 · 6 · 7 = 7
4.
Download free eBooks at bookboon.com

Random variables II
 
22 
2. Law of the total probability
Example 2.3 A box contains N balls with the numbers 1, 2, . . . , N. Choose at random a ball from
the box and note its number X, without returning it to the box. Then select another ball and note its
number Y .
1) Find the distribution of the 2-dimensional random variable (X, Y ).
2) Find the distribution of the random variable Z = |X −Y |.
1) It is obvious that
P{(X, Y ) = (k, n)} =





1
N(N −1)
for k, n ∈{1, . . . , N} and k ̸= n,
0
otherwise.
2) Since X ̸= Y , the random variable Z = |X −Y | can only attain the values 1, 2, . . . , N −1. If
n ∈{1, 2, . . . , N −1}, then
P{Z = n}
=
P{|X −Y | = n} = P{X −Y = n} + P{Y −X = n}
=
B

k=1
P{(X, Y ) = (n + k, k)} +
N

k=1
P{(X, Y ) = (k, n + k)}
=
2
N

k=1
P{(X, Y ) = (k, n + k)} = 2
N−n

k=1
P{(X, Y ) = (k, n + k)} = 2
N −n
N(N −1).
Control. It follows that
N−1

n=1
P{Z = n} =
N−1

n=1
2 ·
N −n
N(N −1) =
2
N(N −1)
N−1

n=1
n =
2
N(N −1) · 1
2 (N −1)N = 1.
Download free eBooks at bookboon.com

Random variables II
 
23 
2. Law of the total probability
3
Correlation coeﬃcient and skewness
Example 3.1 A random variable X has its distribution given by
P{X = i} =
1
100,
i = 1, 2, 3, . . . , 98, 99, 100.
Two random variables Y and Z depend on X, such that
Y =



1,
if X can be divided by at least one of the numbers 2 or 3,
0,
otherwise,
and
Z =



1,
if X can be divided by 3,
0,
otherwise.
Compute the correlation coeﬃcient ϱ(Y, Z).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables II
 
24 
2. Law of the total probability
We shall ﬁnd
ϱ(Y, Z) = Cov(Y, Z)
σ1σ2
,
where
Cov(Y, Z) = E{Y Z} −E{Y }E{Z},
and
σ2
1 = V {Y }
and
σ2
2 = V {Z}.
The distribution functions of Y and Z are found by simply counting,
P{Y = 1}
=
P{X even} + P{X odd, and X is divisible by 3}
=
50

n=1
P{X = 2n} +
17

n=1
P{X = 6n −3} = 50
100 + 17
100 = 67
100,
and
P{Z = 1} = P{X is divisible by 3} =
33

n=1
P{X = 3n} = 33
100.
Since Y and Z can only have the values 0 and 1 (where 02 = 0 and 12 = 1), we get
E

Y 2
= E{Y } =
1

i=0
i(2)P{Y = i} = P{Y = 1} = 67
100,
and
E

Z2
= E{Z} =
1

i=0
i(2)P{Z = i} = P{Z = 1} = 33
100,
hence
σ2
1 = V {Y } = E

Y 2
−(E{Y })2 = 67
100 −
 67
100
2
= 67
100 · 33
100,
and
σ2
2 = V {Z} = E

Z2
−(E{Z})2 = 33
100 −
 33
100
2
= 33
100 · 67
100,
whence
σ1σ2 =

67
100 · 33
100 · 33
100 · 67
100 = 33
100 · 67
100.
Finally,
E{Y Z}
=
1

i=0
1

j=0
ij P{Y = i ∧Z = j} = P{Y = 1 ∧Z = 1}
=
P{X is divisible by 3} = P{Z = 1} = 33
100 = E{Z},
Download free eBooks at bookboon.com

Random variables II
 
25 
2. Law of the total probability
so
Cov(Y, Z) = E{Y Z} −E{Y }E{Z} = 33
100

1 −67
100

= 332
1002 .
We derive that the correlation coeﬃcient is
ϱ(Y, Z) = Cov(Y, Z)
σ1σ2
=
332
1002
67
100 · 33
100
= 33
67.
Example 3.2 Let X denote a random variable, for which E{X} = µ, V {X} = σ2 and E

X3
all
exist.
1. Prove the formula
E

(X −µ)3
= E

X3
−µ

3σ2 + µ2
.
When V {X} is bigger than 0, we deﬁne the skewness (asymmetry) of the distribution by the number
γ(X), given by
γ(X) = E

(X −µ)3
σ3
.
A random variable X has the possible values 0, 1, 2, of the corresponding probabilities p, 1
2, 1
2 −p,
where 0 ≤p ≤1
2.
2. Find the number γ(X) of this distribution.
3. Find the values of p, for which γ(X) = 0.
4. Find γ(X) for p = 1
8.
1) The claim is proved in the continuous case.
The proof in the discrete case is analogous.
A
straightforward computation gives
E

(X −µ)3
=
 ∞
−∞
(x −µ)3f(x) dx =
 ∞
−∞

x3 −3µx2 + 3µ2x −µ3
f(x) dx
=
 ∞
−∞
x3f(x) dx −µ
 ∞
−∞

3x2 −3µx + µ2
f(x) dx
=
E

X3
−µ
 ∞
−∞

3x2 −6µx + 3µ2 + 3µx −2µ2
f(x) dx
=
E

X3
−3µ
 ∞
−∞
(x −µ)2f(x) dx −3µ2
 ∞
−∞
x f(x) dx + 2µ3
 ∞
−∞
f(x) dx
=
E

X3
−3µσ2 −2µ2µ + 2µ3 = E

X3
−µ

3σ2 + µ2
.
Download free eBooks at bookboon.com

Random variables II
 
26 
2. Law of the total probability
Alternatively, apply the following direct proof (all cases),
E

(X −µ)3
=
E

X3 −3µX2 + 3µ2X −µ3
= E

X3
−3µ E

X2
+ 3µ2 E{X} −µ3
=
E

X3
−3µ

E

X2
−(E{X})2
−3µ (E{X})2 + 3µ3 −µ3
=
E

X3
−3µσ2 −3µ3 + 3µ3 −µ3
=
E

X3
−µ

3σ2 + µ2
.
2) If
P{X = 0} = p,
P{X = 1} = 1
2
og
P{X = 2} = 1
2 −p,
where 0 ≤p ≤1
2, then
µ = E{X} =
2

i=0
i P{X = i} = 0 · p + 1 · 1
2 + 2
1
2 −p

= 3
2 −2p,
and
E

X2
=
2

i=0
i2P{X = i} = 0 · p + 1 · 1
2 + 4
1
2 −p

= 5
2 −4p,
hence
σ2
=
E

X2
−(E{X})2 = 5
2 −4p −
3
2 −2p
2
= 5
2 −4p −
9
4 −6p + 4p2

=
1
4 + 2p −4p2 = 1
4

1 + 8p −16p2

≥1
4

.
Finally,
E

X3
=
2

i=0
i3P{X = i} = 0 · p + 1 · 1
2 + 8
1
2 −p

= 9
2 −8p,
thus
E

(X −µ)3
=
E

X3
−µ

3σ2 + µ2
= 9
2 −8p −
3
2 −2p
 
3
4 + 6p −12p2 +
3
2 −2p
2
=
9
2 −8p −
3
2 −2p
 3
4 + 6p −12p2 + 9
4 −6p + 4p2

=
9
2 −8p −
3
2 −2p
 
3 −8p2
= 9
2 −8p −
9
2 −12p2 −6p + 16p3

=
9
2 −8p −9
2 + 12p2 + 6p −16p3 = −2p + 12p2 −16p3 = −p

16p2 −12p + 2

=
−16p

p −1
4
 
p −1
2

.
Download free eBooks at bookboon.com

Random variables II
 
27 
2. Law of the total probability
This implies that
γ(X) = E

(X −µ)3
σ3
=
−16p

p −1
4
 
p −1
2

1
8 (1 + 8p −16p2)3/2
= −
128p

p −1
2
 
p −1
4

{2 −(4p −1)2}3/2
.
3) It follows immediately that γ(X) = 0 for p = 0, 1
4, 1
2.
4) If p = 1
8, then
γ(X) = −
128 · 1
8
1
8 −1
2
 1
8 −1
4


2 −
1
2 −1
23/2
= −
16 · 3
8 · 1
8

2 −1
4
3/2 = −
3
4
7
4

7
4
= −6
7
√
7 ≈−0.324.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables II
 
28 
2. Law of the total probability
Example 3.3 Given for any n ∈N a random variable Xn of the frequency
fn(x) =





1
(n −1)! anxn−1e−ax,
x > 0,
0,
otherwise,
where a is a positive constant.
Compute the skewness γ (Xn), and show that γ (Xn) →0 for n →∞.
According to Example 3.2 the skewness γ (Xn) is deﬁned by
γ (Xn) =
E

(Xn −µn)3
σ3n
,
where
E

(Xn −µn)3
= E

X3
n

−µn

3σ2
n + µ2
n

.
By some small computations,
µn = E {Xn} =
an
(n −1)!
 ∞
0
xne−ax dx =
1
a(n −1)!
 ∞
0
tne−1 dt =
n!
a(n −1)! = n
a ,
and
E

X2
n

=
an
(n −1)!
 ∞
0
xn+1e−ax dx =
(n + 1)!
a2(n −1)! = n(n + 1)
a2
,
hence
σ2
n = E

X2
n

−(E {Xn})2 = n(n + 1)
an
−n2
a2 = n
a2 ,
and
E

X3
n

=
an
(n −1)!
 ∞
0
xn+2e−ax dx =
(n + 2)!
a3(n −1)! = n(n + 1)(n + 2)
a3
,
whence
E

(Xn −µn)3
=
E

X3
n

−µn

3σ2
n + µ2
n

= n(n + 1)(n + 2)
a3
−n
a ·
3n
a2 + n2
a2

=
n
a3

n2 + 3n + 2 −3n −n2
= 2n
a3 .
The skewness is
γ (Xn) =
E

(Xn −µn)3
σ3n
= 2n
a3 · a3
n3/2 =
2
√n →0
for n →∞.
Download free eBooks at bookboon.com

Random variables II
 
29 
2. Law of the total probability
Example 3.4 Assume that the 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =





2
A2 ,
0 < y < x < A,
0,
otherwise,
where A is a positive constant.
1) Find the frequencies of X and Y .
2) Find the means of X and Y .
3) Find the variances of X and Y .
4) Compute the correlation coeﬃcient ϱ between X and Y , and prove that it does not depend on A.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 1: The domain where f(x, y) > 0 for A = 1.
1) If x ∈]0, A[, then
fX(x) =
 x
0
2
A2 dy = 2x
A2 ,
and
fX(x) = 0 otherwise.
If y ∈]0, A[, then
fY (y) =
 A
y
2
A2 dx = 2(A −y)
A2
= 2
A −2y
A2 ,
og
fY (y) = 0 otherwise.
2) The means are
E{X} =
 A
0
2x2
A2 dx = 2
3 A,
and
E{Y } =
 A
0
2y
A −2y2
A2

dy =
y2
A −2
3
y3
A2
A
0
= 1
3 A.
Download free eBooks at bookboon.com

Random variables II
 
30 
2. Law of the total probability
3) It follows from
E

X2
=
 A
0
2x3
A2 dx =
 x4
2A2
A
0
= A2
2
that
V {X} = E

X2
−(E{X})2 = A2
2 −4
9 A2 = A2
18 .
It follows from
E

Y 2
=
 A
0
2y2
A −2y3
A2

dy =
2y3
3A −y4
2A2
A
0
=
2
3 −1
2

A2 = A2
6
that
V {Y } = E

Y 2
−(E{Y })2 = A2
6 −A2
9 = A2
18 .
4) First compute
E{XY }
=
 
R2 xy f(x, y) dxdy = 2
A2
 A
0
 x
0
yx dy

dx = 2
A2
 A
0
xy2
2
x
y=0
dx
=
1
A2
 A
0
x3 dx = A2
4 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
31 
2. Law of the total probability
Then by insertion,
Cov(X, Y ) = E{XY } −E{X} · E{Y } = A2
4 −2
3 A · 1
3 A =
1
4 −2
9

A2 = A2
36 .
Finally, we obtain
ϱ(X, Y ) = Cov(X, Y )
σXσy
=
1
36 A2
1
18 A2 = 1
2,
which is independent of A.
Example 3.5 Consider a 2-dimensional random variable (X, Y ), which in the parallelogram given by
the inequalities
0 ≤x ≤1
and
x ≤y ≤x + 1
has the frequency
f(x, y) = 2
3 (x + y),
while the frequency is equal to 0 anywhere else in the (x, y) plane.
1) Find the frequencies of the de random variables X and Y .
2) Find the means of each of the random variables X and Y .
3) Find the covariance Cov(X, Y ).
0
0.5
1
1.5
2
0.2
0.4
0.6
0.8
1
1) When x ∈]0, 1[, it follows by a vertical integration that
fX(x) = 2
3
 x+1
x
(x + y) dy = 1
3

(x + y)2x+1
y=x = 1
3

(2x + 1)2 −(2x)2
= 4
3 x + 1
3,
Download free eBooks at bookboon.com

Random variables II
 
32 
2. Law of the total probability
thus
fX(x) =





4
3 x + 1
3,
x ∈]0, 1[,
0,
otherwise.
If y /∈]0, 2[, then fY (y) = 0.
If y ∈]0, 1[, then by a horizontal integration,
fY (y) = 2
3
 y
0
(x + y)dx = 1
3

(x + y)2y
x=0 = 1
3

(2y)2 −y2
= y2.
If y ∈]1, 2[, it follows again by a horizontal integration that
fY (y)
=
2
3
 1
y−1
(x + y) dy = 1
3

(x + y)21
x=y−1 = 1
3

(y + 1)2 −(2y −1)2
=
1
3

y2 + 2y + 1 −4y2 + 4y −1

= 2y −y2,
hence
fY (y) =











y2,
y ∈]0, 1[,
2y −y2 = 1 −(y −1)2,
y ∈]1, 2[,
0,
otherwise.
2) The means are
E{X} =
 1
0
x
4
3 x + 1
3

dx =
 1
0
4
3 x2 + 1
3 x

dx = 4
9 + 1
6 = 11
18,
and
E{Y }
=
 1
0
y3dy =
 2
1

2y2 −y3
dy = 1
4 +
2
3 y3 −1
4 y4
2
1
=
1
4 + 16
3 −16
4 −2
3 + 1
4 = 14
3 + 1
2 −4 = 2
3 + 1
2 = 7
6.
Download free eBooks at bookboon.com

Random variables II
 
33 
2. Law of the total probability
3) We ﬁrst compute
E{XY }
=
2
3
 1
0
 x+1
x
xy(x + y) dy

dx = 2
3
 1
0
 x+1
x

x2y + xy2
dy

dx
=
2
3
 1
0
1
2 x2y2 + 1
3 xy3
x+1
y=x
dx
=
2
3
 1
0
1
2 x2 
(x + 1)2 −x2
+ 1
3

(x + 1)3 −x3
dx
=
2
3
 1
0
1
2 x2(2x + 1) + 1
3 x

3x2 + 3x + 1

dx
=
2
3
 1
0

x3 + 1
2 x2 + x3 + x2 + 1
3 x

dx
=
2
3
 1
0

2x3 + 3
2 x2 + 1
3 x

dx = 2
3
1
2 + 1
2 + 1
6

= 2
3 · 7
6 = 7
9.
Then by insertion,
Cov(X, Y ) = E{XY } −E{X}E{Y } = 7
9 −11
18 · 7
6 = 7
9 ·

1 −11
12

=
7
108.
Example 3.6 Consider a 2-dimensional random variable (X, Y ), which in the ﬁrst quadrant has the
frequency
h(x, y) =
a
(1 + x + y)5 ,
while the frequency is equal to 0 anywhere else in the (x, y) plane.
1) Find the constant a.
2) Find the distribution function and the frequency of random variable Z = X + Y .
3) Find the mean E{Z} and the variance V {Z}.
1) When we integrate over the ﬁrst quadrant we obtain
1
=
 ∞
0
 ∞
0
h(x, y) dx dy = a
 ∞
0
 ∞
0
(1 + x + y)5 dx dy
=
a
 ∞
0

−1
4 (1 + x + y)−4
∞
x=0
dy = a
4
 ∞
0
(1 + y)−4 dy = a
12,
from which we conclude that a = 12. Hence the frequency is
h(x, y) =





12
(1 + x + y)5
for x > 0 and y > 0,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
34 
2. Law of the total probability
2) The frequency of Z = X + Y for z > 0 is given by
fZ(z) =
 ∞
−∞
h(x, z −x) dx =
 z
0
h(x, z −x) dx =
 z
0
12
(1 + x + z −x)5 dx =
12z
(1 + z)5 ,
i.e.
fZ(z) =





12z
(1 + z)5
for z > 0,
0
otherwise.
The distribution function is FZ(z) = 0 for z ≤0.
If z > 0, then
FZ(z)
=
 z
0
fZ(t) dt = 12
 z
0
t + 1 −1
(t + 1)5 dt =
 z
0

12(t + 1)−4 −12(t + 1)−5
dt
=

−4(t + 1)−3 + 3(t + 1)−4z
0 = 1 −
4
(z + 1)3 +
3
(z + 1)4 = 1 −4z + 1
(z + 1)4 .
Summing up we get
FZ(z) =





1 −4z + 1
(z + 1)4
for z > 0,
0
for z ≤0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables II
 
35 
2. Law of the total probability
3) The mean is
E{Z}
=
 ∞
0
12z2
(z + 1)5 dz = 12
 ∞
0
z2 + 2z + 1 −2z −2 + 1
(z + 1)5
dz
=
 ∞
0

12(z + 1)−3 −24(z + 1)−4 + 12(z + 1)−5
dz
=

−6(z + 1)−2 + 8(z + 1)−3 −3(z + 1)−4∞
0 = 6 −8 + 3 = 1.
We get in the same way,
E

Z2
=
 ∞
0
12z3
(z + 1)5 = 12
 ∞
0
(z3 + 3z2 + 3z + 1) −(3z2 + 6z + 3) + (3 + 3z) −1
(z + 1)5
dz
=
 ∞
0

12(z + 1)−2 −36(z + 1)−3 + 36(z + 1)−4 −12(z + 1)−5
dz
=

−12(z + 1)−1 + 18(z + 1)−2 −12(z + 1)−3 + 3(z + 1)−4∞
0
=
12 −18 + 12 −3 = 3.
Then ﬁnally,
V {Z} = E

Z2
−(E{Z})2 = 3 −1 = 2.
Example 3.7 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =





1
2 x3 e−x(y+1)
for x > 0 and y > 0,
0
otherwise.
1) Find the frequencies of X and Y .
2) Find ϱ(X, Y ).
1) If x > 0, then
fX(x) = 1
2 x3
 ∞
0
e−x(y+1) dy = 1
2 x2 e−x,
and if y > 0, then
fY (y) = 1
2
 ∞
0
x3 e−x(y+1) dx = 1
2 ·
1
(y + 1)4
 ∞
0
t3 e−t dt =
3
(y + 1)4 ,
hence, by summing up,
fX(x) =





1
2 x2 e−x
for x > 0,
0
for x ≤0,
Download free eBooks at bookboon.com

Random variables II
 
36 
2. Law of the total probability
and
fY (y) =





3
(y + 1)4
for y > 0,
0
otherwise.
2) Then we get
E{X} = 1
2
 ∞
0
x3 e−x dx = 3!
2 = 3,
and
E

X2
= 1
2
 ∞
0
x4 e−x dx = 4!
2 = 12,
hence
V {X} = E

X2
−(E{X})2 = 12 −32 = 3.
Analogously we obtain
E{Y } = 3
 ∞
0
y + 1 −1
(y + 1)4 dy = 3
 ∞
0

1
(y + 1)3 −
1
(y + 1)4

dy = 3
1
2 −1
3

= 1
2,
and
E

Y 2
=
3
 ∞
0
y2 + 2y + 1 −2y −2 + 1
(y + 1)4
dy
=
3
 ∞
0

1
(y + 1)2 −
2
(y + 1)3 +
1
(y + 1)4

dy = 3

1 −1 + 1
3

= 1,
so the variance of Y is
V {Y } = E

Y 2
−(E{Y })2 = 1 −1
4 = 3
4.
Finally,
E{XY }
=
 ∞
0
 ∞
0
1
2 x4 y e−x(y+1) dy

dx =
 ∞
0
1
2 x4e−x
 ∞
0
y e−xy dy

dx
=
 ∞
0
1
2 x2e−x dx = 1.
hence
Cov(X, Y ) = E{XY } −E{X} · E{Y } = 1 −3 · 1
2 = −1
2,
and the correlation coeﬃcient is
ϱ(X, Y ) =
Cov(X, Y )

V {X} · V {Y }
=
−1
2

3 · 3
4
= −1
3.
Download free eBooks at bookboon.com

Random variables II
 
37 
2. Law of the total probability
Example 3.8 Let X1 and X2 be independent, identically distributed random variables of the frequency
f(x) =





1
√
2πx exp

−x
2

,
x > 0,
0,
x ≤0.
1) Find the frequency of Y = X1
X2
.
2) Check if E{Y } exists, and if so, ﬁnd E{Y }.
1) Let fY (y) be the frequency of Y = X1
X2
. Then
fY (y) =
 ∞
−∞
f(yx) f(x) |x| dx.
Clearly, fY (y) = 0 for y ≤0.
If y > 0, then
fY (y)
=
 ∞
0
1
√2πyx exp

−yx
2

·
1
√
2πx exp

−x
2

|x| dx
=
1
2π√y
 ∞
0
exp

−y + 1
2
x

dx =
1
2π√y ·
2
y + 1 = 1
π ·
1
y + 1 · 1
√y ,
hence
fY (y) =







1
π ·
1
y + 1 · 1
√y
for y > 0,
0
for y ≤0.
2) Since fY (y) ̸= 0 is equivalent to y > 0 and fY (y) > 0, the integrand satisﬁes y fY (y) ≥0, hence
the check of the existence is reduced to check the convergence for A →∞of
 A
0
y fY /y) dy
=
1
π
 A
0
y
y + 1 · 1
√y dy = 1
π
 A
0
y + 1 −1
y + 1
· 1
√y dy
=
1
π
 A
0
1
√y dy −1
π
 A
0
1
y + 1 · 1
√y dy
=
1
π [2√y]A
0 −2
π [Arctan √y]A
0
=
2
π
√
A −2
π Arctan
√
A.
Since −2
π Arctan
√
A →−2
π · π
2 = −1 and 2
π
√
A →∞for A →∞, we conclude that E{Y } does
not exist.
Alternatively, it follows that the integrand
y
y + 1 · 1
√y ∼
1
√y , and since
 ∞
0
1
√y dy is divergent,
 ∞
0
y
y + 1
1
√y dy is also divergent, and the mean E{Y } does not exist. ♦
Download free eBooks at bookboon.com

Random variables II
 
38 
2. Law of the total probability
Example 3.9 A 2-dimensional random variable (X, Y ) has in the ﬁrst quadrant the frequency
h(x, y) = 1
2 (x + y) e−(x+y),
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find the frequencies of X and Y .
2) Find frequency of Z = X + Y .
3) Find the mean and the variance of the random variable Z.
4) Find the correlation coeﬃcient ϱ(X, Y ).
1) If x > 0, then
fX(x)
=
1
2
 ∞
0
(x + y) e−(x+y) dy = 1
2 x e−x
 ∞
0
e−y dy + 1
2
 ∞
0
y e−y dy
=
1
2 x e−x + 1
2 e−x = 1
2 (x + 1)e−x.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables II
 
39 
2. Law of the total probability
By the symmetry,
fX(x) =





1
2 (x + 1) e−x,
x > 0,
0,
x ≤0,
and
fY (x) =





1
2 (y + 1) e−y,
y > 0,
0,
y ≤0,
2) If z > 0, then Z = X + Y has the frequency
fZ(z) =
 z
0
h(x, z −x) dx =
 z
0
1
2 z e−z dx = 1
2 z2 e−z,
and if z ≤0, the frequency is 0, thus
fZ(z) =

1
2 z2e−z
for z > 0,
0
for z ≤0.
3) We get
E{Z} =
 ∞
0
1
2 z3 e−z dz = 3,
E

Z2
=
 ∞
0
1
2 z4 e−z dz = 12,
and
V {Z} = 12 −32 = 3.
4) First notice that
E{X} = E{Y } = 1
2 (E{X} + E{Y }) = 1
2 E{Z} = 3
2.
Then
E

X2
= E

Y 2
= 1
2
 ∞
0

t3e−t + t2e−t
dt = 1
2 (3! + 2!) = 4,
hence
V {X} = V {Y } = E

X2
−(E{X})2 = 4 −9
4 = 7
4.
We ﬁnally compute
E{XY }
=
1
2
 ∞
0
 ∞
0
xy(x + y) e−(x+y) dx dy
=
1
2
 ∞
0

ye−y
 ∞
0
x2 e−x dx + y2e−y
 ∞
0
x e−x dx

dy
=
1
2
 ∞
0

2! y e−y + 1! y2e−y
dy = 1
2 (2 · 1! + 1 · 2!) = 2,
Download free eBooks at bookboon.com

Random variables II
 
40 
2. Law of the total probability
thus
Cov(X, Y ) = E{XY } −E{X}E{Y } = 2 −3
2 · 3
2 = 2 −9
4 = −1
4,
and
ϱ(X, Y ) =
Cov(X, Y )

V {X}V {Y }
= −1
4
7
4
= −1
7.
Alternatively, it follows from
V {Z} = V {X} + V {Y } + 2 Cov(X, Y ),
that
Cov(X, Y ) = −1
4,
and hence
ϱ(X, Y ) =
Cov(X, Y )

V {X}V {Y }
= −1/4
7/4 = −1
7.
Example 3.10 A compound experiment can be described by ﬁrst choosing at random a real number
X in the interval ]0, 1[, and then at random to choose a real number Y in the interval ]X, 1[. The
frequency of the 2-dimensional random variable (X, Y ) is denoted by h(x, y).
1) Prove that h(x, y) is 0 outside the triangle in the (x, y) plane of the vertices (0, 0), (0, 1) and (1, 1),
and that h(x, y) inside the mentioned triangle above is given by
h(x, y) =
1
1 −x.
2) Find the frequencies f(x) and g(y) of the random variables X and Y .
3) Find the mean and variance of the random variables X and Y .
1) We see that
fX(x) =



1
for x ∈]0, 1[,
0
otherwise.
If we keep x ∈]0, 1[ ﬁxed, then
f(y | x) =





1
1 −x
for y ∈]x, 1[,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
41 
2. Law of the total probability
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Hence, if x ∈]0, 1[, then
f(y | x) = h(x, y)
fX(x) = h(x, y),
and we have proved that
h(x, y) =





1
1 −x
for 0 < x < y < 1,
0
otherwise.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
2) Obviously,
f(x) = fX(x) =



1
for x ∈]0, 1[,
0
otherwise.
If y ∈]0, 1[, then
g(y) =
 y
0
h(x, y) dx =
 y
0
dx
1 −x = [−ln |1 −x|]y
0 = ln
1
1 −y ,
Download free eBooks at bookboon.com

Random variables II
 
42 
2. Law of the total probability
hence
g(y) =





ln
1
1 −y = −ln(1 −y)
for y ∈]0, 1[,
0
otherwise.
3) Clearly,
E{X} = 1
2
and
E

X2
=
 1
0
x2 dx = 1
3,
so
V {X} = E

X2
−(E{X})2 = 1
3 −1
4 = 1
12.
One may of course instead notice that X is rectangularly distributed, so
E{X} = 1
2
and
V {X} = 1
12.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
43 
2. Law of the total probability
Then turn to Y . We get by the change of variable t = 1 −y,
E{Y }
=
 1
0
y {−ln(1 −y)} dy = −
 1
0
(1 −t) ln t dt = −
 1
0
ln t dt +
 1
0
t ln t dt
=
−[t ln t −t]1
0 +
t2
2 ln t
1
0
−1
2
 1
0
t dt = −{0 −1} + 0 −1
2 · 1
2 = 3
4,
and
E

Y 2
=
 1
0
y2{−ln(1 −y)} dy = −
 1
0
(1 −t)2 ln t dt
=
−
 1
0
ln t dt + 2
 1
0
t ln t dt −
 1
0
t2 ln t dt
=
−[t ln t −t]1
0 + 2
t2
2 ln t
1
0
−
 1
0
t dt −
t3
3 ln t
1
0
+ 1
3
 1
0
t2 dt
=
1 + 2 · 0 −1
2 −0 + 1
9 = 1
2 + 1
9 = 11
18.
Alternatively, perform the computations
E{Y } =
 1
x=0
 1
y=x
y ·
1
1 −x dy

dx = 1
2
 1
0
1 −x2
1 −x dx =
 1
0
1
2 (1 + x) dx = 3
4,
and
E

Y 2
=
 1
x=0
 1
y=x
y2 ·
1
1 −x dy

dx = 1
3
 1
0
1 −x3
1 −x dx
=
1
2
 1
0

1 + x + x2
dx = 11
18.
This gives us the variance,
V {Y }
=
E

Y 2
−(E{Y })2 = 11
18 −9
16
=
1
2
11
9 −9
8

= 1
2
2
9 −1
8

= 16 −9
144
=
7
144.
Download free eBooks at bookboon.com

Random variables II
 
44 
2. Law of the total probability
Example 3.11 The point A is in the (x, y) plane given by its polar coordinates r = OA = 1 and
∠(x, OA) = Θ. The projections of A onto the two coordinate axes are called X and Y .
r=1
theta
A
X
Y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
We assume that Θ is a rectangularly distributed random variable over the interval

−π
2 , π
2

.
1) Find the distribution functions and the frequencies of the two random variables X and Y .
2) Find the means E{X} and E{Y }.
3) Find the variances V {X} and V {Y }.
4) Explain that the random variables X and Y are non-correlated, though not independent of each
other.
The frequency of Θ is
f(θ) =





1
π
for x ∈

−π
2 , π
2

,
0
otherwise.
Furthermore, X = cos Θ and Y = sin Θ.
1) Since cos θ > 0 for θ ∈

−π
2 , π
2

, where cos θ is not monotonous, we get for x ∈]0, 1[,
FX(x)
=
P{X ≤x} = P{cos θ ≤x} = P{Arccos x ≤θ ≤π −Arccos x}
=
1
π
 π−Arccos x
Arccos x
dθ = 1 −2
π Arccos x,
hence
FX(x) =













1,
x ≥1,
1 −2
π Arccos x,
0 < x < 1,
0,
x ≤0,
Download free eBooks at bookboon.com

Random variables II
 
45 
2. Law of the total probability
and
fX(x) =





2
π
1
√
1 −x2 ,
x ∈]0, 1[,
0,
otherwise.
Analogously, we get for y ∈] −1, 1[,
FY (y)
=
P{Y ≤y} = P{sin θ ≤y} = P{θ ≤Arcsin y}
=
1
π

Arcsin y
−π
2
dθ = 1
2 + 1
π Arcsin y,
hence
FY (y) =













1,
y ≥1,
1
2 + 1
π Arcsin y,
−1 < y < 1,
0,
y ≤−1,
and
fY (y) =







1
π
1

1 −y2 ,
y ∈] −1, 1[,
0,
otherwise.
2) The means are
E{X} = 2
π
 1
0
x
√
1 −x2 dx = 2
π

−

1 −x2
1
0 = 2
π ,
and
E{Y } = 1
π = 1
π
 1
−1
y

1 −y2 dy = 0.
3) We get by the substitution x = sin t,
E

X2
=
2
π
 1
0
x2
√
1 −x2 dx = 2
π

π
2
0
sin2 t

1 −sin2 t
· cos t dt
=
1
π

π
2
0

sin2 t + cos2 t

dt = 1
2.
Furthermore,
E

Y 2
= 1
π
 1
−1
y2

1 −y2 dy = 2
π
 1
0
y2

1 −y2 dy = E

X2
= 1
2.
Download free eBooks at bookboon.com

Random variables II
 
46 
2. Law of the total probability
The variances are
V {X} = E

X2
−(E{X})2 = 1
2 −
 2
π
2
= 1
2 −4
π2
(≈0, 095),
and
V {Y } = E

Y 2
−(E{Y })2 = 1
2.
4) Since X2 + Y 2 = 1, it is obvious that X and Y are not independent.
Let f(x, y) be the frequency of Z = (X, Y ). Then
f(x, y) = f(x | y) · fY (y) = f(x | y) · 1
π ·
1

1 −y2
for y ∈] −1, 1[,
where
f(x | y) =



1
for x =

1 −y2,
0
otherwise.
Then
E{XY } =
 1
−1

1 −y2 · y · 1
π ·
1

1 −y2 dy = 1
π
 1
−1
y dy = 0,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables II
 
47 
2. Law of the total probability
Thus
Cov(X, Y ) = E{XY } −E{X} · E{Y } = 0 −2
π · 0 = 0,
so X and Y are non-correlated.
Example 3.12 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =



2a2 e−a(x+y),
0 < x < y,
0,
otherwise,
where a is a positive constant.
1) Find the frequencies of the random variables X and Y .
2) Find the means E{X} and E{Y }.
3) Find Cov(X, Y ).
4) Find the frequency of Z = X + Y .
5) Find the mean E{Z} and the variance V {Z}.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1) When x > 0, we get by a vertical integration,
fX(x) =
 ∞
x
2a2 e−a(x+y) dy = 2a e−ax 
−e−ay∞
x = 2a e−2ax,
hence
fX(x) =



2a e−2ax
for x > 0,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
48 
2. Law of the total probability
When y > 0, we get by a horizontal integration,
fY (y) =
 y
0
2a2 e−a(x+y) dx = 2a e−ay 
−e−axy
0 = 2a e−ay −2a e−2ay,
hence
fY (y) =



2a e−ay −2a e−2ay
for y > 0,
0
otherwise.
2) The means are given by
E{X} =
 ∞
0
2a x e−2ax dx = 1
2a
 ∞
0
t e−t dt = 1
2a,
and
E{Y }
=
 ∞
0
2a y e−ay dy −
 ∞
0
2a y e−2ay dy = 2
a
 ∞
0
t e−t dt −1
2a
 ∞
0
t e−t dt
=
2
a −1
2a = 3
2a.
3) Then we compute
E{XY }
=
 ∞
0
 y
0
xy · 2a2e−a(x+y) dx

dy =
 ∞
0
2y e−ay
 y
0
a x e−ax a dx

dy
=
 ∞
0
2y e−ay
 ay
0
t e−t dt

dt =
 ∞
0
2y e−ay 
−t e−t −e−tay
0 dy
=
 ∞
0
2y e−ay 
1 −ay e−ay −e−ay
dy
=
 ∞
0
2y e−ay dy −
 ∞
0
2a y2 e−2ay dy −
 ∞
0
2y e−2ay dy
=
2
a2
 ∞
0
t e−t dt −
1
4a2
 ∞
0
t2e−t dt −
1
2a2
 ∞
0
t e−t dt
=
2
a2 −
1
2a2 −
1
2a2 = 1
a2 .
It follows that
Cov(X, Y ) = E{XY } −E{X} · E{Y } = 1
a2 −1
2a · 3
2a =
1
4a2 .
4) Clearly, fZ(z) = 0 for z ≤0. N˚ar z > 0, so
fZ(z) =
 ∞
−∞
h(x, z −x) dx =
 ∞
0
h(x, z −x) dx.
The integrand is only ̸= 0, if x < y = z −x, i.e. when x < 1
2 z, hence
fZ(z) =

z
2
0
g(x, z −x) dx = 2a2e−az

z
2
0
dz = a2z e−az,
Download free eBooks at bookboon.com

Random variables II
 
49 
2. Law of the total probability
and thus
fZ(z) =



a2z e−az
for z > 0,
0
otherwise.
5) The mean is
E{Z})E{X} + E{Y } = 1
2a + 3
2a = 2
a,
or alternatively and more elaborated,
E{Z} =
 ∞
0
a2z2e−az dz = 1
a
 ∞
0
t2e−t dt = 2
a.
Furthermore,
E

Z2
=
 ∞
0
a2z3e−az dz = 1
a2
 ∞
0
t3e−t dt = 6
a2 ,
hence
V {Z} = E

Z2
−(E{Z})2 = 6
a2 −4a2 = 2
a2 .
Example 3.13 A 2-dimensional random variable (X, Y ) has the frequency h(x, y) = 1 inside the
triangle in the (x, y) plane of vertices at the points (0, 0), (0, 2) and (1, 1), while the frequency is 0
anywhere else outside this triangle.
1) Find the frequencies of the random variables X and Y .
2) Prove that X and Y are non-correlated, though not independent.
3) Find the distribution function and the frequency for each of the random variables Z = X + Y and
V = X −Y .
1) If x ∈]0, 1[, then
fX(x) =
 2−x
x
dy = 2 −2x,
hence
fX(x) =



2 −2x
for x ∈]0, 1[,
0
otherwise.
If y ∈]0, 1], then
fY (y) =
 y
0
dx = y.
Download free eBooks at bookboon.com

Random variables II
 
50 
2. Law of the total probability
0
0.5
1
1.5
2
0.2
0.4
0.6
0.8
1
If y ∈]1, 2[, then
fY (y) =
 2−y
0
dx = 2 −y.
Summing up,
fY (y) =











y
for y ∈]0, 1],
2 −y
for y ∈]1, 2[,
0
otherwise.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables II
 
51 
2. Law of the total probability
2) It follows by considering a ﬁgure that E{Y } = 1. Furthermore,
E{X} =
 1
0

2x −2x2
dx = 1 −2
3 = 1
3.
Then by a double integration, where we start in the inner integral to integrate vertically after y),
E{XY }
=
 1
0
 2−x
x
xy dy

dx =
 1
0
x
y2
2
2−x
x
dx
=
1
2
 1
0
x(4 −4x) dx =
 1
0

2x −2x2
dx = 1
3.
Since
Cov(X, Y ) = E{XY } −E{X}E{Y } = 1
3 −1
3 · 1 = 0,
it follows that X and Y are non-correlated.
Since fX(x) · fY (y) ̸= 0 in the square ]0, 1[ × ]0, 2[, we see that fX(x) · fY (y) cannot be equal to
h(x, y). [This can of course also be seen directly.] Hence, X and Y are not independent.
3) The frequency of Z = X + Y is
fZ(z) =
 1
0
h(x, z −x) dx.
The integrand is ̸= 0, when y = z −x ∈]x, 2 −x[, e.g. 2x < z < 2, hence
fZ(z) =

z
2
0
h(x, z −x) dx =

z
2
0
dx = z
2,
and we ﬁnd the frequency
fZ(z) =





z
2
for z ∈]0, 2[,
0
otherwise,
and the distribution function
FZ(z) =













0
for z ≤0,
z2
4
for z ∈]0, 2[,
1
for z ≥2.
Then we note that X = X −Y has values in ] −2, 0[. If v ∈] −2, 0[, then
FV (v) = P{X −Y ≤v} =

{x−y≤v}
h(x, y) dx dy =
 2
0
 v+y
0
h(x, y) dx

dy.
We get by a diﬀerentiation,
fV (v) = F ′
V (v) =
 2
0
h(v + y, y) dy.
Download free eBooks at bookboon.com

Random variables II
 
52 
2. Law of the total probability
The integrand is ̸= 0 for
0 < v + y < 1
and
v + y < y < 2 −v −y,
hence
0 < −v < y < 1 −v
2 < 2.
If v ∈] −2, 0[, then
fV (v) =
 1−v
2
−v
dy = 1 −v
2 + v = 1 + v
2,
thus the frequency of V is
fV (v) =





1 + v
2
for v ∈] −2, 0[,
0
otherwise,
and the corresponding distribution function is
FV (v) =









0,
for v ≤−2,

1 + v
2
2
,
for v ∈] −2, 0[,
1,
for v ≥0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
53 
2. Law of the total probability
Example 3.14 Given the functions
f(x) =



12x2(1 −x),
0 < x < 1,
0,
otherwise,
g(y) =



12y(1 −y)2,
0 < y < 1,
0,
otherwise.
1. Prove that f and g are frequencies.
In the remaining part of the example we let X and Y denote random variables, where X has the
frequency f(x), and Y has the frequency g(y).
2. Find the mean and variance of X.
3. Prove that Y has the same distribution as 1 −X.
4. Find the mean and the variance of Y .
5. Prove that X + Y and X −Y are non-correlated.
6. We now assume that X and Y are independent. Explain why the two probabilities
P

X + Y > 1
2

and
P

X −Y > 1
2

are positive (one shall not compute the probabilities). Check, e.g. by applying this result, if X + Y
and X −Y are independent.
7. Here we assume that Cov(X, Y ) = −1
25. Prove that Y is then a function of X, and ﬁnd this
function.
Hint: Compute e.g. the variance of X + Y .
1) It is obvious that f(x) ≥0 for every x ∈R. Since furthermore
 1
0
12x2(1 −x) dx = 12
 1
0

x2 −x3
dx = 12
1
3 −1
4

= 1,
it follows that f(x) is a frequency.
Since g(y) = f(1 −y) and

dx
dy
 = | −1| = 1, it follows that g(y) is also a frequency.
2) The mean of X is
E{X} = 12
 1
0

x3 −x4
dx = 12
1
4 −1
5

= 12
20 = 3
5.
Since furthermore,
E

X2
= 12
 1
0

x4 −x5
dx = 12
1
5 −1
6

= 12
30 = 2
5,
the variance is
V {X} = 2
5 −
3
5
2
= 10 −9
25
= 1
25.
Download free eBooks at bookboon.com

Random variables II
 
54 
2. Law of the total probability
3) The frequency of ϕ(X) = 1 −X is
f(1 −x) ·

d(1 −x)
dx
 = f(1 −x) =



12x(1 −x)2
for 0 < x < 1,
0,
otherwise.
This is precisely the structure of the frequency of Y , with x instead of y, thus Y and 1 −X have
the same distribution.
4) It follows from (3) that
E{Y } = E{1 −X} = 1 −E{X} = 1 −3
5 = 2
5
and
V {Y } = V {1 −X} = V {1} + V {X} = 0 + 1
25 = 1
25 = V {X}.
5) It follows from the deﬁnition,
Cov(X + Y, X −Y ) = V {X} −V {Y } + Cov(Y, X) −Cov(X, Y ) = 1
25 −1
25 = 0,
hence X + Y and X −Y are non-correlated.
6) It is obvious that X and Y both have their values in ]0, 1[ with a positive probability for every
open, non-empty subinterval of ]0, 1[. Then both

X + Y > 3
2

and

X −Y > 1
2

have a positive probability. Since 2X = (X + Y ) + (X −Y ), we get
{X > 1} = {2X > 2} ⫆

X + Y > 3
2

∩

X −Y > 1
2

.
Since
P{X > 1} = 0,
P

X + Y > 3
2

> 0,
P

X −Y > 1
2

> 0,
we get
0
=
P{X > 1} = P

X + Y > 3
2

∩

X −Y > 1
2

̸=
P

X + Y > 3
2

· P

X −Y > 1
2

,
proving that X + Y and X −Y are not independent.
7) Since
V {X + Y } = V {X} + V {Y } −2 Cov(X, Y ) = 1
25 + 1
25 −2
25 = 0,
it follows that X + Y is causal, so X + Y = X + (1 −X) = 1 = a with the only possibility
Y = 1 −X.
Download free eBooks at bookboon.com

Random variables II
 
55 
2. Law of the total probability
Example 3.15 A rectangular triangle has the two smaller sides X1 and X2, where X1 and X2 are
independent random variables of the frequencies
fX1 (x1) =



1,
0 < x1 < 1,
0,
otherwise,
fX2 (x2) =



1
2,
0 < x2 < 2,
0,
otherwise.
Let Y1 = X1 + X2 denote the sum of the lengths of the two smaller sides and let Y2 = 1
2 X1X2 denote
the area of the triangle.
1) Compute the mean and the variance of Y1.
2) Compute the mean and variance of Y2.
3) Prove that
Cov (X1 + X2, X1X2) = E {X1} V {X2} + E {X2} C {X1} ,
and then compute Cov (Y1, Y2).
4) Find the frequency of Y1.
1) The mean of Y1 = X1 + X2 is
E {Y1} = E {X1} + E {X2} 1
2 + 1 = 3
2.
Since X1 and X2 are independent, the variance is
V {Y1} = V {X1} + V {X2} = 1
12

12 + 22
= 5
12.
2) Since X1 and X2 are independent, we ﬁnd that
E {Y2} = 1
2 E {X1} · E {X2} = 1
2 · 1
2 · 1 = 1
4,
and
V {Y2}
=
1
4 V {X1X2} = 1
4

E

X2
1X2
2

−(E {X1} E {X2})2
=
1
4

E

X2
1

E

X2
2

−(E {X1} E {X2})2
= 1
4

1
3 · 4
3 −
1
2
2
= 1
4
4
9 −1
4

=
7
144.
3) By a direct computation,
Cov (X1 + X2, X1X2) = E {(X1 + X2 −E {X1} −E {X2}) (X1X2 −E {X1} · E {X2})}
=
E {(X1 −E {X1}) (X1 −E {X1}) X2} + E {(X1 −E {X1}) X2} · E {X1}
+E {(X2 −E {X2}) (X2 −E {X2}) X1} + E {X1 (X2 −E {X2})} · E {X2}
−E {X1} E {X2} · E {(X1 −E {X1} + X2 −E {X2})}
=
V {X1} E {X2} + 0 + V {X2} E {X1} + 0 + 0
=
E {X1} V {X2} + E {X2} V {X1} .
Download free eBooks at bookboon.com

Random variables II
 
56 
2. Law of the total probability
Then
Cov (Y1, Y2)
=
1
2 Cov (X1 + X2, X1X2) = 1
2 (E {X1} V {X2} + E {X2} V {X1})
=
1
2
1
2 · 1
3 + 1 · 1
12

= 1
2
1
6 + 1
12

= 1
8.
4) Since X1 takes its values in ]0, 1[, and X2 takes its values in ]0, 2[, the sum Y1 = X1 + X2 will take
its values in ]0, 3[. If y ∈]0, 3[, then the frequency of Y is given by
fY (y) =
 y
0
fX1(x)fX2(y −x) dx.
Then we must split the investigation according to the diﬀerent subintervals.
a) If y ∈]0, 1], then
fY (y) =
 y
0
1 · 1
2 dx = y
2.
b) If y ∈]1, 2], then
fY (y) =
 1
0
1 · 1
2 dx = 1
2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables II
 
57 
2. Law of the total probability
c) If y ∈]2, 3], then
fY (y) =
 1
0
1 · fX2(y −x) dx =
 1
y−2
1 · 1
2 dx = 1
2 (3 −y).
Summing up,
fY (y) =

























y
2,
for y ∈]0, 1],
1
2,
for y ∈]1, 2],
1
2 (3 −y),
for y ∈]2, 3[,
0,
otherwise.
Example 3.16 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =



x + y,
0 < x < 1,
0 < y < 1,
0,
otherwise.
1. Find the marginal frequencies of X and Y .
2. Find the means of X and Y .
3. Find the variances of X and Y .
4. Compute the covariance between X and Y , and the correlation coeﬃcient between X and Y .
Let the random variables U and V be given by
U = max{X, Y }
and
V = min{X, Y }.
5. Compute the probability P

U ≤1
2

and the probability P

V ≤1
2

.
1) Due to the symmetry, X and Y have the same marginal frequency. If x ∈[0, 1], then
f(x) =
 1
0
(x + y) dy =
(x + y)2
2
1
y=0
= 1
2

(x + 1)2 −x2
= x + 1
2,
hence
f(x) =





x + 1
2
for x ∈[0, 1],
0
otherwise,
and
g(y) =





y + 1
2
for y ∈[0, 1],
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
58 
2. Law of the total probability
2) It also follows from the symmetry that
E{X} = E{Y } =
 1
0
x

x + 1
2

dx =
 1
0

x2 + x
2

dx =
x3
3 + x2
4
1
0
= 7
12.
3) For the same reason,
E

X2
= E

Y 2
=
 1
0
x2

x + 1
2

dx =
 1
0

x3 + x2
2

dx =
x4
4 + x3
6
1
0
= 1
4 + 1
6 = 5
12.
Hence
V {X} = V {Y } = E

X2
−(E{X})2 = 5
12 −
 7
12
2
= 60 −49
144
= 11
144.
4) According to a formula, the covariance is
Cov(X, Y )
=
E{XY } −E{X} · E{Y } =
 1
0
 1
0
xy(x + y) dy

dx −7
12 · 7
12
=
 1
0
x
 1
0

yx + y2
dy

dx −49
144 =
 1
0
x
1
2 x + 1
3

dx −49
144
=
 1
0
x2
2 + x
3

dx −49
144 = 1
6 + 1
6 −49
144 = 1
3 −49
144 = −1
144.
Then we get the correlation coeﬃcient
ϱ(X, Y ) =
Cov(X, Y )

V {X} · V {Y }
=
−1
144
11
144
= −1
11.
5) If U = max{X, Y }, then
P

U ≤1
2

=
P

X ≤1
2 ∧Y ≤1
2

=

1
2
0

1
2
0
(x + y) dy

dx
=

1
2
0
1
2

(x + y)2 1
2
y=0 dx = 1
2

1
2
0

x + 1
2
2
−x2

dx
=
1
6

x + 1
2
3
−x3
 1
2
0
= 1
6

13 −
1
2
3
−
1
2
3
+ 03

=
1
6

1 −1
4

= 1
6 · 3
4 = 1
8.
Download free eBooks at bookboon.com

Random variables II
 
59 
2. Law of the total probability
If V = min{X, Y }, we get by using the complementary probability that
P

V ≤1
2

=
1 −P

V > 1
2

= 1 −
 1
1
2
 1
1
2
(x + y) dy

dx = 1 −1
2
 1
1
2

(x + y)21
y= 1
2 dx
=
1 −1
2
 1
1
2

(x + 1)2 −

x + 1
2
2
dx = 1 −1
6

(x + 1)3 −

x + 1
2
31
1
2
=
1 −1
6

23 −
3
2
3
−
3
2
3
+ 13

= 1 −1
6

8 −27
4 + 1

=
1 −1
6
36 −27
4

= 1 −1
6 · 9
4 = 1 −3
8 = 5
8.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables II
 
60 
4. Examples concerning the Poisson distribution
4
Examples concerning the Poisson distribution
Example 4.1 Let X and Y be independent random variables, and let X have the frequency f(x), and
let Y have the frequency g(y).
1. Prove that the frequency of the random variable U = X −Y is given by
k(u) =
 ∞
−∞
f(x)g(x −u)dx,
u ∈R.
In the remaining of the example we assume that
f(x) =



λ e−λx,
x > 0,
0,
x ≤0,
g(y) =



µ e−µy,
y > 0,
0,
y ≤0,
where λ and µ are positive constants.
2. Find the frequency of the random variable U.
3. Find the mean E{U} and the variance V {U}.
4. Compute the correlation coeﬃcient ϱ(U, X).
1) Let K(u) be the distribution function of U. Then
K(u) = P{X −Y ≤u} =

{x−y≤u}
f(x)g(y) dx dy =
 ∞
−∞
 u+y
−∞
f(x)g(y) dx

dy.
By diﬀerentiation, followed by the change of variable x = u + y,
k(u) =
 ∞
−∞
f(u + y)g(y) dy =
 ∞
−∞
g(x)g(x −u) dx,
u ∈R.
2) It follows from
k(u) =
 ∞
−∞
g(x)g(x −u) dx =
 ∞
0
f(x)g(x −u) dx
that if u > 0 then the integrand is only ̸= 0 for x > u, thus
k(u)
=
 ∞
u
λ e−λx · µ · e−µ(x−u) dx = λ µ eµu
 ∞
u
e−(λ+µ)x dx
=
λ µ
λ + µ eµu · e−(λ+µ)u =
λ µ
λ + µ e−λ u.
If instead u ≤0, then
k(u) =
 ∞
0
λ e−λx · µ e−µ(x−u) dx = λ µ eµu
 ∞
0
e−(λ+µ)x dx =
λ µ
λ + µ eµ u.
Download free eBooks at bookboon.com

Random variables II
 
61 
4. Examples concerning the Poisson distribution
Summing up,
k(u) =









λ µ
λ + µ e−λ u
for u > 0,
λ µ
λ + µ eµ u
for u ≤0.
3) The mean is
E{U} = E{X} −E{Y } = 1
λ −1
µ = µ −λ
λ µ .
Furthermore,
E

U 2
=
µ
λ + µ
 ∞
0
λ u2 e−λ u du +
λ
λ + µ
 0
−∞
µ u2 eµ u du
=
1
λ + µ · µ
λ2
 ∞
0
t2 e−t dt +
1
λ + µ · λ
µ2
 ∞
0
t2 e−t dt
=
2
λ + µ
 λ
µ2 + µ
λ2

=
2
λ + µ · λ3 + µ3
λ2µ2
= 2 · λ2 −λ µ + µ2
λ2µ2
.
The variance is
V {U}
=
E

U 2
−(E{U})2 = 2 · λ2 −λ µ + µ2
λ2µ2
−λ2 −2λ µ + µ2
λ2µ2
=
λ2 + µ2
λ2µ2
= 1
λ2 + 1
µ2 .
4) It is well-known that
E{X} = 1
λ
and
V {X} = 1
λ2 .
Since X and Y are stochastically independent, we have
E{XY } = E{X}E{Y }.
By the rules of computation,
Cov(U, X) = Cov(X −Y, X) = Cov(X, X) −Cov(Y, X) = V {X} = 1
λ2 ,
hence
ϱ(U, X) =
Cov(U, X)

V {U}V {X}
= 1
λ2 ·
1

λ2 + µ2
λ2µ2
· 1
λ2
=
µ

λ2 + µ2 .
Download free eBooks at bookboon.com

Random variables II
 
62 
4. Examples concerning the Poisson distribution
Example 4.2 A radioactive material emits both α and β particles, where these two types of particles
are emitted independently of each other. We shall study this emission from (and included) the time
t = 0.
Let X1, X1 + X2, X1 + X2 + X3, . . . , indicate the times of the emission of the ﬁrst, second, third,
. . . , α particle.
We assume that the random variables Xi, i = 1, 2, . . . , are mutually independent of the frequency
f(x) =



λ e−λ x,
x ≥0,
0,
x < 0,
λ > 0.
Analogously, Y1, Y1 + Y2, Y1 + Y2 + Y3, . . . , indicates the times of the emission of the ﬁrst, second,
third, . . . , β particle.
We assume that the random variables Yi, i = 1, 2, . . . , also are mutually independent, and then by
the assumption independent of the Xi of the frequency
g(y) =



µ e−µ y,
Y ≥0,
0,
y < 0,
µ > 0.
1) Find the frequency of X1 + X2.
2) Find the probability that there is emitted at least two α particles before one β particle is emitted.
For which value of λ
µ is this probability equal to 1
2?
1) When x > 0, then the frequency of X1 + X2 is given by the convolution integral
f2(x) =
 x
0
f(x)f(x −t) dt =
 x
0
λ e−t λ · λ e−(x−t)λ dt = λ2 x e−λ x,
and f2(x) = 0 otherwise.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
2) We shall ﬁnd P {X1 + X2 < Y1}.
Download free eBooks at bookboon.com

Random variables II
 
63 
4. Examples concerning the Poisson distribution
First method. The simultaneous frequency of (X1 + X2, Y1) is f2(x)g(y), hence
P {X1 + X2 < Y1} =

{x<y}
f2(x)g(y) dx dy
=
 ∞
x=0
f2(x)
 ∞
y=x
g(y) dy

dx =
 ∞
0
λ2 x e−λ x
 ∞
y=x
µ e−µ y dy

dx
=
 ∞
0
λ2 x e−(λ+µ)x dx =
λ2
(λ + µ)2
 ∞
0
t e−t dt =

λ
2
λ
µ + 1

,
where we have applied the substitution t = (λ + µ)x.
Remark 4.1 Here it is diﬃcult to compute the double integral in the order
 ∞
y=0
 y
x=0 · · · , so
we omit this variant. ♦
Second method. (More diﬃcult.) The frequency of Z =
Y1
X1 + X2
is computed according to
some formula. If z > 0, then
k(z)
=
 ∞
0
g(zx)f2(x) x dx =
 ∞
0
µ e−µ z xλ2 x e−λ xx dx
=
µλ2
 ∞
0
x2e−(λ+µ z)x dx =
2λ2µ
(λ + µ z)3 ,
hence
P {X1 + X2 < Y1}
=
P{Z > 1} =
 ∞
1
k(z) dz
=
2λ2µ

1∞(λ + µ z)−3 dz =
λ2
(λ + µ)2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
64 
4. Examples concerning the Poisson distribution
Third method. (Sketch). Find the frequency of X1 + X2
Y1
, cf. the second method.
Fourth method. (Even more diﬃcult; only a sketch). Find the frequency of
U = (X1 + X2) −Y1.
Then
P {X1 + X2 < Y1} = P{U < 0} =
 0
−∞
fU(u) du.
The probability is 1
2, when
λ
µ
λ
µ + 1 =
1
√
2,
and we get
λ
µ =
√
2 + 1.
Example 4.3 . (Continuation of Example 4.2).
1) Find the probability that there is emitted at least three α particles, before the ﬁrst β particle is
emitted.
2) Find the probability that there is emitted precisely two α particles, before the ﬁrst β particle is
emitted.
3) Find the probability Pn(t) that there in the time interval ]0, 1[ is emitted a total of n particles.
1) It follows from Example 4.2 that X1 + X2 has the frequency
f2(x) =



λ2 x e−λx
for x ≥0,
0
for x < 0.
Then X3 has the frequency
f(x) =



λ e−λx
for x ≥0,
0
for x < 0,
so the frequency f3(s) of X1 + X2 + X3 is zero for s ≤0. If s > 0, then
f3(s) =
 s
0
λ2x e−λx · λ e−λ(s−x) dx = λ3e−λs
 s
0
x dx = 1
2 λ3s2e−λs.
Download free eBooks at bookboon.com

Random variables II
 
65 
4. Examples concerning the Poisson distribution
Then (cf. Example 4.2)
P {X1 + X2 + X3 < Y1} =
 0
−∞
 ∞
0
f3(x)g(x −s) dx

ds
=
 0
−∞
 ∞
0
1
2 λ3x2e−λx · µ e−µ(x−s)dx

ds
= 1
2 λ3µ
 0
−∞
e−µsds ·
 ∞
0
x2e−(λ+µ)x dx
= 1
2 λ3 · 1 ·
1
(λ + µ)3
 ∞
0
t2e−tdt =

λ
λ + µ
3
.
2) The probability that there is emitted precisely two α particles before one β particle is emitted is
P {X1 + X2 < Y1} −P {X1 + X2 + X3 < Y1}
=

λ
λ + µ
2
−

λ
λ + µ
3
=

λ
λ + µ
3 λ + µ
λ
−1

=
λ2µ
(λ + µ)3 .
3) Assume that Zn = X1 + · · · + Xn has the frequency fk(s). Then fk(s) = 0 for s ≤0, and we have
for s > 0,
fn(s) =
 s
0
fm−1(x)f(s −x) dx =
 s
0
fn−1(x) λ e−λ(s−x)dx = λ e−λs
 s
0
eλxfn−1(x) dx,
i.e.
f2(s) = λ e−λs  s
0 eλxλ e−λx dx = λ2s e−λs,
s > 0,
f3(s) = λ e−λs  s
0 eλx · λ2x e−λx dx = λ3 · s2
2! e−λs,
s > 0,
and then by induction
fn(s) =







λn
(n −1)! sn−1e−λs,
s > 0,
0,
s ≤0.
It follows that
P {Zk < t} =
 t
0
fk(s) ds =
λk
(k −1)!
 t
0
sk−1eλs ds,
0 ≤k ≤n,
which is the probability that there is emitted at least k of the α particles before time t.
The probability that there is emitted precisely k particles of α type before time t, is
P {Zk < t} −P {Zk+1 < t} =
λk
(k −1)!
 t
0
sk−1e−λsds −λk+1
k!
 t
0
ske−λsds
=
λk
(k −1)!
 t
0
sk−1e−λs dx +
λk
k! ske−λs
t
0
−
λk
(k −1)!
 t
0
sk−1e−λs ds
=
λk
k! ske−λs
t
0
= λk
k! tke−λt = (λt)k
k!
e−λt.
Download free eBooks at bookboon.com

Random variables II
 
66 
4. Examples concerning the Poisson distribution
Analogously, the probability that there is emitted precisely n −k particles of type β in ]0, 1[ is
given by
(µt)n−k
(n −k)! e−µt.
Finally, the probability that there is emitted precisely n particle (of either type α or type β) in
the time interval ]0, 1[ is
n

k=0
(λt)k
k!
e−λt · (µt)n−k
(n −k)! e−µt = 1
n! e−(λ+µ)t
n

k=0
n!
k!(n −k)! (λt)k(µt)n−k
= 1
n! tne−(λ+µ)t
n

k=0

n
k

λkµn−k = 1
n! (λ + µ)ntne−(λ+µ)t.
Example 4.4 An instrument A contains two components, which can fail independently of each other.
The instrument does not work, if just one of the components does not work.
The lifetime for each of the two components has a distribution given by the frequency
f(x) =



λ e−λx,
x > 0,
0,
x ≤0,
where λ is a positive constant.
The task is to ﬁnd the distribution of the lifetime of the instrument A.
There is in another instrument B only one component, the lifetime of which has the same frequency
f(x) as above.
We shall ﬁnd the probability that the lifetime of instrument B is at least the double of the lifetime of
A.
Let us imagine that we ﬁrst apply instrument A, and when it is ruined, then we apply instrument B.
Find the distribution of the total lifetime and ﬁnd the mean of this lifetime.
Let Y1 and Y2 denote the lifetimes of the two components of A, and Y the lifetime of A, and X the
lifetime of B.
Clearly, Y = min {Y1, Y2}.
Then Y is exponentially distributed of frequency
g(y) =



2λ e−2λy,
y > 0,
0,
y ≤0,
In the next subtask we shall ﬁnd P{X ≥2Y }.
A reasonable assumption is that A and B function independently of each other. This means that
(X, Y ) has the simultaneous frequency f(x)g(y), thus
P{X ≥2Y }
=

{x≥2y}
f(x)g(y) dx dy =
 ∞
y=0
2λ e−2λy
 ∞
x=2y
λ e−λx dx

dy
=
 ∞
y=0
2λ e−4λy dy = 1
2.
Download free eBooks at bookboon.com

Random variables II
 
67 
4. Examples concerning the Poisson distribution
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
The frequency of X + Y for z > 0 is given by the convolution integral,
k(z) =
 z
0
λ e−λx · 2λ e−2λ(z−x) dx = 2λ2e−2λz
 z
0
eλx dx = 2λ

e−λx −e−2λz
,
and k(z) = 0 for z ≤0.
Finally,
E{X + Y } = E{X} + E{Y } = 1
λ + 1
2λ = 3
2λ.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
68 
4. Examples concerning the Poisson distribution
Alternatively,
E{X + Y } =
 ∞
0
z k(z) dz = 2λ
 ∞
0

z e−λz −z e−2λz
dz = 2
λ · 1! −1
2λ · 1! = 3
2λ.
Example 4.5 1. Let X be a non-negative random variable of frequency f(x) and mean E{X}. Prove
that
(1) E{X} =
 ∞
0
P{X ≥x} dx.
Hint: Express e.g. P{X ≥x} by means of the frequency f(x).
We shall allow in the following without proof to apply the result that the mean of every non-negative
random variable is given by (1).
Two patients A1 and A2 arrive to a doctor’s waiting room at the times X1 and X1 + X2, where X1
and X2 are independent random variables, both of the frequency
f(x) =



λ e−λx,
x > 0,
0,
x ≤0,
where λ is a positive constant.
The times of treatment of A1 and A2 are assumed to be the random variables Y1 and Y2, which
are mutually independent (and also independent of X1 and X2), and we assume that they have the
frequency
g(y) =



µ e−µy,
y > 0,
0,
y ≤0,
where µ is a positive constant.
The patient A1 is treated immediately after his arrival, while A2 possibly may wait to after the treatment
of A1.
2. Describe, expressed by Y1 and Y2, the event that A2 does not wait for his treatment, and ﬁnd the
probability of this event.
3. Find for every z > 0 the probability that the waiting time Z of A2 is ≥z.
4. Find the mean of the random variable Z.
1) Since f(t) ≥0, and f(t) = 0 for t < 0, we get
 ∞
0
P{X ≥x} dx =
 ∞
0
 ∞
x
f(t) dt

dx =
 ∞
0
 t
0
f(t) dx

dt =
 ∞
0
t f(t) dt = E{X}.
2) The condition that A2 does not have to wait is
X1 + Y1 ≤X1 + X2,
thus
Y1 ≤X2,
Download free eBooks at bookboon.com

Random variables II
 
69 
4. Examples concerning the Poisson distribution
hence
P {X2 ≥Y1}
=
 ∞
y=0
g(y)
 ∞
x=y
f(x) dx

dy =
 ∞
y=0
µ e−µy
 ∞
x=y
λ e−λx dx

dy
=
 ∞
y=0
µ e−(λ+µ)y dy =
µ
λ + µ.
3) When the waiting time is positive, it is described by Z = Y1 −X2. Then for z > 0,
P{Z ≥z}
=
P {Y1 ≥X2 + z} =
 ∞
x=0
f(x)
 ∞
y=x+z
g(y) dy

dx
=
 ∞
x=0
λ e−(λ+µ)x dx · e−µz =
λ
λ + µ · e−µz.
4) It follows from (1) that
E{Z} =
 ∞
0
P{Z ≥z} dz =
λ
λ + µ
 ∞
0
e−µzdz = λ
µ ·
1
λ + µ.
Remark 4.2 The distribution of Z is of mixed type, i.e. neither discrete nor continuous. ♦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables II
 
70 
5. Miscellaneous examples
5
Miscellaneous examples
Example 5.1 A 2-dimensional random variable (X, Y ) has in the domain given by the inequalities
1 ≤x2 + y2 ≤4
the frequency
h(x, y) = 1
3π ,
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find the frequency of the random variable X, and sketch the graph of this function.
2) Find the variance of the random variable X.
3) Explain why the random variable X and Y are non-correlated, though not independent.
4) Find the probability that |X| + |Y | ≥2.
–2
–1
0
1
2
–2
–1
1
2
Figure 2: The frequency has its support in the annulus.
1) If |x| ≥2, then fX(x) = 0.
By the symmetry, fX(−x) = fX(x). If |x| ∈[1, 2], then it follows by a vertical integration (a
consideration of a graph) that
fX(x) = 1
3π · 2

4 −x2 = 2
3π

4 −x2.
If |x| ∈[0, 1], then we get instead
fX(x) = 2
3π

4 −x2 −

1 −x2

.
Summing up,
fX(x)















2
3π
√
4 −x2 −
√
1 −x2
,
x ∈[−1, 1],
2
3π
√
4 −x2,
1 ≤|x| ≤2,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
71 
5. Miscellaneous examples
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
–2
–1
1
2
t
Figure 3: The graph of fX(x).
2) The mean is trivially E{X} = 0, so the variance is
V {X}
=
E

X2
−(E{X})2 = E

X2
=
4
3π
 1
0
x2 
4 −x2 −

1 −x2

+ 4
3π
 2
1
x2
4 −x2 dx
=
4
3π
 2
0
x2
4 −x2 dx −4
3π
 1
0
x2
1 −x2 dx
=
4
3π

π
2
0
4 sin2 t · 2 cos t · 2 cos t dt −4
3π

π
2
0
sin2 t cos t cos t dt
=
16 −1
3π

π
2
0
4 sin2 t cos2 t dt = 5
π

π
2
0
sin2 2t dt = 5
π · π
4 = 5
4.
3) The support of h (i.e. the closure of the set, where h(x, y) ̸= 0) is not a rectangle. Hence, X and
Y cannot be independent.
The annulus is denoted by Ω. By using that E{X} = 0, it follows by the symmetry that
Cov(X, Y ) = E{XY } −E{X}E{Y } =
 
Ω
xy · 1
3π dx dy = 0,
hence X and Y are non-correlated.
4) It follows by considering the ﬁgure that P{|X| + |Y | ≥2} is equal to the integral of h(x, y) over
the four circular segments, thus equal to 1
3π times the area of these four circular segments, hence
P{|X| + |Y | ≥2} = 1
3π

π · 22 −(2
√
2)2
= 4
3π (π −2) = 4
3 −8
3π ≈0.485.
Download free eBooks at bookboon.com

Random variables II
 
72 
5. Miscellaneous examples
–2
–1
1
2
–2
–1
1
2
Figure 4: The domain where |X| + |Y | ≥2, is the union of the four circular segments on the ﬁgure.
Example 5.2 1) Find the pairs of numbers (a, b), for which
g(x, y) =



ax + by
for 0 ≤x ≤2 og 0 ≤y ≤1,
0
otherwise,
is the frequency of a 2-dimensional random variable (X, Y ).
2) Find, expressed by a, the means E{X} and E{Y }.
3) Find the pairs of numbers (a, b), for which the product E{X}E{Y } is largest, and compute the
maximum.
4) Compute for (a, b) =
1
4, 1
2

the covariance Cov(X, Y ).
1) Since g(x, y) ≥0 everywhere, we must have a ≥0 and b ≥0. Furthermore, we derive the condition
1 =
 2
0
ax
 1
0
dy

dx +
 2
0
b
 1
0
y dy

dx = 2a + 2b · 1
2 = 2a + b,
thus b = 1 −2a, where a ∈

0, 1
2

, hence
g(x, y) =



ax + (1 −2a)y
for 0 ≤x ≤2 and 0 ≤y ≤1,
0
otherwise,
a ∈

0, 1
2

.
Download free eBooks at bookboon.com

Random variables II
 
73 
5. Miscellaneous examples
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 5: The support of g(x, y).
0
0.2
0.4
0.6
0.8
1
0.1
0.2
0.3
0.4
0.5
Figure 6: The possible values of (a, b) lie on the oblique line.
2) If a ∈

0, 1
2

we get the mean
E{X}
=
 2
0
x fX(x) dx =
 2
0
 1
0
x{ax + (1 −2a)y}dy

dx
=
 2
0
ax2dx +
 2
0
x dx · (1 −2a)
 1
0
y dy
=
8a
3 + (1 −2a) · 1
2 · 22
2 = 1 + 2
3 a,
and analogously
E{Y }
=
 1
0
y fY (y) dy =
 2
0
 1
0
y{ax + (1 −2a)y} dy

dx
=
a
 2
0
x dx ·
 1
0
y dy?(1 −2a) · 2
 1
0
y2dy
=
2a · 1
2 + 2
3 (1 −2a) = a + 2
3 −4
3 a = 2
3 −1
3 a.
Download free eBooks at bookboon.com

Random variables II
 
74 
5. Miscellaneous examples
3) If we put
ϕ(a)
=
E{X}E{Y } =

1 + 2
3 a
 2
3 −1
3 a

=
1
9 (3 + 2a)(2 −a) = 1
9

6 + a −2a2
,
then
ϕ′(a) = 1 −4a = 0
for a = 1
4.
Since ϕ′(a) > 0 for a < 1
4, and ϕ′(a) < 0 for a > 1
4, it follows that a = 1
4 corresponds to the
maximum
ϕ
1
4

= 1
9

6 + 1
4 −1
8

= 48 + 2 −1
72
= 49
72.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables II
 
75 
5. Miscellaneous examples
4) If (a, b) =
1
4, 1
2

, then
E{XY }
=
 2
0
 1
0
xy
1
4 x + 1
2 y

dy

dx = 1
4
 2
0
x2dx ·
 1
0
y dy + 1
2
 2
0
x dx ·
 1
0
y2dy
=
1
4 · 8
3 · 1
2 + 1
2 · 4
2 · 1
3)1
3 + 1
3 = 2
3,
hence
Cov(X, Y ) = E{XY } −E{X}E{Y } = 2
3 −49
72 = 2 · 24 −49
72
= −1
72.
Example 5.3 A 2-dimensional random variable (X, Y ) has in the square deﬁned by 0 < x < π
2 and
0 < y < π
2 the frequency
h(x, y) = k(sin x + cos y),
while the frequency is 0 outside this square.
1) Prove that the constant k is equal to 1
π .
2) Find the frequencies fX(x) and fY (y) of the random variables X and Y .
3) Find the means E{X} and E{Y } of the random variables X and Y .
4) Find the frequency fZ(z) of the random variable Z = X + Y , and sketch the graph of the function.
1) Clearly, h(x, y) ≥0, if and only if k ≥0. If h(x, y) is a frequency, then necessarily
1
=

π
2
0

π
2
0
h(x, y) dx dy = k

π
2
0

π
2
0
sin x dx

dy +

π
2
0

π
2
0
cos y dy

dx

=
k
π
2 + π
2

= k · π,
and we conclude that k = 1
π as claimed.
2) When x /∈

0, π
2

, then fX(x) = 0. When x ∈

0, π
2

, it follows by a vertical integration that
fX(x) =

π
2
0
h(x, y) dy = 1
π

π
2
0
{sin x + cos y} dy = 1
2 sin x + 1
π .
When x /∈

0, π
2

, then fY (y) = 0. When y ∈

0, π
2

, it follows by a horizontal integration that
fY (y) =

π
2
0
h(x, y) dx = 1
π

π
2
0
{sin x + cos y} dx = 1
2 cos y + 1
π .
Download free eBooks at bookboon.com

Random variables II
 
76 
5. Miscellaneous examples
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Figure 7: The square ]0, π
2 [ × ]0, π
2 [.
3) The means are
E{X}
=

π
2
0
x fX(x) dx =

π
2
0
1
2 x sin x + x
π

dx =

−x
2 cos x + x2
2π
 π
2
0
+ 1
2

π
2
0
cos x dx
=
π
8 + 1
2,
and
E{Y }
=

π
2
0
y fY (y) dy =

π
2
0
1
2 y cos y + y
π

dy =
1
2 y sin y + y2
2π
 π
2
0
−1
2

π
2
0
sin y dy
=
π
4 + π
8 −1
2 = 3π
8 −1
2.
4) Clearly, X+Y has values in ]0, π[. Since X and Y are not independent, the frequency of Z = X+Y
is given by
fZ(z) =
 ∞
−∞
h(x, z −x) dx =

π
2
0
h(x, z −x) dx.
Now let 0 < z < π. The the integrand is ̸= 0, if 0 < x < π
2 and 0 < z−x < π
2 , i.e. if z−π
2 < x < z.
Then we must split into two cases:
a) If 0 < z ≤π
2 , then the domain of integration is 0 < x < z, so
fZ
=
 z
0
h(x, z −x) dx = 1
π
 z
0
{sin x + cos(z −x)} dx
=
1
π [−cos x + sin(x −z)]z
0 = 1
π {1 + sin z −cos z}.
Download free eBooks at bookboon.com

Random variables II
 
77 
5. Miscellaneous examples
0
0.1
0.2
0.3
0.4
0.5
0.6
0.5
1
1.5
2
2.5
3
Figure 8: The graph of fZ(z).
b) If π
2 < z < π, then the domain of integration is z −π
2 < x < π
2 , hence
fZ(z)
=
1
π [−cos x + sin(x −z)]
π
2
z−π
2
=
1
π

−0 + sin
π
2 −z

+ cos

z −π
2

−sin

−π
2

= 1
π {1 + sin z + cos z}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables II
 
78 
5. Miscellaneous examples
Summing up,
fZ(z) =















1
π {1 + sin z −cos z}
for 0 < z ≤π
2 ,
1
π {1 + sin z + cos z}
for π
2 < z < π,
0
otherwise.
Example 5.4 Let X and Y be independent random variables, which both are rectangularly distributed
over the interval ]1, 2[.
1. Find the frequency of the random variable Z = X
Y .
Compute the mean of Z.
Find the median of Z.
A random variable U is given by U = X
Y + Y
Z .
4. Which values can U have?
5. Find the probability that U < 25
12.
1) Clearly, Z has its values in
1
2, 2

.
The frequency of Z = X
Y is given by
fZ(z) =
 ∞
−∞
fX(zx) · fY (x) · |x| dx =
 2
1
fX(zx) x dx.
When z ∈
1
2, 2

, then the conditions become 1 < x < 2 and 1 < zx < 2, hence 1
z < x < 2
z .
a) When z ∈
1
2, 1

, then the interval of integration is
1
z , 2

, hence
fZ(z) =
 2
1
z
x dx =
x2
2
2
1
z
= 2 −
1
2z2

= 4z2 −1
2z2

.
b) When z ∈]1, 2[, then the interval of integration is

1, 2
z

, hence
fZ(z) =

2
z
1
x dx =
x2
2
 2
z
1
= 2
z2 −1
2

= 4 −z2
2z2

.
Download free eBooks at bookboon.com

Random variables II
 
79 
5. Miscellaneous examples
Summing up,
fZ) =

















2 −
1
2z2
for z ∈
1
2, 1

,
2
z2 −1
2
for z ∈]1, 2[,
0
otherwise.
2) The mean is
E{Z}
=
 1
1
2

2z −1
2z

dz +
 2
1
2
z −z
2

dz =

z2 −1
2 ln z
1
1
2
+

2 ln z −z2
4
2
1
=
1 −1
4 + 1
2 ln 1
2 + 2 ln 2 −1 + 1
4 = 3
2 ln 2.
3) For 1
2 ≤z ≤1 the distribution function is given by
FZ(z) =
 z
1
2

2 −1
2t2

dt =

2t + 1
2t
z
1
2
= 2z + 1
2z −1 −1 = 2z + 1
2z −2 = (2z −1)2
2z
.
When z = 1, we get FZ(1) = 1
2, so the median is (Z) = 1, and there is in this question no need to
ﬁnd the expression of the distribution function.
4) If we put z = x
y ∈
1
2, 2

, then u = z + 1
z , which has a minimum for z = 1 and is increasing for
z ∈]1, 2[. It follows that U has its values in

2, 5
2

.
The inequality U = Z + 1
Z < 25
12 is equivalent to Z2 −25
12 Z + 1 < 0, thus Z lies between the roots
of the equation
z2 −25
12 z + 1 = 0.
These roots are
z = 25
24 ±
25
24
2
−1 = 25
24 ±

49
24 · 1
24 = 25
24 ± 7
4 =







4
3,
3
4.
Then
P

U < 25
12

= P
3
4 < Z < 4
3

= FZ
4
3

−FZ
3
4

.
Download free eBooks at bookboon.com

Random variables II
 
80 
5. Miscellaneous examples
We shall now need the explicit expression of the distribution function FZ(z) when z ∈]1, 2[. We
ﬁnd
FZ
4
3

=
1
2 +

4
3
1
 2
t2 −1
2

dt = 1
2 +

−2
t −t
2
 4
3
1
= 1
2 −2
4
3
−
4
3
2 + 2 + 1
2
=
3 −3
2 −2
3 = 3
2 −2
3 = 5
6,
hence by insertion
P

U < 25
12

= 5
6 −
 3
2 −1
2
3
2
= 5
6 −1
6 = 4
6 = 2
3.
Example 5.5 A 2-dimensional random variable (X, Y ) has in the domain given by 0 ≤x ≤a,
x ≤y ≤x + 1 (where a > 0) the frequency
h(x, y) = 1
a,
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find, possibly without ﬁrst ﬁnding the marginal frequencies, the means E{X} and E{Y }, the
variances V {X} and V {Y }, and the mean E{XY }.
2) Indicate, expressed by a, the correlation coeﬃcient ϱ(X, Y ).
3) Find lima→∞ϱ(X, Y ) and lima→0 ϱ(X, Y ).
0
0.5
1
1.5
2
2.5
3
0.5
1
1.5
2
Figure 9: The domain for a = 2.
1) It follows immediately that
fX(x) =





1
a
for x ∈]0, a[,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
81 
5. Miscellaneous examples
thus X is rectangularly distributed, hence
E{X} = a
2
and
V {X} = a2
12.
Then
E{Y }
=
1
a
 a
0
 x+1
x
y dy

dx = 1
2a
 a
0

(x + 1)2 −x2
dx
=
1
2a
 a
0
(2x + 1) dx = 1
2a

x2 + x
a
0 = a2 + a
2a
= a + 1
2
,
and
E

Y 2
=
1
a
 a
0
 x+1
x
y2dy

dx = 1
3a
 a
0

(x + 1)3 −x3
dx
=
1
3a
 a
0

3x2 + 3x + 1

dx = 1
3a

x3 + 3
2 x2 + x
a
0
=
1
3a

a3 + 3
2 a2 + a

= 1
6

2a2 + 3a + 2

,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables II
 
82 
5. Miscellaneous examples
hence
V {Y }
=
1
6

2a2 + 3a + 2

−1
4 (a + 1)2
=
1
12

4a2 + 6a + 4 −3a2 −6a −3

= a2 + 1
12
.
Finally,
E{XY }
=
1
a
 a
0
x
 x+1
x
y dy

dx = 1
2a
 a
0
x{2x + 1} dx = 1
2a
 a
0

2x2 + x

dx
=
1
2a
2
3 a3 + 1
2 a2

= 1
3 a2 + 1
4 a.
2) It follows by insertion,
Cov(X, Y )
=
E{XY } −E{X}E{Y } = 1
3 a2 + 1
4 a −a
2
a + 1
2

=
a2
3 + a
4 −a2
4 −a
4 = a2
12.
This implies that
ϱ(X, Y ) =
Cov(X, Y )

V {X}V {Y }
= a2
12 ·
1

a2
12 · a2 + 1
12
=
a
√
a2 + 1
.
3) The limits are trivial,
lim
a→∞ϱ(X, Y ) = lim
→∞
a
√
a2 + 1
= 1,
and
lim
a→0 ϱ(X, Y ) = 0.
Download free eBooks at bookboon.com

Random variables II
 
83 
5. Miscellaneous examples
Example 5.6 A 2-dimensional random variable (X, Y ) has in the domain D given by 0 < x < 1,
0 < y < 1, the frequency
f(x, y) = 6
5

x + y2
,
while the frequency is 0 everywhere else in the (x, y) plane.
1) Find the frequencies and the distribution function of the random variables X and Y .
2) Find the means E{X} and E{Y }, the variances V {X} and V {Y }, and the covariance Cov(X, Y ).
3) Find the distribution function F(x, y) of the 2-dimensional random variable (X, Y ) in the domain
D.
4) Find the set M of all points in the (x, y) plane, for which
F(x, y) = 7
20,
and sketch the graph of the point set M.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 10: The domain D.
1) When 0 < x < 1, then
fX(x) = 6
5
 1
0

x + y2
dy = 6
5

x + 1
3

= 6
5 x + 2
5,
and fX(x) = 0 otherwise.
When 0 < y < 1, then
fY (y) = 6
5
 1
0

x + y2
dx = 6
5
1
2 + y2

= 3
5 + 6
5 y2,
and fy(y) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables II
 
84 
5. Miscellaneous examples
Summing up, the frequency of X is given by
fX(x) =





6
5 x + 2
5
for 0 < x < 1,
0
otherwise,
and the corresponding distribution function is
FX(x) =













0,
x ≤0,
3
5 x2 + 2
5 x,
0 < x < 1,
1,
x ≥1.
Analogously, the frequency of Y is given by
fY (y) =





3
5 + 6
5 y2
for 0 < y < 1,
0
otherwise,
and the corresponding distribution function is
FY (y) =













0,
y ≤0,
2
5 y3 + 3
5 y,
0 < y < 1,
1,
y ≥1.
2) The means are
E{X} =
 1
0
6
5 x2 + 2
5 x

dx = 2
5 + 1
5 = 3
5,
and
E{Y } =
 1
0
3
5 y + 6
5 y3

dy = 3
10 + 3
10 = 3
5.
Furthermore,
E

X2
=
 1
0
6
5 x3 + 2
5 x2

dx = 1
5
3
2 + 2
3

= 13
30,
and
E

Y 2
= 1
5
 1
0

3y2 + 6y4
dy = 1
5

1 + 6
5

= 11
25,
thus the variances are
V {X} = 13
30 −9
25 = 65 −54
150
= 11
150,
Download free eBooks at bookboon.com

Random variables II
 
85 
5. Miscellaneous examples
and
V {Y } = 11
25 −9
25 = 2
25.
Finally,
E{XY }
=
6
5
 1
0
x
 1
0

xy + y3
dy

dx = 6
5
 1
0
x
x
2 + 1
4

dx
=
3
10
 1
0

2x2 + x

dx = 3
10
2
3 + 1
2

= 3 · 7
60 = 7
20,
hence the covariance is
Cov(X, Y ) = E{XY } −E{X} · E{Y } = 7
20 −9
25 = 35 −36
100
= −1
100.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables II
 
86 
5. Miscellaneous examples
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 11: The domain of integration for the determination of F(x, y).
3) If (x, y) ∈D, i.e. 0 < x < 1 and 0 < y < 1, then the distribution function is given by
F(x, y)
=
6
5
 x
0
 y
0

t + u2
du

dt = 6
5
 x
0

ty + 1
3 y3

dt
=
6
5
1
2 x2y + 1
3 xy3

= 1
5

3x2y + 2xy3
= 1
5 xy

3x + 2y2
.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0.2
0.4
0.6
0.8
1
1.2
1.4
Figure 12: The curve M, where F(x, y) = 7
20.
4) We have in D,
F(x, y) = 3
5 y · x2 + 2
5 y3 · x = 7
20,
when
(12y) · x2 +

8y3
x −7 = 0.
Since y ̸= 0 for every solution, we ﬁnd by solving with respect to x that
x = −8y3 +

64y6 + 4 · 7 · 12y
24y
=

4y6 + 21y −2y3
6y
.
Download free eBooks at bookboon.com

Random variables II
 
87 
5. Miscellaneous examples
If we in particular choose y = 1, then x = 1
6 {√4 + 21 −2} = 1
2- Then F
1
2, y

= 7
20 for every
y ≥1.
Choosing x = 1, the equation is reduced to 8y3 + 12y −7 = 0, the only solution of which in [0, 1]
is y = 1
2. Then F

x, 1
2

= 7
20 for every x ≥1.
Example 5.7 A point set D in the (x, y) plane is the union of the following two sets
D1
=

(x, y)
 0 ≤x ≤1, 0 ≤y ≤x
2

,
D2
=

(x, y)
 0 ≤x ≤1, 1 + x
2
≤y ≤1

.
A 2-dimensional random variable (X, Y ) has in D the frequency f(x, y) = 2, while the frequency is 0
everywhere else in the plane.
1) Find the frequencies fX(x) and fY (y) of the random variable X and Y .
2) Find the means E{X} and E{Y } and the variances V {X} and V {Y }.
3) Find the covariance Cov(X, Y ).
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 13: The subdomain D1 is the lower triangle and the subdomain D2 is the upper triangle.
1) By mental arithmetic (i.e. it is strictly speaking a vertical integration) it follows that
fX(x) =



1
for x ∈[0, 1],
0
otherwise.
so X is rectangularly distributed over [0, 1].
Download free eBooks at bookboon.com

Random variables II
 
88 
5. Miscellaneous examples
When y ∈

0, 1
2

, we get by a horizontal integration (also mental arithmetic) that
fY (y) = 2 · (1 −2y) = 2 −4y.
If on the other hand, y ∈
1
2, 1

, then analogously,
fY (y) = 2 · (2y −1) = 4Y −2.
Summing up,
fY (y) =



















2 −4y
for y ∈

0, 1
2

,
4y −2
for y ∈
1
2, 1

,
0
otherwise,
which is reduced to
fY (y) =



2 |2y −1|
for y ∈[0, 1],
0
otherwise.
2) Since X is rectangularly distributed, we have
E{X} = 1
2
and
V {X} = 1
12.
It follows by the symmetry that E{Y } = 1
2.
Alternatively, this follows by the computation
E{Y }
=

1
2
0

2y −4y2
dy +
 1
1
2

4y2 −2y

dy =

y2 −4
3 y3
 1
2
0
+
4
3 y3 −y2
1
1
2
=
1
4 −1
6 + 4
3 −1 −1
6 + 1
4 = 1
2.
Furthermore,
E

Y 2
=

1
2
0

2y2 −4y3
dy +
 1
1
2

4y3 −2y2
dy =
2
3 y3 −y4
 1
2
0
+

y4 −2
3 y3
1
1
2
=
1
6 −1
16 + 1 −2
3 −1
16 + 1
6 = 1
3 −1
8 + 1
3 = 2
3 −1
8 = 16 −3
24
= 13
24,
hence
V {Y } = 13
24 −1
4 = 7
24.
Download free eBooks at bookboon.com

Random variables II
 
89 
5. Miscellaneous examples
3) First compute
E{XY }
=
2
 
D1
xy dx dy + 2
 
D2
xy dx dy
=
2
 1
0
x

x
2
0
y dy

dx + 2
 1
0
 1
1+x
2
y dy

dx =
 1
0
x
x
2
2
+ 1 −
1 + x
2
2
dx
=
 1
0
x
1 + 2x
2
·

−1
2

+ 1

dx = 1
4
 1
0
x(4 −1 −2x) dx
=
1
4
 1
0

3x −2x2
dx = 1
4
3
2 −2
3

= 1
4 · 0 −4
6
= 5
24.
We ﬁnally get
Cov(X, Y ) = E{XY } −E{X}E{Y } = 5
24 −1
2 · 12 = −1
24.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables II
 
90 
5. Miscellaneous examples
Example 5.8 A 2-dimensional random variable (X, Y ) has in the domain D = ]0, 1[ × ]0, 1[ the fre-
quency
f(x, y) = 3

xy2 + yx2
,
while the frequency is 0 everywhere else in the (x, y) plane.
1. Find the frequency fX(x) and the distribution function FX(x) of the random variable X.
2. Compute the mean E{X} and the variance V {X}.
3. Find for every real number k the simultaneous distribution function F(x, y) of (X, Y ) at the point
(k, k).
4. Find the probability that both X and Y are smaller than 1
2.
5. Find the probability that both X and Y are bigger than 1
2.
The parabolic arcs y = x2 and y = √x, 0 ≤x ≤1, divide D into three domains D1, D2, D3.
6. Find the probabilities
P {(X, Y ) ∈D1} ,
P {(X, y) ∈D2}
and
P {(X, Y ) ∈D3}
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 14: The domain D.
1) When x ∈]0, 1[, we get by a vertical integration,
fX(x) = 3
 1
0

xy2 + y2x

dy = x + 3
2 x2,
thus the frequency is
fX(x) =





x + 3
2 x2,
x ∈]0, 1[,
0,
otherwise,
Download free eBooks at bookboon.com

Random variables II
 
91 
5. Miscellaneous examples
and the distribution function is
F(x) =













0,
x ≤0,
1
2

x2 + x3
,
0 < x < 1,
1,
x ≥1.
2) The mean is
E{X} =
 1
0

x2 + 3
2, x3

dx = 1
3 + 3
8 = 8 + 9
24
= 17
24.
Since
E

X2
=
 1
0

x2 + 3
2 x4

dx = 1
4 + 3
10 = 5 + 6
20
= 11
20,
the variance becomes
V {X} = 11
20 −
17
24
2
= 139
2880.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 15: The domain of integration for the determination of F(k, k); here k = 2
3.
3) When k ≤0, then F(k, k) = 0, and when k ≥1, then F(k, k) = 1.
When 0 < k < 1, then
F(k, k)
=
 k
0
 k
0

3xy2 + 3yx2
dy

dx =
 k
0

xy3 + 3
2 y2x2
k
y=0
dx
=
 k
0

k3x + 3
2 k2x2

dx =
1
2 k3x2 + 1
2 k2x3
k
0
= k5.
Download free eBooks at bookboon.com

Random variables II
 
92 
5. Miscellaneous examples
4) The probability that both X and Y are ≤1
2, is
P

X ≤1
2, Y ≤1
2

= F
1
2, 1
2

=
1
2
5
= 1
32.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 16: The domain of integration of question 5 is the upper square.
5) The probability that both X and Y are ≥1
2, is due to the symmetry,
P

X ≥1
2, Y ≥1
2

=
1 −P

P ≥1
2

−P

Y ≥1
2

+ P

X ≤1
2, Y ≤1
2

=
1 −2 P

X ≤1
2

+ F
1
2, 1
2

= 1 −2 FX
1
2

+ F
1
2, 1
2

=
1 −
1
4 + 1
8

+ 1
32 = 32 −8 −4 + 1
32
= 21
32.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 17: The domains D1 (down most), D2 (in the middle) and D3 (uppermost).
Download free eBooks at bookboon.com

Random variables II
 
93 
5. Miscellaneous examples
6) It follows by the symmetry that
P {(X, Y ) ∈D1} = P {(X, Y ) ∈D3} ,
hence
P {(X, Y ) ∈D2} = 1 −2 P {(X, Y ) ∈D1} .
Then by a planar integral,
P {(X, Y ) ∈D1}
=
 1
0
 x2
0

3xy2 + 3yx2
dy

dx =
 1
0

xy3 + 3
2 y2x2
x2
0
dx
=
 1
0

x7 + 3
2 x6

dx = 1
8 + 3
14 = 7 + 12
56
= 19
56,
hence
P {(X, y) ∈D1} = P {(X, Y ) ∈D3} = 19
56,
and
P {(X, Y ) ∈D2} = 1 −19
28 = 9
28.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables II
 
94 
5. Miscellaneous examples
Example 5.9 Given for every k ∈]0, 1[ a function fk by
fk(x) =



k e−x + 2(1 −k)e−2x
for x > 0,
0
for x ≤0.
1) Prove that fk(x) is a frequency of a random variable, which is denoted by Xk.
2) Find the distribution function Fk(x) of Xk.
3) Find the mean and variance of Xk.
4) Find the median of X 1
2 .
5) The random variable Yk is given by
Yk = exp
Xk
2

.
Find the distribution function and the frequency of Yk, and compute the mean E {Yk}.
6) Then assume that the random variable X 1
2 is observed twice by independent observations. Find
the probability that the second observation is bigger than the half of the ﬁrst one.
1) When k ∈]0, 1[, then fk(x) ≥0. Then by an integration
 ∞
−∞
fk(x) dx =
 ∞
0

k e−x + 2(1 −k)e−2k
dx = k + 2 · 1
2 (1 −k) = 1,
thus fk(x) is a frequency of a random variable Xk.
2) When x ≤0, then Fk(x) = 0. When x > 0, then
Fk(x)
=
 x
0

k e−t + 2(1 −k)e−2t
dt =

−k e−t −(1 −k)e−2tx
0
=
1 −k e−x −(1 −k)e−2k,
hence summing up,
Fk(x) =



1 −k e−x −(1 −k)e−2x
for x > 0,
0
otherwise.
3) The mean is
E{X} =
 ∞
0

k · x e−x + (1 −k) · 2x e−2x
dx = k · 1! + 1
2 (1 −k) · 1! = 1 + k
2
.
Furthermore,
E

X2
=
 ∞
0

k · x2e−x + 2(1 −k) · x2e−2x
dx
=
k · 2! + 1
4 (1 −k)2! = 2k + 1 −k
2
= 1 + 3k
2
,
Download free eBooks at bookboon.com

Random variables II
 
95 
5. Miscellaneous examples
so the variance becomes
V {X} = 1 + 3k
2
−
1 + k
2
2
= 1
4

2 + 6k −1 −2k −k2
= 1 + 4k −k2
4
.
4) The median

X 1
2

is the solution of F 1
2 (x) = 1
2, i.e. of the equation
1 −1
2 e−x −1
2

e−x2 = 1
2.
This is rewritten as the equation of second degree in e−x,

e−x2 +

e−x
−1 = 0,
hence
e−x = −1
2 (±)
√
5
2
=
√
5 −1
2
=
2
√
5 + 1,
and whence

X 1
2

= ln
√
5 + 1
2

.
5) The image of Yk is ]1, ∞[. When y ∈]1, ∞[, then
FYk(y)
=
P

Yk = exp
Xk
2

≤y

= P {Xk ≤2 ln y} = Fk(2 ln y)
=
1 −k e−2 ln y −(1 −k)e−2·2 ln y = 1 −k
y2 −1 −k
y4
,
hence the distribution function is
FYk(y) =





1 −k
y2 −1 −k
y4
for y > 1,
0
for y ≤1.
The corresponding frequency is obtained by a diﬀerentiation,
fYk(y) =







2k
y3 + 4(1 −k)
y5
for y > 1,
0
for y ≤1.
The mean is
E {Yk} =
 ∞
1
y fYk(y) dy =
 ∞
1
2k
y2 + 4(1 −k)
y4

dy = 2k + 4
3 (1 −k) = 2
3 (k + 2).
Download free eBooks at bookboon.com

Random variables II
 
96 
5. Miscellaneous examples
0
0.1
0.2
0.3
0.4
0.5
0.2
0.4
0.6
0.8
1
Figure 18: The domain of integration of question 6 lies in the ﬁrst quadrant above the oblique line.
6) Let X′
1/2 and X′′
1/2 be two independent random variables, both of the frequency f1/2. Then
P

X′′
1/2 > 1
2 X′
1/2

=
 ∞
x=0
 ∞
y= 1
2 x
f1/2(x) f1/2(y) dy

dx
=
 ∞
x=0
1
2

e−x + 2 e−2x
·
 ∞
y= 1
2 x
1
2

e−y + 2 e−2y
dy

dx
=
 ∞
x=0
1
2

e−x + 2 e−2x
·
1
2 e−x/2 + 1
2 e−x

dx
=
 ∞
0
1
4 e−3x/2 + 1
4 e−2x + 1
2 e−5x/2 + 1
2 e−3x

dx
= 1
4 · 2
3 + 1
4 · 1
2 + 1
2 · 2
5 + 1
2 · 1
3 = 79
120.
Download free eBooks at bookboon.com

Random variables II
 
97 
5. Miscellaneous examples
Example 5.10 A rectangle has the side lengths X1 and X2, where X1 and X2 are independent random
variables, and where X1 and X1 are both rectangularly distributed over ]1, 2[.
Let Y1 = 2X1 + 2X2 denote the circumference of the rectangle, and let Y2 = X1X2 denote the area of
the rectangle.
1) Compute the mean and the variance of Y1.
2) Compute the mean and the variance of Y2.
3) Compute the covariance Cov(Y1, Y2).
4) Compute the correlation coeﬃcient ϱ (Y1, Y2).
5) Compute the frequency of Y1.
6) Compute the frequency of Y2.
1) Since X1 and X2 are independent, and e.g.
E {Xi} =
 2
1
t dt = 3
2,
which of course also can be seen directly, we get
E {Y1} = 2 E {X1} + 2 E {X2} = 4
 2
1
t dt = 4 · 3
2 = 6,
and
V {Y1} = 22V {X1} + 22V {X2} = 8
 2
1

t −3
2
2
dt = 8
3

t −3
2
32
1
= 2
3,
just to demonstrate a couple of the possible variants. (There are of course more direct method by
e.g. applying that the mean and variance are known for the rectangular distribution).
2) For the same reason,
E {Y2} = E {X1} · E {X2} = 3
2 · 3
2 = 9
4.
Furthermore,
E

Y 2
2

= E

X2
1

· E

X2
2

=
 2
1
x2dx
2
=
1
3 x3
2
1
2
=
7
3
2
= 49
9 ,
hence
V {Y2} = E

Y 2
2

−(E {Y2})2 = 49
9 −
9
4
2
= 49
9 −81
16 = 55
144.
Download free eBooks at bookboon.com

Random variables II
 
98 
5. Miscellaneous examples
3) Since the covariance is bilinear, we get by insertion of Y1 = 2X1 + 2X2 and Y2 = X1X2 that
Cov (Y1, Y2)
=
Cov (2X1 + 2X2, X1X2)
=
2 Cov (X1, X1X2) + 2 Cov (X2, X1X2) = 4 Cov (X1, X1X2)
=
4 (E {X1 · X1X2} −E {X1} · E {X1X2})
=
4

E

X2
1

· E {X2} −(E {X1})2 · E {X2}

=
4

E

X2
1

−(E {X1})2
· E {X2} = 4 V {X1} · E {X2}
=
4 · 1
12 · 3
2 = 1
2,
because it follows by question 1 that
V {X1} = 1
8 V {Y1} = 1
8 · 2
3 = 1
12,
which we also can obtain directly by using that X1 is rectangularly distributed.
4) We have now
Cov (Y1, Y2) = 1
2,
V {Y1} = 2
3,
V {Y2} = 55
144,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables II
 
99 
5. Miscellaneous examples
so the correlation coeﬃcient is
ϱ (Y1, Y2) =
Cov (Y1, Y2)

X {Y1} · V {Y2}
=
1
2

2
3 · 55
144
=
√
3 · 12
2
√
110 = 6
√
3
√
110 =

54
55 = 3
√
330
55
,
where there are more possibilities of the indication of the result.
5) First compute the frequency of X1 + X2:
g(s) =
 ∞
−∞
f(x) f(s −x) dx,
where
f(x) =



1
for x ∈]1, 2[,
0
otherwise.
If g(s) ̸= 0, then we must have the restrictions
1 < x < 2
og
1 < s −x < 2,
i.e. after a rearrangement
1 < x < 2
and
s −2 < x < s −1.
Then we have two possibilities,
a) When s ∈]2, 3[, then g(s) =
 s−1
1
1 dx = s −2.
b) When s ∈]3, 4[, then g(s) =
 2
s−2 1 dx = 4 −s.
0
0.2
0.4
0.6
0.8
1
1
2
3
4
Figure 19: The graph of g(s).
Summing up we get
g(s) =











s −2,
for 2 ≤s ≤3,
4 −s,
for 3 < s ≤4,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
100 
5. Miscellaneous examples
The random variable Y1 = 2 (X1 + X2) has the frequency
h(s) = 1
2 g
s
2

,
where s
2 ∈]2, 4[ for s ∈]4, 8[, i.e.
(2) h(s) =















1
2
s
2 −2

= s
4 −1,
for 4 ≤s ≤6,
1
2

4 −s
2

= 2 −s
4,
for 6 < s ≤8,
0,
otherwise.
Alternatively, it is seen that 2X1 and 2X2 are both rectangularly distributed over ]2, 4[.
Alternatively we consider a ﬁgure in order to determine the the distribution function of 2X1 +
2X2. We have two cases:
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 20: When 4 ≤s ≤6, then the distribution function H(s) is the area of the triangle of the
ﬁgure.
a) When 4 ≤s ≤6, then the distribution function is equal to the area of the triangle on ﬁgure 5,
the smaller sides of which both have the length s
2 −1, thus
H(s) = 1
2
s
2 −2
2
.
We get the frequency by a diﬀerentiation,
h(s) =
s
2 −2

· 1
2 = s
4 −1.
b) When 6 ≤s ≤8, then the distribution function is equal to the area of the square minus the
area of the triangle on ﬁgure 5a, hence
H(s) = 1 −1
2

4 −s
2
2
.
Download free eBooks at bookboon.com

Random variables II
 
101 
5. Miscellaneous examples
0
0.5
1
1.5
2
2.5
0.5
1
1.5
2
Figure 21: When 4 ≤s ≤6, then the distribution function H(s) is the area of the square minus the
area of the triangle on the ﬁgure.
We get the frequency by a diﬀerentiation,
h(s) =

4 −s
2

· 1
2 = 2 −s
2.
Summing up that we again obtain (2).
6) The frequency of Y2 = X1X2 is
k(s) =
 ∞
−∞
f(x) f
 s
x
 1
|x| dx.
If the integrand is ̸= 0, then we must have 1 < x < 2 and 1 < s
x < 2, thus
1 < x < 2
and
s
2 < x < s.
Again we have two cases.
a) If s ∈]1, 2[, then
k(s) =
 s
1
1
x dx = ln s.
b) If s ∈]2, 4[, then
k(s) =
 2
s
2
1
x dx = ln 2 −ln s
2 = ln 4 −ln s.
Hence we get
(3) k(s) =













ln s,
for 1 < s ≤2,
ln 4 −ln s = ln 4
s,
for 2 < s < 4,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
102 
5. Miscellaneous examples
Alternatively one may again apply a consideration of a ﬁgure in the determination of the
distribution function of X1X2, where we again must consider two cases:
a) When 1 < s < 2, then the distribution function H(s) is equal to the area of the curvilinear
triangle on the ﬁgure on the next page, hence
K(s) =
 s
1
 s
x −1

dx = s ln s −s + 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables II
 
103 
5. Miscellaneous examples
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 22: The distribution function H(s) is the area of the curvilinear triangle.
We obtain the frequency by a diﬀerentiation,
k(s) = ln 2,
for 1 < s < 2.
0
0.5
1
1.5
2
2.5
0.5
1
1.5
2
Figure 23: The distribution function is the area of the square minus the area of the curvilinear triangle.
b) When 2 < s < 4, then H(s) is the area of the square minus the area of the curvilinear triangle,
hence
K(s) = 1 −
 2
s
2

1 −
 s
x −1

dx = 1 −
 2
s
2

2 −s
x

dx = −3 + s ln 4 + s −s ln s.
We obtain the frequency by a diﬀerentiation,
k(s) = ln 4 −ln s,
for 2 < s < 4.
Summing up we again obtain (3).
Download free eBooks at bookboon.com

Random variables II
 
104 
5. Miscellaneous examples
Example 5.11 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =



4x(1 −y),
0 < x < 1, 0 < y < 1,
0,
otherwise.
1) Prove that the random variables X and Y are independent.
2) Find the means E{X} and E{Y }.
3) Find the variances V {X} and V {Y }.
4) Find the frequency of the random variable X −Y .
5) Let C denote the disc x2 + y2 ≤1. Compute P{(X, Y ) ∈C}.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 24: The domain D, where f(x, y) ̸= 0.
1) It follows immediately that
fX(x) =



2x,
0 < x < 1,
0,
otherwise,
and
fY (y) =



2(1 −y),
0 < y < 1,
0,
otherwise,
Furthermore,
f(x, y) = fX(x) · fY (y),
so X and Y are stochastically independent.
Download free eBooks at bookboon.com

Random variables II
 
105 
5. Miscellaneous examples
2) The means are given by
E{X} =
 1
0
2x2 dx = 2
3
and
E{Y } =
 1
0

2y −2y2
dy = 1 −2
3 = 1
3.
3) It follows from
E

X2
=
 1
0
2x3 dx = 1
2,
that
V {X} = 1
2 −4
9 = 1
18.
Similarly
E

Y 2
=
 1
0

2y2 −2y3
dy = 2
3 −1
2 = 1
6,
implies that
V {Y } = 1
6 −1
9 = 1
18.
4) The random variable Z = X −Y has its values in ] −1, 1[. The frequency is for −1 < z < 1 given
by
fZ(z) =
 ∞
−∞
fX(x) fY (x −z) dx.
The integrand is ̸= 0, when 0 < x < 1 and 0 < x −z < 1, i.e. when
0 < x < 1
and
z < x < z + 1.
We shall then split into two cases:
a) If z ∈] −1, 0], then the domain of integration is ]0, z + 1[, thus
fZ(z)
=
 z+1
0
fX(x) fY (x −z) dx = 4
 z+1
0
x(1 + z −x) dx
=

2(1 + z)x2 −4
3 x3
z+1
0
= 2(1 + z)3 −4
3 (1 + z)3 = 2
3 (1 + z)3.
b) If z ∈]0, 1[, then the domain of integration is ]z, 1[, thus
fZ(z)
=

2(1 + z)x2 −4
3 x3
1
z
= 2(1 + z) −4
3 −2(1 + z)z2 + 4
3 z3
=
2
3 + 2z −2
3 z3 −2z2 = 2
3

1 −z3 + 3z(1 −z)

=
2
3

1 + 3z −3z2 −z3

= 2
3 (1 −z)

1 + 4z + z2
.
Download free eBooks at bookboon.com

Random variables II
 
106 
5. Miscellaneous examples
Summing up,
fZ(z) =















2
3

1 + 3z + 3z2 + z3
for z ∈] −1, 0],
2
3

1 + 3z −3z2 −z3
for z ∈]0, 1[,
0
otherwise.
–1
–0.5
0
0.5
1
–1
–0.5
0.5
1
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables II
 
107 
5. Miscellaneous examples
c) By considering a ﬁgure it follows by using polar coordinates.
P{(X, Y ) ∈C}
=
 
C
f(x, y) dx dy =

π
2
0
 1
0
4r cos ϕ (1 −r sin ϕ)r dr

dϕ
=

π
2
0
 1
0

4r2 cos ϕ −4r3 cos ϕ sin ϕ

dr

dϕ
=

π
2
0
4
3 cos ϕ −cos ϕ · sin ϕ

dϕ =
4
3 sin ϕ −1
2 sin2 ϕ
 π
2
0
=
4
3 −1
2 = 5
6.
Example 5.12 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =





1
2 xy
0 < y < x < 2,
0
otherwise.
1) Find the frequencies fX(x) and fY (y) of the random variables X and Y .
2) Find the means E{X} and E{Y } of the random variables X and Y .
3) Find the medians of the random variable X and Y .
4) Find the frequency fZ(z) of the random variable Z = X + Y .
5) Find the means E{Z} and E
 1
Z

of the random variables Z and 1
Z .
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 25: The domain D, where f(x, y) ̸= 0.
1) If 0 < x < 2, then we get by a vertical integration,
fX(x) =
 x
0
1
2 xy dy = 1
4 x3.
If 0 < y < 2, then we get by a horizontal integration,
fY (y) =
 2
y
1
2 xy dx = 1
4 y

4 −y2
= y −1
4 y3.
Download free eBooks at bookboon.com

Random variables II
 
108 
5. Miscellaneous examples
Summing up,
fX(x) =





1
4 x3
0 < x < 2,
0
otherwise,
fY (y) =





y −1
4 y3
0 < y < 2,
0
otherwise.
2) The means are given by
E{X} =
 2
0
1
4 x4 dx =
 1
20 x5
2
0
= 32
20 = 8
5,
and
E{Y } =
 2
0

y2 −1
4 y4

dy =
y3
3 −y5
20
2
0
= 8
3 −8
5 = 16
15.
3) The distribution function FX(x), when 0 < x < 2, is given by
FX(x) =
 x
0
1
4 t3 dt = 1
16 x4

= 1
2
for x =
4√
8

,
hence the median is (X) =
4√
8.
The distribution function FY (y), when 0 < y < 2, is given by
FY (y) = 1
2 y2 −1
16 y4.
The median is given by
1
2 y2 −1
16 y4 = 1
2,
hence
8y2 −y4 = 8,
and whence by a rearrangement,

y22 −8y2 + 8 = 0,
i.e.

y2 −4
2 = 8.
Therefore, we get y2 = 4 ±
√
8. However, since also 0 < y < 2, we cannot apply +, so we conclude
that y2 = 4 −
√
8, which implies that the median is
(Y ) =

4 −2
√
2.
4) Clearly, Z = X + Y has its values in ]0, 4[. The frequency is
fZ(z) =
 ∞
−∞
f(x, z −x) dx,
where the integrand is ̸= 0, when 0 < z −x < x < 2. The conditions are
(4) 0 < x < 2
and
z
2 < x < z,
which both should be fulﬁlled.
When f(x, z −x) ̸= 0, then an integral is given by

f(x, z −x) dx =
 1
2 x(z −x) dx = 1
4 zx2 −1
6 x3 = 1
12 x2(3z −2x).
Download free eBooks at bookboon.com

Random variables II
 
109 
5. Miscellaneous examples
a) When z ∈]0, 2[, then the domain of integration is
z
2, z

, according to (4). Hence
fZ(z) =
 z
z
2
f(x, z −x) dx =
 1
12 x2(3z −2x)
z
z
2
= 1
12 z3 −1
24 z3 = z3
24.
b) When z ∈]2, 4[, then the domain of integration is
z
2, 2

, according to (4). Hence
fZ(z) =
 2
z
2
f(x, z −x) dx =
 1
12 x2(3z −2x)
2
z
2
= 1
3 (3z −4) −z3
24 = z −4
3 −z3
24.
Summing up,
fZ(z) =

















z3
24
for 0 < z ≤2,
z −4
3 −z3
24
for 2 < z < 4,
0
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
110 
5. Miscellaneous examples
5) The means are
E{Z} = E{X} + E{Y })8
5 + 16
15 = 40
15 = 8
3,
and
E
 1
Z

=
 2
0
z2
24 dz +
 4
2

1 −4
3 · 1
z −z2
24

dz = 1
9 + 11
9 −4
3 ln 2 = 4
3 (1 −ln 2).
Example 5.13 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =



e−|x| · e−y,
y > |x|,
0,
otherwise.
1) Find the frequencies fX(x) and fY (y) of the random variables X and Y .
2) Find the means E{X} and E{Y } of the random variables X and Y .
3) Prove that the random variables X and Y are non-correlated.
4) Check if the random variables X are Y independent.
5) Find the frequency fZ(z) of the random variable Z = X + Y .
0
0.5
1
1.5
2
–2
–1
1
2
Figure 26: The support of f(x, y) with a couple of paths of integration.
1) Clearly, we must split into the two cases x ≥0 and x < 0.
a) If x ≥0, then
fX(x) = e−x
 ∞
y=x
e−y dy = e−2x.
Download free eBooks at bookboon.com

Random variables II
 
111 
5. Miscellaneous examples
b) If x < 0, then
fX(x) = ex
 ∞
y=−x
e−y dy = e+2x = e−2|x|.
Summing up,
fX(x) = e−2|x|,
x ∈R.
If y ≤0, then fY (y) = 0. If y > 0, then
fY (y) = e−y
 y
−y
e−|x| dx = 2e−y
 y
0
e−x dx = 2e−y 
1 −e−y
.
Summing up,
fY (y) =



2e−y (1 −e−y)
for y > 0,
0
for y ≤0.
2) Due to the exponential factors, the integrals of the means are clearly convergent. We conclude by
the symmetry that
E{X} =
 ∞
−∞
x e−2|x| dx = 0.
Furthermore,
E{Y } =
 ∞
0

2y e−y −2y e−2y
dy = 2 −1
2 = 3
2.
3) It follows from
E{XY } =
 ∞
y=0
y e−y
 y
x=−y
x e−|x| dx

dy = 0 = E{X} · E{Y },
that X and Y are non-correlated.
4) Since f(x, y) ̸= fX(x) fY (y), we conclude that X and Y are not independent.
5) Since f(x, y) is only ̸= 0 for y > |x|, it follows that Z = X + Y can only have values > 0. If z > 0,
then
fZ(z) =
 ∞
−∞
f(x, z −x) dx.
Since z > 0, the integrand is ̸= 0 for x −z < x < z −x, hence for x < z
2. Then
fZ(z)
=

z
2
−∞
e−|x| ex−z dx =
 0
−∞
e2x dx · e−z +

z
2
0
1 dx · e−z
=
1
2 e−z + z
2 e−z = 1
2 (1 + z) e−z.
Download free eBooks at bookboon.com

Random variables II
 
112 
5. Miscellaneous examples
Summing up,
fZ(z) =





1
2 (1 + z) e−z
for z > 0,
0
for z ≤0.
Example 5.14 A rectangular triangle has its shorter sides X1 and X2, where X1 and X2 are inde-
pendent random variables of the frequencies
fX1 (x1) =





1
2 x1,
0 < x1 < 2,
0,
otherwise.
fX2 (x2) =





1
2
0 < x2 < 2,
0,
otherwise.
Let Y1 = X1 + X2 denote the sum of the lengths of the shorter sides, and let Y2 = 1
2 X1X2 denote the
area of the triangle
1) Compute the mean and the variance of Y1.
2) Compute the mean and the variance of Y2.
3) Compute the frequency of Y1.
4) Compute the frequency of Y2.
5) Check if the random variable Z = X2/X1 has a mean, and if so, ﬁnd it.
We start by the following computations,
E {X1} =
 2
0
1
2 x2
1 dx1 =
1
6 x3
1
2
0
= 4
3,
and
E

X2
1

=
 2
0
1
2 x3
1 dx1 =
1
8 x4
1
2
0
= 2,
thus the variance of X1 is
V {X1} = 2 −16
9 = 2
9.
Analogously,
E {X2} =
 2
0
1
2 x2 dx2 =
1
4 x2
2
2
0
= 1,
Download free eBooks at bookboon.com

Random variables II
 
113 
5. Miscellaneous examples
and
E

X2
2

=
 2
0
1
2 x2
2 dx2 =
1
6 x3
2
2
0
= 4
3,
hence
V {X2} = 4
3 −12 = 1
3,
which also follows directly from the fact that X2 is rectangularly distributed over ]0, 2[.
Since X1 and X2 are stochastically independent, the following computations become much easier.
1) The mean and variance of Y1 are
E {Y1} = E {X1 + X2} = E {X1} + E {X2} = 4
3 + 1 = 7
3,
and
V {Y1} = V {X1 + X2} = V {X1} + V {X2} = 2
9 + 1
3 = 5
9.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables II
 
114 
5. Miscellaneous examples
2) The mean and variance of Y2 are
E {Y2} = E
1
2 X1X2

= 1
2 E {X1} · E {X2} = 1
2 · 4
3 · 1 = 2
3,
and
E

Y 2
2

= 1
4 E

X2
1

· E

X2
2

= 1
4 · 2 · 4
3 = 2
3,
hence
V {Y2} = E

Y 2
2

−(E {Y2})2 = 2
3 −
2
3
2
= 2
9.
3) Since X1 and X2 only have values between 0 and 2, it follows that Y1 = X1 + X2 has only values
between 0 and 4, and the frequency of Y1 is given by the convolution integral
fY1 (y1) =
 ∞
−∞
fX1(x) fX2 (y1 −x) dx.
This expression is only ̸= 0, when 0 < x < 2 and 0 < y1 −x < 2, so the restrictions are
0 < x < 2
and
y1 −2 < x < y1.
a) If 0 < y1 ≤2, the restrictions are reduced to 0 < x < y1, hence
fY1 (y1) =
 y1
0
1
2 x · 1
2 dx = 1
8 y2
1.
b) If 2 < y1 < 4, the restrictions are reduced to y1 −2 < x < 2, hence
fY1 (y1) =
 2
y1−2
1
4 x dx =
1
8 x2
2
y1−2
= 1
2 −1
8 (y1 −2)2 = 1
2 y1 −1
8 y2
1.
Summing up,
fY1 (y1) =











1
8 y2
1,
for 0 < y1 ≤2,
1
2 y1 −1
8 y2
1,
for 2 < y1 < 4,
0,
otherwise.
4) Analogously, Y2 = 1
2 X1X2 = X1 ·
1
2 X2

has only values between 0 and 2. The rewriting is
convenient, because 1
2 X2 is rectangularly distributed over ]0, 1[ of the frequency
g (x2) =



1,
for x ∈]0, 1[,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables II
 
115 
5. Miscellaneous examples
If 0 < y2 < 2, then the frequency of Y2 is given by
fY2 (y2) =
 ∞
−∞
fX1(x) g
y2
x
 1
|x| dx.
the integrand is ̸= 0, when 0 < x < 2 and 0 < y2
x < 1, so we get the restrictions
0 < x < 2
and
0 < y2 < x,
thus
y2 < x < 2.
Hence,
fY2 (y2) =
 2
y2
1
2 x · 1 dx
x =
 2
y2
1
2 dx = 1 −1
2 y2,
and summing up,
fY2 (y2) =





1 −1
2 y2
for 0 < y2 < 2,
0
otherwise.
5) Since X1 and C2 aer independent, we get
E{Z} = E

X2 · 1
X1

= E {X2} · E
 1
X1

= 1 ·
 2
0
1
x1
· 1
2 x1 dx1 =
1
2 x1
2
0
= 1.
In particular, the mean exists.
Remark 5.1 It is possible, though far more diﬃcult ﬁrst to solve the questions 3 and 4, from which
questions 1 and 2 can be derived. These computations are far bigger than the computations above. ♦
Download free eBooks at bookboon.com

Random variables II
 
116 
Index
Index
2-dimensional random variable, 5
almost everywhere, 7
causal distribution, 4
ˇCebyˇsev’s inequality, 13
conditional distribution, 11
conditional distribution function, 11
conditional probability, 11
continuous distribution, 5, 6
continuous random variable, 5, 6
convergence in distribution, 16
convergence in probability, 16
correlation, 15
correlation coeﬃcient, 21
covariance, 15
discrete distribution, 4, 6
discrete random variable, 4, 6
distribution function, 4
expectation, 11
frequency, 5, 6
Helly-Bray’s lemma, 16
independent random variables, 7
Jacobian, 10
law of total probability, 11, 18
marginal distribution, 5
marginal frequency, 6
mean, 11
median, 4
moment, 12
null-set, 7
Poisson distribution, 58
polar coordinates, 42
probability ﬁeld, 4
quantile, 4
random variable, 4
rectangular distribution, 42, 76, 79, 85, 96, 111,
112
simultaneous distribution, 5
simultaneous distribution function, 6
skewness, 21
transformation theorem, 8
weak law of large numbers, 16
Download free eBooks at bookboon.com


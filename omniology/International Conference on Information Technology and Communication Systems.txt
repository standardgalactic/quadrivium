Advances in Intelligent Systems and Computing 640
Gherabi Noreddine
Janusz Kacprzyk    Editors 
International Conference 
on Information 
Technology and 
Communication Systems

Advances in Intelligent Systems and Computing
Volume 640
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Advances in Intelligent Systems and Computing” contains publications on theory,
applications, and design methods of Intelligent Systems and Intelligent Computing. Virtually
all disciplines such as engineering, natural sciences, computer and information science, ICT,
economics, business, e-commerce, environment, healthcare, life science are covered. The list
of topics spans all the areas of modern intelligent systems and computing.
The publications within “Advances in Intelligent Systems and Computing” are primarily
textbooks and proceedings of important conferences, symposia and congresses. They cover
signiﬁcant recent developments in the ﬁeld, both of a foundational and applicable character.
An important characteristic feature of the series is the short publication time and world-wide
distribution. This permits a rapid and broad dissemination of research results.
Advisory Board
Chairman
Nikhil R. Pal, Indian Statistical Institute, Kolkata, India
e-mail: nikhil@isical.ac.in
Members
Rafael Bello Perez, Universidad Central “Marta Abreu” de Las Villas, Santa Clara, Cuba
e-mail: rbellop@uclv.edu.cu
Emilio S. Corchado, University of Salamanca, Salamanca, Spain
e-mail: escorchado@usal.es
Hani Hagras, University of Essex, Colchester, UK
e-mail: hani@essex.ac.uk
László T. Kóczy, Széchenyi István University, Győr, Hungary
e-mail: koczy@sze.hu
Vladik Kreinovich, University of Texas at El Paso, El Paso, USA
e-mail: vladik@utep.edu
Chin-Teng Lin, National Chiao Tung University, Hsinchu, Taiwan
e-mail: ctlin@mail.nctu.edu.tw
Jie Lu, University of Technology, Sydney, Australia
e-mail: Jie.Lu@uts.edu.au
Patricia Melin, Tijuana Institute of Technology, Tijuana, Mexico
e-mail: epmelin@hafsamx.org
Nadia Nedjah, State University of Rio de Janeiro, Rio de Janeiro, Brazil
e-mail: nadia@eng.uerj.br
Ngoc Thanh Nguyen, Wroclaw University of Technology, Wroclaw, Poland
e-mail: Ngoc-Thanh.Nguyen@pwr.edu.pl
Jun Wang, The Chinese University of Hong Kong, Shatin, Hong Kong
e-mail: jwang@mae.cuhk.edu.hk
More information about this series at http://www.springer.com/series/11156

Gherabi Noreddine
• Janusz Kacprzyk
Editors
International Conference
on Information
Technology and
Communication Systems
123

Editors
Gherabi Noreddine
ENSA
Khouribga
Morocco
Janusz Kacprzyk
Systems Research Institute
Polish Academy of Sciences
Warsaw
Poland
ISSN 2194-5357
ISSN 2194-5365
(electronic)
Advances in Intelligent Systems and Computing
ISBN 978-3-319-64718-0
ISBN 978-3-319-64719-7
(eBook)
DOI 10.1007/978-3-319-64719-7
Library of Congress Control Number: 2017950085
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The International Conference on Information Technology and Communication
Systems (ITCS 2017) is organized with the objective to bring together researchers,
developers, and practitioners from academia and industry sectors working in all
facets of information technology and communication systems. The conference will
serve as a forum for the dissemination of state-of-the-art research, development, and
implementation of technologies and applications.
The response to the call for papers for ITCS’17 was encouraging. From more
than 130 full papers submitted, 70 were ﬁnally accepted. The review process was
carried out by the Program Committee members; all are experts in information
technology and communication systems. Each paper was reviewed by at least two
reviewers and also checked by the conference co-chairs. The quality of the papers in
these proceedings is attributed ﬁrst to the authors and second to the quality of the
reviews provided by the experts. We would like to thank the authors for responding
to our call, and we thank the reviewers for their excellent work. We were very
pleased to be able to include in the conference program keynote talks by three
world-renowned experts:
We have the honor to welcome our very notable keynotes’ speakers. Many
thanks go to:
– Yves Duthen who is Research Professor of Artiﬁcial Life and Virtual Reality at
IRIT lab, University of Toulouse (France) and has published more than 150 (one
hundred and ﬁfty)-refereed journal and conference papers.
– Philippe Roose who is a Professor of Computer Engineering at UPPA
(University of Pau) France and has published about 140 (one hundred and forty)
refereed journal and conference papers.
– Mohamed Fayad who is a Full Professor of Computer Engineering at San Jose
State University Washington (USA) and has published more than 300+ papers.
(He will not be present; he encountered a problem in the last minute.)
v

We would also like to thank the members of the local committee for their advice
and help. We are grateful to Springer’s editorial staff for supporting this publication.
Finally, we were very pleased to welcome all the participants to this conference.
Gherabi Noreddine
vi
Preface

Contents
Text Mining, Decision Models, Ontologies, Semantic
Similarity Databases
Decisional Information System for the “NWA” Meteorites . . . . . . . . . . .
3
Lahcen Ouknine, Hanane Bais, Fouad Khiri, Mustapha Machkour,
Abderrahmane Ibhi, and Lahcen Koutti
Collecting and Processing Multilingual Streaming Tweets
for Sentiment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
Abdeljalil Elouardighi, Hafdalla Hammia, and Mohcine Maghfour
Synonym Reduction in a Comparative Study of Clustering
Algorithms in Text Mining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
Abdennour Jalil, Noureddine Aboutabit, and Imad Haﬁdi
Using Statistical and Semantic Analysis for Arabic
Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
Nabil Alami, Yassine El Adlouni, Noureddine En-nahnahi,
and Mohammed Meknassi
Entity Resolution in NoSQL Data Warehouse . . . . . . . . . . . . . . . . . . . . .
51
Lamiae Alami, Imad Haﬁdi, and Abdelmotalib Metrane
Automation of a Fault Management System for Bahraini
Telecommunication Companies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
Abdul Fattah Salman, Mahmood Majeed, and Ahmed Alsahlawi
Towards Classiﬁcation of Web Ontologies Using the Horizontal
and Vertical Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
Redouane Nejjahi, Noreddine Gherabi, and Abderrahim Marzouk
Model-to-Model Transformation in Approach by Modeling
to Generate a RIA Model with GWT . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
Redouane Esbai, Mohammed Erramdani, Fouad Elotmani,
and Mohamed Atounti
vii

An Enhanced Method to Compute the Similarity Between Concepts
of Ontology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
Abdelhadi Daoui, Noreddine Gherabi, and Abderrahim Marzouk
Towards a Completeness Prediction Based on the Complexity
and Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
Jaouad Maqboul and Bouchaib Bounabat
Toward the Development of a General Semantic Repository
for the Interoperability of Information Systems Based
on Ontological Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
Meryem Fakhouri Amr, Khalifa Mansouri, Mohammed Qbadou,
and Bouchaib Riyami
Modeling Aircraft Landing Scheduling in Event B . . . . . . . . . . . . . . . . .
127
Abdessamad Jarrar, Youssef Balouki, Taouﬁq Gadi,
and Sallami Chougdali
Load Signatures Identiﬁcation Based on Real Power Fluctuations . . . . .
143
El Bouazzaoui Cherraqi and Abdelilah Maach
Networks Security, Wireless and Network Computing,Telecom
and Transmission Systems, Intrusion Detection Systems
A New Secure SIP Authentication Scheme Based on Elliptic
Curve Cryptography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
Mourade Azrour, Mohammed Ouanan, and Yousef Farhaoui
An Adaptation of GRA Method for Network Selection
in Vertical Handover Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
Mouad Mansouri and Cherkaoui Leghris
Toward Cluster Head Selection Criterions in Mobile
Ad Hoc Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
Meriem Ait Rahou and Abderrahim Hasbi
A New Method of IPv6 Addressing Based on RFID in Small
Objects Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
Ali El Ksimi, Cherkaoui Leghris, and Khoukhi Faddoul
A New Alert Message Dissemination Protocol for VANETs
and Simulation in a Real Scenario. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
Hamid Barkouk and El Mokhtar En-Naimi
Direction of Arrival in Two Dimensions with Matrix
Pencil Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
Mohammed Amine Ihedrane and Seddik Bri
A Real-Time Risk Assessment Model for Intrusion Detection
Systems Using Pattern Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
El Mostapha Chakir, Mohamed Moughit, and Youness Idrissi Khamlichi
viii
Contents

Compact Tetraband PIFA Antenna for Mobile
Handset Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
Mohamed Tarbouch, Abdelkebir El Amri, and Hanae Terchoune
A Hybrid NIPS Based on PcapSockS Sniffer and Neural MLP. . . . . . . .
253
Azidine Guezzaz, Ahmed Asimi, and Younes Asimi
Convolutional Codes BPSK Modulation with Viterbi Decoder . . . . . . . .
267
Nejwa El Maammar, Seddik Bri, and Jaouad Foshi
Cloud Computing, Big Data Analytics, Software Security
and Monitoring, Multimedia Information Processing
A New Monitoring Approach with Cloud Computing for Autonomic
Middleware-Level Scalability Management Within IoT Systems. . . . . . .
281
Mohamed Nabil Bahiri, Abdellah Zyane, Abdelilah Ghammaz,
and Christophe Chassot
MDA Approach for Application Security Integration with Automatic
Code Generation from Communication Diagram . . . . . . . . . . . . . . . . . . .
297
Lasbahani Abdellatif, Mostafa Chhiba, Abdelmoumen Tabyaoui,
and Oussama Mjihil
Context-Aware for Service Composition Optimization
in Cloud Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311
Ali Bentaleb and Ahmed Ettalbi
A New Architecture of Search Information System in the Bulletin
Ofﬁcial of Kingdom of Morocco. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
322
Anass Smaili, Mohamed Sbihi, and Abdelali Lasfar
Automatic Seeded Region Growing Based on Texture Features
for Mass Segmentation in Digital Mammography . . . . . . . . . . . . . . . . . .
331
Moustapha Mohamed Saleck and Abdelmajide El Moutaouakkil
Robust Video Coding Based on Perceptual Unequal Protection . . . . . . .
339
Ouafae Serrar, Oum el Kheir Abra, Mohamed Youssﬁ,
and Ahmad Tamtaoui
3D Object-Parts Classiﬁcation Based on a Multiclass-SVM
Approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
Omar Herouane, Hajar Hardi, and Lahcen Moumoun
Adapting Alkhalil Morpho Sys as Part of a Translation System
from Arabic to Arabic Sign Language . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
Mourad Brour and Abderrahim Benabbou
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
369
Contents
ix

Text Mining, Decision Models,
Ontologies, Semantic Similarity
Databases

Decisional Information System for the “NWA” Meteorites
Lahcen Ouknine1(✉), Hanane Bais2, Fouad Khiri1, Mustapha Machkour2,
Abderrahmane Ibhi1, and Lahcen Koutti2
1 Petrology, Metallogeny and Meteorites Laboratory, Faculty of Sciences,
Ibn Zohr University, Agadir, Morocco
lahcen.ouknine@edu.uiz.ac.ma
2 Team of Engineering of Information Systems, Information Systems and Vision Laboratory,
Faculty of Sciences, Ibn Zohr University, Agadir, Morocco
baishanan@gmail.com
Abstract. Since 1999, the number of meteorites found in the Northwest of Africa
(Morocco, Mauritania, Mali and Algeria) has seen a considerable increase. Being
responsible for the approval of new meteorites and meteorite names, the Nomen‐
clature Committee of the “Meteoritical Society” reacted promptly and classiﬁed
these meteorites under the acronym “NWA” (North West Africa). As a result,
92% of the meteorites of this region are classiﬁed under this nomenclature. Then,
this acronym undermines the meteoritic heritage of Morocco and its neighboring
countries. We tried to contextualize the 5678 of NWA meteorites ﬁnds until their
oﬃcial declaration dated 31st December 2014, by attributing to every meteorite
the harvest place and an appropriate name. To better manage the information
about these meteorites and allow the feasibility of the detailed studies on their
characteristics, we have created a business intelligence (BI) information system
constituted by a data warehouse with two user interfaces: desktop and WEB.
Keywords: BI · Decisional Information System · Northwest Africa · NWA
meteorites ﬁnds · Relational database
1
Introduction
The Meteorites constitute all the objects of celestial origins, which fall on the Earth
incessantly. However, 75% of these stones disappear mostly in the oceans and 25% of
them remain. Little are recovered due to causes of meteorology and nature of the ground
of the fall [1]. Meteorites can be distinguished according to the circumstances from their
discoveries between “falls”, where the trajectory of the celestial body is observed, and
the sample is recovered then, and “ﬁnds” in case the meteorite is collected spending a
long time on the ground.
The number of the meteorite ﬁnds in North-West Africa (Morocco, Mauritania, Mali
and Algeria) has seen a considerable increase since 1999 [2, 3]. The meteorites collected
in this region include rare specimens of high scientiﬁc value, making them highly sought
by scientists and collectors alike, from all over the world [4, 5]. Nonetheless, the clas‐
siﬁcation of these meteorites, done by the Meteorite Nomenclature Committee of the
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_1

“Meteoritical Society”, only attributes 8% of the total ﬁnds of this region to their speciﬁc
country of origin, leaving the other 92% under the mere appellation “NWA” followed
by a number [6].
The nomenclature “NWA” engenders a negative impact on the meteoritic heritage
of the countries of the Northwest of Africa [7], and the beneﬁt of which they can take
advantage these countries in a case where meteorites are classiﬁed under the names of
proper places of ﬁnds. Besides, the cultural and ecotourism losses and socioeconomic
alterations materialized by the devaluation of the samples collected in this area despite
their possible rarity and their signiﬁcant scientiﬁc value.
This work tries to respond to solicitations from researchers of the region to highlight
the meteorites recovered in Morocco and neighboring countries [8–10]. It consists of
exploring the circumstances of ﬁnds of the various NWA meteorites and attributing to
every sample the harvest place. To manage the information on meteorites eﬃciently and
allow statistics and detailed studies about their characteristics, we created a Business
Intelligent (BI) information system [11] with an analytical database [12] and two user
interfaces; one is the Web type, and the other one is the kind desktop.
This paper is organized as follows. First, we describe the methodology related to the
conception and the modeling of the system. Then, we show some results of this work,
and ﬁnally we present conclusion and some extensions of this work.
2
Methodology: Modeling and Conception of the System
The enhancement of this heritage ﬁrst requires assigning each meteorite country of ﬁnd.
In this sense, we tried to contextualize the 5678 of NWA meteorites ﬁnds oﬃcially
declared by December 31st, 2014. Consequently, we deﬁne the circumstances of each
sample discovery. We depended on information that provides databases and speciﬁc
articles on the subject and according to the guidelines of the new Categorization of Finds
and Falls [13] and the new Guidelines for Meteorite Nomenclature [14] adopted by the
Nomenclature Committee in February 2015. The information about NWA meteorites
collected by following two stages:
– Extraction of meteorites with the acronym “NWA” from the database of Meteoritical
Bulletin (http://www.lpi.usra.edu/meteor/). The number of the oﬃcial NWA mete‐
orites is 5678 samples on December 31st, 2014;
– The precision of the circumstances of the ﬁnd of every oﬃcial NWA meteorite, one
by one, through the information, which is giving by databases and specialized articles
on the subject. The deﬁnition of the place of discovery is made according to the new
categorization of ﬁnds and falls adopted by the Meteorite Nomenclature Committee.
After having collected the information about NWA meteorites, the next step consists
of the modeling and the conception of the database. The phase of design is a major step
in the process of building our database, allowing us to understand and model the infor‐
mation system on which we work, by respecting the structure imposed by the needs
analysis. The creation of this database will allow us to simplify the management of NWA
meteorites and to overcome the use of Excel and Word documents. For modeling the
4
L. Ouknine et al.

database, we use the entity-relationship model [15], which is converted into relational
model [16], and then converted into MySQL database.
To make use of information stored in our database in a simple, easy way, we have
created a desktop and web applications. For creating the desktop application, we have
used the programming language Java with Netbeans software development Platform.
Concerning the creation of web application, we have used J2EE Platform.
3
Results and Discussion
In addition to oﬃcial 1180 “NWA” meteorites whose country of the ﬁnd is approved
by the Nomenclature Committee, we have to reallocate 3240 meteorites to four countries
in Northwest Africa. We concluded that 2994 meteorites (92%) are from Morocco, 79
samples (2.5%) are from Algeria, 34 samples were collected in Mauritania (1.1%), and
12 samples (0.1%) were reassigned to Mali. Nevertheless, 1267 of the remaining NWA
meteorites have no information indicating the country of ﬁnd. This result conﬁrms by
Idomar’s assumption in 2014 [17] on the origin of NWA meteorites.
The database composed of ﬁve tables: the meteorite name, type, subtype, country
and region table. The ﬁgure bellow shows the logical schema of our database (Fig. 1):
Fig. 1. Logical schema of database.
The desktop application allows its user to manage the information stored in the
database with a standard operation (selection, inserting, deleting, and updating). Also
this application can realize a signiﬁcant number of statistics about NWA meteorites. The
following ﬁgure shows the architecture of our desktop application (Fig. 2).
Decisional Information System for the “NWA” meteorites
5

Fig. 2. Architecture of our desktop application.
The following Fig. 3 shows the interface used for manipulating the information
stored in the database:
Fig. 3. The interface used for manipulating the information.
As already said, this interface would allow the user to add, delete, modify and view
the meteorite. In addition, the interface gives the user the ability to add and view images
of diﬀerent meteorites. The user can also perform advanced searches about information
stored in our database.
Figure 4 shows the interface of statistics. This interface helps users to display the
results in diagram form. The user can customize the results obtained and chooses the
type of chart needed for representing the results.
6
L. Ouknine et al.

Fig. 4. The interface of statistics.
To customize the results of the statistics, the user uses the following interface (Fig. 5):
Fig. 5. Interface of customizing.
We have already mentioned that we realize two types of applications: a desktop
application and a web application. The general objective of the web application is to
create a clickable map, which allows user to view information about the meteorites in a
particular point in the map of the Northwest of Africa.
The following ﬁgure shows a capture of the web application screen. The capture
displays the diﬀerent meteorites found in the Zagora region.
Decisional Information System for the “NWA” meteorites
7

To display this information the user simply click on the desired region, the system
will display that information in tabular form. To improve the functioning of this appli‐
cation, we will add the ability to view this information in chart form (Fig. 6).
Fig. 6. Web application of NWA meteorite.
4
Conclusion and Perspectives
This work resulted in the creation of a new decisional information system of Northwest
Africa meteorites. The idea of creating the database, desktop and web application helps
the user to manipulate easily the information being collected and meeting statistical
need.
In the other part, the created Decisional Information System can be information
resource for a possible re-nomenclature of the NWA meteorites by the Meteorite
Nomenclature Committee of the “Meteoritical Society”. This re-nomenclature will
certainly value the meteorite heritage of Morocco and other countries in Northwest of
Africa [18], a priceless heritage of scientiﬁc and socioeconomic value.
References
1. Bland, P.A., Smith, T.B., Berry, F.J., Pillinger, C.T.: The ﬂux of meteorites to the earth over
the last 40,000 years. Meteoritics 30, 488 (1995)
2. Connolly, H.C., Zipfel, J., Folco, L., Smith, C., Jones, R.H., Benedix, G., Righter, K.,
Yamaguchi, A., Aoudjehane, H.C., Grossman, J.N.: The meteoritical bulletin, no. 91.
Meteorit. Planet. Sci. 42, 413–466 (2007)
8
L. Ouknine et al.

3. Ibhi, A., Nachit, H., Abia, H.: Tissint meteorite: new mars meteorite fall in morocco. J. Mater.
Environ. Sci. 4(2), 293–298 (2013)
4. Ibhi, A., Douzi, H., Khiri, F., Ouknine, L.: Musée Universitaire de Météorites, un projet
participative. Premier Colloque International, la Muséologie dans le champ de la Culture
Scientiﬁque et Technique, Rabat, Maroc, 16–18 mai 2016
5. Khiri, F., Ibhi, A., Ouknine, L.: Temporal and spatial distribution of meteorites falls in Africa.
In: Proceedings of the International Meteor Conference, Mistelbach, Austria, pp. 117–119.
IMO (2015)
6. Grossman, J.N.: The meteoritical bulletin, no. 84. Meteorit. Planet. Sci. 35, A199–A225
(2000)
7. Ouknine, L., Khiri, F., Ibhi, A.: Les météorites africaines appelées “NorthWest Africa”,
patrimoine sans ﬁliation. Recueil des résumés du 22ème Colloque International des Bassins
Sedimentaires Marocains, Fès, Maroc, 18–20 Décembre 2015, p. 24 (2015)
8. Ibhi, A.: Morocco meteorite falls and ﬁnds: some statistics. Int. Lett. Chem. Phys. Astron. 1,
18–24 (2014)
9. Chennaoui, H., Jambon, A., Larouci, N.: On meteorites from Morocco and the NWA meteorite
nomenclature. In: 76th Meteoritics and Planetary Science Supplement (2013)
10. Larouci, N., Chennaoui, H., Jambon, A.: Methodology of studying Moroccan meteorites:
alternative solution to “NWA” nomenclature. In: 77th Annual Meteoritical Society Meeting,
p. 5349 (2014)
11. Michalewicz, Z., Schmidt, M., Michalewicz, M., Chiriac, C.: Adaptive Business Intelligence,
pp. 37–46. Springer, Heidelberg (2006)
12. McGuﬀ, F., Kador, J.: Developing Analytical Database Applications. Prentice Hall PTR,
Upper Saddle River (1999)
13. Agee, C., Bullock, E., Bouvier, A., Dunn, T., Gattacceca, J., Grossman, J., Herd, C., Ireland,
T., Metzler, K., Mikouchi, T., Ruzicka, A., Smith, C., Welten, K., Welzenbach, L.:
Categorization of Finds and Falls. Published in Meteoritical Society. http://www.lpi.usra.edu/
meteor/docs/falls-ﬁnds.pdf. Accessed 10 Dec 2015
14. Agee, C., Bullock, E., Bouvier, A., Dunn, T., Gattacceca, J., Grossman, J., Herd, C., Ireland,
T., Metzler, K., Mikouchi, T., Ruzicka, A., Smith, C., Welten, K., Welzenbach, L.: Guidelines
for meteorite nomenclature. Published in Meteoritical Society website. http://
meteoriticalsociety.org/?page_id=59. Accessed 12 Dec 2015
15. Chen, P.P.S.: The entity-relationship model-toward a uniﬁed view of data. ACM Trans.
Database Syst. (TODS) 1, 9–36 (1976)
16. Codd, E.F.: Relational database: a practical foundation for productivity. Commun. ACM 25,
109–117 (1982)
17. Idomar, Y., Chennaoui, H., Jambon, A., Elkerni, H., Larouci, N., Mongy, Y.: Statistical
considerations on meteorites from Morocco and Northwest Africa (NWA). In: 77th Annual
Meteoritical Society Meeting, p. 5351 (2014)
18. Ouknine, L., Khiri, F., Ibhi, A.: Study of the circumstances of meteorites “Northwest Africa”
ﬁnds: contribution to an appropriate renomenclature. In: 79th Annual Meeting of the
Meteoritical Society, p. 656 (2016)
Decisional Information System for the “NWA” meteorites
9

Collecting and Processing Multilingual
Streaming Tweets for Sentiment Analysis
Abdeljalil Elouardighi1,2(B), Hafdalla Hammia1, and Mohcine Maghfour1
1 LM2CE Laboratory, Hassan 1st University, FSJES, Settat, Morocco
abdeljalil.elouardighi@uhp.ac.ma, hhammia@gmail.com,
maghfour.mohcin@gmail.com
2 LRIT Laboratory, FSR, Mohammed V University, Rabat, Morocco
Abstract. Sentiment analysis is a new ﬁeld of study that allows to
transform the publications on the social networks into exploitable data
to analyze trends, probing consumer opinion or direct advertising cam-
paigns. Many studies have focused on the sentiment analysis for the
English language. However, few studies have focused on the Arabic lan-
guage which is a native language for Millions of people who use social
network. This paper addresses some approaches in sentiment analysis for
multilingual tweets. At ﬁrst we have installed and conﬁgured a platform
for real-time collecting and preprocessing multilingual tweets (Arabic,
French and English). In the second time, we have applied factorial corre-
spondence and multiple correspondence analysis for analyzing tweets. We
have used this platform for sentiment analysis on the Mawazine festival,
which took place in Rabat between May 20 and 28, 2016.
Keywords: Sentiment analysis · Hadoop platform · Flume · Correspon-
dence analysis · Multiple correspondence analysis
1
Introduction
Sentiment analysis (SA), or opinion mining is a ﬁeld of study which attempts to
analyze people’s opinions, sentiments, attitudes, and emotions on entities such
as products, services, and organizations. The expression SA was ﬁrst appeared in
[1], and the expression opinion mining ﬁrst appeared in [2]. Several authors would
use the appellations: sentiment analysis and opinion mining interchangeably. The
automated study of the sentiments and the opinions expressed in unstructured
documents gave rise, since a few years, to environments and tools specialized
to analyze the feelings of the users of social networks through their publica-
tions. Twitter is one of the biggest platforms of microblogging where tweets are
massively published every day. Users tend to express their feelings freely in Twit-
ter, which makes it an ideal source for capturing the opinions towards various
interesting topics, such as brands, products or celebrities, etc. [3]. Researchers
have proposed many diﬀerent approaches for SA. In general, there are two main
c
⃝Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7 2

Collecting and Processing Multilingual Tweets
11
methods, the ﬁrst one uses machine learning techniques or supervised techniques,
and the other one uses unsupervised techniques. Pang et al. [4] have used machine
learning techniques for sentiment classiﬁcation. They employed three classiﬁers
(Naive Bayes, Maximum Entropy classiﬁcation, Support Vector machines). Their
data source was the Internet Movie Database (IMDB); Dave et al. [2] have pro-
posed an approach, which begins with training a classiﬁer using a corpus of
self-tagged reviews available from major web sites.
Many researches were proposed to analyze sentiment and extract opinions
from the World Wide Web. This proved to be important due to the large amount
of data contributed by users in websites such as social networks (Facebook) or
micro blogging(Twitter) etc. Among these works we can cite those provided by
Hassan et al. [5]. These authors have used a new approach of adding semantics
as additional features into the training set for SA. For each extracted entity (e.g.
iPhone) from tweets, they add its semantic concept (e.g. “Apple product”) as
an additional feature, and measure the correlation of the representative concept
with negative/positive sentiments. In [6], Kumar et al. expound a hybrid app-
roach using both corpus based and dictionary based methods to determine the
semantic orientation of the opinion words in tweets. A case study was presented
to illustrate the use and eﬀectiveness of the proposed system.
Kirithenko et al. [7] lead a SA for courts informal texts at the level of the
message and the term. This analysis describes a state-of-the-art SA system that
detects: the sentiment of short informal textual messages such as tweets or SMS
(message-level task) and the sentiment of a word or a phrase within a mes-
sage (term-level task). Several semi-supervised approaches for SA have recently
attracted much attention. These approaches include the one proposed by Tang
et al. [8] to evaluate diﬀerent types of emotional signals in Twitter data using a
correlated model. The model presents dual learning based on controlled alternat-
ing propagating and ﬁtting processes operating on labeled and unlabeled data.
Zhou et al. [9] applied a semi-supervised approach Fuzzy Deep Belief Network
(FDBN) for SA. The deep architecture of FDBN consists of a set of unsupervised
hidden layers and a ﬁnal layer of supervised training.
We have noticed that few studies have been conducted for SA using an Arabic
text. The work reported by Farra et al. [10] utilized a set of dictionaries that
store positive, negative and neutral roots. To determine the sentiment or class of
a sentence, a stemmer was applied to transfer words into roots. If the resultant
root exists in the positive/negative/neutral root dictionary, then that sentence
is considered positive/negative/neutral, respectively. If the word is not in the
dictionary, the user is asked to specify its polarity and subsequently its root
is added to the corresponding dictionary. Abdul Majeed et al. [11] presented a
manually annotated corpus developed from standard modern Arabic (MSA) with
a new polarity lexicon. First, they used Machine Translation (MT) to translate
an existing English subjectivity lexicon. Then, they employed a random graph
walk based method to expand automatically an organized Arabic lexicon. Refaee
et al. [12] presented a manually annotated Arabic social corpus of 8,868 Tweets
and they discussed the method of collecting and annotating the corpus. Ibrahim
et al. [13] built a manual corpus of 1,000 tweets and 1000 microblogs and used
it for sentiment analysis task.

12
A. Elouardighi et al.
Our contribution in this work is focused on SA using multilingual tweets that
we have collected. We present at ﬁrst, the platform that we have installed and
conﬁgured for real-time collecting tweets as well as the tools which we have used
for preprocessing and analyzing tweets. Secondly we expose the approach that
we have used to analysis these tweets. This approach is part of the unsuper-
vised techniques and is based on the application of the factorial correspondence
analysis (CA) and the multiple correspondence analysis (MCA). This platform
and approaches were used for SA on multilingual tweets (Arabic, French and
English) which we were able to collect during the Mawazine festival that took
place in Rabat between May 20 and 28, 2016.
This article is organized as follows: in the Sect. 2 we present the platform
and tools that we have installed and have used for real-time collection and pre-
processing of tweets during the period of Mawazine festival. In the third section,
we detail the performed analysis on the pre-processed tweets and the synthesis
of the obtained results. Finally, a conclusion and some future perspectives of this
work, are presented in the last section.
2
Collecting and Pre-processing Streaming Tweets
Data for SA are generally produced in semi or unstructured format containing
opinions, emotions or attitudes. It comes generally from a social networks. In
this work we present a methodology for discovering public sentiment over time
for Twitter users after a particular event. The architecture that we have imple-
mented allows to collect tweets in real-time from Twitter and pre-process them
in Hadoop [14] ecosystem platform. Our tweet analysis methodology is inspired
from the classical Data mining methodology [15] but applied to unstructured
data, like tweets. It can be summarized in three main stages: Data collection
(with Flume), pre-processing (with Pig and R) and analysis (with R). In this
section, we present our methodology for collecting and processing data from
twitter and the platform that we have installed and conﬁgured for this purpose.
2.1
Data Collection Environment
Tweets data are collected in real-time from Twitter [16] and stored in the HDFS
(Hadoop Distributed File System). So for this work, we collected data with
the Streaming API which allows extracting tweets from twitter using apache
Flume that is installed and conﬁgured on a virtual machine Hadoop [14,17].
Streamining API [16] provides collecting real-time tweets by launching a request
for an unlimited period, as long as there is no server connection problem or that
the program was not stopped. Three levels of access are oﬀered to the Streaming
API, “the Spritzer, the Gardenhose and Firehose”, which deliver up to 1%, 10%
and 100% (respectively) of all tweets posted. The only level of access available
to the public is the Spritzer, it can recover a 1% sample of all tweets [16].
Generally, the APIs return query results in a raw format JSON [18] (JavaScript
Object Notation).

Collecting and Processing Multilingual Tweets
13
Apache Hadoop. Hadoop is an open source big data management framework,
developed by the Apache Software Foundation [17]. The core of Apache Hadoop
consists of a storage part, known as Hadoop Distributed File System (HDFS),
and a processing part called MapReduce [19]. With Hadoop, we can process
structured, semi-structured and unstructured.
Flume. Flume [17] is a solution of collection and aggregation streaming data
that will be stored and processed in Apache Hadoop. We have used it to collect
a large amounts of streaming tweets from Twitter using keywords or hashtag
ﬁlter and store them into HDFS [17]. Technically, a Flume agent allows creating
ways to relate a source to a target via channel of exchange (Fig. 1).
– The “Source”: a source of data from which Flume receives data. A variety of
sources allow data to be collected, such as logs and Twitter data.
– The “Channel”: is a buﬀer that stores messages received temporarily.
– The “Sink”: removes the event from the channel and puts it into an external
repository like HDFS (via Flume HDFS sink).
Fig. 1. Flume functioning
2.2
Data Pre-processing Tools
This step involves pre-processing of the extracted data using the Apache Pig [17]
before analyzing them with R software.
Apache Pig for Preprocessing. Apache Pig [17] is a big data processing
platform that allows useful treatment on ﬂow data for special purposes. It is a
component of the Hadoop ecosystem. The minimum steps in a Pig program are:
– Loading data
– Transformations/processing data
– Store results
We have used Pig in the preprocessing phase.

14
A. Elouardighi et al.
R Software for Analysis. R is a programming language and software envi-
ronment for statistical computing and graphics supported by the R Foundation
for Statistical Computing. The R language is widely used among statisticians
and data scientists for developing statistical applications and data analysis [20].
We have used R in the preprocessing and analysis phase.
Dataset Description. We have exploited our platform, from May 20 until
June 2, to collect the exchanged tweets about Mawazine festival that took place
from May 20 to May 28 in Rabat, Morocco. The purpose of our process is to
analyze expressed sentiment about the festival. Thus during this period we have
collected 14 638 tweets. After building the dataset, we have applied some pre-
processing on it. This task was very diﬃcult, because it required a human eﬀort
and time-consuming. To lead well this task, we have used tools, as Pig and R
software for an automatic processing and follow-up of a stage of manual check.
During this stage we have deleted retweets, tweets published by singers and
organizers of the festival, besides the tweets that have nothing to do with the
event. For the variables, we have removed those that were not relevant to our
analysis, mainly variables that contains high level of missing data, alongside with
variables deﬁned by a single category.
Otherwise, to lead our analysis we created two additional variables from
the collected dataset. The ﬁrst one is the variable “user.sex” that represents
the gender of users, it is a qualitative variable which categories are: “male” for
the men, “female” for the women and “unspeciﬁed” for users from whom we
were unable to identify the gender through the collected dataset. To build this
variable we have used several other variables from our dataset, like user name,
user description, user photo, etc.
The second variable that we have created is “sentiment”, it represents the
users sentiment about Mawazine festival, expressed in their tweets. It is a qualita-
tive variable which categories are: “positive” for those who expressed an positive
opinion on the festival, “negative” for those who did not love the festival or
“neutral” for those who expressed no opinion on the festival.
Table 1. Example of tweets to preprocess
Tweet (text)
User.sex
Language Sentiment
Female
Arabic
Positive
Festival Mawazine 2016 Kendji entre en,
conference de presse https://t.co/
142TVQwGbA, via @YouTube
Unspeciﬁed French
Neutral
@FestMawazine love mawazine
Male
English
Positive
#Mawazine
Female
Arabic
Negative
Why does morocco choose to have mawazine
festival one week before ramadan lol so stupid
Female
English
Negative

Collecting and Processing Multilingual Tweets
15
For building the sentiment variable, we were inspired by the methods already
used in previous published works [21,22], therefore this task was accomplished
by examining the text of the tweet in order to detect words and signs that allow
aﬀecting the embedded sentiment in tweets, so it can take the levels “positive”,
“neutral” or “negative”.
After the preprocessing, we end up with a dataset containing 3698 tweets.
We present in the following table an example of the tweets (Table 1), which
we preprocessed besides the users sex, the used language and the expressed
sentiment.
3
Sentiment Analysis Using Factorials Methods
After the completion of data collection and pre-processing outlined in the pre-
vious section, we devote this section to the analysis of tweets with the main
objective of SA. Among the variables that we use in our analysis, we mention
text of the tweet, sex of the tweet publisher and the language used as well, so
the purpose of our analysis is to identify the proﬁles of people who was attended
Mawazine festival and their sentiment about it.
3.1
Descriptive Analysis
In the ﬁrst part of this section, we attempted to describe the key variables of
our study, for this, we determined the relative proportions of each category of
variables: sentiment, user’s sex and language of the tweet. Moreover, we have
explored the tendency of tweets during and after the days of the festival in order
to draw conclusions about their evolution.
First, we notice that one-quarter of tweets (25%) has expressed positive sen-
timent and only 6% who made a negative reaction on Mawazine festival (Fig. 2).
However, more than Two-thirds of tweets (69%) has expressed neutral senti-
ment; this part represents twitter users who reported their information about
the event with an objective prospect. Therefore, we can presume that most of
them are Media, but we have led an analysis to conﬁrm this hypothesis.
Fig. 2. Sentiments of tweets about Mawazine festival

16
A. Elouardighi et al.
Fig. 3. Languages of tweets about Mawazine
The collected dataset contains balanced proportions between French tweets
and the English ones (successively by 45% and 43%), while the Arabic language
concerns only 12% of total tweets (Fig. 3).
The last pie chart (Fig. 4) shows the dominance of tweet proﬁles with no
speciﬁed user sex (42%). in fact, this category contains two user subgroups;
the ﬁrst consists of individuals who did not put any clue about their sex iden-
tiﬁcation in their accounts (e.g. name, photo proﬁle, description. . . ). Whereas
organizations and Media are joined in the second one. Although, by examining
collected dataset, we found that the proportion of men (35%) is higher than the
proportion of women (23%) when posting tweets about Mawazine festival.
Fig. 4. Sex of tweet publishers about Mawazine
Before analyzing tweets evolution, it is important to remind that we have
activated the data collection process during the period of Mawazine festival,
which started in May 20 and ﬁnished in May 28. However, we left the process
activated until June 2nd to collect late feedbacks, we stopped data collection at
this day because we have noticed signiﬁcant decrease in the posted tweets about
the event.
Examination of line graph of tweets (Fig. 5) permits to distinguish three peaks
in global: On May 22, the peak coincides with the third day of the festival; it

Collecting and Processing Multilingual Tweets
17
0
100
200
300
400
500
May 22
May 24
May 26
May 28
May 30
Jun 01
days
Number of tweets
negative
neutral
positive
total
Fig. 5. Evolution of tweets during and after the days of Mawazine festival
reﬂects the ﬁrst wave of reactions by people and Media. On May 25, we have
noticed a high peak of tweets, yet the highest peak was registered on May 28 the
last day of the festival. We explain this by the rise of reactions especially from
the Media who intended to notify the end of festival. Other factors can explain
more or less the peaks recorded in tweets, since we can link the occurrence of
these peaks with the popularity of the artists invited to perform in the festival
concerts [23]. According to media that covered the festival, three singers have
registered Mawazine attendance records: Two stars of Moroccan song (Mai 25
and 28 respectively) and an American star (Mai 28) [24].
3.2
Factorial Analysis
Correspondence Analysis. The interest of using correspondence analysis
(CA) diﬀers along with the goal of each case study. Thus, for analyzing senti-
ments expressed in the collected tweets about Mawazine festival, we will use CA
as a direct synthesis tool for the main object to describe relationships between
the studied variables [25]. We will apply correspondence analysis twice, at ﬁrst,
CA1 to deﬁne the relationship between sentiments and the sex of tweets publish-
ers, by referring to their contingency table (Table 2), then, CA2 to identify the
relationship between this sentiments and the used language for posting tweets,
also by relying on their common contingency table (Table 3).
In both CA1 and CA2, two factors capture 100% of the total inertia within
their respective contingency table, the restricted number of levels makes the ﬁrst
factor representing almost all the information, which is 96% in CA1 and 98% in
CA2 (Table 4).
The graphical representations of CA1 (Fig. 6) and CA2 (Fig. 7) shows the
distance between levels and their relative position to the factors, nevertheless

18
A. Elouardighi et al.
Table 2. Contingency table of user sex and sentiment
Negative Neutral Positive Total
Female
75
427
349
851
Male
107
856
343
1306
Unspeciﬁed
33
1265
243
1541
Total
215
2548
935
3698
Table 3. Contingency table of language and sentiment
Negative Neutral Positive Total
Arabic
68
190
199
457
English
76
1110
412
1598
French
71
1248
324
1643
Total
215
2548
935
3698
Table 4. Quality of factors representation of CA1 and CA2
CA1
CA2
Dim1
Dim2 Dim1 Dim2
Inertia
0.074 0.003
0.06
0.00
% of variance
95.98
4.018
97.98
2.02
% of cumul var 95.98
100
97.98
100
we can only interpret the distance between levels within the same variable [26].
Here, we notice that all similar levels are quite distant from each other, such as
“unspeciﬁed”, “male” and “female” in CA1, or “neutral”, “positive”, “negative”
in both CA1 and CA2; this indicates the diﬀerent distributions that characterize
those categories.
Finally the results of the ﬁrst correspondence analysis shows a strong rela-
tionship between sentiment and user’s sex variables with a khi2 statistic equal
to 284.88 , this is also the case with CA2, because we ﬁnd a khi2 equal to 215.99,
conﬁrming that the expressed sentiment and tweet language are highly related.
Multiple Correspondence Analysis. In the previous part of this section
we have demonstrated trough correspondence analysis the existence of strong
relationship between our analyzed variables, nevertheless so far we didn’t deﬁne
how their categorical levels are connected, so for this reason we are going to
apply multiple correspondence analysis(MCA). MCA allows analyzing a multiple
contingency table involving diﬀerent categorical variables when it is assumed
that all the variables must be analyzed at multiple nominal level [27].

Collecting and Processing Multilingual Tweets
19
female
male
unspecified
negative
neutral
positive
−0.2
−0.1
0.0
0.1
0.2
−0.2
0.0
0.2
0.4
Dim1 (96%)
Dim2 (4%)
Fig. 6. Biplot representation of CA1
arabic
english
french
negative
neutral
positive
−0.04
0.00
0.04
0.08
0.0
0.2
0.4
0.6
Dim1 (98%)
Dim2 (2%)
Fig. 7. Biplot representation of CA2
Table 5. Quality of factors representation with MCA
Dim1 Dim2 Dim3 Dim4 Dim5 Dim6
Inertia
0.50
0.35
0.35
0.31
0.24
0.24
% of variance
25.18
17.81
17.45
15.53
12.12
11.90
% of cumul var 25.18
42.99
60.44
75.98
88.10
100

20
A. Elouardighi et al.
The two ﬁrst factors have limited proportions basing on the total inertia:
with only 25% and 18% respectively (Table 5). These results seem to be pes-
simistic at ﬁrst sight if we consider the same interpretation as in CA, but MCA
often includes a high number of multinomial levels in the analysis what involve a
decreasing inertia proportions in all factors [28]. Thus, choosing a two dimension
factorial plan will be enough to represent the original factorial plan in 100%. If
we consider the absolute contribution that indicates how much inﬂuence a cat-
egory had in determining a certain factor [27], we ﬁnd that “arabic” forms the
ﬁrst factor by 23% alongside with “female” by 22%, the second factor is deﬁned
by “male” with 42% and “unspeciﬁed” with 16% (Table 6). Yet, relative contri-
bution permit to estimate the contribution made by a factor to the reproduction
of the dispersion of each active category, this value measured by square cosine
shows that the ﬁrst factor has a good representation of “neutral”, “female” and
“arabic”, respectively with 48%, 44% and 41%, while the second one deﬁne
“male” by 71%.
Table 6. Absolute and relative contributions of variable levels
Levels
Absolute contribution Relative contribution
Dim1 Dim2
Dim1 Dim2
Arabic
23.66
8.60
0.41
0.10
English
0.15
11.81
0.00
0.22
French
8.69
3.39
0.24
0.07
Female
22.43
7.72
0.44
0.10
Male
0.00
42.93
0.00
0.71
Unspeciﬁed 12.69
16.06
0.33
0.29
Negative
7.38
8.93
0.12
0.10
Neutral
0.12
9.88
0.48
0.00
Positive
15.11
0.75
0.31
0.01
Graphical representation of MCA reveals a high distance between categories
belonging the same variable (Fig. 8); this implies a signiﬁcant diﬀerence between
their distributions [29]. Although when we analyze levels belonging to diﬀerent
variables, we can distinguish three major groups.
The ﬁrst group, which is formed from unspeciﬁed-french-neutral, consists of
French-speakers who have expressed a neutral sentiment and who have uniden-
tiﬁed sex within their twitter proﬁles. This group represents media, reporters,
TV channels and other similar organizations that consider twitter as information
diﬀusion platform; so far, this explains why their tweet contents are often neu-
tral. The second formation who contains female-arabic-positive levels, concerns
female users who have posted Arabic tweets embed with positive sentiment,
from what we conclude that Arabic-speaking women have expressed positive
sentiment about Mawazine festival.

Collecting and Processing Multilingual Tweets
21
negative
neutral
positive
arabic
english
french
female
male
unspecified
−1
0
1
0
1
2
Dim1 (25.2%)
Dim2 (17.8%)
Fig. 8. Variable levels representation with MCA
By considering the ﬁrst axis as the reference, the last group who contains
male-english-negative categories is opposed to the ﬁrst group, this formation
explains that English-speaking men users have expressed negative sentiments
about Mawazine festival. In this work, we have studied the expressed sentiments
that people have shared in twitter about Mawazine festival, one of the most an
important event organized in Morocco. As result, we have conﬁrmed that the use
of a speciﬁc language in posting tweets is indeed related the expressed sentiment,
also have shown that sex of the user can have an inﬂuence on this sentiment as
well. Considering all this, we were able to identify diﬀerent sentimental groups
depending on those criterions; language and user’s sex.
4
Conclusion
This work was focused on SA using multilingual tweets. In the ﬁrst time we
have presented the hadoop based platform and the methodology used for real
time collecting tweets as well as the tools that we have used to transform them
into usable dataset. Secondly we have exposed the approach that we have used
to analysis these tweets. It is a part of unsupervised techniques basing on the
application of the factorial correspondence analysis (CA) and the multiple cor-
respondence analysis (MCA).
This platform and approaches were used for SA on multilingual tweets
(Arabic, French and English) collected during the Mawazine festival that took
place in Rabat between May 20 and 28, 2016. We have studied the expressed
sentiments that people have shared in twitter about this festival. We were able
to identify diﬀerent sentimental groups depending on some features speciﬁed.
For a future work we envisage exploiting this Platform for collecting and
automatic processing tweets written in Moroccan dialectal for SA. In our knowl-
edge no study was led using this type of tweets in massive data.

22
A. Elouardighi et al.
References
1. Nasukawa, T., Yi, J.: Sentiment analysis. In: Proceedings of the International Con-
ference on Knowledge Capture - K-CAP 2003 (2003)
2. Dave, K., Lawrence, S., Pennock, D.: Mining the peanut gallery. In: Proceedings
of the Twelfth International Conference on World Wide Web - WWW 2003 (2003)
3. Wang, X., Wei, F., Liu, X., Zhou, M., Zhang, M.: Topic sentiment analysis in
twitter. In: Proceedings of the 20th ACM International Conference on Information
and Knowledge Management - CIKM 2011 (2011)
4. Pang, B., Lee, L.: A sentimental education. In: Proceedings of the 42nd Annual
Meeting on Association for Computational Linguistics - ACL 2004 (2004)
5. Saif, H., He, Y., Alani, H.: Semantic sentiment analysis of twitter. In: The Semantic
Web ISWC 2012, pp. 508–524 (2012)
6. Kumar, A., Sebastian, T.: Sentiment analysis on twitter. IJCSI Int. J. Comput.
Sci. 9, 372–378 (2012)
7. Kiritchenko, S., Zhu, X., Saif, M.M.: Sentiment analysis of short informal texts. J.
Artif. Intell. Res. 50, 723–762 (2014)
8. Tang, J., Nobata, C., Dong, A., Chang, Y., Liu, H.: Propagation-based sentiment
analysis for microblogging data. In: Proceedings of the 2015 SIAM International
Conference on Data Mining, pp. 577–585 (2015)
9. Zhou, S., Chen, Q., Wang, X.: Fuzzy deep belief networks for semi-supervised
sentiment classiﬁcation. Neurocomputing 131, 312–322 (2014)
10. Farra, N.,Challita, E., Assi, R., et al.: Sentence-level and document-level sentiment
mining for arabic texts. In: 2010 IEEE International Conference on Data Mining
Workshops (ICDMW), pp. 1114–1119. IEEE (2010)
11. Abdul-Mageed, M., Diab, M., Korayem, M.: Subjectivity and sentiment analysis
of modern standard arabic. In: Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies: Short
Papers, vol. 2, pp. 587–591 (2011)
12. Refaee, E., Rieser, V.: An arabic twitter corpus for subjectivity and sentiment
analysis. In: LREC, pp. 2268–2273 (2014)
13. Ibrahim, S., Abdou, H.M., Gheith, M.: Sentiment analysis for modern standard
arabic and colloquial. Int. J. Nat. Lang. Comput. 4, 95–109 (2015)
14. Ingle, A., Kante, A., Samak, S., Kumari, A.: Sentiment analysis of twitter data
using hadoop. Int. J. Eng. Res. Gener. Sci. 3, 144–147 (2015)
15. Han, J., Kamber, M., Pei, J.: Data Mining. Elsevier, New York (2011)
16. Morstatter, F., Pfeﬀer, J., Liu, H., et al.: Is the sample good enough? com-
paring data from twitter’s streaming api with twitter’s ﬁrehose. arXiv preprint
arXiv:1306.5204 (2013)
17. Penchalaiah, C., Murali, G.: Eﬀective sentiment analysis on twitter data using:
apache ﬂume and hive. IJISET 1, 101–105 (2014)
18. Crockford, D.: The application/json media type for javascript object notation
(json) (2006). https://tools.ietf.org/html/rfc4627
19. Patel, A., Birla, M., Nair, U.: Addressing big data problem using Hadoop and
Map Reduce. In: 2012 Nirma University International Conference on Engineering
(NUiCONE) (2012)
20. Kotwal, A., Fulari, P., Jadhav, D., et al.: Improvement in sentiment analysis of
twitter data using hadoop. Imperial J. Interdisc. Res. 2, 440 (2016)
21. Assiri, A., Emam, A., Aldossari, H.: Arabic sentiment analysis: a survey. Int. J.
Adv. Comput. Sci. Appl. 6, 75–85 (2015)

Collecting and Processing Multilingual Tweets
23
22. Alhumoud, S., Altuwaijri, M., Albuhairi, T.: Survey on arabic sentiment analysis
in twitter. Int. Sci. Index 9, 364–368 (2015)
23. Mawazine Festival website. http://www.festivalmawazine.ma/shows
24. h24info website. www.h24info.ma/maroc/mawazine-cette-star-marocaine-battu-le-
record-daﬄuence/43215
25. Mellinger,
M.:
Correspondence
analysis:
the
method
and
its
application.
Chemometr. Intell. Lab. Syst. 2, 61–77 (1987)
26. Husson, F., Le, S., Pages, J.: Exploratory Multivariate Analysis by Example Using
R. CRC Press, Boca Raton (2011)
27. Di Franco, G.: Multiple correspondence analysis: one only or several techniques?
Qual. Quant. 50, 1299–1315 (2015)
28. Greenacre, M.: Interpreting multiple correspondence analysis. Appl. Stoch. Models
Data Anal. 7, 195–210 (1991)
29. Bendixen, M.: A practical guide to the use of correspondence analysis in marketing
research. Mark. Res. On-Line 1, 16–36 (1996)

Synonym Reduction in a Comparative Study of Clustering
Algorithms in Text Mining
Abdennour Jalil
(✉), Noureddine Aboutabit, and Imad Haﬁdi
Laboratoire IPOSI, Univ. Hassan 1, 26000 Settat, Morocco
jalil.abdennour@gmail.com, Noureddine.aboutabit@uhp.ac.ma,
imad.hafidi@gmail.com
Abstract. The growing use of smart phones, social networks and cloud
computing has added a large amount of data. Studies predict that just under 4 tril‐
lion gigabytes of data will exist on earth. This has created a problem related to
the processing of the data exchanged, which is rising exponentially and should
be automatically treated. This paper presents a classical process of knowledge
discovery databases for treating textual data. This process is divided into three
parts: pre-processing, processing and post-processing. In order to improve the
classical process, we propose to introduce synonyms reduction before the
processing step. This introduction, based on dictionaries, reduces the size of
documents.
Keywords: Synonym · Dimension reduction · Text mining · Clustering
1
Introduction
Over the last decade, the amount of data was intensely increased due to the use of
computers and others digital devices. So 90% of the world’s total data has been created
in the last 5 years and 70% of it by individuals. Studies predict that approximately 4 tril‐
lion gigabytes of data will exist on earth. This data overﬂows with an information quan‐
tity which cannot be used manually. It raises the question of document organization and
information extraction from text.
The process of seeking or extracting the useful information from textual data is
known as text mining. It is an exciting research area as it tries to discover knowledge
from unstructured texts, complex and over-dimensioned data. Introduced by Ronen and
Dagan as KDT [1], we ﬁnd as main branches of text mining: text extraction, summa‐
rizing, categorization, etc.
The text mining process contains a set of techniques generally grouped in three steps:
pre-processing, processing and post-processing. The aim of the ﬁrst step is to clean data
and reduce noise [2]. It consists of extracting features in meaningful and uniﬁed forms.
So many processes can be performed. Such processes are gathered in three stages:
removing empty words, lemmatization, and digital transformation. In the last stage, the
text data is converted into digital data as representations to classify them. There are many
models of digital transformation, the most commonly used are: the Boolean model, the
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_3

probabilistic model and the vector model. However, there are two problems associated
with features: the large scale of features, and the eﬃciency of a particular feature for
representing useful information in text data. So, for several applications the reduction
of feature dimension is carried out.
Techniques for dimension reduction include some which select a subset of the most
representative feature based on some criterion, and others which project data with high
dimension into lower dimension space [3–5]. There are many advantages of dimension
reduction. It can improve data representation because it eliminates other common words
not eliminated by the removing empty words stage. Also, by removing redundant and
irrelevant terms from corpus, it can provide better text classiﬁcation results and thus
improves eﬃciency and performance [6]. Sometimes dimension reduction may reduce
computing time and cost depending on the technique used and how the computing time
is calculated (if the execution time of the dimension reduction technique was added or
not to overall computing time).
In the processing step, one of the clustering algorithms is used to create the corpus
[7]. Alternatively documents will be distributed on several clusters. The elements of
each cluster have a common characteristic.
In post-processing, there are some works that use ontologies to give meaning to
clusters. They use labelling [8] and visualization of clustering results to present hier‐
archical relations between the resulting clusters and evaluation. They are generally
looking for the most frequent terms in each cluster to present them using semantic rela‐
tions of ontologies.
The aim of this paper is to present a comparative study on text clustering using 5
kinds of clustering algorithms K-means. We are looking for clustering quality results
with and without using the synonyms approach for dimension reduction. We use the DB
index, the square error and computing time in order to measure clustering quality.
In the next section a review of related work to clustering algorithms in the text mining
context with dimension reduction is presented. Section 3 describes the 5 clustering
algorithms and the reduction technique used. In Sect. 4 experiment results are shown
and the work is concluded.
2
Related Works
To improve the quality and time execution of clustering algorithms, much research has
been done to improve the ﬁnal solution. These improvements were made on three fronts:
The ﬁrst axis is based on the choice of the initial solution:
There is no standard method to determine the initial centers solution of unsupervised
clustering algorithms. The clustering ﬁnal solution, depends on the speciﬁed choice of
starting point values. However, the test of all possible combinations is considered
impossible. Therefore diﬀerent methods have been proposed in literature. Global k-
means method [9] provides a way of determining good initial cluster centers for the k-
means algorithm without having to use random initialization. In 2009, Nazeer et al.
proposed [10] an enhanced k-means, which combines a systematic method for ﬁnding
initial centroids and an eﬃcient way of assigning data points to clusters. Similarly,
Synonym Reduction in a Comparative Study of Clustering
25

Xu et al. (2009) specify a novel initialization scheme [11] to select initial cluster centers
based on reverse nearest neighbor search. But all the above methods do not work well
for high dimensional data sets.
The second axis is based on the choice of the calculation Method Distance:
Fahim A.M. et al. [12] proposed an eﬃcient method for assigning data points to
clusters. The original k-means algorithm is computationally very expensive because
each iteration computes the distances between data points and all the centroids. Fahim’s
approach makes use of two distance functions for this purpose- one similar to k-means
algorithm and another one based on a heuristics to reduce the number of distance calcu‐
lations. But this method presumes that the initial centroids are determined randomly, as
in the case of the original k-means algorithm. Hence there is no guarantee for the accu‐
racy of the ﬁnal clusters.
The third axis is based on changing of dimensions data:
In 2001, Christos Bouras and Vassilis Tsogkas [13] used external knowledge from
WordNet which is a large lexical multilingual database. They are proposing the enhance‐
ment of standard KMeans algorithm using this knowledge by introducing Synonyms
and antonyms in a twofold manner: enriching the documents used prior to the clustering
process and assisting the label generation procedure following it.
Initially, for each given keyword of the article, they generate its graphs of hypernyms
leading to the root hypernym. Following, they combine each individual hypernym graph
to an aggregated one. There are practically two parameters that need to be taken into
consideration for each hypernym of the aggregate tree-like structure in order to deter‐
mine its importance: the depth and the frequency of appearance.
Their experimentation revealed a signiﬁcant improvement over standard KMeans
for a corpus of news articles derived from major news portals. Moreover, the cluster
labeling process generates useful and of high quality cluster tags. Even This method
improves the quality of the solution but takes too long to be executed.
However Petridis [14] used hyper words in 2001 to reduce content in a connectionist
models of directed graphs, he demonstrated how the employment of hyper words implies
a reduction in the number of features to be used for document classiﬁcation, based on
the a priori knowledge of semantics contained in the thesaurus.
In 2004, Chris Ding presented a solution of clustering by introducing PCA (principal
component analysis) [15]. He applied PCA before a clustering algorithm (such as k-
means). He proved that principal components are the continuous solutions to the discrete
cluster membership indicators for k-means clustering. Equivalently, he shown that the
subspace spanned by the cluster centroids are given by spectral expansion of the data
covariance matrix truncated at K − 1 terms. These results indicate that unsupervised
dimension reduction is closely related to unsupervised learning. On dimension reduc‐
tion, the result provides new insights to the observed eﬀectiveness of PCA-based data
reductions, beyond the conventional noise-reduction explanation.
26
A. Jalil et al.

3
Method
It is generally based on the automatic method to analyze and process the whole text.
Among these methods there is the process shown in [2], which is spread over three main
stages as shown in the image below (Fig. 1):
Fig. 1. Classical text mining process
We propose to add a new step before Vectorization, in this one we will use synonyms
to reduce data dimensions by replacing each word by a unique synonym. The proposed
Algorithm is shown below:
Synonym Reduction in a Comparative Study of Clustering
27

After the synonyms step, we use the vector Model to convert the text document to a
numerical one. This conversion is based on TF-IDF formula:
TF −IDFij = TFi,j × IDFij
where
TFi,j =
nij
∑
k nik
and
IDFij = log (|D|)
/(|||dj:ti ∈dj|||
)
where nij is the occurrence of the word ni in the document j. ∑
k nik is the total number
of words in the document |D|: total number of documents in the corpus and |||dj:ti ∈dj|||:
Number of documents where the term ti appears.
In the second stage, we use Clustering Algorithms. Clustering approaches can be
divided into two categories according to their input parameters:
Algorithms that take explicitly the number of input clusters K.
Algorithms that take a threshold τ as an input, Used to determine indirectly the
number of clusters.
KMeans is the most known algorithm in the ﬁrst category. The second category
includes Two-Level-KMeans for example.
All partitioning algorithms use the following concepts:
We note:
group of data: X = {X1, X2, … , Xn}
28
A. Jalil et al.

partitions: S = {S1, S2, … , Sk}k ≤n
where K is the number of desired clusters, n is the cardinality of data.
It aims to minimize the distance between points belonging to each cluster (score).
Args min S
K
∑
i=1
∑
Xj∈Si
‖Xj −𝜇i‖2
With μi is the center of the cluster i. Xj is an element of the set.
Alternatively, we try to ﬁnd clusters with the minimum of inter-cluster distance and
the maximum of intra-class distance.
3.1
Algorithms KMeans
Based on the previous concept, ﬁve partitioning algorithms are presented in the
following:
KMeans
The classic and most widely used algorithm is KMeans [16]. It takes two input param‐
eters: the number of clusters K and a set of data.
This algorithm initializes arbitrarily the center of these clusters. Before running in
several iterations, KMeans aﬀected data to the nearest cluster, and the centers are recal‐
culated at the end of each iteration. The algorithm stops once there are no more new
assignments.
Global KMeans
The majority of proposed solutions previously cannot ensure the convergence to a global
optimum, Global KMeans [17] proposed a new version, which overcomes this problem.
The main idea of the algorithm is to start with a single cluster that contains all the data
set. Then each iteration creating a new cluster with the center that minimizes the squared
error. The algorithm stops once reached the number of clusters speciﬁed by the user.
Fast Global KMeans
This algorithm is proposed by Jim Z.C. et al. [18], to improve the Global KMeans. The
fast global k-means algorithm constitutes a straightforward method to accelerate the
global k-means algorithm. Suppose we are in k − 1 iteration, the new center (xn) will
allocate all points xj whose squared distance from xn is smaller than the distance dj
k−1
from their previously closest center. dj
k−1 is the squared distance between xj and the
closest center among the k − 1 cluster centers Therefore, for each such data point xj the
clustering error will decrease by dj
k−1 −|||xn −xj|||
2
.
Two-Level-KMeans
Taking a threshold τ as an input parameter, the Two-level-KMeans [19] is executed in
two steps. The implementation of the classic KMeans is made ﬁrst. After that, clusters
Synonym Reduction in a Comparative Study of Clustering
29

which do not verify the threshold condition, are selected for subdivision into (ri\τ) * n
subclasses in which ri is the radius of the cluster and n is the data cardinality.
FW-KMeans
FW-KMeans applies on the vector space model (VSM). Unlike KMeans that treats all
elements of the set fairly, the FWK-Means uses the notion of weight to highlight the
most common elements. In addition, FW-Means introduced a constant (very low value)
to avoid the noise problem.
F(W, Z, 𝛬) =
k∑
l=1
n
∑
k=1
m
∑
i=1
wl,j𝜆𝛽l,i
[d(zl,i, xj,i
) + 𝜎]
where σ is a constant, k (≤n) is the number of clusters; β (>1) is an exponent;
W = [wl,j] 
is 
a 
k 
× 
n 
integer 
matrix; 
Z = [Z1, Z2, … , Zk] ∈RK×m
Z = [Z1, Z2, … , Zk
] ∈Rk×m are the k cluster centers; Λ = (Λ1, Λ2, … , Λk
) is the set of
weight vectors for all clusters in which each Λl = (𝜆l,1, 𝜆l,2, … , 𝜆l,m
) is a vector of
weights for m features of the lth cluster; d(Zl,i, Xj,i
) ≥0 is a distance or dissimilarity
measure between the jth document and the center of the lth cluster on the ith feature. In
text clustering, they use the Euclidean distance because the frequencies of terms are
numeric values.
The proposal [20] FW KMeans has the aim of reducing the noise. This reduction is
made by a weighting method to decrease the inﬂuence of noise, by the concentration of
comparisons on the main axes.
4
Results and Discussion
Our used data is retrieved from several info websites (le monde, 01net, JDN etc.) Usually
the documents do not have the same size (number of words), or the same natural category
(political subject, computer etc.). In total 260 documents were retrieved.
Before performing the clustering step, these documents were processed through the
diﬀerent pre-processing stages. At ﬁrst, we recover items from RSS feed in XML format.
Then, we transform the whole to ﬂat ﬁles. After removing empty words, we proceed to
the lemmatization stage: for each word we look for its root using the Tree Tagger Tool
[9]. After that, the synonym approach may be applied to reduce the number of words.
The last part of the pre-processing is reserved to convert results into vectors using
TF-IDF formula.
The data used are processed in a machine 2.5 GHz CPU (Core i5) and 8 GB of Ram,
running on Windows 7.
We launch all algorithms 5 times (except Global K Means) and we take the average
value of each index used as criterion. For the Two-Level-KMeans algorithm, we ﬁx
threshold as the average value of all vectors. Centers initialization is made arbitrarily
for all algorithms. We set also the β parameter of FW-KMeans at 1.5.
Figures 2 and 3 display the obtained values of performance criterions (DB index and
square error) after performing clustering using the 5 algorithms KMeans, Global K
30
A. Jalil et al.

Means, Fast Global KMeans, Two-level KMeans and FW-KMeans. Figures 4 and 5
show results for clustering using the same algorithms after applying synonym reduction
process previously presented. Also, the execution time is reported in Table 1 for all
execution cases.
0
1
2
3
4
5
6
DB index 
10 Clusters
20 Clusters
100 Clusters
Fig. 2. DB index results for clustering without synonyms reduction
0
0.5
1
1.5
2
2.5
Square error 
10 Clusters
20 Clusters
100 Clusters
Fig. 3. Square error results for clustering without synonyms reduction
Synonym Reduction in a Comparative Study of Clustering
31

0
1
2
3
4
DB index 
10 Clusters
20 Clusters
100 Clusters
Fig. 4. DB index results for clustering using synonyms reduction
0
0.5
1
1.5
2
Square error 
10 Clusters
20 Clusters
100 Clusters
Fig. 5. Square error for clustering using synonyms reduction
First of all, by comparing results of all tested algorithms, it, it is shown that on the
one hand, the Global K Means and FW-KMeans presents the best results. But the second
algorithm takes into account the concept of weighing in its partitioning which increases
the quality of the results. On the other hand, the Two-level KMeans is still ineﬀective
in a KDT context. In addition, the choice of the threshold value is still a problem.
Secondly, graphs show that using the synonyms reduction approach improves
performance of clustering in most cases. This improvement is particularly signiﬁcant
for a reduced number of clusters. Indeed, by the using of synonyms reduction, the DB
index and the square error has decreased by over 20% for KMeans, Global K Means,
Fast Global KMeans, Two-level KMeans (except for the square error in case of Fast
Global KMeans) and over 13% for FW-KMeans in the case of 10 clusters.
32
A. Jalil et al.

Table 1. Execution time for clustering
Algorithm
Clusters number
Execution time (ms)
without synonyms
Execution time (ms)
with synonyms
KMeans
10
725
692
20
1675
1641
100
1800
1767
Global KMeans
10
25964,0513 (S)
23354,29369 (S)
20
50546,0388 (S)
46285,10364 (S)
Fast GKMeans
10
4756
4561
20
4988
4832
100
31,409
32,733
Two-Level-KMeans
10
561
524
20
803
789
100
539
602
FW-KMeans
10
41,837
40,523
20
27,392,63
2,719,973
100
30,495,732
29,787,364
Furthermore, it is normal that we achieve in most cases a decrease in the clustering
execution time. Yet, this reduction isn’t so signiﬁcant to be considered interesting. Note
also, the execution time was calculated without taking into account the execution syno‐
nyms part which is about 7645 s. This consequent time is due to the use of the web
dictionary and the process needed to connect for every word.
This paper presents an implementation of a text mining process using 5 clustering
algorithms (KMeans, Global K Means, Fast Global KMeans, Two-level KMeans and
FW-KMeans). The process data was extracted from the web, and then a pre-processing
was applied to the data. In this pre-processing, we introduce a reduction dimension based
on the synonym approach. The evaluation of the classiﬁcation results is performed with
Square error and DB index.
The results of this study conﬁrm ﬁrstly related studies on the literature that KMeans
algorithm was fast and the Two-level KMeans has less quality. Secondly, introducing
dimension reduction phase in pre-processing may improve clustering quality for the
most of the tested algorithms. This improvement is about 20% of used criterions.
On the other hand, we noted a modest improvement of clustering execution time
which does not count the synonyms algorithm time which is consequent.
In future work, we will try to reduce execution time by the parallelization of algo‐
rithms and the integration of ontologies. This may also increase the clustering quality.
References
1. Feldman, R., Dagan, I: Knowledge discovery in textual databases (KDT) (2005)
2. Ibekwe-Sanjuan, F., Sanjuan, E.: Ingénierie linguistique et Fouille de Textes (2007)
3. Zhu, M., Zhu, J., Chen, W.: Eﬀect analysis of dimension reduction on support vector machines.
In: Proceeding of NLP-KE 2005 (2005)
Synonym Reduction in a Comparative Study of Clustering
33

4. Zifeng, C., Baowen, X., Weifeng, Z., Dawei, J., Junling, X.: CLDA: feature selection for text
categorization based on constrained LDA. In: International Conference on Semantic
Computing (2007)
5. Yan, J., Liu, N., Yang, Q., Fan, W., Chen, Z.: TOFA: trace oriented feature analysis in text
categorization. In: Eighth IEEE International Conference on Data Mining (2008)
6. Mugunthadevi, K., Punitha, S.C., Punithavalli, M., Mugunthadevi, K.: Survey on feature
selection in document clustering. Int. J. Comput. Sci. Eng. (IJCSE) 3, 1240–1241 (2011)
7. Khan, K., Sahai, A.: A fuzzy c-means bi-sonar-based metaheuristic optimization algorithm.
Int. J. Interact. Multimedia Artif. Intell. 1(7), 26–32 (2012)
8. Jing, L., Ng, M.K., Yang, X., Huang, J.Z.: A text clustering system based on k-means type
subspace clustering and ontology. World Acad. Sci. Eng. Technol. 2, 91–103 (2008)
9. Likas, A., Vlassis, N., Verbeek, J.J.: The global K-Means clustering algorithm. Pattern
Recogn. 36(2), 451–461 (2003)
10. Abdul Nazeer, K.A., Sebastian, M.P.: Improving the accuracy and eﬃciency of the kmeans
clustering algorithm. In: Proceedings of the World Congress on Engineering, vol. 1, pp. 308–
312 (2009)
11. Junling, X., Baowen, X., Weifeng, Z., Zhang, W., Hou, J.: Stable initialization scheme for k-
means clustering. Wuhan Univ. J. Natl. Sci. 14(1), 24–28 (2009)
12. Fahim, A.M., Salem, A.M., Torkey, A., Ramadan, M.A.: An eﬃcient enhanced k-means
clustering algorithm. J. Zhejiang Univ. 10(7), 1626–1633 (2006)
13. Bouras, C., Tsogkas, V.: Clustering user preferences using W-kmeans in structured data
domains using fuzzy lattice neurocomputing (FLN). IEEE Trans. Knowl. Data Eng. 13(2),
53–60 (2001)
14. Petridis, V., Kaburlasos, V.G.: Clustering and classiﬁcation in structured data domains using
fuzzy lattice neurocomputing (FLN). IEEE Trans. Knowl. Data Eng. 13(2), 245–260 (2001)
15. Bollen, J., Van de Sompel, H., Hagberg, A., Chute, R.: A principal component analysis of 39
scientiﬁc impact measures. PLoS ONE 10, 1371 (2009)
16. MacQueen, J.B.: Some methods for classiﬁcation and analysis of multivariate observations
(1967)
17. Likas, A., Vlassis, N., Verbeek, J.J.: The global K-Means clustering algorithm (2003)
18. Laia, J.Z.C., Huanga, T.-J.: Fast global KMeans clustering using cluster membership and
inequality. Pattern Recogn. (2009). doi:10.1016/j.patcog.2009.11.021
19. Chitta, R., Narasimha Murty, M.: J. Pattern Recogn. Arch. 43(3), 796–804 (2010)
20. Jing, L., Ng, M.K., Yang, X., Huang, J.Z.: A text clustering system based on k-means type
subspace clustering and ontology. World Acad. Sci. Eng. Technol. 2, 04–26 (2008)
34
A. Jalil et al.

Using Statistical and Semantic Analysis
for Arabic Text Summarization
Nabil Alami
(✉), Yassine El Adlouni,
Noureddine En-nahnahi, and Mohammed Meknassi
Laboratory of Informatics and Modelling, Faculty of Science Dhar EL Mahraz,
University Sidi Mohamed Ben Abdellah (USMBA), Fez, Morocco
nab.alami@gmail.com, yeladlouni@gmail.com,
nahnnourd@yahoo.fr, m.meknassi@gmail.com
Abstract. Automatic text summarization is an essential tool to overcome the
problem of information overload. So far this ﬁeld has not been studied enough
for Arabic language and currently only few related works are available. Arabic
text summarization is faced with two main issues: how to extract semantic rela‐
tionships between textual units and deal with redundancy. To overcome these
problems, we propose in this paper a hybrid method to generate an extractive
summary of Arabic documents. Our approach is based on a two-dimensional
undirected and weighted graph with sentences as nodes and each pair of sentences
are connected by two edges representing the statistical and semantic similarity
measure. The statistical similarity measure builds on the content overlap between
two sentences, while the semantic one is based upon semantic information
extracted from Arabic WordNet (AWN) ontology. Then, the score of each
sentence is computed by performing the ranking algorithm PageRank on the
generated graph. Thereafter, the score of each sentence is performed by adding
other statistical features of the text such as TF.ISF and sentence position. The
ﬁnal summary is built by selecting the top-ranking sentences. Finally, we deal
with redundancy and information diversity issues by using an adapted maximal
marginal relevance (MMR) method. Experimental results on EASC dataset show
that our proposed approach outperforms some of existing Arabic summarization
systems.
Keywords: Arabic text summarization · Arabic NLP · Statistic approach ·
Sematic approach · AWN · Graph model
1
Introduction
Automatic Text summarization is the task of automatically generating a condensed
version of an input text that provides useful information for the user with preserving the
main ideas in the original document. Text summarization is one of the most challenging
tasks in natural language processing area (NLP). Hovy [1] deﬁned a summary as a text
that is produced from one or more texts which contains a signiﬁcant portion of the
information in the original text(s), and that is no longer than half of the original text(s).
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_4

As a text document, automatic summarization can be applied to any other media such
as speech, multimedia documents, hypertext or even video.
Because of the rapid growth in online information, the huge amount of information
available electronically becomes unmanageable, and users are unable to read and extract
useful information from them. That is why the automatic summarization tool has become
an essential task that allows users to quickly ﬁnd and extract the data that they need to
quickly make mission-critical business decisions. The ﬁrst work on automatic text
summarization was proposed by Luhn [2] in the late ﬁfties and to date; automatic
summarization is an active ﬁeld in natural language processing area with a challenging
issue.
Unlike English, which many outstanding achievements have been made in the ﬁeld
of automatic summarization, few and no excellent systems have been developed for
automatic summarization of Arabic text. This particular ﬁeld in Arabic natural language
processing is not studied enough compared to the large number of studies carried out
for English. Therefore, there is a considerable opportunity for further research in auto‐
matic summarization for Arabic documents specially when the existing systems are note
mature enough and eﬃcient as we need.
General speaking, Text summarization can be divided into two major classes:
abstractive and extractive. In the abstractive summarization, the system has to re-
generate either the extracted content or the text. It requires human knowledge and heavy
machinery for language generation; however, in extractive class, the sentences have to
be ranked based on the most salient information. The summary is built by the most
ranked sentences arranged with the same order as presented in the original text. This is
equally known as sentence ranking, for which the importance of each sentence is
computed according to linguistic and statistical features.
It is worth noting that Arabic is the language spoken by more than 22 countries and
the ﬁfth most spoken language in the word. Recent studies indicate that Arabic language
is fastest-growing language on the web in terms of the number of internet users. It is
ranked the fourth language used in the web after English, Chinese and Spanish. However,
research on Arabic NLP is still in its infancy since researchers become less interested
in devoting the time and eﬀort to Arabic than to other languages. Therefore, developing
ANLP systems has become paramount for accessing a large number of Arabic docu‐
ments available in the Internet. Automatic summarization is one of these systems.
Traditional summarization systems rely on the Bag of Words (BOW) approach. The
BOW approach is based on the words existing in the text to be summarized. One of the
obvious disadvantages of this approach is that it cannot accurately represent the meaning
of documents, because it ignores the semantic relationship existing between diﬀerent
textual units. The system is always limited to the words explicitly mentioned within the
input text document. For example, if the system is not able to ﬁnd the relationships
between terms like « ( » ﺑــــــﱰولPetroleum) and « ( » ﻧﻔﻂOil), it would handle these words
separately as two diﬀerent unrelated terms, and this may aﬀect negatively their impor‐
tance in the input document.
36
N. Alami et al.

In this paper, we develop an Arabic text summarization system that combines stat‐
istical and semantic approaches to achieve the summarization task. In the ﬁrst step, the
proposed system proceeds to document preprocessing: sentence splitting, tokenization,
stop words removal and root extraction. Second, it computes the similarity between each
pair of sentences. For this, two kinds of similarity are used: (i) statistical similarity based
on the content overlap between two sentences, this measure is used in TextRank [29];
(ii) semantic similarity measure based on the semantic information extracted from AWN
for each pair of words of the given sentences using the ﬁrst concept as a disambiguation
strategy. Based on that, the proposed algorithm converts the text into a graph model with
sentences as nodes and each pair of sentences is connected by two edges representing
two types of relations: statistic similarity; and semantic similarity calculated in the
previous step. Thereafter, PageRank [28], as a ranking algorithm, is performed on a
generated graph in order to compute the score of each sentence. Then, the score of each
sentence is improved by adding other statistical features such as TF.ISF and sentence
position. The summary is formed by including the top-ranked sentences from the input
document, and an adapted version of maximal marginal relevance (MMR) algorithm
[35] is applied in order to remove information that is redundant and enhance the quality
of the result summary.
The proposed summarization approach has several advantages. Firstly, the system
is domain-independent and does not need any domain-speciﬁc knowledge or features.
Secondly, this method does not require any training data or annotated corpus. Thirdly,
and unlike the existing methods, we consider the semantic relationships among words
and we introduce semantic information from Arabic WordNet (AWN) ontologies in
order to accurately represent the meaning of documents. Finally, redundancy is a well-
known issue in text summarization. Therefore, we deal with redundancy and information
diversity issues by using the MMR method.
This paper is presented as follows. Section 2 summarizes existing works on auto‐
matic summarization, especially in Arabic and discusses the limitation of the current
approaches. Section 3 describes in detail the proposed method. System evaluation and
the experimental results are discussed in Sect. 4. A conclusion and future works are
given in Sect. 5.
2
Related Works
Automatic text summarization can be classiﬁed into three approaches: numerical,
symbolic and hybrid approaches. The ﬁrst research in text summarization was published
more than 50 years ago [2, 3]. Luhn [2] used word frequency to determine the importance
of each sentence in a single document. The method proposed by Edmundson [3] used
more than one feature to score sentences, word frequencies, sentence positions and cue
words. These features are still used by many automatic summarization systems devel‐
oped in recent years [4, 9].
Statistical and Semantic Analysis
37

Compared to English and other languages (Spanish, Chinese, French), a very little
works has been made in Arabic document summarization ﬁeld. According to our knowl‐
edge, the ﬁrst system designed for Arabic text summarization was developed by
Douzidia and Lapalme [4]. It uses a linear combination of many statistical features
(tf, position of sentence, etc.).
AlSanie [5] developed a symbolic approach for Arabic based on Rhetorical Structure
Theory (RST) where 11 relations were used in the summarization process. The system
ﬁrst generated all possible Rhetorical Structure Tree (RS-Tree) for the text based on
relationships between textual units and then, the system generated the summary by
selecting the best tree. The author used a set of documents with their summaries gener‐
ated manually in order to evaluate the system performances. The system performed well
the summarization task for small and medium sized documents.
Haboush et al. [6] proposed a single document summarization model based on the
clustering technique. The words can be grouped into diﬀerent clusters based on roots
extracted from the document. The weight of the root is used, instead of the weight of
the word itself, in order to compute the score of each sentence. The score is obtained by
combining the weights of its words; the weight of words is computed according to its
frequency and importance in the paragraph. The most ranked sentences are then selected
to generate the summary.
El-Haj et al. [7] developed a multi-document Arabic summarization system based
on a clustering approach. The authors investigated two kind of experiments. In the ﬁrst
experiment, the authors cluster sentences based on the k-mean clustering technique using
the cosine similarity measure. Then, the summary is generated by selecting sentences
using two selection methods. In the ﬁrst method, sentences are selected from the largest
cluster, while in the second method, the ﬁrst sentence is selected from each cluster in
order to eliminate redundancy within the produced summary. In the second experiment
the diﬀerence is that the sentences are selected before applying the clustering. Thus, the
ﬁrst sentence is selected and then the most similar sentence to the ﬁrst one. The authors
evaluated and compared their system with other English summarizes using the English
and Arabic version of DUC 2002 corpus. The Arabic version of the DUC 2002 datasets
was generated using Google Translate.
Ibrahim and Elghazaly [8] followed a hybrid approach that uses two summarization
techniques: Rhetorical Representation based on RST and Vector Representation based
on Vector Space Model (VSM). The ﬁrst technique used a rhetorical representation of
the text using RST for generating the RS-Tree and extracting the most meaningful para‐
graphs to be included in the summary. The second method uses a cosine similarity metric
to build a vector representation based on VSM. As shown in the evaluation results, the
Rhetorical Representation technique produces better average in precision measure and
better quality in the produced summaries than Vector Representation; however, the
performance of the second was better with long articles.
38
N. Alami et al.

Recently, a number of semantic-based approaches have been developed. WordNet
[11], as a linguistic resources designed for English, is widely used in diﬀerent NLP
systems, and with its semantic relations of terms, it has been extensively used to improve
the quality of automatic text summarization [12–15], text clustering [16–18], word sense
disambiguation [19, 20] and other natural language processing tasks [21–23].
3
Our Arabic Text Summarization System
This paper introduces a new approach for automatic summarization of Arabic documents
using statistical methods and semantic treatment to increase information diversity of the
input Arabic document. The various steps followed by the proposed system are shown
in Fig. 1. The input is a large Arabic document and the output is a small one which
contains the most important sentences. The following section explains in detail each
step used in the proposed system.
Input document
Analysis Module
Sentences Ranking
Graph Construction
Similarity Measure
Statistic & Semantic
Arabic 
WordNet
Features Extraction
Post-processing Module
Tokenization
Stop-word Removal
Root Extraction
Preprocessing Module
Redundancy 
Elimination
Summary Generation
Summary
Fig. 1. The main process of our approach
3.1
Preprocessing
Step 1: Tokenization. Tokenization is an important preprocessing step to enable NLP
tools which require clean and standardized data as input. First, the document is normal‐
ized in order to convert a text to a standard format (removing punctuation, non-letters
and diacritics, replacing أ ,  آ
 ,and ٕ اwith  اand ﬁnal  ىwith  يand ﬁnal  ةwith  .)ﻩThen, the
document is segmented into words (tokens) and sentences. In our system, and depending
on the datasets used, the white space character (“ ”) deﬁnes the boundaries of words and
the dot character (“.”) is considered as a sentence separator.
Step 2: Stop words removal. The most frequent common words which the main func‐
tion is structural and give a little value to the meaning of the text are removed from the
text vocabulary. These words are called stop words. They are ﬁltered out before the pre-
processing step in any NLP task because they serve only a syntactic function, and do
Statistical and Semantic Analysis
39

not indicate subject matter. The performances of automatic summarization systems are
signiﬁcantly reduced when using stop-words. In Arabic, words like (
) are
often used in sentences, and have a little meaning in the implication of a document.
There is no standard list of stop words used by all Arabic NLP tools. In this work we
used a list of 168 words extracted from Khoja [24] to remove stop words.
Step 3: Root extraction. In Arabic, words that share a related meaning are generally
derived from a same root. As shown in Table 1, all the three Arabic words “
”,
“
” and “
” are related to the same root “.”ﻛﺘﺐ
Table 1. Diﬀerent derivation of root “”ﻛﺘﺐ
Arabic word
English sense Root
ﻛﺘﺐWriteﻛﺘﺐ
ﻛﺘﺎبBookﻛﺘﺐ
ﻣﻜﺘﺒﺔLibraryﻛﺘﺐ
Word stemming is one the most challenging issues in Arabic. Several researches
have been made to develop powerful stemming tools. Khoja’s stemmer presented by
[24] is one of the most popular one. It extracts the root using pattern matching after
removing predeﬁned suﬃxes, inﬁxes, and preﬁxes. In this work, we have included the
Khoja’s stemmer to perform the stemming task.
3.2
Analysis
After preprocessing the input Arabic document, the analyzing stage begins scoring the
sentences based on the computed set of features. Each sentence is given two kinds of
scores, implying its signiﬁcance in relation with the rest of the sentences: statistical score
and semantic score. Statistical score is the result of summing all weights given to each
extracted feature. The semantic score is determined by the importance of the sentence
against other sentences using the semantic relationship between them.
Step 4: Features extraction. We used two features to compute the statistical score of
each sentence: TF-ISF and sentence position.
Term frequency/Inverse sentence frequency (TF-ISF). Tf-IDF stands for term
frequency-inverse document frequency. It is composed by two terms: the ﬁrst (TF)
represents the number of time a term appears in a document. The second term is the
Inverse Document Frequency (IDF), which measures the importance of a word in a
document collection. It is calculated by the logarithm of the number of the documents
in the corpus divided by the number of documents where the speciﬁc term appears. TF-
IDF is widely used as a weighting factor in many information retrieval and text mining
applications. In this paper, we have adopted the Term Frequency-Inverse Sentence
Frequency (TF-ISF) measure which is the same of TF-IDF by replacing a set of docu‐
ments by a set of sentences. The inverse sentence frequency (ISF) measures the impor‐
tance of a term within the sentence collection [31].
40
N. Alami et al.

ISFwi = log2
N
dfwi
(1)
Where N is the total number of sentences in the document and dfwi is the number of
sentences where the term wi appears.
Sentence position. Usually, the most important sentences in a document are in a speciﬁc
position. The sentence position feature is another statistical feature which can help in
extracting salient information from the original text. In this paper, the position of
sentence is considered as a scoring factor in the summarization task. We consider the
ﬁrst and the last sentences are most related to the topic so their weight is high.
Step 5: Semantic similarity measure. Semantic similarity is becoming more and more
popular, and plays a signiﬁcant role in many NLP tasks such as text mining, information
retrieval and extraction, text summarization, text categorization, text clustering and so
on. The lack of common terms in two sentences (Or two documents) does not necessarily
mean that the sentences are not related. Therefore, summarization by using only classical
methods will fail to retrieve sentences (Or documents) with semantically similar terms.
This is exactly the problem this work is addressing.
Several semantic similarities have been proposed to quantify the semantic similarity
based on ontology hierarchy. Some utilize the taxonomies within WordNet [32] and the
relations deﬁned between its units. WordNet is the hierarchically-structured repository
that was created by linguistic experts and is rich in its explicitly deﬁned lexical relations.
This kind of measures based on WordNet, has been widely used in NLP applications
[25]. In this work, we used Arabic WordNet (AWN) which is a lexical resource for
standard modern Arabic based on Princeton WordNet, and is built according to methods
developed for EuroWordNet.
In this work, we have adopted the concepts based representation model to compute
the semantic similarity between two terms. In this model, we replace each term by its
associated concepts in AWN ontology, and we apply a disambiguation strategy in order
to assign one concept to one term. In our approach, we have adopted the “First concept”
strategy as a simple disambiguation method. This disambiguation strategy consists in
taking only the ﬁrst concept of the list as the most suitable concept [33]. Figure 2 shows
the ﬂowchart of the semantic similarity measure process between two Arabic words wi
and wj. Moreover, we have used the Wu and Palmer [26] measure to compute the
semantic similarity between any two concepts. This measure was found to be simple to
calculate, and presents more performances while remaining competitive and expressive
as others similarity measures [27].
Based on this measure, the semantic similarity between two sentences is computed
using the formula proposed by [34]. This measure is based on the sum of maximum
word similarity scores of words normalized by the sum of the sentence length. The
sentence semantic similarity formulation is deﬁned in (2).
Statistical and Semantic Analysis
41

Sim(Si, Sj
) =
∑
w∈Si maxSimw(w, Sj) + ∑
w∈Sj maxSimw(w, Si)
||Si|| + |||Sj|||
(2)
In this equation:
• Si and Sj are the given sentences.
• maxSimw(w, Sj) is the maximum similarity scores of the word w and the words in Sj.
• ||Si|| represent the length of the sentence Si.
Fig. 2. Semantic similarity measure between two Arabic words wi and wj
Step 6: Graph construction. In this step we transform an Arabic text document into
graph format. To draw the graph, we need to identify text units that best deﬁne the task
of automatic summarization and consider them as vertices of the graph. Then, we need
to identify relations that connect those units.
In this work, we consider the sentences of the input Arabic document as a text unit
and the similarity between those sentences as a relation between sentences. We build an
undirected weighted graph G = (V, E). In this graph, sentences are represented by a set
of vertices V and the relation between each sentence is represented by the edge that
connects the two correspondent vertices. Two diﬀerent kinds of edges are used:
Semantic similarity and statistical similarity.
42
N. Alami et al.

Semantic similarity: An edge is drawn between sentences that are similar to each other.
The edge represents the semantic similarity between the sentences and edge weight
represents the power of the relationship between the sentences in the text. In this work,
we used the semantic similarity calculated in the previous section.
Statistical similarity: Another edge is considered that represents the content overlap
between two sentences. The weight of the edge represents the number of common tokens
between the two sentences divided by the length of each sentence. Formally, the statis‐
tical similarity between two sentences Si and Sj is deﬁned as [29]:
Similarity
(
Si, Sj
)
=
|||
{wk|wk ∈Si ∩Sj
}|||
log
(||Si||
)
+ log(|||Sj|||)
(3)
Where ||Si|| represents the length of the sentence Si.
Step 7: Sentence ranker. The input of this process is the undirected weighted graph
resulted from the previous step. PageRank algorithm [28] was used to compute a salient
score for each vertex of the graph. PageRank is one of the most popular link analysis
algorithms and was proposed as a method for Web link analysis. The information
existing in the graph structure reﬂects the importance of a vertex against each other. In
our case, the key intuition is that a sentence should be highly ranked if it is recommended
by many other highly ranked sentences. PageRank is well adapted to undirected graph.
In this way, the input edges and the output edges for a vertex are the same. In this work,
since the graph is undirected In(Vi) is equal to Out(Vi).
Equation (4) computes the score of a vertex Vi, where adj(Vi) is the set of vertices
adjacent to Vi, wij is the weight of the edge between vertex Vi and vertex Vj, and d is a
damping factor that can be set between 0 and 1, which has the role of integrating into
the model the probability of jumping from a given vertex to another random vertex in
the graph. The factor d is usually set to 0.85 [29].
PR
(
Vi
)
= (1 −d) + d ∗
∑
Vj∈adj(Vi) wij
PR(Vj)
∑
Vk∈adj(Vj) wjk
(4)
We apply (4) iteratively on a weighed graph G to calculate PR. First, all nodes in the
generated graph are initialized by a score of 1. Then Eq. (4) is applied until the diﬀerence
in scores between iterations becomes less than a predeﬁned threshold for all nodes. in
our experiments we chose the value of the threshold equal to 0.001. The score of each
sentence is given by the weight of its associated vertex. The importance of a sentence
that is salient to the text and has a strong relationship with others sentences is deﬁned
by the weight degree of its vertex.
Statistical and Semantic Analysis
43

It is to be noted that (4) is applied on both statistical and semantic edges. We obtain
two scores for each vertex PRstatic
(
Vi
) and PRsemantic
(
Vi
). The ﬁnal score of each vertex
in the graph is obtained by the following formula:
PR(Vi
) = PRstatic
(Vi
) + PRsemantic
(Vi
)
(5)
In the ﬁnal step of the ranking process, PR(Vi) is improved by other statistical features
like TF.ISF of the root and position of the sentence. The ﬁnal score assigned to each
sentence is given by the following formula:
score(Si
) = PR(Si
) +
∑
wj∈Si TF.ISF(wj)
rootCount(Si)
+ Position(Si
)
(6)
Where:
• PR(Si) is the rank of the sentence Si in the graph given by (5).
• TF.ISF is the term frequency inverse sentence frequency of the root.
• Position(Si
) = 1 for sentences in the ﬁrst and last position, 0 otherwise.
3.3
Post-processing
Post-treatment is the ﬁnal step of our system. It consists of eliminating redundancy from
the best scored sentences by the formula (6). In this way, we are sure that our ﬁnal
generated summary covers a diversity of most information contained in the original input
document.
Step 8: Redundancy elimination and summary generation. In this step, each
sentence has its salient score Score(Si) obtained after the ranking process. Simply and
as other graph based summarization systems, we can choose the sentences with the
highest score to be included in the ﬁnal summary. However, several information is
redundant in the summary, because many similar sentences representing the same idea
in the document have similar score, so they can be included together in the summary.
Moreover, some important sentences may not be included in the ﬁnal summary and other
ideas of the text can be ignored. To overcome this problem, the ﬁnal summary is build
using an adapted version of MMR [35] algorithm. MMR re-rank all sentences and select
the most relevant between them without redundancy.
The main idea of MMR is that the summary is constructed by a high ranked sentences
which are not similar to any existing sentence in the summary. The summary S is initial‐
ized by the most ranked sentence which is removed from the ranked list R. Then, the
sentence with the highest score from (7) is added to the summary and removed from the
ranked list. This process is repeated until the length of the summary is reached. The
MMR method works according to the ﬂowing equation:
MMR = argmaxsi∈R∖S
[
𝜆∗score(si
) −(1 −𝜆) ∗maxsj∈S ∗sim(si, sj)
]
(7)
44
N. Alami et al.

In this equation, R is the set of sentences, S is the set of summary sentences,
score(si) is the score of sentence Si computed by Eq. (6) and sim(si, sj) is the measure of
the semantic similarity between sentences si and sj; λ is a tuning factor between the
importance of the sentence and its relevance to previously selected sentences.
4
Experimentation and Results
4.1
Datasets (Corpus)
To better assess the quality of any automatic summarization method, we need to compare
the obtained results with manually extracted summary. In Arabic language, several
studies have been made to overcome the shortage of the Arabic language in corpora.
El-Haj et al. [10] used Amazon’s Mechanical Turk to build Essex Arabic Summaries
Corpus (EASC). The dataset consists of 153 Arabic articles collected from two Arabic
newspapers and the Arabic version of Wikipedia. The dataset contains 10 main topics:
art and music, education, environment, ﬁnance, health, politics, religion, science and
technology, sports and tourism. For each document, ﬁve model extractive summaries
are available. These model summaries were created by native Arabic speakers using
Mechanical Turk. The model summaries size does not exceed 50% of the source docu‐
ment’s size. The dataset is available in two encodings: UTF-8 and ISO-Arabic. None‐
theless, nowadays and to the best of our knowledge, no standard dataset has been
designed to evaluate the performances Arabic text summarization systems. In this work,
we used the EASC corpus to evaluate the performance of our system over reference
systems.
4.2
Evaluation Metrics
The generated summary is considered relevant or not relevant based on the comparison
between the manual generated summary. Three important measures are commonly used,
precision, recall and F-measure.
Precision (P): The measure of how much information returned by the system is
correct.
P =
||Smanual ∩Sauto||
||Sauto||
(8)
Where Smanual represents the set of sentences in the manual summary and Sauto represents
the set of sentences in the auto-generated summary.
Recall (R): The measure of the coverage of the system. It reﬂects the ratio of relevant
sentences that the system extracted.
R =
||Smanual ∩Sauto||
||Smanual||
(9)
Statistical and Semantic Analysis
45

F-measure (F): Makes a balance between recall and precision using a parameter β.
The (F-Measure/summary size) ratio is important when comparing systems. We obtain
the F1 score by setting the value of β to one:
F = 2 ∗P ∗R
P + R
(10)
4.3
Experiment Setup
We developed all the system described below in java language. We evaluated our system
against four other systems. The ﬁrst system is the Arabic text summarization based on
graph theory (ATSG) [36]. It uses a cosine similarity to compute the similarity between
sentences. It makes a graph representation for an input Arabic document and applies the
PageRank algorithm in order to rank each sentence in the graph. The system is then
performed by removing redundancy from the ﬁnal summary. The second system is
TextRank [29]. TextRank is a graph-based ranking algorithm which is designed for key-
words extraction and text summarization systems. It performs a Google PageRank algo‐
rithm [28] on a graph generated from the original text. Thus, all sentences in the docu‐
ment to be summarized is represented by a node in the graph and the edge between two
note represents the similarity relation computed as a number of similar words existing
between two sentences. The weight of each edge indicates the degree of similarity
between related sentences. Sentences are ranked according to the score given by
PageRank. The ﬁnal summary is built by selecting sentences with the highest score. The
third system is LexRank [30] which is the same of TextRank. The main diﬀerence is
that LexRank is adapted to multi-document summarization and it uses the cosine simi‐
larity metric instead of the number of similar terms between two sentences. We devel‐
oped TextRank and LexRank and adapted these two systems in order to support Arabic
Text. The fourth system is a clustering based Arabic text summarization system
described in [6]. We ran our algorithm to generate summaries for these sample texts in
ﬁve diﬀerent sizes: 20%, 25%, 30%, 35% and 40%.
4.4
Results and Discussion
We have calculated Precision, Recall and F1 score to evaluate the quality of the
summary. Table 2 summarizes the results of running our algorithm on the ESCAS corpus
with diﬀerent sizes. As can be seen in Table 2, when the summary size decreases the
recall also decreases and so will the F1-measure because the co-occurrence between
candidate summary and gold summary increases. This should be clear from Eq. (9).
46
N. Alami et al.

Table 2. Evaluation results of our system
Precision
Recall
F1-measure
Summary size
62.25
47.76
54.05
20%
59.98
54.11
56.89
25%
57.62
58.80
58.20
30%
53.24
65.17
58.60
35%
51.22
70.05
59.52
40%
The comparison between average Recall, precision and F1-measure of our system
with other baseline systems is given in Table 3. The summary size taken into account
in this comparison is 30% of the original document. We can see that our system has the
highest value of average F1 score as compared to other systems. ATSG system has a
good F1 score compared to other baseline systems, which shows that our method
enhances the performance of the Graph-based summarization system.
Table 3. Comparison against other systems with 30% of size
System
Precision
Recall
F1-measure
Our system
57.62
58.80
58.20
ATSG [36]
46.22
47.31
46.76
TextRank [29]
44.26
36.24
39.85
LexRank [30]
31.03
25.71
28.12
Clustering technique [6]
47.45
38.89
42.74
With summary size 30%, the best F1-measure score of the other systems is reported
by the ATSG system with 46.76% for the tested dataset, whereas in our experiment, the
average value of F1-measure is 58.20% for the same dataset.
Firstly, English text summarization systems rely on some existing evaluation work‐
shops organized to encourage research to evaluate their systems on a large test collection.
These workshops like Document Understanding Conferences (DUC) or Text Analysis
Conference (TAC) do not support Arabic language. Thus no approved benchmark
dataset available to evaluate Arabic text summarization systems and each work uses its
own dataset. This situation makes the performance comparison between the proposed
approaches and the existing ones more diﬃcult.
Secondly, and due to the complexity of the evaluation task, a new evaluation metrics
are under development. Thus, various works use diﬀerent evaluation measures in order
to better asses the quality of the summarization task. Moreover, the scientiﬁc community
interested in Arabic NLP and especially in automatic summarization ﬁeld, is small and
not always enough. Furthermore, Arabic is complex in terms of its morphology, vocabu‐
lary and spelling, which make lexical, syntactic and semantic analysis even diﬃcult.
Statistical and Semantic Analysis
47

5
Conclusion and Future Work
This paper presents a novel Arabic text summarization system that the main goal is to
take into account the semantic relationships existing between textual units and to deal
with redundancy and information diversity issues. This system integrates the power of
a graph model in order to score sentences according to the Google PageRank algorithm.
The graph is used to represent the Arabic document using two diﬀerent relations among
sentences: (i) statistical similarity; and (ii) semantic similarity. Other statistical features
extracted from the document are used to improve the summary quality. Our proposed
system is knowledge-rich because it integrates an external knowledge database devel‐
oped by human.
The main contribution of the proposed method is the creation of an Arabic summa‐
rization system that combines statistical and semantic treatment of the input text, besides
using an adapted version of MMR technique to eliminate redundancy from the ﬁnal
summary.
A comparison of performance measures indicates the advantage of our system as
compared to some other summarization systems. Benchmarking the proposed algorithm
using the dataset described above showed that our system outperformed all other systems
and enhances the performance of the Graph-based summarization system. In addition,
the system is domain-dependent and does not use any structural features and was, there‐
fore, successfully used in Arabic summarization task.
In the future work, we are going to investigate several directions in order to improve
the performance of the proposed system. The ﬁrst direction is to extend the dataset used
in this paper by additional documents with their manual summaries. This will give more
value to our method. A second direction is to consider more features of the text such as
part-of speech and co-reference resolution. Another direction is to adapt our system to
Arabic multiple document summarization.
References
1. Hovy, E.H.: Automated text summarization. In: Mitkov, R. (ed.) The Oxford Handbook of
Computational Linguistics, pp. 583–598. Oxford University Press, Oxford (2005)
2. Luhn, H.P.: The automatic creation of literature abstracts. IBM J. Res. Dev. 2, 159–165 (1958)
3. Edmundson, H.P.: New methods in automatic extracting. J. ACM 16(2), 264–285 (1969)
4. Douzidia, F.S., Lapalme, G.: Lakhas, an Arabic summarization system. In: Proceedings of
2004 Document Understanding Conference (DUC 2004), Boston, MA (2004)
5. AlSanie, W.: Towards an infrastructure for Arabic text summarization using rhetorical
structure theory. M.Sc. Thesis. King Saud University, Riyadh, Saudi Arabia (2005)
6. Haboush, A., Momani, A., Al-Zoubi, M., Tarazi, M.: Arabic text summarization model using
clustering techniques. World Comput. Sci. Inf. Technol. J. (WCSIT) 2(3), 62–67 (2012).
ISSN: 2221-0741
7. El-Haj, M., Kruschwitz, U., Fox, C.: Exploring clustering for multi-document arabic
summarisation. In: Salem, M., Shaalan, K., Oroumchian, F., Shakery, A., Khelalfa, H. (eds.)
Information Retrieval Technology. Lecture Notes in Computer Science, vol. 7097, pp. 550–
561. Springer, Berlin (2011)
48
N. Alami et al.

8. Ibrahim, A., Elghazaly, T.: Rhetorical representation and vector representation in
summarizing arabic text. In: Métais, E., Meziane, F., Saraee, M., Sugumaran, V., Vadera, S.
(eds.) Natural Language Processing and Information Systems. Lecture Notes in Computer
Science, vol. 7934, pp. 421–424. Springer, Berlin (2013)
9. Oufaida, H., Nouali, O., Blache, P.: Minimum redundancy and maximum relevance for single
and multidocument Arabic text summarization. J King Saud Univ. Comput. Inf. Sci. 26(4),
450–461 (2014). Special Issue on Arabic NLP
10. El-Haj, M., Kruschwitz, U., Fox, C.: Using mechanical turk to create a corpus of arabic
summaries. In: Proceedings of the International Conference on Language Resources and
Evaluation (LREC), Valletta, Malta. In the Language Resources (LRs) and Human Language
Technologies (HLT) for Semitic Languages Workshop Held in Conjunction with the 7th
International Language Resources and Evaluation Conference, pp. 36–39 (2010)
11. Miller, G.A.: WordNet: a lexical database for English. Commun. ACM 38(11), 39–41 (1995)
12. Ferreira, R., de Souza Cabral, L., Freitas, F., Lins, R.D., de Frana Silva, G., Simske, S.J.,
Favaro, L.: A multi-document summarization system based on statistics and linguistic
treatment. Expert Syst. Appl. 41(13), 5780–5787 (2014)
13. Pal, A.R., Saha, D.: An approach to automatic text summarization using WordNet. In:
Advance Computing Conference (IACC), 2014 IEEE International, pp. 1169–1173 (2014).
doi:10.1109/IAdCC.2014.6779492
14. Estiri, A., Kahani, M., Ghaemi, H., Abasi, M.: Improvement of an abstractive summarization
evaluation tool using lexical-semantic relations and weighted syntax tags in Farsi language.
In: 2014 Iranian Conference on Intelligent Systems (ICIS), pp. 1–6 (2014). doi:10.1109/
IranianCIS.2014.6802594
15. Patil, A.P., Dalmia, S., Abu Ayub Ansari, S., Aul, T., Bhatnagar, V.: Automatic text
summarizer. In: 2014 International Conference on Advances in Computing, Communications
and Informatics (ICACCI), pp. 1530–1534 (2014). doi:10.1109/ICACCI.2014.6968629
16. Wei, T., Lu, Y., Chang, H., Zhou, Q., Bao, X.: A semantic approach for text clustering using
WordNet and lexical chains. Expert Syst. Appl. 42(4), 2264–2275 (2015). doi:10.1016/j.eswa.
2014.10.023. ISSN: 0957-4174
17. Bouras, C., Tsogkas, V.: A clustering technique for news articles using WordNet. Knowl
Based Syst. 36, 115–128 (2012)
18. Chen, C.L., Tseng, F.S., Liang, T.: An integration of WordNet and fuzzy association rule
mining for multi-label document clustering. Data Knowl. Eng. 69(11), 1208–1226 (2010)
19. Sachdeva, P., Verma, S., Singh, S.K.: An improved approach to word sense disambiguation.
In: 2014 IEEE International Symposium on Signal Processing and Information Technology
(ISSPIT), pp. 000235–000240 (2014)
20. Dhungana, U.R., Shakya, S., Baral, K., Sharma, B.: Word Sense Disambiguation using WSD
speciﬁc WordNet of polysemy words. In: 2015 IEEE International Conference on Semantic
Computing (ICSC), pp. 148–152 (2015)
21. Gao, J.B., Zhang, B.W., Chen, X.H.: A WordNet-based semantic similarity measurement
combining edge-counting and information content theory. Eng. Appl. Artif. Intell. 39, 80–88
(2015)
22. Li, Y., Li, H., Cai, Q., Han, D.: A novel semantic similarity measure within sentences. In:
2012 2nd International Conference on Computer Science and Network Technology
(ICCSNT), pp. 1176–1179 (2012). doi:10.1109/ICCSNT.2012.6526134
23. Varelas, G., Voutsakis, E., Raftopoulou, P., Petrakis, E.G.M., Milios, E.E.: Semantic
similarity methods in wordnet and their application to information retrieval on the web. In:
Proceedings of the Seventh Annual ACM International Workshop on Web Information and
Data Management, WIDM 2005, pp. 10–16. ACM, New York (2005)
Statistical and Semantic Analysis
49

24. Khoja, S.: APT: Arabic part-of-speech tagger. In: Proceedings of the Student Workshop at
NAACL, pp. 20–25 (2001)
25. Pedersen, T.: Information content measures of semantic similarity perform better without
sense-tagged text. In: Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, HLT 2010,
pp. 329–332. Association for Computational Linguistics, Stroudsburg (2010)
26. Wu, Z., Palmer, M.: Verbs semantics and lexical selection. Paper Presented at the Proceedings
of the 32nd Annual Meeting on Association for Computational Linguistics (1994)
27. Lin, D.: An information-theoretic deﬁnition of similarity. In: Proceedings of the 15th
International Conference on Machine Learning, Madison, WI, pp. 296–304 (1998)
28. Brin, S., Page, L.: The anatomy of a large-scale hypertextual web search engine. Comput.
Netw. 30(1–7), 107–117 (1998)
29. Mihalcea, R., Tarau, P.: TextRank: bringing order into texts. In: Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP 2004), Barcelona, Spain
(2004)
30. Erkan, G., Radev, D.R.: Lexrank: graph-based lexical centrality as salience in text
summarization. J. Artif. Intell. Res. 22, 457–479 (2004)
31. Alguliyev, R.M., Aliguliyev, R.M., Isazade, N.R.: An unsupervised approach to generating
generic summaries of documents. Appl. Soft Comput. 34, 236–250 (2015). ISSN: 1568-4946
32. The Global Wordnet Association. http://www.globalwordnet.org/
33. Elberrichi, Z., Abidi, K.: Arabic text categorization: a comparative study of diﬀerent
representation modes. Int. Arab J. Inf. Technol. 9, 465–470 (2012)
34. Malik, R., Subramaniam, V., Kaushik, S.: Automatically selecting answer templates to
respond to customer emails. In: Proceedings of IJCAI 2007, Hyderabad, India, pp. 1659–1664
(2007)
35. Carbonell, J.G., Goldstein, J.: The use of MMR, diversity-based re-ranking for reordering
documents and producing summaries. In: SIGIR, pp. 335–336 (1998)
36. Alami, N., Meknassi, M., Alaoui Ouatik, S., Ennahnahi, N.: Arabic text summarization based
on graph theory. In: IEEE/ACS 12th International Conference of Computer Systems and
Applications (AICCSA), Marrakech, pp. 1–8 (2015)
50
N. Alami et al.

Entity Resolution in NoSQL Data Warehouse
Lamiae Alami
(✉), Imad Haﬁdi, and Abdelmotalib Metrane
Laboratoire IPOSI, Université Hassan 1, 25000 Khouribga, Maroc
alami.lamia@gmail.com, imad.hafidi@gmail.com,
ab.metrane@gmail.com
Abstract. With the development of the Internet and cloud computing, there is
the need of databases that will be able to store and process big data, and Not only
SQL ’NoSQL’ databases are becoming increasingly used in the big data domains
and have some interesting strengths such as scalability and ﬂexibility. This paper
explains the growing interest of implementing NoSQL in Data Warehouses. In
addition, this paper investigates the use of data cleaning (the process of detecting
and correcting or removing inaccurate records from a database) in NoSQL data‐
bases. More precisely, we are interested in adapting the data deduplication algo‐
rithms in two NoSQL models: document-oriented and column-oriented. Finally,
a comparison between the implemented algorithms and the results of our
simulations.
Keywords: Data warehouse · NoSQL · Data cleaning · Deduplication ·
Document-oriented · Column-oriented
1
Introduction
The increase of data volume (Big Data) during the last decade is attributed to a variety
of data sources, such as social medias, GPS data, sensor data, surveillance data, text
documents, e-mails, etc. [15]. Factors such as cost, volume, variety and the pace at which
the data is being created and consumed play an important role in deciding how and where
the data should be stored and managed.
Features like high availability, scalability, and replication are all available in the
traditional relational databases but they usually come with a high cost. The main factors
for the popularity of the NoSQL databases include cheap hardware, easy scalability, and
high availability. NoSQL solutions have proven some clear advantages with respect to
relational data- base management systems (RDBMS). In this paper, we focus on two
classes of NoSQL stores: document-oriented systems and column-oriented systems.
Data cleaning, also called data cleansing or scrubbing, deals with detecting and
removing errors and inconsistencies from data to improve its quality. The problem is
due to misspellings during data entry, missing information or writing wrong data. Data
cleaning is very important step in process of integration of heterogeneous Data sources
in the Data warehouse [6].
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_5

NoSQL systems compromise data consistency to achieve better performance, scal‐
ability and high availability. The need for data cleaning is increasing signiﬁcantly
[15, 18]. In this paper, we make the following contributions:
• We present the diﬀerent approaches of the Data cleaning.
• We adapt the deduplication algorithms for column-oriented and document-oriented
databases.
The paper is organized on four sections: The ﬁrst section, we introduce NoSQL
databases and their advantages; we expose the growing interest of implementing NoSQL
in DWs. In the second section, we present the diﬀerent approaches of data cleaning. In
the third section, we present the previous works that solves the problem of deduplication,
and expose the works that treats the same problem in NoSQL databases. The forth section
presents the comparison and the merge process in the NoSQL databases and the algo‐
rithms used in our prototype. And in the last section we present the results of our simu‐
lations and some conclusions are drawn.
2
NoSQL Database
Although traditional relational database management systems (RDBMS) have existed
for decades and are constantly being improved by database providers, the management
of large volumes of data in RDBMS remains a major problem [17]. However, a new
category of databases called NoSQL databases is able to support larger volumes of data
by providing faster access to data and cost savings.
The NoSQL databases diﬀer signiﬁcantly on their approach to maintaining data
integrity and consistency [18]. A more ﬂexible approach to data consistency helps
NoSQL databases to improve data storage performance. As the RDBMS strongly values
data integrity, they use the ACID theorem for data consistency presented by Jim Gray
in the early 1980 s. ACID is an acronym for Atomicity, Coherence, Insulation and
Sustainability and supports the concept of transaction [18]. On the other hand, the
NoSQL databases use the theorem CAP (Coherence, Availability and Tolerance of
Partition) for data consistency, presented in 2000 by Eric Brewer at an ACM sympo‐
sium [18].
The CAP theorem indicates that of the three possible combinations of CAP, only
two are available at a given time. In other words, NoSQL databases can have partition
tolerance (in a distributed environment) and tolerance and consistency or partition avail‐
ability, but not all three factors at the same time. There are four types of databases
NoSQL: key-values, graph-oriented, document-oriented and column-oriented. We inter‐
sect in this paper the last two types.
Document-oriented: evolution of key-value databases. The keys are no longer asso‐
ciated with values in the form of a binary block but with a document whose format is
not imposed, as long as the database is able to manipulate the chosen format in order to
allow processing on documents. Otherwise, it would be equivalent to a key-value data‐
base. Although the documents are structured, this type of database is called “schema
less”, because it’s not necessary to deﬁne the ﬁelds of a document. Structured document
52
L. Alami et al.

storage gives them functionality that is lacking in simple key/value databases such as
the ability to query the content of objects. MongoDB is an example of NoSQL document-
oriented database.
Column-oriented: have been designed by web giants to cope with the management
and processing of large volumes of data. They often integrate a minimalist query system
close to SQL. Although they are widely used, there is yet no formal method or deﬁned
rules for a column-oriented database to be classiﬁed as quality or not. We use the Column
concept: this is the base entity representing a data ﬁeld. Each column is deﬁned by a
key/value pair. A column containing other columns is called a super column. Column
Family: is a container that allows you to group multiple columns or super columns. The
columns are grouped by row and each row is identiﬁed by a unique identiﬁer. They are
generally assimilated to the tables in the relational model and are identiﬁed by a unique
name. HBase is an example of NoSQL column -oriented database.
3
NoSQL Database
The DW literature review is increasingly studying the implementation of DWs in
NoSQL [11–13, 19]. DWs periodically load data from large amounts of data sources,
leaving the probability of dirty data high. DWs are used to make decisions which makes
the obligation to the accuracy of these data is essential to avoid mistaken conclusions.
The implementation of the ETL calls for sub-processes including data cleaning. Data
cleaning remains one of the major problems of the DW and its job is the detection and
removal of errors and inconsistencies to increase the quality of the data.
The need for data cleaning increases signiﬁcantly when integrating multiple data
sources because the sources are often containing redundant data in diﬀerent represen‐
tations. In order to provide access to accurate and consistent data, the consolidation of
diﬀerent representations of data and the elimination of duplicated information become
necessary.
The data cleaning problem is divided into two categories: Single source and Multi-
source, and each category is divided into sub-categories (Schema/Instance, automatic/
non-automatic error detection, etc.).
We are interested in this paper in the Single Source which is divided into two main
categories: problems between schema (lack of integrity, uniqueness of identiﬁers, nomi‐
nation conﬂicts, heterogeneity of sources) and problems related to the instance (dupli‐
cates, contradictory values, Etc.). For NoSQL databases, sources are not governed by
restrictions and integrity the probability of errors increases.
The deﬁnition of quality is multidimensional, we can cite some dimensions: Accu‐
racy: compliance with the current value. Timeliness: The recorded value has not expired.
Completeness: all values in a certain variable are re-registered. Consistency: the repre‐
sentation of the data is uniform in all cases. We study in particular the problem of dislo‐
cation (Entity resolution).
Entity matching is a crucial and diﬃcult task in data integration, enabling entities
that match the same entity in the real world to be identiﬁed. The task of matching entities
Entity Resolution in NoSQL Data Warehouse
53

will be diﬃcult especially if the data more and more heterogeneous so if the quality of
the data is limited.
4
Related Work
Entity resolution (also referred to as duplicate identiﬁcation, record linkage, entity
matching or reference reconciliation) is a crucial task for Data cleaning [1, 8, 10]. This
is to verify the correspondence data (Match) and then merge (Merge) [2, 4, 5, 7, 9].
Many algorithms for entity resolution have been proposed [2, 4, 9].
The ﬁrst algorithm [9] matching approach is sorted neighborhood method (SNM),
which sorts out the duplicates using an appropriate blocking key and only compares
objects within a predeﬁned sliding window size w. The time complexity of SNM is
reduced to O (n x w). In [4] We propose a raw parallelization of SNM with Map Reduce
and by partitioning the data.
Another approach proposed in [2, 10], the algorithms uses match and merge function.
Its complexity is higher than SNM but better in term of quality.
We present in this paper the algorithms mentioned in [2, 10] in NoSQL. To our
knowledge, these algorithms were never implemented in NoSQL data warehouses. In
order to implement these algorithms, we propose Match and Merge functions adapted
to Column-oriented and Document-oriented warehouses.
5
Processing Comparison and Fusion
5.1
Match et Merge Functions
The elimination of similar data requires the implementation of two functions:
• The “Match” function allows deﬁning if two tuples/documents are similar, using
diﬀerent algorithms of distance calculation, the match function compares two tuples
and returns a Boolean value “true” if these two tuples are similar, “False” otherwise.
• The “Merge” function merges two similar tuples/documents and generates a new
richer tuple.
Let us now introduce some notations. T (A1…An) is a table and interpreted by the
set [[T]] of all A1…An tuples on [[Dk]] k = 1, n with A = A1…An the set of attributes
of T. Each element t of [[T]] is a tuple of the table T and each t.Ak is a value of the
attribute in t.
Deﬁnition 1: Similarities between values (denoted by ~ ): For an attribute Ak, note
t.Ak ~ t’.Ak, if dk ≤ sk, with dk is the calculated similarity distance (using one of the
distance algorithms that we will mention later), and sk is the threshold set by the user
(sk in [0,1], sk in S, k = 1..n).
54
L. Alami et al.

Deﬁnition 2: Similarity rules: A similarity rule r is a conjunction of similarities that
deal with attributes Ak (k = 1, n) of the set A of the table T. r = (t.A1 ~ t’.A1)
(T.A2 ~ t’.A2) … (t.An ~ t’.An).
Deﬁnition 3: Similarity between tuples: Two tuples t and t’ are similar if the disjunction
of all the similarity rules deﬁned on T is true. And we note t ~ t’.
The Match function compares two tuples t1 and t2: Match (t1, t2) = True if only
t1 ~ t2. The similarity study deals with the attributes of the set A according to the rules
of similarity. The Similar function (v1, v2, k) makes it possible to say whether two values
v1 and v2 are similar according to a condition k.
The Merge function processes two tuples, which are assumed to be similar, such as
Match (t1, t2) = True. It returns a new tuple which is the result of the merge of the
previous two. The Merge function returns a tuple that provides more information.
For document-oriented databases, since the documents do not follow any precise
schema, we take the document d1 and add all the keys/values of d2 in d1, if the key does
not exist in d1, it is added, If the key already exists in d1, the two values are merged
according to the rules predeﬁned by the user.
For column-oriented database, the function Merge takes in parameter two tuple
supposed similar by the function thus enriched one by the other according to the rules
of the comparisons of the types predeﬁned by the user.
5.2
Similarity Distance
The string metric between two strings is done using several algorithms [14]:
• The Levenshtein distance between two words is the minimum number of single‐
character edits (i.e. insertions, deletions or substitutions) required to change one word
into the other.
• The Jaro-Winkler distance designed and best suited for short strings such as person
names or passwords.
• The Jaccard coeﬃcient measures similarity between Long strings.
By testing these methods, Jaro Winkler seems to work better for strings of diﬀerent
types and lengths.
5.3
Deduplication Algorithms
The eliminating similar data algorithm crosses all the table tuples. It is a very expensive
algorithm especially for large volumes of data. The idea is to reduce the number of
comparisons by merging the matching data. In the following we present and compare
three algorithms:
• Algorithm G: The algorithm G is sequential. It eliminates similar (or almost dupli‐
cate) tuples thanks to the “black box” composed of the two functions Match and
Merge. The algorithm works with two sets of tuples I and I’. I is the set of initial
tuples that have not yet been compared, and I’ contains the ﬁnal result. Every tuple
Entity Resolution in NoSQL Data Warehouse
55

of I is compared to all those of I’. If a tuple t (of I) corresponds to a tuple t’ (of I’), a
new tuple t” (result of the fusion) is created and inserted in I. Then, the dominated
tuple is deleted from I’. If not the tuple t will be added to I’.
• Algorithm R: To improve the performance of the Algorithm G. The algorithm R
makes the following modiﬁcations, when tuple t (of I) corresponds to tuple t’ (of I’),
t is deleted from I (respectively t’ of I’), t” (result of the fusion) is added to I. The
number of comparisons is then reduced and the performance measures we have
performed conﬁrm this improvement.
• Algorithm F: this algorithm is an improvement of the algorithm R to reduce even
more the number of comparisons by introducing two sets of data Pf and Nf to keep
track of the values already encountered and record the redundant comparisons.
F: Feature, in our case we can say that a similarity rule that contains two attributes
“name, ﬁrstname” is a feature.
Pf: records all previously encountered values of the feature “f” and associates with
each value the identiﬁer of the document to which it belongs, when we research for a
value using the function Pf (v), v is the value, if the function returns a document to us,
we can proceed directly to the merge, and if it returns null, that mean that it has never
been met.
Nf: is the set that keeps the values of the key attributes ‘ﬁ’ that have been compared
with all the attribute values for the documents of I’ and does not match any of them, to
avoid redundant comparisons.
6
Works and Results
We implement the F and R algorithms on the Hbase, MongoDB and CSV databases.
The tested data set used is available in CSV ﬁles.
The data is loaded into MongoDB using its own instructions. These allow you to
load data faster from CSV ﬁles. The current version of MongoDB we use loads the data
from the JSON format.
For HBASE, the data is loaded using native instructions. They are supposed to load
data faster when loading ﬁles. The current HBase version loads the data with our logic
model from CSV ﬁles.
6.1
Runtime Environment
The data used are taken from the actual web processed in a machine 2.5 GHz CPU
(Core i5) and 8 GB of Ram, running on Windows 7.
6.2
Used Data
The comparison was made on a set of data in CSV ﬁles counting: 100 000 tuples /
documents. The data generated randomly using the GenerateData tool. Two similarity
rules between tuples were used to perform the merge (Merge). We used the distance
Jaro-winkler with a threshold: 0.8.N.
56
L. Alami et al.

6.3
Results
Diﬀerent measurements (Table 1) were performed on data ranging from 103 to 30000
lines. Both algorithms were tested on the native form CSV, HBASE and MongoDB
(Figs. 1, 2 and 3).
Table 1. Performance measurements of the two algorithms (Seconds)
Lines/Types
Algo
1000
5000
10000
20000
30000
CSV
R
0,525
5,933
26,804
101,234
156,221
F
0,618
3,829
7,775
71,791
117,515
Columns
R
0,25
5,105
22,27
97,956
230,637
F
0,234
1,85
5,134
14,67
27,466
Documents
R
0,483
8,64
31,28
172,03
262,5
F
0,531
6,65
27,63
110,65
169,2
Fig. 1. Performance measurements of the two algorithms for CSV (Seconds).
Fig. 2. Performance measurements of the two algorithms for HBase (Seconds).
Entity Resolution in NoSQL Data Warehouse
57

Fig. 3. Performance measurements of the two algorithms for MongoDB (Seconds).
7
Conclusion
Currently, a large number of information systems use non-SQL databases so their data
warehouses consist of heterogeneous and distributed data of variable quality. There is
a growing need to integrate data and assess data quality. In order to give more meaning
to the collected data, the improvement of duplicate elimination algorithms becomes a
major challenge for improving the quality of the data. Indeed, the development of new
algorithms adapted to NoSQL databases which oﬀers a better quality of cleaning and
which attends the user during all the process.
In our work, we have implemented and adapted the algorithms presented in [2, 9] in
the two HBASE and MongoDB databases. We obtained results which conﬁrms the old
work [2, 9], we also found that these algorithms remain very fast in the case of DW in
CSV earlier than Non SQL. We present in our future work algorithms inspired by F and
R and which exploits the speciﬁc structures of NoSQL storage in both cases oriented
column-oriented documents. We also propose the implementation of these algorithms
in sequential and parallel environments.
References
1. Boufarès, F., Salem, A.B.: Heterogeneous data-integration and data quality: Overview of
conﬂicts. In: Proceedings of the International Conference on Sciences of Electronic,
Technologies of Information and Telecommunications, (SETIT 2011), Sousse, Tunisie,
26–29 October 2011
2. Boufarès, F., BenSalem, A., Correia, S.: Un algorithme de déduplication pour les Bases et
Entrepôts de Données. In: Actes du XXXème Congrès INFormatique des ORganisations et
Systèmes d’Information et de Décision, (INFORSID 2012), Montpellier, France, pp. 497–
504, 29–31 Mai 2012
3. Kulkarni, P.S., Bakal, J.W.: Hybrid approaches for data cleaning in data warehouse. Int. J.
Comput. Appl. 88(18), 8887 (2014)
58
L. Alami et al.

4. Ma, K., Yang, B.: Parallel NoSQL entity resolution approach with MapReduce. In:
International Conference on Intelligent Networking and Collaborative System, pp. 384–389
(2015)
5. Kenig, B., Gal, A.: MFIBlocks An eﬀective blocking algorithm for entity resolution. Inf. Syst.
38(6), 908–926 (2013)
6. Rahm, E., Do, H.H.: Data cleaning: problems and current approaches. In: IEEE Technology.
Bulletin on Data Engineering (2000)
7. Lu, G., Jin, Y., Du, D.H.: Frequency based chunking algorithm for data deduplication. In:
18th Annual Meeting of the IEEE International Symposium on Modeling, Analysis and
Simulation of Computer and Telecommunication Systems (MASCOTS 2010), Miami,
Florida, August 2010
8. Kopcke, H., Rahm, E.: Frameworks for entity matching: a comparison. Data Knowl. Eng.
69(2), 197–210 (2010)
9. Benjalloun, O., Garcia Molina, H., Menestria, D., Su, Q., Whang, S.E., Widom, J.: Swoosh:
a generic approach to entity resolution. Int. J. Very Large Data Bases (VLDB 09) 18(1), 255–
276 (2009)
10. Peng, T.: A framework for data cleaning in data warehouses. In: ICEIS, vol. 1, pp. 473–478
(2008)
11. Chevalier, M., El Malki, M., Kopliku, A., Teste, O., Tournier, R.: Implementing
multidimensional data warehouses into NoSQL. In: ICEIS, vol. 1, pp. 172–183 (2015)
12. Chevalier, M., El Malki, M., Kopliku, A., Teste, O., Tournier, R.: Benchmark for OLAP on
NoSQL technologies comparing NoSQL multidimensional data warehousing solutions. In:
RCIS, pp. 480–485 (2015)
13. Dehdouh, K., Bentayeb, F., Boussaid, O., Kabachi, N.: Using the column oriented NoSQL
model for implementing big data warehouses. In: Proceedings of the International Conference
on Parallel and Distributed Processing Techniques and Applications (PDPTA), PDPTA
(2015)
14. Bilenko, M., Mooney, R.J.: Adaptive duplicate detection using learnable string similarity
measures. In: Proceedings of the 9th ACM SIGKDD International Conference on Knowledge
Discovery, and Data Mining, Washington DC, USA, pp. 39–48 (2003)
15. Fourcet, A.: NonSQL Une nouvelle approche du stockage et de la manipulation des donnél’es,
Livre Blanc Smile (2015)
16. Hernndez, M., Stolfo, S.: The merge/purge problem for large databases. ACM SIGMOD Rec.
24(2), 127–138 (1995)
17. Mohan, C.: History repeats itself: sensible and NonsenSQL aspects of the NoSQL hoopla. In:
EDBT/ICDT 2013 Joint Conference, Genoa-Italy, 18–22 March 2013. ISBN:
878-1-4503-1597-5
18. Roe, C.: ACID vs. BASE: The shifting pH of database transaction processing (2012). http://
www.dataversity.net/acidvs-base-the-shifting-ph-of-databasetransaction-processing/
19. Sahiet, D., Asanka, P.D.: ETL framework design for NOSQL databases in dataware housing.
IJICAR 3(11) (2015)
Entity Resolution in NoSQL Data Warehouse
59

Automation of a Fault Management System
for Bahraini Telecommunication Companies
Abdul Fattah Salman1(✉), Mahmood Majeed2, and Ahmed Alsahlawi3
1 University of Bahrain, Sakhir, Kingdom of Bahrain
fattahsalman@yahoo.com
2 Bahrain Telecommunication Company, Manama, Kingdom of Bahrain
mahmood.majeed15@gmail.com
3 Ahliya Insurance Company, Manama, Kingdom of Bahrain
alsahlawi19@gmail.com
Abstract. This research paper proposes an automatic fault management system
that deals with customer-side faults. It aims at automating all the steps of fault
management from reporting a fault until its resolution and collecting of a customer
signature. The proposed system consists of two parts: the web-based subsystem
and the mobile app. The web-based subsystem is responsible for accomplishing
the following tasks: maintaining faults, assigning faults to staﬀ, managing users
and accounts, and generating reports. The mobile app is responsible for managing
the communication between the customers and ﬁeld staﬀ members from one side
and the web-based system on the other side. The fault assignment algorithm is
based on multiple parameters: priority, type, and geo-location of the fault
(customer), specialty and current geo-location of the staﬀ member. The proposed
algorithm assigns faults to ﬁeld staﬀ members such that all faults are resolved
within the planned ﬁxing time, and minimum distance travelled by each staﬀ
member, and with a workload fairly distributed among ﬁeld staﬀ members.
Keywords: Fault management · Fault assignment algorithm · Customer-side
faults · Field staﬀ member
1
Introduction
In recent days, high speed telecommunication systems are playing a vital role for indi‐
viduals and enterprises. These systems have become very sophisticated and of a high
degree of complexity and thus require high standards of Quality of Service (QoS) as
they have been increasingly used for critical services. Faults, however, are inevitable in
such systems and they can result in degradation of QoS and revenue loss if no proper
fault management techniques are applied [12]. For instance, the 49-minute service
outage of Amazon.com in 2013 led to approximately 5 million US dollars in sales
loss [10].
In general, the process of fault management consists of detecting faults, diagnosing
their causes and recovering from their eﬀects. A system that automates fault
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_6

management assists in building and maintaining reliable networks that are able to
provide and guarantee Quality of Service (QoS) to users, and thus achieving better
performance.
A fault in a system or a component is an observed deviation from the normal oper‐
ation. Fault detection refers to identifying of the symptoms of a fault in a form of system
reported alarms issued from the monitoring processes or trouble reports issued from end
users or personnel [5]. Fault diagnosis refers to the process of specifying the reasons of
a fault. Inputs to the fault diagnosis are the detected symptoms in terms of alarms or
trouble reports. Finally, recovery from the fault eﬀects involves performing required
actions to restore the normal operation of a system or a component. In addition to the
mentioned tasks, fault management may include fault documentation to enhance the
system functionalities. An overview of trouble ticket systems for such documentation
tasks is presented in the thesis of Dreo [6].
In literature, the issues of fault management in various computing and communica‐
tions systems have been addressed and investigated by a large number of researchers
and projects. In [1], Bhagyavati proposed a tool to automate fault management in wire‐
less and mobile networks, while Sihyung Lee et al. provided an overview of present
network-monitoring technologies, identiﬁed open problems, and suggested future direc‐
tions and group issues in these technologies into ﬁve categories [7]. Cynthia S. Hood,
in [2], focused on fault detection problem and proposed an intelligent monitoring system
using adaptive learning machines for communication networks. In [4], Mohammadreza
Davoodi et al. also focused on fault detection and consensus control design for a network
of multi-agent systems and in [3], Chengwei Wu, et al. also investigate fault detection
problem in nonlinear networked systems, and for a network of multi-agent systems. In
[5], Hongjun Li, in his dissertation, designed an intelligent, distributed fault and
performance management system for communication networks based on a distributed
agent paradigm and proposed a framework that supports fault diagnosis for communi‐
cation networks. Zhang Jing, in his dissertation [8], presents a broad overview of
the fault-management mechanisms in optical mesh network and investigates the archi‐
tectures and algorithms for fault management in optical WDM networks. In [9],
Albaghdadi Mouayad investigates various fault management techniques in communi‐
cation systems and focuses on two areas in fault management, namely, fault detection
and event correlation. In [13], Jia R. et al. presented a general fault management approach
for computing systems that can detect, identify and recover a system from faults with
the minimum QoS performance impact.
From the previous review, it is clearly revealed that researchers have investigated
and attained great results in automating all steps of managing faults appearing in
company assets such as network servers, channels, stations, etc. In this research paper,
the proposed system is mainly devoted to automating the steps of managing customer-
side faults including: reporting, assigning to ﬁeld staﬀ member, tracking, resolution, and
evaluation of services.
Automation of a Fault Management System
61

This paper is organized as follows. Section 2 presents the features and problems
associated with the fault management system currently used in Bahraini telecommuni‐
cation companies. The main features and the structure of the proposed system are intro‐
duced in Sect. 3. In Sect. 4, the restrictions, design and implementation of the system
are presented. Finally, conclusions are presented in Sect. 5.
2
The Currently-Used Fault Management System
To provide users with high quality services, telecommunication companies recently use
the state-of-art computing and communication technology and employ highly-qualiﬁed
technical staﬀ. To ensure the availability and reliability of their systems, companies need
a powerful fault management system. Unfortunately, telecommunication companies in
Bahrain are currently using manual fault management system based on phone calls and
emails from customers and operated by system administrators and operators.
For the purpose of this research, faults in telecommunication systems are classiﬁed
into two types: company-side faults, those that occur in company assets, and customer-
side faults, those that occur in customers assets. Both types of faults are not independent
on each other and may aﬀect the continuity and availability of service to the users.
Company-side faults may cease the service for all or groups of users; while customer-
side faults rarely aﬀect other users. Company-side faults are automatically detected and
resolved by special hardware/software systems and not discussed in this paper.
This currently-used system operates as follows. Whenever a customer has a trouble
with his communication, he/she makes a call to the operator of the service provider
company on well-known number which is usually busy most of the time. When the
operator answers, he ﬁlls a special worksheet by questioning the customer about the
problem, its symptoms, and his address and informs the customer to wait a call from the
company within a few working days. The management department manually schedules
reported faults among ﬁeld staﬀ members. At the speciﬁed time and date, the technical
ﬁeld staﬀ arrives at the customer’s location and he tries to identify, locate and solve the
problem. During the ﬁxing process, the staﬀ may need to enquire the customer and/or
to get some information from the technical department by making calls or emails. When
the problem is eventually ﬁxed, the staﬀ ﬁlls some paper form and asks the customer to
sign it to acknowledge that the problem has been solved and the job is done. After one-
two days, the company people may call the customer again to get feedback about the
quality of the done job. The feedback is usually in a form of a verbal questionnaire to
be recorded.
The analysis of the above-mentioned scenario of manual fault management reveals
many issues and serious problems. First, the current system is unfair, error-prone and
time-consuming since the assigning of faults among ﬁeld staﬀ members is done
manually and the whole communication between the involved parties: customer, ﬁeld
staﬀ, and the company operators is based on phone calls between humans. Second, the
currently-used system is very expensive since the management process involves many
oﬃce operators for receiving calls from customers and many qualiﬁed engineers for
consulting technical ﬁeld staﬀ and solving problems. Third, the currently-used system
62
A.F. Salman et al.

uses ineﬃcient documentation tools (papers forms) for documenting the reported trou‐
bles, their resolution, and collecting customers signatures. Fourth, the current system
provides low-eﬃcient tracking and follow-up of faults and their resolution since it uses
paper forms and phone calls. Finally, the degree of customer satisfaction is relatively
low since users have to live long with their communication problems. All the above-
stated issues reveal that the currently-used system is infeasible and an automatic fault
management solution is highly demanded.
3
The Proposed Fault Management System
The main objective of the proposed system is to automate the process of managing the
customer-side faults. This requires automating all the steps of fault management from
reporting a fault until its complete resolution, collecting customer signature, and gener‐
ating of statistical evaluation reports.
The process of managing faults consists of many steps: fault reporting, assigning
faults to ﬁeld staﬀ members, deﬁning ﬁxing times of faults, tracking faults’ status, and
archiving. Fault reporting aims at ﬁlling a form and is performed using two methods:
oﬄine ﬁlling a form from a customer call (currently used) and online ﬁlling a form using
a mobile app (proposed). The ﬁrst method is preserved for the those customers who are
not comfortable with the online form ﬁlling and in this case the form is ﬁlled by the
well-trained company operator based on a phone call from the customer. The suggested
online form ﬁlling frees the company staﬀ from this task and allows the customer to ﬁll
the form by himself. To help the customer in ﬁlling the form online, the system heavily
uses dropdown menus with smart options and assisting comments. Once the form is
ﬁlled and submitted, the system immediately creates a fault ticket that will compete for
resources and ﬁeld staﬀ members.
The proposed system consists of two major parts: web-based subsystem and
mobile app.
3.1
The Proposed Web-Based Subsystem
The web-based subsystem, located on the server, is managed and controlled by the
system administrator and is responsible for performing the major tasks of the proposed
fault management system: maintaining faults, fault assignment algorithm, generating
job schedules, managing users and accounts, and generating reports.
Assume we are given a set of faults F = {fti | i = 1…M}, where each fault is identiﬁed
with a unique value (weight) fti depending on the fault type (ﬁxed lines, mobile lines,
internet, other), priority, required ﬁxing time, and client’s geographic location. The value
of ftj is dynamic and may change from time to time as a new fault is reported or a fault
is ﬁxed. Assume also that we have a set of ﬁeld staﬀ members S = {fsj | j = 1…N},
where each member is identiﬁed with a unique value (weight) fsj depending on his
specialty, his availability, his workload, and his current geographic location. According
to specialty, ﬁeld staﬀ members are classiﬁed into four groups: those responsible of
ﬁxed lines issues, those responsible of mobile lines issues, those responsible of internet
Automation of a Fault Management System
63

issues, and those responsible of other issues. The value of fsj is dynamic and may change
from time to time as a ﬁeld staﬀ member moves from one place to another. So, the main
goal of the fault assignment algorithm is to assign every fault fti to a ﬁeld staﬀ member
fsj, such that all faults are serviced and resolved within the planned service time and
with a workload fairly distributed among ﬁeld staﬀ members.
The structure and the functionality of the proposed web-based subsystem is illus‐
trated in Fig. 1.
Fig. 1. Functional diagram of the proposed web-based system
The fault assigning algorithm, the core of the web-based subsystem, aims at imme‐
diate assigning each reported fault to the most appropriate staﬀ member on the ﬁeld.
The most appropriate ﬁeld staﬀ member is the available one with the needed specialty
with the closest geographic location, and with the lowest workload.
As shown in Fig. 2, this algorithm uses multiple parameters to perform the assign‐
ment task. It assigns a fault to a ﬁeld staﬀ member based on his specialty and availability
ﬁrstly, then his current geographic location, and ﬁnally his workload. By using this
sequence, we aim at ﬁxing more faults within the planned service time, reducing the
distance travelled by staﬀ members, and ensuring a workload fairly distributed among
ﬁeld staﬀ members.
64
A.F. Salman et al.

Fig. 2. Simpliﬁed ﬂowchart of fault assigning algorithm
The web-based subsystem is used by various kinds of users: administrator,
customers, operators, and ﬁeld staﬀ. Each user has privileges and access rights speciﬁed
by the system administrator. The users accounts are managed by the web-based
subsystem under the direct supervision of the system administrator. The customer and
operator use the system to ﬁll and report faults forms.
To support the accountability and auditing functions, the system can generate various
kinds of statistical reports in various formats about faults, workloads and performance
of staﬀ members.
Automation of a Fault Management System
65

3.2
The Proposed Mobile App
The mobile app is used by customers and ﬁeld staﬀ members and is responsible for
accomplishing the communication between the customers and ﬁeld staﬀ members from
one side and the web-based system from the other side.
The ﬁeld staﬀ members use the mobile app to regularly:
• Get the job schedules assigned to them: this part is dynamic since the tasks may
change according to the tasks priority, and whenever a new fault is reported or a fault
is resolved and closed by ﬁeld staﬀ members.
• Get the links to the needed documents: the ﬁeld staﬀ members may need various
documents, charts, and maps needed for accomplishing the tasks. All documents are
located on the server and can be accessed directly using the mobile app by the ﬁeld
staﬀ members, if needed.
• Get the maps of geo-locations of fault addresses.
• Send his/her current geo-locations.
• Signal the starting and ending times for each fault assigned to them. This information
is used by the system for automating the performance evaluation of ﬁeld staﬀ
members and by the learning facility to assist in standardizing of ﬁxing time for each
type of faults.
The customers use the mobile app to:
• Create/edit accounts an sign in as needed.
• Report faults by directly ﬁlling the needed form.
• Track the progress of faults resolution from reporting until accomplishing.
• Provide feedback of the accomplished job that expresses his degree of satisfaction
and includes online signature.
3.3
Data Communication
The complete information about customers, company staﬀ, and faults is stored and
managed by the web-based subsystem. The mobile app is used by customers and the
ﬁeld staﬀ members as discussed in the previous section. The web-based subsystem needs
to publish the records with job schedules and various reports to the mobile app of ﬁeld
staﬀ members. These data transfers between the web-based subsystem and the mobile
app occur frequently and may occupy large portion of memory space of the smartphones
of the ﬁeld staﬀ members and customers. To solve the problem of memory space in
smartphones, a special mechanism is integrated into the system, so that the web-based
subsystem will push the links of the required records on its database to the smartphones
rather than pushing the records themselves. Receiving the links, the mobile app, in turn,
can directly access the real records on the server. To achieve the integrity and improve
the security of the records on the server, limited access rights are assigned by the system
admin to various users.
66
A.F. Salman et al.

4
Implementation and Discussion
The idea of the proposed system was raised by UOB CS students when they held their
training course at one of the Bahraini telecommunication company. Students found out
that faults raised by customers are managed by the company staﬀ manually. So, the
students suggested to develop a system that will automate the entire process of managing
faults.
To simplify the design, development, and implementation of the system (both web
system and the mobile app), the following restrictions were applied:
• Faults are reported by customers using phone calls only.
• Fault are restricted to four types: ﬁxed lines, mobile lines, internet, and others. Each
staﬀ member is specialized in only one of these types.
• The feedback of the customer (signature and rating) is obtained on the staﬀ’s mobile
app after the job is accomplished.
After the defense of the project by students, all the above-mentioned restrictions
were removed and the entire system was redesigned and re-implemented. Reporting
faults is accomplished using the mobile app and/or customers phone calls. The assigning
algorithm is redesigned so as the faults are scheduled dynamically whenever a new fault
is reported or when a fault is resolved/closed. This may cause changes in the job sched‐
ules of some ﬁeld staﬀ members, and thus requires resending the newly generated
schedules to them accordingly. This change in fault assigning algorithm improves its
eﬃciency and applicability.
To provide the ﬂexibility and the scalability of proposed system, it is designed such
that new fault types can be introduced and processed without being modiﬁed.
The proposed mobile app is developed and implemented for smartphones operated
by Android OS since it is the most popular OS of smartphones. The statistics of smart‐
phones market share published in Aug 2016, shows that Android smartphones are domi‐
nating (more than 85%) and expected to maintain its dominant position in the world
market of smartphones in the near future as shown in [11].
The proposed system is implemented in modular form and consists of the following
components as shown in Fig. 3:
• Account Manager: used to create and manage user accounts.
• Fault Reporting: used to report faults from customers.
• Job Schedules generator: used to generate job schedules for ﬁeld staﬀ members.
• Fault Assigning Algorithm: used to assign faults to ﬁeld staﬀ members.
• Staﬀ Performance Evaluator: used to evaluate the ﬁeld staﬀ members.
• Report generator: used to generate statistical reports.
• Learning facility: used to assist in standardizing the service times of various kinds
of faults.
• Geo-Location Finder: used to determine the geo-locations of customers and ﬁeld staﬀ
members.
• Communication: used to accomplish the needed communication between the web-
based system and the mobile app.
Automation of a Fault Management System
67

Fig. 3. Main components of the proposed system
5
Conclusions
In this paper, a system for automating the fault management in a Bahraini telecommu‐
nication company is proposed and developed. The system consists of web-based
subsystem that accomplishes the major tasks of fault management and the mobile app
that manages the communication between the customers and the ﬁeld staﬀ members
from one side and the system on the other side. The main features of the presented fault
management system are: (i) the dynamic and fairness of assigning faults among the ﬁeld
staﬀ members, (ii) the use of GPS for deﬁning the geo-locations of faults and ﬁeld staﬀ
members, (iii) the eﬃcient use of memory space of memory app on smartphones of
customers and ﬁeld staﬀ members, (iv) the normalizing and standardizing of faults
repairing times performed by the learning facility module.
References
1. Bhagyavati: Automated Fault Management in Wireless and Mobile Networks. University of
Louisiana at Lafayette, ProQuest Dissertations Publishing, 3015191 (2001)
2. Hood, C.S., Ji, C.: Probabilistic network fault detection. IEEE Glob. Telecommun. Conf. 3,
1872–1876 (1996)
3. Wu, C., Li, H., Lam, H.-K., Karimi, H.R.: Fault detection for nonlinear networked systems
based on quantization and dropout compensation: an interval type-2 fuzzy-model method.
Neurocomputing 191(26), 409–420 (2016)
4. Davoodi, M., Meskin, N., Khorasani, K.: Simultaneous fault detection and consensus control
design for a network of multi-agent systems. Automatica 66, 185–194 (2016)
68
A.F. Salman et al.

5. Li, H.: Intelligent distributed fault and performance management for communication
networks. University of Maryland, College Park, ProQuest Dissertations Publishing, 3055593
(2002)
6. Dreo, G.: A framework for supporting fault diagnosis in integrated network and systems
management: methodologies for the correlation of trouble tickets and access to problem
solving expertise. Ph.D. thesis, Department of Computer Science, University of Munich
(1995)
7. Lee, S., Levanti, K., Kim, H.S.: Network monitoring: present and future. Comput. Netw. 65,
84–98 (2014)
8. Zhang, J.: Architectures and algorithms for fault management in optical WDM networks.
University of California, Davis, ProQuest Dissertations Publishing, 3171918 (2005)
9. Albaghdadi, M.J.: Fault management techniques in communication systems. Illinois Institute
of Technology, ProQuest Dissertations Publishing, 3051375 (2001)
10. Amazon Outage Could Cost a Lot More than 400,000 Pairs of Unsold Underwear, Marcus
Wohlsen. https://www.wired.com/2013/02/amazon-crash-unsold-underwear/
11. There’s no hope of anyone catching up to Android and iOS, Jeff Dunn. http://www.business
insider.com/smartphone-market-share-android-ios-windows-blackberry-2016–8
12. Dean, J.: Designs, lessons and advice from building large distributed systems. Keynote from
LADIS (2009)
13. Lan, Z., Li., Y.: Adaptive fault management of parallel applications for high-performance
computing. IEEE Trans. Comput. 57(12), 1647–1660 (2008)
Automation of a Fault Management System
69

Towards Classiﬁcation of Web Ontologies Using
the Horizontal and Vertical Segmentation
Redouane Nejjahi1(✉), Noreddine Gherabi2, and Abderrahim Marzouk1
1 FSTS, IR2M Laboratory, Hassan 1st University, Settat, Morocco
nejjahi@gmail.com, amarzouk2004@yahoo.fr
2 ENSAK, LIPOSI Laboratory, Hassan 1st University, Khouribga, Morocco
gherabi@gmail.com
Abstract. The new era of the Web is known as the semantic Web or the Web of
data. The semantic Web depends on ontologies that are seen as one of its pillars.
The bigger these ontologies, the greater their exploitation. However, when these
ontologies become too big other problems may appear, such as the complexity to
charge big ﬁles in memory, the time it needs to download such ﬁles and especially
the time it needs to make reasoning on them. We discuss in this paper approaches
for segmenting such big Web ontologies as well as its usefulness. The segmen‐
tation method extracts from an existing ontology a segment that represents a layer
or a generation in the existing ontology; i.e. a horizontally extraction. The
extracted segment should be itself an ontology.
Keywords: Ontology · Segmentation · OWL · Semantic Web
1
Introduction
The Web ontologies present several interests for the Web, such as annotating data
[1, 2], distinguishing between homonyms and polysemy, generalizing or specializing
concepts, driving intelligent user interfaces and even inferring entirely new (implicit)
information [3, 4]. Ontologies are created by ontology engineers with the help of domain
experts [5]. Let us take as an example an ontology representing a population. This
ontology should have information about the citizens, their dates of birth, relationships,
hobbies, addresses, competences, jobs, etc. It seems to be a great ontology allowing us
to get new information about one person’s tendencies, how these tendencies may be
aﬀected by his relationships; also, the companies can use it to target their advertisements.
But, for one reason or another, we may not be concerned by the population under a
certain age, or may be interested only in a particular city’s population. For such purposes,
we assume that segments of such big ontologies that contain only the desired information
will respond better to the users’ expectations.
Several studies are focused on the extraction, classiﬁcation [6, 7] and segmentation
[3, 8] of data in Web ontology; these data can be represented in the ontology web. For
example, N. Gherabi et al. [9] present a new approach to mapping data stored in relational
databases in the semantic Web, it uses simple mappings based on certain speciﬁcations
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_7

of the database schema and explain how relational databases can be used to deﬁne a
mapping mechanism between the relational database and the OWL ontology. In another
work [10], the authors have developed a method to convert UML schemas to Web
Ontology.
J. Seidenberg and A. Rector have presented in [3] a method for extracting small
segments from large ontologies using the GALEN as an example. The segmentation
algorithm they have presented was based on the classes hierarchy, which leads to
segmenting the ontology by extracting a speciﬁc class hierarchy. Such method responds
to segmenting ontologies like the GALEN ontology where one could be interested in a
concept like HEART and all its super classes and subclasses.
A. Simonyi and M. Szőts have showed in [11] an ontology segmentation tool which
extracts subontologies from a given ontology based on the criterion of relevance supplied
by the end user, mainly concept relevance.
In this paper, we aim to present diﬀerent methods for segmenting OWL ontologies:
horizontally (based on criteria on individuals), vertically (based on the choice of classes
or properties to maintain) and both.
2
Advantages of Web Ontologies Segmentation
The Web ontologies after being created, they grow exponentially in size, either by
importing new information or by inferring additional information using the reasoners.
Once these ontologies are sufficiently massive, we will be faced with problems related
to their treatment. For example, the Gene ontology is of size 148 MB, the Foundational
Model of Anatomy ontology is of size 191 MB, and the Thesaurus ontology is of size
257 MB. Processing such files require machines with a certain level of performance.
By reducing the size of the ontology ﬁle, we can reduce the latency of the data load
in memory. In addition, ﬁles with smaller sizes are highly desirable for the transfer on
the Internet or to be processed by machines with limited resources.
Ontology segmentation algorithms aim to reduce the size of the ontology ﬁle, some
of these algorithms can reduce the size of the ontology ﬁle to a quarter the original size
which can accelerate the reasoning, especially when the original ontology is a big one
like the GALEN ontology which contains 23139 classes [3, 7, 12, 13].
3
The Horizontal Segmentation Algorithm
The OWL ontologies consist of several parts. Among the most important, we have
classes, properties and individuals. By analogy to the relations from relational algebra,
we can say that the classes and properties represent the schema of the ontology while
individuals represent its extension. Large part of the ontology will be that describing
individuals. The segmentation process in the horizontal segmentation will be applied on
the individuals to extract those fulﬁlling the condition(s) of ﬁltering. These conditions
may be expressed as range of ObjectProperty = X, or range of DatatypeProperty > Y.
Towards Classiﬁcation of Web Ontologies
71

3.1
The Example Ontology
The segmentation algorithm is based on the properties of the ontology to segment.
For example, from an ontology that treats citizens, and therefore deﬁnes the
following properties: hasAsFather, hasAsMother, hasABondOfBrotherhood, isMar‐
riedTo, livesIn, dateOfBirth, isLoctedIn (city X isLocatedIn country Y), isFriendOf, etc.
we can extract other ontologies which are segments of the large ontology, and based on
one or more of these properties.
The ontology that we will take as an example is an ontology that describes some
citizens, their hobbies, their studies, their political orientations and their parental rela‐
tionships as well as their dwelling places. Figure 1 illustrates its structure.
Fig. 1. Diagram of the Citizen ontology.
72
R. Nejjahi et al.

For the Class and Object Property hierarchies, they are as follows:
Fig. 2. Class and Object property hierarchies.
3.2
The Ontology Segmentation Algorithm
We start the algorithm with an initial ontology as a data to process. Another input should
be the criteria of the segmentation. It can be a condition on a DatatypeProperty e.g.
dateOfBirth >= ‘01/01/1997’ or a condition on an ObjectProperty e.g. “live
in” - > range = ‘Casablanca’. Some of these criteria may be applied directly on the
ontology, others may need special processing, e.g. looking for people from Morocco,
which leads to look for people with “live in” - > range = x and x “is in” -
 > range = ‘Morocco’.
The algorithm is presented as follows:
1 - Given: SourceOntology, DestinationOntology
2 - // DestOntology is an empty ontology
3 - Begin
4 - CopyStructure (SrcOntology, DestOntology)
5 - For Each individual In SrcOntology.Individuals
6 -
If (condition = True) Then
7 -
addTo (DestOntology, individual)
8 -
End If
9 - End For
10 - DeleteAssertionsReferringToNonexistentIndividuals
11 - End
We start the algorithm with two ﬁles, the ﬁrst one is the Source Ontology, which
represents the whole ontology that we aim to segment, and the second one, called Desti‐
nationOntology, is an empty ﬁle where we will put the extracted segment.
Towards Classiﬁcation of Web Ontologies
73

For our case, the ontology diagram (classes + individuals) contained in the Sour‐
ceOntology ﬁle is shown in Fig. 3.
The ﬁrst step in the algorithm is the CopyStructure procedure, which copies the
structure of the source ontology into the destination ontology ﬁle. By the structure of
the ontology we mean the namespaces declaration, the ontology headers and the classes
and properties deﬁnitions. This structure, in horizontal segmentation, will be the same
for the source ontology and its sub-ontologies.
Fig. 3. Original ontology diagram (Classes + Individuals)
The next step of the algorithm is the main one. It consists of a loop that, for each
individual in SrcOntology.Individuals list, checks whether the individual fulﬁlls the
condition of ﬁltering or not. This condition may be a combination of conditions
connected by logical operators.
Then, if the current individual fulﬁlls the condition, we add it as an individual to the
sub-ontology with the addTo procedure. At the end of this step, the ontology diagram
(classes + individuals) of the sub-ontology is shown in Fig. 4.
74
R. Nejjahi et al.

Fig. 4. Resultant sub-ontology diagram (Classes + Individuals) before puriﬁcation
The objective of the last stage of the algorithm is the puriﬁcation of the sub-ontology.
The DeleteAssertionsReferringToNonexistentIndividuals procedure will look for
assertions which refer to individuals that have not been selected by the segmentation
algorithm. For example, let us say that the criterion of our segmentation algorithm is
dateOfBirth > ‘01/01/1975’. An individual X of dateOfBirth property equal to
‘16/08/1970’ will not show in the resulting ontology; if we had an individual Y where
dateOfBirth property is equal to ‘03/03/2000’, it will do. The individual Y may have a
relationship with the individual X, so we will end up with a resulting ontology that has
an individual referring to a nonexistent one. After adding all the individuals responding
to the criterion, the algorithm will ﬁlter their Object Properties to maintain only those
referring to individuals existing in the resulting ontology to eliminate undesirable infor‐
mation. By the end, our sub-ontology will be as shown in Fig. 5.
Towards Classiﬁcation of Web Ontologies
75

Fig. 5. Final resultant sub-ontology diagram (Classes + Individuals) after puriﬁcation
4
The Vertical Segmentation Algorithm
In this type of segmentation, we keep untouched the extension of the otology, i.e. the
individuals, and we work on its schema, i.e. the classes and properties. The goal here is
to keep a view on parts of the ontology that may concern the end user. For example, we
can have a great ontology of citizens that contains almost everything about them. A
school may enrich this ontology by information concerning a citizen’s studies, i.e. his
classes, teachers, classmates, subjects, marks, etc. A cultural club may add some
personal information about this citizen, e.g. his hobbies, the books he borrowed, etc. and
a bank will be concerned by his ﬁnancial information.
In spite of duplicate the whole ontology structure in all the sites that will work on it,
which leads to more latency when querying such big ontologies with limited resources
in local machines, we propose to implement in each site a segment of the ontology that
contains only the desired information.
A constraint that should be satisﬁed when segmenting ontologies vertically is to
maintain a link between all the segments, so that we can reassemble them into a whole
ontology. There are two kinds of vertical segmentation. One is based on the selection
of properties and the other is based on the selection of the classes.
If the segmentation process concerns the properties, then we can use the class ID
(<owl:Class rdf:ID = “***” >) which exists in all the segments to reassemble the whole
class deﬁnition. On the other hand, if the segmentation process concerns the classes
76
R. Nejjahi et al.

themselves, then we should keep in each segment an object property between a class of
this segment and another class in another segment, so that we can reconstruct the whole
ontology.
Another kind of vertical segmentation can be that combining both previous kinds,
i.e. a segmentation based on both the classes and the properties.
4.1
Vertical Segmentation Based on Properties Selection
In the vertical segmentation based on the selection of the properties, we apply a ﬁltering
process to eliminate some properties that are considered of no interest. For example, and
end user may be interested in a citizen’s city name but not in its area or its number of
population. A club may be interested in a citizen’s email address for communication
purposes but not in the email’s detailed information such as dateOfCreation, lasAcces‐
sedDate, lastAccessedIPAddress, etc. For such reasons, we will remove these properties
in the extracted sub-ontology.
4.2
Vertical Segmentation Based on Classes Selection
When the process of segmentation is based on the selection of the classes, we will keep
in the sub-ontology only the desired classes. If we will implement the sub-ontologies in
diﬀerent sites with diﬀerent interests, all the sites will not have the same schema of the
ontology. Some of the classes containing the source ontology schema may appear in
many sub-ontologies and even in all sub-ontologies, but other classes may be concerned
by a unique site. If we think that at some point we will reassemble the diﬀerent sub-
ontologies in order to recreate the global ontology, we must keep links in the sub-ontol‐
ogies that will allow them to be reassembled. These links are expressed by the object‐
Properties as in the following case:
<owl:ObjectProperty rdf:ID="hasBankAccount">
<rdfs:domain rdf:resource="#Person" />  
<rdfs:range rdf:resource="#BankAccount" /> 
</owl:ObjectProperty> 
which creates a link between the Person and the BankAccount classes.
4.3
Vertical Segmentation Based on Both Classes and Properties Selection
Let us continue on the same example of ontology seen in Fig. 1. If we want to implement
this ontology in diﬀerent sites with diﬀerent purposes, then we should extract from this
ontology, for each site, a segment containing only the desired information. For example,
Fig. 6 shows the sub-ontology structure extracted from the citizen’s ontology to serve
as an ontology for the schools. A school is supposed not to be interested in one’s relatives
or political tendencies.
Towards Classiﬁcation of Web Ontologies
77

Fig. 6. Ontology segment structure extracted from the Citizen’s ontology to implement in schools
We applied here a double ﬁltering process. First, we removed the classes that will
not be in schools’ interest, then we removed from the remaining classes all the properties
that are considered undesirable for the schools. Tables 1 and 2 illustrate classes in Citizen
and Sub-Citizen ontologies, where Tables 3 and 4 illustrate properties of the Person class
in these ontologies.
Table 1. Classes in Citizen ontology.
Person
Man
Woman
City
Country
Email
BankAccount
School
Club
Party
Table 2. Classes in Sub-citizen ontology
Person
Man
Woman
City
Country
Email
Table 3. Properties of the Person class in Citizen ontology
hasAsFather
hasAsMother
lastName
ﬁrstName
dateOfBirth
livesIn
personAddress
hasEmail
hasBankAccount
studiesIn
isClubMember
IsPartyMember
78
R. Nejjahi et al.

After extracting this sub-ontology, we may populate it with individuals from the
source ontology. This population process should be then puriﬁed by removing references
to individuals that no more exist. Figures 7 and 8 illustrate the sub-ontology before and
after puriﬁcation process.
Fig. 7. Sub-ontology diagram (classes and individuals) before puriﬁcation
Table 4. Properties of the Person class in Sub-citizen ontology
hasAsFather
hasAsMother
lastName
ﬁrstName
dateOfBirth
livesIn
personAddress
hasEmail
Towards Classiﬁcation of Web Ontologies
79

Fig. 8. Sub-ontology diagram (classes and individuals) after puriﬁcation
5
Conclusion
The interest of the segmentation of ontologies is to provide a smaller segment which is
itself an ontology, so can undergo all treatments of ontologies such as reasoning. The
segment has the advantage of the “reduced” size occupied either on disk or on memory,
the reduced time necessary for its transfer on the Internet, as well as the reduced time
taken by the processor for its treatment.
We presented in this paper diﬀerent kinds of segmentation. These kinds can be
divided into two categories, horizontal and vertical segmentation. The latter can also be
considered as two types, based on the properties or the classes. A hybrid vertical
segmentation algorithm may combine both methods by ﬁltering the ontology based on
its classes and its classes’ properties. Also, we may be interested in a full hybrid algo‐
rithm where we will combine all the methods by ﬁrst applying a vertical segmentation
based on the classes and the properties selection, then applying a horizontal segmenta‐
tion based on the properties comparison.
80
R. Nejjahi et al.

References
1. Bremer, J.M., Gertz, M.: Web data indexing through external semantic-carrying annotations.
In: 11th International Workshop on Research Issues in Data Engineering, Proceedings, pp.
69–76 (2001)
2. Fensel, D., Horrocks, I., Van Harmelen, F., McGuinness, D., Patel-Schneider, P.F.: OIL:
ontology infrastructure to enable the semantic web. IEEE Intell. Syst. 16(2), 38–45 (2001)
3. Seidenberg, J., Rector, A.: Web ontology segmentation: analysis, classiﬁcation and use. In:
15th International World Wide Web Conference, Edinburgh, Scotland (2006)
4. Fensel, D.: Ontologies. In: Ontologies - A Silver Bullet for Knowledge Management and
Electronic Commerce, pp. 11–18 (2001)
5. Tulasi, R.L., Rao, M.S., Gouda, G.R.: Study of e-learning information retrieval model based
on ontology. Int. J. Comput. Appl. 61(17), 9–13 (2013)
6. Lembo, D., Santarelli, V., Savo, D.F.: Graph-based ontology classiﬁcation in OWL 2 QL. In:
Extended Semantic Web Conference, pp. 320–334. Springer, Heidelberg (2013)
7. Glimm, B., Horrocks, I., Motik, B., Shearer, R., Stoilos, G.: A novel approach to ontology
classiﬁcation. Web Semant. 14, 84–101 (2012)
8. Seidenberg, J.: Web ontology segmentation: extraction, transformation, evaluation, pp. 211–
243. Springer, Heidelberg (2009)
9. Gherabi, N., Bahaj, M., Addakiri, K.: Mapping relational database into OWL structure with
data semantic preservation. (IJCSIS) Int. J. Comput. Sci. Inf. Secur. 10(1), 42–47 (2012)
10. Gherabi, N., Bahaj, M.: A new method for mapping UML class into OWL ontology. Int. J.
Comput. Appl. 1, 5–9 (2013)
11. Simonyi, A., Szőts, M.: An ontology segmentation tool. Acta Cybernet. 19, 591–605 (2010)
12. Seidenberg, J., Rector, A.: Techniques for segmenting large description logic ontologies. In:
Workshop on Ontology Management: Searching, Selection, Ranking, and Segmentation, 3rd
International Conference on Knowledge Capture (K-Cap), pp. 49–56 (2005)
13. Rector, A., Rogers, J.: Ontological issues in using a description logic to represent medical
concepts: experience from GALEN. In: IMIA WG6 Workshop (1999)
Towards Classiﬁcation of Web Ontologies
81

Model-to-Model Transformation in Approach
by Modeling to Generate a RIA Model
with GWT
Redouane Esbai1(&), Mohammed Erramdani1, Fouad Elotmani2,
and Mohamed Atounti2
1 MATSI Laboratory, Mohammed First University, Oujda, Morocco
es.redouane@gmail.com
2 MASI Laboratory, Mohammed First University, Nador, Morocco
fouad.elotmani@hotmail.com
Abstract. The continuing evolution of business needs and technology makes
Web applications more demanding in terms of development, usability and inter-
activity of their user interfaces. The complexity and diversity of these applications
emerges the need of ﬂexibility and combining operations with existing models to
create other new, more complex models. As more complex models are used, the
importance of transformations between models grows. This paper presents the
application of the MDA (Model Driven Architecture) to generate, from the UML
model, the code following the MVP (Model-View-Presenter), DI (Dependency
Injection) and DAO (Data Access Object) patterns for a RIA (Rich Internet
Application) using the standard MOF 2.0 QVT (Meta-Object Facility 2.0 Query-
View-Transformation) as a transformation language. We adopt GWT (Google
web Toolkit), Spring and Hibernate as a Frameworks for creating a target
meta-model to generate an entire GWT-based N-Layers web application. That is
why we have developed two meta-models handling UML class diagrams and
N-Layers Web applications, then we have to set up transformation rules. The
transformation rules deﬁned in this paper can generate, from the class diagram, an
XML ﬁle containing the Presentation, the Business, and the Data Access package.
This ﬁle can be used to generate the necessary code of a RIA N-Layers web
application.
Keywords: GWT  RIA  Model Transformation  Model View Presenter 
QVT  N-Layers
1
Introduction
In recent years many organizations have begun to consider MDA (Model Driven
Architecture) as an approach to design and implement enterprise applications. The key
principle of MDA is the use of models at different phases of application development
by implementing many transformations. The goal of this transformation is to come to a
technical solution on a chosen platform from independent business models of any
platform [1]. These changes are present in MDA, and help transform a CIM (Com-
putation Independent Model) into a PIM (Platform Independent Model) or to obtain a
PSM (Platform Speciﬁc Model) from a PIM.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_8

RIAs (Rich Internet applications) combine the simplicity of the hypertext paradigm
with the ﬂexibility of desktop interfaces. Moreover, RIA provide a new client-server
architecture that reduces signiﬁcantly network trafﬁc using more intelligent asyn-
chronous requests that send only small blocks of data. In fact, the technological
advances of RIAs require from the developer to represent a rich user interface based on
the composition of Graphical User Interface (GUI) widgets, to deﬁne an event-based
choreography between these widgets and to establish a ﬁne grained communication
between the client and the server layers.
N-Layers (multi-tiers) architecture means splitting up the system into N layers,
where N is a number from 1 and up. N-Layers architecture provides a model for
developers to create a ﬂexible and reusable application. By breaking up an application
into layers, developers have to modify or add a speciﬁc layer only, rather than rewriting
the entire application all over again. In this paper we will focus more on the presen-
tation layer, the business layer and the data access layer.
In a recent work [2], the authors have developed a source and a target meta-model.
The ﬁrst was a PIM meta-model speciﬁc to class diagrams. The second was a PSM
meta-model for MVP (Model-View-Presenter) web applications (particularly GWT),
then they have elaborated transformation rules using the approach by modeling. The
purpose of our contribution is to produce and generate an N-Layers PSM model,
implementing MVP, DI (Dependency Injection) and DAO (Data Access Object) pat-
terns, from the class diagram. In this case, we elaborate a number of transformation
rules using the approach by modeling and MOF 2.0 QVT, as transformation language,
to permit the generation of an XML ﬁle that can be used to produce the required code
of the target application. The advantage of this approach is the bidirectional execution
of transformation rules.
This paper is organized as follows: related works are presented in the second
section, the third section deﬁnes the MDA approach, and the fourth section presents the
N-Layers architecture, the MVP, DI and DAO patterns and its implementation as
frameworks, GWT, Spring and Hibernate in this case. The transformation language
MOF 2.0 QVT and the language of OCL constraints are the subject of the ﬁfth section.
In the sixth section, we present the UML, GWT and N-Layers meta-models. In the
seventh section, we present the transformation rules using QVT operational from UML
source model to the GWT and N-Layers target model. The last section concludes this
paper and presents some perspectives.
2
Related Works
Many researches on MDA and generation of code have been conducted in recent years.
The most relevant are [3–15].
The authors of the work [3] show how to generate JSPs and JavaBeans using the
UWE [4], and the ATL transformation language [5]. Among future works cited, the
authors considered the integration of AJAX into the engineering process of UWE.
Model-to-Model Transformation in Approach
83

Two other works followed the same logic and have been the subject of two works
[6, 7]. A meta-model for Ajax was deﬁned using AndroMDA [16] tool. The generation
of Ajax code has been illustrated by an application CRUD (Create, Read, Update, and
Delete) that manages people.
Meliá, Pérez and Díaz propose in [8] a new approach called OOH4RIA which
proposes a model driven development process that extends OOH methodology. It
introduces new structural and behavioral models in order to represent a complete RIA
and to apply transformations that reduce the effort and accelerate its development. In
another work [9] they present a tool called OIDE (OOH4RIA Integrated Development
Environment) aimed at accelerating the RIAs development through the OOH4RIA
approach which establishes a RIA-speciﬁc model-driven process.
The Web Modeling Language (WebML) [10] is a visual notation for specifying the
structure and navigation of legacy web applications. The notation greatly resembles UML
class and Entity-Relation diagrams. Presentation in WebML is mainly focused on look
and feel and lacks the degree of notation needed for AJAX web user interfaces [11, 12].
This paper aims to ﬁnalize the work presented in [2, 14], by applying the standard
MOF 2.0 QVT to develop the transformation rules aiming at generating the N-Layers
and GWT target model with UI.
3
N-Layers Architecture
N-Layers application architecture provides a model for developers to create a ﬂexible
and reusable application and provides some advantages that are vital to the business
continuity of the enterprise. Typical features of a real life N-Layers may include the
Security, Availability, Scalability, Manageability, Easy Maintenance and Data
Abstraction. To most people, an N-Layers application is anything that is divided into
discrete logical parts. The most common choice is a three-part breakdown presentation,
business logic, and data access although other possibilities exist.
3.1
The Presentation Layer with MVP Pattern
The presentation layer of most applications is often critical to the application’s success.
After all, the presentation layer represents the interface between the user and the
application back-end.
The Model View Presenter is a derivative of the Model View Controller Pattern. Its
aim is to provide a cleaner implementation of the Observer connection between
Application Model and View.
MVP is a user interface architectural pattern engineered to facilitate automated unit
testing and improve the separation of concerns in presentation logic.
Based on this model many frameworks are designed to help developers build the
presentation layer of their user interfaces. In the Java community, many frameworks
that implements MVP pattern have emerged, among them: Mvp4g [20], GWT [21],
Echo2 [22], JFace [23], Vaadin [24], ZK [25], Nucleo .NET [26]. The GWT project is
one of the best examples. Implementing MVP in Google Web Toolkit requires only
that some component implement the view interface.
84
R. Esbai et al.

3.2
The GWT Framework
Google Web Toolkit (GWT) [27] is an open source web development framework that
allows developers to easily create high-performance AJAX applications using Java.
With GWT, you are able to write your front end in Java, and it compiles your source
code into highly optimized, browser-compliant JavaScript and HTML.
However, GWT is not the only framework for managing the user interfaces. Indeed,
other frameworks have been designed for the same goal, but GWT is the most mature.
The main advantage of GWT is the reduced complexity compared to other frameworks
of the same degree of power, for instance, JFace, Flex and Vaadin.
3.3
The Business Layer with Data Transfer Object and Dependency
Injection Patterns
Business logic layer is the Layer of abstraction between the presentation layer and
persistence layer to avoid a strong coupling between these two layers and hide the
complexity of the implementation of business processing to presentation layer. All
business treatments will be implemented by this layer.
In an article written in early 2004, Martin Fowler asked what aspect of control is
being inverted. He concluded that it is the acquisition of dependent objects that is being
inverted. Based on that revelation, he coined a better name for inversion of control:
dependency injection [28].
In other words, Dependency Injection is a worthwhile concept used within appli-
cations that we develop. Not only can it reduce coupling between components, but it
also saves us from writing boilerplate factory creation code over and over again. Many
frameworks that implements DI pattern have emerged, among them: Spring [29],
Symfony dependency injection, Spring.NET [30], EJB [32]. (We have used some
Spring classes in our source meta-model).
3.4
The Persistence Layer with DAO Pattern
This layer is the entry point to the database. All operations required to create, retrieve,
update, and delete data in the database are implemented in the components of this layer.
The Data Access Object (DAO) pattern is now a widely accepted mechanism to
abstract the details of persistence in an application. In practice, it is not always easy to
make our DAO’s fully hidden in the underlying persistence layer.
Many frameworks that implements DAO pattern have emerged, among them:
SpringDao [29], Hibernate, iBatis, NHibernate, EJB [32]. We have used Hibernate in
our work because it is the most used solution within the java community.
4
Approach by Modeling MOF 2.0 QVT
Currently, the models’ transformations can be written according to three approaches:
The approach by Programming, the approach by Template and the approach by
Modeling.
Model-to-Model Transformation in Approach
85

The approach by Modeling is the one used in the present paper. It consists of
applying concepts from model engineering to models’ transformations themselves.
The objective is modeling a transformation, to reach perennial and productive
transformation models, and to express their independence towards the platforms of
execution. Consequently, OMG elaborated a standard transformation language called
MOF 2.0 QVT [31]. The advantage of the approach by modeling is the bidirectional
execution of transformation rules. This aspect is useful for the synchronization, the
consistency and the models reverse engineering.
This work uses the QVT-Operational mappings language implemented by Eclipse
modeling [17].
5
UML, N-Layers and GWT Meta-models
To develop the algorithm of transformation between the source and target model, we
present in this section, the different meta-classes forming the UML source meta-model
and the N-Layers target meta-model. The meta-model source structure simpliﬁed UML
model based on a package containing the data types and classes. These classes contain
properties typed and characterized by multiplicities (upper and lower). The classes
contain operations with typed parameters (see Fig. 1).
Figure 2 illustrates the ﬁrst part of the target meta-model. This meta-model is a
simpliﬁed diagram of relational databases. It consists of several tables, themselves
composed of typed columns.
Fig. 1. Simpliﬁed UML meta-model.
86
R. Esbai et al.

Figure 3 illustrates the second part of the target meta-model. This meta-model
represents a simpliﬁed version of the DAO pattern. It presents the different meta-classes
to express the concept of DAO contained in the DaoPackage:
Figure 4 illustrates the third part of the target meta-model. This meta-model is the
business model of the application to be processed. In our case, we opted for compo-
nents such as DTO and DI pattern. Here, we present the different meta-classes to
express the concept of DI contained in the Business Package.
Fig. 2. Simpliﬁed UML meta-model.
Fig. 3. Simpliﬁed meta-model of DaoPackage.
Model-to-Model Transformation in Approach
87

Figure 5 shows the fourth part of the target meta-model. This meta-model repre-
sents a simpliﬁed version of the MVP pattern. It presents the different meta-classes to
express the concept of MVP implementation:
Like the Abstract Window Toolkit (AWT) and Swing, GWT is based on widgets. To
create a user interface, you instantiate widgets, add them to panels, and then add your
panels to the application’s root panel, which is a top-level container that contains all of the
widgets in a particular view. GWT contains many widgets whose classes are described by
an inheritance hierarchy. An illustration of some of those widgets is shown in Fig. 6.
Fig. 4. Simpliﬁed meta-model of a BusinessPackage.
Fig. 5. The proposed MVP meta-model.
88
R. Esbai et al.

6
The Process of Transforming UML Source Model
to N-Layers GWT Target Model
CRUD operations (Create, Read, Update, and Delete) are most commonly imple-
mented in all systems. That is why we have taken into account in our transformation
rules these types of transactions.
We ﬁrst developed ECORE models corresponding to our source and target
meta-models, and then we implemented the algorithm (see Fig. 8) using the transfor-
mation language QVT Operational Mappings.
To validate our transformation rules, we conducted several tests. For example, we
considered the class diagram (see Fig. 7). After applying the transformation on the UML
model, composed by the class Employee, we generated the target model (see Fig. 9).
Fig. 6. Simpliﬁed meta-model of a GWT widgets.
Fig. 7. UML model.
Model-to-Model Transformation in Approach
89

6.1
The Transformation Rules
By source model, we mean model containing the various classes of our business model.
The elements of this model are primarily classes.
input umlModel:UmlPackage
output crudModel:CrudProjectPackage
begin
create CrudProjectPackage crud
create DaoPackage daoPackage
for each e ∈source model
x = transformationRuleOnePojo(e)
link x to dp
x = transformationRuleOneIDao(e)
link x to dp
x = transformationRuleOneDaoImpl(e)
link x dp
end for
create BusinessPackage bp
for each pojo ∈target model
x = transformationRuleTwoDto(pojo)
link x to bp
end for
for each e ∈source model
x = transformationRuleTwoIService(e)
link x to bp
x = transformationRuleTwoSrviceImpl(e)
link x to bp
end for
create UIPackage uip
create MvpPackage mvpPackage
create ClientPackage clientPackage
create MainApp mainapp
link mainapp to clientPackage
create PresenterPackage presenterPackage
create IPresenter ipresenter
ipresenter.name = 'IPresenter'
ipresenter.methods = declaration of {do,bind}
link ipresenter to presenterPackage
for each e ∈source model
x = transformationRuleThreePresenter(e)
link x to presenterPackage
end for
create ViewPackage viewPackage;
Fig. 8. Main algorithm.
90
R. Esbai et al.

The transformation uses as input a UML type model, named umlModel, and as
output a N-Layers type model named crudModel. The entry point of the transformation
is the main method. This method makes the correspondence between all elements of
type UmlPackage of the input model and the elements of type crudProjectPackage
output model.
The objective of the second part of this code is to transform a UML package into
N-Layers gwt package, by creating the elements of type package ‘Dao’, ‘Business’ and
‘UI’. It is a question of transforming each class of package UML, to IPresenter and
PresenterImpl in the Presenter package, to Display contains widgets in the View
Package, to DTO, IService and ServiceImpl in the Business package, and to Pojo, IDao
and DaoImpl in the Dao package, without forgetting to give names to the different
packages.
6.2
Result
Figure 9 shows the result after applying the transformation rules.
The ﬁrst element in the generated PSM model is UIPackage which includes
MvpPackage that contains gwt.xml ﬁle and Client Package. The Client Package con-
tains the main application, the Presenter Package and the View Package that contains
the Three Views, namely CreateEmployeeView, DisplayEmployeeView and Upda-
teEmployeeView. Since the operation of the removal requires any view, we’ll go to
every view element, which contains a multiple element widget like Panel, ﬁrst-
NameTextBox, lastNameTextBox, actionButton and cancelButton. Since the view
Display contains the DataGrid widget that contains removal button. The Presenter
for each e ∈source model
x=transformationRuleThreeView(e)
link x to viewPackage
end for
create GwtXml gwtxml;
link presenterPackage to clientPackage
link viewPackage to clientPackage
link clientPackage to mvpPackage
link mvpPackage to uip
link gwtxml to uip
link dp to crud
link bp to crud
link uip to crud
create object GWTXML gwtxml;
return crud
end
Fig. 8. (continued)
Model-to-Model Transformation in Approach
91

Fig. 9. Generated PSM N-Layers GWT model.
92
R. Esbai et al.

Package includes one presenter’ interface, one presenter’ implementation that contains
methods with their parameters and their implementations.
The second element in the generated PSM model is businessPackage which
includes one service’ interface, one service’ implementation and one Dto’ object
correspond to the object ‘employee’.
The last element in the generated PSM model is DaoPackage which contains one
Pojo’ object that contains their attributes, one Dao’ interface that contains methods with
their parameters and their implementations.
7
Conclusion and Perspectives
In this paper, we applied the MDA to generate, from the UML class diagram, the code
following the MVP, DI and DAO patterns for a RIA web application.
The purpose of our contribution is to ﬁnalize the works presented in [2, 14]. This
involves developing all meta-classes needed to be able to generate an N-Layers
application respecting a MVP, DI and DAO patterns and then we applied the approach
by modeling and used the MOF 2.0 QVT standard as a transformation language. The
transformation rules deﬁned allow browsing the source model instance class diagram,
and generating, through these rules, an XML ﬁle containing layers of N-Layers
architecture according to our target model. This ﬁle can be used to produce the nec-
essary code of the target application. The algorithm of transformation manages all
CRUD operations. Moreover, it can be re-used with any kind of methods represented in
the UML class diagram.
In the future, this work should be extended to allow the generation of other
components of Web application besides the conﬁguration ﬁles. Afterward we can
consider integrating other frameworks like Flex, JSF and JFace or other execution
platforms like PHP and DotNET.
References
1. Pastor, O., Molina, J.C.: Model-Driven Architecture in Practice: A Software Production
Environment Based on Conceptual Modeling. Springer, New York (2013)
2. Esbai, R., Erramdani, M., Mbarki, S.: Model-driven transformation for GWT with approach
by modeling: from UML model to MVP web applications. Int. Rev. Comput. Softw. (I.RE.
CO.S.) 9(9), 1612–1620 (2014)
3. Koch, N.: Transformations techniques in the model-driven development process of UWE.
In: Proceedings of the 2nd International Workshop Model-Driven Web Engineering, Palo
Alto, p. 3 (2006). ISBN 1-59593-435-9
4. Kraus, A., Knapp, A., Koch, N.: Model-driven generation of web applications in UWE. In:
Proceedings of the 3rd International Workshop on Model-Driven Web Engineering,
CEUR-WS, vol. 261 (2007)
5. Jouault, F., Allilaire, F., Bézivin, J., Kurtev, I.: ATL: a model transformation tool. Sci.
Comput. Program. 72(1–2), 31–39 (2008)
Model-to-Model Transformation in Approach
93

6. Distante, D., Rossi, G., Canfora, G.: Modeling business processes in web applications: an
analysis framework. In: Proceedings of the 22nd Annual ACM Symposium on Applied
Computing, p. 1677 (2007). ISBN 1-59593-480-4
7. Gharavi, V., Mesbah, A., Deursen, A.V.: Modeling and generating AJAX applications: a
model-driven approach. In: Proceedings of the 7th International Workshop on Web-Oriented
Software Technologies, New York, USA, p. 38 (2008). ISBN 978-80-227-2899-7
8. Meliá, S., Gómez, J., Pérez, P., Díaz, O.: A model-driven development for GWT-based rich
Internet applications with OOH4RIA. In: Proceedings of ICWE 2008. Eighth International
Conference on Yorktown Heights, NJ, p. 13 (2008). ISBN 978-0-7695-3261-5
9. Meliá, S., Gómez, J., Pérez, S., Diaz, O.: Facing architectural and technological variability of
rich internet applications. IEEE Internet Comput. 99, 30–38 (2010)
10. Ceri, S., Fraternali, P., Bongio, A.: Web modeling language (WebML): a modeling language
for designing web sites. Comput. Netw. 33(1–6), 137–157 (2000)
11. Preciado, J.C., Linaje, M., Comai, S., Sanchez-Figueroa, F.: Designing rich Internet
applications with web engineering methodologies. In: Proceedings of the 9th IEEE
International Symposium on Web Site Evolution, WSE 2007, p. 23 (2007)
12. Trigueros, M.L., Preciado, J.C., Sánchez-Figueroa, F.: A method for model based design of
rich Internet application interactive user interfaces. In: ICWE 2007: Proceedings of the 7th
International Conference Web Engineering, p. 226 (2007)
13. Nasir, M.H.N.M., Hamid, S.H., Hassan, H.: WebML and.NET architecture for developing
students appointment management system. J. Appl. Sci. 9(8), 1432–1440 (2009)
14. Esbai, R., Erramdani, M., Mbarki, S., Arrassen, I., Meziane, A., Moussaoui, M.:
Model-driven transformation with approach by modeling: from UML to N-layers web
model. Int. J. Comput. Sci. Issues (IJCSI) 8(3) (2011). ISSN (Online) 1694-0814
15. Esbai, R., Erramdani, M., Mbarki, S., Arrassen, I., Meziane, A., Moussaoui, M.:
Transformation by modeling MOF 2.0 QVT: from UML to MVC2 web model.
InfoComp J. Comput. Sci. 10(3), 01–11. ISSN 1807-4545 (2011)
16. AndroMDA (2015). Web site http://www.andromda.org/
17. Eclipse modeling (2015). http://www.eclipse.org/modeling/
18. OMG: Meta Object Facility (MOF), Version 2.5.1, November 2016
19. OMG: XML Metadata Interchange (XMI), version 2.5.1 (2015)
20. Google: Mvp4g a framework to build a GWT application the right way (2013). https://code.
google.com/p/mvp4g/
21. Google: GWT source web site (2015). https://code.google.com/p/google-web-toolkit/
22. Echo2: Echo2 source web site (2014). http://echopoint.sourceforge.net/
23. Harris, R., Warner, R.: The deﬁnitive guide to SWT and JFACE, 1st edn. Apress, Berkeley
(2004)
24. Vaadin: Vaadin Framework web site (2012). https://vaadin.com/home
25. ZK: ZK framework web site (2013). http://www.zkoss.org
26. Nucleo: .NET framework web site (2014). http://nucleo.codeplex.com/
27. Google: GWT project web site (2014). http://www.gwtproject.org/
28. Fowler, M.: Inversion of control containers and the dependency injection pattern (2004).
http://martinfowler.com/articles/injection.html
29. Spring: Spring Source Web Site (2014). http://www.springsource.org/
30. SpringNet: SpringNet Web site (2014). http://www.springframework.net/
31. OMG: Meta Object Facility (MOF) 2.0 Query/View/Transformation (QVT), version 1.3
(2016)
32. Panda, D., Rahman, R., Lane, D.: EJB3 in Action. Manning Co., Greenwich (2007)
94
R. Esbai et al.

An Enhanced Method to Compute the Similarity
Between Concepts of Ontology
Abdelhadi Daoui1(✉), Noreddine Gherabi2, and Abderrahim Marzouk1
1 IR2M Laboratory, FSTS, Hassan 1st University, Settat, Morocco
abdo.daoui@gmail.com, amarzouk2004@yahoo.fr
2 LIPOSI Laboratory, ENSAK, Hassan 1st University, Khouribga, Morocco
gherabi@gmail.com
Abstract. With the use of ontologies in several domains such as semantic web,
information retrieval, artiﬁcial intelligence, the concept of similarity measuring
has become a very important domain of research. Therefore, in the current paper,
we propose our method of similarity measuring which uses the Dijkstra’s algo‐
rithm to deﬁne and compute the shortest path. Then, we use this one to compute
the semantic distance between two concepts deﬁned in the same hierarchy of
ontology. Afterward, we base on this result to compute the semantic similarity.
Finally, we present an experimental comparison between our method and other
methods of similarity measuring.
Keywords: Semantic web · Ontologies · Similarity measuring · Dijkstra’s
algorithm
1
Introduction
Today, ontologies play an important role in many domains related to the semantic Web
[1], information retrieval [2], knowledge engineering [3] and knowledge management
[4]. Therefore, several researches and studies have been developed or are being done to
cover this fertile area. These researches can be used in diﬀerent approaches such as
concepts creation, ontology design [5], classiﬁcation [6], or segmentation [7]. The latter
is useful for the processing of large ontologies, which is diﬃcult to maintain, namely
the addition, modiﬁcation or deletion of large ontology parts.
Our work will focus on the measuring of the semantic similarity between concepts
of ontology. This one is an important concept used in diﬀerent areas of research. Jeﬀrey
Hau, William Lee and John Darlington [8] use the semantic similarity to deﬁne compat‐
ibility between semantic web services [9, 10] annotated by OWL ontologies [11]. In [12]
the authors present a method based on multiple information resources (lexical taxonomy,
corpus…) to measure the semantic similarity between words. The similarity is also used
in the correspondence between the shapes for example, the authors in [13] compute the
similarity between outlines of 2D shapes by using a technique based on the extracting
of the shapes contours which are represented by a set of points, then the authors describe
each segment of this contours by a local and global features, these ones will be coded
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_9

in string of symbols and stored into XML ﬁles On which the similarity calculation will
be executed.
Also, several techniques are proposed to compute the semantic similarity between
ontologies [14, 15]. Where, the authors, in [15] propose a new method to compute the
semantic similarity which is based on three steps. In the ﬁrst the authors compute the
semantic similarity of nodes, and then they compute the semantic similarity of relations
between these nodes, at last they combine these two results to form a uniﬁed value of
semantic similarity.
There are two families of approaches to compute the semantic similarity between
concepts:
1. A family based on computing the geometric distance between concepts to deﬁne
their semantic similarity, where the less distance gives the more similarity [16].
2. A family based on degree of information sharing, more common information
between two concepts means more similarity [8].
The principal idea of our method is deﬁning the shortest path between any node of
a graph (in the current paper the term graph is used to describe ontology) and the root
node. Then, we base on these shortest paths and our formula for computing the rate of
semantic similarity between the concepts of this graph.
This paper is organized as follows: in Sect. 2 we describe our method. The next
section presents an experimental comparison with some other methods of similarity
measuring, followed by a discussion of the changes made to the methodology. Finally,
the Sect. 4 presents our conclusion.
2
Proposed Method
Our method is designed to compute the semantic similarity between two concepts that
exist in the same hierarchy of ontology, where all their nodes are connected by “is-a”
relations type. This method is summarized in the algorithm shown below in Fig. 1.
The ﬁrst step in our method is reserved for weight allocation to the arcs which repre‐
sent the relations between the nodes of the studied ontology (Sect. 2.1). Then, we calcu‐
late the shortest path from the node, which we want to measure its similarity to the root
node (Sect. 2.2). Afterward, Sect. 2.3 is devoted to compute the semantic distance
between the two concepts which we need to measure the similarity between them. Then,
we use this semantic distance to deﬁne the rate of similarity between these two concepts
(Sect. 2.4). Finally, in (Sect. 2.5) we present our global algorithm which summarizes
our proposed method.
96
A. Daoui et al.

Fig. 1. A graphical representation of the proposed algorithm for similarity measuring of ontology
concepts.
2.1
Weight Allocation
In the similarity measuring literature, several methods of weight allocation exist, we
distinguish between those that aﬀect the value of weight to the nodes like the methods
proposed in [17, 18] and the others that allocate the value of weight to the relations (arcs)
between nodes [16]. Our method is designed to compute the semantic similarity rate
between two concepts of ontology whose all their nodes are connected by the relations
of “is-a” type (inheritance type). Therefore, in this paper, we have adjusted one from
the second type of weight allocation methods which allows allocating the weight W (m,
n) computed using the formula 1 to all the arcs in the ontology.
W(m, n) =
[
max(depth(m)) +
N◦(n)
NTNodes(G) + 1 + 1
]−1
(1)
Where, m and n represent two nodes directly connected, max (depth (m)) represents
the maximum depth of the node m (the depth of the root node is equal to 0, 1 for the
An Enhanced Method to Compute the Similarity Between Concepts
97

nodes directly connected to the root node and so on), NTNodes and N° (n) represent
successively the total Number of nodes in graph G and the order number of the node n
between their siblings. And this later (N° (n)) is an integer greater or equal to 0.
We consider the following ontology:
Fig. 2. An example of ontology.
The weights of arcs mentioned in the Fig. 2 are calculated by our formula
(formula 1) of weight allocation. In this formula, we have taked max (depth (m)) for
ensuring that the weight of the current arc is always less than the weights of their previous
arcs. Therefore, the semantic similarity between two concepts more speciﬁc is greater
than two concepts more generalist.
2.2
Shortest Path Deﬁning
The shortest path is a famous problem in the science research domain. Therefore, several
algorithms are proposed such as Dijkstra’s algorithm [19], Bellman-Ford algorithm [20],
Floyd–Warshall algorithm [21], but each one of these is designed to deﬁne the shortest
path under some criteria. In this paper, with our criteria which are: (1) the weight is
strictly greater than zero. (2) The relations between the nodes are “is-a” type.
The best solution adapted to our case is Dijkstra’s algorithm with the use of the
formulas (2) and (3). For this, we have adapted this one with some modiﬁcations that
allow stopping the algorithm once the root node is visited, which is beneﬁcial to us either
98
A. Daoui et al.

at the performance or at the execution time. From our second criterion (The relations
between the nodes are “is-a” type), we can deduce that all the arcs of the graph will be
oriented to the root node. At the level of calculation of the shortest path, from a given
node to the root node, only the nodes that can be considered a generalization of the
current node will be visited and not all the nodes of the graph.
W0[m, n] =
{ 0 if m = n
∞m ≠n
}
(2)
For all 0 ≤k ≤S −1
Wk+1[m, n] = min
{
Wk[m, n]
Wk+1[m, x] + G[x, n]
}
(3)
m, n and x represent three nodes, S represents the set of all nodes of graph. The nodes
x and n are directly connected and Wk [m, n] represents the weight of arc (path) [m, n]
at iteration k,
In our adapted algorithm, for deﬁning the shortest path, we use the formula 2 for
initializing the nodes weight, where we give the value zero to the start node and inﬁnity
to all other nodes in the graph, and we rely on formula 3 to compute the weight of the
shortest path which exists between the start node and the root node.
An Enhanced Method to Compute the Similarity Between Concepts
99

Our adapted algorithm is designed as follows (Fig. 3):
Fig. 3. Our adapted algorithm for shortest path deﬁning.
This algorithm will deﬁne the shortest path between a given node in the ontology
and the root node. In contrast to the Dijkstra’s algorithm, this algorithm will stop the
execution once the root node is visited.
In addition, our algorithm allows not only deﬁning the shortest path, but also deﬁning
the value of its weight. And we can’t deﬁne the shortest path without deﬁning its weight.
For example, we consider the ontology presented in the Fig. 2. To deﬁne the shortest
path for the node H. There are three paths from H to A (the root node): (H, E, FC, A),
(H, G, F, D, FC, A) and (H, G, C, FC, A) (Table 1).
100
A. Daoui et al.

Table 1. An example to deﬁne the weight of shortest path.
A
FC
B
C
D
E
F
G
H
∞
∞
∞
∞
∞
∞
∞
∞
0
∞
∞
∞
∞
∞
0.333
∞
0.2
0
∞
∞
∞
0.533
∞
0.333
0.45
0.2
0
∞
0.768
∞
0.533
∞
0.333
0.45
0.2
0
∞
0.768
∞
0.533
0.783
0.333
0.45
0.2
0
∞
0.768
∞
0.533
0.783
0.333
0.45
0.2
0
1.768
0.768
∞
0.533
0.783
0.333
0.45
0.2
0
Exit
We can easily observe that the shortest path between the node H and the root node
is equal to 1.768.
Table 2. An example to deﬁne the shortest path.
A
FC
B
C
D
E
F
G
H
W0
∅
∅
∅
∅
∅
∅
∅
∅
∅
W1
∅
∅
∅
∅
∅
H
∅
H
∅
W2
∅
∅
∅
G
∅
H
G
H
∅
W3
∅
E
∅
G
∅
H
G
H
∅
W4
∅
E
∅
G
F
H
G
H
∅
W5
∅
E
∅
G
F
H
G
H
∅
W6
FC
E
∅
G
F
H
G
H
∅
From the Table 2, we can observe that:
= FC
FC] = E 
π [A]
π [
π [E] = H
SPath (H, E, FC, A)
Where, SPath (H, E, FC, A) represents the shortest path between the nodes H and A
deﬁned by our adapted algorithm.
2.3
Semantic Distance Computing
At this level, to compute the semantic distance between two concepts we use a new
technique based on the shortest path calculated in the previous phase. This technique
allows eliminating unnecessary parts in the shortest paths and only keeps the necessary
parts to calculate the semantic distance between two concepts.
An Enhanced Method to Compute the Similarity Between Concepts
101

This technique is designed as follows:
We consider the colored segment in the ﬁgure below which represents a segment of
the ontology mentioned in Fig. 2.
Fig. 4. A segment of the ontology mentioned previously.
Where, SPath1 and SParh2 represent the shortest paths between the nodes (H and
B) and the root node. FC represents the ﬁrst common node between these two shortest
paths, if we come from the nodes in question towards the root node. And CSPath repre‐
sents common sub shortest path between the FC node and the root node.
SDis (C1, C2) = W [SPath1] + W [SPath2]−2 ∗W [CSPath]
(4)
Where, C1 and C2 represent two concepts (in our Fig. 4 C1 represents the concept
(B, FC, A) and C2 represents the concept (H, E, FC, A)) and
W
[
SPathi
]
=
k∑
j=1
Wj[m, n]
(5)
m and n represent two nodes directly connected in SPathi and k represents the set of arcs
in SPathi.
The formula 4 allows calculating the semantic distance between any two concepts
exist in the same graph.
102
A. Daoui et al.

2.4
Semantic Similarity Computing
There is an inverse relation between semantic similarity and semantic distance.
Increasing of one among them decrease the other. We can categorize the output of the
semantic similarity function in three categories:
1. The two concepts are the same.
2. Nothing in common between them.
3. There is a rate of semantic similarity between them.
Therefore, the function of semantic similarity should verify these three conditions:
1. ∀(C1, C2) ∈G:0 ≤SSim(C1, C2) ≤1
2. ∀C1 ∈G:SSim(C1, C1) = 1
3. ∀(C1, C2, C3) ∈G:if SDis(C1, C2) > SDis(C1, C3)then
SSim(C1, C2) < SSim(C1, C3)
Where, SSim represents the semantic similarity, SDis represents the semantic
distance and (C1, C2 and C3) represent three concepts of graph G. Ci represents the set
of nodes constituting the shortest path between a given node and the root node.
In this paper, we have used the function of semantic similarity computing proposed
in [16].
SSim(C1, C2) =
1
deg∗SDis(C1, C2) + 1
(6)
C1 and C2 represent two concepts and the parameter “deg” represents the impact
degree of Semantic distance on semantic similarity, and it should be between 0 < deg ≤ 1
(the concrete value of “deg” is deﬁned in the experience).
2.5
Global Algorithm
This section is devoted to our global algorithm that includes all the previous algorithms
and techniques (Fig. 5).
An Enhanced Method to Compute the Similarity Between Concepts
103

Fig. 5. Our global algorithm.
3
Experiments
In this section, by consulting the WordNet, we have got a fragment of ontology hierarchy
shown in Fig. 6, concerning the terms: Vehicle, truck, car, family car, sport car, luxury
car and bus which we want to compare the similarity between them.
Fig. 6. A fragment of ontology hierarchy.
After computation of semantic similarity between these concepts by using our
method (where, we have ﬁxed the parameters deg to 0,4) and two others, we have
obtained the results presented in the following Tables 3, 4, and 5.
In this experimental comparison, each node presents in the following tables such as
Truck, Car and so on, represents a concept which is constituted from the set of all nodes
104
A. Daoui et al.

of the shortest path which exists between this node and the root node. For example the
node “SportCar” presents in these tables represents the concept (SportCar, Car, Vehicle).
Table 3. Application of the ﬁrst method [22].
Vehicle
Truck
Car
Family car
Sport car
Vehicle
1
0
0
0
0
Truck
0
1
0
0
0
Car
0
0
1
0
0
Family car
0
0
0
1
0
Sport car
0
0
0
0
1
Table 4. Application of the second method [16].
Vehicle
Truck
Car
Family car
Sport car
Vehicle
1
0.71
0.71
0.58
0.58
Truck
0.71
1
0.55
0.47
0.47
Car
0.71
0.55
1
0.76
0.76
Family car
0.58
0.47
0.76
1
0.62
Sport car
0.58
0.47
0.76
0.62
1
Table 5. Application of our method.
Vehicle
Truck
Car
Family car
Sport car
Vehicle
1
0.758
0.738
0.648
0.643
Truck
0.758
1
0.597
0.536
0.533
Car
0.738
0.597
1
0.841
0.833
Family car
0.648
0.536
0.841
1
0.72
Sport car
0.643
0.533
0.833
0.72
1
By analyzing of the obtained results of semantic similarity, we observe that the ﬁrst
method [22] can only ﬁnd the total similarity (otherwise, it can only discover the simi‐
larity between the same concepts). The second method [16] can ﬁnd the semantic simi‐
larity between concepts but with a low score. Finally, our method can also calculate the
semantic similarity between concepts but with a high score.
The second method [16] allows aﬀecting the same value of the semantic similarity
between the sibling concepts and their parent concept (for example, the semantic simi‐
larity between the concepts (Vehicle) and (Vehicle, Truck) is equal to 0.71 and the same
value of semantic similarity between (Vehicle) and (Vehicle, Car)), but in the reality the
rate of the semantic similarity between these concepts diﬀers. For this reason, we have
proposed our method which allows avoiding this type of problems by aﬀecting the
diﬀerent values of semantic similarity for similar cases.
Also our method uses a simple function of weight allocation and bases on famous
and robust algorithms with some adaptations to minimize the time of execution.
An Enhanced Method to Compute the Similarity Between Concepts
105

Therefore, this one can calculate the semantic similarity in very short time even in
industrial size ontologies.
4
Conclusion
In this paper, we have presented a new method to calculate the similarity between two
concepts of ontology. Then, we have compared it against other methods that already
exist. The obtained results are very interesting and prove the strength of our method.
We are interested in the future works to adjust this method to support the calculation
of similarity between any two concepts that may exist in the same ontology or not and
be related by any type of relations.
References
1. Lee, T.B., Hendler, J., Lassila, O.: The semantic web. Sci. Am. 284(5), 1–18 (2001)
2. Vallet, D., Fernández, M., Castells, P.: An ontology-based information retrieval model. In:
Gómez-Pérez, A., Euzenat, J. (eds.) ESWC 2005. LNCS, vol. 3532, pp. 455–470. Springer,
Heidelberg (2005)
3. Begam, F., Ganapathy, G.: Knowledge Engineering approach for constructing ontology for
e-learning services. In: IEEE ICACSIS 2011, pp. 125–132 (2011)
4. Brandt, S.C., Morbach, J., Miatidis, M., TheiBen, M., Jarke, M., Marquardt, W.: An ontology-
based approach to knowledge management in design processes. Comput. Chem. Eng.
32(2008), 320–342 (2007)
5. Trumbach, C.C., McKesson, C., Ghandehari, P., DeCan, L., Eslinger, O.: Innovation and
design process ontology. In: IEEE Portland International Conference on Management of
Engineering and Technology, pp. 2121–2132 (2015)
6. Melo, G.D., Siersdorfer, S.: Multilingual text classiﬁcation using ontologies. In: Amati, G.,
Carpineto, C., Romano, G. (eds.) ECIR 2007l. LNCS, vol. 4425, pp. 541–548. Springer,
Heidelberg (2007)
7. Seidenberg, J., Rector, A.: Web ontology segmentation: analysis, classiﬁcation and use. In:
The 15th International Conference on World Wide Web, pp. 13–22. ACM, New York (2006)
8. Hau, J., Lee, W., Darlington, J.: A semantic similarity measure for semantic web services. In:
The 14th International Conference on World Wide Web, Chiba, Japan (2005)
9. Burstein, M., Bussler, C., Zaremba, M., Finin, T., Huhns, M.N., Paolucci, M., Sheth, A.P.,
Williams, S.: A semantic web services architecture. IEEE Internet Comput. 5(9), 52–61 (2005)
10. McIlraith, S.A., Martin, D.L.: Bringing semantics to web services. IEEE Intell. Syst. 1(18),
90–93 (2003)
11. LNCS Homepage. https://www.w3.org/TR/owl2-overview. Accessed 17 Nov 2016
12. Li, Y., Bandar, Z.A., McLean, D.: An approach for measuring semantic similarity between
words using multiple information sources. IEEE Trans. Knowl. Data Eng. 4(15), 871–882
(2003)
13. Gherabi, N., Bahaj, M.: Outline matching of the 2D shapes using extracting XML data. In:
Elmoataz A., et al. (eds.) ICISP 2012. LNCS 7340, pp. 502–512. Springer, Heidelberg (2012)
14. Li, F.: An improved method about the similarity calculation of ontology. In: IEEE
International Conference on Multimedia Technology, pp. 1–4 (2010)
15. Wang, H., Han, X.: Research on similarity of semantic web. In: IEEE International Conference
on Computer Application and System Modeling, pp. 166–169 (2010)
106
A. Daoui et al.

16. Ge, J., Qiu, Y.: Concept similarity matching based on semantic distance. In: IEEE 4th
International Conference on Semantics, Knowledge and Grid, pp. 380–383 (2008)
17. Zhong, J., Zhu, H., Li, J., Yu, Y.: Conceptual graph matching for semantic search. In: Priss
U., Corbett D., Angelova G. (eds.): ICCS 2002, LNAI 2393, pp. 92–106. Springer, Heidelberg
(2002)
18. Ganjisaﬀar, Y., Abolhassani, H., Neshati, M., Jamali, M.: A similarity measure for OWL-S
annotated web services. In: IEEE International Conference on Web Intelligence, pp. 621–624
(2006)
19. Zhan, F.B.: Three fastest shortest path algorithms on real road networks: data structures and
procedures. J. Geogr. Inf. Decis. Anal. 1(1), 69–82 (2001)
20. Ma, Q., Steenkiste, P.: Quality-of-service routing for traﬃc with performance guarantees. In:
The 5th International Workshop on Quality of Service, pp. 115–126. Springer, Berlin (1997)
21. Hougardy, S.: The Floyd-Warshall algorithm on graphs with negative cycles. Inf. Process.
Lett. 110(2010), 279–281 (2010)
22. Giunchiglia, F., Shvaiko, P., Yatskevich, M.: S-Match: an algorithm and an implementation
of semantic matching. In: Davies, J., et al. (eds.): ESWS 2004, LNCS 3053, pp. 61–75.
Springer, Heidelberg (2004)
An Enhanced Method to Compute the Similarity Between Concepts
107

Towards a Completeness Prediction Based
on the Complexity and Impact
Jaouad Maqboul(&) and Bouchaib Bounabat
AL QualSADI Laboratory, ENSIAS Mohammed V University of Rabat,
Rabat, Morocco
jaouad_maqboul@um5.ac.ma, bounabat@ensias.ma
Abstract. As long as companies arrived at a maturity, where business pro-
cesses have stagnated or even the diversity of business processes, research is
directed towards the improvement of data quality, that is to say, improving
dimensions quality for data, the ﬁrst problem is to check if this improvement in
one dimension will be held or not given its cost and its impact, the second to ﬁnd
a centrality to improve total quality without penalizing one dimension our
current paper addresses the ﬁrst problem to a dimension that is completeness.
This improvement will be have a cost that we cannot calculate clearly, but we
can measure her complexity, because the complexity increases with the cost, the
approach here is implemented under a Java EE environment.
Keywords: Business process  Data quality  Completeness  Complexity 
Impact  Java EE  Framework  Prediction  Centrality
1
Introduction
There are three basic concepts to tackle a project we need to know and assess: quality,
cost and time, mostly the cost varies with the quality, this principle applies to business
to business (B2B), Business to customer (B2C), Business to Employee (B2E), and
Business to Governance (B2G). Gartner Inc. estimates that the global market for data
quality tools is about $ 1 billion per year [1].
The nature of business processes varies between automatic and semi-automatic to
manual, these processes manage data from text ﬁles to big data, these data must be
supplemented subsequently by other databases or an expert resource to ensure com-
pleteness of data, actions will of course generate a cost that should not be ignored by
policy makers, on the other hand companies looking to increase customer satisfaction,
turnover, minimize defects, secure data.
As against the companies seek to increase customer satisfaction, ﬁnancial results,
minimize defects and errors, and secure data.
As we know Paperless ofﬁce is a work environment in which the use of paper is
eliminated or greatly reduced [2]. This is done by converting documents into digital
form. Proponents claim that “going paperless” can save money, boost productivity,
save space, make documentation and information sharing easier, keep information
more secure. The concept can be extended to communications outside the ofﬁce as
well.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_10

The contribution of digitization:
1. Facilitate and transform data.
2. Reform and modernize processes.
3. Avoid duplicate entries, treatment and research information and increase efﬁciency.
4. Improve service quality and reliability of data exchanged.
5. Share and archive.
To achieve one of the goals of digitization, we must deﬁne all the business pro-
cesses of the organization.
A business process or business method is a collection of related, structured activities
or tasks that, once completed, will accomplish a speciﬁc organizational goal [3, 4, 5].
As in, organizations improve their business processes to increase quality of them, in
order to optimize the daily process execution time, who is manipulated by human or
material resource, also reduce errors and waste, to make right decisions, and impact all
the strategy so increase user satisfaction and, thereafter increase the ﬁnancial situation
of the organization in the short or medium or even long term.
As business processes are improved to a satisfactory level, it looks like the way
how the processes are executed (automatic, semi-automatic, manual) may cause
non-quality and increase poor-quality, so to control the quality, we can check all these
dimensions which costs time and money, or identify the key processes of the organi-
zation and see later the dimensions that are the most important to our entity. Our
approach is closer to the second approach to evaluate quality for chosen dimension and
calculate cost and impact of this improvement.
We can demonstrate our choice as follows, for the ﬁrst approach, the cost is more
important than the second is
Towards a Completeness Prediction
109

2
Data Quality
2.1
Deﬁnition
There is many deﬁnition of data quality. In the ﬁeld of management, quality is the
degree to which a set of inherent characteristics fulﬁlls requirements [6].
According to:
Philip Crosby: Conformance of requirements of the customer [7].
W. Edwards Deming: A predictable degree of uniformity and dependability, at low
cost and suited to the market [8].
Joseph Juran: ﬁtness for use [9].
We can generalize the deﬁnition of quality given by “the extent to which infor-
mation still meets the requirements and expectations of applicants [10].
2.2
Classiﬁcation of the Quality Dimension
The researchers made a classiﬁcation of data quality, Wang and Strong concludes four
categories: intrinsic, contextual, representation, accessibility and quality of data:
intrinsic, contextual, representational, and accessibility data quality [11].
Intrinsic: denotes that data is linked to the quality that the data has on its own. This
aspect of quality is independent of the user’s perspective and context.
Contextual: points to the requirement that data quality must be considered within the
context of the task at hand. In this group, the quality dimensions are subjective prefer-
ences of the user. Contrary to the ﬁrst group, data quality dimensions cannot be assessed
without considering the users point of view about their use of provided information.
Representational and accessibility: emphasize the importance of the role of sys-
tems. These ﬁndings are consistent with our understanding that high-quality data
should be intrinsically good, contextually appropriate for the task, clearly represented,
and accessible to the data consumer–refers to the quality aspects concerned into
accessing distributed information.
3
Selected Dimension of Our Work
3.1
Deﬁnition of Completeness
The chosen dimension is completeness, may be deﬁned as the extent to which data is
not missing and is of sufﬁcient breadth and depth for the task at hand [12], its mean is
all the requisite information available? Is there data values missing, or in an unusable
state? In some cases, missing data is irrelevant, but when the information that is
missing is critical to a speciﬁc business process, completeness becomes an issue.
The digitization of business processes is a critical step, which has a positive
ﬁnancial impact on the organization, our goal is to improve the quality of processes,
without forgetting that this improvement must have a greater impact than cost, this
approach presupposes a breakdown of the impact and the costs in question that are
related to completeness. These issues have a weight depending on the context in which
110
J. Maqboul and B. Bounabat

the organization belongs and the emphasis on factor, for example a hospital will be
more interested in security than other ﬁnancial factors.
As already pointed out the improvement has a cost associated with building quality
and impact depending on the function that links the cost to the quality resembles the
exponential function the increased cost when quality is required (Fig. 1).
According to the schema, the estimated cost at the beginning is difﬁcult, especially
during the design process. The current study is based on the complexity than the cost
because the complexity is more quantiﬁable than the cost, also the cost increases as the
complexity increases and vice versa.
3.2
Calculate Complexity and Impact
The goal of our work is to predict whether there will be an improvement in com-
pleteness after calculating the impact and complexity of it, the following steps describe
how to precede:
1. Identiﬁcation of business processes and then sort them according to their
importance.
2. Extraction of factors and her attributes related to the completeness.
3. Give weight to each factor.
4. Categorize the factors in two categories: impact and complexity.
5. Calculate sum of the impacts and the sum of the complexities to estimate the ROI if
it’s worth it to improve the completeness of this business process.
Fig. 1. The balance between cost and quality [13]
Towards a Completeness Prediction
111

4
Factor to Deploy in Impact and Complexity
Indeed improving the completeness has shareable positive impacts like other dimen-
sions, the most important:
6. Improvement of daily operations.
7. Cost Savings.
8. Improved Response Times.
9. Improve Security and Compliance.
10. Error Reduction.
11. Proﬁtability and efﬁciency.
12. Making decisions and changing strategy.
13. End user satisfaction.
14. Increase security, compliance.
The propose of our survey is to give to the expert in enterprise a tool that can
indicate him the complexity and impact of improvement of completeness, as
mentioned before these Factor have weight that varies from one sector to another,
from expert to expert, the following table summarizes all (Tables 1 and 2).
For Complexity, selected factors:
15. Type and nature of data.
16. Identify data with great weight
17. Existence of repository to complete data.
Table 1. Table factors of impact
Factor
Choice
Value (V)
Weight (W)
Improved daily operations
True
1
False
0
Improved response times
True
1
False
0
Making decisions and changing
strategy
True
1
False
0
Direct impact over time
Short term
X 1
Average term
X 2
Long term
X 3
Financial impact
increasing revenue
Y 1
Cost Savings
Y2
End user satisfaction
Y 3
Proﬁtability and
efﬁciency
Y 4
Error Reduction
Y 5
Security and compliance
True
1
False
0
112
J. Maqboul and B. Bounabat

Suppose that variable N 2 (X, Y, Z, W, V), how to assign value to N, our approach
is to make a survey designated to experts of quality in companies, Their answers will
help as to assign value for each answer, we have chosen google forms to share this
survey with all experts interested.
Link to complexity survey: https://docs.google.com/forms/d/e/1FAIpQLSdzhK_
AeOFcKHL5U65dAtMzarCMtKxj1IptLscGDiDXRK3WHw/viewform?c=0&w=1
Link to impact survey: https://docs.google.com/forms/d/e/1FAIpQLScILMNp
Xf5gUGbdms8bQOhdjOPOUa4S4t7n4_juYrOsu0Znsg/viewform?c=0&w=1
The aim is to assess improvement, the arithmetic division between the impact and
complexity will give meaning to this improvement.
For each factor, a weight that varies from 0 to n, witch n is a number of answers of
one question, 0 refers to “no impact” and a “Very important impact”, the experts of the
organization that qualify the importance that will be awarded for it.
The global impact is the sum value of answers chosen of question multiplied by the
Table 2. Table factors of complexity
Factor
Choice
Value (V)
Weight (W)
Existence of source of data (repository) that
allows to complement or contradict the
data?
repository
Z 1
expert
Z 2
Articles and
research
academic
Z 3
Data base
Z 4
other
Z 5
Identify attributes of data that have great
weight identiﬁcation in relation to another
data source?
True
1
False
0
Nature of data
Golden Data
V 1
Reporting Data
V 2
Transactional
Data
V 3
Master Data
V 4
Reference Data
V 5
Metadata
V 6
Big Data
V 7
Unstructured
Data
V 8
How process is executed
Automated
W 1
Semi-automated
W 2
Manuel
W3
Towards a Completeness Prediction
113

weight of this question, divided by the sum of all weights, so the ﬁnal expression will be:
I x
ð Þ ¼
PN
n¼1 WimpactnPT
h¼1 Vimpacth


PN
n¼1 Wimpactn
It’s the same for complexity
C x
ð Þ ¼
PN
n¼1 Wcomplexityn  PT
h¼1 Vcomplexityh


PN
n¼1 Wcomplexityn
Vimpacth: All choices made for a given question (multiple choice or one).
Wimpactn: Weight attributed to this question.
The improving completeness or ROI will be ROI = Earnings/Costs.
So our equation of ROI = I(x)/C(x).
If ROI = 1, improving not going to bring anything.
If ROI > 1, it is recommended to improve the completeness
If ROI < 1, improving the quality costs more than it brings to the organization.
Therefore, we can predict the decrease in completeness or otherwise, whether the
impact is more important than the complexity, quality improvement is expected in the
otherwise decrease or stagnation in quality because the cost will be important for the
improvement.
This approach will indicate to the experts whether it is necessary to improve
completeness given the contribution that it will bring.
We can deﬁne a grid of importance to be the impact or complexity.
If ROI = 1 ! strong chance that nothing changes
If 1 < ROI ! strong chance that the quality decreases
If ROI > 1 ! strong chance that, the quality increases.
Framework jee to assess and predict the completeness (Figs. 2 and 3)
Fig. 2. Factors of impact
114
J. Maqboul and B. Bounabat

5
Conclusion
Our study focused on the prediction of the improvement of completeness; it involved
factors related to completeness categorized to impact and complexity.
This approach provides a tool to deﬁne the impact and complexity of the
improvement according to the context of their organization on a balance between the
complexity and the impact, they can plan ahead travels the quality, after we can be
projected on other dimensions but it would be good to target dimensions that have a
strong correlation.
Our next work will focus on the identiﬁcation of those dimensions; these dimen-
sions will be categorized like that:
Big dependency: This group will be presented by one of it, partial dependency and
no dependency.
The work will focus on ﬁnding a centrality between these dimensions in order to
improve all these dimensions without penalizing one of them, this tool will give advice
to experts on the value of the weight to be given to the factors to improve the centrality
between dimensions.
References
1. Earls, A.R.: Better data quality process begins with business processes, not tools. http://
searchdatamanagement.techtarget.com/feature/Better-data-quality-process-begins-with-
business-processes-not-tools
2. Rouse, M.: business process. http://searchcio.techtarget.com/deﬁnition/business-process
3. http://www.appian.com/about-bpm/deﬁnition-of-a-business-process/
4. Kachwala, T.T., Mukherjee, P.N.: Operations Management and Productivity Techniques,
p. 245
5. Crosby, P.B.: Quality is free : The Art of Making Quality Certain, p 39 (1979)
6. Deming, E.W.: Quality. Productivity and Competitive Position. MIT Press, Cambridge
(1982). p. 229
7. Juran, J.M.: Juran on Leadership For Quality, p 15
8. http://www.edureka.co/blog/introduction-to-project-quality-management/
Fig. 3. Complexity screen
Towards a Completeness Prediction
115

9. http://iaidq.org/main/glossary.shtml#I
10. Wang, R.Y., Strong, D.M.: Beyond accuracy: what data quality means to data consumers.
J. Manag. Inf. Syst. 12(4), 5–33 (1996)
11. Pipino, L.L., Lee, Y.W., Wang, R.Y.: Data quality assessment. Commun. ACM 45(4), 211–
218 (2002)
12. The balance between cost and quality. http://emmahurstbusiness.weebly.com/the-balance-
between-cost-quality.html
13. Hvenegaard, A., Arendt, J.N., Street, A., Gyrd-Hansen, D.: Exploring the relationship
between costs and quality—does the joint evaluation of costs and quality alter the ranking of
Danish hospital departments? Eur. J. Health Econ. 12(6), 541–551 (2011)
116
J. Maqboul and B. Bounabat

Toward the Development of a General Semantic
Repository for the Interoperability of Information
Systems Based on Ontological Database
Meryem Fakhouri Amr
(✉), Khalifa Mansouri,
Mohammed Qbadou, and Bouchaib Riyami
Higher Normal School of Technical Education, University Hassan II, Casablanca, Morocco
meryemfakhr@gmail.com, khmansouri@hotmail.com,
qbmedn7@gmail.com, b.riyami@gmail.com
Abstract. The evolution of information systems (IS) has forced and obliges
companies to adopt new strategies of behavior, to change in depth their organi‐
zation and to be more open to their environment. Today, the IS is the pillar of all
businesses enabling the interaction between them in a strategy “winner/winner”.
To deal with these new requirements, the companies are often recourse to the
concept of integration and in more speciﬁc way, to the concept of interoperability.
It is now inescapable to ensure the economic sustainability of the company; it
cannot be ensured using models that will guarantee the sharing not only infor‐
mation but of the semantics of the concepts exchanged. Therefore we are looking
through this article to create a General Semantic Repository which has for objec‐
tive to ﬁx the technical rules to ensure the interoperability between any IS wanting
to develop, process, store or transmit information making the item of exchanges
via a model of interfacing and adaptation that we have already developed based
on the architecture MDA and the mapping to ensure this interoperability, this
model which has for objective the Synchronization of the business process BPMN
of every information system interconnected from the Common Global model
BPEL. This General Semantic Repository which is based on ontological database
will store the semantics of the concepts transmitted between the IS of the inter‐
connected companies. This repository will enrich the model of interfacing and
adaptation for the interoperability of IS that it has already developed.
Keywords: Information system · Interoperability · Semantic · Repository · MDA ·
Mapping · Ontology · Model · BPMN · BPEL
1
Introduction
The evolution and the emergence of information systems in all domains, has led many
companies to oﬀer a variety of services and information, creating a real need of sharing
and interoperability [1]. This requires an infrastructure and models of adaptation and
interfacing to ensure interoperability between several information systems [2]. Unfortu‐
nately, the concept of the ontology which is at the heart of this model is not always
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_11

available [3]. Most of the companies develop a common framework of interoperability
in the form of work which often includes the semantic sector. But IS are designed
diﬀerently and developed independently so the technique is also diﬀerent so it is neces‐
sary to understand when an exchange, to agree on the deﬁnition of common concepts
and possibly of values or of the representation of some components [3].
The major problem encountered by most of the companies is to make inter operate
their systems of information especially when they do not have the same business and
the same ﬁeld of activity have not the same semantics which can be the cause of the
problems of communication and information sharing especially with the evolution of
computer technologies [1].
We have proposed a model for the interoperability of several information systems
in a published article [2]. It is the model that will facilitate communication between
information systems and the synchronization of their business process. We used several
approaches in this model, including the MDA (Model Driven Architecture) approach
and the approach by intermediaries (ontologies). After the transformations, we used the
reverse engineering for the reconstitution of the Business Process Model Global which
will be applied on all the information systems to ensure their synchronization.
Through this new article, we want to enrich this model using a General Semantic
Repository based on an ontology which will be supplied by the relational schema of the
databases of every information system interconnected and by the content of the Global
Common BPEL (Business Process Execution Language). We are going to make a trans‐
formation of the databases of every IS in an ontology which will be capitalized in a
single repository. This new enriched model will ensure the semantic interoperability
between the IS by deﬁning a common language allowing the applications of IS partic‐
ipants to interpret in a homogeneous way the nature and the values of the data transmitted
and to reuse without error or loss of information. Our objective through this article is to
beneﬁt of the heritage “data” existing in the DB (Database) of companies to create an
ontology by applying rules of transformation and to enrich this ontology subsequently
by the content resulting from the common global BPEL between the IS interconnected
to have a single semantic source and a vocabulary common and understandable business.
2
Architecture Layers and Components of the Model
of the Interoperability of Information Systems
We have proposed a model for inter-operate several information systems [2]. This model
is composed of many of layers of transformation and adaptation which will be explained
by the following. The communication is established between N information systems,
and regardless of the nature of parity of N, the model remains valid. Through this model
we want to ensure a coherent communication and transform the interconnected infor‐
mation systems, into a single entity, thanks to this architecture that made call to
approaches based on the models.
118
M. Fakhouri Amr et al.

The proposed model refers to several layers of transformations and adaptation:
• Layer 1: The layer is which consists in making the transformation of business process
BPMN (Business Process Model and Notation), by using a language of ATL (ATLAS
Transformation Language) transformation per the MDA approach, in a language
BPEL executable.
• Layer 2: Elaboration of the common language BPEL from the languages BPEL
business process, by using an ontological database to treat the diﬀerences.
Our choice for the use of ontologies in our model is justiﬁed by the fact that the
ontologies oﬀer a formal description of concepts in a domain, they promote reuse, the
sharing of knowledge and the interoperability between diﬀerent systems by facilitating
the exchange of knowledge. The ontologies allow to link data between them and help
in the formalization of the reasoning [4].
• Layer 3: Concern the update that will be applied to the Business Process BPMN of
each IS to synchronize its processes from the global business model BPMN, by
appealing to the retro engineering (reverse engineering) (Fig. 1).
Fig. 1. Model of interoperability between several information systems [2]
Toward the Development of a General Semantic Repository
119

3
Construction of Ontologies from the DB of Information
Systems Interconnected
The increase of the technology and the rapid development of applications create a need
for sharing and interoperability. Ontologies are crucial for the success of this process of
communication between applications that are registered in the framework of the
Semantic Web [5]. Nevertheless, the construction of the ontologies is a heavy and
expensive task or even its manual construction which is quite long and cumbersome [5].
In fact, there are several types of construction of ontologies and several works are inter‐
ested in the construction of ontology from a database for expressing the semantics. The
choice of ontologies is justiﬁed by their ability to face with problems of heterogeneity
and integration of data [6]. Various approaches and tools dealing with the construction
(or the extraction) of the ontology from the DB have been proposed [6]. The variety of
approaches allow them to move either at the conceptual level with a focus on the
conceptual layer/Logic, essentially the relational model, either at a lower level to search
for matches at the physical level, for example at the level of the languages of deﬁnition/
representation and handling of DB and ontologies OWL (Web Ontology
Language) [7].
Despite this variety, there is a certain similarity in the rules used often based on a
“ﬂat” extraction: the structure of the ontology obtained remains very close to the schema
of the DB. For example, the hierarchical structures extracted are a little deep while the
ontology is distinguished by its hierarchical structure [8]. Today the most appropriate
way to store, search, and manipulate data it is the relational databases knowing that the
structure and the constraints of the integrity of the relational model are deﬁned by
diagrams that are not as expressive as ontologies, in everything that concerns the repre‐
sentation of the semantics of the data. That is why it is essential to build ontologies that
support semantically the information contained in these databases. To achieve this
objective, the technique of reverse engineering, seems to be an interesting solution
because it is deﬁned as a process of analysis of a system that allows the identiﬁcation
of the entities and their links in view of the move of a form of representation to another,
level of abstraction is identical or higher. The problem which we want to face is the limit
of the information extracted from the relational schema for the construction of the
ontology [3]:
• Problem of non-compliance with the rules of normalization to optimize the database
schema
• The diagrams are not always in Third Normal Form
• The non-availability of complete information on the relational database, such as
functional dependencies and inclusion
• The Relational Model does not support all the builders of the conceptual model is
therefore a part of the semantics captured in the conceptual schema is lost during the
transition to the relational schema (example of the inheritance)
• In general, the names of the relationships and attributes of the relational schema are
sometimes abbreviated or ambiguous so it will be diﬃcult to deduce the meaning or
the semantics [3].
120
M. Fakhouri Amr et al.

We want to apply an approach to semi-automatic construction of OWL ontology
based on the analysis of a relational database. We know that each information system
is coupled with a database that allows him to store the data. In our model of interoper‐
ability, we are going to beneﬁt from the existence of the DB with the IS to use these data
to generate a DBO (Ontological Database) who will serve us by the suite for the
synchronization of the Process BPMN trades of each IS. The principle of our model is
that we use an ontological database to handle the diﬀerences between the global BPEL
obtained; this database will be each time enriched by the Global BPEL obtained from
the BPMN of each interconnected system. Our goal is to keep the data used and stored
by the company on a basis to move to an ontological database DBO something which
will help us to save the repeated work at the level of the DBO. In this section, we present
our approach of construction of an ontology, which is based on the idea that the semantics
of the relational database can be extracted by analyzing the diﬀerent components of this
database. This semantics will be used to restructure and enrich the relational schema.
This schema will be the basis for the construction of the ontology from a set of trans‐
formation rules (Fig. 2).
Fig. 2. General process of construction of ontology
Toward the Development of a General Semantic Repository
121

4
Architectures of Ontologies for the Interoperability of IS
We present 3 approaches proposed in research work of architectures of ontologies
speciﬁc to the domain of the integration of data: the approach mono-ontology, the
approach multi-ontology and the hybrid approach, which combines both [9, 10]:
Approach mono-ontology: its principle is to use and share a single unique ontology
between the diﬀerent information systems interconnected. This ontology is regarded as
a language pivot which serves to bind semantically the concepts of a system with the
other concepts [10]. The role of this ontology is to consolidate a common understanding
and shared by diﬀerent IS collaborated. Its advantage is that each one of the intercon‐
nected IS recognizes itself and communicates easily with the others; even its imple‐
mentation is simple, especially when the sources of data are references to similar areas.
The major inconvenient that can be an obstacle behind the use of this type of architecture
is the diﬃculty to expand on a single ontology when the trades of interconnected IS are
too distant [10].
Multi-ontologies: in this type of architecture every information system is described
by its own ontology which requires mechanisms for processing and correspondence
between the diﬀerent ontologies to ensure the link between the diﬀerent concepts
(mapping inter-ontologies) [10]. The implementation of these mechanisms is diﬃcult;
this diﬃculty increases with the number of partners and the distance between their areas.
Keep the integrity and independence of the knowledge of a system of information is the
key beneﬁt of this architecture.
Hybrid approach: Combines the advantages of the two previous approaches, each
source is described by its own ontology and all ontologies sources are linked to a global
ontology to share a vocabulary.
We want through this article to use the hybrid approach for the enrichment of our
model of interoperability; the use of ontologies is justiﬁed mainly by their ability to face
with the problems of heterogeneity and integration of data. Our choice of this hybrid
approach is justiﬁed by the beneﬁts proposed by this approach which will assure us that
all IS can communicate even if the trades of interconnected IS are remote. Each IS is
described by its own ontology and with the assistance of the mechanisms of processing
and correspondence between the diﬀerent ontologies one arrives to implement this
architecture and consolidate these ontologies in only one who will be considered as a
repository general semantics for companies.
The new proposed enriched model allows interoperability of several IS, we contented
with presenting two information systems because if the number of IS increase we follow
the same process described previously. Each IS having its DB which will be the basis
of construction of an ontology which we use afterward to feed the hybrid DBO that will
bring together the content of all the odd of IS interconnected, using also the content of
the global BPEL as a source of enrichment of this hybrid ontological DB who is going
to represent a general semantic repository for all IS interconnected (Fig. 3).
122
M. Fakhouri Amr et al.

Fig. 3. Model of interoperability of IS based on a Semantic Repository by using a DBO
5
The Work Dealing with the Mapping Between Ontology and DB
The treatment of the mapping between ontology and a DB is to dispose of these two
components and require the deﬁnition of a link of correspondence between them. The
result of the mapping allows the resolution of the problems of integration and querying
data. A promising voice has seen the day is part of a new direction since it exploits the
DB and ontologies with their own advantages and disadvantages [6].
Some works that describe this guidance have presented a declarative language R2O
(Relational to Ontology) to specify and describe the correspondences between ontology
[11], described in RDFS (Resource Description Framework Schema) or OWL, and a
DB SQL (Structured Query Language) [6]. The mechanism of this approach is based
on the creation of a document for the description of the mapping with all correspondence
between the elements of the schema of the DB and those of the ontology. To exploit the
content of the document R2O as for example the response to the query users. But, R2O
does not deﬁne degrees of similarity between the components of the DB and the
ontology, which makes the judgment on the existence of links between them are very
rigid [6]. Other works have proposed a process of mapping between a relational schema
and ontology, following 4 phases [6]:
• The realization of a heuristic classiﬁcation of the entities in the relational schema and
those of the ontology to facilitate the search for candidates of correspondence
Toward the Development of a General Semantic Repository
123

• The construction for each candidate entity its own “virtual document”, a collection
of weights derived not only from the description of the entity itself, but also of the
descriptions of its neighbors
• The veriﬁcation and validation of the consistency of the mapping
• The supply of matches discovered by instances in the purpose to be used by a semantic
mapping known under the name “contextual mapping” [6].
The whole of these phases that we plan to follow for the construction and the mapping
of each ontology from each DB of information system working together.
6
The Issues of the Semantic Interoperability in Health
Information Systems
The interoperability of information systems is not limited to any domain, it is required
everywhere saw that all the IS are in the obligation to share, communicate, exchange
their data and information [12]. Therefore, our work which consists to inter operate the
IS will have a positive impact if it will be applied in the ﬁeld of health. Therefore, we
intend in the future in a work consisting on the study of the ﬁeld of health and the
interoperability of any entity of which it is composed [12]. The eﬀectiveness and eﬃ‐
ciency of health information system must be improved if we wish to guarantee care
while respecting the budgetary constraints. The work begins by linking the players who
make up the health system to address the limitations of the isolated exercise, then to
ensure comprehensive support for patients and ensure the continuity of care. The estab‐
lishment of this architecture of interoperability requires a communication infrastructure
which will ensure the connection between the diﬀerent actors of the network of care [12].
The model of interfacing and adaptation that we have been proposed for the interoper‐
ability of the IS, will be applied to health information systems based on ontologies which
will help us to store the semantics of this critical area. This model will help us to create
a terminology repository of health which represents the medical information which will
be shared between the services, hospitals, funding agencies, insurance, etc. Given that
our work is based on the use of ontologies, so it will be the only way adopted in our
approach to create the semantic repository, in the ﬁgure which suite on proposes a model
of ontology in health that we want to adopt to ensure this interoperability and ensure the
continuity of this project. This ontology has for objective to classify a disease and its
treatments thing that medicine has tried to classify for years and that is the expected
objective through this study and this model. These ontologies must represent all concepts
in the ﬁeld of health. In this ﬁgure, one ﬁnds that there is a classiﬁcation of the disease
of appendicitis in the digestive violations and in the inﬂammations, this information will
help doctors to communicate between them and convey many implied information and
it is clear to them that it is a condition of the specialty gastro-entomology with a few
consequences in terms of signs suggestive and gravity [13] (Fig. 4).
124
M. Fakhouri Amr et al.

Fig. 4. The Ontology of classiﬁcation of a disease [13]
7
Conclusion
Through this contribution, we have developed a General Semantic Repository based on
the model of interfacing and adaptation for the interoperability of information systems
that have already been developed in another article. This semantic repository is based
on a hybrid ontological database which combines and capitalizes the contents of all
ontological databases of the interconnected information systems. This new enriched
model, allows each information system which has a DB, to transform it into an ontology
which will be a source of enrichment of the Global Semantic Repository. The DBO of
each system will not be the only source of supply of the hybrid DBO but based also on
the content of the global BPEL obtained after the transformation of BPMN models of
each information system interconnected. For the development of this model, we used
several approaches including the MDA approach and the approach by intermediaries
(ontologies). As perspective, we will apply this new model of interoperability which
includes the general semantic repository in the healthcare sector seen the importance
and the beneﬁts guaranteed of its application in this domain based on the approaches
oriented SOA (Services Oriented Architectures) services see their optimizations and
improvements to the business processes of companies.
Toward the Development of a General Semantic Repository
125

References
1. Kamel, M., Gilles, N.A.: Automatic construction of ontologies from database speciﬁcation.
In: Gandon, F.L. (ed.) IC 2009: Journées Francophones 20C of Knowledge Engineering
“Knowledge and Online Communities”, Hammamet, Tunisia, May 2009, pp. 85–96 (2009).
<hal-00377466>
2. Amr, M.F., Mansouri, K., Qbadou, M.: Towards the elaboration of model of adaptation and
interfacing for the interoperability of several information systems. In: Mediterranean
Congress of Telecommunications, CMT 2016, Tetouan, Morocco, May 2016
3. Benslimane, M., Benslimane, D., Malki, M., Amghar, Y., Gargouri, F.: Building an ontology
from a relational database: led approach analyzing HTML forms. Laboratory LIRIS Claude
Bernard University, INSA, Lyon, France, pp. 4–8
4. Noy, N.F., McGuinness, D.L.: Development of an Ontology: A Guide to Creating Your First
Ontology. Stanford University, Stanford (2001)
5. Gargouri, Y.: Contribution to the maintenance of ontologies from textual analyzes: extraction
of terms and relations between terms. Thesis, Université of Quebec in Montreal (2009)
6. Mahfoudh, M., Jaziri, W.: BD coupling approach and ontology using semantics decision:
contribution to the satisfaction of SQL and SPARQL. Technol. Comput. Sci. 32(7–8), 863–
889 (2013). <Hal-00943250>
7. Cullot, N., Ghawi, R., Yétongnon, K.: Db2owl: a tool for automatic database-to-ontology
mapping. In: Proceedings of the 15th Italian Symposium is Advanced Database Systems, pp.
491–494 (2007)
8. Krivine, S., Nobécourt, J., Soualmia, L., Cerbah, F., Duclos, C.: Automatic ontology
construction from a relational database: application to medicine in the ﬁeld of
pharmacovigilance. In: IC 2009 Acts of 20C Journées Francophones Engineering Knowledge,
Hammamet, Tunisia, 25–29 May 2009
9. Wache, H., Vogele, T., Visser, U., Stuckenschmidt, H.: Ontology-based integration of
information, survey of existing approaches. In: Workshops are Ontologies and Information
Sharing, Intelligent Systems Group, Center for Computing Technologies, University of
Bremen, POB 33 04 40, D-28334 Bremen, Germany (2001)
10. Izza, S.: Integration of industrial information systems, a ﬂexible approach based on semantic
services. Modeling and Simulation. Ecole des Mines de Saint-Etienne (2006). French <NNT:
2006EMSE0025>..<Tel-00780240>
11. Barrasa, J.R., Gómez-Pérez, A.: Upgrading legacy relational data to the semantic web. In:
The 15th International Conference on World Wide Web WWW, Edinburgh, Scotland, UK,
23–26 May 2006, pp. 1069–1070 (2006)
12. Degoulet, P., Fieschi, M., Attali, C.: The challenges of semantic interoperability in health
information systems
13. Charlet, J.: Communication between the computerized patient records: ﬁnd new models. The
University Pierre et Marie Curie, December 2010. http://archives.lesclesdedemain.lemonde.
fr/sante/la-communication-entre-les-dossiers-patient-informatises-trouver-de-nouveaux-mo
deles_a-11-265.html
126
M. Fakhouri Amr et al.

Modeling Aircraft Landing Scheduling in Event B
Abdessamad Jarrar1(✉), Youssef Balouki1, Taouﬁq Gadi1, and Sallami Chougdali2
1 Faculty of Sciences and Technologies, Computing, Imaging and Modeling
of Complex Systems Laboratory, Settat, Morocco
abdessamad.jarrar@gmail.com
2 Signals, Distributed Systems and Artiﬁcial Intelligence Laboratory, ENSET,
Hassan II University, Casablanca, Morocco
Abstract. Aircrafts are the safest means of transport, which requires an excellent
management and development. Whereas, Aircraft landing is one of the most
complex problems in this domain, Due to the huge increase of aircrafts number
in the world.
One of the main challenges to develop landing system is specifying it in the
ﬁrst place. In this paper, we focus on modeling aircrafts scheduling in the Traﬃc
Management Advisor, our approach is based on deadline monotonic algorithm
that shows a lot of advantages compared to the currently used method -First come
First Served-. To ensure a very strong assurance of bug’s absence, we model this
system using a formal method -Event-B- prove that it is correct by construction.
This modeling is done by mean of reﬁnement, invariants preservation proof and
deadlock freedom, whereby the result system, will be correct by construction.
However, the number of proofs that ensure system correctness is huge, therefore
we use a platform called RODIN to minimize the number of proofs done
manually.
Keywords: Formal method · Event-B · Aircraft landing management · Platform
RODIN · Reﬁnement patterns
1
Introduction
The Latest statistics in 2016 show that the number of planes in the world is about 19,800
[1], and this number is increasing every day, this means that the improvement of a new
method to land aircrafts is highly required. The currently used method for this purpose
is FCFS (First Come First Served), this means that the ﬁrst plane arrives to the Traﬃc
Management Advisor (TMA) of the airport is the one which will get the permission to
land ﬁrst. This method has two advantages, the ﬁrst is that it is easy to implement and
it also minimizes the number of aircraft deviations. However, this method has a major
drawback, the aircraft with a low landing speed may aﬀect the landing of others faster
ones, so the global landing duration of all the aircrafts is increased, also FCFS doesn’t
oﬀer more ﬂexibility to air traﬃc controllers. These limitations have encouraged us to
develop a new approach [2] based on real-time scheduling algorithm such as Deadline
Monotonic (DM).
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_12

In this work, we will specify formally the landing process. We model our system
using a formal method. In our case, we will use Event-B for two reasons, the ﬁrst is that
it is based on a reﬁnement method, reﬁnement is creating an abstract model of the system
and then enriching it in successive steps, each step contain more details about the real
system until we get a ﬁnal concrete model that contain all the elements of the system,
this method of reﬁnement make modeling easier than trying to model the whole system
at once. The second reason why we chose to use Event-B is that it is based on a math‐
ematical language this make us able to prove using a mathematical logic that our system
is correct. During reﬁnement, we present proofs to ensure that the abstract model is
correct before reﬁning it to get a more concrete model, the advantage of this method is
that when we get the ﬁnal model we can ensure that our model is correct by construction.
This means we don’t need to test the logic of the system because we are sure that our
system will never have problems, the only kind of test that we can do is to ensure that
the system does what is required.
Our goal in this paper is to specify the process of aircrafts landing while focusing
on security and safety properties using invariant preservation proofs, these proofs are to
ensure the preservation of safety and security properties during system life time before
and after the occurrence of an event. An event is a transition of the system state, which
is a set of variables that describe the current system. Also, we must ensure that the system
will never come to a deadlock state, for this we have additional proofs called deadlock
freedom proofs Table 1.
2
Overview on Formal Methods
The failures in software and hardware in critical systems may lead to the loss of lives
and resources, this has encouraged many scientists to develop methods to reduce the
risk of failures; this is when formal methods can be subject of a handy solution. Formal
methods are a kind of mathematically based techniques for the speciﬁcation [3, 4],
development and veriﬁcation of software and hardware systems [5]. There is a variety
of formal methods:
– Petri nets: Petri nets theory allows a system to be modeled by Petri net which is a
mathematical representation of the system. Analysis of Petri net reveals important
information about the structure and the behavior of the system [6].
– LOTOS: LOTOS is a speciﬁcation language that sees the system as a set of processes
which interact and exchange data with each other and with their environment [7].
– The Z language: a speciﬁcation in Z is predicates [8, 9]. The speciﬁcation of invariants
and the speciﬁcation of operations have the form of predicates.
– B method: The B method is a tool-supported formal method based on an abstract
machine notation, used in the development of computer software [10].
– Event-B: the event-B is formal method for software and system design. This method
is based on reﬁnement and proof obligations, which ensure a strong assurance of
bug’s absence. Therefore, we have chosen this method to develop this system [11].
128
A. Jarrar et al.

The development of systems in event-B is done by mean of reﬁnement, which means
developing an abstract model and then enriching it in successive steps to get more
concrete ones, whereby the last reﬁnement is the most concrete. For each reﬁnement,
context represent the static part of the model where we can deﬁne constants, sets and
axioms; this context can be seen by a machine which is the dynamic part.
Variables values that represent a major component of the machine deﬁne its state.
These states are constrained by invariants to ensure the constancy of the system.
Switching the state require the occurrence of an event. An event is a set of actions, and
each action changes the value of a certain variable. However, all the actions in an event
are occurred simultaneously (no sequencing). Usually, an event requires some condi‐
tions to occur, these conditions are called guards. The ﬁrst event that occur is called
INITIALIZATION event, this event deﬁnes the initial values of all variables and must
not contain any guards.
Proof obligations ensure that the current reﬁnement does not contradict with itself
or with the abstract ones. There are several types of proof obligations, the major ones
are invariants preservation and deadlock freedom. Invariants preservation guarantee the
eligibility of an event to hold invariants; which mean that they remain hold before and
after the event occurrence. Deadlock freedom ensure that no matter what is the situation,
at least one of the events must be able to occur, this mean that at least one of the events
guards must be veriﬁed. These proofs ensure that the system in our hand is correct by
construction; this means that there is no need to test its logic after its construction [11].
3
Representation of the Informal Processes
3.1
Informal Description
In the end of every ﬂight, the airplane returns to the ground, this operation is called
landing. When an aircraft wants to land in an airport, it must ﬂy toward a cylindrical
space called TMA (Traﬃc Management Advisor) which contains in its center the
runway. However, before it would be able to land, it needs permission to do so. This is
why it ﬂies toward a special point in the TMA ﬁrst before landing, this point is called
VOR (Very high frequency Omni Range). When an aircraft arrives to the VOR it
becomes ready to land and it waits for permission to do so. The structure of the TMA
in an airport is illustrated in Fig. 1 below:
Modeling Aircraft Landing Scheduling in Event B
129

Fig. 1. Traﬃc Management Advisor
Our approach is based on Deadline Monotonic (DM), which means that when there
is a set of aircrafts waiting for permission to land, the system give the permission to the
aircraft with the shortest deadline ﬁrst [12–14]. This approach is applied in real-time,
this means that the decision of giving permission is based on the current system state
which improves the eﬃciency of our model.
In the beginning of landing process, we initiate the set of all aircrafts in the TMA and
the set of aircrafts ready to land. After that, the aircraft with the shortest deadline will get
permission to land. During this process, we consider aircrafts which leave the TMA. In a
TMA, an aircraft can be either in a Blocked state when it enters the TMA and still doesn’t
reach the VOR yet, a Ready state when it arrives to the VOR, in an Executed state during
landing, or in a Terminated state at the end of landing process [2, 15].
Our approach can be divided into three steps:
– First Step: Create a void set, and then ﬁll it with all the aircrafts in the TMA.
– Second Step: Eliminating all the aircrafts in our set which terminated their landing.
– Third Step: in this step, the system schedule the set of aircrafts.
More details are presented in a separate paper by Sallami Chougdali [2].
3.2
Requirement Document
The system we are specifying is the one of landing aircrafts, so we will be interested in
the process starting by the moment when an aircraft enters the TMA until the moment
it leaves. Most of the time, the requirement document is either missing or very badly
written, this is why we present our requirement document along 3 axes to ease its use
and to make it more readable. These 3 axes are: The ﬁrst axis expresses the main func‐
tions of system (FUN). The second describes these functions and provides some details
130
A. Jarrar et al.

regarding the environment (ENV), and the last one express some safety properties (SAF)
[11]. We present our requirement document as follows:
FUN 1
The system is controlling the landing of aircrafts in a TMA
The system is able to schedule only a limited number of aircrafts:
ENV 1
There are only a ﬁnitely many aircrafts
The system is able to detect the aircrafts entering or leaving the TMA to process the
system in the real-time:
FUN 2
The system considers aircraft entering or leaving the TMA
For each aircraft, the system considers its deadline and its landing duration:
ENV 2
Every aircraft has a deadline and a landing duration
To be able to schedule a set of aircrafts, the system must be able to locate any aircraft
in this set to know if it is between entry point and the VOR or waiting in the VOR or
between the VOR and the Runway:
ENV 3
The system locates any aircraft in the TMA
Per its location, an aircraft has a state as mentioned before (Blocked, Ready, executed
or terminated):
ENV 4
The system associate to an aircraft one of these states (Blocked, Ready, executed or
terminated)
In addition, an aircraft cannot get landing permission if there is another aircraft in
the runway
SAF 1
the system veriﬁes the availability of the runway before giving permission for landing
For safety purposes, we provide a minimum separation distance between two crafts:
SAF 2
The distance between two aircrafts should be always higher than or equal a certain
distance
Modeling Aircraft Landing Scheduling in Event B
131

The main function of our system is scheduling a set of aircraft using deadline mono‐
tonic algorithm
FUN 3
The proposed system schedules aircrafts basing on deadline monotonic
4
Formal Speciﬁcation of the Proposed Process
4.1
Reﬁnement Strategy
Before starting our speciﬁcation, we must identify our reﬁnement strategy. In our case,
we will have four reﬁnements presented below:
– First reﬁnement: we start our speciﬁcation with a very abstract model, this model
will consider Fun 1, Env 1 and Fun 2. This means that we will model entering and
leaving the TMA of the airport while taking account of the fact that the number of
aircrafts is limited.
– Second reﬁnement: in this reﬁnement, we consider Env 2, Env 3, Env 4 and SAF1.
Which mean that we model the diﬀerent steps of aircraft landing process starting by
entering the TMA and ending by terminating landing and leaving the TMA. We also
take account also of aircrafts deadline lines and states (blocked, ready, executed or
terminated).
– Third reﬁnement: here, we model aircraft landing scheduling basing on deadline
monotonic (Fun 3), and we verify that the minimum distance between aircrafts is
well respected (SAF 2).
4.2
First Reﬁnement: Abstraction of the System Process
As mentioned before, we begin by modeling entering and leaving the traffic manage‐
ment advisor (TMA). In beginning, we define what we call a context, a context specifies
a model’s static part, in terms of sets, constants, and axioms [4]. we define a set of
aircrafts and indicate that contains a limited number of aircrafts. Here is our first context
called c0:
We deﬁned a set of aircrafts (AIRCRAFTS) that contain a ﬁnite number of aircrafts
(axm1). In this part of model, we have formalized Env 1.
132
A. Jarrar et al.

After specifying the static part of our reﬁnement, we present the dynamic part which
we call a machine. In this machine, we start by deﬁning the variable in addition of an
invariant.
The variable TMA_Aircrafts is deﬁned as a set of aircrafts (inv1) that contain all the
aircrafts in the TMA. we formalize the initialization event, it is the event executed at the
beginning, we deﬁne an event as a set of actions that can change the value of a variable.
In our case the initialization event is the following:
In our initial model, we initialize the set of aircrafts in the TMA by a void set
assuming that we don’t have any aircraft in the TMA at the beginning. Beside the initi‐
alization event, we formalize the entering and leaving TMA events.
Modeling Aircraft Landing Scheduling in Event B
133

The entering_TMA event is adding an aircraft to the set of aircrafts in the TMA
(TMA_Aircrafts), but before doing so, the system must ensure that the entering aircraft
is not an element of TMA_Aircraft and it is an element of AIRCRAFTS using guards,
guards is the necessary conditions for an event to be occurred. Besides, we formalize
leaving_TMA as an event able to remove an aircraft from the set of aircrafts in the TMA
under a condition that this aircraft is an element of TMA_Aircrafts.
4.3
Second Reﬁnement: Formalizing Aircrafts Characteristics
In this reﬁnement, we formalize more concretely our system by adding some more
details on aircrafts characteristics (deadline and state). Here is the context of this second
reﬁnement c1:
This context is an extension of the previous one, it deﬁnes a set STATE and 4 vari‐
ables Blocked, Ready, Executed and Terminated that are deﬁning the position of the
aircraft in the TMA, if the aircraft is between the entry point and the VOR, we associate
to it the state Blocked, when an aircraft we associate to it the state ready to mention that
it is ready to land, if it gets landing permission it starts landing and we associate to it the
state Executed, after terminating its landing we associate to it the state Terminated. We
deﬁne STATE as a partition of all the states constants (Blocked, Ready, Executed and
Terminated). We also deﬁne the Deadline as a total function from the set of aircrafts
(AIRCRAFTS) to some natural numbers (ℕ), this means that we can associate to any
aircraft a speciﬁc deadline. We formalize this below:
134
A. Jarrar et al.

In the second machine, we deﬁne a second total function (StateOf) from
TMA_Aircrafts to STATE, which will make us able to associate to any aircraft in the
TMA a certain state.
In addition, we formalize more concretely INITIALISATION, entering_TMA and
leaving_TMA events using characteristics deﬁned above. We begin by formalizing
INITIALISATION event as follows:
During Entering_TMA we associate to the entering aircraft a Blocked state, we
formalize this below:
Before letting an aircraft leave the TMA, it must have ended its landing which means
that its state is Terminated. However, when an aircraft enters the TMA we are sure that
our system is able to let it land while respecting all the deadlines of all the aircrafts.
Modeling Aircraft Landing Scheduling in Event B
135

Moreover, we model more events associated to the various steps of our system
process. We begin by the event executed when an aircraft arrives to the VOR (Very high
frequency Omni Range), this event is presented below:
There are two guards for this event to be occurred, the ﬁrst is that the arriving aircraft
is already in the TMA and the second is that its state is blocked which means it has just
entered the TMA. When this event is executed we associate the concerned aircraft to
the state Ready to indicate that it is ready to land. At this stage the aircraft may get the
permission to land and start landing, we formalize this below:
136
A. Jarrar et al.

To give an aircraft the permission to land, the system must verify that it is already
in the TMA ﬁeld, it is ready to land and that there is no other aircraft in the runway
which means that there is no other aircraft that has a state Executed (grd5). When we
verify that all these conditions are satisﬁed the system give an aircraft the permission to
land by associating to it the state Executed (act1).
Now that the aircraft landed, it may leave the TMA by associating to it the state
Terminated. however, we have required for an aircraft to leave the TMA to be in the
state Terminated. We present the terminating event below:
Before associating to an aircraft, the state Terminated (act1), we verify that it is
already in the TMA ﬁeld (grd1) and its state is Executed (grd2).
4.4
Third Reﬁnement: Scheduling Aircraft Basing on Deadline Monotonic
Scheduling a set of objects is associating to each object a natural number, its order,
basing on a certain criteria. Here, the system schedule aircrafts when they arrive to the
VOR basing on their deadlines, which mean the aircraft with the lowest deadline will
get permission to land ﬁrst. To schedule a set of aircraft we will need a function able to
sort a set of aircrafts in the same order as the order of their deadlines, this function will
return a set of pairs of aircrafts and natural numbers which refer to the order of aircraft,
we call this set of pairs Sorted Set, we call this function Sort. Let A, B, C and D four
aircrafts ready to land, the Sort function may return something like this:
Modeling Aircraft Landing Scheduling in Event B
137

This mean that the C aircraft will get the permission to land ﬁrst, the second is A,
the third is B and lastly D. According to this deﬁnition, we deﬁne our Sort function as
follows:
As mentioned before, the aircrafts are sorted in the same order as the order of their
deadlines. To formalize this, let x and y two aircrafts and let n and m are respectively
their order returned by Sort function and A is the sorted Set of aircrafts, for every x, y,
n, m if n is less than m then the deadline of x is less than the deadline of y.
In addition, we must verify that the order of each aircraft is unique which means that
if x and y are not equal aircraft n and m should not be equal too. This is formalized
below:
We deﬁne one more constant Min_Dist associated to the minimum separation
distance between two aircrafts, we deﬁne it as a positive number. Here is the context c2
of the third reﬁnement that extends c1:
138
A. Jarrar et al.

In our proposed process, the system schedule aircrafts in the VOR, which means that
we need a new variable in machine 2 referring to the set of aircrafts waiting for landing
permission this variable is called Sorted_VOR_Aircrafts. Moreover, we need to deﬁne
a function to determine the distance between two aircrafts, this function is deﬁned from
a pair of aircrafts to a natural number. For safety purpose, the distance between two
aircrafts must be greater or equal to the minimal distance deﬁned previously in context
c2. Obviously, the distance from an aircraft x to another one y is the same as the distance
from y to x (inv 4).
At this stage, we need to initialize our variables while obeying all invariants, to do
so we initialize Sorted_VOR_Aircrafts by a void and the distance by an application
associating the distance Min_Dist to the distance between any two aircrafts (act 4).
Modeling Aircraft Landing Scheduling in Event B
139

Now that we deﬁned INITIALISATION event, we can precede reﬁning events. In
this section, we will not present any new events. However, we will focus on reﬁning
Arrive_To_VOR, Get_Landing_Permission and Landing_Terminated events. When an
aircraft arrive to the VOR, we must reuse the Sort function to sort the new set of aircrafts
waiting for landing to generate a new Sorted_VOR_Aircrafts set using action act2:
At this stage, the system give permission to the aircraft with the lowest order in the
Sorted_VOR_Aircrafts set, for this purpose we need to verify that the order of the aircraft
getting permission is the one with the minimum value of order (grd 3). but ﬁrst, we must
verify that the Sorted_VOR_Aircrafts set is a ﬁnite set because we can’t select a
minimum value of an inﬁnite set (grd 4).
140
A. Jarrar et al.

After landing an aircraft, we need to update the Sorted_VOR_Aircrafts:
5
Proving Model Correctness
As mentioned before, we will use RODIN to prove that our model is correct. We present
below the table of statistics of proofs done by RODIN (Table 1):
Table 1. Rodin report
Element name
Total Auto
Manual
Reviewed
Undischarged
Aircraft Landing v13
30
24
6
0
0
c0
0
0
0
0
0
c1
0
0
0
0
0
c2
1
1
0
0
0
c3
2
0
2
0
0
Machine0
0
0
0
0
0
Machine1
11
11
0
0
0
Machine2
14
12
2
0
0
In this table, we give the total number of proofs done, the number of proofs done
automatically by RODIN and the number of proofs done manually. In our case RODIN
was able to do 80% of proofs (24 out of 30).
6
Conclusion
We have presented a formal speciﬁcation and veriﬁcation of an aircraft landing process
using a formal Method, Event-B in our case. Our model is proved correct by construction
by obligation proof obligations as invariant preservation, reﬁnement and Deadlock
freedom. The process presented in this paper is based on Deadline monotonic, which
means scheduling a set of aircrafts when entering to the VOR (Very high frequency
Modeling Aircraft Landing Scheduling in Event B
141

Omni Range) basing on its deadlines, the aircraft with the lowest deadline will get the
permission to land ﬁrst. In addition, our system is able to verify some safety properties
that must be preserved during system occurrence, this will ensure the safety of our
system.
Our model is based on the current structure of airports that contain only one TMA,
we can improve our system by adding more TMA in the same airport, and when an
aircraft enters the airport, the system negotiate QoS (Quality of Service) to choose the
best TMA for the aircraft that will allow it to land in the minimum duration. We can
also treat the case when an aircraft cannot be landed as mentioned in reﬁnement 5.
References
1. All planes and air traﬃc statistics. http://www.live-counter.com/number-of-aircraft/.
Accessed 05 Apr 2017
2. Chougdali, S.: New air traﬃc management approach based on expert system and using real-
time scheduling algorithms. IJIEI 4(3/4), 305–321 (2016)
3. Métayer, C., Voisin L.: The event-B mathematical language, 26 March 2009
4. Biggs, N.: Discrete Mathematics, 2nd edn. Oxford University Press, New Delhi (2003).
ISBN-10: 0198507178
5. R. W. Butler: What is Formal Methods? Accessed 16 Nov 2006
6. Peterson, J.L.L.: Petri Net Theory and the Modeling of Systems. Prentice Hall PTR, Upper
Saddle River (1981). ACM
7. Bolognesi, T., Brinksma, E.: Introduction to the ISO speciﬁcation language LOTOS. Comput.
Netw. ISDN Syst. 14(1), 25–59 (1987)
8. Spivey Oriel, J.M.: Understanding Z: A Speciﬁcation Language and its Formal Semantics.
Cambridge University Press, New York (1988)
9. Abrial, J.-R.: B#: toward a synthesis between Z and B. In: ZB 2003: Formal Speciﬁcation and
Development in Z and B. Lecture Notes in Computer Science, vol. 2651, pp. 168–177,
27 May 2003
10. Abrial, J.-R.: The B Tool (Abstract). In: Bloomﬁeld, R.E., Marshall, L.S., Jones, R.B. (eds.)
VDM—The Way Ahead, Proceedings of the 2nd VDM-Europe Symposium. Lecture Notes
in Computer Science, vol. 328, pp. 86–87. Springer (1988). ISBN 3-540-50214-9
11. Abrial, J.R.: Modeling in Event-B: System and Software Design. Cambridge University Press,
Cambridge (2008)
12. Audsley, N. C., Burns, A., Richardson, M.F., Wellings, A.F.: Hard real-time scheduling: the
deadline-monotonic approach. In: roceedings of the IEEE Workshop on Real-Time Operating
Systems and Software (1991)
13. Audsley, N.C.: Deadline Monotonic Scheduling. University of York, Heslington (1990)
14. Audsley, N.C.: Real-time scheduling: the deadline-monotonic approach. In: Proceedings of
the IEEE Workshop on Real-Time Operating Systems and Software, pp. 133–137 (1991)
15. IVAO HQ training department: Airspace Structure, Version 1.3, 29 September 2015
142
A. Jarrar et al.

Load Signatures Identiﬁcation Based on Real Power
Fluctuations
El Bouazzaoui Cherraqi
(✉) and Abdelilah Maach
Mohammadia School of Engineering, Mohammed V University, Rabat, Morocco
elbouazzaouicherraqi@research.emi.ac.ma, maach@emi.ac.ma
Abstract. Identiﬁcation of individual sources of energy consumption is one new
research area that contributes to reduce electricity consumption in buildings,
generate energy awareness and improve eﬃciency of available energy resources
usage. This paper proposes a solution based on Multilayer Perceptron and the
change of real power to distinguish and identify appliances from the raw load
signatures directly without features extraction. Our simulation indicates that these
raw load signatures can oﬀer a quick and accurate identiﬁcation.
Keywords: Intrusive load monitoring · Non-intrusive load monitoring ·
Appliances recognition · Energy consumption · Multilayer perceptron
1
Introduction
A further revolution has interested the evolution of the traditional network of the energy
to Smart Grid. A Smart Grid is a smart electrical grid, which includes a variety of
operational, and energy measures including smart meters, smart appliances, renewable
energy resources, and energy eﬃciency resources. The eﬀective of exchange of infor‐
mation make the management of the electricity of this new grid more ﬂexible [1]. This
system allows for monitoring, analysis, control and communication within the supply
chain to help improve eﬃciency, reduce the energy consumption and cost, and maximize
the transparency and reliability of the energy supply chain [2]. One of the major compo‐
nents of smart grid is the residential houses and hence one needs to describe with more
details the proﬁle of the consumed among consumers. Automatic identiﬁcation of home
appliances from their electricity consumption contribute to manage electricity consump‐
tion and describe individual consumption proﬁle, this new technique ﬁnds many appli‐
cations in smart buildings [3]. To reach the load signature of appliances it’s possible to
insert an intermediate monitoring device between the socket and the appliance to collect
the information of energy consumption [4].
The load signature allows identifying appliances from its own electrical behavior.
Indeed, it can be carried out using two kinds of methods namely Intrusive Load Moni‐
toring (ILM) [5] and Non-Intrusive Load Monitoring (NILM) [6]. In the ﬁrst technique
we need one or more meters per appliance. This approach provides more accurate infor‐
mation and details, but still considered inconvenient and expensive for large-scale
deployment. In the second we just need one meter for the whole home in order to detect
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_13

all the appliances, where that today’s technology and the increasing deployment of smart
meters and advanced metering infrastructure (AMI). This last technique is less expensive
and very aﬀordable however it needs more work to make it viable and accurate [6, 7].
This work presents a detailed application of home appliance identiﬁcation based on
energy ﬂuctuations, the particularity of our proposal that we use only the changes of real
power to distinguish between appliances. Three categories of typical appliances are
chosen here represented by their own individual electric load signatures. These signa‐
tures are obtained from the trace database which represents the real power change in
time [2].
The paper is organized as follows. Section 2 presents an overview of appliances load
monitoring. Section 3 describes load identiﬁcation and explain Tracebase used in this
paper. In Sect. 4 the Multilayer Perceptron architecture and its learning rule is presented
for the following work. In Sect. 5 our method adopted for load identiﬁcation, simulation
and results are described. Finally, Sect. 6, we conclude this paper.
2
Related Works
The beginnings researches in appliance identiﬁcation is started by Hart in 1992 [8], he
proposed a load identiﬁcation method that examined the steady-state behavior of loads.
This method performs well. However, it has limitations; for example, it doesn’t work
for small appliances which are always on or non-discrete changes in power. Robertson
in 1996 [9] employed a wavelet transformation technique to classify several unknown
transient behaviors for load identiﬁcation. However, it is expensive and the simultaneous
transient of other loads can obscure the detection of transient behavior. After these two
authors, Cole in 1998 [10, 11] examined a data extraction method and a steady-state
load identiﬁcation algorithm for NILM. Cole’s algorithm can be employed for load
switching between individual appliances when one or more appliances are switched on
or oﬀ. However, it can’t recognize appliance power consumption that does not change.
Nowadays, there are several papers in ﬁeld of appliance identiﬁcation, can we
divided by three categories: new power signature analysis algorithms [12, 13], load
identiﬁcation methods [14–16], and feature selection approaches [2, 17] to recognize
loads and solve classiﬁcation problems. Thanks to its high eﬃciency, Artiﬁcial Neural
Networks techniques used in several recent researches for the load identiﬁcation
methods, for example, Srinivasan in [18] proposed a neural-network-based approach to
identify non-intrusive harmonic source. In this approach, NNs are trained to extract
important features from the input current waveform to uniquely identify various types
of devices using their distinct harmonic “signatures”. Srinivasan conclude that MLP was
found to be the best signature identiﬁcation more than SVM. However, it does not
incorporate the various operating modes of each load and operation under diﬀerent
voltage sources. In [19], Chang have applied particle swarm optimization (PSO) to
optimize the parameters of training algorithms in Neural Network for improving the
recognition accuracy. This method performs well. However, time of training could be
quite long.
144
E.B. Cherraqi and A. Maach

To solve the same of disadvantages for the previously published research, a simple
method for load identiﬁcation in smart home is proposed in this paper. This method uses
Artiﬁcial Neural networks and real power ﬂuctuations to improve the recognition accu‐
racy and to reduce training time.
3
Load Identiﬁcation
3.1
Formal Deﬁnitions of Loading Signature
Load signature is the unique pattern intrinsic to each appliance. It is also defined as the
electrical behavior of individual equipment when it is in operation mode [4]. Similar to
any human’s signature, each electrical device contains unique biometric signature:
current, voltage, phase angle, frequency and power measurement. In our case, we focused
on one main feature is power consumption. We studied active power of each appliance
that being used within the house. Following is the formula of the active power (P):
P =
∑∞
k=0 Pk =
∑∞
k = 0 VkIk cos(𝜑k).
(1)
Here, V and I correspond to the magnitude of the measure of voltage and current respec‐
tively, φ is the phase angle between these two measurements and k coincides with the
harmonic order.
Load signature is the most important information used in the method of Non-Intru‐
sive Load Monitoring. In order to extract out the individual consumption of appliances
[6], we can express the load signature as the evolution of energy consumption in time.
3.2
Tracebase
The Tracebase, used in this paper, is a database of individual load signatures of real
power. It has been made available [2] for research community to compare their algo‐
rithms accuracy. This data set of energy consumption is collected using Plug wise system
[10]. It contains more than 1000 power consumption traces from 3 diﬀerent types of
appliances from an unspeciﬁed number of households and oﬃces in Germany [2],
Tracebase is collected in separate value ﬁles.
The information in each ﬁle includes, among others, the date and time of any value
of real power consumption provided periodically in 1 and 8 s respectively. An example
of such ﬁles is shown in the Table 1 below.
In order to get real power ﬂuctuations of the collected traces, we have visualized a
Refrigerator’s power consumption trace in two degrees of detail in Figs. 1 and 2.
Load Signatures Identiﬁcation Based on Real Power Fluctuations
145

0
200
400
600
800
1000
1200
1
247
493
739
985
1231
1477
1723
1969
2215
2461
2707
2953
3199
3445
3691
3937
4183
4429
4675
4921
5167
5413
Real Power
Time
Fig. 1. Load signature of Refrigerator during 24 h
0
200
400
600
800
1000
1200
1
22
43
64
85
106
127
148
169
190
211
232
253
274
295
316
337
358
379
400
Real Power
Time
Fig. 2. Load signature of Refrigerator during 6 min
Table 1. Example of an individual appliance’s trace, indicating time and real power evolution
Date &time
Power/second
Power/8 s
22/01/2012 18:04:55
130
126
22/01/2012 18:04:56
130
126
22/01/2012 18:04:57
130
126
22/01/2012 18:04:58
130
128
22/01/2012 18:04:59
130
128
22/01/2012 18:04:60
130
128
22/01/2012 18:04:61
128
128
22/01/2012 18:04:62
130
128
22/01/2012 18:04:63
128
128
22/01/2012 18:04:64
136
130
22/01/2012 18:04:65
136
130
146
E.B. Cherraqi and A. Maach

4
Artiﬁcial Neural Networks
4.1
Multilayer Perceptron
Artiﬁcial neural networks ANN is one of the most popular and successful machine
learning solutions [11], it can be implemented easily as a software simulation to model
two properties of the human brain, the ﬁrst property is our ability to learn from examples,
and the second one is to generalize our knowledge for the news and unseen examples.
ANN is used in order to solve complex and hard problems, in a reasonable amount of
time, to overcome the limits of deterministic methods.
To form a neural network, it is essential to connect several neurons to form network.
The proposed architecture is the Multilayer Perceptron. This class of ANN is also called
feed-forward neural networks; it consists of successive layers, where each neuron of the
preceding layer is connected to all the neurons of the next layer. Neurons of the same
layer are not connected to each other and the number of the inputs is equal to the number
of components of each example in the database, the number of neurons in the output
layer is equal to the number of homogenous classes as shown in Fig. 3.
4.2
Backpropagation Algorithm
The second step in neuronal solution, after the choice of the architecture, is to choose
the learning algorithm. In this work we use the well-known Backpropagation algorithm
[11], its major role is to adjust the synaptic weights to converge to the desired outputs,
with labelled database. In this case the learning process is called supervised learning.
Indeed, let, 𝐗= {x1, x2, x3, … , xn} is the training database where n is the total
number of examples available to train the resulting Neural Network. The Backpropa‐
gation gradient consists of the propagation from the input to the output layer, by
producing the sum of the scalar products between each neuron and the connection
weights. The ﬁnal state of the neuron is determined by the continuous and diﬀerentiable
function sigmoid. The operation of Backpropagation algorithm using descent of gradient
can be decomposed into two cases:
Load Signatures Identiﬁcation Based on Real Power Fluctuations
147

Fig. 3. Multilayer Perceptron model
4.2.1
Case of Output Layer
Let i, j, and k three neurons respectively from c1, c2, and c3. This case is the simplest
one, because the locale error observed is giving easily by:
ek (t) = dk(t) −yk(t)
(2)
Where dk(t) and yk(t) denote, respectively, the desired and observed output of the
neuron k in the iteration t. The contribution of the neuron k in the global deﬁned by:
E(t) = 1
2
∑
k
(
dk(t) −yk(t)
)2
(3)
This mean square formula represents the criterion to minimize with iterations of the
Backpropagation algorithm:
yk =
1
1 + e−vk
(4)
vk =
∑
j∈C2 wkjyj∀j ∈Previous layer
(5)
vj is the output observed in neuron k before sigmoid activation. For each wkj the
descent of gradient technique gives:
Δwkj = −η 𝜕E
𝜕wkj
(6)
𝜕E
𝜕wkj
= 𝜕E
𝜕ek
× 𝜕ek
𝜕yk
× 𝜕yk
𝜕vk
× 𝜕vk
𝜕wkj
(7)
148
E.B. Cherraqi and A. Maach

𝜕E
𝜕wkj
= −ekyk ⋅(1 −ykyj)
(8)
Δwkj = −ηδkyj
(9)
To update the weight of the output layer this formula is used:
wkj(t) = wkj(t −1) + ηδkyj(t)
(10)
4.2.2
Case of Hidden Layer
For each neuron j of the hidden layer:
Δwji = −η 𝜕E
𝜕wji
∀i ∈Previous layer
(11)
In this step the local error of the hidden layer is unknown.
vj =
∑
i wijyi
(12)
𝜕E
𝜕wji
= 𝜕E
𝜕yj
×
𝜕yj
𝜕vj
×
𝜕vj
𝜕wji
(13)
The ﬁnal equation of local error in hidden layer is:
𝜕E
𝜕wij
= −
[
yj
(1 −yj
) ∑
k δkwkj
]
× yi
(14)
To update the hidden layer weight we use the following equation:
wji(t) = wji(t −1) + ηδjyi(t)∀i ∈Previous layer
(15)
With
δj = yj
(1 −yj
) ∑
k δkwkj
(16)
All weights in the hidden layers are updated with the same way, by propagating the error
signal 𝛿j from the output to the input layer.
Load Signatures Identiﬁcation Based on Real Power Fluctuations
149

5
Simulation and Results
5.1
Training and Testing
In order to form a training database for our Neural Networks, our proposal approach
consists to divide the signals, recorded in Tracebase, into a several signatures, to build
homogenous classes for the same appliance. The idea of this work is to focus on the
basic ﬂuctuation of energy consumption as patterns. During its operation mode, an
appliance makes a unique change of power. A working period of 6 min of ﬂuctuation
for each appliance is considered as a vector of 400 features which also represents the
input of our Multilayer Perceptron as illustrated in Fig. 4.
Fig. 4. Decomposition to a many signatures for the same appliance to form a class.
In this work, we used the C++ language under a Linux environment to implement
our proposed learning algorithm. It contains two main parts, the ﬁrst one concerns the
training processes using the Backpropagation process, and the second part contains a
test of generalization is done with the same classes of load signatures, but unseen in
learning epochs.
A total of three diﬀerent appliances well selected from the Tracebase to provide
physical training data for Neural Network using BP algorithm: Refrigerator, Washing
machine and TV-CRT. We focused on the change of ﬂuctuations in time which provides
accurate results. The training database contains 303 examples of load signatures for
those three diﬀerent appliances extracted in diﬀerent times as shown in Table 2.
Table 2. The training database of load signatures extracted from Tracebase.
Appliance
Refrigerator
Washing machine
TV-CRT
Number of instances
63
148
132
5.2
Results and Discussion
The last step in our simulation is to test the generalization of our Multilayer Perceptron,
68 unseen signatures, non-available in the training database for the same classes are used
150
E.B. Cherraqi and A. Maach

to prove the leaning accuracy, and the ability of our trained architecture in appliances
identiﬁcation, as it described in the Table 3.
Table 3. The test Database of signatures non available in the training dataset
Appliance
Refrigerator
Washing machine
TV- CRT
Number instances
8
30
30
In order to ﬁnd the best results of identiﬁcation, several numbers of architectures
have been trained for diﬀerent numbers of neurons in the hidden layer. The accurate
model is found by the comparison of training error convergence curves with diﬀerent
number of iterations, this technique allows ﬁnding the best architecture that converges
rapidly. Diﬀerent numbers of neurons in the hidden layers are showed in Fig. 5, which
yields relatively diﬀerent recognition performances with number of neurons in the
hidden layers. The accuracies of appliances identiﬁcation is presented for this applica‐
tion reaches 98% as a best result of classiﬁcation. It should be noted that each structure
and parameters of neural network using BP algorithm for diﬀerent number of hidden
neurons is veriﬁed after several experiments. Moreover, increasing the vectors dimen‐
sion will decrease the iterations, but increase the training time.
93
94
95
96
97
98
99
50
100
200
300
400
500
600
700
Accurracy % 
Number of Neurons
Fig. 5. Accuracy of appliances identiﬁcation
6
Conclusion
In this paper, a Neural Network solution had been presented to solve the problem of
appliances identiﬁcation from load signatures of real power. This process was presented
by using the public database Tracebase of individual load signature. The classiﬁcation
was given by Multilayer Perceptron because of its ability to solve almost all Non-linear
function. The results of this application were satisfying, and encourage this study in
order, to reduce the error of identiﬁcation of residential appliances trough raw load
signatures.
As a future works, we plan to evaluate this process for all possible categories of
residential appliances. To train such big data of signals, we will use the well know
generation of MLP deeplearning.
Load Signatures Identiﬁcation Based on Real Power Fluctuations
151

References
1. Ziefman, M., Roth, K.: Non-intrusive appliance load monitoring: Review and outlook. IEEE
Trans. Consum. Electron. 57(1), 76–84 (2011)
2. Reinhardt, A., Baumann, P., Burgstahler, D., Hollick, M., Chonov, H., Werner, M., Steinmetz,
R.: On the accuracy of appliance identiﬁcation based on distributed load metering data. In:
Sustainable Internet and ICT for Sustainability (SustainIT), pp. 1–9. IEEE (2012)
3. Ridi, A., Hennebert, J.: Hidden Markov models for ILM appliance identiﬁcation. In:
Proceedings of the International Workshop on Enabling ICT for Smart Buildings (ICT-SB
2014) (2014)
4. Liang, J., Ng, S.K.K., Kendall, G., Cheng, J.W.M.: Load signature study—part I: basic
concept, structure, and methodology. IEEE Trans. Power Deliv 25(2), 551–560 (2010)
5. Sultanem, F.: Using appliance signatures for monitoring residential loads at meter panel level.
IEEE PES Winter Meeting, No. 91 WM 0166 PWRD, New York (1991)
6. Esa, N.F., Abdullah, M.P., Hassan, M.Y.: A review disaggregation method in non-intrusive
appliance load monitoring. Renew. Sustain. Energy Rev. 66, 163–173 (2016)
7. Zoha, A., Gluhak, A., Imran, M.A., Rajasegarar, S.: Non-intrusive load monitoring
approaches for disaggregated energy sensing: a survey. Sensors 12(12), 16838–16866 (2012)
8. Hart, G.W.: Non-intrusive appliance load monitoring. Proc. IEEE 80(12), 1870–1891 (1992)
9. Robertson, D.C., Camps, O.I., Mayer, J.S., Gish Sr., W.B.: Wavelets and electromagnetic
power system transients. IEEE Trans. Power Deliv. 11(2), 1050–1056 (1996)
10. Cole, A.I., Albicki, A.: Data extraction for eﬀective non-intrusive identiﬁcation of residential
power loads. In: Proceedings of IEEE Instrumentation and Measurement Technology
Conference, pp. 812–815 (1998)
11. Cole, A.I., Albicki, A.: Algorithm for non-intrusive identiﬁcation of residential appliances.
In: Proceedings of IEEE International Symposium on Circuits and Systems, pp. 338–341
(1998)
12. Bouhouras, A.S., Milioudis, A.N., Labridis, D.P.: Development of distinct load signatures
for higher eﬃciency of NILM algorithms. Electr. Power Syst. Res. 117, 163–171 (2014)
13. Porwik, P., Doroz, R., Orczyk, T.: Signatures veriﬁcation based on PNN classiﬁer optimised
by PSO algorithm. Pattern Recogn. 60, 998–1014 (2016)
14. Lin, S., Zhao, L., Li, F., Liu, Q., Li, D., Fu, Y.: A nonintrusive load identiﬁcation method for
residential applications based on quadratic programming. Electr. Power Syst. Res. 133, 241–
248 (2016)
15. George, D., Swan, L.G.: A method for distinguishing appliance, lighting and plug load proﬁles
from electricity ‘smart meter’ datasets. Energy and Buildings (2016)
16. Belley, C., Gaboury, S., Bouchard, B.: AbdenourBouzouane, An eﬃcient and inexpensive
method for activity recognition within a smart home based on load signatures of appliances.
Pervasive Mob. Comput. 12, 58–78 (2014)
17. Kwak, N., Choi, C.H.: Input feature selection for classiﬁcation problems. IEEE Trans. Neural
Netw. 13(1), 143–159 (2002)
18. Srinivasan, D., Ng, W.S., Liew, A.C.: Neural-network-based signature recognition for
harmonic source identiﬁcation. IEEE Trans. Power Deliv. 21(1), 398–405 (2006)
19. Chang, H.H., Lin, L.S., Chen, N., Lee, W.J.: Particle-swarm-optimization-based nonintrusive
demand monitoring and load identiﬁcation in smart meters. IEEE Trans. Ind. Appl. 49(5),
2229–2236 (2013)
152
E.B. Cherraqi and A. Maach

Networks Security, Wireless and
Network Computing,Telecom and
Transmission Systems, Intrusion
Detection Systems

A New Secure SIP Authentication
Scheme Based on Elliptic Curve Cryptography
Mourade Azrour(&), Mohammed Ouanan, and Yousef Farhaoui
M2I Laboratory, ASIA Team, Department of Computer Science,
Faculty of Sciences and Techniques, Moulay Ismail University,
Errachidia, Morocco
azrour.mourade@gmail.com, ouanan_mohammed@yahoo.fr,
youseffarhaoui@gmail.com,
Abstract. The Session Initiation Protocol (SIP) is an authentication protocol
generally used as a signaling protocol to control communications on the Internet
for establishing, maintaining and terminating sessions between different par-
ticipants. Authentication is the most security service required for SIP. To pro-
vide secure communication, many authentication schemes for SIP have been
proposed. In 2013 Farash et al. proposed an enhanced authenticated key
agreement for SIP. They showed that their protocol is secured against several
attacks. However, in this paper we show that Farash et al.’s protocol suffer from
Denning-Sacco attack and Denial of service attack. To solve the problem, we
propose an improved SIP authentication protocol. The security analysis shows
that the proposed protocol is more secure and can resist to various attacks.
Keywords: Session initiation protocol  Security  Authentication protocol 
Elliptic curve cryptography
1
Introduction
SIP was initiated by the group MMUSIC (Multiparty Multimedia Session Control) in
RFC 2543 [2]; then it was taken over and maintained by the SIP Group of the IETF.
The ﬁrst works were started on 1995, which resulted in a ﬁrst version of SIP with the
publication of RFC 2543 [2] in 1999; then a second version of SIP was published in
2002 to correct some defects of the previous version. This latest version is still effective
through RFC 3261 [3].
SIP is a text-based protocol built on the basis of protocols such as HTTP or
SMTP. The exchanges are in the form of dialogues (peer-to-peer relationships between
agents) that include transactions (request/response). It is a widely used protocol, mainly
for telephony type applications on IP. SIP belonged to the application layer of the OSI
model. It provides mechanisms for opening, maintaining and closing an interactive
session of communication between users.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_14

Authentication is the important security service for SIP. The original SIP authen-
tication protocol is HTTP Digest Authentication. This protocol was found vulnerable to
various attacks. In order to reinforce SIP authentication, a large community has par-
ticipated by proposing different protocols based on various mechanisms.
The remainder of this paper is organized as follows. Section 2 delivers general
information on the architecture and the original SIP authentication protocol. In Sect. 3,
we listed the different related works. In the Sect. 4, we brieﬂy reviewed the Farash
et al.’s SIP authentication protocol. Then, this protocol is analyzed in Sect. 5. The
Sect. 6 presents our proposed protocol. The performance security of our proposed
protocol is given in Sects. 7 and 8. Finally the Sect. 9 concludes the paper.
2
Background on SIP Protocol
2.1
SIP Component
The architecture of SIP consists of a proxy server, redirect server, registrar server,
location server, and user agents. The role of each component is described as follows.
• User Agent Client (UAC): generates SIP requests before they were sent.
• User Agent Server (UAS): generates answers to SIP requests (accepting, refusing,
or redirecting).
• User Agent (UA): it can be a SoftPhone (software) or HardPhone (IP phone). It is
able to generate, send and receive SIP requests. It can act at the same time as a UAC
and UAS.
• Registrar Server: handles the registration of SIP terminals. This is a server that
accepts SIP REGISTER requests.
• Proxy Server: it is a server which is connected to ﬁxed or mobile terminals (UA). It
plays the role of a server and client.
• Redirect Server: it is a server that accepts SIP requests, translates the SIP address of
a destination to IP address and returns them to the client.
• Location Server: It provides the proxy server, redirect server, and register server, it
allows for them to look up or register the location of the user agent
2.2
Authentication HTTP Digest
Authentication HTTP Digest is the ﬁrst authentication protocol proposed by the IETF
(Internet Engineering Task Force) RFC2617 [1]. The messages exchanged between the
server and the clients during authentication procedure are illustrated in Fig. 1 and they
are described as following:
• Step 1. Client ! Server: REQUEST
The client sends a REQUEST to the server.
• Step 2. Server ! Client: CHALLENGE (nonce, realm)
The server generates nonce. Then it sends back CHALLENGE that includes a
nonce and realm to the client.
• Step 3. Client ! Server: RESPONSE (nonce, realm, username, response)
156
M. Azrour et al.

After receiving CHALLENGE from the server the client computes the response by
using received nonce, username, secret password, and realm. response = F(nonce,
username, password, realm). F(.) is a one-way hash function. Next, the client sends
back to the server the message RESPONSE which contains the computed response,
username, nonce and realm.
• Step 4. According to username the server extracts the client’s password. Then the
server veriﬁes whether the nonce is correct or not. If it is correct, the server com-
putes F(nonce, username, password, realm) and uses it to compare it with the
received response. If they match, the server authenticates the identity of the client.
2.3
Elliptic Curve Cryptography
Elliptic curve cryptography (ECC) was introduced by Neal Koblitz in 1987 [9]. ECC
proposed as an alternative to established public-key systems such as DSA and RSA.
ECC has lately received a lot of attention in information security. The main reason for
the attractiveness of ECC is the fact that there is no sub-exponential algorithm known
to solve the discrete logarithm problem on a properly chosen elliptic curve. This means
that ECC uses the keys of small size but offers the same levels of security offered by the
Difﬁe-Hellman key large sizes. Some beneﬁts of having smaller key sizes include faster
computations, and reductions in processing power, storage space and bandwidth. This
makes ECC ideal for constrained environments such as cellular phones and smart
cards.
The elliptic curve is a cubic equation of the form in (1).
E : y2 þ axy þ by ¼ x3 þ cx2 þ dx þ e
ð1Þ
where a, b, c and e are real numbers.
REQUEST
CHALLENGE(nonce, realm)
RESPONSE(nonce, realm, username, 
response)
Client
Server
Generate randomly  nonce
compute  response= F(nonce, 
username, password, realm)
Compute  F(nonce, username, 
password, realm)
And compare it with received 
response
Fig. 1. HTTP digest authentication
A New Secure SIP Authentication Scheme
157

In cryptosystems, the elliptic curve equation is deﬁned as the form in (2) over a prim
ﬁnite ﬁeld Fp, where a; b 2 Fp and 4a3 þ 27b2 6¼ 0 mod p
ð
Þ: Given an integer k ∊Fp
* and a
point P 2 Ep a; b
ð
Þ; the scalar multiplication kP over Ep(a, b) can be computed as in (3).
Ep a; b
ð
Þ : y2 ¼ x3 þ ax þ b mod p
ð
Þ
ð2Þ
kP ¼ P þ P þ . . . þ P
ð
Þ k times
ð
Þ
ð3Þ
Deﬁnition 1. Given two points P and Q over Ep a; b
ð
Þ; the elliptic curve discrete
logarithm problem (ECDLP) is to ﬁnd an integer k ∊Fp
* such as Q = kP.
Deﬁnition 2.
Given three points P, sP and kP over Ep a; b
ð
Þ; for s, k ∊Fp
*, the
computational Difﬁe-Hellman problem (CDHP) is to ﬁnd the point skP over Ep a; b
ð
Þ:
Deﬁnition 3.
Given two points P and Q = sP + kP over Ep a; b
ð
Þ for s, k ∊Fp
*, the
elliptic curve factorization problem (ECFP) is to ﬁnd two points sP and kP over Ep a; b
ð
Þ:
3
Related Works
In 2005, Yang et al. [4] demonstrated that the original SIP authentication protocol does
not provide the necessary security, because it is vulnerable to Off-line password
guessing attack and stolen veriﬁer attack. For this, they proposed a new SIP authen-
tication protocol that is based on the Difﬁe-Hellman Key Exchange [5], which depends
on the difﬁculty of the Discrete logarithm problem. After a comparison with HTTP
Digest Authentication and Encrypted Key Exchange (EKE), they claimed that their
protocol is secure against Off-line password guessing attack, server spooﬁng attack and
replay attack.
However the protocol of Yang et al. [4] needs maintenance and conﬁguration of the
passwords table. In addition, it functions on the discrete logarithm problem which
needs an important computation time. Therefore, it is not desirable for applications
with low memory and limited computing capabilities [6].
In 2006, Huang et al. [6] propose a new protocol based only on hash functions.
Then, they compare the computational complexity of their protocol with the Yang
et al.’s protocol. So they concluded that their protocol is the fastest. Furthermore Jo
et al. [7] demonstrated that the protocol of Yang et al. and the protocol of Huang et al.
are both vulnerable to Off-line password guessing attack.
To overcome this weakness, Durlanik and Sogukpinar [8] based on the Yang et al.’s
protocol to propose another SIP authentication protocol using the Elliptic Curve
Cryptography Difﬁe-Hellman (ECCDH). They demonstrated that their protocol reduces
the computation time if it is compared with Difﬁe-Hillman. In addition, it allows the use
of small size key but offers the same security offered by the Difﬁe-Hellman large key
size. Consequently, this protocol offers an advantage in terms of computing time and in
158
M. Azrour et al.

memory spaces. However Yoon et al. [10, 11] showed that the protocol of Durlanik and
Sogukpinar cannot resist the stolen veriﬁer attack and Denning-Sacco attack.
In 2008, Wu et al. [12] proposed a new SIP authentication and key exchange
protocol based on elliptic curve cryptography (ECC). This protocol provides authen-
tication and exchange of the session key at the same time. Wu et al. demonstrated that
their protocol provides several security services such as conﬁdentiality, integrity,
authentication, access control and perfect forward secrecy. Then, they showed that it is
secure against man-in-the-middle attack, replay attack, Off-line password guessing
attack and server spooﬁng attack. However, this protocol is vulnerable to Off-line
password guessing attack, Denning-Sacco attack and stolen veriﬁer attack [13].
In 2008, Tsai [14] proposed an authentication protocol for SIP based on random
nonce. In this protocol all communication messages are encrypted and decrypted by
one-way hash functions, and a bit-wise exclusive-or (XOR) operation. As result, the
calculation time is reduced when computation cost is compared with the existing pro-
tocols. For this, it is desirable for applications with low computing capacity. However,
Yoon et al. [15], then Arshad et al. [16] found that Tsai’s protocol is vulnerable to
Off-line password guessing attack, server spooﬁng attack and stolen veriﬁer attack. In
addition, it does not provide key exchange secrecy, known-key secrecy and perfect
forward secrecy.
In 2009, Yoon and Yoo [15] proposed a new secure authentication protocol based
on elliptic curve cryptography discrete logarithm problem. They demonstrated that
their protocol is quicker when it is compared with previous proposed protocols.
Moreover, it is secure against the man-in-the-middle attack, Off-line password guessing
attack, replay attack, modiﬁcation attack, Denning-Sacco attack and stolen veriﬁer
attack. In addition, it provides mutual authentication, known key secrecy, session key
secrecy and perfect forward secrecy. However, Liu and Koenig [16] demonstrated that
this protocol is vulnerable to Off-line password guessing attack and partition attack.
In 2010, Yoon et al. [11] based on weakness of Sogukpinar and Durlanik protocol
and on the problems of Wu et al.’s protocol to propose another authentication protocol
based on ECC. However, this protocol is vulnerable to Off-line password guessing
attack and stolen veriﬁer attack [17].
In 2011, Arshad and Ikram [21] demonstrated that Tsai et al.’s protocol is vul-
nerable to Off-line password guessing attack and stolen veriﬁer attack, and it does not
provide key known secrecy and perfect forward secrecy. As result, Arshad and Ikram
presented an authentication protocol for SIP based on ECC. They showed that their
protocol is secure against Off-line password guessing attack, modiﬁcation attack, stolen
veriﬁer attack, server spooﬁng attack and man-in-the-middle attack. Furthermore, it
provides known key secrecy, session key secrecy and perfect forward secrecy. After a
comparison of their protocol with Yang et al.’s protocol and Tsai’s protocol, Arshad
and Ikram concluded that their protocol is the more efﬁcient.
In 2012, Xie [17] showed that the protocol of Yoon and Yoo is insecure against
stolen veriﬁer attack and Off-line password guessing attack. Based on these attacks Xie
proposes a new SIP authentication protocol. Then, he demonstrated that his protocol is
more secure, and it is faster when it compared with existing protocols. However Xie’s
A New Secure SIP Authentication Scheme
159

protocol is shown vulnerable to Off-line password guessing attack and impersonation
attack [18].
In 2012, Tang et al. [19] noted that the protocol introduced by Arshad and Ikram is
not secure against attack Off-line password guessing. In order to deal with this problem,
they suggested another secure and efﬁcient SIP authentication protocol based on
ECDLP. In the same year Sadat et al. [20] show that the Tang et al.’s protocol suffers
from Off-line password guessing attack, registration attack and modiﬁcation attack.
Then, they also introduced a new protocol based on ECDLP.
In 2013, Farash et al. [18] noted that the protocol proposed by Xie is defenseless to
Off-line password guessing attack and to impersonation attack. Therefore they pre-
sented another authentication protocol SIP. Then they demonstrated that their protocol
can resist to Off-line password guessing attack, stolen veriﬁer attack, Denning-Sacco
attack, man-in-the-middle attack and replay attack.
In 2014, Sadat Nik and Shahrab [22] proposed a mutual SIP authentication
Scheme Based on ECC, then they demonstrated that their proposed scheme can resist to
modiﬁcation
attack,
Denning
Sacco
attack,
registration
attack,
replay
attack,
man-in-the-middle attack, and stolen veriﬁer attack. Very recently, Azrour et al. [23]
proposed a new SIP authentication protocols. For more information about the proposed
related protocols please refer to [24].
4
Review of Farash et al.’s Protocol
In this section we brieﬂy review the Farash et al.’s SIP authentication protocol. Farash
et al.’s consists of four phases the system setup phase, registration phase, the authen-
tication phase, and the password change phase. The notations used in this paper are
shown in Table 1.
4.1
System Setup Phase
In the system setup phase, S generates the following system parameters: an Elliptic
Curve E over a ﬁnite ﬁeld GFq, an additive group of points on the Elliptic Curve E
(GFq), the generating point P on E(GFq) of order q and an secure hash function h(.).
S also selects an integer Ks 2 1; q
ð
Þ as the long-live secret key, and computes
Qs = KsP as the corresponding public key. Finally, S publishes the parameters
fðE GFq


; P; q; h :ð Þ; Qsg:
160
M. Azrour et al.

4.2
Registration Phase
In this phase the user communicates with the server through a secure channel as
following.
Reg1: The user U freely chooses his/her username and password PW. Then U sends
them to the server through a secure channel.
Reg2: The server computes VPW = h(username || Ks)  h(username||PW) and
stores (username, VPW) in its database.
4.3
Login and Authentication Phase
In this phase, the user communicates with the remote server through a public channel.
When the user U wants the login into the remote server, her/she performs the following
steps to execute a session of the protocol:
A1: ! S: REQUEST username; aP
f
g
U choose a random integer a 2
1; q
ð
Þ; computes and sends aP in the request
message REQUESTfusername; aPÞ to S.
A2: ! U: CHALLENGEfrealm; bP; rÞ
Upon receiving the request message, S ﬁrstly randomly chooses b 2 1; q
ð
Þ and
computes bP; SKs = baP, and r ¼ hðSKsjjKsaP bp
k
kaPÞ: Then, S sends the challenge
message CHALLENGEfrealm; bP; rÞ back to U:
A3: ! S: RESPONSEfrealm; HÞ
Table 1. The notations
Notation
Description
U
A user
username The identity of user
realm
Is used to prompt the username and password
PW
The password of user U
VPW
The password veriﬁer of user U
S
A remote server
KS
The long-live secret key of server
QS
The long-live public key of server
SK
A session key
h(.)
One-way hash function
EKS (.)
A secure symmetric encryption algorithm
DKS (.)
A secure symmetric decryption algorithm
q
A large prime number
GFq
A ﬁnite ﬁeld with order q
E (GFq)
An elliptic curve group deﬁned over GFq
P
A generator of E(GFq) with order q
∥
The string concatenation operation
⊕
The exclusive-or operation
A New Secure SIP Authentication Scheme
161

Upon receiving the challenge message, U computes SKu ¼ abP and checks if
r ¼ hðSKujjaQs bP
j
j
j
jaPÞ: If so, U computes H = h(realm||SKu||h(username||PW) and
sends RESPONSEfrealm; HÞ back to S and computes the session key SK ¼
hðusernamejjSKu aP
j
j
j
jbPÞ: Otherwise, U rejects it.
A4: Upon receiving the response message, S veriﬁes if hðrealm SKs
k
kfVPW 
hðusernamejjKsÞgÞ ¼ H if so, U is authenticated and S computes the session key
SK ¼ hðusernamejjSKs aP
k
kbPÞ: Otherwise, S aborts.
4.4
Password Changing Phase
In this phase, the user can change freely his/her password through a public channel. To
do so, he/she ﬁrstly needs to execute the login and authentication phase with his/her
username and old password PW After receiving the successful authentication conﬁr-
mation from the server and sharing the session key SK. the user U input the new
password PW as follows:
Chang1: ! S:
PWD; V
f
g
The user U computes PWD ¼ hðSKjjusernameÞ  hðusernamejjPWÞ and V = h
(SK||h(username||PW*)), then sends them to the server.
Chang2: ! U:
Accept; R1
f
gor Reject; R2
f
g
Upon receiving the message PWD and V; the server computes H0
2 ¼ PWD 
hðSK username
k
Þ and checks whether V is equal to h(SK||H2
′ ). If so, the server accepts the
password change request, then computes R1 ¼ h Accept username
j
j
j
jPWD V
j j
j
jSK
ð
Þ and
sends
Accept; R1
f
g back to user. Otherwise, the server rejects the password change
request, then computes R2 ¼ h Reject username
j
j
j
jPWD V
j j
j
jSK
ð
Þ and sends Reject; R2
f
g
back to user. Finally, the server replaces PWD with PWD ¼ hðusername Ks
k
Þ  H0
2.
5
Vulnerability of Farash et al.’s Protocol
In this section we show that Farash et al. SIP authentication protocol is not secure
against Denning Sacco attack and Denial of Service attack (DoS).
5.1
Denial of Service Attack
In the password change phase the server change user’s password without asking about
the old one. So, if the user has not closed her/his session and an attacker accessed to the
user’s application, he can change the user’s password without knowing the old one.
Then, if the password is changed the user can never access to her/his account.
162
M. Azrour et al.

5.2
Denning-Sacco Attack
The Denning-Sacco attack is when User or Server compromises an old session key and
an attacker tries to ﬁnd a long-term private key (e.g. user password or server private
key) or other session keys. This attack arises from the fact that compromising a fresh
session key SK enables the scheme to be compromised. Such attacks have been known
for some time. In Farash et al.’s SIP authentication scheme exactly in password change
phase the Denning-Sacco attack is possible:
1. An attacker records one run of Farash et al.’s SIP authentication scheme and
somehow obtains the old shared session key SK between the SIP client and the
server.
2. When a user U want to change his password, the attacker can obtain easily PWD,
V and Accept. Then attacker can ﬁnd the long-term private password PW by per-
forming the following off-line password guessing attack:
The attacker guesses the secret password PW from the password dictionary D.
The attacker checks if hðSKjjhðusernamejjpw0ÞÞ and V are equal.
If yes, the attacker has guessed the correct secret password PW = PW′.
If
not,
the
attacker
repeats
the
above
veriﬁcation
process
until
hðSKjjh usernamejjpw0Þ
ð
Þ ¼ V is correct.
6
Our Proposed Protocol
In this section, we propose a new secure SIP authentication protocol based on Elliptic
Curve Cryptography, wish consists of four phases: system setup, registration, login and
authentication, and password change. The two ﬁrst phases are the same two ﬁrst phases
of Farash et al.
6.1
Login and Authentication Phase
In this phase, whenever a legal user U wishes to login into the server, the following
steps will be executed between server S and user U: The Fig. 2 shows the steps of this
phase.
Auths1: ! S: REQUEST username; aP; W; T1
f
g
U choose a random number a 2 1; q
ð
Þ and computes aP; then computes V = h
(username||PW) and W ¼ hðVQsjjVÞ: Next U sends message REQUEST username; aP;
f
W; T1g to S. T1 denotes the current timestamp here.
Auths2: ! U: RESPONSE realm; bP; s; T3
f
g
Upon receiving the request message form U at time T2, S veriﬁes validity of
T2 −T1  DT. If is not fresh, S aborts the process, otherwise, S extracts
V0 ¼ VPW  hðusername Ks
k
Þ, then veriﬁes the validity of W ¼
? hðV0KsPjjV0Þ. If it is
true, U is authenticated, and S selects randomly b 2 1; q
ð
Þ; computes bP, SKs = baP,
and
s = h(SKs||h(usename||PW)P||aP||bP).
Finally,
S
sends
message
RESPONSE realm; bP; s; T3
f
g back to U:
A New Secure SIP Authentication Scheme
163

Auths3:
Upon receiving the response message form S at time T4, U veriﬁes validity of T4–
T3  DT. If is not fresh, U stops the process. Otherwise, U computes SKu ¼ abP; and
veriﬁes the validity of s?¼ hðSKu VP
j
j
j
jaPjjbPÞ: If it is true S is authenticated by U. and
the shared session key is
SK ¼ hðusernamej SKu
j
j aP
j
jjbPÞ ¼ hðusernamejjSKs aP
j
j
j
jbPÞ
6.2
Password Changing Phase
This phase is illustrated in the Fig. 3. In this phase, if user U wants to change his
password he/she must ﬁrstly execute the login and authentication phase. Then execute
the following steps.
Fig. 2. Authentication phase of our proposed scheme
164
M. Azrour et al.

Pass1: U ! S
a; NPWD
f
g
U choose freely his/her new password PW*, and select randomly a; b 2R Z
P. Then
computes, V = h(username||PW), = h(username||PW*), tag = h(V||a||username), and
NPWD ¼ EKSðusernamej tag
j
j W
j
jjbÞ: U sends
a; NPWD
f
g to S.
Pass2:S ! U: {Rep1/Rep2}
Upon receiving the message, S decrypt NPWD; computes V’ = hðusernamejjKsÞ 
VPW; and veriﬁes if tag¼
? h V0 aj j
j
jusername
ð
Þ: If it’s true, U computes VPW ¼
h usernamejjKs
ð
Þ  W; replace VPW with VPW* and computes Rep1 = EKS(Accept||a||
b||username). Otherwise U computes Rep2 = EKS(Reject||a||b||username). Finally, S
sends back to user Rep1 or Rep2:
7
Security Analysis
7.1
Mutual Authentication
Mutual authentication means that both the user and server are authenticated to each
other within the same protocol. In the proposed scheme the server can authenticate user
after receiving REQUEST by checking W: Upon receiving message CHALLENGE
user can authenticate the server by testing validity of s: Consequently, the proposed
protocol provides mutual authentication.
Fig. 3. Password change of our proposed scheme
A New Secure SIP Authentication Scheme
165

7.2
Session Key Secrecy
Session key secrecy means that at the end of the key exchange anyone cannot know the
session key excepting the legal communication parties (the user and the server). In the
proposed scheme the session key is computed in this way SK = h(username||SKs||aP||
bP) or SK = h(username||SKu||aP||bP) where SKs ¼ SKu ¼ abP: Since, a and b are
secret, the session key cannot calculate by anyone except the server and the client.
Therefore, our proposed protocol provides session key secrecy.
7.3
Stolen Veriﬁer Attack
In the
proposed protocol,
if
an attacker
steals
VPW ¼ hðusernamejjPWÞ 
hðusernamejjKsÞ from database of server, he/she cannot get a correct password PW;
because he/she must know a server secret key
Ks. As result our proposed scheme is secure against stolen veriﬁer attack.
7.4
Off-Line Password Guessing Attack
Suppose that an attacker eavesdrops the communication between user U and server S,
and gets aP; bP; s; SKs; and W: Where a and b are secret and SKs ¼ abP;
can’t
compute abP from aP and bP because he/she will face Elliptic Curve Cryptography
Difﬁe-Hellman (ECCDH). Then, is not able to verify any password guessing.
7.5
Denning-Sacco Attack
In our proposed protocol, the session key is calculated in this way:
SK ¼ h username SKs
j
j aP
j
j
j
jbP
ð
Þ
or SK = h(username||SKu||aP||bP). If an attacker
obtained this key he/she can’t get
user’s password PW or server’s secret key Ks, because neither PW nor Ks are used to
compute session key.
7.6
Server Spooﬁng Attack
The proposed scheme can resist against server spooﬁng attack. Suppose an attacker
wants to impersonate the server and spoof user U, has to computes
V0 ¼ VPW  hðusernamejjKsÞ
and s ¼ h SKs V0
k
aP
k
bP
k
ð
Þ. However, doesn’t have any information about a server
secret key Ks. Therefore, cannot forge a valid CHALLENGE message.
166
M. Azrour et al.

7.7
Replay Attack
When S receives authentication request, it ﬁrst checks the validity of U’s timestamp. If
time stamp is not valid, then S aborts the session. If an adversary replays a previous
message, S can easily detect it and aborts the session. Similarly,
cannot replay S0s
message as it contains current timestamp. Hence, the propose scheme resists replay
attack.
7.8
Denial of Service Attack
In the proposed protocol, if an attacker try to change user’s password he/she have to
obtain a session key. Even if he/she have it or a legitimate user has not closed her/his
session and
accessed to the user’s application, he/she cannot change the user’s
password without knowing the old one. As result our protocol is secured against replay
attack.
7.9
User Impersonation Attack
Assume that attacker wishes to connect to the server as legitimate user U: has to prove
its validity by forging message REQUEST username; aP; W; T1
ð
Þ: While
need to
know some secret information as PW or a. Therefore,
is not capable to send the
validate message. As result, our scheme can resist user impersonation attack.
8
Comparison and Cost Analysis
The effectiveness of a protocol meant resistance to known attacks and quick execution.
In this section we present the results of the comparison between the authentication
protocols SIP and our proposed protocol, in terms of security and in terms of com-
puting time cost.
8.1
Security Comparison
Table 2 shows the results of the comparison between our protocol and some proposed
SIP authentication protocols. As can be seen, our protocol is more secure if it is
compared with Farash et al.’s protocol. In our, protocol if an attacker has obtained an
old session key and try to execute the proposed Denning-Sacco attack, he have ﬁrstly to
know the encryption algorithm used by the user and server to encrypt and decrypt the
exchanged information. Therefore, our protocol is secured against this attack contrary
to Farash et al.’s which is vulnerable.
A New Secure SIP Authentication Scheme
167

8.2
Computation Time Cost Comparison
The time computation of a protocol is generally dependent on the complexity or
simplicity of the calculation. So to calculate the speed of execution of a protocol, we
must ﬁrst know the execution time of each function used. Then, calculate the number of
times of execution of these functions.
Table 3 shows the result of the comparison between our proposed protocol and
some existing protocols.
9
Conclusion
In this paper, we demonstrated that the protocol proposed by Farash et al. cannot
withstand server denial of service attacks and Denning Sacco attacks. In order to
overcome this weakness we proposed an efﬁcient and secure SIP authentication
scheme. By analyzing our scheme, we show that it is secure against various attacks and
can provide many security services. Then, we conclude that our proposed protocol is
suitable for Telephony over IP applications.
Table 2. Security Comparison
Attacks
Zhang et al. [25] Xie [17] Farash et al. [18] Ours
Stolen veriﬁer
Yes
Yes
Yes
Yes
Denning-sacco
Yes
–
No
Yes
Password guessing Yes
Yes
Yes
Yes
Replay
Yes
Yes
Yes
Yes
Denial of service
–
–
No
Yes
Server spooﬁng
–
No
Yes
Yes
Table 3. Computational comparisons between our protocol and related protocols
[1] [6] [14] [4] [8] [12] [11] [21] [17] [19] [20] [18] Ours
H
2
8
7
8
6
6
5
8
6
7
5
8
8
HP
0
0
0
0
0
0
0
0
0
2
2
0
0
XOR 0
4
3
4
4
4
3
2
0
1
3
1
1
Inv
0
0
0
0
0
0
0
0
1
0
0
0
0
AEC
0
0
0
0
0
0
0
0
1
2
2
0
0
MEC 0
0
0
0
4
4
4
5
6
4
4
6
6
SKE
0
0
0
0
0
0
0
0
1
0
0
0
0
SKD
0
0
0
0
0
0
0
0
1
0
0
0
0
EXP
0
0
0
4
0
0
0
0
0
0
0
0
0
H: one-way hash function; XOR: exclusive-or operation. HP: hash of point
operation. AEC: elliptic curve point addition operation. MEC: elliptic curve
point multiplication operation. SKE: symmetric encryption algorithm. SKD:
symmetric decryption algorithm. EXP: operation exponential. Inv: modular
inversion operation.
168
M. Azrour et al.

References
1. Franks, J., Hallam-Baker, P., Hostetler, J., Lawrence, S., Leach, P., Luotonen, A., Stewart,
L.: HTTP authentication: basic and digest access authentication (1999)
2. Handley, M., Schulzrinne, H., Schooler, E., Rosenberg, J.: SIP: Session Initiation Protocol
(1999)
3. Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston, A., Peterson, J., Sparks, R.,
Handley, M., Schooler, E.: SIP: Session Initiation Protocol (2002)
4. Yang, C.-C., Wang, R.-C., Liu, W.-T.: Secure authentication scheme for session initiation
protocol. Comput. Secur. 24, 381–386 (2005)
5. Difﬁe, W., Hellman, M.: New directions in cryptology. IEEE Trans. Inf. Theory 22(6), 644–
654 (1976)
6. Huang, H., Wei, W., Brown, G.E.: A new efﬁcient authentication scheme for session
initiation protocol. In: Proceedings of the 9th Joint Conference on Information Sciences
(2006)
7. Jo, H., Lee, Y., Kim, M., Kim, S., Won, D.: Of-line password guessing attack to Yang’s and
Huang’s authentication schemes for session initiation protocol. In: Proceedings of the 5th
International Joint Conference on INC, IMS and IDC (NCM 2009), pp. 618–621 (2009)
8. Durlanik, A., Sogukpinar, I.: SIP Authentication Scheme using ECDH. World Enfor-
matikaSocityTransations on Engineering Computing and Technology, vol. 8, pp. 350–353
(2005)
9. Koblitz, N.: Elliptic curve cryptosystems. Math. Comput. 48(177), 203–209 (1987)
10. Yoon, E.J., Yoo, K.Y.: Cryptanalysis of DS-SIP authentication scheme using ECDH. In:
2009 International Conference on New Trends in Information and Service Science, pp. 642–
647 (2009)
11. Yoon, E.-J., Yoo, K.-Y., Kim, C., Hong, Y.-S., Jo, M., Chen, H.-H.: A secure and efﬁcient
SIP authentication scheme for converged VoIP networks. Comput. Commun. 33(14), 1674–
1681 (2010)
12. Wu, L., Zhang, Y., Wang, F.: A new provably secure authentication and key agreement
protocol for SIP using ECC. Comput. Stand. Interfaces 31(2), 286–291 (2009)
13. Yoon, E.J., Yoo, K.Y.: Cryptanalysis of NAKE protocol based on ECC for SIP and its
improvement. In: Second International Conference on Future Generation Communication
and Networking Symposia (2008)
14. Tsai, J.L.: Efﬁcient nonce-based authentication scheme for session initiation protocol. Int.
J. Netw. Secur. 8(3), 312–316 (2009)
15. Yoon, E.J., Yoo, K.Y.: A new authentication scheme for session initiation protocol. In: 2009
International Conference on Complex, Intelligent and Software Intensive Systems, CISIS
2009, pp. 549–554 (2009)
16. Liu, F.W., Koenig, H.: Cryptanalysis of a SIP authentication scheme. In: 12th IFIP
TC6/TC11 International Conference, CMS 2011, Lecture Notes in Computer Science, vol.
7025, pp. 134–143 (2011)
17. Xie, Q.: A new authenticated key agreement for session initiation protocol. Int. J. Commun.
Syst. 25(1), 47–54 (2012)
18. Farash, M.S.: An enhanced authenticated key agreement for session initiation protocol. Inf.
Technol. Control 42(4), 333–342 (2013)
19. Tang, H., Liu, X.: Cryptanalysis of Arshad et al.’s ECC-based mutual authentication scheme
for session initiation protocol. Multimed. Tools Appl. 65(3), 165–178 (2013)
20. Mousavi-nik, S.S., et al.: Proposed secureSIP authentication scheme based on elliptic curve
cryptography. Int. J. Comput. Appl. 58(8), 25–30 (2012). (0975–8887)
A New Secure SIP Authentication Scheme
169

21. Arshad, R., Ikram, N.: Elliptic curve cryptography based mutual authentication scheme for
session initiation protocol. Multimed. Tools Appl. 66(2), 165–178 (2013)
22. Nik, S.S.M., Shahrab, M.: Mutual SIP authentication scheme based on ECC. Int. J. Comput.
Electr. Eng. 6(2), 196–200 (2014)
23. Azrour, M., Farhaoui, Y., Ouanan, M.: A new secure authentication and key exchange
protocol for session initiation protocol using smart card. Int. J. Netw. Sec. 19(6), 870–879
(2017). doi:10.6633/IJNS.201711.19(6).02
24. Azrour, M., Ouanan, M., Farhaoui, Y.: Sip authentication protocols based on elliptic curve
cryptography: survey and comparison. Indones. J. Electr. Eng. Comput. Sci. 4(1), 231–239
(2016)
25. Zhang, L., Tang, S., Cai, Z.: Efﬁcient and ﬂexible password authenticated key agreement for
voice over internet protocol session initiation protocol using smart card. Int. J. Commun Syst
27(11), 2691–2702 (2013)
170
M. Azrour et al.

An Adaptation of GRA Method for Network
Selection in Vertical Handover Context
Mouad Mansouri(B) and Cherkaoui Leghris
Department of Computer Sciences, RTM Team, LIM Lab (L@M),
Hassan2 University, FST Mohammedia, Casablanca, Morocco
mansouri.mouad@yahoo.com, cleghris@yahoo.fr
Abstract. Nowadays, more and more people are willing to connect to
wireless networks anywhere, anytime, and to any best available technol-
ogy under the ABC concept (Always Best Connected). We can count sev-
eral wireless networks technologies, namely 3GPP (GPRS, UMTS, . . . ),
Wi-Fi technologies, and WiMAX. In such heterogeneous environment,
the mobile device must ensure a seamless vertical handover (VH), which
means the switching of communication between diﬀerent wireless net-
works types (e.g. 4G to Wi-Fi). This transfer must be done without
session breaks, which necessitates a quick and eﬃcient decision strategy.
This paper aims to optimize the network selection phase of VH, using the
Multi Attribute Decision Making (MADM) methods, which are the best
to be used in this context. We compared the decisions returned by diﬀer-
ent MADM algorithms, changing their normalization to determine which
one is the best, and we found a new adaptation of the Gray Relational
Analysis (GRA) to the VH decision making context.
Keywords: MADM methods · Mobile networks · Networks selection ·
Vertical Handover · Wireless networks
1
Introduction
The development of wireless networks nowadays (GSM, UMTS, HSxPA, LTE,
Wi-Fi, and WiMAX) places mobile terminals in a heterogeneous environment,
and allows them to connect to diﬀerent types of networks at the same time.
Indeed, the current mobile terminals, or mobile nodes (MN), are equipped with
several network interfaces allowing them, to connect to the available wireless
access network, oﬀering them the best connectivity, under the ABC paradigm.
This imposes a good management of the VH, to reach a “Seamless VH” and
so, to satisfy the requirements of the user, in terms of QoS (Quality of service).
Unlike the horizontal Handover, which means a change of the base station while
staying in the same access network, the VH is the process of changing the wire-
less access technology type (ex: from WiFi to 3G). It follows the architecture of
the 802.21 standard, called Media Independent Handover (MIH) standardized
in 2009 by the IEEE. The VH process can be split into three major phases:
c
⃝Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7 15

172
M. Mansouri and C. Leghris
– VH initiation: The MN collects information about available wireless networks;
– VH decision: in which one wireless network is selected from those available
anytime;
– VH execution: in which the MN switches communications to the new chosen
network.
However, these phases require an optimization. The vertical Handover remains
therefore an open research subject and this work aims the optimization of the
network selection phase (2nd phase of vertical Handover); which means choosing
the best network among those discovered in the ﬁrst phase. Mobile nodes (MN)
manages the VH based on information about network parameters and link states,
provided by MIH Function (MIHF) [1]. We found in the literature diﬀerent meth-
ods used in this context, based on diﬀerent theories, like neural networks, gaming
theory, artiﬁcial intelligence, and Multi-Attribute Decision making theory. Since
the network selection is by nature a decision problem, we agreed to push our
researches towards these MADM group algorithms. Moreover, these algorithms
are relatively simple to implement and do not require particular resources. This
advantage allows them to run quickly, and so insure a “Seamless Handover”.
Some MADM theory algorithms and their steps, will be detailed in the Sect. 2
of this paper. Section 3 will be about how we proceeded to compare the results
returned by two main MADM algorithms used in the VH context, changing their
standard normalization method. In Sect. 4, we present our simulations, and we
will discuss the results. Finally, we will conclude and give perspectives of our
contribution.
2
MADM Methods
We found in the literature many applications of these algorithms in various
domains such as economy, R&D, Biology, Medicine, Project feasibility, choice of
the best candidate or chief. . . MADM methods treat diﬀerent decision aspects,
by computing the values of attributes for each alternative of the decision making
problem, to ﬁnally give scores to each alternative (seen all the criteria together).
In the VH decision context [2–7] give schemes and architectures of possible use
of MADM methods. All these methods follow four major steps which are:
1. D matrix construction: Constructing the decision (or evaluation) matrix
(Eq. 1), with m alternatives and n criteria, the intersection of each alter-
native and criterion represents the performance of the alternative i towards
criterion j it is denoted by dij, so as to have a (D)nm matrix.
2. Normalization: Data are normalized to be computable and comparable, there
are diﬀerent normalization methods (Eqs. 2 to 6).
3. Weighting: This phase is treated independently, diﬀerent weights are assigned
to diﬀerent criteria, reﬂecting their relative importance towards other criteria.
4. Ranking: Alternatives are ranked following the score obtained by each one,
after applying the ranking algorithm on the normalized and weighted data;
the alternative with the best score is chosen.

An Adaptation of GRA Method for Network Selection
173
Fig. 1. General MADM steps
These steps can be described through the scheme in (Fig. 1), and are detailed
with equations in the following:
1. Constructing the decision matrix D:
Dmn =
⎛
⎜
⎜
⎜
⎝
d11 d12 · · · d1n
d21 d22 · · · d2n
...
...
...
...
dm1 dm2 · · · dmn
⎞
⎟
⎟
⎟
⎠
(1)
(D)nm matrix is ﬁlled with dij the value of attribute j for the alternative i;
2. Normalization:
The decision matrix (D)nm is normalized to have the normalized decision
matrix(R)nm, using one of the methods presented in the following, this phase
is important because the values of attributes have diﬀerent units, and the
ranges of values are diﬀerent from one attribute to one another, after the
normalization, the ranges of values will be more convergent:
(a) Euclidian method:
rij =
dij
m
i=1 d2
ij
(2)
(b) Sum method:
rij =
dij
m
i=1 dij
(3)
(c) Product method:
rij =
dij
	m
i=1 dij
(4)
(d) Min-Max method:
rij =
dij
max{dij}
For beneﬁt criteria.
= min{dij}
dij
For cost criteria.
(5)

174
M. Mansouri and C. Leghris
(e) Max method:
rij =
dij
max{dij}
For beneﬁt criteria.
= 1 −
dij
max{dij}
For cost criteria.
(6)
As every MADM method uses a particular normalization method, we consid-
ered to compare results returned by two well-known MADM methods, namely
TOPSIS and GRA, changing their usual normalization methods.
3. Weighting:
The most used weighting method is AHP, introduced by T. SAATY in [11],
followed by its extension: ANP, the fuzzy forms of these weighting methods:
FAHP and FANP, are also compared to have an idea about the best one to
be used in this context, detailed description of these methods can be found
in [8–10]. In this phase, the (R)nm matrix is weighted using Eq. 7 to have the
weighted and normalized matrix (V )nm.
vij = rij ∗Wj
(7)
4. Ranking:
(a) TOPSIS:
– After the decision matrix is constructed, normalised and weighted
((V )nm), The best and worst alternatives, respectively V + and V −,
are determined using Eqs. 8 and 9:
V + = max{vij}
For beneﬁt criteria;
= min{vij}
For cost criteria.
(8)
V −= min{vij}
For beneﬁt criteria;
= max{vij}
For cost criteria.
(9)
– Compute the distance between every (v)ij and the best and the worst
alternatives, respectively S+ and S−using Eqs. 10 and 11:
S+
i =




n

i=1
(V + −vij)2
(10)
S−
i =




n

i=1
(vij −V −)2
(11)
– Compute the ﬁnal score for each alternative using Eq. 12:
Ci =
S−
i
S−
i + S+
i
(12)

An Adaptation of GRA Method for Network Selection
175
– Finally, rank the alternatives in decreasing order of Ci. [13,14] propose
the use of TOPSIS in the VH context.
(b) GRA:
– After the decision matrix is constructed, normalised and weighted
((V )nm), The best alternative V + is determined using Eq. 8.
– Calculate the GRA coeﬃcient for each alternative i, using Eq. 13:
GRCi =
1
m
i=1 |vij −V +| + 1
(13)
– Rank the alternatives in decreasing order of GRCi. In [15–17], the
authors treat diﬀerent aspects of the decision making using GRA
method.
3
Our Proposal
In this work, we decided to compare the results obtained by some MADM meth-
ods in the context of VH decision, changing their normalization method, to see
the impact of this crucial step on the ﬁnal decision, so as to ﬁnally determine
relatively, the more suitable normalization for these MADM methods in this
context. No standardized simulation scenario is adopted by all the found works
in the litterature; every author uses his own simulation scenario. In our case, we
gathered the general information about wireless networks that we are going to
use, and we listed them in (Table 1), then, we designed a set of data following the
movement scenario that we found in [18], which is a case study of eight diﬀerent
zones, every zone contains diﬀerent wireless networks available.
Moreover, we variated dynamically the values of data gathered in every single
zone, to simulate the changes of dynamic attributes, to have a close-to-reality
data set, and then we compared the ﬁnal results (Decisions), to see which method
Table 1. Possible values of attributes used during the simulation of numeral method
Networks/
Attributes
T (Kbps)
Av (%) S (%) D (ms)
L (∗106) EC (1–7) C (1–7) J (ms)
GPRS
21.4–171.2
50–100 50
50–70
50–80
2
1
3–20
EDGE
43.2–345.6
40–100 50
20–60
25–70
2
2
3–20
UMTS
144–2000
40–100 60
20–40
15–65
4
4
3–20
HSxPA
14 Mbps
50–100 60
10–50
10–80
4
5
3–20
LTE
10–300 Mbps
40–100 65
10–30
10–40
7
7
3–20
Wi-Fi abg
8–54 Mbps
40–100 60
130–200 30–70
3
1
3–20
Wi-Fi n
72–450 Mbps
30–100 65
100–140 20–60
3
1
3–20
Wi-Fi ac
433–1300 Mbps 50–100 70
90–100
10–40
5
2
3–20
WiMAX
70 Mbps
40–100 60
60 –100 10–70
7
5
3–20

176
M. Mansouri and C. Leghris
is going to ﬁt the VH context in terms of the number of VHs occurred, and the
QoS oﬀered during this scenario, seeing all service classes.
We used in this simulation nine network technologies: GPRS, Edge, UMTS,
HSXPA, LTE, Wi-Fi a/b/g, Wi-Fi n, Wi-Fi ac, and WIMAX, because these
are the most present nowadays. We took into consideration eight criteria, which
are: The Cost (C), the Throughput (F), the Bandwidth Availability (Av), the
packet Losses (L), packet Delay (D), the Security (S), the Jitter (J) and the
Energy Consumption (EC).
4
Results
After choosing the network technologies and the attributes to be considered,
and after choosing the evaluation criteria of the simulation results, described in
Sect. 3, we stored these results which are the decisions taken in every point of
decision over all the designed scenario of movement. We changed the normaliza-
tion method, and redid the simulation to see its impact on the decisions made in
every zone of movement. We used the four weighting vectors for four weighting
methods (AHP, FAHP, ANP, FANP), these vectors are illustrated in Fig. 2.
Fig. 2. Weighting vectors used in the simulations in AHP, ANP, FAHP, and FANP
used to weight the criteria
After executing many simulations, as described before, we got many diﬀerent
results that we are going to illustrate in Figs. 3 and 4.
We denoted by KKxx, the results of the simulation using the combination
xx, using every time, a diﬀerent normalization. The results corresponding to

An Adaptation of GRA Method for Network Selection
177
Fig. 3. VHs occurred in diﬀerent zones using GRA with the sum normalization (a),
and TOPSIS with Euclidian normalization
the use of diﬀerent normalization methods are shown and compared in the next
section. And the number of handoﬀs occurred is calculated regardless to the
unnecessity/necessity of handoﬀs to occur.
Fig. 4. VHs occurred in diﬀerent zones using DIA with the max normalization
In Fig. 3, the best results returned by GRA are those using the Sum normal-
ization method, because it gives less number of handovers as shown in Fig. 5,
moreover, this adapted combination provides good QoS quality for all the traﬃc
classes, and this, by using either FAHP or FANP weighting methods, because
the weight vectors returned by the two methods are sensibly the same (Fig. 2).
The other combinations give unsatisfying results, either because they produce
many handoﬀs (Figs. 4 and 5), or because they do not give the same results
seeing the QoS oﬀered in every moment, along the whole trajectory to serve
all the traﬃc classes. We also had an idea about the product and Min-Max
normalization methods, that give inexpressive results in computing, every time,
when an attributes value is equal to zero, so they cant be used in such context.

178
M. Mansouri and C. Leghris
Fig. 5. Number of VHs occurred for each combination of MADM methods
5
Conclusion and Prospects
In the context of VH decision making, the MADM methods are by nature, the
best to use. We changed the normalization technique of GRA method, and we
compared the results with the results of TOPSIS, which is known for its good
results, to see their results in the VH context. We designed data to simulate the
movement of a MN through diﬀerent zones where diﬀerent wireless technologies
are available, and executed the steps of the two diﬀerent MADM methods, to
ﬁnally have scores that determine decisions taken in every zone. Moreover, we
changed, in every simulation, the normalization and weighting method for both
the MADM methods, to see the eﬀect on the returned results, and concluded
that there are two best combinations in terms of the total number of handoﬀs
done, and the QoS oﬀered during the simulation, which are: FANP Euclidian nor-
malization TOPSIS, and FANP Sum normalization GRA, which is an adapted
combination of MADM methods in the VH network selection context, since GRA
uses basically the Min-Max normalization. We noticed also that FAHP/FANP
are better than AHP/ANP, what gave us the idea to try to mix other MADM
methods with the fuzzy sets theory, and try to eliminate the impact of the impre-
cision and vagueness in the metrics sensed during the discovery phase, and which
can be the cause of the large number of VHs and rank reversals.
We will validate our work towards each service class in the future.
References
1. Lahby, M., Leghris, C., Adib, A.: New optimized network selection decision in
heterogeneous wireless networks. Int. J. Comput. Appl. (0975–8887) 54(16), 0975–
0982, September 2012
2. Bhute, H.A., Karde, P.P., Thakare, V.M.: A vertical handover decision approaches
in next generation wireless networks: a survey. Int. J. Mob. Netw. Commun. Telem-
atics ( IJMNCT) 4(2), 33–43 (2014)

An Adaptation of GRA Method for Network Selection
179
3. Mahardhika, G., Ismail, M., Nordin, R.: Multi-criteria vertical handover decision
algorithm in heterogeneous wireless network. J. Theore. Appl. Inf. Technol. 54(2),
339–345 (2013)
4. Kassar, M., Kervella, B., Pujolle, G.: An overview of vertical handover decision
strategies in heterogeneous wireless networks. J. Comput. Commun. 37(10), 2607–
2620 (2008). Elsevier
5. Vine, H.A.: Comparison between MADM algorithms for vertical handoﬀdecision.
Techn. J. (2010). doi:10.5120/3125-4300. University of Engineering and Technology
Taxila
6. Stevens-Navarro, E., Martinez-Morales, J.D., Pineda-Rico, U.: Evaluation of verti-
cal handoﬀdecision algorithms based on MADM methods for heterogeneous wire-
less networks. J. Appl. Res. Technol. 10, 534–548 (2012)
7. Cheelu, D., et al.: A study of vertical handoﬀdecision strategies in heterogeneous
wireless networks. Int. J. Eng. Technol. (IJET) 5(3), 2541–2554 (2013)
8. Buyukyazici, M., Sucu, M.: The analytic hierarchy and analytic network processes.
Hacettepe J. Math. Stat. 32, 65–73 (2003)
9. Meixner, O.: Fuzzy AHP group decision analysis and its application for the eval-
uation of energy sources, scale transitivity in the AHP. J. Oper. Res. Soc. 54,
896–905 (2011)
10. Khademi, N., Mohaymany, A.S., Shahi, J., Zerguini, J.: An algorithm for the ana-
lytic network process (ANP) structure design. J. Multi-Criteria Decis. Anal. 19,
33–55 (2008)
11. Saaty, T.L.: Decision making with the analytic hierarchy process. Int. J. Serv. Sci.
1(1), 83–98 (2008)
12. Lahby, M., Leghris, C., Adib, A.: A novel ranking algorithm based network selection
for heterogeneous wireless access. J. Netw. 8(2), 263–272 (2013)
13. Savitha, K., Chandrasekar, C.: Trusted network selection using SAW and TOPSIS
algorithms for heterogeneous wireless networks. Int. J. Comput. Appl. (0975–8887)
26(8), 22–29, July 2011
14. Senouci, M. A., Mushtaq, S., Hoceini, S., Mellouk, A.: TOPSIS-based dynamic
approach for mobile network interface selection. Comput. Netw. (2016). doi:10.
1016/j.comnet.2016.04.012
15. Almutairi, A.F., Landolsi, A.M., Al-Hawaj, A.O.: Weighting selection in GRA-
based MADM for vertical handover in wireless networks. In: UKSim-AMSS 18th
International Conference on Computer Modelling and Simulation (2016)
16. Archana, M., Sujatha, V.: Application of fuzzy MOORA and GRA in multi-
criterion decision making problems. Int. J. Comput. Appl. (0975–8887) 53(9), 46–
50, September 2012
17. Huszk, A., Imre, S.: Eliminating rank reversal phenomenon in GRA based network
selection method. In: IEEE ICC Proceedings (2010)
18. IEEE 802.21 Media Independent Handover, IEEE P802.21 Tutorial, 17 July 2006

Toward Cluster Head Selection Criterions in Mobile
Ad Hoc Network
Meriem Ait Rahou
(✉) and Abderrahim Hasbi
Mohammadia School of Engineers, Mohamed V University, Rabat, Morocco
mariamaitrahou@research.emi.ac.ma, hasbi@emi.ac.ma
Abstract. Mobile ad hoc network MANET is dynamic and self-conﬁguring
network formed by many mobile nodes. A set of these nodes make one cluster.
This mechanism is called clustering. The main aim of clustering is dividing
network into many groups in order to have a good utilization, eﬃcient manage‐
ment of resources and prolonging network lifetime. Each group is organized
hierarchically and managed by a particular node. This manager node is respon‐
sible of routing data packets from other groups and also from and between nodes
inside its own group.
The major concern of clustering in mobile ad hoc network is how to divide
network and choose the adequate manager node for each cluster. In this paper,
we attempt to highlight on the one of the most important challenging research
issues to be resolved. It presents a various techniques and recent work to make
cluster and elect cluster head in Manet, then we pinpoint on some prominent
algorithms in the literature.
Keywords: MANET · Clustering · Cluster head
1
Introduction
A mobile ad hoc network (MANET) is a self-conﬁgurable network of wireless mobile
devices that communicate without the help of any centralized administration or ﬁxed
infrastructure. Actually, Manets is introduced in many application domains [1], such as
emergency search-and-rescue operations, meetings or conventions, electronic email and
ﬁle transfer, and military application. In mobile ad hoc network each node is able to
forward data to other nodes in the same transmission range. When mobile nodes are
uncovered by any existing infrastructure, one prominent approach is proposed to enable
wireless nodes operating in the ad hoc mode and self-organize into cluster based network
architecture. Clustering aims to increase network lifetime, use network resources eﬃ‐
ciently and enhance scalability. It’s consists of dividing network into many easily
manageable groups of nodes namely clusters.
In a cluster every node is able to forward data to other nodes in the same transmission
range. There are some particular nodes within the clusters considered as leaders and they
retain some important information such as list of nodes and the route to every node in
the cluster. This type of nodes is called cluster head and it’s responsible to manage
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_16

communication between ordinary nodes and looks after cluster maintenances, routing
table updates and discovery of new routes.
The responsibility of cluster head is to communicate with all the nodes within its
cluster. However it must be able to communicate with the nodes of other clusters as well,
which can be directly or through the respective cluster head or through gateways.
Communication is done in three steps. First of all the cluster head receives the data sent
by its members, secondly it compresses the data, and ﬁnally transmits the data to the
base station or other cluster head. Appropriate cluster head can reduce energy utilization
and enhances the network lifetime [2].
Therefore, the eﬀectiveness of the overall clustering concepts depends on the right
choice of the leaders nodes. If they are elected carefully and eﬃciently the overall
network communication will be enhanced.
For this purpose, several clustering approaches were proposed in the literature in
order to study and ﬁnd the adequate algorithms and mechanisms to divide network into
many clusters and select the leader node of each cluster. In this paper we attempt to
highlight on the diﬀerent criterions which intervene in the clustering and cluster head
selection.
To have a better understanding a background and various clustering techniques is
made in the Sect. 2. The related work is presented in the Sect. 3. A comparison of some
proposed algorithms in the literature is made in Sect. 4 and Finally, Sect. 5 concludes
this paper.
2
Background and Motivation
2.1
MANET’s Characteristics
• Dynamic topology: nodes may move randomly and at unpredictable time with
diﬀerent speeds. They can join or leave the network, making the network topology
dynamic in nature.
• Limited bandwidth: As any wireless networks, the bandwidth in Manet is limited
compared with wired networks. Due to the noise, multiple access, and interference
conditions, bandwidth and throughput are to be optimized to perform with the
maximum eﬃciency and reliability.
• Energy or battery constrained: in Manet, nodes are completely rely on batteries. The
network is to be optimized in order to conserve the energy consumed by the mobiles.
• Distributed nature: Communication in Manets doesn’t depend on any established
infrastructure, but upon cooperation of nodes. Thus routing, security and network
management in general have to be distributed across all nodes which bring many
diﬃculties in fault detection and in protocols implantation especially that requires a
centralizes administration.
2.2
Clustering in Manets
Clustering is a mechanism that consists to divide network onto many small, self-
manageable and geographically adjacent groups that have similarities according to some
Toward Cluster Head Selection Criterions
181

criteria together. A typical cluster structure is shown in Fig. 1. It can be seen that the
nodes are divided into a number of virtual groups (with circles) based on certain rules.
Under a cluster structure, mobile nodes may be assigned a diﬀerent status or function,
such as cluster-head, cluster gateway, or cluster member [3].
Fig. 1. Cluster structure with 3 types of nodes.
2.3
Types of Nodes in a Cluster
As presented in the Fig. 1, in manet cluster contains three important types of nodes:
• Cluster Head: It’s a special node that identiﬁes each cluster. It coordinates among
nodes of its own cluster and performs intra-cluster transmission arrangement and
data forwarding and so on.
• Cluster Member: It’s an ordinary node that belongs to a cluster and which transmit
the information to the cluster head. This type of nodes cannot communicate directly
with other cluster members.
• Cluster Gateway: its primary objective is to connect clusters with each other. It’s the
common node between two or more clusters.
2.4
Clustering Advantages
Clustering presents many important advantages in manet [4]:
• It allows the better performance of the protocol for the Medium Access Control
(MAC) layer by improving the spatial reuse, throughput, scalability and power
consumption.
• Eﬃcient handling of mobility management.
182
M. Ait Rahou and A. Hasbi

• It helps to improve routing at the network layer by reducing the size of the routing
tables.
• It decreases transmission overhead by updating the routing tables after topological
changes occur.
• It helps to aggregate topology information as the nodes of a cluster are smaller when
compared to the nodes of entire network. Here each node stores only a fraction of
the total network routing information.
• It saves energy and communication bandwidth in ad hoc networks.
• It has the ability to adapt, depending on the policy management in the cognitive radio
networks.
2.5
Clustering Drawbacks
Despite the immense important of clustering, it has some disadvantages and drawbacks
[4]:
• The maintenance cost for a large and dynamic mobile network requires explicit
message exchange between mobile node pairs. As the network topology changes
quickly and concerns many mobile nodes, the number of information message
exchange grows to reach a critical point. This information exchange consumes a lot
of network bandwidth and energy of mobile nodes.
• A ripple eﬀect of re-clustering occurs if any local events take place like the movement
or the death of a mobile node, as a result it may lead to the re-election of a new cluster
head. When a new cluster head is re-elected it may cause re-elections in the whole
of the cluster structure. Thus, the performance of upper layer protocols is aﬀected by
the ripple of re-clustering.
• One of the major drawbacks of clustering in MANETs is that some nodes consume
more power to compare to the others nodes in the same cluster. As special node like
a cluster head or a gateway.
• Node manages and forward all messages of the local cluster their power consumption
will be high compared to ordinary nodes. It may cause untimely shutdown of nodes.
Problem Formulation. As cited previously, MANET is a multi-hop wireless network
in which the mobile nodes are dynamic in nature and has a limited bandwidth and
minimum battery power. Due to this challenging environment the mobile nodes can be
grouped into clusters to achieve better stability and scalability. So, the study of MANET
is a very challenging task. Clustering may consider one or more factors for cluster head
selection. Some of considered factors during cluster formation are energy, mobility
behavior, degree, weight, distance and spreading degree of the nodes etc. The main
concern of clustering is how to select the optimal and adequate cluster head in order to
manage cluster optimally and thus overall network.
Toward Cluster Head Selection Criterions
183

2.6
Cluster Head Selection Metrics in Mobile Ad Hoc Network
As showed in the Fig. 2 above, cluster head selection metrics in mobile ad hoc network
can be divided in four categories:
Fig. 2. Cluster head selection metrics in mobile ad hoc network.
Arbitrary metrics based cluster-head selection. In this category, cluster-head is
elected according to algorithms that use random and generally not signiﬁcant values
such as id nodes. Lowest identiﬁer clustering algorithm LID [5] is one of the proposed
methods in the literature. In LID, every node in the network is assigned by a unique
value ID. The node with lowest ID is selected as cluster-head without taken into account
its qualiﬁcation or quality of service i.e. QoS parameters. Least Cluster Change LCC [6]
solved this gab by proposing cluster maintenance and re-clustering. This one is invoked
if two cluster heads exist within each other’s transmission range then one leaves the
cluster head role or if even any mobile node does not access any cluster head or it does
not come within the reach range of any cluster head then the structure of the cluster is
reconstructed according to the LID algorithm. Moreover, in arbitrary metrics based
cluster-head selection category, other algorithms have been proposed to build k-hops
clusters. Max-Min-d cluster algorithm forms D-clusters non-overlapping by heuristic
based algorithm [7]. It’s conducted in three stages. In the ﬁrst one, each node broadcasts
its ID to its D-hops neighbors, collects their IDs and detects the highest received ID. In
the second one, each node broadcasts the highest received ID. Then it keeps the lowest
184
M. Ait Rahou and A. Hasbi

IDs among the highest received IDs. In the last stage, cluster head is elected based on
the IDs which are saved in two previous stages.
Cluster head selection metrics with no metrics. In this category, cluster-head is
chosen without resorting to any selection metrics.
1 hop clustering [8]. Proposed Passive Clustering approach PC. It does not use dedi‐
cated control packets or signals for clustering speciﬁc decision. In PC, nodes have many
states, initial, cluster-head, gateway or ordinary state. At ﬁrst, all the mobile nodes are
with the initial state and only mobile nodes have the possibility and potential to be
cluster-heads. When a potential cluster-head with the “initial” state wants to send some‐
thing he declares himself as a cluster-head indicating its status in the sent packet.
Neighbors can realize the cluster-head request by monitoring the “cluster state” in
the packet, and then record the Cluster ID and the packet receiving time. A mobile node
that hears from just one cluster-head becomes an ordinary node, and a mobile node that
hears from various cluster-head becomes a gateway.
Passive clustering algorithm can form and maintain its structure without exchanging
packets. Each node is responsible for updating its own cluster status by keeping a timer.
When an ordinary node does not hear anymore from its cluster-head for a given period,
its status turns back to “initial”. So, PC can completely eliminate the perceived overhead
in active clustering.
K-cluster algorithm [9]. Presented another algorithm which does not use metrics to
select cluster-head. In this approach, once a node arrives, it’s in the initial state and
claims if their neighbors are aﬃliated to cluster-heads or not. If they are in the initial
state, the node declares itself as cluster-head and broadcasts its decision to neighbors.
So, all unaﬃliated nodes become members of this cluster. Else, the arrived node joins
the cluster of the neighbor with the nearest cluster-head and which is located at most at
k-hops. If all cluster-heads are in the k-hops from this node, it declares itself as cluster-
head and invites k-hops neighbors to join it if their cluster-heads are more distant. In
this algorithm, re-clustering is invoked if the distance between two cluster-heads is least
than k-hops. The cluster-head with the smallest ID yield its role and members must join
other cluster-heads.
Cluster head selection with speciﬁcs metrics. In this category, clustering depends on
a particular metric such as energy, topology, mobility, or load balancing…etc.
Mobility. Many researches were proposed in the literature in this ﬁled. Mobility based
metric clustering algorithms (Mobic) [10]: proposed a stable clustering algorithm. They
take into consideration the mobile ad hoc network that comprises of nodes with dissim‐
ilar capabilities and resources such as computation power, transmission range, battery
backup, etc. They have improved the stability of the cluster compared to LVC algorithm
[11] by taking into consideration the range transmission and the relative mobility of
nodes in order to select the appropriate cluster head. Relative mobility of every node is
calculated by comparing mobility in various intervals. The CH is chosen according to
the lowest value of aggregate relative mobility metric. Otherwise, a node declares itself
Toward Cluster Head Selection Criterions
185

as a cluster member (at most a cluster of 2 hops diameter). If two CH are in contention
for retaining the CH status and have the same mobility metrics, the selection of cluster
head is based on the lowest ID algorithm. If a node is between two cluster head, it is
will be the gateway of the cluster. If a node member status with a low mobility is moved
into the range of CH with high mobility no re-clustering is trigged.
Energy. Thanks to the great role that plays cluster head including coordinating among
nodes, performing intra-cluster transmission arrangement and data forwarding, it more
likely drains rapidly and may produce communication interruption. Since nodes in
manets depend on the batteries, many researchers proposed to take energy as criteria to
select cluster head. Thus, the node with the high energy power is chosen as cluster head.
Enhance Cluster based Energy Conservation Algorithm (ECEC) [12] is one of the
proposed approaches in this ﬁeld. ECEC elects the node with the highest estimated
energy value in its neighborhoods as cluster head. It presented an eﬃcient protocol to
extend ad hoc network lifetime and reduce power consumption. This algorithm ensures
minimum connectivity of nodes in the network, the ability for nodes to reach each other,
and conserve energy by identifying redundant nodes and turning their radios oﬀ.
However, ECEC exchanges more overhead to elect the CHs and getaways.
Topology. In the topology based clustering, the cluster head is elected based on a metric
computed from the network topology like node connectivity. K-hop connectivity ID
clustering algorithm K-CONID is proposed by [13]. Here all other nodes are at distance
of at most k-hops from the cluster head. Initially, each node in the network is deﬁned
by a pair of parameters, the connectivity of node and its identiﬁer. The node with the
highest connectivity is elected as cluster head. If two nodes have the same connectivity
value, the node with the lowest ID is elected as cluster head. For the connectivity purpose
using only node connectivity as a criterion causes several connections between nodes
on other side, using only a lower ID criterion generates more clusters than necessary.
The main aim of K-CONID is to minimize the number of cluster formed in the network.
Highest connectivity clustering algorithm (HCC) [14] is another connectivity-based
algorithm. In HCC, The node which has maximum number of neighboring nodes will
be selected as cluster head. If two nodes or more have the same degree of connectivity,
the node with the lowest ID is elected as a CH. HCC generates a limited number of
clusters.
The algorithm does not limit the number of nodes in a cluster. Therefore, the
throughput of the system decreases as the number of nodes increases.
The re-aﬃliation count of nodes is high due to node movements and as a result, the
highest-degree node (the current CH) may not be re-elected to be a CH even if it loses
one neighbor.
Cluster head selection with combined metrics. In this category, cluster-head is
elected according on many parameters at the same time. Weight based clustering is an
approach that takes a number of metrics into consideration for cluster formation,
including node degree, residual energy capacity, moving speed, and so on. One
advantage of this clustering scheme is that it can ﬂexibly adjust the weighting factors
for each metric to adjust to diﬀerent scenarios. For example, in a system where battery
186
M. Ait Rahou and A. Hasbi

energy is more important, the weighting factor associated with energy capacity can be
set higher. However, not all of these parameters are always available and accurate, and
the information inaccuracy may aﬀect clustering performance [15].
Various mechanisms have been proposed in the literature to solve weight based
clustering problems in mobile ad hoc network. In these algorithms, cluster head is
selected depending on the weight of the node to name but a few, WCA, PIWCA, IWCA,
EEWCA, ODWCA, EBWCA, EWCSA, etc.
This paper presents the comparison between some important proposed approaches.
3
Related Work
3.1
Weight Based Clustering Algorithms WCA
This approach [16] elects the cluster head according to its weight. At ﬁrst, WCA calcu‐
lates the average weight of each node by combining four parameters namely battery
power, communication range, mobility and ability to handle nodes. Then, it compares
the weights of nodes and selects the node with the minimum weight as cluster-head.
This non-periodic process is used only in two cases. The ﬁrst one is the node mobility.
For instance, if a node moves into a zone that is not covered by any cluster-head, then
the cluster-head election procedure is invoked throughout the whole system. The second
one is the inability of the current cluster-head to cover all nodes. Hence, WCA try to
minimize communications over-head.
Moreover, to ensure that the cluster-head will not be overloaded, WCA allows
predeﬁning a threshold which indicates the number of nodes that a cluster-head can
ideally support.
Although WCA has proved better performance than all its previous algorithms, it
lacks in knowing the weights of all the nodes before starting the clustering process and
in draining CHs rapidly. To solve this problem Rohini [17] proposed a probability based
adaptive invoked weighted (PAIWCA).
3.2
Probability Based Adaptive Invoked Weighted Clustering Algorithm
PAIWCA
Probability based adaptive invoked weighted clustering algorithm also known as
PAIWCA [17] was originally proposed by Rohini and Indumathi in which the weight
of nodes is computed based on the probability of a node for being a cluster-head.
In this approach, every node calculates its own weight independently and before the
clustering process. Weight value of a node (1) is computed based on its transmission
range Tr, transmission rate Tx, mobility M, power consumed P, its probability of being
a cluster-head Chprob (2) and the constants w1 to w4 as follow.
W = (w1 ∗Tr) + (w2 ∗Tx) + (w3 ∗M) + (w4 ∗P) −Chprob
(1)
Toward Cluster Head Selection Criterions
187

The PIWCA makes an assumption that the network topology is static during the
execution of the clustering algorithm and each mobile node joins exactly one cluster-
head. It consists to compare weights of all nodes and elects the minimum one as cluster-
head.
After the cluster formation and to preserve as much as of the existing clustering
structure as possible, PIWCA, proposed the maintenance phase which invoked every
time that a node moves to the outside of its cluster boundary or even if a new node joins
the cluster. If a node moves outside of its cluster boundary and it was the cluster-head,
then the clustering algorithm is invoked for cluster reorganization. If it was an ordinary
node, it is required to ﬁnd a new cluster-head to aﬃliate with. If it doesn’t received any
packets from others nodes, it declares itself as cluster-head and form its own cluster. If
a node joins the cluster, PIWCA, propose to calculate its probability for being a cluster-
head and compare its value with the probability of the existing cluster-head.
CHprob = C ∗(Eresidual∕Emax) + Tr
(2)
The node that has the maximum value of cluster-head probability will be chosen as
the new cluster-head and the other one will be the member of that cluster-head.
3.3
Improved Weighted Clustering Algorithm (IWCA)
This approach [18] consists of eliminating malicious nodes from cluster formation. It
contains two phases. The ﬁrst one is responsible to detect if nodes are normal or not. If
any malicious node is there, then it cannot take part in clustering phase. In the second
one, and depending to the collected information about node behavior, WCA algorithm
is invoked.
The behavioral data collection module is responsible to form behavioral dataset that
saves the ratio of amount of a given behavior over the total amount of packets that the
node received. For instance, packet drop rate (PDR), packet misroute rate (PMR) and
packet modiﬁed rate (PMOR). When a node needs to summarize its observation of
misbehaving neighbors, it will calculate the rate of abnormal behaviors over the overall
behaviors it has observed as follows:
– Packet drop rate PDR = Number of Packet Dropped/Total Number of Incoming
Packets.
– Packet misroute rate PMIR = Number of Packet Misrouted/Total Number of
Incoming Packets.
– Packet modiﬁcation rate PMOR = Number of Packet Modiﬁed/Total Number of
Incoming Packets.
Once the calculation is done, for every node, a threshold is checking for PDR, PMIR
and PMOR in order to determine if it’s malicious or normal. The weight then is calcu‐
lated.
Proposed IWCA algorithm can improve the trust of cluster formation followed by
malicious node removal from cluster head or member selection the objective of trust
188
M. Ait Rahou and A. Hasbi

management schemes is to properly evaluate the trustworthiness of nodes and thus
identify and mitigate misbehaviors.
4
Discussion and Comparison of the Mentioned Algorithms
Mobile ad hoc network, clustering plays an important role for organizing and main‐
taining logical hierarchical topology, improving information management, and therefore
facilitating some essential process such as routing. Thus, selecting the adequate cluster
head remains a major concern for researchers in the last decade. As shown before, there
are many factors involved into the process of clustering such as weight clustering algo‐
rithms to name but a few. In this type of mechanisms, the election of CH depends essen‐
tially on its own weight. Weight clustering algorithm WCA, is considered as backbone
of the proposed algorithms and represents one of the ﬁrst proposed protocols that use
combined metric for cluster head selection. WCA allows dividing network to many one-
hop clusters. However, it knows a high cluster head change and thus low cluster stability
and very high total overhead. Concerning Probability based adaptive invoked weighted
clustering algorithm PIWCA, is one of the prominent proposed algorithm based on node
weight, and contrary at WCA, it allows to divide network in clusters where the cluster
head change and total overhead is relatively low, allowing high cluster stability. IWCA
is another automated algorithm based on WCA and in the same time a trust management
scheme for MANETs that uses a PDR, PMR and PMOR three parameters to assess the
trustworthiness of nodes. The table thereafter shows a comparison between WCA,
PIWCA and IWCA, their advantages and drawbacks (Table 1).
Table 1. Comparison between WCA, PIWCA and IWCA
Weight based on
Maintenance by
Advantages
Disadvantages
WCA
Transmission range
Mobility
Energy
Ability to handle nodes
Re-clustering
Cluster head will not be
overloaded (by
deﬁning a threshold)
Lacks of knowing
weights before
clustering process
Overhead of overall
network
Draining cluster head
capabilities
PIWCA
Transmission range
Transmission rate
Mobility
Energy consumed
Probability of being CH
Reduced action
Avoids network
overload
Enhances stability and
connectivity
All the neighbors of the
chosen cluster head are
no more allowed to
participate in the
election procedure
IWCA
Transmission range
Direction of mobile node
Energy
Speed of node movement
Malicious behavior
Re-clustering
Improve the trustiness
of cluster by malicious
node removal from
cluster head or member
selection
Algorithm detects and
removes the malicious
nodes after having
calculates their weight
which drains nodes
capabilities
Toward Cluster Head Selection Criterions
189

5
Conclusion
As MANETs have attracted more attention in recent years, much research has been
addressing all kinds of issues related to them. In This paper we attempted to highlight
an important background. We have also presented the various mechanisms to select the
adequate cluster head namely, algorithms with no metrics, algorithms with speciﬁc or
arbitrary metrics or even algorithms with combined metrics. We focused on the last one
because it associates many parameters such as energy, mobility, node degree. Many
researchers worked on clustering and cluster head selection based on combined metrics.
We have presented and compared some of the proposed work in order to have a clear
and best understanding about clustering in mobile ad hoc network to be able to oﬀer our
own contribution.
References
1. Hussein, A.H., Abu Salem, A.O., Yousef, S.: A ﬂexible weighted clustering algorithm based
on battery power for mobile ad hoc networks. In: 2008 IEEE International Symposium on
Industrial Electronics, Cambridge, pp. 2102–2107 (2008)
2. Hussain, K., Abdullah, A.H., Iqbal, S., Awan, K.M., Ahsan, F.: Eﬃcient cluster head selection
algorithm for MANET. J. Comput. Netw. Commun. 2013, article ID 723913, 7 p. (2013). doi:
10.1155/2013/723913
3. Jane, Y., Yu, P., Chong, H.J.: A survey of clustering schemes for mobile ad hoc networks. In:
IEEE Communications Surveys and Tutorials, vol. 7, no. 1, pp. 32–48 (2005). doi:10.1109/
COMST.2005.1423333
4. Anupama, M., Sathyanarayana, B.: Survey of cluster based routing protocols in mobile ad
hoc networks. Int. J. Comput. Theor. Eng. 3(6), 806–815 (2011)
5. Ephremides, A., Wieselthier, J.E., Baker, D.J.: A design concept for reliable mobile radio
networks with frequency hopping signaling. Proc. IEEE 75, 56–73 (1987)
6. Chiang, C.-C., et al.: Routing in clustered multihop, mobile wireless networks with fading
channel. In: Proceedings of IEEE SICON 1997 (1997)
7. Amis, A., Prakash, R.: Load-balancing clusters in wireless ad hoc networks. In: Proceedings
of the 3rd IEEE Symposium on Application-Speciﬁc Systems and Software Engineering
Technology ASSET 2000, Richardson, Texas, USA, 24–25 March 2000, pp. 25–32. IEEE
(2000). ISBN: 0-7695-0559-7. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=888028
8. Kwon, T.J., et al.: Eﬃcient ﬂooding with passive clustering an overhead-free selective forward
mechanism for ad hoc/sensor networks. Proc. IEEE 91(8), 1210–1220 (2003)
9. Lin, H.C., Chu, Y.-H.: A clustering technique for large multihop mobile wireless networks.
In: The IEEE 51st Vehicular Technology Conference Proceedings (VTC 2000), Tokyo, Japan,
15–18 May 2000, vol. 2, pp. 1545–1549 (2000). ISBN: 0-7803-5718-3. http://
ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=851386
10. Basu, P., Khan, N., Little, T.D.C.: A mobility based metric for clustering in mobile ad hoc
networks. In: Proceedings of IEEE ICDCSW, pp. 413–418 (2001)
11. Neethu, V.V., Singh, A.K.: Mobility aware loose clustering for mobile ad hoc network.
Procedia Comput. Sci. 54, 57–64 (2015). ISSN: 1877-0509. http://dx.doi.org/10.1016/j.procs.
2015.06.007
190
M. Ait Rahou and A. Hasbi

12. Fathi, A., Taheri, H.: Enhance topology control protocol ECEC to conserve energy based
clustering in wireless ad hoc networks. In: 2010 3rd IEEE International Conference on
Computer Science and Information Technology (ICCSIT), Chengdu, pp. 356–360 (2010)
13. Chen, G., Nocetti, F., Gonzalez, J., Stojmenovic, I.: Connectivity based k-hop clustering in
wireless networks. In: Proceedings of the 35th Annual Hawaii International Conference on
System Sciences, vol. 7, p. 188.3 (2002)
14. Gerla, M., Tsai, J.T.: Multiuser, mobile multimedia radio network. Wirel. Netw. 1, 255–265
(1995)
15. Chatterjee, M., Das, S.K., Turgut, D.: An on-demand weighted clustering algorithm WCA for
ad hoc networks. In: Proceeding of the IEEE Globecom 2000, pp. 1697–1701 (2000)
16. Chatterjee, M., Das, S.K., Turgut, D.: WCA: a weighted clustering algorithm for mobile ad
hoc networks. Clust. Comput. J. 5(2), 193–204 (2002)
17. Rohini, S., Indumathi, K.: Probability based adaptive invoked clustering algorithm in
MANETs. http://arxiv.org/abs/1102.1754
18. Gupta, N., Singh, R.K., Shrivastava, M.: Cluster formation through improved weighted
clustering algorithm IWCA for mobile ad-hoc networks. In: 2013 Tenth International
Conference on Wireless and Optical Communications Networks (WOCN), Bhopal, pp. 1–5
(2013)
Toward Cluster Head Selection Criterions
191

A New Method of IPv6 Addressing Based on RFID
in Small Objects Networks
Ali El Ksimi
(✉), Cherkaoui Leghris, and Khoukhi Faddoul
Faculty of Science and Technology, Mohammedia, Morocco
ali.elksimi@yahoo.fr, cleghris@fstm.ac.ma, khoukhif@gmail.com
Abstract. SmObnet6 (Small Objects Network based IPv6) is a generic term
which is used to deﬁne either small or large networks that could be interconnected
with small objects in IoT context. RFID is one of the famous technologies used
in internet of thing. Despite their low costs and high quality, how RFID tags could
make small objects communication via internet knowing that they have just a
hexadecimal identiﬁer and they do not have an IP address? To answer this ques‐
tion, several solutions are proposed in the literature. In this paper, we propose an
algorithm based on the logical operator AND inclusive between the network
preﬁx and the tag ID to derive an IPv6 address of the tag from its ID. In this way,
RFID-IPv6 mapping provides better solution for objects because it minimizes the
chances of address conﬂicts in the network.
Keywords: IPv6 · IoT · RFID · Small objects · EPC
1
Introduction
The SmObNet technologies are going to transform the entire society, including
ourselves. With the emergence of many Internet connected objects, we have indeed a
new dimension of our presence in the world: we ﬁnd out, work, play, eat and are
conducting a number of diﬀerent activities on the net. This digital life is characterized
by its immediacy, density information, omnipresence, in contrast to the world of things;
it’s a new real world. Given that in the coming years, all objects will be connected to
the internet, it is recommended to use a technology that will be the best solution to
address them. In this paper, we focused on RFID technology to this fact. It is based on
the exchange of information carried by electromagnetic waves between a label, or rated
“tag”, and a reader. Today, there are thousands in the number of applications which
include RFID. Transforming the identiﬁer tag in IPv6 address will enable small objects
to communicate via internet.
The main point of having IoT (internet of things) devices is that they can intercon‐
nect, and communicate with one another or with the Internet. IoT devices are thus typi‐
cally equipped with one (or more) network interfaces [1].
In this article, we propose an algorithm to derive the IPv6 address of the tag from its
identiﬁer using the reader as a router generates the network preﬁx.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_17

The rest of this paper is organized as follows: In the next section, we present a related
work to our domain. In Sect. 3, we study an overview of RFID while in Sect. 4; we study
our method and solution for the problem. A conclusion and perspectives are provided
in last section.
2
Related Work
Many objects used in our life don’t have microprocessors and therefore, cannot join a
computer network. Using RFID technology and tag integration in objects, this commu‐
nication will be possible.
Dominikus et al. [2] proposed an approach to integrate RFID systems in the Internet
of things, using readers functioning as IPv6 routers. They used a scheme to map the tag
IDs to network addresses. Moreover, they have studied the problem of mobility tag by
using the Mobile IPv6 (MIPv6) [3]. Unlike the work presented by Dominikus et al. [2],
we propose a method that could generate easily the IPv6 address with minimizing
conﬂicts.
The use of hashing techniques to construct an IPv6 address from an EPC (Electronic
Product Code), as opposed by using a compressed EPC format [2], eases practical
implementations and allows the use of the same mapping scheme for all EPC types.
Other approach proposes to provide the tags themselves with the IPv6 protocol stack,
making them able to use IPv6 communication over the Internet whenever close to a
reader [4]. However, it is proved that this proposition represent a great challenge because
several changes are required to the design of existing tags more speciﬁcally providing
tags with an alimentation source. However, these changes make tags too expensive for
integration into the Internet of Things since the price of the tags could easily exceed the
value of the “things” themselves.
In [5], the authors use objects EPCs to create their current IPv6 addresses. Replace‐
ment of the network preﬁx of an IPv6 address by the EPC was suggested, which only
works for 64-bit EPCs. The reader calls the tag to generate IP address from the EPC and
transmits it to the corresponding EPCIS (xxx) server. The reader stores the created IPv6
address as well. The EPCIS server holds information about the current IP address of
particular tags. If someone wants to retrieve data from the tag, the EPCIS server is asked
for the IPv6 address and the tag can be contacted.
In 2008 [6], RFID networking mechanism using Address management agent is
proposed. In this paper, idea of generating unique 48 bits virtual MAC address based
on RFID tag was proposed which can be further used by the DHCP server to allocate
IP address to RFID tag dynamically. As DHCP server assigns IP address dynamically,
after the lease time is ﬁnished, the client has to reclaim for new IP address which may
not be same as given previously by DHCP on other network.
Antonio J. [7] has considered the need to make legacy technologies accessible via
the Internet to an end-to-end addressing mechanism based on IPv6. Thereby, end devices
can be identiﬁed via the IPv6 address, and emerging protocols, such as CoAP
(Constrained Application Protocol), could be accessed by the native functionality. The
proposed addressing proxies carry out this double task: on the one hand, the addressing
A New Method of IPv6 Addressing Based on RFID
193

mapping and, on the other hand, the functionality mapping from web services, such as
RESTful, and the already mentioned CoAP to native protocols. For this purpose,
Antonio J. [8] presents a set of mapping techniques between legacy technologies from
home automation, industrial and logistic areas to IPv6. The mapping oﬀers a homoge‐
neous addressing and identiﬁcation framework for the IoT.
However, we can conclude from the diﬀerent studied works that the translation of
address requires either extra nodes in the network to construct network addresses or
additional functionalities to the RFID readers residing at the network edge.
In our paper, we used another diﬀerent method that could generate easily the IPv6
address with minimizing conﬂicts.
3
RFID in SmObNet
RFID or Radio Frequency Identiﬁcation technology is a method generally used to facil‐
itate the identiﬁcation data and the identiﬁcation of remote objects, using one or more
RFID chips integrated into objects or even, in some cases, implanted in living organisms.
RFID chips can contain an identiﬁer or a number of information and/or additional data.
This system consists of transponders or RFID tags with one or more readers. Become
more and more sophisticated, the technology is now contributing to the development of
the IoT (Internet of Things).
3.1
Object
An object is, above all, a physical entity; for example, a book, a car, an electric coﬀee
machine or a mobile phone. In the precise context of the Internet of things, regardless
of the vision, this object has at least one unique identiﬁer attached to an identity [9]
expressing its immutable properties (type, color, weight, Etc.) and its state, i.e. all its
characteristics can change over time (position, battery level, etc.).
3.2
Basic Principle of RFID Communication
The RFID system consists of a transponder and a radio receiver, more often known as
a tag and reader. Information related to a given object is stored on an aﬃxed tag and
transmitted to a reader over a radio frequency (RF) connection. The reader in turn
connects via wired or wireless networks to servers hosting RFID applications that make
use of transmitted RFID data, and, in the case of supply chain applications, middleware
manages the ﬂow of RFID data between readers and enterprise applications. Figure 1
shows the basic principle of RFID system.
194
A. El Ksimi et al.

Fig. 1. Basic principle of RFID communication
3.3
EPC
Each RFID object is identiﬁed by its Electronic Product Code (EPC) whose length may
vary from 96 to 256 bits, and its namespace that is segmented into four hierarchically
encapsulated partitions as follows:
Table 1 shows the structure of the EPC numbering scheme. An EPC number consists
of four parts. The ﬁrst part is the header which deﬁnes the version of the EPC number
used. The second is the identiﬁer of the EPC-Manager that assigns the EPC number to
the object. The third part is the object class identiﬁer that essentially deﬁnes the object
product type. The last part is reserved for the unique serial number that identiﬁes the
product from other products among the same object class.
Table 1. EPC partitions
Header
EPCManager
Object class
Serial number
36
28
24
36
To compare EPC identiﬁcation with IPv6 addressing, we provide some comparison
features on Table 2.
Table 2. Comparison between EPC and IPv6
Features
EPC
IPv6
Identifying objects
All physical
All network interfaces
Length
96–256 bits
128 bits
Identiﬁes through
Information pointer
Routing address
Identiﬁer assignment
Permanent
Temporary
3.4
RFID Message Format
The objects with RFID technology could exchange messages with readers. The Fig. 2
shows the EPC message header.
Fig. 2. RFID header
A New Method of IPv6 Addressing Based on RFID
195

3.5
IPv6 Basics
The speciﬁcation of IPv6 can be found in RFC 2640 [10]. The most important change
is the augmentation of the address space from 32 bits (IPv4) to 128 bits (IPv6). Based
on the most pessimistic estimates of address assignment eﬃciency, with this address
space is predicted to provide over 1500 addresses per square foot of the earth’s surface,
which certainly seems like it can serve as basis for an Internet of Things.
An IPv6 packet consists of a ﬁxed-size header followed by multiple optional exten‐
sions headers and the data portion. The basic IPv6 header has a ﬁxed size of 40 bytes
(including version, traﬃc class, ﬂow label, payload length, next header, hop limit, and
source and destination address).
The Fig. 3 shows the structure of an IPv6 packet [10].
version
Traffic class
Flow label
Payload length
Next 
header
Hop 
limit
Source address (128 bits)
Destination address (128 bits)
Fig. 3. The structure of an IPv6 packet
3.6
IPv6-RFID Mapping
The Fig. 4 shows the mapping of IPv6 and RFID:
Fig. 4. The mapping IPv6-RFID
4
Proposed Algorithm
The principle of our approach consists in the use of the logical operator “AND INCLU‐
SIVE” between the network preﬁx which is the same as reader preﬁx and the tag ID to
construct IPv6 address of the tag (i.e. of the object).
196
A. El Ksimi et al.

4.1
The XNOR Operator
The logical operator AND inclusive also called inclusive coincidentally (or logical
equivalence) or exclusive NOR (XNOR = NOT (OR exclusive) can be deﬁned by the
following sentence:
“The output is TRUE if and only if both inputs are identical”.
A logical function translates the relationship between the logical states of the input
and output variables.
The practical realization of logical functions implements binary operators or logical
operators.
In a binary operator, the input and output variables; variables are binary variables
interconnected by deﬁned functions which characterize the function of the binary oper‐
ator.
In our case, we have used the function XNOR.
We can note that this is the negation of XOR often noted XNOR.
Call A and B both operands considered. Agree to represent their value as well:
1 = TRUE and 0 = FALSE.
The operator XNOR is deﬁned by its truth table, indicating, for all possible values
of A and B, the value of the result S (Table 3):
Table 3. The truth table of XNOR
A
B
S = A XNOR B
0
0
1
0
1
0
1
0
0
1
1
1
4.2
Proposed Approach
In this section, we deﬁne the algorithms used in our approach.
In our case, the requirements were quite basic as we just wanted to be able to read
the unique ID on a RFID tag. We then store all the information on the Raspberry (xxx)
Pi rather than trying to read and write IPv6 address onto the RFID tag following the
proposed method.
The bellow ﬁgure shows the connections between the RFID reader and Raspberry
Pi (Fig. 5).
A New Method of IPv6 Addressing Based on RFID
197

Fig. 5. The connections between RFID reader and Raspberry
4.3
Rc522 Rﬁd
The RC522 is a highly integrated reader/writer RFID for contactless communication at
13.56 MH. The RC522’s internal transmitter is able to drive a reader/writer antenna
designed to communicate with ISO/IEC 14443 A/MIFARE cards and transponders
without additional active circuitry. The receiver module provides a robust and efficient
implementation for demodulating and decoding signals from ISO/IEC 14443 A/MIFARE
compatible cards and transponders. The digital module manages the complete ISO/IEC
14443 A framing and error detection (parity and CRC) functionality.
4.4
Raspberry PI
Raspberry Pi (RPi) is a computer with credit-card size also called SBC (single-board
computers) that includes Ethernet, USB and HDMI interfaces that can provide high-
deﬁnition video and enough capacity to run a Linux OS with GUI interface [11].
4.5
Reading the Tag ID
The code below shows the steps required to check for a card and read the UID (xxx).
The UID is made up of a list of 4 values which are printed on a single line:
198
A. El Ksimi et al.

4.6
CRC-32 Algorithm
This algorithm computes the CRC-32 checksum value of the RFID tag ID stored in
vector ID. The elements of ID are interpreted as unsigned bytes (uint8). The result is an
unsigned 32-bit integer (uint32). Polynomial bit positions have been reversed, and the
algorithm modiﬁed, in order to improve performance.
We used the hash function to construct the IPv6 address of the tag from its ID
according to its length.
4.7
Our Approach
In this section, we propose a mechanism to generate an IPv6 for RFID tags from Tag
ID. For that, we will use the XNOR operator with the two operands that are the network
preﬁx and the tag ID.
In our case, the RFID reader plays the role of a router. In other word: reader
preﬁx = network preﬁx.
Based on the length of the tag ID, three scenarios are present:
1. The length of the tag ID is less than 64 bits;
2. The length of the tag ID is equal to 64 bits;
3. The length of the tag ID is more than 64 bits.
A New Method of IPv6 Addressing Based on RFID
199

With this:
• For the ﬁrst case, in order to get the 64 bits, we apply the hash function CRC-32.
Then, we add to the tag ID left zero-padding;
• In the second case, we use directly the 64 bits of the tag ID;
• In the third case, the length of the ID exceeds 64 bits, so we have to use the hash
function CRC-32 and we apply the mapping format presented in Fig. 6.
Fig. 6. General format of the host ID part for RFID
To construct the IPv6 interface identiﬁer of the tag, we apply the format deﬁned in
the Fig. 6.
The ﬁelds from the Fig. 6 have the following meaning:
• A Technology ID Code for identiﬁcation of the legacy protocol.
• U/L bit: indicates if the address is universal (in the format IEEE) or local one for a
locally administered address; to set it to 0 to avoid conﬂicts with EUI-64 mapped
addresses.
• I/G bit: Indicates if the address is individual, in which case the bit is 0 (for a single
machine, unicast) or group (multicast or broadcast), passing the bit to 1 (to set it to
0 because the address is unicast).
• A Reserved ﬁeld: could be used in the future for the identiﬁcation of diﬀerent inter‐
faces for a same technology (in the same subnetwork), avoiding intra protocol
aliasing.
• Tech. mapping: direct mapping (hashing) of the interface identiﬁer.
• EUI-64 ﬁeld: to “0x0000” to avoid conﬂicts with EUI-64 interface identiﬁers.
• Deﬁnition of LSB and MSB:
• MSB: “most signiﬁcant byte” is the byte (or octet) in that position of a multi-byte
number which has the greatest potential value.
• LSB: “least signiﬁcant byte” is the byte (or octet) in that position of a multi-byte
number which has the least potential value.
• IPv6 address construction phase
After obtaining the ID with 64 bits, we use the XNOR operator with the network
preﬁx to generate the IPv6 address of the tag.
The Fig. 7 shows an example of IPv6 network address construction using our
proposed approach:
200
A. El Ksimi et al.

Fig. 7. Example of IPv6 network construction using the proposed approach
Let assume that the preﬁx of the network is 2001:16d8:dd92: aaaa.
The following chart shows the algorithm of our approach:
We suppose that:
• F is a function deﬁned by: F: (x, y) → x XNOR y;
• Example:
Given x = 1011 and y = 0110 then
F(x, y) = x XNOR y = 1011 XNOR 0110
Following the Truth table of XNOR, F(x, y) = 0010
• F is reversible; it means that the ID of tag is easily recovered from its IPv6 address;
According to this property “C = A XNOR B => B = A XNOR C”.
4.8
IPv6 Autoconﬁguration
IPv6 Auto-conﬁguration means that an object gets all the necessary information to
connect to a local network IP without human intervention. The whole process is called
stateless address (auto) conﬁguration and speciﬁed in RFC 4862[12] and 4861[13].
In our approach, we used this process to generate the IPv6 address of the tag.
4.8.1
Creation of the Link-Local Address
IPv6 link-local addresses are a special scope of address which can be used only within
the context of a single layer two domain. Packets sourced from or destined to a link-
local address are not forwarded out of the layer 2 domain by routers.
Link-local addresses are deﬁned in Sect. 2.5.6 [14] as having a ten-bit preﬁx of
0xfe80 followed by 54 zero bits and a 64-bit interface ID.
In our case the link local address of a tag will be written in the following form: FE80::
[interface ID], knowing that interface ID is the suﬃx of the IPv6 address, it is constructed
in Sect. [5].
4.8.2
Creation of a Global Unicast Address
A global unicast address is simply what we call a public IP address in IPv4—that is, an
IP address that is routed across the whole Internet.
The construction of the global address is made in the algorithm Fig. 8.
A New Method of IPv6 Addressing Based on RFID
201

Fig. 8. The ﬂow chart of the proposed approach using XNOR operator
We can see that our approach allows:
• Avoiding address conﬂicts in the network;
• Generating routable addresses easy to derive from the tag ID;
• Having also the possibility to ﬁnd the tag ID that corresponds to a given IPv6 address.
• The code generated by the function XNOR is unique.
4.9
Comparative Synthesis of the Various Approaches
4.9.1
Address Construction Method via Cryptographically Generated Addresses
(CGA)
CGA are IPv6 addresses for which the interface identiﬁer is generated by computing a
cryptographic one-way hash function from a binary input such as e.g. a public key [15].
So a CGA namespace for RFID tags is created using the EPC of a tag and the net ID as
input. Then, an Encode64 function is used to extract 64 bits from the hash. The hashing
result is used as the host ID and it is appended to the net ID in order to construct the
IPv6 address. On one hand, the advantage of the hash function is that it allows generating
a code with ﬁxed length with no need to change the design of existing RFID technology.
202
A. El Ksimi et al.

✓ The weaknesses of this method
• It is possible that two diﬀerent inputs (EPC) within the same context or within
diﬀerent contexts may map to the same CGA and resulting to collisions.
• CGAs work one-way, meaning that it is not possible to create directly the original
identity from the hash.
4.9.2
Eﬀectiveness of Our Method
• The generated address will be a routable address since the Preﬁx of the network is
always preserved.
• The ID of tag is easily recovered from its IPv6 address because of this property
“C = A XNOR B => B = A XNOR C”.
• The address is unique which include that the collision problem is not probable as
well as the risk that a tag can have two diﬀerent addresses if present in the interrog‐
ation range of two readers is out of question.
5
Conclusion
In this paper, we have proposed an approach to derive address IPv6 from RFID tag ID.
We have used the logical operator AND inclusive and hash function. This method
ensures its eﬀectiveness compared to other methods because it allows minimization of
IPv6 address conﬂicts in the network. In the future work, we will study the case when
an object with RFID tag ID is within the frequency range of many readers
References
1. Hahm, O., Baccelli, E., Petersen, H., Tsiftes, N.: Operating systems for low-end devices in
the internet of things: a survey (2015)
2. Dominikus, S., Aigner, M., Kraxberger, S.: Passive RFID technology for the internet of things.
In: 2010 International Conference for Internet Technology and Secured Transactions
(ICITST), pp. 1–8, November (2010)
3. Johnson, D., Perkins, C., Arkko, J.: Mobility support in IPv6, RFC 6275, July 2011. http://
tools.ietf.org/html/rfc6275
4. Barisch, M., Matos, A.: Integrating user identity management systems with the host identity
protocol. In: IEEE Symposium on Computers and Communications, ISCC, pp. 830–836
(2009)
5. Piispanen, M.: EPC and IPv6-based discovery services (2011)
6. Yoon, D.G., Lee, D.H., Chang, H.S., Choi, S.G.: RFID networking mechanism using address
management agent. In: 4th Networked Computing and Advanced Information Management
(NCM2008), vol. 1, pp. 617–622 (2008)
7. Jara, A.J., Moreno-Sanchez, P., Skarmeta, A.F., Varakliotis, S., Kirstein, P.: IPv6 addressing
proxy: mapping native addressing from legacy technologies and devices to the internet of
things (IPv6). Sensors 13, 6687–6712 (2013)
8. Rahman, F.L., Reaz, M.B.I., Ali, M.A.M.: Beyond the Wiﬁ: introducing RFID system using
IPv6. In: Proceedings of ITU-T Kaleidoscobe 2010, pp. 1–4 (2010)
A New Method of IPv6 Addressing Based on RFID
203

9. Atzori, L., Iera, A., Morabito, G.: The internet of things: a survey. Comput. Netw. 54(15),
2787–2805 (2010)
10. RFC 2460, Internet Protocol, Version 6 (IPv6) Speciﬁcation
11. Alex, N.R.: Internet of things implementation with Raspberry Pi (2014)
12. Buenaventura, F.J., Gonzales, J.P., Lu, M.E., Ong, A.V.: IPv6 stateless address
autoconﬁguration (SLAAC) attacks and detection (2015)
13. Shelby, Z., Chakrabarti, S., Nordmark, E., Bormann, C.: Neighbor discovery optimization for
IPv6 over low-power wireless personal area networks (6LoWPANs), RFC 4944 (2012)
14. Hinden, R., Deering, S.: IP version 6 addressing architecture (2016)
15. Jensen, S.E.H., Jacobsen, R.H.: Integrating RFID with IP host identities (2013)
204
A. El Ksimi et al.

A New Alert Message Dissemination Protocol
for VANETs and Simulation in a Real Scenario
Hamid Barkouk
(✉) and El Mokhtar En-Naimi
(✉)
LIST Laboratory, Department of Computer Science, FST of Tangier,
Abdelmalek Essaâdi University, Tangier, Morocco
barkouk@gmail.com, ennaimi@gmail.com
Abstract. VANETs (Vehicular Ad hoc Networks) constitute a new form of
mobile ad hoc networks (MANET). VANETs provide passengers and drivers with
the comfort and security [1, 2].
The number of vehicles in the world is increasing quickly and with it the
number of road traﬃc accidents. As result, the number of deaths is on the rise.
Thus creating a safety system on the roads is an obligation.
To reduce the high number of traﬃc accidents, academic institutions, indus‐
tries and governments have invested large eﬀorts by developing new software
and hardware solutions. One of most important types of protocols in VANETs
networks that improve roads security, are the alert message dissemination proto‐
cols. Vehicles that detect the event in the road send a broadcasting alert message
to neighbor vehicles for helping them know the status of traﬃc [1, 11].
The main goal of our work is ﬁrstly to integrate the alert functionality to one
of existing VANETs routing protocols that is the LEACH protocol (only the basic
routing functionality are used). The procedures that we have integrated to the
LEACH protocol ensure the following features:
–
Event point detection
–
Search for close neighbors
–
Sending of messages to nearby neighbors
–
Communications From vehicles to infrastructure
–
Communications From infrastructure to vehicles
The second goal is to simulate our protocol functionality on a part of a real
map of Tangier and to examine the impact of varying density on the number of
sent/received packets and average delay.
The results of our work show that our proposed algorithm is reliable in view
of the number of packets sent/received and of the low average delay.
Keywords: LEACH · VANET · Routing · Move · Sumo · NS2 · ITS
1
Introduction
Vehicular Ad Hoc Network (VANETs) are self-organizing networks that operate auton‐
omously and without the need for a ﬁxed infrastructure. VANET is a technology that
integrates wireless networking, cellular technology and wireless-LAN to achieve
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_18

vehicle communication infrastructure for intelligent transportation systems (ITS). ITS
will oﬀer the ability to deploy a wide variety of applications. Some of these applications
aim to improve road safety. Others aim to make the travel more comfortable for users
[1, 3].
One of the challenges of VANET networks is the eﬃcient and timely dissemination
of information with reasonable use of resources. Road safety applications are the main
motivation of vehicle networks and have two major requirements, speed and reliability.
Multi-hop communication is an important component of these applications such as road
accident alerts, Alerts for road works, meteorological information, sudden vehicle
braking, etc. One of these services is already implemented in some current cars. This is
the SOS service, which, in the event of an accident, sends a message to warn the nearest
emergency center [4, 12].
Alert message delivery systems are part of the set of security protocols developed
for VANET networks. These existing warning systems consist of two components, the
detection and recognition of the obstacle. In a V2V alert system, diﬀerent types of
information are collected to help the system and the driver to make appropriate decisions
[6]. Alert message, is generated by the following steps (Fig. 1):
Detect ob-
stacle
or risk of 
collision
Generate  
alert 
messages
Alert 
the driver
Fig. 1. Steps of Alert message
The ﬁgure below shows a dissemination example of alerts on VANET networks
(Fig. 2):
Fig. 2. Dissemination alerts on VANET network [7]
206
H. Barkouk and E.M. En-Naimi

Section 2 is a brief presentation of LEACH protocol. Section 3 is a detailed presenta‐
tion of our algorithm. Sections 4 and 5 show the results of the simulation of our algorithm.
2
Brief Introduction to LEACH Protocol
LEACH (Low-Energy Adaptive Clustering Hierarchy) is a hierarchical routing protocol,
using a clustering process that divides the network into two levels: cluster heads and
member nodes. The goal of LEACH is to lower the energy consumption required to
create and maintain clusters in order to improve the lifetime of a wireless sensor network
(Fig. 3).
Fig. 3. LEACH protocol schema [19]
The protocol uses round as unit. Each round consists of two phases: setup phase and
steady phase [13].
Setup phase: The goal of this phase is election of cluster head (CH), a node randomly
picks a number between 0 to 1, compared this random number to the threshold values
T(n), if the number is less than T(n), then it becomes cluster head in this current round,
otherwise it become common node (not CH).
Threshold T(n) is determined by the following:
T(n) =
⎧
⎪
⎨
⎪⎩
p
1 −p
[
r ∗mod
(
1
p
)]
if n ∈G
0
otherwise
A New Alert Message Dissemination Protocol for VANETs
207

– G denotes the set of nodes that are not selected as a cluster head in last 1/P rounds.
– r is the current round.
– P is the desired percentage to become a cluster head.
Once the node is elected as a cluster head, it advertises this to all its neighbors. The
non-cluster head nodes receive the cluster head advertisement and then send join request
to the cluster head informing that they are the members of the cluster under that cluster
head, based on the signal strength of the advertisements from the cluster heads.
Communications within a cluster can be done using the TDMA (Time Division
Multiple Access) method. For this purpose, a TDMA schedule allows the cluster head
assign to each node a time slot for the transmission of its data. In order to save energy,
nodes that are not cluster-head are active only during their transmission time, the rest
of the time they their radio is on standby.
Steady phase: During this phase, the cluster nodes can begin sensing and transmitting
data to the cluster heads. The member sensors in each cluster communicate only with
the cluster head via a single hop transmission. The cluster head then aggregates all the
data collected from the nodes in their cluster and forwards this data to the base station
either directly or via other cluster head. After a certain period is spent in the steady phase,
the network goes into the setup phase again.
3
Proposed Algorithm
3.1
Description of Proposed Algorithm
Steps of the algorithm are described below [8, 14] (Fig. 4):
Fig. 4. Algorithm steps
208
H. Barkouk and E.M. En-Naimi

3.2
Vehicle to Vehicle Communication Proposed Algorithm
As described in the abstract, all procedures below have been integrated in LEACH
protocol [9].
Each vehicle has a GPS receiver for localization of the event point and other vehicles
in the network.
Procedure 1: Detection of Event point
incircle1 : distance between the current vehicle and the event point.
K: X coordinate of the event point.
B: Y coordinate of the event point.
x : X coordinate of the current vehicle.
y : Y coordinate oddddf the current vehicle.
nodeID : current vehicle near the event point.
Now : current time.
round : tour
next_change_time_ : time of the end of round 1 and the beginning of the next round.
Procedure detect_event
Begin 
Obtaining coordinates X and Y of the current vehicle. Calculates the distance between  
the current vehicle and the event point.
If (incircle1 < 200)
If(x < K || y < B)
# the current vehicle detects that it is close to the event point, it will search its close
neighbors and send them the alert message.  
Call the procedure NeighborsNode
EndIf
EndIf
Increment round
for each next_change_time_ repeat these steps.
End
A New Alert Message Dissemination Protocol for VANETs
209

Procedure 2: search of close neighbors
upstream_: neighbors list.
x : X coordinate of the sender vehicle.
y : Y coordinate of the sender vehicle.
nx : X coordinate of other vehicles.
ny : Y coordinate of other vehicles
incircle : distance between the sender vehicle and the other nodes.
nodeID : ID of the sender vehicle.
max_dist : the maximum distance among the distances of the close neighbors.
i : ID of other vehicle.
NextHope : variable used to store the identifier of the neighboring vehicle.
dist_ : variable used to save the incircle distance.
# dist_ is initialized to 0
dist_ = 0
Begin
# Assign to  each sender node a unique code.
If (nodeID != i)
If (((nx == x) && (ny < $y)) || ((ny == y) && (nx <x)))
If (incircle <200)
If (dist_ < incircle)
max_dist = incircle
else
max_dist = dist_
EndIf
NextHope = i
dist_ = incircle
# sender vehicle adds the vehicle (NextHope) to its
list of neighbors
upstream_
EndIf
EndIf
EndIf
# If the list of neighbors is obtained, thesender vehicle call the SendData 
procedure to its neighbors in multicast mode with arguments: lists of neigh
bors and the distance between the sender and these vehicles.
If(upstream_ != "")
Call proceduresendData (upstream_ max_dist)
EndIf
Dist_ = 0
End
210
H. Barkouk and E.M. En-Naimi

Procedure 3: Sends alerts to near neighbors
# variables used
Liste : list of neighbors.
nodeID : ID of sender vehicle
msg : message to send.
dist_ : the distance between the sending vehicle and the receiving vehicle.
SenderCode : code of sender vehicle.
Sender vehicle « nodeID » send the message   « msg » to close neighbors
« Liste » by using send procedure ofLeach protocol.
This procedure takes as a parameter the mac address of the sender vehicle, 
sender code SenderCode , the distance that separates it from neighboring 
dist_, and the message to send rmsg.
3.3
Vehicle to Infrastructure and Infrastructure to Vehicle Communication
Procedure 4: Vehicle to Infrastructure communication proposed algorithm
To add the V2I communication in our proposed algorithm, we have added one
condition on alert message receiver.
# Changes to NeighborsNode procedure.
distB : distance between the sending vehicle and the base station SB
upstream_: list of neighbors sender vehicle.
dist_ : the distance between the sender node and the receiver vehicle.
If (upstream_ != "")
If (distB < dist_ )
# the sender vehicle is close to the SB
Call of the procedure sendDataToBS
else
call the procedure sendData (upstream_ dist_)
EndIF
Else
# the sender vehicle has no close neighbors
Call the proceduresendDataToBS
EndIF
Procedure 5: Infrastructure to Vehicle communication proposed algorithm
After that the sender vehicle has sent the alert message to the base station, it is
broadcast to all network vehicles by eliminating the sender vehicle.
A New Alert Message Dissemination Protocol for VANETs
211

procédure BsSendData
sender : the ID of the sender vehicle of the alert message.
bsID : the identifier of the base station SB.
dist : distance between the vehicle and the SB
send_to : the vehicle to which the message will be send.
Begin
If (send_to != sender)
The base station transmits the alert message to all 
network vehicles
EndIf
End
So that, the vehicle can receive the broadcasted alert message by the base station;
we created a procedure, recvDataBS that takes as a parameter the message sent. This
procedure BsSendDatahas the same structure as the RecvData procedure of Leach, but
we changed the type of messages that the node can distinguish it from the message sent
by the nodes.
4
Simulations
Sending the alert message is a critical action in VANETs applications, it must be ensured
and in real time. This means that all messages sent must be received and in a very short
time, otherwise the message will not be useful.
To test the reliability of our algorithm, we will proceed in this part of our article to
the analysis of our results of sending, receiving and of average delay, generated by a
Tangier-Morocco real map traﬃc simulation.
Average Delay: Is the time taken by a packet to route through the network from a
source to its destination. The average of the lowest end-to-end shows the best perform‐
ance of the routing protocol. It is calculated as follows [2, 5, 15]:
Average Delay (ms) =
∑(RPT −PTT)
∑RP
* 1000
PRT: Packet Reception Time
PTT: Packet Transmission Time
RP: Received Packets
212
H. Barkouk and E.M. En-Naimi

4.1
Simulation Tools
Performance evaluation for our protocol was carried by using the following
environment:
• Open-Street-Map service as our source of network schema.
• Ubuntu 10.04 as operation system.
• SUMO-2.92, MOVE-0.12.3 as generators of mobility [17, 18].
• NS2.34 as network simulator [10, 11, 16, 18].
4.2
Simulation Scenarios Schema
We used Open Street Map service to extract the real network schema below that is a
part of Tangier-Morocco map (Fig. 5).
Fig. 5. Network schema (Iberia – Tangier_Morocco)
4.3
Simulation Parameters
Network parameters we have used for our simulation purpose are shown in the table
below [2, 5] (Table 1).
Table 1. Simulation parameters
Parameter
Font size and style
Topology area
1100 m * 850 m
Protocol
Alert algorithm based on Leach
Simulation time (s)
600 s
Mobile nodes
70, 90
Initial energy
300 J
A New Alert Message Dissemination Protocol for VANETs
213

5
Simulation Results and Analysis
5.1
Results of Sent Packets
Case of 70 vehicles See Fig. 6.
Fig. 6. Number of sent packets in the case of 70 vehicles
Case of 90 vehicles See Fig. 7.
Fig. 7. Number of sent packets in the case of 90 vehicles
214
H. Barkouk and E.M. En-Naimi

These results illustrated in the graph and the content of trace ﬁle of the simulation,
indicate that the communication between the nodes is reliable. Every change of time,
we ﬁnd that the vehicles react and send an alert to its near neighbors or to the base station.
5.2
Results of the Received Packets
Case of 70 vehicles See Fig. 8.
Fig. 8. Number of received packets in the case of 70 vehicles
Case of 90 vehicles See Fig. 9.
Fig. 9. Number of received packets in the case of 90 vehicles
A New Alert Message Dissemination Protocol for VANETs
215

In the two cases, 70 V and 90 V we have noticed that the number of received packets
is at the same degree as that of the packets sent.
5.3
Results of Average Delay
For 70 vehicles See Fig. 10.
Fig. 10. Average delay vs. number of Received packets in the case of 70 vehicles
For 90 vehicles See Fig. 11.
Fig. 11. Average delay vs. number of received packets in the Case of 90 vehicles
216
H. Barkouk and E.M. En-Naimi

We have noticed that the average delay does not exceed two milliseconds in both
simulations, with 70 vehicles and 90 vehicles. The packets arrive at the destination at a
short time.
We notice that the proposed protocol is well suited to dense networks because the
average time in the case of 70 vehicles is often higher than in the case of 90 vehicles.
6
Conclusion and Perspectives
In this study, we have ﬁrstly developed and integrated à new functionality to LEACH
protocol, and secondly we have simulated the new protocol and analyzed the results.
In one of our research works [14], we have simulated our new alert protocol on a
manual schema and we have noticed the same behavior in all studied cases.
By studying the new protocol in VANET, we have noticed that further performance
evaluation is required to verify performance of our protocol with other alert protocols
by using diﬀerent traﬃc scenarios and various metrics. This will be the object of our
next research work.
References
1. Salameh, N.: Conception d’un système d’alerte embarqué basé sur les communications entre
véhicules. Thèse, Institut National des Sciences Appliquées de Rouen, November 2011
2. Singh, A., Verma, A.K.: Simulation and analysis of AODV, DSDV, ZRP in VANET. IJFCST
3(5) (2013)
3. Kumar, A., Tyagi, M.: Geographical topologies of routing protocols in vehicular ad hoc
networks – a survey. IJCSIT 5(3), 3062–3065 (2014)
4. Busanelli, S., Rebecchi, F., Picone, M., Iotti, N., Ferrari, G.: Cross-network information
dissemination in vehicular ad hoc networks (VANETs): experimental results from a
smartphone-based test bed. Future Internet 5, 398–428 (2013)
5. Barkouk, H., En-Naimi, E.: Performance analysis of the Vehicular Ad hoc Networks
(VANET) routing protocols AODV, DSDV and OLSR. In: 5th International Conference on
Information & Communication Technology and Accessibility (ICTA 2015), Marrakech,
Morocco, 21–23 December 2015
6. Srinivetha, R., Gopi, R.: Alert message dissemination protocol for VANET to Improve.
IJETAE J. 4(1) (2014)
7. http://www.spiroprojects.com/blog/cat-view-more.php?id=29. Accessed 30 Mar 2017
8. Beydoun, K.: Conception d’un protocole de routage hiérarchique pour les réseaux de capteurs.
Thèse, L’U.F.R DES SCIENCES ET TECHNIQUESDE L’UNIVERSITE DE FRANCHE-
COMTE, Décembre 2009
9. Yin-fei, D., Ying-yong, Z., Nian-feng, L.: Research overview on vehicular Ad Hoc networks
simulation. IJCA 8(3), 207–216 (2015)
10. Paul, B., Ibrahim, M., Abu Naser Bikas, M: Performance evaluation of AODV & DSR with
varying pause time & node density over TCP&CBR connections in Vanet. IJCSNS 11(7)
(2011)
11. El Alit, F.: Communication unicast dans les réseaux mobiles. Thèse, Université de
Technologie de Compiègne, Décembre 2012
A New Alert Message Dissemination Protocol for VANETs
217

12. Berradj, A.: Contrôle de la diﬀusion multi-saut pour la dissémination de messages. Thèse,
UT3 Paul Sabatier, Octobre 2015
13. Tian, L., Du, H., Huang, Y.: The simulation and analysis of LEACH protocol for wireless
sensor network based on NS2. In: International Conference on System Science and
Engineering, July 2012
14. Barkouk, H., En-Naimi, E.: Development of an alert message dissemination protocol based
on LEACH protocol to Improve VANET road safety. In: 4th Edition of IEEE International
Colloquium on Information Science and Technology (IEEE-ICIST 2016), Tangier-Assilah,
Morocco, 24–26 October 2016
15. Balandin, S., Andreev, S., Koucheryavy, Y.: Internet of Things, Smart Spaces, and Next
Generation Networks and Systems. Springer, New York (2015). ebook
16. The Network Simulator ns-2. http://www.isi.edu/nsnam/ns
17. Pathan, A.S.K., Monowar, M.M., Khan, S.: Simulation Technologies in Networking and
Communications. CRC Press, Boca Raton (2014)
18. Basagni, S., Stojmenovic, M.C.I., Giordano, S.: Ad Hoc Networking. IEEE Press (2012).
eBook
19. Kone, C.T.: Conception de l’Architecture d’un Réseau de Capteurs sans Fil de Grande
Dimension. Thèse, University of Henri Poincaré, Nancy (2011)
218
H. Barkouk and E.M. En-Naimi

Direction of Arrival in Two Dimensions
with Matrix Pencil Method
Mohammed Amine Ihedrane
(✉) and Seddik Bri
Material and Instrumentation (MIN), Electrical Engineering Department,
High School of Technology EST, Moulay Ismail University, Meknes, Morocco
amine.ihedrane@gmail.com, briseddik@gmail.com
Abstract. Smart antennas have recently received increasing for improving the
performance of wireless radio systems. By using Matrix Pencil (MP) algorithms
the weight of the antenna array can be adjusted to form certain amount of adaptive
beam to track corresponding users automatically and to minimize interference
arising from others by introducing nulls in their direction. The interference can
be suppressed and the desire signal can be extracted. One of the adaptive matrix
pencil algorithm Least Mean Square Algorithm (LMS) is presented on this article
using Uniform Circular Array (UCA) structure. Smart antenna incorporates this
algorithm in coded form which calculates complex weights according to the signal
environment. The eﬃciency of (LMS) algorithm is compared on the basis of
normalized array factor and Root Mean Square Error (RMSE) for mobile commu‐
nication. The results demonstrate clearly that the matrix pencil investigate in this
work is more accurate and stable compared to the published measure.
Keywords: DOA · LMS · MP · RMSE · UCA
1
Introduction
Smart antenna has been widely used in many applications such as radar, sonar and
wireless communication systems. Considerable research eﬀorts have been made to esti‐
mate the direction of arrival (DOA) and various array signal process techniques for DOA
estimation have been proposed. In particular, the DOA estimation for uniform circular
arrays (UCAs) has been developed in these scenarios, which desired all-azimuth angle
coverage. By the virtue of their geometry, UCAs are able to provide 360° of coverage
in azimuth plane. Moreover, they are known to be is isotropic. That is, they can estimate
the DOA of incident signal with uniform resolution in the azimuth plane. In addition,
direction patterns synthesized with UCAs can be electronically rotated in the plane of
the array without signiﬁcant change of beam shape.
Another technology that has become equally glamorous is smart antenna technology
[1–3]. In smart antenna technology, a DOA estimation algorithm is usually incorporated
to develop systems that provide accurate location information for wireless services [4].
DOA estimation uses antenna arrays. It is known that antenna radiation main lobe beam
width is inversely proportional to the number of elements in antenna. So, if we consider
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_19

a single antenna then array pattern will be wider and the resolution cannot be good.
Instead of using single antenna, an antenna array system is used in DOA estimation
which will improve the resolution of the received signals (Resolution in DOA estimation
is the ability to distinguish two signals arriving at diﬀerent angles). An array system has
a multiple elements distributed in space.
One major limitation of this method is poor resolution that is its ability to separate
closely spaced signals. Unlike conventional methods, subspace methods exploit the
information of the received data resulting in high resolution. Two main subspace based
algorithms are Multiple Signal Classiﬁcation and Estimation of Signal Parameters via
Rotational Invariance Techniques [5, 6].
Many techniques involve modiﬁcation of the covariance matrix through a prepro‐
cessing scheme called spatial smoothing [7]. Sarkar and Hua [8, 9] utilized the MP to
get the DOA of the signals in a coherent multipath environment. In the MP method,
based on the spatial samples of the data, the analysis is done on a snapshot-by-snapshot
basis, and hence is computationally quite eﬃcient. A snapshot is deﬁned as the voltages
measured at the feed points of all the antenna elements in the array at a particular instance
of time.
In this article, we present a DOA estimation procedure for M correlated signals
impinging on a uniform circular array of N elements using matrix pencil. The stochastic
gradient algorithm LMS (Least Mean Squares) was used to guide the radiation pattern
of the antenna to the desired signal. For the circular array, we have analyzed the
performances of the proposed algorithms (number of antenna elements, number of
snapshots and spacing between elements) and compared them with the published
measure.
2
Matrix Pencil Method
2.1
1-D Matrix Pencil
The Matrix Pencil formulations use the real matrices to estimate the DOA of multiple
signals simultaneously impinging on the ULA [10, 11]. The vector x(n) is the set of
voltages measured at the feed point of antenna element of the ULA. Therefore, x(t) can
be modeled by a sum of complex exponentials.
The observed voltage is given by:
y(t) = x(t) + n(t) =
∑N
i=1 Riesit + n(t)
(1)
y (t) = observed voltages at a specific instance t, n(t) = noise associated with the observa‐
tion and x(t) = actual noise free signal. Therefore, one can write the sampled signal as,
y(P) =
∑N
i=1 Riz p
i + n(p), for p = 0, 1, … , N −1
(2)
where,
220
M.A. Ihedrane and S. Bri

Zi = e
j2𝜋
𝜆
d sin(𝜃)
for i = 1, 2, … , N
(3)
In this presentation, it has been assumed that the damping factor α0 = 0. The objective is
to find the best estimation for θ. Let us consider the matrix Y which is obtained directly
from x(p). Y is a Hankel matrix, and each column of Y is a windowed part of original data
vector, {x(0) x(1) x(2) … x(N − 1)}.
Y =
⎡
⎢
⎢
⎢⎣
x(0)
x(1)
… x(L −1)
x(1)
x(2)
…
x(L)
⋮
⋮
⋱
⋮
x(N −L) x(N −L + 1) … x(N −L)
⎤
⎥
⎥
⎥⎦
(N −(L + 1)x(L))
(4)
The parameter L is called the pencil parameter, it is chosen between N/3 and N/2 for effi‐
cient noise filtering [10, 11]. The variance of the estimated values of Ri and Zi will be
minimal if the values of L are chosen in this range [12], Then our synthesis was to choose
an L closest to N/2 in order to eliminate the unavailable lobes in the simulation. The next
step is to define two sub-matrix of Y, denoted as Ya and Yb. We will generate these
matrices by deleting a single row of Y for each sub-matrix. Let us delete the last row of Y
to form Ya and the first row of Y to form Yb. This yields the matrices Ya and Yb.
Ya =
⎡
⎢
⎢
⎢⎣
x(0)
x(1)
… x(L −1)
x(1)
x(2)
…
x(L)
⋮
⋮
⋱
⋮
x(N −L −1) x(N −L) … x(N −2)
⎤
⎥
⎥
⎥⎦
(N −L)x(L)
(5)
Yb =
⎡
⎢
⎢
⎢⎣
x(1)
x(1)
… x(L −1)
x(2)
x(2)
…
x(L)
⋮
⋮
⋱
⋮
x(N −L) x(N −L + 1) … x(N −1)
⎤
⎥
⎥
⎥⎦
(N −L)x(L)
(6)
We can also write
Ya = ZaR Zb
(7)
Yb = ZaR0Z0Zb
(8)
where the matrices Za, Zb, R0, and Z0 are defined as follows:
Direction of Arrival in Two Dimensions
221

Za =
⎡
⎢
⎢
⎢⎣
1
1
…
1
Z1
Z2
…
ZM
⋮
⋮
⋱
⋮
Z(N−L−1)
1
Z(N−L−1)
2
… Z(N−L−1)
M
⎤
⎥
⎥
⎥⎦
(N −L)x(L)
(9)
Zb =
⎡
⎢
⎢
⎢⎣
1 Z1 … Z(L−1)
1
1 Z2 … Z(L−1)
2
⋮⋮⋱
⋮
1 ZM … Z(L−1)
M
⎤
⎥
⎥
⎥⎦
(10)
R0 = diag [R1, R2, … , RM
]
(11)
Z0 = diag
[
Z1, Z2, … , ZM
]
(12)
Thus, we can express the MP to extract the DOA information as follows:
Yb −𝜆Ya = ZaR0Z0Zb −𝜆ZaR0Zb
(13)
Rewriting (13) in another form, we get
Yb −𝜆Ya = Za −R0
[
Z0 −𝜆I
]
Zb
(14)
Here I is the (M × M) identity matrix it has been shown in [13], one can show that the rank
of Yb − λYa will be M, provided that M ≤ L ≤ N − M [14, 15]. However, if λ = Zi, i = 
1, 2… M the ith row of [Z0 – λI] is zero, and the rank of the matrix pencil becomes
(M × 1). As stated in Ref. [14], the solutions of the pencil λ can thus be found as the
generalized eigenvalues of the matrix pair {Ya, Yb}, or, equivalently, as the eigenvalues of
{Ya
+Yb −𝜆I}, where Ya
+ is the Moore–Penrose pseudo-inverse of Ya, defined by (15).
Y +
a = {Y H
a Ya
}−1Y H
a
(15)
The superscript H denotes the conjugate transpose.
Concerning the noisy data, we form an approximation of the matrix Y from the
components xi(n), i = 0, 1, …, N − 1, of the observed signal vector y(P).
K =
⎡
⎢
⎢
⎢⎣
y(0)
y(1)
… y(L −1)
y(1)
y(2)
…
y(L)
⋮
⋮
⋱
⋮
y(N −L) y(N −L + 1) … y(N −L)
⎤
⎥
⎥
⎥⎦
(N −(L + 1)x(L))
(16)
Singular value decomposition (SVD) is useful to reduce some of the noise eﬀect. The
matrix K obtained by (17) can be written as
222
M.A. Ihedrane and S. Bri

K = U
∑
VH
(17)
where U and V are unitary matrices with columns that are eigenvectors of K KH, respec‐
tively, and ∑ is a diagonal matrix containing the singular values of K in the decreasing
order σ1 ≥ σ2 ≥ … ≥ σmin. Generally, ∑ is not necessarily a square matrix. At this point,
if the incident number of signals is unknown, the procedure suggested in [14] and
described in [15] can be used to estimate M. This can be done by setting M to the largest
i for which 10−r ≈ σi /σmax, where r is the signiﬁcant number of decimal digits in the data
vector y(p). After computing the SVD of the data matrix K, the matrix is divided into
two subspaces, identiﬁed as signal subspace and noise subspace. The matrices Ka and
Kb are constructed from the signal subspace matrix.
DOAs are then extracted from the eigenvalues zi = λi as
θi = sin−1
⎛
⎜
⎜
⎜⎝
Im(log Zi)
2𝜋
𝜆0 d
⎞
⎟
⎟
⎟⎠
(18)
where Zi is deﬁned in (3).
2.2
2-D Matrix Pencil Using Uniform Circular Array
We consider a circular array of isotropic antennas evenly spaced dx and dy respectively
along the axes OX and OY, presented in Fig. 1. The network receives signals Ms with the
angles of incidence (θq, φq), which are respectively φq, θq and the directions of arrival
in elevation and in azimuth [16, 17].
Fig. 1. Uniform circular array with N elements
The values of αx and αy are written in the following form:
Direction of Arrival in Two Dimensions
223

𝛼xi = exp
(
j
(
2𝜋Δ
𝜆0
)
sin θi cos Φi
)
(19)
𝛼yi = exp
(
j
(
2𝜋Δ
𝜆0
)
sin θi sin Φi
)
(20)
The elevation and azimuth are expressed by the following equation:
𝜃i = Arcsin
[(−j𝜆0
2𝜋Δ
)√(Ln 𝛼xi
)2 + (Ln 𝛼yi
)2]
(21)
Φi = Arctg
(Ln 𝛼xi
Ln 𝛼yi
)
(22)
where i = 1, 2, …, Ms.
3
Results and Discussion
In this section, the computer simulation results are given to illustrate the performance
of the matrix pencil method. In order to demonstrate the numerical properties, a compa‐
rative study was made between the matrix pencil and pencil method indicates in
[16–18]. In the following comparison, we compare the LMS investigate at this work and
LMS indicate at [17, 18].
The LMS algorithm used the estimates of the gradient vector from the available data.
LMS incorporates an iterative procedure that makes successive corrections to the weight
vector in the direction of the negative of the gradient vector which eventually leads to
the minimum mean square error. Compared to other algorithms LMS algorithm is
Fig. 2. The array factor using LMS algorithm
224
M.A. Ihedrane and S. Bri

relatively simple; it does not require correlation function calculation nor does it require
matrix inversions.
The array factor plot in Fig. 2 shows that the LMS algorithm is able to iteratively
update the weights to force deep nulls at the direction of the interferers and achieve
maximum in the direction of the desired signal. We observe also that the ﬁnal weighted
array which has a sharp peak at the desired direction of 0° and a null at the interfering
direction of −60°. In Fig. 3 it is observed that the array output acquires and tracks the
desired signal after 20 iterations contrary to the results show at [17, 18].
If the signal characteristics are rapidly changing, the LMS algorithm may not allow
tracking of the desired signal in a satisfactory manner. It is describes the algorithmic
changing the weighting in each iteration from Fig. 4, it shows the graph of signal versus
number of iteration. In which array output acquire and track after 20 iterations. If the
characteristic of the signal rapidly changing, the LMS algorithm may not allow tracking
of the desired signal in a satisfactory manner.
Fig. 3. Acquisition and tracking of desired signal using LMS
Finally, we simulated the MSE error in each iteration plot in Fig. 5 shows the rela‐
tionship between mean square error and number of iteration, The LMS error show that
the LMS algorithm converges. In this case the LMS error is almost 0.014 at around 20
iteration. In which MSE is decreases with iteration and after 20 iteration it become
converged. The LMS algorithm is most commonly used adaptive algorithm because of
its simplicity and a reasonable performance. Since it is an iterative algorithm it can be
used in a highly time-varying signal environment. It has a stable and robust performance
against diﬀerent signal conditions.
Direction of Arrival in Two Dimensions
225

Fig. 4. Magnitude of array weights using LMS algorithm
Fig. 5. The mean square error using LMS algorithm
We compared the Matrix Pencil method investigates in our work and matrix pencil
in [16] for ULA with an SNR taken as 24 dB In Table 1. The MP given in this work
gives a good precision compared to the proposed one indicate at [16]. The authors in
[16] didn’t give the number of snapshots using in this case contrary to the second case
when they use 100 element with 5 faulty elements they used one snapshots to demon‐
strate the eﬃciency of their proposed method, but in our case we have chosen the same
number of snapshots in order to improve the robustness of our method with only one
snapshots even if the number of elements changed or one of them stops working.
226
M.A. Ihedrane and S. Bri

Table 1. Comparative results for matrix pencil for 100 and 95 elements
Number of elements
θin
θout
RMSE
100
Pencil [16]
0
0.0004
0.0019
5
5.0001
10
9.9948
15
15.0014
20
20.0045
30
29.9984
This work
0
0.0000
0.0006
5
5.0001
10
9.9991
15
15.0001
20
20.0000
30
30.0001
95
Pencil [16]
0
0.0067
0.0023
5
5.0024
10
10.0014
15
14.9976
20
19.9965
30
29.9991
This work
0
0.0005
0.0018
5
5.0006
10
10.0008
15
15.0009
20
20.0000
30
29.9999
4
Conclusion
In order to improve the performance of Matrix Pencil using UCA in ﬁnite snapshots and
SNR, the conﬁgurations allows us to work in real time. We analysis the performances
of LMS algorithm for smart antenna systems which are very important for smart antenna
design. The LMS algorithm is compared on the basis of normalized array factor and root
mean square error (RMSE) for Smart Antenna systems. It is observed that an LMS
algorithm is converging after 20 iteration. The attractive quality of LMS algorithm is
less computational complexity then the RMSE. Our ﬁndings are explained in details
above. For the UCA network, the method based on the Matrix Pencil yielded better
results and the side lobe levels are considerably reduced.
Acknowledgements. This work is supported by Moulay Ismail University, Meknes – Morocco.
Direction of Arrival in Two Dimensions
227

References
1. Federal Communications Commission. http://www.fcc.gov/e911/
2. Liberti Jr., J.C., Rappaport, T.S.: Smart Antennas for Wireless Communications: IS-95 and
Third Generation CDMA Applications. Prentice-Hall, Upper Saddle River (1999)
3. Sarkar, T.K.: Smart Antennas. IEEE Press/Wiley-Interscience, New York (2003)
4. Kuchar, A., et al.: A robust DOA-based smart antenna processor for GSM base stations. In:
IEEE International Conference on Communications, vol. 1, pp. 11–16. IEEE Press, New York
(1999)
5. Shauerman, A.K., Shauerman, A.A.: Spectral-based algorithms of direction-of-arrival
estimation for adaptive digital antenna arrays. In: The 9th International Conference and
Seminar on Micro/Nanotechnologies and Electron Devices, Novosibirsk, Russia, pp. 251–
255 (2010)
6. Liao, B., Chan, S.C.: DOA estimation of coherent signals for uniform linear arrays with mutual
coupling. In: Proceedings of the IEEE International Symposium on Circuits and Systems,
pp. 377–380. IEEE Press, Rio de Janeiro (2011)
7. Krim, H., Viberg, M.: Two decades of array signal processing research: the parametric
approach. IEEE Signal Process. Mag. 13(2), 67–94 (1996)
8. Hua, Y., Sarkar, T.K.: Matrix pencil method and system poles. Signal Process. 21(2), 195–
198 (1990)
9. Hua, Y., Sarkar, T.K.: On SVD for estimating generalized eigenvalues of singular matrix
pencil in noise. IEEE Trans. Signal Process. 39(4), 892–900 (1991)
10. Ihedrane, M.A., Bri, S.: Direction of arrival estimation using MUSIC, ESPRIT and maximum-
likelihood algorithms for antenna arrays. Walailak J. Sci. Technol. 13(6), 491–502 (2016)
11. Van Der Veen, A.J., Vanderveen, M.C., Paulraj, A.: Joint angle and delay estimation using
shift invariance techniques. IEEE Trans. Signal Process. 46(2), 405–418 (1998)
12. El Fadl, A., Bri, S., Habibi, M.: Multipath elimination using matrix pencil for smart antenna
with uniform linear array. Eur. J. Sci. Res. 87(2), 397–405 (2012)
13. Yilmazer, N., Jinhwan, K., Sarkar, T.K.: Utilization of a unitary transform for eﬃcient
computation in the matrix pencil method to ﬁnd the direction of arrival. IEEE Trans. Antennas
Propag. 54(1), 175–181 (2006)
14. Hua, Y., Sarkar, T.K.: Matrix pencil method for estimating parameters of exponentially
damped/undamped sinusoids in noise. IEEE Trans. Acoust. Speech Signal Process. 38(5),
814–824 (1990)
15. Sarkar, T.K., Pereira, O.: Using the matrix pencil method to estimate the parameters of a sum
of complex exponentials. Trans. Antennas Propag. 37(1), 48–55 (1995)
16. Yerriswamy, T., Jagadeesha, S.N.: Fault tolerant matrix pencil method for direction of arrival
estimation. Signal Image Process. Int. J. 2, 55–67 (2011)
17. Motiur Rahaman, D.M., Moswer Hossain, M., Masud Rana, M.: Least mean square (LMS)
for smart antenna. Univ. J. Commun. Netw. 1, 16–21 (2013)
18. Singh, S., Kaur, E.M.: A LMS and NLMS algorithm analysis for smart antenna. Int. J. Adv.
Res. Comput. Sci. Softw. Eng. 5, 380–384 (2015)
228
M.A. Ihedrane and S. Bri

A Real-Time Risk Assessment Model for Intrusion
Detection Systems Using Pattern Matching
El Mostapha Chakir1(✉), Mohamed Moughit1,2,3, and Youness Idrissi Khamlichi1,4
1 IR2 M Laboratory, FST, Univ. Hassan 1, Settat, Morocco
chakirsmi@gmail.com, {e.chakir,mohamed.moughit}@uhp.ac.ma
2 IR2 M Laboratory, ENSA, Univ. Hassan 1, Khouribga, Morocco
3 EEA&TI Laboratory, FST, Univ. Hassan 2, Mohammedia, Morocco
4 LERS Laboratory, ENSA, Univ. Sidi Mohamed Ben Abdellah, Fes, Morocco
youness.khamlichi@usmba.ac.ma
Abstract. Intrusion Detection Systems (IDS) are one of the most important tools
in security ﬁeld. The main aim of an IDS is to gather and analyze events from
networks and hosts to identify signs of suspicious traﬃc. Having detected such
signs, they generate alerts to report them. IDSs are known to generate a large
number of false alerts, especially false alerts during the detection. Analyzing the
alerts manually by security administrator need more time and makes it extremely
diﬃcult to correctly identify alerts related to attacks (true positives). In this paper,
we introduce an approach to Intrusion Risk Assessment. The objective is to deter‐
mine the impact of certain events on the security status of a network. In this
approach, we evaluate the risk as a composition of certain parameters of alerts.
Then we tightly integrate the Risk Assessment model with an existing framework,
and we apply the results of the risk assessment to prioritize the alerts produced
by the IDS.
Keywords: Intrusion Detection Systems · Real-Time · Risk assessment · Pattern
matching · False positive · Severity · Reliability · Alerts · Priority
1
Introduction
Intrusion Detection system (IDS) is a security system that monitors the traffic on a
computer or a network system, analyzes the traffic and generates alerts in case any mali‐
cious activity found [1]. Intrusion detection system (IDS) also can be defined as a classi‐
fier which collects the evidences about the presence or not of an any kind of intrusion. The
collected evidences are usually incomplete, uncertain, contradictory or conflicting and may
be complementary [2]. The use of single IDS as a detector is not sufficient to deliver a
complete security solution. It has two major problems: a huge number of false positives
rates and lower intrusion detection coverage. These problems limit the detection perform‐
ance of an IDS in the presence of multiple categories of attack/intrusion.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_20

Organizations need an IDS that can prioritize alerts and provide a level of context
to each alert. Our focus in this work is to improve the detection of intrusions and reduce
the number of false positives by using the Risk Assessment of each generated alerts by
IDS. Risk Assessment (RA) is the process of identifying and characterizing risks of
alerts. The result of RA helps minimize the cost of applying all available sets of
responses. It may be enough in some situations to only apply a subset of available
responses [3, 4]. Risk assessment helps Security analysis to determine the probability
that a detected anomaly is a valid attack that requires attention.
In this paper, we classify existing RA approaches, and we propose our own RA
Model based on our latest work using Pattern Matching [5]. The goal is to improve our
model with a Real-Time Risk Assessment. The rest of this paper is organized as follows:
Sect. 2 Introduces Intrusion Detection Systems. A taxonomy of recent existing RA is
presented in Sect. 3. Section 4, we discuss our proposed model. Finally, Sect. 5 displays
our conclusion.
2
Intrusion Detection System (IDS)
An Intrusion Detection System (IDS) is a software or hardware that monitors the hosts
or activities of network for policy violations or malicious activities and generates alerts
to the security administrator. The main focus of Intrusion detection systems (IDS) is to
identify the possible security breaches, logging information about them and report
them [6].
There are two type of IDS, a network based (NIDS) and host based (HIDS) intrusion
detection systems [6]:
NIDS: are placed at a strategic point within the network to monitor traﬃc to and
from all devices on the network.
HIDS: are run on individual hosts or devices on the network. It monitors the inbound
and outbound packets from the host only and alert the administrator of suspicious activ‐
ities if detected.
There are two type of detection techniques used by IDS, one that detects based on
speciﬁc signatures of known threats, and the other detects based on comparing traﬃc
patterns against a baseline and looking for anomalies [7]:
Signature Based: a signature based IDS monitor packets on the network and
compare them against a database of signatures or attributes from known malicious
threats.
Anomaly Based: an IDS which is anomaly based monitor network traﬃc and
compare it against an established baseline. The baseline will identify what is “normal”
for that network what sort of bandwidth is generally used, what protocols are used, what
ports and devices generally connect to each other, and alert the administrator when
anomalous traﬃc is detected.
IDS systems generate an enormous amount of data. Often there are duplicative events
from various systems, and other alerts that could be characterized as False Positive. A
False Positive is a normal event classiﬁed as an attack. The false positives create a major
problem. They can easily drown out legitimate IDS alerts. A single IDS rule causing
230
E.M. Chakir et al.

false positives can easily create thousands of alerts in a short time. The optimal way to
deal with this problem is to improve the IDS solution by using a Risk Assessment model
that work after alert generation by the IDS. This model takes the generated alerts as input
and calculates the risk of each single alert using diﬀerent parameters extracted from
alerts. Thereby it can help the network administrator focus the eﬀorts on detecting actual
threats.
In this paper, we propose a Real-Time Intrusion Risk Assessment to improve our
model based for Handling Alerts of IDS Based Pattern Matching [5].
3
Taxonomy of Recent Existing Risk Assessment Models
Risk assessment is a central issue in management of networks. Researchers have been
proposing many approaches [3, 4, 8–15, 22] that use an oﬄine Risk Assessment, which
is calculated by assessing all the resources in advance. The value of each resource is
static. Many Real-Time Risk Assessment models have been proposed. According to
[15], these approaches can be grouped into three main categories:
Attack Graph-Based: This approach helps to identify attacks and analyze their
impact on the critical services in the network [3, 4]. The attack graph model can show
the attack paths in a network based on service vulnerabilities [8].
Non-graph-Based: In this approach Risk assessment is carried out independently
of the attack. This means that the IDS can detect an attack and send an alert to the risk
assessment component, thus, performs a risk analysis based on alert statistics and other
information provided in the alert [12, 13].
Service Dependency Graph-Based: For each service in this approach, three prop‐
erties are deﬁned: the conﬁdentiality C(S), integrity I(S), and availability A(S) of service
(S). The impact of the attack on a service is propagated to other services based on the
type of dependency. The attack graph is not used in this type of approach to evaluate
attack cost [11, 16].
Jahnke et al. [3] extended the idea proposed by Toth and Kregel in [17] by using
general, directed graphs showing they proposed a new graph-based approach for
modeling the eﬀects of attacks against resources and the eﬀects of the response measures
taken in reaction to those attacks.
Kanoun et al. [4] proposed a Risk Assessment model based on attack graphs to
evaluate the severity of the total Risk of the system. They used The LAMBDA [18]
language to model attack graphs when an attack is detected. The risk gravity model
begins to compute the risk when an attack graph is obtained, which is a combination of
two major factors: the potentiality, which measures the probability of a given scenario
taking place and successfully achieving its objective, and the impact, which is deﬁned
as a vector with three cells that correspond to the three fundamental security principles:
Availability, Conﬁdentiality, and Integrity. The most interesting point of this model is
that the impact parameters are calculated dynamically. That impact depends on the
importance of the target host, as well as the impact of the level of reduction measures
deployed on the system to reduce and limit the impact, when the attack is successful.
A Real-Time Risk Assessment Model
231

Wang et al. [10] proposed a middle-ware approach systematically integrates attack
graphs and Hidden Markov Models together to bridge the gap between system-level
vulnerabilities and organization-level security metrics. The proposed approach explores
the probabilistic relation between system observations and states. It applies a cost-driven
heuristic algorithm to search for the optimal security hardening from a list of candidates.
A set of security metrics and defense cost factors was speciﬁed in this approach for
calculating attack cost and defense cost. Attack impact was measured by conﬁdentiality
loss, public embarrassment, denial of service, integrity loss and privilege escalation,
while defense cost factors was calculated by operation cost, system downtime, training
cost installation cost, and incompatibility cost.
Mu et al. [12] proposed a non-graph-based Real-Time Risk Assessment approach
based on D-S evidence theory. This theory is a method used for solving a complex
problem where the evidence is incomplete or uncertain. Their model consists of two
steps, which identify: Risk Index and Risk Distribution:
The Risk index is deﬁned as the probability that a malicious activity is a true attack
and can achieve its mission successfully. Five factors are used in D-S evidence theory
to calculate the risk index: Number of alerts, Alert Conﬁdence, Alert Type, Alert
Severity, and Alert Relevance Score. Risk distribution is the real evaluation of risk with
respect to the value of the target host, and can be low, medium, or high.
The Risk distribution has two inputs: the risk index, and the value of the target
host. The latter depends on all the services it provides.
Gehani et al. [13] proposed a non-graph-based real-time risk assessment model
called Rheostat. This model alters dynamically the exposure of a host to contain an
abnormal activity when it occurs. To analyze a system’s risk, a combination of three
factors is considered: the likelihood of occurrence of an attack, the impact on hosts (i.e.,
the loss of conﬁdentiality, integrity, and availability) and the vulnerability’s exposure.
Haslum et al. [16] proposed a fuzzy model for online risk assessment in networks.
They applied Fuzzy logic to capture and automate this process. The fuzzy logic is used
to model threat level, vulnerability eﬀect, and asset value. Threat level (FLC-T) is
modeled using three linguistic variables: Probability of threat success, Intrusion
frequency and Severity. The HMM model used for predicting attacks provides an esti‐
mate of intrusion frequency. The asset value (FLC-A) is derived from three other
linguistic variables: Cost, Criticality, Sensitivity, and Recovery. In addition, the vulner‐
ability eﬀect (FLC-V) has been modeled as a derived variable from Threat Resistance
and Threat Capability. Eventually, the risk is estimated based on the output of the three
fuzzy logic controllers FLC-T, FLC-A, and FLC-V.
Arnes et al. [19] proposed a non-graph-based real-time risk assessment model. This
model is a Multi-Agent system, where each agent observes objects in a network using
sensors. An object is any kind of host in the network that is valuable in terms of security.
Discrete-time Markov chains are used to perform dynamic risk assessment with this
approach. For each object, a Hidden Markov Model (HMM) is used and the HMM states
illustrate the security state which are: Good, Attacked, and Compromised they change
over time.
As we see from the Table 1, the most common approach used for online mode is the
quantitative approach. Quantitative approaches rely on hard numbers, complex
232
E.M. Chakir et al.

calculations, probability theory and statistics to determine the risk exposure and they
may be diﬃcult for non-technical people to interpret [15].
Table 1. Comparison of existing real-time intrusion risk assessment approaches [15]
IRA
Year
Technique used
Quantitative
Qualitative
Hybride
Jahnke et al. [3]
2007 Attack graph
√
Kanoun et al. [4]
2008 Attack graph
√
Wang et al. [10]
2013 Dependancy attack graph
√
Mu et al. [12]
2008 Dempster-Shafer theory
√
√
Gehani et al. [13]
2004 Addition, multiplication,
and division operation
√
√
Haslum et al. [16]
2008 Fuzzy logic
√
Arnes et al. [19]
2005 Hidden Markov Model
√
4
Our Proposed Model
4.1
Overview
In [5] we have proposed a New System Alert Management for IDSs based on a Pattern
Matching algorithm (Fig. 1), which can classify alerts by their importance and reduce
number of false alerts considerably.
To improve our system, we propose in this paper a Real-Time Risk Assessment
System for Intrusion Detection System, we evaluate the risk as a composition of certain
parameters of events and Assets, and we apply the results of the Risk Assessment to
ﬁlter alerts produced by the IDS as High Risk, Medium Risk or Low Risk.
Fig. 1. Our proposed alert management system in [5]
In our proposed system (Fig. 1), we’ve used binary traﬃcs ﬁles of a network KDD
99 dataset [20] instead of real network traﬃcs. The KDD 99 dataset is the most used for
A Real-Time Risk Assessment Model
233

evaluating IDSs according. Snort [21] is used to produce alerts of KDD 99 dataset
network traﬃcs. Snort is an open source signature based IDS which gets KDD 99 online
traﬃc and then generates alert log ﬁles; these ﬁles are entered to our proposed system
as the inputs. A Stateful Pattern Matching algorithm is used to ﬁlter alerts and classify
them to diﬀerent forms.
4.2
Description Our Proposed Model
The process of detection involves three phases (Fig. 2):
Preprocessing Phase: Snort analyzes KDD binary Traﬃc and generates alerts.
Collection Phase: All the information is sent and received at a one location.
Post-processing Phase: In this Phase, once we have all data in one place, we can
implement mechanisms that will improve detection sensitivity and reliability. In our
model, we use three Post-Processing methods:
• Pattern analysis unit: this unit is implemented in our work in [5] using a pattern
matching algorithm. The main objective of this unit is to extract data and classify
alerts into diﬀerent types.
• Risk assessment: Each event is evaluated in relation to its associated risk.
• Prioritization: We prioritize alerts received using the result of the risk assessment
of each single alert.
Fig. 2. Proposed real-time risk assessment model
234
E.M. Chakir et al.

Several parameters make it possible to qualify the level of dangerousness (Risk) of
an alert. It is important to understand their signiﬁcance in order to be able to manage
correctly the alerts according to their level of importance (Table 2):
Table 2. Description of parameters to assess the Risk of Alerts
Parameter
Description
Priority
This level is therefore only dependent on the alert. The value of
this parameter is adjustable between 0 and 5.
The value of the target host This is a value to deﬁne the importance of a host on the network.
This value must be between 0 and 5 (0 = machine Less important,
5 = very important machine). This value is stored into a MySQL
Database for each device of the organization.
Reliability
The term reliability can therefore be translated by the reliability
that an alarm is not a false positive. The value of this parameter is
between 0 and 10. This value is stored into a MySQL Database and
it associated with an independent type of event (IDS Signature).
The severity of alert
Severity level is associated with each generated alert to help us to
know the threat represented by the event, Severity levels is between
0 and 5 (0 = Clear, 5 = Critical).
The Risk Calculation makes it possible to link the four previous parameters by the
following formula in order to calculate the risk of an alert:
Risk Assessment (RA) = (P) ∗(D) ∗(S) ∗(R)
𝟏𝟐𝟓
Noting that the Risk Assessment must not exceed 10, so the 125-value obtained by
calculating the Risk using the Maximum Value of each parameter.
The proposed model estimates the Risk for every single Alert. As we see in Table 2,
the model uses a combination of four decision parameters. The resulting value can be
mapped to the following Risk Categories, (Table 3):
Table 3. Risk Assessment Categories
Risk value
Signiﬁcation
0, 1, 2, 3
Low
4, 5, 6, 7
Medium
8, 9, 10
High
5
Conclusion and Future Work
This paper surveys existing approaches and techniques used for Intrusion Risk Systems.
By reviewing these approaches, we have ﬁnd that the most existing approaches suﬀer
from serious limitations. To improve the detection, we presented a new approach that
evaluates IDS alerts using a new Risk Assessment model based on several parameters
extracted from alerts to calculate the Risk and qualify the level of dangerousness of each
A Real-Time Risk Assessment Model
235

alert. Thereby, we reduce the number of false positives and save time and eﬀort of the
security administrator. The next step of this work is to improve this model by using
advanced functions, and use more sophisticated algorithms.
References
1. Huang, L.C., Hwang, M.S.: Study of intrusion detection systems environment. J. Electron.
Sci. Technol. 4, 6 (2012)
2. Shah, V.M., Agarwal, A.K.: Reliable alert fusion of multiple intrusion detection systems. Int.
J. Netw. Secur. 19(2), 182–192 (2017)
3. Jahnke, M., Thul, C., Martini, P.: Graph-based metrics for intrusion response measures in
computer networks. In: Proceedings of the 3rd LCN Workshop on Network Security. Held in
Conjunction with the 32nd IEEE Conference on Local Computer Networks (LCN), pp. 1035–
1042, Dublin, Ireland (2007)
4. Kanoun, W., Cuppens-Boulahia, N., Cuppens, F., Araujo, J.: Automated reaction based on
risk analysis and attacker’s skills in intrusion detection systems. In: Third International
Conference on Risks and Security of Internet and Systems, pp. 117–124 (2008)
5. Chakir, E., Khamlichi, Y.I., Moughit, M.: Handling alert for intrusion detection system using
stateful pattern matching. In: Proceedings of the 4th IEEE International Colloquium on
Information Science and Technology (CiSt 2016), pp. 139–144 (2016)
6. Goel, R., Sardana, A., Joshi, R.C.: Parallel misuse and anomaly detection model. Int. J. Netw.
Secur. 14(4), 211–222 (2012)
7. Gaikwad, D.P., Jagtap, S., Thakare, K., Budhawant, V.: Anomaly based intrusion detection
system using artiﬁcial neural network and fuzzy clustering. Int. J. Eng. Res. Technol. 1(9)
(2012). ISSN:2278-0181
8. Wang, L., Islam, T., Long, T., Singhal, A., Jajodia, S.: An attack graph-based probabilistic
security metric. In: Proceedings of the 22nd Annual IFIP WG 11.3 Working Conference on
Data and Applications Security (DBSEC08) (2008)
9. Noel, S., Jajodia, S.: Understanding complex network attack graphs through clustered
adjacency matrices. In: Proceedings of the 21st Annual Computer Security Conference
(ACSAC), pp. 160–169 (2005)
10. Wang, L., Liu, A., Jajodia, S.: Using attack graph for correlating, hypothesizing, and
predicting intrusion alerts. Comput. Commun. 29(15), 2917–2933 (2006)
11. Kheir, N., Cuppens-Boulahia, N., Cuppens, F., Debar, H.: A service dependency model for
cost sensitive intrusion response. In: Proceedings of the 15th European Conference on
Research in Computer Security, pp. 626–642 (2010)
12. Mu, C.P., Li, X.J., Huang, H.K., Tian, S.F.: Online risk assessment of intrusion scenarios
using D-S evidence theory. In: Proceedings of the 13th European Symposium on Research in
Computer Security, pp. 35–48, Malaga, Spain (2008)
13. Gehani, A., Kedem, G.: Rheostat: real-time risk management. In: Recent Advances in
Intrusion Detection: 7th International Symposium, (RAID 2004), pp. 296–314, France (2004)
14. Anuar, N.B., Sallehudin, H., Gani, A., Zakaria, O.: Identifying false alarm for network
intrusion detection system using hybrid data mining and decision tree. Malays. J. Comput.
Sci., 110–115 (2008). ISSN:0127-9084
15. Shameli-Sendi, A., et al.: Taxonomy of Intrusion Risk Assessment and Response System, vol.
45, pp. 1–16. Elsevier, San Francisco (2014)
236
E.M. Chakir et al.

16. Haslum, K., Abraham, A., Knapskog, S.: Fuzzy online risk assessment for distributed
intrusion prediction and prevention systems. In: Tenth International Conference on Computer
Modeling and Simulation, pp. 216–223. IEEE Computer Society Press, Cambridge (2008)
17. Toth, T., Kregel, C.: Evaluating the impact of automated intrusion response mechanisms. In:
Proceedings of the 18th Annual Computer Security Applications Conference, Los Alamitos,
USA (2002)
18. Cuppens, F., Ortalo, R.: Lambda: a language to model a database for detection of attacks. In:
Proceedings of the Third International Workshop on Recent Advances in Intrusion Detection
(RAID2000), pp. 197–216, Toulouse, France (2000)
19. Arnes, A., Sallhammar, K., Haslum, K., Brekne, T., Moe, M., Knapskog, S.: Real-time risk
assessment with network sensors and intrusion detection systems. In: Computational
Intelligence and Security. Lecture Notes in Computer Science, vol. 3802, pp. 388–297 (2005)
20. Chakir, E., Youness, I.K., Moughit, M.: False positives reduction in intrusion detection
systems using Alert correlation and datamining techniques. IJARCSSE 5(4) (2015). ISSN:
2277 128X
21. The Snort Project, Snort user’s manual 3 (2016)
22. Wang, F.S., Zhang, Z., Kadobayashi, Y.: Exploring attack graph for cost-beneﬁt security
hardening: a probabilistic approach. Comput. Secur. 32, 158–169 (2013)
A Real-Time Risk Assessment Model
237

Compact Tetraband PIFA Antenna
for Mobile Handset Applications
Mohamed Tarbouch
(✉), Abdelkebir El Amri, and Hanae Terchoune
RITM Laboratory, CED Engineering Sciences, Ecole Supérieure de Technologie,
Hassan II University of Casablanca, Casablanca, Morocco
mtarbouch@gmail.com
Abstract. In this paper, a tetraband planar inverted-F antenna (PIFA) for mobile
handset applications is studied. The proposed structure covers the GSM
(880–960 MHz), DCS (1710–1880 MHz), PCS (1880–1990 MHz), and UMTS
(1920–2170 MHz) bands. The miniaturization in the antenna size is achieved by
introducing a meandered line design. The antenna has compact size of
40 × 26 × 1.7 mm3. The radiation patterns in each operating bandwidth are almost
the same as the omnidirectional characteristics. Moreover the proposed structure
oﬀers remarkable gain in the lower band compared to previous works.
The inﬂuences of various design parameters are investigated using
CADFEKO, a Method of Moment (MoM) based solver.
Keywords: CADFEKO · Miniaturization · Mobile handset · Planar inverted-F
antenna (PIFA) · Tetra-band
1
Introduction
Today, technological advancements in wireless communication and the diversity of
needs in this area are leading to the creation of ever smaller, lighter, and more multi‐
functional mobile handsets [1]. As a result, antennas have gone from external to internal;
they have also become subject to numerous constraints in size and function. For antennas
to satisfy the requirements of the current market, they must be compact while having
multiband capability. Accordingly, attention is being focused on high-performing
antennas with simple structures, one of which is the planar inverted-F antenna (PIFA).
The PIFA consists in general of a ground plane, a top plate element, a feed wire attached
between the ground plane and the top plate, and a shorting wire or strip that is connected
between the ground plane and the top plate.
Because it operates at a resonant length of (𝜆∕4), it is highly conducive to a small
and lightweight design, and thus well-suited for use as an internal antenna [2]. The PIFA
has the advantage of a low proﬁle, but its narrow bandwidth makes it diﬃcult to realize
multiband capability with a single resonator. While this problem can be resolved by
using additional resonators [3], such additions tend to increase the size of the antenna.
This means that with a PIFA, it is diﬃcult to simultaneously achieve miniaturization
and multiband capability.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_21

The miniaturization can aﬀect radiation characteristics, bandwidth, gain, radiation
eﬃciency and polarization purity. The miniaturization approaches are based on either
geometric manipulation (the use of bend forms, meandered lines, PIFA shape, varying
distance between feeder and short plate, using fractal geometries [4–7]) or material
manipulation (Loading with a high-dielectric material, lumped elements, conductors,
capacitors, short plate [8]), or the combination of two or more techniques [9].
To overcome the problem described, we propose a compact Tetraband PIFA. The
studied antenna is composed of two radiators, a meandered patch line radiator and
L-shaped line radiator. The ﬁrst radiator has two resonance frequencies f1 and f2, the
frequency f1 covers mainly the lower band (GSM900) with 10.3% bandwidth and f2
covers a small part of higher bands, while the second radiator has one resonance
frequency f3. The last resonance frequency f3 when combined with the second resonance
frequency f2 allows the structure to covers 30.3% bandwidth which led to cover the three
services GSM1800, PCS and UMTS.
2
Design Methodology
2.1
Antenna Geometry
Figure 1(a) shows the 3-D geometry of the proposed antenna, which consists of a two
radiators, an L-shaped radiator printed in the left of the antenna and a meandered line
radiator printed in the center of the antenna. For eﬃcient radiation, the ground (GND)
found on the bottom surface of the substrate was turned by 180 °. The system GND for
the antenna has a dimension of 75 mm length and 40 mm width and it is connected with
the top radiators elements using a shorting strip. The volume under the radiating
elements is ﬁlled by air except a thin region 1.6 mm who is composed of FR4_epoxy
substrate with relative permittivity of 4.4.
Figure 1(b) shows in details the geometry of the printed radiators as seen from above.
The antenna takes up an area occupying 40*26 mm on the upper surface of the substrate.
The radiators elements are placed at a height D = 1.7 mm from the horizontal plan of
the ground plane. The height D is composed of 1.6 mm the thickness of the substrate
and a 0.1 mm of the Air. The proposed structure is excited with a microstrip feed line
of input impedance (Z0 = 50 ohms).
Figure 2 shows the simulated reﬂection coeﬃcients for the proposed antenna, the
meandered line radiator, and the L-shaped radiator. The parameters for the proposed
antenna were set as mentioned in Fig. 1(b).
As Fig. 2 shows, the proposed antenna has three resonant frequencies. f1 = 0.92 GHz
and f2 = 1.908 GHz are the fundamental frequency and the second harmonic component
of the meandered line radiator, respectively, while f3 frequency is determined by the
total length of the L-shaped radiator, which is 1.944 GHz. Note that the bandwidth used
here is −6 dB which is a widely used value for the internal antenna in practical mobile
phone applications [10–12]. By combining the resonance frequency f3 of L-shaped
resonator with the second harmonic component of the meandered line resonator f2,
broadband characteristics encompassing DCS, PCS, UMTS can be easily implemented.
Compact Tetraband PIFA Antenna for Mobile Handset Applications
239

(a)
(b)
Fig. 1. (a) 3-D geometry of the proposed antenna. (b) Patch antenna geometry as seen from above
in mm
Fig. 2. Simulated reﬂection coeﬃcients for the proposed antenna, the meandered line radiator,
and the L-shaped radiator
240
M. Tarbouch et al.

2.2
The Choice of the Patch Parameters
We set Ws = 40 mm, then we change Ls from 24 mm to 26 mm, by varying Ls the S11
parameter of the antenna versus frequency is shown in Fig. 3. Table 1 summarizes the
bandwidth obtained by varying the Ls parameter. From Table 1 we note that Ls = 26 mm
is the most adapted value in term of bandwidth, and will allows to get the lower band
to be centered in 0.92 GHz which is the centered frequency of the GSM band. Moreover
all other values can be considered not interesting values in term of bandwidth.
Fig. 3. Simulated S11 versus frequency by varying the parameter Ls
Table 1. Bandwidth versus the parameter Ls
Ls (mm)
Bandwidth(MHz)
24
893–1003 & 1744–2300
25
875–977 & 1722–2300
26
872–967 & 1679–2280
2.3
The Choice of the Height D
The height D is the distance between the top plane of the patch and the horizontal plane
of the ground plane. By varying the height D from 1.6 to 5.6 mm the S11 parameter of
the antenna versus frequency is shown in Fig. 4. From the simulation results it’s clear
that the height D aﬀect mainly the UMTS band, except for D = 1.6 witch aﬀect also to
GSM band.
Table 2 summarizes the bandwidth and resonances frequencies obtained by varying
the height D. From Table 2 we note that D = 1.7 mm will allows us to have a good
antenna in term of bandwidth. In addition, a low antenna height (1.7 mm) allows us to
obtain a small antenna volume, which is 1.768 cm3.
Compact Tetraband PIFA Antenna for Mobile Handset Applications
241

Fig. 4. Simulated S11 versus frequency by varying the height D in mm
Table 2. Bandwidth and resonant frequencies versus the height D
D(mm)
Bandwidth (MHz)
Resonant frequency(GHz)/S11(dB)
1.6
884–979
1712–2250
N/A
1.7
872–967
1679–2280
0.92/−22.03
1.908/−10.84&1.944/25.69
1.8
872–968
1700–2272
0.92/−22.01
1.908/10.85&1.940/26.97
2
871–969
1678–2263
0.92/−22.16
1.910/−11.01&1.942/29.64
2.6
871–969
1678–2235
0.92/−22.26
1.906/−11.47 & 1.940/−34.89
3.6
870–968
1657–2190
0.92/−22.19
1.907/−12.07 & 1.940/−24.16
4.6
873–963
1642–2149
N/A
5.6
869–963
1625–2130
N/A
2.4
The Choice of the Meandered Line Parameters
The meandered line parameters are Lm and Wm, by varying the parameter Lm from 9.5
to 11 mm the S11 parameter of the antenna versus frequency is shown in Fig. 5. From
the simulation results, it’s clear that the parameter Lm aﬀect mainly the GSM band,
while higher bandwidth remains unchanged. In addition from the graph we note that as
Lm is increased, the ﬁrst resonance frequency f1 of the meandered line is decreased
while the bandwidth arise from the combination of f2 and f3 remains unchanged.
242
M. Tarbouch et al.

Table 3 summarizes the bandwidth obtained by varying the parameter Lm. From
Table 3 we note that Lm = 10.3 mm will allows us to have a good antenna in term of
bandwidth for higher bands, and will allows us to get the lower band to be centered at
0.92 GHz.
Fig. 5. Simulated S11 versus frequency by varying the parameter Lm
Table 3. Bandwidth versus the parameter Lm
Lm(mm)
Bandwidth (MHz)
9.5
895–1001 & 1698–2280
10
881–983 & 1698–2280
10.3
872–967 & 1679–2280
10.5
859–956 & 1679–2280
11
844–936 & 1679–2280
By varying the parameter Wm from 0.9 to 1.2 mm the S11 parameter of the antenna
versus frequency is shown in Fig. 6. From the simulation results we note that the param‐
eter Wm aﬀect both resonances frequency f1 and f2, which inﬂuence the whole antenna
behavior.
Table 4 summarizes the bandwidth obtained by varying the parameter Wm. From
Table 4 we note that Lm = 1.1 mm is the most adapted value in term of bandwidth, and
will allows to get the lower band to be centered in 0.92 GHz. In reality, the simulation
shows close results for the Wm parameter values 1 mm and 1.1 mm. Also, 0.9 mm and
1.2 can be considered not interesting values.
Compact Tetraband PIFA Antenna for Mobile Handset Applications
243

Fig. 6. Simulated S11 versus frequency by varying the parameter Wm
Table 4. Bandwidth versus the parameter Wm
Wm (mm) Bandwidth (MHz)
0.9
895–1003 & 1706–2256
1
875–968 & 1691–2272
1.1
872–967 & 1679–2280
1.2
871–968 & 1691–1911 & 1924–2200
2.5
The Choice of the L-Shaped Parameters
We set L1 = 7 mm, then we change L2 from 16.5 mm to 18.5, by varying L2 the S11
parameter of the antenna versus frequency is shown in Fig. 7. Table 5 summarizes the
bandwidth obtained by varying the parameter L2. From Table 5 we note that as L2
increased the bandwidth of higher bands decreased while the GSM band remains
unchanged. Moreover we note that L2 = 17.5 mm is the most adapted value in term of
bandwidth, and will allows to get the lower band to be centered in 0.92 GHz. Again, the
simulation shows close results for most L2 parameter values and L2 = 16.5 can be
considered not interesting value.
244
M. Tarbouch et al.

Fig. 7. Simulated S11 versus frequency by varying the parameter L2
Table 5. Bandwidth versus the parameter L2
L2 (mm)
Bandwidth (MHz)
16.5
875–977 & 1730–2280
17
873–971 & 1702–2300
17.5
872–967 & 1679–2280
18
879–979 & 1675–2266
18.5
879–979 & 1655–2241
2.6
The Choice of the Ground Plane Parameters
We set Wg = 40 mm, then we change Lg from 60 mm to 80, by varying Lg the S11
parameter of the antenna versus frequency is shown in Fig. 8.
Table 6 summarizes the bandwidth obtained by varying the Lg parameter. From
Table 6 we note that Lg = 75 mm is the most adapted value in term of bandwidth, and
will allows to get the lower band to be centered in 0.92 GHz. Moreover all other values
can be considered not interesting values in term of bandwidth and also of compactness.
Fig. 8. Simulated S11 versus frequency by varying the parameter Lg
Compact Tetraband PIFA Antenna for Mobile Handset Applications
245

Table 6. Bandwidth versus the parameter Lg
Lg(mm)
Bandwidth (MHz)
60
903–981 & 1425–2300
65
895–977 &1406–2300
70
883–973 & 1623–2308
75
872–967 & 1679–2280
80
861–964 & 1712-2209
3
The Antenna Performance
From the previous parametric study of the proposed antenna, we obtained an optimized
structure which has a compact size and an interesting reﬂection coeﬃcient in the oper‐
ating studied bands. Figure 9 shows the reﬂection coeﬃcient of the proposed antenna.
The antenna has three resonances frequencies: f1 = 0.92 GHz with −22.03 dB return
loss, f2 = 1.908 GHz with −10.84 dB return loss and f3 = 1.944 GHz with −25.69 dB
return loss. The obtained -6 dB bandwidths are 10.3% for GSM900 band and 30.3% for
GSM1800, PCS1900 and UMTS Bands. Figure 10 shows the maximum gain of the
proposed antenna. At the GSM band, the maximum gain is stable and it is equal to 2 dB
in the whole band. At the DCS band, it varies from 3.1 to 3.6 dB, while it varies from 2
to 3.6 dB at the PCS/UMTS bands.
Fig. 9. Simulated S11 versus frequency graph of the ﬁnal proposed antenna design
246
M. Tarbouch et al.

(a)
(b)
Fig. 10. Maximum gain of the proposed antenna: (a) GSM900 band, (b) GSM1800, PCS and
UMTS bands
Figures 11, 12 and 13 show the 3D radiation patterns of the proposed antenna for
the three resonances frequencies f1 = 0.92 GHz, f2 = 1.908 GHz and f3 = 1.944 GHz
respectively. Also Figs. 14, 15 and 16 show the 2D radiation patterns for those three
resonance frequencies. As shown in Fig. 11, the radiation pattern for the f1 frequency
is nearly omnidirectional for three planes, while it’s omnidirectional in the XZ plane for
the two others resonances frequencies.
Compact Tetraband PIFA Antenna for Mobile Handset Applications
247

Fig. 11. The 3D total gain pattern of miniaturized antenna at f1 = 0.92 GHz
Fig. 12. The 3D total gain pattern of miniaturized antenna at f2 = 1.908 GHz
Fig. 13. The 3D total gain pattern of miniaturized antenna at f3 = 1.944 GHz
248
M. Tarbouch et al.

Fig. 14. The 2D radiation pattern of miniaturized antenna at f1 = 0.92 GHz (Phi = 0° for H plane,
Phi = 90° for E Plane)
Fig. 15. The 2D radiation pattern of miniaturized antenna at f2 = 1.908 GHz (Phi = 0° for H
plane, Phi = 90° for E Plane)
Compact Tetraband PIFA Antenna for Mobile Handset Applications
249

Fig. 16. The 2D radiation pattern of miniaturized antenna at f3 = 1.944 GHz (Phi = 0° for H
plane, Phi = 90° for E Plane)
Also, comparison between several diﬀerent multi-bands antennas for handset appli‐
cations is illustrated in Table 7. From this table it’s clear that our structure has a mini‐
aturized design compared to most other antennas. Moreover the proposed antenna has
the best and stable gain in the GSM900 band, which is 2 dB. Moreover the proposed
antenna has reasonable gain in the three other bands.
Table 7. Comparison of the proposed antenna to other antennas for handsets applications
Ref.
Dimension (mm3)
Volume (cm3)
Bands
Gain (dB)
Our work
40*26*1.7
1,768
GSM900
DCS1800
PCS1900
UMTS
2
3.1 to 3.6
2 to 3.7
2 to 3.7
[10]
48*16*1.6
1.229
GSM900
DCS1800
PCS1900
UMTS
WiBro/Blutooth
0.3 to 1.3
0.9 to 4
0.9 to 4
0.9 to 4
4.1 to 5.1
[11]
30*16*9
4.32
GSM900
DCS
DMB
1.19
2.93
1.48
[13]
40*8*7.8
2.496
GSM850
GSM900
GSM1800
GSM1900
UMTS
1 to 1.8
1 to 1.8
3.2 to 3.7
3.2 to 3.7
3.2 to 3.7
[14]
36*15*6
3.24
GSM900
DCS1800
UMTS
WiBro
−0.37 to −0.78
1.24 to 1.78
1.78 to 3.12
2.73
250
M. Tarbouch et al.

4
Conclusion
In this paper, a tetra band PIFA for mobile handset applications was proposed. The
antenna has a small volume of 1.768 cm3, which makes it suitable for use as an internal
antenna. By combining the second resonance frequency of the meandered line radiator
with the resonance frequency of the L-shaped radiator, it was possible to implement
broadband characteristics in the higher band. The simulated results show that the band‐
width of the proposed antenna covers GSM900, DCS1800, PCS, and UMTS bands.
Moreover the antenna has remarkable gain for GSM band compared to several other
works, and reasonable gain for other bands. Therefore, the proposed antenna exhibits
great potential for multiband mobile communication applications. In the next work,
fabrication and measurement should be done to conﬁrm the simulated results. Also, more
reﬁnement should be done to cover the 2.6 GHz LTE band and to change the shape of
the radiation patterns to protect user’s heads against electromagnetic radiation.
References
1. Bhatti, R.A., Pack, S.O.: Hepta-band internal antenna for personal communication handsets.
IEEE Trans. Antennas Propag. 55(12), 3398–3403 (2007)
2. Guo, Y.X., Chia, M.Y.W., Chen, Z.N.: Miniature built-in multiband antennas for mobile
handsets. IEEE Trans. Antennas Propag. 52(8), 1936–1944 (2004)
3. Liu, D., Gaucher, B.: A new multiband antenna for WLAN/cellular applications. In:
Proceedings of the IEEE Vehicular Technology Conference, vol. 1, pp. 243–246, September
2004
4. Chen, H.-T., Wong, K.-L., Chiou, T.-W.: PIFA with a meandered and folded patch for the
dual-band mobile phone application. IEEE Trans. Antennas Propag. 51(9), 2468–2471 (2003)
5. Reha, A., El Amri, A., Benhmammouch, O., Oulad Said, A.: Fractal antennas: a novel
miniaturization technique for wireless networks. Trans. Netw. Commun. 2(5) (2014)
6. Sun, S., Zhu, L.: Miniaturised patch hybrid couplers using asymmetrically loaded cross slots.
IET Microw. Antennas Propag. 4(9), 1427 (2010)
7. Chi, P.-L., Waterhouse, R., Itoh, T.: Antenna miniaturization using slow wave enhancement
factor from loaded transmission line models. IEEE Trans. Antennas Propag. 59(1), 48–57
(2011)
8. Skrivervik, A.K., Zürcher, J.-F., Staub, O., Mosig, J.R.: PCS antenna design: the challenge of
miniaturization. IEEE Antennas and Propag. Mag. 43(4), 12–27 (2001)
9. Reha, A., El Amri, A., Saih, M., Benhmammouch, O., Oulad Said, A.: The behavior of a
CPW-Fed miscrostrip hexagonal patch antenna with H-Tree Fractal slots. Rev.
Méditerranéenne Télécommunication 5(2), 104–108 (2015)
10. Kang, D.G., Sung, Y.: Compact hexaband PIFA antenna for mobilehandset applications. IEEE
Antennas Wirel. Propag. Lett. 9, 1127–1130 (2010)
11. Kim, D.-Y., Lee, J.W., Cho, C.-S., Lee, T.K.: Design of a compact tri-band PIFA based on
independent control of the resonant frequencies. IEEE Trans. Antennas Propag. 56,
1428–1436 (2008)
Compact Tetraband PIFA Antenna for Mobile Handset Applications
251

12. Kearney, D., John, M., Ammann, M.J.: Miniature ceramic dual-PIFA antenna to support band
group 1 UWB functionality in mobile handset. IEEE Trans. Antennas Propag. 59(1), 336–
339 (2011)
13. Wu, C.-H., Wong, K.-L.: Ultrawideband PIFAwith a capacitive feed for penta-band folder-
type mobile phone antenna. IEEE Trans. Antennas Propag. 57(8), 2461–2464 (2009)
14. Seol, K., Jang, Y., Choi, J.: PIFA with PIL patch having bent feed line for GSM/DCS/UMTS/
WiBro applications. Electron. Lett. 43(8), 436–437 (2007)
252
M. Tarbouch et al.

A Hybrid NIPS Based on PcapSockS Sniffer
and Neural MLP
Azidine Guezzaz(&), Ahmed Asimi, and Younes Asimi
Laboratoire LabSiv, Systèmes d’information et vision, Equipe SCCAM,
Sécurité, Cryptographie, Contrôle d’Accès et Modélisation,
Department of Mathematics and Computer Sciences, Faculty of Sciences,
Ibn Zohr University, B.P 8106, City Dakhla, Agadir, Morocco
a.guzzaz@gmail.com, asimiahmed2008@gmail.com,
asimi.younes@gmail.com
Abstract. The rapid growth of computer network requires a high monitoring of
data and resources to be more secure, but also to obtain a faithful communi-
cation between internal network systems. The intrusion detection and prevention
systems IDS and IPS are the last tools used to secure data of enterprises and
persons [3, 8, 9, 13, 21]. Also, their performances assessment and their efﬁ-
ciencies are very useful [1, 10, 15, 18–20]. A number of researches are down in
this domain, they aim conception of a relevant monitoring system. While IDS
makes network very sure, the IPS tries to take an adequate decision and reacts in
real time. The main goal of this article is to analyze, and then evaluate mush of
expanded and more used IPS actually. This assessment aims in one hand to
measure satisfaction of security objectives, authenticity, availability, conﬁden-
tiality and data integrity and in the other hand to test their performances based
on some parameters related on computer security such as type of detection,
ﬁltering methods, real time reaction, updating, alert, logging… We deduct some
limits and vulnerabilities. In addition, a new conception of an IPS is presented
and described in details. It is based on multilayer perceptron and PcapSockS
Sniffer [14] using cryptographic mechanisms for preparing treated data and thus
facilitates intrusions detection by minimizing positive false number and elimi-
nating the false negative generated.
Keywords: Security  Monitoring  Intrusion  Multilayer perceptron 
Classiﬁcation
1
Introduction and Notations
Actually, the computer network integrates a real applicative ﬂow and exchanged
activities are complexes. To protect efﬁciently a network, a simple intrusion blocking
of intrusive or not authorized trafﬁc are not satisfactory. The concerned trafﬁc must be
necessary preprocessed, analyzed and inspected. The detection systems are used to alert
a detected intrusion and inform security administrator to take measures of adequate
reaction and put up controlled system in a sure state. The research in intrusion detection
is oriented towards on automatic response to intrusions. In addition, to alert an intru-
sion, the IPS blocks it by breaking connection for example. To anticipate intrusions, a
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_22

number of solutions are available but any one is complete and satisfactory. The
intrusion prevention systems are an amalgam of security technologies, its main
objective is anticipation and stopping of intrusions [13, 21]. The IPS are the new
security tools classiﬁed in various types according to used detection methods, moni-
toring level, use frequency or reaction state of detection (active and passive) [1, 8, 9].
The positive false are generated when a detection system identiﬁes a normal activity
like intrusion, then negative false corresponds to attacks that not detected. So, any alert
is generated. There is no standard detection system able to overcome all vulnerabilities
of actual systems. The down researches in intrusion detection try to minimize in
maximum those vulnerabilities by proposing optimal architectures integrating a
sophistical techniques and algorithms. The second section is consecrated for a con-
nected study on intrusion detection approaches, hush functions and digital signatures.
In the third section, we assess performances of actual IPS list. This assessment is
ﬁnished by a classiﬁcation based on security objectives and on parameters which are
related to security. A new network intrusion prevention system is speciﬁed in fourth
section with a detailed description of suggested solutions. This article is achieved by a
general conclusion and perspectives. The used notations in present paper are:
M:
Message to sign
M’:
Signed message
S:
Signature
H:
Hush function
KPA:
Private Key of A
KPuA:
Public Key of A
KPB:
Private Key of B
KPuB:
Public Key of B
CPA:
Encryption of KPA
CPuA:
Encryption KPuA
DPuA:
Decryption of KPuA
CPB:
Encryption of KPB
CPuB:
Encryption of KPuB
DPB:
Decryption of KPB
DPuB:
Decryption of KPuB
IDS:
Intrusion Detection System
IDSs:
Intrusion Detection Systems
IPS:
Intrusion Prevention System
IPSs:
Intrusion Prevention Systems
NIPS:
Network Intrusion Prevention System
HIPS:
Host Intrusion Prevention System
TCP:
Transmission Control Protocol
UDP:
User Datagram Packet
VPN:
Virtual Private Network
IPSec:
Internet Protocole Security
OISF:
Open Information Security Forum
XML:
Extensible Markup Language
URI:
Uniform Resource Identiﬁer
254
A. Guezzaz et al.

MAC:
Message Authentiﬁcation Code
ISO:
International Organization for Standardization
PAdES:
PDF Advanced Electronic Signature Proﬁles
CadES:
CMS Advanced Electronic Signatures
XadES:
XML Advanced Electronic Signatures
W3C:
World Wide Web Consortium
MD5:
Message Digital 5
SHA:
Secure Hash Algorithm
RSA:
Rivest Shamir Adleman
DSA:
Digital Signature Algorithm
2
Connected Study
This section cites a state of art on methods, intrusion detection types and details
characteristics of certain actual IDS/IPS.
2.1
Intrusion Detection
The automatic intrusion detection is a number of techniques allow detecting the not
wanted activities. An IDS/IPS can be a software or hardware which monitors infor-
mation and detect in the next malicious events which tray to violate security politic or
an authorized use [13]. The IPSs are developed to take necessary measures to anticipate
with precision a detected intrusion. They are usually considered a second generation of
IDS or active IDS [3, 9, 13, 15, 21]:
• Scenario approach where the detection is to identify an intrusion based on known
conﬁguration to malicious activities or signature.
• Behavioral approach where the anomaly detection is to identify malice based on
deviation of normal intrusion. It is proposed by J.P Anderson (1980) and extended
by D.E Denning (1987). It is based mainly on behavior analysis of users, services or
applications.
The very used IDS/IPS are [13, 15]:
• NIDS/NIPS or network IDS/IPS that have a role to monitor trafﬁc within network.
• HIDS/HIPS or Host IDS/IPS implemented in level of host. They control the con-
cerned host.
• Hybrid IDS/IPS are the HIDS/HIPS that offer basis functions of NIDS/NIPS.
• Application IDS/IPS that receives data on application, generated ﬁle log by man-
agement software of database, web server or ﬁrewall, ….
• Wi-Fi IDS/IPS are similar than NIDS/NIPS. They analyze speciﬁcally wiﬁtrafﬁc
(Table 1).
A Hybrid NIPS Based on PcapSockS Sniffer and Neural MLP
255

The intrusion prevention system is a new advanced technology applied by certain
new IDS. It contains ﬁrewall ﬁltering functions and IDS analysis method. The IPS
prevents or stops a not wanted trafﬁc [13]. It intervenes actively to limit or detect
abnormal trafﬁc or interrupt suspect sessions. It analyzes also the connection contexts,
automatisms the ﬁles analysis. An IPS can monitor a wide network without impacting
normal operating of network and detect exactly abnormal activities that infect system.
They block them rapidly and are very sure to against attacks (see Fig. 1):
Generally, an IPS is characterized by:
• Real time that takes into account temporary constraints and necessary delay related
on offered results.
• Response time that determine the longest length between activation and moments of
elaborated results.
• Blocking that forbids crossing of a suspect activity.
• Alert or message which is product after detection. It contains various events and
informs the manager on existence of intrusive activity.
The IPS reacts by modifying attacked systems environment by blocking certain
ﬂows or certain ports or insulation of network system …. The majority of actual IPSs
integrate heterogeneous systems: IDS, VPN, antivirus, anti-spam, etc. [13].
Table 1. Comparison of IDS and IPS.
IDS
IPS
Placed in network inactively
Placed inline actively
Doesn’t analyze an encrypted trafﬁc
Efﬁcient for application defense
Installed in network segment (NIDS) and in
Host (HIDS)
Installed in network segment (NIPS) and
Host (HIPS)
Reaction is down by generation of alerts
Reaction is down by blocking intrusive
activities
Ideal to identify hacking attacks
Efﬁcient to block web attacks
Fig. 1. Classical architecture of intrusion detection system.
256
A. Guezzaz et al.

• ASQ (1998) is an engine integrating intrusion prevention and eliminates an intru-
sion in real time [11]. Its hardware alternative arranges a Watchdog which realizes
regularly tests of activities. During a parameter time without response, the stopping
and starting again system take place. It was an encryption until 256 bits supporting
Blow Fish algorithm based on DES with longest keys and operates on 64 bits
blocks using key of a variable length and also uses 3 DES algorithms which is slow
than DES. ASQ is based also on authenticity protocol IPSec that suffer a lot of
weakness [5] and digital certiﬁcates X.509 that present limits [6].
• Snort is a network IDS/IPS open source, developed by Sourceﬁre. It is a very wide
technology deployed in the world, its various components and functions are detailed
in [1, 15–20]. It is a scenario and anomaly system (see Fig. 2):
• Suricata is an IDS/IPS open source, developed by OISF. It brings a new ideas and
security technologies. It uses the Snort rules, its important advantage is multi-
threading that means reduction of time and gives also a high performances [15, 18–
20]. David and Benjamin analyzed Snort and Suricata in [10] and conclude that
Suricata is very relevant and exact than Snort.
• McAfee Host Intrusion Prevention is a HIPS aiming to protect systems, resources
and applications. It is a part of McAfee able to establish reporting and gives an
exact management, progressed and easy to use [1, 13].
2.2
Digital Signatures
The cryptography is a study of secrets deﬁning crypto systems and cryptanalysis is
trying to ﬁnd vulnerabilities in these systems. The both are useful and widely studied in
particular in database domain, in case of web applications to prove user of authenticity
web server [30] and the exchange of conﬁdential data signed by hush functions
granting data integrity [22]. Every message modiﬁcation of inputs introduces an
important and unforeseeable in outputs. The hush functions H with output length n is
an algorithm which corresponds to a message M with arbitrary length of an element H
(M) with length of n bits, typically 128, 160, 256 or 512 bits, called hush or
Paquets
Décodeur
Alertes
Preprocessors
Pluggins
Detection
Pluggins
Output Stage
Pluggins
Logs
- Outputs moduls
(SQL, etc)
- Support idmef
- Support idmef
Fig. 2. Snort architecture.
A Hybrid NIPS Based on PcapSockS Sniffer and Neural MLP
257

condensate. It is very difﬁcult for a hacker to ﬁnd two distinct messages with an
identical hush [23]. Among of hush function properties [3, 4]:
• Collision: to ﬁnd two distinct messages M1 and M2 such as H (M1) = H (M2).
• Second preimage: the message is chosen arbitrary. Find a distinct message M2 such
as H(M1) = H(M2).
• Preimage: a hush H1 chosen arbitrary, ﬁnd a message M such as H (M) = H1.
Practically, hush functions are used to protect passwords in a server. For example,
in place to store all user passwords, it is preferable to store their hush. They are
implemented in certain protocols for engaging in advance on chose of certain value or
to conﬁrm knowledge of certain secrets without discovering it. The calculation of a
shared secret between two entities use usually hush functions. They are used also for
integrity veriﬁcation of public ﬁles. The digital signatures are without doubt most
important application of hush function allowing signing message with private key. The
validity veriﬁcation of signature uses correspondent public key [2, 6, 15]. The digital
signature is based on two algorithm used complementary:
• Encryption algorithms with public key where a couple of digital keys are built to
generate a cryptogram from a clear text and one of the keys can’t be found lonely
with a key allow the message encryption.
• Hush functions of unique sense and without collision generating the ﬁngerprints
allowing encryption of message condensate.
The digital signature generates the authenticity, no repudiation and data integrity. It
is based on two processes, message signatures by sender and signature veriﬁcation in
destination [7]. For encrypted communication, we suppose two hosts A and B which
communicate via a network:
• Signature based on encryption uniquely:
A level of A (KPA, KPuA, KPuB, M)
• Calculate M’ = CPuB (M)
• Calculate S = CPA(M)
• Send (S, M’)
A level of B (KPB, KPuB, KPuA, M’, S)
• Calculate M’’ = DPB (M’)
• Calculate S’ = DPuA(M’’)
• Verify S = S’
• Signature based on encryption and hush function:
This signature type allows increasing integrity message, reduce its length end is
rapid.
A level of A (KPA, KPuA, KPuB, M, H)
• Calculate M’ = CPuB (M)
• Calculate S = CPA (H(M))
• Send (S, M’)
A level of B (KPB, KPuB, KPuA, M’, S, H)
• Calculate M’’ = DPB (M’)
• CalculateS’ = DPuA(H(M’’))
• Verify S = S’
258
A. Guezzaz et al.

The main standard of signed digital documents:
• ISO 19005 (PDF/A) that allows a mechanism to prevent electronic documents in
manner that visual appearance is kept for a long time.
• EDIFACT for electronic exchanged data of commercial communication.
• XML developed for exchanged data under text form between machines, published
by W3C.
XML is an infrastructure constituting a real progress for data exchange and inter-
operability between heterogeneous and distributed systems. It provides a reliable
management of documents contents containing a big volume of informations. This is a
normalization language, data transport and making form of data. It makes public a part
of database [24]. The use of XML signature solves security problems, like falsiﬁcation,
spooﬁng and repudiation [25]. There are two norms: XML signatures and XML
encryption [5, 25, 27, 28] (Table 2).
The XML signatures are applied on arbitrary digital contents via an indirection. The
data objects are preprocessed; the result is placed in an element to be signed. They
allow signing all types of digital documents. The signed contents and signature can be
detached or attached to same document. According to relative position of signature and
signed content. The digital XML signature allows creating three types of adapted
signatures to different contents of use [4, 25, 26, 28]:
Enveloped signature where signature is included in XML signature signed 
document.  
<document> 
<signature>  . . . < /signature> 
</document> 
Enveloping signature where signature contains a signed XML documents.  
< signature > 
< document >  . . . < / document >   
</ signature >   
Detached Signature where signature refers to document with an identifying of 
universal resource. 
<signature>  . . . < /signature> 



The XML signature allows signing speciﬁed markups contained in XML data [26].
It has a ﬂow structure (where « ? » means zero or an occurrence, « + » means one or
much occurrences, and « * » means zero, one or much occurrences) [4, 25, 27, 28]:
Table 2. Security measures by XML signature and XML encryption.
Authenticity Authorization Veriﬁcation Integrity Conﬁdentiality No
repeduation
XML
signatures
Yes
No
Yes
Yes
No
Yes
XML
encryption
No
Yes
Yes
Yes
Yes
No
A Hybrid NIPS Based on PcapSockS Sniffer and Neural MLP
259

< Signature ID? > 
         < SignedInfo> 
             < CanonicalizationMethod / > 
                < SignatureMethod / > 
                          (< Reference URI? > 
                          (< Transforms>)? 
                        < DigestMethod> 
                         < DigestValue> 
                         </ Reference>) + 
         </ SignedInfo > 
               < SignatureValue > 
               (< KeyInfo>)? 
                (< Object ID?>) * 
< /Signature> 
The majority of detection systems have a signatures basis to against attacks. They
let to store these signatures in each infection or deterioration and integrate them in their
database. Following, with comparing contents of their basis with activity to control, the
system can detect attacks and stamp it. The signature is a piece of code or serial
characters that identiﬁes malice. Each attack has his own signature which is must be
known by detector. The majority scenario detectors use scan to protect controlled
entities. This scan is lunched under demands or is provoked regularly; it allows ana-
lyzing of many activities one by one and veriﬁes the intrusion presence. During scan,
the detector researches attack traces via its basis. In each intrusion discovering, it is
stored in the basis under form of signature composed by not understood serial char-
acters but are readable by system. To protect in real time, the attack detectors use
snifﬁng to monitor in permanent inner and outer activities of system. It is a continuous
monitoring allows blocking each suspect and intrusive activity.
3
Classiﬁcation and Performances Assessment
This section is consecrated to classify and assess performances of certain IPS and
shows system vulnerabilities.
3.1
Criteria and Assessment Parameters
To choose a best detection system before to install it on concerned network, it is ﬁrstly
useful to assess these systems but also their operational efﬁciency. For stratifying this
assessment, various measures are available, precision, number of positive false, number
of negative false and capacity to manage a high ﬂow trafﬁc [1, 10]. The IPS includes
efﬁcient methods to update its database to detect new attacks. Otherwise, it includes
method to react new attacks without updating signatures [1]. We assess a list of free and
commercial IPS very used to evaluate their performances and then deduct their
260
A. Guezzaz et al.

classiﬁcation. To satisfy this classiﬁcation, a lot of parameters are cited in [1, 8, 14, 15,
18–20]. Thus, We propose guarantee of security objectives (authenticity, conﬁdentiality,
integrity and availability), analyzed data sources (network, system and application),
reaction after intrusion (passive or active), frequency of use (periodic or continuous),
real time analysis, attacks detection (internal or external), blocking method of intrusions,
real time alert, logging, ﬁltering method of trafﬁc, compatible operating systems, ….
The intrusion is trying to violate one of security objectives described in [1, 21]:
• Authenticity which consists to validate that the authorized persons can access to
resources.
• Conﬁdentiality means making information secret between authorized entities.
• No repudiation lets to guarantee that a transaction can’t be refused.
• Integrity stops no appropriate deterioration of information.
• Access control means that access to information is limited and controlled.
• Availability is to be willing and available to use.
3.2
Discussion of Results Assessment
Based on these criteria and refer to detailed study in [1, 8–13, 16–22, 29], we elaborate
the follow classiﬁcation (Tables 3 and 4):
Table 3. Classiﬁcation based on security objectives.
Snort
NetASQ
Suricata
Winpooch
MC Afee
intercept
Bro
Net screen
intrusion
Cisco net
ranger
Authenticity
High
Protocole IPSEC,
Certiﬁcats X.509,
PKI
infrastructures,
SSL
MAC
algorithm
High
High
High
Medium
ACL Lists
Conﬁdentiality
TowFish
Algorithm
DES, 3DES, AES,
BlowFish.
Cryptograﬁc
functions of
TLS protocol
—
—
Include SSH
functions
RC4
Algorithm
—
Integity
5 MB/s,
10 MB/s,
4 GB/s
High speed MD5,
SHA1, SHA2
Hush
functions of
TLS protocol
Includes
scan of
ClamWin
antuvirus
—
High-level
semantic
analysis/detect a
large number of
protocols
5 MB/s,
100 MB/s,
1 GB/s/
High
reliability
Availability
Continuous
frequency
Continuous
frequency
Continuous
frequency
High
Continuous
frequency
Continuous
frequency
Continuous
frequency
Continuous
frequency
Table 4. Classiﬁcation based on proposed security parameters.
Snort
NetASQ
Suricata
Winpooch
MC Afee
intercept
Bro
Net screen
intrusion
Cisco net
ranger
Realtim
analysis
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Type
Nips/Hips
Nips/Hips
Nips
Hips
Hips
Nips
Nips/Hips
Nips
Detection
type
Signature
Signature/
Behavioral
Signature/
Behavioral
Signature
Signature/
Behavioral
Signature/
Behavioral
Signature
Signature
Blocking
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Reaction
Active
Active
Active
Active
Active
passive
Active
Active
Real time
alerts
Yes
Yes
Yes
Yes
Yes/Email,
S NMP
Yes
Yes
Yes
Logging
Yes
Yes
Yes
—
Yes/MS SQL
Server
Yes
Yes
yes
Filtering
Determined by
admin
Filtering and
ﬂow contol
Yes
Scan
ﬁlters
Yes
Yes
Filtering
packet
Yes
Updating
Manual
Automatic
Manual
—
Automatic
Automatic/Manual
—
Automatic
Operating
systems
Linux/Windows
Linux/Windows
OS
Commun
Windows
Windows/
Solaris/
Linux/Free
BSD/MAC OS
Windows/
Linux/Solaris
OS
Commun
A Hybrid NIPS Based on PcapSockS Sniffer and Neural MLP
261

This study leads to highlight certain limits. The majority of IPS systems suffer a
wide generation of positive false and no detection of some negative false, it inﬂuences
negatively on operational functioning of these tools. Certain packets are not treated and
some attacks are not detected during a monitoring of high trafﬁc. A lot of modern
bridges don’t integrate prevention intrusion functions and don’t make ports monitoring.
The IPS cannot analyze encrypted or fragmented packets. In addition, the bad formed
packets are not very treated. The IPS are based on scenario require to put up signatures;
they suffer the use constraints because the development of signatures is a complex task.
Also, the designers do usually errors on alert part. The signatures are developed to
response new reports vulnerabilities. They must be unique to alert only in case of
malicious trafﬁc; the real constraint is that attack code can be easily changed. The IPS
doesn’t produce any alert when an intrusion is being really. The signature basis has to
be updated at any detection [9]. The IPS problem is based on motif exists at level of
deﬁning and maintaining an exact state of reference to model a normal behavior which
any deviation according to this state is detected like an intrusion. Another important
aspect is how many data that analyst can analyze effectively and efﬁciently, the quantity
of data that will be developed rapidly. It is impossible to have a perfect IDS and IPS
because of decision taken by these systems due to number of positive false that are
produced when the system detects by error on anomaly and number of negative false
when not wanted activity is not detected.
4
New Approach
In this section, we purpose and describe new NIPS, for a network IPS based on
scenario and PcapSockS sniffer using neural multi layer perceptron. This proposition is
based essentially on cause that the majority of actual monitoring systems suffer limits
(Fig. 3).
4.1
Our Proposed Scheme
After performances assessment of certain IPS and following of limits cited in previous
section. Our approach proposes an IPS prototype very efﬁcient, able to avoid some
limits. The various model components are described in (see Fig. 1):
The probe has a trafﬁc collector that collects data from a low and high network level
for the both TCP and UDP connections. It is a satisﬁed task by PcapSockS sniffer [14].
The analysis part contains a classiﬁer which compares the intercepted event with stored
events in signatures basis. In case of deviation, the detector announces an intrusion; it
stores it in intrusion basis and generates an alert that informs the administrator on
threat. An alert is deﬁned by signature rules and stored in alert basis. The operator
evokes a response when an intrusion is detected and reacts automatically with intrusion
tentative and helps to block it really. It allows the archiving functions and reactions
logging that can be used in other contexts to elaborate statistics on frequency of like
intrusion for example. The supervisor controls regularly all system components, it is
responsible to lunch the trafﬁc capture, detects place of broken down, conﬁgures the
262
A. Guezzaz et al.

probe, reconﬁgures it in case of problem. The trafﬁc is extracted in a network, it is
preprocessed and normalized.
4.2
Description of Solutions
The PcapSockS sniffer is based on pcap language and sockets. It decodes and pre-
processes the collected and built trafﬁc, thus the normalized collection basis is pre-
sented to analysis phase [14]. It is a relevant sensor assuring two main tasks:
• Collects trafﬁc in high and low level.
• Construction of a collection basis.
The result database is composed by events characterizing collected activities. It is
divided in two parts. The ﬁrst is for model training and the second is for test and
validation. To analyze and classify events contained in collection basis, we designed
speciﬁcally a data classiﬁer based on perception multilayer neural network that reacts
rapidly in real time and let to classify the events after training and test. The new Nips is
also based on signatures basis containing activities ﬁngerprints composing authorized
trafﬁc to circulate in monitored network. These activities are gathered during a sure and
normal state of network at a determined and exact time interval. The basis has ﬁelds
designating the trafﬁc attributes containing much of records constituting lines. In
general, the new Nips operates Under two phases:
Fig. 3. New hybrid network intrusion prevention model.
A Hybrid NIPS Based on PcapSockS Sniffer and Neural MLP
263

• Construction phase (or ofﬂine phase) where controlled segment installs probe with
any connection on exterior. It builds the signature basis used in analysis and vali-
dates the classiﬁer (Fig. 4).
• Detection phase (or Online) where the probe is installed, connected with exterior
and controls enter and outer trafﬁc of a controlled trafﬁc. Any intercepted activity is
pretreated, and then compared with signature base contents. With classiﬁer, if a
deviation exists, an intrusion is detected. The probe reacts in real time by blocking
the intrusive trafﬁc and generates on alert and updates basis (Fig. 5).
The use of database servers is very primordial. The Nips has four bases:
• Collection Basis which receives intercepted trafﬁc.
• Signatures Basis containing the ﬁngerprints characterizing normal activities of
controlled network.
• Intrusions Basis qui stocke les intrusions détectées. Elle est mise à jour pendant
chaque détection.
• Alerts Basis that stores the generated alerts and sent to manager.
Fig. 4. Construction phase.
Fig. 5. Detection phase.
264
A. Guezzaz et al.

The new Nips is a distributed system composed of divided parts. We use XML
language for normalizing communication and make it coherent. XML makes also this
communication encrypted and secret implementing one of the cryptographic techniques
provided by this language that increases security of exchange. It is also applied to
normalize generated alerts to facilitate later treatments. The Nips place depends to
monitored network topologies and to detection type (internal or external). In general, it
is placed after a router and ﬁltering systems (proxy, ﬁrewall, IDS …). If there are a lot
of controlled routes, the probe is installed in any input point.
5
Conclusion and Perspectives
In this paper, we present a connected study on intrusion detection methods, speciﬁcally
the ones of prevention. Two major approaches are available, behavioral or motif
approach and scenario or signature approach. Each of those presents advantages and
disadvantages. A detection system has one of the methods or the both. We show with
performances assessment based on computer security criteria that more IDS/IPS used
actually suffer much limits. A new network IPS based on PcapSockS and MLP is
proposed aiming to minimize limits bringing new solutions. We deduct that is
impossible to ﬁnd a perfect detection/prevention system. Thus, the optimal security is
obtained by combining several techniques. The prevention intrusion technologies need
to be developed in the next according to increased needs security for enterprises and for
persons. The next work will design a complete IPS by purposing an optimal archi-
tecture of Hips to control the host events. This IPS will be used to monitor the networks
and their systems.
References
1. Farhaoui, Y., Asimi, A.: Performance assessment of tools of the intrusion detection and
prevention systems. IJCSIS 10(1), 7 (2012)
2. CGI: Etude technique, cryptographie à clé publique et signature numérique, principes de
fonctionnement (2002)
3. Alanou, V.: Détection d’intrusion dans un système informatique: méthodes et outils.
SUPELEC BP 2835511 Cesson Sévigné Cedex (1996)
4. Cardon, R.: Signatures électroniques dans les applications internet. Mémoire introduit pour
l’obtention du diplôme du grade d’ingénieur civil polytechnicien Bruxelles (2006)
5. Vanhullebus, Y.: Faiblesses d’IPSec en déploiements réels. Netasq (vanhu@netasq.com)
6. Demerjian, J.: Services d’autorisation et Intégration au protocole d’attribution dynamique
des adresses. Thèse, Ecole Nationale Supérieure des Télécommunications, Paris (2004)
7. Yue-sheng, G., Meng-tao, Y., Yong, G.: Web services security based on XML signature and
XML encryption. J. Netw. 5(9), 1092–1097 (2010)
8. Gupta, A., Kumar, M., Rangra, A., Kumar Tiwari, V., Saxena, P.: Network intrusion
detection types and analysis of their tools. Department of Computer Science and Information
Technology, Jaypee University of Information Technology, India (2012)
9. Farhaoui, Y., Asimi, A.: Creating a complete model of an intrusion detection system
effective on the LAN. Int. J. Adv. Comput. Sci. Appl. 3(5), 147–150 (2012)
A Hybrid NIPS Based on PcapSockS Sniffer and Neural MLP
265

10. Day, D.J., Burns, B.: A Performance analysis of snort and suricata network intrusion
detection and prevention engines. In: The Fifth International Conference on Digital Society
(2011)
11. Netasq: A Firewalls NETASQ Évaluation de la suite logicielle IPS-Firewall Version 5,
Évaluation selon un paquet EAL2 augmenté des Critères Communs V2.2
12. Netasq: High performance to secure your future NETASQ IPS & ﬁrewall U series
13. Santos, B., Chandra, T., Ratnakar, M., Baba, S., Sudhakar, N.: Intrusion detection
system-types and prevention. Int. J. Comput. Sci. Inf. Technol. 4(1), 77–82 (2013)
14. Guezzaz, A., Asimi, A., Sadqi, Y., Asimi, Y., Tbatou, Z.: A new hybrid network sniffer
model based on Pcap language and sockets (Pcapsocks). Int. J. Comput. Sci. Inf. Technol. 7
(2), 207–214 (2016)
15. Eldow, O., Chauhan, P., Lalwani, P., Potdar, M.: Computer network security ids tools and
techniques (snort/suricata). Int. J. Sci. Res. Publ. 6(1), 593 (2016). ISSN 2250-3153
16. Scott, C., Wolfe, P., Hayes, B.: SNORT for Dummies. Wiley, Hoboken (2004)
17. Snort++ User Manual
18. White, S., Fitzsimmons, T., Matthews, N., Wallace, H.: Quantitative Analysis of Intrusion
Detection Systems: Snort and Suricata. Coulter School of Engineering, Department of
Computer Science, Whitejs, Clarkson University, Potsdam (2011)
19. China, R., Avadhani, P.: A comparison of two intrusion detection systems. IJCST 4(1)
(2013).ISSN: 0976-8491
20. Ridho, M., Yasin, F., Eng, M.: Analysis and evaluation Snort, Bro, and Suricata as intrusion
detection system based on Linux server. Department of Informatics, Faculty of Commu-
nications and Informatics Universitas Muhammadiyah Surakarta (2014)
21. Majorczyk, F.: Détection d’intrusions comportementale par diversiﬁcation de COTS:
application au cas des serveurs web. No ORDRE 3827 de la thèse. Université Rennes1,
French (2008)
22. Drissi, A., Asimi, A.: One-way hash function based on Goppa codes «OHFGC». Appl.
Math. Sci. 7(143), 7097–7104 (2013)
23. Peyrin, T.: Analyse de fonctions de hachage cryptographiques. THESE présentée et soutenue
publiquement à l’Ecole normale supérieure, Paris pour l’obtention du Doctorat de
l’Université de Versailles Saint-Quentin-en-Yvelines (2008)
24. Bases de données et XML. Évaluation d’un programme XSLT – p. 1/??
25. Saravanaguru, K., Abraham, G., Venkatasubramanian, K., Borasia, K.: Securing Web
Services Using XML Signature and XML Encryption. School of Computer Science and
Engineering, VIT University, Vellore, India (2013)
26. Ardagna, A., Damiani, E., Vimercati, S., Samarati, P.: XML Security. University degli Studi
di MilanoItalia (2007)
27. Hill, B.: A taxonomy of attacks against XML Digital Signatures & Encryption. iSEC
Partners (2007)
28. Miyauchi, K.: XML signature/encryption—the basis of web services security. J. Adv.
Technol. 2(1), 35–39 (2005). Special Issue on Security for Network Society
29. Kumar, G., Varadarajan, G.: Web application attack analysis using bro ids. Manuel
Humberto Santander Peláez, GIAC (GCIA) Gold Certiﬁcation (2012)
30. Sadqi, Y., Asimi, A., Asimi, Y.: A cryptographic mutual authentication scheme for web
applications. Int. J. Netw. Secur. Appl. 6(6), 1–15 (2014)
266
A. Guezzaz et al.

Convolutional Codes BPSK
Modulation with Viterbi Decoder
Nejwa El Maammar1,2(✉), Seddik Bri1,2, and Jaouad Foshi1,2
1 Department of Electronics Instrumentation and Measurements,
University of Errachidia, Errachidia, Morocco
2 Materials and Instrumentation (MIM), Electrical Engineering Department,
High School of Technology, Moulay Ismail University, Meknes, Morocco
nejwa.elmaammar@gmail.com, briseddik@gmail.com,
j.foshi@fste.umi.ac.ma
Abstract. The main aim of any communication schemes is to provide error-free
data transmission. FEC (Forward error correction) processing in a receiver may
be applied to a digital bit stream or in the demodulation of a digitally modulated
carrier. Many FEC coders can generate a bit-error rate (BER). Convolutional
encoding with Viterbi decoding is a good forward error correction technique
suitable for channels aﬀected by noise degradation. It’s considered as a technique
that is widely used in communication systems to improve the bit error rate
performance. The decoding of convoluational codes with the Viterbi algorithm
has been used very successful on satellite and space channels where the channel
is memoryless. This paper is concerned with the ½ Binary convolutional codes
(BCC) in conjunction with Binary Phase Shift Keying (BPSK) modulation and
maximum likelihood Viterbi decoding on Additive White Gaussian noise
(AWGN) and Rayleigh channel. Applying the Viterbi algorithm with hard deci‐
sion decoding comparison of Bit Error Rate versus Signal to Noise ratio in AWGN
and Rayleigh channel is shown for memory or memory less condition.
Keywords: Convolutional encoder · Viterbi decoder · AWGN · BPSK
1
Introduction
In today’s digital communications, the reliability and eﬃciency of data transmission is
the most concerning issue for communication channels [1]. The purpose of this commu‐
nication system is to transmit intelligence signal through a channel from the transmitter
to the receiver. The process starts by a source which generates the messages that are
converted into a sequence of binary digits; this sequence is passed from the source
encoder to the channel. This latter is mostly aﬀected either by noise or fading. The
channel coding technique is usually used to correct errors introduced in the channel; this
technique is done by introducing in a controlled manner some redundancy in the trans‐
mitted encoding data that can be used at the receiver to overcome the eﬀects of noise.
In eﬀect the decoder can later reconstruct the transmitted data using the redundancy
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_23

added in the information sequence that aids to improve ﬁdelity of the received signal
and to increase the reliability of received data. The most sophisticated encoding tech‐
nique implicates taking k information bits at a time and mapping each k-bit sequence
into a unique n-bit sequence that we call a code word. The quantity of redundancy
introduced by encoding the information data is deﬁned by the ratio n/k. The reciprocal
of this ratio, namely k/n, is called simply, the code rate or the rate of the code. The
sequence of binary digit at the output of the channel encoder is passed to the digital
modulator.
It is known that the digital communications over mobile channels often suﬀer from
multipath eﬀects, which result in signal fading that degrades the performance of commu‐
nication system. To combat fading Viterbi algorithm could be used. Viterbi decoder is
used to decode convolutional coding [2]; this algorithm reduced the average number of
computations required per bit of decoded information while achieving comparable BER.
Reconﬁgurable computing has been proposed for signal processing with various objec‐
tives, including high performance, ﬂexibility, specialization, and most recently, adapt‐
ability. Reconﬁguration is characterized by how fast the reconﬁguration can occur and
how many possible reconﬁgurations can be used. The decoding of convolutional codes
with the Viterbi algorithm has been used very successful on satellite and space channels
where the channel is memoryless [3]. This work focuses on the performance comparison
of rate for convolutional codes with BPSK modulation scheme over AWGN and
Rayleigh channels using MATLAB.
2
Channels Model
2.1
Additive White Gaussian Noise Channel
Additive White Gaussian Noise Channel includes the sum of the white noise that follows
a normal or Gaussian density. High data rate over this channel model are limited by
noise. The received signal can be expressed in the interval 0 ≤ t ≤ T as follows.
r(t) = Sm(t) + n(t)
(1)
Where n(t) represent the sample function of additive white Gaussian noise(AWGN)
process with power- spectral density [4].
Fig. 1. Model for received signal passed through AWGN channel.
268
N. El Maammar et al.

2.2
Rayleigh Fading Channel
The wireless environment is highly unstable and fading is due to multipath propagation
[5]. In the ﬂat Rayleigh channel model, the channel is assumed to introduce two random
variables in the signal observed by the receiver; a random ﬂuctuation of the received
signal energy and phase and an additive, white Gaussian noise component, as seen in
the system block diagram of Fig. 1.
Fig. 2. Schematic block diagram of a communication system communicating over a Rayleigh
fading channel.
Assuming that the receiver is able to perfectly track the phase of the channel, the
detector in the receiver observes the signal r = |a|s + n, where a is the complex channel
coeﬃcient. The random variable |a| has a Rayleigh probability density function with
mean _π/2 and n = nI + jnQ is a complex Gaussian noise variable with nI and nQ being
identically distributed zero-mean Gaussian random variables with variance N0/2, where
N0/2 is the double-sided noise power spectral density. Each signal alternative is asso‐
ciated with a decision region Sk, and the receiver determines what Sk the received signal
r falls within and outputs as its estimate’s the corresponding signal sk. A bit error is said
to occur if the estimate’s diﬀers from the transmitted signal’s [6] (Fig. 2).
3
Forward Error Correction
In telecommunication forward error correction (FEC) is a technique that is often used
for controlling errors in data transmission over unfaithful or noisy channels. FEC gives
the receiver the possibility to correct errors without needing a reverse channel to request
retransmission data. FEC is moreover applied in situations where retransmissions are
impossible. The convolutional codes are considered among of the types of FEC that
work on bit streams of diﬀerent length they are often decoded with the Viterbi algorithm,
they are frequently an integral part of the analog to digital conversion procedure, also
involving digital modulation and demodulation.
3.1
Convolutional Codes
Convolutional code is a commonly used forward error-correction code in deep space
communication System and wireless communication system [7, 8]; it is also powerful
Convolutional Codes BPSK Modulation with Viterbi Decoder
269

method for forward error correction and it is widely used in a various applications such
as radio communication, mobile communication, and digital radio.
However, convolution coding is a mealy machine that adopts a very simple procedure
whose the output is a function of the current state and the current input; it consists of
one or more shift register where input bits are stored in ﬁxed length shift register and
they are combined with the help of mod-2 (Fig. 3).
Fig. 3. Convolutional encoder with rate ½, k = 1, n = 2, K = 4, m = 3.
Convolutional codes are described using two parameters which are: the code rate
and the constraint length. The code rate R = k/n is called the ratio for a convolutional
code where k is the number of parallel input bits and n is the number of parallel decoded
output bits. The constraint length parameter K denotes the “length” of the convolutional
encoder, there is also another parameter related to K is the parameter m who is the
number of shift register, this parameter can be thought of as the memory length of the
encoder.
3.2
Viterbi Decoder
In 1967 by Andrew Viterbi was proposed the Viterbi algorithm (VA) and is used to
decoding a bit stream that has been encoded using FEC code [9]. Viterbi decoding algo‐
rithm is frequently used in a wide range of communication and data storage applications
[10]; and it is mostly applied to convolutional encoder. Is also an algorithm that works
by forming trellis structure, which is eventually traced back for decoding the received
information; It reduces the computational complexity by using simpler trellis structure.
The Viterbi Decoder is used in many FEC applications and in systems where data are
transmitted and subject to errors before reception [11]. Viterbi algorithm consists of
three major blocks (Fig. 4).
Fig. 4. Block diagram of the Viterbi Decoder.
270
N. El Maammar et al.

• Branch Metric Unit (BMU): is the ﬁrst unit which measures the distance values
computation at each time instant between the received symbol and the symbol asso‐
ciated with the transition among trellis states. Hamming distance or Euclidean
distance is used for branch metric computation.
• Add Compare Select Unit (ACSU): the current Branch Metric is added to previous
Path Metric, and through the unit of Add-Compare-Select each of the two distances
is compared; the Add-Compare-Select unit counts the number of diﬀering bits. The
measured value of the BMU can be the Hamming distance in case of the hard decision
decoding or the Euclidean distance in case of the soft decision decoding.
• Survivor Path (SMU): it is the ﬁnal component unit which stores the survivor paths
(Fig. 5).
Fig. 5. Flow chart of Viterbi Decoding
Convolutional Codes BPSK Modulation with Viterbi Decoder
271

From the encoder output through the channel the BMU receives input data and
computes a metric for each state and each input bit. There are two methods for calculate
the metrics.
• In Hard decision decoding, The Viterbi algorithm utilizes the trellis diagram to
compute the Branch metrics and path metrics. Its process is done by comparing the
received codeword with the all possible codewords. The codeword which gives the
minimum Hamming distance is selected.
• In Soft decision decoding, the demodulator does not assign a ‘0’ or a ‘1’ to each
received bit but uses multi-bit quantized values. Its process is very similar to Hard
decision algorithm except that the soft decision decoding improves the decision
making process by supplying additional reliability information (calculated Euclidean
distance or calculated log- likelihood ratio).
4
Traceback Method
In a Viterbi Decoder, there are two known memory organization techniques for the
storage of survivor sequences from which the decoded information sequence is retrieved.
The register exchange method is the simplest conceptually but suﬀers from the disad‐
vantage that every bit in the memory must be read and rewritten for each information
bit decoded. The alternative is the traceback method where the interpretation of the
symbols as pointers removes the necessity to move data in the memory. The Traceback
method stores path information in the form of an array of recursive pointers. Unfortu‐
nately, direct implementation of the Traceback method proposed is impossible, since it
treats memory as inﬁnite in size, while any actual implementation contains only a ﬁnite
memory. It is advantageous to think of traceback memory as organized in a two-dimen‐
sional structure, with rows and columns. The number of rows is equal to the number of
states N = 2v. Each column stores the results of N comparisons corresponding to a Trellis
stage-Time. Since the stream of symbols is in general, semi-inﬁnite, storage locations
are periodically reused. The Trace Back unit performs three basic operations going on
in the memory banks every bit time, Traceback Read, Decoded Read and Writing new
data (wr).
• Traceback Read (tb) - This is one of the two read operations and consists of reading
a bit and interpreting this bit in conjunction with the present state number as a pointer
that indicates the previous state number (i.e. state number of the predecessor). Pointer
values from this operation are not output as decoded values; instead they are used to
ensure that all paths have converged with some high probability, so that actual
decoding may take place. The traceback operation is usually run to a predetermined
depth, T, before being used to initiate the decode read operation.
• Decode Read (dc) - This operation proceeds in exactly the same fashion as the
traceback operation, but operates on older data, with the state number of the ﬁrst
decode read in a memory bank being determined by the previously completed trace‐
back. Pointer values from this operation are the decoded values and are sent to the
bit-order reversing circuit. A decode read can serve as a dual decode and traceback
272
N. El Maammar et al.

read, this allows us to decode read multiple columns using one traceback read oper‐
ation of T columns.
• Writing New Data (wr) - The decisions made by the ACS are written into locations
corresponding to the states. The write pointer advances forward as ACS operations
move from one stage to the next in the trellis, and data are written to locations just
freed by the decode read operation. For every set of column write operations (N bits
wide), an average of one decode read must be performed. The overhead of T-column
traceback read can be spread over one or more column decode read operations,
resulting in k > 1 read operations, this includes both decode read operations and
traceback read operations.
5
Modulation Technique
In General, modulation system may be deﬁned as the process of sending a message with
the help of carrier [12], but now the eﬃciency and performance of communication
system depend upon the choice of modulation system. There are two basic classes of
digital modulation techniques that are used for transmission of digital data, in this section
we discuss about BPSK and QPSK modulation technique.
5.1
Binary Phase Shift Keying (BPSK)
In BPSK modulator the carrier assumes one of two phases. Logic 1 produces no phase
change and logic 0 produces an 1800 phase change [12].
g(t) =
√
2Eb
Tb
cos 2𝜋fct
(2)
From (2) we can say that BPSK is described as one dimensional digital carrier
modulation scheme. And the demodulated signal is given by:
s(t) =
√
Ebdt +
√
2
Tb
Tb
∫
0 𝜔t. cos(𝜔ct + 𝜃)dt
(3)
In (3) the ﬁrst term represents desired BPSK signal and second term represents the
eﬀect of noise.
5.2
Quadrature Phase Shift Keying (QPSK)
In QPSK modulator 2 bits are processed to produce a single phase change. In QPSK
modulation each symbol consists of 2bits. Bandwidth efficiency of QPSK is twice as
compared to the BPSK because two bits are transmitted in a single modulated symbol [13].
Convolutional Codes BPSK Modulation with Viterbi Decoder
273

s(t) =
√
2Eb
Tb
cos(2𝜋fct + 𝜃)
(4)
𝜃= (2i −1)𝜋
4
(5)
The initial phases produced by QPSK modulator are π/4, 3π/4, 5π/4 and 7π/4.
6
Simulation Results
A full system model was implemented in MATLAB according to the above described
system. Performance analysis is done for hard and soft decision Viterbi decoding by
taking random data stream of deﬁned length for each of the Viterbi techniques. In this
work we have used BPSK modulation and demodulation for all the simulations. The
encoded data is then passed through Gaussian channel which adds additive white Gaus‐
sian noise to the channel symbols produced by the encoder.
To be able to decode with ﬁnite survivor memory, let’s say we need to start the
traceback at some time instant D+TB, where D is the decoding depth and TB is the
traceback depth. The sequences of events are as follows:
• At time instant D+TB, we start the traceback and continue tracing back through the
survivor path TB times. Once we are done with the traceback, we start estimating the
D decoded bits knowing the current state and previous state.
• Similarly, we again start the traceback at time 2D+TB, do traceback TB times and
then decode bits from 2D to D.
• Once we reach the end of the input sequence at time instance N+K−1, we know that
trellis has converged to state00 and then we do demodulation with traceback.
Fig. 6. BER vs Eb/N0 for Soft and Hard Viterbi Decoder.
274
N. El Maammar et al.

Figure 6 shows Convolutional Coding and the associated hard and soft deci‐
sion Viterbi decoding. The modulation used is BPSK and the channel is assumed to be
AWGN alone. The comparison of hard decision decoding and soft decision decoding
provides around 2 dB of gain for bit error rate of 10−4. In the current simulation model,
soft bit are used with full precision for obtaining the BER curves. However in typical
implementations, soft bits will be quantized to ﬁnite number of bits.
Fig. 7. BER vs Eb/N0 for uncoded Rayleigh, AWGN, Hard Viterbi and BCC Hard Viterbi with
AWGN channel with memory.
On Fig. 7 we have shown a comparison of BER between AWGN channel and ﬂat
Rayleigh fading channel with traceback method. It Combine BER for BCC with Viterbi
Fig. 8. BER vs Eb/N0 for uncoded Rayleigh and BCC Hard Viterbi with AWGN channel.
Convolutional Codes BPSK Modulation with Viterbi Decoder
275

decoding (TB = 15) for BPSK in Rayleigh fading and AWGN channel. From this simu‐
lation it has been observed that doing traceback depth of around 15. We conclude that
AWGN channel is better over Rayleigh channel.
The Fig. 8 indicate BER improvement in ﬂat Rayleigh fading channel with traceback
unit by Viterbi algorithm about 15 dB but by using traceback unit in ﬂat Rayleigh fading.
Fig. 9. BER vs Eb/N0 for uncoded Rayleigh, AWGN, Hard viterbi and BCC Hard viterbi with
AWGN channel with memoryless.
Fig. 10. BER vs Eb/N0 for uncoded Rayleigh and BCC Hard Viterbi with AWGN channel with
memoryless.
Figure 9 we have shown a comparison of BER between AWGN channel and ﬂat
Rayleigh fading channel without traceback method. It Combine BER for BCC with
Viterbi decoding for BPSK in Rayleigh fading and AWGN channel.
276
N. El Maammar et al.

Figure 10 shows BER improvement in ﬂat Rayleigh fading channel without trace‐
back unit by Viterbi algorithm about 7 dB but by using traceback unit in ﬂat Rayleigh
fading channel we get almost 8.5 dB improvement.
7
Conclusion
Viterbi decoder is one of the most important blocks in communication system. Viterbi
algorithm allows safe data transmission via error correction and original message can
recover accurately without any noise. This paper has presented traceback memory
management in Viterbi Decoders. For a high speed, large constraint length, it also shows
the performance comparison when the hard decision is used in Viterbi Algorithm when
it is associated with memory or not, which can be very helpful for decision making for
channel choosing. From simulations, we can identify the minimum value of traceback
depth (TB), which results in performance comparable to inﬁnite survivor path memory
case. From simulations it has been observed that doing traceback depth of around 15
ensures that the obtained BER with ﬁnite survivor state memory is close to the BER
with inﬁnite survivor state memory.
References
1. Kaur, H., Jain, B., Verma, A.: Comparative performance analysis of M-ary PSK modulation
schemes using Simulink. Int. J. Electron. Commun. Technol. 2(3), 204–209 (2011)
2. Sudharani, B.K., Dhananjay, B., Praveen, J., Aghavendra, R.: Eﬃcient convolutional adaptive
Viterbi encoder and decoder using RTL design. Int. J. Innov. Res. Electr. Electron. Instrum.
Control Eng. 3(2), 101–105 (2015)
3. Hagenuer, J.: Viterbi decoding of convolutional codes for fading and burst channels. In:
Sanada, Y., Wang, Q. (eds.) IEEE Seminar on Digital Communications, Catalog No.
80CH1521-4, A Co-Channel Interference, pp. G2.1–G2.7, Zurich (1980)
4. Sharma, V., Shrivastav, A., Jain, A., Panday, A.: BER performance of OFDM-BPSK, -QPSK,
QAM over AWGN channel using forward Error correcting code. Int. J. Eng. Res. Appl. 2(3),
1619–1624 (2012)
5. Chavan, M.S., Chile, R.H., Sawant, S.R.: Multipath fading channel modeling and
performance comparison of wireless channel models. Int. J. Electron. Commun. Eng. 4(2),
189–203 (2011)
6. Islam, M.S., Khandaker, M.U.: Performance comparison of rate ½ convolutional Codeswith
BPSK on Rayleigh and AWGN channels for memory and memory less condition. Int. J.
Comput. Inf. Technol. 2(2), 14–19 (2011)
7. Guofang, L., Naiping, C.: An improved Viterbi decoding algorithm. J. Telem. Track.
Command 27(1), 30–32 (2006)
8. Xiaoying, L., Wang, Y., Xinan, W.: A high speed Viterbi circuit implementation for WLAN
system. Comput. Technol. Dev. 18(1), 11–13 (2008)
9. Katta, K.: Design of convolutional encoder and Viterbi decoder using MATLAB. Int. J. Res.
Emerg. Sci. Technol. 1(7), 10–15 (2014)
10. Kumar, A., Gupta, M.: Design comparative study and analysis of CDMA for diﬀerent
modulation techniques. Egypt. Inf. J. 16(16), 351–365 (2015)
Convolutional Codes BPSK Modulation with Viterbi Decoder
277

11. Sudharani, B.K., Dhananjay, B., Praveen, J., Aghavendra, R.: Eﬃcient convolutional adaptive
Viterbi encoder and decoder using RTL design. Int. J. Innov. Res. Electr. Electron. Instrum.
Control Eng. 3(2), 101–105 (2015)
12. Mandloi, D., Paliwal, P., Yadav, D.: Performance evaluation of ½ rate convolution coding
with diﬀerent modulation techniques for DS-CDMA system over Rician channel. Int. J.
Comput. Appl. 55(12), 0975–8887 (2012)
13. Kumar, A., Gupta, M.: Design comparative study and analysis of CDMA for diﬀerent
modulation techniques. Egypt. Inf. J. 16, 351–365 (2015)
278
N. El Maammar et al.

Cloud Computing, Big Data Analytics,
Software Security and Monitoring,
Multimedia Information Processing

A New Monitoring Approach with Cloud Computing
for Autonomic Middleware-Level Scalability
Management Within IoT Systems
Mohamed Nabil Bahiri1,2(✉), Abdellah Zyane2(✉),
Abdelilah Ghammaz1, and Christophe Chassot3
1 L.S.E.T., FST-G, Cadi Ayyad University, Marrakech, Morocco
bahirimn@gmail.com, a.ghammaz@uca.ma
2 S.A.R.S. Team, ENSA, Cadi Ayyad University, Saﬁ, Morocco
a.zyane@uca.ma
3 INSA, CNRS, LAAS, Toulouse University, 31400 Toulouse, France
chassot@laas.fr
Abstract. Internet of Things (IoT) transformed the Internet into a fully integrated
future Internet by developing the way objects exchange data with users all over
the world; IoT has an important impact on intelligent decisions made by devices
interacting with humans in diﬀerent applications domains. However, IoT suﬀers
from many issues, most importantly the incapability to scale automatically
depending on the overload made by the enormous number of users and/or massive
volume of exchanged data. This paper gives, for the ﬁrst time, a new way to
analyze the scalability issue in the IoT systems. In order to promote the vision of
an autonomic middleware-level scalability management for IoT systems based
on cloud computing, we propose an enhancement of the architectural design of
the monitoring components making our IoT system more scalable without any
QoS symptom degradation. Using two proposed oriented scalability mechanisms,
a series of experiments have been carried out to evaluate the performance of our
approach, for diﬀerent scenarios testbed. Generally, our results are better than
that provided by the current solving approaches taken as reference.
Keywords: Internet of Things · Machine to machine · Scalability · Autonomic
computing · Cloud computing · MAPE-K cycle · Middleware · Monitoring
1
Introduction
The huge advancement of electronics, wireless communications, mobile devices and
services, shifted the way humans uses the Internet, from just connecting them to the
Internet by consulting websites, social media and other simple tasks, to the paradigm
Internet of Things (IoT) which is interconnecting both physical and logical worlds by
connecting anything, anywhere anytime, this way of interacting with the Internet helped
another paradigm to show up named machine to machine (M2M) which eliminate at a
certain level the human intervention.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_24

Since 1999, IoT was ﬁrst mentioned by Kevin Ashton one of the founders of the
original Auto-ID Center at MIT, imagining a world where the physical world presented
by spared sensors and a platform based on real-time communications, connected to
Internet [1]. Few years later, the same group was the ﬁrst to give a deﬁnition of IoT
paradigm [2], that was an innovation of the RFID technologies.
Nowadays IoT is an environment that oﬀers to users diﬀerent services by connecting
heterogeneous devices to the Internet [3], IoT covers a large domain of connected
devices or intelligent products, sensors, actuators, cars, road and buildings infrastructure
elements, and any other device or object [4, 5], equipped with a microcontroller, a radio
interface for communication, with limited capabilities in terms of power and memory
[6], allowing them to exchange data and make smart decisions with or without having
any sort of direct communication interfaces on those devices “things” [6].
IoT is about making things smarter, being able to understand surrounding data, to
make smart decisions, request and change states by interacting with servers over the
Internet [7]. The evolution of smart objects made another IoT vision called machine to
machine (M2M), deﬁned by the ETSI Technical Committee on Machine-to-Machine
Communications (ETSI TC M2M) [8]. M2M is considered as an integral part of IoT
and gives several beneﬁts to the industry and companies [9–11].
Many attempts made to provide a single definition of acronym M2M: Machine-to-
Machine, Machine-to-Mobile (or vice versa), Machine-to-Man, etc. While M2M is
considered “Machine-to-Machine” which is itself a shortened acronym for M2-(CN2)-
M: Machine-to (Communication-Network-to) Machine [9].
One of the biggest challenges for IoT networks is to maintain its system-production
at high and acceptable level of QoS, in order to respond to requests from users and
applications in a dense environment. In other words, the IoT networks needs to scale in
case of overload caused by users and applications requests and/or a huge volume of
exchanged data [8, 9].
The objectives of our proposed approach are to maximize the number of satisﬁed
requests from diﬀerent applications towards IoT networks while sustaining the platform
performances in terms of QoS metrics (RTT, RAM, CPU). In order to do that; a dynamic
adaptation should take place in our middleware monitor so as the latter can make deci‐
sions automatically, based on symptoms detection triggered by the OM2M platform
stats, which help the executer to take diﬀerent decisions on incoming requests.
The paper is organized as follow, Sect. 2 describes the IoT application domains and
normalized architectures, Sect. 3 gives our analysis for the IoT networks scalability
problem and states the existing solving approaches. The solving proposed approach is
detailed in the fourth section, while Sect. 5 presents and analyzes numerical results.
Finally, Sect. 6 concludes the paper.
2
IoT Application Domains and Architectures
2.1
IoT Application Domains and Business Sector
IoT oﬀer huge opportunities to develop intelligent applications in diﬀerent ﬁelds, due
to the capacity to collect data from electronic devices connected to natural phenomena,
282
M.N. Bahiri et al.

user tools and buildings, medical materials and other electronic or physical world
sensors.
The IoT application domains and related applications, can be grouped in three major
domains [9]: (i) Health will-being domain, (ii) Smart City domain and (iii) Industrial
domain.
Beneﬁts on all the ﬁelds that came with IoT fueled the interest of business sector,
governments and standards. Business sector is moving towards the idea of reducing the
costs and increase eﬃciency, in other hand, governments opt for sustainability and
security. In 2009, ARRA (American Recovery and Reinvestment Act) has accelerated
the development of smart grid domain by investing 4.5 trillion dollars in USA [12]. In
2014, BMW, Peugeot and Volvo developed the eCall 3GPP to help the driver in case
of accident. This project was supported by ACEA (Association of European Automobile
Manufacturers), AASM (European Automobile Manufacturers Association) and
ERTICO in Europe [13].
IoT has gain more interest during the years. However, it connects billions of objects,
which will generate much larger traﬃc, and more data storage will be needed [14]. In
addition, IoT will face many challenges especially related to privacy, security, scala‐
bility, interoperability, reliability and QoS [15].
According to the current evolution of the Internet, we expect virtually everything we
use to be connected to the Internet. The “Internet of Things” is growing exponentially,
and its progression varies to a large degree. GSM Association (GSMA) estimates
connecting 24 billion devices by 2020, while Cisco and Ericsson think that IoT will
connect 50 billion devices [15].
IoT networks adopted for the first step, a vertical vision [16] where each appli‐
cation has its dedicated infrastructure and services. This approach results a pointless
redundancy and a cost increase owing to that the similar applications do not share
characteristics for network and services. Those issues were overtaking by the hori‐
zontal approaches [17], offering flexibility in managing the network as well as serv‐
ices by sharing infrastructure, environment and network elements instead of working
in isolation.
Many standard organizations are putting eﬀort to deﬁne IoT reference architecture
[17] emphasizing machine-to-machine (M2M) communication that is pragmatic to
determine functional modules of IoT architecture. ETSI (European Telecommunications
Standards Institute), ITU-T (International Telecommunication Union) [12], TIA (Tele‐
communications Industry Association), OMA (Open Mobile Alliance), GISFI (Global
ICT Standardization Forum for India), CCSA (China Communication Standard Asso‐
ciation) are some examples of standard organizations working on IoT and M2M commu‐
nication.
2.2
IoT Architectures According to ETSI
According to ETSI, the physical-cyber world interacts in three diﬀerent phases/layers
[17]:
• Collection phase: responsible for sensing and collecting data from the physical word.
A New Monitoring Approach with Cloud Computing
283

• Transmission phase: after data collecting, it needs to be transmitted to applications
across the network using heterogeneous access network technologies.
• Process, management and utilization phase: responsible for managing the
processed information and forwarding it to applications.
3
Problem Statement and Related Works
The use of IoT and M2M networks are only limited by the humans needs and imagina‐
tion, it oﬀers huge opportunities for diﬀerent sectors to improve eﬃciency and make
autonomic systems that are able to make decisions with minimal human intervention,
which leads to a huge number of users and applications making, requesting and
exchanging diﬀerent data forms and sizes in every possible ﬁeld.
In the literature, the IoT networks scalability problem is not yet clearly analyzed. In
this section, we will present system scalability issue in general, and for the ﬁrst time,
we will state our vision for the IoT networks scalability problem analysis at diﬀerent
levels.
3.1
Scalability Deﬁnition
Scalability is deﬁned as the ability to maintain the performance of the system-production
through system reconﬁguration by adding or removing resources with minimal cost and
minimal time depending on (i) increase or decrease of number of users, (ii) the nature
and the complexity of the requests and (iii) data types and volumes [4, 18, 19].
After several bibliographic searches in the literature of IoT researches, we found
that, since 2010, the scalability problem is mostly reported, but not suﬃciently analyzed
or treated. Our vision of scalability can be classiﬁed either in terms of IoT architectures
or in terms of overloads.
3.2
Scalability Issue in Terms of Overload
The scalability problem can be classiﬁed into user (devices) and/or data load in a dense
environment.
User (devices) overload
IoT interconnect a huge number of geographically distributed users and devices, using
applications and services [3, 7, 20]. This number of devices and services cause a user
overload. Insuring scalability is the ability the scale system in order to manage a large
number of users and devices in diﬀerent locations [21].
In case of user overload, at the sensing layer, each access media device have a limited
number of possible communications with IoT objects, however, it will be diﬃcult to
manage a massive number of devices trying to be connected and demanding names and
addresses. At the network layer, router functionalities will aﬀect the processing time
and give more delay to handle packets, which will oversize the waiting queue, causing
packets loss. At the application layer, devices will request diﬀerent services provided
284
M.N. Bahiri et al.

by physical machines, in case of enormous number of users (devices), the processing
power will be aﬀected, inﬂuencing the availability of requested services.
Data overload
As the number of embedded technologies grow leading to enormous deployment of
devices (sensors, actuators, etc.), the size of data produced and exchanged between those
devices grow. That leads to a huge amount of generated traﬃc from devices, notably in
case of large size of transmitted data such as videos or heavy load such as complex
events which cause data overload [6, 21, 22].
In this case, at the sensing layer, the volume of data has no eﬀect on the access media.
At the network layer, voluminous data need more bandwidth and more transfer time, if
not managed properly, a huge transfer delay will take place. At the application layer,
devices will exchange voluminous data that need to be stored in physical machines,
those machines have a limited storage capacity that cannot be exceeded. A loss of data
will occur.
The scalability in terms of overload is the ability to maintain performance in case of
users and/or data overload by enabling system adjustment and/or by easily expanding
its resource pool in response of heavier load [23, 24]. Elastic resources that can oﬀer
scalability are one of the major features of Cloud Computing in processing and data
storage. Furthermore, a few research has been made to deal with the scalability issue in
IoT networks coupled with Cloud Computing capabilities [6, 7, 22, 25, 26].
Also, middleware is software that is located between applications, services, and their
elemental distributed architecture and platforms [27]. Middleware is providing many
types of capabilities to developers, such as higher-level programming abstractions to
support the development of applications and services, supporting end-to-end quality
attributes, essentially scalability, and masking the inherent complexity of IoT systems
like system failure and heterogeneity of languages, operating systems, and networks.
In order to make our M2M platform more scalable in case of overload caused either
by data or users/applications, our based cloud computing proposed approach uses a
middleware monitoring that can provide a total control of requests, depending on the
current state of the M2M platform in terms of QoS metrics, to maximize the number of
request handled by the M2M platform.
3.3
Existing Solving Approaches
The IoT paradigm is quite recent. Few approaches have been proposed in the literature
in order to give a solution to the scalability challenge for IoT networks. Here are some
interesting approaches: DIAT - Distributed Internet-like Architecture for Things [28],
CloudThings [7], AAN - Application Assist Network [22] and IoTCloud [25].
One of the most important approaches that tackles QoS (RTT: round-trip time)
management in the IoT networks, is “Monitoring Solution for Autonomic Middleware
level QoS Management within IoT Systems” [27]. This oriented QoS approach propose
a middleware solution that ﬁlters the requests depending on the RTT metric, relaying
on a key paradigm called the Autonomic Computing, which allows dynamicity in an
autonomous way by using the MAPE-K cycle [29].
A New Monitoring Approach with Cloud Computing
285

This approach collects events from ETSI-compliant OM2M open source platform,
generates symptoms and execute plans to eithers allow all the traﬃc, or reject a portion
of exogenous traﬃc coming from diﬀerent applications, in order to promote the vision
of an autonomic middleware-level QoS management for IoT systems.
This approach [27], uses a middleware solution that drops requests from exogenous
applications in case of a detected overload, on one hand, it improves RTT and stabilizes
the OM2M platform. However, on the other hand, it decreases the number of satisﬁed
requests from clients and gives no importance to the used resources in the OM2M plat‐
form.
Our main problem is to maximize, in case of overload, the number of requests
handled by the OM2M platform, making it scalable, respecting the QoS metrics (RTT,
CPU and RAM).
4
Solving Approach Proposed
4.1
Autonomic Scalability-Oriented Middleware for the IoT
In this paper, we explore our vision to improve an ETSI compliant speciﬁcation of the
middleware level for the IoT, in order to provide it with autonomic and dynamic scal‐
ability oriented capabilities. Such an enhanced middleware is proposed to elaborate
application level scalability requirements and to manage them in a dynamic and auto‐
nomic way, with the consideration of evolving resources related to the computing and
network contexts. Inspired from our background in transport and network-oriented
mechanisms, this dynamic and autonomic management is based on the execution of
scalability mechanisms considering RTT, RAM and CPU metrics.
In the interest of providing an autonomic and dynamic scalability oriented capabil‐
ities, we use autonomic computing paradigm based on the MAPE-K cycle, as shown in
Fig. 1:
• Monitoring of the managed resources and its current performances associated with
its current conﬁguration or QoS satisfaction based on SLA, with symptoms detection
based on deﬁned rules.
• Analysis of the generated symptoms with the aim to identify the possible causes of
them, based on stored information in knowledge base, if changes are required, a
request for change is sent to the plan function;
• Planning of the actions needed to be set up with the aim to match the targeted goals;
it creates or select procedures/plans to enact on the managed entity;
• Execution of the elaborated plans to change the behavior of the monitored entity
through the eﬀectors.
Within the IBM model, a Knowledge base allows storing data (metrics, symptoms,
policies, topology information, etc.) that are required for the implementation of the
previous steps.
Inspired from a similar eﬀort performed at monitoring component and on the QoS
issue (RTT only) [27] for the entire MAPE-K loop, our approach is focused on the
monitoring component and on the scalability issue with the consideration of QoS issue
286
M.N. Bahiri et al.

(RTT, RAM and CPU metrics). Our architecture, presented in Fig. 2, is based on the
use of both a middleware monitoring and cloud computing processing power.
Fig. 1. The MAPE-K cycle of the AC paradigm [31]
Cloud Computing provides [25, 30, 31] resource ﬂexibility and elasticity, reliability
and QoS by using the concept of virtualization and migration; Low energy consumption
due to the autonomic management of physical resources; high availability concept by
relaying on redundancy mechanisms and migration; low cost due to service on demand
concept that oﬀers resources allocation and deallocation on demand. The cloud appears
to be the natural home for IoT applications, since it oﬀers an enormous processing power
and storage capacity.
A New Monitoring Approach with Cloud Computing
287

Fig. 2. Our proposed approach architecture
Our architecture is composed of:
(i) Applications which simulate the real-world sensors, actioners, applications and
users. This component generates two types of traﬃcs: (i) an important or utile traﬃc,
(ii) and exogenous traﬃc, depending on the type of request.
(ii) OM2M representing the platform where all the requests will be destined to, with
an implementation of an CPU and RAM observer based on the speciﬁcations of the
standard JMX (Javax Management eXtensions) through its implementation known as
Jolokia agent.
(iii) The middleware monitor will be assuring the following tasks:
• Monitoring: this component, as shown in the Fig. 3, is responsible for collecting
information about CPU and RAM usage and RTT evolution from the OM2M plat‐
form. All collected events are forwarded to the complex event processing (CEP) in
Fig. 3. Monitoring component architecture
288
M.N. Bahiri et al.

order to generate the adequate symptoms. In our approach, we consider only the RTT
metric provoked by the useful traﬃc which is important to satisfy.
• Analyser/plan: generates the plan to execute relaying on the symptoms received
from the monitoring.
• Executer: execute the received plan, which is either forwarding requests, dropping
requests or redirecting requests to the cloud computing.
The applications generate requests that are deﬁned by the targeted OM2M service
along with their numbers and periodicity. Those generated requests are then received
by the executer, which in the ﬁrst-place forwards them automatically to the OM2M
targeted service. This platform hosts various services than can be either satisfying POST
of GET requests arriving from the applications. The monitoring component collects
information about CPU, RAM usage and RTT evolution from the OM2M platform. Then
with the help of the complex event processing (CEP), it generates symptoms that will
be forwarded to the Analyse/Plan component which decides the adequate action that
will be taken by the executer. From now on, every request generated by the applications
will be either forwarded to OM2M, redirected to the Cloud, or dropped depending on
the stat of the OM2M platform decided by the middleware monitor.
Our approach is aiming to maximize the number of requests executed by the IoT
networks, which is scaling depending on the current load, considering QoS (CPU, RAM,
RTT). The next section will present and analyzes numerical results.
4.2
Dynamic Scalability-Oriented Mechanisms
Table 1 present the adequate symptom generated and the decision made by the executer
regarding the used mechanism.
Table 1. Symptoms versus mechanisms decisions
5 RTTs
M0
M1
M2
M3
Preferable
5 RTT <2000 ms
Forward all
requests to the
platform
Forward all requests to the platform
Acceptable
2000 < 5
RTT <4000 ms
Drop 50% of
exogenous traﬃc
Drop 50% of all
traﬃc
Redirect 50% of
all traﬃc
Critical
4000 ms <5 RTT
Drop 100% of
exogenous traﬃc
Drop 100% of all
traﬃc
Redirect 100% of
all traﬃc
The main role of our middleware monitor is to take decisions when an overload
occurs in order to maximize the number of satisﬁed requests while maintaining the
OM2M platform performances in terms of an acceptable RTT and without overusing
the RAM and CPU of the platform.
Our approach forwards less requests to the OM2M platform and drops or redirects
the other ones to the Cloud based on the platform stats so as to respect the QoS metrics.
The following Figs. 4 and 5 explains the M3 mechanism algorithm at the monitoring
and the executer component.
A New Monitoring Approach with Cloud Computing
289

When the burst of events lower than the threshold of 2000 ms is detected by the
monitoring component, the symptom Preferable is generated, and for all mechanisms,
the decision is automatically made to forward all traﬃc to the OM2M platform
Without scalability mechanism-, all traﬃc is forwarded to the OM2M platform; For
M1 mechanism - i.e., a dynamic QoS oriented mechanism is deployed on the proxy
component-, 50% of exogenous traﬃc is dropped depending on the source address [27];
For M2 mechanism - i.e., a dynamic scalability oriented mechanism is deployed-, 50%
of all traﬃc is dropped, while observing the evolution of useful traﬃc RTT metric
depending on the type of traﬃc (web service); For M3 mechanism - i.e., a dynamic
scalability oriented mechanism is deployed-, 50% of all traﬃc is redirected to a cloud
computing infrastructure, while observing the evolution of useful traﬃc RTT metric
depending on the type of traﬃc (web service).
Fig. 4. Monitoring algorithm for M3
When the burst of events upper than the threshold of 4000 ms is detected by the
monitoring component, the symptom Critical is generated; For M0 mechanism - i.e.,
without scalability mechanism-, all traﬃc is forwarded to the OM2M platform; For M1
mechanism - i.e., a dynamic QoS oriented mechanism is deployed on the proxy compo‐
nent-, 50% of exogenous traﬃc is dropped depending on the source address [27]; For
M2 mechanism - i.e., a dynamic scalability oriented mechanism is deployed-, 100% of
all traﬃc is dropped, while observing the evolution of useful traﬃc RTT metric
depending on the type of traﬃc (web service); For M3 mechanism - i.e., a dynamic
scalability oriented mechanism is deployed-, 100% of all traﬃc is redirected to a cloud
290
M.N. Bahiri et al.

computing infrastructure, while observing the evolution of useful traﬃc RTT metric
depending on the type of traﬃc (web service).
Fig. 5. Executer algorithm for M3
The same process of dynamic symptoms detection and decision was applied to the
rest of the observed events.
5
Numerical Results
We compare the rate of satisﬁed (processed) and lost traﬃc (requests), for all the mech‐
anisms M0 (without scalability mechanism), M1 [27], M2 (dynamic scalability oriented
mechanism dropping traﬃc) and M3 (dynamic scalability oriented mechanism redi‐
recting traﬃc to Cloud), as described in Fig. 6. Figure 7 shows the number of satisﬁed
traﬃc inside the OM2M platform and the number of satisﬁed traﬃc in Cloud for M3
mechanism. We also observe the evolution of the QoS parameters (RTT, CPU and RAM
usage) for all mechanisms (respectively Figs. 8, 9 and 10).
Table 2. Scenario testbed
Injector
Request number
Periodicity
Start time (s)
Useful
200
500
0
Exogenous
400
50
10
In the scenario testbed described in Table 2, we consider ﬂows (http requests towards
the OM2M platform) coming from two diﬀerent traﬃc source, called injectors
(useful_injector, and exogenous_injector). Each injector is characterized by: number of
http requests (requests number), request method (e.g., POST, GET), destination, period
between two successive requests (periodicity) and ﬁnally starting time in seconds.
A New Monitoring Approach with Cloud Computing
291

Fig. 6. Satisﬁed and lost requests of the four mechanisms
In Fig. 6, column indicates the number of injected requests. We use diﬀerent colors
to distinguish the number of satisﬁed and lost requests for the deployed mechanisms.
We observe that M0 satisfy more traﬃc requests than M1 and M2, because M0 try to
satisfy requests regardless the respect of QoS metrics, while, for M2 and M3 the QoS
metrics are key parameters to take any decision. M2 drop more requests than the other
mechanisms, because the decision is made at the input of the middleware, before any
processing. We state that M3 satisfy more traﬃc requests than all other mechanisms.
This can be explained as follows: the redirection to the cloud (which provides high
processing power) as a solution when the platform is overloaded permit to maximize
the number of satisﬁed requests without rejecting any traﬃc. Our IoT system has become
more scalable.
From Fig. 7, among all the satisﬁed requests for M3, we observe that about 31% of
the requests are redirected and processed in the Cloud while 69% are satisﬁed inside the
OM2M platform out of all the satisﬁed requests.
Fig. 7. Satisﬁed requests for M3
292
M.N. Bahiri et al.

The use of mechanisms will aﬀect the QoS in terms of RTT, CPU and RAM of the
OM2M platform depending on overload made by the traﬃc. Figure 8 show the evolution
of RTT, while Figs. 9 and 10 shows the CPU and RAM usage depending on the
performed mechanisms.
In Fig. 8, we observe that M2 and M3 mechanisms, permit to return the OM2M
platform to a stabilized state (acceptable symptom), quicker than the mechanisms M0
and M1, because we attempt to avoid the overload inside the OM2M platform by drop‐
ping or redirection supplementary traﬃc. Beneﬁts monitoring and runtime adaptation
including dynamic symptoms detection and simple adaptation activation have been
shown.
Fig. 8. RTT evolution of useful traﬃc
Fig. 9. The RAM usage (%) inside the OM2M platform
A New Monitoring Approach with Cloud Computing
293

Fig. 10. The CPU usage (%) inside the OM2M platform
In Figs. 9 and 10, we observe respectively the evolution of CPU and RAM usage
inside the OM2M platform. Generally, we can note that both metrics are not largely
aﬀected by our proposed approach. They follow the same evolution in the case of M0
and M1 mechanisms.
Our approach makes the OM2M platform scalable all by respecting QoS metrics. It
gives the opportunity to the OM2M platforms to satisfy more requests than any other
proposed approach. Using our middleware monitor and cloud computing, as shown in
this section, we did improve the number of satisﬁed requests. Also, we were able to
stabilize the OM2M platform in a faster way, taking into consideration the QoS metrics.
Concerning the used resources, CPU and RAM were not largely aﬀected.
6
Conclusion
In this paper, we have presented for the ﬁrst time, a new vision to analyze the scalability
issue in the IoT systems. Then, for the purpose to promote the vision of an autonomic
middleware-level scalability management within a real IoT system, we have proposed
an enhancement of the architectural design of the monitoring components, by proposing
a proof of concept implementation principles and tested through an emulation platform
aimed at stressing the ETSI-compliant OM2M open source platform.
We have elaborated more complex symptoms involving several QoS-oriented and
resource oriented metrics, and we have proposed two oriented scalability mechanisms
(adaptation actions) to be activated in order to make our IoT system more scalable
without any symptom degradation.
Simulation results show that M3 mechanism outperforms other mechanisms recently
proposed in the literature of IoT system, because of the utilization of the Cloud
Computing concept.
It would also be interesting to perform new experiments with more complex
scenarios, and diﬀerent types of traﬃc and injectors. Another way consists of introducing
diﬀerent priority of traﬃc according to diﬀerent service legal agreements (SLAs). We
are now investigating the way to possibly improve the scalability of our IoT system by
294
M.N. Bahiri et al.

taking into account the kind of traﬃc requests and mixing the redirecting, dropping and
forwarding actions taken according to QoS metrics evolution.
References
1. Ashton, K.: That ‘internet of things’ thing. RFID J. 22, 1–2 (2009)
2. Brock, D.L.: A naming scheme for physical objects. In: The Electronic Product Code (EPC)
White Paper, pp. 1–21 (2001)
3. Janggwan, I., Seonghoon, K., Daeyoung, K.: IoT mashup as a service: cloud-based mashup
service for the Internet of Things. In: IEEE 10th International Conference on Services
Computing, pp. 462–469 (2013)
4. Rahim Biswas, A., Giaﬀreda, R.: IoT and cloud convergence: opportunities and challenges.
In: IEEE World Forum on Internet of Things (WF-IoT), pp. 375–376 (2014)
5. Manate, B., Ion Munteanu, V., Fortis, T.F.: Towards a scalable multi-agent architecture for
managing IoT. In: Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet
Computing, pp. 270–275 (2013)
6. Cirani, S., Davoli, L., Ferrari, G., Léone, R., Medagliani, P., Picone, M., Veltri, L.: A scalable
and self-conﬁguring architecture for service discovery in the Internet of Things. IEEE Internet
Things J. 1, 1–13 (2014)
7. Zhou, J., Leppänen, T., Harjula, E., Ylianttila, M., Ojala, T., Yu, C., Jin, H., Yang, L.T.:
CloudThings: a common architecture for integrating the Internet of Things with cloud
computing. In: IEEE 17th International Conference on Computer Supported Cooperative
Work in Design (CSCWD), Whistler, BC, pp. 651–657 (2013)
8. ETSI TC: M2M, ETSI TS 102 689 v1.1.1 (2010-08): Machine-to-Machine Communications
(M2M). M2M Service Requirements (2010)
9. Boswarthic, K.D.: M2M communications - a systems approach. In: Proceedings of the IEEE
17th International Conference on Computer Supported Cooperative Work in Design, pp. 651–
657 (2013)
10. Floeck, M., Papageorgiou, A., Schuelke, A., Song, J.: Horizontal M2M platforms boost
vertical industry: eﬀectiveness study for building energy management systems. In: IEEE
World Forum on Internet of Things (WF-IoT), pp. 15–20 (2014)
11. Liu, Y., Yuen, C., Chen, J., Cao, X.: A scalable hybrid MAC protocol for massive M2M
networks. In: IEEE Wireless Communications and Networking Conference (WCNC): MAC,
pp. 250–255 (2013)
12. International Telecommunication Union: Overview of the Internet of Things:
Telecommunication Standardization Sector of ITU (2012)
13. Katusic, D., Weber, M., Bojic, I., Jezic, G., Kusek, M.: Market, standardization, and regulation
development in machine-to-machine communications. In: 20th International Conference on
Software, Telecommunications and Computer Networks, pp. 1–7 (2012)
14. Tan, L., Wang, N.: Future internet: the Internet of Things. In: 3rd International Conference
on Advanced Computer Theory and Engineering (ICACTE), V5, pp. 376–380 (2010)
15. Gang, G., Zeyong, L., Jun, J.: Internet of Things security analysis. In: International Conference
on Internet Technology and Applications (iTAP), pp. 1–4 (2011)
16. Boswarthick, D., Elloumi, O., Hersent, O.: M2M Communications: A Systems Approach.
Wiley, Hoboken (2012)
17. Borgia, E.: The Internet of Things vision: key features, applications and open issues. Comput.
Commun. J. 54, 1–31 (2014)
A New Monitoring Approach with Cloud Computing
295

18. Putnik, G., Sluga, A., ElMaraghy, H., Teti, R., Koren, Y., Tolio, T., Hon, B.: Scalability in
manufacturing systems design and operation: state-of-the-art and future developments
roadmap. CIRP Ann. Manuf. Technol. J. 62, 751–774 (2013)
19. Spicer, P., Koren, Y., Shpitalni, M., Yip-Hoi, D.: Design principles for machining system
conﬁgurations. CIRP Ann. Manuf. Technol. J. 51, 105–108 (2002)
20. Huang, P.K., Qi, E., Park, M., Stephens, A.N.: Energy eﬃcient and scalable device-to-device
discovery protocol with fast discovery. In: IEEE International Workshop of Internet-of-
Things Networking and Control (IoT-NC), pp. 1–9 (2013)
21. Scalable SQL: Communications of the ACM, pp. 48–53 (2011)
22. Matoba, K., Abiru, K., Ishihara, T.: Service oriented network architecture for scalable M2M
and sensor network services. In: 15th International Conference on Intelligence in Next
Generation Networks, pp. 35–40 (2011)
23. Koren, Y.: General RMS characteristics. Comparison with dedicated and ﬂexible systems. In:
Reconﬁgurable Manufacturing Systems and Transformable Factories, pp. 27–45. Springer,
Berlin (2006)
24. Pandey, S., Voorsluys, W., Niu, S., Khandoker, A., Buyya, R.: An autonomic cloud
environment for hosting ECG data analysis services. Future Gener. Comput. Syst. J. 28, 147–
154 (2012)
25. Geoﬀrey, C.F., Kamburugamuve, S., Hartman, R.D.: Architecture and measured
characteristics of a cloud based Internet of Things. In: International Conference on
Collaboration Technologies and Systems (CTS), pp. 6–12 (2012)
26. Blair, G., Schmidt, D., Taconet, C.: Middleware for Internet distribution in the context of
cloud computing and the Internet of Things. Ann. Telecommun. J. 71, 87–92 (2016)
27. Banouar, Y., Reddad, S., Diop, C., Chassot, C.H., Zyane, A.: Monitoring solution for
autonomic middleware level QoS management within IoT systems. In: IEEE/ACS 12th
International Conference of Computer Systems and Applications (AICCSA), pp. 1–8. IEEE
(2015)
28. Sarkar, C., Akshay, U.N.S.N., Venkatesha Prasad, R., Rahim, A.: A scalable distributed
architecture towards unifying IoT applications. In: IEEE World Forum on Internet of Things
(WF-IoT), pp. 508–513 (2014)
29. Horn, P.: An Architectural Blueprint for Autonomic Computing. IBM White Paper, 3rd edn.
(2005)
30. McCabe, L., Aggarwal, S.: La migration vers le Cloud pour les PME. SMB Group, Inc., Boston
(2012)
31. Ramasahayam, R., Deters, R.: Is the cloud the answer to scalability of ecologies? Using GAE
to enable horizontal scalability. In: 5th IEEE Conference DEST, pp. 317–323 (2011)
296
M.N. Bahiri et al.

MDA Approach for Application Security
Integration with Automatic Code Generation
from Communication Diagram
Lasbahani Abdellatif1(✉), Mostafa Chhiba1,
Abdelmoumen Tabyaoui1, and Oussama Mjihil2
1 RMI Laboratory, FST Settat, Univ Hassan 1, 26000 Settat, Morocco
abbdellatif.lasbahani@gmail.com, moschhiba@yahoo.fr,
atabyaoui@gmail.com
2 IR2M Laboratory, FST Settat, Univ Hassan 1, 26000 Settat, Morocco
oussama.mjihil@gmail.com
Abstract. Recently, there has been much research works treating Model Driven
Architecture (MDA) approaches for automatically generating and personalizing
applications using the system models. To the best of our knowledge, most of these
works have been addressing code generation without taking into account the no-
functional aspect: security. In this current work, we are proposing a new contri‐
bution to generating secure applications with their security mechanisms basing
on MDA approach in order to address both the functional and non-functional
aspects during software engineering process.
This paper proposes an MDA-based methodology for integrating security
properties during the system design. These properties are formulated by security
expert based on security requirements of the organization. In this work, we focus
our concentration on some properties such as data encryption, Message Integrity,
and Access Control so that to show the importance of this contribution and make
in practice the generation of secure applications. To do that, these properties will
be incorporated in the form of security models during the system design after their
transformation to security tagged values using UML Proﬁle. Moreover, system
functional models will be enriched with system security requirements through the
proﬁling concepts which contain a set of tagged values about software security
aspect.
Within this context, security models can be merged with system models in
diﬀerent abstraction levels by applying a set of model-to-model transformation.
Finally, source code and the conﬁguration ﬁles will be generated automatically
from communication diagram after its extension by applying a model to code
transformation.
Keywords: Model Driven Architecture · Code generation · Communication
Diagram · Application security · Security policies
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_25

1
Introduction
Currently, there has been a massive use of the web and desktop applications. They are
fast increasing importance according to their broad and diverse deployment. IT solutions
are now used in diﬀerent areas such as banking, health-care, telecommunications,
e-commerce, e-learning, and other domains. Consequently, application security
becomes a determinant factor, thus any communication between the application and its
outside actors shall be performed according to the security constraints already analyzed
by the application designers and security expert.
Nowadays, generated applications don’t respect the aspect of security which is one
of the most signiﬁcant non-functional aspects. In a security context, they will be deﬁned
through ﬁve important essential elements: Availability, Integrity, Traceability,
No-Repudiation, and Conﬁdentiality of critical data abbreviated in CAITN standard.
These criteria of the security will be divided into two important topics which will be
treated in two interesting works. In the ﬁrst, we have limited the area of this work only
to the following elements: Authorization, Authentication, Communication encryption,
and Integrity. While we addressing the others elements in future work.
According to this massive use of mobile and web application, we have addressed
this increasing use by improving software engineering process and automating code
generation by taking into account automating integrating of the security policies in
system design phase in order to ensure secure communication between application and
external parties. So we have contributed by proposing an enhancement on MDA
approach so that it can include security policies and all technical details in MDA process.
Through the use of this methodology which is driven by models, building time of soft‐
ware is reduced and security vulnerability too. So generating secure application will be
done automatically basing on the newly approach.
Getting secure application basing on MDA approach obliges to make a radical
change in classical software development process in order to address code generation
and security policies integration at the same time. This change is produced to resolve
customer’s needs, use all resources in the secure way, and to face to information tech‐
nology evolution. Therefore, to resolve well these needs and continued to grow of use
of the application, MDA approach has been improved to adapt their process to this
change by additional some important iterations for automating generating the secure
application, granting permissions and verify them easily, reducing lead time of the real‐
ization of the software, and improving planning quality.
By means of this methodology, Software development process can represent the
software by a set of models: functional and no-functional models. The ﬁrst one repre‐
sents a structured representation of the system while the second represents the no-func‐
tional aspects such as robustness, availability, security concerns, and performance. In
previous works, the both aspects were treated in separated manner but since nowadays
they will be functioned at the same time by merging them so that to improve the quality
of service. In other word, system requirements will be enhanced with even more
powerful models as security models or security policies corresponding to security
constraints, organization concepts, authorization constraints, and all technical detail at
class diagram level and not during the implementation phase. Authorization policies
298
L. Abdellatif et al.

will be designed basing on security meta-model which is also based on RBAC model
extended with constraints basing on object constraint language (OCL) [1]. Therefore,
generating security infrastructure request a prior injection of the authentication and
authorization properties in system design phase and in all abstractions of the system. In
this work, these injections will be done basing on UML proﬁle and OCL to make
constraints on protected data.
In this work, Message Integrity, Communication encryption, and authorization poli‐
cies are designed basing on proposed meta-model of security which is based o RBAC
model extended with UML proﬁle technology and OCL to enrich the speciﬁcations
models with those concepts such as permissions rules, assigning privileges to user and
protected resources, verifying of the roles which have already grunted to role. Conse‐
quently, these properties should be generated automatically in the XML ﬁle and should
be respected during communication between application and external customers. In
addition, It is important to take into account these properties during the implementation
phase. Security requirements injection and code generation are performed basing on
model transformation which is divided into two types: model to model (M2 M) and
model-to-text (M2T). Using model transformation allows us to enrich system models
with further information and transform them from an abstract model into more secure
concrete models. On the other side, performing a set of transformation allow us to get
a signiﬁcant gain in productivity.
To do so, we have given more attention to modeling and analyzing rather than
programming by using an approach MDA [2] which is an implementation of the Model
Driven engineering (MDE) proposed by the Object Management Group (OMG) [3]
initiated in 2000. The main objectives of MDA are: developing suitable models, building
a new application, obtaining a new application from existing application, controlling the
impact of the new technologies, automatically code generation from models, increasing
the productivity of the models. Consequently, functionality models will be reﬁned to
incorporate security considerations and all no-functional aspects by applying a set of
model transition. Then, the ﬁnal code will be generated automatically from PSM by
applying a set of transformation, and by means of the domain-speciﬁc code generator
which automatically transforms PSMs models into the source code.
The main objectives of this paper are summarized as follows:
• This work automates software development process (SDP) which is beginning by
analysis and design phase and ending by implementation phase and reducing design
mistakes.
• This work proposes an approach based on models allowing security integration.
• This work develops a code generator that include model-driven security and allows
generate application security infrastructure.
• This work allows automatically code generation for chosen platforms (JEE, .Nets,
PHP, etc.) from communication diagram (CD) which is used like platform inde‐
pendent model called in terminology MDA (PIM).
• This work allows generating maintainable, less costly and reusable software systems.
This paper is organized as follows. Section 2 summarizes related works. An Over‐
view MDA is presented in Sect. 3. We discuss the proposed approach in Sect. 4.
MDA Approach for Application Security Integration
299

In Sect. 5 we apply security constraints on system models. In Sect. 6 we discuss on tools
and technicals. We present results in Sect. 7. And ﬁnally, we brieﬂy discuss future works
and conclude our paper in Sect. 8.
2
Related Works
In the last decade, there is a variety of works and many suggestions have been proposed
in the domain of code generation and data security. These works are very interesting
and play an important role in technology evolution and they work been deployed in
numerous areas such as telecommunications, e-commerce, e-learning, scientiﬁc organ‐
ization, and other domains. In this work, we have concentrated on some interesting
projects and suggestions of the high relevance to have a general idea on code generation
and security topics. So we have used them as the knowledge base for starting and
demonstrating our contribution.
The ﬁrst type of code generator has been investigated in traditional paradigm as the
study of code generation represented in many works based on code generation from
Petri-Nets which has a long tradition and used only to add further semantic deﬁnition
because the model UML is still largely undeﬁned from a semantic point of view. Petri-
Nets give several solutions allowing automatic generation of code. One among these
solutions is presented in [4] which give an overview of diﬀerent strategies to generate
code from high-level Petri-Nets. However, in some review, the most of the suggestions
have been focused on automatic code generation from low-level Petri-Nets as [5, 6]
which automates generating of controllers code. But the weakness or vulnerabilities of
these approaches that automatically code generation from low-level Petri-Nets can’t
produce complex systems based on object-oriented principles. In addition, these
approaches are used frequently in validation and veriﬁcation of requirements during
design system process without analyzing no-functional details.
Moreover, the second category of code generation is discovered for solving design
errors, mistakes, and accidents which have appeared in the ﬁrst category of code gener‐
ation. So code generation in the second category is based on model transformation by
using the model as a productive element promises a number of beneﬁts including devel‐
opment of code with high quality, improving productivity performance, improving
maintainability, reducing design errors, keep traceability between customer needs and
the ﬁnal code, integrating chosen platform description during design phase, and keeping
a consistency between ﬁnal code and design. Consequently, this category address the
generated code as a models which can be transformed to other models more speciﬁc by
applying a model to model transformation so as to extended the target language with
other semantics and enhanced generated code with further features such as methods,
interfaces classes, partial classes which allow for a single class’s members to be divided
among multiple sources code ﬁles, and partial method, security requirements, supported
platforms, authorization policies constraints, application security ﬁles, and conﬁguration
ﬁles.
However, model-driven engineering approaches (MDE) based only on model-to-
code transformation or translation from model to plain text without going on the
300
L. Abdellatif et al.

intermediate model. So, the generated models through this process can’t be enhanced
and enriched with others features, so they stay limited on producing plain text based on
text-based notation. According to this approach, several kinds of research have been
proposed as [7] which describe how UML models of a system can be transformed into
a code of an object-oriented imperative programming language or executable models
by deﬁning transformation rules which are based on the reconciliation of the diﬀerences
between UML meta-model and meta-model of the target language. Other studies have
been developed in this context like [8] which provides a method allowing generating
operations speciﬁcations from domain class diagram using transition state diagram.
While [9], addresses code generation from sequence diagram of system’s internal
behavior. In the both last works, they have concentrated on code generation of the oper‐
ations signatures with their bodies without talked about the security properties which
are a key success factor for these generated applications. So, code generation should be
performed to take into account security infrastructures or rather authorization policies
infrastructures and all no functional aspects. In addition, [10] present a case study of
code generation based on model transformation together with stratego which is based
on rewrite rules with programmable strategies for integrating model-to-model, model-
to-code, and code-to-code transformation. These strategies were supported by tow
dimension of transformation modularity: vertical and horizontal modularity. But this
work doesn’t support the entire development process, object-oriented systems, and
security integration.
Furthermore, the complex tasks during the speciﬁcation of conceptual schemas (CS)
is system Behavior modeling, which is represented by a set of operations that are used
for executing application uses cases. For that, some important approaches have been
intervened for solving modeling behavior errors by simplifying the speciﬁcation of
conceptual schemas. [11] Provides a solution for modeling system operations and
describing the system behavior, and a method allowing generating system Behavior by
completing the static aspect of the conceptual schemas, and suﬃce to perform all typical
life-cycle create/remove/update/delete (CRUD) operation. This contribution takes as
input CS expressed as a UML class diagram, which is enriched with the necessary
speciﬁcation, such as speciﬁcation of association. Then, the new CS is generated that
contains all necessary operations to start operating the system. In contrast, there are some
approaches basing on a set of tools for simplifying modeling behavior.
In the security integration domain, there are some very important contributions about
security integration during the design phase. According to these solutions, some tech‐
nical improvements need to be made in order better to achieve secure software and
enhancing software development process with other iteration allowing security policies
integration. [12] Have been addressed security integration into distributed systems by
providing an approach for developing secure distributed systems based on UML and
additional support for specifying authorization constraints. On the other hand, others
approaches have been integrated security aspects in software engineering process using
a modeling language like UML which is used for modeling role-based access control
policies to restrict system access to authorized users as shown in [13, 14], While some
works have extended UML for solving security concerns that have an eﬀect on produc‐
tivity, quality of the software, and data conﬁdentiality. In these works, security concerns
MDA Approach for Application Security Integration
301

not have been solved yet. In addition, we found also UMLSEC contribution which
addresses security integration in the design phase by using UML.
In [15, 16], David basin and al. have combined SecureUML with the design modeling
language basing on class diagrams called ComponentUML, and ControllerUML, which
is also based on states diagrams in order to facilitate security integration and getting
security architectures for distributed systems from models. While, [17] Provides some
technical’s allowing annotated UML models with authorization policies based on RBAC
to authorize signers. In this context, [18] develops RBAC using MDA approach to
beneﬁt from MDA advantages for reducing systems vulnerabilities by providing a tool-
supported framework which use the MDA approach with UML proﬁle to build RBAC
applications, and security speciﬁcations for generating systems security speciﬁcations
in extensible Access Control Markup Language (XACML) format for distributed
systems. There are also several works putting the accent on generating access control
infrastructures for server-based applications as [19]. In addition, Model-driven security
has been extended to cover database security as [20] which provides a methodology to
develop secure XML databases.
Finally, [21] deﬁnes a methodology to reﬁne application models with the appropriate
security policies which have already proposed by a security administrator. In addition,
[22] presents an approach for developing secure data warehouses independent of the
target platform basing on UML for specifying security constraints in conceptual multi‐
dimensional database modeling. [23] Is similar to [22]. But in [23], the proposed
approach is based on MDA.
This paper subscribes in the second category of code generation. Our proposal aims
to generate secure applications via a code generator by applying a set of model trans‐
formation or model transition. Through these transformations and code generator,
implementation phase will be performed automatically. In this work, we have generated
an intermediate model (IM) for chosen platform or the target platform (PSM) to enable
its extensibility by extending generated IM (structural model) for introducing others
further improvements like non-functional aspects. Notwithstanding the diversity of
these code generators and according to our best research, there is no complete code
generator allowing generating secure applications from communication diagram. In
other words, there is no complete code generator allowing security policies integration,
which have already mentioned by application designers basing on solutions provided
by the security expert. According to our comparison, the both categories of code gener‐
ation are concentrating just on functional aspects like validation and veriﬁcation of
requirements, and generation of CRUD operations like [9] and al. However, these works
still far to be the appropriate tools for generating automatically secure applications and
they have a need to further improvements.
The motivation of this work is to complete the previous works that deal with code
generation without security integration into system design phase by automating software
development process based on gait (Uniﬁed Process-extreme Programming) (UP-XP).
Our proposal proposes a code generator that supports the entire development process
together with security policies integration into design phase for generating secure appli‐
cations with its security infrastructures, and allowing generating GUI for each use case.
302
L. Abdellatif et al.

3
Overview MDA
The model Driven architecture - (MDA) [24] is an implementation of Model-driven
engineering (MDE) proposed by Object Management Group (OMG) in 2001, and it is
used extensively for designing and building software basing on the UML standard.
Furthermore, MDA provides many signiﬁcant advantages. Among the beneﬁts oﬀered
by MDA, we found: the separation of concerns between the business logic of applica‐
tions and used platforms. According to this separation, MDA provides three essential
advantages:
• Develop sustainable models; presenting the business domain system without consid‐
ering the architecture of the used platform.
• Improving productivity gain; business logic applications becomes more productive
and competitive through the transformation of models and its extensibility.
• Integrating platform architecture during design system via the transformation of
models; integrating of the technical details of the execution platforms through the
model transformation in order to get a platform speciﬁc model (PSM).
Basing on MDA approach, the software development process becomes more
modular rather than the old methodologies which are based only on UML.MDA
approach deﬁnes three levels of model’s abstraction for representing system models,
and for elaborating advanced design system:
• The ﬁrst level so-called Computational Independent Model (CIM) that gives a
requirements view and describes the situation and environment technical in which
the produced system will be used.
• The second level called Platform Independent Model (PIM) which represents an
analysis and design view. At this level, system speciﬁcations or business logic of
system will be represented without considering used platform in which the system
will be deployed.
• The third level called PSM which is obtained from PIM and gives a code view.PSM
combines the system functionalities with a chosen platform-speciﬁc in which the
system will be used.
Practically, PIMs can be represented by the domain class diagram, sequence diagram
of system’s internal or external behavior, collaboration diagram, transition state
diagram, or communication diagram. In this contribution, we have used communication
diagram (CD) as PIM Because CD more relevant to design phase rather than others.
Then, this PIM can be translated to one or more platform-speciﬁc models (PSMs)
including CORBA, .NET, J2EE, etc.
In addition, success secret of MDA resides in model transformation strategy which
allows automating code generation and to obtain a significant gain in productivity. In MDA
approach, to go from PIM to PIM or PSM, or PSM to PSM or final code, a model transi‐
tion is the obligatory stage. For that, these transitions involve some mechanisms for
model transformation. Regarding model transformation, OMG has proposed a set of the
tools and transformation languages in order to cover these transitions between different
levels of model’s abstraction. We quote Atlas transformation Language (ALT) [25] and
MDA Approach for Application Security Integration
303

Query, View, and Transformation (QVT) which may be considered as the most appro‐
priate model’s transformation languages. There are also others tools.
In MDA, the language used to create and validate diﬀerent MDA models is called
meta-model or meta-modeling language which deﬁnes the structure of the models; UML
technology is the appropriate meta-model in MDA methodology. This meta-model is
also represented in the form of model, and collects a set of classes. Meta-modeling
language has also its meta-model called meta-meta-model which is used to describe a
meta-model structure, and deﬁne a semantic to describe meta-model architecture. In
MDA context, this model known as Meta-Object Facility (MOF) [26], and which has
the ability to describes itself.
Consequently, the ﬁnal code will be generated semi-automatically from PSM models
through models transformation. Figure 1 describes the MDA architecture.
Fig. 1. MDA architecture.
4
Proposed Approach
In this work, we have proposed a methodology for the automation of the entire proposed
software development process by providing an eﬃcient solution allowing security poli‐
cies integration during software development process, and take them into account in the
code generation by generating security infrastructures through a speciﬁc code generator.
To do so, we have proposed a model-driven code generator allowing code generation
from models. In this case, our proposed code generator takes in input an intermediate
model, which is proﬁled with the further improvements, then transform it towards a ﬁnal
code correspond to chosen platform. Moreover, we have proposed communication
diagram meta-model (CDMM) that will be employed as a validator of communication
diagram, and will be used as a semantic or modeling language to deﬁne CD structure,
because it is used as a PIM obtained from CIM models. Otherwise, we have chosen CD
as PIM, because it is the most relevant and appropriate diagram to draw complete and
secure interaction between objects during execution of the operation. In addition, CD
determines correctly the objects participant within the interaction and showing spatially
the objects participant in the interactions.
Practically, by performing a model to model transformation, an intermediate
model has been generated from chosen platform PSM and PIM. Generated IM will be
used to improve java models with security policies which are already negotiated by
security expert and design engineer at the beginning of the analysis and design phase
CIM. To do so, IM will be extended to cover up the further improvements by using UML
proﬁle technology, and OCL [24] which is used to enrich systems models with the
constraints which couldn’t be formulated by UML models. In our work, we have used
OCL to improve system models with security invariants such as data integrity, data
304
L. Abdellatif et al.

encryption, and authorization policies based on RBAC. In addition, we have used OCL
to extend precondition and post-condition of LARMAN operations contract (LOC)
which is used to describe system’s statue before and after operation execution. For that,
we have extended the pre and post conditions to support security rules during interaction
between objects, and resolve the shortcomings related to the assignment of the respon‐
sibilities to the objects.
Fig. 2. Diﬀerent transformation performed to generate secure application starting from CD.
MDA Approach for Application Security Integration
305

By the new deﬁnition of LOC, design mistakes and security vulnerabilities will be
resolved by presenting a new semantic of LOC called extended pre-condition and post-
condition matrix. In this EPPM, we have introduced General responsibilities Assign‐
ment Software patterns GRASP PATTERN for automating assign responsibilities to the
object responsible charge to achieve pre-condition and post-condition of an operation,
and not manually. We have also enhanced this pre-condition and post-condition to take
in account security integration during interaction between objects by providing a secure
interaction basing on OCL.Then, operation body, signature, and object responsible will
be deducted automatically through the tool EPPM with the code generator. For example,
to create a new contact, staﬀ manager should have a creation role, and must be authen‐
ticated the user, and must inform valid data, and data encryption as a password or sensi‐
tive data. For checking these constraints, we have applied EPPM tool by deducing the
objects responsible and participant to achieve create contact operation.
Finally, by applying a model to code transformation, the ﬁnal code will be generated
automatically from IM by the way of the proposed code generator that takes in input a
PSM models. Then transform them to code correspond to chosen platform. Fig. 2 below
describes this approach in detail.
5
Proposed Syntax for Enhancing System Models
with Security Constraints
In this section, we propose syntax to enrich structural model of the java model by security
policies into design system phase by applying OCL and UML Proﬁle. According to the
syntax given below, software engineers can enrich their system models with security
constraints such as data encryption, Access control based on RBAC extend with
constraints, assignment of write/read privileges, data validation, and all informal spec‐
iﬁcations which can’t be formulated with UML models. Consequently, the code source
according to chosen platform will be generated automatically from input models by
respecting the quality of services such as Scalability, reusability, reliability, and data
security.
In other word, Structural model has been enriched by the security constraints basing
on security proﬁle that is expressed according to UML, and OCL. As a result, system
objects and resources will be able to interact securely.
In this work, we have proposed a syntax allowing enhancing system operations and
attributes with the security criteria constraints as shown in the following example.
306
L. Abdellatif et al.

6
Tools and Technicals
In this section, we present a comparison study between diﬀerent famous existing code
generator by describing the advantages and disadvantages of each code generator. This
comparison is based on criteria of the generated code details, and the structure of the
code generator, and functionalities provided by each chosen to code generator. Although
there are several code generators, but these code generator still incapable of generating
secure applications and immature to take into account the both aspects, functional and
no-functional.
For example, the ﬁrst tool is startigo/XT, which is used as a creator of code gener‐
ators. Because its structure doesn’t permit it to generate secure applications which
include at the same time functional and no-functional aspects. In addition, startigo/XT
hasn’t a concrete syntax, and doesn’t cover the entire development process of software,
and still incapable of supporting the object-oriented, and complex system.
In the second category, we found the classical code generator which is based
on Petri nets theory. Really, this category was speciﬁcally designed for requirements
validation because they are not yet standardized to support the entire development
process, and not yet adapted for supporting complex system object-oriented
system. In addition, those generators don’t support concrete syntax and secure applica‐
tion generation.
The third code generator is [27]; this category has been proposed to resolve the
shortcomings resulting from startigo/XT and Petri nets solutions. This category provides
a code generator that supports the entire development process of software, and support
object-oriented system. In addition, it takes into account the structural model. But it
doesn’t generate CRUD operations, and GUI interfaces, and controllers, and security
infrastructure ﬁles.
The fourth code generator is WebML, which is designed for the web application like
WebDSL. This generator provides many advantages such as generation of CRUD oper‐
ation, and method bodies, and GUI interfaces. In addition, it uses a concrete syntax. But
at the same, it has a few weaknesses like:
• WebML doesn’t incorporate the structural model.
• WebML doesn’t support the entire development process.
• WebML doesn’t generate applications controller.
• WebML doesn’t generate security infrastructure ﬁles.
Finally, the ﬁfth code generator is Acceleo, which is more similar to WebML. But
the diﬀerence that Acceleo supports the entire development process and WebML doesn’t
do. In addition, Acceleo stays still immature to generate secure applications supporting
code generation of the security infrastructure ﬁles.
At the beginning, we have used Acceleo as a code generator to start the unit testing
of the proposal. But in the same time, we have built in parallel our code generator for
automating the key phases of software development by covering the entire development
process from speciﬁcations phase (CIM) to implementation phase (PSM). In addition,
our proposal provides many advantages:
MDA Approach for Application Security Integration
307

✓ The proposal supports the structural model.
• The proposal covers the entire development process and object-oriented system.
• The proposal allows applications controller generation.
• The proposal generates security infrastructure ﬁles, and method signatures and their
bodies.
• The proposal generates security infrastructure ﬁles and CRUD operations from
Communication Diagram basing on the new approach.
7
Results
So far, we have proposed a new model-driven methodology allowing automating the
entire software development process, which is beginning from specification requirements
phase to implementation phase. To put into practice the new methodology, we have
proposed a new specific tool called Extended Pre-conditions Post-conditions Matrix
(EPPM), which is based on LARMAN operations contract, security profile, and OCL.
In this tool, we have extended the old LARMAN operations contract to a new version
more detailed description than its original version supporting securities policies inte‐
gration during software design phase and more precisely during interaction between
systems objects. To do so, the generated structural model of the java platform has been
enriched by applying the security proﬁle in order to apply the securities policies rules
on java model. We recall that we have analyzed four essential elements of the security
policies which are: authorization, authentication, data encryption, and data integrity.
In other words, access to an instance of Java Class, class Methods, and java Fields will
be controlled by security policies which are already addressed by the security expert and
transformed in the form of the security proﬁle.
In this work, we have also proposed a communication diagram meta-model allowing
describing communication diagram structure, which will be used as a PIM obtained from
CIM. This PIM has been transformed into PSM by performing a set of model-to-model
transformation. Then, the structural model of the java platform has been generated from
PSM.
Finally, by applying a model-to-code transformation to allow translating the gener‐
ated structural model of the java platform into code source according to the JAVA
platform.
8
Conclusion and Future Work
In this work, we introduced the Model Driven Engineering (MDE) and its implemen‐
tation Model Driven Architecture (MDA), and the application security by proposing a
model-driven methodology that combine functional aspect and no-functional aspect at
the same time to get a tailored solution, which cover all aspects basing on the potential
oﬀered by MDA approach, and its utility in relation to the productivity of the models,
sustainability, and platform description integration trough model transformation.
The main objective of this integration is to enrich the structural model of the chosen
platform with the security policies proposed by a security expert to obtain the secure
308
L. Abdellatif et al.

application to get through proposed code generator. Allowing thus to generate the full
and complete methods signatures, and their body and the permissions needed to do the
method, allowing also obtaining security controller that is used to verify the authoriza‐
tion policies. So this approach is very useful in the case of an object-oriented and
complex system.
In addition, the future version of code generator will be enhanced to include addi‐
tional information on code generation of the embedded system, improving existing code
generator to support GUI interfaces generation for diﬀerent uses cases.
References
1. OMG. Object Constraint Language (OCL) Speciﬁcation, version 2.0 (2006). http://www.omg.
org/spec/OCL/2.0/
2. OMG. MDA GUIDE, Version 1.0.1 Object Management Group document number omg/
2003–06-01. http://www.omg.org/docs/omg/03-06-01.pdf
3. OMG: Object Management Group. www.omg.org. http://www.omg.org/docs/omg/03–06-
01.pdf
4. Girault, C., Valk, R.: Petri-nets for Systems Engineering. Springer, Berlin (2003)
5. Huang, S.S., Smaragdakis, Y.: Easy language extension with Meta-AspectJ. In: ICSE 06
Proceeding of the 28th International Conference on Software Engineering, pp. 865–868.
ACM, New York (2006)
6. Zook, D., Huang, S.S., Smaragdakis, Y.: Generating AspectJ programs with meta-AspectJ.
In: Generative Programming and Component Engineering Conference, GPCE 2004,
Vancouver, Canada, vol. 3286, pp. 1–18, October 2004
7. Code generation through model transformation. http://alexandria.tue.nl/extra2/afstversl/wsk-i/
verstraeten2008.pdf
8. Bouseta, B., El Beggar, O., Gadi, T.: Generating operations speciﬁcations from domain class
diagram using transition state diagram. Int. J. Comput. Inf. Technol. 2, 29–36 (2013)
9. El Beggar, O., Bouseta, B., Gadi Taouﬁq, T.: Automatic code generation by model
transformation from sequence diagram of system’s internal behavior. Int. J. Comput. Inf.
Technol. 2, 129–146 (2013)
10. Hemel, Z., Kats, L.C.L., Visser, E.: Code generation by model transformation a case study in
transformation modularity. In: Theory and Practice of Model Transformations. Lecture Notes
in Computer Science, vol. 5063, pp 183–198 (2008)
11. Manoli, A., Cabot, J., Gómez, C., Pelechano, V.: Generating operation speciﬁcations from
UML class diagrams: a model transformation approach. Data Knowl. Eng. 70, 365–389 (2011)
12. Fernardez, E.B., Larondo-Petrie, M.M., Sorgente, T., Vanhilst, M.: A methodology to develop
secure systems using patterns (2008)
13. Ahn, G.-J., Shin, M.E.: UML-based representation of role-based access control. In:
Proceedings of the 9th IEEE International Workshop on Enabling Technologies:
Infrastructure for Collaborative Enterprises (WETICE 2000), pp. 195–200. IEEE Computer
Society, June 2000
14. Basin, D., Doser, J., Lodderstedt, T.: Model driven security for process-oriented systems. In:
Proceedings of the 8th ACM Symposium on Access Control Models and Technologies
(SACMAT 2003), pp. 100–109. ACM Digital Library, June 2003
15. Jürjens, J.: UMLsec: extending UML for secure systems development. In: Proceedings of
the 5th International Conference on the Unifed Modeling Language (UML 2002). LNCS,
vol. 2460, pp. 412–425, October 2002
MDA Approach for Application Security Integration
309

16. Basin, D., Doser, J., Lodderstedt, T.: Model driven security for process-oriented systems. In:
Proceedings of the 8th ACM Symposium on Access Control Models and Technologies
(SACMAT 2003), pp. 100–109. ACM Press, June 2003
17. Jin, X.: Applying model driven architecture approach to model role based access control
system (Doctoral dissertation, University of Ottawa)
18. Basin, D., Doser, J., Lodderstedt, T.: SecureUML: a UML-based modeling language for
model-driven security. In: Proceedings of the 5th International Conference on the Unifed
Modeling Language (UML 2002). LNCS, vol. 2460, pp. 426–441, October 2002
19. Basin, D., Doser, J., Lodderstedt, T.: Model driven security: From UML models to access
control infrastructures. ACM Trans. Softw. Eng. Methodol. (TOSEM) 15, 39–91 (2006)
20. Fernandez-Medina, E., Trujillo, J., Villarroel, R., Piattini, M.: Developing secure data
warehouses with a UML extension. Inf. Syst. 32, 826–856 (2007)
21. Reznik, J., Ritter, T.: Model driven development of security aspects. In: Proceedings of
the Second International Workshop on Aspect-Based and Model-Based Separation of
Concerns in Software Systems (ABMB 2006). Electronic Notes in Theoretical Computer
Science, vol. 163, pp. 65–79, April 2007
22. Trujillo, J., Soler, E., Fernández-Medina, E., Piattini, M.: An engineering process for
developing secure data warehouses. Inf. Softw. Technol. 51, 1033–1051 (2009)
23. Blanco, C., García-Rodríguez de Guzmán, I., Fernández-Medina, E., Trujillo, J., Piattini, M.:
Applying an MDA-based approach to consider security rules in the development of secure
DWs. IEEE Xplore Digit. Libr. 51, 1–25 (2009)
24. Miller, J., Mukerji, J.: MDA Guide Version 1.0.1. Technical report, Object Management
Group (OMG) (2003)
25. Allilaire, F., Bézivin, J., Jouault, F., Kurtev, I.: ATL—eclipse support for model
transformation. In: Proceedings of the Eclipse Technology eXchange Workshop (eTX) at
ECOOP (2006)
26. Object Management Group, Inc. Meta Object Facility (MOF) 2.0 Core Speciﬁcation, Final
Adopted Speciﬁcation, January 2006
27. Philippi, S.: Automatic code generation from high-level Petri-Nets for model driven systems
engineering. J. Syst. Softw. 79, 1444–1455 (2006)
310
L. Abdellatif et al.

Context-Aware for Service Composition
Optimization in Cloud Computing
Ali Bentaleb
(✉) and Ahmed Ettalbi
(✉)
Models and Systems Engineering Team, SIME Laboratory ENSIAS,
Mohammed V University, Rabat, Morocco
mali.bentaleb@gmail.com, ettalbi1000@gmail.com
http://ensias.um5.ac.ma/
Abstract. Web services nowadays are much more available with several imple‐
mentations and purposes in the Cloud. These Web services could do the same
tasks but may diﬀer in quality of service. Furthermore, some of these Web services
are not always relevant to use in some contexts. Providers are likely interested to
oﬀer more complex Web services through combining raw Web services from the
Cloud to improve services and beneﬁts. Therefore, a signiﬁcant research problem
is how to select the best candidate service from a collection of services and to
take into account the respect of service level agreement made by the customer
and the provider. On the other hand, ensuring the relevance of the Web services
for users is very important. In our paper, we propose a composition model archi‐
tecture based on Cloud SaaS that takes not only the QoS of service, but also Cloud
computing and context-awareness aspect of the composition into consideration.
Tests and simulations are made to prove the solution eﬀectiveness.
Keywords: Web service composition · Cloud computing · SaaS · Service level
agreement · Quality of service · Context-awareness
1
Introduction
Traditional business applications have always been very hard to maintain and expensive.
Nowadays, Cloud based technology is the new evolution of the century and is changing
the entire IT ecosystem. It can help avoid managing hardware and software problems,
on the other hand Cloud can respond to customer’s needs with less cost, and oﬀers
customers the chance to immediately implement services with usage-based billing that
are tailored to their requirements. The Cloud exists in several forms: private Cloud when
mutualizes company resources (through virtualization), private virtual Cloud where only
privileged clients have access to these Clouds (like Amazon), and public Cloud where
data will be placed directly at the supplier. There is a classiﬁcation of services that can
be found in the Cloud. The most popular service type is SaaS, and it’s about making
available.
An application that could be easily requested. The advantages are the low cost and
popularity of such applications. Many companies use these capabilities in order to
perform complex tasks, and thus, through combining diﬀerent services from multiple
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_26

vendors so the customers get the best experience. On the following pages WSC will
denote the Web services composition.
2
Related Work
The problem of QoS-based Web service selection and composition has received a lot of
attention during the last years. In [1, 2], authors’ approach treats the Web service
composition problem by dividing it into small sub problems, with usage of load selection
methods. The work in [3] extends [1, 2] and reduce the search space using skyline oper‐
ator. With the increasing number of Cloud service users worldwide, the need to deploy
dispersed data centers geographically to serve the globally distributed Cloud users. Data
centers capacity is limited, so the load should be distributed between datacenters to
provide stable services [4, 5]. In [7] genetic model to network-aware service composition
in the Cloud leads to propose network aware selection algorithm to ﬁnd services that
will result in low latency. Their inconvenient is that they focus only on response time
QoS. In [10], authors treats Web service composition using hybrid genetic algorithm
(initial population choice, adding service dependencies, service conﬂict, and geograph‐
ically distributed data centers in a Cloud environment.
2.1
Web Service Composition
To have a better understanding, the following deﬁnitions are given:
Raw service: a basic independent Web service available in the Cloud.
Service collection: a set of services functionally equivalent but diﬀer in terms
of QoS.
Let Sj a service collection, which classify the universe of available Web services in
the Cloud according to their functionality. Service providers might provide the same
service in diﬀerent quality levels. Nonfunctional properties of raw Web service Sji,
describe the quality aspects of Sji, and n is the number of possible functionally equivalent
Web services but diﬀer in QoS. So the service collection Sj = (Sj1, .., Sjn). These attributes
could be like response time, availability, price, reputation etc., Some QoS attributes need
to be maximized, such as reputation or availability, whereas others need to be minimized,
such as price or response time. We use the vector QSji = (q1 (Sji), .., qn (Sji) to represent
the QoS values of service Sji which is standarized and combine all the QoS that should
be optimized, which are published by the service provider. The function qi (S) determines
the published QoS value of the i-th attribute of the service sji [13].
The QoS values of a composite service are determined by aggregating QoS values
of each Web service QoS in the composition path and by the composition structure used
(e.g. sequential, parallel, conditional and/or loops). Examples are given in Table 1 [3]
(Fig. 1).
312
A. Bentaleb and A. Ettalbi

Fig. 1. Aggregation functions for diﬀerent QoS.
2.2
Cloud Computing
NIST [6] deﬁnes Cloud Computing as a model for allowing ubiquitous, convenient; on-
demand network access to a shared pool of conﬁgurable computing that can be rapidly
provisioned and released with minimal management eﬀort or service provider interac‐
tion. The service models are:
Software as a Service (SaaS): In this case, consumer can use provider applications
running on Cloud. The services are accessible from diﬀerent Client devices interfaces
such as a Web browser or a program interface.
Platform as a Service (PaaS): Consumer can create applications using Platform onto
Cloud infrastructure which contains services, libraries, and diﬀerent tools supported by
the Cloud provider.
Infrastructure as a Service (IaaS): Consumer has the capability to provision processing,
networks, storage, and infrastructure computing resources where the consumer is able
to deploy and run any software, operating systems or applications.
2.3
Context-Aware Aspect of the WSC in the Cloud
Context is the information that characterizes the interactions between humans, applica‐
tions, and the environment [15]. Several context deﬁnitions were proposed in the liter‐
ature serving various domains, however the context deﬁnition given by Dey and Abowd
remains the most referred. In fact, these authors have deﬁned context as any information
that can be used to characterize the situation of an entity. An entity is a person, place or
object that is considered relevant to the interaction between a user and an application,
including the user and applications themselves [5]. In Service Oriented Computing
(SOC), a service is deﬁned as self-describing and supports rapid, low-cost and easy
composition of loosely coupled and distributed software applications [15]. The context-
aware gives a very interesting value on the oﬀered composite Web service for the user.
In [14], the authors deal with the composition of Web services dynamically with the
context-aware. In [19], they propose architecture for construction collaboration in a
context-aware point of view. However none of these researches treat the subject of opti‐
mizing the quality of service given to the user and taking into account his context.
Context-Aware for Service Composition Optimization
313

3
Problem Statement
3.1
Web Services Composition Need
Cloud Computing technology is making the world much easier with service oﬀering
online that could rapidly integrated with enterprise applications. It resolves the
complexity of maintaining servers and softwares since it’s the provider responsibility
to keep services up to date. Other beneﬁts are the low cost and the upgrades are quick
and safe. Cloud Computing oﬀers business users the chance to immediately compose
services with usage-based billing that are tailored to their requirements. Sky computing
model tends to evolve Cloud Computing towards more eﬃcient IT architectures, more
ﬂexible and open. It also aims to manage the complexity and heterogeneity of Cloud
Computing solutions.
Then again, clients demands are becoming more and more complex and a single Web
service usually could not fulﬁll the need, so the composition of services seems to be
very crucial in Cloud Computing. However, ﬁnding an optimal and feasible composition
path is a challenge because the problem of service composition is an NP-complete
problem. According to [11], there has been more than 130% growth in the number of
published Web services in the period from October 2006 to October 2007. So it is useless
to parse all the composition paths especially if the ﬂow of composition is large. On the
other hand, an important issue in Cloud computing is that providers respect the service
level agreements (SLAs) established with clients [13]. Cloud providers generate beneﬁts
between what is charged to the clients and the cost of their investment on the Cloud.
Therefore, they are interested in maximizing proﬁt and respecting SLA to gain reputation
on the market. Also, potential users are interested in maximizing the QoS of the
requested services from the Cloud to focus more on their own business.
To sum up, using Cloud in order to get composite Web services has a lot of beneﬁts
such as high availability, avoiding vendor lock-in and disaster recovery by geo-distrib‐
uted data centers. But there are some drawbacks like aspects including the respect of
SLA, security and architecture.
3.2
Challenges on Cloud Architecture
Some researches already used genetic algorithm [10] in order to compose Web services
in the Cloud, however, there is neither clear explanation on how the architecture would
be, nor the aspect of multiple Cloud environments.
The ﬁgure above describes a user asking for composite Web services. The scenario
is the following:
User request. The consumer asks for a service, for example book a ﬂight. The compo‐
site service for that case would be: search convenient ﬂight + book a ﬂight + book a
hotel + book a taxi from airport to the hotel.
314
A. Bentaleb and A. Ettalbi

Fig. 2. User request process for composite WS in the Cloud.
SaaS: The request will be routed to the Cloud with its 3 components as described in [6],
the SaaS application will be notiﬁed about the request, and the broker will look for
candidate Webs services that the overall composition will in both ways respect func‐
tional need and SLA.
PaaS: Once the Web services are selected to be composed and that’s by using one of
the Meta heuristics, the composition engine will compose and connects the selected Web
services one to another to fulﬁll the demand.
IaaS: Finally, the IaaS will execute the requested demand on the data centers and then
take all the way up to the SaaS application and send the response to the client.
As we can see, this is the basic architecture in Cloud when client demands for a Web
service composition. Using Meta Heuristics in the Cloud side would enhance the quality
of service of the composite Web service, however the client seems vulnerable since there
is no layer protecting him/her from Cloud attacks, and furthermore, he/she is depending
to what the Cloud oﬀers in terms of quality of service optimization. Furthermore, the
user context is neglected, which will give a good experience if it was taking into consid‐
eration.
3.3
Challenges on Optimizing the Composition
Even with use of such meta heuristics, the need to enhance the solution is always existing
as long as we cannot obtain an optimum for selecting the best candidate to form our
composite Web service, and that’s due to the complexity and the number of Web services
existing today on the Cloud.
3.4
Challenges on Customer Expectations from Cloud Computing
The Cloud nowadays oﬀers a lot of services and advantages, however, it cannot satisfy
completely all the clients with the diﬀerent and multiple needs they present, furthermore,
client expectations are becoming more demanding. So for large companies for which
IT plays a central role or represents a competitive advantage, the need to not rely
Context-Aware for Service Composition Optimization
315

completely on Cloud is important, and so having applications in client side which will
enhance quality of service and interaction with the Cloud is very crucial.
4
Our Proposed Solution
In order to detail the solution, the Skyline deﬁnition is given.
4.1
Skyline Operator
Computing the Skyline is known as the maximum vector problem. We use the term
Skyline because of its graphical representation. More formally, the Skyline is deﬁned
as those points which are not dominated by any other point. A point dominate another
point if it is as good or better in all dimensions and better in at least one dimension [8].
Fig. 3. Skyline operator on Web services.
For simplicity we did represent only two QoS criteria in Fig. 3. The X axis represents
availability, and Y axis represents reputation. As we can see point A dominates all points
in reputation, and point C dominates all points in availability, so they will be selected
by the skyline operator.
Point A dominates point E in both criterias, so point E will be eliminated. Point B
dominates point D and F, so point D and F will be eliminated. Finally the selected points
by the skyline operator are A, B and C. In theory, it is the points that are covering the
graph from above when we talk about positive criteria.
4.2
Context-Aware Aspect
In modern life, context-aware aspect is very important in order to deliver relevant serv‐
ices to customers, it does not only enhance the user experience, but also helps in the
optimization of the QoS of the composite services as it narrows the search space in a
considerable way.
In order to ensure a maximal reliability of the solution, we propose to add a complete
layer under our proposed SaaS layer, which will play the role of context-aware layer
unit while composing the Web services.
316
A. Bentaleb and A. Ettalbi

4.3
Solution Description
Our proposed architecture (Fig. 4) lays on adding a SaaS intermediate to the (Fig. 2)
between Cloud provider and consumer in multiple Cloud environments. The goal is to
route client requests through the SaaS application. This approach guarantees client
security and resolves complexity in multiple Cloud based environments (Fig. 3).
Client side: The client will have a speciﬁc interface, it will intercept client requests
and route it to the Cloud, and sends the response back to the client once it was received
from the Cloud.
SaaS application and the Cloud: The Fig. 4 represents our SaaS application and the
Cloud. It consists of two main modules:
Main Module: Contains a client interface, Cloud interface and Skyline component.
this one will ask for Web services signatures and their respective QoS from the diﬀerent
Cloud Vendors, applies the skyline operator and stores the selected Web services
methods and signatures information locally.
The beneﬁt of such an approach is that we can do this work oﬄine, and so it has no
impact on the quality of service of composite Web service when composing online. Our
skyline operator will choose from each service collection, raw services respecting the
skyline, and so it will improve signiﬁcantly the initial population.
Once the client will ask for a service, it will go through this layer and uses local
UDDI information to choose which Webs services collections will play in the compo‐
sition, then passes it to the Cloud.
The Cloud in our case will implement the genetic algorithm in order to compose
Web services and return a feasible service based on the initial population that we did
generate.
Context-aware module: This module is composed of two main components: The
Context-aware engine which will be connected to several devices collecting user data,
making statics, and taking exterior conditions into account.
The Database Server will contain the stored information’s and will stay in touch with
the context-aware engine to collaborate with it.
Fig. 4. SaaS application architecture to interact with the Cloud.
Context-Aware for Service Composition Optimization
317

4.4
Solution Scenario
The scenario in our proposed solution is modeled with an activity diagram in the Fig. 5.
Fig. 5. Activity diagram for the solution scenario
Client request for CWS: The client initiate a request for a service, and it will be a
composite service in order to fulﬁll the request.
Initial request by the SaaS application: The SaaS application acts as a client and
request the diﬀerent Cloud for Webs services signatures and QoS of each Web service.
In SOA architecture, usually UDDI keeps updated information about each Web service
and its correspondent QoS. The beneﬁt of such an approach is the fact that we can make
these requests oﬄine without impacting execution time of the user.
GetWS from the Cloud: Once the request by the SaaS application is ﬁnished, the
signatures of services, and the QoS of each service requests are stored locally on SaaS.
In our case, we suppose that all the signatures of composite Web services that a user
would ask for are present in the SaaS application.
Apply Skyline: The Skyline operator is applied on those services to form a new group
of service which will potentially give an overall better performance.
Context-aware engine: the engine will be based on user previous choices and on
other devices feedback, to propose a convenient services for use, this will narrow the
search space more and thus will aid in ﬁnding a good solution.
Send selected CWS to the Cloud: Once the user makes his preferences, a reference
of the selected Web services will be sent to the Cloud in order to be composed, the Cloud
ensure the execution of these Web services and feed the client with the results.
318
A. Bentaleb and A. Ettalbi

4.5
Solution Beneﬁts
Optimization enhancement: In our approach, we suppose that the Cloud implements the
genetic algorithm in order to ﬁnd the good Web services verifying the SLA. In our case
the skyline operator increases the convergence of the genetic algorithm as it ameliorates
the initial population which impacts the decreasing of iterations numbers to
converge [12].
Beneﬁts on Cloud architecture: We enhanced the Cloud architecture by introducing
a SaaS application which not only enhance the QoS of the composite service but also
take into consideration the user context to suggest relevant services.
Environment integration: Cloud centers are no longer a problem, since we will dele‐
gate to the SaaS application the communication with the diﬀerent Clouds. And the user
will have only to deal with one application.
User beneﬁts: Reduces time execution and respects more the SLA including cost.
Centralized access: Access to the Cloud is centralized through the SaaS application,
so all the requests are going back and forth on this layer.
Security: Security for clients will be enhanced, since the SaaS application will
centralize and process all the requests and the responses exchanged with the client and
the Cloud.
5
Simulation and Evaluation
In this part, we present our simulations to validate the proposed solution. Please refer
to [13] for genetic algorithm variables. For simplicity, the user preferences (cost, avail‐
ability and execution time) gain equal interest, population size is 100, the termination
criterion is on 100 iterations, and one point of cross over and 10% of mutation rate. Two
kinds of tests are performed in this section: The ﬁrst set of tests is with ten Web services
which have ten diﬀerent implementations and vary on QoS values. The values are
generated randomly. The second set of tests is done with ten Web services for the
composition, but this time with a hundred implementations for each single Web service.
The Fig. 7 presents ten Web services to be composed with ten diﬀerent possibilities to
choose from for each Web service with a roulette wheel selection for initial population.
As we can see the solution takes time to get near the optimum. While in Fig. 6 which
Fig. 6. 10 abstract services with 10 implementa
tions for each one using skyline operator
Fig. 7. 10 abstract services with 10 implementa
tions for each one using roulette Wheel selection
Context-Aware for Service Composition Optimization
319

implements our Skyline operator and takes into consideration the context, the curve
takes no time to get near the optimum and stay close to it (Fig. 7).
The Fig. 8 presents ten Web services to be composed with a hundred diﬀerent possi‐
bilities to choose from for each Web service with a roulette wheel selection for initial
population. The skyline operator computing 9 is faster than the roulette wheel and stay
near the optimum. It takes only 671,85 ms to compute (machine with Windows 7, 4 Gb
RAM and Intel i5 processor). The consumed time to compute is reasonable even when
the number of Web services gets higher which proves scalability of the solution (Fig. 9).
Fig. 8. 10 abstract services with 100 implementa
tions for each one using roulette wheel selection
Fig. 9. 10 abstract services with 100 implementa
tions for each one using skyline operator
6
Conclusion and Future Work
In this paper, we present a solution for composing Web services in Cloud environment,
with the focus to get good quality of service for the client through deploying a SaaS
application in client side. The layer relies on Skyline operator which is easy to implement
and brings a good enhancement to the initial population. We strengthen our approach
by adding a context approach to stay close to the user and proposer relevant services,
and simultaneously minimizing SLA violations. Furthermore, the public SaaS is the
service model of the most popular Cloud that consists of making available ready appli‐
cations to the world. At this level, For Future work, we intend to generalize our models
with metamodels and deﬁne all the transformations needed from the architecture point
of view to the code generation.
References
1. Mohammad, A., Thomas, R., Peter, D., Wolfgang, N.: A scalable approach for qos-based Web
service selection. In: Proceedings of the 2008 Service-Oriented Computing Workshops, pp.
190–199. Springer (2009)
2. Mohammad, A., Thomas, R.: Combining global optimization with local selection for eﬃcient
qos-aware service composition. In: Proceedings of the 18th International Conference on
World Wide Web, pp. 881–890. ACM (2009)
3. Mohammad, A., Dimitrios, S., Thomas, R.: Selecting services for qos-based Web service
composition. In: Proceedings of the 19th International Conference on World Wide Web, pp.
11–20. ACM (2010)
320
A. Bentaleb and A. Ettalbi

4. Agarwal, S., Dunagan, J., Jain, N., Saroiu, S., Wolman, A., Bhogan, H.: Volley: automated
data placement for geo-distributed Cloud services. In: Proceedings of the 7th USENIX
Conference on Networked Systems Design and Implementation, pp. 17–32 (2010)
5. Seokho, S., Gihun, J., Chan, J.S.: An sla-based Cloud computing that facilitates resource
allocation in the distributed data centers of a Cloud provider. J. Supercomput. 64(2), 606–637
(2013)
6. R.W. Group: Nist Cloud computing standards roadmap, National Institute of Stan- dards and
Technology, Manuel dutilisateur (2011)
7. Adrian, K., Fuyuki, I., Shinichi, H.: Towards network-aware service composition in the Cloud.
In: Proceedings of the 21st International Conference on World Wide Web, pp. 959–968. ACM
(2012)
8. Borzsonyi, S., Kossmann, D., Stocker, K.: The skyline operator. In: Proceedings. 17th
International Conference on IEEE (2001)
9. Tang, M., Ai, L.: A hybrid genetic algorithm for the optimal constrained web service selection
problem in web service composition (2010)
10. Wang, D., et al.: A genetic-based approach to web service composition in geo-distributed
Cloud environment. Comput. Electr. Eng. 43, 129–141 (2014)
11. Ai, L., Tang, M.: A penalty-based genetic algorithm for QoS-aware web service composition
with inter-service dependencies and conﬂicts (2008)
12. Al-Masri, E., Mahmoud, Q.H.: Investigating web services on the world wide web. In: WWW,
pp. 795–804 (2008)
13. Poles, S., Yan, F., Rigoni, E.: The eﬀect of initial population sampling on the convergence of
multi-objective genetic algorithms. Multiobject. Program. Goal Program. 618, 123–133
(2009)
14. Brezillon, P.: Focusing on context in human centered computing. IEEE Intell. Syst. 18 (3),
62–66. doi:10.1109/MIS.2003.1200731
15. Dey, A.K., Abowd, G.D.: Towards a better understanding of context and context-awareness.
In: Technical Report GIT-GVU-99–22, GVU Center, Georgia Institute of Technology
16. Bentaleb, A., Ettalbi, A.: Toward Cloud SaaS for web service composition optimization based
on genetic algortihm. In: IEEE International Conference on Cloud Computing Technologies
and Applications (CloudTech2016), May 24–26, Marrakech, Morocco (2016)
17. Baidouri, H., Haﬁddi, H., Nassar, M., Kriouile, A.: Enabling context-awareness for dynamic
service composition, IJAPUC (2015)
18. Fathi, M.S., Abedi, M., Rambat, S., Rawai, S., Zakiyudin, M.Z.: Context-aware Cloud
computing for construction collaboration. JCC (2012)
19. Papazoglou, MP.: Service oriented computing: concepts, characteristics and directions. In:
WISE03, The 4th International Conference on Web Information Systems Engineering. pp. 3–
12. IEEE Computer Society. doi:10.1109/WISE.2003.1254461
Context-Aware for Service Composition Optimization
321

A New Architecture of Search Information System
in the Bulletin Oﬃcial of Kingdom of Morocco
Anass Smaili
(✉), Mohamed Sbihi, and Abdelali Lasfar
LASTIMI (Laboratory of Systems Analysis, Information Processing and Integrated
Management), Superior School of Technology, SALE, University Mohamed V, Rabat, Morocco
anass.smaili@yahoo.fr
Abstract. Computer’s system has continued to take a unique and indispensable
role in the development of administrative work. It is incontestable that the use of
scanned documents is much simpler in terms of research; moreover, it also has a
convenience in terms of location, we can see the administrative databases from
diﬀerent places around the world.
Automation, availability and streamlining are paramount expectations in each
database. These main criteria lead us to determine the degree and the power of a
search Information system. This paper proposes a general architecture to build a
strong Information searching system in order to obtain a pertinent result.
Keywords: Search information system (SIS) · Architecture · Indexing · Word-
image matching
1
Introduction
The existence of a search information system is extremely important. By contrast, the
deﬁnition of what this feature can oﬀer, determining its nature is constantly questioning.
Needs are often considered identical but resulting a diﬀerent services. Thus the
crucial question arises of the use of the classical architecture of search information
system, the right solution for small-size databases, however, is not practical for the large
databases. Indeed, other mechanisms exist in response to large databases needs.
Although the data base of the search information system contains a large amount of
information, user may encounter diﬃculties to get results. Thus, the objective of this
article is to point out the role of the search information system, by oﬀering a new archi‐
tecture of search information system.
Our proposal to build a search information system revolves around two main
modules. This is a ﬁrst step to capture and organize data, and then use these in the
indexing module to create/enhance the index catalog.
We describe in the following each of these two modules giving its various compo‐
nents and their roles in our SIS. A detailed architecture of the SIS will be a summary
for this article including all the modules.
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_27

2
Motivation
The General Secretariat of the Government of Morocco (G.S.G), governmental agency
which is legislation is his main responsibility, has a database which contains the
summary of the Oﬃcial Bulletin (O.B) (Fig. 1) since 1913. GSG has launched a free
online service that allows citizens to search legal information contained in the OB. This
search tool is able to provide the location of the results from a set of documents that
exceeds ten thousand documents. Therefore, this service constitutes the ﬁrst step in the
computerization process deﬁned by the GSG. The slowness of the search engine
prompted me to make an investigation, which subsequently led to a one conclusion: the
data are not prepared upstream, which explains the lack of pertinent information.
In this case, the goal is to present a new architecture of SIS that can improve the
actual SIS of GSG.
3
Data Capture Module
3.1
Input Operation
Data recovery in the various issues of the Oﬃcial Bulletin began with a data entry of
this summary. An IT program was developed in this direction in order to detect the
components of a summary of BO.
Fig. 1. Summary of BO in text format.
The developed program proceeds to detect the titles of each legal text and breaks it
to extract the following information:
• Text number
• Title
A New Architecture of Search Information System
323

• Date of text
• BO number
• Date of BO
• Link to the BO
• Type of text
The result of this operation is a table that will be integrated into our database of our
SIS (Fig. 2).
Fig. 2. Result of the extraction operation of the elements.
3.2
Scanning Files
The mass of documents consists of over than 13,000 documents. A digitization operation
has been established on these documents to create a digital version in PDF form by
providing a fairly good picture quality with an optimum size. The result of this operation
is a digital documentary space.
4
Indexation Module
The indexing operation is a speciﬁc treatment that aims to identify the relevant elements
to be used by the search module. The direct use of the document database without
indexing during a search is a very heavy operation that takes signiﬁcant time according
to the weight of the documents component.
We used two types of indexing [2, 3], indexing full text, and the indexing word-
picture supplied by a set of terms to characterize the content of a document. In the ﬁrst
approach that applies after creating a classical index, a relevance formula is applied
afterwards to rank the most relevant documents ﬁrst. A cleaning of the document is made
at the time of creation of the index to delete all stop words. This content can be an article,
a preposition, conjunction, etc.
The index word-picture process enriches our database by a set of key words repre‐
senting the information content of each document; these keywords are derived from
324
A. Smaili et al.

summaries already entered in the database. The index word-picture process consists of
two stages: an extraction step, it supports recovery of a word or number, contained in a
summary of the text, and then it proceeds to its transformation to an Image. This image
is used to ﬁnd it in all images ﬁles witch compose our digitized documents. A second
stage is concerned with recording the result in the database; it is to assign each line
summary of the other terms that represent him and/or the document that contains the
page.
Once this operation is completed, the indexing module recalculates its indexes by
integrating the column that contains terms added. This updated based indexes for
possible use in the next search sessions.
4.1
Classic Indexation
There are several types of traditional indexing, i.e. hash indexing, tree-B indexing,
cluster Indexing, not cluster, etc. The classic indexing module or text indexing is
intended to create an index (or indexes) on the table columns constituting our database.
An indexing catalog is created containing all the indexes. The ﬁgure below shows the
creation of the classic indexes (Fig. 3).
Fig. 3. Chain of catalog creation of indexes.
4.2
Full Text Indexation
When a table is very large, a sequential scan is a moderately slow process for executing
queries. Creating a Full Text Index can signiﬁcantly improve response time using an
indexing and search engine dedicated functions.
Use applies to the various attributes that contain text. Once we pass the stage of
traditional indexing [4, 5], we select the columns that contain text, for example the
column that contains the summary of the legal text, and indexes. The result is an index
catalog that allows using new search functions dedicated to this type of index.
4.3
Word-Image Indexation
Indexing word-picture [1] that we oﬀer for this type of indexing is based on a comparison
technique of part of an image called “word-Image” with the image. We took the summary
of the digitized documents and we have segmented into words (see diagram below).
Treatment was done on the word list to remove stop words, the types of text and redun‐
dancy. For every word of this list, we carried out the conversion operation of a word to
a word-image. This will be the entry point into the matching process in the pages of
digitized BO.
A New Architecture of Search Information System
325

We also extract numbers-image, by conversing numbers already stored in the data‐
base (Fig. 4).
Fig. 4. List of words used for indexing word-image.
5
Matching Module
The literature shows that the image processing in known character recognition opera‐
tions are not appropriate in the Arabic language which requires propose alternative
approaches that take into account the particular characteristics of Arabic script [1]
exceeds this limitation. We proposed above an indexing method based on the matching
word-image. Indeed, this method has enriched our database with information that is
originally as an image and then oﬀers the user the most relevant results.
The principle of this approach is to take the keywords that are stored in our database
and are retrieved from our database, and convert image (Img1). The images (Img) that
constitute our BO are separated. Img1 will scan to ﬁnd her mirror in them. Find Img2
the mirror of Img1 in Img is the key to the success our indexing process word-image.
5.1
Matching Process
The matching process (see Figure below) compares two images (Img1, Img2) of iden‐
tical size and calculates the distance between them. The goal is to end up saying if Img1
and Img2 are identical or not (Fig. 5).
326
A. Smaili et al.

Fig. 5. Matching process.
Img2 is an image from Img. Img is a page or part of the oﬃcial bulletin page.
Our process takes as input Img and Img1 and performs binarization of the two
images.
A segmentation operation is established on the Img to separate each line (the result
of this operation is a matrix containing for each line item start and end of each line). For
each line, we eliminate the white pixels that are not useful around the 4 sides of each
line and we end up detecting the base line of the line.
We proceed by following the removal of white pixels, which are not helpful,
surrounding the 4 sides of Img1 and eventually detect the center of Img1 (base line).
Img1 runs each line of Img and tries to ﬁnd his mirror Img2 by applying a calculation
distance. If the result of the comparison is valid, the information related to that word-
image will be added to the database (Fig. 6).
A New Architecture of Search Information System
327

(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Fig. 6. Result of the diﬀerent stages of the matching process (a) Image from the BO, (b) image
search, (c) and (d) the result of the binarization operation, (e) the result of the detection of lines
of the image, (f) Result the white of the delete operation that surrounds the image, (g) detecting
a result row (h) (i) result of the detection of the center of the line and the word
328
A. Smaili et al.

6
Conclusion: General Architecture
The composition of the two main modules described above enabled us to show the
general architecture of our system [9–11]. The ﬁgure below shows the general archi‐
tecture of our system for feeding the database and the processing of user queries.
Before launching the application, the user selects the mode of research; it is the
classic mode or advanced. The classic questions the tables directly, against the advanced
mode, the system waits for the user to choose the nature of the text and the query will
be updated by including parameters in the search (Fig. 7).
Fig. 7. General architecture of our SRI
According to the request parameters, the system selected the correct view and
conducts research. The search result has ﬁnally returned to the user.
Our database is powered by four ways:
• The entry summary by entry clerks. These summaries will experience an automated
process that will be a table belonging to our database.
• The digitization of BO and storing them in our ﬁle system.
• The search for the word-images to enrich our database.
• The terms of user queries.
• The introduction by thematic entry clerks.
A New Architecture of Search Information System
329

Our research process uses speciﬁc functions of indexing text. These features allow
us to use more parameters to narrow the search and make the most relevant result to the
user.
References
1. Smaili, A., Lasfar, A., Sbihi, M.: A word image matching: case of the oﬃcial bulletin of
Kingdom of Morocco. Int. J. Comput. Linguist. Res. 3, 143–149 (2012)
2. Cai, J., Guo, Y., Wang, L.: Design and implementation of school search engine based on
Lucene.Net. Comput. Technol. Dev. 16, 73–75 (2006)
3. Fang, Y., Friedman, M., Nair, G., Rys, M., Schmid, A.E.: Spatial indexing in microsoft SQL
server 2008. In: SIGMOD 2008, pp. 1207–1216 (2008)
4. Yaofang, G.: On the full-text retrieval system. New Technol. Libr Inf. Serv. (1992)
5. Hamilton, J.R., Nayak, T.K.: Microsoft SQL server full-text search. IEEE Data Eng. Bull.
24(4), 7–10 (2001)
6. Hogan, A., Harth, A., Umbrich, J., Decker, S.: Towards a scalable search and query engine
for the web. In: WWW, pp. 1301–1302 (2007)
7. Su, Q., Widom, J.: Indexing relational database content oﬄine for eﬃcient keyword-based
search. In: IDEAS (2005)
8. Sauermann, L., Bernardi, A., Dengel, A.: Overview and outlook on the semantic desktop. In:
Proceedings of Semantic Desktop Workshop at the ISWC, Galway, Ireland, 6 November
9. Wang, S., Zhang, K.-L.: Searching databases with keywords. J. Comput. Sci. Technol. 20(1),
55–62 (2005)
10. Yin, W.: Study on lucene-based application. J. Taizhou Polytech. Inst. (2009)
11. Zhou, J., Wang, D.: Research and application of full-text retrieval search engine based on
lucene. J. Qiannan Norm. Coll. Natl. (2009)
330
A. Smaili et al.

Automatic Seeded Region Growing Based on Texture
Features for Mass Segmentation in Digital Mammography
Moustapha Mohamed Saleck
(✉)
 and Abdelmajide El Moutaouakkil
Department of Computer Science, Chouaib Doukkali University, 24000 El Jadida, Morocco
saleck.moustapha@gmail.com, elmsn@hotmail.com
Abstract. Breast cancer is one of the most common causes of deaths among the
women in the world. Digital mammography is the most eﬀective technique for
early detection of masses or abnormalities which is related to breast cancer. In
this paper we present an eﬀective approach on mammography images using
texture features and region growing algorithm for breast cancer segmentation in
mammograms that can be implemented in Computer Aided Diagnosis (CADx)
system. The proposed method used automatic seed selection by extracting the
statistical features. The extraction of the textural features (contrast, energy and
homogeneity) of region of interest (ROI) is done by using gray level co-occur‐
rence matrices (GLCM) which is constructed at four diﬀerent directions (0°, 45°,
90°,135°) for each block from region of interest. The results of this method prove
that the Gray Level Co-occurrence Matrices at each direction with a window size
of 8 × 8 give signiﬁcant texture information to distinguish between masses and
non-masses and then automatic seed point selection.
Keywords: Mammograms · Seed point · Region growing · Texture analysis
1
Introduction
Cancer is a group of diseases characterized by the uncontrolled growth and spread of
abnormal cells. If the spread is not controlled, it can result in death. The most common
form of cancer in the United States is breast cancer, with more than 249,000 new cases
expected in 2016 [1]. Mammography is currently the best method for detecting a breast
cancer early, before the malignant tissue is substantial enough to feel or cause symptoms.
However, the interpretation of a mammogram is often diﬃcult and depends on the
expertise and experience of the radiologist. Most of the problems or limitations in
mammography can be overcome by using digital image processing techniques.
Computer-aided-diagnosis (CAD) system can be used to assist the radiologists and the
physicians analyses the overall images, and ﬁnd tumors that a radiologist might not spot.
Combining computer-aided-diagnosis with mammography will improve the ability to
ﬁnd cancer [2]. Image segmentation plays a crucial role in CAD system by facilitating
the delineation of the masses. Though researchers introduced several images segmen‐
tation methods but, seeded region growing (SRG) technique appears as the natural
choice in the masses segmentation [3]. A seed point is the starting point for region
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_28

growing and its selection is very important for the segmentation result [4]. Appropriate
seed position selection in mammogram images can be diﬃcult to achieve and requires
a radiologist experienced with the algorithm and knowledge of the application ﬁeld. To
overcome this limitation, we introduce a new method to select a seed point automatically
based on features extraction approach using gray level co-occurrence matrices (GLCM).
The rest of the paper is organized as follows. In Sect. 2, we bring a literature review
of automatic seed selection in region growing methods. In Sect. 3, we present our method
for automatic seed selection. While Sect. 4, describes the result. Finally, Sect. 5 presents
the conclusion and future work.
2
Literature Review
Region growing is one of the most common methods for segmentation of medical images
due to its performance and simplicity, this method starts with a seed point its selection
(manually or automatically), is very important for the segmentation result, the process
of automatic seed selection is very required when dealing with automatic image segmen‐
tation procedures, especially in CAD system. Many researches were done for automatic
seed selection for region growing method to segment digital mammogram. In [5] B.
Senthilkumar et al., proposed a novel region growing segmentation algorithm for the
detection of breast cancer, using contrast limited adaptive histogram equalization
(CLAHE) for enhancement, then Harris corner detect theory is used to automate the
process of seed selection. In [6] S. Meenalosini et al., used region growing method and
gabor features for cancer cells segmentation in mammogram, the authors used median
ﬁlter, morphological operations and thresholding methods in pre-processing, in ﬁrst step
of segmentation, contrast enhancement was done using histogram utilization, in the
second step the authors computed histogram and accumulated histogram, then the loca‐
tion of peaks in the histogram was found out, from these results the candidate alarm
pixel was calculated, then the alarm pixel was generated from the selected candidates
using a condition speciﬁc, this alarm pixel is used as seed point to perform the region
growing. Finally Gabor ﬁlter was applied on the image. In [7] J. Shan et al., developed
an automatic seed point selection method for breast lesion segmentation on ultrasound
images. The method used the texture features and the spatial characteristics of a lesion.
In [8] K. Yuvaraj et al., presented automatic mammographic mass segmentation based
on region growing technique with automatic seed selection by extracting the statistical
features, the authors initialized a 20 × 20 mask as part of seed selection, they extracted
17 Haralick features using GLCM, Among which ﬁve features (Mean, Dissimilarity,
Sum average, Sum variance, Correlation) are used to distinguish between the mass and
non-mass, If the results of these features matches with predeﬁned mass features, then
the seed point is ﬁxed. Else, the mask is therefore shift. In [9] H. Shili et al., proposed
the spatial fuzzy c-means (SFCM) algorithm to place initial seeds for the growing
process in order to detect the masses in mammograms, they used the output of the SFCM
as initial seeds of region growing algorithm. In [10] A. Malek et al., proposed an initial
seed point selection for Seed-Based Region Growing (SBRG) to segment the Micro‐
calciﬁcations. They used the mathematical morphology (dilation, erosion, opening,
332
M. M. Saleck and A. El Moutaouakkil

closing, regional maxima) for extracting, modifying and manipulating the features
present in the image based on their structuring element in order to obtain an initial seed
point automatically. In [11] K. K. Rajkumar et al., used the median value of the prede‐
ﬁned block 50 × 50 of pixels in the image for automatic seed point selection, then they
extracted the region of interest (ROI) around the seed point using the modiﬁed version
of region growing algorithm for aggregating the pixels around the seed point. Finally
the authors used the gradient operators for identifying the boundaries of the segmented
region, using these boundaries, the segmented region of the images are cropped and
treated as the region of interest of the mammogram that may constitute the tumor regions.
In [12] A. Melouah, presented a comparative study of automatic seed generation for
mass extraction in mammograms based on edges extraction technique and features
extraction technique, the results of this study, proved that seed selection method based
on features extraction technique is better than seed selection method based on edges
extraction technique.
3
Materials and Methods
Breast segmentation is the ﬁrst step in many Computer Aided Diagnosis methods, the
process of segmentation allows to separate the breast from other objects in the mammo‐
gram Images, the results of this process by region growing technique is very sensitive
to the initial seed point selection. In this study we present an automatic seed selection
method based on the statistical features using region growing algorithm. Figure 1 illus‐
trates the block diagram of the proposed method for automatic seed point selection.
Region of Interest (ROI)
Preprocessing
Gray-Level Quantization
Divide ROI into blocks
Extract: 
•
Mean
•
Homogeneity
•
Energy
•
Contrast
Fig. 1. Block diagram of the proposed automatic seed point selection
3.1
Digital Mammogram Database
For the experiments, the miniMIAS database [13] was used. It contains 322 mammo‐
graphic images from left and right breast of 161 patients, some of them healthy and
others containing lesions such as benign or malign tumors and calciﬁcations.
Automatic Seeded Region Growing Based on Texture Features
333

3.2
Pre-processing
The contrast limited adaptive histogram equalization (CLAHE) method is one of the
most eﬀective techniques of image enhancement and was originally developed to reduce
the noise in medical imaging [14] This method is applied to enhance the contrast of
small tiles and to combine the neighboring tiles in an image by using bilinear interpo‐
lation, which eliminates the artiﬁcially induced boundaries.
3.3
Features Extraction and Selection
Haralick [15] suggested the use of gray level co-occurrence matrices (GLCM) for deﬁ‐
nition of textural features. The matrices are used with a distance of d = 1 at four diﬀerent
angles 0°,45°,90° and 135°. The application of GLCM method at a single direction might
not give enough and reliable texture information to analyze the area, for this reason all
directions are used to extract the texture features that gives us more information about
each masses and non-masses tiles area. The features extracted based on gray level co-
occurrence matrices are: Mean, Homogeneity, Energy and Contrast. Mean gives the
average pixel value of a given image. A homogeneous scene will contain only a few
gray levels, It provides co-occurrence matrix with only a few but relatively high values.
Contrast represents the measure of contrast or local intensity variation of pixel. Energy
returns the sum of squared elements in the GLCM. The idea is to calculate the GLCM
to ﬁnd statistic values of measures: mean homogeneity, contrast and energy that enable
to obtain an appropriate area to place initial seed for the growing process.
3.4
Automatic Selection of Seed Point
The Seeded region growing method proposed by Adams and Bischof [16], is one of the
simplest techniques for medical image segmentation, the overall success of this method
is dependent on seed point selection. In goal to ﬁnd an appropriate seed point by using
an automatic process, we extracted four texture features using diﬀerent window sizes.
The processing block or window is important because it will determine the ability of the
texture descriptor to diﬀerentiate between the masses and non-masses, then a good place
for seed selection. In ﬁrst step, the region of interest is divided into windows of size
32 × 32, this size of window is taken as optimal and ideal for our method after many
tests with several other sizes among the biggest sizes that can held within the mass, and
which did not produce satisfactory results (Fig. 2).
Fig. 2. Selection of area for features extraction process.
334
M. M. Saleck and A. El Moutaouakkil

The GLCM is applied to calculate the mean of each tile, a window is considered as
part of suspicious area if its mean is more than 200. This value is taken after extensive
investigation with several tests on masses area. In second step, the output of tiles that
have categorized as suspicious are divided into smaller blocks with a size of 8 × 8, this
size represents the smallest area that can be hold within the masses, a window would be
selected as suspicious area if its mean is more than 210. Finally, the texture descriptors
are extracted to evaluate the smallest tiles which categorized as suspicious (Fig. 3).
Divide ROI into big block processing
Mean > 200
Combine image then divide into small 
block processing
Extract GLCM features
Homogeneity > 0.9
Energy > 0.6
Contrast < 0.2
We choose a seed point inside the 
window
Non-masses
Mean > 210
Yes
Yes 
Non 
Non
Fig. 3. Decision tree for choosing the seed point using GLCM features.
A seed point would be selected inside of a small tile which their texture criteria are
within the range of masses texture values and having maximum values (at least two
texture descriptors) compared to other windows that achieve the conditions required
(Fig. 4).
Automatic Seeded Region Growing Based on Texture Features
335

Mean >200
Mean >210 
Homogeneity > 0.9
Energy > 0.6
Contrast < 0.2
Fig. 4. Criteria of seed point selection
4
Results and Discussion
In this paper, nineteen images are taken from the MiniMIAS database provided by the
Mammographic Image Analysis Society (MIAS) for analyzing the proposed method.
The following images show the result of automatic seed point selection and the manual
markup by an expert (Fig. 5).
(a)
(b)  
(c)   
(e)                   
(f)   
(g) 
Fig. 5. Seed point generation examples on three images Mdb5, Mdb10, and Mdb 15. (a, b, c)
Original image. (e, f, g) Seed generation result using GLCM texture features on images manual
segmented by a radiologist.
The result of texture features with window of size 8 × 8 proved that the best direction
to distinguish between masses and non-masses tissue is θ = 0°, at this direction the
GLCM give a contrast values for the masses area in the range [0–0.2], homogeneity in
[0.9–1], and energy in [0.6–1], Similarly for breast tissue area (non-masses) the contrast
in [0.26–0.71], homogeneity in [0.61–0.8], and energy in [0.14–0.39]. It is observed that
the texture descriptors for breast tissues and masses area are highly discriminated. This
result proves the usefulness of the texture descriptors in distinguishing between the
masses area and non-masses area, thus a good place for seed point in mammogram
(Fig. 6).
336
M. M. Saleck and A. El Moutaouakkil

Fig. 6. Contrast, Energy and homogeneity values using block size 8 × 8
According to Massich et al. [17], the segmentation performance decrease when the
seed position moves away from the mass center area. In this study, the proposed method
has been tested on 19 masses delineated by radiologist, the results of seed place selection
are:
• 1 seed placed on the boundaries of the mass.
• 8 seed placed in the mass center.
• 10 seed placed near the mass center.
5
Conclusion and Future Work
In the proposed work a new computer aided detection technique has been designed to
select the seed point automatically without any human interruption for the growing
process. The proposal makes use four texture descriptors: the intensity, contrast, Energy,
and Homogeneity extracted by gray level co-occurrence matrices to evaluate the prob‐
ability of a pixel being part of the lesion or of the breast tissue, in order to get a good
place for seed selection, and ﬁnally the performance of the new proposal has been
successfully evaluated. In the future work we will improve this method by using a Multi-
scale version of the gray level co-occurrence matrices.
Automatic Seeded Region Growing Based on Texture Features
337

References
1. American Cancer Society: Cancer Facts & Figures 2016, pp. 1–9 (2016)
2. Metz, C.E.: ROC methodology in radiologic imaging. Invest. Radiol. 21, 720–733 (1986)
3. Mehnert, A., Jackway, P.: An improved seeded region growing algorithm. Pattern Recogn.
Lett. 18(10), 1065–1071 (1997)
4. Li, G., Wan, Y.: Adaptive seeded region growing for image segmentation based on edge
detection, texture extraction and cloud model. Springer, Heidelberg (2010)
5. Senthilkumar, B., Umamaheswari, G., Karthik, J.: A novel region growing segmentation
algorithm for the detection of breast cancer. In: 2010 IEEE International Conference on
Computational Intelligence and Computing Research, pp. 1–4 (2010)
6. Meenalosini, S., Janet, M.T.J., Kannan, E., Tech, M.: Segmentation of cancer cells in
mammogram using region growing method and gabor features. Int. J. Eng. Res. Appl. 2(2),
1055–1062 (2012)
7. Massich, J., et al.: Automatic Seed Placement for Breast Lesion, pp. 308–315 (2012)
8. Yuvaraj, K., Ragupathy, U.S.: Automatic Mammographic Mass Segmentation based on
Region Growing Technique, pp. 169–173 (2013)
9. Shili, H., Ben Romdhane, L.: An Eﬃcient Model Based on Spatial Fuzzy Clustering and
Region Growing for the Automated Detection of Masses in Mammograms (2013)
10. Malek, A.A., Rahman, W.E.Z.W.A., Yasiran, S.S., Jumaat, A.K., Jalil, U.M.A.: Seed point
selection for seed-based region growing in segmenting microcalcifications. In: ICSSBE 2012 -
Proceedings, 2012 International Conference on Statical Science and Business Engineering,
Empowering Decision Making with Statical Science, pp. 321–325, September 2012
11. Rajkumar, K., Raju, G.: Automated mammogram segmentation using seed point identiﬁcation
and modiﬁed region growing algorithm. Br. J. Appl. Sci. Technol. 6(4), 378–385 (2015)
12. Melouah, A.: Automatic Seed Generation for Mass Extraction in Mammograms: Comparative
Study 2(2), 9–12 (2015)
13. The mini-MIAS database of mammograms. http://peipa.essex.ac.uk/info/mias.html
14. Wang, X., Wong, B.S., Guan, T.C.: Image enhancement for radiography inspection, vol. 5852,
pp. 462–468 (2005)
15. Haralick, R.M., Shanmugam, K., Dinstein, I.: Textural features for image classiﬁcation. IEEE
Trans. Syst. Man Cybern. 3(6), 610–621 (1973)
16. Adams, R., Bischof, L.: Seeded region growing. IEEE Trans. Pattern Anal. Mach. Intell.
16(6), 641–647 (1994)
17. Massich, J., Meriaudeau, F., Elsa, P., Mart, R., Mart, J.: Seed selection criteria for breast lesion
segmentation in Ultra-Sound images. Computer (Long. Beach. Calif), pp. 57–64 (2011)
338
M. M. Saleck and A. El Moutaouakkil

Robust Video Coding Based on Perceptual
Unequal Protection
Image Level Classiﬁcation and Pixel Level Classiﬁcation
Ouafae Serrar1(✉), Oum el Kheir Abra1, Mohamed Youssﬁ1, and Ahmad Tamtaoui2
1 ENSET-Mohammedia, Hassan II University, Casablanca, Morocco
Serrar.ouafae@gmail.com
2 INPT, Rabat, Morocco
Abstract. To increase the overall visual quality of the video services without
increasing data rate, we developed an unequal protection technique called UEP3D
(Unequal Error Protection 3D) based on a hierarchy of the video stream in
diﬀerent levels of importance. The determination of levels of importance takes
three classiﬁcation criteria: pixel level, macroblock level and image level. At the
end of the classiﬁcation process, an interpolation between the results of the three-
level selection allows the award of an importance index for each macroblock of
the image to be encoded. In this article we present the method used for classiﬁ‐
cation at pixel level and the one used for classiﬁcation at image level.
Keywords: Unequal protection · Video coding · ROI · Visual quality
1
Introduction
Many subjective studies and experiences in the ﬁelds of human vision and electronic
imaging revealed that the human visual system (HVS) tends to focus on a few favorite
areas in images or scenes data. Usually, a loss occurred in the region of the image
drawing the viewer’s attention, causes a bigger discomfort than if it occurs outside of
this area. Moreover, this discomfort can be ampliﬁed by the temporal spread of damage
related to loss in several images. This phenomenon of propagation is accentuated
because of the intensive use of intra and inters images in hybrid video encoders. In
general, areas or regions of the image that attracts the attention of the HVS are called
regions of interest (ROI), while the rest of the picture is called the background.
Many factors inﬂuencing visual attention have been identiﬁed, and are grouped into
two categories. The ﬁrst includes all spatial information that stimulates visual attention
(color, orientation, intensity, size, etc…). The second category concerns the temporal
information (motion).
A video sequence contains those two types of information. Since the ﬁnal quality is
judged by humans, it is useful to deﬁne the levels of importance of the information,
according to their inﬂuence on the visual quality as perceived by the ﬁnal user. The use
of coding based on the regions of interest in combination with radio resource
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_29

management algorithms Radio Resource Management (RRM) can improve the global
system capacity. However, because of the high eﬃciency of the compression, the
resulting data is very sensitive to the eﬀects of transmission errors. To remedy this
problem, the encoding based on the regions of interest was combined with unequal error
protection approaches.
In the scheme we propose, the determination of levels of importance is made
according to three classiﬁcation criteria: pixel level, macroblock level and image level.
An unequal protection based on correcting codes is applied to the hierarchical video
stream. Therefore, the quality of Regions of Interest (ROI) is largely preserved, at the
expense of a loss of quality in other regions.
This paper is organized as follows. Section 2 presents a summary of previous work
involved the application of the concept of regions of interest in coding image/video.
Section 3 describes the proposed robust video encoder based on the perceptual unequal
error protection. In Sect. 4, the classiﬁcation at pixel level is detailed followed by the
classiﬁcation at image level (Sect. 5). Tests and results are presented in Sect. 6. Finally,
conclusions are given in Sect. 7.
2
The Concept of Regions of Interest Applied in Image/Video
Coding
The concept of regions of interest ROI has been treated by several studies in the ﬁelds
of human vision and electronic imaging. These studies revealed that the human visual
system (HVS) tends to concentrate on certain preferred areas when viewing an image
[1]. They also revealed that certain factors may inﬂuence visual attention. These factors
are: the contrasts, the shape of objects, object size, color, position. The proposed scheme
in [2] exploits the hierarchical nature of the coding based on the regions of interest of
the JPEG2000 standard. Two levels of protection against errors are applied. Strong
protection is assigned to packets of the ROI using an extended Golay code (24, 12),
while a low protection is applied to the other packages using an extended Hamming
code (8, 4). Author in [3] proposes perceptual unequal protection methods. These
encoding techniques, employing Flexible Macroblock Ordering (FMO) for H.264/AVC,
are based on the judgment of the spatiotemporal spread of damage. In [4], a novel arbi‐
trary shape ROI coding for scalable wavelet video codec is presented. The motion infor‐
mation of the ROIs is estimated by macroblock padding and polygon matching. In [5],
authors propose a novel rate control scheme for ROI coding. They set the area covering
people are interested in as the ROI to preserve better quality than the background. A
coeﬃcient ω is set to evaluate the signiﬁcance of ROI, and then it’s used to calculate the
mean absolute distortion (MAD) of the ROI and the background. Finally, the quantiza‐
tion parameter (QP) can be decided by the quadratic model.
340
O. Serrar et al.

3
Robust Video Encoder Based on the Perceptual Unequal
Protection
The inﬂuence of packet loss on the image quality of a video sequence depends on several
factors including the spatial position of the loss in the image, the amount of movement
between images, and the image position in the group of pictures (GOP group of picture).
A protection technique does not take into account these factors, can be costly in terms
of bit rate. So, to increase the global visual quality of service video without increasing
bit rate, we developed an unequal error protection technique based on the prioritization
of the video stream in diﬀerent levels of importance. This technique consists of two main
steps (See Fig. 1):
• The extraction of some image characteristics enabling the development of cards.
These cards allow a classiﬁcation of macroblocks according following selection
criteria:
– Picture Level: The position of the image to be encoded in the group of images.
– Macroblock level: The importance of the motion vector of a macroblock in the
image to be encoded.
– Pixel level: belonging or not of a pixel in a spatial region of interest.
• Interpolate between the results of the three-level selection allows assigning an index
of importance IG to each macroblock of the image to be encoded. This important
index determines the type of channel coding to be applied to the corresponding
macroblock.
Fig. 1. Unequal Error Protection based on a hierarchy of the video stream in diﬀerent levels of
importance
Robust Video Coding Based on Perceptual Unequal Protection
341

Image Level classiﬁcation and Pixel Level classiﬁcation are presented in the
following. Macroblock Level classiﬁcation was presented in [6].
4
Image Classiﬁcation Level
In a video encoder, each video sequence is distributed in GOP Group of Pictures. Each
group of pictures (GOP) begins with an Intra picture I followed by a number of predicted
images P. the impact of an error transmission occurred in an image, on the rest of the
images diﬀers depending on the position of this image in GOP. Indeed, the authors in
[7] have shown that the propagation of errors due to predicted images, depend on the
position of those images within the sequence encoded: If the predicted image P is just
after an intra-image I, then its inﬂuence on the rest of the sequence will be greater than
if it is just before or near an image I. therefore, one can obtain a gain on the quality of
the video increasing the protection of the ﬁrst predicted images P.
Since the redundancy introduced by this approach should not exceed one introduced
by a conventional equal protection technique, the latest predicted images P in each
sequence will more or less sacriﬁced in terms of protection.
So we have to determine the best compromise between the number of the protected
images and one of the least protected images, a compromise that minimizes the global
average distortion evaluated by MSE or PSNR.
MSE is calculated from the decoded sequence Sd (transmitted through an imperfect
channel), the original sequence S0, the image size N * M and the length T of the
sequence [7].
The global MSE is:
MSEg =
1
T M N
T−1
∑
t=0
N−1
∑
i=0
M−1
∑
j=0
[So(i, j, t) −Sd (i, j, t)
]2.
(1)
For an encoded 8-bit image PSNR can be calculated by:
PSNR = 10 log (255 2
MSE )
(2)
Furthermore, SC is the decoded sequence (transmitted through a perfect channel). In
this case the distortion is due to the source encoding (quantization) and can be calculated by:
MSEquant =
1
T M N
T−1
∑
t=0
N−1
∑
i=0
M−1
∑
j=0
[So(i, j, t) −Sc(i, j, t)
]2.
(3)
Hence, the global MSE is:
MSEg = MSEquant + MSEch.
(4)
With: MSEch is the distortion due only to channel errors.
342
O. Serrar et al.

MSEquant depends only on the video sequence and the encoder structure so it will be
considered in the sequel as a constant. So, minimizing the global distortion consists in
minimizing MSEch that can be written as the sum of distortion in each image of the T
length video sequence:
MSEch =
T−1
∑
i=0
MSEch(t).
(5)
The effect of an error, with an intensity σ2, located on the tth image can be modeled by:
MSEch(t) = 𝜎2
1
1 + 𝛾t.
(6)
Tree categories of image are considered: R1 class of high protected images, R2 class
of medium protected images and R3 class of least protected images (see Fig. 2). Deter‐
mine t0 (= 0), t1, t2 and t3 (= T): limits of each class, is to minimize the global distortion
of the sequence.
Fig. 2. P images Classiﬁcation according to their positions relative to I image
Let C1, C2 and C3 correcting codes assigned to classes R1, R2 and R3 respectively.
The relationship between the encoding eﬃciency and the parameters t0, t1, t2 and t3
is formulated as follows:
3
∑
r=1
Cr(tr −tr−1) −CmT = 0
(7)
• t1 is the solution of the following equation:
(C2 −C3
)(P1 −P2
) Ln[1 + 𝛾(T −t1
)] −(C1 −C2
)(P2 −P3
) Ln
[
1 + 𝛾C1 −C2
C2 −C3
]
= 0
(8)
Robust Video Coding Based on Perceptual Unequal Protection
343

P1, P2 and P3 are the error probabilities of the classes R1 R2 and R3 respectively.
• t2 is calculated by:
t2 = T −C1 −C2
C2 −C3
(9)
Since the 1/3 turbo codes are used in UMTS systems, we opted to use the same codes
in our assembly, such as:
• if I G = 1, the macroblock will be coded by C1 = 1/3 code;
• if I G = 2, the macroblock will be coded by a C2 = 2/5 code;
• if I G = 3, the macroblock will be coded by a code C3 = 1/2.
So, taking into account these values, we obtain t1 = 5 and t2 = 25. This means that
the 5 ﬁrst images of GOP belong to the class R1, the following twenty belong to the R2
class and images that remain belong to R3.
5
Pixels Level Classiﬁcation
This classiﬁcation is made according to or not belonging of a pixel to a region of
interest. In an encoding scheme based on ROI, the identiﬁcation of these regions is
crucial. Indeed, the results of ROI automatic identiﬁcation algorithms should be consis‐
tent with the results of identiﬁcation done by a human observer. For this, human percep‐
tion and visual attention (AV) should be taken into account when developing algorithms
to identify regions of interest.
In our work we have chosen to apply the identiﬁcation algorithm of interest Regions
based on visual attention model proposed by [8] (see Fig. 3). Its functional principle is
as follows: ﬁrst, primitive visual characteristics (intensity, color and orientation) are
extracted from the image to form maps of characteristics (see Fig. 4(a)). Then, the cards
are standardized by ﬁltering using a diﬀerence of two-dimensional Gaussian ﬁlter. In
each map, the most salient areas are selected and ﬁnally the maps are combined by a
weighted average to obtain the saliency map of the image (See Fig. 4(b)). From this
map, we developed a belonging card of pixels to a ROI (See Fig. 4(c)).
344
O. Serrar et al.

Fig. 3. Identiﬁcation Technique regions of interest based on the visual attention model proposed
by Itti et al. [8]
In part two of our coding scheme of applying unequal protection, we shall have to
make an interpolation between the card delivered by the Macroblocks classiﬁcation
process [6], and that delivered by the pixel classiﬁcation process. Now, these two cards
do not have the same dimensions (9 × 11 for the ﬁrst and for the second 144 × 176). To
remedy this inconvenience, we create another card representing the membership or not
of a macroblock to a region of interest (See Fig. 4(d)). A macroblock is considered
belonging to a region of interest, if at least half of its pixels belong to a region of interest.
Robust Video Coding Based on Perceptual Unequal Protection
345

6
Tests and Results
We used in the simulation a video sequence of 30 images: Akiyo (QCIF size and
176 × 144 pixels). The GOP size is 30: an Image I is inserted periodically every thirty
images. The transmitted signal is subjected to Rayleigh noise. Our proposed system is
compared to a conventional Equal Error Protection technique (EEP) using a correcting
code of 2/5 rate.
The performance of our proposed scheme, in terms of quality, compared to the equal
error protection (EEP) scheme is shown in Fig. 5. It can be seen that our unequal error
protection shows better results than equal error protection. Indeed, in terms of PSNR,
our method allows an improvement that can reach up to 3 dB. In Fig. 6, are presented
the images from the decoded Akiyo sequence. To illustrate the robustness of our tech‐
nique, errors are injected into the ROI of an Image. The impact of the errors is barely
Fig. 4. Identiﬁcation of regions of interest in image 2 of Akiyo sequence (a) maps of the primitive
visual characteristics; (b) saliency map (c) pixel classiﬁcation map; (d) macroblocks classiﬁcation
map
346
O. Serrar et al.

visible on the image coded by our technology, while image encoded by EEP are consid‐
erably degraded by the eﬀect block.
Fig. 5. PSNR comparison between proposal technique and equal error protection of the Akiyo
sequence
Fig. 6. Image extracted from Akiyo sequence (left) encoded by 3D unequal protection (middle),
and equal protection (right)
7
Conclusion
We have presented an image Level classiﬁcation and a Pixel Level classiﬁcation. In
image level classiﬁcation we have determined the best compromise between the number
of the protected images and one of the least protected images in a group of 30 images.
Tree categories of image are considered: R1, R2 and R3. Limits of each class, is to
minimize the global distortion of the sequence. So, after calculations we obtain that R1
contains the ﬁve ﬁrst images, R2 contains images from 6 to 25, and the last ﬁve images
belong to class R3. In Pixel level classiﬁcation, the importance of a pixel is calculated
Robust Video Coding Based on Perceptual Unequal Protection
347

according to its belonging to a region of interest which is determined by primitive visual
characteristics (intensity, color and orientation). We have extended the results for the
pixels on the macroblocks. Thus, a macroblock is classiﬁed as signiﬁcant if at least half
of its pixels are important.
Tests have shown that the proposed technique of video coding achieves better results
in PSNR than an equal error protection.
References
1. Stelmach, L., Tam, W.J., Hearty, P.J.: Static and dynamic spatial resolution in image coding:
an investigation of eye movements. In: Visual Processing and Digital Display II SPIE Human
Vision, San Jose, USA, vol. 1453, pp. 147–152 (1991)
2. Yatawara, Y., Caldera, M., Kusuma, T.M., Zepernick, H.J.: Unequal error protection for ROI
coded images over fading channels. In: Proceedings of the Systems Communication, Montreal,
Canada, pp. 111–115 (2005)
3. Boulos, F.: Transmission d’images ET de vidéos sur réseaux à pertes de paquets: mécanismes
de protection et optimisation de la qualité perçue, Thèse de doctorat, Université de Nantes
(2010)
4. Lana, X., Zhenga, N., Maa, W., Yuan, Y.: Arbitrary ROI-based wavelet video coding.
Neurocomputing 74(12–13), 2114–2122 (2011)
5. Xi, C., Zongze, W., Xie, Z., Youjun, X., Shengli, X.: One novel rate control scheme for region
of interest coding. In: 12th International Conference, ICIC 2016, Lanzhou, China, pp. 139–148
(2016)
6. Serrar, O., Tamtaoui, A.: Robust video coding based on perceptual unequal protection:
macroblocks classiﬁcation based on Signiﬁcant Motion Vectors. In: 5th International
Conference on Multimedia Computing and Systems (ICMCS 2016), Marrakech (2016)
7. Marx, F., Joumana, F.: A novel approach to achieve unequal error protection for video
transmission over 3G wireless networks. Sig. Process. Image Commun. 19, 313–323 (2004)
8. Itti, L., Koch, C., Niebur, E.: A model of saliency-based visual attention for rapid scene analysis.
IEEE Trans. Pattern Anal. Mach. Intell. 20(11), 1254–1259 (1998)
348
O. Serrar et al.

3D Object-Parts Classiﬁcation Based on a Multiclass-SVM
Approach
Omar Herouane
(✉), Hajar Hardi, and Lahcen Moumoun
Laboratory Informatics, Imaging and Modeling of Complex Systems (IIMSC),
Faculty of Science and Technology, University Hassan 1st, Settat, Morocco
Omarherouane@gmail.com, hardi.hajar@gmail.com, lahcenm@gmail.com
Abstract. Support vector machine (SVM) has been recently proposed as a new
technique for classiﬁcation tasks. However, assigning a speciﬁc label to the suit‐
able class is a hard matter, especially when the risk of assigning simultaneously
a label to many homogenous classes is high. It was designed for linearly separable
classiﬁcation and has been developed to ﬁnd the good separation hyper plane
between two or many classes thanks to the identiﬁcation of the most signiﬁcant
training samples of the side of a class. In this paper, we use a nonlinear SVM for
3D object-parts classiﬁcation, where we use the radial basis function (RBF) as a
kernel method to simulate the projection of data into higher dimension space. The
advantage of such classiﬁer is the ease of training and testing. The excellent
recognition rate achieved in the performed experiments proves that the SVM
classiﬁer is one of the most applicable classiﬁers for 3D object-parts classiﬁcation.
Keywords: 3D object · Support vector machine · Machine learning ·
Classiﬁcation
1
Introduction
Over the last decade, the applications of 3D objects classiﬁcation have become increas‐
ingly popular due to the wide development of 3D objects scanning technology. The
purpose of the 3D model classiﬁcation is to categorize all voxels of a 3D mesh into one
of several classes according to their characteristics. Support Vector Machine (SVM) [1]
has been recently proposed as a new technique and widely used for 2D [2, 3] and 3D
image [4, 5] classiﬁcation. This technique proved to be very promising in supervised
classiﬁcation [6]. It was designed for linearly separable classiﬁcation and has been
developed to ﬁnd the good separation hyper plane between two (binary classiﬁcation)
or many classes (multiclass classiﬁcation) thanks to the identiﬁcation of the most signif‐
icant training samples of the side of a class, these samples are named support vector.
For a nonlinearly separable dataset, a kernel method is used to simulate the projection
of the data into a higher dimension space.
The purpose of this work is to apply the SVM classiﬁer to classify parts of 3D objects.
We ﬁrstly extract the feature vector representing parts of a 3D object. The Shape Spec‐
trum Descriptor (SSD) was used [7] since, it can characterize eﬃciently the shape of
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_30

diﬀerent parts of the 3D object. Then, to learn the classiﬁcation model, we use the SMV
classiﬁer. Therefore, the choice of the kernel function is crucial for the success of all
kernel algorithms. For an ideal kernel, input patterns in the same class should have high
kernel value while input patterns in diﬀerent classes should have low kernel value. The
RBF (Radial Basis Function) [8] kernel was chosen since, it can handle the case of the
nonlinearity between the attributes and class labels. Finally, the feature vectors extracted
previously are used as the input of the SVM classiﬁer and the ﬁnal results are achieved.
The paper is organized as follows. The next section provides an overview of related
works and theoretical understanding of the SVM based classiﬁcation. In Sect. 3, the
proposed framework of 3D object-parts classiﬁcation, containing parameters tuning and
classiﬁcation processes, is described. Experiments and results are presented in Sect. 4.
The last section concludes the paper.
2
SVM Based Classiﬁcation
2.1
State of the Art
SVMs algorithms are used to classify divers data particularly components of an image
or a scene, many recent research works were conceived for this goal. Yin et al. developed
a robust SVM classiﬁer to classify images in a scene into diﬀerent semantic categories,
the optimization of parameters of SVM is done using a particle swarm optimization
(PSO) based algorithm. The one-versus-one strategy is used to construct the Multiclass-
SVM to classify the multiple categories scene [9]. Wang et al. proposed a method based
on SVM with RBF kernel function for classifying the inputted remote sensing image
data, this suggested method uses Getis-Ord Gi index to describe the spatial character‐
istics of the image [10]. In [11], Cascio et al. presented an automatic pattern recognition
system for the classiﬁcation of the cell images into a set of predeﬁned classes, in which
the Radial Basis Kernel is utilized. The one-versus-one strategy is used to construct the
SVM classiﬁer. The proposed work [12] by Chen et al. aimed at improving the stability
and accuracy of SVM with RBF kernel function by optimizing the penalty parameter C
and the kernel parameter 𝛾 using the double chains quantum genetic algorithm. The
performed experiments and the obtained results show the eﬃciency of the proposed
SVM classiﬁer for analogue circuit diagnosis. In [13], the authors developed a new
relevant SVM-RBF based approach using the probabilistic feature and weighted kernel
function for image retrieval.
2.2
SVM for Data Processing
Support Vector Machines are powerful methodologies for solving problems in linear or
nonlinear classiﬁcation and regression. Their aim is to ﬁnd a computationally eﬃcient
way of learning good separating hyper planes in a hyperspace. These hyper planes are
built based on the maximization of the distance between samples in diﬀerent categories
and the hyper planes. SVMs are also a set of supervised learning machine used for
training a classiﬁcation model that will assign to the appropriate class the new input
given patterns.
350
O. Herouane et al.

Considering the classiﬁcation of two classes, given a training dataset {xi, yi}N
i=1, with
xi ∈Rd being the input vector and yi ∈{−1, +1} the class labels. SVMs map the d-
dimensional input vector x from the input space to the dh-dimensional feature space
using a nonlinear function 𝜑(i): Rd →Rdh.
The separating hyper plane in the feature space is then deﬁned as:
wT𝜑(x) + b = 0
(1)
with b ∈R the bias and w represents the weight vector with the same dimension as
𝜑(x).
The SVM classiﬁer satisﬁes the following conditions:
{
wT𝜑(x) + b ≥+1, if y = +1
wT𝜑(x) + b ≤−1, if y = −1
(2)
The optimal hyper plane is the one that maximizes the margin between the samples
and the separating hyper plane which is equal to 2∕‖W‖. This problem can be solved
and written in the dual space using the Lagrangian with Lagrange multipliers, thus the
solution for the Lagrange multipliers is obtained by solving a quadratic programming
problem. Finally, the SVM classiﬁer takes the form:
f(x) = sign
(∑#Ns
i=1 𝛼iyiK(x, xi
) + b
)
(3)
where #Ns represents the number of support vectors, 𝛼i Lagrange multipliers and K
(
x, xi
)
the kernel function.
Several forms of kernel functions are employed in SVM classiﬁers, the most
frequently used are the polynomial kernel and the RBF kernel. Intrinsically, the SVM
has been developed for binary classiﬁcation, but, diﬀerent multiclass strategies can be
adopted, for example, one-versus-one (OVO) and one-versus-all (OVA) strategies
[14, 15]. The OVO strategy consists in constructing one SVM for each pair of classes.
To classify a new input data, the maximum voting is performed, where one SVM clas‐
siﬁer votes for each class. Hence, if we consider n classes, then in OVO approach
n (n−1)∕2 classiﬁers are built where each classiﬁer is trained using data from two classes,
and the ﬁnal decision of classiﬁcation is done using the majority vote strategy. However,
the OVA strategy consists in constructing one SVM classiﬁer for each class. For each
classiﬁer, the class is ﬁtted against all the other classes. Therefore, if we consider n
classes, then the OVA approach only n classiﬁers are needed.
3
The Proposed Framework for 3D Object-Parts Classiﬁcation
The proposed framework used for the 3D object-parts classiﬁcation is presented in this
section. First, the multi-class SVM and its parameters tuning are described. Then,
components of the classiﬁcation processes will be shown.
3D Object-Parts Classiﬁcation Based on a Multiclass-SVM Approach
351

3.1
Parameters Tuning
The authors of this paper developed and implemented a single Multiclass-SVM algo‐
rithm based on the OVO strategy. The OVO strategy is used instead of OVA for handling
multiclass categorization for the reason that, OVO is signiﬁcantly less sensitive to the
imbalanced datasets problems and the time we need for training classiﬁers may decrease.
This decrease is due to the fact that the training dataset for each classiﬁer is much smaller.
The system also uses the Radial Basis Function kernel for SVM since, it works well in
practice and relatively easy to calibrate as opposed to other kernels. Radial Basis Func‐
tion can be formulated as Eq. 4:
K(x, z) = exp (−𝛾‖x −z‖2)
(4)
where K(x, z) the function for all 𝛾> 0 that controls the width of the RFB kernel.
Before implementing the classiﬁcation test, the best optimal parameters of C and
Gamma which belong to the RBF-SVM should be determined. The C parameter trades
oﬀ misclassiﬁcation of training patterns against simplicity of the decision surface. A
low C makes the decision surface smooth, while a high C aims at classifying all training
examples correctly by giving the model freedom to select more samples as support
vectors. However, the gamma parameter deﬁnes how far the inﬂuence of a single training
example reaches. The gamma parameters can be seen as the inverse of the radius of
inﬂuence of samples selected by the model as support vectors.
The process of parameter optimization used in this work is 10-fold cross validation.
In 10-fold cross validation, the classiﬁer is trained on 9/10 folds and tested on the
remaining 1/10 fold. This process is repeated ten times to let the classiﬁer go through
each fold for training and testing. In practice, cross-validation-based techniques are
preferred over generalization error bounds. Even though some of these bounds hold
strong theoretical properties closely linked to the SVM foundations, experimental results
on a large number of problems still favor cross-validation strategies.
3.2
3D Object-Parts Classiﬁcation Components
The proposed framework for 3D objects-parts classiﬁcation has two speciﬁc compo‐
nents: the training process in which the SVM classiﬁer is trained by the one-versus-one
strategy and the classiﬁcation step where a multi-class SVM which parameters are opti‐
mized is employed to classify segments of a 3D object. Speciﬁc processes are shown in
Fig. 1.
The ﬁrst problem of the classiﬁcation of a 3D Object is to express the form of its
parts into a representative term as a feature vector. The more detailed a shape of an
object-part is described the more a classiﬁer will be able to distinguish the diﬀerent parts.
In this work, the shape of a 3D object-parts will be represented by the Shape Spectrum
Descriptor (SSD), it has been proposed by Zaharia et al. [7]. It is based on the notion of
shape index [16] that characterizes locally the shape of a surface. Each part of a 3D
model is represented by the distribution of its faces areas with respect to their shape
index values as shown in Fig. 2. The choice of the shape index is done since it allows
the shape of a 3D object surface to be deﬁned. See [17] for more details.
352
O. Herouane et al.

Fig. 2. Top: ‘Human’ 3D object, bottom: parts distributions of shape index
Finally, the SVM classiﬁer is trained by the SSD feature vectors extracted for each
parts of the object and the classiﬁcation accuracy is obtained by calculating the responses
of testing feature vectors in the SVM classiﬁer.
4
Experiments and Results Analysis
In this paper, we propose a framework for 3D object-parts classiﬁcation based on multi‐
class SVM. To access the eﬀectiveness of the presented scheme for this challenge task,
we apply it to “3D mesh segmentation benchmark” database proposed by Chen et al.
[18]. The benchmark contains 380 3D-models of triangular meshes grouped into 19
Fig. 1. Framework of the 3D object-parts classiﬁcation using SVM
3D Object-Parts Classiﬁcation Based on a Multiclass-SVM Approach
353

categories. These 19 categories of the database are divided into homogeneous and heter‐
ogeneous classes. A set of manually segmentations generated by experts is provided for
each 3D-model. The details of the datasets are given in Table 1.
Table 1. Description of the “3D mesh segmentation benchmark” database used for tests
Class name
Segments/object
Human
6
Cup
2
Glasses
5
Airplane
7
Ant
11
Chair
8
Octopus
9
Table
5
Teddy
8
Hand
6
Plier
5
Fish
6
Bird
5
Armadillo
7
Bust
8
Mech
2
Bearing
5
Vase
5
Fourleg
9
For our experiments, the suitable values of the optimal parameters C and Gamma of
the RBF kernel for each classier are: C = 20 and γ = 0.0001.
To train the classiﬁcation model, the size of each training subset was chosen to be
2/3 of the number of samples from the dataset and the rest for testing. The classiﬁcation
performance of the SVM classiﬁer can be determined by computing the accuracy
parameter which is deﬁned as shown in Eq. 5. Performed tests and obtained results are
shown in Table 2.
Accuracy =
TN + TP
TN + TP + FN + FP
(5)
where TN, TP, FN, and FP are respectively True Negative, True Positive, False Negative,
and False Positive testing data computed using the confusion matrix.
All of our experiments are conducted on an Intel CORE i3.4 GHz dual core processor
with 4 GB RAM. Training and testing operations are implemented using MATLAB.
354
O. Herouane et al.

Table 2. Classiﬁcation accuracy (%) and training time (S)
Class name
Training samples Testing samples
Accuracy (%)
Time (s)
Human
76
38
86
2.3
Cup
27
13
90
0.5
Glasses
47
23
87,2
0.75
Airplane
89
44
76,3
1.2
Ant
73
36
82,3
2.75
Chair
69
34
77,5
1.0
Octopus
114
57
81,5
3.5
Table
37
18
79,5
0.63
Teddy
107
53
85,6
1.23
Hand
80
40
78,5
0.61
Plier
60
30
75,5
2.33
Fish
52
26
75,12
1.85
Bird
63
31
71,23
0.70
Armadillo
61
30
74,5
2.88
Bust
53
26
69
1.9
Mech
27
13
91,2
0.3
Bearing
23
11
74,6
1.0
Vase
47
23
69,2
2.1
Fourleg
78
39
77
3.5
Average
79.00
1.63
Table 2 gives the classiﬁcation statistical results using the SVM with Radial Basis
Function under diﬀerent training samples, including classiﬁcation accuracy and the
training time. The table shows that the SVM performs well in terms of the classifying
accuracy and the classiﬁcation accuracy can reach to 79%. However, the training time
of SVM classiﬁer is very little and can be ignored in practice.
The Area under the curve is another parameter that we used to access the eﬀective‐
ness of the SVM classiﬁer. In order to compute this parameter, we have to plot the ROC
curve that measures the accuracy of a classiﬁer. To do that, two speciﬁc parameters are
computed Eqs. (6) and (7):
Sensitivity =
TP
TP + FN
(6)
1 −Speciﬁty =
FP
TN + FP
(7)
After computing the two parameters, the 1-Speciﬁcity is taken along the x-axis
whereas the Sensitivity parameter takes place along the y-axis and at various threshold
settings, the curve is generated by plotting the Sensitivity against the 1-Speciﬁcity.
Therefore, the performance of SVM with Radial Basis Function for diﬀerent homo‐
genous and heterogeneous categories has been evaluated by plotting the ROC curve.
3D Object-Parts Classiﬁcation Based on a Multiclass-SVM Approach
355

Figure 3 shows the mean ROC cure computed for the homogenous and heterogeneous
classes. It is clear that SVM classiﬁer works well with the homogenous categories.
However the results obtained for the heterogeneous categories are minimum. The mean
accuracy from the ROC Curve in term of area under the ROC is 85% for homogenous
categories and 70% for the heterogeneous ones. Thus, the results of the mentioned
classiﬁer are encouraging and show good accuracy within experimental errors.
Fig. 3. Mean Roc curve of the homogenous and heterogeneous classes
In order to compare this work with other methods using the same dataset, a compa‐
rative table (Table 3) was established to highlight the results obtained by one of our
recent works [17]. We compare the proposed method with two others that adopt nearly
the same methodology as SVM: the ﬁrst one is based on Bagged decision-stump and
the second one on Bagged Decision trees. Bagging is a machine learning technique, it
combines weak base classiﬁers for a strong ﬁnal hypothesis. We mean by a weak clas‐
siﬁer (Decision stump and Random Forest), a one with the accuracy only slightly better
than the chance.
Table 3. Comparison of classiﬁcation accuracy and training time
Classiﬁcation method
Recognition rate (%)
Training time (s)
Bagging with decision stump
75
0.6
Bagging with random forests
80
2.5
SVM
79
1.63
As can be clearly seen, the proposed method is comparable with the others in terms
of performance and training time.
The results show that the classiﬁcation accuracy achieved is signiﬁcant with an
accuracy rate up to 79% with SVM. Comparing this rate with the two others methods,
we remark that the percentage of correctly classiﬁed segments of a 3D object using SVM
is higher than Bagged decision-stump and very close to Bagging with Random Forests.
356
O. Herouane et al.

The presented method shows the strength of the SVM classiﬁer, thus the real chal‐
lenge faced here is grouping multiple SVM Classiﬁers to reach more relevant results.
5
Conclusion
In this paper, we propose a novel approach based on SVM with Radial Basis Function
kernel for automatic 3D object-parts classiﬁcation in a database. We evaluate our algo‐
rithm using “3D mesh segmentation benchmark”. The experimental results obtained
demonstrate that this new approach and the utilized shape descriptor are discriminative
and robust in term of accuracy rate. The obtained 79% accuracy rate obviously shows
that the proposed scheme is powerful with an eﬀortless way and not time consuming.
References
1. Vapnik, V.N., Vapnik, V.: Statistical Learning Theory, vol. 1. Wiley, New York (1998)
2. Mercier, G., Lennon, M.: Support vector machines for hyperspectral image classiﬁcation with
spectral-based kernels. In: Proceedings 2003 IEEE International Geoscience and Remote
Sensing Symposium, IGARSS 2003, vol. 1, pp. 288–290. IEEE (2003)
3. Melgani, F., Bruzzone, L.: Classiﬁcation of hyperspectral remote sensing images with support
vector machines. IEEE Trans. Geosci. Remote Sens. 42(8), 1778–1790 (2004)
4. Pontil, M., Verri, A.: Support vector machines for 3D object recognition. IEEE Trans. Pattern
Anal. Mach. Intell. 20(6), 637–646 (1998)
5. Cohen, I., Li, H.: Inference of human postures by classiﬁcation of 3D human body shape. In:
IEEE International Workshop on Analysis and Modeling of Faces and Gestures, AMFG 2003,
pp. 74–81. IEEE, New Jersey (2003)
6. Muñoz-Marí, J., Bruzzone, L., Camps-Valls, G.: A support vector domain description
approach to supervised classiﬁcation of remote sensing images. IEEE Trans. Geosci. Remote
Sens. 45(8), 2683–2692 (2007)
7. Zaharia, T., Prêteux, F.: Shape-based retrieval of 3D mesh models. In: Proceedings of 2002
IEEE International Conference on Multimedia and Expo, ICME 2002, vol. 1, pp. 437–440.
IEEE, New Jersey (2002)
8. Scholkopf, B., Sung, K.K., Burges, C.J., Girosi, F., Niyogi, P., Poggio, T., Vapnik, V.:
Comparing support vector machines with Gaussian kernels to radial basis function classiﬁers.
IEEE Trans. Signal Process. 45(11), 2758–2765 (1997)
9. Yin, H., Jiao, X., Chai, Y., Fang, B.: Scene classiﬁcation based on single-layer SAE and SVM.
Expert Syst. Appl. 42(7), 3368–3380 (2015)
10. Wang, X., Chen, X.: Classiﬁcation of ASTER image using SVM and local spatial statistics
Gi. In: 2012 International Conference on Computer Vision in Remote Sensing (CVRS), pp.
366–370. IEEE, New Jersey (2012)
11. Cascio, D., Taormina, V., Cipolla, M., Bruno, S., Fauci, F., Raso, G.: A multi-process system
for HEp-2 cells classiﬁcation based on SVM. Pattern Recognit. Lett. 82, 56–63 (2016)
12. Chen, P., Yuan, L., He, Y., Luo, S.: An improved SVM classiﬁer based on double chains
quantum genetic algorithm and its application in analogue circuit diagnosis. Neurocomputing
211, 202–211 (2016)
13. Wang, X.Y., Liang, L.L., Li, W.Y., Li, D.M., Yang, H.Y.: A new SVM-based relevance
feedback image retrieval using probabilistic feature and weighted kernel function. J. Vis.
Commun. Image Represent. 38, 256–275 (2016)
3D Object-Parts Classiﬁcation Based on a Multiclass-SVM Approach
357

14. Mayoraz, E., Alpaydin, E.: Support vector machines for multi-class classiﬁcation. In:
Engineering Applications of Bio-Inspired Artiﬁcial Neural Networks, pp. 833–842. Springer,
Berlin (1999)
15. Weston, J., Watkins, C.: Multi-class support vector machines. Technical report CSD-TR-98–
04, Department of Computer Science, Royal Holloway, University of London (1998)
16. Koenderink, J.J., van Doorn, A.J.: Surface shape and curvature scales. Image Vis. Comput.
10(8), 557–564 (1992)
17. Herouane, O., Moumoun, L., Gadi, T.: Using bagging and boosting algorithms for 3D object
labeling. In: 2016 7th International Conference on Information and Communication Systems
(ICICS), pp. 310–315. IEEE, New Jersey (2016)
18. Chen, X., Golovinskiy, A., Funkhouser, T.: A benchmark for 3D mesh segmentation. In:
ACM Transactions on Graphics (TOG) 28(3). ACM, New York (2009). 73
358
O. Herouane et al.

Adapting Alkhalil Morpho Sys as Part
of a Translation System from Arabic to Arabic
Sign Language
Mourad Brour(&) and Abderrahim Benabbou(&)
Laboratory of Intelligent Systems and Applications,
Department of Computer Sciences, Faculty of Sciences and Technology,
Sidi Mohamed Ben Abdellah University, Fez, Morocco
{mourad.brour,abderrahim.benabbou}@usmba.ac.ma
Abstract. Natural language automatic processing is a sequence of morpho-
logical, syntactic and semantic analysis. The ﬁrst step for our future system is to
have a morphological analyzer that aims to assign to each individual Arabic
word the grammatical categories and some other linguistic characteristics. In
general, making a morphological analyzer can be done by following several
steps: special characters removal, segmentation, special-words isolation, stem-
ming and diacritization. There are numerous well-known morphological ana-
lyzers such as Alkhalil Morpho Sys [1, 5], Buckwalter [6] and ElixirMF [7]. So
regarding our use of Alkhalil Morpho Sys [1] in our translation system, we have
evaluated it by considering that our future system can be used by any kind of
person, linguistic or non-linguistic that can give a text containing some frequent
mistakes.
The main objective from evaluating Alkhalil Morpho Sys [1, 5] is certainly
not to criticize it, because it has already proven itself as one of the most robust
analyzers as we will see in this paper, but to increase the number of results in the
output and reduce the number of fails done by the users. The experimental
results appear to be very encouraging and several future works can be
considered.
Keywords: Arabic language  Stemming  Morphology  Morphological
analysis  Morphological analyzer
1
Introduction
Unlike the majority of other languages, Arabic is a Semitic language written from the
right to the left, it is a derivational and ﬂexional language, which is a morphologically
complex due to several issues such as: the same word have different meanings, different
words from the same root don’t have the same meaning, letters change forms, etc. [9].
Morphology is a domain of natural language analysis with the principal objective of
giving a description of word formation or internal structure like afﬁx, root, pattern, and
stem, etc. Morphology is classiﬁed into two categories, ﬁrstly inﬂectional morphology
that applies some grammatical rules to a given word, to have another form of the word
without affecting its category. Secondly, derivational morphology that is interested in
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7_31

the construction of new words from existing ones using appropriate derivation rules
and this may affect the syntactic category of the given word [3].
Morphological analysis is a technique that consists of extracting from an Arabic text
a sequence of linguistic units (unit is an Arabic word) and attach to each unit a gram-
matical category (gender, number, person …) that represents morphological properties.
According to Sawalha [4], Morphological analysis involves a series of processes which
identiﬁes all possible analyses of the word like Alkhalil [1, 5] has previously done; these
processes include tokenization, stemming, lemmatization, diacritization, etc.
Each morphological analyzer has its own methodology and task. Stemmer which is
responsible for extracting the stem/root of words by eliminating the afﬁxes, Lemmatizer
which identiﬁes the canonical form or lemma of a word, Pattern matching algorithms
which generate the pattern and vocalism of the analyzed word. [4]. several morphological
analyzers have been developed for Arabic text such as: Buckwalter’s Morphological
Analyzer (BAMA) [6], ElixirMF [7], Alkhalil Morpho Sys [1, 5], MORPH2 [8], etc.
Stemming is the principal task of morphological analysis; there are three approa-
ches of stemming. Root-based approach, which usually starts by removing well-known
preﬁxes, sufﬁxes and inﬁxes until extracting the root. Stem-based approach that
removes only the most frequent sufﬁxes and preﬁxes to get the stem. Statistical
approach that is based on measures of similarity [2].
Our objective from building a new translation system from Arabic to Arabic Sign
Language is to facilitate a communication between normal people and deaf people. The
system can be used in chat or as a tool to understand some article in an electronic
journal. Following this direction, the text in the input can contain grammatical errors, to
avoid having no results for some wrong words. And since the Morphological Analysis
is the ﬁrst step of the translation system; we thought of analyzing Alkhalil Morpho Sys
[1, 5] using several types of evaluation corpus to identify all possible fails that can be
done in the input Arabic text and suggest the solutions for these cases.
The structure of this paper is as follows. In the second section, we present a brief
overview of morphology and Arabic language characteristics; the third section presents
the Arabic morphological analyzers related works. In the fourth section, we give the
general process of a morphological analyzer. In the ﬁfth section, we describe our
methodology of evaluating Alkhalil Morpho Sys [1, 5] and the ideas to resolve the
identiﬁed problems. Finally, before giving the conclusion and the future works, we are
going to resume the evaluation of some experiments in the sixth section.
2
Arabic Language and Morphology Overview
We start this section by presenting the different categories of a part of speech.
2.1
Part of Speech
Particle (ﺍﻟﺤﺮﻑ
)
The particles include the prepositions and conjunctions; they are used with verbs and
nouns to complete the sentence. For exampleﻋﻠﻰ، ﻓﻲ
,etc.
360
M. Brour and A. Benabbou

Noun (ﺇﺳﻢ
)
Noun is the name that includes the adjective of a person, a place or something like
. It is classiﬁed into two categories [3, 11]:
• Derived noun-ﺍﻹﺳﻢﺍﻟﻤﺸﺘﻖthat is derived from verb, particle or other noun. For
example, the wordﺐﺗﺎﻛis resulting of the Arabic verb.ﺐﺘﻛ
•Primitive or the particular noun-ﺍﻹﺳﻢﺍﻟﺨﺎﺹ
:which represents the non-derived noun,
this category contains the proper noun likeﺃﺣﻤﺪ
,the name of area likeﻓﺮﻧﺴﺎ
,and
interrogative noun like,ﻒﻴﻛetc.
Verb ()ﻞﻌﻓ
The verb in the Arabic language represents an action that is already done or not,
depending on the tense which is subdivided into three classes: the perfect tense, the
imperfect tense, the imperative tense.
Verbs are subdivided into two classes:
– Weak verbs-ﺍﻷﻓﻌﺎﻝﺍﻟﻤﻌﺘﻠﺔ
:the weak verbs contain one of the weak letters (ﺍﻱﻭ
,)
when the ﬁrst letter of an Arabic word is a weak letter, it is called assimilated-ﺍﻟﻤﺜﺎﻝ
likeﻭﻗﻒ-WaKaFa. Moreover, when the weak letter is located in the end of the word,
it is considered as Defective-ﺍﻟﻨﺎﻗﺺlikeﻣﺸﻰ-MaCha. The word likeﺑﺎﻉ-BaAa is
called Hollow-ﺍﻷﺟﻮﻑwhen the weak letter is located in the middle. When the ﬁrst
and the second letters of a word are weak, we called it LaﬁfMaKRoun-ﺍﻟﻠﻔﻴﻒﺍﻟﻤﻘﺮﻭﻥ
,
And ﬁnally LaFiFAlMaFRouk-ﺍﻟﻠﻔﻴﻒﺍﻟﻤﻔﺮﻭﻕthat contains the separated weak letters.
– Strong verbs-ﺍﻷﻓﻌﺎﻝﺍﻟﺼﺤﻴﺤﺔ
:Strong verbs does not contain any weak letter. For
example the wordsﻛﺘﺐ، ﺧﺮﺝ، ﺫﻫﺐ
,etc.
In addition, verbs can be grouped according to pattern-ﺍﻟﻮﺯﻥ
:
–Group of trilateral verbs-ﺍﻷﻓﻌﺎﻝﺍﻟﺜﻼﺛﻴﺔﺍﻟﻤﺠﺮﺩﺓ
:all verbs which matches the patternﻓﻌﻞ
are members of this group, for exampleﻓﺘﺢ، ﺿﺮﺏ
–Group of quadriliteral verbs-ﺍﻷﻓﻌﺎﻝﺍﻟﺮﺑﺎﻋﻴﺔﺍﻟﻤﺠﺮﺩﺓ
:all verbs that matches with the
patternﻓﻌﻠﻞare members of this group, for exampleﺩﺣﺮﺝ
.
–Group of derived verbal-ﺍﻟﻔﻌﻞﺍﻟﻤﺰﻳﺪﻓﻴﻪ
:the previous groups represent the original
forms of verbs, this group contains the derived verbal forms resulting from the
original forms likeﻛﺴﺮ-ﻳﻜﺴﺮ; ﺗﺮﺟﻢ-ﻳﺘﺮﺟﻢ
.
The part of speech “verb” have also some other characteristics called grammatical
categories.
2.2
Grammatical Categories
In Arabic language, there are some grammatical categories:
– Voice indicates the way of using the verb by speaker, in normal case it is called
active voice, and in other way round, it is called passive voice.
– Tense represents the time of action: perfect and imperfect.
– Mood indicates whether the speaker is telling someone to do something (imperative
mode), or whether the speaker is talking about possibly unreal conditions
(subjunctive).
Adapting Alkhalil Morpho Sys as Part of a Translation System
361

2.3
Morphological Properties
Each Arabic morphological analyzer must give some morphological properties to each
word:
– The word gender: masculine or feminine.
– The word person: ﬁrst, second or third person.
– The word number: singular, dual or plural.
– The word case:ﻣﺮﻓﻮﻉ, ﻣﻨﺼﻮﺏ, ﻣﺠﺮﻭﺭ, ﻣﺠﺰﻭﻡ
–The type of the word: verb, noun or particle.
3
Related Works
Several Arabic morphological analyzers have been developed, the number of them is
increasing, in this part, we describe the well-know and the most referenced of them.
Tim Buckwalter Morphological analyzer [6]: Buckwalter compiled a single lexicon
of all preﬁxes and a corresponding uniﬁed lexicon for sufﬁxes instead of compiling
numerous lexicons of preﬁxes and sufﬁx morphemes. He included short vowels and
diacritics in the lexicons.
AlKhalil Morpho Sys1 [5] is based on Arabic morphological rules and useful
linguistic resources (root database, patterns …). In the output of the analyzer, we have a
highly informative table mainly containing vocalization of the stem, its grammatical
category, its possible roots associated with corresponding patterns, proclitic and
enclitics.
Sakhr’s Arabic Morphological Analyzer [10] covers modern and classical Arabic
language; it extracts the basic form of the word by removing the afﬁxes and using the
patterns.
Xerox Arabic morphological Analyzer [3] is constructed using Finite State Tech-
nology. It adopts the root and pattern approach. It includes 4930 roots and 400 patterns,
effectively generating 90000 stems. It is based on rule-based approach.
Each of the previous analyzers uses its own approaches and personalized list of
data. The fails that most analyzers suffer from are their inaccessibility for the com-
munity, and the problem of removing some original word characters as preﬁx or sufﬁx
[10]. Contrariwise for Alkhalil Morpho Sys [1, 5] which is open source and the
documentations are available for researchers;
4
Morphological Analyzer General Steps
The classical linguistic is the approach used by the most morphological analyzers. It
consists of analyzing any word’s type (verb, noun, particle, etc.) starting by eliminating
the afﬁxes from the word (stem-based stemming), afterwards, the rest of the word
called stem will be compared with a list of patterns, this list have the same length of the
stem. After ﬁnding the matched pattern, the root can be extracted easily and validated
using a list of roots already predeﬁned (root-based stemming). After applying the two
techniques of stemming, the result must be vowelized. This approach needs a rich data
of afﬁxes, patterns and root, etc. (see Fig. 1).
362
M. Brour and A. Benabbou

4.1
Load Data
All the lists are loaded in the beginning of the analysis; these lists must contain all the
most linguistics rules.
4.2
Delete Diacritics and Punctuations
This step consists of removing the special characters in one hand. These special
characters are the symbols that we can ﬁnd in the Arabic text and does not have a
meaning such as $, %, £ and others. In the other hand, we shift to remove the diacritics;
the list of special characters and diacritics has been already built. For example,
applying this step to this sentence
.
4.3
Segmentation
After clearing the text from diacritics and punctuation, the resulting text is segmented
into words like this:
ﺗﺮﺑﻂﺍﻹﻧﺴﺎﻥﺇﻟﻰﻭﻃﻦﺑﻌﻴﻨﻪﺃﻭﺃﻣﺔﻣﺤﺪﺩﺓﺃﻭﺑﻼﺩﻭﺍﺣﺪﺓ
ﻭﺍﺣﺪﺓ
ﺑﻼﺩ
ﺃﻭ
ﻣﺤﺪﺩﺓ
ﺃﻣﺔ
ﺃﻭ
ﻭﻃﻦ
ﺇﻟﻰ
ﺍﻹﻧﺴﺎﻥ
ﺗﺮﺑﻂ
Fig. 1. General steps of a morphological analysis
Adapting Alkhalil Morpho Sys as Part of a Translation System
363

4.4
Special Words Isolation
The special word could be a proper noun likeﻭﻃﻦ
,exceptional word like
or tool
word like,ﻰﻟﺇetc. this type of word doesn’t need to be stemmed, so before applying the
process of stemming it must be isolated. The special word can be found with afﬁxes
likeﺍﻹﻧﺴﺎﻥ
,for that, this step is present also in the process of stemming.
4.5
Light Stemming
Light or stem based stemming is a technique used to extract the stem from a word; this
step consists on isolating the preﬁx and sufﬁx, for example, the wordﺗﺮﺑﻂgive (sufﬁx,
stem, preﬁx) = (not deﬁned,ﺭﺑﻂ, ﺕ
.)
4.6
Heavy Stemming
The given stem from the last step must be validated, for that, we apply the heavy
stemming or root based technique; it consists of extracting the root from the stem by
applying the matched pattern and verifying the obtained root by using a list of roots that
we have already built. For example, from this stemﺭﺑﻂwe getﺭﺑﻂas root by applying
the patternﻓﻌﻞthat hasﻓﻌﻞas pattern of root.
4.7
Vowelization
Vowelization is the process of making diacritics or vowels above or under letters of
Arabic. Various vowelized words can be obtained in this step from the possible
vowelized patterns associated with the pattern of the stem; the corrected one depends
on the context (syntactic and semantic analysis). For example,ﺗَﺮْﺑِﻂُ ﺗُﺮْﺑَﻂُ ﺗُﺮْﺑِﻂُare the
vowelized forms of the wordﺗﺮﺑﻂ
.We get it using the possible vowelized patternsﺗَﻔْﻌَﻞُ
ﺗَﻔْﻌِﻞُ ﺗُﻔْﻌَﻞُof the unvowelized patternﺗﻔﻌﻞ
.
5
Methodology of Improving Alkhalil Morpho Sys
This section presents the methodology we followed to identify the most possible cases
of the frequent mistakes that can be done in the input Arabic text that contributes to null
results and giving how we can resolve these problems.
The ﬁrst step consists of collecting several Arabic texts from different sources:
newspapers, websites, articles, and conversations, stories, etc. and give it as input of
Alkhalil Morpho Sys [1, 5] for evaluation.
The second step is based on identifying all the words returning a null result in the
output, and classifying these words according to the error done by this word. The result
of this step is presented as the following:
– Wrong discretization of the word: when we give an Arabic word that is not vow-
elized well like “ﻭَﻧُﻈُ ﻢُ ﺍﻟْﻤَﻌْﻠُﻮْﻣَﺎﺕِ
,”the analyzers Alkhalil1 [5] and Alkhalil2 [1] does
not give any results. To resolve this, we propose to eliminate all the diacritics from
the word and analyze it.
364
M. Brour and A. Benabbou

– Some few patterns not exist in the database: the wordﺍﻟﺘﻘﺪﻳﺮﻳﺔmatch with the pattern
ﺗﻔﻌﻴﻠﻴﺔ+ﺍﻝ
,it is not analyzed in Alkhalil1 [5] because the pattern is missing from the
database. This problem is resolved in AlKhalil2; in the other hand, the word “ﺍﻟﺘﻠﻘﺎﺋﻴﺔ
”
not analyzed both in Alkhalil1 [5] and Alkhalil2 [1] because the patternﺗﻔﻌﺎﻟﻴﺔdoes
not exist in the list of patterns. Adding this pattern to the list of patterns resolves the
problem.
– Wrong rule says “a segment may be interpreted as proper noun if the stem belongs
the proper noun PN and the preﬁx and sufﬁx class C1” which aims to analyze the
proper nouns does not match with all the proper nouns that have the preﬁxes
different toﻭandﻑlikeﺍﻝ, ﺏ, ﻝ
.For example the words:
are not
analyzed by Alkhalil1 [5] and Alkhalil2 [1]. In order to resolve this fail we have to
change this rule by adding all possible preﬁxes and sufﬁxes that may be found with
a proper noun.
– Problem of a word ending with the letterﺕor:ﺓsome people can confuse between
ﺕandﺓas some words end with the letterﺓorﺕlikeﻣﺤﺎﺫﺍﺓ
,this word can be found as
ﻣﺤﺎﺫﺍﺕwithﺕat the end. To resolve this, we have suggested changing this letter
when the output of analyzing this word doesn’t return any results, for example for
the wordﻣﺤﺎﺫﺍﺕwe replaceﺕbyﺓand analyze it and conversely.
– Some forgotten special words likeﻛﻮﺥ، ﻋﻮﻟﻤﺔ، ﺭﺃﺳﻤﺎﻟﻴﺔthat are not analyzed by
Alkhalil1 [5] and Alkhalil2 [1]; so we have to add all possible forgotten special
words to the list of special word.
– Problem of TaHMizﺍﻟﺘﻬﻤﻴﺰ
:we can ﬁnd a word with letter “”ﺃor “”ﺇthat must have
normally a letter “”ﺍwithout HaMZa “”ﺀlikeﺔﺳﺎﻜﺘﻧﺇ ،ﺔﻣﺎﺴﺘﺑﺇ ،ﺃﺮﻗﺇand conversely like
the wordsﺽﺭﻻﺍ ,ﺕﺩﺭﺍthat must be written using the letter “”ﺃwith HaMZa “”ﺀas
. For this upgrade, we deﬁne some rules that resolve most of the found
cases. We replace “”ﺃby “”ﺍand analyze it, and inversely.
Alkhalil [1, 5] cannot identify a number like
, 5, 639 as an element of
analyzing, so we have to deﬁne a list of numbers in order to analyze it.
6
Evaluation
Some of the proposed solutions in the previous section have been implemented, and to
evaluate the improvement of Alkhalil Morpho Sys2 [1], we have compared the results
using different evaluation corpus:
– SALMA standard gold evaluation corpus text [4] that contains about 1015 vow-
elized words including wrong vowelization.
– Newspaper and Conversation Evaluation corpus containing 750 non-vowelized
words.
The experiment results in Table 1 have shown that AlKhalil2 improved version
performs better than Alkhalil2 [1] in term of resolving the user errors. Among 1015
vowelized words, 739 have been analyzed and 276 not analyzed using Alkhalil2 [1].
For AlKhalil2 improved version, 940 are correctly analyzed and 75 not analyzed in
case of resolving the problem of diacritization, this means that the results are improved
Adapting Alkhalil Morpho Sys as Part of a Translation System
365

by 10% accuracy(201 words), and after including the solution to the TaHMizﺍﻟﺘﻬﻤﻴﺰ
problem, AlKhalil2 improved version give 960 words correctly analyzed which rep-
resents 94%.
Because AlKhalil2 [1] is an opensource project but compiled, we cannot make changes
in some sources, for that we have decided to apply other solutions to AlKhalil1 [5] which is
also open source but not compiled, these solutions include resolving all the problems
described in the previous section excluding the identiﬁcation of numbers. The results are
presented in the Table 2 using SALMA standard gold evaluation corpus text [4].
The Table 2 shows that improving AlKhalil1 [5] leads to analyze 940 (93%) words
correctly instead of just 704 (69%), in this way we have improved the output by 24%.
The same experiments have been done using Newspaper and Conversation Eval-
uation corpus, Tables 3 and 4 presents the results.
Table 1. Results of ANALYSIS: wrong diacritization and Problem of TaHMizﺍﻟﺘﻬﻤﻴﺰcases
SALMA standard gold evaluation corpus AlKhalil2
AlKhalil2 improved
version
Diacritization TaHMiz
Number of words
1015
1015
1015
Number of analyzed words
739 (72%) 940 (92%)
960 (94%)
Number of not analyzed words
276 (28%) 75 (8%)
55 (6%)
Table 2. Results of ANALYSIS AlKhalil1 using vowelized text
SALMA standard gold evaluation corpus AlKhalil1
AlKhalil1 improved version
Number of words
1015
1015
Number of analyzed words
704 (69%) 940 (93%)
Number of not analyzed words
311 (31%) 75 (7%)
Table 3. Results of ANALYSIS: problem of TaHMizﺍﻟﺘﻬﻤﻴﺰcase
Newspaper and Conversation Evaluation corpus AlKhalil2
AlKhalil2 improved version
Number of words
750
750
Number of analyzed words
721 (96%) 737 (98%)
Number of not analyzed words
29 (4%)
13 (2%)
Table 4. Results of ANALYSIS AlKhalil I using no vowelized text
Newspaper and conversation evaluation corpus AlKhalil1
AlKhalil1 improved version
Number of words
750
750
Number of analyzed words
696 (93%) 726 (97%)
Number of not analyzed words
54 (7%)
24 (3%)
366
M. Brour and A. Benabbou

These two tables approve that by using different evaluation corpus, the results in the
output change and progress; by using 750 non-vowelized words in the input, 696 only
analyzed by alkhalil1 [5], after improvement, we get 726 words analyzed which pre-
sents 97% of the inputted text. These results are very encouraging and there are more
enhancement to be done.
7
Conclusions and Future Work
In this paper, we have presented our methodology to improve Alkhalil Morpho Sys [1]
as a part of translation system from Arabic to Sign Language. From this work, we are
going to complete the implementation of the proposed solutions for the problems
described in this paper. By adopting the output of the analyzer, we can use it as an input
of our future translation system.
The future system to be developed is a text translator from Arabic to Arabic Sign
Language, the system permits to communicate more easily, using a text written in
Arabic, with people suffering from a hard hearing and knowing only the sign language.
The system must perform a morpho-syntactic analysis of the input text and convert it to
video sequence phrases representing an avatar that is expressed by the usual signs used
by deaf people.
References
1. Boudchiche, M., Mazroui, A., Bebah, M.O., Lakhouaja, A.: L’Analyseur Morphosyntaxique
AlKhalil Morpho Sys 2. 1ère Journée Doctorale Nationale sur L’Ingénierie de la Langue
Arabe, (JDILA’14), Rabat, Maroc (2014)
2. Hadni, M., Ouatik, S.A., Lachkar, A.: Effective Arabic stemmer based hybrid approach for
Arabic text categorization. Int. J. Data Min. Knowl. Manag. Process 3(4), 1–14 (2013)
3. Gridach, M., Chenfour, N.: Developing a new approach for arabic morphological analysis
and generation. Mathematics and Computer Science Department, Sidi Mohamed Ben
Abdellah University-Faculty of Sciences, Fez, Morocco, IJCNLP, pp. 52–55 (2011)
4. Sawalha, M., Atwell, E., Abushariah, M.A.: SALMA: Standard Arabic Language
Morphological Analysis. IEEE, New York (2013)
5. Boudlal, A., Lakhouaja, A., Mazroui, A., Meziane, A., Bebah, M.O.A.O., Shoul, M.:
Alkhalil Morpho Sys: a morphosyntactic analysis system for arabic texts. In: ACIT 2010
6. Buckwalter, T.: Buckwalter Arabic morphological analyzer version 2.0 (2004)
7. Smrz, O.: ElixirFM—Implementation of Functional Arabic Morphology (2007)
8. Kammoun, N.C., Belguith, L.H., Hamadou, A.B.: The MORPH2 new version: a robust
morphological analyzer for Arabic texts. In: JADT (2010)
9. Al-Omari, A., Abuata, B.: Arabic light stemmer (ARS). J. Eng. Sci. Technol. 9(6), 702–716
(2014)
10. Abouenour, L., El Hassani, S., Yazidy, T., Bouzouba, K., Hamdani, A.: Building an Arabic
Morphological Analyzer as part of an Open Arabic NLP Platform
11. Khoja, S.: Arabic part-of-speech tagger (APT)
Adapting Alkhalil Morpho Sys as Part of a Translation System
367

Author Index
A
Abdellatif, Lasbahani, 297
Aboutabit, Noureddine, 24
Ait Rahou, Meriem, 180
Alami, Lamiae, 51
Alami, Nabil, 35
Alsahlawi, Ahmed, 60
Asimi, Ahmed, 253
Asimi, Younes, 253
Atounti, Mohamed, 82
Azrour, Mourade, 155
B
Bahiri, Mohamed Nabil, 281
Bais, Hanane, 3
Balouki, Youssef, 127
Barkouk, Hamid, 205
Benabbou, Abderrahim, 359
Bentaleb, Ali, 311
Bounabat, Bouchaib, 108
Bri, Seddik, 219, 267
Brour, Mourad, 359
C
Chakir, El Mostapha, 229
Chassot, Christophe, 281
Cherraqi, El Bouazzaoui, 143
Chhiba, Mostafa, 297
Chougdali, Sallami, 127
D
Daoui, Abdelhadi, 95
E
El Adlouni, Yassine, 35
El Amri, Abdelkebir, 238
el Kheir Abra, Oum, 339
El Ksimi, Ali, 192
El Maammar, Nejwa, 267
El Moutaouakkil, Abdelmajide, 331
Elotmani, Fouad, 82
Elouardighi, Abdeljalil, 10
En-nahnahi, Noureddine, 35
En-Naimi, El Mokhtar, 205
Erramdani, Mohammed, 82
Esbai, Redouane, 82
Ettalbi, Ahmed, 311
F
Faddoul, Khoukhi, 192
Fakhouri Amr, Meryem, 117
Farhaoui, Yousef, 155
Foshi, Jaouad, 267
G
Gadi, Taouﬁq, 127
Ghammaz, Abdelilah, 281
Gherabi, Noreddine, 70, 95
Guezzaz, Azidine, 253
H
Haﬁdi, Imad, 24, 51
Hammia, Hafdalla, 10
Hardi, Hajar, 349
Hasbi, Abderrahim, 180
Herouane, Omar, 349
I
Ibhi, Abderrahmane, 3
Ihedrane, Mohammed Amine, 219
J
Jalil, Abdennour, 24
Jarrar, Abdessamad, 127
K
Khamlichi, Youness Idrissi, 229
Khiri, Fouad, 3
Koutti, Lahcen, 3
© Springer International Publishing AG 2018
G. Noreddine and J. Kacprzyk (eds.), International Conference on Information
Technology and Communication Systems, Advances in Intelligent Systems
and Computing 640, DOI 10.1007/978-3-319-64719-7
369

L
Lasfar, Abdelali, 322
Leghris, Cherkaoui, 171, 192
M
Maach, Abdelilah, 143
Machkour, Mustapha, 3
Maghfour, Mohcine, 10
Majeed, Mahmood, 60
Mansouri, Khalifa, 117
Mansouri, Mouad, 171
Maqboul, Jaouad, 108
Marzouk, Abderrahim, 70, 95
Meknassi, Mohammed, 35
Metrane, Abdelmotalib, 51
Mjihil, Oussama, 297
Moughit, Mohamed, 229
Moumoun, Lahcen, 349
N
Nejjahi, Redouane, 70
O
Ouanan, Mohammed, 155
Ouknine, Lahcen, 3
Q
Qbadou, Mohammed, 117
R
Riyami, Bouchaib, 117
S
Saleck, Moustapha Mohamed, 331
Salman, Abdul Fattah, 60
Sbihi, Mohamed, 322
Serrar, Ouafae, 339
Smaili, Anass, 322
T
Tabyaoui, Abdelmoumen, 297
Tamtaoui, Ahmad, 339
Tarbouch, Mohamed, 238
Terchoune, Hanae, 238
Y
Youssﬁ, Mohamed, 339
Z
Zyane, Abdellah, 281
370
Author Index


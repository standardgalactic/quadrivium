Distributed real-time search and analytics with 
Elasticsearch 5.x
Learning
Elasticsearch
Abhishek Andhavarapu
www.ebook3000.com

Learning Elasticsearch
%JTUSJCVUFESFBMUJNFTFBSDIBOEBOBMZUJDTXJUI&MBTUJDTFBSDI
Y
Abhishek Andhavarapu
BIRMINGHAM - MUMBAI

Learning Elasticsearch
Copyright Â© 2017 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or
transmitted in any form or by any means, without the prior written permission of the
publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the
information presented. However, the information contained in this book is sold without
warranty, either express or implied. Neither the author, nor Packt Publishing, and its
dealers and distributors will be held liable for any damages caused or alleged to be caused
directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the
companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
First published: June 2017
Production reference: 1290617
1VCMJTIFECZ1BDLU1VCMJTIJOH-UE
-JWFSZ1MBDF
-JWFSZ4USFFU
#JSNJOHIBN
#1#6,
ISBN 978-1-78712-845-3
XXXQBDLUQVCDPN
www.ebook3000.com

Credits
Authors
Abhishek Andhavarapu
Copy Editor
Manisha Sinha
Reviewers
Dan Noble
Marcelo Ochoa
Project Coordinator
Manthan Patel
Commissioning Editor
Amey Varangaonkar
Proofreader
Safis Editing
Acquisition Editor
Varsha Shetty
Indexer
Tejal Daruwale Soni
Content Development Editor
Jagruti Babaria
Graphics
Tania Dutta
Technical Editor
Danish Shaikh
Production Coordinator
Deepika Naik

About the Author
Abhishek Andhavarapu is a software engineer at eBay who enjoys working on highly
scalable distributed systems. He has a master's degree in Distributed Computing and has
worked on multiple enterprise Elasticsearch applications, which are currently serving
hundreds of millions of requests per day.
He began his journey with Elasticsearch in 2012 to build an analytics engine to power
dashboards and quickly realized that Elasticsearch is like nothing out there for search and
analytics. He has been a strong advocate since then and wrote this book to share the
practical knowledge he gained along the way.
Writing a book is a humongous task, I want to thank my wife Ashwini for her patience and
support during the nights and weekends I spent writing this book. Iâ€™m thankful to my
parents Govinda Rajulu, Jaya Lakshmi, my brother Sarat Kiran and my in-laws Satya Rao
and Suguna for the constant motivation and encouragement throughout the writing of this
book. I'm grateful to all my friends and colleagues, whom I couldn't mention by name, for
their valuable feedback and inputs.
I also would like to thank my publisher and editors at Packt for the continuous support.
www.ebook3000.com

About the Reviewer
Dan Noble is a software engineer with a passion for writing secure, clean, and articulate
code. He enjoys working with a variety of programming languages and software
frameworks, particularly Python, Elasticsearch, and various Javascript frontend
technologies. Dan currently works on geospatial web applications and data processing
systems.
Dan has been a user and advocate of Elasticsearch since 2011. He has given several talks
about Elasticsearch, is the author of the book Monitoring Elasticsearch, and was a technical
reviewer for the book The Elasticsearch Cookbook, Second Edition, by Alberto Paro. Dan is also
the author of the Python Elasticsearch client rawes.
Marcelo Ochoa works at the system laboratory of Facultad de Ciencias Exactas of the
Universidad Nacional del Centro de la Provincia de Buenos Aires and is the CTO at
Scotas.com, a company that specializes in near real-time search solutions using Apache Solr
and Oracle. He divides his time between university jobs and external projects related to
Oracle and big data technologies. He has worked on several Oracle-related projects, such as
the translation of Oracle manuals and multimedia CBTs. His background is in database,
network, web, and Java technologies. In the XML world, he is known as the developer of
the DB Generator for the Apache Cocoon project. He has worked on the open source
projects DBPrism and DBPrism CMS, the Lucene-Oracle integration using the Oracle JVM
Directory implementation, and the Restlet.org project, where he worked on the Oracle XDB
Restlet Adapter, which is an alternative to writing native REST web services inside a
database resident JVM.
Since 2006, he has been part of an Oracle ACE program and recently incorporated into a
Docker Mentor program.
He has coauthored Oracle Database Programming using Java and Web Services by Digital
Press and Professional XML Databases by Wrox Press and been a technical reviewer for
several PacktPub books, such as Mastering Elastic Stack, Mastering Elasticsearch 5.x - Third
Edition, Elasticsearch 5.x Cookbook - Third Edition, and others.

www.PacktPub.com
For support files and downloads related to your book, please visit XXX1BDLU1VCDPN.
Did you know that Packt offers eBook versions of every book published, with PDF and
ePub files available? You can upgrade to the eBook version at XXX1BDLU1VCDPN and as a
print book customer, you are entitled to a discount on the eBook copy. Get in touch with us
at TFSWJDF!QBDLUQVCDPN for more details.
At XXX1BDLU1VCDPN, you can also read a collection of free technical articles, sign up for a
range of free newsletters and receive exclusive discounts and offers on Packt books and
eBooks.
IUUQTXXXQBDLUQVCDPNNBQU
Get the most in-demand software skills with Mapt. Mapt gives you full access to all Packt
books and video courses, as well as industry-leading tools to help you plan your personal
development and advance your career.
Why subscribe?
Fully searchable across every book published by Packt
Copy and paste, print, and bookmark content
On demand and accessible via a web browser
www.ebook3000.com

Customer Feedback
Thanks for purchasing this Packt book. At Packt, quality is at the heart of our editorial
process. To help us improve, please leave us an honest review on this book's Amazon page
at IUUQTXXXBNB[PODPNEQ.
If you'd like to join our team of regular reviewers, you can e-mail us at
DVTUPNFSSFWJFXT!QBDLUQVCDPN. We award our regular reviewers with free eBooks and
videos in exchange for their valuable feedback. Help us be relentless in improving our
products!

Table of Contents
Preface
1
Chapter 1: Introduction to Elasticsearch
7
Basic concepts of Elasticsearch
8
Document
8
Index
9
Type
10
Cluster and node
11
Shard
12
Interacting with Elasticsearch
12
Creating a document
13
Retrieving an existing document
16
Updating an existing document
16
Updating a partial document
17
Deleting an existing document
17
How does search work?
18
Importance of information retrieval
18
Simple search query
19
Inverted index
20
Stemming
22
Synonyms
22
Phrase search
22
Apache Lucene
23
Scalability and availability
24
Relation between node, index, and shard
24
Three shards with zero replicas
24
Six shards with zero replicas
25
Six shards with one replica
26
Distributed search
27
Failure handling
28
Strengths and limitations of Elasticsearch
31
Summary
33
Chapter 2: Setting Up Elasticsearch and Kibana
34
Installing Elasticsearch
34
Installing Java
35
Windows
35
www.ebook3000.com

[ ii ]
Starting and stopping Elasticsearch
36
Mac OS X
37
Starting and stopping Elasticsearch
38
DEB and RPM packages
39
Debian package
39
RPM package
39
Starting and stopping Elasticsearch
39
Sample configuration files
40
Verifying Elasticsearch is running
41
Installing Kibana
42
Mac OS X
42
Starting and stopping Kibana
42
Windows
43
Starting and stopping Kibana
43
Query format used in this book (Kibana Console)
44
Using cURL or Postman
46
Health of the cluster
47
Summary
48
Chapter 3: Modeling Your Data and Document Relations
49
Mapping
49
Dynamic mapping
50
Create index with mapping
53
Adding a new type/field
54
Getting the existing mapping
55
Mapping conflicts
56
Data type
57
Metafields
58
How to handle null values
58
Storing the original document
59
Searching all the fields in the document
60
Difference between full-text search and exact match
61
Core data types
64
Text
64
Keyword
66
Date
67
Numeric
68
Boolean
69
Binary
69
Complex data types
69
Array
69

[ iii ]
Object
70
Nested
72
Geo data type
73
Geo-point data type
73
Specialized data type
75
IP
75
Mapping the same field with different mappings
76
Handling relations between different document types
77
Parent-child document relation
78
How are parent-child documents stored internally?
80
Nested
80
Routing
81
Summary
83
Chapter 4: Indexing and Updating Your Data
84
Indexing your data
84
Indexing errors
86
Node/shards errors
86
Serialization/mapping errors
87
Thread pool rejection error
88
Managing an index
89
What happens when you index a document?
92
Updating your data
95
Update using an entire document
95
Partial updates
96
Scripted updates
97
Upsert
98
NOOP
99
What happens when you update a document?
100
Merging segments
100
Using Kibana to discover
101
Using Elasticsearch in your application
104
Java
105
Transport client
105
Dependencies
106
Initializing the client
106
Sniffing
107
Node client
108
REST client
108
Third party clients
110
Indexing using Java client
110
Concurrency
112
www.ebook3000.com

[ iv ]
Translog
114
Async versus sync
115
CRUD from translog
115
Primary and Replica shards
116
Primary preference
118
More replicas for query throughput
118
Increasing/decreasing the number of replicas
119
Summary
119
Chapter 5: Organizing Your Data and Bulk Data Ingestion
120
Bulk operations
120
Bulk API
121
Multi Get API
125
Update by query
127
Delete by query
131
Reindex API
132
Change mappings/settings
132
Combining documents from one or more indices
133
Copying only missing documents
133
Copying a subset of documents into a new index
134
Copying top N documents
134
Copying the subset of fields into new index
135
Ingest Node
135
Organizing your data
144
Index alias
144
Index templates
148
Managing time-based indices
152
Shrink API
154
Summary
155
Chapter 6: All About Search
156
Different types of queries
157
Sample data
157
Querying Elasticsearch
159
Basic query (finding the exact value)
161
Pagination
163
Sorting based on existing fields
163
Selecting the fields in the response
164
Querying based on range
168
Handling dates
169

[ v ]
Analyzed versus non-analyzed fields
171
Term versus Match query
174
Match phrase query
177
Prefix and match phrase prefix query
179
Wildcard and Regular expression query
180
Exists and missing queries
181
Using more than one query
181
Routing
185
Debugging search query
186
Relevance
186
Queries versus Filters
189
How to boost relevance based on a single field
191
How to boost score based on queries
194
How to boost relevance using decay functions
198
Rescoring
205
Debugging relevance score
207
Searching for same value across multiple fields
209
Best matching fields
211
Most matching fields
213
Cross-matching fields
215
Caching
217
Node Query cache
218
Shard request cache
218
Summary
219
Chapter 7: More Than a Search Engine (Geofilters, Autocomplete, and
More)
220
Sample data
221
Correcting typos and spelling mistakes
221
Fuzzy query
222
Making suggestions based on the user input
224
Implementing
224
Term suggester
225
Phrase suggester
229
Implementing the autocomplete feature
232
Highlighting
235
Handling document relations using parent-child
237
The has_parent query
239
The has_child query
240
Inner hits for parent-child
241
www.ebook3000.com

[ vi ]
How parent-child works internally
242
Handling document relations using nested
243
Inner hits for nested documents
246
Scripting
249
Script Query
249
Post Filter
250
Reverse search using the percolate query
251
Geo and Spatial Filtering
254
Geo Distance
256
Using Geolocation to rank the search results
258
Geo Bounding Box
261
Sorting
263
Multi search
264
Search templates
267
Querying Elasticsearch from Java application
272
Summary
276
Chapter 8: How to Slice and Dice Your Data Using Aggregations
277
Aggregation basics
278
Sample data
279
Query structure
282
Multilevel aggregations
284
Types of aggregations
287
Terms aggregations (group by)
288
Size and error
288
Order
289
Minimum document count
290
Missing values
290
Aggregations based on filters
291
Aggregations on dates ( range, histogram )
294
Aggregations on numeric values (range, histogram)
298
Aggregations on geolocation (distance, bounds)
303
Geo distance
305
Geo bounds
306
Aggregations on child documents
307
Aggregations on nested documents
309
Reverse nested aggregation
312
Post filter
315
Using Kibana to visualize aggregations
318
Caching
320
Doc values
320

[ vii ]
Field data
322
Summary
323
Chapter 9: Production and Beyond
324
Configuring Elasticsearch
324
The directory structure
325
zip/tar.gz
325
DEB/RPM
325
Configuration file
326
Cluster and node name
326
Network configuration
328
Memory configuration
329
Configuring file descriptors
332
Types of nodes
333
Multinode cluster
334
Inspecting the logs
337
How nodes discover each other
339
Node failures
340
X-Pack
341
Windows
341
Mac OS X
342
Debian/RPM
342
Authentication
342
X-Pack basic license
343
Monitoring
344
Monitoring Elasticsearch clusters
345
Monitoring indices
345
Monitoring nodes
346
Thread pools
348
Elasticsearch server logs
349
Slow logs
351
Summary
352
Chapter 10: Exploring Elastic Stack (Elastic Cloud, Security, Graph,
and Alerting)
353
Elastic Cloud
354
High availability
356
Data reliability
357
Security
357
Authentication and roles
357
Securing communications using SSL
359
www.ebook3000.com

[ viii ]
Graph
360
Graph UI
365
Alerting
367
Summary
373
Index
374

Preface
Welcome to Learning Elasticsearch. We will start by describing the basics concepts and
discuss how Elasticsearch handles failures and ensures high availability. You will see how
to install Elasticsearch and Kibana and learn how to index and update your data. We will
use an e-commerce site as an example to explain how a search engine works and how to
query your data. The real power of Elasticsearch is aggregations. You will see how to
perform aggregation-based analytics with ease. You will also see how to use Kibana to
explore and visualize your data. Finally, we will discuss how to use Graph to discover
relations in your data and use alerting to set up alerts and notification on different trends in
your data.
To better explain various concepts, lots of examples have been used throughout the book.
Detailed instructions to install Elasticsearch, Kibana and how to execute the examples is
included in $IBQUFS, Setting Up Elasticsearch and Kibana.
What this book covers
Chapter 1, Introduction to Elasticsearch, describes the building blocks of Elasticsearch and
what makes Elasticsearch scalable and distributed. In this chapter, we also discuss the
strengths and limitations of Elasticsearch.
Chapter 2, Setting Up Elasticsearch and Kibana, covers the installation of Elasticsearch and
Kibana.
$IBQUFS, Modeling Your Data and Document Relations, focuses on modeling your data. To
support text search, Elasticsearch preprocess the data before indexing. This chapter
describes why preprocessing is necessary and various analyzers Elasticsearch supports. In
addition to that, we discuss how to handle relationships between different document types.
Chapter 4, Indexing and Updating Your Data, covers how to index and update your data and
what happens internally when you index and update. The data indexed in Elasticsearch is
only available after a small delay, we discuss the reason for the delay and how to control
the delay.
www.ebook3000.com

Preface
[ 2 ]
Chapter 5, Organizing Your Data and Bulk Data Ingestion, describes how to organize and
manage indices in Elasticsearch using aliases and templates and more. This chapter also
covers various Bulk APIâ€™s Elasticsearch supports and how to rebuild your existing indices
using Reindex and Shrink API.
Chapter 6, All About Search, covers how to search, sort and paginate on your data. The
concept of relevance is introduced and we discuss how to tune the relevance score to get the
most relevant search results at the top.
$IBQUFS, More Than a Search Engine (Geofilters, Autocomplete and More), covers how to filter
based on geolocation, using Elasticsearch suggesters for autocomplete, correcting user
typoâ€™s and lot more.
Chapter 8, How to Slice and Dice Your Data Using Aggregations, covers different kinds of
aggregations Elasticsearch supports and how to visualize the data using Kibana.
Chapter 9, Production and Beyond, covers important settings to configure and monitor in
production.
Chapter 10, Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting), covers Elastic
Cloud, which is managed cloud hosting and other products that are part of X-Pack.
What you need for this book
The book was written using Elasticsearch 5.1.2, and all the examples used in the book
should work with it. The request format used in this book is based on the Kibana Console
and youâ€™ll need Kibana Console or Sense Chrome plugin to execute the examples used in
this book. Please refer to Query format used in this book section of Chapter 2, Setting up
Elasticsearch and Kibana for more details. If using Kibana or Sense is not option, you can use
other HTTP clients such as cURL or Postman. The request format is slightly different and is
explained in the Using cURL or Postman section of Chapter 2, Setting Up Elasticsearch and
Kibana.
Who this book is for
This book is for software developers who are planning to build a search and analytics
engine or are trying to learn Elasticsearch.
Some familiarity with web technologies (JavaScript, REST, JSON) would be helpful.

Preface
[ 3 ]
Conventions
In this book, you will find a number of text styles that distinguish between different kinds
of information. Here are some examples of these styles and an explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions, path
names, dummy URLs, user input, and Twitter handles are shown as follows: "We can
include other contexts through the use of the JODMVEF directive."
A block of code is set as follows:
\
BSUJDMFJE
OBNF*OUSPEVDUJPOUP&MBTUJDTFBSDI
^
When we wish to draw your attention to a particular part of a code block, the relevant lines
or items are set in bold:
\
BSUJDMFJE
"name": "Introduction to Elasticsearch"
^
New terms and important words are shown in bold. Words that you see on the screen, for
example, in menus or dialog boxes, appear in the text like this: "Clicking the Next button
moves you to the next screen."
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
www.ebook3000.com

Preface
[ 4 ]
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about this
bookâ€”what you liked or disliked. Reader feedback is important for us as it helps us
develop titles that you will really get the most out of.
To send us general feedback, simply e-mail GFFECBDL!QBDLUQVCDPN, and mention the
book's title in the subject of your message.
If there is a topic that you have expertise in and you are interested in either writing or
contributing to a book, see our author guide at XXXQBDLUQVCDPNBVUIPST.
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you
to get the most from your purchase.
Downloading the example code
You can download the example code files for this book from your account at IUUQXXXQ
BDLUQVCDPN. If you purchased this book elsewhere, you can visit IUUQXXXQBDLUQVCD
PNTVQQPSUand register to have the files e-mailed directly to you.
You can download the code files by following these steps:
Log in or register to our website using your e-mail address and password.
1.
Hover the mouse pointer on the SUPPORT tab at the top.
2.
Click on Code Downloads & Errata.
3.
Enter the name of the book in the Search box.
4.
Select the book for which you're looking to download the code files.
5.
Choose from the drop-down menu where you purchased this book from.
6.
Click on Code Download.
7.

Preface
[ 5 ]
You can also download the code files by clicking on the Code Files button on the book's
webpage at the Packt Publishing website. This page can be accessed by entering the book's
name in the Search box. Please note that you need to be logged in to your Packt account.
Once the file is downloaded, please make sure that you unzip or extract the folder using the
latest version of:
WinRAR / 7-Zip for Windows
Zipeg / iZip / UnRarX for Mac
7-Zip / PeaZip for Linux
The code bundle for the book is also hosted on GitHub at IUUQTHJUIVCDPN1BDLU1VCM
JTIJOH-FBSOJOH&MBTUJDTFBSDI. We also have other code bundles from our rich catalog
of books and videos available at IUUQTHJUIVCDPN1BDLU1VCMJTIJOH. Check them out!
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do
happen. If you find a mistake in one of our booksâ€”maybe a mistake in the text or the
codeâ€”we would be grateful if you could report this to us. By doing so, you can save other
readers from frustration and help us improve subsequent versions of this book. If you find
any errata, please report them by visiting IUUQXXXQBDLUQVCDPNTVCNJUFSSBUB,
selecting your book, clicking on the Errata Submission Form link, and entering the details of
your errata. Once your errata are verified, your submission will be accepted and the errata
will be uploaded to our website or added to any list of existing errata under the Errata
section of that title.
To view the previously submitted errata, go to IUUQTXXXQBDLUQVCDPNCPPLTDPOUFO
UTVQQPSUand enter the name of the book in the search field. The required information will
appear under the Errata section.
www.ebook3000.com

Preface
[ 6 ]
Piracy
Piracy of copyrighted material on the Internet is an ongoing problem across all media. At
Packt, we take the protection of our copyright and licenses very seriously. If you come
across any illegal copies of our works in any form on the Internet, please provide us with
the location address or website name immediately so that we can pursue a remedy.
Please contact us at DPQZSJHIU!QBDLUQVCDPN with a link to the suspected pirated
material.
We appreciate your help in protecting our authors and our ability to bring you valuable
content.
Questions
If you have a problem with any aspect of this book, you can contact us at
RVFTUJPOT!QBDLUQVCDPN, and we will do our best to address the problem.

1
Introduction to Elasticsearch
In this chapter, we will focus on the basic concepts of Elasticsearch. We will start by
explaining the building blocks and then discuss how to create, modify and query in
Elasticsearch. Getting started with Elasticsearch is very easy; most operations come with
default settings. The default settings can be overridden when you need more advanced
features.
I first started using Elasticsearch in 2012 as a backend search engine to power our Analytics
dashboards. It has been more than five years, and I never looked for any other technologies
for our search needs. Elasticsearch is much more than just a search engine; it supports
complex aggregations, geo filters, and the list goes on. Best of all, you can run all your
queries at a speed you have never seen before. To understand how this magic happens, we
will briefly discuss how Elasticsearch works internally and then discuss how to talk to
Elasticsearch. Knowing how it works internally will help you understand its strengths and
limitations. Elasticsearch, like any other open source technology, is very rapidly evolving,
but the core fundamentals that power Elasticsearch don't change. By the end of this chapter,
we will have covered the following:
Basic concepts of Elasticsearch
How to interact with Elasticsearch
How to create, read, update, and delete
How does search work
Availability and horizontal scalability
Failure handling
Strengths and limitations
www.ebook3000.com

Introduction to Elasticsearch
[ 8 ]
Basic concepts of Elasticsearch
Elasticsearch is a highly scalable open source search engine. Although it started as a text
search engine, it is evolving as an analytical engine, which can support not only search but
also complex aggregations. Its distributed nature and ease of use makes it very easy to get
started and scale as you have more data.
One might ask what makes Elasticsearch different from any other document stores out
there. Elasticsearch is a search engine and not just a key-value store. It's also a very
powerful analytical engine; all the queries that you would usually run in a batch or offline
mode can be executed in real time. Support for features such as autocomplete, geo-location
based filters, multilevel aggregations, coupled with its user friendliness resulted in
industry-wide acceptance. That being said, I always believe it is important to have the right
tool for the right job. Towards the end of the chapter, we will discuss itâ€™s strengths and
limitations.
In this section, we will go through the basic concepts and terminology of Elasticsearch. We
will start by explaining how to insert, update, and perform a search. If you are familiar with
SQL language, the following table shows the equivalent terms in Elasticsearch:
Database Table Row
Column
Index
Type
Document Field
Document
Your data in Elasticsearch is stored as JSON (Javascript Object Notation) documents. Most
NoSQL data stores use JSON to store their data as JSON format is very concise, flexible, and
readily understood by humans. A document in Elasticsearch is very similar to a row when
compared to a relational database. Let's say we have a 6TFS table with the following
information:
Id Name Age Gender Email
1
Luke
100
M
luke@gmail.com
2
Leia
100
F
leia@gmail.com
The users in the preceding user table, when represented in JSON format, will look like the
following:
\
JE

Introduction to Elasticsearch
[ 9 ]
OBNF-VLF
BHF
HFOEFS.
FNBJMMVLF!HNBJMDPN
^
\
JE
OBNF-FJB
BHF
HFOEFS'
FNBJMMFJB!HNBJMDPN
^
A row contains columns; similarly, a document contains fields. Elasticsearch documents are
very flexible and support storing nested objects. For example, an existing user document
can be easily extended to include the address information. To capture similar information
using a table structure, you need to create a new address table and manage the relations
using a foreign key. The user document with the address is shown here:
\
JE
OBNF-VLF
BHF
HFOEFS.
FNBJMMVLF!HNBJMDPN
"address": {
     "street": "123 High Lane",
     "city": "Big City",
     "state": "Small State",
     "zip": 12345
   }
^
Reading similar information without the JSON structure would also be difficult as the
information would have to be read from multiple tables. Elasticsearch allows you to store
the entire JSON as it is. For a database table, the schema has to be defined before you can
use the table. Elasticsearch is built to handle unstructured data and can automatically
determine the data types for the fields in the document. You can index new documents or
add new fields without adding or changing the schema. This process is also known as
dynamic mapping. We will discuss how this works and how to define schema in $IBQUFS
, Modeling Your Data and Document Relations.
www.ebook3000.com

Introduction to Elasticsearch
[ 10 ]
Index
An index is similar to a database. The term index should not be confused with a database
index, as someone familiar with traditional SQL might assume. Your data is stored in one or
more indexes just like you would store it in one or more databases. The word indexing
means inserting/updating the documents into an Elasticsearch index. The name of the
index must be unique and typed in all lowercase letters. For example, in an e-commerce
world, you would have an index for the items--one for orders, one for customer
information, and so on.
Type
A type is similar to a database table, an index can have one or more types. Type is a logical
separation of different kinds of data. For example, if you are building a blog application,
you would have a type defined for articles in the blog and a type defined for comments in
the blog. Let's say we have two types--articles and comments.
The following is the document that belongs to the article type:
\
BSUJDMFJE
OBNF*OUSPEVDUJPOUP&MBTUJDTFBSDI
^
The following is the document that belongs to the comment type:
\
DPNNFOUJE"7N,WU1X8V&VRLF@B3TN
BSUJDMFJE
DPNNFOU*UT"XFTPNF
^
We can also define relations between different types. For example, a parent/child relation
can be defined between articles and comments. An article (parent) can have one or more
comments (children). We will discuss relations further in $IBQUFS, Modeling Your Data and
Document Relations.

Introduction to Elasticsearch
[ 11 ]
Cluster and node
In a traditional database system, we usually have only one server serving all the requests.
Elasticsearch is a distributed system, meaning it is made up of one or more nodes (servers)
that act as a single application, which enables it to scale and handle load beyond what a
single server can handle. Each node (server) has part of the data. You can start running
Elasticsearch with just one node and add more nodes, or, in other words, scale the cluster
when you have more data. A cluster with three nodes is shown in the following diagram:
In the preceding diagram, the cluster has three nodes with the names elasticsearch1,
elasticsearch2, elasticsearch3. These three nodes work together to handle all the indexing
and query requests on the data. Each cluster is identified by a unique name, which defaults
to FMBTUJDTFBSDI. It is often common to have multiple clusters, one for each environment,
such as staging, pre-production, production.
Just like a cluster, each node is identified by a unique name. Elasticsearch will automatically
assign a unique name to each node if the name is not specified in the configuration.
Depending on your application needs, you can add and remove nodes (servers) on the fly.
Adding and removing nodes is seamlessly handled by Elasticsearch.
We will discuss how to set up an Elasticsearch cluster in $IBQUFS, Setting Up Elasticsearch
and Kibana.
www.ebook3000.com

Introduction to Elasticsearch
[ 12 ]
Shard
An index is a collection of one or more shards. All the data that belongs to an index is
distributed across multiple shards. By spreading the data that belongs to an index to
multiple shards, Elasticsearch can store information beyond what a single server can store.
Elasticsearch uses Apache Lucene internally to index and query the data. A shard is
nothing but an Apache Lucene instance. We will discuss Apache Lucene and why
Elasticsearch uses Lucene in the How search works section later.
I know we introduced a lot of new terms in this section. For now, just remember that all
data that belongs to an index is spread across one or more shards. We will discuss how
shards work in the Scalability and Availability section towards the end of this chapter.
Interacting with Elasticsearch
The primary way of interacting with Elasticsearch is via REST API. Elasticsearch provides
JSON-based REST API over HTTP. By default, Elasticsearch REST API runs on port 9200.
Anything from creating an index to shutting down a node is a simple REST call. The APIs
are broadly classified into the following:
Document APIs: CRUD (Create Retrieve Update Delete) operations on
documents
Search APIs: For all the search operations
Indices APIs: For managing indices (creating an index, deleting an index, and so
on)
Cat APIs: Instead of JSON, the data is returned in tabular form
Cluster APIs: For managing the cluster
We have a chapter dedicated to each one of them to discuss more in detail. For example,
indexing documents in $IBQUFS, Indexing and Updating Your Data and search in $IBQUFS
, All About Search and so on. In this section, we will go through some basic CRUD using the
Document APIs. This section is simply a brief introduction on how to manipulate data
using Document APIs. To use Elasticsearch in your application, clients in all major
languages, such as Java, Python, are also provided. The majority of the clients acts as a
wrapper around the REST API.

Introduction to Elasticsearch
[ 13 ]
To better explain the CRUD operations, imagine we are building an e-commerce site. And
we want to use Elasticsearch to power its search functionality. We will use an index named
DIBQUFS and store all the products in the type called QSPEVDU. Each product we want to
index is represented by a JSON document. We will start by creating a new product
document, and then we will retrieve a product by its identifier, followed by updating a
product's category and deleting a product using its identifier.
Creating a document
A new document can be added using the Document API's. For the e-commerce example, to
add a new product, we execute the following command. The body of the request is the
product document we want to index.
165IUUQMPDBMIPTUDIBQUFSQSPEVDU
\
UJUMF-FBSOJOH&MBTUJDTFBSDI
BVUIPS"CIJTIFL"OEIBWBSBQV
DBUFHPSZCPPLT
^
Let's inspect the request:
INDEX
chapter1
TYPE
product
IDENTIFIER
1
DOCUMENT
JSON
HTTP METHOD PUT
The document's properties, such as title, author, the category, are also known as GJFMET,
which are similar to SQL columns.
Elasticsearch will automatically create the index DIBQUFS and type
QSPEVDU if they don't exist already. It will create the index with the
default settings.
www.ebook3000.com

Introduction to Elasticsearch
[ 14 ]
When we execute the preceding request, Elasticsearch responds with a JSON response,
shown as follows:
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_version": 1,
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
"created": true
^
In the response, you can see that Elasticsearch created the document and the version of the
document is 1. Since you are creating the document using the HTTP 165 method, you are
required to specify the document identifier. If you donâ€™t specify the identifier, Elasticsearch
will respond with the following error message:
/PIBOEMFSGPVOEGPSVSJ<DIBQUFSQSPEVDU>BOENFUIPE<165>
If you donâ€™t have a unique identifier, you can let Elasticsearch assign an identifier for you,
but you should use the 1045 HTTP method. For example, if you are indexing log messages,
you will not have a unique identifier for each log message, and you can let Elasticsearch
assign the identifier for you.
In general, we use the HTTP 1045 method for creating an object. The
HTTP 165 method can also be used for object creation, where the client
provides the unique identifier instead of the server assigning the
identifier.
We can index a document without specifying a unique identifier as shown here:
1045IUUQMPDBMIPTUDIBQUFSQSPEVDU
\
UJUMF-FBSOJOH&MBTUJDTFBSDI
BVUIPS"CIJTIFL"OEIBWBSBQV
DBUFHPSZCPPLT
^

Introduction to Elasticsearch
[ 15 ]
In the above request, URL doesn't contain the unique identifier and we are using the HTTP
POST method. Let's inspect the request:
INDEX
chapter1
TYPE
product
DOCUMENT
JSON
HTTP METHOD POST
The response from Elasticsearch is shown as follows:
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
 "_id":AVmKvtPwWuEuqke_aRsm
@WFSTJPO
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
 "created": true
^
You can see from the response that Elasticsearch assigned the unique identifier
"7N,WU1X8V&VRLF@B3TN to the document and created flag is set to true. If a document
with the same unique identifier already exists, Elasticsearch replaces the existing document
and increments the document version. If you have to run the same 165 request from the
beginning of the section, the response from Elasticsearch would be this:
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_version": 2,
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
"created": false
^
In the response, you can see that the created flag is GBMTF since the document with JE
already exists. Also, observe that the version is now .
www.ebook3000.com

Introduction to Elasticsearch
[ 16 ]
Retrieving an existing document
To retrieve an existing document, we need the index, type and a unique identifier of the
document. Letâ€™s try to retrieve the document we just indexed. To retrieve a document we
need to use HTTP (&5 method as shown below:
(&5IUUQMPDBMIPTUDIBQUFSQSPEVDU
Letâ€™s inspect the request:
INDEX
chapter1
TYPE
product
IDENTIFIER
1
HTTP METHOD GET
Response from Elasticsearch as shown below contains the product document we indexed in
the previous section:
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
"found": true,
 "_source": {
UJUMF-FBSOJOH&MBTUJDTFBSDI
BVUIPS"CIJTIFL"OEIBWBSBQV
DBUFHPSZCPPLT
^
^
The actual JSON document will be stored in the @TPVSDF field. Also note the version in the
response; every time the document is updated, the version is increased.
Updating an existing document
Updating a document in Elasticsearch is more complicated than in a traditional SQL
database. Internally, Elasticsearch retrieves the old document, applies the changes, and re-
inserts the document as a new document. The update operation is very expensive. There are
different ways of updating a document. We will talk about updating a partial document
here and in more detail in the Updating your data section in $IBQUFS, Indexing and Updating
Your Data.

Introduction to Elasticsearch
[ 17 ]
Updating a partial document
We already indexed the document with the unique identifier , and now we need to update
the category of the product from just CPPLT to UFDIOJDBMCPPLT. We can update the
document as shown here:
1045IUUQMPDBMIPTUDIBQUFSQSPEVDU_update
\
 "doc": {
DBUFHPSZUFDIOJDBMCPPLT
^
^
The body of the request is the field of the document we want to update and the unique
identifier is passed in the URL.
Please note the @VQEBUF endpoint at the end of the URL.
The response from Elasticsearch is shown here:
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_version": 3,
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
^
As you can see in the response, the operation is successful, and the version of the document
is now . More complicated update operations are possible using scripts and upserts.
Deleting an existing document
For creating and retrieving a document, we used the 1045 and (&5 methods. For deleting
an existing document, we need to use the HTTP %&-&5& method and pass the unique
identifier of the document in the URL as shown here:
%&-&5&IUUQMPDBMIPTUDIBQUFSQSPEVDU
www.ebook3000.com

Introduction to Elasticsearch
[ 18 ]
Let's inspect the request:
INDEX
chapter1
TYPE
product
IDENTIFIER
1
HTTP METHOD DELETE
The response from Elasticsearch is shown here:
\
"found": true,
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
@TIBSET\
UPUBM
 "successful": 1,
GBJMFE
^
^
In the response, you can see that Elasticsearch was able to find the document with the
unique identifier  and was successful in deleting the document.
How does search work?
In the previous section, we discussed how to create, update, and delete documents. In this
section, we will briefly discuss how search works internally and explain the basic query
APIs. Mostly, I want to talk about the inverted index and Apache Lucene. All the data in
Elasticsearch is internally stored in Apache Lucene as an inverted index. Although data is
stored in Apache Lucene, Elasticsearch is what makes it distributed and provides the easy-
to-use APIs. We will discuss Search API in detail in $IBQUFS, All About Search.
Importance of information retrieval
As the computation power is increasing and cost of storage is decreasing, the amount of
day-to-day data we deal with is growing exponentially. But without a way to retrieve the
information and to be able to query it, the information we collect doesn't help.

Introduction to Elasticsearch
[ 19 ]
Information retrieval systems are very important to make sense of the data. Imagine how
hard it would be to find some information on the Internet without Google or other search
engines out there. Information is not knowledge without information retrieval systems.
Simple search query
Let's say we have a 6TFS table as shown here:
Id Name Age Gender Email
1
Luke
100
M
luke@gmail.com
2
Leia
100
F
leia@gmail.com
Now, we want to query for all the users with the name -VLF. A SQL query to achieve this
would be something like this:
TFMFDUGSPNVTFSXIFSFOBNFMJLF`MVLFa
To do a similar task in Elasticsearch, you can use the TFBSDI API and execute the following
command:
(&5IUUQDIBQUFSVTFS@TFBSDIROBNFMVLF
Let's inspect the request:
INDEX chapter1
TYPE
user
FIELD
name
Just like you would get all the rows in the 6TFS table as a result of the SQL query, the
response to the Elasticsearch query would be JSON documents:
\
JE
OBNF-VLF
BHF
HFOEFS.
FNBJMMVLF!HNBJMDPN
^
www.ebook3000.com

Introduction to Elasticsearch
[ 20 ]
Querying using the URL parameters can be used for simple queries as shown above. For
more practical queries, you should pass the query represented as JSON in the request body.
The same query passed in the request body is shown here:
1045IUUQDIBQUFSVTFS@TFBSDI
\
RVFSZ\
UFSN\
OBNFMVLF
^
^
^
The Search API is very flexible and supports different kinds of filters, sort, pagination, and
aggregations.
Inverted index
Before we talk more about search, I want to talk about the inverted index. Knowing about
inverted index will help you understand the limitations and strengths of Elasticsearch
compared with the traditional database systems out there. Inverted index at the core is how
Elasticsearch is different from other NoSQL stores, such as MongoDB, Cassandra, and so
on.
We can compare an inverted index to an old library catalog card system. When you need
some information/book in a library, you will use the card catalog, usually at the entrance of
the library, to find the book. An inverted index is similar to the card catalog. Imagine that
you were to build a system like Google to search for the web pages mentioning your search
keywords. We have three web pages with Yoda quotes from Star Wars, and you are
searching for all the documents with the word GFBS.
Document1: Fear leads to anger
Document2: Anger leads to hate
Document3: Hate leads to suffering
In a library, without a card catalog to find the book you need, you would have to go to
every shelf row by row, look at each book title, and see whether it's the book you need.
Computer-based information retrieval systems do the same.

Introduction to Elasticsearch
[ 21 ]
Without the inverted index, the application has to go through each web page and check
whether the word exists in the web page. An inverted index is similar to the following table.
It is like a map with the term as a key and list of the documents the term appears in as
value.
Term
Document
Fear
1
Anger
1,2
Hate
2,3
Suffering 3
Leads
1,2,3
Once we construct an index, as shown in this table, to find all the documents with the term
GFBS is now just a lookup. Just like when a library gets a new book, the book is added to the
card catalog, we keep building an inverted index as we encounter a new web page. The
preceding inverted index takes care of simple use cases, such as searching for the single
term. But in reality, we query for much more complicated things, and we donâ€™t use the exact
words. Now letâ€™s say we encountered a document containing the following:
Yosemite national park may be closed for the weekend due to forecast of substantial rainfall
We want to visit Yosemite National Park, and we are looking for the weather forecast in the
park. But when we query for it in the human language, we might query something like
XFBUIFSJOZPTFNJUF or SBJOJOZPTFNJUF. With the current approach, we will not be
able to answer this query as there are no common terms between the query and the
document, as shown:
Document Query
rainfall
rain
To be able to answer queries like this and to improve the search quality, we employ various
techniques such as stemming, synonyms discussed in the following sections.
www.ebook3000.com

Introduction to Elasticsearch
[ 22 ]
Stemming
Stemming is the process of reducing a derived word into its root word. For example, rain,
raining, rained, rainfall has the common root word "rain". When a document is indexed, the
root word is stored in the index instead of the actual word. Without stemming, we end up
storing rain, raining, rained in the index, and search relevance would be very low. The
query terms also go through the stemming process, and the root words are looked up in the
index. Stemming increases the likelihood of the user finding what he is looking for. When
we query for SBJOJOZPTFNJUF, even though the document originally had rainfall, the
inverted index will contain term rain.
We can configure stemming in Elasticsearch using Analyzers. We will discuss how to set up
and configure analyzers in $IBQUFS, Modeling Your Data and Document Relations.
Synonyms
Similar to rain and raining, weekend and sunday mean the same thing. The document
might not contain Sunday, but if the information retrieval system can also search for
synonyms, it will significantly improve the search quality. Human language deals with a lot
of things, such as tense, gender, numbers. Stemming and synonyms will not only improve
the search quality but also reduce the index size by removing the differences between
similar words.
More examples:
Pen, Pen[s] -> Pen
Eat, Eating -> Eat
Phrase search
As a user, we almost always search for phrases rather than single words. The inverted index
in the previous section would work great for individual terms but not for phrases.
Continuing the previous example, if we want to query all the documents with a phrase
BOHFSMFBETUP in the inverted index, the previous index would not be sufficient. The
inverted index for terms anger and leads is shown below:
Term
Document
Anger 1,2
Leads
1,2,3

Introduction to Elasticsearch
[ 23 ]
From the preceding table, the words BOHFS and MFBET exist both in EPDVNFOU and
EPDVNFOU. To support phrase search along with the document, we also need to record the
position of the word in the document. The inverted index with word position is shown
here:
Term
Document
Fear
1:1
Anger
1:3, 2:1
Hate
2:3, 3:1
Suffering 3:3
Leads
1:2, 2:2, 3:2
Now, since we have the information regarding the position of the word, we can search if a
document has the terms in the same order as the query.
Term Document
anger 1:3, 2:1
leads
1:2, 2:2
Since EPDVNFOU has BOHFS as the first word and MFBET as the second word, the same
order as the query, EPDVNFOU would be a better match than EPDVNFOU. With the inverted
index, any query on the documents is just a simple lookup. This is just an introduction to
inverted index; in real life, it's much more complicated, but the fundamentals remain the
same. When the documents are indexed into Elasticsearch, documents are processed into
the inverted index.
Apache Lucene
Apache Lucene is one of the most matured implementations of the inverted index. Lucene
is an open source full-text search library. It's very high performing, entirely written in Java.
Any application that requires text search can use Lucene. It allows adding full-text search
capabilities to any application. Elasticsearch uses Apache Lucene to manage and create its
inverted index. To learn more about Apache Lucene, please visit IUUQMVDFOFBQBDIFP
SHDPSF.
We will talk about how distributed search works in Elasticsearch in the next section.
www.ebook3000.com

Introduction to Elasticsearch
[ 24 ]
The term index is used both by Apache Lucene (inverted index) and
Elasticsearch index. For the remainder of the book, unless specified the
term index refers to an Elasticsearch index.
Scalability and availability
Let's say you want to index a billion documents; having just a single machine might be very
challenging. Partitioning data across multiple machines allows Elasticsearch to scale beyond
what a single machine do and support high throughput operations. Your data is split into
small parts called shards. When you create a index, you need to tell Elasticsearch the
number of shards you want for the index and Elasticsearch handles the rest for you. As you
have more data, you can scale horizontally by adding more machines. We will go in to more
details in the sections below.
There are type of shards in Elasticsearch - primary and replica. The data you index is written
to both primary and replica shards. Replica is the exact copy of the primary. In case of the
node containing the primary shard goes down, the replica takes over. This process is
completely transparent and managed by Elasticsearch. We will discuss this in detail in the
Failure Handling section below. Since primary and replicas are the exact copies, a search
query can be answered by either the primary or the replica shard. This significantly
increases the number of simultaneous requests Elasticsearch can handle at any point in
time.
As the index is distributed across multiple shards, a query against an index is executed in
parallel across all the shards. The results from each shard are then gathered and sent back to
the client. Executing the query in parallel greatly improves the search performance.
In the next section, we will discuss the relation between node, index and shard.
Relation between node, index, and shard
Shard is often the most confusing topic when I talk about Elasticsearch at conferences or to
someone who has never worked on Elasticsearch. In this section, I want to focus on the
relation between node, index, and shard. We will use a cluster with three nodes and create
the same index with multiple shard configuration, and we will talk through the differences.

Introduction to Elasticsearch
[ 25 ]
Three shards with zero replicas
We will start with an index called FTJOUSPEVDUJPO with three shards and zero replicas.
The distribution of the shards in a three node cluster is as follows:
In the above screenshot, shards are represented by the green squares. We will talk about
replicas towards the end of this discussion. Since we have three nodes(servers) and three
shards, the shards are evenly distributed across all three nodes. Each node will contain one
shard. As you index your documents into the FTJOUSPEVDUJPO index, data is spread across
the three shards.
Six shards with zero replicas
Now, let's recreate the same FTJOUSPEVDUJPO index with six shards and zero replicas.
Since we have three nodes (servers) and six shards, each node will now contain two shards.
The FTJOUSPEVDUJPO index is split between six shards across three nodes.
www.ebook3000.com

Introduction to Elasticsearch
[ 26 ]
The distribution of shards for an index with six shards is as follows:
The FTJOUSPEVDUJPO index is spread across three nodes, meaning these three nodes will
handle the index/query requests for the index. If these three nodes are not able to keep up
with the indexing/search load, we can scale the FTJOUSPEVDUJPO index by adding more
nodes. Since the index has six shards, you could add three more nodes, and Elasticsearch
automatically rearranges the shards across all six nodes. Now, index/query requests for the
FTJOUSPEVDUJPO index will be handled by six nodes instead of three nodes. If this is not
clear, do not worry, we will discuss more about this as we progress in the book.
Six shards with one replica
Let's now recreate the same FTJOUSPEVDUJPO index with six shards and one replica,
meaning the index will have 6 primary shards and 6 replica shards, a total of 12 shards.
Since we have three nodes (servers) and twelve shards, each node will now contain four
shards. The FTJOUSPEVDUJPO index is split between six shards across three nodes. The
green squares represent shards in the following figure.

Introduction to Elasticsearch
[ 27 ]
The solid border represents primary shards, and replicas are the dotted squares:
As we discussed before, the index is distributed into multiple shards across multiple nodes.
In a distributed environment, a node/server can go down due to various reasons, such as
disk failure, network issue, and so on. To ensure availability, each shard, by default, is
replicated to a node other than where the primary shard exists. If the node containing the
primary shard goes down, the shard replica is promoted to primary, and the data is not lost,
and you can continue to operate on the index. In the preceding figure, the FTJOUSPEVDUJPO
index has six shards split across the three nodes. The primary of shard 2 belongs to node
FMBTUJDTFBSDI, and the replica of the shard 2 belongs to node FMBTUJDTFBSDI. In
the case of the FMBTUJDTFBSDI node going down, the replica in FMBTUJDTFBSDI is
promoted to primary. This switch is completely transparent and handled by Elasticsearch.
Distributed search
One of the reasons queries executed on Elasticsearch are so fast is because they are
distributed. Multiple shards act as one index. A search query on an index is executed in
parallel across all the shards.
www.ebook3000.com

Introduction to Elasticsearch
[ 28 ]
Let's take an example: in the following figure, we have a cluster with two nodes: /PEF,
/PEF and an index named DIBQUFS with two shards: S0, S1 with one replica:
Assuming the DIBQUFS index has 100 documents, S1 would have 50 documents, and S0
would have 50 documents. And you want to query for all the documents that contain the
word &MBTUJDTFBSDI. The query is executed on S0 and S1 in parallel. The results are
gathered back from both the shards and sent back to the client. Imagine, you have to query
across million of documents, using Elasticsearch the search can be distributed. For the
application I'm currently working on, a query on more than 100 million documents comes
back within 50 milliseconds; which is simply not possible if the search is not distributed.
Failure handling
Elasticsearch handles failures automatically. This section describes how the failures are
handled internally. Letâ€™s say we have an index with two shards and one replica. In the
following diagram, the shards represented in solid line are primary shards, and the shards
in the dotted line are replicas:

Introduction to Elasticsearch
[ 29 ]
As shown in preceding diagram, we initially have a cluster with two nodes. Since the index
has two shards and one replica, shards are distributed across the two nodes. To ensure
availability, primary and replica shards never exist in the same node. If the node containing
both primary and replica shards goes down, the data cannot be recovered. In the preceding
diagram, you can see that the primary shard 4 belongs to Node 1 and the replica shard 4
to the Node 2.
Next, just like we discussed in the Relation between Node, Index and Shard section, we will add
two new nodes to the existing cluster, as shown here:
www.ebook3000.com

Introduction to Elasticsearch
[ 30 ]
The cluster now contains four nodes, and the shards are automatically allocated to the new
nodes. Each node in the cluster will now contain either a primary or replica shard. Now,
let's say Node2, which contains the primary shard S1, goes down as shown here:
Since the node that holds the primary shard went down, the replica of S1, which lives in
Node3, is promoted to primary. To ensure the replication factor of , a copy of the shard S1
is made on Node1. This process is known as rebalancing of the cluster.
Depending on the application, the number of shards can be configured while creating the
index. The process of rebalancing the shards to other nodes is entirely transparent to the
user and handled automatically by Elasticsearch.

Introduction to Elasticsearch
[ 31 ]
Strengths and limitations of Elasticsearch
The strengths of Elasticsearch are as follows:
Very flexible Query API:
It supports JSON-based REST API.
Clients are available for all major languages, such as Java, Python,
PHP, and so on.
It supports filtering, sort, pagination, and aggregations in the same
query.
Supports auto/dynamic mapping:
In the traditional SQL world, you should predefine the table
schema before you can add data. Elasticsearch handles
unstructured data automatically, meaning you can index JSON
documents without predefining the schema. It will try to figure out
the field mappings automatically.
Adding/removing the new/existing fields is also handled
automatically.
Highly scalable:
Clustering, replication of data, automatic failover are supported
out of the box and are completely transparent to the user. For more
details, refer to the Availability and Horizontal Scalability section.
Multi-language support:
We discussed how stemming works and why it is important to
remove the difference between the different forms of root words.
This process is completely different for different languages.
Elasticsearch supports many languages out of the box.
Aggregations:
Aggregations are one of the reasons why Elasticsearch is like
nothing out there.
It comes with very a powerful analytics engine, which can help
you slice and dice your data.
It supports nested aggregations. For example, you can group users
first by the city they live in and then by their gender and then
calculate the average age of each bucket.
www.ebook3000.com

Introduction to Elasticsearch
[ 32 ]
Performance:
Due to the inverted index and the distributed nature, it is
extremely high performing. The queries you traditionally run
using a batch processing engine, such as Hadoop, can now be
executed in real time.
Intelligent filter caching:
The most recently used queries are cached. When the data is
modified, the cache is invalidated automatically.
The limitations of Elasticsearch are as follows:
Not real time - eventual consistency (near real time):
The data you index is only available for search after 1 sec. A
process known as refresh wakes up every 1 sec by default and
makes the data searchable.
Doesn't support SQL like joins but provides parent-child and nested to handle
relations.
Doesn't support transactions and rollbacks: Transactions in a distributed system
are expensive. It offers version-based control to make sure the update is
happening on the latest version of the document.
Updates are expensive. An update on the existing document deletes the
document and re-inserts it as a new document.
Elasticsearch might lose data due to the following reasons:
Network partitions.
Multiple nodes going down at the same time.
Elasticsearch has come a long way in improving resiliency. The
current status can be tracked at IUUQTXXXFMBTUJDDPHVJEFF
OFMBTUJDTFBSDISFTJMJFODZDVSSFOUJOEFYIUNM.
We will discuss all these concepts in detail in the further chapters.

Introduction to Elasticsearch
[ 33 ]
Summary
In this chapter, you learned the basic concepts of Elasticsearch. Elasticsearch REST APIs
make most operations simple and straightforward. We discussed how to index, update, and
delete documents. You also learned how distributed search works and how failures are
handled automatically. At the end of the chapter, we discussed various strengths and
limitations of Elasticsearch.
In the next chapter, we will discuss how to set up Elasticsearch and Kibana. Installing
Elasticsearch is very easy as it's designed to run out of the box. Throughout this book,
several examples have been used to better explain various concepts. Once you have
Elasticsearch up and running, you can try the queries for yourself.
www.ebook3000.com

2
Setting Up Elasticsearch and
Kibana
In this chapter, we will discuss how to set up Elasticsearch and Kibana. We will first install
Elasticsearch as a single-node cluster and then install Kibana. Running Elasticsearch is very
easy as it can be started without any configuration; it is designed to run out of the box. Once
we have Elasticsearch up and running, we will go through various APIs that are available
to gauge the health of the cluster. By the end of this chapter, we will have covered the
following:
Installing Elasticsearch
Installing Kibana
Kibana Dev Tools
HTTP clients
Monitoring cluster health
Installing Elasticsearch
In this section, we will install Elasticsearch on a local machine as a single-node cluster. Once
we have the cluster up and running, we will learn how to use cluster APIs to check the
health of the nodes. We have a Windows section for Windows users, a Mac OS X section for
Mac users, and a Debian/RPM section for users who want to install using the Debian
(deb)/RPM package.

Setting Up Elasticsearch and Kibana
[ 35 ]
If you are a Linux/Unix user, you can either install using the Debian/RPM
package or you can follow instructions in the Mac OS X section.
Installing Java
Elasticsearch is a Java-based application. Before we can run Elasticsearch, we need to make
sure that we have Java installed. You need a Java Runtime Environment(JRE). Elasticsearch
recommends Oracle Java Development Kit (JDK) @ or higher. You can check the
Java version installed on your computer by running the following command in the
command prompt or terminal:
KBWBWFSTJPO
If you don't have Java installed or if you have an older version, please follow the
instructions in the following link:
IUUQEPDTPSBDMFDPNKBWBTFEPDTUFDIOPUFTHVJEFTJOTUBMMJOTUBMM@PWFSWJF
XIUNM
If you are a Windows or Mac user, you can skip to the Windows or Mac
OS X section for detailed instructions.
At the time of writing, the latest Java version is @, which can be downloaded from
IUUQXXXPSBDMFDPNUFDIOFUXPSLKBWBKBWBTFEPXOMPBETKELEPXOMPBET
IUNM.
Please accept the license agreement and download the JDK in the choice of your package
format. Once you have Java installed, please verify the Java version using the KBWB
WFSTJPO command.
Windows
First, let's check the Java version using command prompt by executing the KBWBWFSTJPO
command. If you either have an older Java version or don't have Java installed, go to the
following Oracle site, and download the JDK in the FYF format. Accept the license
agreement before you can download the FYF file:
www.ebook3000.com

Setting Up Elasticsearch and Kibana
[ 36 ]
IUUQXXXPSBDMFDPNUFDIOFUXPSLKBWBKBWBTFEPXOMPBETKELEPXOMPBET
IUNM
Once you install Java using the FYF file, verify that the +"7"@)0.& environment variable is
set by executing the following command in the command prompt:
FDIP+"7"@)0.&
If +"7"@)0.& is not set or is pointing to the directory with an older Java version, you can
update your +"7"@)0.& system variable by following the given steps:
Right-click on My Computer, and select Properties.
1.
Click on Advanced system Settings to open System Properties.
2.
In System Properties, go to Advanced Tab, and click on Environment Variables.
3.
Edit/add +"7"@)0.& to $=1SPHSBN'JMFT=+BWB=KEL@ or the
4.
location where you installed the JDK.
Close the command prompt, and open it again to reload the environment
5.
variables.
Verify that +"7"@)0.& is set by running KBWBWFSTJPO.
6.
Once we have verified Java version, let's download the latest version of Elasticsearch in the
[JQ package format from IUUQTXXXFMBTUJDDPEPXOMPBETFMBTUJDTFBSDI.
The &MBTUJDTFBSDI[JQ package can be downloaded from this:
IUUQTBSUJGBDUTFMBTUJDDPEPXOMPBETFMBTUJDTFBSDIFMBTUJDTFBSDI[JQ
Once you have downloaded the ZIP file, unzip the ZIP package to your choice of location.
Starting and stopping Elasticsearch
You can start Elasticsearch using the binary scripts in the bin folder. To start Elasticsearch
instance, execute the following commands:
DED=FMBTUJDTFBSDI
CJO=FMBTUJDTFBSDICBU
If Elasticsearch is started successfully, you will see the TUBSUFE message at the end of the
messages in the console shown as follows:
<5><*/'0><PFI)UUQ4FSWFS><FMBTUJDTFBSDI>
QVCMJTI@BEESFTT\^CPVOE@BEESFTTFT\<GF>^
\<>^\^
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUBSUFE

Setting Up Elasticsearch and Kibana
[ 37 ]
As you can see the from the preceding log message, Elasticsearch is running at
IUUQ. We just started Elasticsearch as a single-node cluster.
Since you started Elasticsearch using the command prompt, to stop Elasticsearch, you can
just terminate the process by pressing Ctrl + C. If you stop Elasticsearch successfully, you
should see a message in the console similar to the one shown as follows:
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUPQQJOH
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUPQQFE
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>DMPTJOH
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>DMPTFE
You can see from the console log that Elasticsearch has been stopped.
Mac OS X
First, let's verify the Java version by executing the KBWBWFSTJPO command in your
terminal. If you either have an older Java version or don't have Java installed, go to the
following Oracle site and download the JDK in the ENH format. Accept the license
agreement before you can download the ENH file:
IUUQXXXPSBDMFDPNUFDIOFUXPSLKBWBKBWBTFEPXOMPBETKELEPXOMPBET
IUNM
Once you install Java using the ENH file, verify that +"7"@)0.& is set by executing the
following command in a terminal:
FDIP+"7"@)0.&
If +"7"@)0.& is not set or is pointing to the directory with an older Java version, you can
update your +"7"@)0.& system variable by following these steps:
Open your terminal, and open your CBTI@QSPGJMF file by executing WJ
1.
_CBTI@QSPGJMF.
You can edit the file by pressing the letter i.
2.
In the file, set the +"7"@)0.& environment variable by adding the following to
3.
the end of the file:
FYQPSU
+"7"@)0.&-JCSBSZ+BWB+BWB7JSUVBM.BDIJOFTKEL@KEL
$POUFOUT)PNF
FYQPSU1"5)+"7"@)0.&CJO1"5)
www.ebook3000.com

Setting Up Elasticsearch and Kibana
[ 38 ]
Save and exit by pressing the ESC key and entering XR.
4.
Close and reopen your terminal to update the environment variables.
5.
Verify +"7"@)0.& by executing FDIP+"7"@)0.&.
6.
Once we have confirmed the Java version, let's download the latest version of Elasticsearch
in the UBSH[ format by running the following command in your terminal:
DVSM-0
IUUQTBSUJGBDUTFMBTUJDDPEPXOMPBETFMBTUJDTFBSDIFMBTUJDTFBSDIUB
SH[
UBSYWGFMBTUJDTFBSDIUBSH[
Once you have downloaded the ZIP file, unzip the tar package to your choice of location.
Starting and stopping Elasticsearch
You can start Elasticsearch using the binary scripts in the bin folder. To start Elasticsearch
instance, execute the following commands:
DEFMBTUJDFBSDI
CJOFMBTUJDTFBSDI
If Elasticsearch is started successfully, you will see the TUBSUFE message at the end of the
messages in the console as shown here:
<5><*/'0><PFI)UUQ4FSWFS><FMBTUJDTFBSDI>
QVCMJTI@BEESFTT\^CPVOE@BEESFTTFT\<GF>^
\<>^\^
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUBSUFE
As you can see from the preceding log message, we have Elasticsearch running at
IUUQ. We just started Elasticsearch as a single-node cluster.
Since you started Elasticsearch using the command prompt. To stop, you can just terminate
the process by pressing $USM$. If you stop Elasticsearch successfully, you should see a
message in the console similar to what shown as follows:
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUPQQJOH
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUPQQFE
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>DMPTJOH
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>DMPTFE
You can see from the preceding console log that Elasticsearch has been successfully
stopped.

Setting Up Elasticsearch and Kibana
[ 39 ]
DEB and RPM packages
You can also install Elasticsearch using the Debian or RPM (Red Hat Package Manager)
package. When installing using DEB or RPM packages, Elasticsearch will be installed to
VTSTIBSFFMBTUJDTFBSDI. Please note that the directory structure is different when
you install using Debian or RPM package compared to the zip/tar file.
Debian package
For a Debian-based operating system, such as Ubuntu, you can download the Debian
package directly from the Elasticsearch website using the following link:
IUUQTBSUJGBDUTFMBTUJDDPEPXOMPBETFMBTUJDTFBSDIFMBTUJDTFBSDIEFC
After you download the package, you can install Elasticsearch using the following
command:
TVEPEQLHJFMBTUJDTFBSDIEFC
When using the Debian package, Elasticsearch will be installed as a service. You can start
and stop Elasticsearch using a service command as shown in the following Starting and
Stopping Elasticsearch section.
RPM package
For RPM-based operating systems, such as Centos and Red Hat, you can download the
RPM package directly from the Elasticsearch website using the following link:
IUUQTBSUJGBDUTFMBTUJDDPEPXOMPBETFMBTUJDTFBSDIFMBTUJDTFBSDISQ
N
After you download the package, you can install Elasticsearch using the following
command:
TVEPSQNJOTUBMMFMBTUJDTFBSDISQN
When using the RPM package, Elasticsearch will be installed as a service. You can start and
stop Elasticsearch using the TFSWJDF command as shown in the following Starting and
Stopping Elasticsearch section.
www.ebook3000.com

Setting Up Elasticsearch and Kibana
[ 40 ]
Starting and stopping Elasticsearch
If you install Elasticsearch using a Debian or RPM package, you can start/stop Elasticsearch
using the TFSWJDF command.
You can start Elasticsearch using the TFSWJDF command shown as follows:
TVEPJTFSWJDFFMBTUJDTFBSDITUBSU
You can stop Elasticsearch using the TFSWJDF command shown as follows:
TVEPJTFSWJDFFMBTUJDTFBSDITUPQ
You can check the MPH file, shown as follows, to make sure Elasticsearch is started/stopped
successfully. If Elasticsearch is started, you should see a log message saying TUBSUFE, and if
it is stopped successfully, you should see a log message saying TUPQQFE:
UBJMWBSMPHFMBTUJDTFBSDIFMBTUJDTFBSDIMPH
If you want to start Elasticsearch automatically on a system startup, you can configure 4ZT7
JOJU or TZTUFNE depending on your operating system.
Sample configuration files
If you are just starting with Elasticsearch, the following are the sample
FMBTUJDTFBSDIZNM files for a cluster with one or more nodes. The configuration is
specific to a single node and has to be configured for all the nodes in the cluster. If you
install Elasticsearch using [JQ or UBSH[, the FMBTUJDTFBSDIZNM file is located in the
config directory in the main FMBTUJDTFBSDI folder. If you install Elasticsearch
using DEB/RPM, it is located by default in the FUDFMBTUJDTFBSDI folder. We will go
into more details in $IBQUFS, Production and Beyond.
If you are starting with just one node, you need to set the cluster name, node name, and the
IP address that the Elasticsearch should bind to. If no IP address is provided, it will bind to
the localhost:
DMVTUFSOBNFFTEFW
OPEFOBNFFMBTUJDTFBSDI
OFUXPSLIPTU
If you are starting with two or more nodes in the cluster, along with the aforementioned
properties, you also have to specify the IP address of other nodes in the cluster so that they
can discover each other.

Setting Up Elasticsearch and Kibana
[ 41 ]
The following is the configuration for OPEF:
DMVTUFSOBNFFTEFW
OPEFOBNFFMBTUJDTFBSDI
OFUXPSLIPTU
EJTDPWFSZ[FOQJOHVOJDBTUIPTUT<OPEF@JQ@BEESFTT
OPEF@JQ@BEESFTT>
The following is the configuration for OPEF:
DMVTUFSOBNFFTEFW
OPEFOBNFFMBTUJDTFBSDI
OFUXPSLIPTU
EJTDPWFSZ[FOQJOHVOJDBTUIPTUT<OPEF@JQ@BEESFTT
OPEF@JQ@BEESFTT>
Note that the cluster name should be the same for all the nodes in the
cluster. Internally, the nodes communicate with each other using the
transport protocol, and the port number for the transport protocol is 9300.
Verifying Elasticsearch is running
In $IBQUFS, Introduction to Elasticsearch, we described the 3&45GVM API that Elasticsearch
provides. By default, Elasticsearch runs on the  HTTP port. Once Elasticsearch is
running, you can verify Elasticsearch by executing the IUUQ URL in
your favorite browser. You should see a JSON response similar to the following:
\
"name": "A_TmR2p",
   "cluster_name": "elasticsearch",
DMVTUFS@VVJE.QU[JZL4KZN'X#:P[.3"
WFSTJPO\
OVNCFS
CVJME@IBTIDDD
CVJME@EBUF5;
CVJME@TOBQTIPUGBMTF
MVDFOF@WFSTJPO
^
UBHMJOF:PV,OPXGPS4FBSDI
^
You can see from the JSON response that the node name is "@5N3Q, cluster name is
FMBTUJDTFBSDI, along with a few other details. By default, Elasticsearch assigns a random
name ("@5N3Q) to the node on startup.
www.ebook3000.com

Setting Up Elasticsearch and Kibana
[ 42 ]
Installing Kibana
In this section, we will install Kibana. Kibana is a web interface to visualize and analyze the
data in Elasticsearch. Kibana also provides developer tools, which is very handy for
running Elasticsearch queries. The queries used in the book can be executed using Kibana
developer tools. You need Elasticsearch up and running before you can start using Kibana.
Make sure the Kibana version matches the Elasticsearch version. At the time of writing, the
latest Elasticsearch version is .
Mac OS X
You can download Kibana for Mac OS X from the following Elasticsearch website:
DVSM-0
IUUQTBSUJGBDUTFMBTUJDDPEPXOMPBETLJCBOBLJCBOBEBSXJOY@UB
SH[
UBSYWGLJCBOBEBSXJOY@UBSH[
Once you have downloaded the ZIP file, unzip the tar package to your choice of location.
Starting and stopping Kibana
You can start Kibana using the binary scripts in the bin folder. If Elasticsearch is not running
on the  port, you have to change the Kibana configuration settings. Open
the configuration as shown next:
DELJCBOBEBSXJOY@
TVEPWJDPOGJHLJCBOBZNM
In the configuration file, find the FMBTUJDTFBSDIVSM setting and replace the value with
the IP address and port of where Elasticsearch is running. To start Kibana, execute the
following commands:
CJOLJCBOB
If Kibana is started successfully, you will see 3FBEZ at the end of the messages in the
console as shown next:
<><JOGP><MJTUFOJOH>4FSWFSSVOOJOHBUhttp://localhost:5601
<><JOGP><TUBUVT><VJTFUUJOHT>4UBUVTDIBOHFEGSPN
VOJOJUJBMJ[FEUPZFMMPX&MBTUJDTFBSDIQMVHJOJTZFMMPX
<><JOGP><TUBUVT><QMVHJOFMBTUJDTFBSDI!>4UBUVTDIBOHFE
GSPNZFMMPXUPZFMMPX/PFYJTUJOH,JCBOBJOEFYGPVOE

Setting Up Elasticsearch and Kibana
[ 43 ]
<><JOGP><TUBUVT><QMVHJOFMBTUJDTFBSDI!>4UBUVTDIBOHFE
GSPNZFMMPXUPHSFFO,JCBOBJOEFYSFBEZ
<><JOGP><TUBUVT><VJTFUUJOHT>4UBUVTDIBOHFEGSPNZFMMPXUP
HSFFOReady
As you can see the from the above log message, we have Kibana running at
IUUQ. Since you started Kibana using the command prompt. To stop
Kibana, you can just terminate the process by pressing Ctrl + C. You can also start Kibana
and detach it from your session by running:
CJOLJCBOBEJTPXO
The output of this command is the process id as shown below:
<>
To stop Kibana, you can use the process id to stop it. You can use LJMM command as
shown below:
LJMM
Windows
You can download Kibana for Windows from Elasticsearch website shown below:
IUUQTBSUJGBDUTFMBTUJDDPEPXOMPBETLJCBOBLJCBOBXJOEPXTY[JQ
Once you have downloaded the ZIP file, unzip the ZIP package to your choice of location.
Starting and stopping Kibana
You can start Kibana using the binary scripts in the bin folder. If Elasticsearch is not running
on port , you have to change the Kibana configuration settings. Open the configuration
file (DPOGJH=LJCBOBZNM) using your favorite text editor.
www.ebook3000.com

Setting Up Elasticsearch and Kibana
[ 44 ]
In the configuration file, find FMBTUJDTFBSDIVSM setting and replace the value with IP
address and port where Elasticsearch is running. To start Kibana, execute the following
commands:
DED=LJCBOBXJOEPXTY
CJO=LJCBOBCBU
If Kibana is started successfully, you will see 3FBEZ at the end of the messages in the
console as shown below:
<><JOGP><MJTUFOJOH>4FSWFSSVOOJOHBUhttp://localhost:5601
<><JOGP><TUBUVT><VJTFUUJOHT>4UBUVTDIBOHFEGSPN
VOJOJUJBMJ[FEUPZFMMPX&MBTUJDTFBSDIQMVHJOJTZFMMPX
<><JOGP><TUBUVT><QMVHJOFMBTUJDTFBSDI!>4UBUVTDIBOHFE
GSPNZFMMPXUPZFMMPX/PFYJTUJOH,JCBOBJOEFYGPVOE
<><JOGP><TUBUVT><QMVHJOFMBTUJDTFBSDI!>4UBUVTDIBOHFE
GSPNZFMMPXUPHSFFO,JCBOBJOEFYSFBEZ
<><JOGP><TUBUVT><VJTFUUJOHT>4UBUVTDIBOHFEGSPNZFMMPXUP
HSFFOReady
As you can see from the preceding log message, we have Kibana running at
IUUQ. Since you started Kibana using the command prompt, to stop
Kibana, you can just terminate the process by pressing Ctrl + C.
Query format used in this book (Kibana
Console)
The query format used in this book is based on Kibana Console. When using other HTTP
clients such as cURL or Postman, the Elasticsearch endpoint (like IUUQ)
should be passed in the URL, when using Kibana Console it is automatically inferred from
Kibana settings. Using Kibana Console while in development or for debugging is strongly
recommended. You can access Kibana Console by going to Dev Tools tab in the navigation
bar on the left.

Setting Up Elasticsearch and Kibana
[ 45 ]
The Kibana Console look like the following:
Kibana Console has a very simple UI and is divided into request and response windows.
The request window makes it very easy to work with JSON requests and supports auto-
completion and auto indentation. The response window makes it very easy to follow the
JSON response as you can expand or minimize parts of the JSON response. You can also
copy any DVSM command in its entirety from this book or anywhere else into the request
window.
In the preceding screenshot, the HTTP method, the Search API endpoint, and query are
specified in the request. This minimalistic design, support for autocompleting the query,
and JSON formatting make it very easy to interact with Elasticsearch. It also saves the 
queries in the request window so that you can run them later.
If running a Kibana instance is not an option, you can also use Sense.
Sense and Kibana Console offer the exact functionality. Sense is moved to
Kibana as Kibana Console. Sense is a Chrome plugin built by Elastic. You
can download Sense plugin for Chrome from
IUUQTDISPNFHPPHMFDPNXFCTUPSFEFUBJMTFOTFCFUBMIKHLNMMDBB
ENPQHNBOQBQNQKHNGDGJHIMFO
You don't need Kibana to use Sense. But you need to specify the
Elasticsearch server address at the top.
www.ebook3000.com

Setting Up Elasticsearch and Kibana
[ 46 ]
If using Kibana or Sense is not an option, in the next section we will discuss alternatives.
Using cURL or Postman
The primary way of interacting with Elasticsearch is using the REST API over HTTP. If
Kibana or Sense is not an option for you, you can use any of the popular HTTP clients, such
as cURL or Postman. Curl is a command line-based client available on most operating
systems. Postman is an UI-based HTTP client available for major operating systems. You 
can get postman from the following link:
IUUQTXXXHFUQPTUNBODPN
To execute the queries in this book using other HTTP clients, you have to specify the
Elasticsearch server address (such as IUUQ) in front of the API
endpoint to execute the query. Let's take an example query found in this book:
1045FTJOEFY@TFBSDI
\
RVFSZ\
NBUDI@BMM\^
^
^
To execute the preceding query in cURL, you should add the DVSM command and the E
flag and wrap the query in single quotes. The )5511045 method should be used as 
91045. The preceding query would look like the following:
curl -X1045http://127.0.0.1:9200/FTJOEFY@TFBSDI-d '\
RVFSZ\
NBUDI@BMM\^
^
^	
To run the preceding query using Postman, you need to specify the HTTP method as POST,
the URL as IUUQFTJOEFY@TFBSDI, and the request should be
passed in the body as shown next:
\
RVFSZ\
NBUDI@BMM\^
^
^
The choice of HTTP client doesn't affect the query or performance in any way.

Setting Up Elasticsearch and Kibana
[ 47 ]
Health of the cluster
Elasticsearch provides various APIs for operational management of clusters and nodes. One
of the important APIs is cluster API. In the previous section, we started a single-node
cluster. We can check the health of the cluster using the cluster API as follows:
IUUQ@DMVTUFSIFBMUI?pretty
The pretty flag at the end of the URL makes the JSON response more readable. The
response from the cluster API is shown next:
\
DMVTUFS@OBNFFTEFW
"status": "green",
UJNFE@PVUGBMTF
OVNCFS@PG@OPEFT
OVNCFS@PG@EBUB@OPEFT
BDUJWF@QSJNBSZ@TIBSET
BDUJWF@TIBSET
SFMPDBUJOH@TIBSET
JOJUJBMJ[JOH@TIBSET
VOBTTJHOFE@TIBSET
EFMBZFE@VOBTTJHOFE@TIBSET
OVNCFS@PG@QFOEJOH@UBTLT
OVNCFS@PG@JO@GMJHIU@GFUDI
UBTL@NBY@XBJUJOH@JO@RVFVF@NJMMJT
BDUJWF@TIBSET@QFSDFOU@BT@OVNCFS
^
You can tell from the JSON response that the current status of the cluster is green (the
cluster status can be green, yellow, or red). In $IBQUFS, Introduction to Elasticsearch, in the
Relation between Node, Index and Shard section, we discussed shards and how they are
allocated to the nodes. The cluster status is mainly the indication of shards being correctly
allocated in the cluster.
www.ebook3000.com

Setting Up Elasticsearch and Kibana
[ 48 ]
The following table describes what each status means:
Status
Description
Red
Red means some shards couldn't be allocated in the cluster. We discussed in $IBQUFS
, Introduction to Elasticsearch, data that belongs to an index is spread across multiple
shards. Let's say one of the nodes containing the shards of the index is down and the
shard couldn't be allocated to any node in the cluster. This can happen due to shard
corruption or nodes running out of disk space and many other reasons. In this case, a
query against the index comes back with incomplete results as all the shards are not
available.
Yellow Yellow means the primary shard is allocated, but the replicas couldn't be allocated. In
this case, any queries against the index still come back with the right results, but if the
node containing the primary shard goes down, your data is lost.
Green
Green means everything is great. Primary and replica shards are allocated correctly,
and you are ready to go.
Next, from the response, you can tell the number of nodes is one and the number of the data
nodes is also one. In the Types of nodes section of $IBQUFS, Production and beyond, we will
describe different types of nodes. In a single-node cluster, the same node acts as master,
data, and ingest nodes. The response also includes the status regarding the shards and the
overall cluster.
Summary
In this chapter, we learned how to install and configure Elasticsearch. We also learned how
to install Kibana. We discussed the various HTTP clients we can use to talk to Elasticsearch.
A lot of examples have been used throughout the book to better explain various concepts.
With Elasticsearch and Kibana up and running, you can now try out the queries in the
examples.
In the next chapter, we will talk about how to model data and manage relations in
Elasticsearch.

3
Modeling Your Data and
Document Relations
In the previous chapter, we learned how to set up and configure Elasticsearch. Once the
cluster is up and running, you can start indexing your data. Elasticsearch will automatically
figure out the schema of the documents when you index, which works great for getting
started. But for all practical purposes, you have to tell Elasticsearch the schema of your data
to avoid any surprises. Modeling your data is one of the most important steps before using
Elasticsearch. In this chapter, youâ€™ll learn how to model your data and handle relations
between different document types. In this chapter, we will cover the following:
Configure mappings
Dynamic mapping
Core data types
Complex data types
Geo location
Modeling relations in Elasticsearch
Mapping
Mapping is the process of defining the schema or the structure of the documents. It
describes the properties of the fields in the document. The properties of the field include the
data type (for example, string, integer, and so on) and the metadata. In the previous
chapters, we discussed how the documents are converted into the inverted index when we
index them. During indexing, the mappings of the fields define how the fields are indexed
and stored in the inverted index.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 50 ]
Just like you would define table schema in SQL, it is important to set the mappings of the
index before you index any data. As we discussed before, a type in Elasticsearch is like an
SQL table, which groups documents of similar nature (you would define one type for users,
one for orders). Each type has its mapping defined. Having different mappings could also
be a motivation to define a new type.
Apache Lucene, which stores your data under covers, has no concept of
types. The type information is stored in metadata and handled by
Elasticsearch.
Before we talk about the different data types Elasticsearch supports, let's take a look at a
document and how to define its mapping. Let's say we want to index the user documents,
shown as follows:
\
JE
OBNF6TFS
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
\
JE
OBNF6TFS
BHF
HFOEFS'
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
Next, we want to query all the users who are older than 50. If we index these documents as
is, we will not be able to run the query as the BHF field will be indexed as text by default,
and you can't perform range-based queries on a text field. Similarly, for the EBUF field, you
have to tell Elasticsearch how the date is formatted. For example, for the last modified date
field , Elasticsearch will not be able to say whether the month is  or . We
have to specify the format. The date format can be specified when the mapping is defined.
In the next section, we will discuss dynamic mapping.

Modeling Your Data and Document Relations
[ 51 ]
Dynamic mapping
When you index a document without specifying the mapping, Elasticsearch will
automatically determine the data types for the fields in the document. To understand how 
dynamic mapping works, let's try indexing a person document as shown next:
165DIBQUFSQFSTPO
\
OBNFKPIO
BHF
EBUF@PG@CJSUI
^
Now, let's check the mapping of the person type, which is set automatically:
(&5DIBQUFSQFSTPO@NBQQJOH
The response of HFU mapping is shown next:
\
DIBQUFS\
NBQQJOHT\
"person": {
QSPQFSUJFT\
BHF\
UZQFJOUFHFS
^
EBUF@PG@CJSUI\
UZQFEBUF
GPSNBUZZZZ..EE))NNTT]]ZZZZ..EE]]FQPDI@NJMMJT
^
OBNF\
UZQFLFZXPSE
^
^
^
^
^
^
You can see from the preceding response that Elasticsearch determined the data type of the
BHF field as long, the EBUF@PG@CJSUI field as date, and the OBNF field as text. When a new
field is encountered, it tries to determine whether the field is Boolean, long, text, or date.
Mapping numbers and boolean data fields are straightforward, but to map date fields, the
string value is first checked to see whether it matches any known date patterns.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 52 ]
By default, the string value is checked against the three formats shown as follows:
TUSJDU@EBUF@PQUJPOBM@UJNFZZZZ..EE))NNTT;ZZZZ..EE;
Although Elasticsearch can determine the data type, you should set the mapping to all
known fields to avoid any surprises. By default, for any fields that are not in the mapping,
data type is determined based on the first encountered value of the field. To avoid
unexpected mapping, you can turn off dynamic mapping, as shown next:
165DIBQUFS
\
NBQQJOHT\
OFXT\
"dynamic" : false,
QSPQFSUJFT\
EFTD\
UZQFUFYU
^
^
^
^
^
The dynamic setting accepts three different values:
true: By default, new fields are automatically added to the mapping
false: Fields not in the mapping are ignored
strict: An exception is thrown if you try to index a document with an unknown
field
Sometimes automatic date detection can cause problems. That data type is determined
based on the first encounter of the field value. If the first string value accidentally matches
the default date format, the data type of the field can be set to date instead of text. When
indexing the next document, a mapping error is thrown as you are trying to index a string
into the date field. To avoid this, you can turn off automatic date detection, as shown next:
165DIBQUFS@NBQQJOHOFXT
\
"date_detection": false,
QSPQFSUJFT\
TPVSDF\
UZQFUFYU
^
^
^

Modeling Your Data and Document Relations
[ 53 ]
Create index with mapping
To define or add a mapping, we have to use the Mapping API. Mapping for a type, like
anything else, is represented in JSON.
Mapping APIs will let you do the following:
Create a new index with mappings
Add a new type to an existing index
Add a new field to an existing type
First, let's create an index named DIBQUFS and define the mapping for the VTFS type. The
mapping for theVTFS type is defined as a JSON document as shown next.
Note that the HTTP method used for the request is PUT.
%FMFUFFYJTUJOHJOEFYJGBOZ
%&-&5&DIBQUFS
PUTDIBQUFS
\
NBQQJOHT\
 "user": {
QSPQFSUJFT\
BHF\
 "type": "integer"
^
FNBJM\
UZQFLFZXPSE
^
HFOEFS\
UZQFLFZXPSE
^
JE\
UZQFJOUFHFS
^
MBTU@NPEJGJFE@EBUF\
UZQFEBUF
"format": "yyyy-MM-dd"
^
OBNF\
UZQFLFZXPSE
^
www.ebook3000.com

Modeling Your Data and Document Relations
[ 54 ]
^
^
^
^
If the index is successfully created, you should see a response, as shown here:
\
BDLOPXMFEHFEUSVF
^
Since the mapping is set, when you index the document, the age field will be indexed as an
integer, and you can execute range queries. In the preceding mapping, also note how we
defined the format of the date as ZZZZ..EE, telling Elasticsearch how to parse the date
field.
Adding a new type/field
In the previous section, we discussed how to create an index with mapping. In this section,
we will discuss how to add a new type and new fields. Let's add a new type called IJTUPSZ
to keep track of user-login history.
Please note @NBQQJOH at the end of the URL. You can add a type named IJTUPSZ to the
DIBQUFS index, as shown next:
165DIBQUFS_mapping/history
\
QSPQFSUJFT\
username\
UZQFLFZXPSE
^
login_date\
UZQFEBUF
GPSNBUZZZZ..EE
^
^
^
You should see an acknowledged true message if the type is successfully added. Next, let's
try adding a new field to the type IJTUPSZ we just added. Along with the VTFSOBNF and
MPHJO@EBUF, we also want to record the location or the IP address from where the user
logged in. To store IP addresses, Elasticsearch has a special IP data type. We can add the
JQ@BEESFTT field to the history type, as shown here:
165DIBQUFS@NBQQJOHIJTUPSZ

Modeling Your Data and Document Relations
[ 55 ]
\
QSPQFSUJFT\
JE@BEESFTT\
"type": "ip"
^
^
^
By setting the JQ@BEESFTT field as *1 data type, we can execute range and aggregation
queries on the IP address. We will discuss the IP data type in the following sections.
Getting the existing mapping
Mapping API is also used to retrieve existing mapping. You can check the mappings of an
existing index or type as shown next:
(&5DIBQUFS@NBQQJOH
Note that the HTTP method used for the request is GET.
The response shown next contains the mapping of all the types in the DIBQUFS index:
\
DIBQUFS\
NBQQJOHT\
 "history": {
QSPQFSUJFT\
JE@BEESFTT\
UZQFJQ
^
MPHJO@EBUF\
UZQFEBUF
GPSNBUZZZZ..EE
^
VTFSOBNF\
UZQFLFZXPSE
^
^
^
"user": {
QSPQFSUJFT\
BHF\
UZQFJOUFHFS
www.ebook3000.com

Modeling Your Data and Document Relations
[ 56 ]
^
FNBJM\
UZQFLFZXPSE
^
HFOEFS\
UZQFLFZXPSE
^
JE\
UZQFJOUFHFS
^
MBTU@NPEJGJFE@EBUF\
UZQFEBUF
GPSNBUZZZZ..EE
^
OBNF\
UZQFLFZXPSE
^
^
^
^
^
^
You can also get the mapping of a single type as shown next:
(&5DIBQUFSVTFS@NBQQJOH
Mapping conflicts
Mapping of the existing fields cannot be changed. As the documents are indexed, they are
stored in the inverted index according to the data type. If you try to update the mapping of
an existing field, you will get a mapping exception, as shown next:
\
FSSPS\
SPPU@DBVTF<
\
UZQFNFSHF@NBQQJOH@FYDFQUJPO
SFBTPO.FSHFGBJMFEXJUIGBJMVSFT\<NBQQFS<username>PG
EJGGFSFOUUZQFDVSSFOU@UZQF<string>NFSHFE@UZQF<integer>>^
^
>
UZQFNFSHF@NBQQJOH@FYDFQUJPO
SFBTPO.FSHFGBJMFEXJUIGBJMVSFT\<NBQQFS<VTFSOBNF>PGEJGGFSFOU
UZQFDVSSFOU@UZQF<TUSJOH>NFSHFE@UZQF<JOUFHFS>>^
^

Modeling Your Data and Document Relations
[ 57 ]
TUBUVT
^
You can always add new fields or use multi-fields to index the same field using multiple
data types, but you cannot update an existing mapping. We will discuss multi-fields in
Mapping the same field with different mappings section below. If you want to change the
mapping, you have to re-create the index or use Reindex API. We will discuss Reindex API
in $IBQUFS, Organizing Your data and Bulk Data Ingestion. If you don't care about the
already indexed data, you can work around by adding a new field with the correct data
type.
Data type
In the traditional SQL world, a column data type can only be a simple data type, such as
integer, boolean, and so on. Since data in Elasticsearch is represented as JSON, it supports
data types that are complex and nested.
The different data types supported are as follows:
Core data types:
Text
Keyword
Date
Numeric
Boolean
Binary
Complex data types:
Array
Object (JSON object)
Nested
Geo data types:
Geo-point
Geo-shape
Specialized data types:
IP
Before we go through each data type, let's talk about the metafields each document
contains.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 58 ]
Metafields
Each document we index has the following meta fields, also known as identity meta fields
as they are used to identify a document uniquely:
@JOEFY: This is the name of the index the document belongs to.
@VJE: This is the combination of @UZQF and @JE
@UZQF: This is the mapping type the document belongs to.
@JE: This is the unique identifier of the document.
In the next section, we will discuss how to deal with null values.
How to handle null values
When Elasticsearch encounters a JSON null value in the document, it skips the field as it
cannot be indexed or searched. But if you want to search for all documents containing a null
value, you can tell Elasticsearch to replace the null value with a default value.
For example, we have a login history document with the VTFSOBNF, EBUF, and
MPHJO@TUBUVT fields as shown next:
\
VTFSOBNFVTFS
MPHJO@EBUF5
MPHJO@TUBUVTTVDDFTTGVM
^
And sometimes the MPHJO@TUBUVT is sent as null by default. If the MPHJO@TUBUVT field is
null, the MPHJO@TUBUVT field is skipped. But we want to store the MPHJO@TUBUVT field as
6/,/08/ whenever the MPHJO@TUBUVT field is null. We can do so by specifying the null
value for MPHJO@TUBUVT in the mapping, as shown next:
165DIBQUFS@NBQQJOHIJTUPSZ
\
QSPQFSUJFT\
VTFSOBNF\
UZQFLFZXPSE
^
MPHJO@EBUF\
UZQFEBUF
GPSNBUZZZZ..EE
^
MPHJO@TUBUVT\

Modeling Your Data and Document Relations
[ 59 ]
UZQFLFZXPSE
"null_value": "UNKNOWN"
^
^
^
While indexing, if the MPHJO@TUBUVT field value of the history document is null, it will be
indexed as 6/,/08/, and we can query for all the login history documents with the
MPHJO@TUBUVT field as 6/,/08/, as shown next:
(&5DIBQUFSIJTUPSZ@TFBSDIRlogin_status:UNKNOWN
Please note that the OVMM@WBMVF should be of the same data type as the
field. An integer field cannot have a null value that is a string.
Storing the original document
In $IBQUFS, Introduction to Elasticsearch, we learned how data is stored internally in the
inverted index. Along with the inverted index, the original JSON document is also stored as
the @TPVSDF field. The @TPVSDF field is used to show the original JSON document in the
result. By default, the source of the document is stored. The @TPVSDFGJFME is used not
only to return the original document but also for scripting updates and reindexing the data.
If you don't foresee using the @TPVSDF field, you can disable it to save the disk space. You
can disable the @TPVSDF field in the mapping, as shown next:
165DIBQUFS@NBQQJOHPSEFS
\
@TPVSDF\
 "enabled": false
^
QSPQFSUJFT\
CVZFS\
UZQFLFZXPSE
^
TFMMFS\
UZQFLFZXPSE
^
JUFNUJUMF\
UZQFUFYU
^
^
^
www.ebook3000.com

Modeling Your Data and Document Relations
[ 60 ]
You can disable the @TPVSDF field if you only want the document ID in the response and
don't plan to update or reindex the data.
Searching all the fields in the document
Elasticsearch lets you search for a value across all the fields in the document. If you are
exploring the data or you don't care about which field contains the search input, you can
use the @BMM field. To make this possible, when you index a document, all the field values
in the document are combined into one big string, separated by a space and indexed as the
@BMM field. For example, we are indexing e-commerce order documents, and order
documents contain CVZFS, TFMMFS, and JUFN@UJUMF fields. An example order document is
shown next:
165DIBQUFSPSEFS
\
CVZFSKPIO
TFMMFSKBLF
JUFN@UJUMFJQIPOF
PSEFS@EBUF
^
When the preceding document is indexed along with the document fields, Elasticsearch
indexes all the field values into one big string (KPIOKBLFJQIPOF) as the
@BMM field. The query for all the orders that contain JUFN@UJUMF as JQIPOF is shown next:
(&5DIBQUFSPSEFS@TFBSDIRJUFN@UJUMFJQIPOF
In the preceding query, we queried for all the documents that contain JUFN@UJUMF as
JQIPOF. But if we want to search for all the fields in the document, we can query the @BMM
field. By default, if no field is specified, Elasticsearch will query the @BMM field. We can
query the @BMM field, as shown next:
(&5DIBQUFSPSEFS@TFBSDIRJQIPOF
When indexing the @BMM field, all the fields in the document are combined into one big
string irrespective of their mapping type. In the preceding document, even though
PSEFS@EBUF is a date field, it is treated as a string when the @BMM field is indexed.
By default, the @BMM field is enabled. If you don't plan to use the @BMM field, it can be
disabled in the mapping as shown next. Disabling it will reduce the size of the index on
disk:
165DIBQUFS@NBQQJOHPSEFS

Modeling Your Data and Document Relations
[ 61 ]
\
@BMM\
"enabled": false
^
QSPQFSUJFT\
CVZFS\
UZQFLFZXPSE
^
TFMMFS\
UZQFLFZXPSE
^
JUFN@UJUMF\
UZQFUFYU
^
^
^
You cannot disable the @BMM field for an existing type. To disable the @BMM field, you have
to recreate the index and set the mapping. Elasticsearch also supports excluding individual
fields from the @BMM field instead of disabling it for the entire type. You can exclude
individual fields from the @BMM field on the fly. For example, if we want the buyer and
seller fields to be excluded from the @BMM field, we can set the JODMVEF@JO@BMM flag to
GBMTF, as shown next:
165DIBQUFS@NBQQJOHPSEFS
\
QSPQFSUJFT\
CVZFS\
UZQFLFZXPSE
"include_in_all": false
^
TFMMFS\
UZQFLFZXPSE
include_in_all": false
^
JUFN@UJUMF\
UZQFUFYU
^
^
^
In the next section, we will discuss analyzers and why mapping is essential to get the
correct search results.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 62 ]
Difference between full-text search and
exact match
In this section, we will describe analyzers and why they are necessary for text search. Let's
say we have a document containing the following information:
\
EBUF
EFTD*UXJMMCFSBJOJOHJOZPTFNJUFUIJTXFFLFOE
^
If we want to search for the documents that contain word ZPTFNJUF, we could run an SQL
query as shown here:
TFMFDUGSPNOFXTXIFSFEFTDMJLF	ZPTFNJUF	
This functionality is very limited and is never sufficient for real-world text-search queries.
For example, if a user is looking for the weather forecast in Yosemite, he/she would query
for the same in human language using something such as SBJOJOZPTFNJUF. Since SQL
can only match the exact words, and the document doesn't contain the word SBJO, the
query will not come back with any results.
Elasticsearch is a full-text search engine and is built to handle these kinds of queries. Before
the documents are indexed into Elasticsearch, the fields in the document are analyzed.
Analyzing the data breaks the text phrases into individual terms, and depending on the
analyzer we use, the words are reduced to their root forms. The following example will
make it more clear. Elasticsearch provides the Analyze API to inspect how it analyzes the
text internally. Let's see what happens when we analyze the document about yosemite,
which contains the text *UXJMMCFSBJOJOHJOZPTFNJUFUIJTXFFLFOE:
(&5
@BOBMZ[FBOBMZ[FSFOHMJTIUFYU*UXJMMCFSBJOJOHJOZPTFNJUFUIJTXFFLFOE
The response of the analyze API is a list of tokens as shown next:
\
UPLFOT<
\
"token": "rain",
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
\

Modeling Your Data and Document Relations
[ 63 ]
"token": "yosemit",
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
\
"token": "weekend",
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
>
^
The text is broken down into three tokens: SBJO, ZPTFNJU, and XFFLFOE, and since we used
the English language analyzer, the terms are also reduced to their root forms (raining
became rain). The tokens you see in the preceding response are then stored in the inverted
index. In the response, along with the token, position and offset information are also stored
to support phrase search.
Similar to how the text fields in the document are analyzed during indexing, the text in the
query is also analyzed. Let's take an example: when the user queries SBJOJOZPTFNJUF it
goes through a similar analysis process. We can use the @BOBMZ[F API to look at how
Elasticsearch break downs the search input as shown next:
(&5@BOBMZ[FBOBMZ[FSenglishUFYUrain+in+yosemite
The response of the analysis is shown here:
\
UPLFOT<
\
UPLFOSBJO
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
\
UPLFOZPTFNJU
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
www.ebook3000.com

Modeling Your Data and Document Relations
[ 64 ]
>
^
You can see from the preceding response that the SBJOJOZPTFNJUF query is broken
down into the SBJO and ZPTFNJU. Using the tokens, Elasticsearch tries to find the
documents in the inverted index. By default, the same analyzer is used for indexing and
querying.
If no analyzer is specified in the mapping, all the text fields are analyzed using a standard
analyzer, which splits the text on space and removes the casing and so on. If you want a
language specific analyzer as shown in preceding example, we can specify the analyzer in
the mapping. We can configure the analyzer of the EFTD field to use an English language
analyzer, as shown next:
165DIBQUFS@NBQQJOHXFBUIFS
\
QSPQFSUJFT\
EFTD\
UZQFUFYU
"analyzer": "english"
^
^
^
Elasticsearch supports lots of analyzers out of the box. The default is the standard analyzer.
You can check how each analyzer works using the Analyze API. For the complete list of all
the analyzers Elasticsearch supports, please visit the following link:
IUUQTXXXFMBTUJDDPHVJEFFOFMBTUJDTFBSDISFGFSFODFDVSSFOUBOBMZTJTBOB
MZ[FSTIUNM
Core data types
In this section, we will discuss the core data types supported by Elasticsearch. You can set
the mapping using the Mapping API.
Text
Starting Elasticsearch 5.0, the TUSJOH data type is deprecated and replaced by the UFYU and
LFZXPSE data types. If you want to perform a full-text search as we discussed in the
previous section, you should use UFYU data type. If you only want an exact match, you
should use LFZXPSE data type. We will discuss LFZXPSE data type in the next section.

Modeling Your Data and Document Relations
[ 65 ]
Let's take the same example we used in $IBQUFS, Introduction to Elasticsearch. We have a
document containing the following fields:
\
EBUF
EFTDSJQUJPO:PTFNJUFOBUJPOBMQBSLNBZCFDMPTFEGPSUIFXFFLFOEEVF
UPGPSFDBTUPGTVCTUBOUJBMSBJOGBMM
^
For the description field in the preceding document, we should use text data type. Text
fields are analyzed, and depending on what analyzer you use, it also takes care of removing
stop words, such as B or UIF. You can set the mapping of the field description to text
and the analyzer to English, as shown next:
165DIBQUFS@NBQQJOHOFXT
\
QSPQFSUJFT\
EFTDSJQUJPO\
"type": "text",
BOBMZ[FSFOHMJTI
^
^
^
Text mapping also accepts several optional parameters. Most of these properties are to
make the search more accurate. I will briefly describe these parameters in this section and
talk about important parameters in more detail in $IBQUFS, All about search. The following
parameters are accepted:
analyzer: This denotes the analyzer that should be used while indexing and
searching.
boost: Boost helps in ordering the search results. When one or more documents
have the same relevancy score, boosting a field helps the ordering of the results.
By default, all fields in the document are equally important. The default value is
. You can set the boost on a field, for example QSJDF, to . When two or
more products have the same relevancy score, the ordering of the products will
be based on the price.
fielddata: Fielddata is an in-memory data structure used to support sorting and
aggregation for text fields. The default value is false.
fields: For example, you have an item title that can be in English or German.
Multifields will let you index the title field using an English analyzer and a
German analyzer so that you search on both the fields during query time.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 66 ]
include_in_all: This property is about whether to include this field in the @BMM
field. The @BMM field helps you search for all the fields in the documents without
specifying a field.
index: This is used if you want the field to be searchable.
index_options: When adding the field to the inverted index, this property
specifies whether any additional information needs to be stored. It accepts docs,
freqs, positions, and offsets and defaults to positions.
norms: This is used when scoring the results to determine whether the length of
the field should be taken into account.
position_increment_gap: This property can be set to better support phase
queries. Defaults to 100
store: This determines whether the field should be stored in the @TPVSDF field.
search_analyzer: By default, the same analyzer that is used for indexing is used
for searching. You can the change the TFBSDI@BOBMZ[FS using this property
search_quote_analyzer: If you are searching for a phrase and want the phrase to
be analyzed differently, this property can be set. This defaults to the
search_analyzer
similarity: The results of a search query are scored based on the similarity
algorithm specified in this property. Defaults to BM25. We will discuss BM25 in
detail in $IBQUFS, All about search.
term_vector: When a field is broken into individual terms, this property controls
whether the term vectors need to be stored.
Some of the aforementioned properties could be very advanced. We will describe the
important properties in the next sections.
Keyword
In the previous section, we discussed the UFYU data type, which is used for free text search.
Keyword data type should be used if you want the exact match. For example name, e-mail,
city, and so on.
You can set the mapping of the name field to the keyword, as shown next:
165DIBQUFS@NBQQJOHVTFS
\
QSPQFSUJFT\
FNBJM\
 "type": "keyword"
^

Modeling Your Data and Document Relations
[ 67 ]
^
^
The keyword data type is used for fields that need sorting or aggregations.
Date
The date data type is used to represent date fields in Elasticsearch. In JSON, the date fields
are passed as a string. To parse the date correctly, you should tell Elasticsearch how the
date string is formatted. If no format is specified, Elasticsearch tries to parse the date field
using the ZZZZ..EE	5	))NNTT[ format, also known as
TUSJDU@EBUF@PQUJPOBM@UJNF, which requires date and optional time.
An example document is shown as follows:
\
DSFBUJPO@EBUF
EFTD:PTFNJUFOBUJPOBMQBSLNBZCFDMPTFEGPSUIFXFFLFOEEVFUP
GPSFDBTUPGTVCTUBOUJBMSBJOGBMM
^
You can tell Elasticsearch how DSFBUJPO@EBUF is formatted in the mapping, as shown next:
165DIBQUFS@NBQQJOHOFXT
\
QSPQFSUJFT\
DSFBUJPO@EBUF\
"type": "date",
       "format": "yyyy-MM-dd"
^
^
^
If you are expecting the date in multiple formats, you can specify multiple formats
separated by ]] in the format field. It will try each format until it finds a successful one.
You can set multiple date formats in the mapping, as shown here:
165DIBQUFS@NBQQJOHOFXT
\
QSPQFSUJFT\
DSFBUJPO@EBUF\
 "type": "date",
"format": "YYYY-mm-dd||YYYY-mm-dd HH:mm:ss"
^
^
^
www.ebook3000.com

Modeling Your Data and Document Relations
[ 68 ]
If no time zone is specified in the date, dates are stored in the UTC (Universal Time
Coordinated) format. Internally, the date field is stored as a long number, which represents
the number of milliseconds elapsed since Jan 1, 1970 (epoch).
Numeric
To represent numeric data type, Elasticsearch provides the following types:
long: This is used to represent a long value (signed 64-bit integer)
integer: This is used to represent an integer (signed 32-bit integer)
short: This is used to represent a shot (signed 16-bit integer)
byte: This is used to represent a byte (signed 8-bit integer)
double: This is used to represent a double value (double precision float)
float: This is used to represent a float value (single precision float)
half_float: This is used to represent a half float value (half precision float)
scaled_float: This is used to represent a float value with scaling factor
You can pick the data type that suits your need best as Elasticsearch optimizes the internal
storage irrespective of the type you choose. Worth mentioning is the TDBMFE@GMPBU data
type. Since it's more expensive to store float than integers, Elasticsearch tries to optimize
this by storing the float as an integer. It converts the float into an integer by multiplying the
float value with the scaling factor. Let's take an example document as shown here:
\
EBUF
DJUZ4BO+PTF
UFNQFSBUVSF
^
Since temperature can only have two decimal points, you can store the UFNQFSBUVSF field
as TDBMFE@GMPBU and set the scaling factor to . You can set the mapping of the
UFNQFSBUVSF field, as shown here:
165DIBQUFS@NBQQJOHXFBUIFS
\
QSPQFSUJFT\
UFNQFSBUVSF\
 "type": "scaled_float",
       "scaling_factor": "100"
^
^
^

Modeling Your Data and Document Relations
[ 69 ]
Internally, temperature is stored as , which is . How Elasticsearch stores
the value internally is entirely transparent to the user. The higher the scaling factor, the
more storage is used to store the float value.
Boolean
The boolean data type is used to represent boolean fields (true or false). Boolean mapping
can be set as follows:
165DIBQUFS@NBQQJOHCPPMFBO
\
QSPQFSUJFT\
CPPMFBO@GJFME\
  "type": "boolean"
^
^
^
Binary
The binary data type is used to store a #BTF encoded string. Binary data type fields are
not indexed and only stored by default. You can set binary mapping as shown here:
165DIBQUFS@NBQQJOHCJOBSZ
\
QSPQFSUJFT\
CJOBSZ@GJFME\
  "type": "binary"
^
^
^
Binary data types can be used to blob data such as images, large objects compressed, and so
on.
Complex data types
In the previous section, we talked about simple data types. In this section, we will talk
about how to set mapping for arrays, objects, and nested objects.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 70 ]
Array
There is no special data type for an array. A field can contain one or more fields of the same
data type. Let's look at an example where we have two documents, as shown next:
Document 1:
\LFZXPSE@GJFMELFZXPSE^
Document 2:
\
LFZXPSE@GJFME<LFZXPSELFZXPSE>
^
The mapping for LFZXPSE@GJFME is defined as shown next:
\
QSPQFSUJFT\
LFZXPSE@GJFME\
 "type": "keyword"
^
^
^
No special handling is required for arrays.
Object
Elasticsearch documents are JSON objects. A field in the document can be a simple integer
or an entire object. For example, the person document as shown next contains name, which
is a simple text field, and address, which is an object. And an address can also have inner
objects. The person object is shown here:
\
JE
OBNF6TFS
BEESFTT\
TUSFFU)JHI-BOF
DJUZ#JH$JUZ
^
^

Modeling Your Data and Document Relations
[ 71 ]
Unlike a simple data type, when an object is stored into inverted index, it is broken down
into key-value pairs. The person document is stored as shown here:
\
JE
OBNF6TFS
BEESFTTTUSFFU)JHI-BOF
BEESFTTDJUZ#JH$JUZ
^
Since the object is stored as key-value pairs, it gets tricky when you have an array of objects.
Let's see what happens when the person has multiple addresses. The person document is
represented as shown next:
\
JE
OBNF6TFS
BEESFTT<
\
TUSFFU)JHI-BOF
DJUZ#JH$JUZ
^
\
TUSFFU-PX-BOF
DJUZ4NBMM$JUZ
^
^
The preceding document is stored internally as follows:
\
JE
OBNF6TFS
BEESFTTTUSFFU<)JHI-BOF-PX-BOF>
BEESFTTDJUZ<#JH$JUZ4NBMM$JUZ>
^
When the address objects are stored in the inverted index, the relation between parts of the
same address is lost. For example, if you query for all documents that contain street as 
)JHI-BOF and 4NBMM$JUZ, by looking at the original document, there should be no
results. But the query comes back with the document we just indexed as a result. The
relation between the TUSFFU and DJUZ is lost due to how the document is stored. If your
application needs the relation intact, you should use nested data type, which we will
discuss in the next section.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 72 ]
You can set the object mapping type as shown next:
\
QSPQFSUJFT\
JE\
UZQFJOUFHFS
^
OBNF\
UZQFLFZXPSE
^
BEESFTT\
QSPQFSUJFT\
TUSFFU\
UZQFLFZXPSE
^
DJUZ\
UZQFLFZXPSE
^
^
^
^
^
Please note that there is no special mapping for an array of addresses (object).
Nested
As we discussed in the previous section, when we have an array of objects, the array is
flattened due to which the object relations don't exist anymore. To solve this, Elasticsearch
provides nested datatype. When you use nested datatype, each object in the array is
indexed as a new document internally. Since the objects are handled internally as separate
documents, you have to use a special type of query to query nested documents. We will
discuss nested queries and sorting in the Handling document relations using nested section of
$IBQUFS, More than a search engine.
The mapping for the address field can be set as nested, as shown next:
\
QSPQFSUJFT\
JE\
UZQFJOUFHFS
^
OBNF\
UZQFLFZXPSE
^

Modeling Your Data and Document Relations
[ 73 ]
BEESFTT@OFTUFE\
 "type": "nested",
QSPQFSUJFT\
TUSFFU\
UZQFLFZXPSE
^
DJUZ\
UZQFLFZXPSE
^
^
^
^
^
Please note how we explicitly set the type as nested for the BEESFTT@OFTUFE field.
Geo data type
In the previous sections, we discussed the simple and complex data types Elasticsearch
supports. In this section, how to store location-based data. Elasticsearch makes it very easy
to work with location-based queries, such as querying within a radius, aggregations based
on location, sorting by location, and so on. With the rapid growth of mobile, location is one
of the key factors driving the search results. To run location-based queries, you have to set
the field data type to geo.
Elasticsearch supports two data types to store location-based data:
HFPQPJOU: This is used to store the longitude and latitude of a location.
HFPTIBQF: This is used to store geo shapes, such as circles and polygons.
In this section, we will only discuss how to set the mapping for the HFPQPJOU data type.
The HFPTIBQF data type is for storing geo shapes. To know more about geo-shape, please 
visit the following link:
IUUQTXXXFMBTUJDDPHVJEFFOFMBTUJDTFBSDISFGFSFODFDVSSFOUHFPTIBQFIU
NM
In the Geo and Spatial Filtering section of DIBQUFS, More than a search engine, we will discuss
how to execute location based queries. Using geo queries is not very different from other
queries, except for a few parameters.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 74 ]
Geo-point data type
We can set the geo-point data mapping for the HFP@MPDBUJPO field, as shown next:
165DIBQUFS@NBQQJOHBEESFTT
\
QSPQFSUJFT\
HFP@MPDBUJPO\
"type": "geo_point"
^
^
^
In the following example, we will index an address document with geolocation:
165DIBQUFSBEESFTT
\
TUSFFU)JHI-BOF
DJUZ#JH$JUZ
"geo_location": {
     "lat": 37.3,
     "lon": 121.8
   }
^
Please make sure the mapping type for HFP@MPDBUJPO is set before you index the address
document otherwise the HFP@MPDBUJPO field will be stored as an object with MBU and MPO
as strings. If all goes well, you should see a response as follows:
\
@JOEFYDIBQUFS
@UZQFBEESFTT
@JE
@WFSTJPO
 "result": "created",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
DSFBUFEUSVF
^

Modeling Your Data and Document Relations
[ 75 ]
Note that the HFP@MPDBUJPO field value in the document can also be represented as:
string in the format of MBUMPO
HFP@MPDBUJPO
array with the format <MPOMBU>
HFP@MPDBUJPO<>
geo hash 
HFP@MPDBUJPOXXXTXLYZX
Specialized data type
Elasticsearch supports the following specialized data types:
IP: This is used to store IP address
Completion: This is used to support the auto-complete feature
Percolator: This is used to support reverse search
We will discuss IP data type in the next section. Completion and percolator are best
explained with examples, and we will discuss them in detail in $IBQUFS, More than a
search engine.
IP
In the previous section, we discussed HFP data type, which is used to store location-based
data. In this section, we will discuss *1 data type, which is used to store IP addresses. Both
IPv4 and IPv6 addresses are supported. For example, we have a login history document,
and we want to store the IP address of the client in the history. We can add an JQ@BEESFTT
field to history mapping, as shown next:
165DIBQUFS@NBQQJOHIJTUPSZ
\
QSPQFSUJFT\
JQ@BEESFTT\
"type": "ip"
^
^
^
www.ebook3000.com

Modeling Your Data and Document Relations
[ 76 ]
You can index the history document with an IP address, as shown here:
\
VTFSOBNFVTFS
JQ@BEESFTT
MPHJO@TUBUVT46$$&44'6-
^
Mapping the same field with different
mappings
Sometimes you want to index the same field with different mappings. For example, you
want to index the title field both as text and as keyword. You can use the keyword field for
an exact match and the text field for text search. You can do this by defining two fields, one
with keyword mapping and other with text mapping, as shown next:
\
QSPQFSUJFT\
UJUMF@UFYU\
 "type": "text"
^
UJUMF@LFZXPSE\
"type": "keyword"
^
^
^
You can index the document as follows:
\
UJUMF@UFYU-FBSOJOH&MBTUJDTFBSDI
UJUMF@LFZXPSE-FBSOJOH&MBTUJDTFBSDI
^
While indexing, the same value is used for both the UJUMF@UFYU and UJUMF@LFZXPSE
fields. The document source will now have two fields with the same value. To avoid data
duplication, while indexing and storing the document source, Elasticsearch provides fields
mapping.
Using fields mapping, the same field can be indexed as both text and keyword. As shown in
the mapping below, UJUMF will be indexed as text and UJUMFFYBDU will be indexed as
keyword which can be used for exact match, sorting and so on.

Modeling Your Data and Document Relations
[ 77 ]
The multi-fields mappings for title field is shown here:
\
QSPQFSUJFT\
title\
UZQFUFYU
"fields": {
FYBDU\
UZQFLFZXPSE
^
^
^
^
^
You can index the document as follows:
\
UJUMF-FBSOJOH&MBTUJDTFBSDI
^
With the multi fields mapping set, during indexing, Elasticsearch will internally index the
title field both as text and keyword. The keyword field can be accessible via UJUMFFYBDU.
Handling relations between different
document types
In the relational world, data is often divided into multiple tables and is linked using foreign
keys. To get the data, a join is used to combine data from one or more tables. But in the
NoSQL world, data is usually denormalized and stored as one big document. However, it is
often advantageous to store these documents separately. Data in Elasticsearch is immutable.
An update to an existing document means fetching the old document, applying the change,
and re-indexing it as a new document. The update is an expensive operation. If possible, we
have to keep the updates to a minimum.
For example, a blog article can have one or more comments, and an order can have one or
more line items. If we can separate the article and comment documents, we don't have to
update the article when there is a new comment. Elasticsearch provides two ways to handle
the relations between the documents. The first one is parent-child, and the second is nested.
Both have their pros and cons. First, let's discuss the parent-child document relation.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 78 ]
Note that Apache Lucene, the underlying data store, only stores data as
flat documents. Elasticsearch adds the abstraction above Lucene to handle
relationships between the documents.
Parent-child document relation
For parent-child document relations, the relationship between two different document
types is set by specifying the parent ID while indexing the child documents. Parent and
child document types are separate documents, both accessible using their IDs. We index
both the documents like any other regular documents, but when indexing the child
document, the parent ID is required. It is a one-to-many relationship. A parent document
can have multiple child documents, but a child document can only have one parent
document.
You should consider using parent-child document relations in the following cases:
Updates to parent and child documents are independent. For example, a new
comment can be added without updating the parent article
The child documents are updated more than the parent documents
Let's set the mappings for a blog application, you have article type, which is the parent, and
comment type, which is the child. The parent-child mapping between article and comment
can be set as shown next:
165CMPH
\
NBQQJOHT\
"article": { #Parent
QSPQFSUJFT\
UJUMF\
UZQFUFYU
^
DBUFHPSZ\
UZQFLFZXPSE
^
^
^
 "comment": { #Child
 "_parent": {
         "type": "article"
       },
QSPQFSUJFT\
DPNNFOU\

Modeling Your Data and Document Relations
[ 79 ]
UZQFUFYU
^
VTFSJE\
UZQFLFZXPSE
^
^
^
^
^
Once the mapping is set, documents can be indexed as follows:
1BSFOU
165CMPHBSUJDMF
\
UJUMF)FMMPXPSME
DBUFHPSZ*OUSPEVDUJPO
^
$IJMESFO
165CMPHDPNNFOUparent=1
\
DPNNFOU5IJTXPSMEJTBXFTPNF
VTFSJEVTFS
^
The article document (parent) is indexed like a regular document, but when indexing the
comment document (child), the ID of the article (parent) is specified.
Elasticsearch provides IBT@DIJME/IBT@QBSFOU to support querying parent-child. For
example, we can query all the articles that contain comments from a particular user, as
shown next:
1045CMPHBSUJDMF@TFBSDI
\
RVFSZ\
 "has_child":\
UZQFDPNNFOU
RVFSZ\
UFSN\
"userid": "user1"
^
^
^
^
^
www.ebook3000.com

Modeling Your Data and Document Relations
[ 80 ]
Don't worry if the preceding query doesn't make sense. We will discuss parent-child queries
in detail in the Handling document relations using parent-child section in $IBQUFS, More than
a search engine. Although the parent-child document relation seems very promising, queries
are very expensive as the Elasticsearch has to maintain the mapping between parent and
child documents in memory.
How are parent-child documents stored internally?
A child document is stored in the same shard as the parent document. By storing both the
documents on the same shard, Elasticsearch can avoid extra trips to fetch the data. When
indexing the child document, it uses the parent ID to find the shard where the parent
document exists. What makes parent-child queries expensive is managing the mapping
between parent and child documents. Elasticsearch stores the join information in what it
calls global-ordinals, which are lazily built the first time the query is executed.
Nested
We discussed how to set the mappings for nested documents in the nested data type section
before. When documents are stored as nested documents, they are stored as hidden
documents and are managed by Elasticsearch. But the disadvantage is that to update the
parent or the nested document, the entire document needs to be updated. For example, we
have the user document as shown here:
\
JE
OBNF6TFS
"address": [ #Nested
\
TUSFFU)JHI-BOF
DJUZ#JH$JUZ
^
\
TUSFFU-PX-BOF
DJUZ4NBMM$JUZ
^
^
To update parent document fields like OBNF or to update the nested document fields like
TUSFFU the entire user document needs to updated. Nested documents like address cannot
be accessed independently as they are hidden.

Modeling Your Data and Document Relations
[ 81 ]
Because of how nested documents are stored, queries on nested documents are much faster
when compared to parent-child. But, if you have to update child documents more
frequently than the parent document or vice versa, as the parent/child documents can be
updated independently, parent-child may be a better fit depending on the application
needs.
Elasticsearch supports querying nested documents using nested queries. We will talk about
nested queries in the Handling document relations using nested section in $IBQUFS, More than
a search engine.
Routing
We discussed before that an index contains one or more shards. During indexing, the
document ID is used to determine which shard the document belongs to, using a simple
formula as follows:
IBTI
EPDVNFOU@JEOP@PG@TIBSET
To retrieve a document using the document ID, the same formula is used to determine the
shard the document belongs to, and the document is retrieved:
When executing a search query, the node that receives the request is known as the
coordinating node. The coordinating node (Node2) sends the query to all the shards of the
index, aggregates the results, and sends them back to the client.
www.ebook3000.com

Modeling Your Data and Document Relations
[ 82 ]
By default, a query has to be executed on all the shards of the index. But if you have a way
to group similar data together, routing can be used to send the requests to a single shard
instead of all the shards in the index.
For example, you want to use Elasticsearch to power the order history of an e-commerce
site, the user should be able to query for his/her orders. We can use routing to store all the
orders that belong to the a user in the same shard. When querying for the orders that belong
to a user, Elasticsearch will use the routing value (user ID) to determine the shard and
execute the query only on a single shard. Without routing, the query needs to be executed
on all the shards.
Potential downfalls of using routing are hot spots. If some users in the system
are much bigger than others, there is a possibility that all the big users can
end up in the same shard. This leads to uneven distribution of data and
underutilization of resources and can potentially bring down the entire
cluster.
At the time of indexing, you should specify the ID on which the document should be
routed. Instead of using the document ID, the routing value is used to determine the shard.
For example, if all the orders are indexed using the user ID as the routing value, when
querying for orders placed by VTFS, the query needs to executed only on one shard.
The order mapping with routing can be configured as follows:
165DIBQUFS@NBQQJOHPSEFS
\
 "_routing": {
     "required": true
   },
QSPQFSUJFT\
PSEFS@JE\
UZQFUFYU
^
VTFS@JE\
UZQFLFZXPSE
^
PSEFS@UPUBM\
UZQFJOUFHFS
^
^
^

Modeling Your Data and Document Relations
[ 83 ]
Once the mapping is set, we can index the document and set the routing as follows:
165DIBQUFSPSEFSrouting=user1
\
PSEFS@JEZ
VTFS@JEVTFS
PSEFS@UPUBM
^
Note how we set the routing in the URL when we are indexing the order. If the routing
value is not specified, a routing exception is thrown. We can get the order document we just
indexed as shown here:
(&5DIBQUFSPSEFSrouting=user1
Since the routing value is used to determine the shard the document lives in, we have to
specify the routing value to retrieve the document using its ID.
Summary
In this chapter, you learned about various simple and complex data types Elasticsearch
supports. You also learned how to handle unstructured data using dynamic mappings. We
discussed how full-text search works and the difference between exact match and full-text
search. We discussed how to manage document relations. We also covered routing and how
it works.
In the next chapter, we will discuss how to index and update your data.
www.ebook3000.com

4
Indexing and Updating Your
Data
In the previous chapter, we discussed how to model your data. In this chapter, we will
discuss how to index and update your data. We will start by discussing how to index and
what happens when you index a document. Elasticsearch is a near real-time system,
meaning the data you index is available for search only after a small delay. We will discuss
the reason for this delay and how we can control the delay. This chapter will also show you
various ways to update your data and we will discuss what happens when you update a
document and why updates are expensive.
In this chapter, we will cover the following:
Indexing your data
Updating your data
Using Kibana to discover
Using Elasticsearch in your application
How Elasticsearch handles concurrency
How primary and replica shards interact
Indexing your data
Document are indexed using the index API. We can index a new person document into the
DIBQUFS index as shown here:
165DIBQUFSQFSTPO
\
JE

Indexing and Updating Your Data
[ 85 ]
OBNFVTFS
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
The document we just indexed is uniquely identified by the index, type and identifier. You
can either specify your identifier or let Elasticsearch pick one for you. If you want to specify
an identifier, you have to use the 165 HTTP method. If you use the 1045 HTTP method, a
unique identifier is automatically assigned to the document. The response to the preceding
command is shown as follows:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
@WFSTJPO
"result": "created",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
"created": true
^
An index operation creates the document if the document doesn't exist already. If a
document with same ID already exists, the contents of the document are replaced with the
request and the version is incremented. You will see a response as shown next:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
@WFSTJPO
"result": "updated",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
 "created": false
^
www.ebook3000.com

Indexing and Updating Your Data
[ 86 ]
You can see from the response that result is updated and created is set to false. You can also
tell Elasticsearch to only create by specifying the op type, as shown here:
165DIBQUFSQFSTPOop_type=create
\
JE
OBNFVTFS
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
If the document with the same ID already exists, the operation is rejected, and you will see
that a document already exists error in the response. The response is shown as follows:
\
FSSPS\
SPPU@DBVTF<
\
UZQFWFSTJPO@DPOGMJDU@FOHJOF@FYDFQUJPO
SFBTPO[person][1]: version conflictEPDVNFOUBMSFBEZFYJTUT

DVSSFOUWFSTJPO<>
JOEFY@VVJESDCV-;3UF%@(SH
TIBSE
JOEFYDIBQUFS
^
>
UZQFWFSTJPO@DPOGMJDU@FOHJOF@FYDFQUJPO
SFBTPO<QFSTPO><>WFSTJPODPOGMJDUEPDVNFOUBMSFBEZFYJTUT

DVSSFOUWFSTJPO<>
JOEFY@VVJESDCV-;3UF%@(SH
TIBSE
JOEFYDIBQUFS
^
 "status": 409
^
The response is sent with a $POGMJDU HTTP response code.
Indexing errors
Different kinds of errors can happen during indexing. In this section, we will list the most
common errors and how to handle them.

Indexing and Updating Your Data
[ 87 ]
Node/shards errors
Node/shards errors can occur if the node is not available or the shard is not allocated to a
node. Index and query responses contain a shards section, which will tell us the number of
the shards on which the operation is successful. Even though the operation is not successful
on all the shards, Elasticsearch will respond with partial results. For example, if you are 
executing a query on an index with two shards and one of the shards is not available, the
query response comes back with the results from just one shard. It is important to watch for
the number of shards the operation is successful from the response.
Let's index a person document as shown here:
165DIBQUFSQFSTPO
\
JE
OBNFVTFS
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
The response to the preceding query is shown as follows:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
@WFSTJPO
SFTVMUDSFBUFE
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
DSFBUFEUSVF
^
You can see from the response that the total number of shards is , and the indexing is only
successful on  shard. In the preceding example, our cluster has only one node. When an
index is created, by default, it is created with one replica. The primary shard and the replica
of a shard cannot be allocated on the same node. As the cluster has only one node, the
replica shard couldn't be assigned to a node in the cluster, and the indexing operation is
successful only on the primary shard.
www.ebook3000.com

Indexing and Updating Your Data
[ 88 ]
Serialization/mapping errors
These kinds of errors happen due to JSON serialization issues or if you are trying to index
an integer into a string field. For example, you are trying to index a person document, and
the mapping for the age field is set to an integer. If you try to index POF as the age field as
shown next, you will see a mapping error:
165DIBQUFSQFSTPO
\
JE
OBNFVTFS
"age": "one",
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
The response to the preceding request is shown next:
\
FSSPS\
SPPU@DBVTF<
\
UZQFNBQQFS@QBSTJOH@FYDFQUJPO
SFBTPOGBJMFEUPQBSTF<BHF>
^
>
UZQFNBQQFS@QBSTJOH@FYDFQUJPO
SFBTPOGBJMFEUPQBSTF<BHF>
DBVTFE@CZ\
 "type": "number_format_exception",
       "reason": "For input string: \"one\""
^
^
TUBUVT
^
Thread pool rejection error
Elasticsearch has a thread pool for index, search, bulk, refresh and so on. If the thread pool
for indexing is full, Elasticsearch will reject the indexing operation. If you get this error
occasionally, you can add application logic to retry the index operation. If you are getting
this error frequently, you should consider getting better hardware or increase the thread
pool size.

Indexing and Updating Your Data
[ 89 ]
The thread pool size is based on the number of CPU processors available in the node.
Elasticsearch doesn't recommend changing the default thread pool size unless you know
what you are doing.
Managing an index
We discussed in $IBQUFS, Modeling your data and document relations, how we could index a
document without creating the index or setting the mapping first. Elasticsearch
automatically creates the index with the default settings and uses dynamic mapping to
figure out the mapping. In the previous section, we indexed a document into the DIBQUFS
index. Let's inspect the default settings:
(&5DIBQUFS@TFUUJOHT
You will see a response as follows:
\
DIBQUFS\
TFUUJOHT\
JOEFY\
DSFBUJPO@EBUF
 "number_of_shards": "5",
         "number_of_replicas": "1",
VVJEHW4D9:K,27N4)T:VC@3CH
WFSTJPO\
DSFBUFE
^
QSPWJEFE@OBNFDIBQUFS
^
^
^
^
You can see from the preceding response that the index has been created with the default
settings of  shards and  replica meaning 5 primary shards and 5 replica shards. If you
need to change the default settings, you have to delete the existing index and recreate the
index. Let's delete the DIBQUFS index so that we can recreate it. You can delete the
DIBQUFS index as shown next:
%&-&5&DIBQUFS
www.ebook3000.com

Indexing and Updating Your Data
[ 90 ]
Note that the HTTP method is %&-&5&. This single command deletes the
index, which cannot be undone. Please double check the index name
before running the delete command.
Let's recreate the DIBQUFS index with  shards and  replicas:
165DIBQUFS
\
TFUUJOHT\
JOEFY\
 "number_of_shards": "3",
     "number_of_replicas": "1"
^
^
^
Now, let's verify the index settings:
(&5DIBQUFS@TFUUJOHT
You will see the response as follows:
\
DIBQUFS\
TFUUJOHT\
JOEFY\
DSFBUJPO@EBUF
 "number_of_shards": "3",
         "number_of_replicas": "1",
VVJEF#S9P.5HZ[SZ+3MX(H
WFSTJPO\
DSFBUFE
^
QSPWJEFE@OBNFDIBQUFS
^
^
^
^
Once the index is created, the number of shards in the index cannot be changed. If you want
to increase or decrease the number of shards you have to create a new index with new
settings and re-index the data. Starting Elasticsearch 5.0, you can use the reindex API to
recreate an index with different index configurations. We will discuss the reindex API in
detail in $IBQUFS, Organizing Your Data and Bulk Data Ingestion.

Indexing and Updating Your Data
[ 91 ]
Unlike the number of shards, the number of replicas can be increased or decreased on the
fly as follows:
165DIBQUFS@TFUUJOHT
\
JOEFY\
"number_of_replicas": 2
^
^
You should see an acknowledged response as follows:
\BDLOPXMFEHFEUSVF^
As shown in the preceding response, Elasticsearch accepted the request and will start
working on replicating the shards in the background. We created the index with the
required shard configuration. Next, let's set mappings for the DIBQUFS index, QSPEVDU
type as shown here:
165DIBQUFS@NBQQJOHproduct
\
QSPQFSUJFT\
JE\
UZQFJOUFHFS
^
OBNF\
UZQFUFYU
^
BHF\
UZQFJOUFHFS
^
HFOEFS\
UZQFLFZXPSE
^
FNBJM\
UZQFLFZXPSE
^
MBTU@NPEJGJFE@EBUF\
UZQFEBUF
^
^
^
We can also set both the TFUUJOHT and NBQQJOHT while creating the index as shown next:
165DIBQUFS
\
TFUUJOHT\
www.ebook3000.com

Indexing and Updating Your Data
[ 92 ]
JOEFY\
OVNCFS@PG@TIBSET
OVNCFS@PG@SFQMJDBT
^
^
NBQQJOHT\
QSPEVDU\
QSPQFSUJFT\
JE\
UZQFJOUFHFS
^
OBNF\
UZQFUFYU
^
BHF\
UZQFJOUFHFS
^
HFOEFS\
UZQFLFZXPSE
^
FNBJM\
UZQFLFZXPSE
^
MBTU@NPEJGJFE@EBUF\
UZQFEBUF
^
^
^
^
^
Now we can index the documents, and there shouldn't be any surprises.
What happens when you index a document?
In this section, we will discuss what happens internally when you index a document. An
Elasticsearch index is nothing but a collection of shards. Each shard, as we discussed before,
is an Apache Lucene index. To be able to search for the documents, the fields in the
documents are analyzed and stored in an inverted index. Unlike SQL databases,
Elasticsearch is a near real-time search engine, meaning the documents you index are only
available after a small delay. The default is  sec. By the end of this section, it will be clear
why there is a delay and how we can control the delay.

Indexing and Updating Your Data
[ 93 ]
First, let's recreate the DIBQUFS index with  shards and  replica:
%&-&5&DIBQUFS
165DIBQUFS
\
TFUUJOHT\
JOEFY\
OVNCFS@PG@TIBSET
OVNCFS@PG@SFQMJDBT
^
^
^
Let's index a document into the DIBQUFS index we just created:
165DIBQUFSQFSTPO
\
JE
OBNFVTFS
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
Since an index can have one more shard, Elasticsearch first determines the shard the
document belongs to using the IBTI
EPDVNFOU@JEOVNCFS@PG@TIBSET formula.
When retrieving the document by its JE, the same formula is used to determine the shard
and fetch the document.
Just like an Elasticsearch index is made up of multiple shards, a shard (Lucene index) is
made up of multiple segments (s1, s2, s3) as shown next. The following diagram represents
the internals of a shard:
www.ebook3000.com

Indexing and Updating Your Data
[ 94 ]
When you index a document into the Lucene index (shard), the document is first written to
an in-memory buffer. A process known as SFGSFTI wakes up on a schedule and reads the
documents from the in-memory buffer and creates a new TFHNFOU. In the preceding
diagram, the next time the Lucene index is refreshed, a new 4 segment is created, which
contains documents % and %.
Each Lucene segment is an independent index that can be searched. As new documents are
added, the refresh process reads the documents from the in-memory buffer and creates new
segments. The segment contains the inverted index and other information required for
searching the documents. When a new segment is created, it is first written to the file
system cache and committed to the physical disk when certain conditions are met.
File system cache is a system memory where a file read from the physical
disk is cached. The next read operation reads the file directly from
memory. File system cache is a cache between processes and the physical
disk. Any writes to the file are written to cache and not the physical disk.
At regular intervals, the data in the file system cache is written to the disk.
In this way, the operating system optimizes the cost of reading and witting
from the physical disk.
By default, the refresh interval is  second, due to which the documents indexed are only
available for search after  second. If you need a document to be searchable immediately
after you index, you can set SFGSFTI to USVF, as shown next:
165DIBQUFSQFSTPOrefresh=true
\
JE
OBNF6TFS
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
The preceding command will index and refresh so that the document is visible for search
immediately. You can also manually refresh the entire DIBQUFS index as shown next:
1045DIBQUFS@SFGSFTI
You can refresh all indexes as follows:
1045@SFGSFTI

Indexing and Updating Your Data
[ 95 ]
Refresh is a costly operation, and depending on whether you need real-time search, you can
increase or decrease the refresh interval. If your data doesn't have to be searchable
immediately after you index, you should consider increasing the refresh interval. For
example, we set the refresh interval to T, the data indexed into the DIBQUFS index is
only searchable after 30 seconds. We can increase the refresh interval for the DIBQUFS
index to  seconds as shown next :
165DIBQUFS@TFUUJOHT
\
JOEFY\
"refresh_interval": "30s"
^
^
During bulk indexing, you can disable refresh temporarily to increase the indexing
performance. Refresh can be disabled by setting SFGSFTI@JOUFSWBM to . After the
indexing is done, you can set the SFGSFTI@JOUFSWBM back to what it was before.
Updating your data
In this section, we will discuss different ways of updating existing documents. Internally, an
update is always a delete and re-index. You can update using the entire document
(replacing the original document), or update a single field or add a new field, or update a
field using scripts, such as incrementing a counter.
Update using an entire document
When you index a document with the existing document ID, it will replace the current
document with the new document. As shown next, we can update the document ID  using
the entire document:
165DIBQUFSQFSTPO
\
JE
"name": "name update 1",
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
www.ebook3000.com

Indexing and Updating Your Data
[ 96 ]
The response is as follows:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
"_version": 2,
 "result": "updated",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
"created": false
^
You can see from the response that the result of the operation is VQEBUFE, version is , and
created is GBMTF.
Partial updates
In the previous section, we discussed how to update using the entire document. In this
section, we will discuss how to update only one or two fields in the document. Elasticsearch
provides the update API to update an existing document partially. The update API first
retrieves the old document, then uses the @TPVSDF of the existing documents to apply the
changes, deletes the old document, and indexes the document as a new document. The
fields to be updated are specified in the doc field of the request.
For partial updates to work, @TPVSDF needs to enabled.
Let's say we want to update just the name of the person and not worry about any other field
in the document. We will use the update API to update the existing document as shown
next:
1045DIBQUFSQFSTPO_update
\
EPD\
OBNFOBNFVEQBUF
^
^

Indexing and Updating Your Data
[ 97 ]
The response is as follows:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
"_version": 3,
"result": "updated",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
^
You can see from the response that the result of the operation is VQEBUFE, version is .
Scripted updates
Scripted updates will come in handy when you want to update a document based on a
condition. Without a way to script, you have to first retrieve the document, check the
conditions on the document, apply the changes, and re-index the document. Partial updates
retrieve the document internally from the shard, recursively apply the changes, and re-
index the documents avoiding the network round trips.
Scripted updates can also be used to increment a counter. The application doesn't have to
worry about the current value. We can increment the age field in the person document
using script as shown next:
1045DIBQUFSQFSTPO_update
\
"script": "ctx._source.age+=1"
^
Elasticsearch supports many scripting languages to execute inline scripts, the default
scripting language is Painless. Let's say we want to classify a document we indexed before
into adults and teenagers. We can use an inline script as shown next to check the person's
age and add a new field called QFSTPO@UZQF. The following command will update a person
document based on a script:
1045DIBQUFSQFSTPO@VQEBUF
\
TDSJQU\
 "inline": "if (ctx._source.age > params.age) { ctx._source.person_type
= 'adult' } else { ctx._source.person_type = 'teenager' }",
www.ebook3000.com

Indexing and Updating Your Data
[ 98 ]
     "params": {
       "age": 19
     }
^
^
Now, let's retrieve the person document with ID 2. The response is as follows:
\
JE
OBNFOBNFVQEBUF
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
"person_type": "adult"
^
You can see from the response that a new QFSTPO@UZQF field is added to the document.
Upsert
We discussed in the previous section that if you want to update only a few fields in the
document, you can do so by specifying the fields in the doc field of the request. When
partially updating a document, if the document doesn't already exist, the update will fail. If
you want to create a new document, if the document doesn't exist, you can set the
EPD@BT@VQTFSU flag to true. Setting the upsert to true will create a new document with the
fields in the doc field. Let's take an example:
1045DIBQUFSQFSTPO@VQEBUF
\
EPD\
OBNFVTFS
^
 "doc_as_upsert": true
^
In the preceding example, since the person document with ID  doesn't exist, a new
document with fields in the doc is created. Now, let's retrieve the person document with ID
. The response is as follows:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE

Indexing and Updating Your Data
[ 99 ]
@WFSTJPO
GPVOEUSVF
"_source": {
     "name": "user3"
   }
^
You can see from the response that the new document only contains the name field.
NOOP
We updated the name of the person document ID  to OBNFVQEBUF in the previous
section, as shown here:
1045DIBQUFSQFSTPO@VQEBUF
\
EPD\
 "name": "name update 2"
^
^
If you try to run the update again, the operation is ignored as there is no change. The
response will contain the result as OPPQ, as shown here:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
@WFSTJPO
"result": "noop",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
^
You can disable this behavior by setting the detect_noop to false:
1045DIBQUFSQFSTPO@VQEBUF
\
EPD\
OBNFOBNFVQEBUF
^
"detect_noop" : "false"
^
www.ebook3000.com

Indexing and Updating Your Data
[ 100 ]
With EFUFDU@OPPQ set to GBMTF, it will update the document no matter what. The response
is as follows:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
"_version": 2,
"result": "updated",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
^
What happens when you update a document?
In the previous section, we discussed what happens when you index a document. In this
section, we will discuss how document updates are handled and why an update is an
expensive operation.
As discussed before, when a document is indexed, it's stored in a segment. By design, a
segment once created cannot be changed. Being immutable offers several advantages. For
example, once the segment file is read into the file system cache, it can live there forever as
it doesn't change, and Lucene doesn't have to worry about locking the file for any changes.
But if a segment cannot be modified, how can we update an existing document? To perform
an update, first the existing document is soft deleted, and the updated document is indexed
as a new document.
As the data in the segments cannot be changed, delete operation is a soft
delete. The ID of the document is recorded into a file that tracks the list of
deleted documents.
Merging segments
By default, the refresh process creates a new segment every second. This will result in the
creation of lots of segments. As a search on the shard has to go through all the segments in
the shard, having lots of segments will slow down the search performance.

Indexing and Updating Your Data
[ 101 ]
Segments also need lots of resources, such as file handlers, CPU, disk space, and memory.
Since segments are immutable, any updates and deletes are only soft deleted due to which
we will eventually run out of disk space. To decrease the number of segments, Lucene
merges the segments of similar size into a bigger segment. While merging the segments, the
documents that are marked as deleted are not copied to the merged segment. Until the
segments merge, the document is not physically removed from the disk:
While the old segments are merged into a bigger segment, search requests are still served
from the old segments. Once the merge is complete, new segments can serve the search
requests and old segments are removed from the disk. The more the updates, the more
frequently segments are merged. Merging involves CPU, memory, and I/O resources due to
which updates are expensive.
Using Kibana to discover
Kibana UI makes it very easy to explore and visualize your data. You can add filters based
on the fields in the document in click of a button and inspect the document source. For each
field, you can see the top five values and their field value statistics. In this section, we will
discuss how to explore using Kibana. Open Kibana by going to IUUQMPDBMIPTU.
Before you can use an index in Kibana, you have to tell Kibana to use an index name or an
index pattern. To do this, perform the following steps:
Go to the Management tab on the left.
1.
Select Index Patterns
2.
In the Index Patterns page, select the +Add New button.
3.
www.ebook3000.com

Indexing and Updating Your Data
[ 102 ]
Input the index name as DIBQUFS, and select the time-field name as
4.
MBTU@NPEJGJFE@EBUF.
Click on Create.
5.
Now go to the Discover tab on the left navigation bar, and you should see a screen as
shown here:
Since we configured the time-field name as MBTU@NPEJGJFE@EBUF, a histogram with the
distribution of the documents is shown. You can also add filters for the fields in the
document. To filter the documents that contain gender FRVBM to ., perform the following
steps:
Click on gender in the field list on the left. The top five values for the field are
displayed.
Click on the Positive Filter icon (magnifying glass with the  symbol).
The documents are now filtered, and gender . is highlighted in the source of the
document.
Kibana, by default, adds a date time filter for the last 15 minutes. In the
preceding example, since MBTU@NPEJGJFE@EBUF is set as the time field, it
adds the filter for MBTU@NPEJGJFE@EBUF OPXN to all the requests.
If you cannot see any documents, that is because of the date time filter.
You can change the time range by clicking on the clock icon in the top

Indexing and Updating Your Data
[ 103 ]
right corner.
www.ebook3000.com

Indexing and Updating Your Data
[ 104 ]
You should see a screen similar to this:
You can change the time range by selecting the clock icon in the top right corner. You can
also sort the documents by the values in the field by clicking on the field name in the
documents table header.
Using Elasticsearch in your application
Until this chapter, we used HTTP clients, such as Kibana or Postman, to talk to
Elasticsearch. In this section, we will discuss how to integrate Elasticsearch into your
application.
You can talk to Elasticsearch in two ways:
Rest API: You can use any HTTP clients, such as Apache HTTP client or Jersey to
interact with the REST API. If your application is Java based, you should consider
using the Elasticsearch Java REST Client. We will discuss this in more detail in
the Java REST Client section.
Native Client: Elasticsearch has a native client for almost all major languages,
such as Java, .NET, PHP, and so on. There are also many community-supported
clients. For more details about the officially supported clients, please visit IUUQT
XXXFMBTUJDDPHVJEFFOFMBTUJDTFBSDIDMJFOUJOEFYIUNM.
For Java, along with the REST client, Elasticsearch also supports transport and node clients.
Transport and node clients use the TCP protocol for communications, other clients use
HTTP to communicate with the cluster.

Indexing and Updating Your Data
[ 105 ]
Official clients provide more than just a way to interact with Elasticsearch. They also offer
the following:
Round robins the requests between all the nodes in the cluster.
Automatic node discovery (sniffing).
Automatic failover in case of node failure.
Connection pooling/persistence. Opening a new TCP connection for every
request is very expensive, and the connection polling significantly improves the
performance.
Trace-level logging. When the trace level is turned on, every request and
response is logged, which is great for debugging purposes.
Java
As you know Elasticsearch is a Java-based application, and along with the REST client, it
also supports two native clients (Transport and Node). Like anything else, there are
pros/cons while choosing one over the other. The easiest to get started with is the transport
client as it would come with the request/response builders. We will discuss further details
in the following sections.
The following are the choices for a Java-based application:
Transport client
Node client
Rest client
Third party clients
Transport client
The transport client connects to Elasticsearch using the transport protocol, a binary protocol
used internally for communication between the nodes. The transport client handles
serialization of the data with very little or almost no overhead. When using the transport
client, it is recommended to have both the client and the server on the same Elasticsearch
and JVM versions. Along with handling the request/response, it also takes care of
discovering the cluster state and load balancing the requests between all the nodes in the
cluster.
www.ebook3000.com

Indexing and Updating Your Data
[ 106 ]
%FQFOEFODJFT
You can add the transport client to your application using Maven or Gradle:
EFQFOEFODZ 
HSPVQ*E PSHFMBTUJDTFBSDIDMJFOUHSPVQ*E 
BSUJGBDU*E USBOTQPSUBSUJGBDU*E 
WFSTJPO WFSTJPO 
EFQFOEFODZ 
As shown in this code snippet, you can include the latest version of Elasticsearch transport
client dependency in your QPNYNM file.
The Gradle version is as follows:
EFQFOEFODJFT\
DPNQJMFHSPVQ	PSHFMBTUJDTFBSDIDMJFOU	OBNF	USBOTQPSU	WFSTJPO
		
^
You also need to add log4j2 dependencies:
EFQFOEFODZ 
HSPVQ*E PSHBQBDIFMPHHJOHMPHKHSPVQ*E 
BSUJGBDU*E MPHKBQJBSUJGBDU*E 
WFSTJPO WFSTJPO 
EFQFOEFODZ 
EFQFOEFODZ 
HSPVQ*E PSHBQBDIFMPHHJOHMPHKHSPVQ*E 
BSUJGBDU*E MPHKDPSFBSUJGBDU*E 
WFSTJPO WFSTJPO 
EFQFOEFODZ 
*OJUJBMJ[JOHUIFDMJFOU
Once you have added the transport client dependency, you can initialize the client as
shown here:
5SBOTQPSU"EESFTTOPEF
OFX*OFU4PDLFU5SBOTQPSU"EESFTT
*OFU"EESFTTHFU#Z/BNF
OPEF
5SBOTQPSU"EESFTTOPEF
OFX*OFU4PDLFU5SBOTQPSU"EESFTT
*OFU"EESFTTHFU#Z/BNF
OPEF
4FUUJOHTTFUUJOH4FUUJOHTCVJMEFS
QVU
DMVTUFSOBNF
FMBTUJDTFBSDICVJME

5SBOTQPSU$MJFOUDMJFOUOFX1SF#VJMU5SBOTQPSU$MJFOU
TFUUJOHT

Indexing and Updating Your Data
[ 107 ]
"EEUIFLOPXOOPEFTUPUIFDMJFOU
DMJFOUBEE5SBOTQPSU"EESFTT
OPEF
DMJFOUBEE5SBOTQPSU"EESFTT
OPEF
To initialize the client, you need the following:
Cluster name
IP address of the nodes in the clusters
Port numbers if Elasticsearch is not running on the defaults ports (Note that the
port used is 9300 instead of 9200)
4OJ`OH
In the preceding example, we initialized the client with OPEF and OPEF. By default, the
transport client will round robin the requests between the nodes provided during the client
initialization. The transport client can also be configured to automatically get the list of
other nodes in the cluster. If nodes are added/removed from the cluster, transport client can
automatically discover the cluster state. Periodically, it will refresh the list of available
nodes and exclude the faulty nodes. When initializing the client, this can be enabled by
setting the DMJFOUUSBOTQPSUTOJGG to USVF.
When sniffing is enabled, the client calls the cluster state API on a schedule to get the cluster
state. The cluster state is refreshed automatically every TFD by default. Sniffing will make
sure your application can still run despite node failures. You can enable client sniffing as
shown next:
5SBOTQPSU"EESFTTOPEF
OFX*OFU4PDLFU5SBOTQPSU"EESFTT
*OFU"EESFTTHFU#Z/BNF
OPEF
5SBOTQPSU"EESFTTOPEF
OFX*OFU4PDLFU5SBOTQPSU"EESFTT
*OFU"EESFTTHFU#Z/BNF
OPEF
4FUUJOHTTFUUJOH4FUUJOHTCVJMEFS

QVU
DMVTUFSOBNFFMBTUJDTFBSDI
put("client.transport.sniff", true)
CVJME

5SBOTQPSU$MJFOUDMJFOUOFX1SF#VJMU5SBOTQPSU$MJFOU
TFUUJOHT
"EEUIFLOPXOOPEFTUPUIFDMJFOU
DMJFOUBEE5SBOTQPSU"EESFTT
OPEF
DMJFOUBEE5SBOTQPSU"EESFTT
OPEF
www.ebook3000.com

Indexing and Updating Your Data
[ 108 ]
The disadvantages of using the transport client are as follows:
Being an internal protocol, you need to use the same Elasticsearch version on
both the client and the server.
You might also have to deal with any dependency version mismatches between
Elasticsearch dependencies, such as log4j, Jackson, Joda, and your application.
The transport client comes with the entire Elasticsearch dependencies, such as
Apache Lucene, and so on.
The advantages of using the transport client are as follows:
It uses TCP to communicate with the cluster. TCP being a low-level protocol than
HTTP, the performance is better when using TCP.
It comes with the complete request and response builders. No client
serialization/deserialization of request/response is required.
It offers connection pooling/persistence. Creating a new TCP connection for every
request is very expensive.
Node client
Similar to transport client, node client is a native client that uses a binary protocol to
communicate with Elasticsearch. The difference is when using a node client, the node client
is added as one of the nodes to the cluster. And since the client is part of the cluster, it can
route the request to the correct nodes directly, saving a few network hopes. Since the node
client acts as one of the nodes in the cluster, other nodes ping the node client. This is a
problem if you have a large cluster. Unless your performance benchmarks says the node
client is better, it is recommended to use the transport or REST client.
REST client
Using transport client means you need to have all the Elasticsearch dependencies in your
application and deal with any library version conflicts between Elasticsearch and your
application. Starting Elasticsearch 5.0, a new REST client is introduced to solve this
problem. The REST client uses the Apache HTTP client internally. The REST client, apart
from calling the Elasticsearch REST API, also takes care of discovering other nodes in the
cluster, failure handling, and a lot more.

Indexing and Updating Your Data
[ 109 ]
You can add the REST client dependency to your application using Maven or Gradle. The
following is an example of using Maven:
EFQFOEFODZ 
HSPVQ*E PSHFMBTUJDTFBSDIDMJFOUHSPVQ*E 
BSUJGBDU*E SFTUBSUJGBDU*E 
WFSTJPO WFSTJPO 
EFQFOEFODZ 
You can include the latest version of Elasticsearch REST client dependency in your QPNYNM
file.
The following is an example of using Gradle:
EFQFOEFODJFT\
DPNQJMFHSPVQ	PSHFMBTUJDTFBSDIDMJFOU	OBNF	SFTU	WFSTJPO
		
^
You can initialize the REST client using the RestClient Builder as shown here:
3FTU$MJFOUDMJFOU3FTU$MJFOUCVJMEFS
OFX)UUQ)PTU
IPTUIUUQ
OFX)UUQ)PTU
IPTUIUUQ
CVJME

Note that the port number is  since the REST client uses HTTP to communicate with
the cluster.
Sniffing is one of the important features Elasticsearch clients offer. When sniffing is enabled,
other nodes in the cluster are automatically discovered. Please refer to the Sniffing section
for more details. It is provided as a library and can be included in your application as a
Maven dependency:
EFQFOEFODZ 
HSPVQ*E PSHFMBTUJDTFBSDIDMJFOUHSPVQ*E 
BSUJGBDU*E TOJGGFSBSUJGBDU*E 
WFSTJPO WFSTJPO 
EFQFOEFODZ 
You can configure to run sniffing every  minute as shown here:
4OJGGFSTOJGGFS
4OJGGFSCVJMEFS
SFTU$MJFOUTFU4OJGG*OUFSWBM.JMMT
CVJME

www.ebook3000.com

Indexing and Updating Your Data
[ 110 ]
When sniffing is enabled, it automatically gets the list of available nodes in the cluster and
round robins the requests to all available data nodes. The REST client is very lightweight; it
doesn't come with Elasticsearch dependencies and doesn't support request/response
builders. The application using the REST client has to build the JSON request and handle
the serialization/deserialization of request/response.
Third party clients
You can also use third-party HTTP clients, such as +FTU, which support request/response
builders and support node discovery as new nodes are added or existing nodes are
removed. For more details, refer to the Jest GitHub page here:
IUUQTHJUIVCDPNTFBSDICPYJP+FTUUSFFNBTUFSKFTU
Indexing using Java client
Once the client is initialized, we will use the index API to index a JSON document. In this
example, we will be using the transport client to index the document. To index, we have to
use the QSFQBSF*OEFY method. All the operations using the client are asynchronous and
will return a future:
'VUVSF*OEFY3FTQPOTF GVUVSF
DMJFOUQSFQBSF*OEFY
DIBQUFSQFSTPOTFU4PVSDF
EPDVNFOU
To make the operation synchronous, we can call the GET method as shown here:
*OEFY3FTQPOTFSFTQPOTFDMJFOUQSFQBSF*OEFY
DIBQUFSQFSTPO
TFU4PVSDF
EPDVNFOUHFU

The following are the details of the preceding example:
Index: chapter4
Type: person
Document ID: 1
The document to be indexed is set using the preceding TFU4PVSDF method. The easiest and
most practical way to generate JSON is to use a third-party library, such as Jackson or Gson.
You can add Jackson to your application using Maven dependency as shown next:
EFQFOEFODZ 
HSPVQ*E DPNGBTUFSYNMKBDLTPODPSFHSPVQ*E 
BSUJGBDU*E KBDLTPOEBUBCJOEBSUJGBDU*E 
WFSTJPO WFSTJPO 

Indexing and Updating Your Data
[ 111 ]
EFQFOEFODZ 
The following is the Java POJO class representing the person document we want to index:
QVCMJDDMBTT1FSTPO\
QSJWBUFJOUJE
QSJWBUF4USJOHOBNF
QSJWBUFJOUBHF
QSJWBUF4USJOHHFOEFS
QSJWBUF4USJOHFNBJM
QSJWBUF4USJOHMBTU.PEJGJFE%BUF
HFUUFSTBOETFUUFST
^
We will use +BDLTPO0CKFDU.BQQFS to serialize the person object to JSON, as shown here:
0CKFDU.BQQFSNBQQFSOFX0CKFDU.BQQFS

$POWFSUUIFPCKFDUUPCZUFT
CZUF<>EPDVNFOUNBQQFSXSJUF7BMVF"T#ZUFT
QFSTPO0CK
*OEFYDIBQUFS
5ZQFQFSTPO
%PD*E
*OEFY3FTQPOTFSFTQPOTFDMJFOUQSFQBSF*OEFY
DIBQUFSQFSTPO
TFU4PVSDF
EPDVNFOUHFU

The document to be indexed can be passed to TFU4PVSDF as the following:
String
Bytes array
Map
We can also pass a Java Map to TFU4PVSDF, which will be converted to JSON during
indexing:
.BQ4USJOH0CKFDU EPDVNFOUOFX)BTI.BQ4USJOH0CKFDU 

EPDVNFOUQVU
JE
EPDVNFOUQVU
OBNFVTFS
EPDVNFOUQVU
BHF
EPDVNFOUQVU
HFOEFS.
EPDVNFOUQVU
FNBJMVTFS!HNBJMDPN
EPDVNFOUQVU
MBTU@NPEJGJFE@EBUFOFX%BUF

*OEFY3FTQPOTFSFTQPOTFDMJFOUQSFQBSF*OEFY
DIBQUFSQFSTPO
TFU4PVSDF
EPDVNFOUHFU

www.ebook3000.com

Indexing and Updating Your Data
[ 112 ]
We can also use the Elasticsearch helper 9$POUFOU#VJMEFS to build the JSON document as
shown next:
JNQPSUTUBUJDPSHFMBTUJDTFBSDIDPNNPOYDPOUFOU9$POUFOU'BDUPSZ
9$POUFOU#VJMEFSCVJMEFSKTPO#VJMEFS

TUBSU0CKFDU

GJFME
JE
GJFME
OBNFVTFS
GJFME
BHF
GJFME
HFOEFS.
GJFME
FNBJMVTFS!HNBJMDPN
GJFME
MBTU@NPEJGJFE@EBUFOFX%BUF

FOE0CKFDU

4USJOHEPDVNFOUCVJMEFSTUSJOH

*OEFY3FTQPOTFSFTQPOTFDMJFOUQSFQBSF*OEFY
DIBQUFSQFSTPO
TFU4PVSDF
EPDVNFOUHFU

You can check the *OEFY3FTQPOTF to make sure there are no failures. As shown next, we
can check the ID of the document in the index response to verify that the document is
successfully indexed:
JG
SFTQPOTFHFU*E
OVMM\
TVDDFTT
^
We discussed the different types of indexing errors that can happen in the Indexing Errors
section.
Concurrency
We discussed before that an update operation has to first retrieve the old document, apply
the changes, and re-index the document. Between retrieving the old document and re-
indexing the document, if some other operation updates the document, you would
potentially overwrite the change. To solve this problem, Elasticsearch increments the
version of the document on each operation.
If the version of the document has been changed between the document retrieval and re-
indexing, the index operation fails. Let's take an example:
1045DIBQUFSQFSTPO@VQEBUF
\
EPD\

Indexing and Updating Your Data
[ 113 ]
OBNFOBNFVQEBUF
^
^
The response to the preceding operation is as follows:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
 "_version": 4,
"result": "updated",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
^
You can see from the preceding response that the version of the document is after the
update operation. For the next update, we can specify the version:
165DIBQUFSQFSTPOversion=4
\
JE
OBNFOBNFVQEBUF
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
The operation only succeeds if the current version of the document is . You can see from
the following response that the document has been updated and the version of the
document is now :
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
"_version": 5,
 "result": "updated",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
^
www.ebook3000.com

Indexing and Updating Your Data
[ 114 ]
If you try running the same update with version , you would see a response )551
SFTQPOTFDPEF, as shown here:
\
FSSPS\
SPPU@DBVTF<
\
UZQFWFSTJPO@DPOGMJDU@FOHJOF@FYDFQUJPO
SFBTPO<QFSTPO><>version conflict, current version [5] is
different than the        one provided [4]",
JOEFY@VVJESDCV-;3UF%@(SH
TIBSE
JOEFYDIBQUFS
^
>
UZQFWFSTJPO@DPOGMJDU@FOHJOF@FYDFQUJPO
SFBTPO<QFSTPO><>WFSTJPODPOGMJDUDVSSFOUWFSTJPO<>JT
EJGGFSFOUUIBOUIFPOFQSPWJEFE<>
JOEFY@VVJESDCV-;3UF%@(SH
TIBSE
JOEFYDIBQUFS
^
 "status": 409
^
To avoid failure due to a conflict, you can ask Elasticsearch to retry. The number of times it
has to retry before the operation fails can be specified via the SFUSZ@PO@DPOGMJDU URL
parameter, as shown next:
1045DIBQUFSQFSTPO@VQEBUFretry_on_conflict=3
\
EPD\
OBNFOBNFVQEBUF
^
^
In a version conflict, Elasticsearch will try for  times before it fails the operation.
Translog
In the What happens when you index a document section, we discussed that when you index a
document, the refresh process creates a new segment. Since writing the segment to the disk
on every refresh is very expensive, the segment is only written to in-memory file system
cache. When certain conditions are met, a process known as Lucene commit writes all the
files (segments) in the memory to a physical disk.

Indexing and Updating Your Data
[ 115 ]
If a node crashes before the files in memory are persisted to the physical disk, the data in
the file system cache is lost, and any uncommitted changes are also lost. But Lucene commit
is very expensive and cannot be done after every operation. To solve this problem,
Elasticsearch introduced transaction log, which is a write-ahead log:
During an index or delete operation, the document is written to both the memory buffer
and the transaction log. The index/delete operation is only acknowledged after the data in
the translog is persisted to disk. If the system crashes before the segments are persisted to
the disk, they are replayed from the translog on the server restart. When the translog
becomes too big and certain other conditions are met, a new translog is created
automatically. This process is known as the flush.
Async versus sync
By default, the index operation is only acknowledged after the translog is committed to disk
in the primary and replica shards. If you are performing a bulk index and don't mind losing
a few seconds of data in case of a failure, you can set the translog to committed on an
interval rather than on every index operation. Changing the translog to async greatly
improves the index performance.
CRUD from translog
We discussed in the previous section that the data you index is only available for search
after one second. That's because the refresh process runs once a second and makes the data
searchable by writing it to the inverted index. But when you retrieve a document by its ID,
you will always get the latest data.
www.ebook3000.com

Indexing and Updating Your Data
[ 116 ]
When you retrieve a document by its ID, Elasticsearch will check the translog to see if the
translog contains the latest data that is not committed to the inverted index yet.
Although documents are searchable only after the refresh process runs, when a document is
retrieved using its ID, you will always get the latest data.
Primary and Replica shards
As you know the data in an index is split across one or more shards. By splitting your data
across multiple shards, Elasticsearch can scale beyond what a single machine can do.
Elasticsearch is a distributed system, and system failures are bound to happen. Since each
shard is an independent Lucene index that can live on any node in the cluster, Elasticsearch
provides a way to maintain a copy of the primary shard in a different node of the cluster. In
case the node containing the primary shard fails, the replica shard (copy), which exists in a
different node, is promoted to primary. For more information, please refer to the Failure
Handling section in $IBQUFS, Introduction to Elasticsearch.
In this section, we will talk about how the data between primary and replica is
synchronized:
Let's say we have a cluster of two nodes as shown in the preceding figure. The shards
represented by the solid lines are primary shards, and the shards represented by the dotted
lines are replicas. We are indexing a document into the DIBQUFS index and the QFSTPO
type, as shown here:
165IUUQOPEFDIBQUFSQFSTPO
\

Indexing and Updating Your Data
[ 117 ]
JE
OBNFVTFS
BHF
HFOEFS.
FNBJMVTFS!HNBJMDPN
MBTU@NPEJGJFE@EBUF
^
To execute the preceding indexing operation, the following will happen:
The request can be sent to any node in the cluster, and the node receiving the
1.
request is called the coordinating node.
The coordinating node uses the document ID or routing value to determine the
2.
shard on which the operation needs to be executed. In this example, let's say the
preceding document with ID 1 belongs to 4.
Internally, the request is forwarded to the primary shard 4 in /PEF.
3.
The primary shard executes the operation locally and sends the request to all the
4.
replicas in parallel.
Once all the replicas respond back to primary, an acknowledgment is sent back to
5.
the client.
Now let's look at the response in the following operation:
\
@JOEFYDIBQUFS
@UZQFQFSTPO
@JE
@WFSTJPO
 "result": "created",
@TIBSET\
UPUBM
"successful": 2,
GBJMFE
^
 "created": true
^
You can see from the preceding response the result of the operation is created. Let's look at
the @TIBSET sections of the response. You can see that the total number of shards are 
(primary and replica), and both are successful.
www.ebook3000.com

Indexing and Updating Your Data
[ 118 ]
Primary preference
A query can be answered by a primary or a replica shard as they are exact copies. Query
requests are usually round robin between primary and replica shards. Since the request is
first executed on the primary shard and then forwarded to the replicas, there may be a time
when the data from the primary is different from the replica. If your application always
needs the latest data, you can use the preference property to control which shard the
request is executed on.
Let's consider the following example:
1045DIBQUFSQFSTPO@TFBSDIpreference=_primary
\
RVFSZ\
NBUDI\
OBNFVTFS
^
^
^
The preceding request will be executed only on the primary shards. The other option is
@QSJNBSZ@GJSTU. The request is executed first on primary, and if the primary is not
available, it is executed on the replica shard:
1045DIBQUFSQFSTPO@TFBSDIpreference=_primary_first
\
RVFSZ\
NBUDI\
OBNFVTFS
^
^
^
We will talk more about the other preferences Elasticsearch supports in $IBQUFS, All about
search.
More replicas for query throughput
Replicas not only help with the failover but also increase the query throughput. A replica is
the exact copy of the primary and is hosted in different node of the cluster. The queries are
usually randomized between primary and replica shards.
If your application is read heavy, you should consider increasing the number of replicas.

Indexing and Updating Your Data
[ 119 ]
If some of the shards are not available or don't respond back in time,
Elasticsearch responds back with a partial result and is indicated in the
@TIBSET section of the response. You should always check the @TIBSET to
make sure the response has complete results.
Increasing/decreasing the number of replicas
We can increase and decrease the number of replicas of an index on the fly. Consider
changing the number of replicas to  before any bulk indexing operations, and you can
change the number of replicas back to the original value once done:
165DIBQUFS@TFUUJOHT
\
JOEFY\
"number_of_replicas" : 0
^
^
You should see a response similar to this:
\BDLOPXMFEHFEUSVF^
Summary
In this chapter, you learned how to index and update your data. We discussed the various
Elasticsearch clients available and how to use them. We discussed what happens internally
when you index or update. You also learned about refresh interval and why there is a delay
between indexing your data and it being available for search. We also covered how
Elasticsearch deals with concurrency, and how data is synchronized between primary and
replica shards.
In the next chapter, will discuss the various bulk APIs Elasticsearch has to offer and how to
organize your data.
www.ebook3000.com

5
Organizing Your Data and Bulk
Data Ingestion
In this chapter, youâ€™ll learn how to manage indices in Elasticsearch. Until this chapter, you
learned about operating on a single document. In this chapter, youâ€™ll learn about the
various APIs Elasticsearch has to offer to support bulk operations. They can be very
effective when its comes to rebuilding the entire index or batching requests together in a
single call. Due to how the data in stored internally in Elasticsearch, the number of shards
or the mapping of the fields cannot be changed after the index creation. You'll learn about
Reindex API, which can rebuild the index with the correct settings. Using Elasticsearch for
time-based data is a very common usage pattern. We will discuss different ways to manage
time-based indices. By the end of this chapter, we will have covered the following:
Multi Get API
Update by Query API
Delete by Query API
Reindex API
Dealing with time-based indexes
Shrink API
Bulk operations
In this section, we will discuss various bulk operations Elasticsearch supports. Batching
multiple requests together saves network round trips, and the requests in the batch can be
executed in parallel. Elasticsearch has a dedicated thread pool for bulk operations, and the
number of requests it can process in parallel depends on the number of the CPU processors
in the node.

Organizing Your Data and Bulk Data Ingestion
[ 121 ]
The following are the different bulk operations supported:
Bulk API: This can be used to batch multiple index and delete operations and
execute them using a single API call.
Multi Get API: This can be used to retrieve documents using document IDs.
Update by query: This can be used to update a set of documents that match a
query.
Delete by query: This can be used to delete the documents that match a query.
Bulk API
Bulk API is ideal for indexing or deleting large sets of data. Create, index, update, and
delete operations are supported. For each request along with the document source,
metadata information, such as index name, type, unique identifier, and routing, should be
provided. Each bulk request is separated by a new line. A bulk request can have a mixture
of create, index, update, and delete requests. The node receiving the bulk request (also
known as coordinating node) groups the requests by the shard they belong to and executes
them in parallel. The thread pools that execute single and bulk requests are independent.
Let's say we are adding shirts to the e-commerce inventory. We can use the bulk API to add
the product documents to the index as follows:
1045@CVML
\create\"_index" : "chapter5", "_type" : "product",@JE^^
\QSPEVDU@OBNF-POH4MFFWF4IJSU8IJUFVOJU@QSJDF^
\index\@JOEFYDIBQUFS@UZQFQSPEVDU@JE^^
\QSPEVDU@OBNF#VUUPOEPXO4IJSUVOJU@QSJDF^
\DSFBUF\@JOEFYDIBQUFS@UZQFQSPEVDU@JE^^
\QSPEVDU@OBNF/PO*SPO4PMJE4IJSUVOJU@QSJDF^
The DSFBUF operation will fail if the document already exists. Instead, you
can also use the JOEFY operation, which will replace the document source
with the request if the document with the same ID already exists. During
the JOEFY operation, if the @JE is not specified, a unique identifier is
generated automatically.
Next, along with the QSPEVDU@OBNF and VOJU@QSJDF fields, we want to add the
TIJQJOH@QSJDF field to the document. We can use either use JOEFY operation and specify
the entire source or use VQEBUF with the partial document. In the following example, we
will use an VQEBUF operation to add the new field.
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 122 ]
Since we are operating on the same index and type, we can also specify the index and type
in the URL instead of the metadata, as shown next:
1045chapter5/product@CVML
\VQEBUF\@JE_retry_on_conflict^^
\EPD\TIJQQJOH@QSJDF^^
\VQEBUF\@JE@SFUSZ@PO@DPOGMJDU^^
\EPD\TIJQQJOH@QSJDF^^
\VQEBUF\@JE@SFUSZ@PO@DPOGMJDU^^
\EPD\TIJQQJOH@QSJDF^^
6QEBUF operation has to first retrieve the original document and then apply the changes.
Between retrieving the document and updating it, if another process makes changes to the
document, we can use the @SFUSZ@PO@DPOGMJDU parameter to specify the number of times
the operation has to be retried before it fails the request. The response of the preceding
query is shown next:
\
UPPL
FSSPSTGBMTF
JUFNT<
\
update\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
"result": "updated",
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
"status": 200
^
^
\
VQEBUF\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
SFTVMUVQEBUFE
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE

Organizing Your Data and Bulk Data Ingestion
[ 123 ]
^
TUBUVT
^
^
\
VQEBUF\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
SFTVMUVQEBUFE
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
TUBUVT
^
^
>
^
Next, if we want to delete the product documents, we can use the bulk query as follows:
1045DIBQUFSQSPEVDU@CVML
\delete\@JE^^
\VQEBUF\"_id" : "4"@SFUSZ@PO@DPOGMJDU^^
\EPD\VOJU@QSJDF^^
Each query in the bulk request is executed independently, and one operation does not
affect the other. In the preceding query, we tried to update a document (ID: 4) that doesn't
exist. The delete operation is successful and update operation error is shown in the
response. The errors flag at the beginning of the bulk response is also set to true, indicating
they are errors while executing the bulk request. The response of the preceding query is as
follows:
\
UPPL
"errors": true
JUFNT<
\
EFMFUF\
GPVOEUSVF
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
SFTVMUEFMFUFE
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 124 ]
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
"status": 200
^
^
\
"update": {
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"status": 404,
 "error": {
"type": "document_missing_exception",
SFBTPO<QSPEVDU><>EPDVNFOUNJTTJOH
JOEFY@VVJE,VOXB*;25Z(X6CU'/T2
TIBSE
JOEFYDIBQUFS
^
^
^
>
^
The number of requests in a batch depends on various factors, such as the document size,
hardware, and so on. Since the entire bulk request has to be loaded into memory before
executing the request, depending on the number of requests, memory can be an issue and
can take a lot longer to transfer the data over the network. You should experiment bulk
with different batch sizes to find the sweet spot.
If you are indexing data that is not required to be searchable immediately, consider turning
off the refresh interval or increasing the refresh interval. This will reduce the strain on the
system and increase the index throughput. You can temporarily disable the refresh as
shown next:
165DIBQUFS@TFUUJOHT
\
JOEFY\
 "refresh_interval": "-1"
^
^

Organizing Your Data and Bulk Data Ingestion
[ 125 ]
The bulk request indexes the data first in primary and then replicas. During bulk indexing,
you can also consider removing the replicas and enable them once the indexing is complete.
Only remove replicas if you are indexing the data for the first time or for an inactive index,
as removing replica might affect the query throughput.
Multi Get API
Multi Get API is used to retrieve multiple documents using a single request. A simple
request to get the documents based on the unique identifier is shown here:
1045DIBQUFSQSPEVDU@NHFU
\
"ids" : ["2", "3"]
^
The response of the preceding query is as follows:
\
EPDT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
GPVOEUSVF
@TPVSDF\
QSPEVDU@OBNF#VUUPOEPXO4IJSU
VOJU@QSJDF
TIJQQJOH@QSJDF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
GPVOEUSVF
@TPVSDF\
QSPEVDU@OBNF/PO*SPO4PMJE4IJSU
VOJU@QSJDF
TIJQQJOH@QSJDF
^
^
>
^
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 126 ]
To retrieve documents from different index/types, metadata information can be specified as
follows:
1045@NHFU
\
EPDT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
^
>
^
By default, the complete source of the document is returned. You can use the @TPVSDF field
to include/exclude the fields of the document in the response, as shown next:
1045@NHFU
\
EPDT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TPVSDF<
"product_name"
>
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TPVSDF\
"exclude": "unit_price"
^
^
>
^

Organizing Your Data and Bulk Data Ingestion
[ 127 ]
The response to the preceding request is as follows:
\
EPDT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
GPVOEUSVF
@TPVSDF\
product_name#VUUPOEPXO4IJSU
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@WFSTJPO
GPVOEUSVF
@TPVSDF\
shipping_price
product_name/PO*SPO4PMJE4IJSU
^
^
>
^
Update by query
Update by Query API is used to update all documents that match a particular query.
Depending on the number of documents in the index, this can be a very resource-
demanding operation and can slow the existing search or index operations. We will discuss
the different ways to control the number of requests per second and how to parallel this
operation.
For example, for the products we indexed using the bulk API in the previous section, we
want to add a new QSJDF@XJUI@UBY field, which contains the price plus tax. We will use
update by query and scripting for the calculation. Assuming  is the tax, we will use the
unit price to determine the price with tax. We will be use the default scripting language
Painless for this operation. The following query is using NBUDI@BMM to match all the
documents, but you can specify a query if you want to limit the documents you want to
update.
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 128 ]
The @VQEBUF@CZ@RVFSZ API is shown here:
1045DIBQUFS_update_by_query
\
TDSJQU\
"inline": "ctx._source.price_with_tax = ctx._source.unit_price * 1.1",
MBOHQBJOMFTT
^
RVFSZ\
NBUDI@BMM\^
^
^
The response to the preceding query is as follows:
\
UPPL
UJNFE@PVUGBMTF
UPUBM
VQEBUFE
EFMFUFE
"batches": 1,
"version_conflicts": 0,
OPPQT
SFUSJFT\
CVML
TFBSDI
^
UISPUUMFE@NJMMJT
SFRVFTUT@QFS@TFDPOE
UISPUUMFE@VOUJM@NJMMJT
GBJMVSFT<>
^
Update by query works by taking a snapshot of the existing index and performing the
update on each document. In the preceding response, you can see that
WFSTJPO@DPOGMJDUT and we don't have conflicts. Version conflicts happen if the
document is updated between taking the snapshot and updating it. By default, if a conflict
occurs, the operation fails and the updates performed so far will remain. If you want the
operation to continue despite version conflicts, you can specify that when running the
request as shown here:
1045DIBQUFS@VQEBUF@CZ@RVFSZconflicts=proceed
\
TDSJQU\
JOMJOFDUY@TPVSDFQSJDF@XJUI@UBYDUY@TPVSDFVOJU@QSJDF
MBOHQBJOMFTT

Organizing Your Data and Bulk Data Ingestion
[ 129 ]
^
RVFSZ\
NBUDI@BMM\^
^
^
The VQEBUF operation will not be failed if there is a conflict, and any conflicts that occur
during the update are specified in the response. In the preceding response, you can see that
the number of batches (CBUDIFT) is . If the index you are trying to update is quite large,
instead of waiting for the response to come back, you can ask Elasticsearch to respond with
a task ID. You can then use the task API to track the progress or cancel the task. To make
Elasticsearch respond with task ID, you need to set the XBJU@GPS@DPNQMFUJPO to GBMTF as
a query parameter as shown below:
1045DIBQUFS@VQEBUF@CZ@RVFSZwait_for_completion=false
\
TDSJQU\
JOMJOFDUY@TPVSDFQSJDF@XJUI@UBYDUY@TPVSDFVOJU@QSJDF
MBOHQBJOMFTT
^
RVFSZ\
NBUDI@BMM\^
^
^
The response to the preceding query is shown here. It contains the task ID:
\
UBTLxZjyFE19Q0yGxehcys6ydg:307842
^
You can track the status of the task using the task ID, as follows:
(&5@UBTLTxZjyFE19Q0yGxehcys6ydg:307842
The status response is shown here:
\
"completed": true,
UBTL\
OPEFY;KZ'&2Z(YFIDZTZEH
JE
UZQFUSBOTQPSU
BDUJPOindices:data/write/update/byquery
TUBUVT\
UPUBM
VQEBUFE
DSFBUFE
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 130 ]
EFMFUFE
CBUDIFT
WFSTJPO@DPOGMJDUT
OPPQT
SFUSJFT\
CVML
TFBSDI
^
UISPUUMFE@NJMMJT
SFRVFTUT@QFS@TFDPOE
UISPUUMFE@VOUJM@NJMMJT
^
EFTDSJQUJPO
TUBSU@UJNF@JO@NJMMJT
SVOOJOH@UJNF@JO@OBOPT
"cancellable": true
^
SFTQPOTF\
UPPL
UJNFE@PVUGBMTF
UPUBM
VQEBUFE
DSFBUFE
EFMFUFE
CBUDIFT
WFSTJPO@DPOGMJDUT
OPPQT
SFUSJFT\
CVML
TFBSDI
^
UISPUUMFE@NJMMJT
SFRVFTUT@QFS@TFDPOE
UISPUUMFE@VOUJM@NJMMJT
GBJMVSFT<>
^
^
You can see in the preceding response that the task is completed, but you can also cancel the
task using the cancel task API as shown here:
1045@UBTLTxZjyFE19Q0yGxehcys6ydg:307842/_cancel

Organizing Your Data and Bulk Data Ingestion
[ 131 ]
Delete by query
Delete by Query API is very similar to update by query but can be used to delete the
documents that match the query. It works by taking a snapshot of the index and deleting
the documents. Between taking the snapshot and executing the delete query, if the
documents are modified, the operation fails due to the version conflict. To proceed on
failures, DPOGMJDUTQSPDFFE can be specified as a URL parameter, and any failures that
occur are reported in the response. For example, to delete all the documents that contain the
word TIJSU, the query is as follows:
1045DIBQUFS@EFMFUF@CZ@RVFSZconflicts=proceed
\
RVFSZ\
NBUDI\
QSPEVDU@OBNFTIJSU
^
^
^
The response to the preceding query is shown here:
\
UPPL
UJNFE@PVUGBMTF
UPUBM
EFMFUFE
CBUDIFT
WFSTJPO@DPOGMJDUT
OPPQT
SFUSJFT\
CVML
TFBSDI
^
UISPUUMFE@NJMMJT
SFRVFTUT@QFS@TFDPOE
UISPUUMFE@VOUJM@NJMMJT
GBJMVSFT<>
^
You can set XBJU@GPS@DPNQMFUJPOGBMTF as a URL parameter to get the task ID as a
response to the query and track the progress of the query using the task API.
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 132 ]
Reindex API
Before Elasticsearch 5.0, to change the index settings, or change the mapping of an index,
you have a create a new index and reindex the data. Reindexing a large index is usually lot
of work, which involves reading the data from the source like a SQL database, transforming
the data into Elasticsearch documents and loading the data into Elasticsearch. For large
applications, batch processing engines such as Hadoop are used to reindex the data.
Depending on how big the index is or how complicated the ETL (Extract, Transform, Load)
process is, reindex can be very expensive. To solve this, Reindex API was introduced. The
original JSON document used for indexing is stored in the @TPVSDF field which can be used
by the Reindex API to reindex the documents. The Reindex API can be used for the
following:
To change the mapping/settings of an existing index
To combine documents from multiple indexes in one index
To copy only missing documents
To copy a subset of documents from one index to other
To copy top N documents based on sort value
To copy only a subset of fields from the source index to destination index
Just like @VQEBUF@CZ@RVFSZ and @EFMFUF@CZ@RVFSZ, reindex works by taking a snapshot
of the existing index. You can set XBJU@GPS@DPNQMFUJPOGBMTF in the URL to get the task
ID as a response to the query and track the progress of the query using the Task API as
shown next:
(&5@UBTLTxZjyFE1970BGxehcys6ydg:309742
Change mappings/settings
Reindex API can be used to change the index settings like increasing/decreasing the number
of shards, disabling the @BMM field and so on. It can also be used to change the mapping of
an existing field, for example, you want to change the mapping of an keyword field to date.
The Shrink API can also be used to decrease the number of shards. To
decrease the number of shards, Shrink API is relatively much cheaper than
the reindex operation. Please refer to the Shrink API section for more
details.

Organizing Your Data and Bulk Data Ingestion
[ 133 ]
To change the index setting/mappings of the existing index, first create the destination
index with the desired settings and use the Reindex API as shown next:
1045@SFJOEFY
\
"source": {
JOEFYTPVSDF@JOEFY
^
"dest": {
JOEFYEFTU@JOEFY
^
^
Combining documents from one or more indices
You can combine the documents from one or more indices into one big index, as shown
next:
1045@SFJOEFY
\
"source": {
JOEFY<
TPVSDF@JOEFY@
TPVSDF@JOEFY@
>
^
"dest": {
JOEFYEFTU@JOEFY
^
^
Copying only missing documents
You can copy only missing documents from one index to another by setting the PQ@UZQF to
create. Any existing documents with the same identifier will get a conflict. By setting
conflicts to proceed in the request, reindex will ignore the existing documents, as shown
next:
1045@SFJOEFY
\
"conflicts": "proceed",
TPVSDF\
JOEFYTPVSDF@JOEFY
^
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 134 ]
EFTU\
JOEFYEFTU@JOEFY
"op_type": "create"
^
^
Copying a subset of documents into a new index
You can use Reindex API to copy only the documents that match a query. Suppose you
have a index which contains application logs and you want to copy only the error logs into
a new index. You can use reindex as shown next:
1045@SFJOEFY
\
TPVSDF\
JOEFYTPVSDF@JOEFY@
UZQFMPH
"query": {
       "term": {
         "level": "ERROR"
       }
     }
^
EFTU\
JOEFYEFTU@JOEFY
^
^
Copying top N documents
You can use Reindex API to copy top N documents based on a field. Suppose you want to
copy only the last 1,000 logs based on the timestamp field to a new index. You can use the
Reindex API as shown here:
1045@SFJOEFY
\
"size": 1000,
TPVSDF\
JOEFYTPVSDF@JOEFY
"sort": {
       "timestamp": "desc"
     }
^
EFTU\

Organizing Your Data and Bulk Data Ingestion
[ 135 ]
JOEFYEFTU@JOEFY
^
^
Copying the subset of fields into new index
You can copy a subset of fields in the document as shown here:
1045@SFJOEFY
\
TPVSDF\
JOEFY<
TPVSDF@JOEFY@
>
"_source": [
       "field1",
       "field2"
     ]
^
EFTU\
JOEFYEFTU@JOEFY
^
^
Depending on how big the source index is, reindex can be an expensive operation. It is
recommended to disable all the replicas on the destination index and enable them once the
reindex is complete.
Ingest Node
Traditionally, Logstash is used to preprocess your data before indexing into Elasticsearch.
Using Logstash, you can define pipelines to extract, transform, and index your data into
Elasticsearch.
In Elasticsearch 5.0, the ingest node has been introduced. Using the ingest node, pipelines to
modify the documents before indexing can be defined. A pipeline is a series of processors,
each processor working on one or more fields in the document. The most commonly used
Logstash filters are available as processors. For example, using a HSPL filter to extract data
from an Apache log file into a document, extracting fields from JSON, changing the date
format, calculating geo-distance from a location, and so on. The possibilities are endless.
Elasticsearch supports many processors out of the box. You can also develop your own
processors using any JVM-supported languages.
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 136 ]
By default, all nodes in the cluster can act as ingest nodes. Depending on the amount of
processing required, you can use new or existing nodes in the cluster as dedicated ingest
nodes. To add or remove a pipeline, @JOHFTU API is provided. Ingest API also supports
simulating the pipeline against a given document to debug and test the pipeline. The basic
structure of a pipeline is shown next:
165@JOHFTUQJQFMJOF<pipeline-name>
\
QJQFMJOF\
EFTDSJQUJPOpipeline-description
QSPDFTTPST<>
^
^
The processors as shown above accept a list of processors, which are executed serially. To
better explain Ingest API, let us take an e-commerce site example. When a user views a
product/item, a message is logged in the format shown next. By the end of this section, we
will define a pipeline to transform the log message into a JSON document and index the
document into Elasticsearch. We will also use aggregations to find the most viewed items:
\
"message": "127.0.0.1 2017-04-11T09:02:34.234+07:00 USA 1 Web"
^
Let's define a pipeline to parse the log message using the HSPL processor.
Grok is nothing but a collection of regular expressions to parse
unstructured data like logs into structured data. The syntax of a grok
pattern is \4:/5"94&."/5*$^, the syntax is the name of the pattern
and semantic is the name of the field. For example, in \*1DMJFOU^ the
IP is the pattern and client is the name of the field in the JSON document.
A list of pre-defined patterns can be found at IUUQTHJUIVCDPNMPHTU
BTIQMVHJOTMPHTUBTIQBUUFSOTDPSFUSFFNBTUFSQBUUFSOT
We can use the following HSPL pattern to transform the above shown log message into a
JSON document :
\*1DMJFOU^\5*.&45".1@*40UJNFTUBNQ^\803%DPVOUSZ^
\/6.#&3JUFN*E^
\803%QMBUGPSN^

Organizing Your Data and Bulk Data Ingestion
[ 137 ]
Before creating the pipeline, we can test the pipeline using the _simulate endpoint as
shown next:
1045@JOHFTUQJQFMJOF_simulate
\
QJQFMJOF\
EFTDSJQUJPO*UFN7JFX1JQFMJOF
QSPDFTTPST<
\
"grok"\
GJFMEMPH
QBUUFSOT<
"%{IP:client} %{TIMESTAMP_ISO8601:timestamp} %{WORD:country}
%{NUMBER:itemId} %{WORD:platform}"
>
^
^
>
^
EPDT<
\
@TPVSDF\
 "log": "127.0.0.1 2017-04-11T09:02:34.234+07:00 USA 1 Web"
^
^
>
^
The response to the preceding query is shown here:
\
EPDT<
\
EPD\
@JE@JE
@JOEFY@JOEFY
@UZQF@UZQF
"_source": {
DPVOUSZ64"
JUFN*E
 "log": "127.0.0.1 2017-04-11T09:02:34.234+07:00 USA 1 Web",
DMJFOU
QMBUGPSN8FC
UJNFTUBNQ5
^
@JOHFTU\
UJNFTUBNQ5
^
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 138 ]
^
^
>
^
You can see from the preceding response that the log message is transformed into a JSON
document using the grok processor. Along with the fields, the original log message is also
stored as the log field in the document. Next, we can add a processor to remove the log field
from the document, as shown here:
1045@JOHFTUQJQFMJOF@TJNVMBUF
\
QJQFMJOF\
EFTDSJQUJPO*UFN7JFX1JQFMJOF
QSPDFTTPST<
\
HSPL\
GJFMEMPH
QBUUFSOT<
\*1DMJFOU^\5*.&45".1@*40UJNFTUBNQ^\803%DPVOUSZ^
\/6.#&3JUFN*E^\803%QMBUGPSN^
>
^
^
{
         "remove": {
           "field": "log"
         }
       }
>
^
EPDT<
\
@TPVSDF\
MPH564"8FC
^
^
>
^
The response to the preceding query is as follows:
\
EPDT<
\
EPD\
@JE@JE
@JOEFY@JOEFY

Organizing Your Data and Bulk Data Ingestion
[ 139 ]
@UZQF@UZQF
@TPVSDF\
DPVOUSZ64"
JUFN*E
DMJFOU
QMBUGPSN8FC
UJNFTUBNQ5
^
@JOHFTU\
UJNFTUBNQ5
^
^
^
>
^
The resulting document looks a lot better than the raw log message. Next, we want to store
the document in a month-based index. All the messages that are logged in the same month
belong to the same index. Grouping the logs based on a time interval makes it very easy to
clean or delete old logs. We will use the EBUF@JOEFY@OBNF processor, which takes a date
field as input and outputs the index name to which the document belongs:
1045@JOHFTUQJQFMJOF@TJNVMBUF
\
QJQFMJOF\
EFTDSJQUJPO*UFN7JFX1JQFMJOF
QSPDFTTPST<
\
HSPL\
GJFMEMPH
QBUUFSOT<
\*1DMJFOU^\5*.&45".1@*40UJNFTUBNQ^\803%DPVOUSZ^
\/6.#&3JUFN*E^\803%QMBUGPSN^
>
^
^
\
SFNPWF\
GJFMEMPH
^
^
{
         "date_index_name": {
           "field": "timestamp",
           "index_name_prefix": "viewitem-",
           "date_rounding": "M",
           "index_name_format": "yyyy-MM"
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 140 ]
         }
       }
>
^
EPDT<
\
@TPVSDF\
MPH564"8FC
^
^
>
^
In the preceding query, we used the EBUF@JOEFY@OBNF processor to figure out the index
the document belongs to. The JOEFY@OBNF@QSFGJY is the prefix value added to the
JOEFY@OBNF@GPSNBU. In the preceding query, the index name will be WJFXJUFN@@.
The EBUF@SPVOEJOH rounds the date field to the date math field specified. The following
EBUF@SPVOEJOH options are supported:
Time unit Description
y
year
M
month
w
week
d
day
h
hour
m
minute
s
second
The response to the preceding query is shown here:
\
EPDT<
\
EPD\
@JE@JE
"_index": "<viewitem_{2017_04||/M{yyyy_MM|UTC}}>",
@UZQF@UZQF
@TPVSDF\
DPVOUSZ64"
JUFN*E
DMJFOU
QMBUGPSN8FC

Organizing Your Data and Bulk Data Ingestion
[ 141 ]
UJNFTUBNQ5
^
@JOHFTU\
UJNFTUBNQ5
^
^
^
>
^
Until now, we used the simulate feature to simulate the pipeline, we can create the pipeline
using the @JOHFTU PUT API as shown here:
PUT _ingest/pipeline/view_item_pipeline
\
EFTDSJQUJPO*UFN7JFX1JQFMJOF
QSPDFTTPST<
\
HSPL\
GJFMEMPH
QBUUFSOT<
\*1DMJFOU^\5*.&45".1@*40UJNFTUBNQ^\803%DPVOUSZ^
\/6.#&3JUFN*E^\803%QMBUGPSN^
>
^
^
\
SFNPWF\
GJFMEMPH
^
^
\
EBUF@JOEFY@OBNF\
GJFMEUJNFTUBNQ
JOEFY@OBNF@QSFGJYWJFXJUFN@
EBUF@SPVOEJOH.
JOEFY@OBNF@GPSNBUZZZZ@..
^
^
>
^
For the complete list of all the processors supported, please visit the official documentation:
IUUQTXXXFMBTUJDDPHVJEFFOFMBTUJDTFBSDISFGFSFODFJOHFTUQSPDFTTPST
IUNM
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 142 ]
Let's index a couple of documents using the pipeline we just defined. We would index the
logs just like regular documents but specify the pipeline as the query parameter, as follows:
1045DIBQUFSMPHpipeline=view_item_pipeline
\
MPH564"18FC
^
1045DIBQUFSMPHQJQFMJOFWJFX@JUFN@QJQFMJOF
\
MPH564"28FC
^
1045DIBQUFSMPHQJQFMJOFWJFX@JUFN@QJQFMJOF
\
MPH564"38FC
^
1045DIBQUFSMPHQJQFMJOFWJFX@JUFN@QJQFMJOF
\
MPH564"8FC
^
Let's look at the response of one of the preceding queries:
\
"_index": "viewitem_2017_04",
@UZQFMPH
@JE"7ZF6TO4J0"YR,Q/(J,
@WFSTJPO
SFTVMUDSFBUFE
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
DSFBUFEUSVF
^
From the preceding response, you can see that the document is indexed to the
WJFXJUFN@@ index, although the original request is to index the document into the
DIBQUFS index. We can retrieve the document using the GET API as shown next:
(&5WJFXJUFN@@MPH"7ZF6TO4J0"YR,Q/(J,

Organizing Your Data and Bulk Data Ingestion
[ 143 ]
The log message "564"8FC" is
converted in to the document shown below. The response to the above query is as follows:
\
@JOEFYWJFXJUFN@@
@UZQFMPH
@JE"7ZF6TO4J0"YR,Q/(J,
@WFSTJPO
GPVOEUSVF
"_source": {
     "country": "USA",
     "itemId": "1",
     "client": "127.0.0.1",
     "platform": "Web",
     "timestamp": "2017-04-11T09:02:34.234+07:00"
   }
^
Next, we can use the WJFXJUFN index to run an aggregation query to get the top five most
viewed items. We will use the terms aggregation to get the top 5 items.
Grok processor outputs all the fields as text by default. You can define a
index template with mappings if you want to change the default
behaviour. Each text field is also indexed as keyword which can be
accessed via textfield.keyword. Terms aggregation can only be executed
on a LFZXPSE field due to which we need to use the JUFN*ELFZXPSE
field instead of JUFN*E for the aggregation.
The aggregation query is shown here:
1045WJFXJUFN@@@TFBSDI
\
TJ[F
BHHT\
NPTU@WJFXT@JUFNT\
UFSNT\
"field": "itemId.keyword",
"size": 5 #No of buckets
^
^
^
^
The response of the preceding query is shown here:
\

www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 144 ]
BHHSFHBUJPOT\
NPTU@WJFXT@JUFNT\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
\
"key": "1",
           "doc_count": 2
^
\
 "key": "2",
           "doc_count": 1
^
\
"key": "3",
           "doc_count": 1
^
>
^
^
^
From the preceding response, you can see that the most viewed item is item ID , followed
by item ID  and . We can also run aggregations to first bucket the data by country and
then compute the most viewed item for each country and so on. We will discuss
aggregations in detail in $IBQUFS, How to slice and dice your data using aggregations.
Organizing your data
In this section, we will discuss how to divide your data into multiple indices. Elasticsearch
provides index aliases, which make it very easy to query multiple indices at once. It also
supports index templates to configure automatic index creation. We will also discuss how
to deal with time-based data, such as logs, which is a common Elasticsearch use case.
Index alias
An index alias is a pointer to one or more indexes. A search operation executed against an
alias is executed across all the indexes the alias points to. The coordinating node executes
the request on all indices, collects the results, and sends them back to the client. The index
operation, on the other hand, cannot be executed on an alias that points to more than one
index. It is recommended to use an alias instead of the actual index name in your
application. If the alias needs to point to a different index later, it would be an easy switch.

Organizing Your Data and Bulk Data Ingestion
[ 145 ]
Due to how the data in stored in the inverted index, mappings of existing fields cannot be
changed on the fly. For example, you want to treat a field that is previously indexed as
string as a date. To do this, you have to create a new index with the correct mappings and
reindex the data. If your application is currently using the index alias named
BQQMJDBUJPO@MPHT and the alias is currently pointing to the index
JOEFY@XJUI@EBUF@BT@TUSJOH, the alias can be easily changed to use the new index
JOEFY@XJUI@EBUF@BT@EBUF as shown here:
1045_aliases
\
BDUJPOT<
\
 "remove": {
JOEFYJOEFY@XJUI@EBUF@BT@TUSJOH
BMJBTBQQMJDBUJPO@MPHT
^
^
\
"add": {
JOEFYJOEFY@XJUI@EBUF@BT@EBUF
BMJBTBQQMJDBUJPO@MPHT
^
^
>
^
Since the application is using an alias, we are able to change the index without any changes
to the client code. The response to the preceding query is as follows:
\
BDLOPXMFEHFEUSVF
^
Since an alias can point to multiple indices, we used the @BMJBTFT endpoint to switch the
index the alias points to. We can also use the @BMJBT endpoint to add an alias. We can add
the JOEFY@XJUI@EBUF@BT@TUSJOH index to the BQQMJDBUJPO@MPHT alias in two ways.
Using the @BMJBT endpoint as shown here:
165JOEFY@XJUI@EBUF@BT@TUSJOH_alias/application_logs
Or using the @BMJBTFT endpoint as shown here:
1045/_aliases
\
BDUJPOT<
\
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 146 ]
BEE\
JOEFYJOEFY@XJUI@EBUF@BT@TUSJOH
BMJBTBQQMJDBUJPO@MPHT
^
^
>
^
You can verify the indices the alias points to as shown here:
(&5@BMJBTBQQMJDBUJPO@MPHT
The response to the preceding query is as follows:
\
JOEFY@XJUI@EBUF@BT@TUSJOH\
BMJBTFT\
BQQMJDBUJPO@MPHT\^
^
^
^
An index alias can also be used to group one or more indices. This is especially useful when
querying for time series data. For example, you are storing the application logs in
Elasticsearch. Depending on the volume of the data, you decided to use one index for one
month of data. All the logs that are produced in the same month are indexed to the same
index. The index that stores the logs for the month of April is named MPHT@@. For
suppose, you only care about the latest logs, you can create a MPHT@MBTU@@NPOUIT alias,
which points to indices of the last two months. The user or the application can query the
alias rather than figuring out the indices for the last  months based on the current date. We
will discuss more in the Managing time-based indexes section.
The index alias can also be used to view the data with a filter. This is similar to a view in a
database. For example, a log document looks like the following:
1045MPHT@@MPH
\
MFWFM&3303
BQQMJDBUJPO4FSWJDF"
UJNFTUBNQ5
NFTTBHF&YDFQUJPOJOUISFBEKBWBMBOH/VMM1PJOUFS&YDFQUJPOBU
DIBQUFSNBJO
4FSWJDF"KBWB
^

Organizing Your Data and Bulk Data Ingestion
[ 147 ]
Let's say we just deployed the latest code and want to make sure they are no errors in the
application. We can create an alias with a filter for errors as shown here:
1045@BMJBTFT
\
BDUJPOT<
\
BEE\
 "index": "logs_04_17",
         "alias": "logs_latest_error",
GJMUFS\
NBUDI\
MFWFM&3303
^
^
^
^
>
^
To get only the errors, you can query the MPHT@MBUFTU@FSSPS index as shown here:
(&5MPHT@MBUFTU@FSSPS@TFBSDI
You can search for errors that are logged for 4FSWJDF" in the last  hour as shown here:
1045logs_latest_error@TFBSDI
\
RVFSZ\
CPPM\
NVTU<
\
NBUDI\
"application": "Service A"
^
^
\
SBOHF\
UJNFTUBNQ\
 "gte": "now-1h",
UJNF@[POF
^
^
^
>
^
^
^
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 148 ]
The response to the preceding query is as follows:
\
UPPL
UJNFE@PVUGBMTF
@TIBSET\
UPUBM
TVDDFTTGVM
GBJMFE
^
IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYMPHT@@
@UZQFMPH
@JE"7UQVEFTMIT)$Z#HE#
@TDPSF
@TPVSDF\
MFWFM&3303
BQQMJDBUJPO4FSWJDF"
UJNFTUBNQ5
NFTTBHF&YDFQUJPOJOUISFBEKBWBMBOH/VMM1PJOUFS&YDFQUJPO
BUDIBQUFSNBJO
4FSWJDF"KBWB
^
^
>
^
^
Index templates
When you index a document, if the index doesn't exist already, Elasticsearch will
automatically create the index based on the default settings. The mappings of the document
are dynamically mapped. You can overwrite the default settings and mapping of the field
using the index templates. By default, a new index is created with  shards and  replica. In
the following template, unless specified, a new index created will have  shard and 
replicas:
165@UFNQMBUFEFGBVMU@UFNQMBUF
\
"template": "*",
PSEFS
TFUUJOHT\
OVNCFS@PG@TIBSET

Organizing Your Data and Bulk Data Ingestion
[ 149 ]
^
^
Index templates work using the index name. It does a wild card match on the index name.
Let's define a template for logs index we discussed in the previous section. A sample log
looks like the following:
1045MPHT@@MPH
\
MFWFM&3303
BQQMJDBUJPO4FSWJDF"
UJNFTUBNQ5
NFTTBHF&YDFQUJPOJOUISFBEKBWBMBOH/VMM1PJOUFS&YDFQUJPOBU
DIBQUFSNBJO
4FSWJDF"KBWB
^
When defining an index template, along with the index settings, the mappings of the index
can also be defined as shown here:
165@UFNQMBUFMPHT@UFNQMBUF
\
"template": "logs*",
PSEFS
 "settings": {
OVNCFS@PG@TIBSET
^
"mappings": {
MPH\
QSPQFSUJFT\
MFWFM\
UZQFLFZXPSE
^
BQQMJDBUJPO\
UZQFLFZXPSE
^
UJNFTUBNQ\
UZQFEBUF
GPSNBUEBUF@PQUJPOBM@UJNF
^
NFTTBHF\
UZQFUFYU
^
^
^
^
^
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 150 ]
If an index name starts with the word MPHT, the settings and the mappings in the template
are automatically applied to the newly created index. Let's delete the existing index and
reindex the sample log as follows:
%FMFUFUIFFYJTUJOHJOEFY
%&-&5&MPHT@@
*OEFYBEPDVNFOU
1045MPHT@@MPH
\
MFWFM&3303
BQQMJDBUJPO4FSWJDF"
UJNFTUBNQ5
NFTTBHF&YDFQUJPOJOUISFBEKBWBMBOH/VMM1PJOUFS&YDFQUJPOBU
DIBQUFSNBJO
4FSWJDF"KBWB
^
7FSJGZUIFJOEFYTFUUJOHT
(&5MPHT@@
The index settings for the MPHT@@ index are as follows:
\
MPHT@@\
BMJBTFT\^
"mappings": {
"log": {
QSPQFSUJFT\
BQQMJDBUJPO\
UZQFLFZXPSE
^
MFWFM\
UZQFLFZXPSE
^
NFTTBHF\
UZQFUFYU
^
UJNFTUBNQ\
UZQFEBUF
GPSNBUEBUF@PQUJPOBM@UJNF
^
^
^
^
TFUUJOHT\
JOEFY\
DSFBUJPO@EBUF
"number_of_shards": "3",
OVNCFS@PG@SFQMJDBT

Organizing Your Data and Bulk Data Ingestion
[ 151 ]
VVJE/KUH:Y2V$081$TD2
WFSTJPO\
DSFBUFE
^
QSPWJEFE@OBNFMPHT@@
^
^
^
^
With the template in place, new indices can be created without worrying about the settings
or the mappings. Also, with the settings and mapping, we can define the alias the index
belongs to. Once the index is created, the index is automatically added to the alias. We can
add the MPHT@@ index to the MPHT@MBTU@@NPOUIT alias as shown next:
165@UFNQMBUFMPHT@UFNQMBUF
\
UFNQMBUFMPHT
PSEFS
TFUUJOHT\
OVNCFS@PG@TIBSET
^
"aliases": {
     "logs_last_2_months": {}
   },
NBQQJOHT\
MPH\
QSPQFSUJFT\
MFWFM\
UZQFLFZXPSE
^
BQQMJDBUJPO\
UZQFLFZXPSE
^
UJNFTUBNQ\
UZQFEBUF
GPSNBUEBUF@PQUJPOBM@UJNF
^
NFTTBHF\
UZQFUFYU
^
^
^
^
^
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 152 ]
Using index templates, new indices can be added to the alias, but existing
indices cannot be removed. For example, when MPHT@@ is added to
the MPHT@MBTU@@NPOUIT alias, the alias now points to more than two
indices. You have to manually remove the old indexes or add a filter.
When a new logs index is created, it will be automatically added to the
MPHT@MBTU@@NPOUI alias. Any query executed on MPHT@MBTU@@NPOUI will also include
the results from the newly created index.
Managing time-based indices
Time series data is a very common Elasticsearch usage pattern. The data is time sensitive;
we can only care about the recent data. For example, application logs, the logs beyond a
certain time are not relevant anymore. The most recent logs are what we want to query.
Since the old logs are not important anymore, to save the disk space and avoid unnecessary
compute cycles, we have to remove the old data. The easiest way to deal with time series
data is to group the indices based on a time interval so that the older indices can be deleted
easily.
In this section, we will summarize the various APIs we discussed so far to manage time-
based indexes. For example, we want to store application logs in day-based indices. We will
use one index per day. To manage the daily indices, we can do the following:
Define an index template to set the mappings on index creation automatically
Use ingest node to transform raw logs into JSON document, index the document
into the correct index based on the day
Define a MPHT@MBTU@@EBZT alias, which points to the indices that contain logs
for the last  days
Remove older indexes from the alias
The application calling Elasticsearch can use the ingest node to transform the data before
indexing. We can define a pipeline to perform the necessary transformations. We can use
the EBUF@JOEFY@OBNF processor to read the timestamp from the document and determine
the index the document belongs to. If we are using daily indexes, a document with the
5 timestamp will be indexed into the MPH@@@
index. A pipeline with EBUF@JOEFY@OBNF processor for day-based indices is shown here:
\
QJQFMJOF\
EFTDSJQUJPO*UFN7JFX1JQFMJOF
QSPDFTTPST<

Organizing Your Data and Bulk Data Ingestion
[ 153 ]
\
EBUF@JOEFY@OBNF\
GJFMEUJNFTUBNQ
"index_name_prefix": "log_",
 "date_rounding": "d",
JOEFY@OBNF@GPSNBUZZZZ@..@EE
^
^
>
^
^
For more information on how to use ingest node, please refer to the Ingest Node section.
Next, let's define an index template to apply the mapping automatically when the index
name starts with the word MPH. When the index is created, it will also be added to
theMPHT@MBTU@@EBZT alias:
165@UFNQMBUFMPHT@UFNQMBUF
\
UFNQMBUFMPH
"aliases": {
     "logs_last_3_days": {}
   }
^
When the ingest nodes write the document into a new index, the settings/mappings from
the template are applied to the index. Please refer to the Index templates section for more
information.
Next, we will remove the older indexes from the MPHT@MBTU@@EBZT alias. When querying
the alias, including a date filter for the last  days will ensure the data accuracy:
1045@BMJBTFT
\
BDUJPOT<
\
"remove": {
JOEFYMPH@@@
BMJBTMPHT@MBTU@@EBZT
^
^
>
^
In the next section, we will discuss the Shrink API, which can be used to reduce the number
of shards of an index.
www.ebook3000.com

Organizing Your Data and Bulk Data Ingestion
[ 154 ]
Shrink API
Shrink API is used to shrink an existing index into a new index with a fewer number of
shards. If the data in the index is no longer changing, the index can be optimized for search
and aggregation by reducing the number of shards. The number of shards in the destination
index must be a factor of the original index. For example, an index with  primary shards
can be shrunk into , , or  shards. When working with time-sensitive data, such as logs,
data is only indexed into the current indexes and older indexes are mostly read only. Shrink
API doesn't re-index the document; it simply relinks the index segments to the new index.
To shrink an index, the index should be marked as read-only, and either a primary or a
replica of all the shards of the index should be moved to one node. We can force the
allocation of the shards to one node and mark it as read only as shown next. The
TISJOL@OPEF@OBNF is the name of the node into which all the shards can be allocated.
165JOEFY@XJUI@TJY@TIBSET@TFUUJOHT
\
TFUUJOHT\
JOEFYSPVUJOHBMMPDBUJPOSFRVJSF@OBNFshrink_node_name
JOEFYCMPDLTXSJUFUSVF
^
^
Once the allocation is finished, we can use the @TISJOL API to shrink the index as shown
here:
1045index_with_six_shards@TISJOLindex_with_one_shard
\
TFUUJOHT\
JOEFYOVNCFS@PG@SFQMJDBT
JOEFYOVNCFS@PG@TIBSET
^
^
You can monitor the shrink process using the cluster health API as shown here:
(&5@DMVTUFSIFBMUI
Once all the shards are changed from initializing to the active state, the new index is ready.

Organizing Your Data and Bulk Data Ingestion
[ 155 ]
Summary
In this chapter, we discussed the various bulk operations Elasticsearch supports. You also
learned about Reindex and Shrink APIs, which can be used to the change the index
configuration, such as the number of shards, mapping of an existing index and so on
without re-indexing the data.
We covered how to organize your data in Elasticsearch using aliases and index templates.
We discussed how to use ingest node to pre-process your data before indexing into
Elasticsearch. You learned how to use ingest node to transform unstructured log data into
JSON documents and automatically index them into a month-based index.
In the next chapter, we will discuss different ways of querying Elasticsearch.
www.ebook3000.com

6
All About Search
This chapter dives into search and helps you understand the different types of queries
Elasticsearch supports. You will learn how to search, sort, and paginate on your data.
Unlike SQL, the query language is based on JSON structure and is very flexible. It is very
easy to combine and nest queries. You will also learn how to execute structured queries and
full-text queries.
Elasticsearch is a search engine. When you run a query on Elasticsearch, each document in
the result is given a relevance score. For example, you are looking for restaurant close by
with decent prices, the relevance score in this case is a combination of distance and price.
The results are ordered based on how relevant is each document to the query. You will
learn the difference between sorting and scoring. You will learn how relevance is calculated
and how to tune the relevance score.
We will discuss how to debug a search query and how it works internally. We will also go
through how queries are automatically cached to improve the performance and the
different types of cache.
By the end of this chapter, you will learn the following:
Structured queries
Full-text queries
Sort and pagination
Relevance
Routing
Caching

All About Search
[ 157 ]
Different types of queries
Elasticsearch queries are executed using the Search API. Like anything else in Elasticsearch,
request and response are represented in JSON.
Queries in Elasticsearch at a high level are divided as follows:
Structured queries: Structured queries are used to query numbers, dates,
statuses, and so on. These are similar to queries supported by a SQL database. For
example, whether a number or date falls within a range or to find all the
employees with +PIO as the first name and so on
Full-text search queries: Full-text search queries are used to search text fields.
When you send a full-text query to Elasticsearch, it first finds all the documents
that match the query, and then the documents are ranked based on how relevant
each document is to the query. We will discuss relevance in detail in the Relevance
section
Both structured and full-text search queries can be combined while querying. In the next
section, we will describe the overall structure of request and response.
Sample data
To better explain the various concepts in this chapter, we will use the e-commerce site as an
example. We will create an index with a list of products. This will be a very simple index
called DIBQUFS with type called QSPEVDU. The mapping for the QSPEVDU type is shown
here:
%FMFUFFYJTUJOHJOEFYJGBOZ
%&-&5&DIBQUFS
.BQQJOH
165DIBQUFS
\
TFUUJOHT\^
NBQQJOHT\
QSPEVDU\
QSPQFSUJFT\
QSPEVDU@OBNF\
UZQFUFYU
"analyzer": "english"
^
EFTDSJQUJPO\
UZQFUFYU
www.ebook3000.com

All About Search
[ 158 ]
"analyzer": "english"
^
^
^
^
^
For the QSPEVDU@OBNF and EFTDSJQUJPO fields, the English analyzer will be used instead
of the default standard analyzer. Let's index some product documents:
*OEFY%PDVNFOUT
165DIBQUFSQSPEVDU
\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
165DIBQUFSQSPEVDU
\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
165DIBQUFSQSPEVDU
\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
We will refer to the preceding three documents for different examples used in this chapter.

All About Search
[ 159 ]
Querying Elasticsearch
One of most powerful features of Elasticsearch is the Query DSL (Domain specific
Language) or the query language. The query language is very expressive and can be used to
define filters, queries, sorting, pagination, and aggregations in the same query. To execute a
search query, an HTTP request should be sent to the @TFBSDI endpoint. The index and type
on which the query should be executed is specified in the URL. Index and type are optional.
If no index/type is specified, Elasticsearch executes the request across all the indexes in the
cluster. A search query in Elasticsearch can be executed in two different ways:
By passing the search request as query parameters.
By passing the search request in the request body.
A simple search query using query parameters is shown here:
(&5DIBQUFSQSPEVDU_searchRQSPEVDU@OBNFKBDLFU
Simple queries can be executed using the URL request parameters. Anything other than a
simple query like in the above example should be passed as the request body. The
preceding query, when passed as a request body, looks like the following:
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
UFSN\
"product_name" : "jacket"
^
^
^
The preceding query is executed on the DIBQUFS index and type named QSPEVDU. The
query can also be executed on multiple indexes/types at the same time, as shown here:
1045chapter5,chapter6product,product_reviews@TFBSDI
\
RVFSZ\
UFSN\
QSPEVDU@OBNFKBDLFU
^
^
^
www.ebook3000.com

All About Search
[ 160 ]
The HTTP verb we used in the preceding example for the @TFBSDI API is
1045. You can also use (&5 instead of 1045. Since most browsers will not
support a request body when using (&5, we used 1045.
The basic structure of the request body is shown here:
\
TJ[F5IFOVNCFSPGSFTVMUTJOUIFSFTQPOTF%FGBVMUTUP
GSPN5IFPGGTFUPGUIFSFTVMUT'PSFYBNQMFUPHFUUIFUIJSE
QBHFGPSBQBHFTJ[FPGZPVTIPVMETFUUIFTJ[FUPBOEGSPNUP
UJNFPVU"UJNFPVUDBOCFTQFDJGJFEBGUFSXIJDIUIFQBSUJBM
SFTVMUTBSFTFOUCBDLJOUIFSFTQPOTF#ZEFGBVMUUIFSFJTOPUJNFPVU*G
UIFSFRVFTUUJNFTPVUUIFUJNFE@PVUWBMVFJOUIFSFTQPOTFXJMMCF
JOEJDBUFEBTUSVF
@TPVSDF5PTFMFDUUIFGJFMETUIBUTIPVMECFJODMVEFEJOUIF
SFTQPOTF'PSFYBNQMF@TPVSDF<QSPEVDU@OBNFEFTDSJQUJPO>
RVFSZ\
2VFSZ
^
BHHT\
"HHSFHBUJPOT
^
TPSU\
)PXUPTPSUUIFSFTVMUT
^
^
We will discuss each section of the request, such as pagination, sort, and so on, in the
following sections. The structure of the response body is shown here:
\
UPPL5JNF&MBTUJDTFBSDIUPPLUPFYFDVUFUIFRVFSZ
UJNFE@PVU%JEUIFRVFSZUJNFPVU#ZEFGBVMUUIFSFJTOPUJNFPVU
&MBTUJDTFBSDIEPFTO	UGBJMUIFSFRVFTUJGTPNFTIBSETEPO	USFTQPOEPS
OPUBWBJMBCMF5IFSFTQPOTFXJMMDPOUBJOQBSUJBMSFTVMUT
@TIBSET\
UPUBM/VNCFSPGTIBSETUIFRVFSZOFFETUPCFFYFDVUFE
TVDDFTTGVM/VNCFSPGTIBSETUIFRVFSZJTTVDDFTTGVMPO

All About Search
[ 161 ]
GBJMFE/VNCFSPGTIBSETUIFRVFSZGBJMFE
^
IJUT\
UPUBM5PUBMOVNCFSPGIJUT
NBY@TDPSF.BYJNVNTDPSFPGBMMUIFEPDVNFOUT
IJUT<
"DUVBMEPDVNFOUT
>
^
^
Basic query (finding the exact value)
The basic query in Elasticsearch is term query. It is very simple and can be used to query
numbers, boolean, dates, and text. Term query is used to look up a single term in the
inverted index. Match query, on the other hand, takes care of the mapping and calls term
query internally. Match query should be your go-to query. We will describe the differences
in detail in the Term versus Match query section.
A simple term query looks like the following:
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
"term":\
QSPEVDU@OBNFKBDLFU
^
^
^
Term query works great for a single term. To query more than one term, we have to use
terms query. It is similar to JODMBVTF in a relational database. If the document matches
any one of the terms, it's a match. For example, we want to find all the documents that
contain KBDLFU or GMFFDF in the product name. The query will look like the following:
1045DIBQUFS@TFBSDI
\
RVFSZ\
"terms":\
QSPEVDU@OBNF<KBDLFUGMFFDF>
^
^
^
www.ebook3000.com

All About Search
[ 162 ]
The response of the query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>

All About Search
[ 163 ]
^
^
Pagination
In the previous section, we learned about the basic query in Elasticsearch. In this section, we
will discuss how to limit the number of results in the response. Pagination is supported
using the GSPN and TJ[F fields in the query. The default page size is . A query to limit
the number of documents to  is shown here:
1BHJOBUJPO
1045DIBQUFS@TFBSDI
\
 "from" : 0,
   "size" : 2,
RVFSZ\
NBUDI\
QSPEVDU@OBNFXPPMKBDLFU
^
^
^
In the preceding query, we are getting the top  results. If the page size is , to get the
third page, we would use GSPN as  and TJ[F as . Pagination in a distributed system is
very expensive when compared to a traditional database. The data belonging to an index is
spread across multiple shards. For example, if the index consists of five shards, to get the
third page, all five shards send the top thirty results to the coordinating node. The
coordinating node, in turn, sorts the results from all the shards and sends the results from
 to . The higher the page number, more expensive is the query.
Sorting based on existing fields
Elasticsearch is a full-text search engine, and the results are ordered by the relevance of each
document to the query. We will talk about relevance in detail in the Relevance section. In this
section, we will only discuss sorting the results based on the existing fields in the document.
You can sort the results based on one or more fields. For example, we want to sort the
product based on the price. A simple sort query is shown here:
4PSU
1045DIBQUFS@TFBSDI
\
RVFSZ\
www.ebook3000.com

All About Search
[ 164 ]
NBUDI\
QSPEVDU@OBNFKBDLFU
^
^
"sort": {
     "unit_price": {
       "order": "desc"
     }
   }
^
We can also sort using multiple fields. In the following query, we will sort the results by the
price and number of reviews:
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI\
QSPEVDU@OBNFKBDLFU
^
^
TPSU<
 {
       "unit_price": {
         "order": "desc"
       }
     },
     {
       "reviews": {
         "order": "desc"
       }
     }
>
^
When sorting using multiple fields, the results are first sorted by first field
and then by other fields. If the field values for the first field are unique, the
other sort will have no effect. Only if two or more documents have the
same field value for the first field, the second field is used to sort.
When we sort by multiple fields like in the preceding example, where the documents are
first sorted by the price and then by the number of reviews.

All About Search
[ 165 ]
Selecting the fields in the response
When you execute a query, by default, the original JSON document which is stored as
@TPVSDF field in the document is returned in the response. Sometimes we want to only
include certain fields in the response, especially when the document is big or has lot of
fields.
This can be done by specifying the fields in @TPVSDF field during the query. The fields
values are fetched from the document @TPVSDF. If the source is disabled via mapping, this
won't work. The response of the following query will only have the QSPEVDU@OBNF field in
the response:
1045DIBQUFSQSPEVDU@TFBSDI
\
"_source": [
     "product_name"
   ],
RVFSZ\
UFSN\
QSPEVDU@OBNFKBDLFU
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Men's Water Resistant Jacket"
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
www.ebook3000.com

All About Search
[ 166 ]
"product_name": "Women's wool Jacket"
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Men's High Performance Fleece Jacket"
^
^
>
^
^
The @TPVSDF field also supports wildcards for the fields names. We can tell Elasticsearch
only to include the fields that start with QS, as shown here:
1045DIBQUFSQSPEVDU@TFBSDI
\
@TPVSDF<
"pr*"
>
RVFSZ\
UFSN\
QSPEVDU@OBNFKBDLFU
^
^
^
When returning the fields in the document, scripts can be evaluated on the field value. For
example, along with the product price, we want to the return the price including tax. If the
sales tax is , we can use the scripted field to calculate the price with tax and return the
field value along the results. The query looks like the following:
1045DIBQUFSQSPEVDU@TFBSDI
\
@TPVSDF<>
RVFSZ\
NBUDI@BMM\^
^
TDSJQU@GJFMET\
"price_including_tax": {
TDSJQU\
 "inline" : "params['_source']['unit_price'] * 1.1"
^
^

All About Search
[ 167 ]
^
^
The default scripting language for Elasticsearch is called Painless. Using the Painless
language, we can access the @TPVSDF of the document to retrieve the VOJU@QSJDF and
multiply with  to calculate the price including tax. The response to the preceding query
is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
GJFMET\
price_including_tax<

>
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
GJFMET\
"price_including_tax"<

>
www.ebook3000.com

All About Search
[ 168 ]
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
GJFMET\
price_including_tax<

>
^
^
>
^
^
The response includes the original price and the price including tax. Although scripting is
very powerful, each document has to be parsed to extract the field value, which might come
with a performance cost.
Querying based on range
Range query can be used on numbers and dates to query for all the documents in a given
range.
Range query supports the following operators:
MU: < less than operator
HU: > greater than operator
MUF: <= less than or equal to operator
HUF: >= greater than or equal to operator
An SQL query to query all the products greater than  and less than  is shown here:
TFMFDUGSPNQSPEVDUXIFSFVOJUQSJDF BOEVOJUQSJDF

All About Search
[ 169 ]
The preceding query can be written using a range query as shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
SBOHF\
"unit_price": {
         "gt": 70,
         "lte": 100
       }
^
^
^
Similarly, a range query can also be used on dates as shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
SBOHF\
"release_date": {
         "gt": "2017-01-01",
         "lte": "now"
       }
^
^
^
We will discuss how to query dates in detail in the next section.
Handling dates
Elasticsearch makes it very easy to work with dates. It supports timezones, adding and
removing hours/days/months/years to a given date. It also supports OPX as shown in the
following example, which returns the current timestamp as datetime value. For example, to
find all the documents that were modified within the last hour, the query is shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
SBOHF\
SFMFBTF@EBUF\
"gt": "now-1h"
^
^
^
www.ebook3000.com

All About Search
[ 170 ]
^
A date string or OPX followed by date math can also be specified as follows. The || symbol
separates the date and date math operation:
1045DIBQUFS@TFBSDI
\
RVFSZ\
SBOHF\
SFMFBTF@EBUF\
"gt": "2017-01-11T20:00:00||+1M"
^
^
^
^
The following are examples of different date operations supported:
OPX.: This adds one month. You can also use a different date unit, such as an
hour, day, month, year, and so on
]].: This subtracts one month
OPXE: This is used to round to the nearest day
OPX.EE: This adds one month, subtract one day, and round to the nearest
day
]])E: This adds one hour to 2017-02-01 and rounds to the
nearest day
The following time units are supported:
Time unit Description
Z
years
.
months
X
weeks
E
days
I
hours
)
hours
N
minutes
T
seconds

All About Search
[ 171 ]
Analyzed versus non-analyzed fields
To store text values, Elasticsearch supports two mapping types:
5FYU: Text fields are analyzed (broken into individual terms) before they are
stored in the inverted index
,FZXPSE: Keyword fields are stored as they are into the inverted index
To better explain text and keyword mapping types, let's use multi-fields to index the same
field value as both text and keyword. The mapping for the QSPEVDU@OBNF field is shown
next, the mapping for the other fields in the document is irrelevant to this section. Once the
mapping is set, we will index a few example documents and run the same query on the
QSPEVDU@OBNF and QSPEVDU@OBNFLFZXPSE fields to look at the differences:
165DIBQUFS@NBQQJOHGSVJU
\
QSPQFSUJFT\
 "product_id": {
UZQFJOUFHFS
^
 "product_name": {
UZQFUFYU
"analyzer": "english",
GJFMET\
LFZXPSE\
 "type": "keyword"
^
^
^
^
^
Let's index a document:
165DIBQUFSGSVJU
\
QSPEVDU@JE
QSPEVDU@OBNF3FE(BMB"QQMF
^
Due to multi-field mapping on the QSPEVDU@OBNF, the value of the field is automatically
indexed as text and keyword. Since the QSPEVDU@OBNF field has text mapping, it will be
analyzed before it is stored in the inverted index. From the preceding example, when the
3FE(BMB"QQMF text is analyzed using the english analyzer, it is broken down into the
SFEHBMBBQQM tokens.
www.ebook3000.com

All About Search
[ 172 ]
You can also use the Analyze API to inspect the list of tokens as shown here:
(&5@BOBMZ[FBOBMZ[FSenglishUFYURed+Gala+Apple
You will see a response similar to the following:
\
UPLFOT<
\
"token": "red",
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
\
 "token": "gala",
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
\
 "token": "appl",
TUBSU@PGGTFU
FOE@PGGTFU
UZQF"-1)"/6. 
QPTJUJPO
^
>
^
However, the inverted index for the QSPEVDU@OBNFLFZXPSE field will have the exact
value (3FE(BMB"QQMF) as the mapping type is a keyword. Let's run the same query on
the QSPEVDU@OBNF and QSPEVDU@OBNFLFZXPSE fields to see how analyzed and non-
analyzed fields are stored differently:
1045DIBQUFSGSVJU@TFBSDI
\
RVFSZ\
UFSN\
"product_name": "appl"
^
^
^

All About Search
[ 173 ]
In the preceding query, we are using a term query on the QSPEVDU@OBNF field, which is
analyzed. The term query will look into the inverted index of the QSPEVDU@OBNF field and
return the documents that contain the term BQQM. The response will have a document with
the 3FE(BMB"QQMF product name.
Let's run the same query on the QSPEVDU@OBNFLFZXPSE field, which is not analyzed:
1045DIBQUFSGSVJU@TFBSDI
\
RVFSZ\
UFSN\
"product_name.keyword": "appl"
^
^
^
The response to this query does not contain any result as the inverted index of the keyword
field doesn't have an entry with BQQM. Since QSPEVDU@OBNFLFZXPSE is not analyzed, the
inverted index will only have 3FE(BMB"QQMF. Let's rerun the query:
1045DIBQUFSGSVJU@TFBSDI
\
RVFSZ\
UFSN\
QSPEVDU@OBNFLFZXPSE3FE(BMB"QQMF
^
^
^
You will see a response similar to the following:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFGSVJU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@JE
 "product_name": "Red Gala Apple"
^
^
www.ebook3000.com

All About Search
[ 174 ]
>
^
^
The response will contain only one hit as this is an exact match. Similarly, if you query for
BQQMF in the QSPEVDU@OBNFLFZXPSE field, you will not get back any results as the
inverted index of QSPEVDU@OBNFLFZXPSE only contains 3FE(BMB"QQMF.
Term versus Match query
Term query is the most commonly used query and can be used to query numbers, boolean,
dates, and text. Term query looks for the exact search input in the inverted index. Term
query is a low-level query and scores the results by default. You can use the term on
analyzed or non-analyzed fields. It does not take the mapping type into the account; it just
looks for the term in the inverted index.
A match query, unlike term, understands the field mapping. The input query is broken
down using the field analyzer. Let's take an example and discuss how match query works:
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
 "term":\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
^
^
^
When you run the preceding query, you will not see any result in the response as term
query looks for 8PNFO	TXPPM+BDLFU in the inverted index for QSPEVDU@OBNF and
does not find any matches. Now if we run the same query using a match instead of the term
query as shown next, you will see three hits in the result:
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
 "match":\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
^
^
^

All About Search
[ 175 ]
The following happens when you use a match query:
Match query first looks at the QSPEVDU@OBNF field mapping and learns that it is
an analyzed field
It uses the same analyzer used during indexing (if no analyzer is specified during
mapping, standard analyzer is default) to break the query term into tokens
Analyzing the 8PNFO	TXPPM+BDLFU query term, it will converted into
XPNFO, XPPM, and KBDLFU tokens
Next, it runs a term filter for each token and combines the results. If the match
query is run on a non-analyzed field, it will run the term filter without analyzing
the query
The response to the match query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Women's wool Jacket",
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
 "product_name": "Men's Water Resistant Jacket",
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
www.ebook3000.com

All About Search
[ 176 ]
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Men's High Performance Fleece Jacket",
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
You can see from the response that along with women's wool Jacket, two other documents
also matched the query. That's because by default, the match query is converted into a
should query, the preceding match query is equivalent to:
\
RVFSZ\
CPPM\
TIPVME<
\
UFSN\
"product_name": "womens"
^
^
\
UFSN\
"product_name": "wool"
^
^
\
UFSN\
 "product_name": "jacket"
^
^
>
^
^
^

All About Search
[ 177 ]
If you want only the documents that contain all three terms, you can change the operator to
BOE as shown here:
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\
RVFSZ8PNFO	TXPPM+BDLFU
"operator": "and"
^
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Women's wool Jacket",
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
Match phrase query
Match phrase query is similar to the match query but is used to query text phrases. Phrase
matching is necessary when the ordering of the words is important. Only the documents
that contain the words in the same order as the search input are matched.
www.ebook3000.com

All About Search
[ 178 ]
During the indexing process, along with the token, the position of the word in the text is
also saved to the inverted index. The position information is then used in the phrase query.
For example, to find an all season jacket, we will use the match phrase query on the
description field. The query is shown here:
.BUDI1ISBTF
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI@QISBTF\
 "description": "All season jacket"
^
^
^
The preceding query will not match any product as the original field value is "MMTFBTPO
GMFFDFKBDLFU and the words season and jacket are not next to each other. To relax the
ordering of the words, you can use the TMPQ parameter. The preceding query with a slop
value of  looks like the following:
.BUDI1ISBTF
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI@QISBTF\
EFTDSJQUJPO\
RVFSZ"MMTFBTPOKBDLFU
"slop": 1
^
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\

All About Search
[ 179 ]
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
 "description": "Best Value. All season fleece jacket",
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
Prefix and match phrase prefix query
Prefix query is used to find all the documents with a given prefix. Like term query, phrase
query is a low-level query and doesn't take the field mapping into consideration (refer to
the Analyzed versus non-analyzed section for more details). Terms in the inverted index are
sorted in an alphabetical order, and prefix query scans the term dictionary to find the
matching terms. For example, if we want to find all the products that start with the prefix
KB, the query is as follows:
1SFGJY
1045DIBQUFS@TFBSDI
\
RVFSZ\
"prefix": {
       "product_name": "ja"
     }
^
^
Match phrase prefix query combines match phrase and prefix query. Prefix matching is
done only on the last term in the query. The number of prefixes that are gathered from the
inverted index can be controlled using the NBY@FYQBOTJPOT parameter, which defaults to
. The Match phrase prefix query looks like the following:
.BUDI1ISBTF1SFGJY
1045DIBQUFS@TFBSDI
\
RVFSZ\
"match_phrase_prefix"\
QSPEVDU@OBNF\
"query": "Men's High Pe",
 "max_expansions": 10
^
^
www.ebook3000.com

All About Search
[ 180 ]
^
^
Wildcard and Regular expression query
Wildcard query is used for wildcard matching on the search input. Just like phrase query,
the query term is not analyzed. The following wildcards are supported:
Wildcard Description
*
Matches any character
?
Matches single character
A sample query looks like the following:
8JMEDBSE
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
XJMEDBSE\
"product_name" : "j*e?"
^
^
^
Regex query is used to find the documents based on regular expression matching. Regular
expression query can be very expensive depending on the prefix. A sample query looks like
the following:
3FHVMBS&YQSFTTJPO
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
SFHFYQ\
"product_name" : "jacke."
^
^
^
The following are the commonly used regex operators:
Operator
Description
.
Matches any character.
?
Matches zero or one character.

All About Search
[ 181 ]
*
Matches zero or more characters.
+
To handle repetitions. Pattern BQM matches, apple.
{min, max} Min and max repetitions. For example, BQ\^MF matches apple.
To avoid scanning the entire term dictionary, avoid using regex operators (such as F) at
the beginning of the prefix.
Exists and missing queries
Exists query is used to find the document that contains a non-null value for the field
specified in the query. For example, if you want to find all the document that contain
product reviews. The query looks like the following:
&YJTUT
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
exists: {
       "field" : "reviews"
     }
^
^
If you want to find all the products that don't contain reviews, we can use the bool
NVTU@OPU query to find the documents that don't contain the field. The query looks like the
following:
.JTTJOH
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
CPPM\
"must_not": {
         "exists": {
           "field": "reviews"
         }
       }
^
^
^
www.ebook3000.com

All About Search
[ 182 ]
Using more than one query
In the relational database world, we would use the BOEPS clauses to combine different
conditions. In Elasticsearch, we would use the bool query to achieve the same. Bool query
is very powerful; a bool query can have other bool queries. When combining different
queries, if matching one query is more important than the other, bool query supports
boosting individual queries. We will discuss more in the How to boost score based on queries
section. The basic structure of bool query is shown here:
\
RVFSZ\
CPPM\
NVTU<>
NVTU@OPU<>
TIPVME<>
GJMUFS<>
^
^
^
The NVTU section of the bool query contains all the must-match queries. The NVTU@OPU
section will contain all the OPU queries. The should section will contain the PS queries. The
filter section will contain the must-match queries but in non-scoring mode. For example, an
SQL query to find jackets priced under  is shown here:
TFMFDUGSPN1SPEVDUXIFSFQSPEVDU@OBNFMJLF	KBDLFU	BOEVOJU@QSJDF

The preceding SQL query can be rewritten using the bool query as shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
"bool": {
       "must": [
\
NBUDI\
QSPEVDU@OBNFKBDLFU
^
^
\
SBOHF\
VOJU@QSJDF\
MU
^
^
^

All About Search
[ 183 ]
>
^
^
^
Since the conditions are wrapped in a NVTU, only the documents that satisfy both the
conditions are returned. Bool query also supports nesting other bool queries, for example, a
user is looking for a water resistant jacket but is also okay with a performance fleece that
costs less than . An SQL query to achieve the same is shown here:
TFMFDUGSPN1SPEVDUXIFSF
QSPEVDU@OBNFMJLF	8BUFS3FTJTUBOU+BDLFU	
PS

QSPEVDU@OBNFMJLF	1FSGPSNBODF'MFFDF	BOEVOJU@QSJDF
In the preceding SQL query, the PS condition becomes a TIPVME clause, and the BOE
condition becomes a NVTU clause. The SQL query can be written using the bool query as
shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
CPPM\
"should": [ # or
\
NBUDI\
QSPEVDU@OBNF\
RVFSZ8BUFS3FTJTUBOU+BDLFU
PQFSBUPSBOE
^
^
^
\
"bool": {
             "must": [ # and
\
NBUDI\
QSPEVDU@OBNF\
RVFSZ1FSGPSNBODF'MFFDF
PQFSBUPSBOE
^
^
^
\
SBOHF\
VOJU@QSJDF\
MUF
www.ebook3000.com

All About Search
[ 184 ]
^
^
^
>
^
^
>
^
^
^
The response to the preceding query is shown here:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Men's High Performance Fleece Jacket",
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
"unit_price": 79.99,
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Men's Water Resistant Jacket",
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
"unit_price": 69.99,
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^

All About Search
[ 185 ]
Scoring is an expensive operation, and if scoring the documents is not a requirement,
individual queries can be wrapped with the filter, or the entire bool query can also be
wrapped with DPOTUBOU@TDPSF. We will discuss scoring in detail in Relevance section
below. A bool query wrapped with DPOTUBOU@TDPSF is shown next:
\
RVFSZ\
 "constant_score": {
GJMUFS\
CPPM\
NVTU<>
NVTU@OPU<>
TIPVME<>
^
^
 }
^
^
The bool query is very powerful. In the following sections, you will see that bool is the go-to
query while using Elasticsearch.
Routing
In the Routing section of $IBQUFS, Modeling Your Data and Document Relations, we
discussed how to setup the mapping to use routing and how to index documents using a
routing value. In this section, we will discuss using the routing value while running a
search query.
Routing can be very handy if you have a way to group your data. For example, you can
index all the documents that belong to the same user using the VTFS@JE as the routing
value. All the documents that are indexed using the same VTFS@JE are indexed to the same
shard. When executing a query, if you specify the routing value, Elasticsearch will execute
the search request only on one shard; the shard the query has to be executed on is
determined based on the routing value. This especially makes sense if the index has a lot of
shards. Instead of executing the request on every shard of the index, the search request has
to be executed only on one shard.
Potential downfalls of using routing are hot spots. If some users in the
system are much bigger than others, there is a possibility that the all the
big users can end up in the same shard. This leads to uneven distribution
of data and underutilization of resources and can potentially bring down
the entire cluster.
www.ebook3000.com

All About Search
[ 186 ]
The routing value can be passed as part of the request, as shown here:
1045DIBQUFSPSEFS@TFBSDIrouting=user1
\
RVFSZ\
NBUDI\
TIJQQJOH@TUBUVTTIJQQFE
^
^
^
Routing should be used only if there is a way of logically dividing your data by avoiding
hot spots.
Debugging search query
If you are ever questioning why a document did not match the query, first check the
mapping of the index/type by using the GET mapping API as shown here:
(&5DIBQUFSQSPEVDU@NBQQJOH
If the mapping is right, verify that the text field value is analyzed correctly. You can use the
Analyze API to inspect the list of tokens as shown here:
(&5@BOBMZ[FBOBMZ[FSenglishUFYUMen's+Performance+Jacket
If the analyzer is behaving as expected, make sure you are using the right kind of query. For
example, using a match instead of term query and so on.
If the mapping is not right, you cannot modify the mapping on the fly. You can add a new
field with the correct mapping and use the DPQZ@UP functionality to the copy the values to
the new field. Only the data indexed after the mapping change are copied to the new field.
The other option is to re-index the entire data after setting the correct mappings.
Relevance
A traditional database usually contains structured data. A query on a database limits the
data depending on different conditions specified by the user. Each condition in the query is
evaluated as true/false, and the rows that don't satisfy the conditions are eliminated.
However, full-text search is much more complicated. The data is unstructured, or at least
the queries are.

All About Search
[ 187 ]
We often need to search for the same text across one or more fields. The documents can be
quite large, and the query word might appear multiple times in the same document and
across several documents. Displaying all the results of the search will not help as there
could be hundreds, if not more, and most documents might not even be relevant to the
search.
To solve this problem, all the documents that match the query are assigned a score. The
score is assigned based on how relevant each document is to the query. The results are then
ranked based on the relevance score. The results on top are most likely what the user is
looking for. In the next few sections, we will discuss how the relevance is calculated and
how to tune the relevance score.
Let's query the DIBQUFS index we created at the beginning of this chapter. We will use a
simple term query to find jackets. The query is shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
UFSN\
QSPEVDU@OBNFKBDLFU
^
^
^
The response of the query looks like the following:
\

IJUT\
UPUBM
"max_score": 0.2876821,
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.2876821,
@TPVSDF\
 "product_name": "Men's Water Resistant Jacket",
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
www.ebook3000.com

All About Search
[ 188 ]
@UZQFQSPEVDU
@JE
"_score": 0.2824934,
@TPVSDF\
"product_name": "Men's High Performance Fleece Jacket",
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.25316024,
@TPVSDF\
"product_name": "Women's wool Jacket",
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
From the preceding response, we can see that each document contains a @TDPSF value. The
scores of the three jackets are as follows:
ID Product name
Score
2
Men's water-resistant jacket
0.2876821
1
Men's high-performance fleece jacket 0.2824934
3
Women's wool jacket
0.25316024
We can see that the document with the ID 2 is scored slightly higher than documents 1 and
3. The score is calculated using the #. similarity algorithm. By default, the results are
sorted using the @TDPSF values.

All About Search
[ 189 ]
At a very high level, #. calculates the score based on the following:
How frequently the term appears in the document--term frequency (tf)
How common is the term across all the documents--inverse document frequency
(idf)
Documents which contains all or most of the query terms are scored higher than
the document that don't
The normalization is based on the document length, shorter documents are
scored better than the longer ones
To learn more about how the #. similarity algorithm works, please visit
IUUQTFOXJLJQFEJBPSHXJLJ0LBQJ@#..
Not every query needs relevance. You can search for the documents that exactly match a
value, such as status, or search for the documents within a given range. Elasticsearch allows
combining both structured and full-text search in the same query. An Elasticsearch query
can be executed in a query context or a filter context. In the query context, a relevance
@TDPSF is calculated for each document matching the query. In a filter context, all the
results that match the query are returned with a default relevancy score of ; we will
discuss more details in the next section.
Queries versus Filters
By default, when a query is executed, the relevance score is calculated for each result. When
running a structured query (such as age equal to 50) or a term query on a non-analyzed
field (such as gender equal to male), we do not need scoring. As these queries are simply
answering yes/no. Calculating the relevance score for each result can be an expensive
operation. By running a query in the filter context, we are telling Elasticsearch not to score
the results.
The relevance score calculated for a query only applies to the current query context and
cannot be reused. Like we discussed in the preceding section, score is based on term and
inverted document frequency (idf), due to which the queries are not cachable. On the other
hand, filters have no relevance to the query and can be cached automatically. To run a
query in the filter context, we have to wrap the query with a DPOTUBOU@TDPSF query as
shown here:
1045DIBQUFS@TFBSDI
\
www.ebook3000.com

All About Search
[ 190 ]
RVFSZ\
DPOTUBOU@TDPSF\
GJMUFS\
UFSN\
QSPEVDU@OBNFXPPM
^
^
^
^
^
The results of the preceding query are not scored, and all the documents will have a score of
. The query runs in the filter context and can be cached. We will discuss caching further in
the Caching section. We can also run queries that need scoring in the query context and
others in the filter context. We will use the bool query to combine various queries as shown
in the following example. We discussed bool query in the Using more than one more query
section. A sample query is shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
CPPM\
NVTU<
\
NBUDI\#Query context
QSPEVDU@OBNFKBDLFU
^
^
\
 DPOTUBOU@TDPSF\ #Filter context
GJMUFS\
SBOHF\
VOJU@QSJDF\
MU
^
^
^
^
^
>
^
^
^
In the preceding query, the match query is executed in the query context, and the range
query is executed in the filter context.

All About Search
[ 191 ]
How to boost relevance based on a single field
Although the default scoring algorithm (#.) works for most cases, it is not domain or
application specific. The ranking is purely based on the content of the document.
Sometimes, a ranking based on just the content may not suffice. For example, for a user
looking for a restaurant, along with his preferences, we may also need to factor the distance
of a restaurant from the current user location. The restaurants closer to the user are scored
better than the ones that are not. In an e-commerce world, we want to factor both price and
rating of the product while scoring the products. Maybe the user can compromise on the
price of the product, given there are good ratings and so on.
Just like we used the DPOTUBOU@TDPSF query to not score the results of that query, we can
use the GVODUJPO@TDPSF query to score the results based on a single field or using the
functions specified in the query. The basic structure of the GVODUJPO@TDPSF query is as
follows:
\
RVFSZ\
GVODUJPO@TDPSF\
RVFSZ\^
GVODUJPOT<>
GJMUFS\^
GJFME@WBMVF@GBDUPS\^
^
^
^
Using GJFME@WBMVF@GBDUPS, we can boost the score using one field in the document. In
the following example, we want the products with better reviews to show up higher on the
list. Along with the full-text score, we can tell Elasticsearch to factor the number of reviews
while calculating the score. Higher the number of reviews, better the document score is. See
below:
1045DIBQUFS@TFBSDI
\
RVFSZ\
 "function_score": {
RVFSZ\
NBUDI\
QSPEVDU@OBNFKBDLFU
^
^
GJFME@WBMVF@GBDUPS\
"field": "reviews"
^
www.ebook3000.com

All About Search
[ 192 ]
^
^
^
Now, let's look at the response to the query. The actual results will not be different from the
regular query, just the ordering of the results will be different:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
"reviews": 250,
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
"reviews": 10,
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF

All About Search
[ 193 ]
 "reviews": 5,
SFMFBTF@EBUF
^
^
>
^
^
You can see from the preceding response that the document with  reviews is scored
much higher than the document with  reviews. That's because when calculating the score,
the main query is executed first, and the score is then multiple by the field value.
Field value factor can only be used to boost the score using one field. To
boost the score using multiple fields, you need to use the bool query or
functions when using the GVODUJPO@TDPSF query.
In the following query, we will use a GJFME@WBMVF@GBDUPS with a factor of . The
factor controls the effect of the number of reviews on the score. It defaults to 1:
1045DIBQUFS@TFBSDI
\
RVFSZ\
GVODUJPO@TDPSF\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\
RVFSZKBDLFU
^
^
^
GJFME@WBMVF@GBDUPS\
GJFMESFWJFXT
 "factor": "0.25"
^
^
^
^
The score is calculated using the following formula:
TDPSF@TDPSF
GBDUPSSFWJFXT
We use the factor to normalize the results. Without the factor, we would see very high score
values. The scores of the documents are as follows:
www.ebook3000.com

All About Search
[ 194 ]
Id Product Name
Reviews Score
1
Men's high-performance fleece jacket 250
7.416882
2
Men's water-resistant jacket
5
0.33382848
3
Women's wool jacket
10
0.16691424
To better normalize the effect of the number of reviews, we can also use a modifier, such as
a logarithmic function. The query with the modifier is shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
GVODUJPO@TDPSF\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\
RVFSZKBDLFU
^
^
^
GJFME@WBMVF@GBDUPS\
GJFMESFWJFXT
 "modifier": "log1p"
^
^
^
^
The scores of the documents with a logarithmic modifier are as follows:
Id Product Name
Reviews Score (Factor 0.25) Score (log1p
modifier)
1
Men's high-performance fleece jacket
250
7.416882
0.28476956
2
Men's water-resistant jacket
5
0.33382848
0.13905862
3
Women's wool jacket
10
0.16691424
0.10390762
You can see from the preceding table that the document with ID  has the highest score as it
has the most reviews. You may also want to factor both the price and the rating of the
product while calculating the relevance. Elasticsearch2VFSZ%4- supports custom scoring
or boosting individual queries. Boosting the queries enables relevance calculation based on
the different signals, such as price or distance from the user location.

All About Search
[ 195 ]
How to boost score based on queries
In this section, we will discuss how to use the bool query to combine different queries and
boost individual queries. By default, the scores of all the queries are added to calculate the
final score. The result documents must match all the NVTU queries. The more TIPVME
queries the document matches, the more relevant the document is. For example, we are
looking for jackets priced under  and have at least  reviews. We will use the match
query to match the term "jacket" in the must clause, price and reviews in the should clause.
The more should clauses the document matches, the higher the score is. Jackets that are
over  or which have reviews less than  can show up in results but are scored very
low when compared to the jackets that match one or both the should conditions. This is
similar to choosing the preferences on the navigation window, usually on the left, when
shopping on an e-commerce site. The bool query to rank the product based on the should
clause is shown here:
#PPTUJOHRVFSJFTVTJOHCPPM
1045DIBQUFS@TFBSDI
\
RVFSZ\
CPPM\
"must": [
\
NBUDI\
QSPEVDU@OBNFKBDLFU
^
^
>
 "should": [
\
SBOHF\
VOJU@QSJDF\
MU
^
^
^
\
SBOHF\
SFWJFXT\
HUF
^
^
^
>
^
^
^
www.ebook3000.com

All About Search
[ 196 ]
The response to the query is as follows:
\

IJUT\
UPUBM
"max_score": 2.1186702,
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 2.1186702,
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
"unit_price": 79.99,
           "reviews": 250,
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 1.1335313,
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
"unit_price": 69.99,
           "reviews": 5,
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 1.1335313,
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
"unit_price": 59.99,
           "reviews": 10,
SFMFBTF@EBUF
^
^
>

All About Search
[ 197 ]
^
^
The scores of the document are as follows:
Id Product Name
Price Reviews Score
1
Men's high-performance fleece jacket 79.99 250
2.1186702
2
Men's water-resistant jacket
69.99 5
1.1335313
3
Women's wool jacket
59.99 10
1.1335313
The range query is a structured query. For every range query the document matches, a
score of  is added to the final score. In the preceding example, both the price and the
number of reviews contribute to the score equally. For example, if having more reviews is
more important than the price, we can boost the reviews query. In the following query, the
score for the reviews query is boosted to , and the score from the price query is boosted to
. Only using the CPPM query, we can control how each query can contribute towards the
final score, which is not possible using TPSU:
1045DIBQUFS@TFBSDI
\
RVFSZ\
CPPM\
NVTU<
\
NBUDI\
QSPEVDU@OBNFKBDLFU
^
^
>
TIPVME<
\
SBOHF\
VOJU@QSJDF\
MU
"boost": 0.5
^
^
^
\
SBOHF\
SFWJFXT\
HUF
"boost": 2
^
^
www.ebook3000.com

All About Search
[ 198 ]
^
>
^
^
^
The scores of the document are as follows:
Id Product Name
Price Reviews Score
1
Men's high-performance fleece jacket 79.99 250
2.1186702
2
Men's water-resistant jacket
69.99 5
0.6335314
3
Women's wool jacket
59.99 10
0.6335314
You can see from the response that due to  boost on the price, the score for the
documents with ID  and  has been reduced to  from . In the next section, we
will factor the release date so that the documents that are released recently are scored more
and thereby ranked higher in the results.
How to boost relevance using decay functions
In the previous section, we used the number of reviews and the price of the product to
compute the relevance score. In this section, we will promote the recent products using
their release date. We want to score the most recently released products higher than the
older ones. By the end of this section, we will combine the scores from the main search, the
number of reviews, and the release date.
We can start by defining range queries for different time intervals and wrap them in a bool
query. Assuming that products released last year are less important than those released in
the current year, in the following query, products that are released in the current year are
boosted by , the products released in last year are boosted by :
1045DIBQUFS@TFBSDI
\
RVFSZ\
CPPM\
TIPVME<
\
SBOHF\
SFMFBTF@EBUF\
"gte": "now/y",
"boost": 1
^

All About Search
[ 199 ]
^
^
\
SBOHF\
SFMFBTF@EBUF\
HUFOPXZZ
MUFOPXZ
"boost": "0.5"
^
^
^
>
^
^
^
In the preceding query, OPXZ represents the current date rounded to the year. Please refer
to the Handling dates section for more details on different date functions that are supported.
Using bool queries to score based on different intervals is not very practical. Products that
are released in January last year are scored equally as December last year. Regardless of the
month a product is released in, it will be classified in the same bucket as all the products in
a given year. To calculate a more accurate score for numeric values, dates and geo-location
decay functions can be used.
Decay functions work by choosing an origin value and decrease the score as the value
moves away from the origin. For example, a user is looking for restaurants close by. We can
use his current location as an origin and score the restaurants based on the distance from
the origin. The restaurants close by are scored higher than the restaurants further away. By
using decay functions, the score is gradually decreased instead of a sudden drop.
In the previous sections, we used the number of reviews to calculate the relevance score; in
this section, we will use the release date along with the number of reviews to calculate the
score. To use a decay function, we have to specify the origin and scale. The scale parameter
defines the rate at which the score is decreased when moving away from the origin, as
shown below:
1045DIBQUFS@TFBSDI
\
RVFSZ\
GVODUJPO@TDPSF\
GVODUJPOT<
\
HBVTT\
SFMFBTF@EBUF\
"origin": "now",
               "scale": "180d"
www.ebook3000.com

All About Search
[ 200 ]
^
^
^
>
^
^
^
Note how we used OPX as origin and E as the scale. Elasticsearch is
calendar aware and will automatically convert these to the appropriate
dates.
With the assumption that new products are released every  months, we set the scale to 180
days. A product that is released close to the current date receives a score of , and a product
released six months prior receives a score of . In the preceding query, we used the
Gaussian function. Depending on how you would like the score to decline, whether it be a
gradual, continuous curve or a sudden fall, you can choose between linear, exponential, or
Gaussian function types.
Although, Elasticsearch supports different time units, such as year, month,
week, day, and hour, at the time of writing, anything beyond days is
currently throwing an exception. As an alternative, I represented 6 months
as 180 days.
Now is "QSJMUI. The scores of the documents are as follows:
ID Product Name
Release Date Score
2
Men's water-resistant jacket
2017-03-02
0.97225076
3
Women's wool jacket
2016-12-15
0.7599717
1
Men's high-performance fleece jacket 2016-08-16
0.30909336
Next, along with the release date, we want to add a function for the price. For example, a
user is trying to find products that are priced around  and that are recently released. We
will the GVODUJPO@TDPSF query to group the functions for the price and the release date.
By default, the scores of the functions are multiplied with each other to calculate the final
score. For example, if the product that is just released has a very high price. Multiplying the
scores from both the functions uses the scores from both the functions. Other scoring modes
such as sum, avg, min, max, and first are also supported, as shown below:
1045DIBQUFS@TFBSDI
\
RVFSZ\

All About Search
[ 201 ]
"function_score": {
GVODUJPOT<
\
"gauss": {
            "unit_price": {
PSJHJO
TDBMF
^
^
^
\
"gauss": {
            "release_date": {
PSJHJOOPX
TDBMFE
^
^
^
>
^
^
^
The results to the preceding query are as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.55876404,
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
www.ebook3000.com

All About Search
[ 202 ]
"_score": 0.28387982,
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.019349905,
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
The scores of the document are as follows:
ID Product Name
Release Date Price Score
3
Women's wool jacket
2016-12-15
59.99 0.55876404
2
Men's water-resistant jacket
2017-03-02
69.99 0.28387982
1
Men's high-performance fleece jacket 2016-08-16
79.99 0.019349905
We can also boost the scores of individual functions. If the release date is more important
than the price, we can boost the queries as shown next:
1045DIBQUFS@TFBSDI
\
RVFSZ\
GVODUJPO@TDPSF\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\
RVFSZKBDLFU
^
^

All About Search
[ 203 ]
^
GVODUJPOT<
\
HBVTT\
"unit_price": {
PSJHJO
TDBMF
^
^
"weight": 1
^
\
HBVTT\
"release_date": {
PSJHJOOPX
TDBMFE
^
^
"weight": 2
^
>
^
^
^
The result to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.14921477,
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
www.ebook3000.com

All About Search
[ 204 ]
@UZQFQSPEVDU
@JE
"_score": 0.07581205,
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.0045918548,
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
The scores of the preceding documents are as follows:
ID Product Name
Price Release Date Score
3
Women's wool jacket
59.99 2016-12-15
0.14921477
2
Men's high-performance fleece jacket 69.99 2017-03-02
0.07581205
1
Men's water-resistant jacket
79.99 2016-08-16
0.0045918548
From the preceding response, you can see that the most expensive jacket, which was
released a while ago, was scored the lowest. Relevance calculation entirely depends on your
application and domain-specific logic. Depending on what is more important, you can
influence the factors that contribute towards the final score.

All About Search
[ 205 ]
Rescoring
For a query that matches a lot of documents, scoring all the matching documents can be
quite expensive. To reduce this cost and improve the precision, Elasticsearch lets you
rescore only the top N documents without scoring all the documents that match the query.
Expensive scoring queries, such as scripting or using geolocation, can be used in the rescore
phrase. The scores from the original query and the rescore query are combined for the final
score. For example, you want the distance as one of the factors driving the relevance score,
but the geo distance query which is used to calculate the distance is expensive. You can use
the main query to calculate the score for all the documents and then use rescoring to order
only the top N documents based on the distance. For suppose, we want to score the top 10
products based on the reviews. A rescoring query based on script is shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\
RVFSZKBDLFU
^
^
^
SFTDPSF\
"window_size": 10,
RVFSZ\
SFTDPSF@RVFSZ\
GVODUJPO@TDPSF\
TDSJQU@TDPSF\
TDSJQU\
 "inline": "Math.log(params['_source']['reviews'])"
^
^
^
^
 "query_weight": 0.5,
       "rescore_query_weight": 1.0
^
^
^
In the preceding query, the rescore query applies the logarithmic function on the reviews
field of each hit. You can see from the following response that the score for the document
with  reviews is not significantly different from the product with  reviews. By
applying the MPH function, we normalized the effect of the reviews on the score.
www.ebook3000.com

All About Search
[ 206 ]
All the documents that match the original query are scored using the default #.
similarity algorithm. Next, for the documents specified in XJOEPX@TJ[F(10), the rescore
query is executed, and the influence of the rescore query is defined using RVFSZ@XFJHIU
and SFTDPSF@RVFSZ@XFJHIU. The default weight is  for both the queries. The response to
the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
 "_score": 5.5807962,
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
 "reviews": 250,
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
 "_score": 2.369351,
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
"reviews": 10,
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 1.6762036,
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF

All About Search
[ 207 ]
 "reviews": 5,
SFMFBTF@EBUF
^
^
>
^
^
Debugging relevance score
To debug how the relevance score is calculated for the text-search query, you can use the
explain query parameter during the search as shown here:
1045DIBQUFS@TFBSDIexplain
\
RVFSZ\
NBUDI\
EFTDSJQUJPOIJLJOH
^
^
^
The response of the query will have the explanation of the calculation. Each hit along with
the source of the document will also contain the details of how the score is calculated. Let's
take explanation of a single hit and walk through:
@FYQMBOBUJPO\
WBMVF
EFTDSJQUJPOXFJHIU
description:hikeJO
<1FS'JFME4JNJMBSJUZ>SFTVMUPG
The explanation will contain the score of the hit, description and the details. The value
(0.98908687) is the score calculated for this document. We are searching for term hike in the
description field. Next are the details on how the score (0.98908687) is calculated:
EFUBJMT<
\
WBMVF
EFTDSJQUJPOTDPSF
EPDGSFRtermFreq=1.0O
QSPEVDUPG
EFUBJMT<
\
WBMVF
EFTDSJQUJPOJEG
docFreq=1, docCount=3
EFUBJMT<>
^
www.ebook3000.com

All About Search
[ 208 ]
The final score of the document is an outcome of different calculations as shown in the
preceding code snippet. The term frequency represents the number of times the term
appears in the document. The inverted document frequency represents the number of
documents the term appears in. If the term is common in the document and relatively rare
in all the documents, it is scored better. The term frequency normalization shown next
reduces the effect of term frequency beyond a limit:
\
WBMVF
EFTDSJQUJPOtfNormDPNQVUFEGSPN
EFUBJMT<
\
WBMVF
EFTDSJQUJPOUFSN'SFR
EFUBJMT<>
^
\
WBMVF
EFTDSJQUJPOQBSBNFUFSL
EFUBJMT<>
^
\
WBMVF
EFTDSJQUJPOQBSBNFUFSC
EFUBJMT<>
^
\
WBMVF
EFTDSJQUJPOBWH'JFME-FOHUI
EFUBJMT<>
^
\
WBMVF
EFTDSJQUJPOGJFME-FOHUI
EFUBJMT<>
^
>
^
>
^
>
^
Try running some text queries with explain parameter and the explanation will more more
sense. The term frequency normalization shown above is the different factors used by #.
to reduce the effect of parameters like field length.

All About Search
[ 209 ]
To learn more about how the #. similarity algorithm works, please visit
IUUQTFOXJLJQFEJBPSHXJLJ0LBQJ@#..
Explain is very handy when trying to understand or debug how the relevance score is
calculated. Don't let the length of the response scare you; the most important things to
watch are the term frequency, inverted document frequency, and term frequency
normalization.
Searching for same value across multiple
fields
The NVMUJ@NBUDI query is used to match the same value across multiple fields. When a
user searches for CJLJOHKBDLFU, searching just the QSPEVDU@OBNF field might not find
any matches. To widen the search, we should most probably also search the EFTDSJQUJPO
field along with the QSPEVDU@OBNF field. The document that contains both CJLJOH and
KBDLFU is shown here:
\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOUJacket
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHbikingBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
Scoring based on a single field is pretty straightforward, scoring based on multiple fields
gets tricky. We can't use a match query with an operator as the terms CJLJOH and KBDLFU
don't exist in the same field. We need to use a NVMUJ@NBUDI query to search across
QSPEVDU@OBNF and EFTDSJQUJPO fields:
.VMUJ.BUDI
(&5DIBQUFS@TFBSDI
\
RVFSZ\
NVMUJ@NBUDI\
"query": "biking jacket",
       "fields": [
         "product_name",
         "description"
       ],
www.ebook3000.com

All About Search
[ 210 ]
"type": "best_fields"
^
^
^
By default, the NVMUJ@NBUDI query runs as CFTU@GJFMET. The score from the best
matching field is returned. We can also use a UJF@CSFBLFS to add the scores from the other
fields, as shown here:
.VMUJ.BUDI
1045DIBQUFS@TFBSDI
\
RVFSZ\
NVMUJ@NBUDI\
RVFSZCJLJOHKBDLFU
GJFMET<
QSPEVDU@OBNF
EFTDSJQUJPO
>
 "tie_breaker": 0.2
^
^
^
The scores from the other fields are multiplied by  and added to the best matching field
score. .VMUJNBUDI is internally translated into a EJT@NBY query. Unlike the CPPM query,
which adds all the scores from all the queries, the EJT@NBY query returns the score of the
best matching query:
%JTNBY2VFSZ
1045DIBQUFS@TFBSDI
\
RVFSZ\
"dis_max":\
RVFSJFT<
\
NBUDI\
QSPEVDU@OBNFCJLJOHKBDLFU
^
^
\
NBUDI\
EFTDSJQUJPOCJLJOHKBDLFU
^
^
>
^

All About Search
[ 211 ]
^
^
.VMUJNBUDI also provides some nice shortcuts to boost the score from individual queries.
If matching the QSPEVDU@OBNF field is more important than matching the EFTDSJQUJPO
field, we can boost the QSPEVDU@OBNF using the caret (^) symbol as shown here:
1045DIBQUFS@TFBSDI
\
RVFSZ\
NVMUJ@NBUDI\
RVFSZCJLJOHKBDLFU
GJFMET<
"product_name^2",
EFTDSJQUJPO
>
^
^
^
Depending on how you want the documents to be scored, three types of NVMUJ@NBUDI
queries are supported. The three types differ in how the scores from different fields are
used to calculate the final score of each document. In the next sections, we will run the same
query using a different type of NVMUJ@NBUDI and look at the how the documents are
scored.
Best matching fields
Best matching fields are the default type, and the score of the best matching field is
returned. Let's inspect the scores of the documents when running the NVMUJ@NBUDI query
as CFTU@GJFMET. For example, if you looking the term KBDLFU in QSPEVDU@OBNF and
EFTDSJQUJPO fields, the NVMUJ@NBUDI query is as follows:
.VMUJ.BUDI
#FTU'JFMET
1045DIBQUFS@TFBSDI
\
RVFSZ\
NVMUJ@NBUDI\
  "query": "jacket",
GJFMET<
QSPEVDU@OBNF
EFTDSJQUJPO
>
UZQFCFTU@GJFMET
"analyzer" : "english"
www.ebook3000.com

All About Search
[ 212 ]
^
^
^
By default, the query term is analyzed and broken into individual terms based on the field
analyzer. For example, we are querying for men's jacket. Using the standard analyzer, the
query is broken into NFO	T and KBDLFU. When using the English analyzer, the query will be
broken into NFO and KBDLFU. By specifying the analyzer, we are asking Elasticsearch to use
the same analyzer for all fields irrespective of their mappings. All the fields are queried for
the same terms. The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.86312973
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDFJacket
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFjacket
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.13353139,
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOUJacket
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE

All About Search
[ 213 ]
"_score": 0.13353139,
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPMJacket
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
The scores of the documents are as follows:
ID Product name
Description
Score
1
Men's high-performance fleece jacket
Best value. All season fleece jacket.
0.86312973
2
Men's water-resistant jacket
Provides comfort during biking and
hiking.
0.13353139
3
Women's wool jacket
Helps you stay warm in winter.
0.13353139
The maximum score between the QSPEVDU@OBNF and EFTDSJQUJPO fields is returned as
the final score. The term KBDLFU appears in the QSPEVDU@OBNF fields of all the documents
but only once in the EFTDSJQUJPO field of document ID 1. Document ID 1 is scored
relatively higher because of the inverted document frequency. When querying the
EFTDSJQUJPO field for the term KBDLFU, it is a rare term due to which it is scored very high
when compared to others.
Most matching fields
The .VMUJ@NBUDI query, when executed as most matching fields type, it adds the scores
from all the fields. We will run the same query in the preceding section but set the type as
NPTU@GJFMET. The query looks like the following:
.VMUJ.BUDI
.PTU'JFMET
1045DIBQUFS@TFBSDI
\
RVFSZ\
NVMUJ@NBUDI\
RVFSZKBDLFU
GJFMET<
QSPEVDU@OBNF
EFTDSJQUJPO
www.ebook3000.com

All About Search
[ 214 ]
>
"type": "most_fields",
BOBMZ[FSFOHMJTI
^
^
^
The result to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
 "_score": 0.98179984,
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.13353139,
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
 "_score": 0.13353139,
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU

All About Search
[ 215 ]
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
The scores of the documents are as follows:
ID Product name
Description
best_fields
score
most_fields
score
1
Men's high-performance
fleece jacket
Best value. All season
fleece jacket.
0.86312973
0.98179984
2
Men's water-resistant jacket
Provides comfort
during biking and
hiking.
0.13353139
0.13353139
3
Women's wool jacket
Helps you stay warm in
winter.
0.13353139
0.13353139
The score of the document ID 1 is the sum of the scores from the QSPEVDU@OBNF and
EFTDSJQUJPO fields. The scores for the other documents remain unchanged.
Cross-matching fields
The best fields and most fields execute the query on the different fields and combine the
scores to produce the final score. In the previous section, we saw how the score is heavily
influenced due to the term frequency of the word KBDLFU. That's because of how the #.
scoring algorithm works. But if you don't want the score to be influenced because of the
term frequencies, you could use DSPTT@GJFMET. Cross fields combine both QSPEVDU@OBNF
and EFTDSJQUJPO as a single field, and the scores are calculated based on the single field.
The query looks like the following:
.VMUJ.BUDI
$SPTT'JFMET
1045DIBQUFS@TFBSDI
\
RVFSZ\
NVMUJ@NBUDI\
RVFSZKBDLFU
GJFMET<
www.ebook3000.com

All About Search
[ 216 ]
QSPEVDU@OBNF
EFTDSJQUJPO
>
"type": "cross_fields",
BOBMZ[FSFOHMJTI
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
"max_score": 0.13353139,
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.13353139,
@TPVSDF\
QSPEVDU@OBNF.FO	T8BUFS3FTJTUBOU+BDLFU
EFTDSJQUJPO1SPWJEFTDPNGPSUEVSJOHCJLJOHBOEIJLJOH
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
"_score": 0.13353139,
@TPVSDF\
QSPEVDU@OBNF8PNFO	TXPPM+BDLFU
EFTDSJQUJPO)FMQTZPVTUBZXBSNJOXJOUFS
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
 "_score": 0.11867011,

All About Search
[ 217 ]
@TPVSDF\
QSPEVDU@OBNF.FO	T)JHI1FSGPSNBODF'MFFDF+BDLFU
EFTDSJQUJPO#FTU7BMVF"MMTFBTPOGMFFDFKBDLFU
VOJU@QSJDF
SFWJFXT
SFMFBTF@EBUF
^
^
>
^
^
The scores of the documents are as follows:
ID Product name
Description
best_fields most_fields cross_fields
1
Men's high-performance
fleece jacket
Best value. All season
fleece jacket.
0.86312973
0.98179984
0.11867011
2
Men's water-resistant
jacket
Provides comfort
during biking and
hiking.
0.13353139
0.13353139
0.13353139
3
Women's wool jacket
Helps you stay warm
in winter.
0.13353139
0.13353139
0.13353139
When both the QSPEVDU@OBNF and EFTDSJQUJPO fields are combined, the term KBDLFU in
the combined field is a common term. Due to inverted frequency, it is scored less when
compared to others. Depending on the application requirements, choose the correct type of
the NVMUJ@NBUDI query.
Caching
In Elasticsearch 5.0, a lot of refactoring has been done to support better caching. The
different types of cache available are as follows:
Node Query cache: Queries that run in filter context are cached here
Shard request cache: The results of the entire query are cached here
www.ebook3000.com

All About Search
[ 218 ]
Node Query cache
Queries, such as numeric or date range, which run in the filter context are great candidates
for caching. Since they have no scoring phase, they can be reused. The Node query cache is
a smart cache; you do not have to worry about invalidating the cache. Individual queries
that run in filter context are cached here. This cache is maintained at a node level and
defaults to  of the heap and can be configured using the FMBTUJDTFBSDIZNM file:
JOEJDFTRVFSJFTDBDIFTJ[F
Frequently used queries are automatically cached, and the eviction from the cache is based
on the least recently used queries.
Shard request cache
Shard request cache is used to cache the results of the entire query. Only hits count,
aggregation, and suggestions are cached; the actual hits are not. The results are only cached
if the size is equal to  (when size is set to  in the request, the actual hits/documents are not
returned as part of the search response). The query JSON is used as a cache key. Just like
query cache, shard request cache is smart. The cache is invalidated automatically when
there is new data. For data that is not changed frequently, the query can be served directly
from the cache avoiding the heavy re-computation. The cache is maintained at a node level
and defaults to  of the heap memory and can be configured using the
FMBTUJDTFBSDIZNM file:
JOEJDFTSFRVFTUTDBDIFTJ[F
Charts on a dashboard that uses aggregations internally greatly benefit from a shard
request cache. If the data is not changing anymore, the request can be served directly from
the cache.

All About Search
[ 219 ]
Summary
In this chapter, you learned how to query Elasticsearch. We discussed the differences
between structured queries and full-text search. We also discussed how to combine
different queries using bool query. We learned what relevance means and how it is
calculated. We used factors such as price and release date to tune the relevance score.
In the next chapter, we will discuss more advanced features, such as location-based
filtering, autocomplete, making suggestions based on the user query, and more.
www.ebook3000.com

7
More Than a Search Engine
(Geofilters, Autocomplete, and
More)
In the previous chapter, we discussed different ways of querying Elasticsearch. For a given
query, we looked into different ways of controlling the relevance score to get back the most
relevant results at the top. In this chapter, we will discuss how to deal with typos, spelling
mistakes, and auto-completing the query before the user finishes typing the query. We will
talk about how to handle relationships and joins using nested and parent-child mappings
and discuss the advantages and disadvantages of using one versus the other. We will also
discuss how to include geolocation in your queries. Youâ€™ll learn about the percolate query,
which is one of the very popular features of Elasticsearch. The main functionality of
percolate query is reverse search; we will explore why it is important and the ways to use it.
We will cover the following in this chapter:
Correcting typos and spelling mistakes
Autocomplete
Highlighting
Handling relations and joins
Geo and spatial filtering
Percolate query
Scripting

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 221 ]
Sample data
To better explain the various concepts in this chapter, we will use the e-commerce site as an
example. We will create an index with a list of products. We will create a simple index
called DIBQUFS with type QSPEVDU. Our sample data looks like the following:
165DIBQUFSQSPEVDU
\QSPEVDU@OBNF"QQMFJ1IPOF^
165DIBQUFSQSPEVDU
\QSPEVDU@OBNF"QQMFJ1IPOF-JHIUOJOH$BCMF^
165DIBQUFSQSPEVDU
\QSPEVDU@OBNF"QQMFJ1IPOF^
165DIBQUFSQSPEVDU
\QSPEVDU@OBNF4BNTVOH(BMBYZ4^
165DIBQUFSQSPEVDU
\QSPEVDU@OBNF4BNTVOH(BMBYZ4^
As we progress through the chapter, we will recreate the DIBQUFS index with different
configurations.
Correcting typos and spelling mistakes
In the previous chapter, we discussed different ways to query documents based on the user
search input. But the search input might contain typos and spelling mistakes. Automatically
correcting the user's spelling mistakes and typos improves the overall search experience.
The term or match query that we discussed in the previous chapter only looks for the exact
term in the inverted index. In this section, we will discuss different types of queries
Elasticsearch provides to correct the typos.
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 222 ]
Fuzzy query
The fuzzy query is provided to look for terms that are close to the original term. It looks for
terms in the inverted index, which are like the query term based on the edit distance. Edit
distance is the number of operations required to make two words similar. An operation is
either addition, deletion, or substitution. The edit distance between motel and hotel is 1 (
hotel -> motel ) as you have to substitute m with h. The edit distance between Sunday and
Monday is 2 ( Sunday -> Munday -> Monday ) as you have to substitute s with m and u with o.
Edit distance is also known as Levenshtein distance. The fuzzy query is used to
automatically correct typos and spelling mistakes in the user query.
Fuzzy query checks the inverted index for possible matches within edit
distance of 2. Most typos and spelling mistakes fall within this. Beyond 2,
the probability of two words meaning the same is very less.
All the heavy lifting of calculating edit distance and matching the terms is done by
Elasticsearch. For example, if a user queries for TBNTVOE instead of TBNTVOH, we can use
the fuzzy query to correct the typos automatically, as shown here:
'V[[Z2VFSZ
1045DIBQUFS@TFBSDI
\
RVFSZ\
"fuzzy": {
       "product_name": "samsund"
     }
^
^
The response to the preceding query will contain documents with product names within an
edit distance of  to TBNTVOE. The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
 "product_name": "Samsung Galaxy S6"

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 223 ]
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
"product_name": "Samsung Galaxy S7"
^
^
>
^
^
Instead of the fuzzy query, we can also use the regular match query and include the
GV[[JOFTT attribute as shown here:
'V[[Z.BUDI2VFSZ
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\
RVFSZTBNTVOEHBMB[Z
"fuzziness": "AUTO",
PQFSBUPSBOE
^
^
^
^
In the preceding query, since we set the GV[[JOFTT property to "650, the fuzziness is based
on the length of the query term as shown in the table here:
Term Length
Fuzziness (Edit Distance)
1-2
0
3-5
1
Greater than 5 2
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 224 ]
For example, in the previous query, the length of term TBNTVOE is greater than 5, so the
query looks for terms within the edit distance of  in the inverted index. Instead of "650,
you can also specify 0, 1, or 2.
The fuzzy query can be very expensive depending on the number of terms in the inverted
index that match the input. To restrict the performance impact, we can limit the number of
terms collected from inverted index using the NBY@FYQBOTJPOT parameter, which defaults
to . In the older versions of Lucene, fuzzy queries are very expensive, as it has to go
through the entire inverted index to find the matching terms. The newer version of Lucene
calculates the edit distance between two words using much simpler and faster algorithm
called Levenshtein automata. The algorithm represents each letter in the word as a node in
the graph.
Making suggestions based on the user input
In the previous section, we discussed fuzzy query to fix the typos automatically. In this
section, we will discuss the suggest API, which can provide word or phrase suggestions to
the user based on the input query. Fuzzy query automatically corrects the fuzziness;
Suggest API simply makes suggestions. Suggest API supports the following:
Term and phrase suggester: You can use the term or phrase suggester to make
suggestions based on the existing documents in case of typos or spelling
mistakes.
Completion suggester: You can use the completion suggester to predict the
query term before the user finishes typing. Helping the user with the right search
phrases improves the overall experience and decreases the load on the servers.
Implementing "did you mean" feature
The term and phrase suggester uses edit distance just like the fuzzy query to make the
suggestions based on the user query. When using Google or other search engines, you often
see suggestions as did you mean followed by the suggestion.

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 225 ]
Elasticsearch supports the following:
Term suggester: This is used to make suggestions based on a single term
Phrase suggester: This is used to make suggestions based on a phrase
Term suggester
The term suggester makes suggestions based on a single term. For example, if a user is
looking for TBNTVOH phones and inputs TBNTVOE as a search query. Since TBNTVOE has a
spelling mistake, it doesn't exist in the inverted index. We can use UFSN@TVHHFTUFS to
display suggestions based on the user input. A simple term suggest query would look like
the following:
5FSN4VHHFTUFS
1045DIBQUFS@TFBSDI
\
TVHHFTU\
UFSN@TVHHFTUFS\
"text" : "samsund",
UFSN\
"field" : "product_name"
^
^
^
^
Note the following from the preceding query:
text: The query term (term that needs suggestions) is passed as the text field
field: Suggestions are based on this field. In the preceding example, we used
QSPEVDU@OBNF, but we could also use the @BMM field
The response to the term suggest query is as follows:
\

TVHHFTU\
UFSN@TVHHFTUFS<
\
 "text": "samsund",
PGGTFU
MFOHUI
PQUJPOT<
\
"text": "samsung",
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 226 ]
TDPSF
GSFR
^
>
^
>
^
^
Each suggestion in the response will have text, score, and frequency:
text: This denotes the suggestion text.
score: This denotes how close the suggestion is to the text we want the suggestion
on.
freq: This denotes the frequency of the term in all the documents. In the
preceding example, the TBNTVOH term has appeared twice.
By default, the suggestions are sorted based on the frequency. Let's take one more example:
5FSN4VHHFTUFS
1045DIBQUFS@TFBSDI
\
TVHHFTU\
UFSN@TVHHFTUFS\
"text" : "samsundcd",
UFSN\
GJFMEQSPEVDU@OBNF
^
^
^
^
The preceding query will not find any suggestions for TBNTVOEDE as there are no terms
within an edit distance of . The edit distance between TBNTVOH and TBNTVOEDE is .
The suggester makes suggestions irrespective of the query in the request. In the example
below, the query for JQIPOF doesn't affect the suggestions for the term TBNTVOEE. The
scope of the suggester includes all the documents in the index. The following query will
result in the same suggestions as in the previous example. The only difference between the
previous example and the following query is that the response will contain hits that match
the query along with the suggestions as shown here:
5FSN4VHHFTUFSXJUINBUDIRVFSZ
1045DIBQUFS@TFBSDI
\
RVFSZ\

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 227 ]
NBUDI\
QSPEVDU@OBNFJQIPOF
^
^
TVHHFTU\
UFSN@TVHHFTUFS\
UFYUTBNTVOEE
"term" : {
GJFMEQSPEVDU@OBNF
^
^
^
^
The response to the preceding query is as follows:
\

"hits": {
     "total": 2,
NBY@TDPSF
IJUT<
\

@TPVSDF\
QSPEVDU@OBNFJ1IPOF
^
^
\

@TPVSDF\
QSPEVDU@OBNFJ1IPOF
^
^
>
^
"suggest": {
     "term_suggester": [
\
UFYUTBNTVOEE
PGGTFU
MFOHUI
PQUJPOT<
\
UFYUTBNTVOH
TDPSF
GSFR
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 228 ]
>
^
>
^
^
The suggest API supports more than one suggester in the same query. For example, in the
following query, we have two suggesters: one for TBNTVOE and the other for JQIPO:
.PSFUIBOPOFTVHHFTUFS
1045DIBQUFS@TFBSDI
\
TVHHFTU\
TVHHFTUFS\
"text" : "samsund",
UFSN\
GJFME@BMM
^
^
TVHHFTUFS\
"text" : "iphon",
UFSN\
GJFME@BMM
^
^
^
^
The response to the preceding query will have suggestions to both the suggesters as shown
here:
\

TVHHFTU\
TVHHFTUFS<
\
UFYUTBNTVOE
PGGTFU
MFOHUI
PQUJPOT<
\
"text": "samsung",
TDPSF
GSFR
^
>
^
>

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 229 ]
TVHHFTUFS<
\
UFYUJQIPO
PGGTFU
MFOHUI
PQUJPOT<
\
"text": "iphone",
TDPSF
^
>
^
>
^
^
Phrase suggester
The term suggester is used for a single term. The phrase suggester is used for making
suggestions based on a phrase. The phrase suggester builds on the functionality of the term
suggester. A simple phrase suggester query would look like the following:
1ISBTF4VHHFTUFS
1045DIBQUFS@TFBSDI
\
TVHHFTU\
UFSN@TVHHFTUFS\
"text" : "samsund galaxy",
 "phrase" : {
GJFMEQSPEVDU@OBNF
^
^
^
^
The phrase suggester is very similar to the term suggester; instead of wrapping the field
using the term for the phrase suggester, we wrap it using the phrase attribute. The response
to the query is as follows:
\

TVHHFTU\
UFSN@TVHHFTUFS<
\
UFYUTBNTVOEHBMBYZ
PGGTFU
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 230 ]
MFOHUI
PQUJPOT<
\
"text": "samsung galaxy",
TDPSF
^
>
^
>
^
^
By default, the maximum number of errors the phrase suggester checks is one. In the
following query, we specified the NBY@FSSPST count as  to check for a maximum of two
errors:
1ISBTF4VHHFTUXJUINBYFSSPS
1045DIBQUFS@TFBSDI
\
TVHHFTU\
UFSN@TVHHFTUFS\
"text" : "samsund galaxh",
 "phrase" : {
GJFMEQSPEVDU@OBNF
"max_errors" : 2
^
^
^
^
The response to the preceding query is as follows:
\

TVHHFTU\
UFSN@TVHHFTUFS<
\
"text": "samsund galaxh",
PGGTFU
MFOHUI
PQUJPOT<
\
"text": "samsung galaxy",
TDPSF
^
\
UFYUTBNTVOHHBMBYI
TDPSF

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 231 ]
^
\
  UFYUTBNTVOEHBMBYZ
TDPSF
^
>
^
>
^
^
The suggestions as shown in the preceding response are based on the input. Not all
suggestions are valid. In the preceding example, along with TBNTVOHHBMBYZ, two other
suggestions are also made. To prune the suggestion that doesn't have any results,
Elasticsearch provides collate, which excludes suggestions that don't produce any results.
The query to check whether the suggestion is valid is passed as a search template. The
preceding query with collate is shown next. The field name in the query is passed in the
QBSBNT, and the suggestion param is autofilled by the suggester. The preceding query with
collate is shown here:
$PMMBUF
1045DIBQUFS@TFBSDI
\
TVHHFTU\
NZ@TVHHFTUJPO\
UFYUTBNTVOEHBMBYI
QISBTF\
GJFMEQSPEVDU@OBNF
NBY@FSSPST
DPMMBUF\
           "query": {
             "inline": {
               "match_phrase": {
                 "{{field_name}}": {
                   "query": "{{suggestion}}",
                   "slop" : 1
                 }
               }
             }
           },
           QBSBNT\
GJFME@OBNFQSPEVDU@OBNF
^
 "prune": true
^
^
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 232 ]
^
^
The response to the preceding query is as follows:
\

TVHHFTU\
NZ@TVHHFTUJPO<
\
UFYUTBNTVOEHBMBYI
PGGTFU
MFOHUI
PQUJPOT<
\
"text": "samsung galaxy",
             "score": 0.47322324,
"collate_match": true
^
\
UFYUTBNTVOHHBMBYI
TDPSF
DPMMBUF@NBUDIGBMTF
^
\
UFYUTBNTVOEHBMBYZ
TDPSF
DPMMBUF@NBUDIGBMTF
^
>
^
>
^
^
You can see from the preceding response that all the suggestions that produce results will
have DPMMBUF@NBUDI set to true. You can also remove the QSVOF setting in the query or set
it to GBMTF to see only the suggestions that produce query results.
Implementing the autocomplete feature
To support autocomplete, Elasticsearch provides the completion suggester. Unlike the term
and phrase suggester, the speed of the response for autocomplete is very important. As the
user types the next letter, the suggester should make suggestions. To support this,
Elasticsearch uses an in-memory data structure called FST (Finite State Transducer).

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 233 ]
For information on FST, please visit IUUQTFOXJLJQFEJBPSHXJLJ'J
OJUFTUBUF@USBOTEVDFS.
FST is nothing but a graph of all the terms, where each letter in the term is a node linked to
the next node. Building FST on the fly is very expensive, and to optimize the performance,
FST structure is built during indexing, and the files are loaded into the memory for the
query execution. Since FST has to be built at indexing time, a special mapping is used to
identify the fields that are used by the completion suggester. The completion mapping can
be set as shown here:
%FMFUFFYJTUJOHJOEFY
%&-&5&DIBQUFS
.BQQJOHGPS$PNQMFUJPO
165DIBQUFS
\
TFUUJOHT\^
NBQQJOHT\
QSPEVDU\
QSPQFSUJFT\
QSPEVDU@OBNF\
UZQFUFYU
"copy_to": "product_suggest"
^
QSPEVDU@TVHHFTU\
 "type": "completion"
^
^
^
^
^
In the mapping, we defined a new QSPEVDU@TVHHFTU field as type completion. In the
QSPEVDU@OBNF mapping, we used DPQZ@UP to automatically copy the value to the
QSPEVDU@TVHHFTU field. Let's index the product documents again as shown here:
4BNQMFEBUB
1045DIBQUFSQSPEVDU@CVML
\JOEFY\^^
\QSPEVDU@OBNF"QQMFJ1IPOF^
\JOEFY\^^
\QSPEVDU@OBNF"QQMFJ1IPOF-JHIUOJOH$BCMF^
\JOEFY\^^
\QSPEVDU@OBNF"QQMFJ1IPOF^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 234 ]
\JOEFY\^^
\QSPEVDU@OBNF4BNTVOH(BMBYZ4^
\JOEFY\^^
\QSPEVDU@OBNF4BNTVOH(BMBYZ4^
We have set correct mapping and indexed the documents. The user input can be passed as
the prefix to the suggest query. We can use the suggest API for autocomplete as shown
here:
$PNQMFUJPO
1045DIBQUFS@TFBSDI
\
TVHHFTU\
NZ@TVHHFTUJPO\
"prefix": "a", #input
DPNQMFUJPO\
 "field": "product_suggest"
^
^
^
^
In the preceding query, when the user input B, we used the suggest API to get the product
suggestions. The response to the preceding query is as follows:
\

TVHHFTU\
NZ@TVHHFTUJPO<
\
UFYUB
PGGTFU
MFOHUI
PQUJPOT<
\
"text": "Apple iPhone 6",
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE"7Y@DN8J0"YR,Q/$L;#
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
^
^
\
"text": "Apple iPhone 7",
@JOEFYDIBQUFS
@UZQFQSPEVDU

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 235 ]
@JE"7Y@DN8J0"YR,Q/$L:@
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
^
^
\
"text": "Apple iPhone Lightning Cable",
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE"7Y@DN8J0"YR,Q/$L;"
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF-JHIUOJOH$BCMF
^
^
>
^
>
^
^
The response comes back with three suggestions (Apple iPhone 6, Apple iPhone 7, and
Apple iPhone Lightning Cable).
Highlighting
Elasticsearch supports highlighting the parts of the response that caused the match. In the
following query, the matches in the QSPEVDU@OBNF field are highlighted:
)JHIMJHIUJOH
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\
"query": "samsung"
^
^
^
"highlight": {
         "fields" : {
             "product_name" : {}
         }
     }
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 236 ]
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE"7T$(%M'+EJ6*M2
@TDPSF
@TPVSDF\
"product_name": "Samsung Galaxy S7"
^
IJHIMJHIU\
QSPEVDU@OBNF<
"<em>Samsung</em> Galaxy S7"
>
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE"7T$Y8'+EJ6*M3#
@TDPSF
@TPVSDF\
"product_name": "Samsung Galaxy S6"
^
IJHIMJHIU\
QSPEVDU@OBNF<
"<em>Samsung</em> Galaxy S6"
>
^
^
>
^
^
The query term is highlighted using the FN tags. We can change the default tags in the
highlight section as shown here:
)JHIMJHIUXJUIUBHT
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI\

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 237 ]
QSPEVDU@OBNF\
RVFSZTBNTVOH
^
^
^
IJHIMJHIU\
 "pre_tags": [
       "<tag1>"
     ],
     "post_tags": [
       "</tag1>"
     ],
GJFMET\
QSPEVDU@OBNF\^
^
^
^
The response will now contain UBH instead of FN.
Handling document relations using parent-
child
In $IBQUFS, Modeling Your Data and Document Relations, we described how to set the
mapping and index parent-child documents. In this section, we will discuss how to query
parent-child documents. To manage relationships in Elasticsearch, parent-child and nested
mappings are provided. The difference between parent-child and nested is in how the
documents are stored. The parent-child documents are costly while querying for the data, and
nested documents are costly while indexing data. We discussed the differences in detail in
$IBQUFS, Modeling your data and Document Relations.
In the previous sections, we indexed product documents for the e-commerce store. In this
section, we will use the parent-child relation to index the reviews for the products as the
child documents. The product document is the parent document. A new review can be
added without modifying the parent document. First, let's set the parent-child mapping in
the DIBQUFS index as shown here:
%FMFUFFYJTUJOHJOEFY
%&-&5&DIBQUFS
.BQQJOH
165DIBQUFS
\
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 238 ]
TFUUJOHT\^
NBQQJOHT\
QSPEVDU\
QSPQFSUJFT\
QSPEVDU@OBNF\
UZQFUFYU
DPQZ@UPQSPEVDU@TVHHFTU
^
QSPEVDU@TVHHFTU\
UZQFDPNQMFUJPO
^
^
^
"product_review": {
       "_parent": {
         "type": "product"
       },
QSPQFSUJFT\^
^
^
^
We set the parent-child mapping. Now let's index some document as shown here:
 #Parent Documents
165DIBQUFSQSPEVDUSFGSFTIUSVF
\QSPEVDU@OBNF"QQMFJ1IPOF^
165DIBQUFSQSPEVDUSFGSFTIUSVF
\QSPEVDU@OBNF"QQMFJ1IPOF^
165DIBQUFSQSPEVDUSFGSFTIUSVF
\QSPEVDU@OBNF4BNTVOH(BMBYZ4^
165DIBQUFSQSPEVDUSFGSFTIUSVF
\QSPEVDU@OBNF4BNTVOH(BMBYZ4^
#Child Documents
1045DIBQUFSQSPEVDU@SFWJFXQBSFOU
\VTFS@JESFWJFXFS
DPNNFOU0OFPGUIFCFTUQIPOFTJOUIFNBSLFU^
The following are two types of parent-child queries that are supported:
IBT@QBSFOU
IBT@DIJME

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 239 ]
The has_parent query
We can use the IBT@QBSFOU to query child documents based on a query on the parent. For
example, we want to find all the reviews written for a given product. The parent document
is QSPEVDU, and the child document is QSPEVDU@SFWJFX. The IBT@QBSFOU query is shown
here:
)BT@1BSFOU
3FWJFXTGPSJQIPOF
1045DIBQUFSproduct_review@TFBSDI
\
RVFSZ\
 "has_parent": {
 "parent_type": "product",
RVFSZ\
UFSN\
QSPEVDU@OBNFJQIPOF
^
^
^
^
^
The response will contain the product reviews for JQIPOF. The response to the preceding
query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
 "_type": "product_review",
@JE"7T.'WQF'+EJ6*M35
@TDPSF
@SPVUJOH
"_parent": "1",
@TPVSDF\
VTFS@JESFWJFXFS
DPNNFOU0OFPGUIFCFTUQIPOFTJOUIFNBSLFU
^
^
>
^
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 240 ]
The has_child query
We will use IBT@DIJME to query parent documents based on a query on children. For
example, to find all the products reviewed by SFWJFXFS, we can use the IBT@DIJME query
as shown here:
)BT@$IJME
1SPEVDUTSFWJFXFECZSFWJFXFS
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
"has_child": {
       "type": "product_review",
RVFSZ\
UFSN\
"user_id": "reviewer1"
^
^
^
^
^
And the response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
"_type": "product",
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
^
^
>
^
^
The response contains all the products reviewed by SFWJFXFS.

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 241 ]
Inner hits for parent-child
In the previous section, we used the IBT@DIJME query on the product document to query
for all the products that are reviewed by SFWJFXFS. The IBT@DIJME query is executed on
its parent mapping type (QSPEVDU), and the response will contain the product documents.
The actual children (QSPEVDU@SFWJFXT) that matched the query are not part of the
response. To solve this problem, the inner hits feature is provided. When the JOOFS@IJUT
tag is included in the query, along with the product documents, the product_review
documents that matched the query are also included in the response. Inner hits can be used
for both the IBT@DIJME and IBT@QBSFOU query. The IBT@DIJME query with inner hits is
shown here:
)BT@$IJME
1SPEVDUTSFWJFXFECZSFWJFXFS
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
IBT@DIJME\
UZQFQSPEVDU@SFWJFX
RVFSZ\
UFSN\
VTFS@JESFWJFXFS
^
^
 "inner_hits": {}
^
^
^
The response to the preceding query is shown here:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
 "_type": "product",
@JE
@TDPSF
 "_source": { # Actual Hits
QSPEVDU@OBNF"QQMFJ1IPOF
^
"inner_hits": {
           "product_review": {
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 242 ]
IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@UZQFQSPEVDU@SFWJFX
@JE"7T33'['+EJ6*M3L
@TDPSF
@SPVUJOH
@QBSFOU
@TPVSDF\
VTFS@JESFWJFXFS
DPNNFOU0OFPGUIFCFTUQIPOFTJOUIFNBSLFU
^
^
>
^
^
^
^
>
^
^
The response contains both the parent (QSPEVDU) document and the child
(QSPEVDU@SFWJFX) documents that matched the query.
How parent-child works internally
Elasticsearch stores the parent and child documents in the same shard. Having both the
parent and child documents in the same shard will make the IBT@QBSFOU and IBT@DIJME
queries possible. The parent ID is used to determine the shard to which the child document
belongs to. To index or retrieve a child document, you need the parent ID. We index a child
document (QSPEVDU@SFWJFX) as shown here:
$IJME%PDVNFOU
1045DIBQUFSQSPEVDU@SFWJFXparent=1
\VTFS@JESFWJFXFSDPNNFOU0OFPGUIFCFTUQIPOFTJOUIF
NBSLFU^
When using parent-child, you are trading indexing performance for query performance.
Internally, Elasticsearch has to maintain a join between the parent documents and child
documents. This mapping is lazy loaded the first time the user runs a parent-child query or
aggregation. Due to this, parent-child queries are ten times slower than corresponding
nested queries.

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 243 ]
Handling document relations using nested
In the Document Relations section in $IBQUFS, Modeling Your Data and Document Relations,
we described how to store nested documents. In this section, we will discuss how to query
them. To better explain querying nested documents, we will change the mappings of the
DIBQUFS index to store the variations of a product as nested documents. For example, an
J1IPOF is available in several storage options, such as (#, (#, and so on. Each
variation has a different price. Each variation of a product will be stored as a nested
document. A product can have one or more variations. In the following query, we will
change the mapping of the type product to include the variations as nested documents:
%FMFUFFYJTUJOHJOEFY
%&-&5&DIBQUFS
4FUNBQQJOHT
165DIBQUFS
\
TFUUJOHT\^
NBQQJOHT\
QSPEVDU\
QSPQFSUJFT\
QSPEVDU@OBNF\
UZQFUFYU
DPQZ@UPQSPEVDU@TVHHFTU
^
 "variations": {
           "type": "nested", #Special Mapping
QSPQFSUJFT\
UZQF\
UZQFLFZXPSE
^
WBMVF\
UZQFLFZXPSE
^
VOJU@QSJDF\
UZQFEPVCMF
^
^
^
QSPEVDU@TVHHFTU\
UZQFDPNQMFUJPO
^
^
^
QSPEVDU@SFWJFX\
@QBSFOU\
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 244 ]
UZQFQSPEVDU
^
QSPQFSUJFT\^
^
^
^
Now let's index some products with different variations as shown here:
1BSFOUEPDVNFOUXJUIOFTUFEWBSJBUJPOT
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNF"QQMFJ1IPOF
WBSJBUJPOT<
\ #Variation 1
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
\ #Variation 2
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
>
^
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNF"QQMFJ1IPOF
WBSJBUJPOT<
\
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
>
^
$IJME%PDVNFOU
1045DIBQUFSQSPEVDU@SFWJFXQBSFOU
\VTFS@JESFWJFXFSDPNNFOU0OFPGUIFCFTUQIPOFTJOUIF
NBSLFU^

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 245 ]
Suppose that we want to query for phones with at least a (# storage. Since variations
are nested documents and are stored as separate hidden documents, we have to use a
special nested type query as shown here:
/FTUFE2VFSZ
2VFSZGPSJ1IPOFXJUI(#
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
CPPM\
NVTU<
\
UFSN\
QSPEVDU@OBNFJQIPOF
^
^
\
"nested": {
 "path": "variations",
RVFSZ\
CPPM\
NVTU<
\
UFSN\
"variations.type": "storage"
^
^
\
UFSN\
 "variations.value": "128GB"
^
^
>
^
^
^
^
>
^
^
^
The response to the preceding query will contain documents that satisfied both the nested
query and main parent (QSPEVDU@OBNFJQIPOF) query as shown here:
\

IJUT\
UPUBM
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 246 ]
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
WBSJBUJPOT<
\
"type": "storage",
               "value": "128GB",
VOJU@QSJDF
^
>
^
^
>
^
^
Note that the response contains the entire product document, not just the variation. If you
need only the variation that matched the query, you can use inner hits, which we will
discuss in the next section.
Inner hits for nested documents
The concept of inner hits is best explained with an example. For suppose, we want to find
out the price of an iPhone with at least (# of storage. Similar to the example in the
previous section, we can use a nested query to get the product document with all its
variations as shown here:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF{
           "product_name": "Apple iPhone 6",

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 247 ]
           "variations": [
             {
               "type": "storage",
               "value": "16GB",
               "unit_price": "600"
             },
             {
               "type": "storage",
               "value": "64GB",
               "unit_price": "700"
             }
           ]
         }
^
>
^
^
The response contains the product document with both (# and (# variation, but what
we are looking for is the price of (# variation. To get the price of the (# variation, you
have to add application-side logic to go through the source document and get the
VOJU@QSJDF. The inner hits feature is provided to solve this problem.
When you include inner hits tag in your query, along with the complete product document,
the nested document that matched the query is also shown in the response. In the following
query, we will set the source to GBMTF to hide the complete product document and use
inner hits to get only the (# variation document:
*OOFSIJUT
1045DIBQUFSQSPEVDU@TFBSDI
\
"_source": false,
RVFSZ\
OFTUFE\
QBUIWBSJBUJPOT
RVFSZ\
CPPM\
NVTU<
\
UFSN\
WBSJBUJPOTUZQFTUPSBHF
^
^
\
UFSN\
WBSJBUJPOTWBMVF(#
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 248 ]
^
>
^
^
"inner_hits": {}
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
JOOFS@IJUT\
WBSJBUJPOT\
IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@OFTUFE\
GJFMEWBSJBUJPOT
PGGTFU
^
@TDPSF
 "_source": {
                     "type": "storage",
                     "value": "64GB",
                     "unit_price": "700"
                   }
^
>
^
^
^
^
>
^
^

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 249 ]
In the preceding response, you can see that only the (# variation is included in the
response. If you want both the complete product document and the variation that matched
the query, just remove the @TPVSDFGBMTF from the query.
Scripting
Scripting is one of the most powerful features of Elasticsearch. In this chapter so far, we
discussed different types of queries Elasticsearch supports. If these queries are not enough,
Elasticsearch also provides script query. Scripting allows you to run user defined scripts to
determine whether a document should be filtered or not. Along with the script query, fields
based on a script, sorting based on a script are also supported. In Elasticsearch 5.0, Painless,
a new scripting language, which is both secure and fast, is introduced. Along with Painless,
special-purpose languages, such as Expression, Mustache, and Java are also supported.
Script Query
Script query can be used to evaluate documents against a user-defined script. For example,
the product document we discussed so far has a nested field variation, which contains the
different variations of each product. We want to query for all the products with more than
one variation. Since this cannot be done with already available Elasticsearch queries, we
will use a script. The query with a script is shown here:
4DSJQU2VFSZ
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
TDSJQU\
TDSJQU\
MBOHQBJOMFTT
 "inline": "params._source.containsKey('variations') &&
params._source.variations.length > 1"
^
^
^
^
In the preceding query, we are using the source of the document to access the WBSJBUJPOT
field. We can also use the EPD values, which is much faster than using a source field. You
can access the field value using the EPD values as shown here:
JOMJOFEPD<	OVNFSJD@GJFME	>WBMVF 
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 250 ]
Scripts are compiled and cached. To reuse the compiled script, any parameters used in the
script should be passed via the QBSBNT. When the parameters are passed via QBSBNT, the
compiled script can be reused for any number of variations. In the preceding example, the
number of variations can be passed as a parameter as shown here:
4DSJQU2VFSZ
1045DIBQUFSQSPEVDU@TFBSDI
\
RVFSZ\
TDSJQU\
TDSJQU\
MBOHQBJOMFTT
JOMJOFQBSBNT@TPVSDFDPOUBJOT,FZ
	WBSJBUJPOT	
params._source.variations.length >params.num_of_variations
QBSBNT\
 "num_of_variations": 1
^
^
^
^
^
Executing scripts on a larger index can be very expensive. To minimize the performance
impact of scripting, we will discuss post filter in the next section.
Post Filter
You can tell Elasticsearch to run an expensive query, such as a script or geolocation, using
post filter. The query in the post filter is only executed after the main query is executed so
that the number of documents the expensive query has to be executed on is minimum. In
the following query, we will run the script query as post filter:
1045DIBQUFSQSPEVDU@TFBSDI
\
"query": {
NBUDI\
QSPEVDU@OBNFJQIPOF
^
^
"post_filter": {
TDSJQU\
TDSJQU\
MBOHQBJOMFTT
JOMJOFQBSBNT@TPVSDFDPOUBJOT,FZ
	WBSJBUJPOT	
QBSBNT@TPVSDFWBSJBUJPOTMFOHUI QBSBNTOVN@PG@WBSJBUJPOT

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 251 ]
QBSBNT\
OVN@PG@WBSJBUJPOT
^
^
^
^
^
In the preceding example, the main query is executed first, and the script is executed on the
documents that match the main query.
Reverse search using the percolate query
The percolate query is one of the popular features of Elasticsearch. Until now, we indexed
the documents and used the search API to query the documents. Percolate query is reverse
search. The actual queries are stored into an index, and we can percolate a document to get
the queries that match the document. By using the percolate query, you are checking
whether the document matches any of the predefined criteria. Common use cases include
alerts and monitoring.
For example, we want to classify the products in an e-commerce store. First, we will add
predefined queries to the DIBQUFS index and use the QFSDPMBUF query to check whether
a product matches any predefined queries. The following example will make it more clear.
To use the QFSDPMBUF query, we need to first add a mapping with the QFSDPMBUPS type. In
the following command, we are adding a new type called MBCFM, which contains a field
named RVFSZ, which is of type QFSDPMBUPS. We will index the actual queries in the
DIBQUFS index and type MBCFM. The mapping for the field query can be added as shown
next:
1FSDPMBUPS
165DIBQUFS@NBQQJOHMBCFM
\
QSPQFSUJFT\
RVFSZ\
"type": "percolator"
^
^
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 252 ]
Now let's add some queries to the new type we just added. The document will contain the
actual query and the metadata. In the following command, we will index two labels/tags:
one for the products with no variations and the other for the products that are
manufactured by Samsung:
/P7BSJBUJPOT
165DIBQUFSMBCFMno_variations
\
"query": {
TDSJQU\
TDSJQU\
JOMJOFQBSBNT@TPVSDFDPOUBJOT,FZ
	WBSJBUJPOT	
^
^
^
 "metadata": {
EFTD1SPEVDUXJUIOPWBSJBUJPOT
^
^
4BNTVOH
165DIBQUFSMBCFMsamsung_product
\
"query": {
NBUDI\
QSPEVDU@OBNFTBNTVOH
^
^
  "metadata": {
EFTD4BNTVOH1SPEVDU
^
^
Next, imagine we have just added a new product and we want to find out the labels/tags for
this product. We will use the percolator query to do that. The product we want to run the
percolate query on is shown here:
165DIBQUFSQSPEVDU
\
QSPEVDU@OBNF4BNTVOH(BMBYZ4
^
We can run the QFSDPMBUF query on the preceding document as shown here:
1045DIBQUFSMBCFM@TFBSDI
\
"_source": false,

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 253 ]
RVFSZ\
QFSDPMBUF\
GJFMERVFSZ
EPDVNFOU@UZQFQSPEVDU
"index" : "chapter7",
        "type" : "product",
        "id" : 4
^
^
^
In the preceding query, the following parameters are used:
GJFME: This refers to the field that is of type percolator. This field holds the actual
query.
EPDVNFOU@UZQF: This refers to the type of the document the percolate query is
executed on.
JOEFY: This refers to the index of the document.
UZQF: This refers to the type of the document.
JE: This refers to the identifier of the document.
The response to the preceding query contains all the labels the document matched. The
response is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFMBCFM
"_id": "no_variations",
@TDPSF
^
\
@JOEFYDIBQUFS
@UZQFMBCFM
"_id": "samsung_product",
@TDPSF
^
>
^
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 254 ]
You can see from the response that the document matched two labels. One is
OP@WBSJBUJPOT, and the other is TBNTVOH@QSPEVDU. In the preceding example, we set the
source to false to see only the label @JE. You can also set the @TPVSDF to metadata or other
fields you want in the response.
In the previous example, we used the id of an existing document in the request. Instead of
the document id, the actual document source can also be used as shown here:
1045DIBQUFSMBCFM@TFBSDI
\
@TPVSDFGBMTF
RVFSZ\
QFSDPMBUF\
GJFMERVFSZ
EPDVNFOU@UZQFQSPEVDU
 "document": {
         "product_name": "Samsung Galaxy S7"
       }
^
^
^
Geo and Spatial Filtering
In modern applications, spatial or location-based filtering is a very common requirement,
not just for filtering the results based on a location, but also as one of the driving factors of
relevance. The results that are closer to the user location appear at the top of the list, the
results that are not close are not removed from the list but simply placed at the bottom of
the list. Elasticsearch makes it very easy to work with geographical data by combining full-
text search and location-based filtering. Sorting the results based on the distance from the
current user location is also supported.
To use geolocation queries, the location information should be indexed using a special
mapping type. The geolocation can be stored using the HFP@QPJOU mapping type if you
want to store the location data in the form of latitude/longitude pairs. If you want to store
the location data in form of circles, rectangles, and so on, you can use the HFP@TIBQF
mapping type. Geo shapes are beyond the scope of this book, and we will only discuss how
to index and search HFP@QPJOU in this section. We will discuss location-based aggregations
in $IBQUFS, How to slice and dice your data using aggregations.

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 255 ]
We will continue to use the e-commerce example to demonstrate location-based queries. We
will add a new type named TUPSF to the existing DIBQUFS index. The new type will
contain documents representing physical stores with the store OBNF, BEESFTT, IBT@XJGJ,
and MPDBUJPO fields. We can add the new mapping as shown here :
(&0
165DIBQUFS@NBQQJOHTUPSF
\
QSPQFSUJFT\
OBNF\
UZQFLFZXPSE
^
BEESFTT\
UZQFUFYU
^
IBT@XJGJ\
UZQFCPPMFBO
^
MPDBUJPO\
"type": "geo_point"
^
^
^
The mapping type for the MPDBUJPO field is specified as HFP@QPJOU. The HFP@QPJOU is
used to store location information using coordinates.
Now let's index some store documents as shown here:
*OEFY4UPSFT
165DIBQUFSTUPSF
\
OBNF4UPSF
BEESFTT)JHI-BOF
IBT@XJGJUSVF
"location" : {
     "lat" : "37.339724",
     "lon" : "-121.873848"
   }
^
165DIBQUFSTUPSF
\
OBNF4UPSF
BEESFTT)JHI-BOF
IBT@XJGJUSVF
"location" : {
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 256 ]
     "lat" : "37.338495",
     "lon" : "-121.880736"
   }
^
As you can see, location information is represented using longitude/latitude pairs. The
following variations are also supported:
# As an Object
MPDBUJPO\
MBU
MPO
^
# As a String
MPDBUJPO
# As an Array
MPDBUJPO<>
# As a Geo Hash
MPDBUJPORLURQUU
The following location-based queries are supported:
Geo Distance
Geo Bounding Box
Geo Distance
Geo Distance query is used to find the documents that are located within a certain distance
from a given location. Let's query for all the stores within 1 mile of the user location. For
this example, I'm assuming the user location as follows:
MBUMPO

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 257 ]
In the following Google Maps screenshot, the red pin represents the current user location,
and the blue circle represents the 1-mile radius from the user location:
The query would look like the following:
1045DIBQUFSTUPSF@TFBSDI
\
RVFSZ\
HFP@EJTUBODF\
"distance": "1mi",
"location": {
          "lat": "37.348929",
          "lon": "-121.888536"
        }
^
^
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 258 ]
The preceding query limits to stores that are within 1 mile from the given location. The
response to the query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFTUPSF
@JE
@TDPSF
@TPVSDF\
"name": "Store2",
BEESFTT)JHI-BOF
IBT@XJGJUSVF
MPDBUJPO\
MBU
MPO
^
^
^
>
^
^
You can see from the response that there is only one store within  mile of the user location.
Using Geolocation to rank the search results
In this section, we will use the Geo Distance query to order the search results based on the
distance. The documents closer to the user location are shown higher in the result list. For
example, we want to find all the stores that have wifi in them and are within  mile of the
user location. If none of the stores are within a 1-mile radius, filtering on the location will
exclude all the stores in the result. In some cases, the user might be okay with a 2-mile
radius. Instead of excluding the stores that are far away, we can rank them lower than the
stores that are near.

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 259 ]
We will use the CPPM query to tune the relevance. We need all the stores that have wifi in
them, so we place the IBT@XJGJ query in the must block. Next, the location query is placed
in the TIPVME block. Stores that match the geolocation query have better relevance than
those that don't. The CPPM query simply adds the relevance from all the queries. The more
queries the document matches, the higher the relevance score is. The CPPM query is shown
next:
1045DIBQUFSTUPSF@TFBSDI
\
RVFSZ\
  "bool": {
NVTU<
\
  "term": {
IBT@XJGJUSVF
^
^
>
TIPVME<
\
  "geo_distance": {
"distance": "1mi",
MPDBUJPO\
MBU
MPO
^
^
^
>
^
^
^
The response to the CPPM query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFTUPSF
@JE
"_score": 1.287682,
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 260 ]
@TPVSDF\
OBNF4UPSF
BEESFTT)JHI-BOF
IBT@XJGJUSVF
MPDBUJPO\
MBU
MPO
^
^
^
\
@JOEFYDIBQUFS
@UZQFTUPSF
@JE
"_score": 0.2876821,
@TPVSDF\
OBNF4UPSF
BEESFTT)JHI-BOF
IBT@XJGJUSVF
MPDBUJPO\
MBU
MPO
^
^
^
>
^
^
You can see from the preceding response that the score for 4UPSF is higher than 4UPSF as
4UPSF is within  mile of the location. The beauty of Elasticsearch is the ability to combine
the geolocation-based query with full-text search and structured search as shown in the
previous example.

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 261 ]
Geo Bounding Box
Geo Bounding Box query is used to find all documents within a bounding box. A bounding
box is defined by the coordinates of the UPQ@MFGU and CPUUPN@SJHIU. Unlike the Geo
Distance query, which has to calculate the distance of every document from the given
location, the Geo Bounding query can quickly check whether the coordinates of the
document are within the four corners of the bounding box. The Geo Bounding query is less
expensive when compared to Geo Distance. A bounding box on Google Maps is shown
here:
As shown in the screenshot from Google Maps, we are trying to find all the stores within
the bounding box. The query would look like the following:
(&0#PVOEJOH#PY
1045DIBQUFSTUPSF@TFBSDI
\
RVFSZ\
HFP@CPVOEJOH@CPY\
MPDBUJPO\
"top_left": {
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 262 ]
MBU
MPO
^
 "bottom_right": {
MBU
MPO
^
^
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFTUPSF
@JE
@TDPSF
@TPVSDF\
"name": "Store2",
BEESFTT)JHI-BOF
IBT@XJGJUSVF
MPDBUJPO\
MBU
MPO
^
^
^
>
^
^
The response will contain all the stores within the bounding box.

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 263 ]
Sorting
In this section, we will discuss how to sort the results by the distance to the given location.
In most cases, scoring the results like we did in the Geo Distance query section is a better
option than sorting the results. The real power of Elasticsearch is combining the different
type of queries and assigning weights to each query. The query to sort all the stores based
on the distance from the current location is shown here:
(&04PSU
1045DIBQUFSTUPSF@TFBSDI
\
RVFSZ\
NBUDI@BMM\^
^
TPSU<
\
"_geo_distance": {
         "location": {
           "lat": "37.348929",
           "lon": "-121.888536"
         },
PSEFSBTD
VOJUNJ
^
^
>
^
In the preceding query, the result is sorted in ascending ordering of the distance from the
given location. And the sort value in the response is shown in miles. The response to the
preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSFOVMM
IJUT<
\
@JOEFYDIBQUFS
@UZQFTUPSF
@JE
@TDPSFOVMM
@TPVSDF\
OBNF4UPSF
BEESFTT)JHI-BOF
IBT@XJGJUSVF
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 264 ]
MPDBUJPO\
MBU
MPO
^
^
TPSU<
 0.8386315194291991
>
^
\
@JOEFYDIBQUFS
@UZQFTUPSF
@JE
@TDPSFOVMM
@TPVSDF\
OBNF4UPSF
BEESFTT)JHI-BOF
IBT@XJGJUSVF
MPDBUJPO\
MBU
MPO
^
^
TPSU<
 1.0273440725719407
>
^
>
^
^
You can see from the response that the sort value is the distance of the store from the given
location.
Multi search
Multi search allows us to group search requests together. It is similar to the multi-get and
the other bulk APIs. By grouping the requests together, we can save the network round
trips and execute the queries in the request in parallel. We can control the number of the
requests that are executed in parallel. A simple multi search request is shown here:
.VMUJ4FBSDI
(&5DIBQUFS_msearch
{"type" : "product"}
\RVFSZ\NBUDI@BMM\^^GSPNTJ[F^

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 265 ]
{"type" : "product_review"}
\RVFSZ\NBUDI@BMM\^^^
The response of the multi search query is a list of responses. Each request is executed
independently, and the failure of one request will not affect the others. The response to the
preceding query will contain responses to two queries as shown next:
\
SFTQPOTFT<
\

IJUT\
 "total": 3,
         NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
 "product_name": "Apple iPhone 7",
WBSJBUJPOT<
\
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
>
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF4BNTVOH(BMBYZ4
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
WBSJBUJPOT<
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 266 ]
\
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
\
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
>
^
^
>
^
"status": 200
^
\

IJUT\
 "total": 1,
         "max_score": 1,
IJUT<
\
"_index": "chapter7",
             "_type": "product_review",
@JE"7Y@HGVPJ0"YR,Q/$MP%
@TDPSF
@SPVUJOH
@QBSFOU
@TPVSDF\
VTFS@JESFWJFXFS
DPNNFOU0OFPGUIFCFTUQIPOFTJOUIFNBSLFU
^
^
>
^
 "status": 200
^
>
^

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 267 ]
The number of queries executed in parallel can be controlled using the
NBY@DPODVSSFOU@TFBSDIFT parameter as shown next. The default value depends on the
number of nodes in the cluster and the thread pool size. The preceding query with
NBY@DPODVSSFOU@TFBSDIFT equal to  is shown here:
.VMUJ4FBSDI
(&5DIBQUFS@NTFBSDI?max_concurrent_searches=1
\UZQFQSPEVDU^
\RVFSZ\NBUDI@BMM\^^GSPNTJ[F^
\UZQFQSPEVDU@SFWJFX^
\RVFSZ\NBUDI@BMM\^^^
Search templates
Search templates are very similar to stored procedures in the relational database.
Commonly used queries can be defined as a template, and the applications using
Elasticsearch can simply refer to the query by its ID. The template accepts parameters,
which can be specified at the runtime. Search templates are stored on the server side and
can be modified without changes to the client code. Templates are expressed using the 
Mustache template engine. For more information on mustache, please visit IUUQNVTUBDI
FHJUIVCJPNVTUBDIFIUNM.
Let's start by defining a template query to find all the products by their name. The query is
as follows:
%FGJOF5FNQMBUF
1045@TFBSDIUFNQMBUFfind_product_by_name
\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\\QSPEVDU@OBNF^^
^
^
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 268 ]
Once the template is defined, you can execute the template by passing the QSPEVDU@OBNF
parameter as shown here:
&YFDVUF5FNQMBUF
1045DIBQUFSQSPEVDU@TFBSDIUFNQMBUF
\
JEfind_product_by_name
QBSBNT\
"product_name" : "iphone"
^
^
Applications calling Elasticsearch don't have to worry about the query logic. Any changes
to the query can be made on the server side without worrying about changing the client
code. Templates are also easy to debug. The result of the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
WBSJBUJPOT<
\
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
>
^
^
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
WBSJBUJPOT<
\
UZQFTUPSBHF

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 269 ]
WBMVF(#
VOJU@QSJDF
^
\
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
>
^
^
>
^
^
You can also store the template as a NVTUBDIF file in DPOGJHTDSJQUT. For the
preceding example, add the following contents to a GJOE@QSPEVDU@CZ@OBNFNVTUBDIF
file:
\
RVFSZ\
NBUDI\
QSPEVDU@OBNF\\QSPEVDU@OBNF^^
^
^
^
Before you can execute the template, the
GJOE@QSPEVDU@CZ@OBNFNVTUBDIF template file should be copied to all
the nodes in the cluster. The template file should be placed in the
DPOGJHTDSJQUT folder.
The GJOE@QSPEVDU@CZ@OBNFNVTUBDIF template file can be executed as follows:
&YFDVUF5FNQMBUF
1045DIBQUFSQSPEVDU@TFBSDIUFNQMBUF
\
fileGJOE@QSPEVDU@CZ@OBNF
QBSBNT\
QSPEVDU@OBNFJQIPOF
^
^
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 270 ]
Mustache templates also support conditioning. For example, in the above template along
with the QSPEVDU@OBNF we want to pass an optional WBSJBUJPO parameter to filter the
products based on storage. The above query with the optional variation parameter is shown
here:
&YFDVUF5FNQMBUF
1045DIBQUFSQSPEVDU@TFBSDIUFNQMBUF
\
GJMFGJOE@QSPEVDU@CZ@OBNF
QBSBNT\
QSPEVDU@OBNFJQIPOF
 "variation": "128GB"
^
^
Let's update the template query to accept an optional WBSJBUJPO parameter along with the
product name. The updated template looks like the following:
\
RVFSZ\
CPPM\
NVTU<
\
NBUDI\
QSPEVDU@OBNF\\QSPEVDU@OBNF^^
^
^
  {{#variation}}

\
OFTUFE\
QBUIWBSJBUJPOT
RVFSZ\
CPPM\
NVTU<
\
UFSN\
WBSJBUJPOTUZQFTUPSBHF
^
^
\
UFSN\
WBSJBUJPOTWBMVF\\WBSJBUJPO^^
^
^
>
^

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 271 ]
^
^
^
 {{/variation}}
>
^
^
^
In the preceding query, we have used mustache conditionals to check whether the variation
parameter is passed in the request. The variation tags used in the query are shown here:
{{ #variation }}  {{ /variation }}
Only if the WBSJBUJPO parameter is provided in the request, the template will conditionally
add the nested query to the bool query. Due to how the mustache conditionals are
formatted, the preceding template is not a valid JSON and can only be used by saving it as a
GJOE@QSPEVDU@CZ@OBNFNVTUBDIF file. You don't have to restart the node after
adding/updating a script file. Elasticsearch will automatically pick up the script.
Note that variation is an optional parameter. A template can be executed
with or without the variation parameter.
The results of the preceding query are as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYDIBQUFS
@UZQFQSPEVDU
@JE
@TDPSF
@TPVSDF\
QSPEVDU@OBNF"QQMFJ1IPOF
WBSJBUJPOT<
\
UZQFTUPSBHF
WBMVF(#
VOJU@QSJDF
^
>
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 272 ]
^
^
>
^
^
In the preceding example, since variations is a nested type, a nested query is used. Once the
template is defined, the application can simply use the template name to execute the query
without worrying about using special type of queries for nested, parent-child, and so on.
Querying Elasticsearch from Java
application
In this section, we will discuss how to use a Java client to query Elasticsearch. In $IBQUFS,
Indexing and Updating Your Data, we discussed different types of Java clients and how to use
them in your application for indexing. In this section, we will discuss how to use the client
to query Elasticsearch. Let's take a simple match query as shown here:
.BUDI2VFSZ
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI\
QSPEVDU@OBNFJQIPOF
^
^
^
All the queries available via the REST API are also made available via the transport client.
To execute the query using the Java client, first set up the client as shown next:
5SBOTQPSU"EESFTTOPEFOFX
*OFU4PDLFU5SBOTQPSU"EESFTT
*OFU"EESFTTHFU#Z/BNF

4FUUJOHTTFUUJOH4FUUJOHTCVJMEFS
QVU
DMVTUFSOBNFFTEFW
QVU
DMJFOUUSBOTQPSUTOJGGUSVFCVJME

5SBOTQPSU$MJFOUUSBOTQPSU$MJFOUOFX1SF#VJMU5SBOTQPSU$MJFOU
TFUUJOH
"EEUIFLOPXOOPEFTUPUIFDMJFOU
USBOTQPSU$MJFOUBEE5SBOTQPSU"EESFTT
OPEF

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 273 ]
Next, we will use the 2VFSZ#VJMEFST to build the query. The query builders are available
by importing 2VFSZ#VJMEFST as shown here:
JNQPSUstaticPSHFMBTUJDTFBSDIJOEFYRVFSZ2VFSZ#VJMEFST
We are using a static import for 2VFSZ#VJMEFS so that we can use the queries without
specifying the class or package. We will use the transport client to execute the query as
shown here:
// Imports
JNQPSUPSHFMBTUJDTFBSDIBDUJPOTFBSDI4FBSDI3FTQPOTF
JNQPSUPSHFMBTUJDTFBSDIJOEFYRVFSZ2VFSZ#VJMEFS
JNQPSUPSHFMBTUJDTFBSDITFBSDI4FBSDI)JU
// Build a match Query
2VFSZ#VJMEFSNBUDI2VFSZNBUDI2VFSZ
QSPEVDU@OBNFJQIPOF
// Execute the query on chapter7 index.
4FBSDI3FTQPOTFSFTQPOTF
USBOTQPSU$MJFOUQSFQBSF4FBSDI
DIBQUFSTFU5ZQFT
QSPEVDU
TFU2VFSZ
NBUDI2VFSZFYFDVUF
BDUJPO(FU

// Number of hits in the response
MPOHUPUBM)JUTSFTQPOTFHFU)JUT
HFU5PUBM)JUT

// Actual Hit
4FBSDI)JUTFBSDI)JUSFTQPOTFHFU)JUT
HFU"U

Once we get the hits from the response, we will use 0CKFDU.BQQFS to convert the hit to a
1SPEVDU object. The 1SPEVDU class definition is shown here:
JNQPSUKBWBVUJM-JTU
JNQPSUDPNGBTUFSYNMKBDLTPOBOOPUBUJPO+TPO*HOPSF1SPQFSUJFT
JNQPSUDPNGBTUFSYNMKBDLTPOBOOPUBUJPO+TPO1SPQFSUZ
!+TPO*HOPSF1SPQFSUJFT
JHOPSF6OLOPXOUSVF
QVCMJDDMBTT1SPEVDU\
!+TPO1SPQFSUZ
QSPEVDU@OBNF
QSJWBUF4USJOHQSPEVDU/BNF
QSJWBUF-JTU7BSJBUJPO WBSJBUJPOT
QVCMJD4USJOHHFU1SPEVDU/BNF
\
SFUVSOQSPEVDU/BNF
^
QVCMJDWPJETFU1SPEVDU/BNF
4USJOHQSPEVDU/BNF\
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 274 ]
UIJTQSPEVDU/BNFQSPEVDU/BNF
^
QVCMJD-JTU7BSJBUJPO HFU7BSJBUJPOT
\
SFUVSOWBSJBUJPOT
^
QVCMJDWPJETFU7BSJBUJPOT
-JTU7BSJBUJPO WBSJBUJPOT\
UIJTWBSJBUJPOTWBSJBUJPOT
^
TUBUJDDMBTT7BSJBUJPO\
QSJWBUF4USJOHUZQF
QSJWBUF4USJOHWBMVF
!+TPO1SPQFSUZ
VOJU@QSJDF
QSJWBUF4USJOHVOJU1SJDF
QVCMJD4USJOHHFU5ZQF
\
SFUVSOUZQF
^
QVCMJDWPJETFU5ZQF
4USJOHUZQF\
UIJTUZQFUZQF
^
QVCMJD4USJOHHFU7BMVF
\
SFUVSOWBMVF
^
QVCMJDWPJETFU7BMVF
4USJOHWBMVF\
UIJTWBMVFWBMVF
^
QVCMJD4USJOHHFU6OJU1SJDF
\
SFUVSOVOJU1SJDF
^
QVCMJDWPJETFU6OJU1SJDF
4USJOHVOJU1SJDF\
UIJTVOJU1SJDFVOJU1SJDF
^
^
^
We can use the 0CKFDU.BQQFS to convert the response from Elasticsearch to a 1SPEVDU
object as shown next:
0CKFDU.BQQFSNBQQFSOFX0CKFDU.BQQFS

1SPEVDUQSPEVDUNBQQFSSFBE7BMVF
TFBSDI)JUHFU4PVSDF"T4USJOH

1SPEVDUDMBTT

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 275 ]
The response using the Java client is very similar to the response from the REST API. The
complete application code is shown here:
JNQPSUTUBUJDPSHFMBTUJDTFBSDIJOEFYRVFSZ2VFSZ#VJMEFSTmatchQuery
JNQPSUKBWBOFU*OFU"EESFTT
JNQPSUKBWBVUJM"SSBZ-JTU
JNQPSUKBWBVUJM-JTU
JNQPSUPSHFMBTUJDTFBSDIBDUJPOTFBSDI4FBSDI3FTQPOTF
JNQPSUPSHFMBTUJDTFBSDIDMJFOUUSBOTQPSU5SBOTQPSU$MJFOU
JNQPSUPSHFMBTUJDTFBSDIDPNNPOTFUUJOHT4FUUJOHT
JNQPSUPSHFMBTUJDTFBSDIDPNNPOUSBOTQPSU*OFU4PDLFU5SBOTQPSU"EESFTT
JNQPSUPSHFMBTUJDTFBSDIDPNNPOUSBOTQPSU5SBOTQPSU"EESFTT
JNQPSUPSHFMBTUJDTFBSDIJOEFYRVFSZ2VFSZ#VJMEFS
JNQPSUPSHFMBTUJDTFBSDITFBSDI4FBSDI)JU
JNQPSUPSHFMBTUJDTFBSDIUSBOTQPSUDMJFOU1SF#VJMU5SBOTQPSU$MJFOU
JNQPSUDPNGBTUFSYNMKBDLTPOEBUBCJOE0CKFDU.BQQFS
JNQPSUDPNMFBSOJOHFMBTUJDTFBSDI1SPEVDU
QVCMJDDMBTT"QQMJDBUJPO\
QVCMJDTUBUJDWPJENBJO
4USJOH<>BSHTUISPXT&YDFQUJPO\
 //Query Elasticsearch for all the product with iPhone in the
product name.
-JTU1SPEVDU QSPEVDUTsearchProduct
JQIPOF
^
QSJWBUFTUBUJD-JTU1SPEVDU TFBSDI1SPEVDU
4USJOHQSPEVDU/BNFUISPXT
&YDFQUJPO\
-JTU1SPEVDU SFTVMUOFX"SSBZ-JTU1SPEVDU 

 // Initialize Transport Client
5SBOTQPSU"EESFTTOPEFOFX
*OFU4PDLFU5SBOTQPSU"EESFTT
*OFU"EESFTTgetByName

4FUUJOHTTFUUJOH4FUUJOHTbuilder
QVU
DMVTUFSOBNFFT
EFWQVU
DMJFOUUSBOTQPSUTOJGGUSVFCVJME

5SBOTQPSU$MJFOUUSBOTQPSU$MJFOUOFX
1SF#VJMU5SBOTQPSU$MJFOU
TFUUJOH
// Add the known nodes to the client
USBOTQPSU$MJFOUBEE5SBOTQPSU"EESFTT
OPEF
           // Build a match query
2VFSZ#VJMEFSNBUDI2VFSZmatchQuery
QSPEVDU@OBNF
QSPEVDU/BNF
 // Execute the query on chapter7 index and product type
4FBSDI3FTQPOTFSFTQPOTF
USBOTQPSU$MJFOUQSFQBSF4FBSDI
DIBQUFSTFU5ZQFT
QSPEVDUTFU2VFSZ
NBUD
www.ebook3000.com

More Than a Search Engine (Geofilters, Autocomplete, and More)
[ 276 ]
I2VFSZ
FYFDVUF
BDUJPO(FU

 // Total Hits
MPOHUPUBM)JUTSFTQPOTFHFU)JUT
HFU5PUBM)JUT

 // Initialize Object Mapper
0CKFDU.BQQFSNBQQFSOFX0CKFDU.BQQFS

GPS
4FBSDI)JUIJUSFTQPOTFHFU)JUT
\
 // Use ObjectMapper to convert the source into Java POJO
1SPEVDUQNBQQFSSFBE7BMVF
IJUHFU4PVSDF"T4USJOH

1SPEVDUDMBTT
SFTVMUBEE
Q
4ZTUFNoutQSJOUMO
IJUTPVSDF"T4USJOH

^
SFUVSOSFTVMU
^
^
Summary
In this chapter, we discussed how to implement autocomplete, highlighting, and correcting
user typos. Elasticsearch doesn't support traditional SQL joins, and you learned how to use
parent-child and nested mapping to handle relationships between different document
types. We discussed filtering based on geolocation and how to use location as one of factors
driving the relevance score. We also discussed using Painless scripting language to query
based on user-defined scripts. We also covered Search Templates and how to query
Elasticsearch from your application.
In the next chapter, we will discuss aggregations and how to use them to slice and dice your
data.

8
How to Slice and Dice Your
Data Using Aggregations
In this chapter, youâ€™ll learn how to unleash the analytics power of Elasticsearch. In
Elasticsearch 5.0, the aggregation framework has been completely revamped. The query
syntax is very simple and easy to understand. The distributed nature of Elasticsearch makes
the queries very performant and can easily scale to large datasets. We will go through the
different types of aggregations Elasticsearch supports and how easy it is to run these
queries. We will discuss how to use Kibana to visualize the data. You will also learn doc
values and field data, the internal data structures used to power aggregations.
By the end of this chapter, we will cover the following:
Different types of aggregations
Child aggregations
Aggregation on nested documents
Aggregation on geolocations
Doc values
Memory considerations
Data visualization using Kibana
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 278 ]
Aggregation basics
Aggregation is one of many reasons why Elasticsearch is nothing like anything out there; it
is an analytics engine on steroids. Aggregation operations, such as distinct, count, and
average on large data sets, are traditionally run on batch processing systems, such as
Hadoop, due to the heavy computation involved. As running these kind of queries on a
large dataset using a traditional SQL database can be very challenging. Elasticsearch
enables these queries to run in real-time sub-second queries. In my first project with
Elasticsearch, we solely used Elasticsearch for its aggregation capabilities and few search
capabilities.
Aggregations in Elasticsearch are very powerful as you can nest aggregations. Let's take a
query from the SQL world:
TFMFDUBWH
SBUJOHGSPN1SPEVDUHSPVQCZDBUFHPSZ
To execute the query, the products are first grouped by category. Products that belong to
the same category are grouped into the same bucket. The second level of aggregation
computes the average ratings of each bucket (category).
Nesting aggregations in Elasticsearch is very simple. These kind of queries are much more
performant than SQL and can easily scale. Take the following for example:
First, you can bucket all the products using their categories.
Next, for each category, you can further group the products by their
manufacturers.
For each manufacturer, you can compute the average price/rating.
And you can keep going.
You can use nest any level of aggregations. You can also use pipeline aggregations to use
the result of the one aggregation as an input to a different aggregation. Aggregations are
calendar aware, meaning you can bucket the data by month, year, and so on. Aggregations
are location aware, meaning you can bucket the data by regions or locations.
In next few sections, we will go through the different types of aggregations that are
supported. Once you understand the basic types of aggregations and their syntax, you will
be able to use aggregations for any data.

How to Slice and Dice Your Data Using Aggregations
[ 279 ]
Sample data
Similar to the previous chapters, we will use examples to explain aggregations better. In this
chapter, we are going to use aggregations that are useful to an e-commerce site. Our data
will be about the products in an e-commerce site.
In the product mapping, we will use type LFZXPSE for fields we want to
run aggregations on. Keywords are better for running aggregation, sorting
and can advantage of doc values. We will discuss doc values further in
sections below.
First, let's set the mapping for the DIBQUFS index:
%FMFUFFYJTUJOHJOEFY
%&-&5&DIBQUFS
$IBQUFS.BQQJOHT
165DIBQUFS
\
TFUUJOHT\^
NBQQJOHT\
"product": {
QSPQFSUJFT\
QSPEVDU@OBNF\
"type": "text"
^
NBOVGBDUVSFS\
 "type": "keyword"
^
DBUFHPSZ\
"type": "keyword"
^
QSPEVDUT@TPME\
UZQFJOUFHFS
^
WBSJBUJPOT\
UZQFOFTUFE
QSPQFSUJFT\
DPMPS\
UZQFLFZXPSE
^
TUPSBHF\
UZQFLFZXPSE
^
VOJU@QSJDF\
UZQFEPVCMF
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 280 ]
^
^
^
^
"product_review": {
@QBSFOU\
UZQFQSPEVDU
^
QSPQFSUJFT\^
^
^
^
Next, we will index some sample documents:
1BSFOU%PDVNFOUT
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNFJ1IPOF
NBOVGBDUVSFS"QQMF
SFMFBTF@EBUF
QSPEVDUT@TPME
DBUFHPSZ$FMM1IPOFT
WBSJBUJPOT<
\
DPMPS(SBZ
TUPSBHF(#
VOJU@QSJDF
^
>
^
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNFJ1IPOF
NBOVGBDUVSFS"QQMF
SFMFBTF@EBUF
QSPEVDUT@TPME
DBUFHPSZ$FMM1IPOFT
WBSJBUJPOT<
\
DPMPS3PTF(PME
TUPSBHF(#
VOJU@QSJDF
^
\
DPMPS3PTF(PME
TUPSBHF(#

How to Slice and Dice Your Data Using Aggregations
[ 281 ]
VOJU@QSJDF
^
>
^
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNF4BNTVOH(BMBYZ4
NBOVGBDUVSFS4BNTVOH
SFMFBTF@EBUF
QSPEVDUT@TPME
DBUFHPSZ$FMM1IPOFT
WBSJBUJPOT<
\
DPMPS(SBZ
TUPSBHF(#
VOJU@QSJDF
^
>
^
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNF6OJWFSTBMDFMMQIPOFDIBSHFS
NBOVGBDUVSFS6OJ1PXFS
SFMFBTF@EBUF
QSPEVDUT@TPME
DBUFHPSZ$FMM1IPOFT"DDFTTPSJFT
WBSJBUJPOT<
\
VOJU@QSJDF
^
>
^
$IJME%PDVNFOUT
1045DIBQUFSQSPEVDU@SFWJFXQBSFOU
\
VTFS@JEVTFS
DPNNFOU0OFPGUIFCFTUQIPOFTJOUIFNBSLFU
^
In the next few sections, we will use aggregations to analyze the products sold on the e-
commerce site.
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 282 ]
Query structure
The aggregation query is executed using the search API. You can add a query to restrict the
document you want to run the aggregations on. Let's start by looking at a simple
aggregation to group all the products by their category. In the SQL world, the query would
look like the following:
TFMFDUDPVOU
GSPN1SPEVDU
HSPVQCZDBUFHPSZ
In Elasticsearch, to achieve the same, we would do the following:
1045DIBQUFS@TFBSDI
\
BHHT\
DBUFHPSZ@QSPEVDUT\
 "terms" : {
       "field" : "category"
      }
^
^
^
The basic structure is as follows:
BHHT\$BOCFBHHTPSBHHSFHBUJPO
DBUFHPSZ@QSPEVDUT\/BNFPGUIFBHHSFHBUJPO
UFSNT\5ZQFPGUIFBHHSFHBUJPO
The results of the preceding query are as follows:
\

BHHSFHBUJPOT\
"category_products": { # Name of the aggregation. When you have
multiple levels, the name can used to identify the aggregation.
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
\
"key": "Cell Phones",
EPD@DPVOU
^
\
"key": "Cell Phones Accessories",
EPD@DPVOU
^
>

How to Slice and Dice Your Data Using Aggregations
[ 283 ]
^
^
^
It is often not effective to run aggregations on the complete data set. You should filter the
data before running the aggregations, for example, if you want to view all the products
made by Samsung grouped by their category. The SQL query would look the following:
TFMFDUDPVOU
GSPN1SPEVDU
XIFSFNBOVGBDUVSFS	4BNTVOH	
HSPVQCZDBUFHPSZ
In Elasticsearch, to achieve the same, we would do the following:
1045DIBQUFS@TFBSDI
\
RVFSZ\
NBUDI\
"manufacturer": "Samsung"
^
^
BHHT\
DBUFHPSZ@QSPEVDUT\
UFSNT\
"field": "category"
^
^
^
^
The results of the preceding query are as follows:
\

BHHSFHBUJPOT\
"category_products": {

CVDLFUT<
\
"key": "Cell Phones",
EPD@DPVOU
^
>
^
^
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 284 ]
As you can see from the preceding response, there is only one category for Samsung. If you
want only the aggregation results and not the actual documents (hits), you can set the size
to zero as shown next:
1045DIBQUFS@TFBSDI
\
 "size": 0,
BHHT\
DBUFHPSZ@QSPEVDUT\
UFSNT\
GJFMEDBUFHPSZ
^
^
^
^
The response to the preceding query will contain only the aggregations.
Multilevel aggregations
In the previous section, we discussed running aggregation on a single field; the real power
of aggregation is nesting multiple levels of aggregations. We want to group all the products
by their category and compute the average number of products sold in each bucket. The
SQL query would look like the following:
TFMFDUBWH
QSPEVDUT@TPMEGSPN1SPEVDU
HSPVQCZDBUFHPSZ
In Elasticsearch, we need to use two levels of aggregations:
The first level of aggregation groups the products by their category. The 
1.
aggregations used to bucket or group all the documents satisfying a particular
criterion are known as bucket aggregations.
The second level of aggregation, which calculates the average number of
2.
products sold for each bucket, is known as metric aggregation. Metric 
aggregation takes one or more values as input and calculates a single-value
metric, such as averages, min, and max. The input can be the result of a different 
aggregation or a field in the document.

How to Slice and Dice Your Data Using Aggregations
[ 285 ]
In the preceding example, to calculate the average number of products sold, we will use
average aggregation, which is a metric aggregation:
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
 "category_products": {
"terms": { # Bucket Aggregation
GJFMEDBUFHPSZ
^
BHHT\
"products_sold": {
"avg": { # Metric Aggregation
GJFMEQSPEVDUT@TPME
^
^
^
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
"category_products": {
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
\
"key": "Cell Phones", // First Level
EPD@DPVOU
"products_sold": { // Second Level
WBMVF
^
^
\
"key": "Cell Phones Accessories",
EPD@DPVOU
"products_sold": {
WBMVF
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 286 ]
^
^
>
^
^
^
Suppose we want to find out the manufacturers for each category. To do this, we need to
use terms aggregation to first group the products by their category and then use nested
aggregation to group the products in each category by the manufacturer. The query is as
follows:
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
"category_products": {
UFSNT\
GJFMEDBUFHPSZ
^
BHHT\
 "manufacturer": {
UFSNT\
GJFMENBOVGBDUVSFS
^
^
^
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
"category_products": { // First Level
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
"buckets": [ // Buckets in the first level
\
"key": "Cell Phones",
EPD@DPVOU

How to Slice and Dice Your Data Using Aggregations
[ 287 ]
"manufacturer": { // Second Level
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
"buckets": [ // Buckets in the second level
\
LFZ"QQMF
EPD@DPVOU
^
\
LFZ4BNTVOH
EPD@DPVOU
^
>
^
^
\
LFZ$FMM1IPOFT"DDFTTPSJFT
EPD@DPVOU
NBOVGBDUVSFS\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
\
LFZ6OJ1PXFS
EPD@DPVOU
^
>
^
^
>
^
^
^
As you see from the preceding response, the first level, DBUFHPSZ@QSPEVDUT, contains
products grouped by their category (Cell Phone, Cell Phone Accessories). Each bucket in the
DBUFHPSZ@QSPEVDUT group is further grouped by the manufacturer (Apple, Samsung, and
UniPower).
Types of aggregations
At a higher level, Elasticsearch supports four types of aggregation:
Bucket aggregation: This can be used to group or create buckets. Buckets can be
created based on an existing field, custom filters, ranges, and so on
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 288 ]
Metric aggregation: This can be used to calculate a metric, such as a count, sum,
average, and so on
Pipeline aggregation: This can be used to chain aggregations. The output of other
aggregations can be the input for a pipeline aggregation
Matrix aggregation: This can be used to calculate statistics over a set of fields
In this section, we will discuss bucket and metric aggregations. Pipeline and matrix
aggregations are still experimental and out of the scope of this book.
Terms aggregations (group by)
Terms aggregation is one of the most commonly used methods of aggregation. Terms 
aggregation is similar to HSPVQCZ in the SQL world. Terms aggregation creates a bucket
for every unique value. A simple terms aggregation is shown here:
1045DIBQUFS@TFBSDI
\
BHHT\
DBUFHPSZ@QSPEVDUT\
"terms" : {
       "field" : "category"
      }
^
^
^
The response will contain the top buckets.
Size and error
Your index data is spread across multiple shards, so the aggregation query is sent to all the
shards, and each shard responds back with its view of the data. Each shard responds with
its top buckets, and the coordinating node combines the results and sends them back to the
client.
Each shard only responds with the top buckets, and there is a possibility of
error, and the number of documents in each bucket is approximated,
which is indicated in the response.

How to Slice and Dice Your Data Using Aggregations
[ 289 ]
You may go through the following query:
BHHSFHBUJPOT\
DBUFHPSZ@QSPEVDUT\
 "doc_count_error_upper_bound" : 0,
CVDLFUT<#VDLFUTJOUIFGJSTUMFWFM
\
You can also specify the number of buckets you want in the response by specifying the size
as shown here:
1045DIBQUFS@TFBSDI
\
BHHT\
DBUFHPSZ@QSPEVDUT\
UFSNT\
GJFMEDBUFHPSZ
 "size": 2
^
^
^
^
When the size is specified, each shard is asked to return the number of the buckets equal to
the size. If the number of buckets is equal the size, the document count in the buckets will
be accurate.
Order
By default, the buckets are ordered in descending order based on the document count. The
buckets with the most documents are shown at the top. You can change the order to be
ascending, alphabetical, or even based on the value from sub-aggregation. Doing so will
increase the possibility of the error. The top buckets will be the most accurate. If the results
are calculated based on a single shard due to the use of routing or on a single shard index,
the results are always accurate. The terms aggregation with order is shown here:
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
DBUFHPSZ@QSPEVDUT\
UFSNT\
GJFMEDBUFHPSZ
"order": {
           "_term": "asc"
         }
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 290 ]
^
^
^
^
In the preceding query, the buckets are ordered alphabetically.
Minimum document count
When you have a large number of buckets, it is possible to filter the buckets that contain
documents below a specified threshold value:
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
DBUFHPSZ@QSPEVDUT\
UFSNT\
GJFMEDBUFHPSZ
"min_doc_count": 5
^
^
^
^
In the preceding query, all the buckets that don't contain a minimum of  documents will be
excluded from the results.
Missing values
The documents that don't contain the field value are ignored by default. You can assign a
default value for the documents missing the value:
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
DBUFHPSZ@QSPEVDUT\
UFSNT\
GJFMEDBUFHPSZ
 "missing": "Other"
^
^
^
^

How to Slice and Dice Your Data Using Aggregations
[ 291 ]
In the preceding query, all the documents that don't contain the DBUFHPSZ field will fall
into the 0UIFS bucket.
Aggregations based on filters
In the previous sections, we discussed how to run aggregations on an existing field. In this
section, we will discuss how to use filters to define the bucket criterion. For example, we
want to group the products based on the released date. If we group by the existing
SFMFBTF@EBUF field, it will result in a lot of buckets. So we will use the filters to group the
products that are released before 2016 as old products and the ones that are released after
2016 as new products. The query is as follows:
#BTFEPO'JMUFST
1045DIBQUFS@TFBSDI
\
BHHT\
DBUFHPSZ@QSPEVDUT\
GJMUFST\
GJMUFST\
"new_products": {
SBOHF\
SFMFBTF@EBUF\
HUF
^
^
^
"old_products": {
SBOHF\
SFMFBTF@EBUF\
MUF
^
^
^
^
^
^
^
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 292 ]
As shown in the preceding query, we created two buckets using the filters based on the
release date. Next, we will use nested aggregation to calculate the average number of
products sold in each bucket. We will also modify the preceding query to have three
buckets:
2016_products: This comprises products released after 2016
2015_products: This comprises products released in 2015
Before_2015: This comprises products released before 2015
The query is as follows:
"HHTCBTFTPO'JMUFST
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
DBUFHPSZ@QSPEVDUT\
GJMUFST\
"other_bucket_key": "Before_2015", // Default bucket
GJMUFST\
2016_Products\
SBOHF\
SFMFBTF@EBUF\
HUF
^
^
^
2015_Products\
SBOHF\
SFMFBTF@EBUF\
HUF
MUF
^
^
^
^
^
   "aggs": {
         "products_sold": {
BWH\
GJFMEQSPEVDUT@TPME
^
^
^
^
^
^

How to Slice and Dice Your Data Using Aggregations
[ 293 ]
While running the aggregation, a document is placed in one of the filter buckets if it
matches the filter criteria. If the document doesn't match any of the filter criteria, a default
bucket can be defined using PUIFS@CVDLFU@LFZ. In the preceding example, products
released after  are placed in the @1SPEVDUT bucket. Products released between
 and  are placed in the @1SPEVDUT bucket. All the other documents are
placed in the #FGPSF@ bucket.
Once the buckets are created, QSPEVDUT@TPME, which is a metric aggregation, calculates the
average number of products sold in each bucket. The response to the preceding query is as
follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
DBUFHPSZ@QSPEVDUT\
CVDLFUT\
"2015_Products": {
EPD@DPVOU
"products_sold": {
WBMVFOVMM
^
^
"2016_Products": {
EPD@DPVOU
"products_sold": {
WBMVF
^
^
 "Before_2015": {
EPD@DPVOU
"products_sold": {
WBMVF
^
^
^
^
^
^
You can see from the preceding response that three buckets are created and the average
number of products sold in each bucket is calculated.
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 294 ]
Aggregations on dates ( range, histogram )
Elasticsearch aggregations are calendar aware, which makes working with dates very
simple. Let's start with range aggregation. Just like we used filter aggregations to define
buckets based on filters, we can use date range aggregation to define buckets based on date
ranges. The advantage of using date range aggregation is that it is calendar aware and can
understand date math (1M, 1h, 1Y).
For example, suppose we want to group all the products based on the product release date.
The example used in filter aggregation can be rewritten using range aggregation as shown
here:
%BUF3BOHF
1045DIBQUFS@TFBSDI
\
BHHT\
SBOHF\
EBUF@SBOHF\
GJFMESFMFBTF@EBUF
"ranges": [
\
GSPN
"to": "now"
^
\
GSPN
UP
^
\
UP
^
>
^
^
^
^
The range includes GSPN value and excludes UP value.

How to Slice and Dice Your Data Using Aggregations
[ 295 ]
Note that OPX in the preceding query is in the UTC time zone. Time zone can be specified
using the UJNF@[POF parameter as shown next:
%BUF3BOHFXJUI5JNF[POF
1045DIBQUFS@TFBSDI
\
BHHT\
SBOHF\
EBUF@SBOHF\
GJFMESFMFBTF@EBUF
"time_zone": "PST8PDT",
SBOHFT<
\
GSPN
UPOPX
^
\
GSPN
UP
^
\
UP
^
>
^
^
^
^
In the preceding example, we are using the Pacific Time zone. You can choose the time zone
from the Joda website shown here:
IUUQXXXKPEBPSHKPEBUJNFUJNF[POFTIUNM
We used range aggregations to divide the data into three buckets. A simpler way to do this
is to use date histogram. Date histogram automatically creates the buckets based on the
interval specified. The preceding query can be written using data histogram as shown next:
%BUF)JTUPHSBN
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
QSPEVDU@SFMFBTF\
EBUF@IJTUPHSBN\
GJFMESFMFBTF@EBUF
"interval": "1y",
         "format": "YYYY-MM-dd"
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 296 ]
^
^
^
^
Since we are using the date histogram, it's calendar aware, and the interval can be set to one
year. The interval can either be a year, quarter, month, week, day, hour, minute, or second,
or a time unit can also be specified as shown in the preceding query. The following time
units are supported:
Time unit Description
y
years
M
months
w
weeks
d
days
h
hours
H
hours
m
minutes
s
seconds
Date internally is stored as a long value, and the format parameter, as shown in the
preceding query, can be used to show the bucket keys in the format specified. The response
to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
QSPEVDU@SFMFBTF\
CVDLFUT<
\
"key_as_string": "2014-01-01",
LFZ
EPD@DPVOU
^
\
"key_as_string": "2015-01-01",

How to Slice and Dice Your Data Using Aggregations
[ 297 ]
LFZ
EPD@DPVOU
^
\
 "key_as_string": "2016-01-01",
LFZ
EPD@DPVOU
^
\
"key_as_string": "2017-01-01",
LFZ
EPD@DPVOU
^
>
^
^
^
As you can see, histogram is very handy and one of the most popular features of
Elasticsearch. Once the buckets are computed using histograms, you can use a metric
aggregation, for example, to get the average number of product sold in each year (bucket)
as shown here:
"WFSBHFQSPEVDUTTPMEFBDIZFBS
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
QSPEVDU@SFMFBTF\
"date_histogram": { #Bucket Aggregation
GJFMESFMFBTF@EBUF
JOUFSWBMZ
GPSNBU::::..EE
^
BHHT\
BWH@QSPEVDUT@TPME\
"avg": { #Metric Aggregation
GJFMEQSPEVDUT@TPME
^
^
^
^
^
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 298 ]
Aggregations on numeric values (range,
histogram)
Just like date range aggregation, range aggregation is used to define buckets based on the
ranges specified. In the following example, we will use range aggregation to group the
products based on the number of products sold. The query is as follows:
"HHTCBTFTPO'JMUFST
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
QSJDF@SBOHF\
SBOHF\
GJFMEQSPEVDUT@TPME
"ranges": [
\
UP
^
\
GSPN
UP
^
\
GSPN
^
>
^
^
^
^
The response is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
QSJDF@SBOHF\
CVDLFUT<
\
 "key": "*-10.0",
UP

How to Slice and Dice Your Data Using Aggregations
[ 299 ]
EPD@DPVOU
^
\
 "key": "10.0-20.0",
GSPN
UP
EPD@DPVOU
^
\
 "key": "20.0-*",
GSPN
EPD@DPVOU
^
>
^
^
^
You can see from the preceding response that the keys of the buckets are formed using the
GSPN and UP values. We can also define the keys for each bucket as shown next:
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
QSPEVDUT@TPME\
SBOHF\
GJFMEQSPEVDUT@TPME
SBOHFT<
\
"key": "Low",
UP
^
\
 "key": "Medium",
GSPN
UP
^
\
"key": "High",
GSPN
^
>
^
^
^
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 300 ]
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
QSPEVDUT@TPME\
CVDLFUT<
\
"key": "Low",
UP
EPD@DPVOU
^
\
"key": "Medium",
GSPN
UP
EPD@DPVOU
^
\
"key": "High",
GSPN
EPD@DPVOU
^
>
^
^
^
Histogram aggregation makes it very easy to bucket the data based on an interval. When
using range aggregation, we defined our own buckets; histogram aggregation defines the
buckets automatically based on an interval. For example, suppose we want to bucket the
number of cell phones sold. We can use histogram aggregation as shown next:
1045DIBQUFS@TFBSDI
\
TJ[F
RVFSZ\
NBUDI\
DBUFHPSZ$FMM1IPOFT
^
^
BHHT\
QSPEVDUT@TPME\

How to Slice and Dice Your Data Using Aggregations
[ 301 ]
 "histogram": {
GJFMEQSPEVDUT@TPME
JOUFSWBM
^
^
^
^
In the preceding query, we set the interval as . The buckets are created in the intervals of .
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
QSPEVDUT@TPME\
CVDLFUT<
\
"key": 10,
EPD@DPVOU
^
\
"key": 15,
EPD@DPVOU
^
\
"key": 20,
EPD@DPVOU
^
\
"key": 25,
EPD@DPVOU
^
\
"key": 30,
EPD@DPVOU
^
\
 "key": 35,
EPD@DPVOU
^
>
^
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 302 ]
^
You can see from the preceding response that the number of products that are sold at least
10 times is  and the number of products that are sold at least 35 times is also . The min
and max buckets are calculated based on the data.
If we use histogram aggregations to power dashboards, we can ask Elasticsearch to return
empty buckets within a start value and an end value. The response can be used to build a
chart without modifying the response. We can use extended bounds to show the empty
buckets. In the previous example, the first bucket is  and the last bucket is . In the
following example, we will define min as  and max as . Due to extended bounds, the
response will contain buckets from  to . The query is shown next:
1045DIBQUFS@TFBSDI
\
TJ[F
RVFSZ\
NBUDI\
DBUFHPSZ$FMM1IPOFT
^
^
BHHT\
QSPEVDUT@TPME\
"histogram": {
GJFMEQSPEVDUT@TPME
JOUFSWBM
"extended_bounds": {
NJO
NBY
^
^
^
^
^
The response will now contain six buckets as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
QSPEVDUT@TPME\
CVDLFUT<

How to Slice and Dice Your Data Using Aggregations
[ 303 ]
\
"key": 0,
EPD@DPVOU
^
\
"key": 10,
EPD@DPVOU
^
\
"key": 20,
EPD@DPVOU
^
\
"key": 30,
EPD@DPVOU
^
\
 "key": 40,
EPD@DPVOU
^
\
  "key": 50,
EPD@DPVOU
^
>
^
^
^
As you can see from the preceding response, that empty buckets are added to the response.
The response can be now used a input to build a chart.
Aggregations on geolocation (distance, bounds)
In this section, we will discuss how to use geolocation for aggregations. Only fields with
explicit geolocation mapping can be used for the geolocation aggregation. The following 
geolocation aggregations are supported:
Distance
Bounds
GeoHash grid
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 304 ]
In this section, we will discuss geo distance and geo bounds aggregation and geohash grid
aggregation is out of the scope for this book. To better demonstrate location-based
aggregations, let's add a new type named TUPSF to the existing DIBQUFS index. The new
type will contain documents representing physical stores with the store name and location
fields. We can add the new mapping as shown here:
4UPSF.BQQJOH
165DIBQUFS@NBQQJOHstore
\
QSPQFSUJFT\
OBNF\
UZQFLFZXPSE
^
MPDBUJPO\
"type": "geo_point"
^
^
^
Let's index some store documents:
*OEFY4UPSFT
165DIBQUFSTUPSF
\
OBNF4UPSF
MPDBUJPO\
MBU
MPO
^
^
165DIBQUFSTUPSF
\
OBNF4UPSF
MPDBUJPO\
MBU
MPO
^
^

How to Slice and Dice Your Data Using Aggregations
[ 305 ]
Geo distance
Geo distance aggregation is very similar to range aggregation. It is used to create buckets
based on the distance from an origin location. The aggregation calculates the distance
between the document value and origin. Depending on the distance, the document is placed
to one of the buckets.
For example, suppose you want to group physical stores based on the distance from your
current location. In the following query, we will use geo-distance aggregation to define
three buckets--within  miles, between UP miles, and above  miles:
(FP%JTUBODF
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
EJTUBODF\
HFP@EJTUBODF\
GJFMEMPDBUJPO
"origin": "37.3382, -121.8863",
VOJUNJ
 "ranges": [
           {
             "to": 5
           },
           {
             "from": 5,
             "to": 10
           },
           {
             "from": 10
           }
         ]
^
^
^
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 306 ]
The documents within  miles from the origin are placed in the  miles bucket and so on.
The distance is computed as miles since the unit is specified as mi (miles). The following
distance units are supported:
Unit
Description
Mile
mi or miles
Yard
yd or yards
Feet
ft or feet
Inch
in or inch
Kilometer
km or kilometer
Meter
m or meters
Centimeter
cm or centimeters
Millimeter
mm or millimeters
Nautical mile NM or nmi or nauticalmiles
Geo bounds
Geo bounds aggregation is used to compute the smallest boundary that encloses all the geo
points. Geo bounds is a metric aggregation that takes the documents or buckets (in the case
of sub-aggregation) and outputs the boundaries. Take the following for example:
(FP#PVOET
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
CPVOEBSZ\
 "geo_bounds": {
GJFMEMPDBUJPO
^
^
^
^

How to Slice and Dice Your Data Using Aggregations
[ 307 ]
The result of the response as shown next will be the boundary that can enclose all the geo
points:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
CPVOEBSZ\
CPVOET\
"top_left": {
MBU
MPO
^
"bottom_right": {
MBU
MPO
^
^
^
^
^
As you can see from the response, the bounds will contain UPQ@MFGU and CPUUPN@SJHIU.
Aggregations on child documents
As defined in the sample data, we have the products as parent documents and the product
reviews as children documents. A sample document is shown next:
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNFJ1IPOF
NBOVGBDUVSFS"QQMF
WBSJBUJPOT<
\
DPMPS(SBZ
TUPSBHF(#
VOJU@QSJDF
^
>
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 308 ]
$IJME%PDVNFOUT
1045DIBQUFSQSPEVDU@SFWJFXQBSFOU
\
VTFS@JEVTFS
"comment": "One of the best phones in the market"
^
Suppose we want to find the number of product reviews for each manufacturer. The
product review is in the child document, and the manufacturer is in the parent document.
Due to parent-child mapping, to use the children aggregation, the child document type
(QSPEVDU@SFWJFX) should be mentioned explicitly. The query looks like the following:
$IJMESFO"HHSFHBUJPO
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
NBOVGBDUVSFS\'JSTUMFWFM
UFSNT\
"field": "manufacturer"
^
BHHT\
SFWJFXT\
"children": {#Children Aggregation
"type": "product_review"
^
BHHT\
QSPEVDU@SFWJFXT\
"value_count": { #Count
                 "field": "comment.keyword"
               }
^
^
^
^
^
^
^
In the preceding query, we used terms aggregation to group the products by their
manufacturer. Next, we used the children aggregation to count the number of comments for
each manufacturer. The response to the preceding query is as follows:
\

IJUT\
UPUBM

How to Slice and Dice Your Data Using Aggregations
[ 309 ]
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
NBOVGBDUVSFS\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
\
 "key": "Apple",
EPD@DPVOU
SFWJFXT\
EPD@DPVOU
"product_reviews": {
               "value": 1
^
^
^
\
"key": "Samsung",
EPD@DPVOU
SFWJFXT\
EPD@DPVOU
"product_reviews": {
               "value": 0
^
^
^
>
^
^
^
Since we used the WBMVF@DPVOU aggregation, the comments for each manufacturer are
counted.
Aggregations on nested documents
Just like children aggregation, aggregation on nested documents is also supported. The
product variations are stored as nested documents. A sample product document is as
follows:
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNFJ1IPOF
NBOVGBDUVSFS"QQMF
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 310 ]
"variations": [ #Nested documents
     {
       "color": "Rose Gold",
       "storage": "128GB",
       "unit_price": "900"
     },
     {
       "color": "Rose Gold",
       "storage": "64GB",
       "unit_price": "800"
     }
   ]
^
Suppose we want to find the storage options available for each color. First, we group the
products by their color using terms aggregations and then aggregate using the storage. Just
like children aggregation, to run aggregation on nested document, the path should be
mentioned explicitly. The query looks like the following:
/FTUFE"HHSFHBUJPOT
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
WBSJBUJPOT\
OFTUFE\
 "path": "variations"
^
BHHT\
 "by_color": {
UFSNT\
GJFMEWBSJBUJPOTDPMPS
^
BHHT\
 "by_storage": {
UFSNT\
GJFMEWBSJBUJPOTTUPSBHF
^
^
^
^
^
^
^
^

How to Slice and Dice Your Data Using Aggregations
[ 311 ]
We have to specify a nested path, which is then followed by the aggregation. The results of
the preceding query are as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
WBSJBUJPOT\
EPD@DPVOU
CZ@DPMPS\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
\
"key": "Gray",
EPD@DPVOU
TUPSBHF\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
{
                   "key": "16GB",
                   "doc_count": 1
                 },
                 {
                   "key": "64GB",
                   "doc_count": 1
                 }
>
^
^
\
"key": "Rose Gold",
EPD@DPVOU
TUPSBHF\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
{
                   "key": "128GB",
                   "doc_count": 1
                 },
                 {
                   "key": "64GB",
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 312 ]
                   "doc_count": 1
                 }
>
^
^
>
^
^
^
^
You can see from the response that there are two variations (16GB, 64GB) available in the
color gray and two variations (64GB, 128GB) in rose gold color.
Reverse nested aggregation
In the previous example, we aggregated on the nested documents. Reverse nested
documents will allow you to aggregate on the nested document and then aggregate on the
parent documents. The flexibility of running the aggregation on one type and then
combining it with the other is the one of the reasons why it is like nothing out there. The
following example will make it more clear. A sample product document is shown here:
165DIBQUFSQSPEVDUSFGSFTIUSVF
\
QSPEVDU@OBNFJ1IPOF
NBOVGBDUVSFS"QQMF
WBSJBUJPOT<
\
DPMPS3PTF(PME
TUPSBHF(#
VOJU@QSJDF
^
\
DPMPS3PTF(PME
TUPSBHF(#
VOJU@QSJDF
^
>
^

How to Slice and Dice Your Data Using Aggregations
[ 313 ]
For example, we want to query for the manufacturers for each color. The manufacturer field
is in the parent document, and the color is in variation stored as a nested document. We
first use terms aggregation to group the products by their color and use SFWFSTF@OFTUFE
to aggregate on the parent documents:
3FWFSTF/FTUFE"HHSFHBUJPO
1045DIBQUFS@TFBSDI
\
TJ[F
BHHT\
WBSJBUJPOT\
OFTUFE\
QBUIWBSJBUJPOT
^
BHHT\
"by_color": { //First level
UFSNT\
GJFMEWBSJBUJPOTDPMPS
^
BHHT\
NBOVGBDUVSFS\
"reverse_nested": {}, // Parent
BHHT\
QSPEVDU\
UFSNT\
"field": "manufacturer"
^
^
^
^
^
^
^
^
^
^
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<>
^
BHHSFHBUJPOT\
WBSJBUJPOT\
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 314 ]
EPD@DPVOU
CZ@DPMPS\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
\
"key": "Gray",
EPD@DPVOU
NBOVGBDUVSFS\
EPD@DPVOU
QSPEVDU\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
 {
                     "key": "Apple",
                     "doc_count": 1
                   },
                   {
                     "key": "Samsung",
                     "doc_count": 1
                   }
>
^
^
^
\
"key": "Rose Gold",
EPD@DPVOU
NBOVGBDUVSFS\
EPD@DPVOU
QSPEVDU\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<
 {
                     "key": "Apple",
                     "doc_count": 1
                   }
>
^
^
^
>
^
^
^
^

How to Slice and Dice Your Data Using Aggregations
[ 315 ]
Going back to the parent document is only possible on nested documents. Using the parent-
child document structure, you can only aggregate on the children document. If you need
reverse aggregation, you need to use the nested document structure.
Post filter
Post filter is best explained with an example. Suppose in an e-commerce store, a user is
looking for a silver-colored cell phone. The products are stored as Elasticsearch documents,
and the variations, such as storage and color, are stored as nested documents. To find the
cell phone in silver color, the query would look like the following:
4JMWFS$PMPS$FMM1IPOFT
1045DIBQUFS@TFBSDI
\
RVFSZ\
CPPM\
NVTU<
\
NBUDI\
"category": "Cell Phones"
^
^
\
OFTUFE\
QBUIWBSJBUJPOT
RVFSZ\
NBUDI\
 "variations.color": "Silver"
^
^
^
^
>
^
^
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 316 ]
The response to the preceding query contains cell phones in silver color. Suppose that along
with silver, we want to display all the color variations that are available. To get the list of all
the available colors, we can use terms aggregation on the WBSJBUJPOTDPMPS field. We
cannot add the aggregation to the existing query as the aggregation is performed on the
documents that are a result of the main query. In the preceding example, the result of the
main query is cell phones that are in silver color, but we need to run the aggregation on all
the cell phones. To do this, we can use post filter. Post filter is executed after the
aggregation is run. In the following query, we will use the main query to filter the products
to cell phones and the post filter query to filter them to the silver color. When the following
query is executed, the order of execution is as follows:
First, the top-level query for category cell phones is executed.
1.
Next, the CZ@DPMPS aggregation is executed on the products that are cell phones.
2.
Next, the QPTU@GJMUFS query is executed to filter the cell phones that are silver
3.
in color.
Using post filter, the aggregation is performed on products that belong to the cell phone
category. The actual hits in the response are the cell phones in silver, and the aggregation
results will include all the available color options:
1PTU'JMUFS
1045DIBQUFS@TFBSDI
\
"query": {
NBUDI\
"category": "Cell Phones"
^
^
"aggs": {
WBSJBUJPOT\
OFTUFE\
QBUIWBSJBUJPOT
^
BHHT\
"by_color": {
UFSNT\
 "field": "variations.color"
^
^
^
^
^
"post_filter": {
OFTUFE\
QBUIWBSJBUJPOT

How to Slice and Dice Your Data Using Aggregations
[ 317 ]
RVFSZ\
NBUDI\
"variations.color": "Silver"
^
^
^
^
^
The response to the preceding query is as follows:
\

IJUT\
"total": 0, # No Silver color
NBY@TDPSFOVMM
IJUT<>
^
BHHSFHBUJPOT\
WBSJBUJPOT\
EPD@DPVOU
CZ@DPMPS\
EPD@DPVOU@FSSPS@VQQFS@CPVOE
TVN@PUIFS@EPD@DPVOU
CVDLFUT<0UIFSPQUJPOT
\
 "key": "Gray",
EPD@DPVOU
^
\
"key": "Rose Gold",
EPD@DPVOU
^
>
^
^
^
^
You can see from the response that the number of hits is . We don't have cell phones in
silver, but we can show the user the gray and rose gold color options.
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 318 ]
Using Kibana to visualize aggregations
In this section, we will discuss how to use Kibana to visualize aggregations. We will
demonstrate a very simple bar chart to represent the terms aggregation. To use an index in
Kibana, we have to add the index pattern to Kibana. To do this, perform the following
steps:
Go to the Management tab on the left.
1.
Choose Index Pattern, and you should see a screen similar to the following:
2.
Input the index name or index pattern you would like to run the visualizations on. If you
input an index pattern with a wildcard such as DIBQUFS, all the indices that start with the
name DIBQUFS are included in the visualization. Once you click on create, the next screen
should show you the mappings of the index. If the mappings look OK, perform the
following steps to create a new visualization:
Go to the Visualize tab on the left.
1.
In the Create New Visualization screen, you can choose the type of visualization
2.
you would like. For this example, let's choose a vertical bar chart.
Next, choose the index or index pattern you would like the visualization on.
3.
You will be redirected to a screen similar to the following:
4.

How to Slice and Dice Your Data Using Aggregations
[ 319 ]
For the Y-Axis, we will use the number of documents, which is represented as
5.
count.
For the X-Axis, we will use terms aggregation on the manufacturer field.
6.
For Order By, we will use the default metric count.
7.
After everything is set, press the play button next to the options tab to display the
8.
visualization.
If you do not see any data in the chart, that is probably because of the date
time filter, which is automatically added to every Kibana request. It
defaults to the last 15 minutes. The date time filter is based on the date
field selected while adding a new index pattern. The date time filter can be
changed by clicking on the clock icon in the top right corner.
Kibana visualizations are very flexible. For the preceding chart, you can also add sub-
buckets to split the bar chart. For example, we first grouped the products by the
manufacturer (Apple, Samsung); we can add sub-buckets to further group each bucket by
the category (Cell Phone, Cell Phone Accessories).
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 320 ]
Caching
When you execute an aggregation query, the node that receives the request sends the query
to all the shards of the index. The results from each shard are gathered back and sent to the
client. The aggregation results are cached at a shard level. Computing an aggregation is an
expensive operation. Caching the response greatly improves the performance and reduces
the strain on the system. Elasticsearch cache is smart and is automatically invalidated where
there is new data. Since the cache is per shard, the cache is invalidated only for the shards
that have new/modified data. Starting Elasticsearch 5.0, the request cache is enabled by
default. The query JSON is used as a key.
Cache greatly improves performance for indexes that have static data. For example, if you
have a time-based index, the old indexes are not changed anymore. The aggregation results
for the old data can be served directly from the cache. By default,  of the heap memory is
allocated to the request cache. If your data is constantly changing, having a cache may not
help at all. If you have dashboards based on aggregations that are refreshed automatically
on a schedule, running the entire aggregation every few minutes or seconds can be very
expensive. With the request cache, the aggregation needs to be computed only on the
recently added data. The request cache can be increased using the FMBTUJDTFBSDIZNM file
as shown here:
JOEJDFTSFRVFTUTDBDIFTJ[F
Doc values
Before we jump into doc values, let's quickly refresh what an inverted index is and why it is
needed. Let's says we have the following documents:
Doc 1: Apple
Doc2: Apple
Doc3: Samsung
The inverted index for the preceding documents looks like the following:
Term
Doc ID
Apple
1, 2
Samsung 3

How to Slice and Dice Your Data Using Aggregations
[ 321 ]
To find all the products manufactured by Apple, we would simply use a match query as
shown here:
\
RVFSZ\
NBUDI\
NBOVGBDUVSFS"QQMF
^
^
^
With the help of inverted index, we can quickly look up all the documents associated with
term Apple. But if you want to sort or run the aggregation using the inverted index, we
have to go through the entire terms list and collect the document IDs, which is practically
not possible. To solve this problem, doc values are introduced. Doc values for the preceding
documents are as follows:
Doc ID Terms
1
Apple
2
Apple
3
Samsung
Let's take an example of terms aggregation on the manufacturer:
\
TJ[F
BHHT\
NBOVGBDUVSFS\
UFSNT\
GJFMENBOVGBDUVSFS
^
^
^
^
The results of the preceding query are as follows:
\
BHHSFHBUJPOT\
NBOVGBDUVSFS\
CVDLFUT<
\
"key": "Apple",
EPD@DPVOU
^
www.ebook3000.com

How to Slice and Dice Your Data Using Aggregations
[ 322 ]
\
"key": "Samsung",
EPD@DPVOU
^
>
^
^
^
To execute the preceding aggregation query, Elasticsearch uses the doc values to read the
term and create a bucket for each unique term or increment the EPD@DPVOU if the bucket
already exists. Similarly, to sort by manufacturer, Elasticsearch uses doc values to get the
value of the fields and sort them accordingly. Doc values are also used by the script to
access a field value in a document. Doc values are enabled, by default, for all fields except
analyzed fields. For analyzed fields, field data can be used, which is turned off by default.
When you index a document, the document in inserted into both inverted index and doc
values. Doc values, just like inverted index, are persisted to disk, and Elasticsearch uses the
file system cache instead of loading them into the JVM memory. By not loading doc values
into the JVM heap, less memory can be allocated to JVM, which, in turn, means fewer
garbage collections and less heap pressure.
Field data
Only non-analyzed fields are stored in doc values. For aggregations, sorting, and scripting
on an analyzed field, an in-memory structure called field data is used. Unlike doc values,
which live on disk, field data lives in the JVM heap memory due to which it is not very
scalable and can cause out-of-memory exceptions. Field data is lazily loaded the first time
you try to run an aggregation or sort on an analyzed field. Field data is built from the
inverted index of the field, which is an expensive operation and can use significant
memory.
Non-analyzed fields are, by default, stored in the doc values, and you can use multi-fields
to index the same field as analyzed and non-analyzed fields. You can use the analyzed field
for searching and the non-analyzed field for aggregations and so on. Field data is disabled
by default, and if you need to run aggregations on an analyzed field, you can enable field
data by setting GJFMEEBUB to USVF in the field mapping.

How to Slice and Dice Your Data Using Aggregations
[ 323 ]
Summary
In this chapter, you learned about the different types of aggregations supported by
Elasticsearch. We discussed how to run multilevel aggregations, for example, grouping a
document based on a field and computing the average of each bucket. We also discussed
how to run aggregations on geolocation, parent-child, and nested documents. You learned
how to use Kibana to visualize the aggregations. We also discussed about doc values and
field data which power the aggregations internally.
In the next chapter, we will discuss how to configure Elasticsearch.
www.ebook3000.com

9
Production and Beyond
This chapter is a flight checklist before going to production. Youâ€™ll learn about some
important Elasticsearch metrics to monitor once you are in production. Since Elasticsearch
is an open source, there are a lot of configurable settings. Youâ€™ll learn about some of the
most important settings and how to tailor them to your needs. You'll also learn how to
install X-Pack and use the monitoring feature of X-Pack.
In this chapter, we will cover the following topics:
Configuration
Cluster API
Monitoring
X-Pack
Thread Pools
Elasticsearch server logs
Configuring Elasticsearch
Elasticsearch is designed to work out of the box. Most settings can be changed on the fly
using the settings API. While in development, the default settings should be sufficient. But
for production, depending on your application, you need to modify settings, such as
memory, file descriptors, and so on. In this section, we will look at the important settings to
configure before going to production. First, let's start by looking at the directory structure of
Elasticsearch.

Production and Beyond
[ 325 ]
The directory structure
You can choose between [JQUBSH[ or EFCSQN to install Elasticsearch. Depending on
how you install, the directory structure, location of log files, location of the configuration
file, and how you can start/stop Elasticsearch are different.
zip/tar.gz
Let's look at the directory structure when you installed Elasticsearch using a [JQUBSH[
file. By default, when you unzip the FMBTUJDTFBSDI package, the following
directories are within it:
Name
Description
bin
This contains all the binary files to start Elasticsearch and to install plugins.
config
This contains configuration files (elasticsearch.yml).
data
This contains all the data files. There is where your data is stored.
logs
This contains all the log files.
plugins This contains all the plugins.
lib
This contains all the libraries required to run Elasticsearch.
The data and logs directories are created the first time Elasticsearch was run. We can change
the default locations of the directories from the config file. For example, if we want the data
folder in a different location, we can modify the location in the configuration file.
DEB/RPM
Let's look at the directory structure when you installed Elasticsearch using the DEB/RPM
package:
Name
Description
Location
home
This is the location where Elasticsearch is installed.
/usr/share/elasticsearch
bin
This contains all the binary files to start Elasticsearch and
to install plugins.
/usr/share/elasticsearch/bin
conf
This contains configuration files (FMBTUJDTFBSDIZNM).
/etc/elasticsearch
conf
This contains configuration for environment variables.
/etc/sysconfig/elasticsearch
www.ebook3000.com

Production and Beyond
[ 326 ]
data
This contains all the data files. There is where your data is
stored.
/var/lib/elasticsearch
logs
This contains all the log files.
/var/log/elasticsearch
plugins This contains all the plugins.
/usr/share/elasticsearch/plugins
script
This contains all the script files.
/etc/elasticsearch/scripts
We can change the default locations of the directories from the configuration file. For
example, if we want the data folder in a different location, we can change the location in the
configuration file.
Configuration file
The configuration file contains settings specific to a single node. Each node in the cluster
has to be configured independently. The configuration files are located in the config
directory. Config directory will have the following files:
FMBTUJDTFBSDIZNM: This is the Elasticsearch configuration file. It is divided
into cluster, node, paths, memory, and so on. Each section contains the defaults,
which can be edited.
MPHKQSPQFSUJFT: This is the logging configuration.
KWNPQUJPOT: This is the configuration file to set the JVM settings, such as heap
size and so on. If you installed Elasticsearch using a DEB/RPM package, you
should configure the environment variables, such as heap size, in
FUDTZTDPOGJHFMBTUJDTFBSDI.
We will talk about important sections in the configuration file in the sections to follow.
Cluster and node name
In this section, you will learn how to change the cluster and node name. Having a unique
cluster name is crucial. By default, Elasticsearch assigns random names to the nodes, and
the cluster name defaults to FMBTUJDTFBSDI. It is very common to have a cluster for
development, staging, production, and so on. Without a unique cluster name, a node
joining a cluster doesn't know which cluster to join. Also, setting the node name to
something such as OPEF or FMBTUJDTFBSDI comes in handy when monitoring or
debugging.

Production and Beyond
[ 327 ]
We can change the node and cluster name by editing the configuration file. Any changes to
the configuration file will come into effect only after you restart Elasticsearch. Follow the
instructions provided next to modify the cluster and node name:
Open the FMBTUJDTFBSDIZNM file located in the config directory using your
1.
favorite text editor.
In the config file under the cluster section, change the cluster name shown as
2.
follows:
DMVTUFSOBNFFTEFW
Under the node section, modify the node name shown as follows:
3.
OPEFOBNFFMBTUJDTFBSDI
Save the config file.
4.
If you have an Elasticsearch instance already running, restart it.
5.
Verify the new settings by pointing to IUUQ in your favorite
6.
browser, and you should see a response similar to the following:
\
4.
OBNFelasticsearch1
DMVTUFS@OBNFes-dev
DMVTUFS@VVJE.QU[JZL4KZN'X#:P[.3"
WFSTJPO\
OVNCFS
CVJME@IBTIDDD
CVJME@EBUF5;
CVJME@TOBQTIPUGBMTF
MVDFOF@WFSTJPO
^
UBHMJOF:PV,OPXGPS4FBSDI
^
You can see from the JSON response that the node name is now FMBTUJDTFBSDI and the
cluster name is FTEFW.
www.ebook3000.com

Production and Beyond
[ 328 ]
By default, all the lines in the configuration file are commented out using
the  symbol at the beginning of the line, for example, DMVTUFSOBNF
NZBQQMJDBUJPO. You have to remove the  symbol for the setting to
come into effect.
Network configuration
Elasticsearch binds to the localhost or  by default, which works great for
development. When you bind to , only clients running locally will be able to
access it. Only after you bind to an external IP address, external clients can communicate
with the node.
Elasticsearch assumes it is in the development mode as long as it binds to
the default address (). The default address is a loopback
address and can't communicate externally. Once an external interface is
set, it assumes it is in the production mode and enforces more checks
before starting the instance. What previously were warnings while in the
development mode will now become errors.
In the configuration file under the network section, you can set the OFUXPSLIPTU to an IP
address or a hostname, the node should bind to. For example, if you want to bind the node
to , you can do this by the setting the OFUXPSLIPTU as shown next:
OFUXPSLIPTU
When you start Elasticsearch with the OFUXPSLIPTU set, you will see in the console/log
that Elasticsearch is bound to the new address.
You can also bind Elasticsearch to . In the context of a server,
 means all IPv4 addresses available. For example, if a server has
two IP addresses,  and , the Elasticsearch
instance can be reached via both the IP addresses. When you bind to
, it is also accessible via  locally.
If you need more than one node in the cluster, you need to bind Elasticsearch to an external
address so that other nodes in the cluster can communicate.

Production and Beyond
[ 329 ]
Memory configuration
By default, when you start Elasticsearch, the heap size of the JVM is (#. The more memory
available to Elasticsearch, the more memory it can use for caching, which improves the
overall performance. Also, to run aggregations on your data, there are memory
considerations. When you are ready for production, it important to make sure Elasticsearch
has enough memory. You can monitor the heap usage and garbage collections using the
node stats API. You can configure the JVM settings, such as heap size, garbage collection,
and so on in the KWNPQUJPOT file in the config directory.
Let's look at the FMBTUJDTFBSDI memory usage using the node stats API as shown next:
IUUQ@OPEFTFMBTUJDTFBSDITUBUTKWNhumanQSFUUZ
The human flag at the end of the URL makes the numbers in the response
more human readable. Let's say we are looking for IFBQ@VTFE in the node
stats response; without the flag, you will see the following:
IFBQ@VTFE
With the flag, you will see the following:
IFBQ@VTFENC
IFBQ@VTFE@JO@CZUFT
The nodes stats API response is a JSON response as shown next. I've replaced parts of the
JSON response with 		 to increase readability as the original response is quite long.
We will go through the important sections of the response in the following sections:
\
DMVTUFS@OBNFFTEFW
OPEFT\

>
KWN\
NFN\

QPPMT\
ZPVOH\

^
TVSWJWPS\

^
PME\
www.ebook3000.com

Production and Beyond
[ 330 ]

^
^
^
UISFBET\
DPVOU
QFBL@DPVOU
^
HD\
DPMMFDUPST\

^
^
^
^
Let's inspect parts of the stats response. First, let's look at the memory section:
NFN\
"heap_used": "332.9mb",
IFBQ@VTFE@JO@CZUFT
 "heap_used_percent": 16,
IFBQ@DPNNJUUFEHC
IFBQ@DPNNJUUFE@JO@CZUFT
 "heap_max": "1.9gb",
IFBQ@NBY@JO@CZUFT
OPO@IFBQ@VTFENC
OPO@IFBQ@VTFE@JO@CZUFT
OPO@IFBQ@DPNNJUUFENC
OPO@IFBQ@DPNNJUUFE@JO@CZUFT
You can see from the preceding response that the memory available for this Elasticsearch
instance is 1.9 Gb (IFBQ@NBY). And this Elasticsearch instance is currently using 332.9 MB
(IFBQ@VTFE), which is 16% (IFBQ@VTFE@QFSDFOU) of the total memory. We are currently
using only 16% of the available heap since we just started the node. But as we start using it,
memory is necessary for caching and aggregations. Depending on the available physical
memory, you can monitor the heap usage and increase the heap size. At the end of the
section, we will discuss how to increase the heap size.

Production and Beyond
[ 331 ]
Another important metric to watch while monitoring memory usage is garbage collection
(GC). Garbage collection and heap size apply to any Java application that runs on JVM. I'll
try to describe GC at a very high level as it is beyond the scope of this book. Unlike C and
C++, Java does the memory management. It lets developers create new objects without
worrying about memory deallocation. The garbage collector automatically claims the
memory from the objects that are not being used. JVM pauses the application threads to run
the garbage collection, or the application will run out of the memory. This process is also
known as TUPQUIFXPSME as the threads will pause until the garbage collection is done. It
is very important to keep the TUPQUIFXPSME time to a minimum.
Let's inspect the HD section of the node stats:
HD\
DPMMFDUPST\
ZPVOH\
DPMMFDUJPO@DPVOU
DPMMFDUJPO@UJNF@JO@NJMMJT
^
PME\
DPMMFDUJPO@DPVOU
DPMMFDUJPO@UJNF@JO@NJMMJT
^
^
^
You can see from the preceding JSON DPMMFDUJPO@UJNF@JO@NJMMJT is the time spent in
garbage collection. Keeping DPMMFDUJPO@DPVOU and DPMMFDUJPO@UJNF@JO@NJMMJT to a
minimum is important.
It is recommended not to have more than  GB JVM heap size. Around
 GB, JVM doesn't compress object pointers. Elasticsearch recommends
around  GB as a safe cutoff.
Elasticsearch provides a configuration file called KWNPQUJPOT to configure the JVM
settings. You can increase the heap size by setting the 9NT and 9NY settings. In the
KWNPQUJPOT file, under the JVM Heap Size section, set the following:
9NTH
9NYH
www.ebook3000.com

Production and Beyond
[ 332 ]
The preceding settings should set the minimum and the maximum heap to H. Once you
make these changes, restart your Elasticsearch instance and use the node stats API to verify
heap size. You should see the memory section of response similar to the following:
NFN\
IFBQ@VTFENC
IFBQ@VTFE@JO@CZUFT
IFBQ@VTFE@QFSDFOU
IFBQ@DPNNJUUFEHC
IFBQ@DPNNJUUFE@JO@CZUFT
"heap_max" : "3.9gb",
IFBQ@NBY@JO@CZUFT
OPO@IFBQ@VTFENC
OPO@IFBQ@VTFE@JO@CZUFT
OPO@IFBQ@DPNNJUUFENC
OPO@IFBQ@DPNNJUUFE@JO@CZUFT
You can see from the preceding response that the maximum heap memory available is
(.
When running in production, it is recommended to run Elasticsearch in such a way that it is
not sharing the resources with any other applications running on the same machine.
Depending on the available memory, set the heap size to not more than  of your
physical memory so that Elasticsearch can take advantage of the file system cache. Having
enough memory for the file system cache can give you a significant performance boost.
Configuring file descriptors
If you installed Elasticsearch using a DEB/RPM package, this setting is configured
automatically. This configuration only applies to Linux and Mac OS. In UNIX operating
system, everything is a file including a network connection. A file descriptor is nothing but
a handler to the file assigned by the operating system. Once a file is opened, a file
descriptor, which is an integer, is assigned to the file. As you index documents, under the
covers they are written to Apache Lucene segments. Apache Lucene uses a lot of files to
operate. Also, Elasticsearch uses a lot of file descriptors for network communications as the
nodes need to communicate with each other. The default number of file descriptors the
operating system allows will not be sufficient for Elasticsearch. If the operating system runs
out of file descriptors, Elasticsearch might lose data. You need to increase the number of file
descriptors for the user running Elasticsearch. Elasticsearch recommends  or higher.

Production and Beyond
[ 333 ]
You can either set the descriptors by running the following command as root before starting
Elasticsearch:
VMJNJUO
Alternatively, you can set the limit permanently by editing the
FUDTFDVSJUZMJNJUTDPOG and setting the OPGJMF for the user running Elasticsearch.
Types of nodes
Each Elasticsearch instance can be configured to serve a particular purpose, and you can
think of setting the node types once you are ready for production to improve the overall
performance and stability.
Following are the different types of nodes:
Master eligible node: Master node is responsible for cluster
management, which includes index creation/deletion and keeping
track of the nodes that are part of the cluster. For example, adding and
removing a node to a cluster are managed by the master node. There
will be only one master node at any point of time. A node can act as
both master and data nodes. If the master node goes down, one of the
data nodes is promoted to master automatically
Data node: Data nodes contain the actual index data. They handle all
the index and search operations on the documents
Ingest node: Ingest node is a new type of node added in Elasticsearch
5.0, which can preprocess data before indexing the data
Tribe node: Tribe node can read and write to multiple clusters at a
time
Coordinating node: Coordinating node is the node receiving a request.
It sends the query to all the shards the query needs to be executed on,
gathers the results, and sends them back to the client
Master node doesn't mean all the requests and responses are channeled
through it. You can send a request to any of the nodes in the cluster, and
the requests are routed internally by Elasticsearch. The master node is
primarily responsible for cluster management and has nothing to do with
serving the requests. The nodes communicate with each other internally to
serve the request.
www.ebook3000.com

Production and Beyond
[ 334 ]
A node can be master, data, and ingest node at the same time. Elasticsearch provides these
configuration settings to support production environments. Once in production, depending
on the criticality of your application, it is important to have a dedicated master node for the
overall stability of the cluster. Since the master node performs very lightweight operations
when compared to data nodes, the hardware for master nodes can be much lower grade
than the hardware used for data nodes. Data nodes are usually CPU and memory intensive.
The type of the node can be configured in the configuration file under the node section as
shown here:
OPEFNBTUFSUSVF
OPEFEBUBUSVF
OPEFJOHFTUUSVF
By default, a node is master eligible, data, and ingest node. Depending on the application
requirements, the configuration can be changed.
Elasticsearch provides cat API to look at different stats about health, pending tasks, nodes,
and so on. Unlike the other RESTful API, the cat API response is in a tabular format, which
is more human and terminal friendly. You can view all the nodes in the cluster and their
roles using the following nodes API. The W at the end of the URL stands for verbose:
IUUQ@DBUOPEFTW
You will see a response similar to the following table:
ip
heap.percent
ram.percent
cpu
load_1m
node.role
master
name
127.0.0.1
12
100
10
1.71
mdi
*
elasticsearch1
You can see from the response that there is only one in the cluster right now. And the role
of the node is mentioned in the OPEFSPMF column as NEJ, meaning master, data, and
ingest node. And since we only have one node in the cluster, it is also the master node
indicated by the  symbol in the master column, and the name of the node is
FMBTUJDTFBSDI.
Multinode cluster
Elasticsearch is a distributed system, and a cluster contains one or more nodes. In this
section, we will discuss how to add a new node to the cluster.
Let's say we have two servers with IP addresses  and .

Production and Beyond
[ 335 ]
Follow the instructions laid down next to start a two-node cluster:
Install Elasticsearch in the first server. Please follow the instructions in the
1.
Installing Elasticsearch section in $IBQUFS, Setting up Elasticsearch and Kibana.
Open the configuration file in the first server, and make the following changes:
2.
Change the cluster name under the cluster section:
1.
DMVTUFSOBNFFTEFW
Change the node name under the node section:
2.
OPEFOBNFFMBTUJDTFBSDI
Change the host address to bind under the network section:
3.
OFUXPSLIPTU
Set the address of other nodes in the cluster in the discover section:
4.
EJTDPWFSZ[FOQJOHVOJDBTUIPTUT
<>
Start the Elasticsearch instance in the first server.
5.
Install Elasticsearch in the second server.
2.
Open the configuration file in the second server and make the following changes:
3.
Change the cluster name under the cluster section. Having the same
1.
cluster name for both the nodes is important:
DMVTUFSOBNFFTEFW
Change the node name under the node section:
2.
OPEFOBNFFMBTUJDTFBSDI
Change the host address to bind under the network section:
3.
OFUXPSLIPTU
Set the address of other Elasticsearch nodes in the cluster in the
4.
discover section:
EJTDPWFSZ[FOQJOHVOJDBTUIPTUT
<>
www.ebook3000.com

Production and Beyond
[ 336 ]
When you start the second instance, it will try to detect the already running Elasticsearch
instances from the list of node addresses specified in
EJTDPWFSZ[FOQJOHVOJDBTUIPTUT. We will discuss discovery further in the How
nodes discover each other section.
Start Elasticsearch in the second server.
1.
Make sure both the Elasticsearch instances are started.
2.
Now that we have two nodes in the cluster, we can view the nodes in the cluster and their
roles by using the @DBU"1* as shown here:
IUUQ@DBUOPEFTW
Unlike other APIs, the response of the @DBU"1* will be in a tabular format:
ip
heap.percent ram.percent cpu load_1m node.role master name
192.168.0.1 23
100
7
1.40
mdi
*
elasticsearch1
192.168.0.2 21
100
7
1.40
mdi
-
elasticsearch2
From the response, you can see that they are two nodes in the cluster, and
FMBTUJDTFBSDI is the master node.
Now let's look at the cluster health using the cluster API as shown here:
IUUQ@DMVTUFSIFBMUIQSFUUZ
The response of the cluster API is as follows:
\
 "cluster_name": "es-dev",
"status": "green",
UJNFE@PVUGBMTF
"number_of_nodes": 2,
OVNCFS@PG@EBUB@OPEFT
BDUJWF@QSJNBSZ@TIBSET
BDUJWF@TIBSET
SFMPDBUJOH@TIBSET
JOJUJBMJ[JOH@TIBSET
VOBTTJHOFE@TIBSET
EFMBZFE@VOBTTJHOFE@TIBSET
OVNCFS@PG@QFOEJOH@UBTLT
OVNCFS@PG@JO@GMJHIU@GFUDI
UBTL@NBY@XBJUJOH@JO@RVFVF@NJMMJT
BDUJWF@TIBSET@QFSDFOU@BT@OVNCFS

Production and Beyond
[ 337 ]
^
You can see from the response that the number of nodes is UXP, the data nodes are UXP in
number, and the cluster status is HSFFO.
Inspecting the logs
If you are interested in knowing what happens internally when you add a new node, go
through the following section. Let's inspect the console/log messages when we start the first
Elasticsearch instance. You can find the location of the logs from Directory Structure section:
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>JOJUJBMJ[JOH

<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>JOJUJBMJ[FE
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUBSUJOH
<5><*/'0><PFU5SBOTQPSU4FSWJDF><FMBTUJDTFBSDI>
QVCMJTI@BEESFTT\^CPVOE@BEESFTTFT\<GF>^
\<>^\^
<5><*/'0><PFDT$MVTUFS4FSWJDF><FMBTUJDTFBSDI>
OFX@NBTUFS
\FMBTUJDTFBSDI^\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\
^\^SFBTPO[FOEJTDPFMFDUFEBTNBTUFS
<>OPEFT
KPJOFE
<5><*/'0><PFI)UUQ4FSWFS><FMBTUJDTFBSDI>
QVCMJTI@BEESFTT\^CPVOE@BEESFTTFT\<GF>^
\<>^\^
<5><*/'0><PFO/PEF>[elasticsearch1] started
This is the first Elasticsearch instance we started. From the console messages, we can see the
name of the node as FMBTUJDTFBSDI. Let's inspect parts of the console message that are
important. First, let's look at the message from the cluster service, which is responsible for
cluster management:
<5><*/'0>[o.e.c.s.ClusterService ] [elasticsearch1]
new_master
\FMBTUJDTFBSDI^\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\
^\^SFBTPO[FOEJTDPFMFDUFEBTNBTUFS
<>OPEFT
KPJOFE
In the console message, you can see that since this is the first Elasticsearch instance,
FMBTUJDTFBSDI is elected as the new master:
<PFU5SBOTQPSU4FSWJDF><FMBTUJDTFBSDI>publish_address
{192.168.0.1:9300}CPVOE@BEESFTTFT\<GF>^\<>^
\^
www.ebook3000.com

Production and Beyond
[ 338 ]
In the previous console message, you can see the message from 5SBOTQPSU4FSWJDF.
5SBOTQPSU4FSWJDF is the internal communication service. Elasticsearch uses TCP protocol
for internal communication between the nodes. From the console message, you can see that
transport service for FMBTUJDTFBSDI is now running at :
 is the default port for the HTTP service, and  is the default port
for transport service.
<5><*/'0><PFI)UUQ4FSWFS><FMBTUJDTFBSDI>
publish_address {192.168.0.1:9200}CPVOE@BEESFTTFT\<GF>^
\<>^\^
In the previous console message, you can see that FMBTUJDTFBSDI HttpServer is now
running at . The RESTful API is now available at
IUUQ.
Let's inspect the console log when we start the second instance:
<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>JOJUJBMJ[JOH

<5><*/'0><PFO/PEF><FMBTUJDTFBSDI>TUBSUJOH
<5><*/'0><PFU5SBOTQPSU4FSWJDF><FMBTUJDTFBSDI>
QVCMJTI@BEESFTT\^CPVOE@BEESFTTFT\<GF>^
\<>^\^
<5><*/'0><PFDT$MVTUFS4FSWJDF><FMBTUJDTFBSDI>
EFUFDUFE@NBTUFS
\FMBTUJDTFBSDI^\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\
^\^BEEFE
\\FMBTUJDTFBSDI^\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\
^\^^SFBTPO[FOEJTDPSFDFJWF
GSPNNBTUFS<NBTUFS
\FMBTUJDTFBSDI^\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\
^\^DPNNJUUFEWFSTJPO<>>
<5><*/'0><PFI)UUQ4FSWFS><FMBTUJDTFBSDI>
QVCMJTI@BEESFTT\^CPVOE@BEESFTTFT\<GF>^
\<>^\^
<5><*/'0><PFO/PEF>[elasticsearch2] started
Let's examine the cluster service part of the preceding log. When we started the first
instance, it is elected as master. Since we already have a master, the second instance will
automatically detect the current master as shown in the following log:
<5><*/'0><PFDT$MVTUFS4FSWJDF><FMBTUJDTFBSDI>
detected_master
{elasticsearch1}\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\

Production and Beyond
[ 339 ]
^\^BEEFE
\\FMBTUJDTFBSDI^\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\
^\^^SFBTPO[FOEJTDPSFDFJWF
GSPNNBTUFS<NBTUFS
\FMBTUJDTFBSDI^\Y;KZ'&2Z(YFIDZTZEH^\%::;R[D5N994&OS,D"^\
^\^DPNNJUUFEWFSTJPO<>>
Elasticsearch uses a discovery mechanism it calls zen discovery to detect the other nodes in
the cluster.
How nodes discover each other
Zen discovery is the discovery module used by Elasticsearch. Since Elasticsearch is a
distributed system, you can think of this module as a glue that keeps the cluster together.
Cluster management and failure detection are handled automatically by Elasticsearch.
In the configuration file, there is a discovery section dedicated to zen discovery. One of the
settings in the discovery section is EJTDPWFSZ[FOQJOHVOJDBTUIPTUT. This setting is
a list of other hosts Elasticsearch is running, so that the node can join the existing nodes to
form a cluster. When we start the FMBTUJDTFBSDI instance, this instance will first try to
ping the hosts in EJTDPWFSZ[FOQJOHVOJDBTUIPTUT. It will scan the ports  to
 and find FMBTUJDTFBSDI running at . (Note that  is the
port for internal communication,  is the HTTP server.) To join the cluster, both the
nodes should have the same cluster name. If it cannot find any other Elasticsearch instances
running, it elects itself as the master and starts as a single-node cluster.
When you are running Elasticsearch on two different machines, it is important to set the IP
address of the two nodes in the EJTDPWFSZ[FOQJOHVOJDBTUIPTUT settings. Without
this, there is no way for Elasticsearch to discover other nodes. Starting Elasticsearch 5, only
unicast is supported and multicast has been removed.
In the discovery section of the config file, set the following:
EJTDPWFSZ[FOQJOHVOJDBTUIPTUT<OPEFOPEF>
www.ebook3000.com

Production and Beyond
[ 340 ]
Node failures
Failure detection is also an important functionality that the discovery module takes care of.
Master nodes ping all the nodes in the cluster frequently to make sure they are alive as
shown in the following diagram. In the same way, all the non-master nodes ping the master
node regularly to ensure the master node is alive. If any failures are detected, they are
handled as we described in the Failure Handling section in $IBQUFS, Introduction to
Elasticsearch.
Frequent garbage collections on a node can make it look like a dead node
due to theTUPQ@UIF@XPSME phase. During the TUPQ@UIF@XPSME phase,
the node can't respond to the ping from other nodes; the node might be
mistaken for a dead node. In reality, it just couldn't respond due to long
garbage collection running on the node. It is important to watch for long
running garbage collections.
Master election is the other important functionality the discovery module takes care of. If
the master node is dead, the first node to discover that master node is dead initiates the
election process, and one of the nodes is elected as the master node. Nodes send
ping/heartbeat to the other nodes in the cluster on a regular interval. See below:

Production and Beyond
[ 341 ]
If no communication is received from the node within a certain time, the node is assumed
dead. When the cluster detects that the node is dead, it will start copying the shards to a
different node to maintain the replication factor as we discussed in the Failure Handling
section in $IBQUFS, Introduction to Elasticsearch.
Elasticsearch handles all of this internally and is completely transparent to the user.
X-Pack
X-Pack is an elastic offering that enables security, alerting, monitoring, and graph
functionality for Elasticsearch. X-Pack requires a paid license to use all the features. When
you install X-pack for the first time, you are given a 30-day trial. The basic or free version
will provide only monitoring. The other features are available with a paid subscription. In
this section, we will install X-Pack and discuss on how to update your license to basic if you
are not planning to buy a license. For more details on all the subscription offerings of
Elasticsearch, please visit the following link:
IUUQTXXXFMBTUJDDPTVCTDSJQUJPOT
X-Pack monitoring, which is part of basic or free license, provides UI with easy-to-read
graphs to monitor nodes and the indexes. It is available via Kibana. To use X-Pack, you
need both Elasticsearch and Kibana. For instructions on how to install Kibana, refer to the
Installing Kibana section in $IBQUFS, Setting up Elasticsearch and Kibana.
Windows
Using the command prompt, change the directory to where you installed Elasticsearch or
&4@)0.&. And install X-Pack using the following commands:
DE$=FMBTUJDTFBSDI
CJO=FMBTUJDTFBSDIQMVHJOJOTUBMMYQBDLCBUDI
You need to install X-Pack on all the nodes in the cluster and restart the cluster. To install X-
Pack on Kibana, follow the commands shown next:
DE$=LJCBOBXJOEPXTY
CJO=LJCBOBQMVHJOCBUJOTUBMMYQBDL
www.ebook3000.com

Production and Beyond
[ 342 ]
Mac OS X
Using the command prompt, change the directory to where you installed Elasticsearch or
&4@)0.&. And install X-Pack using the following commands:
DEFMBTUJDTFBSDI
CJOFMBTUJDTFBSDIQMVHJOJOTUBMMYQBDLCBUDI
You need to install X-Pack on all the nodes in the cluster and restart the cluster. To install X-
Pack on Kibana, follow the commands shown next:
DELJCBOBEBSXJOY@
CJOLJCBOBQMVHJOJOTUBMMYQBDL
Debian/RPM
To install X-Pack, follow the commands shown next. The configuration files, by default, are
at FUDFMBTUJDTFBSDI, as seen below:
DEVTSTIBSFFMBTUJDTFBSDI
CJOFMBTUJDTFBSDIQMVHJOJOTUBMMYQBDLCBUDI
You need to install X-Pack on all the nodes in the cluster and restart the cluster. To install X-
Pack on Kibana, follow the commands shown next:
DELJCBOBMJOVYY@
CJOLJCBOBQMVHJOJOTUBMMYQBDL
Authentication
When you install X-Pack, authentication is enabled by default. You require a username and
password to access Elasticsearch and Kibana. The defaults credentials are as follows:
VTFSOBNFFMBTUJD
QBTTXPSEDIBOHFNF
To disable authentication, set the following in Elasticsearch (FMBTUJDTFBSDIZNM) and
Kibana(LJCBOBZNM) configuration files:
YQBDLTFDVSJUZFOBCMFEGBMTF
All the nodes in the Elasticsearch cluster, Kibana have to be restarted after you install X-
Pack.

Production and Beyond
[ 343 ]
X-Pack basic license
When you install X-Pack for the first time, you will have 30 days to either purchase a license
or register for a basic license. To continue using the monitoring feature, in this section, we
will update the license to basic license. Downgrading to basic license will also remove the
authentication. First, you have to register for the basic license by going to the following:
IUUQTSFHJTUFSFMBTUJDDP
After the registration, you will receive an e-mail with the license file. You can then use the
@YQBDL API to import the basic license. By default, when you install X-Pack, authentication
is enabled. You need to pass the username and password as shown next to import the
license file:
DVSM9165-u elastic
	IUUQ@YQBDLMJDFOTFBDLOPXMFEHFUSVF	)$POUFOU5ZQF
BQQMJDBUJPOKTPOE!MJDFOTFKTPO
You can also use Postman to import the license. When using Postman, in the Authorization
tab, you should set the username and password. And in the body tab, select the type as
binary, and choose the license file elastic sent you via e-mail:
www.ebook3000.com

Production and Beyond
[ 344 ]
If everything goes well, you should see a response similar to as shown in the preceding
screenshot.
Monitoring
Elasticsearch exposes several APIs to monitor cluster health. In this section, we will discuss
the native APIs and also X-Pack monitoring, which is available via Kibana. Kibana provides
a nice UI and, more importantly, historical data to compare the metrics. You can open
Kibana by going to the following URL:
IUUQMPDBMIPTU
The default username and password for Kibana are FMBTUJD and DIBOHFNF. You should a
screen similar to this:
You can see that monitoring is available for both Elasticsearch and Kibana. If you have
Logstash installed, you can also monitor Logstash.

Production and Beyond
[ 345 ]
Monitoring Elasticsearch clusters
You can use @DMVTUFS"1* to retrieve the cluster stats:
(&5@DMVTUFSTUBUTIVNBOQSFUUZ
Refer to the Configuration Elasticsearch section to understand the important sections in the
stats response. You can also monitor cluster-level metric, such as index rate/search rate from
the Monitoring page as shown here:
At the top right corner, click on the clock icon to choose the time range for the metrics. The
current refresh interval for the charts is 10 seconds. You also change the rate at the which
the charts should be updated.
Monitoring indices
You can also monitor individual index stats by going to the following:
(&5DIBQUFS@TUBUTIVNBOQSFUUZ
www.ebook3000.com

Production and Beyond
[ 346 ]
The response contains all the stats of the index. You can also query for particular sections as
shown next:
(&5DIBQUFS@TUBUTJOEFYJOH
You can also look at the index level stats via the Monitoring page as shown here:
There is also an Advanced tab, which contains more advanced metrics, such Lucene
memory, disk rate, and so on.
Monitoring nodes
We discussed important node-level metrics to watch for in the Configuring Elasticsearch
section. Using Kibana, you can get a complete picture of the health of the node. The basic
tab has all the important metrics, such as jvm heap size, CPU, load, and so on:

Production and Beyond
[ 347 ]
:PVDBOBMTPXBUDIGPSBEWBODFENFUSJDTTVDIBTHBSCBHFDPMMFDUJPOBOEUIFOVNCFSPGUISFBETJOUIF
AdvancedUBC
www.ebook3000.com

Production and Beyond
[ 348 ]
If your current hardware is running hot, most problems can be solved by setting the right
mapping types and rethinking the search queries. If all else fails, then you can think of
upgrading your hardware.
Thread pools
Elasticsearch has a thread pool for all the major modules. Each thread pool has a queue
associated with the pool. For example, if the index thread pool receives more requests than
it can process, the requests are queued up. If the queue is full, the request is rejected.
Watching for the number of requests in the queue is important. If the requests are
frequently queued, the response times are degraded. The pool size is calculated based on
the available processors. The important thread pools are as follows:
Thread pool Description
index
This is used for index and delete operations. It has a queue size of 200.
search
This is used for search operations. It has a queue size of 1,000.
bulk
This is used for bulk operations. It has a queue size of 50.
refresh
This is used for refresh operations.
Note that if the queue is full, the request is rejected with an )551
error.
You can use cat API to look at the active thread in each pool:
(&5@DBUUISFBE@QPPMWIOBNFBDUJWFRVFVFSFKFDUFEDPNQMFUFE
The response to the preceding query is as follows:
 name                active queue rejected completed
CVML
GFUDI@TIBSE@TUBSUFE
GFUDI@TIBSE@TUPSF
GMVTI
GPSDF@NFSHF
HFOFSJD
HFU
JOEFY
MJTUFOFS
NBOBHFNFOU

Production and Beyond
[ 349 ]
SFGSFTI
TFBSDI
TOBQTIPU
XBSNFS
XBUDIFS
You should watch for the rejected request and queue. You can also use X-Pack monitoring
to look at the thread pools in the node stats in the Advanced tab:
The size of each thread pool is based on the number of processors. Elasticsearch doesn't
recommend changing the thread pool size. But the number of threads and the queue size for
each pool can be changed in the Elasticsearch configuration file.
Elasticsearch server logs
The server logs should be the go-to place when you are trying to figure out why a node is
not starting or why shards are not being allocated. The logs provide insight into what's
wrong. Elasticsearch uses MPHK to handle the logging. The logs are written to the
following:
&4@)0.&MPHTDMVTUFS@OBNFMPH
By default, the logs are rotated every day. If you look at the logs directory, you should find
something like this:
www.ebook3000.com

Production and Beyond
[ 350 ]
In the preceding example, the cluster name is FTEFW. The current logs are written to FT
EFWMPH. If you want to change the default log level, you can do so using the cluster setting
API as shown next. In the following command, we are changing the logging level for the
root logger. Elasticsearch also supports changing the log level for a single module-like
discovery:
165@DMVTUFSTFUUJOHT
\
USBOTJFOU\
MPHHFS@SPPU%&#6(
^
^
You should see a response similar to the following:
\
BDLOPXMFEHFEUSVF
QFSTJTUFOU\^
USBOTJFOU\
MPHHFS\
@SPPU%&#6(
^
^

Production and Beyond
[ 351 ]
^
Slow logs
Slow logs are an extremely useful feature, especially when you have a working cluster with
a lot of indices. Slow logs are index-level logs, which are logged when the request takes
more time than a threshold value. For example, if you want to log all the search queries that
are taking more than T at the debug-log level. You can enable slow logs for search as
shown here:
165DIBQUFS@TFUUJOHT
\
JOEFY\
TFBSDITMPXMPHMFWFMJOGP
 "search.slowlog.threshold.query.debug": "5s"
^
^
Similarly, you can enable logs while indexing as shown here:
165DIBQUFS@TFUUJOHT
\
JOEFY\
JOEFYJOHTMPXMPHMFWFMJOGP
JOEFYJOHTMPXMPHTPVSDF
"indexing.slowlog.threshold.index.debug": "5s"
^
^
Any indexing requests that take more than T are logged as debug level. And when logging
the indexing request, the first  characters of the source are logged. Depending on your
application, you can set the preceding values. The slow logs are available in the logs
directory. Assuming the cluster name as FTEFW, indexing slow logs are logged as FT
EFW@JOEFY@JOEFYJOH@TMPXMPHMPH, and the search logs are logged as FT
EFW@JOEFY@TFBSDI@TMPXMPHMPH.
www.ebook3000.com

Production and Beyond
[ 352 ]
Summary
In this chapter, we discussed the most important settings to be configured before going to
production. You learned about various cluster management APIs to monitor the status of
the cluster. We discussed how to use X-Pack monitoring to watch for the important metrics,
such as CPU, memory, and so on.
In the next chapter, we will talk about X-Pack premium features, such as graph which can
be used to discover relations in your data and alerting which allows you to set up alerts and
notifications.

10
Exploring Elastic Stack (Elastic
Cloud, Security, Graph, and
Alerting)
Elastic, the company behind Elasticsearch, also offers managed cloud hosting and X-Pack
which is available as a part of their premium subscriptions. In the previous chapters, we
used Kibana to create visualizations of your data. X-Pack adds more functionality, such as
graph, monitoring to Kibana. The graph will let you discover relations in your data. We
will discuss how to use graph to make recommendations based on the previous purchases
in an e-commerce store. Along with graph and monitoring, X-Pack also provides security,
extremely flexible alerts, and a notification system. We will briefly talk about the various
products in Elastic Stack and Elastic Cloud in this chapter. By the end of this chapter, we
will discuss the following:
Elasticsearch Cloud (Managed Elasticsearch)
Security
Graph
Alerting
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 354 ]
Elastic Cloud
Elastic Cloud is the scalable cloud offering by the company that built Elasticsearch.
Elasticsearch and Kibana are offered as a service. Your cluster is hosted on Amazon AWS
and is completely managed by Elastic Cloud. You can create a new cluster across multiple
data centers with the click of a button. Once you create a cluster, depending on the
application needs, you can scale up or down very easily. The monitoring of clusters is
available via Kibana monitoring. X-Pack is automatically included for every cluster running
on Elastic Cloud.
Elastic offers a 14-day trial to new customers to try the service. You can
sign up for a trial by going to IUUQTDMPVEFMBTUJDDP
The advantages of Elastic Cloud are as follows:
.POJUPSFEBOENBOBHFECZ&MBTUJD - If a node or the cluster goes down, its
1.
taken care by the Elastic team. There is no need of a dedicated operations team to
monitor and manage you cluster.
)JHIMZBWBJMBCMF - Your cluster is span across multiple data centers. Even if
2.
couple of nodes or the entire data center goes down, your data is still available.
"VUPNBUJDEBUBCBDLVQT - Data is automatically backed up every 30 mins.
3.
&BTZUPTDBMF - Hosted on AWS cloud, you can scale you cluster up and down.
4.
For example, you are launching a new product and except more traffic than
usual, you can add more resources to your cluster and scale it down later.
Depending on the number of nodes you need, it can get quite expensive.
You can look at the current pricing at IUUQTXXXFMBTUJDDPDMPVEBT
BTFSWJDFQSJDJOH
We will discuss the above features in detail in the sections below.
Once you create a Elastic Cloud account, you can create a new cluster by going to the Create
Cluster page shown here:

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 355 ]
Following are the steps to create a cluster:
Choose the memory and storage you would need. You can always scale your
1.
cluster up or down later.
Choose the region near you (Amazon AWS regions).
2.
For high availability, choose 1 or 2 data centers depending on your application
3.
needs. We will discuss this in detail in the next section.
Once you click on Create, your new cluster will be provisioned.
4.
Once the new cluster is provisioned, the Elasticsearch and Kibana endpoints are available
on the Clusters page. By default, security is enabled on the cluster. To access Elasticsearch
and Kibana, you will need a username and a password. The default username is FMBTUJD,
and you can reset your password by going to the Security tab on the left and clicking on the
3FTFU button.
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 356 ]
High availability
Elastic Cloud supports high availability by spreading the cluster across two or three data
centers or AWS availability zones. AWS availability zones are independent zones within an
AWS region. Availability zones are connected using low-latency networks. In the case of an
availability zone going down, the nodes in the different availability zone will automatically
take over.
What does this mean in terms of Elasticsearch? Elasticsearch supports high availability by
writing your data to both primary and replica shards. When using Elastic Cloud, the
primary and replica shards of an index are stored in the nodes that belong to different data
centers/availability zones. If a node goes down or an entire availability zone goes down, the
replicas in a different availability zone can still support the read/write operations, so that
your data is always available.
To look at the shard distribution, let us create a cluster with high availability option of two
data centers. Once we have the cluster up and running, let us create an index with  shards
and  replica, the index will have a total of  shards. The distribution of shards can be
viewed in Kibana Monitoring page by the following the steps here:
Go to the Monitoring page in Kibana.
1.
Select indices to get a list of available indices.
2.
Choose the index you would like and scroll down to Shard Legend at the end of
3.
the page.
The shard legend of an index is shown here:

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 357 ]
The cluster contains two nodes and they belong to different data centers. The two instances
shown in preceding screenshot are the nodes of the cluster. The first instance contains the
primary of shard  and the replica of shard  and the second instance contains the primary
of shard  and the replica of shard . If the first instance goes down, replica of shard  in the
second instance is promoted to primary and no data is lost. By distributing the shards
across nodes that belong to different availability zones, your data is made highly available.
Data reliability
Elastic Cloud supports creating your cluster across three data centers or availability zones.
In the case of a node going down, the shards are automatically replicated to different nodes.
In the case of all the availability zones going down (which is very rare), you can use the
snapshot and restore feature. Elastic Cloud automatically backs up your data every  mins
to Amazon S3 buckets. You can go to the Snapshots tab on the left to restore a snapshot.
Security
Security is also part of X-Pack gold and premium subscriptions. Security for Elasticsearch is
two-fold:
User authentication to access the cluster using username and password
Securing the communication between the nodes and the client using SSL
Authentication and roles
X-Pack provides role-based authentication. When you install X-Pack, authentication is
enabled automatically. To get started, two default users are created:
Username Password Role
elastic
changeme This is a superuser
kibana
changeme To access Kibana, the user has the required permissions to talk to
Elasticsearch
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 358 ]
You can add more users or roles in the Kibana Management Console by going to the Users
page. Along with creating new users, you can also add or change roles by going to the Roles
page. For example, let's create a new role DIBQUFS@TVQFSVTFS. The users in this role
have super user permissions to the DIBQUFS index only. In the following screenshot, you
can see that we restricted the index privileges to only the DIBQUFS index:
When creating a new role, you can set privileges for the user, such as read, write, and so on.
You can also grant access to only certain fields. For example, you can restrict access to a
sensitive field, such as credit card information. Once the role is defined, you can add a new
user by going back to the Users page as shown here:

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 359 ]
When authentication is enabled, requests without username and password are rejected.
Anonymous access can be enabled by setting the roles for anonymous access in the
Elasticsearch configuration file. You can also completely disable authentication by adding
the following to the FMBTUJDTFBSDIZNM and LJCBOBZNM configuration files:
YQBDLTFDVSJUZFOBCMFEGBMTF
Securing communications using SSL
If you are storing sensitive information on your cluster, SSL is a must. When SSL is enabled,
the internal communication between the nodes and the communication between the client
and the cluster are encrypted. SSL is also used to authenticate the nodes before joining the
cluster. To enable SSL, you have to generate a private key certificate. For more information,
please visit the following URL:
IUUQTXXXFMBTUJDDPHVJEFFOYQBDLTTMUMTIUNM
When talking to a cluster with SSL enabled, the transport client must be configured to use
SSL to encrypt/decrypt the communications.
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 360 ]
Graph
In this section, we will discuss Graph, which is part of the X-Pack Gold and Platinum
subscription. Graph lets you discover and analyze relationships in your data. It works on
your existing indexes and doesn't require any special configuration. The Graph has two
components:
The functionality required for Elasticsearch to compute the Graph.
The UI in Kibana to visualize the graphical representation.
To better explain the functionality of Graph, let's build a recommendation system for an
online store. We want to know the relations between items frequently bought together and
use that information to make suggestions to the users. This information can be very
valuable. For example, in a physical store, items frequently bought together can be placed
adjacent to each other. We can also use this information to give the user a coupon or e-mail
the user about various discounts on those products. We will create an index called
DIBQUFS and index the user's purchases as documents. First, let's create the index and set
the mappings:
%FMFUFFYJTUJOHJOEFY
%&-&5&DIBQUFS
.BQQJOH
165DIBQUFS
\
TFUUJOHT\
OVNCFS@PG@TIBSET
OVNCFS@PG@SFQMJDBT
^
NBQQJOHT\
"order": {
QSPQFSUJFT\
MJOF@JUFNT\
UZQFLFZXPSE
^
VTFS@JE\
UZQFLFZXPSE
^
PSEFS@DSFBUJPO@EBUF\
UZQFEBUF
^
^
^
^
^

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 361 ]
Next, let's index some documents:
165DIBQUFSPSEFS
\
VTFS@JE
MJOF@JUFNT<
JDFDSFBN
DIPDPMBUF
CFFS
DIJQT
TPEB
>
^
165DIBQUFSPSEFS
\
VTFS@JE
MJOF@JUFNT<
JDFDSFBN
CBOBOB
BWPDBEP
TPEB
QFBOVUCVUUFS
>
^
165DIBQUFSPSEFS
\
VTFS@JE
MJOF@JUFNT<
JDFDSFBN
DIPDPMBUF
TPEB
DIJQT
>
^
165DIBQUFSPSEFS
\
VTFS@JE
MJOF@JUFNT<
JDFDSFBN
CBOBOB
CSFBE
DIJQT
>
^
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 362 ]
For example, we want to show recommendations for users who bought TPEB and DIJQT.
We will make the recommendations based on what other users bought along with soda and
chips. To do this, we can use a simple terms aggregation and get the count of each line item
(items in an order). But the recommendations based on terms aggregation might not be apt
as there is no significance to soda and chips; the aggregation simply calculates the count for
each unique line item. Instead of terms aggregation, we can use the TJHOJGJDBOU@UFSNT
aggregation to calculate the significance of each term to provide a better recommendation.
The following example will better explain why we need to use the TJHOJGJDBOU@UFSN
aggregation. Let's run the query and look at the results:
1045DIBQUFS@TFBSDI
\
TJ[F
RVFSZ\
CPPM\
NVTU<
\
UFSN\
  "line_items": "soda"
^
^
\
UFSN\
 "line_items": "chips"
^
^
>
^
^
BHHT\
SFDPNNFOEBUJPOT\
"significant_terms": {
GJFMEMJOF@JUFNT
NJO@EPD@DPVOU
^
^
^
^
The documents that match the preceding query have the following line items:
JDFDSFBNDIPDPMBUFCFFSDIJQTTPEB
JDFDSFBNDIPDPMBUFTPEBDIJQT

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 363 ]
You can see that the common terms between the two documents are as follows:
JDFDSFBNDIPDPMBUFCFFS
If you look at the other purchases, all the purchases have ice cream. If you recommend ice
cream, you are recommending what anybody would buy in general. But we want to find
out what users who bought chips and soda like. Significant terms aggregation calculates
the significance of each term globally and in the scope of the current query. Terms such as
ice cream are significant globally and ranked lower. Let's look at the results of the preceding
query:
\

BHHSFHBUJPOT\
SFDPNNFOEBUJPOT\
EPD@DPVOU
CVDLFUT<
\
 "key": "chocolate",
EPD@DPVOU
"score": 1,
CH@DPVOU
^
\
"key": "beer",
EPD@DPVOU
 "score": 0.5,
CH@DPVOU
^
\
LFZTPEB
EPD@DPVOU
TDPSF
CH@DPVOU
^
\
LFZDIJQT
EPD@DPVOU
TDPSF
CH@DPVOU
^
>
^
^
^
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 364 ]
You can see from the results that DIPDPMBUF and CFFS are the top recommendations and
ice cream is not even recommended. For this example data set, users who bought soda and
chips also like chocolate and beer. When we use the Kibana Graph UI, you will see the
following graph:
You can see from the preceding graph that the common vertices (also known as nodes)
between chips and soda are DIPDPMBUF and CFFS. You can also see vertices and
connections (also known as edges) between other products. You can change the input, and a
new graphical representation appears instantly. The queries are executed using the query
DSL and are very fast. In the next section, we will learn how to use Kibana Graph UI. In a
bigger data set with lots of correlations, Graph is very useful in finding hidden relations
between the data points.

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 365 ]
Graph UI
In this section, we will learn how to use Graph UI to get better insights into user data. To
use the DIBQUFS index in Kibana, we have to first add the index pattern. If you have
Kibana installed locally or on a remote server, you can open Kibana by going to the
following URL:
IUUQMPDBMIPTU
If you are using Elastic Cloud, the Kibana endpoint can be found on the
cluster overview page.
Once in Kibana, select the Management tab on the left navigation panel, and select Index
patterns and you will see a screen similar to as shown here:
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 366 ]
Enter the index name as DIBQUFS and unselect the *OEFYDPOUBJOTUJNFCBTFE
FWFOUT option as we don't have any date fields in the data set. Click on the $SFBUF button
to use the DIBQUFS index in Kibana. Once the index is added to Kibana, let's open Graph
by going to the Graph tab on the left navigation bar. Once in the Graph Workspace page, you
will a screen similar to shown here:
Select the index pattern as DIBQUFS and click on the plus (+) button to add MJOF@JUFNT
field to the graph. Input the search text as DIJQT"/%TPEB and click on the search button
(Magnifying glass icon) to see the graph. Since the dataset has only  records, we have to
change some settings to execute the graph query. To change the settings, select the Settings
tab at the top right corner and you will see a screen similar to shown here:

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 367 ]
Change the $FSUBJOUZ to  as the data set is small. You can minimize settings by clicking
on the Settings tab. Click the search button represented by magnifying glass icon and you
will see the graph as shown here:
You can select a node and use the advanced options window to change the styling, group
nodes together, remove a node, and so on. In the Settings tab, you can also view the last
request and response.
Alerting
Just like Graph, alerting is a part of X-Pack Gold and Platinum subscription. Alerting was
formerly known as . You can define watchers on cluster events and existing indexes.
Watcher can be configured to be executed on schedule. Elasticsearch alerting is very
flexible, and the input can be the response to an HTTP request or a query on existing
Elasticsearch index. A watch is triggered if the input matches a predefined condition. For
example, the input can be the cluster status and condition is "if the cluster status is red".
When a condition is triggered, you can define what action has to be taken. An action can be
sending an e-mail, indexing a document, sending a slack message, and so on.
For an e-commerce store, let's define a watcher to alert if less than five orders have been
placed within the last 30 minutes. The action we will take when the condition is met is
indexing a document. We can also send an e-mail, but in this example, we will index a
document instead for further analysis. We want to run the watcher every 30 mins. We can
define a schedule as follows:
165@YQBDLXBUDIFSXBUDIPSEFS@DPVOU
\
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 368 ]
USJHHFS\
TDIFEVMF\
"interval": "30m"
^
^
Next, let's define the input for the watcher. The input for this watcher is an Elasticsearch
query that searches for the number of orders placed in the last 30 minutes:
JOQVU\
TFBSDI\
SFRVFTU\
JOEJDFT<
DIBQUFS
>
UZQFT<
PSEFS
>
CPEZ\
RVFSZ\
SBOHF\
"order_creation_date": {
                 "gte": "now-30m",
UJNF@[POF
^
^
^
^
^
^
^
The response of the input is validated against a condition. Let's define the condition, which,
in this case, is the number of orders less than five:
DPOEJUJPO\
DPNQBSF\
DUYQBZMPBEIJUTUPUBM\
 "lte": "5"
^
^
^

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 369 ]
If the input matches the condition, an action can be defined. The action we want to take
when the number of orders placed is less than five is to index a document into an index
named PSEFS@BMFSUT:
BDUJPOT\
JOEFY@QBZMPBE\
JOEFY\
"index": "order_alerts",
EPD@UZQFMUF@
FYFDVUJPO@UJNF@GJFMEBMFSU@UJNFTUBNQ
^
^
^
^
The complete query is as follows:
165@YQBDLXBUDIFSXBUDIPSEFS@DPVOU
\
 "trigger": {
TDIFEVMF\
JOUFSWBMN
^
^
"input": {
TFBSDI\
SFRVFTU\
JOEJDFT<
DIBQUFS
>
UZQFT<
PSEFS
>
CPEZ\
RVFSZ\
SBOHF\
PSEFS@DSFBUJPO@EBUF\
HUFOPXN
UJNF@[POF
^
^
^
^
^
^
^
 "condition": {
DPNQBSF\
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 370 ]
DUYQBZMPBEIJUTUPUBM\
MUF
^
^
^
"actions": {
JOEFY@QBZMPBE\
JOEFY\
JOEFYPSEFS@BMFSUT
EPD@UZQFMUF@
FYFDVUJPO@UJNF@GJFMEBMFSU@UJNFTUBNQ
^
^
^
^
To test the preceding watcher, let's index an order:
165DIBQUFSPSEFS
\
VTFS@JE
MJOF@JUFNT<
JDFDSFBN
CBOBOB
CSFBE
DIJQT
>
PSEFS@DSFBUJPO@EBUF5
^
The watcher is scheduled to run every 30 minutes. We can manually execute the watcher
using the @FYFDVUF endpoint as shown here:
1045@YQBDLXBUDIFSXBUDIPSEFS@DPVOU_execute
The results of the execute are as follows:
\
@JEPSEFS@DPVOU@GDGD
FFFBGBBF5;
XBUDI@SFDPSE\
"watch_id": "order_count",
"state": "executed",
USJHHFS@FWFOU\
"type": "manual",
USJHHFSFE@UJNF5;
NBOVBM\
TDIFEVMF\

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 371 ]
TDIFEVMFE@UJNF5;
^
^
^
JOQVU\

^
SFTVMU\
FYFDVUJPO@UJNF5;
FYFDVUJPO@EVSBUJPO
JOQVU\
UZQFTFBSDI
"status": "success",
QBZMPBE\

^
 "actions": [
\
JEJOEFY@QBZMPBE
UZQFJOEFY
"status": "success",
JOEFY\
SFTQPOTF\
DSFBUFEUSVF
SFTVMUDSFBUFE
JE"7V5P*%Z5IK1HYO[
WFSTJPO
UZQFMUF@
JOEFYPSEFS@BMFSUT
^
^
^
>
^
NFTTBHFT<>
^
^
You can see from the response that the status of the action is a success. Let's check the
PSEFS@BMFSUT index to make sure the alert is recorded:
1045PSEFS@BMFSUT@TFBSDI
www.ebook3000.com

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 372 ]
The response to the preceding query is as follows:
\

IJUT\
UPUBM
NBY@TDPSF
IJUT<
\
@JOEFYPSEFS@BMFSUT
@UZQFMUF@
@JE"7V5FL.XZ5IK1HYOG
@TDPSF
@TPVSDF\
@TIBSET\
UPUBM
GBJMFE
TVDDFTTGVM
^
IJUT\
IJUT<>
UPUBM
NBY@TDPSFOVMM
^
UPPL
UJNFE@PVUGBMTF
"alert_timestamp": "2017-04-22T03:07:01.116Z"
^
^
>
^
^
Since this is a test watcher, you can disable the watcher as shown next:
1045@YQBDLXBUDIFSXBUDIPSEFS@DPVOU_deactivate
The preceding example is a very simple watcher. Actions such as slack, email, and hip chat
are also supported.

Exploring Elastic Stack (Elastic Cloud, Security, Graph, and Alerting)
[ 373 ]
Summary
In this chapter, we discussed Elastic Cloud, which is the scalable cloud offering for
Elasticsearch and Kibana. We also discussed various features of X-Pack. With monitoring
and alerting, you can make sure your cluster is up and running and that your data is always
available.
Although Elasticsearch started as a search engine, it is evolving as an analytics engine. In
this chapter, we discussed Graph, which can be used to discover relations in your data. We
used the example of an e-commerce store to make recommendations, but this functionality
has great potential. For example, for fraud detection or to find out the similarity between
customers who like or don't like your product have in common. The graph provides
actionable insights into your data.
The alert and notification system that we discussed in this chapter is very flexible. We used
alerting to get notified, if the number of orders placed in last 30 minutes are less than five.
Think of the possibilities of such a system; you can set up alerts on different trends and get
notified. You can do all this without the need of a new system.
Elasticsearch is also working towards including machine learning as part of X-Pack, which
supports unsupervised machine learning. The current beta version supports machines
learning on time-series data to find data anomalies.
Elasticsearch is a highly scalable analytics and search engine, which is very flexible and
high performing. The things Elasticsearch can do are continuously evolving, but the
fundamentals discussed in this book will not change.
www.ebook3000.com

Index
A
aggregations
   based on dates  
   based on filters  
   based on geolocation  
   based on numeric values  , 
   basics  
   bucket aggregations  
   metric aggregation  
   multilevel aggregations  
   nesting  
   on child documents  
   on nested documents  
   query structure  , 
   sample data  
   term aggregations  
   types  
   visualizing, Kibana used  
alerting  
Analyze API  
analyzed fields
   versus non-analyzed fields  
analyzers
   reference  
Apache Lucene
   about  , , 
   reference  
array  
autocomplete feature
   implementing  
availability  
B
basic concepts, Elasticsearch
   cluster  
   document  , 
   index  
   nodes  
   shard  
   type  
basic query, Elasticsearch  , 
best matching fields  
binary data type  
BM25 similarity algorithm
   reference  
bool query  
boolean data type  
bucket aggregation  , 
Bulk API
   about  
   using  , 
bulk operations  
C
cache
   node query cache  
   shard request cache  
caching  , 
Cat APIs  
Cluster APIs  
cluster health
   monitoring  
cluster name
   modifying  
cluster
   about  
   green state  
   red status  
   yellow status  
collate  
completion suggester  
complex data types
   about  

[ 375 ]
   array  
   nested data type  
   object  
concurrency  
configuration file  
coordinating node  , 
core data types
   about  
   binary  
   boolean  
   date  
   keyword  
   numeric  
   text  
cross-matching fields  
cURL
   using  
D
data node  
data types
   about  
   complex data types  
   core data types  , 
   geo data types  
   specialized data types  
data
   entire document, updating  
   indexing  , , 
   organizing  
   updating  
date data type  
dates
   handling  
Debian package
   about  
   used, for installing Elasticsearch  
decay functions
   used, for boosting relevance  
Delete by Query API
   using  
did you mean feature
   implementing  
directory structure
   about  
   DEB/RPM  
   zip/tar.gz  
distributed search  
doc values  
Document APIs  
document relations
   handling, nested used  
   handling, parent-child used  
document types
   relations, handling between  
document
   about  , 
   creating  , , 
   existing document, deleting  , 
   existing document, retrieving  
   existing document, updating  
   fields, searching in  
   indexing  , , 
   original document, storing  
   partial document, updating  
   updating  
dynamic mapping
   about  
   working  
E
Elastic Cloud
   about  
   data reliability  
   high availability  
Elasticsearch clusters
   monitoring  
Elasticsearch server logs
   about  
   slow logs  
Elasticsearch, communicating ways
   native client  
   REST API  
Elasticsearch
   about  
   basic concepts  
   basic query  , 
   configuring  
   execution, running  
   installing  
www.ebook3000.com

[ 376 ]
   installing, on Mac OS X  
   installing, on Windows  
   interacting with  
   limitations  
   querying  
   querying, from Java application  
   starting  , , 
   stopping  , 
   strengths  
   using, in application  
exact match
   versus full-text search  
exists query  
F
failure handling  , , 
field data  
fields mapping  
fields
   searching, in document  
   selecting, in response  
file descriptors
   configuring  
filters
   versus queries  
FST (Finite State Transducer)
   about  
   reference  
full-text search queries  
full-text search
   versus exact match  
fuzzy query  
G
garbage collection (GC)  
Gelocation
   used, for ranking search results  
Geo Bounding Box query  
geo bounds aggregation  
Geo data type
   about  
   geo-point  
   geo-shape  
geo distance aggregation  
Geo Distance query  
geo filtering  
geo-point data type  
geo-shape
   reference  
geolocation aggregations
   about  
   bounds  
   distance  
geolocation queries
   using  
global-ordinals  
Graph  
Graph UI  
Grok  
H
has_child query  
has_parent query  
highlighting  
histogram aggregation  
I
index alias  , , 
index templates  , 
index
   about  , 
   mapping  , , 
indexing errors
   about  
   node/shards errors  
   specialization/mapping errors  
   thread pool injection error  
Indices APIs  
indicies
   monitoring  
information retrieval  
ingest node
   about  
   using  , , , , 
inner hits feature  
inverse document frequency (idf)  
inverted document frequency (idf)  
inverted index  , , 
IP data type  

[ 377 ]
J
Java application
   Elasticsearch, querying from  
Java client
   using, in indexing  
Java Runtime Environment (JRE)  
Java-based application
   about  
   node client  
   REST client  
   third-party clients  
   transport client  
Java
   installing  
Jest
   reference  
JSON  
JSON (JavaScript Object Notation)  
K
keyword  
Kibana Console  , 
Kibana
   downloading, for Mac OS X  
   downloading, for Windows  
   installing  
   starting  , 
   stopping  , 
   used, for visualizing aggregations  
   using  , 
L
Levenshtein automata  
Levenshtein distance  
Logstash  
M
mapping error  
mapping
   about  
   conflicts  
   defining  
   dynamic mapping  
   existing mapping, obtaining  
   index, creating with  
   new type/field, adding  
   same field, mapping with different mappings  
   setting, for nested documents  
master eligible node  
match phrase prefix query  
match phrase query  , 
match query
   versus term query  , , 
matrix aggregation  
memory configuration  
metafields  
metric aggregation  , 
missing queries  
monitoring  
most matching fields  
Multi Get API
   about  
   using  , 
multi search  , 
multinode cluster
   about  
   logs, inspecting  
multiple data centers  
multiple fields
   value, searching across  
multiple queries
   using  
Mustache template
   reference  
N
nested datatype  
nested documents
   about  
   inner hits  
   mappings, setting for  
nested
   used, for handling document relations  
network configuration  
node client, Java-based application  
node errors  
node failures  
node name
   changing  
www.ebook3000.com

[ 378 ]
node query cache  
nodes
   about  
   coordinating node  
   data node  
   discovery section  
   ingest node  
   master eligible node  
   monitoring  
   tribe node  
non-analyzed fields
   versus analyzed fields  , 
NOOP  
null values
   handling  
numeric data type
   about  
   byte  
   double  
   float  
   half_float  
   integer  
   long  
   scaled_float  
   short  
O
object  
Oracle Java Development Kit (JDK)  
P
pagination  
Painless  , 
parameters, text mapping
   analyzer  
   boost  
   fielddata  
   fields  
   include-in_all  
   index  
   index_options  
   norms  
   position_increment_gap  
   search_analyzer  
   search_quote_analyzer  
   similarity  
   store  
   term_vector  
parent-child document relations  
parent-child
   inner hits feature  
   used, for handling document relations  
   working  
partial updates  
percolate query
   using, for reverse search  
phrase search  
phrase suggester  , 
pipeline aggregation  
post filter
   about  , 
   using  
Postman
   reference  
   using  
prefix query  
primary shard  
Q
queries
   about  
   full-text search queries  
   structured queries  
   versus filters  
Query DSL (Domain specific Language)  
R
rain  
range query  , 
ranked based  
rebalancing  
regular expression query  
regular match query  
Reindex API
   about  
   documents, combining from one or more indicies 

   mappings/settings, changing  
   missing documents, copying  
   subset of documents, copying into new index 

[ 379 ]

   subset of fields, copying into index  
   top N documents, copying  
relations
   about  
   handling, between document types  
relevance score
   debugging  
relevance
   about  , 
   boosting, based on single field  
   boosting, decay functions used  
replica shard  
rescoring  
response
   fields, selecting in  
REST API
   used, for interacting with Elasticsearch  
REST client, Java-based application  
reverse nested aggregation  
reverse search
   with percolate query  
routing  , 
RPM package
   used, for installing Elasticsearch  
S
sample configuration files  
sample data  , 
scalability  
score
   boosting, based on queries  
script query  
scripted updates  
scripting  
Search API  
Search APIs  
search engine  
search query
   debugging  
search templates  , 
search
   working  
Security
   about  
   authentication  
   communications securing, SSL used  
   roles  
segments
   merging  
Sense plugin  
Sense plugin, for Chrome
   download link  
serialization error  
shard  
shard request cache  
shards errors  
shards, Elasticsearch
   number of replicas, changing  
   primary  , 
   primary preference  
   replica  , 
   replicas, for query throughput  
Shrink API  
simple search query  
six shards
   with one replica  
   with zero replicas  
slow logs  
sorting
   about  
   based, on existing fields  , 
spatial filtering  
special mapping type  
special nested type query  
specialized data type
   about  
   completion  
   IP  
   percolator  
spelling mistakes
   correcting  
stemming  
stored procedures  
structured queries  
subscription offering, Elasticsearch
   reference  
suggest API
   about  
   completion suggester  
www.ebook3000.com

   phrase suggester  
suggestions
   making, based on user input  
synonyms  
T
term aggregations
   error  
   minimum document count  
   missing values  
   order  
   size  
term frequency (tf)  
term query
   versus match query  , , 
term suggester  , 
text data type  
text mapping
   parameters  
thread pool rejection error  
thread pools
   about  
   bulk  
   index  
   refresh  
   search  
three shards
   with zero replicas  
time series data  
time-based indices
   managing  , 
translog
   about  
   async, versus sync  
   CRUD, from translog  
transport client, Java-based application
   advantages  
   client, initializing  
   dependencies  
   disadvantages  
   sniffing  
tribe node  
type  
typos
   correcting  
U
unstructured data  
Update by Query API
   about  
   using  , 
updates
   partial updates  
   scripted updates  
upsert
   setting  
user defined scripts  
UTC (Universal Time Coordinated) format  
V
value
   searching, across multiple fields  
W
wildcard query  
X
X-Pack
   about  
   authentication, enabling  
   basic license  
   installing, for Debian/RPM  
   installing, on Mac OS X  
   installing, on Windows  
Z
Zen discovery  


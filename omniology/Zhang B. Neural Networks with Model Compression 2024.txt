Computational Intelligence Methods and Applications
Baochang Zhang
Tiancheng Wang
Sheng Xu
David Doermann
Neural 
Networks 
with Model 
Compression

Computational Intelligence Methods and 
Applications 
Founding Editors 
Sanghamitra Bandyopadhyay 
Ujjwal Maulik 
Patrick Siarry 
Series Editor 
Patrick Siarry, LiSSi, E.A. 3956, Université Paris-Est Créteil, Vitry-sur-Seine, 
France

The monographs and textbooks in this series explain methods developed in compu-
tational intelligence (including evolutionary computing, neural networks, and fuzzy 
systems), soft computing, statistics, and artiﬁcial intelligence, and their applications 
in domains such as heuristics and optimization; bioinformatics, computational 
biology, and biomedical engineering; image and signal processing, VLSI, and 
embedded system design; network design; process engineering; social networking; 
and data mining.

Baochang Zhang • Tiancheng Wang • Sheng Xu •
David Doermann 
Neural Networks with Model 
Compression

Baochang Zhang 
Institute of Artiﬁcial Intelligence 
Beihang University 
Beijing, China 
Sheng Xu 
School of Automation Science and 
Electrical Engineering 
Beihang University 
Beijing, China 
Tiancheng Wang 
Institute of Artiﬁcial Intelligence 
Beihang University 
Beijing, China 
David Doermann 
Department of Computer Science and 
Engineering 
University at Buffalo, State University 
Buffalo, NY, USA 
ISSN 2510-1765
ISSN 2510-1773 (electronic) 
Computational Intelligence Methods and Applications 
ISBN 978-981-99-5067-6
ISBN 978-981-99-5068-3 (eBook) 
https://doi.org/10.1007/978-981-99-5068-3 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore 
Pte Ltd. 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. 
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, 
Singapore 
Paper in this product is recyclable.

Preface 
With the swift development of information technology, cloud computing with 
centralized data processing cannot meet the needs of applications that require 
processing massive amounts of data, and they can only be effectively used when 
privacy requires the data to remain at the front-end device. Thus, edge computing 
has become necessary to handle the data from embedded devices. Intelligent 
edge devices beneﬁt many requirements within real-time unmanned aerial systems, 
industrial systems, and privacy-preserving applications. 
In recent years, deep learning has been applied to different applications, dramat-
ically improving many artiﬁcial intelligence (AI) tasks. However, the incomparable 
accuracy of deep learning models is achieved by paying the cost of hungry memory 
consumption and high computational complexity, which signiﬁcantly impedes their 
deployment in edge devices with low memory resources. For example, the VGG-
16 network can achieve 92.7% top-5 test accuracy on image classiﬁcation tasks 
with the ImageNet dataset. Still, the entire network contains about 140 million 32-
bit ﬂoating-point parameters, requiring more than 500 megabytes of storage space 
and performing 1.6 times 10 Superscript 101.6 × 1010 ﬂoating-point operations. Yet, FPGA-based embedded 
devices typically have only a few thousand compute units, which cannot handle the 
millions of ﬂoating-point operations in standard deep neural network models. On the 
other hand, complex neural networks are often accompanied by slower computing 
speed and longer inference time, which are not allowed in applications with strict 
latency requirements, such as vehicle detection and tracking. Therefore, a natural 
thought is to perform model compression and acceleration in neural networks 
without signiﬁcantly decreasing the model performance. 
This book introduces the signiﬁcant advancements of neural networks with 
model compression. While quantized operations can enhance the efﬁciency of 
neural networks, they typically result in a decrease in performance. In the last 5 
years, many methods have been introduced to improve the performance of quantized 
neural networks. To better review these methods, we focus on six aspects: gradient 
approximation, quantization, structural design, loss design, optimization, and neural 
architecture search. We also review the applications of neural networks with model 
compression in visual and audio analysis. There are also other model compression
v

vi
Preface
techniques, such as model compression with network pruning, widely used in edge 
computing, which we introduce for completeness in this book. From our previous 
studies, network pruning and quantized neural networks can be used simultaneously 
to complement each other, whereas network pruning on quantized neural networks 
can further compress models and improve the generalization ability for many 
downstream applications. 
Beijing, China
Baochang Zhang 
Beijing, China
Tiancheng Wang 
Beijing, China
Sheng Xu 
Buffalo, NY, USA
David Doermann

Contents 
1 
Introduction ..................................................................
1 
1.1 Background.............................................................. 
1 
1.2 Introduction of Deep Learning .........................................
2 
1.3 Model Compression and Acceleration .................................
4 
References .....................................................................
5 
2 
Binary Neural Networks ....................................................
7 
2.1 Introduction .............................................................
7 
2.2 Gradient Approximation ................................................
9 
2.3 Quantization ............................................................. 
10 
2.4 Structural Design........................................................ 
14 
2.5 Loss Design ............................................................. 
17 
2.6 Optimization ............................................................ 
19 
2.7 Algorithms for Binary Neural Networks............................... 
22 
2.7.1 BNN: Binary Neural Network ................................. 
23 
2.7.2 XNOR-Net: ImageNet Classiﬁcation Using Binary 
Convolutional Neural Networks ............................... 
24 
2.7.3 SA-BNN: State-Aware Binary Neural Network............... 
26 
2.7.4 PCNN: Projection Convolutional Neural Networks........... 
31 
References ..................................................................... 
44 
3 
Binary Neural Architecture Search ........................................ 
49 
3.1 Introduction ............................................................. 
49 
3.2 Neural Architecture Search ............................................. 
50 
3.2.1 ABanditNAS: Anti-bandit for Neural Architecture Search ... 
50 
3.2.2 IDARTS: Interactive Differentiable Architecture Search ..... 
59 
3.2.3 Fast and Unsupervised Neural Architecture 
Evolution for Visual Representation Learning ................ 
67 
3.3 Binary Neural Architecture Search..................................... 
76 
3.3.1 BNAS: Binarized Neural Architecture Search for 
Efﬁcient Object Recognition ................................... 
76
vii

viii
Contents
3.3.2 BDetNAS: A Fast Binarized Detection Neural 
Architecture Search............................................. 
88 
References ..................................................................... 
95 
4 
Quantization of Neural Networks .......................................... 101 
4.1 Introduction ............................................................. 101 
4.2 Quantitative Arithmetic Principles ..................................... 101 
4.3 Uniform and Nonuniform Quantization................................ 102 
4.4 Symmetric and Asymmetric Quantization ............................. 103 
4.5 Comparison of Different Quantization Methods ...................... 104 
4.5.1 LSQ: Learned Step Size Quantization ......................... 104 
4.5.2 TRQ: Ternary Neural Networks with Residual 
Quantization..................................................... 108 
4.5.3 OMPQ: Orthogonal Mixed Precision Quantization ........... 119 
References ..................................................................... 127 
5 
Network Pruning............................................................. 131 
5.1 Introduction ............................................................. 131 
5.2 Structured Pruning ...................................................... 133 
5.3 Unstructured Pruning ................................................... 134 
5.4 Network Pruning ........................................................ 134 
5.4.1 Efﬁcient Structured Pruning Based on Deep Feature 
Stabilization ..................................................... 134 
5.4.2 Toward Compact and Sparse CNNs via 
Expectation-Maximization ..................................... 147 
5.4.3 Pruning Multi-view Stereo Net for Efﬁcient 3D 
Reconstruction .................................................. 157 
5.4.4 Cogradient Descent for Dependable Learning ................ 168 
5.5 Network Pruning on BNNs ............................................. 189 
5.5.1 Rectiﬁed Binary Convolutional Networks with 
Generative Adversarial Learning............................... 189 
5.5.2 BONN: Bayesian Optimized Binary Neural Network ........ 198 
References ..................................................................... 213 
6 
Applications .................................................................. 219 
6.1 Introduction ............................................................. 219 
6.2 Image Classiﬁcation .................................................... 219 
6.3 Speech Recognition ..................................................... 220 
6.3.1 1-Bit WaveNet: Compression of a Generative Neural 
Network in Speech Recognition with Two Binarized 
Methods ......................................................... 220 
6.4 Object Detection and Tracking ......................................... 227 
6.4.1 Data-Adaptive Binary Neural Networks for Efﬁcient 
Object Detection and Recognition ............................. 228

Contents
ix
6.4.2 Amplitude Suppression and Direction Activation in 
Networks for Faster Object Detection ......................... 238 
6.4.3 Q-YOLO: Efﬁcient Inference for Real-Time Object 
Detection ........................................................ 249 
References ..................................................................... 256

Chapter 1 
Introduction 
1.1 
Background 
Recently, there has been a signiﬁcant increase in the complexity of deep learning 
models, with models becoming more and more intricate [2, 3, 7–10]. However, the 
hardware on which these models are deployed has not kept up with the increasing 
computational demands. Practical limitations such as latency, battery life, and 
temperature have created a signiﬁcant gap between the computational requirements 
of these models and the available hardware resources. 
To bridge this gap, network quantization has emerged as a popular approach 
[1, 4–6]. Network quantization involves mapping single-precision ﬂoating-point 
weights or activations to lower bit integers, leading to compression and acceleration 
of the model. One notable technique in this area is binary neural network (BNN), 
which is the simplest version of low-bit networks and has gained signiﬁcant 
attention due to its highly compressed parameters and activation features [1]. 
Notably, the company Xnor.ai has become prominent for its work on BNNs. 
Founded in 2016, the company has raised substantial funding to develop tools 
that enable AI algorithms to run on devices rather than remote data centers. This 
approach allows for greater privacy and faster processing. Recently, Apple Inc. 
acquired Xnor.ai and plans to leverage BNN technology to enhance user privacy 
and accelerate processing on its devices. 
Deep learning has gained signiﬁcant importance due to its exceptional perfor-
mance; however, it faces challenges in terms of large memory requirements and high 
computational demands, making it difﬁcult to deploy on resource-constrained front-
end devices. For instance, unmanned systems rely on UAVs as computing terminals 
with limited memory and computational resources, posing obstacles to real-time 
data processing using convolutional neural networks (CNNs). To address these 
efﬁciency concerns, binary neural networks (BNNs) have emerged as promising 
solutions for practical applications. BNNs are neural networks that binarize weights, 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
B. Zhang et al., Neural Networks with Model Compression, 
Computational Intelligence Methods and Applications, 
https://doi.org/10.1007/978-981-99-5068-3_1
1

2
1
Introduction
offering improved storage and computation efﬁciency. Taking this approach further, 
1-bit CNNs achieve extreme compression by binarizing both the weights and 
activations, reducing the model size and computational costs even further. Such 
highly compressed models are well-suited for front-end computing tasks. Alongside 
BNNs, other techniques like pruning neural networks involving quantization are 
widely utilized in edge computing. 
This book comprehensively analyzes the latest advancements in model com-
pression technologies speciﬁcally designed for front-end computing. It offers an 
extensive review and summary of existing research, categorized into binary neural 
networks, binary neural architecture search, quantization of neural networks, and 
network pruning. Furthermore, the book explores the practical applications of these 
techniques in computer vision and speech recognition, shedding light on their 
potential for future applications in edge computing. 
1.2 
Introduction of Deep Learning 
Deep learning is a subset of machine learning that focuses on developing and 
applying artiﬁcial neural networks with multiple layers, also known as deep neural 
networks. It is inspired by the structure and function of the human brain, speciﬁcally 
the interconnectedness of neurons. 
Deep learning models, also known as deep neural networks, comprise multiple 
layers of interconnected artiﬁcial neurons called units or nodes. These layers include 
an input layer, one or more hidden layers, and an output layer. Each unit in the 
network receives input signals, applies a mathematical transformation to them, and 
produces an output signal that is passed to the next layer. The weights associated 
with each connection between the units determine the strength and impact of the 
signals. The key features and concepts of deep learning are as follows: 
Neural Network Architecture Deep learning models can have many architectures, 
depending on the task and data being addressed. Common architectures include 
feedforward neural networks, convolutional neural networks (CNNs) for image 
analysis, recurrent neural networks (RNNs) for sequence data, and transformers for 
natural language processing tasks. 
Training Deep learning models learn from data through training. During training, 
the model is presented with a labeled dataset and adjusts its weights to minimize 
the difference between its predictions and the true labels. This optimization is 
achieved using an algorithm called back propagation (BP), which calculates the 
gradients of the model’s performance concerning the weights and updates the 
weights accordingly. The process iterates until the model converges to a satisfactory 
level of performance. 
Activation Functions Activation functions introduce nonlinearities to the neural 
network, allowing it to model complex relationships between inputs and outputs.

1.2
Introduction of Deep Learning
3
Common activation functions include sigmoid, hyperbolic tangent (tanh), and 
rectiﬁed linear unit (ReLU). They help the network learn nonlinear patterns and 
make the model more expressive. 
Loss Functions Loss functions measure the discrepancy between the predicted 
outputs of the model and the true labels in the training data. They provide a 
quantitative measure of how well the model is performing. Common loss functions 
include mean squared error (MSE) for regression tasks and categorical cross-
entropy for classiﬁcation tasks. 
Optimization Algorithms Optimization algorithms are used to update the neural 
network weights during training. Stochastic gradient descent (SGD) is a widely used 
optimization algorithm that iteratively adjusts the weights based on the gradients 
computed through back propagation. Variants of SGD, such as Adam and RMSprop, 
are also commonly employed to improve training efﬁciency and convergence. 
Regularization Deep learning models are prone to overﬁtting, which occurs when 
the model becomes too specialized to the training data and performs poorly on 
unseen data. Regularization techniques, such as L1 and L2 regularization, dropout, 
and early stopping, are used to prevent overﬁtting and improve the model’s 
generalization ability. 
One of the key advantages of deep learning is its ability to automatically learn 
feature representations from raw data. Traditionally, in machine learning, feature 
engineering is a crucial step where domain experts manually extract relevant 
features from the data. In deep learning, the neural network learns these features 
directly from the raw data during training. This removes the need for manual feature 
engineering and allows the model to discover complex patterns and representations. 
Deep learning has achieved remarkable success in various domains. In computer 
vision, deep neural networks have achieved state-of-the-art results in tasks such as 
image classiﬁcation, object detection, and image segmentation. Deep learning has 
revolutionized machine translation, sentiment analysis, and speech recognition in 
natural language processing. It has also been applied to recommender systems, drug 
discovery, ﬁnance, and autonomous vehicles. 
The success of deep learning is due to several factors. Firstly, the availability of 
large-scale datasets, such as ImageNet for computer vision or the Common Crawl 
dataset for natural language processing, has enabled the training of deep neural 
networks with millions or even billions of parameters. Secondly, advancements in 
computing power, particularly GPUs (graphics processing units), have accelerated 
the training process by performing parallel computations. Lastly, developing efﬁ-
cient algorithms like stochastic gradient descent and its variants has made it feasible 
to train deep neural networks effectively. 
However, deep learning also presents challenges. Training deep neural networks 
requires substantial computational resources, and training times can be lengthy, 
especially for complex models. Deep learning models are also data-hungry and 
often require large labeled datasets, which may only sometimes be readily available.

4
1
Introduction
Overﬁtting, where the model becomes too specialized to the training data and 
performs poorly on unseen data, is another challenge that needs to be addressed. 
Researchers have been exploring techniques to overcome these challenges in 
recent years, such as transfer learning, which enables pre-training on large-scale 
datasets and ﬁne-tuning on smaller task-speciﬁc datasets. There is also ongoing 
research on model compression, model acceleration, developing more efﬁcient 
architectures, regularization techniques, and ways to leverage smaller datasets 
effectively. 
Deep learning has revolutionized the ﬁeld of artiﬁcial intelligence, enabling 
machines to learn and make intelligent decisions from vast amounts of data. Its 
ability to learn complex patterns and representations has signiﬁcantly advanced in 
various domains. 
1.3 
Model Compression and Acceleration 
Model compression and acceleration techniques reduce deep learning models’ size 
and computational requirements, making them more efﬁcient and practical for 
deployment on resource-constrained devices or in real-time applications. These 
techniques aim to balance model performance and efﬁciency, enabling faster 
inference times and reducing memory footprint while preserving or minimizing the 
loss in accuracy. 
Pruning Pruning involves removing unimportant connections or weights from a 
trained neural network. It can be done in various ways, such as magnitude-based 
pruning, where weights below a certain threshold are pruned, or structured pruning, 
where entire ﬁlters or layers are pruned. Pruning reduces the number of parameters 
and connections in the network, resulting in a more compact model. 
Quantization Quantization reduces the precision of weights and activations in the 
neural network from ﬂoating-point representation (32-bit) to lower bit representa-
tions (e.g., 8-bit or even lower). By using lower precision, quantization reduces 
memory usage and improves computational efﬁciency, as integer operations are 
typically faster than ﬂoating-point operations. 
Low-Rank Factorization This technique reduces the number of parameters in 
a neural network by approximating weight matrices using low-rank factorization 
methods. Composing weight matrices into smaller matrices of lower rank can sig-
niﬁcantly reduce the number of parameters while maintaining reasonable accuracy. 
Knowledge Distillation Knowledge distillation involves training a smaller “stu-
dent” network to mimic the behavior of a larger “teacher” network. The teacher 
network provides soft targets (probability distributions) instead of hard labels during 
training. The student network learns to generalize from the teacher’s knowledge, 
resulting in a compact model that can achieve comparable performance to the larger 
model.

References
5
Model Architecture Design Efﬁcient model architecture design aims to create 
compact and lightweight models from scratch. Techniques like depth-wise separable 
convolutions, bottleneck layers, and skip connections can reduce the number of 
parameters and computational complexity while maintaining or improving perfor-
mance. 
Model Parallelism and Model Parallel Training Model parallelism divides 
a deep learning model across multiple devices or processors, allowing parallel 
computation and reducing the memory requirements for model inference. Similarly, 
model parallel training divides the training process across multiple devices, reducing 
the memory demand during training and enabling faster convergence. 
Hardware Acceleration Hardware accelerators, such as graphics processing 
units (GPUs), tensor processing units (TPUs), or ﬁeld-programmable gate 
arrays (FPGAs), are specialized devices designed to accelerate deep learning 
computations. These accelerators can signiﬁcantly speed up the inference and 
training processes and improve energy efﬁciency. 
These techniques can be used individually or in combination to achieve model 
compression and acceleration. The choice of techniques depends on the speciﬁc 
requirements of the deployment scenario and the trade-off between model size, 
computational efﬁciency, and accuracy. 
Model compression and acceleration techniques have enabled the deployment of 
deep learning models on edge devices, mobile devices, and embedded systems, mak-
ing real-time inference and applications like object detection, speech recognition, 
and natural language processing feasible in resource-constrained environments. 
These techniques have also paved the way for advancements in autonomous 
vehicles, the Internet of Things (IoT), and edge computing, where efﬁcient and 
lightweight models are crucial for efﬁcient and scalable deployment. 
References 
1. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep 
neural networks with binary weights during propagations. In Advances in neural information 
processing systems, pages 3123–3131, 2015. 
2. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 
Deep residual learning for 
image recognition. In Proceedings of the IEEE conference on computer vision and pattern 
recognition, pages 770–778, 2016. 
3. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias 
Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural 
networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 
4. Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim. Binaryduo: Reducing gradient 
mismatch in binary activation network by coupling binary activations. 
In International 
Conference on Learning Representations. 
5. Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong 
Ji, and David Doermann. Circulant binary convolutional networks: Enhancing the performance 
of 1-bit DCNNs with circulant back propagation. In Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition, pages 2691–2699, 2019.

6
1
Introduction
6. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 
XNOR-net: 
Imagenet classiﬁcation using binary convolutional neural networks. In European Conference 
on Computer Vision, pages 525–542. Springer, 2016. 
7. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 
Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 4510–4520, 2018. 
8. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale 
image recognition. arXiv preprint arXiv:1409.1556, 2014. 
9. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, 
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. 
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 
2015. 
10. Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient 
convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017.

Chapter 2 
Binary Neural Networks 
2.1 
Introduction 
This chapter provides an overview of the most recent developments in binary 
neural network (BNN) technologies, with a particular focus on their suitability for 
front-end, edge-based computing. The content includes a thorough examination and 
synthesis of current research, organized into various categories such as gradient 
approximation, quantization techniques, architectural considerations, loss functions, 
optimization methods, and binary neural architecture search. Moreover, the chapter 
delves into the real-world applications of BNNs in computer vision and speech 
recognition while also contemplating the promising future prospects of BNNs across 
diverse domains. 
In this chapter, we conduct a comprehensive review of the noteworthy advance-
ments in binary neural networks and 1-bit CNNs. While binarization operations 
offer improved efﬁciency, they often come at the cost of reduced performance. 
However, over the past 5 years, several techniques have emerged to enhance the 
performance of binary neural networks signiﬁcantly. To facilitate a comprehensive 
review of these methods, we categorize them into six key aspects: gradient 
approximation, quantization, structural design, loss design, optimization, and binary 
neural architecture search. 
Additionally, we delve into the applications of BNNs in object detection, object 
tracking, and audio analysis, assessing their efﬁcacy and potential in these speciﬁc 
domains. By presenting a holistic examination of the recent advancements and 
practical use-cases of binary neural networks, we aim to shed light on the promising 
future prospects of this technology. 
BinaryConnect [12] was the ﬁrst work attempting to conﬁne weights to either 
+1 or minus−1 during propagation without binarizing the inputs. Binary operations 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
B. Zhang et al., Neural Networks with Model Compression, 
Computational Intelligence Methods and Applications, 
https://doi.org/10.1007/978-981-99-5068-3_2
7

8
2
Binary Neural Networks
possess simplicity and ease of comprehension. One of the approaches to binarize 
convolutional neural networks (CNNs) involves the utilization of a sign function: 
omega Subscript b Baseline equals StartLayout Enlarged left brace 1st Row 1st Column plus 1 comma 2nd Column i f omega greater than or equals 0 2nd Row 1st Column negative 1 comma 2nd Column o t h e r w i s e EndLayout commaωb =
{+1, if
ω ≥0
−1, otherwise ,
(2.1) 
where omega Subscript bωb is the binarized weight and omegaω the real-valued weight. A second way is to 
binarize scholastically: 
omega Subscript b Baseline equals StartLayout Enlarged left brace 1st Row 1st Column plus 1 comma 2nd Column w i t h p r o b a b i l i t y p equals sigma left parenthesis omega right parenthesis 2nd Row 1st Column negative 1 comma 2nd Column w i t h p r o b a b i l i t y 1 minus p EndLayout commaωb =
{ +1, with
probability
p = σ(ω)
−1,
with
probability
1 −p
,
(2.2) 
where sigmaσ is the “hard sigmoid” function. The training process for binary neural 
networks differs slightly from that of full-precision neural networks. During forward 
propagation, binary neural networks employ binarized weights instead of full-
precision weights, while backward propagation follows conventional methods. The 
gradient StartFraction partial differential upper C Over partial differential omega Subscript b Baseline EndFraction ∂C
∂ωb (where C is the cost function) needs to be calculated and then 
combined with the learning rate to directly update the full-precision weights. 
BinaryNet [25] extends beyond BinaryConnect by not only binarizing the 
weights but also quantizing the activations. To enforce both weights and activa-
tions to be either +1 or minus−1, BinaryNet introduces two methods. Additionally, it 
incorporates several modiﬁcations to accommodate binary activations. Firstly, it 
implements shift-based batch normalization (SBN) to avoid additional multiplica-
tions. Secondly, it employs shift-based AdaMax instead of the ADAM learning rule, 
which reduces the number of multiplications. The third modiﬁcation concerns the 
operation performed on the input of the ﬁrst layer, though speciﬁc details are not 
provided in this statement. For continuous-valued inputs of the ﬁrst layer, BinaryNet 
represents them as ﬁxed-point numbers with m bits of precision. 
While BinaryConnect and BinaryNet demonstrate promising performance on 
representative datasets (as shown in Table 5.1), they struggle to perform well on 
larger datasets. The constraint of weights to +1 and minus−1 hinders effective learning. 
Therefore, new methods for training binary neural networks and 1-bit networks need 
to be developed to address these limitations. It is worth noting that QNN (quantized 
neural networks) [26] proposed training neural networks with extremely low-bit 
weights and activations, but the speciﬁc details of QNN are omitted in this review 
as we primarily focus on binary networks. 
Wang et al. [60] proposed binarized deep neural networks (BDNNs) for image 
classiﬁcation tasks, where all the values and operations in the network are binarized. 
While BinaryNet deals with convolutional neural networks, BDNNs target essen-
tial artiﬁcial neural networks consisting of full-connection layers. Bitwise neural 
networks [31] also present a completely bitwise network where all participating 
variables are bipolar binaries.

2.2
Gradient Approximation
9
2.2 
Gradient Approximation 
In BNNs and 1-bit networks, the parameter updates involve full-precision weights 
using the gradient StartFraction partial differential upper C Over partial differential omega Subscript b Baseline EndFraction∂C
∂ωb . However, during forward propagation, a sign function is 
applied between full-precision and binarized weights. Consequently, the gradient of 
the sign function must be considered when updating the full-precision weights. As 
the derivative of the sign function is zero almost everywhere and becomes inﬁnite at 
zero points, approximations using derivable functions are commonly employed to 
effectively handle the update process. 
The ﬁrst solution for addressing this issue in a 1-bit network was introduced by 
BinaryNet [25]. Assuming that an estimator of g Subscript qgq for the gradient StartFraction partial differential upper C Over partial differential q EndFraction∂C
∂q , where q is 
upper S i g n left parenthesis r right parenthesisSign(r), has been obtained, the straight-through estimator of StartFraction partial differential upper C Over partial differential r EndFraction∂C
∂r is simply: 
g Subscript r Baseline equals g Subscript q Baseline 1 Subscript StartAbsoluteValue r EndAbsoluteValue less than or equals 1 Baseline commagr = gq1|r|≤1,
(2.3) 
where 1 Subscript StartAbsoluteValue r EndAbsoluteValue less than or equals 11|r|≤1 equals 1 when StartAbsoluteValue r EndAbsoluteValue less than or equals 1|r| ≤1. And it equals 0 in other cases. It can also be 
seen as propagating the gradient through the hard tanh, which is a piecewise-linear 
activation function. 
The Bi-Real Net [44] addresses the approximation of the derivative of the sign 
function for activations in binary neural networks. Instead of using Htanh [25] for  
this purpose, the Bi-Real Net employs a piecewise polynomial function, resulting in 
a more accurate approximation. 
Furthermore, the Bi-Real Net introduces a magnitude-aware gradient for weights. 
In traditional binary neural networks, the gradient StartFraction partial differential upper C Over partial differential upper W EndFraction ∂C
∂W is solely determined by the 
sign of weights and is independent of their magnitude. To enhance the learning 
process, the Bi-Real Net replaces the sign function with a magnitude-aware 
function, allowing the model to take into account both the sign and magnitude 
of weights during parameter updates. This approach contributes to more effective 
and ﬁne-grained weight updates, leading to improved overall performance in binary 
neural networks. 
Xu et al. [72] propose a higher-order approximation for weight binarization 
in binary neural networks. They use a long-tailed approximation for activation 
binarization, striking a balance between tight approximation and smooth back 
propagation. 
In DSQ [17], a differentiable soft quantization function is introduced to approx-
imate the standard binary and uniform quantization process. This function uses 
hyperbolic tangent functions to gradually approach the staircase function, speciﬁ-
cally for low-bit quantization (similar to the sign function in 1-bit CNN). The binary 
DSQ function is as follows: 
upper Q Subscript s Baseline left parenthesis x right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column negative 1 comma 2nd Column x less than negative 1 2nd Row 1st Column 1 comma 2nd Column x greater than 1 3rd Row 1st Column s t a n h left parenthesis k x right parenthesis comma 2nd Column o t h e r w i s e EndLayout commaQs(x) =
⎧
⎨
⎩
−1,
x < −1
1,
x > 1
stanh(kx), otherwise
,
(2.4)

10
2
Binary Neural Networks
with 
k equals one half l o g left parenthesis StartFraction 2 Over alpha EndFraction minus 1 right parenthesis comma s equals StartFraction 1 Over 1 minus alpha EndFraction periodk = 1
2log( 2
α −1), s =
1
1 −α .
(2.5) 
DSQ approximates uniform quantization well, especially with a small alphaα, making 
it valuable for training high-accuracy quantized models. Being differentiable, DSQ 
allows for smooth parameter updates, contributing to improved accuracy compared 
to non-differentiable methods. 
In summary, the methods discussed introduce differentiable functions to approxi-
mate the sign function in BinaryConnect. This allows for more accurate computation 
of gradients during training. As a result, binary neural networks and 1-bit networks 
converge more easily during the training process, leading to improved network 
performance and higher accuracy. These differentiable approximations have signif-
icantly advanced the ﬁeld of binary neural networks and made them more practical 
and effective for various applications. 
2.3 
Quantization 
BinaryConnect and BinaryNet use simple quantization methods where the binary 
weights are generated by taking the sign of full-precision weights after their update. 
However, this approach may lead to signiﬁcant quantization errors. 
Before discussing new methods to improve the quantization process, let’s clarify 
the notations used in XNOR-Net [53] for each layer in a convolutional neural 
network. For each layer in a convolutional neural network, I is the input, W is 
the weight ﬁlter, B is the binarized weight (+-1), and H is the binarized input. 
In their work, Rastegari et al. [53] introduce binary weight networks (BWN) and 
XNOR-Networks. BWN approximates weights with binary values, representing a 
variation of binary neural networks. On the other hand, XNOR-Networks binarize 
both weights and activation bits, making it a 1-bit network. Both networks utilize a 
scaling factor. 
In BWN, the real-valued weight ﬁlter W is estimated using a binary ﬁlter B and 
a scaling factor alphaα. The convolutional operation is then approximated as follows: 
upper I asterisk upper W almost equals left parenthesis upper I circled plus upper B right parenthesis alpha commaI ∗W ≈(I ⊕B)α,
(2.6) 
where circled plus⊕indicates a convolution without multiplication. By introducing the scaling 
factor, binary weight ﬁlters reduce memory usage by a factor of 32 times32× compared to 
single-precision ﬁlters. To ensure W is approximately equal to alpha upper BαB, BWN deﬁnes an 
optimization problem, and the optimal solution is: 
upper B Superscript asterisk Baseline equals s i g n left parenthesis upper W right parenthesis commaB∗= sign(W),
(2.7) 
alpha Superscript asterisk Baseline equals StartFraction upper W Superscript upper T Baseline s i g n left parenthesis upper W right parenthesis Over n EndFraction equals StartFraction sigma summation StartAbsoluteValue upper W Subscript i Baseline EndAbsoluteValue Over n EndFraction equals StartFraction 1 Over n EndFraction parallel to upper W Subscript r Baseline parallel to Subscript l 1 Baseline periodα∗= W T sign(W)
n
=
Σ |Wi|
n
= 1
n||Wr||l1.
(2.8)

2.3
Quantization
11
Indeed, in both BWN and XNOR-Networks, the optimal estimation of binary 
weight ﬁlters involves taking the sign of weight values. The optimal scaling factor, 
alphaα, for BWN is the absolute average of the absolute weight values. This scaling 
factor is crucial in the calculation of gradients during back propagation, allowing 
for effective weight updates. 
For XNOR-Networks, another scaling factor, betaβ, is used when binarizing the 
input I into H. The core idea of XNOR-Networks is similar to BWN, but the 
introduction of betaβ during activation binarization provides additional optimization 
beneﬁts. The experiments demonstrate that this approach signiﬁcantly outperforms 
BinaryConnect and BNN on ImageNet. 
In Xu et al.’s work [72], a trainable scaling factor is deﬁned for both weights 
and activations, enhancing the adaptability and performance of quantized neural 
networks. 
LQ-Nets [76] quantize both weights and activations using arbitrary bit widths, 
including 1 bit. The learnable nature of the quantizers allows them to be compatible 
with bitwise operations, preserving the fast inference beneﬁts of properly quantized 
neural networks. 
Based on XNOR-Net [53], HORQ [37] introduces a high-order binarization 
scheme to achieve a more accurate approximation while retaining the advantages of 
binary operations. High-order residual quantization (HORQ) calculates the residual 
error and then performs additional thresholding operations to further approximate 
the residual. This binary approximation of the residual can be considered a higher-
order binary input. Similar to XNOR-Net, HORQ deﬁnes the ﬁrst-order residual 
tensor upper R 1 left parenthesis x right parenthesisR1(x) by computing the difference between the real input and the ﬁrst-order 
binary quantization: 
upper R 1 left parenthesis x right parenthesis equals upper X minus beta 1 upper H 1 almost equals beta 2 upper H 2 commaR1(x) = X −β1H1 ≈β2H2,
(2.9) 
where upper R 1 left parenthesis x right parenthesisR1(x) is a real value tensor. By this analogy, upper R 2 left parenthesis x right parenthesisR2(x) can be seen as the second-
order residual tensor, and beta 3 upper H 3β3H3 also approximates it. After recursively performing 
the above operations, they obtain order-K residual quantization: 
upper X equals sigma summation Underscript i equals 1 Overscript upper K Endscripts beta Subscript i Baseline upper H Subscript i Baseline periodX =
K
Σ
i=1
βiHi.
(2.10) 
During the training of the HORQ network, the input tensor can be reshaped 
into a matrix, allowing it to be expressed as any order of residual quantization. 
By considering higher-order residual approximations, HORQ-Net achieves a more 
accurate representation of binary values. Experimental results demonstrate that 
HORQ-Net outperforms XNOR-Net in terms of accuracy on the CIFAR dataset. 
ABC-Net [38] is another network designed to improve the performance of binary 
networks. ABC-Net approximates the full-precision weight ﬁlter W with a linear

12
2
Binary Neural Networks
combination of M binary ﬁlters upper B 1 comma upper B 2 comma ellipsis comma upper B Subscript upper M Baseline element of StartSet plus 1 comma negative 1 EndSetB1, B2, . . . , BM ∈{+1, −1} such that upper W almost equals alpha 1 beta 1 plus ellipsis plus alpha Subscript upper M Baseline beta Subscript upper MW ≈α1β1+
. . . + αMβM. These binary ﬁlters are ﬁxed as follows: 
upper B Subscript i Baseline equals upper F Subscript u Sub Subscript i Subscript Baseline left parenthesis upper W right parenthesis colon equals s i g n left parenthesis upper W overbar plus u Subscript i Baseline s t d left parenthesis upper W right parenthesis right parenthesis comma i equals 1 comma 2 comma ellipsis comma upper M commaBi = Fui(W) := sign( ¯W + uistd(W)), i = 1, 2, . . . , M,
(2.11) 
where upper W overbar ¯W and s t d left parenthesis upper W right parenthesisstd(W) are the mean and standard derivation of W. For activations, 
ABC-Net employs multiple binary activations to alleviate information loss. Like the 
binarization weights, the real activation I is estimated using a linear combination of 
N activations upper A 1 comma upper A 2 comma ellipsis comma upper A Subscript upper N BaselineA1, A2, . . . , AN such that upper I equals beta 1 upper A 1 plus ellipsis plus beta Subscript upper N Baseline upper A Subscript upper NI = β1A1 + . . . + βNAN, where 
upper A 1 comma upper A 2 comma ellipsis comma upper A Subscript upper N Baseline equals upper H Subscript v 1 Baseline left parenthesis upper R right parenthesis comma upper H Subscript v 2 Baseline left parenthesis upper R right parenthesis comma ellipsis comma upper H Subscript v Sub Subscript upper N Subscript Baseline left parenthesis upper R right parenthesis periodA1, A2, . . . , AN = Hv1(R), Hv2(R), . . . , HvN (R).
(2.12) 
upper H left parenthesis upper R right parenthesisH(R) in Eq. 2.12 is a binary function, h is a bounded activation function, I is 
the indicator function, and v is a shift parameter. Unlike the input weights, the 
parameters betaβ and v are trainable. Without explicit linear regression, the network 
tunes beta prime Subscript n Baseline sβ'
ns and v prime Subscript n Baseline sv'
ns during training and is ﬁxed for testing. They are expected to learn 
and utilize the statistical features of full-precision activations. 
Ternary-binary network (TBN) [57] is a convolutional neural network with 
ternary inputs and binary weights. It leverages accelerated ternary-binary matrix 
multiplication, using efﬁcient operations like XOR, AND, and bit count commonly 
found in standard CNNs. TBN achieves an optimal trade-off between memory, 
efﬁciency, and performance. Wang et al. [59] propose a two-step quantization 
framework (TSQ) that decomposes network quantization into two stages: code 
learning and transformation function learning based on the learned codes. TSQ is 
mainly designed for 2-bit neural networks. 
LBCNN [28] introduces a local binary convolution (LBC) layer, inspired by local 
binary patterns (LBP) used in image descriptors, especially in face recognition. 
The LBC layer comprises ﬁxed, sparse, and predeﬁned binary convolutional ﬁlters 
that remain unchanged during training. It includes a nonlinear activation function 
and learnable linear weights. The linear weights combine the activated ﬁlter 
responses, approximating the corresponding activated ﬁlter responses of a standard 
convolutional layer. The LBC layer signiﬁcantly reduces the number of learnable 
parameters, offering parameter savings of 9x to 169x compared to a standard 
convolutional layer. Additionally, due to the sparse and binary nature of the weights, 
it results in up to 169x savings in model size when compared to conventional 
convolutions. 
In MCN [61], modulation ﬁlters (M-Filters) are introduced to recover binarized 
ﬁlters. M-Filters are designed to approximate unbinarized convolutional ﬁlters 
within an end-to-end framework. Each layer shares only one M-Filter, leading to 
a signiﬁcant reduction in model size. To reconstruct the unbinarized ﬁlters, MCN 
employs a modulated process based on the M-Filters and binarized ﬁlters. Figure 2.1 
illustrates an example of the modulation process. In this example, the M-Filter 
has four planes, each expanding to a 3D matrix according to the channels of the 
binarized ﬁlter. The reconstructed ﬁlter Q is obtained through the ring◦operation 
between the binarized ﬁlter and each expanded M-Filter.

2.3
Quantization
13
Fig. 2.1 Modulation process based on an M-Filter 
Fig. 2.2 MCNs’ convolution 
As depicted in Fig. 2.2, the reconstructed ﬁlters Q are utilized to compute the 
output feature maps F. Figure 2.2 shows four planes, resulting in four channels in 
the feature maps. The key advantage of MCN’s convolution is that it maintains the 
same number of input and output channels for each feature map, facilitating module 
replication and easy implementation of MCNs. 
Unlike previous approaches that independently binarize each ﬁlter, Bulat et 
al. [8] propose parameterizing the weight tensor of each layer using a matrix 
or tensor decomposition. The binarization process involves using a quantization 
function (e.g., sign function) for the reconstructed weights, while computation in 
the latent factorized space is performed in the real domain. This approach offers 
several advantages. First, the latent factorization enforces a coupling of ﬁlters 
before binarization, leading to a signiﬁcant improvement in the accuracy of trained 
models. This coupling allows the model to capture more complex and ﬁne-grained 
features, contributing to higher accuracy in tasks. Second, during training, each 
convolutional layer’s binary weights are parametrized using a real-valued matrix 
or tensor decomposition. However, during inference, reconstructed (binary) weights 
are used, which retains the efﬁciency beneﬁts of binary neural networks during the 
testing phase.

14
2
Binary Neural Networks
In contrast to previous approaches that use the same binary method for both 
weights and activations, Huang et al. [24] propose a different approach. They 
believe that the best performance for binarized neural networks can be achieved 
by applying different quantization methods to weights and activations. In their 
method, they simultaneously binarize the weights while quantizing the activations. 
This simultaneous approach aims to reduce bandwidth. 
ReActNet [43] introduces a novel approach to binarized neural networks. It 
replaces the traditional sign function with ReAct-Sign and the PReLU function 
with ReAct-PReLU. These operations involve a simple channel-wise reshaping and 
shifting operation for the activation distribution. In ReAct-Sign and ReAct-PReLU, 
the parameters can be updated during training, allowing the network to learn and 
adapt to the data. This feature makes ReActNet more ﬂexible and capable of 
capturing complex patterns in the data, leading to improved performance compared 
to traditional binarized neural networks. 
Compared to XNOR-Net [53], both HORQ-Net [37] and ABC-Net [38] use  
multiple binary weights and activations, leading to improved performance on binary 
tasks. However, this improvement comes at the cost of increased complexity, which 
goes against the initial intention of binary neural networks to be efﬁcient and speedy. 
To address this challenge, new neural network architectures are continuously being 
explored. MCN [61] and LBCNN [28] propose innovative ﬁlters while quantizing 
parameters. Additionally, they introduce new loss functions to learn these additional 
ﬁlters. 
2.4 
Structural Design 
Indeed, the fundamental structure of networks like BinaryConnect [12] and Bina-
ryNet [25] closely resembles that of traditional convolutional neural networks, 
which may not be optimally suited for binary processing. As a result, researchers 
have sought to modify the architecture of binary neural networks to enhance their 
accuracy. 
In XNOR-Net [53], the block structure in a typical CNN is changed to further 
decrease information loss due to binarization. A typical block in a CNN typically 
contains different layers in the following order: 1-Convolutional, 2-BatchNorm, 3-
Activation, and 4-Pooling. Before binarization, the input is normalized to have zero 
means. This normalization step is crucial in minimizing quantization error during 
thresholding at zero. The order of the layers in XNOR-Net is shown in Fig. 2.3. 
In the context of Bi-Real Net [44], the poor performance of 1-bit CNNs is 
attributed to their limited representation capacity. Representation capacity refers 
to the number of possible conﬁgurations of a variable, which could be a scalar, 
vector, matrix, or tensor. To address this limitation and increase the representation 
capability of 1-bit CNNs, Bi-Real Net proposes a straightforward shortcut. The 
shortcut in Bi-Real Net preserves the real-valued activations before the sign 
function, effectively increasing the network’s representation capacity. The structure

2.4
Structural Design
15
Fig. 2.3 A block in XNOR-Net 
Fig. 2.4 1-bit CNN with shortcut 
of the block is depicted as “Sign right arrow→1-bit convolution right arrow→batch normalization right arrow→
addition operator” in Fig. 2.4. The shortcut connects the input activations, which 
pass through the sign function in the current block, to the output activations after 
the batch normalization in the same block. These two sets of activations are then 
combined using an addition operator. The resulting combined activations are then 
passed to the sign function in the subsequent block. 
By introducing this shortcut and preserving the real activations, Bi-Real Net 
seeks to enhance the expressiveness of 1-bit CNNs, ultimately improving their 
performance and accuracy in various tasks. 
BinaryDenseNet [6] is a new binary neural network (BNN) architecture that 
addresses the main drawbacks of BNNs. It is based on DenseNets [23], which 
utilize shortcut connections to maintain the information ﬂow throughout the depth 
of the network. However, the bottleneck design in DenseNets reduces the ﬂow of 
information between layers, which is not suitable for BNNs due to their limited 
representation capacity. To overcome this limitation, BinaryDenseNet increases the 
growth rate or the number of blocks in the architecture to achieve satisfactory 
performance. Speciﬁcally, to maintain the same number of parameters as a given 
BinaryDenseNet, the growth rate is halved, and the number of blocks is doubled 
simultaneously. The architecture of BinaryDenseNet is shown in Fig. 2.5.

16
2
Binary Neural Networks
Fig. 2.5 BinaryDenseNet 
MeliusNet [4] introduces a novel architecture that utilizes alternating Dense-
Blocks to increase the feature capacity. Additionally, they propose an Improvement-
Block to enhance the quality of the features. This approach enables 1-bit CNNs to 
achieve accuracy comparable to the popular compact network MobileNet-v1 while 
maintaining similar model size, number of operations, and accuracy. The building 
blocks of MeliusNet are shown in Fig. 2.6. 
Group-Net [81] is another approach that enhances the performance of 1-bit 
CNNs through structural design. The inspiration behind Group-Net comes from 
the idea of a ﬁxed number of binary digits representing a ﬂoating-point number in 
a computer. Group-Net proposes a novel approach to decompose a network into 
binary structures while ensuring that its representability is preserved. Instead of 
directly quantizing the network via “value decomposition,” Group-Net leverages 
this structured approach. 
Bulat et al. [9] were pioneers in exploring the impact of neural network bina-
rization on localization tasks, such as human pose estimation and face alignment. 
They introduced a novel hierarchical, parallel, and multiscale residual architecture 
that leads to remarkable performance improvements over the standard bottleneck 
block, all while keeping the number of parameters unchanged. This achievement 
effectively bridges the gap between the original network and its binarized version. 
The new architecture introduced by Bulat et al. enhances the size of the receptive 
ﬁeld, which enables the network to capture more context from the input data. 
Additionally, it improves the gradient ﬂow within the network, leading to more 
efﬁcient and effective learning. 
LightNN [15] is a novel model that replaces multiplications in traditional neural 
networks with efﬁcient shift and add operations. This innovative approach forms a 
new kind of model that signiﬁcantly reduces the computational complexity while 
maintaining high accuracy. 
In this section, we have discussed several works that modify the structure of 
binary neural networks, leading to improved performance and convergence. XNOR-
Net and Bi-Real Net make subtle adjustments to the original networks to enhance

2.5
Loss Design
17
Fig. 2.6 Building blocks of MeliusNet (c denotes the number of channels in the feature map) 
their representation capacity. On the other hand, MCN introduces new ﬁlters and 
convolutional operations to improve the overall accuracy of the network. Moreover, 
the loss function is also adapted to incorporate the new ﬁlters, which will be further 
elaborated in Sect. 2.5. 
2.5 
Loss Design 
In binary neural networks (BNNs), the loss function plays a crucial role in 
estimating the difference between the actual and predicted values of the model. 
While classical loss functions like least squares loss and cross-entropy loss are 
commonly used in standard neural networks for classiﬁcation and regression tasks, 
speciﬁc loss functions have been developed to suit the unique requirements of 
BNNs. 
In MCNs [61], a novel loss function is introduced, which combines three 
components: ﬁlter loss, center loss, and softmax loss, in an end-to-end framework. 
The overall loss function in MCNs is composed of two main parts: 
upper L equals upper L Subscript upper M Baseline plus upper L Subscript upper S Baseline periodL = LM + LS.
(2.13)

18
2
Binary Neural Networks
The ﬁrst part upper L Subscript upper MLM is: 
upper L Subscript upper M Baseline equals StartFraction theta Over 2 EndFraction sigma summation Underscript i comma l Endscripts double vertical bar upper C Subscript i Superscript l Baseline minus ModifyingAbove upper C Subscript i Superscript l Baseline With caret ring upper M Superscript l Baseline double vertical bar squared plus StartFraction lamda Over 2 EndFraction sigma summation Underscript m Endscripts double vertical bar f Subscript m Baseline left parenthesis ModifyingAbove upper C With caret comma bold italic upper M right parenthesis minus ModifyingAbove f With bar left parenthesis ModifyingAbove upper C With caret comma bold italic upper M right parenthesis double vertical bar squared commaLM = θ
2
Σ
i,l
||||Cl
i −ˆCl
i ◦Ml||||2 + λ
2
Σ
m
||||fm( ˆC, M) −¯f ( ˆC, M)
||||2,
(2.14) 
where C is the full-precision weights, ModifyingAbove upper C With caretˆC is the binarized weights, M is the M-Filters 
deﬁned in Sect. 4.5.3, f Subscript mfm denotes the feature map of the last convolutional layer 
for the mth sample, and f overbar ¯f denotes the class-speciﬁc mean feature map of previous 
samples. The ﬁrst entry of upper L Subscript upper MLM represents the ﬁlter loss, while the second entry 
calculates the center loss using a conventional loss function, such as the softmax 
loss. 
In PCNNs (projection convolutional neural networks) [19], a novel projection 
loss is introduced for discrete back propagation. It deﬁnes the quantization of the 
input variable as a projection onto a set, enabling the use of a projection loss for 
optimization. 
BONNs (Bayesian-optimized 1-bit CNNs) [77] propose a Bayesian-optimized 
1-bit CNN model, aiming to signiﬁcantly improve the performance of 1-bit CNNs. 
BONNs incorporate prior distributions of full-precision kernels, features, and 
ﬁlters into a Bayesian framework to construct 1-bit CNNs comprehensively in an 
end-to-end manner. In BONNs, the quantization error is denoted as y, and the 
full-precision weights as x. To minimize the reconstructed error, they maximize 
p left parenthesis x vertical bar y right parenthesisp(x|y), optimizing x for quantization. This optimization problem can be converted 
to a maximum a posteriori (MAP) since the distribution of x is known. For feature 
quantization, a similar method is employed; the Bayesian loss is as follows: 
StartLayout 1st Row upper L Subscript upper B Baseline equals StartFraction lamda Over 2 EndFraction sigma summation Underscript l equals 1 Overscript l Endscripts sigma summation Underscript i equals 1 Overscript upper C Subscript o Superscript l Baseline Endscripts sigma summation Underscript n equals 1 Overscript upper C Subscript i Superscript l Baseline Endscripts left brace double vertical bar ModifyingAbove k With caret Subscript n Superscript l comma i Baseline minus w Superscript l Baseline ring k Subscript n Superscript l comma i Baseline double vertical bar Subscript 2 Superscript 2 Baseline 2nd Row plus v left parenthesis k Subscript n plus Superscript l comma i Baseline minus mu Subscript i plus Superscript l Baseline right parenthesis Superscript upper T Baseline left parenthesis normal upper Psi Subscript i plus Superscript l Baseline right parenthesis Superscript negative 1 Baseline left parenthesis k Subscript n plus Superscript l comma i Baseline minus mu Subscript i plus Superscript l Baseline right parenthesis 3rd Row plus v left parenthesis k Subscript n minus Superscript l comma i Baseline minus mu Subscript i minus Superscript l Baseline right parenthesis Superscript upper T Baseline left parenthesis normal upper Psi Subscript i minus Superscript l Baseline right parenthesis Superscript negative 1 Baseline left parenthesis k Subscript n minus Superscript l comma i Baseline minus mu Subscript i minus Superscript l Baseline right parenthesis 4th Row v l o g left parenthesis d e t left parenthesis normal upper Psi Superscript l Baseline right parenthesis right parenthesis right brace plus StartFraction theta Over 2 EndFraction sigma summation Underscript m equals 1 Overscript upper M Endscripts left brace double vertical bar f Subscript m Baseline minus c Subscript m Baseline double vertical bar squared 5th Row plus sigma summation Underscript n equals 1 Overscript upper N Subscript f Baseline Endscripts left bracket sigma Subscript m comma n Superscript negative 2 Baseline left parenthesis f Subscript m comma n Baseline minus c Subscript m comma n Baseline right parenthesis squared plus l o g left parenthesis sigma Subscript m comma n Superscript 2 Baseline right parenthesis right bracket right brace comma EndLayoutLB = λ
2
lΣ
l=1
Cl
o
Σ
i=1
Cl
i
Σ
n=1
{
||||ˆkl,i
n −wl ◦kl,i
n
||||2
2
+ v(kl,i
n+ −μl
i+)T (Ψ l
i+)−1(kl,i
n+ −μl
i+)
+ v(kl,i
n−−μl
i−)T (Ψ l
i−)−1(kl,i
n−−μl
i−)
vlog(det(Ψ l))} + θ
2
M
Σ
m=1
{
||||fm −cm
||||2
+
Nf
Σ
n=1
[
σ −2
m,n(fm,n −cm,n)2 + log(σ 2
m,n)
]
},
(2.15) 
where k is the full-precision kernels, w is the reconstructed matrix, v is the variance 
of y, muμ is the mean of the kernels, normal upper PsiΨ is the covariance of the kernels, f Subscript mfm are the 
features of class m, and c is the mean of f Subscript mfm. 
In the work by Zheng et al. [78], they introduce a novel quantization loss 
that measures the discrepancy between binary weights and learned real values. 
The theoretical analysis provided by Zheng et al. demonstrates the importance of

2.6
Optimization
19
minimizing this weight quantization loss to enhance the performance of binarized 
neural networks. On the other hand, Ding et al. [14] propose the use of a distribution 
loss to explicitly regulate the activation ﬂow within the network. They develop a 
systematic framework to formulate this distribution loss, which helps in guiding 
the training process effectively. The empirical results from Ding et al.’s work 
illustrate that their proposed distribution loss is robust in terms of selecting training 
hyperparameters. 
These methods all aim to minimize the error and information loss of quantization, 
which improves the compactness and capacity of 1-bit CNNs. 
2.6 
Optimization 
Absolutely, researchers have been actively seeking new training methods to enhance 
the performance of binary neural networks (BNNs) and overcome their inherent 
limitations. These methods are aimed at improving the effectiveness of BNNs 
across various tasks and applications. One approach involves integrating techniques 
from other ﬁelds into BNNs. By borrowing insights and methods from different 
domains, researchers aim to augment the capabilities and performance of BNNs. 
This cross-disciplinary approach allows for innovative solutions that can address 
speciﬁc challenges faced by binary networks. Moreover, improving the training 
process is a key focus for enhancing BNNs. Researchers are exploring modiﬁcations 
to the optimization algorithms used in classical BNNs. These adaptations target 
the optimization process to achieve better convergence, stability, and overall 
performance. 
The work by Sari et al.  [60] sheds light on the importance of the BatchNorm 
layer in the training process of binary neural networks (BNNs). They demonstrate 
that BatchNorm plays a crucial role in preventing exploding gradients, which 
can be a signiﬁcant issue in BNNs due to the binary nature of the weights. 
Their ﬁndings also suggest that the standard initialization methods commonly 
used in full-precision networks may not be suitable for BNNs, highlighting the 
need for specialized techniques to handle weight initialization in binary networks. 
Additionally, they provide insights into the components of BatchNorm, showing 
that only minibatch centering is necessary, which can simplify the implementation 
of BatchNorm in BNNs. On the other hand, the experiments conducted by Alizadeh 
et al. [1] offer valuable empirical evidence regarding common tricks used in binary 
training models. They show that techniques like gradient and weight clipping, often 
employed to stabilize training in BNNs, are primarily needed during the ﬁnal stages 
of training to achieve the best performance. 
XNOR-Net++ [10] presents an innovative training algorithm for 1-bit convo-
lutional neural networks (CNNs), building upon the foundation of XNOR-Net. In 
XNOR-Net++, the authors introduce a novel approach to combine activation and 
weight scaling factors into a single scalar, which is learned discriminatively through 
back propagation. By unifying these scaling factors, the method aims to streamline

20
2
Binary Neural Networks
the training process and enhance the efﬁciency of 1-bit CNNs. Additionally, XNOR-
Net++ explores various strategies to construct the shape of the scale factors while 
ensuring that the computational budget remains ﬁxed. 
The work by Leng et al. [35] draws inspiration from the alternating direction 
method of multipliers (ADMM) to address the challenges of training binary neural 
networks. By leveraging the principles of ADMM, they propose a novel approach 
to decouple the continuous parameters from the discrete constraints in the network. 
This decoupling allows them to break down the original complex optimization 
problem into several subproblems, each with its own set of constraints. To solve 
these subproblems efﬁciently, Leng et al. employ different gradient and iterative 
quantization algorithms. By doing so, they achieve considerably faster convergence 
rates compared to traditional optimization methods used in binary neural networks. 
In the work of deterministic binary ﬁlters (DBFs) [56], the researchers propose 
a novel approach to learn weighted coefﬁcients of predeﬁned orthogonal binary 
bases instead of directly learning the convolutional ﬁlters, as is typically done in 
conventional methods. DBFs generate ﬁlters by representing them as a linear com-
bination of orthogonal binary codes. These orthogonal binary bases are predeﬁned, 
and the learning process focuses on ﬁnding the optimal weighted coefﬁcients for 
these bases. By doing so, the ﬁlters can be efﬁciently generated in real time. 
BinaryRelax [75] presents a two-phase algorithm for training convolutional 
neural networks (CNNs) with quantized weights, including binary weights. The 
goal is to overcome the challenges posed by hard constraints on binary weights 
during training. In the ﬁrst phase, BinaryRelax relaxes the hard constraint of 
binary weights into a continuous regularizer using the Moreau envelope [48]. 
This regularization term is deﬁned as the squared Euclidean distance between the 
weights and the set of quantized weights. By gradually increasing the regularization 
parameter, BinaryRelax narrows the gap between the continuous weights and the 
quantized state, effectively transitioning toward a binary solution. In the second 
phase, BinaryRelax introduces the same quantization scheme but with a small 
learning rate. This guarantees that the weights eventually converge to fully quantized 
binary values. 
CBCNs [41] propose a novel approach to enhance the capacity of binarized 
convolutional features using circulant ﬁlters (CiFs) and circulant binary convolution 
(CBConv). CiFs are 4D tensors of size upper K times upper K times upper H times upper HK × K × H × H, generated by applying 
a circulant transfer matrix M to a learned ﬁlter. The matrix M rotates the learned 
ﬁlter at different angles, effectively expanding its representation capacity. To create 
a CiF, the original 2D upper H times upper HH × H learned ﬁlter is transformed into a 3D tensor by 
replicating it three times and concatenating them. This 3D tensor is then combined 
with the circulant transfer matrix M to form the 4D CiF. By utilizing circulant ﬁlters 
and circulant binary convolution, CBCNs can improve the representation capacity 
of binarized neural networks without altering the model size (Fig. 2.7). 
Rectiﬁed binary convolutional networks (RBCNs) [40] introduce a novel 
approach to train 1-bit binary networks using a generative adversarial network 
(GAN). The training process involves using the guidance of the corresponding 
full-precision model, which leads to signiﬁcant performance improvements in

2.6
Optimization
21
Fig. 2.7 The generation of CiF 
1-bit CNNs. The key innovation in RBCNs is the incorporation of rectiﬁed 
convolutional layers, which are designed to be generic and ﬂexible. These layers 
can be easily integrated into existing deep convolutional neural networks (DCNNs) 
like WideResNets and ResNets. 
Martinez et al. [45] focus on minimizing the discrepancy between the binary 
output and the corresponding real-valued convolution in 1-bit CNNs. They propose 
a real-to-binary attention matching approach that is tailored for training these 
networks. Additionally, they introduce a progressive bridging strategy to reduce the 
architectural gap between real and binary networks through a sequence of teacher-
student pairs. 
In contrast, Bethge et al. [5] take a different approach by directly training a 
binary network from scratch, without relying on pre-trained full-precision models 
or other standard methods. Their training implementation is based on the BMXNet 
framework [74]. 
Helwegen et al. [22] highlight that latent weights with real values in binary neural 
networks serve a different purpose compared to weights in real-valued networks. 
They propose the binary optimizer (Bop), speciﬁcally designed for BNNs, to handle 
the unique characteristics of binary weights effectively during the optimization 
process. 
BinaryDuo [30] presents a novel training scheme for binary activation networks 
by coupling two binary activations into a ternary activation during training. They 
achieve this by ﬁrst decoupling a ternary activation into two binary activations, 
effectively doubling the number of weights. However, to maintain the parameter 
size of the decoupled model and the baseline model, they reduce the coupled ternary 
model. The independent update of each weight after decoupling allows for better 
optimization, as the two weights no longer share the same value. 
BENN [80] leverages classical ensemble methods to enhance the performance 
of 1-bit CNNs. While ensemble techniques were traditionally believed to have 
limited impact on robust classiﬁers like deep neural networks, BENN’s analysis 
and experiments demonstrate that ensembles are exceptionally effective in boosting 
BNNs. The ensemble strategies used in BENN draw from various works such as 
[7, 11, 49].

22
2
Binary Neural Networks
Table 2.1 Experimental results of some famous binary methods on ImageNet 
Full-precision 
Binarized accuracy 
accuracy 
Methods
Weights 
Activations 
Model
Top 1 
Top 5
Top 1 
Top 5 
XNOR-Net [53]
Binary 
Binary
ResNet-18 
51.2
73.2
69.3 
89.2 
ABC-Net [38]
Binary 
Binary
ResNet-50 
70.1
89.7
76.1 
92.8 
LBCNN [27]
Binary 
–
–
62.43a 
–
64.94 
– 
Bi-Real Net [44]
Binary 
Binary
ResNet-34 
62.2
83.9
73.3 
91.3 
RBCN [40]
Binary 
Binary
ResNet-18 
59.5
81.6
69.3 
89.2 
BinaryDenseNet 
[6] 
–
–
–
62.5
83.9
–
– 
Superscript aa 13times×13 Filter 
TentacleNet [47] builds on the theory of ensemble learning and makes further 
advancements beyond BENN. TentacleNet demonstrates that binary ensembles can 
achieve high accuracy while requiring fewer computational resources. 
BayesBiNN [46] adopts a principled approach to discrete optimization by using a 
distribution over the binary variable. They introduce a Bayesian learning rule [29] to  
estimate a Bernoulli approximation to the posterior, resulting in a principled method 
for dealing with binary neural networks (Table 2.1). 
2.7 
Algorithms for Binary Neural Networks 
Binarization, the most extreme form of quantization, is the main focus of this book. 
It involves representing data using only one bit, either minus−1 (or 0) or +1, resulting in 
1-bit quantization. Both weights and activations in a binary neural network can be 
compressed into a single bit, leading to signiﬁcant memory savings and hardware-
friendly advantages, such as faster execution, reduced memory consumption, and 
improved power efﬁciency. Groundbreaking works like BNN [25] and XNOR-Net 
[53] have demonstrated the effectiveness of binarization, with XNOR-Net achieving 
up to 58. 
Since the advent of binary neural networks, extensive research has been con-
ducted in computer vision and machine learning ﬁelds [21, 42, 54], leading to their 
application in various tasks, including image classiﬁcation [12, 44, 51, 53, 66, 73], 
object detection [64, 67, 70, 71], point cloud processing [50, 68], object reiden-
tiﬁcation [69], and more. Binarization’s hardware-friendly beneﬁts and practical 
applications have made it a promising area of research in recent years. 
Binarizing a layer in a neural network helps identify its signiﬁcance and impact 
on performance. If performance suffers after binarization, the layer is crucial for the 
network. This process aids explainable machine learning and veriﬁes if binarization 
preserves essential information. Understanding binarized models contributes to 
improving binary neural networks.

2.7
Algorithms for Binary Neural Networks
23
Researchers have extensively studied model binarization to understand its behav-
iors and its relationship with the architecture of deep neural networks. Exploring 
binary neural networks helps answer fundamental questions about network topology 
and deep network functionality. Thorough exploration of binary neural network 
studies contributes to a better understanding of effective and reliable deep learning 
models. Notable works, like Bi-Real Net [44], have revealed how components 
in binary neural networks function, such as incorporating shortcuts to mitigate 
information loss due to binarization. 
The structure of shortcuts in binary neural networks, similar to ResNet shortcuts, 
allows for better information ﬂow between shallow and deep layers during both 
forward and backward propagation. This mechanism helps in avoiding issues like 
gradient disappearance and improves the overall performance of the network. 
Ensemble approaches in binary neural networks, like building weak classiﬁer 
groups, can lead to performance improvements. However, they may also encounter 
overﬁtting problems. Understanding the trade-off between the number of neurons 
and the bit width is essential, as it can inﬂuence the network’s performance. 
Interestingly, real-valued neurons may not be necessary in deep neural networks, 
aligning with the idea of biological neural networks. Reducing the bit width of 
speciﬁc layers can be an efﬁcient method to examine the interpretability of deep 
neural networks. Investigating how sensitive different layers are to binarization is 
crucial in designing effective binary neural networks. Typically, the ﬁrst and last 
layers in binary neural networks should be kept at higher precision since they play 
a more critical role in predicting the network’s output. This section attempts to state 
the nature of binary neural networks by introducing some representative work. 
2.7.1 
BNN: Binary Neural Network 
Given an N-layer CNN model, we denote its weight set as bold upper W equals left brace bold w Superscript n Baseline right brace Subscript n equals 1 Superscript upper NW = {wn}N
n=1 and the 
input feature map set as bold upper A equals left brace bold a Subscript i n Superscript n Baseline right brace Subscript n equals 1 Superscript upper NA = {an
in}N
n=1. The  bold w Superscript n Baseline element of double struck upper R Superscript upper C Super Subscript o u t Super Superscript n Superscript times upper C Super Subscript i n Super Superscript n Superscript times upper K Super Superscript n Superscript times upper K Super Superscript nwn ∈RCn
out×Cn
in×Kn×Kn and bold a Subscript i n Superscript n Baseline element of double struck upper R Superscript upper C Super Subscript i n Super Superscript n Superscript times upper W Super Subscript i n Super Superscript n Superscript times upper H Super Subscript i n Super Superscript nan
in ∈
RCn
in×W n
in×H n
in are the convolutional weight and the input feature map in the n-th 
layer, where upper C Subscript i n Superscript nCn
in, upper C Subscript o u t Superscript nCn
out, and upper K Superscript nKn, respectively, represent the input channel number, 
the output channel number, and the kernel size. In addition, upper W Subscript i n Superscript nW n
in and upper H Subscript i n Superscript nH n
in are the 
width and height of the feature maps. Then, the convolutional outputs bold a Subscript o u t Superscript nan
out can be 
technically formulated as: 
bold a Subscript o u t Superscript n Baseline equals bold w Superscript n Baseline circled times bold a Subscript i n Superscript n Baseline commaan
out = wn ⊗an
in,
(2.16) 
where circled times⊗represents the convolution operation. In this book, we omit the nonlinear 
function for simplicity. Following the prior works [12, 25], binary neural network 
(BNN) intends to represent bold w Superscript nwn and bold a Superscript nan in a binary discrete set as: 
StartLayout 1st Row double struck upper B colon equals StartSet minus 1 left parenthesis 0 right parenthesis comma plus 1 EndSet period EndLayoutB := {−1(0), +1}.

24
2
Binary Neural Networks
Thus, the 1-bit format of bold w Superscript nwn and bold a Superscript nan is respectively bold b Superscript bold w Super Superscript n Baseline element of double struck upper B Superscript upper C Super Subscript o u t Super Superscript n Superscript times upper C Super Subscript i n Super Superscript n Superscript times upper K Super Superscript n Superscript times upper K Super Superscript nbwn ∈BCn
out×Cn
in×Kn×Kn and 
bold b Superscript bold a Super Subscript i n Super Superscript n Baseline element of double struck upper B Superscript upper C Super Subscript i n Super Superscript n Superscript times upper W Super Subscript i n Super Superscript n Superscript times upper H Super Subscript i n Super Superscript nban
in ∈BCn
in×W n
in×H n
in such that the efﬁcient XNOR and bit-count instructions can 
approximate the ﬂoating-point convolutional outputs as: 
bold a Subscript o u t Superscript n Baseline almost equals bold b Superscript bold w Super Superscript n Superscript Baseline circled dot bold b Superscript bold a Super Subscript i n Super Superscript n Superscript Baseline commaan
out ≈bwn O ban
in,
(2.17) 
where ring◦represents channel-wise multiplication and circled dotO denotes XNOR and bit-count 
instructions. 
However, this quantization mode will cause the output amplitude to increase 
dramatically, different from the full-precision convolution calculation, and cause 
the homogenization of characteristics [53]. Several novel objects are proposed to 
address this issue, which will be introduced in the following. 
2.7.2 
XNOR-Net: ImageNet Classiﬁcation Using Binary 
Convolutional Neural Networks 
The scaling factor was ﬁrst proposed by XNOR-Net [53] to solve this problem. The 
weights and the inputs to the convolutional and fully connected layers in XNOR-
Nets are approximated with binary values double struck upper BB. 
The XNOR-Net binarization approach seeks to identify the most accurate 
convolutional approximations. Speciﬁcally, XNOR-Net employs a scaling factor, 
which plays a vital role in the learning of BNNs, and improves the forward pass of 
BNNs as: 
bold a Subscript o u t Superscript n Baseline almost equals bold italic alpha Superscript n Baseline ring left parenthesis bold b Superscript bold w Super Superscript n Superscript Baseline circled dot bold b Superscript bold a Super Subscript i n Super Superscript n Superscript Baseline right parenthesis commaan
out ≈αn ◦(bwn O ban
in),
(2.18) 
where bold italic alpha Superscript n Baseline equals StartSet alpha 1 Superscript n Baseline comma alpha 2 Superscript n Baseline comma ellipsis comma alpha Subscript upper C Sub Subscript o u t Sub Superscript n Subscript Superscript n Baseline EndSet element of double struck upper R Subscript plus Superscript upper C Super Subscript o u t Super Superscript nαn = {αn
1, αn
2, . . . , αn
Cn
out } ∈RCn
out
+
is known as the channel-wise scaling 
factor vector to mitigate the output gap between Eq. 2.16 and its approximation of 
Eq. 2.18. We denote script upper A equals left brace bold italic alpha Superscript n Baseline right brace Subscript n equals 1 Superscript upper NA = {αn}N
n=1. Since the weight values are binary, XNOR-Net 
can implement the convolution with additions and subtractions. In the following, 
we state the XNOR operation for a speciﬁc convolution layer, thus omitting the 
superscript n for simplicity. Most existing implementations simply follow earlier 
studies [44, 53]to optimize script upper AA based on nonparametric optimization as: 
bold italic alpha Superscript asterisk Baseline comma bold b Superscript bold w Baseline Superscript asterisk Baseline equals arg min Subscript bold italic alpha comma bold b Sub Superscript bold w Subscript Baseline upper J left parenthesis bold italic alpha comma bold b Superscript bold w Baseline right parenthesis commaα∗, bw∗= arg minα,bwJ(α, bw),
(2.19) 
upper J left parenthesis bold italic alpha comma bold b Superscript bold w Baseline right parenthesis equals parallel to bold w minus bold italic alpha Superscript n Baseline ring bold b Superscript bold w Baseline parallel to Subscript 2 Superscript 2 Baseline periodJ(α, bw) = ||w −αn ◦bw||2
2.
(2.20) 
By expanding Eq. 2.20, we have:  
upper J left parenthesis bold italic alpha comma bold b Superscript bold w Baseline right parenthesis equals bold italic alpha squared left parenthesis bold b Superscript bold w Baseline right parenthesis Superscript sans serif upper T Baseline bold b Superscript bold w Baseline minus 2 bold italic alpha ring bold w Superscript sans serif upper T Baseline bold b Superscript bold w Baseline plus bold w Superscript sans serif upper T Baseline bold wJ(α, bw) = α2(bw)Tbw −2α ◦wTbw + wTw
(2.21)

2.7
Algorithms for Binary Neural Networks
25
where bold b Superscript bold w Baseline element of double struck upper Bbw ∈B. Thus, left parenthesis bold b Superscript bold w Baseline right parenthesis Superscript sans serif upper T Baseline bold b Superscript bold w Baseline equals upper C Subscript i n Baseline times upper K times upper K(bw)Tbw = Cin × K × K. bold w Superscript sans serif upper T Baseline bold wwTw is also a constant due to bold ww
being a known variable. Thus, Eq. 2.21 can be rewritten as: 
upper J left parenthesis bold italic alpha comma bold b Superscript bold w Baseline right parenthesis equals bold italic alpha squared times upper C Subscript i n Baseline times upper K times upper K minus 2 bold italic alpha ring bold w Superscript sans serif upper T Baseline bold b Superscript bold w Baseline plus c o n s t a n t periodJ(α, bw) = α2 × Cin × K × K −2α ◦wTbw + constant.
(2.22) 
The optimal solution can be achieved by maximizing the following constrained 
optimization: 
bold b Superscript bold w Baseline Superscript asterisk Baseline equals arg max Underscript bold b Superscript bold w Baseline Endscripts bold w Superscript sans serif upper T Baseline bold b Superscript bold w Baseline comma s period t period bold b Superscript bold w Baseline element of double struck upper B commabw∗= arg max
bw
wTbw, s.t.
bw ∈B,
(2.23) 
which can be solved by the sign function: 
StartLayout 1st Row StartLayout 1st Row bold b Superscript bold w Super Subscript i Baseline equals StartLayout Enlarged left brace 1st Row 1st Column plus 1 2nd Column bold w Subscript i Baseline greater than or equals 0 2nd Row 1st Column negative 1 2nd Column bold w Subscript i Baseline less than 0 EndLayout EndLayout EndLayoutbwi =
{ +1 wi ≥0
−1 wi < 0
which is the optimal solution and is also widely used as a general solution to BNNs 
in the following numerous works [44]. To ﬁnd the optimal value for the scaling 
factor bold italic alpha Superscript asteriskα∗, we take the derivative of upper J left parenthesis dot right parenthesisJ(·) w.r.t. bold italic alphaα and set it to zero as: 
bold italic alpha Superscript asterisk Baseline equals StartFraction bold w Superscript sans serif upper T Baseline bold b Superscript bold w Baseline Over upper C Subscript i n Superscript n Baseline times upper K Superscript n Baseline times upper K Superscript n Baseline EndFraction periodα∗=
wTbw
Cn
in × Kn × Kn .
(2.24) 
By replacing bold b Superscript bold wbw with the sign function, we have that a closed-form solution of bold italic alphaα
can be derived via the channel-wise absolute mean (CAM) as: 
bold italic alpha Subscript i Baseline equals StartFraction parallel to bold w Subscript i comma colon comma colon comma colon Baseline parallel to Subscript 1 Baseline Over upper C Subscript i n Baseline times upper K times upper K EndFractionαi =
||wi,:,:,:||1
Cin × K × K
(2.25) 
bold italic alpha Subscript i Baseline equals StartFraction parallel to bold w Subscript i comma colon comma colon comma colon Baseline parallel to Subscript 1 Baseline Over upper M EndFractionαi = ||wi,:,:,:||1
M
. Therefore, the optimal estimation of a binary weight ﬁlter can be 
achieved simply by taking the sign of weight values. The optimal scaling factor is 
the average of the absolute weight values. 
Based on the explicitly solved bold italic alpha Superscript asteriskα∗, the training objective of the XNOR-Net-like 
BNNs is given in a bilevel form: 
StartLayout 1st Row 1st Column Blank 2nd Column bold upper W Superscript asterisk Baseline equals arg min Underscript bold upper W Endscripts script upper L left parenthesis bold upper W semicolon script upper A Superscript asterisk Baseline right parenthesis comma 2nd Row 1st Column Blank 2nd Column s period t period arg min Underscript bold italic alpha Superscript n Baseline comma bold b Superscript bold w Super Superscript n Superscript Baseline Endscripts upper J left parenthesis bold italic alpha comma bold b Superscript bold w Baseline right parenthesis comma EndLayout
W∗= arg min
W
L(W; A∗),
s.t. arg min
αn,bwn J(α, bw),
(2.26) 
which is also known as hard binarization [44]. In the following, we show some 
variants of such a binarization function.

26
2
Binary Neural Networks
2.7.3 
SA-BNN: State-Aware Binary Neural Network 
Binary neural networks (BNNs) have received much attention due to their memory 
and computation efﬁciency. However, the sizable performance gap between BNNs 
and their full-precision counterparts hinders BNNs from being deployed in resource-
constrained platforms. The challenge of the performance drop instinctively comes 
from the minimal binarization states StartSet negative 1 comma 1 EndSet{−1, 1}, which would bring many propagation 
errors in forward and backward procedures and lead to misleading weight update. 
We want to suggest a method to make the training more efﬁcient by suppressing 
the ﬂuctuation of the weight update. Speciﬁcally, we ﬁnd that existing methods 
[44, 53] possess the identical gradient amplitude for all quantization states StartSet negative 1 comma 1 EndSet{−1, 1}. 
According to our analysis, the frequent weight ﬂip is more likely to happen in this 
case. The intuition here is about “whether we can calibrate the amplitude of the two 
states slightly distinctive to make their chance of weight ﬂip different to increase 
the difﬁculty of frequent weight ﬂip further?” Inspired by this, a novel state-aware 
binary neural network (SA-BNN) [39] equipped with a well-designed state-aware 
gradient is proposed in this paper. Expressly, we set separate learnable gradient 
coefﬁcients for different states. In this way, the unnecessary weight update can 
be impeded efﬁciently. Besides, we lead to a theorem that the state-aware gradient 
can effectively mitigate the frequent weight ﬂip problem, alleviating the ineffective 
update issue in BNN optimization. 
2.7.3.1 
Method 
To suppress the frequent weight ﬂips in BNNs, we propose the following state-aware 
gradient to stabilize the optimization: 
StartFraction partial differential script upper L Over partial differential x EndFraction equals StartLayout Enlarged left brace 1st Row 1st Column StartFraction partial differential script upper L Over partial differential ModifyingAbove x With caret EndFraction left parenthesis tau Subscript negative 1 Baseline StartFraction partial differential ModifyingAbove x With caret Over partial differential x EndFraction right parenthesis 2nd Column i f ModifyingAbove x With caret equals negative 1 2nd Row 1st Column StartFraction partial differential script upper L Over partial differential ModifyingAbove x With caret EndFraction left parenthesis tau 1 StartFraction partial differential ModifyingAbove x With caret Over partial differential x EndFraction right parenthesis 2nd Column normal o normal t normal h normal e normal r normal w normal i normal s normal e EndLayout comma∂L
∂x =
{
∂L
∂ˆx (τ−1 ∂ˆx
∂x ) if ˆx = −1
∂L
∂ˆx (τ1 ∂ˆx
∂x )
otherwise ,
(2.27) 
where tau Subscript negative 1 Baseline comma tau 1 element of double struck upper Rτ−1, τ1 ∈R are learnable coefﬁcients, which are introduced on the activation 
gradients to distinctively treat the two states. We do not apply the distinguishable 
parameters tau equals StartSet tau Subscript negative 1 Baseline comma tau 1 EndSetτ = {τ−1, τ1} on the weight gradient (StartFraction partial differential script upper L Over partial differential w EndFraction∂L
∂w ), since the weights themselves 
are learnable in the training process. It is equivalent to regard the state-aware 
coefﬁcients tauτ and the weights as a whole. Therefore, we do not consider the 
state-aware gradient on the weights and instead focus on that on activation in 
the following. According to Eq. 2.27, we leverage an extra scale factor on the 
activation gradients for each binarization state to impose a mild constraint on the 
weight updating. When the two scale factors are equal (tau Subscript negative 1 Baseline equals tau 1τ−1 = τ1) , it reduces 
to the traditional weight updating with state-consistent gradients. Otherwise, it is 
the proposed state-aware gradient-based BNNs. Next, we analyze the difference 
between these two mechanisms.

2.7
Algorithms for Binary Neural Networks
27
Proposition 2.1 The state-aware gradients (StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue not equals StartAbsoluteValue tau 1 EndAbsoluteValue|τ−1| /= |τ1|) can suppress frequent 
weight ﬂip effectively compared with the corresponding state-consistent gradients 
(StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue equals StartAbsoluteValue tau 1 EndAbsoluteValue|τ−1| = |τ1|), leading to more stable training. 
Based on the gradient chain rule, the weight-updating procedure can be described 
as: 
StartLayout 1st Row StartLayout 1st Row 1st Column w Superscript l comma t plus 1 Baseline equals w Superscript l comma t Baseline minus eta StartFraction partial differential script upper L Over partial differential w Superscript l comma t Baseline EndFraction 2nd Column equals w Superscript l comma t Baseline minus eta StartFraction partial differential script upper L Over partial differential ModifyingAbove x With caret Superscript l plus 1 comma t Baseline EndFraction left parenthesis tau Superscript l plus 1 comma t Baseline StartFraction partial differential ModifyingAbove x With caret Superscript l plus 1 comma t Baseline Over partial differential x Superscript l plus 1 comma t Baseline EndFraction right parenthesis StartFraction partial differential x Superscript l plus 1 comma t Baseline Over partial differential ModifyingAbove w With caret Superscript l comma t Baseline EndFraction StartFraction partial differential ModifyingAbove w With caret Superscript l comma t Baseline Over partial differential w Superscript l comma t Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals w Superscript l comma t Baseline minus eta StartFraction partial differential script upper L Over partial differential ModifyingAbove x With caret Superscript l plus 1 comma t Baseline EndFraction left parenthesis tau Superscript l plus 1 comma t Baseline StartFraction partial differential ModifyingAbove x With caret Superscript l plus 1 comma t Baseline Over partial differential x Superscript l plus 1 comma t Baseline EndFraction right parenthesis ModifyingAbove x With caret Superscript l comma t Baseline StartFraction partial differential ModifyingAbove w With caret Superscript l comma t Baseline Over partial differential w Superscript l comma t Baseline EndFraction 3rd Row 1st Column Blank 2nd Column equals w Superscript l comma t Baseline minus tau Superscript l plus 1 comma t Baseline b Superscript l comma t Baseline comma EndLayout EndLayout
wl,t+1 = wl,t −η ∂L
∂wl,t = wl,t −η ∂L
∂ˆxl+1,t (τ l+1,t ∂ˆxl+1,t
∂xl+1,t )∂xl+1,t
∂ˆwl,t
∂ˆwl,t
∂wl,t
= wl,t −η ∂L
∂ˆxl+1,t (τ l+1,t ∂ˆxl+1,t
∂xl+1,t )ˆxl,t ∂ˆwl,t
∂wl,t
= wl,t −τ l+1,tbl,t,
(2.28) 
where etaη is the learning rate, t represents the t-th iteration, and 
b Superscript l comma t Baseline equals eta StartFraction partial differential script upper L Over partial differential ModifyingAbove x With caret Superscript l plus 1 comma t Baseline EndFraction StartFraction partial differential ModifyingAbove x With caret Superscript l plus 1 comma t Baseline Over partial differential x Superscript l plus 1 comma t Baseline EndFraction ModifyingAbove x With caret Superscript l comma t Baseline StartFraction partial differential ModifyingAbove w With caret Superscript l comma t Baseline Over partial differential w Superscript l comma t Baseline EndFraction periodbl,t = η ∂L
∂ˆxl+1,t
∂ˆxl+1,t
∂xl+1,t ˆxl,t ∂ˆwl,t
∂wl,t .
For simplicity, we ignore the layer index superscript l in the following analysis. 
According to Eq. 2.28, to enable a weight ﬂip (namely, let s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis not equals s i g n left parenthesis w Superscript t Baseline right parenthesissign(wt+1) /= sign(wt)), 
it requires to satisfy the constraints s i g n left parenthesis tau Superscript t Baseline b Superscript t Baseline right parenthesis equals s i g n left parenthesis w Superscript t Baseline right parenthesissign(τ tbt) = sign(wt) and StartAbsoluteValue tau Superscript t Baseline b Superscript t Baseline EndAbsoluteValue greater than StartAbsoluteValue w Superscript t Baseline EndAbsoluteValue|τ tbt| > |wt|, where 
StartAbsoluteValue dot EndAbsoluteValue|·| represents the amplitude of the input. We assume the initial state s i g n left parenthesis w Superscript t Baseline right parenthesis equals negative 1sign(wt) = −1, 
and the process is similar for the initial state s i g n left parenthesis w Superscript t Baseline right parenthesis equals 1sign(wt) = 1. 
1. If StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue equals StartAbsoluteValue tau 1 EndAbsoluteValue|τ−1| = |τ1|, the ﬂip probability from the iteration t to t plus 1t + 1 is: 
upper P left parenthesis s i g n left parenthesis w Superscript t Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis right parenthesis equals upper N Subscript StartAbsoluteValue w Sub Superscript t Subscript EndAbsoluteValue Baseline divided by upper N commaP(sign(wt) /= sign(wt+1)) = N|wt|/N,
(2.29) 
where upper N Subscript StartAbsoluteValue w Sub Superscript t Subscript EndAbsoluteValueN|wt| represents the total number of b Superscript tbt satisfying s i g n left parenthesis tau 1 Superscript t Baseline b Superscript t Baseline right parenthesis equals s i g n left parenthesis w Superscript t Baseline right parenthesissign(τ t
1bt) = sign(wt)
and StartAbsoluteValue tau 1 Superscript t Baseline b Superscript t Baseline EndAbsoluteValue greater than StartAbsoluteValue w Superscript t Baseline EndAbsoluteValue|τ t
1bt| > |wt|, and N represents the total number of b. Similarly, the ﬂip 
probability from the iteration t plus 1t + 1 to t plus 2t + 2 is 
upper P left parenthesis s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 2 Baseline right parenthesis right parenthesis equals upper N Subscript StartAbsoluteValue w Sub Superscript t plus 1 Subscript EndAbsoluteValue Baseline divided by upper N commaP(sign(wt+1) /= sign(wt+2)) = N|wt+1|/N,
(2.30) 
where upper N Subscript StartAbsoluteValue w Sub Superscript t plus 1 Subscript EndAbsoluteValueN|wt+1| represents the total number of b Superscript t plus 1bt+1 satisfying s i g n left parenthesis tau Subscript negative 1 Superscript t plus 1 Baseline b Superscript t plus 1 Baseline right parenthesis equals s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesissign(τ t+1
−1 bt+1) =
sign(wt+1) and StartAbsoluteValue tau Subscript negative 1 Superscript t plus 1 Baseline b Superscript t plus 1 Baseline EndAbsoluteValue greater than StartAbsoluteValue w Superscript t plus 1 Baseline EndAbsoluteValue|τ t+1
−1 bt+1| > |wt+1|. Thus, the sequential ﬂip probability from 
the iteration t to t plus 2t + 2 is: 
StartLayout 1st Row 1st Column upper P left parenthesis left parenthesis s i g n left parenthesis w Superscript t Baseline right parenthesis 2nd Column not equals s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis right parenthesis intersection left parenthesis s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column not equals s i g n left parenthesis w Superscript t plus 2 Baseline right parenthesis right parenthesis right parenthesis equals left parenthesis upper N Subscript StartAbsoluteValue w Sub Superscript t Subscript EndAbsoluteValue Baseline upper N Subscript StartAbsoluteValue w Sub Superscript t plus 1 Subscript EndAbsoluteValue Baseline right parenthesis divided by upper N squared period EndLayoutP((sign(wt) /= sign(wt+1)) ∩(sign(wt+1)
/= sign(wt+2))) = (N|wt|N|wt+1|)/N2.
(2.31) 
2. If StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue less than StartAbsoluteValue tau 1 EndAbsoluteValue|τ−1| < |τ1|, it remains the same ﬂip probability from the iteration t to t plus 1t + 1
as Eq. 2.29. However, when considering the ﬂip probability from iteration t plus 1t + 1

28
2
Binary Neural Networks
to t plus 2t + 2, the number of b Superscript t plus 1bt+1 that satisfying StartAbsoluteValue tau Subscript negative 1 Superscript t plus 1 Baseline b Superscript t plus 1 Baseline EndAbsoluteValue greater than StartAbsoluteValue w Superscript t plus 1 Baseline EndAbsoluteValue|τ t+1
−1 bt+1| > |wt+1|, in this case, is 
less than that in the case of StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue equals StartAbsoluteValue tau 1 EndAbsoluteValue|τ−1| = |τ1|. 
Therefore, the state-aware gradient (i.e., StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue less than StartAbsoluteValue tau 1 EndAbsoluteValue|τ−1| < |τ1|) has a lower probability 
of sequential weight ﬂip compared with the conventional state-consistent meth-
ods (i.e., StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue equals StartAbsoluteValue tau 1 EndAbsoluteValue|τ−1| = |τ1|): 
StartLayout 1st Row StartLayout 1st Row 1st Column Blank 2nd Column upper P left parenthesis left parenthesis s i g n left parenthesis w Superscript t Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis right parenthesis intersection left parenthesis s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 2 Baseline right parenthesis right parenthesis StartAbsoluteValue EndAbsoluteValue tau Subscript negative 1 Baseline StartAbsoluteValue less than EndAbsoluteValue tau 1 vertical bar right parenthesis 2nd Row 1st Column Blank 2nd Column less than upper P left parenthesis left parenthesis s i g n left parenthesis w Superscript t Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis right parenthesis intersection left parenthesis s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 2 Baseline right parenthesis right parenthesis StartAbsoluteValue EndAbsoluteValue tau Subscript negative 1 Baseline StartAbsoluteValue equals EndAbsoluteValue tau 1 vertical bar right parenthesis period EndLayout EndLayout
P((sign(wt) /= sign(wt+1)) ∩(sign(wt+1) /= sign(wt+2))||τ−1| < |τ1|)
< P((sign(wt) /= sign(wt+1)) ∩(sign(wt+1) /= sign(wt+2))||τ−1| = |τ1|).
(2.32) 
3. If StartAbsoluteValue tau 1 EndAbsoluteValue less than StartAbsoluteValue tau Subscript negative 1 Baseline EndAbsoluteValue|τ1| < |τ−1|, the process is similar to 2). The state-aware gradient also has a 
lower probability of sequential weight ﬂip as: 
StartLayout 1st Row 1st Column Blank 2nd Column upper P left parenthesis left parenthesis s i g n left parenthesis w Superscript t Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis right parenthesis intersection left parenthesis s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 2 Baseline right parenthesis right parenthesis StartAbsoluteValue EndAbsoluteValue tau 1 StartAbsoluteValue less than EndAbsoluteValue tau Subscript negative 1 Baseline vertical bar right parenthesis 2nd Row 1st Column Blank 2nd Column less than upper P left parenthesis left parenthesis s i g n left parenthesis w Superscript t Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis right parenthesis intersection left parenthesis s i g n left parenthesis w Superscript t plus 1 Baseline right parenthesis not equals s i g n left parenthesis w Superscript t plus 2 Baseline right parenthesis right parenthesis StartAbsoluteValue EndAbsoluteValue tau Subscript negative 1 Baseline StartAbsoluteValue equals EndAbsoluteValue tau 1 vertical bar right parenthesis period EndLayout
P((sign(wt) /= sign(wt+1)) ∩(sign(wt+1) /= sign(wt+2))||τ1| < |τ−1|)
< P((sign(wt) /= sign(wt+1)) ∩(sign(wt+1) /= sign(wt+2))||τ−1| = |τ1|).
(2.33) 
Based on the above analysis, we propose an efﬁcient yet simple solution to realize 
the state-aware gradient: 
x Superscript l plus 1 Baseline equals StartLayout Enlarged left brace 1st Row 1st Column left parenthesis s i g n left parenthesis tau Subscript negative 1 Superscript l Baseline x Superscript l Baseline right parenthesis asterisk s i g n left parenthesis w Superscript l Baseline right parenthesis right parenthesis alpha 2nd Column i f ModifyingAbove x With caret equals negative 1 2nd Row 1st Column left parenthesis s i g n left parenthesis tau 1 Superscript l Baseline x Superscript l Baseline right parenthesis asterisk s i g n left parenthesis w Superscript l Baseline right parenthesis right parenthesis alpha 2nd Column o t h e r w i s e EndLayout periodxl+1 =
{(sign(τ l
−1xl) ∗sign(wl))α if ˆx = −1
(sign(τ l
1xl) ∗sign(wl))α
otherwise .
(2.34) 
Compared to traditional BNNs, we multiply the scale tauτ on the activation based on 
its state. Note that our paper’s learnable coefﬁcients tauτ are per-channel granularity. 
In this way, our SA-BNN is established in exchange for a small increase in 
computational complexity (only an extra point-wise product between tauτ and x). 
In particular, Helwegen et al. [22] argue that latent weights are not necessary for 
gradient-based optimization of BNNs, and they directly update the state of binarized 
weights with: 
w Superscript t Baseline equals StartLayout Enlarged left brace 1st Row 1st Column minus w Superscript t minus 1 Baseline 2nd Column i f StartAbsoluteValue g Superscript t Baseline EndAbsoluteValue greater than or equals beta a n d s i g n left parenthesis g Superscript t Baseline right parenthesis equals s i g n left parenthesis w Superscript t minus 1 Baseline right parenthesis 2nd Row 1st Column w Superscript t minus 1 Baseline 2nd Column o t h e r w i s e EndLayout commawt =
{ −wt−1 if |gt| ≥β and sign(gt) = sign(wt−1)
wt−1
otherwise
,
(2.35) 
where g Superscript t Baseline equals left parenthesis 1 minus gamma right parenthesis g Superscript t minus 1 Baseline plus gamma StartFraction partial differential script upper L Over partial differential w Superscript t Baseline EndFractiongt = (1 −γ )gt−1 + γ ∂L
∂wt , g Superscript tgt is the exponential moving average and gammaγ
is the adaptivity rate. Then, under the constraint of gamma equals 1γ
= 1, it is easy for the 
weight to ﬂip when StartAbsoluteValue StartFraction partial differential script upper L Over partial differential w Superscript t Baseline EndFraction EndAbsoluteValue greater than or equals beta| ∂L
∂wt | ≥β and hard to ﬂip when StartAbsoluteValue StartFraction partial differential script upper L Over partial differential w Superscript t Baseline EndFraction EndAbsoluteValue less than beta| ∂L
∂wt | < β, in which betaβ
is consistent with the coefﬁcients tauτ in our method. However, the method in [22] 
suppresses the weight ﬂip equally for different states, while SA-BNN treats different 
binarization states distinctively by employing an independent coefﬁcient for each 
state. SA-BNN can effectively suppress the frequent weight ﬂip problem, alleviating 
the ineffective update issue in BNN optimization. Moreover, unlike the handcrafted

2.7
Algorithms for Binary Neural Networks
29
hyperparameters betaβ, the coefﬁcients tauτ are learnable, avoiding careful tuning during 
optimization. 
Furthermore, Bai et al. [2] propose ProxQuant by formulating the quantized 
network training as a regularized learning problem instead and optimizing it via 
the prox-gradient method. Speciﬁcally, ProxQuant has access to additional gradient 
information at non-quantized points, which avoids the misleading weight update in 
training. However, unlike the ProxQuant, which suppresses the frequent weight ﬂip 
by designing a dedicated optimizer, SA-BNN alleviates this problem by introducing 
independent learnable coefﬁcients for different states, which can work with existing 
methods for back propagation and stochastic gradient descent. 
In addition, due to the non-differentiability of the sign function in the binarization 
process, most existing works employ a surrogate for the gradients [44, 53], in 
which the gradients are forced to be 0 for values outside left bracket negative 1 comma plus 1 right bracket[−1, +1]. However, once 
the value falls outside the truncation interval, the corresponding weight cannot be 
updated anymore. This phenomenon greatly limits the training ability of backward 
propagation [52]. Different from these methods (i.e., tau Subscript negative 1 Baseline equals tau 1τ−1 = τ1), our SA-BNN has 
the ability to preserve more gradients through learnable coefﬁcients, thus alleviating 
the unreliable gradients in BNN optimization. 
2.7.3.2 
Experiments 
We perform experiments on the large-scale dataset ImageNet (ILSVRC12) [55], 
which contains approximately 1.21.2 million training images and 50K validation 
images from 1000 categories. In our experiments, we employ 224 times 224224 × 224 random 
crop and center crop for training and inference, respectively. We use ResNet as 
our backbone, including ResNet-18, ResNet-34, and ResNet-50 [21]. We use Adam 
[33] with the momentum of 0.90.9 and set the weight decay to be 0. For the 18-layer 
SA-BNN, we run the training algorithm for 90 epochs with a batch size of 256. 
The learning rate starts from 0.0010.001 and is decayed twice by multiplying 0.10.1 at the 
75th and the 85th epoch. For the 34-layer SA-BNN, the training process includes 
90 epochs, and the batch size is set to 256. The learning rate starts from 0.0010.001 and 
is multiplied by 0.10.1 at the 60th and the 80th epoch, respectively. For the 50-layer 
SA-BNN, the training process is 70 epochs, and the batch size is 64. The learning 
rate starts from 0.00050.0005 and is multiplied by 0.10.1 at the 40th and the 60th epoch, 
respectively. 
We carry out a comparative study with six methods: IR-Net [52], Bop [22], 
CI-Net [63], BONN [20], Bi-Real Net [44], and XNOR-Net [53] on ResNet-18, 
ResNet-34, and ResNet-50 in Table 2.2. These six works are representative methods 
of binarizing both weights and activations for CNNs and achieving state-of-the-art 
results. 
The comparison in Table 2.2 demonstrates that our SA-BNNs outperform other 
networks by a considerable margin regarding the Top-1 accuracy. Note that the 
results of the other six works are quoted directly from the corresponding references. 
Speciﬁcally, the proposed SA-BNN with backbone ResNet-18 outperforms its

30
2
Binary Neural Networks
Table 2.2 Comparison on Top-1 and Top-5 accuracy (%) of SA-BNN with other state-of-the-art 
binarization methods, including IR-Net [52], Bop [22], CI-Net [63], BONN [20], Bi-Real Net [44], 
and XNOR-Net [53]. “FP” means full precision 
SA-BNN IR-Net Bop 
CI-Net BONN Bi-Real Net XNOR-Net FP 
ResNet-18 Top-1 61.7
58.1
56.6 59.9
59.3
56.4
51.2
69.3 
Top-5 82.8
80.0
79.4 84.2
81.6
79.5
73.2
89.2 
ResNet-34 Top-1 65.5
62.9
−
64.9
−
62.2
−
73.3 
Top-5 85.8
84.1
−
86.6
−
83.9
−
91.3 
ResNet-50 Top-1 68.7
−
−
−
−
62.6
63.1
74.7 
Top-5 87.4
−
−
−
−
83.9
83.6
92.1 
Epoch 
0 
10 20 30 40 50 60 70 80 90 
0 
10 20 30 40 50 60 70 80 90 
10 
20 
30 
40 
50 
60 
70 
80 
90 
Epoch 
10 
20 
30 
40 
50 
60 
70 
Validation Accuracy 
Validation Accuracy 
Top-1 Accuracy on ImageNet 
XNOR-Net 
SA-BNN 
Bi-Real Net 
95 
XNOR-Net 
SA-BNN 
Bi-Real Net 
95 
Top-5 Accuracy on ImageNet 
Fig. 2.8 Validation accuracy curves of SA-BNN, Bi-Real Net, and XNOR-Net with ResNet-18 
backbone on ImageNet 
counterpart Bi-Real Net by 5.3% and achieves a roughly 2% relative improvement 
over CI-Net. Similar improvements can be observed for ResNet-34 and ResNet-50 
networks. In Fig. 2.8, we plot the validation accuracy curves of XNOR-Net, Bi-
Real Net, and SA-BNN (without the contribution of PBN and SC). All networks 
are implemented under the same hyperparameter setting. It clearly shows that our 
method converges faster and better by learning distinctive gradient coefﬁcients for 
binarization states than XNOR-Net and Bi-Real Net. Moreover, our training curve 
is smoother, indicating the training process is more stable. Therefore, SA-BNN is 
more competitive than other state-of-the-art binary networks. 
We further analyze the memory usage saving and speedup in Table 2.3. We  
keep the weights and activations in the ﬁrst convolutional and the last fully 
connected layers to be full-precision [44, 53]. For a fair comparison, we use FLOPs 
[44] and BOPs [3] to measure the total multiplication computation and bitwise 
operations in SA-BNNs, respectively. For ResNet-18 and ResNet-34, the proposed 
SA-BNNs reduce the memory usage by 11.14× and 15.81×, respectively, and 
achieve computation reduction by 10.74× and 18.21×, in comparison with the full-
precision networks. Compared with Bi-Real Net, we obtain more than 4% accuracy 
improvement on ResNet-18 with small additional memory and computational cost.

2.7
Algorithms for Binary Neural Networks
31
Table 2.3 Memory usage, FLOPs, and BOPs calculation in our method. “MU” represents 
memory usage and “MS” represents memory saving 
MU
MS
FLOPs
BOPs
Speedup 
ResNet-18 
SA-BNN
33.6 Mbit 
11.14 times11.14×
1.68 times 10 Superscript 81.68 × 108
1.08 times 10 Superscript 101.08 × 1010
10.74 times10.74×
Bi-Real Net
33.6 Mbit 
11.14 times11.14×
1.63 times 10 Superscript 81.63 × 108
1.04 times 10 Superscript 101.04 × 1010
11.06 times11.06×
XNOR-Net
33.7 Mbit 
11.10 times11.10×
1.67 times 10 Superscript 81.67 × 108
1.07 times 10 Superscript 101.07 × 1010
10.86 times10.86×
Full-precision 
374.1 Mbit 
minus−
1.81 times 10 Superscript 91.81 × 109
1.16 times 10 Superscript 111.16 × 1011
minus−
ResNet-34 
SA-BNN
44.1 Mbit 
15.81 times15.81×
2.01 times 10 Superscript 82.01 × 108
1.29 times 10 Superscript 101.29 × 1010
18.21 times18.21×
Bi-Real Net
43.7 Mbit 
15.97 times15.97×
1.93 times 10 Superscript 81.93 × 108
1.24 times 10 Superscript 101.24 × 1010
18.99 times18.99×
XNOR-Net
43.9 Mbit 
15.88 times15.88×
1.98 times 10 Superscript 81.98 × 108
1.27 times 10 Superscript 101.27 × 1010
18.47 times18.47×
Full-precision 
697.3 Mbit 
minus−
3.66 times 10 Superscript 93.66 × 109
2.34 times 10 Superscript 112.34 × 1011
minus−
ResNet-50 
SA-BNN
144.4 Mbit 
5.43 times5.43×
3.89 times 10 Superscript 83.89 × 108
2.49 times 10 Superscript 102.49 × 1010
14.65 times14.65×
Bi-Real Net
143.1 Mbit 
5.48 times5.48×
3.74 times 10 Superscript 83.74 × 108
2.39 times 10 Superscript 102.39 × 1010
15.24 times15.24×
XNOR-Net
143.2 Mbit 
5.47 times5.47×
3.81 times 10 Superscript 83.81 × 108
2.44 times 10 Superscript 102.44 × 1010
14.96 times14.96×
Full-precision 
784.0 Mbit 
minus−
5.70 times 10 Superscript 95.70 × 109
3.65 times 10 Superscript 113.65 × 1011
minus−
2.7.4 
PCNN: Projection Convolutional Neural Networks 
Modulated convolutional networks (MCNs) are presented in [62] to binarize kernels, 
achieving better results than the baselines. However, in the inference step, MCNs 
require reconstructing full-precision convolutional ﬁlters from binarized ﬁlters, 
limiting their use in computationally limited environments. It has been theoretically 
and quantitatively demonstrated that simplifying the convolution procedure via 
binarized kernels and approximating the original unbinarized kernels is an up-and-
coming solution toward DCNNs’ compression. 
Although prior BNNs signiﬁcantly reduce storage requirements, they also gen-
erally have signiﬁcant accuracy degradation compared to those using full-precision 
kernels and activations. This is mainly because CNN binarization could be solved 
by considering discrete optimization in the back propagation (BP) process. Discrete 
optimization methods can often guarantee the quality of the solutions they ﬁnd and 
lead to much better performance in practice [16, 32, 34]. Second, the loss caused by 
the binarization of CNNs has yet to be well studied. 
We propose a new discrete back propagation via projection (DBPP) algorithm to 
efﬁciently build our projection convolutional neural networks (PCNNs) [18] and 
obtain highly accurate yet robust BNNs. Theoretically, we achieve a projection 
loss by taking advantage of our DBPP algorithms’ ability to perform discrete 
optimization on model compression. The advantages of the projection loss also 
lie in that it can be jointly learned with the conventional cross-entropy loss in the 
same pipeline as back propagation. The two losses are simultaneously optimized in 
continuous and discrete spaces, optimally combined by the projection approach in 
a theoretical framework. They can enrich the diversity and thus improve modeling 
capacity. As shown in Fig. 2.9, we develop a generic projection convolution layer 
that can be used in existing convolutional networks. Both the quantized kernels and

32
2
Binary Neural Networks
Fig. 2.9 In PCNNs, a new discrete back propagation via projection is proposed to build binarized 
neural networks in an end-to-end manner. Full-precision convolutional kernels upper C Subscript i Superscript lCl
i are quantized 
by projection as ModifyingAbove upper C With caret Subscript i comma j Superscript lˆCl
i,j. Due to multiple projections, the diversity is enriched. The resulting kernel 
tensor upper D Subscript i Superscript lDl
i is used the same as in conventional ones. Both the projection loss upper L Subscript pLp and the traditional 
loss upper L Subscript sLs are used to train PCNNs. We illustrate our network structure basic block unit based on 
ResNet, and more speciﬁc details are shown in the dotted box (projection convolution layer). © 
indicates the concatenation operation on the channels. Note that inference does not use projection 
matrices upper W Subscript j Superscript lW l
j and full-precision kernels upper C Subscript i Superscript lCl
i
the projection are jointly optimized in an end-to-end manner. Our project matrices 
are optimized but not for reference, resulting in a compact and efﬁcient learning 
architecture. As a general framework, other loss functions (e.g., center loss) can also 
be used to further improve the performance of our PCNNs based on a progressive 
optimization method. 
Discrete optimization is one of the hot topics in mathematics and is widely 
used to solve computer vision problems [32, 34]. Conventionally, the discrete 
optimization problem is solved by searching for an optimal set of discrete values 
concerning minimizing a loss function. This paper proposes a new discrete back 
propagation algorithm that uses a projection function to binarize or quantize the 
input variables in a uniﬁed framework. Due to the ﬂexible projection scheme, we 
obtain diverse binarized models with higher performance than the previous ones.

2.7
Algorithms for Binary Neural Networks
33
2.7.4.1 
Projection 
In our work, we deﬁne the quantization of the input variable as a projection onto a 
set: 
upper Omega colon equals StartSet a 1 comma a 2 comma ellipsis comma a Subscript upper U Baseline EndSet commao := {a1, a2, . . . , aU},
(2.36) 
where each element a Subscript iai, i equals 1 comma 2 comma ellipsis comma upper Ui = 1, 2, . . . , U satisﬁes the constraint a 1 less than a 2 less than ellipsis less than a Subscript upper Ua1 < a2 < . . . < aU
and is the discrete value of the input variable. Then we deﬁne the projection of x element of normal upper Rx ∈R
onto upper Omegao as: 
StartLayout 1st Row 1st Column upper P Subscript upper Omega Baseline left parenthesis omega comma x right parenthesis 2nd Column equals arg min Underscript a Subscript i Baseline Endscripts double vertical bar omega ring x minus a Subscript i Baseline double vertical bar comma i element of StartSet 1 comma ellipsis comma upper U EndSet comma EndLayout Po(ω, x) = arg min
ai ||ω ◦x −ai||, i ∈{1, . . . , U},
(2.37) 
where omegaω is a projection matrix and ring◦denotes the Hadamard product. Equation 2.37 
indicates that the projection aims to ﬁnd the closest discrete value for each 
continuous value x. Equation 2.37 is also equal to: 
StartLayout 1st Row 1st Column upper P Subscript upper Omega Baseline left parenthesis omega comma x right parenthesis 2nd Column equals arg min Underscript a Subscript i Baseline Endscripts double vertical bar x minus ModifyingAbove omega With caret ring a Subscript i Baseline double vertical bar comma i element of StartSet 1 comma ellipsis comma upper U EndSet comma EndLayout Po(ω, x) = arg min
ai ||x −ˆω ◦ai||, i ∈{1, . . . , U},
(2.38) 
where StartFraction 1 Over omega EndFraction equals ModifyingAbove omega With caret 1
ω = ˆω. During the following derivation of back propagation, we still use 
Eq. 2.37 as the basic equation, but in its implementation, one can also use Eq. 2.38 
to achieve the optimization of PCNN. 
2.7.4.2 
Optimization 
Minimizing f left parenthesis x right parenthesisf (x) are restricted to discrete values, which becomes more challenging 
when training a large-scale problem on a huge dataset [13]. We solve the problem 
within the back propagation framework by considering (1) the inference process 
of the optimized model is based on the quantized variables, which means that the 
variable must be quantized in the forward pass (corresponding to the inference) 
during training, and the loss is calculated based on the quantized variables; the 
variable for back propagation process is not necessarily quantized, which however 
needs to fully consider the relationship between quantized variables and their 
counterparts. Based on the above considerations, we propose that in the kth iteration, 
based on the projection in Eq. 2.37, x Superscript left bracket k right bracketx[k] is quantized to ModifyingAbove x With caret Superscript left bracket k right bracketˆx[k] in the forward pass as: 
ModifyingAbove x With caret Superscript left bracket k right bracket Baseline equals upper P Subscript upper Omega Baseline left parenthesis omega comma x Superscript left bracket k right bracket Baseline right parenthesis commaˆx[k] = Po(ω, x[k]),
(2.39) 
which is used to improve the back propagation process by deﬁning an objective as: 
StartLayout 1st Row 1st Column min 2nd Column f left parenthesis omega comma x right parenthesis 2nd Row 1st Column s period t period 2nd Column ModifyingAbove x With caret Subscript j Superscript left bracket k right bracket Baseline equals upper P Subscript upper Omega Superscript j Baseline left parenthesis omega Subscript j Baseline comma x right parenthesis comma EndLayoutmin
f (ω, x)
s.t. ˆx[k]
j
= P j
o(ωj, x),
(2.40)

34
2
Binary Neural Networks
where omega Subscript j Baseline comma j element of StartSet 1 comma ellipsis comma upper J EndSetωj, j ∈{1, . . . , J} is the jth projection matrix,1 and J is the total number of 
projection matrices. To solve the problem in (2.40), we deﬁne our update rule as: 
x left arrow x Superscript left bracket k right bracket Baseline minus eta delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline commax ←x[k] −ηδ[k]
ˆx ,
(2.41) 
where the superscript left bracket k plus 1 right bracket[k + 1] is removed from x, delta Subscript ModifyingAbove x With caretδˆx is the gradient of f left parenthesis omega comma x right parenthesisf (ω, x) with 
respect to x equals ModifyingAbove x With caretx = ˆx, and etaη is the learning rate. The quantization process ModifyingAbove x With caret Superscript left bracket k right bracket Baseline left arrow x Superscript left bracket k right bracketˆx[k] ←x[k], 
that is, upper P Subscript upper Omega Superscript j Baseline left parenthesis omega Subscript j Baseline comma x Superscript left bracket k right bracket Baseline right parenthesisP j
o(ωj, x[k]), is equivalent to ﬁnding the projection of omega Subscript j Baseline ring left parenthesis x plus eta delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline right parenthesisωj ◦(x + ηδ[k]
ˆx ) onto 
upper Omegao as: 
ModifyingAbove x With caret Superscript left bracket k right bracket Baseline equals arg min Underscript ModifyingAbove x With caret Endscripts left brace double vertical bar ModifyingAbove x With caret minus omega Subscript j Baseline ring left parenthesis x plus eta delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline right parenthesis double vertical bar squared comma ModifyingAbove x With caret element of upper Omega right brace periodˆx[k] = arg min
ˆx {||ˆx −ωj ◦(x + ηδ[k]
ˆx )||2, ˆx ∈o}.
(2.42) 
Obviously, ModifyingAbove x With caret Superscript left bracket k right bracketˆx[k] is the solution to the problem in (2.42). So, by incorporating 
(2.42) into f left parenthesis omega comma x right parenthesisf (ω, x), we obtain a new formulation for (2.40) based on the Lagrangian 
method as: 
min f left parenthesis omega comma x right parenthesis plus StartFraction lamda Over 2 EndFraction sigma summation Underscript j Overscript upper J Endscripts double vertical bar ModifyingAbove x With caret Superscript left bracket k right bracket Baseline minus omega Subscript j Baseline ring left parenthesis x plus eta delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline right parenthesis double vertical bar squared period min f (ω, x) + λ
2
J
Σ
j
||ˆx[k] −ωj ◦(x + ηδ[k]
ˆx )||2.
(2.43) 
The newly added part (right) shown in (2.43) is a quadratic function and is referred 
to as projection loss. 
2.7.4.3 
Theoretical Analysis 
We closely examine the projection loss in Eq. 2.43 and have: 
ModifyingAbove x With caret Superscript left bracket k right bracket Baseline minus omega ring left parenthesis x plus eta delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline right parenthesis equals ModifyingAbove x With caret Superscript left bracket k right bracket Baseline minus omega ring x minus omega ring eta delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline periodˆx[k] −ω ◦(x + ηδ[k]
ˆx ) = ˆx[k] −ω ◦x −ω ◦ηδ[k]
ˆx .
(2.44) 
We only consider one projection function in this case, so the subscript j of omega Subscript jωj
is omitted for simplicity. For multiple projections, the analysis is given after that. 
In the forward step, only the discrete values participate in the calculation, so their 
gradients can be obtained by: 
StartFraction partial differential f left parenthesis omega comma ModifyingAbove x With caret Superscript left bracket k right bracket Baseline right parenthesis Over partial differential ModifyingAbove x With caret Superscript left bracket k right bracket Baseline EndFraction equals omega ring delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline comma∂f (ω, ˆx[k])
∂ˆx[k]
= ω ◦δ[k]
ˆx ,
(2.45) 
as omegaω and ModifyingAbove x With caretˆx are bilinear with each other as omega ring ModifyingAbove x With caret Superscript left bracket k right bracketω ◦ˆx[k]. In our discrete optimization 
framework, the values of convolutional kernels are updated according to their 
gradients. Taking Eq. 2.45 into consideration, we derive the update rule for ModifyingAbove x With caret Superscript left bracket k plus 1 right bracketˆx[k+1]
1 Since the kernel parameters x are represented as a matrix, omega Subscript jωj denotes a matrix as omegaω. 

2.7
Algorithms for Binary Neural Networks
35
as: 
ModifyingAbove x With caret Superscript left bracket k plus 1 right bracket Baseline equals ModifyingAbove x With caret Superscript left bracket k right bracket Baseline minus eta StartFraction partial differential f left parenthesis omega comma ModifyingAbove x With caret Superscript left bracket k right bracket Baseline right parenthesis Over partial differential ModifyingAbove x With caret Superscript left bracket k right bracket Baseline EndFraction equals ModifyingAbove x With caret Superscript left bracket k right bracket Baseline minus omega ring eta delta Subscript ModifyingAbove x With caret Superscript left bracket k right bracket Baseline periodˆx[k+1] = ˆx[k] −η∂f (ω, ˆx[k])
∂ˆx[k]
= ˆx[k] −ω ◦ηδ[k]
ˆx .
(2.46) 
By plugging Eq. 2.46 into Eq. 2.44, we achieve a new objective function or a loss 
function that minimizes: 
StartAbsoluteValue EndAbsoluteValue ModifyingAbove x With caret Superscript left bracket k plus 1 right bracket Baseline minus omega ring x StartAbsoluteValue EndAbsoluteValue comma||ˆx[k+1] −ω ◦x||,
(2.47) 
to approximate: 
ModifyingAbove x With caret equals omega ring x comma x equals omega Superscript negative 1 Baseline ring ModifyingAbove x With caret periodˆx = ω ◦x, x = ω−1 ◦ˆx.
(2.48) 
We further discuss multiple projections, based on Eq. 2.48 and projection loss in 
(2.43), and have: 
min one half sigma summation Underscript j Overscript upper J Endscripts StartAbsoluteValue EndAbsoluteValue x minus omega Subscript j Superscript negative 1 Baseline ring ModifyingAbove x Subscript j Baseline With caret StartAbsoluteValue EndAbsoluteValue squared period min 1
2
J
Σ
j
||x −ω−1
j
◦ˆxj||2.
(2.49) 
We set g left parenthesis x right parenthesis equals one half sigma summation Underscript j Overscript upper J Endscripts StartAbsoluteValue EndAbsoluteValue x minus omega Subscript j Superscript negative 1 Baseline ring ModifyingAbove x Subscript j Baseline With caret StartAbsoluteValue EndAbsoluteValue squaredg(x) = 1
2
ΣJ
j ||x −ω−1
j
◦ˆxj||2 and calculate its derivative as g prime left parenthesis x right parenthesis equals 0g'(x) = 0, and 
we have: 
x equals StartFraction 1 Over upper J EndFraction sigma summation Underscript j Overscript upper J Endscripts omega Subscript j Superscript negative 1 Baseline ring ModifyingAbove x Subscript j Baseline With caret commax = 1
J
J
Σ
j
ω−1
j
◦ˆxj,
(2.50) 
which shows that multiple projections can better reconstruct the full kernels based 
on binarized counterparts. 
2.7.4.4 
Projection Convolutional Neural Networks 
Projection convolutional neural networks (PCNNs), shown in Fig. 2.9, work using 
DBPP for model quantization. We accomplish this by reformulating our projection 
loss shown in (2.43) into the deep learning paradigm as: 
upper L Subscript p Baseline equals StartFraction lamda Over 2 EndFraction sigma summation Underscript l comma i Overscript upper L comma upper I Endscripts sigma summation Underscript j Overscript upper J Endscripts StartAbsoluteValue EndAbsoluteValue ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket Baseline minus upper W overTilde Subscript j Superscript l comma left bracket k right bracket Baseline ring left parenthesis upper C Subscript i Superscript l comma left bracket k right bracket Baseline plus eta delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l comma left bracket k right bracket Subscript Baseline right parenthesis StartAbsoluteValue EndAbsoluteValue squared commaLp = λ
2
L,I
Σ
l,i
J
Σ
j
|| ˆCl,[k]
i,j
−-
W l,[k]
j
◦(Cl,[k]
i
+ ηδ ˆCl,[k]
i,j )||2,
(2.51)

36
2
Binary Neural Networks
where upper C Subscript i Superscript l comma left bracket k right bracketCl,[k]
i
, l element of StartSet 1 comma ellipsis comma upper L EndSet comma i element of StartSet 1 comma ellipsis comma upper I EndSetl ∈{1, . . . , L}, i ∈{1, . . . , I} denotes the ith kernel tensor of the lth 
convolutional layer in the kth iteration. ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket ˆCl,[k]
i,j
is the quantized kernel of upper C Subscript i Superscript l comma left bracket k right bracketCl,[k]
i
via 
projection upper P Subscript upper Omega Superscript l comma j Baseline comma j element of StartSet 1 comma ellipsis comma upper J EndSetP l,j
o , j ∈{1, . . . , J} as: 
ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket Baseline equals upper P Subscript upper Omega Superscript l comma j Baseline left parenthesis upper W overTilde Subscript j Superscript l comma left bracket k right bracket Baseline comma upper C Subscript i Superscript l comma left bracket k right bracket Baseline right parenthesis comma ˆCl,[k]
i,j
= P l,j
o ( -
W l,[k]
j
, Cl,[k]
i
),
(2.52) 
where upper W overTilde Subscript j Superscript l comma left bracket k right bracket -
W l,[k]
j
is a tensor, calculated by duplicating a learned projection matrix upper W Subscript j Superscript l comma left bracket k right bracketW l,[k]
j
along the channels, which thus ﬁts the dimension of upper C Subscript i Superscript l comma left bracket k right bracketCl,[k]
i
. delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l comma left bracket k right bracketδ ˆCl,[k]
i,j
is the gradient at 
ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket ˆCl,[k]
i,j
calculated based on upper L Subscript upper SLS, that is, delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l comma left bracket k right bracket Baseline equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket Baseline EndFractionδ ˆCl,[k]
i,j
=
∂LS
∂ˆCl,[k]
i,j
. The iteration index left bracket k right bracket[k] is 
omitted for simplicity. 
In PCNNs, both the cross-entropy loss and projection loss are used to build the 
total loss as: 
upper L equals upper L Subscript upper S Baseline plus upper L Subscript upper P Baseline periodL = LS + LP .
(2.53) 
The proposed projection loss regularizes the continuous values converging onto upper Omega Superscript upper NoN
while minimizing the cross-entropy loss, illustrated in Figs. 2.11 and 2.12. 
2.7.4.5 
Forward Propagation Based on Projection Convolution Layer 
For each full-precision kernel upper C Subscript i Superscript lCl
i, the corresponding quantized kernels ModifyingAbove upper C With caret Subscript i comma j Superscript l ˆCl
i,j are 
concatenated to construct the kernel upper D Subscript i Superscript lDl
i that actually participates in the convolution 
operation as: 
upper D Subscript i Superscript l Baseline equals ModifyingAbove upper C With caret Subscript i comma 1 Superscript l Baseline circled plus ModifyingAbove upper C With caret Subscript i comma 2 Superscript l Baseline circled plus midline horizontal ellipsis circled plus ModifyingAbove upper C With caret Subscript i comma upper J Superscript l Baseline commaDl
i = ˆCl
i,1 ⊕ˆCl
i,2 ⊕· · · ⊕ˆCl
i,J ,
(2.54) 
where circled plus⊕denotes the concatenation operation on the tensors. In PCNNs, the 
projection convolution is implemented based on upper D Superscript lDl and upper F Superscript lF l to calculate the next 
layer’s feature map upper F Superscript l plus 1F l+1: 
upper F Superscript l plus 1 Baseline equals upper C o n v Baseline 2 upper D left parenthesis upper F Superscript l Baseline comma upper D Superscript l Baseline right parenthesis commaF l+1 = Conv2D(F l, Dl),
(2.55) 
where Conv2D is the traditional 2D convolution. Although our convolutional 
kernels are 3D-shaped tensors, we design the following strategy to ﬁt the traditional 
2D convolution as: 
upper F Subscript h comma j Superscript l plus 1 Baseline equals sigma summation Underscript i comma h Endscripts upper F Subscript h Superscript l Baseline circled times upper D Subscript i comma j Superscript l Baseline commaF l+1
h,j =
Σ
i,h
F l
h ⊗Dl
i,j,
(2.56) 
upper F Subscript h Superscript l plus 1 Baseline equals upper F Subscript h comma 1 Superscript l Baseline circled plus midline horizontal ellipsis circled plus upper F Subscript h comma upper J Superscript l Baseline commaF l+1
h
= F l
h,1 ⊕· · · ⊕F l
h,J ,
(2.57)

2.7
Algorithms for Binary Neural Networks
37
where circled times⊗denotes the convolutional operation. upper F Subscript h comma j Superscript l plus 1F l+1
h,j is the jth channel of the hth 
feature map at the left parenthesis l plus 1 right parenthesis(l + 1)th convolutional layer and upper F Subscript h Superscript lF l
h denotes the hth feature map 
at the lth convolutional layer. To be more precise, for example, when h equals 1h = 1, for  the  
jth channel of an output feature map, upper F Subscript 1 comma j Superscript l plus 1F l+1
1,j is the sum of the convolutions between 
all the h input feature maps and i corresponding quantized kernels. All channels 
of the output feature maps are obtained as upper F Subscript h comma 1 Superscript l plus 1 Baseline comma period period comma upper F Subscript h comma j Superscript l plus 1 Baseline comma ellipsis comma upper F Subscript h comma upper J Superscript l plus 1F l+1
h,1 , .., F l+1
h,j , . . . , F l+1
h,J , and they are 
concatenated to construct the hth output feature map upper F Subscript h Superscript l plus 1F l+1
h
. 
It should be emphasized that we can utilize multiple projections to increase the 
diversity of convolutional kernels upper D Superscript lDl. However, the single projection can perform 
much better than the existing BNNs. The essential use of DBPP differs from [38] 
based on a single quantization scheme. Within our convolutional scheme, there is 
no dimensional disagreement on feature maps and kernels in two successive layers. 
Thus, we can replace the traditional convolutional layers with ours to binarize 
widely used networks, such as VGGs and ResNets. At inference time, we only store 
the set of quantized kernels upper D Subscript i Superscript lDl
i instead of the full-precision ones; that is, projection 
matrices upper W Subscript j Superscript lW l
j are not used for inference, achieving a reduction in storage. 
2.7.4.6 
Backward Propagation 
According to Eq. 2.53, what should be learned and updated are the full-precision 
kernels upper C Subscript i Superscript lCl
i and the projection matrix upper W Superscript lW l (upper W overTilde Superscript l-
W l) using the updated equations described 
below. 
Updating upper C Subscript i Superscript lCl
i We deﬁne delta Subscript upper C Sub Subscript iδCi as the gradient of the full-precision kernel upper C Subscript iCi and have: 
delta Subscript upper C Sub Subscript i Sub Superscript l Subscript Baseline equals StartFraction partial differential upper L Over partial differential upper C Subscript i Superscript l Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper C Subscript i Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper P Baseline Over partial differential upper C Subscript i Superscript l Baseline EndFraction commaδCl
i = ∂L
∂Cl
i
= ∂LS
∂Cl
i
+ ∂LP
∂Cl
i
,
(2.58) 
upper C Subscript i Superscript l Baseline left arrow upper C Subscript i Superscript l Baseline minus eta 1 delta Subscript upper C Sub Subscript i Sub Superscript l Subscript Baseline commaCl
i ←Cl
i −η1δCl
i ,
(2.59) 
where eta 1η1 is the learning rate for the convolutional kernels. More speciﬁcally, for 
each item in Eq. 2.58, we have:  
StartLayout 1st Row 1st Column StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper C Subscript i Superscript l Baseline EndFraction 2nd Column equals sigma summation Underscript j Overscript upper J Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper C With caret Subscript i comma j Superscript l Baseline EndFraction StartFraction partial differential upper P Subscript upper Omega Sub Superscript upper N Subscript Superscript l comma j Baseline left parenthesis upper W overTilde Subscript j Superscript l Baseline comma upper C Subscript i Superscript l Baseline right parenthesis Over partial differential left parenthesis upper W overTilde Subscript j Superscript l Baseline ring upper C Subscript i Superscript l Baseline right parenthesis EndFraction StartFraction partial differential left parenthesis upper W overTilde Subscript j Superscript l Baseline ring upper C Subscript i Superscript l Baseline right parenthesis Over partial differential upper C Subscript i Superscript l Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript j Overscript upper J Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper C With caret Subscript i comma j Superscript l Baseline EndFraction ring double struck 1 Subscript negative 1 less than or equals upper W overTilde Sub Subscript j Sub Superscript l Subscript ring upper C Sub Subscript i Sub Superscript l Subscript less than or equals 1 Baseline ring upper W overTilde Subscript j Superscript l Baseline comma EndLayout
∂LS
∂Cl
i
=
J
Σ
j
∂LS
∂ˆCl
i,j
∂P l,j
oN ( -
W l
j, Cl
i)
∂( -
W l
j ◦Cl
i)
∂( -
W l
j ◦Cl
i)
∂Cl
i
=
J
Σ
j
∂LS
∂ˆCl
i,j
◦1−1≤-
W l
j ◦Cl
i≤1 ◦-
W l
j,
(2.60) 
StartFraction partial differential upper L Subscript upper P Baseline Over partial differential upper C Subscript i Superscript l Baseline EndFraction equals lamda sigma summation Underscript j Overscript upper J Endscripts left bracket upper W overTilde Subscript j Superscript l Baseline ring left parenthesis upper C Subscript i Superscript l Baseline plus eta delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l Subscript Baseline right parenthesis minus ModifyingAbove upper C With caret Subscript i comma j Superscript l Baseline right bracket ring upper W overTilde Subscript j Superscript l Baseline comma∂LP
∂Cl
i
= λ
J
Σ
j
[
-
W l
j ◦
(
Cl
i + ηδ ˆCl
i,j
)
−ˆCl
i,j
]
◦-
W l
j,
(2.61)

38
2
Binary Neural Networks
where double struck 11 is the indicator function [53] widely used to estimate the gradient of the 
nondifferentiable function. More speciﬁcally, the output of the indicator function is 
1 only if the condition is satisﬁed; otherwise, 0. 
Updating upper W Subscript j Superscript lW l
j Likewise, the gradient of the projection parameter delta Subscript upper W Sub Subscript j Sub Superscript lδW l
j consists of 
the following two parts: 
delta Subscript upper W Sub Subscript j Sub Superscript l Subscript Baseline equals StartFraction partial differential upper L Over partial differential upper W Subscript j Superscript l Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper W Subscript j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper P Baseline Over partial differential upper W Subscript j Superscript l Baseline EndFraction commaδW l
j = ∂L
∂W l
j
= ∂LS
∂W l
j
+ ∂LP
∂W l
j
,
(2.62) 
upper W Subscript j Superscript l Baseline left arrow upper W Subscript j Superscript l Baseline minus eta 2 delta Subscript upper W Sub Subscript j Sub Superscript l Subscript Baseline commaW l
j ←W l
j −η2δW l
j ,
(2.63) 
where eta 2η2 is the learning rate for upper W Subscript j Superscript lW l
j. We also have the following: 
StartLayout 1st Row 1st Column StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper W Subscript j Superscript l Baseline EndFraction 2nd Column equals sigma summation Underscript h Overscript upper J Endscripts left parenthesis StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper W overTilde Subscript j Superscript l Baseline EndFraction right parenthesis Subscript h Baseline 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript h Overscript upper J Endscripts left parenthesis sigma summation Underscript i Overscript upper I Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper C With caret Subscript i comma j Superscript l Baseline EndFraction StartFraction partial differential upper P Subscript upper Omega Sub Superscript upper N Subscript Superscript l comma j Baseline left parenthesis upper W overTilde Subscript j Superscript l Baseline comma upper C Subscript i Superscript l Baseline right parenthesis Over partial differential left parenthesis upper W overTilde Subscript j Superscript l Baseline ring upper C Subscript i Superscript l Baseline right parenthesis EndFraction StartFraction partial differential left parenthesis upper W overTilde Subscript j Superscript l Baseline ring upper C Subscript i Superscript l Baseline right parenthesis Over partial differential upper W overTilde Subscript j Superscript l Baseline EndFraction right parenthesis Subscript h Baseline 3rd Row 1st Column Blank 2nd Column equals sigma summation Underscript h Overscript upper J Endscripts left parenthesis sigma summation Underscript i Overscript upper I Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper C With caret Subscript i comma j Superscript l Baseline EndFraction ring double struck 1 Subscript negative 1 less than or equals upper W overTilde Sub Subscript j Sub Superscript l Subscript ring upper C Sub Subscript i Sub Superscript l Subscript less than or equals 1 Baseline ring upper C Subscript i Superscript l Baseline right parenthesis Subscript h Baseline comma EndLayout
∂LS
∂W l
j
=
J
Σ
h
(
∂LS
∂-
W l
j
)
h
=
J
Σ
h
⎛
⎝
I
Σ
i
∂LS
∂ˆCl
i,j
∂P l,j
oN ( -
W l
j, Cl
i)
∂( -
W l
j ◦Cl
i)
∂( -
W l
j ◦Cl
i)
∂-
W l
j
⎞
⎠
h
=
J
Σ
h
( I
Σ
i
∂LS
∂ˆCl
i,j
◦1−1≤-
W l
j ◦Cl
i≤1 ◦Cl
i
)
h
,
(2.64) 
StartFraction partial differential upper L Subscript upper P Baseline Over partial differential upper W Subscript j Superscript l Baseline EndFraction equals lamda sigma summation Underscript h Overscript upper J Endscripts left parenthesis sigma summation Underscript i Overscript upper I Endscripts left bracket upper W overTilde Subscript j Superscript l Baseline ring left parenthesis upper C Subscript i Superscript l Baseline plus eta delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l Subscript Baseline right parenthesis minus ModifyingAbove upper C With caret Subscript i comma j Superscript l Baseline right bracket ring left parenthesis upper C Subscript i Superscript l Baseline plus eta delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l Subscript Baseline right parenthesis right parenthesis Subscript h Baseline comma∂LP
∂W l
j
=λ
J
Σ
h
( I
Σ
i
[
-
W l
j ◦
(
Cl
i +ηδ ˆCl
i,j
)
−ˆCl
i,j
]
◦
(
Cl
i +ηδ ˆCl
i,j
))
h
,
(2.65) 
where h indicates the hth plane of the tensor along the channels. It shows that the 
proposed algorithm can be trained from end to end, and we summarize the training 
procedure in Algorithm 1. In the implementation, we use the mean of W in the 
forward process but keep the original W in the backward propagation. 
Note that in PCNNs for BNNs, we set U = 2 and a 2a2 = minus a 1−a1. Two binarization 
processes are used in PCNNs. The ﬁrst is the kernel binarization, which is done 
based on the projection onto upper Omega Superscript upper NoN, whose elements are calculated based on the mean 
absolute values of all full-precision kernels per layer [53] as:  
StartFraction 1 Over upper I EndFraction sigma summation Underscript i Overscript upper I Endscripts left parenthesis parallel to upper C Subscript i Superscript l Baseline parallel to Subscript 1 Baseline right parenthesis comma1
I
I
Σ
i
(
||Cl
i||1
)
,
(2.66) 
where I is the total number of kernels.

2.7
Algorithms for Binary Neural Networks
39
Algorithm 1: Discrete back propagation via projection 
An algo
rith m for Dis crete bac kpro pagation via pr ojection . I t ca lls the inp uts, in iti aliz es C and 
W rand oml y, i mpli
es forwa
rd p ropagat ion , calcul ates c ross -entro py loss , implie s back pro
pag ation, and update s the output
s C  and W 
ba
sed  on the binary or te
rn
ary P C N  N s.
2.52 An algori thm  for Discrete backpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.68 An algorit
hm
 fo
r D iscrete backpropagati on via pr ojection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.54 
An
 algorit hm for Disc rete backpropa gation v ia p rojec tion. It 
ca
lls the inpu ts,  initializes C a nd W r andomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.55An algorithm for Discrete backpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.56 An algorithm for Discrete backpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.57 
An 
algo rith
m f
or Discret e backpropagat ion v ia p
roj
ect ion. It c alls the inp
uts
, initia lizes 
C an d W ra
ndomly
, im
pl
ies 
forw ard Lprop ag ati
on,
 ca lculates c ros -entropy l
oss
, implies back
prop aga tion, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.58An algorithm for Discrete backpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.60 An algorithm for Discrete backpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.61 
An 
algorithm fo r 
Disc ret e back propagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.62An algorithm for Discrete backpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.64 An algorithm for Discrete backpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.65 
An 
alg orithm for Discrete ba
ckp
rop
ag ation
 via proje
cti on . It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.59 
An 
a lg
or ith m 
fo r Disc re
te b ackpropagation via projection. It calls the inputs, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns. 2.63 
An 
algo rith
m f
or Disc rete  backprop agatio n v ia p roje
ctio n. It cal s the in puts, initializes C and W randomly, implies forward propagation, calculates cross-entropy loss, implies backpropagation, and updates the outputs C and W based on the binary or ternary P C N Ns.
2.7.4.7 
Progressive Optimization 
Training 1-bit CNNs is a highly non-convex optimization problem, and initialization 
states will signiﬁcantly impact the convergence. Unlike the method in [44] that a 
real-valued CNN model with the clip function pre-trained on ImageNet initializes 
the 1-bit CNN models, we propose applying a progressive optimization strategy in 
training 1-bit CNNs. However, a real-valued CNN model can achieve pretty high 
classiﬁcation accuracy, we wonder if the converging states between real-value and 
1-bit CNNs, which may mistakenly guide the converging process of 1-bit CNNs. 
We believe that compressed ternary CNNs such as TTN [79] and TWN [36] 
have better initialization states for binary CNNs. Theoretically, the performance of 
models with ternary weights is slightly better than those with binary weights and far 
worse than those of real-valued ones. Still, they provide an excellent initialization 
state for 1-bit CNNs in our proposed progressive optimization framework. Subse-
quent experiments show that our PCNNs trained from a progressive optimization 
strategy perform better than those from scratch, even better than the ternary PCNNs 
from scratch.

40
2
Binary Neural Networks
The discrete set for ternary weights is a special case, deﬁned as upper Omega colon equals StartSet a 1 comma a 2 comma a 3 EndSeto := {a1, a2, a3}. 
We further require a 1 equals minus a 3 equals upper Deltaa1 = −a3 = A as Eq. 2.66 and a 2 equals 0a2 = 0 to be hardware friendly 
[36]. Regarding the threshold for ternary weights, we follow the choice made in [58] 
as: 
upper Delta Superscript l Baseline equals sigma times upper E left parenthesis StartAbsoluteValue upper C Superscript l Baseline EndAbsoluteValue right parenthesis almost equals StartFraction sigma Over upper I EndFraction sigma summation Underscript i Overscript upper I Endscripts left parenthesis parallel to upper C Subscript i Superscript l Baseline parallel to Subscript 1 Baseline right parenthesis commaAl = σ × E(|Cl|) ≈σ
I
I
Σ
i
(
||Cl
i||1
)
,
(2.67) 
where sigmaσ is a constant factor for all layers. Note that [58] applies to Eq. 2.67 on 
convolutional inputs or feature maps; we ﬁnd it appropriate in convolutional weights 
as well. Consequently, we redeﬁne the projection in Eq. 2.37 as: 
StartLayout 1st Row 1st Column upper P Subscript upper Omega Baseline left parenthesis omega comma x right parenthesis 2nd Column equals arg min Underscript a Subscript i Baseline Endscripts double vertical bar omega ring x minus 2 a Subscript i Baseline double vertical bar comma i element of StartSet 1 comma ellipsis comma upper U EndSet period EndLayout Po(ω, x) = arg min
ai ||ω ◦x −2ai||, i ∈{1, . . . , U}.
(2.68) 
In our proposed progressive optimization framework, the PCNNs with ternary 
weights (ternary PCNNs) are ﬁrst trained from scratch and then served as pre-trained 
models to progressively ﬁne-tune the PCNNs with binary weights (binary PCNNs). 
To alleviate the disturbance caused by the quantization process, intraclass 
compactness is further deployed based on the center loss function [65] to improve 
performance. Given the input features x Subscript i Baseline element of double struck upper R Superscript dxi ∈Rd or upper Omegao and the y Subscript iyith class center 
c Subscript y Sub Subscript i Baseline element of double struck upper R Superscript dcyi ∈Rd or upper Omegao of the input features, we have: 
upper L Subscript upper C Baseline equals StartFraction gamma Over 2 EndFraction sigma summation Underscript i equals 1 Overscript m Endscripts parallel to x Subscript i Baseline minus c Subscript y Sub Subscript i Subscript Baseline parallel to Subscript 2 Superscript 2 Baseline commaLC = γ
2
m
Σ
i=1
||xi −cyi||2
2,
(2.69) 
where m denotes the total number of samples or batch size and gammaγ is a hyperparameter 
to balance the center loss with other losses. More details on center loss can be found 
in [65]. By incorporating Eq. 2.69 into Eq. 2.53, the total loss is updated as: 
upper L equals upper L Subscript upper S Baseline plus upper L Subscript upper P Baseline plus upper L Subscript upper C Baseline periodL = LS + LP + LC.
(2.70) 
We note that the center loss is successfully deployed to handle feature variations 
in the training and will be omitted in the inference, so there is no additional 
memory storage and computational cost. More intuitive illustrations can be found 
in Fig. 2.10, and a more detailed training procedure is described in Algorithm 2. 
2.7.4.8 
Ablation Study 
Parameter As mentioned above, the proposed projection loss, similar to cluster-
ing, can control quantization. We computed the distributions of the full-precision 
kernels and visualized the results in Figs. 2.11 and 2.12. The hyperparameter lamdaλ is 
designed to balance projection loss and cross-entropy loss. We vary it from 1 e minus 31e −3

2.7
Algorithms for Binary Neural Networks
41
Fig. 2.10 In our proposed progressive optimization framework, the two additional losses, projec-
tion loss, and center loss are simultaneously optimized in continuous and discrete spaces, optimally 
combined by the projection approach in a theoretical framework. The subﬁgure on the left explains 
the softmax function in the cross-entropy loss. The subﬁgure in the middle illustrates the process 
of progressively turning ternary kernel weights into binary ones within our projection approach. 
The subﬁgure on the right shows the function of center loss to force the learned feature maps to 
cluster together, class by class. Best viewed in color 
Fig. 2.11 We visualize the distribution of kernel weights of the ﬁrst convolution layer of PCNN-
22. The variance increases when the ratio decreases λ, which balances projection loss and cross-
entropy loss. In particular, when λ = 0 (no projection loss), only one group is obtained, where 
the kernel weights are distributed around 0, which could result in instability during binarization. 
In contrast, two Gaussians (with projection loss, λ >  0) are more powerful than the single one 
(without projection loss), which thus results in better BNNs, as also validated in Table 2.4 
to 1 e minus 51e −5 and ﬁnally set it to 0 in Fig. 2.11, where the variance increases as the 
number of lamdaλ. When lamda equals 0λ = 0, only one cluster is obtained, where the kernel weights 
are tightly distributed around the threshold = 0. This could result in instability during

42
2
Binary Neural Networks
Algorithm 2: Progressive optimization with center loss 
Input: The training dataset; the full-precision kernels C; the pre-trained kernels tC from 
ternary PCNNs; the projection matrix W; the learning rates η1 and η2. 
Output: The binary PCNNs are based on the updated C and W. 
1: Initialize W randomly but C from tC; 
2: repeat 
3:
// Forward propagation 
4:
for l = 1 to  L do 
5:
ˆCl
i,j ←P (W, Cl
i); // using Eq. 2.52 
6:
Dl
i ←Concatenate( ˆCi,j); // using Eq. 2.54 
7:
Perform activation binarization; //using the sign function 
8:
Traditional 2D convolution; // using Eqs. 2.55, 2.56 and 2.57 
9: 
end for 
10:
Calculate cross-entropy loss LS; 
11: 
if using center loss then 
12:
L' = LS + LC; 
13: 
else 
14:
L' = LS; 
15: 
end if 
16:
// Backward propagation 
17:
Compute δ ˆCl
i,j =
∂L'
∂ˆCl
i,j
; 
18: 
for l = L to 1 do 
19:
// Calculate the gradients 
20:
calculate δCl
i ; // using Eqs. 2.58, 2.60 and 2.61 
21:
calculate δW l
j ; // using Eqs. 2.62, 2.64 and 2.65 
22:
// Update the parameters 
23:
Cl
i ←Cl
i −η1δCl
i ; // Eq. 2.59 
24:
W l
j ←W l
j −η2δW l
j ; // Eq. 2.63 
25:
end for 
26:
Adjust the learning rates η1 and η2. 
27: until the network converges 
–0.06 –0.04 –0.02 
0.00 
0.02 
0.04 
0.06 –0.03 –0.02 –0.01 
0.00 
0.01 
0.02 
0.03 
–0.03 –0.02 –0.01 
0.00 
0.01 
0.02 
0.03 
epoch=200 
Var=1.09e-05 
epoch=20 
Var=3.24e-05 
epoch=2 
Var=8.32e-05 
Fig. 2.12 With λ ﬁxed to 1e −4, the variance of the kernel weights decreases from the 2nd epoch 
to the 200th epoch, which conﬁrms that the projection loss does not affect the convergence

2.7
Algorithms for Binary Neural Networks
43
Table 2.4 With different λ, 
the accuracy of PCNN-22 and 
PCNN-40 based on WRN-22 
and WRN-40, respectively, 
on CIFAR10 dataset 
λ 
Model
1e − 3 
1e − 4 
1e − 5 
0 
PCNN-22 
91.92 
92.79 
92.24 
91.52 
PCNN-40 
92.85 
93.78 
93.65 
92.84 
Fig. 2.13 Training and testing curves of PCNN-22 when λ = 0 and  1e − 4, which shows that the 
projection affects little on the convergence 
binarization because little noise may cause a positive weight to be negative and vice 
versa. 
We also show the evolution of the distribution of how projection loss works in 
the training process in Fig. 2.12. A natural question is: do we always need a large λ? 
As a discrete optimization problem, the answer is no. The experiment in Table 2.4 
can verify it, i.e., both the projection and cross-entropy losses should be considered 
simultaneously with good balance. For example, when λ is set to 1e−4, the accuracy 
is higher than those with other values. Thus, we ﬁx λ to 1e − 4 in the following 
experiments. 
Learning Convergence For PCNN-22 in Table 2.4, the PCNN model is trained for 
200 epochs and then used to perform inference. In Fig. 2.13, we plot training and 
test loss with λ = 0 and λ = 1e −4, respectively. It clearly shows that PCNNs with 
λ = 1e − 4 (blue curves) converge faster than PCNNs with λ = 0 (yellow curves) 
when the epoch number > 150. 
Diversity Visualization In Fig. 2.14, we visualize four channels of the binary 
kernels Dl 
i in the ﬁrst row, the feature maps produced by Dl 
i in the second row,

44
2
Binary Neural Networks
1 
3  4 
2 
Fig. 2.14 Illustration of binary kernels Dl 
i (ﬁrst row), feature maps produced by Dl 
i (second row), 
and corresponding feature maps after binarization (third row) when J = 4. This conﬁrms the 
diversity in PCNNs 
and the corresponding feature maps after binarization in the third row when J = 4. 
This way helps illustrate the diversity of kernels and feature maps in PCNNs. Thus, 
multiple projection functions can capture diverse information and perform highly 
based on compressed models. 
References 
1. Milad Alizadeh, Javier Fernández-Marqués, Nicholas D Lane, and Yarin Gal. An empirical 
study of binary neural networks’ optimisation. In Proceedings of the International Conference 
on Learning Representations, 2018. 
2. Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal 
operators. arXiv preprint arXiv:1810.00861, 2018. 
3. Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes, Alex M 
Bronstein, and Avi Mendelson. Uniq: Uniform noise injection for non-uniform quantization of 
neural networks. arXiv preprint arXiv:1804.10969, 2018. 
4. Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, and Christoph Meinel. 
Melius-
net: Can binary neural networks achieve mobilenet-level accuracy?
arXiv preprint 
arXiv:2001.05936, 2020.

References
45
5. Joseph Bethge, Marvin Bornstein, Adrian Loy, Haojin Yang, and Christoph Meinel. Training 
competitive binary neural networks from scratch. arXiv preprint arXiv:1812.01965, 2018. 
6. Joseph Bethge, Haojin Yang, Marvin Bornstein, and Christoph Meinel. 
Binarydensenet: 
developing an architecture for binary neural networks. 
In Proceedings of the IEEE/CVF 
International Conference on Computer Vision Workshops, pages 0–0, 2019. 
7. Leo Breiman. Bias, variance, and arcing classiﬁers. Technical report, Tech. Rep. 460, Statistics 
Department, University of California, Berkeley ..., 1996. 
8. Adrian Bulat, Jean Kossaiﬁ, Georgios Tzimiropoulos, and Maja Pantic. Matrix and tensor 
decompositions for training binary neural networks. arXiv preprint arXiv:1904.07852, 2019. 
9. Adrian Bulat and Georgios Tzimiropoulos. Binarized convolutional landmark localizers for 
human pose estimation and face alignment with limited resources. In Proceedings of the IEEE 
International Conference on Computer Vision, pages 3706–3714, 2017. 
10. Adrian Bulat and Georgios Tzimiropoulos. XNOR-Net++: Improved binary neural networks. 
arXiv preprint arXiv:1909.13863, 2019. 
11. John G Carney, Pádraig Cunningham, and Umesh Bhagwan. 
Conﬁdence and prediction 
intervals for neural network ensembles. 
In IJCNN’99. International Joint Conference on 
Neural Networks. Proceedings (Cat. No. 99CH36339), volume 2, pages 1215–1218. IEEE, 
1999. 
12. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep 
neural networks with binary weights during propagations. In Advances in neural information 
processing systems, pages 3123–3131, 2015. 
13. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern 
Recognition, pages 248–255, 2009. 
14. Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. 
Regularizing activation 
distribution for training binarized deep networks. 
In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, pages 11408–11417, 2019. 
15. Ruizhou Ding, Zeye Liu, Rongye Shi, Diana Marculescu, and RD Blanton. LightNN: Filling 
the gap between conventional deep neural networks and binarized networks. In Proceedings of 
the on Great Lakes Symposium on VLSI 2017, pages 35–40, 2017. 
16. Pedro Felzenszwalb and Ramin Zabih. Discrete optimization algorithms in computer vision. 
Tutorial at IEEE International Conference on Computer Vision, 2007. 
17. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei 
Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural 
networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 
4852–4861, 2019. 
18. Jiaxin Gu, Ce Li, Baochang Zhang, J. Han, Xianbin Cao, Jianzhuang Liu, and David S. 
Doermann. 
Projection convolutional neural networks for 1-bit CNNs via discrete back 
propagation. ArXiv, abs/1811.12755, 2018. 
19. Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David 
Doermann. 
Projection convolutional neural networks for 1-bit cnns via discrete back 
propagation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2019. 
20. Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Jianzhuang Liu, Guodong Guo, and 
Rongrong Ji. Bayesian optimized 1-bit cnns. In Proceedings of the IEEE/CVF international 
conference on computer vision, pages 4909–4917, 2019. 
21. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 
Deep residual learning for 
image recognition. In Proceedings of the IEEE conference on computer vision and pattern 
recognition, pages 770–778, 2016. 
22. Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, and 
Roeland Nusselder. 
Latent weights do not exist: Rethinking binarized neural network 
optimization. In Advances in neural information processing systems, pages 7531–7542, 2019. 
23. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. 
Densely 
connected convolutional networks. In Proceedings of the IEEE conference on Computer Vision 
and Pattern Recognition, pages 4700–4708, 2017.

46
2
Binary Neural Networks
24. Kun Huang, Bingbing Ni, and Xiaokang Yang. 
Efﬁcient quantization for neural networks 
with binary weights and low bitwidth activations. In Proceedings of the AAAI Conference on 
Artiﬁcial Intelligence, volume 33, pages 3854–3861, 2019. 
25. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 
Binarized neural networks. In Advances in neural information processing systems, pages 4107– 
4115, 2016. 
26. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quan-
tized neural networks: Training neural networks with low precision weights and activations. 
The Journal of Machine Learning Research, 18(1):6869–6898, 2017. 
27. Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Local binary convolutional 
neural networks. CoRR, abs/1608.06049, 2016. 
28. Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Local binary convolutional 
neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, pages 19–28, 2017. 
29. Mohammad Emtiyaz Khan and Haavard Rue. Learningalgorithms from bayesian principles. 
arXiv preprint arXiv:2002.10778, 2(4), 2020. 
30. Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim. Binaryduo: Reducing gradient 
mismatch in binary activation network by coupling binary activations. 
In International 
Conference on Learning Representations. 
31. Minje Kim and Paris Smaragdis. Bitwise neural networks. arXiv preprint arXiv:1601.06071, 
2016. 
32. Seungryong Kim, Dongbo Min, Stephen Lin, and Kwanghoon Sohn. 
DCTM: Discrete-
continuous transformation matching for semantic ﬂow. 
In Proceedings of the IEEE 
International Conference on Computer Vision, volume 6, 2017. 
33. Diederik P Kingma and Jimmy Ba. 
Adam: A method for stochastic optimization. 
arXiv 
preprint arXiv:1412.6980, 2014. 
34. Emanuel Laude, Jan-Hendrik Lange, Jonas Sch pfer, Csaba Domokos, Leal-Taix? Laura, 
Frank R. Schmidt, Bjoern Andres, and Daniel Cremers. 
Discrete-continuous ADMM for 
transductive inference in higher-order MRFs. In Proceedings of the IEEE/CVF Conference 
on Computer Vision and Pattern Recognition, pages 4539–4548, 2018. 
35. Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural 
network: Squeeze the last bit out with admm. In Proceedings of the AAAI Conference on 
Artiﬁcial Intelligence, pages 3466–3473, 2018. 
36. Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 
2016. 
37. Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao. Performance guaranteed 
network acceleration via high-order residual quantization. 
In Proceedings of the IEEE 
International Conference on Computer Vision, pages 2584–2592, 2017. 
38. Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. 
In Advances in Neural Information Processing Systems, pages 345–353, 2017. 
39. Chunlei Liu, Peng Chen, Bohan Zhuang, Chunhua Shen, Baochang Zhang, and Wenrui Ding. 
SA-BNN: State-aware binary neural network. 
In Proceedings of the AAAI Conference on 
Artiﬁcial Intelligence, volume 35, pages 2091–2099, 2021. 
40. Chunlei Liu, Wenrui Ding, Yuan Hu, Baochang Zhang, Jianzhuang Liu, Guodong Guo, 
and David Doermann. Rectiﬁed binary convolutional networks with generative adversarial 
learning. International Journal of Computer Vision, 129:998–1012, 2021. 
41. Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong 
Ji, and David Doermann. Circulant binary convolutional networks: Enhancing the performance 
of 1-bit DCNNs with circulant back propagation. In Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition, pages 2691–2699, 2019. 
42. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, 
and Alexander C Berg. SSD: Single shot multibox detector. In Proc. of ECCV, 2016. 
43. Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards 
precise binary neural network with generalized activation functions. 
arXiv preprint 
arXiv:2003.03488, 2020.

References
47
44. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real 
net: Enhancing the performance of 1-bit cnns with improved representational capability and 
advanced training algorithm. In Proceedings of the European conference on computer vision 
(ECCV), pages 722–737, 2018. 
45. Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural 
networks with real-to-binary convolutions. In ICLR. 2020. 
46. Xiangming Meng, Roman Bachmann, and Mohammad Emtiyaz Khan. Training binary neural 
networks using the bayesian learning rule. In International conference on machine learning, 
pages 6852–6861. PMLR, 2020. 
47. Luca Mocerino and Andrea Calimera. Tentaclenet: A pseudo-ensemble template for accurate 
binary convolutional neural networks. In 2020 2nd IEEE International Conference on Artiﬁcial 
Intelligence Circuits and Systems (AICAS), pages 261–265. IEEE, 2020. 
48. Jean-Jacques Moreau. Proximité et dualité dans un espace hilbertien. Bulletin de la Société 
mathématique de France, 93:273–299, 1965. 
49. Nikunj C Oza and Stuart J Russell. Online bagging and boosting. In International Workshop 
on Artiﬁcial Intelligence and Statistics, pages 229–236. PMLR, 2001. 
50. Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong 
Liu, and Hao Su. Bipointnet: Binary neural network for point clouds. In Proceedings of the 
International Conference on Learning Representations, 2021. 
51. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and 
Jingkuan Song. 
Forward and backward information retention for accurate binary neural 
networks. 
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 
Recognition, pages 2250–2259, 2020. 
52. Haotong Qin, Ruihao Gong, Xianglong Liu, Ziran Wei, Fengwei Yu, and Jingkuan Song. Ir-
net: Forward and backward information retention for highly accurate binary neural networks. 
arXiv preprint arXiv:1909.10788, 2019. 
53. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet 
classiﬁcation using binary convolutional neural networks. 
In European Conference on 
Computer Vision, pages 525–542. Springer, 2016. 
54. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time 
object detection with region proposal networks. In NeurIPS, 2015. 
55. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng 
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual 
recognition challenge. International journal of computer vision, 115(3):211–252, 2015. 
56. VW-S Tseng, Sourav Bhattachara, Javier Fernández-Marqués, Milad Alizadeh, Catherine 
Tong, and Nicholas D Lane. Deterministic binary ﬁlters for convolutional neural networks. 
International Joint Conferences on Artiﬁcial Intelligence Organization, 2018. 
57. Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng Tao Shen. Tbn: 
Convolutional neural network with ternary inputs and binary weights. In Proceedings of the 
European Conference on Computer Vision (ECCV), pages 315–332, 2018. 
58. Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng Tao Shen. Tbn: 
Convolutional neural network with ternary inputs and binary weights. In Proceedings of the 
European Conference on Computer Vision, pages 315–332, 2018. 
59. Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. Two-step 
quantization for low-bit neural networks. In Proceedings of the IEEE Conference on computer 
vision and pattern recognition, pages 4376–4384, 2018. 
60. Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, and Satoshi Naoi. 
On study of 
the binarized deep neural network for image classiﬁcation. arXiv preprint arXiv:1602.07373, 
2016. 
61. Xiaodi Wang, Baochang Zhang, Ce Li, Rongrong Ji, Jungong Han, Xianbin Cao, and 
Jianzhuang Liu. Modulated convolutional networks. In IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR), pages 840–848, 2018. 
62. Xiaodi Wang, Baochang Zhang, Ce Li, Rongrong Ji, Jungong Han, Xianbin Cao, and 
Jianzhuang Liu. 
Modulated convolutional networks. 
In Proceedings of the IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, pages 840–848, 2018.

48
2
Binary Neural Networks
63. Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. 
Learning channel-wise 
interactions for binary convolutional neural networks. In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, pages 568–577, 2019. 
64. Ziwei Wang, Ziyi Wu, Jiwen Lu, and Jie Zhou. Bidet: An efﬁcient binarized object detector. In 
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 
2049–2058, 2020. 
65. Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning 
approach for deep face recognition. In European Conference on Computer Vision (ECCV), 
pages 499–515, 2016. 
66. Sheng Xu, Yanjing Li, Tiancheng Wang, Teli Ma, Baochang Zhang, Peng Gao, Yu Qiao, 
Jinhu Lü, and Guodong Guo. 
Recurrent bilinear optimization for binary neural networks. 
In European Conference on Computer Vision, pages 19–35. Springer, 2022. 
67. Sheng Xu, Yanjing Li, Bohan Zeng, Teli Ma, Baochang Zhang, Xianbin Cao, Peng Gao, 
and Jinhu Lü. Ida-det: An information discrepancy-aware distillation for 1-bit detectors. In 
European Conference on Computer Vision, pages 346–361. Springer, 2022. 
68. Sheng Xu, Yanjing Li, Junhe Zhao, Baochang Zhang, and Guodong Guo. Poem: 1-bit point-
wise operations based on expectation-maximization for efﬁcient point cloud processing. In 
Proceedings of the British Machine Vision Conference, 2021. 
69. Sheng Xu, Chang Liu, Baochang Zhang, Jinhu Lü, Guodong Guo, and David Doermann. 
Bire-id: Binary neural network for efﬁcient person re-id. ACM Transactions on Multimedia 
Computing, Communications, and Applications (TOMM), 18(1s):1–22, 2022. 
70. Sheng Xu, Zhendong Liu, Xuan Gong, Chunlei Liu, Mingyuan Mao, and Baochang Zhang. 
Amplitude suppression and direction activation in networks for 1-bit faster r-cnn. In Proc. of 
EMDL, 2020. 
71. Sheng Xu, Junhe Zhao, Jinhu Lu, Baochang Zhang, Shumin Han, and David Doermann. Layer-
wise searching for 1-bit detectors. In Proceedings of the IEEE/CVF Conference on Computer 
Vision and Pattern Recognition, pages 5682–5691, 2021. 
72. Zhe Xu and Ray CC Cheung. 
Accurate and compact convolutional neural networks with 
trained binarization. In 30th British Machine Vision Conference, 2019. 
73. Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong Tian, and 
Rongrong Ji. ReCU: Reviving the dead weights in binary neural networks. arXiv preprint 
arXiv:2103.12369, 2021. 
74. Haojin Yang, Martin Fritzsche, Christian Bartz, and Christoph Meinel. BMXNet: An open-
source binary neural network implementation based on MXNet. In Proceedings of the 25th 
ACM international conference on Multimedia, pages 1209–1212, 2017. 
75. Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. 
Binaryrelax: A relaxation approach for training deep neural networks with quantized weights. 
SIAM Journal on Imaging Sciences, 11(4):2205–2223, 2018. 
76. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. 
LQ-Nets: Learned 
quantization for highly accurate and compact deep neural networks. In Proceedings of the 
European conference on computer vision (ECCV), pages 365–382, 2018. 
77. Junhe Zhao, Sheng Xu, Baochang Zhang, Jiaxin Gu, David Doermann, and Guodong Guo. 
Towards compact 1-bit cnns via bayesian learning. International Journal of Computer Vision, 
pages 1–25, 2022. 
78. Feng Zheng, Cheng Deng, and Heng Huang. Binarized neural networks for resource-efﬁcient 
hashing with minimizing quantization loss. In IJCAI, pages 1032–1040, 2019. 
79. Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. 
arXiv preprint arXiv:1612.01064, 2016. 
80. Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network 
or more networks per bit? In Proceedings of the IEEE Conference on Computer Vision and 
Pattern Recognition, pages 4923–4932, 2019. 
81. Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Structured binary 
neural networks for accurate image classiﬁcation and semantic segmentation. In Proceedings 
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 413–422, 2019.

Chapter 3 
Binary Neural Architecture Search 
3.1 
Introduction 
Deep convolutional neural networks (DCNNs) have achieved state-of-the-art per-
formance in various computer vision tasks, including image classiﬁcation, instance 
segmentation, and object detection. The success of DCNNs is attributed to effective 
architecture design. Neural architecture search (NAS) is an emerging approach that 
automates the process of designing neural architectures, replacing manual design. 
NAS has enabled signiﬁcant improvements in performance across a wide 
range of computer vision tasks. Traditionally, network architectures were manually 
designed, but NAS automates this process by generating sophisticated neural 
architectures. Existing NAS methods can be classiﬁed into three main categories: 
evolution-based, reinforcement learning-based, and one-shot-based approaches. 
These methods leverage different optimization strategies to search for the best neural 
architecture for a speciﬁc task. NAS has shown promising results in achieving com-
petitive and even superior performance compared to manually designed networks. 
To speed up the architecture search process, researchers have explored methods 
to reduce the evaluation cost of each candidate architecture. One early approach was 
to share weights between searched and newly generated networks [7]. Later, this 
idea evolved into a more efﬁcient framework called one-shot architecture search. 
In one-shot architecture search, an over-parameterized network or super-network 
that includes all candidate operations is trained only once. The ﬁnal architecture 
is obtained by sampling from this super-network. Different approaches have been 
proposed to implement one-shot architecture search. For example, some methods 
use HyperNet to train the over-parameterized network [4], while others share 
parameters among child models to avoid retraining each candidate architecture from 
scratch [62]. 
Differentiable architecture search (DARTS) is a popular one-shot architecture 
search method that introduces a differentiable framework, combining the search and 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
B. Zhang et al., Neural Networks with Model Compression, 
Computational Intelligence Methods and Applications, 
https://doi.org/10.1007/978-981-99-5068-3_3
49

50
3
Binary Neural Architecture Search
evaluation stages into one [56]. Despite its simplicity, DARTS has some limitations, 
leading researchers to propose improved approaches like PDARTS, which allows 
the depth of searched architectures to grow gradually during the training procedure, 
reducing search time [15]. 
ProxylessNAS is another notable method that adopts a differentiable framework 
and searches architectures on the target task instead of using a proxy-based 
framework [9]. These approaches have signiﬁcantly accelerated the architecture 
search process and led to state-of-the-art neural network architectures. 
Binary neural architecture search replaces the real-valued weights and activations 
with binarized ones, which consumes much less memory and computational 
resources to search binary networks and provides a more promising way to ﬁnd 
network architectures efﬁciently. These methods can be categorized into direct 
binary architecture search and auxiliary binary architecture search. Direct binary 
architecture search yields binary architectures directly from well-designed binary 
search spaces. As the ﬁrst art in this ﬁeld, BNASSubscript 11 [11] effectively reduces search 
time by channel sampling and search space pruning in the early training stages 
for a differentiable NAS. BNASSubscript 22 [43] utilizes diversity in the early search to 
learn better performing binary architectures. BMES [63] learns an efﬁcient binary 
MobileNet [40] architecture through evolution-based search. However, the accuracy 
of the direct binary architecture search can be improved by the auxiliary binary 
architecture search [6]. BATS [6] designs a new search space specially tailored for 
the binary network and incorporates it into the DARTS framework. 
Unlike the methods above, our work is driven by the performance discrepancy 
between the 1-bit neural architecture and its real-valued counterpart. We introduce 
tangent propagation to explore the accuracy discrepancy and further accelerate 
the search process by applying the GGN to the Hessian matrix in optimization. 
Furthermore, we introduce a novel decoupled optimization to address asynchronous 
convergence in such a differentiable NAS process, leading to better-performed 1-bit 
CNNs. The overall framework leads to a novel and effective BNAS process. 
To introduce the advances of the NAS area, we separately introduce the 
representative works in the NAS and binary NAS in the following. 
3.2 
Neural Architecture Search 
3.2.1 
ABanditNAS: Anti-bandit for Neural Architecture 
Search 
Low search efﬁciency has prevented NAS from its practical use, and the introduction 
of adversarial optimization and a more extensive search space further exacerbates 
the issue. Early work directly regards network architecture search as a black-box 
optimization problem in a discrete search space and takes thousands of GPU days. 
To reduce the search space, a common idea is to adopt a cell-based search space [96].

3.2
Neural Architecture Search
51
However, when searching in a vast and complicated search space, prior cell-based 
works may still suffer from memory issues and are computationally intensive with 
the number of meta-architecture. For example, DARTS [56] can only optimize over 
a small subset of eight cells stacked to form a deep network of 20. To increase 
search efﬁciency, we reformulate NAS as a multi-armed bandit problem with a vast 
search space. The multi-armed bandit algorithm targets predicting the best arm in 
a sequence of trials to balance the result and its uncertainty. Likewise, NAS aims 
to get the best operation from an operation pool at each edge of the model with 
ﬁnite optimization steps, similar to the multi-armed bandit algorithm. They are both 
exploration and exploitation problems. Therefore, we tried to introduce the multi-
armed bandit algorithm into NAS. In addition, the multi-armed bandit algorithm 
avoids the gradient descent process and provides good search speed for NAS. 
Unlike traditional upper conﬁdence bound (UCB) bandit algorithms that prefer to 
sample using UCB and focus on exploration, we propose anti-bandit to exploit 
further both UCB and lower conﬁdence bound (LCB) to balance exploration and 
exploitation. We achieve an accuracy-bias trade-off during the search process for 
the operation performance estimation. Using the test performance to identify the 
optimal architecture quickly is desirable. With the help of the anti-bandit algorithm, 
our anti-bandit NAS (ABanditNAS) [10] can handle the vast and complicated search 
space, where the number of operations that deﬁne the space can be 960! 
Speciﬁcally, our proposed anti-bandit algorithm uses UCB to reduce search 
space, and LCB guarantees that every arm is thoroughly tested before abandoning 
it, as shown in Fig. 3.1. Based on the observation that the early optimal operation 
is not necessarily the optimal one in the end, and the worst operations in the 
early stage usually have worse performance in the end [89], we pruned the 
operations with the worst UCB, after enough trials selected by the worst LCB. 
This means that the operations we ﬁnally reserve are certainly a near-optimal 
solution. The more tests conducted, the closer UCB and LCB are to the average 
value. Therefore, LCB increases, and UCB decreases with increasing sampling 
times. Speciﬁcally, operations with poor performance in the early stages, such 
as parameterized operations, will receive more opportunities but are abandoned 
once they are conﬁrmed to be wrong. Meanwhile, weight-free operations will be 
Sampling opera ons 
An -Bandit LCB 
Bi 
Bj 
Reducing the search space 
1 
M 
m m v  
K 
1 
(
1)
 
M 
m m v  
K – 
CONV 
3x3 
CONV 
5x5 
MAX POOL 
3x3
Iden ty
CONV 
3x3 
Depth-Wise 
CONV 3x3 
CONV 
5x5 
CONV 
3x3 
MAX POOL 
3x3 
Depth-Wise 
CONV 3x3
Iden ty 
CONV 
5x5 
MAX POOL 
3x3
Iden ty
Depth-Wise 
CONV 3x3 
Ω
L
– 
log
(i,j)
(i,j) 
k
k,t
(i,j) 
k,t 
2
N
s (o  
)= m  
n 
An -Bandit UCB 
log 
U 
(i, j)
(i, j) 
k
k,t
(i, j) 
k,t 
2
N
s (o  )=  m  +  
n 
Fig. 3.1 ABanditNAS is divided into two steps: sampling using LCB and abandoning using UCB

52
3
Binary Neural Architecture Search
compared only with parameterized operations when well-trained. On the other hand, 
with the operation pruning process, the search space becomes smaller and smaller, 
leading to an efﬁcient search process. 
3.2.1.1 
Anti-Bandit Algorithm 
Our goal is to search for network architectures effectively and efﬁciently. However, 
a dilemma exists for NAS about maintaining a network structure that offers 
signiﬁcant rewards (exploitation) or further investigating other network structures 
(exploration). Based on probability theory, the multi-armed bandit can solve the 
aforementioned exploration-versus-exploitation dilemma, which makes decisions 
among competing choices to maximize their expected gain. Speciﬁcally, we propose 
an anti-bandit that chooses and discards the arm k in the trial based on: 
r overTilde Subscript k Baseline minus delta overTilde Subscript k Baseline less than or equals r Subscript k Baseline less than or equals r overTilde Subscript k Baseline plus delta overTilde Subscript k Baseline comma˜rk −˜δk ≤rk ≤˜rk + ˜δk,
(3.1) 
where r Subscript krk, r overTilde Subscript k˜rk, and delta overTilde Subscript k˜δk are the true reward, the average reward, and the estimated 
variance obtained from arm k. r overTilde Subscript k˜rk is the value term that favors actions that historically 
perform well, and delta overTilde Subscript k˜δk is the exploration term that gives actions an exploration bonus. 
r overTilde Subscript k Baseline minus delta overTilde Subscript k˜rk −˜δk and r overTilde Subscript k Baseline plus delta overTilde Subscript k˜rk + ˜δk can be interpreted as the lower and upper bounds of a conﬁdence 
interval. 
The traditional UCB algorithm, which optimistically substitutes r Subscript k Baseline overTilde plus delta overTilde ˜rk + ˜δ for r Subscript krk, 
emphasizes exploration but ignores exploitation. Unlike the UCB bandit, we further 
exploited the LCB and UCB to balance exploration and exploitation. A smaller LCB 
usually has little expectations but a signiﬁcant variance and should be given a larger 
chance to be sampled for more trials. Then, based on the observation that the worst 
operations in the early stage usually have worse performance at the end [89], we 
use UCB to prune the operation with the worst performance and reduce the search 
space. In summary, we adopt LCB, r Subscript k Baseline overTilde minus delta overTilde ˜rk −˜δ, to sample the arm, which should be 
further optimized, and use UCB, r Subscript k Baseline overTilde plus delta overTilde ˜rk + ˜δ, to abandon the operation with the minimum 
value. Because the variance is bounded and converges, the operating estimate value 
is always close to the actual value and gradually approaches the true value as the 
number of trials increases. Our anti-bandit algorithm overcomes the limitations 
of an exploration-based strategy, including levels of understanding and suboptimal 
gaps. The deﬁnitions of the value and variance terms and the proof of our proposed 
method are shown below. 
Deﬁnition 1 If an operation on arm k has been recommended n Subscript knk times, r e w a r d Subscript irewardi is 
the reward on arm  k on all trails. The value term of anti-bandit is deﬁned as follows: 
r Subscript k Baseline overTilde equals StartFraction sigma summation r e w a r d Subscript i Baseline Over n Subscript k Baseline EndFraction period ˜rk =
ε rewardi
nk
.
(3.2) 
The value of selecting an operation r Subscript k Baseline overTilde˜rk is the expected reward sigma summation r e w a r d Subscript iε rewardi we receive 
when we take an operation from the possible set of operations. If n Subscript knk approaches

3.2
Neural Architecture Search
53
inﬁnity, r Subscript k Baseline overTilde˜rk approaches the actual value of the operation r Subscript krk. However, the number of 
operations n Subscript knk cannot be inﬁnite. Therefore, we should approximate the actual value 
as closely as possible through the variance. 
Deﬁnition 2 There exists a difference between the estimated probability r Subscript k Baseline overTilde˜rk and the 
actual probability r Subscript krk, and we can estimate the variance concerning the value: 
delta overTilde Subscript k Baseline equals StartRoot StartFraction 2 ln upper N Over n EndFraction EndRoot comma˜δk =
/
2 ln N
n
,
(3.3) 
where N is the total number of trails. 
Proof Suppose upper X element of left bracket 0 comma 1 right bracketX ∈[0, 1] represents the theoretical value of each independently 
distributed operation. n is the number of times the arm has been played up to trial, 
and p Subscript ipi is the actual value of the operation in the i t hith trial. Furthermore, we deﬁne 
p equals StartFraction sigma summation Underscript i Endscripts p Subscript i Baseline Over n EndFractionp =
ε
i pi
n
and q equals 1 minus pq = 1 −p. Since the variance boundary of independent operations 
can represent the global variance boundary (see the Appendix), based on Markov’s 
inequality, we can arrive at below: 
StartLayout 1st Row 1st Column upper P left bracket upper X greater than p plus delta right bracket 2nd Column equals upper P left bracket sigma summation Underscript i Endscripts left parenthesis upper X Subscript i Baseline minus p Subscript i Baseline right parenthesis greater than delta right bracket 2nd Row 1st Column Blank 2nd Column equals upper P left bracket e Superscript lamda sigma summation Underscript i Endscripts left parenthesis upper X Super Subscript i Superscript minus p Super Subscript i Superscript right parenthesis Baseline greater than e Superscript lamda delta Baseline right bracket 3rd Row 1st Column Blank 2nd Column less than or equals StartFraction upper E left bracket e Superscript lamda sigma summation Underscript i Endscripts left parenthesis upper X Super Subscript i Superscript minus p Super Subscript i Superscript right parenthesis Baseline right bracket Over e Superscript lamda delta Baseline EndFraction period EndLayout
P[X > p + δ] = P[
ε
i
(Xi −pi) > δ]
= P[eλ ε
i(Xi−pi) > eλδ]
≤E[eλ ε
i(Xi−pi)]
eλδ
.
(3.4) 
Since we can get 1 plus x less than or equals e Superscript x Baseline less than or equals 1 plus x plus x squared1 + x ≤ex ≤1 + x + x2 when 0 less than or equals StartAbsoluteValue x EndAbsoluteValue less than or equals 1 right parenthesis0 ≤|x| ≤1), upper E left bracket e Superscript lamda sigma summation Underscript i Endscripts left parenthesis upper X Super Subscript i Superscript minus p Super Subscript i Superscript right parenthesis Baseline right bracketE[eλ ε
i(Xi−pi)]
in Eq. 3.4 can be further approximated as follows: 
StartLayout 1st Row 1st Column upper E left bracket e Superscript lamda sigma summation Underscript i Endscripts left parenthesis upper X Super Subscript i Superscript minus p Super Subscript i Superscript right parenthesis Baseline right bracket 2nd Column equals product Underscript i Endscripts upper E left bracket e Superscript lamda left parenthesis upper X Super Subscript i Superscript minus p Super Subscript i Superscript right parenthesis Baseline right bracket 2nd Row 1st Column Blank 2nd Column less than or equals product Underscript i Endscripts upper E left bracket 1 plus lamda left parenthesis upper X Subscript i Baseline minus p Subscript i Baseline right parenthesis plus lamda squared left parenthesis upper X Subscript i Baseline minus p Subscript i Baseline right parenthesis squared right bracket 3rd Row 1st Column Blank 2nd Column equals product Underscript i Endscripts left parenthesis 1 plus lamda squared v Subscript i Superscript 2 Baseline right parenthesis 4th Row 1st Column Blank 2nd Column less than or equals e Superscript lamda squared v squared Baseline comma EndLayout
E[eλ ε
i(Xi−pi)] =
||
i
E[eλ(Xi−pi)]
≤
||
i
E[1 + λ(Xi −pi) + λ2(Xi −pi)2]
=
||
i
(1 + λ2v2
i )
≤eλ2v2,
(3.5) 
where v denotes the variance of X. Combining Eqs. 3.4 and 3.5 gives upper P left bracket upper X greater than p plus delta right bracket less than or equals StartFraction e Superscript lamda squared v squared Baseline Over e Superscript lamda delta Baseline EndFractionP[X > p +
δ] ≤eλ2v2
eλδ . Since lamdaλ is a positive constant, it can be obtained by the transformation of 
the values upper P left bracket upper X greater than p plus delta right bracket less than or equals e Superscript minus 2 n delta squaredP[X > p + δ] ≤e−2nδ2. According to the symmetry of the distribution, 
we have upper P left bracket upper X less than p minus delta right bracket less than or equals e Superscript minus 2 n delta squaredP[X < p −δ] ≤e−2nδ2. Finally, we get the following inequality: 
upper P left bracket StartAbsoluteValue upper X minus p EndAbsoluteValue less than or equals delta right bracket greater than or equals 1 minus 2 e Superscript minus 2 n delta squared Baseline periodP[|X −p| ≤δ] ≥1 −2e−2nδ2.
(3.6)

54
3
Binary Neural Architecture Search
We need to decrease deltaδ as operating recommendations increase. Therefore, we 
choose StartRoot StartFraction 2 ln upper N Over n EndFraction EndRoot
/
2 ln N
n
as delta overTilde˜δ. That is to say, p minus StartRoot StartFraction 2 ln upper N Over n EndFraction EndRoot less than or equals upper X less than or equals p plus StartRoot StartFraction 2 ln upper N Over n EndFraction EndRootp −
/
2 ln N
n
≤X ≤p +
/
2 ln N
n
is implemented 
at least with probability 1 minus StartFraction 2 Over upper N Superscript 4 Baseline EndFraction1 −
2
N4 . The variance value will gradually decrease as the 
trial progresses, and r Subscript k Baseline overTilde˜rk will gradually approach r Subscript krk. Equation 3.7 shows that we can 
achieve a probability of 0.992 when the number of the trial gets 4: 
StartRoot 1 minus StartFraction 2 Over upper N Superscript 4 Baseline EndFraction EndRoot equals StartLayout Enlarged left brace 1st Row 1st Column 0.857 2nd Column upper N equals 2 2nd Row 1st Column 0.975 2nd Column upper N equals 3 3rd Row 1st Column 0.992 2nd Column upper N equals 4 period EndLayout
/
1 −2
N4 =
⎧
⎪⎪⎨
⎪⎪⎩
0.857
N = 2
0.975
N = 3
0.992
N = 4.
(3.7) 
According to Eq. 3.6, the variance in the anti-bandit algorithm is bounded, and 
the lower/upper conﬁdence bounds can be estimated as: 
r Subscript k Baseline overTilde minus StartRoot StartFraction 2 ln upper N Over n EndFraction EndRoot less than or equals r Subscript k Baseline less than or equals r Subscript k Baseline overTilde plus StartRoot StartFraction 2 ln upper N Over n EndFraction EndRoot period ˜rk −
/
2 ln N
n
≤rk ≤˜rk +
/
2 ln N
n
.
(3.8) 
3.2.1.2 
Search Space 
Following [56, 89, 96], we search for computation cells as the building blocks of 
the ﬁnal architecture. A cell is a fully connected directed acyclic graph (DAG) of 
M nodes, i.e., StartSet upper B 1 comma upper B 2 comma ellipsis comma upper B Subscript upper M Baseline EndSet{B1, B2, . . . , BM} as shown in Fig. 3.2a. Here, each node is a speciﬁc 
tensor (e.g., a feature map in convolutional neural networks), and each directed edge 
left parenthesis i comma j right parenthesis(i, j) between upper B Subscript iBi and upper B Subscript jBj denotes an operation o Superscript left parenthesis i comma j right parenthesis Baseline left parenthesis period right parenthesiso(i,j)(.), which is sampled from 
Fig. 3.2 (a) A cell containing four intermediate nodes, namely, upper B 1B1, upper B 2B2, upper B 3B3, upper B 4B4, which apply 
sampled operations on the input node upper B 0B0. upper B 0B0 is from the output of the last cell. The output node 
concatenates the outputs of the four intermediate nodes. (b) Gabor ﬁlter. (c) A generic denoising 
block. Following [81], it wraps the denoising operation with a 1 times 11 × 1 convolution and an identity 
skip connection [36]

3.2
Neural Architecture Search
55
upper Omega Superscript left parenthesis i comma j right parenthesis Baseline equals StartSet o 1 Superscript left parenthesis i comma j right parenthesis Baseline comma ellipsis comma o Subscript upper K Superscript left parenthesis i comma j right parenthesis Baseline EndSeto(i,j) = {o(i,j)
1
, . . . , o(i,j)
K
}. StartSet upper Omega Superscript left parenthesis i comma j right parenthesis Baseline EndSet{o(i,j)} is the search space of a cell. Each node upper B Subscript jBj
takes its dependent nodes as input and can be obtained by upper B Subscript j Baseline equals upper Sigma Subscript i less than j Baseline o Superscript left parenthesis i comma j right parenthesis Baseline left parenthesis upper B Subscript i Baseline right parenthesisBj = Σi<jo(i,j)(Bi). 
The constraint i less than ji < j here is to avoid cycles in a cell. Each cell takes the output 
of the last cell as input. For brevity, we denote by upper B 0B0 the last node of the previous 
cell and the ﬁrst node of the current cell. Unlike existing approaches that use only 
normal and reduction cells, we search for v (v greater than 2v > 2) cells instead. For general NAS 
search, we follow [56] and take seven normal operations, i.e., 3 times 33 × 3 max pooling, 
3 times 33×3 average pooling, skip connection (identity), 3 times 33×3 convolution with rate 2, 5 times 55×5
convolution with rate 2, 3 times 33 × 3 depth-wise separable convolution, and 5 times 55 × 5 depth-
wise separable convolution. Considering adversarially robust optimization for NAS, 
we introduce two additional operations, the 3 times 33 × 3 Gabor ﬁlter and denoising block, 
for model defense. Therefore, the size of the entire search space is upper K Superscript StartAbsoluteValue script upper E Super Subscript script upper M Superscript EndAbsoluteValue times vK|EM|×v, where 
script upper E Subscript script upper MEM is the set of possible edges with M intermediate nodes in the fully connected 
DAG. In the case with upper M equals 4M = 4 and v equals 6v = 6, together with the input node, the total 
number of cell structures in the search space is 9 Superscript left parenthesis 1 plus 2 plus 3 plus 4 right parenthesis times 6 Baseline equals 9 Superscript 10 times 69(1+2+3+4)×6 = 910×6. Here, we 
brieﬂy introduce the two additional operations. 
Gabor ﬁlter Gabor ﬁlters [24, 25] containing frequency and orientation rep-
resentations can characterize the spatial frequency structure in images while 
preserving spatial relationships. This operation provides superb robustness for the 
network [64]. Gabor ﬁlters are deﬁned as exp left parenthesis minus StartFraction x Superscript prime 2 Baseline plus gamma squared y Superscript prime 2 Baseline Over 2 sigma squared EndFraction right parenthesis cosine left parenthesis 2 pi StartFraction x prime Over lamda EndFraction plus psi right parenthesisexp(−x'2+γ 2y'2
2σ 2
) cos(2π x'
λ + ψ). Here, 
x prime equals x cosine theta plus y sine thetax' = x cos θ + y sin θ and y prime equals minus x sine theta plus y cosine thetay' = −x sin θ + y cos θ. sigmaσ, gammaγ , lamdaλ, psiψ, and thetaθ are learnable 
parameters. Note that the symbols used here apply only to the Gabor ﬁlter and 
are different from the symbols used in the rest of this paper. Figure 3.2b shows  
an example of Gabor ﬁlters. 
Denoising block As described in [81], adversarial perturbations on images will 
introduce noise in the features. Therefore, denoising blocks can improve adversarial 
robustness by denoising features. Following this, we add the nonlocal mean 
denoising block [5] as shown in Fig. 3.2c to the search space to denoise the 
features. Calculate a denoised feature map z of an input feature map x by taking 
a weighted mean of the spatial locations of the features in general script upper LL as z Subscript p Baseline equals StartFraction 1 Over upper C left parenthesis x right parenthesis EndFraction sigma summation Underscript for all q element of script upper L Endscripts f left parenthesis x Subscript p Baseline comma x Subscript q Baseline right parenthesis dot x Subscript qzp =
1
C(x)
ε
∀q∈L f (xp, xq) · xq, where f left parenthesis x Subscript p Baseline comma x Subscript q Baseline right parenthesisf (xp, xq) is a feature-dependent weighting 
function and upper C left parenthesis x right parenthesisC(x) is a normalization function. 
3.2.1.3 
Anti-bandit Strategy for NAS 
As described in [85, 89], the validation accuracy ranking of different network 
architectures is not a reliable indicator of the ﬁnal quality of the architecture. 
However, the experimental results suggest that if an architecture performs poorly 
at the beginning of training, there is little hope that it can be part of the ﬁnal 
optimal model [89]. As training progresses, this observation becomes more and 
more speciﬁc. Based on this observation, we derive a simple but effective training 
strategy. During training and the increasing epochs, we progressively abandon the

56
3
Binary Neural Architecture Search
worst-performing operation and sample the operations with little expectations but a 
signiﬁcant variance for each edge. Unlike [89], which uses the performance as the 
evaluation metric to decide which operation should be pruned, we use the anti-bandit 
algorithm described in Sect. 3.2.1.1 to decide. 
Following UCB in the bandit algorithm, we obtain the initial performance for 
each operation on every edge. Speciﬁcally, we sample one of the K operations in 
upper Omega Superscript left parenthesis i comma j right parenthesiso(i,j) for every edge, and then obtain the validation accuracy a, which is the initial 
performance m Subscript k comma 0 Superscript left parenthesis i comma j right parenthesism(i,j)
k,0 by adversarially training the sampled network for one epoch and 
ﬁnally assigning this accuracy to all the sampled operations. 
By considering the conﬁdence of the kth operation using Eq. 3.8, the LCB is 
calculated by: 
s Subscript upper L Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis equals m Subscript k comma t Superscript left parenthesis i comma j right parenthesis Baseline minus StartRoot StartFraction 2 log upper N Over n Subscript k comma t Superscript left parenthesis i comma j right parenthesis Baseline EndFraction EndRoot commasL(o(i,j)
k
) = m(i,j)
k,t
−
/
2 log N
n(i,j)
k,t
,
(3.9) 
where N is the total number of samples, n Subscript k comma t Superscript left parenthesis i comma j right parenthesisn(i,j)
k,t
refers to the number of times the 
kth operation of the edge left parenthesis i comma j right parenthesis(i, j) has been selected, and t is the epoch index. The 
ﬁrst item in Eq. 3.9 is the value term (see Eq. 3.2) which favors the operations 
that look good historically. The second is the exploration term (see Eq. 3.3), which 
allows operations to get an exploration bonus that grows with log upper Nlog N. The selection 
probability for each operation is deﬁned as: 
p left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis equals StartFraction exp left brace minus s Subscript upper L Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis right brace Over sigma summation Underscript m Endscripts exp left brace minus s Subscript upper L Baseline left parenthesis o Subscript m Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis right brace EndFraction periodp(o(i,j)
k
) =
exp{−sL(o(i,j)
k
)}
ε
m exp{−sL(o(i,j)
m
)}
.
(3.10) 
The minus sign in Eq. 3.10 means we prefer to sample operations with smaller 
conﬁdence. After sampling one operation for every edge based on p left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesisp(o(i,j)
k
), we  
obtain the validation accuracy a by training adversarially the sampled network for 
one epoch and then update the performance m Subscript k comma t Superscript left parenthesis i comma j right parenthesism(i,j)
k,t
that historically indicates the 
validation accuracy of all the sampled operations o Subscript k Superscript left parenthesis i comma j right parenthesiso(i,j)
k
as: 
m Subscript k comma t Superscript left parenthesis i comma j right parenthesis Baseline equals left parenthesis 1 minus lamda right parenthesis m Subscript k comma t minus 1 Superscript left parenthesis i comma j right parenthesis Baseline plus lamda asterisk a commam(i,j)
k,t
= (1 −λ)m(i,j)
k,t−1 + λ ∗a,
(3.11) 
where lamdaλ is a hyperparameter. 
Finally, after upper K asterisk upper TK ∗T samples where T is a hyperparameter, we calculate the 
conﬁdence with the UCB according to Eq. 3.8 as: 
s Subscript upper U Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis equals m Subscript k comma t Superscript left parenthesis i comma j right parenthesis Baseline plus StartRoot StartFraction 2 log upper N Over n Subscript k comma t Superscript left parenthesis i comma j right parenthesis Baseline EndFraction EndRoot periodsU(o(i,j)
k
) = m(i,j)
k,t
+
/
2 log N
n(i,j)
k,t
.
(3.12) 
The operation with minimal UCB for every edge is abandoned. This means that 
operations that are given more opportunities but result in poor performance are

3.2
Neural Architecture Search
57
removed. With this pruning strategy, the search space is signiﬁcantly reduced from 
StartAbsoluteValue upper Omega Superscript left parenthesis i comma j right parenthesis Baseline EndAbsoluteValue Superscript 10 times 6|o(i,j)|10×6 to left parenthesis StartAbsoluteValue upper Omega Superscript left parenthesis i comma j right parenthesis Baseline EndAbsoluteValue minus 1 right parenthesis Superscript 10 times 6(|o(i,j)| −1)10×6, and the reduced space becomes: 
upper Omega Superscript left parenthesis i comma j right parenthesis Baseline left arrow upper Omega Superscript left parenthesis i comma j right parenthesis Baseline minus StartSet arg min Underscript o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline Endscripts s Subscript upper U Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis EndSet comma for all left parenthesis i comma j right parenthesis periodo(i,j) ←o(i,j) −{arg min
o(i,j)
k
sU(o(i,j)
k
)}, ∀(i, j).
(3.13) 
The reduction procedure is repeated until the optimal structure is obtained, where 
only one operation is left on each edge. 
Complexity Analysis There are script upper O left parenthesis upper K Superscript StartAbsoluteValue script upper E Super Subscript script upper M Superscript EndAbsoluteValue times v Baseline right parenthesisO(K|EM|×v) combinations in the search space 
discovery process with v types of different cells. In contrast, ABanditNAS reduces 
the search space for every upper K asterisk upper TK ∗T epoch. Therefore, the complexity of the proposed 
method is the following: 
script upper O left parenthesis upper T times sigma summation Underscript k equals 2 Overscript upper K Endscripts k right parenthesis equals script upper O left parenthesis upper T upper K squared right parenthesis periodO(T ×
K
ε
k=2
k) = O(T K2).
(3.14) 
3.2.1.4 
Adversarial Optimization 
The goal of adversarial training [58] is to learn networks that are robust to 
adversarial attacks. Given a network f Subscript thetafθ parameterized by thetaθ, a dataset left parenthesis x Subscript e Baseline comma y Subscript e Baseline right parenthesis(xe, ye), a  
loss function l, and a threat model upper DeltaA, the learning problem can be formulated as the 
following optimization problem: min Underscript theta Endscripts sigma summation Underscript e Endscripts max Underscript delta element of upper Delta Endscripts l left parenthesis f Subscript theta Baseline left parenthesis x Subscript e Baseline plus delta right parenthesis comma y Subscript e Baseline right parenthesisminθ
ε
e maxδ∈A l
(
fθ(xe + δ), ye
)
, where deltaδ is the 
adversarial perturbation. In this paper, we consider the typical l Subscript normal infinityl∞threat model [58], 
upper Delta equals StartSet delta colon parallel to delta parallel to Subscript normal infinity Baseline less than or equals epsilon EndSetA = {δ : ||δ||∞≤ε} for some epsilon greater than 0ε > 0. Here, parallel to dot parallel to Subscript normal infinity|| · ||∞is the l Subscript normal infinityl∞norm distance metric 
and epsilonε is the adversarial manipulation budget. The adversarial training procedure uses 
attacks to approximate inner maximization over upper DeltaA, followed by some variation of 
gradient descent on model parameters thetaθ. For example, one of the earliest versions of 
adversarial training uses the fast gradient sign method (FGSM) [29] to approximate 
the inner maximization. This could be seen as a relatively inaccurate approximation 
of inner maximization for l Subscript normal infinityl∞perturbations, and it has the closed-form solution: 
theta equals epsilon dot sign left parenthesis nabla Subscript x Baseline l left parenthesis f left parenthesis x right parenthesis comma y right parenthesis right parenthesisθ = ε·sign
(
∇xl
(
f (x), y
))
. A better approximation of inner maximization is to take 
multiple smaller FGSM steps of size alphaα instead. However, the number of gradient 
computations caused by the multiple steps is proportional to script upper O left parenthesis upper E upper F right parenthesisO(EF) in a single 
epoch, where E is the size of the dataset and F is the number of steps taken by the 
adversary PGD. This is F times higher than standard training with script upper O left parenthesis upper E right parenthesisO(E) gradient 
computations per epoch, and adversarial training is typically F times slower. To 
accelerate adversarial training, we combine FGSM with random initialization [77] 
for our ABanditNAS. Our ABanditNAS with adversarial training is summarized in 
Algorithm 3.

58
3
Binary Neural Architecture Search
Algorithm 3: ABanditNAS with adversarial training 
A set o f algorit hms fo r ABanditNA S with  adversari al training. It calls the  inputs and c alc
ulates the o utput structu re wit h t  = 0 an d c = 0. It calcu lat es S subs cr i pt 
L and p,  tra ins the se lected a rchitecture
 advers arially
, ge ts the a ccuracy, and  updates
 the  p
erform ance.
3.9A 
set of alg orithms fo
r 
ABanditN AS with adversarial training. It calls the inputs and calculates the output structure with t = 0 and c = 0. It calculates S subscript L and p, trains the selected architecture adversarially, gets the accuracy, and updates the performance. 3.10A 
set of alg orithms for A Ban ditNAS wi th a dversarial  train ing . It calls
 t
he inp uts and cal
culate s the 
ou tput s truc ture with  t = 0 and c = 0. It calcula
tes S subs c r i p t  L a
nd p, trains th e se
lected archite
c
tur
e
 adver sarial
ly
, 
get s t
h
e accu racy, 
a
nd
 update s t
h
e perfo rmance
.
3.11A 
set  of  al go rithm
s for ABan ditNAS with
 a
dversari al training. It calls the inputs and calculates the output structure with t = 0 and c = 0. It calculates S subscript L and p, trains the selected architecture adversarially, gets the accuracy, and updates the performance. 3.12A 
set of algo rithms for AB anditNAS with a dversarial training. It calls the inputs and calculates the output structure with t = 0 and c = 0. It calculates S subscript L and p, trains the selected architecture adversarially, gets the accuracy, and updates the performance. 3.13A 
set  of
 algorithms
 for
 ABanditNAS with adversarial training. It calls the inputs and calculates the output structure with t = 0 and c = 0. It calculates S subscript L and p, trains the selected architecture adversarially, gets the accuracy, and updates the performance.
3.2.1.5 
Analysis 
Effect on the hyperparameter lamdaλ The hyperparameter lamdaλ balances the performance 
between the past and the current. Different values of lamdaλ result in similar search costs. 
The performance of the structures searched by ABanditNAS with different values of 
lamdaλ is used to ﬁnd the best lamdaλ. We train the structures in the same setting. From Fig. 3.3, 
we can see that when lamda equals 0.7λ = 0.7, ABanditNAS is most robust. 
Effect on the search space We test the performance of ABanditNAS with different 
search spaces. We adopt the same experimental setting as the general NAS in this 
part. The search space of the general NAS has seven operations. We incrementally 
add the Gabor ﬁlter, denoising block, 1×1 dilated convolution with rate 2, and 7×7 
dilated convolution with rate 2 until the number of operations in the search space 
reaches 11. In Table 3.1, # Search Space represents the number of operations in the 
search space. Although the search difﬁculty increases with increasing search space, 
ABanditNAS can effectively select the appropriate operations. Each additional

3.2
Neural Architecture Search
59
Fig. 3.3 Performances of structures searched by ABanditNAS with different hyperparameter 
values λ 
Table 3.1 The performance of ABanditNAS with different search spaces on CIFAR10 
# Search
Accuracy
# Params
Search cost
Search 
Architecture
space
(%)
(M)
(GPU days)
method 
ABanditNAS
7
97.13
3.0
0.09
Anti-bandit 
ABanditNAS
8
97.47
3.3
0.11
Anti-bandit 
ABanditNAS
9
97.52
4.1
0.13
Anti-bandit 
ABanditNAS
10
97.53
2.7
0.15
Anti-bandit 
ABanditNAS
11
97.66
3.7
0.16
Anti-bandit 
operation has little effect on search efﬁciency, demonstrating the efﬁciency of 
our search method. When the number of operations in the search space is 9, the 
classiﬁcation accuracy of the model searched by ABanditNAS exceeds all the 
methods with the same level of search cost. 
3.2.2 
IDARTS: Interactive Differentiable Architecture Search 
In part, NAS has signiﬁcantly impacted computer vision by reducing the need for 
manual work. Recently, Liu et al. [56] proposed differentiable architecture search 
(DARTS) as an alternative that makes architecture search more efﬁcient. DARTS 
relaxes the search space to be continuous and differentiable. DARTS learns the

60
3
Binary Neural Architecture Search
weight of each operation with gradient descent so that the architecture can be 
optimized concerning its validation set performance by gradient descent. Despite 
its sophisticated design, DARTS is still subject to an ample yet redundant space 
of network architectures and thus suffers from signiﬁcant memory and compu-
tation overhead. To address the problems of DARTS, researchers have proposed 
alternative formulations, including PDARTS [16], DARTS+ [49], PC-DARTS [82], 
ProxylessNAS [9], CDARTS [86], Fair DARTS [20], and SGAS [47]. Among them, 
PC-DARTS [82] reduces redundancy in the network space by performing a more 
efﬁcient search without compromising performance. PC-DARTS only samples a 
subset of channels in a super-net during the search to reduce computation and 
introduces edge normalization to stabilize the search for network connectivity by 
explicitly learning an extra set of edge-selection parameters. 
However, these DARTS alternatives need to pay more attention to the intrinsic 
relationship between different parameters, and as a result, the selected architecture 
could be more robust due to an insufﬁcient training process. The reason is that the 
coupling relationship will affect the training of the network architecture to its limit 
before it is selected or left out. To address this issue, we introduce a bilinear model 
into DARTS and develop a new backpropagation method to decouple the hidden 
relationships among variables to facilitate the optimization process. To the best of 
our knowledge, few works have formulated DARTS as a bilinear problem. 
We address these issues by formulating DARTS as a bilinear optimization 
problem and developing the efﬁcient interactive differentiable architecture search. 
Figure 3.4 shows the framework of IDARTS [84]. Figure 3.4b shows that the dotted 
line results are inefﬁcient compared with IDARTS shown in the solid line. t 1t1, 
t 2t2 marks the results where the architecture parameter alphaα is backtracked. IDARTS 
coordinates the training of different parameters and fully explores their interaction 
based on the backtracking method. Our method allows operations to be selected only 
Fig. 3.4 An  overview of IDARTS.  (a) alphaα and betaβ are coupled in IDARTS. The edge and operation 
(beta Subscript lβl and alpha Subscript lαl) are coupled during the neural architecture search. x Subscript ixi and x Subscript jxj represent node 0 and node 
2, respectively. x Subscript j Baseline equals alpha Subscript l comma m Baseline dot beta l dot upper W Subscript l comma m Baseline circled times x Subscript ixj = αl,m · βl · Wl,m ⊗xi is speciﬁcally described in Eq. 3.16. (b) A backtracking 
method is introduced to coordinate the training of different parameters, which can fully explore 
their interaction during training. The dotted line results indicate that the lack of backtracking leads 
to the inadequate training of alphaα, and the solid line indicates an efﬁcient training of IDARTS

3.2
Neural Architecture Search
61
when they are sufﬁciently trained. We evaluate our IDARTS on image classiﬁcation 
and conduct experiments on the CIFAR10 and ImageNet datasets. The experimental 
results show that IDARTS achieves superior performance compared to existing 
DARTS approaches, including PC-DARTS [82], CDARTS [86], and FairDARTS 
[20]. 
3.2.2.1 
Bilinear Models for DARTS 
We ﬁrst show how DARTS can be formulated as a bilinear optimization problem. 
Assume that there are L edges in a cell, and the edge between node upper N Subscript iNi and node upper N Subscript jNj
is the lth edge. Following [56, 82], we take the lth edge, which is formulated as: 
StartLayout 1st Row 1st Column f Subscript l Baseline left parenthesis bold upper W Subscript l comma m Baseline comma bold x Subscript bold i Baseline right parenthesis 2nd Column equals sigma summation Underscript o Subscript l comma m Baseline element of script upper O Subscript left parenthesis l right parenthesis Baseline Endscripts alpha Subscript l comma m Baseline dot o Subscript l comma m Baseline bold left parenthesis bold upper W Subscript l comma m Baseline circled times bold x Subscript bold i Baseline right parenthesis comma EndLayout fl(Wl,m, xi) =
ε
ol,m∈O(l)
αl,m · ol,m(Wl,m ⊗xi),
(3.15) 
where bold upper W Subscript l comma mWl,m denotes the kernels of the mth convolution operation. We assume that 
there are M operations on one edge. M refers to the number of all operations. bold x Subscript bold ixi
denotes the feature map of upper N Subscript iNi, script upper O Subscript lOl denotes the set of operations, and alpha Subscript l comma mαl,m is the 
parameter of operation o Subscript l comma mol,m on lth edge processed by softmax operation: 
StartLayout 1st Row 1st Column bold x Subscript bold j 2nd Column equals sigma summation Underscript i less than j Endscripts StartSet beta Subscript l Baseline EndSet dot f Subscript l Baseline left parenthesis x Subscript i Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript i less than j Endscripts sigma summation Underscript o Subscript l comma m Baseline element of script upper O Subscript l Baseline Endscripts beta Subscript l Baseline alpha Subscript l comma m Baseline dot o Subscript l comma m Baseline left parenthesis bold upper W Subscript bold l bold comma bold m Baseline circled times bold x Subscript bold i Baseline right parenthesis comma EndLayout
xj =
ε
i<j
{βl} · fl(xi)
=
ε
i<j
ε
ol,m∈Ol
βlαl,m · ol,m(Wl,m ⊗xi),
(3.16) 
where beta Subscript lβl denotes the parameter of lth edge. To calculate the ﬁnal architecture, the 
softmax is deﬁned on betaβ and alphaα. For each intermediate node, we will choose two 
edges, which are jointly determined by alphaα and betaβ. In Fig. 3.4, we see  alphaα and betaβ are 
coupled in the inference process as shown in Eq. 3.16. x Subscript jxj is linearly dependent 
on both alphaα and betaβ, a classic bilinear problem. If an improper operation is selected, 
it will affect the selection of the edge and vice versa. It suggests that we should 
consider their relationship for better optimization. A basic bilinear optimization 
problem attempts to optimize the following objective function in the architecture 
search: 
arg min Underscript bold italic beta comma bold italic alpha Endscripts upper G left parenthesis bold upper W comma bold italic beta comma bold italic alpha right parenthesis equals arg min Underscript bold italic beta comma bold italic alpha Endscripts left parenthesis script upper L left parenthesis bold upper W comma bold italic beta comma bold italic alpha right parenthesis plus upper R left parenthesis bold italic beta right parenthesis right parenthesis comma arg min
β,α
G(W, β, α) = arg min
β,α
(L(W, β, α) + R(β)),
(3.17) 
where alpha element of double struck upper R Superscript upper L times upper Mα ∈RL×M and beta element of double struck upper R Superscript upper L times 1β ∈RL×1 are variables to be optimized, alphaα denotes the 
matrix, L is the number of edges, M is the number of operations at each edge, and 
upper R left parenthesis dot right parenthesisR(·) represents the constraint, which determines where the backtracking occurs. 
script upper L left parenthesis dot right parenthesisL(·) denotes the loss function in the original DARTS models.

62
3
Binary Neural Architecture Search
Following [56, 82], the weights of the kernels bold upper WW and the architectural parameters 
bold italic alphaα, bold italic betaβ are optimized sequentially. The learning procedure for the architectural 
parameters involves an optimization as: 
StartLayout 1st Row 1st Column Blank 2nd Column bold upper W Superscript bold t plus bold 1 Baseline equals arg min Underscript bold upper W Endscripts script upper L Subscript t r a i n Baseline left parenthesis bold upper W Superscript bold t Baseline comma alpha Superscript t Baseline comma beta Superscript t Baseline right parenthesis comma 2nd Row 1st Column Blank 2nd Column alpha Superscript t plus 1 Baseline equals arg min Underscript alpha Endscripts script upper L Subscript v a l Baseline left parenthesis bold upper W Superscript bold t plus bold 1 Baseline comma alpha Superscript t Baseline comma beta Superscript t Baseline right parenthesis comma 3rd Row 1st Column Blank 2nd Column beta Superscript t plus 1 Baseline equals arg min Underscript beta Endscripts script upper L Subscript v a l Baseline left parenthesis bold upper W Superscript bold t plus bold 1 Baseline comma alpha Superscript t Baseline comma beta Superscript t Baseline right parenthesis comma EndLayout
Wt+1 = arg min
W Ltrain(Wt, αt, βt),
αt+1 = arg min
α Lval(Wt+1, αt, βt),
βt+1 = arg min
β Lval(Wt+1, αt, βt),
(3.18) 
where alpha Superscript t plus 1αt+1 and beta Superscript t plus 1βt+1 denote the parameters of operation and edge in the left parenthesis t plus 1 right parenthesis(t + 1)th 
step, and bold upper W Superscript bold t plus bold 1Wt+1 denotes the kernel of the convolution at the left parenthesis t plus 1 right parenthesis(t + 1)th step. 
In Eq. 3.18, alphaα and betaβ are updated independently. However, optimizing alphaα and betaβ
independently is improper due to their coupling relationship. We consider the search 
process of differentiable architecture search as a bilinear optimization problem and 
solve the problem using a new backtracking method. The details will be shown in 
Sect. 3.2.2.3. 
3.2.2.2 
Search Space 
By simplifying the architecture search to ﬁnd the best cell structure, cell-based NAS 
methods try to learn a scalable and transferable architecture. Following [56, 82], we 
search for normal and reduction computation cells to build the ﬁnal architecture. 
The reduction cells are located at 1 divided by 31/3 and 2 divided by 32/3 of the total network depth; the rest 
are normal cells. A normal cell uses operations with a stride of 1 to keep the size 
of the input feature map unchanged. The number of output channels is identical to 
the number of input channels. A reduction cell uses operations with a stride of 2 
to reduce the spatial resolution of feature maps, and the number of output channels 
is twice the number of input channels. The set of operations includes 3 times 33 × 3 and 
5 times 55 × 5 separable convolution, 3 times 33 × 3 and 5 times 55 × 5 dilated separable convolution, 3 times 33 × 3
max pooling, 3 times 33 × 3 average pooling, a zero(none), and a skip connection. A cell 
(Fig. 3.5) is a fully connected directed acyclic graph (DAG) of seven nodes. Each 
bold x Subscript bold ixi is a latent representation (e.g., a feature map in convolutional networks). Each 
directed edge left parenthesis i comma j right parenthesis(i, j) between node upper N Subscript iNi and node upper N Subscript jNj denotes the set of operations 
script upper O Subscript l Baseline equals StartSet o Subscript l comma 1 Baseline comma ellipsis comma o Subscript l comma upper M Baseline EndSetOl = {ol,1, . . . , ol,M}. Following [56], there are 2 input nodes, 4 intermediate nodes, 
1 output node, and 14 edges per cell during the search. Each cell takes the outputs 
of the two previous cells as the input. The output node of a cell is the depth-wise 
concatenation of all of the intermediate nodes.

3.2
Neural Architecture Search
63
Fig. 3.5 A cell contains seven nodes, which are two input nodes upper N Subscript negative 1N−1 and upper N 0N0; four intermediate 
nodes upper N 1N1, upper N 2N2, upper N 3N3, and upper N 4N4; and one output node 
3.2.2.3 
Backtracking Back Propagation 
We consider the problem from a new perspective where the bold italic betaβ and bold italic alphaα are coupled 
in Eq. 3.17. We note that the calculation of the derivative of alphaα should consider its 
coupling relationship with betaβ. Based on the chain rule [61] and its notations, we 
have: 
StartLayout 1st Row 1st Column ModifyingAbove bold italic alpha With bold caret Superscript t plus 1 2nd Column equals bold italic alpha Superscript bold italic t Baseline plus eta 1 left parenthesis StartFraction partial differential upper G Over partial differential bold italic alpha EndFraction plus eta 2 upper T r left parenthesis left parenthesis StartFraction partial differential upper G Over partial differential bold italic beta EndFraction right parenthesis Superscript upper T Baseline StartFraction partial differential bold italic beta Over partial differential bold italic alpha EndFraction right parenthesis right parenthesis comma EndLayout ˆαt+1 = αt + η1
(
∂G
∂α + η2T r
((∂G
∂β
)T ∂β
∂α
))
,
(3.19) 
where eta 1η1 represents the learning rate, eta 2η2 represents the coefﬁcient of backtracking, 
ModifyingAbove alpha With caret Subscript t plus 1ˆαt+1 denotes the value backtracked from alpha Subscript t plus 1αt+1, and upper T r left parenthesis dot right parenthesisT r(·) represents the trace of 
the matrix. upper T r left parenthesis dot right parenthesisT r(·) means that each element in the matrix StartFraction partial differential upper G Over partial differential bold italic alpha EndFraction∂G
∂α adds the trace of the 
corresponding matrix related to bold italic alphaα. Here, bold upper WW is omitted for simplicity, and only 
structure parameters bold italic alpha bold comma bold italic betaα, β are considered during the back propagation process. We 
further deﬁne: 
ModifyingAbove upper G With caret left parenthesis bold italic beta comma bold italic alpha right parenthesis equals left parenthesis StartFraction partial differential upper G Over partial differential bold italic beta EndFraction right parenthesis Superscript upper T Baseline divided by bold italic alpha comma ˆG(β, α) =
(∂G
∂β
)T
/α,
(3.20) 
where ModifyingAbove upper G With caretˆG is deﬁned by considering the bilinear optimization problem as in Eq. 3.17. 
Note that upper R left parenthesis dot right parenthesisR(·) is only considered when backtracking. Then we have: 
StartLayout 1st Row StartFraction partial differential upper G left parenthesis beta comma alpha right parenthesis Over partial differential bold italic alpha EndFraction equals upper T r left bracket alpha ModifyingAbove upper G With caret StartFraction partial differential bold italic beta Over partial differential alpha EndFraction right bracket period EndLayout ∂G(β, α)
∂α
= T r
[
α ˆG∂β
∂α
]
.
(3.21)

64
3
Binary Neural Architecture Search
We denote ModifyingAbove upper G With caret equals left bracket ModifyingAbove g With caret Subscript 1 Baseline comma ellipsis comma ModifyingAbove g With caret Subscript upper L Baseline right bracket ˆG = [ ˆg1, . . . , ˆgL]. Assuming that bold italic beta Subscript lβl and alpha Subscript mαm are independent when 
l not equals ml /= m, alpha Subscript mαm denotes a column vector, and alpha Subscript 1 comma mα1,m denotes an element in matrix alphaα, we  
have: 
StartFraction partial differential bold italic beta Over partial differential alpha EndFraction equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column ellipsis 3rd Column StartFraction partial differential bold italic beta Subscript m Baseline Over partial differential alpha Subscript 1 comma m Baseline EndFraction 4th Column ellipsis 5th Column 0 2nd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 3rd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 4th Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 5th Row 1st Column 0 2nd Column ellipsis 3rd Column StartFraction partial differential bold italic beta Subscript m Baseline Over partial differential alpha Subscript upper L comma m Baseline EndFraction 4th Column ellipsis 5th Column 0 EndMatrix comma∂β
∂α =
⎡
⎢⎢⎢⎢⎢⎢⎣
0 . . .
∂βm
∂α1,m . . . 0
.
.
.
.
.
.
.
.
.
0 . . .
∂βm
∂αL,m . . . 0
⎤
⎥⎥⎥⎥⎥⎥⎦
,
(3.22) 
and 
bold italic alpha ModifyingAbove upper G With caret equals Start 5 By 5 Matrix 1st Row 1st Column alpha 1 ModifyingAbove g With caret Subscript 1 Baseline 2nd Column ellipsis 3rd Column alpha 1 ModifyingAbove g With caret Subscript l Baseline 4th Column ellipsis 5th Column alpha 1 ModifyingAbove g With caret Subscript upper L Baseline 2nd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 3rd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 4th Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 5th Row 1st Column alpha Subscript upper L Baseline ModifyingAbove g With caret Subscript 1 Baseline 2nd Column ellipsis 3rd Column alpha Subscript upper L Baseline ModifyingAbove g With caret Subscript l Baseline 4th Column ellipsis 5th Column alpha Subscript upper L Baseline ModifyingAbove g With caret Subscript upper L Baseline EndMatrix periodα ˆG =
⎡
⎢⎢⎢⎢⎢⎣
α1 ˆg1 . . . α1 ˆgl . . . α1 ˆgL
.
.
.
.
.
.
.
.
.
αL ˆg1 . . . αL ˆgl . . . αL ˆgL
⎤
⎥⎥⎥⎥⎥⎦
.
(3.23) 
We combine Eqs. 3.22 and 3.23 and get: 
bold italic alpha ModifyingAbove upper G With caret StartFraction partial differential bold italic beta Over partial differential alpha EndFraction equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column ellipsis 3rd Column alpha 1 sigma summation Underscript l equals 1 Overscript upper L Endscripts ModifyingAbove g With caret Subscript l Baseline StartFraction partial differential bold italic beta Subscript m Baseline Over partial differential alpha Subscript l comma m Baseline EndFraction 4th Column ellipsis 5th Column 0 2nd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 3rd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 4th Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 5th Row 1st Column 0 2nd Column ellipsis 3rd Column alpha Subscript upper L Baseline sigma summation Underscript l equals 1 Overscript upper L Endscripts ModifyingAbove g With caret Subscript l Baseline StartFraction partial differential bold italic beta Subscript m Baseline Over partial differential alpha Subscript l comma m Baseline EndFraction 4th Column ellipsis 5th Column 0 EndMatrix periodα ˆG∂β
∂α =
⎡
⎢⎢⎢⎢⎢⎢⎣
0 . . . α1
εL
l=1 ˆgl
∂βm
∂αl,m . . . 0
.
.
.
.
.
.
.
.
.
0 . . . αL
εL
l=1 ˆgl
∂βm
∂αl,m . . . 0
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(3.24) 
After that, the trace of Eq. 3.19 is then calculated by: 
upper T r left bracket bold italic alpha Superscript t Baseline ModifyingAbove upper G With caret StartFraction partial differential bold italic beta Over partial differential alpha Subscript m Baseline EndFraction right bracket equals alpha Subscript m Baseline sigma summation Underscript l equals 1 Overscript upper L Endscripts ModifyingAbove g With caret Subscript l Baseline StartFraction partial differential bold italic beta Subscript m Baseline Over partial differential alpha Subscript l comma m Baseline EndFraction periodT r
[
αt ˆG ∂β
∂αm
]
= αm
L
ε
l=1
ˆgl
∂βm
∂αl,m
.
(3.25)

3.2
Neural Architecture Search
65
Remembering that bold italic alpha Superscript t plus 1 Baseline equals bold italic alpha Superscript t Baseline plus eta 1 StartFraction partial differential upper G left parenthesis beta comma alpha right parenthesis Over partial differential bold italic alpha EndFractionαt+1 = αt + η1
∂G(β,α)
∂α
, IDARTS combines Eqs. 3.19 and 3.25: 
StartLayout 1st Row 1st Column ModifyingAbove bold italic alpha With caret Superscript t plus 1 2nd Column equals bold italic alpha Superscript t plus 1 Baseline plus eta Start 5 By 1 Matrix 1st Row sigma summation Underscript l equals 1 Overscript upper L Endscripts ModifyingAbove g With caret Subscript l Baseline StartFraction partial differential bold italic beta 1 Over partial differential alpha Subscript l comma 1 Baseline EndFraction 2nd Row period 3rd Row period 4th Row period 5th Row sigma summation Underscript l equals 1 Overscript upper L Endscripts ModifyingAbove g With caret Subscript l Baseline StartFraction partial differential bold italic beta Subscript upper L Baseline Over partial differential alpha Subscript l comma upper L Baseline EndFraction EndMatrix circled dot Start 5 By 1 Matrix 1st Row alpha 1 2nd Row period 3rd Row period 4th Row period 5th Row alpha Subscript upper L Baseline EndMatrix 2nd Row 1st Column Blank 2nd Column equals bold italic alpha Superscript t plus 1 Baseline plus eta Start 5 By 1 Matrix 1st Row less than ModifyingAbove upper G With caret comma StartFraction partial differential bold italic beta 1 Over partial differential alpha 1 EndFraction greater than 2nd Row period 3rd Row period 4th Row period 5th Row less than ModifyingAbove upper G With caret comma StartFraction partial differential bold italic beta Subscript upper L Baseline Over partial differential alpha Subscript upper L Baseline EndFraction greater than EndMatrix circled dot Start 5 By 1 Matrix 1st Row alpha 1 2nd Row period 3rd Row period 4th Row period 5th Row alpha Subscript upper L Baseline EndMatrix 3rd Row 1st Column Blank 2nd Column equals bold italic alpha Superscript t plus 1 Baseline plus eta bold italic gamma circled dot bold italic alpha Superscript t Baseline comma EndLayout
ˆαt+1 = αt+1 + η
⎡
⎢⎢⎢⎢⎢⎢⎣
εL
l=1 ˆgl
∂β1
∂αl,1
.
.
.
εL
l=1 ˆgl
∂βL
∂αl,L
⎤
⎥⎥⎥⎥⎥⎥⎦
O
⎡
⎢⎢⎢⎢⎢⎣
α1
.
.
.
αL
⎤
⎥⎥⎥⎥⎥⎦
= αt+1 + η
⎡
⎢⎢⎢⎢⎢⎣
< ˆG, ∂β1
∂α1 >
.
.
.
< ˆG, ∂βL
∂αL >
⎤
⎥⎥⎥⎥⎥⎦
O
⎡
⎢⎢⎢⎢⎢⎣
α1
.
.
.
αL
⎤
⎥⎥⎥⎥⎥⎦
= αt+1 + ηγ O αt,
(3.26) 
where circled dotO represents the Hadamard product and eta equals eta 1 eta 2η
=
η1η2. To simplify the 
calculation, StartFraction partial differential bold italic beta Over partial differential alpha EndFraction∂β
∂α can be approximated by StartFraction upper Delta bold italic beta Over upper Delta alpha EndFractionAβ
Aα . Equation 3.26 shows our method 
is based on a projection function to solve the coupling problem of the bilinear 
optimization by gammaγ . In this method, we consider the inﬂuence of bold italic alpha Superscript tαt and backtrack 
the optimized state at the left parenthesis t plus 1 right parenthesis t h(t + 1)th step to form ModifyingAbove bold italic alpha With caret Superscript t plus 1ˆαt+1. We ﬁrst decide when the 
optimization should be backtracked, and the update rule of the proposed IDARTS is 
deﬁned as: 
ModifyingAbove bold italic alpha With caret Superscript t plus 1 Baseline equals StartLayout Enlarged left brace 1st Row 1st Column upper P left parenthesis bold italic alpha Superscript t plus 1 Baseline comma bold italic alpha Superscript t Baseline right parenthesis 2nd Column i f upper R left parenthesis beta right parenthesis less than zeta comma 2nd Row 1st Column bold italic alpha Superscript t plus 1 Baseline 2nd Column o t h e r w i s e comma EndLayoutˆαt+1 =
{
P(αt+1, αt)
if R(β) < ζ,
αt+1
otherwise,
(3.27) 
where upper P left parenthesis bold italic alpha Superscript t plus 1 Baseline comma bold italic alpha Superscript t Baseline right parenthesis equals bold italic alpha Superscript t plus 1 Baseline plus eta bold italic gamma circled dot bold italic alpha Superscript tP(αt+1, αt) = αt+1 + ηγ O αt. upper R left parenthesis beta right parenthesisR(β) represents the ranking of StartAbsoluteValue beta Subscript l Baseline EndAbsoluteValue|βl| and zetaζ
represents the threshold. We then have: 
zeta equals left floor left parenthesis upper S minus upper T right parenthesis dot lamda dot upper L right floor commaζ = L(S −T ) · λ · L|,
(3.28) 
where T and S denote the beginning and ending epoch of backtracking, lamdaλ denotes 
the coefﬁcient, and L denotes the number of edges in a cell. As shown in Eq. 3.28comma zeta, ζ
will be increased during searching. By doing so, alphaα will be backtracked, according 
to betaβ. 
3.2.2.4 
Comparison of Searching Methods 
Figure 3.6 illustrates the comparison of alphaα for IDARTS and PC-DARTS in the 
shallowest edge. The label of the x-axis is the epoch in searching, and the label 
of the y-axis is the value of alphaα. We freeze the hyperparameters, alphaα and betaβ, in the ﬁrst

66
3
Binary Neural Architecture Search
Algorithm 4: IDARTS interactive differentiable architecture search 
A set o f algorit hms fo r IDARTS in teract ive differ entiable arch itecture search. It cal
ls  the inp uts 
and cal culates the ou tput struct ure for S epo chs, u pdates the para meters alpha and  beta, u pd
ates we ig hts, a nd finds t he final architec tur e based  on the  lear ned a lph
a and be ta.
3.27A set  of alg orithms  for ID ART S interactive
 differentiab le ar chitecture search. It calls the inputs and calculates the output structure for S epochs, updates the parameters alpha and beta, updates weights, and finds the final architecture based on the learned alpha and beta. 3.26A 
set 
of algo rithms f or 
IDA RTS  in
terac tive  dif erentiable ar chitec tur e se arch. It  calls  th
e inputs and calculates the output structure for S epochs, updates the parameters alpha and beta, updates weights, and finds the final architecture based on the learned alpha and beta.
Fig. 3.6 Comparison of alphaα values in the shallowest edge of IDARTS and PC-DARTS on CIFAR10 
15 epochs (only network parameters are updated), alphaα remains unchanged. As the 
shortage of interaction between alphaα and betaβ in PC-DARTS, alphaα and betaβ might easily fall 
into the local minima. However, we backtrack the insufﬁciently trained operations 
on this edge to escape from the local minima to select a better operation and, thus, a 
better architecture by considering the intrinsic relationship between alphaα and betaβ. Due  to  
the backtracking of alphaα, the competition between different operations is intensiﬁed in 
the IDARTS search process, as shown in Fig. 3.6. As a result, it is more conducive 
to choosing the most valuable operation than PC-DARTS. In Fig. 3.7, the label of 
y-axis is script upper L Subscript v a lLval. We also show that the convergence of IDARTS is similar to that 
of PC-DARTS. Although the two have the same convergence rate, we can see that 
the ﬁnal loss of IDARTS converges to a smaller value. The main reason is that 
IDARTS has explored the relationship between different parameters and used our 
backtracking method to train the architecture parameter alphaα entirely. We theoretically

3.2
Neural Architecture Search
67
Fig. 3.7 Comparison of searching loss on CIFAR10 with IDARTS and PC-DARTS 
derive our method under the framework of gradient descent, which provides a solid 
foundation for the convergence analysis of our method. 
3.2.3 
Fast and Unsupervised Neural Architecture Evolution for 
Visual Representation Learning 
Learning high-level representations from labeled data and deep learning models 
in an end-to-end manner is one of the biggest successes in computer vision in 
recent history. These techniques make manually speciﬁed features redundant and 
signiﬁcantly improve the state of the art for many real-world applications. Many 
challenges remain, however, such as the cost of annotating large datasets and an 
insufﬁcient ability to generalize the model. For example, a learned representation 
from supervised learning for image classiﬁcation may lack information such as 
texture, which matters little for classiﬁcation but can be more relevant for later tasks. 
Yet adding it makes the representation less general and might be irrelevant for tasks 
such as image captioning. Thus, improving representation learning requires features 
to be focused on solving a speciﬁc task. Unsupervised learning is an important 
stepping stone towards robust and generic representation learning [34]. The main 
challenge is a signiﬁcant performance gap compared with supervised learning. 
In a scenario where we cannot obtain sufﬁcient annotation, self-supervised 
learning is a popular approach to leverage the mutual information of unlabeled 
data for training. However, its performance still needs improvement compared 
with the supervised methods. One obstacle is that only parameters are learned 
in conventional self-supervised methods. To break the performance bottleneck, a 
natural idea is to explore NAE to optimize the architectures along with parameter

68
3
Binary Neural Architecture Search
training. Speciﬁcally, we can initialize with an architecture found using NAS on 
a small supervised dataset and then evolve the architecture on a larger dataset 
using unsupervised learning. Currently, existing architecture evolution methods 
[7, 94] could be more efﬁcient and cannot deal effectively with the challenging 
unsupervised representation learning. Our approach is highly efﬁcient with a 
complexity of upper O left parenthesis n squared right parenthesisO(n2) where n is the size of the operation space. 
Here we propose our fast and unsupervised neural architecture evolution (FaU-
NAE) [83] method to search architectures for representation learning. Although 
UnNAS [54] discusses the value of a label and discovers that labels are not 
necessary for NAS, it cannot solve the problems above because it is computationally 
expensive and is trained using supervised learning for real applications. FaUNAE is 
introduced to evolve an architecture from an existing architecture manually designed 
or searched from one small-scale dataset on another large-scale dataset. This partial 
optimization can utilize the existing models to reduce the search cost and improve 
search efﬁciency. The strategy is more practical for real applications, as it can 
efﬁciently adapt to new scenarios with minimal requirements for data labeling. 
First, we adopt a trial-and-test method to evolve the initial architecture, which 
is more efﬁcient than the traditional evolution methods, which are computationally 
expensive and require large amounts of labeled data. Second, we note that the quality 
of the architecture could be better estimated due to the absence of labeled data. 
To address this, we explore contrastive loss [34] as the evaluation metric for the 
operation evaluation. Although our method is built based on contrastive loss [34], 
we model our method on the teacher-student framework to mimic the supervised 
learning and then estimate the operation performance even without annotations. 
Then the architecture can be evolved based on the estimated performance. Third, 
we address that one bottleneck in NAS is its explosive search space of up to 
14 Superscript 8148. The search space issue is even more challenging for unsupervised NAS built 
on an ambiguous performance estimation that further deteriorates the training 
process. To address this issue, we follow the principle of survival of the ﬁttest and 
eliminating the inferior to build our search algorithm. This signiﬁcantly improves 
search efﬁciency. Our framework is shown in Fig. 3.8. 
Teacher 
Reduce 
search space 
t 
s0 
s1 
s2 
... 
Contrastive 
loss 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
Mutation & Train 
K Times 
Mutation & Train 
K Times 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
Student 
Search space 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
INPIT 
OUTPUT 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
Search space 
←
Ω
Ω -{arg max
 
}
i
i
i
 
k,n 
k 
w(o 
) 
log 
sa 
i
i
 
k,n
k,n
i 
k 
2
N
w(o 
)= l(o 
) -
Fig. 3.8 The main framework of the proposed teacher-student search strategy

3.2
Neural Architecture Search
69
Teacher
Student 
Performance evalution 
Search space 
Reduce 
FU-NAE 
Teacher
Student 
Performance evalution 
Search space 
Reduce 
FU-NAE 
InfoNCE loss 
Teacher
Student
EMA 
TS Contrastive Learning 
InfoNCE loss 
Teacher
Student
EMA 
TS Contrastive Learning 
(a) 
Conv 
1x1 
Conv 
3x3 
Conv 
1x1 
Conv
1x1
Conv 
1x1 
Conv
3x3
Conv 
3x3 
Conv
1x1
Conv 
1x1 
Conv 
1x1 
Conv 
1x1 
Conv 
3x3 
Conv 
5x5 
SAConv 
3x3 
SAConv 
5x5 
Conv
1x1
Conv 
1x1 
Conv
1x1
Conv 
1x1 
Conv
3x3
Conv 
3x3 
Conv
5x5
Conv 
5x5 
SAConv
3x3
SAConv 
3x3 
SAConv
5x5
SAConv 
5x5 
fmap in memory 
fmap not in memory 
(1) Bottleneck
(2) Search Block 
(b) 
Fig. 3.9 (a) The main framework of teacher-student model, which focuses on both the unsuper-
vised neural architecture evolution (left) and contrastive learning (right). (b) Compared with the 
original bottleneck (1) in ResNet, a new search block is designed for FaUNAE (2) 
3.2.3.1 
Search Space 
We have experimentally determined that for unsupervised learning, ResNet [36] 
is better than cell-based methods for building an architectural space. We denote 
this space as StartSet upper Omega Superscript i Baseline EndSet{oi}, where i represents given block. Rather than repeating the 
bottleneck (building block in ResNet) with various operations, however, we allow a 
set of search blocks shown in Fig. 3.9b with various operations including traditional 
convolution with kernel sizes StartSet 3 comma 5 EndSet{3, 5} and split-attention convolution [87] with kernel 
sizes StartSet 3 comma 5 EndSet{3, 5} and radixes StartSet 2 comma 4 EndSet{2, 4}. This reduces the model size by sharing the 1 times 11 × 1
convolution to improve the efﬁciency. To enable a direct trade-off between depth 
and block size (indicated by the parameters of the selected operations), we initiate 
a deeper over-parameterized network and allow a block to be skipped by adding 
the identity operation to the candidate set of its mixed operation. So the set of the 
operations upper Omega Superscript ioi in the ith block consists of upper M equals 7M = 7 operations. With a limited model 
size, the network can either be shallower by skipping more blocks and using larger

70
3
Binary Neural Architecture Search
ones or choose to be deeper by keeping smaller ones. To accelerate the evolution 
process and make use of prior knowledge, the initial structure alpha 0α0 is ﬁrst manually 
designed (e.g., ResNet-50, without weight parameters) or searched for by another 
NAS (e.g., ProxylessNAS [9]) on different datasets in supervised manner1 , which 
are then remapping to the search space. 
3.2.3.2 
Evolution 
The evolutionary strategy is summarized in Algorithm 5. Unlike AmoebaNet [66] 
that evaluates the performance of sub-networks sampled from the search space in 
a population, our method targets evolving the operation in each block using a trial-
and-test manner. We ﬁrst mutate the operation based on its mutation probability, 
followed by an evaluation step to make sure the mutation is ultimately used. 
Mutation An initial structure alpha 0α0 is manually designed (e.g., ResNet-50)2 or 
searched by another NAS (e.g., ProxylessNAS [9]) on a different dataset using 
supervised learning. The initial sub-network f Subscript theta Sub Subscript sfθs, which is generated by searching 
over-parameterized network based on alpha 0α0, is then trained using Eq. 3.34 for k steps 
to obtain the evaluation metric l left parenthesis o Subscript k Superscript i Baseline right parenthesisl(oi
k). A new architecture alpha Subscript nαn (o Subscript k comma nok,n) is then constructed 
from the old architecture alpha Subscript n minus 1αn−1 (o Subscript k comma n minus 1ok,n−1) by a transformation or a mutation. The 
mutation probability p Subscript mtpmt is deﬁned as: 
p Subscript mt Baseline left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column 1 minus epsilon comma o Subscript k comma n Superscript i Baseline equals o Subscript k comma n minus 1 Superscript i Baseline 2nd Row 1st Column Blank 2nd Column StartFraction 1 Over upper K minus 1 EndFraction left parenthesis 1 minus StartFraction sa Subscript k Superscript i Baseline Over sigma summation Underscript k Superscript prime Baseline Endscripts sa Subscript k Sub Superscript prime Subscript Superscript i Baseline EndFraction right parenthesis epsilon comma o t h e r w i s e EndLayoutpmt(oi
k,n) =
⎧
⎪⎪⎨
⎪⎪⎩
1 −ε, oi
k,n = oi
k,n−1
1
K −1(1 −
sai
k
ε
k' sai
k'
)ε, otherwise
(3.29) 
where sa Subscript k Superscript isai
k represent the sampling times of the operation o Subscript k Superscript ioi
k. In general, the operation 
in each block is kept constant with a probability 1 minus epsilon1 −ε in the beginning. For the 
mutation with the probability of epsilonε, younger (less sample time) operations are more 
likely to be selected. Intuitively, keeping the operation constant can be considered 
to provide exploitation, while mutations provide exploration [66]. We use two main 
mutations, the depth mutation and the op mutation, as in AmoebaNet [66], to modify 
the structure generated by the search space described above. 
The operation mutation pays attention to the selection of operations in each 
block. Once the operation in a block is chosen to mutate, the mutation picks one 
of the other operations based on Eq. 3.29. The depth mutation can change the depth 
of the sub-network from the over-parameterized network by setting the operation 
of one block to the “Identity” operation. We limit the model size as a restriction 
metric to search more efﬁciently and evaluate more reasonable operations. The
1 The evolution is based on unsupervised learning. 
2 No weight parameters 

3.2
Neural Architecture Search
71
structure can then evolve into a sub-network with the same computational burden. 
The probability of the restriction metric p Subscript rm Superscript ipi
rm is deﬁned as: 
p Subscript rm Baseline left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis equals StartFraction minus exp MS left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis Over sigma summation Underscript k Superscript prime Baseline Endscripts exp MS left parenthesis o Subscript k prime comma n Superscript i Baseline right parenthesis EndFraction commaprm(oi
k,n) =
−exp MS(oi
k,n)
ε
k' exp MS(oi
k',n),
(3.30) 
where MS left parenthesis o Subscript k comma n Superscript i Baseline right parenthesisMS(oi
k,n) represents number of parameters of the kth operation in the ith 
block. The ﬁnal evolution probability p that combines p Subscript mtpmt and p Subscript rmprm is deﬁned as: 
p left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis equals lamda 1 asterisk p Subscript mt Baseline left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis plus left parenthesis 1 minus lamda 1 right parenthesis asterisk p Subscript rm Baseline left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis commap(oi
k,n) = λ1 ∗pmt(oi
k,n) + (1 −λ1) ∗prm(oi
k,n),
(3.31) 
where lamda 1λ1 is hyperparameter. 
Mutation validation After each evolution, the sub-network is trained using 
Eq. 3.34, and the loss is used as the evaluation metric. We observe the current 
validation loss a and accordingly update the loss l left parenthesis o Subscript k comma n Superscript i Baseline right parenthesisl(oi
k,n), which historically indicates 
the validation loss of all the sampled operations o Subscript k Superscript left parenthesis i comma j right parenthesiso(i,j)
k
as: 
l left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis equals lamda 2 asterisk l left parenthesis o Subscript k comma n minus 1 Superscript i Baseline right parenthesis plus left parenthesis 1 minus lamda 2 right parenthesis asterisk a commal(oi
k,n) = λ2 ∗l(oi
k,n−1) + (1 −λ2) ∗a,
(3.32) 
where lamda 2λ2 is a hyperparameter. If the operation which is mutated performs better (less 
loss), we apply it as the base of the next evolution; otherwise, we use the original 
operation as the base of the next evolution: 
o Subscript k comma n Superscript i Baseline equals StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column o Subscript k comma n Superscript i Baseline comma l left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis greater than l left parenthesis o Subscript k comma n minus 1 Superscript i Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column o Subscript k comma n minus 1 Superscript i Baseline comma e l s e EndLayoutoi
k,n =
⎧
⎨
⎩
oi
k,n, l(oi
k,n) > l(oi
k,n−1)
oi
k,n−1, else
(3.33) 
3.2.3.3 
Contrastive Learning 
Contrastive learning [31] can signiﬁcantly improve the performance of unsupervised 
visual representation learning. The goal is to make positive sample pairs close 
and negative sample pairs far away in the latent space. Prior works [34, 74] 
usually investigate contrastive learning by exploring the sample pairs calculated 
from the encoder and the momentum encoder [34]. Based on the investigation, we 
reformulate the unsupervised/self-supervised NAS as a teacher-student model, as 
shown in Fig. 3.9a. Following [34], we build dynamic dictionaries, and the “keys” 
(e.g., tokens) t in the dictionary are sampled from data (e.g., images or patches) 
and are represented by the teacher network. In general, the keys representation is 
t equals f Subscript theta Sub Subscript t Baseline left parenthesis x Subscript t Baseline right parenthesist = fθt (xt), where f Subscript theta Sub Subscript t Baseline left parenthesis period right parenthesis equals f left parenthesis o Subscript k comma n Superscript i Baseline semicolon theta Subscript t Baseline semicolon period right parenthesisfθt (.) = f (oi
k,n; θt; .) is a teacher network and x Subscript txt is a key sample. 
Likewise, the “query” x Subscript sxs is represented by s equals f Subscript theta Sub Subscript s Baseline left parenthesis x Subscript s Baseline right parenthesiss = fθs(xs), where f Subscript theta Sub Subscript s Baseline equals f left parenthesis o Subscript k comma n Superscript i Baseline semicolon theta Subscript s Baseline semicolon period right parenthesisfθs = f (oi
k,n; θs; .)
is a student network. Unsupervised learning trains the student network to perform

72
3
Binary Neural Architecture Search
Algorithm 5: FaUNAE 
A set o f algorit hms fo r F a U N A  E. It  calls t he inputs and
 creates th e output s tructure alp
ha with the sear ching hype r-
gra ph. It con
str ucts t he Teacher 
an
d th e Stud e n t  m odels, tra
in
s, and updates by E M A, get s th e ev aluation loss  on t he val ida tion  data, and
 reduces the  search space. 3.31A 
se
t of algor ithm s for F a U N A E.  It calls th e inpu ts an d cr eates  the output s truc
ture  alph a with  the sea rching hyp er-graph.  It cons truc ts the Teac her and the St ude
nt m odels,  trains, and updates by E M A, gets the evaluation loss on the validation data, and reduces the search space. 3.35A 
se
t of  alg orithms for  F a U N  A E . It calls the i nputs and creates the output structure alpha with the searching hyper-graph. It constructs the Teacher and the Student models, trains, and updates by E M A, gets the evaluation loss on the validation data, and reduces the search space. 3.34A 
set of algorithms for F a U N A E. It calls the inputs and creates the output structure alpha with the searching hyper-graph. It constructs the Teacher and the Student models, trains, and updates by E M A, gets the evaluation loss on the validation data, and reduces the search space. 3.32 A s et of al gori thms for F a  U N  A E. I t cal ls the inpu ts and c
reates the o
utput s tructure alpha with the searching hyper-graph. It constructs the Teacher and the Student models, trains, and updates by E M A, gets the evaluation loss on the validation data, and reduces the search space. 3.33A 
set
 of algo
rit
hms  for F a U N A E
. 
It call s the
 inputs and  creates the output structure alpha with the searching hyper-graph. It constructs the Teacher and the Student models, trains, and updates by E M A, gets the evaluation loss on the validation data, and reduces the search space. 3.36A 
set
 of alg orit hms for  F a U  N A  E. It c alls
 t
he in
puts an
d c
rea tes the
 out
put str
uctu re a lpha w
ith the sea rching hyper-graph. It constructs the Teacher and the Student models, trains, and updates by E M A, gets the evaluation loss on the validation data, and reduces the search space.
dictionary lookups. An encoded “query” s should be similar to its matching key and 
dissimilar to others. The student and teacher models are NAE sub-networks from 
the over-parameterized network described in Sect. 3.2.3.1. 
Using a contrastive loss, we train a visual representation student model by 
matching an encoded query s to a dictionary of encoded keys. The value of the 
contrastive loss is lower when s and t are from the same (positive) sample and 
higher when s and t are from different (negative) samples. The contrastive loss 
is also deployed in FaUNAE to guide structure evolution to obtain the optimal 
structure based on the unlabeled dataset. InfoNCE [60] shown in Fig. 3.9a measures 
the similarity using the dot product and is used as our evaluation metric: 
script upper L equals minus log StartFraction exp left parenthesis s dot t Subscript plus Baseline divided by tau right parenthesis Over sigma summation Underscript n equals 0 Overscript upper N Endscripts exp left parenthesis s dot t Subscript n Baseline divided by tau right parenthesis EndFraction commaL = −log
exp(s · t+/τ)
εN
n=0 exp(s · tn/τ)
,
(3.34) 
where tauτ is a temperature hyperparameter per [80] and t Subscript plust+ represents the feature 
calculated from the same sample with s. InfoNCE is over one positive and M 
negative sample. Intuitively, it is a log loss of a left parenthesis upper M plus 1 right parenthesis(M+1)-way softmax-based classiﬁer 
that tries to classify s as t Subscript plust+. Our method is general and can be based on other 
contrastive loss functions [31, 39, 76, 80], such as margin-based losses and variants 
of NCE losses.

3.2
Neural Architecture Search
73
Following [34, 72], the teacher model is updated as an exponential moving 
average (EMA) of the student model: 
theta Subscript t Baseline equals m asterisk theta Subscript t Baseline plus left parenthesis 1 minus m right parenthesis asterisk theta Subscript s Baseline commaθt = m ∗θt + (1 −m) ∗θs,
(3.35) 
where theta Subscript sθs and theta Subscript tθt are the weight of the student model and teacher model, respectively, 
updated by back propagation in contrastive learning, and m element of left bracket 0 comma 1 right parenthesism ∈[0, 1) is a smoothing 
coefﬁcient hyperparameter. 
3.2.3.4 
Fast Evolution by Eliminating Operations 
One of the most challenging aspects of NAS lies in the inefﬁcient search process, 
and we address this issue by eliminating the least potential operations. After StartAbsoluteValue upper Omega Superscript i Baseline EndAbsoluteValue asterisk upper E|oi|∗E
epochs, we remove the operations in each block based on performances (loss) and 
the sampling times. We deﬁne the combination of the two as: 
w left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis equals l left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis minus StartRoot StartFraction 2 log upper N Over sa Subscript k Superscript i Baseline EndFraction EndRoot commaw(oi
k,n) = l(oi
k,n) −
/
2 log N
sai
k
,
(3.36) 
where N is the total number of evolutions and mutations and sa Subscript k Superscript isai
k refers to the number 
of times the kth operation of the ith block has been selected. The ﬁrst item l left parenthesis o Subscript k comma n Superscript i Baseline right parenthesisl(oi
k,n) in 
Eq. 3.36 is calculated based on an accumulation of the validation loss, which favors 
the operations that look good historically, and the second term is the exploration 
term which allows operations to get an exploration bonus that grows as log upper Nlog N. The  
operation with the minimal w for every block is abandoned. This means that the 
operations that are given more opportunities, but result in poor performance, are 
removed. With this strategy, the search space which has v blocks is signiﬁcantly 
reduced from StartAbsoluteValue upper Omega Superscript i Baseline EndAbsoluteValue Superscript v|oi|v to left parenthesis StartAbsoluteValue upper Omega Superscript i Baseline EndAbsoluteValue minus 1 right parenthesis Superscript v(|oi| −1)v, and the reduced space becomes: 
upper Omega Superscript i Baseline left arrow upper Omega Superscript i Baseline minus StartSet arg max Underscript k Endscripts w left parenthesis o Subscript k comma n Superscript i Baseline right parenthesis EndSet periodoi ←oi −{arg max
k
w(oi
k,n)}.
(3.37) 
The reduction procedure is repeated until the optimal structure is obtained when 
only one operation is left in each block. 
3.2.3.5 
Experiments 
This section compares FaUNAE with human-designed networks and state-of-the-
art NAS methods for classiﬁcation on the ImageNet and CIFAR10 datasets. The 
evolved architecture on ImageNet is also applied as the backbone of object detection 
on the PASCAL VOC and COCO datasets. Due to page limitations, the experimental 
results on CIFAR10 and PASCAL VOC are shown in the supplemental material.

74
3
Binary Neural Architecture Search
Evolution and Training Protocol The evolution and training protocol used in 
our experiments is described in this section. We ﬁrst set global average pooling 
and a two-layer MLP [14] head (hidden layer 2048-d, with ReLU) which has a 
ﬁxed-dimensional output (128-d [80]) after the hypernet and searched network. The 
output vector is normalized by its L2 norm [80], representing the query or key. 
The temperature tauτ in Eq. 3.34 is set as 0.20.2 [80], and the smoothing coefﬁcient 
hyperparameter m in Eq. 3.35 is set as 0.9990.999. The data augmentation setting 
follows MoCoV2 [17]. A 224 times 224224 × 224-pixel cropped patch is taken from a randomly 
resized image and is then subjected to random color jittering, random horizontal 
ﬂip, random grayscale conversion, and blur augmentation [14]. We use the SGD 
optimizer with an initial learning rate of 0.030.03 (annealed down to zero following a 
cosine schedule without restart), a momentum of 0.90.9, a weight decay of 0.00010.0001, and 
batch size of 256 in 8 GPUs. 
In experiments, we ﬁrst evolve the initial structure alpha 0α0 on an over-parameterized 
network that uses ResNet50 as the backbone to build the architecture space (details 
can be found in Sect. 3.2.3.1) on ImageNet. We set initial structure alpha 0α0 as a 
random structure, ResNet50, and structure searched by Proexyless on ImageNet100, 
respectively, to show the importance of prior knowledge. During the architecture 
search, the 128M training samples of ImageNet are divided into two subsets, 80 percent sign80%
for the training set for training the network weights and the remainder as a validation 
set for mutation validation and search space reduction. We set the channel as half 
of that of ResNet50 for efﬁciency and attention to the evolution of operation rather 
than the channel. So the model size of search space can be reduced to a quarter, 
and we set hyperparameter upper E equals 3E = 3, so the total number of epochs is sigma summation Underscript m equals 2 Overscript upper M Endscripts k asterisk upper EεM
m=2 k ∗E. 
The hyperparameter lamda 1λ1 and lamda 2λ2 are set to 0.90.9 and 0.30.3. After evolution, we train the 
searched network on ImageNet unsupervised for 200 epochs. We run the experiment 
multiple times and ﬁnd that the resulting architectures only show slight variation in 
performance, demonstrating the proposed method’s stability. 
Results for Classiﬁcation Following a common protocol, we verify our method by 
linearly classifying frozen features. In this subsection, we perform unsupervised pre-
training on ImageNet, and then we freeze the features and train a supervised linear 
classiﬁer (a fully connected layer followed by softmax). We train this classiﬁer 
on the global average pooling features of the evaluated network for 100 epochs. 
We report Top-1 classiﬁcation accuracy on the ImageNet validation set. For this 
experiment, we set the initial learning rate as 30 and weight decay 0 same with [34]. 
The results for different architectures on ImageNet are summarized in Table 3.2. 
We use 8 Tesla V100 GPUs to search for about 46 hours. Table 3.2 shows that 
FaUNAE outperforms ResNet50, ResNet101, ResNet170, and AMDIM Subscript small slash largeAMDIMsmall/large
with higher accuracy. FaUNAE also performs better than the structure sampled 
randomly from the search space described in Sect. 3.2.3.1 on Top-1 accuracy (68.368.3
vs. 66.266.2), demonstrating our method’s effectiveness. When compared with other 
NAS methods like Proxyless, which uses the same search space as FaUNAE, our

3.2
Neural Architecture Search
75
Table 3.2 Comparisons under the linear classiﬁcation protocol on ImageNet 
Params 
Search cost 
Search 
Architecture
Method
Accuracy (%) (M)
(GPU days) 
method 
ResNet50
InstDisc [80] 
54.0
24
–
Manual 
ResNet50
LocalAgg [93] 
58.8
24
–
Manual 
ResNet101
CPC v1 [39]
48.7
28
–
Manual 
upper R e s upper N e t 170 Subscript widerResNet170wider
CPC v2 [37]
65.9
303
–
Manual 
upper R e s upper N e t 50 Subscript upper L plus abResNet50L+ab
CMC [73]
64.1
47
–
Manual 
upper A upper M upper D upper I upper M Subscript smallAMDIMsmall
AMDIM [3]
63.5
194
–
Manual 
upper A upper M upper D upper I upper M Subscript largeAMDIMlarge
AMDIM [3]
68.1
626
–
Manual 
ResNet50
MoCo v1 [34] 
60.6
24
–
Manual 
ResNet50
MoCo v2 [17] 
67.5
24
–
Manual 
ResNet50
SimCLR [14] 
66.6
24
–
Manual 
Random
MoCo v2
66.2
23
–
Random 
Proxyless
MoCo v2
67.8
23
23.1
Gradient-base 
FaUNAE (Random) 
MoCo v2
67.4
24
15.3
Evolution 
FaUNAE (ResNet50) 
MoCo v2
67.8
24
15.3
Evolution 
FaUNAE (Proxyless) 
MoCo v2
68.3
30
15.3
Evolution 
SAConv4 3x3 
SAConv2 3x3 
Conv 3x3 
SAConv4 5x5 
SAConv4 3x3 
SAConv4 3x3 
SAConv4 3x3 
Conv 3x3 
SAConv4 3x3 
SAConv4 3x3 
SAConv2 3x3 
SAConv2 3x3 
SAConv4 3x3 
SAConv4 3x3 
SAConv2 3x3 
SAConv4 3x3 
SAConv4 3x3 
SAConv4 3x3 
SAConv2 3x3 
Conv 3x3 
SAConv4 5x5 
SAConv4 3x3 
SAConv4 3x3 
SAConv4 3x3 
Conv 3x3 
SAConv4 3x3 
SAConv4 3x3 
SAConv2 3x3 
SAConv2 3x3 
SAConv4 3x3 
SAConv4 3x3 
SAConv2 3x3 
SAConv4 3x3 
SAConv4 3x3 
3x224x224 
32x112x112
64x56x56 
128x28x28 
256x14x14 
512x7x7 
Fig. 3.10 Detailed structures of the best structure discovered on ImageNet. “SAConv2” and 
“SAConv4” denote split-attention bottleneck convolution layer with radixes of 2 and 4, respectively 
method obtains a better performance with higher accuracy (67.867.8 vs. 68.368.3) and with 
a much faster search speed (23.123.1 vs. 15.315.3 GPU days). 
We also set different initial structures alpha 0α0 including random structure, ResNet50, 
and structure searched by Proxyless on ImageNet100. As shown in Table 3.2, 
we ﬁnd that the better the initial structure, the better the performance, which 
shows the importance of prior knowledge. For the structure (Fig. 3.10) obtained 
by ABanditNAS on ImageNet, we ﬁnd that the structure on unsupervised learning 
prefers a small kernel size and a split-attention convolution [87], which also shows 
the effective of split-attention convolution and the rationality of FaUNAE. 
Results on Object Detection and Segmentation Learning transferable features is 
the primary goal of unsupervised learning. ImageNet supervised pre-training is most 
inﬂuential when initializing ﬁne-tuning in object detection and segmentation (e.g., 
[26, 27, 67]). Next, we compare FaUNAE with ImageNet supervised pre-training, 
transferred to various tasks including PASCAL VOC [22] (in the attached ﬁles), 
COCO [52].

76
3
Binary Neural Architecture Search
Table 3.3 Object detection and instance segmentation results on COCO with Mask R-CNN. AP Superscript bbAPbb
means bounding-box AP and AP Superscript mkAPmk means mask AP 
Architecture
Method
upper A upper P Superscript bbAP bb
upper A upper P 50 Superscript bbAP bb
50
upper A upper P 75 Superscript bbAP bb
75
upper A upper P Superscript mkAP mk
upper A upper P 50 Superscript mkAP mk
50
upper A upper P 75 Superscript mkAP mk
75
ResNet50
super.
40.0
59.9
43.1
34.7
56.5
36.9 
ResNet50
MoCo v1[34]
40.7
60.5
44.1
35.4
57.3
37.6 
FaUNAE
MoCo v2
43.1
63.0
47.2
37.7
60.2
40.6 
We apply Mask R-CNN [35] with the C4 backbone as the detector, with batch 
normalization tuned and implemented as in [79]. All layers are ﬁne-tuned end-to-
end, and the image scale is 480x800 pixels during training and 800x800 at inference. 
We ﬁne-tune all layers end-to-end. We ﬁne-tune on the train2017 set (tilde 118∼118k 
images) and evaluate it on val2017. The schedule is the default 2 times2× [28]. 
Table 3.3 shows the results on the COCO dataset with the C4 backbones. With 
the 2 times2× schedule, FaUNAE is better than its ImageNet-supervised counterpart on all 
metrics. Due to the absent result of MoCo v2 [17], we do not compare it with our 
FaUNAE. We run their code for this comparison, which is even worse than v1. Also, 
FaUNAE is better than ResNet50 trained with unsupervised MoCo v1 [34]. 
3.3 
Binary Neural Architecture Search 
3.3.1 
BNAS: Binarized Neural Architecture Search for 
Efﬁcient Object Recognition 
Efﬁcient computing has become one of the hottest topics in academia and industry. 
It will be vital for the 5G networks to provide hardware-friendly and efﬁcient 
solutions for practical and wild applications [59]. Edge computing is computing 
resources that are closer to the end user. This makes applications faster and more 
user-friendly [13]. It enables mobile or embedded devices to provide real-time 
intelligent analysis of big data, reducing the pressure on the cloud computing center 
and improving availability [33]. However, edge computing is still challenged by its 
limited computational ability, memory and storage, and severe performance loss, 
making edge computing models inefﬁcient for feature calculation and inference 
[46]. 
A possible solution for efﬁcient edge computing can be achieved based on 
compressed deep models, which fall mainly into network pruning, knowledge 
distillation, and model quantization. Network pruning [32] aims to remove network 
connections with less signiﬁcance, and knowledge distillation [38] introduces 
a teacher-student model, which uses the soft targets generated by the teacher 
model to guide the student model with a much smaller model size, to achieve 
knowledge transfer. Differently, model quantization [42] calculates neural networks 
with low-bit weights and activations to compress a model more efﬁciently, which

3.3
Binary Neural Architecture Search
77
is also orthogonal to the other two. The binarized model is widely considered 
one of the most efﬁcient ways to perform computing on embedded devices with 
extremely low computational cost. Binarized ﬁlters have been used in traditional 
convolutional neural networks (CNNs) to compress deep models [42, 57, 65], 
showing up to 58-time speedup and 32-time memory saving. In [65], the XNOR 
network is presented where the weights and inputs attached to the convolution 
are approximated with binary values. This efﬁciently implements convolutional 
operations by reconstructing the unbinarized ﬁlters with a single scaling factor. 
[92] introduces 2 tilde 42 ∼4-bit quantization based on a two-stage approach to quantizing 
weights and activations, signiﬁcantly improving the efﬁciency and performance 
of quantized models. Furthermore, WAGE [78] is proposed to discretize both the 
training and inference processes and quantizes not only weights and activations 
but also gradients and errors. In [30], a projection convolutional neural network 
(PCNN) is proposed to realize binarized neural networks (BNNs) based on a 
simple back propagation algorithm. In our previous work [88], we propose a 
novel approach called Bayesian-optimized 1-bit CNNs (denoted BONNs), taking 
advantage of Bayesian learning to signiﬁcantly improve the performance of extreme 
1-bit CNNs. Other practices in [1, 21, 71] with improvements over previous work. 
Binarized models show the advantages of computational cost reduction and memory 
savings but, unfortunately, suffer from performance loss when handling wild data 
in practical applications. The main reasons are twofold. On the one hand, there is 
still a gap between low-bit weights/activations and full-precision weights/activations 
on feature representation, which should be investigated from new perspectives. On 
the other hand, traditional binarized networks are based on the neural architecture 
manually designed for full-precision networks, which means that the design of 
binarized architecture still needs to be explored. 
Traditional neural architecture search (NAS) has attracted signiﬁcant atten-
tion with remarkable performance in various deep learning tasks. For example, 
impressive results have been shown for reinforcement learning (RL)-based methods 
[95, 96], which train and evaluate more than 20,00020,000 neural networks across 500 
GPUs over 4 days. Recent methods like differentiable architecture search (DARTS) 
reduce search time by formulating the task differently [56]. DARTS relaxes the 
search space to be continuous so that the architecture can be optimized concerning 
its validation set performance by gradient descent, which provides a fast solution 
for an effective network architecture search. To reduce redundancy in the network 
space, partially connected DARTS (PC-DARTS) was recently introduced to perform 
a more efﬁcient search without compromising the performance of DARTS [82]. 
Although DARTS or its variants have a smaller model size than traditional 
light models, the searched network still needs to improve its inference process 
due to the complicated architectures generated by multiple stacked full-precision 
convolution operations. Consequently, the searched network for embedded devices 
must still be more computationally expensive and efﬁcient. At the same time, 
existing gradient-based approaches select operations without meaningful guidance. 
The search process is inefﬁcient, and the selected operation might exhibit signiﬁcant

78
3
Binary Neural Architecture Search
Output
O
PC-DARTS 
Performance-based 
B-1
B0 
Averaging 
B4
B2
B3
B1 
Binarization 
MSE 
Fig. 3.11 The proposed binarized neural architecture search (BNAS) framework. In BNAS, the 
search cell is a fully connected directed acyclic graph with four nodes calculated based on PC-
DARTS and a performance-based method. We also reformulate the optimization of binarization of 
CNNs in the same framework 
vulnerability to model attacks based on gradient information [29, 58], also for wild 
data. These problems require further exploration to overcome these challenges. 
To address these challenges, we transfer the NAS to a binarized neural architec-
ture search (BNAS) [12], exploring the advantages of binarized neural networks 
(BNNs) on memory saving and computational cost reduction. In our BNAS 
framework, as shown in Fig. 3.11, we use PC-DARTS as a warm-up step, followed 
by the performance-based method to improve the robustness of the resulting BNNs 
for the wild data. Furthermore, based on observation, the early optimal operation is 
not necessarily optimal at the end, and the worst operation at the early stage usually 
performs worse at the end [90]. We take advantage of PC-DARTS and performance 
evaluation to reduce operating space. This means that the operations we ﬁnally 
reserve are certainly a near-optimal solution. On the other hand, with the operation 
pruning process, the search space becomes smaller and smaller, leading to an 
efﬁcient search process. We show that the BNNs obtained by BNAS can outperform 
conventional BNN models by a large margin. It is a signiﬁcant contribution to the 
ﬁeld of BNNs considering that the performance of conventional BNNs is not yet 
comparable with those of their corresponding full-precision models in terms of 
accuracy. To further validate the performance of our method, we also implemented a 
1-bit BNAS in the same framework. Unlike BNNs (only kernels are binarized), 1-bit 
CNNs suffer from a poor performance evaluation problem for binarized operations 
with binarized activations in the beginning due to insufﬁcient training. We assume 
BNAS as a multi-armed bandit problem and introduce an exploration term based on 
the upper conﬁdence bound (UCB) [2] to improve the search performance. 
The exploration term handles the exploration-exploitation dilemma in the multi-
armed bandit problem. We lead a new performance measure based on UCB by

3.3
Binary Neural Architecture Search
79
(1) 
Upda ng 
likelihoods 
Reducing 
search space 
Selec ng 
opera ons 
Bi
Bj
0.021
0.12
-0.22
0.321
...
-0.069
α
CONV 
3x3 
MAX POOL 
3x3 
Bi
Bj
Iden ty 
Sampling
0.67+0.34
0.89+0.295
0.42+0.26
1.1+0.295
... 0.7+0.19
CONV 
5x5 
MAX POOL 
3x3 
Iden ty
CONV 
3x3 
Depth-Wise 
CONV 3x3 
Searching
1.01
1.185
0.68
1.395
...
0.89
Feature map in memory 
Feature map not in memory 
CONV 
3x3 
MAX POOL 
3x3 
CONV 
5x5 
CONV 
3x3 
MAX POOL 
3x3 
Depth-Wise 
CONV 3x3
Iden ty 
Iden ty 
CONV 
5x5 
CONV 
3x3 
MAX POOL 
3x3 
Depth-Wise 
CONV 3x3
Iden ty 
likelihoods 
(2) 
(3) 
(4)
(5) 
Fig. 3.12 The main steps of our BNAS: (1) Search for an architecture based on O(i,j) using PC-
DARTS. (2) Select half of the operations with less potential from O(i,j) for each edge, resulting in 
O(i,j)
smaller. (3) Select an architecture by sampling (without replacement) one operation from O(i,j)
smaller
for every edge and then train the selected architecture. (4) Update the likelihood of selection of the 
operation s(o(i,j)
k
) based on the accuracy obtained from the selected architecture on the validation 
data. (5) Abandon the operation with the minimal likelihood of selection of the search space {O(i,j)}
for every edge 
considering both the performance evaluation and the number of trials for operation 
pruning in the same framework. This means the operation is ultimately abandoned 
only when sufﬁciently evaluated (Fig. 3.12). 
The search process for our BNAS consists of two steps. One is the potential oper-
ation ordering based on partially connected DARTS (PC-DARTS) [82], which also 
serves as a baseline for our BNAS. It is further improved with a second operation 
reduction step guided by a performance-based strategy. In the operation reduction 
step, we prune one operation at each iteration from one-half of the operations with 
less potential, as calculated by PC-DARTS. As such, the optimization of the two 
steps becomes faster and faster because the search space is reduced due to the 
operation pruning. We can take advantage of the differential framework of DARTS, 
where search and performance evaluation are in the same setting. We also enrich 
the DARTS search strategy. The gradient is used to determine which operation is 
better, and the proposed performance evaluation is included to reduce the search 
space further. 
3.3.1.1 
Search Space 
Following [56, 95, 96], we search for a computing cell as the building block of 
the ﬁnal architecture. A network consists of a predeﬁned number of cells [95], 
which can be normal cells or reduction cells. Each cell takes the outputs of the two 
previous cells as input. A cell is a fully connected directed acyclic graph (DAG) of 
M nodes, i.e., {B1, B2, . . . , BM}, as illustrated in Fig. 3.13a. Each node Bi takes

80
3
Binary Neural Architecture Search
(a) Cell 
(b) Operation Set 
B-1
-1
N 
0
N 
2
N 
3
N 
4
N 
Output 
1
N 
+ 
3×3 binarized 
conv 
Depth-wise 
binarized conv 
Identity 
Bi
Bj 
Zero
Zero 
Fig. 3.13 (a) A cell contains seven nodes; two input nodes upper B Subscript negative 1B−1 and upper B 0B0; four intermediate nodes 
upper B 1B1, upper B 2B2, upper B 3B3, upper B 4B4 that apply sampled operations on the input nodes and upper nodes; and an output 
node that concatenates the outputs of the four intermediate nodes. (b) The set of operations script upper O Superscript left parenthesis i comma j right parenthesisO(i,j)
between upper B Subscript iBi and upper B Subscript jBj, including binarized convolutions 
its dependent nodes as input and generates an output through a sum operation 
upper B Subscript j Baseline equals sigma summation Underscript i less than j Endscripts o Superscript left parenthesis i comma j right parenthesis Baseline left parenthesis upper B Subscript i Baseline right parenthesisBj = ε
i<j o(i,j)(Bi). Here each node is a speciﬁc tensor. 
Unlike conventional convolutions, our BNAS is achieved by transforming all 
convolutions in script upper OO into binarized convolutions. We denote the full-precision and 
binarized kernels as X and ModifyingAbove upper X With caretˆX, respectively. A convolution operation in script upper OO is 
represented as upper B Subscript j Baseline equals upper B Subscript i Baseline circled times ModifyingAbove upper X With caretBj = Bi ⊗ˆX as shown in Fig. 3.13b, where circled times⊗denotes convolution. 
To build BNAS, one critical step is how to binarize the kernels from X to ModifyingAbove upper X With caretˆX, which 
can be implemented based on state-of-the-art BNNs, such as XNOR or PCNN. 
Optimizing BNNs is more challenging than conventional CNNs [30, 65], adding an 
additional burden to NAS. To solve it, we introduce channel sampling and reduction 
in operating space in differentiable NAS to signiﬁcantly reduce the cost of GPU 
hours, leading to efﬁcient BNAS.

3.3
Binary Neural Architecture Search
81
3.3.1.2 
Binarized Optimization for BNAS 
The inference process of a BNN model is based on binarized kernels, which means 
that the kernels must be binarized in the forward step (corresponding to inference) 
during training. Contrary to the forward process, the resulting kernels are not 
binarized during back propagation and can be full-precision. 
To achieve binarized weights, we ﬁrst divide each convolutional kernel into two 
parts (amplitude and direction) and formulate the current binarized methods in a 
uniﬁed framework. We elaborate D, A, and ModifyingAbove upper A With caret ˆA: upper D Subscript i Superscript lDl
i are the directions of the full-
precision kernels upper X Subscript i Superscript lXl
i of the l t hlth convolutional layer, l element of StartSet 1 comma midline horizontal ellipsis comma upper N EndSetl ∈{1, · · · , N}; upper A Superscript lAl shared by 
all upper D Subscript i Superscript lDl
i represents the amplitude of the l t hlth convolutional layer; ModifyingAbove upper A With caret Superscript l ˆAl and upper A Superscript lAl are of the 
same size; and all elements of ModifyingAbove upper A With caret Superscript l ˆAl are equal to the average of the elements of upper A Superscript lAl. 
In the forward pass, ModifyingAbove upper A With caret Superscript l ˆAl is used instead of the full-precision upper A Superscript lAl. In this case, ModifyingAbove upper A With caret Superscript l ˆAl can 
be considered a scalar. Full-precision upper A Superscript lAl is only used for back propagation during 
training. Note that our formulation can represent both XNOR based on the scalar 
and simpliﬁed PCNN [30] whose scalar is learnable as a projection matrix. 
We represent ModifyingAbove upper X With caretˆX by the amplitude and direction as 
ModifyingAbove upper X With caret equals ModifyingAbove upper A With caret circled dot upper D comma ˆX = ˆA O D,
(3.38) 
where circled dotO denotes the element-wise multiplication between matrices. We then deﬁne 
an amplitude loss function to reconstruct the full-precision kernels as: 
upper L Subscript ModifyingAbove upper A With caret Baseline equals StartFraction theta Over 2 EndFraction sigma summation Underscript i comma l Endscripts parallel to upper X Subscript i Superscript l Baseline minus ModifyingAbove upper X With caret Subscript i Superscript l Baseline parallel to squared equals StartFraction theta Over 2 EndFraction sigma summation Underscript i comma l Endscripts parallel to upper X Subscript i Superscript l Baseline minus ModifyingAbove upper A With caret Superscript l Baseline circled dot upper D Subscript i Superscript l Baseline parallel to squared commaL ˆA = θ
2
ε
i,l
||Xl
i −ˆXl
i||2 = θ
2
ε
i,l
||Xl
i −ˆAl O Dl
i||2,
(3.39) 
where upper D Subscript i Superscript l Baseline equals s i g n left parenthesis upper X Subscript i Superscript l Baseline right parenthesisDl
i = sign(Xl
i) represents the binarized kernel. upper X Subscript i Superscript lXl
i is the full-precision model 
updated during the backpropagation process in PCNNs, while ModifyingAbove upper A With caret Superscript l ˆAl is calculated 
based on a closed-form solution in XNOR. Element-wise multiplication combines 
binarized kernels and amplitude matrices to approximate full-precision kernels. The 
ﬁnal loss function is deﬁned by considering: 
upper L Subscript upper S Baseline equals StartFraction 1 Over 2 upper S EndFraction sigma summation Underscript s Endscripts parallel to ModifyingAbove upper Y With caret Subscript s Baseline minus upper Y Subscript s Baseline parallel to Subscript 2 Superscript 2 Baseline commaLS = 1
2S
ε
s
|| ˆYs −Ys||2
2,
(3.40) 
where ModifyingAbove upper Y With caret Subscript sˆYs is the label of the s t hsth example and upper Y Subscript sYs is the corresponding classiﬁcation 
results. Finally, the overall loss function L is applied to supervise the training of 
BNAS in back propagation as: 
upper L equals upper L Subscript upper S Baseline plus upper L Subscript ModifyingAbove upper A With caret Baseline periodL = LS + L ˆA.
(3.41) 
Binarized optimization is used to optimize neural architecture search, leading 
to our binarized neural architecture search (BNAS). To this end, we use partially

82
3
Binary Neural Architecture Search
connected DARTS (PC-DARTS) to achieve operation potential ordering, which 
serves as a warm-up step for our BNAS. Denote by upper L Subscript t r a i nLtrain and upper L Subscript v a lLval the training 
and validation losses, respectively. Both losses are determined by the architecture 
alphaα and the binarized weights ModifyingAbove upper X With caretˆX in the network. The goal of the warm-up step is to 
ﬁnd ModifyingAbove upper X With caret Superscript asteriskˆX∗and alpha Superscript asteriskα∗that minimize the validation loss upper L Subscript v a l Baseline left parenthesis ModifyingAbove upper X With caret Superscript asterisk Baseline comma alpha Superscript asterisk Baseline right parenthesisLval( ˆX∗, α∗), where the weights 
ModifyingAbove upper X With caret Superscript asteriskˆX∗associated with the architecture are obtained by minimizing the training loss 
ModifyingAbove upper X With caret Superscript asterisk Baseline equals arg min Underscript ModifyingAbove upper X With caret Endscripts upper L Subscript t r a i n Baseline left parenthesis ModifyingAbove upper X With caret comma alpha Superscript asterisk Baseline right parenthesis ˆX∗= arg min
ˆX
Ltrain( ˆX, α∗). 
This implies a bilevel optimization problem with alphaα as the upper-level variable 
and ModifyingAbove upper X With caretˆX as the lower-level variable: 
StartLayout 1st Row 1st Column arg min Underscript alpha Endscripts 2nd Column upper L Subscript v a l Baseline left parenthesis ModifyingAbove upper X With caret Superscript asterisk Baseline comma alpha right parenthesis 2nd Row 1st Column s italic period t italic period 2nd Column ModifyingAbove upper X With caret Superscript asterisk Baseline equals arg min Underscript ModifyingAbove upper X With caret Endscripts upper L Subscript t r a i n Baseline left parenthesis ModifyingAbove upper X With caret comma alpha right parenthesis period EndLayout
arg min
α
Lval( ˆX∗, α)
s.t. ˆX∗= arg min
ˆX
Ltrain( ˆX, α).
(3.42) 
To better understand our method, we also review the core idea of PC-DARTS, 
which can take advantage of partial channel connections to improve memory 
efﬁciency. For example, the connection from upper B Subscript iBi to upper B Subscript jBj involves deﬁning a channel 
sampling mask upper S Superscript left parenthesis i comma j right parenthesisS(i,j), which assigns 1 to selected channels and 0 to masked ones. 
The selected channels are sent to a mixed computation of StartAbsoluteValue script upper O Superscript left parenthesis i comma j right parenthesis Baseline EndAbsoluteValue|O(i,j)| operations, while 
the masked ones bypass these operations. They are copied directly to the output, 
which is formulated as: 
StartLayout 1st Row 1st Column Blank 2nd Column f Superscript left parenthesis i comma j right parenthesis Baseline left parenthesis upper B Subscript i Baseline comma upper S Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript o Subscript k Superscript i comma j Baseline element of script upper O Superscript left parenthesis i comma j right parenthesis Baseline Endscripts StartFraction e x p left brace alpha Subscript o Sub Subscript k Sub Superscript left parenthesis i comma j right parenthesis Subscript Baseline right brace Over sigma summation Underscript o Subscript k prime Superscript left parenthesis i comma j right parenthesis Baseline element of script upper O Superscript left parenthesis i comma j right parenthesis Baseline Endscripts e x p left brace alpha Subscript o Sub Subscript k prime Sub Superscript left parenthesis i comma j right parenthesis Subscript Baseline right brace EndFraction dot o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline left parenthesis upper S Superscript left parenthesis i comma j right parenthesis Baseline asterisk upper B Subscript i Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column plus left parenthesis 1 minus upper S Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis asterisk upper B Subscript i Baseline comma EndLayout
f (i,j)(Bi, S(i,j))
=
ε
oi,j
k ∈O(i,j)
exp{αo(i,j)
k
}
ε
o(i,j)
k'
∈O(i,j) exp{αo(i,j)
k'
} · o(i,j)
k
(S(i,j) ∗Bi)
+ (1 −S(i,j)) ∗Bi,
(3.43) 
where upper S Superscript left parenthesis i comma j right parenthesis Baseline asterisk upper B Subscript iS(i,j) ∗Bi and left parenthesis 1 minus upper S Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis asterisk upper B Subscript i(1 −S(i,j)) ∗Bi denote the selected and masked channels, 
respectively, and alpha Subscript o Sub Subscript k Sub Superscript left parenthesis i comma j right parenthesisαo(i,j)
k
is the parameter of operation o Subscript k Superscript left parenthesis i comma j right parenthesiso(i,j)
k
between upper B Subscript iBi and upper B Subscript jBj. 
PC-DARTS sets the proportion of selected channels to 1 divided by upper C1/C by considering C as a 
hyperparameter. In this case, the computational cost can be reduced by C. However, 
the size of the entire search space is 2 times upper K Superscript StartAbsoluteValue script upper E Super Subscript script upper M Superscript EndAbsoluteValue2×K|EM|, where script upper E Subscript script upper MEM is the set of possible edges 
with M intermediate nodes in the fully connected DAG, and the “2” comes from the 
two types of cells. In our case with upper M equals 4M = 4, together with the two input nodes, the 
total number of cell structures in the search space is 2 times 8 Superscript 2 plus 3 plus 4 plus 5 Baseline equals 2 times 8 Superscript 142 × 82+3+4+5 = 2 × 814. This  
is a vast space to search for binarized neural architectures, which need more time 
than a full-precision NAS. Therefore, efﬁcient optimization strategies are required 
for BNAS.

3.3
Binary Neural Architecture Search
83
3.3.1.3 
Performance-Based Strategy for BNAS 
Reinforcement learning could be more efﬁcient in architecture search due to delayed 
rewards in network training. That is, the evaluation of a structure is usually done 
after the network training converges. On the other hand, we can evaluate a cell 
when training the network. Inspired by [85], we use a performance-based strategy 
to increase search efﬁciency by a large margin. [85] did a series of experiments 
showing that in the early stage of training, the validation accuracy ranking of 
different network architectures is not a reliable indicator of the quality of the 
ﬁnal architecture. However, we observe that the results of the experiments suggest 
that if an architecture performs poorly at the beginning of training, there is little 
hope that it can be part of the ﬁnal optimal model. As training progresses, this 
observation shows less uncertainty. Based on this observation, we derive a simple 
yet effective operation-abandoning process. We progressively abandon the worst-
performing operation on each edge during training and increasing epochs. 
To this end, we reduce the search space StartSet script upper O Superscript left parenthesis i comma j right parenthesis Baseline EndSet{O(i,j)} after the warm-up step achieved 
by PC-DARTS to increase search efﬁciency. According to StartSet alpha Subscript o Sub Subscript k Sub Superscript left parenthesis i comma j right parenthesis Subscript Baseline EndSet{αo(i,j)
k
}, we can select 
half of the operations with less potential than script upper O Superscript left parenthesis i comma j right parenthesisO(i,j) for each edge, resulting in 
script upper O Subscript s m a l l e r Superscript left parenthesis i comma j right parenthesisO(i,j)
smaller. After that, we randomly sample one operation from the upper K divided by 2K/2 operations in 
script upper O Subscript s m a l l e r Superscript left parenthesis i comma j right parenthesisO(i,j)
smaller for every edge, then obtain the validation accuracy by training the sampled 
network for one epoch, and ﬁnally assign this accuracy to all the sampled operations. 
These three steps are performed upper K divided by 2K/2 times by sampling without replacement, giving 
each operation exactly one accuracy for every edge. 
We repeat it T times. Thus, each operation for every edge has T accuracies 
StartSet y Subscript k comma 1 Superscript left parenthesis i comma j right parenthesis Baseline comma y Subscript k comma 2 Superscript left parenthesis i comma j right parenthesis Baseline comma ellipsis comma y Subscript k comma upper T Superscript left parenthesis i comma j right parenthesis Baseline EndSet{y(i,j)
k,1 , y(i,j)
k,2 , . . . , y(i,j)
k,T }. Then we deﬁne the selection likelihood of the kth oper-
ation in script upper O Subscript s m a l l e r Superscript left parenthesis i comma j right parenthesisO(i,j)
smaller for each edge as: 
s Subscript s m a l l e r Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis equals StartFraction e x p left brace y overbar Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right brace Over sigma summation Underscript m Endscripts e x p left brace y overbar Subscript m Superscript left parenthesis i comma j right parenthesis Baseline right brace EndFraction commassmaller(o(i,j)
k
) =
exp{ ¯y(i,j)
k
}
ε
m exp{ ¯y(i,j)
m
}
,
(3.44) 
where y overbar Subscript k Superscript left parenthesis i comma j right parenthesis Baseline equals StartFraction 1 Over upper T EndFraction sigma summation Underscript t Endscripts y Subscript k comma t Superscript left parenthesis i comma j right parenthesis ¯y(i,j)
k
=
1
T
ε
t y(i,j)
k,t . And the selection likelihoods of the other operations 
not in script upper O Subscript s m a l l e r Superscript left parenthesis i comma j right parenthesisO(i,j)
smaller are deﬁned as: 
StartLayout 1st Row StartLayout 1st Row 1st Column Blank 2nd Column s Subscript l a r g e r Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis equals 3rd Column one half left parenthesis max Underscript o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline Endscripts left brace s Subscript s m a l l e r Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis right brace plus StartFraction 1 Over left ceiling upper K divided by 2 right ceiling EndFraction sigma summation Underscript o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline Endscripts s Subscript s m a l l e r Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis right parenthesis comma EndLayout EndLayoutslarger(o(i,j)
k
) =
1
2(max
o(i,j)
k
{ssmaller(o(i,j)
k
)} +
1
|K/2|
ε
o(i,j)
k
ssmaller(o(i,j)
k
)),
(3.45) 
where left ceiling upper K divided by 2 right ceiling|K/2| denotes the smallest integer greater than or equals upper K divided by 2≥K/2. It is used because K can be 
an odd integer during iteration in the proposed Algorithm 6. Equation 3.45 is an

84
3
Binary Neural Architecture Search
estimate for the remaining operations using a value balanced between the maximum 
and the average of s Subscript s m a l l e r Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesisssmaller(o(i,j)
k
). Then, s left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesiss(o(i,j)
k
) is updated by: 
StartLayout 1st Row StartLayout 1st Row 1st Column s left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis left arrow 2nd Column one half s left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis plus q Subscript k Superscript left parenthesis i comma j right parenthesis Baseline s Subscript s m a l l e r Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis plus 3rd Column left parenthesis 1 minus q Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis s Subscript l a r g e r Baseline left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis comma EndLayout EndLayouts(o(i,j)
k
) ←1
2s(o(i,j)
k
) + q(i,j)
k
ssmaller(o(i,j)
k
)+
(1 −q(i,j)
k
)slarger(o(i,j)
k
),
(3.46) 
where q Subscript k Superscript left parenthesis i comma j right parenthesisq(i,j)
k
is a mask, which is 1 for the operations in script upper O Subscript s m a l l e r Superscript left parenthesis i comma j right parenthesisO(i,j)
smaller and 0 for the others. 
When searching for BNAS, we do not use PC-DARTS as a warm-up to 
consider efﬁciency because quantizing feature maps is slower. Therefore, script upper O Subscript s m a l l e r Superscript left parenthesis i comma j right parenthesisO(i,j)
smaller
is script upper O Superscript left parenthesis i comma j right parenthesisO(i,j). Also, we introduce an exploration term into Eq. 3.46 based on bandit [2]. 
In machine learning, the multi-armed bandit problem is a classic reinforcement 
learning problem that exempliﬁes the exploration-exploitation trade-off dilemma: 
shall we stick to an arm that has given high reward so far (exploitation) or rather 
probe other arms further (exploration)? The upper conﬁdence bound (UCB) is 
widely used for dealing with the exploration-exploitation dilemma in the multi-
armed bandit problem. Then, with the above analysis, Eq. 3.46 becomes: 
s left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis left arrow s left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis plus delta asterisk StartRoot StartFraction 2 log upper N Over n Subscript k comma t Superscript left parenthesis i comma j right parenthesis Baseline EndFraction EndRoots(o(i,j)
k
) ←s(o(i,j)
k
) + δ ∗
/
2 log N
n(i,j)
k,t
(3.47) 
where N is the total number of samples, n Subscript k comma t Superscript left parenthesis i comma j right parenthesisn(i,j)
k,t
refers to the number of times the kth 
operation of the edge left parenthesis i comma j right parenthesis(i, j) has been selected, and t is the epoch index. The ﬁrst item 
in Eq. 3.47 is the value term, which favors historically good operations. The second 
is the exploration term, which allows operations to get an exploration bonus that 
grows with log upper Nlog N. And this work uses delta equals 2δ = 2 to balance the value and exploration 
terms. We also test other values, which achieve slightly worse results. Thus, 1-bit 
convolutions, which misbehave in sufﬁcient trials, are prone to be abandoned. 
Finally, we abandon the operation with a minimal likelihood of selection for 
each edge. The size of the search space is signiﬁcantly reduced from 2 times StartAbsoluteValue script upper O Superscript left parenthesis i comma j right parenthesis Baseline EndAbsoluteValue Superscript 142 × |O(i,j)|14
to 2 times left parenthesis StartAbsoluteValue script upper O Superscript left parenthesis i comma j right parenthesis Baseline EndAbsoluteValue minus 1 right parenthesis Superscript 142 × (|O(i,j)| −1)14. We have the following: 
script upper O Superscript left parenthesis i comma j right parenthesis Baseline left arrow script upper O Superscript left parenthesis i comma j right parenthesis Baseline minus StartSet arg min Underscript o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline Endscripts s left parenthesis o Subscript k Superscript left parenthesis i comma j right parenthesis Baseline right parenthesis EndSet periodO(i,j) ←O(i,j) −{arg min
o(i,j)
k
s(o(i,j)
k
)}.
(3.48) 
The optimal structure is obtained when only one operation is left on each edge. 
Our performance-based search algorithm is presented in Algorithm 6. Note that 
in line 1, PC-DARTS is performed for L epochs as a warm-up to ﬁnd an initial 
architecture, and line 14 is used to update the architecture parameters alpha Subscript o Sub Subscript k Sub Superscript left parenthesis i comma j right parenthesisαo(i,j)
k
for all 
edges due to reduction of the search space StartSet script upper O Superscript left parenthesis i comma j right parenthesis Baseline EndSet{O(i,j)}.

3.3
Binary Neural Architecture Search
85
Algorithm 6: Performance-based search 
A set of algori thms f or Performa nce-ba sed search . It calls t he inputs an d  cr ea tes t
he opt imal outpu
t 
str uc ture  alp ha. It
 searche s an arc hitecture fo
r L  epochs  by  sampling, tr ains  the sele cted a rch itecture, get s the acc
ura cy on the validat
io
n data,  and up
dates the search sp ac e.
3.46A 
set
 of  alg orithm
s f
or Perf ormance-ba
se
d search . It calls the inputs and creates the optimal output structure alpha. It searches an architecture for L epochs by sampling, trains the selected architecture, gets the accuracy on the validation data, and updates the search space. 3.47A 
set 
of a lgo
rit
hms for  Per formanc e-base d search.  It ca lls the inputs and creates the optimal output structure alpha. It searches an architecture for L epochs by sampling, trains the selected architecture, gets the accuracy on the validation data, and updates the search space. 3.48A 
set
 of alg orit hms for Perfo rman ce -based search . It calls the inputs and
 cr
eates the o
ptim al o utput structure alpha. It searches an architecture for L epochs by sampling, trains the selected architecture, gets the accuracy on the validation data, and updates the search space.
3.3.1.4 
Gradient Update for BNAS 
In BNAS, ModifyingAbove upper X With caret Superscript lˆXl in the l t hlth layer is used to calculate the output feature maps upper F Superscript l plus 1F l+1 as: 
upper F Superscript l plus 1 Baseline equals upper A upper C c o n v left parenthesis upper F Superscript l Baseline comma ModifyingAbove upper X With caret Superscript l Baseline right parenthesis commaF l+1 = ACconv(F l, ˆXl),
(3.49) 
where ACconv denotes the amplitude convolution operation designed in Eq. 3.50. 
In ACconv, the output feature map channels are generated as follows: 
upper F Subscript h Superscript l plus 1 Baseline equals sigma summation Underscript i comma g Endscripts upper F Subscript g Superscript l Baseline circled times ModifyingAbove upper X With caret Subscript i Superscript l Baseline commaF l+1
h
=
ε
i,g
F l
g ⊗ˆXl
i,
(3.50) 
where circled times⊗denotes the convolution operation; upper F Subscript h Superscript l plus 1F l+1
h
is the h t hhth feature map in the 
left parenthesis l plus 1 right parenthesis t h(l + 1)th convolutional layer; and upper F Subscript g Superscript lF l
g denotes the g t hgth feature map in the l t hlth
convolutional layer. Note that the BNAS kernels are binarized, whereas for 1-bit 
BNAS, both the kernels and the activations are binarized. Similar to previous work 
[30, 57, 65], the 1-bit BNAS is obtained by binarizing the kernels and activations 
simultaneously. In addition, we replace ReLU with PReLU to reserve harmful 
elements generated by a 1-bit convolution.

86
3
Binary Neural Architecture Search
In BNAS, full-precision kernels upper X Subscript iXi and amplitude matrices A need to be learned 
and updated. The kernels and the matrices are jointly learned. BNAS updates the 
full-precision kernels and amplitude matrices in each convolutional layer. In what 
follows, the layer index l is omitted for simplicity. 
We denote delta Subscript upper X Sub Subscript iδXi as the gradient of the full-precision kernel upper X Subscript iXi, and we have: 
delta Subscript upper X Sub Subscript i Subscript Baseline equals StartFraction partial differential upper L Over partial differential upper X Subscript i Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper X Subscript i Baseline EndFraction plus StartFraction partial differential upper L Subscript ModifyingAbove upper A With caret Baseline Over partial differential upper X Subscript i Baseline EndFraction commaδXi = ∂L
∂Xi
= ∂LS
∂Xi
+ ∂L ˆA
∂Xi
,
(3.51) 
upper X Subscript i Baseline left arrow upper X Subscript i Baseline minus eta 1 delta Subscript upper X Sub Subscript i Subscript Baseline commaXi ←Xi −η1δXi,
(3.52) 
where eta 1η1 is a learning rate. Then we have: 
StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper X Subscript i Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper X With caret Subscript i Baseline EndFraction dot StartFraction partial differential ModifyingAbove upper X With caret Subscript i Baseline Over partial differential upper X Subscript i Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper X With caret Subscript i Baseline EndFraction dot ModifyingAbove upper A With caret dot double struck 1 comma∂LS
∂Xi
= ∂LS
∂ˆXi
· ∂ˆXi
∂Xi
= ∂LS
∂ˆXi
· ˆA · 1,
(3.53) 
StartFraction partial differential upper L Subscript ModifyingAbove upper A With caret Baseline Over partial differential upper X Subscript i Baseline EndFraction equals theta dot left parenthesis upper X Subscript i Baseline minus ModifyingAbove upper A With caret circled dot upper D Subscript i Baseline right parenthesis comma∂L ˆA
∂Xi
= θ · (Xi −ˆA O Di),
(3.54) 
where upper X Subscript iXi is the full-precision convolutional kernel corresponding to upper D Subscript iDi and double struck 11 is the 
indicator function [65] widely used to estimate the gradient of the nondifferentiable 
function. 
After updating X, we update the amplitude matrix A. Let  delta Subscript upper AδA be the gradient of 
upper AA. According to Eq. 3.41, we have:  
delta Subscript upper A Baseline equals StartFraction partial differential upper L Over partial differential upper A EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper A EndFraction plus StartFraction partial differential upper L Subscript ModifyingAbove upper A With caret Baseline Over partial differential upper A EndFraction commaδA = ∂L
∂A = ∂LS
∂A + ∂L ˆA
∂A ,
(3.55) 
upper A left arrow StartAbsoluteValue upper A minus eta 2 delta Subscript upper A Baseline EndAbsoluteValue commaA ←|A −η2δA|,
(3.56) 
where eta 2η2 is another learning rate. Note that the amplitudes are always set to be 
nonnegative. Then we have: 
StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper A EndFraction equals sigma summation Underscript i Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper X With caret Subscript i Baseline EndFraction dot StartFraction partial differential ModifyingAbove upper X With caret Subscript i Baseline Over partial differential ModifyingAbove upper A With caret EndFraction dot StartFraction partial differential ModifyingAbove upper A With caret Over partial differential upper A EndFraction equals sigma summation Underscript i Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper X With caret Subscript i Baseline EndFraction dot upper D Subscript i Baseline comma∂LS
∂A =
ε
i
∂LS
∂ˆXi
· ∂ˆXi
∂ˆA
· ∂ˆA
∂A =
ε
i
∂LS
∂ˆXi
· Di,
(3.57) 
StartFraction partial differential upper L Subscript ModifyingAbove upper A With caret Baseline Over partial differential upper A EndFraction equals StartFraction partial differential upper L Subscript ModifyingAbove upper A With caret Baseline Over partial differential ModifyingAbove upper A With caret EndFraction dot StartFraction partial differential ModifyingAbove upper A With caret Over partial differential upper A EndFraction equals negative theta dot left parenthesis upper X Subscript i Baseline minus ModifyingAbove upper A With caret circled dot upper D Subscript i Baseline right parenthesis dot upper D Subscript i Baseline comma∂L ˆA
∂A = ∂L ˆA
∂ˆA
· ∂ˆA
∂A = −θ · (Xi −ˆA O Di) · Di,
(3.58) 
where StartFraction partial differential ModifyingAbove upper A With caret Over partial differential upper A EndFraction∂ˆA
∂A is set to 1 for an easy implementation of the algorithm. Note that ModifyingAbove upper A With caret ˆA and 
A are used in the forward-pass and back propagation asynchronously. The above 
derivations show that BNAS is learnable with the new BP algorithm.

3.3
Binary Neural Architecture Search
87
3.3.1.5 
Ablation Study 
We use the same datasets and evaluation metrics as the existing NAS works 
[8, 55, 56, 96]. First, most experiments are conducted on CIFAR-10 [44], and the 
color intensities of all images are normalized to left bracket negative 1 comma plus 1 right bracket[−1, +1]. During the architecture 
search, the 50K training samples of CIFAR-10 are divided into two subsets of 
equal size, one to train the network weights and the other to search the architecture 
hyperparameters. When reducing the search space, we randomly select 5K images 
from the training set as a validation set (used on line 8 of Algorithm 6). Especially 
for 1-bit BNAS, we replace ReLU with PReLU to avoid the disappearance of 
negative numbers generated by a 1-bit convolution. The bandit strategy is introduced 
to solve the insufﬁcient training problem caused by the binarization of both kernels 
and activations. To further show the efﬁciency of our method, we also search for the 
architecture on ImageNet directly. 
In the search process, we consider a total of six cells in the network, where 
the reduction cell is inserted in the second and fourth layers, and the others are 
normal cells. There are upper M equals 4M = 4 intermediate nodes in each cell. Our experiments 
follow PC-DARTS. We set the hyperparameter C in PC-DARTS to 2 for CIFAR-
10 so that only 1 divided by 21/2 features are sampled for each edge. The batch size is set to 
128 during the search for an architecture for upper L equals 5L = 5 epochs based on script upper O Superscript left parenthesis i comma j right parenthesisO(i,j) (line 
1 of Algorithm 6). Note that for 5 less than or equals upper L less than or equals 105 ≤L ≤10, a larger L has little effect on the 
ﬁnal performance but costs more search time, as shown in Table 3.4. We freeze 
network hyperparameters, such as alphaα, and allow only network parameters, such as 
ﬁlter weights, to be tuned in the ﬁrst three epochs. Then in the next two epochs, we 
train both the network hyperparameters and the network parameters. This is done to 
provide initialization for the network parameters, thus alleviating the drawback of 
parameterized operations compared to free-parameter operations. We also set upper T equals 3T = 3
(line 4 in Algorithm 6) and upper V equals 1V = 1 (line 14), so the network is trained in fewer than 
60 epochs, with a larger batch size of 400 (due to few operation samplings) during 
the reduction of the search space. The initial number of channels is 16. We use 
momentum-based SGD to optimize network weights, with an initial learning rate 
of 0.0250.025 (annealed to zero following a cosine schedule), a momentum of 0.9, and 
a weight decay of 5 times 10 Superscript negative 45 × 10−4. The learning rate to ﬁnd the hyperparameters is set to 
0.010.01. When we search for the architecture directly on ImageNet, we use the same 
parameters as when searching on CIFAR-10, except that the initial learning rate is 
set to 0.050.05
Table 3.4 With different L, the accuracy and search cost of BNAS based on PCNN on the 
CIFAR10 dataset 
L 
Model
3
5
7
9
11 
Accuracy (%)
95.8
96.06
95.94
96.01
96.03 
Search cost
0.0664
0.09375
0.1109
0.1321
0.1687

88
3
Binary Neural Architecture Search
3.3.2 
BDetNAS: A Fast Binarized Detection Neural 
Architecture Search 
3.3.2.1 
Search Space 
We follow the same settings as previous NAS works [56, 91] to search for a 
computation cell as the building block of the ﬁnal architecture. As plotted in 
Fig. 3.14, the search space is related to three main elements: node, cell, and network. 
We will describe the binarized architecture search space and the method to build the 
binarized network as below. 
Node As the fundamental elements that compose cells, each node Fi is a set of 
speciﬁc feature maps. To formulate a directed acyclic graph (DAG), each node has 
its connections. Possible operation set between nodes (i, j) is denoted as Oi,j, where 
a practicable operation is selected to transform Fi to Fj as shown in Fig. 3.14c. 
In a cell, nodes can be divided into three categories: input node, intermediate 
node, and output node. Each cell takes the output of previous two cells as the input 
node. And each intermediate node takes the input node and previous intermediate 
nodes as the input. Then we concatenate all intermediate nodes to formulate the 
ﬁnal output node. Following this guideline as [56], we form a possible operation 
set, denoted as O, consisting of |O| = 8 operations: (1) 3 × 3 max pooling, (2) no 
Fig. 3.14 (a) The Faster R-CNN detector with searched network consisting of stacked cell. (b) A  
cell contains seven nodes; two input nodes F−1, F0; four intermediate nodes F1, F2, F3, F4 that 
apply sampled operations on the input nodes and upper nodes; and an output node that concatenates 
the outputs of the four intermediate nodes. © denotes the concatenating operation. (c) The search 
space of BDetNAS, link operation between input, and intermediate nodes will be selected among 
the possible operations Oi,j

3.3
Binary Neural Architecture Search
89
connection (zero), (3) 3 times 33×3 average pooling, (4) skip connection (identity), (5) 3 times 33×3
dilated convolution with rate 2, (6) 5 times 55 × 5 dilated convolution with rate 2, (7) 3 times 33 × 3
depth-wise separable convolution, and (8) 5 times 55 × 5 depth-wise separable convolution. 
Moreover, a binarized NAS is achieved by transforming all the convolutions in script upper OO to 
binarized convolutions as shown in Fig. 3.14c. 
Cell A cell is deﬁned as a tiny convolutional network with complex connections 
and multiple operation layers. Cells can be categorized into two classes, i.e., normal 
cell and reduction cell. We deﬁne the input shape of cells as upper K times upper W times upper CK × W × C. A normal 
cell uses the operations with stride 1, so its input and output shape are identical, i.e., 
upper K times upper W times upper CK × W × C. Following the guideline of common heuristic in most human designed 
convolutional neural networks [36, 41, 70], C is doubled when the stride is 2. Hence, 
a reduction cell uses the operations with the stride set to 2, and the output shape is 
upper K divided by 2 times upper W divided by 2 times 2 upper CK/2 × W/2 × 2C. 
We set the cell according to [56], which is formed by seven nodes and 
correspondingly 2 plus 3 plus 4 plus 5 equals 142+3+4+5 = 14 possible connections as illustrated in Fig. 3.14b. 
The edge between two nodes denotes a possible operation which will be selected 
according to the performance-based strategy. In training, we form an architecture 
every epoch by sampling operations without replacement. And we optimize the 
search space according to our search space reduction algorithm. In addition, we 
should cut the 14 possible connections down to 8. Thus, we select the top eight 
probabilities to generate the ﬁnal cells in testing. Therefore, the size of the whole 
search space is 2 times 8 Superscript 2 plus 3 plus 4 plus 5 Baseline equals 2 times 8 Superscript 142×82+3+4+5 = 2×814, which is an extremely large space to search. 
Hence, efﬁcient optimization methods are required. 
Network A backbone network consists of a predeﬁned number of stacked cells, 
which take the output of two previous cells as the input. Among the cells, there 
are either normal cells with stride set as 1 or reduction cells with the stride set 
as 2. Following [56], we employ two stem cells with the total stride set as 8 to 
preprocess the raw image. Hence, only two kinds of cells are generated. Based on 
the performance ranking hypothesis [91], we train a small stacked network with six 
cells (two reduction cells and f our  normal cells) for search. And then we employ 
the corresponding 20-cell network of the optimal 6-cell network for pre-training and 
ﬁne-tuning. A Faster R-CNN detector [68] with the searched backbone is plotted as 
shown in Fig. 3.14a. 
3.3.2.2 
Performance-Based Strategy for BDetNAS 
The core idea of our search algorithm is to sample randomly and reduce the search 
space step by step according to the testing accuracy. In general, we select an edge 
between speciﬁc nodes left parenthesis i comma j right parenthesis(i, j) from operation sets and test the network compose of 
the selected edges without replacement. We record the performance information 
according to the test accuracy and accordingly optimize the search space.

90
3
Binary Neural Architecture Search
To accomplish this, we implement the operation sampling on script upper O Subscript i comma jOi,j. We randomly 
sample an edge o Subscript i comma j Superscript kok
i,j from script upper O Subscript i comma jOi,j to form a network for test. After that, we update the 
performance of each operation between nodes i and j as: 
s left parenthesis o Subscript i comma j Superscript k Baseline right parenthesis equals StartFraction sigma summation Underscript e equals 1 Overscript upper N Endscripts y Superscript e Baseline dot m Subscript i comma j Superscript k comma e Baseline Over sigma summation Underscript e equals 1 Overscript upper N Endscripts m Subscript i comma j Superscript k comma e Baseline EndFraction commas(ok
i,j) =
εN
e=1 ye · mk,e
i,j
εN
e=1 mk,e
i,j
,
(3.59) 
where N denotes the current training epoch and 1 less than or equals e less than or equals upper N1 ≤e ≤N. y Superscript eye denotes the test 
accuracy of the e-th epoch. And m Subscript i comma j Superscript k comma emk,e
i,j denotes an indicative variable deﬁned as: 
m Subscript i comma j Superscript k comma e Baseline equals StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column 1 comma o Subscript i comma j Superscript k Baseline i s s e l e c t e d i n e negative t h e p o c h 2nd Row 1st Column Blank 2nd Column 0 comma e l s e EndLayoutmk,e
i,j =
{ 1,
ok
i,j is selected in e−th epoch
0,
else
(3.60) 
Equation 3.59 indicates taking the average test accuracy of the epochs, where o Subscript i comma j Superscript kok
i,j is 
selected, as the performance of s left parenthesis o Subscript i comma j Superscript k Baseline right parenthesiss(ok
i,j). 
We deﬁne the iteration times script upper T equals 3T = 3 for sampling without replacement. We ﬁrst 
repeat sampling StartAbsoluteValue script upper O Subscript i comma j Baseline EndAbsoluteValue times script upper T equals 8 times 3 equals 24|Oi,j| × T = 8 × 3 = 24 epochs and then reduce the search space 
as: 
script upper O Subscript i comma j Baseline left arrow script upper O Subscript i comma j Baseline minus arg min Underscript o Subscript i comma j Superscript k Baseline Endscripts s left parenthesis o Subscript i comma j Superscript k Baseline right parenthesis periodOi,j ←Oi,j −arg min
ok
i,j
s(ok
i,j).
(3.61) 
After Eq. 3.61, the search space size of every edge is reduced to StartAbsoluteValue script upper O Subscript i comma j Baseline EndAbsoluteValue minus 1|Oi,j| −1 for 
one cell. As a result, the whole search space size is signiﬁcantly reduced from 
2 times left parenthesis StartAbsoluteValue script upper O Subscript i comma j Baseline EndAbsoluteValue right parenthesis Superscript upper L2 × (|Oi,j|)L to 2 times left parenthesis StartAbsoluteValue script upper O Subscript i comma j Baseline EndAbsoluteValue minus 1 right parenthesis Superscript upper L2 × (|Oi,j| −1)L, where L is the number of cells. Then we repeat 
the search space reduction process until StartAbsoluteValue script upper O Subscript i comma j Baseline EndAbsoluteValue equals 1|Oi,j| = 1 to achieve the ﬁnal architecture. 
The number of total epochs is left parenthesis 8 plus 7 plus midline horizontal ellipsis plus 2 right parenthesis asterisk 3 equals 108(8 + 7 + · · · + 2) ∗3 = 108, which is efﬁcient. 
3.3.2.3 
Optimization for BDetNAS 
To achieve a binarized NAS, kernel weights are binarized by decomposing the full-
precision kernel X into amplitude and direction as: 
ModifyingAbove upper X With caret equals upper A dot upper D comma ˆX = A · D,
(3.62) 
where A and D respectively denote the amplitude and the direction of X. D is the 
script l 1l1-normalized matrix, which is element-wisely calculated by s i g n left parenthesis upper X right parenthesissign(X) as StartFraction negative 1 Over StartAbsoluteValue upper X EndAbsoluteValue EndFraction−1
|X| for 
negative X and StartFraction 1 Over StartAbsoluteValue upper X EndAbsoluteValue EndFraction 1
|X| for positive X. StartAbsoluteValue upper X EndAbsoluteValue|X| denotes the number of elements in X. A is a 
scalar. Then a binarized convolution can be formulated as: 
upper F Subscript j Baseline equals upper F Subscript i Baseline ring ModifyingAbove upper X With caret Subscript i comma j Baseline commaFj = Fi ◦ˆXi,j,
(3.63) 
where ring◦denotes convolution.

3.3
Binary Neural Architecture Search
91
Algorithm 7: BDetNAS framework 
A set o f algorit hms fo r B D e t N  A S f ramew ork.  It c
alls t he  inp uts and cre ates the opti mal  ba
ckbone a rchitect ure. It t rains the sel ecte d archit ec
ture, tests t he trained 
net work, calcu lates the te
st accurac
y,
 and up dat
es t he pre-tr ain s ear ched weights.
3.65A 
se
t of  alg
or
ithms  for  B D e t  N A S f rame work. It c alls  the inputs an d cr
ea
tes the  opt imal 
backbone architecture. It trains the selected architecture, tests the trained network, calculates the test accuracy, and updates the pre-train searched weights. 3.59A 
set
 of  alg orithm s  for B 
D e
 t N A  S fr amew ork. It calls the inputs and creates the optimal backbone architecture. It trains the selected architecture, tests the trained network, calculates the test accuracy, and updates the pre-train searched weights. 3.61A 
set 
of a lgo
rit
hms  fo r 
B D e t N  A S f r am
ewor k. It call s the inp uts and  cre at
es  th e optimal 
back bone archit ecture
. It t rains the  se lected arc
hite ctur e, test s 
the trained network, calculates the test accuracy, and updates the pre-train searched weights.
We then deﬁne an amplitude loss function to reconstruct the full-precision 
kernels as: 
StartLayout 1st Row 1st Column script upper L Subscript upper A 2nd Column equals sigma summation Underscript l Overscript upper L Endscripts sigma summation Underscript upper F Subscript i Baseline comma upper F Subscript j Baseline element of script upper C Subscript l Baseline Endscripts parallel to upper X Subscript i comma j Baseline minus ModifyingAbove upper X With caret Subscript i comma j Baseline parallel to Subscript 2 Superscript 2 Baseline 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript l Overscript upper L Endscripts sigma summation Underscript upper F Subscript i Baseline comma upper F Subscript j Baseline element of script upper C Subscript l Baseline Endscripts parallel to upper X Subscript i comma j Baseline minus upper A Subscript i comma j Baseline dot upper D Subscript i comma j Baseline parallel to Subscript 2 Superscript 2 Baseline comma EndLayoutLA =
L
ε
l
ε
Fi,Fj ∈Cl
||Xi,j −ˆXi,j||2
2
=
L
ε
l
ε
Fi,Fj ∈Cl
||Xi,j −Ai,j · Di,j||2
2,
(3.64) 
where script upper C Subscript lCl denotes the l-th cell. upper X Subscript i comma jXi,j denotes the full-precision kernel and ModifyingAbove upper X With caret Subscript i comma jˆXi,j
denotes a reconstructed one. The total loss for optimization in search process is: 
script upper L equals script upper L Subscript upper C l s Baseline plus StartFraction alpha Over 2 EndFraction script upper L Subscript upper A Baseline commaL = LCls + α
2 LA,
(3.65) 
where script upper L Subscript upper C l sLCls is the conventional loss function, e.g., cross entropy. alphaα is a hyperparam-
eter. 
3.3.2.4 
Experiments 
In this section, we compare our BDetNAS with state-of-the-art manually designed 
and other NAS object detectors. Moreover, we also compare the BNNs obtained by 
our BDetNAS based on XNOR [65] to validate effectiveness of our method. More

92
3
Binary Neural Architecture Search
experimental results are also provided in the supplementary material. GPU days are 
counted according to NVIDIA GTX 1080Ti, which is the same as DetNAS [18]. All 
the experiments and models are implemented with PyTorch. 
Experimental Settings 
Search on ImageNet+VOC/COCO For search process, we use the commonly 
used 
1.281.28M ImageNet ILSVRC2012 [45] and Cropped&Resized detection 
dataset for training images, as plotted in Fig. 5.5. The Cropped&Resized VOC 
trainval07+12 [23] has 
46.946.9k images over 20 classes. Likewise, the 
Cropped&Resized COCO trainval35k [53] has 0.860.86M images over 80 classes. 
Hence, we get an augmented dataset of 
1.331.33M images for search on VOC 
trainval07+12 and of 2.142.14M images for search on COCO trainval35k. 
When calculating the accuracy, we randomly select 5K images from the training 
set as a validation set (in line 7 of Algorithm 7). As illustrated in Sect. 3.3.2.2, 108 
epochs are needed for search. And we use a batch size of 512 on 4 NVIDIA 
GTX 1080Ti GPUs for 280k iterations for ImageNet ILSVRC2012 + VOC  
trainval07+12 and 450k iterations on ImageNet ILSVRC2012 + COCO  
trainval35k. 
Pre-training on ImageNet For ImageNet classiﬁcation dataset, we use the com-
monly used 1.281.28M ImageNet ILSVRC2012 [45]. To get a pre-trained backbone on 
ImageNet, the network is trained from scratch for 250 epochs with a batch size of 
512. We use the SGD optimizer with a momentum of 0.90.9, an initial learning rate 
of 0.050.05 (decayed down to zero following a cosine schedule), and a weight decay of 
3 times 10 Superscript negative 53 × 10−5. Additional enhancements are adopted including label smoothing and an 
auxiliary loss tower during training. 
Fine-tuning on VOC/COCO We validate our method with Faster R-CNN [68] 
detector. The training images are randomly ﬂipped for augmentation. Then a 
superposition of the original data and the augmented data is used for training. 40k 
input images are employed for VOC trainval07+12 and 230k input images are 
employed for COCO trainval35k. Note that COCO trainval35k used here 
is the left part with 5k COCO minival taken away. We train on 4 GPUs with a 
total of 4 images per mini-batch for 27k iterations on VOC and 150k iterations on 
COCO. The weights of backbone are initialized with ImageNet pre-training. The 
parameters of region proposal network (RPN) are randomly initialized. We set the 
weight decay as 1 times 10 Superscript negative 41 × 10−4 and momentum as 0.90.9. Initial learning rate is 4 times 10 Superscript negative 34 × 10−3
and we decay the rate at the 8th epoch of the total ten epochs. 
Results on VOC test2007 
Hyperparameter alphaα is set as 2 times 10 Superscript negative 52 × 10−5 for search on VOC trainval07+12. 
Relevant ablation study is attached in supplementary material. We compare our 
method with manually designed networks with a similar model size, state-of-the-
art quantization methods, and networks searched by NAS. The manually designed 
backbones include ResNet [36] and VGG [70]. Binarized ResNet-34 implemented

3.3
Binary Neural Architecture Search
93
Table 3.5 Comparison with the state-of-the-art object detectors on VOC test2007 
Backbone Params 
Search Cost 
Detector
Backbone
mAP 
(M)
(GPU days) 
Faster R-CNN [68]
ResNet-18 [36]
73.2 
10.67 (32 bits)
– 
Faster R-CNN [68]
ResNet-34[36]
75.6 
20.27 (32 bits)
– 
Faster R-CNN [68]
VGG-16 [70]
73.5 
15.21 (32 bits)
– 
Faster R-CNN [68]
ResNet-34 [75]
59.0 
20.27 (1 bit)
– 
Faster R-CNN [68]
ResNet-34 [65]
54.7 
20.27 (1 bit)
– 
FPN [50]
DetNAS [18]
81.5 
4.34 (32 bits)
35 
RetinaNet [51]
DetNAS [18]
80.1 
5.07 (32 bits)
35 
Faster R-CNN [68]
FairNAS [19]
67.3 
6.72 (1 bit)
8.1 
Faster R-CNN [68]
BDetNAS (XNOR[65])
68.8 
6.23 (1 bit)
8.3 
Faster R-CNN[68]
BDetNAS
70.8 
6.51 (1 bit)
8.1 
by TBN [75] and XNOR [65] are considered in our comparison. In addition, we 
compare our BDetNAS with state-of-the-art DetNAS [18]. 
As illustrated in Table 3.5, Faster R-CNN  [68] with full-precision ResNet-18, 
VGG-16 and ResNet-34 achieves 73.273.2, 73.573.5, and75.675.6 mAP on VOC test2007, 
respectively, while BDetNAS incurs only 1.4 percent sign1.4%, 1.7 percent sign1.7%, and 4.8 percent sign4.8% mAP loss with 
a compressed model size by 
52 times52×, 
74 times74×, and 
99 times99×. For binarized ResNet-34 
implemented via XNOR [65] and TBN [75], our BDetNAS achieve 16.1 percent sign16.1% and 
11.8 percent sign11.8% mAP higher as well as compress the memory usage by 3.2 times3.2×. 
Compared with the full-precision detectors obtained by DetNAS [18], the 
binarized networks with our BDetNAS have acceptable mAP loss but with much 
more compressed models. Note that the numbers of parameters of backbones 
searched by DetNAS [18] are less than 5M. However, the binarized networks only 
need 1 bit to save one parameter, while the full-precision networks need 32 bits. 
Hence, our BDetNAS saves about 21 times21× and 25 times25× memory, which is an obviously 
superior trade-off for real applications. In terms of search efﬁciency, our framework 
searches directly on image classiﬁcation task from scratch and needs no advanced 
pre-training or ﬁne-tuning compared to DetNAS. Hence, our BDetNAS is more than 
4 times4× faster compared with DetNAS. The superiority is attributed to the proposed 
scheme of search space reduction and novel search framework. 
In addition, we reimplement FairNAS [19], i.e., random search under the same 
setup as ours for fair comparison. As illustrated in the last rows of Table 3.5, 
BDetNAS outperforms FairNAS [19] with 3.5 percent sign3.5% mAP higher after searching for 
same epochs. This demonstrates that our BDetNAS can effectively improve the 
performance of the backbone. Compared to BDetNAS implemented by XNOR [65], 
the BDetNAS with our novel quantization method achieves higher mAP with similar 
memory usage. This demonstrates our novel quantization framework is of great 
effect (Fig. 3.15).

94
3
Binary Neural Architecture Search
F-1 
F1 
skip_connect 
F3 
sep_conv_3x3 
F0 
max_pool_3x3 
F2 
max_pool_3x3 
max_pool_3x3 
Output
max_pool_3x3 
F4
dil_conv_3x3 
max_pool_3x3 
F-1 
F1 
avg_pool_3x3 
F2 
dil_conv_3x3 
F3 
dil_conv_5x5 
F0 
sep_conv_3x3 
dil_conv_5x5 
sep_conv_3x3 
F4 
sep_conv_3x3 
Output 
sep_conv_3x3 
F-1 
F1 
skip_connect 
F4 
max_pool_3x3 
F0 
max_pool_3x3 
F2 
sep_conv_3x3 
F3 
max_pool_3x3 
max_pool_3x3 
Output 
dil_conv_3x3 
max_pool_3x3 
F-1
F1 
max_pool_3x3 
F3 
max_pool_3x3 
F4 
sep_conv_3x3 
F0 
sep_conv_3x3 
F2 
dil_conv_5x5 
dil_conv_3x3 
sep_conv_3x3 
Output 
sep_conv_3x3 
(b) 
(a) 
(c) 
(d) 
Fig. 3.15 Detailed structures of the best cells discovered using BDetNAS based on our quantiza-
tion methods. In the normal cell, the stride of the operations on two input nodes is 1, and in the 
reduction cell, the stride is 2. (a) Normal cell on VOC. (b) Reduction cell on VOC. (c) Normal cell 
on COCO. (d) Reduction cell on COCO 
Results on COCO minival 
We further compare BDetNAS with other state of the arts on COCO minival. 
Hyperparameter α is set as 1×10−5 for search on COCO trainval35k. Relevant 
ablation study is attached in supplementary material. The backbones for comparison

References
95
Table 3.6 Comparison with the state-of-the-art object detectors on COCO minival 
mAP
Backbone params 
Search cost 
Detector
Backbone
AP 
APSuperscript 0.50.5
APSuperscript 0.750.75
(M)
(GPU days) 
Faster R-CNN 
[68] 
ResNet-18 [36]
32.2 
53.8 
34.0
10.67 (32 bits)
– 
Faster R-CNN 
[68] 
MobileNetV2 [69] 
29.0 
49.7 
29.5
3.4(32 bits)
– 
Faster R-CNN 
[68] 
ResNet-18 [48]
28.1 
48.4 
29.3
10.67 (1 bit)
– 
Faster R-CNN 
[68] 
MobleNetV2 [48] 
25.5 
45.3 
25.7
3.4 (1 bit)
– 
RetinaNet [51]
DetNAS [18]
34.1 
–
–
5.07 (32 bits)
44 
Faster R-CNN 
[68] 
BDetNAS
29.0 
49.2 
29.7
6.30 (1 bit)
13.4 
consist of manually designed full-precision ones such as MobileNetV2 [69] and 
ResNet-18 [36], binarized one such as FQN [48], and searched one by DetNAS [18]. 
From the results in Table 3.6, we have the following observations: (1) BDetNAS 
performs equally to human-designed light full-precision networks MobileNetV2 
(29.029.0 vs. 29.029.0) as well as save memory usage by 17 times17× on the same detector. (2) 
Compared with binarized ResNet-18 by FQN [48], BDetNAS achieves 0.4 percent sign0.4% mAP 
higher as well as compress the model by 1.7 times1.7×. And BDetNAS achieves 3.5percent sign%
mAP higher compared with binarized MobileNetV2 by FQN[48]. (3) BDetNAS 
saves memory usage by 26 times26× with only 5.1 percent sign5.1%mAP lower (29.029.0 vs. 34.134.1) compared 
with DetNAS [18] on RetinaNet [51]. Moreover, our search cost is only 30.4 percent sign30.4% of 
DetNAS. 
References 
1. Milad Alizadeh, Javier Fernández-Marqués, Nicholas D Lane, and Yarin Gal. An empirical 
study of binary neural networks’ optimisation. In Proceedings of the International Conference 
on Learning Representations, 2018. 
2. P. Auer, N. Cesa-Bianchi, and P. Fischer. 
Finite-time analysis of the multiarmed bandit 
problem. In Machine learning, 2002. 
3. Philip Bachman, R Devon Hjelm, and William Buchwalter. 
Learning representations by 
maximizing mutual information across views. In NeurIPS, 2019. 
4. Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model 
architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017. 
5. A. Buades, B. Coll, and J. Morel. A non-local algorithm for image denoising. In CVPR, 2005. 
6. Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Bats: Binary architecture search. 
In Proc. of ECCV, pages 309–325, 2020. 
7. Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efﬁcient architecture search 
by network transformation. In AAAI, 2018. 
8. Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. 
Path-level network 
transformation for efﬁcient architecture search. 
In International Conference on Machine 
Learning, pages 678–687. PMLR, 2018.

96
3
Binary Neural Architecture Search
9. Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on 
target task and hardware. In ICLR, 2019. 
10. Hanlin Chen, Baochang Zhang, Shenjun Xue, Xuan Gong, Hong Liu, Rongrong Ji, and 
David S. Doermann. 
Anti-bandit neural architecture search for model defense. 
ArXiv, 
abs/2008.00698, 2020. 
11. Hanlin Chen, Li’an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu, Rongrong Ji, 
David Doermann, and Guodong Guo. Binarized neural architecture search for efﬁcient object 
recognition. International Journal of Computer Vision, 129(2):501–516, 2021. 
12. Hanlin Chen, Li’an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu, Rongrong Ji, 
David S. Doermann, and Guodong Guo. 
Binarized neural architecture search for efﬁcient 
object recognition. International Journal of Computer Vision, 129:501–516, 2020. 
13. Jiasi Chen and Xukan Ran. Deep learning with edge computing: A review. In Proceedings of 
the IEEE, 2019. 
14. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework 
for contrastive learning of visual representations. arXiv:2002.05709, 2020. 
15. Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: 
Bridging the depth gap between search and evaluation. 
In Proceedings of the IEEE/CVF 
international conference on computer vision, pages 1294–1303, 2019. 
16. Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: 
Bridging the depth gap between search and evaluation. In Proc. of ICCV, 2019. 
17. Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum 
contrastive learning. arXiv:2003.04297, 2020. 
18. Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng, Xinyu Xiao, and Jian Sun. Detnas: 
Backbone search for object detection. In NIPS, pages 6638–6648, 2019. 
19. Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Jixiang Li. Fairnas: Rethinking evaluation fairness 
of weight sharing neural architecture search. arXiv preprint arXiv:1907.01845, 2019. 
20. Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li. 
Fair darts: Eliminating unfair 
advantages in differentiable architecture search. In Proc. of ECCV, 2020. 
21. Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. 
Regularizing activation 
distribution for training binarized deep networks. 
In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, pages 11408–11417, 2019. 
22. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisser-
man. The pascal visual object classes (voc) challenge. IJCV, 2010. 
23. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisser-
man. The pascal visual object classes (voc) challenge. International Journal of Computer 
Vision, 2010. 
24. D. Gabor. Electrical engineers part iii: Radio and communication engineering, j. Journal 
of the Institution of Electrical Engineers - Part III: Radio and Communication Engineering 
1945-1948, 1946. 
25. D. Gabor. Theory of communication. part 1: The analysis of information. Journal of the 
Institution of Electrical Engineers-Part III: Radio and Communication Engineering, 1946. 
26. Ross Girshick. Fast r-cnn. In ICCV, 2015. 
27. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for 
accurate object detection and semantic segmentation. In CVPR, 2014. 
28. Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. Detectron. 
https://github.com/facebookresearch/detectron, 2018. 
29. I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. 
arXiv, 2014. 
30. Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David 
Doermann. 
Projection convolutional neural networks for 1-bit cnns via discrete back 
propagation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2019. 
31. Raia Hadsell, Sumit Chopra, and Yann LeCun. 
Dimensionality reduction by learning an 
invariant mapping. In CVPR, 2006.

References
97
32. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections 
for efﬁcient neural network. In NIPS, pages 1135–1143, 2015. 
33. Yiwen Han, Xiaofei Wang, Victor Leung, Dusit Niyato, Xueqiang Yan, and Xu Chen. 
Convergence of edge computing and deep learning: A comprehensive survey. In arXiv, 2019. 
34. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for 
unsupervised visual representation learning. In CVPR, 2020. 
35. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 
36. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 
Deep residual learning for 
image recognition. In Proceedings of the IEEE conference on computer vision and pattern 
recognition, pages 770–778, 2016. 
37. Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient 
image recognition with contrastive predictive coding. arXiv:1905.09272, 2019. 
38. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. 
Computer Science, 14(7):38–39, 2015. 
39. R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, 
Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information 
estimation and maximization. In ICLR, 2019. 
40. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias 
Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural 
networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 
41. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias 
Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural 
networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 
42. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 
Binarized neural networks. In Advances in neural information processing systems, pages 4107– 
4115, 2016. 
43. Dahyun Kim, Kunal Pratap Singh, and Jonghyun Choi. 
Learning architectures for binary 
networks. In Proc. of ECCV, pages 575–591, 2020. 
44. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 
2009. 
45. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep 
convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 
pages 1097–1105, 2012. 
46. En Li, Liekang Zeng, Zhi Zhou, and Xu Chen. Edge ai: On-demand accelerating deep neural 
network inference via edge computing. In IEEE Transactions on Wireless Communications, 
2019. 
47. Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali Thabet, and Bernard 
Ghanem. Sgas: Sequential greedy architecture search. In Proc. of CVPR, 2020. 
48. Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully quantized 
network for object detection. 
In The IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR), June 2019. 
49. Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang, 
and Zhenguo Li. DARTS+: improved differentiable architecture search with early stopping. 
arXiv, 2019. 
50. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge 
Belongie. 
Feature pyramid networks for object detection. 
In Proceedings of the IEEE 
conference on computer vision and pattern recognition, pages 2117–2125, 2017. 
51. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense 
object detection. In Proceedings of the IEEE international conference on computer vision, 
pages 2980–2988, 2017. 
52. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, 
Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 
2014.

98
3
Binary Neural Architecture Search
53. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, 
Piotr Dollár, and C Lawrence Zitnick. 
Microsoft coco: Common objects in context. 
In 
European Conference on Computer Vision (ECCV), pages 740–755, 2014. 
54. Chenxi Liu, Piotr Dollár, Kaiming He, Ross Girshick, Alan Yuille, and Saining Xie. Are labels 
necessary for neural architecture search? arXiv:2003.12056, 2020. 
55. Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, 
Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In 
Proceedings of the European Conference on Computer Vision, pages 19–34, 2018. 
56. H. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search. In ICLR, 2019. 
57. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real 
net: Enhancing the performance of 1-bit cnns with improved representational capability and 
advanced training algorithm. In Proceedings of the European conference on computer vision 
(ECCV), pages 722–737, 2018. 
58. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models 
resistant to adversarial attacks. In ICLR, 2017. 
59. Yuyi Mao, Changsheng You, Jun Zhang, Kaibin Huang, and Khaled B Letaief. Mobile edge 
computing: Survey and research outlook. In arXiv, 2017. 
60. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive 
predictive coding. arXiv:1807.03748, 2018. 
61. Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical 
University of Denmark, 7(15):510, 2008. 
62. Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 
Efﬁcient neural 
architecture search via parameter sharing. In ICML, 2018. 
63. Hai Phan, Zechun Liu, Dang Huynh, Marios Savvides, Kwang-Ting Cheng, and Zhiqiang 
Shen. Binarizing mobilenet via evolution-based searching. In Proceedings of the IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, pages 13420–13429, 2020. 
64. Juan C. Pérez, Motasem Alfarra, Guillaume Jeanneret, Adel Bibi, Ali Kassem Thabet, Bernard 
Ghanem, and Pablo Arbeláez. Robust gabor networks. arXiv, 2019. 
65. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet 
classiﬁcation using binary convolutional neural networks. 
In European Conference on 
Computer Vision, pages 525–542. Springer, 2016. 
66. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for 
image classiﬁer architecture search. In AAAI, 2019. 
67. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time 
object detection with region proposal networks. In NeurIPS, 2015. 
68. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time 
object detection with region proposal networks. IEEE Transactions on Pattern Analysis and 
Machine Intelligence, 2016. 
69. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 
Mobilenetv2: Inverted residuals and linear bottlenecks. In The IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR), June 2018. 
70. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale 
image recognition. arXiv preprint arXiv:1409.1556, 2014. 
71. Wei Tang, Gang Hua, and Liang Wang. How to train a compact binary neural network with 
high accuracy? In Thirty-First AAAI conference on artiﬁcial intelligence, 2017. 
72. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged 
consistency targets improve semi-supervised deep learning results. In NIPS, 2017. 
73. Yonglong Tian, Dilip Krishnan, and Phillip Isola.
Contrastive multiview coding. 
arXiv:1906.05849, 2019. 
74. Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In 
ICLR, 2020. 
75. Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng Tao Shen. Tbn: 
Convolutional neural network with ternary inputs and binary weights. In Proceedings of the 
European Conference on Computer Vision (ECCV), pages 315–332, 2018.

References
99
76. Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using 
videos. In CVPR, 2015. 
77. Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial 
training. In ICLR, 2020. 
78. Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. 
Training and inference with integers 
in deep neural networks. 
In Proceedings of the International Conference on Learning 
Representationss, pages 1–14, 2018. 
79. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. 
https://github.com/facebookresearch/detectron2, 2019. 
80. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via 
non-parametric instance discrimination. In CVPR, 2018. 
81. C. Xie, Y. Wu, L. V. D. Maaten, A. L. Yuille, and K. He. Feature denoising for improving 
adversarial robustness. In CVPR, 2019. 
82. Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. 
Pc-darts: Partial channel connections for memory-efﬁcient architecture search. arXiv preprint 
arXiv:1907.05737, 2019. 
83. Shenjun Xue, Hanlin Chen, Chunyu Xie, Baochang Zhang, Xuan Gong, and David S. 
Doermann. 
Fast and unsupervised neural architecture evolution for visual representation 
learning. IEEE Computational Intelligence Magazine, 16:22–32, 2021. 
84. Shenjun Xue, Runqi Wang, Baochang Zhang, Tian Wang, Guodong Guo, and David S. 
Doermann. Idarts: Interactive differentiable architecture search. 2021 IEEE/CVF International 
Conference on Computer Vision (ICCV), pages 1143–1152, 2021. 
85. C. Ying, A. Klein, E. Real, E. Christiansen, K. Murphy, and F. Hutter. Nas-bench-101: Towards 
reproducible neural architecture search. In ICML, 2019. 
86. Hongyuan Yu and Houwen Peng. Cyclic differentiable architecture search. arXiv, 2020. 
87. Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong 
He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv:2004.08955, 
2020. 
88. Junhe Zhao, Sheng Xu, Baochang Zhang, Jiaxin Gu, David Doermann, and Guodong Guo. 
Towards compact 1-bit cnns via bayesian learning. International Journal of Computer Vision, 
pages 1–25, 2022. 
89. Xiawu Zheng, Rongrong Ji, Lang Tang, Yan Wan, Baochang Zhang, Yongjian Wu, Yunsheng 
Wu, and Ling Shao. Dynamic distribution pruning for efﬁcient network architecture search. 
arXiv preprint arXiv:1905.13543, 2019. 
90. Xiawu Zheng, Rongrong Ji, Lang Tang, Baochang Zhang, Jianzhuang Liu, and Qi Tian. 
Multinomial distribution learning for effective neural architecture search. In CVPR, 2019. 
91. Xiawu Zheng, Rongrong Ji, Lang Tang, Baochang Zhang, Jianzhuang Liu, and Qi Tian. 
Multinomial distribution learning for effective neural architecture search. In ICCV, October 
2019. 
92. Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective 
low-bitwidth convolutional neural networks. In Proceedings of the IEEE/CVF Conference on 
Computer Vision and Pattern Recognition, pages 7920–7928, 2018. 
93. Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised 
learning of visual embeddings. In ICCV, 2019. 
94. B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning transferable architectures for scalable 
image recognition. In CVPR, 2018. 
95. Barret Zoph and Quoc V Le. 
Neural architecture search with reinforcement learning. 
In 
International Conference on Learning Representations (ICLR), pages 1–16, 2017. 
96. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. 
Learning transferable 
architectures for scalable image recognition. In Proceedings of the IEEE/CVF Conference 
on Computer Vision and Pattern Recognition, pages 8697–8710, 2018.

Chapter 4 
Quantization of Neural Networks 
4.1 
Introduction 
Quantization has emerged as a highly successful strategy for both training and infer-
ence of neural networks (NN). While the challenges of numerical representation 
and quantization have been long-standing in digital computing, NNs offer unique 
opportunities for advancements in this area. Although this survey primarily focuses 
on quantization for inference, it is important to acknowledge that quantization has 
also shown promise in NN training [2, 7, 15, 25]. 
In particular, innovations in half-precision and mixed precision training have 
played a crucial role in achieving higher throughput in AI accelerators [9, 20]. 
However, pushing beyond half-precision without extensive tuning has proven to be 
a signiﬁcant challenge, and recent research on quantization has mainly centered 
around the inference stage of neural networks. 
4.2 
Quantitative Arithmetic Principles 
Given a neural network (NN) model with N layers, we represent the set of weights 
as bold upper W equals bold w Superscript n Baseline n equals 1 Superscript upper NW = wnn = 1N and the set of input features as bold upper A equals bold a Superscript n Baseline i n n equals 1 Superscript upper NA = aninn = 1N. Here, bold w Superscript nwn is the 
convolutional weight matrix for the n-th layer, with dimensions upper C Superscript n Baseline o u t times upper C Subscript i n Superscript nCnout ×Cn
in, where 
upper C Subscript i n Superscript nCn
in and upper C Subscript o u t Superscript nCn
out are the input and output channel numbers, respectively. Similarly, bold a Subscript i n Superscript nan
in
is the input feature map for the n-th layer, with dimensions upper C Subscript i n Superscript nCn
in. 
The output feature map bold a Subscript o u t Superscript nan
out of the n-th layer can be technically formulated as: 
bold a Subscript o u t Superscript n Baseline equals bold w Superscript n Baseline dot bold a Subscript i n Superscript n Baseline commaan
out = wn · an
in,
(4.1) 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
B. Zhang et al., Neural Networks with Model Compression, 
Computational Intelligence Methods and Applications, 
https://doi.org/10.1007/978-981-99-5068-3_4
101

102
4
Quantization of Neural Networks
where dot· represents matrix multiplication. For simplicity, we omit the nonlinear 
activation function in this formulation. Following the prior works [24], quantized 
neural network (QNN) intends to represent bold w Superscript nwn and bold a Superscript nan in a low-bit format as: 
StartLayout 1st Row 1st Column double struck upper Q colon 2nd Column equals StartSet q 1 comma midline horizontal ellipsis comma q Subscript upper U Baseline EndSet comma EndLayoutQ : = {q1, · · · , qU},
where q Subscript iqi, i equals 1 comma midline horizontal ellipsis comma upper Ui = 1, · · · , U satisfying q 1 less than midline horizontal ellipsis less than q Subscript upper Uq1 < · · · < qU, are deﬁned as quantized values 
of the original variable. Note that x can be the input feature bold a Superscript nan or the weights bold w Superscript nwn. In  
this way, bold q Superscript bold w Super Superscript n Baseline element of double struck upper Q Superscript upper C Super Subscript o u t Super Superscript n Superscript times upper C Super Subscript i n Super Superscript nqwn ∈QCn
out×Cn
in and bold q Superscript bold a Super Subscript i n Super Superscript n Baseline element of double struck upper Q Superscript upper C Super Subscript i n Super Superscript nqan
in ∈QCn
in such that the ﬂoat-point convolutional 
outputs can be approximated by the efﬁcient XNOR and bit-count instructions as: 
bold a Subscript o u t Superscript n Baseline almost equals bold q Superscript bold w Super Superscript n Superscript Baseline circled dot bold q Superscript bold a Super Subscript i n Super Superscript n Superscript Baseline periodan
out ≈qwn ⊙qan
in.
(4.2) 
The key challenge in QNNs is how to deﬁne the quantization set double struck upper QQ, and the methods 
to achieve this are further described in the following sections. 
4.3 
Uniform and Nonuniform Quantization 
In quantized neural networks (QNNs), we need to deﬁne a function that can quantize 
the weights and activations of the neural network to a ﬁnite set of values. One 
popular choice for this quantization function is the uniform quantization function, 
which is deﬁned as follows: 
bold q Superscript x Baseline equals upper I upper N upper T left parenthesis StartFraction x Over upper S EndFraction right parenthesis minus upper Z commaqx = INT
x
S

−Z,
(4.3) 
where x is a real-valued input (activation or weight), S is a real-valued scaling 
factor, and Z is an integer zero point. The function upper I upper N upper TINT converts a real number 
to an integer value using a rounding technique such as rounding to the nearest 
integer or truncation. In other words, the quantization function maps real values x 
to some integer value, allowing us to represent the original continuous values with 
a ﬁnite set of discrete values. This method of quantization is also known as uniform 
quantization. 
Besides, nonuniform quantization methods produce quantized values that are not 
necessarily uniformly spaced. The formal deﬁnition of nonuniform quantization is 
shown as: 
bold q Superscript x Baseline equals StartLayout Enlarged left brace 1st Row 1st Column q 1 comma 2nd Column i f x less than or equals upper Delta 1 comma 2nd Row 1st Column ellipsis 3rd Row 1st Column q Subscript i Baseline comma 2nd Column i f upper Delta Subscript i minus 1 Baseline less than x less than or equals upper Delta Subscript i Baseline comma 4th Row 1st Column ellipsis 5th Row 1st Column q Subscript upper U Baseline comma 2nd Column i f x greater than upper Delta Subscript upper U Baseline period EndLayoutqx =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
q1, if x ≤∆1,
. . .
qi, if ∆i−1 < x ≤∆i,
. . .
qU, if x > ∆U.
(4.4)

4.4
Symmetric and Asymmetric Quantization
103
where q Subscript iqi represents the discrete quantization levels and upper Delta Subscript i∆i denotes the quantization 
steps. When the value of a real number x falls between the quantization steps upper Delta Subscript i Baseline minus 1∆i −1
and i plus 1i +1, the quantizer Q projects it to the associated quantization level q Subscript iqi. It should 
be noted that neither q Subscript iqi nor upper Delta Subscript i∆i are evenly spaced. 
Nonuniform quantization techniques offer the potential to achieve higher accu-
racy for a ﬁxed bit width compared to uniform quantization. This is because 
nonuniform quantization allows for better representation of data distributions by 
focusing on essential value regions and determining appropriate dynamic ranges. 
One common scenario where nonuniform quantization is beneﬁcial is when dealing 
with bell-shaped distributions of weights and activations, which often have long 
tails. In such cases, various nonuniform quantization methods have been developed 
to accommodate these speciﬁc distributions. One popular approach is the rule-
based nonuniform quantization using a logarithmic distribution. In this method, the 
quantization steps and levels are increased exponentially instead of linearly. 
Recent advances in quantization techniques have treated quantization as an 
optimization problem to enhance performance. The objective is to minimize the 
difference between the original tensor and its quantized version by adjusting the 
quantization steps or levels in the quantizer bold q Superscript xqx. This can be formulated as an 
optimization problem: 
min Underscript bold q Endscripts StartAbsoluteValue bold q Superscript x Baseline minus x EndAbsoluteValue Subscript 2 Superscript 2 min
q |qx −x|2
2
(4.5) 
Nonuniform quantization can also beneﬁt from learnable quantizers, where the 
quantization steps are optimized through an iterative process or gradient descent 
along with the model parameters. 
Overall, nonuniform quantization offers the advantage of better representing 
data by distributing bits and discretizing the parameter range unevenly. However, 
implementing nonuniform quantization effectively on standard computer hardware, 
such as GPUs and CPUs, can be challenging. As a result, uniform quantization 
remains the predominant method due to its straightforward implementation and 
efﬁcient mapping to hardware, making it more suitable for practical deployment 
in various computing platforms. 
4.4 
Symmetric and Asymmetric Quantization 
The choice of the scaling factor, S, in uniform quantization (Eq. 4.2) is critical as 
it determines the granularity of quantization and ultimately impacts the accuracy of 
the quantized representation. The value of S affects how the range of real values, x, 
is divided into a speciﬁed number of segments, and it directly inﬂuences the size of 
each partition. The clip range left bracket alpha comma beta right bracket[α, β] deﬁnes the range of real values that should be

104
4
Quantization of Neural Networks
quantized, and the bit width b determines the number of bits used for quantization. 
The formula for S is given by: 
upper S equals StartFraction beta minus alpha Over 2 Superscript b Baseline minus 1 EndFraction commaS = β −α
2b −1,
(4.6) 
where left bracket alpha comma beta right bracket[α, β] is the clip range and b is the bit width. Choosing an appropriate clip 
range is crucial as it directly affects the quantization precision and the overall quality 
of the quantized model. This process is known as calibration, an essential step in 
uniform quantization. 
Asymmetric quantization may use a tighter clip range compared to symmetric 
quantization. This is especially useful for signals with imbalanced values, such as 
activations after ReLU, which always have nonnegative values. 
Symmetric quantization, on the other hand, simpliﬁes the quantization function 
by centering the zero point at upper Z equals 0Z = 0, resulting in the following expression: 
bold q Superscript x Baseline equals upper I upper N upper T left parenthesis StartFraction x Over upper S EndFraction right parenthesis periodqx = INT
x
S

.
(4.7) 
In practice, using the whole-range approach often leads to greater accuracy. 
Symmetric quantization is commonly employed for quantizing weights due to 
its simplicity and reduced computational cost during inference. However, for 
quantizing activations, asymmetric quantization may be more effective as the offset 
in asymmetric activations can be absorbed into the bias or used to initialize the 
accumulator, leading to improved performance. 
4.5 
Comparison of Different Quantization Methods 
4.5.1 
LSQ: Learned Step Size Quantization 
Fixed quantization methods that rely on user-deﬁned settings do not guarantee 
optimal network performance and may still produce suboptimal results even if they 
minimize quantization error. An alternative approach is learning the quantization 
mapping by minimizing task loss, directly improving the desired metric. However, 
this method is challenging because the quantizer is discontinuous and requires 
an accurate approximation of its gradient, which existing methods [8] have done 
roughly that overlooks the effects of transitions between quantized states. 
This section introduces a new method for learning the quantization mapping for 
each layer in a deep network called learned step size quantization (LSQ) [13]. LSQ 
improves on previous methods with two key innovations. First, we offer a simple 
way to estimate the gradient of the quantizer step size, considering the impact of 
transitions between quantized states. This results in more advanced optimization 
when learning the step size as a model parameter. Second, we introduce a heuristic to

4.5
Comparison of Different Quantization Methods
105
balance the magnitude of step size updates with weight updates, leading to improved 
convergence. Our approach can be used to quantize both activations and weights and 
is compatible with existing techniques for back propagation and stochastic gradient 
descent. 
4.5.1.1 
Notations 
The goal of quantization in deep networks is to reduce the precision of the weights 
and the activations during the inference time to increase computational efﬁciency. 
Given the data to quantize v, the quantizer step size s, and the number of positive 
and negative quantization levels (upper Q Subscript upper PQP and upper Q Subscript upper NQN), a quantizer is used to compute 
ModifyingAbove v With caretˆv, a quantized representation on the whole scale of the data, and ModifyingAbove v With caretˆv, a quantized 
representation of the data at the same scale as v: 
v overbar equals left floor c l i p left parenthesis v divided by s comma minus upper Q Subscript upper N Baseline comma upper Q Subscript upper P Baseline right parenthesis right ceiling¯v = ⎿clip(v/s, −QN, QP )⏌
(4.8) 
ModifyingAbove v With caret equals v overbar times sˆv = ¯v × s
(4.9) 
This technique uses low-precision inputs, represented by w overbar¯w and x overbar¯x, in matrix  
multiplication units for convolutional or fully connected layers in deep learning 
networks. The low-precision integer matrix multiplication units can be computed 
efﬁciently, and a step size then scale the output with a relatively low-cost high-
precision scalar-tensor multiplication. This scaling step has the potential to be 
combined with other operations, such as batch normalization, through algebraic 
merging, as shown in Fig. 4.1. This approach aims to minimize the memory and 
computational costs associated with matrix multiplication. 
Fig. 4.1 Computation of a low-precision convolution or fully connected layer, as envisioned here

106
4
Quantization of Neural Networks
4.5.1.2 
Step Size Gradient 
LSQ offers a way of determining s based on the training loss through the incorpora-
tion of a gradient into the step size parameter of the quantizer as: 
∂ˆv
∂s =
⎧
⎨
⎩
−v/s + ⎿v/s⏌, if −QN < v/s < Qp,
−QN,
if v/s ≤x,
QP ,
if v/s ≥Qp.
(4.10) 
The gradient is calculated using the straight-through estimator, as proposed by 
[4], to approximate the gradient through the round function as a direct pass. The 
round function remains unchanged to differentiate downstream operations, while all 
other operations are differentiated conventionally. 
The gradient calculated by LSQ is different from other similar approximations 
(Fig. 4.2) in that it does not transform the data before quantization (Jung et al., 
2018) or estimate the gradient by algebraically canceling terms after removing 
the round operation from the forward equation, resulting in ∂ˆv/∂s = 0 when 
−QN
< v/s < QP [8]. In these previous methods, the proximity of v to 
the transition point between quantized states does not impact the gradient of the 
quantization parameters. However, it is intuitive that the closer a value of v is to a 
quantization transition point, the more likely it is to change its quantization bin ˆv
with a slight change in s, resulting in a signiﬁcant jump in ˆv. This means that ∂ˆv/∂s
should increase as the distance from v to a transition point decreases, as observed in 
the LSQ gradient. Notably, this gradient emerges naturally from the simple quantizer 
formulation and the use of the straight-through estimator for the round function. 
In LSQ, each layer of weights and each layer of activations have their unique 
step size represented as a 32-bit ﬂoating point value. These step sizes are initialized 
3 
2 
1 
0 
0 
3 
2 
1 
0 
1
2
3
4
Transition 
Point 
Transition 
Point 
Transition 
Point
0
1
2
3
4
Transition 
Point 
Transition 
Point 
Transition 
Point 
∂v 
∂s
ˆ
v
ˆ
V
V 
LSQ 
QIL 
PACT 
a
b
Fig. 4.2 Given s = 1, QN = 0, QP = 3, (a) quantizer output and (b) gradients of the quantizer 
output concerning step size, s, for LSQ, or a related parameter controlling the width of the 
quantized domain (equal to s(QP + QN)) for  QIL [26] and  PACT  [8]. The gradient employed by 
LSQ is sensitive to the distance between v and each transition point, whereas the gradient employed 
by QIL [26] is sensitive only to the distance from quantizer clip points and the gradient employed 
by PACT [8] is zero everywhere below the clip point. Here, we demonstrate that networks trained 
with the LSQ gradient reach a higher accuracy than those trained with the QIL or PACT gradients 
in prior work

4.5
Comparison of Different Quantization Methods
107
to 2 StartAbsoluteValue v EndAbsoluteValue divided by StartRoot upper Q Subscript upper P Baseline EndRoot2|v|/√QP and calculated from the initial weight values or the ﬁrst batch of 
activations, respectively. 
4.5.1.3 
Step Size Gradient Scale 
It has been demonstrated that good convergence during training can be achieved 
when the ratio of average update magnitude to average parameter magnitude is 
consistent across all weight layers in a network. Setting the learning rate correctly 
helps prevent updates from being too large and causing repeated overshooting of 
local minima or too small, leading to a slow convergence time. Based on this 
reasoning, it is reasonable to assume that each step size should also have its 
update magnitude proportional to its parameter magnitude, similarly to the weights. 
Therefore, for a network trained on a loss function L, the ratio 
upper R equals StartFraction nabla Subscript s Baseline upper L Over s EndFraction divided by StartFraction parallel to nabla Subscript w Baseline upper L parallel to Over parallel to w parallel to EndFraction commaR = ∇sL
s
/‖∇wL‖
‖w‖ ,
(4.11) 
should be close to 1, where parallel to x parallel to‖x‖ denotes the l2-norm of z. However, as precision 
increases, the step size parameter is expected to be smaller (due to ﬁner quantiza-
tion), and the step size updates are expected to be larger (due to the accumulation 
of updates from more quantized items when computing its gradient). A gradient 
scale g is multiplied by the step size loss to address this. For the weight step size, 
g is calculated as 1 divided by StartRoot upper N Subscript upper W Baseline upper Q Subscript upper P Baseline EndRoot1/√NWQP , and for the activation step size, g is calculated as 
1 divided by StartRoot upper N Subscript upper W Baseline upper Q Subscript upper P Baseline EndRoot1/√NWQP , where upper N Subscript upper WNW is the number of weights in a layer and upper N Subscript fNf is the number 
of features in a layer. 
4.5.1.4 
Training 
LSQ trains the model quantizers by making the step sizes learnable parameters, 
with the loss gradient computed using the quantizer gradient mentioned earlier. 
In contrast, other model parameters can be trained with conventional techniques. 
A common method of training quantized networks [10] is employed where full-
precision weights are stored and updated, while quantized weights and activations 
are used for forward and backward passes. The gradient through the quantizer round 
function is calculated using the straight-through estimator [4] so that: 
StartFraction partial differential ModifyingAbove v With caret Over partial differential v EndFraction equals StartLayout Enlarged left brace 1st Row 1st Column 1 comma 2nd Column i f minus upper Q Subscript upper N Baseline less than v divided by s less than upper Q Subscript p Baseline comma 2nd Row 1st Column 0 comma 2nd Column o t h e r w i s e comma EndLayout∂ˆv
∂v =
 1, if −QN < v/s < Qp,
0, otherwise,
(4.12) 
and stochastic gradient descent is used to update parameters. 
For ease of training, the input to the matrix multiplication layers is set to ModifyingAbove v With caretˆv, 
mathematically equivalent to the inference operations described above. The input 
activations and weights are set to 2, 3, 4, or 8 bits for all matrix multiplication layers

108
4
Quantization of Neural Networks
except the ﬁrst and last, which are always set to 8 bits. This standard practice in 
quantized networks has been shown to improve performance signiﬁcantly. All other 
parameters are represented using FP32. Quantized networks are initialized using 
weights from a trained full-precision model with a similar architecture before being 
ﬁne-tuned in the quantized space. 
4.5.2 
TRQ: Ternary Neural Networks with Residual 
Quantization 
4.5.2.1 
Preliminary 
The main operation in deep neural networks is expressed as 
z equals bold w Superscript down tack Baseline bold a commaz = w⏉a,
(4.13) 
where bold w element of double struck upper R Superscript nw ∈Rn indicates the weight vector and bold a element of double struck upper R Superscript na ∈Rn indicates the input activation 
vector computed by the previous network layer. 
A ternary neural network means representing the ﬂoating-point weights and/or 
activations with ternary values. Formally, the quantization can be expressed as: 
upper Q Subscript x Baseline left parenthesis bold x right parenthesis equals beta Subscript x Baseline bold upper T Subscript bold x Baseline commaQx(x) = βxTx,
(4.14) 
where bold xx indicates ﬂoating-point parameters including weights bold ww and activations bold aa
and bold upper T Subscript bold xTx denotes ternary values after the quantization on bold xx. beta Subscript xβx is a scalar used to 
scale the ternary values, which can be computed from the ﬂoating-point parameters 
or learned via back propagation. bold upper T Subscript bold xTx is usually obtained by thresholding function: 
bold upper T Subscript bold x Baseline equals StartLayout Enlarged left brace 1st Row plus 1 i f bold x greater than upper Delta 2nd Row 0 i f StartAbsoluteValue bold x EndAbsoluteValue less than or slanted equals upper Delta comma 3rd Row minus 1 i f bold x less than negative upper Delta EndLayoutTx =
⎧
⎪⎪⎨
⎪⎪⎩
+1 if x > ∆
0
if |x| ⩽∆,
−1 if x < −∆
(4.15) 
where upper Delta∆denotes a ﬁxed threshold used for quantization. With the ternary weights 
and activations, the vector multiplications in the forward propagation can be 
reformulated as: 
z equals upper Q Subscript w Baseline left parenthesis bold w right parenthesis Superscript down tack Baseline upper Q Subscript a Baseline left parenthesis bold a right parenthesis equals beta Subscript w Baseline beta Subscript a Baseline left parenthesis bold upper T Subscript bold w Baseline circled dot bold upper T Subscript bold a Baseline right parenthesis commaz = Qw(w)⏉Qa(a) = βwβa(Tw ⊙Ta),
(4.16) 
where circled dot⊙represents the inner product for vectors with bitwise operations. 
In general, the derivative of quantization function upper Q Subscript x Baseline left parenthesis bold x right parenthesisQx(x) is non-differentiable and 
thus unpractical to directly apply the back propagation to perform the training phase.

4.5
Comparison of Different Quantization Methods
109
For this issue, we follow the now widely adopted “straight-through estimator (STE)” 
[23] to approximate the partial gradient calculation, which is formally expressed as: 
StartFraction partial differential upper Q Subscript x Baseline left parenthesis bold x right parenthesis Over partial differential bold x EndFraction almost equals beta bold 1 Subscript StartAbsoluteValue bold x EndAbsoluteValue less than or slanted equals 1 Baseline period∂Qx (x)
∂x
≈β1|x|⩽1.
(4.17) 
TNNs with Residual Quantization (TRQ) 
Existing TNNs are based on directly thresholding method for ternary implemen-
tation, inevitably causing performance degradation due to an inaccurate mapping 
of full-precision values to ternary counterparts. To deal with the issue, residual 
quantization (TRQ) [29] is introduced to learn TNNs. TRQ can extract binarized 
stem and residual, respectively, by performing recursive quantization on full-
precision weights, which are combined to generate reﬁned ternary representation, 
leading to the stem-residual framework for TNNs. 
In our stem-residual ternarization framework, the stem is ﬁrst extracted as a 
coarse ﬁtting for full-precision weight bold ww, which is calculated by performing s i g n left parenthesis dot right parenthesissign(·)
on bold ww as: 
bold upper S Subscript bold w Baseline equals alpha s i g n left parenthesis bold w right parenthesis commaSw = αsign(w),
(4.18) 
where alphaα is a learnable coefﬁcient, which avoids a very careful tuning to seek the 
optimal quantization scale compared with the previous methods. Then, we further 
calculate the quantization error as: 
bold upper R equals bold w minus bold upper S Subscript bold wR = w −Sw
(4.19) 
Furthermore, we calculate the residual bold upper R Subscript bold wRw from bold upper RR by performing s i g n left parenthesis dot right parenthesissign(·) on the 
quantization error bold upper RR: 
bold upper R Subscript bold w Baseline equals alpha s i g n left parenthesis bold upper R right parenthesis periodRw = αsign(R).
(4.20) 
Based on Eqs. 4.18 and 4.20, we ﬁnally obtain our ternary weight designed for 
more accurate approximation as: 
bold upper T Subscript bold w Baseline equals bold upper S Subscript bold w Baseline plus bold upper R Subscript bold w Baseline periodTw = Sw + Rw.
(4.21) 
Up to now, we achieve the ternary quantization in a stem framework, with the full-
precision weights quantized to ternary values, i.e., StartSet minus 2 alpha comma 0 comma 2 alpha EndSet{−2α, 0, 2α}. Obviously, seeking 
a better coefﬁcient alphaα is signiﬁcantly important for the effectiveness of quantizer, 
which would be elaborated in the following section. 
Backward Propagation of TRQ 
In the backward propagation, what need to be learned and updated are the full-
precision weight bold ww and the learnable coefﬁcient alphaα. For the stem-residual framework,

110
4
Quantization of Neural Networks
the two kinds of parameters are jointly learned. And in each layer, TRQ updates the 
bold ww ﬁrst and then the alphaα. 
Update bold ww: For bold ww updating, the gradient through the quantizer to weights are 
estimated by a STE that pass the gradient whose weight value is in the range of 
(-2alphaα, 2alphaα): 
StartLayout 1st Row StartLayout 1st Row 1st Column StartFraction partial differential bold upper T Subscript bold w Baseline Over partial differential bold w EndFraction 2nd Column equals bold 1 Subscript StartAbsoluteValue bold x EndAbsoluteValue less than or slanted equals 2 alpha Baseline period EndLayout EndLayout ∂Tw
∂w = 1|x|⩽2α.
(4.22) 
Then, we can obtain the updating process of bold ww: 
StartLayout 1st Row 1st Column delta Subscript bold w 2nd Column equals StartFraction partial differential upper L Over partial differential bold upper T Subscript bold w Baseline EndFraction StartFraction partial differential bold upper T Subscript bold w Baseline Over partial differential bold w EndFraction comma EndLayout δw = ∂L
∂Tw
∂Tw
∂w ,
(4.23) 
StartLayout 1st Row 1st Column bold w 2nd Column left arrow bold w minus eta delta Subscript bold w Baseline comma EndLayout w ←w −ηδw,
(4.24) 
where L is the loss function and etaη is the learning rate. 
Update alphaα: The coefﬁcient alphaα determines the scale of binarized stem and residual, 
which is directly related to the quality of the ternary weights. Moreover, we also 
empirically ﬁnd the recognition performance is quite sensitive to the alphaα. Thus, rather 
than a coarse gradient acquired like bold ww, we disassemble the quantizer to calculate a 
ﬁner gradient of alphaα: 
StartFraction partial differential bold upper T Subscript bold w Baseline Over partial differential alpha EndFraction equals s i g n left parenthesis bold w right parenthesis plus s i g n left parenthesis bold upper R right parenthesis plus alpha StartFraction partial differential s i g n left parenthesis bold upper R right parenthesis Over partial differential alpha EndFraction∂Tw
∂α = sign(w) + sign(R) + α ∂sign(R)
∂α
(4.25) 
where 
StartLayout 1st Row 1st Column StartFraction partial differential s i g n left parenthesis bold upper R right parenthesis Over partial differential alpha EndFraction 2nd Column equals StartFraction partial differential s i g n left parenthesis bold upper R right parenthesis Over partial differential bold upper R EndFraction StartFraction partial differential bold upper R Over partial differential alpha EndFraction 2nd Row 1st Column Blank 2nd Column equals bold 1 Subscript StartAbsoluteValue bold upper R EndAbsoluteValue less than or slanted equals 1 Baseline left parenthesis minus s i g n left parenthesis bold w right parenthesis right parenthesis period EndLayout
∂sign(R)
∂α
= ∂sign(R)
∂R
∂R
∂α
= 1|R|⩽1 (−sign(w)) .
(4.26) 
Then, we can obtain the updating process of alphaα: 
StartLayout 1st Row 1st Column delta Subscript alpha 2nd Column equals sigma summation StartFraction partial differential upper L Over partial differential bold upper T Subscript bold w Baseline EndFraction StartFraction partial differential bold upper T Subscript bold w Baseline Over partial differential alpha EndFraction comma EndLayout δα =
	 ∂L
∂Tw
∂Tw
∂α ,
(4.27) 
StartLayout 1st Row 1st Column alpha 2nd Column left arrow alpha minus eta delta Subscript alpha Baseline period EndLayout α ←α −ηδα.
(4.28) 
4.5.2.2 
Generalization to n-Bit Quantization 
We focus on ternary quantization in this paper, while it does not mean that TRQ is 
limited to ternary applications. Actually, TRQ could also be generalized to multiple

4.5
Comparison of Different Quantization Methods
111
bits by recursively encoding residual. In this section, we propose a feasible scheme 
for TRQ expansion, which is not the only way and could be further explored in the 
future work. 
We obtain the subtly quantized weights by recursively performing quantization 
on full-precision weights. In this process, residual at different quantization levels 
is generated for reﬁning the quantized weights. Here for n-bit (n equals 2 comma 3 comma 4 comma ellipsisn = 2, 3, 4, . . .) 
quantization, we deﬁne the residual at level i left parenthesis i equals 1 comma 2 comma ellipsis comma 2 Superscript n Baseline minus 3 right parenthesisi (i = 1, 2, . . . , 2n −3) as bold upper R Subscript bold w Superscript iRi
w, which 
could be computed as: 
bold upper R Subscript bold w Superscript i Baseline equals alpha s i g n left parenthesis bold w minus bold upper T Subscript bold w Superscript i minus 1 Baseline right parenthesis commaRi
w = αsign(w −Ti−1
w ),
(4.29) 
where bold upper T Subscript bold w Superscript i minus 1Ti−1
w
denotes the quantized weights at (i minus 1i −1)th level, and we recursively 
acquire the quantized weights at level i as: 
bold upper T Subscript bold w Superscript i Baseline equals bold upper T Subscript bold w Superscript i minus 1 Baseline plus bold upper R Subscript bold w Superscript i Baseline periodTi
w = Ti−1
w
+ Ri
w.
(4.30) 
Here we regard the ternary quantization as the initial state for recursive quantization 
as: 
bold upper T Subscript bold w Superscript 0 Baseline equals bold upper S Subscript bold w Baseline plus alpha s i g n left parenthesis bold w minus bold upper S Subscript bold w Baseline right parenthesis periodT0
w = Sw + αsign(w −Sw).
(4.31) 
Based on such recursive quantization, we could easily obtain the residual at different 
levels, thus reﬁning the residual and reducing the approximation error with the full-
precision counterparts. 
For the updating of alphaα in backward propagation, due to the complexity of recursive 
process, we just roughly estimate the gradient alphaα by regarding it as the coefﬁcient of 
bold upper S Subscript bold wSw and bold upper R Subscript bold w Superscript iRi
w: 
StartFraction partial differential bold upper T Subscript bold w Baseline Over partial differential alpha EndFraction equals bold upper S Subscript bold w Baseline plus sigma summation Underscript i equals 0 Overscript 2 Superscript n Baseline minus 3 Endscripts bold upper R Subscript bold w Superscript i Baseline period∂Tw
∂α = Sw +
2n−3
	
i=0
Ri
w.
(4.32) 
4.5.2.3 
Complexity Analysis 
A comprehensive comparison on computational complexity is shown in Table 4.1. 
We assume that the input number of the neuron is N, i.e., N inputs and one neuron 
output. For computational complexity of TNNs, we follow the setting of GXNOR-
Net [11] for a comparison. As described in GXNOR-Net, with the event-driven 
paradigm, the resting computation would occur when the weight or activation of 
TNNs is zero, and the exception cases are achieved by XNOR operations. As 
a result, the computational complexity of TNNs is similar to BNNs, half of the 
network with 1-bit weights and 2-bit activations. Noted that for TRQ, the stem-
residual framework is only employed on weights; thus, it also enjoys the low 
complexity upper O left parenthesis upper N right parenthesisO(N) as normal TNNs.

112
4
Quantization of Neural Networks
Table 4.1 Operation overhead comparisons with different computing bit width 
Operations 
Bit width(A/W)
Multiplication
Accumulation
XNOR
BitCount
Complexity 
32/32
N
N
0
0
– 
1/1
0
0
N
1
O(N) 
2/1
0
0
2N
1
O(2N) 
ter/ter
0
0
0tilde∼N
0/1
O(N) 
4.5.2.4 
Differences of TRQ from Existing Residual Quantization Methods 
Residual quantization has been ﬁrst proposed in high-order residual quantization 
(HORQ) [32] to enhance the performance of BNNs, further being explored by 
[16, 18] to be encoded into low-bit width CNNs. All above works compute residual 
error and recursively approximate it by a series of binary maps. However, limited 
by the residual scales, they can be just applied to n-bit quantization, with no 
generalization ability to arbitrary value quantization even parameter changes, such 
as the TNNs emphasized in this paper. Instead, our TRQ enjoys the ﬂexibility 
by the skillful combination of the binarized stem and residual, thus enabling 
ternary quantization and even arbitrary value quantization by recursively reﬁning 
the residual. Moreover, a key feature of the prior residual schemes is the use of 
analytically calculated scaling coefﬁcients, which can be suboptimal. In contrast, 
our TRQ employs learnable coefﬁcient alphaα to minimize the training loss, thus 
fully utilizing the strength of back propagation algorithm to seek for the suitable 
quantization scale automatically. 
4.5.2.5 
Implementation Details 
Data Preprocessing 
For CIFAR-10/100, all the images are padded with 4 pixels on each side, and then 
a random 32 × 32 crop is applied, followed by a random horizontal ﬂip. During 
inference, the scaled images are used without any augmentation. For ImageNet, 
training images are randomly cropped into the resolution of 224 × 224. After that, 
the images are normalized using the mean and standard deviation. No additional 
augmentations are performed except the random horizontal ﬂip. However, for 
validation images, we use center crop instead of random crop and no ﬂip is applied 
(Fig. 4.3). 
Training Procedure 
We conduct experiments mainly on ResNet [21] backbones, including ResNet-18 
and ResNet-34. VGG-Small [38] is also leveraged for the CIFAR-10 and CIFAR-
100 experiments. Similar with previous works [17, 28, 34], we do not quantize 
the ﬁrst and last layers. For experiments on CIFAR-10/100, we run the training 
algorithm for 200 epochs with a batch size of 256. Besides, a linear learning rate

4.5
Comparison of Different Quantization Methods
113
Fig. 4.3 The Top-1 accuracy (%) on CIFAR-10 and CIFAR-100 with different initial α
decay scheduler is used, and the initial learning rate is set to 0.01. For experiments 
on ImageNet, we train the models for up to 100 epochs with a batch size of 256. 
The learning rate starts from 0.001 and is decayed twice by multiplying 0.1 at 75th 
and 95th epoch. For all settings, Adam with momentum of 0.9 is adopted as the 
optimizer. 
4.5.2.6 
Ablation Study on CIFAR 
In this section, we ﬁrst perform hyperparameter sweeps to determine the value of 
initial α to use. Following this we analyze the necessity of α, then show TRQ’s 
generalization to multiple bits, and ﬁnally evaluate the effectiveness of TRQ on 
CIFAR datasets. 
Initial Value of α
The initiation of parameters is always important for network training. Thus, we 
set different initial values 0.10, 0.3, 0.5, 0.8, 1,  1.5, and 2 to α, to explore their 
inﬂuence on classiﬁcation. The experiments are performed on the CIFAR-10/100 
with ResNet-18 backbone. From the results on CIFAR-10 in Fig. 4.4, we can 
observe that the performance is similar when the initial values of α are set between 
0.8 and 1.5, and the best performance can be obtained at 1.5. Meanwhile, from the 
results on CIFAR-100, the good performance plateau appears when initial α is at the

114
4
Quantization of Neural Networks
Fig. 4.4 Evolution of α values in different layers during training with ResNet-18 backbone on 
CIFAR-100 
Table 4.2 The accuracy (%) 
of TRQ with and without α
(TRQ and TRQ-wo) and with 
ﬁxed α = 0.6 (TRQ-0.6) on  
CIFAR-100 
Width
TRQ-wo 
TRQ 
TRQ-0.6
ResNet-18 
16-16-32-64 
52.1
54.9 
53.9 
ResNet-18 
32-32-64-128 
60.5
62.7 
61.3 
VGG-Small 
–
62.6
65.4 
60.5 
range between 0.8 and 1, and the performance of initial value 1 performs slightly 
better than that of 0.8. For both CIFAR-10 and CIFAR-100, the performance of 
initial values outside the 0.5 to 1.5 is fairly worse, which shows the importance 
of setting the initial value of α carefully. Based on the above discoveries, we set 
the initial value of α as 1 in the following experiments, which shows a stably high 
classiﬁcation performance on both two datasets. 
Analysis of α
α is introduced in stem-residual framework to automatically seek for a reasonable 
quantization scale. To valid the necessity of α, we provide the experiments with and 
without α on CIFAR-100 with the backbone ResNet-18. As shown in Table 4.2, 
compared with the TRQ without α (TRQ-wo), TRQ achieves better performance by 
a large margin (more than 2%), thus indicating that α is quite important for training 
TRQ.

4.5
Comparison of Different Quantization Methods
115
Simultaneously, as illustrated in Fig. 4.4, we explore how the value of alphaα changes 
during training. It can be observed that alphaα converges to around 0.60.6 with training. 
However, this doesn’t mean that alphaα should be ﬁxed and not optimized. As shown in 
Table 4.2, we compare the results in two cases, i.e., alphaα is ﬁxed to 0.60.6 (TRQ-0.6) and 
alphaα is optimized by back propagation. As we can see, when ﬁxed alphaα as 0.60.6, a greater 
performance decrease happens. When employed with VGG-Small backbone, the 
accuracy even drops nearly 5 percent sign5% compared with the learnable alphaα, thus validating the 
superiority of the learnable alphaα. We conjecture that is because with the learnable alphaα
in stem-residual framework, the quantizer could be automatically ﬁne-tuned to ﬁnd 
the best quantization mapping for each layer, thus yielding better performance than 
the ﬁxed case. 
Quantization Error 
In order to better understand our TRQ, which achieves more accurate mapping 
between ternary weights and their full-precision counterparts, we adopt mean square 
error (MSE) [14] to calculate the quantization error between bold ww and bold upper T Subscript bold wTw: 
bold upper E equals StartFraction 1 Over upper M EndFraction sigma summation left parenthesis StartFraction bold w minus bold upper T Subscript bold w Baseline Over bold w EndFraction right parenthesis squared commaE = 1
M
	 
w −Tw
w
2
,
(4.33) 
where M denotes the total number of weights in each layer. In Fig. 4.5, we plot  
the quantization error for the 2th–17th layer of ResNet-18. The results show our 
methods (the red histogram) have lower quantization error compared with baseline 
(the gray histogram) which achieved with the method in Section 3.1 in most layers. 
In particular, the quantization error can be reduced by more than 25 percent sign25% (0.80.8 vs 0.60.6) 
in the 9th layer. 
Fig. 4.5 Quantization error of TRQ and baseline based on ResNet-18 backbone

116
4
Quantization of Neural Networks
Fig. 4.6 The results of TRQ with multi-bits expansion on CIFAR-100 
Generalization to n-Bit Quantization 
We illustrate that our TRQ can not only improve the performance on ternary 
quantization but also could be generalized to multiple bits. Here we adopt the 
expansion method described in Sect. 3.4 and perform the experiments on CIFAR-
100 with the backbone of ResNet-18. The baseline model is implemented in a 
similar way as DoReFa-Net [45]. As shown in Fig. 4.6, we can see that the accuracy 
of TRQ increases (56.2 percent sign right arrow 58.3 percent sign right arrow 58.5 percent sign56.2% →58.3% →58.5%) as the bit width increases from 
2bit to 4bit, indicating that the compound residual at multi-levels could reﬁne the 
quantized weights, thus improving the recognition accuracy. Moreover, our TRQ 
consistently surpasses the baseline on each bit width (0.4 percent sign comma 1.1 percent sign comma 1.0 percent sign0.4%, 1.1%, 1.0% on 2bit, 
3bit, and 4bit, respectively), which demonstrates the superiority and potential of the 
residual quantization on multiple bits. 
Evaluation on CIFAR 
To validate the effectiveness of TRQ, here we perform ablation evaluation on 
CIFAR datasets. Three backbones are used in this experiment, including VGG-
Small, ResNet-18 with the width of 16-16-32-64 and 32-32-64-128. We report the 
performance of baseline and TRQ on both CIFAR-10 and CIFAR-100 in Table 4.3. 
As shown in Table 4.3, for ResNet-18, TRQ achieves stable improvement on both 
CIFAR-10 and CIFAR-100 datasets compared with the corresponding baseline. 
Moreover, TRQ with the backbone ResNet-18 whose width is 32-32-64-128 even

4.5
Comparison of Different Quantization Methods
117
Table 4.3 The experimental comparison of baseline and TRQ on CIFAR datasets 
CIFAR-10/%
CIFAR-100/% 
ResNet-18
Full-precision
87.7
58.2 
16-16-32-64
Baseline
85.2
54.8 
TRQ
85.5
54.9 
ResNet-18
Full-precision
90.9
63.0 
32-32-64-128
Baseline
87.5
60.6 
TRQ
89.3
62.7 
VGG-Small
Full-precision
92.6
66.8 
Baseline
89.1
61.8 
TRQ
91.2
65.4 
realizes nearly lossless ternarization on CIFAR-100 (only with a 0.3 percent sign0.3% performance 
drop). All these demonstrate the effectiveness of TRQ on ResNet. For VGG-
Small, our TRQ consistently surpasses the baseline by a margin of 2.1 percent sign2.1% and 
3.5 percent sign3.5% on CIFAR-10 and CIFAR-100, respectively, which further shows the general 
improvement brought by TRQ. 
Comparison on ImageNet 
We further analyze the effectiveness of TRQ on the large-scale dataset ImageNet. 
Since the dataset is challenged for network optimization, we use multi-batch 
normalization (multi-bn) strategy on ResNet architecture to alleviate optimization 
problems, which is termed as TRQ-bn in the experiment. For a basic block in 
TRQ-bn, three batch normalization layers are employed: the ﬁrst is a pre-bn [43] 
before quantization, the second is a normal bn following the ternary convolutional 
layer, and the last is an additional bn following the shortcut. Such multi-bn can 
signiﬁcantly improve the network performance by improving the distribution of 
feature maps with only small additional memory and computation. 
We illustrate the training and validation accuracy curves of baseline, TRQ, and 
TRQ-bn in Fig. 4.7, which are based on a ResNet-18 backbone. From Fig. 4.7, 
we can observe that TRQ greatly improves the convergence speed of TNNs. 
Simultaneously, from the results in Table 4.4, TRQ improves baseline by 1.0 percent sign1.0% on 
both ResNet-18 and ResNet-34 Top-1 accuracy, which validates the effectiveness 
of our TRQ on large-scale dataset. Moreover, TRQ-bn could further obtain an 
improvement of about 2 percent sign2% on both the two networks, which ﬁnally achieves 
approximately 93 percent sign93% of the accuracy of their full-precision counterparts. 
To evaluate the overall performance of TRQ, we further compare TRQ with 
four state-of-the-art quantization on ImageNet, i.e., XNOR-Net [36], BiReal-Net 
[34], LQ-Net [43], HWGQ [6], and RTN [30]. To perform fair comparison with 
RTN whose quantization procedure of weight and activation are both improved, we 
apply residual quantization to activation as well, leading to TRQ-a. The results are 
reported in Table 4.4. From Table 4.4, by comparing with the state-of-the-art BNNs 
including XNOR-Net and BiReal-Net, we can signiﬁcantly boost the performance.

118
4
Quantization of Neural Networks
Fig. 4.7 Accuracy curves of baseline, TRQ, and TRQ-bn with ResNet-18 backbone on ImageNet. 
(a) Training accuracy curves on ImageNet. (b) Validation accuracy curves on ImageNet 
For example, TRQ outperforms XNOR-Net and BiReal-Net by 11 percent sign11% and 6 percent sign6% on 
ResNet-18, respectively. It is because that ternary values StartSet negative 1 comma 0 comma 1 EndSet{−1, 0, 1} have stronger 
representational capability than binary values StartSet negative 1 comma 1 EndSet{−1, 1}, while the complexity of the 
two methods is the same because of the event-driven paradigm in TNNs. Moreover, 
our TRQ can even achieve better performance than the methods with upper O left parenthesis 2 upper N right parenthesisO(2N)
complexity, including the “A/W = 2/1” cases in LQ-Net and HWGQ. Besides,

4.5
Comparison of Different Quantization Methods
119
Table 4.4 Comparison of Top-1 and Top-5 accuracy on ImageNet 
Network
Method
A/W
Top-1/%
Top-5/%
Complexity 
ResNet-18
Full-precision
32/32
69.3
89.2
– 
Baseline
ter/ter
61.6
82.7
O(N) 
TRQ(ours)
ter/ter
62.6
83.7
O(N) 
TRQ-bn(ours)
ter/ter
64.4
85.1
O(N) 
TRQ-a(ours)
ter/ter
65.7
85.9
O(N) 
RTN
ter/ter
64.5
–
O(N) 
XNOR-Net
1/1
51.2
73.2
O(N) 
BiReal-Net
1/1
56.4
79.5
O(N) 
LQ-Net
2/1
62.6
84.3
O(2N) 
ResNet-34
Full-precision
32/32
73.3
91.3
– 
Baseline
ter/ter
65.2
85.7
O(N) 
TRQ(ours)
ter/ter
66.2
86.3
O(N) 
TRQ-bn(ours)
ter/ter
68.2
87.7
O(N) 
BiReal-Net
1/1
62.2
83.9
O(N) 
LQ-Net
2/1
66.6
86.9
O(2N) 
HWGQ
2/1
64.3
85.7
O(2N) 
our TRQ-a surpasses RTN 1.2% in accuracy, demonstrating the advantage of the 
effective residual quantization scheme. 
4.5.3 
OMPQ: Orthogonal Mixed Precision Quantization 
Recently, we have seen a noticeable trend in deep learning, that models have 
a rapidly increasing complexity [21, 22, 37–39, 44]. Due to practical limitations 
such as latency, battery, and temperature, the host hardware where the models 
are deployed cannot keep up with this trend. It results in a large, ever-increasing 
gap between computational demands and resources. To address this issue, network 
quantization [3, 23, 27, 33, 36], which maps single-precision ﬂoating-point weights 
or activations to lower bit integers for compression and acceleration, has attracted 
considerable research attention. Network quantization can be naturally formulated 
as an optimization problem, and a straightforward approach is to relax the con-
straints to make it a tractable optimization problem at the cost of an approximated 
solution. e.g. straight-through estimation (STE) [4]. 
With the recent development of inference hardware, arithmetic operations with 
variable bit width have become possible, bringing further ﬂexibility to network 
quantization. To take full advantage of hardware capabilities, mixed precision 
quantization [12, 31, 41, 42] aims to quantize different network layers to different 
bit widths to achieve a better trade-off between compression ratio and accuracy. 
While beneﬁting from the extra ﬂexibility, mixed precision quantization also needs

120
4
Quantization of Neural Networks
a more complicated and challenging optimization problem with a non-differentiable 
and extremely nonconvex objective function. Therefore, existing approaches [12, 
31, 41, 42] often require numerous data and computing resources to search for the 
optimal bit conﬁguration. 
For example, FracBits [42] approximates bit width by performing a ﬁrst-order 
Taylor expansion on the adjacent integer, making the bit variable differentiable. 
This allows it to integrate the search process into training to obtain the optimal 
bit conﬁguration. However, the search and training process still requires many 
computation resources to derive a decent solution. To resolve the signiﬁcant demand 
for training data, Dong et al. [12] use the average eigenvalue of the Hessian matrix 
of each layer as the metric for bit allocation. However, the matrix-free Hutchinson 
algorithm to implicitly calculate the average of the eigenvalues of the Hessian matrix 
still needs 50 iterations for each network layer. Another direction is black-box 
optimization. For example, Wang et al. [41] use reinforcement learning to allocate 
the bits of each layer. Li et al. [31] use an evolutionary search algorithm [19] to  
derive the optimal bit conﬁguration, together with a block reconstruction strategy 
to optimize the quantized model efﬁciently. But the population evolution process 
requires input data 1 comma 0241, 024 and iterations 100, which are time-consuming. 
Different from the existing approaches of black-box optimization or constraint 
relaxation, we propose constructing a proxy metric, which could have a substantially 
different form, but be highly correlated with the objective function of the original 
linear programming. In general, we propose to obtain the optimal bit conﬁguration 
by using the orthogonality of the neural network. Speciﬁcally, we deconstruct the 
neural network model into a set of functions and deﬁne the orthogonality of the 
model by extending its deﬁnition from a function f colon double struck upper R right arrow double struck upper Rf
: R →R to the entire 
network f colon double struck upper R Superscript m Baseline right arrow double struck upper R Superscript nf : Rm →Rn. Orthogonality measurement could be performed efﬁciently 
with Monte Carlo sampling and the Cauchy-Schwarz inequality, based on which 
we propose an efﬁcient metric named ORthogonality Metric (ORM) as the proxy 
metric. As illustrated in Fig. 4.8, we only need a single-pass search process on a 
small amount of data with ORM. In addition, we derive an equivalent form of ORM 
to accelerate the computation. 
On the other hand, model orthogonality and quantization accuracy are positively 
correlated on different networks. Therefore, maximizing model orthogonality is 
taken as our objective function. Meanwhile, our experiments show that layer 
orthogonality and bit width are positively correlated. We assign a more signiﬁcant 
bit width to the layer with larger orthogonality while combining speciﬁc constraints 
to construct a linear programming problem. The optimal bit conﬁguration can be 
obtained simply by solving the linear programming problem (Fig. 4.9). 
4.5.3.1 
Network Orthogonality 
A neural network can be naturally decomposed into a set of layers or functions. 
Formally, for the given input x element of double struck upper R Superscript 1 times left parenthesis upper C times upper H times upper W right parenthesisx ∈R1×(C×H×W), we decompose a neural network 
into script upper F equals StartSet f 1 comma f 2 comma midline horizontal ellipsis comma f Subscript upper L Baseline EndSetF = {f1, f2, · · · , fL}, where f Subscript ifi represents the transformation from the input x

4.5
Comparison of Different Quantization Methods
121
Searching Cost (iterations) 
FracBits 
HAWQ 
BRECQ 
Ours 
1050 
120 
0 
100 
40
0
80
120
~ 1040
1080
160 
Searching Data 
300
0
600
900
~ 1.1M
1.2M
1200 
FracBits 
HAWQ 
BRECQ 
Ours 
1.2M 
256 
64 
1024 
Fig. 4.8 Comparison of the resources used to obtain the optimal bit conﬁguration between our 
algorithm and other mixed precision algorithms (FracBits [42], HAWQ [12], BRECQ [31]) on 
ResNet-18. “Search Data” is the number of input images 
32 bit 
32 bit 
32 bit 
32 bit 
( ) 
( ) 
( ) 
(
,
) 
ORM Matrix 
3 bit 
8 bit 
7 bit 
4 bit 
Mini-Batch Data 
( ) 
(a) Deconstruct the Network
(b) Calculate ORM Matrix
(c) Construct Linear Programming Problem 
Feasible 
Region 
Objective Function Gradient 
Interior Point 
Fig. 4.9 Overview. Left: Deconstruct the model into a set of functions F. Middle: ORM symmetric 
matrix calculated from F. Right: Linear programming problem constructed by the importance 
factor θ to derive optimal bit conﬁguration 
to the result of the i-th layer. In other words, if gi represents the function of of the 
i-th layer, then fi(x) = gi

fi−1(x)

= gi

gi−1

· · · g1(x)

. Here, we introduce 
the inner product [1] between the functions fi and fj, which is formally deﬁned as: 

fi, fj

P(x) =

D
fi(x)P(x)fj(x)T dx,
(4.34)

122
4
Quantization of Neural Networks
where f Subscript i Baseline left parenthesis x right parenthesis element of double struck upper R Superscript 1 times left parenthesis upper C Super Subscript i Superscript times upper H Super Subscript i Superscript times upper W Super Subscript i Superscript right parenthesisfi(x) ∈R1×(Ci×Hi×Wi), f Subscript j Baseline left parenthesis x right parenthesis element of double struck upper R Superscript 1 times left parenthesis upper C Super Subscript j Superscript times upper H Super Subscript j Superscript times upper W Super Subscript j Superscript right parenthesisfj(x) ∈R1×(Cj ×Hj ×Wj ) are known functions 
when the model is given and script upper DD is the domain of x. If we set  f Subscript i Superscript left parenthesis m right parenthesis Baseline left parenthesis x right parenthesisf (m)
i
(x) to be the 
m-th element of f Subscript i Baseline left parenthesis x right parenthesisfi(x), then upper P left parenthesis x right parenthesis element of double struck upper R Superscript left parenthesis upper C Super Subscript i Superscript times upper H Super Subscript i Superscript times upper W Super Subscript i Superscript right parenthesis times left parenthesis upper C Super Subscript j Superscript times upper H Super Subscript j Superscript times upper W Super Subscript j Superscript right parenthesisP(x) ∈R(Ci×Hi×Wi)×(Cj ×Hj ×Wj ) is the probability 
density matrix between f Subscript i Baseline left parenthesis x right parenthesisfi(x) and f Subscript j Baseline left parenthesis x right parenthesisfj(x), where upper P Subscript m comma n Baseline left parenthesis x right parenthesisPm,n(x) is the probability density 
function of the random variable f Subscript i Superscript left parenthesis m right parenthesis Baseline left parenthesis x right parenthesis dot f Subscript j Superscript left parenthesis n right parenthesis Baseline left parenthesis x right parenthesisf (m)
i
(x) · f (n)
j
(x). According to the deﬁnition in 
[1], left angle bracket f Subscript i Baseline comma f Subscript j Baseline right angle bracket Subscript upper P left parenthesis x right parenthesis Baseline equals 0

fi, fj

P(x) = 0 means that f Subscript ifi and f Subscript jfj are weighted orthogonal. In other 
words, left angle bracket f Subscript i Baseline comma f Subscript j Baseline right angle bracket Subscript upper P left parenthesis x right parenthesis

fi, fj

P(x) is negatively correlated with the orthogonality between f Subscript ifi and 
f Subscript jfj. When we have a known set of functions to quantify script upper F equals left brace f Subscript i Baseline right brace Subscript i equals 1 Superscript upper LF = {fi}L
i=1, to approximate 
an arbitrary function h Superscript asteriskh∗, the quantization error can be expressed as the mean 
square error: xi integral Underscript script upper D Endscripts StartAbsoluteValue h Superscript asterisk Baseline left parenthesis x right parenthesis minus sigma summation Underscript i Endscripts psi Subscript i Baseline f Subscript i Baseline left parenthesis x right parenthesis EndAbsoluteValue squared d xξ

D |h∗(x) −
i ψifi(x)|2dx, where xiξ and psi Subscript iψi are the combination 
coefﬁcient. According to the Parseval equality [40], if script upper FF is an orthogonal basis 
function set, the mean square error could reach 0. Furthermore, the orthogonality 
between the basis functions is more substantial; the mean square error is smaller, 
i.e., and the model corresponding to the linear combination of basis functions has 
a more robust representation capability. Here, we further introduce this insight to 
network quantization. The larger the bit, the greater the representational capability 
of the corresponding model [34]. Speciﬁcally, we propose to assign a larger bit 
width to the layer with stronger orthogonality against all other layers to maximize 
the representation capability of the model. However, Eq. 4.34 has the integral of a 
continuous function, which is untractable in practice. Therefore, we derive a novel 
metric to efﬁciently approximate the orthogonality of each layer in Sect. 4.5.3.2. 
4.5.3.2 
Efﬁcient Orthogonality Metric 
To avoid the intractable integral, we propose using Monte Carlo sampling to 
approximate the orthogonality of the layers. Speciﬁcally, from the Monte Carlo 
integration perspective in [5], Eq. 4.34 can be rewritten as: 
StartLayout 1st Row 1st Column left angle bracket f Subscript i Baseline comma f Subscript j Baseline right angle bracket Subscript upper P left parenthesis x right parenthesis 2nd Column equals integral Underscript script upper D Endscripts f Subscript i Baseline left parenthesis x right parenthesis upper P left parenthesis x right parenthesis f Subscript j Baseline left parenthesis x right parenthesis Superscript upper T Baseline d x 2nd Row 1st Column Blank 2nd Column equals double vertical bar upper E Subscript upper P left parenthesis x right parenthesis Baseline left bracket f Subscript j Baseline left parenthesis x right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis x right parenthesis right bracket double vertical bar Subscript upper F Baseline period EndLayout

fi, fj

P(x) =

D
fi(x)P(x)fj(x)T dx
=
EP(x)[fj(x)T fi(x)]

F .
(4.35) 
We randomly obtain N samples x 1 comma x 2 comma ellipsis comma x Subscript upper N Baselinex1, x2, . . . , xN from a training dataset with the 
probability density matrix upper P left parenthesis x right parenthesisP(x), which allows the expectation upper E Subscript upper P left parenthesis x right parenthesis Baseline left bracket f Subscript j Baseline left parenthesis x right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis x right parenthesis right bracketEP(x)[fj(x)T fi(x)]
to be further approximated as: 
StartLayout 1st Row 1st Column double vertical bar upper E Subscript upper P left parenthesis x right parenthesis Baseline left bracket f Subscript j Baseline left parenthesis x right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis x right parenthesis right bracket double vertical bar Subscript upper F 2nd Column almost equals StartFraction 1 Over upper N EndFraction double vertical bar sigma summation Underscript n equals 1 Overscript upper N Endscripts f Subscript j Baseline left parenthesis x Subscript n Baseline right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis x Subscript n Baseline right parenthesis double vertical bar Subscript upper F Baseline 2nd Row 1st Column Blank 2nd Column equals StartFraction 1 Over upper N EndFraction double vertical bar f Subscript j Baseline left parenthesis upper X right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis upper X right parenthesis double vertical bar Subscript upper F Baseline comma EndLayout
EP(x)[fj(x)T fi(x)]

F ≈1
N

N
	
n=1
fj(xn)T fi(xn)

F
= 1
N
fj(X)T fi(X)

F ,
(4.36)

4.5
Comparison of Different Quantization Methods
123
where f Subscript i Baseline left parenthesis upper X right parenthesis element of double struck upper R Superscript upper N times left parenthesis upper C Super Subscript i Superscript times upper H Super Subscript i Superscript times upper W Super Subscript i Superscript right parenthesisfi(X) ∈RN×(Ci×Hi×Wi) represents the output of the i-th layer, f Subscript j Baseline left parenthesis upper X right parenthesis element of double struck upper R Superscript upper N times left parenthesis upper C Super Subscript j Superscript times upper H Super Subscript j Superscript times upper W Super Subscript j Superscript right parenthesisfj(X) ∈
RN×(Cj ×Hj ×Wj ) represents the output of the j-th layer, and StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue Subscript upper F|| · ||F is the Frobenius 
norm. From Eqs. 4.35–4.36, we have:  
upper N integral Underscript script upper D Endscripts f Subscript i Baseline left parenthesis x right parenthesis upper P left parenthesis x right parenthesis f Subscript j Baseline left parenthesis x right parenthesis Superscript upper T Baseline d x almost equals double vertical bar f Subscript j Baseline left parenthesis upper X right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis upper X right parenthesis double vertical bar Subscript upper F Baseline periodN

D
fi(x)P(x)fj(x)T dx ≈
fj(X)T fi(X)

F .
(4.37) 
However, the comparison of orthogonality between different layers is difﬁcult due 
to differences in dimensionality. To this end, we use the Cauchy-Schwarz inequality 
to normalize it to left bracket 0 comma 1 right bracket[0, 1] for the different layers. Applying the Cauchy-Schwarz 
inequality to the left side of Eq. 4.37, we have:  
StartLayout 1st Row 1st Column 0 2nd Column less than or equals left parenthesis upper N integral Underscript script upper D Endscripts f Subscript i Baseline left parenthesis x right parenthesis upper P left parenthesis x right parenthesis f Subscript j Baseline left parenthesis x right parenthesis Superscript upper T Baseline d x right parenthesis squared 2nd Row 1st Column Blank 2nd Column less than or equals integral Underscript script upper D Endscripts upper N f Subscript i Baseline left parenthesis x right parenthesis upper P Subscript i Baseline left parenthesis x right parenthesis f Subscript i Baseline left parenthesis x right parenthesis Superscript upper T Baseline d x integral Underscript script upper D Endscripts upper N f Subscript j Baseline left parenthesis x right parenthesis upper P Subscript j Baseline left parenthesis x right parenthesis f Subscript j Baseline left parenthesis x right parenthesis Superscript upper T Baseline d x period EndLayout
0 ≤

N

D
fi(x)P(x)fj(x)T dx
2
≤

D
Nfi(x)Pi(x)fi(x)T dx

D
Nfj(x)Pj(x)fj(x)T dx.
(4.38) 
We substitute Eq. 4.37 into Eq. 4.38 and perform some simpliﬁcations to derive our 
ORthogonality Metric (ORM):1 
upper O upper R upper M left parenthesis upper X comma f Subscript i Baseline comma f Subscript j Baseline right parenthesis equals StartFraction StartAbsoluteValue EndAbsoluteValue f Subscript j Baseline left parenthesis upper X right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis upper X right parenthesis StartAbsoluteValue EndAbsoluteValue Subscript upper F Superscript 2 Baseline Over StartAbsoluteValue EndAbsoluteValue f Subscript i Baseline left parenthesis upper X right parenthesis Superscript upper T Baseline f Subscript i Baseline left parenthesis upper X right parenthesis StartAbsoluteValue EndAbsoluteValue Subscript upper F Baseline StartAbsoluteValue EndAbsoluteValue f Subscript j Baseline left parenthesis upper X right parenthesis Superscript upper T Baseline f Subscript j Baseline left parenthesis upper X right parenthesis StartAbsoluteValue EndAbsoluteValue Subscript upper F Baseline EndFraction commaORM(X, fi, fj) =
||fj(X)T fi(X)||2
F
||fi(X)T fi(X)||F ||fj(X)T fj(X)||F
,
(4.39) 
where ORM element of left bracket 0 comma 1 right bracket∈[0, 1]. f Subscript ifi and f Subscript jfj are orthogonal when ORM equals 0ORM = 0. On the contrary, 
f Subscript ifi and f Subscript jfj depend on ORM equals 1ORM = 1. Therefore, ORM is negatively correlated with 
orthogonality. 
Calculation Acceleration Given a speciﬁc model, calculating Eq. 4.39 involves 
huge matrices. Suppose that f Subscript i Baseline left parenthesis upper X right parenthesis element of double struck upper R Superscript upper N times left parenthesis upper C Super Subscript i Superscript times upper H Super Subscript i Superscript times upper W Super Subscript i Superscript right parenthesisfi(X) ∈RN×(Ci×Hi×Wi), f Subscript j Baseline left parenthesis upper X right parenthesis element of double struck upper R Superscript upper N times left parenthesis upper C Super Subscript j Superscript times upper H Super Subscript j Superscript times upper W Super Subscript j Superscript right parenthesisfj(X) ∈RN×(Cj ×Hj ×Wj ), 
and that the dimension of the features in the j-th layer is larger than that of 
the i-th layer. Furthermore, the time complexity of computing upper O upper R upper M left parenthesis upper X comma f Subscript i Baseline comma f Subscript j Baseline right parenthesisORM(X, fi, fj) is 
bold italic upper O left parenthesis upper N upper C Subscript j Superscript 2 Baseline upper H Subscript j Superscript 2 Baseline upper W Subscript j Superscript 2 Baseline right parenthesisO(NC2
j H 2
j W 2
j ). The huge matrix occupies a lot of memory resources and increases 
the entire algorithm’s time complexity by several orders of magnitude. Therefore, 
we derive an equivalent form to accelerate the calculation. If we take upper Y equals f Subscript i Baseline left parenthesis upper X right parenthesisY = fi(X), 
upper Z equals f Subscript j Baseline left parenthesis upper X right parenthesisZ = fj(X) as an example, then upper Y upper Y Superscript upper T Baseline comma upper Z upper Z Superscript upper T Baseline element of double struck upper R Superscript upper N times upper NYY T , ZZT ∈RN×N. We have the following: 
StartAbsoluteValue EndAbsoluteValue upper Z Superscript upper T Baseline upper Y StartAbsoluteValue EndAbsoluteValue Subscript upper F Superscript 2 Baseline equals left angle bracket bold v e c left parenthesis upper Y upper Y Superscript upper T Baseline right parenthesis comma bold v e c left parenthesis upper Z upper Z Superscript upper T Baseline right parenthesis right angle bracket comma||ZT Y||2
F =

vec(YY T ), vec(ZZT )

,
(4.40)
1 ORM is formally consistent with CKA. However, we pioneered discovering its relationship with 
quantized model accuracy. We conﬁrmed its validity in mixed precision quantization from the 
perspective of function orthogonality, and CKA explores the relationship between hidden layers 
from the perspective of similarity. In other words, CKA implicitly veriﬁes the validity of ORM 
further. 

124
4
Quantization of Neural Networks
where vec(dot·) represents the operation of ﬂattening matrix into vector. From Eq. 4.40, 
the time complexity of calculating 
upper O upper R upper M left parenthesis upper X comma f Subscript i Baseline comma f Subscript j Baseline right parenthesisORM(X, fi, fj) becomes 
bold italic upper O left parenthesis upper N squared upper C Subscript j Baseline upper H Subscript j Baseline upper W Subscript j Baseline right parenthesisO(N2CjHjWj)
through the inner product of vectors. When the number of samples N is larger than 
the dimension of features upper C times upper H times upper WC × H × W, the norm form is faster to calculate due to 
the lower time complexity and vice versa. 
4.5.3.3 
Mixed Precision Quantization 
Effectiveness of ORM on Mixed Precision Quantization ORM directly indicates 
the importance of the layer in the network, which can eventually be used to 
decide the bit width conﬁguration. We conducted extensive experiments to provide 
sufﬁcient and reliable evidence for this claim. Speciﬁcally, we ﬁrst sample different 
quantization conﬁgurations for ResNet-18 and MobileNetV2. We were then ﬁne-
tuning to obtain the performance. Meanwhile, the overall orthogonality of the 
sampled models is calculated separately. Interestingly, we ﬁnd that the orthogonality 
and performance of the model are positively correlated with the sum of ORM in 
Fig. 4.10. Naturally, inspired by this ﬁnding, maximizing orthogonality is taken as 
our objective function, which is employed to integrate the model size constraints 
and construct a linear programming problem to obtain the ﬁnal bit conﬁguration. 
For a speciﬁc neural network, we can calculate an orthogonality matrix K, where 
kij = ORM(X, fi, fj). K is a symmetric matrix, and the diagonal elements are 1. 
We add the non-diagonal elements of each row of the matrix: 
gamma Subscript i Baseline equals sigma summation Underscript j equals 1 Overscript upper L Endscripts k Subscript i j Baseline minus 1 periodγi =
L
	
j=1
kij −1.
(4.41) 
Fig. 4.10 Relationship between orthogonality and accuracy for different quantization conﬁgura-
tions on ResNet-18 and MobileNetV2

4.5
Comparison of Different Quantization Methods
125
The smaller γi means stronger orthogonality between fi and other functions in the 
set of functions F, and it also means that the former i layers of the neural network 
are more independent. Thus, we use the monotonically decreasing function e−x to 
model this relationship: 
theta Subscript i Baseline equals e Superscript minus beta gamma Super Subscript i Superscript Baseline commaθi = e−βγi,
(4.42) 
where β is a hyperparameter to control the bit width difference between different 
layers; we also investigate the other monotonically decreasing functions (for details, 
refer to Sect. 4.5.3.5). θi is used as the important factor for the former i layers of the 
network, and then we deﬁne a linear programming problem as follows: 
StartLayout 1st Row 1st Column Objective colon 2nd Column max Underscript bold b Endscripts sigma summation Underscript i equals 1 Overscript upper L Endscripts left parenthesis StartFraction b Subscript i Baseline Over upper L minus i plus 1 EndFraction sigma summation Underscript j equals i Overscript upper L Endscripts theta Subscript j Baseline right parenthesis comma 2nd Row 1st Column Constraints colon 2nd Column sigma summation Underscript i Overscript upper L Endscripts upper M Superscript left parenthesis b Super Subscript i Superscript right parenthesis Baseline less than or equals script upper T period EndLayout
Objective: max
b
L
	
i=1
⎛
⎝
bi
L −i + 1
L
	
j=i
θj
⎞
⎠,
Constraints:
L
	
i
M(bi) ≤T.
(4.43) 
M(bi) is the model size of the i-th layer under bi bit quantization and T represents 
the target model size. b is the optimal bit conﬁguration. Maximizing the objective 
function means assigning the larger bit width to a more independent layer, which 
implicitly maximizes the model’s representation capability. 
Solving the linear programming problem in Eq. 4.43 is highly efﬁcient which 
only takes a few seconds on a single CPU. In other words, our method is highly 
efﬁcient (9s on MobileNetV2) compared to previous methods [12, 31, 42], which 
require a lot of data or iterations to search. In addition, our algorithm can be 
combined as a plug-and-play module with quantization-aware training or post-
training quantization schemes due to the high efﬁciency and low data requirements. 
4.5.3.4 
Experiment 
The ImageNet dataset includes 1.2M training data and 50,000 validation data. We 
randomly obtain 64 training data samples for ResNet-18/50 and 32 training data 
samples for MobileNetV2 following similar data preprocessing [21] to derive the 
set of functions script upper FF. For the models with many parameters, we directly adopt the 
round function to convert the bit width into an integer after linear programming. 
Meanwhile, we adopt a depth-ﬁrst search (DFS) to ﬁnd the bit conﬁguration that 
strictly meets the different constraints for a small model, e.g. ResNet-18. The 
processes above are highly efﬁcient and only take a few seconds on these devices. 
Additionally, OMPQ [35] is ﬂexible and can leverage different search spaces with 
QAT and PTQ under different requirements. The ﬁne-tuning implementation details 
are listed below.

126
4
Quantization of Neural Networks
For the experiments on the QAT quantization scheme, we use two NVIDIA 
Tesla V100 GPUs. Our quantization framework does not contain integer division 
or ﬂoating-point numbers in the network. In the training process, the initial learning 
rate is set to 1 e minus 41e −4, and the batch size is set to 128. We use the cosine learning 
rate scheduler and the SGD optimizer with weight decay 1 e minus 41e −4 during 90 epochs 
without distillation. Following the previous work, we ﬁx the weight and activation 
values of the ﬁrst and last layers at 8 bits, where the search space is 4–8 bits. 
4.5.3.5 
Ablation Study 
Monotonically Decreasing Function We then investigate the monotonically 
decreasing function in Eq. 4.42. The second-order derivatives of monotonically 
decreasing functions in Eq. 4.42 inﬂuence the changing rate of orthogonality 
differences. In other words, the variance of the orthogonality between different 
layers becomes larger as the rate increases. We test the accuracy of ﬁve different 
monotonically decreasing functions on quantization-aware training of ResNet-18 
(6.7Mb) and post-training quantization of MobileNetV2 (0.9Mb). We ﬁxed the 
activation to 8 bit. 
It can be seen from Table 4.5 that accuracy gradually decreases while the 
change rate increases. We also observe that a more signiﬁcant change rate for the 
corresponding bit conﬁguration means a more aggressive bit allocation strategy. 
In other words, OMPQ tends to assign more different bits between layers at 
a high rate of change, leading to worse performance in network quantization. 
Another interesting observation is the accuracy of ResNet-18 and MobileNetV2. 
Speciﬁcally, quantization-aware training on ResNet-18 requires numerous data, 
making the accuracy change insigniﬁcant. In contrast, post-training quantization on 
MobileNetV2 cannot assign bit conﬁguration that meets the model constraints when 
the functions are set to −x3 or −ex. To this end, we select e−x as our monotonically 
decreasing function in the following experiments. 
Deconstruction Granularity We study the impact of different granularities of 
deconstruction on the model’s accuracy. Speciﬁcally, we tested four different granu-
larities, including layer-wise, block-wise, stage-wise, and net-wise, in the quantized-
aware training of ResNet-18 and the post-training quantization of MobileNetV2. 
Table 4.5 The Top-1 
accuracy (%) with different 
monotonically decreasing 
functions on ResNet-18 and 
MobileNetV2 
Decreasing 
ResNet-18 
MobileNetV2 
Changing 
Function
(percent sign%)
(percent sign%)
Rate 
e Superscript negative xe−x
72.3072.30
63.51
e Superscript negative xe−x
minus l o g x−logx
72.2672.26
63.2063.20
x Superscript negative 2x−2
negative x−x
72.36
63.063.0
0 
minus x cubed−x3
71.7171.71
–
6 x6x
minus e Superscript x−ex
–
–
e Superscript xex

References
127
Table 4.6 Top-1 accuracy (%) of different deconstruction granularity. The activation bit widths 
of MobileNetV2 and ResNet-18 are both 8. ∗means a mixed bit 
Model
W bit
Layer
Block
Stage
Net 
ResNet-18
5∗
72.51
72.52
72.47
72.31 
MobileNetV2
3∗
69.37
69.10
68.86
63.99 
As reported in Table 4.6, the accuracy of the two models increases with ﬁner 
granularities. This difference is more signiﬁcant in MobileNetV2 due to the different 
sensitiveness between point-wise and depth-wise convolutions. Thus, we employ 
layer-wise granularity in the following experiments. 
References 
1. George B Arfken and Hans J Weber. Mathematical methods for physicists, 1999. 
2. Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training 
of neural networks. Advances in neural information processing systems, 31, 2018. 
3. Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolu-
tional networks for rapid-deployment. Neural Information Processing Systems(NeurIPS), 32, 
2019. 
4. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients 
through stochastic neurons for conditional computation. 
arXiv preprint arXiv:1308.3432, 
2013. 
5. Russel E Caﬂisch. Monte carlo and quasi-monte carlo methods. Acta numerica, 7:1–49, 1998. 
6. Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision 
by half-wave gaussian quantization. In Proceedings of the IEEE conference on computer vision 
and pattern recognition, pages 5918–5926, 2017. 
7. Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner, and Daniel Soudry. 
Neural gradients are near-lognormal: improved quantized and sparse training. arXiv preprint 
arXiv:2006.08173, 2020. 
8. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi 
Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized 
neural networks. arXiv preprint arXiv:1805.06085, 2018. 
9. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks 
with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014. 
10. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep 
neural networks with binary weights during propagations. In Advances in neural information 
processing systems, pages 3123–3131, 2015. 
11. Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu, and Guoqi Li. Gxnor-net: Training deep neural 
networks with ternary weights and activations without full-precision memory under a uniﬁed 
discretization framework. Neural Networks, 100:49–58, 2018. 
12. Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt 
Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. In Neural 
Information Processing Systems(NeurIPS), pages 18518–18529, 2020. 
13. Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and 
Dharmendra S. Modha. Learned step size quantization. ArXiv, abs/1902.08153, 2019. 
14. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and 
Dharmendra S Modha. 
Learned step size quantization. 
arXiv preprint arXiv:1902.08153, 
2019.

128
4
Quantization of Neural Networks
15. Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M Roy, and Ali Ramezani-
Kebrya. Adaptive gradient quantization for data-parallel sgd. Advances in neural information 
processing systems, 33:3174–3185, 2020. 
16. Joshua Fromm, Shwetak Patel, and Matthai Philipose. Heterogeneous bitwidth binarization in 
convolutional neural networks. In Advances in Neural Information Processing Systems, pages 
4006–4015, 2018. 
17. Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David 
Doermann. 
Projection convolutional neural networks for 1-bit cnns via discrete back 
propagation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, 
pages 8344–8351, 2019. 
18. Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen. Network sketching: Exploiting binary 
structure in deep cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, pages 5955–5963, 2017. 
19. Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. 
Single path one-shot neural architecture search with uniform sampling. In Computer Vision– 
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part 
XVI 16, pages 544–560. Springer, 2020. 
20. Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning 
with limited numerical precision. 
In International conference on machine learning, pages 
1737–1746. PMLR, 2015. 
21. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 
Deep residual learning for 
image recognition. In Proceedings of the IEEE conference on computer vision and pattern 
recognition, pages 770–778, 2016. 
22. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias 
Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural 
networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 
23. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 
Binarized neural networks. In Advances in neural information processing systems, pages 4107– 
4115, 2016. 
24. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quan-
tized neural networks: Training neural networks with low precision weights and activations. 
The Journal of Machine Learning Research, 18(1):6869–6898, 2017. 
25. Tianchu Ji, Shraddhan Jain, Michael Ferdman, Peter Milder, H Andrew Schwartz, and Niranjan 
Balasubramanian. On the distribution, sparsity, and inference-time quantization of attention 
values in transformers. arXiv preprint arXiv:2106.01335, 2021. 
26. Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Youngjun Kwak, Jae-Joon Han, 
and Changkyu Choi. Joint training of low-precision neural network with quantization interval 
parameters. arXiv preprint arXiv:1808.05779, 2, 2018. 
27. Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim. Binaryduo: Reducing gradient 
mismatch in binary activation network by coupling binary activations. 
In International 
Conference on Learning Representations. 
28. Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim. 
Binaryduo: Reducing 
gradient mismatch in binary activation network by coupling binary activations. arXiv preprint 
arXiv:2002.06517, 2020. 
29. Yue Li, Wenrui Ding, Chunlei Liu, Baochang Zhang, and Guodong Guo. Trq: Ternary neural 
networks with residual quantization. In AAAI Conference on Artiﬁcial Intelligence, 2021. 
30. Yuhang Li, Xin Dong, Sai Qian Zhang, Haoli Bai, Yuanpeng Chen, and Wei Wang. 
Rtn: 
Reparameterized ternary network. In AAAI, pages 4780–4787, 2020. 
31. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, 
and Shi Gu. {BRECQ}: Pushing the limit of post-training quantization by block reconstruction. 
In International Conference on Learning Representations (ICLR), 2021. 
32. Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao. Performance guaranteed 
network acceleration via high-order residual quantization. 
In Proceedings of the IEEE 
International Conference on Computer Vision, pages 2584–2592, 2017.

References
129
33. Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, 
and David Doermann. Circulant binary convolutional networks: Enhancing the performance 
of 1-bit dcnns with circulant back propagation. In Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition, pages 2691–2699, 2019. 
34. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real 
net: Enhancing the performance of 1-bit cnns with improved representational capability and 
advanced training algorithm. In Proceedings of the European conference on computer vision 
(ECCV), pages 722–737, 2018. 
35. Yuexiao Ma, Taisong Jin, Xiawu Zheng, Yan Wang, Huixia Li, Guannan Jiang, Wei Zhang, and 
Rongrong Ji. Ompq: Orthogonal mixed precision quantization. ArXiv, abs/2109.07865, 2021. 
36. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet 
classiﬁcation using binary convolutional neural networks. 
In European Conference on 
Computer Vision, pages 525–542. Springer, 2016. 
37. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 
Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 4510–4520, 2018. 
38. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale 
image recognition. arXiv preprint arXiv:1409.1556, 2014. 
39. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, 
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. 
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 
2015. 
40. James Tanton. Encyclopedia of Mathematics. Facts on ﬁle, 2005. 
41. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated 
quantization with mixed precision. In Computer Vision and Pattern Recognition (CVPR), pages 
8612–8620, 2019. 
42. Linjie Yang and Qing Jin. Fracbits: Mixed precision quantization via fractional bit-widths. 
AAAI Conference on Artiﬁcial Intelligence (AAAI), 35:10612–10620, 2021. 
43. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. 
Lq-nets: Learned 
quantization for highly accurate and compact deep neural networks. In Proceedings of the 
European conference on computer vision (ECCV), pages 365–382, 2018. 
44. Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient 
convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017. 
45. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 
Dorefa-
net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv 
preprint arXiv:1606.06160, 2016.

Chapter 5 
Network Pruning 
5.1 
Introduction 
Network pruning is a technique used in deep learning to reduce the size and 
complexity of neural networks by eliminating unnecessary connections or param-
eters. Pruning aims to create more efﬁcient and streamlined models that maintain 
or improve performance while reducing computational requirements and memory 
footprint. 
Pruning involves identifying and removing unimportant connections or param-
eters from the neural network. Importance criteria are used to determine the 
signiﬁcance of each connection or parameter. Some standard criteria include: 
Magnitude-based criteria Connections or parameters with small magnitudes are 
considered less important and are pruned. This can be done by setting a threshold 
below which connections are removed or keeping the top-k connections with the 
highest magnitudes. 
Sensitivity-based criteria Connections or parameters that have the most negligible 
impact on the model’s performance are pruned. This is typically determined by 
calculating the gradients or sensitivities of the output concerning each connection 
or parameter. 
Once the important criteria are deﬁned, pruning methods remove the connections 
or parameters identiﬁed as unimportant. Pruning can be categorized into different 
techniques: 
Weight pruning Weight pruning eliminates individual connections or parameters 
based on their importance. This can result in a sparse model where some connections 
have zero values. 
Structured pruning Structured pruning removes entire ﬁlters, channels, or layers 
instead of individual connections. This approach can lead to more efﬁcient and 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
B. Zhang et al., Neural Networks with Model Compression, 
Computational Intelligence Methods and Applications, 
https://doi.org/10.1007/978-981-99-5068-3_5
131

132
5
Network Pruning
regular network structures and may require specialized hardware support for 
practical inference. 
Unit pruning Unit pruning involves removing entire neurons or units from the 
network. This can be based on their importance or by analyzing their impact on the 
network’s performance. 
Network pruning is often performed iteratively to achieve higher levels of 
sparsity (the proportion of pruned connections in the network). The process involves 
multiple pruning iterations, where the least essential connections are successively 
removed. Pruning can be applied layer-wise or globally across the entire network. 
Pruning can be integrated into the training process itself. This approach, known 
as “structured sparsity training” or “learning with sparsity,” involves introducing 
regularization techniques that naturally encourage the network to sparsify during 
training. This leads to a more efﬁcient model, eliminating the need for post-training 
pruning. 
One of the primary advantages of network pruning is the reduction in model size. 
Pruning creates a more compact model by eliminating unnecessary connections and 
parameters from the neural network. This size reduction has practical implications, 
especially in limited storage or memory capacity. Smaller models are easier to store, 
transmit, and deploy, making them more feasible for real-world applications. 
Pruning also leads to improved computational efﬁciency. By removing unim-
portant connections, the computational workload during inference is reduced. This 
results in faster inference times, which is crucial for real-time applications or 
situations requiring low-latency responses. The streamlined model can also leverage 
efﬁcient sparse matrix operations and specialized hardware accelerators, further 
enhancing computational efﬁciency. 
Another beneﬁt of network pruning is the reduction in memory footprint. The 
memory requirements for storing and processing the model are reduced by prun-
ing sparse connections. This is particularly advantageous in resource-constrained 
environments like mobile devices or embedded systems. It allows for the efﬁcient 
execution of deep learning models in memory-limited scenarios. 
Network pruning facilitates the compression and deployment of models. The 
reduced size and complexity of pruned models make them easier to compress and 
deploy. They require less storage space and bandwidth, making them suitable for 
scenarios with limited resources or low-bandwidth networks. Pruned models can be 
efﬁciently deployed on edge devices, IoT devices, or cloud platforms, enabling the 
scalable and resource-efﬁcient deployment of deep learning models. 
It’s important to note that the degree of pruning should be carefully balanced 
to achieve the desired beneﬁts without signiﬁcantly sacriﬁcing model performance. 
Aggressive pruning may lead to accuracy degradation, so a trade-off between model 
size reduction and performance should be considered during the pruning process.

5.2
Structured Pruning
133
5.2 
Structured Pruning 
Structured pruning reduces model size and complexity by pruning entire structured 
components, such as channels, layers, or blocks, rather than individual weights or 
ﬁlters. Structured pruning methods aim to maintain the inherent structure of the 
network while achieving model compression. By removing entire structured com-
ponents, rather than randomly selecting individual parameters, structured pruning 
preserves the architectural characteristics of the network. This allows for more 
efﬁcient hardware acceleration, reduced memory footprint, and simpliﬁed model 
deployment. 
There are different types of structured pruning techniques commonly used: 
Channel Pruning Channel pruning involves removing entire channels or feature 
maps from convolutional layers. Channels represent speciﬁc patterns or feature 
detectors learned by the network. Channel pruning reduces the model’s complexity 
and computational requirements by eliminating redundant or less important chan-
nels. 
Layer Pruning Layer pruning focuses on pruning entire layers from the network 
architecture. Less critical or contributing layers are identiﬁed and removed, resulting 
in a shallower model. This reduces the number of parameters and simpliﬁes the 
overall structure of the network. 
Block Pruning Block pruning targets the removal of entire blocks or modules 
within the network. Blocks often represent repeated structures or groups of layers. 
The network’s complexity is reduced by pruning these blocks while maintaining 
the essential architectural characteristics. This can be particularly effective in deep 
networks with complex architectures. 
Structured Sparsity Structured sparsity techniques enforce structured sparsity 
patterns within the network. Instead of pruning individual weights or ﬁlters, speciﬁc 
structured patterns or structures within the network must be sparse. This can involve 
enforcing sparsity in speciﬁc rows, columns, or other structured patterns of weight 
matrices. 
The beneﬁts of structured pruning are numerous. Structured pruning enables 
efﬁcient hardware acceleration by leveraging specialized hardware accelerators 
to exploit structured sparsity patterns. This results in faster inference times and 
improved energy efﬁciency. Secondly, it signiﬁcantly reduces the memory footprint 
of neural networks, making them more suitable for deployment on memory-
constrained devices or in scenarios with limited storage capacity. Structured pruning 
simpliﬁes model deployment by reducing complexity and facilitating the model 
transfer, compression, and integration into production systems.

134
5
Network Pruning
5.3 
Unstructured Pruning 
Unstructured pruning is a technique used in deep learning to reduce the size 
and complexity of neural networks by selectively removing individual weights or 
ﬁlters without considering their structural relationships. Unlike structured pruning, 
which prunes entire structured components, unstructured pruning focuses on the 
ﬁne-grained elimination of individual parameters based on their importance or 
redundancy. This approach offers ﬂexibility in achieving model compression but 
may result in irregular and unstructured sparsity patterns. 
Unstructured pruning offers ﬂexibility in achieving model compression since 
individual parameters can be selectively pruned. This allows for ﬁne-grained control 
over the sparsity level and signiﬁcantly reduces model size. Targeting and removing 
redundant or less important weights can also achieve high compression rates. The 
model’s size and memory requirements can be signiﬁcantly reduced by eliminating 
these parameters. 
Unstructured pruning does not disrupt the structural integrity of the network 
since individual weights or ﬁlters are pruned independently. This means the model’s 
architecture remains unchanged, allowing for easier integration and transfer of 
pruned models. 
However, the irregular and unstructured sparsity patterns introduced by this 
technique may need to be more efﬁciently utilized by hardware accelerators 
designed for structured sparsity. Additionally, unstructured pruning may require 
careful ﬁne-tuning to recover performance, as removing individual parameters can 
lead to a more signiﬁcant performance drop than structured pruning. 
5.4 
Network Pruning 
5.4.1 
Efﬁcient Structured Pruning Based on Deep Feature 
Stabilization 
Conventional ﬁlter pruning methods generally rely on the important criteria such as 
the script l 1l1-norm value [40] and script l 2l2-norm value [21] of ﬁlters. Two central problems 
may lie in the existing methods. Firstly, the important criterion can only be 
employed on ﬁlter selection, i.e., a block cannot be evaluated by criteria such 
as norm criterion. Secondly, the importance of each ﬁlter seems too simple and 
inefﬁcient due to the existence of batch normalization (BN) [30] and nonlinear 
activation functions, e.g., rectiﬁer linear unit (ReLU) [13]. To overcome these 
two shortcomings, the reconstruction-based method is introduced. He et al. [23] 
propose a channel pruning method based on the local reconstruction error of every 
block and optimized the reconstruction loss via most minor absolute shrinkage 
and selection operator (LASSO) [59] regression. Likewise, accelerated proximal 
gradient (APG) [73] and Taylor expansion [42] are employed to optimize the

5.4
Network Pruning
135
Fig. 5.1 An illustration of EPFS. From left to right lies the training process. The yellow and green 
square sets denote feature maps and ﬁlters, respectively. As represented in the middle, our EPFS 
can be effectively implemented in block or ﬁlter selection by setting the speciﬁc full-precision 
soft masks after speciﬁc layers. The soft mask and other parameters will be updated via FISTA and 
SGD, respectively. When the mask element is zero, the corresponding ﬁlter or block is equivalent to 
being pruned. script upper L Subscript upper MLM, script upper L Subscript upper CLC and script upper L Subscript upper SLS are employed to supervise mask sparsity, deep feature stabilization, 
and network output, respectively 
reconstruction loss. However, the small reconstruction error might be magniﬁed and 
propagated in the deep networks, leading to large reconstruction errors in the global 
outputs. 
We propose an end-to-end efﬁcient pruning method based on feature stability 
(EPFS) [68]. The framework of EPFS is shown in Fig. 5.1. For block pruning, 
we introduce a mask on the output of the layers and use the sparsity supervision, 
i.e., script l 1l1-norm, to supervise the updating of the mask. We introduce a novel script l 2l2-
regularization term for ﬁlter pruning to supervise mask updating. The sparsity 
supervision and cross-entropy make up a couple of adversaries between sparsity 
and accuracy. The center loss [64] is employed to further stabilize the deep feature 
during learning. However, using conventional stochastic gradient descent (SGD) 
to optimize the mask tends to obtain lower performance. Thus, we introduce a 
fast iterative shrinkage-thresholding algorithm (FISTA) [3, 14] to optimize the 
learning process, achieving a faster and more reliable pruning process of the 
mask. 
5.4.1.1 
Preliminaries 
Consider a CNN model consisting of L layers (convolutional and fully connected 
layers) interlaced with rectiﬁer linear units (ReLU) and pooling. We can formulate 
the convolutional layer’s output size as upper K Superscript l Baseline times upper W Superscript l Baseline times upper C Superscript lKl × W l × Cl. Hence, we deﬁne a 
convolution-batch normalization (Conv-BN) operation transforming the input tensor

136
5
Network Pruning
bold italic x Superscript l minus 1 Baseline element of double struck upper R Superscript upper K Super Superscript l minus 1 Superscript times upper W Super Superscript l minus 1 Superscript times upper C Super Superscript l minus 1xl−1 ∈RKl−1×W l−1×Cl−1 to the output tensor bold italic x Superscript l Baseline element of double struck upper R Superscript upper K Super Superscript l Superscript times upper W Super Superscript l Superscript times upper C Super Superscript lxl ∈RKl×W l×Cl as: 
StartLayout 1st Row 1st Column x Subscript j Superscript l 2nd Column equals f Superscript l Baseline left parenthesis bold italic x Superscript l minus 1 Baseline comma bold italic upper F Superscript l Baseline comma bold italic pi Superscript l Baseline comma bold italic beta Superscript l Baseline comma bold italic tau Superscript l Baseline comma bold italic gamma Superscript l Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals gamma Subscript j Superscript l Baseline StartFraction sigma summation Underscript i equals 1 Overscript upper C Superscript l minus 1 Baseline Endscripts x Subscript i Superscript l minus 1 Baseline asterisk upper F Subscript i comma j Superscript l Baseline minus pi Subscript j Superscript l Baseline Over tau Subscript j Superscript l Baseline EndFraction plus beta Subscript j Superscript l Baseline comma EndLayoutxl
j = f l(xl−1, F l, πl, βl, τ l, γ l)
= γ l
j
ΣCl−1
i=1 xl−1
i
∗F l
i,j −πl
j
τ l
j
+ βl
j,
(5.1) 
where upper C Superscript lCl represents the number of channels in the l-th layer. x Subscript j Superscript lxl
j and x Subscript i Superscript l minus 1xl−1
i
are the 
j-th output feature map and the i-th input feature map at the l-th layer. asterisk∗denotes 
the convolutional operation. left parenthesis pi Subscript j Superscript l Baseline comma tau Subscript j Superscript l Baseline comma beta Subscript j Superscript l Baseline comma gamma Subscript j Superscript l Baseline right parenthesis(πl
j, τ l
j, βl
j, γ l
j) are the corresponding BN parameters 
of the j-th channel. upper F Subscript i comma j Superscript lF l
i,j is the i-th kernel of the j-th ﬁlter of the l-th layer. 
5.4.1.2 
Sparse Supervision for Block Pruning 
As shown in Fig. 5.1, different soft masks should be deployed for different pruning 
tasks. We will state this subsection in three parts, i.e., mask setups for block and 
ﬁlter pruning, respectively, and loss formulation. 
We ﬁrst modify the denotation in Eq. 5.1 block-wise for block selection. As 
plotted in Fig. 5.1, we introduce a scalar m Superscript kmk for selecting the k-th block. Considering 
the k-th block containing k Subscript upper NkN layers, we formulate it as: 
StartLayout 1st Row 1st Column bold italic x Superscript k 2nd Column equals m Superscript k Baseline dot script upper F Superscript k Baseline left parenthesis bold italic x Superscript k minus 1 Baseline comma bold italic upper F Superscript k Baseline comma bold italic pi Superscript k Baseline comma bold italic beta Superscript k Baseline comma bold italic tau Superscript k Baseline comma bold italic gamma Superscript k Baseline right parenthesis plus script upper S left parenthesis bold italic x Superscript k minus 1 Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals m Superscript k Baseline dot f Superscript k 1 Baseline ring f Superscript k 2 Baseline ring f Superscript k 3 Baseline ring midline horizontal ellipsis f Superscript k Super Subscript upper N Superscript Baseline plus script upper S left parenthesis bold italic x Superscript k minus 1 Baseline right parenthesis EndLayoutxk = mk · Fk(xk−1, F k, πk, βk, τ k, γ k) + S(xk−1)
= mk · f k1 ◦f k2 ◦f k3 ◦· · · f kN + S(xk−1)
(5.2) 
where f Superscript 1 Baseline ring f squared equals f squared left parenthesis f Superscript 1 Baseline right parenthesisf 1 ◦f 2 = f 2(f 1). f Superscript k Super Subscript if ki denotes the k Subscript nkn-th layer in the k-th block in sequence, 
i.e., f Superscript k Super Subscript n Baseline left parenthesis x Superscript k Super Subscript n Superscript minus 1 Baseline comma bold italic upper F Superscript k Super Subscript n Superscript Baseline comma bold italic pi Superscript k Super Subscript n Superscript Baseline comma bold italic beta Superscript k Super Subscript n Superscript Baseline comma bold italic tau Superscript k Super Subscript n Superscript Baseline comma bold italic gamma Superscript k Super Subscript n Superscript Baseline right parenthesisf kn(xkn−1, F kn, πkn, βkn, τ kn, γ kn). script upper S left parenthesis right parenthesisS() denotes the shortcut transformation. 
We introduce a learnable mask bold italic m equals left bracket m Superscript 1 Baseline comma midline horizontal ellipsis comma m Superscript upper L Baseline right bracketm = [m1, · · · , mL] to scale the output. To guarantee 
the input of the following blocks is not 0, we only implement the mask to the blocks 
having residual connections, rather than blocks in generalized meaning such as [26, 
54]. For the convergence of mask, we use script l 1l1-regularization to punish bold italic mm to optimize 
the elements to 0. Therefore, bold italic mm is learned by: 
script upper L Subscript upper M Baseline equals mu sigma summation Underscript k equals 1 Overscript upper K Endscripts StartAbsoluteValue m Superscript k Baseline EndAbsoluteValue commaLM = μ
K
Σ
k=1
|mk|,
(5.3) 
where m Superscript kmk represents the mask scalar for every block.

5.4
Network Pruning
137
5.4.1.3 
Constrained Sparse Supervision for Filter Pruning 
For ﬁlter pruning, we introduce a vector bold italic m Superscript bold italic lml sized 1 times 1 times upper C Superscript l1 × 1 × Cl to scale the output of 
l-th Conv-BN layer. We formulate the ﬁlter selection process as: 
StartLayout 1st Row 1st Column x Subscript j Superscript l 2nd Column equals f Superscript l Baseline left parenthesis bold italic m Superscript l minus 1 Baseline circled times bold italic x Superscript l minus 1 Baseline comma upper F Subscript j Superscript l Baseline comma pi Subscript j Superscript l Baseline comma beta Subscript j Superscript l Baseline comma tau Subscript j Superscript l Baseline comma gamma Subscript j Superscript l Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals gamma Subscript j Superscript l Baseline StartFraction sigma summation Underscript i equals 1 Overscript upper C Superscript l minus 1 Baseline Endscripts left parenthesis m Subscript i Superscript l minus 1 Baseline dot x Subscript i Superscript l minus 1 Baseline right parenthesis asterisk upper F Subscript i comma j Superscript l Baseline minus pi Subscript j Superscript l Baseline Over tau Subscript j Superscript l Baseline EndFraction plus beta Subscript j Superscript l Baseline comma EndLayoutxl
j = f l(ml−1 ⊗xl−1, F l
j, πl
j, βl
j, τ l
j, γ l
j)
= γ l
j
ΣCl−1
i=1 (ml−1
i
· xl−1
i
) ∗F l
i,j −πl
j
τ l
j
+ βl
j,
(5.4) 
where circled times⊗denotes element-wise multiplication. Equation 5.4 denotes scaling the 
output of left parenthesis l minus 1 right parenthesis(l −1)-th Conv-BN layer by soft mask bold italic m Superscript bold italic l minus bold 1ml−1. 
Unlike the prior work [23, 43], we introduce a novel constraint for sparse 
supervision deployed on ﬁlter pruning as: 
script upper L Subscript upper M Baseline equals mu sigma summation Underscript l equals 1 Overscript upper L Endscripts parallel to bold italic m Superscript l Baseline parallel to Subscript 1 Baseline plus alpha sigma summation Underscript l equals 1 Overscript upper L Endscripts sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts parallel to m Subscript j Superscript l Baseline minus parallel to upper F Subscript j Superscript l Baseline parallel to Subscript 2 Baseline parallel to Subscript 2 Superscript 2 Baseline commaLM = μ
L
Σ
l=1
||ml||1 + α
L
Σ
l=1
Cl
Σ
j=1
||ml
j −||F l
j||2||2
2,
(5.5) 
where we introduce a strong constraint to guarantee the safe convergence of bold italic mm. 
As we observe that the script l 1l1-regularization may cause damage to the architecture, we 
introduce the script l 2l2-regularization to consider the magnitude of ﬁlters. We only zero m Subscript j Superscript lml
j
when the ﬁlter magnitude parallel to upper F Subscript j Superscript l Baseline parallel to Subscript 2||F l
j||2 is close to zero. In Eq. 5.5, script upper L Subscript upper MLM represents the loss 
function to update the mask. K and L denote the network’s total blocks and layers. 
muμ and alphaα are hyperparameters to control the proportion of sparsity and constraint. 
Note that the constraint parallel to m Subscript j Superscript l Baseline minus parallel to upper F Subscript j Superscript l Baseline parallel to Subscript 2 Baseline parallel to Subscript 2 Superscript 2 Baseline||ml
j −||F l
j||2||2
2 is only employed in updating bold italic mm. 
5.4.1.4 
Loss Function 
Cross-entropy is still employed in the learning process to improve image classiﬁca-
tion accuracy. It is formulated as: 
script upper L Subscript upper S Baseline equals minus sigma summation Underscript q equals 1 Overscript upper Q Endscripts log StartFraction e Superscript upper W Super Subscript y Super Sub Subscript q Super Subscript Super Superscript upper L Superscript bold italic x Super Subscript left parenthesis q right parenthesis Super Superscript upper L minus 1 Superscript plus b Super Subscript y Super Sub Subscript q Super Subscript Super Superscript upper L Superscript Baseline Over sigma summation Underscript p equals 1 Overscript upper P Endscripts e Superscript upper W Super Subscript p Super Superscript upper L Superscript bold italic x Super Subscript left parenthesis q right parenthesis Super Superscript upper L minus 1 Superscript plus b Super Subscript p Super Superscript upper L Superscript Baseline EndFraction commaLS = −
Q
Σ
q=1
log
eW L
yq xL−1
(q) +bL
yq
ΣP
p=1 eW L
p xL−1
(q) +bLp
,
(5.6) 
where bold italic x Subscript left parenthesis q right parenthesis Superscript upper L minus 1xL−1
(q) denotes the deep feature of the q-th input in mini-batch. Q denotes the 
mini-batch size. For q element of upper Qq ∈Q, y Subscript qyq denotes the network’s forecast of the y Subscript qyq-th class in 
P.

138
5
Network Pruning
Furthermore, we introduce center loss [64] to maintain the feature stabilization 
of the pruned networks. The center loss function is deﬁned as: 
script upper L Subscript upper C Baseline equals StartFraction lamda Over 2 EndFraction sigma summation Underscript q equals 1 Overscript upper Q Endscripts parallel to bold italic x Subscript left parenthesis q right parenthesis Superscript upper L minus 1 Baseline minus c Subscript y Sub Subscript q Subscript Baseline parallel to Subscript 2 Superscript 2 Baseline periodLC = λ
2
Q
Σ
q=1
||xL−1
(q) −cyq||2
2.
(5.7) 
In Eq. 5.7, x Subscript left parenthesis q right parenthesis Superscript upper L minus 1xL−1
(q)
is the deep feature of q-th input image in batch. g left parenthesis x right parenthesis element of double struck upper R Superscript upper C Super Superscript upper L minus 1 Superscript times upper C Super Superscript upper Lg(x) ∈RCL−1×CL. 
upper C Superscript upper L minus 1CL−1 and upper C Superscript upper LCL are the channels of input and output, respectively. c Subscript y Sub Subscript qcyq is the feature 
center of the ground truth label of q-th input, which denotes the feature center of 
every class. And lamdaλ is the hyperparameter for balancing the proportion of center loss 
and two others. SGD updates c. We will give a more detailed description in the next 
subsection. 
We use joint supervision through cross-entropy and center loss [64] to achieve 
discriminative feature learning. The formulation is given as: 
StartLayout 1st Row 1st Column script upper L 2nd Column equals script upper L Subscript upper S Baseline plus script upper L Subscript upper C Baseline plus script upper L Subscript upper M Baseline EndLayout StartLayout 2nd Row 1st Column Blank 2nd Column equals minus sigma summation Underscript q equals 1 Overscript upper Q Endscripts log StartFraction e Superscript upper W Super Subscript y Super Sub Subscript q Super Subscript Super Superscript upper L Superscript bold italic x Super Superscript bold italic upper L minus bold 1 Superscript plus b Super Subscript y Super Sub Subscript q Super Subscript Super Superscript upper L Superscript Baseline Over sigma summation Underscript p equals 1 Overscript upper P Endscripts e Superscript upper W Super Subscript p Super Superscript upper L Superscript bold italic x Super Superscript bold italic upper L minus bold 1 Superscript plus b Super Subscript p Super Superscript upper L Superscript Baseline EndFraction plus StartFraction lamda Over 2 EndFraction sigma summation Underscript q equals 1 Overscript upper Q Endscripts parallel to bold italic x Subscript left parenthesis q right parenthesis Superscript upper L minus 1 Baseline minus c Subscript y Sub Subscript q Subscript Baseline parallel to Subscript 2 Superscript 2 Baseline plus script upper L Subscript upper M Baseline period EndLayoutL = LS + LC + LMAlt-text already available with first equation in this block
(5.8) 
= − 
Q
Σ
q=1 
log 
e W L 
yq xL−1+bL 
yq
ΣP 
p=1 eW L 
p xL−1+bL 
p 
+ λ 
2 
Q
Σ
q=1
||xL−1 
(q) −cyq||2 
2 + LM.
(5.9) 
The joint loss well supervises the output and deep feature. 
5.4.1.5 
Optimization 
SGD can be directly introduced to update the feature center 
c Subscript pcp and model 
parameters upper WW to solve the optimization problem in Eq. 5.9. We update upper WW and c Subscript pcp
as 
c Subscript p Superscript t plus 1 Baseline equals c Superscript t Baseline minus beta delta Subscript c Sub Subscript pct+1
p
= ct −βδcp
(5.10) 
upper W Superscript t plus 1 Baseline equals c Superscript t Baseline minus eta delta Subscript upper WW t+1 = ct −ηδW
(5.11) 
delta Subscript c Sub Subscript p Baseline equals StartFraction sigma summation Underscript p equals 1 Overscript upper P Endscripts script upper I left parenthesis y Subscript q Baseline equals p right parenthesis dot left parenthesis c Subscript p Baseline minus bold italic x Superscript upper L minus 1 Baseline right parenthesis Over 1 plus sigma summation Underscript p equals 1 Overscript upper P Endscripts script upper I left parenthesis y Subscript q Baseline equals p right parenthesis EndFractionδcp =
ΣP
p=1 I(yq = p) · (cp −xL−1)
1 + ΣP
p=1 I(yq = p)
(5.12)

5.4
Network Pruning
139
StartLayout 1st Row 1st Column delta Subscript upper W 2nd Column equals StartFraction partial differential script upper L Subscript upper S Baseline Over partial differential upper W EndFraction plus lamda StartFraction partial differential script upper L Subscript upper C Baseline Over partial differential upper W EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction partial differential script upper L Subscript upper S Baseline Over partial differential upper W EndFraction plus lamda StartFraction partial differential script upper L Subscript upper C Baseline Over partial differential bold italic x Superscript upper L minus 1 Baseline EndFraction dot StartFraction partial differential bold italic x Superscript upper L minus 1 Baseline Over partial differential upper W EndFraction 3rd Row 1st Column Blank 2nd Column equals StartFraction partial differential script upper L Subscript upper S Baseline Over partial differential upper W EndFraction plus lamda StartFraction partial differential bold italic x Superscript upper L minus 1 Baseline Over partial differential upper W EndFraction dot left parenthesis bold italic x Superscript upper L minus 1 Baseline minus c Subscript y Sub Subscript q Subscript Baseline right parenthesis EndLayoutδW = ∂LS
∂W + λ∂LC
∂W
= ∂LS
∂W + λ ∂LC
∂xL−1 · ∂xL−1
∂W
= ∂LS
∂W + λ∂xL−1
∂W
· (xL−1 −cyq)
(5.13) 
In Eq. 5.12, script upper I left parenthesis y Subscript q Baseline equals p right parenthesisI(yq = p) is the indicative function, deﬁned as: 
script upper I left parenthesis y Subscript q Baseline equals p right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column 0 comma y Subscript q Baseline not equals p 2nd Row 1st Column Blank 2nd Column 1 comma y Subscript q Baseline equals p EndLayoutI(yq = p) =
{ 0,
yq /= p
1,
yq = p
(5.14) 
Then every simple weight in convolution or BN layer can be updated as Eq. 5.13. 
However, it is unreliable to implement SGD to solve the optimization problem in 
Eq. 5.3. Because the SGD may bring the vibration of script upper L Subscript upper MLM, affecting the convergence 
of the mask learning. The misleading mask may remove the necessary structures and 
decrease the accuracy. Thus, a threshold is required to limit the vibration. Under this 
guiding ideology, we introduce FISTA [14] to optimize script upper L Subscript upper MLM. 
We ﬁrst use SGD to optimize the script upper W Subscript pWp and c. The whole procedure relies on the 
original forward-backward pass. Then, we introduce FISTA to solve bold italic mm as: 
arg min Underscript bold italic m Endscripts script upper L Subscript upper M plus script upper R left parenthesis bold italic m right parenthesis arg min
m
LM + R(m)
(5.15) 
The mask bold italic mm can be updated by FISTA with the initial tau Superscript 1 Baseline equals 1τ 1 = 1: 
StartLayout 1st Row 1st Column tau Superscript 1 2nd Column equals 1 2nd Row 1st Column tau Superscript t plus 1 2nd Column equals StartFraction 1 plus StartRoot 1 plus 4 left bracket tau Superscript t plus 1 Baseline right bracket squared EndRoot Over 2 EndFraction EndLayout StartLayout 3rd Row 1st Column bold italic y Superscript t plus 1 2nd Column equals bold italic m Superscript t Baseline plus StartFraction tau Superscript t Baseline minus 1 Over tau Superscript t plus 1 Baseline EndFraction left bracket bold italic m Superscript t Baseline minus bold italic m Superscript t minus 1 Baseline right bracket EndLayout StartLayout 4th Row 1st Column bold italic w Superscript t plus 1 2nd Column equals bold italic y Superscript t plus 1 Baseline minus eta Superscript t plus 1 Baseline dot StartFraction partial differential script upper G left bracket bold italic m Superscript t plus 1 Baseline right bracket Over partial differential bold italic m Superscript t plus 1 Baseline EndFraction EndLayout StartLayout 5th Row 1st Column bold italic m Superscript t plus 1 2nd Column equals sign left parenthesis bold italic w Superscript t plus 1 Baseline right parenthesis ring max left brace StartAbsoluteValue bold italic w Superscript t plus 1 Baseline EndAbsoluteValue minus eta Superscript t plus 1 Baseline dot mu right brace period EndLayoutτ 1 = 1
τ t+1 = 1 +
√
1 + 4[τ t+1]2
2
Alt-text already available with first equation in this block
(5.16) 
yt+1 = mt + τ t − 1 
τ t+1 [mt − mt−1]Alt-text already available with first equation in this block
(5.17) 
wt+1 = yt+1 − ηt+1 · ∂G[mt+1] 
∂mt+1 
Alt-text already available with first equation in this block
(5.18) 
mt+1 = sign(wt+1) ◦max
{
|wt+1| −ηt+1 · μ
}
.
(5.19) 
etaη is an iterative learning rate. The whole optimization procedure is shown in 
Algorithm 8.

140
5
Network Pruning
Algorithm 8: The updating algorithm of EPFS 
An algo rithm for  upda tin g  alg
orit h m  o f E
 P F  S. I t includes 10  step s. The i nput an d o utput a
re depict ed.  
5.10 An algorithm for updating algorithm of E P F S. It includes 10 steps. The input and output are depicted. 5.12 
A
n algor ithm  for upd ati ng algorithm of E P F S. It includes 10 steps. The input and output are depicted. 5.11 An algorithm for updating algorithm of E P F S. It includes 10 steps. The input and output are depicted. 5.13 
A
n a lgorithm  for upd ati ng a lgori thm of
 
E P F S . It  includ es 10 steps . The input and output are depicted. 5.15 An algorithm for updating algorithm of E P F S. It includes 10 steps. The input and output are depicted. 5.19An
 
alg
o
rit
h
m f or upd ating algori thm  of E P F S. I t includ es 10 steps . The input and output are depicted. 
5.4.1.6 
Pruning on ResNet 
Unlike VGGNets [55] or AlexNets [35], in ResNets, each residual block contains 
two or three convolutional layers (followed by both BN and ReLU) and shortcut 
connections. For the consistency of two parts for the sum operation, the number 
of output feature maps in the last convolutional layer must be consistent with that 
of the projection shortcut layer. In particular, when the dimensions of input/output 
channels are mismatched in a residual block, the shortcut connections perform a 
linear projection. 
This work implements our EPFS method on ResNet-18 (two convolutional layers 
in each block) for ﬁlter pruning. We focus on pruning the ﬁrst layer in each residual 
block, as illustrated in Fig. 5.2c. And no pruning operation is conducted in the last 
convolutional layer of each residual block. About StartFraction 128 minus 76 Over 128 EndFraction equals 40.2 percent sign 128−76
128
= 40.2% parameters are 
Fig. 5.2 Illustration of pruning ResNet-18. The red value is the number of remaining ﬁl-
ters/channels. (a) Original block of ResNet-18. (b) Block pruning for ResNet-18. (c) Filter pruning 
for ResNet-18

5.4
Network Pruning
141
pruned as illustrated in Fig. 5.2c, which make up a large proportion in the residual 
block. 
For block pruning, the parameters of shortcut connection are far less than those 
of residual mapping. And the shortcut provides the chance to scale the mapping 
out to 0 without breaking the shape of the output. So, the strategy to set masks is 
clear. We add the mask before the comprehensive point of mapping and shortcut. 
The pruned ResNet-18 can be viewed as Fig. 5.2b. 
5.4.1.7 
Experiments 
We implement extensive experiments to validate the effectiveness of our EPFS 
on ﬁlter pruning and block pruning. For ﬁlter pruning, we use six convolutional 
networks on two datasets, i.e., ResNet-20, ResNet-56 [21], VGGNet [56], and 
MobileNetV3-Large/Small [26] on CIFAR-10 [34] and ResNet-18 [19] on Ima-
geNet ILSVRC2012 [35]. For block pruning, we also use six convolutional networks 
on two datasets, i.e., ResNet-20, ResNet-56, MobileNetV3-Large/Small on CIFAR-
10, ResNet-18, and MobileNetV3-Small on ImageNet ILSVRC2012. Furthermore, 
we implement the comprehensive pruning (pruning block and ﬁlter sequentially), 
using three models on two datasets, i.e., ResNet-20, ResNet-56 on CIFAR-10, and 
ResNet-18 on ImageNet ILSVRC2012. 
We use PyTorch [51] to implement our EPFS method. We use four NVIDIA GTX 
2080 Ti GPUs with 128 GB of RAM to compute the above learning process. The 
hyperparameter muμ is selected in the left bracket 0.001 comma 1 right bracket[0.001, 1] range after cross-validation. And lamdaλ
is set to 0.00030.0003. The weight decay is set to 0.00010.0001, and the momentum is 0.90.9. The  
initial learning rate of script upper L Subscript upper SLS, script upper L Subscript upper MLM, i.e., etaη is set as 0.10.1. And it will be scaled by a factor 
of 0.10.1 every 20 epochs. The learning rate of feature center, i.e., betaβ, is set as 0.50.5. Total 
epochs are 60. 
We evaluate the performance of EPFS on CIFAR-10 for ﬁve networks, ResNet-
20/ResNet-56, MobileNetV3-Large/MobileNetV3-Small, and VGGNet. For block 
pruning, ResNet-20 and ResNet-56 have 9 and 27 blocks for pruning, respectively. 
In MobileNetV3, there are no shortcuts in the downsampling blocks. Therefore, 
MobileNetV3-Large has 15 blocks, in which only 10 have residual connections 
that can be pruned. Likewise, MobileNetV3-Small has 7 of 11 blocks that can be 
pruned. For ﬁlter pruning, we implement it on ResNet-20/ResNet-56 and VGGNet. 
Moreover, we implement the comprehensive pruning on ResNet-20/ResNet-56 and 
compare its performance with the ones using block/ﬁlter pruning alone. P.S., we  
use EPFS-B/F-muμ denoting implementing block/ﬁlter pruning via EPFS with special 
hyperparameter muμ. EPFS-C-mu 1μ1-mu 2μ2 denotes the comprehensive pruning, i.e., block 
and ﬁlter pruning sequentially. mu 1μ1 and mu 2μ2 denote the hyperparameters used for block 
and ﬁlter pruning, respectively. alphaα is set to 1 times 10 Superscript negative 41 × 10−4. 
ResNet-20 To evaluate the effectiveness of our method, we prune ResNet-20, 
where the mask’s effectiveness can be examined subtly. The pruning results are 
shown in Table 5.1. For ResNet-20, when muμ is set to 0.60.6, three out of nine

142
5
Network Pruning
Table 5.1 Filter/block/comprehensive pruning for ResNet-20 on CIFAR-10 
Model
Top-1/+FT%
Params(PR%)
FLOPs(PR%)
Speedup 
ResNet-20 [19]
92.1792.17
0.270.27M
40.5540.55M
– 
MIL [9]
–/91.4391.43
–
32.3132.31M(20.3 percent sign20.3%)
1.26 times1.26×
SFP [21]
90.8390.83/–
–
23.4423.44M(42.2 percent sign42.2%)
1.73 times1.73×
EPFS-B-0.60.6
91.5191.51/91.9191.91
0.200.20M(24.6 percent sign24.6%)
30.8330.83M(24.0 percent sign24.0%)
1.32 times1.32×
EPFS-B-0.80.8
91.3191.31/91.5091.50
0.170.17M(36.9 percent sign36.9%)
22.7222.72M(44.0 percent sign44.0%)
1.79 times1.79×
EPFS-F-0.050.05
90.2090.20/90.8390.83
0.140.14M(51.1 percent sign51.1%)
20.8420.84M(48.6 percent sign48.6%)
1.94 times1.94×
EPFS-C-0.60.6-0.050.05
90.0190.01/90.9890.98
0.120.12M(56.0 percent sign56.0%)
18.9818.98M(53.2 percent sign53.2%)
2.14 times2.14×
Table 5.2 Filter/block/comprehensive pruning for ResNet-56 on CIFAR-10 
Model
Top-1/+FT%
Params(PR%)
FLOPs(PR%)
Speedup 
ResNet-56 [19]
93.2693.26
0.850.85M
125.49125.49M
– 
PFEC [40]
–/93.0693.06
–
90.990.9M(27.6 percent sign27.6%)
1.21 times1.21×
CP [23]
90.8090.80/91.8091.80
–
62M(50.6 percent sign50.6%)
2.02 times2.02×
NISP [71]
–/93.0193.01
–
81M(35.5 percent sign35.5%)
1.55 times1.55×
EPFS-B-0.60.6
91.1691.16/92.8992.89
0.610.61M(27.7 percent sign27.7%)
75.9175.91M(39.5 percent sign39.5%)
1.65 times1.65×
EPFS-B-0.80.8
90.9190.91/92.3492.34
0.350.35M(58.6 percent sign58.6%)
65.3265.32M(47.9 percent sign47.9%)
1.92 times1.92×
EPFS-F-0.010.01
92.1092.10/92.9692.96
0.680.68M(20.0 percent sign20.0%)
89.6089.60M(28.6 percent sign28.6%)
1.40 times1.40×
EPFS-F-0.050.05
90.9290.92/92.0992.09
0.340.34M(40.1 percent sign40.1%)
64.5064.50M(44.7 percent sign44.7%)
1.81 times1.81×
EPFS-C-0.60.6-0.050.05
91.7191.71/92.5392.53
0.280.28M(67.1 percent sign67.1%)
56.4756.47M(55.0 percent sign55.0%)
2.22 times2.22×
residual blocks are pruned with 24.0 percent sign24.0% FLOPs pruned rate, and we only have 0.26 percent sign0.26%
accuracy decrease. This indicates that there are redundant blocks for ResNet-20. 
Compared with SFP [21], our method achieves a much better performance. As for 
ﬁlter pruning, when muμ is set to 0.050.05, 48.6 percent sign48.6% FLOPs are removed with 1.34 percent sign1.34% absolute 
accuracy drop. Moreover, the comprehensive pruning can remove 53.2 percent sign53.2% FLOPs 
with only a 1.19 percent sign1.19% performance decrease, demonstrating that we have achieved a 
new state-of-the-art result. 
ResNet-56 For ResNet-56, the pruning results are shown in Table 5.2. For block 
pruning, when muμ is set to 0.60.6, 11 out of 27 residual blocks are pruned, thus 
realizing 39.5 percent sign39.5% FLOPs pruned rate with a decrease of 0.37 percent sign0.37% accuracy. Compared 
with CP [23], PFEC [40], and NISP [71], our method achieves a much better 
trade-off between accuracy and compression rate. As for ﬁlter pruning, when muμ
is set to 0.010.01, 28.6 percent sign28.6% FLOPs are removed with 0.30 percent sign0.30% absolute accuracy drop. 
Moreover, comprehensive pruning can lead to a 55.0 percent sign55.0% pruning rate with only a 
0.73 percent sign0.73% performance decrease. It gains a higher accuracy and pruning rate than EPFS-
B-0.80.8 and EPFS-F-0.050.05, demonstrating the structured redundancy accounting for a 
large proportion in both width and depth aspects. 
MobileNetV3-Large/MobileNetV3-Small MobileNetV3 is the state-of-the-art 
model. It was obtained by neural architecture search (NAS) [77]. We implement 
the block pruning for MobileNetV3 to validate our method’s effectiveness and

5.4
Network Pruning
143
Table 5.3 Block pruning for MobileNetV3-Large/MobileNetV3-Small on CIFAR-10 
Model
Top-1/+FT% 
Params(PR%)
FLOPs(PR%)
Speedup 
MobileNetV3-Large [26] 
94.0594.05
4.184.18M
227.74227.74M
– 
EPFS-B-0.20.2
93.7193.71/93.8193.81
3.413.41M(18.4 percent sign18.4%) 
179.50179.50M(21.2 percent sign21.2%) 
1.26 times1.26×
EPFS-B-0.60.6
93.8393.83/94.0794.07
2.952.95M(29.4 percent sign29.4%) 
140.03140.03M(38.5 percent sign38.5%) 
1.62 times1.62×
EPFS-B-0.80.8
93.2293.22/93.3293.32
3.013.01M(27.9 percent sign27.9%) 
146.97146.97M(35.5 percent sign35.5%) 
1.55 times1.55×
MobileNetV3-Small [26] 
93.1493.14
2.932.93M
66.1766.17M
– 
EPFS-B-0.20.2
92.8192.81/92.9092.90
2.792.79M(4.64 percent sign4.64%) 
45.0045.00M(32.0 percent sign32.0%)
1.47 times1.47×
EPFS-B-0.60.6
92.7992.79/92.8292.82
2.552.55M(12.9 percent sign12.9%) 
38.1838.18M(42.3 percent sign42.3%)
1.73 times1.73×
EPFS-B-0.80.8
90.9990.99/91.1391.13
2.552.55M(27.9 percent sign27.9%) 
37.5137.51M(43.3 percent sign43.3%)
1.77 times1.77×
Table 5.4 Filter pruning for VGGNet on CIFAR-10 
Model
Top-1/+FT%
Params(PR%)
FLOPs(PR%)
Speedup 
VGGNet [55]
93.5093.50
14.9814.98M
313.73313.73M
– 
PFEC [40]
93.4093.40
5.405.40(64.0 percent sign64.0%)
206.00206.00M(34.3 percent sign34.3%)
1.21 times1.21×
EPFS-F-0.0010.001
91.3791.37/93.6193.61
6.496.49M(56.7 percent sign56.7%)
200.2200.2M(36.2 percent sign36.2%)
1.57 times1.57×
EPFS-F-0.0050.005
92.5792.57/94.6794.67
4.414.41M(69.1 percent sign69.1%)
156.87156.87M(47.5 percent sign47.5%)
1.90 times1.90×
generalization ability. Results are shown in Table 5.3. For MobileNetV3-Large, 
when muμ is set to 0.60.6, 4 out of 11 blocks are pruned with 38.5 percent sign38.5% FLOPs pruned 
rate, and we achieve a 0.02 percent sign0.02% accuracy increase. For MobileNetV3-Small, when muμ
is set to 0.60.6, two out of seven blocks are pruned with 42.3 percent sign42.3% FLOPs pruned rate, 
and we achieve only 0.28 percent sign0.28% accuracy decrease. Finally, the pruned model has about 
93 percent sign93% accuracy and only 36.9136.91M FLOPs on CIFAR-10, resulting in new state-of-
the-art results. The experiments on MobileNetV3 demonstrate that the searched 
architectures by NAS still have redundancy, while our EPFS can efﬁciently reduce 
it. 
VGGNet For VGGNet, there are no blocks. Hence, we deploy ﬁlter pruning via 
EPFS on VGGNet, whose results are shown in Table 5.4. We set the hyperparameter 
muμ in left bracket 0.001 comma 0.01 right bracket[0.001, 0.01], for  a small muμ that is needed to balance the script upper L Subscript upper MLM produced by 4736 
channels in VGGNet. When muμ is set to 0.0050.005, 2022 out of 4736 channels are pruned, 
thus removing 47.5 percent sign47.5% FLOPs. Compared with PFEC [40], our method achieves a 
much better accuracy and compression rate performance (Fig. 5.3). 
We further evaluate the performance of EPFS on large-scale ImageNet and 
ILSVRC2012 in two networks, ResNet-18 and MobileNetV3-Small. ResNet-18 and 
MobileNetV3-Small have eight and six blocks to prune for the block pruning, 
respectively. We then implement ﬁlter pruning on ResNet-18. Moreover, we imple-
ment the comprehensive pruning on ResNet-18 and compare its performance with 
the ones using block/ﬁlter pruning alone. And alphaα is set to 1 times 10 Superscript negative 41 × 10−4. 
ResNet-18 We also evaluated our method on ImageNet using ResNet-18. We train 
the pruned network with a mini-batch size of 128 for 60 epochs. As shown in 
Table 5.5, our method can obtain 1.41 times1.41× and 1.86 times1.86× speedup by setting muμ to 0.20.2

144
5
Network Pruning
Fig. 5.3 ImageNet ILSVRC2012 dataset 
Table 5.5 Filter/block/comprehensive pruning for ResNet-18 on ImageNet ILSVRC2012 
Model
Top-1/+FT% 
Top-5/+FT% 
Params (PR%) 
FLOPs (PR%) 
Speed Up 
ResNet-18 [19] 
69.7569.75
89.2489.24
10.6710.67M
1.811.81B
– 
MIL [9]
–/66.3366.33
–/86.9486.94
–
1.181.18B(34.6 percent sign34.6%) 
1.54 times1.54×
SFP [21]
–/67.1067.10
–/87.7887.78
–
1.051.05B(41.8 percent sign41.8%) 
1.72 times1.72×
FPGM [22]
67.7867.78/68.3468.34
88.0188.01/88.5388.53
–
1.051.05B(41.8 percent sign41.8%) 
1.72 times1.72×
EPFS-B-0.20.2
67.9167.91/68.2168.21
87.8087.80/88.2088.20
8.138.13M(23.8 percent sign23.8%) 
1.281.28B(29.3 percent sign29.3%) 
1.41 times1.41×
EPFS-B-0.60.6
66.7966.79/67.5367.53
86.9186.91/87.8387.83
7.127.12M(33.3 percent sign33.3%) 
0.980.98B(46.0 percent sign46.0%) 
1.86 times1.86×
EPFS-F-0.050.05
67.2167.21/67.8167.81
87.1287.12/88.3788.37
6.986.98M(34.6 percent sign34.6%) 
1.051.05B(42.1 percent sign42.1%) 
1.72 times1.72×
EPFS-C-0.60.6-0.050.05
67.4167.41/68.1268.12
87.3087.30/88.2988.29
5.655.65M(47.0 percent sign47.0%) 
0.810.81B(55.2 percent sign55.2%) 
2.23 times2.23×
and 0.50.5 for block pruning, with the decrease of 1.56 percent sign1.56%/1.04 percent sign1.04% and 2.23 percent sign2.23%/1.41 percent sign1.41%
in Top-1/Top-5 accuracy, respectively. For ﬁlter pruning, our EPFS obtained the 
67.81 percent sign67.81% Top-1 accuracy with 34.6 percent sign34.6% FLOPs removed. Furthermore, we implement 
comprehensive pruning for ResNet-18 on ImageNet ILSVRC2012. We set the 
hyperparameter muμ as 0.60.6 and 0.050.05 for block and ﬁlter pruning, respectively. 
Our EPFS can obtain 68.12 percent sign68.12% Top-1 and 88.29 percent sign88.29% Top-5 accuracy with 2.23 times2.23×
acceleration, largely outperforming the state of the art. 
MobileNetV3-Small We implement the block pruning in MobileNetV3-Small on 
ImageNet ILSVRC2012 to validate the effectiveness of our method. Results are 
shown in Table 5.6. When muμ is set to 0.20.2, three out of seven blocks are pruned with 
31.8 percent sign31.8% FLOPs pruned rate, and we achieve only 1.30 percent sign1.30% accuracy drop. And when muμ

5.4
Network Pruning
145
Table 5.6 Block pruning for MobileNetV3-Small on ImageNet ILSVRC2012 
Model
Top-1/+FT% 
Top-5/+FT% 
Params (PR%) 
FLOPs (PR%) 
Speed Up 
MobileNetV3-
Small[26]
67.467.4
87.187.1
2.932.93M
66.1766.17M
– 
EPFS-B-0.20.2
64.8264.82/66.1066.10
85.4585.45/86.1586.15
1.921.92M(34.5 percent sign34.5%) 
45.1245.12M(31.8 percent sign31.8%) 
1.41 times1.41×
EPFS-B-0.60.6
64.5164.51/65.8165.81
84.7884.78/85.9185.91
1.711.71M(41.6 percent sign41.6%) 
37.1037.10M(43.9 percent sign43.9%) 
1.86 times1.86×
Table 5.7 Controlled block pruning experiments for different optimizers on CIFAR-10. muμ is set 
as 0.60.6 and 0.20.2 for MobileNetV3-Small and ResNet-20, respectively 
Model
Top-1/+FT%
Params(PR%)
FLOPs(PR%) 
ResNet-20 [19]
92.1792.17
0.270.27M
40.5540.55M 
EPFS-FISTA
91.5191.51/91.9191.91
0.200.20M(24.6 percent sign24.6%)
30.8330.83M(24.0 percent sign24.0%) 
EPFS-SGD
91.5191.51/91.5591.55
0.210.21M(22.2 percent sign22.2%)
32.3432.34M(20.2 percent sign20.2%) 
EPFS-LASSO
90.1390.13/90.9990.99
0.190.19M(29.6 percent sign29.6%)
28.3128.31M(30.2 percent sign30.2%) 
MobileNetV3-Small
93.1493.14
2.932.93M
66.1766.17M 
EPFS-FISTA
92.8192.81/92.9092.90
2.792.79M(4.64 percent sign4.64%)
45.0045.00M(32.0 percent sign32.0%) 
EPFS-SGD
91.8291.82/91.8891.88
2.622.62M(10.6 percent sign10.6%)
43.1143.11M(34.8 percent sign34.8%) 
EPFS-LASSO
90.1290.12/90.4590.45
2.372.37M(19.1 percent sign19.1%)
38.1238.12M(42.4 percent sign42.4%) 
is set to 0.60.6, f our  out of seven blocks are pruned with 41.6 percent sign41.6% FLOPs pruned rate, 
and we lead to 1.59 percent sign1.59% accuracy decrease and new state-of-the-art results. 
5.4.1.8 
Ablation Study 
The effectiveness of our method comes from FISTA and center loss. To examine 
how they affect the ﬁnal performance, we select ResNet-20 and MobileNetV3-
Large/MobileNetV3-Small for an ablation study. 
FISTA In our ablation, we use ResNet-20 and MobileNetV3-Small to prune 
networks based on FISTA, SGD, and LASSO. The results are presented in Table 5.7. 
Compared to SGD, FISTA achieves a higher accuracy and better pruning rate in 
the same experimental settings. Compared to LASSO, FISTA achieves a higher 
accuracy with a lower pruning rate. However, these results also demonstrate that 
the pruned network via FISTA achieves a better trade-off than LASSO. The SGD 
optimizer can provide better initial parameters to prune a network, but its ﬁne-tuning 
performance is worse than FISTA. Generally, EPFS with FISTA can fast and steadily 
prune the network and achieve better accuracy than SGD and LASSO. As plotted 
in Fig. 5.4, script upper L Subscript upper MLM, script upper L Subscript upper SLS, and script upper L Subscript upper CLC can converge pretty well. script upper L Subscript upper MLM is updated on speciﬁc 
iterations via FISTA so that the loss curve descends discretely.

146
5
Network Pruning
(a)
(b) 
(c) 
Fig. 5.4 Loss curve of LM, LS, and LC
Table 5.8 Controlled block pruning experiments for center loss on CIFAR-10. μ is set as 0.6. 
EPFS∗denotes the control group without center loss 
Model
Top-1/+FT%
Params(PR%)
FLOPs(PR%) 
MobileNetV3-Large [26]
94.05
4.18M
227.74M 
EPFS
93.83/94.07
2.95M(29.4%)
140.03M(38.5%) 
EPFS∗
93.54/93.65
2.72M(22.2%)
128.76M(43.5%) 
MobileNetV3-Small [26]
93.14
2.93M
66.17M 
EPFS
92.79/92.82
2.55M(12.9%)
38.18M(42.3%) 
EPFS∗
91.60/91.86
2.51M(14.3%)
29.48M(55.4%) 
Center Loss To ﬁnd out whether the center loss works, we used MobileNetV3-
Large/MobileNetV3-Small to prune in two situations distinguished by the existence 
of center loss. The results are presented in Table 5.8. For MobileNetV3-Large, 
the EPFS without center loss got the decrease of 0.38% accuracy. Also, the EPFS 
without center loss achieves a decrease of 0.96% accuracy for MobileNetV3-Small. 
We conclude that center loss is vital in optimizing masks and stabilizing deep 
features.

5.4
Network Pruning
147
5.4.2 
Toward Compact and Sparse CNNs via 
Expectation-Maximization 
Among structured pruning, ﬁlter pruning has attracted the most attention for its 
ability to slim the network, making it a thinner architecture without speciﬁc 
hardware support for accelerating. Similarly, block pruning can reduce the FLOPs 
of networks by shortening the network architecture. We produce a thinner and 
shorter architecture after pruning using the two methods. Most previous ﬁlter 
pruning methods [21, 22, 40] were based on the information of ﬁlters, such as 
the value of ﬁlter norm. They use the norm information to evaluate the ﬁlter and 
then hard prune or zero the ﬁlters which fail the criterion. Other methods are 
based on various techniques to zero out the ﬁlters, including generative adversarial 
learning [43], greedy search [46], Taylor expansion [42], etc.. These methods have 
achieved a high pruning rate with an acceptable performance drop. However, prior 
methods have three main areas for improvement. The ﬁrst one is that ﬁlter pruning 
criterion based only on ﬁlter information remains insufﬁcient, which resulted from 
the existence of nonlinear activation functions (e.g., rectiﬁer linear unit (ReLU)), 
and other complex operations (e.g., batch normalization (BN)). For example, 
computing the convolution of vector a equals left parenthesis 0 comma 1 right parenthesis Superscript upper Ta = (0, 1)T by vector b equals left parenthesis 5 comma 1 right parenthesis Superscript upper Tb = (5, 1)T and 
c equals left parenthesis 1 comma 4 right parenthesis Superscript upper Tc = (1, 4)T , we have  upper R e upper L upper U left parenthesis a asterisk b right parenthesis less than upper R e upper L upper U left parenthesis a asterisk c right parenthesisReLU(a ∗b) < ReLU(a ∗c), while parallel to c parallel to Subscript 1 Baseline less than parallel to b parallel to Subscript 1 Baseline||c||1 < ||b||1 and 
parallel to c parallel to Subscript 2 Baseline less than parallel to b parallel to Subscript 2 Baseline||c||2 < ||b||2. The second one is that prior work devoted to zeroing the output 
of ﬁlters may cause permanent structured damage in training. For instance, a full-
precision mask is employed to sparse the output feature maps under the supervision 
of script l 1l1 regularization while lacking an efﬁcient backtracking mechanism. Once the 
damage is caused to structured, i.e., an unsatisfactory element is updated to zero, 
it will never be repaired for the sparsity supervision of script l 1l1-norm. Moreover, ﬁne-
tuning always demands zeroing out the pruning pattern for the damage caused in 
training. However, the ﬁne-tuning process may cause additional redundancy for 
pruned architecture, which causes less sparsity of remaining ﬁlters and more or less 
performance drop for prior pruning methods. 
We focus on training a network with less redundancy and higher sparsity. The 
intention is to employ the expectation-maximization (EM) algorithm in the training 
process, as illustrated in Fig. 5.5. First, we analyze the distribution of ﬁlters in 
hyperspace, i.e., employ the Gaussian mixture models (GMM) to analyze the ﬁlters 
and EM algorithm to solve the GMM. The expectation step (E-step) is deployed to 
cluster the ﬁlters into the maximum likelihood distribution group. The maximization 
step (M-step) is employed in calculating the maximum likelihood distribution 
parameters and formulating a well-deﬁned loss function to monitor the ﬁlters with 
similar distribution to converge to be consistent. Dynamic clustering method is 
implemented to reanalyze and re-cluster the ﬁlters to improve the distribution 
diversity. After a certain number of epochs, the distribution loss can converge to 
zero, which means the current network is identical to the pruned one. We ﬁne-tune 
the clustered networks to optimal inter-cluster sparsity and then we can prune the 
network with the optimal weights.

148
5
Network Pruning
Fig. 5.5 Illustration of SPEM scheme. (a) The process includes warming up the EM algorithm, 
clustering the ﬁlters via the expectation step, averaging the output feature maps of every cluster, 
calculating the loss, optimizing parameters, and pruning the well-trained model. (b) Detailed 
illustration of training via EM, including clustering the ﬁlters with different distributions in 
hyperspace, averaging the output of the same cluster. For instance, the ﬁrst layer’s 1-st and 2-
nd ﬁlters are clustered together. (c) Pruning two layers with ﬁlters of the same cluster sharing the 
consistent distribution. For instance, trim the ﬁlters sharing the same distribution to one such as 
the 2-nd ﬁlter in the ﬁrst layer and 2-nd and 3-rd ﬁlters in the second layer. In particular, the 2-nd 
kernels of every ﬁlter in the second layer should be pruned by adding the second kernels to their 
corresponding 1-st ones 
5.4.2.1 
Preliminaries 
As  shown in Fig. 5.5, we analyze the l-th layer, and we should categorize the ﬁlters 
into distributions. In modern CNNs, the BN is always followed after convolutions. 
For the consistency of pruned networks, we regard the possible subsequent BN and 
scaling layers as part of the convolutional layer. First, denote the output of the j-th 
ﬁlter in l-th layer as: 
xl
j = γ l
j
ΣCl−1
i=1 xl−1
i
∗F l
i,j −πl
j
τ l
j
+ βl
j,
(5.20) 
where Cl represents the number of channels in l-th layer. xl
j and xl−1
i
are the 
j output feature map and i-th input feature map at the l-th layer. F l
i,j denotes 
the i-th kernel of j-th ﬁlter at l-th layer. ∗denotes the convolutional operation. 
(πl
j, τ l
j, βl
j, γ l
j) are the corresponding BN parameters of j-th channel. Hence, the 
parameter set of j-th channel at i-th layer is formulated as: 
Wl
j =
{
F l
j, γ l
j, βl
j, πl
j, τ l
j
}
.
(5.21)

5.4
Network Pruning
149
To analyze the distribution, we propose the hypothesis that every ﬁlter in a 
pre-trained network approximately satisﬁes multidimensional Gaussian distribution, 
i.e., upper F Subscript j Superscript l Baseline tilde script upper N left parenthesis mu Subscript j Superscript l Baseline comma upper Sigma Subscript j Superscript l Baseline right parenthesisF l
j ∼N(μl
j, Σl
j). And the ﬁlters satisfy the individual hypothesis for the 
linearity of convolution. The clustering process can be simpliﬁed by categorizing the 
ﬁlters with similar distributions into the same cluster. To simplify the computation, 
we reshape the matrix upper F Subscript j Superscript l Baseline element of double struck upper R Superscript normal k Super Superscript l Superscript times normal w Super Superscript l Superscript times upper C Super Superscript l minus 1F l
j ∈Rkl×wl×Cl−1 to a vector upper F Subscript j Superscript l Baseline element of double struck upper R Superscript left parenthesis normal k Super Superscript l Superscript dot normal w Super Superscript l Superscript dot upper C Super Superscript l minus 1 Superscript right parenthesis times 1F l
j ∈R(kl·wl·Cl−1)×1 during 
cluster process, which also satisﬁes the individual hypothesis. 
5.4.2.2 
Distribution-Aware Forward and Loss Function 
Based on the hypothesis above, the l-th layer satisﬁes the GMM. The pruning ratio 
can deﬁne the GMM’s dimension upper K Superscript lKl. Then we have: 
StartLayout 1st Row 1st Column Blank 2nd Column upper K Superscript l Baseline equals less than upper C Superscript l Baseline times left parenthesis 1 minus script upper E right parenthesis greater than 2nd Row 1st Column Blank 2nd Column script upper P left parenthesis bold italic upper F Superscript l Baseline vertical bar bold italic upper Theta Superscript l Baseline right parenthesis equals alpha Subscript k Superscript l Baseline sigma summation Underscript k equals 1 Overscript upper K Superscript l Baseline Endscripts upper Phi left parenthesis bold italic upper F Superscript l Baseline vertical bar upper Theta Subscript k Superscript l Baseline right parenthesis comma EndLayoutKl =< Cl × (1 −E) >
P(F l|Ol) = αl
k
Kl
Σ
k=1
0(F l|Ol
k),
(5.22) 
where script upper EE is a pre-deﬁned pruning ratio to supervise the clustering process. less than a greater than< a >
represents rounding the ﬂoat a to its integer approximation. alpha Subscript k Superscript lαl
k represents the ratio 
of k-th distribution accounting. upper C Superscript lCl is the dimension of l-th layer. Hence, upper K Superscript lKl is the 
number of clusters and the dimension of GMM. upper Theta Subscript k Superscript lOl
k denote the parameter of k-th 
distribution, i.e., left parenthesis mu Subscript k Superscript l Baseline comma upper Sigma Subscript k Superscript l Baseline right parenthesis(μl
k, Σl
k). bold italic upper Theta Superscript bold italic lOl denotes the assembly of upper Theta Subscript k Superscript lOl
k. bold italic upper F Superscript lF l denotes the assembly 
of ﬁlters at l-th layer as well as the observed data. Then we introduce the hidden 
variable xi Subscript j k Superscript lξl
jk to formulate the maximum likelihood estimation (MLE) of GMM as: 
xi Subscript j k Superscript l Baseline equals StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column 1 comma upper F Subscript j Superscript l Baseline element of script upper D Subscript k Superscript l Baseline 2nd Row 1st Column Blank 2nd Column 0 comma normal e normal l normal s normal e EndLayoutξl
jk =
{ 1, F l
j ∈Dl
k
0, else
(5.23) 
In Eq. 5.23, xi Subscript j k Superscript lξl
jk is the hidden variable describing the afﬁliation between j-th ﬁlter 
and k-th cluster at l-th layer. script upper D Subscript k Superscript lDl
k denotes the k-th distribution at l-th layer. 
Based on the multi-dim Gaussian distribution hypothesis and the preliminaries 
above, we formulate MLE as: 
StartLayout 1st Row script upper P left parenthesis bold italic upper F Superscript l Baseline vertical bar bold italic upper Theta Superscript l Baseline right parenthesis equals product Underscript k equals 1 Overscript upper K Superscript l Baseline Endscripts alpha Superscript StartAbsoluteValue script upper D Super Subscript k Super Superscript l Superscript EndAbsoluteValue Baseline product Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts left brace StartFraction 1 Over left parenthesis 2 pi right parenthesis Superscript StartFraction d Over 2 EndFraction Baseline StartAbsoluteValue upper Sigma Subscript j Superscript l Baseline EndAbsoluteValue Superscript one half Baseline EndFraction times exp left bracket minus one half left parenthesis upper F Subscript j Superscript l Baseline minus mu Subscript k Superscript l Baseline right parenthesis Superscript upper T Baseline upper Sigma Subscript j Superscript l Super Superscript negative 1 Superscript Baseline left parenthesis upper F Subscript j Superscript l Baseline minus mu Subscript k Superscript l Baseline right parenthesis right bracket right brace Superscript xi Super Subscript j k Super Superscript l Superscript Baseline comma EndLayoutP(F l|Ol) =
Kl
||
k=1
α|Dl
k|
Cl
||
j=1
⎧
⎨
⎩
1
(2π)
d
2 |Σl
j|
1
2
× exp
[
−1
2(F l
j −μl
k)T Σl−1
j
(F l
j −μl
k)
]⎫
⎬
⎭
ξl
jk
,
(5.24) 
where StartAbsoluteValue script upper D Subscript k Superscript l Baseline EndAbsoluteValue equals sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts xi Subscript j k|Dl
k| = ΣCl
j=1 ξjk and upper C Superscript l Baseline equals sigma summation Underscript k equals 1 Overscript upper K Superscript l Baseline Endscripts StartAbsoluteValue script upper D Subscript k Superscript l Baseline EndAbsoluteValueCl = ΣKl
k=1 |Dl
k|. d represents the dimension of ﬁlter, 
i.e., normal k Superscript l Baseline dot normal w Superscript l Baseline dot upper C Superscript l minus 1kl · wl · Cl−1. Next, we introduce the EM algorithm.

150
5
Network Pruning
Expectation step First, we formulate the script upper QQ function as: 
StartLayout 1st Row 1st Column script upper Q left parenthesis bold italic upper Theta Superscript l Baseline comma bold italic upper Theta Superscript l comma e Baseline right parenthesis equals 2nd Column double struck upper E left bracket log script upper P left parenthesis bold italic upper F Superscript l Baseline comma bold italic xi Superscript bold italic l Baseline vertical bar bold italic upper Theta Superscript l Baseline right parenthesis vertical bar bold italic upper F Superscript l Baseline comma bold italic upper Theta Superscript l comma e Baseline right bracket 2nd Row 1st Column equals 2nd Column sigma summation Underscript k equals 1 Overscript upper K Superscript l Endscripts left brace sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts left parenthesis double struck upper E xi Subscript j k Superscript l Baseline right parenthesis log alpha Subscript k Superscript l Baseline 3rd Row 1st Column Blank 2nd Column plus sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts left parenthesis double struck upper E xi Subscript j k Superscript l Baseline right parenthesis left bracket minus one half log left parenthesis 2 pi right parenthesis Superscript d Baseline StartAbsoluteValue upper Sigma Subscript j Superscript l Baseline EndAbsoluteValue plus one half left parenthesis upper F Subscript j Superscript l Baseline minus mu Subscript k Superscript l Baseline right parenthesis Superscript upper T Baseline upper Sigma Subscript j Superscript l Super Superscript negative 1 Superscript Baseline left parenthesis upper F Subscript j Superscript l Baseline minus mu Subscript k Superscript l Baseline right parenthesis right bracket right brace comma EndLayout
Q(Ol, Ol,e) =E
[
log P(F l, ξl|Ol)|F l, Ol,e]
=
Kl
Σ
k=1
⎧
⎨
⎩
Cl
Σ
j=1
(Eξl
jk)logαl
k
+
Cl
Σ
j=1
(Eξl
jk)
[
−1
2log (2π)d|Σl
j| +1
2(F l
j −μl
k)T Σl−1
j
(F l
j −μl
k)
]}
,
where e denotes the current epoch number since we only cluster the ﬁlter once at the 
beginning of every epoch. Then compute the MLE of xi Subscript j k Superscript lξl
jk, i.e., double struck upper E left parenthesis xi Subscript j k Superscript l Baseline vertical bar bold italic upper F Superscript l Baseline comma bold italic upper Theta Superscript l Baseline right parenthesisE(ξl
jk|F l, Ol) via: 
StartLayout 1st Row 1st Column double struck upper E left parenthesis xi Subscript j k Superscript l Baseline vertical bar bold italic upper F Superscript l Baseline comma bold italic upper Theta Superscript l Baseline right parenthesis equals 2nd Column script upper P left parenthesis xi Subscript j k Superscript l Baseline equals 1 vertical bar bold italic upper F Superscript l Baseline comma bold italic upper Theta Superscript l Baseline right parenthesis 2nd Row 1st Column equals 2nd Column StartFraction alpha Subscript k Superscript l Baseline upper Phi left parenthesis upper F Subscript j Superscript l Baseline vertical bar upper Theta Subscript k Superscript l Baseline right parenthesis Over sigma summation Underscript k equals 1 Overscript upper K Superscript l Baseline Endscripts alpha Subscript k Superscript l Baseline upper Phi left parenthesis upper F Subscript j Superscript l Baseline vertical bar upper Theta Subscript k Superscript l Baseline right parenthesis EndFraction period EndLayoutE(ξl
jk|F l, Ol) =P(ξl
jk = 1|F l, Ol)
=
αl
k0(F l
j|Ol
k)
ΣKl
k=1 αl
k0(F l
j|Ol
k)
.
(5.25) 
We denote double struck upper E left parenthesis xi Subscript j k Superscript l Baseline vertical bar bold italic upper F Superscript l Baseline comma bold italic upper Theta Superscript l Baseline right parenthesisE(ξl
jk|F l, Ol) as ModifyingAbove xi With caret Subscript j k Superscript lˆξl
jk for simpliﬁcation. ModifyingAbove xi With caret Subscript j k Superscript lˆξl
jk represents the relativity 
between k-th and j-th ﬁlter. Then we can modify the Q function by substituting 
double struck upper E xi Subscript j k Superscript lEξl
jk by the estimation ModifyingAbove xi With caret Subscript j k Superscript lˆξl
jk, so we have:  
StartLayout 1st Row 1st Column script upper Q left parenthesis bold italic upper Theta Superscript l Baseline comma bold italic upper Theta Superscript l comma t Baseline right parenthesis equals 2nd Column sigma summation Underscript k equals 1 Overscript upper K Superscript l Endscripts left brace script upper D Subscript k Superscript l Baseline log alpha Subscript k Superscript l Baseline plus 2nd Row 1st Column Blank 2nd Column sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts ModifyingAbove xi With caret Subscript j k Superscript l Baseline left bracket minus one half log left parenthesis 2 pi right parenthesis Superscript d Baseline StartAbsoluteValue upper Sigma Subscript j Superscript l Baseline EndAbsoluteValue plus one half left parenthesis upper F Subscript j Superscript l Baseline minus mu Subscript k Superscript l Baseline right parenthesis Superscript upper T Baseline upper Sigma Subscript j Superscript l Super Superscript negative 1 Superscript Baseline left parenthesis upper F Subscript j Superscript l Baseline minus mu Subscript k Superscript l Baseline right parenthesis right bracket right brace period EndLayoutQ(Ol, Ol,t) =
Kl
Σ
k=1
{
Dl
klog αl
k+
Cl
Σ
j=1
ˆξl
jk
[
−1
2log (2π)d|Σl
j| +1
2(F l
j −μl
k)T Σl−1
j
(F l
j −μl
k)
]}
.
(5.26) 
In this way, we ﬁnish the E-step. 
Then we ﬁnd the coordinations of the max value of every row in ModifyingAbove bold italic xi With bold caret Superscript lˆξ
l. For instance, 
if ModifyingAbove xi With caret Subscript j k Superscript lˆξl
jk is the maximum one in j-th row, the j-th ﬁlter will be categorized into the 
k-th distribution cluster, as well as the corresponding BN parameter. Thus, we ﬁnish 
the clustering process by the E-step. Now the ﬁlters with similar distributions have 
been sorted out; we can deﬁne the forward and update the rule of ﬁlters. Take the 
k-th cluster, including the j-th ﬁlter at l-th layer, for example. We deﬁne the forward 
function as: 
x Subscript k Superscript l Super Superscript asterisk Superscript Baseline equals StartFraction 1 Over StartAbsoluteValue script upper D Subscript k Superscript l Baseline EndAbsoluteValue EndFraction sigma summation Underscript upper F Subscript j Superscript l Baseline element of script upper D Subscript k Superscript l Baseline Endscripts StartFraction sigma summation Underscript i equals 1 Overscript upper C Superscript l minus 1 Baseline Endscripts x Subscript i Superscript l minus 1 Super Superscript asterisk Superscript Baseline asterisk upper F Subscript i comma j Superscript l Baseline minus pi Subscript j Superscript l Baseline Over tau Subscript j Superscript l Baseline EndFraction plus beta Subscript j Superscript l Baseline periodxl∗
k =
1
|Dl
k|
Σ
F l
j ∈Dl
k
ΣCl−1
i=1 xl−1∗
i
∗F l
i,j −πl
j
τ l
j
+ βl
j.
(5.27)

5.4
Network Pruning
151
Equation 5.27 represents the procedure in that we average the output of parameters 
in the same cluster. x Subscript k Superscript l Super Superscript asteriskxl∗
k represents the averaged output feature map of k-th cluster. 
Due to the distribution of ﬁlters in the same cluster being similar and the linearity 
of convolution, the outputs should also be similar. Consequently, the average 
conduction will not dramatically inﬂuence the network accuracy. In this way, we 
have formulated the forward propagation of the network successfully. 
Maximization step We formulate the optimization problem as: 
StartLayout 1st Row bold italic upper Theta Superscript l comma e plus 1 Baseline equals arg max Underscript bold italic upper Theta Superscript l Baseline Endscripts script upper Q left parenthesis bold italic upper Theta Superscript l Baseline comma bold italic upper Theta Superscript l comma e Baseline right parenthesis EndLayoutOl,e+1 = arg max
Ol Q(Ol, Ol,e)
(5.28) 
As mentioned above, upper Theta Subscript k Superscript lOl
k represents left parenthesis mu Subscript k Superscript l Baseline comma upper Sigma Subscript k Superscript l Baseline right parenthesis(μl
k, Σl
k), and we can compute mu Subscript k Superscript l Baseline comma upper Sigma Subscript k Superscript lμl
k, Σl
k and alpha Subscript k Superscript lαl
k
by enforcing their corresponding partial derivatives toward script upper QQ to 0. Then we have: 
StartLayout 1st Row 1st Column ModifyingAbove mu With caret Subscript k Superscript l comma e plus 1 2nd Column equals StartFraction sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts ModifyingAbove xi With caret Subscript j k Superscript l comma e Baseline upper F Subscript j Superscript l comma e Baseline Over sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts ModifyingAbove xi With caret Subscript j k Superscript l comma e Baseline EndFraction EndLayout StartLayout 2nd Row 1st Column ModifyingAbove upper Sigma With caret Subscript k Superscript l comma e plus 1 2nd Column equals StartFraction sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts ModifyingAbove xi With caret Subscript j k Superscript l comma e Baseline left parenthesis upper F Subscript j Superscript l comma e Baseline minus mu Subscript k Superscript l comma e Baseline right parenthesis Superscript upper T Baseline left parenthesis upper F Subscript j Superscript l comma e Baseline minus mu Subscript k Superscript l comma e Baseline right parenthesis Over sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts ModifyingAbove xi With caret Subscript j k Superscript l comma e Baseline EndFraction EndLayout StartLayout 3rd Row 1st Column ModifyingAbove alpha With caret Subscript k Superscript l comma e 2nd Column equals StartFraction sigma summation Underscript j equals 1 Overscript upper C Superscript l Baseline Endscripts ModifyingAbove xi With caret Subscript j k Superscript l comma e Baseline Over upper C Superscript l Baseline EndFraction EndLayout ˆμl,e+1
k
=
ΣCl
j=1 ˆξl,e
jk F l,e
j
ΣCl
j=1 ˆξl,e
jk
Alt-text already available with first equation in this block
(5.29) 
ˆΣl,e+1 
k 
=
ΣCl 
j=1 ˆξ l,e 
jk (F l,e 
j − μ l,e 
k )T (F l,e 
j − μ l,e 
k )
ΣCl 
j=1 ˆξ l,e 
jk 
Alt-text already available with first equation in this block
(5.30) 
ˆα l,e 
k =
ΣCl 
j=1 ˆξ l,e 
jk 
Cl
(5.31) 
To supervise ﬁlters in the same cluster to converge to the same distribution, the 
loss function of i-th layer during e-th epoch can be formulated as: 
script upper L Subscript upper D Sub Subscript mu Subscript Superscript l comma e Baseline equals sigma summation Underscript k equals 1 Overscript upper K Superscript l Baseline Endscripts StartFraction ModifyingAbove alpha With caret Subscript k Superscript l comma e Baseline Over StartAbsoluteValue script upper D Subscript k Superscript l Baseline EndAbsoluteValue EndFraction sigma summation Underscript upper F Subscript j Superscript l Baseline element of script upper D Subscript k Superscript l Baseline Endscripts parallel to upper F Subscript j Superscript l Baseline minus ModifyingAbove mu With caret Subscript k Superscript l comma e Baseline parallel to Subscript 2 Superscript 2 Baseline commaLl,e
Dμ =
Kl
Σ
k=1
ˆαl,e
k
|Dl
k|
Σ
F l
j ∈Dl
k
||F l
j −ˆμl,e
k ||2
2,
(5.32) 
script upper L Subscript upper D Sub Subscript upper Sigma Subscript Superscript l comma e Baseline equals sigma summation Underscript k equals 1 Overscript upper K Superscript l Baseline Endscripts StartFraction ModifyingAbove alpha With caret Subscript k Superscript l comma e Baseline Over StartAbsoluteValue script upper D Subscript k Superscript l Baseline EndAbsoluteValue EndFraction sigma summation Underscript upper F Subscript j Superscript l Baseline element of script upper D Subscript k Superscript l Baseline Endscripts parallel to left parenthesis upper F Subscript j Superscript l Baseline minus ModifyingAbove mu With caret Subscript k Superscript l comma e Baseline right parenthesis Superscript upper T Baseline left parenthesis upper F Subscript j Superscript l Baseline minus ModifyingAbove mu With caret Subscript k Superscript l comma e Baseline right parenthesis minus ModifyingAbove upper Sigma With caret Subscript k Superscript l comma e Baseline parallel to Subscript 2 Superscript 2 Baseline periodLl,e
DΣ =
Kl
Σ
k=1
ˆαl,e
k
|Dl
k|
Σ
F l
j ∈Dl
k
||(F l
j −ˆμl,e
k )T (F l
j −ˆμl,e
k ) −ˆΣl,e
k ||2
2.
(5.33) 
In Eqs. 5.32 and 5.33, we formulate the supervision to constrain the constraint of 
ﬁlters. We denote script upper L Subscript upper D Superscript l comma eLl,e
D as script upper L Subscript upper D Superscript lLl
D for simpliﬁcation. Likewise, the script upper L Subscript upper DLD of BN parameters 
can be solved according to single-dimensional GMM. Hence, we can formulate script upper L Subscript upper DLD
as: 
script upper L Subscript upper D Baseline equals sigma summation Underscript l equals 1 Overscript upper L Endscripts script upper L Subscript upper D Sub Subscript mu Subscript Superscript l Baseline plus script upper L Subscript upper D Sub Subscript upper Sigma Subscript Superscript l Baseline commaLD =
L
Σ
l=1
Ll
Dμ + Ll
DΣ,
(5.34)

152
5
Network Pruning
where L denotes the number of total layers of the network. The total loss supervising 
the parameters is formulated as: 
StartLayout 1st Row script upper L equals script upper L Subscript upper S Baseline plus lamda script upper L Subscript upper D Baseline period EndLayoutL = LS + λLD.
(5.35) 
In Eq. 5.35, the  loss script upper LL consists of script upper L Subscript upper SLS and script upper L Subscript upper DLD, i.e., cross-entropy and the distribution 
loss deﬁned above. 
5.4.2.3 
Optimization and Analysis 
We use the stochastic gradient descent (SGD) to optimize the upper F Subscript j Superscript lF l
j. Since we average 
some parameter output, the gradient derived from script upper L Subscript upper SLS should satisfy the chain rule. 
In contrast, the script upper L Subscript upper DLD comprises the parameters themselves, and the gradient can be 
computed straightforwardly. Hence, we formulate the optimization of the ﬁlter in 
k-th cluster as: 
upper F Subscript j Superscript l comma t plus 1 Baseline left arrow upper F Subscript j Superscript l comma t Baseline minus eta upper Delta upper F Subscript j Superscript l comma t Baseline commaF l,t+1
j
←F l,t
j
−ηAF l,t
j ,
(5.36) 
where etaη is the learning rate. Then the gradient upper Delta upper F Subscript j Superscript l comma tAF l,t
j
can be solved as: 
StartLayout 1st Row 1st Column upper Delta upper F Subscript j Superscript l comma t Baseline equals StartFraction partial differential script upper L Over partial differential upper F Subscript j Superscript l Baseline EndFraction equals 2nd Column StartFraction partial differential script upper L Subscript upper S Baseline Over partial differential upper F Subscript j Superscript l Baseline EndFraction plus lamda StartFraction partial differential script upper L Subscript upper D Baseline Over partial differential upper F Subscript j Superscript l Baseline EndFraction 2nd Row 1st Column equals 2nd Column sigma summation Underscript k equals 1 Overscript upper K Superscript l Baseline Endscripts StartFraction ModifyingAbove xi Subscript j k Superscript l Baseline With caret Over StartAbsoluteValue script upper D Subscript k Superscript l Baseline EndAbsoluteValue EndFraction dot StartFraction partial differential script upper L Subscript upper S Baseline Over partial differential x Subscript k Superscript l Super Superscript asterisk Superscript Baseline EndFraction dot StartFraction partial differential x Subscript j Superscript l Baseline Over partial differential upper F Subscript j Superscript l Baseline EndFraction plus lamda left parenthesis StartFraction partial differential script upper L Subscript upper D Sub Subscript mu Subscript Superscript l Baseline Over partial differential upper F Subscript j Superscript l Baseline EndFraction plus delta dot lamda StartFraction partial differential script upper L Subscript upper D Sub Subscript sigma Subscript Superscript l Baseline Over partial differential upper F Subscript j Superscript l Baseline EndFraction right parenthesis period EndLayoutAF l,t
j
= ∂L
∂F l
j
=∂LS
∂F l
j
+ λ∂LD
∂F l
j
=
Kl
Σ
k=1
ˆ
ξl
jk
|Dl
k| · ∂LS
∂xl∗
k
·
∂xl
j
∂F l
j
+ λ
(∂Ll
Dμ
∂F l
j
+ δ · λ
∂Ll
Dσ
∂F l
j
)
.
(5.37) 
Based on Eq. 5.37, all the gradients become solvable. And the optimization of upper F Subscript j Superscript lF l
j
becomes easy. Likewise, the parameter of BN can be solved similarly. Then the 
general train algorithm is shown in Algorithm 1. 
5.4.2.4 
Filter Modiﬁcation 
After training, the well-trained bold script upper WW is outputted. The ﬁlters of k-th cluster follow the 
same distribution, so we can leave the ﬁrst one and trim others. Then we use upper F Subscript k Superscript lF l
k to 
denote the left one. So the l-th layer parameter set after ﬁlter pruning is: 
StartLayout 1st Row 1st Column Blank 2nd Column bold italic upper F Superscript l Baseline equals StartSet upper F Subscript k Superscript l Baseline element of script upper D Subscript k Superscript l Baseline vertical bar 1 less than or equals k less than or equals upper K Superscript l Baseline EndSet EndLayoutF l =
{
F l
k ∈Dl
k|1 ≤k ≤Kl}
(5.38)

5.4
Network Pruning
153
Algorithm 9: Algorithm of FEM-BP 
An algo rithm for  F E M-B  
P
. T
he  i n pu t a
n
d
 out put data are depic ted. 
5.22 An algorithm for F E M-B P. The input and output data are depicted. 5.26An
 algorit hm f or F  E M-B P. The  input and output data are depicted. 5.27 An algorithm for F E M-B P. The input and output data are depicted. 5.31An
 alg ori
thm for  F E M-B  P.
 The inp ut a nd output  data  ar e
 
dep
icte d .  
5.32 An algorithm for F E M-B P. The input and output data are depicted. 5.35An
 algori t hm fo r F E M-B P. The input and output data are depicted. 5.36 An algorithm for F E M-B P. The input and output data are depicted. 5.37An
 alg orit
hm f or F E M-B  P. The inp ut a nd outpu t data a re
 depicte d. 
As the output dimension of the l-th layer drops to upper K Superscript l Baseline equals less than upper C Superscript l Baseline times left parenthesis 1 minus script upper E right parenthesis greater thanKl =< Cl × (1 −E) > after 
pruning, the input dimension left parenthesis l plus 1 right parenthesis(l + 1)-th layer changes correspondingly. In the same 
way, the input dimension of l-th layer changes to upper K Superscript l minus 1Kl−1. Hence, we sum the kernels 
corresponding to the same input to make the dimension identical, as illustrated 
in Fig. 5.5c. Then k-th ﬁlter in l-th layer changes to upper F Subscript k Superscript l Super Superscript asterisk Baseline element of double struck upper R Superscript k Super Superscript l Superscript times w Super Superscript l Superscript times upper K Super Superscript l minus 1F l∗
k
∈Rkl×wl×Kl−1, and BN 
parameters obey the existence of their ﬁlters. 
5.4.2.5 
Experiments 
Models and datasets We evaluated our SPEM method by conducting compre-
hensive experiments using eight convolutional neural networks on two datasets, 
i.e., ResNet-20/ResNet-32/ResNet-56 and VGGNet on CIFAR-10 [34] and ResNet-
18/ResNet-34/ResNet-50 [19] VGGNet on ImageNet ILSVRC2012 [35]. CIFAR-
10 is a dataset consisting of 50,000 training images and 10,000 test images with 
a size of  32 times 3232 × 32 from 10 classes. And ImageNet ILSVRC2012 is the large-scale 
dataset with 1.28M training images and 50,000 validation images with a size of 
224 times 224224 × 224 from 1000 classes. 
Implementations We implement the ResNets for experiments conducted on 
CIFAR-10 and according to [19] for experiments conducted on ImageNet 
ILSVRC2012. We implemented our training process on 1 NVIDIA 2080TI GPU 
with 11 GB and 128G RAM for experiments conducted on CIFAR-10. And for 
experiments conducted on ImageNet ILSVRC2012, we implemented our training 
process on 3 NVIDIA TITANV GPUs with 12 GB and 96 GB RAM. The weight 
decay is set as 1 × 10−4, and momentum is 0.9. The hyperparameter λ is set as 
1 × 10−4 and 5 × 10−6 for experiments on CIFAR-10 and ImageNet ILSVRC2012,

154
5
Network Pruning
respectively. P.S., we use SPEM-E to present the setting of the pruning rate in 
experiments. 
We evaluate SPEM’s performance on CIFAR-10 using ResNet-20/ResNet-
32/ResNet-56 and VGGNet. The initial learning rate η is set to 0.1, and the learning 
strategy is to scale the η by a factor of 0.1 at the 100-th and 150-th epoch. The 
total number of epochs is 200. And batch size is 128. Table 5.9 shows the accuracy 
of the base and pruned model and their absolute disparity. FLOPs of the pruned 
model and the pruning rate of FLOPs are also shown. We only display the results 
with ﬁlter pruning rate set as 30% and 40% in Table 5.9, which can achieve about 
1 − (1 − 30%)2 = 51% and 1 − (1 − 40%)2 = 64% FLOPs pruning rate. Prior 
works mostly prune the FLOPs at a ratio from 30% to 65%. So we show these two 
kinds of results for fair comparison. More results with a ﬁlter pruning rate set from 
10% to 90% will be displayed in efﬁciency analysis. 
ResNets As  shown in Table  5.9, we present the experimental results with hyperpa-
rameter E set as 30% and 40%. Hence, the corresponding architecture after pruning 
is 11-22-45 and 10-19-38, respectively. Our SPEM achieved the state-of-the-art 
trade-off between acceleration and accuracy. For example, FPEM-40% achieves 
an 11.8% higher pruning rate on ResNet-20 compared to FPGM with ﬁne-tuning, 
with only 0.08% Top-1 accuracy lower. Likewise, FPEM-30% achieves only a 1.5% 
lower pruning rate on ResNet-20 compared to FPGM while having 0.36% Top-1 
accuracy higher. To conclude, SPEM achieves a better trade-off between accuracy 
and acceleration than FPGM. Similarly, SPEM far outperforms MIL and SFP. 
On ResNet-32, situations are similar to the ones on ResNet-20. Compared to 
FPGM, SPEM-30% can achieve a comparable pruning rate with 0.19% accuracy 
higher. And SPEM-40% achieves 1.05% and 1.71% accuracy higher than MIL/SFP 
with muck higher pruning rate. In conclusion, SPEM advances the state of the art 
on pruning ResNet-32. 
On ResNet-56, we observe three phenomena: (1) FPEM-30% outperforms all 
the listed work with the lower pruning rate on Top-1 accuracy. (2) FPEM-40% 
outperforms C-SGD with a higher accuracy by 0.13% and higher pruning rate by 
13.3%. (3) FPEM achieves a higher Top-1 accuracy than the baseline as well as 
accelerating the base ResNet-56 by a factor of 2.11× and 2.83×, which indicates 
that SPEM can achieve an architecture with fewer redundancy as well as higher 
capability to extracting features. 
VGGNet We further validate our SPEM on the single-branch network such as 
VGGNet. Results are listed at the bottom of Table 5.9. As our work can prune 
without ﬁne-tuning for the particular pruning method based on consistency, we 
selected the ﬁne-tuned results of prior works for a fair comparison. Compared 
to PFEC, GAL, and FPGM, SPEM-30% can achieve a higher pruning rate with 
the Top-1 accuracy 1.01%, 0.99%, and 0.41% higher. Moreover, SPEM-40% can 
achieve 63.9% pruning rate, i.e., 2.77× acceleration, with only 0.05% accuracy 
drop, which is the best pruning result.

5.4
Network Pruning
155
Table 5.9 Results on CIFAR-10. The “Acc. ↓” is the accuracy drop between the pruned model and 
the baseline model; the smaller, the better. And the “FLOPs ↓” is the rate describing how much the 
pruned FLOPs count in the baseline model; the higher, the better 
Model
Method
Base Top-1% Pruned Top-1% Top-1 Acc. ↓% FLOPs FLOPs ↓ % 
ResNet-20 MIL [9]
–
91.68
–
2.60E7 20.3 
ResNet-20 SFP [21]
92.20
90.83
1.37
2.43E7 42.2 
ResNet-20 FPGM [22] 92.20
91.99
0.21
1.87E7 54.0 
ResNet-20 SPEM-30% 92.17
92.35
−0.18
1.92E7 52.5 
ResNet-20 SPEM-40% 92.17
91.91
0.26
1.38E7 65.8 
ResNet-32 MIL [9]
92.33
90.74
1.59
4.70E7 31.2 
ResNet-32 SFP [21]
92.63
92.08
0.55
4.03E7 41.5 
ResNet-32 FPGM [22] 92.63
92.82
−0.19
3.23E7 53.2 
ResNet-32 SPEM-30% 92.63
93.03
−0.43
3.26E7 52.6 
ResNet-32 SPEM-40% 92.63
92.79
−0.13
2.35E7 65.9 
ResNet-56 PFEC [40] 
93.04
93.06
−0.02
9.09E7 27.6 
ResNet-56 CP [23]
92.80
91.80
1.00
–
50.0 
ResNet-56 SFP [21]
93.59
92.26
1.33
5.94E7 52.6 
ResNet-56 FPGM [22] 93.59
93.49
0.10
5.94E7 52.6 
ResNet-56 C-SGD [8] 
93.39
93.44
−0.05
4.91E7 60.9 
ResNet-56 SPEM-30% 93.26
93.76
−0.50
5.94E7 52.7 
ResNet-56 SPEM-40% 93.26
93.57
−0.31
4.28E7 65.9 
VGGNet 
PFEC [40] 
93.58
93.40
0.56
2.16E8 34.3 
VGGNet 
GAL [43] 
93.96
93.42
0.54
1.80E8 45.2 
VGGNet 
FPGM [22] 93.58
94.00
−0.42
–
34.2 
VGGNet 
SPEM-30% 93.96
94.41
−0.45
1.54E8 50.9 
VGGNet 
SPEM-40% 93.96
93.91
0.05
1.13E8 63.9 
SPEM is further evaluated on ImageNet ILSVRC2012 with ResNet-18/ResNet-
34/ResNet-50 and VGGNet. We train the pruning network for 100 epochs with an 
initial learning rate η set as 0.1 and scaled by a factor of 0.1 at the 40-th, 60-th, 
and 80-th epoch. We implement the process on three GPUs with a mini-batch size 
of 128 for ResNet-18/ResNet-34 and VGGNet and 64 for ResNet-50, respectively. 
We plan to prune about 40%–60% FLOPs for models implemented on ImageNet 
ILSVRC2012. Hence, we will set E as 30% and 40$ for comparison. 
ResNets As shown in Table 5.10, SPEM outperforms the prior works on ILSVRC-
2012 dataset again. On ResNet-18, SPEM achieves the higher inference speedup, 
but its Top-1 accuracy exceeds by 1.99% and 1.22%, respectively. Compared to 
FPGM, SPEM-40% achieves higher accuracy by 21.1% with only an accuracy lower 
by 0.09%.

156
5
Network Pruning
Table 5.10 Result on ImageNet ILSVRC2012 
Model
Method 
Base 
Top-1% 
Pruned 
Top-1% 
Top-1 
Acc. ↓% 
Base 
Top-5% 
Pruned 
Top-5% 
Top-5 
Acc. ↓% 
FLOPs 
↓ % 
ResNet-18
MIL [9]
69.98
66.33
3.65
89.24
86.94
2.30
34.6 
ResNet-18
SFP [21]
70.28
67.10
3.18
89.63
87.78
1.85
41.8 
ResNet-18
FPGM [22]
70.28
68.41
1.87
89.63
88.48
1.15
41.8 
ResNet-18
SPEM-40%
69.70
68.32
1.58
89.39
88.16
1.22
62.9 
ResNet-34
SFP[21]
73.92
71.83
2.09
91.62
90.33
1.29
41.1 
ResNet-34
FPGM[22]
73.92
72.63
1.29
91.62
91.08
0.54
41.1 
ResNet-34
SPEM-40%
73.92
91.62 
ResNet-50
ThiNet[46]
72.88
72.04
0.84
91.14
90.67
0.47
36.8 
ResNet-50
GDP[42]
75.13
71.89
1.39
92.30
90.71
1.59
51.3 
ResNet-50
SFP[21]
76.15
74.61
1.54
92.87
92.06
0.81
41.8 
ResNet-50
FPGM[22]
76.15
74.83
1.32
92.87
92.32
0.55
53.5 
ResNet-50
C-SGD[8]
75.33
74.93
0.40
92.56
90.27
0.29
46.2 
ResNet-50
MetaPruning[8]
76.60
75.40
1.20
-
-
-
51.2 
ResNet-50
SPEM-30%
75.22
74.96
0.26
92.41
91.98
0.43
50.2 
VGGNet
SFP[21]
73.92
71.83
2.09
91.62
90.33
1.29
41.1 
VGGNet
FPGM[22]
73.92
72.63
1.29
91.62
91.08
0.54
41.1 
VGGNet
SPEM-40%
73.92
91.62

5.4
Network Pruning
157
5.4.2.6 
Efﬁciency Analysis 
Model Sparsity with Pruning Rate Inspired by the linearity of convolution, the 
amount of feature information extracted by convolutions relies much on the sparsity 
of ﬁlters. Hence, we deﬁne the sparsity of the ﬁlter upper F Subscript j Superscript lF l
j as parallel to upper F Subscript j Superscript l Baseline minus upper F Subscript upper G upper M Superscript l Baseline parallel to Subscript 2||F l
j −F l
GM||2, where 
upper F Subscript upper G upper M Superscript lF l
GM is the geometric median of ﬁlters at l-th layer as deﬁned in [22]. We plot the 
layer-wise sparsity of ResNet-20, ResNet-32, ResNet-56, and VGGNet in Fig. 5.6. 
For example, we can observe that the sparsity of pruned ResNet-32 with a pruning 
rate set from 10 percent sign10% to 40 percent sign40% can achieve comparable or higher layer-wise sparsity than 
baseline ResNet-32, which can subtly clarify the reason why the pruned ResNet-32 
via SPEM can achieve even higher accuracy as well as satisfying acceleration rate 
compared to the baseline. In addition, we analyze the source of sparsity. It is derived 
from the strong constraint set in Eq. 5.27. For the outputs of ﬁlters in one cluster to 
be averaged, one cluster can only extract the same amount of features as just one 
ﬁlter. Then the gradient derived from cross-entropy forces the cluster to optimize 
for a sparser distribution. Hence, our main motivation can be proved theoretically 
and experimentally. 
Another phenomenon that should be paid attention to is that the deeper network 
gains lower sparsity. Hence, the deeper network can accept the higher pruning 
rate while maintaining accuracy. For instance, pruning 40 percent sign40% ﬁlters on ResNet-
20 and ResNet-32 will achieve 0.26 percent sign0.26% and negative 0.13 percent sign−0.13% accuracy drop. This subtly 
demonstrates our viewpoint. Moreover, based on this viewpoint, we can further 
consider pruning the blocks and ﬁltering for deep redundant networks such as 
ResNets and MoblieNets. 
Joint Pruning Cooperated with Block Pruning Inspired by the GAL [43] and 
the model sparsity analysis, we conduct some additional experiments on pruning 
cooperated with block pruning set as [43] to analyze the efﬁciency on ResNet-56. 
As illustrated in Table 5.11, SPEM can work jointly with the block pruning methods. 
In particular, training via SPEM can effectively substitute the ﬁne-tuning process. 
SPEM+GAL can achieve 3.75× and 4.69× theoretical acceleration rates. 
5.4.3 
Pruning Multi-view Stereo Net for Efﬁcient 3D 
Reconstruction 
Recent improvements in compatibility enable a fast 3D reconstruction with better 
accuracy and completeness, which have a wide range of applications, ranging from 
mapping, photogrammetry, autonomous driving, and robot planning to augmented 
reality and virtual reality, among many other scenarios [31, 48, 69]. To model a 
3D space, depth information has to be inferred when reconstructing 3D scenarios. 
The most common approaches for depth inference are based on cameras with depth 
sensors such as Kinect, which restricts the accessibility for the outdoor environment.

158
5
Network Pruning
(a)
(b) 
(c)
(d) 
Fig. 5.6 Layer-wise sparsity analysis on various models and accuracy analysis. (a) Sparsity  
analysis on ResNet-20. (b) Sparsity analysis on ResNet-32. (c) Sparsity analysis on ResNet-56. 
(d) Accuracy of pruned model 
Table 5.11 Comparison on GAL, SPEM, and SPEM+GAL. Accele. Denotes the theoretical 
acceleration rate 
Method
Base Top-1% Pruned Top-1% Top-1 Acc. ↓% FLOPs FLOPs ↓ % Accele. 
SPEM-30% 
93.26
93.76
−0.50
5.94E7 52.7
2.09× 
GAL-0.6 [43] 93.26
93.38
−0.12
8.21E7 37.6
1.60× 
SPEM-
30%+GAL-
0.6 
[43] 
93.26
93.26
0.00
3.35E7 73.3
3.75× 
SPEM-40% 
93.26
93.57
−0.31
4.28E7 65.9
2.93× 
SPEM-
40%+GAL-
0.6 
[43] 
93.26
93.07
0.19
2.80E7 78.7
4.69×

5.4
Network Pruning
159
Also, such methods usually exert other forms of inﬂuence on the surface, which 
may cause other problems. In such a scenario, reconstructing from visible images is 
a practical choice. The stereo reconstruction problem was initially solved based on 
two visible images, while real-time multi-view processing could be more practical 
due to high computational cost [31, 48]. 
We are particularly interested in multi-view stereo (MVS). It is a more popular 
option in applications since more than two views improve accuracy and complete-
ness, especially in occlusion cases. In terms of application scenarios, if multiple 
views are from a single moving camera and the camera captures an object from 
different views, a slightly different scene, or a moving object over time, then the 
sequence of multi-view stereo settings can be used to solve the problems of 3D 
reconstruction [12], structure from motion [61, 65, 66], visual SLAM [38] and visual 
odometry [25], and so on. 
5.4.3.1 
Channel Pruning for 2D CNNs 
To speed up the reconstruction, we introduce PruMVS [67], which adds a soft mask 
to the 2D CNNs for feature extraction to prune the redundant channels and train a 
dense encoder-decoder structure to help prune the 3D CNNs. We ﬁrst focus on ﬁlter 
pruning (a.k.a., channel pruning) and aim to zero out some unnecessary ﬁlters by 
learning a soft mask. More speciﬁcally, we warp masks on each channel on the 2D 
part to mark the importance of the according to ﬁlter (Fig. 5.7b). The approach has 
several advantages. Firstly, such a technique could signiﬁcantly reduce the model’s 
size by exploiting the redundancy among ﬁlters since it has been observed that some 
ﬁlters are unimportant in a common CNN structure [43, 63]. Secondly, without 
customizing other structures in the network, pruning ﬁlters can handle many other 
similar MVS learning models with similar architectures. Lastly, we can produce 
desired redundancy by adding regularization terms on loss according to the mask 
and training the network. The value of the related mask could indicate the less 
important information. By eliminating the channel of the 2D network, the data fed 
into the 3D part would be signiﬁcantly reduced. 
5.4.3.2 
Optimization Based on a Mixed Back Propagation 
The soft mask removes the corresponding channels and ﬁlters for 2D CNNs. We 
deﬁne the weights in 2D CNNs as script upper WW, the soft mask as bold mm, and lamdaλ as the parameter 
controlling L1 regularization term and denote the loss function as script upper L left parenthesis script upper W comma bold m right parenthesisL(W, m), which 
will be detailed in the next section. The model parameters script upper WW, bold mm are learned by 
solving: 
arg min Underscript script upper W comma bold m Endscripts script upper L left parenthesis script upper W comma bold m right parenthesis plus lamda StartAbsoluteValue EndAbsoluteValue bold m StartAbsoluteValue EndAbsoluteValue Subscript 1 Baseline period arg min
W,m
L(W, m) + λ||m||1.
(5.39)

160
5
Network Pruning
Fig. 5.7 PruMVS overview. (a) is a general description of our network, (b) corresponds to the 
mask, and (c) corresponds to reﬁnement in (a). Detailed description: (a) The architecture of 
PruMVS. Reference and source images go through an eight-layer 2D CNN with a mask attached to 
the last layer to generate feature maps and sort out the redundant ﬁlters. Differentiable homography 
is used to warp the 2D images to 3D volumes and operate a variance-based algorithm to aggregate 
all the volumes into a single cost volume. The output depth map is generated from a 3D CNN 
similar to U-Net. (b) The illustration of channel pruning. The yellow channel in the ﬁgure denotes 
the redundant ﬁlter, of which the corresponding mask would be trained to zero. (c) Depth map 
reﬁnement 
Our pruning approach is simple yet principled [1, 2], and we just use bold mm as a soft 
mask added on each ﬁlter as: 
upper F Subscript h Superscript l plus 1 Baseline equals bold m Subscript g Baseline f left parenthesis sigma summation Underscript g Endscripts upper F Subscript g Superscript l Baseline asterisk script upper W Subscript h comma g Superscript l Baseline right parenthesis commaF l+1
h
= mgf (
Σ
g
F l
g ∗Wl
h,g),
(5.40) 
where upper F Subscript g Superscript lF l
g and upper F Subscript h Superscript l plus 1F l+1
h
are the hh-th input feature map and the gg-th output feature 
map at the ll-th layer. asterisk∗and f left parenthesis dot right parenthesisf (·) refer to the convolutional operator and activation, 
respectively. The mask bold mm can be learned end-to-end in the mixed backpropagation 
process, which will be detailed later. In particular, the fast iterative shrinkage-
thresholding algorithm (FISTA) [3, 43] is used to optimize bold mm, which leads to a 
sparse solution of the soft mask and is built based on the L1-norm minimization. 
Stochastic gradient descent (SGD) or RMSprop can be directly introduced to 
solve the optimization problem in Eq. 5.39. However, they are less efﬁcient in 
convergence, and by using RMSprop, we have observed non-convergence scaling 
factors in the soft mask bold mm. Also, most factors are of the same order of magnitude, 
i.e., which does not create enough sparsity in the soft mask layer. Therefore, we need 
a threshold to remove the corresponding structures whose scaling factors are lower 
than the threshold. By doing so, the accuracy of the pruned network is signiﬁcantly 
lower than the baseline. To solve this problem, we use the proximal operator,

5.4
Network Pruning
161
a.k.a., the proximal gradient optimization, where the SGD and the constraint are 
updated separately [1, 2]. We introduce FISTA into the model to effectively solve 
the optimization via two alternating steps updating script upper WW and bold mm. 
1. Fixing bold mm, we use RMSprop to update script upper WW by descending its gradient. 
2. Fixing script upper WW, then bold mm is updated by FISTA with the initialization of alpha Subscript left parenthesis 1 right parenthesis Baseline equals 1α(1) = 1: 
bold n Subscript left parenthesis k plus 1 right parenthesis Baseline equals bold m Subscript left parenthesis k right parenthesis Baseline plus StartFraction alpha Subscript left parenthesis k right parenthesis Baseline minus 1 Over alpha Subscript left parenthesis k plus 1 right parenthesis Baseline EndFraction left parenthesis bold m Subscript left parenthesis k right parenthesis Baseline minus bold m Subscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis comman(k+1) = m(k) + α(k) −1
α(k+1)
(
m(k) −m(k−1)
)
,
(5.41) 
alpha Subscript left parenthesis k plus 1 right parenthesis Baseline equals one half left parenthesis 1 plus StartRoot 1 plus 4 alpha Subscript left parenthesis k right parenthesis Superscript 2 Baseline EndRoot right parenthesis commaα(k+1) = 1
2
(
1 +
/
1 + 4α2
(k)
)
,
(5.42) 
bold m Subscript left parenthesis k plus 1 right parenthesis Baseline equals bold p r o x Subscript gamma Sub Subscript left parenthesis k plus 1 right parenthesis Subscript lamda StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue Sub Subscript 1 Subscript Baseline left parenthesis bold n Subscript left parenthesis k plus 1 right parenthesis Baseline minus gamma Subscript left parenthesis k plus 1 right parenthesis Baseline StartFraction partial differential script upper L left parenthesis dot comma bold n Subscript left parenthesis k plus 1 right parenthesis Baseline right parenthesis Over partial differential bold n Subscript left parenthesis k plus 1 right parenthesis Baseline EndFraction right parenthesis commam(k+1) = proxγ(k+1)λ||·||1
(
n(k+1) −γ(k+1)
∂L(·, n(k+1))
∂n(k+1)
)
,
(5.43) 
where gamma Subscript left parenthesis k plus 1 right parenthesisγ(k+1) is the learning rate at the iteration k plus 1k + 1 and bold p r o x Subscript gamma Sub Subscript left parenthesis k plus 1 right parenthesis Subscript lamda StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue Sub Subscript 1 Baseline left parenthesis bold x Subscript i Baseline right parenthesis equals s i g n left parenthesis bold x Subscript i Baseline right parenthesis ring left parenthesis StartAbsoluteValue bold x Subscript i Baseline EndAbsoluteValue minus gamma Subscript left parenthesis k plus 1 right parenthesis Baseline lamda right parenthesis Subscript plusproxγ(k+1)λ||·||1(xi) =
sign(xi) ◦(|xi| −γ(k+1)λ)+. We can learn desirable zero and sparsity in soft mask 
bold mm with an appropriate learning rate (Fig. 5.8). 
5.4.3.3 
3D CNN Pruning 
As mentioned in our motivation, 3D CNNs are vital to an MVS system. For 
a UNet-like architecture, simply pruning channels based on existing pruning 
techniques are impractical for 3D CNN architecture, considering skip connections, 
and deconvolutions prevent us from using the most regular pruning strategies. 
We have made several attempts to prune the 3D CNNs by adding soft masks, 
a frequently used method, yet the result could have been better. The many soft 
masks trained are of the same magnitude and don’t show a speciﬁc pattern, i.e., 
zeroing out a speciﬁc channel. When a threshold ﬁlters out channels, the resulting 
reconstruction quality slumps signiﬁcantly. This is likely caused by the fact that 
(1) the 3D channels in the 3D CNNs relate to each other closely and (2) the 3D 
CNNs are of the UNet-like architecture, which contains many skip connections 
and deconvolutions. The feature maps from the decoder are combined with those 
from the encoder sub-network via skip connections. Therefore, removing a speciﬁc 
feature map may lead to the information being lost in both the decoder and encoder. 
To address it, we propose a novel pruning technique by training a hierarchical 
architecture and simply adopting its more minor part in our proposed PruMVS 
model. As shown in the network architecture (see Fig. 5.9), we add nested and dense 
skip connections, which is similar to U-Net++ [76], a deeply supervised encoder-
decoder network where the encoder and decoder sub-networks are connected 
through a series of nested, dense skip pathways. When trained in deep supervision, 
the convolutional layers we add to the skip pathway can be pruned; thus, only level 
0 is left. By adding convolutional blocks in the skipping connections and training

162
5
Network Pruning
Fig. 5.8 Illustration of different levels in the 3D nested U-Net. The ﬁrst level consists of two 
convolutional operations, while the other levels are shown in the picture above. Every level has an 
output, and we pick everyone to evaluate the ﬁnal result, which helps pruning 
the extensive network altogether, the model integrates features of different levels, 
and at last, we prune all the upper levels out, which helps prevent the interference 
of the connections between levels. 
Every level could be regarded as a smaller version of the higher level and could 
generate its result. The re-designed skip pathways aim at reducing the semantic 
gap between the feature maps of the encoder and decoder sub-networks. The 
underlying hypothesis behind our architecture is that the model can more effectively 
capture ﬁne-grained details of the foreground objects when high-resolution feature 
maps from the encoder network are gradually enriched before fusion with the 
corresponding semantically rich feature maps from the decoder network. Moreover, 
the network would have a more accessible learning task when the feature maps from 
the decoder and encoder networks are semantically similar. Higher-level results 
could help lower-level weights to train better through back propagation, i.e., to 
improve gradient ﬂow through the skip connections. To help training converge more 
efﬁciently and prevent the deep model from gradients vanishing or exploding, we

5.4
Network Pruning
163
Fig. 5.9 A hierarchal architecture for 3D regularization. The 3D network shapes like a nested U-
Net and has four levels. It consists of an encoder and decoder whose sub-network is connected by 
skip pathways represented by arrows 
also adopt the deep supervision idea [37] by adding loss from different levels’ 
results. 
Now, we formulate the volume as follows: 
upper V Superscript i j Baseline equals StartLayout Enlarged left brace 1st Row 1st Column bold upper C bold 2 left parenthesis upper V Superscript i minus 1 comma j Baseline right parenthesis comma 2nd Column j equals 0 2nd Row 1st Column bold upper C bold 1 left parenthesis upper V Superscript i comma 0 Baseline right parenthesis comma 2nd Column j equals 1 3rd Row 1st Column sigma summation Underscript 0 Overscript j minus 1 Endscripts bold upper C bold 1 left parenthesis upper V Superscript i comma k Baseline right parenthesis plus bold upper D left parenthesis upper V Superscript i plus 1 comma j minus 1 Baseline right parenthesis comma 2nd Column j greater than 1 EndLayoutV ij =
⎧
⎪⎨
⎪⎩
C2(V i−1,j),
j = 0
C1(V i,0),
j = 1
Σj−1
0
C1(V i,k) + D(V i+1,j−1), j > 1
(5.44) 
upper V Superscript i jV ij denotes the volume at the location left parenthesis i comma j right parenthesis(i, j) where i indexes the de-convolutional 
layer along the encoder; j denotes the sequential volume at the same ith level. bold upper C bold 2 left parenthesis dot right parenthesisC2(·)
is a two-stride convolutional operation with a batch normalization to downsample 
the input, and bold upper C bold 1 left parenthesis dot right parenthesisC1(·) is a one-stride convolutional layer. bold upper D left parenthesis dot right parenthesisD(·) denotes an up-
sampling layer through a two-stride de-convolutional operation. Figure 5.9 shows 
the overview of the proposed architecture. To be more speciﬁc, volumes at level 
j equals 0j = 0 only receive an input from the previous downsampling convolutional layer, 
and volumes at level j equals 1j = 1 receive an input from a common convolutional layer. 
Volumes at level j greater than 1j > 1 receive two inputs from the neighboring two volumes from 
the j minus 1j −1 level, generating one from the convolutional layer and the other from the 
up-sampling de-convolutional operation. We obtain the output of j minus 1j −1 level volume 
by adding these two inputs.

164
5
Network Pruning
Our new investigation shows that training a hierarchical architecture improves 
the performance of a low-level network compared to simply training a low-level 
network. As a result, a lower-level network can obtain performance close to the 
whole net with a smaller model size. The completeness of the proposed network is 
better than other networks, mainly because the redundant parameters are pruned out, 
and only the nonredundant parameters remain. Due to the reduction of parameters, 
the model is not only more efﬁcient but also more generalizable. The result is shown 
in the Experiments section. 
5.4.3.4 
Loss Function 
Following MVSNet [69], we let p Subscript v a l i dpvalid denote the set that contains valid ground 
truth pixels, d left parenthesis p right parenthesisd(p) denote the ground truth depth value of pixel p, d Subscript i Baseline left parenthesis p right parenthesisdi(p) denote the 
initial depth estimation, and d Subscript r Baseline left parenthesis p right parenthesisdr(p) denote the reﬁned depth estimation, and then we 
deﬁne the loss function as: 
upper L o s s equals sigma summation Underscript p element of bold p Subscript v a l i d Baseline Overscript Endscripts ModifyingBelow double vertical bar d left parenthesis p right parenthesis minus ModifyingAbove d Subscript i Baseline With caret left parenthesis p right parenthesis double vertical bar With bottom brace Underscript upper L o s s Baseline 0 Endscripts plus sigma dot ModifyingBelow double vertical bar d left parenthesis p right parenthesis minus ModifyingAbove d Subscript r Baseline With caret left parenthesis p right parenthesis double vertical bar With bottom brace Underscript upper L o s s Baseline 1 EndscriptsLoss =
Σ
p∈pvalid
||d(p) −ˆdi(p)||
'
''
'
Loss0
+σ · ||d(p) −ˆdr(p)||
'
''
'
Loss1
(5.45) 
Here Loss0 came from the distance between the initial depth estimation and 
the ground truth depth, and Loss1 came from the distance between reﬁned depth 
estimation and the ground truth depth. sigmaσ leverages the two losses in the loss 
function. The total loss function will be modiﬁed in the following due to pruning. 
Taking advantage of the nested skip connections, all the semantic level creates 
feature maps of the same size. As we enabled deep supervision in the new 
architecture, results from different levels will all be considered in the loss function. 
Thus, we obtain a new loss function as follows: 
script upper L equals sigma summation Underscript l equals 0 Overscript 3 Endscripts sigma summation Underscript p element of bold p Subscript v a l i d Baseline Overscript Endscripts ModifyingBelow double vertical bar d left parenthesis p right parenthesis minus ModifyingAbove d Subscript i Superscript j Baseline With caret left parenthesis p right parenthesis double vertical bar Subscript l Baseline With bottom brace Underscript upper L o s s Baseline 0 Endscripts plus sigma dot ModifyingBelow double vertical bar d left parenthesis p right parenthesis minus ModifyingAbove d Subscript r Superscript j Baseline With caret left parenthesis p right parenthesis double vertical bar Subscript l Baseline With bottom brace Underscript upper L o s s Baseline 1 EndscriptsL =
3
Σ
l=0
Σ
p∈pvalid
||d(p) −ˆdj
i (p)||l
'
''
'
Loss0
+σ · ||d(p) −ˆdj
r (p)||l
'
''
'
Loss1
(5.46) 
The parameter sigmaσ is set to 1.01.0 in the experiment. l denotes the level of the network. 
The model can infer two modes through deep supervision: improved performance 
by taking the average of different level outputs and a pruned mode by adopting the 
single network level. 
5.4.3.5 
Implementation of 2D/3D MVS Net 
Feature extraction The input bold upper I Subscript iIi includes selected source images and a reference 
image. An eight-layer 2D CNN is applied, where the strides of layers 3 and 6 are 
set to 2 to divide the feature pyramids into three scales. Two convolutional layers

5.4
Network Pruning
165
are applied to extract the higher-level image representation within each scale. Each 
convolutional layer is followed by a rectiﬁed linear unit (ReLU) except for the last 
layer. Also, similar to common matching tasks, parameters are shared among all 
feature pyramids for efﬁciency. The outputs of the 2D CNNs are N 32-channel 
feature maps downsized by four in each dimension compared with input images. 
Compared with simply performing dense matching on original images, the extracted 
feature maps signiﬁcantly boost the reconstruction quality. 
Finally, we will obtain N feature maps according to N different views, each 
of which is the size of StartFraction upper H Over 4 EndFraction times StartFraction upper W Over 4 EndFraction times upper C H
4 × W
4 × C, where H and W is the height and width 
of the input image and C indicates the number of channels. It is noteworthy 
that though the image frame is downsized after feature extraction, the original 
neighboring information of each remaining pixel has already been encoded into 
the 32-channel pixel descriptor, which prevents dense matching from losing helpful 
context information. 
2D to 3D Next, a 3D volume is built from the extracted feature maps and input 
cameras. While previous works [32] divide the space using regular grids, for our task 
of depth map inference, we construct the cost volume upon the reference viewing 
frustum. All feature maps are warped into different frontal-parallel planes of the 
reference camera to form N feature volumes {Vi}N 
i=1. 
In the 3D vision, a homography matrix H is used to relate a plane from one 
camera view into another and is subject to the rotation and translation of both views. 
As captured by a perspective transformation, 3D points are mapped onto image 
planes using the transformation matrix as x(i) = K[R|T]X, where K represents the 
camera intrinsic, R is the rotation matrix, and T denotes the translation. Formally, 
let {Ki, Ri, Ti} be the camera parameters of image ith and ni be the principal axis of 
the reference camera, and the homography for the ith feature map and the reference 
feature map at depth d could be expressed as a 3 × 3 matrix  Hi(d) [69]: 
bold upper H Subscript i Baseline left parenthesis d right parenthesis equals bold upper K Subscript i Baseline dot bold upper R Subscript i Baseline dot left parenthesis bold upper I minus StartFraction left parenthesis bold t 1 minus bold t Subscript i Baseline right parenthesis dot bold n 1 Superscript upper T Baseline Over d EndFraction right parenthesis dot bold upper R 1 Superscript upper T Baseline dot bold upper K 1 Superscript upper THi(d) = Ki · Ri · (I −(t1 −ti) · nT
1
d
) · RT
1 · KT
1
(5.47) 
Without loss of generality, the homography for reference feature map F1 itself is 
a 3 × 3 identity matrix. The warping process is similar to that of the classical plane 
sweeping stereo [7], except that the differentiable bilinear interpolation is used to 
sample pixels from feature maps{Fi}N 
i=1 rather than images {Ii}N 
i=1. As the core step 
to bridge the 2D feature extraction and the 3D regularized networks, the warping 
operation is implemented differently, enabling end-to-end training. 
Cost volume As we obtain the feature volumes Vi(d) of multiple angles, including 
reference image and source images numbered i, the next step is to fuse the multiple 
features into one cost volume in the 3D space, representing the extent of which each 
point in the 3D space matches among different angles. Note that the size of each 
feature volume is W 
4 × H 
4 × D × C, where H, W, D, C are the input image height, 
width, depth sample number, and the channel number of the feature map. The cost 
volume should be of the same size to represent every point in the 3D space. We

166
5
Network Pruning
then adapt variance among all feature volumes and deﬁne the cost volume as such 
variance, i.e.: 
bold upper C equals bold upper V a r left parenthesis bold upper V right parenthesis equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis bold upper V Subscript bold i Baseline minus ModifyingAbove bold upper V With quotation dash right parenthesis squared Over upper N EndFractionC = Var(V) =
ΣN
i=1(Vi −V)2
N
(5.48) 
where N means the number of feature volumes, the same as the number of input 
images. As the squared deviation of a random variable from its mean, the variance 
measures the extent to which the value deviated from their mean, i.e., a point 
holding a relatively low variance shows that its value matches from various angles, 
explaining that the point in the 3D space is more probable to exist as a frontier in the 
real world since it also shows that various viewing rays should intersect at that point. 
On the other hand, a point having a high variance indicates that it is less probable to 
represent a point in the real-world 3D space. 
Regularization As shown in some other research, particularly researches involving 
learning approaches [27, 32, 33], the raw cost volume generated by directly 
matching between different angles could be noisy and thus need reﬁnement. Such 
a step is also known as cost volume regularization. It should consider smoothness 
constraints and depth information to reﬁne the cost-volume point representation. 
As described above, the cost volume C represents the matching extent of each 
3D point. Therefore, the regularization step takes in cost volume C, which contains 
complete information of the agreement on each point from various angles, reﬁnes 
it, and outputs the probability volume P, which indicates the probability distribution 
for the depth inference, i.e., the probability of each point as the boundary of the 3D 
object in the real world. 
Notably, such a process is usually accomplished by a 3D CNN regularization 
network, which we replaced with the multi-scale 3D CNNs in our proposed 
architecture, as shown in Fig. 5.7a. 
Reﬁnement While the depth map retrieved from the probability volume is a 
qualiﬁed output, the reconstruction boundaries may suffer from over-smoothing due 
to the large receptive ﬁeld involved in the regularization, similar to the problems 
in semantic segmentation and image matting. Since the reference image contains 
boundary cues, the initial depth map is reﬁned using the reference image as a 
supplement whose boundary information is complete, as shown in Fig. 5.7c. 
The initial depth map and the resized reference image are concatenated as a 4-
channel input. This is then passed through three 32-channel 2D convolutional layers 
and one 1-channel convolutional layer to learn the depth residual. The initial depth 
map is then added back to generate the reﬁned depth map. Also, to prevent bias at 
a particular depth scale, we pre-scale the initial depth magnitude to the range [0, 1] 
and convert it back after the reﬁnement. The last layer does not contain the BN layer 
and the ReLU unit to learn the negative residual. 
Filtering and depth map fusion The original depth maps are inaccurate, and those 
outliers must be ﬁltered out. Photometric and geometric consistency are the criteria

5.4
Network Pruning
167
we propose to ﬁlter out the outliers. Photometric consistency measures the matching 
quality, while geometric consistency measures the consistency of the maps among 
various views, and the pixels should be visible in at least three viewpoints. As 
is shown in the network, cost volume will go through a softmax layer to get a 
probability volume, and we ﬁlter out the pixel whose probability is lower than a 
threshold of 0.8. 
Referring to many other MVS approaches, we choose to operate a fusion step to 
aggregate every depth map, which use different reference images in various views 
to decrease the error of the reconstructed model. We use the fusion algorithm of 
Gipuma [11] in our model, which differs from [69], and obtain different results in 
our comparison. 
The mismatches mainly occur in untextured and shaded areas outside the 
camera’s viewing frustum. Many of these cases can be found because depth maps 
estimated from different viewpoints differ. To detect those mismatches, we again 
declare that each image, in turn, the reference view, converts its depth map into a 
dense set of 3D points and re-projects them onto each of the N − 1 other views, 
producing a 2D coordinate pi and a parallax ˆdi for each view. If ˆdi is equal to the 
relating di, which is kept in the depth map, the match is considered to be correct with 
a threshold fε which is based on the scale of the reconstructed ﬁgure. The depth is 
accepted if consistent in more than fε. The parameters that ﬁlter out some pixels are 
unreliable and must be set under a trade-off between accuracy and completeness. 
Different applications require different settings. In our work, we apply the same 
setting on every model to estimate their performance impartially. 
5.4.3.6 
Performance Comparison 
To show how our proposed PruMVS balance the reconstruction performance and the 
model size pretty well, we examine our approach on the evaluation set consisting of 
22 scenes and compare it with the other state-of-the-art methods, including not only 
the baseline MVSNet [69] but also Gipuma [11], Camp [6], Tola [60], SurfaceNet 
[32], and Furu [10] which is called PMVS in [69]. 
To ensure the methods are all evaluated similarly, we utilized the same depth 
fusion algorithm with the same settings on our network. For other models that 
reconstruct from grids and point clouds with only steps similar to depth fusion and 
ﬁltering, we experiment on the best settings they proposed with their techniques, 
which is fair to them. Notably, due to the variety of 3D reconstruction approaches 
(point cloud, depth map, grids, voxels), such post-processing and reﬁnement steps 
(fusion and ﬁltering) may vary signiﬁcantly. But we have controlled varying factors 
as much as possible in the evaluation. 
Table 5.12 compares the quantitative evaluations: PruMVS maintains good 
accuracy and completeness, which is comparable to the best ones. Even the smallest 
model outperforms most other approaches, and the 32-channel level 0 model 
obtained the best completeness. It is noticeable that the objective of our model is 
to accelerate the reconstruction, which is very useful for practical applications.

168
5
Network Pruning
Table 5.12 Comparison with 
other methods 
Methods
Error 
Comp.(err) 
Overall(err) 
Furu (PMVS)
0.613 
0.941
0.777 
Gipuma
0.283 
0.873
0.578 
Camp
0.834 
0.554
0.694 
Tola
0.342 
1.190
0.766 
Colmap
0.400 
0.664
0.532 
SurfaceNet
0.454 
1.354
0.904 
MVSNet
0.569 
0.609
0.589 
PruMVS level 0
0.495 
0.433
0.464 
PruMVS level 0 16C 
0.510 
0.451
0.481 
Fig. 5.10 Qualitative comparison to ground truth in DTU dataset and the reconstruction model 
generated by other networks 
Figure 5.10 summarizes the qualitative comparison against the ground truth and 
the baseline MVSNet and Furu, SurfaceNet, and Tola. 
5.4.4 
Cogradient Descent for Dependable Learning 
5.4.4.1 
Gradient Descent 
A basic bilinear optimization problem attempts to optimize the following objective 
function as: 
arg min Underscript bold upper A comma bold x Endscripts upper G left parenthesis bold upper A comma bold x right parenthesis equals parallel to bold b minus bold upper A bold x parallel to Subscript 2 Superscript 2 Baseline plus lamda parallel to bold x parallel to Subscript 1 Baseline plus upper R left parenthesis upper A right parenthesis comma arg min
A,x
G(A, x) = ||b −Ax||2
2 + λ||x||1 + R(A),
(5.49) 
where bold b element of double struck upper R Superscript upper M times 1b ∈RM×1 is an observation that can be characterized by bold upper A element of double struck upper R Superscript upper M times upper NA ∈RM×N and 
bold x element of double struck upper R Superscript upper N times 1x ∈RN×1. upper R left parenthesis dot right parenthesisR(·) represents the regularization, typically the script l 1l1 or script l 2l2 norm. parallel to bold b minus bold upper A bold x parallel to Subscript 2 Superscript 2||b−Ax||2
2
can be replaced by any function with the form bold upper A bold xAx. Bilinear models generally have 
one variable with a sparsity constraint such as script l 1l1 regularization with the aim of 
avoiding overﬁtting.

5.4
Network Pruning
169
Assuming bold upper AA and bold xx are independent, the conventional gradient descent method can 
be used to solve the bilinear optimization problem as: 
bold upper A Superscript t plus 1 Baseline equals bold upper A Superscript t Baseline plus eta 1 StartFraction partial differential upper G Over partial differential bold upper A EndFraction commaAt+1 = At + η1
∂G
∂A ,
(5.50) 
where 
left parenthesis StartFraction partial differential upper G Over partial differential bold upper A EndFraction right parenthesis Superscript upper T Baseline equals bold x Superscript t Baseline left parenthesis bold upper A bold x Superscript t Baseline minus bold b right parenthesis Superscript upper T Baseline equals bold x Superscript t Baseline ModifyingAbove upper G With caret left parenthesis bold upper A comma bold x right parenthesis period(∂G
∂A )T = xt(Axt −b)T = xt ˆG(A, x).
(5.51) 
The function ModifyingAbove upper G With caretˆG is deﬁned by considering the bilinear optimization problem as in 
Eq. 5.49, and we have: 
ModifyingAbove upper G With caret left parenthesis upper A comma x right parenthesis equals left parenthesis bold upper A bold x Superscript t Baseline minus bold b right parenthesis Superscript upper T Baseline period ˆG(A, x) = (Axt −b)T .
(5.52) 
Equation 5.51 shows that the gradient for bold upper AA tends to vanish, when bold xx approaches 
zero due to the sparsity regularization term StartAbsoluteValue EndAbsoluteValue x StartAbsoluteValue EndAbsoluteValue Subscript 1||x||1. Although it has a chance to 
be corrected in some tasks, more likely, the update will cause an asynchronous 
convergence. Note that for simplicity, the regularization term on A is not considered. 
Similarly, for bold xx, we have:  
bold x Superscript t plus 1 Baseline equals bold x Superscript t Baseline plus eta 2 StartFraction partial differential upper G Over partial differential bold x EndFraction periodxt+1 = xt + η2
∂G
∂x .
(5.53) 
eta 1η1 and eta 2η2 are the learning rates. The conventional gradient descent algorithm for 
bilinear models iteratively optimizes one variable while keeping the other ﬁxed. This 
unfortunately ignores the relationship of the two hidden variables in optimization. 
5.4.4.2 
Cogradient Descent for Dependable Learning 
We consider the problem from a new perspective such that bold upper AA and bold xx are coupled. 
Firstly, based on the chain rule [52] and its notations, we have: 
StartLayout 1st Row 1st Column ModifyingAbove x With caret Subscript j Superscript t plus 1 2nd Column equals x Subscript j Superscript t Baseline plus eta 2 left parenthesis StartFraction partial differential upper G Over partial differential x Subscript j Baseline EndFraction plus upper T r left parenthesis left parenthesis StartFraction partial differential upper G Over partial differential bold upper A EndFraction right parenthesis Superscript upper T Baseline StartFraction partial differential bold upper A Over partial differential x Subscript j Baseline EndFraction right parenthesis right parenthesis comma EndLayout ˆxt+1
j
= xt
j + η2
(
∂G
∂xj
+ T r
((∂G
∂A
)T ∂A
∂xj
))
,
(5.54) 
where left parenthesis StartFraction partial differential upper G Over partial differential bold upper A EndFraction right parenthesis Superscript upper T Baseline equals bold x Superscript t Baseline ModifyingAbove upper G With caret left parenthesis bold upper A comma bold x right parenthesis( ∂G
∂A )T = xt ˆG(A, x) as shown in Eq. 5.51. upper T r left parenthesis dot right parenthesisT r(·) represents the trace of 
the matrix, which means that each element in the matrix StartFraction partial differential upper G Over partial differential x Subscript j Baseline EndFraction ∂G
∂xj adds the trace of the 
corresponding matrix related to x Subscript jxj. Considering: 
StartFraction partial differential upper G Over partial differential bold upper A EndFraction equals bold upper A bold x Superscript t Baseline bold x Superscript upper T comma t Baseline minus b bold x Superscript upper T comma t Baseline comma∂G
∂A = AxtxT,t −bxT,t,
(5.55)

170
5
Network Pruning
we have: 
StartLayout 1st Row 1st Column StartFraction partial differential upper G left parenthesis bold upper A right parenthesis Over partial differential x Subscript j Baseline EndFraction 2nd Column equals upper T r left bracket left parenthesis bold upper A bold x Superscript t Baseline bold x Superscript upper T comma t Baseline minus b bold x Superscript upper T comma t Baseline right parenthesis Superscript upper T Baseline StartFraction partial differential bold upper A Over partial differential x Subscript j Baseline EndFraction right bracket 2nd Row 1st Column Blank 2nd Column equals upper T r left bracket left parenthesis left parenthesis bold upper A bold x Superscript t Baseline minus b right parenthesis bold x Superscript upper T comma t Baseline right parenthesis Superscript upper T Baseline right bracket StartFraction partial differential bold upper A Over partial differential x Subscript j Baseline EndFraction 3rd Row 1st Column Blank 2nd Column equals upper T r left bracket bold x Superscript t Baseline ModifyingAbove upper G With caret StartFraction partial differential bold upper A Over partial differential x Subscript j Baseline EndFraction right bracket comma EndLayout
∂G(A)
∂xj
= T r[(AxtxT,t −bxT,t)T ∂A
∂xj
]
= T r[((Axt −b)xT,t)T ] ∂A
∂xj
= T r[xt ˆG ∂A
∂xj
],
(5.56) 
where ModifyingAbove upper G With caret equals left parenthesis bold upper A bold x Superscript t Baseline minus b right parenthesis Superscript upper T Baseline equals left bracket ModifyingAbove g With caret Subscript 1 Baseline comma ellipsis comma ModifyingAbove g With caret Subscript upper M Baseline right bracket ˆG = (Axt −b)T = [ ˆg1, . . . , ˆgM]. Supposing that bold upper A Subscript iAi and x Subscript jxj are independent 
when i not equals ji /= j, we have:  
StartFraction partial differential bold upper A Over partial differential x Subscript j Baseline EndFraction equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column ellipsis 3rd Column StartFraction partial differential bold upper A Subscript 1 j Baseline Over partial differential x Subscript j Baseline EndFraction 4th Column ellipsis 5th Column 0 2nd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 3rd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 4th Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 5th Row 1st Column 0 2nd Column ellipsis 3rd Column StartFraction partial differential bold upper A Subscript upper M j Baseline Over partial differential x Subscript j Baseline EndFraction 4th Column ellipsis 5th Column 0 EndMatrix comma ∂A
∂xj
=
⎡
⎢⎢⎢⎢⎢⎢⎣
0 . . . ∂A1j
∂xj
. . . 0
.
.
.
.
.
.
.
.
.
0 . . . ∂AMj
∂xj
. . . 0
⎤
⎥⎥⎥⎥⎥⎥⎦
,
(5.57) 
and: 
bold x ModifyingAbove upper G With caret equals Start 5 By 5 Matrix 1st Row 1st Column x 1 ModifyingAbove g With caret Subscript 1 Baseline 2nd Column ellipsis 3rd Column x 1 ModifyingAbove g With caret Subscript j Baseline 4th Column ellipsis 5th Column x 1 ModifyingAbove g With caret Subscript upper M Baseline 2nd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 3rd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 4th Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 5th Row 1st Column x Subscript upper N Baseline ModifyingAbove g With caret Subscript 1 Baseline 2nd Column ellipsis 3rd Column x Subscript upper N Baseline ModifyingAbove g With caret Subscript j Baseline 4th Column ellipsis 5th Column x Subscript upper N Baseline ModifyingAbove g With caret Subscript upper M Baseline EndMatrix periodx ˆG =
⎡
⎢⎢⎢⎢⎢⎣
x1 ˆg1 . . . x1 ˆgj . . . x1 ˆgM
.
.
.
.
.
.
.
.
.
xN ˆg1 . . . xN ˆgj . . . xN ˆgM
⎤
⎥⎥⎥⎥⎥⎦
.
(5.58) 
Combining Eqs. 5.57 and 5.58, we have:  
bold x ModifyingAbove upper G With caret StartFraction partial differential bold upper A Over partial differential x Subscript j Baseline EndFraction equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column ellipsis 3rd Column x 1 sigma summation Underscript i Overscript upper M Endscripts ModifyingAbove g With caret Subscript i Baseline StartFraction partial differential bold upper A Subscript i j Baseline Over partial differential x Subscript j Baseline EndFraction 4th Column ellipsis 5th Column 0 2nd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 3rd Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 4th Row 1st Column period 2nd Column Blank 3rd Column period 4th Column Blank 5th Column period 5th Row 1st Column 0 2nd Column ellipsis 3rd Column x Subscript upper N Baseline sigma summation Underscript i Overscript upper M Endscripts ModifyingAbove g With caret Subscript i Baseline StartFraction partial differential bold upper A Subscript i j Baseline Over partial differential x Subscript j Baseline EndFraction 4th Column ellipsis 5th Column 0 EndMatrix periodx ˆG ∂A
∂xj
=
⎡
⎢⎢⎢⎢⎢⎢⎣
0 . . . x1
ΣM
i
ˆgi
∂Aij
∂xj
. . . 0
.
.
.
.
.
.
.
.
.
0 . . . xN
ΣM
i
ˆgi
∂Aij
∂xj . . . 0
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(5.59) 
The trace of Eq. 5.59 is then calculated by: 
upper T r left bracket bold x Superscript t Baseline ModifyingAbove upper G With caret StartFraction partial differential bold upper A Over partial differential x Subscript j Baseline EndFraction right bracket equals x Subscript j Baseline sigma summation Underscript i Overscript upper M Endscripts ModifyingAbove g With caret Subscript i Baseline StartFraction partial differential bold upper A Subscript i j Baseline Over partial differential x Subscript j Baseline EndFraction periodT r[xt ˆG ∂A
∂xj
] = xj
M
Σ
i
ˆgi
∂Aij
∂xj
.
(5.60)

5.4
Network Pruning
171
Remembering that bold x Superscript t plus 1 Baseline equals bold x Superscript t Baseline plus eta 2 StartFraction partial differential upper G Over partial differential bold x EndFractionxt+1 = xt + η2 ∂G
∂x , CoGD is established by combining Eqs. 5.54 
and 5.60: 
StartLayout 1st Row 1st Column ModifyingAbove bold x With caret Superscript t plus 1 2nd Column equals bold x Superscript t plus 1 Baseline plus eta 2 Start 5 By 1 Matrix 1st Row sigma summation Underscript i Overscript upper M Endscripts ModifyingAbove g With caret Subscript i Baseline StartFraction partial differential bold upper A Subscript i Baseline 1 Baseline Over partial differential x 1 EndFraction 2nd Row period 3rd Row period 4th Row period 5th Row sigma summation Underscript i Overscript upper M Endscripts ModifyingAbove g With caret Subscript i Baseline StartFraction partial differential bold upper A Subscript i upper N Baseline Over partial differential x Subscript upper N Baseline EndFraction EndMatrix circled dot Start 5 By 1 Matrix 1st Row x 1 2nd Row period 3rd Row period 4th Row period 5th Row x Subscript upper N Baseline EndMatrix 2nd Row 1st Column Blank 2nd Column equals bold x Superscript t plus 1 Baseline plus eta 2 Start 5 By 1 Matrix 1st Row less than ModifyingAbove upper G With caret comma StartFraction partial differential bold upper A 1 Over partial differential x 1 EndFraction greater than 2nd Row period 3rd Row period 4th Row period 5th Row less than ModifyingAbove upper G With caret comma StartFraction partial differential bold upper A Subscript upper N Baseline Over partial differential x Subscript upper N Baseline EndFraction greater than EndMatrix circled dot Start 5 By 1 Matrix 1st Row x 1 2nd Row period 3rd Row period 4th Row period 5th Row x Subscript upper N Baseline EndMatrix 3rd Row 1st Column Blank 2nd Column equals bold x Superscript t plus 1 Baseline plus eta 2 bold c circled dot bold x Superscript t Baseline period EndLayout
ˆxt+1 = xt+1 + η2
⎡
⎢⎢⎢⎢⎢⎣
ΣM
i
ˆgi ∂Ai1
∂x1
.
.
.
ΣM
i
ˆgi ∂AiN
∂xN
⎤
⎥⎥⎥⎥⎥⎦
O
⎡
⎢⎢⎢⎢⎢⎣
x1
.
.
.
xN
⎤
⎥⎥⎥⎥⎥⎦
= xt+1 + η2
⎡
⎢⎢⎢⎢⎢⎣
< ˆG, ∂A1
∂x1 >
.
.
.
< ˆG, ∂AN
∂xN >
⎤
⎥⎥⎥⎥⎥⎦
O
⎡
⎢⎢⎢⎢⎢⎣
x1
.
.
.
xN
⎤
⎥⎥⎥⎥⎥⎦
= xt+1 + η2c O xt.
(5.61) 
We further deﬁne the kernelized version of bold cc and have: 
StartLayout 1st Row bold c equals Start 5 By 1 Matrix 1st Row ModifyingAbove upper K With caret left parenthesis ModifyingAbove upper G With caret comma StartFraction partial differential bold upper A 1 Over partial differential x 1 EndFraction right parenthesis 2nd Row period 3rd Row period 4th Row period 5th Row ModifyingAbove upper K With caret left parenthesis ModifyingAbove upper G With caret comma StartFraction partial differential bold upper A Subscript upper N Baseline Over partial differential x Subscript upper N Baseline EndFraction right parenthesis EndMatrix comma EndLayout c =
⎡
⎢⎢⎢⎢⎢⎣
ˆK( ˆG, ∂A1
∂x1 )
.
.
.
ˆK( ˆG, ∂AN
∂xN )
⎤
⎥⎥⎥⎥⎥⎦
,
(5.62) 
where ModifyingAbove upper K With caret left parenthesis period comma period right parenthesis ˆK(., .) is a kernel function.1 Remembering that Eq. 5.53, bold x Superscript t plus 1 Baseline equals bold x Superscript t Baseline plus eta 2 StartFraction partial differential upper G Over partial differential bold x EndFractionxt+1 = xt + η2 ∂G
∂x , 
Eq. 5.54 then becomes: 
ModifyingAbove bold x With caret Superscript t plus 1 Baseline equals bold x Superscript t plus 1 Baseline plus eta 2 bold c Superscript t Baseline circled dot bold x Superscript t Baseline commaˆxt+1 = xt+1 + η2ct O xt,
(5.63) 
where circled dotO represents the Hadamard product. It is then reformulated as a projection 
function as: 
ModifyingAbove bold x With caret Superscript t plus 1 Baseline equals upper P left parenthesis bold x Superscript t plus 1 Baseline comma bold x Superscript t Baseline right parenthesis equals bold x Superscript t plus 1 Baseline plus beta circled dot bold x Superscript t Baseline commaˆxt+1 = P(xt+1, xt) = xt+1 + β O xt,
(5.64) 
which shows the rationality of our method, i period e periodi.e., it is based on a projection function 
to solve the asynchronous problem of the bilinear optimization by controlling betaβ. 
We ﬁrst judge when an asynchronous convergence happens in the optimization 
based on a form of logical operation as: 
left parenthesis normal not sign s left parenthesis bold x right parenthesis right parenthesis and left parenthesis s left parenthesis bold upper A right parenthesis right parenthesis equals 1 comma(¬s(x)) ∧(s(A)) = 1,
(5.65)
1 ModifyingAbove upper K With caret left parenthesis x Baseline 1 comma x Baseline 2 right parenthesis equals left parenthesis x Baseline 1 dot x Baseline 2 right parenthesis Superscript k ˆK(x1, x2) = (x1 · x2)k. 

172
5
Network Pruning
and 
s left parenthesis asterisk right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column 1 2nd Column i f upper R left parenthesis asterisk right parenthesis greater than or equals alpha comma 2nd Row 1st Column 0 2nd Column o t h e r w i s e comma EndLayouts(∗) =
{
1
if R(∗) ≥α,
0
otherwise,
(5.66) 
where alphaα represents the threshold which changes for different applications. Equa-
tion 5.65 describes an assumption that an asynchronous convergence happens for bold upper AA
and bold xx when their norms become signiﬁcantly different. Accordingly, the update rule 
of the proposed CoGD [62] is deﬁned as: 
ModifyingAbove bold x With caret Superscript t plus 1 Baseline equals StartLayout Enlarged left brace 1st Row 1st Column upper P left parenthesis bold x Superscript t plus 1 Baseline comma bold x Superscript t Baseline right parenthesis 2nd Column i f left parenthesis normal not sign s left parenthesis bold x right parenthesis right parenthesis and left parenthesis s left parenthesis bold upper A right parenthesis right parenthesis equals 1 comma 2nd Row 1st Column bold x Superscript t plus 1 Baseline 2nd Column o t h e r w i s e comma EndLayoutˆxt+1 =
{
P(xt+1, xt)
if (¬s(x)) ∧(s(A)) = 1,
xt+1
otherwise,
(5.67) 
which leads to a synchronous convergence and generalizes the conventional gradient 
descent method. CoGD is then established. 
Note that c in Eq. 5.61 is calculated based on ModifyingAbove upper G With caretˆG, which differs for applications. 
StartFraction partial differential bold upper A Subscript j Baseline Over partial differential x Subscript j Baseline EndFraction almost equals StartFraction upper Delta bold upper A Subscript j Baseline Over upper Delta x Subscript j Baseline EndFraction ∂Aj
∂xj ≈AAj
Axj , where upper DeltaA denotes the difference of the variable over the epoch related 
to the convergence speed. StartFraction partial differential bold upper A Subscript j Baseline Over partial differential x Subscript j Baseline EndFraction equals bold 1 ∂Aj
∂xj = 1, if  upper Delta x Subscript jAxj or x Subscript jxj approaches to zero. With above 
derivation, we deﬁne CoGD within the gradient descent framework, providing a 
solid foundation for the convergence analysis of CoGD. Based on CoGD, the 
variables are sufﬁciently trained and decoupled, which can enhance the causality 
of the learning system [74]. 
5.4.4.3 
Applications 
We apply the proposed algorithm on convolutional sparse coding (CSC) and deep 
learning to validate its general applicability to bilinear problems including image 
inpainting, image reconstruction, network pruning, and CNN model training. 
Convolutional Sparse Coding 
CSC operates on the whole image, decomposing a global dictionary and set of 
features. The CSC problem is theoretically more challenging than the patch-
based sparse coding [47] and requires more sophisticated optimization model. The 
reconstruction process is usually based on a bilinear optimization model formulated 
as: 
StartLayout 1st Row 1st Column arg min Underscript bold upper A comma bold x Endscripts 2nd Column one half double vertical bar bold b minus bold upper A bold x double vertical bar Subscript upper F Superscript 2 plus lamda double vertical bar bold x double vertical bar Subscript 1 2nd Row 1st Column italic s period t period 2nd Column double vertical bar bold upper A Subscript k Baseline double vertical bar Subscript 2 Superscript 2 Baseline less than or equals 1 for all k element of StartSet 1 comma ellipsis comma upper K EndSet comma EndLayout
arg min
A,x
1
2 ||b −Ax||2
F + λ ||x||1
s.t. ||Ak||2
2 ≤1
∀k ∈{1, . . . , K},
(5.68) 
where bold bb denotes input images.

5.4
Network Pruning
173
.x = [xT
1 , . . . , xT
K]T denotes coefﬁcients under sparsity regularization. lamdaλ is the 
sparsity regularization factor. bold upper A equals left bracket bold upper A 1 comma ellipsis comma bold upper A Subscript upper K Baseline right bracketA = [A1, . . . , AK] is a concatenation of Toeplitz 
matrices representing the convolution with the kernel ﬁlters bold upper A Subscript kAk, where K is the 
number of the kernels. 
In Eq. 5.68, the optimized objectives or models are inﬂuenced by two or more 
hidden factors that interact to produce the observations. Existing solution tends to 
decompose the bilinear optimization problem into manageable subproblems [24, 
50]. Without considering the relationship between two hidden factors, however, 
existing methods suffer from suboptimal solutions caused by an asynchronous 
convergence speed of the hidden variables. We attempt to purse an optimized 
solution based on the proposed CoGD. 
Speciﬁcally, we introduce a diagonal or block-diagonal matrix bold mm to the sparse 
coding framework deﬁned in [17] and reformulate Eq. 5.68 as: 
StartLayout 1st Row 1st Column Blank 2nd Column arg min Underscript bold upper A comma bold x Endscripts f 1 left parenthesis bold upper A bold x right parenthesis plus sigma summation Underscript k equals 1 Overscript upper K Endscripts left parenthesis f 2 left parenthesis bold x Subscript k Baseline right parenthesis plus f 3 left parenthesis bold upper A Subscript k Baseline right parenthesis right parenthesis comma EndLayout arg min
A,x
f1(Ax) +
K
Σ
k=1
(f2(xk) + f3(Ak)),
(5.69) 
where 
StartLayout 1st Row 1st Column f 1 left parenthesis bold v right parenthesis 2nd Column equals one half double vertical bar bold b minus bold m bold v double vertical bar Subscript upper F Superscript 2 Baseline comma 2nd Row 1st Column f 2 left parenthesis bold v right parenthesis 2nd Column equals lamda double vertical bar bold v double vertical bar Subscript 1 Baseline comma 3rd Row 1st Column f 3 left parenthesis bold v right parenthesis 2nd Column equals i n d Subscript c Baseline left parenthesis bold v right parenthesis period EndLayout
f1(v) = 1
2 ||b −mv||2
F ,
f2(v) = λ ||v||1 ,
f3(v) = indc(v).
(5.70) 
In Eq. 5.70, i n d Subscript c Baseline left parenthesis dot right parenthesisindc(·) is an indicator function deﬁned on the convex set of the 
constraints upper C equals StartSet bold x vertical bar double vertical bar bold upper S x double vertical bar Subscript 2 Superscript 2 Baseline less than or equals 1 EndSetC = {x| ||Sx||2
2 ≤1}. Similar to Eq. 5.67, we have:  
StartLayout 1st Row 1st Column Blank 2nd Column ModifyingAbove bold x With caret Subscript k Baseline equals StartLayout Enlarged left brace 1st Row 1st Column upper P left parenthesis bold x Subscript k Superscript t plus 1 Baseline comma bold x Subscript k Superscript t Baseline right parenthesis 2nd Column i f left parenthesis normal not sign s left parenthesis bold x Subscript k Baseline right parenthesis right parenthesis and left parenthesis s left parenthesis bold upper A Subscript k Baseline right parenthesis right parenthesis equals 1 2nd Row 1st Column bold x Subscript k Superscript t plus 1 Baseline 2nd Column o t h e r w i s e EndLayout comma EndLayout
ˆxk =
{
P (xt+1
k
, xt
k)
if (¬s(xk)) ∧(s(Ak)) = 1
xt+1
k
otherwise
,
(5.71) 
which solve the two coupled variables iteratively. upper P left parenthesis bold x Subscript k Superscript t plus 1 Baseline comma bold x Subscript k Superscript t Baseline right parenthesisP(xt+1
k
, xt
k) is calculated based 
on ModifyingAbove upper G With caret left parenthesis bold upper A comma bold x right parenthesis ˆG(A, x), which is deﬁned in Eq. 5.52. 
5.4.4.4 
Network Pruning 
Network pruning, particularly convolutional channel pruning, has received 
increased attention for compressing CNNs. Early works in this area tended 
to directly prune the kernel based on simple criteria like the norm of kernel 
weights [40] or use a greedy algorithm [46]. Recent approaches have formulated 
network pruning as a bilinear optimization problem with soft masks and sparsity 
regularization [23, 28, 43, 70]. 
Based on the framework of channel pruning [23, 28, 43, 70], we apply the 
proposed CoGD for network pruning. To prune a channel of the network, the soft

174
5
Network Pruning
Conv-Mask-BN-ReLU-Mask 
BN 
ReLU 
Input 
Feature Maps 
0.81 
0.34 
Sign(Mask) 
0.04 
0 
1 
1 
0 
0 
Output 
Feature Maps 
0.78 
0.34 
Rollback 
0 
0 
0.81 
0.34 
0.04 
0 
Cogradient(Mask) 
l+1-Layer
l-Layer
I-Layer 
Dense 
Sparse 
Fig. 5.11 The forward process with the soft mask 
mask m is introduced after the convolutional layer to guide the output channel 
pruning. This is deﬁned as a bilinear model as: 
F l+1
j
= f (
Σ
i
F l
i ⊗(W l
i,jmj)),
(5.72) 
where F l
j and F l+1
j
are the i-th input and the j-th output feature maps at the l-th 
and (l +1)-th layer. W l
i,j are convolutional ﬁlters corresponding to the soft mask m. 
⊗and f (·) respectively refer to convolutional operator and activation. 
In this framework shown in Fig. 5.11, the soft mask m is learned end-to-end in 
the back propagation process. To be consistent with other pruning works, we use W
and m instead of A and x. A general optimization function for network pruning with 
a soft mask is deﬁned as: 
arg min
W,m
L(W, m) + λ||m||1 + R(W),
(5.73) 
where L(W, m) is the loss function, described in details below. With the sparsity 
constraint on m, the convolutional ﬁlters with zero value in the corresponding 
soft mask are regarded as useless ﬁlters. This means that these ﬁlters and their 
corresponding channels in the feature maps have no signiﬁcant contribution to the 
network predictions and should be pruned. There is, however, a dilemma in the 
pruning-aware training in that the pruned ﬁlters are not evaluated well before they 
are pruned, which leads to suboptimal pruning. In particular, the soft mask m and 
the corresponding kernels are not sparse in a synchronous manner, which can prune 
the kernels still of potentials. To address this problem, we apply the proposed CoGD 
to calculate the soft mask m, by reformulating Eq. 5.67 as: 
ˆml,t+1
j
=
{
P (mj l,t+1, mj l,t)
if (¬s(ml,t
j )) ∧s(Σ
i W l
i,j)=1
ml,t+1
j
otherwise,
(5.74)

5.4
Network Pruning
175
where upper W Subscript i comma jWi,j represents the 2D kernel of the i-th input channel of the j-th ﬁlter. betaβ, 
alpha Subscript upper WαW, and alpha Subscript bold mαm are detailed in experiments. The form of ModifyingAbove upper G With caretˆG is speciﬁc for different 
applications. For CNN pruning, based on Eq. 5.51, we simplify the calculation of ModifyingAbove upper G With caretˆG
as: 
ModifyingAbove upper G With caret equals StartFraction partial differential script upper L Over partial differential upper W Subscript i comma j Baseline EndFraction divided by bold m Subscript j Baseline period ˆG =
∂L
∂Wi,j
/mj.
(5.75) 
Note that the autograd package in deep learning frameworks such as PyTorch [51] 
can automatically calculate StartFraction partial differential script upper L Over partial differential upper W Subscript i comma j Baseline EndFraction ∂L
∂Wi,j . We then substitute Eq. 5.75 into Eq. 5.62 to train 
our network and prune CNNs based on the new mask ModifyingAbove bold m With caretˆm in Eq. 5.74. 
To examine how our CoGD works for network pruning, we use GAL [43] as  
an example to describe our CoGD for CNN pruning. A pruned network obtained 
through GAL with script l 1l1-regularization on the soft mask is used to approximate the 
pre-trained network by aligning their outputs. The discriminator D with weights 
upper W Subscript upper DWD is introduced to discriminate between the output of the pre-trained network 
and the pruned network. The pruned network generator G with weights upper W Subscript upper GWG and 
soft mask bold mm is learned together with D by using the knowledge from supervised 
features of the baseline. Accordingly, the soft mask bold mm, the new mask ModifyingAbove bold m With caretˆm, the pruned 
network weights upper W Subscript upper GWG, and the discriminator weights upper W Subscript upper DWD are simultaneously learned 
by solving the optimization problem as follows: 
StartLayout 1st Row 1st Column arg min Underscript upper W Subscript upper G Baseline comma bold m Endscripts max Underscript upper W Subscript upper D Baseline comma ModifyingAbove bold m With caret Endscripts 2nd Column script upper L Subscript upper A d v Baseline left parenthesis upper W Subscript upper G Baseline comma ModifyingAbove bold m With caret comma upper W Subscript upper D Baseline right parenthesis plus script upper L Subscript d a t a Baseline left parenthesis upper W Subscript upper G Baseline comma ModifyingAbove bold m With caret right parenthesis 2nd Row 1st Column plus 2nd Column script upper L Subscript r e g Baseline left parenthesis upper W Subscript upper G Baseline comma bold m comma upper W Subscript upper D Baseline right parenthesis period EndLayout
arg min
WG,m max
WD, ˆmLAdv(WG, ˆm, WD) + Ldata(WG, ˆm)
+Lreg(WG, m, WD).
(5.76) 
where script upper L left parenthesis upper W comma bold m right parenthesis equals script upper L Subscript upper A d v Baseline left parenthesis upper W Subscript upper G Baseline comma ModifyingAbove bold m With caret comma upper W Subscript upper D Baseline right parenthesis plus script upper L Subscript d a t a Baseline left parenthesis upper W Subscript upper G Baseline comma ModifyingAbove bold m With caret right parenthesisL(W, m) = LAdv(WG, ˆm, WD)+Ldata(WG, ˆm) and script upper L Subscript r e g Baseline left parenthesis upper W Subscript upper G Baseline comma bold m comma upper W Subscript upper D Baseline right parenthesisLreg(WG, m, WD) are 
related to lamda double vertical bar bold m double vertical bar Subscript 1 plus upper R left parenthesis upper W right parenthesisλ||m||1 + R(W) in Eq. 5.73. script upper L Subscript upper A d v Baseline left parenthesis upper W Subscript upper G Baseline comma ModifyingAbove bold m With caret comma upper W Subscript upper D Baseline right parenthesisLAdv(WG, ˆm, WD) is the adversarial loss to 
train the two-player game between the pre-trained network and the pruned network 
that compete with each other. 
The advantages of CoGD in network pruning are threefold. First, CoGD that 
optimizes the bilinear pruning model leads to a synchronous gradient convergence. 
Second, the process is controllable by a threshold, which makes the pruning rate 
easy to adjust. Third, the CoGD method for network pruning is scalable, i period e periodi.e., it can 
be built upon other state-of-the-art networks for better performance. 
CNN Training 
The last but not the least, CoGD can be fused with the batch normalization (BN) 
layer and improve the performance of CNN models. As is known, the BN layer 
can redistribute the features, resulting in the convergence of the feature and kernel 
learning in an asynchronous manner. CoGD is then introduced to synchronize their 
learning speeds to sufﬁciently train CNN models. Speciﬁcally, we backtrack sparse 
convolutional kernels through evaluating the sparsity of the BN layer, leading to 
an efﬁcient training process. An interesting application of CoGD is studied in CNN 
learning. Considering the linearity of the convolutional operation, CNN training can

176
5
Network Pruning
also be considered as a bilinear optimization task as: 
upper F Subscript j Superscript l plus 1 Baseline equals f left parenthesis upper B upper N left parenthesis sigma summation Underscript i Endscripts upper F Subscript i Superscript l Baseline circled times upper W Subscript i comma j Superscript l Baseline right parenthesis right parenthesis commaF l+1
j
= f (BN(
Σ
i
F l
i ⊗W l
i,j)),
(5.77) 
where upper F Subscript j Superscript lF l
j and upper F Subscript j Superscript l plus 1F l+1
j
are the ii-th input and the jj-th output feature maps at the l-th 
and left parenthesis l plus 1 right parenthesis(l + 1)-th layer, upper W Subscript i comma j Superscript lW l
i,j are convolutional ﬁlters, and circled times⊗, upper B upper N left parenthesis dot right parenthesisBN(·), and f left parenthesis dot right parenthesisf (·) refer to 
convolutional operator, batch normalization, and activation, respectively. However, 
the convolutional operation is not as efﬁcient as a traditional bilinear model. We 
instead consider a batch normalization (BN) layer to validate our method, which can 
be formulated as a bilinear optimization problem as detailed in Sect. 4.2. We use  the  
CoGD to replace SGD to efﬁciently learn the CNN, with the aim of validating the 
effectiveness of the proposed method. 
To ease presentation, we ﬁrst copy Eq. 5.77 as: 
upper F Subscript j Superscript l plus 1 Baseline equals f left parenthesis upper B upper N left parenthesis sigma summation Underscript i Endscripts upper F Subscript i Superscript l Baseline circled times upper W Subscript i comma j Superscript l Baseline right parenthesis right parenthesis comma a n dF l+1
j
= f (BN(
Σ
i
F l
i ⊗W l
i,j)), and
(5.78) 
then redeﬁne the BN model as: 
StartLayout 1st Row upper B upper N left parenthesis x right parenthesis equals gamma StartFraction x minus mu Subscript upper B Baseline Over StartRoot sigma Subscript upper B Baseline plus epsilon EndRoot EndFraction plus beta comma 2nd Row mu Subscript upper B Baseline equals StartFraction 1 Over m EndFraction sigma summation Underscript i equals 1 Overscript m Endscripts x Subscript i Baseline comma 3rd Row sigma Subscript upper B Baseline equals StartFraction 1 Over m EndFraction sigma summation Underscript i equals 1 Overscript m Endscripts left parenthesis x Subscript i Baseline minus mu Subscript upper B Baseline right parenthesis squared comma EndLayout
BN(x) = γ x −μB
√σB + ε + β,
μB = 1
m
m
Σ
i=1
xi,
σB = 1
m
m
Σ
i=1
(xi −μB)2,
(5.79) 
where m is the mini-batch size, mu Subscript upper BμB and sigma Subscript upper BσB are mean and variance obtained by 
feature calculation in the BN layer. gammaγ and betaβ are the learnable parameters, and epsilonε is a 
small number to avoid dividing by zero. 
According to Eqs. 5.78 and 5.79, we can easily know that 
gammaγ and W are 
bilinear. We use the sparsity of gammaγ instead of the whole convolutional features for 
kernel backtracking, which simpliﬁes the operation and improves the backtracking 
efﬁciency. Similar to network pruning, we also use gammaγ and W instead of bold upper AA and bold xx in 
this part. A general optimization for CNN training with the BN layer is: 
arg min Underscript upper W comma gamma Endscripts script upper L left parenthesis upper W comma gamma right parenthesis plus lamda double vertical bar upper W double vertical bar Subscript 1 Baseline comma arg min
W,γ
L(W, γ ) + λ||W||1,
(5.80)

5.4
Network Pruning
177
where script upper L left parenthesis upper W comma gamma right parenthesisL(W, γ ) is the loss function deﬁned on Eqs. 5.78 and 5.79. CoGD is then 
applied to train CNNs, by reformulating Eq. 5.67 as: 
StartLayout 1st Row 1st Column ModifyingAbove upper W With caret Subscript i comma j Superscript l comma t plus 1 Baseline equals 2nd Column StartLayout Enlarged left brace 1st Row 1st Column upper P left parenthesis upper W Subscript i comma j Baseline Superscript l comma t plus 1 Baseline comma upper W Subscript i comma j Baseline Superscript l comma t Baseline right parenthesis 2nd Column i f left parenthesis normal not sign s left parenthesis gamma Subscript j Superscript l Baseline right parenthesis right parenthesis and s left parenthesis sigma summation Underscript i Endscripts upper W Subscript i comma j Superscript l Baseline right parenthesis equals 1 2nd Row 1st Column upper W Subscript i comma j Baseline Superscript l comma t 2nd Column o t h e r w i s e comma EndLayout EndLayout ˆW l,t+1
i,j
=
{
P (Wi,j l,t+1, Wi,j l,t)
if (¬s(γ l
j)) ∧s(Σ
i W l
i,j)=1
Wi,j l,t
otherwise,
(5.81) 
where gamma Subscript j Superscript lγ l
j is the j-th learnable parameter in the l-th BN layer. upper W Subscript i comma jWi,j represents the 
2D kernel of the i-th input channel of the j-th ﬁlter. Similar to network pruning, we 
deﬁne: 
ModifyingAbove upper G With caret equals StartFraction partial differential script upper L Over partial differential upper W Subscript i comma j Baseline EndFraction divided by bold italic gamma Subscript j Baseline comma ˆG =
∂L
∂Wi,j
/γ j,
(5.82) 
where StartFraction partial differential script upper L Over partial differential upper W Subscript i comma j Baseline EndFraction ∂L
∂Wi,j is obtained based on the autograd package in deep learning frameworks 
such as PyTorch [51]. Similar to network pruning, we substitute Eq. 5.82 into 
Eq. 5.62, to use CoGD for CNN training. 
5.4.4.5 
Experiments 
In this section, CoGD is ﬁrst analyzed and compared with classical optimization 
methods on a baseline problem. It is then validated on the problems of CSC, network 
pruning, and CNN model training. 
Baseline Problem 
A baseline problem is ﬁrst used as an example to illustrate the superiority of our 
algorithm. The problem is the optimization of Beale function2 under constraint of 
upper F left parenthesis x 1 comma x 2 right parenthesis equals b e a l e left parenthesis x 1 comma x 2 right parenthesis plus double vertical bar x 1 double vertical bar plus x 2 squaredF(x1, x2) = beale(x1, x2) + ||x1|| + x2
2. The Beale function has the same form 
as Eq. 5.49 and can be regraded as a bilinear optimization problem with respect to 
variables x 1 x 2x1x2. During optimization, the learning rate eta 2η2 is set as 0.0010.001, 0.0050.005, and 
0.10.1 for “SGD,” “momentum,” and “Adam,” respectively. The thresholds alpha Subscript x 1αx1 and alpha Subscript x 2αx2
for CoGD are set to 1 and 0.50.5. beta equals 0.001 eta 2 bold c Superscript tβ = 0.001η2ct with StartFraction partial differential x 2 Over partial differential x 1 EndFraction almost equals StartFraction upper Delta x 2 Over upper Delta x 1 EndFraction ∂x2
∂x1 ≈Ax2
Ax1 , where upper DeltaA denotes the 
difference of variable over the epoch. StartFraction upper Delta x 2 Over upper Delta x 1 EndFraction equals bold 1 Ax2
Ax1 = 1, when upper Delta x 2Ax2 or x 2x2 approaches zero. 
The total number of iterations is 200. 
In Fig. 5.12, we compare the optimization paths of CoGD with those of the three 
widely used optimization methods – “SGD,” “momentum,” and “Adam.” It can be 
seen that algorithms equipped with CoGD have shorter optimization paths than their 
counterparts. Particularly, the ADAM-CoGD algorithm has a much shorter path than 
ADAM, demonstrating the fast convergence of the proposed CoGD algorithm. The 
similar convergence with shorter paths means that CoGD facilitates efﬁcient and 
sufﬁcient training.
2 b e a l e left parenthesis x 1 comma x 2 right parenthesis equals left parenthesis 1.5 minus x 1 plus x 1 x 2 right parenthesis squared plus left parenthesis 2.25 minus x 1 plus x 1 x 2 squared right parenthesis squared plus left parenthesis 2.62 minus x 1 plus x 1 x 2 cubed right parenthesis squaredbeale(x1, x2) = (1.5 −x1 + x1x2)2 + (2.25 −x1 + x1x2
2)2 + (2.62 −x1 + x1x3
2)2. 

178
5
Network Pruning
x1
-3.6
-2.7
-1.8
-0.9
0
0.9
1.8
2.7
3.6
4.5 
x2
-3.6
-2.7
-1.8
-0.9 
0 
0.9 
1.8 
2.7 
3.6 
4.5 
SGD 
SGD+CoGD 
Momentum 
Momentum+CoGD 
Adam 
Adam+CoGD 
Fig. 5.12 Comparison of classical gradient algorithm with CoGD.The background is the contour 
map of Beale functions. The algorithms with CoGD have short optimization paths compared with 
their counterparts, which shows that CoGD facilitates efﬁcient and sufﬁcient training 
Convolutional Sparse Coding 
Experimental Setting The CoGD for convolutional sparse coding (CSC) is evalu-
ated on two public datasets: the fruit dataset [72] and the city dataset [24, 72], each 
of which consists of ten images with 100 × 100 resolution. To evaluate the quality 
of the reconstructed images, we use two standard metrics, the peak signal-to-noise 
ratio (PSNR, unit: dB), and the structural similarity (SSIM). The higher the PSNR 
and the SSIM values are, the better the visual quality of the reconstructed image is. 
The evaluation metrics are deﬁned as: 
PSNR = 10 × log10(MAX2
MSR ),
(5.83) 
where MSE is the mean square error of clean image and noisy image. MAX is the 
maximum pixel value of the image: 
SSIM(x, y) =
(2μxμy + C1)(2σxy + C2)
(mu2x + μ2y + C1)(σ 2x + σ 2y + C2),
(5.84) 
where μ is the mean of samples. σ is the variance of samples. σxy is the covariance 
of the samples. C is a constant, C1 = (0.01 × MAX)2, C2 = (0.03 × MAX)2

5.4
Network Pruning
179
Implementation Details The reconstruction model is implemented based on the 
conventional CSC method [17], while we introduce the CoGD with the kernelized 
projection function to achieve a better convergence and higher reconstruction 
accuracy. One hundred of ﬁlters with size 11times×11 are set as model parameters. alpha Subscript bold xαx
is set to the mean of double vertical bar bold x Subscript k Baseline double vertical bar Subscript 1||xk||1. alpha Subscript bold upper AαA is calculated as the median of the sorted results 
of beta Subscript kβk. As shown  in  Eq. 5.62, linear and polynomial kernel functions are used in 
the experiment, which can both improve the performance of our method. For a fair 
comparison, we use the same hyperparameters (eta 2η2) in both our method and [17]. We 
also test beta equals 0.1 eta 2 bold c Superscript tβ = 0.1η2ct, which achieves a similar performance as the linear kernel. 
Results The CSC with the proposed CoGD algorithm is evaluated with two tasks 
including image reconstruction and image inpainting. 
For image inpainting, we randomly sample the data with a 75% subsampling rate, 
to obtain the incomplete data. Like [24], we test our method on contrast-normalized 
images. We ﬁrst learn ﬁlters from all the incomplete data under the guidance of 
the soft mask bold mm and then reconstruct the incomplete data by ﬁxing the learned 
ﬁlters. We show inpainting results of the normalized data in Fig. 5.13. Moreover, to 
Image 
Incomplete 
Observations 
Ours 
FFCSC 
Fig. 5.13 Inpainting for the normalized city dataset. From top to bottom: the original images, 
incomplete observations, reconstructions with FFCSC [24], and reconstructions with our proposed 
algorithm

180
5
Network Pruning
compare with FFCSC, inpainting results on the fruit and city datasets are shown in 
Table 5.13. It can be seen that our method achieves a better PSNR and SSIM in all 
cases, while the average PSNR and SSIM improvements are impressively 1.78 and 
0.017 db. 
For image reconstruction, we reconstruct the images on the fruit and city 
datasets. One hundred of 11times×11 ﬁlters are trained and compared with those of 
FFCSC [24]. Figure 5.14 shows the resulting ﬁlters after convergence within the 
same 20 iterations. It can be seen that the proposed reconstruction method, driven 
with CoGD, converges with a lower loss. When comparing the PSNR and the SSIM 
of our method with FFCSC in Table 5.14, we can see that in most cases, our method 
achieves higher PSNR and SSIM. The average PSNR and SSIM improvements are 
respectively 1.271.27 db and 0.0050.005. 
Considering that PSNR is calculated with a loglog function, the performance 
improvement shown in Tables 5.13 and 5.14 is signiﬁcant. Such improvements show 
that the kernelized projection function improves the performance of the algorithm 
and reveal the nonlinear interaction of the variables. 
Network Pruning We have evaluated the proposed CoGD algorithm on network 
pruning using the CIFAR-10 and ILSVRC12 ImageNet datasets for the image 
classiﬁcation tasks. The commonly used ResNets and MobileNetV2 are used as 
the backbone networks to get the pruned network models. 
Experimental Setting 
Datasets CIFAR-10 is a natural image classiﬁcation dataset containing a training 
set of 50,00050,000 and a testing set of 10,00010,000 32 times 3232 × 32 color images distributed over 
ten classes, including airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, 
ships, and trucks. The ImageNet classiﬁcation dataset is more challenging due to 
the signiﬁcant increase of image categories, image samples, and sample diversity. 
For the 1000 categories of images, there are 1.21.2 million images for training and 
50k images for validation. The large data divergence set a ground challenge for the 
optimization algorithms when pruning network models. 
Implementation We use PyTorch to implement our method with 3 NVIDIA 
TITAN V and 2 Tesla V100 GPUs. The weight decay and the momentum are set 
to 0.00020.0002 and 0.90.9, respectively. The hyperparameter lamda Subscript bold mλm is selected through cross-
validation in the range left bracket 0.01 comma 0.1 right bracket[0.01, 0.1] for ResNet and MobileNetv2. The drop rate is set 
to 0.10.1. The other training parameters are described on a per experiment basis. 
To better demonstrate our method, we denote CoGD-a as an approximated 
pruning rate of left parenthesis 1 minus a right parenthesis(1 −a)% for corresponding channels. a is associated with the 
threshold alpha Subscript upper WαW, which is given by its sorted result. For example, if a equals 0.5a = 0.5, alpha Subscript upper WαW is the 
median of the sorted result. alpha Subscript bold mαm is set to be 0.50.5 for easy implementation. Similarly, 
beta equals 0.001 eta 2 bold c Superscript tβ = 0.001η2ct with StartFraction partial differential bold upper W Over partial differential m Subscript j Baseline EndFraction almost equals StartFraction upper Delta bold upper W Over upper Delta m Subscript j Baseline EndFraction ∂W
∂mj ≈
AW
Amj . Note that our training cost is similar to that of 
[43], since we use our method once per epoch without additional cost.

5.4
Network Pruning
181
Table 5.13 Inpainting results for convolutional ﬁlters learned with the proposed method and with FFCSC [24]. All reconstructions are performed with 75% 
data subsampling. The proposed CoGD achieves better PSNR and SSIM in all cases 
Dataset
Fruit
1
2
3
4
5
6
7
8
9
10
Average 
PSNR (dB) 
[24]
25.37
24.31
25.08
24.27
23.09
25.51
22.74
24.10
19.47
22.58
23.65 
CoGD(kernelized, 
k equals 1k = 1) 
26.37
24.45
25.19
25.43
24.91
27.90
24.26
25.40
24.70
24.46
25.31 
CoGD(kernelized, 
k equals 2k = 2) 
27.93
26.73
27.19
25.25
23.54
25.02
26.29
24.12
24.48
24.04
25.47 
CoGD(kernelized, 
k equals 3k = 3) 
28.85
26.41
27.35
25.68
24.44
26.91
25.56
25.46
24.51
22.42
25.76 
SSIM
[24]
0.9118 
0.9036 
0.9043 
0.8975 
0.8883 
0.9242 
0.8921 
0.8899 
0.8909 
0.8974 
0.9000 
CoGD(kernelized, 
k equals 1k = 1) 
0.9452 
0.9217 
0.9348 
0.9114 
0.9036 
0.9483 
0.9109 
0.9041 
0.9215 
0.9097 
0.9211 
CoGD(kernelized, 
k equals 2k = 2) 
0.9483 
0.9301 
0.9294 
0.9061 
0.8939 
0.9454 
0.9245 
0.8990 
0.9208 
0.9054 
0.9203 
CoGD(kernelized, 
k equals 3k = 3) 
0.9490 
0.9222 
0.9342 
0.9181 
0.8810 
0.9464 
0.9137 
0.9072 
0.9175 
0.8782 
0.9168 
Dataset
City
1
2
3
4
5
6
7
8
9
10
Average 
PSNR (dB) 
[24]
26.55
24.48
25.45
21.82
24.29
25.65
19.11
25.52
22.67
27.51
24.31 
CoGD(kernelized, 
k equals 1k = 1) 
26.58
25.75
26.36
25.06
26.57
24.55
21.45
26.13
24.71
28.66
25.58 
CoGD(kernelized, 
k equals 2k = 2) 
27.93
26.73
27.19
25.83
24.41
25.31
26.29
24.70
24.48
24.62
25.76 
CoGD(kernelized, 
k equals 3k = 3) 
25.91
25.95
25.21
26.26
26.63
27.68
21.54
25.86
24.74
27.69
25.75
(continued)

182
5
Network Pruning
Table 5.13 (continued) 
Dataset 
Fruit
1
2
3
4
5
6
7
8
9
10
Average 
SSIM
[24]
0.9284 
0.9204 
0.9368 
0.9056 
0.9193 
0.9202 
0.9140 
0.9258 
0.9027 
0.9261 
0.9199 
CoGD(kernelized, 
k equals 1k = 1) 
0.9397 
0.9269 
0.9433 
0.9289 
0.9350 
0.9217 
0.9411 
0.9298 
0.9111 
0.9365 
0.9314 
CoGD(kernelized, 
k equals 2k = 2) 
0.9498 
0.9316 
0.9409 
0.9176 
0.9189 
0.9454 
0.9360 
0.9305 
0.9323 
0.9284 
0.9318 
CoGD(kernelized, 
k equals 3k = 3) 
0.9372 
0.9291 
0.9429 
0.9254 
0.9361 
0.9333 
0.9373 
0.9331 
0.9178 
0.9372 
0.9329

5.4
Network Pruning
183
Fig. 5.14 Filters learned on fruit and city datasets. Thumbnails of the datasets along with ﬁlters 
learned with FFCSC [24] (left) and with CoGD (right) are shown. The proposed reconstruction 
method reports lower objectives. (Best viewed in color with zoom) 
CIFAR-10 
We evaluated the proposed network pruning method on CIFAR-10 for two popular 
networks, ResNets and MobileNetV2. The stage kernels are set to 64-128-256-512 
for ResNet-18 and 16-32-64 for ResNet-110. For all networks, we add a soft mask 
only after the ﬁrst convolutional layer within each block to simultaneously prune 
the output channel of the current convolutional layer and input channel of the next 
convolutional layer. The mini-batch size is set to be 128 for 100 epochs, and the 
initial learning rate is set to 0.010.01, scaled by 0.10.1 over 30 epochs. 
Fine-tuning In the network ﬁne-tuning after pruning, we only reserve the student 
model. According to the “zeros” in each soft mask, we remove the corresponding 
output channels of the current convolutional layer and corresponding input channels 
of the next convolutional layer. We then obtain a pruned network with fewer

184
5
Network Pruning
Table 5.14 Reconstruction results for ﬁlters learned with the proposed method and with FFCSC [24]. With the exception of six images, the proposed method 
achieves better PSNR and SSIM 
Dataset
Fruit
1
2
3
4
5
6
7
8
9
10
Average 
PSNR (dB) 
[24]
30.90
29.52
26.90
28.09
22.25
27.93
27.10
27.05
23.65
23.65
26.70 
CoGD(kernelized, k equals 1k = 1) 
31.46
29.12
27.26
28.80
25.21
27.35
26.25
27.48
25.30
27.84
27.60 
CoGD(kernelized, k equals 2k = 2) 
30.54
28.77
30.33
28.64
25.72
30.31
28.07
27.46
25.22
26.14
28.12 
SSIM
[24]
0.9706 
0.9651 
0.9625 
0.9629 
0.9433 
0.9712 
0.9581 
0.9524 
0.9608 
0.9546 
0.9602 
CoGD(kernelized, k equals 1k = 1) 
0.9731 
0.9648 
0.9640 
0.9607 
0.9566 
0.9717 
0.9587 
0.9562 
0.9642 
0.9651 
0.9635 
CoGD(kernelized, k equals 2k = 2) 
0.9705 
0.9675 
0.9660 
0.9640 
0.9477 
0.9728 
0.9592 
0.9572 
0.9648 
0.9642 
0.9679 
Dataset
City
1
2
3
4
5
6
7
8
9
10
Average 
PSNR (dB) 
[24]
30.11
27.86
28.91
26.70
27.85
28.62
18.63
28.14
27.20
25.81
26.98 
CoGD(kernelized, k equals 1k = 1) 
30.29
28.77
28.51
26.29
28.50
30.36
21.22
29.07
27.45
30.54
28.10 
CoGD(kernelized, k equals 2k = 2) 
30.61
28.57
27.37
27.66
28.57
29.87
21.48
27.08
26.82
29.86
27.79 
SSIM
[24]
0.9704 
0.9660 
0.9703 
0.9624 
0.9619 
0.9613 
0.9459 
0.9647 
0.9531 
0.9616 
0.9618 
CoGD(kernelized, k equals 1k = 1) 
0.9717 
0.9660 
0.9702 
0.9628 
0.9627 
0.9624 
0.9593 
0.9663 
0.9571 
0.9632 
0.9642 
CoGD(kernelized, k equals 2k = 2) 
0.9697 
0.9646 
0.9681 
0.962
0.9613 
0.9594 
0.9541 
0.9607 
0.9538 
0.9620 
0.9631

5.4
Network Pruning
185
Table 5.15 Pruning results of ResNet-18/ResNet-110 and MobilenetV2 on CIFAR-10. M = 
million (10 Superscript 6106) 
Model
FLOPs (M)
Reduction
Accuracy/+FT (%) 
ResNet-18 [20]
555.42
–
95.31 
CoGD-0.5
274.74
0.51times×
95.11/95.30 
CoGD-0.8
423.87
0.24times×
95.19/95.41 
ResNet-56 [20]
125.49
–
93.26 
GAL-0.6 [43]
78.30
0.38times×
92.98/93.38 
GAL-0.8 [43]
49.99
0.60times×
90.36/91.58 
CoGD-0.5
48.90
0.61times×
92.38/92.95 
CoGD-0.8
82.85
0.34times×
93.16/93.59 
ResNet-110 [20]
252.89
–
93.68 
GAL-0.1 [43]
205.70
0.20times×
92.65/93.59 
GAL-0.5 [43]
130.20
0.49times×
92.65/92.74 
CoGD-0.5
95.03
0.62times×
93.31/93.45 
CoGD-0.8
135.76
0.46times×
93.42/93.66 
MobileNet-V2 [54]
91.15
–
94.43 
CoGD-0.5
50.10
0.45times×
94.25/– 
parameters and that requires fewer FLOPs. We use the same batch size of 256 for 
60 epochs as in training. The initial learning rate is changed to 0.10.1 and scaled by 0.10.1
over 15 epochs. Note that a similar ﬁne-tuning strategy was used in GAL [43]. 
Results Two kinds of networks are tested on the CIFAR-10 database – ResNets 
and MobileNet-V2. In this section, we only test the linear kernel, which achieves a 
similar performance as the full-precision model. 
Results for ResNets are shown in Table 5.15. Compared to the pre-trained 
network for ResNet-18 with 95.3195.31% accuracy, CoGD-0.50.5 achieves a 0.51 times0.51× FLOP 
reduction with negligible accuracy drop 0.01 percent sign0.01%. Among other structured pruning 
methods for ResNet-110, CoGD-0.50.5 has a larger FLOP reduction than GAL-0.10.1
(95.03 upper M95.03M vs. 205.70 upper M205.70M), but with similar accuracy (93.4593.45% vs.  93.5993.59%). These 
results demonstrate that our method can prune the network efﬁciently and generate 
a more compressed model with higher performance. 
For MobileNetV2, the pruning results are summarized in Table 5.15. Compared 
to the pre-trained network, CoGD-0.50.5 achieves a 0.45 times0.45× FLOP reduction with 
a 0.180.18% accuracy drop. The results indicate that CoGD is easily employed on 
efﬁcient networks with depth-wise separable convolution, which is worth exploring 
in practical applications. 
ImageNet 
For ILSVRC12 ImageNet, we test our CoGD based on ResNet-50. We train the 
network with a batch size of 256 for 60 epochs. The initial learning rate is set to 
0.010.01 and scaled by 0.10.1 over 15 epochs. Other hyperparameters follow the settings

186
5
Network Pruning
Table 5.16 Pruning results of ResNet-50 on ImageNet. B means billion (10 Superscript 9109) 
Model
FLOPs (B)
Reduction
Accuracy/+FT (%) 
ResNet-50 [20]
4.09
–
76.24 
ThiNet-50 [46]
1.71
0.58times×
71.01 
ThiNet-30 [46]
1.10
0.73times×
68.42 
CP[23]
2.73
0.33times×
72.30 
GDP-0.5 [42]
1.57
0.62times×
69.58 
GDP-0.6 [42]
1.88
0.54times×
71.19 
SSS-26 [29]
2.33
0.43times×
71.82 
SSS-32 [29]
2.82
0.31times×
74.18 
RBP [75]
1.78
0.56times×
71.50 
RRBP [75]
1.86
0.55times×
73.00 
GAL-0.1 [43]
2.33
0.43times×
–/71.95 
GAL-0.5 [43]
1.58
0.61times×
–/69.88 
CoGD-0.5
2.67
0.35times×
75.15/75.62 
used on CIFAR-10. The ﬁne-tuning process follows the setting on CIFAR-10 with 
the initial learning rate 0.000010.00001. 
Table 5.16 shows that CoGD achieves state-of-the-art performance on the 
ILSVRC12 ImageNet. For ResNet-50, CoGD-0.50.5 further shows a 0.35 times0.35× FLOP 
reduction while achieving only a 0.620.62% drop in accuracy. 
5.4.4.6 
Ablation Study 
We use ResNet-18 on CIFAR-10 for an ablation study to evaluate the effectiveness 
of our method. 
Effect on CoGD We train the pruned network with and without CoGD by using 
the same parameters. As shown in Table 5.17, we obtain an error rate of 4.704.70% and 
a 0.51 times0.51× FLOP reduction with CoGD, compared to the error rate of 5.195.19% and a 
0.32 times0.32× FLOP reduction without CoGD, validating the effectiveness of CoGD. 
Synchronous convergence In Fig. 5.15, the training curve shows that the conver-
gence of CoGD is similar to that of GAL with SGD-based optimization within 
an epoch, especially for the last epochs when converging in a similar speed. We 
theoretically derive CoGD within the gradient descent framework, which provides 
a theoretical foundation for the convergence, which is validated by the experiments. 
As a summary, the main differences between SGD and CoGD are twofold. First, we 
change the initial point for each epoch. Second, we explore the coupling relationship 
between the hidden factors to improve a bilinear model within the gradient descent 
framework. Such differences do not change the convergence of CoGD compared 
with the SGD method.

5.4
Network Pruning
187
Table 5.17 Pruning results 
on CIFAR-10 with CoGD or 
SGD. M means million (106) 
Optimizer 
Accuracy (%) 
FLOPs/Baseline (M) 
SGD
94.81
376.12/555.43 
CoGD
95.30
274.74/555.43 
Fig. 5.15 Comparison of training loss on CIFAR-10 with CoGD and SGD 
In Fig. 5.16, we show the convergence in a synchronous manner of the 4th layer’s 
variables when pruning CNNs. For better visualization, the learning rate of m
is enlarged by 100x. On the curves, we observe that the two variables converge 
synchronously and that neither variable gets stuck into a local minimum. This 
validates that CoGD avoids vanishing gradient for the coupled variables. 
CNN Training 
Similar to network pruning, we have further evaluated CoGD algorithm for CNN 
model training on CIFAR-10 and ILSVRC12 ImageNet datasets. Speciﬁcally, we 
use ResNet-18 as the backbone CNN to test our algorithm. The network stages are 
64-128-256-512. The learning rate is optimized by a cosine updating schedule with 
an initial learning rate 0.1. The algorithm iterates 200 epochs. The weight decay 
and momentum are respectively set to 0.0001 and 0.9. The model is trained on 2 
GPUs (Titan V) with a mini-batch size of 128. We follow the similar augmentation 
strategy in [20] and add the cutout method for training. When training the model, 
horizontal ﬂipping and 32 × 32 crop are used as data augmentation. The cutout 
size is set to 16. Similar to CNN pruning, a is set to 0.95 to compute αγ and αW. 
To improve the efﬁciency, we directly backtrack the corresponding weights. For

188
5
Network Pruning
(a) 
(b) 
Fig. 5.16 Convergence comparison of the variables in the fourth convolutional layer when pruning 
CNNs. The curves are obtained using SGD and CoGD-0.50.5 on CIFAR-10. With CoGD, the two 
variables converge synchronously while avoiding either variable gets stuck in a local convergence 
(local minimum of the objective function), which validates that CoGD can avoid vanishing gradient 
for the coupled variables 
ILSVRC12 ImageNet, the initial learning rate is set to 0.010.01, and the total epochs are 
120. 
With ResNet-18 backbone, we simply replace the SGD algorithm with the pro-
posed CoGD for model training. In Table 5.18, it can be seen that the performance 
is improved by 1.251.25% (70.75% vs. 69.50%) on the large-scale ImageNet dataset. In 
addition, the improvement is also observed on CIFAR-10. We report the results for 
different kernels, which show that the performance are relatively stable for k equals 1k = 1

5.5
Network Pruning on BNNs
189
Table 5.18 Results for CNN 
training on CIFAR-10 and 
ImageNet 
Accuracy(%) 
Models
CIFAR-10 
ImageNet 
ResNet-18 (SGD)[20]
95.31
69.50 
ResNet-18 (CoGD with k = 1) 
95.80
70.30 
ResNet-18 (CoGD with k = 2) 
96.10
70.75 
and k equals 2k = 2. These results validate the effectiveness and generality of the proposed 
CoGD algorithm. 
5.5 
Network Pruning on BNNs 
5.5.1 
Rectiﬁed Binary Convolutional Networks with Generative 
Adversarial Learning 
Quantization techniques involve representing network weights and activations using 
low-bit ﬁxed-point integers, enabling efﬁcient computation with bitwise operations. 
Binarization, as proposed in [45, 53], takes quantization to the extreme by using 
only a single bit to represent both weights and activations, where they are assigned 
values of either plus 1+1 or negative 1−1. This work focuses on creating compact binary neural 
networks (BNNs) by combining quantization and network pruning strategies. 
Despite advancements in 1-bit quantization and network pruning, only a few 
studies have merged these methods into a cohesive framework to enhance their syn-
ergy. Introducing pruning techniques into 1-bit CNNs becomes necessary because 
not all ﬁlters and kernels have equal signiﬁcance or warrant identical quantization. 
One potential solution involves pruning the network ﬁrst and then applying 1-bit 
quantization to the remaining weights, resulting in a more compressed network. 
However, this approach must consider the disparities between binarized and full-
precision parameters during pruning. As a promising alternative, one can prune the 
quantized network directly. Nevertheless, devising a uniﬁed framework to combine 
quantization and pruning remains an open question. 
To tackle these challenges, we propose a novel approach called rectiﬁed binary 
convolutional network (RBCN) [44] to train a binary neural network (BNN) using 
a generative adversarial network (GAN) framework. Our motivation stems from 
GANs’ ability to match two data distributions, namely, the full-precision and 1-bit 
networks. This can be seen as distilling or exploiting the knowledge from the full-
precision model to beneﬁt its 1-bit counterpart. 
In the training process of RBCN, the key binarization steps are depicted in 
Fig. 5.17. Here, the full-precision model and the 1-bit model (generator) generate 
“real” and “fake” feature maps, respectively, which are then fed to the discrimina-
tors. The discriminators aim to distinguish between the “real” and “fake” samples, 
while the generator attempts to deceive the discriminators. This process results

190
5
Network Pruning
Fig. 5.17 This ﬁgure shows the framework for integrating the rectiﬁed binary convolutional 
network (RBCN) with generative adversarial network (GAN) learning. The full-precision model 
provides “real” feature maps, while the 1-bit model (as a generator) provides “fake” feature maps to 
discriminators trying to distinguish “real” from “fake.” Meanwhile, the generator tries to make the 
discriminators work improperly. When this process is repeated, both the full-precision feature maps 
and kernels (across all convolutional layers) are sufﬁciently employed to enhance the capacity of 
the 1-bit model. Note that (1) the full-precision model is used only in learning but not in inference; 
(2) after training, the full-precision learned ﬁlters W are discarded, and only the binarized ﬁlters 
ModifyingAbove upper W With caret ˆW and the shared learnable matrices C are kept in RBCN for the calculation of the feature maps 
in inference 
in a rectiﬁed operation and a unique architecture that provides a more accurate 
estimation of the full-precision model. 
Furthermore, we explore the application of pruning to enhance the practical 
usability of the 1-bit model within the GAN framework. To achieve this goal, we 
seamlessly integrate quantization and pruning into a uniﬁed framework. 
5.5.1.1 
Loss Function 
The rectiﬁcation process involves combining full-precision kernels and feature 
maps to improve the binarization process. It includes two main components: kernel 
approximation and adversarial learning. 
The learnable kernel approximation results in a unique architecture that provides 
a precise estimation of the convolutional ﬁlters by minimizing the kernel loss. This 
allows the RBCN to achieve better performance and more accurate representations. 
To accomplish this, discriminators denoted as upper D left parenthesis dot right parenthesisD(·) with ﬁlters Y are introduced. 
Their purpose is to distinguish between feature maps R obtained from the full-
precision model and feature maps T generated by the RBCN. The RBCN generator, 
equipped with ﬁlters W and matrices C, is trained using knowledge from the 
supervised feature maps R.

5.5
Network Pruning on BNNs
191
In summary, the optimization problem involves learning the parameters W, C, 
and Y by solving the following optimization problem: 
StartLayout 1st Row StartLayout 1st Row 1st Column Blank 2nd Column arg min Underscript upper W comma ModifyingAbove upper W With caret comma upper C Endscripts max Underscript upper Y Endscripts script upper L equals script upper L Subscript upper A d v Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C comma upper Y right parenthesis plus script upper L Subscript upper S Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C right parenthesis plus script upper L Subscript upper K e r n e l Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C right parenthesis comma EndLayout EndLayoutarg min
W, ˆW,C
max
Y
L = LAdv(W, ˆW, C, Y) + LS(W, ˆW, C) + LKernel(W, ˆW, C),
(5.85) 
where script upper L Subscript upper A d v Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C comma upper Y right parenthesisLAdv(W, ˆW, C, Y) is the adversarial loss as: 
StartLayout 1st Row script upper L Subscript upper A d v Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C comma upper Y right parenthesis equals l o g left parenthesis upper D left parenthesis upper R semicolon upper Y right parenthesis right parenthesis plus l o g left parenthesis 1 minus upper D left parenthesis upper T semicolon upper Y right parenthesis right parenthesis comma EndLayout LAdv(W, ˆW, C, Y) = log(D(R; Y)) + log(1 −D(T ; Y)),
(5.86) 
where 
upper D left parenthesis dot right parenthesisD(·) consists of a series of basic blocks, each containing linear and 
LeakyRelu layers. We also have multiple discriminators to rectify the binarization 
training process. 
In addition, script upper L Subscript upper K e r n e l Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C right parenthesisLKernel(W, ˆW, C) denotes the kernel loss between the learned full-
precision ﬁlters W and the binarized ﬁlters ModifyingAbove upper W With caret ˆW and is deﬁned as: 
StartLayout 1st Row script upper L Subscript upper K e r n e l Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C right parenthesis equals lamda 1 divided by 2 StartAbsoluteValue EndAbsoluteValue upper W minus upper C ModifyingAbove upper W With caret StartAbsoluteValue EndAbsoluteValue squared comma EndLayout LKernel(W, ˆW, C) = λ1/2||W −C ˆW||2,
(5.87) 
where lamda 1λ1 is a balance parameter. Finally, script upper L Subscript upper SLS is a traditional problem-dependent loss, 
such as softmax loss. The adversarial, kernel, and softmax loss are regularizations 
on script upper LL . 
We also have omitted l o g left parenthesis dot right parenthesislog(·) and rewritten the optimization in Eq. 5.85 as in 
Eq. 5.88 for simplicity: 
StartLayout 1st Row StartLayout 1st Row min Underscript upper W comma ModifyingAbove upper W With caret comma upper C Endscripts script upper L Subscript upper S Baseline left parenthesis upper W comma ModifyingAbove upper W With caret comma upper C right parenthesis plus lamda 1 divided by 2 sigma summation Underscript l Endscripts sigma summation Underscript i Endscripts StartAbsoluteValue EndAbsoluteValue upper W Subscript i Superscript l Baseline minus upper C Superscript l Baseline ModifyingAbove upper W With caret Subscript i Superscript l Baseline StartAbsoluteValue EndAbsoluteValue squared plus sigma summation Underscript l Endscripts sigma summation Underscript i Endscripts StartAbsoluteValue EndAbsoluteValue 1 minus upper D left parenthesis upper T Subscript i Superscript l Baseline semicolon upper Y right parenthesis StartAbsoluteValue EndAbsoluteValue squared period EndLayout EndLayout min
W, ˆW,C
LS(W, ˆW, C) + λ1/2
Σ
l
Σ
i
||W l
i −Cl ˆW l
i ||2 +
Σ
l
Σ
i
||1 −D(T l
i ; Y)||2.
(5.88) 
where i represents the i t hith channel and l represents the l t hlth layer. In Eq. 5.88, the  
objective is to obtain W, ModifyingAbove upper W With caret ˆW and C with Y ﬁxed, which is why the term upper D left parenthesis upper R semicolon upper Y right parenthesisD(R; Y) in 
Eq. 5.85 can be ignored. The advantage of our formulation in Eq. 5.88 lies in that 
the loss function is trainable, which means it can be easily incorporated into existing 
learning frameworks. 
5.5.1.2 
Learning RBCNs 
In RBCNs, convolution is implemented using upper W Superscript lW l, upper C Superscript lCl, and upper F Subscript i n Superscript lF l
in to calculate output 
feature maps upper F Subscript o u t Superscript lF l
out as: 
StartLayout 1st Row upper F Subscript o u t Superscript l Baseline equals upper R upper B upper C o n v left parenthesis upper F Subscript i n Superscript l Baseline semicolon ModifyingAbove upper W With caret Superscript l Baseline comma upper C Superscript l Baseline right parenthesis equals upper C o n v left parenthesis upper F Subscript i n Superscript l Baseline comma ModifyingAbove upper W With caret Superscript l Baseline circled dot upper C Superscript l Baseline right parenthesis comma EndLayout F l
out = RBConv(F l
in; ˆW l, Cl) = Conv(F l
in, ˆW l O Cl),
(5.89)

192
5
Network Pruning
where RBConv denotes the convolution operation implemented as a new module, 
upper F Subscript i n Superscript lF l
in and upper F Subscript o u t Superscript lF l
out are the feature maps before and after convolution, respectively. upper W Superscript lW l are 
full-precision ﬁlters, the values of ModifyingAbove upper W With caret Superscript l ˆW l are 1 or  negative 1−1, and circled dotO is the operation of the 
element-by-element product. 
During the backward propagation process of RBCNs, the full-precision ﬁlters 
W and the learnable matrices C must be learned and updated. These two sets of 
parameters are jointly learned. We update W ﬁrst and then C in each convolutional 
layer. 
Update W Let delta Subscript upper W Sub Subscript i Sub Superscript lδW l
i be the gradient of the full-precision ﬁlter upper W Subscript i Superscript lW l
i . During back 
propagation, the gradients are ﬁrst passed to ModifyingAbove upper W With caret Subscript i Superscript l ˆW l
i and then to upper W Subscript i Superscript lW l
i . Thus: 
delta Subscript upper W Sub Subscript i Sub Superscript l Subscript Baseline equals StartFraction partial differential script upper L Over partial differential upper W Subscript i Superscript l Baseline EndFraction equals StartFraction partial differential script upper L Over partial differential ModifyingAbove upper W Subscript i Superscript l Baseline With caret EndFraction StartFraction partial differential ModifyingAbove upper W Subscript i Superscript l Baseline With caret Over partial differential upper W Subscript i Superscript l Baseline EndFraction commaδW l
i = ∂L
∂W l
i
= ∂L
∂ˆ
W l
i
∂ˆ
W l
i
∂W l
i
,
(5.90) 
where 
StartLayout 1st Row StartFraction partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline Over partial differential upper W Subscript i Superscript l Baseline EndFraction equals StartLayout Enlarged left brace 1st Row 1 period 2 plus 2 upper W Subscript i Superscript l Baseline comma negative 1 less than or equals upper W Subscript i Superscript l Baseline less than 0 comma 2nd Row 2 minus 2 upper W Subscript i Superscript l Baseline comma 0 less than or equals upper W Subscript i Superscript l Baseline less than 1 comma 3rd Row StartLayout 1st Row 1st Column 1 0 comma o t h e r w i s e comma 2nd Column Blank EndLayout EndLayout EndLayout∂ˆW l
i
∂W l
i
=
⎧
⎨
⎩
1.2 + 2W l
i ,
−1 ≤W l
i < 0,
2 −2W l
i ,
0 ≤W l
i < 1,
10,
otherwise,
(5.91) 
which is an approximation of 2 times2× the Dirac delta function [45]. Furthermore: 
StartLayout 1st Row StartFraction partial differential script upper L Over partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline EndFraction equals StartFraction partial differential script upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper K e r n e l Baseline Over partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper A d v Baseline Over partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline EndFraction comma EndLayout
∂L
∂ˆW l
i
= ∂LS
∂ˆW l
i
+ ∂LKernel
∂ˆW l
i
+ ∂LAdv
∂ˆW l
i
,
(5.92) 
and: 
upper W Subscript i Superscript l Baseline left arrow upper W Subscript i Superscript l Baseline minus eta 1 delta Subscript upper W Sub Subscript i Sub Superscript l Subscript Baseline commaW l
i ←W l
i −η1δW l
i ,
(5.93) 
where eta 1η1 is the learning rate. Then: 
StartLayout 1st Row 1st Column StartFraction partial differential script upper L Subscript upper K e r n e l Baseline Over partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline EndFraction 2nd Column equals minus lamda 1 left parenthesis upper W Subscript i Superscript l Baseline minus upper C Superscript l Baseline ModifyingAbove upper W With caret Subscript i Superscript l Baseline right parenthesis upper C Superscript l Baseline comma EndLayout
∂LKernel
∂ˆW l
i
= −λ1(W l
i −Cl ˆW l
i )Cl,
(5.94) 
StartFraction partial differential script upper L Subscript upper A d v Baseline Over partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline EndFraction equals minus 2 left parenthesis 1 minus upper D left parenthesis upper T Subscript i Superscript l Baseline semicolon upper Y right parenthesis right parenthesis StartFraction partial differential upper D Over partial differential ModifyingAbove upper W With caret Subscript i Superscript l Baseline EndFraction period∂LAdv
∂ˆW l
i
= −2(1 −D(T l
i ; Y)) ∂D
∂ˆW l
i
.
(5.95) 
Update C We further update the learnable matrix upper C Superscript lCl with upper W Superscript lW l ﬁxed. Let delta Subscript upper C Sub Superscript lδCl be the 
gradient of upper C Superscript lCl. Then we have: 
delta Subscript upper C Sub Superscript l Subscript Baseline equals StartFraction partial differential script upper L Subscript upper S Baseline Over partial differential upper C Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper K e r n e l Baseline Over partial differential upper C Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper A d v Baseline Over partial differential upper C Superscript l Baseline EndFraction commaδCl = ∂LS
∂Cl + ∂LKernel
∂Cl
+ ∂LAdv
∂Cl
,
(5.96)

5.5
Network Pruning on BNNs
193
and: 
upper C Superscript l Baseline left arrow upper C Superscript l Baseline minus eta 2 delta Subscript upper C Sub Superscript l Subscript Baseline commaCl ←Cl −η2δCl,
(5.97) 
where eta 2η2 is another learning rate. Furthermore: 
StartLayout 1st Row 1st Column StartFraction partial differential script upper L Subscript upper K e r n e l Baseline Over partial differential upper C Superscript l Baseline EndFraction 2nd Column equals minus lamda 1 sigma summation Underscript i Endscripts left parenthesis upper W Subscript i Superscript l Baseline minus upper C Superscript l Baseline ModifyingAbove upper W With caret Subscript i Superscript l Baseline right parenthesis ModifyingAbove upper W With caret Subscript i Superscript l Baseline comma EndLayout
∂LKernel
∂Cl
= −λ1
Σ
i
(W l
i −Cl ˆW l
i ) ˆW l
i ,
(5.98) 
StartFraction partial differential script upper L Subscript upper A d v Baseline Over partial differential upper C Superscript l Baseline EndFraction equals minus sigma summation Underscript i Endscripts 2 left parenthesis 1 minus upper D left parenthesis upper T Subscript i Superscript l Baseline semicolon upper Y right parenthesis right parenthesis StartFraction partial differential upper D Over partial differential upper C Superscript l Baseline EndFraction period∂LAdv
∂Cl
= −
Σ
i
2(1 −D(T l
i ; Y)) ∂D
∂Cl .
(5.99) 
The derivations presented demonstrate that the rectiﬁed process is trainable in an 
end-to-end manner. During training, we update the other parameters independently 
while keeping the convolutional layer’s parameters ﬁxed. This approach helps 
to enhance the variety of feature maps in each layer, which accelerates training 
convergence and fully explores the potential of 1-bit networks. 
In our implementation, we replace all the values of upper C Superscript lCl with their average 
during the forward process. This simpliﬁcation reduces the matrix calculations 
to scalar operations, leading to faster computation during inference. By utilizing 
this approach, we achieve a signiﬁcant speedup in the model’s execution without 
compromising its performance. 
5.5.1.3 
Network Pruning 
After binarizing the CNNs, we further prune the resulting 1-bit CNNs to increase 
model efﬁciency and improve the ﬂexibility of RBCNs in practical scenarios. The 
optimization pruning process is performed under the generative adversarial learning 
framework using the method described in [43]. 
To achieve this, we employ a soft mask that allows us to remove speciﬁc 
structures, such as ﬁlters, while maintaining performance close to the baseline 
accuracy. The discriminator upper D Subscript p Baseline left parenthesis dot right parenthesisDp(·) with weights upper Y Subscript pYp is introduced to distinguish 
between the output of the baseline network upper R Subscript pRp and that of the pruned 1-bit network 
upper T Subscript pTp. 
The pruned network is denoted by parameters upper W Subscript pWp, ModifyingAbove upper W With caret Subscript p ˆWp, upper C Subscript pCp, and a soft mask upper M Subscript pMp. 
These parameters are learned together with upper Y Subscript pYp using knowledge from the supervised 
features of the baseline network.

194
5
Network Pruning
We jointly optimize the parameters upper W Subscript pWp, ModifyingAbove upper W With caret Subscript p ˆWp, upper C Subscript pCp, upper M Subscript pMp, and upper Y Subscript pYp by solving the 
following optimization problem: 
StartLayout 1st Row StartLayout 1st Row 1st Column Blank 2nd Column arg min Underscript upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline comma upper M Subscript p Baseline Endscripts max Underscript upper Y Subscript p Baseline Endscripts script upper L Subscript p Baseline equals script upper L Subscript upper A d v normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline comma upper M Subscript p Baseline comma upper Y Subscript p Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column plus script upper L Subscript upper K e r n e l normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column script upper L Subscript upper S normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline right parenthesis plus script upper L Subscript upper D a t a normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline comma upper M Subscript p Baseline right parenthesis plus script upper L Subscript upper R e g normal bar p Baseline left parenthesis upper M Subscript p Baseline comma upper Y Subscript p Baseline right parenthesis comma EndLayout EndLayout
arg
min
Wp, ˆWp,Cp,Mp
max
Yp
Lp = LAdv_p(Wp, ˆWp, Cp, Mp, Yp)
+ LKernel_p(Wp, ˆWp, Cp)
LS_p(Wp, ˆWp, Cp) + LData_p(Wp, ˆWp, Cp, Mp) + LReg_p(Mp, Yp),
(5.100) 
where script upper L Subscript pLp is the pruning loss function, and the forms of script upper L Subscript upper A d v normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline comma upper M Subscript p Baseline comma upper Y Subscript p Baseline right parenthesisLAdv_p(Wp, ˆWp, Cp,
Mp, Yp) and script upper L Subscript upper K e r n e l normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline right parenthesisLKernel_p(Wp, ˆWp, Cp) are: 
StartLayout 1st Row script upper L Subscript upper A d v normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline comma upper M Subscript p Baseline comma upper Y Subscript p Baseline right parenthesis equals l o g left parenthesis upper D Subscript p Baseline left parenthesis upper R Subscript p Baseline semicolon upper Y Subscript p Baseline right parenthesis right parenthesis plus l o g left parenthesis 1 minus upper D Subscript p Baseline left parenthesis upper T Subscript p Baseline semicolon upper Y Subscript p Baseline right parenthesis right parenthesis comma EndLayoutLAdv_p(Wp, ˆWp, Cp, Mp, Yp) = log(Dp(Rp; Yp)) + log(1 −Dp(Tp; Yp)),
(5.101) 
StartLayout 1st Row script upper L Subscript upper K e r n e l normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline right parenthesis equals lamda 1 divided by 2 StartAbsoluteValue EndAbsoluteValue upper W Subscript p Baseline minus upper C Subscript p Baseline ModifyingAbove upper W With caret Subscript p Baseline StartAbsoluteValue EndAbsoluteValue squared period EndLayout LKernel_p(Wp, ˆWp, Cp) = λ1/2||Wp −Cp ˆWp||2.
(5.102) 
script upper L Subscript upper S normal bar pLS_p is a traditional problem-dependent loss such as softmax loss. script upper L Subscript upper D a t a normal bar pLData_p is the 
data loss between the output features of the baseline and the pruned network and is 
used to align the output of these two networks. The data loss can then be expressed 
as the MSE loss: 
StartLayout 1st Row script upper L Subscript upper D a t a normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline comma upper M Subscript p Baseline right parenthesis equals StartFraction 1 Over 2 n EndFraction double vertical bar upper R Subscript p Baseline minus upper T Subscript p Baseline double vertical bar squared comma EndLayout LData_p(Wp, ˆWp, Cp, Mp) = 1
2n
||||Rp −Tp
||||2,
(5.103) 
where n is the size of the mini-batch. 
.LReg_p(Mp, Yp) is a regularizer on upper W Subscript pWp,ModifyingAbove upper W With caret Subscript p ˆWp,upper M Subscript pMp, and upper Y Subscript pYp, which can be split into 
two parts as follows: 
script upper L Subscript upper R e g normal bar p Baseline left parenthesis upper M Subscript p Baseline comma upper Y Subscript p Baseline right parenthesis equals script upper R Subscript lamda Baseline left parenthesis upper M Subscript p Baseline right parenthesis plus script upper R left parenthesis upper Y Subscript p Baseline right parenthesis commaLReg_p(Mp, Yp) = Rλ(Mp) + R(Yp),
(5.104) 
where script upper R left parenthesis upper Y Subscript p Baseline right parenthesis equals l o g left parenthesis upper D Subscript p Baseline left parenthesis upper T Subscript p Baseline semicolon upper Y Subscript p Baseline right parenthesis right parenthesisR(Yp) = log(Dp(Tp; Yp)), script upper R Subscript lamda Baseline left parenthesis upper M Subscript p Baseline right parenthesisRλ(Mp) is a sparsity regularizer form with 
parameters lamdaλ and script upper R Subscript lamda Baseline left parenthesis upper M Subscript p Baseline right parenthesis equals lamda StartAbsoluteValue EndAbsoluteValue upper M Subscript p Baseline StartAbsoluteValue EndAbsoluteValue Subscript l 1Rλ(Mp) = λ||Mp||l1. 
As with the process in binarization, the update of the discriminators is omitted in 
the following description. We have also omitted l o g left parenthesis dot right parenthesislog(·) for simplicity and rewritten 
the optimization of Eq. 5.100 as: 
StartLayout 1st Row 1st Column Blank 2nd Column min Underscript upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline comma upper M Subscript p Baseline Endscripts lamda 1 divided by 2 sigma summation Underscript l Endscripts sigma summation Underscript i Endscripts StartAbsoluteValue EndAbsoluteValue upper W Subscript p comma i Superscript l minus upper C Superscript l Baseline ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline StartAbsoluteValue EndAbsoluteValue squared plus sigma summation Underscript l Endscripts sigma summation Underscript i Endscripts StartAbsoluteValue EndAbsoluteValue 1 minus upper D left parenthesis upper T Subscript p comma i Superscript l Baseline semicolon upper Y right parenthesis StartAbsoluteValue EndAbsoluteValue squared 2nd Row 1st Column Blank 2nd Column plus script upper L Subscript upper S normal bar p Baseline left parenthesis upper W Subscript p Baseline comma ModifyingAbove upper W With caret Subscript p Baseline comma upper C Subscript p Baseline right parenthesis plus StartFraction 1 Over 2 n EndFraction double vertical bar upper R Subscript p Baseline minus upper T Subscript p Baseline double vertical bar squared plus lamda StartAbsoluteValue EndAbsoluteValue upper M Subscript p Baseline StartAbsoluteValue EndAbsoluteValue Subscript l 1 Baseline period EndLayout
min
Wp, ˆWp,Cp,Mp
λ1/2
Σ
l
Σ
i
||W l
p,i −Cl ˆW l
p,i||2 +
Σ
l
Σ
i
||1 −D(T l
p,i; Y)||2
+ LS_p(Wp, ˆWp, Cp) + 1
2n
||||Rp −Tp
||||2 + λ||Mp||l1.
(5.105)

5.5
Network Pruning on BNNs
195
5.5.1.4 
Learning Pruned RBCNs 
In pruned RBCNs, the convolution is implemented as: 
StartLayout 1st Row StartLayout 1st Row 1st Column upper F Subscript o u t comma p Superscript l 2nd Column equals upper R upper B upper C o n v left parenthesis upper F Subscript i n comma p Superscript l Baseline semicolon ModifyingAbove upper W With caret Subscript p Superscript l Baseline ring upper M Subscript p Superscript l Baseline comma upper C Subscript p Superscript l Baseline right parenthesis equals upper C o n v left parenthesis upper F Subscript i n comma p Superscript l Baseline comma left parenthesis ModifyingAbove upper W With caret Subscript p Baseline ring upper M Subscript p Superscript l Baseline right parenthesis circled dot upper C Subscript p Superscript l Baseline right parenthesis comma EndLayout EndLayoutF l
out,p = RBConv(F l
in,p; ˆW l
p ◦Ml
p, Cl
p) = Conv(F l
in,p, ( ˆWp ◦Ml
p) O Cl
p),
(5.106) 
where ring◦is an operator that obtains the pruned weight with mask upper M Subscript pMp. The other part 
of the forward propagation in the pruned RBCNs is the same as in the RBCNs. 
In pruned RBCNs, what needs to be learned and updated are full-precision ﬁlters 
upper W Subscript pWp, learnable matrices upper C Subscript pCp, and soft mask upper M Subscript pMp. In each convolutional layer, these 
three sets of parameters are jointly learned. 
Update upper M Subscript pMp
upper M Subscript pMp is updated by FISTA [42] with the initialization of alpha Subscript left parenthesis 1 right parenthesis Baseline equals 1α(1) = 1. Then 
we obtain the following: 
StartLayout 1st Row alpha Subscript left parenthesis k plus 1 right parenthesis Baseline equals one half left parenthesis 1 plus StartRoot 1 plus 4 alpha Subscript left parenthesis k right parenthesis Superscript 2 Baseline EndRoot right parenthesis comma EndLayout α(k+1) = 1
2(1 +
/
1 + 4α2
(k)),
(5.107) 
StartLayout 1st Row y Subscript left parenthesis k plus 1 right parenthesis Baseline equals upper M Subscript p comma left parenthesis k right parenthesis Baseline plus StartFraction a Subscript left parenthesis k right parenthesis Baseline minus 1 Over a Subscript left parenthesis k plus 1 right parenthesis Baseline EndFraction left parenthesis upper M Subscript p comma left parenthesis k right parenthesis Baseline minus upper M Subscript p comma left parenthesis k minus 1 right parenthesis Baseline right parenthesis comma EndLayout y(k+1) = Mp,(k) + a(k) −1
a(k+1)
(Mp,(k) −Mp,(k−1)),
(5.108) 
StartLayout 1st Row upper M Subscript p comma left parenthesis k plus 1 right parenthesis Baseline equals p r o x Subscript eta left parenthesis k plus 1 right parenthesis Sub Superscript lamda StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue Sub Super Subscript 1 Sub Superscript Subscript Baseline left parenthesis y Subscript left parenthesis k plus 1 right parenthesis Baseline minus eta Subscript k plus 1 Baseline StartFraction partial differential left parenthesis script upper L Subscript upper A d v normal bar p Baseline plus script upper L Subscript upper D a t a normal bar p Baseline right parenthesis Over partial differential left parenthesis y Subscript left parenthesis k plus 1 right parenthesis Baseline right parenthesis EndFraction right parenthesis comma EndLayout Mp,(k+1) = proxη(k+1)λ||·||1(y(k+1) −ηk+1
∂(LAdv_p + LData_p)
∂(y(k+1))
),
(5.109) 
where eta Subscript k plus 1ηk+1 is the learning rate in iteration k plus 1k+1 and p r o x Subscript eta left parenthesis k plus 1 right parenthesis Sub Superscript lamda StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue Sub Super Subscript 1 Baseline left parenthesis z Subscript i Baseline right parenthesis equals s i g n left parenthesis z Subscript i Baseline right parenthesis dot left parenthesis StartAbsoluteValue z Subscript i Baseline EndAbsoluteValue minus eta 0 lamda right parenthesis Subscript plusproxη(k+1)λ||·||1(zi) = sign(zi)·
(|zi| −η0λ)+; more details can be found in [43]. 
Update upper W Subscript pWp Let delta Subscript upper W Sub Subscript p comma i Sub Superscript lδW l
p,i be the gradient of the full-precision ﬁlter upper W Subscript p comma i Superscript lW l
p,i. During back 
propagation, the gradients pass to ModifyingAbove upper W With caret Subscript p comma i Superscript l ˆW l
p,i ﬁrst and then to upper W Subscript p comma i Superscript lW l
p,i. Furthermore: 
StartLayout 1st Row delta Subscript upper W Sub Subscript p comma i Sub Superscript l Subscript Baseline equals StartFraction partial differential script upper L Subscript p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction equals StartFraction partial differential script upper L Subscript upper S normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper A d v normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper K e r n e l normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper D a t a normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction comma EndLayout δW l
p,i = ∂Lp
∂ˆW l
p,i
= ∂LS_p
∂ˆW l
p,i
+ ∂LAdv_p
∂ˆW l
p,i
+ ∂LKernel_p
∂ˆW l
p,i
+ ∂LData_p
∂ˆW l
p,i
,
(5.110) 
and: 
upper W Subscript p comma i Superscript l Baseline left arrow upper W Subscript p comma i Superscript l Baseline minus eta Subscript p comma 1 Baseline delta Subscript upper W Sub Subscript p comma i Sub Superscript l Subscript Baseline commaW l
p,i ←W l
p,i −ηp,1δW l
p,i,
(5.111) 
where eta Subscript p comma 1ηp,1 is the learning rate, StartFraction partial differential script upper L Subscript upper K e r n e l normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction ∂LKernel_p
∂ˆW l
p,i
and StartFraction partial differential script upper L Subscript upper A d v normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction ∂LAdv_p
∂ˆW l
p,i
are: 
StartLayout 1st Row 1st Column StartFraction partial differential script upper L Subscript upper K e r n e l normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction 2nd Column equals minus lamda 1 left parenthesis upper W Subscript p comma i Superscript l Baseline minus upper C Subscript p Superscript l Baseline ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline right parenthesis upper C Subscript p Superscript l Baseline comma EndLayout
∂LKernel_p
∂ˆW l
p,i
= −λ1(W l
p,i −Cl
p ˆW l
p,i)Cl
p,
(5.112)

196
5
Network Pruning
StartFraction partial differential script upper L Subscript upper A d v normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction equals minus 2 left parenthesis 1 minus upper D left parenthesis upper T Subscript p comma i Superscript l Baseline semicolon upper Y Subscript p Baseline right parenthesis right parenthesis StartFraction partial differential upper D Subscript p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction period∂LAdv_p
∂ˆW l
p,i
= −2(1 −D(T l
p,i; Yp)) ∂Dp
∂ˆW l
p,i
.
(5.113) 
And: 
StartLayout 1st Row StartFraction partial differential script upper L Subscript upper D a t a normal bar p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction equals minus StartFraction 1 Over n EndFraction left parenthesis upper R Subscript p Baseline minus upper T Subscript p Baseline right parenthesis StartFraction partial differential upper T Subscript p Baseline Over partial differential ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline EndFraction comma EndLayout
∂LData_p
∂ˆW l
p,i
= −1
n(Rp −Tp) ∂Tp
∂ˆW l
p,i
,
(5.114) 
Update upper C Subscript pCp We further update the learnable matrix upper C Subscript p Superscript lCl
p with upper W Subscript p Superscript lW l
p and upper M Subscript p Superscript lMl
p ﬁxed. Let 
delta Subscript upper C Sub Subscript p Sub Superscript lδClp be the gradient of upper C Subscript p Superscript lCl
p. Then we have: 
delta Subscript upper C Sub Subscript p Sub Superscript l Subscript Baseline equals StartFraction partial differential script upper L Subscript p Baseline Over partial differential ModifyingAbove upper C With caret Subscript p Superscript l Baseline EndFraction equals StartFraction partial differential script upper L Subscript upper S normal bar p Baseline Over partial differential ModifyingAbove upper C With caret Subscript p Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper A d v normal bar p Baseline Over partial differential ModifyingAbove upper C With caret Subscript p Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper K e r n e l normal bar p Baseline Over partial differential ModifyingAbove upper C With caret Subscript p Superscript l Baseline EndFraction plus StartFraction partial differential script upper L Subscript upper D a t a normal bar p Baseline Over partial differential ModifyingAbove upper C With caret Subscript p Superscript l Baseline EndFraction commaδClp = ∂Lp
∂ˆClp
= ∂LS_p
∂ˆClp
+ ∂LAdv_p
∂ˆClp
+ ∂LKernel_p
∂ˆClp
+ ∂LData_p
∂ˆClp
,
(5.115) 
and: 
upper C Subscript p Superscript l Baseline left arrow upper C Subscript p Superscript l Baseline minus eta Subscript p comma 2 Baseline delta Subscript upper C Sub Subscript p Sub Superscript l Subscript Baseline periodCl
p ←Cl
p −ηp,2δClp.
(5.116) 
andStartFraction partial differential script upper L Subscript upper K e r n e l normal bar p Baseline Over partial differential upper C Subscript p Superscript l Baseline EndFraction ∂LKernel_p
∂Clp
and StartFraction partial differential script upper L Subscript upper A d v normal bar p Baseline Over partial differential upper C Subscript p Superscript l Baseline EndFraction ∂LAdv_p
∂Clp
are: 
StartLayout 1st Row 1st Column StartFraction partial differential script upper L Subscript upper K e r n e l normal bar p Baseline Over partial differential upper C Subscript p Superscript l Baseline EndFraction 2nd Column equals minus lamda 1 sigma summation Underscript i Endscripts left parenthesis upper W Subscript p comma i Superscript l Baseline minus upper C Subscript p Superscript l Baseline ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline right parenthesis ModifyingAbove upper W With caret Subscript p comma i Superscript l Baseline comma EndLayout
∂LKernel_p
∂Clp
= −λ1
Σ
i
(W l
p,i −Cl
p ˆW l
p,i) ˆW l
p,i,
(5.117) 
StartFraction partial differential script upper L Subscript upper A d v normal bar p Baseline Over partial differential upper C Subscript p Superscript l Baseline EndFraction equals minus sigma summation Underscript i Endscripts 2 left parenthesis 1 minus upper D Subscript p Baseline left parenthesis upper T Subscript p comma i Superscript l Baseline semicolon upper Y Subscript p Baseline right parenthesis right parenthesis StartFraction partial differential upper D Subscript p Baseline Over partial differential upper C Subscript p Superscript l Baseline EndFraction period∂LAdv_p
∂Clp
= −
Σ
i
2(1 −Dp(T l
p,i; Yp))∂Dp
∂Clp
.
(5.118) 
Furthermore: 
StartLayout 1st Row StartFraction partial differential script upper L Subscript upper D a t a normal bar p Baseline Over partial differential upper C Subscript p Superscript l Baseline EndFraction equals StartFraction 1 Over n EndFraction sigma summation Underscript i Endscripts left parenthesis upper R Subscript p Baseline minus upper T Subscript p Baseline right parenthesis StartFraction partial differential upper T Subscript p Baseline Over partial differential upper C Subscript p Superscript l Baseline EndFraction period EndLayout
∂LData_p
∂Clp
= 1
n
Σ
i
(Rp −Tp) ∂Tp
∂Clp
.
(5.119) 
The complete training process is summarized in Algorithm 10, including the update 
of the discriminators. 
5.5.1.5 
Ablation Study 
This section investigates the contributions of kernel approximation, GAN, and the 
update strategy in improving the performance of RBCNs, using CIFAR-100 dataset 
and ResNet-18 with different kernel stages.

5.5
Network Pruning on BNNs
197
Algorithm 10: Pruned RBCN 
Input: The training dataset, the pre-trained 1-bit CNNs model, the feature maps Rp from the 
pre-trained model, the pruning rate, and the hyper-parameters, including the initial learning 
rate, weight decay, convolution stride, and padding size. 
Output: The pruned RBCN with updated parameters Wp, ˆWp, Mp and Cp. 
1: repeat 
2:
Randomly sample a mini-batch; 
3:
// Forward propagation 
4:
Training a pruned architecture // Using Eqs. 5.17–5.22 
5: 
for all l = 1 to  L convolutional layer do 
6:
F l 
out,p = Conv(F l 
in,p, (  ˆW l 
p ◦Mp) O Cl 
p); 
7: 
end for 
8:
// Backward propagation 
9: 
for all l = L to 1 do 
10:
Update the discriminators Dl 
p(·) by ascending their stochastic gradients: 
11:
∇Dl p (log(Dl 
p(Rl 
p; Yp)) + log(1 −Dl 
p(T l 
p; Yp)) + log(Dl 
p(Tp; Yp))); 
12:
Update soft mask Mp by FISTA // Using Eqs. 5.24–5.26 
13:
Calculate the gradients δW l p ; // Using Eqs. 5.27–5.31 
14:
W l 
p ←W l 
p − ηp,1δW l p ; // Update the weights 
15:
Calculate the gradient δCl p ; // Using Eqs. 5.32–5.36 
16:
Cl 
p ←Cl 
p − ηp,2δCl p ; // Update the learnable matrix 
17:
end for 
18: until the maximum epoch 
19: 
ˆW = sign(W). 
Table 5.19 Performance (accuracy, %) contributions of the components in RBCNs on CIFAR-
100, where Bi = Bi-Real Net, R = RBConv, G = GAN, and B = update strategy. The numbers in 
bold represent the best results 
Kernel stage
Bi
R
R+G
R+G+B 
RBCN
32-32-64-128
54.92
56.54
59.13
61.64 
RBCN
32-64-128-256
63.11
63.49
64.93
65.38 
RBCN
64-64-128-256
63.81
64.13
65.02
66.27 
1. We replace the convolution in Bi-Real Net with our kernel approximation 
(RBConv) and compare the results. The comparison is shown in the “Bi” and 
“R” columns of Table 5.19. RBCN achieves an accuracy improvement of 1.62 percent sign1.62%
over Bi-Real Net (56.54 percent sign56.54% vs. 54.92 percent sign54.92%) using the same network structure as 
in ResNet-18. This substantial improvement validates the effectiveness of the 
learnable matrices. 
2. Incorporating GAN into RBCN results in a further performance boost of 2.59 percent sign2.59%
(59.13 percent sign59.13% vs. 56.54 percent sign56.54%) with the kernel stage of 32-32-64-128. This demonstrates 
that GAN helps to mitigate the problem of getting stuck in poor local minima 
during training, leading to better overall performance. 
3. We enhance RBCNs by updating the batch normalization (BN) layers with ﬁxed 
W and C after each epoch. This strategy further increases the accuracy by

198
5
Network Pruning
2.51 percent sign2.51% (61.64 percent sign61.64% vs. 59.13 percent sign59.13%) in CIFAR-100 with 32-32-64-128 kernel stage. This 
improvement shows the effectiveness of the update strategy and its ability to 
contribute to the model’s performance. 
In summary, the kernel approximation, GAN, and the update strategy play crucial 
roles in enhancing the accuracy of RBCNs, and their combined effect results in 
signiﬁcant improvements over the baseline Bi-Real Net, making RBCNs a powerful 
choice for image classiﬁcation tasks on CIFAR-100 dataset. 
5.5.2 
BONN: Bayesian Optimized Binary Neural Network 
Bayesian learning is a statistical modeling paradigm based on Bayes’ theorem. It 
provides practical learning algorithms and facilitates understanding of other learn-
ing methods. Bayesian learning is particularly advantageous in solving probabilistic 
graphical models, enabling information exchange between perception and inference 
tasks, handling conditional dependencies in high-dimensional data, and effective 
uncertainty modeling. Bayesian neural networks (BayesNNs) have been extensively 
studied, with recent developments in their efﬁcacy [4, 36, 41, 57]. 
Estimating the posterior distribution is essential in Bayesian inference as it 
represents uncertainties for both data and parameters. However, obtaining an exact 
analytical solution for the posterior distribution is challenging due to the large 
number of parameters in neural networks. To address this, various approaches have 
been proposed, including optimization-based techniques like variational inference 
(VI) and sampling-based methods such as Markov chain Monte Carlo (MCMC). 
MCMC provides sampling-based estimates of the posterior distribution but is com-
putationally expensive for large datasets. On the other hand, VI tends to converge 
faster and has been applied to various Bayesian models, including BayesNNs [5, 58]. 
Despite the progress in 1-bit quantization and network pruning, few works have 
integrated both in a uniﬁed framework to enhance each other. However, introducing 
pruning techniques into 1-bit CNNs is crucial. Not all ﬁlters and kernels are equally 
important or suitable for quantization, as veriﬁed in subsequent experiments. 
One potential approach is to perform pruning ﬁrst, removing less important ﬁlters 
or parameters from the network, and then apply 1-bit quantization to the remaining 
network to achieve further compression. 
However, pruning a 1-bit CNN requires special considerations due to the 
difference between binarized and full-precision parameters. While 1-bit CNNs 
tend to be more redundant before and after binarization, the pruning process must 
carefully account for the impact of quantization on the remaining parameters. 
Alternatively, conducting pruning over Bayesian neural networks (BNNs) is 
a promising alternative. BNNs have been shown to provide better uncertainty 
modeling and representation ability, making them suitable candidates for pruning 
while preserving performance.

5.5
Network Pruning on BNNs
199
However, developing a uniﬁed framework to ﬁrst calculate a 1-bit network and 
then prune it remains an open challenge. The representation ability of 1-bit networks 
may deteriorate due to quantization, affecting the backpropagation process and 
rendering existing optimization schemes ineffective. 
To tackle the challenge of designing a uniﬁed framework for pruning 1-bit 
CNNs, Bayesian learning, a well-established global optimization scheme [5, 49], 
is leveraged to prune 1-bit CNNs [16]. 
The Bayesian learning approach begins by binarizing the full-precision kernels 
to two quantization values (centers), resulting in 1-bit CNNs. The quantization error 
is minimized by modeling the full-precision kernels as a Gaussian mixture, where 
each Gaussian is centered on its corresponding quantization value. 
Using the two centers for 1-bit CNNs, a mixture model is constructed to 
represent the full-precision kernels. Subsequently, the Bayesian learning framework 
introduces a novel pruning operation for 1-bit CNNs. Filters are divided into two 
groups, with the assumption that ﬁlters within each group follow the same Gaussian 
distribution. The weights of ﬁlters in one group are then replaced with their average, 
effectively pruning the network and reducing its complexity. 
The general framework for this approach is illustrated in Fig. 5.18, and it 
incorporates three innovative elements in the learning procedure of 1-bit CNNs with 
compression: (1) minimizing the reconstruction error of the parameters before and 
after quantization, (2) modeling the parameter distribution as a Gaussian mixture 
with two modes centered on the binarized values, and (3) pruning the quantized 
network by maximizing a posterior probability. 
Further analysis leads to the development of three new losses and their corre-
sponding learning algorithms, namely, the Bayesian kernel loss, Bayesian feature 
loss, and Bayesian pruning loss. These losses are jointly applied with the con-
ventional cross-entropy loss within the same back propagation pipeline, inheriting 
the advantages of Bayesian learning during model quantization and pruning. 
Additionally, the proposed losses comprehensively supervise the 1-bit CNN training 
process concerning kernel and feature distributions. 
In conclusion, the application of Bayesian learning in pruning 1-bit CNNs 
presents a promising direction for improving the compressed model’s applicability 
in practical applications. 
5.5.2.1 
Bayesian Formulation for Compact 1-Bit CNNs 
The state-of-the-art methods for learning 1-bit CNNs, such as [15, 39, 53], involve 
optimization in both continuous and discrete spaces. Training a 1-bit CNN typically 
comprises three steps: a forward pass (inference) using binarized weights (ModifyingAbove bold italic x With caretˆx), a 
backward pass involving gradient calculations, and a parameter update that leads to 
full-precision weights (bold italic xx). 
The crucial factor inﬂuencing the performance of a quantized network, as 
demonstrated in [15, 39, 53], is how to connect the binarized weights ModifyingAbove bold italic x With caretˆx with 
the full-precision weights bold italic xx. This connection determines the effectiveness of the

200
5
Network Pruning
Fig. 5.18 The evolution of the prior p left parenthesis bold italic x right parenthesisp(x), the distribution of the observation bold italic yy, and the posterior 
p left parenthesis bold italic x vertical bar bold italic y right parenthesisp(x|y) during learning, where bold italic xx is the latent variable representing the full-precision parameters 
and bold italic yy is the quantization error. Initially, the parameters bold italic xx are initialized according to a single-
mode Gaussian distribution. When our learning algorithm converges, the ideal case is that (i) p left parenthesis bold italic y right parenthesisp(y)
becomes a Gaussian distribution script upper N left parenthesis 0 comma nu right parenthesisN(0, ν), which corresponds to the minimum reconstruction error, 
and (ii) p left parenthesis bold italic x vertical bar bold italic y right parenthesis equals p left parenthesis bold italic x right parenthesisp(x|y) = p(x) is a Gaussian mixture distribution with two modes where the binarized 
values ModifyingAbove bold italic x With caretˆx and minus ModifyingAbove bold italic x With caret−ˆx are located 
quantized model. In this paper, the authors propose to address this challenge using 
a probabilistic framework to learn optimal 1-bit CNNs. 
5.5.2.2 
Bayesian Learning Losses 
Bayesian kernel loss Given a network weight parameter bold italic xx, its quantized code 
should be as close to its original (full-precision) code as possible so that the 
quantization error is minimized. We then deﬁne: 
bold italic y equals bold italic w Superscript negative bold 1 Baseline ring ModifyingAbove bold italic x With caret minus bold italic x commay = w−1 ◦ˆx −x,
(5.120) 
where bold italic x comma ModifyingAbove bold italic x With caret element of bold upper R Superscript nx, ˆx ∈Rn are the full-precision and quantized vectors, respectively, bold italic w element of bold upper R Superscript nw ∈Rn
denotes the learned vector to reconstruct bold italic xx, ring◦represents the Hadamard product, and 
bold italic y tilde upper G left parenthesis 0 comma nu right parenthesisy ∼G(0, ν) is the reconstruction error that is assumed to obey a Gaussian prior 
with zero mean and variance nuν. Under the most probable bold italic yy (corresponding to bold italic y equals bold 0y = 0
and bold italic x equals bold italic w Superscript negative 1 Baseline ring ModifyingAbove bold italic x With caretx = w−1 ◦ˆx, i.e., the minimum reconstruction error), we maximize p left parenthesis bold italic x vertical bar bold italic y right parenthesisp(x|y) to 
optimize bold italic xx for quantization (e.g., 1-bit CNNs) as: 
max p left parenthesis bold italic x vertical bar bold italic y right parenthesis comma max p(x|y),
(5.121)

5.5
Network Pruning on BNNs
201
Fig. 5.19 By considering the prior distributions of the kernels and features in the Bayesian 
framework, we achieve three new Bayesian losses to optimize the 1-bit CNNs. The Bayesian 
kernel loss improves the layer-wise kernel distribution of each convolutional layer, the Bayesian 
feature loss introduces the intraclass compactness to alleviate the disturbance induced by the 
quantization process, and the Bayesian pruning loss centralizes channels following the same 
Gaussian distribution for pruning. The Bayesian feature loss is applied only to the fully connected 
layer 
which can be solved based on Bayesian learning that uses Bayes’ theorem to 
determine the conditional probability of a hypothesis given limited observations. 
We note that the calculation of BNNs is still based on optimizing bold italic xx, as shown  
in Fig. 5.19, where the binarization is performed based on the sign function. 
Equation 5.121 is complicated and difﬁcult to solve due to the unknown w Superscript negative 1w−1 as 
shown in Eq. 5.120. From a Bayesian learning perspective, we resolve this problem 
via maximum a posteriori (MAP): 
StartLayout 1st Row 1st Column max p left parenthesis bold italic x vertical bar bold italic y right parenthesis 2nd Column equals max p left parenthesis bold italic y vertical bar bold italic x right parenthesis p left parenthesis bold italic x right parenthesis 2nd Row 1st Column Blank 2nd Column equals min StartAbsoluteValue EndAbsoluteValue ModifyingAbove bold italic x With caret minus bold italic w ring bold italic x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline minus 2 nu log left parenthesis p left parenthesis bold italic x right parenthesis right parenthesis comma EndLayout
max p(x|y) = max p(y|x)p(x)
= min ||ˆx −w ◦x||2
2 −2ν log
(
p(x)
)
,
(5.122) 
where 
p left parenthesis bold italic y vertical bar bold italic x right parenthesis proportional to exp left parenthesis minus StartFraction 1 Over 2 nu EndFraction StartAbsoluteValue EndAbsoluteValue bold italic y StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right parenthesis proportional to exp left parenthesis minus StartFraction 1 Over 2 nu EndFraction StartAbsoluteValue EndAbsoluteValue ModifyingAbove bold italic x With caret minus bold italic w ring bold italic x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right parenthesis periodp(y|x) ∝exp(−1
2ν ||y||2
2) ∝exp(−1
2ν ||ˆx −w ◦x||2
2).
(5.123) 
In Eq. 5.123, we assume that all components of the quantization error y are i.i.d., 
thus resulting in a simpliﬁed form. As shown in Fig. 5.19, for 1-bit CNNs, x is 
usually quantized to two numbers with the same absolute value. We neglect the 
overlap between the two numbers, and thus p(x) is modeled as a Gaussian mixture

202
5
Network Pruning
with two modes: 
StartLayout 1st Row 1st Column Blank 2nd Column p left parenthesis bold italic x right parenthesis equals one half left parenthesis 2 pi right parenthesis Superscript minus StartFraction upper N Over 2 EndFraction Baseline det left parenthesis bold italic upper Psi right parenthesis Superscript negative one half Baseline left brace exp left parenthesis minus StartFraction left parenthesis bold italic x minus bold italic mu right parenthesis Superscript upper T Baseline bold italic upper Psi Superscript negative 1 Baseline left parenthesis bold italic x minus bold italic mu right parenthesis Over 2 EndFraction right parenthesis 2nd Row 1st Column Blank 2nd Column plus exp left parenthesis minus StartFraction left parenthesis bold italic x plus bold italic mu right parenthesis Superscript upper T Baseline bold italic upper Psi Superscript negative 1 Baseline left parenthesis bold x bold plus bold italic mu right parenthesis Over 2 EndFraction right parenthesis right brace 3rd Row 1st Column Blank 2nd Column almost equals one half left parenthesis 2 pi right parenthesis Superscript minus StartFraction upper N Over 2 EndFraction Baseline det left parenthesis bold italic upper Psi right parenthesis Superscript negative one half Baseline left brace exp left parenthesis minus StartFraction left parenthesis bold italic x Subscript plus Baseline minus bold italic mu Subscript plus Baseline right parenthesis Superscript upper T Baseline bold italic upper Psi Subscript bold plus Superscript negative 1 Baseline left parenthesis bold italic x Subscript plus Baseline minus bold italic mu Subscript plus Baseline right parenthesis Over 2 EndFraction right parenthesis 4th Row 1st Column Blank 2nd Column plus exp left parenthesis minus StartFraction left parenthesis bold italic x Subscript minus Baseline plus bold italic mu Subscript minus Baseline right parenthesis Superscript upper T Baseline bold italic upper Psi Subscript bold minus Superscript negative 1 Baseline left parenthesis bold italic x Subscript minus Baseline plus bold italic mu Subscript minus Baseline right parenthesis Over 2 EndFraction right parenthesis right brace comma EndLayout
p(x)= 1
2(2π)−N
2 det(ψ)−1
2
{
exp
(
−(x −μ)T ψ−1(x −μ)
2
)
+ exp
(
−(x + μ)T ψ−1(x + μ)
2
)}
≈1
2(2π)−N
2 det(ψ)−1
2
{
exp
(
−(x+−μ+)Tψ+−1(x+ −μ+)
2
)
+ exp
(
−(x−+ μ−)T ψ−−1(x−+ μ−)
2
)}
,
(5.124) 
where x is divided into x+ and x− according to the signs of the elements in x, and 
N is the dimension of x. Accordingly, Eq. 5.122 can be rewritten as: 
StartLayout 1st Row 1st Column min 2nd Column StartAbsoluteValue EndAbsoluteValue ModifyingAbove bold italic x With caret minus bold italic w ring bold italic x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 plus nu left parenthesis bold italic x Subscript plus Baseline minus bold italic mu Subscript plus Baseline right parenthesis Superscript upper T Baseline bold italic upper Psi Subscript plus Superscript negative 1 Baseline left parenthesis bold italic x Subscript plus Baseline minus bold italic mu Subscript plus Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column plus nu left parenthesis bold italic x Subscript minus Baseline plus bold italic mu Subscript minus Baseline right parenthesis Superscript upper T Baseline bold italic upper Psi Subscript minus Superscript negative 1 Baseline left parenthesis bold italic x Subscript minus Baseline plus bold italic mu Subscript minus Baseline right parenthesis plus nu log left parenthesis det left parenthesis bold italic upper Psi right parenthesis right parenthesis comma EndLayout
min||ˆx −w ◦x||2
2 + ν(x+ −μ+)T ψ−1
+ (x+ −μ+)
+ ν(x−+ μ−)T ψ−1
−(x−+ μ−) + ν log
(
det(ψ)
)
,
(5.125) 
where μ− and μ+ are solved independently. det(ψ) is accordingly set to be the 
determinant of the matrix ψ− or ψ+. We call Eq. 5.125 the Bayesian kernel loss. 
Bayesian feature loss We also design a Bayesian feature loss to alleviate the 
disturbance caused by the extreme quantization process in 1-bit CNNs. Considering 
the intraclass compactness, the features f m of the m-th class supposedly follow a 
Gaussian distribution with the mean cm as revealed in the center loss [64]. Similarly 
to the Bayesian kernel loss, we deﬁne ym 
f = f m − cm and ym 
f ∼N(0, σ m), and we 
have: 
StartLayout 1st Row 1st Column min StartAbsoluteValue EndAbsoluteValue bold italic f Subscript m minus bold italic c Subscript m Baseline StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 2nd Column plus sigma summation Underscript n equals 1 Overscript upper N Subscript f Baseline Endscripts left bracket sigma Subscript m comma n Superscript negative 2 Baseline left parenthesis f Subscript m comma n Baseline minus c Subscript m comma n Baseline right parenthesis squared plus log left parenthesis sigma Subscript m comma n Superscript 2 Baseline right parenthesis right bracket comma EndLayout min||f m −cm||2
2+
Nf
Σ
n=1
[
σ −2
m,n(fm,n−cm,n)2+log(σ 2
m,n)
]
,
(5.126) 
which is called the Bayesian feature loss. In Eq. 5.126, σm,n, fm,n and cm,n are the 
n-th elements of σ m, f m and cm, respectively. We take the latent distributions of 
kernel weights and features into consideration in the same framework and introduce 
Bayesian losses to improve the capacity of 1-bit CNNs. 
5.5.2.3 
Bayesian Pruning 
After binarizing CNNs, we prune the 1-bit CNNs using a Bayesian learning 
framework. The idea is to group similar channels together and then replace ﬁlters 
within each group with their average during optimization. The representation of the 
kernel weights of the l-th layer is a tensor bold italic upper K Superscript lKl with dimensions upper C Subscript o Superscript l Baseline times upper C Subscript i Superscript l Baseline times upper H Superscript l Baseline times upper W Superscript lCl
o × Cl
i × H l × W l,

5.5
Network Pruning on BNNs
203
where upper C Subscript o Superscript lCl
o and upper C Subscript i Superscript lCl
i are the numbers of output and input channels, respectively, and upper H Superscript lH l
and upper W Superscript lW l represent the height and width of the kernels. 
To simplify the notation, we deﬁne bold italic upper K Superscript lKl as a concatenation of individual ﬁlters 
bold italic upper K Subscript i Superscript lKl
i for i equals 1 comma 2 comma ellipsis comma upper C Subscript o Superscript li = 1, 2, . . . , Cl
o, where bold italic upper K Subscript i Superscript lKl
i is a three-dimensional ﬁlter with dimensions 
upper C Subscript i Superscript l Baseline times upper H Superscript l Baseline times upper W Superscript lCl
i × H l × W l. 
The pruning process begins by using the K-means algorithm to divide the ﬁlters 
into different groups based on similarity. The assumption is that ﬁlters within each 
group follow the same Gaussian distribution during training. The goal is to ﬁnd 
the average ModifyingAbove bold italic upper K With bold quotation dashK that can replace all bold italic upper K Subscript iKi’s within the same group, which effectively 
assimilates similar ﬁlters into a single one. 
This pruning problem leads to a similar formulation as in Eq. 5.122, and it 
involves learning the average ﬁlter ModifyingAbove bold italic upper K With bold quotation dashK with a Gaussian distribution constraint. This 
type of learning process with a Gaussian distribution constraint has been widely 
considered in other works [18]. 
Accordingly, Bayesian learning is used to prune 1-bit CNNs. We denote bold italic epsilonε as the 
difference between a ﬁlter and its mean, i.e., bold italic epsilon equals bold italic upper K minus ModifyingAbove bold italic upper K With bold quotation dashε = K −K, following a Gaussian 
distribution for simplicity. To calculate ModifyingAbove bold italic upper K With bold quotation dashK, we minimize bold italic epsilonε based on MAP in our 
Bayesian framework, and we have: 
ModifyingAbove bold italic upper K With bold quotation dash equals arg max Underscript bold italic upper K Endscripts p left parenthesis bold italic upper K vertical bar bold italic epsilon right parenthesis equals arg max Underscript bold italic upper K Endscripts p left parenthesis bold italic epsilon vertical bar bold italic upper K right parenthesis p left parenthesis bold italic upper K right parenthesis commaK = arg max
K p(K|ε) = arg max
K p(ε|K)p(K),
(5.127) 
p left parenthesis bold italic epsilon vertical bar bold italic upper K right parenthesis proportional to exp left parenthesis minus StartFraction 1 Over 2 nu EndFraction StartAbsoluteValue EndAbsoluteValue bold italic epsilon StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right parenthesis proportional to exp left parenthesis minus StartFraction 1 Over 2 nu EndFraction StartAbsoluteValue EndAbsoluteValue bold italic upper K minus ModifyingAbove bold italic upper K With bold quotation dash StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right parenthesis commap(ε|K) ∝exp(−1
2ν ||ε||2
2) ∝exp(−1
2ν ||K −K||2
2),
(5.128) 
and p left parenthesis bold italic upper K right parenthesisp(K) is similar to Eq. 5.124 but with one mode. Thus, we have: 
StartLayout 1st Row 1st Column min 2nd Column StartAbsoluteValue EndAbsoluteValue bold italic upper K minus ModifyingAbove bold italic upper K With bold quotation dash StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 plus nu left parenthesis bold italic upper K minus ModifyingAbove bold italic upper K With bold quotation dash right parenthesis Superscript upper T Baseline bold italic upper Psi Superscript negative 1 Baseline left parenthesis bold italic upper K minus ModifyingAbove bold italic upper K With bold quotation dash right parenthesis 2nd Row 1st Column Blank 2nd Column plus nu log left parenthesis det left parenthesis bold italic upper Psi right parenthesis right parenthesis comma EndLayout
min||K −K||2
2 + ν(K −K)T ψ−1(K −K)
+ ν log
(
det(ψ)
)
,
(5.129) 
which is called the Bayesian pruning loss. In summary, the proposed Bayesian 
pruning approach is more general, assuming that similar kernels follow a Gaussian 
distribution and are represented by their centers for pruning. This results in a 
more ﬂexible and suitable pruning method for binary neural networks compared 
to existing techniques. We introduce Bayesian losses and Bayesian pruning within 
the same framework, considering the latent distributions of kernel weights, features, 
and ﬁlters. This enhances the capacity of 1-bit CNNs and captures uncertainties, 
leading to improved performance. Experimental results demonstrate that the pro-
posed Bayesian optimization-based neural networks (BONNs) outperform existing 
pruning methods.

204
5
Network Pruning
5.5.2.4 
BONNs 
We employ the three Bayesian losses to optimize 1-bit CNNs, which form our 
Bayesian optimized 1-bit CNNs (BONNs). To do this, we reformulate the ﬁrst two 
Bayesian losses for 1-bit CNNs as: 
StartLayout 1st Row 1st Column upper L Subscript upper B 2nd Column equals StartFraction lamda Over 2 EndFraction sigma summation Underscript l equals 1 Overscript upper L Endscripts sigma summation Underscript i equals 1 Overscript upper C Subscript o Superscript l Baseline Endscripts sigma summation Underscript n equals 1 Overscript upper C Subscript i Superscript l Baseline Endscripts left brace StartAbsoluteValue EndAbsoluteValue ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline minus bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline 2nd Row 1st Column Blank 2nd Column plus nu left parenthesis bold italic k Subscript n Superscript l comma i Baseline Subscript plus Baseline minus bold italic mu Subscript i Superscript l Baseline Subscript plus Baseline right parenthesis Superscript upper T Baseline left parenthesis bold italic upper Psi Subscript i Superscript l Baseline Subscript plus Baseline right parenthesis Superscript negative 1 Baseline left parenthesis bold italic k Subscript n Superscript l comma i Baseline Subscript plus Baseline minus bold italic mu Subscript i Superscript l Baseline Subscript plus Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column plus nu left parenthesis bold italic k Subscript n Superscript l comma i Baseline Subscript minus Baseline plus bold italic mu Subscript i Superscript l Baseline Subscript minus Baseline right parenthesis Superscript upper T Baseline left parenthesis bold italic upper Psi Subscript i Superscript l Baseline Subscript minus Baseline right parenthesis Superscript negative 1 Baseline left parenthesis bold italic k Subscript n Superscript l comma i Baseline Subscript minus Baseline plus bold italic mu Subscript i Superscript l Baseline Subscript minus Baseline right parenthesis 4th Row 1st Column Blank 2nd Column plus nu log left parenthesis det left parenthesis bold italic upper Psi Superscript l Baseline right parenthesis right parenthesis right brace plus StartFraction theta Over 2 EndFraction sigma summation Underscript m equals 1 Overscript upper M Endscripts left brace StartAbsoluteValue EndAbsoluteValue bold italic f Subscript m Baseline minus bold italic c Subscript m Baseline StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline 5th Row 1st Column Blank 2nd Column plus sigma summation Underscript n equals 1 Overscript upper N Subscript f Baseline Endscripts left bracket sigma Subscript m comma n Superscript negative 2 Baseline left parenthesis f Subscript m comma n Baseline minus c Subscript m comma n Baseline right parenthesis squared plus log left parenthesis sigma Subscript m comma n Superscript 2 Baseline right parenthesis right bracket right brace comma EndLayout
LB = λ
2
L
Σ
l=1
Cl
o
Σ
i=1
Cl
i
Σ
n=1
{
||ˆk
l,i
n −wl ◦kl,i
n ||2
2
+ ν(kl,i
n + −μl
i+)T (ψl
i+)−1(kl,i
n + −μl
i+)
+ ν(kl,i
n −+ μl
i−)T (ψl
i−)−1(kl,i
n −+ μl
i−)
+ ν log(det(ψl))
}
+ θ
2
M
Σ
m=1
{
||f m −cm||2
2
+
Nf
Σ
n=1
[
σ −2
m,n(fm,n −cm,n)2 + log(σ 2
m,n)
] }
,
(5.130) 
where bold italic k Subscript n Superscript l comma i Baseline comma l element of StartSet 1 comma ellipsis comma upper L EndSet comma i element of StartSet 1 comma ellipsis comma upper C Subscript o Superscript l Baseline EndSet comma n element of StartSet 1 comma ellipsis comma upper C Subscript i Superscript l Baseline EndSetkl,i
n , l ∈{1, . . . , L}, i ∈{1, . . . , Cl
o}, n ∈{1, . . . , Cl
i}, is the vectorization 
of the i-th kernel matrix at the l-th convolutional layer, bold italic w Superscript lwl is a vector used to 
modulate bold italic k Subscript n Superscript l comma ikl,i
n , and bold italic mu Subscript i Superscript lμl
i and bold italic upper Psi Subscript i Superscript lψl
i are the mean and covariance of the i-th kernel vector 
at the l-th layer, respectively. And we term upper L Subscript upper BLB the Bayesian optimization loss. 
Furthermore, we assume that the parameters in the same kernel are independent. 
Thus, bold italic upper Psi Subscript i Superscript lψl
i becomes a diagonal matrix with the identical value left parenthesis sigma Subscript i Superscript l Baseline right parenthesis squared(σ l
i )2, where left parenthesis sigma Subscript i Superscript l Baseline right parenthesis squared(σ l
i )2 is 
the variance of the i-th kernel of the l-th layer. 
In order to speed up the calculation of the inverse of bold italic upper Psi Subscript i Superscript lψl
i, all elements of bold italic mu Subscript i Superscript lμl
i
are made identical and equal to mu Subscript i Superscript lμl
i. Additionally, during the forward process in the 
implementation, all elements of bold italic w Superscript lwl are replaced by their average. This optimization 
results in only a scalar instead of a matrix being involved in the inference, leading 
to signiﬁcantly accelerated computation. 
After training 1-bit CNNs, the Bayesian pruning loss upper L Subscript upper PLP is utilized for the 
optimization of feature channels. The expression for upper L Subscript upper PLP is given by: 
StartLayout 1st Row 1st Column Blank 2nd Column upper L Subscript upper P Baseline equals sigma summation Underscript l equals 1 Overscript upper L Endscripts sigma summation Underscript j equals 1 Overscript upper J Subscript l Baseline Endscripts sigma summation Underscript i equals 1 Overscript upper I Subscript j Baseline Endscripts left brace StartAbsoluteValue EndAbsoluteValue bold italic upper K Subscript i comma j Superscript l Baseline minus ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline 2nd Row 1st Column Blank 2nd Column plus nu left parenthesis bold italic upper K Subscript i comma j Superscript l Baseline minus ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline right parenthesis Superscript upper T Baseline left parenthesis bold italic upper Psi Subscript j Superscript l Baseline right parenthesis Superscript negative 1 Baseline left parenthesis bold italic upper K Subscript i comma j Superscript l Baseline minus ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline right parenthesis plus nu log left parenthesis det left parenthesis bold italic upper Psi Subscript j Superscript l Baseline right parenthesis right parenthesis right brace comma EndLayout
LP =
L
Σ
l=1
Jl
Σ
j=1
Ij
Σ
i=1
{
||Kl
i,j −K
l
j||2
2
+ ν(Kl
i,j −K
l
j)T (ψl
j)−1(Kl
i,j −K
l
j) + ν log
(
det(ψl
j)
)}
,
(5.131) 
where upper J Subscript lJl is the number of Gaussian clusters (groups) of the l-th layer, and bold italic upper K Subscript i comma j Superscript lKl
i,j for 
i equals 1 comma 2 comma ellipsis comma upper I Subscript j Baselinei = 1, 2, . . . , Ij, are  the  bold italic upper K Subscript i Superscript lKl
i’s that belong to the j-th group. In the implementation,

5.5
Network Pruning on BNNs
205
upper J Subscript lJl is deﬁned as int left parenthesis upper C Subscript o Superscript l Baseline times epsilon right parenthesisint(Cl
o × ε), where epsilonε is a predeﬁned pruning rate, and one epsilonε is used 
for all layers. 
Notably, when the j-th Gaussian has only one sample bold italic upper K Subscript i comma j Superscript lKl
i,j, ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline equals bold italic upper K Subscript i comma j Superscript lK
l
j = Kl
i,j, and bold italic upper Psi Subscript jψj
becomes a unit matrix. 
In the BONN framework, the total loss L is a combination of three individual 
losses: the cross-entropy loss upper L Subscript upper SLS, the Bayesian optimization loss upper L Subscript upper BLB, and the 
Bayesian pruning loss upper L Subscript upper PLP . The expression for the total loss is given as: 
upper L equals upper L Subscript upper S Baseline plus upper L Subscript upper B Baseline plus zeta upper L Subscript upper P Baseline commaL = LS + LB + ζLP ,
(5.132) 
where zetaζ is a hyperparameter that is set to 0 during binarization training and 1 
during pruning. The Bayesian optimization loss upper L Subscript upper BLB constrains the distribution of 
the convolution kernels to a symmetric Gaussian mixture with two modes. It ensures 
that the quantization error is minimized through the term StartAbsoluteValue EndAbsoluteValue ModifyingAbove bold italic k With caret Subscript n Superscript l comma i minus bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2||ˆk
l,i
n −wl ◦kl,i
n ||2
2, where 
ModifyingAbove bold italic k With caret Subscript n Superscript l comma iˆk
l,i
n is the quantized kernel and bold italic w Superscript lwl is the learnable vector used to reconstruct the full-
precision kernel bold italic k Subscript n Superscript l comma ikl,i
n . The Bayesian feature loss works to modify the distribution of 
the features, reducing intraclass variation for improved classiﬁcation performance. 
Finally, the Bayesian pruning loss drives the kernels toward their means, effectively 
compressing the 1-bit CNNs further by assimilating similar ﬁlters into single ones. 
5.5.2.5 
Forward Propagation 
During forward propagation in BONNs, the binarized kernels and activations 
signiﬁcantly accelerate the convolution computation. The reconstruction vector, 
denoted as bold italic ww in Eq. 5.120, plays a crucial role in 1-bit CNNs. bold italic w Superscript lwl becomes a scalar ModifyingAbove w With quotation dash Superscript lwl
in each layer, where ModifyingAbove w With quotation dash Superscript lwl is the mean of bold italic w Superscript lwl and is calculated online. The convolution 
process can be represented as: 
bold italic upper O Superscript l plus 1 Baseline equals left parenthesis left parenthesis ModifyingAbove w With quotation dash Superscript l Baseline right parenthesis Superscript negative 1 Baseline ModifyingAbove bold italic upper K With bold caret Superscript l Baseline right parenthesis asterisk ModifyingAbove bold italic upper O With bold caret Superscript l Baseline equals left parenthesis ModifyingAbove w With quotation dash Superscript l Baseline right parenthesis Superscript negative 1 Baseline left parenthesis ModifyingAbove bold italic upper K With bold caret Superscript l Baseline asterisk ModifyingAbove bold italic upper O With bold caret Superscript l Baseline right parenthesis commaOl+1 = ((wl)−1 ˆK
l) ∗ˆO
l = (wl)−1( ˆK
l ∗ˆO
l),
(5.133) 
where ModifyingAbove bold italic upper O With bold caret Superscript l ˆO
l denotes the binarized feature map of the l-th layer, and upper O Superscript l plus 1Ol+1 is the feature 
map of the left parenthesis l plus 1 right parenthesis(l + 1)-th layer. As shown in Eq. 5.133, the actual convolution is still 
binary, and upper O Superscript l plus 1Ol+1 is obtained by simply multiplying left parenthesis ModifyingAbove w With quotation dash Superscript l Baseline right parenthesis Superscript negative 1(wl)−1 and the binarization 
convolution. For each layer, only one ﬂoating-point multiplication is added, which 
is negligible for BONNs. 
In addition, we consider the Gaussian distribution in the forward process of 
Bayesian pruning, which updates every ﬁlter in one group based on its mean. 
Speciﬁcally, we replace each ﬁlter bold italic upper K Subscript i comma j Superscript l Baseline equals left parenthesis 1 minus gamma right parenthesis bold italic upper K Subscript i comma j Superscript l Baseline plus gamma ModifyingAbove bold italic upper K With quotation dash Subscript j Superscript lKl
i,j = (1 −γ )Kl
i,j + γ Kl
j during pruning.

206
5
Network Pruning
Algorithm 11: Optimizing 1-bit CNNs with Bayesian learning 
AN algo
rith m for optimizin g 1-bit C N  Ns with Bayesian l earning . 
5.134AN algorithm for optimizing 1-bit C N Ns with Bayesian learning. 5.141 
AN 
algorit hm for opti miz
ing 1-b it 
C N  N
s with B ayesi
an l
earn ing.
 
5.5.2.6 
Asynchronous Backward Propagation 
To minimize Eq. 5.130, we update bold italic k Subscript n Superscript l comma ikl,i
n , bold italic w Superscript lwl, mu Subscript i Superscript lμl
i, sigma Subscript i Superscript lσ l
i , bold italic c Subscript mcm, and bold italic sigma Subscript mσ m using stochastic 
gradient descent (SGD) in an asynchronous manner, which updates bold italic ww instead of ModifyingAbove w With quotation dashw
as elaborated below. 
Updating bold italic k Subscript n Superscript l comma ikl,i
n
We deﬁne delta Subscript bold italic k Sub Subscript n Sub Superscript l comma iδkl,i
n as the gradient of the full-precision kernel bold italic k Subscript n Superscript l comma ikl,i
n , and 
we have: 
delta Subscript bold italic k Sub Subscript n Sub Superscript l comma i Subscript Baseline equals StartFraction partial differential upper L Over partial differential bold italic k Subscript n Superscript l comma i Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential bold italic k Subscript n Superscript l comma i Baseline EndFraction plus StartFraction partial differential upper L Subscript upper B Baseline Over partial differential bold italic k Subscript n Superscript l comma i Baseline EndFraction periodδkl,i
n = ∂L
∂kl,i
n
= ∂LS
∂kl,i
n
+ ∂LB
∂kl,i
n
.
(5.134) 
For each term in Eq. 5.134, we have:  
StartLayout 1st Row 1st Column StartFraction partial differential upper L Subscript upper S Baseline Over partial differential bold italic k Subscript n Superscript l comma i Baseline EndFraction 2nd Column equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline EndFraction StartFraction partial differential ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline Over partial differential left parenthesis bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline right parenthesis EndFraction StartFraction partial differential left parenthesis bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline right parenthesis Over partial differential bold italic k Subscript n Superscript l comma i Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline EndFraction ring double struck 1 Subscript negative 1 less than or equals bold italic w Sub Superscript l Subscript ring bold italic k Sub Subscript n Sub Superscript l comma i Subscript less than or equals 1 Baseline ring bold italic w Superscript l Baseline comma EndLayout
∂LS
∂kl,i
n
= ∂LS
∂ˆk
l,i
n
∂ˆk
l,i
n
∂(wl ◦kl,i
n )
∂(wl ◦kl,i
n )
∂kl,i
n
= ∂LS
∂ˆk
l,i
n
◦1−1≤wl◦kl,i
n ≤1 ◦wl,
(5.135)

5.5
Network Pruning on BNNs
207
StartLayout 1st Row 1st Column StartFraction partial differential upper L Subscript upper B Baseline Over partial differential bold italic k Subscript n Superscript l comma i Baseline EndFraction 2nd Column equals lamda left brace bold italic w Superscript l Baseline ring left bracket bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline minus ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline right bracket 2nd Row 1st Column Blank 2nd Column plus nu left bracket left parenthesis bold italic sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 2 ring left parenthesis bold italic k Subscript i Superscript l Baseline Subscript plus Baseline minus bold italic mu Subscript i Superscript l Baseline Subscript plus Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column plus left parenthesis bold italic sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 2 Baseline ring left parenthesis bold italic k Subscript i Superscript l Baseline Subscript minus Baseline plus bold italic mu Subscript i Superscript l Baseline Subscript minus Baseline right parenthesis right bracket comma EndLayout
∂LB
∂kl,i
n
= λ{wl ◦
[
wl ◦kl,i
n −ˆk
l,i
n
]
+ ν[(σ l
i)−2 ◦(kl
i+ −μl
i+)
+ (σ l
i)−2 ◦(kl
i−+ μl
i−)],
(5.136) 
where double struck 11 is the indicator function that is widely used to estimate the gradient of 
nondifferentiable parameters [53], and left parenthesis bold italic sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 2(σ l
i)−2 is a vector whose elements are all 
equal to left parenthesis sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 2(σ l
i )−2. 
Updating bold italic w Superscript lwl Unlike the forward process, bold italic ww is used in back propagation to 
calculate the gradients. This process is similar to the way to calculate ModifyingAbove bold italic x With bold caretˆx from bold italic xx
asynchronously. Speciﬁcally, delta Subscript bold italic w Sub Superscript lδwl is composed of the following two parts: 
delta Subscript bold italic w Sub Superscript l Subscript Baseline equals StartFraction partial differential upper L Over partial differential bold italic w Superscript l Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential bold italic w Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper B Baseline Over partial differential bold italic w Superscript l Baseline EndFraction periodδwl = ∂L
∂wl = ∂LS
∂wl + ∂LB
∂wl .
(5.137) 
For each term in Eq. 5.137, we have:  
StartLayout 1st Row 1st Column StartFraction partial differential upper L Subscript upper S Baseline Over partial differential bold italic w Superscript l Baseline EndFraction 2nd Column equals sigma summation Underscript i equals 1 Overscript upper I Subscript l Baseline Endscripts sigma summation Underscript n equals 1 Overscript upper N Subscript upper I Sub Subscript l Baseline Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline EndFraction StartFraction partial differential ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline Over partial differential left parenthesis bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline right parenthesis EndFraction StartFraction partial differential left parenthesis bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline right parenthesis Over partial differential bold italic w Superscript l Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript i equals 1 Overscript upper I Subscript l Baseline Endscripts sigma summation Underscript n equals 1 Overscript upper N Subscript upper I Sub Subscript upper L Subscript Baseline Endscripts StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline EndFraction ring double struck 1 Subscript negative 1 less than or equals bold italic w Sub Superscript l Subscript ring bold italic k Sub Subscript n Sub Superscript l comma i Subscript less than or equals 1 Baseline ring bold italic k Subscript n Superscript l comma i Baseline comma EndLayout
∂LS
∂wl =
Il
Σ
i=1
NIl
Σ
n=1
∂LS
∂ˆk
l,i
n
∂ˆk
l,i
n
∂(wl ◦kl,i
n )
∂(wl ◦kl,i
n )
∂wl
=
Il
Σ
i=1
NIL
Σ
n=1
∂LS
∂ˆk
l,i
n
◦1−1≤wl◦kl,i
n ≤1 ◦kl,i
n ,
(5.138) 
StartFraction partial differential upper L Subscript upper B Baseline Over partial differential bold italic w Superscript l Baseline EndFraction equals lamda sigma summation Underscript i equals 1 Overscript upper I Subscript l Baseline Endscripts sigma summation Underscript n equals 1 Overscript upper N Subscript upper I Sub Subscript l Subscript Baseline Endscripts left parenthesis bold italic w Superscript l Baseline ring bold italic k Subscript n Superscript l comma i Baseline minus ModifyingAbove bold italic k With caret Subscript n Superscript l comma i Baseline right parenthesis ring bold italic k Subscript n Superscript l comma i Baseline period∂LB
∂wl = λ
Il
Σ
i=1
NIl
Σ
n=1
(wl ◦kl,i
n −ˆk
l,i
n ) ◦kl,i
n .
(5.139) 
Updating mu Subscript i Superscript lμl
i and sigma Subscript i Superscript lσ l
i Note that we use the same mu Subscript i Superscript lμl
i and sigma Subscript i Superscript lσ l
i for each kernel (see 
Sect. 3.2). So, the gradients here are scalars. The gradients delta Subscript mu Sub Subscript i Sub Superscript lδμl
i and delta Subscript sigma Sub Subscript i Sub Superscript lδσ l
i are calculated 
as: 
StartLayout 1st Row 1st Column Blank 2nd Column delta Subscript mu Sub Subscript i Sub Superscript l Baseline equals StartFraction partial differential upper L Over partial differential mu Subscript i Superscript l Baseline EndFraction equals StartFraction partial differential upper L Subscript upper B Baseline Over partial differential mu Subscript i Superscript l Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction lamda nu Over upper C Subscript i Superscript l Baseline times upper H Superscript l Baseline times upper W Superscript l Baseline EndFraction sigma summation Underscript n equals 1 Overscript upper C Subscript i Superscript l Baseline Endscripts sigma summation Underscript p equals 1 Overscript upper H Superscript l Baseline times upper W Superscript l Baseline Endscripts StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column left parenthesis sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 2 Baseline left parenthesis mu Subscript i Superscript l Baseline minus k Subscript n comma p Superscript l comma i Baseline right parenthesis comma 3rd Column k Subscript n comma p Superscript l comma i Baseline greater than or equals 0 comma 2nd Row 1st Column Blank 2nd Column left parenthesis sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 2 Baseline left parenthesis mu Subscript i Superscript l Baseline plus k Subscript n comma p Superscript l comma i Baseline right parenthesis comma 3rd Column k Subscript n comma p Superscript l comma i Baseline less than 0 comma EndLayout EndLayout
δμl
i = ∂L
∂μl
i
= ∂LB
∂μl
i
=
λν
Cl
i ×H l×W l
Cl
i
Σ
n=1
H l×W l
Σ
p=1
⎧
⎨
⎩
(σ l
i )−2(μl
i −kl,i
n,p),
kl,i
n,p ≥0,
(σ l
i )−2(μl
i + kl,i
n,p),
kl,i
n,p < 0,
(5.140)

208
5
Network Pruning
StartLayout 1st Row 1st Column Blank 2nd Column delta Subscript sigma Sub Subscript i Sub Superscript l Baseline equals StartFraction partial differential upper L Over partial differential sigma Subscript i Superscript l Baseline EndFraction equals StartFraction partial differential upper L Subscript upper B Baseline Over partial differential sigma Subscript i Superscript l Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction lamda nu Over upper C Subscript i Superscript l Baseline times upper H Superscript l Baseline times upper W Superscript l Baseline EndFraction sigma summation Underscript n equals 1 Overscript upper C Subscript i Superscript l Baseline Endscripts sigma summation Underscript p equals 1 Overscript upper H Superscript l Baseline times upper W Superscript l Baseline Endscripts StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column minus left parenthesis sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 3 Baseline left parenthesis k Subscript n comma p Superscript l comma i Baseline minus mu Subscript i Superscript l Baseline right parenthesis squared plus left parenthesis sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 1 Baseline comma k Subscript n comma p Superscript l comma i Baseline greater than or equals 0 comma 2nd Row 1st Column Blank 2nd Column minus left parenthesis sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 3 Baseline left parenthesis k Subscript n comma p Superscript l comma i Baseline plus mu Subscript i Superscript l Baseline right parenthesis squared plus left parenthesis sigma Subscript i Superscript l Baseline right parenthesis Superscript negative 1 Baseline comma k Subscript n comma p Superscript l comma i Baseline less than 0 comma EndLayout EndLayout
δσ l
i = ∂L
∂σ l
i
= ∂LB
∂σ l
i
=
λν
Cl
i×H l×W l
Cl
i
Σ
n=1
H l×W l
Σ
p=1
⎧
⎨
⎩
−(σ l
i )−3(kl,i
n,p−μl
i)2+(σ l
i )−1,kl,i
n,p ≥0,
−(σ l
i )−3(kl,i
n,p+μl
i)2+(σ l
i )−1,kl,i
n,p <0,
(5.141) 
where k Subscript n comma p Superscript l comma i Baseline comma p element of StartSet 1 comma ellipsis comma upper H Superscript l Baseline times upper W Superscript l Baseline EndSetkl,i
n,p, p ∈{1, . . . , H l × W l}, denotes the p-th element of bold italic k Subscript n Superscript l comma ikl,i
n . In the ﬁne-
tuning process, we update bold italic c Subscript mcm using the same strategy as center loss [64]. The update 
of sigma Subscript m comma nσm,n based on upper L Subscript upper BLB is straightforward and is not elaborated here for brevity. 
Updating bold italic upper K Subscript i comma j Superscript lKl
i,j In pruning, we aim to converge the ﬁlters to their mean gradually. 
So we replace each ﬁlter bold italic upper K Subscript i comma j Superscript lKl
i,j with its corresponding mean ModifyingAbove bold italic upper K With quotation dash Subscript i comma j Superscript lKl
i,j. The gradient of the 
mean is represented as follows: 
StartLayout 1st Row 1st Column StartFraction partial differential upper L Over partial differential bold italic upper K Subscript i comma j Superscript l Baseline EndFraction 2nd Column equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential bold italic upper K Subscript i comma j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper B Baseline Over partial differential bold italic upper K Subscript i comma j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper P Baseline Over partial differential bold italic upper K Subscript i comma j Superscript l Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline EndFraction StartFraction partial differential ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline Over partial differential bold italic upper K Subscript i comma j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper B Baseline Over partial differential ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline EndFraction StartFraction partial differential ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline Over partial differential bold italic upper K Subscript i comma j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper P Baseline Over partial differential bold italic upper K Subscript i comma j Superscript l Baseline EndFraction 3rd Row 1st Column Blank 2nd Column equals StartFraction 1 Over upper I Subscript j Baseline EndFraction left parenthesis StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper B Baseline Over partial differential ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline EndFraction right parenthesis plus 2 left parenthesis bold italic upper K Subscript i comma j Superscript l Baseline minus ModifyingAbove bold italic upper K With bold quotation dash Subscript j Baseline right parenthesis 4th Row 1st Column Blank 2nd Column plus 2 nu left parenthesis bold italic upper Psi Subscript j Superscript l Baseline right parenthesis Superscript negative 1 Baseline left parenthesis bold italic upper K Subscript i comma j Superscript l Baseline minus ModifyingAbove bold italic upper K With bold quotation dash Subscript j Baseline right parenthesis comma EndLayout
∂L
∂Kl
i,j
= ∂LS
∂Kl
i,j
+ ∂LB
∂Kl
i,j
+ ∂LP
∂Kl
i,j
= ∂LS
∂K
l
j
∂K
l
j
∂Kl
i,j
+ ∂LB
∂K
l
j
∂K
l
j
∂Kl
i,j
+ ∂LP
∂Kl
i,j
= 1
Ij
( ∂LS
∂K
l
j
+ ∂LB
∂K
l
j
)
+ 2(Kl
i,j −Kj)
+ 2ν(ψl
j)−1(Kl
i,j −Kj),
(5.142) 
where ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript l Baseline equals StartFraction 1 Over upper I Subscript j Baseline EndFraction sigma summation Underscript i equals 1 Overscript upper I Subscript j Baseline Endscripts bold italic upper K Subscript i comma j Superscript lK
l
j =
1
Ij
ΣIj
i=1 Kl
i,j that is used to update the ﬁlters in a group by mean 
ModifyingAbove bold italic upper K With bold quotation dash Subscript j Superscript lK
l
j. We leave the ﬁrst ﬁlter in each group to prune redundant ﬁlters and remove the 
others. However, such an operation changes the distribution of the input channel of 
the batch norm layer, resulting in a dimension mismatch for the next convolutional 
layer. To solve the problem, we keep the size of the batch norm layer, whose values 
correspond to the removed ﬁlters, set to zero. In this way, the removed information 
is retained to the greatest extent. In summary, we show that the proposed method 
is trainable from end to end. The learning procedure is detailed in Algorithms 11 
and 12 (Figs. 5.20 and 5.21). 
5.5.2.7 
Ablation Study 
Hyperparameter Selection In this section, we conduct evaluations to study the 
effects of hyperparameters on the performance of BONNs, speciﬁcally focusing on 
lamdaλ and thetaθ. These hyperparameters are used to balance the Bayesian kernel loss and the 
Bayesian feature loss, respectively, and are crucial in adjusting the distributions of

5.5
Network Pruning on BNNs
209
Algorithm 12: Pruning 1-bit CNNs with Bayesian learning 
An algo
rith m for prunin g 1-bi t C N Ns w ith B ayesian lea rni ng. 
5.137An algorithm for pruning 1-bit C N Ns with Bayesian learning. 5.142 
An 
algorit hm for prun ing
 1-bit C N
 N s  w
it h Baye sian 
lear
ning . 
kernels and features for better performance.We use wide residual networks (WRN-
22 and WRN-40) for our evaluations. The implementation details are provided 
below. 
In Table 5.20, we vary lamdaλ while setting thetaθ to zero to understand the inﬂuence of the 
Bayesian kernel loss on the kernel distribution. The results show that incorporating 
the Bayesian kernel loss effectively improves the accuracy on CIFAR-10. However, 
simply increasing lamdaλ does not lead to higher accuracy. Instead, ﬁnding an appropriate 
value of lamdaλ is essential to strike the right balance between the cross-entropy loss and 
the Bayesian kernel loss. For instance, when lamdaλ is set to 1 e minus 41e −4, we achieve the best 
classiﬁcation accuracy, indicating an optimal balance. 
Next, we study the effect of the hyperparameter thetaθ on the intraclass variations 
of features using different values of thetaθ. Similar to the observations with lamdaλ, the  
classiﬁcation accuracy varies with thetaθ, demonstrating that the Bayesian feature loss 
can contribute to better classiﬁcation accuracy when an appropriate value of thetaθ is 
chosen. 
Furthermore, we evaluate the convergence performance of our method in com-
parison to other methods using ResNet-18 on ImageNet ILSVRC12. The training 
curve of XNOR-Net shows vigorous oscillations, which suggests suboptimal learn-
ing. In contrast, our BONN achieves better training and test accuracy, indicating 
improved convergence performance.

210
5
Network Pruning
Fig. 5.20 The images on the left are the input images chosen from the ImageNet ILSVRC12 
dataset. Right images are feature maps and binary feature maps from different layers of BONNs. 
The ﬁrst and third rows are feature maps for each group, while the second and fourth rows are 
corresponding binary feature maps. Although binarization of the feature map causes information 
loss, BONNs could extract essential features for accurate classiﬁcation 
Effectiveness of Bayesian Binarization on ImageNet ILSVRC12 We experi-
mented with examining how each loss affects performance better to understand 
Bayesian losses on the large-scale ImageNet ILSVRC12 dataset. Based on the 
experiments described above, we set lamdaλ to 1 e minus 41e −4 and thetaθ to 1 e minus 31e −3 if they are used. 
As shown in Table 5.21, both the Bayesian kernel loss and Bayesian feature loss 
can independently improve the accuracy on ImageNet. When applied together, the 
Top-1 accuracy reaches the highest value of 59.3%. 
Weight Distribution Figure 5.22 further illustrates the distribution of the kernel 
weights, with lamdaλ ﬁxed to 1 e minus 41e −4. During the training process, the distribution 
gradually approaches the two-mode GMM, as assumed previously, conﬁrming 
the effectiveness of the Bayesian kernel loss in a more intuitive way. We also 
compare the kernel weight distribution between XNOR-Net and BONN. As shown 
in Fig. 5.23, the kernel weights learned in XNOR-Net are tightly distributed around

5.5
Network Pruning on BNNs
211
Epoch 
0
10
 20
 30
 40
 50
 60
 70
 
Accuracy
10 
15 
20 
25 
30 
35 
40 
45 
50 
55 
60 
Top-1 on ImageNet 
BONN-Train 
BONN-Test 
XNOR-Train 
XNOR-Test 
Epoch 
0
 10
20
30
40
50
60
70
 
Accuracy
20 
30 
40 
50 
60 
70 
80 
Top-5 on ImageNet 
BONN-Train 
BONN-Test 
XNOR-Train 
XNOR-Test 
Fig. 5.21 Training and test accuracies on ImageNet when λ = 1e −4 shows the superiority of the 
proposed BONN over XNOR-Net. The backbone of the two networks is ResNet-18 
Table 5.20 With different λ and θ, we evaluate the accuracies of BONNs based on WRN-22 
and WRN-40 on CIFAR-10/CIFAR-100. When varying λ, the Bayesian feature loss is not used 
(θ = 0). However, when varying θ, we choose the optimal loss weight (λ = 1e −4) for  the  
Bayesian kernel loss 
WRN-22 (BONN)
WRN-40 (BONN) 
Hyper-param.
CIFAR-10
CIFAR-100
CIFAR-10
CIFAR-100 
λ
1e −3
85.82
59.32
85.79
58.84 
1e −4
86.23
59.77
87.12
60.32 
1e −5
85.74
57.73
86.22
59.93 
0
84.97
55.38
84.61
56.03 
θ
1e −2
87.34
60.31
87.23
60.83 
1e −3
86.49
60.37
87.18
61.25 
1e −4
86.27
60.91
87.41
61.03 
0
86.23
59.77
87.12
60.32 
Table 5.21 Effect of 
Bayesian losses on the 
ImageNet dataset. The 
backbone is ResNet-18 
Bayesian kernel loss
x
Y
x
Y
Bayesian feature loss
x
x
Y
Y
Accuracy 
Top-1
56.3 
58.3 
58.4 
59.3 
Top-5
79.8 
80.8 
80.8 
81.6 
the threshold value, but those in BONN are regularized in a two-mode GMM style. 
Figure 5.24 shows the evolution of the binarized values during the training process 
of XNOR-Net and BONN. The two different patterns indicate that the binarized 
values learned in BONN are more diverse.

212
5
Network Pruning
Fig. 5.22 We demonstrate the kernel weight distribution of the ﬁrst binarized convolutional layer 
of BONNs. Before training, we initialize the kernels as a single-mode Gaussian distribution. From 
the 2-th epoch to the 200-th epoch, with lamdaλ ﬁxed to 1 e minus 41e −4, the distribution of the kernel weights 
becomes more and more compact with two modes, which conﬁrms that the Bayesian kernel loss 
can regularize the kernels into a promising distribution for binarization 
Fig. 5.23 The weight distributions of XNOR and BONN are based on WRN-22 (2nd, 8th, and 
14th convolutional layers) after 200 epochs. The weight distribution difference between XNOR and 
BONN indicates that the kernels are regularized across the convolutional layers with the proposed 
Bayesian kernel loss

References
213
–0.06 –0.04 –0.02 0.00 
0.02 
0.04 
0.06 –0.03 –0.02 –0.01 0.00 
0.01 
0.02 
0.03 –0.03 –0.02 –0.01 0.00 
0.01 
0.02 
0.03 
epoch=200 
Var=1.09e-05 
epoch=20 
Var=3.24e-05 
epoch=2 
Var=8.32e-05 
Fig. 5.24 Evolution of the binarized values, |x|s, during the XNOR and BONN training process. 
They are both based on WRN-22 (2nd, 3rd, 8th, and 14th convolutional layers), and the curves 
do not share the same y-axis. The binarized values of XNOR-Net tend to converge to small and 
similar values, but these of BONN are learned diversely 
Table 5.22 Effect of 
Bayesian feature loss on the 
ImageNet dataset. The core is 
ResNet-18 and ResNet-50 
with real value 
Model
ResNet-18 
ResNet-50 
Bayesian feature loss
x
Y
x
Y
Accuracy 
Top-1
69.3 
69.9 
76.6 
77.0 
Top-5
89.2 
89.8 
92.4 
92.7 
Effectiveness of Bayesian Feature Loss on Real-Valued Models 
We have applied our Bayesian feature loss on real-valued models, speciﬁcally 
ResNet-18 and ResNet-50 [19]. During retraining, we incorporated our Bayesian 
feature loss for 70 epochs, setting the hyperparameter θ to 1e −3. The SGD 
optimizer was used with an initial learning rate of 0.1, and a learning rate schedule 
that decreases to 10% every 30 epochs. 
The results, as shown in Table 5.22, demonstrate that our Bayesian feature loss 
signiﬁcantly improves the performance of models with real values. Speciﬁcally, 
the Top-1 accuracies of ResNet-18 and ResNet-50 are boosted by 0.6% and 0.4%, 
respectively. 
References 
1. Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In 
Advances in neural information processing systems, pages 2270–2278, 2016. 
2. Jose M Alvarez and Mathieu Salzmann. Compression-aware training of deep networks. In 
Advances in Neural Information Processing Systems, pages 856–867, 2017. 
3. Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear 
inverse problems. SIAM journal on imaging sciences, 2(1):183–202, 2009. 
4. Christopher M Bishop. Bayesian neural networks. Journal of the Brazilian Computer Society, 
4(1):61–68, 1997. 
5. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncer-
tainty in neural network. In Proceedings of the International Conference on Machine Learning, 
pages 1613–1622, 2015.

214
5
Network Pruning
6. Neill DF Campbell, George Vogiatzis, Carlos Hernández, and Roberto Cipolla. Using multiple 
hypotheses to improve depth-maps for multi-view stereo. In Proceedings of the European 
Conference on Computer Vision (ECCV), pages 766–779. Springer, 2008. 
7. Robert T Collins. A space-sweep approach to true multi-image matching. In Proceedings 
CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 
358–363. IEEE, 1996. 
8. Xiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong Han. Centripetal sgd for pruning 
very deep convolutional networks with complicated structure. In CVPR, pages 4943–4953, 
2019. 
9. Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicated 
network with less inference complexity. In IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR), pages 5840–5848, 2017. 
10. Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE 
transactions on pattern analysis and machine intelligence, 32(8):1362–1376, 2009. 
11. Silvano Galliani, Katrin Lasinger, and Konrad Schindler. 
Massively parallel multiview 
stereopsis by surface normal diffusion. In Proceedings of the IEEE International Conference 
on Computer Vision, pages 873–881, 2015. 
12. Andre Gaschler, Darius Burschka, and Gregory Hager. Epipolar-based stereo tracking without 
explicit 3d reconstruction. In 2010 20th International Conference on Pattern Recognition, 
pages 1755–1758. IEEE, 2010. 
13. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In 
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 315–323, 
2011. 
14. Tom Goldstein, Christoph Studer, and Richard Baraniuk. A ﬁeld guide to forward-backward 
splitting with a fasta implementation. arXiv preprint arXiv:1411.3406, 2014. 
15. Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David 
Doermann. 
Projection convolutional neural networks for 1-bit cnns via discrete back 
propagation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2019. 
16. Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Jianzhuang Liu, Guodong Guo, and 
Rongrong Ji. Bayesian optimized 1-bit cnns. In Proceedings of the IEEE/CVF international 
conference on computer vision, pages 4909–4917, 2019. 
17. Shuhang Gu, Wangmeng Zuo, Qi Xie, Deyu Meng, Xiangchu Feng, and Lei Zhang. Convolu-
tional sparse coding for image super-resolution. In ICCV, pages 1823–1831, 2015. 
18. Trevor Hastie, Robert Tibshirani, Jerome Friedman, and James Franklin. 
The elements of 
statistical learning: data mining, inference and prediction. The Mathematical Intelligencer, 
27(2):83–85, 2005. 
19. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 
Deep residual learning for 
image recognition. In Proceedings of the IEEE conference on computer vision and pattern 
recognition, pages 770–778, 2016. 
20. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image 
recognition. In CVPR, pages 770–778, 2016. 
21. Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. 
Soft ﬁlter pruning 
for accelerating deep convolutional neural networks. In International Joint Conference on 
Artiﬁcial Intelligence (IJCAI), pages 2234–2240, 2018. 
22. Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median 
for deep convolutional neural networks acceleration. In CVPR, pages 4340–4349, 2019. 
23. Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural 
networks. In ICCV, pages 1398–1406, 2017. 
24. Felix Heide, Wolfgang Heidrich, and Gordon Wetzstein. Fast and ﬂexible convolutional sparse 
coding. In CVPR, pages 5135–5143, 2015. 
25. Andrew Howard. 
Real-time stereo visual odometry for autonomous ground vehicles. 
In 
2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3946–3952. 
IEEE, 2008.

References
215
26. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, 
Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. 
In IEEE International Conference on Computer Vision (ICCV), pages 1314–1324, 2019. 
27. Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: 
Learning multi-view stereopsis. In Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition, pages 2821–2830, 2018. 
28. Zehao Huang and Naiyan Wang. 
Data-driven sparse structure selection for deep neural 
networks. In ECCV, pages 304–320, 2018. 
29. Zehao Huang and Naiyan Wang. 
Data-driven sparse structure selection for deep neural 
networks. In ECCV, pages 304–320, 2018. 
30. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training 
by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 
31. Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale 
multi-view stereopsis evaluation. In Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition, pages 406–413, 2014. 
32. Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d 
neural network for multiview stereopsis. In Proceedings of the IEEE International Conference 
on Computer Vision, pages 2307–2315, 2017. 
33. Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham 
Bachrach, and Adam Bry. 
End-to-end learning of geometry and context for deep stereo 
regression. In Proceedings of the IEEE International Conference on Computer Vision, pages 
66–75, 2017. 
34. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 
2009. 
35. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep 
convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 
pages 1097–1105, 2012. 
36. Jouko Lampinen and Aki Vehtari. Bayesian approach for neural networks—review and case 
studies. Neural networks, 14(3):257–274, 2001. 
37. Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-
supervised nets. In Artiﬁcial intelligence and statistics, pages 562–570, 2015. 
38. Thomas Lemaire, Cyrille Berger, Il-Kyun Jung, and Simon Lacroix. Vision-based slam: Stereo 
and monocular approaches. International Journal of Computer Vision, 74(3):343–364, 2007. 
39. Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural 
network: Squeeze the last bit out with admm. In Proceedings of the AAAI Conference on 
Artiﬁcial Intelligence, pages 3466–3473, 2018. 
40. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for 
efﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016. 
41. Faming Liang, Qizhai Li, and Lei Zhou. 
Bayesian neural networks for selection of drug 
sensitive genes. Journal of the American Statistical Association, 113(523):955–972, 2018. 
42. Shaohui Lin, Rongrong Ji, Yuchao Li, Yongjian Wu, Feiyue Huang, and Baochang Zhang. 
Accelerating convolutional networks via global & dynamic ﬁlter pruning. In IJCAI, pages 
2425–2432, 2018. 
43. Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, and David Doermann. Towards 
optimal structured cnn pruning via generative adversarial learning. In CVPR, 2019. 
44. Chunlei Liu, Wenrui Ding, Yuan Hu, Baochang Zhang, Jianzhuang Liu, Guodong Guo, 
and David Doermann. Rectiﬁed binary convolutional networks with generative adversarial 
learning. International Journal of Computer Vision, 129:998–1012, 2021. 
45. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real 
net: Enhancing the performance of 1-bit cnns with improved representational capability and 
advanced training algorithm. In Proceedings of the European conference on computer vision 
(ECCV), pages 722–737, 2018. 
46. Jianhao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁlter level pruning method for deep 
neural network compression. In ICCV, pages 5068–5076, 2017.

216
5
Network Pruning
47. Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix 
factorization and sparse coding. Journal of Machine Learning Research, 11(Jan):19–60, 2010. 
48. M. Menze and A. Geiger. Object scene ﬂow for autonomous vehicles. In Proceedings of the 
IEEE Conference on Computer Vision and Pattern Recognition, pages 3061–3070, 2015. 
49. Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of bayesian methods 
for seeking the extremum. Towards global optimization, 2(117-129):2, 1978. 
50. Neal Parikh, Stephen Boyd, et al. 
Proximal algorithms. 
Foundations and Trends® in 
Optimization, 1(3):127–239, 2014. 
51. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, 
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative 
style, high-performance deep learning library. In Advances in Neural Information Processing 
Systems, pages 8024–8035, 2019. 
52. Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical 
University of Denmark, 7(15):510, 2008. 
53. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet 
classiﬁcation using binary convolutional neural networks. 
In European Conference on 
Computer Vision, pages 525–542. Springer, 2016. 
54. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 
Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 4510–4520, 2018. 
55. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale 
image recognition. arXiv preprint arXiv:1409.1556, 2014. 
56. Pravendra Singh, Vinay Kumar Verma, Piyush Rai, and Vinay Namboodiri. Leveraging ﬁlter 
correlations for deep model compression. In The IEEE Winter Conference on Applications of 
Computer Vision (WACV), pages 835–844, 2020. 
57. Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty 
in bayesian neural networks. In Proceedings of the Artiﬁcial Intelligence and Statistics, pages 
1283–1292, 2017. 
58. Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. 
Functional variational 
bayesian neural networks. 
In Proceedings of the International Conference on Learning 
Representations, pages 1–22, 2019. 
59. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal 
Statistical Society: Series B (Methodological), 58(1):267–288, 1996. 
60. Engin Tola, Christoph Strecha, and Pascal Fua. Efﬁcient large-scale multi-view stereo for ultra 
high-resolution image sets. Machine Vision and Applications, 23(5):903–920, 2012. 
61. Hanzi Wang, Daniel Mirota, Masaru Ishii, and Gregory D Hager. Robust motion estimation and 
structure recovery from endoscopic image sequences with an adaptive scale kernel consensus 
estimator. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–7. 
IEEE, 2008. 
62. Runqi Wang, Baochang Zhang, Li’an Zhuo, Qixiang Ye, and David Doermann. Cogradient 
descent for dependable learning. arXiv preprint arXiv:2106.10617, 2021. 
63. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity 
in deep neural networks. In Advances in neural information processing systems, pages 2074– 
2082, 2016. 
64. Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning 
approach for deep face recognition. In European Conference on Computer Vision (ECCV), 
pages 499–515, 2016. 
65. Xiang Xiang. A brief review on visual tracking methods. In Third Chinese Conference on 
Intelligent Visual Surveillance, 2011. 
66. Xiang Xiang, Daniel Mirota, Austin Reiter, and Gregory D Hager. Is multi-model feature 
matching better for endoscopic motion estimation? In International Workshop on Computer-
Assisted and Robotic Endoscopy, pages 88–98. Springer, 2014. 
67. Xiang Xiang, Zhiyuan Wang, Shan Lao, and Baochang Zhang. Pruning multi-view stereo net 
for efﬁcient 3d reconstruction. Isprs Journal of Photogrammetry and Remote Sensing, 168:17– 
27, 2020.

References
217
68. Sheng Xu, Hanlin Chen, Xuan Gong, Kexin Liu, Jinhu Lu, and Baochang Zhang. Efﬁcient 
structured pruning based on deep feature stabilization. Neural Computing and Applications, 
33:7409–7420, 2021. 
69. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for 
unstructured multi-view stereo. 
In Proceedings of the European Conference on Computer 
Vision (ECCV), pages 767–783, 2018. 
70. Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethinking the smaller-norm-less-informative 
assumption in channel pruning of convolution layers. In ICLR, 2018. 
71. Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei 
Gao, Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance 
score propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, pages 9194–9203, 2018. 
72. Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. 
Deconvolutional 
networks. In CVPR, pages 2528–2535. IEEE, 2010. 
73. Ziming Zhang and Venkatesh Saligrama. 
Rapid: Rapidly accelerated proximal gradient 
algorithms for convex minimization. In International Conference on Acoustics, Speech and 
Signal Processing (ICASSP), pages 3796–3800, 2015. 
74. et al. Zhang K, Schölkopf B. Learning causality and causality-related learning: some recent 
progress. National science review, 2018. 
75. Yuefu Zhou, Ya Zhang, Yanfeng Wang, and Qi Tian. Accelerate cnn via recursive bayesian 
pruning. In ICCV, pages 3306–3315, 2019. 
76. Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. 
Unet++: A nested u-net architecture for medical image segmentation. In Deep Learning in 
Medical Image Analysis and Multimodalf Learning for Clinical Decision Support, pages 3–11. 
Springer, 2018. 
77. Barret Zoph and Quoc V Le. 
Neural architecture search with reinforcement learning. 
In 
International Conference on Learning Representations (ICLR), pages 1–16, 2017.

Chapter 6 
Applications 
6.1 
Introduction 
The success of binary neural networks makes it possible to apply deep learning 
models to edge computing. Neural network models have been used in various real 
tasks with the help of these binary methods, including image classiﬁcation, image 
classiﬁcation, speech recognition, and object detection and tracking. In this section, 
we introduce the applications of binary neural networks in these ﬁelds. 
In this chapter, we introduce the applications of binary neural networks in com-
puter vision. Speciﬁcally, we introduce the vision tasks, including object detection, 
speech recognition, person reidentiﬁcation, and 3D point cloud processing. 
6.2 
Image Classiﬁcation 
Image classiﬁcation aims to group images into different semantic classes together. 
Many works regard the completion of image classiﬁcation as the criterion for 
the success of binary neural networks. Five datasets are commonly used for 
image classiﬁcation tasks: MNIST [42], SVHN, CIFAR-10 [26], CIFAR-100, and 
ImageNet [52]. Among them, ImageNet is the most difﬁcult to train and consists of 
100 classes of images. Table 2.1 shows the experimental results of some of the most 
popular binary methods on ImageNet. 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
B. Zhang et al., Neural Networks with Model Compression, 
Computational Intelligence Methods and Applications, 
https://doi.org/10.1007/978-981-99-5068-3_6
219

220
6
Applications
6.3 
Speech Recognition 
Speech recognition is a technique or capability that enables a program or system to 
process human speech. We can use binary methods to complete speech recognition 
tasks in edge computing devices. 
Xiang et al. [67] applied binary DNNs to speech recognition tasks. Experiments 
on TIMIT phone recognition and 50-hour Switchboard speech recognition show that 
binary DNNs can run about four times faster than standard DNNs during inference, 
with roughly 10.0% relative accuracy reduction. 
Zheng et al. [74] and Yin et al. [71] also implement binarized convolutional 
neural network-based speech recognition tasks. 
6.3.1 
1-Bit WaveNet: Compression of a Generative Neural 
Network in Speech Recognition with Two Binarized 
Methods 
Instead of traditional speech recognition applications on remote servers, speech 
recognition is gradually becoming popular on mobile devices. However, signiﬁcant 
memory and computational resource requirements restrict full-precision neural 
networks. Before solving the hardware deployment problem on mobile devices, we 
needed more parameters to run or store these DCNNs. To resolve these challenges, 
Rastegari et al. [47] use binary operations to approximate convolutions using 
binarized kernel weights and input. In recent years, Zhou et al. presented DoReFa-
Net [76] that could speed training and inference with 1-bit convolution ﬁlters with 
low bit width parameter gradients. Lin et al. [33] binarized multiple activations 
and weights to approximately replace the weights of real value. Consequently, the 
degradation of prediction accuracy is decreased. Zhuang et al. proposed a two-
stage optimization method to quantize weights and activations and then train a 
4-bit model. This results in no performance reduction compared to its real value 
counterpart at the baseline. McDonnell [39] achieves equivalent binarized results 
compared to the basic baseline by applying scaling factors to balance each layer 
with constant unlearned values and standard deviations speciﬁc to the initial layer. 
In contrast to baseline models, Wang et al. [61] proposed MCNs that only substitute 
full-precision kernels with binarized parameters and obtain excellent performance. 
Although these BNNs save considerable memory and computational power, 
the accuracy of vision or speech tasks is reduced. The main explanations are as 
follows: (1) In previous work, they rarely resolved the process of CNN binarization 
by discrete optimization [10]. (2) Considering binarized and ﬁlter losses, existing 
approaches could not trade them off well. 
In this work, considering the outstanding achievements of BNNs in computer 
vision and existing binary RNN research in speech recognition [46], we propose 
a new binarization application through two technologies (Bi-Real Net [37] and

6.3
Speech Recognition
221
PCNN [15]) on WaveNet to accomplish our speech keyword recognition mission 
and acquire the closest-to-baseline accuracy with subtle numerical error. We 
demonstrate the principle of our new 1-bit WaveNet via extraordinary dilated causal 
convolutions and residual blocks, which compress the baseline up to a third of its 
original size with similar accuracy. Three technical novelties of our work include 
the following: (1) the entire framework of our new 1-bit WaveNet [11] based on 
binary dilated causal convolutions, which enlarge receptive ﬁelds, is presented in 
our speech keyword recognition tasks to save memory and computational resources; 
(2) a new application in speech recognition is proposed by binarization of 1D 
convolution; and (3) an audio keywords dataset that could be tested by our model 
and prepared to facilitate future research is collected and labeled. 
6.3.1.1 
Network Architecture 
In this work, we propose a 1-bit neural network model [23] based on WaveNet that 
has achieved exceptional performance on the raw audio waveform. WaveNet is a 
deep autoregressive neural network with a point-by-point sampling method, and it 
could achieve high-quality audio via a conditional probability formula as follows: 
upper P left parenthesis upper X right parenthesis equals product Underscript n equals 1 Overscript upper N Endscripts upper P left parenthesis x Subscript n Baseline vertical bar x 1 comma ellipsis comma x Subscript n minus 1 Baseline right parenthesisP(X) =
N
||
n=1
P (xn | x1, . . . , xn−1)
(6.1) 
where past speech samples from previous steps generate each x Subscript nxn. 
Figure 6.1 shows the 1-bit WaveNet architecture, which contains a preprocessing 
data module that converts raw clear keyword spectrogram data into Mel frequency 
cepstrum coefﬁcient (MFCC) and then inputs these data into the main network, the 
principal part of this WaveNet, which is composed of several residual blocks, and 
a DenseNet that ensures that the outputs could be distributed as a categorical form 
to facilitate the calculation, and, meanwhile, it could solve the overﬂow problem in 
the model. 
Dilated Causal Convolutions This WaveNet model is based on PixelCNN [56], 
which discarded the pooling layers in the architecture but used a unique 1D 
convolution many times, called the causal convolution. The modeling process could 
certainly be in the correct time sequence, that is, the output upper P left parenthesis x Subscript n plus 1 Baseline vertical bar x 1 comma ellipsis comma x Subscript n Baseline right parenthesisP (xn+1 | x1, . . . , xn)
produced by the model can only be generated from the present steps, but not from 
the predictions of the future. After predicting each audio sample, the model receives 
and applies it to the next prediction. Causal convolution-trained complicated 
sequences save more time than traditional RNNs such as LSTMs or GRUs [44] 
due to cutoff recurrent connections. Furthermore, convolutions with holes (dilated 
convolutions) include a new hyperparameter named the dilation rate to increase 
kernels’ reception ﬁeld efﬁciently. Similarly to subsampled layers, the output and 
the input of dilated convolutions have an equivalent size. Our model could use 
only several layers to enlarge the receptive ﬁelds with considerable input resolution

222
6
Applications
Spectogram
MFCC 
Residual 
Block 
Labels 
CTC 
Loss 
DensNet 
Fig. 6.1 In the 1-bit WaveNet, a new binarized application via Bi-Real Net and PCNN is used to 
compress the speech recognition model. This ﬂowchart illustrates the network architecture with all 
techniques in this work. See the text for a detailed description of the model 
and reasonable computational resources. In our work, we apply ﬁve layers with the 
dilation from 2 Superscript 020 to 2 Superscript 424. 
Gated Activation Units and Residual Blocks We used the same gated activation 
unit as the original WaveNet [44]: 
bold z equals hyperbolic tangent left parenthesis upper W Subscript f comma k Baseline asterisk bold x right parenthesis circled dot sigma left parenthesis upper W Subscript g comma k Baseline asterisk bold x right parenthesisz = tanh
(
Wf,k ∗x
)
O σ
(
Wg,k ∗x
)
(6.2) 
where x is the output of dilated causal convolutions, W is a trainable convolution 
kernel, circled dotO denotes the Hadamard product, asterisk∗denotes the convolutional operation, 
and sigma left parenthesis dot right parenthesisσ(·) is a sigmoid function whose nonlinearity works better than other activation 
functions in speech recognition tasks [41]. 
This model uses three residual blocks to accelerate the convergence process when 
training deep convolutional neural networks. Figure 6.2 illustrates more details 
about one of our residual blocks.

6.3
Speech Recognition
223
Fig. 6.2 The proposed residual block modiﬁed from WaveNet [44], which adds some of the 
BatchNorm layers and our new 1-bit causal convolution.circled times⊗denotes the element-wise multiplication 
operator 
6.3.1.2 
Bi-Real Net Binarization 
In our neural network, we binarized weights through a sign function in 1-bit dilated 
causal convolutions: 
w Subscript b Baseline equals upper S i g n left parenthesis w Subscript r Baseline right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column negative 1 2nd Column if w Subscript r Baseline less than 0 2nd Row 1st Column plus 1 2nd Column otherwise EndLayoutwb = Sign (wr) =
{ −1 if wr < 0
+1 otherwise
(6.3) 
where w Subscript rwr denotes the real weight. In the backward propagation of the training 
process, we will use the real weight to update the binary weights called the 
magnitude-aware gradient regarding the weights [37] to update the binary weights, 
i.e., bold upper W Subscript b Superscript l Baseline element of StartSet negative 1 comma plus 1 EndSetWl
b ∈{−1, +1}. Because convolutions are one-dimensional in our 1-bit 
WaveNet, bold upper W Subscript b Superscript lWl
b in the following equations is a one-dimensional vector. Traditional 
gradient descent is too small to update binary weights, so Courbariaux proposed 
a training approach that used a full-precision weight and a sign function [5]. 
Therefore, bold upper W Subscript r Superscript lWl
r will be changed in the back propagation as follows: 
bold upper W Subscript r Superscript l comma t plus 1 Baseline equals bold upper W Subscript r Superscript l comma t Baseline minus eta StartFraction partial differential script upper L Over partial differential bold upper W Subscript r Superscript l comma t Baseline EndFraction equals bold upper W Subscript r Superscript l comma t Baseline minus eta StartFraction partial differential script upper L Over partial differential bold upper W Subscript b Superscript l comma t Baseline EndFraction StartFraction partial differential bold upper W Subscript b Superscript l comma t Baseline Over partial differential bold upper W Subscript r Superscript l comma t Baseline EndFractionWl,t+1
r
= Wl,t
r −η ∂L
∂Wl,t
r
= Wl,t
r −η ∂L
∂Wl,t
b
∂Wl,t
b
∂Wl,t
r
(6.4)

224
6
Applications
StartFraction partial differential bold upper W Subscript b Superscript l comma t Baseline left parenthesis i comma j right parenthesis Over partial differential bold upper W Subscript r Superscript l comma t Baseline left parenthesis i comma j right parenthesis EndFraction equals StartLayout Enlarged left brace 1st Row 1st Column 1 2nd Column if bold upper W Subscript r Superscript l comma t Baseline left parenthesis i comma j right parenthesis element of left bracket negative 1 comma 1 right bracket 2nd Row 1st Column 0 2nd Column otherwise EndLayout∂Wl,t
b (i, j)
∂Wl,t
r (i, j)
=
{
1 if Wl,t
r (i, j) ∈[−1, 1]
0 otherwise
(6.5) 
where StartFraction partial differential bold upper W Subscript b Superscript l comma t Baseline left parenthesis i comma j right parenthesis Over partial differential bold upper W Subscript r Superscript l comma t Baseline left parenthesis i comma j right parenthesis EndFraction ∂Wl,t
b (i,j)
∂Wl,t
r (i,j) denotes the element-wise derivative and StartFraction partial differential script upper L Over partial differential bold upper W Subscript b Superscript l comma t Baseline EndFraction ∂L
∂Wl,t
b
is derived from the 
chain rule. Given that, we proposed a new magnitude-aware function to substitute 
the sign function as follows: 
ModifyingAbove bold upper W With quotation dash Subscript b Superscript l comma t Baseline equals StartFraction double vertical bar bold upper W Subscript r Superscript l comma t Baseline double vertical bar Subscript 1 comma 1 Baseline Over StartAbsoluteValue bold upper W Subscript r Superscript l comma t Baseline EndAbsoluteValue EndFraction upper S i g n left parenthesis bold upper W Subscript r Superscript l comma t Baseline right parenthesisW
l,t
b =
||||||Wl,t
r
||||||
1,1
|||Wl,t
r
|||
Sign
(
Wl,t
r
)
(6.6) 
Hence, upper W Subscript r Superscript lW l
r updates to: 
bold upper W Subscript r Superscript l comma t plus 1 Baseline equals bold upper W Subscript r Superscript l comma t Baseline minus eta StartFraction partial differential script upper L Over partial differential ModifyingAbove bold upper W With quotation dash Subscript b Superscript l comma t Baseline EndFraction StartFraction partial differential ModifyingAbove bold upper W With quotation dash Subscript b Superscript l comma t Baseline Over partial differential bold upper W Subscript r Superscript l comma t Baseline EndFractionWl,t+1
r
= Wl,t
r −η ∂L
∂W
l,t
b
∂W
l,t
b
∂Wl,t
r
(6.7) 
where 
StartFraction partial differential ModifyingAbove bold upper W With quotation dash Subscript b Superscript l comma t Baseline Over partial differential bold upper W Subscript r Superscript l comma t Baseline EndFraction almost equals StartFraction double vertical bar bold upper W Subscript r Superscript l comma t Baseline double vertical bar Subscript 1 comma 1 Baseline Over StartAbsoluteValue bold upper W Subscript r Superscript l comma t Baseline EndAbsoluteValue EndFraction∂W
l,t
b
∂Wl,t
r
≈
||||||Wl,t
r
||||||
1,1
|||Wl,t
r
|||
(6.8) 
StartFraction partial differential upper S i g n left parenthesis bold upper W Subscript r Superscript l comma t Baseline right parenthesis Over partial differential bold upper W Subscript r Superscript l comma t Baseline EndFraction almost equals bold 1 Subscript StartAbsoluteValue bold upper W Sub Subscript r Sub Superscript l comma t Subscript EndAbsoluteValue less than 1
∂Sign
(
Wl,t
r
)
∂Wl,t
r
≈1|||Wl,t
r
|||<1
(6.9) 
and theta overbar Superscript l comma t¯θl,t is related to the magnitude of upper W Subscript b Superscript l comma tW l,t
b . Eventually, as we show in Fig. 6.3, this  
magnitude-aware function changes the weight’s sign obviously so that the stochastic 
gradient descent (SGD) could not achieve this signiﬁcant effect [37]. 
6.3.1.3 
Projection Convolutional Neural Network Binarization 
Projection Loss Figure 6.4 shows the projection convolutional neural network 
(PCNN) strategy that uses a discrete back propagation via projection [15] to  
compress our model. The projection loss obtained by optimization is as follows: 
upper L Subscript p Baseline equals StartFraction lamda Over 2 EndFraction sigma summation Underscript l comma i Overscript upper L comma upper I Endscripts sigma summation Underscript j Overscript upper J Endscripts double vertical bar ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket Baseline minus upper W overTilde Subscript j Superscript l comma left bracket k right bracket Baseline circled times left parenthesis upper C Subscript i Superscript l comma left bracket k right bracket Baseline plus eta delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l Subscript comma left bracket k right bracket Baseline right parenthesis double vertical bar squaredLp = λ
2
L,I
Σ
l,i
J
Σ
j
|||||| ˆCl,[k]
i,j
−-
W l,[k]
j
⊗
(
Cl,[k]
i
+ ηδ ˆCl
i,j ,[k]
)||||||
2
(6.10)

6.3
Speech Recognition
225
Fig. 6.3 Illustration of the 
training architecture of 1-bit 
WaveNet by Bi-Real Net. 
Note that W is weighted; the 
superscript l means the lth 
block, which includes 
magnitude-aware sign, 1-bit 
causal convolution, and 
BatchNorm; and the subscript 
r and b denote real values and 
binary values, respectively 
where circled times⊗denotes the element-wise multiplication operator, J is the total number 
of projections, l denotes the layer index, left bracket k right bracket[k] is the iteration index, and i, j is the 
kernel index. The projection index and lamdaλ, respectively, denote a trade-off scaler 
for the projection loss. In speech recognition, W is generally a 1D projection 
vector. Speciﬁcally, in the kth iteration, upper C Subscript i Superscript l comma left bracket k right bracketCl,[k]
i
means the ith ﬁlter vector of the 
lth convolutional layer and ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket Baseline equals upper P Subscript upper Omega Superscript l comma j Baseline left parenthesis upper W overTilde Subscript j Superscript l comma left bracket k right bracket Baseline circled times upper C Subscript i Superscript l comma left bracket k right bracket Baseline right parenthesis ˆCl,[k]
i,j
= P l,j
O
(
-
W l,[k]
j
⊗Cl,[k]
i
)
denotes the binarized 
kernel of upper C Subscript i Superscript l comma left bracket k right bracketCl,[k]
i
that includes a duplicated dimensional-corresponding projection 
vector upper W overTilde Subscript j Superscript l comma left bracket k right bracket -
W l,[k]
j
. eta delta Subscript ModifyingAbove upper C With caret Sub Subscript i comma j Sub Superscript l Subscript comma left bracket k right bracketηδ ˆCl
i,j ,[k] means the gradient of ModifyingAbove upper C With caret Subscript i comma j Superscript l comma left bracket k right bracket ˆCl,[k]
i,j
from the loss of CTC at the 
beginning [14]. Figure 6.4 shows the principle of projection loss. We omit left bracket k right bracket[k] in the 
following content for convenience. In our 1-bit WaveNets, we should calculate both 
the CTC loss and projection loss as the total loss as follows: 
upper L equals upper L Subscript upper C Baseline plus upper L Subscript upper PL = LC + LP
(6.11) 
Forward Propagation In PCNNs, we concatenate each ˆCl 
i,j that is binarized from 
the relevant real value ﬁlter Cl 
i,jto deﬁne the kernel Dl 
i. 
upper D Subscript i Superscript l Baseline equals ModifyingAbove upper C With caret Subscript i comma 1 Superscript l Baseline circled plus ModifyingAbove upper C With caret Subscript i comma 2 Superscript l Baseline circled plus midline horizontal ellipsis circled plus ModifyingAbove upper C With caret Subscript i comma upper J Superscript lDl
i = ˆCl
i,1 ⊕ˆCl
i,2 ⊕· · · ⊕ˆCl
i,J
(6.12) 
Note that ⊕ is the convolutional concatenating operation. From Dl and F l, we  
achieve the projection convolution and then calculate the feature map F l+1 of the 
next layer: 
upper F Superscript l plus 1 Baseline equals upper C o n v 1 upper D left parenthesis upper F Superscript l Baseline comma upper D Superscript l Baseline right parenthesisF l+1 = Conv 1D
(
F l, Dl)
(6.13)

226
6
Applications
Fig. 6.4 In one-dimensional PCNNs, we propose a discrete back propagation through projection 
to binarize our WaveNet end-to-end [15]. By using the projection, we binarize the convolutional 
ﬁlters of real value upper C Subscript i Superscript lCl
i to the binary counterpart h a t upper C Subscript i comma j Superscript lhatCl
i,j. Solid and dashed lines indicate the paths 
of upper L Subscript upper CLC and upper L Subscript upper PLP . Note that circled plus⊕is the convolutional concatenation operation in the network 
where Conv 1D is the ordinary 1D convolution. We use this method to ﬁt the 
dimensional difference between 1D convolutions and 2D convolutional ﬁlters: 
upper F Subscript h comma j Superscript l plus 1 Baseline equals sigma summation Underscript i comma h Endscripts upper F Subscript h Superscript l Baseline ring upper D Subscript i comma j Superscript lF l+1
h,j =
Σ
i,h
F l
h ◦Dl
i,j
(6.14) 
upper F Subscript h Superscript l plus 1 Baseline equals upper F Subscript h comma 1 Superscript l Baseline circled plus midline horizontal ellipsis circled plus upper F Subscript h comma upper J Superscript lF l+1
h
= F l
h,1 ⊕· · · ⊕F l
h,J
(6.15) 
where ◦ denotes the convolutional operation and h is the index of the feature map.

6.4
Object Detection and Tracking
227
Backward Propagation Taking into account projection loss, we should train and 
update the real value ﬁlters Cl 
i and the projection matrix W l by these formulas as 
follows: 
delta Subscript upper C Sub Subscript i Sub Superscript l Baseline equals StartFraction partial differential upper L Over partial differential upper C Subscript i Superscript l Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper C Subscript i Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper P Baseline Over partial differential upper C Subscript i Superscript l Baseline EndFractionδCl
i = ∂L
∂Cl
i
= ∂LS
∂Cl
i
+ ∂LP
∂Cl
i
(6.16) 
upper C Subscript i Superscript l Baseline left arrow upper C Subscript i Superscript l Baseline minus eta 1 delta Subscript upper C Sub Subscript i Sub Superscript lCl
i ←Cl
i −η1δCl
i
(6.17) 
delta Subscript upper W Sub Subscript j Sub Superscript l Baseline equals StartFraction partial differential upper L Over partial differential upper W Subscript j Superscript l Baseline EndFraction equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper W Subscript j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper P Baseline Over partial differential upper W Subscript j Superscript l Baseline EndFractionδW l
j = ∂L
∂W l
j
= ∂LS
∂W l
j
+ ∂LP
∂W l
j
(6.18) 
upper W Subscript j Superscript l Baseline left arrow upper W Subscript j Superscript l Baseline minus eta 2 delta Subscript upper W Sub Subscript j Sub Superscript lW l
j ←W l
j −η2δW l
j
(6.19) 
Note that η1, η2 is the learning rate of the convolutional ﬁlters and W l 
j. We could 
substitute PCNNs for ordinary convolutions due to the dimensional consistency of 
two continuous layers [15]. 1-bit WaveNets achieve a decrease in computational 
resources using this method. 
6.4 
Object Detection and Tracking 
Object detection is the process of ﬁnding a target from a scene, while object 
tracking is the follow-up of a target in consecutive frames in a video. Deep learning-
based object detection can generally be classiﬁed into two categories: two-stage 
and single-stage object detection. Two-stage detectors, for example, Faster R-
CNN [49], FPN [29], and Cascade R-CNN [4], generate region proposals in the 
ﬁrst stage and reﬁne them in the second. In localization, R-CNN [13] utilizes the L2 
norm between predicted and target offsets as the object function, which can cause 
gradient explosions when errors are signiﬁcant. Fast R-CNN [12] and Faster R-
CNN [49] proposed a smooth loss of L1 that keeps the gradient of large prediction 
errors consistent. One-stage detectors, e.g., RetinaNet [30] and YOLO [48], classify 
and regress objects concurrently, which are highly efﬁcient but suffer from lower 
accuracy. Recent methods [51, 72] have been used to improve localization accuracy 
using IoU-related values (insertion over union) as regression targets. IoU loss [72] 
utilized the negative log of IoU as object functions directly, which incorporates the 
dependency between box coordinates and adapts to multi-scale training. GIoU [51] 
extends the IoU loss to nonoverlapping cases by considering the shape properties of 
the compared objects. CIoU loss [75] incorporates more geometric measurements, 
that is, overlap area, central point distance, and aspect ratio, and achieves better 
convergence.

228
6
Applications
Table 6.1 Results reported 
in Liu et al. [34] 
Dataset
Index
SiamFC 
XNOR 
RB-SF 
GOT-10K 
AO
0.348
0.251
0.327 
SR
0.383
0.230
0.343 
OTB50
Precision 
0.761
0.457
0.706 
SR
0.556
0.323
0.496 
OTB100 
Precision 
0.808
0.541
0.786 
SR
0.602
0.394
0.572 
UAV123 
Precision 
0.745
0.547
0.688 
SR
0.528
0.374
0.497 
Sun et al. [54] propose a fast object detection algorithm based on BNNs. 
Compared to full-precision convolution, this new method results in 62 times faster 
convolutional operations and 32 times memory saving in theory. 
Liu et al. [34] experiment on object tracking after proposing RBCNs. They used 
the SiamFC network as the backbone for object tracking and binarized the SiamFC 
as the rectiﬁed binary convolutional SiamFC network (RB-SF). They evaluated 
RBSF in four datasets, GOT-10K [22], OTB50 [65], OTB100 [66], and UAV123 
[40], using accuracy occupy (AO) and success rate (SR). The results are shown in 
Table 6.1. 
Yang et al. [69] propose a new method to optimize a deep neural network based 
on YOLO-based object tracking simultaneously using approximate weight bina-
rization, trainable threshold group binarization activation function, and separable 
convolution methods according to depth, signiﬁcantly reducing the complexity of 
computation and model size. 
6.4.1 
Data-Adaptive Binary Neural Networks for Efﬁcient 
Object Detection and Recognition 
One of the ugliest aspects of 1-bit CNNs lies in the gap between full-precision 
weights and their quantization counterpart. The focus of the existing methods is to 
minimize the gap. To this end, the convolutional kernel is usually divided into two 
parts, the amplitude and direction, while the feature maps are only in the direction 
for efﬁcient calculation. The existing binarization methods can be formulated in a 
uniﬁed framework where (1) upper D Subscript i Superscript lDl
i are the directions of the full-precision kernels upper W Subscript i Superscript lW l
i
of the ith channel in the lth convolutional layer, i element of StartSet 1 comma midline horizontal ellipsis comma upper I EndSeti ∈{1, · · · , I}, l element of StartSet 1 comma midline horizontal ellipsis comma upper N EndSetl ∈{1, · · · , N}; 
(2) upper A Superscript lAl is shared by all upper D Subscript i Superscript lDl
i represents the amplitude of the lth convolutional layer; 
and (3) ModifyingAbove upper A With caret Subscript i Superscript l ˆAl
i and upper A Subscript i Superscript lAl
i are of the same size and all the elements of ModifyingAbove upper A With caret Subscript i Superscript l ˆAl
i are equal to the 
average of the elements of upper A Subscript i Superscript lAl
i. ModifyingAbove upper X With caret Subscript i Superscript lˆXl
i denotes the direction of input upper X Subscript i Superscript lXl
i. In the forward 
pass, ModifyingAbove upper A With caret Superscript l ˆAl is used instead of the full-precision weights upper A Superscript lAl. The full-precision weights 
upper A Superscript lAl are only used for back propagation during training. Note that the formulation 
can represent XNOR based on a scalar [47], and also simpliﬁed PCNN [15] whose

6.4
Object Detection and Tracking
229
scalar is learnable as a projection matrix, or even XNOR++, which decomposes 
tensor ModifyingAbove upper A With caret ˆA into vectors in the three directions of the channel, height, and width [3]. 
We represent ModifyingAbove upper W With caret ˆW by the amplitude and direction as: 
ModifyingAbove upper W With caret equals upper D circled dot ModifyingAbove upper A With caret comma ˆW = D O ˆA,
(6.20) 
where circled dotO denotes the element-wise multiplication between matrices. We can calcu-
late the binary convolution output ModifyingAbove upper O With caret ˆO as: 
ModifyingAbove upper O With caret equals ModifyingAbove upper X With caret asterisk ModifyingAbove upper W With caret equals left parenthesis ModifyingAbove upper X With caret circled asterisk upper D right parenthesis circled dot ModifyingAbove upper A With caret comma ˆO = ˆX ∗ˆW = ( ˆX * D) O ˆA,
(6.21) 
where asterisk∗and circled asterisk* denote convolution (ﬂoating-point multiplication and addition) and 
bit convolution (bitwise XNOR and pop-count operations), respectively. 
6.4.1.1 
Data-Adaptive Amplitude Method 
Existing methods fail to calculate the data-adaptive amplitude to better approx-
imate the full-precision feature maps. This explains the primary reason for the 
performance gap between 1-bit CNNs and their full-precision counterpart. Without 
considering the amplitude of X, there is an inevitable gap between ModifyingAbove upper O With caret ˆO and O because 
the ﬁxed ModifyingAbove upper A With caret ˆA is irrelevant to input X. To address this issue, an intuitive idea is to let 
ModifyingAbove upper A With caret ˆA become a function ModifyingAbove upper A With caret left parenthesis upper X right parenthesis ˆA(X) with X as the input. In 1-bit CNNs, we use ModifyingAbove upper X With caret circled asterisk upper D ˆX * D to 
substitute for X because ModifyingAbove upper X With caret circled asterisk upper D ˆX * D contains the information of both X and W, which 
will have a better representation capacity. Because the amplitude ModifyingAbove upper A With caret ˆA is not ﬁxed but 
adaptive to the input data, we call our method data-adaptive binary neural network 
(DA-BNN) [73].  And we have:  
ModifyingAbove upper O With caret equals left parenthesis ModifyingAbove upper X With caret circled asterisk upper D right parenthesis circled dot ModifyingAbove upper A With caret left parenthesis ModifyingAbove upper X With caret circled asterisk upper D right parenthesis comma ˆO = ( ˆX * D) O ˆA( ˆX * D),
(6.22) 
where ModifyingAbove upper A With caret left parenthesis period right parenthesis ˆA(.) is related to the input and will burden the computation of CNNs. To 
address this problem, we use attention-based methods [60, 64] and introduce a 
lightweight module to implement ModifyingAbove upper A With caret left parenthesis period right parenthesis ˆA(.). The module is designed by considering both 
channel and spatial (height times× width) levels. For simplicity, we denote ModifyingAbove upper X With caret circled asterisk upper D ˆX * D as ModifyingAbove upper M With caret ˆM
in the following sections. To solve the problem, we introduce the attention method 
to calculate a data-adaptive amplitude for better performance. We lead two data-
adaptive amplitude methods: channel-based and spatial-based. 
6.4.1.2 
Data-Adaptive Channel Amplitude 
To calculate the channel amplitude ModifyingAbove upper A With caret Subscript upper C Baseline left parenthesis ModifyingAbove upper M With caret right parenthesis ˆAC( ˆM), we consider the feature maps from 
two perspectives, within and between channels, similar to the attention mechanism 
[21, 60]. We introduce the global average pooling layer to reduce the other two 
dimensions and extract features within channels. Compared with the convolution,

230
6
Applications
Fig. 6.5 The calculation of 
the channel amplitude 
the global average pooling layer adds no extra parameters and fewer calculations. 
Considering the cross-channel interaction, a 1D convolution is applied to fuse the 
information of each channel with its neighbors. 
However, because real-valued convolution parameters are often nearly zero, 
easily inﬂuenced by the weight decay, the binarization of parameters always 
means an ampliﬁcation compared with the real-value convolution. The result of 
the binary convolution is usually much more signiﬁcant when compared with the 
corresponding real-valued convolution [36, 53]. Thus, the amplitude ˆA( ˆM) should 
be a small value, solved by a sigmoid function that maps the amplitude into (0, 1). 
Furthermore, the sigmoid function is also used to guarantee ˆA(.) merely learns the 
amplitude information, not the direction. By doing so, we represent the channel 
amplitude ˆAC( ˆM) as: 
ˆAC( ˆM) = σ(kc ∗AvgPool( ˆM)),
(6.23) 
where σ denotes the sigmoid function and kc is the kernel of 1D convolution. A 
speciﬁc module is illustrated in Fig. 6.5. 
6.4.1.3 
Data-Adaptive Spatial Amplitude 
Similar to calculating the attention network, we use pooling layers and convolution 
to calculate the spatial amplitude. In Fig. 6.6, we demonstrate the corresponding 
structure. We utilize the average and maximum pooling together and then use a

6.4
Object Detection and Tracking
231
Fig. 6.6 The calculation of 
spatial amplitude 
3 times 33 × 3 convolution instead of the 1D convolution to deal with the spatial data. We 
calculate the spatial amplitude ModifyingAbove upper A With caret Subscript upper S Baseline left parenthesis ModifyingAbove upper M With caret right parenthesis ˆAS( ˆM) as: 
ModifyingAbove upper A With caret Subscript upper S Baseline left parenthesis ModifyingAbove upper M With caret right parenthesis equals sigma left parenthesis k Subscript s Baseline asterisk left bracket upper A v g upper P o o l left parenthesis ModifyingAbove upper M With caret right parenthesis semicolon upper M a x upper P o o l left parenthesis ModifyingAbove upper M With caret right parenthesis right bracket right parenthesis period ˆAS( ˆM) = σ(ks ∗
[
AvgPool( ˆM); MaxPool( ˆM)
]
).
(6.24) 
However, when the features are binarized in the next block, the ampliﬁcation 
information will be eliminated, and only the direction information will be retained. 
To keep the amplitude information, we redistribute features using an additional 
BN, added before the binarization of the feature map. By doing so, the amplitude 
will be partially converted into the direction information by improving the feature 
distribution. 
6.4.1.4 
Experiment on Object Recognition 
We use ImageNet [8] to train our models. Considering the size of the dataset, we 
apply ResNet-18 [19] on ImageNet for a fair comparison with other quantization 
networks. 
ILSVRC12 ImageNet is a large-scale dataset that contains over 1.2 million 
training images and 50K validation images from 1000 categories. To train ResNet-
18 on ImageNet, models are trained in a two-step training method, similar to 
[36–38]. The training process is divided into two stages. In the ﬁrst stage, we 
train a full-precision network that keeps the weights and activations real-valued 
for 60 epochs. Networks are optimized using stochastic gradient descent (SGD) 
to stabilize the pre-trained model. At this stage, we set the weight decay to 3eminus−4, 
the momentum to 0.9, and the learning rate to 0.1. In the second stage, the network

232
6
Applications
loads the parameters and binarizes the weights and activations. An Adam optimizer 
is used to sufﬁciently train the binary model, following the settings of [37]. The 
learning rate is set to 1e−3, and the weight decay is ﬁxed to 0. In both stages, the 
batch size is 360, and the learning rate is adjusted following a cosine schedule until 
annealing down to 0. Following the settings in [38], we use PReLU activations [18] 
instead of the ReLU activations and keep the real-valued downsample and double 
skip connections. 
6.4.1.5 
Ablation Study on Object Recognition 
In this section, we evaluate the effects of data-adaptive amplitude on the perfor-
mance of 1-bit CNNs. 
The BN layer in ResNet-18 is set after the convolution. We, however, add an extra 
BN layer in front of the 1-bit convolution to redistribute the features and turn the 
information of amplitude into the direction to cope with the information loss in the 
binarization process. We test the effectiveness of the addition of BN on ImageNet by 
ResNet-18. The channel amplitude and spatial amplitude are used in parallel. The 
speciﬁc structure is demonstrated in Fig. 6.7 and the results are shown in Table 6.2: 
Fig. 6.7 Network architectures of ResNet-18, Bi-Real Net on ResNet-18, and DA-BNN on 
ResNet-18. Note that the scale factor is added to the convolution in Bi-Real Net, and we adjust 
its position to make the comparison clear with the same principle 
Table 6.2 Different structures of binary neural networks are tested on ImageNet ILSVRC12. 
“BN” refers to the use of the front batch normalization layer. All the models are based on ResNet-
18 
Binary
BN
Amplitude
Acc. 
×
×
×
69.30 
√
×
×
57.60 
√
√
×
59.32 
√
√
√
63.08
The bolds denote the best results

6.4
Object Detection and Tracking
233
Table 6.3 We apply the channel and spatial adaptive amplitude, respectively, to evaluate their 
effectiveness. All models are trained in a two-step method. The accuracy of step 1 corresponds to 
the real-valued model, while the accuracy of step 2 corresponds to the binarization counterpart 
Method
Acc.of step1
Acc. Of step2 
Baseline
67.73
57.60 
Spatial
67.36
60.18 
Channel
69.32
61.63 
Channel+spatial
69.41
62.48 
Channel & spatial
69.29
63.08 
The bolds denote the best results
directly using the additional front BN, an increase of 1.72% in accuracy is observed 
compared to the normal binarization in ResNet-18. If we add the adaptive amplitude, 
the accuracy of networks can be improved by about 4%, which proves that the 
additional BN is helpful for further improving the performance of binary neural 
networks. 
In Eq. 6.22, the data-adaptive amplitude ModifyingAbove upper A With caret left parenthesis period right parenthesis ˆA(.) can be calculated using the channel 
amplitude and spatial amplitude in sequence or parallel. To this end, we test 
three data-adaptive amplitude combinations of the channel amplitude and spatial 
amplitude: sequential channel-spatial, sequential spatial-channel, and parallel, using 
both amplitude modules. Scale factor methods are also tested for comparison. 
We evaluate the performance of data-adaptive amplitude on ImageNet, based on 
ResNet-18. Unlike the experiments in Table 6.3, a two-step training method is used 
here. In the ﬁrst stage, we train the model with real-valued weights and feature maps 
as a pre-training step. In the second step, the model loads the parameters trained in 
the ﬁrst step and binarizes the weights and features corresponding to the binary 
models. We record the best performance of the model at each stage. 
Figure 6.8 shows the curves for the Top-1 accuracy of different methods. The 
sudden drop of the curves denotes the switch of training stages from full-precision to 
binarized models. The best accuracy of different methods is illustrated in Table 6.3: 
spatial amplitude has little inﬂuence on the real-valued model but increases the 
performance of binary neural network by about 2.5%, whereas channel amplitudes 
can improve the accuracy of both full-precision model and binarized model by 
2% and 4%, respectively. Different conﬁgurations of adaptive amplitude methods 
inﬂuence the performance of full-precision models and binary models differently. 
The sequential channel and spatial method performs better on full-precision models, 
while the one in parallel performs better for binary models. These results verify that 
a proper arrangement of the amplitude methods is essential to further improve the 
performance. 
6.4.1.6 
Network Accuracy Comparison on ImageNet 
To evaluate the performance of DA-BNN, we compare its performance with 
other state-of-the-art quantized networks, including BWN [47] DoReFa-Net [76],

234
6
Applications
0
20
 40
 60
 80
 
100
 
120
 
Epochs 
10 
20 
30 
40 
50 
60 
70 
Acc. 1 
baseline 
spatial 
channel 
channel + spatial 
channel & spatial 
Fig. 6.8 The convergence curves of DA-BNN. The “baseline” label means no extra modules are 
used. We use our proposed modiﬁed structure. The “channel” and “spatial” labels denote applying 
the corresponding amplitude modules. The “channel + spatial” label and “channel & spatial” 
represent different combination methods. It should be noted that the sudden drop in 60 epochs 
is caused by switching the training steps when both weights and activations are binarized 
TBN [57], BNN [6], XNOR-Net [47], ABC-Net [33], Bi-Real Net [37], PCNN [15], 
BONN [17], CI-Net [62], BinaryDuo [25], real-to-binary [38], and ReActNet [36] 
and reported Top-1 and Top-5 accuracies in Table 6.4. Note that all models are based 
on ResNet-18 with 69.3% Top-1 accuracy on the full-precision model. 
We ﬁrst applied our DA-BNN based on Bi-Real Net, achieving outstanding 
performance among neural networks with binary weights and activations. However, 
Bi-Real Net focuses more on optimizing binarization and ignores the signiﬁcance 
of amplitude in 1-bit convolution, just using the mean of the real-valued weights 
as the scale factor. In contrast, our method focuses on the adaptive amplitude to 
improve the representation capacity, an essential enhancement to Bi-Real Net. By 
applying our DA-BNN on Bi-Real Net, we use our adaptive amplitude instead of the 
scale factor and modify those above. Above 6% improvement is achieved under the 
Bi-Real Net framework, which exceeds most binarization methods. It is also worth 
mentioning that DoReFa-Net and TBN use more than 1-bit to quantify activations, 
yet we still perform better. 
However, our DA-BNN is not limited to a speciﬁc quantization method and can 
be combined with other binarization methods for more signiﬁcant improvement. To 
further evaluate the potential of our DA-BNN, we combine it with ReActNet, which 
achieves the highest binary accuracy based on ResNet-18 on ImageNet, to the best 
of our knowledge. Note that for a fair comparison, we remove the scale factor used 
in ReActNet to ensure the used amplitude is learned in our adaptive methods. Based 
on the ReActNet, it obtains even higher accuracy, with just a 3% gap to the full-
precision model.

6.4
Object Detection and Tracking
235
Table 6.4 Accuracy of 
state-of-the-art quantization 
networks and our DA-BNN 
on ImageNet. “W” and “A” 
refer to the weight and 
activation of the bit width, 
respectively. All the models 
are based on ResNet-18 
Model
W/A(bit) 
Top-1 
Top-5 
ResNet-18
32/32
69.3
89.2 
BWN
1/32
60.8
83.0 
DoReFa-Net
1/4
59.2
81.5 
TBN
1/2
55.6
79.0 
BNN
1/1
42.2
67.1 
XNOR-Net
1/1
51.2
73.2 
ABC-Net
1/1
42.7
67.6 
Bi-Real Neta
1/1
56.4
79.5 
PCNN
1/1
57.3
80.0 
BONN
1/1
59.3
81.6 
CI-Net
1/1
59.9
84.2 
BinaryDuoa
1/1
60.9
82.6 
DA-BNNa (based on Bi-Real Net) 
1/1
63.1 
84.3 
Real-to-binarya
1/1
65.4
– 
ReActNeta
1/1
65.5
– 
DA-BNNa (based on ReActNet)
1/1
66.3 
86.7 
The bolds denote the best results 
aA real-valued or partly real-valued (just binarized activation) 
model is used for pre-training. Because our method is not speciﬁc 
to the quantitative process, we use two different binarization 
frameworks, Bi-Real Net and ReActNet. Note that the ﬁrst DA-
BNN is based on Bi-Real Net, and the experimental settings refer 
to the description above. As for ReActNet, we keep all the settings 
the same except for the scale factor change with our data-adaptive 
amplitude 
In short, we achieve a new state-of-the-art performance compared to other 
BNNs, and a much closer performance to full-precision models, which validates 
the superiority of DA-BNN for the BNN calculation. 
6.4.1.7 
Experiment on Object Detection 
We evaluate our method on the PASCAL VOC dataset, composed of natural images 
from 20 classes. We train our model on the VOC 2007 and VOC 2012 trainval sets, 
which consist of around 16k images, and we evaluate our method on the VOC 2007 
test set, including about 5k images. Following the setting of [9], we use the mean 
average precision (mAP) as the evaluation criterion. 
We train our DA-BNN with the Faster R-CNN [50] detection framework with the 
ResNet-18 backbone [19] aforementioned. Following implementing binary neural 
networks in [37], we remain the ﬁrst and last layer in the detection networks’ real-
valued. The same pipeline as [50] is utilized when training our DA-BNN with a 
Faster R-CNN detector. For efﬁcient object detection, we binarize all the 3 times 33 × 3
convolution operations in the following models, except the ﬁrst convolution and

236
6
Applications
full-connected layer in Faster R-CNN, following the same settings as XNOR-Net 
[47] and BiDet [63]. We modify the architecture of ResNets following [63]. 
As for the details of training settings, we pre-train the binary backbone network 
in DA-BNN fashion on the ImageNet dataset, as depicted in Sect. 6.4.1.4. Then we 
ﬁne-tune the backbone and detection parts collaboratively for the object detection 
task. The batch size is assigned to be 16, with the SGD optimizer applied. The 
number of epochs is 12, and the learning rate varies according to the framework 
and backbone. A multistep learning rate schedule is employed for the Faster-RCNN, 
which decays twice by multiplying by 0.1 at the 8th and 11th epoch of the 12 epochs 
(Table 6.5 and Fig. 6.9). 
6.4.1.8 
Performance Comparison on PASCAL VOC 
In this section, we compare the proposed DA-BNN with state-of-the-art 1-bit neural 
networks, including XNOR-Net [47], Bi-Real Net [37], and BiDet [63] for the task 
of object detection on the PASCAL VOC datasets. 
Table 6.5 Comparison of mAP (percent sign%) with state-of-the-art BNNs in Faster R-CNN frameworks 
with ResNet-18 on VOC test2007. The detector with the real-valued and multi-bit backbone is 
given for reference. Input resolution is set as 600 times 1000600 × 1000. The bold denotes the best result 
Quantization method
W/A(bit)
mAP(percent sign%) 
Full-precision
32/32
74.5 
XNOR-Net
1/1
48.9 
Bi-Real Net
1/1
58.2 
BiDet
1/1
59.5 
DA-BNN
1/1
63.5 
Fig. 6.9 Qualitative results on PASCAL VOC test2007 (best viewed in color)

6.4
Object Detection and Tracking
237
Compared to other 1-bit methods, we observe a signiﬁcant performance advan-
tage over other state of the arts. With the ResNet-18 backbone, we achieve 63.5 percent sign63.5%
mAP, outperforming XNOR-Net, Bi-Real Net, and BiDet by 14.6percent sign%, 5.3percent sign%, and 4.0percent sign%
mAP with the similar memory usage and FLOPs. 
In short, we achieved a new state-of-the-art performance compared to other 
BNNs on PASCAL VOC. We are also much closer in performance to full-precision 
models, as demonstrated in experiments, validating the superiority of DA-BNN. 
6.4.1.9 
Computation and Storage Analysis 
Inevitably, using a data-adaptive amplitude will increase the computation and 
storage for a more accurate approximation to real-valued convolution. However, the 
additional structures are lightweight and efﬁcient. The additional part is negligible 
compared to the computation and storage of 1-bit convolution. In detail, the 
additional storage in adaptive amplitude is the weight of the convolution. In channel 
amplitude, we use a simple 1D convolution with a size of three, and thus the number 
of additional parameters is 3 times 323 × 32 (32 denotes 32 bits). A 3 times 33 × 3 convolution is used 
in the spatial amplitude with two input and one output channels. Thus, its storage 
is 9 times 2 times 329 × 2 × 32. Both are far less than the storage of corresponding 1-bit convolution 
due to the large numbers of convolution channels. The primary source of the extra 
computation comes from the structure modiﬁcation, where just a few parameters 
are introduced compared to the whole model. So the storage increase has almost no 
inﬂuence on the storage of original 1-bit networks. 
We calculate the computational and storage complexity compared to BNN 
networks and full-precision networks to show the ignorable addition of memory 
and speed up during inferences. The memory usage is represented by the storage 
for parameters of networks, which is calculated as the summation of 32-bit times 
real-valued parameters and 1-bit times binary parameters. We use FLOPs to 
measure computational complexity. Referring to [37, 47], the acceleration of 1-bit 
convolution is about 64 times the real-valued convolution. We follow these methods 
and calculate corresponding FLOPs. 
Table 6.6 compares computational complexity, and storage cost, across different 
quantization methods on ResNet-18 and Faster R-CNN frameworks. The proposed 
DA-BNN saves the storage cost by 11.04 times11.04× and reduces the computation by 10.80 times10.80×
in ResNet-18. On Faster R-CNN, as a result of the decrease of full-precision 
parameters in the fully connected layer, better performance of saving the storage and 
computation by 18.62 times18.62× and 15.77 times15.77×, respectively, has been achieved, which keeps 
the same level as other 1-bit CNN methods. In summary, our adaptive amplitude 
introduces negligible storage (less than 1%) and little computation (less than 8%) 
but can signiﬁcantly enhance BNNs’ performance.

238
6
Applications
Table 6.6 We show the memory usage as well as the ﬂops of the DA-BNN. The calculation 
method is the same as Bi-Real Net 
Model
Method
Memory usage
Memory saving
FLOPs (times 10 Superscript 8×108) 
ResNet-18
Full-precision
374.1 Mbit
–
18.26 
XNOR-Net
33.7 Mbit
11.10times×
1.67 
Bi-Real Net
33.6 Mbit
11.14times×
1.63 
Channel
33.9 Mbit
11.04times×
1.65 
Spatial
33.9 Mbit
11.04times×
1.67 
DA-BNN
33.9 Mbit
11.04times×
1.69 
Faster R-CNN
Full-precision
379.9 Mbit
–
360.14 
XNOR-Net
20.2 Mbit
18.81times×
21.29 
Bi-Real Net
20.1 Mbit
18.90times×
21.27 
BiDet
20.1 Mbit
18.90times×
21.27 
DA-BNN
20.4 Mbit
18.62times×
22.84 
6.4.2 
Amplitude Suppression and Direction Activation in 
Networks for Faster Object Detection 
6.4.2.1 
Methodology 
In this paper, we propose an amplitude suppression and direction activation in 
the Faster R-CNN framework (ASDA-FRCNN) [68] to compress DCNNs for 
highly efﬁcient object detection. The shared amplitude between full-precision and 
quantized kernels is signiﬁcantly suppressed during binarization, which can lead to 
a new simple but effective loss. The concept of ASDA is generic and ﬂexible and 
can be easily incorporated into existing DCNNs such WideResNets and ResNets 
and applied to many vision tasks including object classiﬁcation. 
Problem Formulation 
The inference process of any binary neural network (BNN) model is based on the 
binarized kernels. This means that the kernels must be binarized in the forward 
step (corresponding to the inference) during training, so that the training loss is 
calculated based on the binarized ﬁlters. Unlike the forward process, during back 
propagation, the resulting kernels do not need to be binarized and can be full-
precision. In this case, the full-precision kernels are binarized to gradually bridge 
the binarization gap during training. Therefore, the learning of most BNN models 
involves both discrete and continuous spaces, which poses a great challenge in 
practice. 
To address these challenges and improve the optimization of binarizing CNNs, 
we decouple the full-precision kernel X and represent it by the amplitude and 
direction as: 
ModifyingAbove upper X With caret equals upper A dot upper D comma ˆX = A · D,
(6.25)

6.4
Object Detection and Tracking
239
where A and D respectively denote the amplitude and the direction of X. D is the 
script l 1l1-normalized matrix and calculated by s i g n left parenthesis upper X right parenthesissign(X) as minus StartFraction 1 Over s i z e left parenthesis upper X right parenthesis EndFraction−
1
size(X) for negative X and 
StartFraction 1 Over s i z e left parenthesis upper X right parenthesis EndFraction
1
size(X) for positive X. A is a scalar. 
Corollary 6.1 To obtain an optimized BNN, we solve: 
upper X equals ModifyingAbove upper X With caret equals upper A dot upper D commaX = ˆX = A · D,
(6.26) 
based on the assumption that X and ModifyingAbove upper X With caretˆX share similar amplitude. 
This corollary is a bilinear problem, where A and D need to be calculated 
simultaneously. Existing methods tend to split the problem into easily solved 
subproblems, and then solve them using the alternating direction method of 
multipliers (ADMM) [20, 70], which might be less efﬁcient for the BNN calculation. 
To simplify the process, we proposed to calculate the amplitude A based on the 
back propagation algorithm since D can be solved based on the s i g n left parenthesis period right parenthesissign(.) function. 
In addition, due to the shared amplitude between the full-precision kernels and 
binarized kernels, we can easily suppress it and thus lead to a highly efﬁcient 
detector. 
Forward Propagation in ASDA-FRCNN 
In order to achieve binarized weights, we design a new loss function in ASDA-
FRCNN. Note that only the kernels of ASDA-FRCNN are binarized, while for 1-bit 
ASDA-FRCNN, both the kernels and the activations are binarized. These are brieﬂy 
described at the end of Sect. 6.4.2.2. Here we deﬁne D, A, and A as follows. upper D Subscript i comma j Superscript lDl
i,j is 
the direction of the full-precision kernel upper X Subscript i comma j Superscript lXl
i,j. upper X Subscript i comma j Superscript lXl
i,j denotes the i-th kernel in the j-th 
ﬁlter at l-th convolutional layer, l element of StartSet 1 comma midline horizontal ellipsis comma upper N EndSetl ∈{1, · · · , N}; upper A Superscript lAl shared by all upper D Subscript i comma j Superscript lDl
i,j represents the 
amplitude of the l-th convolutional layer; upper A Superscript lAl and upper A Superscript lAl are of the same size and all the 
elements of upper A Superscript lAl are equal to the average of the elements of upper A Superscript lAl. In the forward pass, 
upper A Superscript lAl is used instead of the full-precision upper A Superscript lAl. In this situation, upper A Superscript lAl can be considered a 
scalar. The full-precision upper A Superscript lAl is only used for back propagation during training. This 
process is the same as the way of calculating ModifyingAbove upper X With caretˆX from X in an asynchronous manner, 
which is also illustrated in Fig. 5.5. 
Accordingly, Eq. 6.25 is represented for ASDA-FRCNN at l-th layer as: 
upper D Subscript i comma j Superscript l Baseline equals StartFraction s i g n left parenthesis upper X Subscript i comma j Superscript l Baseline right parenthesis Over s i z e left parenthesis upper X Subscript i comma j Superscript l Baseline right parenthesis EndFraction commaDl
i,j =
sign(Xl
i,j)
size(Xl
i,j),
(6.27) 
ModifyingAbove upper X With caret Subscript i comma j Superscript l Baseline equals upper A Superscript l Baseline dot upper D Subscript i comma j Superscript l Baseline comma ˆXl
i,j = Al · Dl
i,j,
(6.28) 
where upper D Subscript i comma j Superscript lDl
i,j represents the binarized kernel, i period e period commai.e., direction. s i z e left parenthesis upper X Subscript i comma j Superscript l Baseline right parenthesissize(Xl
i,j) is the number 
of weights of s i z e left parenthesis upper X Subscript i comma j Superscript l Baseline right parenthesissize(Xl
i,j). With the i-th binary kernel in j-th ﬁlter at l-th layer

240
6
Applications
reconstructed, we can formulate the forward path of feature maps as: 
StartLayout 1st Row 1st Column upper F Subscript j Superscript l plus 1 2nd Column equals script upper H Subscript j Superscript l plus 1 Baseline left parenthesis bold italic upper F Superscript l Baseline comma upper A Superscript l Baseline comma bold italic upper D Superscript l Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript i Endscripts upper F Subscript i Superscript l Baseline circled times ModifyingAbove upper X With caret Subscript i comma j Superscript l Baseline 3rd Row 1st Column Blank 2nd Column equals upper A Superscript l Baseline sigma summation Underscript i Endscripts upper F Subscript i Superscript l Baseline circled times upper D Subscript i comma j Superscript l Baseline comma EndLayoutF l+1
j
= Hl+1
j
(F l, Al, Dl)
=
Σ
i
F l
i ⊗ˆXl
i,j
= Al Σ
i
F l
i ⊗Dl
i,j,
(6.29) 
where we use script upper H Superscript l plus 1Hl+1 to denote the mapping at l plus 1l + 1-th layer in the abstract sense and 
script upper H Subscript j Superscript l plus 1Hl+1
j
is the j-th output feature map. circled times⊗denotes the convolution operation; upper F Subscript j Superscript l plus 1F l+1
j
is 
the j-th feature map in the left parenthesis l plus 1 right parenthesis(l + 1) aligned convolutional layer. bold italic upper F Superscript l Baseline comma bold italic upper D Superscript lF l, Dl denotes the 
aggregate of feature maps and directions at l-th layer, respectively. upper F Subscript i Superscript lF l
i denotes the 
i-th feature map in the lth convolutional layer. 
Loss Function of ASDA-FRCNN 
We then deﬁne an amplitude loss function to reconstruct the full-precision kernels 
as: 
StartLayout 1st Row 1st Column upper L Subscript upper A 2nd Column equals sigma summation Underscript l Endscripts sigma summation Underscript j Endscripts sigma summation Underscript i Endscripts parallel to upper X Subscript i comma j Superscript l Baseline minus ModifyingAbove upper X With caret Subscript i comma j Superscript l Baseline parallel to Subscript 2 Superscript 2 Baseline 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript l Endscripts upper A Superscript l Baseline sigma summation Underscript j Endscripts sigma summation Underscript i Endscripts parallel to script upper X Subscript i comma j Superscript l Baseline minus upper D Subscript i comma j Superscript l Baseline parallel to Subscript 2 Superscript 2 Baseline comma EndLayoutLA =
Σ
l
Σ
j
Σ
i
||Xl
i,j −ˆXl
i,j||2
2
=
Σ
l
Al Σ
j
Σ
i
||Xl
i,j −Dl
i,j||2
2,
(6.30) 
script upper X Subscript i comma j Superscript lXl
i,j is normalized by dividing parallel to upper X Subscript i comma j Superscript l Baseline parallel to Subscript 1||Xl
i,j||1. Under Corollary 6.1, X and ModifyingAbove upper X With caretˆX share similar 
amplitude, thus formulating a strong supervision to minimize the reconstruction 
error. Then we also need a loss to monitor the detection process as: 
StartLayout 1st Row 1st Column upper L Subscript upper S Baseline equals 2nd Column StartFraction 1 Over upper S EndFraction sigma summation Underscript k Endscripts upper L Subscript c l s Baseline left parenthesis p Subscript k Baseline comma p Subscript k Superscript g t Baseline right parenthesis plus lamda StartFraction 1 Over upper M EndFraction sigma summation Underscript k Endscripts p Subscript k Superscript g t Baseline upper L Subscript r e g Baseline left parenthesis t Subscript k Baseline comma t Subscript k Superscript g t Baseline right parenthesis 2nd Row 1st Column equals 2nd Column StartFraction 1 Over upper S EndFraction sigma summation Underscript k Endscripts minus log left bracket p Subscript k Superscript g t Baseline dot p Subscript k Baseline plus left parenthesis 1 minus p Subscript k Superscript g t Baseline right parenthesis dot left parenthesis 1 minus p Subscript k Baseline right parenthesis right bracket 3rd Row 1st Column plus 2nd Column lamda StartFraction 1 Over upper M EndFraction sigma summation Underscript k Endscripts p Subscript k Superscript g t Baseline s m o o t h Subscript script l 1 Baseline left parenthesis t Subscript k Baseline comma t Subscript k Superscript g t Baseline right parenthesis comma EndLayoutLS = 1
S
Σ
k
Lcls(pk, pgt
k ) + λ 1
M
Σ
k
pgt
k Lreg(tk, tgt
k )
= 1
S
Σ
k
−log
[
pgt
k · pk + (1 −pgt
k ) · (1 −pk)
]
+λ 1
M
Σ
k
pgt
k smoothl1(tk, tgt
k ),
(6.31) 
where S denotes the mini-batch size and M denotes the anchor locations. p Subscript k Baseline comma t Subscript k Baselinepk, tk, 
are a positive prediction and a vector presenting four coordinates of anchor k. Their 
detailed deﬁnitions are: 
StartLayout 1st Row 1st Column p Subscript k 2nd Column equals upper P Subscript s Baseline left bracket script upper H Superscript upper N Baseline left parenthesis bold italic upper F Superscript upper N minus 1 Baseline comma upper A Superscript upper N minus 1 Baseline comma bold italic upper D Superscript upper N minus 1 Baseline right parenthesis right bracket EndLayout StartLayout 2nd Row 1st Column t Subscript k 2nd Column equals upper T Subscript k Baseline left bracket script upper H Superscript upper N Baseline left parenthesis bold italic upper F Superscript upper N minus 1 Baseline comma upper A Superscript upper N minus 1 Baseline comma bold italic upper D Superscript upper N minus 1 Baseline right parenthesis right bracket comma EndLayoutpk = Ps
[
HN(F N−1, AN−1, DN−1)
]
Alt-text already available with first equation in this block
(6.32) 
tk = Tk
[
HN (F N−1, AN−1, DN−1)
]
,
(6.33)

6.4
Object Detection and Tracking
241
p Subscript k Superscript g tpgt
k and t Subscript k Superscript g ttgt
k
are their ground truth labels, respectively. upper P Subscript kPk and upper T Subscript kTk denote obtaining 
the probability and location information of k-th anchor from last layer. N is the total 
number of layers and the function s m o o t h Subscript script l 1 Baseline left parenthesis x right parenthesissmoothl1(x) is deﬁned as: 
s m o o t h Subscript script l 1 Baseline left parenthesis x right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column 0.5 dot x squared comma i f StartAbsoluteValue x EndAbsoluteValue less than 1 2nd Row 1st Column Blank 2nd Column StartAbsoluteValue x EndAbsoluteValue minus 0.5 comma normal e normal l normal s normal e EndLayoutsmoothl1(x) =
{
0.5 · x2, if |x| < 1
|x| −0.5, else
(6.34) 
Finally, the overall loss function L is applied to supervise the training of ASDA-
FRCNN in the back propagation algorithm and is deﬁned as: 
upper L equals upper L Subscript upper S Baseline plus mu upper L Subscript upper A Baseline commaL = LS + μLA,
(6.35) 
6.4.2.2 
Back Propagation 
In ASDA-FRCNN, what needs to be learned and updated are the full-precision 
kernels upper X Subscript iXi and the amplitude A. The kernels and the matrices are jointly optimized. 
In each convolutional layer, ASDA-FRCNN updates the full-precision kernels and 
then the amplitude. In what follows, the layer index l is omitted for simplicity. 
Updating X 
We denote delta Subscript upper X Sub Subscript iδXi as the gradient of the full-precision kernel upper X Subscript i comma j Superscript lXl
i,j and have: 
upper X Subscript i comma j Superscript l Baseline left arrow upper X Subscript i comma j Superscript l Baseline minus eta 1 delta Subscript upper X Sub Subscript i comma j Sub Superscript l Subscript Baseline commaXl
i,j ←Xl
i,j −η1δXl
i,j ,
(6.36) 
where eta 1η1 is a learning rate. delta Subscript upper X Sub Subscript i comma j Sub Superscript lδXl
i,j is calculated as: 
StartLayout 1st Row 1st Column delta Subscript upper X Sub Subscript i comma j Sub Superscript l 2nd Column equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper X Subscript i comma j Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper A Baseline Over partial differential upper X Subscript i comma j Superscript l Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper X With caret Subscript i comma j Superscript l Baseline EndFraction dot StartFraction partial differential ModifyingAbove upper X With caret Subscript i comma j Superscript l Baseline Over partial differential upper X Subscript i comma j Superscript l Baseline EndFraction plus 2 dot upper A left parenthesis script upper X Subscript i comma j Superscript l Baseline minus upper D Subscript i comma j Superscript l Baseline right parenthesis StartFraction partial differential script upper X Subscript i comma j Superscript l Baseline Over partial differential upper X Subscript i comma j Superscript l Baseline EndFraction 3rd Row 1st Column Blank 2nd Column equals upper A dot left bracket StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper X With caret Subscript i comma j Superscript l Baseline EndFraction dot double struck 1 plus 2 left parenthesis script upper X Subscript i comma j Superscript l Baseline minus upper D Subscript i comma j Superscript l Baseline right parenthesis StartFraction partial differential script upper X Subscript i comma j Superscript l Baseline Over partial differential upper X Subscript i comma j Superscript l Baseline EndFraction right bracket comma EndLayoutδXl
i,j = ∂LS
∂Xl
i,j
+ ∂LA
∂Xl
i,j
= ∂LS
∂ˆXl
i,j
·
∂ˆXl
i,j
∂Xl
i,j
+ 2 · A(Xl
i,j −Dl
i,j)
∂Xl
i,j
∂Xl
i,j
= A ·
[
∂LS
∂ˆXl
i,j
· 1 + 2(Xl
i,j −Dl
i,j)
∂Xl
i,j
∂Xl
i,j
]
,
(6.37) 
upper X Subscript i comma j Superscript lXl
i,j is the full-precision convolutional kernel corresponding to upper D Subscript i comma j Superscript lDl
i,j, and double struck 11 is the 
indicator function [47] widely used to estimate the gradient of the non-differentiable 
function.

242
6
Applications
6.4.2.3 
Amplitude Calculation and Suppression 
After updating X, we update the amplitude A. Let  delta Subscript upper AδA be the gradient of upper AA. 
According to Eq. 6.35, we have:  
upper A Superscript l Baseline left arrow StartAbsoluteValue upper A Superscript l Baseline minus eta 2 delta Subscript upper A Sub Superscript l Subscript Baseline EndAbsoluteValue commaAl ←|Al −η2δAl|,
(6.38) 
where eta 2η2 is another learning rate. And delta Subscript upper A Sub Superscript lδAl is calculated as: 
StartLayout 1st Row 1st Column delta Subscript upper A Sub Superscript l 2nd Column equals StartFraction partial differential upper L Subscript upper S Baseline Over partial differential upper A Superscript l Baseline EndFraction plus StartFraction partial differential upper L Subscript upper A Baseline Over partial differential upper A Superscript l Baseline EndFraction 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript j Endscripts sigma summation Underscript i Endscripts left bracket StartFraction partial differential upper L Subscript upper S Baseline Over partial differential ModifyingAbove upper X With caret Subscript i comma j Superscript l Baseline EndFraction dot upper D Subscript i comma j Superscript l Baseline plus parallel to upper X Subscript i comma j Superscript l Baseline minus upper D Subscript i comma j Superscript l Baseline parallel to Subscript 2 Superscript 2 Baseline right bracket comma EndLayoutδAl = ∂LS
∂Al + ∂LA
∂Al
=
Σ
j
Σ
i
[
∂LS
∂ˆXl
i,j
· Dl
i,j + ||Xl
i,j −Dl
i,j||2
2
]
,
(6.39) 
Note that the amplitudes are always set to nonnegative. By setting a very small muμ
in Eq. 6.35, we actually suppress amplitude upper A Superscript lAl directly. The parameter evaluation is 
extensively explored in the experimental section, which shows that such suppression 
is highly effective. On the contrary, the direction information is always used in the 
forward process. 
Our 1-bit ASDA-FRCNN is also based on binarizing the kernels and activations 
simultaneously as in [7, 47]. These derivations show that ASDA-FRCNN is learn-
able with our BP algorithm. We summarize the training procedure in Algorithm 13. 
Algorithm 13: Optimized ASDA-FRCNN via back propagation 
An algo
rith m illustr ates how the training datase t, full pre cisi on kernels , ampli tud e sc alar, and
 learn ing  rat es a
re used 
to c ompute forw ard a nd b ackward pro pag
ati ons with A S D A- F R C N N.
6.27 An algorithm illustrates how the training dataset, full precision kernels, amplitude scalar, and learning rates are used to compute forward and backward propagations with A S D A-F R C N N. 6.28An
 a
lgo ri thm illustra tes h ow th
e 
t ra
inin g datas et
, ful
 p
reci sio
n 
kernels,  ampl
it
ude scalar, and learning rates are used to compute forward and backward propagations with A S D A-F R C N N. 6.29A n algor
ithm
 ill ustr
ate
s h ow the tr aining datas
et, 
full  pr ecisi on  ke
rne
ls, amplit ude 
scalar , and  le arning  rates are used to compute forward and backward propagations with A S D A-F R C N N. 6.36An algorithm illustrates how the training dataset, full precision kernels, amplitude scalar, and learning rates are used to compute forward and backward propagations with A S D A-F R C N N.6.39 
An 
algorit hm illustra tes
 how  the  training  data set, full pre
cisi
on k erne
ls, amplit ude scalar, an d learning rates are used to compute forward and backward propagations with A S D A-F R C N N.

6.4
Object Detection and Tracking
243
6.4.2.4 
Experiments 
Datasets and Implementation Details 
We evaluated our ASDA-FRCNN method on two most widely applied detection 
datasets: PASCAL VOC and MS COCO. PASCAL VOC 2007 [9] dataset consists 
of about 5k train/val images and 5k test images over 20 object categories. We also 
provide results by training on PASCAL VOC 2007+2012 train/val and testing on 
PASCAL VOC 2007 test. More experiments are deployed on MS COCO 2014 [31], 
which consists of 240k train/val images, 5k minival images, and 40k test-dev images 
over 80 object categories. Furthermore, as our approach shows great feasibility, we 
deploy ASDA ResNet-18 on ImageNet ILSVRC2012 [27] in ResNet-18 [19]. 
We implemented the training process plotted in Algorithm 13 on 3 NVIDIA 
TITAN Xp GPUs with 128GB of RAM via PyTorch [45]. The weight decay, 
momentum, and hyperparameter λ are set as 0.0001, 0.9, and 10, respectively. W and 
A are the weight and activation, respectively. Full-precision model is implemented 
with 32-bit weight and 32-bit activation. And 1-bit ASDA Faster is implemented 
with 1-bit weight and 1-bit activation. We modify the architecture of ResNet-18 
and ResNet-34 following [37] by substituting ReLU with PReLU [18], and the ﬁnal 
results of our ASDA Res-18 are ﬁne-tuned based on the pre-trained models with 
only kernel weights binarized, halving the learning rate during training. NOTE: 1-
bit ASDA-FRCNN is employed in ablation study; thus, W and A are 1-bit. 
6.4.2.5 
Ablation Study 
Parameter μ 
As mentioned above, the proposed loss has the ability to control the process of 
quantization. Hyperparameter μ is introduced in Eq. 6.35 to balance the loss and 
suppress the inﬂuence of the amplitude. To evaluate the inﬂuence of μ, we deploy 
Table 6.7 Test mAP on PASCAL VOC 2007 dataset in ResNet-18 backbone. Training method 
includes VOC2007 only and VOC2007+2012. The bolds represent the best results 
μ 
Model
1e −4
5e −5
2e − 5
1e −5 
1-bit ASDA-FRCNN VOC07
47.4
51.1
54.6
48.6 
1-bit ASDA-FRCNN VOC07+12
56.3
61.5
63.4
61.1 
Table 6.8 Test mAP on PASCAL VOC 2007 dataset in ResNet-34 backbone. Training method is 
VOC2007+2012 
μ 
Model
5e −5
2e −5
1e − 5
5e −6 
1-bit ASDA-FRCNN VOC07+12
54.1
60.2
65.5
61.7
The bolds denote the best results

244
6
Applications
1
2
3
4
5
6
7
8
9
 10
 
Epochs 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
Amplitude 
=1e-4 
=5e-5 
=2e-5 
=1e-6 
Fig. 6.10 Al at ﬁrst Conv layer in the sixth block with different μ. Training method is 
VOC2007+2012. Model is ResNet-18 
controlled experiments on ResNet-18 on PASCAL VOC 2007. Results are shown in 
Tables 6.7 and 6.8. 
In ResNet-18, it is observed that the network achieve the better performance as 
we suppress μ. Thus, we ﬁx μ to 5e −4 in the following experiments in ResNet-18. 
In addition, we analyze the amplitude of a certain layer in ResNet-18. As plotted 
in Fig. 6.10, Al is suppressed more deeply as the μ becomes smaller, which subtly 
demonstrates our intuition in Sect. 6.4.2.3. 
In ResNet-34, LA increases as the model size expends. And the hyperparameter 
μ should be lower to suppress the amplitude more. As shown in Table 6.8, the  
network obtains the best performance when μ is set to 1e −5. Thus, we ﬁx μ to 
1e − 5 in the following experiments in ResNet-34. 
Learning Convergence 
Figure 6.11 plots the LS curve with different μ. Obviously, when μ is set to 
2e − 5, LS can converge to a lower level, which shows the network obtains a better 
performance. 
Experimental Veriﬁcation of Corollary 6.1 
As plotted in Fig. 6.12, l2-norm summation of kernels in the ﬁrst and second layer 
in 6-th block is similar to the corresponding amplitudes as the scatters distribute 
uniformly around the positive scale curve. This ablation result strongly proves our 
Corollary 6.1 (Fig. 6.13).

6.4
Object Detection and Tracking
245
0
1
2
3
4
5
6
7
8
9
 
104 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
=1e-4 
=5e-5 
=2e-5 
=1e-6 
Fig. 6.11 Training LS with different μ. Training method is VOC2007+2012. Model is ResNet-18 
8.8
9
9.2
9.4
9.6
9.8
10 
8.8 
9 
9.2 
9.4 
9.6 
9.8 
10 
39
40
41
42
43
44
45 
39 
40 
41 
42 
43 
44 
45 
Fig. 6.12 Training ||X||2 and || ˆX||2 scatter with epochs. Red line is the positive scale curve. The 
left one is the ﬁrst layer in the sixth block and the right one is the second layer 
6.4.2.6 
Object Detection 
Results on PASCAL VOC Datasets 
We compare the performance of our results with other state-of-the-art binary 
methods such as XNOR [47], TBN [57], and Bi-Real [37]. The comparison results 
for object detection are illustrated in Tables 6.9 and 6.10.

246
6
Applications
Fig. 6.13 Detection results on PASCAL VOC 2007 test 
Table 6.9 Test mAP on 
PASCAL VOC 2007 dataset 
in ResNet-18 backbone. 
Training method is VOC2007 
only and VOC2007+2012. 
“W” and “A” refer to the 
weight and activation bit 
width, respectively 
Model
W 
A 
mAP 
FPS 
VOC2007 only 
Faster R-CNN-Res18 
32 
32 
67.8 
12.26 
Bi-Real [37]
1 
1 
51.0 
12.26a 
ASDA-FRCNN
1 
32 
56.6 
12.26 
1-bit ASDA-FRCNN 
1 
1 
54.6 
12.26a 
VOC2007+2012 
Faster R-CNN-Res18 
32 
32 
73.2 
12.26 
Bi-Real [37]
1 
1 
60.6 
12.26a 
ASDA-FRCNN
1 
32 
66.4 
12.26 
1-bit ASDA-FRCNN 
1 
1 
63.4 
12.26a 
The bolds denote the best results 
aDue to hardware constraints, binary acceleration 
cannot be reﬂected on the PC, but theoretically it 
can accelerate 58 times. So we estimate FPS as 711 
Table 6.10 Test mAP on 
PASCAL VOC 2007 dataset 
in ResNet-34 backbone. 
Training method is 
VOC2007+2012 
Model
W 
A 
mAP 
FPS 
Faster R-CNN-Res34 
32 
32 
75.6 
8.01 
XNOR [47]
1 
2 
54.7 
– 
TBN [57]
1 
2 
59.0 
– 
ASDA-FRCNN
1 
32 
TBD 
8.01 
1-bit ASDA-FRCNN 
1 
1 
65.5 
8.01a 
The bolds denote the best results 
a Estimated 464 FPS 
It is observed that at least a 6.5% mAP as well as 1.45× acceleration improve-
ment is gained with our 1-bit ASDA-FRCNN over TBN in ResNet-34. When μ is 
set to 2e −5, the detection performance is the highest. In ResNet-18, we deploy Bi-
Real [37] in the same experimental settings for contrast. Our 1-bit ASDA-FRCNN 
outperforms the other two methods with the same compression ratio.

6.4
Object Detection and Tracking
247
We further plot the test AP of every class in the PASCAL VOC 2007 test. As is 
shown in Fig. 6.14, ASDA-FRCNN achieves a higher AP on all 20 classes than Bi-
Real, and 1-bit ASDA-FRCNN outperforms Bi-Real in 16 out of 20 classes. Hence, 
we can conclude that ASDA Faster R-CNN achieves the better performance than 
Bi-Real. 
Results on MS COCO Datasets 
We use the μ value of 2e −5 empirically. Then we compare the performance of 
our results with other state-of-the-art algorithms including one-stage fast object 
detection methods SSD [35], YOLO [48], RetinaNet [30] and CenterNet [77]. The 
comparison results for object detection are illustrated in Table 6.11. 
aero bike bird boat bottle bus 
car 
cat chair cow table dog horse mbik perso plant sheep sofa train 
tv 
Class 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
Faster R-CNN-Res18 
1-bit ASDA FRCNN 
Bi-Real 
ASDA FRCNN 
Fig. 6.14 APs of every class on PASCAL VOC 2007 test 
Table 6.11 Test mAP@.5 and mAP@[.5, .95] on MS COCO test-dev in ResNet-18 backbone. 
Training method includes MS COCO Train+Val. Note that only ASDA-FRCNN is the binary 
approach 
Model
W/A
mAP @.5
mAP @[.5, .95]
FPS 
SSD321 [35]
32/32
45.4
28.0
61 
YOLOv3-320 [48]
32/32
59.0
28.2
45 
RetinaNet-50-500 [30]
32/32
59.0
32.5
14 
CenterNet-Res18 [77]
32/32
44.9
28.1
142 
Faster R-CNN [50]
32/32
42.7
21.9
6.25 
ASDA-FRCNN
1/32
41.5
21.4
6.25 
1-bit ASDA-FRCNN
1/1
37.5
19.4
6.25a 
The bolds denote the best results 
aEstimated 362 FPS

248
6
Applications
It is observed that our 1-bit ASDA-FRCNN is faster than the other one-stage 
detectors. It can process estimated 362 images per second, which is far more than 
other state-of-the-art methods. These results are very meaningful on mobile and 
embedded devices. 
6.4.2.7 
Image Classiﬁcation 
For ImageNet [27], we employ two data augmentation techniques sequentially: (1) 
randomly cropping patches of 224 times 224224×224 from the original image and (2) horizontally 
ﬂipping the extracted patches in the training. While in the testing, the Top-1 and 
Top-5 accuracies on the validation set with single center crop are measured. 
In Table 6.12, we compare our ASDA ResNet-18 with several other state-of-the-
art models. The ﬁrst part of the comparison is based on ResNet-18 with 69.3% Top-1 
accuracy on the full-precision model. Although BWN [47] and DoReFa-Net [76] 
achieve Top-1 accuracy with degradation of less than 10%, it should be noted that 
they apply full-precision and 4-bit activations, respectively. With both of the weights 
and activations binarized, the BNN model in [7], ABC-Net [33], and XNOR-Net 
[47] fail to maintain the accuracy and are inferior to our 1-bit ASDA Res-18. For 
example, compared with the result of PCNN [16], 1-bit ASDA Res-18 increases 
the Top-1 accuracy by 2.29%. Note that our algorithm still works very well on the 
classiﬁcation task, which further validates the signiﬁcance of our method. In short, 
we achieved a new state-of-the-art performance compared to other BNNs, which 
clearly validate the superiority of our method for the BNN computing. 
Memory Usage and Efﬁciency Analysis 
Memory use is analyzed by comparing our approach with the state-of-the-art 
XNOR-Net [47] and the corresponding full-precision network. The memory usage 
is computed as the sum of 32 bits multiplied by the number of full-precision kernels 
and 1 bit times the number of the binary kernels in the networks. As shown in 
Table 6.13, our proposed ASDA-FRCNN reduces the memory usage by 10.2times× and 
14.4times× compared with the full-precision Faster R-CNN based on ResNet-18 and 
Table 6.12 Test Top-1 and 
Top-5 accuracy on ImageNet 
ILSVRC2012 in ResNet-18. 
The bolds represent the best 
results 
Model
W 
A 
Top-1 
Top-5 
ResNet-18 [19]
32 
32 
69.3
89.2 
BWN [47]
1 
32 
60.8
83.0 
DoReFa-Net [76]
1
4 
59.2
81.5 
TBN [57]
1
2 
55.6
79.0 
XNOR-Net [47]
1
1 
51.2
73.2 
BNN [7]
1
1 
42.2
67.1 
ABC-Net [33]
1
1 
42.7
67.6 
Bi-Real Net [37]
1
1 
56.4
79.5 
PCNN [16]
1
1 
57.3
80.0 
1-bit ASDA Res-18 
1
1 
59.59 
82.11

6.4
Object Detection and Tracking
249
Table 6.13 Memory usage and efﬁciency of convolution comparison on detection and classiﬁca-
tion binary and full-precision models 
Model
Memory usage
Memory saving
Speedup 
1-bit ASDA-FRCNN Res-18
35.5 Mbit
10.2times×
58times×
Faster R-CNN Res-18
361.3 Mbit
–
– 
1-bit ASDA-FRCNN Res-34
46.5 Mbit
14.4times×
58times×
TBN Res-34 [57]
46.5 Mbit
14.4times×
40times×
Faster R-CNN Res-34
669.9 Mbit
–
– 
1-bit ASDA Res-18
33.7 Mbit
11.1times×
58times×
XNOR-Net [47]
33.7 Mbit
11.1times×
58times×
ResNet-18
374.1 Mbit
–
– 
ResNet-34, respectively. Our proposed ASDA ResNet-18 realizes 11.1times× memory 
saving and 58times× acceleration compared to the full-precision one. The reason is that 
the projection parameters upper W Subscript j Superscript lW l
j are only used when training for enriching the diversity 
in ASDA-FRCNN, whereas they are not used during inference. For efﬁciency 
analysis, if all of the operands of the convolutions are binary, then the convolutions 
can be estimated by XNOR and bitcounting operations, which gains 58 times58× speedup 
in CPUs [47]. 
6.4.3 
Q-YOLO: Efﬁcient Inference for Real-Time Object 
Detection 
6.4.3.1 
Preliminaries 
Network Quantization Process 
We ﬁrst review the main steps of the post-training quantization (PTQ) process 
and supply the details. Firstly, the network is trained or provided as a pre-trained 
model using full-precision and ﬂoating-point arithmetic for weights and activations. 
Subsequently, numerical representations of weights and activations are suitably 
transformed for quantization. Finally, the fully quantized network is either deployed 
on integer arithmetic hardware or simulated on GPUs, enabling efﬁcient inference 
with reduced memory storage and computational requirements while maintaining 
reasonable accuracy levels. 
6.4.3.2 
Uniform Quantization 
Assuming the quantization bit width is b, the quantizer normal upper Q left parenthesis bold x vertical bar b right parenthesisQ(x|b) can be formulated as 
a function that maps a ﬂoating-point number bold x element of double struck upper Rx ∈R to the nearest quantization bin: 
normal upper Q left parenthesis bold x vertical bar b right parenthesis colon double struck upper R right arrow ModifyingAbove bold x With caret commaQ(x|b) : R →ˆx,
(6.40)

250
6
Applications
ModifyingAbove bold x With caret equals StartLayout Enlarged left brace 1st Row 1st Column Blank 2nd Column StartSet minus 2 Superscript b minus 1 Baseline comma midline horizontal ellipsis comma 2 Superscript b minus 1 Baseline minus 1 EndSet 3rd Column Signed comma 2nd Row 1st Column Blank 2nd Column left brace 0 midline horizontal ellipsis comma 2 Superscript b Baseline minus 1 right brace 3rd Column Unsigned period EndLayoutˆx =
{ {−2b−1, · · · , 2b−1 −1}
Signed,
{0 · · · , 2b −1}
Unsigned.
(6.41) 
There are various quantizers normal upper Q left parenthesis bold x vertical bar b right parenthesisQ(x|b), where uniform [24] are typically used. Uniform 
quantization is well supported on most hardware platforms. Its unsigned quantizer 
normal upper Q left parenthesis bold x vertical bar b right parenthesisQ(x|b) can be deﬁned as: 
normal upper Q left parenthesis bold x vertical bar b right parenthesis equals c l i p left parenthesis left floor StartFraction bold x Over s Subscript bold x Baseline EndFraction right ceiling plus z p Subscript bold x Baseline comma 0 comma 2 Superscript b Baseline minus 1 right parenthesis commaQ(x|b) = clip(L x
sx
| + zpx, 0, 2b −1),
(6.42) 
where s Subscript bold xsx (scale) and z p Subscript bold xzpx (zero-point) are quantization parameters. In Eq. 6.43, u 
(upper) and l (lower) deﬁne the quantization grid limits: 
StartLayout 1st Row 1st Column s Subscript bold x Baseline equals 2nd Column StartFraction u minus l Over 2 Superscript b Baseline minus 1 EndFraction comma z p Subscript bold x Baseline equals c l i p left parenthesis left floor minus StartFraction l Over s EndFraction right ceiling comma 0 comma 2 Superscript b Baseline minus 1 right parenthesis period EndLayoutsx = u −l
2b −1, zpx = clip(L−l
s |, 0, 2b −1).
(6.43) 
The dequantization process can be formulated as follows: 
bold x overTilde equals left parenthesis ModifyingAbove bold x With caret minus z p Subscript bold x Baseline right parenthesis times s Subscript bold x Baseline period˜x = (ˆx −zpx) × sx.
(6.44) 
6.4.3.3 
Quantization Range Setting 
The quantization range setting establishes the quantization grid’s upper and lower 
clipping thresholds, denoted as u and l, respectively. The crucial trade-off in range 
setting lies in the balance between two types of errors: clipping error and rounding 
error. Clipping error arises when data is truncated to ﬁt within the predeﬁned 
grid limits, as described in Eq. 6.43. Such truncation leads to information loss 
and decreased precision in the resulting quantized representation. On the other 
hand, rounding error occurs due to the imprecision introduced during the rounding 
operation, as described in Eq. 6.42. This error can accumulate over time and impact 
the overall accuracy of the quantized representation. The following methods provide 
different trade-offs between the two quantities. MinMax In the experiments, we use 
the MinMax method for weight quantization, where clipping thresholds l Subscript bold xlx and u Subscript bold xux
are formulated as: 
StartLayout 1st Row 1st Column l Subscript bold x Baseline equals 2nd Column min left parenthesis bold x right parenthesis comma u Subscript bold x Baseline equals max left parenthesis bold x right parenthesis period EndLayoutlx = min(x), ux = max(x).
(6.45) 
This leads to no clipping error. However, this approach is sensitive to outliers, as 
strong outliers may cause excessive rounding errors. Mean Squared Error (MSE) 
One way to mitigate the problem of large outliers is by employing an MSE-based 
range setting. In this method, we determine l Subscript bold xlx and u Subscript bold xux that minimize the mean 
squared error (MSE) between the original and quantized tensor: 
arg min Underscript l Subscript bold x Baseline comma u Subscript bold x Baseline Endscripts MSE left parenthesis bold x comma bold upper Q Subscript l Sub Subscript bold x Subscript comma u Sub Subscript bold x Subscript Baseline right parenthesis commaarg min
lx,ux
MSE(x, Qlx,ux),
(6.46)

6.4
Object Detection and Tracking
251
where bold xx represents the original tensor and bold upper Q Subscript l Sub Subscript bold x Subscript comma u Sub Subscript bold x SubscriptQlx,ux denotes the quantized tensor 
produced using the determined clipping thresholds l Subscript bold xlx and u Subscript bold xux. The optimization 
problem is commonly solved using grid search, golden section method, or analytical 
approximations with closed-form solutions. 
6.4.3.4 
Unilateral Histogram (UH)-Based Activation Quantization 
To address the issue of activation value imbalance, we propose a new approach 
called unilateral histogram (UH)-based activation quantization. We empirically 
study the activation values after forward propagation through the calibration dataset. 
We observe a concentrated distribution of values near the lower bound, accompanied 
by a noticeable decrease in occurrences above zero. Further analysis of the activation 
values reveals that the empirical value of minus−0.2785 is the lower bound. This 
phenomenon can be attributed to the frequent utilization of the Swish (SILU) 
activation function in the YOLO series. 
Algorithm 14: Unilateral histogram (UH)-based activation quantization 
An algorit hm pr esents how  F P 32  hist ogram
 H with  2048  bins is pr ocesse d u
si
ng the U H -based activa tio n q uanti
za
tion, to obtain  th e inde
x m with 
mi
n imal  M  S E.
Based on the empirical evidence, we introduce an asymmetric quantization 
approach called unilateral histogram (UH)-based activation quantization. In UH, we 
iteratively determine the maximum truncation value that minimizes the quantization 
error while keeping the minimum truncation value ﬁxed at minus−0.2785, as illustrated 
in the following: 
StartLayout 1st Row u Subscript bold x Baseline equals arg min Underscript l Subscript bold x Baseline comma u Subscript bold x Baseline Endscripts MSE left parenthesis bold x comma bold upper Q Subscript l Sub Subscript bold x Subscript comma u Sub Subscript bold x Subscript Baseline right parenthesis comma l Subscript bold x Baseline equals negative 0.2785 period EndLayoutux = arg min
lx,ux
MSE(x, Qlx,ux), lx = −0.2785.
(6.47) 
To evaluate the quantization error during the search for the maximum truncation 
value, we utilize the fp32 ﬂoating-point numbers derived from the center values 
of the gathered 2048 bins, as introduced in Algorithm 14. These numbers are 
successively quantized, considering the current maximum truncation value. Through

252
6
Applications
this iterative process, we identify the optimal truncation range. The UH activation 
quantization method offers two key advantages. Firstly, it signiﬁcantly reduces 
calibration time. Secondly, it ensures stable activation quantization by allowing a 
more extensive set of integers to represent the frequently occurring activation values 
between 0 and minus−0.2785, thereby improving quantization accuracy. 
6.4.3.5 
Experiments 
To assess the performance of the proposed Q-YOLO [59] detectors, we conducted 
a comprehensive series of experiments on the widely recognized COCO 2017 
[32] detection benchmark. As one of the most popular object detection datasets, 
COCO 2017 [32] has become instrumental in benchmarking state-of-the-art object 
detectors, thanks to its rich annotations and challenging scenarios. Throughout our 
experimental analysis, we employed standard COCO metrics on the bounding box 
detection task to evaluate the efﬁcacy of our approach. 
Implementation Details 
We randomly selected 1500 training images from the COCO train2017 dataset 
[32] as the calibration data, which served as the foundation for optimizing the model 
parameters. The performance evaluation occurred on the COCO val2017 dataset 
[32], comprising 5000 images. The image size is set to 640times×640. 
Unless otherwise noted, our experiments employed symmetric channel-wise 
quantization for weights and asymmetric layer-wise quantization for activations. 
We consistently applied the MinMax approach for quantizing weights to ensure a 
fair and unbiased comparison. The input and output layers of the model are more 
sensitive to the loss of accuracy. To maintain the model’s overall performance, the 
original accuracy of these layers is usually retained. We also follow this practice. 
Main Results 
We apply our proposed Q-YOLO to quantize YOLOv5s [55], YOLOv5m [55], 
YOLOv7 [58], and YOLOv7x [58], which have an increasing number of parameters. 
The results of the full-precision model and the 8-bit and 4-bit quantized models 
using MinMax, percentile, and Q-YOLO methods are all presented in Table 6.14. 
Table 6.14 compares several quantization approaches and detection methods 
in computing complexity and storage cost. Our Q-YOLO signiﬁcantly accelerates 
computation and reduces storage requirements for various YOLO detectors. Simi-
larly, in terms of detection accuracy, when using Q-YOLO to quantize the YOLOv5 
series models to 8 bits, there is virtually no decline in the average precision (AP) 
value compared to the full-precision model. As the number of model parameters 
increases dramatically, quantizing the YOLOv7 series models to 8 bits results in 
a slight decrease in accuracy. When quantizing models to 4 bits, the accuracy 
experiences a signiﬁcant loss due to the reduced expressiveness of 4-bit integer 
representation. Particularly, when using the MinMax quantization method, the 
model loses all its accuracy, whereas the percentile method, which roughly truncates 
99.99% of the extreme values, fails to bring notable improvement. Differently, Q-

6.4
Object Detection and Tracking
253
Table 6.14 A comparison of various quantization methods applied to YOLOv5s [55], YOLOv5m [55], YOLOv7 [58], and YOLOv7x [58], which have an 
increasing number of parameters, on the COCO val2017 dataset [32]. The term bits (W-A) represents the bit width of weights and activations. The best 
results are displayed in bold 
Models
Method
Bits
SizeSubscript left parenthesis upper M upper B right parenthesis(MB)
OPsSubscript left parenthesis normal upper G right parenthesis(G)
AP
APSubscript 5050
APSubscript 7575
APSubscript ss
APSubscript mm
APSubscript ll
YOLOv5s [55]
Real-valued
32–32
57.6
16.5
37.4
57.1
40.1
21.6
42.3
48.9 
MinMax
8–8
14.4
4.23
37.2
56.9
39.8
21.4
42.2
48.5 
Percentile [28]
36.9
56.4
39.6
21.3
42.4
48.1 
Q-YOLO
37.4
56.9
39.8
21.4
42.4
48.8 
Percentile [28]
4–4
7.7
2.16
7.0
14.2
6.3
4.1
10.7
7.9 
Q-YOLO
14.0
26.2
13.5
7.9
17.6
19.0 
YOLOv5m [55]
Real-valued
32–32
169.6
49.0
45.1
64.1
49
28.1
50.6
57.8 
MinMax
8–8
42.4
12.4
44.9
64
48.9
27.8
50.5
57.4 
Percentile [28]
44.6
63.5
48.4
28.4
50.4
57.8 
Q-YOLO
45.1
64.1
48.9
28
50.6
57.7 
Percentile [28]
4–4
21.2
6.33
19.4
35.6
19.1
14.6
28.3
17.2 
Q-YOLO
28.8
46
30.5
15.4
33.8
38.7 
YOLOv7 [58]
Real-valued
32–32
295.2
104.7
50.8
69.6
54.9
34.9
55.6
66.3 
MinMax
8–8
73.8
27.2
50.6
69.5
54.8
34.1
55.5
65.9 
Percentile [28]
50.5
69.3
54.6
34.5
55.4
66.2 
Q-YOLO
50.7
69.5
54.8
34.8
55.5
66.2 
Percentile [28]
4–4
36.9
14.1
16.7
26.9
17.8
10.3
20.1
20.2 
Q-YOLO
37.3
55.0
40.9
21.5
41.4
53.0 
YOLOv7x [58]
Real-valued
32–32
25.5
189.9
52.5
71.0
56.6
36.6
57.3
68.0 
MinMax
8–8
142.6
49.5
52.3
70.9
56.7
36.6
57.1
67.7 
Percentile [28]
52.0
70.5
56.1
36.0
56.8
67.9 
Q-YOLO
52.4
70.9
56.5
36.2
57.2
67.8 
Percentile [28]
4–4
71.3
25.6
36.8
55.3
40.5
21.2
41.7
49.3 
Q-YOLO
37.6
57.8
42.1
23.7
43.8
49.1

254
6
Applications
YOLO successfully identiﬁes a more appropriate scale for quantization, resulting 
in a considerable enhancement compared to conventional post-training quantization 
(PTQ) methods. 
6.4.3.6 
Ablation Study 
Symmetry in Activation Quantization 
Nowadays, quantization schemes are often subject to hardware limitations; for 
instance, NVIDIA [43] only supports symmetric quantization, as it is more 
inference-speed friendly. Therefore, discussing the symmetry in activation value 
quantization is meaningful. Table 6.15 compares results using Q-YOLO for 
symmetric and asymmetric quantization, with the latter exhibiting higher accuracy. 
The range of negative activation values lies between 0 and −0.2785, while the 
range of positive activation values exceeds that of the negative ones. The accuracy 
will naturally decrease if we force equal integer expression bit numbers on both 
positive and negative sides. Moreover, this decline becomes more pronounced as 
the quantization bit number decreases. 
6.4.3.7 
Quantization Type 
In Table 6.16, we analyze the impact of different quantization types on the perfor-
mance of the YOLOv5s and YOLOv5m models, considering three cases: quantizing 
only the weights (only weights), quantizing only the activation values (only activa-
tion), and quantizing both weights and activation values (weights+activation). The 
results demonstrate that, compared to quantizing the activation values, quantizing 
the weights consistently induces more considerable performance degradation. Addi-
tionally, the lower the number of bits, the greater the loss incurred by quantization. 
Table 6.15 A comparison of symmetrical analysis of activation value quantization. Asymmetric 
indicates the use of an asymmetric activation value quantization scheme, while symmetric refers to 
the symmetric quantization of activation values 
models
Bits
Symmetry
AP
APSubscript 5050
APSubscript 7575
APSubscript ss
APSubscript mm
APSubscript ll
YOLOv5s [55]
Real-valued 
–
37.4 
57.1 
40.1 
21.6 
42.3 
48.9 
6–6
Asymmetric 
35.9 
55.7 
38.3 
20.4 
41.0 
47.6 
Symmetric
34.4 
53.9 
37.0 
19.3 
39.8 
45.0 
4–4
Asymmetric 
14.0 
26.2 
13.5 
7.9
17.6 
19.0 
Symmetric
2.7
5.9
2.2 
1.3
4.2
4.6 
YOLOv5m [55] 
Real-valued 
–
45.1 
64.1 
49.0 
28.1 
50.6 
57.8 
6–6
Asymmetric 
44.0 
63.1 
47.7 
28
49.9 
56.8 
Symmetric
42.4 
61.1 
46.0 
25.3 
48.3 
55.9 
4–4
Asymmetric 
28.8 
46.0 
30.5 
15.4 
33.8 
38.7 
Symmetric
11.3 
24.8
8.6 
7.5
15.2 
14.5

6.4
Object Detection and Tracking
255
Table 6.16 A comparison of quantization type. The term only weights signiﬁes that only the 
weights are quantized, only activation indicates that only the activation values are quantized, and 
weights+activation represents the quantization of both activation values and weights 
models
Bits
Quantization type 
AP
AP50 AP75 APs APm APl 
YOLOv5s [55] 
Real-valued –
37.4
57.1 
40.1 
21.6 42.3 48.9 
6–32
Only weights
36.7(−0.7) 
56.6 
39.3 
20.9 41.4 48.4 
32–6
Only activation
36.6(−0.8) 
56.2 
39.3 
21.0 42.0 47.9 
6–6
Weights+activation 35.9
55.7 
38.3 
20.4 41.0 47.6 
4–32
Only weights
19.6(−16.3) 35.6 
19.3 
11.3 22.5 25.7 
32–4
Only activation
30.6(−5.3) 
49.1 
32.6 
17.0 36.7 40.7 
4–4
Weights+activation 14.0
26.2 
13.5 
7.9 17.6 19 
YOLOv5m [55] Real-valued –
45.1
64.1 
49.0 
28.1 50.6 57.8 
6–32
Only weights
44.7(−0.4) 
63.9 
48.6 
28.0 50.3 57.3 
32–6
Only activation
44.3(−0.8) 
63.4 
48.1 
28.4 50.3 57.2 
6–6
Weights+activation 44
63.1 
47.7 
28.0 49.9 56.8 
4–32
Only weights
34.6(−9.4) 
54.0 
37.3 
20.0 39.2 45.3 
32–4
Only activation
37.7(−6.3) 
57.3 
41.8 
23.7 44.1 51.0 
4–4
Weights+activation 28.8
46.0 
30.5 
15.4 33.8 38.7 
In YOLO, the weights learned by a neural network essentially represent the knowl-
edge acquired by the network, making the precision of the weights crucial for model 
performance. In contrast, activation values serve as intermediate representations 
of input data propagating through the network and can tolerate some degree of 
quantization error to a certain extent. 
6.4.3.8 
Inference Speed 
To practically verify the acceleration beneﬁts brought about by our quantization 
scheme, we conducted inference speed tests on both GPU and CPU platforms. For 
the GPU, we selected the commonly used desktop GPU NVIDIA RTX 4090 [43] 
and the NVIDIA Tesla T4 [43], often used in computing centers for inference 
tasks. Due to our limited CPU resources, we only tested Intel products, the 
i7-12700H and i9-10900, both of which have times×86 architecture. We chose 
TensorRT [1] and OpenVINO [2] for deployment tools. The entire process involved 
converting the weights from the torch framework into an ONNX model with QDQ 
nodes and deploying them onto speciﬁc inference frameworks. The inference mode 
was set to single-image serial inference, with an image size of 640 times 640640 × 640. As  
most current inference frameworks only support symmetric quantization and 8-
bit quantization, we had to choose a symmetric 8-bit quantization scheme, which 
resulted in a minimal decrease in accuracy compared to asymmetric schemes. As 
shown in Table 6.17, the acceleration is extremely signiﬁcant, especially for the 
larger YOLOv7 model, wherein the speedup ratio when using a GPU even exceeded

256
6
Applications
Table 6.17 The inference speed of the quantized model is essential. The quantization scheme 
adopts uniform quantization, with single-image inference mode and an image size of 640 times× 640. 
TensorRT [1]is selected as the GPU inference library, while OpenVINO [2] is chosen for the CPU 
inference library 
GPU speed/ms
Intel CPU speed/ms 
Models
Bits
AP
RTX 4090 
Tesla T4 
i7-12700H(times×86) 
i9-10900(times×86) 
YOLOv5s 
32–32 
37.4
4.9
7.1
48.7
38.7 
8–8
37.3
3.0
4.5
33.6
23.4 
YOLOv7
32–32 
50.8 
16.8
22.4
269.8
307.8 
8–8
50.6
5.4
7.8
120.4
145.2 
3times× compared to the full-precision model. This demonstrates that quantization in 
real-time detectors can bring about a remarkable acceleration. 
References 
1. NVIDIA TensorRT. https://developer.nvidia.com/tensorrt. Accessed: 2022-09-03. 
2. OpenVINO Toolkit. https://docs.openvinotoolkit.org/latest/index.html. Accessed: 2022-09-03. 
3. Adrian Bulat and Georgios Tzimiropoulos. Xnor-net++: Improved binary neural networks. 
arXiv preprint arXiv:1909.13863, 2019. 
4. Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. 
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 
6154–6162, 2018. 
5. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep 
neural networks with binary weights during propagations. In Advances in neural information 
processing systems, pages 3123–3131, 2015. 
6. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 
Binarized neural networks: Training deep neural networks with weights and activations 
constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. 
7. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 
Binarized neural networks: Training deep neural networks with weights and activations 
constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. 
8. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern 
Recognition, pages 248–255, 2009. 
9. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisser-
man. The pascal visual object classes (voc) challenge. International Journal of Computer 
Vision, 2010. 
10. Pedro Felzenszwalb and Ramin Zabih. Discrete optimization algorithms in computer vision. 
Tutorial at CVPR, 2007. 
11. Sicheng Gao, Runqi Wang, Liuyang Jiang, and Baochang Zhang. 1-bit wavenet: compressing 
a generative neural network in speech recognition with two binarized methods. In 2021 IEEE 
16th conference on industrial electronics and applications (ICIEA), pages 2043–2047. IEEE, 
2021. 
12. Ross Girshick. Fast r-cnn. In ICCV, 2015. 
13. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for 
accurate object detection and semantic segmentation. In CVPR, 2014.

References
257
14. Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist 
temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks. 
In Proceedings of the 23rd international conference on Machine learning, pages 369–376, 
2006. 
15. Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David 
Doermann. 
Projection convolutional neural networks for 1-bit cnns via discrete back 
propagation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2019. 
16. Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David 
Doermann. 
Projection convolutional neural networks for 1-bit cnns via discrete back 
propagation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, 
pages 8344–8351, 2019. 
17. Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Jianzhuang Liu, Guodong Guo, and 
Rongrong Ji. Bayesian optimized 1-bit cnns. In Proceedings of the IEEE/CVF international 
conference on computer vision, pages 4909–4917, 2019. 
18. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 
Delving deep into rectiﬁers: 
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE 
international conference on computer vision, pages 1026–1034, 2015. 
19. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 
Deep residual learning for 
image recognition. In Proceedings of the IEEE conference on computer vision and pattern 
recognition, pages 770–778, 2016. 
20. Felix Heide, Wolfgang Heidrich, and Gordon Wetzstein. Fast and ﬂexible convolutional sparse 
coding. In CVPR, pages 5135–5143, 2015. 
21. Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE 
conference on Computer Vision and Pattern Recognition, pages 7132–7141, 2018. 
22. Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark 
for generic object tracking in the wild. IEEE transactions on pattern analysis and machine 
intelligence, 43(5):1562–1577, 2019. 
23. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 
Binarized neural networks. In Advances in neural information processing systems, pages 4107– 
4115, 2016. 
24. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, 
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for 
efﬁcient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer 
vision and pattern recognition, pages 2704–2713, 2018. 
25. Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim. Binaryduo: Reducing gradient 
mismatch in binary activation network by coupling binary activations. 
In International 
Conference on Learning Representations, 2019. 
26. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 
2009. 
27. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep 
convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 
pages 1097–1105, 2012. 
28. Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully quantized 
network for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision 
and Pattern Recognition, pages 2810–2819, 2019. 
29. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge 
Belongie. 
Feature pyramid networks for object detection. 
In Proceedings of the IEEE 
conference on computer vision and pattern recognition, pages 2117–2125, 2017. 
30. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense 
object detection. In Proceedings of the IEEE international conference on computer vision, 
pages 2980–2988, 2017. 
31. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, 
Piotr Dollár, and C Lawrence Zitnick. 
Microsoft coco: Common objects in context. 
In 
European Conference on Computer Vision (ECCV), pages 740–755, 2014.

258
6
Applications
32. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, 
Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 
2014. 
33. Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. 
In Advances in Neural Information Processing Systems, pages 345–353, 2017. 
34. Chunlei Liu, Wenrui Ding, Yuan Hu, Baochang Zhang, Jianzhuang Liu, Guodong Guo, 
and David Doermann. Rectiﬁed binary convolutional networks with generative adversarial 
learning. International Journal of Computer Vision, 129:998–1012, 2021. 
35. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, 
and Alexander C Berg. Ssd: Single shot multibox detector. In Proc. of ECCV, 2016. 
36. Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards 
precise binary neural network with generalized activation functions. 
arXiv preprint 
arXiv:2003.03488, 2020. 
37. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real 
net: Enhancing the performance of 1-bit cnns with improved representational capability and 
advanced training algorithm. In Proceedings of the European conference on computer vision 
(ECCV), pages 722–737, 2018. 
38. Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. 
Training binary 
neural networks with real-to-binary convolutions. In International Conference on Learning 
Representations, 2019. 
39. Mark D McDonnell. Training wide residual networks for deployment using a single bit for 
each weight. arXiv preprint arXiv:1802.08530, 2018. 
40. Matthias Mueller, Neil Smith, and Bernard Ghanem. 
A benchmark and simulator for uav 
tracking. 
In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The 
Netherlands, October 11–14, 2016, Proceedings, Part I 14, pages 445–461. Springer, 2016. 
41. Vinod Nair and Geoffrey E Hinton. 
Rectiﬁed linear units improve restricted boltzmann 
machines. In Proceedings of the 27th international conference on machine learning (ICML-
10), pages 807–814, 2010. 
42. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. 
Reading digits in natural images with unsupervised feature learning. 2011. 
43. NVIDIA. Nvidia corporation, 2022. Available at: https://www.nvidia.com/. 
44. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex 
Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative 
model for raw audio. arXiv preprint arXiv:1609.03499, 2016. 
45. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, 
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in 
pytorch. In NIPS Workshops, 2017. 
46. Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, and Lan McGraw. On the compression 
of recurrent neural networks with an application to lvcsr acoustic modeling for embedded 
speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal 
Processing (ICASSP), pages 5970–5974. IEEE, 2016. 
47. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet 
classiﬁcation using binary convolutional neural networks. 
In European Conference on 
Computer Vision, pages 525–542. Springer, 2016. 
48. Joseph Redmon and Ali Farhadi. 
Yolov3: An incremental improvement. 
arXiv preprint 
arXiv:1804.02767, 2018. 
49. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time 
object detection with region proposal networks. In NeurIPS, 2015. 
50. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time 
object detection with region proposal networks. IEEE Transactions on Pattern Analysis and 
Machine Intelligence, 2016.

References
259
51. Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio 
Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. 
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 
658–666, 2019. 
52. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng 
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual 
recognition challenge. International journal of computer vision, 115(3):211–252, 2015. 
53. Kunal Pratap Singh, Dahyun Kim, and Jonghyun Choi. 
Learning architectures for binary 
networks. arXiv preprint arXiv:2002.06963, 2020. 
54. Siyang Sun, Yingjie Yin, Xingang Wang, De Xu, Wenqi Wu, and Qingyi Gu. Fast object 
detection based on binary deep convolution neural networks. CAAI transactions on intelligence 
technology, 3(4):191–197, 2018. 
55. Ultralytics. YOLOv5: PyTorch implementation of YOLOv5 real-time object detection. https:// 
github.com/ultralytics/yolov5, 2021. 
56. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. 
Conditional image generation with pixelcnn decoders. 
Advances in neural information 
processing systems, 29, 2016. 
57. Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng Tao Shen. Tbn: 
Convolutional neural network with ternary inputs and binary weights. In Proceedings of the 
European Conference on Computer Vision (ECCV), pages 315–332, 2018. 
58. Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-
of-freebies sets new state-of-the-art for real-time object detectors. 
In Proceedings of the 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7464–7475, 2023. 
59. Ming Wang, Hui Xian Sun, Jun Shi, Xuhui Liu, Baochang Zhang, and Xianbin Cao. Q-yolo: 
Efﬁcient inference for real-time object detection. ArXiv, abs/2307.04816, 2023. 
60. Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. Eca-
net: Efﬁcient channel attention for deep convolutional neural networks. In Proceedings of 
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11534–11542, 
2020. 
61. Xiaodi Wang, Baochang Zhang, Ce Li, Rongrong Ji, Jungong Han, Xianbin Cao, and 
Jianzhuang Liu. Modulated convolutional networks. In IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR), pages 840–848, 2018. 
62. Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. 
Learning channel-wise 
interactions for binary convolutional neural networks. In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, pages 568–577, 2019. 
63. Ziwei Wang, Ziyi Wu, Jiwen Lu, and Jie Zhou. Bidet: An efﬁcient binarized object detector. In 
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 
2049–2058, 2020. 
64. Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional 
block attention module. 
In Proceedings of the European conference on computer vision 
(ECCV), pages 3–19, 2018. 
65. Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. 
Online object tracking: A benchmark. 
In 
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2411– 
2418, 2013. 
66. Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE Transactions 
on Pattern Analysis & Machine Intelligence, 37(09):1834–1848, 2015. 
67. Xu Xiang, Yanmin Qian, and Kai Yu. Binary deep neural networks for speech recognition. In 
INTERSPEECH, pages 533–537, 2017. 
68. Sheng Xu, Zhendong Liu, Xuan Gong, Chunlei Liu, Mingyuan Mao, and Baochang Zhang. 
Amplitude suppression and direction activation in networks for 1-bit faster r-cnn. Proceedings 
of the 4th International Workshop on Embedded and Mobile Deep Learning, 2020. 
69. Li Yang, Zhezhi He, and Deliang Fan. Binarized depthwise separable neural network for object 
tracking in fpga. In Proceedings of the 2019 on Great Lakes Symposium on VLSI, pages 347– 
350, 2019.

260
6
Applications
70. Linlin Yang, Ce Li, Jungong Han, Chen Chen, Qixiang Ye, Baochang Zhang, Xianbin Cao, 
and Wanquan Liu. Image reconstruction via manifold constrained convolutional sparse coding 
for image sets. JSTSP, 11(7):1072–1081, 2017. 
71. Shouyi Yin, Peng Ouyang, Shixuan Zheng, Dandan Song, Xiudong Li, Leibo Liu, and Shaojun 
Wei. A 141 uw, 2.46 pj/neuron binarized convolutional neural network based self-learning 
speech recognition processor in 28nm cmos. 
In 2018 IEEE Symposium on VLSI Circuits, 
pages 139–140. IEEE, 2018. 
72. Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and Thomas Huang. Unitbox: An 
advanced object detection network. In Proceedings of the 24th ACM international conference 
on Multimedia, pages 516–520, 2016. 
73. Junhe Zhao, Sheng Xu, Runqi Wang, Baochang Zhang, Guodong Guo, David S. Doermann, 
and Dianmin Sun. Data-adaptive binary neural networks for efﬁcient object detection and 
recognition. Pattern Recognit. Lett., 153:239–245, 2021. 
74. Shixuan Zheng, Peng Ouyang, Dandan Song, Xiudong Li, Leibo Liu, Shaojun Wei, and Shouyi 
Yin. An ultra-low power binarized convolutional neural network-based speech recognition 
processor with on-chip self-learning. IEEE Transactions on Circuits and Systems I: Regular 
Papers, 66(12):4648–4661, 2019. 
75. Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-
iou loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI 
conference on artiﬁcial intelligence, volume 34, pages 12993–13000, 2020. 
76. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 
Dorefa-
net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv 
preprint arXiv:1606.06160, 2016. 
77. Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. 
Objects as points. 
arXiv preprint 
arXiv:1904.07850, 2019.

